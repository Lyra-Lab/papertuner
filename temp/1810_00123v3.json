{
  "id": "http://arxiv.org/abs/1810.00123v3",
  "title": "Generalization and Regularization in DQN",
  "authors": [
    "Jesse Farebrother",
    "Marlos C. Machado",
    "Michael Bowling"
  ],
  "abstract": "Deep reinforcement learning algorithms have shown an impressive ability to\nlearn complex control policies in high-dimensional tasks. However, despite the\never-increasing performance on popular benchmarks, policies learned by deep\nreinforcement learning algorithms can struggle to generalize when evaluated in\nremarkably similar environments. In this paper we propose a protocol to\nevaluate generalization in reinforcement learning through different modes of\nAtari 2600 games. With that protocol we assess the generalization capabilities\nof DQN, one of the most traditional deep reinforcement learning algorithms, and\nwe provide evidence suggesting that DQN overspecializes to the training\nenvironment. We then comprehensively evaluate the impact of dropout and\n$\\ell_2$ regularization, as well as the impact of reusing learned\nrepresentations to improve the generalization capabilities of DQN. Despite\nregularization being largely underutilized in deep reinforcement learning, we\nshow that it can, in fact, help DQN learn more general features. These features\ncan be reused and fine-tuned on similar tasks, considerably improving DQN's\nsample efficiency.",
  "text": "Generalization and Regularization in DQN\nJesse Farebrother∗1, Marlos C. Machado2, and Michael Bowling1,3\n1University of Alberta, 2Google Research, 3DeepMind Alberta\nAbstract\nDeep reinforcement learning algorithms have shown an impressive\nability to learn complex control policies in high-dimensional tasks. However,\ndespite the ever-increasing performance on popular benchmarks, policies\nlearned by deep reinforcement learning algorithms can struggle to generalize\nwhen evaluated in remarkably similar environments. In this paper we\npropose a protocol to evaluate generalization in reinforcement learning\nthrough diﬀerent modes of Atari 2600 games. With that protocol we assess\nthe generalization capabilities of DQN, one of the most traditional deep\nreinforcement learning algorithms, and we provide evidence suggesting that\nDQN overspecializes to the training environment. We then comprehensively\nevaluate the impact of dropout and ℓ2 regularization, as well as the\nimpact of reusing learned representations to improve the generalization\ncapabilities of DQN. Despite regularization being largely underutilized in\ndeep reinforcement learning, we show that it can, in fact, help DQN learn\nmore general features. These features can be reused and ﬁne-tuned on\nsimilar tasks, considerably improving DQN’s sample eﬃciency.\n1\nIntroduction\nRecently, reinforcement learning (RL) algorithms have proven very successful on\ncomplex high-dimensional problems, in large part due to the use of deep neural\nnetworks for function approximation (e.g., Mnih et al., 2015; Silver et al., 2016).\nDespite the generality of the proposed solutions, applying these algorithms to\nslightly diﬀerent environments often requires agents to learn the new task from\nscratch. The learned policies rarely generalize to other domains and the learned\nrepresentations are seldom reusable. On the other hand, deep neural networks\nare lauded for their generalization capabilities (e.g., Lecun et al., 1998), with\nsome communities heavily relying on reusing learned representations in diﬀerent\nproblems. In light of the successes of supervised learning methods, the lack of\ngeneralization or reusable knowledge (i.e., policies, representation) acquired by\ncurrent deep RL algorithms is somewhat surprising.\n∗Corresponding author. Contact: jfarebro@cs.ualberta.ca.\n1\narXiv:1810.00123v3  [cs.LG]  17 Jan 2020\nIn this paper we investigate whether the representations learned by deep RL\nmethods can be generalized, or at the very least reused and reﬁned on small\nvariations to the task at hand. We evaluate the generalization capabilities of\nDQN (Mnih et al., 2015), one of the most representative algorithms in the family\nof value-based deep RL methods; and we further explore whether the experience\ngained by the supervised learning community to improve generalization and to\navoid overﬁtting can be used in deep RL. We employ conventional supervised\nlearning techniques such as regularization and ﬁne-tuning (i.e., reusing and\nreﬁning the representation) to DQN and we show that a learned representation\ntrained with regularization allows us to learn more general features that can be\nreused and ﬁne-tuned.\nWe are interested in agents that generalize across tasks that have similar\nunderlying dynamics but that have diﬀerent observation spaces. In this context,\nwe see generalization as the agent’s ability to abstract aspects of the environment\nthat do not matter. The main contributions of this work are:\n1. We propose the use of the new modes and diﬃculties of Atari 2600 games\nas a platform for evaluating generalization in RL and we provide the ﬁrst\nbaseline results in this platform. These game modes allow agents to be\ntrained in one environment and evaluated in a slightly diﬀerent environment\nthat still captures key concepts of the original environment (e.g., game\nsprites, dynamics).\n2. Under this new notion of generalization in RL, we thoroughly evaluate the\ngeneralization capabilities of DQN and we provide evidence that it exhibits\nan overﬁtting trend.\n3. Inspired by the current literature in regularizing deep neural networks to\nimprove robustness and adaptability, we apply regularization techniques\nto DQN and show they vastly improve its sample eﬃciency when faced\nwith new tasks. We do so by analyzing the impact of regularization on the\npolicy’s ability to not only perform zero-shot generalization, but to also\nlearn a more general representation amenable to ﬁne-tuning on diﬀerent\nproblems.\n2\nBackground\nWe begin our exposition with an introduction of basic terms and concepts for\nsupervised learning and reinforcement learning. We then discuss the related\nwork, focusing on generalization in reinforcement learning.\n2.1\nRegularization in Supervised Learning\nIn the supervised learning problem we are given a dataset of examples represented\nby a matrix X ∈Rm×n with m training examples of dimension n, and a vector\ny ∈R1×m denoting the output target yi for each training example Xi. We want\n2\nto learn a function which maps each training example Xi to its predicted output\nlabel ˆyi. The goal is to learn a robust model that accurately predicts yi from Xi\nwhile generalizing to unseen training examples. In this paper we focus on using\na neural network parameterized by the weights θ to learn the function f such\nthat f(Xi; θ) = ˆyi. We typically train these models by minimizing\nmin\nθ\nλ\n2 ∥θ∥2\n2 + 1\nm\nm\nX\ni=1\nL\n\u0000yi, f(Xi; θ)\n\u0001\n,\nwhere L is a diﬀerentiable loss function which outputs a scalar determining the\nquality of the prediction (e.g., squared error loss). The ﬁrst term is a form\nof regularization, that is, ℓ2 regularization, which encourages generalization\nby imposing a penalty on large weight vectors. The hyperparameter λ is the\nweighted importance of the regularization term.\nAnother popular regularization technique is dropout (Srivastava et al., 2014).\nWhen using dropout, during forward propagation each neural unit is set to zero\naccording to a Bernoulli distribution with probability p ∈[0, 1], referred to as the\ndropout rate. Dropout discourages the network from relying on a small number\nof neurons to make a prediction, making memorization of the dataset harder.\nPrior to training, the network parameters are usually initialized through a\nstochastic process such as Xavier initialization (Glorot and Bengio, 2010). We\ncan also initialize the network using pre-trained weights from a diﬀerent task. If\nwe reuse one or more pre-trained layers we say the weights encoded by those\nlayers will be ﬁne-tuned during training (e.g., Razavian et al., 2014; Long et al.,\n2015), a topic we explore in Section 6.\n2.2\nReinforcement Learning\nIn the reinforcement learning (RL) problem an agent interacts with an environ-\nment with the goal of maximizing cumulative long term reward. RL problems\nare often modeled as a Markov decision process (MDP), deﬁned by a 5-tuple\n⟨S, A, p, r, γ⟩. At a discrete time step t, the agent observes the current state St ∈S\nand takes an action At ∈A to transition to the next state St+1 ∈S according\nto the transition dynamics function p(s′ | s, a) .= P(St+1 = s′ | St = s , At = a).\nThe agent receives a reward signal Rt+1 according to the reward function\nr : S × A →R. The agent’s goal is to learn a policy π : S × A →[0, 1], written\nas π(a | s), which is deﬁned as the conditional probability of taking action a in\nstate s. The learning agent reﬁnes its policy with the objective of maximizing\nthe expected return, that is, the cumulative discounted reward incurred from\ntime t, deﬁned by Gt .= P∞\nk=0 γkRt+k+1, where γ ∈[0, 1) is the discount factor.\nQ-learning (Watkins and Dayan, 1992) is a traditional approach to learning\nan optimal policy from samples obtained from interactions with the environment.\nFor a given policy π, we deﬁne the state-action value function as the expected\nreturn conditioned on a state and action qπ(s, a) .= Eπ\n\u0002\nGt|S0 = s, A0 = a\n\u0003\n. The\nagent iteratively updates the state-action value function based on samples from\n3\nthe environment using the update rule\nQ(St, At) ←Q(St, At) + α\n\u0002\nRt+1 + γ max\na′∈A Q(St+1, a′) −Q(St, At)\n\u0003\n,\nwhere t denotes the current timestep and α the step size. Generally, due to the\nexploding size of the state space in many real-world problems, it is intractable to\nlearn a state-action pairing for the entire MDP. Instead we learn an approximation\nto the true function qπ.\nDQN approximates the state-action value function such that Q(s, a; θ) ≈\nqπ(s, a), where θ denotes the weights of a neural network. The network takes as\ninput some encoding of the current state St and outputs |A| scalars corresponding\nto the state-action values for St. DQN is trained to minimize\nL\nDQN =\nE\nτ ∼U(·)\n\u0002\u0000Rt+1 + γ max\na′∈A Q(St+1, a′; θ−) −Q(St, At; θ)\n\u00012\u0003\n,\nwhere τ = (St, At, Rt+1, St+1) are uniformly sampled from U(·), the experience\nreplay buﬀer ﬁlled with experience collected by the agent. The weights θ−of a\nduplicate network are updated less frequently for stability purposes.\n2.3\nRelated Work\nIn reinforcement learning, regularization is rarely applied to value-based methods.\nThe few existing studies often focus on single-task settings with linear function\napproximation (e.g., Farahmand et al., 2008; Kolter and Ng, 2009). Here we look\nat the reusability, in diﬀerent tasks, of learned representations. The closest work\nto ours is Cobbe et al.’s (2019), which also looks at regularization techniques\napplied to deep RL. However, diﬀerent from Cobbe et al., here we also evaluate\nthe impact of regularization when ﬁne-tuning value functions. Moreover, in this\npaper we propose a diﬀerent platform for evaluating generalization in RL, which\nwe discuss below.\nThere are several recent papers that support our results with respect to the\nlimited generalization capabilities of deep RL agents. Nevertheless, they often\ninvestigate generalization in light of diﬀerent aspects of an environment such as\nnoise (e.g., Zhang et al., 2018a) and start state distribution (e.g., Rajeswaran\net al., 2017; Zhang et al., 2018a,b). There are also some proposals for evaluating\ngeneralization in RL through procedurally generated or parametrized environ-\nments (e.g., Finn et al., 2017; Juliani et al., 2019; Justesen et al., 2018; Whiteson\net al., 2011; Witty et al., 2018). These papers do not investigate generalization\nin deep RL the same way we do. Moreover, as aforementioned, here we also\npropose using a diﬀerent testbed, the modes and diﬃculties of Atari 2600 games.\nWith respect to that, Witty et al.’s (2018) work is directly related to ours, as\nthey propose parameterizing a single Atari 2600 game, Amidar, as a way to\nevaluate generalization in RL. The use of modes and diﬃculties is much more\ncomprehensive and it is free of experimenters’ bias.\nIn summary, our work adds to the growing literature on generalization in\nreinforcement learning. To the best of our knowledge, our paper is the ﬁrst to\n4\nFreeway\nHero\nBreakout\nSpace Invaders\nFigure 1: Column show the variations between two ﬂavours of each game.\ndiscuss overﬁtting in Atari 2600 games, to present results using the Atari 2600\nmodes as testbed, and to demonstrate the impact regularization can have in\nvalue function ﬁne-tuning in reinforcement learning.\n3\nThe ALE as a Platform for Evaluating Gener-\nalization in Reinforcement Learning\nThe Arcade Learning Environment (ALE) is a platform used to evaluate agents\nacross dozens of Atari 2600 games (Bellemare et al., 2013). It is one of the\nstandard evaluation platforms in the ﬁeld and has led to several exciting al-\ngorithmic advances (e.g., Mnih et al., 2015). The ALE poses the problem of\ngeneral competency by having agents use the same learning algorithm to perform\nwell in as many games as possible, without using any game speciﬁc knowledge.\nLearning to play multiple games with the same agent, or learning to play a\ngame by leveraging knowledge acquired in a diﬀerent game is harder, with fewer\nsuccesses being known (Rusu et al., 2016; Kirkpatrick et al., 2016; Parisotto\net al., 2016; Schwarz et al., 2018; Espeholt et al., 2018).\nThroughout this paper we evaluate the generalization capabilities of our\nagents using hold out test environments. We do so with diﬀerent modes and\ndiﬃculties of Atari 2600 games, features the ALE recently started to support\n(Machado et al., 2018).\nGame modes, which were originally native to the\nAtari 2600 console, generally give us modiﬁcations of each Atari 2600 game by\nmodifying sprites, velocities, and the observability of objects. These modes oﬀer\nan excellent framework for evaluating generalization in RL. They were designed\nseveral decades ago and remain free from experimenter’s bias as they were not\ndesigned with the goal of being a testbed for AI agents, but with the goal of\nbeing varied1 and entertaining to humans. Figure 1 depicts some of the diﬀerent\n1There are 48 Atari 2600 games with more than one ﬂavour in the ALE. These games have\n414 diﬀerent ﬂavours (Machado et al., 2018). Notice that, on average, each game has less than\n5\nmodes and diﬃculties available in the ALE. As Machado et al. (2018), hereinafter\nwe call each mode/diﬃcult pair a ﬂavour.\nBesides having the properties that made the ALE successful in the RL\ncommunity, the diﬀerent game ﬂavours allow us to look at the problem of\ngeneralization in RL from a diﬀerent perspective. Because of hardware limitations,\nthe diﬀerent ﬂavours of an Atari 2600 game could not be too diﬀerent from\neach other.2 Therefore, diﬀerent ﬂavours can be seen as small variations of the\ndefault game, with few latent variables being changed. In this context, we pose\nthe problem of generalization in RL as the ability to identify invariances across\ntasks with high-dimensional observation spaces. Such an objective is based on\nthe assumption that the underlying dynamics of the world does not vary much.\nInstead of requiring an agent to play multiple games that are visually very\ndiﬀerent or even non-analogous, the notion of generalization we propose requires\nagents to play games that are visually very similar and that can be played with\npolicies that are conceptually similar, at least from a human perspective. In a\nsense, the notion of generalization we propose requires agents to be invariant to\nchanges in the observation space.\nIntroducing ﬂavours to the ALE is not one of our contributions, this was\ndone by Machado et al. (2018). Nevertheless, here we provide a ﬁrst concrete\nsuggestion on how to use these ﬂavours in reinforcement learning. Our paper\nalso provides the ﬁrst baseline results for diﬀerent ﬂavours of Atari 2600 games\nsince Machado et al. (2018) incorporated them to the ALE but did not report\nany results on them. The baseline results for the traditional deep RL setting are\navailable in Table 5 while the full baseline results for regularization are available\nin Table 6. Because these baseline results are quite broad, encompassing multiple\ngames and ﬂavours, and because we wanted to ﬁrst discuss other experiments\nand analyses, Tables 5 and 6 are at the end of the paper. They follow Machado\net al.’s (2018) suggestions on how to report Atari 2600 games results.\nWe believe our proposal is a more realistic and tractable way of deﬁning\ngeneralization in decision-making problems. Instead of focusing on the samples\n(s, a, s′, r), simply requiring them be drawn from the same distribution, we look\nat a more general notion of generalization where we consider multiple tasks, with\nthe assumption that tasks are sampled from the same distribution, similar to\nthe meta-RL setting. Nevertheless, we concretely constrain the distribution of\ntasks with the notion that only few latent variables describing the environment\ncan vary. This also allows us to have a new perspective towards an agents’\ninability to succeed in slightly diﬀerent tasks from those they are trained on.\nAt the same time, this is more challenging than using, for example, diﬀerent\nparametrizations of an environment, as often done when evaluating meta-RL\nalgorithms. In fact, we could not obtain any positive results in these Atari\n2600 games with traditional meta-RL algorithms (e.g., Finn et al., 2017; Nichol\net al., 2018a) and to the best of our knowledge, there are no reports of meta-RL\nalgorithms succeeding in Atari 2600 games. Because of that, we do not further\n10 ﬂavours though. This is another challenge since other settings often assume access to many\nmore environment variations (e.g., via procedural content generation).\n2The Atari 2600 console has only 2KB of RAM.\n6\nFreeway: a chicken must cross a road containing multiple lanes of moving traﬃc within a\nprespeciﬁed time limit. In all modes of Freeway the agent is rewarded for reaching the top of\nthe screen and is subsequently teleported to the bottom of the screen. If the chicken collides\nwith a vehicle in diﬃculty 0 it gets bumped down one lane of traﬃc, alternatively, in diﬃculty\n1 the chicken gets teleported to its starting position at the bottom of the screen. Mode 1\nchanges some vehicle sprites to include buses, adds more vehicles to some lanes, and increases\nthe velocity of all vehicles. Mode 4 is almost identical to Mode 1; the only diﬀerence being\nvehicles can oscillate between two speeds. Mode 0, with diﬃculty 0, is the default one.\nHero: you control a character who must navigate a maze in order to save a trapped miner\nwithin a cave system. The agent scores points for forward progression such as clearing an\nobstacle or killing an enemy. Once the miner is rescued, the level is terminated and you\ncontinue to the next level in a diﬀerent maze. Some levels have partially observable rooms,\nmore enemies, and more diﬃcult obstacles to traverse. Past the default mode (m0d0), each\nsubsequent mode starts oﬀat increasingly harder levels denoted by a level number increasing\nby multiples of 5. The default mode starts you oﬀat level 1, mode 1 starts at level 5, etc.\nBreakout: you control a paddle which can move horizontally along the bottom of the screen.\nAt the beginning of the game, or on a loss of life, the ball is set into motion and can bounce\noﬀthe paddle and collide with bricks at the top of the screen. The objective of the game\nis to break all the bricks without having the ball fall below your paddles horizontal plane.\nSubsequently, mode 12 of Breakout hides the bricks from the player until the ball collides\nwith the bricks in which case the bricks ﬂash for a brief moment before disappearing again.\nSpace Invaders: you control a spaceship which can move horizontally along the bottom of\nthe screen. There is a grid of aliens above you and the objective of the game is to eliminate all\nthe aliens. You are aﬀorded some protection from the alien bullets with three barriers just\nabove your spaceship. Diﬃculty 1 of Space Invaders widens your spaceships sprite making it\nharder to dodge enemy bullets. Mode 1 of Space Invaders causes the shields above you to\noscillate horizontally. Mode 9 of Space Invaders is similar to Mode 12 of Breakout where\nthe aliens are partially observable until struck with the player’s bullet. Mode 0, with diﬃculty\n0, is the default one.\nFigure 2: Description of the game ﬂavours used in the paper.\ndiscuss these approaches.\nIn this paper we focus on a subset of Atari 2600 games with multiple ﬂavours.\nBecause we wanted to provide exhaustive results averaging over multiple trials,\nhere we use 13 ﬂavours obtained from 4 games: Freeway, HERO, Breakout,\nand Space Invaders. In Freeway, the diﬀerent modes vary the speed and\nnumber of vehicles, while diﬀerent diﬃculties change how the player is penalized\nfor running into a vehicle. In HERO, subsequent modes start the player oﬀat\nincreasingly harder levels of the game. The mode we use in Breakout makes\nthe bricks partially observable. Modes of Space Invaders allow for oscillating\nshield barriers, increasing the width of the player sprite, and partially observable\naliens. Figure 1 depicts some of these ﬂavours and Figure 2 further explains the\ndiﬀerence between the ALE ﬂavours we used.3\n3Videos of the diﬀerent modes are available in the following link: https://goo.gl/pCvPiD.\n7\nTable 1: Direct policy evaluation. Each agent is initially trained in the default\nﬂavour for 50M frames then evaluated in each listed game ﬂavour. Reported\nnumbers are averaged over ﬁve runs. Std. dev. is reported between parentheses.\nGame Variant\nEvaluation\nLearn Scratch\nFreeway\nm1d0\n0.2\n(0.2)\n4.8\n(9.3)\nm1d1\n0.1\n(0.1)\n0.0\n(0.0)\nm4d0\n15.8\n(1.0)\n29.9\n(0.7)\nHero\nm1d0\n82.1\n(89.3)\n1425.2\n(1755.1)\nm2d0\n33.9\n(38.7)\n326.1\n(130.4)\nBreakout\nm12d0\n43.4\n(11.1)\n67.6\n(32.4)\nSpace Invaders\nm1d0\n258.9\n(88.3)\n753.6\n(31.6)\nm1d1\n140.4\n(61.4)\n698.5\n(31.3)\nm9d0\n179.0\n(75.1)\n518.0\n(16.7)\n4\nGeneralization of the Policies Learned by DQN\nIn order to test the generalization capabilities of DQN, we ﬁrst evaluate whether\na policy learned in one ﬂavour can perform well in a diﬀerent ﬂavour.\nAs\naforementioned, diﬀerent modes and diﬃculties of a single game look very\nsimilar. If the representation encodes a robust policy we might expect it to\nbe able to generalize to slight variations of the underlying reward signal, game\ndynamics, or observations. Evaluating the learned policy in a similar but diﬀerent\nﬂavour can be seen as evaluating generalization in RL, similar to cross-validation\nin supervised learning.\nTo evaluate DQN’s ability to generalize across ﬂavours, we evaluate the\nlearned ϵ-greedy policy on a new ﬂavour after training for 50M frames in the\ndefault ﬂavour, m0d0 (mode 0, diﬃculty 0). We measure the cumulative reward\naveraged over 100 episodes in the new ﬂavour, adhering to the evaluation protocol\nsuggested by Machado et al. (2018). The results are summarized in Table 1.\nBaseline results where the agent is trained from scratch for 50M frames in the\ntarget ﬂavour used for evaluation are reported in the baseline column Learn\nScratch. Theoretically, this baseline can be seen as an upper bound on the\nperformance DQN can achieve in that ﬂavour, as it represents the agent’s\nperformance when evaluated in the same ﬂavour it was trained on. Full baseline\nresults with the agent’s performance after diﬀerent number of frames can be\nfound in Tables 5 and 6.\nWe can see in the results that the policies learned by DQN do not generalize\nwell to diﬀerent ﬂavours, even when the ﬂavours are remarkably similar. For\nexample, in Freeway, a high-level policy applicable to all ﬂavours is to go up\nwhile avoiding cars. This does not seem to be what DQN learns. For example,\nthe default ﬂavour m0d0 and m4d0 comprise of exactly the same sprites, the only\n8\n10M\n20M\n30M\n40M\n50M\nFrames before evaluation\n0\n5\n10\n15\n20\nCumulative Reward (log scale)\nFreeway Policy Evaluation\nm1d0\nm1d1\nm4d0\nFigure 3: Performance of a trained agent in the default ﬂavour of Freeway and\nevaluated every 500,000 frames in each target ﬂavour. Error bars were omitted\nfor clarity and the learning curves were smoothed using a moving average over\ntwo data points. Results were averaged over ﬁve seeds.\ndiﬀerence is that in m4d0 some cars accelerate and decelerate over time. The\nclose to optimal policy learned in m0d0 is only able to score 15.8 points when\nevaluated on m4d0, which is approximately half of what the policy learned from\nscratch in that ﬂavour achieves (29.9 points). The learned policy when evaluated\non ﬂavours that diﬀer more from m0d0 perform even worse (for example, when\na new sprite is introduced, or when there are more cars in each lane).\nAs aforementioned, the diﬀerent modes of HERO can be seen as giving the\nagent a curriculum or a natural progression. Interestingly, the agent trained in\nthe default mode for 50M frames can progress to at least level 3 and sometimes\nlevel 4. Mode 1 starts the agent oﬀat level 5 and performance in this mode\nsuﬀers greatly during evaluation. There are very few game mechanics added to\nlevel 5, indicating that perhaps the agent is memorizing trajectories instead of\nlearning a robust policy capable of solving each level.\nResults in some ﬂavours suggest that the agent is overﬁtting to the ﬂavour it is\ntrained on. We tested this hypothesis by periodically evaluating the learned policy\nin each other ﬂavour of that game. This process involved taking checkpoints\nof the network every 500,000 frames and evaluating the ϵ-greedy policy in the\nprescribed ﬂavour for 100 episodes, further averaged over ﬁve runs. The results\nobtained in Freeway, the most pronounced game in which we observe overﬁtting,\nare depicted in Figure 3. Learning curves for all ﬂavours can be found in the\nAppendix.\nIn Freeway, while we see the policy’s performance ﬂattening out in m4d0,\nwe do see the traditional bell-shaped curve associated to overﬁtting in the\nother modes. At ﬁrst, improvements in the original policy do correspond to\nimprovements in the performance of that policy in other ﬂavours. With time,\nit seems that the agent starts to reﬁne its policy for the speciﬁc ﬂavour it is\n9\nbeing trained on, overﬁtting to that ﬂavour. With other game ﬂavours being\nsigniﬁcantly more complex in their dynamics and gameplay, we do not observe\nthis prominent bell-shaped curve.\nIn conclusion, when looking at Table 1, it seems that the policies learned\nby DQN struggle to generalize to even small variations encountered in game\nﬂavours. The results in Freeway even exhibit a troubling notion of overﬁtting.\nNevertheless, being able to generalize across small variations of the task the agent\nwas trained on is a desirable property for truly autonomous agents. Based on\nthese results we evaluate whether deep RL can beneﬁt from established methods\nfrom supervised learning promoting generalization.\n5\nRegularization in DQN\nIn order to evaluate the hypothesis that the observed lack of generalization is\ndue to overﬁtting, we revisit some popular regularization methods from the\nsupervised learning literature. We evaluate two forms of regularization: dropout\nand ℓ2 regularization.\nFirst we want to understand the eﬀect of regularization on deploying the\nlearned policy in a diﬀerent ﬂavour. We do so by applying dropout to the ﬁrst\nfour layers of the network during training, that is, the three convolutional layers\nand the ﬁrst fully connected layer. We also evaluate the use of ℓ2 regularization\non all weights in the network during training. A grid search was performed on\nFreeway to ﬁnd reasonable hyperparameters for the convolutional and fully\nconnected dropout rate pconv, pfc ∈{(0.05, 0.1), (0.1, 0.2), (0.15, 0.3), (0.2,\n0.4), (0.25, 0.5)} , and the ℓ2 regularization parameter λ ∈{10−2, 10−3, 10−4,\n10−5, 10−6}. Each parameter was swept individually as well as exhausting\nthe cartesian product of both sets of parameters for a total of ﬁve runs per\nconﬁguration. The in-depth ablation study, discussing the impact of diﬀerent\nvalues for each parameter, and their interaction, can be found in the Appendix.\nWe ended up combining dropout and ℓ2 regularization as this provided a good\nbalance between training and evaluation performance. This conﬁrms Srivastava\net al.’s (2014) result that these methods provide beneﬁt in tandem. For all future\nexperiments we use λ = 10−4, and pconv, pfc = 0.05, 0.1.\nWe follow the same evaluation scheme described when evaluating the non-\nregularized policy to diﬀerent ﬂavours. We evaluate the policy learned after 50M\nframes of the default mode of each game. We contrast these results with the\nresults presented in the previous section. This evaluation protocol allows us\nto directly evaluate the eﬀect of regularization on the learned policy’s ability\nto generalize. The results are presented in Table 2, on the next page, and the\nevaluation curves are available in the Appendix.\nWhen using regularization during training we sometimes observe a perfor-\nmance hit in the default ﬂavour. Dropout generally requires increased training\niterations to reach the same level of performance one would reach when not using\ndropout. However, maximal performance in one ﬂavour is not our goal. We are\ninterested in the setting where one may be willing to take lower performance on\n10\nTable 2: Policy evaluation using regularization. Each agent was initially trained\nin the default ﬂavour for 50M frames with dropout and ℓ2 regularization then\nevaluated on each listed ﬂavour. Reported numbers are averaged over ﬁve runs.\nStandard deviation is reported between parentheses.\nGame Variant\nEval. with\nRegularization\nEval.\nwithout\nRegularization\nFreeway\nm1d0\n5.8\n(3.5)\n0.2\n(0.2)\nm1d1\n4.4\n(2.3)\n0.1\n(0.1)\nm4d0\n20.6\n(0.7)\n15.8\n(1.0)\nHero\nm1d0\n116.8\n(76.0)\n82.1\n(89.3)\nm2d0\n30.0\n(36.7)\n33.9\n(38.7)\nBreakout\nm12d0\n31.0\n(8.6)\n43.4\n(11.1)\nSpace Invaders\nm1d0\n456.0\n(221.4)\n258.9\n(88.3)\nm1d1\n146.0\n(84.5)\n140.4\n(61.4)\nm9d0\n290.0\n(257.8)\n179.0\n(75.1)\none task in order to obtain higher performance, or adaptability, on future tasks.\nFull baseline results using regularization can also be found in Table 6.\nIn most ﬂavours, when looking at Table 2, we see that evaluating the policy\ntrained with regularization does not negatively impact performance when com-\npared to the performance of the policy trained without regularization. In some\nﬂavours we even see an increase in performance. When using regularization the\nagent’s performance in Freeway improves for all ﬂavours and the agent even\nlearns a policy capable of outperforming the baseline learned from scratch in\ntwo of the three ﬂavours. Moreover, in Freeway we now observe increasing\nperformance during evaluation throughout most of the learning procedure as\ndepicted in Figure 4, on the next page. These results seem to conﬁrm the notion\nof overﬁtting observed in Figure 3.\nDespite slight improvements from these techniques, regularization by itself\ndoes not seem suﬃcient to enable policies to generalize across ﬂavours. Learning\nfrom scratch in these new ﬂavours is still more beneﬁcial than re-using a policy\nlearned with regularization. As shown in the next section, the real beneﬁt of\nregularization in deep RL seems to come from the ability to learn more general\nfeatures. These features lead to a more adaptable representation which can be\nreused and subsequently ﬁne-tuned on other ﬂavours.\n11\n10M\n20M\n30M\n40M\n50M\nFrames before evaluation\n0\n5\n10\n15\n20\n25\nCumulative Reward (log scale)\nFreeway Policy Evaluation w/ Regularization\nm1d0\nm1d1\nm4d0\nm1d0 dropout+ℓ2\nm1d1 dropout+ℓ2\nm4d0 dropout+ℓ2\nFigure 4: Performance of an agent evaluated every 500, 000 frames after it was\ntrained in the default ﬂavour of Freeway with dropout and ℓ2 regularization.\nError bars were omitted for clarity and the learning curves were smoothed using\na moving average (n = 2). Results were averaged over ﬁve seeds. Dotted lines\ndepict the data presented in Figure 3.\n6\nValue function ﬁne-tuning\nWe hypothesize that the beneﬁt of regularizing deep RL algorithms may not come\nfrom improvements during evaluation, but instead in having a good parameter\ninitialization that can be adapted to new tasks that are similar. We evaluate this\nhypothesis using two common practices in machine learning. First, we use the\nweights trained with regularization as the initialization for the entire network.\nWe subsequently ﬁne-tune all weights in the network. This is similar to what\nclassiﬁcation methods do in computer vision problems (e.g., Razavian et al.,\n2014). Secondly, we evaluate reusing and ﬁne-tuning only early layers of the\nnetwork. This has been shown to improve generalization in some settings (e.g.,\nYosinski et al., 2014), and is sometimes used in natural language processing\nproblems (e.g., Mou et al., 2016; Howard and Ruder, 2018).\n6.1\nFine-Tuning the Entire Neural Network\nIn this setting we take the weights of the network trained in the default ﬂavour\nfor 50M frames and use them to initialize the network commencing training in\nthe new ﬂavour for 50M frames. We perform this set of experiments twice (for\nthe weights trained with and without regularization, as described in the previous\nsection). Each run is averaged over ﬁve seeds. For comparison, we provide a\nbaseline trained from scratch for 50M and 100M frames in each ﬂavour. Directly\ncomparing the performance obtained after ﬁne-tuning to the performance after\n50M frames (Scratch) shows the beneﬁt of re-using a representation learned\nin a diﬀerent task instead of randomly initializing the network. Comparing\n12\nthe performance obtained after ﬁne-tuning to the performance of 100M frames\n(Scratch) lets us take into consideration the sample eﬃciency of the whole\nlearning process. The results are presented on the next page, in Table 3.\nFine-tuning from a non-regularized representation yields conﬂicting conclu-\nsions. Although in Freeway we obtained positive ﬁne-tuning results, we note\nthat rewards are so sparse in mode 1 that this initialization is likely to be acting\nas a form of optimistic initialization, biasing the agent to go up. The agent\nobserves rewards more often, therefore, it learns quicker about the new ﬂavour.\nHowever, the agent is still unable to reach the maximum score in these ﬂavours.\nThe results of ﬁne-tuning the regularized representation are more exciting.\nIn Freeway we observe the highest scores on m1d0 and m1d1 throughout the\nwhole paper. In HERO we vastly outperform ﬁne-tuning from a non-regularized\nrepresentation. In Space Invaders we obtain higher scores across the board\nwhen comparing to the same amount of experience. These results suggest that\nreusing a regularized representation in deep RL might allow us to learn more\ngeneral features which can be more successfully ﬁne-tuned.\nInitializing the network with a regularized representation also seems to be\nbetter than initializing the network randomly, that is, when learning from scratch.\nThese results are impressive when we consider the potential regularization has\nin reducing the sample complexity of deep RL algorithms.\nInitializing the\nnetwork with a regularized representation seems even better than learning from\nscratch when we take the total number of frames seen between two ﬂavours\ninto consideration. When we look at the rows Regularized Fine-tuning\nand Scratch in Table 3 we are comparing two algorithms that observed 100M\nframes. However, to generate the results in the column Scratch for two ﬂavours\nwe used 200M frames while we only used used 150M frames to generate the\nresults in the column Regularized Fine-tuning (50M frames are used to\nlearn in the default ﬂavour and then 50M frames are used in each ﬂavour you\nactually care about). Obviously, this distinction becomes larger as more tasks\nare taken into consideration.\n6.2\nFine-Tuning Early Layers to Learn Co-Adaptations\nWe also investigated which layers may encode general features able to be ﬁne-\ntuned. We were inspired by other studies showing that neural networks can re-\nlearn co-adaptations when their ﬁnal layers are randomly initialized, sometimes\nimproving generalization (Yosinski et al., 2014). We conjectured DQN may\nbeneﬁt from re-learning the co-adaptations between early layers comprising\ngeneral features and the randomly initialized layers which ultimately assign\nstate-action values. We hypothesized that it might be beneﬁcial to re-learn the\nﬁnal layers from scratch since state-action values are ultimately conditioned on\nthe ﬂavour at hand. Therefore, we also evaluated whether ﬁne-tuning only the\nconvolutional layers, or the convolutional layers and the ﬁrst fully connected\nlayer, was more eﬀective than ﬁne-tuning the whole network. This does not\nseem to be the case. The performance when we ﬁne-tune the whole network is\nconsistently better than when we re-learn co-adaptations, as shown in Table 4.\n13\nTable 3: Experiments ﬁne-tuning the entire network with and without regularization (dropout + ℓ2). An agent is trained with\ndropout + ℓ2 regularization in the default ﬂavour of each game for 50M frames, then DQN’s parameters were used to initialize\nthe ﬁne-tuning procedure on each new ﬂavour for 50M frames. The baseline agent is trained from scratch up to 100M frames.\nStandard deviation is reported between parentheses.\nFine-tuning\nRegularized Fine-tuning\nScratch\nGame Variant\n10M\n50M\n10M\n50M\n50M\n100M\nFreeway\nm1d0\n2.9\n(3.7)\n22.5\n(7.5)\n20.2\n(1.9)\n25.4\n(0.2)\n4.8\n(9.3)\n7.5\n(11.5)\nm1d1\n0.1\n(0.2)\n17.4\n(11.4)\n18.5\n(2.8)\n25.4\n(0.4)\n0.0\n(0.0)\n2.5\n(7.3)\nm4d0\n20.8\n(1.1)\n31.4\n(0.5)\n22.6\n(0.7)\n32.2\n(0.5)\n29.9\n(0.7)\n32.8\n(0.2)\nHero\nm1d0\n220.7\n(98.2)\n496.7\n(362.8)\n322.5\n(39.3)\n4104.6\n(2192.8)\n1425.2\n(1755.1)\n5026.8\n(2174.6)\nm2d0\n74.4\n(31.7)\n92.5\n(26.2)\n84.8\n(56.1)\n211.0\n(100.6)\n326.1\n(130.4)\n323.5\n(76.4)\nBreakout\nm12d0\n11.5\n(10.7)\n69.1\n(14.9)\n48.2\n(4.1)\n96.1\n(11.2)\n67.6\n(32.4)\n55.2\n(37.2)\nSpace Invaders\nm1d0\n617.8\n(55.9)\n926.1\n(56.6)\n701.8\n(28.5)\n1033.5\n(89.7)\n753.6\n(31.6)\n979.7\n(39.8)\nm1d1\n482.6\n(63.4)\n799.4\n(52.5)\n656.7\n(25.5)\n920.0\n(83.5)\n698.5\n(31.3)\n906.9\n(56.5)\nm9d0\n354.8\n(59.4)\n574.1\n(37.0)\n519.0\n(31.1)\n583.0\n(17.5)\n518.0\n(16.7)\n567.7\n(40.1)\n14\nTable 4: Experiments ﬁne-tuning early layers of the network trained with regularization. An agent is trained with dropout\n+ ℓ2 regularization in the default ﬂavour of each game for 50M frames, then DQN’s parameters were used to initialize the\ncorresponding layers to be further ﬁne-tuned on each new ﬂavour. Remaining layers were randomly initialized. We also compare\nagainst ﬁne-tuning the entire network from Table 3. Standard deviation is reported between parentheses.\nRegularized Fine-Tuning\n3Conv\nRegularized Fine-Tuning\n3Conv + 1FC\nRegularized Fine-Tuning\nEntire Network\nGame Variant\n10M\n50M\n10M\n50M\n10M\n50M\nFreeway\nm1d0\n0.0\n(0.0)\n0.7\n(1.4)\n0.1\n(0.1)\n4.9\n(9.9)\n20.2\n(1.9)\n25.4\n(0.2)\nm1d1\n0.0\n(0.0)\n0.0\n(0.0)\n0.1\n(0.1)\n10.0\n(12.3)\n18.5\n(2.8)\n25.4\n(0.4)\nm4d0\n7.3\n(3.5)\n30.4\n(0.6)\n4.9\n(4.8)\n30.7\n(1.7)\n22.6\n(0.7)\n32.2\n(0.5)\nHero\nm1d0\n405.1\n(82.0)\n1949.1\n(2076.4)\n350.3\n(52.1)\n3085.3\n(2055.6)\n322.5\n(39.3)\n4104.6\n(2192.8)\nm2d0\n232.1\n(30.1)\n455.2\n(170.4)\n150.4\n(38.5)\n307.6\n(64.8)\n84.8\n(56.1)\n211.0\n(100.6)\nBreakout\nm12d0\n4.3\n(1.7)\n63.7\n(26.6)\n5.4\n(0.8)\n89.1\n(16.7)\n48.2\n(4.1)\n96.1\n(11.2)\nSpace Invaders\nm1d0\n669.3\n(29.1)\n998.1\n(78.8)\n681.3\n(17.2)\n989.6\n(39.4)\n701.8\n(28.5)\n1033.5\n(89.7)\nm1d1\n609.8\n(16.6)\n836.3\n(55.9)\n638.7\n(19.1)\n883.4\n(38.1)\n656.7\n(25.5)\n920.0\n(83.5)\nm9d0\n436.1\n(18.9)\n581.0\n(12.2)\n439.9\n(40.3)\n586.7\n(39.7)\n519.0\n(31.1)\n583.0\n(17.5)\n15\n7\nDiscussion and conclusion\nMany studies have tried to explain generalization of deep neural networks\nin supervised learning settings (e.g., Zhang et al., 2018b; Dinh et al., 2017).\nAnalyzing generalization and overﬁtting in deep RL has its own issues on top of\nthe challenges posed in the supervised learning case. Actually, generalization\nin RL can be seen in diﬀerent ways. We can talk about generalization in RL\nin terms of conditioned sub-goals within an environment (e.g., Andrychowicz\net al., 2017; Sutton, 1995), learning multiple tasks at once (e.g., Teh et al., 2017;\nParisotto et al., 2016), or sequential task learning as in a continual learning\nsetting (e.g., Schwarz et al., 2018; Kirkpatrick et al., 2016). In this paper we\nevaluated generalization in terms of small variations of high-dimensional control\ntasks. This provides a candid evaluation method to study how well features\nand policies learned by deep neural networks in RL problems can generalize.\nThe approach of studying generalization with respect to the representation\nlearning problem intersects nicely with the aforementioned problems in RL\nwhere generalization is key.\nThe results presented in this paper suggest that DQN generalizes poorly,\neven when tasks have very similar underlying dynamics. Given this lack of\ngenerality, we investigated whether dropout and ℓ2 regularization can improve\ngeneralization in deep reinforcement learning. Other forms of regularization that\nhave been explored in the past are sticky-actions, random initial states, entropy\nregularization (e.g., Zhang et al., 2018b), and procedural generation of environ-\nments (e.g., Justesen et al., 2018). More related to our work, regularization in\nthe form of weight constraints has been applied in the continual learning setting\nin order to reduce the catastrophic forgetting exhibited by ﬁne-tuning on many\nsequential tasks (Kirkpatrick et al., 2016; Schwarz et al., 2018). Similar weight\nconstraint methods were explored in multitask learning (Teh et al., 2017).\nEvaluation practices in RL often focus on training and evaluating agents\non exactly the same task. Consequently, regularization has traditionally been\nunderutilized in deep RL. With a renewed emphasis on generalization in RL,\nregularization applied to the representation learning problem can be a feasible\nmethod for improving generalization on closely related tasks. Our results suggest\nthat dropout and ℓ2 regularization seem to be able to learn more general purpose\nfeatures which can be adapted to similar problems. Although other communities\nrelying on deep neural networks have shown similar successes, this is of particular\nimportance for the deep RL community which struggles with sample eﬃciency\n(Henderson et al., 2018). This work is also related to recent meta-learning\nprocedures like MAML (Finn et al., 2017) which aim to ﬁnd a parameter\ninitialization that can be quickly adapted to new tasks. As previously mentioned,\ntechniques such as MAML (Finn et al., 2017) and REPTILE (Nichol et al.,\n2018b) did not succeed in the setting we used.\nSome of the results here can also be seen under the light of curriculum learning.\nThe regularization techniques we have evaluated here seem to be eﬀective in\nleveraging situations where an easier task is presented ﬁrst, sometimes leading\nto unseen performance levels (e.g., Freeway).\n16\nTable 5: DQN baseline results for each tested game ﬂavour. We report the\naverage over ﬁve runs (std. deviations are reported between parentheses). Results\nwere obtained with the default value of sticky actions (Machado et al., 2018).\nGame Variant\n10M\n50M\n100M\nBest Action\nFreeway\nm0d0\n3.0\n(1.0)\n31.4\n(0.2)\n32.1\n(0.1)\n23.0\n(1.4)\nm1d0\n0.0\n(0.1)\n4.8\n(9.3)\n7.5\n(11.5)\n5.0\n(1.5)\nm1d1\n0.0\n(0.0)\n0.0\n(0.0)\n2.5\n(7.3)\n4.2\n(1.3)\nm4d0\n4.4\n(1.4)\n29.9\n(0.7)\n32.8\n(0.2)\n7.5\n(2.8)\nHero\nm0d0\n3187.8\n(78.3)\n9034.4\n(1610.9)\n13961.0\n(181.9)\n150.0\n(0.0)\nm1d0\n326.9\n(40.3)\n1425.2\n(1755.1)\n5026.8\n(2174.6)\n75.8\n(7.5)\nm2d0\n116.3\n(11.0)\n326.1\n(130.4)\n323.5\n(76.4)\n12.0\n(27.5)\nBreakout\nm0d0\n17.5\n(2.0)\n72.5\n(7.7)\n73.4\n(13.5)\n2.3\n(1.3)\nm12d0\n17.7\n(1.3)\n67.6\n(32.4)\n55.2\n(37.2)\n1.8\n(1.1)\nSpace Invaders\nm0d0\n250.3\n(16.2)\n698.8\n(32.2)\n927.1\n(85.3)\n243.6\n(95.9)\nm1d0\n203.6\n(24.3)\n753.6\n(31.6)\n979.7\n(39.8)\n192.6\n(65.7)\nm1d1\n193.6\n(11.0)\n698.5\n(31.3)\n906.9\n(56.5)\n180.9\n(101.9)\nm9d0\n173.0\n(17.8)\n518.0\n(16.7)\n567.7\n(40.1)\n174.6\n(65.9)\nTable 6: Baseline results in the default ﬂavour with dropout and ℓ2 regularization.\nWe report the average over ﬁve runs (std. deviations are reported between\nparentheses). We used the default value of sticky actions (Machado et al., 2018).\nGame Variant\n10M\n50M\n100M\nBest Action\nFreeway\nm0d0\n4.6\n(5.0)\n25.9\n(0.6)\n29.0\n(0.8)\n23.0\n(1.4)\nHero\nm0d0\n2466.5\n(630.8)\n6505.9\n(1843.0)\n12446.9\n(397.4)\n150.0\n(0.0)\nBreakout\nm0d0\n6.1\n(2.7)\n34.1\n(1.8)\n66.4\n(3.6)\n2.3\n(1.3)\nSpace Invaders\nm0d0\n214.6\n(13.8)\n623.1\n(16.3)\n617.4\n(29.6)\n243.6\n(95.9)\nFinally, it is obvious that we want algorithms that can generalize across\ntasks. Ultimately we want agents that can keep learning as they interact with\nthe world in a continual learning fashion. We believe the ﬂavours of Atari\n2600 games can be a stepping stone towards this goal. Our results suggested\nthat regularizing and ﬁne-tuning representations in deep RL might be a viable\napproach towards improving sample eﬃciency and generalization on multiple\ntasks. It is particularly interesting that ﬁne-tuning a regularized network was the\nmost successful approach because this might also be applicable in the continual\nlearning settings where the environment changes without the agent being told\nso, and re-initializing layers of a network is obviously not an option.\n17\nAcknowledgments\nThe authors would like to thank Matthew E. Taylor, Tom van de Wiele, and\nMarc G. Bellemare for useful discussions, as well as Vlad Mnih for feedback on\na preliminary draft of the manuscript. This work was supported by funding\nfrom NSERC and Alberta Innovates Technology Futures through the Alberta\nMachine Intelligence Institute (Amii). Computing resources were provided by\nCompute Canada through CalculQu´ebec. Marlos C. Machado performed part of\nthis work while at the University of Alberta.\nReferences\nMarcin Andrychowicz, Dwight Crow, Alex Ray, Jonas Schneider, Rachel Fong,\nPeter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech\nZaremba. 2017. Hindsight Experience Replay. In Advances in Neural Informa-\ntion Processing Systems (NeurIPS). 5048–5058.\nMarc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. 2013. The\nArcade Learning Environment: An Evaluation Platform for General Agents.\nJournal of Artiﬁcial Intelligence Research 47 (2013), 253–279.\nKarl Cobbe, Oleg Klimov, Christopher Hesse, Taehoon Kim, and John Schulman.\n2019. Quantifying Generalization in Reinforcement Learning. In Proceedings\nof the International Conference on Machine Learning (ICML). 1282–1289.\nLaurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. 2017. Sharp\nMinima Can Generalize For Deep Nets. In Proceedings of the International\nConference on Machine Learning (ICML). 1019–1028.\nLasse Espeholt, Hubert Soyer, R´emi Munos, Karen Simonyan, Volodymyr Mnih,\nTom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane\nLegg, and Koray Kavukcuoglu. 2018. IMPALA: Scalable Distributed Deep-RL\nwith Importance Weighted Actor-Learner Architectures. In Proceedings of the\nInternational Conference on Machine Learning (ICML). 1406–1415.\nAmir Massoud Farahmand, Mohammad Ghavamzadeh, Csaba Szepesv´ari, and\nShie Mannor. 2008.\nRegularized Policy Iteration. In Advances in Neural\nInformation Processing Systems (NeurIPS). 441–448.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-Agnostic Meta-\nLearning for Fast Adaptation of Deep Networks. In Proceedings of the Inter-\nnational Conference on Machine Learning (ICML). 1126–1135.\nXavier Glorot and Yoshua Bengio. 2010. Understanding the Diﬃculty of Train-\ning Deep Feedforward Neural Networks. In Proceedings of the International\nConference on Artiﬁcial Intelligence and Statistics (AISTATS). 249–256.\n18\nPeter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup,\nand David Meger. 2018. Deep Reinforcement Learning That Matters. In\nProceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI). 3207–\n3214.\nJeremy Howard and Sebastian Ruder. 2018. Fine-tuned Language Models for\nText Classiﬁcation. CoRR abs/1801.06146 (2018).\nArthur Juliani, Ahmed Khalifa, Vincent-Pierre Berges, Jonathan Harper, Ervin\nTeng, Hunter Henry, Adam Crespi, Julian Togelius, and Danny Lange. 2019.\nObstacle Tower: A Generalization Challenge in Vision, Control, and Planning.\nIn Proceedings of the International Joint Conference on Artiﬁcial Intelligence\n(IJCAI). 2684–2691.\nNiels Justesen, Ruben Rodriguez Torrado, Philip Bontrager, Ahmed Khalifa,\nJulian Togelius, and Sebastian Risi. 2018.\nProcedural Level Generation\nImproves Generality of Deep Reinforcement Learning. CoRR abs/1806.10729\n(2018).\nJames Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume\nDesjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho,\nAgnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan\nKumaran, and Raia Hadsell. 2016. Overcoming Catastrophic Forgetting in\nNeural Networks. CoRR abs/1612.00796 (2016).\nJ. Zico Kolter and Andrew Y. Ng. 2009. Regularization and Feature Selec-\ntion in Least-Squares Temporal Diﬀerence Learning. In Proceedings of the\nInternational Conference on Machine Learning (ICML). 521–528.\nYann Lecun, L´eon Bottou, Yoshua Bengio, and Patrick Haﬀner. 1998. Gradient-\nbased Learning Applied to Document Recognition.\nIEEE 86, 11 (1998),\n2278–2324.\nJonathan Long, Evan Shelhamer, and Trevor Darrell. 2015. Fully Convolutional\nNetworks for Semantic Segmentation. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR). 3431–3440.\nMarlos C. Machado, Marc G. Bellemare, Erik Talvitie, Joel Veness, Matthew J.\nHausknecht, and Michael Bowling. 2018. Revisiting the Arcade Learning\nEnvironment: Evaluation Protocols and Open Problems for General Agents.\nJournal of Artiﬁcial Intelligence Research 61 (2018), 523–562.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel\nVeness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas\nFidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis\nAntonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg,\nand Demis Hassabis. 2015. Human-Level Control through Deep Reinforcement\nLearning. Nature 518, 7540 (2015), 529–533.\n19\nLili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu, Lu Zhang, and Zhi Jin. 2016. How\nTransferable are Neural Networks in NLP Applications?. In Proceedings of the\nConference on Empirical Methods in Natural Language Processing (EMNLP).\n479–489.\nAlex Nichol, Joshua Achiam, and John Schulman. 2018a. On First-Order Meta-\nLearning Algorithms. CoRR abs/1803.02999 (2018).\nAlex Nichol, Vicki Pfau, Christopher Hesse, Oleg Klimov, and John Schulman.\n2018b. Gotta Learn Fast: A New Benchmark for Generalization in RL. CoRR\nabs/1804.03720 (2018).\nEmilio Parisotto, Lei Jimmy Ba, and Ruslan Salakhutdinov. 2016. Actor-Mimic:\nDeep Multitask and Transfer Reinforcement Learning. In Proceedings of the\nInternational Conference on Learning Representations (ICLR).\nAravind Rajeswaran, Kendall Lowrey, Emanuel Todorov, and Sham M. Kakade.\n2017.\nTowards Generalization and Simplicity in Continuous Control. In\nAdvances in Neural Information Processing Systems (NeurIPS). 6550–6561.\nAli Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson.\n2014. CNN Features Oﬀ-the-Shelf: An Astounding Baseline for Recognition.\nIn Workshops of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR). 512–519.\nAndrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James\nKirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. 2016.\nProgressive Neural Networks. CoRR abs/1606.04671 (2016).\nJonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-\nBarwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. 2018. Progress\n& Compress: A Scalable Framework for Continual Learning. In Proceedings\nof the International Conference on Machine Learning (ICML). 4535–4544.\nDavid Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George\nvan den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Pan-\nneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham,\nNal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach,\nKoray Kavukcuoglu, Thore Graepel, and Demis Hassabis. 2016. Mastering\nthe Game of Go with Deep Neural Networks and Tree Search. Nature 529,\n7587 (2016), 484–489.\nNitish Srivastava, Geoﬀrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and\nRuslan Salakhutdinov. 2014. Dropout: a Simple Way to Prevent Neural\nNetworks from Overﬁtting.\nJournal of Machine Learning Research 15, 1\n(2014), 1929–1958.\nRichard S. Sutton. 1995. Generalization in Reinforcement Learning: Successful\nExamples Using Sparse Coarse Coding. In Advances in Neural Information\nProcessing Systems (NeurIPS). 1038–1044.\n20\nYee Whye Teh, Victor Bapst, Wojciech M. Czarnecki, John Quan, James Kirk-\npatrick, Raia Hadsell, Nicolas Heess, and Razvan Pascanu. 2017. Distral:\nRobust Multitask Reinforcement Learning. In Advances in Neural Information\nProcessing Systems (NeurIPS). 4496–4506.\nChristopher Watkins and Peter Dayan. 1992.\nTechnical Note: Q-Learning.\nMachine Learning 8, 3-4 (1992).\nShimon Whiteson, Brian Tanner, Matthew E. Taylor, and Peter Stone. 2011. Pro-\ntecting Against Evaluation Overﬁtting in Empirical Reinforcement Learning.\nIn IEEE Symposium on Adaptive Dynamic Programming And Reinforcement\nLearning (ADPRL). 120–127.\nSam Witty, Jun Ki Lee, Emma Tosch, Akanksha Atrey, Michael L. Littman, and\nDavid D. Jensen. 2018. Measuring and Characterizing Generalization in Deep\nReinforcement Learning. CoRR abs/1812.02868 (2018).\nJason Yosinski, JeﬀClune, Yoshua Bengio, and Hod Lipson. 2014. How Transfer-\nable are Features in Deep Neural Networks?. In Advances in Neural Information\nProcessing Systems (NeurIPS). 3320–3328.\nAmy Zhang, Nicolas Ballas, and Joelle Pineau. 2018a. A Dissection of Over-\nﬁtting and Generalization in Continuous Reinforcement Learning. CoRR\nabs/1806.07937 (2018).\nChiyuan Zhang, Oriol Vinyals, R´emi Munos, and Samy Bengio. 2018b. A Study\non Overﬁtting in Deep Reinforcement Learning. CoRR abs/1804.06893 (2018).\n21\nAppendix\nGame Modes\nWe provide a brief description of each game ﬂavour used in the paper.\nFreeway\nFreeway m0d0\nFreeway m1d0\nFreeway m4d0\nIn Freeway a chicken must cross a road containing multiple lanes of moving\ntraﬃc within a prespeciﬁed time limit. In all modes of Freeway the agent is\nrewarded for reaching the top of the screen and is subsequently teleported to the\nbottom of the screen. If the chicken collides with a vehicle in diﬃculty 0 it gets\nbumped down one lane of traﬃc, alternatively, in diﬃculty 1 the chicken gets\nteleported to its starting position at the bottom of the screen. Mode 1 changes\nsome vehicle sprites to include buses, adds more vehicles to some lanes, and\nincreases the velocity of all vehicles. Mode 4 is almost identical to Mode 1; the\nonly diﬀerence being vehicles can oscillate between two speeds.\nHero\nHero m0d0\nHero m1d0\nHero m2d0\nIn Hero you control a character who must navigate a maze in order to save\na trapped miner within a cave system. The agent scores points for any forward\nprogression such as clearing an obstacle or killing an enemy. Once the miner\nis rescued, the level is terminated and you continue to the next level with a\ndiﬀerent maze. Some levels have partially observable rooms, more enemies, and\nVideos of the diﬀerent modes are available in the following link: https://goo.gl/pCvPiD.\n22\nmore diﬃcult obstacles to traverse. Past the default mode, each subsequent\nmode starts oﬀat increasingly harder levels denoted by a level number increasing\nby multiples of 5. The default mode starts you oﬀat level 1, mode 1 starts at\nlevel 5, and so on.\nBreakout\nBreakout m0d0\nBreakout m12d0\nIn Breakout you control a paddle which can move horizontally along the\nbottom of the screen. At the beginning of the game, or on a loss of life the ball\nis set into motion and can bounce oﬀthe paddle and collide with bricks at the\ntop of the screen. The objective of the game is to break all the bricks without\nhaving the ball fall below your paddles horizontal plane. Subsequently, mode\n12 of Breakout hides the bricks from the player until the ball collides with\nthe bricks in which case the bricks ﬂash for a brief moment before disappearing\nagain.\nSpace Invaders\nSpace Invaders m0d0 Space Invaders m1d1 Space Invaders m9d0\nWhen playing Space Invaders you control a spaceship which can move\nhorizontally along the bottom of the screen. There is a grid of aliens above you\nand the objective of the game is to eliminate all the aliens. You are aﬀorded some\nprotection from the alien bullets with three barriers just above your spaceship.\nDiﬃculty 1 of Space Invaders widens your spaceships sprite making it harder\nto dodge enemy bullets. Mode 1 of Space Invaders causes the shields above\nyou to oscillate horizontally. Mode 9 of Space Invaders is similar to Mode 12\nof Breakout where the aliens are partially observable until struck with the\nplayer’s bullet.\n23\nExperimental Details\nArchitecture and hyperparameters\nAll experiments performed in this paper utilized the neural network architecture\nproposed by Mnih et al. (2015). That is, a convolutional neural network with\nthree convolutional layers and two fully connected layers. A visualization of this\nnetwork can be found in Figure 5. Unless otherwise speciﬁed, hyperparametes\nare kept consistent with the ALE baselines discussed by Machado et al. (2018).\nA summary of the parameters, which were consistent across all experiments, can\nbe found in in Table 7.\n1024\n18\nReLU\nReLU\nReLU\nReLU\nfc\nfc\nConv\n32,8x8\nstride 4  \nConv\n64,4x4\nstride 2  \nConv\n64,3x3\nstride 1  \nQ(St, ·; ✓)\nFigure 5: Network architecture used by DQN to predict state-action values.\nTable 7: Hyperparameters for baseline results.\nLearning rate α\n0.00025\nMinibatch size\n32\nLearning frequency\n4\nFrame skip\n5\nSticky action prob.\n0.25\nReplay buﬀer size\n1, 000, 000\nϵ decay period\n1M frames\nϵ initial\n1.0\nϵ ﬁnal\n0.01\nDiscount factor γ\n0.99\nEvaluation\nWe adhere to the evaluation methodologies set out by Machado et al. (2018).\nThis includes the use of all 18 primitive actions in the ALE, not utilizing loss of\nlife as episode termination, and the use of sticky actions to inject stochasticity.\nEach result outlined in this paper averages the agents performance over 100\nepisodes further averaged over ﬁve runs. We do not take the maximum over runs\nnor the maximum over the learning curve.\nWhen comparing results in this paper and with other evaluation methodolo-\ngies it is worth noting the following terminology and time scales. We use a frame\nskip of 5 frames, i.e., following every action executed by the agent the simulator\nadvances 5 frames into the future. The agent will take # frames/5 actions within\nthe environment over the duration of each experiment. One step of stochastic\ngradient descent to update the network parameters is performed every 4 actions.\nThe training routine will perform # frames/5·4 gradient updates over the duration\nof each experiment. Therefore, when we discuss experiments with a duration\nof 50M frames this is in actuality 50M simulator frames, 10M agent steps, and\n2.5M gradient updates.\nCode available at https://github.com/jessefarebro/dqn-ale.\n24\n10M\n20M\n30M\n40M\n50M\nNumber of Frames\n5\n10\n15\n20\n25\n30\nCumulative Reward\nFreeway m0d0 train ℓ2\nλ = 10−6\nλ = 10−5\nλ = 10−4\nλ = 10−3\nλ = 10−2\n(a) Performance dur-\ning training in the de-\nfault mode of Free-\nway with various val-\nues for λ.\n0M\n10M\n20M\n30M\n40M\n50M\nFrames before evaluation\n1\n2\n3\n4\n5\n6\n7\nCumulative Reward\nFreeway m1d0 eval. ℓ2\nλ = 10−6\nλ = 10−5\nλ = 10−4\nλ = 10−3\nλ = 10−2\n(b)\nPerformance\nin\nFreeway m1d0 from\nan agent trained with\nvarious values of λ in\nFreeway m0d0.\n0M\n10M\n20M\n30M\n40M\n50M\nFrames before evaluation\n5\n10\n15\n20\n25\nCumulative Reward\nFreeway m4d0 eval. ℓ2\nλ = 10−6\nλ = 10−5\nλ = 10−4\nλ = 10−3\nλ = 10−2\n(c)\nPerformance\nin\nFreeway m4d0 from\nan agent trained with\nvarious values of λ in\nFreeway m0d0.\nFigure 6: Training and evaluation performance for DQN in Freeway using\ndiﬀerent values of λ.\nRegularization Ablation Study\nTo gain better insight into the overﬁtting results presented in the paper, we\nperformed an ablation study on the two main hyperparameters used to study\ngeneralization, ℓ2 regularization and dropout (Srivastava et al., 2014). To perform\nthis ablation study we trained an agent in the default ﬂavour of Freeway (i.e.,\nm0d0) for 50M frames and evaluated it in two diﬀerent ﬂavours, Freeway m1d0,\nand Freeway m4d0. In the evaluation phase we took checkpoints every 500, 000\nframes during training and subsequently recorded the mean performance over\n100 episodes. All results presented in this section are averaged over 5 seeds.\nWe tested the eﬀects of ℓ2 regularization, dropout, and the combination of\nthese two methods. We varied the weighted importance λ of our ℓ2 term in the\nDQN loss function as well as studied the dropout rate for the three convolutional\nlayers pconv, and the ﬁrst fully connected layer pfc. We used the loss function\nL\nDQN =\nE\nτ ∼U(·)\n\u0002\u0000Rt+1 + γ max\na′∈A Q(St+1, a′; θ−) −Q(St, At; θ)\n\u00012\u0003\n+ λ ∥θ∥2\n2 ,\nwhere τ = (St, At, Rt+1, St+1) are uniformly sampled from U(·), the experience\nreplay buﬀer ﬁlled with experience collected by the agent. We considered the\nvalues λ ∈{10−2, 10−3, 10−4, 10−5, 10−6} for ℓ2 regularization, as well as the\nvalues pconv, pfc ∈{(0.05, 0.1), (0.1, 0.2), (0.15, 0.3), (0.2, 0.4), (0.25, 0.5)} for\ndropout. We conclude by analyzing the cartesian product of these two sets to\nstudy the eﬀects of combining the two methods.\nℓ2 regularization\nWe begin by analyzing the training performance for DQN in Freeway m0d0\nfor diﬀerent values of λ. We also provide evaluation curves for m1d0, and m4d0\nof Freeway. Both sets of experiments are presented in Figure 6.\n25\n10M\n20M\n30M\n40M\n50M\nNumber of Frames\n5\n10\n15\n20\n25\n30\nCumulative Reward\nFreeway m0d0 train dropout\npconv, pfc = 0.05 0.1\npconv, pfc = 0.15 0.3\npconv, pfc = 0.1 0.2\npconv, pfc = 0.25 0.5\npconv, pfc = 0.2 0.4\n(a) Performance dur-\ning training in the de-\nfault mode of Free-\nway with various val-\nues for pconv, pfc.\n0M\n10M\n20M\n30M\n40M\n50M\nFrames before evaluation\n1\n2\n3\n4\n5\n6\n7\nCumulative Reward\nFreeway m1d0 eval. dropout\npconv, pfc = 0.05 0.1\npconv, pfc = 0.15 0.3\npconv, pfc = 0.1 0.2\npconv, pfc = 0.25 0.5\npconv, pfc = 0.2 0.4\n(b)\nPerformance\nin\nFreeway m1d0 from\nan agent trained with\nvarious\nvalues\nfor\npconv, pfc in m0d0.\n0M\n10M\n20M\n30M\n40M\n50M\nFrames before evaluation\n5\n10\n15\n20\n25\nCumulative Reward\nFreeway m4d0 eval. dropout\npconv, pfc = 0.05 0.1\npconv, pfc = 0.15 0.3\npconv, pfc = 0.1 0.2\npconv, pfc = 0.25 0.5\npconv, pfc = 0.2 0.4\n(c)\nPerformance\nin\nFreeway m4d0 from\nan agent trained with\nvarious\nvalues\nfor\npconv, pfc in m0d0.\nFigure 7: Training and evaluation performance for DQN in Freeway using\ndiﬀerent values pconv, pfc, the dropout rate for the convolutional layers and the\nﬁrst fully connected layer respectively.\nLarge values of λ seem to hurt training performance and smaller values are\nweak enough that the agent begins to overﬁt to m0d0. It is worth noting the\nperformance during evaluation in m4d0 is similar to an agent trained without\nℓ2 regularization. The beneﬁts of ℓ2 do not seem to be apparent in m4d0 but\nprovide improvement in m1d0.\nDropout\nWe provide results in Figure 7 depicting the training performance of the Free-\nway m0d0 agent with varying values of pconv, pfc. As with ℓ2 regularization, we\nfurther evaluate each agent checkpoint for 100 episodes in the target ﬂavour dur-\ning training.\nDropout seems to have a much bigger impact on the training performance\nwhen contrasting the results presented for ℓ2 regularization in Figure 6. Curiously,\nlarger values for the dropout rate can cause the agents’ performance to ﬂatline\nin both training and evaluation. The network may learn to bias a speciﬁc action,\nor sequence of actions independent of the state. However, reasonable dropout\nrates seem to improve the agents ability to generalize in both m1d0 and m4d0.\nCombining ℓ2 regularization and dropout\nCommonly, we see dropout and ℓ2 regularization combined in many supervised\nlearning applications. We want to further explore the possibility that these\ntwo methods can provide beneﬁts in tandem. We exhaust the cross product\nof the two sets of values examined above. We ﬁrst analyze the impact these\nmethods have on the training procedure in Freeway m0d0. Learning curves\nare presented in Figure 8.\nInterestingly, the combination of these methods can provide increased stability\nto the training procedure compared to the results in Figure 7. For example,\n26\n10M\n20M\n30M\n40M\n50M\nNumber of Frames\n5\n10\n15\n20\n25\n30\nCumulative Reward\nFreeway m0d0 train ℓ2 + dropout (pconv, pfc = 0.05, 0.1)\npconv, pfc = 0.05, 0.1; λ = 10−6\npconv, pfc = 0.05, 0.1; λ = 10−5\npconv, pfc = 0.05, 0.1; λ = 10−4\npconv, pfc = 0.05, 0.1; λ = 10−3\npconv, pfc = 0.05, 0.1; λ = 10−2\n10M\n20M\n30M\n40M\n50M\nNumber of Frames\n5\n10\n15\n20\n25\n30\nCumulative Reward\nFreeway m0d0 train ℓ2 + dropout (pconv, pfc = 0.1, 0.2)\npconv, pfc = 0.1, 0.2; λ = 10−6\npconv, pfc = 0.1, 0.2; λ = 10−5\npconv, pfc = 0.1, 0.2; λ = 10−4\npconv, pfc = 0.1, 0.2; λ = 10−3\npconv, pfc = 0.1, 0.2; λ = 10−2\n10M\n20M\n30M\n40M\n50M\nNumber of Frames\n5\n10\n15\n20\n25\n30\nCumulative Reward\nFreeway m0d0 train ℓ2 + dropout (pconv, pfc = 0.15, 0.3)\npconv, pfc = 0.15, 0.3; λ = 10−6\npconv, pfc = 0.15, 0.3; λ = 10−5\npconv, pfc = 0.15, 0.3; λ = 10−4\npconv, pfc = 0.15, 0.3; λ = 10−3\npconv, pfc = 0.15, 0.3; λ = 10−2\n10M\n20M\n30M\n40M\n50M\nNumber of Frames\n5\n10\n15\n20\n25\n30\nCumulative Reward\nFreeway m0d0 train ℓ2 + dropout (pconv, pfc = 0.2, 0.4)\npconv, pfc = 0.2, 0.4; λ = 10−6\npconv, pfc = 0.2, 0.4; λ = 10−5\npconv, pfc = 0.2, 0.4; λ = 10−4\npconv, pfc = 0.2, 0.4; λ = 10−3\npconv, pfc = 0.2, 0.4; λ = 10−2\n10M\n20M\n30M\n40M\n50M\nNumber of Frames\n5\n10\n15\n20\n25\n30\nCumulative Reward\nFreeway m0d0 train ℓ2 + dropout (pconv, pfc = 0.25, 0.5)\npconv, pfc = 0.25, 0.5; λ = 10−6\npconv, pfc = 0.25, 0.5; λ = 10−5\npconv, pfc = 0.25, 0.5; λ = 10−4\npconv, pfc = 0.25, 0.5; λ = 10−3\npconv, pfc = 0.25, 0.5; λ = 10−2\nFigure 8: Performance during training on the default ﬂavour of Freeway. For\neach plot pconv, pfc is held constant while varying the ℓ2 regularization term λ.\nEach parameter conﬁguration is averaged over ﬁve seeds.\nthe conﬁguration pconv, pfc = 0.1, 0.2 scores less than 15 when solely utilizing\ndropout. When applying ℓ2 regularization in tandem we can see the performance\nhover around 20 for moderate values of λ. We continue observe the ﬂatline\nbehaviour for large values of pconv, pfc, regardless of ℓ2 regularization.\nWe now examine the evaluation performance for each parameter conﬁguration\nin both Freeway m1d0, and Freeway m4d0. These results are presented in\nFigure 9 for m1d0, and Figure 10 for m4d0.\nWe observe that ℓ2 regularization struggled to provide much beneﬁt in\nFreeway m4d0.\nReasonable values of dropout seem to aid generalization\nperformance in both modes tested. It does seem that balancing the two methods\nof regularization can provide some beneﬁts, such as an increased training stability\nand more consistent zero-shot generalization performance.\nFrom the beginning we maintained a heuristic prescribing a balance between\ntraining performance and zero-shot generalization performance. In order to\nstrike this balance we chose the parameters pconv, pfc = 0.05, 0.1 for the dropout\nrate, and λ = 10−4 for the ℓ2 regularization parameter. These seemed to strike\nthe best balance in early testing and the results in the ablation study seem to\nconﬁrm our intuitions.\n27\n0M\n10M\n20M\n30M\n40M\n50M\nFrames before evaluation\n1\n2\n3\n4\n5\n6\n7\nCumulative Reward\nFreeway m1d0 ℓ2 + dropout (pconv, pfc = 0.05, 0.1)\npconv, pfc = 0.05, 0.1; λ = 10−6\npconv, pfc = 0.05, 0.1; λ = 10−5\npconv, pfc = 0.05, 0.1; λ = 10−4\npconv, pfc = 0.05, 0.1; λ = 10−3\npconv, pfc = 0.05, 0.1; λ = 10−2\n0M\n10M\n20M\n30M\n40M\n50M\nFrames before evaluation\n1\n2\n3\n4\n5\n6\n7\nCumulative Reward\nFreeway m1d0 ℓ2 + dropout (pconv, pfc = 0.1, 0.2)\npconv, pfc = 0.1, 0.2; λ = 10−6\npconv, pfc = 0.1, 0.2; λ = 10−5\npconv, pfc = 0.1, 0.2; λ = 10−4\npconv, pfc = 0.1, 0.2; λ = 10−3\npconv, pfc = 0.1, 0.2; λ = 10−2\n0M\n10M\n20M\n30M\n40M\n50M\nFrames before evaluation\n1\n2\n3\n4\n5\n6\n7\nCumulative Reward\nFreeway m1d0 ℓ2 + dropout (pconv, pfc = 0.15, 0.3)\npconv, pfc = 0.15, 0.3; λ = 10−6\npconv, pfc = 0.15, 0.3; λ = 10−5\npconv, pfc = 0.15, 0.3; λ = 10−4\npconv, pfc = 0.15, 0.3; λ = 10−3\npconv, pfc = 0.15, 0.3; λ = 10−2\n0M\n10M\n20M\n30M\n40M\n50M\nFrames before evaluation\n1\n2\n3\n4\n5\n6\n7\nCumulative Reward\nFreeway m1d0 ℓ2 + dropout (pconv, pfc = 0.2, 0.4)\npconv, pfc = 0.2, 0.4; λ = 10−6\npconv, pfc = 0.2, 0.4; λ = 10−5\npconv, pfc = 0.2, 0.4; λ = 10−4\npconv, pfc = 0.2, 0.4; λ = 10−3\npconv, pfc = 0.2, 0.4; λ = 10−2\n0M\n10M\n20M\n30M\n40M\n50M\nFrames before evaluation\n1\n2\n3\n4\n5\n6\n7\nCumulative Reward\nFreeway m1d0 ℓ2 + dropout (pconv, pfc = 0.25, 0.5)\npconv, pfc = 0.25, 0.5; λ = 10−6\npconv, pfc = 0.25, 0.5; λ = 10−5\npconv, pfc = 0.25, 0.5; λ = 10−4\npconv, pfc = 0.25, 0.5; λ = 10−3\npconv, pfc = 0.25, 0.5; λ = 10−2\nFigure 9: Evaluation performance for Freeway m1d0 post-training on Freeway\nm0d0 with dropout and ℓ2. For each plot pconv, pfc is held constant while varying\nthe ℓ2 regularization term λ. Each conﬁguration is averaged over ﬁve seeds.\n0M\n10M\n20M\n30M\n40M\n50M\nFrames before evaluation\n5\n10\n15\n20\n25\nCumulative Reward\nFreeway m4d0 ℓ2 + dropout (pconv, pfc = 0.05, 0.1)\npconv, pfc = 0.05, 0.1; λ = 10−6\npconv, pfc = 0.05, 0.1; λ = 10−5\npconv, pfc = 0.05, 0.1; λ = 10−4\npconv, pfc = 0.05, 0.1; λ = 10−3\npconv, pfc = 0.05, 0.1; λ = 10−2\n0M\n10M\n20M\n30M\n40M\n50M\nFrames before evaluation\n5\n10\n15\n20\n25\nCumulative Reward\nFreeway m4d0 ℓ2 + dropout (pconv, pfc = 0.1, 0.2)\npconv, pfc = 0.1, 0.2; λ = 10−6\npconv, pfc = 0.1, 0.2; λ = 10−5\npconv, pfc = 0.1, 0.2; λ = 10−4\npconv, pfc = 0.1, 0.2; λ = 10−3\npconv, pfc = 0.1, 0.2; λ = 10−2\n0M\n10M\n20M\n30M\n40M\n50M\nFrames before evaluation\n5\n10\n15\n20\n25\nCumulative Reward\nFreeway m4d0 ℓ2 + dropout (pconv, pfc = 0.15, 0.3)\npconv, pfc = 0.15, 0.3; λ = 10−6\npconv, pfc = 0.15, 0.3; λ = 10−5\npconv, pfc = 0.15, 0.3; λ = 10−4\npconv, pfc = 0.15, 0.3; λ = 10−3\npconv, pfc = 0.15, 0.3; λ = 10−2\n0M\n10M\n20M\n30M\n40M\n50M\nFrames before evaluation\n5\n10\n15\n20\n25\nCumulative Reward\nFreeway m4d0 ℓ2 + dropout (pconv, pfc = 0.2, 0.4)\npconv, pfc = 0.2, 0.4; λ = 10−6\npconv, pfc = 0.2, 0.4; λ = 10−5\npconv, pfc = 0.2, 0.4; λ = 10−4\npconv, pfc = 0.2, 0.4; λ = 10−3\npconv, pfc = 0.2, 0.4; λ = 10−2\n0M\n10M\n20M\n30M\n40M\n50M\nFrames before evaluation\n5\n10\n15\n20\n25\nCumulative Reward\nFreeway m4d0 ℓ2 + dropout (pconv, pfc = 0.25, 0.5)\npconv, pfc = 0.25, 0.5; λ = 10−6\npconv, pfc = 0.25, 0.5; λ = 10−5\npconv, pfc = 0.25, 0.5; λ = 10−4\npconv, pfc = 0.25, 0.5; λ = 10−3\npconv, pfc = 0.25, 0.5; λ = 10−2\nFigure 10: Evaluation performance for Freeway m4d0 post-training on Free-\nway m0d0 with dropout and ℓ2. We used the same method described in Figure 9.\n28\nPolicy Evaluation Learning Curves\nWe provide learning curves for policy evaluation from a ﬁxed representation in the\ndefault ﬂavour of each game we analyzed. Each subplot results from evaluating a\npolicy in the target ﬂavour which was trained with and without regularization in\nthe default ﬂavour. We speciﬁcally took weight checkpoints during training every\n500, 000 frames, up to 50M frames in total. Each checkpoint was then evaluated\nin the target ﬂavour for 100 episodes averaged over ﬁve runs. The regularized\nrepresentation was trained using a dropout rate of pconv, pfc = 0.05, 0.1, and\nλ = 10−4 for ℓ2 regularization.\n10M\n20M\n30M\n40M\n50M\nFrames before evaluation\n1\n2\n3\n4\n5\n6\n7\n8\nCumulative Reward\nFreeway m1d0\nm1d0\nm1d0 dropout + ℓ2\n10M\n20M\n30M\n40M\n50M\nFrames before evaluation\n1\n2\n3\n4\n5\n6\nCumulative Reward\nFreeway m1d1\nm1d1\nm1d1 dropout + ℓ2\n10M\n20M\n30M\n40M\n50M\nFrames before evaluation\n5\n10\n15\n20\nCumulative Reward\nFreeway m4d0\nm4d0\nm4d0 dropout + ℓ2\n0M\n10M\n20M\n30M\n40M\n50M\nFrames before evaluation\n25\n50\n75\n100\n125\n150\n175\nCumulative Reward\nHero m1d0\nm1d0\nm1d0 dropout + ℓ2\n0M\n10M\n20M\n30M\n40M\n50M\nFrames before evaluation\n20\n40\n60\n80\n100\nCumulative Reward\nHero m2d0\nm2d0\nm2d0 dropout + ℓ2\n0M\n10M\n20M\n30M\n40M\n50M\nFrames before evaluation\n10\n20\n30\n40\n50\nCumulative Reward\nBreakout m12d0\nm12d0\nm12d0 dropout + ℓ2\n10M\n20M\n30M\n40M\n50M\nFrames before evaluation\n50\n100\n150\n200\n250\n300\n350\nCumulative Reward\nSpace Invaders m1d0\nm1d0\nm1d0 dropout + ℓ2\n10M\n20M\n30M\n40M\n50M\nFrames before evaluation\n50\n100\n150\n200\n250\nCumulative Reward\nSpace Invaders m1d1\nm1d1\nm1d1 dropout + ℓ2\n10M\n20M\n30M\n40M\n50M\nFrames before evaluation\n50\n100\n150\n200\n250\n300\nCumulative Reward\nSpace Invaders m9d0\nm9d0\nm9d0 dropout + ℓ2\nFigure 11: Performance curves for policy evaluation results. The x-axis is the\nnumber of frames before we evaluated the ϵ-greedy policy from the default ﬂavour\non the target ﬂavour. The y-axis is the cumulative reward the agent incurred.\nGreen curves depict performance with regularization and red curves without.\n29\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2018-09-29",
  "updated": "2020-01-17"
}