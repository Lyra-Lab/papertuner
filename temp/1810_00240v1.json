{
  "id": "http://arxiv.org/abs/1810.00240v1",
  "title": "Reinforcement Learning in R",
  "authors": [
    "Nicolas Pröllochs",
    "Stefan Feuerriegel"
  ],
  "abstract": "Reinforcement learning refers to a group of methods from artificial\nintelligence where an agent performs learning through trial and error. It\ndiffers from supervised learning, since reinforcement learning requires no\nexplicit labels; instead, the agent interacts continuously with its\nenvironment. That is, the agent starts in a specific state and then performs an\naction, based on which it transitions to a new state and, depending on the\noutcome, receives a reward. Different strategies (e.g. Q-learning) have been\nproposed to maximize the overall reward, resulting in a so-called policy, which\ndefines the best possible action in each state. Mathematically, this process\ncan be formalized by a Markov decision process and it has been implemented by\npackages in R; however, there is currently no package available for\nreinforcement learning. As a remedy, this paper demonstrates how to perform\nreinforcement learning in R and, for this purpose, introduces the\nReinforcementLearning package. The package provides a remarkably flexible\nframework and is easily applied to a wide range of different problems. We\ndemonstrate its use by drawing upon common examples from the literature (e.g.\nfinding optimal game strategies).",
  "text": "1\nReinforcement Learning in R\nby Nicolas Pröllochs, Stefan Feuerriegel\nAbstract Reinforcement learning refers to a group of methods from artiﬁcial intelligence where an\nagent performs learning through trial and error. It differs from supervised learning, since reinforcement\nlearning requires no explicit labels; instead, the agent interacts continuously with its environment.\nThat is, the agent starts in a speciﬁc state and then performs an action, based on which it transitions to\na new state and, depending on the outcome, receives a reward. Different strategies (e.g. Q-learning)\nhave been proposed to maximize the overall reward, resulting in a so-called policy, which deﬁnes the\nbest possible action in each state. Mathematically, this process can be formalized by a Markov decision\nprocess and it has been implemented by packages in R; however, there is currently no package available\nfor reinforcement learning. As a remedy, this paper demonstrates how to perform reinforcement\nlearning in R and, for this purpose, introduces the ReinforcementLearning package. The package\nprovides a remarkably ﬂexible framework and is easily applied to a wide range of different problems.\nWe demonstrate its use by drawing upon common examples from the literature (e.g. ﬁnding optimal\ngame strategies).\nIntroduction\nReinforcement learning represents an approach to solving sequential and usually stochastic decision-\nmaking problems (Sutton and Barto, 1998). In contrast to supervised machine learning, it requires\nno explicit labels indicating what the correct solution is; instead, it interacts with the environment\nand learns through trial and error. In many cases, this approach appears quite natural by mimicking\nthe fundamental way humans learn and can thereby infer a rewarding strategy on its own. For this\npurpose, reinforcement learning assumes an agent that sequentially undertakes different actions\nbased on which it transitions between states. Here it receives only limited feedback in the form of a\nnumerical reward that is to be maximized over time. The agent usually performs the action promising\nthe highest long-term reward, but will sometimes follow a random choice as part of its exploration.\nMathematically, the underlying problem relies upon the Markov decision process (MDP in short; cf.\nBellman, 1957), which we formalize in Section 2.2.2. To this end, we refer to a detailed introduction to\nthe literature, such as Sutton and Barto (1998).\nReinforcement learning can be applied to a wide range of sequential decision-making problems\nfrom numerous domains. We detail a few illustrative examples in the following. Among others, it is\nparticularly popular in robotics (e.g. Smart and Kaelbling, 2002; Matari´c, 1997) where reinforcement\nlearning navigates robots with the aim of avoiding collisions with obstacles (Beom and Cho, 1995).\nAnalogously, reinforcement learning facilitates decision-making problems underlying control theory,\nsuch as improving the performance of elevators (Crites and Barto, 1996). Beyond engineering settings,\nits methodology also applies to the ﬁeld of game theory (Erev and Roth, 1998; Erev and Rapoport, 1998)\nand, similarly, allows algorithms to derive strategies for computer games (Mnih et al., 2015). Beyond\nthat, it also helps in optimizing ﬁnancial trading (Nevmyvaka et al., 2006) or tuning hyperparameters\nin machine learning algorithms (Zoph and Le, 2016). In the context of computational linguistics,\nSchefﬂer and Young (2002) utilize reinforcement learning to enhance human-computer dialogs, while\nPröllochs et al. (2016) derive policies for detecting negation scopes in order to improve the accuracy of\nsentiment analysis. Altogether, reinforcement learning helps to address all kind of problems involving\nsequential decision-making.\nReinforcement learning techniques can primarily be categorized in two groups, namely, model-\nbased and model-free approaches (Sutton and Barto, 1998). The former, model-based algorithms, rely\non explicit models of the environment that fully describe the probabilities of state transitions, as well\nas the consequences of actions and the associated rewards. Speciﬁcally, corresponding algorithms are\nbuilt upon explicit representations of the environment given in the form of Markov decision processes.\nThese MDPs can be solved by various, well-known algorithms, including value iteration and policy\niteration, in order to derive the optimal behavior of an agent. These algorithms are also implemented\nwithin the statistical software R. For instance, the package MDPtoolbox solves such models based\non an explicit formalization of the MDP, i.e. settings in which the transition probabilities and reward\nfunctions are known a priori (Chades et al., 2017).\nThe second category of reinforcement learning comprises model-free approaches that forgo any\nexplicit knowledge regarding the dynamics of the environment. These approaches learn the optimal\nbehavior through trial-and-error by directly interacting with the environment. In this context, the\nlearner has no explicit knowledge of either the reward function or the state transition function (Hu\nand Wellman, 2003). Instead, the optimal behavior is iteratively inferred from the consequences of\nactions on the basis of past experience. As a main advantage, this method is applicable to situations\narXiv:1810.00240v1  [cs.LG]  29 Sep 2018\n2\nin which the dynamics of the environment are unknown or too complex to evaluate. However, the\navailable tools in R are not yet living up to the needs of users in such cases. In fact, there is currently\nno package available that allows one to perform model-free reinforcement learning in R. Hence, users\nthat aim to teach optimal behavior through trial-and-error learning must implement corresponding\nlearning algorithms in a manual way.\nAs a remedy, we introduce the ReinforcementLearning package for R, which allows an agent to\nlearn the optimal behavior based on sampling experience consisting of states, actions and rewards\n(Pröllochs and Feuerriegel, 2017)1. The training examples for reinforcement learning can originate\nfrom real-world applications, such as sensor data. In addition, the package is shipped with the built-in\ncapability to sample experience from a function that deﬁnes the dynamics of the environment. In both\ncases, the result of the learning process is a highly interpretable reinforcement learning policy that\ndeﬁnes the best possible action in each state. The package provides a remarkably ﬂexible framework,\nwhich makes it readily applicable to a wide range of different problems. Among other functions, it\nallows one to customize a variety of learning parameters and elaborates on how to mitigate common\nperformance in common solution algorithms (e.g. experience replay).\nAs our main contribution, we present a novel package for implementing reinforcement learning in\nR. This can have a considerable impact for practitioners in the ﬁeld of artiﬁcial intelligence and, hence,\nimmediately reveals manifold implications: one can utilize our package to implement self-learning\nsystems. We thus detail the applicability and usage of the package throughout this paper. For this\npurpose, we demonstrate the package by drawing upon common examples from the literature.\nThe remainder of this paper is structured as follows. Section 2.2 speciﬁes a mathematical formal-\nization of reinforcement learning and details how it solves sequential decision-making problems in a\ndynamic environment. Subsequently, we introduce the ReinforcementLearning package and demon-\nstrate its main functionality and routines (Section 2.3). Section 2.4 provides a illustrative example in\nwhich a robot needs to ﬁnd the optimal movements in a maze, while Section 2.5 concludes.\nBackground\nMarkov decision process\nA Markov decision process (Bellman, 1957) is deﬁned by a 5-tuple (S, A, P, R, γ) where S is a set of\nstates. Let s ∈S refer to the current state. Then, in each state, one can undertake an action a ∈A\nfrom the set of actions A. Here one sometimes assumes only a subset As ⊂A dependent on a state s.\nBy taking an action, the state transitions from s to s′ with a probability Pa(s, s′). Upon arriving in a\nnew state s′ due to action a, one receives a reward given by Ra(s, s′). Lastly, the parameter γ ∈[0, 1]\ndenotes a discount factor, which controls how important the current reward is in future settings.\nAs a result, we observe a few characteristics of our problem: ﬁrst, the problem is time-dependent\nin the sense that choosing an action inﬂuences our future state. Second, the formulation implies a\nstochastic setting where the new state is not deterministic but can only be predicted with a certain\nprobability. Third, a model-free setup requires that parameters P and R are not known a priori.\nInstead, once can merely learn the state transitions and expected rewards from interacting with the\nenvironment. This motivates the following approach underlying reinforcement learning.\nReinforcement learning\nProblem speciﬁcation\nReinforcement learning assumes a so-called agent which interacts with the environment over a sequence\nof discrete steps. Thereby, the agent develops an understanding of the environment (mathematically,\nP and R), while seeking to maximize the cumulative reward over time. Formally, we rewrite the\nproblem formulation as an iterative process, as visualized in Figure 1. Based on the current state si in\nstep i, the agent picks an action ai ∈Asi ⊆A out of the set of available actions for the given state si.\nSubsequently, the agent receives feedback related to its decision in the form of a numerical reward\nri+1, after which it then moves to the next state si+1.\nThe resulting objective of reinforcement learning is then to determine a policy π(si, ai) that deﬁnes\nwhich action ai to take in state si. In other words, the goal of the agent is to learn the optimal policy\nfunction for all states and actions. To do so, the approach maintains a state-action function Q(si, ai),\nspecifying the expected reward for each possible action ai in state si. This knowledge can then be used\nto infer the optimal behavior, i.e. the policy that maximizes the expected reward from any state. The\n1The ReinforcementLearning R package is available from the Comprehensive R Archive Network (CRAN) at\nhttp://cran.r-project.org/package=ReinforcementLearning.\n3\na1\na0\ns0\naT 1\nsT\nAgent\nEnvironment\nπ \nCumulative reward\ns1\nr1\ns2\nr2\nrT\nFigure 1: The agent-environment interaction in reinforcement learning.\noptimal policy π∗maximizes the overall expected reward or, more speciﬁcally, the sum of discounted\nrewards given by\nπ∗= max\nπ\nE\n\"\n∞\n∑\ni=0\nγirt ∥s0 = s\n#\n.\n(1)\nIn practice, one chooses the actions ai that maximize Q(si, ai). A common approach to computing\nthis policy is via Q-learning (Watkins and Dayan, 1992). As opposed to a model-based approach,\nQ-learning has no explicit knowledge of either the reward function or the state transition function (Hu\nand Wellman, 2003). Instead, it iteratively updates its action-value Q(si, ai) based on past experience.\nAlternatives are, for instance, SARSA or temporal-difference (TD) learning, but these are less common\nin practice nowadays.\nExploration\nThe above descriptions specify how to choose the optimal action after having computed Q. For this\npurpose, reinforcement learning performs an exploration in which it randomly selects action without\nreference to Q. Thereby, it collects knowledge about how rewarding actions are in each state. One such\nexploration method is ε-greedy, where the agent chooses a random action uniformly with probability\nε ∈(0, 1) and otherwise follows the one with the highest expected long-term reward. Mathematically,\nthis yields\nai =\n(\nmaxa∈Asi Q(si, ai),\nwith probability 1 −ε,\npick random a ∈Asi,\nwith probability ε.\n(2)\nFor a description of alternative heuristics, we refer to Sutton and Barto (1998).\nQ-learning\nThe Q-learning algorithm (Watkins and Dayan, 1992) starts with arbitrary initialization of Q(si, ai)\nfor all si ∈S and ai ∈A. The agent then proceeds with the aforementioned interaction steps: (1) it\nobserves the current state si, (2) it performs an action ai, (3) it receives a subsequent reward ri+1 after\nwhich it observes the next state si+1. Based on this information, it then estimates the optimal future\nvalue Q(si+1, ai+1), i.e. the maximum possible reward across all actions available in si+1. Subsequently,\nit uses this knowledge to update the value of Q(si, ai). This is accomplished via the update rule\nQ(si, ai) ←Q(si, ai) + α\n\"\nri+1 + γ max\nai+1∈Asi\nQ(si+1, ai+1) −Q(si, ai)\n#\n,\n(3)\nwith a given learning rate α and discount factor γ. Thus, Q-learning learns an optimal policy while\nfollowing another policy for action selection. This feature is called off-policy learning. More detailed\nmathematical explanations and a thorough survey of this and other reinforcement learning methods\nare given in, for example, Kaelbling et al. (1996) and Sutton and Barto (1998).\n4\nExtensions\nBatch learning\nReinforcement learning generally maximizes the sum of expected rewards in an agent-environment\nloop. However, this setup often needs many interactions until convergence to an optimal policy. As\na result, the approach is hardly feasible in complex, real-world scenarios. A suitable remedy comes\nin the form of so-called batch reinforcement learning (Lange et al., 2012), which drastically reduces the\ncomputation time in most settings. Different from the previous setup, the agent no longer interacts\nwith the environment by processing single state-action pairs for which it updates the Q-table on\nan individual basis. Instead, it directly receives a set of tuples (si, ai, ri+1, si+1) sampled from the\nenvironment. After processing this batch, it updates the Q-table, as well as the policy. This procedure\ncommonly results in greater efﬁciency and stability of the learning process, since the noise attached to\nindividual explorations cancels out as one combines several explorations in a batch.\nThe batch process is grouped into three phases (see Figure 2). In the ﬁrst phase, the learner explores\nthe environment by collecting transitions with an arbitrary sampling strategy, e.g. using a purely\nrandom policy or ε-greedy. These data points can be, for example, collected from a running system\nwithout the need for direct interaction. In a second step, the stored training examples then allow the\nagent to learn a state-action function, e.g. via Q-learning. The result of the learning step is the best\npossible policy for every state transition in the input data. In a ﬁnal step, the learned policy can be\napplied to the system. Here the policy remains ﬁxed and is no longer improved.2\nLearning\nOut-of-sample\nevaluation\nBatch reinforcement learning\nExploration\nPolicy update\nBatch\nPolicy π* \n)\n,\n,\n,\n(\n1\n1\n\n\ni\ni\ni\ni\ns\nr\na\ns\nFigure 2: Batch reinforcement learning.\nExperience replay\nAnother performance issue that can result in poor performance in reinforcement learning cases is\nthat information might not easily spread through the whole state space. For example, an update of\nQ-value in step i of the state-action pair (si, ai) might update the values of (si−1, a) for all a ∈Asi].\nHowever, this update is not back-propagated immediately to all the involved preceding states. Instead,\npreceding states are only updated the next time they are visited. These local updates can result\nin serious performance issues since additional interactions are needed to spread already-available\ninformation through the whole state space.\nAs a remedy, the concept of experience replay allows reinforcement learning agents to remember\nand reuse experiences from the past (Lin, 1992). The underlying idea is to speed up convergence by\nreplaying observed state transitions repeatedly to the agent, as if they were new observations collected\nwhile interacting with a system. Experience replay thus makes more efﬁcient use of this information\nby resampling the connection between individual states. As its main advantage, experience replay can\nspeed up convergence by allowing for the back-propagation of information from updated states to\npreceding states without further interaction.\nPackage features\nEven though reinforcement learning has become prominent in machine learning, the R landscape is\nnot living up to the needs of researchers and practitioners. The ReinforcementLearning package is\n2Alternatively, the policy can be used for validation purposes or to collect new data points (e.g. in order to\niteratively improve the current policy). This variant of reinforcement learning is referred to as growing batch\nreinforcement learning. In this method, the agent alternates between exploration and training phases, which is\noftentimes the model of choice when applying reinforcement learning to real-world problems. It is also worth\nnoting that this approach is similar to the general reinforcement learning problem: the agent improves its policy\nwhile interacting with the system.\n5\nintended to close this gap and offers the ability to perform model-free reinforcement learning in a\nhighly customizable framework. The following sections present its usage and main functionality.\nBefore running all subsequent code snippets, one ﬁrst needs to load the package via:\nlibrary(\"ReinforcementLearning\")\nData preparation\nThe ReinforcementLearning package utilizes different mechanisms for reinforcement learning, in-\ncluding Q-learning and experience replay. It thereby learns an optimal policy based on past experience\nin the form of sample sequences consisting of states, actions and rewards. Consequently, each training\nexample consists of a state-transition tuple (si, ai, ri+1, si+1) as follows:\n• si is the current environment state.\n• ai denotes the selected action in the current state.\n• ri+1 speciﬁes the immediate reward received after transitioning from the current state to the\nnext state.\n• si+1 refers to the next environment state.\nThe training examples for reinforcement learning can (1) be collected from an external source and\ninserted into a tabular data structure, or (2) generated dynamically by querying a function that deﬁnes\nthe behavior of the environment. In both cases, the corresponding input must follow the same tuple\nstructure (si, ai, ri+1, si+1). We detail both variants in the following.\nLearning from pre-deﬁned observations\nThis approach is beneﬁcial when the input data is pre-determined or one wants to train an agent that\nreplicates past behavior. In this case, one merely needs to insert a tabular data structure with past\nobservations into the package. This might be the case when the state-transition tuples have been\ncollected from an external source, such as sensor data, and one wants to learn an agent by eliminating\nfurther interaction with the environment.\nExample. The following example shows the ﬁrst ﬁve observations of a representative dataset\ncontaining game states of randomly sampled tic-tac-toe games3. In this dataset, the ﬁrst column\ncontains a representation of the current board state in a match. The second column denotes the\nobserved action of player X in this state, whereas the third column contains a representation of the\nresulting board state after performing the action. The fourth column speciﬁes the resulting reward for\nplayer X. This dataset is thus sufﬁcient as input for learning the agent.\ndata(\"tictactoe\")\nhead(tictactoe, 5)\n#>\nState Action NextState Reward\n#> 1 .........\nc7 ......X.B\n0\n#> 2 ......X.B\nc6 ...B.XX.B\n0\n#> 3 ...B.XX.B\nc2 .XBB.XX.B\n0\n#> 4 .XBB.XX.B\nc8 .XBBBXXXB\n0\n#> 5 .XBBBXXXB\nc1 XXBBBXXXB\n0\nDynamic learning from an interactive environment function\nAn alternative strategy is to deﬁne a function that mimics the behavior of the environment. One can\nthen learn an agent that samples experience from this function. Here the environment function takes a\nstate-action pair as input. It then returns a list containing the name of the next state and the reward. In\nthis case, one can also utilize R to access external data sources, such as sensors, and execute actions via\ncommon interfaces. The structure of such a function is represented by the following pseudocode:\nenvironment <- function(state, action) {\n...\nreturn(list(\"NextState\" = newState,\n3The tic-tac-toe dataset contains game states of 406,541 randomly-sampled tic-tac-toe games and is included\nin the ReinforcementLearning R package. All states are observed from the perspective of player X who is also\nassumed to have played ﬁrst. The player who succeeds in placing three of their marks in a horizontal, vertical, or\ndiagonal row wins the game. Reward for player X is +1 for ’win’, 0 for ’draw’, and -1 for ’loss’.\n6\n\"Reward\" = reward))\n}\nAfter specifying the environment function, we can use sampleExperience() to collect random\nsequences from it. Thereby, the input speciﬁes number of samples (N), the environment function, the\nset of states (i.e. S) and the set of actions (i.e. A). The return value is then a data frame containing\nthe experienced state transition tuples (si, ai, ri+1, si+1) for i = 1, . . . , N. The following code snippet\nshows how to generate experience from an exemplary environment function.4\n# Define state and action sets\nstates <- c(\"s1\", \"s2\", \"s3\", \"s4\")\nactions <- c(\"up\", \"down\", \"left\", \"right\")\nenv <- gridworldEnvironment\n# Sample N = 1000 random sequences from the environment\ndata <- sampleExperience(N = 1000,\nenv = env,\nstates = states,\nactions = actions)\nLearning phase\nGeneral setup\nThe routine ReinforcementLearning() bundles the main functionality, which teaches a reinforcement\nlearning agent using the previous input data. For this purpose, it requires the following arguments:\n(1) A data argument that must be a data frame object in which each row represents a state transition\ntuple (si, ai, ri+1, si+1). (2) The user is required to specify the column names of the individual tuple\nelements within data.\nThe following pseudocode demonstrates the usage for pre-deﬁned data from an external source,\nwhile Section 2.4 details the interactive setup. Here the parameters s, a, r and s_new contain strings\nspecifying the corresponding column names in the data frame data.\n# Load dataset\ndata(\"tictactoe\")\n# Perform reinforcement learning\nmodel <- ReinforcementLearning(data = tictactoe,\ns = \"State\",\na = \"Action\",\nr = \"Reward\",\ns_new = \"NextState\",\niter = 1)\nParameter conﬁguration\nSeveral parameters can be provided to ReinforcementLearning() in order to customize the learning\nbehavior of the agent.\n• alpha The learning rate, set between 0 and 1. Setting it to 0 means that the Q-values are never\nupdated and, hence, nothing is learned. Setting a high value, such as 0.9, means that learning\ncan occur quickly.\n• gamma Discount factor, set between 0 and 1. Determines the importance of future rewards. A\nfactor of 0 will render the agent short-sighted by only considering current rewards, while a\nfactor approaching 1 will cause it to strive for a greater reward over the long run.\n• epsilon Exploration parameter, set between 0 and 1. Deﬁnes the exploration mechanism in\nε-greedy action selection. In this strategy, the agent explores the environment by selecting an\naction at random with probability ε. Alternatively, the agent exploits its current knowledge by\nchoosing the optimal action with probability 1 −ε. This parameter is only required for sampling\nnew experience based on an existing policy.\n4The exemplary grid world environment function is included in the package and can be loaded via\ngridworldEnvironment. We will detail this example later in Section 2.4.\n7\n• iter Number of repeated learning iterations. This must be an integer greater than 0. The default\nis set to 1. This parameter is passed directly to ReinforcementLearning().\nThe learning parameters alpha, gamma, and epsilon must be provided in an optional control\nobject passed to the ReinforcementLearning() function.\n# Define control object\ncontrol <- list(alpha = 0.1, gamma = 0.1, epsilon = 0.1)\n# Pass learning parameters to reinforcement learning function\nmodel <- ReinforcementLearning(data, iter = 10, control = control)\nDiagnostics\nThe result of the learning process is an object of type \"rl\" that contains the state-action table and an\noptimal policy with the best possible action in each state. The command policy(model) shows the\noptimal policy, while print(model) outputs the state-action table, i.e. the Q-value of each state-action\npair. In addition, summary(model) prints further model details and summary statistics.\n# Print policy\npolicy(model)\n# Print state-action table\nprint(model)\n# Print summary statistics\nsummary(model)\nIllustrative example\nThis section demonstrates the capabilities of the ReinforcementLearning package with the help of a\npractical example.\nProblem deﬁnition\nOur practical example aims at teaching optimal movements to a robot in a grid-shaped maze (Sutton\nand Barto, 1998). Here the agent must navigate from a random starting position to a ﬁnal position\non a simulated 2 × 2 grid (see Figure 3). Each cell on the grid reﬂects one state, yielding a total of 4\ndifferent states. In each state, the agent can perform one out of four possible actions, i.e. to move up,\ndown, left, or right, with the only restriction being that it must remain on the grid. In other words, the\ngrid is surrounded by a wall, which makes it impossible for the agent to move off the grid. A wall\nbetween s1 and s4 hinders direct movements between these states. Finally, the reward structures is as\nfollows: each movement leads to a negative reward of -1 in order to penalize routes that are not the\nshortest path. If the agent reaches the goal position, it earns a reward of 10.\ns1\ns4\ns2\ns3\nFigure 3: Simulated 2 × 2 grid.\nDeﬁning an environment function\nWe ﬁrst deﬁne the sets of available states (states) and actions (actions).\n# Define state and action sets\nstates <- c(\"s1\", \"s2\", \"s3\", \"s4\")\nactions <- c(\"up\", \"down\", \"left\", \"right\")\n8\nWe then rewrite the above problem formulation into the following environment function. As\npreviously mentioned, this function must take a state and an action as input. The if-conditions\ndetermine the current combination of state and action. In our example, the state refers to the agent’s\nposition on the grid and the action denotes the intended movement. Based on these, the function\ndecides upon the next state and a numeric reward. These together are returned as a list.\n# Load built-in environment function for 2x2 gridworld\nenv <- gridworldEnvironment\nprint(env)\n#> function (state, action)\n#> {\n#>\nnext_state <- state\n#>\nif (state == state(\"s1\") && action == \"down\")\n#>\nnext_state <- state(\"s2\")\n#>\nif (state == state(\"s2\") && action == \"up\")\n#>\nnext_state <- state(\"s1\")\n#>\nif (state == state(\"s2\") && action == \"right\")\n#>\nnext_state <- state(\"s3\")\n#>\nif (state == state(\"s3\") && action == \"left\")\n#>\nnext_state <- state(\"s2\")\n#>\nif (state == state(\"s3\") && action == \"up\")\n#>\nnext_state <- state(\"s4\")\n#>\nif (next_state == state(\"s4\") && state != state(\"s4\")) {\n#>\nreward <- 10\n#>\n}\n#>\nelse {\n#>\nreward <- -1\n#>\n}\n#>\nout <- list(NextState = next_state, Reward = reward)\n#>\nreturn(out)\n#> }\n#> <bytecode: 0x000000001654bcc0>\n#> <environment: namespace:ReinforcementLearning>\nLearning an optimal policy\nAfter having speciﬁed the environment function, we can use the built-in sampleExperience() function\nto sample observation sequences from the environment. The following code snippet generates a data\nframe data containing 1000 random state-transition tuples (si, ai, ri+1, si+1).\n# Sample N = 1000 random sequences from the environment\ndata <- sampleExperience(N = 1000,\nenv = env,\nstates = states,\nactions = actions)\nhead(data)\n#>\nState Action Reward NextState\n#> 1\ns1\ndown\n-1\ns2\n#> 2\ns1\ndown\n-1\ns2\n#> 3\ns3\ndown\n-1\ns3\n#> 4\ns1\nup\n-1\ns1\n#> 5\ns3\nright\n-1\ns3\n#> 6\ns4\nright\n-1\ns4\nWe can now use the observation sequence in data in order to learn the optimal behavior of the agent.\nFor this purpose, we ﬁrst customize the learning behavior of the agent by deﬁning a control object. We\nfollow the default parameter choices and set the learning rate alpha to 0.1, the discount factor gamma to\n0.5 and the exploration greediness epsilon to 0.1. Subsequently, we use the ReinforcementLearning()\nfunction to learn the best possible policy for the the input data.\n# Define reinforcement learning parameters\ncontrol <- list(alpha = 0.1, gamma = 0.5, epsilon = 0.1)\n9\n# Perform reinforcement learning\nmodel <- ReinforcementLearning(data,\ns = \"State\",\na = \"Action\",\nr = \"Reward\",\ns_new = \"NextState\",\ncontrol = control)\nEvaluating policy learning\nThe ReinforcementLearning() function returns an \"rl\" object. We can evoke policy(model) in order\nto display the policy that deﬁnes the best possible action in each state. Alternatively, we can use\nprint(model) in order to write the entire state-action table to the screen, i.e. the Q-value of each\nstate-action pair. Evidently, the agent has learned the optimal policy that allows it to take the shortest\npath from an arbitrary starting position to the goal position s4.\n# Print policy\npolicy(model)\n#>\ns1\ns2\ns3\ns4\n#>\n\"down\" \"right\"\n\"up\"\n\"left\"\n# Print state-action function\nprint(model)\n#> State-Action function Q\n#>\nright\nup\ndown\nleft\n#> s1 -0.7210267 -0.7275138\n0.6573701 -0.7535771\n#> s2\n3.5286336 -0.7862925\n0.6358511\n0.6607884\n#> s3\n3.5460468\n9.1030684\n3.5353220\n0.6484856\n#> s4 -1.8697756 -1.8759779 -1.8935405 -1.8592323\n#>\n#> Policy\n#>\ns1\ns2\ns3\ns4\n#>\n\"down\" \"right\"\n\"up\"\n\"left\"\n#>\n#> Reward (last iteration)\n#> [1] -340\nUltimately, we can use summary(model) to inspect the model further. This command outputs\nadditional diagnostics regarding the model such as the number of states and actions. Moreover, it\nallows us to analyze the distribution of rewards. For instance, we see that the total reward in our\nsample (i.e. the sum of the rewards column r) is highly negative. This indicates that the random policy\nused to generate the state transition samples deviates from the optimal case. Hence, the next section\nexplains how to apply and update a learned policy with new data samples.\n# Print summary statistics\nsummary(model)\n#>\n#> Model details\n#> Learning rule:\nexperienceReplay\n#> Learning iterations:\n1\n#> Number of states:\n4\n#> Number of actions:\n4\n#> Total Reward:\n-340\n#>\n#> Reward details (per iteration)\n#> Min:\n-340\n#> Max:\n-340\n#> Average:\n-340\n#> Median:\n-340\n#> Standard deviation:\nNA\n10\nApplying a policy to unseen data\nWe now apply an existing policy to unseen data in order to evaluate the out-of-sample performance\nof the agent. The following example demonstrates how to sample new data points from an existing\npolicy. The result yields a column with the best possible action for each given state.\n# Example data\ndata_unseen <- data.frame(State = c(\"s1\", \"s2\", \"s1\"),\nstringsAsFactors = FALSE)\n# Pick optimal action\ndata_unseen$OptimalAction <- predict(model, data_unseen$State)\ndata_unseen\n#>\nState OptimalAction\n#> 1\ns1\ndown\n#> 2\ns2\nright\n#> 3\ns1\ndown\nUpdating an existing policy\nFinally, one can update an existing policy with new observational data. This is beneﬁcial when, for\ninstance, additional data points become available or when one wants to plot the reward as a function\nof the number of training samples. For this purpose, the ReinforcementLearning() function can take\nan existing \"rl\" model as an additional input parameter. Moreover, it comes with an additional\npre-deﬁned action selection mode, namely ε-greedy, thereby following the best action with probability\n1 −ε and a random one with ε.\n# Sample N = 1000 sequences from the environment\n# using epsilon-greedy action selection\ndata_new <- sampleExperience(N = 1000,\nenv = env,\nstates = states,\nactions = actions,\nactionSelection = \"epsilon-greedy\",\nmodel = model,\ncontrol = control)\n# Update the existing policy using new training data\nmodel_new <- ReinforcementLearning(data_new,\ns = \"State\",\na = \"Action\",\nr = \"Reward\",\ns_new = \"NextState\",\ncontrol = control,\nmodel = model)\nThe following code snippet shows that the updated policy yields signiﬁcantly higher rewards\nas compared to the previous policy. These changes can also be visualized in a learning curve via\nplot(model_new).\n# Print result\nprint(model_new)\n#> State-Action function Q\n#>\nright\nup\ndown\nleft\n#> s1 -0.6506604 -0.6718149\n0.7627013 -0.6704840\n#> s2\n3.5253380 -0.7694555\n0.7197440\n0.6965138\n#> s3\n3.5364934\n9.0505592\n3.5328707\n0.7194558\n#> s4 -1.8989173 -1.9085736 -1.9072245 -1.9494559\n#>\n#> Policy\n#>\ns1\ns2\ns3\ns4\n#>\n\"down\" \"right\"\n\"up\" \"right\"\n11\n#>\n#> Reward (last iteration)\n#> [1] 1464\n# Plot reinforcement learning curve\nplot(model_new)\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n0\n1000\nReinforcement learning curve\nLearning iteration\nReward\nConclusion\nReinforcement learning has gained considerable traction as it mines real experiences with the help of\ntrial-and-error learning. In this sense, it attempts to imitate the fundamental method used by humans\nto learn optimal behavior without the need for of an explicit model of the environment. In contrast to\nmany other approaches from the domain of machine learning, reinforcement learning solves sequential\ndecision-making problems of arbitrary length and can be used to learn optimal strategies in many\napplications such as robotics and game playing.\nImplementing reinforcement learning is programmatically challenging, since it relies on continu-\nous interactions between an agent and its environment. To remedy this, we introduce the Reinforce-\nmentLearning package for R, which allows an agent to learn from states, actions and rewards. The\nresult of the learning process is a policy that deﬁnes the best possible action in each state. The package\nis highly ﬂexible and incorporates a variety of learning parameters, as well as substantial performance\nimprovements such as experience replay.\nBibliography\nR. Bellman. A markovian decision process. Journal of Mathematics and Mechanics, 6(5):679–684, 1957.\n[p1, 2]\nH. R. Beom and H. S. Cho. A sensor-based navigation for a mobile robot using fuzzy logic and\nreinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics, 25(3):464–477, 1995. [p1]\nI. Chades, G. Chapron, M.-J. Cros, F. Garcia, and R. Sabbadin. MDPtoolbox, 2017. URL https://cran.r-\nproject.org/web/packages/MDPtoolbox/index.html. R package version 4.0-3. [p1]\nR. H. Crites and A. G. Barto. Improving elevator performance using reinforcement learning. Advances\nin Neural Information Processing Systems, 8:1017–1023, 1996. [p1]\nI. Erev and A. Rapoport. Coordination, “magic,” and reinforcement learning in a market entry game.\nGames and Economic Behavior, 23(2):146–175, 1998. [p1]\nI. Erev and A. E. Roth. Predicting how people play games: Reinforcement learning in experimental\ngames with unique, mixed strategy equilibria. American Economic Review, pages 848–881, 1998. [p1]\nJ. Hu and M. P. Wellman. Nash q-learning for general-sum stochastic games. Journal of Machine\nLearning Research, 4:1039–1069, 2003. [p1, 3]\n12\nL. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement learning: A survey. Journal of Artiﬁcial\nIntelligence Research, 4:237–285, 1996. [p3]\nS. Lange, T. Gabel, and M. Riedmiller. Batch reinforcement learning. In M. Wiering and M. van Otterlo,\neditors, Reinforcement Learning, volume 12 of Adaptation, Learning, and Optimization, pages 45–73.\nSpringer, Heidelberg and New York, 2012. [p4]\nL.-J. Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching.\nMachine Learning, 8(3):293–321, 1992. [p4]\nM. J. Matari´c. Reinforcement learning in the multi-robot domain. In R. C. Arkin and G. A. Bekey,\neditors, Robot Colonies, pages 73–83. Springer, Boston, MA, 1997. [p1]\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller,\nA. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran,\nD. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning.\nNature, 518(7540):529–533, 2015. [p1]\nY. Nevmyvaka, Y. Feng, and M. Kearns. Reinforcement learning for optimized trade execution. In\nProceedings of the 23rd International Conference on Machine Learning, ICML ’06, pages 673–680, New\nYork, NY, USA, 2006. ACM. [p1]\nN. Pröllochs and S. Feuerriegel. ReinforcementLearning, 2017. URL https://CRAN.R-project.org/\npackage=ReinforcementLearning. R package version 1.0-1. [p2]\nN. Pröllochs, S. Feuerriegel, and D. Neumann. Negation scope detection in sentiment analysis:\nDecision support for news-driven trading. Decision Support Systems, 88:67–75, 2016. [p1]\nK. Schefﬂer and S. Young. Automatic learning of dialogue strategy using dialogue simulation and\nreinforcement learning. In Proceedings of the Second International Conference on Human Language\nTechnology Research, pages 12–19, 2002. [p1]\nW. D. Smart and P. L. Kaelbling. Effective reinforcement learning for mobile robots. In IEEE International\nConference on Robotics and Automation (ICRA), pages 3404–3410, 2002. [p1]\nR. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. Adaptive Computation and\nMachine Learning. MIT Press, Cambridge, MA, 1998. [p1, 3, 7]\nC. J. C. H. Watkins and P. Dayan. Q-learning. Machine Learning, 8(3-4):279–292, 1992. [p3]\nB. Zoph and Q. V. Le.\nNeural architecture search with reinforcement learning.\narXiv preprint\narXiv:1611.01578, 2016. [p1]\nNicolas Pröllochs\nUniversity of Oxford\nWalton Well Rd, Oxford OX2 6ED, United Kingdom\nnicolas.prollochs@eng.ox.ac.uk\nStefan Feuerriegel\nETH Zurich\nWeinbergstr. 56/58, 8092 Zurich, Switzerland\nsfeuerriegel@ethz.ch\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2018-09-29",
  "updated": "2018-09-29"
}