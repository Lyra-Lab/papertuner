{
  "id": "http://arxiv.org/abs/1810.00368v2",
  "title": "Deep Quality-Value (DQV) Learning",
  "authors": [
    "Matthia Sabatelli",
    "Gilles Louppe",
    "Pierre Geurts",
    "Marco A. Wiering"
  ],
  "abstract": "We introduce a novel Deep Reinforcement Learning (DRL) algorithm called Deep\nQuality-Value (DQV) Learning. DQV uses temporal-difference learning to train a\nValue neural network and uses this network for training a second Quality-value\nnetwork that learns to estimate state-action values. We first test DQV's update\nrules with Multilayer Perceptrons as function approximators on two classic RL\nproblems, and then extend DQV with the use of Deep Convolutional Neural\nNetworks, `Experience Replay' and `Target Neural Networks' for tackling four\ngames of the Atari Arcade Learning environment. Our results show that DQV\nlearns significantly faster and better than Deep Q-Learning and Double Deep\nQ-Learning, suggesting that our algorithm can potentially be a better\nperforming synchronous temporal difference algorithm than what is currently\npresent in DRL.",
  "text": "Deep Quality-Value (DQV) Learning\nMatthia Sabatelli\nMonteﬁore Institute\nUniversité de Liège, Belgium\nm.sabatelli@uliege.be\nGilles Louppe\nMonteﬁore Institute\nUniversité de Liège, Belgium\ng.louppe@uliege.be\nPierre Geurts\nMonteﬁore Institute\nUniversité de Liège, Belgium\np.geurts@uliege.be\nMarco A. Wiering\nBernoulli Institute for Mathematics, Computer Science\nand Artiﬁcial Intelligence\nUniversity of Groningen, The Netherlands\nm.a.wiering@rug.nl\nAbstract\nWe introduce a novel Deep Reinforcement Learning (DRL) algorithm called Deep\nQuality-Value (DQV) Learning. DQV uses temporal-difference learning to train\na Value neural network and uses this network for training a second Quality-value\nnetwork that learns to estimate state-action values. We ﬁrst test DQV’s update rules\nwith Multilayer Perceptrons as function approximators on two classic RL problems,\nand then extend DQV with the use of Deep Convolutional Neural Networks,\n‘Experience Replay’ and ‘Target Neural Networks’ for tackling four games of the\nAtari Arcade Learning environment. Our results show that DQV learns signiﬁcantly\nfaster and better than Deep Q-Learning and Double Deep Q-Learning, suggesting\nthat our algorithm can potentially be a better performing synchronous temporal\ndifference algorithm than what is currently present in DRL.\n1\nIntroduction\nIn Reinforcement Learning (RL), Temporal-Difference (TD) learning has become a design choice\nwhich is shared among the most successful algorithms that are present in the ﬁeld [2]. Whether it is\nused in a Tabular-RL setting [11, 33], or in combination with a function approximator [28, 30], TD\nmethods aim to learn a Value function, V, by directly bootstrapping their own experiences at different\ntime-steps t. This is done with respect to a discount factor, γ, and a reward, r, which allow the\ncomputation of the TD-errors, rt +γV(st+1)−V(s). Since the TD-errors can be computed directly\nwhile interacting with the environment, TD learning algorithms turn out to be a much faster and better\nperforming alternative, when compared to other RL approaches such as Monte Carlo (MC) learning.\nIn fact, the latter methods require the RL episodes to end before being able to estimate how to predict\nfuture rewards.\nThe recent successful marriage between the most popular RL algorithms, and the use of Deep Neural\nNetworks [16], has highlighted the power of TD-learning methods. Despite having to deal with the\ninstabilities of a non-linear function approximator, such algorithms have led to remarkable results,\nwhich have made Deep Reinforcement Learning (DRL) one of the most popular ﬁelds within Machine\nLearning. Temporal-difference learning algorithms have been shown to be very efﬁcient, both when\napplied on learning an action-value function [10, 21, 22, 31, 36], and when used in combination with\npolicy gradient methods [6, 17, 20].\nIn this work we exploit the powerful capabilities of TD learning by extending a Tabular-RL algorithm,\ncalled QV(λ) learning [34], so that its update rules can be successfully used in combination with\nPreprint. Work in progress.\narXiv:1810.00368v2  [stat.ML]  10 Oct 2018\nDeep Artiﬁcial Neural Networks. Furthermore, we take advantage of the most popular strategies\nwhich are known to ensure stability when training DRL algorithms, in order to construct our novel\nDRL algorithm called Deep Quality-Value learning.\nThe structure of this paper is as follows: in Section 2 we present the most important theoretical\nconcepts of the ﬁeld of RL, which serve as preliminary knowledge for our novel algorithm presented\nin Section 3. We then present the methodological details about the experimental setup of this work in\nSection 4 and the results that have been obtained in Section 5. An analysis of the performances of our\nalgorithm, together with a discussion about the relevance of our work, concludes the paper in Section\n6.\n2\nPreliminaries\nReinforcement Learning (RL) is the branch of Machine Learning in which artiﬁcial agents have to\nlearn an optimal behavior while interacting with a particular environment [27]. An RL problem can\nbe formalized as a Markov Decision Process (MDP) consisting of a set of possible states, S , and a set\nof possible actions A . By choosing an action a ∈A , the agent performs a state transition from state\nst at time t to st+1 that is deﬁned by a transition probability distribution p(st+1|st,at). Associated\nto this transition probability there is an immediate reward function, ℜ(st,at,st+1), that speciﬁes the\nreward rt of being in state st+1 based on the action at that the agent has taken in state st. The agent\nchooses which actions to perform based on its policy, which maps every state to a particular action\nπ : s →a. If a policy is followed, it is possible to compute for each state its Value (V):\nV π(s) = E\n\"\n∞\n∑\nk=0\nγkrt+k\n\f\f\f\f\fst = s\n#\n,\n(1)\nwhich corresponds to the expected cumulative reward that an agent will gain while being in state s,\nand by following policy π. The discount factor γ is set between [0,1], when its value is close to 0 the\nagent will only consider the rewards that are obtained in an immediate future, while if its value is\nclose to 1, the agent will also aim to maximize the rewards obtained in a more distant future. Training\nan RL agent consists in ﬁnding an optimal policy, π∗, which maximizes the expected future reward\nfrom all possible states such that\nπ∗(s) = argmax\nπ\nV π(s).\n(2)\nIn addition to the V function, there is a second function, Qπ(s,a), which indicates how good or bad it\nis to select a particular action a in state s while following policy π. The goal is to ﬁnd a policy that\nmaximizes Q, which is known to correspond to satisfying the Bellman equation:\nQπ(st,at) = ∑\nst+1∈S\np(st+1|st,at)\n\u0012\nℜ(st,at,st+1)+γ max\nat+1∈A Qπ(st+1,at+1)\n\u0013\n.\n(3)\nOnce Q is learned, the optimal policy can be easily found by choosing the action that maximizes Q in\neach state.\nWhether the goal is to learn the V or the Q function, Tabular based RL aims to formulate the different\nRL update rules similar to Dynamic Programming algorithms, which makes it possible to exploit the\nMarkov properties of such rules. However, it is well known that this approach is computationally very\ndemanding and is only suitable for problems which have a limited state-action space. In order to deal\nwith more complicated RL problems non parametric function approximators can be used [5]. While\nseveral types of regression algorithms can be used to do so, such as Support Vector Machines [19] or\nTree-based approaches [7], a large popularity has been gained with the use of Deep Artiﬁcial Neural\nNetworks. The combination of RL and Deep Learning has recently made it possible to master RL\nproblems of the most various domains, ranging from videogames [8, 15, 20, 21, 32], to boardgames\n[14, 23, 24] and complex robotics control problems [1, 9, 17].\n2\n3\nDeep Quality-Value (DQV) Learning\nWe now introduce our novel DRL algorithm called Deep Quality-Value (DQV) learning. We ﬁrst\npresent the online RL algorithm QV(λ) together with its update rules, and then extend these update\nrules to objective functions that can be used to train Artiﬁcial Neural Networks (ANNs) for solving\ncomplex RL problems.\n3.1\nQV(λ) Learning\nQV(λ) has been presented in [34] and essentially consists of a value function RL algorithm that keeps\ntrack of both the Q function and the V function. Its main idea consists of learning the V function\nthrough the use of temporal difference TD(λ) learning [25], and using these estimates to learn the Q\nfunction with an update rule similar to one step Q-Learning [33]. The main beneﬁt of this approach\nis that the V function might converge faster than the Q function, since it is not directly dependent\non the actions that are taken by the RL agent. After a transition,⟨st, at, rt, st+1 ⟩, QV(λ) uses the\nTD(λ) learning rule [25] to update the V function for all states:\nV(s) := V(s)+α\n\u0002\nrt +γV(st+1)−V(st)\n\u0003\net(s),\n(4)\nwhere α stands for the learning rate and γ is again the discount factor. It is worth noting how the V\nfunction is updated according to a similar learning rule as standard Q-Learning [33] to update the Q\nfunction:\nQ(st,at) := Q(st,at)+α\n\u0002\nrt +γ max\nat+1∈A Q(st+1,at+1)−Q(st,at)\n\u0003\n.\n(5)\nBesides this TD update, QV(λ) also makes use of eligibility traces, deﬁned as et(s), that are necessary\nto keep track if a particular state has occurred before a certain time-step or not. These are updated for\nall states as follows:\net(s) = γλet−1(s)+ηt(s),\n(6)\nwhere ηt(s) is an indicator function that returns a value of 1 whether a particular state occurred at\ntime t and 0 otherwise. Before updating the V function, QV(λ) updates the Q function ﬁrst, and does\nthis via the following update rule:\nQ(st,at) := Q(st,at)+α\n\u0002\nrt +γV(st+1)−Q(st,at)\n\u0003\n.\n(7)\nIn [34] it is shown that QV(λ) outperforms different ofﬂine and online RL algorithms in Sutton’s Dyna\nmaze environment. However, this algorithm has so far only been used with tabular representations\nand Shallow Neural Networks [35] and never in combination with Deep Artiﬁcial Neural Networks.\nThus we now present its extension: Deep QV Learning. We show how to transform the update rules 4\nand 7 as objective functions to train ANNs, and how to make the training procedure stable with the\nuse of ‘Experience Replay’ [18] and ‘Target Networks’ [21].\n3.2\nDQV Learning\nSince the aim is to approximate both the V and the Q function, we train two ANNs with two distinct\nobjective functions. We thus deﬁne the parametrized V neural network as Φ and the Q neural network\nas θ. In order to build the two objective functions it is possible to simply express QV-Learning’s\nupdate rules in Mean Squared Error terms similarly to how DQN addresses the Q-Learning update\nrule 5:\nLθ = E\n\u0002\n(rt +γ max\nat+1∈A Q(st+1,at+1,θ)−Q(st,at,θ))2\u0003\n.\n(8)\n3\nHence we obtain the following objective function when aiming to train the V function:\nLΦ = E\n\u0002\n(rt +γV(st+1,Φ)−V(st,Φ))2\u0003\n,\n(9)\nwhile the following one can be used to train the Q function:\nLθ = E\n\u0002\n(rt +γV(st+1,Φ)−Q(st,at,θ))2\u0003\n.\n(10)\nBy expressing the update rules 4 and 7 as such it becomes possible to minimize the just presented\nobjective functions by gradient descent.\nQV-Learning is technically an online reinforcement learning algorithm since it assumes that its update\nrules get computed each time an agent has performed an action and has observed its relative reward.\nHowever, when it comes to more complex control problems (like the games of the Arcade Learning\nEnvironment, ALE, presented in [21]), training a Deep Convolutional Neural Network (DCNN) in an\nonline setting is computationally not suitable. This would in fact make each experience usable for\ntraining exactly one time, and as a result, a very large set of experiences will have to be collected to\nproperly train the ANNs. To overcome this issue it is possible to make use of ‘Experience Replay’\n[17, 21, 36], a technique which allows to learn from past episodes multiple times and that has proven\nto be extremely beneﬁcial when tackling RL problems with DCNNs.\nExperience Replay essentially consists of a memory buffer, D, of size N, in which experiences are\nstored in the form ⟨st, at, rt, st+1 ⟩. Once this memory buffer is ﬁlled with a large set of these\nquadruples, N , it becomes possible to randomly sample batches of such experiences for training the\nmodel. By doing so the RL agent can learn from previous experiences multiple times and does not\nhave to rely only on the current ⟨st, at, rt, st+1 ⟩quadruple for training. In our experiments we use\nan Experience Replay buffer that stores the most recent 400,000 frames that come from the ALE, we\nuniformly sample batches of 32 experiences ⟨st, at, rt, st+1 ⟩∼U(D), and use them for optimizing\nthe loss functions 9 and 10 each time the agent has performed an action. Training starts as soon as\n50,000 frames have been collected in the buffer.\nBesides making it possible to exploit past experiences multiple times, training from Experience Replay\nis also known for improving the stability of the training procedure. In fact, since the trajectories that\nare used for optimizing the networks get randomly sampled from the memory buffer, this makes the\nsamples used for training much less correlated to each other, which yields more robust training. A\nsecond idea that serves the same purpose when it comes to TD algorithms has been proposed in [21],\nand is known as the ‘Target Neural Network’.\nTarget Neural Network: it consists of a separate ANN that is speciﬁcally designed for estimating\nthe targets (yt) that are necessary for computing the TD errors. This ANN has the exact same structure\nas the one which is getting optimized, but its weights do not change each time RL experiences are\nsampled from the Experience Replay buffer to train the value function. On the contrary, the weights\nof the target network are temporally frozen, and only periodically get updated with the parameters of\nthe main network. In the classic DQN scenario the target network is parametrized as θ −, while the\nDeep Q-Network uses θ, leading to a slight modiﬁcation of the loss function presented in Equation 8:\nLθ = E\n\u0002\n(rt +γ max\nat+1∈A Q(st+1,at+1,θ −)−Q(st,at,θ))2\u0003\n.\n(11)\nSince DQV also computes TD errors we include a speciﬁcally designed Value-Target-Network to our\nalgorithm, which, similarly to what happens in DQN, slightly modiﬁes the original loss functions\npresented in Equations 9 and 10 to:\nLΦ = E\n\u0002\n(rt +γV(st+1,Φ−)−V(st,Φ))2\u0003\n(12)\nand\nLθ = E\n\u0002\n(rt +γV(st+1,Φ−)−Q(st,at,θ))2\u0003\n.\n(13)\n4\nFor our experiments we update the Value-Target-Network Φ−with the weights of our original Value\nNetwork Φ every 10,000 actions as deﬁned by the hyperparameter c.\nWe have now deﬁned all elements of our novel DRL algorithm which is summarized by the pseu-\ndocode presented in Algorithm 1.\nAlgorithm 1 DQV Learning\nRequire: Experience Replay Stack D with size N\nRequire: Q network with parameters θ\nRequire: V networks with parameters Φ and Φ−\nRequire: total_a = 0\nRequire: total_e = 0\nRequire: c = 10000\nRequire: N = 50000\n1: for e ∈EPISODES do\n2:\nwhile e ¬ over do\n3:\nobserve st\n4:\nselect at ∈A for st with policy π\n5:\nget rt and st+1\n6:\nstore e as ⟨st, at, rt, st+1 ⟩in D\n7:\ntotal_e += 1\n8:\nif total_e ≧N then\n9:\nsample minibatch of 32 e from D\n10:\nif st+1 ∈e is over then\n11:\nyt := rt\n12:\nelse\n13:\nyt := rt +γV(st+1,Φ−)\n14:\nend if\n15:\nθ := argmin\nθ\nE[(yt −Q(st,at,θ))2]\n16:\nΦ := argmin\nΦ\nE[(yt −V(st,Φ))2]\n17:\ntotal_a += 1\n18:\nif total_a = c then\n19:\nΦ−:= Φ\n20:\ntotal_a := 0\n21:\nend if\n22:\nend if\n23:\nend while\n24: end for\n4\nExperimental Setup\nFor our experiments we use two groups of environments that are both provided by the Open-AI Gym\npackage [4]. The ﬁrst group is intended to explore DQV’s performance on a set of RL tasks that do\nnot rely on computationally expensive GPU support and make it possible to quickly ﬁne tune all the\nnecessary hyperparameters of the algorithm. It consists of two environments that simulate two classic\ncontrol problems that are well known in RL literature: Acrobot [26] and Cartpole [3]. The second\ngroup of environments is more challenging and consists of several Atari 2600 games. In particular we\ninvestigate DQV’s performances on the games Pong, Enduro, Boxing and Ice-Hockey. A visualization\nof such environments can be seen in Figure 1. In all the experiments we compare the performance of\nDQV with two other well known self implemented TD DRL algorithms: DQN [21] and DDQN [31].\nWe use a two hidden layer Multilayer Perceptron (MLP) activated by a ReLU non linearity (f(x) =\nmax(0,x)) when we test the DRL algorithms on the ﬁrst two environments. While a three hidden\nlayer DCNN, followed by a fully connected layer as originally presented in [21], when tackling\nthe Atari 2600 games. For the latter set of environments we use the Deterministic-v4 versions\nof the games, which, as proposed in [21], use ‘Frame-Skipping’, a design choice which lets the\n5\nagent select an action every 4th frame instead of every single one. Furthermore, we use the standard\nAtari-preprocessing scheme in order to resize each frame to an 84×84 gray-scaled matrix.\nThe Adam optimizer [13] has been used for optimizing the MLPs while the RMSprop optimizer [29]\nhas been used for the DCNNs. As DQV’s exploration strategy we use the well known epsilon-greedy\napproach with an initial ε value set to 0.5 which gets linearly decreased to 0.1. The discount factor γ\nis set to 0.99. Lastly, since the scale of the rewards changes from game to game, we ‘clip’ all the\nrewards to the [−1,1] range.\nFigure 1: A visualization of the six RL environments that have been used in our experiments as\nprovided by the Open-AI Gym package [4]. From left to right: Acrobot, Cartpole, Pong, Enduro,\nBoxing, Ice-Hockey.\n5\nResults\nWe ﬁrst evaluate the performance that DQV has obtained on the Acrobot and Cartpole environments\nwhere we have used MLPs as function approximators. Given the relative simplicity of these problems\nwe did not integrate DQV with the Value-Target-Network. In fact, we have only optimized the two\ndifferent MLPs according to the objective functions 9 and 10 presented in Section 3.2. Regarding the\nExperience Replay buffer we did not include it in the Acrobot environment while it is used in the\nCartpole one. However its size is far smaller than the one which has been presented in Algorithm 1,\nin fact, since the problem tackled is much simpler, we only store 200 experiences which get randomly\nsampled with a batch size of 16.\nWe then present the results obtained on the Atari 2600 games, where instead of MLPs, we have\nused DCNNs and have integrated the algorithm with a larger Experience Replay buffer and the\nValue-Target-Network. The algorithm used is thus the one presented in Algorithm 1. We report on the\nx axis of all ﬁgures the amount of RL episodes that have been used to train the different algorithms,\nand on the y axis the cumulative reward that has been obtained by the agents in each episode. All\nresults report the average cumulative reward which has been obtained over 5 different simulations\nwith 5 different random seeds. Please note that we smoothed the reward function with a ‘Savgol\nFilter’ to improve the interpretability of the plots.\nThe results obtained on Acrobot and Cartpole show how promising the update rules 9 and 10 can\nbe when used in combination with an MLP. In fact as shown in Figure 2 it is possible to see that\nDQV is able to signiﬁcantly outperform DQN and DDQN in both environments. It is however worth\nnoting that the problems which have been tackled in this set of experiments are not particularly\ncomplex. Hence, in order to concretely establish the performance of our novel algorithm, the Atari\n2600 games serve as an ideal testbed. The results presented in Figure 3 show how DQV is again able\nto systematically outperform DQN and DDQN in all the experiments we have performed. When DQV\nis tested on the games Pong and Boxing it can be seen that it learns signiﬁcantly faster when compared\nto the other two algorithms. This is particularly true for the game Pong which gets solved in less\nthan 400 episodes, making DQV more than two times faster than DQN and DDQN. Similar results\nhave been obtained on the Boxing environment, where DQV approaches the maximum possible\ncumulative reward in the game (≈80) almost 300 episodes before the other two algorithms. On\nthe Enduro and Ice-Hockey games we can see that DQV does not only learn faster, but also obtains\nmuch higher cumulative rewards during the game. These rewards are almost twice as high in the\nEnduro environment, while the results also show that DQV is the only DRL algorithm which is able\nto optimize the policy over 800 episodes when it is tested on the Ice-Hockey game. The latter results\nare particularly interesting since they correspond to the ﬁrst and only case so far, in which we have\nobserved that DQN and DDQN are not able to improve the policy during the time that is required by\nDQV to do so.\n6\nFigure 2: The results that have been obtained when using DQV in combination with an MLP on two\nclassic RL problems (Acrobot and Cartpole). DQV learns signiﬁcantly faster when compared with\nDQN and DDQN.\nFigure 3: The results which have been obtained when using DQV in combination with Deep\nConvolutional Neural Networks, Experience Replay and Target Neural Networks on four Atari games.\nIt can be seen that DQV learns signiﬁcantly faster on the games Pong and Boxing while it also yields\nbetter results on the games Enduro and Ice-Hockey.\n7\n6\nDiscussion and Conclusion\nBesides introducing a novel DRL algorithm, we believe that the methodologies and results proposed\nin this paper contribute to the ﬁeld of DRL in several important ways. We showed how it is possible to\nbuild upon existing Tabular-RL work, in order to successfully extend the set of DRL algorithms which\nare currently present in the ﬁeld. So far, the results obtained by DQV-Learning are very promising and\ndeﬁnitely suggest that our algorithm is a better alternative if compared to the well known DQN and\nDDQN algorithms. Furthermore, DQV also reafﬁrms the beneﬁts of TD-Learning. More speciﬁcally,\nit is particularly interesting to see how effective the use of the V neural networks is, when learning\nan action-value function with the Q network. When QV(λ) has been initially introduced, one of the\nmain intuitions behind the algorithm relied on the fact that the V function could converge faster than\nthe Q function, which can make the algorithm learn faster. The results obtained in this paper support\nthis idea, with DQV being the fastest algorithm in all the experiments that we have performed.\nMoreover, we also noticed how important it is to integrate a DRL algorithm with a speciﬁcally\ndesigned Target-Network that is explicitly built for estimating the target values while learning. It\nis worth noting however, that unlike DQN and DDQN, this Target-Network is required for the V\nfunction and not the Q function, because the V function is used to update both value functions. Lastly,\nwe also remark the importance of including an Experience-Replay buffer in the algorithm when\ntackling complex RL problems as the ones provided by the Arcade Learning Environment. We aim to\nresearch more in detail the role of the Target-Network. In fact, there is to the best of our knowledge\nno real formal theory that motivates the need for this additional neural architecture. Nevertheless, it\nis a design choice which is well known to be useful in the ﬁeld. With DQV there is a new algorithm\nthat makes use of it, and which could help for gaining a better understanding of this additional neural\narchitecture.\nThe main strength of DQV certainly relies on the speed that our algorithm obtains for learning.\nHowever, we are aware that more sophisticated approaches such as the Asynchronous-Advantage-\nActor-Critic (A3C) algorithm have already been proposed in the ﬁeld for dealing with long RL\ntraining times. As future work we want to work on a multi-threaded version of DQV, and investigate\nif, in case its update rules are used for training multiple agents in parallel, our algorithm could\nperform better and faster than A3C. Furthermore, by estimating both the V function and the Q\nfunction, DQV-Learning allows for a novel method to estimate the Advantage function, and we want\nto study if this can help algorithms such as A3C to perform better. Finally, we want to take into\naccount the several contributions that have been proposed in the literature for making DQN more data\nefﬁcient and faster [12]. We will perform a similar ablation study on DQV, and investigate whether\nsuch improvements will be as successful when applied to our algorithm and result in state-of-the-art\nperformance on many different Atari 2600 games.\nAcknowledgements\nMatthia Sabatelli acknowledges the ﬁnancial support of BELSPO, Federal Public Planning Service\nScience Policy, Belgium, in the context of the BRAIN-be project.\nReferences\n[1] Pieter Abbeel, Adam Coates, Morgan Quigley, and Andrew Y Ng. An application of rein-\nforcement learning to aerobatic helicopter ﬂight. In Advances in neural information processing\nsystems, pages 1–8, 2007.\n[2] Artemij Amiranashvili, Alexey Dosovitskiy, Vladlen Koltun, and Thomas Brox. TD or not\nTD: Analyzing the role of temporal differencing in deep reinforcement learning. arXiv preprint\narXiv:1806.01175, 2018.\n[3] Andrew G Barto, Richard S Sutton, and Charles W Anderson. Neuronlike adaptive elements\nthat can solve difﬁcult learning control problems. IEEE transactions on systems, man, and\ncybernetics, (5):834–846, 1983.\n[4] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,\nand Wojciech Zaremba. OpenAI gym. arXiv preprint arXiv:1606.01540, 2016.\n8\n[5] Lucian Busoniu, Robert Babuska, Bart De Schutter, and Damien Ernst. Reinforcement learning\nand dynamic programming using function approximators. CRC press, 2010.\n[6] Dotan D Castro, Dmitry Volkinshtein, and Ron Meir. Temporal difference based actor critic\nlearning-convergence and neural implementation. In Advances in neural information processing\nsystems, pages 385–392, 2009.\n[7] Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement\nlearning. Journal of Machine Learning Research, 6(Apr):503–556, 2005.\n[8] Jakob Foerster, Nantas Nardelli, Gregory Farquhar, Triantafyllos Afouras, Philip HS Torr,\nPushmeet Kohli, and Shimon Whiteson. Stabilising experience replay for deep multi-agent\nreinforcement learning. arXiv preprint arXiv:1702.08887, 2017.\n[9] Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning\nfor robotic manipulation with asynchronous off-policy updates. In Robotics and Automation\n(ICRA), 2017 IEEE International Conference on, pages 3389–3396. IEEE, 2017.\n[10] Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous deep Q-\nlearning with model-based acceleration. In International Conference on Machine Learning,\npages 2829–2838, 2016.\n[11] Hado van Hasselt. Double Q-learning. In Advances in Neural Information Processing Systems,\npages 2613–2621, 2010.\n[12] Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dab-\nney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining\nimprovements in deep reinforcement learning. arXiv preprint arXiv:1710.02298, 2017.\n[13] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n[14] Matthew Lai. Giraffe: Using deep reinforcement learning to play chess. arXiv preprint\narXiv:1509.01549, 2015.\n[15] Guillaume Lample and Devendra Singh Chaplot. Playing FPS games with deep reinforcement\nlearning. In AAAI, pages 2140–2146, 2017.\n[16] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436,\n2015.\n[17] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,\nDavid Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv\npreprint arXiv:1509.02971, 2015.\n[18] Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and\nteaching. Machine learning, 8(3-4):293–321, 1992.\n[19] Mario Martin. On-line support vector machine regression. In European Conference on Machine\nLearning, pages 282–294. Springer, 2002.\n[20] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap,\nTim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep rein-\nforcement learning. In International Conference on Machine Learning, pages 1928–1937,\n2016.\n[21] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G\nBellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.\nHuman-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.\n[22] Vitchyr Pong, Shixiang Gu, Murtaza Dalal, and Sergey Levine. Temporal difference models:\nModel-free deep RL for model-based control. arXiv preprint arXiv:1802.09081, 2018.\n9\n[23] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess-\nche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al.\nMastering the game of go with deep neural networks and tree search. Nature, 529(7587):484,\n2016.\n[24] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur\nGuez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering\nchess and shogi by self-play with a general reinforcement learning algorithm. arXiv preprint\narXiv:1712.01815, 2017.\n[25] Richard S Sutton. Learning to predict by the methods of temporal differences. Machine learning,\n3(1):9–44, 1988.\n[26] Richard S Sutton. Generalization in reinforcement learning: Successful examples using sparse\ncoarse coding. In Advances in neural information processing systems, pages 1038–1044, 1996.\n[27] Richard S Sutton and Andrew G Barto. Introduction to reinforcement learning, volume 135.\nMIT press Cambridge, 1998.\n[28] Richard S Sutton, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba\nSzepesvári, and Eric Wiewiora. Fast gradient-descent methods for temporal-difference learning\nwith linear function approximation. In Proceedings of the 26th Annual International Conference\non Machine Learning, pages 993–1000. ACM, 2009.\n[29] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running\naverage of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26–\n31, 2012.\n[30] John N Tsitsiklis and Benjamin Van Roy. Analysis of temporal-diffference learning with function\napproximation. In Advances in neural information processing systems, pages 1075–1081, 1997.\n[31] Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double\nQ-learning. In AAAI, volume 16, pages 2094–2100, 2016.\n[32] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando\nDe Freitas. Dueling network architectures for deep reinforcement learning. arXiv preprint\narXiv:1511.06581, 2015.\n[33] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279–292,\n1992.\n[34] Marco A Wiering. QV (lambda)-learning: A new on-policy reinforcement learning algrithm. In\nProceedings of the 7th European Workshop on Reinforcement Learning, pages 17–18, 2005.\n[35] Marco A Wiering and Hado van Hasselt. The QV family compared to other reinforcement\nlearning algorithms. In Adaptive Dynamic Programming and Reinforcement Learning, 2009.\nADPRL’09. IEEE Symposium on, pages 101–108. IEEE, 2009.\n[36] Dongbin Zhao, Haitao Wang, Kun Shao, and Yuanheng Zhu. Deep reinforcement learning with\nexperience replay based on Sarsa. In Computational Intelligence (SSCI), 2016 IEEE Symposium\nSeries on, pages 1–6. IEEE, 2016.\n10\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2018-09-30",
  "updated": "2018-10-10"
}