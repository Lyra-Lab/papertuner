{
  "id": "http://arxiv.org/abs/1810.01112v1",
  "title": "The Dreaming Variational Autoencoder for Reinforcement Learning Environments",
  "authors": [
    "Per-Arne Andersen",
    "Morten Goodwin",
    "Ole-Christoffer Granmo"
  ],
  "abstract": "Reinforcement learning has shown great potential in generalizing over raw\nsensory data using only a single neural network for value optimization. There\nare several challenges in the current state-of-the-art reinforcement learning\nalgorithms that prevent them from converging towards the global optima. It is\nlikely that the solution to these problems lies in short- and long-term\nplanning, exploration and memory management for reinforcement learning\nalgorithms. Games are often used to benchmark reinforcement learning algorithms\nas they provide a flexible, reproducible, and easy to control environment.\nRegardless, few games feature a state-space where results in exploration,\nmemory, and planning are easily perceived. This paper presents The Dreaming\nVariational Autoencoder (DVAE), a neural network based generative modeling\narchitecture for exploration in environments with sparse feedback. We further\npresent Deep Maze, a novel and flexible maze engine that challenges DVAE in\npartial and fully-observable state-spaces, long-horizon tasks, and\ndeterministic and stochastic problems. We show initial findings and encourage\nfurther work in reinforcement learning driven by generative exploration.",
  "text": "The Dreaming Variational Autoencoder for\nReinforcement Learning Environments\nPer-Arne Andersen(\f), Morten Goodwin, and Ole-Christoﬀer Granmo\nDepartment of ICT, University of Agder, Grimstad, Norway\n{per.andersen,morten.goodwin,ole.granmo}@uia.no\nAbstract. Reinforcement learning has shown great potential in gener-\nalizing over raw sensory data using only a single neural network for value\noptimization. There are several challenges in the current state-of-the-art\nreinforcement learning algorithms that prevent them from converging to-\nwards the global optima. It is likely that the solution to these problems\nlies in short- and long-term planning, exploration and memory manage-\nment for reinforcement learning algorithms. Games are often used to\nbenchmark reinforcement learning algorithms as they provide a ﬂexible,\nreproducible, and easy to control environment. Regardless, few games\nfeature a state-space where results in exploration, memory, and plan-\nning are easily perceived. This paper presents The Dreaming Variational\nAutoencoder (DVAE), a neural network based generative modeling archi-\ntecture for exploration in environments with sparse feedback. We further\npresent Deep Maze, a novel and ﬂexible maze engine that challenges\nDVAE in partial and fully-observable state-spaces, long-horizon tasks,\nand deterministic and stochastic problems. We show initial ﬁndings and\nencourage further work in reinforcement learning driven by generative\nexploration.\nKeywords: Deep Reinforcement Learning · Environment Modeling ·\nNeural Networks · Variational Autoencoder · Markov Decision Processes\n· Exploration · Artiﬁcial Experience-Replay\n1\nIntroduction\nReinforcement learning (RL) is a ﬁeld of research that has quickly become one\nof the most promising branches of machine learning algorithms to solve artiﬁcial\ngeneral intelligence [2,10,12,16]. There have been several breakthroughs in rein-\nforcement learning in recent years for relatively simple environments [6,14,15,21],\nbut no algorithms are capable of human performance in situations where com-\nplex policies must be learned. Due to this, a number of open research questions\nremain in reinforcement learning. It is possible that many of the problems can\nbe resolved with algorithms that adequately accounts for planning, exploration,\nand memory at diﬀerent time-horizons.\nIn current state-of-the-art RL algorithms, long-horizon RL tasks are diﬃcult\nto master because there is as of yet no optimal exploration algorithm that is\narXiv:1810.01112v1  [cs.LG]  2 Oct 2018\ncapable of proper state-space pruning. Exploration strategies such as ϵ-greedy is\nwidely used in RL, but cannot ﬁnd an adequate exploration/exploitation balance\nwithout signiﬁcant hyperparameter-tuning. Environment modeling is a promis-\ning exploration technique where the goal is for the model to imitate the behavior\nof the target environment. This limits the required interaction with the target\nenvironment, enabling nearly unlimited access to exploration without the cost\nof exhausting the target environment. In addition to environment-modeling, a\nbalance between exploration and exploitation must be accounted for, and it is,\ntherefore, essential for the environment model to receive feedback from the RL\nagent.\nBy combining the ideas of variational autoencoders with deep RL agents, we\nﬁnd that it is possible for agents to learn optimal policies using only generated\ntraining data samples. The approach is presented as the dreaming variational\nautoencoder. We also show a new learning environment, Deep Maze, that aims\nto bring a vast set of challenges for reinforcement learning algorithms and is the\nenvironment used for testing the DVAE algorithm.\nThis paper is organized as follows. Section 3 brieﬂy introduces the reader\nto preliminaries. Section 4 proposes The Dreaming Variational Autoencoder for\nenvironment modeling to improve exploration in RL. Section 5 introduces the\nDeep Maze learning environment for exploration, planning and memory man-\nagement research for reinforcement learning. Section 6 shows results in the Deep\nLine Wars environment and that RL agents can be trained to navigate through\nthe deep maze environment using only artiﬁcial training data.\n2\nRelated Work\nIn machine learning, the goal is to create an algorithm that is capable of con-\nstructing a model of some environment accurately. There is, however, little re-\nsearch in game environment modeling in the scale we propose in this paper. The\nprimary focus of recent RL research has been on the value and policy aspect\nof RL algorithm, while less attention has been put into perfecting environment\nmodeling methods.\nIn 2016, the work in [3] proposed a method of deducing the Markov Decision\nProcess (MDP) by introducing an adaptive exploration signal (pseudo-reward),\nwhich was obtained using deep generative model. Their method was to compute\nthe Jacobian of each state and used it as the pseudo-reward when using deep\nneural networks to learn the state-generalization.\nXiao et al. proposed in [22] the use of generative adversarial networks (GAN)\nfor model-based reinforcement learning. The goal was to utilize GAN for learning\ndynamics of the environment in a short-horizon timespan and combine this with\nthe strength of far-horizon value iteration RL algorithms. The GAN architecture\nproposed illustrated near authentic generated images giving comparable results\nto [14].\nIn [9] Higgins et al. proposed DARLA, an architecture for modeling the\nenvironment using β-VAE [8]. The trained model was used to extract the optimal\npolicy of the environment using algorithms such as DQN [15], A3C [13], and\nEpisodic Control [4]. DARLA is to the best of our knowledge, the ﬁrst algorithm\nto properly introduce learning without access to the target environment during\ntraining.\nBuesing et al. recently compared several methods of environment modeling,\nshowing that it is far better to model the state-space then to utilize Monte-\nCarlo rollouts (RAR). The proposed architecture, state-space models (SSM) was\nsigniﬁcantly faster and produced acceptable results compared to auto-regressive\n(AR) methods. [5]\nHa and Schmidhuber proposed in [7] World Models, a novel architecture for\ntraining RL algorithms using variational autoencoders. This paper showed that\nagents could successfully learn the environment dynamics and use this as an\nexploration technique requiring no interaction with the target domain.\n3\nBackground\nWe base our work on the well-established theory of reinforcement learning and\nformulate the problem as a MDP [20]. An MDP contains (S, A, T , r) pairs that\ndeﬁne the environment as a model. The state-space, S represents all possible\nstates while the action-space, A represents all available actions the agent can\nperform in the environment. T denotes the transition function (T : S ×A →S),\nwhich is a mapping from state st ∈S and action at ∈A to the future state\nst+1. After each performed action, the environment dispatches a reward signal,\nR : S →r.\nWe call a sequence of states and actions a trajectory denoted as\nτ = (s0, a0, . . . , st, at) and the sequence is sampled through the use of a stochas-\ntic policy that predicts the optimal action in any state: πθ(at|st), where π is\nthe policy and θ are the parameters. The primary goal of the reinforcement\nlearning is to reinforce good behavior. The algorithm should try to learn the\npolicy that maximizes the total expected discounted reward given by, J (π) =\nE(st,at)∼p(π)\n\u0002 PT\ni=0 γiR(si)\n\u0003\n[15].\n4\nThe Dreaming Variational Autoencoder\nThe Dreaming Variational Autoencoder (DVAE) is an end-to-end solution for\ngenerating probable future states ˆst+n from an arbitrary state-space S using\nstate-action pairs explored prior to st+n and at+n.\nThe DVAE algorithm, seen in Figure 1 works as follows. First, the agent\ncollects experiences for utilizing experience-replay in the Run-Agent function. At\nthis stage, the agent explores the state-space guided by a Gaussian distributed\npolicy. The agent acts, observes, and stores the observations into the experience-\nreplay buﬀer D. After the agent reaches terminal state, the DVAE algorithm\nencodes state-action pairs from the replay-buﬀer D into probable future states.\nThis is stored in the replay-buﬀer for artiﬁcial future-states ˆD.\nP(X|z)\nQ(z|X)\nzt\nst\nat\nŝt+1\nP(X|z)\nŝt+2\nP(X|z)\nŝt+3\nzt+1\nzt+2\nQ(z|X)\nŝt+1\nat+1\nQ(z|X)\nŝt+2\nat+2\nFig. 1: Illustration of the DVAE model. The model consumes state and action\npairs, yielding the input encoded in latent-space. Latent-space can then be de-\ncoded to a probable future state. Q(z|X) is the encoder, zt is latent-space, and\nP(X|z) is the decoder. DVAE can also use LSTM to better learn longer sequences\nin continuous state-spaces.\nTable 1: DVAE algorithm for generating states using Tθ versus the real transition\nfunction T . First, a real state is collected from the replay-memory. DVAE can\nthen produce new states from current the trajectory τ using the state-action\npairs. θ represent the trainable model parameters.\n1 0\n0 1\n0 0\nReal States\n0 0 T (s0, Aright)\n0 0 T (s1, Adown)\n0 1\ns0\ns1\ns2\n0 1\n0 0\nGenerated States N/A Tθ(s0, Aright, θ) 0 0 Tθ(ˆs1, Adown, θ) 0 1\nˆs1\nˆs2\nAlgorithm 1 The Dreaming Variational Autoencoder\n1: Initialize replay memory D and ∧\nD to capacity N\n2: Initialize policy πθ\n3: function Run-Agent(T , D)\n4:\nfor i = 0 to N EPISODES do\n5:\nObserve starting state, s0 ∼N(0, 1)\n6:\nwhile st not TERMINAL do\n7:\nat ←πθ(st = s)\n8:\nst+1, rt, terminalt ←T (st, at)\n9:\nStore experience into replay buﬀer D(st, at, rt, st+1, terminalt)\n10:\nst ←st+1\n11:\nend while\n12:\nend for\n13: end function\n14: Initialize encoder Q(z|X)\n15: Initialize decoder P(X|z)\n16: Initialize DVAE model Tθ = P(X|Q(z|X))\n17: function DVAE\n18:\nfor di in D do\n19:\nst, at, rt, st+1 ←di\n▷Expand replay buﬀer pair\n20:\nXt ←st, at\n21:\nzt ←Q(Xt)\n▷Encode Xt into latent-space\n22:\nˆst+1 ←P(zt)\n▷Decode zt into probable future state\n23:\nStore experience into artiﬁcial replay buﬀer ∧\nD(ˆst, at, rt, ˆst+1, terminalt)\n24:\nˆst = ˆst+1\n25:\nend for\n26:\nreturn ∧\nD\n27: end function\nTable 1 illustrates how the algorithm can generate sequences of artiﬁcial\ntrajectories using Tθ = P(X|Q(z|X)), where z = Q(z|X) is the encoder, and\nTθ = P(X|z) is the decoder. With state s0 and action Aright as input, the\nalgorithm generates state ˆs1 which in the table can be observed is similar to\nthe real state s1. With the next input, Adown, the DVAE algorithm generates\nthe next state ˆs2 which again can be observed to be equal to s2. Note that\nthis is without ever observing state s1. Hence, the DVAE algorithm needs to be\ninitiated with a state, e.g. s0, and actions follows. It then generates (dreams)\nnext states,\nThe requirement is that the environment must be partially discovered so\nthat the algorithm can learn to behave similarly to the target environment. To\npredict a trajectory of three timesteps, the algorithm does nesting to generate\nthe whole sequence: τ = ˆs1, a1, ˆs2, a2, ˆs3, a3 = Tθ(Tθ(Tθ(s0, Arnd), Arnd), Arnd).\nThe algorithm does this well in early on, but have diﬃculties with long sequences\nbeyond eight in continuous environments.\n5\nEnvironments\nThe DVAE algorithm was tested on two game environments. The ﬁrst environ-\nment is Deep Line Wars [1], a simpliﬁed Real-Time Strategy game. We introduce\nDeep Maze, a ﬂexible environment with a wide range of challenges suited for re-\ninforcement learning research.\n5.1\nThe Deep Maze Environment\nThe Deep Maze is a ﬂexible learning environment for controlled research in\nexploration, planning, and memory for reinforcement learning algorithms. Maze\nsolving is a well-known problem, and is used heavily throughout the RL literature\n[20], but is often limited to small and fully-observable scenarios. The Deep Maze\nenvironment extends the maze problem to over 540 unique scenarios including\nPartially-Observable Markov Decision Processes (POMDP). Figure 2 illustrates\na small subset of the available environments for Deep Maze, ranging from small-\nscale MDP’s to large-scale POMDP’s. The Deep Maze further features custom\ngame mechanics such as relocated exits and dynamically changing mazes.\nThe game engine is modularized and has an API that enables a ﬂexible\ntool set for third-party scenarios. This extends the capabilities of Deep Maze to\nsupport nearly all possible scenario combination in the realm of maze solving.1\nState Representation RL agents depend on sensory input to evaluate and\npredict the best action at current timestep. Preprocessing of data is essential so\nthat agents can extract features from the input. For this reason, Deep Maze has\nbuilt-in state representation for RGB Images, Grayscale Images, and raw state\nmatrices.\n1 The Deep Maze is open-source and publicly available at https://github.com/CAIR/\ndeep-maze.\n(a) A Small, Fully Observable MDP\n(b) A Large, Fully Observable MDP\n(c) Partially Observable MDP having a vi-\nsion distance of 3 tiles\n(d) Partially Observable MDP having ray-\ntraced vision\nFig. 2: Overview of four distinct MDP scenarios using Deep Maze.\nScenario Setup The Deep Maze learning environment ships with four scenario\nmodes: (1) Normal, (2) POMDP, (3) Limited POMDP, and (4) Timed Limited\nPOMDP.\nThe ﬁrst mode exposes a seed-based randomly generated maze where the\nstate-space is modeled as an MDP. The second mode narrows the state-space\nobservation to a conﬁgurable area around the player. In addition to radius based\nvision, the POMDP mode also features ray-tracing vision that better mimic the\nsight of a physical agent. The third and fourth mode is intended for memory\nresearch where the agent must ﬁnd the goal in a limited number of time-steps.\nIn addition to this, the agent is presented with the solution but fades after a few\ninitial time steps. The objective is the for the agent to remember the solution\nto ﬁnd the goal. All scenario setups have a variable map-size ranging between\n2 × 2 and 56 × 56 tiles.\n5.2\nThe Deep Line Wars Environment\nThe Deep Line Wars environment was ﬁrst introduced in [1]. Deep Line Wars is\na real-time strategy environment that makes an extensive state-space reduction\nto enable swift research in reinforcement learning for RTS games.\nFig. 3: The Graphical User Interface of the Deep Line Wars environment.\nThe game objective of Deep Line Wars is to invade the enemy player with\nmercenary units until all health points are depleted, see Figure 3). For every\nfriendly unit that enters the far edge of the enemy base, the enemy health pool\nis reduced by one. When a player purchases a mercenary unit, it spawns at\na random location inside the edge area of the buyers base. Mercenary units\nautomatically move towards the enemy base. To protect the base, players can\nconstruct towers that shoot projectiles at the opponents mercenaries. When a\nmercenary dies, a fair percentage of its gold value is awarded to the opponent.\nWhen a player sends a unit, the income is increased by a percentage of the units\ngold value. As a part of the income system, players gain gold at ﬁxed intervals.\n6\nExperiments\n6.1\nDeep Maze Environment Modeling using DVAE\nThe DVAE algorithm must be able to generalize over many similar states to\nmodel a vast state-space. DVAE aims to learn the transition function, bringing\nthe state from st to st+1 = T (st, at). We use the deep maze environment because\nit provides simple rules, with a controllable state-space complexity. Also, we can\nomit the importance of reward for some scenarios.\nWe trained the DVAE model on two No-Wall Deep Maze scenarios of size 2×2\nand 8×8. For the encoder and decoder, we used the same convolution architecture\nas proposed by [17] and trained for 5000 epochs for 8 × 8 and 1000 epochs for\n2 × 2 respectively. For the encoding of actions and states, we concatenated the\nﬂattened state-space and action-space, having a fully-connected layer with ReLU\nactivation before calculating the latent-space. We used the Adam optimizer [11]\nwith a learning-rate of 1e-08 to update the parameters.\n0\n200\n400\n600\n800\n1000\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\nLogarithmic Loss\nTraining Loss for DVAE in 2x2 No-Wall Deep-Maze \nEpisode\n(a) A Small, Fully Observable MDP\n0\n1000\n2000\n3000\n4000\n5000\n0\n1\n2\n3\n4\n5\nLogarithmic Loss\nTraining Loss for DVAE in 8x8 No-Wall Deep-Maze\nEpisode\n(b) A Large, Fully Observable MDP\nFig. 4: The training loss for DVAE in the 2 × 2 No-Wall and 8 × 8 Deep Maze\nscenario. The experiment is run for a total of 1000 (5000 for 8 × 8) episodes.\nThe algorithm only trains on 50% of the state-space to the model for the 2 × 2\nenvironment while the whole state-space is trainable in the 8 × 8 environment.\nFigure 4 illustrates the loss of the DVAE algorithm in the No-Wall Deep\nMaze scenario. In the 2 × 2 scenario, DVAE is trained on only 50% of the state\nspace, which results in noticeable graphic artifacts in the prediction of future\nstates, see Figure 5. Because the 8 × 8 environment is fully visible, we see in\nFigure 6 that the artifacts are exponentially reduced.\nŝt+1 \nŝt+2 \nŝt+3 \nŝt+4 \nŝt+5 \nŝt+6 \nŝt+7 \nŝt+8 \nRight\nDown\nLeft\nUp\nDown\nRight\nUp\nFig. 5: For the 2 × 2 scenario, only 50% of the environment is explored, leav-\ning artifacts on states where the model is uncertain of the transition function.\nIn more extensive examples, the player disappears, teleports or gets stuck in\nunexplored areas.\nTable 2: Results of the deep maze 11 × 11 and 21 × 21 environment, comparing\nDQN [15], TRPO [18], and PPO [19]. The optimal path yields performance of\n100% while no solution yields 0%. Each of the algorithms ran 10000 episodes\nfor both map-sizes. The last number represents at which episode the algorithm\nconverged.\nAlgorithm Avg Performance 11 × 11 Avg Performance 21 × 21\nDQN- ∧\nD\n94.56% @ 9314\n64.36% @ N/A\nTRPO- ∧\nD\n96.32% @ 5320\n78.91% @ 7401\nPPO- ∧\nD\n98.71% @ 3151\n89.33% @ 7195\nDQN-D\n98.26% @ 4314\n84.63% @ 8241\nTRPO-D\n99.32% @ 3320\n92.11% @ 4120\nPPO-D\n99.35% @ 2453\n96.41% @ 2904\n6.2\nUsing ∧\nD for RL Agents in Deep Maze\nThe goal of this experiment is to observe the performance of RL agents using\nthe generated experience-replay ∧\nD from Figure 1 in Deep Maze environments of\nsize 11 × 11 and 21 × 21. In Table 2, we compare the performance of DQN [14],\nTRPO [18], and PPO [19] using the DVAE generated ∧\nD to tune the parameters.\nFigure 7 illustrates three maze variations of size 11 × 11, where the AI has\nlearned the optimal path. We see that the best performing algorithm, PPO [19]\nbeats DQN and TRPO using either ∧\nD or D. The DQN-∧\nD agent did not converge\nin the 21 × 21 environment, but it is likely that value-based algorithms could\nstruggle with graphical artifacts generated from the DVAE algorithm. These\nartifacts signiﬁcantly increase the state-space so that direct-policy algorithms\ncould perform better.\nŝt+1 \nŝt+2 \nŝt+3 \nŝt+4 \nŝt+5 \nŝt+6 \nŝt+7 \nŝt+8 \nŝt+9 \nŝt+10 \nŝt+11 \nŝt+12 \nRight\nRight\nUp\nUp\nUp\nLeft\nLeft\nDown\nDown\nDown\nDown\nFig. 6: Results of 8 × 8 Deep Maze modeling using the DVAE algorithm. To\nsimplify the environment, no reward signal is received per iteration. The left\ncaption describes current state, st, while the right caption is the action performed\nto compute, st+1 = T (st, at).\nText\nFig. 7: A typical deep maze of size 11 × 11. The lower-right square indicates\nthe goal state, the dotted-line indicates the optimal path, while the ﬁnal square\nrepresents the player’s current position in the state-space. The controller agent\nis DQN, TRPO, and PPO (from left to right).\n6.3\nDeep Line Wars Environment Modeling using DVAE\nThe DVAE algorithm works well in more complex environments, such as the\nDeep Line Wars game environment [1]. Here, we expand the DVAE algorithm\nwith LSTM to improve the interpretation of animations, illustrated Figure 1.\nEpoch 50                          Epoch 1 000                     Epoch 1 500\nEpoch 2 000                    Epoch 2 500                     Epoch 3 000\nEpoch 3 500                    Epoch 4 000                      Epoch 4 500\nEpoch 5 000                     Epoch 5 500                     Epoch 6 000\nFig. 8: The DVAE algorithm applied to the Deep Line Wars environment. Each\nepoch illustrates the quality of generated states in the game, where the left image\nis real state s and the right image is the generated state ˆs.\nFigure 8 illustrates the state quality during training of DVAE in a total of\n6000 episodes (epochs). Both players draw actions from a Gaussian distributed\npolicy. The algorithm understands that the player units can be located in any\ntiles after only 50 epochs, and at 1000 we observe the algorithm makes a more\naccurate statement of the probability of unit locations (i.e., some units have\nincreased intensity). At the end of the training, the DVAE algorithm is to some\ndegree capable of determining both towers, and unit locations at any given time-\nstep during the game episode.\n7\nConclusion and Future Work\nThis paper introduces the Dreaming Variational Autoencoder (DVAE) as a neu-\nral network based generative modeling architecture to enable exploration in envi-\nronments with sparse feedback. The DVAE shows promising results in modeling\nsimple non-continuous environments. For continuous environments, such as Deep\nLine Wars, DVAE performs better using a recurrent neural network architecture\n(LSTM) while it is suﬃcient to use only a sequential feed-forward architecture\nto model non-continuous environments such as Chess, Go, and Deep Maze.\nThere are, however, several fundamental issues that limit DVAE from fully\nmodeling environments. In some situations, exploration may be a costly act that\nmakes it impossible to explore all parts of the environment in its entirety. DVAE\ncannot accurately predict the outcome of unexplored areas of the state-space,\nmaking the prediction blurry or false.\nReinforcement learning has many unresolved problems, and the hope is that\nthe Deep Maze learning environment can be a useful tool for future research.\nFor future work, we plan to expand the model to model the reward function\nˆR using inverse reinforcement learning. DVAE is an ongoing research question,\nand the goal is that reinforcement learning algorithms could utilize this form of\ndreaming to reduce the need for exploration in real environments.\nReferences\n1. Andersen, P.A., Goodwin, M., Granmo, O.C.: Towards a deep reinforcement learn-\ning approach for tower line wars. In: Bramer, M., Petridis, M. (eds.) Lecture Notes\nin Computer Science (including subseries Lecture Notes in Artiﬁcial Intelligence\nand Lecture Notes in Bioinformatics). vol. 10630 LNAI, pp. 101–114 (2017)\n2. Arulkumaran, K., Deisenroth, M.P., Brundage, M., Bharath, A.A.: Deep reinforce-\nment learning: A brief survey. IEEE Signal Processing Magazine 34(6), 26–38\n(2017)\n3. Bangaru, S.P., Suhas, J., Ravindran, B.: Exploration for Multi-task Reinforce-\nment Learning with Deep Generative Models. arxiv preprint arXiv:1611.09894 (nov\n2016)\n4. Blundell, C., Uria, B., Pritzel, A., Li, Y., Ruderman, A., Leibo, J.Z., Rae,\nJ., Wierstra, D., Hassabis, D.: Model-Free Episodic Control. arxiv preprint\narXiv:1606.04460 (jun 2016)\n5. Buesing, L., Weber, T., Racaniere, S., Eslami, S.M.A., Rezende, D., Reichert,\nD.P., Viola, F., Besse, F., Gregor, K., Hassabis, D., Wierstra, D.: Learning and\nQuerying Fast Generative Models for Reinforcement Learning. arxiv preprint\narXiv:1802.03006 (feb 2018)\n6. Chen, K.: Deep Reinforcement Learning for Flappy Bird. cs229.stanford.edu p. 6\n(2015)\n7. Ha, D., Schmidhuber, J.: World Models. arxiv preprint arXiv:1803.10122 (mar\n2018)\n8. Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mo-\nhamed, S., Lerchner, A.: beta-VAE: Learning Basic Visual Concepts with a Con-\nstrained Variational Framework. International Conference on Learning Represen-\ntations (nov 2016)\n9. Higgins, I., Pal, A., Rusu, A., Matthey, L., Burgess, C., Pritzel, A., Botvinick,\nM., Blundell, C., Lerchner, A.: DARLA: Improving Zero-Shot Transfer in Rein-\nforcement Learning. In: Precup, D., Teh, Y.W. (eds.) Proceedings of the 34th\nInternational Conference on Machine Learning. Proceedings of Machine Learning\nResearch, vol. 70, pp. 1480–1490. PMLR, International Convention Centre, Sydney,\nAustralia (2017)\n10. Kaelbling, L.P., Littman, M.L., Moore, A.W.: Reinforcement Learning: A Survey.\nJournal of Artiﬁcial Intelligence Research (apr 1996)\n11. Kingma, D.P., Ba, J.L.: Adam: A Method for Stochastic Optimization. Proceed-\nings, International Conference on Learning Representations 2015 (2015)\n12. Li,\nY.:\nDeep\nReinforcement\nLearning:\nAn\nOverview.\narxiv\npreprint\narXiv:1701.07274 (jan 2017)\n13. Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D.,\nKavukcuoglu, K.: Asynchronous Methods for Deep Reinforcement Learning. In:\nBalcan, M.F., Weinberger, K.Q. (eds.) Proceedings of The 33rd International Con-\nference on Machine Learning. Proceedings of Machine Learning Research, vol. 48,\npp. 1928–1937. PMLR, New York, New York, USA (2016)\n14. Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D.,\nRiedmiller, M.: Playing Atari with Deep Reinforcement Learning. Neural Informa-\ntion Processing Systems (dec 2013)\n15. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G.,\nGraves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G., Petersen, S., Beattie, C.,\nSadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., Hassabis,\nD.: Human-level control through deep reinforcement learning. Nature 518(7540),\n529–533 (feb 2015)\n16. Mousavi, S.S., Schukat, M., Howley, E.: Deep Reinforcement Learning: An\nOverview. In: Bi, Y., Kapoor, S., Bhatia, R. (eds.) Proceedings of SAI Intelli-\ngent Systems Conference (IntelliSys) 2016. pp. 426–440. Springer International\nPublishing, Cham (2018)\n17. Pu, Y., Gan, Z., Henao, R., Yuan, X., Li, C., Stevens, A., Carin, L.: Variational\nAutoencoder for Deep Learning of Images, Labels and Captions. In: Lee, D.D.,\nSugiyama, M., Luxburg, U.V., Guyon, I., R., G. (eds.) Advances in Neural Infor-\nmation Processing Systems. pp. 2352–2360. Curran Associates, Inc. (2016)\n18. Schulman, J., Levine, S., Abbeel, P., Jordan, M., Moritz, P.: Trust Region Pol-\nicy Optimization. In: Bach, F., Blei, D. (eds.) Proceedings of the 32nd Interna-\ntional Conference on Machine Learning. Proceedings of Machine Learning Re-\nsearch, vol. 37, pp. 1889–1897. PMLR, Lille, France (2015)\n19. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O.: Proximal Policy\nOptimization Algorithms. arxiv preprint arXiv:1707.06347 (jul 2017)\n20. Sutton, R.S., Barto, A.G.: Reinforcement Learning: An Introduction, vol. 9. MIT\nPress (1998)\n21. Van Seijen, H., Fatemi, M., Romoﬀ, J., Laroche, R., Barnes, T., Tsang, J.: Hybrid\nReward Architecture for Reinforcement Learning. In: Guyon, I., Luxburg, U.V.,\nBengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (eds.) Advances\nin Neural Information Processing Systems 30, pp. 5392–5402. Curran Associates,\nInc. (2017)\n22. Xiao, T., Kesineni, G.: Generative Adversarial Networks for Model Based Rein-\nforcement Learning with Tree Search. Tech. rep., University of California, Berkeley\n(2016)\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2018-10-02",
  "updated": "2018-10-02"
}