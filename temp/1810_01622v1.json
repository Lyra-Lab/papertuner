{
  "id": "http://arxiv.org/abs/1810.01622v1",
  "title": "Theory of Generative Deep Learning : Probe Landscape of Empirical Error via Norm Based Capacity Control",
  "authors": [
    "Wendi Xu",
    "Ming Zhang"
  ],
  "abstract": "Despite its remarkable empirical success as a highly competitive branch of\nartificial intelligence, deep learning is often blamed for its widely known low\ninterpretation and lack of firm and rigorous mathematical foundation. However,\nmost theoretical endeavor is devoted in discriminative deep learning case,\nwhose complementary part is generative deep learning. To the best of our\nknowledge, we firstly highlight landscape of empirical error in generative case\nto complete the full picture through exquisite design of image super resolution\nunder norm based capacity control. Our theoretical advance in interpretation of\nthe training dynamic is achieved from both mathematical and biological sides.",
  "text": "Proceedings of CCIS2018 \n \nTheory of Generative Deep Learning â…¡:Probe Landscape of \nEmpirical Error via Norm Based Capacity Control  \nWendi Xu1,3, Ming Zhang1,2 \n1Xinjiang Astronomical Observatories,Chinese Academy of Sciences, Urumqi 830011, China \n2Key Laboratory for Radio Astronomy, Chinese Academy of Sciences, Nanjing 210008, China \n3University of Chinese Academy of Sciences, Beijing 100876, China  \nxuwendi@xao.ac.cn, zhang.ming@xao.ac.cn \nAbstract: Despite its remarkable empirical success as a \nhighly competitive branch of  artificial intelligence, \ndeep learning is often blamed for its widely known low \ninterpretation \nand \nlack \nof \nfirm \nand \nrigorous \nmathematical foundation. However, most theoretical \nendeavor is devoted in discriminative deep learning case, \nwhose complementary part is generative deep learning. \nTo the best of our knowledge, we firstly highlight \nlandscape of empirical error in generative case to \ncomplete the full picture through exquisite design of \nimage super resolution under norm based capacity \ncontrol. Our theoretical advance in interpretation of the \ntraining dynamic is achieved from both mathematical \nand biological sides. \nKeywords: Deep learning; Statistical learning theory; \nImage super resolution; Capacity control; Regularization \ntechniques; Brain-inspired intelligence \n1  Introduction \n1.1 Discriminative V.S. generative deep learning: \nlandscapes of empirical error \nDespite its remarkable empirical success as a highly \ncompetitive branch of  artificial intelligence, deep \nlearning is often blamed for its widely known low \ninterpretation \nand \nlack \nof \nfirm \nand \nrigorous \nmathematical foundation. In [1] â€œTheory of deep \nlearning â…¡: landscape of the empirical risk in deep \nlearningâ€, authors from MIT explore the landscape of \nempirical error in deep learning via case study of a \n6-layer convolutional network (CNN) for image \nclassification to tackle the problem of theoretical puzzles. \nTheir works are constrained in discriminative deep \nlearning case.  \nHowever, the full picture of deep learning is made up of  \nboth discriminative and generative cases. The former \nparadigm works as follow: the input data is processed \ninto representations, then the representations will be sent \nto classifiers (sometimes a softmax layer), the pipelines \noutput the desired label. Image detection, image \nrecognition and image classification etc. fall in the \nregime. Whereas, the latter paradigm  performs as \nfollow: the later stages of generative case  involve \nrepresentations learned, then the representations are \ntransformed \nto \nâ€œgenerateâ€(where \nthe \nname \nof \nâ€œgenerativeâ€ deep learning comes from) new data. \nDecoders in machine translation and image caption \nâ€œgenerateâ€ the target language from  codes, decoders in \nimage denoising and image super resolution also \nâ€œgenerateâ€ the image from its representations. \nIn order to complete the  full picture, we make a step \ntowards the scarcely explored landscape of empirical \nerror in generative deep learning. Our theoretical and \nexperimental explorations are conveyed by an 8-layer \nCNN for image super resolution within early stages. \nBoth discriminative(authors from MIT) deep learning \nand generative(we) deep learning case are shown in \nTable I. \nTable I  Comparison of landscapes of empirical error \n \nDiscri. DL     Gener. DL \nNetwork \n6-layer CNN \n8-layer CNN \n \nTask(Image) \nClassification \nSuper Resolution \n \nLandscape \nEmpirical Error \n \n \n1.2 Regularization terms: working as norm \nbased  capacity control to probe landscape of  \nempirical error \nRegularization terms accompanying  the loss function \nwill modify the weights, thus control the parameter \nspace, control the model capacity (the number of \nparameters).  \nIn this article, we choose specified norm based \nregularization terms, which are widely known as L1 or \nL2 norm weight decay, working as capacity control to \nprobe landscape of empirical error in generative deep \nlearning case.  \n1.3 Related works: family of  regularization \ntechniques  \nIn practice, training big networks with regularization \ntechniques gives superior test performance to smaller \nnetworks trained without regularization. \nFamily of regularization techniques includes early \nstopping, \nstochastic \ngradient \ndescent \n(implicit \nregularization controlled by the number of iterations), \ndrop out in activation of fully connected layers, \nDroConnect [2]  in weights of fully connected layers \nand  regularization terms of norms like spectral norm, \nProceedings of CCIS2018 \n \ngroup norm, path norm, Fish-Rao norm [3], and the \nprobably  simplest norm, L1 and L2 weight decay, \nwhich is the topic of the article. We believe that simple \nis not simple at first glance. \n2 Main contributions  \n1. To the best of our knowledge, we first investigate the \ntheory \nof \ngenerative \ndeep \nlearning, \nwhich \nis \ncomplementary \nto \nthe \nintensively \nstudied  \ndiscriminative deep learning. \n2. We design three training settings of norm based \ncapacity control, and characterize the training dynamic \nof 8-layer CNN within 45 epochs to reveal the landscape \nof empirical error. \n3. We relate mathematics and neuron science to interpret  \nthe training dynamic, towards a better understanding of  \ntheory  of  generative deep learning. \n3 Theoretical preparation \n3.1 Optimization problem of  deep learning  \nMathematical theory of deep learning is a  combination \nof statistics, information theory, theory of algorithms, \nprobability and functional analysis (Beyond math, \ntheory of deep learning is also rooted in biology and \nphysics.). Optimization lays at the heart of the \nmathematical theory [4]. \nThe general problem of deep learning can be cast as \nsearching for the hypothesis  h in function space H, \nwhich characterize the structure of the data space of XÃ—\nY(X âŠ†ğ‘…ğ‘ƒ, Y âŠ†ğ‘…ğ¾), i.e.,  â„Î¸ : Xâ†’Y, that solves the \nfollowing optimization problem:  \n             min\nâ„âˆˆğ»ğ¿ğ‘„(â„) = E\nğ‘„(L(h x , y))         (1) \nwhere Q is a probability measure over the space XÃ—Y. \nAnd h can be realized [3] by feed forward neural network \nof depth L with coordinate-wise activation functions  ğœğ‘™: \nRâ†’R, i.e., \n â„Î¸  ğ‘¥ =  ğœğ¿+1( ğœğ¿ â€¦  ğœ2  ğœ1 ğ‘¥ğ‘‡ğ‘Š0 ğ‘Š1 ğ‘Š2 â€¦ )ğ‘Šğ¿),    \nwhere the parameter vector  Î¸ âˆˆÎ˜ L âŠ†ğ‘…ğ‘‘, \n          d = p ğ‘˜1 +  \n ğ‘˜ğ‘– ğ‘˜ğ‘–+1 +  ğ‘˜ğ¿K\nLâˆ’1\ni=1\n, \nand \nÎ˜ L =  ğ‘Š0 âˆˆğ‘…ğ‘ƒÃ— ğ‘˜1, ğ‘Š1 âˆˆğ‘…ğ‘˜1Ã— ğ‘˜2, â€¦ , ğ‘Šğ¿âˆˆğ‘… ğ‘˜ğ¿Ã—K (2).   \n \nWe set all bias terms to zero for simplicity, and set \nÏƒ(z) = max 0, z ,i.e.,ReLU.  \n3.2 Overparametrization \nThe most successful [1] deep CNNs such as VGG and \nResNets \nare \nbest \nused \nwith \na \ndegree \nof \nâ€œoverparametrizationâ€, that is more weights than data \npoints. Those practical and theoretical results are valid in \nthe discriminative deep learning case. \n \nWithout rigorous proof, we extend the results to \ngenerative deep learning case, and assume that a proper \noverparametrization also works well for deep learning \nbased image super resolution.  \n3.3 Duration of  landscape: 45 epochs  \nWhy 45? SRCNN [5] is trained for more than 109  \nsteps, which is not computationally feasible. We need to \nreduce the number of  steps while still attaining \ncomparable performance. \n \nWe \ndesign \na \ntest \nmodel \nof \n7-layer \n \nCNN, \nWARSHIP-XZNet [6] with 936778 parameters, for \nimage super resolution to gain peak signal to noise ratio \n(PSNR) of  36.58, by 45 epochs, 6601 steps  using [7] \n91 images. \n \nSo we also set number of our epochs to be 45, number of \nsteps to be 6601steps. \n4 Empirical settings and result \n4.1 Model settingï¼š945318, WARSHIP-XZNetÎ²   \nImage super resolution is the process of inferring a \nHigh-Resolution (HR) version of  a Low-Resolution \n(LR) input image, which could be divided into 3 \nsubtasks, corresponding to 3 subnets.  \nThe subtask of  embedding sub-net (Enet) is the \ntransformation from image space to  representation \nspace. Whereas the function of reconstruction sub-net \n(Rnet) is converting  representation back to image. The \nbridge between those two subnets is the inference \nsub-net (Inet), which will compute the map from low \nresolution representation (output of Enet) to high \nresolution representation  (input of  Rnet). \n \n                  \nFigure 1 overall architecture of WARSHIP-XZNetÎ² \n \nNotation: â€œConv layerâ€ mentioned below means a \nconvolutional layer and its following activation layer. In \nEnet, the 1st Conv layer stores the input/image, then the \n2nd Conv layer transforms the image to features, finally \nthe 3rd shrinking Conv layer with filter size of 1x1 \ncombine 2N input features into N out features. In Inet, \nresidual block with 1 Conv layer and additional \nconvolutional layer is recurred 4 times. We extract every \nintermediate output of features by each recurrence to \nRnet. In Rnet, expanding Conv layer with filter size of \n1x1 inverses the sparsity in Inet, then another Conv layer \nProceedings of CCIS2018 \n \ntransform features into several images, finally a weighed \nConv layer weights the images into a single image. The \nFigure1 is the architecture of WARSHIP-XZNetÎ². (The \ncopyright of  two pictures of accretion disk of super \nmassive black hole in figure 1 belongs to Hotaka \nShiokawa .) \n \nOur model WARSHIP-XZNetÎ² is fully CNN of  3 + 2 \n+ 3 = 8 layers. We set ReLU as activation layers and 192 \nfeatures as input of 2nd Conv layer. In each Conv layer, \nfilter size is 3x3 except that in shrinking and expanding \nlayers with 1x1.The total parameters is 945 318. \n945318 >936778 in WARSHIP-XZNet, satisfying the \nrequirement of overparametrization. \n4.2 Training settings : norm based capacity \ncontrol  \n1.Empirical Error \nThe mean squared error of final output or empirical error \nis \n     ğ¿1 ğœƒ =  \n1\n2ğ‘\nğ‘\nğ‘–=1\nâˆ¥ğ‘¦(ğ‘–) âˆ’ğ‘¦ğ‘™ğ‘’ğ‘ğ‘Ÿğ‘›ğ‘’ğ‘‘\n(ğ‘–)\nâˆ¥         (3) \nwhere Î¸ is the overall parameter set of the model, N is \nnumber of training samples; ğ‘¦(ğ‘–) is the i-th training \nsample, ğ‘¦ğ‘™ğ‘’ğ‘ğ‘Ÿğ‘›ğ‘’ğ‘‘\n(ğ‘–)\n is the output image learned by the \nmodel. \n2.Regularization Terms Work as Norm Based Capacity \nControl \nFor intermediate outputs, the cost function is \n         ğ¿2 ğœƒ =  \n \n1\n2ğ‘…ğ‘\nğ‘…\nğ‘Ÿ=1\nğ‘\nğ‘–=1\nâˆ¥ğ‘¦(ğ‘–) âˆ’ğ‘¦ğ‘Ÿ\n(ğ‘–) âˆ¥ \n          ğ‘¦ğ‘™ğ‘’ğ‘ğ‘Ÿğ‘›ğ‘’ğ‘‘\n(ğ‘–)\n=  \nğ‘¤ğ‘Ÿğ‘¦ğ‘Ÿ\n(ğ‘–)\nğ‘…\nğ‘Ÿ=1\n        \nwhere N, ğ‘¦(ğ‘–)and ğ‘¦ğ‘™ğ‘’ğ‘ğ‘Ÿğ‘›ğ‘’ğ‘‘\n(ğ‘–)\n are the same as (3), R is the \ntimes of recurrence, ğ‘¤ğ‘Ÿ denotes the weight that will \naverage images extracting from 4 recurrences into the \nfinal high resolution image, ğ‘¦ğ‘Ÿ\n(ğ‘–)  is the output image \nextracting from the r-th recurrence. \nWe devise 3 settings of regularization terms (weight \ndecay) working as different norm based capacity control. \nNotation:  ğœƒ(ğ¸) , ğœƒ(ğ¼) , and ğœƒ(ğ‘…)  are the parameter \nspace of Enet, Inet and Rnet, respectively, âˆ¥âˆ¥2  and \nâˆ¥âˆ¥1   indicate L2 and L1 norm, Î» is a balancing \nparameter and  set to 0.0002. \nAll-L2 means that all subnets are armed with  L2 norm. \nThat is,  \nğ¿3 ğœƒ = Î» âˆ¥ğœƒ(ğ¸) âˆ¥2+ Î» âˆ¥ğœƒ(ğ¼) âˆ¥2+ Î» âˆ¥ğœƒ(ğ‘…) âˆ¥2. \nIn setting of Mix, Enet and Rnet are imposed with  L2 \nnorm, while Inet is equipped with L1 norm. That is, \nğ¿3 ğœƒ = Î» âˆ¥ğœƒ(ğ¸) âˆ¥2+ Î» âˆ¥ğœƒ(ğ¼) âˆ¥1+ Î» âˆ¥ğœƒ(ğ‘…) âˆ¥2. \nFor All-L1 setup, all subnets are controlled by L1 norm. \nThat is, \nğ¿3 ğœƒ = Î» âˆ¥ğœƒ(ğ¸) âˆ¥1+ Î» âˆ¥ğœƒ(ğ¼) âˆ¥1+ Î» âˆ¥ğœƒ(ğ‘…) âˆ¥1. \n \nThus we get the overall cost function, \n      ğ¿(ğœƒ) = (1 âˆ’ğ›¼)ğ¿1 ğœƒ + ğ›¼ğ¿2 ğœƒ + ğ¿3 ğœƒ . \nwhere Î± is the weight of  ğ¿2 ğœƒ .The chain of 3 \nconsecutive changes works as follow: different norm of \nğ¿3 ğœƒ  causes ğ¿(ğœƒ) to vary, then variation of ğ¿(ğœƒ) \ncauses ğ‘Šğ‘–(i=0,â€¦,L) in (2) to be updated, finally update \nof ğ‘Šğ‘–(i=0,â€¦,L) causes ğ¿1 ğœƒ , i.e., empirical error, to \nchange. In the chain, update of  ğ‘Šğ‘–(i=0,â€¦,L) directly \ncontrol the model capacity. Details of norm based \ncapacity control are illustrated below in Table â…¡. \nTable â…¡ Norm based capacity control(weight decay). \nNorm Based Capacity Control \nTraining Settings \nAll-L2 \nMix  \nAll-L1 \nNetwork \nShare the same 8-layer CNN \nNorm on Enet \nL2 \nL2 \n \n \nL1 \n \nNorm on Inet \nL2 \nL1 \n \n \nL1 \n \nNorm on Rnet \nL2 \nL2 \n \n \nL1 \n \n3.Training  Strategy \nOur training dataset is  also dataset of  91 images. \nWe adopt the same training setting as DRCN [13]. Split \ntraining images into 41 by 41 patches with stride 21 and \n64 patches, which are used as mini-batch for stochastic \ngradient descent. We use the method described in  [8] \nto initialize weights in non-recursive layers, and set all \nweights to zero except self-connections (connection to \nthe same neuron in the next layer) for recursive \nconvolutions. Bias are set to zero. We choose the \nbeginning value of learning rate to be 0.01 and then \ndecreased it by a factor of 10 if the validation error dose \nnot decrease for 5 epochs. \nUnder the theoretical guideline in  3.3 ,the duration of \ntraining dynamic is set to 45 epochs (6601 steps) to \nexplore landscape of empirical error within early stages. \n4.3 Result of training dynamic: landscape of \nempirical error  \nPSNR is inversely proportional to the empirical error. \nThe higher PSNR implies lower empirical error. \n \nThe training dynamic is viewed from 3 consecutive \nstages, 1-15 epochs, 16-30 epochs and 31-45 epochs, \nwhich character the landscape of empirical error. The \nultimate PSNR of 3 settings are near 35, which is above \n33.66 gained by traditional super resolution method, \nBicubic. \n \n   \nProceedings of CCIS2018 \n \n \n  \nFigure 2 Training dynamic of 1st stage. â˜† denotes setting of \nMix, â–¡ is for All-L2 and â— represents All-L1 \n \n  \nFigure 3  Training dynamic of 2nd  stage. \n \n  \nFigure 4  Training dynamic of  3rd   stage. \n \n \n  \nFigure 5 Training dynamic of All 3 stages. \n \n5 Discussion and conclusion: interpretation \nof landscapes that relates mathematics and \nneuroscience  \n5.1Mathematical side  of regularization terms: \ndifferent sparsity  \nRegularizing with an L2 norm [9] is known as ridge \nregression in statistics and Tikhonov regularization in \nanalysis. L2 regularization involves adding an extra term \nto the cost functions that penalizes the sum of squares  \nof  weight, leading to small weights, which may or may \nnot be zero. \nL1 regularization is known as Lasso in statistics, \npenalize the absolute values of the weights, cause  \nmany parameters exactly equal to zero, produce a sparse \nparameter vector. As a comparison, sparsity gained by  \nL1 tend to be higher than that of  L2. \n5.2 Biological  side  of  regularization terms: \ndifferent population sparseness  \nThere are four definitions [10] of sparseness for firing \npatterns in research of systems neuroscience, low mean \nfiring rate, lifetime sparseness, information per spike \nand population sparseness. The biological corresponding \nmechanism of L2 and L1 regularization tend to be \npopulation sparseness, which means that the population \nresponse distribution elicited by each stimulus is peaked. \nA peaked distribution contains a lot small (or 0) values \nand few large values. It has been suggested that neural \ncodes with high population sparseness are advantageous \nbecause they resemble the inherently sparse structure of \nthe sensory environment. \nBased on the mathematical analysis of sparity mentioned \nabove, L2 regularization is inclined to be higher \npopulation sparseness than L2. \n5.3 Different subnets correspond to different \nvisual  areas: preservation and distillation  \nWe tend to conjecture that Enet and Rnet is very \nbiologically-plausible to primary visual cortex (V1). \nEnet and Rnet is about simple coder and decoder \nbetween images and representations. Correspondingly, \nearly visual areas like primary visual cortex [11] encode \nthe sensory features such as visual edges as sparse \nrepresentations, which is less abstract than higher \nrepresentations. \nBoth \nartificial \nand \nbiological \nencoder/decoder should preserve/retain the full/most \ninformation of the images, and involve  less \ninformation distillation. To the opposite side, distillation \nis quite necessary for higher/later stages in image \nclassification to perform more abstract representations \nbiologically and artificially. As we meet at the beginning \nof Introduction, for the discriminative deep learning case, \nwhen deep CNNs distill irrelevant representations, they \nshow nice/necessary properties of [12] invariance and \nselectivity (even need pooling layer to discard irrelevant \nrepresentation), towards strong robustness and high \nefficiency. \nProceedings of CCIS2018 \n \nInet enables the inference, which works in  a quite \ndifferent mechanism from that in Enet and Rnet. It \nrewrite the key and core part of  physical law which \ngoverns  the transformation from low resolution image \nto high resolution image. It may share some common \nfeatures with higher inference in neural system(like \nimage classification), but to a great extent, seems to \nwork in a distinguish style(at least, our eyes can not \nperform task of image super resolution.). So we should \nbe careful to absorb the biological inspiration from V2 \nand V4. \n5.4 Interpretation of  landscapes of empirical \nerror mathematically and biologically  \nFor Enet and Rnet, comparison of Mix and All-L1 \nduring 2nd and 3rd  stages (We abandon the 1st stage \nwhich may be caused by too many factors.) tends to \nconfirm that regularization term of L1 norm is more \nsuitable than L2 norm to build a better landscape of \nempirical error. Mathematically, L1 exhibits higher \nsparse mechanism than L2. Additional biological \nobservation discussed in 5.2 and 5.3 tend to prove that \nhigher population sparseness in primary visual cortex is \na better candidate for Enet and Rnet.  \nFor Inet ,comparison of All-L2 and Mix during 2nd and \n3rd  stages (We also abandon the 1st stage which may \nbe caused by too many factors.) is inclined to conclude \nthat L2 norm is more suitable than L1 norm to achieve a \nbetter landscape of empirical error. Whereas, any \nbiological clue/inspiration for Inet  is not clear. \n6 Theoretical exploration in generative deep \nlearning in the future: beyond vision \ntowards NLP   \nOur open theoretical insights into both discriminative \nand generative deep learning should benefit from \ninteraction of 3  main AI tasks, that is, vision, audio \nand natural language processing(NLP).  \nWhat does matter is the essence, not the superficies. For \nexample, image super resolution [6], image caption and \nmachine translation [15] may sound quite different at \nfirst sight, but they actually belong to the same issue \nwhen viewed from lens of theoretical generative deep \nlearning(at \nleast, \nwhen \nconsidering \ninformation \npreservation). So in the future, beyond vision, we may \nexperimentally and theoretically explore two NLP tasks \nof image caption(a special task that bridges vision and \nNLP) and machine translation to reveal the veil of \ntheoretical generative deep learning, contributing to the \n[14]science of intelligence. \nAcknowledgements \nThis work was supported by the  National Basic Research \nProgram of China (2012CB821804 and 2015CB857100), the \nNational Science Foundation of China (11103055  and \n11773062) and the West Light Foundation of Chinese \nAcademy of Sciences (RCPY201105, 2017-XBQNXZ-A-008 \nand 2016-QNXZ-B-25). \nReferences \n[1] \nQianli Liao, Tomaso Poggio. Theory of Deep Learningâ…¡\n:Landscape of the Empirical Risk in Deep Learning. \narXiv:1703.09833, 2017. \n[2] \nLi Wan, Matthew Zeiler, Sixin Zhang, Yann LeCun, Rob \nFergus. Regularization of neural networks using \nDropConnect. Journal of Machine Learning Research \n28:1058-1066, 2013. \n[3] \nTengyuan Liang, Tomaso Poggio, Alexander Rakhlin, \nJames Stokes. Fisher-Rao Metric, Geometry, and \nComplexity of Neural Networks. arXiv:1711.01530, \n2017. \n[4] \nA. Munoz. Machine Learning and Optimization. \nhttps://cims.nyu.edu/~munoz/files/ml_optimization.pdf. \n[5] \nChao Dong, Chen Change Loy, Kaiming He, Xiaoou \nTang. \nImage \nSuper-Resolution \nUsing \nDeep \nConvolutional Networks.  arxiv:1501.00092v3, 2015. \n[6] \nWendi \nXu, \nMing \nZhang. \nTowards \nWARSHIP: \nCombining Components of Brain-Inspired Computing of \nRSH for Image Super Resolution.  IEEE International \nConference on Cloud Computing and Intelligence \nSystems,  2018. \n[7] \nYang, Jianchao, et al. Image Super-Resolution via Sparse \nRepresentation[J]. \nIEEE \nTransactions \non \nImage \nProcessing, 2010, 19(11):2861-2873. \n[8] \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. \nDelving Deep into Rectifiers: Surpassing Human-Level \nPerformance \non \nImageNet \nClassification[J]. \n2015:1026-1034. \n[9] \nConstantine Caramanis, Shie Mannor, Huan Xu. Robust \nOptimization in Machine Learning[J]. Optimizatio, 2011. \n[10] Ben D. B. Willmore, James A. Mazer, Jack L. Gallant. \nSparse Coding in Striate and Extrastriate Visual Cortex,â€ \nJournal of Neurophysiology Vol. 105, No. 6,pp.  \n2907-2919, June 2011. \n[11] Subutai Ahmad, Jeff Hawkins. How Do Neurons Operate \non Sparse Distributed Representations? a Mathematical \nTheory of Sparsity, Neurons and Active Dendrites. \narXiv:1601.00720v1, 2016. \n[12] Yoshua Bengio, Aaron Courville, Pascal Vincent. \nRepresentation \nLearning: \na \nReview \nand \nNew \nPerspectives. arXiv:1206.5538v3, 2014. \n[13] Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee. \nDeeply-Recursive Convolutional Network for Image \nSuper-Resolution.  arxiv:1511.04491v2, 2016. \n[14] Tomaso Poggio. Deep Learning: Mathematics and \nNeuroscience. \nview&review.center \nfor \nbrain,mind,machine, 2016. \n[15] Yirong Pan, Xiao Li, Yating Yang et al. A Content-Based \nNeural Reordering Model for Statistical Machine \nTranslation, \nCommunications \nin \nComputer \nand \nInformation Science, 2017 \n \n \n \n \n",
  "categories": [
    "cs.LG",
    "cs.CV"
  ],
  "published": "2018-10-03",
  "updated": "2018-10-03"
}