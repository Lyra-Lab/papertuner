{
  "id": "http://arxiv.org/abs/1810.02334v6",
  "title": "Unsupervised Learning via Meta-Learning",
  "authors": [
    "Kyle Hsu",
    "Sergey Levine",
    "Chelsea Finn"
  ],
  "abstract": "A central goal of unsupervised learning is to acquire representations from\nunlabeled data or experience that can be used for more effective learning of\ndownstream tasks from modest amounts of labeled data. Many prior unsupervised\nlearning works aim to do so by developing proxy objectives based on\nreconstruction, disentanglement, prediction, and other metrics. Instead, we\ndevelop an unsupervised meta-learning method that explicitly optimizes for the\nability to learn a variety of tasks from small amounts of data. To do so, we\nconstruct tasks from unlabeled data in an automatic way and run meta-learning\nover the constructed tasks. Surprisingly, we find that, when integrated with\nmeta-learning, relatively simple task construction mechanisms, such as\nclustering embeddings, lead to good performance on a variety of downstream,\nhuman-specified tasks. Our experiments across four image datasets indicate that\nour unsupervised meta-learning approach acquires a learning algorithm without\nany labeled data that is applicable to a wide range of downstream\nclassification tasks, improving upon the embedding learned by four prior\nunsupervised learning methods.",
  "text": "Published as a conference paper at ICLR 2019\nUNSUPERVISED LEARNING VIA META-LEARNING\nKyle Hsu†\nUniversity of Toronto\nkyle.hsu@mail.utoronto.ca\nSergey Levine, Chelsea Finn\nUniversity of California, Berkeley\n{svlevine,cbfinn}@eecs.berkeley.edu\nABSTRACT\nA central goal of unsupervised learning is to acquire representations from unla-\nbeled data or experience that can be used for more effective learning of down-\nstream tasks from modest amounts of labeled data. Many prior unsupervised\nlearning works aim to do so by developing proxy objectives based on reconstruc-\ntion, disentanglement, prediction, and other metrics. Instead, we develop an unsu-\npervised meta-learning method that explicitly optimizes for the ability to learn a\nvariety of tasks from small amounts of data. To do so, we construct tasks from un-\nlabeled data in an automatic way and run meta-learning over the constructed tasks.\nSurprisingly, we ﬁnd that, when integrated with meta-learning, relatively simple\ntask construction mechanisms, such as clustering embeddings, lead to good per-\nformance on a variety of downstream, human-speciﬁed tasks. Our experiments\nacross four image datasets indicate that our unsupervised meta-learning approach\nacquires a learning algorithm without any labeled data that is applicable to a wide\nrange of downstream classiﬁcation tasks, improving upon the embedding learned\nby four prior unsupervised learning methods.\n1\nINTRODUCTION\nUnsupervised learning is a fundamental, unsolved problem (Hastie et al., 2009) and has seen promis-\ning results in domains such as image recognition (Le et al., 2013) and natural language understand-\ning (Ramachandran et al., 2017). A central use case of unsupervised learning methods is enabling\nbetter or more efﬁcient learning of downstream tasks by training on top of unsupervised representa-\ntions (Reed et al., 2014; Cheung et al., 2015; Chen et al., 2016) or ﬁne-tuning a learned model (Erhan\net al., 2010). However, since the downstream objective requires access to supervision, the objectives\nused for unsupervised learning are only a rough proxy for downstream performance. If a central goal\nof unsupervised learning is to learn useful representations, can we derive an unsupervised learning\nobjective that explicitly takes into account how the representation will be used?\nThe use of unsupervised representations for downstream tasks is closely related to the objective\nof meta-learning techniques: ﬁnding a learning procedure that is more efﬁcient and effective than\nlearning from scratch. However, unlike unsupervised learning methods, meta-learning methods\nrequire large, labeled datasets and hand-speciﬁed task distributions. These dependencies are major\nobstacles to widespread use of these methods for few-shot classiﬁcation.\nTo begin addressing these problems, we propose an unsupervised meta-learning method: one which\naims to learn a learning procedure, without supervision, that is useful for solving a wide range of\nnew, human-speciﬁed tasks. With only raw, unlabeled observations, our model’s goal is to learn\na useful prior such that, after meta-training, when presented with a modestly-sized dataset for a\nhuman-speciﬁed task, the model can transfer its prior experience to efﬁciently learn to perform the\nnew task. If we can build such an algorithm, we can enable few-shot learning of new tasks without\nneeding any labeled data nor any pre-deﬁned tasks.\nTo perform unsupervised meta-learning, we need to automatically construct tasks from unlabeled\ndata. We study several options for how this can be done. We ﬁnd that a good task distribution\nshould be diverse, but also not too difﬁcult: na¨ıve random approaches for task generation produce\ntasks that contain insufﬁcient regularity to enable useful meta-learning. To that end, our method pro-\nposes tasks by ﬁrst leveraging prior unsupervised learning algorithms to learn an embedding of the\n†Work done as a visiting student researcher at the University of California, Berkeley.\n1\narXiv:1810.02334v6  [cs.LG]  21 Mar 2019\nPublished as a conference paper at ICLR 2019\ninput data, and then performing an overcomplete partitioning of the dataset to construct numerous\ncategorizations of the data. We show how we can derive classiﬁcation tasks from these catego-\nrizations for use with meta-learning algorithms. Surprisingly, even with simple mechanisms for\npartitioning the embedding space, such as k-means clustering, we ﬁnd that meta-learning acquires\npriors that, when used to learn new, human-designed tasks, learn those tasks more effectively than\nmethods that directly learn on the embedding. That is, the learning algorithm acquired through un-\nsupervised meta-learning achieves better downstream performance than the original representation\nused to derive meta-training tasks, without introducing any additional assumptions or supervision.\nSee Figure 1 for an illustration of the complete approach.\nThe core idea in this paper is that we can leverage unsupervised embeddings to propose tasks for\na meta-learning algorithm, leading to an unsupervised meta-learning algorithm that is particularly\neffective as pre-training for human-speciﬁed downstream tasks. In the following sections, we for-\nmalize our problem assumptions and goal, which match those of unsupervised learning, and discuss\nseveral options for automatically deriving tasks from embeddings. We instantiate our method with\ntwo meta-learning algorithms and compare to prior state-of-the-art unsupervised learning meth-\nods. Across four image datasets (MNIST, Omniglot, miniImageNet, and CelebA), we ﬁnd that our\nmethod consistently leads to effective downstream learning of a variety of human-speciﬁed tasks,\nincluding character recognition tasks, object classiﬁcation tasks, and facial attribute discrimination\ntasks, without requiring any labels or hand-designed tasks during meta-learning and where key hy-\nperparameters of our method are held constant across all domains. We show that, even though our\nunsupervised meta-learning algorithm trains for one-shot generalization, one instantiation of our ap-\nproach performs well not only on few-shot learning, but also when learning downstream tasks with\nup to 50 training examples per class. In fact, some of our results begin to approach the performance\nof fully-supervised meta-learning techniques trained with fully-speciﬁed task distributions.\n...\n,\n,\n...\n,\n,\n2a. cluster embeddings multiple times\nembedding function\n1. run embedding learning\ntest\ntrain\n3. run meta-learning on tasks\n2b. automatically construct tasks without supervision\n,\n...\n,\nmeta-learner\nlearning procedure\nFigure 1: Illustration of the proposed unsupervised meta-learning procedure. Embeddings of raw observations\nare clustered with k-means to construct partitions, which give rise to classiﬁcation tasks. Each task involves\ndistinguishing between examples from N = 2 clusters, with Km-tr = 1 example from each cluster being a\ntraining input. The meta-learner’s aim is to produce a learning procedure that successfully solves these tasks.\n2\nUNSUPERVISED META-LEARNING\nIn this section, we describe our problem setting in relation to that of unsupervised and semi-\nsupervised learning, provide necessary preliminaries, and present our approach.\n2.1\nPROBLEM STATEMENT\nOur goal is to leverage unlabeled data for the efﬁcient learning of a range of human-speciﬁed down-\nstream tasks. We only assume access to an unlabeled dataset D = {xi} during meta-training. After\nlearning from the unlabeled data, which we will refer to as unsupervised meta-training, we want\nto apply what was learned towards learning a variety of downstream, human-speciﬁed tasks from\na modest amount of labeled data, potentially as few as a single example per class. These down-\nstream tasks may, in general, have different underlying classes or attributes (in contrast to typical\nsemi-supervised problem assumptions), but are assumed to have inputs from the same distribution\nas the one from which datapoints in D are drawn. Concretely, we assume that downstream tasks\nare M-way classiﬁcation tasks, and that the goal is to learn an accurate classiﬁer using K labeled\ndatapoints (xk, yk) from each of the M classes, where K is relatively small (i.e. between 1 and 50).\n2\nPublished as a conference paper at ICLR 2019\nThe unsupervised meta-training phase aligns with the unsupervised learning problem in that it in-\nvolves no access to information about the downstream tasks, other than the fact that they are M-way\nclassiﬁcation tasks, for variable M upper-bounded by N. The upper bound N is assumed to be\nknown during unsupervised meta-training, but otherwise, the values of M and K are not known a\npriori. As a result, the unsupervised meta-training phase needs to acquire a sufﬁciently general prior\nfor applicability to a range of classiﬁcation tasks with variable quantities of data and classes. This\nproblem deﬁnition is our prototype for a practical use-case in which a user would like to train an\napplication-speciﬁc image classiﬁer, but does not have an abundance of labeled data.\n2.2\nPRELIMINARIES\nUnsupervised embedding learning. An unsupervised embedding learning algorithm E is a pro-\ncedure that takes as input an unlabeled dataset D = {xi} and outputs a mapping from {xi} to\nembeddings {zi}. These embedded points are typically lower-dimensional and arranged such that\ndistances correspond to meaningful differences between inputs, in contrast to distances between the\noriginal inputs, such as image pixels, which are not meaningful measures of image similarity.\nTask.\nAn M-way K-shot classiﬁcation task T consists of K training datapoints and labels\n{(xk, ℓk)} per class, which are used for learning a classiﬁer, and Q query datapoints and labels\nper class, on which the learned classiﬁer is evaluated. That is, in a task there are K + Q = R\ndatapoints and labels for each of the M classes.\nMeta-learning. A supervised meta-learning algorithm M(·) takes as input a set of supervised meta-\ntraining tasks {Tt}. It produces a learning procedure F(·), which, in turn, ingests the supervised\ntraining data of a task to produce a classiﬁer f(·). The goal of M is to learn F such that, when faced\nwith a meta-test time task Tt′ held-out from {Tt}, F can learn a ft′ that accomplishes Tt′. At a\nhigh level, the quintessential meta-learning strategy is to have M iterate over {Tt}, cycling between\napplying the current form of Ft on training data from Tt to learn ft, assessing its performance\nby calculating some meta-loss L on held-out data from the task, and optimizing L to improve the\nlearning procedure.\nWe build upon two meta-learning algorithms: model agnostic meta-learning (MAML) (Finn et al.,\n2017) and prototypical networks (ProtoNets) (Snell et al., 2017). MAML aims to learn the initial\nparameters of a deep network such that one or a few gradient steps leads to effective generalization;\nit speciﬁes F as gradient descent starting from the meta-learned parameters. ProtoNets aim to meta-\nlearn a representation in which a class is effectively identiﬁed by its prototype, deﬁned to be the\nmean of the class’ training examples in the meta-learned space; F is the computation of these class\nprototypes, and f is a linear classiﬁer that predicts the class whose prototype is closest in Euclidean\ndistance to the query’s representation.\nTask generation for meta-learning. We brieﬂy summarize how tasks are typically generated from\nlabeled datasets {(xi, yi)} for supervised meta-learning, as introduced by Santoro et al. (2016). For\nsimplicity, consider the case where the labels are discrete scalar values yi. To construct an N-way\nclassiﬁcation task T (assuming N is not greater than the number of unique yi), we can sample\nN classes, sample R datapoints {xr}n for each of the N classes, and sample a permutation of N\ndistinct one-hot vectors (ℓn) to serve as task-speciﬁc labels of the N sampled classes. The task is\nthen deﬁned as T = {(xn,r, ℓn) | xn,r ∈{xr}n}. Of course, this procedure is only possible with\nlabeled data; in the next section, we discuss how we can construct tasks without ground-truth labels.\n2.3\nUNSUPERVISED META-LEARNING WITH AUTOMATICALLY CONSTRUCTED TASKS\nWe approach our problem from a meta-learning perspective, framing the problem as the acquisition,\nfrom unlabeled data, of an efﬁcient learning procedure that is transferable to human-designed tasks.\nIn particular, we aim to construct classiﬁcation tasks from the unlabeled data and then learn how\nto efﬁciently learn these tasks. If such tasks are adequately diverse and structured, then meta-\nlearning these tasks should enable fast learning of new, human-provided tasks. A key question, then,\nis how to automatically construct such tasks from unlabeled data D = {xi}. Notice that in the\nsupervised meta-learning task generation procedure detailed in Section 2.2, the labels yi induce a\npartition P = {Cc} over {xi} by assigning all datapoints with label yc to subset Cc. Once a partition\nis obtained, task generation is simple; we can reduce the problem of constructing tasks to that of\n3\nPublished as a conference paper at ICLR 2019\nconstructing a partition over {xi}. All that’s left is to ﬁnd a principled alternative to human labels\nfor deﬁning the partition.\nA na¨ıve approach is to randomly partition the data D. While such a scheme introduces diverse\ntasks, there is no structure; that is, there is no consistency between a task’s training data and query\ndata, and hence nothing to be learned during each task, let alone across tasks. As seen in Table 3,\nproviding a meta-learner with purely random tasks results in failed meta-learning.\nTo construct tasks with structure that resembles that of human-speciﬁed labels, we need to group dat-\napoints into consistent and distinct subsets based on salient features. With this motivation in mind,\nwe propose to use k-means clustering. Consider the partition P = {Cc} learned by k-means as a\nsimpliﬁcation of a Gaussian mixture model p(x|c)p(c). If the clusters can recover a semblance of\nthe true class-conditional generative distributions p(x|c), creating tasks based on treating these clus-\nters as classes should result in useful unsupervised meta-training. However, the result of k-means is\ncritically dependent on the metric space on which its objective is deﬁned. Clustering in pixel-space\nis unappealing for two reasons: (1) distance in pixel-space correlates poorly with semantic meaning,\nand (2) the high dimensionality of raw images renders clustering difﬁcult in practice. We empirically\nshow in Table 3 that meta-learning with tasks deﬁned by pixel-space clusters, with preprocessing as\ndirected by Coates & Ng (2012), also fails.\nWe are now motivated to cluster in spaces in which common distance functions correlate to semantic\nmeaning. However, we must satisfy the constraints of our problem statement in the process of learn-\ning such spaces. To these ends, we use state-of-the-art unsupervised learning methods to produce\nuseful embedding spaces. For qualitative evidence in the unsupervised learning literature that such\nembedding spaces exhibit semantic meaning, see Cheung et al. (2015); Bojanowski & Joulin (2017);\nDonahue et al. (2017). We note that while a given embedding space may not be directly suitable\nfor highly-efﬁcient learning of new tasks (which would require the embedding space to be precisely\naligned or adaptable to the classes of those tasks), we can still leverage it for the construction of\nstructured tasks, a process for which requirements are less strict.\nThus, we ﬁrst run an out-of-the-box unsupervised embedding learning algorithm E on D, then map\nthe data {xi} into the embedding space Z, producing {zi}. To produce a diverse task set, we gener-\nate P partitions {Pp} by running clustering P times, applying random scaling to the dimensions of\nZ to induce a different metric, represented by diagonal matrix A, for each run of clustering. With\nµc denoting the learned centroid of cluster Cc, a single run of clustering can be summarized with\nP, {µc} = arg min\n{Cc},{µc}\nk\nX\nc=1\nX\nz∈Cc\n∥z −µc∥2\nA\n(1)\nWe derive tasks for meta-learning from the partitions using the procedure detailed in Section 2.2,\nexcept we begin the construction of each task by sampling a partition from the uniform distribution\nU(P), and for xi ∈Cc, specify yi = c. To avoid imbalanced clusters dominating the meta-training\ntasks, we opt not to sample from p(c) ∝|Cc|, but instead sample N clusters uniformly without\nreplacement for each task. We note that Caron et al. (2018) are similarly motivated in their design\ndecision of sampling data from a uniform distribution over clusters.\nWith the partitions being constructed over {zi}, we have one more design decision to make: should\nwe perform meta-learning on embeddings or images? We consider that, to successfully solve new\ntasks at meta-test time, a learning procedure F that takes embeddings as input would depend on the\nembedding function’s ability to generalize to out-of-distribution observations. On the other hand, by\nmeta-learning on images, F can separately adapt f to each evaluation task from the rawest level of\nrepresentation. Thus, we choose to meta-learn on images.\nWe call our method clustering to automatically construct tasks for unsupervised meta-learning\n(CACTUs). We detail the task construction algorithm in Algorithm 1, and provide an illustration of\nthe complete unsupervised meta-learning approach for classiﬁcation in Figure 1.\n3\nRELATED WORK\nThe method we propose aims to address the unsupervised learning problem (Hastie et al., 2009; Le\net al., 2013), namely acquiring a transferable learning procedure without labels. We show that our\n4\nPublished as a conference paper at ICLR 2019\nAlgorithm 1 CACTUs for classiﬁcation\n1: procedure CACTUS(E, D, P, k, T, N, Km-tr, Q)\n2:\nRun embedding learning algorithm E on D and produce embeddings {zi} from observations {xi}.\n3:\nRun k-means on {zi} P times (with random scaling) to generate a set of partitions {Pp = {Cc}p}.\n4:\nfor t from 1 to the number of desired tasks T do\n5:\nSample a partition P uniformly at random from the set of partitions {Pp}.\n6:\nSample a cluster Cn uniformly without replacement from P for each of the N classes desired for a\ntask.\n7:\nSample an embedding zr without replacement from Cn for each of the R = Km-tr +Q training and\nquery examples desired for each class, and record the corresponding datapoint xn,r.\n8:\nSample a permutation (ℓn) of N one-hot labels.\n9:\nConstruct Tt = {(xn,r, ℓn)}.\n10:\nreturn {Tt}\nmethod is complementary to a number of unsupervised learning methods, including ACAI (Berth-\nelot et al., 2018), BiGAN (Donahue et al., 2017; Dumoulin et al., 2017), DeepCluster (Caron et al.,\n2018), and InfoGAN (Chen et al., 2016): we leverage these prior methods to learn embeddings\nused for constructing meta-learning tasks, and demonstrate that our method learns a more useful\nrepresentation than the embeddings. The ability to use what was learned during unsupervised pre-\ntraining to better or more efﬁciently learn a variety of downstream tasks is arguably one of the most\npractical applications of unsupervised learning methods, which has a long history in neural network\ntraining (Hinton et al., 2006; Bengio et al., 2007; Ranzato et al., 2006; Vincent et al., 2008; Erhan\net al., 2010). Unsupervised pre-training has demonstrated success in a number of domains, includ-\ning speech recognition (Yu et al., 2010), image classiﬁcation (Zhang et al., 2017), machine transla-\ntion (Ramachandran et al., 2017), and text classiﬁcation (Dai & Le, 2015; Howard & Ruder, 2018;\nRadford et al., 2018). Our approach, unsupervised meta-learning, can be viewed as an unsupervised\nlearning algorithm that explicitly optimizes for few-shot transferability. As a result, we can expect it\nto better learn human-speciﬁed downstream tasks, compared to unsupervised learning methods that\noptimize for other metrics, such as reconstruction (Vincent et al., 2010; Higgins et al., 2017), ﬁdelity\nof constructed images (Radford et al., 2016; Salimans et al., 2016; Donahue et al., 2017; Dumoulin\net al., 2017), representation interpolation (Berthelot et al., 2018), disentanglement (Bengio et al.,\n2013; Reed et al., 2014; Cheung et al., 2015; Chen et al., 2016; Mathieu et al., 2016; Denton &\nBirodkar, 2017), and clustering (Coates & Ng, 2012; Kr¨ahenb¨uhl et al., 2016; Bojanowski & Joulin,\n2017; Caron et al., 2018). We empirically evaluate this hypothesis in the next section. In contrast to\nmany previous evaluations of unsupervised pre-training, we focus on settings in which only a small\namount of data for the downstream tasks is available, since this is where the unlabeled data can be\nmaximally useful.\nUnsupervised pre-training followed by supervised learning can be viewed as a special case of the\nsemi-supervised learning problem (Zhu, 2011; Kingma et al., 2014; Rasmus et al., 2015; Oliver\net al., 2018). However, in contrast to our problem statement, semi-supervised learning methods\nassume that a signiﬁcant proportion of the unlabeled data, if not all of it, shares underlying labels\nwith the labeled data. Additionally, our approach and other unsupervised learning methods are well-\nsuited for transferring their learned representation to many possible downstream tasks or labelings,\nwhereas semi-supervised learning methods typically optimize for performance on a single task, with\nrespect to a single labeling of the data.\nOur method builds upon the ideas of meta-learning (Schmidhuber, 1987; Bengio et al., 1991; Naik\n& Mammone, 1992) and few-shot learning (Santoro et al., 2016; Vinyals et al., 2016; Ravi &\nLarochelle, 2017; Munkhdalai & Yu, 2017; Snell et al., 2017). We apply two meta-learning al-\ngorithms, model-agnostic meta-learning (Finn et al., 2017) and prototypical networks (Snell et al.,\n2017), to tasks constructed in an unsupervised manner. Similar to our problem setting, some prior\nworks have aimed to learn an unsupervised learning procedure with supervised data (Garg & Kalai,\n2017; Metz et al., 2018). Instead, we consider a problem setting that is entirely unsupervised, aim-\ning to learn efﬁcient learning algorithms using unlabeled datasets. Our problem setting is similar to\nthat considered by Gupta et al. (2018), but we develop an approach that is suitable for supervised\ndownstream tasks, rather than reinforcement learning problems, and demonstrate our algorithm on\nproblems with high-dimensional visual observations.\n5\nPublished as a conference paper at ICLR 2019\n4\nEXPERIMENTS\nWe begin the experimental section by presenting our research questions and how our experiments are\ndesigned to address them. Links to code for the experiments can be found at https://sites.\ngoogle.com/view/unsupervised-via-meta.\nBeneﬁt of meta-learning. Is there any signiﬁcant beneﬁt to doing meta-learning on tasks derived\nfrom embeddings, or is the embedding function already sufﬁcient for downstream supervised learn-\ning of new tasks? To investigate this, we run MAML and ProtoNets on tasks generated via CACTUs\n(CACTUs-MAML, CACTUs-ProtoNets). We compare to ﬁve alternate algorithms, with four being\nsupervised learning methods on top of the embedding function. i) Embedding knn-nearest neighbors\nﬁrst infers the embeddings of the downstream task images. For a query test image, it predicts the\nplurality vote of the labels of the knn training images that are closest in the embedding space to\nthe query’s embedding. ii) Embedding linear classiﬁer also begins by inferring the embeddings of\nthe downstream task images. It then ﬁts a linear classiﬁer using the NK training embeddings and\nlabels, and predicts labels for the query embeddings using the classiﬁer. iii) Embedding multilayer\nperceptron instead uses a network with one hidden layer of 128 units and tuned dropout (Srivastava\net al., 2014). iv) To isolate the effect of meta-learning on images, we also compare to embedding\ncluster matching, i.e. directly using the meta-training clusters for classiﬁcation by labeling clusters\nwith a task’s training data via plurality vote. If a query datapoint maps to an unlabeled cluster,\nthe closest labeled cluster is used. v) As a baseline, we forgo any unsupervised pre-training and\ntrain a model with the MAML architecture from standard random network initialization via gradient\ndescent separately for each evaluation task.\nDifferent embedding spaces. Does CACTUs result in successful meta-learning for many distinct\ntask-generating embeddings? To investigate this, we run unsupervised meta-learning using four\nembedding learning algorithms: ACAI (Berthelot et al., 2018), BiGAN (Donahue et al., 2017),\nDeepCluster (Caron et al., 2018), and InfoGAN (Chen et al., 2016). These four approaches collec-\ntively cover the following range of objectives and frameworks in the unsupervised learning literature:\ngenerative modeling, two-player games, reconstruction, representation interpolation, discriminative\nclustering, and information maximization. We describe these methods in more detail in Appendix A.\nApplicability to different tasks. Can unsupervised meta-learning yield a good prior for a variety of\ntask types? In other words, can unsupervised meta-learning yield a good representation for tasks that\nassess the ability to distinguish between features on different scales, or tasks with various amounts of\nsupervision signal? To investigate this, we evaluate our procedure on tasks assessing recognition of\ncharacter identity, object identity, and facial attributes. For this purpose we choose to use the existing\nOmniglot (Santoro et al., 2016) and miniImageNet (Ravi & Larochelle, 2017) datasets and few-shot\nclassiﬁcation tasks and, inspired by Finn et al. (2018), also construct a new few-shot classiﬁcation\nbenchmark based on the CelebA dataset and its binary attribute annotations. For miniImageNet,\nwe consider both few-shot downstream tasks and tasks involving larger datasets (up to 50-shot).\nSpeciﬁcs on the datasets and human-designed tasks are presented in Appendix B.\nOracle. How does the performance of our unsupervised meta-learning method compare to super-\nvised meta-learning with a human-speciﬁed, near-optimal task distribution derived from a labeled\ndataset? To investigate this, we use labeled versions of the meta-training datasets to run MAML and\nProtoNets as supervised meta-learning algorithms (Oracle-MAML, Oracle-ProtoNets). To facilitate\nfair comparison with the unsupervised variants, we control for the relevant hyperparameters.\nTask construction ablation. How do the alternatives for constructing tasks from the embeddings\ncompare? To investigate this, we run MAML on tasks constructed via clustering (CACTUs-MAML)\nand MAML on tasks constructed via random hyperplane slices of the embedding space with varying\nmargin (Hyperplanes-MAML). The latter partitioning procedure is detailed in Appendix C. For the\nexperiments where tasks are constructed via clustering, we also investigate the effect of sampling\nbased on a single partition versus multiple partitions. We additionally experiment with tasks based\non random assignments of images to “clusters” (Random-MAML) and tasks based on pixel-space\nclusters (Pixels CACTUs-MAML) with the Omniglot dataset.\nTo investigate the limitations of our method, we also consider an easier version of our problem\nstatement where the data distributions at meta-training and meta-test time perfectly overlap, i.e.\nthe images share a common set of underlying labels (Appendix D). Finally, we present results on\nminiImageNet after unsupervised meta-learning on most of ILSVRC 2012 (Appendix G).\n6\nPublished as a conference paper at ICLR 2019\n(a)\n(b)\ntrain\ntest\nCelebA\nminiImageNet\nFigure 2: Examples of three DeepCluster-embedding cluster-based classes (a) and a 2-way 5-shot test task (b)\nfor two datasets. (a) Some of the clusters correspond well to unseen labels (top left, bottom left). Others exhibit\nsemantic meaning despite members not being grouped as such in the labeled version of the dataset (top middle:\npair of objects, bottom middle: white hat). Still others are uninterpretable (top right) or are based on image\nartifacts (bottom right). (b) We evaluate unsupervised pre-training based on the ability to learn downstream,\nhuman-designed tasks with held-out images and underlying classes.\n4.1\nEXPERIMENTAL PROTOCOL SUMMARY\nAs discussed by Oliver et al. (2018), keeping proper experimental protocol is particularly important\nwhen evaluating unsupervised and semi-supervised learning algorithms. Our foremost concern is\nto avoid falsely embellishing the capabilities of our approach by overﬁtting to the speciﬁc datasets\nand task types that we consider. To this end, we adhere to two key principles. We do not perform\nany architecture engineering: we use architectures from prior work as-is, or lightly adapt them to\nour needs if necessary. We also keep hyperparameters related to the unsupervised meta-learning\nstage as constant as possible across all experiments, including the MAML and ProtoNets model\narchitectures. Details on hyperparameters and architectures are presented in Appendix E. We assume\nknowledge of an upper bound on the number of classes N present in each downstream meta-testing\ntask for each dataset. However, regardless of the number of shots K, we do not assume knowledge\nof K during unsupervised meta-learning. We use N-way 1-shot tasks during meta-training, but test\non larger values of K during meta-testing.\nWe partition each dataset into meta-training, meta-validation, and meta-testing splits. For Omniglot\nand miniImageNet, these splits contain disjoint sets of classes. For all algorithms, we run unsuper-\nvised pre-training on the unlabeled meta-training split and report performance on downstream tasks\ndictated by the labeled data of the meta-testing split, generated using the procedure from prior work\nrecounted in Section 2.2. For the supervised meta-learning oracles, meta-training tasks are con-\nstructed in the same manner but from the dataset’s meta-training split. See Figure 2 for illustrative\nexamples of embedding-derived clusters and human-designed test tasks.\nTo facilitate analysis on meta-overﬁtting, we use the labels of the meta-validation split (instead\nof clustering embeddings) to construct tasks for meta-validation. However, because our aim is to\nperform meta-learning without supervision, we do not tune hyperparameters on this labeled data. We\nuse a ﬁxed number of meta-training iterations, since there is no suitable criterion for early stopping.\nWhen we experiment with the embedding-plus-supervised-learning methods used as fair compar-\nisons to unsupervised meta-learning, we err on the side of providing more supervision and data than\ntechnically allowed. Speciﬁcally, we separately tune the supervised learning hyperparameters for\neach dataset and each task difﬁculty on the labeled version of the meta-validation split. With Deep-\nCluster embeddings, we also use the entire meta-testing split’s statistics to perform dimensionality\nreduction (via PCA) and whitening, which is unfair as this shares information across tasks.\n4.2\nRESULTS\nOur primary results are summarized in Tables 1 and 2. Task construction ablations are summarized\nin Tables 3 and 4.\n7\nPublished as a conference paper at ICLR 2019\nBeneﬁt of meta-learning. CACTUs-MAML consistently yields a learning procedure that results\nin more successful downstream task performance than all other unsupervised methods, including\nthose that learn on top of the embedding that generated meta-training tasks for MAML. We ﬁnd the\nsame result for CACTUs-ProtoNets for 1-shot downstream tasks. However, as noted by Snell et al.\n(2017), ProtoNets perform best when meta-training shot and meta-testing shot are matched; this\ncharacteristic prevents ProtoNets from improving upon ACAI for 20-way 5-shot Omniglot and upon\nDeepCluster for 50-shot miniImageNet. We attribute the success of CACTUs-based meta-learning\nover the embedding-based methods to two factors: its practice in distinguishing between many dis-\ntinct sets of clusters from modest amounts of signal, and the underlying classes of the meta-testing\nsplit data being out-of-distribution. In principle, the latter factor is solely responsible for the success\nover embedding cluster matching, since this algorithm can be viewed as a meta-learner on embed-\ndings that trivially obtains perfect accuracy (via memorization) on the meta-training tasks. The same\nfactor also helps explain why training from standard network initialization is, in general, competi-\ntive with directly using the task-generating embedding as a representation. On the other hand, the\nMNIST results (Table 7 in Appendix F) suggest that when the meta-training and meta-testing data\ndistributions have perfect overlap and the embedding is well-suited enough that embedding cluster\nmatching can already achieve high performance, CACTUs-MAML yields only a small beneﬁt, as\nwe would expect.\nTable 1: Results of unsupervised learning on Omniglot images, averaged over 1000 downstream character\nrecognition tasks. CACTUs experiments use k = 500 clusters for each of P = 100 partitions. Embedding\ncluster matching uses the same k. For complete results with conﬁdence intervals, see Table 8 in Appendix F.\nAlgorithm\n(way, shot)\n(5, 1)\n(5, 5)\n(20, 1)\n(20, 5)\nTraining from scratch\n52.50%\n74.78%\n24.91%\n47.62%\nACAI knn-nearest neighbors\n57.46%\n81.16%\n39.73%\n66.38%\nACAI linear classiﬁer\n61.08%\n81.82%\n43.20%\n66.33%\nACAI MLP with dropout\n51.95%\n77.20%\n30.65%\n58.62%\nACAI cluster matching\n54.94%\n71.09%\n32.19%\n45.93%\nACAI CACTUs-MAML (ours)\n68.84%\n87.78%\n48.09%\n73.36%\nACAI CACTUs-ProtoNets (ours)\n68.12%\n83.58%\n47.75%\n66.27%\nBiGAN knn-nearest neighbors\n49.55%\n68.06%\n27.37%\n46.70%\nBiGAN linear classiﬁer\n48.28%\n68.72%\n27.80%\n45.82%\nBiGAN MLP with dropout\n40.54%\n62.56%\n19.92%\n40.71%\nBiGAN cluster matching\n43.96%\n58.62%\n21.54%\n31.06%\nBiGAN CACTUs-MAML (ours)\n58.18%\n78.66%\n35.56%\n58.62%\nBiGAN CACTUs-ProtoNets (ours)\n54.74%\n71.69%\n33.40%\n50.62%\nOracle-MAML (control)\n94.46%\n98.83%\n84.60%\n96.29%\nOracle-ProtoNets (control)\n98.35%\n99.58%\n95.31%\n98.81%\nDifferent embedding spaces. CACTUs is effective for a variety of embedding learning methods\nused for task generation. The performance of unsupervised meta-learning can largely be predicted\nby the performance of the embedding-based non-meta-learning methods. For example, the ACAI\nembedding does well with Omniglot, leading to the best unsupervised results with ACAI CACTUs-\nMAML. Likewise, on miniImageNet, the best performing prior embedding (DeepCluster) also cor-\nresponds to the best performing unsupervised meta-learner (DeepCluster CACTUs-MAML).\nApplicability to different tasks. CACTUs-MAML learns an effective prior for a variety of task\ntypes. This can be attributed to the application-agnostic task-generation process and the expressive\npower of MAML (Finn & Levine, 2018). We also observe that, despite all meta-learning models\nbeing trained for N-way 1-shot classiﬁcation of unsupervised tasks, the models work well for a\nvariety of M-way K-shot tasks, where M ≤N and K ≤50. As mentioned previously, the\nrepresentation that CACTUs-ProtoNets learns is best suited for downstream tasks which match the\nsingle shot used for meta-training.\nOracle. The penalty for not having ground truth labels to construct near-optimal tasks ranges from\nsubstantial to severe, depending on the difﬁculty of the downstream task. Easier downstream tasks\n(which have fewer classes and/or more supervision) incur less of a penalty. We conjecture that\nwith such tasks, the difference in the usefulness of the priors matters less since the downstream\ntask-speciﬁc evidence has more power to shape the posterior.\n8\nPublished as a conference paper at ICLR 2019\nTable 2: Results of unsupervised learning on miniImageNet and CelebA images, averaged over 1000 down-\nstream human-designed tasks. CACTUs experiments use k = 500 for each of P = 50 partitions. Embedding\ncluster matching uses the same k. For complete results with conﬁdence intervals, see Tables 9 and 10 in\nAppendix F.\nminiImageNet\nCelebA\nAlgorithm\n(way, shot)\n(5, 1)\n(5, 5)\n(5, 20)\n(5, 50)\n(2, 5)\nTraining from scratch\n27.59%\n38.48%\n51.53%\n59.63%\n63.19%\nBiGAN knn-nearest neighbors\n25.56%\n31.10%\n37.31%\n43.60%\n56.15%\nBiGAN linear classiﬁer\n27.08%\n33.91%\n44.00%\n50.41%\n58.44%\nBiGAN MLP with dropout\n22.91%\n29.06%\n40.06%\n48.36%\n56.26%\nBiGAN cluster matching\n24.63%\n29.49%\n33.89%\n36.13%\n56.20%\nBiGAN CACTUs-MAML (ours)\n36.24%\n51.28%\n61.33%\n66.91%\n74.98%\nBiGAN CACTUs-ProtoNets (ours)\n36.62%\n50.16%\n59.56%\n63.27%\n65.58%\nDeepCluster knn-nearest neighbors\n28.90%\n42.25%\n56.44%\n63.90%\n61.47%\nDeepCluster linear classiﬁer\n29.44%\n39.79%\n56.19%\n65.28%\n59.57%\nDeepCluster MLP with dropout\n29.03%\n39.67%\n52.71%\n60.95%\n60.65%\nDeepCluster cluster matching\n22.20%\n23.50%\n24.97%\n26.87%\n51.51%\nDeepCluster CACTUs-MAML (ours)\n39.90%\n53.97%\n63.84%\n69.64%\n73.79%\nDeepCluster CACTUs-ProtoNets (ours)\n39.18%\n53.36%\n61.54%\n63.55%\n74.15%\nOracle-MAML (control)\n46.81%\n62.13%\n71.03%\n75.54%\n87.10%\nOracle-ProtoNets (control)\n46.56%\n62.29%\n70.05%\n72.04%\n85.13%\nTask construction ablation. As seen in Tables 3 and 4, CACTUs-MAML consistently outperforms\nHyperplanes-MAML with any margin. We hypothesize that this is due to the issues with zero-margin\nHyperplanes-MAML pointed out in Appendix C, and the fact that nonzero-margin Hyperplanes-\nMAML is able to use less of the meta-training split to generate tasks than CACTUs-MAML is.\nWe ﬁnd that using multiple partitions for CACTUs-MAML, while beneﬁcial, is not strictly neces-\nsary. Using non-zero margin with Hyperplanes-MAML is crucial for miniImageNet, but not for\nOmniglot. We conjecture that the enforced degree of separation between classes is needed for mini-\nImageNet because of the dataset’s high diversity. Meta-learning on random tasks or tasks derived\nfrom pixel-space clustering (Table 3) results in a prior that is much less useful than any other con-\nsidered algorithm, including a random network initialization; evidently, practicing badly is worse\nthan not practicing at all.\nNote on overﬁtting. Because of the combinatorially many unsupervised tasks we can create from\nmultiple partitions of the dataset, we do not observe substantial overﬁtting to the unsupervised meta-\ntraining tasks. However, we observe that meta-training performance is sometimes worse than meta-\ntest time performance, which is likely due to a portion of the automatically generated tasks being\nbased on nonsensical clusters (for examples, see Figure 2). Additionally, we ﬁnd that, with a few\nexceptions, using multiple partitions has a regularizing effect on the meta-learner: a diverse task set\nreduces overﬁtting to the meta-training tasks and increases the applicability of the learned prior.\nTable 3: Ablation study of task construction methods on Omniglot. For a more complete set of results with\nconﬁdence intervals, see Table 8 in Appendix F.\nAlgorithm\n(way, shot)\n(5, 1)\n(5, 5)\n(20, 1)\n(20, 5)\nRandom-MAML, P = 2400, k = 500\n25.99%\n25.74%\n6.51%\n6.74%\nPixels CACTUs-MAML, P = 1, k = 500\n30.55%\n40.19%\n12.05%\n19.01%\nACAI Hyperplanes-MAML, P = 2400, m = 0\n62.34%\n81.81%\n39.30%\n63.18%\nACAI Hyperplanes-MAML, P = 2400, m = 1.2\n62.44%\n83.20%\n41.86%\n65.23%\nACAI CACTUs-MAML, P = 1, k = 500\n66.49%\n85.60%\n45.04%\n69.14%\nACAI CACTUs-MAML, P = 100, k = 500\n68.84%\n87.78%\n48.09%\n73.36%\nBiGAN Hyperplanes-MAML, P = 2400, m = 0\n53.60%\n74.60%\n29.02%\n50.77%\nBiGAN Hyperplanes-MAML, P = 2400, m = 0.5\n53.18%\n73.55%\n29.98%\n50.14%\nBiGAN CACTUs-MAML, P = 1, k = 500\n55.92%\n76.28%\n32.44%\n54.22%\nBiGAN CACTUs-MAML, P = 100, k = 500\n58.18%\n78.66%\n35.56%\n58.62%\n9\nPublished as a conference paper at ICLR 2019\nTable 4: Ablation study of task construction methods on miniImageNet. For a more complete set of results\nwith conﬁdence intervals, see Table 9 in Appendix F.\nAlgorithm\n(way, shot)\n(5, 1)\n(5, 5)\n(5, 20)\n(5, 50)\nBiGAN Hyperplanes-MAML, P = 4800, m = 0\n20.00%\n20.00%\n20.00%\n20.00%\nBiGAN Hyperplanes-MAML, P = 4800, m = 0.9\n29.67%\n41.92%\n51.32%\n54.72%\nBiGAN CACTUs-MAML, P = 1, k = 500\n37.75%\n52.59%\n62.70%\n67.98%\nBiGAN CACTUs-MAML, P = 50, k = 500\n36.24%\n51.28%\n61.33%\n66.91%\nDeepCluster Hyperplanes-MAML, P = 4800, m = 0\n20.02%\n20.01%\n20.00%\n20.01%\nDeepCluster Hyperplanes-MAML, P = 4800, m = 0.1\n35.85%\n49.54%\n60.68%\n65.55%\nDeepCluster CACTUs-MAML, P = 1, k = 500\n38.75%\n52.73%\n62.72%\n67.77%\nDeepCluster CACTUs-MAML, P = 50, k = 500\n39.90%\n53.97%\n63.84%\n69.64%\n5\nDISCUSSION\nWe demonstrate that meta-learning on tasks produced using simple mechanisms based on embed-\ndings improves upon the utility of these representations in learning downstream, human-speciﬁed\ntasks. We empirically show that this holds across benchmark datasets and tasks in the few-shot\nclassiﬁcation literature (Santoro et al., 2016; Ravi & Larochelle, 2017; Finn et al., 2018), task difﬁ-\nculties, and embedding learning methods while ﬁxing key hyperparameters across all experiments.\nIn a sense, CACTUs can be seen as a facilitating interface between an embedding learning method\nand a meta-learning algorithm. As shown in the results, the meta-learner’s performance signiﬁcantly\ndepends on the nature and quality of the task-generating embeddings. We can expect our method\nto yield better performance as the methods that produce these embedding functions improve, be-\ncoming better suited for generating diverse yet distinctive clusterings of the data. However, the gap\nbetween unsupervised and supervised meta-learning will likely persist because, with the latter, the\nmeta-training task distribution is human-designed to mimic the expected evaluation task distribu-\ntion as much as possible. Indeed, to some extent, supervised meta-learning algorithms ofﬂoad the\neffort of designing and tuning algorithms onto the effort of designing and tuning task distributions.\nWith its evaluation-agnostic task generation, CACTUs-based meta-learning trades off performance\nin speciﬁc use-cases for broad applicability and the ability to train on unlabeled data. In principle,\nCACTUs-based meta-learning may outperform supervised meta-learning when the latter is trained\non a misaligned task distribution. We leave this investigation to future work.\nWhile we have demonstrated that k-means is a broadly useful mechanism for constructing tasks\nfrom embeddings, it is unlikely that combinations of k-means clusters in learned embedding spaces\nare universal approximations of arbitrary class deﬁnitions. An important direction for future work\nis to ﬁnd examples of datasets and human-designed tasks for which CACTUs-based meta-learning\nresults in ineffective downstream learning. This will result in better understanding of the practical\nscope of applicability for our method, and spur further development in automatic task construction\nmechanisms for unsupervised meta-learning.\nA potential concern of our experimental evaluation is that MNIST, Omniglot, and miniImageNet\nexhibit particular structure in the underlying class distribution (i.e., perfectly balanced classes), since\nthey were designed to be supervised learning benchmarks. In more practical applications of machine\nlearning, such structure would likely not exist. Our CelebA results indicate that CACTUs is effective\neven in the case of a dataset without neatly balanced classes or attributes. An interesting direction\nfor future work is to better characterize the performance of CACTUs and other unsupervised pre-\ntraining methods with highly-unstructured, unlabeled datasets.\nSince MAML and ProtoNets produce nothing more than a learned representation, our method can\nbe viewed as deriving, from a previous unsupervised representation, a new representation particu-\nlarly suited for learning downstream tasks. Beyond visual classiﬁcation tasks, the notion of using\nunsupervised pre-training is generally applicable to a wide range of domains, including regression,\nspeech (Oord et al., 2018), language (Howard & Ruder, 2018), and reinforcement learning (Shel-\nhamer et al., 2017). Hence, our unsupervised meta-learning approach has the potential to improve\nunsupervised representations for a variety of such domains, an exciting avenue for future work.\n10\nPublished as a conference paper at ICLR 2019\nACKNOWLEDGMENTS\nWe thank Kelvin Xu, Richard Zhang, Brian Cheung, Ben Poole, A¨aron van den Oord, Luke Metz,\nSiddharth Reddy, and the anonymous reviewers for feedback on an early draft of this paper.\nREFERENCES\nYoshua Bengio, Samy Bengio, and Jocelyn Cloutier. Learning a synaptic learning rule. In Interna-\ntional Joint Conference on Neural Networks (IJCNN), 1991.\nYoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training\nof deep networks. In Advances in Neural Information Processing Systems (NIPS), 2007.\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new\nperspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2013.\nDavid Berthelot, Colin Raffel, Aurko Roy, and Ian Goodfellow.\nUnderstanding and improving\ninterpolation in autoencoders via an adversarial regularizer. arXiv preprint arXiv:1807.07543,\n2018.\nPiotr Bojanowski and Armand Joulin. Unsupervised learning by predicting noise. In International\nConference on Machine Learning (ICML), 2017.\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for un-\nsupervised learning of visual features. In European Conference on Computer Vision (ECCV),\n2018.\nXi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. InfoGAN:\nInterpretable representation learning by information maximizing generative adversarial nets. In\nNeural Information Processing Systems (NIPS), 2016.\nBrian Cheung, Jesse A Livezey, Arjun K Bansal, and Bruno A Olshausen. Discovering hidden\nfactors of variation in deep networks. In International Conference on Learning Representations\n(ICLR), 2015.\nAdam Coates and Andrew Y Ng. Learning feature representations with k-means. In Neural Net-\nworks: Tricks of the Trade. Springer, 2012.\nAndrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Neural Information Pro-\ncessing Systems (NIPS), 2015.\nEmily L Denton and Vighnesh Birodkar. Unsupervised learning of disentangled representations\nfrom video. In Neural Information Processing Systems (NIPS), 2017.\nJeff Donahue, Philipp Kr¨ahenb¨uhl, and Trevor Darrell. Adversarial feature learning. In International\nConference on Learning Representations (ICLR), 2017.\nVincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Ar-\njovsky, and Aaron Courville. Adversarially learned inference. In International Conference on\nLearning Representations (ICLR), 2017.\nDumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, and\nSamy Bengio. Why does unsupervised pre-training help deep learning?\nJournal of Machine\nLearning Research (JMLR), 2010.\nChelsea Finn and Sergey Levine. Meta-learning and universality: Deep representations and gradi-\nent descent can approximate any learning algorithm. In International Conference on Learning\nRepresentations (ICLR), 2018.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation\nof deep networks. In International Conference on Machine Learning (ICML), 2017.\nChelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. In Neural\nInformation Processing Systems (NIPS), 2018.\n11\nPublished as a conference paper at ICLR 2019\nVikas K Garg and Adam Kalai.\nSupervising unsupervised learning.\narXiv preprint\narXiv:1709.05262, 2017.\nAbhishek Gupta, Benjamin Eysenbach, Chelsea Finn, and Sergey Levine.\nUnsupervised meta-\nlearning for reinforcement learning. arXiv preprint arXiv:1806.04640, 2018.\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman. Unsupervised learning. In The Elements of\nStatistical Learning. Springer, 2009.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual\nnetworks. European Conference on Computer Vision (ECCV), 2016.\nIrina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,\nShakir Mohamed, and Alexander Lerchner. β-VAE: Learning basic visual concepts with a con-\nstrained variational framework. In International Conference on Learning Representations (ICLR),\n2017.\nGeoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief\nnets. Neural Computation, 2006.\nJeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁcation.\nIn Association for Computational Linguistics (ACL), 2018.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In International Conference on Machine Learning (ICML), 2015.\nJaehong Kim, Sangyeul Lee, Sungwan Kim, Moonsu Cha, Jung Kwon Lee, Youngduck Choi,\nYongseok Choi, Dong-Yeon Cho, and Jiwon Kim. Auto-Meta: Automated gradient based meta\nlearner search. arXiv preprint arXiv:1806.06927, 2018.\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of\nthe 3rd International Conference on Learning Representations (ICLR), 2014.\nDiederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised\nlearning with deep generative models. In Neural Information Processing Systems (NIPS), 2014.\nPhilipp Kr¨ahenb¨uhl, Carl Doersch, Jeff Donahue, and Trevor Darrell. Data-dependent initializa-\ntions of convolutional neural networks. In International Conference on Learning Representations\n(ICLR), 2016.\nQuoc V Le, Marc’Aurelio Ranzato, Rajat Monga, Matthieu Devin, Gregory S. Corrado, Kai Chen,\nJeffrey Dean, and Andrew Y Ng. Building high-level features using large scale unsupervised\nlearning. In International Conference on Acoustics, Speech and Signal Processing (ICASSP),\n2013.\nMichael F Mathieu, Junbo Jake Zhao, Junbo Zhao, Aditya Ramesh, Pablo Sprechmann, and Yann\nLeCun. Disentangling factors of variation in deep representation using adversarial training. In\nNeural Information Processing Systems (NIPS), 2016.\nLuke Metz, Niru Maheswaranathan, Brian Cheung, and Jascha Sohl-Dickstein. Learning to learn\nwithout labels. In International Conference on Learning Representations (ICLR), 2018.\nTsendsuren Munkhdalai and Hong Yu. Meta networks. In International Conference on Machine\nLearning (ICML), 2017.\nDevang K Naik and RJ Mammone. Meta-neural networks that learn by learning. In International\nJoint Conference on Neural Networks (IJCNN), 1992.\nAvital Oliver, Augustus Odena, Colin Raffel, Ekin D Cubuk, and Ian J Goodfellow. Realistic eval-\nuation of deep semi-supervised learning algorithms. In International Conference on Learning\nRepresentations (ICLR), 2018.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-\ntive coding. arXiv preprint arXiv:1807.03748, 2018.\n12\nPublished as a conference paper at ICLR 2019\nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep\nconvolutional generative adversarial networks. In International Conference on Learning Repre-\nsentations (ICLR), 2016.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\nstanding by generative pre-training. Preprint, 2018.\nPrajit Ramachandran, Peter J Liu, and Quoc V Le. Unsupervised pretraining for sequence to se-\nquence learning. In Empirical Methods in Natural Language Processing (EMNLP), 2017.\nMarc’Aurelio Ranzato, Christopher Poultney, Sumit Chopra, and Yann LeCun. Efﬁcient learning of\nsparse representations with an energy-based model. In Neural Information Processing Systems\n(NIPS), 2006.\nAntti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko.\nSemi-\nsupervised learning with ladder networks. In Neural Information Processing Systems (NIPS),\n2015.\nSachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International\nConference on Learning Representations (ICLR), 2017.\nScott Reed, Kihyuk Sohn, Yuting Zhang, and Honglak Lee. Learning to disentangle factors of\nvariation with manifold interaction. In International Conference on Machine Learning (ICML),\n2014.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Fei-Fei\nLi. ImageNet large scale visual recognition challenge. International Journal of Computer Vision\n(IJCV), 2015.\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\nImproved techniques for training GANs. In Neural Information Processing Systems (NIPS), 2016.\nAdam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-\nlearning with memory-augmented neural networks.\nIn International Conference on Machine\nLearning (ICML), 2016.\nJ¨urgen Schmidhuber. Evolutionary principles in self-referential learning. PhD thesis, Institut f¨ur\nInformatik, Technische Universit¨at M¨unchen, 1987.\nEvan Shelhamer, Parsa Mahmoudieh, Max Argus, and Trevor Darrell. Loss is its own reward: Self-\nsupervision for reinforcement learning. In International Conference on Learning Representations\n(ICLR), 2017.\nJake Snell, Kevin Swersky, and Richard S Zemel. Prototypical networks for few-shot learning. In\nNeural Information Processing Systems (NIPS), 2017.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: A simple way to prevent neural networks from overﬁtting. Journal of Machine Learning\nResearch (JMLR), 2014.\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and\ncomposing robust features with denoising autoencoders. In International Conference on Machine\nLearning (ICML), 2008.\nPascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol.\nStacked denoising autoencoders: Learning useful representations in a deep network with a local\ndenoising criterion. Journal of Machine Learning Research (JMLR), 2010.\nOriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Match-\ning networks for one shot learning. In Neural Information Processing Systems (NIPS), 2016.\nDong Yu, Li Deng, and George Dahl. Roles of pre-training and ﬁne-tuning in context-dependent\nDBN-HMMs for real-world speech recognition. In NIPS Workshop on Deep Learning and Unsu-\npervised Feature Learning, 2010.\n13\nPublished as a conference paper at ICLR 2019\nRichard Zhang, Phillip Isola, and Alexei A Efros. Split-brain autoencoders: Unsupervised learning\nby cross-channel prediction. In Computer Vision and Pattern Recognition (CVPR), 2017.\nXiaojin Zhu. Semi-supervised learning. In Encyclopedia of Machine Learning. Springer, 2011.\n14\nPublished as a conference paper at ICLR 2019\nAPPENDIX A\nTHE EMBEDDING LEARNING ZOO\nWe evaluate four distinct methods from prior work for learning the task-generating embeddings.\nIn adversarially constrained autoencoder interpolation (ACAI), a convolutional autoencoder’s pixel-\nwise L2 loss is regularized with a term encouraging meaningful interpolations in the latent\nspace (Berthelot et al., 2018). Speciﬁcally, a critic network takes as input a synthetic image gener-\nated from a convex combination of the latents of two dataset samples, and regresses to the mixing\nfactor. The decoder of the autoencoder and the generator for the critic are one and the same. The reg-\nularization term is minimized when the autoencoder fools the critic into predicting that the synthetic\nimage is a real sample.\nThe bidirectional GAN (BiGAN) is an instance of a generative-adversarial framework in which\nthe generator produces both synthetic image and embedding from real embedding and image, re-\nspectively (Donahue et al., 2017; Dumoulin et al., 2017). Discrimination is done in joint image-\nembedding space.\nThe DeepCluster method does discriminative clustering by alternating between clustering the fea-\ntures of a convolutional neural network and using the clusters as labels to optimize the network\nweights via backpropagating a standard classiﬁcation loss (Caron et al., 2018).\nThe InfoGAN framework conceptually decomposes the generator’s input into a latent code and\nincompressible noise (Chen et al., 2016). The structure of the latent code is hand-speciﬁed based\non knowledge of the dataset. The canonical GAN minimax objective is regularized with a mutual\ninformation term between the code and the generated image. In practice, this term is optimized using\nvariational inference, involving the approximation of the posterior with an auxiliary distribution\nQ(code|image) parameterized by a recognition network.\nWhereas ACAI explicitly optimizes pixel-wise reconstruction error, BiGAN only encourages the\nﬁdelity of generated image and latent samples with respect to their respective prior distributions.\nWhile InfoGAN also encourages the ﬁdelity of generated images, it leverages domain-speciﬁc\nknowledge to impose a favorable structure on the embedding space and information-theoretic meth-\nods for optimization. DeepCluster departs from the aforementioned methods in that it is not con-\ncerned with generation or decoding, and only seeks to learn general-purpose visual features by way\nof end-to-end discriminative clustering.\nAPPENDIX B\nDATASET INFORMATION\nThe Omniglot dataset consists of 1623 characters each with 20 hand-drawn examples. Ignoring\nthe alphabets from which the characters originate, we use 1100, 100, and 423 characters for our\nmeta-training, meta-validation, and meta-testing splits. The miniImageNet dataset consists of 100\nclasses each with 600 examples. The images are predominantly natural and realistic. We use the\nsame meta-training/meta-validation/meta-testing splits of 64/16/20 classes as proposed by Ravi &\nLarochelle (2017). The CelebA dataset includes 202,599 facial images of celebrities and 40 binary\nattributes that annotate every image. We follow the prescribed 162,770/19,867/19,962 data split.\nFor Omniglot and miniImageNet, supervised meta-learning tasks and evaluation tasks are con-\nstructed exactly as detailed in Section 2.2: for an N-way K-shot task with Q queries per class,\nwe sample N classes from the data split and K + Q datapoints per class, labeling the task’s data\nwith a random permutation of N one-hot vectors.\nFor CelebA, we consider binary classiﬁcation tasks (i.e., 2-way), each deﬁned by 3 attributes and\nan ordering of 3 Booleans, one for each attribute. Every image in a task-speciﬁc class shares\nall task-speciﬁc attributes with each other and none with images in the other class. For exam-\nple, the task illustrated in Figure 2 involves distinguishing between images whose subjects sat-\nisfy not Sideburns, Straight Hair, and not Young, and those whose subjects satisfy\nSideburns, not Straight Hair, and Young. To keep with the idea of having distinct\nclasses for meta-training and meta-testing, we split the task-deﬁning attributes. For the supervised\nmeta-learning oracle, we construct meta-training tasks from the ﬁrst 20 attributes (when alphabet-\nically ordered), meta-validation tasks from the next 10, and meta-testing tasks from the last 10.\nDiscarding tasks with too few examples in either class, this results in 4287, 391, and 402 task\n15\nPublished as a conference paper at ICLR 2019\nprototypes (but many more possible tasks). We use the same meta-test time tasks to evaluate the\nunsupervised methods. We only consider assessment with 5-shot tasks because, given that there are\nmultiple attributes other than the task-deﬁning ones, any 1-shot task is likely to be ill-deﬁned.\nAPPENDIX C\nTASK CONSTRUCTION VIA RANDOM HYPERPLANES\nGiven a set of embedding points {zi} in a space Z, a simple way of deﬁning a partition P = {Cc}\non {zi} is to use random hyperplanes to slice Z into subspaces and assign the embeddings that lie\nin the c-th subspace to subset Cc. However, a hyperplane slicing can group together two arbitrarily\nfar embeddings, or separate two arbitrarily close ones; given our assumption that good embedding\nspaces have a semantically meaningful metric, this creates ill-deﬁned classes. This problem can be\npartially alleviated by extending the hyperplane boundaries with a non-zero margin, as empirically\nshown in Section 4.2.\nWe now describe how to generate tasks via random hyperplanes in the embedding space. We ﬁrst\ndescribe a procedure to generate a partition P of the set of embeddings {zi} for constructing meta-\ntraining tasks. A given hyperplane slices the embedding space into two, so for an N-way task, we\nneed H = ⌈log2 N⌉hyperplanes to deﬁne sufﬁciently many subsets/classes for a task. To randomly\ndeﬁne a hyperplane in d-dimensional embedding space, we sample a normal vector n and a point\non the plane z0, each with d elements. For an embedding point z, the signed point-plane distance\nis given by\nn\n|n|2 · (z −z0). Deﬁning H hyperplanes in this manner, we discard embeddings for\nwhich the signed point-plane distance to any of the H hyperplanes lies within (−m, m), where m\nis a desired margin. The H hyperplanes collectively deﬁne 2H subspaces. We assign embedding\npoints in the c-th subspace to subset Cc. We deﬁne the partition as P = {Cc}. We prune subsets that\ndo not have at least R = Km-tr + Q members, and check that the partition has at least N remaining\nsubsets; if not, we reject the partition and restart the procedure. After obtaining partitions {Pp},\nmeta-training tasks can be generated by following Algorithm 1 from Line 4.\nIn terms of practical implementation, we pre-compute 1000 hyperplanes and pruned pairs of subsets\nof {zi}. We generate partitions by sampling combinations of the hyperplanes and taking intersec-\ntions of their associated subsets to deﬁne the elements of the partition. We determine the number of\npartitions needed for a given Hyperplanes-MAML run by the number of meta-training tasks desired\nfor the meta-learner: we ﬁx 100 tasks per partition.\nAPPENDIX D\nMNIST EXPERIMENTS\nThe MNIST dataset consists of 70,000 hand-drawn examples of the 10 numerical digits. Our split\nrespects the original MNIST 60,000/10,000 training/testing split. We assess on 10-way classiﬁcation\ntasks. This setup results in examples from all 10 digits being present for both meta-training and\nmeta-testing, making the probem setting essentially equivalent to that of semi-supervised learning\nsans a ﬁxed permutation of the labels. The MNIST scenario is thus a special case of the problem\nsetting considered in the rest of the paper. For MNIST, we only experiment with MAML as the\nmeta-learning algorithm.\nFor ACAI and InfoGAN we constructed the meta-validation split from the last 5,000 examples of the\nmeta-training split; for BiGAN this ﬁgure was 10,000. After training the ACAI model and inferring\nembeddings, manually assigning labels to 10 clusters by inspection results in a classiﬁcation accu-\nracy of 96.00% on the testing split. As the ACAI authors observe, we found it important to whiten\nthe ACAI embeddings before clustering. The same metric for the InfoGAN embedding (taking an\nargmax over the categorical dimensions instead of actually running clustering) is 96.83%. Note that\nthese results are an upper-bound for embedding cluster matching. To see this, consider the 10-way\n1-shot scenario. 1 example sampled from each cluster is insufﬁcient to guarantee the optimal label\nfor that cluster; 1 example sampled from each label is not guaranteed to each end up in the optimal\ncategory.\nAside from CACTUs-MAML, embedding knn-nearest neighbors, embedding linear classiﬁer, and\nembedding direct clustering, we also ran CACTUs-MAML on embeddings instead of raw images,\nusing a simple model with 2 hidden layers with 64 units each and ReLU activation, and all other\nMAML hyperparameters being the same as in Table 5.\n16\nPublished as a conference paper at ICLR 2019\nDeparting from the ﬁxed k = 500 used for all other datasets, we deliberately use k = 10 to better\nunderstand the limitations of CACTUs-MAML. The results can be seen in Table 7 in Appendix B.\nIn brief, with the better embeddings (ACAI and InfoGAN), there is only little beneﬁt of CACTUs-\nMAML over embedding cluster matching. Additionally, even in the best cases, CACTUs-MAML\nfalls short of state-of-the-art semi-supervised learning methods.\nAPPENDIX E\nHYPERPARAMETERS AND ARCHITECTURES\nE.1\nMAML\nTable 5: MAML hyperparameter summary.\nHyperparameter\nMNIST\nOmniglot\nminiImageNet\nCelebA\nInput size\n28 × 28\n28 × 28\n84 × 84 × 3\n84 × 84 × 3\nOuter (meta) learning rate\n0.001\n0.001\n0.001\n0.001\nInner learning rate\n0.05\n0.05\n0.05\n0.05\nTask batch size\n8\n8\n8\n8\nInner adaptation steps (meta-training)\n5\n5\n5\n5\nMeta-training iterations\n30,000\n30,000\n60,000\n60,000\nAdaptation steps (evaluation)\n50\n50\n50\n50\nClasses per task (meta-training)\n10\n20\n5\n2\nShots per class (meta-training)\n1\n1\n1\n1\nQueries per class\n5\n5\n5\n5\nFor MNIST and Omniglot we use the same 4-block convolutional architecture as used by Finn et al.\n(2017) for their Omniglot experiments, but with 32 ﬁlters (instead of 64) for each convolutional layer\nfor consistency with the model used for miniImageNet and CelebA, which is the same as what Finn\net al. (2017) used for their miniImageNet experiments. When evaluating the meta-learned 20-way\nOmniglot model with 5-way tasks, we prune the unused output dimensions. The outer optimizer\nis Adam (Kingma & Ba, 2014), and the inner optimizer is SGD. We build on the authors’ publicly\navailable codebase found at https://github.com/cbfinn/maml.\nWhen using batch normalization (Ioffe & Szegedy, 2015) to process a task’s training or query in-\nputs, we observe that using only 1 query datapoint per class can allow the model to exploit batch\nstatistics, learning a strategy analogous to a process of elimination that causes signiﬁcant, but spuri-\nous, improvement in accuracy. To mitigate this, we ﬁx 5 queries per class for every task’s evaluation\nphase, meta-training or meta-testing.\nE.2\nPROTONETS\nTable 6: ProtoNets hyperparameter summary.\nHyperparameter\nOmniglot\nminiImageNet\nCelebA\nInput size\n28 × 28\n84 × 84 × 3\n84 × 84 × 3\nLearning rate\n0.001\n0.001\n0.001\nTask batch size\n1\n1\n1\nTraining iterations\n30,000\n60,000\n60,000\nClasses per task (meta-training)\n20\n5\n2\nShots per class (meta-training)\n1\n1\n1\nQueries per class (meta-training/meta-testing)\n15/5\n15/5\n15/5\nFor the three considered datasets we use the same architecture as used by Snell et al. (2017) for their\nOmniglot and miniImageNet experiments. This is a 4-block convolutional architecture with each\n17\nPublished as a conference paper at ICLR 2019\nblock consisting of a convolutional layer with 64 3 × 3 ﬁlters, stride 1, and padding 1, followed\nby BatchNorm, ReLU activation, and 2 × 2 MaxPooling. The ProtoNets embedding is simply the\nﬂattened output of the last block. We follow the authors and use the Adam optimizer, but do not\nuse a learning rate scheduler. We build upon the authors’ publicly available codebase found at\nhttps://github.com/jakesnell/prototypical-networks.\nE.3\nCACTUS\nFor Omniglot, miniImageNet, and CelebA we ﬁx the number of clusters k to be 500. For Omniglot\nwe choose the number of partitions P = 100, but in the interest of keeping runtime manageable,\nchoose P = 50 for miniImageNet and CelebA.\nE.4\nUSE OF UNSUPERVISED LEARNING METHODS\nACAI (Berthelot et al., 2018): We run ACAI for MNIST and Omniglot.\nWe pad the images\nby 2 and use the authors’ architecture. We use a 256-dimensional embedding for all datasets.\nWe build upon the authors’ publicly available codebase found at https://github.com/\nbrain-research/acai.\nWe unsuccessfully try running ACAI on 64 × 64 miniImageNet and CelebA. To facilitate this input\nsize, we add one block consisting of two convolutional layers (512 ﬁlters each) and one down-\nsampling/upsampling layer to the encoder and decoder. However, because of ACAI’s pixel-wise\nreconstruction loss, for these datasets the ACAI embedding prioritizes information about the few\n“features” that dominate the reconstruction pixel count, resulting in clusters that only corresponded\nto a limited range of factors, such as background color and pose. For curiosity’s sake, we tried run-\nning meta-learning on tasks derived from these uninteresting clusters anyways, and found that the\nmeta-learner quickly produced a learning procedure that obtained high accuracy on the meta-training\ntasks. However, this learned prior was not useful for solving downstream tasks.\nBiGAN (Donahue et al., 2017): For MNIST, we follow the BiGAN authors and specify a uni-\nform 50-dimensional prior on the unit hypercube for the latent. The BiGAN authors use a 200-\ndimensional version of the same prior for their ImageNet experiments, so we follow suit for Om-\nniglot, miniImageNet, and CelebA. For MNIST and Omniglot, we use the permutation-invariant\narchitecture (i.e. fully connected layers only) used by the authors for their MNIST results; for mini-\nImageNet and CelebA, we randomly crop to 64×64 and use the AlexNet-inspired architecture used\nby Donahue et al. (2017) for their ImageNet results. We build upon the authors’ publicly available\ncodebase found at https://github.com/jeffdonahue/bigan.\nDeepCluster (Caron et al., 2018): We run DeepCluster for miniImageNet and CelebA, which we\nrespectively randomly crop and resize to 64 × 64. We modify the ﬁrst layer of the AlexNet archi-\ntecture used by the authors to accommodate this input size. We follow the authors and use the input\nto the (linear) output layer as the embedding. These are 4096-dimensional, so we follow the au-\nthors and apply PCA to reduce the dimensionality to 256, followed by whitening. We build upon the\nauthors’ publicly available codebase found at https://github.com/facebookresearch/\ndeepcluster.\nInfoGAN (Chen et al., 2016): We only run InfoGAN for MNIST. We follow the InfoGAN authors\nand specify the product of a 10-way categorical distribution and a 2-dimensional uniform distribution\nas the latent code. We use the authors’ architecture. Given an image, we use the recognition network\nto obtain its embedding. We build upon the authors’ publicly available codebase found at https:\n//github.com/openai/InfoGAN.\nAPPENDIX F\nEXPERIMENTAL RESULTS\nThis section containsfull experimental results for the MNIST, Omniglot, miniImageNet, and CelebA\ndatasets, including consolidated versions of the tables found in the main text. The metric is clas-\nsiﬁcation accuracy averaged over 1000 tasks based on human-speciﬁed labels of the testing split,\nwith 95% conﬁdence intervals. d: dimensionality of embedding, h: number of hidden units in a\nfully connected layer, k: number of clusters in a partition, P: number of partitions used during\nmeta-learning, m: margin on boundary-deﬁning hyperplanes.\n18\nPublished as a conference paper at ICLR 2019\nTable 7: MNIST digit classiﬁcation results averaged over 1000 tasks. ± denotes a 95% conﬁdence interval. k: number of clusters in a partition, P: number of partitions used during\nmeta-learning\nAlgorithm\n(way, shot)\n(10, 1)\n(10, 5)\n(10, 10)\nACAI, d = 256\nEmbedding knn-nearest neighbors\n74.49 ± 0.82 %\n88.80 ± 0.27 %\n91.90 ± 0.17 %\nEmbedding linear classiﬁer\n76.53 ± 0.81 %\n92.17 ± 0.25 %\n94.58 ± 0.15 %\nEmbedding cluster matching, k = 10\n91.28 ± 0.58 %\n95.92 ± 0.16 %\n96.01 ± 0.12 %\nCACTUs-MAML on images (ours), P = 1, k = 10\n92.66 ± 0.34 %\n96.08 ± 0.12 %\n96.29 ± 0.12 %\nCACTUs-MAML on embeddings (ours), P = 1, k = 10\n94.77 ± 0.28 %\n96.56 ± 0.11 %\n96.80 ± 0.11 %\nBiGAN, d = 50\nEmbedding knn-nearest neighbors\n29.25 ± 0.83 %\n44.59 ± 0.44 %\n51.98 ± 0.30 %\nEmbedding linear classiﬁer\n30.86 ± 0.89 %\n51.69 ± 0.44 %\n60.70 ± 0.31 %\nEmbedding cluster matching, k = 10\n33.72 ± 0.54 %\n50.21 ± 0.36 %\n52.95 ± 0.34%\nCACTUs-MAML on images (ours), P = 1, k = 10\n43.75 ± 0.46 %\n62.20 ± 0.33 %\n68.38 ± 0.29 %\nCACTUs-MAML on images (ours), P = 100, k = 10\n49.73 ± 0.45 %\n77.05 ± 0.30 %\n83.90 ± 0.24 %\nCACTUs-MAML on embeddings (ours), P = 1, k = 10\n36.33 ± 0.48 %\n51.78 ± 0.34 %\n57.41 ± 0.30 %\nCACTUs-MAML on embeddings (ours), P = 100, k = 10\n37.32 ± 0.41 %\n60.74 ± 0.34 %\n67.34 ± 0.30 %\nInfoGAN, d = 12\nEmbedding knn-nearest neighbors\n94.53 ± 0.51 %\n96.05 ± 0.17 %\n96.24 ± 0.12 %\nEmbedding linear classiﬁer\n95.78 ± 0.42 %\n96.61 ± 0.21 %\n96.85 ± 0.11 %\nEmbedding cluster matching, k = 10\n93.42 ± 0.57 %\n96.97 ± 0.15 %\n96.99 ± 0.10 %\nCACTUs-MAML on images (ours), P = 1, k = 10\n95.30 ± 0.23 %\n97.18 ± 0.10 %\n97.28 ± 0.10 %\nCACTUs-MAML on images (ours), P = 100, k = 10\n96.08 ± 0.19 %\n97.22 ± 0.10 %\n97.31 ± 0.09 %\nCACTUs-MAML on embeddings (ours), P = 1, k = 10\n96.69 ± 0.17 %\n97.13 ± 0.10 %\n97.23 ± 0.10 %\nCACTUs-MAML on embeddings (ours), P = 100, k = 10\n96.48 ± 0.17 %\n97.08 ± 0.10 %\n97.22 ± 0.10 %\nSupervised pre-training\nOracle-MAML (control)\n97.31 ± 0.17 %\n98.51 ± 0.07 %\n98.51 ± 0.07 %\n19\nPublished as a conference paper at ICLR 2019\nTable 8: Omniglot character classiﬁcation results averaged over 1000 tasks. ± denotes a 95% conﬁdence interval. d: dimensionality of embedding, h: number of hidden units in a\nfully connected layer, k: number of clusters in a partition, P: number of partitions used during meta-learning, m: margin on boundary-deﬁning hyperplanes.\nAlgorithm\n(way, shot)\n(5, 1)\n(5, 5)\n(20, 1)\n(20, 5)\nBaselines\nTraining from scratch\n52.50 ± 0.84 %\n74.78 ± 0.69 %\n24.91 ± 0.33 %\n47.62 ± 0.44 %\nRandom-MAML, P = 2400, k = 500\n25.99 ± 0.73 %\n25.74 ± 0.69 %\n6.51 ± 0.18 %\n6.74 ± 0.18 %\nPixels CACTUs-MAML, P = 1, k = 500\n30.55 ± 0.63 %\n40.19 ± 0.71 %\n12.05 ± 0.23 %\n19.01 ± 0.29 %\nACAI, d = 256\nEmbedding knn-nearest neighbors\n57.46 ± 1.35 %\n81.16 ± 0.57 %\n39.73 ± 0.38 %\n66.38 ± 0.36 %\nEmbedding linear classiﬁer\n61.08 ± 1.32 %\n81.82 ± 0.58 %\n43.20 ± 0.69 %\n66.33 ± 0.36 %\nEmbedding MLP with dropout, h = 128\n51.95 ± 0.82 %\n77.20 ± 0.65 %\n30.65 ± 0.39 %\n58.62 ± 0.41 %\nEmbedding cluster matching, k = 500\n54.94 ± 0.85 %\n71.09 ± 0.77 %\n32.19 ± 0.40 %\n45.93 ± 0.40 %\nHyperplanes-MAML (ours), P = 2400, m = 0\n62.34 ± 0.82 %\n81.81 ± 0.60 %\n39.30 ± 0.37 %\n63.18 ± 0.38 %\nHyperplanes-MAML (ours), P = 2400, m = 1.2\n62.44 ± 0.82 %\n83.20 ± 0.58 %\n41.86 ± 0.38 %\n65.23 ± 0.37 %\nCACTUs-MAML (ours), P = 1, k = 500\n66.49 ± 0.80 %\n85.60 ± 0.53 %\n45.04 ± 0.41 %\n69.14 ± 0.36 %\nCACTUs-MAML (ours), P = 100, k = 500\n68.84 ± 0.80 %\n87.78 ± 0.50 %\n48.09 ± 0.41 %\n73.36 ± 0.34 %\nCACTUs-ProtoNets (ours), P = 100, k = 500\n68.12 ± 0.84 %\n83.58 ± 0.61 %\n47.75 ± 0.43 %\n66.27 ± 0.37 %\nBiGAN, d = 200\nEmbedding knn-nearest neighbors\n49.55 ± 1.27 %\n68.06 ± 0.71 %\n27.37 ± 0.33 %\n46.70 ± 0.36 %\nEmbedding linear classiﬁer\n48.28 ± 1.25 %\n68.72 ± 0.66 %\n27.80 ± 0.61 %\n45.82 ± 0.37 %\nEmbedding MLP with dropout, h = 128\n40.54 ± 0.79 %\n62.56 ± 0.79 %\n19.92 ± 0.32 %\n40.71 ± 0.40 %\nEmbedding cluster matching, k = 500\n43.96 ± 0.80 %\n58.62 ± 0.78 %\n21.54 ± 0.32 %\n31.06 ± 0.37 %\nHyperplanes-MAML (ours), P = 2400, m = 0\n53.60 ± 0.82 %\n74.60 ± 0.69 %\n29.02 ± 0.33 %\n50.77 ± 0.39 %\nHyperplanes-MAML (ours), P = 2400, m = 0.5\n53.18 ± 0.81 %\n73.55 ± 0.69 %\n29.98 ± 0.35 %\n50.14 ± 0.38 %\nCACTUs-MAML (ours), P = 1, k = 500\n55.92 ± 0.80 %\n76.28 ± 0.65 %\n32.44 ± 0.35 %\n54.22 ± 0.39 %\nCACTUs-MAML (ours), P = 100, k = 500\n58.18 ± 0.81 %\n78.66 ± 0.65 %\n35.56 ± 0.36 %\n58.62 ± 0.38 %\nCACTUs-ProtoNets (ours), P = 100, k = 500\n54.74 ± 0.82 %\n71.69 ± 0.73 %\n33.40 ± 0.37 %\n50.62 ± 0.39 %\nSupervised meta-learning\nOracle-MAML (control)\n94.46 ± 0.35 %\n98.83 ± 0.12 %\n84.60 ± 0.32 %\n96.29 ± 0.13 %\nOracle-ProtoNets (control)\n98.35 ± 0.22 %\n99.58 ± 0.09 %\n95.31 ± 0.18 %\n98.81 ± 0.07 %\n† Result used 64 ﬁlters per convolutional layer, 3× data augmentation, and folded the validation set into the training set\nafter hyperparameter tuning.\n20\nPublished as a conference paper at ICLR 2019\nTable 9: miniImageNet object classiﬁcation results averaged over 1000 tasks. ± denotes a 95% conﬁdence interval. d: dimensionality of embedding, h: number of hidden units in a\nfully connected layer, k: number of clusters in a partition, P: number of partitions used during meta-learning, m: margin on boundary-deﬁning hyperplanes.\nAlgorithm\n(way, shot)\n(5, 1)\n(5, 5)\n(5, 20)\n(5, 50)\nBaselines\nTraining from scratch\n27.59 ± 0.59 %\n38.48 ± 0.66 %\n51.53 ± 0.72 %\n59.63 ± 0.74 %\nBiGAN, d = 200\nEmbedding knn-nearest neighbors\n25.56 ± 1.08 %\n31.10 ± 0.63 %\n37.31 ± 0.40 %\n43.60 ± 0.37 %\nEmbedding linear classiﬁer\n27.08 ± 1.24 %\n33.91 ± 0.64 %\n44.00 ± 0.45 %\n50.41 ± 0.37 %\nEmbedding MLP with dropout, h = 128\n22.91 ± 0.54 %\n29.06 ± 0.63 %\n40.06 ± 0.72 %\n48.36 ± 0.71 %\nEmbedding cluster matching, k = 500\n24.63 ± 0.56 %\n29.49 ± 0.58 %\n33.89 ± 0.63 %\n36.13 ± 0.64 %\nHyperplanes-MAML (ours), P = 4800, m = 0\n20.00 ± 0.00 %\n20.00 ± 0.00 %\n20.00 ± 0.00 %\n20.00 ± 0.00 %\nHyperplanes-MAML (ours), P = 4800, m = 0.9\n29.67 ± 0.64 %\n41.92 ± 0.69 %\n51.32 ± 0.71 %\n54.72 ± 0.71 %\nCACTUs-MAML (ours), P = 1, k = 500\n37.75 ± 0.74 %\n52.59 ± 0.75 %\n62.70 ± 0.68 %\n67.98 ± 0.68 %\nCACTUs-MAML (ours), P = 50, k = 500\n36.24 ± 0.74 %\n51.28 ± 0.68 %\n61.33 ± 0.67 %\n66.91 ± 0.68 %\nCACTUs-ProtoNets (ours), P = 50, k = 500\n36.62 ± 0.70 %\n50.16 ± 0.73 %\n59.56 ± 0.68 %\n63.27 ± 0.67 %\nDeepCluster, d = 256\nEmbedding knn-nearest neighbors\n28.90 ± 1.25 %\n42.25 ± 0.67 %\n56.44 ± 0.43 %\n63.90 ± 0.38 %\nEmbedding linear classiﬁer\n29.44 ± 1.22 %\n39.79 ± 0.64 %\n56.19 ± 0.43 %\n65.28 ± 0.34 %\nEmbedding MLP with dropout, h = 128\n29.03 ± 0.61 %\n39.67 ± 0.69 %\n52.71 ± 0.62 %\n60.95 ± 0.63 %\nEmbedding cluster matching, k = 500\n22.20 ± 0.50 %\n23.50 ± 0.52 %\n24.97 ± 0.54 %\n26.87 ± 0.55 %\nHyperplanes-MAML (ours), P = 4800, m = 0\n20.02 ± 0.06 %\n20.01 ± 0.01 %\n20.00 ± 0.01 %\n20.01 ± 0.02 %\nHyperplanes-MAML (ours), P = 4800, m = 0.1\n35.85 ± 0.66 %\n49.54 ± 0.72 %\n60.68 ± 0.69 %\n65.55 ± 0.66 %\nCACTUs-MAML (ours), P = 1, k = 500\n38.75 ± 0.70 %\n52.73 ± 0.72 %\n62.72 ± 0.69 %\n67.77 ± 0.62 %\nCACTUs-MAML (ours), P = 50, k = 500\n39.90 ± 0.74 %\n53.97 ± 0.70 %\n63.84 ± 0.70 %\n69.64 ± 0.63 %\nCACTUs-ProtoNets (ours), P = 50, k = 500\n39.18 ± 0.71 %\n53.36 ± 0.70 %\n61.54 ± 0.68 %\n63.55 ± 0.64 %\nSupervised meta-learning\nOracle-MAML (control)\n46.81 ± 0.77 %\n62.13 ± 0.72 %\n71.03 ± 0.69 %\n75.54 ± 0.62 %\nOracle-ProtoNets (control)\n46.56 ± 0.76 %\n62.29 ± 0.71 %\n70.05 ± 0.65 %\n72.04 ± 0.60 %\n21\nPublished as a conference paper at ICLR 2019\nTable 10: CelebA facial attribute classiﬁcation results averaged over 1000 tasks. ± denotes a 95% conﬁdence\ninterval. d: dimensionality of embedding, h: number of hidden units in a fully connected layer, k: number of\nclusters in a partition, P: number of partitions used during meta-learning.\nAlgorithm\n(way, shot)\n(2, 5)\nBaselines\nTraining from scratch\n63.19 ± 1.06 %\nBiGAN, d = 200\nEmbedding knn-nearest neighbors\n56.15 ± 0.89 %\nEmbedding linear classiﬁer\n58.44 ± 0.90 %\nEmbedding MLP with dropout, h = 128\n56.26 ± 0.94 %\nEmbedding cluster matching, k = 500\n56.20 ± 1.00 %\nCACTUs-MAML (ours), P = 50, k = 500\n74.98 ± 1.02 %\nCACTUs-ProtoNets (ours), P = 50, k = 500\n65.58 ± 1.04 %\nDeepCluster, d = 256\nEmbedding knn-nearest neighbors\n61.47 ± 0.99 %\nEmbedding linear classiﬁer\n59.57 ± 0.98 %\nEmbedding MLP with dropout, h = 128\n60.65 ± 0.98 %\nEmbedding cluster matching, k = 500\n51.51 ± 0.89 %\nCACTUs-MAML (ours), P = 50, k = 500\n73.79 ± 1.01 %\nCACTUs-ProtoNets (ours), P = 50, k = 500\n74.15 ± 1.02 %\nSupervised meta-learning\nOracle-MAML (control)\n87.10 ± 0.85 %\nOracle-ProtoNets (control)\n85.13 ± 0.92 %\n22\nPublished as a conference paper at ICLR 2019\nAPPENDIX G\nIMAGENET EXPERIMENTS\nWe investigate unsupervised meta-learning in the context of a larger unsupervised meta-training\ndataset by using the ILSVRC 2012 dataset’s training split (Russakovsky et al., 2015), which is a\nsuperset of the miniImageNet dataset (including meta-validation and meta-testing data) consisting\nof 1000 classes and over 1,200,000 images. To facilitate comparison to the previous miniImageNet\nexperiments, for meta-validation and meta-test we use the miniImageNet meta-validation and meta-\ntest splits. To avoid task leakage, we hold out all data from these 36 underlying classes from the rest\nof the data to construct the meta-training split.\nFor CACTUs, we use the best-performing unsupervised learning method from the previous experi-\nments, DeepCluster, to obtain the embeddings. Following Caron et al. (2018), we run DeepCluster\nusing the VGG-16 architecture with a 256-dimensional feature space and 10,000 clusters on the\nmeta-training data until the normalized mutual information between the data-cluster mappings of\ntwo consecutive epochs converges. To our knowledge, no prior works have yet been published on\nusing MAML for ImageNet-sized meta-learning. We extend the standard convolutional neural net-\nwork model class with residual connections (He et al., 2016), validate hyperparameters with super-\nvised meta-learning, then use it for unsupervised meta-learning without further tuning. See Table 11\nfor MAML hyperparameters. The training from scratch, embedding knn-nearest neighbors, and em-\nbedding linear classiﬁer algorithms are the same as they were in the previous sets of experiments.\nFor Oracle-MAML, we generated tasks using the ground-truth 964 ImageNet meta-training classes.\nWe also run semi-supervised MAML, with the meta-training tasks consisting of CACTUs-based\ntasks as well as tasks constructed from the 64 miniImageNet meta-training classes. The unsuper-\nvised/supervised task proportion split was ﬁxed according to the ratio of the number of data available\nto each task proposal method. As before, the meta-learning methods only meta-learned on 1-shot\ntasks.\nTable 11: MAML hyperparameter summary for ImageNet.\nHyperparameter\nValue\nInput size\n224 × 224\nOuter (meta) learning rate\n0.0001\nInner learning rate\n0.001\nTask batch size\n3\nInner adaptation steps (meta-training)\n5\nMeta-training iterations\n240,000\nAdaptation steps (evaluation)\n100\nClasses per task (meta-training)\n5\nShots per class (meta-training)\n1\nQueries per class\n5\nResidual blocks\n5\nLayers per residual block\n2\nWe ﬁnd that the vastly increased amount of unlabeled meta-training data (in comparison to miniIm-\nageNet) results in signiﬁcant increases for all methods over their counterparts in Table 9 (other than\ntraining from scratch, which does not use this data). We ﬁnd that CACTUs-MAML slightly outper-\nforms embedding linear classiﬁer for the 1-shot test tasks, but that the linear classiﬁer on top of the\nunsupervised embedding becomes better as the amount of test time supervision increases. Augment-\ning the unsupervised tasks with (a small number of) supervised tasks during meta-training results\nin slight improvement for the 1-shot test tasks. The lackluster performance of CACTUs-MAML is\nunsurprising insofar as meta-learning with large task spaces is still an open problem: higher shot\nOracle-MAML only marginally stays ahead of the embedding linear classiﬁer, which is not the case\nin the other, smaller-scale experiments. We expect that using a larger architecture in conjunction\nwith MAML (such as Kim et al. (2018)) would result in increased performance for all methods\nbased on MAML. Further, given the extensive degree to which unsupervised learning methods have\nbeen studied, we suspect that unsupervised task construction coupled with better meta-learning al-\ngorithms and architectures will result in improved performance on the entire unsupervised learning\nproblem. We leave such investigation to future work.\n23\nPublished as a conference paper at ICLR 2019\nTable 12: miniImageNet object classiﬁcation results averaged over 1000 tasks, with ImageNet-scale meta-training. ± denotes a 95% conﬁdence interval. d: dimensionality of\nembedding, k: number of clusters in a partition, P: number of partitions used during meta-learning.\nAlgorithm\n(way, shot)\n(5, 1)\n(5, 5)\n(5, 20)\n(5, 50)\nBaselines\nTraining from scratch\n28.64 ± 0.55 %\n39.71 ± 0.63 %\n53.67 ± 0.75 %\n59.68 ± 0.77 %\nDeepCluster, d = 256\nEmbedding knn-nearest neighbors\n50.17 ± 0.61 %\n69.34 ± 0.51 %\n79.81 ± 0.43 %\n84.72 ± 0.34 %\nEmbedding linear classiﬁer\n58.73 ± 0.62 %\n79.05 ± 0.44 %\n87.41 ± 0.31 %\n90.10 ± 0.27 %\nCACTUs-MAML (ours), P = 1, k = 10000\n60.11 ± 0.74 %\n76.42 ± 0.72 %\n82.85 ± 0.66 %\n85.25 ± 0.67 %\nSemi-supervised meta-learning\nSemi-supervised MAML\n61.75 ± 0.75 %\n76.43 ± 0.70 %\n82.83 ± 0.69 %\n85.27 ± 0.61 %\nSupervised meta-learning\nOracle-MAML (control)\n74.52 ± 0.77 %\n86.23 ± 0.71 %\n91.14 ± 0.69 %\n92.06 ± 0.65 %\n24\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CV",
    "stat.ML"
  ],
  "published": "2018-10-04",
  "updated": "2019-03-21"
}