{
  "id": "http://arxiv.org/abs/1810.03198v1",
  "title": "Reinforcement Evolutionary Learning Method for self-learning",
  "authors": [
    "Kumarjit Pathak",
    "Jitin Kapila"
  ],
  "abstract": "In statistical modelling the biggest threat is concept drift which makes the\nmodel gradually showing deteriorating performance over time. There are state of\nthe art methodologies to detect the impact of concept drift, however general\nstrategy considered to overcome the issue in performance is to rebuild or\nre-calibrate the model periodically as the variable patterns for the model\nchanges significantly due to market change or consumer behavior change etc.\nQuantitative research is the most widely spread application of data science in\nMarketing or financial domain where applicability of state of the art\nreinforcement learning for auto-learning is less explored paradigm.\nReinforcement learning is heavily dependent on having a simulated environment\nwhich is majorly available for gaming or online systems, to learn from the live\nfeedback. However, there are some research happened on the area of online\nadvertisement, pricing etc where due to the nature of the online learning\nenvironment scope of reinforcement learning is explored. Our proposed solution\nis a reinforcement learning based, true self-learning algorithm which can adapt\nto the data change or concept drift and auto learn and self-calibrate for the\nnew patterns of the data solving the problem of concept drift.\n  Keywords - Reinforcement learning, Genetic Algorithm, Q-learning,\nClassification modelling, CMA-ES, NES, Multi objective optimization, Concept\ndrift, Population stability index, Incremental learning, F1-measure, Predictive\nModelling, Self-learning, MCTS, AlphaGo, AlphaZero",
  "text": "Reinforcement Evolutionary Learning Method for self-learning\nKumarjit Pathaka, Jitin Kapilaa\naData Science Researchers, Bangalore, India\nAbstract\nIn statistical modelling the biggest threat is concept drift which makes the model gradually showing\ndeteriorating performance over time. There are state of the art methodologies to detect the impact of concept\ndrift, however general strategy considered to overcome the issue in performance is to rebuild or re-calibrate\nthe model periodically as the variable patterns for the model changes signiﬁcantly due to market change or\nconsumer behavior change etc. Quantitative research is the most widely spread application of data science in\nMarketing or ﬁnancial domain where applicability of state of the art reinforcement learning for auto-learning\nis less explored paradigm. Reinforcement learning is heavily dependent on having a simulated environment\nwhich is majorly available for gaming or online systems, to learn from the live feedback. However, there\nare some research happened on the area of online advertisement, pricing etc where due to the nature of the\nonline learning environment scope of reinforcement learning is explored.\nOur proposed solution is a reinforcement learning based, true self-learning algorithm which can adapt to\nthe data change or concept drift and auto learn and self-calibrate for the new patterns of the data solving\nthe problem of concept drift.\nIndex Terms— Reinforcement learning, Genetic Algorithm, Q-learning, Classiﬁcation mod-\nelling, CMA-ES, NES, Multi objective optimization, Concept drift, Population stability index,\nIncremental learning, F1-measure, Predictive Modelling, Self-learning, MCTS, AlphaGo, Al-\nphaZero\n1. Introduction\nConcept drift is well known challenge for sustainability of any machine learning predictive model over\ntime. Machine learning oﬀers diverse techniques to understand the underlying pattern of the data and\nassociate the same with prediction objective. Any predictive modelling activity in either Marketing, Finance,\nManagement are heavily dependent on the assumption that the training data represents the pattern of\ntarget population under speciﬁc study such as Fraud Identiﬁcation, Customer churn prediction, Marketing\nmix modelling, Target customer identiﬁcation for speciﬁc type of promotion etc. However due to social &\neconomic development, customer behavior changes combined with other external factors making past learned\npattern, irrelevant for current predictions. Model maintenance is one of the key activity for the companies,\nwo are using machine learning for decision making. Model maintenance involves identifying sign of concept\ndrift and performance decay of model periodically and re-calibrate model parameters or many a times model\ncomplexity in case of drastic change in the data to handle performance issue of the model. With current\ndevelopment of reinforcement learning providing hope for general purpose artiﬁcial intelligence. However,\nthe application areas remain conﬁned to online learning only, where the reinforcement learning agent can\nlearn from a simulated environment or live environment in parallel with a stable statistical model and learn\nby failing millions of times to learn about the environment. In parallel it learns eﬀective weight for it’s\nown neural network brain to represent the understanding of the environment to be able to start performing\nEmail addresses: Kumarjit.pathak@outlook.com (Kumarjit Pathak), Jitin.kapila@outlook.com (Jitin Kapila)\n1\narXiv:1810.03198v1  [cs.LG]  7 Oct 2018\ncertain activities in live environment. Understandably so as the cost of live failure is very high, and risk\nattached with the same has high penalty. Just to reinforce the point of view, we can think of an example\nthat if self-driving cars are trained on live environment it would pose being threat to safety on the roads, as\nwell as damage of the car itself pose cost challenge.\nAnother example if a robot is being trained on how to jump or do some critical work it is imperative\nthat it is done in a simulated environment or else cost of damage to the robot or surrounding environment\nwould make such experiment un-sponsorable. There are lots of recent development happened on usage of\nreinforcement learning in creating general purpose AI. Simulated environment is one of the fundamental ask\nfor any RL-Agent to learn. David Silver in his paper, Silver, Schrittwieser, et al. (2017), has indicated oﬄine\nlearning by observing human play video for the world champion beating algorithm AlphaGo.\nOur proposed solution is inspired from diﬀerent research work on reinforcement learning, genetic algorithm,\nincremental learning and concept drift. Our goal is to create a framework which would self-learn based on\ngiven target, self-optimize and self-calibrate in case of performance issue due to concept drift. We used\nconcepts from reinforcement learning, AlphaGo, genetic algorithm along with supervised learning to achieve\nour goal.\nWith our approach of Reinforcement Evolutionary Learning Method (RELM), we were able to demonstrate\nthat combinations of reinforcement learning and genetic algorithm along with deep neural network (a.k.a\nmatrix function approximates), can adapt to concept drift and learn new behavior of the data. Our main\ncontributions are :\n• Providing variant of genetic algorithm as an alternative to tune weights of the deep neural network\nagainst current successful back-propagation method.\n• Demonstration of reinforcement learning strategy for regular marketing analytics modelling.\n• Demonstration of self-learning capability to eﬀectively handle concept drift issue.\n2. Literature Review\nSelf-learning, Incremental learning, concept drift are the areas of research since decades. Concept drift\n(Gepperth and Hammer (2016)) refers to the change in either the input data distribution or change in the\nrelationship between predictors and dependent variable. Generally, two types of concept drifts observed\n(Virtual concept drift & Real concept drift). Virtual concept drift refers to the phenomenon that the\ndistribution of the predictors changes over time. Real concept drift refers to the phenomenon that the\nrelationship between predictor and dependent variable p(Y|X) itself changes.\nIn general concept drift gives raise to the performance issue of the model. There are diﬀerent methods to\ndetect concept drift, one of the popular method is to check population stability index (PSI). This method\n(Gadidov and McBurnett 2015) checks for the performance corrosion over consecutive time frame. We used\nPSI to detect real concept drift.\nPSI =\nd\nX\n1\n(actual % event expected % event) ∗log actual % event\nexpected % even\nwhere d refers to range of interval group for each feature.\nAnother way to identify concept drift is using statistical tests like Hoeﬀding bound, by Blanco et al.\n(2015), or using Hellinger distance etc. However, these are not part of the proposed RELM architecture.\nIncremental learning generally refers to a continuous adaptation of the machine learning model based on\nthe input data changes. Gepperth and Hammer (2016) has described an area of challenge in Incremental\nlearning, that is Stability-Plasticity dilemma, which refers to the decision dilemma, by M. Mermillod and\nBonin (2013), of how much the new concept should be learned and how much the old concept needs to be\nforgotten. Experience shows that rapid adaptation gives rise to catastrophic forgetting problem, by French\n2\n(1992) & McCloskey and Cohen (1989). Catastrophic forgetting refers to the fact that algorithm might be\nable to learn the new concepts quickly but equally rapidly the old information is forgotten.\nOur approach is inspired by AlphaGo paper, by Silver, Schrittwieser, et al. (2017) & Silver, Hubert, et al.\n(2017), is based on dynamic sample generation method for experience replay in order to calibrate the old and\nnew learning together\nFearnet presents another brain inspired architecture proposed by Kemker and Kanan (2018). Author\nhas eﬃciently evaluated past research work done by R. French who recommended a strategy to mitigate\ncatastrophic forgetting with dual separate memory centers. One with short term and one with long term\nmemory. Fearnet is inspired based on the rehearsal method was proposed to have to have a mix of both\nnew and old data to reduce catastrophic forgetting. Generative model was proposed as an alternative to\ngain memory eﬃciency where the algorithm can generate random vectors of learning data from the past\nand augment with new data. Fearnet was proposed inspired from brain consolidation. Fearnet uses to\ncomplementary memory centers, HC which stores more recent information is a probabilistic neural network.\nmPFC is described as old memory storage which is a dual purpose DNN (deep Neural Network) to predict the\nclass with respect to the input vector and reconstruct the input using symmetric encoder decoder. However,\nfor marketing analytics and consumer behavior study keeping very old information does not add value to the\npredictive model as consumer preference is dynamically evolving and hence very old data patterns becomes\nobsolete, however our approach is to balance both old and new learning with incremental shift in the sampling\nreference frame with respect to time.\nReinforcement learning, by Sutton and Barto (1998) & Silver (2015), is an active area of research and\ncurrently acting as the backbone of the modern artiﬁcial intelligence. Sutton explain in his book (Sutton and\nBarto (1998)), diﬀerent type of reinforcement learning method which can learn without supervision with\napproximating the environment behavior incrementally well, as more and more experiments happens between\nagent and environment. David Silver and his team has reinforced the applicability of reinforcement learning\nin complicated environments, such as learning how to play, the game GO against the best player in the world\nusing reinforcement learning. Reinforcement learning by nature adapts to the environment and create more\nand more accurate representation of the environment as it interacts more and more with the environment\ngives rise to the potential diﬀerent application that even if the environment behavior is changing then agent\nwould continuously learn to adapt to the new patterns as well, this can be used as one of the most suitable\nalternative for incremental and continuous learning which can focus on speciﬁc task. We aim to use this\nfeature of RL to create our framework.\nDavid Silver in his paper (Silver, Schrittwieser, et al. (2017)) , has showcased that with suﬃcient training\ncomputer can learn to play one of the most complicated, intuitive and high variance game such as GO.\nHe has showcased usage of DNN in both AlphaGO and AlphaZero, Silver, Hubert, et al. (2017) & Silver,\nSchrittwieser, et al. (2017), to learn the game paly using raw pixcel as an input and understanding next\nprobable move along with the potential value of the same. Work of a genius where he used trimmed MCTS to\nrepresent the learning by the network and helped to have quick decision during self-play phase. Reinforcement\nlearning is in use for marketing analytics challenges like dynamic pricing (Roberto Maestre 2018). Author has\nshowcased usage of a variant of reinforcement learning called Q-Learning with Neural Network approximation\nof the value function to adaptively select the right price. Jain index is also used to measure fairness to the\npricing and used a composite value function to learn.\nUsage of reinforcement learning is becoming state of the art in promotion strategy as well. Usage of multi\narm bandit, a classical reinforcement learning is used to personalize the promotions based on each customer’s\nbehavior to get maximum clicks. However, the basic assumption for applying any reinforcement learning\nthat we are dealing with non-IID data. In simple terms data has some sequence which is represented as\nstates and algorithm tries to learn the transition of states maximizing value function. However most of the\nmarketing research problems deal with cross-sectional data and hence many a case the online feedback is not\npossible, and it becomes a bottle-neck for applicability of reinforcement learning. Our approach postulates\nthe problem of classiﬁcation modelling in any marketing analytics/ﬁnancial analysis scenario as a policy\ngradient RL method.\nWe also explored the current application of genetic algorithm on the ﬁeld of deep learning and artiﬁcial\nintelligence as heuristic search method. Till today back propagation has undoubtedly achieved high success\n3\nand all latest and greatest algorithms are learning based on back propagation. Paul Werbos propose that\nbackpropagation could be used for neural nets after analyzing it in depth in his 1974 PhD Thesis. In 1986\nthrough the work of David E. Rumelhart, Geoﬀrey E. Hinton, Ronald J. Williams, and James McClelland,\nbackpropagation gained recognition. Currently all the state of the art methods (Vu N.P. Dao 2001) in\ndeep learning majorly trains the parameters using backpropagation method. However, there are active\nresearch happening to overcome the limitations of backpropagation in diﬀerent ways. Limitations are majorly\nvanishing gradient problem, which occurs due to high number of layers used in deep learning architecture.\nGradient updates may get low value while back propagation in the initial layers compared to the layers near\nto the output. To mitigate this problem, skip connections are suggested in FRCNN papers also concepts like\nLSTM came up. Another problem area which sometimes becomes critical, is to have a large unbiased dataset\nto train a large neural network. Neural network has tendency of overﬁtting if the data is not suﬃciently\nlarge and success of the generalization aspect of back propagations is majorly controlled by the size and the\nquality of the input data.\nWe explored all recent applications of reinforcement learning where the value function approximation or\nthe policy function approximation is done using deep learning architecture. This has made the algorithm so\npowerful that with mare visual input like videos and pictures the algorithm is able to decide the actions\nsuitable of current state keeping in mind long term rewards of winning the game. After doing considerable\namount of study we could see that the function approximations are driven by backpropagation method in all\nrecent reinforcement learning application. Our approach here proposes an architecture to have the function\napproximator learn through a variant of genetic algorithm which gives the algorithm more capability to learn\nand explore and adapt to the changed scenario of market. Related work was done by Felipe Petroski Such\n(Felipe Petroski Such 2018) to showcase applicability of genetic algorithm augmented learning instead of\nback propagation with distributed computing using either multiple CPU or GPU. Author has showcased\na novel method for distributed deep genetic algorithm where large parameter vectors are represented by a\nfunction with initial seed and list of random seeds which produces the series of mutation. This is done using\na deterministic mutation function.\nWe have used CMA-ES covariance matrix adaptation evolution strategy for the learning of algorithm.\nGenetic algorithm augmentation on deep learning has been gaining momentum recently and many researches\nare coming up in diﬀerent development. Genetic Algorithm is majorly used in two ways, evolving the\narchitecture/topology of the neural network & weight updates of the network. Application of genetic\nalgorithm in neural network for optimization has proven to be great. Pattern recall analysis by Kumar and\nPratap (2010), showcases uses of GA to update the optimal weights. Stoc price index prediction using GA\naugmented neural network achieved better result. Cervical cancer detection by P. Mitra (2001), also shows\nclose to 90% accuracy. There are number of applications showcased by diﬀerent researchers are given in\nAppendix-1 where success of genetic algorithm augmented neural network has proven to have better accuracy\nthan training the network with back propagation.\n3. Method\nOur solution framework works on the concept of reinforcement learning with policy gradient. Idea being\nformulating the regular quantitative marketing and ﬁnancial analysis problems with cross sectional data as a\ndeep evolutionary reinforcement learning problem to overcome the issue of “Concept Drift”. Challenge of\nself-learning, auto-learning and concept drift is wide across industries. Our solution has the capability to\navoid model maintenances activity and have the model automatically tune itself for any sort of classiﬁcation\nmodel using quantitative data.\nReﬁnforcement Learning :\nReinforcement learning refers to an active learning(RL) method without supervision based on either\na value function or policy function. RL Agent experiments to understand the environment and store the\n4\nlearning based on the rewards continuously and observe the state transition. This helps agent to make an\napproximately representative model of the environment behavior. This means the environment becomes\npredictable for the agent and now at any given state agent would know what the best possible action is\nto take to get optimum long-term reward. Reinforcement learning exploits the Markov property and with\napproximation function tries to achieve the same. Markov property refers to the fact the current state\ninformation is suﬃcient to predict the best action and the next state transition. Reinforcement learning uses\nBellman equation heavily to iteratively update the representation or approximation of the environment using\npolicy approximator or value approximator or both.\nV alueP olicy π\nState S = Rewardaction = a\nstate = s + δ\nX\ns′ϵS\nPss′V (s′)\nwhere s is the current state s′ is the next state, Pss′ = probability of the state transition and Policy = π\nThese approximation function or model is currently done using deep learning models which helps to have\nthe universal function approximation capability and based on the gradient update during back propagation\nit updates the representation.\nThis policy function and the value function approximations are now done using deep neural networks\ngiving rise to the concept of Deep Reinforcement Learning.\nDiﬀerent architecture and methodology has been researched over decades such as Q- learning, Deep Q\nlearning, Dueling Deep Q learning, Monte Carlo learning, Temporal Diﬀerencing Learning and SARSA, etc.\nto learn based on the state or action value function. However, in many cases understanding the state value\nbecomes diﬃcult due to continuous states or very high number of states. This gives rise to policy gradient\nmethods.\nReﬁnforcement Learning :\nThe method is based in directly parameterizing the policy. This means what action to take being on the\ncurrent state is a direct output from the function approximator itself. Hence, the concepts tends to get rid of\nthe limitation of value based learning. Value based learning has a limitation of near deterministic policy\neither greedy or ϵ −greedy .It is observed that due to the imbalance of exploration vs exploitation, in many\ncases the value based approximation get’s stuck in the local optimum.\nGoal of policy gradient method is to ﬁnd best parameters to optimize policy at each state and we can\nmeasure the quality of the policy using :\n• Start value in case of episodic environment\n• Average value for continuous environment\n• Average reward per time step\nAdvantage of this method has been really promising.\n• It generally ﬁnds better convergence.\n• Very eﬀective in case if the state space is high dimensional or continuous\n• Main advantage is that it can learn stochastic policies. Policy can change due to any factor , however\nthis advantage is not there in value based reinforcement learning method.\nIt suﬀers from few disadvantages as well:\n• Policy evaluation is high variance and becomes ineﬃcient with respect to state sometimes.\n• Local optimum issue\nDiﬀerent methods under policy gradient been researched over time such as “Deep Policies”, which uses\na neural network to represent a policy output model. Action = f(state, weight). Action is a function of\nstate and parameter weight. Objective is to optimize total discounted reward.\n5\nFigure 1: Architecture of RELM\nDDPG (Deterministic Deep Policy Gradient) method uses an actor and critic network where actor\nis a policy network and critic being q-learning based network. This method uses “Experience Replay” for\nboth actor and critic with a target frozen for an episode of learning to avoid dynamic oscillation of the target\neach time of update and avoid oscillation of target. As oscillation of target at each iteration makes the\nlearning instable due to frequent dynamic variation.\n4. Procedure\nRLEM-architecture:\nOur innovative approach formulates a diﬀerent variant of “Deep Policy” agent augmented with experience\nreplay and CMA-ES (Covariance Matrix Adaptation Evolution Strategy) for the policy function optimization.\nUsing experience replay gives us the potential to learn from new stream of data on continuous basis. Instead\nof gradient backpropagation-based learning we have used CMA-ES to mitigate the problem with policy\ngradient method being stuck in local optimum. This also increases the eﬃciency of the learning and faster\nconvergence.\nCMA-ES takes the result of each generation and adaptively increase and decrease the search space for\nnext generation. It calculates and adapt for the mean variance and calculate the entire covariance matrix\nfor update. Thus, it understands the relations of intermediate generations’ genomes. CMA-ES modiﬁes the\ncovariance calculation in intelligent way to help the optimization. This algorithm ﬁrst focuses on top N-best\nsolutions (genomes) from the current generation it calculates the covariance matrix for next generation.\nBottleneck of this optimization method is the size of the covariance metrics. To overcome the size of the\ncovariance matrix which is dependent on the number of parameters we used concept of latent space modelling\nto reduce the number of dimension at the ﬁrst go. This also enables us to model based on the latent feature\nand reduce the noise in the data variations which is evident for any marketing research problem.\n6\nRLEM-Environment:\nBy nature reinforcement learning is an online learning algorithm where agent learns from the interaction\nwith the environment. This area was not part of active research and hence applicability of reinforcement\nlearning for regular classiﬁcation modelling in Marketing analytics was not explored to the best of it’s\npossibility. We intend to break this paradigm and hence we created an environment with the initial static\ndata of any regular classiﬁcation modelling. Idea being representing the input vector of each row in the\ndata-frame as state(S) and the right output label as the right action(a).\nData sample generator is used to generate sample for current learned data points for the algorithm so\nthat the old learning is not forgotten by the network. We don’t intend to keep all the data points from every\ntime period and hence created an window shift mechanism for the data. Data with more than t period old\nare discarded automatically from the environment. Time t is adjustable. This is just to avoid memory issues\nand performance issues keeping in mind that for marketing analysis data beyond a certain period does not\nrelevant for learning due to continuous shift in socio-economic development and change in customer behavior.\nLaten Space:\nMarketing research problem generally deals with two kind of data type, discrete and continuous. However,\nwe used this block of the architecture to transform both the data type into a continuous and reduced\ndimensions and extract relevant information’s without noise for the state representation which is our primary\nassumption for using “Deep Policy” method of reinforcement learning. We segregated discrete and continuous\nfeatures from the data frame and separately utilized at this layer.\nFor continuous data we used Variational Auto Encoder,by Kingma and Welling (2013) & Pu et al. (2016),\nto generate the bottleneck layer of representation eliminating noise from input feature set. For discrete\ndata we used Restricted Boltzmann Machine Salakhutdinov, Mnih, and Hinton (2007) to bring the discrete\nvariables in continuous reduced dimensional representation.\nRELM Agent:\nhis block represents a deep neural network with layers of tanh activation and sigmoid activation consisting\nof INPUT-FC[45]-tanh-FC[15]-tanh-FC[6]-tanh-FC[1]-sigmoid-OUTPUT. This deep learning model takes\ninput from the latent representation of each vector (‘S’ = latent state representation) and produces action (‘a’\n= Y = output feature). Learning mechanism is augmented with CMA-ES, hence it is gradient free heuristic\nsearch.\nEvaluator:\nThis block is responsible maintaining the sanity of the environment and supervise the overall activity\nof the architecture. During the initial phase when agent is learning from self-play with the environment\nit checks if the performance of the agent is increasing or constant over time. It triggers to save the best\nsolutions periodically wherever it shows improvement. During the new data input which environment has\nnot seen earlier and does not belong to the current environment it evaluates the input distribution of each\nfeature and measure “Accuracy”, “F1-measure” and PSI (Population Stability Index) value. It checks if\nagent is showing a performance drift and imitate the agent for re-training based on the new data and label.\nDuring any production environment this facility can be directly used if feedback is sent back to Evaluator\non the accuracy of last batch prediction. This will create a feedback-loop. Based on the periodic oﬄine or\nonline feedback, algorithm would start tuning itself to adjust to new data patterns again and again.\n7\nFigure 2: Log Loss Across Generations\nFigure 3: Accuracy and F1 across Generations\n8\nProblem Formulation:\nLet’s assume a classiﬁcation modelling in either marketing or ﬁnancial domain. We try to solve any\nsupervised classiﬁcation with dependent variable Y and independent variables x1, x2, x3, x4, x5, x6. . . ..\nassuming a mathemetical non linear or linear relationship between Y and Xs. Let’s assume the mathematical\nfunction Y = f(X, w) where X represents independent feature matrix, w- represents the weight matrix and\nY represents Dependent feature matrix. Our hypothesis , we can represent each vector of the observation\nwith respect to the independent variables as vector of ⃗X . So for N observations we have N vector of Xs and\nYs. Vector of Xs can be assumed to be the continuous states (in the latent space).\nNow it can be safely assumed to have met the criteria for “Deep Policy”\" learning algorithm. Deep policy\nlearning represents a functional equation between state(S = ⃗X) and weight(w) to solve the action(a).\na = f(S, w)\nObjective function is taken here is undiscounted reward function based on number of right predictions\nand the F1 score of the prediction.\n5. Experminetal Results\nWe tested this concept on diﬀerent industrial data with consistent results, however due to sensitivity\nof the data showcasing the same is not considered. To showcase our result we using public data related to\ncredit card fault detection from Kaggle. This data comes with the PCA features already and hence already\nin the latent space. To stop repeating the latent dimension projection, we muted the latent space block and\nwent ahead to apply concept of RELM. Results were encouraging. Fig 2 and 3 represents the initial learning\nof the RELM agent. As we can see within 40th generation accuracy becomes stable. Objective function is\nalso comprising of F1-score and hence post 40th generation improvement on accuracy was very less however,\nalgorithm starts to tune itself for F1 score keeping accuracy intact.\nTo prove that RELM’s capability to adapt to concept drift we kept 70K records separately not being part\nof the reinforcement environment during initial training. Once the agent is comfortable with the current\ndata we start pushing the holdout data to RELM environment.\nFig 4 and 5 shows that post initial training when RELM encounters the new data it had a performance\ndecay which was caught by the evaluator and initiated the re-calibration of the RL agent. Due to the\nre-calibration to new data patterns algorithm starts adapting to the new trends and regain it’s conﬁdence on\nthe environment.\n6. Conclusion and Future Scope\nFrom our study and analysis, we are conﬁdent that RELM architecture can be used industrially for\nsolving the issue of concept drift in any classiﬁcation modelling related marketing or ﬁnancial domain with\nquantitative data. We also break the paradigm of not using reinforcement learning for oﬄine learning. RELM\ncan self-learn, self-optimize, self-calibrate based the changes in the data and issue in performance. With this\nauto-ML capability we open the door for automating the model maintenance activity and provide a ﬂexible\nand powerful solution each time of customer preference and market scenario changes.\nAppendix -1 : List of References where ANN with genetic Algorithm has been used\n1) V. Bevilacqua, G. Mastronardi andF. Menolascina, Genetic Algorithm and Neural Network Based\nClassiﬁcation in Microarray Data Analysis with Biological Validity Assessment, Berlin Heidelberg:\nSpringer-Verlag, 475-484 (2006).\n9\nFigure 4: Illus. 1 - Showcasing RELM result with concept drift entry and how it calibrates itself by learning from new data\nFigure 5: Illus. 2 - Showcasing RELM result with concept drift entry and how it calibrates itself by learning from new data\n10\n2) H. Karimi and F. Youseﬁ, Application of artiﬁcial neural networkgenetic algorithm (ANNGA) to\ncorrelation of density in nanoﬂuids, Fluid Phase Equilibr. 336, 79-83 (2012).\n3) L. Jianfei, L. Weitie, C. Xiaolong andL. Jingyuan, Optimization of Fermentation Media for Enhancing\nNitrite-oxidizing Activity by Artiﬁcial Neural Network Coupling Genetic Algorithm, Chinese Journal\nof Chemical Engineering, 20, 5, 950-957 (2012).\n4) G. Kim, J. Yoona, S. Ana, H. Chob and K. Kanga, Neural network model incorporating a genetic\nalgorithm in estimating construction costs. Build Env. 39, 1333-1340 (2004).\n5) H. Kim, K. Shin andK. Park, Time Delay Neural Networks and Genetic Algorithms for Detecting\nTemporal Patterns in Stock Markets, Berlin Heidelberg: Springer-Verlag, 1247-1255 (2005).\n6) Kim H, Shin K (2007) A hybrid approach based on neural networks and genetic algorithms for detecting\ntemporal patterns in stock markets. Appl Soft Comput 7:569-576\n7) K.P. Ferentinos, Biological engineering applications of feedforward neural networks designed and\nparameterized by genetic algorithms, Neural Network, 18, 934-950 (2005).\n8) K. Kim and I. Han, Genetic algorithms approach to feature discretization in artiﬁcial neural networks\nfor the prediction of stock price index, Expert Syst. Appl. 19,125-132(2000).\n9) M.A. Ali and S.S. Reza, Intelligent approach for prediction of minimum miscible pressure by evolving\ngenetic algorithm and neural network, Neural Comput. Appl. DOI 10.1007/s00521-012-0984-4, 1-8\n(2013).\n10) Z.M.R. Asif, S. Ahmad and R. Samar, Neural network optimized with evolutionary computing technique\nfor solving the 2-dimensional Bratu problem, Neural Comput. Appl. DOI 10.1007/s00521-012-1170- 4\n(2012).\n11) S. Ding, Y. Zhang, J. Chen andW. Jia, Research on using genetic algorithms to optimize Elman neural\nnetworks, Neural Comput. Appl.23,2, 293-297 (2013).\n12) Y. Feng, W. Zhang, D. Sun andL. Zhang, Ozone concentration forecast method based on genetic\nalgorithm optimized back propagation neural networks and support vector machine data classiﬁcation,\nAtmos. Environ. 45, 1979-1985 (2011).\n13) W. Ho andC. Chang, Genetic-algorithm-based artiﬁcial neural network modeling for platelet transfusion\nrequirements on acute myeloblastic leukemia patients, Expert Syst. Appl. 38, 6319-6323 (2011).\n14) G. Huse, E. Strand andJ. Giske, Implementing behavior in individual-based models using neural\nnetworks and genetic algorithms, Evol. Ecol. 13, 469- 483 (1999).\n15) A. Johari, A.A. Javadi and G. Habibagahi, Modelling the mechanical behaviour of unsaturated soils\nusing a genetic algorithm-based neural network, Comput. Geotechn. 38, 2-13 (2011).\n16) R.J. Kuo, A sales forecasting system based on fuzzy neural network with initial weights generated by\ngenetic algorithm, Eur. J. Oper. Res. 129, 496-517 (2001).\n17) Z. Liu,A. Liu, C.Wang andZ. Niu, Evolving neural network using real coded genetic algorithm (GA)\nfor multispectral image classiﬁcation, Future Gener. Comput. Syst. 20, 1119-1129 (2004).\n18) L. Xin-lai, L. Hu, W. Gang-lin andW.U. Zhe, Helicopter Sizing Based on Genetic Algorithm Optimized\nNeural Network, Chinese J. Aeronaut. 19,3, 213-218 (2006).\n19) H. Mahmoudabadi, M. Izadi and M.M. Bagher, A hybrid method for grade estimation using genetic\nalgorithm and neural networks, Comput. Geosci. 13, 91-101 (2009).\n20) Y. Min, W. Yun-jia andC. Yuan-ping, An incorporate genetic algorithm based back propagation neural\nnetwork model for coal and gas outburst intensity prediction, ProcediaEarth. Pl. Sci. 1, 12851292\n(2009).\n21) P. Mitra, S. Mitra, S.K. Pal, Evolutionary Modular MLP with Rough Sets and ID3 Algorithm for\nStaging of Cervical Cancer, Neural Comput. Appl. 10, 67- 76(2001).\n22) M. Nasseri, K. Asghari and M.J. Abedini, Optimized scenario for rainfall forecasting using genetic\nalgorithm coupled with artiﬁcial neural network, Expert Syst. Appl. 35, 1415-1421 (2008).\n23) P.C. Pendharkar, Genetic algorithm based neural network approaches for predicting churnin cellular\nwireless network services, Expert Syst. Appl. 36, 6714- 6720 (2009).\n24) H. Peng and X. Ling, Optimal design approach for the plate-ﬁn heat exchangers usingneural networks\ncooperated with genetic algorithms, Appl. Therm. Eng. 28, 642-650 (2008).\n25) S. Koer and M.C. Rahmi, Classifying Epilepsy Diseases Using Artiﬁcial Neural Networks and Genetic\n11\nAlgorithm, J. Med. Syst. 35, 489-498 (2011).\n26) A. Sedki, D. Ouazar and E. El Mazoudi, Evolving neural network using real coded genetic algorithm\nfor daily rainfallrunoﬀforecasting, Expert Syst. Appl. 36, 4523-4527(2009)\nReferences\nBlanco, Isvani Inocencio Frías, José del Campo-Ávila, Gonzalo Ramos-Jiménez, Rafael Morales Bueno,\nAgustín Alejandro Ortiz Díaz, and Yailé Caballero Mota. 2015. “Online and Non-Parametric Drift Detection\nMethods Based on Hoeﬀding’s Bounds.” IEEE Transactions on Knowledge and Data Engineering 27: 810–23.\nFelipe Petroski Such, Edoardo Conti, Vashisht Madhavan.\n2018.\n“Deep Neuroevolution: Genetic\nAlgorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning.”\nArxiv.org.\nFrench, Robert. 1992. “Semi-Distributed Representations and Catastrophic Forgetting in Connectionist\nNetworks.” Connection Science 4: 1–10.\nGadidov, Bogdan, and Benjamin McBurnett. 2015. “Population Stability and Model Performance Metrics\nReplication for Business Model at Suntrust Bank.” SESUG 4.\nGepperth, Alexander, and Barbara Hammer. 2016. “Incremental Learning Algorithms and Application.”\nESANN, Computational Intelligence and Machine Learning, 1–12.\nKemker, Ronald, and Christopher Kanan.\n2018.\n“FearNet: Brain-Inspired Model for Incremental\nLearning.” https://openreview.net/forum?id=SJ1Xmf-Rb.\nKingma, Diederik P, and Max Welling. 2013. “Auto-Encoding Variational Bayes.” arXiv Preprint\narXiv:1312.6114.\nKumar, S., and M.S. Pratap. 2010. “Pattern Recall Analysis of the Hopﬁeld Neural Network with a\nGenetic Algorithm.” Comput. Math. Application 60: 1049–57.\nMcCloskey, Michael, and Neal J. Cohen. 1989. “Catastrophic Interference in Connectionist Networks:\nThe Sequential Learning Problem.” Edited by Gordon H. Bower, Psychology of learning and motivation, 24.\nAcademic Press: 109–65. https://doi.org/https://doi.org/10.1016/S0079-7421(08)60536-8.\nM. Mermillod, A. Bugaiska, and P. Bonin. 2013. “The Stability-Plasticity Dilemma: Investigating the\nContinuum from Catastrophic Forgetting to Age-Limited Learning Eﬀects.” Frontiers in Psychology 4: 504–6.\nP. Mitra, S.K. Pal, S. Mitra. 2001. “Evolutionary Modular Mlp with Rough Sets and Id3 Algorithm for\nStaging of Cervical Cancer.” Neural Comput. Appllication 10: 60–76.\nPu, Yunchen, Zhe Gan, Ricardo Henao, Xin Yuan, Chunyuan Li, Andrew Stevens, and Lawrence\nCarin. 2016. “Variational Autoencoder for Deep Learning of Images, Labels and Captions.” Edited by\nDaniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, 2352–60.\nhttp://dblp.uni-trier.de/db/conf/nips/nips2016.html#PuGHYLSC16.\nRoberto Maestre, Alberto Rubio, Juan Duque. 2018. “Reinforcement Learning for Fair Dynamic Pricing.”\nIntelligent Systems Conference, London, UK, 1–7.\nSalakhutdinov, Ruslan, Andriy Mnih, and Geoﬀrey E. Hinton. 2007. “Restricted Boltzmann Machines\nfor Collaborative Filtering.” Edited by Zoubin Ghahramani, ACM international conference proceeding series,\n227 (October). ACM: 791–98. http://dblp.uni-trier.de/db/conf/icml/icml2007.html#SalakhutdinovMH07.\nSilver, David. 2015. “Reinforcement Learning.” http://www0.cs.ucl.ac.uk/staﬀ/d.silver/web/Teaching.\nhtml.\nSilver, David, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,\nMarc Lanctot, et al. 2017. “Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning\nAlgorithm.” CoRR abs/1712.01815. http://dblp.uni-trier.de/db/journals/corr/corr1712.html#abs-1712-\n01815.\nSilver, David, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,\nThomas Hubert, et al. 2017. “Mastering the Game of Go Without Human Knowledge.” Nature 550\n(October).\nMacmillan Publishers Limited, part of Springer Nature.\nAll rights reserved.: 354.\nhttp:\n//dx.doi.org/10.1038/nature24270.\nSutton, Richard S., and Andrew G. Barto. 1998. Reinforcement Learning: An Introduction. MIT Press.\nhttp://www.cs.ualberta.ca/~sutton/book/the-book.html.\n12\nVu N.P. Dao, Rao Vemuri. 2001. “A Performance Comparison of Diﬀerent Back Propagation Neural\nNetworks Methods in Computer Network Intrusion Detection.” Cyber Defense Initiative, Washington, 3–*.\n13\n",
  "categories": [
    "cs.LG",
    "cs.NE",
    "stat.ML"
  ],
  "published": "2018-10-07",
  "updated": "2018-10-07"
}