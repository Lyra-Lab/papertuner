{
  "id": "http://arxiv.org/abs/1810.04066v2",
  "title": "Deep learning with differential Gaussian process flows",
  "authors": [
    "Pashupati Hegde",
    "Markus Heinonen",
    "Harri Lähdesmäki",
    "Samuel Kaski"
  ],
  "abstract": "We propose a novel deep learning paradigm of differential flows that learn a\nstochastic differential equation transformations of inputs prior to a standard\nclassification or regression function. The key property of differential\nGaussian processes is the warping of inputs through infinitely deep, but\ninfinitesimal, differential fields, that generalise discrete layers into a\ndynamical system. We demonstrate state-of-the-art results that exceed the\nperformance of deep Gaussian processes and neural networks",
  "text": "Deep learning with diﬀerential Gaussian process ﬂows\nPashupati Hegde, Markus Heinonen, Harri L¨ahdesm¨aki, Samuel Kaski\nHelsinki Institute for Information Technology HIIT\nDepartment of Computer Science, Aalto University\nOctober 16, 2018\nAbstract\nWe propose a novel deep learning paradigm of diﬀeren-\ntial ﬂows that learn a stochastic diﬀerential equation\ntransformations of inputs prior to a standard classi-\nﬁcation or regression function. The key property of\ndiﬀerential Gaussian processes is the warping of inputs\nthrough inﬁnitely deep, but inﬁnitesimal, diﬀerential\nﬁelds, that generalise discrete layers into a dynamical\nsystem. We demonstrate state-of-the-art results that\nexceed the performance of deep Gaussian processes\nand neural networks.\n1\nINTRODUCTION\nGaussian processes are a family of ﬂexible kernel func-\ntion distributions (Rasmussen and Williams, 2006).\nThe capacity of kernel models is inherently deter-\nmined by the function space induced by the choice\nof the kernel, where standard stationary kernels lead\nto models that underperform in practice. Shallow –\nor single – Gaussian processes are often suboptimal\nsince ﬂexible kernels that would account for the non-\nstationary and long-range connections of the data\nare diﬃcult to design and infer. Such models have\nbeen proposed by introducing non-stationary kernels\n(Tolvanen et al., 2014; Heinonen et al., 2016), ker-\nnel compositions (Duvenaud et al., 2011; Sun et al.,\n2018), spectral kernels (Wilson et al., 2013; Remes\net al., 2017), or by applying input-warpings (Snoek\net al., 2014) or output-warpings (Snelson et al., 2004;\nL´azaro-Gredilla, 2012). Recently, Wilson et al. (2016)\nproposed to transform the inputs with a neural net-\nwork prior to a Gaussian process model. The new\nneural input representation can extract high-level pat-\nterns and features, however, it employs rich neural\nnetworks that require careful design and optimization.\nDeep Gaussian processes elevate the performance\nof Gaussian processes by mapping the inputs through\nmultiple Gaussian process ’layers’ (Damianou and\nLawrence, 2013; Salimbeni and Deisenroth, 2017), or\nas a network of GP nodes (Duvenaud et al., 2011;\nWilson et al., 2012; Sun et al., 2018). However, deep\nGP’s result in degenerate models if the individual\nGP’s are not invertible, which limits their capacity\n(Duvenaud et al., 2014).\nIn this paper we propose a novel paradigm of learn-\ning continuous-time transformations or ﬂows of the\ndata instead of learning a discrete sequence of layers.\nWe apply stochastic diﬀerential equation systems in\nthe original data space to transform the inputs before\na classiﬁcation or regression layer. The transformation\nﬂow consists of an inﬁnite path of inﬁnitesimal steps.\nThis approach turns the focus from learning iterative\nfunction mappings to learning input representations\nin the original feature space, avoiding learning new\nfeature spaces.\nOur experiments show state-of-the-art prediction\nperformance on a number of benchmark datasets on\nclassiﬁcation and regression. The performance of the\nproposed model exceeds that of competing Bayesian\napproaches, including deep Gaussian processes.\n2\nBACKGROUND\nWe begin by summarising useful background of Gaus-\nsian processes and continuous-time dynamicals mod-\nels.\n2.1\nGaussian processes\nGaussian processes (GP) are a family of Bayesian\nmodels that characterise distributions of functions\n(Rasmussen and Williams, 2006). A Gaussian process\nprior on a function f(x) over vector inputs x ∈RD,\nf(x) ∼GP(0, K(x, x′)),\n(1)\n1\narXiv:1810.04066v2  [cs.LG]  15 Oct 2018\ndeﬁnes a prior distribution over function values f(x)\nwhose mean and covariances are\nE[f(x)] = 0\n(2)\ncov[f(x), f(x′)] = K(x, x′).\n(3)\nA GP prior deﬁnes that for any collection of N in-\nputs, X = (x1, . . . , xN)T , the corresponding function\nvalues f = (f(x1), . . . , f(xN))T ∈RN are coupled to\nfollowing the multivariate normal distribution\nf ∼N(0, K),\n(4)\nwhere K = (K(xi, xj))N\ni,j=1 ∈RN×N is the kernel\nmatrix.\nThe key property of GP’s is that output\npredictions f(x) and f(x′) correlate depending on\nhow similar are their inputs x and x′, as measured by\nthe kernel K(x, x′) ∈R.\nWe consider sparse Gaussian process functions by\naugmenting the Gaussian process with a small number\nM of inducing ‘landmark’ variables u = f(z) (Snelson\nand Ghahramani, 2006). We condition the GP prior\nwith the inducing variables u = (u1, . . . , uM)T ∈RM\nand Z = (z1, . . . , zM)T to obtain the GP posterior\npredictions at data points\nf|u; Z ∼N(Qu, KXX −QKZZQT )\n(5)\nu ∼N(0, KZZ),\n(6)\nwhere Q = KXZK−1\nZZ, and where KXX ∈RN×N is\nthe kernel between observed image pairs X × X, the\nkernel KXZ ∈RN×M is between observed images X\nand inducing images Z, and kernel KZZ ∈RM×M\nis between inducing images Z × Z.\nThe inference\nproblem of sparse Gaussian processes is to learn the\nparameters θ of the kernel (such as the lengthscale),\nand the conditioning inducing variables u, Z.\n2.2\nStochastic diﬀerential equations\nStochastic diﬀerential equations (SDEs) are an eﬀec-\ntive formalism for modelling continuous-time systems\nwith underlying stochastic dynamics, with wide range\nof applications (Friedrich et al., 2011). We consider\nmultivariate continuous-time systems governed by a\nMarkov process xt described by SDE dynamics\ndxt = µ(xt)dt +\np\nΣ(xt)dWt,\n(7)\nwhere xt ∈RD is the state vector of a D-dimensional\ndynamical system at continuous time t ∈R, µ(xt) ∈\nRD is a deterministic state evolution vector ﬁeld,\np\nΣ(xt) ∈RD×D is the diﬀusion matrix ﬁeld of the\nFigure 1: An example vector ﬁeld deﬁned by the\ninducing vectors (a) results in the ODE ﬂow solutions\n(b) of a 2D system. Including the colored Wiener\ndiﬀusion (c) leads to SDE trajectory distributions\n(d).\nstochastic multivariate Wiener process Wt ∈RD. The\np\nΣ(xt) is the square root matrix of a covariance ma-\ntrix Σ(xt), where we assume Σ(xt) =\np\nΣ(xt)\np\nΣ(xt)\nholds. A Wiener process has zero initial state W0 = 0,\nand independent, Gaussian increments Wt+s −Wt ∼\nN(0, sID) over time with standard deviation √sID\n(See Figure 1).\nThe SDE system (7) transforms states xt forward\nin continuous time by the deterministic drift function\nµ : RD →RD, while the diﬀusion Σ : RD →RD×D\nis the scale of the random Brownian motion Wt that\nscatter the state xt with random ﬂuctuations. The\nstate solutions of an SDE are given by the stochastic\nItˆo integral (Oksendal, 2014)\nxt = x0 +\nZ t\n0\nµ(xτ)dτ +\nZ t\n0\np\nΣ(xτ)dWτ,\n(8)\nwhere we integrate the system state from an initial\nstate x0 for time t forward, and where τ is an auxiliary\ntime variable. SDEs produce continuous, but non-\nsmooth trajectories x0:t over time due to the non-\ndiﬀerentiable Brownian motion. This causes the SDE\nsystem to not have a time derivative\ndxt\ndt , but the\nstochastic Itˆo integral (8) can still be deﬁned.\nThe only non-deterministic part of the solution (8)\n2\n(a) Sparse GP\n(b) Deep GP\n(c) Diﬀerentially deep GP\nFigure 2: The sparse Gaussian processes uncouples the observations through global inducing variables ug (a).\nDeep Gaussian process is a hierarchical model with a nested composition of Gaussian processes introducing\nlayer dependency. Layer-speciﬁc inducing variables u(ℓ)\ng\nintroduce conditional independence between function\nvalues g(ℓ)\ni\nwithin each layer (b). In our formulation deepness is introduced as a temporal dependency across\nstates xi(t) (indicated by dashed line) with a GP prior over their diﬀerential function value fi (c). Global\ninducing variables uf can be used to introduce conditional independence between diﬀerential function values\nat a particular time point.\nis the Brownian motion Wτ, whose random realisa-\ntions generate path realisations x0:t that induce state\ndistributions\nxt ∼pt(x; µ, Σ, x0)\n(9)\nat any instant t, given the drift µ and diﬀusion Σ\nfrom initial state x0. The state distribution is the solu-\ntion to the Fokker-Planck-Kolmogorov (FPK) partial\ndiﬀerential equation, which is intractable for general\nnon-linear drift and diﬀusion.\nIn practise the Euler-Maruyama (EM) numerical\nsolver can be used to simulate trajectory samples from\nthe state distribution (Yildiz et al., 2018) (See Figure\n1d). We assume a ﬁxed time discretisation t1, . . . , tN\nwith ∆t = tN/N being the time window (Higham,\n2001). The EM method at tk is\nxk+1 = xk + µ(xk)∆t +\np\nΣ(xk)∆Wk,\n(10)\nwhere ∆Wk = Wk+1 −Wk ∼N(0, ∆tID) with stan-\ndard deviation\n√\n∆t.\nThe EM increments ∆xk =\nxk+1 −xk correspond to samples from a Gaussian\n∆xk ∼N(µ(xk)∆t, Σ(xk)∆t).\n(11)\nThen, the full N length path is determined from the N\nrealisations of the Wiener process, each of which is a D-\ndimensional. More eﬃcient high-order approximations\nhave also been developed (Kloeden and Platen, 1992;\nLamba et al., 2006).\nSDE systems are often constructed by manually\ndeﬁning drift and diﬀusion functions to model speciﬁc\nsystems in ﬁnance, biology, physics or in other do-\nmains (Friedrich et al., 2011). Recently, several works\nhave proposed learning arbitrary drift and diﬀusion\nfunctions from data (Papaspiliopoulos et al., 2012;\nGarc´ıa et al., 2017; Yildiz et al., 2018).\n3\nDEEP\nDIFFERENTIAL\nGAUSSIAN PROCESS\nIn this paper we propose a paradigm of continuous-\ntime deep learning, where inputs xi are not treated as\nconstant, but are instead driven by an SDE system.\nWe propose a continuous-time deep Gaussian process\nmodel through inﬁnite, inﬁnitesimal diﬀerential com-\npositions, denoted as DiﬀGP. In DiﬀGP, a Gaussian\nprocess warps or ﬂows an input x through an SDE\nsystem until a predeﬁned time T, resulting in x(T),\nwhich is subsequently classiﬁed or regressed with a\nseparate function. We apply the process to both train\nand test inputs. We impose GP priors on both the\nstochastic diﬀerential ﬁelds and the predictor function\n(See Figure 2). A key parameter of the diﬀerential\nGP model is the amount of simulation time T, which\ndeﬁnes the length of ﬂow and the capacity of the sys-\ntem, analogously to the number of layers in standard\ndeep GPs or deep neural networks.\nWe\nassume\na\ndataset\nof\nN\ninputs\nX\n=\n(x1, . . . , xN)T\n∈RN×D of D-dimensional vectors\nxi\n∈\nRD, and associated scalar outputs y\n=\n(y1, . . . , yN)T ∈RN that can be continuous for a\nregression problem or categorical for classiﬁcation,\nrespectively. We redeﬁne the inputs as temporal func-\ntions x : T →RD over time such that state paths xt\nover time t ∈T = R+ emerge, where the observed\n3\ninputs xi,t ≜xi,0 correspond to initial states xi,0 at\ntime 0. We classify or regress the ﬁnal data points\nXT = (x1,T , . . . , xN,T )T after T time of an SDE ﬂow\nwith a predictor Gaussian process\ng(xT ) ∼GP(0, K(xT , x′\nT ))\n(12)\nto classify or regress the outputs y. The framework\nreduces to a conventional Gaussian process with zero\nﬂow time T = 0 (See Figure 2).\nThe prediction depends on the ﬁnal dataset XT\nstructure, determined by the SDE ﬂow dxt from the\noriginal data X. We consider SDE ﬂows of type\ndxt = µ(xt)dt +\np\nΣ(xt)dWt\n(13)\nwhere\nµ(x) = KxZf K−1\nZf Zf vec(Uf)\n(14)\nΣ(x) = Kxx −KxZf K−1\nZf Zf KZf x\n(15)\nare the vector-valued Gaussian process posteriors con-\nditioned on equalities between f(z), inducing vec-\ntors Uf = (uf\n1, . . . , uf\nM)T and inducing states Zf =\n(zf\n1, . . . , zf\nM). These choices of drift and diﬀusion cor-\nrespond to an underlying GP\nf(x) ∼GP(0, K(x, x′))\n(16)\nf(x)|Uf, Zf ∼N(µ(x), Σ(x))\n(17)\nwhere K(x, x′) ∈RD×D is a matrix-valued ker-\nnel of the vector ﬁeld f(x) ∈RD, and KZf Zf =\n(K(zf\ni , zf\nj ))M\ni,j=1 ∈RMD×MD block matrix of matrix-\nvalued kernels (similarly for KxZf ).\nThe vector ﬁeld f(x) is now a GP with determin-\nistic conditional mean µ and covariance Σ at every\nlocation x given the inducing variables. We encode\nthe underlying GP ﬁeld mean and covariance uncer-\ntainty into the drift and diﬀusion of the SDE ﬂow\n(13). The Wiener process Wt of an SDE samples a\nnew ﬂuctuation from the covariance Σ around the\nmean µ at every instant t. This corresponds to an\naﬃne transformation\n(f(x) −µ(x))\n√\n∆t + µ(x)∆t ∼N(µ(x)∆t, Σ(x)∆t),\n(18)\nwhich shows that samples from the vector ﬁeld\nGP match the SDE Euler-Maruyama increment\n∆xk\ndistribution\n(11).\nThe\nstate\ndistribu-\ntion pT (x; µ, Σ, x0) can then be represented as\np(xT |Uf) =\nR\np(xT |f)p(f|Uf)df where p(xT |f) is an\nDirac distribution of the end point of a single Euler-\nMaruyama simulated path, and where the vector ﬁeld\np(f) is marginalized along the Euler-Maruyama path.\nOur model corresponds closely to the doubly-\nstochastic deep GP, where the Wiener process was\nreplaced by random draws from the GP posterior\nεl · Σl(f l−1) per layer l (Salimbeni and Deisenroth,\n2017). In our approach the continuous time t cor-\nresponds to continuously indexed states, eﬀectively\nallowing inﬁnite layers that are inﬁnitesimal.\n3.1\nSpatio-temporal ﬁelds\nEarlier we assumed a global, time-independent vector\nﬁeld f(xt), which in the standard models would corre-\nspond to a single ‘layer’ applied recurrently over time\nt. To extend the model capacity, we consider spatio-\ntemporal vector ﬁelds ft(x) := f(x, t) that themselves\nevolve as a function of time, eﬀectively applying a\nsmoothly changing vector ﬁeld ‘layer’ at every in-\nstant t. We select a separable spatio-temporal ker-\nnel K((x, t), (x′, t′)) = K(x, x′)k(t, t′) that leads to\nan eﬃcient Kronecker-factorised (Stegle et al., 2011)\nspatio-temporal SDE ﬂow\nft(x)|(Zs\nf, Zt\nf, Uf) ∼N(µt(x), Σt(x))\n(19)\nµt(x) = CxZf C−1\nZf Zf vec(Uf)\n(20)\nΣt(x) = Cxx −CxZf C−1\nZf Zf CZf x,\n(21)\nwhere Cxx = Kxxktt, CxZ = KxZs\nf ⊗KtZt\nf and\nCZf Zf = KZs\nf Zs\nf ⊗KZt\nf Zt\nf , and where the spatial in-\nducing states are denoted by Zs\nf and the temporal\ninducing times by Zt\nf. In practice we place usually\nonly a few (e.g. 3) temporal inducing times equidis-\ntantly on the range [0, T]. This allows the vector ﬁeld\nitself to curve smoothly throughout the SDE. We only\nhave a single inducing matrix Uf for both spatial and\ntemporal dimensions.\n3.2\nStochastic variational inference\nThe diﬀerential Gaussian process is a combination of\na conventional prediction GP g(·) with an SDE ﬂow\nGP f(·) fully parameterised by Z, U as well as kernel\nparameters θ. We turn to variational inference to\nestimate posterior approximations q(Uf) and q(ug)\nfor both models.\nWe start by augmenting the predictor function g\nwith M inducing locations Zg = (zg1, . . . , zgM) with\nassociated inducing function values g(z) = u in a vec-\ntor ug = (ug1, . . . , ugM)T ∈RM. We aim to learn the\ndistribution of the inducing values u, while learning\n4\nFigure 3: (a)Illustration of samples from a 2D deep Gaussian processes prior. DGP prior exhibits a pathology\nwherein representations in deeper layers concentrate on low-rank manifolds.(b) Samples from a diﬀerentially\ndeep Gaussian processes prior result in rank-preserving representations.(c) The continuous-time nature of\nthe warping trajectories results from smooth drift and structured diﬀusion (d).\npoint estimates of the inducing locations Z, which we\nhence omit from the notation below. The prediction\nconditional distribution is (Titsias, 2009)\np(g|ug, XT ) = N(g|QT ug, KXT XT −QT KZgZgQT\nT )\n(22)\np(ug) = N(ug|0, KZgZg),\n(23)\nwhere we denote QT = KXT ZgK−1\nZgZg.\nThe joint density of a single path and prediction of\nthe augmented system is\np(y, g, ug, XT , f, Uf)\n(24)\n= p(y|g)\n| {z }\nlikelihood\np(g|ug, XT )p(ug)\n|\n{z\n}\nGP prior of g(x)\np(XT |f)\n|\n{z\n}\nSDE\np(f|Uf)p(Uf)\n|\n{z\n}\nGP prior of f(x)\n.\nThe joint distribution contains the likelihood term,\nthe two GP priors, and the SDE term p(XT |f) rep-\nresenting the Euler-Maruyama paths of the dataset.\nThe inducing vector ﬁeld prior follows\np(Uf) =\nD\nY\nd=1\nN(ufd|0, KZf dZf d),\n(25)\nwhere ufd\n=\n(uf\n1(d)T , . . . , uf\nM(d)) and Zf d\n=\n(zf\n1(d), . . . , zf\nM(d))T .\nWe consider optimizing the marginal likelihood\nlog p(y) = log Ep(g|XT )p(XT )p(y|g),\n(26)\nwhere the p(g|XT ) is a Gaussian process predictive dis-\ntribution, and the state distribution p(XT ) marginal-\nizes the trajectories,\np(XT ) =\nZZ\np(XT |f)p(f|Uf)p(Uf)dfdUf,\n(27)\n5\nwith no tractable solution.\nWe follow stochastic variational inference (SVI) by\nHensman et al. (2015), where standard variational\ninference (Blei et al., 2016) is applied to ﬁnd a lower\nbound of the marginal log likelihood, or in other words\nmodel evidence. In particular, a variational lower\nbound for the evidence (26) without the state distri-\nbutions has already been considered by Hensman et al.\n(2015), which tackles both problems of cubic complex-\nity O(N 3) and marginalization of non-Gaussian likeli-\nhoods. We propose to include the state distributions\nby simulating Monte Carlo state trajectories.\nWe propose a complete variational posterior approx-\nimation over both f and g,\nq(g, ug, XT , f, Uf) = p(g|ug, XT )q(ug)\n(28)\n· p(XT |f)p(f|Uf)q(Uf)\nq(ug) = N(ug|mg, Sg)\n(29)\nq(Uf) =\nD\nY\nd=1\nN(ufd|mfd, Sfd),\n(30)\nwhere Mf = (mf1, . . . , mfD) and Sf = (Sf1, . . . , SfD)\ncollect the dimension-wise inducing parameters. We\ncontinue by marginalizing out inducing variables ug\nand Uf from the above joint distribution, arriving at\nthe joint variational posterior\nq(g, XT , f) = q(g|XT )p(XT |f)q(f),\n(31)\nwhere\nq(g|XT ) =\nZ\np(g|ug, XT )q(ug)dug\n(32)\n= N(g|QT mg, KXT XT + QT (Sg −KZgZg)QT\nT )\n(33)\nq(f) =\nZ\np(f|Uf)q(Uf)dUf = N(f|µq, Σq)\n(34)\nµq = Qfvec(Mf)\n(35)\nΣq = KXX + Qf(Sf −KZf Zf )QT\nf ,\n(36)\nwhere Qf = KXZf K−1\nZf Zf . We plug the derived vari-\national posterior drift µq and diﬀusion Σq estimates\nto the ﬁnal variational SDE ﬂow\ndxt = µq(xt)dt +\nq\nΣq(xt)dWt,\n(37)\nwhich conveniently encodes the variational approxi-\nmation of the vector ﬁeld f.\nNow the lower bound for our diﬀerential deep GP\nmodel can be written as (detailed derivation is pro-\nvided in the appendix)\nlog p(y) ≥\nN\nX\ni=1\n\u001a 1\nS\nS\nX\ns=1\nEq(g|x(s)\ni,T ) log p(yi|gi)\n|\n{z\n}\nvariational expected likelihood\n−\nkl[q(ug)||p(ug)]\n|\n{z\n}\nprior divergence of g(x)\n−kl[q(Uf)||p(Uf)]\n|\n{z\n}\nprior divergence of f(x)\n\u001b\n,\n(38)\nwhich factorises over both data and SDE paths with\nunbiased samples x(s)\ni,T ∼pT (x; µq, Σq, xi) by numeri-\ncally solving the variational SDE (37) using the Euler-\nMaruyama method.\nFor likelihoods such as Gaussian for regression prob-\nlems, we can further marginalize g from the lower-\nbound as shown by Hensman et al. (2013). For other\nintractable likelihoods, numerical integration tech-\nniques such as Gauss-Hermite quadrature method can\nbe used (Hensman et al., 2015).\n3.3\nRank pathologies in deep models\nA deep Gaussian process f L(· · · f 2(f 1(x))) is a compo-\nsition of L Gaussian process layers f l(x) (Damianou\nand Lawrence, 2013). These models typically lead\nto degenerate covariances, where each layer in the\ncomposition reduces the rank or degrees of freedom of\nthe system (Duvenaud et al., 2014). In practice the\nrank reduces via successive layers mapping inputs to\nidentical values (See Figure 3a), eﬀectively merging\ninputs and resulting in a reduced-rank covariance ma-\ntrix with repeated rows and columns. To counter this\npathology Salimbeni and Deisenroth (2017) proposed\npseudo-monotonic deep GPs by using identity mean\nfunction in all intermediate GP layers.\nUnlike the earlier approaches, our model does not\nseem to suﬀer from this degeneracy.\nThe DiﬀGP\nmodel warps the input space without seeking low-\nvolume representations. In particular the SDE diﬀu-\nsion scatters the trajectories preventing both narrow\nmanifolds and input merging. In practice, this results\nin a rank-preserving model (See Figure 3b-d). There-\nfore, we can use zero mean function for the Gaussian\nprocesses responsible for diﬀerential warpings.\n6\nboston\nenergy\nconcrete\nwine red\nkin8mn\npower\nnaval\nprotein\nN\n506\n768\n1,030\n1,599\n8,192\n9,568\n11,934\n45,730\nD\n13\n8\n8\n22\n8\n4\n26\n9\nLinear\n4.24(0.16)\n2.88(0.05)\n10.54(0.13)\n0.65(0.01)\n0.20(0.00)\n4.51(0.03)\n0.01(0.00)\n5.21(0.02)\nBNN\nL = 2\n3.01(0.18)\n1.80(0.05)\n5.67(0.09)\n0.64(0.01)\n0.10(0.00)\n4.12(0.03)\n0.01(0.00)\n4.73(0.01)\nSparse GP\nM = 100\n2.87(0.15)\n0.78(0.02)\n5.97(0.11)\n0.63(0.01)\n0.09(0.00)\n3.91(0.03)\n0.00(0.00)\n4.43(0.03)\nM = 500\n2.73(0.12)\n0.47(0.02)\n5.53(0.12)\n0.62(0.01)\n0.08(0.00)\n3.79(0.03)\n0.00(0.00)\n4.10(0.03)\nDeep GP\nM = 100\nL = 2\n2.90(0.17)\n0.47(0.01)\n5.61(0.10)\n0.63(0.01)\n0.06(0.00)\n3.79(0.03)\n0.00(0.00)\n4.00(0.03)\nL = 3\n2.93(0.16)\n0.48(0.01)\n5.64(0.10)\n0.63(0.01)\n0.06(0.00)\n3.73(0.04)\n0.00(0.00)\n3.81(0.04)\nL = 4\n2.90(0.15)\n0.48(0.01)\n5.68(0.10)\n0.63(0.01)\n0.06(0.00)\n3.71(0.04)\n0.00(0.00)\n3.74(0.04)\nL = 5\n2.92(0.17)\n0.47(0.01)\n5.65(0.10)\n0.63(0.01)\n0.06(0.00)\n3.68(0.03)\n0.00(0.00)\n3.72(0.04)\nDiﬀGP\nM = 100\nT = 1.0\n2.80(0.13)\n0.49(0.02)\n5.32(0.10)\n0.63(0.01)\n0.06(0.00)\n3.76(0.03)\n0.00(0.00)\n4.04(0.04)\nT = 2.0\n2.68(0.10)\n0.48(0.02)\n4.96(0.09)\n0.63(0.01)\n0.06(0.00)\n3.72(0.03)\n0.00(0.00)\n4.00(0.04)\nT = 3.0\n2.69(0.14)\n0.47(0.02)\n4.76(0.12)\n0.63(0.01)\n0.06(0.00)\n3.68(0.03)\n0.00(0.00)\n3.92(0.04)\nT = 4.0\n2.67(0.13)\n0.49(0.02)\n4.65(0.12)\n0.63(0.01)\n0.06(0.00)\n3.66(0.03)\n0.00(0.00)\n3.89(0.04)\nT = 5.0\n2.58(0.12)\n0.50(0.02)\n4.56(0.12)\n0.63(0.01)\n0.06(0.00)\n3.65(0.03)\n0.00(0.00)\n3.87(0.04)\nTable 1: Test RMSE values of 8 benchmark datasets (reproduced from Salimbeni & Deisenroth 2017). Uses\nrandom 90% / 10% training and test splits, repeated 20 times.\n4\nEXPERIMENTS\nWe optimize the inducing vectors, inducing locations,\nkernel lengthscales and signal variance of both the\nSDE function f equation (13) and the predictor func-\ntion g(xT ). We also optimize noise variance in prob-\nlems with Gaussian likelihoods. The number of induc-\ning points M is manually chosen, where more inducing\npoints tightens the variational approximation at the\ncost of additional computation. All parameters are\njointly optimised against the evidence lower bound\n(38). The gradients of the lower bound back-propagate\nthrough the prediction function g(xT ) and through\nthe SDE system from x(T) back to initial values x(0).\nGradients of an SDE system approximated by an\nEM method can be obtained with the autodiﬀdif-\nferentiation of TensorFlow (Abadi et al., 2016). The\ngradients of continuous-time systems follow from for-\nward or reverse mode sensitivity equations (Kokotovic\nand Heller, 1967; Raue et al., 2013; Fr¨ohlich et al.,\n2017; Yildiz et al., 2018). We perform stochastic opti-\nmization with mini-batches and the Adam optimizer\n(Kingma and Ba, 2014) with a step size of 0.01. For\nnumerical solutions of SDE, we use Euler-Maruyama\nsolver with 20 time steps. Also, initializing parame-\nters of g(·) with values learned through SGP results in\nearly convergence; we initialize DiﬀGP training with\nSGP results and a very weak warping ﬁeld Uf ≈0 and\nkernel variance σ2\nf ≈0.01. We use diagonal approx-\nimation of the Σq. We also use GPﬂow (Matthews\net al., 2017), a Gaussian processes framework built on\nTensorFlow in our implementation.\n4.1\nStep function estimation\nWe begin by highlighting how the DiﬀGP estimates a\nsignal with multiple highly non-stationary step func-\ntions. Figure 4 shows the univariate signal observa-\ntions (top), the learned SDE ﬂow (middle), and the\nresulting regression function on the end points X(t)\n(bottom). The DiﬀGP separates the regions around\nthe step function such that the ﬁnal regression func-\ntion g with a standard stationary Gaussian kernel can\nﬁt the transformed data X(t). The model then has\nlearned the non-stationarities of the system with un-\ncertainty in the signals being modelled by the inherent\nuncertainties arising from the diﬀusion.\n4.2\nUCI regression benchmarks\nWe compare our model on 8 regression benchmarks\nwith the previously reported state-of-the-art results\nin (Salimbeni and Deisenroth, 2017). We test all the\ndatasets on diﬀerent ﬂow time values from 1 to 5.\nWe use the RBF kernel with ARD and 100 inducing\npoints for both the diﬀerential Gaussian Process and\nthe regression Gaussian Process. Each experiment is\nrepeated 20 times with random 90% / 10% training\nand test splits. While testing, we compute predictive\nmean and predictive variance for each of the sample\ngenerated from (37), and compute the average of sum-\nmary statistics (RMSE and log likelihood) over these\nsamples.\nThe mean and standard error of RMSE\nvalues are reported in Table 1.\nOn Boston, Concrete and Power datasets, where\n7\nFigure 4: Step function estimation: Observed input space (a) is transformed through stochastic continuous-\ntime mappings (b) into a warped space (c). The stationary Gaussian process in the warped space gives a\nsmooth predictive distribution corresponding to a highly non-stationary predictions in the original observed\nspace.\ndeep models show improvement over shallow models,\nour model outperforms previous best results of DGP.\nThere is a small improvement by having a non-linear\nmodel on the Kin8mn dataset and our results match\nthat of DGP. Energy and Wine are small datasets\nwhere single Gaussian Processes perform the best. As\nexpected, both DiﬀGP and DGP recover the shal-\nlow model indicating no over-ﬁtting. Regression task\non the Protein dataset is aimed at predicting RMSD\n(Root Mean Squared Deviation) between modeled and\nnative protein structures using 9 diﬀerent properties\nof the modeled structures (Rana et al., 2015). We\nsuspect DGP particularly performs better than Dif-\nfGP in the task because of its capability to model\nlong-range correlations.\n4.3\nUCI classiﬁcation benchmarks\nWe perform binary classiﬁcation experiments on large-\nscale HIGGS and SUSY datasets with a data size\nin the order of millions.\nWe use the AUC as the\nperformance measure and compare the results with\nthe previously reported results using DGP (Salim-\nbeni and Deisenroth, 2017) and DNN (Baldi et al.,\n2014). The classiﬁcation task involves identifying pro-\ncesses that produce Higgs boson and super-symmetric\nparticles using data from Monte Carlo simulations.\nPreviously, deep learning methods based on neural\nnetworks have shown promising results on these tasks\n(Baldi et al., 2014). On the HIGGS dataset, the pro-\nposed DiﬀGP model shows state-of-the-art (0.878)\nresults, equal or even better than the earlier reported\nresults using DGPs (0.877) and DNNs (0.876). On the\nSUSY dataset, we reach the performance of 4-hidden\nlayer DGP (0.841) with non-temporal DiﬀGP (0.842).\nConsidering the consistent improvement in the perfor-\nmance of DGP models with additional layers, we tried\nincreasing the capacity of DiﬀGP model using the\ntemporal extension proposed in Section 3.1. In partic-\nular, we used 100 spatial inducing vectors along with\n3 temporal inducing vectors. The temporal DiﬀGP\nmodel gives an AUC of 0.878 on HIGGS and 0.846 on\nSUSY datasets matching the best reported results of\nDGP (see appendix for detailed comparison).\n4.4\nImportance of ﬂow time\nIn this we experiment we study the SDE ﬂow time\nparameter on Concrete dataset. Increasing integration\ntime provides more warping ﬂexibility to the SDE\nwarping component. That is, with increase in the ﬂow\ntime, the SDE system can move observations further\naway from the initial state, however at the cost of\nexposing the state to more diﬀusion which acts as a\nprincipled regularization. Thus increasing time can\nlead to an increase in the model capacity without\nover-ﬁtting.\nWe empirically support this claim in\nthe current experiment by ﬁtting a regression model\n8\nFigure 5: Concrete dataset: increasing the ﬂow time\nvariable T improves the train and test errors (a,c)\nand likelihoods (b,d). The horizontal line indicates\nGP and DGP2 performance. The model convergence\nindicates the improved capacity upon increased ﬂow\ntime (e).\nmultiple times and maintaining same experimental\nsetup, expect for the ﬂow time. Figure 5 shows the\nvariation in RMSE, log likelihood and the lower bound\non marginal likelihood across diﬀerent ﬂow times. It\ncan be seen that the improvement in the performance\nalmost saturates near time = 10.\n5\nDISCUSSION\nWe have proposed a novel paradigm, continuous-time\nGaussian process deep learning. The proposed def-\nerentially deep composition is a continuous-time ap-\nproach wherein a Gaussian processes input locations\nare warped through stochastic and smooth diﬀeren-\ntial equations. This results in a principled Bayesian\napproach with a smooth non-linear warping; the un-\ncertainty through diﬀusion acts as a key regularizer.\nWe empirically show excellent results in various\nregression and classiﬁcation tasks. Also, DGP with\nthe model speciﬁcation as proposed by Salimbeni and\nDeisenroth (2017), uses a total of O(LDM) number of\ninducing parameters for the regression results, where\nL is the number of layers, D is the input dimension,\nM is the number of inducing points for each latent\nGP. In contrast, with a smaller number of inducing pa-\nrameters O(DM), we arrive at similar or even better\nresults.\nThe continuous-time deep model admits ‘decision-\nmaking paths’, where we can explicitly follow the\ntransformation applied to a data point xi. Analyz-\ning these paths could lead to a better interpretable\nmodel. However, modeling in the input space without\nintermediate low-dimensional latent representations\npresents scalability issues. We leave scaling the ap-\nproach to high dimensions as future work, while we\nalso intend to explore new optimisation modes, such\nas SG-MCMC (Ma et al., 2015) or Stein inference\n(Liu and Wang, 2016) in the future.\nReferences\nMart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng\nChen, Andy Davis, Jeﬀrey Dean, Matthieu Devin,\nSanjay Ghemawat, Geoﬀrey Irving, Michael Isard,\net al. Tensorﬂow: A system for large-scale machine\nlearning. In OSDI, volume 16, pages 265–283, 2016.\nPierre Baldi, Peter Sadowski, and Daniel Whiteson.\nSearching for exotic particles in high-energy physics\nwith deep learning.\nNature Communications, 5:\n4308, 2014.\nD. Blei, A. Kucukelbir, and J. McAuliﬀe. Variational\ninference: A review for statisticians. Journal of\nthe American Statistical Association, 112:859–877,\n2016.\nAndreas Damianou and Neil Lawrence. Deep gaussian\nprocesses. In Artiﬁcial Intelligence and Statistics,\npages 207–215, 2013.\nDavid Duvenaud, Oren Rippel, Ryan Adams, and\nZoubin Ghahramani. Avoiding pathologies in very\ndeep networks. In Artiﬁcial Intelligence and Statis-\ntics, pages 202–210, 2014.\nDavid K Duvenaud, Hannes Nickisch, and Carl E Ras-\nmussen. Additive gaussian processes. In Advances\nin Neural Information Processing Systems, pages\n226–234, 2011.\nRudolf Friedrich, Joachim Peinke, Muhammad Sahimi,\nand M Reza Rahimi Tabar. Approaching complexity\nby stochastic methods: From biological systems to\nturbulence. Physics Reports, 506(5):87–162, 2011.\nFabian Fr¨ohlich, Barbara Kaltenbacher, Fabian J.\nTheis, and Jan Hasenauer. Scalable parameter esti-\nmation for genome-scale biochemical reaction net-\nworks. PLOS Computational Biology, 13(1):1–18,\n01 2017. doi: 10.1371/journal.pcbi.1005331.\n9\nC. Garc´ıa, A. Otero, P. Felix, J. Presedo, and D. Mar-\nquez. Nonparametric estimation of stochastic dif-\nferential equations with sparse Gaussian processes.\nPhysical Review E, 96(2):022104, 2017.\nM. Heinonen, H. Mannerstr¨om, J. Rousu, S. Kaski,\nand H. L¨ahdesm¨aki. Non-stationary Gaussian pro-\ncess regression with Hamiltonian Monte Carlo. In\nAISTATS, volume 51, pages 732–740, 2016.\nJ. Hensman, N. Fusi, and N. Lawrence. Gaussian pro-\ncesses for big data. In Proceedings of the Twenty-\nNinth Conference on Uncertainty in Artiﬁcial In-\ntelligence, pages 282–290. AUAI Press, 2013.\nJ. Hensman, A. Matthews, and Z. Ghahramani. Scal-\nable variational Gaussian process classiﬁcation. In\nArtiﬁcial Intelligence and Statistics, pages 351–360,\n2015.\nDesmond Higham. An algorithmic introduction to\nnumerical simulation of stochastic diﬀerential equa-\ntions. SIAM Rev., 43:525–546, 2001.\nDiederik P Kingma and Jimmy Lei Ba.\nAdam:\nAmethod for stochastic optimization. In Proc. 3rd\nInt. Conf. Learn. Representations, 2014.\nP.E. Kloeden and E. Platen.\nNumerical Solution\nof Stochastic Diﬀerential Equations. Applications\nof Mathematics. Springer-Verlag, 1992.\nISBN\n9783540540625. URL https://books.google.fi/\nbooks?id=7bkZAQAAIAAJ.\nP Kokotovic and J Heller. Direct and adjoint sensi-\ntivity equations for parameter optimization. IEEE\nTransactions on Automatic Control, 12(5):609–610,\n1967.\nH Lamba, Jonathan C Mattingly, and Andrew M\nStuart. An adaptive euler–maruyama scheme for\nsdes: convergence and stability. IMA journal of\nnumerical analysis, 27:479–506, 2006.\nMiguel L´azaro-Gredilla. Bayesian warped gaussian\nprocesses. In Advances in Neural Information Pro-\ncessing Systems, pages 1619–1627, 2012.\nQiang Liu and Dilin Wang. Stein variational gradi-\nent descent: A general purpose bayesian inference\nalgorithm. In Advances in Neural Information Pro-\ncessing Systems, pages 2378–2386, 2016.\nYi-An Ma, Tianqi Chen, and Emily Fox. A complete\nrecipe for stochastic gradient mcmc. In Advances\nin Neural Information Processing Systems, pages\n2917–2925, 2015.\nAlexander G. de G. Matthews, Mark van der Wilk,\nTom Nickson, Keisuke. Fujii, Alexis Boukouvalas,\nPablo Le´on-Villagr´a, Zoubin Ghahramani, and\nJames Hensman.\nGPﬂow: A Gaussian process\nlibrary using TensorFlow.\nJournal of Machine\nLearning Research, 18(40):1–6, apr 2017.\nURL\nhttp://jmlr.org/papers/v18/16-537.html.\nB. Oksendal. Stochastic Diﬀerential Equations: An\nIntroduction with Applications. Springer, 6th edi-\ntion, 2014.\nOmiros Papaspiliopoulos, Yvo Pokern, Gareth O\nRoberts, and Andrew M Stuart. Nonparametric\nestimation of diﬀusions: a diﬀerential equations\napproach. Biometrika, 99(3):511–531, 2012.\nPrashant Singh Rana, Harish Sharma, Mahua Bhat-\ntacharya, and Anupam Shukla. Quality assessment\nof modeled protein structure using physicochemical\nproperties. Journal of bioinformatics and computa-\ntional biology, 13(02):1550005, 2015.\nC.E. Rasmussen and K.I. Williams. Gaussian pro-\ncesses for machine learning. MIT Press, 2006.\nAndreas Raue, Marcel Schilling, Julie Bachmann, An-\ndrew Matteson, Max Schelker, Daniel Kaschek,\nSabine Hug, Clemens Kreutz, Brian D. Harms,\nFabian J. Theis, Ursula Klingm¨uller, and Jens Tim-\nmer. Lessons learned from quantitative dynamical\nmodeling in systems biology. PLOS ONE, 8(9):\n1–17, 2013.\nS. Remes, M. Heinonen, and S. Kaski. Non-stationary\nspectral kernels. Advances in Neural Information\nProcessing Systems, 2017.\nHugh Salimbeni and Marc Deisenroth. Doubly stochas-\ntic variational inference for deep gaussian processes.\nIn Advances in Neural Information Processing Sys-\ntems, pages 4591–4602, 2017.\nEdward Snelson and Zoubin Ghahramani.\nSparse\ngaussian processes using pseudo-inputs. In Advances\nin Neural Information Processing Systems, pages\n1257–1264, 2006.\nEdward Snelson, Zoubin Ghahramani, and Carl E\nRasmussen. Warped gaussian processes. In Ad-\nvances in Neural Information Processing Systems,\npages 337–344, 2004.\nJasper Snoek, Kevin Swersky, Rich Zemel, and Ryan\nAdams. Input warping for bayesian optimization of\n10\nnon-stationary functions. In International Confer-\nence on Machine Learning, pages 1674–1682, 2014.\nOliver Stegle, Christoph Lippert, Joris M Mooij,\nNeil D Lawrence, and Karsten M Borgwardt. Ef-\nﬁcient inference in matrix-variate gaussian models\nwith iid observation noise. In Advances in neu-\nral information processing systems, pages 630–638,\n2011.\nS. Sun, G. Zhang, C. Wang, W. Zeng, J. Li, and\nR. Grosse. Diﬀerentiable compositional kernel learn-\ning for gaussian processes. In International Confer-\nence on Machine Learning, 2018.\nM. Titsias. Variational learning of inducing variables\nin sparse Gaussian processes. In Artiﬁcial Intelli-\ngence and Statistics, pages 567–574, 2009.\nVille Tolvanen, Pasi Jyl¨anki, and Aki Vehtari. Expec-\ntation propagation for nonstationary heteroscedas-\ntic gaussian process regression. In Machine Learning\nfor Signal Processing (MLSP), 2014 IEEE Interna-\ntional Workshop on, pages 1–6. IEEE, 2014.\nA. Wilson, E. Gilboa, A. Nehorai, and J. Cunning-\nham. Fast multidimensional pattern extrapolation\nwith gaussian processes. Artiﬁcial Intelligence and\nStatistics, 2013.\nAndrew Gordon Wilson, David A Knowles, and\nZoubin Ghahramani. Gaussian process regression\nnetworks. In Proceedings of the 29th International\nCoference on International Conference on Machine\nLearning, pages 1139–1146. Omnipress, 2012.\nAndrew Gordon Wilson, Zhiting Hu, Ruslan Salakhut-\ndinov, and Eric P Xing. Deep kernel learning. In\nArtiﬁcial Intelligence and Statistics, pages 370–378,\n2016.\nCagatay Yildiz, Markus Heinonen, Jukka Intosalmi,\nHenrik Mannerstr¨om, and Harri L¨ahdesm¨aki. Learn-\ning stochastic diﬀerential equations with gaussian\nprocesses without gradient matching. In Machine\nLearning in Signal Processing, 2018.\n11\nAPPENDIX\nA. Derivation of the stochastic variational inference\nThe diﬀerential Gaussian process is a combination of a conventional prediction GP g(·) with an SDE ﬂow GP\nf(·) fully parameterised by Z, U as well as kernel parameters θ. We turn to variational inference to estimate\nposterior approximations q(Uf) and q(ug) for both models.\nExact inference of Gaussian processes has a limiting complexity of O(N 3). Instead, we apply stochastic\nvariational inference (SVI) (Hensman et al., 2015), which has been demonstrated to scale GP’s up to a billion\ndata points (Salimbeni and Deisenroth, 2017). We here summarise the SVI procedure following Hensman\net al. (2015).\nWe start with the joint density of a single path and prediction of the augmented system\np(y, g, ug, XT , f, Uf) = p(y|g)\n| {z }\nlikelihood\np(g|ug, XT )p(ug)\n|\n{z\n}\nGP prior of g(x)\np(XT |f)\n|\n{z\n}\nSDE\np(f|Uf)p(Uf)\n|\n{z\n}\nGP prior of f(x)\n.\n(39)\nwhere we have augmented the predictor function g with M inducing locations Zg = (zg1, . . . , zgM) with\nassociated inducing function values g(z) = u in a vector ug = (ug1, . . . , ugM)T ∈RM with a GP prior. The\nconditional distribution is (Titsias, 2009)\np(g|ug, XT ) = N(g|QT ug, KXT XT −QT KZgZgQT\nT )\n(40)\np(ug) = N(ug|0, KZgZg),\n(41)\nwhere we denote QT = KXT ZgK−1\nZgZg.\nSimilarly, the warping function f is augmented with inducing variables ufd = (uf\n1(d)T , . . . , uf\nM(d)) and\ninducing locations Zf d = (zf\n1(d), . . . , zf\nM(d))T .\np(Uf) =\nD\nY\nd=1\nN(ufd|0, KZf dZf d),\n(42)\nThe joint distribution (39) contains the likelihood term, the two GP priors, and the SDE term p(XT |f)\nrepresenting the Euler-Maruyama paths of the dataset.\ndxt = µ(xt)dt +\np\nΣ(xt)dWt\n(43)\nµ(xt) = RUf\n(44)\nΣ(xt) = Kxx −RKZf Zf R\n(45)\nWe consider optimizing the marginal likelihood\nlog p(y) = log Ep(g|XT )p(XT )p(y|g),\n(46)\np(g|XT ) =\nZ\np(g|ug, XT )p(ug)dug\n(47)\np(XT ) =\nZZ\np(XT |f)p(f|Uf)p(Uf)dfdUf,\n(48)\nwith no tractable solution due to the FPK state distribution p(XT ).\nA variational lower bound for the evidence (46) without the state distributions has already been considered\nby Hensman et al. (2015). We propose to include the state distributions by simulating Monte Carlo state\ntrajectories.\n12\nWe propose a complete variational posterior approximation over both f and g,\nq(g, ug, XT , f, Uf) = p(g|ug, XT )q(ug)p(XT |f)p(f|Uf)q(Uf)\n(49)\nq(ug) = N(ug|mg, Sg)\n(50)\nq(Uf) =\nD\nY\nd=1\nN(ufd|mfd, Sfd),\n(51)\nwhere Mf = (mf1, . . . , mfD) and Sf = (Sf1, . . . , SfD) collect the dimension-wise inducing parameters. We\ncontinue by marginalizing out inducing variables ug and Uf from the above joint distribution arriving at the\njoint variational posterior\nq(g, XT , f) = q(g|XT )p(XT |f)q(f),\n(52)\nwhere\nq(g|XT ) =\nZ\np(g|ug, XT )q(ug)dug\n(53)\n= N(g|QT mg, KXT XT + QT (Sg −KZgZg)QT\nT )\n(54)\nq(f) =\nZ\np(f|Uf)q(Uf)dUf\n(55)\n= N(f|µq, Σq)\nµq = QfMf\n(56)\nΣq = KXX + Qf(Sf −KZf Zf )QT\nf\n(57)\nwhere Qf = KXZf K−1\nZf Zf . We plug the derived variational posterior drift µq and diﬀusion Σq estimates to\nthe SDE to arrive at the ﬁnal variational SDE ﬂow\ndxt = µq(xt)dt +\nq\nΣq(xt)dWt,\n(58)\nwhich conveniently encodes the variational approximation of f.\nNow the lower bound for our diﬀerential deep GP model can be written as\nlog p(y) ≥\nZ\nq(g, ug, XT , f, Uf) log p(y, g, ug, XT , f, Uf)\nq(g, ug, XT , f, Uf) dgdugdXT dfdUf\n(59)\n≥\nZ\np(g|ug, XT )q(ug)p(XT |f)p(f|Uf)q(Uf) log p(y|g)p(ug)p(Uf)\nq(ug)q(Uf)\ndgdugdXT dfdUf\n(60)\n≥\nZ\nq(g|XT )q(XT ) log p(y|g)dgdXT −kl[q(ug)||p(ug)] −kl[q(Uf)||p(Uf)]\n(61)\n≥\nN\nX\ni=1\n\u001a 1\nS\nS\nX\ns=1\nEq(g|x(s)\ni,T ) log p(yi|gi)\n|\n{z\n}\nvariational expected likelihood\n−kl[q(ug)||p(ug)]\n|\n{z\n}\ng(x) prior divergence\n−kl[q(Uf)||p(Uf)]\n|\n{z\n}\nf(x) prior divergence\n\u001b\n.\n(62)\n13\nB. Regression and classiﬁcation benchmarks\nboston\nenergy\nconcrete\nwine red\nkin8mn\npower\nnaval\nprotein\nN\n506\n768\n1,030\n1,599\n8,192\n9,568\n11,934\n45,730\nD\n13\n8\n8\n22\n8\n4\n26\n9\nLinear\n4.24(0.16)\n2.88(0.05)\n10.54(0.13)\n0.65(0.01)\n0.20(0.00)\n4.51(0.03)\n0.01(0.00)\n5.21(0.02)\nBNN\nL = 2\n3.01(0.18)\n1.80(0.05)\n5.67(0.09)\n0.64(0.01)\n0.10(0.00)\n4.12(0.03)\n0.01(0.00)\n4.73(0.01)\nSparse GP\nM = 100\n2.87(0.15)\n0.78(0.02)\n5.97(0.11)\n0.63(0.01)\n0.09(0.00)\n3.91(0.03)\n0.00(0.00)\n4.43(0.03)\nM = 500\n2.73(0.12)\n0.47(0.02)\n5.53(0.12)\n0.62(0.01)\n0.08(0.00)\n3.79(0.03)\n0.00(0.00)\n4.10(0.03)\nDeep GP\nM = 100\nL = 2\n2.90(0.17)\n0.47(0.01)\n5.61(0.10)\n0.63(0.01)\n0.06(0.00)\n3.79(0.03)\n0.00(0.00)\n4.00(0.03)\nL = 3\n2.93(0.16)\n0.48(0.01)\n5.64(0.10)\n0.63(0.01)\n0.06(0.00)\n3.73(0.04)\n0.00(0.00)\n3.81(0.04)\nL = 4\n2.90(0.15)\n0.48(0.01)\n5.68(0.10)\n0.63(0.01)\n0.06(0.00)\n3.71(0.04)\n0.00(0.00)\n3.74(0.04)\nL = 5\n2.92(0.17)\n0.47(0.01)\n5.65(0.10)\n0.63(0.01)\n0.06(0.00)\n3.68(0.03)\n0.00(0.00)\n3.72(0.04)\nDiﬀGP\nM = 100\nT = 1.0\n2.80(0.13)\n0.49(0.02)\n5.32(0.10)\n0.63(0.01)\n0.06(0.00)\n3.76(0.03)\n0.00(0.00)\n4.04(0.04)\nT = 2.0\n2.68(0.10)\n0.48(0.02)\n4.96(0.09)\n0.63(0.01)\n0.06(0.00)\n3.72(0.03)\n0.00(0.00)\n4.00(0.04)\nT = 3.0\n2.69(0.14)\n0.47(0.02)\n4.76(0.12)\n0.63(0.01)\n0.06(0.00)\n3.68(0.03)\n0.00(0.00)\n3.92(0.04)\nT = 4.0\n2.67(0.13)\n0.49(0.02)\n4.65(0.12)\n0.63(0.01)\n0.06(0.00)\n3.66(0.03)\n0.00(0.00)\n3.89(0.04)\nT = 5.0\n2.58(0.12)\n0.50(0.02)\n4.56(0.12)\n0.63(0.01)\n0.06(0.00)\n3.65(0.03)\n0.00(0.00)\n3.87(0.04)\nTable 2: Test RMSE values of 8 benchmark datasets (reproduced from from Salimbeni & Deisenroth 2017).\nUses random 90% / 10% training and test splits, repeated 20 times.\nboston\nenergy\nconcrete\nwine red\nkin8mn\npower\nnaval\nprotein\nN\n506\n768\n1,030\n1,599\n8,192\n9,568\n11,934\n45,730\nD\n13\n8\n8\n22\n8\n4\n26\n9\nLinear\n-2.89(0.03)\n-2.48(0.02)\n-3.78(0.01)\n-0.99(0.01)\n0.18(0.01)\n-2.93(0.01)\n3.73(0.00)\n-3.07(0.00)\nBNN\nL = 2\n-2.57(0.09)\n-2.04(0.02)\n-3.16(0.02)\n-0.97(0.01)\n0.90(0.01)\n-2.84(0.01)\n3.73(0.01)\n-2.97(0.00)\nSparse GP\nM = 100\n-2.47(0.05)\n-1.29(0.02)\n-3.18(0.02)\n-0.95(0.01)\n0.63(0.01)\n-2.75(0.01)\n6.57(0.15)\n-2.91(0.00)\nM = 500\n-2.40(0.07)\n-0.63(0.03)\n-3.09(0.02)\n-0.93(0.01)\n1.15(0.00)\n-2.75(0.01)\n7.01(0.05)\n-2.83(0.00)\nDeep GP\nM = 100\nL = 2\n-2.47(0.05)\n-0.73(0.02)\n-3.12(0.01)\n-0.95(0.01)\n1.34(0.01)\n-2.75(0.01)\n6.76(0.19)\n-2.81(0.00)\nL = 3\n-2.49(0.05)\n-0.75(0.02)\n-3.13(0.01)\n-0.95(0.01)\n1.37(0.01)\n-2.74(0.01)\n6.62(0.18)\n-2.75(0.00)\nL = 4\n-2.48(0.05)\n-0.76(0.02)\n-3.14(9 .01)\n-0.95(0.01)\n1.38(0.01)\n-2.74(0.01)\n6.61(0.17)\n-2.73(0.00)\nL = 5\n-2.49(0.05)\n-0.74(0.02)\n-3.13(0.01)\n-0.95(0.01)\n1.38(0.01)\n-2.73(0.01)\n6.41(0.28)\n-2.71(0.00)\nDiﬀGP\nM = 100\nT = 1.0\n-2.36(0.05)\n-0.65(0.03)\n-3.05(0.02)\n-0.96(0.01)\n1.36(0.01)\n-2.75(0.01)\n6.58(0.02)\n-2.79(0.04)\nT = 2.0\n-2.32(0.04)\n-0.63(0.03)\n-2.96(0.02)\n-0.97(0.02)\n1.37(0.00)\n-2.74(0.01)\n6.26(0.03)\n-2.78(0.04)\nT = 3.0\n-2.31(0.05)\n-0.63(0.02)\n-2.93(0.04)\n-0.97(0.02)\n1.37(0.01)\n-2.72(0.01)\n6.00(0.03)\n-2.79(0.00)\nT = 4.0\n-2.33(0.06)\n-0.65(0.02)\n-2.91(0.04)\n-0.98(0.02)\n1.37(0.01)\n-2.72(0.01)\n5.86(0.02)\n-2.78(0.00)\nT = 5.0\n-2.30(0.05)\n-0.66(0.02)\n-2.90(0.05)\n-0.98(0.02)\n1.36(0.01)\n-2.72(0.01)\n5.78(0.02)\n-2.77(0.00)\nTable 3: Test log likelihood values of 8 benchmark datasets (reproduced from from Salimbeni & Deisenroth\n2017)\n14\nSUSY\nHIGGS\nN\n5,500,000\n11,000,000\nD\n18\n28\nDNN\n0.876\n0.885\nSparse GP\nM = 100\n0.875\n0.785\nM = 500\n0.876\n0.794\nDeep GP\nM = 100\nL = 2\n0.877\n0.830\nL = 3\n0.877\n0.837\nL = 4\n0.877\n0.841\nL = 5\n0.877\n0.846\nDiﬀGP\nM = 100\nt = 1.0\n0.878\n0.840\nt = 3.0\n0.878\n0.841\nt = 5.0\n0.878\n0.842\nDiﬀGP Temporal\nMs = 100\nMt = 3\nt = 5.0\n0.878\n0.846\nTable 4: Test AUC values for large-scale classiﬁcation datasets. Uses random 90% / 10% training and test\nsplits.\n15\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2018-10-09",
  "updated": "2018-10-15"
}