{
  "id": "http://arxiv.org/abs/1810.06746v1",
  "title": "Using Deep Reinforcement Learning for the Continuous Control of Robotic Arms",
  "authors": [
    "Winfried L√∂tzsch"
  ],
  "abstract": "Deep reinforcement learning enables algorithms to learn complex behavior,\ndeal with continuous action spaces and find good strategies in environments\nwith high dimensional state spaces. With deep reinforcement learning being an\nactive area of research and many concurrent inventions, we decided to focus on\na relatively simple robotic task to evaluate a set of ideas that might help to\nsolve recent reinforcement learning problems. We test a newly created\ncombination of two commonly used reinforcement learning methods, whether it is\nable to learn more effectively than a baseline. We also compare different ideas\nto preprocess information before it is fed to the reinforcement learning\nalgorithm. The goal of this strategy is to reduce training time and eventually\nhelp the algorithm to converge. The concluding evaluation proves the general\napplicability of the described concepts by testing them using a simulated\nenvironment. These concepts might be reused for future experiments.",
  "text": "Using Deep Reinforcement Learning for\nthe Continuous Control of Robotic Arms\nWinfried L√∂tzsch\nmatriculation number: 373351\nBACHELOR THESIS\nto obtain the academic grade\n\"Bachelor of Science\" in Computer Science\nFirst Supervisor: Dr. habil. Julien Vitay\nSecond Supervisor: Prof. Dr. Fred Hamker\nSeptember 2017\nFaculty of Computer Science, Professorship Artificial Intelligence\narXiv:1810.06746v1  [cs.LG]  15 Oct 2018\nContents\nAbstract\niv\n1\nIntroduction\n1\n2\nDifferent Types of Learning\n3\n2.1\nSupervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n2.2\nUnsupervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.3\nReinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n3\nVariants of Reinforcement Learning\n6\n3.1\nCommon Reinforcement Learning Issues . . . . . . . . . . . . . . . . . . . .\n6\n3.2\nAlgorithms that Optimize Value Functions . . . . . . . . . . . . . . . . . . .\n8\n3.2.1\nReinforcement Learning with Tabular Value Functions . . . . . . . .\n8\n3.2.2\nUsing Stochastic Policies to Improve Exploration . . . . . . . . . . .\n10\n3.2.3\nUsing Eligibility Traces to Improve Bootstrapping\n. . . . . . . . . .\n11\n3.2.4\nUsing Neural Networks to Approximate the Q-function . . . . . . . .\n13\n3.2.5\nUsing Convolutional Layers to Process Pixel Images\n. . . . . . . . .\n14\n3.2.6\nDeep Q-network (DQN) . . . . . . . . . . . . . . . . . . . . . . . . .\n16\ni\nContents\nii\n3.2.7\nImprovements to DQN . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n3.3\nAlgorithms that Follow the Policy Gradient . . . . . . . . . . . . . . . . . .\n22\n3.3.1\nFinite-Difference Methods . . . . . . . . . . . . . . . . . . . . . . . .\n23\n3.3.2\nLikelihood-Ratio Methods . . . . . . . . . . . . . . . . . . . . . . . .\n23\n3.3.3\nActor-Critic Methods\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n3.3.4\nDeep Deterministic Policy Gradient (DDPG) . . . . . . . . . . . . .\n26\n3.3.5\nAsynchronous Advantage Actor-Critic (A3C) . . . . . . . . . . . . .\n28\n4\nExtensions to Reinforcement Learning\n30\n4.1\nPretraining a State Model Using the Physical States . . . . . . . . . . . . .\n31\n4.2\nPretraining a State Model with a Deep Autoencoder . . . . . . . . . . . . .\n31\n4.3\nTraining Inverse- or Forward-Models . . . . . . . . . . . . . . . . . . . . . .\n35\n5\nMethods\n37\n5.1\nTraining Environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n5.1.1\nSimulation with Matplotlib . . . . . . . . . . . . . . . . . . . . . . .\n38\n5.1.2\nMore Realistic Simulation with Dart . . . . . . . . . . . . . . . . . .\n39\n5.2\nImplementation Details\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n5.2.1\nUsed Software and Hardware . . . . . . . . . . . . . . . . . . . . . .\n41\n5.2.2\nAsynchronously Executing Multiple Environments\n. . . . . . . . . .\n43\n5.3\nTraining Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n45\n6\nExperimental Results\n47\n6.1\nUsing the Physical States for Training and Testing . . . . . . . . . . . . . .\n48\n6.2\nUsing the Physical States for Training and Pixel Images for Testing . . . . .\n51\nContents\niii\n6.3\nUsing Pixel Images for Training and Testing . . . . . . . . . . . . . . . . . .\n55\n7\nDiscussion\n58\nA Raw Scores\n65\nAbstract\nDeep reinforcement learning enables algorithms to learn complex behavior, deal with\ncontinuous action spaces and find good strategies in environments with high dimensional\nstate spaces. With deep reinforcement learning being an active area of research and many\nconcurrent inventions, we decided to focus on a relatively simple robotic task to evaluate a\nset of ideas that might help to solve recent reinforcement learning problems. We test a newly\ncreated combination of two commonly used reinforcement learning methods, whether it is\nable to learn more effectively than a baseline. We also compare different ideas to preprocess\ninformation before it is fed to the reinforcement learning algorithm. The goal of this strategy\nis to reduce training time and eventually help the algorithm to converge. The concluding\nevaluation proves the general applicability of the described concepts by testing them using\na simulated environment. These concepts might be reused for future experiments.\niv\nChapter 1\nIntroduction\nArtificical intelligence is an interdisciplinary field, that receives input from many sources. A\nrecent renaissance of neural networks in machine learning is mainly motivated by technical\nconsiderations. Especially the availability of large data sets and massive computational\npower enabled deep network structures to solve problems, that were considered infeasible\nbefore (Silver, Huang, et al. 2016). The term deep learning generally refers to training any\nneural network structure with multiple computing layers. As more layers, that compute\nnon-linear functions of previous layers, are added, the complexity of the network rises and\nsolutions to more difficult tasks can be found. It has been shown, that even if using a shallow\nneural network model is possible, deeper architectures are often much more efficient, as they\ncan be trained faster and consume less memory (Goodfellow, Bengio, and Courville 2016).\nAn interesting implication of the success of deep neural networks is their application to\nreinforcement learning. In contrast to other machine learning techniques, reinforcement\nlearning can be used to learn complex strategies or behavior. Applications of reinforcement\nlearning range from optimization problems to robotics and complex control problems\n(Abbeel et al. 2007). Deep reinforcement learning incorporates the insights of deep learning\ninto reinforcement learning and thereby effectively overcomes many common limitations\nlike being able to act, when the received data is multi-dimensional and complex.\nThis thesis shortly motivates the use of reinforcement learning in chapter 2 by comparing it\nvery generally to other machine learning approaches. Chapter 3 states common limitations\nof reinforcement learning in practice and describes various ways to overcome these\nlimitations. The main purpose of this chapter is to show, that deep learning is the answer\nto many open questions in the field. Several ways to benefit from deep learning under the\ngiven circumstances will be discussed and compared.\n1\n1. Introduction\n2\nFinding solutions to complex problems with deep reinforcement learning can still be slow\nand there is sometimes no gurantee for a standard algorithm to succeed solving a previously\nunseen task, especially if there is no time to carefully choose the hyperparameters of\nthe algorithm. Chapter 4 describes ideas to train models that preprocess the available\ninformation in advance and thus simplify the reinforcement learning task. We call this\nstrategy pretraining.\nOne goal of this thesis is to investigate the concepts of chapter 4 and their practical\napplicability to algorithms of chapter 3. We also combine DDPG (deep deterministic\npolicy gradient; Lillicrap et al. 2015) and asynchronous methods (Mnih, Badia, et al.\n2016), which both are commonly used reinforcement learning methods, to form two new\nreinforcement learning algorithms, which will be investigated. We call these algorithms\ndistributed and asynchronous DDPG. The intention of combining two inventions, that were\nrecently succesful in the field, is to combine their respective benefits. Asynchronous methods\nenable fast training and generalize well to unseen situations. The DDPG algorithm is a good\nchoice for learning a continuous action space. This means, that a discrete choice between\nseveral options is not satisfactory, but complex continuous actions need to be generated for\na correct behavior. Chapter 5 introduces these algorithms, but also describes a simulated\nrobotic environment that was used for all evaluations and some interesting implementation\ndetails. We implemented the distributed and asynchronous DDPG algorithms and reused\na DDPG implementation as baseline.\nChapter 6 explains the experimental setups and states the obtained results. We evaluate\nall variants of DDPG and compare the respective scores, but also combine the DDPG\nbaseline with differently pretrained models for comparing the quality of various pretraining\ntechniques. Chapter 7 concludes with a discussion.\nChapter 2\nDifferent Types of Learning\n2.1\nSupervised Learning\nA large number of machine learning tasks like regression, classification or pattern\nrecognition aim to approximate a function ùë¶= ùëì(ùë•) given a set of training examples\n(ùë•(ùëñ), ùë¶(ùëñ)). These tasks can be viewed as approximating the probability distribution of the\ngiven data ùëùùëëùëéùë°ùëé(ùë¶|ùë•). The formulation is taken from Goodfellow, Bengio, and Courville\n(2016). While other formulations are possible, estimating a probability distribution is very\ngeneral and has the pleasing property that many learning algorithms can be derived easily,\nfor example using maximum likelihood estimation to match the distribution of the model\nto the data distribution.\nAs an example, performing maximum likelihood estimation in a simple supervised learning\nmodel with only one output is equal to minimizing the mean squared error (MSE) between\nthe outputs of the model and the targets ùë¶(ùëñ), if the distribution of the model is assumed to\nbe gaussian with fixed variance and mean ^ùë¶specified by the model with learnable parameters\nùúÉ(Goodfellow, Bengio, and Courville 2016). Maximum likelihood estimation is performed\nby maximizing the conditional log-likelihood over the training examples (ùë•(ùëñ), ùë¶(ùëñ)) to turn\nthe possibly large product of probabilities ‚àèÔ∏Ä\nùëñùëùùëöùëúùëëùëíùëô(ùë¶(ùëñ)|ùë•(ùëñ); ùúÉ) into a sum and overcome\nissues like numerical underflow. The training examples are assumed to be i.i.d. to make this\na valid conversion.\n^ùë¶(ùëñ) = ùëìùëöùëúùëëùëíùëô(ùë•(ùëñ); ùúÉ)\n(2.1.1)\nùëùùëöùëúùëëùëíùëô(ùë¶|ùë•; ùúÉ) = ùí©(ùë¶; ^ùë¶, ùúé2)\n(2.1.2)\n3\n2. Different Types of Learning\n4\nùúÉùëÄùêø= arg max\nùúÉ\n‚àëÔ∏Å\nùëñ\nlog ùëùùëöùëúùëëùëíùëô(ùë¶(ùëñ)|ùë•(ùëñ); ùúÉ)\n(2.1.3)\n= arg max\nùúÉ\n‚àëÔ∏Å\nùëñ\nlog(\n1\n‚àöÔ∏Ä\n2ùúãùúé2 ùëí\n‚àí(ùë¶(ùëñ)‚àí^ùë¶(ùëñ))2\n2ùúé2\n)\n(2.1.4)\n= arg max\nùúÉ\n‚àëÔ∏Å\nùëñ\n‚àílog ùúé‚àí1\n2 log(2ùúã) ‚àí(ùë¶(ùëñ) ‚àí^ùë¶(ùëñ))2\n2ùúé2\n(2.1.5)\nRemoving all terms that do not depend on the parameters ùúÉyields a formula very similar to\nMSE. Usually it is not possible to directly estimate ùúÉùëÄùêødue to computational limitations.\nTherefore, it is necessary to perform gradient descent to optimize the parameters step by\nstep.\nùúÉùëÄùêø= arg min\nùúÉ\n‚àëÔ∏Å\nùëñ\n(ùë¶(ùëñ) ‚àí^ùë¶(ùëñ))2\n(2.1.6)\n2.2\nUnsupervised Learning\nWhen there is no output specified, machine learning models can still approximate a\ndistribution ùëùùëëùëéùë°ùëé(ùë•) and thus learn to find structures inherent in the data. This can be\nuseful for tasks like denoising, clustering or pretraining some parameters of a model for\na supervised learning task. Vincent et al. (2008) and Kingma and Welling (2013) provide\nexamples of successfully applied unsupervised learning.\n2.3\nReinforcement Learning\nMany real world applications like robotic tasks or playing video games require learning\nto perform a series of actions ùëéùë°, for instance pressing buttons or moving a robotic arm\nto a target by controlling motors, while the state of the environment ùë†ùë°can be observed\nbetween the actions often forming a trajectory over time: ùúè= {ùë†1, ùëé1, ùë†2, ùëé2...ùë†ùëá}. Each\ncomplete trajectory starting from an initial state forms an episode, which can be ended\nafter a predefined number of time steps or after reaching a terminal state. Besides trajectory\ncentric or episodic reinforcement learning, there are continuing environments that do not\nuse trajectories of fixed length, but visit every possible state infinitely often with a certain\nprobability (Sutton and Barto 1998). A common property of reinforcement learning tasks is\nthat there is no previously known solution to the problem but an implicit aim in form of a\nreward, which could be the score in a video game or a distance measure stating how well a\nrobot performed the task of moving to a specific position. Learning from this kind of signal\n2. Different Types of Learning\n5\nis biologically plausible, because some brain areas like the basal ganglias are supposed to\nfollow the same strategy (Doya 2000).\nReinforcement learning enables intelligent algorithms to learn, when the objective is not to\ndirectly model a specific probability distribution like discussed above, but to maximize a\nscalar reward signal ùëüùë°+1 = ùëü(ùë†ùë°, ùëéùë°) assigned to each pair of state and action over time. The\nreward at each time step depends on both the current state and the chosen action. At each\ntime step the action ùëéùë°is sampled from a policy function ùëù(ùëéùë°) = ùúãùúÉ(ùëéùë°|ùë†ùë°) with learned\nparameters ùúÉ, which can also take the deterministic form ùëéùë°= ùúáùúÉ(ùë†ùë°). The next state ùë†ùë°+1\nis then generated by the dynamics of the environment, that are restricted to satisfy the\nmarkov property ùëù(ùë†ùë°+1|ùë†ùë°, ùëéùë°) = ùëù(ùë†ùë°+1|ùë†1, ùëé1...ùë†ùë°, ùëéùë°). The trajectories are thus sampled\nfrom a markov decision process (MDP). The state ùë†ùë°sometimes can be only partially\nobserved (Hausknecht and Stone 2015), which yields a partially observable markov decision\nprocess (POMDP).\nChapter 3\nVariants of Reinforcement Learning\nThe goal of episodic reinforcement learning algorithms is to maximize the expected\naccumulated reward or return ùëÖ1 = Eùúã(‚àëÔ∏Äùëá‚àí1\nùëñ=1 ùõæùëñ‚àí1ùëüùëñ+1) for all initial states ùë†1, where\nT denotes the end of the episode, ùëüùë°+1 the reward for state ùë†ùë°and action ùëéùë°, and ùõæ‚àà[0, 1]\nis a discounting factor for future rewards. At each time step, the direct reward depends\non the current state and the action chosen by the model and thus on the executed policy.\nTheoretically, many sampled trajectories including all possible states and actions at different\npositions in time would be needed to infer a policy that reliably maximizes the expected\nreturn over all possible trajectories. For continuing environments, an estimate of the average\nreward per time step denoted as ‚àëÔ∏Ä\nùë†‚ààùëÜùëùùúã(ùë†) ‚àëÔ∏Ä\nùëé‚ààùê¥ùúã(ùëé|ùë†)ùëü(ùë†, ùëé) can be used as a measure\nof the policy quality, where ùëÜand ùê¥are the respective sets of all states and actions and\nùëùùúã(ùë†) is the probability of arriving at state ùë†when following policy ùúã(Sutton and Barto\n1998). While we focus on episodic reinforcement learning in the following, it is possible to\napply most of the discussed algorithms also to continuing environments.\n3.1\nCommon Reinforcement Learning Issues\nMany recent improvements in the field of reinforcement learning are caused by the difficul-\nties that occur, when trying to apply standardized algorithms in realistic environments. In\nthe following, we will first name some of the main difficulties reinforcement learning algo-\nrithms are confronted with and then present various methods researchers have invented to\novercome these limitations, starting with relatively simple tabular methods that have been\nused for a long time and continuing with recent approaches to improve training performance\nand especially enable algorithms to deal with highly complex environments.\n6\n3. Variants of Reinforcement Learning\n7\nExploring the state space:\nIn contrast to randomly exploring the state space, especially deterministic policies, when\nthey are used to sample trajectories, do not visit large parts of the state space and thus\nlead to algorithms converging to suboptimal solutions. During sampling it is required to\ndeliberately include suboptimal actions to explore unseen parts of the state space. Whether\nto greedily choose the assumed best action or a random action is known as the exploitation-\nexploration tradeoff (Kaelbling, Littman, and Moore 1996; Woergoetter and Porr 2008).\nComplex state spaces:\nIn practice, reinforcement learning algorithms should be able to deal with large state spaces\nlike pixel images that could be observations of a camera (Mnih, Kavukcuoglu, Silver, Graves,\net al. 2013). This imposes the difficulty of learning a policy even when only a small subset\nof all possible states will be visited during training. The fact that any image generated by\na random generator almost never looks like any realistic scene, although there is a non-zero\nprobability for it to do so, suggests that comparatively few examples of an image of given\nsize correspond to natural images (Goodfellow, Bengio, and Courville 2016). Furthermore,\nsome of the possible states might not be reached during training due to restricted training\ntime. Images from cameras or other complex state representations might also appear noisy,\nwhich again complicates the task of learning a good policy.\nContinuous action spaces:\nFor realistic environments it is often not sufficient to design a policy that deterministically\noutputs a discrete action or stochastically models a probability distribution over a set of\ndiscrete actions. In robotic applications, motor torques must be produced that can take\nany continuous value and must be exact to make the robot move correctly. Although it is\nsometimes possible to successfully discretize a continuous action space, using this approach\nalways means losing flexibility (Lillicrap et al. 2015). For problems with multiple continuous\nactions, like producing multiple motor torques in parallel, the number of corresponding\ndiscrete actions rises exponentially, which quickly makes following this solution impossible.\nHigh variance of the trained estimator:\nWhile the policy estimation should normally converge to an optimum when the number\nof training examples is large, practical environments induce strong correlations between\nsamples that are temporally close to each other. A robot might be expected to behave very\nsimilarly in close situations and the state representation might not change significantly.\nThus, the variance of the trained estimator will be high, as the samples from the\nenvironment change slowly and the learned model will always tend to overfit the training\ndata currently presented and forget important past transitions (Lin 1992). This means it\nwill more likely fail to generalize to other data. The lack of generalization will at least\nnegatively affect the training performance or the algorithm will not be able to learn a\nresonable policy at all.\n3. Variants of Reinforcement Learning\n8\nPartially observable environments:\nFor some environments it might be impossible to observe the complete state ùë†ùë°. A robotic\ncamera for instance might not be able to capture the whole scene with all objects relevant\nfor the task, but only parts of it. The impact of the action that has to be generated by\nthe policy depends on the system dynamics, which base on ùë†ùë°. That makes it necessary\nto gather more information by memorizing multiple observations to predict a reasonable\naction (Hausknecht and Stone 2015).\n3.2\nAlgorithms that Optimize Value Functions\n3.2.1\nReinforcement Learning with Tabular Value Functions\nThe expected return for the initial state with respect to the policy is the target for\noptimization, like discussed above, as it spans the whole trajectory. However, only\nestimating ùëÖ1 is not sufficient to derive an optimal policy, because the policy also needs to\noutput an optimal action for every intermediate state. If both the state and action space\nare discrete, the straightforward way to find an optimal policy and thus maximize the\nexpected return over trajectories is to either estimate the optimal state-value function ùëâ*\nor the optimal action-value function ùëÑ* (Sutton and Barto 1998). The state-value function\nspecifies the expected return of a state ùë†ùë°when following an optimal policy, whereas the\naction-value function specifies the expected return when choosing action ùëéùë°in state ùë†ùë°and\nfollowing an optimal policy subsequently.\nùëâ*(ùë†ùë°) = max\nùëéùë°E[ùëüùë°+1 + ùõæùëâ*(ùë†ùë°+1)]\n(3.2.1)\nùëÑ*(ùë†ùë°, ùëéùë°) = E[ùëüùë°+1 + ùõæmax\nùëéùë°+1 ùëÑ*(ùë†ùë°+1, ùëéùë°+1)]\n(3.2.2)\nWith good estimates of all ùëâ*(ùë†ùë°), an optimal greedy policy can be easily derived by always\nchoosing the action which most probably leads to the best rated states. To estimate the\nstates the environment might take after executing an action, it is required to know the\ndynamics of the system. For system dynamics ùëù(ùë†ùë°+1|ùë†ùë°, ùëéùë°) action ùëéùë°would be chosen\naccording to:\narg max\nùëéùë°\n‚àëÔ∏Å\nùë†ùë°+1\nùëù(ùë†ùë°+1|ùë†ùë°, ùëéùë°)ùëâ*(ùë†ùë°+1)\n(3.2.3)\n3. Variants of Reinforcement Learning\n9\nWith good estimates of all ùëÑ*(ùë†ùë°, ùëéùë°) it is no longer necessary to know the dynamics of the\nsystem, because the action-value function implicitly captures the transition probabilities.\nMany learning algorithms thus focus on approximating ùëÑ* rather than ùëâ*. These algorithms\ncan be applied to a broad range of environments with different dynamics without the need\nto train a model of the environment and are thus called model-free (Kaelbling, Littman,\nand Moore 1996). The optimal action with respect to ùëÑ* can be chosen as simply as:\narg max\nùëéùë°\nùëÑ*(ùë†ùë°, ùëéùë°)\n(3.2.4)\nTo form a learning algorithm, the action-value function is redefined with respect to an\narbitrary policy ùúã. The value of ùëÑùúã(ùë†ùë°, ùëéùë°) corresponds to the expected return when taking\naction ùëéùë°in step ùë†ùë°and following policy ùúãsubsequently. The calculated estimates can\nthen be used to improve the policy. Intuitively, for optimal ùúã, the action-value function\nwith respect to the policy converges to ùëÑ*(ùë†ùë°, ùëéùë°). It is possible to estimate ùëÑùúã(ùë†ùë°, ùëéùë°) for\nexample using a Monte-Carlo estimate of the expected return after the end of each episode.\nThis strategy is called offline learning, because it must wait until an episode has finished.\nHowever, it is much more practical to store estimates of ùëÑùúã(ùë†ùë°, ùëéùë°) in an array ùëÑ(ùë†ùë°, ùëéùë°) and\nreuse existing estimates for future states, which is called bootstrapping and yields an online\nlearning rule (Sutton and Barto 1998):\nùëÑ(ùë†ùë°, ùëéùë°) += ùõº(ùëüùë°+1 + ùõæùëÑ(ùë†ùë°+1, ùëéùë°+1) ‚àíùëÑ(ùë†ùë°, ùëéùë°))\n(3.2.5)\nThe transitions between states are sampled from the system dynamics and the actions from\nthe policy ùúã. Continuously updating the estimates of ùëÑùúã(ùë†ùë°, ùëéùë°) while improving the policy\nto follow the maximal Q-values leads to the SARSA algorithm presented in Listing 3.1.\nListing 3.1: SARSA (State-Action-Reward-State-Action), reproduced in essence from Sutton\nand Barto (1998).\n1 Initialize Q(s,a) arbitrarily\n2 Repeat (for each episode):\n3\nInitialize ùë†1\n4\nChoose ùëé1 from ùë†1 using policy derived from ùëÑ\n5\nRepeat (for each ùë°of episode):\n6\nTake action ùëéùë°, observe ùëüùë°, ùë†ùë°+1\n7\nChoose ùëéùë°+1 from ùë†ùë°+1 using policy derived from ùëÑ\n8\nùëÑ(ùë†, ùëé) ‚ÜêùëÑ(ùë†, ùëé) + ùõº[ùëüùë°+1 + ùõæùëÑ(ùë†ùë°+1, ùëéùë°+1) ‚àíùëÑ(ùë†ùë°, ùëéùë°)]\n9\nùë†ùë°‚Üêùë†ùë°+1; ùëéùë°‚Üêùëéùë°+1;\nSARSA performs on-policy bootstrapping, because the expected return ùëÑ(ùë†ùë°+1, ùëéùë°+1) of\nthe next state depends on the policy ùúã. It is also possible to alter the update rule to\nbootstrap using the best action in the next state yielding the popular Q-learning algorithm\n3. Variants of Reinforcement Learning\n10\nintroduced by Watkins and Dayan (1992), which is presented in Listing 3.2. This algorithm\ncan be viewed as directly approximizing the action-value function of an optimal policy, as\nthe policy used for bootstrapping is directly given by the algorithm and does not necessarily\nmatch the policy ùúãused for sampling. Therefore Q-learning is called an off-policy algorithm.\nThe Q-values in Q-learning are updated as follows:\nùëÑ(ùë†ùë°, ùëéùë°) += ùõº(ùëüùë°+1 + ùõæùëöùëéùë•ùëéùëÑ(ùë†ùë°+1, ùëé) ‚àíùëÑ(ùë†ùë°, ùëéùë°))\n(3.2.6)\nListing 3.2: Q-Learning, reproduced in essence from Sutton and Barto (1998).\n1 Initialize Q(s,a) arbitrarily\n2 Repeat (for each episode):\n3\nInitialize ùë†1\n4\nRepeat (for each ùë°of episode):\n5\nChoose ùëéùë°from ùë†ùë°using policy derived from ùëÑ\n6\nTake action ùëéùë°, observe ùëüùë°, ùë†ùë°+1\n7\nùëÑ(ùë†, ùëé) ‚ÜêùëÑ(ùë†, ùëé) + ùõº[ùëüùë°+1 + ùõæmaxùëéùëÑ(ùë†ùë°+1, ùëé) ‚àíùëÑ(ùë†ùë°, ùëéùë°)]\n8\nùë†ùë°‚Üêùë†ùë°+1;\n3.2.2\nUsing Stochastic Policies to Improve Exploration\nOne beneficial property of an off-policy learning approach like Q-learning (Listing 3.2) is\nthat any policy can be used for sampling, while the learning rule is not affected. It is in\nparticular possible to design a policy for sampling that encourages exploration and thus\nleads to a better search in the state space as stated above, while the learned policy is still\ngreedy.\nAn exploring policy needs to be based on some kind of randomization to detect unknown or\nless frequently visited regions in state space. Policies of that kind do not deterministically\npredict an action to execute but rather model a probability distribution from which\nthe actions will be sampled. With off-policy training algorithms it is possible to train a\ndeterministic policy while using a stochastic policy for sampling.\nA popular choice for a policy that can be used to encourage exploration during sampling,\nwhen in most cases the best action known so far (greedy action) should be executed is the\nùúñ-greedy policy (Sutton and Barto 1998). The parameter ùúñ‚àà[0, 1] specifies the probability\nof selecting a completely random action. In all other cases the assumed best action will be\nexecuted:\nùëéùë°=\n{Ô∏É\na random action\nwith probability ùúñ\narg maxùëéùëÑ(ùë†ùë°, ùëé)\notherwise\n(3.2.7)\n3. Variants of Reinforcement Learning\n11\nThis strategy however ignores the fact, that there might be large differences between the Q-\nvalues of the non-optimal actions, which are all chosen with equal probability. The softmax\npolicy (Sutton and Barto 1998) consideres this fact and assigns a different probability to\neach action that depends on the Q-value of the action:\nùëù(ùëéùë°)ùë†ùë°=\nùëíùëÑ(ùë†ùë°,ùëéùë°)/ùúè\n‚àëÔ∏Ä\nùëéùëíùëÑ(ùë†ùë°,ùëé)/ùúè\n(3.2.8)\nThe parameter ùúèis called the temperature and regularizes the amount of randomization\ninduced. When ùúèdecreases, the policy becomes more and more deterministic or greedy,\nwhile increasing ùúèencourages exploration.\n3.2.3\nUsing Eligibility Traces to Improve Bootstrapping\nThe bootstrapping idea presented in section 3.2.1 only supports direct updates of the value\nfunction estimates with respect to the estimated value of the following action. This can\nconsiderably slow down learning, especially if there are long chains of consecutive actions.\nA robot might need to appoach a target for a long time before reaching it and thus might\nget a reward only after executing many steps beforehand. In this case, the reward associated\nwith the last action is known as a delayed reward (Watkins 1989). To obtain positive value\nestimates for the first actions in the chain, many episodes are needed, because during the\nfirst episode only the value of the last action is updated, then the value of the second\nlast action due to bootstrapping and so on. This problem is also known as the credit\nassignment problem (Woergoetter and Porr 2008), as present actions might be essential for\nfuture rewards, but do not directly benefit from it.\nWith Monte Carlo estimates it is not necessary to repeatedly visit the same consecutive\nactions multiple times to obtain valid value estimates at the beginning of the chain, because\nthe accumulated return over all future actions is directly used to form the estimates. Using\nMonte Carlo methods however causes other issues like high variance. It is possible to\novercome this tradeoff by using n-step bootstrapping, that spans over multiple steps and\ncan also include weights to raise the impact of rewards to temporally close actions (Sutton\nand Barto 1998). N-step bootstrapping provides an intermediate solution that combines the\nadvantages of Monte Carlo estimates and direct bootstrapping from the next action-value.\nThe Q-Learning algorithm can be viewed as comparing the expected return by choosing a\nspecific action to the current action-value estimate. The expected return is obtained using\nbootstrapping from the next action value, called one-step bootstrapping. The update of the\ncurrent Q-value with a learning rate ùõºis then given by:\n3. Variants of Reinforcement Learning\n12\nŒîùëÑ(ùë†ùë°, ùëéùë°) = ùõº(ùëÖ(1)\nùë°\n‚àíùëÑ(ùë†ùë°, ùëéùë°))\n(3.2.9)\nùëÖ(1)\nùë°\n= ùëüùë°+1 + ùõæùëöùëéùë•ùëéùëÑ(ùë†ùë°+1, ùëé)\n(3.2.10)\nThe term ùëÖ(1)\nùë°\nis called the one-step return. In contrast, the Monte Carlo estimate starting\nfrom state t would accumulate the rewards till the end of the episode without any\nbootstrapping:\nùëÖùë°=\nùëá‚àí1\n‚àëÔ∏Å\nùëñ=ùë°\nùõæùëñ‚àíùë°‚àí1ùëüùëñ+1\n(3.2.11)\nThe n-step return mixes both views and bootstraps from the value of the action executed\nn timesteps after t. If the episode ends earlier, the n-step return is equivalent to the Monte\nCarlo estimate ùëÖùë°.\nùëÖ(ùëõ)\nùë°\n= ùõæùëõùëöùëéùë•ùëéùëÑ(ùë†ùë°+ùëõ, ùëé) +\nùë°+ùëõ‚àí1\n‚àëÔ∏Å\nùëñ=ùë°\nùõæùëñ‚àíùë°‚àí1ùëüùëñ+1\n(3.2.12)\nAs the best value of n is hard to predict, but it is straightforward to see that temporally\ncloser actions should have a greater effect on each other, we can accumulate all n-step\nreturns with a decaying weighting factor ùúÜ(ùëõ‚àí1):\nùëÖ(ùúÜ)\nùë°\n= (1 ‚àíùúÜ)\n‚àû\n‚àëÔ∏Å\nùëõ=1\nùúÜùëõ‚àí1ùëÖ(ùëõ)\nùë°\n(3.2.13)\nIt is even possible to formulate an online learning algorithm using this idea, by memorizing\nactions executed in the past. The eligibility trace (Sutton 1988) assigns a value to each\npair of state and action that is reset to 1, when the action is executed in the respective state\nand then decays by ùõæùúÜafter every time step. Q-Learning with eligibility traces is called\nQ(ùúÜ) learning and updates all Q-values at every time step as follows:\n‚àÄùë†,ùëé: ùëÑ(ùë†, ùëé) = ùëÑ(ùë†, ùëé) + ùõºùõøùë°ùëí(ùë†, ùëé)\n(3.2.14)\nùõøùë°= ùëüùë°+1 + ùõæmax\nùëé\nùëÑ(ùë†ùë°+1, ùëé) ‚àíùëÑ(ùë†ùë°, ùëéùë°)\n(3.2.15)\n‚àÄùë†,ùëé: ùëí(ùë†, ùëé) =\n{Ô∏É\n1\nùëñùëì(ùë†, ùëé) = (ùë†ùë°, ùëéùë°)\nùõæùúÜùëí(ùë†, ùëé)\nùëúùë°‚Ñéùëíùëüùë§ùëñùë†ùëí\n(3.2.16)\n3. Variants of Reinforcement Learning\n13\nThe term ùõøùë°is known as the TD-error for step t and ùëí(ùë†, ùëé) is the eligibility trace assigned\nto state s and action a. It is straightforward to see, that eligibility traces mix the ideas\nof Monte Carlo estimates and one-step bootstrapping, as the update rules are equivalent\nto Monte Carlo estimates for ùúÜ= 1 and to one-step bootstrapping for ùúÜ= 0. In practice,\nfurther adjustments might be necessary to regard the case, when exploration happens (see\nsection 3.2.2) and thus the chain of assumed optimal actions is broken (Watkins 1989).\nIt is also possible to accumulate the value of eligibility traces, when a pair of state and\naction is visited often in a short length of time. Figure 3.1 compares replacing traces used\nin equation 3.2.16 to accumulating traces.\nFigure 3.1: Accumulating eligibility traces do not have a fixed maximum value, but can grow\nlarger than 1, when a pair of state and action is visited often. Figure adapted from Sutton\nand Barto (1998).\n3.2.4\nUsing Neural Networks to Approximate the Q-function\nThe tabular methods described in the sections above do not apply to large discrete or\ncontinuous state spaces, which require a specific function approximator to estimate the\nQ-function. Estimating the Q-function can be done by a supervised learning algorithm\nwith the targets for training given by the reinforcement learning algorithm. Therefore a\nloss function is introduced that drives the function approximator to output the correct\nQ-values, where ùëÑ(ùë†ùë°, ùëé; ùúÉ) is a function, parametrized by learned parameters ùúÉ:\n‚Ñí(ùë†ùë°, ùëéùë°, ùëüùë°+1, ùë†ùë°+1, ùúÉ) = (ùëüùë°+1 + ùõæùëöùëéùë•ùëéùëÑ(ùë†ùë°+1, ùëé; ùúÉ) ‚àíùëÑ(ùë†ùë°, ùëéùë°; ùúÉ))2\n(3.2.17)\nNeural networks have proven to be effective for function approximation and supervised\nlearning tasks. Architectures solving supervised learning tasks, like estimating the Q-\nvalues in the reinforcement learning setting, usually take the form of feed-forward neural\nnetworks. This means, the neurons are organized in layers, that are ordered and connected\n3. Variants of Reinforcement Learning\n14\nin only one direction from the input layer to the output layer. Normally, there are no\nconnections between neurons in a single layer. Deep feed-forward neural networks with\nmultiple intermediate layers (hidden layers) between input and output layer are able to\ndeal with complex state spaces and can generalize to unknown states that were never\nobserved during training (Levine, Finn, et al. 2016). A neural network with only feed-\nforward connections, non-linear activation functions and one or more hidden layers is\nalso known as a multi-layer perceptron (MLP). The architecture of a simple Q-network\nis depicted in figure 3.2. The network takes the state representation as input and outputs\nthe Q-values for all actions using seperate output neurons. The action space thus still needs\nto be discrete to enable the network to learn the Q-values for all actions.\nstates\nmultilayer neural network\nQ value 1\nQ value 2\nQ value 3\nFigure 3.2: Architecture of a simple deep Q-network with 3 discrete actions.\n3.2.5\nUsing Convolutional Layers to Process Pixel Images\nTo extract useful information from pixel images, it is often not sufficient to only use fully\nconnected feed-forward layers, where every neuron of one layer is connected to every unit\nof the next layer. Like required by other supervised learning tasks, in the reinforcement\nlearning setup, neural networks should be able to perform a generalization task and reduce\nthe complexity of the state space to being able to correctly predict the Q-values later. While\nit is theoretically possible to capture the information presented in an image by purely using\nfully connected layers, network structures like these induce several problems.\nFirst of all, a great number of neurons and thus even more connections would be needed,\nbecause each pixel in an image must be represented by a single neuron or even multiple\n3. Variants of Reinforcement Learning\n15\nFigure 3.3: Convolutional layers can be used to detect features in images and effectively\neliminate background noise. (a) Sample activations of the input layer of a neural network.\n(b) Corresponding activations of neurons in the first hidden layer, produced by a filter that\ndetects mainly low level features like edges. (c) Corresponding activations of neurons in the\nsecond hidden layer, produced by a filter that detects high level features like the position of\nthe star in the image.\nneurons for multiple color channels. To store the associated parameters, massive amounts\nof memory would be required and it would also take comparatively long time to calculate\nthe outputs of the network for given inputs. Because of the large amount of parameters,\nfully connected layers converge slowly and can even completely fail to capture the relevant\ninformation of larger pixel images, if not carefully designed. Downsampling of the images\nreduces the complexity of the network, but also discards a lot information.\nConvolutional networks (CNNs) were successfully applied by Krizhevsky, Sutskever, and\nG. E. Hinton (2012) to win the ImageNet challenge with an exceptional good score. The\nconvolution operation applied to neural networks deliberately restricts the connections\nbetween two layers to be local. This is a reasonable assumption, as the pixels in an image,\nthat are close to each other, are normally much stronger correlated. Each unit of the first\nhidden layer is thus a function of only a small patch of the input image. Furthermore, the\nconnections between each image patch and the corresponding unit in the next layer are\nrestricted to be equal over the whole image, which again reduces the amount of parameters\nneeded. Figure 3.3 shows a simple example of convolutions applied to identify an object in\nan image.\nThe filter function that slides over the image to produce the next neural layer is called\na kernel and output of the convolution operation a feature map of the input (Goodfellow,\nBengio, and Courville 2016). Most of the time, an additional activation function is executed\nafter the convolution operation to transform the elements of the feature map to the\nactivations of the next layer. Multiple learned kernels can be used to produce multiple\nfeature maps for detecting different features in the image. Stacking convolutional layers\n3. Variants of Reinforcement Learning\n16\nenables the network to learn a hierarchical form of dependencies between pixels in distant\nregions of the original input image.\nEspecially for applications that are only interested in extracting features from images\nand not in where these features occur in the image, it is useful to downsample the\nintermediate feature maps. This operation is called pooling (Scherer, M√ºller, and Behnke\n2010). Fully connected layers can then be used to further process the output after several\nconvolutional layers, which has greatly reduced dimensionality. Convolutional layers can\nalso be transposed to reconstruct image data from a lower dimensional representation.\nEach activation in a feature map produced by a convolutional layer is a function of the\nconvolution kernel and the input. For X being the two-dimensional input image, K the\nkernel and Y the elements of the feature map, the convolution operation (denoted as *) can\nbe applied to transform X to Y:\nùëå= ùëã* ùêæ\n(3.2.18)\nThe convolution operation flips the kernel to obtain a commutative operation. If a two-\ndimensional matrix kernel is used, that has odd height m and width n to be centered on\none pixel whose coordinates in the kernel are defined to be (0,0), equation 3.2.18 decomposes\nto:\nùëå(ùëñ, ùëó) = (ùëã* ùêæ)(ùëñ, ùëó) =\nùëö‚àí1\n2\n‚àëÔ∏Å\nùë†1= ‚àíùëö+1\n2\nùëõ‚àí1\n2\n‚àëÔ∏Å\nùë†2= ‚àíùëõ+1\n2\nùëã(ùëñ‚àíùë†1, ùëó‚àíùë†2)ùêæ(ùë†1, ùë†2)\n(3.2.19)\n3.2.6\nDeep Q-network (DQN)\nMnih, Kavukcuoglu, Silver, Graves, et al. (2013) showed that deep learning with\nconvolutional layers can enable reinforcement learning algorithms to successfully learn to\nplay Atari 2600 games. An improved version of this approach was presented later as deep\nQ-network (Mnih, Kavukcuoglu, Silver, Rusu, et al. 2015), that was able to use direct\ntraining from pixels to actions to play 49 different Atari games without the need to change\nthe hyperparameters of the network or make any other modifications for a specific game.\nZhang et al. (2015) showed how to train a robotic arm with the DQN approach to perform\nreaching tasks while only observing camera images. The performance on Atari games is\ncomparatively impressive, as the learned policies were often able to outperform human\nplayers and only the pixel images and the game score were used as input to the training\nalgorithm, which means that there was no domain knowledge available to the algorithm.\n3. Variants of Reinforcement Learning\n17\nWhile some Atari games can be directly modeled as fully observable MDPs as discussed in\nsection 2.3, it is not possible to infer properties like the velocity of objects from a single\nimage, which is necessary to establish effective strategies for some games. Therefore, a\nsequence of four frames is passed into the network to provide the missing information to\nthe network and approximately satisfy the markov property.\nThe success of DQN however does not only rely on the usage of a neural network function\napproximator. There are some problems that would practically prevent neural networks\nas nonlinear function approximators from converging. First, the loss function given in\nequation 3.2.17 includes the parameters ùúÉtwice, which arguably makes learning instable.\nThe foresight into the future ùëÑ(ùë†ùë°+1, ùëé; ùúÉ) to be used for bootstrapping should not directly\ndepend on ùúÉto stabilize learning and reduce the variance of the approximated function.\nThe DQN training method therefore introduces a target Q-network, that copies the\nparameters from the trained Q-network only after several hundred or thousand training\nsteps and thus does not change rapidly and enables the algorithm to learn stable long term\ndependencies (Mnih, Kavukcuoglu, Silver, Rusu, et al. 2015). With parameters ùúÉ‚àíof the\ntarget network, the loss function changes to:\n‚Ñíùê∑ùëÑùëÅ(ùë†ùë°, ùëéùë°, ùëüùë°+1, ùë†ùë°+1, ùúÉ, ùúÉ‚àí) = (ùëüùë°+1 + ùõæùëöùëéùë•ùëéùëÑ(ùë†ùë°+1, ùëé; ùúÉ‚àí) ‚àíùëÑ(ùë†ùë°, ùëéùë°; ùúÉ))2\n(3.2.20)\nUnfortunately, simple gradient descent on the loss function with target network can still\nlead to high variance of the function estimator due to the inherent structure of the learned\ndata. Especially if collecting more training examples is costly as it is with data generated\nby a real robotic system, other ways need to be found to encourage generalization of the\ntrained network. The technique used for training the DQN is essentially equivalent to\nstochastic gradient descent on a memory of past transitions of the reinforcement learning\nenvironment called the experience replay memory (Lin 1992). The idea behind stochastic\ngradient descent is to use random samples of relatively few training examples to estimate\nthe expectation of the true training error. When the examples are sampled from very\ndifferent time steps and were generated under different conditions, they can be sufficient\nto provide a good estimate of the true trainig error with relatively low variance. The\nexperience replay memory stores transitions between states sampled in the past, even for\nmultiple episodes, and also memorizes the corresponding actions and rewards to being able\nto correctly calculate the loss at every time step in the future. Thus, the memory consists\nof samples (ùë†ùëñ, ùëéùëñ, ùëüùëñ+1, ùë†ùëñ+1) for each recorded time step.\n3. Variants of Reinforcement Learning\n18\nListing 3.3: Deep Q-Network (DQN) with experience replay and target network, adapted\nfrom Mnih, Kavukcuoglu, Silver, Rusu, et al. (2015).\n1 Initialize replay memory ùê∑\n2 Initialize action-value function ùëÑwith random weights ùúÉ\n3 Initialize target action-value function ^ùëÑwith weights ùúÉ‚àí= ùúÉ\n4 for episode = 1 to ùëÄdo\n5\nInitialize sequence ùë†1 = {ùë•1} and preprocessed sequence ùúë1 = ùúë(ùë†1)\n6\nfor ùë°= 1 to ùëádo\n7\nSelect ùëéùë°=\n{Ô∏É\na random action\nwith probability ùúñ\narg maxùëéùëÑ(ùúë(ùë†ùë°), ùëé; ùúÉ)\notherwise\n8\n9\nExecute action ùëéùëñin emulator and observe reward ùëüùë°and image ùë•ùë°+1\n10\nSet ùë†ùë°+1 = ùë†ùë°, ùëéùë°, ùë•ùë°+1 and preprocess ùúëùë°+1 = ùúë(ùë†ùë°+1)\n11\nStore transition (ùúëùë°, ùëéùë°, ùëüùë°, ùúëùë°+1) in ùê∑\n12\n13\n// sample from experience replay memory\n14\nSample random minibatch of transitions (ùúëùëó, ùëéùëó, ùëüùëó, ùúëùëó+1) from ùê∑\n15\nSet ùë¶ùëó=\n{Ô∏É\nùëüùëó\nif episode terminates at step ùëó+ 1\nùëó+ ùõæmaxùëé‚Ä≤ ^ùëÑ(ùúëùëó+1, ùëé‚Ä≤; ùúÉ‚àí)\notherwise\n16\nPerform a gradient descent step on (ùë¶ùëó‚àíùëÑ(ùúëùëó, ùëéùëó; ùúÉ))2 w.r.t. to the network\nparameters ùúÉ\n17\n18\n// update target network\n19\nEvery ùê∂steps reset ^ùëÑ= ùëÑ, that means, set ùúÉ‚àí= ùúÉ\n20\nend for\n21 end for\nLet ùëödenote the number of all examples stored in the experience replay memory and\nùëö‚Ä≤ a relatively small number of examples, which are supposed to be randomly sampled\nfrom the memory, called the batch size, where ùëö‚Ä≤ ‚â™ùëö. The goal is to minimize the\nloss for all examples ùêΩ(ùúÉ) = 1\nùëö\n‚àëÔ∏Äùëö\nùëñ=1 ùêø(ùë†ùëñ, ùëéùëñ, ùëüùëñ+1, ùë†ùëñ+1, ùúÉ). For simplicity, the uninteresting\ndependency of the loss function on ùúÉ‚àíis left out here, but could be easily added. The sum\nof all losses can be approximized as follows:\nùêΩ(ùúÉ) = 1\nùëö\nùëö\n‚àëÔ∏Å\nùëñ=1\n‚Ñí(ùë†ùëñ, ùëéùëñ, ùëüùëñ+1, ùë†ùëñ+1, ùúÉ) ‚âà1\nùëö‚Ä≤\nùëö‚Ä≤\n‚àëÔ∏Å\nùëñ=1\n‚Ñí(ùë†ùëñ, ùëéùëñ, ùëüùëñ+1, ùë†ùëñ+1, ùúÉ)\n(3.2.21)\nGradient descent can then be intuitively performed by shifting the parameters in the\ndirection of the negative gradient:\nùëî= ‚àáùúÉùêΩ(ùúÉ) ‚âà1\nùëö‚Ä≤ ‚àáùúÉ\nùëö‚Ä≤\n‚àëÔ∏Å\nùëñ=1\n‚Ñí(ùë†ùëñ, ùëéùëñ, ùëüùëñ+1, ùë†ùëñ+1, ùúÉ)\n(3.2.22)\n3. Variants of Reinforcement Learning\n19\nùúÉ‚àí= ùõºùëî\n(3.2.23)\nListing 3.3 shows the DQN algorithm of the original publication in pseudocode including all\nmain concepts discussed so far, namely: using neural networks as function approximators\nfor the action-value function (Q-function), including target networks for bootstrapping of\nthe action-value function, and using experience replay as a variant of stochastic gradient\ndescent.\nThe parameter ùõºin equation 3.2.23 denotes the learning rate. In fact, simple gradient\ndescent like this can work reasonably well for a learning rate that is appropriate for the\noptimized problem, but also very likely fails to converge for a randomly chosen and fixed\nlearning rate. While it is possible to find a suitable learning rate by searching the the\nhyperparameter space, this would be computationally expensive as many training steps\nneed be done to evaluate a single learning rate. Gradient descent can be seen as navigating\nthrough the hyperplane spanned by the unified loss ùêΩ(ùúÉ) (error function) and all parameters\nùúÉùëñbeing the coordinates. Learning with any fixed learning rate thus can be slow, because it\ntakes long time to move across flat regions, where the gradient is very small. When gradient\ndescent arrives at a specific point of the hyperplane, it is possible for the error function to\nrapidly decrease in only one or few directions of the parameter space while moving in other\ndirections leaves the error nearly unchanged (Goodfellow, Bengio, and Courville 2016). This\nmotivates the use of learning algorithms that adapt to the shape of the parameter space.\nFigure 3.4: With the additional momentum term, gradient descent arrives faster at the\nminimum of the cost function without wasting too much time for oscillation. Figure adapted\nfrom Goodfellow, Bengio, and Courville (2016).\n3. Variants of Reinforcement Learning\n20\nThe RMSProp algorithm introduced by Tieleman and G. Hinton (2012) is able to sidestep\nthe issue of a fixed learning rate by using a seperate learning rate for each direction of\nthe parameter space and automatically adapting these learning rates to the magnitude\nof the gradient for the respective direction. It is often beneficial to include an additional\nmomentum term in the parameter update that plays the same role like velocity in physics.\nThis term is for example good for reducing the probability of oscillation, when the\nhyperplane for which the minimum should be found looks like a valley with steep sides. An\nexample of such a situation is depicted in figure 3.4. The Adam algorithm (Kingma and Ba\n2014) is a modification of RMSProp, that includes a momentum term by default.\n3.2.7\nImprovements to DQN\nDouble DQN (D-DQN) as proposed by Van Hasselt, Guez, and Silver (2016) was again\nable to improve the performance of DQN applied to Atari games by a minor modification\nof the training target. The loss function with target network given in equation 3.2.20 is\nconsidered problematic, because it tends to overestimate the future action values. Both the\nselection and evaluation of future actions depend on the parameters of the target network\nùúÉ‚àí. Equation 3.2.20 can thus be rewritten as:\n‚Ñíùê∑ùëÑùëÅ(ùë†ùë°, ùëéùë°, ùëüùë°+1, ùë†ùë°+1, ùúÉ, ùúÉ‚àí) =\n(ùëüùë°+1 + ùõæùëÑ(ùë†ùë°+1, arg max\nùëé\nùëÑ(ùë†ùë°+1, ùëé; ùúÉ‚àí); ùúÉ‚àí) ‚àíùëÑ(ùë†ùë°, ùëéùë°; ùúÉ))2\n(3.2.24)\nThe loss function used by D-DQN disentangles the action selection from the evaluation of\nthe selected action by using the trained parameters ùúÉto select future actions instead of\nthose of the target network:\n‚Ñíùê∑‚àíùê∑ùëÑùëÅ(ùë†ùë°, ùëéùë°, ùëüùë°+1, ùë†ùë°+1, ùúÉ, ùúÉ‚àí) =\n(ùëüùë°+1 + ùõæùëÑ(ùë†ùë°+1, arg max\nùëé\nùëÑ(ùë†ùë°+1, ùëé; ùúÉ); ùúÉ‚àí) ‚àíùëÑ(ùë†ùë°, ùëéùë°; ùúÉ))2\n(3.2.25)\nAnother popular approach to improve DQN called a dueling network architecture is\nproposed by Wang et al. (2015). The authors do not directly use the neural network to\npredict the Q-function. The network itself predicts two functions of the input with respect\nto the policy currently followed: A state-value function ùëâùúã(ùë†ùë°) and an advantage function\nùê¥ùúã(ùë†ùë°, ùëéùë°) defined as:\nùê¥ùúã(ùë†ùë°, ùëéùë°) = ùëÑùúã(ùë†ùë°, ùëéùë°) ‚àíùëâùúã(ùë†ùë°)\n(3.2.26)\n3. Variants of Reinforcement Learning\n21\nBoth outputs are unified to form an estimate of the Q-value which is then used for training.\nThe loss to be optimized can therefore include all previously made improvements and for\ninstance take the form of equation 3.2.25. It is also possible to use experience replay and\na target network exactly like discussed above. The straightforward way to derive ùëÑùúã(ùë†ùë°, ùëéùë°)\nfrom ùëâùúã(ùë†ùë°) and ùê¥ùúã(ùë†ùë°, ùëéùë°) would be to simply reorganize equation 3.2.26:\nùëÑùúã(ùë†ùë°, ùëéùë°) = ùê¥ùúã(ùë†ùë°, ùëéùë°) + ùëâùúã(ùë†ùë°)\n(3.2.27)\nAs the learning algorithm only optimizes the Q-function and does not know anything about\nthe underlying components, optimizing equation 3.2.27 almost never leads to ùëâùúã(ùë†ùë°) or\nùê¥ùúã(ùë†ùë°, ùëéùë°) converging to good estimates of a state-value or advantage function. To force\ngradient descent on ùëÑùúã(ùë†ùë°, ùëéùë°) to properly estimate these functions, a correction term is\nintroduced:\nùëÑùúã(ùë†ùë°, ùëéùë°) = ùê¥ùúã(ùë†ùë°, ùëéùë°) ‚àímax\nùëé\nùê¥ùúã(ùë†ùë°, ùëé) + ùëâùúã(ùë†ùë°)\n(3.2.28)\nWhen the assumed optimal action ùëé* = arg maxùëéùê¥ùúã(ùë†ùë°, ùëé) = arg maxùëéùëÑùúã(ùë†ùë°, ùëé) is chosen in\nstate ùë†ùë°, all terms including the advantage function cancel out and the state-value function\nis optimized to be equal to the action-value function. This is a valid optimization, as the\nvalue of a state corresponds to the value of the best action to be chosen in that state,\nformally: ùëâùúã(ùë†ùë°) = maxùëéùëÑùúã(ùë†ùë°, ùëé) = ùëÑùúã(ùë†ùë°, ùëé*). As ùëâùúã(ùë†ùë°) is tied to be a valid estimate\nof the state-value function, the other terms naturally approximate the advantage function\ndefined as the difference between state-value and action-value function. Other forms of the\ncorrection term like averaging over all possible actions are able to achieve a similar effect\nand yield better results in practice (Wang et al. 2015).\n3. Variants of Reinforcement Learning\n22\n3.3\nAlgorithms that Follow the Policy Gradient\nSection 3.2 showed that it is possible to derive reasonably performing policies from good\nestimates of value functions, especially useful are estimates of action-values provided by the\nQ-function. However, because policies derived from value functions search over a discrete\nnumber of Q-values to find the best action, it is not possible to directly obtain policies that\noutput continuous actions using one of the methods described above.\nPolicy gradient methods directly parametrize the policy as a probability function ùúãùúÉ(ùëéùë°|ùë†ùë°)\nthat can be completely described by the parameters ùúÉand thus provide maximal freedom\nto learn any action-generating function. To evaluate different policies, the expected return\nfollowing ùúãover all trajectories conditioned by the policy, formally ùúè‚àºùëùùúã(ùúè) = ùëù(ùúè|ùúÉ), is\nused. The return over a single trajectory ùëü(ùúè) is equal to the measure introduced at the\nbeginning of chapter 3 and takes the form:\nùëü(ùúè) =\nùëá‚àí1\n‚àëÔ∏Å\nùëñ=1\nùõæùëñ‚àí1ùëüùëñ+1\n(3.3.1)\nThe term ùëüùëñ+1 is the reward given to action ùëéùëñexecuted in state ùë†ùëñof the respective\ntrajectory. The probability distribution over trajectories ùëù(ùúè|ùúÉ) decomposes as follows:\nùëù(ùúè|ùúÉ) = ùëù({ùë†1, ùëé1..ùë†ùëá, ùëéùëá}|ùúÉ) = ùëù(ùë†1)\nùëá‚àí1\n‚àèÔ∏Å\nùëñ=1\nùúãùúÉ(ùëéùëñ|ùë†ùëñ)ùëù(ùë†ùëñ+1|ùë†ùëñ, ùëéùëñ)\n(3.3.2)\nWith these definitions, the target for optimization can be defined:\nùêΩ(ùúÉ) =\n‚àëÔ∏Å\nùúè‚ààùëá\nùëù(ùúè|ùúÉ)ùëü(ùúè)\n(3.3.3)\nWhile other methods like using evolutionary algorithms or random search are possible\n(Deisenroth, Neumann, Peters, et al. 2013), it is straightforward to optimize equation 3.3.3\nusing gradient ascend:\nùúÉ+= ùõº‚àáùúÉùêΩ\n(3.3.4)\nAccording to Peters and Bagnell (2011), following the policy gradient to solve reinforcement\nlearning tasks only slightly modifies the parameters of the policy in contrast to value based\nmethods, where large jumps between two estimated policies are possible. This property\n3. Variants of Reinforcement Learning\n23\narguably improves training stability and convergence towards an optimal policy. In the\nfollowing, several methods to estimate the gradient of the true expected return with respect\nto the parameters of the policy will be discussed.\n3.3.1\nFinite-Difference Methods\nThe main difficulty imposed by equation 3.3.4 is to derive an appropriate estimate of ‚àáùúÉùêΩ.\nAnalytically calculating the gradient is impossible, as it would be necessary to sum over\npossibly infinitely many trajectories. The dynamics of the environment ùëù(ùë†ùë°+1|ùë†ùë°, ùëéùë°) might\nalso be unknown and not differentiable anyway. Finite-difference methods (FDM) are simple\nnumerical methods to estimate first order gradients. FDM use the first two terms of the\ntaylor series expansion and rearrange them to get an approximation of the true gradient.\nAmong others, Peters and Bagnell (2011) provide a compact description of how to apply\nFDM to find the policy gradient in the reinforcement learning setup. Let ùúÉùëñdenote the i-th\nelement of the parameter vector ùúÉand ùúÉ+ ùõºùë¢ùëña small perturbation of the i-th component\nof ùúÉ. The taylor series expansion approximates ùêΩ(ùúÉ+ ùõºùë¢ùëñ) by using its partial derivatives:\nùêΩ(ùúÉ+ ùõºùë¢ùëñ) = ùêΩ(ùúÉ) + ùõºùúïùêΩ(ùúÉ)\nùúïùúÉùëñ\n+ ùõº2\n2\nùúï2ùêΩ(ùúÉ)\nùúïùúÉùëñ\n2\n+ ... + ùõºùëõ\nùëõ!\nùúïùëõùêΩ(ùúÉ)\nùúïùúÉùëñ\nùëõ\n+ ùëÖùëõ(ùúÉ+ ùõºùë¢ùëñ)\n(3.3.5)\nThe partial derivative of ùêΩwith respect to ùúÉùëñcan thus be approximated as follows:\nùúïùêΩ(ùúÉ)\nùúïùúÉùëñ\n‚âàùêΩ(ùúÉ+ ùõºùë¢ùëñ) ‚àíùêΩ(ùúÉ)\nùõº\n(3.3.6)\nTo obtain the gradient for all components of the parameter vector, all partial derivatives\nmust be approximated. The FDM approach has the beneficial property that it can be used\nfor arbitrary policies, even for not differentiable policies. While the estimate of the gradient\ncan be improved by averaging multiple estimated gradients for different perturbations, badly\nchosen perturbation can still make learning instable or cause it to fail (Peters and Bagnell\n2011). For realistic applications, the used policy can be assumed to be a differentiable\nfunction, which enables other estimations to work that are less error-prone and noisy.\n3.3.2\nLikelihood-Ratio Methods\nLet ùëù(ùúè|ùúÉ) = ùëùùúÉ(ùúè) be the probability of trajectory ùúèunder policy ùúãùúÉ. The likelihood ratio\ntrick, best known for its application in the REINFORCE algorithm introduced by Williams\n3. Variants of Reinforcement Learning\n24\n(1992), rewrites the gradient in equation 3.3.4 using the property ‚àáùúÉlog ùëùùúÉ(ùúè) = ‚àáùúÉùëùùúÉ(ùúè)\nùëùùúÉ(ùúè)\nas\nfollows:\n‚àáùúÉùêΩ(ùúÉ) =\n‚àëÔ∏Å\nùúè‚ààùëá\nùëùùúÉ(ùúè)‚àáùúÉlog ùëùùúÉ(ùúè)ùëü(ùúè)\n(3.3.7)\n= Eùúè‚àºùëù(ùúè|ùúÉ)[‚àáùúÉlog ùëùùúÉ(ùúè)ùëü(ùúè)]\n(3.3.8)\nThe expectation of equation 3.3.8 is useful to estimate the gradient of ùêΩùúÉwhile avoiding the\nsum over all trajectories, which is intractable. The inner term ‚àáùúÉlog ùëùùúÉ(ùúè)ùëü(ùúè) still depends\non the possibly unknown or not differentiable system dynamics, which now can be easily\nexcluded using equation 3.3.2, because they do not depend on the parameters ùúÉ:\n‚àáùúÉlog ùëùùúÉ(ùúè) = ‚àáùúÉlog ùëù(ùë†1) +\nùëá‚àí1\n‚àëÔ∏Å\nùëñ=1\n‚àáùúÉlog ùúãùúÉ(ùëéùëñ|ùë†ùëñ) +\nùëá‚àí1\n‚àëÔ∏Å\nùëñ=1\n‚àáùúÉlog ùëù(ùë†ùëñ+1|ùë†ùëñ, ùëéùëñ)\n(3.3.9)\n=\nùëá‚àí1\n‚àëÔ∏Å\nùëñ=1\n‚àáùúÉlog ùúãùúÉ(ùëéùëñ|ùë†ùëñ)\n(3.3.10)\nThis means, all knowledge about the dynamics of the environment can be easily discarded\nto form a model-free estimate of the parameter gradient. In fact, the gradient can be\napproximated by sampling trajectories from the reinforcement learning environment to\nform a Monte-Carlo estimate yielding the REINFORCE learning rule (Williams 1992). For\nany differentiable stochastic policy, it is straightforward to obtain an unbiased estimate of\nthe gradient using this technique. Therefore, equation 3.3.1 and 3.3.10 are incorporated into\nequation 3.3.8 with m being the number of sampled trajectories and ùëáùëñthe length of i-th\ntrajectory. ùë†ùëñ\nùëóis the j-th state of the i-th trajectory, ùëéùëñ\nùëóthe j-th action of the i-th trajectory\nand ùëüùëñ\nùëó+1 the reward associated to both:\n‚àáùúÉùêΩ(ùúÉ) ‚âà1\nùëö\nùëö\n‚àëÔ∏Å\nùëñ=1\nùëáùëñ‚àí1\n‚àëÔ∏Å\nùëó=1\n‚àáùúÉlog ùúãùúÉ(ùëéùëñ\nùëó|ùë†ùëñ\nùëó)ùõæùëó‚àí1ùëüùëñ\nùëó+1\n(3.3.11)\nThe original REINFORCE algorithm additionally uses a baseline term to reduce the\nvariance of the gradient estimation. Williams (1992) show, that the baseline term does\nnot introduce a bias, if it is chosen independently from the selected actions. According to\n3. Variants of Reinforcement Learning\n25\nDegris, Pilarski, and Sutton (2012), a reasonable choice for the baseline term is to use an\nestimate of the state value ùëâùúã(ùë†ùë°). Equation 3.3.11 with incorporated baseline becomes to:\n‚àáùúÉùêΩ(ùúÉ) ‚âà1\nùëö\nùëö\n‚àëÔ∏Å\nùëñ=1\nùëáùëñ‚àí1\n‚àëÔ∏Å\nùëó=1\n‚àáùúÉlog ùúãùúÉ(ùëéùëñ\nùëó|ùë†ùëñ\nùëó)(ùõæùëó‚àí1ùëüùëñ\nùëó+1 ‚àíùëè(ùë†ùëñ\nùëó))\n(3.3.12)\n3.3.3\nActor-Critic Methods\nSection 3.3.2 has shown how to use the likelihood-ratio to estimate the gradient of the\ntarget function ùêΩ. Sutton, McAllester, et al. (2000) generalize this insight to the form of\nthe policy gradient theorem:\n‚àáùúÉùêΩ(ùúÉ) = E{ùë†1,ùëé1..ùë†ùëá,ùëéùëá}‚àºùëù(ùúè|ùúÉ)\n[Ô∏Åùëá‚àí1\n‚àëÔ∏Å\nùëñ=1\n‚àáùúÉlog ùúãùúÉ(ùëéùëñ|ùë†ùëñ)ùëÑùúãùúÉ(ùë†ùëñ, ùëéùëñ)\n]Ô∏Å\n(3.3.13)\nBecause there are many well studied methods to approximate value functions, for instance\nthose described in section 3.2, it seems natural to use a second function estimator to also\napproximate a suitable value function to be used as training target for the policy gradient\nmethod. One advantage of this dual training approach in comparision to purely value based\nmethods is, that the policy is still parametrized independently and thus can be used to\noutput continuous actions. Other advantages of policy gradient based methods over value\nbased methods, like making small updates to the parameters of the policy, also still apply\nwhen using estimates of a value function as training target.\nMethods that learn a value function, which is used as training target for an independently\nparametrized policy are called actor-critic architectures (Sutton and Barto 1998). The\nactor is the function estimator for the policy discussed so far. The critic is a second\nfunction estimator, that estimates a value function. Both parts can be modeled by neural\nnetworks. The interactions of the two function estimators are depicted in figure 3.5. A\nstraightforward way to derive an actor-critic like training method from the REINFORCE\nalgorithm discussed in section 3.3.2 is to use the state-value function as baseline, which\nobviously requires a seperate estimate of a value function. Degris, Pilarski, and Sutton\n(2012) compare different actor-critic methods and show that they can be applied to solve\na robotic task.\nBy approximizing the gradient of the true expected return by the gradient of a value\nfunction, a bias might be introduced. Sutton, McAllester, et al. (2000) show that under\ncertain conditions, the gradient is still exact. This formulation is known as the policy\ngradient theorem with function approximation.\n3. Variants of Reinforcement Learning\n26\nstates\nactor/policy\ncritic\naction\nQ-value\nFigure 3.5: Simple Actor-Critic architecture, where the critic estimates the action-value\nfunction.\nIn order to reduce the variance of the critic, it is very sensible not to approximize\nthe Q-function, but another meaningful function, that can be used as target for policy\noptimization. For example, subtracting the state-value function ùëâùúãùúÉ(ùë†ùë°) from ùëÑùúãùúÉ(ùë†ùë°, ùëéùë°)\nleaves the policy gradient theorem intact, as ùëâùúãùúÉ(ùë†ùë°) is independent from the selected action\nand thus does not alter the gradient with respect to the parameters of the policy. The term\nùëÑùúãùúÉ(ùë†ùë°, ùëéùë°) ‚àíùëâùúãùúÉ(ùë†ùë°), which is known as the advantage function ùê¥ùúãùúÉ(ùë†ùë°, ùëéùë°) from section\n3.2.7, has very reduced variance however, as it only models the impact of the currently\nselected action on the expected return.\n3.3.4\nDeep Deterministic Policy Gradient (DDPG)\nWhile it is possible to derive the gradient of a stochastic policy via the policy gradient\ntheorem (equation 3.3.13), it can be beneficial to use a deterministic policy for calculating\nthe gradient. Silver, Lever, et al. (2014) show that the deterministic policy gradient is\nthe expected gradient of the action-value function. They also prove, that the deterministic\npolicy gradient is a special case of the gradient of many stochastic policies, when variance\napproaches zero. Let ùëù(ùë†|ùúáùúÉ) denote the probability of arriving in state s, when following\nthe policy ùúáùúÉ. Because the policy is now a deterministic function, no trick is needed for\ndifferentiation and the policy gradient can be decomposed as follows:\n‚àáùúÉùêΩ(ùúÉ) = Eùë†‚àºùëù(ùë†|ùúáùúÉ)\n[Ô∏Ä\n‚àáùúÉùëÑùúáùúÉ(ùë†, ùúáùúÉ(ùë†))\n]Ô∏Ä\n(3.3.14)\n= Eùë†‚àºùëù(ùë†|ùúáùúÉ)\n[Ô∏Ä\n‚àáùúÉùúáùúÉ(ùë†)‚àáùúáùúÉ(ùë†)ùëÑùúáùúÉ(ùë†, ùúáùúÉ(ùë†))\n]Ô∏Ä\n(3.3.15)\nAs the expectation is taken only with respect to the states, it can be estimated more\neffectively than in the stochastic case, where the expectation depends on both the states\nand actions (see equation 3.3.13 for comparision). Obviously, the learning algorithm uses the\ngradient of the action-value function with respect to the action to improve the policy. Each\ntraining step modifies the policy in the way, that it‚Äôs outputs are pushed in the direction\n3. Variants of Reinforcement Learning\n27\nof the positive gradient of the action-value function. Especially for continuous actions, this\nstrategy is very effective, as it directly pushes the generated actions towards the assumed\nbest action with respect to the action-value estimations. For a stochastic policy the same\nprocedure would require a more exhaustive search in the action space to find the assumed\nbest action.\nLillicrap et al. (2015) apply these insights to problems with complex continuous action\nspaces and successfully combine the deterministic policy gradient with a deep Q-network\n(DQN) to obtain the deep deterministic policy gradient (DDPG) algorithm, that is shown in\nlisting 3.4. To encourage exploration, a stochastic policy is still used to generate the training\nsamples, which yields an off-policy training algorithm. Like DQN, the DDPG algorithm uses\ntarget networks for both the actor and the critic and experience replay. In contrast to DQN,\nthe target networks are updated after each gradient step to slowly replicate the changes\nmade to the trained networks.\nListing 3.4: DDPG algorithm. Reproduced from Lillicrap et al. (2015).\n1 Randomly initialize critic network ùëÑ(ùë†, ùëé|ùúÉùëÑ) and actor ùúá(ùë†|ùúÉùúá) with weights ùúÉùëÑand ùúÉùúá\n2 Initialize target network ùëÑ‚Ä≤ and ùúá‚Ä≤ with weights ùúÉùëÑ‚Ä≤\n‚ÜêùúÉùëÑ, ùúÉùúá‚Ä≤\n‚ÜêùúÉùúá\n3 Initialize replay buffer ùëÖ\n4 for episode = 1, M do\n5\nInitialize a random process ùí©for action exploration\n6\nReceive initial observation state ùë†1\n7\nfor t = 1, T do\n8\nSelect action ùëéùë°= ùúá(ùë†ùë°|ùúÉùúá) + ùí©ùë°according to the current policy and exploration\nnoise\n9\nExecute action ùëéùë°and observe reward ùëüùë°and observe new state ùë†ùë°+1\n10\nStore transition (ùë†ùë°, ùëéùë°, ùëüùë°, ùë†ùë°+1) in ùëÖ\n11\nSample a random minibatch of ùëÅtransitions (ùë†ùëñ, ùëéùëñ, ùëüùëñ, ùë†ùëñ+1) from ùëÖ\n12\nSet ùë¶ùëñ= ùëüùëñ+ ùõæùëÑ‚Ä≤(ùë†ùëñ+1, ùúá‚Ä≤(ùë†ùëñ+1|ùúÉùúá‚Ä≤\n)|ùúÉùëÑ‚Ä≤\n)\n13\nUpdate critic by minimizing the loss: ùêø=\n1\nùëÅ\n‚àëÔ∏Ä\nùëñ(ùë¶ùëñ‚àíùëÑ(ùë†ùëñ, ùëéùëñ|ùúÉùëÑ))2\n14\nUpdate the actor policy using the sampled policy gradient:\n15\n16\n‚àáùúÉùúáùêΩ‚âà1\nùëÅ\n‚àëÔ∏Å\nùëñ\n‚àáùëéùëÑ(ùë†, ùëé|ùúÉùëÑ)|ùë†=ùë†ùëñ,ùëé=ùúá(ùë†ùëñ)‚àáùúÉùúáùúá(ùë†|ùúÉùúá)|ùë†=ùë†ùëñ\n17\n18\nUpdate the target networks:\n19\nùúÉùëÑ‚Ä≤\n‚ÜêùúèùúÉùëÑ+ (1 ‚àíùúè)ùúÉùëÑ‚Ä≤\n20\nùúÉùúá‚Ä≤\n‚ÜêùúèùúÉùúá+ (1 ‚àíùúè)ùúÉùúá‚Ä≤\n21\nend for\n22 end for\n3. Variants of Reinforcement Learning\n28\n3.3.5\nAsynchronous Advantage Actor-Critic (A3C)\nThe asynchronous advantage actor-critic algorithm (A3C), introduced by Mnih, Badia, et\nal. (2016) among other algorithms, is a popular recent implementation of an actor-critic\nmodel, that improves state-of-the-art performance on many experiments. The authors show\nfor instance, that their algorithm is able to perform better than the deep Q-network, that is\ndiscussed in section 3.2.6, on the task of playing many different Atari games. Levine, Pastor,\net al. (2016) and Gu, Holly, et al. (2017) show successful applications of asynchronous\nupdates similar to those of A3C in robotics, where the algorithm is able to generalize to\ndifferent robotic hardware. Another important benefit of the A3C algorithm is, that it can\nbe efficiently implemented and is therefore faster than many other methods, that achieve\ncomparable performance.\nThe key idea that motivated asynchronous algorithms is that learning can be parallelized\nusing different threads, that independently collect experience. The independent execution\nof multiple different environments reduces the variance of the trained estimators, because\nit provides the learning algorithm with many decorrelated training examples at one time.\nTechniques like experience replay used for training the DQN are thus no longer necessary.\nBy choosing different starting conditions and exploration rates for the threads, it can be\nensured that the training examples produced at one time are sufficiently varying. Figure\n3.6 depicts the main components of the A3C algorithm.\nAsynchronous advantage actor-critic and other similar algorithms like asynchronous n-step\nQ-learning (Mnih, Badia, et al. 2016) use a mix of explicitly computed n-step returns.\nUnlike algorithms that rely on eligibility traces, which were discussed in section 3.2.3, A3C\ndirectly executes a sequence of steps with fixed length n. After that, the one-step return is\nused to obtain the gradient update for the last pair of state and action, the two-step return\nfor the second last and so on.\nEach asynchronous thread independently computes updates for the parameters of both\nnetworks using the gradient of equation 3.3.13 to determine the update of the actor and the\nrespective n-step update for updating the critic. Although there is a chance of overriding\nchanges made by other threads, the gradient updates can be synchronized without any locks,\nif the learning rate is sufficiently small. This updating mechanism is known as Hogwild!\nstyle updating (Recht et al. 2011). Each thread maintains a local copy of the two global\nnetworks to being able to compute the updates independently from all other threads. After\neach update, the locally copied networks are updated. In comparision to other state-of-the-\nart methods, A3C is very fast, because of the possibility to massively parallelize it with\nminimal overhead for synchronization. Listing 3.5 pictures the algorithm in pseudocode.\n3. Variants of Reinforcement Learning\n29\nLocal actor & critic\nGlobal actor & critic network\nsync\nEnvironment 2\nComputing thread 2\ntrain\nsimulate\nLocal actor & critic\nsync\nEnvironment 1\nComputing thread 1\ntrain\nsimulate\nTarget actor & critic network\nupdate\nFigure 3.6: Asynchronous advantage actor-critic (A3C) still uses target netorks for stability,\nbut no experience replay. Many threads, each with a separate instance of the environment,\ntrain local instances of the actor and critic network in parallel, while only two threads are\nexemplary displayed. The local updates are synchronized with the global networks at regular\nintervals.\nListing 3.5: A3C algorithm for each learner thread. The threads repeatedly synchronize\ntheir respective weight updates. Reproduced from Mnih, Badia, et al. (2016).\n1 // Assume global shared parameter vectors ùúÉand ùúÉùë£and global shared counter ùëá= 0\n2 // Assume thread-specific parameter vectors ùúÉ‚Ä≤ and ùúÉ‚Ä≤\nùë£\n3 Initialize thread step counter ùë°‚Üê1\n4 repeat\n5\nReset gradients: ùëëùúÉ‚Üê0 and ùëëùúÉùë£‚Üê0\n6\nSynchronize thread-specific parameters ùúÉ‚Ä≤ = ùúÉand ùúÉ‚Ä≤\nùë£= ùúÉùë£\n7\nùë°ùë†ùë°ùëéùëüùë°= ùë°\n8\nGet state ùë†ùë°\n9\nrepeat\n10\nPerform ùëéùë°according to policy ùúã(ùëéùë°|ùë†ùë°; ùúÉ‚Ä≤)\n11\nReceive reward ùëüùë°and new state ùë†ùë°+1\n12\nùë°‚Üêùë°+ 1\n13\nùëá‚Üêùëá+ 1\n14\nuntil terminal ùë†ùë°or ùë°‚àíùë°ùë†ùë°ùëéùëüùë°== ùë°ùëöùëéùë•\n15\nùëÖ=\n{Ô∏É\n0\nfor terminal ùë†ùë°\nùëâ(ùë†ùë°, ùúÉ‚Ä≤\nùë£)\nfor non-terminal ùë†ùë°// Bootstrap from last state\n16\nfor ùëñ‚àà{ùë°‚àí1, ..., ùë°ùë†ùë°ùëéùëüùë°} do\n17\nùëÖ‚Üêùëüùëñ+ ùõæùëÖ\n18\nAccumulate gradients wrt ùúÉ‚Ä≤ : ùëëùúÉ‚ÜêùëëùúÉ+ ‚àáùúÉ‚Ä≤ log ùúã(ùëéùëñ|ùë†ùëñ; ùúÉ‚Ä≤)(ùëÖ‚àíùëâ(ùë†ùëñ; ùúÉ‚Ä≤\nùë£))\n19\nAccumulate gradients wrt ùúÉ‚Ä≤\nùë£: ùëëùúÉùë£‚ÜêùëëùúÉùë£+ ùúï(ùëÖ‚àíùëâ(ùë†ùëñ; ùúÉ‚Ä≤\nùë£))/ùúïùúÉ‚Ä≤\nùë£\n20\nend for\n21\nPerform asynchronous update of ùúÉusing ùëëùúÉand of ùúÉùë£using ùëëùúÉùë£\n22 until ùëá> ùëáùëöùëéùë•\nChapter 4\nExtensions to Reinforcement Learning\nRecent research often focuses on improving model-free reinforcement learning algorithms to\nbeing able to solve a wide variety of different challenging tasks with the same neural network\nstructure and minimal or no changes to the used learning algorithms or hyperparameters\n(Mnih, Badia, et al. 2016). These methods, some of which were discussed at the end of\nsection 2.3, have in common that they directly train a policy to solve the given task with one\nlearning algorithm. The technique of learning a function approximator that directly predicts\nthe desired output from the given input data without intermediate stages can be named\nend-to-end learning (Levine, Finn, et al. 2016), mostly from pixels to actions. The execution\nof the policy during test time is straightforward and minimal or no preprocessing of the\ninput data is required. While the obtained results are impressive, it is still reasonable to\nassume that incorporating information about the model of the environment or preprocessing\nthe input data can improve the learned behavior.\nAs a practical example, Levine, Pastor, et al. (2016) use a training objective, which is\nsimilar to that of reinforcement learning, to predict the probability of successfully grasping\nan object, when the position of a robotic gripper is known. For testing the learned behavior,\na sampling algorithm is used, that samples movements of the robot and evaluates them\naccording to the predicted probability of a successful grasp. This approach incorporates\nknowledge of the environment into the training and testing process and thereby improves\nthe performance of the robot after training and is able to generalize to different robotic\nhardware.\nIn the following, three possible ways to extend reinforcement learning algorithms will be\ndiscussed. These ideas require at least some knowledge about the environment or process\nthe given information in several stages to produce the desired output and thus cannot\nbe described as end-to-end learning algorithms. Nevertheless, we assume that some of\n30\n4. Extensions to Reinforcement Learning\n31\nthe gained insights can be transfered to other similar tasks or environments with minor\nadaptions.\n4.1\nPretraining a State Model Using the Physical States\nSometimes additional information can be observed during training for tasks that should be\ncarried out while only observing pixel data in the test case. A robotic system for instance\nmight have access to the physical states of the important components like positions of\nobjects or parts of the robot during training. Reinforcement learning only on the physical\nstates is usually much more effective than learning directly from pixels, because the possibly\ncomplicated task of extracting the necessary information from the high dimensional pixel\ndata is no longer required.\nIn addition, it is also possible to learn a policy that is able to act on pixels while\nincorporating knowledge about the physical states to enhance the training performance.\nLevine, Finn, et al. (2016) use a dual training approach to train a neural network to directly\npredict robotic motor torques using pixel images as input and thus are able to form an end-\nto-end learning algorithm. The training procedure however does include the physical states\nof the robot and other objects to force the network to learn useful features.\nWe suppose that introducing a model, that is trained to predict the physical states from a\npixel image using a supervised learning algorithm (Figure 4.1), shortens training time and\nsimplifies the reinforcement learning task. We call this model an internal model. The term\ninternal model sometimes refers to models learning the dynamics of the system (Kawato\n1999), while we label those inverse- or forward-models. The internal model can be trained\nin parallel to a second model, that uses reinforcement learning to predict actions based\non the physical states. The resulting hybrid of the two parallel trained models cannot be\ndescribed as an end-to-end model anymore, but it is still able to fulfill the same task like\na model trained end-to-end from pixels to actions during test time as it predicts actions\nfrom pixels with an intermediate step in between but without any additional information\nrequired.\n4.2\nPretraining a State Model with a Deep Autoencoder\nWithout the knowledge of the physical states, it is more difficult to pretrain a model, that\ntransforms the input pixels into an intermediate representation to be used as input for\na reinforcement learning algorithm. Although no supervised learning target is available,\nunsupervised learning techniques can be applied to extract information inherent in the\nprovided data.\n4. Extensions to Reinforcement Learning\n32\npixel image\ninternal model\nphysical states\nFigure 4.1: The internal model predicts the physical states from a pixel image and thus\nenables any function estimator trained with reinforcement learning on the physical states to\nwork on pixels.\nA convenient structure for learning to extract useful information from high dimensional data\nin an unsupervised manner is an autoencoder (Figure 4.2), which is trained to reproduce\nits input through an internal representation or latent code (Goodfellow, Bengio, and\nCourville 2016). In our case, the internal representation serves as input to the reinforcement\nlearning process to form a hybrid training approach similar to the combination of internal\nmodel and reinforcement learning in section 4.1. The part of the autoencoder, that generates\nthe latent code from the input is known as the encoder, while the other part, that\ngenerates the reconstruction from the hidden code is named the decoder. Both parts can\nbe complicated function estimators, usually neural networks with multiple layers including\nconvolutional layers and transposed convolutional layers.\nTo make the autoencoder learn something useful, it must be discouraged from simply\ncopying the input to the output, which would make it useless. Building the latent code\nin fact must be tied to learning the important variations of the data. The autoencoder\nthus might not be able to recover the input exactly, but will focus on its main aspects.\nAutoencoders can be viewed as a feed forward network from the input to the reconstruction.\nTherefore, they can be trained by simple gradient descent similar to a supervised training\nprocedure, where the input and the target are equal. The loss function to be optimized,\nthat measures the difference of the input to the reconstruction in any way, is called the\nreconstruction error.\nGoodfellow, Bengio, and Courville (2016) summarize several ways to train autoencoders.\nAn autoencoder that is forced to learn useful information inherent in the data by\nmaking the dimension of its latent code smaller than the dimension of the input is\ncalled an undercomplete autoencoder. However, various regularization strategies enable\nautoencoders to still learn useful features of the data, when they are not necessarily\nundercomplete. This can be for example a sparsity contraint (Lee et al. 2007) imposed\non the latent code to form a sparse autoencoder. Denoising autoencoders add noise\nto the input and thus force the autoencoder to remove it and learn to distinguish realistic\ndata from noise (Vincent et al. 2008). Another strategy is to penalize the derivatives of the\n4. Extensions to Reinforcement Learning\n33\npixel image\nlatent representation\nreconstructed image\nFigure 4.2: Any autoencoder maps an input vector to a reconstruction through a latent\ninternal representation. In the special case of using pixel images, the autoencoder is trained\nto reconstruct a given input image as precisely as possible.\nlatent code with respect to the input to form a contractive autoencoder, which learns\nlocally stable features (Rifai et al. 2011).\nRecent extensions to the general idea of learning from the task of reconstruction include\ndeep autoencoders, that use deep neural networks to build the encoder and decoder\nfunctions. Another recent innovation in this field are autoencoders, that generalize the\nencoder and decoder functions to stochastic mappings. As popular examples of these\nprobabilistic models, the adversarial autoencoder (Makhzani et al. 2015) and the variational\nautoencoder (Kingma and Welling 2013) were successfully applied to a range of tasks\nincluding denoising, compression or semi-supervised classification, that is able to work on\npartially labled training data. Let ùëù(ùë•) denote the data distribution, ùúÉthe parameters of\nthe encoder and ùúëthe parameters of the decoder. The reconstruction error for both the\nencoder ùëùùëíùëõùëêùëúùëëùëíùëü(ùëß|ùë•) = ùëûùúÉ(ùëß|ùë•) and the decoder ùëùùëëùëíùëêùëúùëëùëíùëü(ùë•|ùëß) = ùëùùúë(ùë•|ùëß) being stochastic\ncan be expressed as the negative log-likelihood of reconstructing x, when x is the input:\n‚Ñí(ùë•, ùúÉ, ùúë)ùëüùëíùëê= Eùëß‚àºùëûùúÉ(ùëß|ùë•)\n[Ô∏Ä\n‚àílog(ùëùùúë(ùë•|ùëß))\n]Ô∏Ä\n(4.2.1)\nThe variational autoencoder (Kingma and Welling 2013) imposes a prior probability\ndistribution on the latent code to regularize it and present the learned features in an\nappealing form that can be used to solve tasks like classification. The imposed prior usually\nis a multivariate gaussian disribution. The mean and variance of each variable in the\nlatent code are modeled seperately and forced to match the prior distribution by adding a\nsecond term to the loss function beside the reconstruction error. The newly introduced term\n4. Extensions to Reinforcement Learning\n34\nmeasures the difference between the actual probability distribution of the latent variables\nand the imposed prior ùëù(ùëß) using the Kullback‚ÄìLeibler divergence:\n‚Ñí(ùë•, ùúÉ, ùúë) = ‚Ñí(ùë•, ùúÉ, ùúë)ùëüùëíùëê+ ùê∑ùêæùêø[ùëûùúÉ(ùëß|ùë•)||ùëù(ùëß)]\n(4.2.2)\nIn addition to a training a deep autoencoder only on pixel data, in the case when\nphysical states are available, they might be incorporated into the training procedure of\nthe autoencoder to help extracting sensible features. The structure of the autoencoder can\nbe extended to not only predict the reconstructed image from the latent variables, but also\nthe physical states (Figure 4.3). The training objective is to minimize the sum of the two\nlosses for the reconstructed image and the predicted physical states respectively. To enhance\nthe regularization effect, the latent variables of the autoencoder can be forced to become\na linear function of the physical states, by drawing only simple linear connections between\nthe two layers. When the dimension of the latent code is chosen to be larger than the\ndimension of the physical states, this learning technique might lead to a better estimation\nof the true physical states, because reducing the high dimensional pixel image to very few\nphysical states can be error-prone and inexact.\npixel image\nlatent representation\nreconstructed image\nphysical states\nFigure 4.3: A network structure similar to that of an autoencoder can be trained jointly\nto predict the reconstructed image and the physical states. The learned latent representation\nideally encodes the physical states with less error than the purely supervised model of section\n4.1.\n4. Extensions to Reinforcement Learning\n35\n4.3\nTraining Inverse- or Forward-Models\nReinforcement learning algorithms often do not require to directly learn a model of the\nsystem dynamics. Nevertheless they must maintain an implicit understanding of those\ndynamics to being able to act. Especially for end-to-end training approaches it is not easy\nto tell what kind of knowledge the trained model has aquired about its environment. Most\nof the time, only the actions taken and thus the behavior can be evaluated. Other recent\ninnovations that are only loosely coupled to reinforcement learning but solve very similar\nproblems directly attempt to learn the dynamics of the environment.\nThere are two main possibilities to learn the system dynamics ùëù(ùë†ùë°+1|ùë†ùë°, ùëéùë°). While looking\ninto the future, forward models (Figure 4.4) can be trained to predict the next state ùë†ùë°+1.\nOften it is also helpful to estimate the action that was executed between two states. Models\nof this kind are called inverse models (Figure 4.5). Kawato (1999) introduce both types\nof models and discuss possible applications to motor control problems.\nBoth approaches usually rely on neural networks as function approximators, when they are\nused in settings similar to the reinforcement learning tasks described so far. Dosovitskiy\nand Koltun (2016) train a forward model to being able to compare a set of discrete actions\nby evaluating their respective impact in the context of the 3D game Doom. Agrawal et al.\n(2016) jointly train a forward and an inverse model in a robotic environment to intuitively\nlearn about the dynamics of different physical objects by displacing or rotating them.\nstate s1\nneural network\nstate s2\naction\nFigure 4.4: Abstract architecture of a simple forward model.\n4. Extensions to Reinforcement Learning\n36\nstate s1\nneural network\naction\nstate s2\nFigure 4.5: Abstract architecture of a simple inverse model.\nChapter 5\nMethods\nAn important property of many recent inventions in the field of reinforcement learning is,\nthat the underlying ideas can be transferred to many different environments with minimal\nchanges. In contrast to this, to evaluate the different reinforcement learning methods\ndiscussed so far and extensions to them, we focus on a single simulated robotic task for two\nreasons. First, we assume that the relative scores obtained by training a single robotic task\nwith different algorithms are still meaningful and can be used to compare the algorithms,\nwhile training on a larger set of tasks would require a more sophisticated design of the\nfunction estimators beforehand and thus would slow down the experiments. Second, solving\na single task allows to incorporate information about the model of the environment into\nthe training algorithm, as discussed in section 4. An important point to investigate is the\ninfluence of this additional knowledge on the training performance. Recently published\nideas, that analyse the benefits of directly modeling the underlying system dynamics, for\ninstance the algorithm proposed by Dosovitskiy and Koltun (2016), are often tested on a\nvery small set of similar environments too.\nIn the following, we will first describe the single simulated robotic task used for all\nexperiments and its variations. Then follows a discussion of the software components\nused for conducting the experiments. Finally we summarize the different algorithms that\nwere used. Two new algorithms, that we call distributed and asynchronous DDPG will be\npresented. With these training approaches, we aim to combine asynchronous methods and\nDDPG to form asynchronous learning methods, which are able to effectively learn to predict\ncontinuous actions.\n37\n5. Methods\n38\n5.1\nTraining Environments\nWe trained all experiments on a single robotic task, that is very similar to the OpenAI\nGym reacher task1. OpenAI Gym2 is an open source platform for comparing reinforcement\nlearning algorithms. It is used to obtain benchmark results for algorithms trained on many\ndifferent environments like video games, robotic tasks or board games through a unified\nprogramming interface.\nThe reacher task consists of a simulated arm with two degrees of freedom, where the gripper\nof the robotic arm, which is the endpoint of the last arm segment, should be guided to reach\na target. While it would be possible to transfer the reacher problem to a 3D space with\nmore degrees of freedom, we restrict to the 2D space for all experiments and only use two\narm segments.\nThe original reacher task provides the learning algorithm with the physical states, which\nresemble the angles of the arm segments and the position of the target, during training and\ntest time. In contrast to this, we try to learn a policy, that is able to predict actions from\npixel data. We therefore provide most experimentally learned policies only with screenshots\nof the simulated reacher task during test time. This task is much harder, as the state space\nis very much larger than before and the tested algorithm needs to find a way to extract\nuseful information from pixels. Ideally, we would like to be able to train this kind of policy\nonly on pixel data and thus leave out the physical states completely. Nevertheless, many\nexperiments still include the information about the physical states as guide for the algorithm\nto help it extracting useful information from the pixels.\n5.1.1\nSimulation with Matplotlib\nAs the OpenAI Gym reacher environment relies on the commercial mujoco physics engine,\nwe decided to replicate the simulation in essence using matplotlib3. Figure 5.1 shows five\nimages, that were randomly generated using the matplotlib simulation. To make the vision\ntask harder, which means making it harder to extract useful information from pixels, noise\ncan be added in the background. The noise is however static and thus it looks always the\nsame on every pixel image.\nThe rendered images have 64 pixels in both dimensions. The environment outputs the\nphysical states in parallel, which consist of the two angles of the arm segments and the\ncoordinates of the goal. Figure 5.2 shows a screenshot of the environment with the physical\nstates highlighted. Executed actions are tuples with two components specifying the desired\n1https://gym.openai.com/envs/Reacher-v1, last downloaded 2017-09-14\n2https://gym.openai.com/, last downloaded 2017-09-14\n3https://matplotlib.org/, last downloaded 2017-09-14\n5. Methods\n39\nFigure 5.1: Five visualizations of the matplotlib reacher at different random states. The\nfirst two images are rendered without background noise, while the other images include static\nnoise in the background.\nrotation of both arm segments. Each arm can rotate maximally by 2 degrees in each\ndirection. The components of the actions are clipped to lie in the interval between -1 and 1:\nùëé‚àà[‚àí1, 1]2. Action (-1,-1) for instance means to rotate both segments left by two degrees\nand action (1,-1) means rotating only the first segment right by two degrees and the second\nsegment left by two degrees.\nThe reward returned by the environment for each step is designed to guide the gripper\nto the target as fast as possible. We therefore introduced a distance related part of\nthe reward function, that depends on the distance between the gripper and the target:\nùëüùëëùëñùë†ùë°= ùëí‚àí|ùë•ùëîùëüùëñùëùùëùùëíùëü‚àíùë•ùë°ùëéùëüùëîùëíùë°|. Early experiments showed that this kind of reward leads to\na strategy, where the gripper circles around the target without ever reaching it, while\ncollecting a large amount of reward, because the distance is very small. To overcome this\nproblem, we multiplied the distance based reward with a control term, that only depends\non the currently executed action: ùëüùëêùë°ùëüùëô= |ùë•ùëîùëüùëñùëùùëùùëíùëü‚àíùëúùëôùëë‚àíùë•ùë°ùëéùëüùëîùëíùë°| ‚àí|ùë•ùëîùëüùëñùëùùëùùëíùëü‚àíùë•ùë°ùëéùëüùëîùëíùë°|, which\nforces the action to be taken in the direction of the target. As the total reward ùëü= ùëüùëëùëñùë†ùë°¬∑ùëüùëêùë°ùëüùëô\nis the product of both terms, actions that are taken in the wrong direction, when being\nclose to the target, will be severely punished. The total reward is normalized to lie in the\ninterval [-1,1].\nWhen ùëüùëëùëñùë†ùë°falls below 0.1 the episode is considered successfully terminated. This distance is\nsufficiently small for reaching the target, as both axes of the simulated environment range\nfrom -1 to 1. The episode fails, when 1000 steps are executed without reaching the target.\n5.1.2\nMore Realistic Simulation with Dart\nThe same task of reaching a target with a robotic arm, that has two degrees of freedom,\nhas been replicated as an extension to OpenAI Gym using the open source physics engine\nDart4. We also experimented with this visualization to obtain more realistic pixel images.\nThe simulation with Dart has however not been designed to allow training from pixel data.\n4https://github.com/DartEnv/dart-env, last downloaded 2017-09-14\n5. Methods\n40\nFigure 5.2: Visualization of the reacher environment with highlighted and labeled physical\nstates. The simulation relies on matplotlib. The arm segments are simple lines with dots in\nbetween. The target has the shape of a star. Noise in the background can be added in form\nof white dots.\nFigure 5.3: Screenshot of the original visualization of the Dart environment, which simulates\nthe reacher task in 3D, where the arm is only allowed to move on a two-dimensional plane.\nThe target point is already enlarged here.\nHence, we had to apply preprocessing to the pixel images, which means that we enlarged\nthe target point to make it better recognizable by the training algorithm. Then we cropped\nthe important region of the image, where the arm and the target are shown and resized the\ncropped images to 64x64 pixels. The physical states were adjusted to be similar to those of\nthe matplotlib simulation for better comparision. Instead of the 11 physical states originally\nreturned by the Dart environment, we used these to calculate four variables representing\nthe angles and the target position, that are finally passed to the learning algorithm.\n5. Methods\n41\nFigure 5.4: Five visualizations of the Dart reacher at different random states. The images\nhave been cropped to the important region and resized to 64x64 pixels.\n5.2\nImplementation Details\n5.2.1\nUsed Software and Hardware\nAll implementations purely consist of python code, while numpy is used for general\ndata processing. We used tensorflow5 for neural network training and keras6 to build\nthe architecture of the respective networks. Keras can be seen as an easy interface to\ntensorflow, while it also supports other backends. For drawing plots and statistics we\nrelied on matplotlib7 and tensorboard, which is a part of the tensorflow framework and\nprovides easy to use visualization tools for tensorflow training and neural network training\narchitectures (graphs).\nWith tensorflow it is relatively easy to set up an asynchronous training algorithm,\nas the framework itself heavily supports multithreading and is even able to run on a\nGPU without any changes to the code of the algorithm. When executed without any\nrestrictions, Tensorflow occupies all computing resources it can find, which means it\ncreates a computing thread for every CPU and also reserves the whole memory of all\navailable GPUs. For CPU training, tensorflow distinguishes between intra-op-parallelism\nand inter-op-parallelism. For a single computing operation, the threads of the intra-op-\nparallelism pool are used to execute this operation in parallel, while the threads in\nthe inter-op-parallelism pool execute multiple operations at one time. For asynchronous\ntraining, especially the inter-op-parallelism is important, as multiple threads independently\ncompute gradients, which corresponds to executing multiple gradient operations in parallel.\nThe intra_op_parallelism_threads variable and the inter_op_parallelism_threads\nvariable control the size of the respective thread pools. Listing 5.1 shows a sample\nconfiguration of both variables.\n5https://www.tensorflow.org/, last downloaded 2017-09-14\n6https://keras.io/, last downloaded 2017-09-14\n7https://matplotlib.org/, last downloaded 2017-09-14\n5. Methods\n42\nListing 5.1: An example of how to specify the size of the tensorflow thread pools at the\nbeginning of the python program. Both sizes are exemplary set to 4. The newly created\ntensorflow session should be set as default session for the backend of the keras library.\n1 import tensorflow as tf\n2 from keras import backend as K\n3\n4 def main(_):\n5\nintra_threads = 4\n6\ninter_threads = 4\n7\nsess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=intra_threads,\ninter_op_parallelism_threads=inter_threads))\n8\nwith sess.as_default():\n9\nK.set_session(sess)\n10\n# do calculations ...\nTo restrict tensorflow to use only one GPU of a cluster, the CUDA_VISIBLE_DEVICES switch\nis necessary to specify the indices of the visible GPUs. For most cases it makes sense to\ntrain on a single GPU, while multiple indices could be set by using a colon as delimiter. The\nswitch can be set for each run separately, e.g. CUDA_VISIBLE_DEVICES=0 python main.py\nfor a file named main.py. Listing 5.2 depicts how to rewrite the main function to support\npassing the GPU index as command line argument for easier use. The equivalent to directly\nusing the switch would then be the shorter form: main.py - -gpu=0.\nListing 5.2: Initializing the CUDA_VISIBLE_DEVICES switch in code from a command line\nparameter is a practical solution to prevent tensorflow from unnecessarily blocking the\nmemory of all GPUs. If an invalid index (e.g. -1) is specified, tensorflow runs only on the\nCPUs.\n1 import tensorflow as tf\n2\n3 flags = tf.app.flags\n4 flags.DEFINE_integer('gpu', 0, 'index of GPU to use')\n5 FLAGS = flags.FLAGS\n6\n7 def main(_):\n8\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = str(FLAGS.gpu)\nIt is also possible to assign a specific device to a subset of tensorflow operations. The block,\nwhere the operations are defined has to start with a wrapping call to tf.device(device).\nListing 5.3 shows this method for a simple example. We were however unable to achieve any\nperformance improvements when directly placing operations on devices and thus decided\nto let tensorflow automatically assign the operations to the available hardware for all\nexperiments.\n5. Methods\n43\nListing 5.3: Tensorflow operations can be directly assigned to hardware units, for example\nthe CPU with index 1.\n1 import tensorflow as tf\n2\n3 with tf.device('/cpu:1'):\n4\n# all operations defined here will run on CPU 1\nFor training on CPUs we used a computing server with 4x8 Intel Xeon E5-4650 CPUs\nand 256 GB RAM. The GPU based training was performed on a second machine with\na shared-memory system, 16 CPU cores, 8 Tesla K20m GPUs and 126 GB RAM. The\nreinforcement learning algorithms were run only on the CPUs without exception. As they\nrequire frequent interactions with the environment, which requires communication with the\nCPU, there were no speed gains observed when using a GPU. Especially asynchronous\nimplementations can be efficiently run on a parallel CPU system, because execution can\nbe carried out by multiple parallel threads, that only require minimal communication. All\ndifferent forms of pretraining, which do not involve reinforcement learning, were entirely\nperformed on the GPUs and heavily benefited from the speedup. Especially implementations\nthat use large amounts of training data at once can be efficiently parallelized on a GPU.\n5.2.2\nAsynchronously Executing Multiple Environments\nFor asynchronous training, we identified the execution of the environment being a major\nbottleneck, when using a single threaded instance of the environment for asynchronous\ntraining algorithms. Tensorflow takes care of parallelizing the network training operations,\nbut does not parallelize the executions of the environment, which run synchronously\ndue to pythons global interpreter lock8. We thus decided to use the multiprocessing\nmodule to create separate processes for all instances of the environment. Listing 5.4\nshows the snippet where a new simulation process is initialized. Figure 5.5 shows how\nthe AsyncEnvironment class is used, that provides an interface for creating and managing\nasynchronous environments. For easy usage, this class exposes the same function like a\nlocal environment, but sends the commands across process boundaries instead of directly\nexecuting them.\nIt is important to notice, that the creation mode of the new process must be set to spawn.\nThe standard setting is fork on Unix/Linux platforms, which copies the entire memory of\nthe computing process to all simulating processes and leads to explosion of memory. This\nhappens especially, when the parameters of the neural networks, which are stored for the\ncomputing process, already consume large amounts of memory.\n8https://wiki.python.org/moin/GlobalInterpreterLock, last downloaded 2017-09-14\n5. Methods\n44\ntensorflow parallelization\nenv. parallelization\nComputing thread 1\nAsyncEnvironment 1\nComputing thread 2\nAsyncEnvironment 2\nEnvironment 1\nreset / step\nEnvironment 2\nreset / step\npixels / ph. states\npixels / ph. states\nFigure 5.5: The execution of different instances of the environment cannot be automatically\nparallelized by tensorflow. We therefore create a separate process for each instance of the\nenvironment and attach it to the computing thread.\nAnother problem was that we were unable to simulate the Dart environment discussed in\nsection 5.1.2 on a computing server, because the simulation is carried out with OpenGL and\nneeds an active display to function properly. To separate the simulation from the training\nalgorithm, we decided to set up a TCP server for simulation.\nEspecially for running asynchronous algorithms with multiple independent instances of\nthe environment, it is crucial for the server to be able to effectively manage multiple client\nconnections in parallel. Each client requests a unique id, which is bound to a single instance\nof the environment on the server side. The client then sends the actions to be executed and\na reset command at the start of each episode to the server, while receiving the current pixel\nimage and the physical states in return.\nThe architecture of the simulation server is thus very similar to the asynchronously simu-\nlated environments depicted in figure 5.5. The only difference is, that the communication\nwith the environments is now TCP-based and carried out over the network.\n5. Methods\n45\nListing 5.4: The class AsynchronousEnvironment encapsulates the creation of separate\nprocesses with multiprocessing. The _ _init_ _ method starts a new process without\ncopying the memory of the current process to it. The processes use pipes to communicate.\n1 import multiprocessing\n2\n3 class AsynchronousEnvironment:\n4\ndef __init__(self, env_name):\n5\nctx = multiprocessing.get_context('spawn')\n6\nsim_pipe, self.pipe = ctx.Pipe()\n7\nself.proc = ctx.Process(\n8\nname=\"data_generator\",\n9\ntarget=func_proc,\n10\nargs=(sim_pipe,env_name))\n11\nself.proc.start()\n5.3\nTraining Algorithms\nAs the robotic task we were aiming to solve uses continuous actions, we mainly focused\non the DDPG algorithm (Lillicrap et al. 2015), that was introduced in section 3.3.4, and\ndeterministic policies. We use the DDPG implementation from keras-rl9 as a basline for\nour own experiments, but also provide an own implementation of two extended DDPG\nalgorithms, that combine the ideas of asynchronous training algorithms like A3C (Mnih,\nBadia, et al. 2016; see section 3.3.5) with the deterministic policy gradient. As this\ncombination is a novel approach, one aim of the conducted experiments was to evaluate the\nperformance of the extended DDPG algorithms.\nWe first investigated a variant of DDPG with one-step updates, that directly bootstrap\nfrom the next state (see section 3.2.3). This algorithm does not use Hogwild! style updates\n(Recht et al. 2011) to synchronize the updates of the different threads, but locks the weights\nof the networks during training for other threads. The algorithm executes five steps before\nperforming a gradient update and stores the experience collected so far in a very small local\nmemory. We therefore suggest to view this method as implementing a distributed experience\nreplay memory rather than a fully asynchronous algorithm and call it distributed DDPG.\nThis idea can be seen as an intermediate step between using experience replay and fully\nasynchronous updates. We still used target networks and updated them in the same way\nlike proposed for plain DDPG (Lillicrap et al. 2015).\nSubsequent experiments also included a variant of DDPG with lock-free Hogwild! style\nupdates, that we call asynchronous DDPG. We also switched to using the same mix of\nexplicitly computed n-step returns like A3C (see section 3.3.5).\n9https://github.com/matthiasplappert/keras-rl, last downloaded 2017-09-14\n5. Methods\n46\nListing 5.5: Distributed DDPG algorithm for each actor thread with globally shared counter\nT and globally shared parameter vectors ùúÉùúá, ùúÉùëÑ, ùúÉ‚àí\nùúáand ùúÉ‚àí\nùëÑ.\n1 while ùëá< ùëáùëöùëéùë•do\n2\nt = 0\n3\nGet state ùë†ùë°\n4\nrepeat\n5\nExecute action ùëéùë°according to policy ùúá(ùë†ùë°; ùúÉùúá) + ùúñùí©\n6\nReceive reward ùëüùë°+1 and observe new state ùë†ùë°+1\n7\nCompute ùëÖùë°=\n{Ô∏É\nùëüùë°+1 + ùõæùëÑ(ùë†ùë°+1, ùúá(ùë†ùë°+1; ùúÉ‚àí\nùúá); ùúÉ‚àí\nùëÑ) if not terminal\nùëüùë°+1 otherwise.\n8\nStore (ùë†ùë°, ùëéùë°, ùëÖùë°) in buffer\n9\nif ùë°% 5 == 0 or terminal then\n10\nUpdate critic using gradient:\n5\n‚àëÔ∏Å\nùëñ=1\n‚àáùúÉùëÑ(ùëÖùëñ‚àíùëÑ(ùë†ùëñ, ùëéùëñ; ùúÉùëÑ))2\n11\nUpdate actor using gradient:\n5\n‚àëÔ∏Å\nùëñ=1\n‚àáùúá(ùë†ùëñ;ùúÉùúá)ùëÑ(ùë†ùëñ, ùúá(ùë†ùëñ; ùúÉùúá); ùúÉùëÑ)‚àáùúÉùúáùúá(ùë†ùëñ; ùúÉùúá)\n12\nUpdate target critic: ùúÉ‚àí\nùëÑ‚ÜêùúèùúÉùëÑ+ (1 ‚àíùúè)ùúÉ‚àí\nùëÑ\n13\nUpdate target actor:\nùúÉ‚àí\nùúá‚ÜêùúèùúÉùúá+ (1 ‚àíùúè)ùúÉ‚àí\nùúá\n14\nEmpty buffer\n15\nend if\n16\nùë°= ùë°+ 1\n17\nùëá= ùëá+ 1\n18\nuntil ùë°> 1000 or terminal\nListing 5.6: Asynchronous DDPG algorithm for each actor thread with globally shared\nparameter vectors ùúÉùúá, ùúÉùëÑ, ùúÉ‚àí\nùëÑ, ùúÉ‚àí\nùúáand counter T. ùúÉ‚Ä≤\nùúáand ùúÉ‚Ä≤\nùëÑare thread-specific copies.\n1 while ùëá< ùëáùëöùëéùë•do\n2\nt = 0\n3\nGet state ùë†ùë°\n4\nrepeat\n5\nExecute action ùëéùë°according to policy ùúá(ùë†ùë°; ùúÉ‚Ä≤\nùúá) + ùúñùí©\n6\nReceive reward ùëüùë°+1 and observe new state ùë†ùë°+1\n7\nStore (ùë†ùë°, ùëéùë°, ùëüùë°+1) in buffer\n8\nùë°= ùë°+ 1\n9\nùëá= ùëá+ 1\n10\nif ùë°% 5 == 0 or terminal then\n11\nùëÖ=\n{Ô∏É\n0\nfor terminal ùë†ùë°\nùëÑ(ùë†ùë°+1, ùúá(ùë†ùë°+1; ùúÉ‚àí\nùúá); ùúÉ‚àí\nùëÑ)\nfor non-terminal ùë†ùë°\n12\nfor ùëñ‚àà{ùë°‚àí1, ..., ùë°‚àí5} do\n13\nùëÖ‚Üêùëüùëñ+ ùõæùëÖ\n14\nAccumulate gradients wrt ùúÉ‚Ä≤\nùëÑand ùúÉ‚Ä≤\nùúá:\n15\nùëëùúÉùëÑ‚Üê‚àáùúÉ‚Ä≤\nùëÑ(ùëÖ‚àíùëÑ(ùë†ùëñ, ùëéùëñ; ùúÉ‚Ä≤\nùëÑ))2\n16\nùëëùúÉùúá‚Üê‚àáùúá(ùë†ùëñ;ùúÉ‚Ä≤\nùúá)ùëÑ(ùë†ùëñ, ùúá(ùë†ùëñ; ùúÉ‚Ä≤\nùúá); ùúÉ‚Ä≤\nùëÑ)‚àáùúÉ‚Ä≤\nùúáùúá(ùë†ùëñ; ùúÉ‚Ä≤\nùúá)\n17\nend for\n18\nPerform asynchronous update of ùúÉùëÑusing ùëëùúÉùëÑand of ùúÉùúáusing ùëëùúÉùúá\n19\nUpdate target critic: ùúÉ‚àí\nùëÑ‚ÜêùúèùúÉùëÑ+ (1 ‚àíùúè)ùúÉ‚àí\nùëÑ\n20\nUpdate target actor:\nùúÉ‚àí\nùúá‚ÜêùúèùúÉùúá+ (1 ‚àíùúè)ùúÉ‚àí\nùúá\n21\nReset gradients: ùëëùúÉùëÑ‚Üê0 and ùëëùúÉùúá‚Üê0\n22\nSynchronize thread-specific parameters ùúÉ‚Ä≤\nùëÑ‚ÜêùúÉùëÑand ùúÉ‚Ä≤\nùúá‚ÜêùúÉùúá\n23\nEmpty buffer\n24\nend if\n25\nuntil ùë°> 1000 or terminal\nChapter 6\nExperimental Results\nThe experimental analysis aims to evaluate two different ideas. First, we compare the\ndistributed and asynchronous DDPG algorithms to a standard DDPG baseline. Second, we\ninvestigate the effect of various ways to incorporate knowlege of the environment into the\ntraining process. The final goal is to build a training algorithm, that works only on pixel\ndata during training and test time, although this algorithm does not necessarily have to\nbe an end-to-end reinforcement learning algorithm. In the last section of the experiental\nanalysis we will focus on ideas to apply preprocessing to pixel images without any knowledge\nof the physical states. The sections before will include the physical states either only during\nthe training process or for both training and testing.\nDue to computational limitations, it was not possible to perform an extensive hyperparam-\neter search for any of the provided experiments. We therefore reused many hyperparameters\nand some elements of the network structure from Lillicrap et al. (2015) and Mnih, Badia,\net al. (2016). For comparing the different variants of the DDPG algorithm, the same hy-\nperparameters and network structure were used to obtain a reliable relative performance.\nWe used the following hyperparameters for all experiments:\nparameter\nvalue\nupdate rate for target networks ùúè\n0.001\nlearning rate critic\n0.0001\nlearning rate actor\n0.0001\ndiscount factor ùõæ\n0.97\nweight penalty l2 (for critic weights to output neuron)\n0.02\nmaximum number of steps per episode\n1000\nTable 6.1: Overview of the hyperparameters used for all experiments.\n47\n6. Experimental Results\n48\n6.1\nUsing the Physical States for Training and Testing\nAt first, we compared our extended DDPG variants and the DDPG baseline, while learning\ndirectly from the physical states of the matplotlib simulation. This obviously requires the\nphysical states also during test time. Figure 6.1 depicts the network structure we used\nfor the three experiments. We trained each algorithm for 1.600.000 steps and repeated\nthe experiment 15 times. The asynchronous and distributed version both used 16 parallel\nthreads. After each training phase, the trained model was tested for 100 episodes and the\npercentage of the successfully solved episodes was monitored. An episode is considered\nsuccessful, when it takes less than 1000 steps for the gripper to reach the target. We state\nthe mean and standard derivation for all independent tests and also the highest and the\nlowest of the 15 scores.\nIn addition to the experiments using DDPG and its adapted variants, we also tested a\nsimple inverse model, which predicts the action that is needed to guide the gripper of\nthe robotic arm to a desired target position. This model was trained by simply executing\nrandom actions and training the model to output the executed actions while it observes the\nangles of the robotic arm at the state before the action was executed and the position of\nthe gripper after the execution. The inverse model, that is shown in figure 6.2, was trained\non 250.000 batches of size 1.000 drawn from an experience replay memory consisting of\n1.000 episodes with random starting conditions and length 1.000 each. During evaluation\nwe trained the inverse model three times from scratch and then tested each of these models\n5 times on 100 episodes to obtain an evaluation similar to the DDPG experiments. The\ninverse model then does not receive an imagined next position of the gripper, but the true\ntarget position, that is most of the times out of reach.\nExperiment\nScore Mean\nSD\nMax\nMin\ninverse model\n98.20%\n1.28%\n100.00%\n97.20%\nDDPG baseline\n49.40%\n45.49%\n100.00%\n0.00%\ndistributed DDPG\n68.60%\n12.53%\n86.00%\n46.00%\nasynchronous DDPG\n74.67%\n22.30%\n96.00%\n9.00%\nTable 6.2: Summary of the experiments conducted using only the physical states. The inverse\nmodel performs best, while the asynchronous DDPG algorithm outperforms the baseline and\nthe distributed version.\nThe results as depicted in table 6.2 show that asynchronous DDPG outperforms both other\nvariants comparing the mean scores and is able to replicate the best results of the DDPG\nbaseline. Although the asynchronous DDPG algorithm uses Hogwild! style updates, that\nonly approximate the true gradient and induce a chance of one thread overriding the updates\nof another thread, this method seems to improve generalization and eventually yields better\ntest results. Both modifications of DDPG reduce the training variance and do not depend\n6. Experimental Results\n49\nActor\nCritic\nPhysical States (4)\nFully (200, relu)\nFully (200, relu)\nActions (2, tanh)\nPhysical States (4)\nFully (200, relu)\nFully (200, linear)\nAdd\nFully (200, relu)\nActions (2)\nFully (200, linear)\nFully (200, relu)\nQ-value (1, linear)\nFigure 6.1: Network structure used for all variants of DDPG to learn on the physical states.\nA weight penalty is added to the output neuron of the critic to prevent too fast rising Q-values.\non the starting conditions as heavily as the baseline. The distributed DDPG algorithm has\nvery low variance, but is not able to replicate the best results of the baseline.\nFigure 6.3 compares all variants of DDPG and shows the relative amount of successfully\ncompleted episodes during training. The modified DDPG algorithms both outperform the\nbaseline and usually converge fast to better scores. The distributed DDPG algorithm\nconverges best but lacks generalization as the test scores above showed. We also\ndemonstrated that asynchronous training and DDPG can be combined in general to obtain\na working algorithm. The training of our asynchronous DDPG implementation is more than\nfive times faster than the DDPG baseline, because of the parallelization.\n6. Experimental Results\n50\nCurrent angles (2)\nConcatenate\nFully (25, relu)\nTarget gripper pos (2)\nFully (50, relu)\nFully (25, relu)\nActions (2, linear)\nFigure 6.2: Inverse model, which is trained to predict the action that moves the gripper to\na target position, when the current angles of the arm segments are given.\nThe best results however were obtained by using the inverse model, which is very stable\nand achieves 100% success rate most of the time. The inverse model was solely trained on\nsmall movements of the robotic arm. Each arm segment is allowed to maximally move by\n2 degrees in any direction, while the true target that is used as desired position for the\ngripper is often far away and requires many succeeding actions to be reached. The inverse\nmodel is still able to predict very good actions at every time step, although most of the\ndata seen during testing has certainly never been observed during training. We like to see\nthis result as evidence, that the inverse model is able to generalize very well and learns a\ngood understanding of the system dynamics. Because of its very good performance we also\ntested the inverse model on the physical states of the Dart environment (see section 5.1.2),\nwhere one inverse model was trained and tested three times for 500 episodes. The results\nare comparable to those before. We achieved an average score of 99.00% with a standard\nderivation of 0.28%. These findings demonstrate the ability of the inverse model to learn\ndynamics under different physical conditions.\n6. Experimental Results\n51\n0\n20\n40\n60\n80\n100\n120\n140\n160\nnum episode (*10^4)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccessful episodes ratio\nasynchronous DDPG\ndistributed DDPG\nDDPG baseline\nFigure 6.3: Comparing asynchronous DDPG and distributed DDPG to the DDPG baseline.\nThe plots state the relative amount of successful episodes over the last 25 episodes at the\nrespective time. Every algorithm was trained 15 times for 1.600.000 steps and the scores were\naveraged.\n6.2\nUsing the Physical States for Training and Pixel Images for Testing\nWe tested different options to include the physical states in the training process. As pixel\nimages should be used later, it is required to base the reinforcement learning process on a\nrepresentation that can be directly derived from pixels. We use the physical states during\ntraining to regularize a pretraining process, that learns to extract useful information from\nthe pixels. In a second step, we use reinforcement learning to predict actions based on the\noutputs of the pretrained model. In the following, we will describe three different ways to\nextract information from pixels:\n1. Internal model: The easiest way to make use of the physical states is pretraining\na model that directly predicts the physical states from pixels, which we call an internal\nmodel. The concept has been described in section 4.1. The detailed internal model is shown\nin figure 6.4.\n2. Autoencoder, that additionally predicts physical states: An autoencoder is\nnormally used to simply reconstruct images. We extended an autoencoder structure to\nalso predict the physical states like described in section 4.2. The detailed network structure\n6. Experimental Results\n52\nPixel Image (64x64)\nConvolution (8 filters, 3x3 kernel, relu)\nConvolution (32 filters, 5x5 kernel, relu)\nFully (200, relu)\nPhysical states (4, linear)\nFigure 6.4: Detailed architecture of the internal model, that consists of two convolutional\nlayers and one fully connected hidden layer.\nof the extended autoencoder is depicted in figure 6.5. The physical states as additional\ntraining target are supposed to help the autoencoder finding more useful features and\nbetter encoding the pixel data.\n3. Forward model with physical states as output: We trained a simple forward\nmodel (see section 4.3 for comparision), that predicts the physical states of the following\nsystem state given the pixel image of the current state and the executed action in between.\nThe last layer, that only depends on the pixel image of the current state, is used as state\nrepresentation for the reinforcement learning algorithm. By forcing the model to learn about\nthe dynamics of the system, this learned representation is expected to extract useful features\nfor reinforcement learning. Figure 6.6 states the structure of the forward model.\nThe evaluation results of the internal model, the autoencoder with physical states and the\nforward model with physical states are given in table 6.3. We tested every method with\na static background included in all images and also without a background. Every model\nwas trained two times on 100.000 batches of size 250, that were randomly sampled from\na memory consisting of 1.000 episodes with 1.000 steps each. All actions executed during\nthe episodes were randomly sampled. We used the DDPG baseline for evaluation and ran\nreinforcement learning three times for each pretrained model. This means, we conducted\nsix evaluations for every combination of model architecture and background, as each model\nwas pretrained twice. The actor-critic architecture used for reinforcement learning is the\nsame like depicted in figure 6.1, while only the size of the state input has been adapted for\n6. Experimental Results\n53\nPixel Image (64x64)\nConvolution (8 filters, 3x3 kernel, relu)\nConvolution (32 filters, 5x5 kernel, relu)\nFully (200, relu)\nLatent Variables (15, sigmoid)\nFully (200, relu)\nPhysical States (4, linear)\nFully (58 * 58 * 32, relu)\nTransposed Convolution (8 filters, 5x5 kernel, relu)\nTransposed Convolution (1 filter, 3x3 kernel, sigmoid) = Reconstruction\nFigure 6.5: Detailed architecture of the extended autoencoder, that also includes the physical\nstates. We suppose the latent code of the autoencoder to become more useful, when the\ntraining process forces the physical states to be a linear function of it. The latent code is used\nas state input to the reinforcement learning algorithm.\nthe autoencoder and the forward model. Every final actor was tested for 100 episodes and\nthe relative amount of successfully completed episodes was recorded. As the variance for all\nexperiments is high and due to long training time only few independent experiments could\nbe performed for one pretrained model, we only state the maximum scores for comparision\nin table 6.3. The raw scores are listed in appendix A.\n6. Experimental Results\n54\nCurrent Pixel Image (64x64)\nConvolution (8 filters, 3x3 kernel, relu)\nConvolution (32 filters, 5x5 kernel, relu)\nFully (200, relu)\nFully (10, sigmoid)\nConcatenate\nNext Physical States (4, linear)\nActions (2)\nFigure 6.6: Detailed architecture of the simple forward model. Learning the dynamics of the\nunderlying physical system is expected to help finding useful features. The last layer, that\nonly depends on the current pixels (sigmoid layer with 10 units) is fed to the reinforcement\nlearning algorithm and can be predicted without knowledge of the physical states or actions.\nThe best results are obtained by using the autoencoder that predicts both images and\nphysical states, when no background is included. The latent variables of the autoencoder\nthus provide a representation of the system state that is less error-prone than the physical\nstates. The same argumentation applies to the performance of the forward model, which\nalso outperforms the internal model in the case, when no background is included. The\nautoencoder gets heavily distracted by additive noise in the background of the images\nand the maximum score decreases. We therefore trained another version of this kind of\nextended autoencoder using noisy images, but performing mean removal before further\nprocessing them. This approach effectively removed most of the noise and again increased\nthe score, which supports the assumption, that generating images with background noise\nis comparatively hard.\n6. Experimental Results\n55\nExperiment\nbackground enabled\nMax. Score\nInternal model\nyes\n73%\nInternal model\nno\n63%\nAutoencoder w. physical states\nyes\n62%\nAutoencoder w. physical states\nno\n90%\nAutoencoder w. physical states\nyes (mean removal)\n89%\nForward model w. physical states\nyes\n60%\nForward model w. physical states\nno\n82%\nTable 6.3: Summary of the experiments conducted using the physical states during training\nand only pixel data for testing.\nThe additive noise makes image generation harder for the autoencoder, but has a weaker\nimpact, when images are only used as input and convolutional layers can be used to remove\nthe noise. For the inverse model, the scores for images with background are even a little bit\nhigher.\n6.3\nUsing Pixel Images for Training and Testing\nWe trained an autoencoder structure, that is shown in figure 6.7 with the mean-squared-\nerror function and mean removal to reconstruct pixel images. The trained autoencoder was\nable to reconstruct the input images almost perfectly for the matplotlib environment and\neven for the more sophisticated Dart simulation, as depicted in figure 6.8, with only minor\nmodifications of the architecture to being able to process colored images. Reinforcement\nlearning using the latent code of this autoencoder however produced poor results. The\nbest of six independently trained actor-critic architectures, that were trained using the\nlatent code of converged autoencoders as input and tested for 100 episodes each, was only\nable to solve 25% of all episodes. We trained the autoencoder two times on the matplotlib\nenvironment and ran reinforcement learning three times for each pretrained model to obtain\nan evaluation similar to section 6.2. A fully random policy might also be able to achieve that\nscore by randomly hitting the target sometimes. We also trained a variational autoencoder\nby replacing the 30 latent variables with 2x30 neurons that model the variance and mean of\na multivariate gaussian distribution with 30 variables. The variational autoencoder did not\nconverge. Furthermore, we tested several variants of inverse- and forward models only on\npixels, but these did not help to improve the previously obtained score, or also completely\nfailed to converge.\nWe were however able to use our implementation of the asynchronous DDPG algorithm\nto obtain a converging end-to-end learning algorithm. The architecture of the actor-critic\nnetwork is very similar to figure 6.1. Two convolutional layers were added to process\n6. Experimental Results\n56\nPixel Image (64x64)\nConvolution (8 filters, 3x3 kernel, relu)\nConvolution (32 filters, 5x5 kernel, relu)\nFully (200, relu)\nFully (30, sigmoid)\nFully (200, relu)\nFully (58 * 58 * 32, relu)\nTransposed Convolution (8 filters, 5x5 kernel, relu)\nTransposed Convolution (1 filter, 3x3 kernel, sigmoid) = Reconstruction\nFigure 6.7: Detailed architecture of the autoencoder, that simply reconstructs pixel images.\nWhile the model converges very well, it does not provide a useful state representation in the\nlatent code and reinforcement learning fails to find a good policy.\nthe visual information both for the actor and the critic. These layers replace the input\nof the physical states. Because of the long training time, when the gradient must be\nbackpropagated through many additional weights, that are added with the convolutional\nlayers, we only trained the end-to-end model once and tested it for 500 episodes. The\nmodel was able to solve 87% of all episodes. The distributed DDPG algorithm and\nthe DDPG baseline repeatedly failed the end-to-end learning task while using the same\nhyperparameters.\n6. Experimental Results\n57\nFigure 6.8: Input to the converged autoencoder and reconstructed images, exemplary shown\nfor the Dart environment: We apply mean removal to all images before passing them to the\nautoencoder and add the mean to the reconstructed images. The reconstruction looks very\nsimilar to the input. The two rows represent two different random inputs. The columns from\nleft to right depict the following: 1 - input image, 2 - input image minus mean image, 3 -\nreconstruction of the autoencoder, 4 - mean image, 5 - reconstruction plus mean image.\nChapter 7\nDiscussion\nAfter describing the main concepts of deep reinforcement learning, we introduced two novel\nalgorithms, that combine the deep deterministic policy gradient (DDPG) with asynchronous\nmethods in different ways. We first evaluated these algorithms using a simple robotic task,\nwhile the true physical states of the environment were given, and compared both to a DDPG\nbaseline. The most important finding of these experiments is, that the variant of DDPG\nusing asynchronous lock-free gradient updates generalizes better than the variant with locks\nand also converges more often than a DDPG baseline, when executing multiple runs. The\ndecision to use a lock free approach as introduced by Mnih, Badia, et al. (2016) was mostly\nmotivated by performance considerations and not compared to a variant without locks. We\nsuggest that lock-free updates might be beneficial not only to shorten training time but also\nfor improving the generalization of many algorithms using asynchronous updates. We also\nshowed, that the combination of DDPG and asynchronous updates can be applied to solve\nan end-to-end learning task. Another advantage of our asynchronous DDPG implementation\nis, that it is about five times faster than the DDPG baseline due to the parallelization.\nWe also investigated the effect of different pretraining techniques and successfully\nimplemented multiple forms of pretraining, that use the physical states of the environment\nduring training, but do not require them for testing. Levine, Finn, et al. (2016) show that\nreal world robotic tasks sometimes provide access to the true physical states during training,\nbut later require the trained model to act, while only observing camera images. The hybrid\npretraining approach is much faster than reinforcement learning on pixel data, because\npretraining is a standard deep learning task, that can be heavily parallelized and performed\nentirely on the GPU. In contrast to deep reinforcement learning, where it is necessary to\nregularly call the environment and thus communicate with the CPU, large data files can\nbe prepared, which only need to be loaded once into memory. The simplified reinforcement\nlearning task that follows after pretraining is also much faster than reinforcement learning\n58\n7. Discussion\n59\non pixels, as the preprocessed state vector is comparatively small and the actor-critic model\nthus is far less complex.\nThe conducted experiments show, that it is more helpful to train a custom state\nrepresentation than just predicting the physical states. This can be accomplished by adding\nan image reconstruction target similar to that of a deep autoencoder or modeling the system\ndynamics with inverse-/forward models. Referring to Agrawal et al. (2016), we were able to\nshow for a very simple experiment, that an inverse model can generalize well, while learning\nthe system dynamics (see section 6.1). An important question, that still remains open with\nour work, is how this generalization effect can be transfered to effectively process pixel data.\nThe internal model of section 6.2 performs better on images with background noise, which\nat first might seem irrational. While this effect could be random variance and caused by the\nsmall number of collected scores, noisy images in fact sometimes proved useful for learning\nto detect features. Vincent et al. (2008) showed that it can be beneficial to include additional\nnoise in input images, because the trained model needs to find ways to distinguish noise\nfrom important features and thus learns a better representation of the important variations\nin the images. The same effect might apply here. For both other architectures, noisy images\nnegatively influence the test scores. Learning to reconstruct images or learning the system\ndynamics presumably improves the detection of features in other ways and thus adding noise\nhas an impact that is contrary to the positive effect we observed with the internal model.\nIt might still be interesting to investigate the ability of all pretrained models to benefit\nfrom background noise, for example by using noisy images as input to the autoencoder\nand images without noise as reconstruction target. This is the principle of the denoising\nautoencoder (Vincent et al. 2008).\nIn section 6.2 we also showed, that a major problem of pretraining pixel based models, is\nthe need to generate pixel images. This is for instance the case when using an autoencoder\nnetwork structure. Agrawal et al. (2016) showed that a combined inverse-/forward model\ncan generalize well on pixel data, while they sidestep the complicated problem of generating\nimages by first transforming the pixel images to a learned latent representation. Thereby,\nthey establish a model, that combines the advantages of autoencoders with those of inverse-\nand forward-models. During our experiments, we tested both concepts independently. The\nconvolutional layers, that carry out the transformation to the latent space are jointly trained\nwith the combined inverse-/forward model. This joint training approach cannot be applied\nto our experiment, because the model only learns to detect objects in the images it is able to\nphysically interact with. The arm of our simulated reacher task however does not physically\ninteract with the target, which thus would have never been recognized.\nBoth of our simulated reacher experiments are similar to the OpenAI Gym reacher task1,\nwhich we did not directly use, as it requires the proprietary mujoco library. The Dart\nenvironment has been compared to the OpenAI Gym reacher task and it has been shown,\n1https://gym.openai.com/envs/Reacher-v1/, last downloaded 2017-09-14\n7. Discussion\n60\nthat learned policies can be transfered between the two environments2. We however\nconcentrated on the simple matplotlib simulation for the most experiments, which makes\nlearning arguably easier, because it does not include realistic physics and the visualization\nis very simple even when additional noise is added. The experiments we conducted are\ntherefore not very comprehensive. For comparing to state-of-the-art methods in the field\nof deep reinforcement learning, most researchers evaluate their algortihms on many video\ngames and simulated robotic tasks. It has become a quasi-standard to state the scores\nfor all Atari games and use the mujoco simulations for robotic experiments (Mnih, Badia,\net al. 2016; Lillicrap et al. 2015). An interesting next step would therefore be to test the\ntwo novel variants of DDPG on a range of these experiments to being able to compare the\nrespective scores and reliably assess the quality of these algorithms, while we showed that\nboth algorithms can learn reasonable policies. Our distributed and asyncronous variants of\nDDPG are mainly motivated by the A3C algorithm. Both A3C and DDPG enjoy a high\npopularity and the underlying concepts are still used and further refined (O‚ÄôDonoghue et al.\n2016; Gu, Lillicrap, et al. 2016) or applied to various real world tasks (e.g. Gu, Holly, et al.\n2017). Hence, we think that deterministic policies and asynchronous learning in general still\nprovide a good starting point for future research.\nThe DDPG baseline we used has very high variance, but was still used for all experiments\nwith pretrained models, because we did not want to mix up the effects of the modifications\nto DDPG with the effects of pretraining a state model. We observed that the variance\nincreased when we did not train on the physical states directly, but on an intermediate\nrepresentation obtained by executing a pretrained model. This can be caused by the fact,\nthat we used two different pretrained models of the same kind for all experiments in section\n6.2 and one model probably converged better than the other. Because the whole training\nprocess with pretraining and repeatedly predicting the intermediate representation during\nreinforcement learning is already relatively slow, we also collected less data than for the\nexperiments on physical states (see section 6.1). We also suppose, that learning on the\nintermediate representation is in general harder and thus the DDPG baseline fails more\noften. For the end-to-end learning algorithm, we were only able to train one model with\nasynchronous DDPG as training the convolutional layers was still very slow, despite the\nuse of asynchronous updates.\nWe were generally able to provide a good solution with a score above 85% for all three\nmain categories of experiments: reinforcement learning directly on the physical states,\npretraining a state model using the physical states supplementary to pixels and learning\nonly from pixels. An unsuccesful approach was to pretrain a state model only from pixel\ndata. Goodfellow, Bengio, and Courville (2016) state, that unsupervised pretraining might\nbe outdated for many applications, where end-to-end learning is possible. The reinforcement\nsignal is supposed to provide helpful information to the algorithm and thus enhances the\ndetection of features in images, that are useful for the task to solve. Purely unsupervised\n2https://github.com/DartEnv/dart-env/wiki/OpenAI-Gym-Environments, last downloaded 2017-09-14\n7. Discussion\n61\npretraining like training an autoencoder does not have this information and is therefore\nless efficient. Recent innovations like the variational autoencoder detect features in images\nvery well, but are designed to generalize to small changes in position, orientation or shape\nof objects (Doersch 2016). Robotic applications however often require exact knowledge of\npositions of objects or angles like in our simulated example (see section 5.1). Nevertheless, it\nis possible to still make use of inverse- or forward models, as they require the model to learn\nthe dynamics of the system. Dosovitskiy and Koltun (2016) use a forward model to solve a\ncontrol task without reinforcement learning, but only focus on discrete actions. We suppose\nthat under these conditions, unsupervised pretraining can work, but might be more useful\nin other scenarios with different environements than ours. The model capacity of our tested\nmodels for unsupervised pretraining should be sufficient, as a very similar model was able\nto solve an end-to-end learning task, but regularization strategies like batch normalization\n(Ioffe and Szegedy 2015) or others might be added. Mean removal seems to be crucial for\nthe success of all models, that need to generate images. We also think, that the success of\nthe simple inverse model of section 6.1 is mainly caused by its very fast training speed and\nthe ability to train it on many millions of example transitions in few hours. It thus might\nbe worthwhile to further investigate especially inverse- and forward models, that only have\naccess to pixel data, and train them for a very long time.\nNone of our eperiments included any form of recurrence or memory in the network structure.\nRecurrent neural networks like the LSTM (Hochreiter and Schmidhuber 1997) have been\nsuccessfully used by many researchers to solve reinforcement learning problems, where only\nparts of the environment can be observed and the whole process forms a POMDP (e.g. Mnih,\nBadia, et al. 2016). Partially observed environments were shortly mentioned in section 3.1,\nwhile we did not consider them in any experiment throughout this thesis. Nevertheless, we\nsuppose that many of the used network structures can be augmented with recurrent layers\nand thus can also be applied to partially observable environments.\nBibliography\nAbbeel, Pieter et al. (2007). ‚ÄúAn application of reinforcement learning to aerobatic\nhelicopter flight‚Äù. In: Advances in neural information processing systems, pp. 1‚Äì8 (cit.\non p. 1).\nAgrawal, Pulkit et al. (2016). ‚ÄúLearning to poke by poking: Experiential learning of intuitive\nphysics‚Äù. In: Advances in Neural Information Processing Systems, pp. 5074‚Äì5082 (cit.\non pp. 35, 59).\nDegris, Thomas, Patrick M Pilarski, and Richard S Sutton (2012). ‚ÄúModel-free reinforce-\nment learning with continuous action in practice‚Äù. In: American Control Conference\n(ACC), 2012. IEEE, pp. 2177‚Äì2182 (cit. on p. 25).\nDeisenroth, Marc Peter, Gerhard Neumann, Jan Peters, et al. (2013). ‚ÄúA survey on policy\nsearch for robotics‚Äù. Foundations and Trends in Robotics 2.1‚Äì2, pp. 1‚Äì142 (cit. on p. 22).\nDoersch, Carl (2016). ‚ÄúTutorial on variational autoencoders‚Äù. arXiv preprint arXiv:1606.05908\n(cit. on p. 61).\nDosovitskiy, Alexey and Vladlen Koltun (2016). ‚ÄúLearning to act by predicting the future‚Äù.\narXiv preprint arXiv:1611.01779 (cit. on pp. 35, 37, 61).\nDoya, Kenji (2000). ‚ÄúComplementary roles of basal ganglia and cerebellum in learning and\nmotor control‚Äù. Current opinion in neurobiology 10.6, pp. 732‚Äì739 (cit. on p. 5).\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville (2016). Deep learning. MIT press\n(cit. on pp. 1, 3, 7, 15, 19, 32, 60).\nGu, Shixiang, Ethan Holly, et al. (2017). ‚ÄúDeep reinforcement learning for robotic\nmanipulation with asynchronous off-policy updates‚Äù. In: International Conference on\nRobotics and Automation (ICRA). IEEE, pp. 3389‚Äì3396 (cit. on pp. 28, 60).\nGu, Shixiang, Timothy Lillicrap, et al. (2016). ‚ÄúQ-prop: Sample-efficient policy gradient\nwith an off-policy critic‚Äù. arXiv preprint arXiv:1611.02247 (cit. on p. 60).\nHausknecht, Matthew and Peter Stone (2015). ‚ÄúDeep recurrent q-learning for partially\nobservable mdps‚Äù. CoRR, abs/1507.06527 (cit. on pp. 5, 8).\nHochreiter, Sepp and J√ºrgen Schmidhuber (1997). ‚ÄúLong short-term memory‚Äù. Neural\ncomputation 9.8, pp. 1735‚Äì1780 (cit. on p. 61).\n62\nBibliography\n63\nIoffe, Sergey and Christian Szegedy (2015). ‚ÄúBatch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift‚Äù. In: International Conference on\nMachine Learning, pp. 448‚Äì456 (cit. on p. 61).\nKaelbling, Leslie Pack, Michael L Littman, and Andrew W Moore (1996). ‚ÄúReinforcement\nlearning: A survey‚Äù. Journal of artificial intelligence research 4, pp. 237‚Äì285 (cit. on\npp. 7, 9).\nKawato, Mitsuo (1999). ‚ÄúInternal models for motor control and trajectory planning‚Äù.\nCurrent opinion in neurobiology 9.6, pp. 718‚Äì727 (cit. on pp. 31, 35).\nKingma, Diederik and Jimmy Ba (2014). ‚ÄúAdam: A method for stochastic optimization‚Äù.\narXiv preprint arXiv:1412.6980 (cit. on p. 20).\nKingma, Diederik and Max Welling (2013). ‚ÄúAuto-encoding variational bayes‚Äù. arXiv\npreprint arXiv:1312.6114 (cit. on pp. 4, 33).\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton (2012). ‚ÄúImagenet classification\nwith deep convolutional neural networks‚Äù. In: Advances in neural information processing\nsystems, pp. 1097‚Äì1105 (cit. on p. 15).\nLee, Honglak et al. (2007). ‚ÄúEfficient sparse coding algorithms‚Äù. In: Advances in neural\ninformation processing systems, pp. 801‚Äì808 (cit. on p. 32).\nLevine, Sergey, Chelsea Finn, et al. (2016). ‚ÄúEnd-to-end training of deep visuomotor\npolicies‚Äù. Journal of Machine Learning Research 17.39, pp. 1‚Äì40 (cit. on pp. 14, 30,\n31, 58).\nLevine, Sergey, Peter Pastor, et al. (2016). ‚ÄúLearning hand-eye coordination for robotic\ngrasping with deep learning and large-scale data collection‚Äù. The International Journal\nof Robotics Research (cit. on pp. 28, 30).\nLillicrap, Timothy et al. (2015). ‚ÄúContinuous control with deep reinforcement learning‚Äù.\narXiv preprint arXiv:1509.02971 (cit. on pp. 2, 7, 27, 45, 47, 60).\nLin, Long-H (1992). ‚ÄúSelf-improving reactive agents based on reinforcement learning,\nplanning and teaching‚Äù. Machine learning 8.3/4, pp. 69‚Äì97 (cit. on pp. 7, 17).\nMakhzani, Alireza et al. (2015). ‚ÄúAdversarial autoencoders‚Äù. arXiv preprint arXiv:1511.05644\n(cit. on p. 33).\nMnih, Volodymyr, Adria Puigdomenech Badia, et al. (2016). ‚ÄúAsynchronous methods\nfor deep reinforcement learning‚Äù. In: International Conference on Machine Learning,\npp. 1928‚Äì1937 (cit. on pp. 2, 28‚Äì30, 45, 47, 58, 60, 61).\nMnih, Volodymyr, Koray Kavukcuoglu, David Silver, Alex Graves, et al. (2013). ‚ÄúPlaying\natari with deep reinforcement learning‚Äù. arXiv preprint arXiv:1312.5602 (cit. on pp. 7,\n16).\nMnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A Rusu, et al. (2015). ‚ÄúHuman-\nlevel control through deep reinforcement learning‚Äù. Nature 518.7540, pp. 529‚Äì533 (cit.\non pp. 16‚Äì18).\nO‚ÄôDonoghue, B. et al. (2016). ‚ÄúCombining policy gradient and Q-learning‚Äù. ArXiv e-prints\narXiv:1611.01626 (cit. on p. 60).\nPeters, Jan and J Andrew Bagnell (2011). ‚ÄúPolicy gradient methods‚Äù. In: Encyclopedia of\nMachine Learning. Springer, pp. 774‚Äì776 (cit. on pp. 22, 23).\nBibliography\n64\nRecht, Benjamin et al. (2011). ‚ÄúHogwild: A lock-free approach to parallelizing stochastic\ngradient descent‚Äù. In: Advances in neural information processing systems, pp. 693‚Äì701\n(cit. on pp. 28, 45).\nRifai, Salah et al. (2011). ‚ÄúContractive auto-encoders: Explicit invariance during feature\nextraction‚Äù. In: Proceedings of the 28th international conference on machine learning\n(ICML-11), pp. 833‚Äì840 (cit. on p. 33).\nScherer, Dominik, Andreas M√ºller, and Sven Behnke (2010). ‚ÄúEvaluation of pooling\noperations in convolutional architectures for object recognition‚Äù. Artificial Neural\nNetworks‚ÄìICANN 2010, pp. 92‚Äì101 (cit. on p. 16).\nSilver, David, Aja Huang, et al. (2016). ‚ÄúMastering the game of Go with deep neural\nnetworks and tree search‚Äù. Nature 529.7587, pp. 484‚Äì489 (cit. on p. 1).\nSilver, David, Guy Lever, et al. (2014). ‚ÄúDeterministic policy gradient algorithms‚Äù. In:\nProceedings of the 31st International Conference on Machine Learning (ICML-14),\npp. 387‚Äì395 (cit. on p. 26).\nSutton, Richard S (1988). ‚ÄúLearning to predict by the methods of temporal differences‚Äù.\nMachine learning 3.1, pp. 9‚Äì44 (cit. on p. 12).\nSutton, Richard S and Andrew G Barto (1998). Reinforcement learning: An introduction.\nMIT press Cambridge (cit. on pp. 4, 6, 8‚Äì11, 13, 25).\nSutton, Richard S, David A McAllester, et al. (2000). ‚ÄúPolicy gradient methods for rein-\nforcement learning with function approximation‚Äù. In: Advances in neural information\nprocessing systems, pp. 1057‚Äì1063 (cit. on p. 25).\nTieleman, Tijmen and Geoffrey Hinton (2012). ‚ÄúLecture 6.5-rmsprop: Divide the gradient by\na running average of its recent magnitude‚Äù. COURSERA: Neural networks for machine\nlearning 4.2, pp. 26‚Äì31 (cit. on p. 20).\nVan Hasselt, Hado, Arthur Guez, and David Silver (2016). ‚ÄúDeep Reinforcement Learning\nwith Double Q-Learning.‚Äù In: AAAI, pp. 2094‚Äì2100 (cit. on p. 20).\nVincent, Pascal et al. (2008). ‚ÄúExtracting and composing robust features with denoising\nautoencoders‚Äù. In: Proceedings of the 25th international conference on Machine learning.\nACM, pp. 1096‚Äì1103 (cit. on pp. 4, 32, 59).\nWang, Ziyu et al. (2015). ‚ÄúDueling network architectures for deep reinforcement learning‚Äù.\narXiv preprint arXiv:1511.06581 (cit. on pp. 20, 21).\nWatkins, Christopher John Cornish Hellaby (1989). ‚ÄúLearning from delayed rewards‚Äù. PhD\nthesis. King‚Äôs College, Cambridge (cit. on pp. 11, 13).\nWatkins, Christopher John Cornish Hellaby and Peter Dayan (1992). ‚ÄúQ-learning‚Äù. Machine\nlearning 8.3-4, pp. 279‚Äì292 (cit. on p. 10).\nWilliams, Ronald J (1992). ‚ÄúSimple statistical gradient-following algorithms for connec-\ntionist reinforcement learning‚Äù. Machine learning 8.3-4, pp. 229‚Äì256 (cit. on pp. 23,\n24).\nWoergoetter, Florentin and Bernd Porr (2008). ‚ÄúReinforcement learning‚Äù. Scholarpedia 3.3,\np. 1448 (cit. on pp. 7, 11).\nZhang, Fangyi et al. (2015). ‚ÄúTowards vision-based deep reinforcement learning for robotic\nmotion control‚Äù. arXiv preprint arXiv:1511.03791 (cit. on p. 16).\nAppendix A\nRaw Scores\nExperiment\nbackground enabled\nRaw Scores\nInternal model\nyes\n73%, 65%, 62%, 52%, 48%, 2%\nInternal model\nno\n63%, 61%, 55%, 2%, 1%, 1%\nAutoencoder w. physical states\nyes\n62%, 59%, 34%, 28%, 1%, 0%\nAutoencoder w. physical states\nno\n90%, 3%, 2%, 2%, 0%, 0%\nAutoencoder w. physical states\nyes (mean removal)\n89%, 85%, 82%, 56%, 1%, 0%\nForward model w. physical states\nyes\n60%, 60%, 52%, 0%, 0%, 0%\nForward model w. physical states\nno\n82%, 50%, 23%, 1%, 0%, 0%\nTable A.1: Raw scores for the experiments conducted using the physical states during\ntraining and only pixel data for testing.\nExperiment\nRaw Scores\nFull Autoencoder\n25%, 12%, 8%, 5%, 3%, 2%\nTable A.2: Raw scores for the experiments conducted using pixel images during training\nand testing.\n65\n",
  "categories": [
    "cs.LG",
    "cs.RO",
    "stat.ML"
  ],
  "published": "2018-10-15",
  "updated": "2018-10-15"
}