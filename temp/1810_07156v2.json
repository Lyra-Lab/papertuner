{
  "id": "http://arxiv.org/abs/1810.07156v2",
  "title": "Strategies for Language Identification in Code-Mixed Low Resource Languages",
  "authors": [
    "Soumil Mandal",
    "Sankalp Sanand"
  ],
  "abstract": "In recent years, substantial work has been done on language tagging of\ncode-mixed data, but most of them use large amounts of data to build their\nmodels. In this article, we present three strategies to build a word level\nlanguage tagger for code-mixed data using very low resources. Each of them\nsecured an accuracy higher than our baseline model, and the best performing\nsystem got an accuracy around 91%. Combining all, the ensemble system achieved\nan accuracy of around 92.6%.",
  "text": "arXiv:1810.07156v2  [cs.CL]  31 Oct 2018\nStrategies for Language Identiﬁcation in Code-Mixed Low Resource\nLanguages\nSoumil Mandal, Sankalp Sanand\nDepartment of Computer Science & Engineering\nSRM Institute of Science & Technology, Chennai, India\n{soumil.mandal, sankalp.sanand}@gmail.com\nAbstract\nIn recent years, substantial work has been\ndone on language tagging of code-mixed\ndata, but most of them use large amounts\nof data to build their models. In this ar-\nticle, we present three strategies to build\na word level language tagger for code-\nmixed data using very low resources. Each\nof them secured an accuracy higher than\nour baseline model, and the best per-\nforming system got an accuracy around\n91%. Combining all, the ensemble system\nachieved an accuracy of around 92.6%.\n1\nIntroduction\nOn social media, we can often see bilinguals\nswitching back and forth between two languages,\na phenomenon commonly referred to as code-\nswitching or code-mixing (Sridhar and Sridhar,\n1980). For processing such type of textual data,\nwe see that traditional approaches perform quite\npoorly, especially due to mixing of different rules\nof grammar, ambiguity, typing variances, as well\nas other types of noises contributed by informality\nof the social media aspect. Two types of code-\nmixing can be commonly seen, one where both\nthe languages share a similar mother script, like\nEnglish and Spanish. The other being where the\nmother scripts of both the languages being mixed\nare different, for example English and Hindi. In\nthe ﬁrst case, typing is done in the respective na-\ntive scripts, while in the latter, one of the lan-\nguages is typed in its transliterated form, so as to\nmaintain homogeneity as well as increase ease of\ntyping. Though there are some standard transliter-\nation rules, for example ISO 1, ITRANS 2, it’s al-\nmost impossible to follow them in real life. Thus,\n1https://en.wikipedia.org/wiki/ISO 9\n2https://en.wikipedia.org/wiki/ITRANS\nwe see variances in transliteration typing, con-\ntributed by different phonetic judgments, dialects,\ninformality, etc. which increases the challenges of\nprocessing such data even more. Thus, for making\nany systems for such data, it is extremely impor-\ntant to develop a good language tagger as this will\ngreatly affect the successive modules.\nIn this article, we present three methods for\nbuilding language taggers when the amount of\ncode-mixed data available is relatively low. These\ninclude usage of convolutional neural networks,\ndata augmentation prior to training, and use of\nsiamese networks.\nAt the end, we have also\nbuilt an ensemble classiﬁer combining all these 3\nmethods. The language pairs we worked on are\nBengali-English (Bn-En) & Hindi-English (Hi-\nEn) code-mixed data. Both are Indic languages,\nhaving Eastern Nagari Script 3 and Devnagari\nScript 4 as their native scripts respectively. Thus,\nthe Bn/Hi tokens are in their phonetically translit-\nerated form in Roman.\n2\nRelated Work\nIn the earlier days, two major strategies were used,\nn-grams (Cavnar et al., 1994) and dictionary look\nup (Rehurek and Kolkus, 2009). Several research\nwork has been done in the recent past in order\nto improve such taggers.\nNguyen and Do˘gru¨oz\n(2013) used linear chain CRFs with context in-\nformation limited to bigrams. Das and Gamb¨ack\n(2014) utilized multiple features like word level\ncontext, dictionary, n-gram, edit distance as fea-\ntures for their classiﬁer.\nJhamtani et al. (2014)\ncreated an ensemble model.\nThe ﬁrst classiﬁer\nuses edit distance, character n-grams and word fre-\nquency, while the second classiﬁer takes output\nfrom the ﬁrst one for current word, along with\n3https://en.wikipedia.org/wiki/Eastern Nagari script\n4https://en.wikipedia.org/wiki/Devanagari\nPOS tags of the neighboring tokens. Vyas et al.\n(2014) proposed a method which uses logistic re-\ngression and code-switching probability.\nIn the\nﬁrst shared task of code-mixed language identi-\nﬁcation (Solorio et al., 2014), the most popular\nsystems used char n-grams combined with rules,\nas well as CRFs and HMMs.\nPiergallini et al.\n(2016) used capitalization along with character n-\ngrams. For arbitrary set of languages, a general-\nized architecture based on HMM was developed\nby Rijhwani et al. (2017). Choudhury et al. (2017)\ncreated a model which concatenates character and\nword embeddings for learning. They also experi-\nmented with several curriculum, or order in which\nthe data is presented while training. Mandal et al.\n(2018a) trained character and phonetic embedding\nmodels and then combined them to create an en-\nsemble model. To the best of our knowledge, no\nwork has been done where the amount of data used\nfor building the supervised models is low.\n3\nData Sets\nWe collected Bengali (Bn) words from the code-\nmixed data prepared in Mandal et al. (2018b)\nwhile Hindi (Hi) words were collected from\nthe data released in ICON 17, tools contest\n(Patra et al., 2018). The number of unique tokens\ncollected for both Bn and Hi each was 6000. As\nthe goal is to train our models using low resource,\nwe decided to set the size of our training data at\n1000 unique tokens. To have convincing conclu-\nsions, as one may argue that the models depend\nhighly on the selected low amount of words, we\ndecided to train multiple models on batches of size\n1000 and take the average of the results on our\ntesting data.\nLang\nTrain\nDev\nTest\nEn\n4x1000\n1000\n1000\nBn\n4x1000\n1000\n1000\nHi\n4x1000\n1000\n1000\nTable 1: Data distribution.\nAll the experiments in the following sections were\nperformed individually on 8 sets of data, Bn-En\n(4) + Hi-En (4), and then average was taken. Also,\nsince testing the ﬁnal models at instance level is\nmore logical, we extracted random 1000 instances\nof Bn-En and Hn-En type each for ﬁnal testing.\nThe mean code-mixing index (Das and Gamb¨ack,\n2014) were 21.4 and 18.7 respectively.\n4\nBaseline System\nAs a baseline, we decided to use the charac-\nter embedding based architecture described in\nMandal et al. (2018a), which uses stacked LSTMs\n(Hochreiter and Schmidhuber, 1997) of sizes 15-\n35-15-1, where 15 is the size of input layer, 1 is\nthe size of output layer, and 35, 25 are the sizes\nof the hidden layers. Batch size was kept at 64\nand epochs were set to 100. Optimizer used was\nAdam (Kingma and Ba, 2014), loss function was\nbinary cross-entropy and activation function for\noutput cell was sigmoid. Bn/Hi was labeled 0 and\nEn as 1. On a whole 8 models were trained (4 for\nBn-En and 4 for Hi-En) using distribution shown\nin Table 1, tuned using threshold on development\ndata, and tested. Tuning is a simple method used\nto convert the sigmoid output into a classiﬁcation,\nmaximizing the accuracy on the development data\nby altering the threshold using brute force. For ex-\nample, if sigmoid out ≥θ, output 1 (En), else 0\n(Bn/Hi). The detailed results along with the aver-\nage is shown in Table 2.\nBn-En\nModel\nθ\nAcc\nRec\nPrec\nF1\n1\n0.87\n80.25\n78.83\n82.70\n80.23\n2\n0.89\n79.05\n77.17\n82.50\n79.02\n3\n0.87\n82.10\n81.10\n83.70\n82.09\n4\n0.86\n81.60\n80.73\n83.00\n81.59\navg\n0.87\n80.75\n79.45\n82.97\n80.73\nHi-En\nModel\nθ\nAcc\nRec\nPrec\nF1\n1\n0.84\n80.00\n76.50\n86.60\n79.91\n2\n0.83\n80.40\n76.61\n87.50\n80.30\n3\n0.83\n80.15\n75.62\n86.40\n80.10\n4\n0.84\n80.65\n77.14\n86.45\n80.45\navg\n0.83\n80.30\n76.46\n86.73\n80.19\nTable 2: Results of baseline system.\nWe can see that the average accuracy achieved for\nBn-En is 80.75% while that for Hi-En is 80.3%.\n5\nConvolutional 1D Network\nConvolutional\nneural\nnetworks\n(LeCun et al.,\n1999) have been rapidly gaining popularity in the\nNLP world, especially as it gives quite satisfac-\ntory results, but yet is much faster than recur-\nrent units. In the recent years, CNNs have shown\namazing results for text classiﬁcation problems\n(Johnson and Zhang (2014), Johnson and Zhang\n(2015)). For our experiments, we used the Con-\nvolutional 1D variant of CNNs, which is ideal for\ntext. The texts can be thought as images of length\n15 (embedding size) and height 1. Thus, n-gram\nfeatures can be captured using a kernel of size n.\n5.1\nTraining\nFor\nimplementation\nwe\nused\nthe\nKeras\n(Chollet et al.,\n2015)\nAPI.\nThe\narchitecture\nthat we employed for training had layers 15-\nCNN-D(0.2)-CNN-D(0.2)-1.\nThat is, the input\nlayer size is 15, and the output layer size is 1. The\nhidden layers are CNN units in order, and had\nﬁlters of size 32. Filters are essentially vectors\nof weights using which the input is convolved,\nit provides a measure for how close a patch of\ninput resembles a feature. D(0.2) is the dropout\nlayer, and 0.2 is the dropout rate. This reduces\ncomputation cost even further, as well as chances\nof overﬁtting.\nThe ﬁrst CNN unit had kernel\nsize 2 and stride 1 (capturing bigrams), while\nthe second CNN had kernel size 3 and stride 1\n(capturing trigrams).\nLoss function used was\nbinary crossentropy, optimizer used was nadam\n(Sutskever et al., 2013), and activation function\nfor output unit was sigmoid. Batch size was kept\nat 64 while number of epochs was set to 100\n(identical to baseline). Post training, in order to\nconvert the sigmoid outputs to classiﬁcation, a\nsimilar brute force as discussed in baseline system\nwas used.\n5.2\nEvaluation\nThe results obtained using stacked CNNs are\nshown below in Table 3.\nBn-En\nModel\nθ\nAcc\nRec\nPrec\nF1\n1\n0.89\n84.90\n89.12\n79.50\n84.85\n2\n0.89\n84.85\n89.11\n79.40\n84.80\n3\n0.89\n84.70\n88.90\n79.30\n84.65\n4\n0.88\n84.85\n88.93\n79.60\n84.80\navg\n0.89\n84.83\n89.02\n79.45\n84.78\nHi-En\nModel\nθ\nAcc\nRec\nPrec\nF1\n1\n0.88\n84.30\n88.28\n79.10\n84.25\n2\n0.87\n83.75\n87.71\n78.50\n83.71\n3\n0.89\n84.15\n88.15\n78.90\n84.11\n4\n0.88\n83.80\n87.80\n78.50\n83.75\navg\n0.88\n84.00\n87.98\n78.75\n83.96\nTable 3: Results using stacked Conv1D.\nA modest improvement can be seen compared to\nthe baseline (about 4%), which translates to ≈80\nmore correct predictions.\n6\nData Augmentation\nData augmentation has proven to be a useful strat-\negy when data is limited but supervised mod-\nels are required to be made (Ragni et al. (2014),\nFadaee et al. (2017)).\nThe idea we will be em-\nploying is to increase the amount of data using\nprobabilistic and neural models which will lead to\nan increase of feature points. Such augmentation\nhas high chances of extracting new features and\nessential ones which can boost up the system per-\nformance. Two methods were tested for augment-\ning words in order to increase dataset size, one is\na simple n-gram based probabilistic generator, an-\nother is a neural generator using LSTM. Both the\ngenerator models take two parameters, seed and\ngeneration length. The seed is the string which\nthe generator takes as the input in order to gen-\nerate following characters in order to produce the\nﬁnal word. The generation length is the length of\nthe string the generator is asked to generate.\n6.1\nN-Gram Generator\nFor building the n-gram generator, the classic lan-\nguage modeling approach was taken (Ney et al.,\n1994), based on Markov assumption. For prepar-\ning the data for modeling, the tokens were simply\nsplit character wise and stored in an array. Then,\nthe n-gram probabilities were estimated. This is\nessentially P(ci+1|ci, ci-1) = C(ci+1, ci, ci-1)/C(ci,\nci-1), where C(.) counts the number of times the\ngiven n-gram occurs.\nWe considered character\nlevel unigrams, bigrams and trigrams for model-\ning words, i.e. 3 generators (each for Bn and Hi).\nFor example, the trigram model can be represented\nas P(c1, c2, ..., cn) ≈P(c1)P(c2|c1)Qn\ni=3P(ci|ci-1,\nci-2). A random function is used to switch ran-\ndomly between unigram, bigram and trigram. Our\nn-gram generator had one extra parameter which\nwould control whether the argmax character is\noutputted or the argsecondmax, it was determined\nusing a random function. This was done in order\nto increase variance in generated strings.\n6.2\nLSTM Generator\nIn order to increase variance in our augmented\ndata, we decided to make a LSTM based gener-\nator. For data preparation, a simple character wise\nsplit was done of the existing 1000 tokens. Each\nword is treated as a separate time series entity.\nThese instances were then numerically mapped\nwith an integer, to create embeddings. Now, sim-\nply for preparing the training data, the target is set\njust by shifting the input sequence by n steps. We\ncreated 3 generators (each for Bn and Hi), by tak-\ning n ranging from (1, 3). These generators are\nessentially learning character based word model-\ning.\nMathematically, given a training sequence\n(x1, x2, ..., xn), the LSTM uses a sequence of out-\nput vectors (o1, o2, ..., on) to learn a sequence of\npredictive distributions P(xt+1|x≤t) = softmax(ot).\nThe softmax is expressed as P(softmax(ot = j) =\nexp(ot(j))/ P\nkexp(ot(k)). Objective of learning is to\nmaximize the total log probability of the training\nsequence PT−1\nt=0 log P(xt+1|x≤t).\nSampling from\nthis conditional distribution, the next character\nis generated and is also fed back to the LSTM\n(Sutskever et al., 2011). Our models were imple-\nmented using the Keras (Chollet et al., 2015) API.\nThe layer sizes of our LSTM generator was 26-\n200-26.\nActivation function used was softmax,\nloss was categorical cross-entropy, and optimizer\nused was rmsprop with a learning rate of 0.001.\nAll other parameters were kept at default. Batch\nsize was kept at 32 and number of epochs was set\nto 50.\n6.3\nAugmentation\nBefore augmentation, we needed to create a list\nof seeds using which the generators would model\nwords and augment data.\nFor this, we simply\nextracted 1000 substrings, bigram (100), trigram\n(300), quadgram (600) from training data having\nhighest frequencies. The generation length which\nwe decided upon ranged from (3, 16). This value\nwas passed to the generator functions using a ran-\ndom function generating a value between (3, 16).\nUsing these, 1500 tokens was generated by n-gram\nand LSTM generator each, i.e.\na total of 3000\naugmented tokens. Combining this with the 1000\ntraining tokens, the number of tokens in the ﬁnal\ntraining data was 4000 for each language.\n6.4\nTraining\nThe architecture and hyperparameters used for\ntraining was identical to that of the baseline sys-\ntem. This was done to keep parity, and make the\nresults obtained post augmentation comparable to\nthat of the baseline system. As neural networks are\nvery parameter sensitive, a change in batch size, or\nany learning functions will force a very different\nlearning making the results incomparable.\n6.5\nEvaluation\nThe results of the models trained on augmented\ndata is shown below in Table 4.\nBn-En\nModel\nθ\nAcc\nRec\nPrec\nF1\n1\n0.93\n88.45\n87.30\n89.35\n88.44\n2\n0.93\n88.55\n87.60\n89.29\n88.54\n3\n0.94\n88.40\n87.70\n88.94\n88.39\n4\n0.93\n88.50\n87.90\n88.97\n88.49\navg\n0.93\n88.48\n87.63\n89.14\n88.47\nHi-En\nModel\nθ\nAcc\nRec\nPrec\nF1\n1\n0.92\n88.20\n87.20\n88.98\n88.20\n2\n0.93\n88.15\n87.00\n89.05\n88.15\n3\n0.93\n88.05\n87.00\n88.87\n88.05\n4\n0.92\n87.90\n86.80\n88.75\n87.89\navg\n0.93\n88.08\n87.00\n88.91\n88.07\nTable 4: Results using data augmentation.\nWe can see that augmentation has improved the\naccuracy by a fair amount (about 8%), which\ntranslates to ≈160 more correctly predicted to-\nkens.\n7\nSiamese Network\nSiamese networks\n(Bromley et al.,\n1994)\nare\nneural networks whose architecture contains two\nidentical sub-networks, which are joined at a sin-\ngle point, and hence the name. The sub-networks\nhave identical\nconﬁguration,\nparameters\nand\nweights. In such a network, back-propagation and\nupdating is mirrored across both the networks.\nA unique property of such networks is that even\nthough it takes two inputs, the order doesn’t\nmatter (i.e symmetry). This architecture performs\nquite well on low resource problems or even\none-shot tasks (Koch et al. (2015), Vinyals et al.\n(2016)). This is because sharing weights across\nsub-networks results in requirement of fewer\nparameters to train, which reduces chances of\noverﬁtting. They are quite good, and thus popular\nwhere ﬁnding similarity or relationship between\ntwo comparable things is a concern. In the recent\npast, it has become popular for ﬁnding textual\nsimilarity\nas\nwell\n(Mueller and Thyagarajan\n(2016), Neculoiu et al. (2016), Mou et al. (2016)).\nThe siamese network is built around learning\na distance metric dis between two vectors x1 and\nx2 (each coming from one sub-network), thus the\norder doesn’t matter, as dis(x1, x2) should be same\nas dis(x2, x1). This key feature is critical to our\nalgorithm.\nThe idea is to consider 1000 Bn/Hi\ntokens as belonging to one class, and 1000 En\ntokens belonging to another class. Distance dis is\nassigned following the equation below.\ndis(x1, x2) =\n(\n0, x1, x2 ∈sameclass\n1, x1, x2 /∈sameclass\nAs a siamese network is trained pair wise, i.e. each\nvector is paired with every other vector once, with\nrespective targets following the dis function, the\ntotal training data size increases quadratically. For\nZ members each of C classes, which is Z·C vectors\non a whole, the total number of possible pairs is\ngiven by\nNpairs =\n\u0012Z · C\n2\n\u0013\n=\n(Z · C)!\n2!(Z · C −2)!\nIn our case, Z = 1000, C = 2, which results in a to-\ntal of 19,99,000 vector pairs for our training data.\nThus, we can see a huge increment in the size of\ntraining data, thus reducing the chances of overﬁt-\nting, which is a big problem for neural networks,\nespecially when amount of data is low like in our\ncase.\n7.1\nArchitecture\nFor each of the sub-networks of our siamese net-\nwork, the layer sizes were 15-128-128-64-D(0.1)-\n32-D(0.1)-16.\nHere, 15 is the size of the input\nlayer, while 16 is the size of the output layer. 128-\n128-64-32 are the sizes of the hidden layers in\norder.\nThe layers of sizes 128 were GRU cells\n(Cho et al., 2014), while 64-32 were dense net-\nworks with feed forward cells. D(0.1) is a dropout\nlayer with rate 0.1 added in between the layers 64-\n32 and 32-16, in order to reduce overﬁtting further\nmore, which additionally reduces computing cost\nas well. The two tensors of sizes (1, 16) from each\nof the subnetworks are then concatenated, and sent\nto the output cell. This model was implemented\nusing Tensorﬂow (Abadi et al., 2016).\n7.2\nTraining\nThe whole model was trained to minimize con-\ntrastive loss which is given by the equation below\n(1 −Y)1\n2(Dw)2 + (Y)1\n2{max(0, m −(Dw)}2\nWhere Dw is deﬁned as the euclidean distance\nbetween the outputs from the two sub-networks,\nwhich mathematically can be represented as the\nequation below.\nλ denotes a very small value,\nwhich in our case was 10-6.\nDw =\np\n{Gw(X1) −Gw(X2)}2 + λ\nAn L2 weight decay (van Laarhoven, 2017) term\nwas also used in the loss to encourage the network\nto learn less noisy weights and improve general-\nization. The activation function used for the output\ncell was sigmoid, and optimizer used was rmsprop\nwith a learning rate of 0.0001 (other values kept at\ndefault). Batch size was kept at 128, and number\nof epochs was set to 10.\n7.3\nTuning\nSince our siamese model gives a sigmoid out as\nwell, thus, to convert it into a classiﬁer we had to\nﬁnd a threshold for which the accuracy is maxi-\nmum. This was done using the development data.\nA random selection of 100 tokens was done from\ntraining data (Bn/Hi) to create our support set.\nSupport set is the data with which the network\ncompares the inputs to give a similarity score. Our\nalgorithm essentially takes an input token, com-\npares it with tokens in our support set, and stores\nthe similarity scores in an array. The sum of this\narray is considered as the ﬁnal similarity score. As\nour classiﬁers output is sigmoid, the range of the\nsum of this array is (0, 100). This process was\ndone for 2x1000 tokens of our development data.\nFinally, using a similar brute force technique used\nfor the other models, the θ for which the accuracy\nwas maximum on the development was calculated\n(shown in Table 5).\n7.4\nEvaluation\nFor evaluation on our testing data, the algorithm\nthat was used is described below.\nAlgorithm 1: Siamese language tagger\nData: token to be tagged (token)\nResult: language tag\nsimilarities = [], similarity = 0\nfor support in support set do\nsimilarity = siamese(support, token);\nsimilarities.append(similarity);\nend\nif sum(similarities) ≤θ then\nreturn 0;\nelse\nreturn 1;\nend\nUsing the above algorithm, the results on our test-\ning data is shown in Table 5.\nBn-En\nModel\nθ\nAcc\nRec\nPrec\nF1\n1\n22.7\n89.45\n87.80\n90.79\n89.45\n2\n22.8\n89.55\n87.90\n90.89\n89.55\n3\n22.8\n89.85\n88.10\n91.29\n89.85\n4\n22.7\n89.45\n88.10\n90.54\n89.45\navg\n22.8\n89.58\n87.98\n90.88\n89.56\nHi-En\nModel\nθ\nAcc\nRec\nPrec\nF1\n1\n22.6\n89.50\n88.00\n90.72\n89.49\n2\n22.6\n89.35\n87.80\n90.61\n89.35\n3\n22.7\n89.65\n88.30\n90.75\n89.65\n4\n22.6\n89.50\n88.10\n90.64\n89.49\navg\n22.6\n89.50\n88.05\n90.68\n89.50\nTable 5: Results using siamese network.\nWe can see that the siamese architecture has per-\nformed quite well as expected, and have achieved\naccuracies above 89% for both Bn-En and Hi-En.\nThis is an improvement of 9% from our baseline\nsystem, i.e. it has correctly predicted ≈180 tokens\nmore. Also, unlike the other systems, here the ac-\ncuracy difference between the Bn-En and Hi-En\nhas reduced quite a bit (0.03%). This is probably\ndue to the fact that the threshold was calculated\nover 100 instances, which normalizes variances to\nquite an extent.\n8\nEnsemble Model\nAs we had created multiple systems for lan-\nguage tagging, we decided to ensemble them us-\ning simple weighted voting technique. It’s basi-\ncally bootstrap aggregating (Hothorn and Lausen,\n2003) with different weights, based on their ac-\ncuracies.\nWeight wn associated with classiﬁer\ncn can be calculated as an/Pn\ni=0 ai, where an is\nthe respective accuracies on testing data. Thus,\nPn\ni=0 wi = 1. The class having the greater weight\nis given as the ﬁnal output. Using this method, and\naveraging over all the four data sets, the results ob-\ntained are shown in Table 6.\nLang\nAcc\nRec\nPrec\nF1\nBn-En\n89.95\n88.90\n90.80\n89.95\nHi-En\n89.80\n88.50\n90.86\n89.79\nTable 6: Averaged ensemble results.\nA slight improvement in accuracy can bee seen,\nbut is still quite important as the tokens are all\nunique.\nThe accuracy scores achieved, 89.95%\nand 89.80% are very close to that of Mandal et al.\n(2018a), which reports an accuracy of 91.71% us-\ning character embedding, where ≈6.6k tokens\nwere utilized for training (6.6x times data used for\nbuilding our models). Also, recall and precision\nboth have close values, suggesting the model is not\nbiased towards a single language. This is unlike\nthe individual systems, and is quite a good aspect.\n9\nInstance Level Testing\nTo get an estimate on how our trained models will\nperform in real applications, we tested our trained\nand tuned models at instance level. This is also\nimportant as it maybe the case that the model has\nfailed to learn very commonly used tokens for a\ncertain language, for example articles like ’the’,\n’an’, etc which will result in a much reduced ac-\ncuracy when tested at instance level. The best per-\nforming classiﬁers from each of the methods were\nchosen for testing. The results obtained are shown\nbelow in Table 7.\nBn-En\nMethod\nAcc\nRec\nPrec\nF1\nConv1D\n88.15\n85.80\n90.03\n88.14\nAugment\n90.85\n89.60\n91.89\n90.85\nSiamese\n91.00\n90.99\n92.53\n90.99\nEnsemble\n92.65\n91.90\n93.29\n92.65\nHi-En\nMethod\nAcc\nRec\nPrec\nF1\nConv1D\n88.10\n85.90\n89.85\n88.09\nAugment\n90.25\n88.40\n91.79\n90.25\nSiamese\n91.05\n89.20\n92.62\n91.05\nEnsemble\n92.60\n91.20\n93.83\n92.59\nTable 7: Results on instance level testing.\nFrom the table, we can see that the all the three\nmodels have performed quite well at instance level\nas well, proving their effectiveness. Here, the im-\npact of the ensemble model is more visible as well,\nas it as achieved the highest accuracy by > 1%\nmargin.\n10\nSummary & Discussion\nAll the three methods that have been demon-\nstrated, have their own sets of advantages and dis-\nadvantages. Though the siamese network has got\nthe overall best scores, the classiﬁer is relatively\nslow, having a tag time of ≈1 sec per 5 token.\nThe Conv1D has the fastest tagging time, but is\nrelatively inferior in terms of performance. Aug-\nmentation method performs almost as good as the\nsiamese method, plus is faster, but creating two\n5i5-6300HQ (2.8 GHz) + 16GB RAM\ngenerators for augmentation is a slightly tedious\nwork. One interesting thing that can be noticed\nfrom all the experiments is that for all 4x2 datasets,\nthe results are quite close, suggesting that above\na certain amount, the weights learned by the net-\nwork are quite similar. Recall and precision values\nare also pretty close to each other, which tells us\nthe architectures doesn’t create very biased mod-\nels. Also, other than Conv1D, in the other meth-\nods (including baseline), we can see precision >\nrecall. This suggests that the models generalized\nthe Indic character patterns quite well, which re-\nsults in higher number of true positives.\n11\nConclusion & Future Work\nIn this article, we present experimental results\nfrom our ongoing work on building word level\nlanguage taggers for code-mixed data when the\namount of resources is quite low, which is actu-\nally the case many a times. We present three effec-\ntive methods for this, which are Conv1D networks,\ndata augmentation and siamese. Over all, siamese\nnetworks gave the best results with an accuracy of\n91%. Conv1D and data augmentation performed\nquite satisfactorily as well and gave accuracies of\n88% and 90% respectively. We also create an en-\nsemble classiﬁer combining these, which achieves\nan accuracy of 92.6%. In future, we would like to\nexplore more methods, especially using memory\naugmented neural networks (Santoro et al., 2016).\nUsing transfer learning (Pratt (1993), Do and Ng\n(2006)) may prove to be an effective solution as\nwell.\nReferences\nMart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng\nChen, Andy Davis, Jeffrey Dean, Matthieu Devin,\nSanjay Ghemawat, Geoffrey Irving, Michael Isard,\net al. 2016.\nTensorﬂow: a system for large-scale\nmachine learning. In OSDI, volume 16, pages 265–\n283.\nJane Bromley, Isabelle Guyon, Yann LeCun, Eduard\nS¨ackinger, and Roopak Shah. 1994. Signature ver-\niﬁcation using a” siamese” time delay neural net-\nwork. In Advances in neural information processing\nsystems, pages 737–744.\nWilliam B Cavnar, John M Trenkle, et al. 1994. N-\ngram-based text categorization.\nAnn arbor mi,\n48113(2):161–175.\nKyunghyun Cho, Bart Van Merri¨enboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014.\nLearning\nphrase representations using rnn encoder-decoder\nfor statistical machine translation.\narXiv preprint\narXiv:1406.1078.\nFranc¸ois\nChollet\net\nal.\n2015.\nKeras.\nhttps://keras.io.\nMonojit\nChoudhury,\nKalika\nBali,\nSunayana\nSitaram,\nand\nAshutosh\nBaheti.\n2017.\nCurriculum design for code-switching: Experiments with language id\nIn Proceedings of the 14th International Conference\non Natural Language Processing (ICON-2017),\npages 65–74, Kolkata, India. NLP Association of\nIndia.\nAmitava Das and Bj¨orn Gamb¨ack. 2014. Identifying\nlanguages at the word level in code-mixed indian so-\ncial media text.\nChuong B Do and Andrew Y Ng. 2006. Transfer learn-\ning for text classiﬁcation. In Advances in Neural In-\nformation Processing Systems, pages 299–306.\nMarzieh Fadaee,\nArianna Bisazza,\nand Christof\nMonz.\n2017.\nData\naugmentation\nfor\nlow-\nresource neural machine translation. arXiv preprint\narXiv:1705.00440.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997.\nLong short-term memory.\nNeural computation,\n9(8):1735–1780.\nTorsten Hothorn and Berthold Lausen. 2003. Double-\nbagging: Combining classiﬁers by bootstrap aggre-\ngation. Pattern Recognition, 36(6):1303–1309.\nHarsh Jhamtani, Suleep Kumar Bhogi, and Vaskar Ray-\nchoudhury. 2014. Word-level language identiﬁca-\ntion in bi-lingual code-switched texts. In Proceed-\nings of the 28th Paciﬁc Asia Conference on Lan-\nguage, Information and Computing.\nRie Johnson and Tong Zhang. 2014.\nEffective\nuse of word order for text categorization with\nconvolutional neural networks.\narXiv preprint\narXiv:1412.1058.\nRie Johnson and Tong Zhang. 2015. Semi-supervised\nconvolutional neural networks for text categoriza-\ntion via region embedding. In Advances in neural\ninformation processing systems, pages 919–927.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nGregory Koch, Richard Zemel, and Ruslan Salakhut-\ndinov. 2015. Siamese neural networks for one-shot\nimage recognition. In ICML Deep Learning Work-\nshop, volume 2.\nTwan van Laarhoven. 2017.\nL2 regularization ver-\nsus batch and weight normalization. arXiv preprint\narXiv:1706.05350.\nYann LeCun, Patrick Haffner, L´eon Bottou, and\nYoshua Bengio. 1999.\nObject recognition with\ngradient-based learning.\nIn Shape, contour and\ngrouping in computer vision,\npages 319–345.\nSpringer.\nSoumil Mandal, Sourya Dipta Das, and Dipankar Das.\n2018a. Language identiﬁcation of bengali-english\ncode-mixed data using character & phonetic based\nlstm models. arXiv preprint arXiv:1803.03859.\nSoumil Mandal, Sainik Kumar Mahata, and Dipankar\nDas. 2018b. Preparing bengali-english code-mixed\ncorpus for sentiment analysis of indian languages.\narXiv preprint arXiv:1803.04000.\nLili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu,\nLu Zhang, and Zhi Jin. 2016. How transferable are\nneural networks in nlp applications? arXiv preprint\narXiv:1603.06111.\nJonas Mueller and Aditya Thyagarajan. 2016. Siamese\nrecurrent architectures for learning sentence similar-\nity. In AAAI, volume 16, pages 2786–2792.\nPaul Neculoiu, Maarten Versteegh, and Mihai Rotaru.\n2016. Learning text similarity with siamese recur-\nrent networks. In Proceedings of the 1st Workshop\non Representation Learning for NLP, pages 148–\n157.\nHermann Ney, Ute Essen, and Reinhard Kneser. 1994.\nOn structuring probabilistic dependences in stochas-\ntic language modelling. Computer Speech & Lan-\nguage, 8(1):1–38.\nDong Nguyen and A Seza Do˘gru¨oz. 2013. Word level\nlanguage identiﬁcation in online multilingual com-\nmunication. In Proceedings of the 2013 Conference\non Empirical Methods in Natural Language Pro-\ncessing, pages 857–862.\nBraja Gopal Patra, Dipankar Das, and Amitava Das.\n2018.\nSentiment analysis of code-mixed indian\nlanguages: An overview of sail code-mixed shared\ntask@ icon-2017. arXiv preprint arXiv:1803.06745.\nMario Piergallini, Rouzbeh Shirvani, Gauri S Gautam,\nand Mohamed Chouikha. 2016.\nWord-level lan-\nguage identiﬁcation and predicting codeswitching\npoints in swahili-english language data. In Proceed-\nings of the Second Workshop on Computational Ap-\nproaches to Code Switching, pages 21–29.\nLorien Y Pratt. 1993. Discriminability-based transfer\nbetween neural networks. In Advances in neural in-\nformation processing systems, pages 204–211.\nAnton Ragni, Kate M Knill, Shakti P Rath, and\nMark JF Gales. 2014. Data augmentation for low\nresource languages. In Fifteenth Annual Conference\nof the International Speech Communication Associ-\nation.\nRadim Rehurek and Milan Kolkus. 2009. Language\nidentiﬁcation on the web: Extending the dictionary\nmethod.\nIn International Conference on Intelli-\ngent Text Processing and Computational Linguistics,\npages 357–368. Springer.\nShruti Rijhwani, Royal Sequiera, Monojit Choud-\nhury, Kalika Bali, and Chandra Shekhar Maddila.\n2017.\nEstimating code-switching on twitter with\na novel generalized word-level language detection\ntechnique. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), volume 1, pages 1971–\n1982.\nAdam Santoro, Sergey Bartunov, Matthew Botvinick,\nDaan Wierstra, and Timothy Lillicrap. 2016. One-\nshot learning with memory-augmented neural net-\nworks. arXiv preprint arXiv:1605.06065.\nThamar\nSolorio,\nElizabeth\nBlair,\nSuraj\nMahar-\njan,\nSteven\nBethard,\nMona\nDiab,\nMahmoud\nGhoneim, Abdelati Hawwari, Fahad AlGhamdi, Ju-\nlia Hirschberg, Alison Chang, et al. 2014. Overview\nfor the ﬁrst shared task on language identiﬁcation\nin code-switched data. In Proceedings of the First\nWorkshop on Computational Approaches to Code\nSwitching, pages 62–72.\nShikaripur N Sridhar and Kamal K Sridhar. 1980. The\nsyntax and psycholinguistics of bilingual code mix-\ning. Canadian Journal of Psychology/Revue cana-\ndienne de psychologie, 34(4):407.\nIlya Sutskever, James Martens, George Dahl, and Ge-\noffrey Hinton. 2013. On the importance of initial-\nization and momentum in deep learning. In Interna-\ntional conference on machine learning, pages 1139–\n1147.\nIlya Sutskever, James Martens, and Geoffrey E Hin-\nton. 2011.\nGenerating text with recurrent neural\nnetworks. In Proceedings of the 28th International\nConference on Machine Learning (ICML-11), pages\n1017–1024.\nOriol Vinyals, Charles Blundell, Tim Lillicrap, Daan\nWierstra, et al. 2016. Matching networks for one\nshot learning. In Advances in Neural Information\nProcessing Systems, pages 3630–3638.\nYogarshi Vyas, Spandana Gella, Jatin Sharma, Ka-\nlika Bali, and Monojit Choudhury. 2014. Pos tag-\nging of english-hindi code-mixed social media con-\ntent.\nIn Proceedings of the 2014 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 974–979.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2018-10-16",
  "updated": "2018-10-31"
}