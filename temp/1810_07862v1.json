{
  "id": "http://arxiv.org/abs/1810.07862v1",
  "title": "Applications of Deep Reinforcement Learning in Communications and Networking: A Survey",
  "authors": [
    "Nguyen Cong Luong",
    "Dinh Thai Hoang",
    "Shimin Gong",
    "Dusit Niyato",
    "Ping Wang",
    "Ying-Chang Liang",
    "Dong In Kim"
  ],
  "abstract": "This paper presents a comprehensive literature review on applications of deep\nreinforcement learning in communications and networking. Modern networks, e.g.,\nInternet of Things (IoT) and Unmanned Aerial Vehicle (UAV) networks, become\nmore decentralized and autonomous. In such networks, network entities need to\nmake decisions locally to maximize the network performance under uncertainty of\nnetwork environment. Reinforcement learning has been efficiently used to enable\nthe network entities to obtain the optimal policy including, e.g., decisions or\nactions, given their states when the state and action spaces are small.\nHowever, in complex and large-scale networks, the state and action spaces are\nusually large, and the reinforcement learning may not be able to find the\noptimal policy in reasonable time. Therefore, deep reinforcement learning, a\ncombination of reinforcement learning with deep learning, has been developed to\novercome the shortcomings. In this survey, we first give a tutorial of deep\nreinforcement learning from fundamental concepts to advanced models. Then, we\nreview deep reinforcement learning approaches proposed to address emerging\nissues in communications and networking. The issues include dynamic network\naccess, data rate control, wireless caching, data offloading, network security,\nand connectivity preservation which are all important to next generation\nnetworks such as 5G and beyond. Furthermore, we present applications of deep\nreinforcement learning for traffic routing, resource sharing, and data\ncollection. Finally, we highlight important challenges, open issues, and future\nresearch directions of applying deep reinforcement learning.",
  "text": "1\nApplications of Deep Reinforcement Learning in\nCommunications and Networking: A Survey\nNguyen Cong Luong, Dinh Thai Hoang, Member, IEEE, Shimin Gong, Member, IEEE, Dusit Niyato, Fellow,\nIEEE, Ping Wang, Senior Member, IEEE, Ying-Chang Liang, Fellow, IEEE, Dong In Kim, Senior Member, IEEE\nAbstract—This paper presents a comprehensive literature re-\nview on applications of deep reinforcement learning in com-\nmunications and networking. Modern networks, e.g., Internet\nof Things (IoT) and Unmanned Aerial Vehicle (UAV) networks,\nbecome more decentralized and autonomous. In such networks,\nnetwork entities need to make decisions locally to maximize the\nnetwork performance under uncertainty of network environment.\nReinforcement learning has been efﬁciently used to enable the\nnetwork entities to obtain the optimal policy including, e.g.,\ndecisions or actions, given their states when the state and\naction spaces are small. However, in complex and large-scale\nnetworks, the state and action spaces are usually large, and the\nreinforcement learning may not be able to ﬁnd the optimal policy\nin reasonable time. Therefore, deep reinforcement learning, a\ncombination of reinforcement learning with deep learning, has\nbeen developed to overcome the shortcomings. In this survey,\nwe ﬁrst give a tutorial of deep reinforcement learning from\nfundamental concepts to advanced models. Then, we review deep\nreinforcement learning approaches proposed to address emerging\nissues in communications and networking. The issues include\ndynamic network access, data rate control, wireless caching,\ndata ofﬂoading, network security, and connectivity preservation\nwhich are all important to next generation networks such as\n5G and beyond. Furthermore, we present applications of deep\nreinforcement learning for trafﬁc routing, resource sharing,\nand data collection. Finally, we highlight important challenges,\nopen issues, and future research directions of applying deep\nreinforcement learning.\nKeywords- Deep reinforcement learning, deep Q-learning, net-\nworking, communications, spectrum access, rate control, security,\ncaching, data ofﬂoading, data collection.\nI. INTRODUCTION\nReinforcement learning [1] is one of the most important\nresearch directions of machine learning which has signiﬁcant\nimpacts to the development of Artiﬁcial Intelligence (AI) over\nthe last 20 years. Reinforcement learning is a learning process\nin which an agent can periodically make decisions, observe\nthe results, and then automatically adjust its strategy to achieve\nthe optimal policy. However, this learning process, even though\nN. C. Luong and D. Niyato are with School of Computer Science\nand Engineering, Nanyang Technological University, Singapore. E-mails:\nclnguyen@ntu.edu.sg, dniyato@ntu.edu.sg.\nD. T. Hoang is with the Faculty of Engineering and Information Technology,\nUniversity of Technology Sydney, Australia. E-mail: hoang.dinh@uts.edu.au.\nS. Gong is with the Shenzhen Institute of Advanced Technology, Chinese\nAcademy of Sciences, Shenzhen 518055, China. E-mail: sm.gong@siat.ac.cn.\nP. Wang is with Department of Electrical Engineering & Computer Science,\nYork University, Canada. E-mail: pingw@yorku.ca.\nY.-C. Liang is with Center for Intelligent Networking and Communications\n(CINC), with University of Electronic Science and Technology of China,\nChengdu, China. E-mail: liangyc@ieee.org.\nD. I. Kim is with School of Information and Communication Engineering,\nSungkyunkwan University, Korea. Email: dikim@skku.ac.kr.\nproved to converge, takes a lot of time to reach the best policy\nas it has to explore and gain knowledge of an entire system,\nmaking it unsuitable and inapplicable to large-scale networks.\nConsequently, applications of reinforcement learning are very\nlimited in practice. Recently, deep learning [2] has been\nintroduced as a new breakthrough technique. It can overcome\nthe limitations of reinforcement learning, and thus open a new\nera for the development of reinforcement learning, namely\nDeep Reinforcement Learning (DRL). The DRL embraces\nthe advantage of Deep Neural Networks (DNNs) to train\nthe learning process, thereby improving the learning speed\nand the performance of reinforcement learning algorithms.\nAs a result, DRL has been adopted in a numerous applica-\ntions of reinforcement learning in practice such as robotics,\ncomputer vision, speech recognition, and natural language\nprocessing [2]. One of the most famous applications of DRL\nis AlphaGo [3], the ﬁrst computer program which can beat a\nhuman professional without handicaps on a full-sized 19×19\nboard.\nIn the areas of communications and networking, DRL\nhas been recently used as an emerging tool to effectively\naddress various problems and challenges. In particular, modern\nnetworks such as Internet of Things (IoT), Heterogeneous\nNetworks (HetNets), and Unmanned Aerial Vehicle (UAV)\nnetwork become more decentralized, ad-hoc, and autonomous\nin nature. Network entities such as IoT devices, mobile users,\nand UAVs need to make local and autonomous decisions, e.g.,\nspectrum access, data rate selection, transmit power control,\nand base station association, to achieve the goals of different\nnetworks including, e.g., throughput maximization and energy\nconsumption minimization. Under uncertain and stochastic\nenvironments, most of the decision-making problems can be\nmodeled by a so-called Markov Decision Process (MDP) [4].\nDynamic programming [5], [6] and other algorithms such\nas value iteration, as well as reinforcement learning tech-\nniques can be adopted to solve the MDP. However, the\nmodern networks are large-scale and complicated, and thus the\ncomputational complexity of the techniques rapidly becomes\nunmanageable. As a result, DRL has been developing to be\nan alternative solution to overcome the challenge. In general,\nthe DRL approaches provide the following advantages:\n• DRL can obtain the solution of sophisticated network\noptimizations. Thus, it enables network controllers, e.g.,\nbase stations, in modern networks to solve non-convex\nand complex problems, e.g., joint user association, com-\nputation, and transmission schedule, to achieve the op-\narXiv:1810.07862v1  [cs.NI]  18 Oct 2018\n2\ntimal solutions without complete and accurate network\ninformation.\n• DRL allows network entities to learn and build knowl-\nedge about the communication and networking environ-\nment. Thus, by using DRL, the network entities, e.g., a\nmobile user, can learn optimal policies, e.g., base station\nselection, channel selection, handover decision, caching\nand ofﬂoading decisions, without knowing channel model\nand mobility pattern.\n• DRL provides autonomous decision-making. With the\nDRL approaches, network entities can make observation\nand obtain the best policy locally with minimum or\nwithout information exchange among each other. This not\nonly reduces communication overheads but also improves\nsecurity and robustness of the networks.\n• DRL improves signiﬁcantly the learning speed, especially\nin the problems with large state and action spaces. Thus,\nin large-scale networks, e.g., IoT systems with thousands\nof devices, DRL allows network controller or IoT gate-\nways to control dynamically user association, spectrum\naccess, and transmit power for a massive number of IoT\ndevices and mobile users.\n• Several other problems in communications and network-\ning such as cyber-physical attacks, interference manage-\nment, and data ofﬂoading can be modeled as games, e.g.,\nthe non-cooperative game. DRL has been recently used\nas an efﬁcient tool to solve the games, e.g., ﬁnding the\nNash equilibrium, without the complete information.\nAlthough there are some surveys related to DRL, they do\nnot focus on communications and networking. For example,\nthe surveys of applications of DRL for computer vision and\nnatural language processing can be found in [7] and [8].\nAlso, there are surveys related to the use of only “deep\nlearning” for networking. For example, the survey of machine\nlearning for wireless networks is given in [9], but it does not\nfocus on the DRL approaches. To the best of our knowledge,\nthere is no survey speciﬁcally discussing the applications\nof DRL in communications and networking. This motivates\nus to deliver the survey with the tutorial of DRL and the\ncomprehensive literature review on the applications of DRL\nto address issues in communications and networking. For\nconvenience, the related works in this survey are classiﬁed\nbased on issues in communications and networking as shown\nin Fig. 2. The major issues include network access, data rate\ncontrol, wireless caching, data ofﬂoading, network security,\nconnectivity preservation, trafﬁc routing, and data collection.\nAlso, the percentages of DRL related works for different\nnetworks and different issues in the networks are shown in\nFigs. 1(a) and 1(b), respectively. From the ﬁgures, we observe\nthat the majority of the related works are for the cellular\nnetworks. Also, the related works to the wireless caching and\nofﬂoading have received more attention than the other issues.\nThe rest of this paper is organized as follows. Section II\npresents the introduction of reinforcement learning and dis-\ncusses DRL techniques as well as their extensions. Section III\nreviews the applications of DRL for dynamic network access\nand adaptive data rate control. Section IV discusses the ap-\nIoT 9%\nCellular\n31%\nAd-hoc\n19%\nSpace \ncommunications\n13%\nCongitive \nradio\n9%\nOthers\n28%\n(a)\nNetwork access\n13%\nRate \ncontrol\n8%\nWireless \ncaching\n19%\nComputational \noffloading\n13%\nNetwork security\n12%\nConnectivity \nprevservation \n8%\nTraffic \nrouting\n9%\nResource \nscheduling\n9%\nData collection\n9%\n(b)\nFig. 1: Percentages of related work for (a) different networks and\n(b) different issues in the networks.\nTABLE I: List of abbreviations\nAbbreviation\nDescription\nANN/APF\nArtiﬁcial Neural Network/Artiﬁcial Potential Field\nA3C\nAsynchronous Advantage Actor- Critic\nCRN\nCognitive Radio Network\nCNN\nConvolutional Neural Network\nDRL/DQL\nDeep Reinforcement Learning/Deep Q-Learning\nDNN\nDeep Neural Network\nDQN/DDQN/DRQN\nDeep\nQ-Network/Double\nDQN/Deep\nRecurrent\nQ-\nLearning\nDASH\nDynamic Adaptive Streaming over HTTP\nDoS\nDenial-of-Service\nESN\nEcho State Network\nFNN/RNN\nFeedforward Neural Network/Recurrent Neural Network\nFSMC\nFinite-State Markov Channel\nHVFT\nHigh Volume Flexible Time\nITS\nIntelligent Transportation System\nLSM/LSTM\nLiquid State Machine/Long Short-Term Memory\nMEC\nMobile Edge Computing\nMDP/POMDP\nMarkov Decision Process/Partially Observable MDP\nNFSP\nNeural Fictitious Self-Play\nNFV\nNetwork Function Virtualization\nRDPG\nRecurrent Deterministic Policy Gradient\nRCNN\nRecursive Convolutional Neural Network\nRRH/BBU\nRemote Radio Head/BaseBand Unit\nRSSI\nReceived Signal Strength Indicators\nSPD\nSequential Prisoner’s Dilemma\nSBS/BS\nSmall Base Station/Base Station\nSDN\nSoftware-Deﬁned Network\nSU/PU\nSecondary User/Primary User\nUDN/UAN\nUltra-Density Network/Underwater Acoustic Network\nUAV\nUnmanned Aerial Vehicle\nVANET/V2V\nVehicular Ad hoc Network/Vehicle-to-Vehicle\nplications of DRL for wireless caching and data ofﬂoading.\nSection V presents DRL related works for network security\nand connectivity preservation. Section VI considers how to use\nDRL to deal with other issues in communications and network-\ning. Important challenges, open issues, and future research\n3\nApplications of deep reinforcement learning \nfor communications and networking\nNetwork access and rate \ncontrol \nMiscellaneous issues\nSecurity and \nconnectivity preservation\nCaching and offloading \n \nNetwork\n access\nAdaptive \nrate control\nProactive \ncaching\nData  \noffloading\nNetwork \nsecurity\nConnectivity\npreservation\nTraffic \nrouting\nResource \nscheduling\nData \ncollection\nFig. 2: A taxonomy of the applications of deep reinforcement learning for communications and networking.\ndirections are outlined in Section VII. Section VIII concludes\nthe paper. The list of abbreviations commonly appeared in\nthis paper is given in Table I. Note that DRL consists of two\ndifferent algorithms which are Deep Q-Learning (DQL) and\npolicy gradients [10]. In particular, DQL is mostly used for\nthe DRL related works. Therefore, in the rest of the paper, we\nuse “DRL” and “DQL” interchangeably to refer to the DRL\nalgorithms.\nII. DEEP REINFORCEMENT LEARNING: AN OVERVIEW\nIn this section, we ﬁrst present fundamental knowledge of\nMarkov decision processes, reinforcement learning, and deep\nlearning techniques which are important branches of machine\nlearning theory. We then discuss DRL technique that can\ncapitalize on the capability of the deep learning to improve\nefﬁciency and performance in terms of the learning rate for\nreinforcement learning algorithms. Afterward, advanced DRL\nmodels and their extensions are reviewed.\nA. Markov Decision Processes\nMDP [4] is a discrete time stochastic control process. MDP\nprovides a mathematical framework for modeling decision-\nmaking problems in which outcomes are partly random and\nunder control of a decision maker or an agent. MDPs are useful\nfor studying optimization problems which can be solved by\ndynamic programming and reinforcement learning techniques.\nTypically, an MDP is deﬁned by a tuple (S, A, p, r) where\nS is a ﬁnite set of states, A is a ﬁnite set of actions, p is\na transition probability from state s to state s′ after action\na is executed, and r is the immediate reward obtained after\naction a is performed. We denote π as a “policy” which is\na mapping from a state to an action. The goal of an MDP\nis to ﬁnd an optimal policy to maximize the reward function.\nAn MDP can be ﬁnite or inﬁnite time horizon. For an inﬁnite\ntime horizon MDP, we aim to ﬁnd an optimal policy π∗to\nmaximize the expected total reward deﬁned by\n∞\nP\nt=0\nγrt(st, at),\nwhere at = π∗(st), and γ ∈[0, 1] is the discount factor.\n1) Partially Observable Markov Decision Process:\nIn\nMDPs, we assume that the system state is fully observ-\nable by the agent. However, in many cases, the agent only\ncan observe a part of the system state, and thus Partially\nObservable Markov Decision Processes (POMDPs) [11] can\nbe used to model the decision-making problems. A typical\nPOMDP model is deﬁned by a 6-tuple (S, A, p, r, Ω, O),\nwhere S, A, p, r are deﬁned the same as in the MDP model, Ω\nand O are deﬁned as the set of observations and observation\nprobabilities, respectively. At each time step, the system is at\nstate s ∈S. Then, the agent takes an action a ∈A and the\nsystem transits to state s′ ∈S. At the same time, the agent\nhas a new observation o ∈Ωwith probability O(o|s, a, s′).\nFinally, the agent receives an immediate reward r that is\nequal to r(s, a) in the MDP. Similar to the MDP model, the\nagent in POMDP also aims to ﬁnd the optimal policy π∗in\norder to maximize its expected long-term discounted reward\n∞\nP\nt=0\nγrt(st, π∗(st)).\n2) Markov Games: In game theory, a Markov game, or\na stochastic game [12], is a dynamic game with proba-\nbilistic transitions played by multiple players, i.e., agents.\nA typical Markov game model is deﬁned by a tuple\n(I, S, {Ai}i∈I, p, {ri}i∈I), where\n• I ≜{1, . . . , i, . . . , I} is a set of agents,\n• S ≜{S1, . . . , Si, . . . , SI} is the global state space of the\nall agents with Si being the state space of agent i,\n• {Ai}i∈I are sets of action spaces of the agents with Ai\nbeing the action space of agent i,\n• p ≜S×A1×· · ·×AI →[0, 1] is the transition probability\nfunction of the system.\n• {ri}i∈I are payoff functions of the agents with\nri ≜S × A1 × · · · × AI →R, i.e., the payoff of agent i\nobtained after all actions of the agents are executed.\nIn a Markov game, the agents start at some initial state\ns0 ∈S. After observing the current state, all the agents\nsimultaneously select their actions a = {a1, . . . , aI} and they\nwill receive their corresponding rewards together with their\nown new observations. At the same time, the system will\ntransit to a new state s′ ∈S with probability p(s′|s, a). The\n4\nprocedure is repeated at the new state and continues for a ﬁnite\nor inﬁnite number of stages. In this game, all the agents try\nto ﬁnd their optimal policies to maximize their own expected\nlong-term average rewards, i.e.,\n∞\nP\nt=0\nγiri\nt(st, π∗\ni (st)), ∀i. The\nset of all optimal policies of this game, i.e., {π∗\n1, . . . , π∗\nI} is\nknown to be the equilibrium of this game. If there is a ﬁnite\nnumber of players and the sets of states and actions are ﬁnite,\nthen the Markov game always has a Nash equilibrium [13]\nunder a ﬁnite number of stages. The same is true for Markov\ngames with inﬁnite stages, but the total payoff of agents is the\ndiscounted sum [13].\nB. Reinforcement Learning\nReinforcement learning, an important branch of machine\nlearning, is an effective tool and widely used in the literature\nto address MDPs [1]. In a reinforcement learning process, an\nagent can learn its optimal policy through interaction with its\nenvironment. In particular, the agent ﬁrst observes its current\nstate, and then takes an action, and receives its immediate\nreward together with its new state as illustrated in Fig. 3(a).\nThe observed information, i.e., the immediate reward and new\nstate, is used to adjust the agent’s policy, and this process\nwill be repeated until the agent’s policy approaches to the\noptimal policy. In reinforcement learning, Q-learning is the\nmost effective method and widely used in the literature. In the\nfollowing, we will discuss the Q-learning algorithm and its\nextensions for advanced MDP models.\n1) Q-Learning Algorithm: In an MDP, we aim to ﬁnd an\noptimal policy π∗: S →A for the agent to minimize the\noverall cost for the system. Accordingly, we ﬁrst deﬁne value\nfunction Vπ : S →R that represents the expected value\nobtained by following policy π from each state s ∈S. The\nvalue function V for policy π quantiﬁes the goodness of the\npolicy through an inﬁnite horizon and discounted MDP that\ncan be expressed as follows:\nVπ(s) = Eπ\nh ∞\nX\nt=0\nγrt(st, at)|s0 = s\ni\n= Eπ\nh\nrt(st, at) + γVπ(st+1)|s0 = s\ni\n.\n(1)\nSince we aim to ﬁnd the optimal policy π∗, an optimal ac-\ntion at each state can be found through the optimal value func-\ntion expressed by V∗(s) = max\nat\nn\nEπ\n\u0002\nrt(st, at)+γVπ(st+1)\n\u0003o\n.\nIf we denote Q∗(s, a) ≜rt(st, at)+γEπ\n\u0002\nVπ(st+1)\n\u0003\nas the\noptimal Q-function for all state-action pairs, then the optimal\nvalue function can be written by V∗(s) = max\na\n\b\nQ∗(s, a)\n\t\n.\nNow, the problem is reduced to ﬁnd optimal values of Q-\nfunction, i.e., Q∗(s, a), for all state-action pairs, and this\ncan be done through iterative processes. In particular, the Q-\nfunction is updated according to the following rule:\nQt+1(s, a) =Qt(s, a)+\nαt\nh\nrt(s, a) + γ max\na′ Qt(s, a′) −Qt(s, a)\ni\n.\n(2)\nThe core idea behind this update is to ﬁnd the Temporal\nDifference (TD) between the predicted Q-value, i.e., rt(s, a)+\nγmax\na′ Qt(s, a′) and its current value, i.e., Qt(s, a). In (2),\nthe learning rate αt is used to determine the impact of new\ninformation to the existing Q-value. The learning rate can\nbe chosen to be a constant, or it can be adjusted dynami-\ncally during the learning process. However, it must satisfy\nAssumption 1 to guarantee the convergence for the Q-learning\nalgorithm.\nAssumption 1. The step size αt is deterministic, nonnegative\nand satisﬁes the following conditions: αt ∈[0, 1],\n∞\nP\nt=0\nαt = ∞,\nand\n∞\nP\nt=0\n(αt)2 < ∞.\nThe step size adaptation αt = 1\nt is one of the most common\nexamples used in reinforcement learning. More discussions\nfor selecting an appropriate step size can be found in [14].\nThe details of the Q-learning algorithm are then provided in\nAlgorithm 1.\nAlgorithm 1 The Q-learning algorithm\nInput: For each state-action pair (s, a), initialize the table\nentry Q(s, a) arbitrarily, e.g., to zero. Observe the current\nstate s, initialize a value for the learning rate α and the\ndiscount factor γ.\nfor t := 1 to T do\nFrom the current state-action pair (s, a), execute action\na and obtain the immediate reward r and a new state s′.\nSelect an action a′ based on the state s′ and then update\nthe table entry for Q(s, a) as follows:\nQt+1(s, a) ←Qt(s, a) + αt\nh\nrt(s, a)+\nγ max\na′ Qt(s′, a′) −Qt(s, a)\ni\n(3)\nReplace s ←s′.\nend for\nOutput: π∗(s) = arg maxa Q∗(s, a).\nOnce either all Q-values converge or a certain number\nof iterations is reached, the algorithm will terminate. The\nalgorithm then yields the optimal policy indicating an action\nto be taken at each state such that Q∗(s, a) is maximized for\nall states in the state space, i.e., π∗(s) = arg max\na Q∗(s, a).\nUnder the assumption of the step size (i.e., Assumption 1), it\nis proved in [15] that the Q-learning algorithm converges to\nthe optimum action-values with probability one.\n2) SARSA: An Online Q-Learning Algorithm: Although the\nQ-learning algorithm can ﬁnd the optimal policy for the agent\nwithout requiring knowledge about the environment, this al-\ngorithm works in an ofﬂine fashion. In particular, Algorithm 1\ncan obtain the optimal policy only after all Q-values converge.\nTherefore, this section presents an alternative online learning\nalgorithm, i.e., the SARSA algorithm, which allows the agent\nto approach the optimal policy in an online fashion.\nDifferent from the Q-learning algorithm, the SARSA al-\ngorithm is an online algorithm which allows the agent to\n5\nEnvironment\nAgent\nAction a\nImmediate reward r\nObserved state s\nController \npolicy π \nInput layer\nHidden layer\nOutput layer\nOutputs\nInputs\nWeights θ \nEnvironment\nAgent\nAction a\nImmediate reward r\nObserved state s\nController policy π \nfeatures\nWeights θ \n(a)\n(b)\n(c)\nFig. 3: (a) Reinforcement learning, (b) Artiﬁcial neural network, and (c) Deep Q-learning.\nchoose optimal actions at each time step in a real-time fashion\nwithout waiting until the algorithm converges. In the Q-\nlearning algorithm, the policy is updated according to the\nmaximum reward of available actions regardless of which\npolicy is applied, i.e., an off-policy method. In contrast, the\nSARSA algorithm interacts with the environment and updates\nthe policy directly from the actions taken, i.e., an on-policy\nmethod. Note that the SARSA algorithm updates Q-values\nfrom the quintuple Q(s, a, r, s′, a′).\n3) Q-Learning for Markov Games: To apply Q-learning\nalgorithm to the Markov game context, we ﬁrst deﬁne the\nQ-function for agent i by Qi(s, ai, a−i), where a−i\n≜\n{a1, . . . , ai−1, ai+1, . . . , aI} denotes the set of actions of all\nagents except i. Then, the Nash Q-function of agent i is\ndeﬁned by:\nQ∗\ni (s, ai, a−i) = ri(s, ai, a−i)+\nβ\nX\ns′∈S\np(s′|s, ai, a−i)Vi(s′, π∗\n1, . . . , π∗\nI),\n(4)\nwhere (π∗\n1, . . . , π∗\nI) is the joint Nash equilibrium strategy,\nri(s, ai, a−i) is agent i’s immediate reward in state s under\nthe joint action (ai, a−i), and Vi(s′, π∗\n1, . . . , π∗\nI) is the total\ndiscounted reward over an inﬁnite time horizon starting from\nstate s′ given that all the agents follow the equilibrium\nstrategies.\nIn [13], the authors propose a multi-agent Q-learning algo-\nrithm for general-sum Markov games which allows the agents\nto perform updates based on assuming Nash equilibrium\nbehavior over the current Q-values. In particular, agent i will\nlearn its Q-values by forming an arbitrary guess from starting\ntime of the game. At each time step t, agent i observes the\ncurrent state and takes an action ai. Then, it observes its\nimmediate reward ri, actions taken by others a−i, others’\nimmediate rewards, and the new system state s′. After that,\nagent i calculates a Nash equilibrium (π1(s′), . . . , πI(s′)) for\nthe state game (Qt\n1(s′), . . . , Qt\nI(s′)), and updates its Q-values\naccording to:\nQt+1\ni\n(s, ai, a−i) = (1−αt)Qt\ni(s, ai, a−i)+αt[ri\nt +γN i\nt (s′)],\n(5)\nwhere αt ∈(0, 1) is the learning rate and N i\nt (s′) ≜Qt\ni(s′)×\nπ1(s′) × · · · × πI(s′).\nIn order to calculate the Nash equilibrium, agent i needs to\nknow (Qt\n1(s′), . . . , Qt\nI(s′)). However, the information about\nother agents’ Q-values is not given, and thus agent i must learn\nthis information too. To do so, agent i will set estimations\nabout others’ Q-values at the beginning of the game, e.g.,\nQj\n0(s, ai, a−i) = 0, ∀j, s. As the game proceeds, agent i\nobserves other agents’ immediate rewards and previous ac-\ntions. That information can then be used to update agent i’s\nconjectures on other agents’ Q-functions. Agent i updates its\nbeliefs about agent j’s Q-function, according to the same\nupdating rule in (5). Then, the authors prove that under\nsome highly restrictive assumptions on the form of the state\ngames during learning, the proposed multi-agent Q-learning\nalgorithm is guaranteed to be converged.\nC. Deep Learning\nDeep learning [2] is composed of a set of algorithms and\ntechniques that attempt to ﬁnd important features of data and\nto model its high-level abstractions. The main goal of deep\nlearning is to avoid manual description of a data structure (like\nhand-written features) by automatic learning from the data. Its\nname refers to the fact that typically any neural network with\ntwo or more hidden layers is called DNN. Most deep learning\nmodels are based on an Artiﬁcial Neural Network (ANN), even\nthough they can also include propositional formulas or latent\nvariables organized layer-wise in deep generative models such\nas the nodes in Deep Belief Networks and Deep Boltzmann\nMachines.\nAn ANN is a computational nonlinear model based on the\nneural structure of the brain that is able to learn to perform\ntasks such as classiﬁcation, prediction, decision-making, and\nvisualization. An ANN consists of artiﬁcial neurons and is\norganized into three interconnected layers: input, hidden, and\noutput as illustrated in Fig. 3(b). The input layer contains input\nneurons that send information to the hidden layer. The hidden\nlayer sends data to the output layer. Every neuron has weighted\ninputs (synapses), an activation function (deﬁnes the output\ngiven an input), and one output. Synapses are the adjustable\nparameters that convert a neural network to a parameterized\nsystem.\nDuring the training phase, ANNs use backpropagation as an\neffective learning algorithm to compute quickly a gradient de-\nscent with respect to the weights. Backpropagation is a special\ncase of automatic differentiation. In the context of learning,\nbackpropagation is commonly used by the gradient descent\n6\noptimization algorithm to adjust the weights of neurons by\ncalculating the gradient of the loss function. This technique is\nalso sometimes called backward propagation of errors, because\nthe error is calculated at the output and distributed back\nthrough the network layers.\nA DNN is deﬁned as an ANN with multiple hidden layers.\nThere are two typical DNN models, i.e., Feedforward Neural\nNetwork (FNN) and Recurrent Neural Network (RNN). In the\nFNN, the information moves in only one direction, i.e., from\nthe input nodes, through the hidden nodes and to the output\nnodes, and there are no cycles or loops in the network as shown\nin Fig. 4. In FNNs, Convolutional Neural Network (CNN) is\nthe most well known model with a wide range of applications\nespecially in image and speech recognition. The CNN contains\none or more convolutional layers, pooling or fully connected,\nand uses a variation of multilayer perceptrons discussed above.\nConvolutional layers use a convolution operation to the input\npassing the result to the next layer. This operation allows the\nnetwork to be deeper with much fewer parameters.\nRecurrent Neural Network (RNN)\nFeed-Forward Neural Network (FNN)\nFig. 4: RNN vs CNN.\nUnlike FNNs, the RNN is a variant of a recursive artiﬁcial\nneural network in which connections between neurons make\ndirected cycles. It means that an output depends not only\non its immediate inputs, but also on the previous further\nstep’s neuron state. The RNNs are designed to utilize sequen-\ntial data, when the current step has some relation with the\nprevious steps. This makes the RNNs ideal for applications\nwith a time component, e.g., time-series data, and natural\nlanguage processing. However, all RNNs have feedback loops\nin the recurrent layer. This lets RNNs maintain information\nin memory over time. Nevertheless, it can be difﬁcult to train\nstandard RNNs to solve problems that require learning long-\nterm temporal dependencies. The reason is that the gradient\nof the loss function decays exponentially with time, which is\ncalled the vanishing gradient problem. Thus, Long Short-Term\nMemory (LSTM) is often used in RNNs to address this issue.\nThe LSTM is designed to model temporal sequences and their\nlong-range dependencies are more accurate than conventional\nRNNs. The LSTM does not use an activation function within\nits recurrent components, the stored values are not modiﬁed,\nand the gradient does not tend to vanish during training.\nUsually, LSTM units are implemented in “blocks” with several\nunits. These blocks have three or four “gates”, e.g., input gate,\nforget gate, output gate, that control information ﬂow drawing\non the logistic function.\nD. Deep Q-Learning\nThe Q-learning algorithm can efﬁciently obtain an optimal\npolicy when the state space and action space are small.\nHowever, in practice, with complicated system models, these\nspaces are usually large. As a result, the Q-learning algorithm\nmay not be able to ﬁnd the optimal policy. Thus, Deep Q-\nLearning (DQL) algorithm is introduced to overcome this\nshortcoming. Intuitively, the DQL algorithm implements a\nDeep Q-Network (DQN), i.e., a DNN, instead of the Q-table to\nderive an approximate value of Q∗(s, a) as shown in Fig. 3(c).\nAs stated in [16], the average reward obtained by rein-\nforcement learning algorithms may not be stable or even\ndiverge when a nonlinear function approximator is used.\nThis stems from the fact that a small change of Q-values\nmay greatly affect the policy. Thus, the data distribution and\nthe correlations between the Q-values and the target values\nR + γ maxa′ Q(s′, a′) are varied. To address this issue, two\nmechanisms, i.e., experience replay and target Q-network, can\nbe used.\n• Experience replay mechanism: The algorithm ﬁrst initial-\nizes a replay memory D, i.e., the memory pool, with\ntransitions (st, at, rt, st+1), i.e., experiences, generated\nrandomly, e.g., through using ϵ-greedy policy. Then, the\nalgorithm randomly selects samples, i.e., minibatches,\nof transitions from D to train the DNN. The Q-values\nobtained by the trained DNN will be used to obtain new\nexperiences, i.e., transitions, and these experiences will\nbe then stored in the memory pool D. This mechanism\nallows the DNN trained more efﬁciently by using both\nold and new experiences. In addition, by using the ex-\nperience replay, the transitions are more independent and\nidentically distributed, and thus the correlations between\nobservations can be removed.\n• Fixed target Q-network: In the training process, the Q-\nvalue will be shifted. Thus, the value estimations can\nbe out of control if a constantly shifting set of values\nis used to update the Q-network. This leads to the\ndestabilization of the algorithm. To address this issue,\nthe target Q-network is used to update frequently but\nslowly the primary Q-networks’ values. In this way,\nthe correlations between the target and estimated Q-\nvalues are signiﬁcantly reduced, thereby stabilizing the\nalgorithm.\nThe DQL algorithm with experience replay and ﬁxed target\nQ-network is presented in Algorithm 2. DQL inherits and\npromotes advantages of both reinforcement and deep learning\ntechniques, and thus it has a wide range of applications in\npractice such as game development [3], transportation [17],\nand robotics [18].\nE. Advanced Deep Q-Learning Models\n1) Double Deep Q-Learning: In some stochastic envi-\nronments, the Q-learning algorithm performs poorly due to\nthe large over-estimations of action values [19]. These over-\nestimations result from a positive bias that is introduced be-\ncause Q-learning uses the maximum action value as an approx-\nimation for the maximum expected action value as shown in\n7\nAlgorithm 2 The DQL Algorithm with Experience Replay\nand Fixed Target Q-Network\n1: Initialize replay memory D.\n2: Initialize the Q-network Q with random weights θ.\n3: Initialize the target Q-network ˆQ with random weights θ′.\n4: for episode=1 to T do\n5:\nWith probability ϵ select a random action at, otherwise\nselect at = arg max Q∗(st, at, θ).\n6:\nPerform action at and observe immediate reward rt and\nnext state st+1.\n7:\nStore transition (st, at, rt, st+1) in D.\n8:\nSelect randomly samples c(sj, aj, rj, sj+1) from D.\n9:\nThe weights of the neural network then are optimized\nby using stochastic gradient descent with respect to the\nnetwork parameter θ to minimize the loss:\nh\nrj + γ max\naj+1\nˆQ(sj+1, aj+1; θ′) −Q(sj, aj; θ)\ni2\n. (6)\n10:\nReset ˆQ = Q after every a ﬁxed number of steps.\n11: end for\nEq. (3). The reason is that the same samples are used to decide\nwhich action is the best, i.e., with highest expected reward, and\nthe same samples are also used to estimate that action-value.\nThus, to overcome the over-estimation problem of the Q-\nlearning algorithm, the authors in [20] introduce a solution us-\ning two Q-value functions, i.e., Q1 and Q2, to simultaneously\nselect and evaluate action values through the loss function as\nfollows:\nh\nrj + γQ2\n\u0010\nsj+1, arg max\naj+1Q1\n\u0000sj+1, aj+1; θ1\n\u0001\n; θ2\n\u0011\n−\nQ1(sj, aj; θ1)\ni2\n.\nNote that the selection of an action, in the arg max, is\nstill due to the online weights θ1. This means that, as in Q-\nlearning, we are still estimating the value of the greedy policy\naccording to the current values, as deﬁned by θ1. However,\nthe second set of weights θ2 is used to evaluate fairly the\nvalue of this policy. This second set of weights can be updated\nsymmetrically by switching the roles of θ1 and θ2. Inspired\nby this idea, the authors in [20] then develop Double Deep\nQ-Learning (DDQL) model [21] using a Double Deep Q-\nNetwork (DDQN) with the loss function updated as follows:\nh\nrj + γ ˆQ\n\u0010\nsj+1, arg max\naj+1 Q\n\u0000sj+1, aj+1; θ\n\u0001\n; θ′\u0011\n−Q(sj, aj; θ)\ni2\n.\n(7)\nUnlike double Q-learning, the weights of the second net-\nwork θ2 are replaced with the weights of the target networks\nθ′ for the evaluation of the current greedy policy as shown\nin Eq. (7). The update to the target network stays unchanged\nfrom DQN, and remains a periodic copy of the online network.\nDue to the effectiveness of DDQL, there are some applications\nof DDQL introduced recently to address dynamic spectrum\naccess problems in multichannel wireless networks [22] and\nresource allocation in heterogeneous networks [23].\n2) Deep Q-Learning with Prioritized Experience Replay:\nExperience replay mechanism allows the reinforcement learn-\ning agent to remember and reuse experiences, i.e., transitions,\nfrom the past. In particular, transitions are uniformly sampled\nfrom the replay memory D. However, this approach simply\nreplays transitions at the same frequency as that the agent\nwas originally experienced, regardless of their signiﬁcance.\nTherefore, the authors in [24] develop a framework for prior-\nitizing experiences, so as to replay important transitions more\nfrequently, and therefore learn more efﬁciently. Ideally, we\nwant to sample more frequently those transitions from which\nthere is much to learn. As a proxy for learning potential, the\nproposed Prioritized Experience Replay (PER) [24] samples\ntransitions with probability pt relative to the last encountered\nabsolute error deﬁned as follows:\npt ∝\n\f\f\frj + γ max\na′\nˆQ(sj+1, a′; θ′) −Q(sj, aj; θ)\n\f\f\f\nω\n,\n(8)\nwhere ω is a hyper-parameter that determines the shape of the\ndistribution. New transitions are inserted into the replay buffer\nwith maximum priority, providing a bias towards recent tran-\nsitions. Note that stochastic transitions may also be favoured,\neven when there is little left to learn about them. Through real\nexperiments on many Atari games, the authors demonstrate\nthat DQL with PER outperforms DQL with uniform replay on\n41 out of 49 games. However, this solution is only appropriate\nto implement when we can ﬁnd and deﬁne the important\nexperiences in the replay memory D.\n3) Dueling Deep Q-Learning: The Q-values, i.e., Q(s, a),\nused in the Q-learning algorithm, i.e., Algorithm 1, are to\nexpress how good it is to take a certain action at a given state.\nThe value of an action a at a given state s can actually be\ndecomposed into two fundamental values. The ﬁrst value is\nthe state-value function, i.e., V (s), to estimate the importance\nof being in a particular state s. The second value is the\naction-value function, i.e., A (a), to estimate the importance of\nselecting an action a compared with other actions. As a result,\nthe Q-value function can be expressed by two fundamental\nvalue functions as follows: Q(s, a) = V (s) + A (a).\nStemming from the fact that in many MDPs, it is unnec-\nessary to estimate both values, i.e., action and state values of\nQ-function Q(s, a), at the same time. For example, in many\nracing games, moving left or right matters if and only if the\nagent meets the obstacles or enemies. Inspired by this idea, the\nauthors in [25] introduce an idea of using two streams, i.e.,\ntwo sequences, of fully connected layers instead of using a\nsingle sequence with fully connected layers for the DQN. The\ntwo streams are constructed such that they are able to provide\nseparate estimations on the action and state value functions,\ni.e., V (s) and A (a). Finally, the two streams are combined\nto generate a single output Q(s, a) as follows:\nQ(s, a; α, β) = V (s; β) +\n\u0010\nA (s, a; α) −\nP\na′ A (s, a′; α)\n|A|\n\u0011\n,\n(9)\nwhere β and α are the parameters of the two streams V (s; β)\nand A (s, a′; α), respectively. Here, |A| is the total number\nof actions in the action space A. Then, the loss function is\nderived in the similar way to (6) as follows:\n8\nh\nrj\n+\nγmax\naj+1\nˆQ(sj+1, aj+1; α′, β′)\n−\nQ(sj, aj; α, β)\ni2\n.\nThrough the simulation, the authors show that the proposed\ndueling DQN can outperform DDQN [21] in 50 out of\n57 learned Atari games. However, the proposed dueling\narchitecture only clearly beneﬁts for MDPs with large action\nspaces. For small state spaces, the performance of dueling\nDQL is even not as good as that of double DQL as shown in\nsimulation results in [25].\n4) Asynchronous Multi-step Deep Q-Learning: Most of the\nQ-learning methods such as DQL and dueling DQL rely\non the experience replay method. However, such kind of\nmethod has several drawbacks. For example, it uses more\nmemory and computation resources per real interaction, and\nit requires off-policy learning algorithms that can update from\ndata generated by an older policy. This limits the applications\nof DQL. Therefore, the authors in [26] introduce a method\nusing multiple agents to train the DNN in parallel. In partic-\nular, the authors propose a training procedure which utilizes\nasynchronous gradient decent updates from multiple agents at\nonce. Instead of training one single agent that interacts with\nits environment, multiple agents are interacting with their own\nversion of the environment simultaneously. After a certain\namount of timesteps, accumulated gradient updates from an\nagent are applied to a global model, i.e., the DNN. These\nupdates are asynchronous and lock free. In addition, to tradeoff\nbetween bias and variance in the policy gradient, the authors\nadopt n-step updates method [1] to update the reward function.\nIn particular, the truncated n-step reward function can be\ndeﬁned by r(n)\nt\n=\nn−1\nP\nk=0\nγ(k)rt+k+1. Thus, the alternative loss\nfor each agent will be derived by:\nh\nr(n)\nj\n+ γ(n)\nj\nmax\na′\nˆQ(sj+n, a′; θ′) −Q(sj, aj; θ)\ni2\n.\n(10)\nThe effects of training speed and quality of the proposed\nasynchronous DQL with multi-step learning are analyzed\nfor various reinforcement learning methods, e.g., 1-step Q-\nlearning, 1-step SARSA, and n-step Q-learning. They show\nthat asynchronous updates have a stabilizing effect on policy\nand value updates. Also, the proposed method outperforms the\ncurrent state-of-the-art algorithms on the Atari games while\ntraining for half of the time on a single multi-core CPU\ninstead of a GPU. As a result, some recent applications of\nasynchronous DQL have been developed for handover control\nproblems in wireless systems [27]\n5) Distributional Deep Q-learning:\nAll aforementioned\nmethods use the Bellman equation to approximate the expected\nvalue of future rewards. However, if the environment is\nstochastic in nature and the future rewards follow multimodal\ndistribution, choosing actions based on expected value may\nnot lead to the optimal outcome. For example, we know\nthat the expected transmission time of a packet in a wireless\nnetwork is 20 minutes. However, this information may not be\nso meaningful because it may overestimate the transmission\ntime most of the time. For example, the expected transmission\ntime is calculated based on the normal transmissions (without\ncollisions) and the interference transmissions (with collisions).\nAlthough the interference transmissions are very rare to hap-\npen, but it takes a lot of time. Then, the estimation about the\nexpected transmission is overestimated most of the time. This\nmakes estimations not useful for the DQL algorithms.\nThus, the authors in [28] introduce a solution using distribu-\ntional reinforcement learning to update Q-value function based\non its distribution rather than its expectation. In particular,\nlet Z(s, a) be the return obtained by starting from state s,\nexecuting action a, and following the current policy, then\nQ(s, a) = E[Z(s, a)]. Here, Z represents the distribution of\nfuture rewards, which is no longer a scalar quantity like Q-\nvalues. Then we obtain the distributional version of Bellman\nequation as follows: Z(s, a) = r + γZ(s′, a′). For example,\nif we use the DQN and extract an experience (s, a, r, s′) from\nthe replay buffer, then the sample of the target distribution is\nZ(s, a) = r + γZ(s′, a∗) with a∗= arg max\na′ Q(s, a′). Al-\nthough the proposed distributional deep Q-learning is demon-\nstrated to outperform the conventional DQL [16] on many\nAtari 2600 Games (45 out of 57 games), its performance relies\nmuch on the distribution function Z. If Z is well deﬁned, the\nperformance of distributional deep Q-learning is much more\nsigniﬁcant than that of the DQL. Otherwise, its performance\nis even worse than that of the DQL.\n6)\nDeep Q-learning with Noisy Nets: In [29], the authors\nintroduce Noisy Net, a type of neural network whose bias\nand weights are iteratively perturbed during training by a\nparametric function of the noise. This network basically adds\nthe Gaussian noise to the last (fully-connected) layers of the\nnetwork. The parameters of this noise can be adjusted by the\nmodel during training, which allows the agent to decide when\nand in what proportion it wants to introduce the uncertainty to\nits weights. In particular, to implement the noisy network, we\nﬁrst replace the ϵ-greedy policy by a randomized action-value\nfunction. Then, the fully connected layers of the value network\nare parameterized as a noisy network, where the parameters\nare drawn from the noisy network parameter distribution\nafter every replay step. For replay, the current noisy network\nparameter sample is held ﬁxed across the batch. Since the\nDQL takes one step of optimization for every action step, the\nnoisy network parameters are re-sampled before every action.\nAfter that, the loss function can be updated as follows:\nL = E\nh\nE(s,a,r,s′)∼D\n\u0002\nr+γ max\na′∈A\nˆQ(s′, a′, ϵ′; θ′)−Q(s, a, ϵ; θ)\n\u0003i\n,\n(11)\nwhere the outer and inner expectations are with respect to\ndistributions of the noise variables ϵ and ϵ′ for the noisy value\nfunctions ˆQ(s′, a′, ϵ′; θ′) and Q(s, a, ϵ; θ), respectively.\nThrough experimental results, the authors demonstrate that\nby adding the Gaussian noise layer to the DNN, the per-\nformance of conventional DQL [16], dueling DQL [25], and\nasynchronous DQL [26] can be signiﬁcantly improved for a\nwide range of Atari games. However, the impact of noise to\nthe performance of the deep DQL algorithms is still under\ndebating in the literature, and thus analysis on the impact of\nnoise layer requires further investigations.\n7) Rainbow Deep Q-learning: In [30], the authors propose\na solution which integrates all advantages of seven aforemen-\ntioned solutions (including DQL) into a single learning agent,\ncalled Rainbow DQL. In particular, this algorithm ﬁrst deﬁnes\n9\nTABLE II: Performance comparison among DQL algorithms\nDQL Algorithms\nNo Operations\nHuman Starts\nPublish\nDeveloper\nDQL\n79%\n68%\nNature 2015 [16]\nGoogle DeepMind\nDDQL\n117%\n110%\nAAAI 2016 [21]\nGoogle DeepMind\nPrioritized DDQL\n140%\n128%\nICLR 2015 [24]\nGoogle DeepMind\nDueling DDQL\n151%\n117%\nICML 2016 [25]\nGoogle DeepMind\nAsynchronous DQL\n-\n116%\nICML 2016 [26]\nGoogle DeepMind\nDistributional DQL\n164%\n125%\nICML 2017 [28]\nGoogle DeepMind\nNoisy Nets DQL\n118%\n102%\nICLR 2018 [29]\nGoogle DeepMind\nRainbow\n223%\n153%\nAAAI 2018 [30]\nGoogle DeepMind\nthe loss function based on the asynchronous multi-step and\ndistributional DQL. Then, the authors combine the multi-step\ndistributional loss with double Q-learning by using the greedy\naction in st+n selected according to the Q-network as the\nbootstrap action a∗\nt+n, and evaluate the action by using the\ntarget network.\nIn standard proportional prioritized replay [24] technique,\nthe absolute TD-error is used to prioritize the transitions.\nHere, TD-error at a time slot is the error in the estimate\nmade at the time slot. However, in the proposed Rainbow\nDQL algorithm, all distributional Rainbow variants prioritize\ntransitions by the Kullbeck-Leibler (KL) loss because this\nloss may be more robust to noisy stochastic environment.\nAlternatively, the dueling architecture of DNNs is presented\nin [25]. Finally, the Noisy Net layer [30] is used to replace\nall linear layers in order to reduce the number of independent\nnoise variables. Through simulation, the authors show that this\nis the most advanced technique which outperforms almost all\ncurrent DQL algorithms in the literature over 57 Atari 2600\ngames.\nIn Table II, we summarize the DQL algorithms and their\nperformance under the parameter settings used in [30]. As\nobserved in Table II, all of the DQL algorithms have been\ndeveloped by Google DeepMind based on the original work\nin [16]. So far, through experimental results on Atari 2600\ngames, the Rainbow DQL presents very impressive results over\nall other DQL algorithms. However, more experiments need\nto be further conducted in different domains to conﬁrm the\nreal efﬁciency of the Rainbow DQL algorithm.\nF. Deep Q-Learning for Extensions of MDPs\n1) Deep Deterministic Policy Gradient Q-Learning for\nContinuous Action: Although DQL algorithm can solve prob-\nlems with high-dimensional state spaces, it can only handle\ndiscrete and low-dimensional action spaces. However, systems\nin many applications have continuous, i.e., real values, and\nhigh dimensional action spaces. The DQL algorithms cannot\nbe straightforwardly applied to continuous actions since they\nrely on choosing the best action that maximizes the Q-value\nfunction. In particular, a full search in a continuous action\nspace to ﬁnd the optimal action is often infeasible.\nIn [31], the authors introduce a model-free off-policy actor-\ncritic algorithm using deep function approximators that can\nlearn policies in high-dimensional, continuous action spaces.\nThe key idea is based on the deterministic policy gradient\n(DPG) algorithm proposed in [32]. In particular, the DPG\nalgorithm maintains a parameterized actor function µ(s; θµ)\nwith parameter vector θ which speciﬁes the current policy\nby deterministically mapping states to a speciﬁc action. The\ncritic Q(s, a) is learned by using the Bellman equation as in\nQ-learning. The actor is updated by applying the chain rule to\nthe expected return from the start distribution J with respect\nto the actor parameters as follows:\n∇θµJ ≈Est∼ρβ\n\u0002\n∇θµQ(s, a; θQ)|s=st,a=µ(st|θµ)\n\u0003\n≈Est∼ρβ\nh\n∇aQ(s, a; θQ)|s=st,a=µ(st)∇θµµ(s; θµ)|s=st\ni\n.\n(12)\nBased on this update rule, the authors then introduce\nDeep DPG (DDPG) algorithm which can learn competitive\npolicies by using low-dimensional observations (e.g. cartesian\ncoordinates or joint angles) under the same hyper-parameters\nand network structure. The detail of the DDPG algorithm\nis presented in 3. The algorithm makes a copy of the actor\nand critic networks Q′(s, a; θQ′) and µ′(s; θµ′), respectively,\nto calculate the target values. The weights of these target\nnetworks are then updated with slowly tracking on the learned\nnetworks, i.e., θ′ ←τθ + (1 −τ)θ′ with τ ≪1. This means\nthat the target values are constrained to change slowly, greatly\nimproving the stability of learning. Note that a major challenge\nof learning in continuous action spaces is exploration. There-\nfore, in Algorithm 3, an exploration policy µ′ is constructed\nby adding noise sampled from a noise process N to the actor\npolicy.\n2) Deep Recurrent Q-Learning for POMDPs: To tackle\nproblems with partially observable environments by deep\nreinforcement learning, the authors in [33] propose a frame-\nwork called Deep Recurrent Q-Learning (DRQN) in which an\nLSTM layer was used to replace the ﬁrst post-convolutional\nfully-connected layer of the conventional DQN. The recurrent\nstructure is able to integrate an arbitrarily long history to\nbetter estimate the current state instead of utilizing a ﬁxed-\nlength history as in DQNs. Thus, DRQNs estimate the function\nQ(ot, ht−1; θ) instead of Q(st, at); θ), where θ denotes the\nparameters of entire network, ht−1 denotes the output of the\nLSTM layer at the previous step, i.e., ht = LSTM(ht−1, ot).\nDRQN matches DQN’s performance on standard MDP prob-\nlems and outperforms DQN in partially observable domains.\nRegarding the training process, DRQN only considers the\nconvolutional features of the observation history instead of\nexplicitly incorporating the actions. Through the experiments,\nthe authors demonstrate that DRQN is capable of handling\npartial observability, and recurrency confers beneﬁts when the\nquality of observations changes during evaluation time.\n10\nAlgorithm 3 DDPG algorithm\n1: Randomly initialize critic network Q(s, a; θQ) and actor\nµ(s; θµ) with weights θQ and θµ, respectively.\n2: Initialize target network Q′ and µ′ with weights\nθQ′ ←θQ, and θµ′ ←θµ, respectively.\n3: Initialize replay memory D.\n4: for episode=1 to M do\n5:\nInitialize a random process N for action exploration\n6:\nReceive initial observation state s1\n7:\nfor t=1 to T do\n8:\nSelect action at = µ(st; θµ) + Nt according to the\ncurrent policy and exploration noise.\n9:\nExecute action at and observe reward rt and new\nstate st+1.\n10:\nStore transition (st, at, rt, st+1) in D.\n11:\nSample a random mini-batch of N\ntransitions\n(si, ai, ri, si+1) from D.\n12:\nSet yi = ri + γQ\n′\u0000si+1, µ\n′(si+1; θµ′); θQ′\u0001\n.\n13:\nUpdate critic by minimizing the loss:\nL = 1\nN\nP\ni(yi −Q\n\u0000si, ai; θQ)\n\u00012\n14:\nUpdate the actor policy by using the sampled policy\ngradient: ∇θµJ ≈1\nN\nP\ni ∇aQ(s, a; θQ)|s=si,a=µ(si)\n∇θµµ(s|θµ)|s=si\n15:\nUpdate the target networks:\nθQ′ ←τθQ + (1 −τ)θQ′\nθµ′ ←τθµ + (1 −τ)θµ′\n16:\nend for\n17: end for\n3) Deep SARSA Learning: In [34], the authors introduce a\nDQL technique based on SARSA learning to help the agent\ndetermine optimal policies in an online fashion. As shown\nin Algorithm 4, given the current state s, a CNN is used to\nobtain the current state-action value Q(s, a). Then, the current\naction a is selected by the ϵ-greedy algorithm. After that, the\nimmediate reward r and the next state s′ can be observed.\nIn order to estimate the current Q(s, a), the next state-action\nvalue Q(s′, a′) is obtained. Here, when the next state s′ is\nused as the input of the CNN, Q(s′, a′) can be obtained as\nthe output. Then, a label vector related to Q(s, a) is deﬁned as\nQ(s′, a′) which represents the target vector. The two vectors\nonly have one different component, i.e., r + γQ(s′, a′) →\nQ(s, a). It should be noted that during the training phase, the\nnext action a′ for estimating the current state-action value is\nnever greedy. On the contrary, there is a small probability that\na random action is chosen for exploration.\n4) Deep Q-Learning for Markov Games:\nIn [35], the\nauthors introduce the general notion of sequential prisoner’s\ndilemma (SPD) to model real world prisoner’s dilemma (PD)\nproblems. Since SPD is more complicated than PD, existing\napproaches addressing learning in matrix PD games cannot be\ndirectly applied in SPD. Thus, the authors propose a multi-\nagent DRL approach for mutual cooperation in SDP games.\nThe deep multi-agent reinforcement learning towards mutual\ncooperation consists of two phases, i.e., ofﬂine and online\nphases. The ofﬂine phase generates policies with varying co-\nAlgorithm 4 Deep SARSA learning algorithm\n1: Initialize data stack D with size of N\n2: Initialize parameters θ of the CNN\n3: for episode=1 to M do\n4:\nInitialize state s1 and pre-process state φ1 = φ(s1)\n5:\nSelect a1 by the ϵ-greedy method\n6:\nfor t=1 to T do\n7:\nTake action at, observe rt and next state st+1\n8:\nφt+1 = φ(st+1)\n9:\nStore data (φt, at, rt, φt+1) into stack D\n10:\nSample data from stack D\n11:\nSelect action a′ by the ϵ-greedy method\n12:\nif episode terminates at step j + 1 then\n13:\nSet yj = rj\n14:\nelse\n15:\nset yj = rj + Q(φt+1, a′; θ)\n16:\nend if\n17:\nMinimize the loss function: (yj −Q(φt, a′; θ))2\n18:\nUpdate at ←a′\n19:\nend for\n20: end for\noperation degrees. Since the number of policies with different\ncooperation degrees is inﬁnite, it is computationally infeasible\nto train all the policies from scratch. To address this issue,\nthe algorithm ﬁrst trains representative policies using actor-\ncritic until it converges, i.e., cooperation and defection baseline\npolicy. Second, the algorithm synthesizes the full range of\npolicies from the above baseline policies. Another task is to\ndetect effectively the cooperation degree of the opponent. The\nalgorithm divides this task into two steps. First, the algorithm\ntrains an LSTM-based cooperation degree detection network\nofﬂine, which will be then used for real-time detection during\nthe online phase. In the online phase, the agent plays against\nthe opponents by reciprocating with a policy of a slightly\nhigher cooperation degree than that of the opponent. On one\nhand, intuitively the algorithm is cooperation-oriented and\nseeks for mutual cooperation whenever possible. On the other\nhand, the algorithm is also robust against selﬁsh exploitation\nand resorts to defection strategy to avoid being exploited\nwhenever necessary.\nUnlike [35] which considers a repeated normal form game\nwith complete information, in [36], the authors introduce\nan application of DRL for extensive form games with im-\nperfect information. In particular, the authors in [36] intro-\nduce Neural Fictitious Self-Play (NFSP), a DRL method for\nlearning approximate Nash equilibria of imperfect-information\ngames. NFSP combines FSP with neural network function\napproximation. An NFSP agent has two neural networks.\nThe ﬁrst network is trained by reinforcement learning from\nmemorized experience of play against fellow agents. This\nnetwork learns an approximate best response to the historical\nbehaviour of other agents. The second network is trained by\nsupervised learning from memorized experience of the agent’s\nown behaviour. This network learns a model that averages\nover the agent’s own historical strategies. The agent behaves\n11\naccording to a mixture of its average strategy and best response\nstrategy.\nIn the NSFP, all players of the game are controlled by\nseparate NFSP agents that learn from simultaneous play\nagainst each other, i.e., self-play. An NFSP agent interacts\nwith its fellow agents and memorizes its experience of game\ntransitions and its own best response behaviour in two mem-\nories, MRL and MSL. NFSP treats these memories as two\ndistinct datasets suitable for DRL and supervised classiﬁcation,\nrespectively. The agent trains a neural network, Q(s, a; θQ),\nto predict action values from data in MRL using off-policy\nreinforcement learning. The resulting network deﬁnes the\nagent’s approximate best response strategy, β = ϵ-greedy(Q),\nwhich selects a random action with probability ϵ and otherwise\nchooses the action that maximizes the predicted action values.\nThe agent trains a separate neural network Π(s, a; θΠ) to\nimitate its own past best response behavior by using supervised\nclassiﬁcation on the data in MSL. NFSP also makes use of\ntwo technical innovations in order to ensure the stability of the\nresulting algorithm as well as to enable simultaneous self-play\nlearning. Through experimental results, the authors show that\nthe NFSP can converge to approximate Nash equilibria in a\nsmall poker game.\nSummary: In this section, we have presented the basics of\nreinforcement learning, deep learning, and DQL. Furthermore,\nwe have discussed various advanced DQL techniques and their\nextensions. Different DQL techniques can be used to solve\ndifferent problems in different network scenarios. In the next\nsections, we review DQL related works for various problems\nin communications and networking.\nIII. NETWORK ACCESS AND RATE CONTROL\nModern networks such as IoT become more decentralized\nand ad-hoc in nature. In such networks, entities such as\nsensors and mobile users need to make independent decisions,\ne.g., channel and base station selections, to achieve their\nown goals, e.g., throughput maximization. However, this is\nchallenging due to the dynamic and the uncertainty of network\nstatus. Learning algorithms such as DQL allow to learn and\nbuild knowledge about the networks that are used to enable\nthe network entities to make their optimal decisions. In this\nsection, we review the applications of DQL for the following\nissues:\n• Dynamic spectrum access: Dynamic spectrum access\nallows users to locally select channels to maximize their\nthroughput. However, the users may not have full obser-\nvations of the system, e.g., channel states. Thus, DQL\ncan be used as an effective tool for dynamic spectrum\naccess.\n• Joint user association and spectrum access: User asso-\nciation is implemented to determine which user to be\nassigned to which Base Station (BS). The joint user\nassociation and spectrum access problems are studied in\n[37] and [38]. However, the problems are typically com-\nbinatorial and non-convex which require nearly complete\nand accurate network information to obtain the optimal\nstrategy. DQL is able to provide distributed solutions\nwhich can be effectively used for the problems.\n• Adaptive rate control: This refers to bitrate/data rate con-\ntrol in dynamic and unpredictable environments such as\nDynamic Adaptive Streaming over HTTP (DASH). Such\na system allows clients or users to independently choose\nvideo segments with different bitrates to download. The\nclient’s objective is to maximize its Quality of Experience\n(QoE). DQL can be adopted to effectively solve the\nproblem instead of dynamic programming which has high\ncomplexity and demands complete information.\nA. Network Access\nThis section discusses how to use DQL to solve the spec-\ntrum access and user association in networks.\n1) Dynamic Spectrum Access: The authors in [39] propose\na dynamic channel access scheme of a sensor based on the\nDQL for IoT. At each time slot, the sensor selects one of\nM channels for transmitting its packet. The channel state is\neither in low interference, i.e., successful transmission, or in\nhigh interference, i.e., transmission failure. Since the sensor\nonly knows the channel state after selecting the channel, the\nsensor’s optimization decision problem can be formulated as\na POMDP. In particular, the action of sensor is to select\none of M channels. The sensor receives a positive reward\n“+1” if the selected channel is in low interference, and a\nnegative reward “-1” otherwise. The objective is to ﬁnd an\noptimal policy which maximizes the sensor’s the expected\naccumulated discounted reward over time slots. A DQN1 using\nFNN with experience replay [40] is then adopted to ﬁnd the\noptimal policy. The input of the DQN is a state of the sensor\nwhich is the combination of actions and observations, i.e., the\nrewards, in the past time slots. The output includes Q-values\ncorresponding to the actions. To balance the exploration of\nthe current best Q-value with the exploration of the better\none, the ϵ-greedy policy is adopted for the action selection\nmechanism. The simulation results based on real data from\n[41] show that the proposed scheme can achieve the average\naccumulated reward close to the myopic policy [42] without\na full knowledge of the system.\n[39] can be considered to be a pioneer work using the DQL\nfor the channel access. However, the DQL keeps following the\nlearned policy over time slots and stops learning a suitable\npolicy. Actual IoT environments are dynamic, and the DQN\nin the DQL needs to be re-trained. An adaptive DQL scheme\nis proposed in [43] which evaluates the accumulated reward\nof the current policy for every period. When the reward is\nreduced by a given threshold, the DQN is re-trained to ﬁnd a\nnew good policy. The simulation results [43] show that when\nthe states of the channels change, the adaptive DQL scheme\ncan detect the change and start re-learning to obtain the high\nreward.\nThe models in [39] and [43] are constrained to only one\nsensor. Consider a multi-sensor scenario, the authors in [44]\naddress the joint channel selection and packet forwarding\nusing the DQL. The model is shown in Fig. 5 in which\none sensor as a relay forwards packets received from its\nneighboring sensors to the sink. The sensor is equipped with\n1Remind that DQN is the core of the DQL algorithms.\n12\nRelay\n(agent)\nSensor\n(neighbor)\nSink\nChannels\nSensor\n(neighbor)\nBuffer\nFig. 5: Joint channel selection and packet forwarding in IoT.\na buffer to store the received packets. At each time slot, the\nsensor selects a set of channels for the packet forwarding so\nas to maximize its utility, i.e., the ratio of the number of\ntransmitted packets to the transmit power. Similar to [39], the\nsensor’s problem can be formulated as an MDP. The action is\nto select a set of channels, the number of packets transmitted\non the channels, and a modulation mode. To avoid packet loss,\nthe state is deﬁned as the combination of the buffer state and\nchannel state. The MDP is then solved by the DQL in which\nthe input is the state and the output is the action selection.\nThe DQL uses the stacked autoencoder to reduce the massive\ncalculation and storage in the Q-learning phase. The sensor’s\nutility function is proved to be bounded which can guarantee\nthe convergence of the algorithm. As shown in the simulation\nresults, the proposed scheme can converge after a certain\nnumber of iterations. Also, the proposed scheme signiﬁcantly\nimproves the system utility compared with the random action\nselection scheme. However, as the packet arrival rate increases,\nthe system utility of the proposed scheme decreases since the\nsensor needs to consume more power to transmit all packets.\nConsuming more power leads to poor sensor’s performance\ndue to its energy constraint, i.e., a shorter IoT system lifetime.\nThe channel access problem in the energy harvesting-enabled\nIoT system is investigated in [45]. The model consists of\none BS and energy harvesting-based sensors. The BS as a\ncontroller allocates channels to the sensors. However, the\nuncertainty of ambient energy availability at the sensors may\nmake the channel allocation inefﬁcient. For example, the\nchannel allocated to the sensor with low available energy may\nnot be fully utilized since the sensor cannot communicate later.\nTherefore, the BS’s problem is to predict the sensors’\nbattery states and select sensors for the channel access so as\nto maximize the total rate. Since the sensors are distributed\nrandomly over a geographical area, the complete statistical\nknowledge of the system dynamics, e.g., the battery states\nand channel states, may not be available. Thus, the DQL is\nused to solve the problem of the BS, i.e., the agent. The DQL\nuses a DQN consisting of two LSTM-based neural network\nlayers. The ﬁrst layer generates the predicted battery states\nof sensors, and the second layer uses the predicted states\nalong with Channel State Information (CSI) to determine the\nchannel access policy. The state space consists of (i) channel\naccess scheduling history, (ii) the history of predicted battery\ninformation, (iii) the history of the true battery information,\nand (iv) the current CSI of the sensors. The action space\ncontains all sets of sensors to be selected for the channel\naccess, and the reward is the difference between the total rate\nand the prediction error. As shown in the simulation results, the\nproposed scheme outperforms the myopic policy [42] in terms\nof total rate. Moreover, the battery prediction error obtained\nfrom the proposed scheme is close to zero.\nThe above schemes, e.g., [39] and [45], focus on the\nrate maximization. In IoT systems such as Vehicle-to-Vehicle\n(V2V) communications, latency also needs to be considered\ndue to the mobility of V2V transmitters/receivers and vital\napplications in the trafﬁc safety. One of the problems of each\nV2V transmitter is to select a channel and a transmit power\nlevel to maximize its capacity under a latency constraint. Given\nthe decentralized network, a DQN is adopted to make optimal\ndecisions as proposed in [46]. The model consists of V2V\ntransmitters, i.e., agents, which share a set of channels. The\nactions of each V2V transmitter include choosing channels\nand transmit power levels. The reward is a function of the\nV2V transmitter’s capacity and latency. The state observed\nby the V2V transmitter consists of (i) the instantaneous\nCSI of the corresponding V2V link, (ii) the interference to\nthe V2V link in the previous time slot, (iii) the channels\nselected by the V2V transmitter’ neighbors in the previous\ntime slot, and (iv) the remaining time to meet the latency\nconstraint. The state is also an input of the DQN. The output\nincludes Q-values corresponding to the actions. As shown in\nthe simulation results, by dynamically adjusting the power\nand channel selection when V2V links are likely to violate\nthe latency constraint, the proposed scheme has more V2V\ntransmitters meeting the latency constraint compared with the\nrandom channel allocation.\nTo reduce spectrum cost, the above IoT systems often use\nunlicensed channels. However, this may cause the interference\nto existing networks, e.g., WLANs. The authors in [47]\npropose to use the DQN to jointly address the dynamic channel\naccess and interference management. The model consists of\nSmall Base Stations (SBSs) which share unlicensed channels\nin an LTE network. At each time slot, the SBS selects one\nof channels for transmitting its packet. However, there may\nbe WLAN trafﬁcs on the selected channel, and thus the SBS\naccesses the selected channel with a probability. The actions\nof the SBS include pairs of channel selection and channel\naccess probability. The problem of the SBS is to determine an\naction vector so as to maximize its total throughput, i.e., its\nutility, over all channels and time slots. The resource allocation\nproblem can be formulated as a non-cooperative game, and the\nDQN using LSTM can be adopted to solve the game. The input\nof the DQN is the history trafﬁc of the SBSs and the WLAN\non the channels. The output includes predicted action vectors\nof the SBSs. The utility function of each SBS is proved to\nbe convex, and thus the DQN-based algorithm converges to a\nNash equilibrium of the game. The simulation results based\non real trafﬁc data from [48] show that the proposed scheme\ncan improve the average throughput up to 28% compared with\nthe standard Q-learning [15]. Moreover, deploying more SBSs\nin the LTE network does not allow more airtime fraction for\nthe network. This implies that the proposed scheme can avoid\ncausing performance degradation to the WLAN. However, the\n13\nproposed scheme requires synchronization between the SBSs\nand the WLAN which is challenging in real networks.\nIn the same cellular network context, the authors in [22]\naddress the dynamic spectrum access problem for multiple\nusers sharing K channels. At a time slot, the user selects a\nchannel with a certain attempt probability or chooses not to\ntransmit at all. The state is the history of the user’s actions\nand its local observations, and the user’s strategy is mapping\nfrom the history to an attempt probability. The problem of the\nuser is to ﬁnd a vector of the strategies, i.e., the policy, over\ntime slots to maximize its expected accumulated discounted\ndata rate of the user.\nThe above problem is solved by training a DQN. The\ninput of the DQN includes past actions and the corresponding\nobservations. The output includes estimated Q-values of the\nactions. To avoid the overestimation in the Q-learning, the\nDDQN [20] is used. Moreover, the dueling DQN [49] is\nemployed to improve the estimated Q-value. The DQN is\nthen ofﬂine trained at a base station. Similar to [47], the\nmultichannel random access is modeled as a non-cooperative\ngame. As proved in [22], the game has a subgame perfect\nequilibrium. Note that some users can keep increasing their\nattempt probability to increase their rates. This makes the\nequilibrium point inefﬁcient, and thus the strategy space of\nthe users is restricted to avoid the situation. The simulation\nresults show that the proposed scheme can achieve twice the\nchannel throughput compared with the slotted-Aloha [50]. The\nreason is that in the proposed scheme, each user only learns\nfrom its local observation without an online coordination or\ncarrier sensing. However, the proposed scheme requires the\ncentral unit which may raise the message exchanges as the\ntraining is frequently updated.\nIn the aforementioned models, the number of users is ﬁxed\nin all time slots, and the arrival of new users is not considered.\nThe authors in [51] address the channel allocation to new\narrival users in a multibeam satellite system. The multibeam\nsatellite system generates a geographical footprint subdivided\ninto multiple beams which provide services to ground User\nTerminals (UTs). The system has a set of channels. If there\nexist available channels, the system allocates a channel to the\nnew arrived UT, i.e., the new service is satisﬁed. Otherwise, the\nservice is blocked. The system’s problem is to ﬁnd a channel\nallocation decision to minimize the total service blocking\nprobability of the new UT over time slots without causing\nthe interference to the current UTs.\nThe system’s problem can be viewed as a temporal corre-\nlated sequential decision-making optimization problem which\nis effectively solved by the DQN. Here, the satellite system\nis the agent. The action is an index indicating which channel\nis allocated to the new arrived UT. The reward is positive\nwhen the new service is satisﬁed and is negative when the\nservice is blocked. The state includes the set of current UTs,\nthe current channel allocation matrix, and the new arrived UT.\nNote that the state has the spatial correlation feature due to\nthe co-channel interference, and thus it can be represented\nin an image-like fashion, i.e., an image tensor. Therefore,\nthe DQN adopts the CNN to extract useful features of the\nstate. The simulation results show that the proposed DQN\nalgorithm converges after a certain number of training steps.\nAlso, by allocating available channels to the new arrived UTs,\nthe proposed scheme can improve the system trafﬁc up to\n24.4% compared with the ﬁxed channel allocation scheme.\nHowever, as the number of current UTs increases, the number\nof available channels is low or even zero. Therefore, the\ndynamic channel allocation decisions of the proposed scheme\nbecome meaningless, and the performance difference between\nthe two schemes becomes insigniﬁcant. For the future work,\na joint channel and power allocation algorithm based on the\nDQL can be investigated.\n2) Joint User Association and Spectrum Access: The joint\nuser association and spectrum access problems are typically\nnon-convex. DQL is able to provide distributed solutions, and\nthus it can be effectively used to solve the problems without\nrequiring complete and accurate network information.\nThe authors in [23] consider a HetNet which consists\nof multiple users and BSs including macro base stations\nand femto base stations. The BSs share a set of orthogonal\nchannels, and the users are randomly located in the network.\nThe problem of each user is to select one BS and a channel\nto maximize its data rate while guaranteeing that the Signal-\nto-Interference-plus-Noise Ratio (SINR) of the user is higher\nthan a minimum Qualtiy of Service (QoS) requirement. The\nDQL is adopted to solve the problem in which each user is\nan agent, and its state is a vector including QoS states of\nall users, i.e., the global state. Here, the QoS state of the\nuser refers to whether its SINR exceeds the minimum QoS\nrequirement or not. At each time slot, the user takes an action.\nIf the QoS is satisﬁed, the user receives utility as its immediate\nreward. Otherwise, it receives a negative reward, i.e., an action\nselection cost. Note that the cumulative reward of one user\ndepends on actions of other users, then the user’s problem can\nbe deﬁned as an MDP. Similar to [22], the DDQN and the\ndueling DQN are used to learn the optimal policy, i.e., the\njoint BS and channel selections, for the user to maximize its\ncumulative reward. The simulation results from [23] show that\nthe proposed scheme outperforms the Q-learning implemented\nin [15] in terms of convergence speed and system capacity.\nThe scheme proposed in [23] is considered to be the\nﬁrst work using the DQL for the joint user association and\nspectrum access problem. Inspired by this work, the authors\nin [52] propose to use the DQL for a joint user association,\nspectrum access, and content caching problem. The network\nmodel is an LTE network which consists of UAVs serving\nground users. The UAVs are equipped with storage units and\ncan act as cached-enabled LTE-BSs. The UAVs are able to\naccess both licensed and unlicensed bands in the network.\nThe UAVs are controlled by a cloud-based server, and the\ntransmissions from the cloud to the UAVs are implemented by\nusing the licensed cellular band. The problem of each UAV is\nto determine (i) its optimal user association, (ii) the bandwidth\nallocation indicators on the licensed band, (iii) the time slot\nindicators on the unlicensed band, and (iv) a set of popular\ncontents that the users can request to maximize the number\nof users with stable queue, i.e., users satisﬁed with content\ntransmission delay.\nThe UAV’s problem is combinatorial and non-convex, and\n14\nClient\n(agent)\n(Video segment & bitrate)\n(Video segment)\nBuffer\nContent delivery \nnetwork\nServer\nVideo \nsegments\nServer\nFig. 6: A dynamic adaptive streaming system based on HTTP\nstandard.\nthe DQL can be used to solve it. The UAVs do not know\nthe users’ content requests, and thus the Liquid State Machine\napproach (LSM) [53] is adopted to predict the content request\ndistribution of the users and to perform resource allocation.\nIn particular, predicting the content request distribution is\nimplemented at the cloud based on an LSM-based prediction\nalgorithm. Then, given the request distributions, each UAV as\nan agent uses an LSM-based learning algorithm to ﬁnd its\noptimal users association. Speciﬁcally, the input of the LSM-\nbased learning algorithm consists of actions, i.e., UAV-user\nassociation schemes, that other UAVs take, and the output\nincludes the expected numbers of users with stable queues\ncorresponding to actions that the UAV can take. After the user\nassociation is done, the optimal content caching is determined\nbased on the results of [54, Theorem 2], and the optimal spec-\ntrum allocation is done by using linear programming. Based\non the Gordon’s Theorem [55], the proposed DQL is proved\nto converge with probability one. The simulation results using\ncontent request data from [56] show that the proposed DQL\ncan converge in around 400 iterations. Compared with the Q-\nlearning, the proposed DQN improves the convergence time up\nto 33% . Moreover, the proposed DQL signiﬁcantly improves\nthe number of users with stable queues up to 50% compared\nwith the Q-learning without cache. In fact, energy efﬁciency is\nalso important for the UAVs, and thus applying the DQL for a\njoint user association, spectrum access, and power allocation\nproblem needs to be investigated.\nB. Adaptive Rate Control\nDynamic Adaptive Streaming over HTTP (DASH) becomes\nthe dominant standard for video streaming [57]. DASH is able\nto leverage existing content delivery network infrastructure\nand is compatible with a multitude of client-side applications.\nA general DASH system is shown in Fig. 6 in which the\nvideos are stored in servers as multiple segments, i.e., chunks.\nEach segment is encoded at different compression levels to\ngenerate representations with different bitrates, i.e., different\nvideo visual quality. At each time slot, the client chooses\na representation, i.e., a segment with a certain bitrate, to\ndownload. The client’s problem is to ﬁnd an optimal policy\nwhich maximizes its QoE such as maximizing average bitrate\nand minimizing rebuffering, i.e., the time which the video\nplayout freezes.\nAs presented in [58], the above problem can be modeled\nas an MDP in which the agent is the client and the action\nis choosing a representation to download. To maximize the\nQoE, the reward is deﬁned as a function of (i) visual quality\nof the video, (ii) video quality stability, (iii) rebuffering event,\nand (iv) buffer state. Given the reward formulation, the state\nof the client should include (i) the video quality of the last\ndownloaded segment, (ii) the current buffer state, (iii) the\nrebuffering time, and (iv) the channel capacities experienced\nduring downloading of segments in the past time slots. The\nMDP can be solved by using dynamic programming, but the\ncomputational complexity rapidly becomes unmanageable as\nthe size of the problem increases. Thus, the authors in [58]\nadopt the DQL to solve the problem. Similar to [45], the\nLSTM networks are used in which the input is the state of\nthe client, and the output includes Q-values corresponding to\nthe client’s possible actions. To improve the performance of\nthe standard LSTM, peephole connections are added into the\nLSTM networks. The simulation results based on dataset from\n[59] show that the proposed DQL algorithm can converge\nmuch faster than Q-learning. Moreover, the proposed DQL\nimproves the video quality and reduces the rebuffering since\nit is able to dynamically manage the buffer by considering the\nbuffer state and channel capacity.\nThe network model and the optimization problem in [58]\nare also found in [60]. However, different from [58], the\nauthors in [60] adopt the Asynchronous Advantage Actor-\nCritic (A3C) method [26] for the DQL to further enhance\nand speed up the training. As presented in Section II-F1,\nA3C includes two neural networks, namely, actor network and\ncritic network. The actor network is to choose bitrates for the\nclient, and the critic network helps train the actor network.\nFor the actor network, the input is the client’s state, and the\noutput is a policy, i.e., a probability distribution over possible\nactions given states that the client can take. Here, the action is\nchoosing the next representation, i.e., the next segment with a\ncertain bitrate, to download. For the critic network, the input\nis the client’s state, and the output is the expected total reward\nwhen following the policy obtained from the actor network.\nThe simulation results based on the mobile dataset from [61]\nshow that the proposed DQL can improve the average QoE up\nto 25% compared with the bitrate control scheme [62]. Also,\nby having sufﬁcient buffer to handle the network’s throughput\nﬂuctuations, the proposed DQL reduces the rebuffering around\n32.8% compared with the baseline scheme.\nIn practice, the DQL algorithm proposed in [60] can be\neasily deployed in a multi-client network since A3C is able\nto support parallel training for multiple agents. Accordingly,\neach client, i.e., an agent, is conﬁgured to observe its reward.\nThen, the client sends a tuple including its state, action, and\nreward to a server. The server uses the actor-critic algorithm\nto update its actor network model. The server then pushes the\nnewest model to the agent. This update process can happen\nasynchronously among all agents which improves quality and\nspeeds up the training. Although the parallel training scheme\nmay incur a Round-Trip Time (RTT) between the clients and\nthe server, the simulation results in [60] show that the RTT\nbetween the clients and the server reduces the average QoE by\nonly 3.5%. The performance degradation is small, and thus the\nproposed DQL can be implemented in real network systems.\nIn [58] and [60], the input of the DQL, i.e., the client’s\n15\nstate, includes the video quality of the last downloaded video\nsegment. The video segment is raw which may cause “state\nexplosion” to the state space [63]. To reduce the state space\nand to improve the QoE, the authors in [63] propose to use\na video quality prediction network. The prediction network\nextracts useful features from the raw video segments using\nCNN and RNN. Then, the output of the prediction network,\ni.e., the predicted video quality, is used as one of the inputs of\nthe DQL which is proposed in [60]. The simulation results\nbased on the broadband dataset from [64] show that the\nproposed DQL can improve the average QoE up to 25%\ncompared with the Google Hangout, i.e., a communication\nplatform developed by Google. Moreover, the proposed DQL\ncan reduce the average latency of video transmission around\n45% due to the small state space.\nApart from the DASH systems, the DQL can be effectively\nused for the rate control in High Volume Flexible Time\n(HVFT) applications. HVFT applications use cellular networks\nto deliver IoT trafﬁc. The HVFT applications have a large\nvolume of trafﬁc, and the trafﬁc scheduling, e.g., data rate\ncontrol, in the HVFT applications is necessary. One common\napproach is to assign static priority classes per trafﬁc type, and\nthen trafﬁc scheduling is based on its priority class. However,\nsuch an approach does not evolve to accommodate new trafﬁc\nclasses. Thus, learning methods such as DQL should be used to\nprovide adaptive rate control mechanisms as proposed in [65].\nThe network model is a single cell including one BS as a\ncentral controller and multiple mobile users. The problem at\nthe BS is to ﬁnd a proper policy, i.e., data rate for the users,\nto maximize the amount of transmitted HVFT trafﬁc while\nminimizing performance degradation to existing data trafﬁcs.\nIt is shown in [65] that the problem can be formulated as\nan MDP. The agent is the BS, and the state includes the\ncurrent network state and the useful features extracted from\nnetwork states in the past time slots. The network state at\na time slot includes (i) the congestion metric, i.e., the cell’s\ntrafﬁc load, at the time slot, (ii) the total number of network\nconnections, and (iii) the cell efﬁciency, i.e., the cell quality.\nThe action that the BS takes is a combination of the trafﬁc\nrate for the users. To achieve the BS’ objective, the reward\nis deﬁned as a function of (i) the sum of HVFT trafﬁc, (ii)\ntrafﬁc loss to existing applications due to the presence of\nthe HVFT trafﬁc, and (iii) the amount of bytes served below\ndesired minimum throughput. The DQL using the actor and\ncritic networks with LSTM is then adopted. By using the real\nnetwork data collected in Melbourne, the simulation results\nshow that the proposed DQL increases the HVFT trafﬁc up to\n2 times compared with the heuristic control scheme. However,\nhow the proposed scheme reduces the trafﬁc loss is not shown.\nIn the aforementioned approaches, the maximum number of\nobjectives is constrained, e.g., to 3 in [66]. The authors in [67]\nshow that the DQL can be used for the rate control to achieve\nmultiple objectives in complex communication systems. The\nnetwork model is a future space communication system which\nis expected to operate in unpredictable environments, e.g.,\norbital dynamics, atmospheric and space weather, and dy-\nnamic channels. In the system, the transmitter needs to be\nconﬁgured with several transmit parameters, e.g., symbol rate\nand encoding rate, to achieve multiple conﬂict objectives, e.g.,\nlow Bit Error Rate (BER), throughput improvement, power\nand spectral efﬁciency. The adaptive coding and modulation\nschemes, i.e., [68], can be used. However, the methods allow to\nachieve only limited number objectives. Learning algorithms\nsuch as the DQL can be thus used. The agent is the transmitter\nin the system. The action is a combination of (i) symbol rate,\n(ii) energy per symbol, (iii) modulation mode, (iv) number\nof bits per symbol, and (v) encoding rate. The objective is to\nmaximize the system performance. Thus, the reward is deﬁned\nas a ﬁtness function of performance parameters including (i)\nBER estimated at the receiver, (ii) throughput, (iii) spectral\nefﬁciency, (iv) power consumption, and (v) transmit power\nefﬁciency. The state is the system performance measured by\nthe transmitter, and thus the state is the reward. To achieve\nmultiple objectives, the DQL is implemented by using a set of\nmultiple neural networks in parallel. The input of the DQL is\nthe current state and the channel conditions, and the output is\nthe predicted action. The neural networks are trained by using\nthe Levenberg-Marquardt backpropagation algorithm [69]. The\nsimulation results show that the proposed DQL can achieve the\nﬁtness score, i.e., the weighted sum of different objectives,\nclose to the ideal, i.e., the exhaustive search approach. This\nimplies that the DQL is able to select near-optimal actions\nand learn the relationship between rewards and actions given\ndynamic channel conditions.\nSummary: This section reviews applications of DQL for\nthe dynamic network access and adaptive rate control. The\nreviewed approaches are summarized along with the references\nin Table III. We observe that the problems are mostly modeled\nas an MDP. Moreover, DQL approaches for the IoT and DASH\nsystems receive more attentions than other networks. Future\nnetworks, e.g., 5G networks, involve multiple network entities\nwith multiple conﬂicting objectives, e.g., provider’s revenue\nversus users’ utility maximization. This poses a number of\nchallenges to the traditional resource management mechanisms\nthat deserve in-depth investigation. In the next section, we\nreview the adoption of DQL for the emerging services, i.e.,\nofﬂoading and caching.\nIV. CACHING AND OFFLOADING\nAs one of the key features of information-centric net-\nworking, in-network caching can efﬁciently reduce duplicated\ncontent transmissions. The studies on wireless caching has\nshown that access delays, energy consumption, and the total\namount of trafﬁc can be reduced signiﬁcantly by caching\ncontents in wireless devices. Big data analytics [70] also\ndemonstrate that with limited cache size, proactive caching\nat network edge nodes can achieve 100% user satisfaction\nwhile ofﬂoading 98% of the backhaul trafﬁc. Joint content\ncaching and ofﬂoading can address the gap between the\nmobile users’ large data demands and the limited capacities\nin data storage and processing. This motivates the study on\nMobile Edge Computing (MEC). By deploying both com-\nputational resources and caching capabilities close to end\nusers, MEC signiﬁcantly improves energy efﬁciency and QoS\nfor applications that require intensive computations and low\n16\nTABLE III: A summary of approaches using DQL for network access and adaptive rate control.\nISSUES\nREF.\nMODEL\nLEARNING\nALGORITHMS\nAGENT\nSTATES\nACTIONS\nREWARDS\nNETWORKS\nNetwork access\n[39]\nPOMDP\nDQN using\nFNN\nSensor\nPast channel selections and\nobservations\nChannel selection\nScore +1 or -1\nIoT\n[44]\nMDP\nDQN using\nFNN\nSensor\nCurrent buffer state and\nchannel state\nChannel, packets,\nand modulation\nmode selection\nRatio of number of\ntransmitted packets\nto transmit power\nIoT\n[45]\nMDP\nDQN with\nLSTM\nBase station\nChannel access history,\npredicted and true battery\ninformation history, and\ncurrent CSI\nSensor selection\nfor channel access\nTotal rate and\nprediction error\nIoT\n[46]\nMDP\nDQN with\nLSTM\nV2V\ntransmitter\nCurrent CSI, past\ninterference, past channel\nselections, and remaining time\nto meet the latency constraints\nChannel and\ntransmit power\nselection\nCapacity and latency\nIoT\n[47]\nGame\nDQN with\nLSTM\nSmall base\nstation\nTrafﬁc history of small base\nstations and the WLAN\nChannel selection\nand channel access\nprobability\nThroughput\nLTE\nnetwork\n[22]\nGame\nDDQN and\ndueling DQN\nMobile user\nPast channel selections and\nobservations\nChannel selection\nData rate\nCRN\n[51]\nMDP\nDQN with\nCNN\nSatellite\nsystem\nCurrent user terminals,\nchannel allocation matrix, and\nthe new arrival user\nChannel selection\nScore +1 or -1\nSatellite\nsystem\n[23]\nMDP\nDDQN and\ndueling DQN\nMobile user\nQoS states\nBase station and\nchannel selection\nUtility\nHetNet\n[52]\nGame\nDQN with\nLSM\nUAV\nContent request distribution\nBase station\nselection\nUsers with stable\nqueues\nLTE\nnetwork\nRate control\n[58]\nMDP\nDQN with\nLSTM and\npeephole\nconnections\nClient\nLast segment quality, current\nbuffer state, rebuffering time,\nand channel capacities\nBitrate selection\nfor segment\nVideo quality,\nrebuffering even, and\nbuffer state\nDASH\nsystem\n[60]\nMDP\nDQN with\nA3C\nClient\nLast segment quality, current\nbuffer state, rebuffering time,\nand channel capacities\nBitrate selection\nfor segment\nVideo quality,\nrebuffering even, and\nbuffer state\nDASH\nsystem\n[63]\nMDP\nDQN with\nCNN and RNN\nClient\nPredicted video quality,\ncurrent buffer state,\nrebuffering time, and channel\ncapacities\nBitrate selection\nfor segment\nVideo quality,\nrebuffering even, and\nbuffer state\nDASH\nsystem\n[65]\nMDP\nDQN using\nA3C and\nLSTM\nBase station\nCongestion metric, current\nnetwork connections, and cell\nefﬁciency\nTrafﬁc rate\ndecisions for\nmobile users\nHVFT trafﬁc, trafﬁc\nloss to existing\napplications, and the\namount of served\nbytes\nHVFT\napplication\n[67]\nMDP\nDQN using\nFNN\nBase station\nMeasurements of BER,\nthroughput, spectral\nefﬁciency, power\nconsumption, and transmit\npower efﬁciency\nSymbol rate,\nenergy per symbol,\nmodulation mode,\nnumber of bits per\nsymbol, and\nencoding rate\nSame as the state\nSpace com-\nmunication\nsystem\nlatency. A uniﬁed study on caching, ofﬂoading, networking,\nand transmission control in MEC scenarios involves very com-\nplicated system analysis because of strong couplings among\nmobile users with heterogeneities in application demand, QoS\nprovisioning, mobility pattern, radio access interface, and\nwireless resources. A learning-based and model-free approach\nbecomes a promising candidate to manage huge state space\nand optimization variables, especially by using DNNs. In this\nsection, we review the modeling and optimization of caching\nand ofﬂoading policies in wireless networks by leveraging the\nDRL framework.\nA. Wireless Proactive Caching\nWireless proactive caching has attracted great attentions\nfrom both academia and industry. Statistically, a few pop-\nular contents are usually requested by many users during a\nshort time span, which accounts for most of the trafﬁc load.\nTherefore, proactively caching popular contents can avoid the\nheavy trafﬁc burden of the backhaul links. In particular, this\ntechnique aims at pre-caching the contents from the remote\ncontent servers at the edge devices or BSs that are close to\nthe end users. If the requested contents are already cached\nlocally, the BS can directly serve the end users with small\ndelay. Otherwise, the BS requests these contents from the\noriginal content server and updates the local cache based on\nthe caching policy, which is one of the main design problem\nfor wireless proactive caching.\n1) QoS-Aware Caching:\nContent popularity is the key\nfactor used to solve the content caching problem. With a\nlarge number of contents and their time-varying popularities,\nDQL is an attractive strategy to tackle this problem with\nhigh-dimensional state and action spaces. The authors in [70]\npresent a DQL scheme to improve the caching performance.\nThe system model consists of a single BS with a ﬁxed cache\n17\nsize. For each request, the BS as an agent makes a decision\non whether or not to store the currently requested content in\nthe cache. If the new content is kept, the BS determines which\nlocal content will be replaced. The state is the feature space of\nthe cached contents and the currently requested content. The\nfeature space consists of the total number of requests for each\ncontent in a speciﬁc short-, medium-, and long-term. There\nare two types of actions: (i) to ﬁnd a pair of contents and\nexchange the cache states of the two contents and (ii) to keep\nthe cache states of the contents unchanged. The aim of the BS\nis to maximize the long-term cache hit rate, i.e., reward.\nThe DQL scheme in [70] trains the policy by using\nthe DDPG method [71] and employs Wolpertinger architec-\nture [72] to reduce the size of the action space and avoid miss-\ning an optimal policy. The Wolpertinger architecture consists\nof three main parts: an actor network, K-Nearest Neighbors\n(K-NN), and a critic network. The actor network is to avoid a\nlarge action space. The critic network is to correct the decision\nmade by the actor network. The DDPG method is applied\nto update both critic and actor networks. K-NN can help to\nexplore a set of actions to avoid poor decisions. The actor\nand critic networks are then implemented by using FNNs.\nThe simulation results show that the proposed DQL scheme\noutperforms the ﬁrst-in ﬁrst-out scheme in terms of long-term\ncache hit rate.\nMaximizing the long-term cache hit rate in [70] implies\nthat the cache stores the most popular contents. In a dynamic\nenvironment, contents stored in a cache have to be replaced\naccording to the users’ dynamic requests. An optimization of\nthe placement or replacement of cached contents is studied\nin [73] by a deep learning method. The optimization algorithm\nis trained by a DNN in advance and then used for real-\ntime caching or scheduling with minimum delay. The authors\nin [74] propose an optimal caching policy to learn the cache\nexpiration times, i.e., Time-To-Live (TTL), for dynamically\nchanging requests in content delivery networks. The system\nincludes a cloud database server and multiple mobile devices\nthat can issue queries and update entries in a single database.\nThe query results can be cached for a speciﬁed time interval\nat server-controlled caches. All cached queries will become\ninvalid if one of the cached records has been updated. A large\nTTL will strain cache capacities while a small TTL increases\nlatencies signiﬁcantly if the database server is physically\nremote.\nUnlike the DDPG approach used in [70], the authors in [74]\npropose to utilize Normalized Advantage Functions (NAFs)\nfor continuous DQL scheme to learn optimal cache expiration\nduration. The key problem in continuous DQL is to select an\naction maximizing the Q-function, while avoiding performing\na costly numerical optimization at each step. The use of NAFs\nobviates a second actor network that needs to be trained sepa-\nrately. Instead, a single neural network is used to output both\na value function and an advantage term. The DQL agent at the\ncloud database uses an encoding of a query itself and the query\nmiss rates as the system states, which allows for an easier\ngeneralization. The system reward is linearly proportional to\nthe current load, i.e., the number of cached queries divided\nby the total capacity. This reward function can encourage\nlonger TTLs when fewer queries are cached, and shorter TTLs\nwhen the load is close to the system capacity. Considering\nincomplete measurements for rewards and next-states at run-\ntime, the authors introduce the Delayed Experience Injection\n(DEI) approach that allows the DQL agent to keep track of\nincomplete transitions when measurements are not immedi-\nately available. The authors evaluate the learning algorithm\nby Yahoo! cloud serving benchmark with customized web\nworkloads [75]. The simulation results verify that the learning\napproach based on NAFs and DEI outperforms a statistical\nestimator.\n2) Joint Caching and Transmission Control: The caching\npolicies determine where to store and retrieve the requested\ncontent efﬁciently, e.g., by learning the contents’ popular-\nities [70] and cache expiration time [74]. Another impor-\ntant aspect of caching design is the transmission control of\nthe content delivery from caches to end users, especially\nfor wireless systems with dynamic channel conditions. To\navoid mutual interference in multi-user wireless networks,\nthe transmission control decides which cached contents can\nbe transmitted concurrently as well as the most appropriate\ncontrol parameters, e.g., transmit power, precoding, data rate,\nand channel allocation. Hence, the joint design of caching\nand transmission control is required to enable efﬁcient content\ndelivery in multi-user wireless networks.\nThe authors in [76]–[78] propose a DQL framework to\naddress the joint caching and interference alignment to tackle\nmutual interference in multi-user wireless networks. The au-\nthors consider an MIMO system with limited backhaul capac-\nity and the caches at the transmitter. The precoding design for\ninterference alignment requires the global CSI at each trans-\nmitter. A central scheduler is responsible for collecting CSI\nand cache status from each user via the backhaul, scheduling\nthe users’ transmission, and optimizing the resource allocation.\nBy enabling content caching at individual transmitters, we can\ndecrease the demand for data transfer and thus save more back-\nhaul capacity for real-time CSI update and sharing. Using the\nDQL-based approach at the central scheduler can reduce the\nexplicit demand for CSI and the computational complexity in\nmatrix optimization, especially with time-varying channel con-\nditions. The DQL agent implements the DNN to approximate\nthe Q-function with experience replay in training. To make the\nlearning process more stable, the target Q-network parameter\nis updated by the Q-network for every a few time instants.\nThe collected information is assembled into a system state and\nsent to the DQL agent, which feeds back an optimal action\nfor the current time instant. The action indicates which users\nto be active, and the resource allocation among active users.\nThe system reward represents the total throughput of multiple\nusers. An extended work of [76] and [77] with a similar DQL\nframework is presented in [78], in which a CNN-based DQN\nis adopted and evaluated in a more practical conditions with\nimperfect or delayed CSI. Simulation results show that the\nperformance of the MIMO system is signiﬁcantly improved\nin terms of the total throughput and energy efﬁciency.\nInterference management is an important requirement of\nwireless systems. The application-related QoS or user experi-\nence is also an essential metric. Different from [76]–[78], the\n18\nauthors in [79] propose a DQL approach to maximize Quality\nof Experience (QoE) of IoT devices by jointly optimizing\nthe cache allocation and transmission rate in content-centric\nwireless networks. The system state is speciﬁed by the nodes’\ncaching conditions, e.g., the service information and cached\ncontents, as well as the transmission rates of the cached\ncontents. The aim of the DQL agent is to minimize continu-\nously the network cost or maximize the QoE. The proposed\nDQL framework is further enhanced with the use of PER and\nDDQN. PER replays important transitions more frequently so\nthat DQN can learn from samples more efﬁciently. The use\nof DDQN can stabilize the learning by providing two value\nfunctions in separated neural networks. This avoids an over-\nestimation of the DQN with the increasing number of actions.\nThese two neural networks are not completely decoupled as\nthe target network is a periodic copy of estimation network. A\ndiscrete simulator ccnSim [80] is used to model the caching\nbehavior in various graph structures. The output data trace of\nthe simulator is then imported to Matlab and used to evaluate\nthe learning algorithm. The simulation results show that the\nDQL framework by using PER and DDQN outperforms the\nstandard penetration test scheme in terms of QoE.\nThe QoE can be used to characterize the users’ percep-\ntion of Virtual Reality (VR) services. The authors in [81]\naddress the joint content caching and transmission strategy\nin a wireless VR network, where UAVs capture videos on live\ngames and transmit them to small-cell BSs servicing the VR\nusers. Millimeter wave (mmWave) downlink backhaul links\nare used for VR content transmission from the UAVs to BSs.\nThe BSs can also cache the popular contents that may be\nrequested frequently by end users. The joint content caching\nand transmission problem is formulated as an optimization to\nmaximize the users’ reliability, i.e., the probability that the\ncontent transmission delay satisﬁes the instantaneous delay\ntarget. The maximization involves the control of transmission\nformat, users’ association, the set and format of cached con-\ntents. A DQL framework combining the Liquid State Machine\n(LSM) and Echo State Network (ESN) is proposed for each\nBS to ﬁnd the optimal transmission and caching strategies. As\na randomly generated spiking neural network [82], LSM can\nstore information about the network environment over time\nand adjust the users’ association policy, cached contents and\nformats according to the users’ content requests. It has been\nused in [83] to predict the users’ content request distribution\nwhile having only limited information regarding the network\nand different users. Conventional LSM uses FNNs as the\noutput function, which demands high complexity in training\ndue to the computation of gradients for all of the neurons.\nConversely, the proposed DQL framework uses an ESN as the\noutput function, which uses historical information to ﬁnd the\nrelationship between the users’ reliability, caching, and content\ntransmission. It also has a lower complexity in training and\na better memory for network information. Simulation results\nshow that the proposed DQL framework can yield 25.4%\ngain in terms of users’ reliability compared to the baseline\nQ-learning.\n3) Joint Caching, Networking, and Computation: Caching\nand transmission control will become more involved in a Het-\nNet that integrates different communication technologies, e.g.,\ncellular system, device-to-device network, vehicular network,\nand networked UAVs, to support various application demands.\nThe network heterogeneity raises the problem of complicated\nsystem design that needs to address challenging issues such\nas mutual interference, differentiated QoS provisioning, and\nresource allocation, hopefully in a uniﬁed framework. Obvi-\nously this demands a joint optimization far beyond the extent\nof joint caching and transmission control.\nAccordingly, the authors in [84] propose a DQL framework\nfor energy-efﬁcient resource allocation in green wireless net-\nworks, jointly considering the couplings among networking,\nin-network caching and computation. The system consists\nof a Software-Deﬁned Network (SDN) with multiple virtual\nnetworks and mobile users requesting for video on-demand\nﬁles that require a certain amount of computational resource\nat either the content server or at local devices. In each virtual\nnetwork, an authorized user issues a request to download ﬁles\nfrom a set of available SBSs in its neighborhood area. The\nwireless channels between each mobile user and the SBSs\nare characterized as Finite-State Markov Channels (FSMC).\nThe states are the available cache capacity at the SBSs,\nthe channel conditions between mobile users and SBSs, the\ncomputational capability of the content servers and mobile\nusers. The DQL agent at each SBS decides an association\nbetween each mobile user and SBS, where to perform the\ncomputational task, and how to schedule the transmissions of\nSBSs to deliver the required data. The objective is to minimize\nthe total energy consumption of the system from data caching,\nwireless transmission, and computation.\nThe DQL scheme proposed in [84] has been applied to\nimprove the performance of Vehicular Ad doc NETworks\n(VANETs) in [85]–[87]. The network model includes multiple\nBSs, Road Side Units (RSUs), MEC servers, and content\nservers. All devices are controlled by a mobile virtual network\noperator. The vehicles request for video contents that can be\ncached at the BSs or retrieved from remote content servers.\nThe authors in [85] formulate the resource allocation problem\nas a joint optimization of caching, networking, and comput-\ning, e.g., compressing and encoding operations of the video\ncontents. The system states include the CSI from each BS, the\ncomputational capability, and cache size of each MEC/content\nserver. The network operator feeds the system state to the\nFNN-based DQN and gets the optimal policy that determines\nthe resource allocation for each vehicle. To exploit spatial\ncorrelations in learning, the authors in [86] enhance Q-learning\nby using CNNs in DQN. This makes it possible to extract\nhigh-level features from raw input data. Two schemes have\nbeen introduced in [87] to improve stability and performance\nof the ordinary DQN method. Firstly, DDQN is designed to\navoid over-estimation of Q-value in ordinary DQN. Hence, the\naction can be decoupled from the target Q-value generation.\nThis makes the training process faster and more reliable.\nSecondly, the dueling DQN approach is also integrated in the\ndesign with the intuition that it is not always necessary to\nestimate the reward by taking some action. The state-action Q-\nvalue in dueling DQN is decomposed into one value function\nrepresenting the reward in the current state, and the advantage\n19\nCache\nCellular BS\nSmall cell BS\nCloud\nJoint design of caching, networking, \nand transmission control strategies\nFig. 7: Joint caching, networking, and transmission control to\noptimize cache hit rate [70], cache expiration time [74], interfer-\nence alignment [76]–[78], Quality of Experience [79], [81], energy\nefﬁciency [84], resource allocation [85]–[87], trafﬁc latency, or re-\ndundancy [89], [91].\nfunction that measures the relative importance of a certain\naction compared with other actions. The enhanced DQL agent\ncombining these two schemes can achieve better performance\nand faster training speed.\nConsidering the huge action space and high complexity with\nthe vehicle’s mobility and service delay deadline Td, a multi-\ntime scale DQN framework is proposed in [88] to minimize\nthe system cost by the joint design of communication, caching\nand computing in VANET. The policy design accounts for\nlimited storage capacities and computational resources at the\nvehicles and the RSUs. The small timescale DQN is for every\ntime slot and aims to maximize the exact immediate reward.\nAdditionally, the large timescale DQN is designed for every\nTd time slots within the service delay deadline, and used to\nestimate the reward considering the vehicle’s mobility in a\nlarge timescale.\nThe aforementioned DQL framework for VANETs, e.g.,\n[85]–[87], has also been generalized to smart city applica-\ntions in [89], which necessitates dynamic orchestration of\nnetworking, caching, and computation to meet different ser-\nvicing requirements. Through Network Function Virtualization\n(NFV) [90], the physical wireless network in smart cities\ncan be divided logically into several virtual ones by the\nnetwork operator, which is responsible for network slicing\nand resource scheduling, as well as allocation of caching\nand computing capacities. The use cases in smart cities are\npresented in [91], [92], which apply the generalized DQL\nframework to improve the security and efﬁciency for trust-\nbased data exchange, sharing, and delivery in mobile social\nnetworks through the resource allocation and optimization\nof MEC allocation, caching, and D2D (Device-to-Device)\nnetworking.\nB. Data and Computation Ofﬂoading\nWith limited computation, memory and power supplies,\nIoT devices such as sensors, wearable devices, and handheld\ndevices become the bottleneck to support advanced applica-\ntions such as interactive online gaming and face recognition.\nTo address such a challenge, IoT devices can ofﬂoad the\ncomputational tasks to nearby MEC servers, integrated with\nthe BSs, Access Points (APs), and even neighboring Mobile\nUsers (MUs). As a result, data and computation ofﬂoading can\npotentially reduce the processing delay, save the battery en-\nergy, and even enhance security for computation-intensive IoT\napplications. However, the critical problem in the computation\nofﬂoading is to determine the ofﬂoading rate, i.e., the amount\nof computational workload, and choose the MEC server from\nall available servers. If the chosen MEC server experiences\nheavy workloads and degraded channel conditions, it may take\neven longer time for the IoT devices to ofﬂoad data and receive\nthe results from the MEC server. Hence, the design of an\nofﬂoading policy has to take into account the time-varying\nchannel conditions, user mobility, energy supply, computation\nworkload and the computational capabilities of different MEC\nservers.\nThe authors in [93] focus on minimizing the mobile user’s\ncost and energy consumption by ofﬂoading cellular trafﬁc to\nWLAN. Each mobile user can either access the cellular net-\nwork, or the complimentary WLAN as illustrated in Fig. 8(a),\nbut with different monetary costs. The mobile user also has to\npay a penalty if the data transmission does not ﬁnish before\nthe deadline. The mobile user’s data ofﬂoading decision can\nbe modeled as an MDP. The system state includes the mobile\nuser’s location and the remaining ﬁle size of all data ﬂows.\nThe mobile user will choose to transmit data through either\nWLAN or cellular network, and decide how to allocate channel\ncapacities to concurrent ﬂows. Without knowing the mobility\npattern in advance, the DQL is proposed for each mobile user\nto learn the optimal ofﬂoading policy from past experiences.\nCNNs are employed in the DQL to predict a continuous\nvalue of the mobile user’s remaining data. Simulation results\nreveal that the DQN-based scheme generally outperforms the\ndynamic programming algorithm for the MDP. The reason is\nthat the DQN can learn from experience while the dynamic\nprogramming algorithm cannot obtain the optimal policy with\nincorrect transition probability.\nThe allocation of limited computational resources at the\nMEC server is critical for cost and energy minimization. The\nauthors in [94] consider an MEC-enabled cellular system, in\nwhich multiple mobile users can ofﬂoad their computational\ntasks via wireless channels to one MEC server, co-located with\nthe cellular BS as shown in Fig. 8(b). Each mobile user has\na computational-intensive task, characterized by the required\ncomputational resources, CPU cycles, and the maximum tol-\nerable delay. The capacity of the MEC server is limited to\naccommodate all mobile users’ task loads. The bandwidth\nsharing between different mobile users’ ofﬂoading also affects\nthe overall delay performance and energy consumptions. The\nDQL is used to minimize the cost of delay and power\nconsumptions for all mobile users, by jointly optimizing the\nofﬂoading decision and computational resource allocation. The\nsystem states include the sum of cost of the entire system\nand the available computational capacity of the MEC server.\nThe action of BS is to determine the resource allocation and\n20\nofﬂoading decision for each mobile user. To limit the size of\naction space, a pre-classiﬁcation step is proposed to check the\nmobile users’ feasible set of actions.\nIn contrast to [94], multiple BSs in an ultra-dense network\nis considered in [95] and [96], as shown in Fig. 8(c), with\nthe objective of minimizing the long-term cost of delay in\ncomputation ofﬂoading. All computational tasks are ofﬂoaded\nto the shared MEC server via different BSs. Besides the\nallocation of computational resources and transmission con-\ntrol, the ofﬂoading policy also has to optimize the asso-\nciation between mobile users and the BSs. With dynamic\nnetwork conditions, the mobile users’ decision-making can\nbe formulated as an MDP. The system states are the channel\nconditions between the mobile user and the BSs, the states\nof energy and task queues. The cost function is deﬁned\nas a weighted sum of the execution delay, the handover\ndelay and the computational task dropping cost. The authors\nin [96] ﬁrstly propose a DDQN-based DQL algorithm to learn\nthe optimal ofﬂoading policy without knowing the network\ndynamics. By leveraging the additive structure of the utility\nfunction, the Q-function decomposition combined with the\nDDQN further leads to a novel online SARSA-based DRL\nalgorithm. Numerical experiments show that the new algorithm\nachieves a signiﬁcant improvement in computation ofﬂoading\nperformance compared with the baseline policies, e.g., the\nDQN-based DQL algorithm and some heuristic ofﬂoading\nstrategies without learning. The high density of SBSs can\nrelieve the data ofﬂoading pressure in peak trafﬁc hours but\nconsume a large amount of energy in off-peak time. Therefore,\nthe authors in [97], [98], and [99] propose a DQL-based\nstrategy for controlling the (de)activation of different SBSs\nto minimize the energy consumption without compromising\nthe quality of provisioning. In particular, in [97], the on/off\ndecision framework uses a DQL scheme to approximate both\nthe policy and value functions in an actor-critic method.\nThe reward of the DQL agent is deﬁned as a cost function\nrelating to energy consumption, QoS degradation, and the\nswitching cost of SBSs. The DDPG approach is also employed\ntogether with an action reﬁnement scheme to expedite the\ntraining process. Through extensive numerical simulations, the\nproposed scheme is shown to greatly outperform other baseline\nmethods in terms of both energy and computational efﬁciency.\nWith a similar model to that in [96], computation ofﬂoading\nﬁnds a proper application for cloud-based malware detection\nin [100]. A review of the threat models and the RL-based\nsolutions for security and privacy protection in mobile ofﬂoad-\ning and caching are discussed in [101]. With limited energy\nsupply, computational resources, and channel capacity, mobile\nusers cannot always update the local malware database and\nprocess all application data in time and thus are vulnerable to\nzero-day attacks [102]. By leveraging the remote MEC server,\nall mobile users can ofﬂoad their application data and detection\ntasks via different BSs to the MEC/security server with larger\nand more sophisticated malware database, more computa-\ntional capabilities, and powerful security services. This can\nbe modeled by a dynamic malware detection game in which\nmultiple mobile users interact with each other in resource\ncompetition, e.g., the allocation of wireless channel capacities\nand the computational capabilities of the MEC/security server.\nA DQL scheme is proposed for each mobile user to learn its\nofﬂoading data rate to the MEC/security server. The system\nstates include the channel state and the size of application\ntraces. The objective is to optimize the detection accuracy of\nthe security server, which is deﬁned as a concave function\nin the total amount of malware samples. The Q-value is\nestimated by using a CNN in the DQL framework. The\nauthors also propose the hotbooting Q-learning technique that\nprovides a better initialization for Q-learning by exploiting\nthe ofﬂoading experiences in similar scenarios. It can save\nexploration time at the initial stage and accelerate the learning\nspeed compared with a standard Q-learning algorithm with all-\nzero initialization of the Q-value [103]. The proposed DQL\nscheme not only improves the detection speed and accuracy,\nbut also increases the mobile users’ battery life. The simulation\nresults reveal that compared with the hotbooting Q-learning,\nthe DQL-based malware detection has the faster learning rate,\nthe higher accuracy, and the shorter detection delay.\nMultiple MEC servers have been considered in [104], [105],\nas illustrated in Fig. 8(d). The authors in [104] aim to\ndesign optimal ofﬂoading policy for IoT devices with energy\nharvesting capabilities. The system consists of multiple MEC\nservers, such as BSs and APs, with different capabilities\nin computation and communications. The IoT devices are\nequipped with energy storage and energy harvesters. They\ncan execute computational tasks locally and ofﬂoad the tasks\nto the MEC servers. The IoT device’s ofﬂoading decision\ncan be formulated as an MDP. The system states include the\nbattery status, the channel capacity, and the predicted amount\nof harvested energy in the future. The IoT device evaluates\nthe reward based on the overall delay, energy consumption,\nthe task drop loss and the data sharing gains in each time\nslot. Similar to [100], the authors in [104] enhance Q-learning\nby the hotbooting technique to save the random exploration\ntime at the beginning of learning. The authors also propose a\nfast DQL ofﬂoading scheme that uses hotbooting to initialize\nthe CNN and accelerates the learning speed.\nThe authors\nin [105] view the MEC-enabled BSs as different physical\nmachines constituting a part of the cloud resources. The cloud\noptimizes the MUs’ computation ofﬂoading to different virtual\nmachines residing on the physical machines. A two-layered\nDQL algorithm is proposed for the ofﬂoading problem to\nmaximize the utilization of cloud resources. The system state\nrelates to the waiting time of each computational task and the\nnumber of virtual machines. The ﬁrst layer is implemented by\na CNN-based DQL framework to estimate an optimal cluster\nfor each computational task. Different clusters of physical\nmachines are generated based on the K-NN algorithm. The\nsecond layer determines the optimal serving physical machine\nwithin the cluster by Q-learning method.\nThe aforementioned works all focus on data or computation\nofﬂoading in cellular system via BSs to remote MEC servers,\ne.g., [93]–[96], [100], [104], [105]. In [107] and [106], the\nauthors study QoS-aware computation ofﬂoading in an ad-hoc\nmobile network. By making a certain payment, the mobile user\ncan ofﬂoad its computational tasks to nearby mobile users\nconstituting a mobile cloudlet, as shown in Fig. 8(d). Each\n21\nCloud\nCellular BS\nMEC Server\n(b)\nMEC Server\nCloud\n(c)\nCellular BS\nWLAN AP\nCloud\n(a)\nMobile user\nData/Energy \nqueue\nCloud\n(d)\nMobile \ncloudlets\nFig. 8: Data/computation ofﬂoading models in cellular networks: (a)\nOfﬂoading cellular trafﬁc to WLAN [93], (b) Ofﬂoading to a single\nMEC-enabled BS [94], (c) Ofﬂoading to one shared MEC server\nvia multiple BSs [95], [96], [100], (d) Ofﬂoading to multiple MEC-\nenabled BSs [104], [105] and mobile cloudlets [106], [107].\nmobile user has a ﬁrst-in-ﬁrst-out queue with limited buffer\nsize to store the arriving tasks arriving as a Poisson process.\nThe mobile user selects nearby cloudlets within D2D com-\nmunication range for task ofﬂoading. The ofﬂoading decision\ndepends on the states including the number of remaining tasks,\nthe quality of the links between mobile users and the cloudlet,\nand the availability of the cloudlet’s resources. The objective\nis to maximize a composite utility function, subject to the\nmobile user’s QoS requirements, e.g., energy consumption and\nprocessing delay. The utility function is ﬁrstly an increasing\nfunction of the total number of tasks that have been processed\neither locally or remotely by the cloudlets. It is also related to\nthe user’s beneﬁt such as energy efﬁciency and payment for\ntask ofﬂoading. This problem can be formulated as an MDP,\nwhich can be solved by linear programming and Q-learning\napproaches, depending on the availability of information about\nthe state transition probabilities. This work is further enhanced\nby leveraging DNN or DQN to learn the decision strategy\nmore efﬁciently. A similar model is studied in [108], where the\ncomputation ofﬂoading is formulated as an MDP to minimize\nthe cost of computation ofﬂoading. The solution to the MDP\ncan be used to train a DNN by supervised learning. The well-\ntrained DNN is then applied to unseen network conditions for\nreal-time decision-making. Simulation results show that the\nuse of deep supervised learning achieves signiﬁcant perfor-\nmance gain in ofﬂoading accuracy and cost saving.\nData and computation ofﬂoading is also used in fog com-\nputing. The mobile application demanding a set of data and\ncomputational resources can be hosted in a container, e.g.,\nvirtual machine of a fog node. With user’s mobility, the\ncontainer has to be migrated or ofﬂoaded to other nodes and\ndynamically consolidated. With the container migration, some\nnodes with low resource utilization can be switched off to\nreduce power consumption. The authors in [109] model the\ncontainer migration as a multi-dimensional MDP, which is\nsolved by the DQL. The system states consist of the delay,\nthe power consumption and the migration cost. The action\nincludes the selection policy that selects the containers to be\nemigrated from each source node, and the allocation policy\nthat determines the destination node of each container. The\naction space can be optimized for more efﬁcient exploration by\ndividing fog nodes into under-utilization, normal-utilization,\nand over-utilization groups. By powering off under-utilization\nnodes, all their containers will be migrated to other nodes\nto reduce power consumption. The training process is also\noptimized by using DDQN and PER which assigns different\npriorities to the transitions in experience memory. This helps\nthe DQL agent at each fog node to perform better in terms\nof faster learning speed and more stability. Simulation results\nreveal that the DQL scheme achieves fast decision-making and\noutperforms the existing baseline approaches signiﬁcantly in\nterms of delay, power consumption, and migration cost.\nSummary: This section reviews the applications of the\nDQL for wireless caching and data/computation ofﬂoading,\nwhich are inherently coupled with networking and allocation\nof channel capacity, computational resources, and caching\ncapabilities, etc. We observe that the DQL framework for\ncaching is typically centralized and mostly implemented at\nthe network controller, e.g., the BS, service provider, and\ncentral scheduler, which is more powerful in information col-\nlection and cross-layer policy design. On the contrary, the end\nusers have more control over their ofﬂoading decisions, and\nhence we observe more popular implementation of the DQL\nagent at local devices, e.g., mobile users, IoT devices, and\nfog nodes. Though an orchestration of networking, caching,\ndata and computation ofﬂoading in one uniﬁed DQL frame-\nwork is promising for network performance maximization,\nwe face many challenges in designing highly-stable and fast-\nconvergent learning algorithms, due to excessive delay and\nunsynchronized information collection from different network\nentities.\nV. NETWORK SECURITY AND CONNECTIVITY\nPRESERVATION\nFuture networks become more decentralized and ad-hoc in\nnature which are vulnerable to various attacks such as Denial-\nof-Service (DoS) and cyber-physical attack. Recently, the DQL\nhas been used as an effective solution to avoid and prevent the\nattacks. In this section, we review the applications of DQL in\naddressing the following security issues:\n• Jamming attack: In the jamming attack, attackers as\njammers transmit Radio Frequency (RF) jamming signals\nwith high power to cause interference to the legitimate\ncommunication channels, thus reducing the SINR at\nlegitimate receivers. Anti-jamming techniques such as the\nfrequency hopping [110] and user mobility, i.e., moving\nout from the heavy jamming area, have been commonly\nused. However, without being aware of the radio channel\n22\nTABLE IV: A summary of approaches using DQL for caching and ofﬂoading.\nISSUES\nREF.\nMODEL\nLEARNING\nALGORITHMS\nAGENT\nSTATES\nACTIONS\nREWARDS\nNETWORKS\nWireless proactive caching\n[70]\nMDP\nDQN using\nactor-critic,\nDDPG\nBase station\nCached contents and\nrequested content\nReplace selected\ncontent or not\nCache hit rate\n(score 1 or 0)\nCRN\n[84]\nMDP\nDQN using\nFNN\nBase station\nChannel states and\ncomputational capabilities\nUser association,\ncomputational\nunit, content\ndelivery\nEnergy consumption\nCRN\n[74]\nMDP\nDQN using\nNAFs\nCloud\ndatabase\nEncoding of a query, query\ncache miss rate\nCache expiration\ntimes\nCache hit rates, CDN\nutilization\nCloud\ndatabase\n[76]\n[77]\nMDP\nDQN using\nFNN\nCentral\nscheduler\nChannel coefﬁcients, cache\nstate\nActive users and\nresource allocation\nNetwork throughput\nMU MIMO\nsystem\n[78]\nMDP\nDQN using\nCNN\nCentral\nscheduler\nChannel coefﬁcients, cache\nstate\nActive users and\nresource allocation\nNetwork throughput\nMU MIMO\nsystem\n[79]\nMDP\nDDQN\nService\nprovider\nConditions of cache nodes,\ntransmission rates of content\nchunks\nThe content\nchunks to cache\nand to remove\nNetwork cost, QoE\nContent\ncentric IoT\n[81]\nMDP\nDQN using\nLSM and ESN\nBase station\nHistorical content request\nUser association,\ncached contents\nand formats\nReliability\nCellular\nsystem\n[86]\n[89]\nMDP\nDQN using\nCNN\nService\nprovider\nAvailable BS, MEC, and\ncache\nUser association,\ncaching, and\nofﬂoading\nComposite revenue\nVehicular\nad hoc\nnetwork\n[85]\nMDP\nDQN using\nFNN\nService\nprovider\nAvailable BS, MEC, and\ncache\nUser association,\ncaching, and\nofﬂoading\nComposite revenue\nVehicular\nad hoc\nnetwork\n[87]\nMDP\nDDQN and\ndueling DQN\nService\nprovider\nAvailable BS, MEC, and\ncache\nUser association,\ncaching, and\nofﬂoading\nComposite revenue\nVehicular\nad hoc\nnetwork\n[91]\nMDP\nDQN using\nCNN\nBase station\nChannel state, computational\ncapability, content/version\nindicator, and the trust value\nUser association,\ncaching, and\nofﬂoading\nRevenue\nMobile\nsocial\nnetwork\nData and computation ofﬂoading\n[93]\nMDP\nDQN using\nCNN\nMobile user\nUser’s location and remaining\nﬁle size\nIdle, transmit via\nWLAN or cellular\nnetwork\nTotal data rate\nCellular\nsystem\n[94]\nMDP\nDQN using\nFNN\nBase station\nSum of cost and\ncomputational capacity of the\nMEC server\nOfﬂoading\ndecision and\nresource allocation\nSum of cost of delay\nand energy\nconsumption\nCellular\nsystem\n[95]\nMDP\nDQN using\nFNN\nMobile user\nChannel qualities, states of\nenergy and task queues\nOfﬂoading and\nresource allocation\nLong term cost\nfunction\nCellular\nsystem\n[96]\nMDP\nDDQN,\nSARSA\nMobile user\nChannel qualities, states of\nenergy and task queues\nOfﬂoading\ndecision and\ncomputational\nresource allocation\nLong term cost\nfunction\nCellular\nsystem\n[100]\nGame\nDQN using\nCNN,\nhotbooting\nQ-learning\nMobile user\nChannel states, size of App\ntraces\nOfﬂoading rate\nUtility related to\ndetection accuracy,\nresponse speed, and\nthe transmission cost\nCellular\nsystem\n[109]\nMDP\nDDQN\nFog node\nDelay, container’s location\nand resource allocation\nContainer’s next\nlocation\nComposite utility\nrelated to delay,\npower consumption,\nand migration cost\nFog\ncomputing\nmodel and the jamming methods, it is challenging for\nthe users to choose an appropriate frequency channel as\nwell as to determine how to leave and avoid the attack.\nDQL enables the users to learn an optimal policy based\non their past observations, and thus DQL can be used to\naddress the above challenge.\n• Cyber-physical attack: The cyber-physical attack is an\nintegrity attack in which an attacker manipulates data\nto alter control signals in the system. This attack of-\nten happens in autonomous systems such as Intelligent\nTransportation Systems (ITSs) and increases the risk\nof accidents to Autonomous Vehicles (AVs). The DQL\nallows the AVs to learn optimal actions based on their\ntime-varying observations of the attacker’ activities. Thus,\nthe DQL can be used to achieve robust and dynamic\ncontrol of the AV to the attacks.\n• Connectivity preserving: This refers to maintaining the\nconnectivity among the robots, e.g., UAVs, to support\nthe communication and exchange of information among\nthem. The system and network environment is generally\ndynamic and complex, and thus the DQL which allows\neach robot to make dynamic decisions based on its state\ncan be effectively used to preserve the connectivity in the\nsystem.\n23\nBS\nAttacker\nPU\nAttacker\nPU\nSU \n(agent)\nReceiver\nBS\nMove ?\nJamming\nJamming\nFig. 9: Jamming attack in cognitive radio network [111].\nA. Network Security\nThis section discusses the applications of DQL to address\nthe jamming attack and the cyber-physical attack.\n1) Jamming Attack: A pioneer work using the DQL for\nthe anti-jamming is [111]. The network model is a Cognitive\nRadio Network (CRN) as shown in Fig. 9 which consists of\none Secondary User (SU), multiple Primary Users (PUs), and\nmultiple jammers. The network has a set of frequency channels\nfor hopping. At each time slot, each jammer can arbitrarily\nselect one of the channels to send its jamming signal, and\nthe SU, i.e., the agent, needs to choose a proper action based\non the SU’s current state. The action is (i) selecting one of\nthe channels to send its signals or (ii) leaving the area to\nconnect to another BS. The jammers are assumed to avoid\ncausing interference to the PUs, and thus the SU’s current\nstate consists of the number of PUs and the discretized SINR\nof the SU signal at the last time slot. The objective of the\nSU is to maximize its expected discounted utility over time\nslots. Note that when the SU chooses to leave the area to\nconnect to another BS, it spends a mobility cost. Thus, the\nutility is deﬁned as a function of the SINR of the SU signal\nand the mobility cost. Since the number of frequency channels\nmay be large that results in a large action set, the CNN is\nused for the DQL to quickly learn the optimal policy. As\nshown in the simulation results, the proposed DQL has a\nfaster convergence speed than that of the Q-learning algorithm.\nMoreover, considering the scenario with two jammers, the\nproposed DQL outperforms the frequency-hopping method in\nterms of the SINR and the mobility cost.\nThe model in [111] is constrained to two jammers. As the\nnumber of jammers in the network increases, the proposed\nscheme may not be effective. The reason is that it becomes\nhard for the SU to ﬁnd good actions when the number\nof jammed channels increases. An appropriate solution, as\nproposed in [112], allows the receiver of the SU to leave its\ncurrent location. Since the leaving incurs the mobility cost, the\nreceiver, i.e., the agent, needs an optimal policy, i.e., staying\nat or leaving the current location, to maximize its utility. In\nthis scenario, the DQL based on CNN can be used for the\nreceiver to ﬁnd the optimal action to maximize its expected\nutility. Here, the utility and state of the receiver are essentially\ndeﬁned similarly to that of the agent in [111]. In particular,\nthe state includes the discretized SINR of the signal measured\nby the receiver at the last time slot.\nThe above approaches, i.e., in [111] and [112], deﬁne states\nof the agents based on raw SINR values of the signals. In\npractical wireless environments, the number of SINR values\nmay be large and even inﬁnite. Moreover, the raw SINR can be\ninaccurate and noisy. To cope with the challenge of the inﬁnite\nnumber of states, the DQL can use a recursive Convolutional\nNeural Network (RCNN) as proposed in [113]. By using\nthe pre-processing layer and recursive convolution layers, the\nRCNN is able to remove noise from the network environment\nand extract useful features of the SINR, i.e., discrete spectrum\nsample values greater than a noise threshold, thus reducing\nthe computational complexity. The network model and the\nproblem formulation considered in [113] are similar to those\nin [111]. However, instead of directly using the raw SINR, the\nstate of the SU is the extracted features of the SINR. Also,\nthe action of the SU includes only frequency-hopping decision.\nThe simulation results show that the proposed DQL based on\nthe RCNN can converge in both ﬁxed and dynamic jamming\nscenarios while the Q-learning cannot converge in the dynamic\njamming one. Furthermore, the proposed DQL can achieve\nthe average throughput close to that of the optimal scheme,\ni.e., an anti-jamming scheme with completely known jamming\nactions.\nInstead of ﬁnding the frequency-hopping decisions, the\nauthors in [114] propose the use of DQL to ﬁnd an optimal\npower control policy for the anti-jamming. The model is an\nIoT network including IoT devices and one jammer. The\njammer can observe the communications of the transmitter and\nchooses a jamming strategy to reduce the SINR at the receiver.\nThus, the transmitter chooses an action, i.e., transmit power\nlevel, to maximize its utility. Here, the utility is the difference\nbetween the SINR and the energy consumption cost due to the\ntransmission. Note that choosing the transmit power impacts\nthe future jamming strategy, and thus the interaction between\nthe transmitter and the jammer can be formulated as an MDP.\nThe transmitter is the agent, and the state is SINR measured\nat its receiver at the last time slot. The DQN using the CNN\nis then adopted to ﬁnd an optimal power control policy for the\ntransmitter to maximize its expected accumulated discounted\nreward, i.e., the utility, over time slots. The simulation results\nshow that the proposed DQL can improve the utility of the\ntransmitter up to 17.7% compared with the Q-learning scheme.\nAlso, the proposed DQL reduces the utility of the jammer\naround 18.1% compared with the Q-learning scheme.\nTo prevent the jammer’s observations of communications,\nthe transmitter can change its communication strategy, e.g., by\nusing relays that are far from the jamming area. The relays can\nbe UAVs as proposed in [115]. The model consists of one UAV,\ni.e., a relay, one jammer, one mobile user and its serving BS\n(see Fig. 10). The mobile user transmits messages to its server\nvia the serving BS. In the case that the serving BS is heavily\njammed, the UAV helps the mobile user to relay the messages\nto the server through a backup BS. In particular, depending\non the SINR and Bit Error Rate (BER) values sent from the\nserving BS, the UAV as an agent decides the relay power level\nto maximize its utility, i.e., the difference between the SINR\nand the relay cost. The relay power level can be considered to\nbe the UAV’s actions, and the SINR and BER are its states. As\n24\nServing BS\nAttacker\nJamming\nUser\nRelay UAV \n(agent)\nServer (receiver)\nBackup BS \nMessages\nMessages\nJamming\n Compromised \nUAV\nMessages\nFig. 10: Anti-jamming scheme based on UAV [115].\nsuch, the next state observed by the UAV is independent of all\nthe past states and actions. The problem is formulated as an\nMDP. To quickly achieve the optimal relay policy for the UAV,\nthe DQL based on CNN is then adopted. The simulation results\nin [115] show that the proposed DQL scheme takes only 200\ntime slots to converge to the optimal policy, which is 83.3%\nless than that of the relay scheme based on Q-learning [116].\nMoreover, the proposed DQL scheme reduces the BER of the\nuser by 46.6% compared with the hill climbing-based UAV\nrelay scheme [117].\nThe scheme proposed in [115] assumes that the relay UAV is\nsufﬁciently far from the jamming area. However, as illustrated\nin Fig. 10, the attacker can use a compromised UAV close\nto the relay UAV to launch the jamming attack to the relay\nUAV. In such a scenario, the authors in [118] show that the\nDQL can still be used to address the attack. The system model\nis based on physical layer security and consists of one UAV\nand one attacker. The attacker is assumed to be “smarter” than\nthat in the model in [115]. This means that the attacker can\nobserve channels that the UAV uses to communicate with the\nBS in the past time slots and then chooses jamming power\nlevels on the target channels. Therefore, the UAV needs to\nﬁnd a power allocation policy, i.e., transmit power levels on\nthe channels, to maximize the secrecy capacity of the UAV-\nBS communication. Similar to [115], the DQL based on CNN\nis used which enables the UAV to choose its actions, i.e.,\ntransmit power levels on the channels, based on its state, i.e.,\nthe attacker’s jamming power level in the last time slot. The\nreward is the difference between the secrecy capacity of the\nUAV and BS and the energy consumption cost.\nThe simulation results in [118] show that the proposed DQL\ncan improve the UAV’s utility up to 13% compared with the\nbaseline scheme [119] which uses the Win or Learn Faster-\nPolicy Hill Climbing (WoLF-PHC) to prevent the attack. Also,\nthe safe rate of the UAV, i.e., the probability that the UAV\nis attacked, obtained by the proposed DQL is 7% higher\nthan that of the baseline. However, the proposed DQL is\napplied only to a single-UAV system. For the future work,\nscenarios with multiple UAVs need to be considered. In such a\nAttacker\nFaulty data\nSensing\ndata\nSensor\nSensor\nAV\nAV(agent)\nSafe spacing\nAV\nSensor\nSensing\ndata\nFig. 11: Car-following model with cyber-physical attack.\nscenario, more computational overhead is expected and multi-\nagent DQL algorithms can be applied.\n2) Cyber-Physical Attack: In autonomous systems such as\nITSs, the attacker can seek to inject faulty data to information\ntransmitted from the sensors to the AVs. The AVs which\nreceive the injected information may inaccurately estimate\nthe safe spacing among them. This increases the risk of\nAV accidents. Vehicular communication security algorithms,\ne.g., [120], can be used to minimize the spacing deviation.\nHowever, the attacker’s actions in these algorithms are as-\nsumed to be stable which may not be applicable in practical\nsystems. The DQL that enables the AVs to learn optimal ac-\ntions based on their time-varying observations of the attacker’\nactions can be thus used.\nThe ﬁrst work using the DQL for the cyber-physical attack\nin an ITS can be found in [121]. The system is a car-\nfollowing model [122] of the General Motors as shown in\nFig. 11. In the model, each AV updates its speed based\non measurement information received from the closest road\nsmart sensors. The attacker attempts to inject faulty data to\nthe measurement information. However, the attacker cannot\ninject the measurements of different sensors equally due to\nits resource constraint. Thus, the AV can choose less-faulty\nmeasurements by selecting a vector of measurement weights.\nThe objective of the attacker is to maximize the deviation, i.e.,\nthe utility, from the safe spacing between the AV and its nearby\nAV while that of the AV is to minimize the deviation. The\ninteraction between the attacker and the AV can be modeled\nas a zero-sum game. The authors in [121] show that the DQL\ncan be used to ﬁnd the equilibrium strategies. In particular,\nthe action of the AV is to choose a weight vector. Its state\nincludes the past actions, i.e., the weight vectors, and the\npast deviation values. Since the actions and deviations have\ncontinuous values, the state space is inﬁnite. Thus, LSTM\nunits that are able to extract useful features are adopted for the\nDQL to reduce the state space. The simulation results show\nthat by using the past actions and deviations for learning the\nattacker’s action, the proposed DQL scheme can guarantee\na lower steady-state deviation than the Kalmar ﬁlter-based\nscheme [120]. Moreover, by using the LSTM units, the results\nshow that the proposed DQL scheme can converge much faster\nthan the baseline scheme.\nAnother work that uses the LSTM to extract useful features\nfrom the measurement information to detect the cyber-physical\n25\nIoT \ndevice\nLSTM\nCloud \nLSTM\nLSTM\nIoT \ndevice\nIoT\ndevice\nSignal\nProbs. of attacking IoT devices\nActions on \nIoT devices\nSignal\nSignal\nLSTM\nFig. 12: Cyber-physical detection in IoT systems using DQL.\nattack is proposed in [123]. The model is an IoT system\nincluding a cloud and a set of IoT devices. The IoT devices\ngenerate signals and transmit the signals to the cloud (see\nFig. 12). The cloud uses the received signals for estimation and\ncontrol of the IoT devices’ operation. An attacker can launch\nthe cyber-physical attack by manipulating the IoT devices’\noutput signals that causes control errors at the cloud and\ndegrades the performance of the IoT system. To detect the\nattack, the cloud uses LSTM units to extract stochastic features\nor ﬁngerprints such as ﬂatness, skewness, and kurtosis, of\nthe IoT devices’ signals. The cloud sends the ﬁngerprints\nback to the IoT devices, and the IoT devices embed, i.e.,\nwatermark, the ﬁngerprints inside the signals. The cloud uses\nthe ﬁngerprints to authenticate the IoT devices’ signals to\ndetect the attack.\nThe algorithm proposed in [123] is also called dynamic\nwatermarking [124] which is able to detect the cyber-physical\nattack and to prevent eavesdropping attacks. However, the\nalgorithm requires large computational resources at the cloud\nfor the IoT device signal authentication. Consequently, the\ncloud can only authenticate a limited number of vulnerable\nIoT devices. The cloud can choose the vulnerable IoT devices\nby observing their security status. However, this can be im-\npractical since the IoT devices may not report their security\nstatus. Thus, the authors in [125] propose to use the DQL that\nenables the cloud to decide which IoT devices to authenticate\nwith the incomplete information. Since IoT devices with more\nvaluable data are likely to be attacked, the reward is deﬁned\nas a function of data values of IoT devices. The cloud’s state\nincludes attack actions of the attacker on the IoT devices in\nthe past time slots. The actions of the attacker on the IoT\ndevices can be obtained by using the dynamic watermarking\nalgorithm in [123] (see Fig. 12). The DQL then uses an LSTM\nunit to ﬁnd the optimal policy. The input of the LSTM unit\nis the state of the cloud, and the output includes probabilities\nof attacking the IoT devices. By using a real dataset from the\naccelerometers, the simulation results show that the proposed\nDQL can improve the cloud’s utility up to 30% compared with\nthe case in which the cloud chooses the IoT devices with equal\nprobability.\nBase station\nControl \nsignals\nConnectivity\nConnectivity\nLeader\nFollower\nFollower\nHandover\nBase station\nFig. 13: Connectivity preservation of a multi-UAV network.\nB. Connectivity Preservation\nMulti-robot systems such as multi-UAV cooperative net-\nworks have been widely applied in many ﬁelds such as\nmilitary, e.g., enemy detecting. In the cooperative multi-robot\nsystem, the connectivity among the robots, e.g., UAVs in\nFig 13, is required to enable the communication and exchange\nof information. To tackle the connectivity preservation prob-\nlem, the Artiﬁcial Potential Field (APF) algorithm [126] has\nbeen used. However, the algorithm cannot be directly adopted\nwhen the robots are undertaking missions in dynamic and\ncomplex environments. The DQL which allows each robot\nto make dynamic decisions based on its own state can be\neffectively applied to preserve the connectivity in the multi-\nrobot system. Such an approach is proposed in [127].\nThe model in [127] consists of two robots or UAVs, i.e., one\nleader robot and one follower robot. In the model, a central\ncontrol, i.e., a ground BS, adjusts the velocity of the follower\nsuch that the follower stays in the communication range of the\nleader at all time (see Fig 13). The connectivity preservation\nproblem can be thus formulated as an MDP. The agent is the\nBS, and the states are the relative position and the velocity\nof the leader with respect to the follower. The action space\nconsists of possible velocity values of the follower. Taking\nan action returns a reward which is +1 if the follower is\nin the range of the leader, and -1 otherwise. A DQN using\nFNN is used which enables the BS to learn an optimal policy\nto maximize the expected discounted cumulative reward. The\ninput of the DQN includes the states of the two robots, and\nthe output is the action space of the follower. The simulation\nresults show that the proposed scheme can achieve better\nconnectivity between the two robots than that of the APF\nmethod. However, a general scenario with more than one\nleader and one follower needs to be investigated.\nConsidering the general scenario, the authors in [128]\naddress the connectivity preservation between multiple lead-\ners and multiple followers. The robot system is deﬁnitely\nconnected if any two robots are connected via a direct link\nor multi-hop link. To express the connectivity in such a\nrobot system, the authors introduce the concept of algebraic\nconnectivity [129] which is the second smallest eigenvalue\nof a Laplacian matrix. The robot system is connected if the\nalgebraic connectivity of the system is positive. Thus, the\n26\nproblem is to adjust the velocity of the followers such that the\nalgebraic connectivity is positive over time slots. This problem\ncan be formulated as an MDP in which the agent is the ground\nBS, the state is a combination of the states of all robots, the\naction is a set of possible velocity values for the followers.\nThe reward is +1 if the algebraic connectivity of the system\nincreases or holds, and becomes a penalty of -1 if the algebraic\nconnectivity decreases. Similar to [127], a DQN is adopted.\nDue to the large action space of the followers, the actor-critic\nneural network [26] is used. The simulation results show that\nthe followers always follow the motion of the leaders even\nif the leaders’ trajectory dynamically changes. However, the\nproposed DQN requires more time to converge than that in\n[127] because of the presence of more followers.\nThe proposed schemes in [127] and [128] do not consider\na minimum distance between the leaders and followers. The\nleaders and followers can collide with each other if the\ndistance between them is too short. Thus, the BS needs to\nguarantee the minimum distance between them. One solution\nis to have the minimum distance in the reward as proposed\nin [130]. In particular, if the leader is too close to its follower,\nthe reward of the system is penalized regarding the minimum\ndistance. The DQL algorithm proposed in [128] is then used\nsuch that the BS learns proper actions, e.g., turning left and\nright, to maximize the cumulative reward.\nWhen BSs are densely deployed, the UAVs or mobile\nusers need to trigger a frequent handover to preserve the\nconnectivity. The frequent handover increases communication\noverhead and energy consumption of the mobile users, and\ninterrupts data ﬂows. Thus, it is essential to maintain an\nappropriate handover rate. The authors in [27] address the\nhandover decision problem in an ultra-density network. The\nnetwork model consists of multiple mobile users, SBSs, and\none central controller. At each time slot, the user needs to\ndecide its serving SBS. The handover decision process can\nbe modeled as an MDP, and the DQL is adopted to ﬁnd an\noptimal handover policy for each user to minimize the number\nof handover occurrences while ensuring certain throughput.\nThe state of the user, i.e., the agent, includes reference signal\nquality received from candidate SBSs and the last action of the\nuser. The reward is deﬁned as the difference between the data\nrate of the user and its energy consumption for the handover\nprocess. Given a high density of users, the DQL using A3C\nand LSTM is adopted to ﬁnd the optimal policy in short\ntraining time. The simulation results show that the proposed\nDQL can achieve better throughput and lower handover rate\nthan those of the upper conﬁdence bandit algorithm [131] with\nsimilar training time.\nTo enhance the reliability of the communication between\nthe SBSs and the mobile users, the SBSs should be able\nto handle network faults and failure automatically as self-\nhealing. The DQL can be applied as proposed in [132] to make\noptimal parameter adjustments based on the observation of the\nnetwork performance. The model is the 5G network including\none MBS. The MBS as an agent needs to handle network\nfaults such as transmit diversity faults and antenna azimuth\nchange, e.g., because of wind. These faults are represented as\nthe MBS’s state that is the number of active alarms. Based on\nthe alarms, the MBS can take actions including (i) enabling\nthe transmit diversity and (ii) setting the antenna azimuth\nto default value. The reward that the MBS receives is the\nscores, e.g., -1, 0, and +1, depending on the number of faults\nhappening. The DQL is used to learn the optimal policy. The\nsimulation results show that the proposed DQL can achieve\nnetwork throughput close to that of the oracle-based self-\nhealing, i.e., the upper-performance bound, but incurs less fault\nmessage passing overhead.\nSummary: This section reviews applications of DQL for the\nnetwork security and connectivity preservation. The reviewed\napproaches are summarized along with the references in Ta-\nble V. We observe that the CNN is mostly used for the DQL to\nenhance the network security. Moreover, DQL approaches for\nthe anonymous system such as robot systems and ITS receive\nmore attentions than other networks. However, the applications\nof DQL for the cyber-physical security are relatively few and\nneed to be investigated.\nVI. MISCELLANEOUS ISSUES\nThis section reviews applications of DRL to solve some\nother issues in communications and networking. The issues\ninclude (i) trafﬁc engineering and routing, (ii) resource sharing\nand scheduling, and (iii) data collection.\nA. Trafﬁc Engineering and Routing\nTrafﬁc Engineering (TE) in communication networks refers\nto Network Utility Maximization (NUM) by optimizing a path\nto forward the data trafﬁc, given a set of network ﬂows from\nsource to destination nodes. Traditional NUM problems are\nmostly model-based. However, with the advances of wire-\nless communication technologies, the network environment\nbecomes more complicated and dynamic, which makes it hard\nto model, predict, and control. The recent development of\nDQL methods provides a feasible and efﬁcient way to design\nexperience-driven and model-free schemes that can learn and\nadapt to the dynamic wireless network from past observations.\nRouting optimization is one of the major control problems\nin trafﬁc engineering. The authors in [133] present the ﬁrst\nattempt to use the DQL for the routing optimization. Through\nthe interaction with the network environment, the DQL agent\nat the network controller determines the paths for all source-\ndestination pairs. The system state is represented by the\nbandwidth request between each source-destination pair, and\nthe reward is a function of the mean network delay. The DQL\nagent leverages the actor-critic method for solving the routing\nproblem that minimizes the network delay, by adapting routing\nconﬁgurations automatically to current trafﬁc conditions. The\nDQL agent is trained using the trafﬁc information generated by\na gravity model [134]. The routing solution is then evaluated\nby OMNet+ discrete event simulator [135]. The well-trained\nDQL agent can produce a near-optimal routing conﬁguration\nin a single step and thus the agent is agile for real-time network\ncontrol. The proposed approach is attractive as the traditional\noptimization-based techniques require a large number of steps\nto produce a new conﬁguration. The authors in [136] consider\n27\nTABLE V: A summary of approaches using DQL for network security and connectivity preservation.\nISSUES\nREF.\nMODEL\nLEARNING\nALGORITHMS\nAGENT\nSTATES\nACTIONS\nREWARDS\nNETWORKS\nNetwork security\n[111]\nGame\nDQN using\nCNN\nSecondary\nuser\nNumber of PUs and signal\nSINR\nChannel selection\nand leaving\ndecision\nSINR and mobility\ncost\nCRN\n[112]\nGame\nDQN using\nCNN\nReceiving\ntransducer\nSignal SINR\nStaying and\nleaving decisions\nSINR and mobility\ncost\nUnderwater\nacoustic\nnetwork\n[113]\nMDP\nDQN using\nRCNN\nSU\nSignal SINR\nChannel selection\nSINR and mobility\ncost\nCRN\n[114]\nMDP\nDQN using\nCNN\nTransmit\nIoT device\nSignal SINR\nChannel selection\nSINR and energy\nconsumption cost\nIoT\n[115]\nMDP\nDQN using\nCNN\nRelay UAV\nSignal SINR and BER\nRelay power\nSINR and relay cost\nUAV\n[118]\nMDP\nDQN using\nCNN\nTransmit\nUAV\nJamming power\nTransmit power\nSecrecy capacity and\nenergy consumption\ncost\nUAV\n[121]\nGame\nDQN using\nLSTM units\nAutonomous\nvehicle\nDeviation values\nMeasurement\nweight selection\nSafe spacing\ndeviation\nITS\n[125]\nGame\nDQN using\nLSTM units\nCloud\nAttack actions on IoT devices\nIoT device set\nselection\nIoT devices’ data\nvalues\nIoT\nConnectivity preservation\n[127]\nMDP\nDQN using\nFNN\nGround\nbase station\nRelative positions and the\nvelocity of robots\nVelocity decision\nSore +1 and -1\nRobot\nsystem\n[128]\nMDP\nDQN using\nA3C\nGround\nbase station\nRelative positions and the\nvelocity of robots\nVelocity decision\nSore +1 and -1\nRobot\nsystem\n[130]\nPOMDP\nDQN using\nA3C\nGround\nbase station\nInformation of distances\namong robots\nTurning left and\nturning right\ndecisions\nSore +1 and -1\nRobot\nsystem\n[27]\nMDP\nDQN using\nA3C and\nLSTM\nMobile\nusers\nReference signal received\nquality and the last action\nServing SBS\nselection\nData rate and energy\nconsumption\nUltra-dense\nnetwork\n[132]\nMDP\nDQN using\nCNN\nMBS\nThe number of active alarms\nEnabling transmit\ndiversity and\nchanging antenna\nazimuth\nScore -1, 0, +1, and\n+5\nSelf-\norganization\nnetwork\na similar network model with multiple end-to-end commu-\nnication sessions. Each source-destination pair has a set of\ncandidate paths that can transport the trafﬁc load. Experimental\nresults show that the conventional DDPG method does not\nwork well for the continuous control problem in [136]. One\npossible explanation is that DDPG utilizes uniform sampling\nfor experience replay, which ignores different signiﬁcance of\nthe transition samples.\nThe authors in [136] also combine two new techniques to\noptimize DDPG particularly for trafﬁc engineering problems,\ni.e., TE-aware exploration and actor-critic-based PER meth-\nods. The TE-aware exploration leverages the shortest path\nalgorithm and NUM-based solution as the baseline during\nexploration. The PER method is conventionally used in DQL,\ne.g., [79] and [109], while the authors in [136] integrate\nthe PER method with the actor-critic framework for the ﬁrst\ntime. The proposed scheme assigns different priorities to\ntransitions in the experience replay. Based on the priority, the\nproposed scheme samples the transitions in each epoch. The\nsystem state consists of throughput and delay performance of\neach communication session. The action speciﬁes the amount\nof trafﬁc load going through each of the paths. By learn-\ning the dynamics of network environment, the DQL agent\naims to maximize the total utility of all the communication\nsessions, which is deﬁned based on end-to-end throughput\nand delay [137]. Packet-level simulations using NS-3 [138],\ntested on well-known network topologies as well as random\ntopologies generated by BRITE [139], reveal that the proposed\nDQL scheme signiﬁcantly reduces the end-to-end delay and\nimproves the network utility, compared with the baseline\nschemes including DDPG and the NUM-based solutions.\nThe networking and routing optimization become more\ncomplicated in the UAV-based wireless communications. The\nauthors in [130] model autonomous navigation of one single\nUAV in a large-scale unknown complex environment as a\nPOMDP, which can be solved by actor-critic-based DRL\nmethod. The system state includes its distances and orientation\nangles to nearby obstacles, the distance and angle between its\npresent position and the destination. The UAV’s action is to\nturn left or right or keep ahead. The reward is composed of\nfour parts: an exponential penalty term if it is too close to\nany obstacles, a linear penalty term to encourage minimum\ntime delay, the transition and direction rewards if the UAV is\ngetting close to the target position in a proper direction. In-\nstead of using conventional DDPG for continuous control, the\nRecurrent Deterministic Policy Gradient (RDPG) is proposed\nfor the POMDP by approximating the actor and critic using\nRNNs. Considering that RDPG is not suitable for learning\nusing memory replay, the authors in [130] propose the fast-\nRDPG method by utilizing the actor-critic framework with\nfunction approximation [140]. The proposed method derives\npolicy update for POMDP by directly maximizing the expected\nlong-term accumulated discounted reward.\nPath planning for multiple UAVs connected via cellular\nsystems is studied in [141] and [142]. Each UAV aims to\nachieve a tradeoff between maximizing energy efﬁciency and\n28\nminimizing both latency and interference caused to the ground\nnetwork along its path. The network state observable by each\nUAV includes its distances and orientation angles to cellular\nBSs, the orientation angle to its destination, and the horizontal\ncoordinates of all UAVs. The action of each UAV includes an\noptimal path, transmit power, and cell association along its\npath. The interaction among UAVs is cast as a dynamic game\nand solved by a multi-agent DRL framework. The use of ESN\nin the DRL framework allows each UAV to retain previous\nmemory states and make a decision for unseen network states,\nbased on the reward obtained from previous states. ESN is a\nnew type of RNNs with feedback connections, consisting of\nthe input, recurrent, and output weight matrices. ESN training\nis typically quick and computationally efﬁcient compared with\nother RNNs. Deep ESNs can exploit the advantages of a\nhierarchical temporal feature representation at different levels\nof abstraction, hence disentangling the difﬁculties in modeling\ncomplex tasks. Simulation results show that the proposed\nscheme improves the tradeoff between energy efﬁciency, wire-\nless latency, and the interference caused to the ground network.\nResults also show that each UAV’s altitude is a function of the\nground network density and the UAV’s objective function is\nan important factor in achieving the UAV’s target.\nBesides networked UAVs, vehicle-to-infrastructure also con-\nstitutes an important part and provides rich application impli-\ncations in 5G ecosystem. The authors in [143] adopt the DQL\nto achieve an optimal control policy in communication-based\ntrain control system, which is supported by bidirectional train-\nground communications. The control problem aims to optimize\nthe handoff decision and train control policy, i.e., accelerate or\ndecelerate, based on the states of stochastic channel conditions\nand real-time information including train position, speed,\nmeasured SNR from APs, and handoff indicator. The objective\nof the DQL agent is to minimize a weighted combination of\noperation proﬁle tracking error and energy consumption.\nB. Resource Sharing and Scheduling\nSystem capacity is one of the most important performance\nmetrics in wireless communication networks. System capacity\nenhancements can be based on the optimization of resource\nsharing and scheduling among multiple wireless nodes. The\nintegration of DRL into 5G systems would revolutionize the\nresource sharing and scheduling schemes from model-based to\nmodel-free approaches and meet various application demands\nby learning from the network environment.\nThe authors in [144] study the user scheduling in a multi-\nuser massive MIMO system. User scheduling is responsible for\nallocating resource blocks to BSs and mobile users, taking into\naccount the channel conditions and QoS requirements. Based\non this user scheduling strategy, a DRL-based coverage and\ncapacity optimization is proposed to obtain dynamically the\nscheduling parameters and a uniﬁed threshold of QoS metric.\nThe performance indicators are calculated as the average spec-\ntrum efﬁciency of all the users. The system state is an indicator\nof the average spectrum efﬁciency. The action of the scheduler\nis a set of scheduling parameters to maximize the reward as a\nfunction of the average spectrum efﬁciency. The DRL scheme\nuses policy gradient method to learn a policy function (instead\nof a Q-function) directly from trajectories generated by the\ncurrent policy. The policy network is trained with a variant\nof the REINFORCE algorithm [140]. The simulation results\nin [144] show that compared with the optimization-based\nalgorithms that suffer from incomplete network information,\nthe policy gradient method achieves much better performance\nin terms of network coverage and capacity.\nIn [145], the authors focus on dynamic resource allocation\nin a cloud radio access network and present a DQL-based\nframework to minimize the total power consumption while\nfulﬁlling mobile users’ QoS requirements. The system model\ncontains multiple Remote Radio Heads (RRHs) connected to\na cloud BaseBand Unit (BBU). The information of RRHs\ncan be shared in a centralized manner. The system state\ncontains information about the mobile users’ demands and\nthe RRHs’ working states, e.g., active or sleep. According\nto the system state and the result of last execution, the\nDQL agent at the BBU decides whether to turn on or off\ncertain RRH(s), and how to allocate beamforming weight\nfor each active RRH. The objective is to minimize the total\nexpected power consumption. The authors propose a two-step\ndecision framework to reduce the size of action space. In\nthe ﬁrst step, the DQL agent determines the set of active\nRRHs by Q-learning and DNNs. In the second step, the\nBBU derives the optimal resource allocation for the active\nRRHs by solving a convex optimization problem. Through\nthe combination of DQL and optimization techniques, the\nproposed framework results in a relatively small action space\nand low online computational complexity. Simulation results\nshow that the framework achieves signiﬁcant power savings\nwhile satisfying user demands and is robust in highly dynamic\nnetwork environment. The aforementioned works mostly focus\non simulations and numerical comparisons. With one step\nfurther, the authors in [146] implement a multi-objective DQL\nframework as the radio-resource-allocation controller for space\ncommunications. The implementation uses modular software\narchitecture to encourage re-use and easy modiﬁcation for\ndifferent algorithms, which is integrated into the real space-\nground system developed by NASA Glenn Research Center.\nIn emerging and future wireless networks, BSs are deployed\nwith a high density, and thus the interference among the BSs\nmust be considered. The authors in [147] propose to use a\nDQL scheme which allows the BSs to learn their optimal\npower control policy. In the proposed scheme, each BS is\nan agent, the action is choosing power levels, and the state\nincludes interference that the BS caused to its neighbors in\nthe last time slot. The objective is to maximize the BS’s data\nrate. The DQN using FNN is then adopted to implement the\nDQL algorithm. For the future work, a joint power control and\nchannel selection can be considered.\nNetwork slicing [148] and NFV [90] are two emerging\nconcepts for resource allocation in the 5G ecosystem to\nprovide cost-effective services with better performance. The\nnetwork infrastructure, e.g., cache, computation, and radio\nresources, is comparatively static while the upper-layer Vir-\ntualized Network Functions (VNFs) are dynamic to support\ntime-varying application-speciﬁc service requests. The concept\n29\nof network slicing is to divide the network resources into\nmulti-layer slices, managed by different service renderers\nindependently with minimal conﬂicts. The concept of Service\nFunction Chaining (SFC) is to orchestrate different VNFs to\nprovide required functionalities and QoS provisioning.\nThe authors in [149] propose a DQL scheme for QoS/QoE-\naware SFC in NFV-enabled 5G systems. Typical QoS metrics\nare bandwidth, delay, throughput, etc. The evaluation of QoE\nnormally involves the end-user’s participation in rating the ser-\nvice based on direct user perception. The authors quantify QoE\nby measurable QoS metrics without end-user involvements,\naccording to the Weber-Fechner Law (WFL) [150] and expo-\nnential interdependency of QoE and QoS hypothesis [151].\nThese two principles actually deﬁne nonlinear relationship\nbetween QoE and QoS. The system state represents the\nnetwork environment including network topology, QoS/QoE\nstatus of the VNF instances, and the QoS requirements of the\nSFC request. The DQL agent selects a certain direct succes-\nsive VNF instance as an action. The reward is a composite\nfunction of the QoE gain, the QoS constraint penalty, and the\nOPEX penalty. A DQL based on CNNs is implemented to\napproximate the action-value function. The authors in [152]\nreview the application of a DQL framework in two typical\nresource management scenarios using network slicing. For ra-\ndio resource slicing, the authors simulate a scenario containing\none single BS with different types of services. The reward can\nbe deﬁned as a weighted sum of spectrum efﬁciency and QoE.\nFor priority-based core network slicing, the authors simulate\na scenario with 3 SFCs demanding different computational\nresources and waiting time. The reward is the sum of waiting\ntime in different SFCs. Simulation results in both scenarios\nshow that the DQL framework could exploit more implicit\nrelationship between user activities and resource allocation in\nresource constrained scenarios, and enhance the effectiveness\nand agility for network slicing.\nResource allocation and scheduling problems are also im-\nportant for computer clusters or database systems. This usually\nleads to an online decision-making problem depending on\nthe information of workload and environment. The authors\nin [153] propose a DRL-based solution, DeepRM, by em-\nploying policy gradient methods [140] to manage resources in\ncomputer systems directly from experience. The same policy\ngradient method is also used in [144] for user scheduling\nand resource management in wireless systems. DeepRM is\na multi-resource cluster scheduler that learns to optimize\nvarious objectives such as minimizing average job slowdown\nor completion time. The system state is the current allocation\nof cluster resources and the resource proﬁles of jobs in\nthe queue. The action of the scheduler is to decide how\nto schedule the pending jobs. By simulations with synthetic\ndataset, DeepRM is shown to perform comparably or better\nthan state-of-the-art heuristics, e.g., Shortest-Job-First (SJF). It\nadapts to different conditions and converges quickly, without\nany prior knowledge of system behavior. In [154], the authors\nuse the actor-critic method to address the scheduling prob-\nlem in a general-purpose distributed data stream processing\nsystems, which deal with processing of continuous data ﬂow\nin real time or near-real-time. The system model contains\nmultiple threads, processes, and machines. The system state\nconsists of the current scheduling decision and the workload\nof each data source. The scheduling problem is to assign each\nthread to a process of a machine. The agent at the scheduler\ndetermines the assignment of each thread, with the objective of\nminimizing the average processing time. The DRL framework\nincludes three components, i.e., an actor network, an optimizer\nproducing a K-NN set of the actor network’s output action,\nand the critic network predicting the Q-value for each action\nin the set. The action is selected from the K-NN set with the\nmaximum Q-value. The use of optimizer may avoid unstable\nlearning and divergence problems in conventional actor-critic\nmethods [155].\nC. Power Control and Data Collection\nWith the prevalence of IoT and smart mobile devices,\nmobile crowdsensing becomes a cost-effective solution for\nnetwork information collection to support more intelligent\noperations of wireless systems. The authors in [156] con-\nsider spectrum sensing and power control in non-cooperative\ncognitive radio networks. There is no information exchange\nbetween PUs and SUs. As such, the SU outsources the sensing\ntask to a set of spatially distributed sensing devices to collect\ninformation about the PU’s power control strategy. The SU’s\npower control can be formulated as an MDP. The system\nstate is determined by the Received Signal Strength (RSS)\nat individual sensing devices. The SU chooses its transmit\npower from the set of pre-speciﬁed power levels based on the\ncurrent state. A reward is obtained if both primary and SUs can\nfulﬁll their SNR requirements. Considering the randomness in\nRSS measurements, the authors propose a DQL scheme for the\nSU to learn and adjust its transmit power. The DQL is then\nimplemented by a DQN by using FNN. The simulation results\nshow that the proposed DQL scheme is able to converge to a\nclose-to-optimal solution.\nThe authors in [157] leverage the DQL framework for\nsensing and control problems in a Wireless Sensor and Actor\nNetwork (WSAN), which is a group of wireless devices with\nthe ability to sense events and to perform actions based on the\nsensed data shared by all sensors. The system state includes\nprocessing power, mobility abilities, and functionalities of\nthe actors and sensors. The mobile actor can choose its\nmoving direction, networking, sensing and actuation policies\nto maximize the number of connected actor nodes and the\nnumber of sensing events.\nThe authors in [158] focus on mobile crowdsensing\nparadigm, where data inference is incorporated to reduce\nsensing costs while maintaining the quality of sensing. The\ntarget sensing area is split into a set of cells. The objective of\na sensing task is to collect data (e.g., temperature, air quality)\nin all the cells. A DQL-based cell selection mechanism is\nproposed for the mobile sensors to decide which cell is a better\nchoice to perform sensing tasks. The system state includes the\nselection matrices for a few past decision epochs. The reward\nfunction is determined by the sensing quality and cost in the\nchosen cells. To extract temporal correlations in learning, the\nauthors propose the DRQN that uses LSTM layers in DQL\n30\nto capture the hidden patterns in state transitions. Considering\ninter-data correlations, the authors use the transfer learning\nmethod to reduce the amount of data in training. That is, the\ncell selection strategy learned for one task can beneﬁt another\ncorrelated task. Hence, the parameters of DRQN can be ini-\ntialized by another DRQN with rich training data. Simulations\nare conducted based on two real-life datasets collected from\nsensor networks, i.e., the Sensor-Scope dataset [159] in the\nEPFL campus and the U-Air dataset of air quality readings\nin Beijing [160]. The experiments verify that DRQN reduces\nup to 15% of the sensed cells with the same inference quality\nguarantee. The authors in [161] combine UAV and unmanned\nvehicle in mobile crowdsensing for smart city applications.\nThe UAV cruises in the above of the target region for city-level\ndata collection. Meanwhile, the unmanned vehicle carrying\nmobile charging stations moves on the ground and can charge\nthe UAV at a preset charging point.\nThe target region is divided into multiple subregions and\neach subregion has a different sample priority. The authors\nin [161] propose a DQL-based control framework for the\nunmanned vehicle to schedule its data collection, constrained\nby limited energy supply. The system state includes infor-\nmation about the sample priority of each subregion, the\nlocation of charging point, and the moving trace of the UAV\nand unmanned vehicle. The UAV and unmanned vehicle can\nchoose the moving direction. The DQL framework utilizes\nCNNs for extracting the correlation of adjacent subregions,\nwhich can increase the convergence speed in training. The\nDQL algorithm can be enhanced by using a feasible control\nsolution as the baseline during exploration. The PER method\nis also used in DQL to assign higher priorities to important\ntransitions so that the DQL agent can learn from samples more\nefﬁciently. The proposed scheme is evaluated by using real\ndataset of taxi traces in Rome [162]. Simulation results reveal\nthat the proposed DQL algorithm can obtain the highest data\ncollection rate compared with the MDP and other heuristic\nbaselines.\nMobile crowdsensing is vulnerable to faked sensing attacks,\nas selﬁsh users may report faked sensing results to save\ntheir sensing costs and avoid compromising their privacy.\nThe authors in [163] formulate the interactions between the\nserver and a number of crowdsensing users as a Stackelberg\ngame. The server is the leader that sets and broadcasts its\npayment policy for different sensing accuracy. In particular,\nthe higher payment is set for more sensing accuracy. Based\non the server’s sensing policy, each user as a follower then\nchooses its sensing effort and thus the sensing accuracy to\nreceive the payment. The payment motivates the users to put\nin sensing efforts and thus the payment decision process can\nbe modeled as an MDP. In a dynamic network, the server\nuses the DQL to derive the optimal payment to maximize its\nutility, based on the system state consisting of the previous\nsensing quality and the payment policy. The DQL uses a\ndeep CNN to accelerate the learning process and improve the\ncrowdsensing performance against selﬁsh users. Simulation\nresults show that the DQL-based scheme produces a higher\nsensing quality, lower attack rate, and higher utility of the\nserver, exceeding those of both the Q-learning and the random\npayment strategies.\nSocial networking is an important component of smart\ncity applications. The authors in [164] aim to extract useful\ninformation by observing and analyzing the users’ behaviors in\nsocial networking. One of the main difﬁculties is that the social\nbehaviors are usually fuzzy and divergent. The authors model\npervasive social networking as a monopolistically competitive\nmarket, which contains different users as data providers selling\ninformation at a certain price. Given the market model, the\nDQL can be used to estimate the users’ behavior patterns\nand ﬁnd the market equilibrium. Considering the costly deep\nlearning structure, the authors in [164] propose a Decentralized\nDRL (DDRL) framework that decomposes the costly deep\ncomponent from the RL algorithms at individual users. The\ndeep component can be a feature extractor integrated with\nthe network infrastructure and provide mutual knowledge for\nall individuals. Multiple RL agents can purchase the most\ndesirable data from the mutual knowledge. The authors com-\nbine well-known RL algorithms, i.e., Q-learning and learning\nautomata, to estimate users’ patterns which are described by\nvectors of probabilities representing the users’ preferences\nor altitudes to different information. In social networking\nand smart city applications with human involvement, there\ncan be both labeled and unlabeled data and hence a semi-\nsupervised DRL framework can be designed, by combining\nthe strengths of DNNs and statistical modeling to improve\nthe performance and accuracy in learning. Then, the authors\nin [165] introduce the semi-supervised DRL framework that\nutilizes variational auto-encoders [166] as an inference engine\nto infer the classiﬁcation of unlabeled data. As a case study,\nthe proposed DRL framework is customized to provide indoor\nlocalization based on the RSS from Bluetooth devices. The\npositioning environment contains a set of positions. Each\nposition is associated with the set of RSS values from the\nset of anchor devices with known positions. The system state\nincludes a vector of RSS values, the current location, and the\ndistance to the target. The DQL agent, i.e., the positioning\nalgorithm itself, chooses a moving direction to minimize the\nerror distance to the target point. Simulations tested on real-\nworld dataset show an improvement of 23% in terms of the\nerror distance to the target compared with the supervised DRL\nscheme.\nSummary: In this section, we review miscellaneous uses\nof DRL in wireless and networked systems. DRL provides\na ﬂexible tool in rich and diversiﬁed applications, conven-\ntionally involving dynamic system modeling and multi-agent\ninteractions. All these imply a huge space of state transitions\nand actions. These approaches are summarized along with the\nreferences in Table VI. We observe that the NUM problems\nin 5G ecosystem for trafﬁc engineering and resource alloca-\ntion face very diversiﬁed control variables, including discrete\nindicators, e.g., for BS (de)activation, user/cell association,\nand path selection, as well as continuous variables such as\nbandwidth allocation, transmit power, and beamforming opti-\nmization. Hence, both DQL and policy gradient methods are\nused extensively for discrete and continuous control problems,\nrespectively.\n31\nTABLE VI: A summary of applications of DQL for trafﬁc engineering, resource scheduling, and data collection.\nISSUES\nREF.\nMODEL\nLEARNING\nALGORITHMS\nAGENT\nSTATES\nACTIONS\nREWARDS\nSCENARIOS\nTrafﬁc engineering and routing\n[133]\nMDP\nDQN using\nactor-critic\nnetworks\nNetwork\ncontroller\nBandwidth request of each\nnode pair\nTrafﬁc load split\non different paths\nMean network delay\n5G network\n[136]\nNUM\nDQN using\nactor-critic\nnetworks\nNetwork\ncontroller\nThroughput and delay\nperformance\nTrafﬁc load split\non different paths\nα-fairness utility\n5G network\n[130]\nPOMDP\nDQN using\nactor-critic\nnetworks\nUAV\nLocal sensory information,\ne.g., distances and angles\nTurn left or right\nComposite reward\nUAV\nnavigation\n[141]\n[142]\nGame\nDQN using\nESN\nUAV\nCoordinates, distances, and\norientation angles\nPath, transmit\npower, and cell\nassociation\nWeighted sum of\nenergy efﬁciency,\nlatency, and\ninterference\nCellular-\nconnected\nUAVs\n[143]\nMDP\nDQN using\nFNN\nTrain\nscheduler\nChannel conditions, train\nposition, speed, SNR, and\nhandoff indicator\nMaking handoff of\nconnection, or\naccelerate or\ndecelerate the train\nTracking error and\nenergy consumption\nVehicle-to-\ninfrastructure\nsystem\nResource sharing and scheduling\n[145]\nMDP\nDQN using\nFNN\nCloud\nbaseband\nunit\nMUs’ demands and the\nRRHs’ working states\nTurn on or off\ncertain RRH(s),\nand beamforming\nallocation\nExpected power\nconsumption\nCloud RAN\n[149]\nMDP\nDQN with\nCNN\nNetwork\ncontroller\nNetwork topology, QoS/QoE\nstatus, and the QoS\nrequirements\nSuccessive VNF\ninstance\nComposite function\nof QoE gain, QoS\nconstraints penalty,\nand OPEX penalty\nCellular\nsystem\n[152]\nMDP\nDQN using\nFNN\nNetwork\ncontroller\nThe number of arrived\npackets/the priority and\ntime-stamp of ﬂows\nBandwidth/SFC\nallocation\nWeighted sum of\nspectrum efﬁciency\nand QoE/waiting\ntime in SFCs\n5G network\n[154]\nMDP\nDQN using\nactor-critic\nnetworks\nCentral\nscheduler\nCurrent scheduling decision\nand the workload\nAssignment of\neach thread\nAverage processing\ntime\nDistributed\nstream data\nprocessing\nData collection\n[156]\nMDP\nDQN using\nFNN\nSecondary\nuser\nReceived signal strength at\nindividual sensors\nTransmit power\nFixed reward if QoS\nsatisﬁed\nCRN\n[158]\nMDP\nDRQN, LSTM,\ntransfer\nlearning\nMobile\nsensors\nCell selection matrices\nNext cell for\nsensing\nA function of the\nsensing quality and\ncost\nWSN\n[161]\nMDP\nDQN using\nCNN\nUAV and\nunmanned\nvehicle\nSubregions’ sample priority,\ncharging point’s location, and\ntrace of the UAV and\nunmanned vehicle\nMoving direction\nof the UAV and\nunmanned vehicle\nFixed reward related\nto subregions’\nsample priority\nUAV and\nvehicle\n[163]\nGame\nDQN using\nCNN\nCrowdsensing\nserver\nPrevious sensing quality and\npayment policy\nCurrent payment\npolicy\nUtility\nMobile\ncrowdsens-\ning\n[164]\nGame\nDDQN\nMobile\nusers\nCurrent preferences\nPositive or\nnegative altitude\nReward or proﬁt\nMobile\nsocial\nnetwork\nVII. CHALLENGES, OPEN ISSUES, AND FUTURE\nRESEARCH DIRECTIONS\nDifferent approaches reviewed in this survey evidently show\nthat DRL can effectively address various emerging issues\nin communications and networking. There are existing chal-\nlenges, open issues, and new research directions which are\ndiscussed as follows.\nA. Challenges\n1) State Determination in Density Networks: The DRL\napproaches, e.g., [23], allow the users to ﬁnd an optimal ac-\ncess policy without having complete and/or accurate network\ninformation. However, the DRL approaches often require the\nusers to report their local states at every time slot. To observe\nthe local state, the user needs to monitor Received Signal\nStrength Indicators (RSSIs) from its neighboring BSs, and then\nit temporarily connects to the BS with the maximum RSSI.\nHowever, the future networks will deploy a high density of the\nBSs, and the RSSIs from different BSs may not be different.\nThus, it is challenging for the users to determine the temporary\nBS [167].\n2) Knowledge of Jammers’ Channel Information: The DRL\napproach for wireless security as proposed in [118] enables\nthe UAV to ﬁnd optimal transmit power levels to maximize\nthe security capacity of the UAV and the BS. However,\nto formulate the reward of the UAV, a perfect knowledge\nof channel information of the jammers is required. This is\nchallenging and even impossible in practice.\n3) Multi-agent DRL in Dynamic HetNets: Most of the\nexisting works focus on the customizations of DRL framework\nfor individual network entities, based on locally observed\nor exchanged network information. Hopefully, the network\nenvironment is relatively static to ensure convergent learning\nresults and stable policies. This requirement may be challenged\n32\nin a dynamic heterogenous 5G network, which consists of\nhierarchically nested IoT devices/networks with fast changing\nservice requirements and networking conditions. In such a\nsituation, the DQL agents for individual entities have to be\nlight-weighted and agile to the change of network conditions.\nThis implies a reduce to the state and action spaces in learning,\nwhich however may compromise the performance of the\nconvergent policy. The interactions among multiple agents also\ncomplicate the network environment and cause a considerable\nincrease to the state space, which inevitably slows down the\nlearning algorithms.\n4) Training and Performance Evaluation of DRL Frame-\nwork: The DRL framework requires large amounts of data for\nboth training and performance evaluation. In wireless systems,\nsuch data is not easily accessible as we rarely have referential\ndata pools as other deep learning scenarios, e.g., computer\nvision. Most of the existing works rely on simulated dataset,\nwhich undermines the conﬁdence of the DRL framework in\npractical system. The simulated data set is usually generated\nby a speciﬁc stochastic model, which is a simpliﬁcation of the\nreal system and may overlook the hidden patterns. Hence, a\nmore effective way for generating simulation data is required\nto ensure that the training and performance evaluation of the\nDRL framework are more consistent with practical system.\nB. Open Issues\n1) Distributed DRL Framework in Wireless Networks: The\nDRL framework requires large amounts of training for DNNs.\nThis may be implemented at a centralized network controller,\nwhich has sufﬁcient computational capacity and the capability\nfor information collection. However, for massive end users\nwith limited capabilities, it becomes a meaningful task to\ndesign distributed implementation for the DRL framework that\ndecomposes resource-demanding basic functionalities, e.g.,\ninformation collection, sharing, and DNN training, from rein-\nforcement learning algorithms at individual devices. The basic\nfunctionalities can be integrated with the network controller. It\nremains an open issue for the design of network infrastructure\nthat supports these common functionalities for distributed\nDRL. The overhead of information exchange between end\nusers and network controller also has to be well controlled.\n2)\nBalance between Information Quality and Learning\nPerformance: The majority of the existing works consider the\norchestration of networking, transmission control, ofﬂoading,\nand caching decisions in one DRL framework to derive the\noptimal policy, e.g., [84]–[89], [91], [92]. However, from a\npractical viewpoint, the network system will have to pay sub-\nstantially increasing cost for information gathering. The cost\nis incurred from large delay, pre-processing of asynchronous\ninformation, excessive energy consumption, reduced learning\nspeed, etc. Hence, an open issue is to ﬁnd the optimal balance\nbetween information quality and learning performance so that\nthe DQL agent does not consume too much resources only\nto achieve insigniﬁcantly marginal increase in the learning\nperformance.\nC. Future Research Directions\n1) DRL for Channel Estimation in Wireless Systems:\nMassive MIMO will be deployed for 5G to achieve high-speed\ncommunications at Gbps. For this, the channel estimation is\nthe prerequisite for realizing massive MIMO. However, in a\nlarge-scale heterogeneous cellular network foreseen for 5G or\nbeyond, the required channel estimation is very challenging.\nThus, DRL will play an important role in acquiring the chan-\nnel estimates with regard to dynamic time-varying wireless\nchannels.\nAlso, we expect that the combination of Wireless Power\nTransfer (WPT) and Mobile Crowd Sensing (MCS), namely\nWireless-Powered Crowd Sensing (WPCS) will be a promising\ntechnique for the emerging IoT services. To this end, a higher\npower transfer efﬁciency of WPT is very critical to enable\nthe deployment of WPCS in low-power wide area network.\nA \"large-scale array antenna based WPT\" will achieve this\ngoal of higher WPT efﬁciency, but the channel estimation\nshould be performed with minimal power consumption at a\nsensor node. This is because of that the sensor must operate\nwith self-powering via WPT from the dedicated energy source,\ne.g., power beacon, Wi-Fi or small-cell access point, and/or\nambient RF sources, e.g., TV tower, Wi-Fi AP and cellular\nBS. In this regard, the channel estimation based on the receive\npower measurements at the sensor node is one viable solution,\nbecause the receive power can be measured by the passive-\ncircuit power meter with negligible power consumption. DRL\ncan be used for the time-varying wireless channels with\ntemporal correlations over time by taking the receive power\nmeasurements from the sensor node as the input for DRL,\nwhich will enable the channel estimation for WPT efﬁciently.\n2) DRL for Crowdsensing Service Optimization: In MCS,\nmobile users contribute sensing data to a crowdsensing service\nprovider and receive an incentive in return. However, due to\nlimited resources, e.g., bandwidth and energy, the mobile user\nhas to decide on whether and how much data to be uploaded\nto the provider. Likewise, the provider aiming to maximize its\nproﬁt has to determine the amount of incentive to be given.\nThe provider’s decision depends on the actions of the mobile\nusers. For example, with many mobile users contributing data\nto the crowdsensing service provider, the provider can lower\nthe incentive. Due to a large state space of a large number of\nusers and dynamic environment, DRL can be applied to obtain\nan optimal crowdsensing policy similar to [168].\n3) DRL for Cryptocurrency Management in Wireless Net-\nworks: Pricing and economic models have been widely ap-\nplied to wireless networks [169], [170]. For example, wireless\nusers pay money to access radio resources or mobile services.\nAlternatively, the users can receive money if they contribute to\nthe networks, e.g., offering a relay or cache function. However,\nusing real money and cash in such scenarios faces many\nissues related to accounting, security, and privacy. Recently,\nthe concept of cryptocurrency based on the blockchain tech-\nnology has been introduced and adopted in wireless networks,\ne.g., [171], which has been shown to be a secure and efﬁcient\nsolution. However, the value of cryptocurrency, i.e., token\nor coin, can be highly dynamic depending on many market\n33\nfactors. The wireless users possessing the tokens can decide\nto keep or spend the tokens, e.g., for radio resource access\nand service usage or exchange into real money. In the random\ncryptocurrency market environment, DRL can be applied to\nachieve the maximum long-term reward of the cryptocurrency\nmanagement for wireless users as in [172].\n4) DRL for Auction:\nAn auction has been effectively\nused for radio resource management, e.g., spectrum alloca-\ntion [173]. However, obtaining the solution of the auction,\ne.g., a winner determination problem, can be complicated and\nintractable when the number of participants, i.e., bidders and\nsellers, become very large. Such a scenario is typical in next-\ngeneration wireless networks such as 5G highly-dense hetero-\ngeneous networks. DRL appears to be an efﬁcient approach\nfor solving different types of auctions such as in [174].\nVIII. CONCLUSIONS\nThis paper has presented a comprehensive survey of the\napplications of deep reinforcement learning to communica-\ntions and networking. First, we have presented an overview of\nreinforcement learning, deep learning, and deep reinforcement\nlearning. Then, we have introduced various deep reinforce-\nment learning techniques and their extensions. Afterwards,\nwe have provided detailed reviews, analyses, and compar-\nisons of the deep reinforcement learning to solve different\nissues in communications and networking. The issues include\ndynamic network access, data rate control, wireless caching,\ndata ofﬂoading, network security, connectivity preservation,\ntrafﬁc routing, and data collection. Finally, we have outlined\nimportant challenges, open issues as well as future research\ndirections.\nREFERENCES\n[1] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT press Cambridge, 1998.\n[2] I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio, Deep learning.\nMIT press Cambridge, 2016.\n[3] (2016, Jan.) Google achieves ai “breakthrough” by beating go\nchampion.\nBBC.\n[Online].\nAvailable:\nhttps://www.bbc.com/news/\ntechnology-35420579\n[4] M. L. Puterman, Markov decision processes: discrete stochastic dy-\nnamic programming.\nJohn Wiley & Sons, 2014.\n[5] D. P. Bertsekas, D. P. Bertsekas, D. P. Bertsekas, and D. P. Bertsekas,\nDynamic programming and optimal control.\nAthena scientiﬁc Bel-\nmont, MA, 2005, vol. 1, no. 3.\n[6] R. Bellman, Dynamic programming.\nMineola, NY: Courier Corpora-\ntion, 2013.\n[7] Y. Li, “Deep reinforcement learning: An overview,” arXiv preprint\narXiv:1701.07274, 2017.\n[8] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath,\n“A brief survey of deep reinforcement learning,” IEEE Signal Process-\ning Magazine, vol. 34, no. 6, pp. 26–38, Nov. 2017.\n[9] M. Chen, U. Challita, W. Saad, C. Yin, and M. Debbah, “Machine\nlearning for wireless networks with artiﬁcial intelligence: A tutorial on\nneural networks,” arXiv preprint arXiv:1710.02913, 2017.\n[10] R. Lowe, Y. Wu, A. Tamar, J. Harb, O. P. Abbeel, and I. Mordatch,\n“Multi-agent actor-critic for mixed cooperative-competitive environ-\nments,” in Advances in Neural Information Processing Systems, 2017,\npp. 6379–6390.\n[11] G. E. Monahan, “State of the art-a survey of partially observable\nmarkov decision processes: theory, models, and algorithms,” Manage-\nment Science, vol. 28, no. 1, pp. 1–16, 1982.\n[12] L. S. Shapley, “Stochastic games,” Proceedings of the national academy\nof sciences, vol. 39, no. 10, pp. 1095–1100, 1953.\n[13] J. Hu and M. P. Wellman, “Nash q-learning for general-sum stochastic\ngames,” Journal of machine learning research, vol. 4, no. Nov, pp.\n1039–1069, 2003.\n[14] W. C. Dabney, “Adaptive step-sizes for reinforcement learning,” Ph.D.\ndissertation, 2014.\n[15] C. J. Watkins and P. Dayan, “Q-learning,” Machine learning, vol. 8,\nno. 3-4, pp. 279–292, 1992.\n[16] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski\net al., “Human-level control through deep reinforcement learning,”\nNature, vol. 518, no. 7540, p. 529, 2015.\n[17] Y. Lin, X. Dai, L. Li, and F.-Y. Wang, “An efﬁcient deep rein-\nforcement learning model for urban trafﬁc control,” arXiv preprint\narXiv:1808.01876, 2018.\n[18] S. Gu, E. Holly, T. Lillicrap, and S. Levine, “Deep reinforcement learn-\ning for robotic manipulation with asynchronous off-policy updates,” in\nIEEE International Conference on Robotics and Automation (ICRA),\n2017, pp. 3389–3396.\n[19] S. Thrun and A. Schwartz, “Issues in using function approximation\nfor reinforcement learning,” in Proceedings of Connectionist Models\nSummer School Hillsdale, NJ. Lawrence Erlbaum, 1993.\n[20] H. V. Hasselt, “Double q-learning,” in Advances in Neural Information\nProcessing Systems, 2010, pp. 2613–2621.\n[21] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning\nwith double q-learning.” in AAAI, vol. 2, Phoenix, AZ, Feb. 2016, pp.\n2094–2100.\n[22] O. Naparstek and K. Cohen, “Deep multi-user reinforcement learning\nfor dynamic spectrum access in multichannel wireless networks,” arXiv\npreprint arXiv:1704.02613, 2017.\n[23] N. Zhao, Y.-C. Liang, D. Niyato, Y. Pei, M. Wu, and Y. Jiang, “Deep\nreinforcement learning for user association and resource allocation in\nheterogeneous networks,” in IEEE GLOBECOM, Abu Dhabi, UAE,\nDec. 2018, pp. 1–6.\n[24] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience\nreplay,” arXiv preprint arXiv:1511.05952, 2015.\n[25] Z. Wang, T. Schaul, M. Hessel, H. Van Hasselt, M. Lanctot, and\nN. De Freitas, “Dueling network architectures for deep reinforcement\nlearning,” in International Conference on Machine Learning, New\nYork, NY, Jun. 2016.\n[26] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,\nD. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep rein-\nforcement learning,” in International Conference on Machine Learning,\nNew York City, New York, Jun. 2016, pp. 1928–1937.\n[27] Z. Wang, Y. Xu, L. Li, H. Tian, and S. Cui, “Handover control\nin wireless systems via asynchronous multi-user deep reinforcement\nlearning,” arXiv preprint arXiv:1801.02077, 2018.\n[28] M. G. Bellemare, W. Dabney, and R. Munos, “A distributional per-\nspective on reinforcement learning,” arXiv preprint arXiv:1707.06887,\n2017.\n[29] M. Fortunato, M. G. Azar, B. Piot, J. Menick, I. Osband, A. Graves,\nV. Mnih, R. Munos, D. Hassabis, O. Pietquin et al., “Noisy networks for\nexploration,” in International Conference on Learning Representations,\n2018.\n[30] M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski,\nW. Dabney, D. Horgan, B. Piot, M. Azar, and D. Silver, “Rainbow:\nCombining improvements in deep reinforcement learning,” in The\nThirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.\n[31] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,\nD. Silver, and D. Wierstra, “Continuous control with deep reinforce-\nment learning,” San Juan, Puerto Rico, USA, May 2016.\n[32] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Ried-\nmiller, “Deterministic policy gradient algorithms,” in ICML, 2014.\n[33] M. Hausknecht and P. Stone, “Deep recurrent q-learning for partially\nobservable mdps,” CoRR, abs/1507.06527, vol. 7, no. 1, 2015.\n[34] D. Zhao, H. Wang, K. Shao, and Y. Zhu, “Deep reinforcement learning\nwith experience replay based on sarsa,” in IEEE Symposium Series on\nComputational Intelligence (SSCI), 2016, pp. 1–6.\n[35] W. Wang, J. Hao, Y. Wang, and M. Taylor, “Towards cooperation\nin sequential prisoner’s dilemmas: a deep multiagent reinforcement\nlearning approach,” arXiv preprint arXiv:1803.00162, 2018.\n[36] J. Heinrich and D. Silver, “Deep reinforcement learning from self-play\nin imperfect-information games,” arXiv preprint arXiv:1603.01121,\n2016.\n[37] D. Fooladivanda and C. Rosenberg, “Joint resource allocation and\nuser association for heterogeneous wireless cellular networks,” IEEE\nTransactions on Wireless Communications, vol. 12, no. 1, pp. 248–257,\n2013.\n34\n[38] Y. Lin, W. Bao, W. Yu, and B. Liang, “Optimizing user association and\nspectrum allocation in hetnets: A utility perspective,” IEEE Journal\non Selected Areas in Communications, vol. 33, no. 6, pp. 1025–1039,\n2015.\n[39] S. Wang, H. Liu, P. H. Gomes, and B. Krishnamachari, “Deep rein-\nforcement learning for dynamic multichannel access,” in International\nConference on Computing, Networking and Communications (ICNC),\n2017.\n[40] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou,\nD. Wierstra, and M. A. Riedmiller, “Playing atari with deep reinforce-\nment learning,” CoRR, vol. abs/1312.5602, 2013.\n[41] R. Govindan. Tutornet: A low power wireless iot testbed. [Online].\nAvailable: http://anrg.usc.edu/www/tutornet/\n[42] Q. Zhao, B. Krishnamachari, and K. Liu, “On myopic sensing for multi-\nchannel opportunistic access: structure, optimality, and performance,”\nIEEE Transactions on Wireless Communications, vol. 7, no. 12, pp.\n5431–5440, December 2008.\n[43] S. Wang, H. Liu, P. H. Gomes, and B. Krishnamachari, “Deep\nreinforcement learning for dynamic multichannel access in wireless\nnetworks,” IEEE Transactions on Cognitive Communications and Net-\nworking, to appear.\n[44] J. Zhu, Y. Song, D. Jiang, and H. Song, “A new deep-q-learning-\nbased transmission scheduling mechanism for the cognitive internet\nof things,” IEEE Internet of Things Journal, 2017.\n[45] M. Chu, H. Li, X. Liao, and S. Cui, “Reinforcement learning based\nmulti-access control and battery prediction with energy harvesting in\niot systems,” arXiv preprint arXiv:1805.05929, 2018.\n[46] H. Ye and G. Y. Li, “Deep reinforcement learning for resource\nallocation in v2v communications,” arXiv preprint arXiv:1711.00968,\n2017.\n[47] U. Challita, L. Dong, and W. Saad, “Proactive resource manage-\nment in lte-u systems: A deep learning perspective,” arXiv preprint\narXiv:1702.07031, 2017.\n[48] M. Balazinska and P. Castro. (2003) Ibm watson research center.\n[Online]. Available: https://crawdad.org/ibm/watson/20030219\n[49] Z. Wang, T. Schaul, M. Hessel, H. Van Hasselt, M. Lanctot, and\nN. De Freitas, “Dueling network architectures for deep reinforcement\nlearning,” 2015.\n[50] H. Li, “Multiagent learning for aloha-like spectrum access in cognitive\nradio systems,” EURASIP Journal on Wireless Communications and\nNetworking, vol. 2010, no. 1, pp. 1–15, May 2010.\n[51] S. Liu, X. Hu, and W. Wang, “Deep reinforcement learning based\ndynamic channel allocation algorithm in multibeam satellite systems,”\nIEEE ACCESS, vol. 6, pp. 15 733–15 742, 2018.\n[52] M. Chen, W. Saad, and C. Yin, “Liquid state machine learning for\nresource allocation in a network of cache-enabled lte-u uavs,” in IEEE\nGLOBECOM, 2017, pp. 1–6.\n[53] W. Maass, “Liquid state machines: motivation, theory, and applica-\ntions,” in Computability in context: computation and logic in the real\nworld.\nWorld Scientiﬁc, 2011, pp. 275–296.\n[54] M. Chen, M. Mozaffari, W. Saad, C. Yin, M. Debbah, and C. S. Hong,\n“Caching in the sky: Proactive deployment of cache-enabled unmanned\naerial vehicles for optimized quality-of-experience,” IEEE Journal on\nSelected Areas in Communications, vol. 35, no. 5, pp. 1046–1061,\n2017.\n[55] I. Szita, V. Gyenes, and A. L˝orincz, “Reinforcement learning with\necho state networks,” in International Conference on Artiﬁcial Neural\nNetworks.\nSpringer, 2006, pp. 830–839.\n[56] Tyouku of china network video index. [Online]. Available: http:\n//index.youku.com/\n[57] T. Stockhammer, “Dynamic adaptive streaming over http–: standards\nand design principles,” in Proceedings of the second annual ACM\nconference on Multimedia systems.\nACM, 2011, pp. 133–144.\n[58] M. Gadaleta, F. Chiariotti, M. Rossi, and A. Zanella, “D-dash: A deep\nq-learning framework for dash video streaming,” IEEE Transactions on\nCognitive Communications and Networking, vol. 3, no. 4, pp. 703–718,\n2017.\n[59] J. Klaue, B. Rathke, and A. Wolisz, “Evalvid–a framework for video\ntransmission and quality evaluation,” in International conference on\nmodelling techniques and tools for computer performance evaluation,\n2003, pp. 255–272.\n[60] H. Mao, R. Netravali, and M. Alizadeh, “Neural adaptive video\nstreaming with pensieve,” in Proceedings of the Conference of the ACM\nSpecial Interest Group on Data Communication.\nACM, 2017, pp.\n197–210.\n[61] H. Riiser, P. Vigmostad, C. Griwodz, and P. Halvorsen, “Commute\npath bandwidth traces from 3g networks: analysis and applications,” in\nProceedings of the 4th ACM Multimedia Systems Conference.\nACM,\n2013, pp. 114–118.\n[62] X. Yin, A. Jindal, V. Sekar, and B. Sinopoli, “A control-theoretic\napproach for dynamic adaptive video streaming over http,” in ACM\nSIGCOMM Computer Communication Review, vol. 45, no. 4.\nACM,\n2015, pp. 325–338.\n[63] T. Huang, R.-X. Zhang, C. Zhou, and L. Sun, “Qarc: Video quality\naware rate control for real-time video streaming based on deep rein-\nforcement learning,” arXiv preprint arXiv:1805.02482, 2018.\n[64] (2016) Measuring ﬁxed broadband report. [Online]. Available: https:\n//www.fcc.gov/reports-research/reports/measuring-broadband-america/\nraw-data-measuring-broadband-america-2016\n[65] S. Chinchali, P. Hu, T. Chu, M. Sharma, M. Bansal, R. Misra,\nM. Pavone, and K. Sachin, “Cellular network trafﬁc scheduling with\ndeep reinforcement learning,” in National Conference on Artiﬁcial\nIntelligence (AAAI), 2018.\n[66] Z. Zhang, Y. Zheng, M. Hua, Y. Huang, and L. Yang, “Cache-enabled\ndynamic rate allocation via deep self-transfer reinforcement learning,”\narXiv preprint arXiv:1803.11334, 2018.\n[67] P. V. R. Ferreira, R. Paffenroth, A. M. Wyglinski, T. M. Hackett,\nS. G. Bilén, R. C. Reinhart, and D. J. Mortensen, “Multi-objective\nreinforcement learning for cognitive satellite communications using\ndeep neural network ensembles,” IEEE Journal on Selected Areas in\nCommunications, 2018.\n[68] D. Tarchi, G. E. Corazza, and A. Vanelli-Coralli, “Adaptive coding and\nmodulation techniques for next generation hand-held mobile satellite\ncommunications,” in IEE ICC, 2013, pp. 4504–4508.\n[69] M. T. Hagan and M. B. Menhaj, “Training feedforward networks\nwith the marquardt algorithm,” IEEE transactions on Neural Networks,\nvol. 5, no. 6, pp. 989–993, 1994.\n[70] C. Zhong, M. C. Gursoy, and S. Velipasalar, “A deep reinforce-\nment learning-based framework for content caching,” arXiv preprint\narXiv:1712.08132, 2017.\n[71] T.\nP.\nLillicrap,\nJ.\nJ.\nHunt,\nA.\nPritzel,\nN.\nHeess,\nT.\nErez,\nY. Tassa, D. Silver, and D. Wierstra, “Continuous control with deep\nreinforcement learning,” CoRR, vol. abs/1509.02971, 2015. [Online].\nAvailable: http://arxiv.org/abs/1509.02971\n[72] G.\nDulac-Arnold,\nR.\nEvans,\nP.\nSunehag,\nand\nB.\nCoppin,\n“Reinforcement\nlearning\nin\nlarge\ndiscrete\naction\nspaces,”\nCoRR,\nvol.\nabs/1512.07679,\n2015.\n[Online].\nAvailable:\nhttp:\n//arxiv.org/abs/1512.07679\n[73] L. Lei, L. You, G. Dai, T. X. Vu, D. Yuan, and S. Chatzinotas, “A deep\nlearning approach for optimizing content delivering in cache-enabled\nHetNet,” in Int’l Sym. Wireless Commun. Systems (ISWCS), Aug. 2017,\npp. 449–453.\n[74] M. Schaarschmidt, F. Gessert, V. Dalibard, and E. Yoneki, “Learning\nruntime parameters in computer systems with delayed experience\ninjection,” arXiv preprint arXiv:1610.09903, 2016.\n[75] B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan, and R. Sears,\n“Benchmarking cloud serving systems with YCSB,” in proc. 1st ACM\nSym. Cloud Comput., 2010, pp. 143–154.\n[76] Y. He and S. Hu, “Cache-enabled wireless networks with opportunistic\ninterference alignment,” arXiv preprint arXiv:1706.09024, 2017.\n[77] Y. He, C. Liang, F. R. Yu, N. Zhao, and H. Yin, “Optimization of\ncache-enabled opportunistic interference alignment wireless networks:\nA big data deep reinforcement learning approach,” in IEEE ICC, 2017,\npp. 1–6.\n[78] Y. He, Z. Zhang, F. R. Yu, N. Zhao, H. Yin, V. C. Leung, and\nY. Zhang, “Deep-reinforcement-learning-based optimization for cache-\nenabled opportunistic interference alignment wireless networks,” IEEE\nTransactions on Vehicular Technology, vol. 66, no. 11, pp. 10 433–\n10 445, 2017.\n[79] X. He, K. Wang, H. Huang, T. Miyazaki, Y. Wang, and S. Guo, “Green\nresource allocation based on deep reinforcement learning in content-\ncentric iot,” IEEE Transactions on Emerging Topics in Computing, to\nappear.\n[80] Q. Wu, Z. Li, and G. Xie, “CodingCache: Multipath-aware CCN\ncache with network coding,” in proc. ACM SIGCOMM Workshop on\nInformation-centric Networking, 2013, pp. 41–42.\n[81] M. Chen, W. Saad, and C. Yin, “Echo-liquid state deep learning for\n360 content transmission and caching in wireless vr networks with\ncellular-connected uavs,” arXiv preprint arXiv:1804.03284, 2018.\n[82] M. Chen, U. Challita, W. Saad, C. Yin, and M. Debbah, “Machine\nlearning for wireless networks with artiﬁcial intelligence: A tutorial\n35\non neural networks,” CoRR, vol. abs/1710.02913, 2017. [Online].\nAvailable: http://arxiv.org/abs/1710.02913\n[83] M. Chen, W. Saad, and C. Yin, “Liquid state machine learning for\nresource allocation in a network of cache-enabled LTE-U UAVs,” in\nIEEE GLOBECOM, Dec. 2017.\n[84] Y. He, Z. Zhang, and Y. Zhang, “A big data deep reinforcement\nlearning approach to next generation green wireless networks,” in IEEE\nGLOBECOM, 2017, pp. 1–6.\n[85] Y. He, C. Liang, Z. Zhang, F. R. Yu, N. Zhao, H. Yin, and Y. Zhang,\n“Resource allocation in software-deﬁned and information-centric ve-\nhicular networks with mobile edge computing,” in IEEE Vehicular\nTechnology Conference, 2017, pp. 1–5.\n[86] Y. He, F. R. Yu, N. Zhao, H. Yin, and A. Boukerche, “Deep reinforce-\nment learning (drl)-based resource management in software-deﬁned\nand virtualized vehicular ad hoc networks,” in Proceedings of the 6th\nACM Symposium on Development and Analysis of Intelligent Vehicular\nNetworks and Applications, 2017, pp. 47–54.\n[87] Y. He, N. Zhao, and H. Yin, “Integrated networking, caching, and\ncomputing for connected vehicles: A deep reinforcement learning\napproach,” IEEE Transactions on Vehicular Technology, vol. 67, no. 1,\npp. 44–55, 2018.\n[88] T. L. Thanh and R. Q. Hu, “Mobility-aware edge caching and comput-\ning framework in vehicle networks: A deep reinforcement learning,”\nIEEE Transactions on Vehicular Technology, to appear.\n[89] Y. He, F. R. Yu, N. Zhao, V. C. Leung, and H. Yin, “Software-deﬁned\nnetworks with mobile edge computing and caching for smart cities: A\nbig data deep reinforcement learning approach,” IEEE Communications\nMagazine, vol. 55, no. 12, pp. 31–37, 2017.\n[90] B. Han, V. Gopalakrishnan, L. Ji, and S. Lee, “Network function\nvirtualization: Challenges and opportunities for innovations,” IEEE\nCommunications Magazine, vol. 53, no. 2, pp. 90–97, 2015.\n[91] Y. He, F. R. Yu, N. Zhao, and H. Yin, “Secure social networks in\n5g systems with mobile edge computing, caching and device-to-device\n(d2d) communications,” IEEE Wireless Communications, vol. 25, no. 3,\npp. 103–109, Jun. 2018.\n[92] Y. He, C. Liang, F. R. Yu, and Z. Han, “Trust-based social networks\nwith computing, caching and communications: A deep reinforcement\nlearning approach,” IEEE Transactions on Network Science and Engi-\nneering, to appear.\n[93] C. Zhang, Z. Liu, B. Gu, K. Yamori, and Y. Tanaka, “A deep rein-\nforcement learning based approach for cost-and energy-aware multi-\nﬂow mobile data ofﬂoading,” IEICE Transactions on Communications,\npp. 2017–2025.\n[94] L. Ji, G. Hui, L. Tiejun, and L. Yueming, “Deep reinforcement learning\nbased computation ofﬂoading and resource allocation for mec,” in IEEE\nWCNC, 2018, pp. 1–5.\n[95] X. Chen, H. Zhang, C. Wu, S. Mao, Y. Ji, and M. Bennis, “Perfor-\nmance optimization in mobile-edge computing via deep reinforcement\nlearning,” arXiv preprint arXiv:1804.00514, 2018.\n[96] X. Chen, H. Zhang, C. Wu, S. Mao, Y. Ji, and M. Bennis, “Optimized\ncomputation ofﬂoading performance in virtual edge computing systems\nvia deep reinforcement learning,” arXiv preprint arXiv:1805.06146,\n2018.\n[97] J. Ye and Y.-J. A. Zhang, “DRAG: Deep reinforcement learning based\nbase station activation in heterogeneous networks,” arXiv:1809.02159,\nSep. 2018.\n[98] H. Li, H. Gao, T. Lv, and Y. Lu, “Deep q-learning based dynamic\nresource allocation for self-powered ultra-dense networks,” in IEEE\nICC (ICC Workshops), 2018, pp. 1–6.\n[99] J. Liu, B. Krishnamachari, S. Zhou, and Z. Niu, “Deepnap: Data-driven\nbase station sleeping operations through deep reinforcement learning,”\nIEEE Internet of Things Journal, 2018.\n[100] X. Wan, G. Sheng, Y. Li, L. Xiao, and X. Du, “Reinforcement learning\nbased mobile ofﬂoading for cloud-based malware detection,” in IEEE\nGLOBECOM, 2017, pp. 1–6.\n[101] L. Xiao, X. Wan, C. Dai, X. Du, X. Chen, and M. Guizani, “Security\nin mobile edge caching with reinforcement learning,” CoRR, vol.\nabs/1801.05915, 2018. [Online]. Available: http://arxiv.org/abs/1801.\n05915\n[102] A. S. Shamili, C. Bauckhage, and T. Alpcan, “Malware detection on\nmobile devices using distributed machine learning,” in proc. Int’l Conf.\nPattern Recognition, Aug. 2010, pp. 4348–4351.\n[103] Y. Li, J. Liu, Q. Li, and L. Xiao, “Mobile cloud ofﬂoading for malware\ndetections with learning,” in IEEE INFOCOM Workshops, Apr. 2015,\npp. 197–201.\n[104] M. Min, D. Xu, L. Xiao, Y. Tang, and D. Wu, “Learning-based\ncomputation ofﬂoading for iot devices with energy harvesting,” arXiv\npreprint arXiv:1712.08768, 2017.\n[105] L. Quan, Z. Wang, and F. Ren, “A novel two-layered reinforcement\nlearning for task ofﬂoading with tradeoff between physical machine\nutilization rate and delay,” Future Internet, vol. 10, no. 7, 2018.\n[106] D. V. Le and C. Tham, “Quality of service aware computation of-\nﬂoading in an ad-hoc mobile cloud,” IEEE Transactions on Vehicular\nTechnology, pp. 1–1, 2018.\n[107] D. V. Le and C.-K. Tham, “A deep reinforcement learning based\nofﬂoading scheme in ad-hoc mobile clouds,” in Proceedings of IEEE\nINFOCOM IECCO Workshop, Honolulu, USA., apr 2018.\n[108] S. Yu, X. Wang, and R. Langar, “Computation ofﬂoading for mobile\nedge computing: A deep learning approach,” in IEEE PIMRC, Oct.\n2017.\n[109] Z. Tang, X. Zhou, F. Zhang, W. Jia, and W. Zhao, “Migration\nmodeling and learning algorithms for containers in fog computing,”\nIEEE Transactions on Services Computing, 2018.\n[110] P. Popovski, H. Yomo, and R. Prasad, “Strategies for adaptive fre-\nquency hopping in the unlicensed bands,” IEEE Wireless Communica-\ntions, vol. 13, no. 6, pp. 60–67, 2006.\n[111] G. Han, L. Xiao, and H. V. Poor, “Two-dimensional anti-jamming\ncommunication based on deep reinforcement learning,” in Proceedings\nof the 42nd IEEE International Conference on Acoustics, Speech and\nSignal Processing,, 2017.\n[112] L. Xiao, D. Jiang, X. Wan, W. Su, and Y. Tang, “Anti-jamming under-\nwater transmission with mobility and learning,” IEEE Communications\nLetters, 2018.\n[113] X. Liu, Y. Xu, L. Jia, Q. Wu, and A. Anpalagan, “Anti-jamming com-\nmunications using spectrum waterfall: A deep reinforcement learning\napproach,” IEEE Communications Letters, 2018.\n[114] Y. Chen, Y. Li, D. Xu, and L. Xiao, “Dqn-based power control for\niot transmission against jamming,” in IEEE 87th Vehicular Technology\nConference (VTC Spring), 2018, pp. 1–5.\n[115] X. Lu, L. Xiao, and C. Dai, “Uav-aided 5g communications\nwith deep reinforcement learning against jamming,” arXiv preprint\narXiv:1805.06628, 2018.\n[116] L. Xiao, X. Lu, D. Xu, Y. Tang, L. Wang, and W. Zhuang, “Uav relay\nin vanets against smart jamming with reinforcement learning,” IEEE\nTransactions on Vehicular Technology, vol. 67, no. 5, pp. 4087–4097,\n2018.\n[117] S. Lv, L. Xiao, Q. Hu, X. Wang, C. Hu, and L. Sun, “Anti-jamming\npower control game in unmanned aerial vehicle networks,” in IEEE\nGLOBECOM, 2017, pp. 1–6.\n[118] L. Xiao, C. Xie, M. Min, and W. Zhuang, “User-centric view of\nunmanned aerial vehicle transmission against smart attacks,” IEEE\nTransactions on Vehicular Technology, vol. 67, no. 4, pp. 3420–3430,\n2018.\n[119] M. Bowling and M. Veloso, “Multiagent learning using a variable\nlearning rate,” Artiﬁcial Intelligence, vol. 136, no. 2, pp. 215–250,\n2002.\n[120] Y. Chen, S. Kar, and J. M. Moura, “Cyber-physical attacks with control\nobjectives,” IEEE Transactions on Automatic Control, vol. 63, no. 5,\npp. 1418–1425, 2018.\n[121] A. Ferdowsi, U. Challita, W. Saad, and N. B. Mandayam, “Robust deep\nreinforcement learning for security and safety in autonomous vehicle\nsystems,” arXiv preprint arXiv:1805.00983, 2018.\n[122] M. Brackstone and M. McDonald, “Car-following: a historical review,”\nTransportation Research Part F: Trafﬁc Psychology and Behaviour,\nvol. 2, no. 4, pp. 181–196, 1999.\n[123] A. Ferdowsi and W. Saad, “Deep learning-based dynamic watermarking\nfor secure signal authentication in the internet of things,” in IEEE ICC,\n2018, pp. 1–6.\n[124] B. Satchidanandan and P. R. Kumar, “Dynamic watermarking: Active\ndefense of networked cyber–physical systems,” Proceedings of the\nIEEE, vol. 105, no. 2, pp. 219–240, 2017.\n[125] A. Ferdowsi and W. Saad, “Deep learning for signal authentication\nand security in massive internet of things systems,” arXiv preprint\narXiv:1803.00916, 2018.\n[126] P. Vadakkepat, K. C. Tan, and W. Ming-Liang, “Evolutionary artiﬁcial\npotential ﬁelds and their application in real time robot path planning,”\nin Proceedings of the 2000 Congress on Evolutionary Computation,\nvol. 1, 2000, pp. 256–263.\n[127] W. Huang, Y. Wang, and X. Yi, “Deep q-learning to preserve connec-\ntivity in multi-robot systems,” in Proceedings of the 9th International\nConference on Signal Processing Systems.\nACM, 2017, pp. 45–50.\n36\n[128] W. Huang, Y. Wang, and X. Yi, “A deep reinforcement learning\napproach to preserve connectivity for multi-robot systems,” in In-\nternational Congress on Image and Signal Processing, BioMedical\nEngineering and Informatics (CISP-BMEI), 2017, pp. 1–7.\n[129] H. A. Poonawala, A. C. Satici, H. Eckert, and M. W. Spong, “Collision-\nfree formation control with decentralized connectivity preservation for\nnonholonomic-wheeled mobile robots,” IEEE Transactions on control\nof Network Systems, vol. 2, no. 2, pp. 122–130, 2015.\n[130] C. Wang, J. Wang, X. Zhang, and X. Zhang, “Autonomous naviga-\ntion of uav in large-scale unknown complex environment with deep\nreinforcement learning,” in IEEE GlobalSIP, 2017, pp. 858–862.\n[131] C. Shen, C. Tekin, and M. van der Schaar, “A non-stochastic learning\napproach to energy efﬁcient mobility management,” IEEE Journal on\nSelected Areas in Communications, vol. 34, no. 12, pp. 3854–3868,\n2016.\n[132] M. Faris and E. Brian, “Deep q-learning for self-organizing net-\nworks\nfault\nmanagement\nand\nradio\nperformance\nimprovement,”\nhttps://arxiv.org/abs/1707.02329, 2018.\n[133] G.\nStampa,\nM.\nArias,\nD.\nSanchez-Charles,\nV.\nMuntes-Mulero,\nand A. Cabellos, “A deep-reinforcement learning approach for\nsoftware-deﬁned networking routing optimization,” arXiv preprint\narXiv:1709.07080, 2017.\n[134] M. Roughan, “Simplifying the synthesis of internet trafﬁc matrices,”\nACM SIGCOMM Computer Communication Review, vol. 35, no. 5,\npp. 93–96, 2015. [Online]. Available: http://arxiv.org/abs/1710.02913\n[135] A. Varga and R. Hornig, “An overview of the OMNeT++ simulation\nenvironment,” in proc. Int’l Conf. Simulation Tools and Techniques for\nCommunications, Networks and Systems & Workshops, 2008.\n[136] Z. Xu, J. Tang, J. Meng, W. Zhang, Y. Wang, C. H. Liu, and D. Yang,\n“Experience-driven networking: A deep reinforcement learning based\napproach,” arXiv preprint arXiv:1801.05757, 2018.\n[137] K. Winstein and H. Balakrishnan, “TCP ex Machina: Computer-\ngenerated congestion control,” in ACM SIGCOMM, 2013, pp. 123–134.\n[138] R. G.F. and H. T.R., Modeling and Tools for Network Simulation.\nSpringer, Berlin, Heidelberg, 2010, ch. The ns-3 Network Simulator.\n[139] A. Medina, A. Lakhina, I. Matta, and J. Byers, “BRITE: an approach\nto universal topology generation,” in IEEE MASCOTS, Aug. 2001, pp.\n346–353.\n[140] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour, “Policy gradient\nmethods for reinforcement learning with function approximation,” in\nproc. 12th Int’l Conf. Neural Inform. Process. Syst., 1999, pp. 1057–\n1063.\n[141] U. Challita, W. Saad, and C. Bettstetter, “Deep reinforcement learning\nfor interference-aware path planning of cellular connected uavs,” in\nIEEE ICC, Kansas City, MO, May 2018, pp. 1–6.\n[142] U. Challita, W. Saad, and C. Bettstetter, “Cellular-connected uavs over\n5g: Deep reinforcement learning for interference management,” arXiv\npreprint arXiv:1801.05500, 2018.\n[143] L. Zhu, Y. He, F. R. Yu, B. Ning, T. Tang, and N. Zhao,\n“Communication-based train control system performance optimization\nusing deep reinforcement learning,” IEEE Transactions on Vehicular\nTechnology, vol. 66, no. 12, pp. 10 705–10 717, 2017.\n[144] Y. Yang, Y. Li, K. Li, S. Zhao, R. Chen, J. Wang, and S. Ci, “Decco:\nDeep-learning enabled coverage and capacity optimization for massive\nmimo systems,” IEEE Access, to appear.\n[145] Z. Xu, Y. Wang, J. Tang, J. Wang, and M. C. Gursoy, “A deep\nreinforcement learning based framework for power-efﬁcient resource\nallocation in cloud rans,” in IEEE ICC, 2017, pp. 1–6.\n[146] T. M. Hackett, S. G. Bilén, P. V. R. Ferreira, A. M. Wyglinski, and\nR. C. Reinhart, “Implementation of a space communications cognitive\nengine,” in Cognitive Communications for Aerospace Applications\nWorkshop (CCAA), 2017, pp. 1–7.\n[147] Y. S. Nasir and D. Guo, “Deep reinforcement learning for dis-\ntributed dynamic power allocation in wireless networks,” arXiv preprint\narXiv:1808.00490, 2018.\n[148] X. Foukas, G. Patounas, A. Elmokashﬁ, and M. K. Marina, “Network\nslicing in 5g: Survey and challenges,” IEEE Communications Maga-\nzine, vol. 55, no. 5, pp. 94–100, 2017.\n[149] X. Chen, Z. Li, Y. Zhang, R. Long, H. Yu, X. Du, and M. Guizani, “Re-\ninforcement learning based qos/qoe-aware service function chaining in\nsoftware-driven 5g slices,” arXiv preprint arXiv:1804.02099, 2018.\n[150] P. Reichl, S. Egger, R. Schatz, and A. D’Alconzo, “The logarithmic\nnature of qoe and the role of the weber-fechner law in qoe assessment,”\nin IEEE ICC, Cape Town, South Africa, May 2010, pp. 1–5.\n[151] M. Fiedler, T. Hossfeld, and P. Tran-Gia, “A generic quantitative\nrelationship between quality of experience and quality of service,”\nIEEE Network, vol. 24, no. 2, pp. 36–41, 2014.\n[152] Z. Zhao, R. Li, Q. Sun, Y. Yang, X. Chen, M. Zhao, H. Zhang\net al., “Deep reinforcement learning for network slicing,” arXiv preprint\narXiv:1805.06591, 2018.\n[153] H. Mao, M. Alizadeh, I. Menache, and S. Kandula, “Resource man-\nagement with deep reinforcement learning,” in Proceedings of the 15th\nACM Workshop on Hot Topics in Networks, 2016, pp. 50–56.\n[154] T. Li, Z. Xu, J. Tang, and Y. Wang, “Model-free control for distributed\nstream data processing using deep reinforcement learning,” Proceed-\nings of the VLDB Endowment, vol. 11, no. 6, pp. 705–718, 2018.\n[155] G. D. Arnold, R. Evans, H. v. Hasselt, P. Sunehag, T. Lillicrap, J. Hunt,\nT. Mann, T. Weber, T. Degris, and B. Coppin, “Deep reinforcement\nlearning in large discrete action spaces,” arXiv: 1512.07679, 2016.\n[156] X. Li, J. Fang, W. Cheng, H. Duan, Z. Chen, and H. Li, “Intelligent\npower control for spectrum sharing in cognitive radios: A deep rein-\nforcement learning approach,” arXiv preprint arXiv:1712.07365, 2017.\n[157] T. Oda, R. Obukata, M. Ikeda, L. Barolli, and M. Takizawa, “Design\nand implementation of a simulation system based on deep q-network\nfor mobile actor node control in wireless sensor and actor networks,”\nin International Conference on Advanced Information Networking and\nApplications Workshops (WAINA), 2017, pp. 195–200.\n[158] L. Wang, W. Liu, D. Zhang, Y. Wang, E. Wang, and Y. Yang,\n“Cell selection with deep reinforcement learning in sparse mobile\ncrowdsensing,” arXiv preprint arXiv:1804.07047, 2018.\n[159] F. Ingelrest, G. Barrenetxea, G. Schaefer, M. Vetterli, O. Couach, and\nM. Parlange., “SensorScope: Application-speciﬁc sensor network for\nenvironmental monitoring,” ACM Transactions on Sensor Networks,\nvol. 6, no. 2, pp. 1–32, 2010.\n[160] Y. Zheng, F. Liu, and H. P. Hsieh, “U-Air: when urban air quality\ninference meets big data,” in ACM SIGKDD Int’l Conf. Knowledge\nDiscovery and Data Mining, 2013.\n[161] B. Zhang, C. H. Liu, J. Tang, Z. Xu, J. Ma, and W. Wang, “Learning-\nbased energy-efﬁcient data collection by unmanned vehicles in smart\ncities,” IEEE Transactions on Industrial Informatics, vol. 14, no. 4, pp.\n1666–1676, 2018.\n[162] L. Bracciale, M. Bonola, P. Loreti, G. Bianchi, R. Amici, and\nA. Rabufﬁ. (2014, Jul.) CRAWDAD dataset roma/taxi (v. 2014-07-\n17). [Online]. Available: http://crawdad.org/roma/taxi/20140717\n[163] L. Xiao, Y. Li, G. Han, H. Dai, and H. V. Poor, “A secure mobile\ncrowdsensing game with deep reinforcement learning,” IEEE Transac-\ntions on Information Forensics and Security, vol. 13, no. 1, pp. 35–47,\nJan. 2018.\n[164] Y. Zhang, B. Song, and P. Zhang, “Social behavior study under\npervasive social networking based on decentralized deep reinforcement\nlearning,” Journal of Network and Computer Applications, vol. 86, pp.\n72–81, 2017.\n[165] M. Mohammadi, A. Al-Fuqaha, M. Guizani, and J.-S. Oh, “Semisu-\npervised deep reinforcement learning in support of iot and smart city\nservices,” IEEE Internet of Things Journal, vol. 5, no. 2, pp. 624–635,\n2018.\n[166] D. P. Kingma, S. Mohamed, D. J. Rezende, and M. Welling, “Semi-\nsupervised learning with deep generative models,” in Advances in\nNeural Information Processing Systems, 2014, pp. 3581–3589.\n[167] G. Cao, Z. Lu, X. Wen, T. Lei, and Z. Hu, “Aif: An artiﬁcial\nintelligence framework for smart wireless network management,” IEEE\nCommunications Letters, vol. 22, no. 2, pp. 400–403, 2018.\n[168] Y. Zhan, Y. Xia, J. Zhang, T. Li, and Y. Wang, “Crowdsensing game\nwith demand uncertainties: A deep reinforcement learning approach,”\nsubmitted.\n[169] N. C. Luong, P. Wang, D. Niyato, Y. Wen, and Z. Han, “Resource\nmanagement in cloud networking using economic analysis and pricing\nmodels: a survey,” IEEE Communications Surveys & Tutorials, vol. 19,\nno. 2, pp. 954–1001, Jan. 2017.\n[170] N. C. Luong, D. T. Hoang, P. Wang, D. Niyato, D. I. Kim, and Z. Han,\n“Data collection and wireless communication in internet of things\n(iot) using economic analysis and pricing models: A survey,” IEEE\nCommunications Surveys & Tutorials, vol. 18, no. 4, pp. 2546–2590,\nJun. 2016.\n[171] F. Shi, Z. Qin, and J. A. McCann, “Oppay: Design and implementation\nof a payment system for opportunistic data services,” in IEEE Inter-\nnational Conference on Distributed Computing Systems, Atlanta, GA,\nJul. 2017, pp. 1618–1628.\n[172] Z. Jiang and J. Liang, “Cryptocurrency portfolio management with deep\nreinforcement learning,” in Intelligent Systems Conference (IntelliSys),\n2017, pp. 905–913.\n[173] N. C. Luong, P. Wang, D. Niyato, Y.-C. Liang, F. Hou, and Z. Han, “Ap-\nplications of economic and pricing models for resource management in\n37\n5g wireless networks: A survey,” IEEE Communications Surveys and\nTutorials, to appear.\n[174] J. Zhao, G. Qiu, Z. Guan, W. Zhao, and X. He, “Deep reinforce-\nment learning for sponsored search real-time bidding,” arXiv preprint\narXiv:1803.00259, 2018.\n",
  "categories": [
    "cs.NI",
    "cs.LG"
  ],
  "published": "2018-10-18",
  "updated": "2018-10-18"
}