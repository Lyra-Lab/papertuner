{
  "id": "http://arxiv.org/abs/1810.08033v1",
  "title": "Adaptivity of deep ReLU network for learning in Besov and mixed smooth Besov spaces: optimal rate and curse of dimensionality",
  "authors": [
    "Taiji Suzuki"
  ],
  "abstract": "Deep learning has shown high performances in various types of tasks from\nvisual recognition to natural language processing, which indicates superior\nflexibility and adaptivity of deep learning. To understand this phenomenon\ntheoretically, we develop a new approximation and estimation error analysis of\ndeep learning with the ReLU activation for functions in a Besov space and its\nvariant with mixed smoothness. The Besov space is a considerably general\nfunction space including the Holder space and Sobolev space, and especially can\ncapture spatial inhomogeneity of smoothness. Through the analysis in the Besov\nspace, it is shown that deep learning can achieve the minimax optimal rate and\noutperform any non-adaptive (linear) estimator such as kernel ridge regression,\nwhich shows that deep learning has higher adaptivity to the spatial\ninhomogeneity of the target function than other estimators such as linear ones.\nIn addition to this, it is shown that deep learning can avoid the curse of\ndimensionality if the target function is in a mixed smooth Besov space. We also\nshow that the dependency of the convergence rate on the dimensionality is tight\ndue to its minimax optimality. These results support high adaptivity of deep\nlearning and its superior ability as a feature extractor.",
  "text": "arXiv:1810.08033v1  [stat.ML]  18 Oct 2018\nADAPTIVITY OF DEEP RELU NETWORK FOR LEARN-\nING IN BESOV AND MIXED SMOOTH BESOV SPACES:\nOPTIMAL RATE AND CURSE OF DIMENSIONALITY\nTaiji Suzuki\nThe University of Tokyo, Tokyo, Japan\nCenter for Advanced Intelligence Project, RIKEN\nJapan Digital Design\ntaiji@mist.i.u-tokyo.ac.jp\nABSTRACT\nDeep learning has shown high performances in various types of tasks from visual\nrecognition to natural language processing, which indicates superior ﬂexibility\nand adaptivity of deep learning. To understand this phenomenon theoretically, we\ndevelop a new approximation and estimation error analysis of deep learning with\nthe ReLU activation for functions in a Besov space and its variant with mixed\nsmoothness. The Besov space is a considerably general function space includ-\ning the H¨older space and Sobolev space, and especially can capture spatial inho-\nmogeneity of smoothness. Through the analysis in the Besov space, it is shown\nthat deep learning can achieve the minimax optimal rate and outperform any non-\nadaptive (linear) estimator such as kernel ridge regression, which shows that deep\nlearning has higher adaptivity to the spatial inhomogeneity of the target function\nthan other estimators such as linear ones. In addition to this, it is shown that deep\nlearning can avoid the curse of dimensionality if the target function is in a mixed\nsmooth Besov space. We also show that the dependency of the convergence rate\non the dimensionality is tight due to its minimax optimality. These results support\nhigh adaptivity of deep learning and its superior ability as a feature extractor.\n1\nINTRODUCTION\nDeep learning has shown great success in several applications such as computer vision and nat-\nural language processing. As its application range is getting wider, theoretical analysis to reveal\nthe reason why deep learning works so well is also gathering much attention. To understand deep\nlearning theoretically, several studies have been developed from several aspects such as approxima-\ntion theory and statistical learning theory. A remarkable property of neural network is that it has\nuniversal approximation capability even if there is only one hidden layer (Cybenko, 1989; Hornik,\n1991; Sonoda & Murata, 2015). Thanks to this property, deep and shallow neural networks can\napproximate any function with any precision (of course, the meaning of the terminology “any”\nmust be rigorously deﬁned like “any function in L1(R)”). A natural question coming next to the\nuniversal approximation capability is its expressive power. It is shown that the expressive power\nof deep neural network grows exponentially against the number of layers (Montufar et al., 2014;\nBianchini & Scarselli, 2014; Cohen et al., 2016; Cohen & Shashua, 2016; Poole et al., 2016) where\nthe “expressive power” is deﬁned by several ways.\nThe expressive power of neural network can be analyzed more precisely by specifying the target\nfunction’s property such as smoothness. Barron (1993; 1994) developed an approximation theory\nfor functions having limited “capacity” that is measured by integrability of their Fourier transform.\nAn interesting point of the analysis is that the approximation error is not affected by the dimen-\nsionality of the input. This observation matches the experimental observations that deep learning is\nquite effective also in high dimensional situations. Another typical approach is to analyze function\nspaces with smoothness conditions such as the H¨older space. In particular, deep neural network\nwith the ReLU activation (Nair & Hinton, 2010; Glorot et al., 2011) has been extensively studied\nrecently from the view point of its expressive power and its generalization error. For example,\n1\nTable 1: Comparison between the performances achieved by deep learning and linear methods.\nHere, N is the number of parameters to approximate a function in a Besov space (Bs\np,q([0, 1]d)), and\nn is the sample size. The approximation error is measured by Lr-norm. The ˜O symbol hides the\npoly-log order.\nModel\nDeep learning\nLinear method\nApproximation error rate\n˜O(N −s\nd )\n˜O\n\u0010\nN −s\nd +( 1\np −1\nr )+\u0011\nEstimation error rate\n˜O(n−\n2s\n2s+d )\nΩ\n\u0000n−\n2s−(2/(p∧1)−1)\n2s+1−(2/(p∧1)−1) \u0001\nYarotsky (2016) derived the approximation error of the deep network with the ReLU activation for\nfunctions in the H¨older space. Schmidt-Hieber (2017) evaluated the estimation error of regularized\nleast squared estimator performed by deep ReLU network based on this approximation error analysis\nin a nonparametric regression setting. Petersen & Voigtlaender (2017) generalized the analysis by\nYarotsky (2016) to the class of piece-wise smooth functions. Imaizumi & Fukumizu (2018) utilized\nthis analysis to derive the estimation error to estimate the piece-wise smooth function and concluded\nthat deep leaning can outperform linear estimators in that setting; here, the linear method indicates\nan estimator which is linearly dependent on the output observations (y1, . . . , yn) (it could be non-\nlinearly dependent on the input (x1, . . . , xn); for example, the kernel ridge regression depends on\nthe output observations linearly, but it is nonlinearly dependent on the inputs). Although these error\nanalyses are standard from a nonparametric statistics view point and the derived rates are known\nto be (near) minimax optimal, the analysis is rather limited because the analyses are given mainly\nbased on the H¨older space. However, there are several other function spaces such as the Sobolev\nspace and the space of ﬁnite total variations. A comprehensive analysis to deal with such function\nclasses from a uniﬁed view point is required.\nIn this paper, we give generalization error bounds of deep ReLU networks for a Besov space and\nits variant with mixed smoothness, which includes the H¨older space, the Sobolev space, and the\nfunction class with total variation as special cases. By doing so, (i) we show that deep learning\nachieves the minimax optimal rate on the Besov space and notably it outperforms any linear esti-\nmator such as the kernel ridge regression, and (ii) we show that deep learning can avoid the curse\nof dimensionality on the mixed smooth Besov space and achieves the minimax optimal rate. As\nrelated work, Mhaskar & Micchelli (1992); Mhaskar (1993); Chui et al. (1994); Mhaskar (1996);\nPinkus (1999) also developed an approximation error analysis which essentially leads to analyses\nfor Besov spaces. However, the ReLU activation is basically excluded and comprehensive analyses\nfor the Besov space have not been given. Consequently, it has not been clear whether ReLU neural\nnetworks can outperform another representative methods such as kernel methods. As a summary,\nthe contribution of this paper is listed as follows:\n(i) To investigate adaptivity of deep learning, we give an explicit form of approximation and\nestimation error bounds for deep learning with the ReLU activation where the target func-\ntions are in the Besov spaces (Bs\np,q) for s > 0 and 0 < p, q ≤∞with s > d(1/p −1/r)+\nwhere Lr-norm is used for error evaluation. In particular, deep learning outperforms any\nlinear estimator such as kernel ridge regression if the target function has highly spatial\ninhomogeneity of its smoothness. See Table 1 for the overview.\n(ii) To investigate the effect of dimensionality, we analyze approximation and estimation prob-\nlems in so-called the mixed smooth Besov space by ReLU neural network. It is shown that\ndeep learning with the ReLU activation can avoid the curse of dimensionality and achieve\nthe near minimax optimal rate. The theory is developed on the basis of the sparse grid\ntechnique (Smolyak, 1963). See Table 2 for the overview.\n2\nSET UP OF FUNCTION SPACES\nIn this section, we deﬁne the function classes for which we develop error bounds. In particular, we\ndeﬁne the Besov space and its variant with mixed smoothness. The typical settings in statistical\nlearning theory is to estimate a function with a smoothness condition. There are several ways to\n2\nTable 2: Summary of relation between related existing work and our work for a mixed smooth Besov\nspace. N is the number of parameters in the deep neural network, n is the sample size. β represents\nthe smoothness parameter, and d represents the dimensionality of the input. The approximation\naccuracy is measured by L2-norm and estimation accuracy is measures by the square of L2-norm.\nSee Theorem 3 for the deﬁnition of u.\nFunction class\nH¨older\nBarron class\nm-Sobolev\n(0 < β ≤2)\nm-Besov\n(0 < β)\nApproximation\nAuthor\nYarotsky\n(2016),\nLiang & Srikant (2016)\nBarron (1993)\nMontanelli & Du\n(2017)\nThis work\nApprox. error\n˜O(N −β\nd )\n˜O(N −1/2)\n˜O(N −β)\n˜O(N −β)\nEstimation\nAuthor\nSchmidt-Hieber (2017)\nBarron (1993)\n—-\nThis work\nEstimation er-\nror\n˜O(n−\n2β\n2β+d )\n˜O(n−1\n2 )\n—-\n˜O(n−\n2β\n2β+1\n×\nlog(n)\n2(d−1)(u+β)\n1+2β\n)\ncharacterize “smoothness.” Here, we summarize the deﬁnitions of representative functional spaces\nthat are appropriate to deﬁne the smoothness assumption.\nLet Ω⊂Rd be a domain of the functions. Throughout this paper, we employ Ω= [0, 1]d. For\na function f : Ω→R, let ∥f∥p := ∥f∥Lp(Ω) := (\nR\nΩ|f|pdx)1/p for 0 < p < ∞. For p =\n∞, we deﬁne ∥f∥∞:= ∥f∥L∞(Ω) := supx∈Ω|f(x)|. For α ∈Rd, let |α| = Pd\nj=1 |αj|. Let\nC0(Ω) be the set of continuous functions equipped with L∞-norm: C0(Ω) := {f : Ω→R |\nf is continuous and ∥f∥∞< ∞} 1. For α ∈Nd\n+, we denote by Dαf(x) =\n∂|α|f\n∂α1 x1...∂αdxd (x) 2.\nDeﬁnition 1 (H¨older space (Cβ(Ω))). Let β > 0 with β ̸∈N be the smoothness parameter. For an\nm times differentiable function f : Rd →R, let the norm of the H¨older space Cβ(Ω) be ∥f∥Cβ :=\nmax|α|≤m\n\r\rDαf∥∞+ max|α|=m supx,y∈Ω\n|∂αf(x)−∂αf(y)|\n|x−y|β−m\n, where m = ⌊β⌋. Then, (β-)H¨older\nspace Cβ(Ω) is deﬁned as Cβ(Ω) = {f | ∥f∥Cβ < ∞}.\nThe parameter β > 0 controls the “smoothness” of the function. Along with the H¨older space, the\nSobolev space is also important.\nDeﬁnition 2 (Sobolev space (W k\np (Ω))). Sobolev space (W k\np (Ω)) with a regularity parameter k ∈\nN and a parameter 1 ≤p ≤∞is a set of functions such that the Sobolev norm ∥f∥W k\np :=\n(P\n|α|≤k ∥Dαf∥p\np)\n1\np is ﬁnite.\nThere are some ways to deﬁne a Sobolev space with fractional order, one of which will be deﬁned\nby using the notion of interpolation space (Adams & Fournier, 2003), but we don’t pursue this\ndirection here. Finally, we introduce Besov space which further generalizes the deﬁnition of the\nSobolev space. To deﬁne the Besov space, we introduce the modulus of smoothness.\nDeﬁnition 3. For a function f ∈Lp(Ω) for some p ∈(0, ∞], the r-th modulus of smoothness of f\nis deﬁned by\nwr,p(f, t) =\nsup\n∥h∥2≤t\n∥∆r\nh(f)∥p,\nwhere ∆r\nh(f)(x) =\n(Pr\nj=0\n\u0000r\nj\n\u0001\n(−1)r−jf(x + jh)\n(x ∈Ω, x + rh ∈Ω),\n0\n(otherwise).\nBased on the modulus of smoothness, the Besov space is deﬁned as in the following deﬁnition.\n1Since Ω= [0, 1]d in our setting, the boundedness automatically follows from the continuity.\n2 We let N+ := {0, 1, 2, 3, . . . }, Nd\n+ := {(z1, . . . , zd) | zi ∈N+}, R+ := {x ≥0 | x ∈R}, and\nR++ := {x > 0 | x ∈R}.\n3\nDeﬁnition 4 (Besov space (Bα\np,q(Ω))). For 0 < p, q ≤∞, α > 0, r := ⌊α⌋+ 1, let the seminorm\n| · |Bα\np,q be\n|f|Bα\np,q :=\n(\u0000R ∞\n0 (t−αwr,p(f, t))q dt\nt\n\u0001 1\nq\n(q < ∞),\nsupt>0 t−αwr,p(f, t)\n(q = ∞).\nThe norm of the Besov space Bα\np,q(Ω) can be deﬁned by ∥f∥Bα\np,q := ∥f∥p + |f|Bα\np,q, and we have\nBα\np,q(Ω) = {f ∈Lp(Ω) | ∥f∥Bα\np,q < ∞}.\nNote that p, q < 1 is also allowed. In that setting, the Besov space is no longer a Banach space but a\nquasi-Banach space. The Besov space plays an important role in several ﬁelds such as nonparametric\nstatistical inference (Gin´e & Nickl, 2015) and approximation theory (Temlyakov, 1993a). These\nspaces are closely related to each other as follows (Triebel, 1983):\n• For m ∈N, Bm\np,1(Ω) ֒→W m\np (Ω) ֒→Bm\np,∞(Ω), and Bm\n2,2(Ω) = W m\n2 (Ω).\n• For 0 < s < ∞and s ̸∈N, Cs(Ω) = Bs\n∞,∞(Ω).\n• For 0 < s, p, q, r ≤∞with s > δ := d(1/p −1/r)+, it holds that Bs\np,q(Ω) ֒→Bs−δ\nr,q (Ω).\nIn particular, under the same condition, from the deﬁnition of ∥· ∥Bsp,q, it holds that\nBs\np,q(Ω) ֒→Lr(Ω).\n(1)\n• For 0 < s, p, q ≤∞, if s > d/p, then\nBs\np,q(Ω) ֒→C0(Ω).\n(2)\nHence, if the smoothness parameter satisﬁes s > d/p, then it is continuously embedded in the set\nof the continuous functions. However, if s < d/p, then the elements in the space are no longer\ncontinuous. Moreover, it is known that B1\n1,1([0, 1]) is included in the space of bounded total varia-\ntion (Peetre & Dept, 1976). Hence, the Besov space also allows spatially inhomogeneous smooth-\nness with spikes and jumps; which makes difference between linear estimators and deep learning\n(see Sec. 4.1).\nIt is known that the minimax rate to estimate f o is lower bounded by n−2s/(2s+d), (Gin´e & Nickl,\n2015). We see that the curse of dimensionality is unavoidable as long as we consider the Besov\nspace. This is an undesirable property because we easily encounter high dimensional data in several\nmachine learning problems. Hence, we need another condition to derive approximation and estima-\ntion error bounds that are not heavily affected by the dimensionality. To do so, we introduce the\nnotion of mixed smoothness.\nThe Besov space with mixed smoothness is deﬁned as follows (Schmeisser, 1987; Sickel & Ullrich,\n2009). To deﬁne the space, we deﬁne the coordinate difference operator as\n∆r,i\nh (f)(x) = ∆r\nh(f(x1, . . . , xi−1, ·, xi+1, . . . , xd))(xi)\nfor f : Rd →R, h ∈R+, i ∈[d], and r ≥1. Accordingly, the mixed differential operator for\ne ⊂{1, . . . , d} and h ∈Rd is deﬁned as\n∆r,e\nh (f) =\n\u0010Q\ni∈e ∆r,i\nhi\n\u0011\n(f), ∆r,∅\nh (f) = f.\nThen, the mixed modulus of smoothness is deﬁned as\nwe\nr,p(f, t) := sup|hi|≤ti,i∈e ∥∆r,e\nh (f)∥p\nfor t ∈Rd\n+ and 0 < p ≤∞. Letting 0 < p, q ≤∞, α ∈Rd\n++ and ri := ⌊αi⌋+ 1, the semi-norm\n| · |MBα,e\np,q based on the mixed smoothness is deﬁned by\n|f|MBα,e\np,q :=\n\n\n\nnR\nΩ[(Q\ni∈e t−αi\ni\n)we\nr,p(f, t)]q\ndt\nQ\ni∈e ti\no1/q\n(0 < q < ∞),\nsupt∈Ω(Q\ni∈e t−αi\ni\n)we\nr,p(f, t)\n(q = ∞).\nBy summing up the semi-norm over the choice of e, the (quasi-)norm of the mixed smooth Besov\nspace (abbreviated to m-Besov space) is deﬁned by\n∥f∥MBα\np,q := ∥f∥p +\nX\ne⊂{1,...,d}\n|f|MBα,e\np,q ,\n4\nand thus MBα\np,q(Ω) := {f ∈Lp(Ω) | ∥f∥MBα\np,q < ∞} where 0 < p, q ≤1 and α ∈Rd\n++. In this\npaper, we assume that α1 = · · · = αd. With a slight abuse of notation, we also use the notation\nMBα\np,q for α > 0 to indicate MB(α,...,α)\np,q\n.\nFor α ∈Rd\n+, if p = q, the m-Besov space has an equivalent norm with the tensor product of the\none-dimensional Besov spaces:\nMBα\np,p = Bα1\np,p ⊗δp · · · ⊗δp Bαd\np,p,\nwhere ⊗δp is a tensor product with respect to the p-nuclear tensor norm (see Sickel & Ullrich (2009)\nfor its deﬁnition and more details). We can see that the following models are included in the m-Besov\nspace:\n• Additive model Meier et al. (2009): if fj ∈Bαj\np,q([0, 1]) for j = 1, . . . , d,\nf(x) =\nd\nX\nr=1\nfd(xd) ∈MBα\np,q(Ω),\n• Tensor model Signoretto et al. (2010): if fr,j ∈Bαj\np,q([0, 1]) for r = 1, . . . , R and j =\n1, . . . , d,\nf(x) =\nR\nX\nr=1\nd\nY\nj=1\nfr,j(xj) ∈MBα\np,q(Ω).\n(m-Besov space allows R →∞if the summation converges with respect to the quasi-norm\nof ∥· ∥MBα\np,q).\nIt is known that an appropriate estimator in these models can avoid curse of dimensionality\n(Meier et al., 2009; Raskutti et al., 2012b; Kanagawa et al., 2016; Suzuki et al., 2016). What we\nwill show in this paper supports that this fact is also applied to deep learning from a unifying view-\npoint.\nThe difference between the (normal) Besov space and the m-Besov space can be informally ex-\nplained as follows. For regularity condition αi ≤2 (i = 1, 2), the m-Besov space consists of\nfunctions for which the following derivatives are “bounded”:\n∂f\n∂x1\n, ∂f\n∂x2\n, ∂2f\n∂x2\n1\n, ∂2f\n∂x2\n2\n,\n∂2f\n∂x1∂x2\n,\n∂3f\n∂x1∂x2\n2\n,\n∂3f\n∂x2\n1∂x2\n,\n∂4f\n∂x2\n1∂x2\n2\n.\nThat is, the “max” of the orders of derivatives over coordinates needs to be bounded by 2. On the\nother hand, the Besov space only ensures the boundedness of the following derivatives:\n∂f\n∂x1\n, ∂f\n∂x2\n, ∂2f\n∂x2\n1\n, ∂2f\n∂x2\n2\n,\n∂2f\n∂x1∂x2\n,\nwhere the “sum” of the orders needs to be bounded by 2. This difference directly affects the rate of\nconvergence of approximation accuracy. Further details about this space and related topics can be\nfound in a comprehensive survey (D˜ung et al., 2016).\nRelation to Barron class.\nBarron (1991; 1993; 1994) showed that, if the Fourier transform of\na function f satisﬁes some integrability condition, then we may avoid curse of dimensionality for\nestimating neural networks with sigmoidal activation functions. The integrability condition is given\nby\nZ\nCd ∥ω∥| ˆf(ω)|dω < ∞,\nwhere ˆf is the Fourier transform of a function f. We call the class of functions satisfying this\ncondition Barron class. A similar function class is analyzed by Klusowski & Barron (2016) too. We\ncannot compare directly the m-Besov space and Barron class, but they are closely related. Indeed,\nif p = q = 2 and s = α1 = · · · = αd, then m-Besov space MBs\n2,2(Ω) is equivalent to the tensor\nproduct of Sobolev space Sickel & Ullrich (2011) which consists of functions f : Ω→R satisfying\nZ\nCd\nd\nY\ni=1\n(1 + |ωi|2)s| ˆf(ω)|2dω < ∞.\nTherefore, our analysis gives a (similar but) different characterization of conditions to avoid curse\nof dimensionality.\n5\n3\nAPPROXIMATION ERROR ANALYSIS\nIn this section, we evaluate how well the functions in the Besov and m-Besov spaces can be ap-\nproximated by neural networks with the ReLU activation. Let us denote the ReLU activation by\nη(x) = max{x, 0} (x ∈R), and for a vector x, η(x) is operated in an element-wise manner. Deﬁne\nthe neural network with height L, width W, sparsity constraint S and norm constraint B as\nΦ(L, W, S, B) := {(W (L)η(·) + b(L)) ◦· · · ◦(W (1)x + b(1))\n| W (ℓ) ∈RW×W , b(ℓ) ∈RW ,\nL\nX\nℓ=1\n(∥W (ℓ)∥0 + ∥b(ℓ)∥0) ≤S, max\nℓ\n∥W (ℓ)∥∞∨∥b(ℓ)∥∞≤B},\nwhere ∥· ∥0 is the ℓ0-norm of the matrix (the number of non-zero elements of the matrix) and ∥· ∥∞\nis the ℓ∞-norm of the matrix (maximum of the absolute values of the elements). We want to evaluate\nhow large L, W, S, B should be to approximate f o ∈MBα\np,q(Ω) by an element f ∈Φ(L, W, S, B)\nwith precision ǫ > 0 measured by Lr-norm: minf∈Φ ∥f −f o∥r ≤ǫ.\n3.1\nAPPROXIMATION ERROR ANALYSIS FOR BESOV SPACES\nHere, we show how the neural network can approximate a function in the Besov space which is\nuseful to derive the generalization error of deep learning. Although its derivation is rather standard\nas considered in Chui et al. (1994); B¨olcskei et al. (2017), it should be worth noting that the bound\nderived here cannot be attained any non-adaptive method and the generalization error based on the\nanalysis is also unattainable by any linear estimators including the kernel ridge regression. That\nexplains the high adaptivity of deep neural network and how it outperforms usual linear methods\nsuch as kernel methods.\nTo show the approximation accuracy, a key step is to show that the ReLU neural network can ap-\nproximate the cardinal B-spline with high accuracy. Let N(x) = 1 (x ∈[0, 1]), 0 (otherwise), then\nthe cardinal B-spline of order m is deﬁned by taking m + 1-times convolution of N:\nNm(x) = (N ∗N ∗· · · ∗N\n|\n{z\n}\nm + 1 times\n)(x),\nwhere f ∗g(x) :=\nR\nf(x −t)g(t)dt. It is known that Nm is a piece-wise polynomial of order m.\nFor k = (k1, . . . , kd) ∈Nd and j = (j1, . . . , jd) ∈Nd, let M d\nk,j(x) = Qd\ni=1 Nm(2kixi −ji). Even\nfor k ∈N, we also use the same notation to express M d\nk,j(x) = Qd\ni=1 Nm(2kxi −ji). Here, k\ncontrols the spatial “resolution” and j speciﬁes the location on which the basis is put. Basically, we\napproximate a function f in a Besov space by a super-position of M m\nk,j(x), which is closely related\nto wavelet analysis (Mallat, 1999).\nMhaskar & Micchelli (1992); Chui et al. (1994) have shown the approximation ability of neural net-\nwork for a function with bounded modulus of smoothness. However, the activation function dealt\nwith by the analysis does not include ReLU but it deals with a class of activation functions satisfying\nthe following conditions,\nlim\nx→∞η(x)/xk →1,\nlim\nx→−∞η(x)/xk = 0, ∃K > 1 s.t. |η(x)| ≤K(1 + |x|)k (x ∈R),\n(3)\nfor k = 2 which excludes ReLU. Mhaskar (1993) analyzed deep neural network under the same\nsetting but it restricts the smoothness parameter to s = k + 1. Mhaskar (1996) considered the\nSobolev space W m\np\nwith an inﬁnitely many differentiable “bump” function which also excludes\nReLU. However, approximating the cardinal B-spline by ReLU can be attained by appropriately\nusing the technique developed by Yarotsky (2016) as in the following lemma.\nLemma 1 (Approximation of cardinal B-spline basis by the ReLU activation). There exists a con-\nstant c(d,m) depending only on d and m such that, for all ǫ > 0, there exists a neural network ˇ\nM ∈\nΦ(L0, W0, S0, B0) with L0 := 3+2\nl\nlog2\n\u0010\n3d∨m\nǫc(d,m)\n\u0011\n+ 5\nm\n⌈log2(d ∨m)⌉, W0 := 6dm(m+2)+2d,\nS0 := L0W 2\n0 and B0 := 2(m + 1)m that satisﬁes\n∥M d\n0,0 −ˇ\nM∥L∞(Rd) ≤ǫ,\nand ˇ\nM(x) = 0 for all x ̸∈[0, m + 1]d.\n6\nThe proof is in Appendix A. Based on this lemma, we can translate several B-spline approxima-\ntion results into those of deep neural network approximation. In particular, combining this lemma\nand the B-spline interpolant representations of functions in Besov spaces (DeVore & Popov, 1988;\nDeVore et al., 1993; D˜ung, 2011b), we obtain the optimal approximation error bound for deep neural\nnetworks. Here, let U(H) be the unit ball of a quasi-Banach space H, and for a set F of functions,\ndeﬁne the worst case approximation error as\nRr(F, H) :=\nsup\nf o∈U(H)\ninf\nf∈F ∥f o −f∥Lr([0,1]d).\nProposition 1 (Approximation ability for Besov space). Suppose that 0 < p, q, r ≤∞and 0 <\ns < ∞satisfy the following condition:\ns > d(1/p −1/r)+.\n(4)\nAssume that m ∈N satisﬁes 0 < s < min(m, m −1 + 1/p). Let ν = (s −δ)/(2δ). For sufﬁciently\nlarge N ∈N and ǫ = N −s/d−(ν−1+d−1)(d/p−s)+ log(N)−1, let\nL = 3 + 2⌈log2\n\u0012 3d∨m\nǫc(d,m)\n\u0013\n+ 5⌉⌈log2(d ∨m)⌉,\nW = NW0,\nS = (L −1)W 2\n0 N + N,\nB = O(N (ν−1+d−1)(1∨(d/p−s)+)),\nthen it holds that\nRr(Φ(L, W, S, B), Bs\np,q([0, 1]d)) ≲N −s/d.\nRemark 1. By Eq. (1), the condition (4) indicates that f o ∈Bs\np,q satisﬁes f o ∈Lr(Ω). If we set\np = q = ∞and r = ∞, then Bs\np,q(Ω) = Cs(Ω) which yields the result by Yarotsky (2016) as a\nspecial case.\nThe proof is in Appendix B. An interesting point is that the statement is valid even for p ̸= r. In par-\nticular, the theorem also supports non-continuous regime (s < d/p) in which L∞-convergence does\nno longer hold but instead Lr-convergence is guaranteed under the condition s > d(1/p −1/r)+.\nIn that sense, the convergence of the approximation error is guaranteed in considerably general set-\ntings. Pinkus (1999) gave an explicit form of convergence when 1 ≤p = r for the activation\nfunctions satisfying Eq. (3) which does not cover ReLU and an important setting p ̸= r. Petrushev\n(1998) considered p = r = 2 and activation function with Eq. (3) where s is an integer and\ns ≤k +1+(d−1)/2. Chui et al. (1994) and B¨olcskei et al. (2017) dealt with the smooth sigmoidal\nactivation satisfying the condition (3) with k ≥2 or a “smoothed version” of the ReLU activation\nwhich excludes ReLU; but B¨olcskei et al. (2017) presented a general strategy for neural-net approx-\nimation by using the notion of best M-term approximation. Mhaskar & Micchelli (1992) gives an\napproximation bound using the modulus of smoothness, but the smoothness s and the order of sig-\nmoidal function k in (3) is tightly connected and f o is assumed to be continuous which excludes the\nsituation s < d/p. On the other hand, the above proposition does not require such a tight connection\nand it explicitly gives the approximation bound for Besov spaces. Williamson & Bartlett (1992)\nderived a spline approximation error bound for an element in a Besov space when d = 1, but the de-\nrived bound is only O(N −s+(1/p−1/r)+) which is the one of non-adaptive methods described below,\nand approximation by a ReLU activation network is not discussed. We may also use the analysis of\nCohen et al. (2001) which is based on compactly supported wavelet bases, but the cardinal B-spline\nis easy to handle through quasi-interpolant representation as performed in the proof of Proposition\n1.\nIt should be noted that the presented approximation accuracy bound is not trivial because it can not\nbe achieved by a non-adaptive method. Actually, the linear N-width (Tikhomirov, 1960) of the\nBesov space is lower bounded as\ninf\nLN\nsup\nf∈U(MBs\np,q)\n∥f −LN(f)∥r ≳\n\n\n\n\n\n\n\nN −s/d+(1/p−1/r)+\n\n\n\neither\n(0 < p ≤r ≤2),\nor\n(2 ≤p ≤r ≤∞),\nor\n(0 < r ≤p ≤∞),\nN −s/d+1/p−1/2\n(0 < p < 2 < r < ∞, s > d max(1 −1/r, 1/p),\n(5)\n7\nwhere the inﬁmum is taken over all linear oprators LN with rank N from Bs\np,q to Lr (see Vyb´aral\n(2008) for more details). Similarly, the best N-term approximation error (Kolmorogov width) of\nthe Besov space is lower bounded as\ninf\nSN⊂Bsp,q\nsup\nf∈U(Bsp,q)\ninf\nˇ\nf∈SN\n∥f −ˇf∥Lr(Ω) ≳\n\n\n\nN −s/d+(1/p−1/r)+\n(1 < p < r ≤2, s > d(1/p −1/r)),\nN −s/d+1/p−1/2\n(1 < p < 2 < r ≤∞, s > d/p),\nN −s/d\n(2 ≤p < r ≤∞, s > d/2),\n(6)\nif 1 < p < r ≤∞, 1 ≤q < ∞and 1 < s, where SN is any N-dimensional subspace of Bs\np,q (see\nVyb´aral (2008), and see also Romanyuk (2009); Myronyuk (2016) for a related space). That is, any\nlinear/non-linear approximator with ﬁxed N-bases does not achieve the approximation error N −α/d\nin some parameter settings such as 0 < p < 2 < r. On the other hand, adaptive methods including\ndeep learning can improve the error rate up to N −α/d which is rate optimal (D˜ung, 2011b). The\ndifference is signiﬁcant when p < r. This implies that deep neural network possesses high adaptivity\nto ﬁnd which part of the function should be intensively approximated. In other words, deep neural\nnetwork can properly extracts the feature of the input (which corresponds to construct an appropriate\nset of bases) to approximate the target function in the most efﬁcient way.\n3.2\nAPPROXIMATION ERROR ANALYSIS FOR M-BESOV SPACE\nHere, we deal with m-Besov spaces instead of the ordinary Besov space. The next theorem gives the\napproximation error bound to approximate functions in the m-Besov spaces by deep neural network\nmodels. Here, deﬁne Dk,d :=\n\u00001 + d−1\nk\n\u0001k \u0010\n1 +\nk\nd−1\n\u0011d−1\n. Then, we have the following theorem.\nTheorem 1 (Approximation ability for m-Besov space). Suppose that 0 < p, q, r ≤∞and s < ∞\nsatisﬁes s > (1/p −1/r)+. Assume that m ∈N satisﬁes 0 < s < min(m, m −1 + 1/p). Let\nδ = (1/p −1/r)+ and ν = (s −δ)/(2δ). For any K ≥1, let K∗= ⌈K(1 +\n2δ\nα−δ)⌉. Then, for\nN = (2 + (1 −2−ν)−1)2KDK∗,d, if we set\nL = 3 + 2\nl\nlog2\n\u0010\n3d∨m\nc(d,m)\n\u0011\n+ 5 + (s + ( 1\np −s)+ + 1)K∗+ log([e(m + 1)]d(1 + K∗))\nm\n⌈log2(d ∨m)⌉,\nW = NW0, S = (L −1)NW 2\n0 + N, B = O(N (ν−1+1)(1∨(1/p−s)+)),\nthen it holds that\n(i) For p ≥r,\nRr(Φ(L, W, S, B), MBs\np,q([0, 1]d)) ≲2−KsD(1/ min(r,1)−1/q)+\nK,d\n,\n(7a)\n(ii) For p < r,\nRr(Φ(L, W, S, B), MBs\np,q([0, 1]d)) ≲\n(\n2−KsD(1/r−1/q)+\nK,d\n(r < ∞),\n2−KsD(1−1/q)+\nK,d\n(r = ∞).\n(7b)\nThe proof is given in Appendix C. Now, the number S of non-zero parameters for a given K is\nevaluated as S = Ω(N) ≃2KDK,d in this theorem. It holds that N ≃2KK(d−1), which implies\n2−K ≃N −1 logd−1(N) if N ≫d (see also the discussion right after Theorem 5 in Appendix\nD.1 for more details of calculation). Therefore, when r ≫q, the approximation error is given as\nO(N −s logs(d−1)(N)) in which the effect of dimensionality d is much milder than that of Proposi-\ntion 1. This means that the curse of dimensionality is much eased in the mixed smooth space.\nThe obtained bound is far from obvious. Actually, it is better than any linear approximation methods\nas follows. Let the linear M-width introduced by Tikhomirov (1960) be\nλN(MBs\np,q, Lr) := inf\nLN\nsup\nf∈U(MBs\np,q)\n∥f −LN(f)∥r,\nwhere the inﬁmum is taken over all linear oprators LN with rank N from MBs\np,q to Lr. The linear\nN-width of the m-Besov space has been extensively studies as in the following proposition (see\nLemma 5.1 of D˜ung (2011a), and Romanyuk (2001)).\n8\nProposition 2. Let 1 ≤p, r ≤∞, 0 < q ≤∞and s > (1/p −1/r)+. Then we have the following\nasymptotic order of the linear width for the asymptotics N ≫d:\n(a) For p ≥r,\nλN(MBs\np,q, Lr) ≃\n\n\n\n\n\n\n\n\n\n\n\n(N −1 logd−1(N))s\n\n\n\n(q ≤2 ≤r ≤p < ∞),\n(q ≤1, p = r = ∞),\n(1 < p = r ≤2, q ≤r),\n(N −1 logd−1(N))s(logd−1(N))1/r−1/q\n(1 < p = r ≤2, q > r),\n(N −1 logd−1(N))s(logd−1(N))(1/2−1/q)+\n(2 ≤q, 1 < r < 2 ≤p < ∞),\n(b) For 1 < p < r < ∞,\nλM(MBs\np,q, Lr) ≃\n\u001a(N −1 logd−1(N))s+1/r−1/p\n(2 ≤p, 2 ≤q ≤r),\n(N −1 logd−1(N))s+1/r−1/p(logd−1(N))(1/r−1/q)+\n(r ≤2).\nTherefore, the approximation error given in Theorem 1 achieves the optimal linear width\n((N −1 logd−1(N))s) for several parameter settings of p, q, s. In particular, when p < r, the bound\nin Theorem 1 is better than that of Proposition 2. This is because to prove Theorem 1, we used an\nadaptive recovery technique instead of a linear recovery method. This implies that, by constructing\na deep neural network accurately, we achieve the same approximation accuracy as the adaptive one\nwhich is better than that of linear approximation.\n4\nESTIMATION ERROR ANALYSIS\nIn this section, we connect the approximation theory to generalization error analysis (estimation er-\nror analysis). For the statistical analysis, we assume the following nonparametric regression model:\nyi = f o(xi) + ξi\n(i = 1, . . . , n),\nwhere xi ∼PX with density 0 ≤p(x) < R on [0, 1]d, and ξi ∼N(0, σ2). The data Dn =\n(xi, yi)n\ni=1 is independently identically distributed. We want to estimate f o from the data. Here, we\nconsider a regularized learning procedure:\nbf =\nargmin\n¯\nf:f∈Φ(L,W,S,B)\nn\nX\ni=1\n(yi −¯f(xi))2\nwhere ¯f is the clipping of f deﬁned by ¯f = min{max{f, −F}, F} for F > 0 which is realized by\nReLU units. Since the sparsity level is controlled by S and the parameter is bounded by B, this esti-\nmator can be regarded as a regularized estimator. In practice, it is hard to exactly compute bf. Thus,\nwe approximately solve the problem by applying sparse regularization such as L1-regularization and\noptimal parameter search through Bayesian optimization. The generalization error that we present\nhere is an “ideal” bound which is valid if the optimal solution bf is computable.\n4.1\nESTIMATION ERROR IN BESOV SPACES\nIn this subsection, we provide the estimation error rate of deep learning to estimate functions in\nBesov spaces by using the approximation error bound given in the previous sections.\nTheorem 2. Suppose that 0 < p, q ≤∞and s > d(1/p −1/2)+. If f o ∈Bs\np,q(Ω) ∩L∞(Ω) and.\n∥f o∥Bsp,q ≤1 and ∥f o∥∞≤F for F ≥1, then letting (W, L, S, B) be as in Proposition 1 with\nN ≍n\nd\n2s+d , we obtain\nEDn[∥f o −bf∥2\nL2(PX)] ≲n−\n2s\n2s+d log(n)2,\nwhere EDn[·] indicates the expectation w.r.t. the training data Dn.\nThe proof is given in Appendix E. The condition ∥f o∥∞≤F is required to connect the empirical\nL2-norm 1\nn\nPn\ni=1( bf(xi) −f o(xi))2 to the population L2-norm ∥bf −f o∥2\nL2(PX). It is known that\nthe convergence rate n−\n2s\n2s+d is mini-max optimal (Donoho et al., 1998; Gin´e & Nickl, 2015). Thus,\n9\nit cannot be improved by any estimator. Therefore, deep learning can achieve the minimax optimal\nrate up to log(n)2-order. The term log(n)2 could be improved to log(n) by using the construction\nof Petersen & Voigtlaender (2017). However, we don’t pursue this direction for simplicity.\nHere an important remark is that this minimax optimal rate cannot be achieved by any linear es-\ntimator. We call an estimator linear when the estimator depends on (yi)n\ni=1 linearly (it can be\nnon-linearly dependent on (xi)n\ni=1). Several classical methods such as the kernel ridge regression,\nthe Nadaraya-Watson estimator and the sieve estimator are included in the class of linear estimators\n(e.g., kernel ridge regression is given as bf(x) = kx,X(kXX + λI)−1Y ). The following proposition\ngiven by Donoho et al. (1998); Zhang et al. (2002) states that the minimax rate of linear estimators\nis lower bounded by n\n−\n2s−2(1/p−1/2)+\n2s+1−2(1/p−1/2)+ which is larger than the minimax rate n−\n2s\n2s+2 if p < 2.\nProposition 3 (Donoho et al. (1998); Zhang et al. (2002)). Suppose that d = 1 and the input dis-\ntribution PX is the uniform distribution on [0, 1]. Assume that s > 1/p, 1 ≤p, q ≤∞or\ns = p = q = 1. Then,\ninf\nb\nf: linear\nsup\nf o∈U(Bsp,q)\nEDn[∥f o −bf∥2\nL2(PX)] ≳n−\n2s−v\n2s+1−v\nwhere v = 2/(p∧2)−1 and bf runs over all linear estimators, that is, bf depends on (yi)n\ni=1 linearly.\nWhen p < 2, the smoothness of the Besov space is somewhat inhomogeneous, that is, a function in\nthe Besov space contains spiky/jump parts and smooth parts (remember that when s = p = q = 1\nfor d = 1, the Besov space is included in the set of functions with bounded total variation). Here, the\nsetting p < 2 is the regime where there appears difference between non-adaptive methods and deep\nlearning in terms of approximation accuracy (see Eq. (6)). On the other hand, the linear estimator\ncaptures only global properties of the function and cannot capture variability of local shapes of the\nfunction. Hence, the linear estimator cannot achieve the minimax optimal rate if the function has\nspatially inhomogeneous smoothness. However, deep learning possesses adaptivity to the spatial\ninhomogeneity.\nWe would like to remark that The shrinkage estimator proposed in Donoho et al. (1998); Zhang et al.\n(2002) achieves the minimax optimal rate for s > 1/p with d = 1 and 1 ≤p, q ≤∞which excludes\nan interesting setting such as s = p = q = 1. However, the result of Theorem 2 also covers more\ngeneral settings where d ≥1 and s > d(1/p −1/2)+ with 0 < p, q ≤∞.\nImaizumi & Fukumizu (2018) has pointed out that such a discrepancy between deep learning and\nlinear estimator appears when the target function is non-smooth. Interestingly, the parameter set-\nting s > 1/p assumed in Proposition 3 ensures smoothness (see Eq. (2)). This means that non-\nsmoothness is not necessarily required to characterize the superiority of deep learning, but non-\nconvexity of the set of target functions is essentially important. In fact, the gap is coming from\nthe property that the quadratic hull of the model U(Bs\np,q) is strictly larger than the original set\n(Donoho et al., 1998).\n4.2\nESTIMATION ERROR IN MIXED SMOOTH BESOV SPACES\nHere, we provide the estimation error rate of deep learning to estimate functions in mixed smooth\nBesov spaces.\nTheorem 3. Suppose that 0 < p, q ≤∞and s > (1/p −1/2)+. Let u = (1 −1/q)+ for p ≥2 and\nu = (1/2 −1/q)+ for p < 2. If f o ∈MBs\np,q(Ω) ∩L∞(Ω) and ∥f o∥MBsp,q ≤1 and ∥f o∥∞≤F\nfor F ≥1, then letting (W, L, S, B) be as in Theorem 1, we obtain\nEDn[∥f o −bf∥2\nL2(PX)] ≲n−\n2s\n2s+1 log(n)\n2(d−1)(u+s)\n1+2s\nlog(n)2.\nUnder the same assumption, if s > u log2(e) is additionally satisﬁed, we also have\nEDn[∥f o −bf∥2\nL2(PX)] ≲n−\n2s−2u log2(e)\n2s+1+(1−2u) log2(e) log(n)2.\nThe proof is given in Appendix E. The risk bound (Theorem 3) indicates that the curse of dimen-\nsionality can be eased by assuming the mixed smoothness compared with the ordinary Besov space\n10\n(n−\n2s\n2s+d ). We show that this is almost minimax optimal in Theorem 4 below. In the ﬁrst bound, the\ndimensionality d comes in the exponent of poly log(n) term. If u = 0, then the effect of d can be\nfurther eased. Actually, in this situation (u = 0), the second bound can be rewritten as\nn−\n2s\n2s+1+log2(e) log(n)2,\nwhere the effect of the dimensionality d completely disappears from the exponent. This explains\npartially why deep learning performs well for high dimensional data. Montanelli & Du (2017) has\nanalyzed the mixed smooth H¨older space with s < 2. However, our analysis is applicable to the\nm-Besov space which is more general than the mixed smooth H¨older space and the covered range\nof s, p, q is much larger.\nHere, we again remark the adaptivity of deep learning. Remind that this rate cannot be achieved by\nthe linear estimator for p < 2 when d = 1 by Proposition 3. This explains the adaptivity ability of\ndeep learning to the spatial inhomogeneity of the smoothness.\nMinimax optimal rate for estimating a function in the m-Besov space\nHere, we show the min-\nimax optimality of the obtained bound as follows.\nTheorem 4. Assume that 0 < p, q ≤∞and s > (1/p −1/2)+ and PX is the uniform distribution\nover [0, 1]d. Regarding d as a constant, the minimax learning rate in the asymptotics of n →∞is\nlower bounded as follows: There exists a constant bC1 such that\ninf\nb\nf\nsup\nf o∈U(MBsp,q)\nEDn[∥bf −f o∥2\nL2(PX)] ≥bC1n−\n2s\n2s+1 log(n)\n2(d−1)(s+1/2−1/q)+\n2s+1\n(8)\nwhere “inf” is taken over all measurable functions of the observations (xi, yi)n\ni=1 and the expecta-\ntion is taken for the sample distribution.\nThe proof is given in Appendix F. Because of this theorem, our bound given in Theorem 3 achieves\nthe minimax optimal rate in the regime of p < 2 and 1/2 −1/q > 0 up to log(n)2 order. Even\noutside of this parameter setting, the discrepancy between our upper bound and the minimax lower\nbound is just a poly-log oder. See also Neumann (2000) for some other related spaces and speciﬁc\nexamples such as p = q = 2.\n5\nCONCLUSION\nThis paper investigated the learning ability of deep ReLU neural network when the target function\nis in a Besov space or a mixed smooth Besov space. Based on the analysis for the Besov space, it\nis shown that deep learning using the ReLU activation can achieve the minimax optimal rate and\noutperform the linear method when p < 2 which indicates the spatial inhomogeneity of the shape\nof the target function. The analysis for the mixed smooth Besov space shows that deep learning can\nadaptively avoid the curse of dimensionality. The bound is derived by sparse grid technique. All\nanalyses in the paper adopted the cardinal B-spline expansion and the adaptive non-linear approx-\nimation technique, which allowed us to show the minimax optimal rate. The consequences of the\nanalyses partly support the superiority of deep leaning in terms of adaptivity and ability to avoid\ncurse of dimensionality. From more high level view point, these favorable property is reduced to its\nhigh feature extraction ability.\nThis paper did not discuss any optimization aspect of deep learning. However, it is important to\ninvestigate what kind of practical algorithms can actually achieve the optimal rate derived in this\npaper in an efﬁcient way. We leave this important issue for future work.\nACKNOWLEDGMENT\nTS was partially supported by MEXT Kakenhi (25730013, 25120012, 26280009, 15H05707 and\n18H03201), Japan Digital Design, and JST-CREST.\nREFERENCES\nR.A. Adams and J.J.F. Fournier. Sobolev Spaces. Pure and Applied Mathematics. Elsevier Science,\n2003.\n11\nAndrew Barron. Approximation and estimation bounds for artiﬁcial neural networks. In Proceedings\nof the Fourth Annual Workshop on Computational Learning Theory, pp. 243–249, 1991.\nAndrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function.\nIEEE Transactions on Information theory, 39(3):930–945, 1993.\nAndrew R Barron. Approximation and estimation bounds for artiﬁcial neural networks. Machine\nLearning, 14(1):115–133, 1994.\nMonica Bianchini and Franco Scarselli. On the complexity of neural network classiﬁers: A compar-\nison between shallow and deep architectures. IEEE transactions on neural networks and learning\nsystems, 25(8):1553–1565, 2014.\nHelmut B¨olcskei, Philipp Grohs, Gitta Kutyniok, and Philipp Petersen. Optimal approximation with\nsparsely connected deep neural networks. arXiv preprint arXiv:1705.01714, 2017.\nCK Chui, Xin Li, and HN Mhaskar. Neural networks for localized approximation. Mathematics of\nComputation, 63(208):607–623, 1994.\nAlbert Cohen, Wolfgang Dahmen, Ingrid Daubechies, and Ronald DeVore. Tree approximation and\noptimal encoding. Applied and Computational Harmonic Analysis, 11(2):192–226, 2001.\nNadav Cohen and Amnon Shashua. Convolutional rectiﬁer networks as generalized tensor decom-\npositions. In Proceedings of the 33th International Conference on Machine Learning, volume 48\nof JMLR Workshop and Conference Proceedings, pp. 955–963, 2016.\nNadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor\nanalysis. In The 29th Annual Conference on Learning Theory, pp. 698–728, 2016.\nGeorge Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Con-\ntrol, Signals, and Systems (MCSS), 2(4):303–314, 1989.\nRonald A DeVore and Vasil A Popov. Interpolation of besov spaces. Transactions of the American\nMathematical Society, 305(1):397–414, 1988.\nRonald A DeVore, George Kyriazis, Dany Leviatan, and Vladimir M Tikhomirov. Wavelet com-\npression and nonlinearn-widths. Advances in Computational Mathematics, 1(2):197–214, 1993.\nDavid L Donoho, Iain M Johnstone, et al. Minimax estimation via wavelet shrinkage. The Annals\nof Statistics, 26(3):879–921, 1998.\nDinh D˜ung. On recovery and one-sided approximation of periodic functions of several variables. In\nDokl. Akad. SSSR, volume 313, pp. 787–790, 1990.\nDinh D˜ung. On optimal recovery of multivariate periodic functions. In Satoru Igari (ed.), ICM-90\nSatellite Conference Proceedings, pp. 96–105, Tokyo, 1991. Springer Japan. ISBN 978-4-431-\n68168-7.\nDinh D˜ung. Optimal recovery of functions of a certain mixed smoothness. Vietnam Journal of\nMathematics, 20(2):18–32, 1992.\nDinh D˜ung. B-spline quasi-interpolant representations and sampling recovery of functions with\nmixed smoothness. Journal of Complexity, 27(6):541–567, 2011a.\nDinh D˜ung. Optimal adaptive sampling recovery. Advances in Computational Mathematics, 34(1):\n1–41, 2011b.\nDinh D˜ung, Vladimir N Temlyakov, and Tino Ullrich. Hyperbolic cross approximation. arXiv\npreprint arXiv:1601.03978, 2016.\n´E. M. Galeev. Linear widths of h¨older-nikol’skii classes of periodic functions of several variables.\nMatematicheskie Zametki,, 59(2):189–199, 1996.\nE. Gin´e and R. Nickl. Mathematical Foundations of Inﬁnite-Dimensional Statistical Models. Cam-\nbridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2015.\n12\nXavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectiﬁer neural networks. In Pro-\nceedings of the 14th International Conference on Artiﬁcial Intelligence and Statistics, volume 15\nof Proceedings of Machine Learning Research, pp. 315–323, 2011.\nKurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural Networks, 4\n(2):251–257, 1991.\nMasaaki Imaizumi and Kenji Fukumizu. Deep neural networks learn non-smooth functions effec-\ntively. arXiv preprint arXiv:1802.04474, 2018.\nHeishiro Kanagawa, Taiji Suzuki, Hayato Kobayashi, Nobuyuki Shimizu, and Yukihiro Tagami.\nGaussian process nonparametric tensor estimator and its minimax optimality. In Proceedings of\nthe 33rd International Conference on Machine Learning (ICML2016), pp. 1632–1641, 2016.\nJason M Klusowski and Andrew R Barron. Risk bounds for high-dimensional ridge function com-\nbinations including neural networks. arXiv preprint arXiv:1607.01434, 2016.\nShiyu Liang and R Srikant. Why deep neural networks for function approximation? arXiv preprint\narXiv:1610.04161, 2016. ICLR2017.\nStephane Mallat. A Wavelet Tour of Signal Processing. Academic Press, 1999.\nLukas Meier, Sara van de Geer, and Peter B¨uhlmann. High-dimensional additive modeling. The\nAnnals of Statistics, 37(6B):3779–3821, 2009.\nHrushikesh N Mhaskar. Neural networks for optimal approximation of smooth and analytic func-\ntions. Neural computation, 8(1):164–177, 1996.\nHrushikesh N Mhaskar and Charles A Micchelli. Approximation by superposition of sigmoidal and\nradial basis functions. Advances in Applied mathematics, 13(3):350–373, 1992.\nHrushikesh Narhar Mhaskar. Approximation properties of a multilayered feedforward artiﬁcial neu-\nral network. Advances in Computational Mathematics, 1(1):61–80, 1993.\nHadrien Montanelli and Qiang Du. Deep relu networks lessen the curse of dimensionality. arXiv\npreprint arXiv:1712.08688, 2017.\nGuido F. Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear\nregions of deep neural networks. In Z. Ghahramani, M. Welling, C. Cortes, N.d. Lawrence, and\nK.q. Weinberger (eds.), Advances in Neural Information Processing Systems 27, pp. 2924–2932.\nCurran Associates, Inc., 2014.\nV Myronyuk. Kolmogorov widths of the anisotropic Besov classes of periodic functions of many\nvariables. Ukrainian Mathematical Journal, 68(5), 2016.\nVinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines.\nIn Proceedings of the 27th International Conference on Machine Learning, pp. 807–814, 2010.\nMichael H. Neumann. Multivariate wavelet thresholding in anisotropic function spaces. Statistica\nSinica, 10(2):399–431, 2000.\nJ. Peetre and Duke University. Mathematics Dept. New thoughts on Besov spaces. Duke University\nmathematics series. Mathematics Dept., Duke University, 1976.\nPhilipp Petersen and Felix Voigtlaender. Optimal approximation of piecewise smooth functions\nusing deep relu neural networks. arXiv preprint arXiv:1709.05289, 2017.\nPencho P Petrushev. Approximation by ridge functions and neural networks. SIAM Journal on\nMathematical Analysis, 30(1):155–189, 1998.\nAllan Pinkus. Approximation theory of the mlp model in neural networks. Acta numerica, 8:143–\n195, 1999.\n13\nBen Poole, Subhaneil Lahiri, Maithreyi Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Expo-\nnential expressivity in deep neural networks through transient chaos. In D. D. Lee, M. Sugiyama,\nU. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Sys-\ntems 29, pp. 3360–3368. Curran Associates, Inc., 2016.\nGarvesh Raskutti, Martin Wainwright, and Bin Yu. Minimax-optimal rates for sparse additive mod-\nels over kernel classes via convex programming. Journal of Machine Learning Research, 13:\n389–427, 2012a.\nGarvesh Raskutti, Martin J Wainwright, and Bin Yu. Minimax-optimal rates for sparse additive\nmodels over kernel classes via convex programming. The Journal of Machine Learning Research,\n13(1):389–427, 2012b.\nA. S. Romanyuk. Linear widths of the besov classes of periodic functions of many variables. ii.\nUkrainian Mathematical Journal, 53(6):965–977, Jun 2001.\nA. S. Romanyuk. Bilinear approximations and Kolmogorov widths of periodic Besov classes. The-\nory of Operators, Differential Equations, and the Theory of Functions, 6(1):222–236, 2009. Proc.\nof the Institute of Mathematics, Ukrainian National Academy of Sciences.\nH-J Schmeisser. An unconditional basis in periodic spaces with dominating mixed smoothness\nproperties. Analysis Mathematica, 13(2):153–168, 1987.\nJ. Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU activation\nfunction. ArXiv e-prints, August 2017.\nWinfried Sickel and Tino Ullrich. Tensor products of Sobolev–Besov spaces and applications to\napproximation from the hyperbolic cross. Journal of Approximation Theory, 161(2):748–786,\n2009.\nWinfried Sickel and Tino Ullrich. Spline interpolation on sparse grids. Applicable Analysis, 90(3-4):\n337–383, 2011.\nM. Signoretto, L. De Lathauwer, and J.A.K. Suykens. Nuclear norms for tensors and their use for\nconvex multilinear estimation. Technical Report 10-186, ESAT-SISTA, K.U.Leuven, 2010.\nSergey Smolyak. Quadrature and interpolation formulas for tensor products of certain classes of\nfunctions. In Soviet Math. Dokl., volume 4, pp. 240–243, 1963.\nSho Sonoda and Noboru Murata. Neural network with unbounded activation functions is universal\napproximator. Applied and Computational Harmonic Analysis, 2015.\nTaiji Suzuki, Heishiro Kanagawa, Hayato Kobayashi, Nobuyuki Shimizu, and Yukihiro Tagami.\nMinimax optimal alternating minimization for kernel nonparametric tensor learning. In Advances\nIn Neural Information Processing Systems, pp. 3783–3791, 2016.\nV.N. Temlyakov. Approximation of periodic functions of several variables with bounded mixed\ndifference. Math. USSR Sb, 41(1):53–66, 1982.\nV.N. Temlyakov. Approximation of Periodic Functions. Nova Science Publishers, 1993a.\nV.N. Temlyakov. On approximate recovery of functions with bounded mixed derivative. Journal of\nComplexity, 9:41–59, 1993b.\nVladimir Mikhailovich Tikhomirov. Diameters of sets in function spaces and the theory of best\napproximations. Uspekhi Matematicheskikh Nauk, 15(3):81–120, 1960.\nHans Triebel. Theory of function spaces. Monographs in mathematics. Birkh¨auser Verlag, 1983.\nISBN 9783764313814.\nA. W. van der Vaart and J. A. Wellner. Weak Convergence and Empirical Processes: With Applica-\ntions to Statistics. Springer, New York, 1996.\nJan Vyb´aral. Widths of embeddings in function spaces. Journal of Complexity, 24:545–570, 2008.\n14\nRobert C Williamson and Peter L Bartlett. Splines, rational functions and neural networks. In\nAdvances in Neural Information Processing Systems, pp. 1040–1047, 1992.\nYuhong Yang and Andrew Barron. Information-theoretic determination of minimax rates of conver-\ngence. The Annals of Statistics, 27(5):1564–1599, 1999.\nDmitry Yarotsky. Error bounds for approximations with deep relu networks. CoRR, abs/1610.01145,\n2016.\nShuanglin Zhang, Man-Yu Wong, and Zhongguo Zheng. Wavelet threshold estimation of a regres-\nsion function with random design. Journal of multivariate analysis, 80(2):256–284, 2002.\nA\nPROOF OF LEMMA 1\nProof of Lemma 1. First note that Nm(x) =\n1\nm!\nPm+1\nj=0 (−1)j\u0000m+1\nj\n\u0001\n(x −j)m\n+ (see Eq. (4.28) of\nMhaskar & Micchelli (1992) for example). Thus, if we can make an approximation of η(x)m, then\nby taking a summation of those basis, we obtain an approximate of Nm(x). It is shown by Yarotsky\n(2016) that, for D ∈N and any ǫ > 0, there exists a neural network φmult ∈Φ(L, W, S, B) with\nL = ⌈log2\n\u0010\n3D\nǫ\n\u0011\n+ 5⌉⌈log2(D)⌉, W = 6d, S = LW 2 and B = 1 such that\nsup\nx∈[0,1]d\n\f\f\f\f\fφmult(x1, . . . , xD) −\nD\nY\ni=1\nxi\n\f\f\f\f\f ≤ǫ,\nand φmult(0, . . . , 0) = 0 for y ∈Rd such that Qd\nj=1 yj = 0. Moreover, for any M > 0, we can\nrealize the function min{M, max{x, 0}} by a single-layer neural network φ(0,M)(x) := η(x) −\nη(x −M)(= min{M, max{x, 0}}). Thus, for x ∈R, it holds that\nsup\nx∈[0,M]\n\f\fφmult(φ(0,1)(x/M), . . . , φ(0,1)(x/M)) −(φ(0,1)(x/M))m\f\f ≤ǫ.\nNow,\nsince\nNm(x)\n=\n0\nfor\nx\n̸∈\n[0, m + 1],\nit\nalso\nholds\nNm(x)\n=\n1\nm!\nPm+1\nj=0 (−1)j\u0000m+1\nj\n\u0001\nφ(0,m+1−j)(x\n−\nj)m\n=\n1\nm!\nPm+1\nj=0 (−1)j\u0000m+1\nj\n\u0001\n(m\n+\n1)mφ(0,1−j/(m+1))((x −j)/(m + 1))m. Therefore, letting\nf(x) = 1\nm!\nm+1\nX\nj=0\n(−1)j(m+1)m\n\u0012m + 1\nj\n\u0013\nφmult\n \nφ(0,1−\nj\nm+1 )\n\u0012 x −j\nm + 1\n\u0013\n, . . . , φ(0,1−\nj\nm+1 )\n\u0012 x −j\nm + 1\n\u0013\n|\n{z\n}\nm-times\n!\n,\nwe have that f(x) = 0 for all x ≤0 and\nsup\n0≤x≤m+1\n|Nm(x) −f(x)| ≤1\nm!\nm+1\nX\nj=0\n\u0012m + 1\nj\n\u0013\n(m + 1)mǫ ≤\n(m + 1)m\n√\n2πmm+1/2e−m 2m+1ǫ\n≤e(2e)m\n√m ǫ =: ǫ′,\nwhere we used Pm+1\nj=0\n\u0000m+1\nj\n\u0001\n= 2m+1 and Stirling’s approximation m! ≥\n√\n2πmm+1/2e−m in the\nsecond inequality. Hence, we also have\nf(x) = 1\nm!\nm+1\nX\nj=0\n(−1)j\n\u0012m + 1\nj\n\u0013\n(m + 1)m\n× φmult\n\u0012\nφ(0,1−\nj\nm+1 )\n\u0012m + 1 −j\nm + 1\n\u0013\n, . . . , φ(0,1−\nj\nm+1 )\n\u0012m + 1 −j\nm + 1\n\u0013\u0013\n=: δ′ (∀x > m + 1).\n15\nIt holds that |δ′| ≤ǫ′. Because of this and noting 0 ≤Nm(x) ≤1, we see that g(x) := φ(0,1)(f(x)−\nδ′\nm+1φ(0,m+1)(x)) yields\nsup\nx∈R\n|Nm(x) −g(x)| ≤2ǫ′,\nsupx∈R |g(x)| ≤1, and g(x) = 0 for all x ̸∈[0, m + 1]. Hence, by applying φmult again, we ﬁnally\nobtain that\nsup\nx∈[0,1]d |M d\n0,0(x) −φmult(g(x1), . . . , g(xd))|\n≤\nsup\nx∈[0,1]d\n\f\f\f\f\f\f\nM d\n0,0(x) −\nd\nY\nj=1\ng(xj)\n\f\f\f\f\f\f\n+\nsup\nx∈[0,1]d\n\f\f\f\f\f\f\nd\nY\nj=1\ng(xj) −φmult(g(x1), . . . , g(xd))\n\f\f\f\f\f\f\n≤2dǫ′ + ǫ.\nWe again applying φ(0,1), we obtain that h = φ(0,1) ◦φmult(g(x1), . . . , g(xd)) satisﬁes ∥M d\n0,0 −\nh∥L∞(Rd) ≤2dǫ′+ǫ, h(x) = 0 for all x ̸∈[0, m+1]d, and ∥h∥∞≤1. Finally, by carefully checking\nthe network construction, it is shown that h ∈Φ(L, W, S, B) with L = 3 + 2⌈log\n\u0010\n3d∨m\nǫ\n\u0011\n+\n5⌉⌈log2(d ∨m)⌉, W = 6dm(m + 2) + 2d, S = LW 2 and B = 2(m + 1)m. Hence, resetting\nǫ ←2dǫ′ + ǫ = (1 + 2de (2e)m\n√m )ǫ, this h is the desired ˇ\nM.\nB\nPROOF OF PROPOSITION 1\nFor the order m ∈N of the cardinal B-spline bases, let J(k) = {−m, −m + 1, . . . , 2k −1, 2k}d\nand the quasi-norm of the coefﬁcient (αk,j)k,j for k ∈N+ and j ∈J(k) be\n∥(αk,j)k,j∥bsp,q. =\n\n\n\nX\nk∈N+\n\n2k(s−d/p)\u0010 X\nj∈J(k)\n|αk,j|p\u00111/p\n\n\nq\n\n\n1/q\n.\nLemma 2. Under one of the conditions (4) in Proposition 1 and the condition 0 < s < min(m, m−\n1 + 1/p) where m ∈N is the order of the cardinal B-spline bases, for any f ∈Bs\np,q(Ω), there exists\nfN that satisﬁes\n∥f −fN∥Lr(Ω) ≲N −s/d∥f∥Bsp,q\n(9)\nfor N ≫1, and has the following form:\nfN(x) =\nK\nX\nk=0\nX\nj∈J(k)\nαk,jM d\nk,j(x) +\nK∗\nX\nk=K+1\nnk\nX\ni=1\nαk,jiM d\nk,ji(x),\n(10)\nwhere (ji)nk\ni=1\n⊂\nJ(k), K\n=\n⌈C1 log(N)/d⌉, K∗\n=\n⌈log(λN)ν−1⌉+ K + 1, nk\n=\n⌈λN2−ν(k−K)⌉(k = K + 1, . . . , K∗) for δ = d(1/p −1/r)+ and ν = (s −δ)/(2δ), and the real\nnumber constants C1 > 0 and λ > 0 are chosen to satisfy PK\nk=1(2k + m)d + PK∗\nk=K+1 nk ≤N\nindependently to N. Moreover, we can choose the coefﬁcients (αk,j) to satisfy\n∥(αk,j)k,j∥bs\np,q ≲∥f∥Bs\np,q.\nProof of Lemma 2. DeVore & Popov (1988) constructed a linear bounded operator Pk having the\nfollowing form:\nPk(f)(x) =\nX\nj∈J(k)\nak,jM d\nk,j(x)\n(11)\nwhere αk,j is constructed in a certain way, where for every f ∈Lp([0, 1]d) with 0 < p ≤∞, it\nholds\n∥f −Pk(f)∥Lp ≤Cwr,p(f, 2−k).\n(12)\n16\nLet\npk(f) := Pk(f) −Pk−1(f), P−1(f) = 0.\nThen, it is shown that for 0 < p, q ≤∞and 0 < s < min(m, m −1 + 1/p), f belongs to Bs\np,q if\nand only if f can be decomposed into\nf =\n∞\nX\nk=0\npk(f),\nwith the convergence condition ∥(pk(f))∞\nk=0∥bsp(Lp)\n<\n∞;\nin particular,\n∥f∥Bsp,q\n≃\n∥(pk(f))∞\nk=0∥bs\np(Lp) =: (P\nk∈N+(2sk∥pk∥Lp)q)1/q. Here, each pk can be expressed as pk(x) =\nP\nj∈J(k) αk,jM d\nk,j(x) for a coefﬁcient (αk,j)k,j (which could be different from (ak,j)k,j appearing\nin Eq. (11)). Hence, f ∈Bs\np,q can be decomposed into\nf =\n∞\nX\nk=0\nX\nj∈J(k)\nαk,jM d\nk,j(x)\n(13)\nwith\nconvergence\nin\nthe\nsence\nof\nLp.\nMoreover,\nit\nis\nshown\nthat\n∥pk∥Lp\n≃\n(2−kd P\nj∈J(k) |αk,j|p)1/p and thus\n∥f∥Bsp,q ≃∥(αk,j)k,j∥bsp,q.\n(14)\nBased on this decomposition, D˜ung (2011b) proposed an optimal adaptive recovery method such that\nthe approximator has the form (10) under the conditions for K, K∗, nk given in the statement and\nsatisﬁes the approximation accuracy (9). This can be proven by applying the proof of Theorem 3.1 in\nD˜ung (2011b) to the decomposition (13) instead of Eq. (3.8) of that paper. See also Theorem 5.4 of\nD˜ung (2011b). Moreover, the equivalence (14) gives the norm bound of the coefﬁcient (αk,j).\nProof of Proposition 1. Basically, we combine Lemma 1 and Lemma 2.\nWe substitute the ap-\nproximated cardinal B-spline basis ˇ\nM into the decomposition of fN (10). Let the set of indexes\n(k, j) ∈N×N that consists fN given in Eq. (10) be EN: fN = P\n(k,j)∈EN αk,jM d\nk,j. Accordingly,\nwe set ˇf := P\n(k,j)∈EN αk,j ˇ\nM d\nk,j. For each x ∈Rd, it holds that\n|fN(x) −ˇf(x)| ≤\nX\n(k,j)∈EN\n|αk,j||M d\nk,j(x) −ˇ\nM d\nk,j(x)|\n≤ǫ\nX\n(k,j)∈EN\n|αk,j|1{M d\nk,j(x) ̸= 0}\n≤ǫ(m + 1)d(1 + K∗)2K∗(d/p−s)+∥f∥Bs\np,q\n≲log(N)N (ν−1+d−1)(d/p−s)+ǫ∥f∥Bsp,q,\nwhere we used the deﬁnition of K∗in the last inequality. Therefore, for each f ∈U(Bs\np,q([0, 1]d)),\nit holds that\n∥f −ˇf∥Lr ≲∥f −fN∥Lr + ∥fN −ˇf∥Lr ≲log(N)N (ν−1+d−1)(d/p−s)+∥f∥Bsp,qǫ + N −s/d.\nBy\ntaking\nǫ\nto\nsatisfy\nlog(N)N (ν−1+d−1)(d/p−s)+ǫ\n≤\nN −s/d\n(i.e.,\nǫ\n≤\nN −s/d−(ν−1+d−1)(d/p−s)+ log(N)−1), then we obtain the approximation error bound.\nNext, we bound the magnitude of the coefﬁcients.\nEach coefﬁcient αj,k satisﬁes |αj,k| ≲\n2k(d/p−s)+∥f∥Bsp,q ≤2k(d/p−s)+ ≲N (ν−1+d−1)(d/p−s)+ for k ≤K∗.\nFinally, the magni-\ntudes of the coefﬁcients hidden in ˇ\nM d\nk,j are evaluated. Remembering that ˇ\nM m\nk,j(x) = ˇ\nM(2kx1 −\nj1, . . . , 2kxd −jd), we see that we just need to bound the quantity 2k (k ≤K∗). However, this is\nbounded by 2k ≤N ν−1+d−1 for k ≤K∗. Hence, we obtain the assertion.\n17\nC\nPROOF OF THEOREM 1\nLet Nd\n+(e)\n:=\n{s\n∈\nNd\n+\n|\nsi\n=\n0, i\n̸∈\ne} and for k\n∈\nNd\n+(e), we deﬁne\n2−k := (2−ki1 , . . . , 2−ki|e|) ∈R|e|\n+ where (i1, . . . , i|e|) = e.\nBy deﬁning ∥(gk)k∥bα,e\nq\n:=\n\u0010P\nk∈Nd\n+(e)(2α∥k∥1|gk|)q\u00111/q\nfor a sequence (gk)k∈Nd\n+(e), then it holds that\n|f|MBα,e\np,q =\nX\ne⊂{1,...,d}\n∥(we\nr,p(f, 2−k))k∥bα,e\nq .\nProof of Theorem 1. The result is immediately follows from Theorem 5. Let the set of indexes of\n(k, j) consisting of RK be EK: RK(f) = P\n(k,j)∈EK αk,jM d\nk,j(x). As in the proof of Proposition\n1, we approximate RK(f) by a neural network given as\nˇf(x) =\nX\n(k,j)∈EK\nαk,j ˇ\nM d\nk,j(x).\nEach coefﬁcient αj,k satisﬁes |αj,k| ≲2∥k∥1(1/p−s)+∥f∥MBsp,q ≲2K∗(1/p−s)+. The difference\nbetween\n|RK(f) −ˇf(x)| ≤\nX\n(k,j)∈EK\n|αk,j||M d\nk,j(x) −ˇ\nM d\nk,j(x)|\n≤ǫ\nX\n(k,j)∈EK\n|αk,j|1{M d\nk,j(x) ̸= 0}\n≲ǫ(m + 1)d(1 + K∗)DK∗,d2K∗(1/p−s)+∥f∥MBsp,q.\nTherefore, by taking ǫ so that ǫ(m + 1)d(1 + K∗)DK∗,d2K∗(1/p−s)+ ≤2−Ks is satisﬁed, it holds\nthat\n|RK(f) −ˇf(x)| ≲2−Ks.\nBy the inequality DK∗,d ≤eK∗+d−1, it sufﬁces to let ǫ ≤e−K∗(s+(1/p−s)++1)\n[e(m+1)]d(1+K∗) . The cardinality of\nE(K) is bounded as\nX\nκ=0,...,K\n2κ\n\u0012κ + d −1\nd −1\n\u0013\n+\nX\nk:K<∥k∥1≤K∗\nnk\n≤2K+1\n\u0012K + d −1\nd −1\n\u0013\n+\nX\nK<κ≤K∗\n2K−s−δ\n2δ (κ−K)\n\u0012κ + d −1\nd −1\n\u0013\n≤2K+1DK,d + 2K(1 −2−s−δ\n2δ )−1DK∗,d ≤(2 + (1 −2−s−δ\n2δ )−1)2KDK∗,d = N.\nSince each unit ˇ\nM d\nk,j requires width W0, the whole width becomes W = NW0. The number of\nnonzero parameters to construct ˇ\nM d\nk,j is bounded by S = (L −1)W 2\n0 N + N.\nFinally, the magnitudes of the coefﬁcients hidden in\nˇ\nM d\nk,j are evaluated.\nRemembering that\nˇ\nM d\nk,j(x) = ˇ\nM(2k1x1 −j1, . . . , 2kdxd −jd), here maximum of 2kj is bounded by 2K∗≲N (1+1/ν).\nHence, we obtain the assertion. Similarly, it holds that |αj,k| ≲N (1+1/ν){1∨(1/p−s)+}.\nD\nPROOF OF THEOREM 5\nD.1\nPREPARATION: SPARSE GRID\nHere, we give technical details behind the approximation bound. The analysis utilizes the so called\nsparse grid technique Smolyak (1963) which has been developed in the function approximation\ntheory ﬁeld.\nAs we have seen in the above, in a typical B-spline approximation scheme, we put the basis functions\nM m\nk,j(x) on a “regular grid” for k = 1, . . . , K and (j1, . . . , jd) ∈J(k), and take its superposition as\n18\nf(x) ≈P\nk=1,...,K\nP\nj∈J(k) αk,jM m\nk,j(x), which consists of O(2Kd) terms (see Eq. (10)). Hence,\nthe number of parameters O(2Kd) is affected by the dimensionality d in an exponential order. How-\never, to approximate functions with mixed smoothness, we do not need to put the basis on the whole\nrange of the regular grid. Instead, we just need to put them on a sparse grid which is a subset of the\nregular grid and has much smaller cardinality than the whole set. The approximation algorithm uti-\nlizing sparse grid is based on Smolyak’s construction (Smolyak, 1963) and its applications to mixed\nsmooth spaces (D˜ung, 1990; 1991; 1992; Temlyakov, 1982; 1993a;b). D˜ung (2011a) studied an\noptimal non-adaptive linear sampling recovery method for the mixed smooth Besov space based on\nthe cardinal B-spline bases. We adopt this method, and combining this with the adaptive technique\ndeveloped in D˜ung (2011b), we give the following approximation bound using a non-linear adaptive\nmethod to obtain better convergence for the setting p < r.\nBefore we state the theorem, we deﬁne an quasi-norm of a set of coefﬁcients αk,j ∈R for k ∈Nd\n+\nand j ∈Jd\nm(k) := {−m, −m + 1, . . . , 2k1 −1, 2k1} × · · · × {−m, −m + 1, . . . , 2kd −1, 2kd} as\n∥(αk,j)k,j∥mbα\np,q :=\n\nX\nk∈Nd\n+\n\n2(α−1/p)∥k∥1\u0010\nX\nj∈Jd\nm(k)\n|αk,j|p\u00111/p\n\n\nq\n\n1/q\n.\nTheorem 5. Suppose that 0 < p, q, r ≤∞and α > (1/p −1/r)+. Assume that the order m ∈N\nof the cardinal B-spline satisﬁes 0 < s < min(m, m −1 + 1/p). Let δ = (1/p −1/r)+. Then, for\nany f ∈MBs\np,q(Ω) and K > 0, there exists RK(f) such that RK(f) can be represented as\nRK(f)(x) =\nX\nk∈Nd\n+:\n∥k∥1≤K\nX\nj∈Jd\nm(k)\nαk,jM d\nk,j(x) +\nX\nk∈Nd\n+:\nK<∥k∥1≤K∗\nnk\nX\ni=1\nαk,j(k)\ni\nM d\nk,j(k)\ni\n(x),\nwhere K∗= ⌈K(1 +\n2δ\nα−δ )⌉, (j(k)\ni\n)nk\ni=1 ⊂Jd\nm(k), and nk = ⌈2K−α−δ\n2δ (∥k∥1−K)⌉, and has the\nfollowing properties:\n(i) For p ≥r,\n∥f −RK(f)∥r ≲2−KαD(1/ min(r,1)−1/q)+\nK,d\n∥f∥MBsp,q.\n(ii) For p < r,\n∥f −RK(f)∥r ≲\n(\n2−KαD(1/r−1/q)+\nK,d\n∥f∥MBsp,q\n(r < ∞),\n2−KαD(1−1/q)+\nK,d\n∥f∥MBsp,q\n(r = ∞).\nMoreover, the coefﬁcients (αk,j)k,j can be taken to hold ∥(αk,j)k,j∥mbα\np,q ≲∥f∥MBα\np,q.\nThe proof is given in Appendix D.2. The total number of cardinal B-spline bases consisting of\nRK(f) can be evaluated as\n2K+1\n\u0012K + d −1\nd −1\n\u0013\n+\nX\nk:K<∥k∥1≤K∗\nnk\n≲2KDK,d + 2KDK∗,d ≲2KDK,d\n(∵Eq. (17)).\nHere, DK,d can be evaluated as\nDK,d ≲Kd−1 or DK,d ≲dK.\nTherefore, the total number of bases can be evaluated as\n2K min{Kd−1, dK}\nwhich is much smaller than 2Kd which is required to approximate functions in the ordinal Besov\nspace (see Lemma 2). In this proposition, K controls the resolution and as M goes to inﬁnity, the\napproximation error goes to 0 exponentially fast. A remarkable point in the proposition is in the\n19\nconstruction of RK(f) in which the superposition is taken over ∥k∥1 ≤M instead of ∥k∥∞≤\nK∗= O(K). Hence, the number of terms appearing in the summation is at most O(2KKd−1)\nwhile the full grid takes O(2Kd) terms. This represents how the mixed smoothness is important to\nease the curse of dimensionality.\nSeveral aspects of the m-Besov space such as the optimal N-term approximation error and Kol-\nmogorov widths have been extensively studied in the literature (see a comprehensive survey\nD˜ung et al. (2016)). An analogous result is already given by D˜ung (2011a) in which α > 1/p\nis assumed and a linear interpolation method is investigated. However, our result only requires\nα > (1/p −1/q)+. This difference comes from a point that our analysis allows nonlinear adap-\ntive interpolation instead of (linear) non-adaptive sampling considered in D˜ung (2011a). Because of\nthis, our bound is better than the optimal rate of linear methods (Galeev, 1996; Romanyuk, 2001)\nand non-adaptive methods (D˜ung, 1990; 1991; 1992; Temlyakov, 1982; 1993a;b) especially in the\nregime of p < r (D˜ung (1992) also deals with adaptive method but does not cover p < r for adaptive\nmethod). See Proposition 2 for comparison.\nD.2\nPROOF OF THEOREM 5\nNow we are ready to prove Theorem 5.\nProof of Theorem 5. For k = (k1, . . . , kd) ∈Nd\n+, let P (i)\nki f(x) be the function operating Pk deﬁned\nin (11) to f as a function of xi with other components xj (j ̸= i) ﬁxed, and let\npk :=\nd\nY\ni=1\n(P (i)\nki −P (i)\nki−1)f.\n(15)\nThen, pk can be expressed as pk(x) = P\nj∈Jd\nm(k) αk,jM d\nk,j(x).\nLet T (i)\nki\n= I −P (i)\nki and ∥f∥p,i be the Lp-norm of f as a function of xi with other components\nxj (j ̸= i) ﬁxed (i.e., if p < ∞, ∥f∥p\np,i =\nR\n|f(x)|pdxi), then Eq. (12) gives\n∥T (i)\nki f∥p,i ≲\nsup\n|hi|≤2−ki\n∥∆r,i\nhi (f)∥p,i.\nThus, by applying the same argument again, it also holds\n∥∥T (i)\nki T (j)\nkj f∥p,i∥p,j ≲∥\nsup\n|hi|≤2−ki\n∥∆r,i\nhi (Tkjf)∥p,i∥p,j\n=\nsup\n|hi|≤2−ki\n∥∥Tkj∆r,i\nhi (f)∥p,j∥p,i (∵the deﬁnition of ∆r,i\nhi and Fubini’s theorem)\n≲\nsup\n|hi|≤2−ki\nsup\n|hj|≤2−kj\n∥∥∆r,i\nhi (∆r,j\nhj (f))∥p,j∥p,i,\nfor i ̸= j. Thus, applying the same argument recursively, for u ⊂[d], it holds that\n\r\r\r\r\r\nY\ni∈u\nT (i)\nki f\n\r\r\r\r\r\np\n≲wu\nr,p(f, 2−k)\nfor\nk\n∈\nNd\n+(u).\nTherefore,\nsince\npk\n=\nQd\ni=1(T (i)\nki−1\n−\nT (i)\nki )f\n=\nP\nu⊂[d](−1)|u| \u0010Q\ni∈u T (i)\nki\nQ\ni̸∈u T (i)\nki−1\n\u0011\nf, by letting e = {i | ki > 0}, we have that\n∥pk∥p ≲\nX\nu⊂[d]\n\r\r\r\r\r\r\n\nY\ni∈u\nT (i)\nki\nY\ni̸∈u\nT (i)\nki−1\n\nf\n\r\r\r\r\r\r\np\n≲\nX\nu⊂[d]\nwˆu\nr,p(f, 2−(ku)ˆ\nu) ≲\nX\ne⊂u\nwu\nr,p(f, 2−ku)\nwhere ku\ni := ki (i ∈u) and ku\ni := ki −1 (i ̸∈u), ˆu = {i | ku\ni ≥0}, and (ku)ˆu is a vector such that\n(ku)ˆu,i = ku\ni for i ∈ˆu and (ku)ˆu,i = 0 for i ̸∈ˆu. Now let\n∥(pk)k∥bα\nq (Lp) =\n X\nk∈Nd\n+\n\u00002α∥k∥1∥pk∥Lp\u0001q\n!1/q\n20\nfor pk ∈Lp(Ω) (k ∈Nd\n+). Hence, if we set ak = P\ne⊂u wu\nr,p(f, 2−ku) for k ∈Nd and e = {i |\nki > 0}, we have that\n∥(pk)k∥bα\nq (Lp) ≲∥(ak)∥bα\nq (Lp) ≃∥f∥MBsp,q.\nOn the other hand, following the same line of Theorem 2.1 (ii) of D˜ung (2011a), we also obtain the\nopposite inequality ∥f∥MBsp,q ≃∥(ak)∥bα\nq (Lp) ≲∥(pk)k∥bα\nq (Lp) (note that the analogous inequality\nto Lemma 2.3 of D˜ung (2011a) also holds in our setting by replacing qs with ps and ωe\nr(f, 2−k)p by\nwe\nr,p).\nTherefore, f ∈MBα\np,q if and only if (pk)k∈Nd\n+ given by Eq. (15) satisﬁes ∥(pk)k∥bα\nq (Lp) < ∞\nand f can be decomposed into f = P\nk∈Nd\n+ pk where convergence is in MBα\np,q. Moreover, it\nholds that ∥f∥MBα\np,q ≃∥(pk)k∥bα\nq (Lp). This can be shown by Theorem 2.1 of D˜ung (2011a).\nMoreover, by the quasi-norm equivalence ∥pk∥p ≃2−∥k∥1/p(P\nj∈Jd\nm(k) |αk,j|p)1/p, we also have\n∥(αk,j)k,j∥mbα\np,q ≃∥f∥MBα\np,q.\nIf p ≥r, the assertion can be shown in the same manner as Theorem 3.1 of D˜ung (2011a).\nFor the setting of p < r, we need to use an adaptive approximation method. In the following, we\nassume p < r. For a given K, by choosing K∗appropriately later, we set\nRK(f)(x) =\nX\nk∈Nd\n+:∥k∥1≤K\npk +\nX\nk∈Nd\n+:K<∥k∥1≤K∗\nGk(pk),\nwhere Gk(pk) is given as\nGk(pk) =\nX\n1≤i≤nk\nαk,jiM d\nk,ji(x)\nwhere (αk,ji)|Jd\nm(k)|\ni=1\nis the sorted coefﬁcients in decreasing order of their absolute value: |αk,j1| ≥\n|αk,j2| ≥· · · ≥|αk,j|Jd\nm(k)||. Then, it holds that\n∥pk −Gk(pk)∥r ≤∥pk∥p2δ∥k∥1n−δ\nk ,\nwhere δ := (1/p −1/r) (see the proof of Theorem 3.1 of D˜ung (2011b) and Lemma 5.3 of D˜ung\n(2011a)). Moreover, we also have\n∥pk∥r ≤∥pk∥p2δ∥k∥1\nfor k ∈Nd\n+ with ∥k∥1 > K∗.\nHere, we deﬁne N as\nN = ⌈log2(K)⌉.\nLet ǫ = (α −δ)/(2δ), and\nK∗= ⌈K(1 + 1/ǫ)⌉,\nand nk = ⌈2K−ǫ(∥k∥1−K)⌉for k ∈Nd\n+ with K + 1 ≤∥k∥1 ≤K∗.\nThen, by Lemma 5.3 of D˜ung (2011a), we have that\n∥f −RK(f)∥r\nLr ≲\nX\nK<∥k∥1≤K∗\n∥pk −Gk(pk)∥r\nLr +\nX\nK∗<∥k∥1\n∥pk∥Lr]r\n≲\nX\nK<∥k∥1≤K∗\n[∥pk∥p2δ∥k∥1n−δ\nk ]r +\nX\nK∗<∥k∥1\n[2δ∥k∥1∥pk∥Lp]r.\n(16)\nIn the following, we require an upper bound of\n\u0000k+d−1\nd−1\n\u0001\n. Hence, we evaluate this quantity before-\nhand. This can be upper bounded by the Stering’s formula as\n\u0012k + d −1\nd −1\n\u0013\n≤\n√\n2e\n2π\n\u0012\n1 + d −1\nk\n\u0013k \u0012\n1 +\nk\nd −1\n\u0013d−1\n|\n{z\n}\n=Dk,d\n≤Dk,d.\n21\nLet ξ > 0 be a positive real number satisfying 1 + ξ ≥K∗/K. We can see that ξ can be chosen as\nξ = 1/ǫ + o(1). Then, we have that\nDK∗,d = DK,d\n(1 + d−1\nK∗)K∗\n(1 + d−1\nK )K\n(1 + K∗\nd−1)d−1\n(1 +\nK\nd−1)d−1 ≤DK,d\n(1 + d−1\nK∗)K∗\n(1 + d−1\nK∗)K\n \n1\n1 +\nK\nd−1\n+\nK∗\n(d −1)(1 +\nK\nd−1)\n!d−1\n≤DK,d\n\u0012\n1 + d −1\nK∗\n\u0013K∗−K \u0012d −1 + K∗\nd −1 + K\n\u0013d−1\n= DK,d\n\u0012\n1 + d −1\nK\n\u0013ξK\n(1 + ξ)d−1\n≤DK,de(d−1)ξ(1 + ξ)d−1 ≃DK,d.\n(17)\n(a) Suppose that q ≤r and r < ∞. Then\n∥f −RK(f)∥q\nLr = ∥f −RK(f)∥\nr q\nr\nLr\n≲\n\n\n\nX\nK<∥k∥1≤K∗\n[2δ∥k∥1n−δ\nk ∥pk∥Lp]r +\nX\nK∗<∥k∥1\n[2δ∥k∥1∥pk∥Lp]r\n\n\n\nq\nr\n(∵Eq. (16))\n≲\nX\nK<∥k∥1≤K∗\n[2δ∥k∥1n−δ\nk ∥pk∥Lp]q +\nX\nK∗<∥k∥1\n[2δ∥k∥1∥pk∥Lp]q\n≤N −δq2−(α−δ)Kq\nX\nK<∥k∥1≤K∗\n[2−(α−δ−δǫ)(∥k∥1−K)\n|\n{z\n}\n≤1\n2α∥k∥1∥pk∥Lp]q + 2−q(α−δ)K∗\nX\nK∗<∥k∥1\n[2α∥k∥1∥pk∥Lp]q\n≲(N −δ2−(α−δ)K + 2−(α−δ)K∗)q∥f∥q\nMBα\np,q\n≤(N −α)q∥f∥q\nMBα\np,q.\n(b) Suppose that q > r and r < ∞. Then, letting ν = q/r(> 1) and ν′ = 1/(1 −1/ν) = q/(q −r),\nwe have\n∥f −RK(f)∥r\nLr ≲\nX\nK<∥k∥1≤K∗\n[2δ∥k∥1n−δ\nk ∥pk∥Lp]r +\nX\nK∗<∥k∥1\n[2δ∥k∥1∥pk∥Lp]r\n(∵Eq. (16))\n≤N −δr2−(α−δ)Kr\nX\nK<∥k∥1≤K∗\n[2−(α−δ−δǫ)(∥k∥1−K)2α∥k∥1∥pk∥Lp]r +\nX\nK∗<∥k∥1\n[2α∥k∥1∥pk∥Lp]r(2−(α−δ)∥k∥1)r\n≤(N −δ2−(α−δ)K + 2−(α−δ)K∗)rn\nX\nK<∥k∥1≤K∗\n[2−(α−δ−δǫ)(∥k∥1−K)2α∥k∥1∥pk∥Lp]r\n+\nX\nK∗<∥k∥1\n[2α∥k∥1∥pk∥Lp]r2−(α−δ)(∥k∥1−K∗)ro\n≤(N −δ2−(α−δ)K + 2−(α−δ)K∗)r\n\n\n\nX\nK<∥k∥1≤K∗\n[2α∥k∥1∥pk∥Lp]rν +\nX\nK∗<∥k∥1\n[2α∥k∥1∥pk∥Lp]rν\n\n\n\n1/ν\n×\n\n\n\nX\nK<∥k∥1≤K∗\n[2−(α−δ−δǫ)(∥k∥1−K)]rν′ +\nX\nK∗<∥k∥1\n[2−(α−δ)(∥k∥1−K∗)]rν′\n\n\n\n1/ν′\n≲(N −δ2−(α−δ)K + 2−(α−δ)K∗)r∥f∥r\nMBα\np,qDr(1/r−1/q)\nK,d\n(∵Eq. (17))\n≲(N −αD1/r−1/q\nK,d\n)r∥f∥r\nMBα\np,q.\n(c) Suppose that r = ∞. Then, similarly to the analysis in (b), we can evaluate\n∥f −RK(f)∥Lr\n≲N −δ2−(α−δ)K\nX\nK<∥k∥1≤K∗\n[2−(α−δ−δǫ)(∥k∥1−K)2α∥k∥1∥pk∥Lp] +\nX\nK∗<∥k∥1\n[2α∥k∥1∥pk∥Lp](2−(α−δ)∥k∥1)\n22\n≲(N −δ2−(α−δ)K + 2−(α−δ)K∗)D(1−1/q)+\nK,d\n∥f∥MBα\np,q\n≲N −αD(1−1/q)+\nK,d\n∥f∥MBα\np,q.\nE\nPROOFS OF THEOREMS 2 AND 3\nProof of Theorem 2. We use Proposition 4. We just need to evaluate the covering number of ˆF =\n{ ¯f | f ∈Ψ(L, W, S, B)} for (L, W, S, B) given in Theorem 1 where ¯f is the clipped function for\na given f. Note that the covering number of ˆF is not larger than that of Ψ(L, W, S, B). Thus, we\nmay evaluate that of Ψ(L, W, S, B). From Lemma 3, the covering number is obtained as\nlog N(δ, ˆF, ∥· ∥∞) ≲N[log(N)2 + log(δ−1)].\nFrom Proposition 1, it holds that\n∥f o −RK(f o)∥2 ≲N −s/d.\nNote that\n∥f −f o∥2\nL2(PX) ≲∥f −f o∥2\n2.\nfor any f : [0, 1]d →R because p(x) ≤R. Therefore, by applying Proposition 4 with δ = 1/n, we\nhave that\nEDn[∥bf −f o∥2\nL2(PX)] ≲N −2s/d + N(log(N)2 + log(n))\nn\n+ 1\nn.\n(18)\nHere, the right hand side is minimized by setting N ≍n\nd\n2s+d up to log(n)2-order, and then have an\nupper bound of the RHS as\nn−\n2s\n2s+d log(n)2.\nThis gives the assertion.\nProof of Theorem 3. The proof follows the almost same line as the proof of Theorem 2. By noting\nS = O(2KDK,d), L = O(K) and W = O(2KDK,d), Lemma 3 gives an upper bound of the\ncovering number as\nlog N(δ, ˆF, ∥· ∥∞) ≲2KDK,d[K log(2KDK,d) + log(δ−1)] ≲2KDK,d(K2 + log(1/δ)).\nLetting r = 2, we have that\n∥f o −RK(f o)∥2 ≲2−sKDu\nK,d\nwhere u = (1 −1/q)+ for p ≥2 and u = (1/2 −1/q)+ for p < 2.\nThen, by noting that\n∥f −f o∥2\nL2(PX) ≲∥f −f o∥2\n2,\nfor any f : [0, 1]d →R, and by applying Proposition 4 with δ = 1/n, we have that\nEDn[∥bf −f o∥2\nL2(PX)] ≲2−2sKD2u\nK,d + 2KDK,d(K2 + log(δ−1))\nn\n+ 1\nn.\n(19)\nHere, we use the following evaluations for DK,d: (a) DK,d ≲Kd−1, and (b) DK,d ≲[e(1 + d\nK )]K.\n(a) For the evaluation, DK,d ≲Kd−1, we have an upper bound of the right hand side of Eq. (19) as\n2−2sKK2u(d−1) + 2KKd−1(K2 + log(n))\nn\n,\nwhich is minimized by setting K = ⌈\n1\n1+2s log2(n) + (2u−1)(d−1)\n1+2s\nlog2 log(n)⌉up to log log(n)-\norder. In this situation, we have the generalization error bound as\nn−\n2s\n2s+1 log(n)\n2(d−1)(u+s)\n1+2s\nlog(n)2.\n23\n(b) For the evaluation, DK,d ≲[e(1 + d\nK )]K ≤eKed, Eq. (19) gives an upper bound of\n2−2sKe2uK + 2KeK(K2 + log(n))\nn\n.\nThen, the right hand side is minimized by K = ⌈\n1\n1+2s+(1−2u) log2(e) log2(n)⌉. Then, we have that\nn−\n2s−2u log2(e)\n1+2s+(1−2u) log2(e) log(n)2.\nThis gives the assertion.\nF\nMINIMAX OPTIMALITY\nProof of Theorem 4. First note that since PX is the uniform distribution, it holds that ∥· ∥L2(PX) =\n∥· ∥L2([0,1]d). The ǫ-covering number N(ǫ, G, L2(PX)) with respect to L2(PX) for a function class\nG is the minimal number of balls with radius ǫ measured by L2(PX)-norm needed to cover the set\nG (van der Vaart & Wellner, 1996). The δ-packing number M(δ, G, L2(PX)) of a function class\nG with respect to L2(PX) norm is the largest number of functions {f1, . . . , fM} ⊆G such that\n∥fi −fj∥L2(PX) ≥δ for all i ̸= j. It is easily checked that\nN(δ/2, G, L2(PX)) ≤M(δ, G, L2(PX)) ≤N(δ, G, L2(PX)).\n(20)\nFor a given δn > 0 and εn > 0, let Q be the δn packing number M(δn, U(MBs\np,q), L2(PX)) of\nU(MBs\np,q) and N be the εn covering number of that. Raskutti et al. (2012a) utilized the techniques\ndeveloped by Yang & Barron (1999) to show the following inequality in their proof of Theorem 2(b)\n:\ninf\nb\nf\nsup\nf ∗∈U(MBsp,q)\nEDn[∥bf −f ∗∥2\nL2(PX)] ≥inf\nb\nf\nsup\nf ∗∈U(MBsp,q)\nδ2\nn\n2 P[∥bf −f ∗∥2\nL2(PX) ≥δ2\nn/2]\n≥δ2\nn\n2\n\u0012\n1 −log(N) +\nn\n2σ2 ε2\nn + log(2)\nlog(Q)\n\u0013\n.\nThus by taking δn and εn to satisfy\nn\n2σ2 ε2\nn ≤log(N),\n(21)\n8 log(N) ≤log(Q),\n(22)\n4 log(2) ≤log(Q),\n(23)\nthe minimax rate is lower bounded by δ2\nn\n4 . This can be achieved by properly setting εn ≃δn. Now,\nfor given N with respect to δn > 0, M = log(N) satisﬁes\nδn ≳M −s log(M)(d−1)(s+1/2−1/q)+\n(Theorem 6.24 of D˜ung et al. (2016)). Hence, it sufﬁces to take\nM ≃n\n1\n2s+1 log(n)\n2(d−1)(s+1/2−1/q)+\n2s+1\n,\n(24)\nεn ≃δn ≃n−\n2s\n2s+1 log(n)\n2(d−1)(s+1/2−1/q)+\n2s+1\n,\n(25)\nwhich gives the assertion.\nG\nAUXILIARY LEMMAS\nLet the ǫ-covering number with respect to L2(PX) for a function class G be N(ǫ, G, L2(PX)) as\ndeﬁned in the proof of Theorem 4.\n24\nProposition 4 (Schmidt-Hieber (2017)). Let F be a set of functions. Let bf be any estimator in F.\nDeﬁne\n∆n := EDn\n\"\n1\nn\nn\nX\ni=1\n(yi −bf(xi))2 −inf\nf∈F\n1\nn\nn\nX\ni=1\n(yi −f(xi))2\n#\n.\nAssume that ∥f o∥∞≤F and all f ∈F satisﬁes ∥f∥∞≤F for some F ≥1. If 0 < δ < 1 satisﬁes\nN(δ, F, ∥· ∥∞) ≥3, then there exists a universal constant C such that\nEDn[∥bf −f o∥2\nL2(PX)]\n≤C(1 + ǫ)2\n\u0014\ninf\nf∈F ∥f −f o∥2\nL2(PX) + F 2 log N(δ, F, ∥· ∥∞) −log(δ)\nnǫ\n+ δF 2 + ∆n\n\u0015\n,\nfor any ǫ ∈(0, 1].\nProof of Proposition 4. This is almost direct consequence of Lemma 8 of Schmidt-Hieber (2017)3.\nThe only difference is the assumption of ∥f∥∞≤F for f ∈F and f = f o while Lemma 8 of\nSchmidt-Hieber (2017) assumed 0 ≤f(x) ≤F ′ for F ′ > 1. However, this can be easily ﬁxed by\nshifting the function value by +F then the range of f is modiﬁed to [0, 2F]. Then, our situation is\nreduced to that of Lemma 8 of Schmidt-Hieber (2017) by substituting F ′ ←2F.\nLemma 3 (Covering number evaluation). The covering number of Φ(L, W, S, B) can be bounded\nby\nlog N(δ, Φ(L, W, S, B), ∥· ∥∞) ≤S log(δ−1L(B ∨1)L−1(W + 1)2L)\n≤2SL log(δ−1L(B ∨1)(W + 1)).\nProof of Lemma 3. Given a network f ∈Φ(L, W, S, B) expressed as\nf(x) = (W (L)η(·) + b(L)) ◦· · · ◦(W (1)x + b(1)),\nlet\nAk(f)(x) = η ◦(W (k−1)η(·) + b(k−1)) ◦· · · ◦(W (1)x + b(1)),\nand\nBk(f)(x) = (W (L)η(·) + b(L)) ◦· · · ◦(W (k)η(x) + b(k)),\nfor k = 2, . . . , L. Corresponding to the last and ﬁrst layer, we deﬁne BL+1(f)(x) = x and\nA1(f)(x) = x. Then, it is easy to see that f(x) = Bk+1(f) ◦(W (k) · +b(k)) ◦Ak(f)(x). Now,\nsuppose that a pair of different two networks f, g ∈Φ(L, W, S, B) given by\nf(x) = (W (L)η(·)+b(L))◦· · ·◦(W (1)x+b(1)), g(x) = (W (L)′η(·)+b(L)′)◦· · ·◦(W (1)′x+b(1)′),\nhas a parameters with distance δ: ∥W (ℓ) −W (ℓ)′∥∞≤δ and ∥b(ℓ) −b(ℓ)′∥∞≤δ. Now, not that\n∥Ak(f)∥∞≤maxj ∥W (k−1)\nj,:\n∥1∥Ak−1(f)∥∞+ ∥b(k−1)∥∞≤WB∥Ak−1(f)∥∞+ B ≤(B ∨\n1)(W + 1)∥Ak−1(f)∥∞≤(B ∨1)k−1(W + 1)k−1, and similarly the Lipshitz continuity of Bk(f)\nwith respect to ∥· ∥∞-norm is bounded as (BW)L−k+1. Then, it holds that\n|f(x) −g(x)|\n=\n\f\f\f\f\f\nL\nX\nk=1\nBk+1(g) ◦(W (k) · +b(k)) ◦Ak(f)(x) −Bk+1(g) ◦(W (k)′ · +b(k)′) ◦Ak(f)(x)\n\f\f\f\f\f\n≤\nL\nX\nk=1\n(BW)L−k∥(W (k) · +b(k)) ◦Ak(f)(x) −(W (k)′ · +b(k)′) ◦Ak(f)(x)∥∞\n≤\nL\nX\nk=1\n(BW)L−kδ[W(B ∨1)k−1(W + 1)k−1 + 1]\n3We noticed that there exit some technical ﬂaws in the proof of the lemma, e.g., an incorrect application of\nthe uniform bound to derive the risk of an estimator. However, these ﬂaws can be ﬁxed and the statement itself\nholds with a slight modiﬁcation. Accordingly, there appears −log(δ) term and δF is replaced by δF 2\n25\n≤\nL\nX\nk=1\n(BW)L−kδ(B ∨1)k−1(W + 1)k ≤δL(B ∨1)L−1(W + 1)L.\nThus, for a ﬁxed sparsity pattern (the locations of non-zero parameters), the covering number is\nbounded by\n\u0000δ/[L(B ∨1)L−1(W + 1)L]\n\u0001−S. There are the number of conﬁgurations of the spar-\nsity pattern is bounded by\n\u0000(W+1)L\nS\n\u0001\n≤(W + 1)LS. Thus, the covering number of the whole space\nΦ is bounded as\n(W + 1)LS \b\nδ/[L(B ∨1)L−1(W + 1)L]\n\t−S = [δ−1L(B ∨1)L−1(W + 1)2L]S,\nwhich gives the assertion.\n26\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2018-10-18",
  "updated": "2018-10-18"
}