{
  "id": "http://arxiv.org/abs/1810.09028v2",
  "title": "RLgraph: Modular Computation Graphs for Deep Reinforcement Learning",
  "authors": [
    "Michael Schaarschmidt",
    "Sven Mika",
    "Kai Fricke",
    "Eiko Yoneki"
  ],
  "abstract": "Reinforcement learning (RL) tasks are challenging to implement, execute and\ntest due to algorithmic instability, hyper-parameter sensitivity, and\nheterogeneous distributed communication patterns. We argue for the separation\nof logical component composition, backend graph definition, and distributed\nexecution. To this end, we introduce RLgraph, a library for designing and\nexecuting reinforcement learning tasks in both static graph and define-by-run\nparadigms. The resulting implementations are robust, incrementally testable,\nand yield high performance across different deep learning frameworks and\ndistributed backends.",
  "text": "RLGRAPH: MODULAR COMPUTATION GRAPHS\nFOR DEEP REINFORCEMENT LEARNING\nMichael Schaarschmidt * 1 Sven Mika * 2 Kai Fricke 3 Eiko Yoneki 1\nABSTRACT\nReinforcement learning (RL) tasks are challenging to implement, execute and test due to algorithmic instability,\nhyper-parameter sensitivity, and heterogeneous distributed communication patterns. We argue for the separation\nof logical component composition, backend graph deﬁnition, and distributed execution. To this end, we introduce\nRLgraph, a library for designing and executing reinforcement learning tasks in both static graph and deﬁne-by-run\nparadigms. The resulting implementations are robust, incrementally testable, and yield high performance across\ndifferent deep learning frameworks and distributed backends.\n1\nINTRODUCTION\nThe recent wave of new research and applications in deep\nlearning has been fueled by both hardware improvements\nand deep learning frameworks simplifying design and train-\ning of neural networks (Chen et al., 2015; Abadi et al., 2016;\nSeide & Agarwal, 2016; Paszke et al., 2017). Reinforce-\nment learning (RL) algorithms combined with deep neural\nnetworks have in parallel emerged as an active area of re-\nsearch due to promising results in complex control tasks\n(Levine et al., 2016; Tobin et al., 2017; Silver et al., 2017).\nHowever, their design and execution have not found similar\nstandardization. This is a consequence of the highly var-\nied resource requirements, scheduling, and communication\npatterns found in constantly evolving RL methods. Imple-\nmentations hence require a high degree of customization.\nA number of RL libraries has emerged to focus on dis-\ntinct aspects of managing such workloads.\nFor exam-\nple, OpenAI baselines provides reference implementations\nmeant to reproduce speciﬁc benchmark environments (e.g.\nAtari) (Sidor & Schulman, 2017). TensorForce provides\na declarative API focusing on ease of use in applications\n(Schaarschmidt et al., 2018). Ray RLlib seeks to simplify\ndistributing RL workloads by moving from hand-designed\ndistributed communication to actor-based centralized execu-\ntion on Ray (Liang et al., 2018; Moritz et al., 2017).\nWhile these libraries serve different purposes, many of them\nsuffer from similar design problems leading to difﬁculties\nin testing, distributed execution, and extensibility. The root\n*Equal contribution 1University of Cambridge 2rlcore 3Helmut\nSchmidt University. Correspondence to: Michael Schaarschmidt\n<michael.schaarschmidt@cl.cam.ac.uk>.\nProceedings of the 2 nd SysML Conference, Palo Alto, CA, USA,\n2019. Copyright 2019 by the author(s).\ncause of these difﬁculties lies in a lack of separation of con-\ncerns. Composition of logical components deﬁned within\nan RL algorithm is tightly coupled with code fragments\nspeciﬁc to a deep learning framework (e.g. TensorFlow\ncalls to deﬁne placeholders and variables). This leads to\nill-deﬁned APIs and also makes the reuse and testing of\ncomponents difﬁcult. Similarly, the often complex dataﬂow\nwithin RL algorithms is intertwined with control ﬂow regu-\nlating (distributed) execution, environment interaction, and\ndevice management. This results in distributed execution\nand local device strategies being tied to speciﬁc algorithms.\nThe central contribution of this paper is RLgraph, a mod-\nular framework to design and execute RL workloads from\nhigh-level dataﬂow. RLgraph addresses these issues by\nseparating logical component composition, creation of oper-\nations, variables and placeholders, and ﬁnally local and dis-\ntributed execution of the computation graph (Fig. 1). At the\ncore of our design is a novel component graph architecture\nresponsible for assembling and connecting algorithmic com-\nponents, such as buffers or neural networks, and for expos-\ning their functionality to a common API. Importantly, this\ncomponent graph exists independently of implementation\nspeciﬁc notions (e.g. TensorFlow variables), and instead\nrelies on generalized space objects and operations. This\nmeans it can both be built for static graph and deﬁne-by-run\nbackends, and RLgraph currently supports both TensorFlow\n(TF) and PyTorch (PT).\nThe component graph is built into a backend-dependent\ncomputation graph via a graph builder which generates op-\nerations, internal state (e.g. variables), device assignments,\nand a registry for the model’s API. Developers are freed\nfrom tedious manual placeholder and variable deﬁnition, as\nthey only need to specify type and shape of input spaces to\nan algorithm’s outermost container component (root com-\narXiv:1810.09028v2  [cs.LG]  28 Feb 2019\nRLgraph: Modular Computation Graphs for Deep Reinforcement Learning\nHardware: CPU, GPU, TPU, FPGAs...\nRay\nDistributed TF\nHorovod\nTensorFlow\nPyTorch\n...\nRLgraph component graph\nLocal backends \nvariables/operations\nModel design, \ndataflow composition\nDistributed \nexecution engine\nExecution, \norchestration \nAPI, Component configuration\nPre­built models,\ninference\nFigure 1. RLgraph stack for using and designing RL algorithms.\nponent). They can then rely on the RLgraph utilities to cen-\ntrally handle most aspects of connecting complex models,\ne.g. splitting and merging complex nested spaces. A re-\nsource aware graph executor expands the component graph\nto add operations for local and distributed device strategies,\ne.g. by creating subgraph replicas for GPUs or managing\nglobally shared state. At runtime, the graph executor (e.g.\nfor TensorFlow) serves requests to the agent API by de-\ntermining relevant input placeholders and operations from\nthe op-registry and batching together all relevant operations\ninto a single session call. Our design provides numerous\nadvantages over many existing libraries:\n1. Distributed execution. By separating concerns of\ndesign and execution, resulting agents can be imple-\nmented towards any distributed execution paradigm,\ne.g. using distributed TensorFlow (Abadi et al., 2016),\nRay (Moritz et al., 2017), and plugins such as Uber’s\nHorovod (Sergeev & Balso, 2018).\n2. Static and deﬁne-by-run backends. The component\ngraph does not impose restrictions on its execution.\nIt supports end-to-end static graphs including control\nﬂow (Yu et al., 2018) and deﬁne-by-run semantics (e.g.\nPyTorch (Paszke et al., 2017)) through a uniﬁed execu-\ntion interface.\n3. Fast development cycles. RLgraph’s abstractions en-\nables users to focus on high level dataﬂow when com-\nposing components. The build process manages back-\nend scaffolding and creates operations based on user-\nprovided input spaces.\n4. Incremental building and testing. Existing libraries\ncannot efﬁciently identify problems in individual com-\nponents as they do not offer a modular build sys-\ntem. In RLgraph, all components (including pre/post-\nprocessing heuristics) are ﬁrst-class citizens which are\nindividually built and incrementally tested.\nIn the remainder of the paper, we analyze RL design prob-\nlems and survey existing libraries (§2). We then discuss the\ndesign of RLgraph (§3, §4). In the evaluation, we compare\nRLgraph against reference implementations using different\nexecution paradigms (§5). Our results show RLgraph can\nimprove sample throughput over existing implementations\nby up to 180%. In related work, we discuss emerging ap-\nproaches in programming models and optimization (§6).\nRLgraph is available as open source1.\n2\nMOTIVATION\n2.1\nRL workloads\nThe central difﬁculty of executing RL workloads lies in the\nneed for frequent interaction with the problem environment\nduring training to evaluate and update the model. Environ-\nments may take the form of expensive physical systems\n(robots), 3D scene simulators, games, or generally any sys-\ntem exposing a state representation and an action interface.\nThis is in contrast to supervised workloads, where train-\ning data is typically entirely available in advance, thus en-\nabling straightforward batching and synchronization strate-\ngies (Sergeev & Balso, 2018). As a consequence of fast\nmoving and empirically driven research, RL algorithms\nvary across all dimensions of execution (recently discussed\nby Liang et al. (Liang et al., 2018)).\nState management. Sample trajectories are often collected\nin a distributed fashion where workers interact with dedi-\ncated (simulation) environment copies. Algorithms manage\nsynchronization of model weights between one or multiple\nlearners and sample collectors, employing synchronous and\nasynchronous strategies. In addition, they must process and\ntransmit samples to learners efﬁciently, sometimes involving\nhierarchies of local and distributed shared buffers to split\npost-processing tasks (Horgan et al., 2018).\nResource requirements and scale. Recent successes in\napplying RL at scale in gaming (e.g. OpenAI Five (Ope-\nnAI, 2018), AlphaGo (Silver et al., 2017)) were enabled by\ntraining models on up to tens of thousands of CPU cores\nand hundreds of GPUs. In contrast, models for environ-\nments which are not easily parallelized may be executed on\na single CPU but might have stringent latency requirements.\nModels and optimization strategies.\nNeural networks\nused to represent policies range from small multi-layer\nperceptrons to complex hierarchical representations (Sil-\nver et al., 2016; Wayne et al., 2018). Learning approaches\nvary from small incremental updates to expensive but in-\nfrequent policy optimizations over large batches, making\neffective use of hardware accelerators difﬁcult.\n2.2\nExisting abstractions\nReference implementations.\nMany libraries primarily\nserve as reference implementations. For example, OpenAI\n1https://github.com/rlgraph/rlgraph\nRLgraph: Modular Computation Graphs for Deep Reinforcement Learning\nbaselines (Sidor & Schulman, 2017), Keras-rl (Plappert,\n2016) and Google’s Dopamine (Bellemare et al., 2018) pro-\nvide collections of well-tuned algorithms on benchmarks\nsuch as OpenAI gym (Brockman et al., 2016) or ALE (Belle-\nmare et al., 2013). Nervana Coach (Caspi et al., 2017)\ncontains a similar collection but with added tools for vi-\nsualizing progress, and facilities for hierarchical learning\nand distributed training. Horizon focuses on building end-\nto-end pipelines for off-policy training at Facebook (Gauci\net al., 2018).\nReference implementations share some components be-\ntween algorithms (e.g. network architectures) but typically\nignore many practical considerations in favour of concise\ncode. Retooling them to different execution modes, envi-\nronment semantics, or device strategies (e.g. multi-GPU\nsupport) requires signiﬁcant work due to hard-coded, tightly\ncoupled designs.\nCentralized control. Ray RLlib (Liang et al., 2018) de-\nﬁnes a set of abstractions for scalable RL. It relies on Ray’s\nactor model (Moritz et al., 2017) to execute RL algorithms\nvia centralized control. At the core of RLlib’s hierarchical\ntask parallelism approach lies a set of optimizer classes.\nEach optimizer implements a step() function which dis-\ntributes sampling to remote actors, manages buffers, and\nupdates weights. For example, an AsyncReplayOptimizer\nimplements distributed prioritized experience replay (Hor-\ngan et al., 2018). Each step, the optimizer loop pulls samples\nfrom actors, inserts them into replay buffers, and performs\ntraining on an asynchronous learner thread. A core claim\nof RLlib is the separation of the execution plane in the op-\ntimizer from the deﬁnition of the RL algorithm within a\npolicy graph. However, each optimizer encapsulates both\nlocal and distributed device execution. This means for ex-\nample that only the dedicated multi-gpu optimizer class\nsupports splitting input batches synchronously over multi-\nple GPUs. RLlib’s optimizer abstractions also mix Python\ncontrol ﬂow, Ray calls, and TensorFlow calls throughout its\ncomponents. Algorithms implemented in RLlib are hence\nnot easily portable as training is principally meant to be\nexecuted only on Ray.\nFixed end-to-end graphs. TensorForce (Schaarschmidt\net al., 2018) is a TF library providing a declarative inter-\nface to a number of RL algorithms. TensorForce focuses\non applied use cases where control ﬂow is driven by exter-\nnal application contexts, not simulation environments. Its\nend-to-end in-graph control ﬂow design accelerates execu-\ntion by avoiding unneeded context switches between Python\ninterpreter and TF runtime (Yu et al., 2018). A key dis-\nadvantage of this design (also adopted by Batch PPO and\nTF-Agents (Hafner et al., 2017; Guadarrama et al., 2018))\nis that partial data-ﬂow is difﬁcult to test due to limited\nin-graph-debugging facilities. Further, execution assign-\nments via device and variable sharing decorators are not\nseparate from algorithm logic in the absence of a modular\nbuild process.\n3\nFRAMEWORK DESIGN\n3.1\nDesign principles\nIn the absence of a single dominant design pattern, frame-\nworks must resolve the tension between ﬂexible prototyping,\nreusable components, and scalable execution mechanisms.\nRLgraph’s design is driven by a number of insights:\nSeparating algorithms and execution. RL algorithms re-\nquire complex control ﬂow to coordinate distributed state\nand sample collection on one hand, and internal training\nlogic on the other hand. Separating these aspects is difﬁcult\nbut essential to avoid re-implementing execution strategies.\nRLgraph manages local execution via graph executors which\nexpose a clear interface between a high level API and the\ncomponent graph. Distributed coordination is delegated to\ndedicated distributed executors (e.g. on Ray), or as part of\nthe graph build in the executor (distributed TF).\nReusable components with strict interfaces. Deep learn-\ning frameworks enable quick prototyping of neural networks\nby exposing APIs to combine different types of layers with\ncompatible interfaces. Providing a similar set of interchange-\nable components towards RL is complicated by the multi-\ntude of learning and execution semantics. This is exacer-\nbated by implementations containing deﬁnitions in multiple\nexecution contexts, e.g. Python control ﬂow interleaved\nwith calls to TF runtime. Tight coupling of components,\nand in turn a lack of well-deﬁned interfaces and component\nboundaries, means that re-usability is severely constrained.\nRLgraph addresses this problem via its modular component\narchitecture. Components only interact via declared API\nmethods and guarantee reusability as they are fully speciﬁed\nthrough compatible input spaces.\nIncremental sub-graph testing. An undesirable conse-\nquence of incorporating stochastic approximations at all\nlevels is numerical sensitivity and non-determinism (Na-\ngarajan et al., 2018). RL algorithms can require an over-\nwhelming number of hyperparameters (often in excess of\n25). This has created severe issues for robustness and re-\nproducibility (Henderson et al., 2017; Mania et al., 2018).\nImplementations are notoriously difﬁcult to debug and test\nin part because generating and verifying inputs and outputs\nof partial dataﬂow is tedious (e.g. manually creating tensors\nof required shapes). RLgraph enables sub-graph testing\nby allowing users to send example data from input spaces\nthrough arbitrary components and component combinations.\nRLgraph: Modular Computation Graphs for Deep Reinforcement Learning\n3.2\nComponents and graphs\nComponents. Next, we discuss the design of RLgraph’s\ncomponent graph. For simplicity, we use TensorFlow as the\nprimary backend and describe the implementation of other\nbackends (e.g. PyTorch) in §4.2. RLgraph’s core abstrac-\ntion is the Component class which encapsulates arbitrary\ncomputations via graph functions. RLgraph components\nare conceptually similar to DeepMind Sonnet’s components\n(DeepMind, 2017) but offer more advanced notions of com-\nposition. Consider a replay buffer component which exposes\nPrioritizedReplay\n \nScope/name: prioritized­replay\n \nDevice: CPU\nBackend:  TensorFlow\ninsert_records\n(API)\nVariables: \nbuffers & indices\ngraph_fn\n TensorFlow\n operations\ngraph_fn\nupdate (API)\n \nget_records (API)\nSegmentTree \n(sub component) \ngraph_fn\nAPI\ngraph_fn\nFigure 2. Example memory component with three API methods.\nfunctionality to insert experiences and sample mini-batches\naccording to priority weights. Implementing this buffer in\nan imperative language such as Python is straight-forward,\nbut including it as part of a TensorFlow graph requires cre-\nating and managing many variables through control ﬂow\noperators (e.g. to update priorities). Composing multiple\nsuch components in a re-usable way is difﬁcult due to an\nimpedance mismatch between class-based programming in\na driver language, and functional transformations within\na dataﬂow graph. Using a deﬁne-by-run framework (e.g.\nPyTorch) eases development but can create difﬁculties in\nlarge scale distributed execution and program export.\nExisting high-level APIs for neural networks such as Son-\nnet, (DeepMind, 2017), Keras (Chollet et al., 2015), Gluon\n(Rochel et al., 2018), or TF.Learn (Tang, 2016) focus on\nassembly and training of neural networks. Implementing\nRL workloads in these frameworks usually means mixing\nimperative Python control ﬂow with deep learning graph\nobjects, leading to the design issues discussed before.\nWhen building for a static graph backend, RLgraph’s com-\nponent API enables fast composition of end-to-end differ-\nentiable dataﬂow graphs with in-graph control ﬂow. The\ngraph builder and executor automatically manage burden-\nsome tasks such as variable and placeholder creation, scopes,\ninput spaces, and device assignments.\nExample component. In ﬁgure 2, we show a simpliﬁed\nprioritized replay buffer component. All components inherit\nfrom the generic Component class and assemble logic by\ncombining their own sub-components. The buffer has a\nsegment tree sub-component to manage priority orders. It\nexposes API methods to insert, sample, and update priorities\nwhich under the hood map to three graph functions. The\ndifference between simple object methods and RLgraph API\nmethods is that registered API methods are identiﬁed and\nmanaged in the build. Input shapes can be inferred automat-\nically via dataﬂow from inputs to the root component.\nDevelopers can declare methods as API methods by call-\ning a register function (or in the future using a decorator).\nTechnically, not all functionality of a component needs to\nbe registered as an API method. Users can also implement\nhelper functions or utilities, e.g. using TensorFlow opera-\ntions without including them as API methods, if they do not\nneed to be called from external components. Implementing\nsuch utilities as RLgraph components with API methods is\nnonetheless useful because these features can then be built\nand tested as sub-graphs.\nComponents can call arbitrary sub-components (and their\nsub-components). A component may have multiple API\nmethods where input spaces to one method depend on out-\nputs of its other methods. The build process ensures com-\nponent computations and internal variables are only created\nonce its input spaces are known. Instead of creating implicit\ndevice assignments, scopes, and variable sharing through\nnested contexts, RLgraph explicitly manages these proper-\nties per component.\n3.3\nBuilding component graphs\nRLgraph models are assembled in three distinct phases:\n1. Component composition phase in which component\nobjects are deﬁned and combined, including arbitrary\nnesting of sub-components.\n2. Assembly phase in which a type- and dimension-less\ndataﬂow graph (the component graph) is created. This\nis achieved by calling each of the root component’s API\nmethods once to traverse its graph. The API-methods\nof the root component deﬁne the externally visible API\nof the component graph.\n3. Graph compilation/building phase in which all com-\nputation operations are deﬁned for each component.\nInside a component class, these deﬁnitions are placed\nin special functions called graph functions. Graph func-\ntions are the only places in the code where backend\ndependent objects are used (e.g. TF ops).\nThere are also intermittent sub-phases for the initialization\nof execution aspects (e.g. session management) which are\nexplained in §4. A wide range of off-the-shelf component\nimplementations such as buffers, optimizers, neural net-\nworks, or nested space splitters and mergers means that most\nRLgraph: Modular Computation Graphs for Deep Reinforcement Learning\n 1 @rlgraph_api\n 2 def update(self, batch_size):\n 3    sample = self.memory.sample(batch_size)\n 4\n 5    s, a, r, next_s, t = self.splitter.split(sample)\n 6 \n 7    \n 8    loss = self.dqn_loss.get_loss(s, a, r, next_s, t)\n 9 \n10   variables = self.policy.variables()\n11   update = self.optimizer.step(loss, variables)\n12   return update, loss\n13\n14\n15\n16\n17 @rlgraph_api(split=True)\n18 def observe(self, records):\n19   return self.memory.insert(records)\nsub­component\nAPI method of sub­somponent\nCreate end­to­end graphs by connecting sub­components\nUse space­hints to auto­split/merge/flatten; API is actually called \nonce per records component (if records is a dict/tuple­container)\nAPIs return abstract “component graph­records” …\n… that can be passed into other API methods\nFigure 3. Example API methods show framework-independent\ncontrol-ﬂow construction.\nusers will only need to deﬁne few components to prototype\nnew algorithms (e.g. loss function, network architecture).\n1. Component composition and nesting. All components\nneeded by a model are deﬁned as Python objects. Compo-\nnents are logically organized as nested subcomponents of a\nroot container component exposes the external API. Note\nthat an agent can in theory deﬁne multiple root components\nto act and learn different policies in parallel.\n2. Assembling the component graph. Next, users deﬁne\nthe dataﬂow through the model via connections between\ncomponents. Neither data types nor shape information are\nnecessary at this stage. Each component comes with a set of\nAPI-methods (Figure 3). The data (tensors) are interpreted\ninside these methods as abstract meta-graph operator ob-\njects, and their shapes and types will be inferred at build\ntime. This is achieved via decorators for API and graph\nfunctions which provide options to split, merge, un-nest/re-\nnest inputs and outputs. Return values of an API call can\nnow be passed into other API-methods, a sub-component’s\nAPI-method, or into a graph function for numerical ma-\nnipulation. RLgraph spaces can conveniently nest, merge,\nsplit and fold time and batch dimensions of tensors through\ncomponents. In our experience, these utilities drastically\nreduce development times as the different build phases au-\ntomatically detect problems when manipulating complex\nspaces, e.g. records containing multiple states and actions\nwith batch and time dimensions.\nA simpliﬁed component graph assembly procedure is shown\nin Algorithm 1. The root component exposing the external\ninterface (e.g. act, observe, update) and the input spaces for\nthe external API are passed to the component graph builder.\nThis builder generates the backend-independent dataﬂow\ngraph and the API by iterating over all API methods deﬁned\nin the root component. For each method, a component graph\nAlgorithm 1 Component graph build procedure\nInput: component root, input spaces spaces\napi = dict()\n// Call all api methods once, generate op columns.\nfor method, record in root.api do\nin ops records = list()\n// Create one input record per API input param.\nfor param in record.input args do\nin ops records.append(Op(param.space))\nend for\n// Traverse graph from root for this method.\nout ops records = method(in ops records)\n// Register method with graph inputs and output ops.\napi[method] = [in ops records, out ops records]\nend for\nreturn ComponentGraph(root, api)\nop is created for each of its parameters and looked up in\nthe input graph (type checks and default argument handling\nomitted). The component graph is then traversed through\ncomposed API functions which infer parameters and return\nvalues for each call , and these are stored as records in the\ncomponent graph. Finally, the API method is registered in\nan API registry which contains the input spaces and ﬁnal\noutput ops (identiﬁed through the traversal).\n3. Building computation graphs. All numerical opera-\ntions occur in the third phase inside graph functions which\nimplement each component’s API. Operations deﬁned in a\ngraph function include for example deﬁning loss functions,\nor sampling from a buffer. As sometimes the component’s\nvariables must be accessed to complete these operations\n(e.g. a neural network layer must read its weights), RLgraph\nensures that any such computation function is only called\nafter all variables of a component have been deﬁned. For ex-\nample, the memory component in Fig. 2 can only deﬁne its\nbuffers (e.g. TensorFlow variables) once it receives shapes\nand types of buffer contents.\nThis barrier is enforced during the build, and custom com-\nponents only need to override a generic method for variable\ncreation. The method is called automatically and receives\ntypes and shapes of variables as input arguments. Develop-\ners thus only need to specify the external input spaces to\nthe program (e.g. int/ﬂoat boxes with batch and time ranks,\ncontainer spaces for nested data). In practice, these are pri-\nmarily state and action layouts deﬁned by the environment.\nWe brieﬂy describe the intuition behind the main build algo-\nrithm. The build begins by calling all API methods deﬁned\nat the root from the provided input spaces until a component\nis input-complete, i.e. all spaces for all its computations\nare available. It then executes a completion function which\ncalls the component’s create variables, and subsequently its\nRLgraph: Modular Computation Graphs for Deep Reinforcement Learning\ngraph functions under the correct device and scope to deﬁne\nits operations. Once a component is complete, the outputs\nof its graph functions become available as input spaces for\nsubsequent components/graph functions. We then simply\nperform breadth-ﬁrst-search until there are no more com-\nponents to build or a constraint violation is detected. Input\nplaceholders and op names are created and stored for all ops\nand output combinations deﬁned in the API.\n1 state_space = FloatBox(shape=(64,), add_batch_rank=True,\n2\nadd_time_rank=True)\n3 # Dict space: 1 discrete, 1 continuous action.\n4 action_space = Dict(discrete=IntBox(),\n5\ncont=FloatBox(), add_batch_rank=True)\n6 policy = Policy(\"recurrent_policy.json\", action_space)\n7\n8 # Construct sub graph from spaces, auto-gen placeholders.\n9 test = ComponentTest(policy, dict(nn_input=state_space),\n10\naction_space=action_space))\n11 # Test with any inputs in the input space.\n12 action = test.test(policy.get_action, state_space.sample())\nListing 1. Testing sub-graphs from arbitrary spaces.\nTesting sub-graphs. Identifying bugs in sub-tasks is difﬁ-\ncult without a systematic mechanism for ﬁne-granular input\ngeneration. Consider the RLgraph test class example in\nListing 1. Here, we build a Policy component (with subcom-\nponents for a recurrent network and action selection) for the\nspeciﬁed state and action spaces (with options for batch and\ntime ranks). The test helper builds the sub-graph for the\npolicy via the phases describe above. Users can then run the\ntest and call an API method (e.g. by sampling an input from\nthe input space). This call is delegated to a graph executor\nwhich executes the corresponding op. Every component\n(including all pre/post processing and learning heuristics)\nis continuously tested separately and in integration tests for\nlarger graphs (i.e. complete RL algorithms).\n3.4\nAgent API\nPre-built models can be conﬁgured via declarative conﬁgu-\nrations similar to TensorForce (Schaarschmidt et al., 2018).\nConﬁgurations are provided as e.g. JSON documents speci-\nfying an algorithm and its components (network with list of\nlayers, buffer, optimizers, device strategy etc.). The agent\ninterface deﬁnes a set of abstract methods which agents\nmust support to access certain execution modes (Listing 2).\nThe main difference between RLgraph and existing APIs\nlies in strictly enforced component boundaries and more\nexplicit execution semantics. Fine-grained device control\nis managed via a device map where each components oper-\nations and variables can be assigned separately and selec-\ntively. Components may only exchange data along edges\nof the component graph where an edge corresponds to a\ncall to a declared API method. This ensures well-deﬁned\nAPIs and also avoids a common case where two components\nare implemented to always be used together, thus making\nindividual reuse difﬁcult. In RLgraph, all components can\nbe used and built individually from any input spaces. Next,\nwe discuss graph execution.\n1 abstract class rlgraph.agent:\n2\n# Build with default devices, variable sharing, ..\n3\ndef build(options)\n4\ndef get_actions(states, explore=True, preprocess=True)\n5\n# Update from internal buffer or external data.\n6\ndef update(batch=None, sequence_indices=None)\n7\n# Observe samples for named environments.\n8\ndef observe(state, action, reward, terminal, env_id)\n9\ndef get_weights, def set_weights\n10\ndef import_model, def export_model\nListing 2. High level agent API.\n4\nEXECUTING GRAPHS\n4.1\nGraph executors.\nAll build phases are invoked from a graph executor which\nserves as the execution bridge between the component graph\nand a backend framework. Graph executors expose an exe-\ncute() method which takes name and arguments of an API\nmethod and returns the result. They further manage any\nbackend-speciﬁc initialization, monitoring, and devices. For\nexample, the TensorFlow executor assumes the following\ntasks. First, it initializes the TensorFlow session and vari-\nables, and builds hooks for summaries or proﬁling. For\noperation execution, it fetches input placeholders and op\nnames from the graph operation registry and assembles\nsession inputs. Importantly, there is no other interaction\nbetween user programs and graph other than through API\noperations deﬁned in the root component.\nDevice management. Graph executors also handle device\nassignments by interleaving the build with a phase to ini-\ntialize device strategies. Upon initialization, local device\ninformation (e.g. CUDA visible devices) is read and com-\npared against user-deﬁned device maps or synchronization\nmodes. Consider a synchronous multi-GPU strategy where\neach input batch is split to one graph copy per GPU, with\ngradients averaged for the ﬁnal update. Managing this syn-\nchronization requires the creation of additional operations\nand variables for the device copies and the splitting and av-\neraging logic. The graph executor does this by creating core\ncomponent copies and connecting them through generic\ninput space splitters before building the component graph.\nSince no variables or placeholders have been created at this\nstage, components or sub-graphs can straightforwardly be\nmodiﬁed, replaced or extended. In addition to generic device\nstrategies, users can deﬁne a device map which speciﬁes a\ndevice assignment for each component’s ops and variables.\nDistributed execution. RLgraph can be executed in dis-\ntributed mode either with framework-speciﬁc mechanisms\nor by using any other means to create and synchronize\nRLgraph: Modular Computation Graphs for Deep Reinforcement Learning\nAgent API\nGraph\nexecutor/ \ndevices/ \nproﬁling \nGeneral purpose API: get_action, update, export,..\nLocal backends\nRay executor\nRay\nWorker_1\nDistributed \ncoordination \nlayer \nRay\nWorker_n\nRLgraph \nlocal  \nexecution \nlayer \nLocal \nRLgraph  \nagent \nGraph\nBuilder\nOP registry\n...\nDistributed TF/PS \nVectorized \nsample \ncollection \nTF\nWorker_1\nTF\nWorker_n\n...\nGraph executor  \nsyncs variables to PS, \nmanages plugins (Horovod) \nFigure 4. RLgraph execution stack.\nagents. For example, when using distributed TensorFlow,\nthe TF graph executor can create the necessary parameter\nserver and global/local synchronization operations. It can\nalso plug-in third party tools such as Uber’s Horovod to\nassume speciﬁc aspects of distributed communication for\na backend (e.g. ring all-reduce (Sergeev & Balso, 2018)).\nTo demonstrate RLgraph’s ﬂexibility, we also built a Ray\nexecutor which can execute arbitrary RLgraph implemen-\ntations on Ray’s centralized execution model (Moritz et al.,\n2017). In the evaluation, we show that our implementation\noutperforms Ray’s native library RLlib (Liang et al., 2018).\nFigure 4 illustrates how local execution and distributed co-\nordination are separated. When using Ray, we simply pass\nan agent conﬁguration to our Ray executor which creates\nRay workers, each locally generating their component graph\nand graph executor to interact with environments. When\nusing non-centralized control (e.g. distributed TensorFlow),\nthe graph executor creates the necessary session and server\nobjects and handles parameter server synchronization on\neach worker. Common features such as fault tolerance are\ndelegated to the underlying execution engines via a conﬁgu-\nration interpreted by the respective graph executors.\n4.2\nBackend support and code generation\nDeﬁne-by-run graphs. While there are many advantages to\ndeﬁning models as end-to-end computation graphs, deﬁne-\nby-run semantics and eager execution are increasingly pop-\nular for ease of use. RLgraph’s component graph can be\nexecuted both in static graph construction or deﬁne-by-run\nmode. To implement a PyTorch backend, we only had to\nmodify the build procedure as follows. As there are no\nplaceholders, we simply create torch tensors during the\nbuild phase as artiﬁcial placeholders to push through the\ndataﬂow graph for shape and type inference of variables\n(e.g. tensors used to store state). This is because even in\ndeﬁne-by-run mode, automatically dealing with nesting and\nsplitting of complex spaces across time and batch dimen-\nsions can be handled via decorators. Next, after building\nthe component graph, we change the execution mode ﬂag\nfor API methods from ’build’ to ’deﬁne-by-run’. In this\nmode, instead of returning operation objects used for graph\nconstruction, RLgraph simply directly evaluates a call-chain\nof graph functions to retrieve the result. RLgraph hence pro-\nvides a uniﬁed interface for executing its component graph\nAPI in deﬁne-by-run and static-graph mode.\nAutograph and graph optimization. An emerging but\nearly trend in deep learning frameworks are features to au-\ntomatically convert imperative code to static computation\ngraphs. Examples of this include TensorFlow’s AutoGraph\n(Moldovan et al., 2018) mechanism and PyTorch’s JIT trac-\ning. We encountered temporary limitations e.g. in Auto-\nGraph on stateful operations which would need to convert\nlist manipulation to TF variable updates. We plan to merge\nbackend-dependent graph-function implementations into\nsingle-stream functions, which will then be auto-converted\nwhere possible.\nThe ad-hoc mixed-backend implementation style of many\nRL libraries creates hurdles for systematically exploiting\ngraph generation. RLgraph provides a natural ﬁt for these\nfeatures due to its organization of all backend code into\ngraph functions as logical units. Features such as auto-\ngraph can hence be integrated centrally in decorators during\nthe build process (similar to device assignment).\nRLgraph’s separation of concerns opens up opportunities\nfor optimization at all stages. Emerging approaches in op-\ntimizing execution (e.g. via automated device placement\n(Mirhoseini et al., 2018)), or backend-dependent compila-\ntions can be integrated at the graph build stage.\n5\nEVALUATION\nWe evaluate RLgraph’s performance using different dis-\ntributed execution engines, local deep learning backends,\nand device strategies. Our aim is not to benchmark the\nunderlying frameworks but to show RLgraph can perform\ncompetitively compared to native implementations.\n5.1\nResults\nBuild overhead. We begin by comparing the one-time build\noverhead of RLgraph’s abstractions on different backends.\nRecall there are two sources of overhead during initializa-\ntion. First, the component graph is created by traversing\nthe graph from the root. Second, creating variables and\npotentially static graph operations by letting input spaces\nﬂow through the component graph requires moving through\ncomponents in the iterative build procedure.\nIn Figure 5a, we show component graph and main build\nRLgraph: Modular Computation Graphs for Deep Reinforcement Learning\noverhead on a single memory component and a common RL\narchitecture (DQN) for the TensorFlow (TF) (v1.11) and\nPyTorch (PT) (v0.4) backends. Overhead here refers to the\ntime spent on top of creating variables and operations, which\nwould have to be done irrespective of using RLgraph. The\noverhead for both build phases to build a single component\n(e.g. for modular performance testing and debugging) is less\nthan 100 ms. For a common architecture (dueling DQN with\nprioritized replay, 43 components), the combined overhead\nis about 1 s for TF and 650 ms for PT. PyTorch builds only\nrequire a few milliseconds as variables (e.g. to represent\nmemory state) are native Python lists or NumPy arrays.\nPrioritized replay\nDQN\nArchitecture\n0.00\n0.25\n0.50\n0.75\n1.00\nRuntime (s)\nTF trace\nTF build\nPT trace\nPT build\n(a) Build overheads.\n1\n2\n4\n8\n16\n32\nParallel Pong environments\n800\n1100\n1400\n1700\n2000\nEnv frames per second\nTF RLgraph\nPT RLgraph\nPT hand-tuned\n(b) Worker act performance.\nFigure 5. TensorFlow and PyTorch backend comparison.\nWe also compare backend runtime performance by testing\nact (inference) throughput on a single-threaded worker act-\ning on a vector of environments (Fig. 5b). We use the\nAtari Pong environment and a standard 3-layer convolu-\ntional architecture followed by a dueling network for action\nselection. The TF backend (TF RLgraph) does not incur\nruntime overhead because the component graph is discarded\nafter building. The graph executor only looks up the name\nof the operation in the op registry and executes the corre-\nsponding TF op. In deﬁne-by-run mode using PyTorch (PT\nRLgraph), RLgraph incurs some overhead when calls are\nrouted through components. To understand this overhead,\nwe also implemented a bare-bones PyTorch actor, including\nﬁne-tuning OpenMP and MKL settings (PT hand-tuned).\nTensorFlow outperforms both PyTorch variants as batch-size\nincreases on a CPU, thus making it more suitable for per-\nforming batch-acting and batch-postprocessing. RLgraph’s\nPT backend carries overhead due to requiring additional\nlookups when traversing the graph via API decorators. This\noverhead becomes negligible as batch size increases and\nruntime is dominated by the network forward passes. To\nreduce this overhead when traversing the component graph,\nwe implemented some initial fast-path calls. For some cases,\nthe graph builder can identify edge-contractions (where calls\nare edges and components are vertices), so deﬁne-by-run\nexecution through the relevant sub-graph requires no inter-\nmediate component calls.\nTo the best of our knowledge, RLgraph represents the ﬁrst\ncommon interface to TF and PT on a component level. High-\nlevel APIs like Keras only support static graph approaches.\nLibraries such as Ray RLlib support TF/PT backends but\nonly on an algorithm level where entire implementations\ncan be parallelized via RLlib’s distributed abstractions. Us-\ning RLgraph, developers assemble logic via the backend-\nindependent API, and can then implement e.g. a new loss\ncomponent in their backend of choice.\nDistributed execution on Ray. Next, we evaluate RLgraph\non the distributed execution engine Ray in comparison to\nRay’s native library, RLlib (v0.5.2). We implemented dis-\ntributed prioritized experience replay (Ape-X (Horgan et al.,\n2018)), a state-of-the-art distributed Q-learning algorithm\non our Ray executor. We further implemented a vector-\nized environment worker for sample collection, including\nall heuristics described in the Ape-X paper and found in\nRLlib’s implementation (e.g. worker-side prioritization).\nAll experiments were performed on Google Cloud with the\nlearner being hosted on a GPU instance with 1 active V100\nGPU, 24 vCPUs and 104 GiB RAM. Sample collection\nnodes had 64 vCPUs and 256 GiB RAM. Figure 6 shows\n16\n32\n64\n128\n256\nNumber of workers\n0\n25000\n50000\n75000\n100000\n125000\n150000\n175000\nEnvironment frames/s\nRLLib\nRLgraph\nFigure 6. Distributed sample throughput on Pong.\nsampling performance on the Pong environment. The x-axis\nrepresents the number of policy-evaluators/Ray-workers re-\nspectively (RLlib, RLgraph), each initialized with a single\nCPU, and the y-axis shows environment frames per second\n(including frame skips). Each worker executed 4 environ-\nments, and we used 4 instances of replay memories to feed\nthe learner (we did not observe improvements using more\nmemories due to limited vCPUs on GPU nodes). All set-\ntings were run with 8 sample nodes except 256 workers\n(16 sample nodes) to ensure sufﬁcient memory. RLgraph\noutperforms RLlib by a large margin (185% on 16, 60% on\n256 workers) despite implementing the same algorithm with\nequivalent hyper-parameters, model size, and environments.\nPerformance for 16 workers is highest due to better resource\nutilization. RLgraph also completes its learning tasks faster\ndue to improved implementations.\nThe reason for RLgraph’s performance is systematic com-\nponent analysis yielding insights into efﬁcient sample pro-\ncessing. For example, RLlib’s policy evaluators execute\nmultiple session calls to incrementally post-process batches.\nRLgraph instead splits post-processing in incremental and\nRLgraph: Modular Computation Graphs for Deep Reinforcement Learning\nbatched parts to minimize calls to the TensorFlow runtime.\nThese effects can also be observed at the scale of a single\n200\n400\n800\n1600\n3200\nWorker task size (num samples)\n750\n1000\n1250\n1500\nEnv frames/s\nRLgraph\nRLlib\n1 envs\n4 envs\n8 envs\n(a) Single worker throughput.\n0\n1k\n2k\n3k\n4k\n5k\n6k\n7k\n8k\nTime in seconds\n−21\n−10\n0\n10\n21\nMean worker rewards\nRLlib\nRLgraph\n(b) Training times for Pong.\nFigure 7. Single task throughput and learning comparison.\ntask across different task lengths and number of environ-\nments (called sequentially). Figure 7a shows the requested\nnumber of samples versus the achieved frames per second\nusing a single RayWorker (RLgraph) versus a policy eval-\nuator (RLlib). Both use the same agent and conﬁguration\nas in the distributed setting (10 warm up runs, mean across\n50 runs). RLgraph is not only more effective on a single en-\nvironment, it also scales better on vectorized environments\ndue to faster accounting across environments and episodes.\nWe show learning results (Fig. 7b) to conﬁrm that RLgraph’s\nthroughput is not at the cost of training performance. We\nuse RLlib’s provided tuned Pong conﬁguration (32 workers).\nIn RL, the same code using the same hyper-parameters can\nvary drastically across runs, so reliably comparing learning\nis difﬁcult (Henderson et al., 2017). We ran 10 random\nseeds and averaged across the 3 best runs (both libraries\ndid not learn anything for some seeds as expected). In line\nwith throughput, RLgraph learns to solve (reward 21) Pong\nsubstantially faster than RLlib.\nNote that RLlib’s published results on Ape-X throughput\ndo not include updating without stating this explicitly, and\nlater reported results including updates2 are up to 130 k\nframes per second on 256 workers (versus 170 k max for\nRLgraph). Experimental versions of Ray’s new backend\ninclude improved garbage collection of which RLgraph\nwould beneﬁt to the same extent as RLlib3. Our results\nshow RLgraph’s execution-agnostic design can integrate\nwith external execution engines, and perform competitively.\nWhile RLlib could adopt more efﬁcient implementations,\nour insight is that RLgraph can be used on Ray via few\nwrapper classes. Implementing other distributed semantics\non Ray with RLgraph only requires extending the generic\nRay executor to implement a coordination loop. Finally,\nRLgraph’s modularization helps visualizing computation\ngraphs when compared to programming models such as\n2Source:\nRLlib\nauthors,\nhttps://github.com/\nray-project/ray/issues/2466\n3During experimentation using Ray’s original backend, we ex-\nperienced some difﬁculties with the Ray engine, including memory\nleaks and task initialization crashes.\nRLlib’s, which scatter code across fragmented tasks. We\nvisualize both Ape-X implementations in Appendix A.2.\n0\n1k\n2k\n3k\n4k\n5k\n6k\n7k\n8k\nTime in seconds\n−21\n−10\n0\n10\n21\nMean worker rewards\nSingle GPU\nMulti-GPU-Sync\nFigure 8. RLgraph synchronous multi-GPU device strategy\nMulti-GPU strategies. We also implemented device strat-\negy prototypes where the component graph is automatically\nexpanded during the build phase (e.g. to create sub-graph\nreplicas). When using a synchronous GPU replica strategy,\nthe update batch is internally split into multiple sub-batches,\nand gradients are averaged across towers. Fig. 8 contains\nApe-X results using 1 and 2 V100 GPUs. We observe the\nexpected speed-up in convergence.\nDistributed TensorFlow. Finally, we evaluate RLgraph\nusing the distributed TensorFlow backend on DeepMind’s\n(DM) importance-weighted actor-learner architecture (IM-\nPALA) (Espeholt et al., 2018). The authors have open-\nsourced an optimized implementation4. IMPALA perhaps\nbest represents the end-to-end computation graph paradigm,\nwhere even environment interaction is fused into the TF\ngraph. We implemented IMPALA in RLgraph to demon-\nstrate its ability to generate such graphs. To this end, RL-\ngraph provides generic execution components for graph-\nfused environment stepping. IMPALA executes updates by\nletting each actor perform a rollout step (100 samples) and\ninput its samples into a globally shared blocking queue. The\nlearner dequeues rollouts and uses a staging area to hide\nGPU latency. Figure 9 compares throughput using the large\n16\n32\n64\n128\n256\nNumber of workers\n0\n5000\n10000\n15000\n20000\nEnvironment frames/s\nDeepmind IMPALA\nRLgraph IMPALA\nFigure 9. IMPALA throughput comparison on seekavoid arena 01\nnetwork described in the paper on a DM lab 3D task (which\nare more expensive to render than Atari tasks). We again\nuse a single V100 GPU for the learner and let 4 workers\n4Code\nat:\nhttps://github.com/deepmind/\nscalable_agent\nRLgraph: Modular Computation Graphs for Deep Reinforcement Learning\neach share a 8 vCPU instance. RLgraph achieves about\n10-15% higher mean throughput (5 runs) for fewer workers\nuntil both implementations are limited by updates. DM’s\nimplementation exhibited higher variance due to subtle dif-\nferences in preprocessing tensors after unstaging. DM’s\ncode also carried out unneeded variable assignments in the\nactor. Removing these yielded 20% improvement in a single-\nworker setting for RLgraph. Emerging tools in graph opti-\nmization (e.g. TF XLA, TVM (Chen et al., 2018)) can help\nwith optimizing data layouts and numerical transformations.\nRLgraph helps most in improving high level dataﬂow as it\nenforces well-deﬁned interactions between components (c.f.\nAppendix A.1). This in turn improves reasoning about the\ncomplex design patterns found in RL algorithms.\n5.2\nDiscussion\nOur results show that using RLgraph to deﬁne algorithms via\ncombining backend-independent components yields high-\nperforming implementations across backends. Assembling\nthe graph through the different build phases only adds up to\none second of build overhead. Using Ray as a distributed\nbackend, we demonstrated that RLgraph run on its own\nRay executor outperforms Ray RLlib. This indicates that\nwide-spread mixed backend (e.g. Python/TF) RL implemen-\ntations can be signiﬁcantly accelerated via careful dataﬂow\nanalysis, and without changing algorithm logic. Using\ndistributed TF, we found that RLgraph can help improve\ndataﬂow in end-to-end static computation graphs. Our main\ntake-away is that developers can use RLgraph to focus on\nlogical component composition in RL independently of the\nunderlying execution paradigm.\n6\nRELATED WORK\nRLgraph builds upon the experiences of many prior libraries\nwhich we discussed in §2.2. Here, we discuss emerging\ntrends in programming models and optimizations.\n6.1\nProgramming models\nThe success of deep learning frameworks has given rise\nto several higher-level learning APIs seeking to free users\nfrom dealing with lower level tensor operations. Keras\n(Chollet et al., 2015) is a popular framework for quick\nassembly and training of deep learning models with sup-\nport for multiple static-graph backends (e.g. TensorFlow\n(Abadi et al., 2016), CNTK (Seide & Agarwal, 2016),\nMXNet (Chen et al., 2015)). Gluon provides a concise\nAPI for imperative, dynamic neural networks on top of\nMXNet (Rochel et al., 2018). TF.Learn offers a high level\nAPI for constructing symbolic TensorFlow graphs (Tang,\n2016). Among these frameworks, TensorFlow’s program-\nming model (Abadi et al., 2017) is distinct because it sup-\nports in-graph control-ﬂow (Yu et al., 2018). A key issue\nin the high-level APIs above is that they typically assume\ncontrol-ﬂow to be implemented in the driver language (fre-\nquently Python). RLgraph bridges this gap by providing\nSonnet-style (DeepMind, 2017) composable components,\nwith the addition of API methods handling in-graph control\nﬂow, or deﬁne-by-run graph execution.\n6.2\nGraph optimizations and code generation\nProgramming models for machine learning and in partic-\nular deep learning typically prioritize high level APIs and\nusability over performance. Code is written in a multitude\nof languages and libraries, and is executed over a variety\nof backends. Frameworks like Weld (Palkar et al., 2017)\noptimize performance by integrating library calls into a com-\nmon intermediate representation which is then mapped to\nefﬁcient multi-threaded code. Mirhoseini et al. identify\neffective TensorFlow device placements via hierarchical re-\ninforcement learning (Mirhoseini et al., 2018). FlexFlow\nfurther improves parallelization strategies for speciﬁc de-\nployments using an execution simulator and ﬁne-grained\nrandomized search across execution dimensions (Jia et al.,\n2018). TVM is a compiler stack to optimize tensor opera-\ntions across diverse hardware backends (Chen et al., 2018).\nRLgraph constructs component graphs irrespective of ex-\necution semantics. Optimizations can be performed at the\nlevel of graph executors by including them in the build.\nThe proliferation of deep learning frameworks which often\nfocus development efforts on Python front-ends has also\ncreated the need for new shared representations. The aim\nof standard formats such as ONNX (Facebook Inc., 2017)\nis to enable model interoperability, deﬁne a common route\nfrom prototyping to production deployment, and to create a\nshared runtime for optimizations. From a RL perspective,\nthe aim of deployable graphs is more difﬁcult to achieve due\nto the extensive control-ﬂow and state management in RL\nworkloads. Implementing code using e.g. TensorFlow con-\ntrol ﬂow operators allows immediate export, optimization\nand deployment of the entire RL program but can slow down\ndevelopment. Framework developers have recognized this\ntension and are designing tools such as AutoGraph in TF for\nautomatic graph generation. As these initiatives mature, we\nexpect RLgraph components to merge backend-dependent\ngraph functions by leveraging automatic graph generation.\n7\nCONCLUSION\nRLgraph is a new open source framework for designing and\nexecuting computation graphs for reinforcement learning.\nRLgraph’s component graph abstraction allows develop-\ners to separate the composition of logical components in\na RL algorithm from their local backend and distributed\nexecution. The resulting implementations are fast, robust,\nincrementally testable, and easy to extend or re-use.\nRLgraph: Modular Computation Graphs for Deep Reinforcement Learning\nACKNOWLEDGMENTS\nMichael Schaarschmidt is supported by a Google PhD Fel-\nlowship. We are also grateful for receiving research credits\nfrom Google Cloud. This research was supported by the\nEPSRC (grant references EP/M508007/1 and EP/P004024),\nthe Alan Turing Institute, and a Sansom scholarship. Further,\nwe thank Lasse Espeholt for providing support to replicate\nIMPALA results. We also want to thank the RLlib authors\nfor helping to replicate Ape-X results.\nREFERENCES\nAbadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean,\nJ., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al.\nTensorﬂow: A system for large-scale machine learning.\nIn OSDI, volume 16, pp. 265–283, 2016.\nAbadi, M., Isard, M., and Murray, D. G.\nA computa-\ntional model for tensorﬂow: An introduction. In Pro-\nceedings of the 1st ACM SIGPLAN International Work-\nshop on Machine Learning and Programming Languages,\nMAPL 2017, pp. 1–7, New York, NY, USA, 2017.\nACM. ISBN 978-1-4503-5071-6. doi: 10.1145/3088525.\n3088527. URL http://doi.acm.org/10.1145/\n3088525.3088527.\nBellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M.\nThe arcade learning environment: An evaluation platform\nfor general agents. J. Artif. Intell. Res.(JAIR), 47:253–279,\n2013.\nBellemare, M. G., Castro, P. S. C., Gelada, C., Kumar, S.,\nand Moitra, S. Dopamine. https://github.com/\ngoogle/dopamine, 2018.\nBrockman, G., Cheung, V., Pettersson, L., Schneider, J.,\nSchulman, J., Tang, J., and Zaremba, W. Openai gym.\nCoRR, abs/1606.01540, 2016. URL http://arxiv.\norg/abs/1606.01540.\nCaspi, I., Leibovich, G., and Novik, G. Reinforcement\nlearning coach, December 2017. URL https://doi.\norg/10.5281/zenodo.1134899.\nChen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., Xiao,\nT., Xu, B., Zhang, C., and Zhang, Z. Mxnet: A ﬂexible\nand efﬁcient machine learning library for heterogeneous\ndistributed systems. CoRR, abs/1512.01274, 2015. URL\nhttp://arxiv.org/abs/1512.01274.\nChen, T., Moreau, T., Jiang, Z., Zheng, L., Yan, E.,\nCowan, M., Shen, H., Wang, L., Hu, Y., Ceze, L.,\nGuestrin, C., and Krishnamurthy, A. Tvm: An auto-\nmated end-to-end optimizing compiler for deep learning.\nhttp://arxiv.org/abs/1802.04799v2, 2018.\nChollet, F. et al. Keras. https://keras.io, 2015.\nDeepMind.\nSonnet:\nTensorFlow-based neural net-\nwork library. https://github.com/deepmind/\nsonnet, 2017.\nEspeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih,\nV., Ward, T., Doron, Y., Firoiu, V., Harley, T., Dun-\nning, I., Legg, S., and Kavukcuoglu, K. IMPALA: scal-\nable distributed deep-rl with importance weighted actor-\nlearner architectures. CoRR, abs/1802.01561, 2018. URL\nhttp://arxiv.org/abs/1802.01561.\nFacebook Inc. ONNX- Open Neural Network Exchange\nFormat. https://onnx.ai, 2017.\nGauci, J., Conti, E., Liang, Y., Virochsiri, K., He, Y., Kaden,\nZ., Narayanan, V., and Ye, X. Horizon: Facebook’s open\nsource applied reinforcement learning platform. CoRR,\nabs/1811.00260, 2018. URL http://arxiv.org/\nabs/1811.00260.\nGuadarrama, S., Korattikara, A., Ramirez, O., Castro,\nP., Holly, E., Fishman, S., Wang, K., Gonina, E.,\nHarris, C., Vanhoucke, V., and Brevdo, E.\nTF-\nAgents: A library for reinforcement learning in ten-\nsorﬂow.\nhttps://github.com/tensorflow/\nagents, 2018.\nURL https://github.com/\ntensorflow/agents.\n[Online;\naccessed 30-\nNovember-2018].\nHafner, D., Davidson, J., and Vanhoucke, V.\nTensor-\nﬂow agents: Efﬁcient batched reinforcement learning\nin tensorﬂow.\nCoRR, abs/1709.02878, 2017.\nURL\nhttp://arxiv.org/abs/1709.02878.\nHenderson, P., Islam, R., Bachman, P., Pineau, J., Precup,\nD., and Meger, D. Deep reinforcement learning that\nmatters. CoRR, abs/1709.06560, 2017. URL http:\n//arxiv.org/abs/1709.06560.\nHorgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel,\nM., van Hasselt, H., and Silver, D. Distributed prioritized\nexperience replay. CoRR, abs/1803.00933, 2018. URL\nhttp://arxiv.org/abs/1803.00933.\nJia, Z., Zaharia, M., and Aiken, A.\nBeyond data\nand model parallelism for deep neural networks.\nhttp://arxiv.org/pdf/1807.05358v1, 2018.\nLevine, S., Finn, C., Darrell, T., and Abbeel, P.\nEnd-\nto-end training of deep visuomotor policies. J. Mach.\nLearn. Res., 17(1):1334–1373, January 2016. ISSN 1532-\n4435.\nURL http://dl.acm.org/citation.\ncfm?id=2946645.2946684.\nLiang, E., Liaw, R., Nishihara, R., Moritz, P., Fox, R., Gold-\nberg, K., Gonzalez, J., Jordan, M., and Stoica, I. Rllib:\nAbstractions for distributed reinforcement learning. In\nInternational Conference on Machine Learning, pp. 3059–\n3068, 2018.\nRLgraph: Modular Computation Graphs for Deep Reinforcement Learning\nMania, H., Guy, A., and Recht, B.\nSimple random\nsearch provides a competitive approach to reinforce-\nment learning.\nCoRR, abs/1803.07055, 2018.\nURL\nhttp://arxiv.org/abs/1803.07055.\nMirhoseini, A., Goldie, A., Pham, H., Steiner, B., Le, Q. V.,\nand Dean, J.\nHierarchical planning for device place-\nment.\n2018.\nURL https://openreview.net/\npdf?id=Hkc-TeZ0W.\nMoldovan, D., Decker, J. M., Wang, F., Johnson, A. A., Lee,\nB. K., Nado, Z., Sculley, D., Rompf, T., and Wiltschko,\nA. B. Autograph: Imperative-style coding with graph-\nbased performance. CoRR, abs/1810.08061, 2018. URL\nhttp://arxiv.org/abs/1810.08061.\nMoritz, P., Nishihara, R., Wang, S., Tumanov, A., Liaw, R.,\nLiang, E., Paul, W., Jordan, M. I., and Stoica, I. Ray:\nA distributed framework for emerging AI applications.\nCoRR, abs/1712.05889, 2017. URL http://arxiv.\norg/abs/1712.05889.\nNagarajan, P., Warnell, G., and Stone, P. The impact of\nnondeterminism on reproducibility in deep reinforcement\nlearning. https://arxiv.org/abs/1809.05676, 2018.\nOpenAI. OpenAI Five DOTA. Website, June 2018. URL\nhttps://blog.openai.com/openai-five/.\nPalkar, S., Thomas, J. J., Shanbhag, A., Narayanan, D.,\nPirk, H., Schwarzkopf, M., Amarasinghe, S., Zaharia,\nM., and InfoLab, S. Weld: A common runtime for high\nperformance data analytics. In Conference on Innovative\nData Systems Research (CIDR), 2017.\nPaszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E.,\nDeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer,\nA. Automatic differentiation in pytorch. In NIPS-W,\n2017.\nPlappert, M.\nkeras-rl.\nhttps://github.com/\nmatthiasplappert/keras-rl, 2016.\nRochel, S. et al.\nGluon - A clear, concise, simple yet\npowerful and efﬁcient API for deep learning. https:\n//github.com/gluon-api/gluon-api, 2018.\nSchaarschmidt, M., Kuhnle, A., Ellis, B., Fricke, K., Gessert,\nF., and Yoneki, E. Lift: Reinforcement learning in com-\nputer systems by learning from demonstrations. CoRR,\nabs/1808.07903, 2018. URL http://arxiv.org/\nabs/1707.06347.\nSeide, F. and Agarwal, A. Cntk: Microsoft’s open-source\ndeep-learning toolkit. In Proceedings of the 22nd ACM\nSIGKDD International Conference on Knowledge Dis-\ncovery and Data Mining, pp. 2135–2135. ACM, 2016.\nSergeev, A. and Balso, M. D.\nHorovod:\nfast and\neasy distributed deep learning in tensorﬂow.\nCoRR,\nabs/1802.05799, 2018. URL http://arxiv.org/\nabs/1802.05799.\nSidor, S. and Schulman, J.\nOpenai baselines.\nweb-\nsite, 2017.\nURL https://blog.openai.com/\nopenai-baselines-dqn/.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,\nVan Den Driessche, G., Schrittwieser, J., Antonoglou, I.,\nPanneershelvam, V., Lanctot, M., et al. Mastering the\ngame of go with deep neural networks and tree search.\nNature, 529(7587):484–489, 2016.\nSilver, D., Schrittwieser, J., Simonyan, K., Antonoglou,\nI., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M.,\nBolton, A., et al. Mastering the game of go without\nhuman knowledge. Nature, 550(7676):354, 2017.\nTang, Y. Tf.learn: Tensorﬂow’s high-level module for dis-\ntributed machine learning. CoRR, abs/1612.04251, 2016.\nURL http://arxiv.org/abs/1612.04251.\nTobin, J., Zaremba, W., and Abbeel, P. Domain randomiza-\ntion and generative models for robotic grasping. CoRR,\nabs/1710.06425, 2017. URL http://arxiv.org/\nabs/1710.06425.\nWayne, G., Hung, C., Amos, D., Mirza, M., Ahuja, A.,\nGrabska-Barwinska, A., Rae, J. W., Mirowski, P., Leibo,\nJ. Z., Santoro, A., Gemici, M., Reynolds, M., Harley,\nT., Abramson, J., Mohamed, S., Rezende, D. J., Saxton,\nD., Cain, A., Hillier, C., Silver, D., Kavukcuoglu, K.,\nBotvinick, M., Hassabis, D., and Lillicrap, T. P. Unsuper-\nvised predictive memory in a goal-directed agent. CoRR,\nabs/1803.10760, 2018. URL http://arxiv.org/\nabs/1803.10760.\nYu, Y., Abadi, M., Barham, P., Brevdo, E., Burrows, M.,\nDavis, A., Dean, J., Ghemawat, S., Harley, T., Hawkins,\nP., Isard, M., Kudlur, M., Monga, R., Murray, D., and\nZheng, X.\nDynamic control ﬂow in large-scale ma-\nchine learning. In Proceedings of the Thirteenth EuroSys\nConference, EuroSys ’18, pp. 18:1–18:15, New York,\nNY, USA, 2018. ACM. ISBN 978-1-4503-5584-1. doi:\n10.1145/3190508.3190551. URL http://doi.acm.\norg/10.1145/3190508.3190551.\nRLgraph: Modular Computation Graphs for Deep Reinforcement Learning\nA\nVISUALIZING COMPUTATION GRAPHS\nA.1\nIMPALA\nTo illustrate how RLgraph helps organize computation\ngraphs, we visualize the IMPALA implementation discussed\nin the evaluation.\nFigure 10 illustrates the RLgraph implementation using Ten-\nsorBoard. As each component’s scope and variables are\nmanaged by RLgraph during the build, device assignments\nand dataﬂow are easy to visualize automatically with exist-\ning tools. Here, green components are on the GPU while\nblue components are on the CPU. The dataﬂow from bottom\nto top clearly illustrates how tensors are moved from the\nshared queue, preprocessed, then moved to a staging area.\nThe prior batch is taken from the staging area, then pre-\nprocessed and passed to the policy and loss function. The\noptimizer and policy interact with a shared scope as learner\nand workers share policy variables.\nMixed colours imply that a component has multiple sub-\ncomponents on different devices. For example, the IMPALA\nloss-function computes an importance correction on the\nCPU as it is difﬁcult to parallelize, so the loss-component\nhas mixed assignments. We found that RLgraph speciﬁ-\ncally helps to make more effective use of existing tools like\nTensorBoard, as clean visualizations are key to identifying\nproblems (e.g. with device assignments).\nIn Figures 11 and 12 (split due to size in TensorBoard), we\nshow for comparison a visualization of DeepMind’s open\nsource IMPALA implementation. As is common for self-\ncontained RL scripts, scopes, devices, and operation names\nare handled on an ad-hoc basis. The resulting graph, while\nimplementing the same logic as our code, is highly frag-\nmented in the visualization, making insights into potential\nproblems much more difﬁcult.\nA.2\nApe-X\nWhen executing on Ray, computation graphs are fragmented\ninto separate tasks represented by Ray actors, as opposed\nto end-to-end differentiable graphs such as in distributed\nTensorFlow (Abadi et al., 2017; Yu et al., 2018). In RLlib,\ntask code is scattered across agent classes, policy graphs,\nand backend-speciﬁc utilities. Understanding dataﬂow be-\ntween components is difﬁcult due to a lack of consistent\nmodularization across imperative function calls. RLgraph’s\nseparation of execution semantics and graph design results\nin consistent graph creation irrespective of the backend used.\nIn Figure 13, we show the corresponding TensorBoard visu-\nalization for RLgraph’s Ape-X learner. Figures 14 and 15\nshow RLlib’s highly fragmented Ape-X graph.\nFigure 10. TensorBoard visualization of RLgraph’s IMPALA\nlearner. As all operations and variables are organized in com-\nponents under separate scopes, dataﬂow between components is\nclear.\nRLgraph: Modular Computation Graphs for Deep Reinforcement Learning\nFigure 11. TensorBoard visualization of DeepMind’s IMPALA learner (left).\nFigure 12. TensorBoard visualization of DeepMind’s IMPALA learner (right).\nRLgraph: Modular Computation Graphs for Deep Reinforcement Learning\nFigure 13. TensorBoard visualization of RLgraph’s Ape-X learner.\nRLgraph: Modular Computation Graphs for Deep Reinforcement Learning\nFigure 14. TensorBoard visualization of RLlib’s Ape-X learner (left).\nFigure 15. TensorBoard visualization of RLlib’s Ape-X learner (right).\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2018-10-21",
  "updated": "2019-02-28"
}