{
  "id": "http://arxiv.org/abs/1810.11505v1",
  "title": "Stability-certified reinforcement learning: A control-theoretic perspective",
  "authors": [
    "Ming Jin",
    "Javad Lavaei"
  ],
  "abstract": "We investigate the important problem of certifying stability of reinforcement\nlearning policies when interconnected with nonlinear dynamical systems. We show\nthat by regulating the input-output gradients of policies, strong guarantees of\nrobust stability can be obtained based on a proposed semidefinite programming\nfeasibility problem. The method is able to certify a large set of stabilizing\ncontrollers by exploiting problem-specific structures; furthermore, we analyze\nand establish its (non)conservatism. Empirical evaluations on two decentralized\ncontrol tasks, namely multi-flight formation and power system frequency\nregulation, demonstrate that the reinforcement learning agents can have high\nperformance within the stability-certified parameter space, and also exhibit\nstable learning behaviors in the long run.",
  "text": "STABILITY-CERTIFIED REINFORCEMENT LEARNING: A\nCONTROL-THEORETIC PERSPECTIVEâˆ—\nMING JINâ€  AND JAVAD LAVAEIâ€¡\nAbstract. We investigate the important problem of certifying stability of reinforcement learning\npolicies when interconnected with nonlinear dynamical systems. We show that by regulating the\ninput-output gradients of policies, strong guarantees of robust stability can be obtained based on a\nproposed semideï¬nite programming feasibility problem. The method is able to certify a large set of\nstabilizing controllers by exploiting problem-speciï¬c structures; furthermore, we analyze and establish\nits (non)conservatism. Empirical evaluations on two decentralized control tasks, namely multi-ï¬‚ight\nformation and power system frequency regulation, demonstrate that the reinforcement learning agents\ncan have high performance within the stability-certiï¬ed parameter space, and also exhibit stable\nlearning behaviors in the long run.\nKey words. Reinforcement learning, robust control, policy gradient optimization, decentralized\ncontrol synthesis, safe reinforcement learning\nAMS subject classiï¬cations. 68T05, 93E35, 93D09\n1. Introduction. Remarkable progress has been made in reinforcement learning\n(RL) using (deep) neural networks to solve complex decision-making and control\nproblems [43]. While RL algorithms, such as policy gradient [52, 26, 41], Q-learning\n[49, 35], and actor-critic methods [32, 34] aim at optimizing control performance, the\nsecurity aspect is of great importance for mission-critical systems, such as autonomous\ncars and power grids [20, 4, 44]. A fundamental problem is to analyze or certify\nstability of the interconnected system in both RL exploration and deployment stages,\nwhich is challenging due to its dynamic and nonconvex nature [20].\nThe problem under study focuses on a general continuous-time dynamical system:\n(1.1)\nË™x(t) = ft(x(t), u(t)),\nwith the state x(t) âˆˆRns and the control action u(t) âˆˆRna. In general, ft can be\na time-varying and nonlinear function, but for the purpose of stability analysis, we\nstudy the important case that\n(1.2)\nft(x(t)) = Ax(t) + Bu(t) + gt(x(t)),\nwhere ft comprises of a linear time-invariant (LTI) component A âˆˆRnsÃ—ns that is\nHurwitz (i.e., every eigenvalue of A has strictly negative real part), a control matrix\nB âˆˆRnsÃ—na, and a slowly time-varying component gt that is allowed to be nonlinear\nand even uncertain.1 The condition that A is stable is a basic requirement, but the\ngoal of reinforcement learning is to design a controller that optimizes some performance\nmetric that is not necessarily related to the stability condition. For feedback control,\nwe also allow the controller to obtain observations y(t) = Cx(t) âˆˆRns that are a linear\nfunction of the states, where C âˆˆRnsÃ—ns may have a sparsity pattern to account for\npartial observations in the context of decentralized controls [8].\nâˆ—This work was supported by the ONR grants N00014-17-1-2933 and N00014-15-1-2835, DARPA\ngrant D16AP00002, and AFOSR grant FA9550-17-1-0163.\nâ€ Department of Industrial Engineering and Operations Research, University of California, Berkeley.\nEmail: jinming@berkeley.edu.\nâ€¡Department of Industrial Engineering and Operations Research, and the Tsinghua-Berkeley\nShenzhen Institute, University of California, Berkeley. Email: lavaei@berkeley.edu.\n1This requirement is not diï¬ƒcult to meet in practice, because one can linearize any nonlinear\nsystems around the equilibrium point to obtain a linear component and a nonlinear part.\n1\narXiv:1810.11505v1  [cs.SY]  26 Oct 2018\n2\nM. JIN AND J. LAVAEI\nEnvironment\nğº\nğ‘¥\nRL policy ğœ‹(ğ‘¦; ğœƒ)\nReward ğ‘Ÿ\nAction ğ‘¤\n+\nInput ğ‘¢\nOutput ğ‘¦\nExploration ğ‘’\nFig. 1: Overview of the interconnected system of an RL policy and the environment.\nThe goal of RL is to maximize expected rewards through interaction and exploration.\nSuppose that u(t) = Ï€t(y(t); Î¸t) + e(t) is a neural network given by an RL agent\n(parametrized by Î¸t, which can be time-varying due to learning) to optimize some\nreward r(x, u) revealed through the interaction with the environment. The exploration\nvector e(t) âˆˆRna captures the additive randomization eï¬€ect during the learning phase,\nand is assumed to have a bounded energy over time (âˆ¥eâˆ¥2 =\nqR\n|e(t)|2\n2dt â‰¤âˆ). The\nmain goal is to analyze the stability of the system with the actuation of Ï€t, which\nis typically a neural network controller, as illustrated in Figure 1. Speciï¬cally, the\nstability criterion is stated using the concept of L2 gain [55, 16].2\nDefinition 1.1 (Input-output stability). The L2 gain of the system G controlled\nby Ï€ is the worst-case ratio between total output energy and total input energy:\n(1.3)\nÎ³(G, Ï€) = sup\nuâˆˆL2\nâˆ¥yâˆ¥2\nâˆ¥uâˆ¥2\n,\nwhere L2 is the set of all square-summable signals, âˆ¥yâˆ¥2 =\nqR\n|y(t)|2\n2dt is the total\nenergy over time, and u(t) = Ï€t(y(t); Î¸t) + e(t) is the control input with exploration. If\nÎ³(G, Ï€) is ï¬nite, then the interconnected system is said to have input-output stability\n(or ï¬nite L2 gain).\nThis study investigates the possibility of using the gradient information of the\npolicy Ï€t(y(t); Î¸t) to obtain a stability certiï¬cate, because this information can be\neasily extracted in real-time and is generic enough to include a large set of performance-\noptimizing nonlinear controllers. Let [n] = {1, ..., n} be the set notation. By denoting\n(1.4)\nP(Î¾) =\nn\nÏ€\n\f\f\f Î¾ij â‰¤âˆ‚jÏ€i(y) â‰¤Î¾ij, âˆ€i âˆˆ[na], j âˆˆ[ns], y âˆˆRnso\nas the set of controllers whose partial derivatives are bounded by Î¾ âˆˆRnaÃ—ns and\nÎ¾ âˆˆRnaÃ—ns, it is desirable to provide stability certiï¬cate as long as the RL policy\nremains within the above â€œsafety set.â€ Indeed, this can be checked eï¬ƒciently, as stated\n(informally) in the following theorem.\n2This stability metric is widely adopted in practice, and is closely related to bounded-input\nbounded-output (BIBO) stability and absolute stability (or asymptotic stability). For controllable\nand observable LTI systems, the equivalence can be established.\nSTABILITY-CERTIFIED REINFORCEMENT LEARNING\n3\nTheorem 1.2 (Main result). If there exist constants Î¾ and Î¾ such that the condi-\ntion (3.21) is feasible for the system (1.1), then the interconnected system has a ï¬nite\nL2 gain as long as Ï€t âˆˆP(Î¾, Î¾) for all t â‰¥0.\nWe call the constants Î¾ and Î¾ stability-certiï¬ed gradient bounds for the underlying\nsystem. The above result is based on the intuition that a real-world stable controller\nshould exhibit â€œsmoothnessâ€ in the sense that small changes in the input should lead\nto small changes in the output. This incorporates the special case where controllers\nare known to have bounded Lipschitz constants (a simple strategy to calculate the\nLipschitz constant of a deep neural network is suggested in [48]). To compute the\ngradient bounds, we borrow powerful ideas from the framework of integral quadratic\nconstraint (in frequency domain) [33] and dissipativity theory (in time domain) [51]\nfor robustness analysis. While these tools are celebrated with their non-conservatism\nin the robust control literature, existing characterizations of multi-input multi-output\n(MIMO) Lipschitz functions are insuï¬ƒcient. Thus, one major obstacle is to derive\nnon-trivial bounds that could be of use in practice.\nTo this end, we develop a new quadratic constraint on gradient-bounded functions,\nwhich exploits the sparsity of the control architecture and the non-homogeneity of\nthe output vector. Some key features of the stability-certiï¬ed smoothness bounds\nare as follows: (a) the bounds are inherent to the targeted real-world control task;\n(b) they can be computed eï¬ƒciently by solving some semi-deï¬nite programming\n(SDP) problem; (c) they can be used to certify stability when reinforcement learning\nis employed in real-world control with either oï¬€-policy or on-policy learning [47].\nFurthermore, the stability certiï¬cation can be regarded as an S-procedure, and we\nanalyze its conservatism to show that it is necessary for the robustness of a surrogate\nsystem that is closely related to the original system.\nThe paper is organized as follows. Preliminaries on policy gradient reinforcement\nlearning, the integrated quadratic constraint (IQC) and dissipativity frameworks are\npresented in Section 2. Main results on gradient bounds for a linear or nonlinear\nsystem G are presented in Section 3, where we also analyze the conservatism of the\ncertiï¬cate. The method is evaluated in Section 4 on two nonlinear decentralized control\ntasks. Conclusions are drawn in Section 5.\n2. Preliminary. In this section, we give an overview of the main topics relevant\nto this study, namely policy gradient reinforcement learning and robustness analysis\nbased on IQC framework and dissipativity theory.\n2.1. Reinforcement learning using policy gradient. Reinforcement learning\naims at guiding an agent to perform a task as eï¬ƒciently and skillfully as possible\nthrough interactions with the environment. The control task is modeled as a Markov\ndecision process (MDP), deï¬ned by the tuple (X, U, T , r, Ï), where X is the set of\nstates x, U is a set of actions u, T : X Ã— U â†’X indicates the world dynamics as\nin (1.1), r(x, u) is the reward at state x and action u, and Ï âˆˆ(0, 1] is the factor to\ndiscount future rewards. A control strategy is deï¬ned by a policy Ï€Î¸(x), which can\nbe approximated by a neural network with parameters Î¸. For a continuous control,\nthe actions follow a multivariate normal distribution, where Ï€Î¸(x) is the mean, and\nthe standard deviation in each action dimension is set to be a diminishing number\nduring exploration or learning, and 0 during actual deployment. With a slight abuse\nof notations, we use Ï€Î¸(u|x) to denote this normal distribution over actions, and use\n4\nM. JIN AND J. LAVAEI\nxt to denote x(t) for simplicity. The goal of RL is to maximize the expected return:\n(2.1)\nÎ·(Ï€Î¸) =\nE\nx0,utâˆ¼Ï€Î¸(Â·|xt),xt+1âˆ¼T (xt,ut)\n\u0014XT\nt=0 Ïtr(xt, ut)\n\u0015\n,\nwhere T is the control horizon, and the expectation is taken over the policy, the initial\nstate distribution and the world dynamics.\nFrom a practitionerâ€™s point of view, the existing methods can be categorized\ninto four groups based on how the optimal policy is determined: (a) policy gradient\nmethods directly optimize the policy parameters Î¸ by estimating the gradient of the\nexpected return (e.g., REINFORCE [52], natural policy gradient [26], and trust region\npolicy optimization (TRPO) [41]); (b) value-based algorithms like Q-learning do not\naim at optimizing the policy directly, but instead approximate the Q-value of the\noptimal policy for the available actions [49, 35]; (c) actor-critic algorithms keep an\nestimate of the value function (critic) as well as a policy that maximizes the value\nfunction (actor) (e.g., DDPG [32] and A3C [34]); lastly, (d) model-based methods\nfocus on the learning of the transition model for the underlying dynamics, and then use\nit for planning or to improve a policy (e.g., Dyna [46] and guided policy search [30]).\nWe adopt an approach based on end-to-end policy gradient that combines TRPO [41]\nwith natural gradient [26] and smoothness penalty (this method is very useful for RL\nin dynamical systems described by partial or diï¬€erence equations).\nTrust region policy optimization is a policy gradient method that constrains\nthe step length to be within a â€œtrust regionâ€ so that the local estimation of the\ngradient/curvature has a monotonic improvement guarantee. By manipulating the\nexpected return Î·(Ï€) using the identity proposed in [25], the â€œsurrogate objectiveâ€\nLÏ€old(Ï€) can be designed:\n(2.2)\nLÏ€old(Ï€) =\nE\nx,uâˆ¼Ï€old\n\u0014 Ï€(u|x)\nÏ€old(u|x)Î›Ï€old(x, u)\n\u0015\n,\nwhere the expectation is taken over the old policy Ï€old, the ratio inside the expectation\nis also known as the importance weight, and Î›Ï€old(x, u) is the advantage function\ngiven by:\n(2.3)\nÎ›Ï€old(x, u) =\nE\nxâ€²âˆ¼T (x,u) [r(x, u) + ÏV Ï€old(xâ€²) âˆ’V Ï€old(x)] ,\nwhere the expectation is with respect to the dynamics xâ€² âˆ¼T (x, u) (the dependence\non Î¸old is omitted), and it measures the improvement of taking action u at state x\nover the old policy in terms of the value function V Ï€old. A bound on the diï¬€erence\nbetween LÏ€old(Ï€) and Î·(Ï€) has been derived in [41], which also proves a monotonic\nimprovement result as long as the KL divergence between the new and old policies is\nsmall (i.e., the new policy stays within the trust region). In practice, the surrogate\nloss LÏ€old(Ï€) can be estimated using trajectories sampled from Ï€old as follows,\n(2.4)\nbLÏ€old(Ï€) =\nX\nt\nÏ€(ut|xt)\nÏ€old(ut|xt)\nbÎ›Ï€old(x, u),\nand the averaged KL divergence over observed states 1\nT\nP\nt KL [Ï€old(Â·|xt), Ï€(Â·|xt)] can\nbe used to estimate the trust region.\nNatural gradient is deï¬ned by a metric based on the probability manifold\ninduced by the KL divergence. It improves the standard gradient by making a step\nSTABILITY-CERTIFIED REINFORCEMENT LEARNING\n5\ninvariant to reparametrization of the parameter coordinates [3]:\n(2.5)\nÎ¸t+1 â†Î¸t âˆ’Î»Hâˆ’1\nÎ¸ Î¶t,\nwhere Î¶t is the standard gradient, HÎ¸ =\n1\nT\nP\nt\n\u0000 âˆ‚\nâˆ‚Î¸Ï€Î¸(log ut|xt)\n\u0001 \u0000 âˆ‚\nâˆ‚Î¸ log Ï€Î¸(ut|xt)\n\u0001âŠ¤\nis the Fisher information matrix estimated with the trajectory data, and Î» is the\nstep size. In practice, when the number of parameters is large, conjugate gradient is\nemployed to estimate the term Hâˆ’1\nÎ¸ Î¶t without requiring any matrix inversion. Since\nthe Fisher information matrix coincides with the second-order approximation of the\nKL divergence, one can perform a back-tracking line search on the step size Î» to ensure\nthat the updated policy stays within the trust region.\nSmoothness penalty is introduced in this study to empirically improve learning\nperformance on physical dynamical systems. Speciï¬cally, we propose to use\n(2.6)\nLexplore =\nXT\nt=1 âˆ¥utâˆ’1 âˆ’Ï€Î¸(xt)âˆ¥2\nas a regularization term to induce consistency during exploration. The intuition is\nthat since the change in states between two consecutive time steps is often small, it is\ndesirable to ensure small changes in output actions. This is closely related to another\npenalty term that has been used in [15], which is termed â€œdouble backpropagationâ€,\nand recently rediscovered in [37, 22]:\n(2.7)\nLsmooth =\nXT\nt=1\n\r\r\r\r\nâˆ‚\nâˆ‚Î¸Ï€Î¸(xt)\n\r\r\r\r\n2\n,\nwhich penalizes the gradient of the policy along the trajectories. Since bounded\ngradients lead to bounded Lipshitz constant, these penalties will induce smooth neural\nnetwork functions, which is essential to ensure generalizability and, as we will show,\nstability. In addition, we incorporate a hard threshold (HT) approach that rescales\nthe weight matrices at each layer by (lâ—¦/l(Ï€Î¸))1/nL if l(Ï€Î¸) > lâ—¦, where l(Ï€Î¸) is the\nLipschitz constant of the neural network Ï€Î¸, nL is the number of layers of the neural\nnetwork and lâ—¦is the certiï¬ed Lipschitz constant. This ensures that the Lipschitz\nconstant of the RL policy remains bounded by lâ—¦.\nIn summary, our policy gradient is based on the weighted objective:\n(2.8)\nLpol(Ï€Î¸) = bLÏ€old(Ï€Î¸) + w1Lexplore(Ï€Î¸) + w2Lsmooth(Ï€Î¸),\nwhere the penalty coeï¬ƒcients w1 and w2 are selected such that the scales of the\ncorresponding terms are about [0.01, 0.05] of the surrogate loss value bLÏ€old(Ï€Î¸). In\neach round, a set of trajectories are collected using Ï€old, which are used to estimate\nthe gradient\nâˆ‚\nâˆ‚Î¸Lpol(Ï€Î¸) and the Fisher information matrix HÎ¸; a backtracking line\nsearch on the step size is then conducted to ensure that the updated policy stays\nwithin the trust region. This learning procedure is known as on-policy learning [47].\n2.2. Overview of IQC framework. The IQC theory is celebrated for sys-\ntematic and eï¬ƒcient stability analysis of a large class of uncertain, dynamic, and\ninterconnected systems [33]. It uniï¬es and extends classical passitivity-based multiplier\ntheory, and has close connections to dissipativity theory in the time domain [42].\nTo state the IQC framework, some terminologies are necessary. We deï¬ne the\nspace Ln\n2[0, âˆ) = {x :\nR âˆ\nt=0 |x(t)|2\n2dt < âˆ} for signals supported on t â‰¥0, where\nn denotes the spatial dimension of x(t), and the extended space Ln\n2e[0, âˆ) = {x :\n6\nM. JIN AND J. LAVAEI\nR T\nt=0 |x(t)|2\n2dt < âˆ, âˆ€T â‰¥0} (we will use L2 and L2e if it is not necessary to specify\nthe dimension and signal support), where we use x to denote the signal in general\nand x(t) to denote its value at time t. For a vector or matrix, we use superscript âˆ—to\ndenote its conjugate transpose. An operator is causal if the current output does not\ndepend on future inputs. It is bounded if it has a ï¬nite L2 gain. Let Î¦ : H â†’H be a\nbounded linear operator on a Hilbert space. Then, its Hilbert adjoint is the operator\nÎ¦âˆ—: H â†’H such that âŸ¨Î¦x, yâŸ©= âŸ¨x, Î¦âˆ—yâŸ©for all x, y âˆˆH, where âŸ¨Â·, Â·âŸ©denotes the\ninner product. It is self-adjoint if Î¦ = Î¦âˆ—.\nConsider the system (see also Figure 1)\ny = G(u)\n(2.9)\nu = âˆ†(y) + e,\n(2.10)\nwhere G is the transfer function of a causal and bounded LTI system (i.e., it maps input\nu âˆˆLna to output y âˆˆLno through the internal state dynamics Ë™x = Ax(t) + Bu(t)),\ne âˆˆLna is the disturbance, and âˆ†: Lno â†’Lna is a bounded and causal function that\nis used to represent uncertainties in the system. IQC provides a framework to treat\nuncertainties such as nonlinear dynamics, model approximation and identiï¬cation\nerrors, time-varying parameters and disturbance noise, by using their input-output\ncharacterizations.\nDefinition 2.1 (Integral quadratic constraints). Consider the signals w âˆˆL2\nand y âˆˆL2 associated with Fourier transforms Ë†w and Ë†y, and w = âˆ†(y), where âˆ†is a\nbounded and causal operator. We present both the frequency- and time-domain IQC\ndeï¬nitions:\n(a) (Frequency domain) Let Î  be a bounded and self-adjoint operator. Then, âˆ†is\nsaid to satisfy the IQC deï¬ned by Î  (i.e., âˆ†âˆˆIQC(Î )) if:\n(2.11)\nÏƒÎ (Ë†y, Ë†w) =\nZ âˆ\nâˆ’âˆ\n\u0014\nË†y(jÏ‰)\nË†w(jÏ‰)\n\u0015âˆ—\nÎ (jÏ‰)\n\u0014 Ë†y(jÏ‰)\nË†w(jÏ‰)\n\u0015\ndÏ‰ â‰¥0.\n(b) (Time domain) Let (Î¨, M) be any factorization of Î  = Î¨âˆ—MÎ¨ such that Î¨\nis stable and M = M âŠ¤. Then, âˆ†is said to satisfy the hard IQC deï¬ned by\n(Î¨, M) (i.e., âˆ†âˆˆIQC(Î¨, M)) if:\n(2.12)\nZ T\n0\nz(t)âŠ¤Mz(t)dt â‰¥0,\nâˆ€T â‰¥0,\nwhere z = Î¨\n\u0014y\nw\n\u0015\nis the ï¬ltered output given by the stable operator Î¨. If instead\nof requiring nonnegativity at each time T, the nonnegativity is considered only\nwhen T â†’âˆ, then the corresponding condition is called soft IQC.\nAs established in [42], the time- and frequency-domain IQC deï¬nitions are equiva-\nlent if there exists Î  = Î¨âˆ—MÎ¨ as a spectral factorization of Î  with M =\n\u0014\nI\n0\n0\nâˆ’I\n\u0015\nsuch that Î¨ and Î¨âˆ’1 are stable.\nExample 2.2 (Sector IQC). A single-input single-output uncertainty âˆ†: R â†’R\nis called â€œsector boundedâ€ between [Î±, Î²] if Î±y(t) â‰¤âˆ†(y(t)) â‰¤Î²y(t), for all y âˆˆR and\nt â‰¥0. It thus satisï¬es the sector IQC(Î¨, M) with Î¨ = I and M =\n\u0014âˆ’2Î±Î²\nÎ± + Î²\nÎ± + Î²\nâˆ’2\n\u0015\n.\nIt also satisï¬es IQC(Î ) with Î  = M deï¬ned above.\nSTABILITY-CERTIFIED REINFORCEMENT LEARNING\n7\nExample 2.3 (L2 gain bound). A MIMO uncertainty âˆ†: Rn â†’Rm has the L2\ngain Î³ if\nR âˆ\n0\nâˆ¥w(t)âˆ¥2dt â‰¤Î³2 R âˆ\n0\nâˆ¥y(t)âˆ¥2dt, where w(t) = âˆ†(y(t)). Thus, it satisï¬es\nIQC(Î¨, M) with Î¨ = In+m and M =\n\u0014Î»Î³2In\n0\n0\nâˆ’Î»Im\n\u0015\n, where Î» > 0. It also satisï¬es\nIQC(Î ) with Î  = M deï¬ned above.\nThis can be used to characterize nonlinear\noperators with fast time-varying parameters.\nBefore stating a stability result, we deï¬ne the system (2.9)â€“(2.10) (see Figure 1)\nto be well-posed if for any e âˆˆL2e, there exists a solution u âˆˆL2e, which depends\ncausally on e. A main IQC result for stability is stated below:\nTheorem 2.4 ([33]). Consider the interconnected system (2.9)â€“(2.10). Assume\nthat: (i) the interconnected system (G, Ï„âˆ†) is well posed for all Ï„ âˆˆ[0, 1]; (ii)\nÏ„âˆ†âˆˆIQC(Î ) for Ï„ âˆˆ[0, 1]; and (iii) there exists Ïµ > 0 such that\n(2.13)\n\u0014 Ë†G(jÏ‰)\nI(jÏ‰)\n\u0015âˆ—\nÎ (jÏ‰)\n\u0014 Ë†G(jÏ‰)\nI(jÏ‰)\n\u0015\nâ‰¤âˆ’ÏµI,\nâˆ€Ï‰ âˆˆ[0, âˆ).\nThen, the system (2.9)â€“(2.10) is input-output stable (i.e., ï¬nite L2 gain).\nThe above theorem requires three technical conditions. The well-posedness condi-\ntion is a generic property for any acceptable model of a physical system. The second\ncondition is implied if Î  =\n\u0014Î 11\nÎ 12\nÎ âˆ—\n12\nÎ 22\n\u0015\nhas the properties Î 11 âª°0 and Î 22 âª¯0. The\nthird condition is central, and it requires checking the feasibility at every frequency,\nwhich represents a main obstacle. As discussed in Section Section 3.2, this condition\ncan be equivalently represented as a linear matrix inequality (LMI) using the Kalman-\nYakubovich-Popov (KYP) lemma. In general, the more IQCs exist for the uncertainty,\nthe better characterization can be obtained. If âˆ†âˆˆIQC(Î k), k âˆˆ[nK], where nK is\nthe number of IQCs satisï¬ed by âˆ†, then it is easy to show that âˆ†âˆˆIQC(PnK\nk=1 Ï„kÎ k),\nwhere Ï„k â‰¥0; thus, the stability test (2.13) becomes a convex program, i.e., to ï¬nd\nÏ„k â‰¥0 such that:\n(2.14)\n\u0014 Ë†G(jÏ‰)\nI(jÏ‰)\n\u0015âˆ— nK\nX\nk=1\nÏ„kÎ k(jÏ‰)\n! \u0014 Ë†G(jÏ‰)\nI(jÏ‰)\n\u0015\nâ‰¤âˆ’ÏµI, âˆ€Ï‰ âˆˆ[0, âˆ).\nThe counterpart for the frequency-domain stability condition in the time-domain\ncan be stated using a standard dissipation argument [42].\n2.3. Related work. To close this section, we summarize some connections\nto existing literature. This work is closely related to the body of works on safe\nreinforcement learning, deï¬ned as the process of learning policies that maximize\nperformance in problems where safety is required during the learning and/or deployment\n[20]. A detailed literature review can be found in [20], which has categorized two\nmain approaches by modifying: (1) the optimality condition with a safety factor,\nand (2) the exploration process to incorporate external knowledge or risk metrics.\nRisk-aversion can be speciï¬ed in the reward function, for example, by deï¬ning risk\nas the probability of reaching a set of unknown states in a discrete Markov decision\nprocess setting [14, 21]. Robust MDP is designed to maximize rewards while safely\nexploring the discrete state space [36, 50]. For continuous states and actions, robust\nmodel predictive control can be employed to ensure robustness and safety constraints\nfor the learned model with bounded errrors [7]. These methods require an accurate\nor estimated models for policy learning. Recently, model-free policy optimization\n8\nM. JIN AND J. LAVAEI\nhas been successfully demonstrated in real-world tasks such as robotics, business\nmanagement, smart grid and transportation [31]. Safety requirement is high in these\nsettings. Existing approaches are based on constraint satisfaction that holds with high\nprobability [45, 1].\nThe present analysis tackles the safe reinforcement learning problem from a robust\ncontrol perspective, which is aimed at providing theoretical guarantees for stability [55].\nLyapunov functions are widely used to analyze and verify stability when the system\nand its controller are known [39, 10]. For nonlinear systems without global convergence\nguarantees, region of convergence is often estimated, where any state trajectory that\nstarts within this region stays within the region for all times and converges to a\ntarget state eventually [27]. For example, recently, [9] has proposed a learning-based\nLyapunov stability veriï¬cation for physical systems, whose dynamics are sequentially\nestimated by Gaussian processes. In the same vein, [2] has employed reachability\nanalysis to construct safe regions in the state space by solving a partial diï¬€erential\nequation. The main challenge of these methods is to ï¬nd a suitable non-conservative\nLyapunov function to conduct the analysis.\nThe IQC framework proposed in [33] has been widely used to analyze the stability\nof large-scale complex systems such as aircraft control [19]. The main advantages of\nIQC are its computational eï¬ƒciency, non-conservatism, and uniï¬ed treatment of a\nvariety of nonlinearities and uncertainties. It has also been employed to analyze the\nstability of small-sized neural networks in reinforcement learning [28, 5]; however, in\ntheir analysis, the exact coeï¬ƒcients of the neural network need to be known a priori for\nthe static stability analysis, and a region of safe coeï¬ƒcients needs to be calculated at\neach iteration for the dynamic stability analysis. This is computationally intensive, and\nit quickly becomes intractable when the neural network size grows. On the contrary,\nbecause the present analysis is based on a broad characterization of control functions\nwith bounded gradients, it does not need to access the coeï¬ƒcients of the neural network\n(or any forms of the controller). In general, robust analysis using advanced methods\nsuch as structured singular value [38] or IQC can be conservative. There are only few\ncases where the necessity conditions can be established, such as when the uncertain\noperator has a block diagonal structure of bounded singular values [16], but this\nset of uncertainties is much smaller than the set of performance-oriented controllers\nlearned by RL. To this end, we are able to reduce conservatism of the results by\nintroducing more informative quadratic constraints for those controllers, and analyze\nthe necessity of the certiï¬cate criteria. This signiï¬cantly extends the possibilities of\nstability-certiï¬ed reinforcement learning to large and deep neural networks in nonlinear\nlarge-scale real-world systems, whose stability is otherwise impossible to be certiï¬ed\nusing existing approaches.\n3. Main results. This section will introduce a set of quadratic constraints on\ngradient-bounded functions, describe the computation of a smoothness margin for\nlinear (Theorem 3.3) and nonlinear systems (Theorem 3.4). Furthermore, we examine\nthe conservatism of the certiï¬cate condition in Theorem 3.3 for linear systems.\n3.1. Quadratic constraints on gradient-bounded functions. The starting\npoint of this analysis is a less conservative constraint on general vector-valued functions.\nWe start by recalling the deï¬nition of a Lipschitz continuous function:\nDefinition 3.1 (Lipschitz continuous function). We deï¬ne both the local and\nglobal versions of the Lipschitz continuity for a function f : Rn â†’Rm:\n(a) The function f is locally Lipschitz continuous on the open subset B if there\nSTABILITY-CERTIFIED REINFORCEMENT LEARNING\n9\nexists a constant Î¾ > 0 (i.e., Lipschitz constant of f on B) such that\n(3.1)\n|f(x) âˆ’f(y)| â‰¤Î¾|x âˆ’y|,\nâˆ€x, y âˆˆB.\n(b) If f is Lipschitz continuous on Rn with a constant Î¾ (i.e., B = Rn in (3.1)),\nthen f is called globally Lipschitz continuous with the Lipschitz constant Î¾.\nLipschitz continuity implies uniform continuity. The above deï¬nition also establishes\na connection between locally and globally Lipschitz continuity. The norm | Â· | in the\ndeï¬nition can be any norm, but the Lipschitz constant depends on the particular choice\nof the norm. Unless otherwise stated, we use the Euclidean norm in our analysis.\nTo explore some useful properties of Lipschitz continuity, consider a scalar-valued\nfunction (i.e., m = 1). Let h(j)\nxy =\n\u0002y1, y2, . . . , yj, xj+1, . . . , xn\n\u0003âŠ¤âˆˆRn denote a hybrid\nvector between x and y, with h(0)\nxy = x and h(n)\nxy = y. Then, local Lipschitz continuity\nof f : Rn â†’R on B implies that\n(3.2)\n|f(h(j)\nxy ) âˆ’f(h(jâˆ’1)\nxy\n)|\n|xj âˆ’yj|\nâ‰¤Î¾,\nâˆ€x, y âˆˆB, xj Ì¸= yj, j âˆˆ[n].\nIf we were to assume that f is diï¬€erentiable, then it follows that its (partial) derivative\nis bounded by the Lipschitz constant. For a vector-valued function f =\n\u0002f1, . . . , fm\n\u0003âŠ¤\nthat is Î¾-Lipschitz, it is necessary that every component fi be Î¾-Lipschitz. In general,\nevery continuously diï¬€erentiable function is locally Lipschitz, but the reverse is not true,\nsince the deï¬nition of Lipschitz continuity does not require diï¬€erentiability. Indeed,\nby the Rademacherâ€™s theorem, if f is locally Lipschitz on B, then it is diï¬€erentiable at\nalmost every point in B [13].\nFor the purpose of stability analysis, we can express (3.1) as a point-wise quadratic\nconstraint:\n(3.3)\n\u0014\nx âˆ’y\nf(x) âˆ’f(y)\n\u0015âŠ¤\u0014\nÎ¾2In\n0\n0\nâˆ’Im\n\u0015 \u0014\nx âˆ’y\nf(x) âˆ’f(y)\n\u0015\nâ‰¥0,\nâˆ€x, y âˆˆB.\nThe above constraint, nevertheless, can be sometimes too conservative, because it\ndoes not explore the structure of a given problems. To elaborate on this, consider the\nfunction f : R2 â†’R2 deï¬ned as\n(3.4)\nf(x1, x2) =\n\u0002tanh(0.5x1) âˆ’ax1, sin(x2)\u0003âŠ¤,\nwhere x1, x2 âˆˆR and |a| â‰¤0.1 is a deterministic but unknown parameter with a\nbounded magnitude. Clearly, to satisfy (3.1) on R2 for all possible tuples (a, x1, x2),\nwe need to choose Î¾ â‰¥1 (i.e., the function has the Lipshitz constant 1). However, this\ncharacterization is too general in this case, because it ignores the non-homogeneity\nof f1 and f2, as well as the sparsity of the problem representation. Indeed, f1 only\ndepends on x1 with its slope restricted to [âˆ’0.1, 0.6] for all possible |a| â‰¤0.1, and f2\nonly depends on x2 with its slope restricted to [âˆ’1, 1]. In the context of controller\ndesign, the non-homogeneity of control outputs often arises from physical constraints\nand domain knowledge, and the sparsity of control architecture is inherent in scenarios\nwith distributed local information. To explicitly address these requirements, we state\nthe following quadratic constraint.\nLemma 3.2. For a vector-valued function f : Rn â†’Rm that is diï¬€erentiable with\nbounded partial derivatives on B (i.e., Î¾ij â‰¤âˆ‚jfi(x) â‰¤Î¾ij for all x âˆˆB), the following\n10\nM. JIN AND J. LAVAEI\nquadratic constraint is satisï¬ed for all Î»ij â‰¥0, i âˆˆ[m], j âˆˆ[n], and x, y âˆˆB:\n(3.5)\n\u0014 x âˆ’y\nq(x, y)\n\u0015âŠ¤\nM(Î»; Î¾)\n\u0014 x âˆ’y\nq(x, y)\n\u0015\nâ‰¥0,\nwhere M(Î»; Î¾) is given by\n(3.6)\n\u0014diag\n\u0000\bP\ni Î»ij(c2\nij âˆ’c2\nij)\n\t\u0001\nU({Î»ij, cij})âŠ¤\nU({Î»ij, cij})\ndiag ({âˆ’Î»ij})\n\u0015\n,\nwhere diag(x) denotes a diagonal matrix with diagonal entries speciï¬ed by x, and\nq(x, y) =\n\u0002q11, . . . , q1n, . . . , qm1, . . . , qmn\n\u0003âŠ¤is determined by x and y, {âˆ’Î»ij} is a set\nof non-negative multipliers that follow the same index order as q, U({Î»ij, cij}) =\n\u0002diag ({âˆ’Î»1jc1j})\nÂ· Â· Â·\ndiag ({âˆ’Î»mjcmj})\u0003\nâˆˆRnÃ—mn, cij =\n1\n2\n\u0010\nÎ¾ij + Î¾ij\n\u0011\n, cij =\nÎ¾ij âˆ’cij, and q is related to the output of f by the constraint:\n(3.7)\nf(x) âˆ’f(y) =\n\u0002Im âŠ—11Ã—n\n\u0003\nq = Wq,\nwhere âŠ—denotes the Kronecker product.\nProof. For a vector-valued function f : Rn â†’Rm that is diï¬€erentiable with\nbounded partial derivatives on B (i.e., Î¾ij â‰¤âˆ‚jfi(x) â‰¤Î¾ij for all x âˆˆB), there exist\nfunctions Î´ij : Rn Ã— Rn â†’R bounded by Î¾ij â‰¤Î´ij(x, y) â‰¤Î¾ij for all i âˆˆ[m] and\nj âˆˆ[n] such that\n(3.8)\nf(x) âˆ’f(y) =\nï£®\nï£¯ï£°\nPn\nj=1 Î´1j(x, y)(xj âˆ’yj)\n...\nPn\nj=1 Î´mj(x, y)(xj âˆ’yj)\nï£¹\nï£ºï£».\nBy deï¬ning qij = Î´ij(x, y)(xj âˆ’yj), since (Î´ij(x, y) âˆ’cij)2 â‰¤c2\nij, it follows that\n(3.9)\n\u0014\nxj âˆ’yj\nqij\n\u0015âŠ¤\u0014c2\nij âˆ’c2\nij\ncij\ncij\nâˆ’1\n\u0015 \u0002â‹†\u0003\nâ‰¥0.\nThe result follows by introducing nonnegative multipliers Î»ij â‰¥0, and the fact that\nfi(x) âˆ’fi(y) = Pm\nj=1 qij.\nThis above bound is a direct consequence of standard tools in real analysis [54]. To\nunderstand this result, it can be observed that (3.5) is equivalent to:\n(3.10)\nX\ni,j\nÎ»ij\n\u0010\n(c2\nij âˆ’c2\nij)(xj âˆ’yj)2 + 2cijqij(xj âˆ’yj) âˆ’q2\nij\n\u0011\nâ‰¥0,\nâˆ€Î»ij â‰¥0,\nwith fi(x) âˆ’fi(y) = Pn\nj=1 qij, where qij depends on x and y. Since (3.10) holds for all\nÎ»ij â‰¥0, it is equivalent to the condition that (c2\nijâˆ’c2\nij)(xjâˆ’yj)2+2cijqij(xjâˆ’yj)âˆ’q2\nij â‰¥\n0 for all i âˆˆ[m] and j âˆˆ[n], which is a direct result of the bounds imposed on\nthe partial derivatives of fi. To illustrate its usage, let us apply the constraint to\ncharacterize the example function (3.4), where Î¾11 = âˆ’0.1, Î¾11 = 0.6, Î¾22 = âˆ’1, Î¾22 = 1,\nand all the other bounds (Î¾12, Î¾12, Î¾21, Î¾21) are zero.\nThis clearly yields a more\ninformative constraint than merely relying on the Lipschitz constraint (3.3). In fact,\nSTABILITY-CERTIFIED REINFORCEMENT LEARNING\n11\nfor a diï¬€erentiable l-Lipschitz function, we have Î¾ij = âˆ’Î¾ij = l, and by limiting the\nchoice of Î»ij =\n(\nÎ»\nif i = 1\n0\nif i Ì¸= 1, (3.10) is reduced to (3.3). However, as illustrated in this\nexample, the quadratic constraint in Lemma 3.2 can incorporate richer information\nabout the structure of the problem; therefore, it often gives rise to non-trivial stability\nbounds in practice.\nThe constraint introduced above is not a classical IQC, since it involves an\nintermediate variable q that relates to the output f through a set of linear equalities.\nFor stability analysis, let y = xâˆ—âˆˆB be the equilibrium point, and without loss of\ngenerality, assume that xâˆ—= 0 and f(xâˆ—) = 0. Then, one can deï¬ne the quadratic\nfunctions\nÏ†ij(x, q) = (c2\nij âˆ’c2\nij)x2\nj + 2cijqijxj âˆ’q2\nij,\nand the condition (3.5) can be written as\n(3.11)\nX\nij\nÎ»ijÏ†ij(x, q) â‰¥0,\nâˆ€Î»ij â‰¥0,\nwhich can be used to characterize the set of (x, q) associated with the function f, as\nwe will discuss in Section 3.4.\nTo simplify the mathematical treatment, we have focused on diï¬€erentiable func-\ntions in Lemma 3.2; nevertheless, the analysis can be extended to non-diï¬€erentiable but\ncontinuous functions (e.g., the ReLU function max{0, x}) using the notion of general-\nized gradient [13, Chap. 2]. In brief, by re-assigning the bounds on partial derivatives\nto uniform bounds on the set of generalized partial derivatives, the constraint (3.5)\ncan be directly applied.\nIn relation to the existing IQCs, this constraint has wider applications for the\ncharacterization of gradient-bounded functions. The Zames-Falb IQC introduced in\n[53] has been widely used for single-input single-output (SISO) functions f : R â†’R,\nbut it requires the function to be monotone with the slope restricted to [Î±, Î²] with\nÎ± â‰¥0, i.e., 0 â‰¤Î± â‰¤f(x)âˆ’f(y)\nxâˆ’y\nâ‰¤Î² whenever x Ì¸= y. The MIMO extension holds true\nonly if the nonlinear function f : Rn â†’Rn is restricted to be the gradient of a convex\nreal-valued function [40, 24]. As for the sector IQC, the scalar version can not be\nused (because it requires fi(x) = 0 whenever there exists j âˆˆ[n] such that xj = 0,\nwhich is extremely restrictive), and the vector version is in fact (3.3). In contrast, the\nquadratic constraint in Lemma 3.2 can be applied to non-monotone, vector-valued\nLipschitz functions.\n3.2. Computation of the smoothness margin. With the newly developed\nquadratic constraint in place, this subsection explains the computation for a smoothness\nmargin of an LTI system G, whose state-space representation is given by:\n(3.12)\nï£±\nï£´\nï£²\nï£´\nï£³\nË™xG = AxG + Bu\nw\n= Ï€(xG)\nu\n= e + w\nwhere xG âˆˆRns is the state (the dependence on t is omitted for simplicity). The\nsystem is assumed to be stable, i.e., A is Hurwitz. We can connect this linear system\nin feedback with a controller Ï€ : Rns â†’Rna. The signal e âˆˆRna is the exploration\n12\nM. JIN AND J. LAVAEI\nvector introduced in reinforcement learning, and w âˆˆRna is the policy action. We\nare interested in certifying the set of gradient bounds Î¾ âˆˆRnsÃ—na of Ï€ such that the\ninterconnected system is input-output stable at all time T â‰¥0, i.e.,\n(3.13)\nZ T\n0\n|y(t)|2 dt â‰¤Î³2\nZ T\n0\n|e(t)|2 dt,\nwhere Î³ is a ï¬nite upper bound for the L2 gain. Let A âª°B or A â‰»B denote that\nA âˆ’B is positive semideï¬nite or positive deï¬nite, respectively. To this end, deï¬ne the\nSDP(P, Î», Î³, Î¾) as follows:\n(3.14)\nSDP(P, Î», Î³, Î¾) :\n\u0014O(P, Î», Î¾)\nS(P)\nS(P)âŠ¤\nâˆ’Î³I\n\u0015\nâ‰º0,\nwhere P = P âŠ¤âª°0 and\nO(P, Î», Î¾) =\n\u0014AâŠ¤P + PA\nPBW\nW âŠ¤BâŠ¤P\n0\n\u0015\n+ 1\nÎ³\n\u0014I\n0\n0\n0\n\u0015\n+ M(Î»; Î¾), S(P) =\n\u0014PB\n0\n\u0015\n,\nwhere M(Î»; Î¾) is deï¬ned in (3.6). We will show next that the stability of the intercon-\nnected system can be certiï¬ed using linear matrix inequalities.\nTheorem 3.3. Let G be stable (i.e., A is Hurwitz) and Ï€ âˆˆRns â†’Rna be a\nbounded causal controller. Assume that:\n(i) the interconnection of G and Ï€ is well-posed;\n(ii) Ï€ has bounded partial derivatives on B (i.e., Î¾ij â‰¤âˆ‚jÏ€i(x) â‰¤Î¾ij, for all x âˆˆB,\ni âˆˆ[na] and j âˆˆ[ns]).\nIf there exist P âª°0 and a scalar Î³ > 0 such that SDP(P, Î», Î³, Î¾) is feasible, then the\nfeedback interconnection of G and Ï€ is stable (i.e., it satisï¬es (3.13)).\nProof. The proof follows a standard dissipation argument. To proceed, we multiply\n\u0002\nxâŠ¤\nG\nqâŠ¤\neâŠ¤\u0003âŠ¤to the left and its transpose to the right of the augmented matrix\nin (3.14), and use the constraints w = Wq and y = xG. Then, SDP(P, Î», Î³, Î¾) can be\nwritten as a dissipation inequality:\nË™V (xG) +\n\u0014xG\nq\n\u0015âŠ¤\nM(Î»; Î¾)\n\u0014xG\nq\n\u0015\n< Î³eâŠ¤e âˆ’1\nÎ³ yâŠ¤y,\nwhere V (xG) = xâŠ¤\nGPxG is known as the storage function, and Ë™V (Â·) is its derivative\nwith respect to time t. Because the second term is guaranteed to be non-negative by\nLemma 3.2, if SDP(P, Î», Î³, Î¾) is feasible with a solution (P, Î», Î³, Î¾), we have:\n(3.15)\nË™V (xG) + 1\nÎ³ yâŠ¤y âˆ’Î³eâŠ¤e < 0,\nwhich is satisï¬ed at all times t. From well-posedness, the above inequality can be\nintegrated from t = 0 to t = T, and then it follows from P âª°0 that:\n(3.16)\nZ T\n0\n|y(t)|2dt â‰¤Î³2\nZ T\n0\n|e(t)|2dt.\nHence, the interconnected system with the RL policy Ï€ is stable.\nSTABILITY-CERTIFIED REINFORCEMENT LEARNING\n13\nThe above theorem requires that G be stable when there is no feedback policy Ï€. This\nis automatically satisï¬ed in many physical systems with an existing stabilizing (but not\nperformance-optimizing) controller. In the case that the original system is not stable,\none needs to ï¬rst design a controller to stablize the system or design the controller\nunder uncertainty (in this case, the RL policy), which are well-studied problems in the\nliterature (e.g., Hâˆcontroller synthesis [16]). Then, the result can be used to ensure\nstability while delegating reinforcement learning to optimize the performance of the\npolicy under gradient bounds.\nThe above result essentially suggests a computational approach in robust control\nanalysis. Given a stable LTI system depicted in (3.12), the ï¬rst step is to represent\nthe RL policy as an uncertainty block in a feedback interconnection. Because the\nparameters of the neural network policy may not be known a priori and will be\ncontinuously updated during learning, we characterize it using bounds on partial\ngradients (e.g., if it is known that the action is positively correlated with certain\nobservation metric, we can specify its partial gradient to be mostly positive with\nonly a small negative margin). A simple but conservative choice is a L2-gain bound\nIQC; nevertheless, to achieve a less conservative result, we can employ the quadratic\nconstraint developed in Lemma 3.2, which exploits both the sparsity of the control\narchitecture and the non-homogeneity of the outputs. For a given set of gradient\nbounds Î¾, we ï¬nd the smallest Î³ such that (3.14) is feasible, and Î³ corresponds to the\nupper bound on the L2 gain of the interconnected system both during learning (with\nthe excitation e added to facilitate policy exploration) and actual deployment. If Î³ is\nï¬nite, then the system is provably stable in the sense of (3.13).\nWe remark that SDP(P, Î», Î³, Î¾) is quasiconvex, in the sense that it reduces to a\nstandard LMI with a ï¬xed Î³. To solve it numerically, we start with a small Î³ and\ngradually increase it until a solution (P, Î») is found. This is repeated for multiple sets\nof Î¾. Each iteration (i.e., LMI for a given set of Î³ and Î¾) can be solved eï¬ƒciently\nby interior-point methods. As an alternative to searching on Î³ for a given Î¾, more\nsophisticated methods for solving the generalized eigenvalue optimization problem can\nbe employed [11].\n3.3. Extension to nonlinear systems with uncertainty. The previous anal-\nysis for LTI systems can be extended to a generic nonlinear system described in (1.1).\nThe key idea is to model the nonlinear and potentially time-varying part gt(x(t)) as\nan uncertain block with IQC constraints on its behavior. Speciï¬cally, consider the LTI\ncomponent G:\n(3.17)\n(\nË™xG = AxG + Bu + v\ny\n= xG\nwhere xG âˆˆRns is the state and y âˆˆRns is the output. The linearized system is\nassumed to be stable, i.e., A is Hurwitz. The nonlinear part is connected in feedback:\n(3.18)\nï£±\nï£´\nï£²\nï£´\nï£³\nu = e + w\nw = Ï€(y)\nv = gt(y)\nwhere e âˆˆRna and w âˆˆRna are deï¬ned as before, and gt : Rns â†’Rns is the nonlinear\nand time-varying component. In addition to characterizing Ï€ using the Lipschitz\nproperty as in (3.5), we assume that gt : Rns â†’Rns satisï¬es the IQC deï¬ned by\n14\nM. JIN AND J. LAVAEI\n(Î¨, Mg) as in Deï¬nition 2.1. The system Î¨ has the state-space representation:\n(3.19)\n( Ë™Ïˆ = AÏˆÏˆ + Bv\nÏˆv + By\nÏˆy\nz = CÏˆÏˆ + Dv\nÏˆv + Dy\nÏˆy ,\nwhere Ïˆ âˆˆRns is the internal state and z âˆˆRnz is the ï¬ltered output. By denoting\nx =\n\u0002\nxâŠ¤\nG\nÏˆâŠ¤\u0003âŠ¤âˆˆR2ns as the new state, one can combine (3.17) and (3.19) via\nreducing y and letting w = Wq:\n(3.20)\nï£±\nï£´\nï£´\nï£´\nï£´\nï£´\nï£´\nï£´\nï£²\nï£´\nï£´\nï£´\nï£´\nï£´\nï£´\nï£´\nï£³\nË™x =\n\"\nA\n0\nBy\nÏˆ\nAÏˆ\n#\n|\n{z\n}\nA\nx +\n\"\nB\n0\n#\n|{z}\nBe\ne +\n\"\nBW\n0\n#\n| {z }\nBq\nq +\n\"\nI\nBv\nÏˆ\n#\n| {z }\nBv\nv\nz =\nh\nDy\nÏˆ\nCÏˆ\ni\n|\n{z\n}\nC\nx + Dv\nÏˆv\n,\nwhere A, Be, Bq, Bv, C are matrices of proper dimensions deï¬ned above. Similar\nto the case of LTI systems, the objective is to ï¬nd the gradient bounds on Ï€ such\nthat the system becomes stable in the sense of (3.13). In the same vein, we deï¬ne\nSDP(P, Î», Î³, Î¾) as:\n(3.21)\nSDP(P, Î», Î³, Î¾) :\nï£®\nï£°\nO(P, Î», Î¾)\nOv(P)\nS(P)\nOv(P)âŠ¤\nDvâŠ¤\nÏˆ MqDv\nÏˆ\n0\nS(P)âŠ¤\n0\nâˆ’Î³I\nï£¹\nï£»â‰º0,\nwhere P âª°0, and\nO(P, Î», Î¾) =\n\u0014AâŠ¤P + PA\nPBq\nBâŠ¤\nq P\n0\n\u0015\n+\n\u0014\nCâŠ¤MgC\n0\n0\n0\n\u0015\n+ M(Î»; Î¾) + 1\nÎ³\n\u0014I\n0\n0\n0\n\u0015\n,\nOv(P) =\n\u0014\nCâŠ¤MqDv\nÏˆ + PBv\n0\n\u0015\n, S(P) =\n\u0014PBe\n0\n\u0015\n,\nwhere M(Î»; Î¾) is deï¬ned in (3.6). The next theorem provides a stability certiï¬cate for\nthe nonlinear time-varying system (1.1).\nTheorem 3.4. Let G be stable (i.e., A in (3.17) is Hurwitz) and Ï€ âˆˆRns â†’Rna\nbe a bounded causal controller. Assume that:\n(i) the interconnection of G, Ï€, and gt is well-posed;\n(ii) Ï€ has bounded partial derivatives on B (i.e., Î¾ij â‰¤âˆ‚jÏ€i(x) â‰¤Î¾ij for all x âˆˆB,\ni âˆˆ[na] and j âˆˆ[ns]);\n(iii) gt âˆˆIQC(Î¨, Mg), where Î¨ is stable.\nIf there exist P âª°0 and a scalar Î³ > 0 such that SDP(P, Î», Î³, Î¾) in (3.21) is feasible,\nthen the feedback interconnection of the nonlinear system (1.1) and Ï€ is stable (i.e., it\nsatisï¬es (3.13)).\nProof. The proof is in the same vein as that of Theorem 3.3. The main tech-\nnical diï¬€erence is the consideration of the ï¬ltered state Ïˆ and the output z to im-\npose IQC constraints on the nonlinearities gt(y) in the dynamical system [33]. The\ndissipation inequality follows by multiplying both sides of the matrix in (3.21) by\n\u0002\nxâŠ¤\nqâŠ¤\nvâŠ¤\neâŠ¤\u0003âŠ¤and its transpose:\nË™V (x) + zâŠ¤Mgz +\n\u0014xG\nq\n\u0015âŠ¤\nMÏ€\n\u0014xG\nq\n\u0015\n< Î³eâŠ¤e âˆ’1\nÎ³ yâŠ¤y,\nSTABILITY-CERTIFIED REINFORCEMENT LEARNING\n15\nwhere x and z are deï¬ned in (3.20), and V (x) = xâŠ¤Px is the storage function\nwith Ë™V (Â·) as its time derivative. The second term on the left side is non-negative\nbecause gt âˆˆIQC(Î¨, Mg), and the third term is non-negative due to the smoothness\nquadratic constraint in Lemma 3.2. Thus, if there exists a feasible solution P âª°0 to\nSDP(P, Î», Î³, Î¾), integrating the inequality from t = 0 to t = T yields that:\n(3.22)\nZ T\n0\n|y(t)|2dt â‰¤Î³2\nZ T\n0\n|e(t)|2dt.\nHence, the nonlinear system interconnected with the RL policy Ï€ is certiï¬ably stable\nin the sense of a ï¬nite L2 gain.\n3.4. Analysis of conservatism of the stability certiï¬cate. We focus on the\ncase where an LTI system G is interconnected with an RL policy Ï€ âˆˆP(Î¾) (i.e.,\na function with bounded partial gradients). This corresponds to the system (3.12)\nstudied in Section 3.2. To certify the stability of (3.12), as will be shown in the next\nproposition, it suï¬ƒces to examine the stability of the following system:\n(3.23)\nï£±\nï£´\nï£´\nï£´\nï£²\nï£´\nï£´\nï£´\nï£³\nË™xG = AxG + Bu\nq\n= ËœÏ€(xG)\nw\n= Wq\nu\n= e + w\nwhere ËœÏ€ âˆˆeP is a function in the uncertainty set:\n(3.24)\neP(Î¾) =\nn\nËœÏ€\n\f\f\f Î¾ijxj â‰¤ËœÏ€ij(x) â‰¤Î¾ijxj, âˆ€x âˆˆRns, i âˆˆ[na], j âˆˆ[ns]\no\n.\nProposition 3.5. If the system (3.23) is stable for all ËœÏ€ âˆˆeP(Î¾), then the system\n(3.12) is stable for all Ï€ âˆˆP(Î¾).\nProof. It suï¬ƒces to show that for any Ï€ âˆˆP(Î¾), there exists a policy ËœÏ€ âˆˆeP(Î¾)\nsuch that Ï€ = W ËœÏ€. Let y0\nj =\n\u00020\nÂ· Â· Â·\n0\nyj+1\nÂ· Â· Â·\nyns\n\u0003\nâˆˆRns for every j âˆˆ\n{0, 1, ..., ns}, and y0\n0 = y, y0\nns = 0. Then, one can write:\nÏ€i(y) =\nns\nX\nj=1\nÏ€i(y0\njâˆ’1) âˆ’Ï€i(y0\nj ) =\nns\nX\nj=1\nËœÏ€ij(y),\nwhere ËœÏ€ij(y) satisï¬es\nËœÏ€ij(y)\nyj\n= Ï€i(y0\njâˆ’1) âˆ’Ï€i(y0\nj )\n|y0\njâˆ’1 âˆ’y0\nj |\nâˆˆ[Î¾ij, Î¾ij]\nif yj Ì¸= 0 and ËœÏ€ij(y) = 0 if yj = 0. The bound is due to the mean-value theorem and\nthe bounds on the partial derivatives of Ï€i. Since the above argument is valid for all\ni âˆˆ[na], it means that ËœÏ€ âˆˆeP(Î¾), and Ï€ = W ËœÏ€.\nProposition 3.5 implies that one potential source of conservatism comes from\nthe decomposition of a gradient-bounded function into a sum of sector-bounded\ncomponents. Henceforth, we focus the subsequent analysis by examining (3.23). By\nconsidering the state-space representation of G =\n\u0014 A\nBW\nB\nI\n0\n0\n\u0015\n=\n\u0002 G11\nG12\n\u0003\n,\n16\nM. JIN AND J. LAVAEI\none can write system (3.23) as:\n(3.25)\nï£±\nï£´\nï£²\nï£´\nï£³\nxG =\nh\nG11\nG12\ni \"\nq\ne\n#\nq\n= ËœÏ€(xG)\n.\nIt is known that the system is input-output stable if and only if I âˆ’G11ËœÏ€ is nonsingu-\nlar [16]. To understand this, note that if I âˆ’G11ËœÏ€ is nonsingular, then the transfer\nfrom e to xG is given by:\ne 7â†’xG = H(e) = (I âˆ’G11ËœÏ€)âˆ’1G12e,\nand |xG| = |H(e)| â‰¤âˆ¥(I âˆ’G11ËœÏ€)âˆ’1G12âˆ¥|e|. From the previous section (in particular,\nLemma 3.2), we know that if the function Ï€ is gradient-bounded, then the set of\ninput/output signals belongs to:\nS(Î¾) =\n\b\n(x, q) | Ï†ij(x, q) = (c2\nij âˆ’c2\nij)x2\nj + 2cijqijxj âˆ’q2\nij â‰¥0,\nâˆ€i âˆˆ[na], j âˆˆ[ns]\n\t\n,\nwhere we use cij = 1\n2\n\u0010\nÎ¾ij + Î¾ij\n\u0011\n, cij = Î¾ij âˆ’cij for simplicity. We now show that\nthe pair (x, q) belongs to S(Î¾) if and only if there exists a sector-bounded function\nËœÏ€ âˆˆËœP(Î¾) such that it satisï¬es q = ËœÏ€(x).\nLemma 3.6. Suppose that x âˆˆRns and q âˆˆRnans, and cij â‰¥0 for every i âˆˆ[na]\nand j âˆˆ[ns]. Then, the pair (x, q) belongs to S(Î¾) if and only if there exists an operator\nËœÏ€ : Rn â†’Rnans, such that q = ËœÏ€(x), and ËœÏ€ satisï¬es the following conditions: (i)\nËœÏ€ij(x) = 0 if xj = 0, and (ii) ËœÏ€ is sector bounded, i.e., (cij âˆ’cij)xj â‰¤ËœÏ€ij(x) â‰¤\n(cij + cij)xj holds for all i âˆˆ[na] and j âˆˆ[ns].\nProof. To show the suï¬ƒciency direction, conditions (i) and (ii) yield that\n(cijxj)2 â‰¥\n\u0012\u0012Ï€ij(x)\nxj\nâˆ’cij\n\u0013\nxj\n\u00132\n= (qij âˆ’cijxj)2 .\nBy rearranging the above inequality, it can be concluded that (x, q) âˆˆS(Î¾).\nFor the necessary direction, note that the condition Ï†ij(x, q) â‰¥0 is equivalent\nto |qij âˆ’cijxj| â‰¤|cijxj|. Thus, we have Ï€ij(x) = 0 if xj = 0. Since cij â‰¥0, one can\nobtain\n\f\f\f qij\nxj âˆ’cij\n\f\f\f â‰¤cij, which is equivalent to the sector bounds.\nBy slightly overloading the notations, we can extend the result of the previous\nlemma from static mapping to the case that x âˆˆLns and q âˆˆLnans with the operator\nËœÏ€ : Lns â†’Lnans. We can then extend the deï¬nition of S(Î¾) to this space accordingly.\nLemma 3.7. Suppose that x âˆˆLns and q âˆˆLnans, and that cij â‰¥0 for all i âˆˆ[na]\nand j âˆˆ[ns]. Then, the pair (x, q) belongs to S(Î¾) where\n(3.26)\nS(Î¾) = {(x, q) | Ï†ij(x, q) â‰¥0,\nâˆ€i âˆˆ[na], j âˆˆ[ns]} ,\nand\n(3.27)\nÏ†ij(x, q) = (c2\nij âˆ’c2\nij)âˆ¥xjâˆ¥2 + 2cijâŸ¨qij, xjâŸ©âˆ’âˆ¥qijâˆ¥2 â‰¥0,\nif and only if there exists an operator ËœÏ€ : Lns â†’Lnans such that q = ËœÏ€(x) and ËœÏ€\nsatisï¬es the following conditions: (i) ËœÏ€ij(x) = 0 if xj = 0, and (ii) ËœÏ€ is sector bounded,\ni.e., (cij âˆ’cij)âˆ¥xjâˆ¥â‰¤âˆ¥ËœÏ€ij(x)âˆ¥â‰¤(cij + cij)âˆ¥xjâˆ¥for all i âˆˆ[m] and j âˆˆ[n].\nSTABILITY-CERTIFIED REINFORCEMENT LEARNING\n17\nProof. For the suï¬ƒciency condition, since ËœÏ€ij is sector bounded, and ËœÏ€ij(x) = 0 if\nxj = 0, without loss of generality, assume that cij â‰¤0. One can write\nâˆ¥cijxjâˆ¥2 â‰¥\n\r\r\r\r\n\f\f\f\f\nâˆ¥ËœÏ€ij(x)âˆ¥\nâˆ¥xjâˆ¥\nâˆ’cij\n\f\f\f\f xj\n\r\r\r\r\n2\nâ‰¥\n\r\r\r\r\n\u0012âˆ¥ËœÏ€ij(x)âˆ¥\nâˆ¥xjâˆ¥\nâˆ’cij\n\u0013\nxj\n\r\r\r\r\n2\n= âˆ¥cijxjâˆ¥2 + âˆ¥ËœÏ€ij(x)âˆ¥2 âˆ’2\n\u001c\ncijxj, âˆ¥ËœÏ€ij(x)âˆ¥\nâˆ¥xjâˆ¥\nxj\n\u001d\n= âˆ¥cijxjâˆ¥2 + âˆ¥ËœÏ€ij(x)âˆ¥2 âˆ’2cijâˆ¥xjâˆ¥âˆ¥ËœÏ€ij(x)âˆ¥\nâ‰¥âˆ¥cijxjâˆ¥2 + âˆ¥ËœÏ€ij(x)âˆ¥2 âˆ’2cij âŸ¨ËœÏ€ij(x), xjâŸ©\n= âˆ¥qij âˆ’cijxjâˆ¥2 .\nBy rearranging the above inequality, it can be concluded that (x, q) âˆˆS(Î¾).\nFor the necessary direction, we can construct ËœÏ€(y) = q âŸ¨y,xâŸ©\n|x|2 for all y âˆˆLns. This\nleads to ËœÏ€(x) = q, and the condition Ï†ij(x, q) â‰¥0 is equivalent to âˆ¥qij âˆ’cijxjâˆ¥â‰¤\ncij âˆ¥xjâˆ¥. Thus, we have ËœÏ€ij(x) = 0 if xj = 0. Without loss of generality, assume that\ncij â‰¤0. Therefore, âˆ¥qijâˆ¥â‰¤cijâˆ¥xjâˆ¥+ cijâˆ¥xjâˆ¥and âˆ¥qijâˆ¥â‰¥âˆ’cijâˆ¥xjâˆ¥+ cijâˆ¥xjâˆ¥, which\nare equivalent to the sector bound condition.\nThe previous result indicates that the input and output pair of ËœÏ€ can be described\nby S(Î¾). We show next that this set should be separated from the signal space of the\ndynamical system in order to ensure robust stability.\nLemma 3.8. If (G, ËœÏ€) is robustly stable, then there cannot exist a nonzero q âˆˆL2\nsuch that x = Gq and (x, q) âˆˆS(Î¾).\nProof. We prove this lemma by contraposition. If there exists a nonzero q âˆˆL2\nsuch that (x, q) âˆˆS(Î¾), then it follows from Lemma 3.7 that there exists a linear\noperator ËœÏ€ such that q = ËœÏ€(x) = ËœÏ€(Gq). This implies that the operator (I âˆ’ËœÏ€G) is\nsingular, and therefore, (I âˆ’GËœÏ€) is singular, implying that the interconnected system\nis not robustly stable.\nThe path of examining the necessity of the SDP condition (3.14) has become clear.\nConsider the set generated by the LTI system:\n(3.28)\nÎ¨ = {(Ï†ij(x, q) : q âˆˆLnans, âˆ¥qâˆ¥= 1, x = Gq} ,\nand the positive orthant\n(3.29)\nÎ  = {(rij) âˆˆRnans : rij â‰¥0,\nâˆ€i âˆˆ[na], j âˆˆ[ns]} .\nLemma 3.8 implies that the two sets Î¨ and Î  are separated if (G, ËœÏ€) is robustly\nstable. The goal is to show that there exists a separating hyperplane, whose pa-\nrameters are related to the solution of (3.14). For simplicity, deï¬ne the matrices\nâ„¦ij,x = diag\n\u0000\b\n{c2\nij âˆ’c2\nij\n\t\u0001\n, â„¦ij,q and â„¦ij,xq with their (k, l)-th elements [â„¦ij,q]kl =\n(\n1\nif k = in + j\n0\notherwise\n, and [â„¦ij,xq]kl =\n(\ncij\nif k = j, l = in + j\n0\notherwise\n.\nTo write Ï†ij(x =\nGq, q) as an inner product, deï¬ne\nTij = Gâˆ—â„¦ij,xG âˆ’â„¦ij,q + Gâˆ—â„¦âˆ—\nij,xq + â„¦ij,xqG.\n18\nM. JIN AND J. LAVAEI\nIt results from the deï¬nition (3.27) that\n(3.30)\nÏ†ij(x = Gq, q) = âˆ¥Gqâˆ¥2\nâ„¦ij,x + 2Re âŸ¨â„¦ij,xqGq, qâŸ©âˆ’âˆ¥qâˆ¥2\nâ„¦ij,q = âŸ¨q, TijqâŸ©.\nLemma 3.9. For a given linear time-invariant operator G, the closure Î¨ of Î¨\ndeï¬ned in (3.28) is convex.\nProof. Because G is time-invariant, by denoting DÏ„ as the delay operator at scale\nÏ„, we obtain Dâˆ—\nÏ„TijDÏ„ = Tij. Let y = Ï†(q) and Ëœy = Ï†(Ëœq) be the elements of Î¨, with\nâˆ¥qâˆ¥= âˆ¥Ëœqâˆ¥= 1. By considering qÏ„ = âˆšÎ±q + âˆš1 âˆ’Î±DÏ„ Ëœq, one can write\nÏ†ij(qÏ„) = Î± âŸ¨Tijq, qâŸ©+ (1 âˆ’Î±) âŸ¨TijDÏ„ Ëœq, DÏ„ ËœqâŸ©+ 2Î±\nâˆš\n1 âˆ’Î±Re âŸ¨Tijq, DÏ„ ËœqâŸ©\n= Î±Ï†ij(q) + (1 âˆ’Î±)Ï†ij(Ëœq) + 2Î±\nâˆš\n1 âˆ’Î±Re âŸ¨Tijq, DÏ„ ËœqâŸ©.\nBy letting Ï„ â†’âˆ, we obtain Re âŸ¨Tijq, DÏ„ ËœqâŸ©â†’0, where Re(x) denotes the real part\nof a complex vector x. Thus,\nlim\nÏ„â†’âˆÏ†ij(qÏ„) = Î±Ï†ij(q) + (1 âˆ’Î±)Ï†ij(Ëœq)\nand limÏ„â†’âˆâˆ¥qÏ„âˆ¥2 = Î±âˆ¥qâˆ¥2 + (1 âˆ’Î±)âˆ¥Ëœqâˆ¥2 = 1. Therefore,\nlim\nÏ„â†’âˆÏ†\n\u0012 qÏ„\nâˆ¥qÏ„âˆ¥\n\u0013\n= Î±y + (1 âˆ’Î±)Ëœy âˆˆÎ¨.\nNow, we show that strict separation occurs when the system is robustly stable.\nLemma 3.10. Suppose that I âˆ’GËœÏ€ is nonsingular. Then, the sets Î  and Î¨ are\nstrictly separated, namely D(Î , Î¨) = infrâˆˆÎ ,yâˆˆÎ¨ |r âˆ’y| > 0.\nTo prove this result, we need the following lemma.\nLemma 3.11. Suppose that D(Î , Î¨) = infrâˆˆÎ ,yâˆˆÎ¨ |r âˆ’y| = 0. Given any Ïµ > 0\nand t0 â‰¥0, there exist a closed interval [t0, t1] and two signals x âˆˆLns and q âˆˆLnans\nwith âˆ¥qâˆ¥= 1 such that\nÏ†ij(x, q) â‰¥0,\nâˆ€i âˆˆ[na], j âˆˆ[ns]\n(3.31)\nÏµ2 > âˆ¥(I âˆ’Î“[t0,t1])Gqâˆ¥\n(3.32)\nÏµ = âˆ¥x âˆ’Î“[t0,t1]Gqâˆ¥â„¦ij,x,\n(3.33)\nwhere âˆ¥qâˆ¥â„¦= âˆšqâˆ—â„¦q is the scaled norm and Î“[t0,t1] projects the signal onto the support\nof [t0, t1]. With the above choice of q, x and [t0, t1], there exists an operator ËœÏ€ âˆˆËœP(Î¾)\nsuch that âˆ¥(I âˆ’ËœÏ€Î“[t0,t1]G)qâˆ¥â‰¤CÏµ for some constant C > 0 that depends on the sector\nbounds Î¾.\nProof. For a given Ïµ > 0, by hypothesis, there exists q âˆˆLnans with âˆ¥qâˆ¥= 1\nsatisfying Ï†ij(x, q) > âˆ’Ïµ2 for all i âˆˆ[na] and j âˆˆ[ns], i.e.,\nÏµ2 + âˆ¥Gqâˆ¥2\nâ„¦ij,x + 2Re âŸ¨â„¦ij,xqGq, qâŸ©> âˆ¥qâˆ¥2\nâ„¦ij,q,\nwhere â„¦ij,x and â„¦ij,xq are deï¬ned previously. Clearly, if q is truncated to a suï¬ƒciently\nlong interval, and q is rescaled to have a unit norm, the above inequality will still hold.\nSince Gq âˆˆLns, by possibly enlarging the truncation interval to [t0, t1], we obtain\n(3.32), and\nÏµ2 + âˆ¥Î“[t0,t1]Gqâˆ¥2\nâ„¦ij,x + 2Re\n\nâ„¦ij,xqÎ“[t0,t1]Gq, q\n\u000b\n> âˆ¥qâˆ¥2\nâ„¦ij,q,\nSTABILITY-CERTIFIED REINFORCEMENT LEARNING\n19\nNext, we choose Î· âˆˆLns such that âˆ¥Î·âˆ¥2\nâ„¦ij,x = Ïµ2, and that Î· is orthogonal to Î“[t0,t1]Gq\nand â„¦âˆ—\nij,xqq for all i âˆˆ[na] and j âˆˆ[ns]. Then, by considering x = Î“[t0,t1]Gq + Î·, we\nobtain\nâˆ¥xâˆ¥2\nâ„¦ij,x = âˆ¥Î“[t0,t1]Gq + Î·âˆ¥2\nâ„¦ij,x = Ïµ2 + âˆ¥Î“[t0,t1]Gqâˆ¥2\nâ„¦ij,x,\nwhich leads to Ï†ij(x, q) â‰¥0 and (3.33). Now, we can invoke Lemma 3.7 to construct\nËœÏ€ âˆˆP(Î¾) based on (3.31) such that ËœÏ€ becomes sector bounded and q = ËœÏ€x. Then,\n(I âˆ’ËœÏ€Î“[t0,t1]G)q = ËœÏ€(x âˆ’Î“[t0,t1]Gq).\nLet âˆ¥ËœÏ€âˆ¥â‰¤C (which depends on the sector bounds). Then,\nâˆ¥(I âˆ’ËœÏ€Î“[t0,t1]G)qâˆ¥â‰¤CÏµ\n.\nWe are now ready to prove the strict separation result.\nProof of Lemma 3.10. Assume that D(Î , Î¨) = infrâˆˆÎ ,yâˆˆÎ¨ |r âˆ’y| = 0. Consider\na sequence Ïµn â†’0 as n tends to âˆ. For each Ïµn, construct signals q(n) with a bounded\nsupport on [tn, tn+1], and ËœÏ€(n) according to Lemma 3.11. Deï¬ne\nËœÏ€ =\nâˆ\nX\nn=1\nËœÏ€(n)Î“[tn,tn+1].\nWe have\nËœÏ€Gq(n) = ËœÏ€(n)Î“[tn,tn+1]Gq(n) + ËœÏ€(I âˆ’Î“[tn,tn+1])Gq(n),\nand\nâˆ¥(I âˆ’Ï€G)q(n)âˆ¥â‰¤âˆ¥(I âˆ’ËœÏ€(n)Î“[tn,tn+1]G)q(n)âˆ¥+ âˆ¥(I âˆ’Î“[tn,tn+1])Gq(n)âˆ¥\nâ‰¤CÏµn + Ïµ2\nn\nBecause Ïµn â†’0, the right-hand side approaches 0, and so does the left-hand side.\nTherefore, since âˆ¥q(n)âˆ¥= 1, the mapping Iâˆ’ËœÏ€G cannot be invertible, which contradicts\nthe robust stability assumption. This implies that Î  and Î¨ are strictly separable.\nTo draw the connection to the SDP problem (3.14), observe that\n(3.34)\nÏ†ij(x, q) =\n\u001c\u0014x\nq\n\u0015\n, M ij\nÏ€\n\u0014x\nq\n\u0015\u001d\n,\nwhere\n[M ij\nÏ€ ]kl =\nï£±\nï£´\nï£²\nï£´\nï£³\nc2\nij âˆ’c2\nij\n(k, l) = (j, j)\ncij\n(k, l) = (i, i âˆ—n + j) or (i âˆ—n + j, i)\nâˆ’1\n(k, l) = (i âˆ—n + j, i âˆ—n + j)\n,\nand M(Î»; Î¾) = P\niâˆˆ[na],jâˆˆ[ns] Î»ijM ij\nÏ€ as deï¬ned in Lemma 3.2.\nProposition 3.12. The SDP condition (3.14) is feasible if and only if there exist\nmultipliers Î»ij â‰¥0 and Ïµ > 0 such that\n(3.35)\nX\niâˆˆ[na],jâˆˆ[ns]\nÎ»ijÏ†ij(x, q) â‰¤âˆ’Ïµâˆ¥qâˆ¥2\nfor all q âˆˆLnans and x = Gq.\n20\nM. JIN AND J. LAVAEI\nProof. Since Ï†ij(x, q) =\n\u001c\u0014x\nq\n\u0015\n, M ij\nÏ€\n\u0014x\nq\n\u0015\u001d\n, the condition (3.35) is equivalent to\n(3.36)\n\u0014G\nI\n\u0015âˆ—\nM(Î»; Î¾)\n\u0014G\nI\n\u0015\nâ‰º0.\nBy the KYP lemma, this is equivalent to the existence of P âª°0 such that:\n(3.37)\n\u0014AâŠ¤P + PA\nPBW\nW âŠ¤BâŠ¤P\n0\n\u0015\n+ M(Î»; Î¾) â‰º0.\nBy Schur complement, P satisï¬es the KYP condition if and only if it satisï¬es (3.14).\nThus, the claim is shown.\nTheorem 3.13. Let ËœÏ€ : Lns â†’Lnans be a bounded causal controller such that\nËœÏ€ âˆˆËœP(Î¾). Assume that the interconnection of G and ËœÏ€ is well-posed. Then, the\ninput-output stability of the feedback interconnection of system (3.23) implies that there\nexist P âª°0, Î³ > 0 and Î» â‰¥0 such that SDP(P, Î», Î³, Î¾) in (3.14) is feasible.\nProof. Since the system is input-output stable, the sets Î  and Î¨ are strictly\nseparable due to Lemma 3.10. Since both Î  and Î¨ are convex (Lemma 3.9), there\nexist a strictly separating hyperplane parametrized by Î» âˆˆRmn and scalars Î±, Î², such\nthat\nâŸ¨Î», Ï†âŸ©â‰¤Î± < Î² â‰¤âŸ¨Î», yâŸ©\nfor all Ï† âˆˆÎ¨ and y âˆˆÎ . Since âŸ¨Î», yâŸ©is bounded from below, we must have Î» â‰¥0, and\nwithout loss of generality, we can set Î² = 0 and Î± < 0. This condition is equivalent to\n(3.35), and by Proposition 3.12, this implies that the SDP condition is feasible.\n4. Numerical examples. In this section, we empirically study the stability-\ncertiï¬ed reinforcement learning in real-world problems such as ï¬‚ight formation [23] and\npower grid frequency regulation [18]. Designing an optimal controller for these systems\nis challenging, because they consist of many interconnected subsystems that have\nlimited information sharing, and also their underlying models are typically nonlinear\nand even time-varying and uncertain. Indeed, for the case of decentralized control,\nwhich aims at designing a set of local controllers whose interactions are speciï¬ed by\nphysical and informational structures, it has been long known that it amounts to\nan NP-hard optimization problem in general [8]. End-to-end reinforcement learning\ncomes in handy, because it does not require model information by simply interacting\nwith the environment while collecting rewards.\nIn a multi-agent setting, each agent explores and learns its own policy independently\nwithout knowing about other agentsâ€™ policies [12]. For the simplicity of implementation,\nwe consider the synchronous and cooperative scenario, where agents conduct an action\nat each time step and observe the reward for the whole system. Their goal is to\ncollectively maximize the rewards (or minimize the costs) shared equally among them.\nThe present analysis aims at oï¬€ering safety certiï¬cates of existing RL algorithms\nwhen applied to real-world dynamical systems, by simply monitoring the gradients\ninformation of the neural network policy. This is orthogonal to the line of research\nthat aims at improving the performance of the existing RL algorithms. The examples\nare taken from [23, 18, 17], but we deal directly with the underlying nonlinear physics\nrather than a linearized model.\nSTABILITY-CERTIFIED REINFORCEMENT LEARNING\n21\n4.1. Multi-agent ï¬‚ight formation. Consider the multi-agent ï¬‚ight formation\nproblem [23], where each agent can only observe the relative distance from its neighbors,\nas illustrated in Figure 2. The goal is to design a local controller for each aircraft such\na predeï¬ned pattern is formed as eï¬ƒciently as possible. The physical model3 for each\nTime (sec)\nPositions (m)\nFig. 2: Illustration of the multi-agent ï¬‚ight formation problem.\naircraft is given by:\nÂ¨zi(t) = vi(t)\nÂ¨Î¸i(t) = 1\nÎ´\n\u0000sin Î¸i(t) + vi(t)\n\u0001\n,\nwhere zi and Î¸i denote the horizontal position and angle of aircraft i, respectively, and\nÎ´ > 0 characterizes the physical coupling of rolling moment and lateral acceleration.\nTo stabilize the system, a simple feedback rule is proposed in [6],\n(4.1)\nvi(t) = Î± Ë™zi(t) + Î²Î¸i(t) + Î³ Ë™Î¸i(t) + ui(t)\nwhere the parameters of the ï¬rst three terms are designed to maintain the internal\nstability of the horizontal speed and angle of each aircraft (speciï¬cally, Î± = 90.62,\nÎ² = âˆ’42.15, Î³ = âˆ’13.22, Î´ = 0.1 as explained in [6]), and the last term is an external\ninput optimized for performance (e.g., to move the aircraft to a target state as fast\nas possible). For each agent, by deï¬ning the state xi(t) =\n\u0002\nË™zi(t)\nÎ¸i(t)\nË™Î¸(t)\n\u0003âŠ¤, the\nabove dynamics can be written as\n(4.2)\nË™xi(t) =\nï£®\nï£°\nÎ±\nÎ²\nÎ³\n0\n0\n1\nÎ±\nÎ´\nÎ²+1\nÎ´\nÎ³\nÎ´\nï£¹\nï£»\n|\n{z\n}\nAi\nxi(t) +\nï£®\nï£°\n1\n0\n1\nÎ´\nï£¹\nï£»\n|{z}\nBi\nui(t) +\nï£®\nï£°\n0\n0\nsin Î¸i(t) âˆ’Î¸i(t)\nï£¹\nï£»\n|\n{z\n}\ngi(xi(t))\n,\nwhere gi(xi(t)) is a nonlinear function of xi(t) that is neglected for a linearized model\n[6, 17]. In a distributed control setting, each agent only has access to the relative\ndistance from its neighbors; therefore, for agents i = 1, 2, 3, deï¬ne\n(4.3)\nexi(t) =\n\u0002\nzi(t) âˆ’zi+1(t) âˆ’d\nxi(t)âŠ¤\u0003âŠ¤,\n3The cosine term in the original formulation is omitted for simplicity, though it can be incorporated\nin a more comprehensive treatment.\n22\nM. JIN AND J. LAVAEI\nwhere d is the desired distance between agents. The state-space model of the intercon-\nnected system can be written in the form of (1.1):\n(4.4)ï£®\nï£¯ï£¯ï£¯ï£°\nË™ex\n1\nË™ex\n2\nË™ex\n3\nË™x4\nï£¹\nï£ºï£ºï£ºï£»=\nï£®\nï£¯ï£¯ï£°\neA1\nH4\n0\n0\n0\neA2\nH4\n0\n0\n0\neA3\nH3\n0\n0\n0\nA4\nï£¹\nï£ºï£ºï£»\n|\n{z\n}\nA\nï£®\nï£¯ï£¯ï£°\nex1\nex2\nex3\nx4\nï£¹\nï£ºï£ºï£»\n| {z }\nx(t)\n+\nï£®\nï£¯ï£¯ï£°\neB1\n0\n0\n0\n0\neB2\n0\n0\n0\n0\neB3\n0\n0\n0\n0\nB4\nï£¹\nï£ºï£ºï£»\n|\n{z\n}\nB\nï£®\nï£¯ï£¯ï£°\nu1\nu2\nu3\nu4\nï£¹\nï£ºï£ºï£»\n| {z }\nu(t)\n+\nï£®\nï£¯ï£¯ï£°\neg1(x1)\neg2(x2)\neg3(x3)\ng4(x4)\nï£¹\nï£ºï£ºï£»\n|\n{z\n}\ng(x(t))\n,\nwhere H3 (or H4) is a 4 Ã— 3 (or 4 Ã— 4) matrix whose (i, j)th entry is equal to âˆ’1 if\n(i, j) = (1, 1) (or (i, j) = (1, 2)) and is zero otherwise, and where eAi, eBi and egi(xi(t))\nfor i = 1, 2, 3 are augmented to account for the state of relative positions, given by\n(4.5)\neAi =\nï£®\nï£¯ï£¯ï£°\n0\n1\n0\n0\n0\nÎ±\nÎ²\nÎ³\n0\n0\n0\n1\n0\nÎ±\nÎ´\nÎ²+1\nÎ´\nÎ³\nÎ´\nï£¹\nï£ºï£ºï£»,\neBi =\nï£®\nï£¯ï£¯ï£°\n0\n1\n0\n1\nÎ´\nï£¹\nï£ºï£ºï£», egi(xi(t)) =\nï£®\nï£¯ï£¯ï£°\n0\n0\n0\nsin Î¸i(t) âˆ’Î¸i(t)\nï£¹\nï£ºï£ºï£».\nOne particular strength of RL is that the reward function can be highly nonconvex,\nnonlinear, and arbitrarily designed; however, since quadratic costs are widely used\nin the control literature, consider the case r(x(t), u(t)) = x(t)âŠ¤Qx(t) + u(t)âŠ¤Ru(t).\nFor the following experiments, assume that Q = 1000 Ã— I15 and R = I4. In addition,\nbecause the original system A has its largest eigenvalue at 0, we need a nominal\ndistributed linear controller Kd, whose primary goal is to make the largest eigenvalue\nof A + BKn negative. Such controller could be designed using methods such as robust\ncontrol synthesis for the linearized system [16, 55]. With the nominal controller in\nplace, we can deï¬ne the new system matrix AG = A + BKn and replace A in (4.4).\nThe task for multi-agent RL is to learn the controller ui(t), which only takes\ninputs of the relative distances of agent i to its neighbors. For example, agent 1 can\nonly observe z1(t) âˆ’z2(t) âˆ’d (i.e., the 1st entry of x(t)); similarly, agent 2 can only\nobserve z1(t) âˆ’z2(t) âˆ’d and z2(t) âˆ’z3(t) âˆ’d (i.e., the 1st and 5th entries of x(t)).\nStability certiï¬cate: To obtain the stability certiï¬cate of (4.4), we apply the\nmethod in Section 3.3. The nonzero entries of the nonlinear component g(x(t)) are\nin the form of sin(Î¸) âˆ’Î¸, which can be treated as an uncertainty block with the\nslope restricted to [âˆ’1, 0] for Î¸ âˆˆ[âˆ’Ï€\n2 , Ï€\n2 ]; therefore, the Zames-Falb IQCs can be\nemployed to construct (3.19) [53, 29]. As for the RL agents ui, their gradient bounds\ncan be certiï¬ed according to Theorem 3.4. Speciï¬cally, we assume that each agent ui is\nl-Lipschitz continuous, and solve (3.21) for a given set of Î³ and l. The certiï¬ed gradient\nbounds (Lipschitz constants) are plotted in Figure 3 using diï¬€erent constraints. The\nconservative L2 constraint (3.3) is only able to certify stability for Lipschitz constants\nup to 0.8. By incorporating the sparsity of distributed controller, we can increase the\nmargin to 1.2, which is satisï¬ed throughout the learning process.\nIn order to further increase the set of certiï¬able stable controllers, we monitor\nthe partial gradient information for each agent and encode them as non-homogeneous\ngradient bounds.\nFor instance, if\nâˆ‚Ï€i(x)\nâˆ‚xj\nhas been consistently positive for latest\niterations, we will set Î¾ij = l and Î¾ij = âˆ’Ïµl, where Ïµ > 0 is a small margin, such as\n0.1, to allow explorations. By performing this during learning, it would be possible to\nsigniï¬cantly enlarge the certiï¬ed Lipschitz bound to up to 2.5, as shown in Figure 3.\nPolicy gradient RL: To perform multi-agent reinforcement learning, we employ\ntrust region policy optimization with natural gradients and smoothness policies. During\nSTABILITY-CERTIFIED REINFORCEMENT LEARNING\n23\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nLipschitz const.\n0\n5\n10\n15\n20\nCertified L2 gain\nL2\nInp. sp.\nInp. sp & out. nh.\nFig. 3: Stability-certiï¬ed Lipschitz constants obtained by the standard L2 bound (L2)\nin (3.3) and the method proposed in Lemma 3.2, which considers input sparsity (inp.\nsp.) and output non-homogeneity (out. nh.).\nlearning, we employ the hard-thresholding step introduced in Section 2.1 to ensure\nthat the gradient bounds are satisï¬ed. The trajectories of rewards averaged over\nthree independent experiments are shown in Figure 4. In this example, agents with\na 1-layer neural network (each with 5 hidden units) can learn most eï¬ƒciently when\nemployed with the smoothness penalties (coeï¬ƒcients are set to be Ï‰1 = Ï‰2 = 0.01) in\n(2.8). Without the guidance of these penalties, the linear controller and 1-layer neural\nnetwork apparently cannot eï¬€ectively explore the parameter space.\n0\n50\n100\n150\n200\nIterations\n3000\n2750\n2500\n2250\nRewards\n1-layer NN, SL\n3-layer NN, SL\n5-layer NN, SL\n1-layer NN\nLin\nFig. 4: Learning performance of diï¬€erent control structures (1-layer neural network,\n5-layer neural network, and linear controller). By the inclusion of a smoothness loss\n(SL) in the learning objective (2.8), the exploration becomes more eï¬€ective.\nThe learned 5-layer neural network policy is employed in an actual control task, as\nshown in Figure 5. Compared to the nominal controller, the ï¬‚ights can be maneuvered\nmore eï¬ƒciently in this case with only local information. In terms of the actual cost,\nthe RL agents achieve the cost 41.0, which is about 30% lower than that of the\nnominal controller (58.3). This result can be examined both in the actual state-action\ntrajectories in Figure 5 or the control behaviors in Figure 6. The results indicate that\nRL is able to improve a given controller when the underlying system is nonlinear and\nunknown.\n4.2. Power system frequency regulation. In this case study, we focus on\nthe problem of distributed control for power system frequency regulation [18]. The\nIEEE 39-Bus New England Power System under analysis is shown in Figure 7. In a\ndistributed control setting, each generator can only share its rotor angle and frequency\ninformation with a pre-speciï¬ed set of counterparts that are geographically distributed.\n24\nM. JIN AND J. LAVAEI\n0\n25\n50\n75\n100\nTime (sec)\n1.4\n1.6\n1.8\n2.0\nRel. dist. (m)\nA1-A2\nA2-A3\nA3-A4\n(a) Nom.: reletive distances.\n0\n25\n50\n75\n100\nTime (sec)\nâˆ’1.0\nâˆ’0.5\n0.0\nAngle Î¸ (rad)\nA1\nA2\nA3\nA4\n(b) Nom.: angles Î¸i.\n0\n25\n50\n75\n100\nTime (sec)\nâˆ’5\n0\n5\nActions\nA1\nA2\nA3\nA4\n(c) Nom.: actions.\n0\n25\n50\n75\n100\nTime (sec)\n1.6\n1.8\n2.0\nRel. dist. (m)\nA1-A2\nA2-A3\nA3-A4\n(d) NN: relative distances.\n0\n25\n50\n75\n100\nTime (sec)\nâˆ’0.5\n0.0\nAngle Î¸ (rad)\nA1\nA2\nA3\nA4\n(e) NN: angles Î¸i.\n0\n25\n50\n75\n100\nTime (sec)\nâˆ’10\nâˆ’5\n0\n5\nActions\nA1\nA2\nA3\nA4\n(f) NN: actions.\nFig. 5: State and action trajectories in a typical contral task, where the nominal\ncontroller (Nom) and the RL agents achieve costs of 58.3 and 41.0, respectively.\nâˆ’1\n0\n1\nX2 âˆ’X1\nâˆ’10\nâˆ’5\n0\n5\n10\nActions\nRL action\nNom. action\n(a) Controller of Agent 1.\nâˆ’1\n0\n1\nX4 âˆ’X3\nâˆ’10\n0\n10\nActions\nRL action\nNom. action\n(b) Controller of Agent 4.\nFig. 6: Demonstration of control outputs for the nominal action and RL agents.\nThe main goal is to optimally adjust the mechanical power input to each generator\nsuch that the phase and frequency at each bus can be restored to their nominal values\nafter a possible perturbation. Let Î¸i denote the voltage angle at a generator bus i (in\nrad). The physics of power systems are modeled by the per-unit swing equation:\n(4.6)\nmiÂ¨Î¸i + di Ë™Î¸ = pmi âˆ’pei\nwhere pmi is the mechanical power input to the generator at bus i (in p.u.), pei is\nthe electrical active power injection at bus i (in p.u.), mi is the inertia coeï¬ƒcient\nof the generator at bus i (in p.u.-sec2/rad), and di is the damping coeï¬ƒcient of the\ngenerator at bus i (in p.u.-sec/rad). The electrical real power injection pei depends\non the voltage angle diï¬€erence in a nonlinear way, as governed by the AC power ï¬‚ow\nSTABILITY-CERTIFIED REINFORCEMENT LEARNING\n25\nequation:\n(4.7)\npei =\nn\nX\nj=1\n|vi||vj| (gij cos(Î¸i âˆ’Î¸j) + bij sin(Î¸i âˆ’Î¸j))\nwhere n is the number of buses in the system, gij and bij are the conductance and\nsusceptance of the transmission line that connects buses i and j, vi is the voltage phasor\nat bus i, and |vi| is its voltage magnitude. Because the conductance gij is typically\nseveral orders of magnitude smaller than the susceptance bij, for the simplicity of\nmathematical treatment, we omit the cosine term and only keep the sine term that\naccounts for the majority of nonlinearity. Each generator needs to make decisions on\nthe value of the mechanical power pmi to inject in order to maintain the stability of\nthe power system.\n(a) Star-connected structure.\nG8#\nG10#\nG1#\nG2#\nG3#\nG9#\nG6#\nG5#\nG7#\nG4#\n30#\n2#\n1#\n3#\n39#\n4#\n5#\n7#\n8#\n9#\n18#\n37#\n26#\n6#\n31#\n11#\n12#\n14#\n13#\n10#\n32#\n17#\n27#\n16#\n15#\n28#\n24#\n29#\n38#\n35#\n22#\n21#\n19#\n23#\n36#\n33#\n34#\n20#\n25#\n(b) IEEE 39-bus system.\nFig. 7: Illustration of the frequency regulation problem for the New England power\nsystem. The communication among generators follows a star topology.\nLet the rotor angles and the frequency states be denoted as Î¸ =\n\u0002Î¸1\nÂ· Â· Â·\nÎ¸n\n\u0003âŠ¤\nand Ï‰ =\n\u0002Ï‰1\nÂ· Â· Â·\nÏ‰n\n\u0003âŠ¤, and the generator mechanical power injections be denoted\nas pm =\n\u0002pm1\nÂ· Â· Â·\npmn\n\u0003âŠ¤. Then, the state-space representation of the nonlinear\nsystem is given by:\n(4.8)\n\u0014 Ë™Î¸\nË™Ï‰\n\u0015\n=\n\u0014\n0\nI\nâˆ’M âˆ’1L\nâˆ’M âˆ’1D\n\u0015\n|\n{z\n}\nA\n\u0014Î¸\nÏ‰\n\u0015\n|{z}\nx\n+\n\u0014\n0\nM âˆ’1\n\u0015\n| {z }\nB\npm +\n\u0014 0\ng(Î¸)\n\u0015\n| {z }\ng(x)\nwhere g(Î¸) =\n\u0002g1(Î¸)\nÂ· Â· Â·\ngn(Î¸)\u0003âŠ¤with gi(Î¸) = Pn\nj=1\nbij\nmj ((Î¸i âˆ’Î¸j) âˆ’sin(Î¸i âˆ’Î¸j)),\nand M = diag ({mi}n\ni=1), D = diag ({di}n\ni=1), and L is a Laplacian matrix whose\nentries are speciï¬ed in [18, Sec. IV-B]. For linearization (also known as DC approxi-\nmation), the nonlinear part g(x) is assumed to be zero when the phase diï¬€erences are\nsmall [18, 17]. On the contrary, we deal with this term in the stability certiï¬cation to\ndemonstrate its capability of producing non-conservative results even for nonlinear\n26\nM. JIN AND J. LAVAEI\nsystems. Similar to the ï¬‚ight formation case, we assume that there exists a distributed\nnominal controller that stablizes the system. To conduct multi-agent RL, each con-\ntroller pmi is a neural network that takes the available phases and frequencies as the\ninput and determines the mechanical power injection at bus i. The main focus is to\nstudy the certiï¬ed-gradient bounds for each agent policy in this large-scale setting.\nStability certiï¬cate: Similar to the ï¬‚ight formation problem, the nonlinearities\nin g(x) are in the form of âˆ†Î¸ij âˆ’sin âˆ†Î¸ij, where âˆ†Î¸ij = Î¸i âˆ’Î¸j represents the phase\ndiï¬€erence, which has its slope restricted to [0, 1 âˆ’cos(Î¸)] for every âˆ†Î¸ij âˆˆ[âˆ’Î¸, Î¸] and\nthus can be treated using the Zames-Falb IQC. In the smoothness margin analysis,\nassume that Î¸ = Ï€\n3 , which requires the phase angle diï¬€erence to be within [âˆ’Ï€\n3 , Ï€\n3 ].\nThis is a large set of uncertainties that includes both normal and abnormal operational\nconditions. To study the stability of the multi-agent policies, we adopt a black-box\napproach by simply considering the input-output constraint. By simply applying the\nL2 constraint in (3.3), we can only certify stability for Lipschitz constants up to 0.4,\nas shown in Figure 8. Because the distributed control is sparse, we can leverage it by\nsetting the lower and upper bounds Î¾ij = Î¾ij = 0 for each agent i that does not utilize\nobservation j, and Î¾ij = âˆ’Î¾ij = l otherwise, where l is the Lipschitz constant to be\ncertiï¬ed. This information can be encoded in SDP(P, Î», Î³, Î¾) in (3.21), which can be\nsolved for L up to 0.6 (doubling the certiï¬cate provided by the L2 constraint).\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nLipschitz const.\n0\n50\n100\n150\n200\nCertified L2 gain\nL2\nInp. sp.\nInp. sp & out. nh.\nFig. 8: Certiï¬ed Lipschitz constants for the power system regulation task.\nDue to the problem nature, we further observe that for each agent, the partial\ngradient of the policy with respect to certain observations is primarily one-sided, as\nshown in Figure 9. With a band of Â±0.1, the partial gradients remain within either\n[âˆ’0.1, 1] or [âˆ’1, 0.1] throughout the learning process. This information is gleaned\nduring the learning phase, and we can incorporate it into the partial gradient bounds\n(e.g., Î¾ij = âˆ’0.1l and Î¾ij = l for agent i which exhibits positive gradient with respect\nto observation j) to extend the certiï¬cate up to 1.1.\nPolicy gradient RL: Similar to the ï¬‚ight formation task, we perform multi-agent\npolicy gradient RL. The learned neural network controller is implemented in a typical\ncontrol case, whose trajectories are shown in Figure 10. As can be seen, the RL\npolicies can regulate the frequencies more eï¬ƒciently than the nominal controller, with\na signiï¬cantly lowered cost (50.8 vs. 23.9). More importantly, we compare the cases\nof RL with and without regulating the Lipschitz constants in Figure 11. Without\nregulating the gradients, the RL is able to reach a performance slightly higher than its\nstability-certiï¬ed counterpart. However, after about iteration 500, the performance\nstarts to deteriorate (due to a possibly large gradient variance and high sensitivity to\nstep size) until it completely loses the previous gains and starts to break the system.\nSTABILITY-CERTIFIED REINFORCEMENT LEARNING\n27\nÎ¸i, i âˆˆ{1, ..., 10} Ï‰i, i âˆˆ{1, ..., 10}\nâˆ’1\n0\n1\nPartial grads\n(a) G10.\nÎ¸1 Î¸5 Ï‰1 Ï‰5\nâˆ’1\n0\n1\n(b) G4.\nÎ¸1 Î¸8 Ï‰1 Ï‰8\nâˆ’1\n0\n1\n(c) G7.\nFig. 9: Box plots of partial gradients of individual generators (G10, G4, G7) with\nrespect to local information. Grey dashed lines indicate Â±0.1.\n0\n6\n12\n18\nTime (sec)\nâˆ’0.25\n0.00\n0.25\n0.50\nStates\nPhase Î¸i\nFrequency Ï‰i\n(a) Nom.: phase Î¸i and frequency Ï‰i.\n0\n6\n12\n18\nTime (sec)\nâˆ’0.8\nâˆ’0.4\n0.0\nActions\nG1\nG2\nG3\nG4\nG5\nG6\nG7\nG8\nG9\nG10\n(b) Nom.: actions.\n0\n6\n12\n18\nTime (sec)\nâˆ’0.25\n0.00\n0.25\n0.50\nStates\nPhase Î¸i\nFrequency Ï‰i\n(c) NN: phase Î¸i and frequency Ï‰i.\n0\n6\n12\n18\nTime (sec)\nâˆ’0.8\nâˆ’0.4\n0.0\nActions\nG1\nG2\nG3\nG4\nG5\nG6\nG7\nG8\nG9\nG10\n(d) NN: actions.\nFig. 10: State and action trajectories of the nominal and neural network controllers\nfor power system frequency regulation, with costs of 50.8 and 23.9, respectively.\nThis intolerable behavior is due to the large Lipschitz gains that grow unboundedly,\nas shown in Figure 12. In comparison, RL with regulated gradient bounds is able to\nmake a substantial improvement, and also exhibits a more stable behavior.\n5. Conclusions. In this paper, we focused on the challenging task of ensuring\nthe stability of reinforcement learning in real-world dynamical systems. By solving the\nproposed SDP feasibility problem, we can oï¬€er a preventative certiï¬cate of stability\nfor a broad class of neural network controllers with bounded gradients. Furthermore,\nwe analyzed the (non)conservatism of the certiï¬cate, which was demonstrated in the\nempirical investigation of decentralized nonlinear control tasks, including multi-agent\nï¬‚ight formation and power grid frequency regulation. Results indicated that the set of\nstability-certiï¬ed controllers was signiï¬cantly larger than what the existing approaches\n28\nM. JIN AND J. LAVAEI\n0\n500\n1000\nIterations\n60\n35\nRewards\n1-layer NN\n5-layer NN, SP\n1-layer NN, SP\n1-layer NN, HT\nFig. 11: Long-term performance of RL for agents with regulated gradients by soft\npenalty (SP), which adaptively adjusts the coeï¬ƒcients Ï‰2 in (2.8), and hard threshold-\ning (HT), which shrinks the network last layer to satisfy the gradient bounds. The RL\nagents without regulating the gradients exhibit â€œdangerousâ€ behaviors in the long run.\n0\n500\n1000\nIterations\n0\n20\nLip. const.\n(a) No gradient regulation.\n0\n500\n1000\nIterations\n0.5\n1.0\nLip. const.\nG1\nG2\nG3\nG4\nG5\nG6\nG7\nG8\nG9\nG10\n(b) Gradient regulation.\nFig. 12: Trajectories of Lipschitz constants with and without regulation.\ncan oï¬€er, and that the RL agents can substantially improve the performance of nominal\ncontrollers while staying within the safe set. Most importantly, regulation of gradient\nbounds was able to improve on-policy learning stability and avoid â€œcatastropicâ€ eï¬€ects\ncaused by the unregulated high gains. The present study represents a key step towards\nsafe deployment of reinforcement learning in mission-critical real-world systems.\nREFERENCES\n[1] J. Achiam, D. Held, A. Tamar, and P. Abbeel, Constrained policy optimization, in Proc. of\nthe International Conference on Machine Learning, 2017, pp. 22â€“31.\n[2] A. K. Akametalu, J. F. Fisac, J. H. Gillula, S. Kaynama, M. N. Zeilinger, and C. J.\nTomlin, Reachability-based safe learning with Gaussian processes, in Proc. of the IEEE\nConference on Decision and Control, 2014, pp. 1424â€“1431.\n[3] S.-I. Amari, Natural gradient works eï¬ƒciently in learning, Neural computation, 10 (1998),\npp. 251â€“276.\n[4] D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. ManÂ´e, Concrete\nproblems in AI safety, arXiv preprint arXiv:1606.06565, (2016).\n[5] C. W. Anderson, P. M. Young, M. R. Buehner, J. N. Knight, K. A. Bush, and D. C.\nHittle, Robust reinforcement learning control using integral quadratic constraints for\nrecurrent neural networks, IEEE Transactions on Neural Networks, 18 (2007), pp. 993â€“1002.\n[6] M. Arcak, Synchronization and pattern formation in diï¬€usively coupled systems, in Proc. of\nthe IEEE Conference on Decision and Control, 2012, pp. 7184â€“7192.\n[7] A. Aswani, H. Gonzalez, S. S. Sastry, and C. Tomlin, Provably safe and robust learning-based\nmodel predictive control, Automatica, 49 (2013), pp. 1216â€“1226.\nSTABILITY-CERTIFIED REINFORCEMENT LEARNING\n29\n[8] L. Bakule, Decentralized control: An overview, Annual reviews in control, 32 (2008), pp. 87â€“98.\n[9] F. Berkenkamp, M. Turchetta, A. Schoellig, and A. Krause, Safe model-based reinforce-\nment learning with stability guarantees, in Advances in Neural Information Processing\nSystems, 2017, pp. 908â€“919.\n[10] R. Bobiti and M. Lazar, A sampling approach to ï¬nding lyapunov functions for nonlinear\ndiscrete-time systems, in Proc. of the IEEE European Control Conference, 2016, pp. 561â€“566.\n[11] S. Boyd, L. El Ghaoui, E. Feron, and V. Balakrishnan, Linear matrix inequalities in\nsystem and control theory, vol. 15, SIAM, 1994.\n[12] L. BusÂ¸oniu, R. BabuË‡ska, and B. De Schutter, Multi-agent reinforcement learning: An\noverview, in Innovations in multi-agent systems and applications-1, Springer, 2010, pp. 183â€“\n221.\n[13] F. H. Clarke, Optimization and nonsmooth analysis, vol. 5, SIAM, 1990.\n[14] S. P. Coraluppi and S. I. Marcus, Risk-sensitive and minimax control of discrete-time,\nï¬nite-state Markov decision processes, Automatica, 35 (1999), pp. 301â€“309.\n[15] H. Drucker and Y. Le Cun, Improving generalization performance using double backpropaga-\ntion, IEEE Transactions on Neural Networks, 3 (1992), pp. 991â€“997.\n[16] G. E. Dullerud and F. Paganini, A course in robust control theory: a convex approach,\nvol. 36, Springer Science & Business Media, 2013.\n[17] S. Fattahi, G. Fazelnia, J. Lavaei, and M. Arcak, Transformation of optimal centralized\ncontrollers into near-globally optimal static distributed controllers, IEEE Transactions on\nAutomatic Control, (2018), pp. 1â€“1, https://doi.org/10.1109/TAC.2018.2829473.\n[18] G. Fazelnia, R. Madani, A. Kalbat, and J. Lavaei, Convex relaxation for optimal distributed\ncontrol problems, IEEE Transactions on Automatic Control, 62 (2017), pp. 206â€“221.\n[19] J. M. Fry, M. Farhood, and P. Seiler, IQC-based robustness analysis of discrete-time linear\ntime-varying systems, International Journal of Robust and Nonlinear Control, 27 (2017),\npp. 3135â€“3157.\n[20] J. GarcÄ±a and F. FernÂ´andez, A comprehensive survey on safe reinforcement learning, Journal\nof Machine Learning Research, 16 (2015), pp. 1437â€“1480.\n[21] P. Geibel and F. Wysotzki, Risk-sensitive reinforcement learning applied to control under\nconstraints, Journal of Artiï¬cial Intelligence Research, 24 (2005), pp. 81â€“108.\n[22] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville, Improved\ntraining of wasserstein gans, in Advances in Neural Information Processing Systems, 2017,\npp. 5767â€“5777.\n[23] J. Hauser, S. Sastry, and G. Meyer, Nonlinear control design for slightly non-minimum\nphase systems: Application to v/stol aircraft, Automatica, 28 (1992), pp. 665â€“679.\n[24] W. P. Heath and A. G. Wills, Zames-Falb multipliers for quadratic programming, in Proc. of\nthe IEEE Conference on Decision and Control, 2005, pp. 963â€“968.\n[25] S. Kakade and J. Langford, Approximately optimal approximate reinforcement learning, in\nProc. of the International Conference on Machine Learning, 2002, pp. 267â€“274.\n[26] S. M. Kakade, A natural policy gradient, in Advances in neural information processing systems,\n2002, pp. 1531â€“1538.\n[27] H. K. Khalil, Noninear systems, Prentice-Hall, New Jersey, 2 (1996), pp. 5â€“1.\n[28] R. M. Kretchmara, P. M. Young, C. W. Anderson, D. C. Hittle, M. L. Anderson, and\nC. Delnero, Robust reinforcement learning control, in Proc. of the IEEE American Control\nConference, vol. 2, 2001, pp. 902â€“907.\n[29] L. Lessard, B. Recht, and A. Packard, Analysis and design of optimization algorithms via\nintegral quadratic constraints, SIAM Journal on Optimization, 26 (2016), pp. 57â€“95.\n[30] S. Levine and P. Abbeel, Learning neural network policies with guided policy search under\nunknown dynamics, in Advances in Neural Information Processing Systems, 2014, pp. 1071â€“\n1079.\n[31] Y. Li, Deep reinforcement learning: An overview, arXiv preprint arXiv:1701.07274, (2017).\n[32] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver,\nand D. Wierstra, Continuous control with deep reinforcement learning, arXiv preprint\narXiv:1509.02971, (2015).\n[33] A. Megretski and A. Rantzer, System analysis via integral quadratic constraints, IEEE\nTransactions on Automatic Control, 42 (1997), pp. 819â€“830.\n[34] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and\nK. Kavukcuoglu, Asynchronous methods for deep reinforcement learning, in Proc. of the\nInternational Conference on Machine Learning, 2016, pp. 1928â€“1937.\n[35] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra,\nand M. Riedmiller, Playing atari with deep reinforcement learning, arXiv preprint\narXiv:1312.5602, (2013).\n30\nM. JIN AND J. LAVAEI\n[36] T. M. Moldovan and P. Abbeel, Safe exploration in Markov decision processes, in Proc. of\nthe International Conference on Machine Learning, 2012, pp. 1451â€“1458.\n[37] A. G. Ororbia II, D. Kifer, and C. L. Giles, Unifying adversarial training algorithms with\ndata gradient regularization, Neural computation, 29 (2017), pp. 867â€“887.\n[38] A. Packard and J. Doyle, The complex structured singular value, Automatica, 29 (1993),\npp. 71â€“109.\n[39] T. J. Perkins and A. G. Barto, Lyapunov design for safe reinforcement learning, Journal of\nMachine Learning Research, 3 (2002), pp. 803â€“832.\n[40] M. G. Safonov and V. V. Kulkarni, Zames-Falb multipliers for MIMO nonlinearities, in\nProc. of the American Control Conference, vol. 6, 2000, pp. 4144â€“4148.\n[41] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, Trust region policy\noptimization, in Proc. of the International Conference on Machine Learning, 2015, pp. 1889â€“\n1897.\n[42] P. Seiler, Stability analysis with dissipation inequalities and integral quadratic constraints,\nIEEE Transactions on Automatic Control, 60 (2015), pp. 1704â€“1709.\n[43] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrit-\ntwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al., Mastering the game\nof go with deep neural networks and tree search, Nature, 529 (2016), p. 484.\n[44] I. Stoica, D. Song, R. A. Popa, D. Patterson, M. W. Mahoney, R. Katz, A. D. Joseph,\nM. Jordan, J. M. Hellerstein, J. E. Gonzalez, et al., A Berkeley view of systems\nchallenges for AI, arXiv preprint arXiv:1712.05855, (2017).\n[45] Y. Sui, A. Gotovos, J. Burdick, and A. Krause, Safe exploration for optimization with\nGaussian processes, in Proc. of the International Conference on Machine Learning, 2015,\npp. 997â€“1005.\n[46] R. S. Sutton, Integrated architecture for learning, planning, and reacting based on approximat-\ning dynamic programming, in Proc. of the International Conference on Machine Learning,\n1990, pp. 216â€“224.\n[47] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction, vol. 1, MIT press\nCambridge, 1998.\n[48] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and\nR. Fergus, Intriguing properties of neural networks, in Proc. of the International Conference\non Learning Representations, 2014.\n[49] C. Watkins and P. Dayan, Q-learning, Machine learning, 8 (1992), pp. 279â€“292.\n[50] W. Wiesemann, D. Kuhn, and B. Rustem, Robust Markov decision processes, Mathematics of\nOperations Research, 38 (2013), pp. 153â€“183.\n[51] J. C. Willems, Dissipative dynamical systems, Part II: Linear systems with quadratic supply\nrates, Archive for rational mechanics and analysis, 45 (1972), pp. 352â€“393.\n[52] R. J. Williams, Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning, Machine learning, 8 (1992), pp. 229â€“256.\n[53] G. Zames and P. Falb, Stability conditions for systems with monotone and slope-restricted\nnonlinearities, SIAM Journal on Control, 6 (1968), pp. 89â€“108.\n[54] A. Zemouche and M. Boutayeb, On LMI conditions to design observers for lipschitz nonlinear\nsystems, Automatica, 49 (2013), pp. 585â€“591.\n[55] K. Zhou, J. C. Doyle, K. Glover, et al., Robust and optimal control, vol. 40, Prentice hall\nNew Jersey, 1996.\n",
  "categories": [
    "cs.SY",
    "cs.LG"
  ],
  "published": "2018-10-26",
  "updated": "2018-10-26"
}