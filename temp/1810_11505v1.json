{
  "id": "http://arxiv.org/abs/1810.11505v1",
  "title": "Stability-certified reinforcement learning: A control-theoretic perspective",
  "authors": [
    "Ming Jin",
    "Javad Lavaei"
  ],
  "abstract": "We investigate the important problem of certifying stability of reinforcement\nlearning policies when interconnected with nonlinear dynamical systems. We show\nthat by regulating the input-output gradients of policies, strong guarantees of\nrobust stability can be obtained based on a proposed semidefinite programming\nfeasibility problem. The method is able to certify a large set of stabilizing\ncontrollers by exploiting problem-specific structures; furthermore, we analyze\nand establish its (non)conservatism. Empirical evaluations on two decentralized\ncontrol tasks, namely multi-flight formation and power system frequency\nregulation, demonstrate that the reinforcement learning agents can have high\nperformance within the stability-certified parameter space, and also exhibit\nstable learning behaviors in the long run.",
  "text": "STABILITY-CERTIFIED REINFORCEMENT LEARNING: A\nCONTROL-THEORETIC PERSPECTIVE∗\nMING JIN† AND JAVAD LAVAEI‡\nAbstract. We investigate the important problem of certifying stability of reinforcement learning\npolicies when interconnected with nonlinear dynamical systems. We show that by regulating the\ninput-output gradients of policies, strong guarantees of robust stability can be obtained based on a\nproposed semideﬁnite programming feasibility problem. The method is able to certify a large set of\nstabilizing controllers by exploiting problem-speciﬁc structures; furthermore, we analyze and establish\nits (non)conservatism. Empirical evaluations on two decentralized control tasks, namely multi-ﬂight\nformation and power system frequency regulation, demonstrate that the reinforcement learning agents\ncan have high performance within the stability-certiﬁed parameter space, and also exhibit stable\nlearning behaviors in the long run.\nKey words. Reinforcement learning, robust control, policy gradient optimization, decentralized\ncontrol synthesis, safe reinforcement learning\nAMS subject classiﬁcations. 68T05, 93E35, 93D09\n1. Introduction. Remarkable progress has been made in reinforcement learning\n(RL) using (deep) neural networks to solve complex decision-making and control\nproblems [43]. While RL algorithms, such as policy gradient [52, 26, 41], Q-learning\n[49, 35], and actor-critic methods [32, 34] aim at optimizing control performance, the\nsecurity aspect is of great importance for mission-critical systems, such as autonomous\ncars and power grids [20, 4, 44]. A fundamental problem is to analyze or certify\nstability of the interconnected system in both RL exploration and deployment stages,\nwhich is challenging due to its dynamic and nonconvex nature [20].\nThe problem under study focuses on a general continuous-time dynamical system:\n(1.1)\n˙x(t) = ft(x(t), u(t)),\nwith the state x(t) ∈Rns and the control action u(t) ∈Rna. In general, ft can be\na time-varying and nonlinear function, but for the purpose of stability analysis, we\nstudy the important case that\n(1.2)\nft(x(t)) = Ax(t) + Bu(t) + gt(x(t)),\nwhere ft comprises of a linear time-invariant (LTI) component A ∈Rns×ns that is\nHurwitz (i.e., every eigenvalue of A has strictly negative real part), a control matrix\nB ∈Rns×na, and a slowly time-varying component gt that is allowed to be nonlinear\nand even uncertain.1 The condition that A is stable is a basic requirement, but the\ngoal of reinforcement learning is to design a controller that optimizes some performance\nmetric that is not necessarily related to the stability condition. For feedback control,\nwe also allow the controller to obtain observations y(t) = Cx(t) ∈Rns that are a linear\nfunction of the states, where C ∈Rns×ns may have a sparsity pattern to account for\npartial observations in the context of decentralized controls [8].\n∗This work was supported by the ONR grants N00014-17-1-2933 and N00014-15-1-2835, DARPA\ngrant D16AP00002, and AFOSR grant FA9550-17-1-0163.\n†Department of Industrial Engineering and Operations Research, University of California, Berkeley.\nEmail: jinming@berkeley.edu.\n‡Department of Industrial Engineering and Operations Research, and the Tsinghua-Berkeley\nShenzhen Institute, University of California, Berkeley. Email: lavaei@berkeley.edu.\n1This requirement is not diﬃcult to meet in practice, because one can linearize any nonlinear\nsystems around the equilibrium point to obtain a linear component and a nonlinear part.\n1\narXiv:1810.11505v1  [cs.SY]  26 Oct 2018\n2\nM. JIN AND J. LAVAEI\nEnvironment\n𝐺\n𝑥\nRL policy 𝜋(𝑦; 𝜃)\nReward 𝑟\nAction 𝑤\n+\nInput 𝑢\nOutput 𝑦\nExploration 𝑒\nFig. 1: Overview of the interconnected system of an RL policy and the environment.\nThe goal of RL is to maximize expected rewards through interaction and exploration.\nSuppose that u(t) = πt(y(t); θt) + e(t) is a neural network given by an RL agent\n(parametrized by θt, which can be time-varying due to learning) to optimize some\nreward r(x, u) revealed through the interaction with the environment. The exploration\nvector e(t) ∈Rna captures the additive randomization eﬀect during the learning phase,\nand is assumed to have a bounded energy over time (∥e∥2 =\nqR\n|e(t)|2\n2dt ≤∞). The\nmain goal is to analyze the stability of the system with the actuation of πt, which\nis typically a neural network controller, as illustrated in Figure 1. Speciﬁcally, the\nstability criterion is stated using the concept of L2 gain [55, 16].2\nDefinition 1.1 (Input-output stability). The L2 gain of the system G controlled\nby π is the worst-case ratio between total output energy and total input energy:\n(1.3)\nγ(G, π) = sup\nu∈L2\n∥y∥2\n∥u∥2\n,\nwhere L2 is the set of all square-summable signals, ∥y∥2 =\nqR\n|y(t)|2\n2dt is the total\nenergy over time, and u(t) = πt(y(t); θt) + e(t) is the control input with exploration. If\nγ(G, π) is ﬁnite, then the interconnected system is said to have input-output stability\n(or ﬁnite L2 gain).\nThis study investigates the possibility of using the gradient information of the\npolicy πt(y(t); θt) to obtain a stability certiﬁcate, because this information can be\neasily extracted in real-time and is generic enough to include a large set of performance-\noptimizing nonlinear controllers. Let [n] = {1, ..., n} be the set notation. By denoting\n(1.4)\nP(ξ) =\nn\nπ\n\f\f\f ξij ≤∂jπi(y) ≤ξij, ∀i ∈[na], j ∈[ns], y ∈Rnso\nas the set of controllers whose partial derivatives are bounded by ξ ∈Rna×ns and\nξ ∈Rna×ns, it is desirable to provide stability certiﬁcate as long as the RL policy\nremains within the above “safety set.” Indeed, this can be checked eﬃciently, as stated\n(informally) in the following theorem.\n2This stability metric is widely adopted in practice, and is closely related to bounded-input\nbounded-output (BIBO) stability and absolute stability (or asymptotic stability). For controllable\nand observable LTI systems, the equivalence can be established.\nSTABILITY-CERTIFIED REINFORCEMENT LEARNING\n3\nTheorem 1.2 (Main result). If there exist constants ξ and ξ such that the condi-\ntion (3.21) is feasible for the system (1.1), then the interconnected system has a ﬁnite\nL2 gain as long as πt ∈P(ξ, ξ) for all t ≥0.\nWe call the constants ξ and ξ stability-certiﬁed gradient bounds for the underlying\nsystem. The above result is based on the intuition that a real-world stable controller\nshould exhibit “smoothness” in the sense that small changes in the input should lead\nto small changes in the output. This incorporates the special case where controllers\nare known to have bounded Lipschitz constants (a simple strategy to calculate the\nLipschitz constant of a deep neural network is suggested in [48]). To compute the\ngradient bounds, we borrow powerful ideas from the framework of integral quadratic\nconstraint (in frequency domain) [33] and dissipativity theory (in time domain) [51]\nfor robustness analysis. While these tools are celebrated with their non-conservatism\nin the robust control literature, existing characterizations of multi-input multi-output\n(MIMO) Lipschitz functions are insuﬃcient. Thus, one major obstacle is to derive\nnon-trivial bounds that could be of use in practice.\nTo this end, we develop a new quadratic constraint on gradient-bounded functions,\nwhich exploits the sparsity of the control architecture and the non-homogeneity of\nthe output vector. Some key features of the stability-certiﬁed smoothness bounds\nare as follows: (a) the bounds are inherent to the targeted real-world control task;\n(b) they can be computed eﬃciently by solving some semi-deﬁnite programming\n(SDP) problem; (c) they can be used to certify stability when reinforcement learning\nis employed in real-world control with either oﬀ-policy or on-policy learning [47].\nFurthermore, the stability certiﬁcation can be regarded as an S-procedure, and we\nanalyze its conservatism to show that it is necessary for the robustness of a surrogate\nsystem that is closely related to the original system.\nThe paper is organized as follows. Preliminaries on policy gradient reinforcement\nlearning, the integrated quadratic constraint (IQC) and dissipativity frameworks are\npresented in Section 2. Main results on gradient bounds for a linear or nonlinear\nsystem G are presented in Section 3, where we also analyze the conservatism of the\ncertiﬁcate. The method is evaluated in Section 4 on two nonlinear decentralized control\ntasks. Conclusions are drawn in Section 5.\n2. Preliminary. In this section, we give an overview of the main topics relevant\nto this study, namely policy gradient reinforcement learning and robustness analysis\nbased on IQC framework and dissipativity theory.\n2.1. Reinforcement learning using policy gradient. Reinforcement learning\naims at guiding an agent to perform a task as eﬃciently and skillfully as possible\nthrough interactions with the environment. The control task is modeled as a Markov\ndecision process (MDP), deﬁned by the tuple (X, U, T , r, ρ), where X is the set of\nstates x, U is a set of actions u, T : X × U →X indicates the world dynamics as\nin (1.1), r(x, u) is the reward at state x and action u, and ρ ∈(0, 1] is the factor to\ndiscount future rewards. A control strategy is deﬁned by a policy πθ(x), which can\nbe approximated by a neural network with parameters θ. For a continuous control,\nthe actions follow a multivariate normal distribution, where πθ(x) is the mean, and\nthe standard deviation in each action dimension is set to be a diminishing number\nduring exploration or learning, and 0 during actual deployment. With a slight abuse\nof notations, we use πθ(u|x) to denote this normal distribution over actions, and use\n4\nM. JIN AND J. LAVAEI\nxt to denote x(t) for simplicity. The goal of RL is to maximize the expected return:\n(2.1)\nη(πθ) =\nE\nx0,ut∼πθ(·|xt),xt+1∼T (xt,ut)\n\u0014XT\nt=0 ρtr(xt, ut)\n\u0015\n,\nwhere T is the control horizon, and the expectation is taken over the policy, the initial\nstate distribution and the world dynamics.\nFrom a practitioner’s point of view, the existing methods can be categorized\ninto four groups based on how the optimal policy is determined: (a) policy gradient\nmethods directly optimize the policy parameters θ by estimating the gradient of the\nexpected return (e.g., REINFORCE [52], natural policy gradient [26], and trust region\npolicy optimization (TRPO) [41]); (b) value-based algorithms like Q-learning do not\naim at optimizing the policy directly, but instead approximate the Q-value of the\noptimal policy for the available actions [49, 35]; (c) actor-critic algorithms keep an\nestimate of the value function (critic) as well as a policy that maximizes the value\nfunction (actor) (e.g., DDPG [32] and A3C [34]); lastly, (d) model-based methods\nfocus on the learning of the transition model for the underlying dynamics, and then use\nit for planning or to improve a policy (e.g., Dyna [46] and guided policy search [30]).\nWe adopt an approach based on end-to-end policy gradient that combines TRPO [41]\nwith natural gradient [26] and smoothness penalty (this method is very useful for RL\nin dynamical systems described by partial or diﬀerence equations).\nTrust region policy optimization is a policy gradient method that constrains\nthe step length to be within a “trust region” so that the local estimation of the\ngradient/curvature has a monotonic improvement guarantee. By manipulating the\nexpected return η(π) using the identity proposed in [25], the “surrogate objective”\nLπold(π) can be designed:\n(2.2)\nLπold(π) =\nE\nx,u∼πold\n\u0014 π(u|x)\nπold(u|x)Λπold(x, u)\n\u0015\n,\nwhere the expectation is taken over the old policy πold, the ratio inside the expectation\nis also known as the importance weight, and Λπold(x, u) is the advantage function\ngiven by:\n(2.3)\nΛπold(x, u) =\nE\nx′∼T (x,u) [r(x, u) + ρV πold(x′) −V πold(x)] ,\nwhere the expectation is with respect to the dynamics x′ ∼T (x, u) (the dependence\non θold is omitted), and it measures the improvement of taking action u at state x\nover the old policy in terms of the value function V πold. A bound on the diﬀerence\nbetween Lπold(π) and η(π) has been derived in [41], which also proves a monotonic\nimprovement result as long as the KL divergence between the new and old policies is\nsmall (i.e., the new policy stays within the trust region). In practice, the surrogate\nloss Lπold(π) can be estimated using trajectories sampled from πold as follows,\n(2.4)\nbLπold(π) =\nX\nt\nπ(ut|xt)\nπold(ut|xt)\nbΛπold(x, u),\nand the averaged KL divergence over observed states 1\nT\nP\nt KL [πold(·|xt), π(·|xt)] can\nbe used to estimate the trust region.\nNatural gradient is deﬁned by a metric based on the probability manifold\ninduced by the KL divergence. It improves the standard gradient by making a step\nSTABILITY-CERTIFIED REINFORCEMENT LEARNING\n5\ninvariant to reparametrization of the parameter coordinates [3]:\n(2.5)\nθt+1 ←θt −λH−1\nθ ζt,\nwhere ζt is the standard gradient, Hθ =\n1\nT\nP\nt\n\u0000 ∂\n∂θπθ(log ut|xt)\n\u0001 \u0000 ∂\n∂θ log πθ(ut|xt)\n\u0001⊤\nis the Fisher information matrix estimated with the trajectory data, and λ is the\nstep size. In practice, when the number of parameters is large, conjugate gradient is\nemployed to estimate the term H−1\nθ ζt without requiring any matrix inversion. Since\nthe Fisher information matrix coincides with the second-order approximation of the\nKL divergence, one can perform a back-tracking line search on the step size λ to ensure\nthat the updated policy stays within the trust region.\nSmoothness penalty is introduced in this study to empirically improve learning\nperformance on physical dynamical systems. Speciﬁcally, we propose to use\n(2.6)\nLexplore =\nXT\nt=1 ∥ut−1 −πθ(xt)∥2\nas a regularization term to induce consistency during exploration. The intuition is\nthat since the change in states between two consecutive time steps is often small, it is\ndesirable to ensure small changes in output actions. This is closely related to another\npenalty term that has been used in [15], which is termed “double backpropagation”,\nand recently rediscovered in [37, 22]:\n(2.7)\nLsmooth =\nXT\nt=1\n\r\r\r\r\n∂\n∂θπθ(xt)\n\r\r\r\r\n2\n,\nwhich penalizes the gradient of the policy along the trajectories. Since bounded\ngradients lead to bounded Lipshitz constant, these penalties will induce smooth neural\nnetwork functions, which is essential to ensure generalizability and, as we will show,\nstability. In addition, we incorporate a hard threshold (HT) approach that rescales\nthe weight matrices at each layer by (l◦/l(πθ))1/nL if l(πθ) > l◦, where l(πθ) is the\nLipschitz constant of the neural network πθ, nL is the number of layers of the neural\nnetwork and l◦is the certiﬁed Lipschitz constant. This ensures that the Lipschitz\nconstant of the RL policy remains bounded by l◦.\nIn summary, our policy gradient is based on the weighted objective:\n(2.8)\nLpol(πθ) = bLπold(πθ) + w1Lexplore(πθ) + w2Lsmooth(πθ),\nwhere the penalty coeﬃcients w1 and w2 are selected such that the scales of the\ncorresponding terms are about [0.01, 0.05] of the surrogate loss value bLπold(πθ). In\neach round, a set of trajectories are collected using πold, which are used to estimate\nthe gradient\n∂\n∂θLpol(πθ) and the Fisher information matrix Hθ; a backtracking line\nsearch on the step size is then conducted to ensure that the updated policy stays\nwithin the trust region. This learning procedure is known as on-policy learning [47].\n2.2. Overview of IQC framework. The IQC theory is celebrated for sys-\ntematic and eﬃcient stability analysis of a large class of uncertain, dynamic, and\ninterconnected systems [33]. It uniﬁes and extends classical passitivity-based multiplier\ntheory, and has close connections to dissipativity theory in the time domain [42].\nTo state the IQC framework, some terminologies are necessary. We deﬁne the\nspace Ln\n2[0, ∞) = {x :\nR ∞\nt=0 |x(t)|2\n2dt < ∞} for signals supported on t ≥0, where\nn denotes the spatial dimension of x(t), and the extended space Ln\n2e[0, ∞) = {x :\n6\nM. JIN AND J. LAVAEI\nR T\nt=0 |x(t)|2\n2dt < ∞, ∀T ≥0} (we will use L2 and L2e if it is not necessary to specify\nthe dimension and signal support), where we use x to denote the signal in general\nand x(t) to denote its value at time t. For a vector or matrix, we use superscript ∗to\ndenote its conjugate transpose. An operator is causal if the current output does not\ndepend on future inputs. It is bounded if it has a ﬁnite L2 gain. Let Φ : H →H be a\nbounded linear operator on a Hilbert space. Then, its Hilbert adjoint is the operator\nΦ∗: H →H such that ⟨Φx, y⟩= ⟨x, Φ∗y⟩for all x, y ∈H, where ⟨·, ·⟩denotes the\ninner product. It is self-adjoint if Φ = Φ∗.\nConsider the system (see also Figure 1)\ny = G(u)\n(2.9)\nu = ∆(y) + e,\n(2.10)\nwhere G is the transfer function of a causal and bounded LTI system (i.e., it maps input\nu ∈Lna to output y ∈Lno through the internal state dynamics ˙x = Ax(t) + Bu(t)),\ne ∈Lna is the disturbance, and ∆: Lno →Lna is a bounded and causal function that\nis used to represent uncertainties in the system. IQC provides a framework to treat\nuncertainties such as nonlinear dynamics, model approximation and identiﬁcation\nerrors, time-varying parameters and disturbance noise, by using their input-output\ncharacterizations.\nDefinition 2.1 (Integral quadratic constraints). Consider the signals w ∈L2\nand y ∈L2 associated with Fourier transforms ˆw and ˆy, and w = ∆(y), where ∆is a\nbounded and causal operator. We present both the frequency- and time-domain IQC\ndeﬁnitions:\n(a) (Frequency domain) Let Π be a bounded and self-adjoint operator. Then, ∆is\nsaid to satisfy the IQC deﬁned by Π (i.e., ∆∈IQC(Π)) if:\n(2.11)\nσΠ(ˆy, ˆw) =\nZ ∞\n−∞\n\u0014\nˆy(jω)\nˆw(jω)\n\u0015∗\nΠ(jω)\n\u0014 ˆy(jω)\nˆw(jω)\n\u0015\ndω ≥0.\n(b) (Time domain) Let (Ψ, M) be any factorization of Π = Ψ∗MΨ such that Ψ\nis stable and M = M ⊤. Then, ∆is said to satisfy the hard IQC deﬁned by\n(Ψ, M) (i.e., ∆∈IQC(Ψ, M)) if:\n(2.12)\nZ T\n0\nz(t)⊤Mz(t)dt ≥0,\n∀T ≥0,\nwhere z = Ψ\n\u0014y\nw\n\u0015\nis the ﬁltered output given by the stable operator Ψ. If instead\nof requiring nonnegativity at each time T, the nonnegativity is considered only\nwhen T →∞, then the corresponding condition is called soft IQC.\nAs established in [42], the time- and frequency-domain IQC deﬁnitions are equiva-\nlent if there exists Π = Ψ∗MΨ as a spectral factorization of Π with M =\n\u0014\nI\n0\n0\n−I\n\u0015\nsuch that Ψ and Ψ−1 are stable.\nExample 2.2 (Sector IQC). A single-input single-output uncertainty ∆: R →R\nis called “sector bounded” between [α, β] if αy(t) ≤∆(y(t)) ≤βy(t), for all y ∈R and\nt ≥0. It thus satisﬁes the sector IQC(Ψ, M) with Ψ = I and M =\n\u0014−2αβ\nα + β\nα + β\n−2\n\u0015\n.\nIt also satisﬁes IQC(Π) with Π = M deﬁned above.\nSTABILITY-CERTIFIED REINFORCEMENT LEARNING\n7\nExample 2.3 (L2 gain bound). A MIMO uncertainty ∆: Rn →Rm has the L2\ngain γ if\nR ∞\n0\n∥w(t)∥2dt ≤γ2 R ∞\n0\n∥y(t)∥2dt, where w(t) = ∆(y(t)). Thus, it satisﬁes\nIQC(Ψ, M) with Ψ = In+m and M =\n\u0014λγ2In\n0\n0\n−λIm\n\u0015\n, where λ > 0. It also satisﬁes\nIQC(Π) with Π = M deﬁned above.\nThis can be used to characterize nonlinear\noperators with fast time-varying parameters.\nBefore stating a stability result, we deﬁne the system (2.9)–(2.10) (see Figure 1)\nto be well-posed if for any e ∈L2e, there exists a solution u ∈L2e, which depends\ncausally on e. A main IQC result for stability is stated below:\nTheorem 2.4 ([33]). Consider the interconnected system (2.9)–(2.10). Assume\nthat: (i) the interconnected system (G, τ∆) is well posed for all τ ∈[0, 1]; (ii)\nτ∆∈IQC(Π) for τ ∈[0, 1]; and (iii) there exists ϵ > 0 such that\n(2.13)\n\u0014 ˆG(jω)\nI(jω)\n\u0015∗\nΠ(jω)\n\u0014 ˆG(jω)\nI(jω)\n\u0015\n≤−ϵI,\n∀ω ∈[0, ∞).\nThen, the system (2.9)–(2.10) is input-output stable (i.e., ﬁnite L2 gain).\nThe above theorem requires three technical conditions. The well-posedness condi-\ntion is a generic property for any acceptable model of a physical system. The second\ncondition is implied if Π =\n\u0014Π11\nΠ12\nΠ∗\n12\nΠ22\n\u0015\nhas the properties Π11 ⪰0 and Π22 ⪯0. The\nthird condition is central, and it requires checking the feasibility at every frequency,\nwhich represents a main obstacle. As discussed in Section Section 3.2, this condition\ncan be equivalently represented as a linear matrix inequality (LMI) using the Kalman-\nYakubovich-Popov (KYP) lemma. In general, the more IQCs exist for the uncertainty,\nthe better characterization can be obtained. If ∆∈IQC(Πk), k ∈[nK], where nK is\nthe number of IQCs satisﬁed by ∆, then it is easy to show that ∆∈IQC(PnK\nk=1 τkΠk),\nwhere τk ≥0; thus, the stability test (2.13) becomes a convex program, i.e., to ﬁnd\nτk ≥0 such that:\n(2.14)\n\u0014 ˆG(jω)\nI(jω)\n\u0015∗ nK\nX\nk=1\nτkΠk(jω)\n! \u0014 ˆG(jω)\nI(jω)\n\u0015\n≤−ϵI, ∀ω ∈[0, ∞).\nThe counterpart for the frequency-domain stability condition in the time-domain\ncan be stated using a standard dissipation argument [42].\n2.3. Related work. To close this section, we summarize some connections\nto existing literature. This work is closely related to the body of works on safe\nreinforcement learning, deﬁned as the process of learning policies that maximize\nperformance in problems where safety is required during the learning and/or deployment\n[20]. A detailed literature review can be found in [20], which has categorized two\nmain approaches by modifying: (1) the optimality condition with a safety factor,\nand (2) the exploration process to incorporate external knowledge or risk metrics.\nRisk-aversion can be speciﬁed in the reward function, for example, by deﬁning risk\nas the probability of reaching a set of unknown states in a discrete Markov decision\nprocess setting [14, 21]. Robust MDP is designed to maximize rewards while safely\nexploring the discrete state space [36, 50]. For continuous states and actions, robust\nmodel predictive control can be employed to ensure robustness and safety constraints\nfor the learned model with bounded errrors [7]. These methods require an accurate\nor estimated models for policy learning. Recently, model-free policy optimization\n8\nM. JIN AND J. LAVAEI\nhas been successfully demonstrated in real-world tasks such as robotics, business\nmanagement, smart grid and transportation [31]. Safety requirement is high in these\nsettings. Existing approaches are based on constraint satisfaction that holds with high\nprobability [45, 1].\nThe present analysis tackles the safe reinforcement learning problem from a robust\ncontrol perspective, which is aimed at providing theoretical guarantees for stability [55].\nLyapunov functions are widely used to analyze and verify stability when the system\nand its controller are known [39, 10]. For nonlinear systems without global convergence\nguarantees, region of convergence is often estimated, where any state trajectory that\nstarts within this region stays within the region for all times and converges to a\ntarget state eventually [27]. For example, recently, [9] has proposed a learning-based\nLyapunov stability veriﬁcation for physical systems, whose dynamics are sequentially\nestimated by Gaussian processes. In the same vein, [2] has employed reachability\nanalysis to construct safe regions in the state space by solving a partial diﬀerential\nequation. The main challenge of these methods is to ﬁnd a suitable non-conservative\nLyapunov function to conduct the analysis.\nThe IQC framework proposed in [33] has been widely used to analyze the stability\nof large-scale complex systems such as aircraft control [19]. The main advantages of\nIQC are its computational eﬃciency, non-conservatism, and uniﬁed treatment of a\nvariety of nonlinearities and uncertainties. It has also been employed to analyze the\nstability of small-sized neural networks in reinforcement learning [28, 5]; however, in\ntheir analysis, the exact coeﬃcients of the neural network need to be known a priori for\nthe static stability analysis, and a region of safe coeﬃcients needs to be calculated at\neach iteration for the dynamic stability analysis. This is computationally intensive, and\nit quickly becomes intractable when the neural network size grows. On the contrary,\nbecause the present analysis is based on a broad characterization of control functions\nwith bounded gradients, it does not need to access the coeﬃcients of the neural network\n(or any forms of the controller). In general, robust analysis using advanced methods\nsuch as structured singular value [38] or IQC can be conservative. There are only few\ncases where the necessity conditions can be established, such as when the uncertain\noperator has a block diagonal structure of bounded singular values [16], but this\nset of uncertainties is much smaller than the set of performance-oriented controllers\nlearned by RL. To this end, we are able to reduce conservatism of the results by\nintroducing more informative quadratic constraints for those controllers, and analyze\nthe necessity of the certiﬁcate criteria. This signiﬁcantly extends the possibilities of\nstability-certiﬁed reinforcement learning to large and deep neural networks in nonlinear\nlarge-scale real-world systems, whose stability is otherwise impossible to be certiﬁed\nusing existing approaches.\n3. Main results. This section will introduce a set of quadratic constraints on\ngradient-bounded functions, describe the computation of a smoothness margin for\nlinear (Theorem 3.3) and nonlinear systems (Theorem 3.4). Furthermore, we examine\nthe conservatism of the certiﬁcate condition in Theorem 3.3 for linear systems.\n3.1. Quadratic constraints on gradient-bounded functions. The starting\npoint of this analysis is a less conservative constraint on general vector-valued functions.\nWe start by recalling the deﬁnition of a Lipschitz continuous function:\nDefinition 3.1 (Lipschitz continuous function). We deﬁne both the local and\nglobal versions of the Lipschitz continuity for a function f : Rn →Rm:\n(a) The function f is locally Lipschitz continuous on the open subset B if there\nSTABILITY-CERTIFIED REINFORCEMENT LEARNING\n9\nexists a constant ξ > 0 (i.e., Lipschitz constant of f on B) such that\n(3.1)\n|f(x) −f(y)| ≤ξ|x −y|,\n∀x, y ∈B.\n(b) If f is Lipschitz continuous on Rn with a constant ξ (i.e., B = Rn in (3.1)),\nthen f is called globally Lipschitz continuous with the Lipschitz constant ξ.\nLipschitz continuity implies uniform continuity. The above deﬁnition also establishes\na connection between locally and globally Lipschitz continuity. The norm | · | in the\ndeﬁnition can be any norm, but the Lipschitz constant depends on the particular choice\nof the norm. Unless otherwise stated, we use the Euclidean norm in our analysis.\nTo explore some useful properties of Lipschitz continuity, consider a scalar-valued\nfunction (i.e., m = 1). Let h(j)\nxy =\n\u0002y1, y2, . . . , yj, xj+1, . . . , xn\n\u0003⊤∈Rn denote a hybrid\nvector between x and y, with h(0)\nxy = x and h(n)\nxy = y. Then, local Lipschitz continuity\nof f : Rn →R on B implies that\n(3.2)\n|f(h(j)\nxy ) −f(h(j−1)\nxy\n)|\n|xj −yj|\n≤ξ,\n∀x, y ∈B, xj ̸= yj, j ∈[n].\nIf we were to assume that f is diﬀerentiable, then it follows that its (partial) derivative\nis bounded by the Lipschitz constant. For a vector-valued function f =\n\u0002f1, . . . , fm\n\u0003⊤\nthat is ξ-Lipschitz, it is necessary that every component fi be ξ-Lipschitz. In general,\nevery continuously diﬀerentiable function is locally Lipschitz, but the reverse is not true,\nsince the deﬁnition of Lipschitz continuity does not require diﬀerentiability. Indeed,\nby the Rademacher’s theorem, if f is locally Lipschitz on B, then it is diﬀerentiable at\nalmost every point in B [13].\nFor the purpose of stability analysis, we can express (3.1) as a point-wise quadratic\nconstraint:\n(3.3)\n\u0014\nx −y\nf(x) −f(y)\n\u0015⊤\u0014\nξ2In\n0\n0\n−Im\n\u0015 \u0014\nx −y\nf(x) −f(y)\n\u0015\n≥0,\n∀x, y ∈B.\nThe above constraint, nevertheless, can be sometimes too conservative, because it\ndoes not explore the structure of a given problems. To elaborate on this, consider the\nfunction f : R2 →R2 deﬁned as\n(3.4)\nf(x1, x2) =\n\u0002tanh(0.5x1) −ax1, sin(x2)\u0003⊤,\nwhere x1, x2 ∈R and |a| ≤0.1 is a deterministic but unknown parameter with a\nbounded magnitude. Clearly, to satisfy (3.1) on R2 for all possible tuples (a, x1, x2),\nwe need to choose ξ ≥1 (i.e., the function has the Lipshitz constant 1). However, this\ncharacterization is too general in this case, because it ignores the non-homogeneity\nof f1 and f2, as well as the sparsity of the problem representation. Indeed, f1 only\ndepends on x1 with its slope restricted to [−0.1, 0.6] for all possible |a| ≤0.1, and f2\nonly depends on x2 with its slope restricted to [−1, 1]. In the context of controller\ndesign, the non-homogeneity of control outputs often arises from physical constraints\nand domain knowledge, and the sparsity of control architecture is inherent in scenarios\nwith distributed local information. To explicitly address these requirements, we state\nthe following quadratic constraint.\nLemma 3.2. For a vector-valued function f : Rn →Rm that is diﬀerentiable with\nbounded partial derivatives on B (i.e., ξij ≤∂jfi(x) ≤ξij for all x ∈B), the following\n10\nM. JIN AND J. LAVAEI\nquadratic constraint is satisﬁed for all λij ≥0, i ∈[m], j ∈[n], and x, y ∈B:\n(3.5)\n\u0014 x −y\nq(x, y)\n\u0015⊤\nM(λ; ξ)\n\u0014 x −y\nq(x, y)\n\u0015\n≥0,\nwhere M(λ; ξ) is given by\n(3.6)\n\u0014diag\n\u0000\bP\ni λij(c2\nij −c2\nij)\n\t\u0001\nU({λij, cij})⊤\nU({λij, cij})\ndiag ({−λij})\n\u0015\n,\nwhere diag(x) denotes a diagonal matrix with diagonal entries speciﬁed by x, and\nq(x, y) =\n\u0002q11, . . . , q1n, . . . , qm1, . . . , qmn\n\u0003⊤is determined by x and y, {−λij} is a set\nof non-negative multipliers that follow the same index order as q, U({λij, cij}) =\n\u0002diag ({−λ1jc1j})\n· · ·\ndiag ({−λmjcmj})\u0003\n∈Rn×mn, cij =\n1\n2\n\u0010\nξij + ξij\n\u0011\n, cij =\nξij −cij, and q is related to the output of f by the constraint:\n(3.7)\nf(x) −f(y) =\n\u0002Im ⊗11×n\n\u0003\nq = Wq,\nwhere ⊗denotes the Kronecker product.\nProof. For a vector-valued function f : Rn →Rm that is diﬀerentiable with\nbounded partial derivatives on B (i.e., ξij ≤∂jfi(x) ≤ξij for all x ∈B), there exist\nfunctions δij : Rn × Rn →R bounded by ξij ≤δij(x, y) ≤ξij for all i ∈[m] and\nj ∈[n] such that\n(3.8)\nf(x) −f(y) =\n\n\nPn\nj=1 δ1j(x, y)(xj −yj)\n...\nPn\nj=1 δmj(x, y)(xj −yj)\n\n.\nBy deﬁning qij = δij(x, y)(xj −yj), since (δij(x, y) −cij)2 ≤c2\nij, it follows that\n(3.9)\n\u0014\nxj −yj\nqij\n\u0015⊤\u0014c2\nij −c2\nij\ncij\ncij\n−1\n\u0015 \u0002⋆\u0003\n≥0.\nThe result follows by introducing nonnegative multipliers λij ≥0, and the fact that\nfi(x) −fi(y) = Pm\nj=1 qij.\nThis above bound is a direct consequence of standard tools in real analysis [54]. To\nunderstand this result, it can be observed that (3.5) is equivalent to:\n(3.10)\nX\ni,j\nλij\n\u0010\n(c2\nij −c2\nij)(xj −yj)2 + 2cijqij(xj −yj) −q2\nij\n\u0011\n≥0,\n∀λij ≥0,\nwith fi(x) −fi(y) = Pn\nj=1 qij, where qij depends on x and y. Since (3.10) holds for all\nλij ≥0, it is equivalent to the condition that (c2\nij−c2\nij)(xj−yj)2+2cijqij(xj−yj)−q2\nij ≥\n0 for all i ∈[m] and j ∈[n], which is a direct result of the bounds imposed on\nthe partial derivatives of fi. To illustrate its usage, let us apply the constraint to\ncharacterize the example function (3.4), where ξ11 = −0.1, ξ11 = 0.6, ξ22 = −1, ξ22 = 1,\nand all the other bounds (ξ12, ξ12, ξ21, ξ21) are zero.\nThis clearly yields a more\ninformative constraint than merely relying on the Lipschitz constraint (3.3). In fact,\nSTABILITY-CERTIFIED REINFORCEMENT LEARNING\n11\nfor a diﬀerentiable l-Lipschitz function, we have ξij = −ξij = l, and by limiting the\nchoice of λij =\n(\nλ\nif i = 1\n0\nif i ̸= 1, (3.10) is reduced to (3.3). However, as illustrated in this\nexample, the quadratic constraint in Lemma 3.2 can incorporate richer information\nabout the structure of the problem; therefore, it often gives rise to non-trivial stability\nbounds in practice.\nThe constraint introduced above is not a classical IQC, since it involves an\nintermediate variable q that relates to the output f through a set of linear equalities.\nFor stability analysis, let y = x∗∈B be the equilibrium point, and without loss of\ngenerality, assume that x∗= 0 and f(x∗) = 0. Then, one can deﬁne the quadratic\nfunctions\nφij(x, q) = (c2\nij −c2\nij)x2\nj + 2cijqijxj −q2\nij,\nand the condition (3.5) can be written as\n(3.11)\nX\nij\nλijφij(x, q) ≥0,\n∀λij ≥0,\nwhich can be used to characterize the set of (x, q) associated with the function f, as\nwe will discuss in Section 3.4.\nTo simplify the mathematical treatment, we have focused on diﬀerentiable func-\ntions in Lemma 3.2; nevertheless, the analysis can be extended to non-diﬀerentiable but\ncontinuous functions (e.g., the ReLU function max{0, x}) using the notion of general-\nized gradient [13, Chap. 2]. In brief, by re-assigning the bounds on partial derivatives\nto uniform bounds on the set of generalized partial derivatives, the constraint (3.5)\ncan be directly applied.\nIn relation to the existing IQCs, this constraint has wider applications for the\ncharacterization of gradient-bounded functions. The Zames-Falb IQC introduced in\n[53] has been widely used for single-input single-output (SISO) functions f : R →R,\nbut it requires the function to be monotone with the slope restricted to [α, β] with\nα ≥0, i.e., 0 ≤α ≤f(x)−f(y)\nx−y\n≤β whenever x ̸= y. The MIMO extension holds true\nonly if the nonlinear function f : Rn →Rn is restricted to be the gradient of a convex\nreal-valued function [40, 24]. As for the sector IQC, the scalar version can not be\nused (because it requires fi(x) = 0 whenever there exists j ∈[n] such that xj = 0,\nwhich is extremely restrictive), and the vector version is in fact (3.3). In contrast, the\nquadratic constraint in Lemma 3.2 can be applied to non-monotone, vector-valued\nLipschitz functions.\n3.2. Computation of the smoothness margin. With the newly developed\nquadratic constraint in place, this subsection explains the computation for a smoothness\nmargin of an LTI system G, whose state-space representation is given by:\n(3.12)\n\n\n\n\n\n˙xG = AxG + Bu\nw\n= π(xG)\nu\n= e + w\nwhere xG ∈Rns is the state (the dependence on t is omitted for simplicity). The\nsystem is assumed to be stable, i.e., A is Hurwitz. We can connect this linear system\nin feedback with a controller π : Rns →Rna. The signal e ∈Rna is the exploration\n12\nM. JIN AND J. LAVAEI\nvector introduced in reinforcement learning, and w ∈Rna is the policy action. We\nare interested in certifying the set of gradient bounds ξ ∈Rns×na of π such that the\ninterconnected system is input-output stable at all time T ≥0, i.e.,\n(3.13)\nZ T\n0\n|y(t)|2 dt ≤γ2\nZ T\n0\n|e(t)|2 dt,\nwhere γ is a ﬁnite upper bound for the L2 gain. Let A ⪰B or A ≻B denote that\nA −B is positive semideﬁnite or positive deﬁnite, respectively. To this end, deﬁne the\nSDP(P, λ, γ, ξ) as follows:\n(3.14)\nSDP(P, λ, γ, ξ) :\n\u0014O(P, λ, ξ)\nS(P)\nS(P)⊤\n−γI\n\u0015\n≺0,\nwhere P = P ⊤⪰0 and\nO(P, λ, ξ) =\n\u0014A⊤P + PA\nPBW\nW ⊤B⊤P\n0\n\u0015\n+ 1\nγ\n\u0014I\n0\n0\n0\n\u0015\n+ M(λ; ξ), S(P) =\n\u0014PB\n0\n\u0015\n,\nwhere M(λ; ξ) is deﬁned in (3.6). We will show next that the stability of the intercon-\nnected system can be certiﬁed using linear matrix inequalities.\nTheorem 3.3. Let G be stable (i.e., A is Hurwitz) and π ∈Rns →Rna be a\nbounded causal controller. Assume that:\n(i) the interconnection of G and π is well-posed;\n(ii) π has bounded partial derivatives on B (i.e., ξij ≤∂jπi(x) ≤ξij, for all x ∈B,\ni ∈[na] and j ∈[ns]).\nIf there exist P ⪰0 and a scalar γ > 0 such that SDP(P, λ, γ, ξ) is feasible, then the\nfeedback interconnection of G and π is stable (i.e., it satisﬁes (3.13)).\nProof. The proof follows a standard dissipation argument. To proceed, we multiply\n\u0002\nx⊤\nG\nq⊤\ne⊤\u0003⊤to the left and its transpose to the right of the augmented matrix\nin (3.14), and use the constraints w = Wq and y = xG. Then, SDP(P, λ, γ, ξ) can be\nwritten as a dissipation inequality:\n˙V (xG) +\n\u0014xG\nq\n\u0015⊤\nM(λ; ξ)\n\u0014xG\nq\n\u0015\n< γe⊤e −1\nγ y⊤y,\nwhere V (xG) = x⊤\nGPxG is known as the storage function, and ˙V (·) is its derivative\nwith respect to time t. Because the second term is guaranteed to be non-negative by\nLemma 3.2, if SDP(P, λ, γ, ξ) is feasible with a solution (P, λ, γ, ξ), we have:\n(3.15)\n˙V (xG) + 1\nγ y⊤y −γe⊤e < 0,\nwhich is satisﬁed at all times t. From well-posedness, the above inequality can be\nintegrated from t = 0 to t = T, and then it follows from P ⪰0 that:\n(3.16)\nZ T\n0\n|y(t)|2dt ≤γ2\nZ T\n0\n|e(t)|2dt.\nHence, the interconnected system with the RL policy π is stable.\nSTABILITY-CERTIFIED REINFORCEMENT LEARNING\n13\nThe above theorem requires that G be stable when there is no feedback policy π. This\nis automatically satisﬁed in many physical systems with an existing stabilizing (but not\nperformance-optimizing) controller. In the case that the original system is not stable,\none needs to ﬁrst design a controller to stablize the system or design the controller\nunder uncertainty (in this case, the RL policy), which are well-studied problems in the\nliterature (e.g., H∞controller synthesis [16]). Then, the result can be used to ensure\nstability while delegating reinforcement learning to optimize the performance of the\npolicy under gradient bounds.\nThe above result essentially suggests a computational approach in robust control\nanalysis. Given a stable LTI system depicted in (3.12), the ﬁrst step is to represent\nthe RL policy as an uncertainty block in a feedback interconnection. Because the\nparameters of the neural network policy may not be known a priori and will be\ncontinuously updated during learning, we characterize it using bounds on partial\ngradients (e.g., if it is known that the action is positively correlated with certain\nobservation metric, we can specify its partial gradient to be mostly positive with\nonly a small negative margin). A simple but conservative choice is a L2-gain bound\nIQC; nevertheless, to achieve a less conservative result, we can employ the quadratic\nconstraint developed in Lemma 3.2, which exploits both the sparsity of the control\narchitecture and the non-homogeneity of the outputs. For a given set of gradient\nbounds ξ, we ﬁnd the smallest γ such that (3.14) is feasible, and γ corresponds to the\nupper bound on the L2 gain of the interconnected system both during learning (with\nthe excitation e added to facilitate policy exploration) and actual deployment. If γ is\nﬁnite, then the system is provably stable in the sense of (3.13).\nWe remark that SDP(P, λ, γ, ξ) is quasiconvex, in the sense that it reduces to a\nstandard LMI with a ﬁxed γ. To solve it numerically, we start with a small γ and\ngradually increase it until a solution (P, λ) is found. This is repeated for multiple sets\nof ξ. Each iteration (i.e., LMI for a given set of γ and ξ) can be solved eﬃciently\nby interior-point methods. As an alternative to searching on γ for a given ξ, more\nsophisticated methods for solving the generalized eigenvalue optimization problem can\nbe employed [11].\n3.3. Extension to nonlinear systems with uncertainty. The previous anal-\nysis for LTI systems can be extended to a generic nonlinear system described in (1.1).\nThe key idea is to model the nonlinear and potentially time-varying part gt(x(t)) as\nan uncertain block with IQC constraints on its behavior. Speciﬁcally, consider the LTI\ncomponent G:\n(3.17)\n(\n˙xG = AxG + Bu + v\ny\n= xG\nwhere xG ∈Rns is the state and y ∈Rns is the output. The linearized system is\nassumed to be stable, i.e., A is Hurwitz. The nonlinear part is connected in feedback:\n(3.18)\n\n\n\n\n\nu = e + w\nw = π(y)\nv = gt(y)\nwhere e ∈Rna and w ∈Rna are deﬁned as before, and gt : Rns →Rns is the nonlinear\nand time-varying component. In addition to characterizing π using the Lipschitz\nproperty as in (3.5), we assume that gt : Rns →Rns satisﬁes the IQC deﬁned by\n14\nM. JIN AND J. LAVAEI\n(Ψ, Mg) as in Deﬁnition 2.1. The system Ψ has the state-space representation:\n(3.19)\n( ˙ψ = Aψψ + Bv\nψv + By\nψy\nz = Cψψ + Dv\nψv + Dy\nψy ,\nwhere ψ ∈Rns is the internal state and z ∈Rnz is the ﬁltered output. By denoting\nx =\n\u0002\nx⊤\nG\nψ⊤\u0003⊤∈R2ns as the new state, one can combine (3.17) and (3.19) via\nreducing y and letting w = Wq:\n(3.20)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n˙x =\n\"\nA\n0\nBy\nψ\nAψ\n#\n|\n{z\n}\nA\nx +\n\"\nB\n0\n#\n|{z}\nBe\ne +\n\"\nBW\n0\n#\n| {z }\nBq\nq +\n\"\nI\nBv\nψ\n#\n| {z }\nBv\nv\nz =\nh\nDy\nψ\nCψ\ni\n|\n{z\n}\nC\nx + Dv\nψv\n,\nwhere A, Be, Bq, Bv, C are matrices of proper dimensions deﬁned above. Similar\nto the case of LTI systems, the objective is to ﬁnd the gradient bounds on π such\nthat the system becomes stable in the sense of (3.13). In the same vein, we deﬁne\nSDP(P, λ, γ, ξ) as:\n(3.21)\nSDP(P, λ, γ, ξ) :\n\n\nO(P, λ, ξ)\nOv(P)\nS(P)\nOv(P)⊤\nDv⊤\nψ MqDv\nψ\n0\nS(P)⊤\n0\n−γI\n\n≺0,\nwhere P ⪰0, and\nO(P, λ, ξ) =\n\u0014A⊤P + PA\nPBq\nB⊤\nq P\n0\n\u0015\n+\n\u0014\nC⊤MgC\n0\n0\n0\n\u0015\n+ M(λ; ξ) + 1\nγ\n\u0014I\n0\n0\n0\n\u0015\n,\nOv(P) =\n\u0014\nC⊤MqDv\nψ + PBv\n0\n\u0015\n, S(P) =\n\u0014PBe\n0\n\u0015\n,\nwhere M(λ; ξ) is deﬁned in (3.6). The next theorem provides a stability certiﬁcate for\nthe nonlinear time-varying system (1.1).\nTheorem 3.4. Let G be stable (i.e., A in (3.17) is Hurwitz) and π ∈Rns →Rna\nbe a bounded causal controller. Assume that:\n(i) the interconnection of G, π, and gt is well-posed;\n(ii) π has bounded partial derivatives on B (i.e., ξij ≤∂jπi(x) ≤ξij for all x ∈B,\ni ∈[na] and j ∈[ns]);\n(iii) gt ∈IQC(Ψ, Mg), where Ψ is stable.\nIf there exist P ⪰0 and a scalar γ > 0 such that SDP(P, λ, γ, ξ) in (3.21) is feasible,\nthen the feedback interconnection of the nonlinear system (1.1) and π is stable (i.e., it\nsatisﬁes (3.13)).\nProof. The proof is in the same vein as that of Theorem 3.3. The main tech-\nnical diﬀerence is the consideration of the ﬁltered state ψ and the output z to im-\npose IQC constraints on the nonlinearities gt(y) in the dynamical system [33]. The\ndissipation inequality follows by multiplying both sides of the matrix in (3.21) by\n\u0002\nx⊤\nq⊤\nv⊤\ne⊤\u0003⊤and its transpose:\n˙V (x) + z⊤Mgz +\n\u0014xG\nq\n\u0015⊤\nMπ\n\u0014xG\nq\n\u0015\n< γe⊤e −1\nγ y⊤y,\nSTABILITY-CERTIFIED REINFORCEMENT LEARNING\n15\nwhere x and z are deﬁned in (3.20), and V (x) = x⊤Px is the storage function\nwith ˙V (·) as its time derivative. The second term on the left side is non-negative\nbecause gt ∈IQC(Ψ, Mg), and the third term is non-negative due to the smoothness\nquadratic constraint in Lemma 3.2. Thus, if there exists a feasible solution P ⪰0 to\nSDP(P, λ, γ, ξ), integrating the inequality from t = 0 to t = T yields that:\n(3.22)\nZ T\n0\n|y(t)|2dt ≤γ2\nZ T\n0\n|e(t)|2dt.\nHence, the nonlinear system interconnected with the RL policy π is certiﬁably stable\nin the sense of a ﬁnite L2 gain.\n3.4. Analysis of conservatism of the stability certiﬁcate. We focus on the\ncase where an LTI system G is interconnected with an RL policy π ∈P(ξ) (i.e.,\na function with bounded partial gradients). This corresponds to the system (3.12)\nstudied in Section 3.2. To certify the stability of (3.12), as will be shown in the next\nproposition, it suﬃces to examine the stability of the following system:\n(3.23)\n\n\n\n\n\n\n\n\n\n˙xG = AxG + Bu\nq\n= ˜π(xG)\nw\n= Wq\nu\n= e + w\nwhere ˜π ∈eP is a function in the uncertainty set:\n(3.24)\neP(ξ) =\nn\n˜π\n\f\f\f ξijxj ≤˜πij(x) ≤ξijxj, ∀x ∈Rns, i ∈[na], j ∈[ns]\no\n.\nProposition 3.5. If the system (3.23) is stable for all ˜π ∈eP(ξ), then the system\n(3.12) is stable for all π ∈P(ξ).\nProof. It suﬃces to show that for any π ∈P(ξ), there exists a policy ˜π ∈eP(ξ)\nsuch that π = W ˜π. Let y0\nj =\n\u00020\n· · ·\n0\nyj+1\n· · ·\nyns\n\u0003\n∈Rns for every j ∈\n{0, 1, ..., ns}, and y0\n0 = y, y0\nns = 0. Then, one can write:\nπi(y) =\nns\nX\nj=1\nπi(y0\nj−1) −πi(y0\nj ) =\nns\nX\nj=1\n˜πij(y),\nwhere ˜πij(y) satisﬁes\n˜πij(y)\nyj\n= πi(y0\nj−1) −πi(y0\nj )\n|y0\nj−1 −y0\nj |\n∈[ξij, ξij]\nif yj ̸= 0 and ˜πij(y) = 0 if yj = 0. The bound is due to the mean-value theorem and\nthe bounds on the partial derivatives of πi. Since the above argument is valid for all\ni ∈[na], it means that ˜π ∈eP(ξ), and π = W ˜π.\nProposition 3.5 implies that one potential source of conservatism comes from\nthe decomposition of a gradient-bounded function into a sum of sector-bounded\ncomponents. Henceforth, we focus the subsequent analysis by examining (3.23). By\nconsidering the state-space representation of G =\n\u0014 A\nBW\nB\nI\n0\n0\n\u0015\n=\n\u0002 G11\nG12\n\u0003\n,\n16\nM. JIN AND J. LAVAEI\none can write system (3.23) as:\n(3.25)\n\n\n\n\n\nxG =\nh\nG11\nG12\ni \"\nq\ne\n#\nq\n= ˜π(xG)\n.\nIt is known that the system is input-output stable if and only if I −G11˜π is nonsingu-\nlar [16]. To understand this, note that if I −G11˜π is nonsingular, then the transfer\nfrom e to xG is given by:\ne 7→xG = H(e) = (I −G11˜π)−1G12e,\nand |xG| = |H(e)| ≤∥(I −G11˜π)−1G12∥|e|. From the previous section (in particular,\nLemma 3.2), we know that if the function π is gradient-bounded, then the set of\ninput/output signals belongs to:\nS(ξ) =\n\b\n(x, q) | φij(x, q) = (c2\nij −c2\nij)x2\nj + 2cijqijxj −q2\nij ≥0,\n∀i ∈[na], j ∈[ns]\n\t\n,\nwhere we use cij = 1\n2\n\u0010\nξij + ξij\n\u0011\n, cij = ξij −cij for simplicity. We now show that\nthe pair (x, q) belongs to S(ξ) if and only if there exists a sector-bounded function\n˜π ∈˜P(ξ) such that it satisﬁes q = ˜π(x).\nLemma 3.6. Suppose that x ∈Rns and q ∈Rnans, and cij ≥0 for every i ∈[na]\nand j ∈[ns]. Then, the pair (x, q) belongs to S(ξ) if and only if there exists an operator\n˜π : Rn →Rnans, such that q = ˜π(x), and ˜π satisﬁes the following conditions: (i)\n˜πij(x) = 0 if xj = 0, and (ii) ˜π is sector bounded, i.e., (cij −cij)xj ≤˜πij(x) ≤\n(cij + cij)xj holds for all i ∈[na] and j ∈[ns].\nProof. To show the suﬃciency direction, conditions (i) and (ii) yield that\n(cijxj)2 ≥\n\u0012\u0012πij(x)\nxj\n−cij\n\u0013\nxj\n\u00132\n= (qij −cijxj)2 .\nBy rearranging the above inequality, it can be concluded that (x, q) ∈S(ξ).\nFor the necessary direction, note that the condition φij(x, q) ≥0 is equivalent\nto |qij −cijxj| ≤|cijxj|. Thus, we have πij(x) = 0 if xj = 0. Since cij ≥0, one can\nobtain\n\f\f\f qij\nxj −cij\n\f\f\f ≤cij, which is equivalent to the sector bounds.\nBy slightly overloading the notations, we can extend the result of the previous\nlemma from static mapping to the case that x ∈Lns and q ∈Lnans with the operator\n˜π : Lns →Lnans. We can then extend the deﬁnition of S(ξ) to this space accordingly.\nLemma 3.7. Suppose that x ∈Lns and q ∈Lnans, and that cij ≥0 for all i ∈[na]\nand j ∈[ns]. Then, the pair (x, q) belongs to S(ξ) where\n(3.26)\nS(ξ) = {(x, q) | φij(x, q) ≥0,\n∀i ∈[na], j ∈[ns]} ,\nand\n(3.27)\nφij(x, q) = (c2\nij −c2\nij)∥xj∥2 + 2cij⟨qij, xj⟩−∥qij∥2 ≥0,\nif and only if there exists an operator ˜π : Lns →Lnans such that q = ˜π(x) and ˜π\nsatisﬁes the following conditions: (i) ˜πij(x) = 0 if xj = 0, and (ii) ˜π is sector bounded,\ni.e., (cij −cij)∥xj∥≤∥˜πij(x)∥≤(cij + cij)∥xj∥for all i ∈[m] and j ∈[n].\nSTABILITY-CERTIFIED REINFORCEMENT LEARNING\n17\nProof. For the suﬃciency condition, since ˜πij is sector bounded, and ˜πij(x) = 0 if\nxj = 0, without loss of generality, assume that cij ≤0. One can write\n∥cijxj∥2 ≥\n\r\r\r\r\n\f\f\f\f\n∥˜πij(x)∥\n∥xj∥\n−cij\n\f\f\f\f xj\n\r\r\r\r\n2\n≥\n\r\r\r\r\n\u0012∥˜πij(x)∥\n∥xj∥\n−cij\n\u0013\nxj\n\r\r\r\r\n2\n= ∥cijxj∥2 + ∥˜πij(x)∥2 −2\n\u001c\ncijxj, ∥˜πij(x)∥\n∥xj∥\nxj\n\u001d\n= ∥cijxj∥2 + ∥˜πij(x)∥2 −2cij∥xj∥∥˜πij(x)∥\n≥∥cijxj∥2 + ∥˜πij(x)∥2 −2cij ⟨˜πij(x), xj⟩\n= ∥qij −cijxj∥2 .\nBy rearranging the above inequality, it can be concluded that (x, q) ∈S(ξ).\nFor the necessary direction, we can construct ˜π(y) = q ⟨y,x⟩\n|x|2 for all y ∈Lns. This\nleads to ˜π(x) = q, and the condition φij(x, q) ≥0 is equivalent to ∥qij −cijxj∥≤\ncij ∥xj∥. Thus, we have ˜πij(x) = 0 if xj = 0. Without loss of generality, assume that\ncij ≤0. Therefore, ∥qij∥≤cij∥xj∥+ cij∥xj∥and ∥qij∥≥−cij∥xj∥+ cij∥xj∥, which\nare equivalent to the sector bound condition.\nThe previous result indicates that the input and output pair of ˜π can be described\nby S(ξ). We show next that this set should be separated from the signal space of the\ndynamical system in order to ensure robust stability.\nLemma 3.8. If (G, ˜π) is robustly stable, then there cannot exist a nonzero q ∈L2\nsuch that x = Gq and (x, q) ∈S(ξ).\nProof. We prove this lemma by contraposition. If there exists a nonzero q ∈L2\nsuch that (x, q) ∈S(ξ), then it follows from Lemma 3.7 that there exists a linear\noperator ˜π such that q = ˜π(x) = ˜π(Gq). This implies that the operator (I −˜πG) is\nsingular, and therefore, (I −G˜π) is singular, implying that the interconnected system\nis not robustly stable.\nThe path of examining the necessity of the SDP condition (3.14) has become clear.\nConsider the set generated by the LTI system:\n(3.28)\nΨ = {(φij(x, q) : q ∈Lnans, ∥q∥= 1, x = Gq} ,\nand the positive orthant\n(3.29)\nΠ = {(rij) ∈Rnans : rij ≥0,\n∀i ∈[na], j ∈[ns]} .\nLemma 3.8 implies that the two sets Ψ and Π are separated if (G, ˜π) is robustly\nstable. The goal is to show that there exists a separating hyperplane, whose pa-\nrameters are related to the solution of (3.14). For simplicity, deﬁne the matrices\nΩij,x = diag\n\u0000\b\n{c2\nij −c2\nij\n\t\u0001\n, Ωij,q and Ωij,xq with their (k, l)-th elements [Ωij,q]kl =\n(\n1\nif k = in + j\n0\notherwise\n, and [Ωij,xq]kl =\n(\ncij\nif k = j, l = in + j\n0\notherwise\n.\nTo write φij(x =\nGq, q) as an inner product, deﬁne\nTij = G∗Ωij,xG −Ωij,q + G∗Ω∗\nij,xq + Ωij,xqG.\n18\nM. JIN AND J. LAVAEI\nIt results from the deﬁnition (3.27) that\n(3.30)\nφij(x = Gq, q) = ∥Gq∥2\nΩij,x + 2Re ⟨Ωij,xqGq, q⟩−∥q∥2\nΩij,q = ⟨q, Tijq⟩.\nLemma 3.9. For a given linear time-invariant operator G, the closure Ψ of Ψ\ndeﬁned in (3.28) is convex.\nProof. Because G is time-invariant, by denoting Dτ as the delay operator at scale\nτ, we obtain D∗\nτTijDτ = Tij. Let y = φ(q) and ˜y = φ(˜q) be the elements of Ψ, with\n∥q∥= ∥˜q∥= 1. By considering qτ = √αq + √1 −αDτ ˜q, one can write\nφij(qτ) = α ⟨Tijq, q⟩+ (1 −α) ⟨TijDτ ˜q, Dτ ˜q⟩+ 2α\n√\n1 −αRe ⟨Tijq, Dτ ˜q⟩\n= αφij(q) + (1 −α)φij(˜q) + 2α\n√\n1 −αRe ⟨Tijq, Dτ ˜q⟩.\nBy letting τ →∞, we obtain Re ⟨Tijq, Dτ ˜q⟩→0, where Re(x) denotes the real part\nof a complex vector x. Thus,\nlim\nτ→∞φij(qτ) = αφij(q) + (1 −α)φij(˜q)\nand limτ→∞∥qτ∥2 = α∥q∥2 + (1 −α)∥˜q∥2 = 1. Therefore,\nlim\nτ→∞φ\n\u0012 qτ\n∥qτ∥\n\u0013\n= αy + (1 −α)˜y ∈Ψ.\nNow, we show that strict separation occurs when the system is robustly stable.\nLemma 3.10. Suppose that I −G˜π is nonsingular. Then, the sets Π and Ψ are\nstrictly separated, namely D(Π, Ψ) = infr∈Π,y∈Ψ |r −y| > 0.\nTo prove this result, we need the following lemma.\nLemma 3.11. Suppose that D(Π, Ψ) = infr∈Π,y∈Ψ |r −y| = 0. Given any ϵ > 0\nand t0 ≥0, there exist a closed interval [t0, t1] and two signals x ∈Lns and q ∈Lnans\nwith ∥q∥= 1 such that\nφij(x, q) ≥0,\n∀i ∈[na], j ∈[ns]\n(3.31)\nϵ2 > ∥(I −Γ[t0,t1])Gq∥\n(3.32)\nϵ = ∥x −Γ[t0,t1]Gq∥Ωij,x,\n(3.33)\nwhere ∥q∥Ω= √q∗Ωq is the scaled norm and Γ[t0,t1] projects the signal onto the support\nof [t0, t1]. With the above choice of q, x and [t0, t1], there exists an operator ˜π ∈˜P(ξ)\nsuch that ∥(I −˜πΓ[t0,t1]G)q∥≤Cϵ for some constant C > 0 that depends on the sector\nbounds ξ.\nProof. For a given ϵ > 0, by hypothesis, there exists q ∈Lnans with ∥q∥= 1\nsatisfying φij(x, q) > −ϵ2 for all i ∈[na] and j ∈[ns], i.e.,\nϵ2 + ∥Gq∥2\nΩij,x + 2Re ⟨Ωij,xqGq, q⟩> ∥q∥2\nΩij,q,\nwhere Ωij,x and Ωij,xq are deﬁned previously. Clearly, if q is truncated to a suﬃciently\nlong interval, and q is rescaled to have a unit norm, the above inequality will still hold.\nSince Gq ∈Lns, by possibly enlarging the truncation interval to [t0, t1], we obtain\n(3.32), and\nϵ2 + ∥Γ[t0,t1]Gq∥2\nΩij,x + 2Re\n\nΩij,xqΓ[t0,t1]Gq, q\n\u000b\n> ∥q∥2\nΩij,q,\nSTABILITY-CERTIFIED REINFORCEMENT LEARNING\n19\nNext, we choose η ∈Lns such that ∥η∥2\nΩij,x = ϵ2, and that η is orthogonal to Γ[t0,t1]Gq\nand Ω∗\nij,xqq for all i ∈[na] and j ∈[ns]. Then, by considering x = Γ[t0,t1]Gq + η, we\nobtain\n∥x∥2\nΩij,x = ∥Γ[t0,t1]Gq + η∥2\nΩij,x = ϵ2 + ∥Γ[t0,t1]Gq∥2\nΩij,x,\nwhich leads to φij(x, q) ≥0 and (3.33). Now, we can invoke Lemma 3.7 to construct\n˜π ∈P(ξ) based on (3.31) such that ˜π becomes sector bounded and q = ˜πx. Then,\n(I −˜πΓ[t0,t1]G)q = ˜π(x −Γ[t0,t1]Gq).\nLet ∥˜π∥≤C (which depends on the sector bounds). Then,\n∥(I −˜πΓ[t0,t1]G)q∥≤Cϵ\n.\nWe are now ready to prove the strict separation result.\nProof of Lemma 3.10. Assume that D(Π, Ψ) = infr∈Π,y∈Ψ |r −y| = 0. Consider\na sequence ϵn →0 as n tends to ∞. For each ϵn, construct signals q(n) with a bounded\nsupport on [tn, tn+1], and ˜π(n) according to Lemma 3.11. Deﬁne\n˜π =\n∞\nX\nn=1\n˜π(n)Γ[tn,tn+1].\nWe have\n˜πGq(n) = ˜π(n)Γ[tn,tn+1]Gq(n) + ˜π(I −Γ[tn,tn+1])Gq(n),\nand\n∥(I −πG)q(n)∥≤∥(I −˜π(n)Γ[tn,tn+1]G)q(n)∥+ ∥(I −Γ[tn,tn+1])Gq(n)∥\n≤Cϵn + ϵ2\nn\nBecause ϵn →0, the right-hand side approaches 0, and so does the left-hand side.\nTherefore, since ∥q(n)∥= 1, the mapping I−˜πG cannot be invertible, which contradicts\nthe robust stability assumption. This implies that Π and Ψ are strictly separable.\nTo draw the connection to the SDP problem (3.14), observe that\n(3.34)\nφij(x, q) =\n\u001c\u0014x\nq\n\u0015\n, M ij\nπ\n\u0014x\nq\n\u0015\u001d\n,\nwhere\n[M ij\nπ ]kl =\n\n\n\n\n\nc2\nij −c2\nij\n(k, l) = (j, j)\ncij\n(k, l) = (i, i ∗n + j) or (i ∗n + j, i)\n−1\n(k, l) = (i ∗n + j, i ∗n + j)\n,\nand M(λ; ξ) = P\ni∈[na],j∈[ns] λijM ij\nπ as deﬁned in Lemma 3.2.\nProposition 3.12. The SDP condition (3.14) is feasible if and only if there exist\nmultipliers λij ≥0 and ϵ > 0 such that\n(3.35)\nX\ni∈[na],j∈[ns]\nλijφij(x, q) ≤−ϵ∥q∥2\nfor all q ∈Lnans and x = Gq.\n20\nM. JIN AND J. LAVAEI\nProof. Since φij(x, q) =\n\u001c\u0014x\nq\n\u0015\n, M ij\nπ\n\u0014x\nq\n\u0015\u001d\n, the condition (3.35) is equivalent to\n(3.36)\n\u0014G\nI\n\u0015∗\nM(λ; ξ)\n\u0014G\nI\n\u0015\n≺0.\nBy the KYP lemma, this is equivalent to the existence of P ⪰0 such that:\n(3.37)\n\u0014A⊤P + PA\nPBW\nW ⊤B⊤P\n0\n\u0015\n+ M(λ; ξ) ≺0.\nBy Schur complement, P satisﬁes the KYP condition if and only if it satisﬁes (3.14).\nThus, the claim is shown.\nTheorem 3.13. Let ˜π : Lns →Lnans be a bounded causal controller such that\n˜π ∈˜P(ξ). Assume that the interconnection of G and ˜π is well-posed. Then, the\ninput-output stability of the feedback interconnection of system (3.23) implies that there\nexist P ⪰0, γ > 0 and λ ≥0 such that SDP(P, λ, γ, ξ) in (3.14) is feasible.\nProof. Since the system is input-output stable, the sets Π and Ψ are strictly\nseparable due to Lemma 3.10. Since both Π and Ψ are convex (Lemma 3.9), there\nexist a strictly separating hyperplane parametrized by λ ∈Rmn and scalars α, β, such\nthat\n⟨λ, φ⟩≤α < β ≤⟨λ, y⟩\nfor all φ ∈Ψ and y ∈Π. Since ⟨λ, y⟩is bounded from below, we must have λ ≥0, and\nwithout loss of generality, we can set β = 0 and α < 0. This condition is equivalent to\n(3.35), and by Proposition 3.12, this implies that the SDP condition is feasible.\n4. Numerical examples. In this section, we empirically study the stability-\ncertiﬁed reinforcement learning in real-world problems such as ﬂight formation [23] and\npower grid frequency regulation [18]. Designing an optimal controller for these systems\nis challenging, because they consist of many interconnected subsystems that have\nlimited information sharing, and also their underlying models are typically nonlinear\nand even time-varying and uncertain. Indeed, for the case of decentralized control,\nwhich aims at designing a set of local controllers whose interactions are speciﬁed by\nphysical and informational structures, it has been long known that it amounts to\nan NP-hard optimization problem in general [8]. End-to-end reinforcement learning\ncomes in handy, because it does not require model information by simply interacting\nwith the environment while collecting rewards.\nIn a multi-agent setting, each agent explores and learns its own policy independently\nwithout knowing about other agents’ policies [12]. For the simplicity of implementation,\nwe consider the synchronous and cooperative scenario, where agents conduct an action\nat each time step and observe the reward for the whole system. Their goal is to\ncollectively maximize the rewards (or minimize the costs) shared equally among them.\nThe present analysis aims at oﬀering safety certiﬁcates of existing RL algorithms\nwhen applied to real-world dynamical systems, by simply monitoring the gradients\ninformation of the neural network policy. This is orthogonal to the line of research\nthat aims at improving the performance of the existing RL algorithms. The examples\nare taken from [23, 18, 17], but we deal directly with the underlying nonlinear physics\nrather than a linearized model.\nSTABILITY-CERTIFIED REINFORCEMENT LEARNING\n21\n4.1. Multi-agent ﬂight formation. Consider the multi-agent ﬂight formation\nproblem [23], where each agent can only observe the relative distance from its neighbors,\nas illustrated in Figure 2. The goal is to design a local controller for each aircraft such\na predeﬁned pattern is formed as eﬃciently as possible. The physical model3 for each\nTime (sec)\nPositions (m)\nFig. 2: Illustration of the multi-agent ﬂight formation problem.\naircraft is given by:\n¨zi(t) = vi(t)\n¨θi(t) = 1\nδ\n\u0000sin θi(t) + vi(t)\n\u0001\n,\nwhere zi and θi denote the horizontal position and angle of aircraft i, respectively, and\nδ > 0 characterizes the physical coupling of rolling moment and lateral acceleration.\nTo stabilize the system, a simple feedback rule is proposed in [6],\n(4.1)\nvi(t) = α ˙zi(t) + βθi(t) + γ ˙θi(t) + ui(t)\nwhere the parameters of the ﬁrst three terms are designed to maintain the internal\nstability of the horizontal speed and angle of each aircraft (speciﬁcally, α = 90.62,\nβ = −42.15, γ = −13.22, δ = 0.1 as explained in [6]), and the last term is an external\ninput optimized for performance (e.g., to move the aircraft to a target state as fast\nas possible). For each agent, by deﬁning the state xi(t) =\n\u0002\n˙zi(t)\nθi(t)\n˙θ(t)\n\u0003⊤, the\nabove dynamics can be written as\n(4.2)\n˙xi(t) =\n\n\nα\nβ\nγ\n0\n0\n1\nα\nδ\nβ+1\nδ\nγ\nδ\n\n\n|\n{z\n}\nAi\nxi(t) +\n\n\n1\n0\n1\nδ\n\n\n|{z}\nBi\nui(t) +\n\n\n0\n0\nsin θi(t) −θi(t)\n\n\n|\n{z\n}\ngi(xi(t))\n,\nwhere gi(xi(t)) is a nonlinear function of xi(t) that is neglected for a linearized model\n[6, 17]. In a distributed control setting, each agent only has access to the relative\ndistance from its neighbors; therefore, for agents i = 1, 2, 3, deﬁne\n(4.3)\nexi(t) =\n\u0002\nzi(t) −zi+1(t) −d\nxi(t)⊤\u0003⊤,\n3The cosine term in the original formulation is omitted for simplicity, though it can be incorporated\nin a more comprehensive treatment.\n22\nM. JIN AND J. LAVAEI\nwhere d is the desired distance between agents. The state-space model of the intercon-\nnected system can be written in the form of (1.1):\n(4.4)\n\n˙ex\n1\n˙ex\n2\n˙ex\n3\n˙x4\n\n=\n\n\neA1\nH4\n0\n0\n0\neA2\nH4\n0\n0\n0\neA3\nH3\n0\n0\n0\nA4\n\n\n|\n{z\n}\nA\n\n\nex1\nex2\nex3\nx4\n\n\n| {z }\nx(t)\n+\n\n\neB1\n0\n0\n0\n0\neB2\n0\n0\n0\n0\neB3\n0\n0\n0\n0\nB4\n\n\n|\n{z\n}\nB\n\n\nu1\nu2\nu3\nu4\n\n\n| {z }\nu(t)\n+\n\n\neg1(x1)\neg2(x2)\neg3(x3)\ng4(x4)\n\n\n|\n{z\n}\ng(x(t))\n,\nwhere H3 (or H4) is a 4 × 3 (or 4 × 4) matrix whose (i, j)th entry is equal to −1 if\n(i, j) = (1, 1) (or (i, j) = (1, 2)) and is zero otherwise, and where eAi, eBi and egi(xi(t))\nfor i = 1, 2, 3 are augmented to account for the state of relative positions, given by\n(4.5)\neAi =\n\n\n0\n1\n0\n0\n0\nα\nβ\nγ\n0\n0\n0\n1\n0\nα\nδ\nβ+1\nδ\nγ\nδ\n\n,\neBi =\n\n\n0\n1\n0\n1\nδ\n\n, egi(xi(t)) =\n\n\n0\n0\n0\nsin θi(t) −θi(t)\n\n.\nOne particular strength of RL is that the reward function can be highly nonconvex,\nnonlinear, and arbitrarily designed; however, since quadratic costs are widely used\nin the control literature, consider the case r(x(t), u(t)) = x(t)⊤Qx(t) + u(t)⊤Ru(t).\nFor the following experiments, assume that Q = 1000 × I15 and R = I4. In addition,\nbecause the original system A has its largest eigenvalue at 0, we need a nominal\ndistributed linear controller Kd, whose primary goal is to make the largest eigenvalue\nof A + BKn negative. Such controller could be designed using methods such as robust\ncontrol synthesis for the linearized system [16, 55]. With the nominal controller in\nplace, we can deﬁne the new system matrix AG = A + BKn and replace A in (4.4).\nThe task for multi-agent RL is to learn the controller ui(t), which only takes\ninputs of the relative distances of agent i to its neighbors. For example, agent 1 can\nonly observe z1(t) −z2(t) −d (i.e., the 1st entry of x(t)); similarly, agent 2 can only\nobserve z1(t) −z2(t) −d and z2(t) −z3(t) −d (i.e., the 1st and 5th entries of x(t)).\nStability certiﬁcate: To obtain the stability certiﬁcate of (4.4), we apply the\nmethod in Section 3.3. The nonzero entries of the nonlinear component g(x(t)) are\nin the form of sin(θ) −θ, which can be treated as an uncertainty block with the\nslope restricted to [−1, 0] for θ ∈[−π\n2 , π\n2 ]; therefore, the Zames-Falb IQCs can be\nemployed to construct (3.19) [53, 29]. As for the RL agents ui, their gradient bounds\ncan be certiﬁed according to Theorem 3.4. Speciﬁcally, we assume that each agent ui is\nl-Lipschitz continuous, and solve (3.21) for a given set of γ and l. The certiﬁed gradient\nbounds (Lipschitz constants) are plotted in Figure 3 using diﬀerent constraints. The\nconservative L2 constraint (3.3) is only able to certify stability for Lipschitz constants\nup to 0.8. By incorporating the sparsity of distributed controller, we can increase the\nmargin to 1.2, which is satisﬁed throughout the learning process.\nIn order to further increase the set of certiﬁable stable controllers, we monitor\nthe partial gradient information for each agent and encode them as non-homogeneous\ngradient bounds.\nFor instance, if\n∂πi(x)\n∂xj\nhas been consistently positive for latest\niterations, we will set ξij = l and ξij = −ϵl, where ϵ > 0 is a small margin, such as\n0.1, to allow explorations. By performing this during learning, it would be possible to\nsigniﬁcantly enlarge the certiﬁed Lipschitz bound to up to 2.5, as shown in Figure 3.\nPolicy gradient RL: To perform multi-agent reinforcement learning, we employ\ntrust region policy optimization with natural gradients and smoothness policies. During\nSTABILITY-CERTIFIED REINFORCEMENT LEARNING\n23\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nLipschitz const.\n0\n5\n10\n15\n20\nCertified L2 gain\nL2\nInp. sp.\nInp. sp & out. nh.\nFig. 3: Stability-certiﬁed Lipschitz constants obtained by the standard L2 bound (L2)\nin (3.3) and the method proposed in Lemma 3.2, which considers input sparsity (inp.\nsp.) and output non-homogeneity (out. nh.).\nlearning, we employ the hard-thresholding step introduced in Section 2.1 to ensure\nthat the gradient bounds are satisﬁed. The trajectories of rewards averaged over\nthree independent experiments are shown in Figure 4. In this example, agents with\na 1-layer neural network (each with 5 hidden units) can learn most eﬃciently when\nemployed with the smoothness penalties (coeﬃcients are set to be ω1 = ω2 = 0.01) in\n(2.8). Without the guidance of these penalties, the linear controller and 1-layer neural\nnetwork apparently cannot eﬀectively explore the parameter space.\n0\n50\n100\n150\n200\nIterations\n3000\n2750\n2500\n2250\nRewards\n1-layer NN, SL\n3-layer NN, SL\n5-layer NN, SL\n1-layer NN\nLin\nFig. 4: Learning performance of diﬀerent control structures (1-layer neural network,\n5-layer neural network, and linear controller). By the inclusion of a smoothness loss\n(SL) in the learning objective (2.8), the exploration becomes more eﬀective.\nThe learned 5-layer neural network policy is employed in an actual control task, as\nshown in Figure 5. Compared to the nominal controller, the ﬂights can be maneuvered\nmore eﬃciently in this case with only local information. In terms of the actual cost,\nthe RL agents achieve the cost 41.0, which is about 30% lower than that of the\nnominal controller (58.3). This result can be examined both in the actual state-action\ntrajectories in Figure 5 or the control behaviors in Figure 6. The results indicate that\nRL is able to improve a given controller when the underlying system is nonlinear and\nunknown.\n4.2. Power system frequency regulation. In this case study, we focus on\nthe problem of distributed control for power system frequency regulation [18]. The\nIEEE 39-Bus New England Power System under analysis is shown in Figure 7. In a\ndistributed control setting, each generator can only share its rotor angle and frequency\ninformation with a pre-speciﬁed set of counterparts that are geographically distributed.\n24\nM. JIN AND J. LAVAEI\n0\n25\n50\n75\n100\nTime (sec)\n1.4\n1.6\n1.8\n2.0\nRel. dist. (m)\nA1-A2\nA2-A3\nA3-A4\n(a) Nom.: reletive distances.\n0\n25\n50\n75\n100\nTime (sec)\n−1.0\n−0.5\n0.0\nAngle θ (rad)\nA1\nA2\nA3\nA4\n(b) Nom.: angles θi.\n0\n25\n50\n75\n100\nTime (sec)\n−5\n0\n5\nActions\nA1\nA2\nA3\nA4\n(c) Nom.: actions.\n0\n25\n50\n75\n100\nTime (sec)\n1.6\n1.8\n2.0\nRel. dist. (m)\nA1-A2\nA2-A3\nA3-A4\n(d) NN: relative distances.\n0\n25\n50\n75\n100\nTime (sec)\n−0.5\n0.0\nAngle θ (rad)\nA1\nA2\nA3\nA4\n(e) NN: angles θi.\n0\n25\n50\n75\n100\nTime (sec)\n−10\n−5\n0\n5\nActions\nA1\nA2\nA3\nA4\n(f) NN: actions.\nFig. 5: State and action trajectories in a typical contral task, where the nominal\ncontroller (Nom) and the RL agents achieve costs of 58.3 and 41.0, respectively.\n−1\n0\n1\nX2 −X1\n−10\n−5\n0\n5\n10\nActions\nRL action\nNom. action\n(a) Controller of Agent 1.\n−1\n0\n1\nX4 −X3\n−10\n0\n10\nActions\nRL action\nNom. action\n(b) Controller of Agent 4.\nFig. 6: Demonstration of control outputs for the nominal action and RL agents.\nThe main goal is to optimally adjust the mechanical power input to each generator\nsuch that the phase and frequency at each bus can be restored to their nominal values\nafter a possible perturbation. Let θi denote the voltage angle at a generator bus i (in\nrad). The physics of power systems are modeled by the per-unit swing equation:\n(4.6)\nmi¨θi + di ˙θ = pmi −pei\nwhere pmi is the mechanical power input to the generator at bus i (in p.u.), pei is\nthe electrical active power injection at bus i (in p.u.), mi is the inertia coeﬃcient\nof the generator at bus i (in p.u.-sec2/rad), and di is the damping coeﬃcient of the\ngenerator at bus i (in p.u.-sec/rad). The electrical real power injection pei depends\non the voltage angle diﬀerence in a nonlinear way, as governed by the AC power ﬂow\nSTABILITY-CERTIFIED REINFORCEMENT LEARNING\n25\nequation:\n(4.7)\npei =\nn\nX\nj=1\n|vi||vj| (gij cos(θi −θj) + bij sin(θi −θj))\nwhere n is the number of buses in the system, gij and bij are the conductance and\nsusceptance of the transmission line that connects buses i and j, vi is the voltage phasor\nat bus i, and |vi| is its voltage magnitude. Because the conductance gij is typically\nseveral orders of magnitude smaller than the susceptance bij, for the simplicity of\nmathematical treatment, we omit the cosine term and only keep the sine term that\naccounts for the majority of nonlinearity. Each generator needs to make decisions on\nthe value of the mechanical power pmi to inject in order to maintain the stability of\nthe power system.\n(a) Star-connected structure.\nG8#\nG10#\nG1#\nG2#\nG3#\nG9#\nG6#\nG5#\nG7#\nG4#\n30#\n2#\n1#\n3#\n39#\n4#\n5#\n7#\n8#\n9#\n18#\n37#\n26#\n6#\n31#\n11#\n12#\n14#\n13#\n10#\n32#\n17#\n27#\n16#\n15#\n28#\n24#\n29#\n38#\n35#\n22#\n21#\n19#\n23#\n36#\n33#\n34#\n20#\n25#\n(b) IEEE 39-bus system.\nFig. 7: Illustration of the frequency regulation problem for the New England power\nsystem. The communication among generators follows a star topology.\nLet the rotor angles and the frequency states be denoted as θ =\n\u0002θ1\n· · ·\nθn\n\u0003⊤\nand ω =\n\u0002ω1\n· · ·\nωn\n\u0003⊤, and the generator mechanical power injections be denoted\nas pm =\n\u0002pm1\n· · ·\npmn\n\u0003⊤. Then, the state-space representation of the nonlinear\nsystem is given by:\n(4.8)\n\u0014 ˙θ\n˙ω\n\u0015\n=\n\u0014\n0\nI\n−M −1L\n−M −1D\n\u0015\n|\n{z\n}\nA\n\u0014θ\nω\n\u0015\n|{z}\nx\n+\n\u0014\n0\nM −1\n\u0015\n| {z }\nB\npm +\n\u0014 0\ng(θ)\n\u0015\n| {z }\ng(x)\nwhere g(θ) =\n\u0002g1(θ)\n· · ·\ngn(θ)\u0003⊤with gi(θ) = Pn\nj=1\nbij\nmj ((θi −θj) −sin(θi −θj)),\nand M = diag ({mi}n\ni=1), D = diag ({di}n\ni=1), and L is a Laplacian matrix whose\nentries are speciﬁed in [18, Sec. IV-B]. For linearization (also known as DC approxi-\nmation), the nonlinear part g(x) is assumed to be zero when the phase diﬀerences are\nsmall [18, 17]. On the contrary, we deal with this term in the stability certiﬁcation to\ndemonstrate its capability of producing non-conservative results even for nonlinear\n26\nM. JIN AND J. LAVAEI\nsystems. Similar to the ﬂight formation case, we assume that there exists a distributed\nnominal controller that stablizes the system. To conduct multi-agent RL, each con-\ntroller pmi is a neural network that takes the available phases and frequencies as the\ninput and determines the mechanical power injection at bus i. The main focus is to\nstudy the certiﬁed-gradient bounds for each agent policy in this large-scale setting.\nStability certiﬁcate: Similar to the ﬂight formation problem, the nonlinearities\nin g(x) are in the form of ∆θij −sin ∆θij, where ∆θij = θi −θj represents the phase\ndiﬀerence, which has its slope restricted to [0, 1 −cos(θ)] for every ∆θij ∈[−θ, θ] and\nthus can be treated using the Zames-Falb IQC. In the smoothness margin analysis,\nassume that θ = π\n3 , which requires the phase angle diﬀerence to be within [−π\n3 , π\n3 ].\nThis is a large set of uncertainties that includes both normal and abnormal operational\nconditions. To study the stability of the multi-agent policies, we adopt a black-box\napproach by simply considering the input-output constraint. By simply applying the\nL2 constraint in (3.3), we can only certify stability for Lipschitz constants up to 0.4,\nas shown in Figure 8. Because the distributed control is sparse, we can leverage it by\nsetting the lower and upper bounds ξij = ξij = 0 for each agent i that does not utilize\nobservation j, and ξij = −ξij = l otherwise, where l is the Lipschitz constant to be\ncertiﬁed. This information can be encoded in SDP(P, λ, γ, ξ) in (3.21), which can be\nsolved for L up to 0.6 (doubling the certiﬁcate provided by the L2 constraint).\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nLipschitz const.\n0\n50\n100\n150\n200\nCertified L2 gain\nL2\nInp. sp.\nInp. sp & out. nh.\nFig. 8: Certiﬁed Lipschitz constants for the power system regulation task.\nDue to the problem nature, we further observe that for each agent, the partial\ngradient of the policy with respect to certain observations is primarily one-sided, as\nshown in Figure 9. With a band of ±0.1, the partial gradients remain within either\n[−0.1, 1] or [−1, 0.1] throughout the learning process. This information is gleaned\nduring the learning phase, and we can incorporate it into the partial gradient bounds\n(e.g., ξij = −0.1l and ξij = l for agent i which exhibits positive gradient with respect\nto observation j) to extend the certiﬁcate up to 1.1.\nPolicy gradient RL: Similar to the ﬂight formation task, we perform multi-agent\npolicy gradient RL. The learned neural network controller is implemented in a typical\ncontrol case, whose trajectories are shown in Figure 10. As can be seen, the RL\npolicies can regulate the frequencies more eﬃciently than the nominal controller, with\na signiﬁcantly lowered cost (50.8 vs. 23.9). More importantly, we compare the cases\nof RL with and without regulating the Lipschitz constants in Figure 11. Without\nregulating the gradients, the RL is able to reach a performance slightly higher than its\nstability-certiﬁed counterpart. However, after about iteration 500, the performance\nstarts to deteriorate (due to a possibly large gradient variance and high sensitivity to\nstep size) until it completely loses the previous gains and starts to break the system.\nSTABILITY-CERTIFIED REINFORCEMENT LEARNING\n27\nθi, i ∈{1, ..., 10} ωi, i ∈{1, ..., 10}\n−1\n0\n1\nPartial grads\n(a) G10.\nθ1 θ5 ω1 ω5\n−1\n0\n1\n(b) G4.\nθ1 θ8 ω1 ω8\n−1\n0\n1\n(c) G7.\nFig. 9: Box plots of partial gradients of individual generators (G10, G4, G7) with\nrespect to local information. Grey dashed lines indicate ±0.1.\n0\n6\n12\n18\nTime (sec)\n−0.25\n0.00\n0.25\n0.50\nStates\nPhase θi\nFrequency ωi\n(a) Nom.: phase θi and frequency ωi.\n0\n6\n12\n18\nTime (sec)\n−0.8\n−0.4\n0.0\nActions\nG1\nG2\nG3\nG4\nG5\nG6\nG7\nG8\nG9\nG10\n(b) Nom.: actions.\n0\n6\n12\n18\nTime (sec)\n−0.25\n0.00\n0.25\n0.50\nStates\nPhase θi\nFrequency ωi\n(c) NN: phase θi and frequency ωi.\n0\n6\n12\n18\nTime (sec)\n−0.8\n−0.4\n0.0\nActions\nG1\nG2\nG3\nG4\nG5\nG6\nG7\nG8\nG9\nG10\n(d) NN: actions.\nFig. 10: State and action trajectories of the nominal and neural network controllers\nfor power system frequency regulation, with costs of 50.8 and 23.9, respectively.\nThis intolerable behavior is due to the large Lipschitz gains that grow unboundedly,\nas shown in Figure 12. In comparison, RL with regulated gradient bounds is able to\nmake a substantial improvement, and also exhibits a more stable behavior.\n5. Conclusions. In this paper, we focused on the challenging task of ensuring\nthe stability of reinforcement learning in real-world dynamical systems. By solving the\nproposed SDP feasibility problem, we can oﬀer a preventative certiﬁcate of stability\nfor a broad class of neural network controllers with bounded gradients. Furthermore,\nwe analyzed the (non)conservatism of the certiﬁcate, which was demonstrated in the\nempirical investigation of decentralized nonlinear control tasks, including multi-agent\nﬂight formation and power grid frequency regulation. Results indicated that the set of\nstability-certiﬁed controllers was signiﬁcantly larger than what the existing approaches\n28\nM. JIN AND J. LAVAEI\n0\n500\n1000\nIterations\n60\n35\nRewards\n1-layer NN\n5-layer NN, SP\n1-layer NN, SP\n1-layer NN, HT\nFig. 11: Long-term performance of RL for agents with regulated gradients by soft\npenalty (SP), which adaptively adjusts the coeﬃcients ω2 in (2.8), and hard threshold-\ning (HT), which shrinks the network last layer to satisfy the gradient bounds. The RL\nagents without regulating the gradients exhibit “dangerous” behaviors in the long run.\n0\n500\n1000\nIterations\n0\n20\nLip. const.\n(a) No gradient regulation.\n0\n500\n1000\nIterations\n0.5\n1.0\nLip. const.\nG1\nG2\nG3\nG4\nG5\nG6\nG7\nG8\nG9\nG10\n(b) Gradient regulation.\nFig. 12: Trajectories of Lipschitz constants with and without regulation.\ncan oﬀer, and that the RL agents can substantially improve the performance of nominal\ncontrollers while staying within the safe set. Most importantly, regulation of gradient\nbounds was able to improve on-policy learning stability and avoid “catastropic” eﬀects\ncaused by the unregulated high gains. The present study represents a key step towards\nsafe deployment of reinforcement learning in mission-critical real-world systems.\nREFERENCES\n[1] J. Achiam, D. Held, A. Tamar, and P. Abbeel, Constrained policy optimization, in Proc. of\nthe International Conference on Machine Learning, 2017, pp. 22–31.\n[2] A. K. Akametalu, J. F. Fisac, J. H. Gillula, S. Kaynama, M. N. Zeilinger, and C. J.\nTomlin, Reachability-based safe learning with Gaussian processes, in Proc. of the IEEE\nConference on Decision and Control, 2014, pp. 1424–1431.\n[3] S.-I. Amari, Natural gradient works eﬃciently in learning, Neural computation, 10 (1998),\npp. 251–276.\n[4] D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. Man´e, Concrete\nproblems in AI safety, arXiv preprint arXiv:1606.06565, (2016).\n[5] C. W. Anderson, P. M. Young, M. R. Buehner, J. N. Knight, K. A. Bush, and D. C.\nHittle, Robust reinforcement learning control using integral quadratic constraints for\nrecurrent neural networks, IEEE Transactions on Neural Networks, 18 (2007), pp. 993–1002.\n[6] M. Arcak, Synchronization and pattern formation in diﬀusively coupled systems, in Proc. of\nthe IEEE Conference on Decision and Control, 2012, pp. 7184–7192.\n[7] A. Aswani, H. Gonzalez, S. S. Sastry, and C. Tomlin, Provably safe and robust learning-based\nmodel predictive control, Automatica, 49 (2013), pp. 1216–1226.\nSTABILITY-CERTIFIED REINFORCEMENT LEARNING\n29\n[8] L. Bakule, Decentralized control: An overview, Annual reviews in control, 32 (2008), pp. 87–98.\n[9] F. Berkenkamp, M. Turchetta, A. Schoellig, and A. Krause, Safe model-based reinforce-\nment learning with stability guarantees, in Advances in Neural Information Processing\nSystems, 2017, pp. 908–919.\n[10] R. Bobiti and M. Lazar, A sampling approach to ﬁnding lyapunov functions for nonlinear\ndiscrete-time systems, in Proc. of the IEEE European Control Conference, 2016, pp. 561–566.\n[11] S. Boyd, L. El Ghaoui, E. Feron, and V. Balakrishnan, Linear matrix inequalities in\nsystem and control theory, vol. 15, SIAM, 1994.\n[12] L. Bus¸oniu, R. Babuˇska, and B. De Schutter, Multi-agent reinforcement learning: An\noverview, in Innovations in multi-agent systems and applications-1, Springer, 2010, pp. 183–\n221.\n[13] F. H. Clarke, Optimization and nonsmooth analysis, vol. 5, SIAM, 1990.\n[14] S. P. Coraluppi and S. I. Marcus, Risk-sensitive and minimax control of discrete-time,\nﬁnite-state Markov decision processes, Automatica, 35 (1999), pp. 301–309.\n[15] H. Drucker and Y. Le Cun, Improving generalization performance using double backpropaga-\ntion, IEEE Transactions on Neural Networks, 3 (1992), pp. 991–997.\n[16] G. E. Dullerud and F. Paganini, A course in robust control theory: a convex approach,\nvol. 36, Springer Science & Business Media, 2013.\n[17] S. Fattahi, G. Fazelnia, J. Lavaei, and M. Arcak, Transformation of optimal centralized\ncontrollers into near-globally optimal static distributed controllers, IEEE Transactions on\nAutomatic Control, (2018), pp. 1–1, https://doi.org/10.1109/TAC.2018.2829473.\n[18] G. Fazelnia, R. Madani, A. Kalbat, and J. Lavaei, Convex relaxation for optimal distributed\ncontrol problems, IEEE Transactions on Automatic Control, 62 (2017), pp. 206–221.\n[19] J. M. Fry, M. Farhood, and P. Seiler, IQC-based robustness analysis of discrete-time linear\ntime-varying systems, International Journal of Robust and Nonlinear Control, 27 (2017),\npp. 3135–3157.\n[20] J. Garcıa and F. Fern´andez, A comprehensive survey on safe reinforcement learning, Journal\nof Machine Learning Research, 16 (2015), pp. 1437–1480.\n[21] P. Geibel and F. Wysotzki, Risk-sensitive reinforcement learning applied to control under\nconstraints, Journal of Artiﬁcial Intelligence Research, 24 (2005), pp. 81–108.\n[22] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville, Improved\ntraining of wasserstein gans, in Advances in Neural Information Processing Systems, 2017,\npp. 5767–5777.\n[23] J. Hauser, S. Sastry, and G. Meyer, Nonlinear control design for slightly non-minimum\nphase systems: Application to v/stol aircraft, Automatica, 28 (1992), pp. 665–679.\n[24] W. P. Heath and A. G. Wills, Zames-Falb multipliers for quadratic programming, in Proc. of\nthe IEEE Conference on Decision and Control, 2005, pp. 963–968.\n[25] S. Kakade and J. Langford, Approximately optimal approximate reinforcement learning, in\nProc. of the International Conference on Machine Learning, 2002, pp. 267–274.\n[26] S. M. Kakade, A natural policy gradient, in Advances in neural information processing systems,\n2002, pp. 1531–1538.\n[27] H. K. Khalil, Noninear systems, Prentice-Hall, New Jersey, 2 (1996), pp. 5–1.\n[28] R. M. Kretchmara, P. M. Young, C. W. Anderson, D. C. Hittle, M. L. Anderson, and\nC. Delnero, Robust reinforcement learning control, in Proc. of the IEEE American Control\nConference, vol. 2, 2001, pp. 902–907.\n[29] L. Lessard, B. Recht, and A. Packard, Analysis and design of optimization algorithms via\nintegral quadratic constraints, SIAM Journal on Optimization, 26 (2016), pp. 57–95.\n[30] S. Levine and P. Abbeel, Learning neural network policies with guided policy search under\nunknown dynamics, in Advances in Neural Information Processing Systems, 2014, pp. 1071–\n1079.\n[31] Y. Li, Deep reinforcement learning: An overview, arXiv preprint arXiv:1701.07274, (2017).\n[32] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver,\nand D. Wierstra, Continuous control with deep reinforcement learning, arXiv preprint\narXiv:1509.02971, (2015).\n[33] A. Megretski and A. Rantzer, System analysis via integral quadratic constraints, IEEE\nTransactions on Automatic Control, 42 (1997), pp. 819–830.\n[34] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and\nK. Kavukcuoglu, Asynchronous methods for deep reinforcement learning, in Proc. of the\nInternational Conference on Machine Learning, 2016, pp. 1928–1937.\n[35] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra,\nand M. Riedmiller, Playing atari with deep reinforcement learning, arXiv preprint\narXiv:1312.5602, (2013).\n30\nM. JIN AND J. LAVAEI\n[36] T. M. Moldovan and P. Abbeel, Safe exploration in Markov decision processes, in Proc. of\nthe International Conference on Machine Learning, 2012, pp. 1451–1458.\n[37] A. G. Ororbia II, D. Kifer, and C. L. Giles, Unifying adversarial training algorithms with\ndata gradient regularization, Neural computation, 29 (2017), pp. 867–887.\n[38] A. Packard and J. Doyle, The complex structured singular value, Automatica, 29 (1993),\npp. 71–109.\n[39] T. J. Perkins and A. G. Barto, Lyapunov design for safe reinforcement learning, Journal of\nMachine Learning Research, 3 (2002), pp. 803–832.\n[40] M. G. Safonov and V. V. Kulkarni, Zames-Falb multipliers for MIMO nonlinearities, in\nProc. of the American Control Conference, vol. 6, 2000, pp. 4144–4148.\n[41] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, Trust region policy\noptimization, in Proc. of the International Conference on Machine Learning, 2015, pp. 1889–\n1897.\n[42] P. Seiler, Stability analysis with dissipation inequalities and integral quadratic constraints,\nIEEE Transactions on Automatic Control, 60 (2015), pp. 1704–1709.\n[43] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrit-\ntwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al., Mastering the game\nof go with deep neural networks and tree search, Nature, 529 (2016), p. 484.\n[44] I. Stoica, D. Song, R. A. Popa, D. Patterson, M. W. Mahoney, R. Katz, A. D. Joseph,\nM. Jordan, J. M. Hellerstein, J. E. Gonzalez, et al., A Berkeley view of systems\nchallenges for AI, arXiv preprint arXiv:1712.05855, (2017).\n[45] Y. Sui, A. Gotovos, J. Burdick, and A. Krause, Safe exploration for optimization with\nGaussian processes, in Proc. of the International Conference on Machine Learning, 2015,\npp. 997–1005.\n[46] R. S. Sutton, Integrated architecture for learning, planning, and reacting based on approximat-\ning dynamic programming, in Proc. of the International Conference on Machine Learning,\n1990, pp. 216–224.\n[47] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction, vol. 1, MIT press\nCambridge, 1998.\n[48] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and\nR. Fergus, Intriguing properties of neural networks, in Proc. of the International Conference\non Learning Representations, 2014.\n[49] C. Watkins and P. Dayan, Q-learning, Machine learning, 8 (1992), pp. 279–292.\n[50] W. Wiesemann, D. Kuhn, and B. Rustem, Robust Markov decision processes, Mathematics of\nOperations Research, 38 (2013), pp. 153–183.\n[51] J. C. Willems, Dissipative dynamical systems, Part II: Linear systems with quadratic supply\nrates, Archive for rational mechanics and analysis, 45 (1972), pp. 352–393.\n[52] R. J. Williams, Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning, Machine learning, 8 (1992), pp. 229–256.\n[53] G. Zames and P. Falb, Stability conditions for systems with monotone and slope-restricted\nnonlinearities, SIAM Journal on Control, 6 (1968), pp. 89–108.\n[54] A. Zemouche and M. Boutayeb, On LMI conditions to design observers for lipschitz nonlinear\nsystems, Automatica, 49 (2013), pp. 585–591.\n[55] K. Zhou, J. C. Doyle, K. Glover, et al., Robust and optimal control, vol. 40, Prentice hall\nNew Jersey, 1996.\n",
  "categories": [
    "cs.SY",
    "cs.LG"
  ],
  "published": "2018-10-26",
  "updated": "2018-10-26"
}