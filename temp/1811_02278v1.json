{
  "id": "http://arxiv.org/abs/1811.02278v1",
  "title": "Off-the-Shelf Unsupervised NMT",
  "authors": [
    "Chris Hokamp",
    "Sebastian Ruder",
    "John Glover"
  ],
  "abstract": "We frame unsupervised machine translation (MT) in the context of multi-task\nlearning (MTL), combining insights from both directions. We leverage\noff-the-shelf neural MT architectures to train unsupervised MT models with no\nparallel data and show that such models can achieve reasonably good\nperformance, competitive with models purpose-built for unsupervised MT.\nFinally, we propose improvements that allow us to apply our models to\nEnglish-Turkish, a truly low-resource language pair.",
  "text": "arXiv:1811.02278v1  [cs.CL]  6 Nov 2018\nOff-the-Shelf Unsupervised NMT\nChris Hokamp1 and Sebastian Ruder1,2 and John Glover1\n1Aylien Ltd., Dublin, Ireland\n2Insight Centre, NUI Galway\n{chris,sebastian,john}@aylien.com\nAbstract\nWe frame unsupervised machine translation\n(MT) in the context of multi-task learning\n(MTL), combining insights from both direc-\ntions and draw connections to policy-based re-\ninforcement learning.\nWe leverage off-the-\nshelf neural MT architectures to train unsu-\npervised MT models with no parallel data and\nshow that such models can achieve reasonably\ngood performance, competitive with models\npurpose-built for unsupervised MT. Finally,\nwe propose improvements that allow us to ap-\nply our models to English-Turkish, a truly low-\nresource language pair.\n1\nIntroduction\nRecent\ninnovations\nin\nneural\nsequence-to-\nsequence (s2s) models (Vaswani et al., 2017;\nWu et al.,\n2016;\nMiceli Barone et al.,\n2017)\nhave improved the state of the art in Machine\nTranslation (MT). However, the major obstacle\nto better MT performance for most language\npairs is still the lack of high-quality parallel\ndatasets.\nGiven that there is often no shortage\nof\nmonolingual\ndata,\nunsupervised\nmachine\ntranslation has recently become an attractive area\nof research (Artetxe et al., 2018; Lample et al.,\n2018).\nCurrent unsupervised MT architectures\nrequire purpose-built loss functions and model\narchitectures that are speciﬁcally designed for the\nunsupervised setting. In this work we take a step\nback and experiment with a different approach;\ntraining a single model, or a pair of models,\nwith no additional parameters or loss functions\nbeyond the vanilla sequence-to-sequence model.\nOur approach is attractive because it allows us\nto seamlessly leverage any off-the-shelf MT\nsystem and we show results competitive with\nthe state-of-the-art for unsupervised MT. Current\nunsupervised MT methods rely on bilingual dic-\ntionaries for initialization, and we also investigate\nsettings for bilingual dictionary induction which\nmake our models more robust. Finally, we report\nresults for English↔Turkish, a truly low-resource\nlanguage pair.\n2\nMulti-Task Unsupervised MT\nMulti-task learning (Caruana, 1993) encourages\nthe creation of representations that are shared\namong related tasks, potentially leading to better\nmodel generalization. Intuitively, knowledge can\nbe transferred between similar tasks, resulting in\nbetter performance and reduced data sparsity over\nmodels trained for each task individually. Simi-\nlar to work on learning multi-way translation mod-\nels (Firat et al., 2017; Johnson et al., 2017), recent\nwork on unsupervised MT can be viewed through\nthe lens of multi-task learning. In particular, exist-\ning approaches make use of two types of tasks:\nT1 : X −→Y (Translation)\nT2 : ˆY −→Y (Autoencoding)\nHere we use X and Y to denote two distinct do-\nmains, in our case text from two different lan-\nguages, and ˆY is a corrupted version of Y.\nTranslation\nIn the unsupervised setting, where\nwe only have access to monolingual data, the\nobstacle in practice is how to obtain the source\ntranslations X of the target data Y.\nRecent\nwork on unsupervised bilingual dictionary induc-\ntion (Conneau et al., 2018; Artetxe et al., 2018)\nhas shown that a reasonably good word- or phrase-\nlevel dictionary can be used to obtain a noisy ver-\nsion of X, denoted ˆX, by mapping each word or\nphrase in Y to its corresponding word or phrase\nin X, according to the dictionary.\nˆX may ini-\ntially be a poor approximation of X, because it\ndoes not have the expected ordering of the source\ntask, and because the dictionary is noisy, and does\nnot take context into account when performing\nthe mapping. However, ˆX can be iteratively re-\nﬁned by jointly learning the inverse of T1 and\nT2, and updating the translation model with the\nnewly generated instances. In practice, we thus\nalso perform the same tasks in the other direction,\nT3 : ˆY −→X and T4 : ˆX −→X.\nAutoencoding\nSeveral studies have recently\nshown that including monolingual data in Neural\nMT (NMT) training curricula can improve per-\nformance (Gulcehre et al., 2015; Sennrich et al.,\n2016; Currey et al., 2017). Monolingual data has\nlong been a key part of Statistical MT (SMT)\nsystems (Koehn, 2010) and can be included into\nmulti-task systems using the autoencode task. To\nmake the task more challenging and avoid over-\nﬁtting, noise may be added to the input of the auto-\nencoding task by scrambling or dropping tokens\n(Lample et al., 2018).\nTherefore, training an unsupervised model from\ntwo disjoint sets of monolingual data entails the\nfollowing high-level steps:\n1) obtain bilingual\nword or phrase dictionaries in both translation di-\nrections (§2.1); 2) generate synthetic data (ˆX, Y)\nand ( ˆY, X) (§2.2); 3) initialize model(s) using\nsynthetic data for the translation tasks, along\nwith monolingual data for the auto-encoding tasks\n(§2.3); 4) train initialized models using back-\ntranslation (§2.4). We describe each of these in\nthe following sections.\n2.1\nUnsupervised Bilingual Dictionary\nInduction\nIn order to boostrap ˆX −→Y and ˆY −→X, we\nlearn an unsupervised bilingual lexicon of word-\nto-word translations.\nSimilar to Søgaard et al.\n(2018), we found the method by Lample et al.\n(2018) to only work for closely related languages.\nFor a truly low-resource language pair we\nfound it essential to use a seed dictionary of\nidentical strings shared by both vocabularies\n(Søgaard et al., 2018) instead of adversarial train-\ning (Conneau et al., 2018). In addition, when ex-\ntracting the bilingual dictionary, we restrict ex-\ntracted pairs to those within a certain frequency\nrank window of one another. We also use n-grams\nin the bilingual dictionary (Lample et al., 2018).\n2.2\nSynthetic Data Generation\nTwo methods are used to generate synthetic train-\ning example pairs for jump-starting training: (1)\nword-translation, which simply maps each token\nin X to Y, if it exists in the bilingual dictionary,\nand vice versa (Lample et al., 2018); and (2) smt-\ntranslation, which uses a phrase table built from\nthe bilingual lexicon, along with a monolingual\nlanguage model, to generate synthetic translations\nusing an SMT decoder (Lample et al., 2018). In\norder to obtain the translation probability p(e|f)\nfor the phrase table, we select the top-k words Yi\nfor source word xi based on cross-domain similar-\nity local scaling (CSLS) (Conneau et al., 2018) as\nfollows:\nYi = k-argmax\nyj∈Y\nCSLS(xi, yj).\n(1)\nWe then obtain the normalized translation proba-\nbility for each pair (xi, yj) with Eq. 2:\np(yj|xi) =\neβ∗CSLS(xi,yj)\nP\nyk∈Yi\neβ∗CSLS(xi,yk),\n(2)\nwhere β is a temperature hyperparameter that we\nﬁx at β = 10.\nThe other component of the\nSMT model is a 5-gram language model trained\non the target language in each direction.\nWe\nuse word-translation or smt-translation to translate\neach monolingual dataset into the other language.1\n2.3\nInitialization\nWe now train initial NMT model(s) on the syn-\nthetic translation task data, combined with the au-\ntoencode task data, which is created by scrambling\nsegments from each language2.\n2.4\nTraining\nFinally,\nwe use the initial models to jump-\nstart unsupervised training with dynamic epochs,\nwhich generate translation task data by back-\ntranslating.\nDuring unsupervised training, short\ntraining epochs are generated by asking the cur-\nrent model to backtranslate data from X −→ˆY,\nY −→ˆX, training with this data, then updating\nthe current model.\nAs using the unsupervised\nvalidation metric of Lample et al. (2018) is time-\nconsuming, we validate the number of epochs on\n1Note that the actual monolingual data is always the tar-\nget side of each synthetic training example. In other words,\nwe create training data (ˆX, Y) and ( ˆY, X), but not (Y, ˆX)\nor (X, ˆY).\n2We use the same approach as Lample et al. (2018), drop-\nping out words with probability = 0.1, and shufﬂing word\norder within a window of size = 3 from each word’s original\nindex.\nAlgorithm 1 Our approach to unsupervised MT.\nInitialize epoch counter E ←0\nInitialize machine translation model weights θ\nrepeat\nBuild a new dataset (sample tasks, run translation\nmodel with weights θ)\nInitialize batch counter K ←0\nInitialize MT weights ˆθ ←θ\nrepeat\nCompute grad ∇ˆθ ≈\n1\nN\nPN\ni=1 ∇ˆθ log Pˆθ(yi|xi)\nUpdate parameters ˆθ ←ˆθ + α∇ˆθ\nuntil K > Kmax\nUpdate machine translation model weights θ ←ˆθ\nuntil E > Emax\nthe ﬁrst run and ﬁx the number of epochs for all\nother runs. The pseudocode for our unsupervised\nMT method is given in Algorithm 1.\nConnection\nto\npolicy-based\nreinforcement\nlearning\nOur approach can also be viewed\nas a form of policy-based reinforcement learn-\ning (RL) (Williams, 1992; Sutton et al., 1999).\nFrom this viewpoint, our translation model is\nan instance of a policy πθ, that takes actions a\n(predicting the next word in the target sequence\ny) based on observations s (the source sequence\nx, and any translations produced up to the current\ntimestep).\nWe sample from the policy πθ and\nthen perform a gradient update step. In particular,\nanalogous to RL, the data encountered by our\nmodel is non-stationary, which was also observed\nin Xia et al. (2016).\n2.5\nModels\nWe test our method with two types of models:\ns2s with attention (Bahdanau et al., 2014) and the\ntransformer model (Vaswani et al., 2017). We in-\ntroduce two ways of re-purposing these models for\nunsupervised NMT:\nSynchronized\nTwo models are trained in paral-\nlel, one for X −→Y and another for Y −→X,\neach supporting the auto-encoding and translation\ntask for one target language.\nEach model gen-\nerates new training instances for the other one,\nfor example, X −→Y takes monolingual sam-\nples from X, and produces outputs ˆY. The re-\nsulting ( ˆY, X) outputs are then used as translation\ntask training examples for the Y −→X system.\nMulti-task\nA single model is trained to do all\nfour tasks. Input data from each of the four tasks\nis pre-pended with a special token indicating the\ntarget task that should be used to produce the out-\nput. In this setting, the model produces its own\ntraining data intrinsically, using the same process\ndescribed for the synchronized setting.\nNote that, in the synchronized setting, we do not\nneed to explicitly indicate tasks to the model, since\neach model only decodes into one language. In the\nmulti-task setting, we preﬁx inputs with a special\ntoken indicating the target language that should be\ndecoded.\n3\nExperiments\nWe conduct two sets of experiments to test the vi-\nability of learning an NMT system in an unsuper-\nvised setting, using monolingual, non-parallel data\nonly, and off-the-shelf s2s models, with no modi-\nﬁcation to model source code. The ﬁrst set of ex-\nperiments follows Lample et al. (2018), studying\nthe EN ↔FR language pair, using the Multi30k\ndataset (Bojar et al., 2017). The second set of ex-\nperiments is an attempt to establish a realistic un-\nsupervised baseline for EN ↔TR, a truly low-\nresource language pair with little typological sim-\nilarity.\nSYSTEM\nFR-EN\nWORD TRANSLATION\nword-for-word\n13.48\nNMT INITIALIZATION\nBaseline s2s\n20.85\n+autoencode\n22.57\n+scramble\n21.12\n+pretrained-embeddings\n(trainable)\n22.55\n+pretrained-embeddings\n(static)\n21.8\nTable 1: BLEU scores for FR-EN on the Multi-30k de-\nvelopment set, training with the word-translation and\n(optionally) autoencode tasks, translating in one direc-\ntion.\n3.1\nEN-FR\nInitialization\nIn order to determine reasonable\nsettings, we trained a single FR-EN model for the\nmulti30k dataset, using the baseline s2s with at-\ntention model in Marian (Junczys-Dowmunt et al.,\n2018) and\nˆ\nFR −→EN data created using the\nword-translation method.\nWe present results in\nTable 1. Using trainable pretrained embeddings\nachieved slightly lower results than the best con-\nﬁguration, but converged much faster.\nWe thus\nchoose this conﬁguration for all future experi-\nments.\nSYSTEM\nFR-EN\nEN-FR\nWORD TRANSLATION\nword-for-word\n13.48\n8.18\nNMT INITIALIZATION\nInitialization (multi-task)\n18.56\n21.46\nInitialization (sync)\n26.6\n23.4\nNMT UNSUPERVISED\nUnsupervised (multi-task)\n18.71\n18.54\nUnsupervised (sync)\n31.15\n30.2\nTable 2: Initialization and Unsupervised training re-\nsults for multi30k development set.\nSYSTEM\nFR-EN\nEN-FR\nOurs\n28.45\n27.57\nLample et al. 2018\n32.07\n32.76\nTable 3: Comparing our best model with Lample et al.\n(2018) on the multi30k test set.\nWe note that\nthey\ndo\nnot\nspecify\ntheir\ntokenizer\nor\nBLEU\nscore\ncomputation\nmethod,\nwhereas\nwe\nuse\ndetokenized,\nlowercased BLEU, as computed by\nmulti-bleu-detok.perl.\nUnsupervised Training\nFor EN\n↔\nFR,\nwe follow Lample et al. (2018) in preparing the\ndatasets. See appendix A for conﬁguration details.\nThe two component models in the synchronized\nsetting use the default s2s with attention, whereas\nthe multi-task model uses the Transformer archi-\ntecture (Vaswani et al., 2017), as implemented in\nMarian (Junczys-Dowmunt et al., 2018)3.\nResults are presented in table 2.\nSurpris-\ningly, we observe that the synchronized s2s mod-\nels perform much better than the multi-task trans-\nformer, especially during the unsupervised train-\ning phase. We hypothesize that the Transformer\nmodel quickly overﬁts to the autoencoding task\nour this small dataset. Table 3 compares the best\nsystem with Lample et al. (2018). Their model,\nwhich includes a special adversarial loss and is de-\nsigned speciﬁcally for NMT, is a few BLEU points\nbetter, but our simple synchronized models still\nperform remarkably well.\n3Note that source and target embeddings must be shared\nin the multi-task setting\n3.2\nCase study: Unsupervised MT on a truly\nlow-resource language pair\nIn order to experiment in a more challenging set-\nting, we choose English-Turkish (EN-TR). We\nuse only 2.5 million randomly selected segments\nfrom the news-crawl portion of the monolin-\ngual training data provided by the WMT task or-\nganizers4.\n3.2.1\nInitialization\nFor EN-TR, we experimented with SMT initial-\nization, hypothesizing that the language model\ncomponent of the SMT decoder might be able\nto correct issues due to the drastic differences in\nword order between the languages (see appendix\nfor examples of system outputs).\nEN-TR\nUnsupervised*\n32.71\nSupervised*\n39.22\nFasttext vectors,\nsupervised\n(default) training\n45.80\nMax Rank 20000\n48.80\nMax Rank 30000\n48.93\nTable 4: Bilingual dictionary induction results. *: Re-\nsults are from (Søgaard et al., 2018) and not directly\ncomparable. \"Max Rank\" indicates the maximum fre-\nquency rank for word pairs in Procrustes reﬁnement.\nThe ﬁrst challenge for this language pair was\nto ﬁnd a way to do unsupervised alignment of the\nembeddings in a way that resulted in a reason-\nably good dictionary. Table 4 shows the results\nof several experiments. By increasing the maxi-\nmum rank of the extracted pairs, and using the su-\npervised training paradigm, initialized with identi-\ncally spelled strings (Søgaard et al., 2018), we ob-\ntain a signiﬁcant performance boost over the base-\nline.\nConﬁguration details are given in appendix B.\nTable 5 shows the results for word translation and\nSMT, initialization, and unsupervised training. To\nthe best of our knowledge, these are the ﬁrst re-\nported results on completely unsupervised MT for\nEN-TR. On the newstest2016 test set, our\nsystem also achieves BLEU scores of 5.66 for\nEN →TR and 7.29 for TR →EN. Given that\nCurrey et al. (2017) report a BLEU score of 9.4 for\ntheir EN →TR baseline, which uses all avail-\n4http://www.statmt.org/wmt18/translation-task.html\nSYSTEM\nEN-TR\nTR-EN\nBILINGUAL DICTIONARY\nword-for-word\n1.49\n2.29\nSMT\n0.92\n1.94\nNMT INITIALIZATION\nBaseline s2s + ﬁltered\n1.52\n2.75\nBaseline s2s + unﬁltered\n1.55\n3.12\nTransformer + unﬁltered\n2.05\n3.13\nNMT UNSUPERVISED\nTransformer\n4.42\n6.26\nTable\n5:\nnewsdev2016\nlowercased,\ndetok-\nenized BLEU scores for EN↔TR, computed with\nmulti-bleu-detok.perl.\n\"ﬁltered\" indicates\nlimiting training data to those pairs which get at least\n40 BLEU when round-tripped through both SMT\nsystems.\nable parallel training data, we consider these re-\nsults quite encouraging.\n4\nConclusions\nWe have framed unsupervised MT as multi-task\nlearning, and shown that carefully designed ex-\nperimental setups can achieve good performance\non unsupervised MT with no changes to exist-\ning s2s models. Thus, any existing NMT frame-\nwork could already be used to train unsupervised\nMT models.\nWe compared to existing purpose\nbuilt unsupervised MT systems, and established a\nstrong baseline for EN-TR, a truly low-resource\nlanguage pair. In future work we hope to focus\nupon including more than two languages in a sin-\ngle model, as well as experimenting with ways to\nimprove initialization.\nReferences\nMikel Artetxe, Gorka Labaka, Eneko Agirre, and\nKyunghyun Cho. 2018. Unsupervised Neural Ma-\nchine Translation. In Proceedings of ICLR 2018.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014.\nNeural machine translation by jointly\nlearning to align and translate.\narXiv preprint\narXiv:1409.0473.\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann,\nYvette Graham, Barry Haddow, Shujian Huang,\nMatthias Huck, Philipp Koehn, Qun Liu, Varvara\nLogacheva, Christof Monz, Matteo Negri, Matt\nPost, Raphael Rubino, Lucia Specia, and Marco\nTurchi. 2017.\nFindings of the 2017 conference\non machine translation (wmt17).\nIn Proceedings\nof the Second Conference on Machine Translation,\nVolume 2:\nShared Task Papers, pages 169–214,\nCopenhagen, Denmark. Association for Computa-\ntional Linguistics.\nRich\nCaruana.\n1993.\nMultitask\nlearning:\nA\nknowledge-based source of inductive bias. In Pro-\nceedings of the Tenth International Conference on\nMachine Learning.\nAlexis Conneau, Guillaume Lample, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Hervé Jégou. 2018.\nWord Translation Without Parallel Data.\nIn Pro-\nceedings of ICLR 2018.\nAnna Currey, Antonio Valerio Miceli Barone, and Ken-\nneth Heaﬁeld. 2017. Copied monolingual data im-\nproves low-resource neural machine translation. In\nProceedings of the Second Conference on Machine\nTranslation, pages 148–156. Association for Com-\nputational Linguistics.\nOrhan Firat, Kyunghyun Cho, Baskaran Sankaran,\nFatos T. Yarman-Vural, and Yoshua Bengio. 2017.\nMulti-way, multilingual neural machine translation.\nComputer Speech & Language, 45:236–252.\nC. Gulcehre, O. Firat, K. Xu, K. Cho, L. Barrault, H.-C.\nLin, F. Bougares, H. Schwenk, and Y. Bengio. 2015.\nOn Using Monolingual Corpora in Neural Machine\nTranslation. ArXiv e-prints.\nKenneth Heaﬁeld. 2011. Kenlm: Faster and smaller\nlanguage model queries. In Proceedings of the Sixth\nWorkshop on Statistical Machine Translation, WMT\n’11, pages 187–197, Stroudsburg, PA, USA. Associ-\nation for Computational Linguistics.\nMelvin Johnson, Mike Schuster, Quoc V. Le, Maxim\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\nFernanda Viégas, Martin Wattenberg, Greg Corrado,\nMacduff Hughes, and Jeffrey Dean. 2017. Google’s\nmultilingual neural machine translation system: En-\nabling zero-shot translation. Transactions of the As-\nsociation for Computational Linguistics, 5:339–351.\nMarcin Junczys-Dowmunt,\nRoman Grundkiewicz,\nTomasz Dwojak, Hieu Hoang, Kenneth Heaﬁeld,\nTom Neckermann, Frank Seide, Ulrich Germann,\nAlham Fikri Aji, Nikolay Bogoychev, André F. T.\nMartins, and Alexandra Birch. 2018. Marian: Fast\nneural machine translation in c++. arXiv preprint\narXiv:1804.00344.\nPhilipp Koehn. 2010. Statistical Machine Translation,\n1st edition. Cambridge University Press, New York,\nNY, USA.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan,\nWade Shen, Christine Moran,\nRichard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra\nConstantin, and Evan Herbst. 2007. Moses: Open\nsource toolkit for statistical machine translation. In\nProceedings of the 45th Annual Meeting of the ACL\non Interactive Poster and Demonstration Sessions,\nACL ’07, pages 177–180, Stroudsburg, PA, USA.\nAssociation for Computational Linguistics.\nG. Lample, M. Ott, A. Conneau, L. Denoyer, and\nM. Ranzato. 2018. Phrase-Based and Neural Un-\nsupervised Machine Translation. ArXiv e-prints.\nGuillaume\nLample,\nLudovic\nDenoyer,\nand\nMarc’Aurelio\nRanzato.\n2018.\nUnsupervised\nMachine Translation Using Monolingual Corpora\nOnly. In Proceedings of ICLR 2018.\nAntonio Valerio Miceli Barone, Jindrich Helcl, Rico\nSennrich, Barry Haddow, and Alexandra Birch.\n2017.\nDeep Architectures for Neural Machine\nTranslation. Association for Computational Lin-\nguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Improving Neural Machine Translation Mod-\nels with Monolingual Data. Association for Compu-\ntational Linguistics (ACL).\nAnders Søgaard, Sebastian Ruder, and Ivan Vuli´c.\n2018. On the Limitations of Unsupervised Bilingual\nDictionary Induction. In Proceedings of ACL 2018.\nRichard S. Sutton, David McAllester, Satinder Singh,\nand Yishay Mansour. 1999. Policy Gradient Meth-\nods for Reinforcement Learning with Function Ap-\nproximation. NIPS.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V. Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran As-\nsociates, Inc.\nRonald J. Williams. 1992. Simple Statistical Gradient-\nFollowing Algorithms for Connectionist Reinforce-\nment Learning. Machine Learning, 3(8):229–256.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe,\nMohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun,\nYuan Cao,\nQin\nGao,\nKlaus\nMacherey, et al. 2016.\nGoogle’s neural ma-\nchine translation system: Bridging the gap between\nhuman and machine translation.\narXiv preprint\narXiv:1609.08144.\nYingce Xia, Di He, Tao Qin, Liwei Wang, Nenghai Yu,\nTie-Yan Liu, and Wei-Ying Ma. 2016. Dual Learn-\ning for Machine Translation. NIPS.\nA\nEN-FR Training Conﬁguration\nWe select alternating segments from the English\nand French portions of the Multi30k corpus, so\nthere are no parallel segments in the two resulting\nmonolingual corpora.\nWe test models in both the synchronized and\nmulti-task settings described above. During unsu-\npervised training, we replace the data-generating\nmodels with a new model after 30000 training ex-\namples have been seen, and train until we have\nseen 500000 examples total.\nTo create the bilingual dictionaries for word\ntranslation, we use the pretrained fasttext embed-\ndings5, which are then aligned using MUSE. The\nresulting aligned embeddings achieve 81.86 P@1\non the evaluation dataset used by Conneau et al.\n(2018).\nFor the multi-task setting, because we are train-\ning a single model which can translate in both\ndirections, we also use 2.5 million sentences\nfrom the News Crawl 2017 monolingual EN and\nFR datasets6 to create a shared vocabulary us-\ning wordpiece encoding with 32000 symbols\n(Johnson et al., 2017), and train a shared embed-\nding using Fasttext.\nAll models are trained for 10 epochs over the\ntraining data, batches are evenly sampled from the\nauto-encoding and translation tasks.\nB\nEN-TR Training Conﬁguration\nUsing the best unsupervised alignment method\nfrom table 4, we extract an SMT phrase table by\nmapping each word to its K most similar words in\nthe other language, using the technique discussed\nin section 2.2. When extracting the dictionary, we\nrequire that extracted SRC →TRG mappings\nare within frequency rank ±10000 of one another.\nFor each language, we also include the top 100000\nn-grams up to length 5 in the phrase table, fol-\nlowing the technique of Lample et al. (2018). We\ntrain a 5-gram language model for each language\nusing KenLM (Heaﬁeld, 2011), and produce the\n( ˆ\nEN, TR) and ( ˆ\nTR, EN) by decoding in both\ndirections with Moses (Koehn et al., 2007).\nWe use a shared wordpiece vocabulary of 8000\nsymbols, because we hypothesize that the morpho-\nlogical complexity of Turkish will be better cap-\ntured by a token set which aggressively segments\nwords.\n5https://fasttext.cc/\n6http://www.statmt.org/wmt18/translation-task.html\nThe initialization models are trained until all\nSMT-generated data has been seen once, evenly\nsampling from the translation and autoencoding\ntasks. The unsupervised model is trained until it\nhas seen 5,000,000 examples, updating the data-\ngenerating model every 30,000 examples.\nEvaluation is done on the full WMT 2016 EN-\nTR development and test sets, using lowercased,\ndetokenized BLEU. This should make our exper-\nimental settings easy to replicate, and put a lower\nbound on what is achievable with more training\ndata, or more sophisticated initialization methods.\nC\nExample Outputs\nTable 6 provides some examples of the output of\nour models.\nSource\nbir ba¸ska ö˘grenci mikel sykes ise Lamb’in kendisine 2014-15 ö˘grenim yılı son-\nlarında stresle bo˘gu¸stu˘gunu söyledi˘gini belirtti.\nReference\nanother student, mikel sykes, said Lamb told him he was dealing with stress at the\nend of the 2014-15 academic year.\nword-for-word\na other student darri maccartney and mumford ” him - graduate year in sometime\nby reducing visibly telling stated.\nSMT\nfor david gilmour - year graduate student and another sometime visibly shocked by\nreducing hesitation mumford said.\nTransformer (initial-\nization)\nanother student, mike sykes, acknowledged telling him he had been under pressure\nwhile learning him in the past two years.\nTransformer\n(unsu-\npervised)\nanother student, mikel sykes, said lamb’s learning was under pressure last year while\ntelling him to drown him in 2014-15.\nSource\nzaman zaman çok ¸siddetli çatı¸smalara sahne olan ilçede vatanda¸slar ilçeyi terk\nediyor.\nReference\nresidents are leaving the district, which has occasionally witnessed very violent\nclashes.\nword-for-word\ntime time extremely severe insurgent stage whose titagarh citizens towns, leave\nshould.\nSMT\ncitizens should leave time onstage towns , has seen a very intense bloodshed\nTransformer (initial-\nization)\nthe violence is a very violent time for citizens to abandon the town ’s stage.\nTransformer\n(unsu-\npervised)\ntime to see more violent clashes in the towns where citizens were abandoned.\nSource\nson çeyrekte , yer hizmetleri geliri % arttı ancak i¸sletme geliri büyük paketler ve\ndahili sigortadaki yüksek maliyetler nedeniyle temelde durgundu.\nReference\nIn the latest quarter, ground revenue rose 29 percent but operating income was ba-\nsically ﬂat on higher costs for larger packages and self-insurance.\nword-for-word\nlast quarter, place services proﬁts % increased however management proﬁts huge\npackets and, but now high affordability due radically.\nSMT\nfourth - quarter proﬁts and revenues increased %, due in larger packets services\nbusiness and high affordability , but now durgundu sigortadaki radically.\nTransformer (initial-\nization)\nhowever, revenues increased by % in the fourth quarter due to higher proﬁts of $.\nbn in basic services and management packages.\nTransformer\n(unsu-\npervised)\nlast quarter, revenue increased %29 per share, however, to packages of large busi-\nness income and included higher insurance costs due to basic duration.\nTable 6: Example outputs of different systems for TR →EN.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2018-11-06",
  "updated": "2018-11-06"
}