{
  "id": "http://arxiv.org/abs/1811.02640v2",
  "title": "Deep Probabilistic Ensembles: Approximate Variational Inference through KL Regularization",
  "authors": [
    "Kashyap Chitta",
    "Jose M. Alvarez",
    "Adam Lesnikowski"
  ],
  "abstract": "In this paper, we introduce Deep Probabilistic Ensembles (DPEs), a scalable\ntechnique that uses a regularized ensemble to approximate a deep Bayesian\nNeural Network (BNN). We do so by incorporating a KL divergence penalty term\ninto the training objective of an ensemble, derived from the evidence lower\nbound used in variational inference. We evaluate the uncertainty estimates\nobtained from our models for active learning on visual classification. Our\napproach steadily improves upon active learning baselines as the annotation\nbudget is increased.",
  "text": "arXiv:1811.02640v2  [stat.ML]  30 Nov 2018\nDeep Probabilistic Ensembles: Approximate\nVariational Inference through KL Regularization\nKashyap Chitta\nCarnegie Mellon University\nkchitta@andrew.cmu.edu\nJose M. Alvarez\nNVIDIA\njosea@nvidia.com\nAdam Lesnikowski\nNVIDIA\nalesnikowski@nvidia.com\nAbstract\nIn this paper, we introduce Deep Probabilistic Ensembles (DPEs), a scalable tech-\nnique that uses a regularized ensemble to approximate a deep Bayesian Neural\nNetwork (BNN). We do so by incorporating a KL divergence penalty term into\nthe training objective of an ensemble, derived from the evidence lower bound\nused in variational inference. We evaluate the uncertainty estimates obtained from\nour models for active learning on visual classiﬁcation. Our approach steadily im-\nproves upon active learning baselines as the annotation budget is increased.\n1\nIntroduction\nModeling uncertainty for deep neural networks has a wide range of potential applications: it can\nprovide important information about the reliability of predictions, or better strategies for labeling\ndata in order to improve performance. Bayesian methods, which provide an approach to do this, have\nrecently gained momentum, and are beginning to ﬁnd more widespread use in practice [6, 3, 5, 10].\nThe formulation of a Bayesian Neural Network (BNN) involves placing a prior distribution over all\nthe parameters of a network, and obtaining the posterior given the observed data [13]. The distribu-\ntion of predictions provided by a trained BNN helps capture the model’s uncertainty. However, train-\ning a BNN involves marginalization over all possible assignments of weights, which is intractable\nfor deep BNNs without approximations [6, 3, 5]. Existing approximation algorithms limit their ap-\nplicability, since they do not speciﬁcally address the fact that deep BNNs on large datasets are more\ndifﬁcult to optimize than deterministic networks, and require extensive parameter tuning to provide\ngood performance and uncertainty estimates [14]. Furthermore, estimating uncertainty in BNNs\nrequires drawing a large number of samples at test time, which can be extremely computationally\ndemanding.\nIn practice, a common approach to estimate uncertainty is based on ensembles [12, 1]. Different\nmodels in a trained ensemble of networks are treated as if they were samples drawn directly from a\nBNN posterior. Ensembles are easy to optimize and fast to execute. However, they do not approxi-\nmate uncertainty in the same manner as a BNN. For example, the parameters in the ﬁrst kernel of the\nﬁrst layer of a convolutional neural network may serve a completely different purpose in different\nmembers of an ensemble. Therefore, the variance of the values of these parameters after training\ncannot be compared to the variance that would have been obtained in the ﬁrst kernel of the ﬁrst layer\nof a trained BNN with the same architecture.\nIn this paper, we propose Deep Probabilistic Ensembles (DPEs), a novel approach to approximate\nBNNs using ensembles. Speciﬁcally, we use variational inference [2], a popular technique for train-\ning BNNs, to derive a KL divergence regularization term for ensembles. DPEs are parallelizable,\neasy to implement, yield high performance, and in our experiments, provide better uncertainty esti-\nmates than existing methods when used for active learning on visual classiﬁcation benchmarks.\nThird workshop on Bayesian Deep Learning (NeurIPS 2018), Montréal, Canada.\n2\nDeep Probabilistic Ensembles\nFor inference in a BNN, we can consider the weights w to be latent variables with some prior\ndistribution, p(w). These weights relate to the observed dataset x through the likelihood, p(x|w).\nWe aim to compute the posterior p(w|x) that best explains the observed data. Variational inference\ninvolves restricting ourselves to a family of distributions D over the latent variables, and optimizing\nfor the member of this family q∗(w) that is closest to the true posterior in terms of KL divergence,\nq∗(w) = arg min\nq(w)∈D\nKL(q(w)||p(w|x)).\n(1)\nSimplifying this objective, we obtain the negative Evidence Lower Bound (ELBO) [2],\n−ELBO = KL(q(w)||p(w)) −E[log p(x|w)],\n(2)\nwhere the ﬁrst term is the KL divergence between q(w) and the chosen prior distribution p(w); and\nthe second term is the expected negative log likelihood (NLL) of the data x based on the current pa-\nrameters w. The optimization difﬁculty in variational inference arises partly due to this expectation\nof the NLL. In deterministic networks, fragile co-adaptations exist between different parameters,\nwhich can be crucial to their performance [17]. Features typically interact with each other in a\ncomplex way, such that the optimal setting for certain parameters is highly dependent on speciﬁc\nconﬁgurations of the other parameters in the network. Co-adaptation makes training easier, but can\nreduce the ability of deterministic networks to generalize [9]. Popular regularization techniques to\nimprove generalization, such as Dropout, can be seen as a trade-off between co-adaptation and train-\ning difﬁculty [16]. An ensemble of deterministic networks exploits co-adaptation, as each network\nin the ensemble is optimized independently, making them easy to optimize.\nFor BNNs, the nature of the objective, an expectation over all possible assignments of q(w), prevents\nthe BNN from exploiting co-adaptations during training, since we seek to minimize the NLL for\nany generic deterministic network sampled from the BNN. While in theory, this should lead to great\ngeneralization, it becomes very difﬁcult to tune BNNs to produce competitive results. In this paper,\nwe propose a form of regularization to use the optimization simplicity of ensembles for training\nBNNs.\nKL regularization. The standard approach to training neural networks involves regularizing each\nindividual parameter with L1 or L2 penalty terms. We instead apply the KL divergence term in Eq.\n2 as a regularization penalty Ω, to the set of values of a given parameter takes over all members\nin an ensemble. If we choose the family of Gaussian functions for p(w) and q(w), this term can\nbe analytically computed by assuming mutual independence between the network parameters and\nfactoring the term into individual Gaussians. The KL divergence between two Gaussians with means\nµq and µp, standard deviations σq and σp is given by\nKL(q||p) = 1\n2\n \nlog σ2\nq\nσ2p\n+ σ2\np + (µq −µp)2\nσ2q\n−1\n!\n.\n(3)\nChoice of prior. In our experiments, we choose the network initialization technique proposed by\nHe et al. [7] as a prior. For batch normalization parameters, we ﬁx σ2\np = 0.01, and set µp = 1\nfor the weights and µp = 0 for the biases. For the weights in convolutional layers with the ReLU\nactivation (with ni input channels, no output channels, and kernel dimensions w × h) we set µp = 0\nand σ2\np =\n2\nnowh. Linear layers can be considered a special case of convolutions, where the kernel\nhas the same dimensions as the input activation. The KL regularization of a layer Ωl is obtained by\nremoving the terms independent of q and substituting for µp and σp in Eq. 3,\nΩl =\nninowh\nX\ni=1\n\u0012\nlog σ2\ni +\n2\nnowhσ2\ni\n+ µ2\ni\nσ2\ni\n\u0013\n,\n(4)\nwhere µi and σi are the mean and standard deviation of the set of values taken by a parameter across\ndifferent ensemble members. In Eq. 4, the ﬁrst term prevents extremely large variances compared\nto the prior, so the ensemble members do not diverge completely from each other. The second\nterm heavily penalizes variances less than the prior, promoting diversity between members. The\nthird term closely resembles weight decay, keeping the mean of the weights close that of the prior,\nespecially when their variance is also low.\n2\nTable 1: Validation accuracies (in %) on an active learning task. DPEs give consistent improvements\nin results over both random sampling and standard ensembles on both datasets. Relative performance\nto the upper bound performance for that data sampling strategy is given in (brackets).\nTask\nData Sampling\nAccuracy @8%\nAccuracy @16%\nAccuracy @32%\nRandom\n80.60 (84.66)\n86.80 (91.18)\n91.08 (95.67)\nCIFAR-10\nEnsemble\n82.41 (86.56)\n90.05 (94.59)\n94.13 (98.87)\nDPE (Ours)\n82.88 (87.06)\n90.15 (94.70)\n94.33 (99.09)\nRandom\n39.57 (50.18)\n54.92 (69.64)\n66.65 (84.51)\nCIFAR-100\nEnsemble\n40.49 (51.34)\n56.89 (72.14)\n69.68 (88.36)\nDPE (Ours)\n40.87 (51.83)\n56.94 (72.20)\n70.12 (88.92)\nObjective function. We can now rewrite the minimization objective from Eq. 2 for an ensemble as:\nΘ∗= arg min\nΘ\nN\nX\ni=1\nE\nX\ne=1\nH(yi, Me(xi, Θe)) + βΩ(Θ),\n(5)\nwhere {(xi, yi)}N\ni=1 is the training data, E is the number of models in the ensemble, and Θe refers\nto the parameters of the model Me. H is the cross-entropy loss for classiﬁcation and Ωis our KL\nregularization penalty over all the parameters of the ensemble Θ. We obtain Ωby aggregating the\npenalty from Eq. 4 over all the layers in a network, and use a scaling term β to balance the regularizer\nwith the likelihood loss. By summing the loss over each independent model, we are approximating\nthe expectation of the ELBO’s NLL term in Eq. 2 over only the current ensemble conﬁguration, a\nsubset of all possible assignments of q(w). This is the main distinction between our approach and\ntraditional variational inference.\n3\nExperiments\nActive learning allows a model to choose the data from which it learns, by iteratively using the\npartially trained model to decide which examples to annotate from a large unlabeled dataset [4].\nIn our approach, the data with the highest entropy for the average prediction across the ensemble\nmembers is added to the training set from the unlabeled pool. We experiment with active learning\non the CIFAR dataset, which has two object classiﬁcation tasks over natural images: one coarse-\ngrained over 10 classes and one ﬁne-grained over 100 classes [11]. There are 50k training images\nand 10k validation images of resolution 32 × 32.\nWe use eight models in our ensembles, each a pre-activation ResNet-18 [8]. Our ﬁrst experiment\ninvolves initializing models by training with a random subset of 4% of the dataset, and then re-\ntraining the model at three additional intervals after adding more data (at 8%, 16% and 32% of\nthe data). We evaluate three approaches for adding data to the training set, (1) Random: pick the\nrequired percentage of data through random sampling; (2) Ensemble: pick the samples with highest\naverage entropy across our eight models with standard L2 regularization for each model; and (3)\nDeep Probabilistic Ensemble (DPE): pick samples with the highest average entropy across our eight\nmodels jointly regularized with our KL regularization. Our results are shown in Table 1. Our\nﬁgures correspond to the mean accuracy of 3 experimental trials. As shown, active learning with\nDPEs clearly outperforms random baselines, and consistently provides better results compared to\nensemble based active learning methods. Further, though the gap is small, it typically grows as the\nannotation budget (size of the labeled dataset) increases, which is a particularly appealing property\nfor large-scale labeling projects.\nIn our second experiment, we compare our approach to state-of-the-art results on the CIFAR-10\ndataset using 20% of the labeled training data. We include results for (1) the proposed DPE; (2)\na deterministic network using the same architecture and L2 regularization; (3) Core-set selection\nwith a single VGG-16 [15] and; (4) an ensemble of DenseNet-121 models [1]. We also include the\nupper bound of each experiment. That is, the accuracy obtained when the model is trained using\nall the data available. For the ﬁrst two methods, we start training at 4% of the data and then re-\ntrain the model at four additional intervals (at 8%, 12%, 16% and 20%). For the others we use the\nperformance reported in the corresponding papers. As shown in Table 2, our approach outperforms\nall the others, not only achieving higher accuracy with limited training data, but also reducing the\ngap to the corresponding upper bound.\n3\nTable 2: CIFAR-10: Comparison of our proposed approach to active learning baselines for learning\nwith limited labels. DPEs show state-of-the-art performance. Relative performance refers to the\nperformance for each method using a limited labeling budget compared to the performance using\nthe whole dataset.\nMethod\nAccuracy @20%\nAccuracy @100%\nRelative Performance\nCore-set [15]\n74%\n90%\n82.2%\nEnsemble [1]\n85%\n95.5%\n89.0%\nDeterministic Network\n87.5%\n94.4%\n92.7%\nDPE (Ours)\n92%\n95.2%\n96.3%\n4\nConclusion\nIn this paper, we introduced Deep Probabilistic Ensembles (DPEs) for uncertainty estimation in deep\nneural networks. The key idea is to train ensembles with a novel KL regularization term as a means\nto approximate variational inference for BNNs. Our results demonstrate that DPEs improve perfor-\nmance on active learning tasks over baselines and state-of-the-art active learning techniques on two\nimage classiﬁcation datasets. Importantly, contrary to traditional Bayesian methods, our approach is\nsimple to integrate in existing frameworks and scales to large models and datasets without impacting\nperformance. We look forward to future work with these models for any downstream tasks requiring\nprecise uncertainty information.\nReferences\n[1] William H. Beluch, Tim Genewein, Andreas Nürnberger, and Jan M. Köhler. The power of ensembles for\nactive learning in image classiﬁcation. In CVPR, 2018.\n[2] D. M. Blei, A. Kucukelbir, and J. D. McAuliffe. Variational Inference: A Review for Statisticians. ArXiv\ne-prints, 2016.\n[3] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural\nnetworks. In ICML, 2015.\n[4] David Cohn, Les Atlas, and Richard Ladner. Improving generalization with active learning. Machine\nLearning, 1994.\n[5] Y. Gal and Z. Ghahramani. Bayesian Convolutional Neural Networks with Bernoulli Approximate Varia-\ntional Inference. ArXiv e-prints, 2015.\n[6] Alex Graves. Practical variational inference for neural networks. In NIPS. 2011.\n[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing\nhuman-level performance on imagenet classiﬁcation. In ICCV, 2015.\n[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn CVPR, 2016.\n[9] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Improving neural\nnetworks by preventing co-adaptation of feature detectors. ArXiv e-prints, 2012.\n[10] Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer\nvision? In NIPS, 2017.\n[11] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\n[12] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncer-\ntainty estimation using deep ensembles. In NIPS, 2017.\n[13] Radford M. Neal. Bayesian learning for neural networks. PhD thesis, University of Toronto, 1995.\n[14] Ian Osband. Risk versus uncertainty in deep learning: Bayes, bootstrap and the dangers of dropout. In\nNIPS Workshops, 2016.\n[15] Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach.\nICLR, 2018.\n[16] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout:\nA simple way to prevent neural networks from overﬁtting. ICML, 2014.\n[17] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural\nnetworks? In NIPS, 2014.\n4\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2018-11-06",
  "updated": "2018-11-30"
}