{
  "id": "http://arxiv.org/abs/1811.03273v1",
  "title": "Information Flow in Pregroup Models of Natural Language",
  "authors": [
    "Peter M. Hines"
  ],
  "abstract": "This paper is about pregroup models of natural languages, and how they relate\nto the explicitly categorical use of pregroups in Compositional Distributional\nSemantics and Natural Language Processing. These categorical interpretations\nmake certain assumptions about the nature of natural languages that, when\nstated formally, may be seen to impose strong restrictions on pregroup grammars\nfor natural languages.\n  We formalize this as a hypothesis about the form that pregroup models of\nnatural languages must take, and demonstrate by an artificial language example\nthat these restrictions are not imposed by the pregroup axioms themselves. We\ncompare and contrast the artificial language examples with natural languages\n(using Welsh, a language where the 'noun' type cannot be taken as primitive, as\nan illustrative example).\n  The hypothesis is simply that there must exist a causal connection, or\ninformation flow, between the words of a sentence in a language whose purpose\nis to communicate information. This is not necessarily the case with formal\nlanguages that are simply generated by a series of 'meaning-free' rules. This\nimposes restrictions on the types of pregroup grammars that we expect to find\nin natural languages; we formalize this in algebraic, categorical, and\ngraphical terms.\n  We take some preliminary steps in providing conditions that ensure pregroup\nmodels satisfy these conjectured properties, and discuss the more general forms\nthis hypothesis may take.",
  "text": "B. Coecke, J. Hedges, D. Kartsaklis, M. Lewis, D. Marsden (Eds.):\n2018 Workshop on Compositional Approaches\nfor Physics, NLP, and Social Sciences (CAPNS)\nEPTCS 283, 2018, pp. 13–27, doi:10.4204/EPTCS.283.2\nInformation Flow in Pregroup Models of Natural Language\nPeter M. Hines\nYCCSA, University of York\npeter.hines@york.ac.uk\nThis paper is about pregroup models of natural languages, and how they relate to the explicitly\ncategorical use of pregroups in Compositional Distributional Semantics and Natural Language Pro-\ncessing. These categorical interpretations make certain assumptions about the nature of natural\nlanguages that, when stated formally, may be seen to impose strong restrictions on pregroup gram-\nmars for natural languages.\nWe formalize this as a hypothesis about the form that pregroup models of natural languages must\ntake, and demonstrate by an artiﬁcial language example that these restrictions are not imposed by\nthe pregroup axioms themselves. We compare and contrast the artiﬁcial language examples with\nnatural languages (using Welsh, a language where the ‘noun’ type cannot be taken as primitive, as\nan illustrative example).\nThe hypothesis is simply that there must exist a causal connection, or information ﬂow, between\nthe words of a sentence in a language whose purpose is to communicate information. This is not\nnecessarily the case with formal languages that are simply generated by a series of ‘meaning-free’\nrules. This imposes restrictions on the types of pregroup grammars that we expect to ﬁnd in natural\nlanguages; we formalize this in algebraic, categorical, and graphical terms.\nWe take some preliminary steps in providing conditions that ensure pregroup models satisfy these\nconjectured properties, and discuss the more general forms this hypothesis may take.\n1\nIntroduction\nLambek pregroups are algebraic structures with a strongly categorical / logical ﬂavor, proposed for mod-\neling linguistic phenomena [17]. More recently they have been used heavily within Natural Language\nProcessing – in particular, the ﬁeld of Compositional Distributional Semantics [5]. In language process-\ning, their utility is heavily based on a strongly categorical interpretation, and we will freely mix algebraic\nand categorical descriptions throughout this paper – hopefully pointing out how the two are related.\nA particularly puzzling feature of pregroups is that they seem to be ‘overspeciﬁed’; they have dual\nnotions of expansion and contraction (see Section 2.1), and a corresponding neat graphical calculus\nof underscores and overscores (Section 2.1). However, for linguistic purposes, only the expansions /\nunderscores are relevant. From Lambek’s original work onwards, this has been something of a mystery;\n[17] proposes that although the ‘extra structure’ has no linguistic interpretation, it nevertheless helps\ndetermine the algebraic structures that are in fact useful.\nAn alternative viewpoint comes from categorical perspectives on Natural Language Processing such\nas [5, 8]. Although this ﬁeld works with a degenerate notion of pregroups (i.e. compact closed categories\n– see Section 3.1), both ‘underscores’ and ‘overscores’ of graphical models are crucial in describing\nﬂow of information or causal connections in the sense of [16]. In the linguistic setting, this is a direct\nclaim that they model information ﬂow or interaction between distinct parts of (grammatically correct)\nsentences. We refer to this as the categorical hypothesis (see Section 6).\nBy contrast with linguistic models generally, Compositional Distributional Semantics uses a very\ndegenerate form of pregroups. However, the categorical hypothesis is equally applicable to arbitrary\n14\nInformation Flow in Natural Languages\npregroup models of natural language. The purpose of this paper is to show that it is not simply a useful\nheuristic or worldview, but also makes concrete claims about the structure of (pregroup models of) natural\nlanguages.\nWe ﬁrst give the basic formalism, including the construction of free pregroups (the particular form\nused in linguistic models), give a categorical interpretation of this, and relate it to the highly graphical\nproperties of pregroup representations. This is followed by an illustrative example of modeling natural\nlanguage grammar using pregroups, together with a formal categorical description of this process. We\nthen use this formal description to consider some concrete consequences of the categorical hypothesis.\nWe then give a formal language example of a pregroup grammar that does not conform to the pre-\ndictions of the categorical hypothesis, and explain why we would not expect to ﬁnd such behavior in a\npregroup model of natural language. We conclude by considering potential counterexamples, possible\nextensions, and take the ﬁrst steps towards an algebraic axiomatisation of grammars with the predicted\nbehavior.\n2\nBasic deﬁnitions & properties\nIn [17], pregroups are deﬁned as follows:\nDeﬁnition 1. A (Lambek) pregroup is a monoid P equipped with a partial order ≤compatible with\ncomposition (so p ≤q and r ≤s implies pr ≤qs), and two operations ( )l,( )r : P →P called the\nleft- and right- adjoints respectively. These are related by the deﬁning identities, pl p ≤1 ≤ppl and\nppr ≤1 ≤prp.\nFollowing [4], we say that a pregroup P is proper if some left- and right- adjoints are distinct – i.e.\nthere exists some a ∈P satisfying ar ̸= al. Extending this, we say that P is fully proper if all non-unit\nleft- and right- adjoints are distinct – i.e. ar ̸= al for all a ̸= 1 ∈P.\nRemark 2. Free pregroups are, of course, fully proper. However, there are few concrete models of proper\npregroups. The vector space models used in Compositional Distributional Semantics not only identify\nleft- and right- adjoints, but also identify elements with their own adjoints.\nThe following results on pregroups are standard; see [17]\nLemma 3. Let (P,≤,( )l,( )r) be a pregroup. Then, for all p,q ∈P, the adjoints are contravariant,\nand order-inverting, so (pq)l = ql pl and p ≤q ⇔ql ≤pl, and similarly for the right adjoint. The\nidentity is its own left and right adjoint, so 1r = 1 = 1l and the two adjoints are mutually inverse, so\n(pr)l = p = (pl)r.\n2.1\nContractions & expansions\nThe following deﬁnitions provide useful notational tools for calculations within pregroups:\nDeﬁnition 4. Given a pregroup (P,≤,( )l,( )r), contractions are inequalities of the form uaarv ≤uv\nor ualav ≤uv, and expansions are inequalities of the form uv ≤uarav or uv ≤uaalv. Graphically,\ncontractions (resp. expansions) are indicated by underscores (resp. overscores). These may be nested\nand combined, to give a concise notation for expressing inequalities; the decorated word\nnr n nl n nl n\nnr n nl n nl n\nexpresses, via the contractions, the inequality nrnnlnnln ≤nrnnln ≤nrn and via the expansions, the\ninequality 1 ≤nrn ≤nrnnln ≤nrnnlnnln.\nP. Hines\n15\nRemark 5 (Normal forms for expansions and contractions). As demonstrated in [17], any inequality in\na pregroup may be derived by ﬁrst considering contractions, and then expansions. This gives a normal\nform for patterns of underscores & overscores. Logically / categorically, this may be seen as a cut-\nelimination or coherence result.\nWe will rely on normal forms throughout, but ﬁrst ﬁrst consider the categorical status of pregroups\nand the free pregroup construction.\n3\nFree pregroups and quasi-pregroups\nLinguistic applications commonly use free pregroups. These are deﬁned in [4] in terms of free quasi-\npregroups, as follows:\nDeﬁnition 6. A quasi-pregroup (Q,≺,( )l,( )r) is a weakening of Deﬁnition 1 to the case where ≺is a\ncompatible preorder rather than a partial order on the monoid Q. The free quasi-pregroup FQPG on a\nposet (G,≤G) is deﬁned as follows:\n• The underlying monoid is the free monoid (G×Z)∗.\n• The left- and right- adjoints are deﬁned by inductively by\n– 1l = 1 = 1r\n– (g,z)r = (g,z+1) and (g,z)l = (g,z−1), for all (g,z) ∈G×Z,\n– (uv)r = (vr)(ur) and (uv)l = (vl)(ul), for all u,v ∈(G×Z)∗.\n• The preorder ≺is the smallest preorder containing the inductively deﬁned relation R, given by\n– g ≤g′ ∈G implies (g,z)R(g′,z) for even z ∈Z, and (g′,z)R(g,z) for odd z ∈Z.\n– (g,z)(g,z+1) R 1 R (g,z+1)(g,z) for all (g,z) ∈G×Z,\n– a R b ⇒uav R ubv, for all a,b,u,v ∈(G×Z)∗.\nBy basic algebra, ≺= S∞\nj=0 R j is the reﬂexive transitive closure, or Kleene star, of R.\nThe free quasi-pregroup on a set H is given by assuming the discrete partial ordering (i.e. equality).\nRemark 7. The preorder ≺in the above deﬁnition is not a partial order; (g,0)(g,1)(g,0) ≺(g,0) and\n(g,0)(g,1)(g,0) ≺(g,0), but (g,0) ̸= (g,0)(g,1)(g,0).\nFree pregroups then arise as quotients of free quasi-pregroups by the induced equivalence relation.\nDeﬁnition 8. Let (P,≺) be a preordered set. The induced equivalence relation ∼≺is deﬁned by p ∼≺\nq ⇔p ≺q and q ≺p, and the quotient P/∼≺is a poset w.r.t. the induced partial order ≺/ ∼≺.\nProposition 9. Let Q,≺,( )l,( )r) be a quasi-pregroup. Then the induced equivalence relation is a\ncongruence on the quasi-pregroup structure.\nProof. This is a key feature of the construction of free pregroups from free quasi-pregroups found in [4],\nwhere it is shown that, ax∼≺a′x′, (a)l∼≺(a′)l, and (a)r∼≺(a′)r, for all a ∼≺a′,x ∼≺x′ ∈Q.\nDeﬁnition 10. The free pregroup on a partially ordered set (G,≤), denoted PG, is deﬁned in [4] to be\nthe quotient of FQPG by the induced equivalence relation ∼≺.\nThe construction of free pregroups from free quasi-pregroups may of course be viewed categorically.\n16\nInformation Flow in Natural Languages\n3.1\nPregroups and quasi-pregroups, categorically\nIt is natural to treat pregroups and quasi-pregroups as categories; these are of a special form.\nDeﬁnition 11. A category is called posetal if each hom-set has at most one element. We deﬁne POSETAL\nto be the category whose objects are posetal categories and whose arrows are functors, and POSET to be\nthe full subcategory whose objects are posets, considered as small categories.\nInterpretation of pregroups as posetal categories is well-established [17, 6]. Any preordered set may\nbe treated as a category with arrows given by the preorder. Monoid composition is then a monoidal\ntensor, with the interchange law corresponding to compatibility, and the identity being a strict unit. The\nadjoints are contravariant monoidal endofunctors, and the deﬁning identities become the axioms for a\nnon-commutative form of compact closure.\nCompact closure was originally deﬁned in terms of abstract 2-categorical properties. In [15], the\nabstract 2-categorical deﬁnition is shown to have a neat characterization in terms of the existence of a\nduality, and distinguished arrows,giving both a coherence theorem and diagrammatic calculus. We take\nthe following deﬁnition, from [15] as fundamental:\nDeﬁnition 12. A compact closed category (CCC) is a symmetric monoidal category with a dual\n(C ,⊗,σ,( )∗,I), equipped with unit & co-unit arrows ηA : I →A ⊗A∗and εA : A∗⊗A →I at every\nobject, A ∈Ob(C ). These satisfy the yanking axiom (1A ⊗ε)(η ⊗1A) = 1A = (εA∗⊗1A)(1A ⊗ηA∗).\nAs is well-established [17], the correct setting for pregroups is the non-commutative form of the\nabove:\nDeﬁnition 13. A (non-symmetric) compact closed category (NSCCC) is a monoidal category with left-\nand right- duals ( )l,( )r, and left- and right- unit and co-unit arrows η(l)\nA ,η(r)\nA\nand ε(l)\nA ,ε(r)\nA\nsatisfying\nthe obvious non-symmetric analogues of the yanking axiom.\nNote that the above deﬁnitions give CCCs as degenerate cases of NSCCCs. A great deal of literature\nexists on graphical interpretations and calculii for both symmetric & non-symmetric compact closure\n(e.g. [13, 14, 19]) so we do not reproduce it here.\nRemark 14. It is well-established that pregroups are posetal non-symmetric compact closed categories\n[17, 5]. Identical reasoning demonstrates that quasi-pregroups are also posetal non-symmetric compact\nclosed categories.\nNotation 15 (Duals in category theory, the free monoid functor, and the Kleene star). There is an un-\nfortunate clash of notation between the category theorists dual ( )∗, the algebraists Kleene star ( )∗of\nrelations, and the free monoid construction ( )∗on sets (which we will treat as a functor).\nHopefully the intended meaning of the overloaded ‘star’ notation will be clear from the context, as\nthis paper works with fully proper pregroups, and hence distinguishes left- and right- duals.\nDeﬁnition 16. We deﬁne NSCCC to be the category whose objects are non-symmetric compact closed cat-\negories and whose arrows are adjoint-preserving monoidal functors, and CCC to be the full subcategory\nof compact closed categories.\nEssentially by deﬁnition, free constructions are functorial, and pregroups / quasi-pregroups are no\nexception – this follows from the universal property for freeness. We make the following deﬁnitions:\nDeﬁnition 17. We deﬁne FQP : POSET →NSCCC to be the functor that takes a poset to the free quasi-\npregroup on that poset. Similarly, FP : POSET →NSCCC is the functor that takes a poset to the free\npregroup on that poset.\nP. Hines\n17\nProposition 18. There exists a natural transformation from FQP to FP whose components are given by\nthe ‘quotienting by the induced equivalence relation’ step of Deﬁnition 10.\nProof. Categorically, Proposition 9 states that for a given poset G, quotienting by the induced equiv-\nalence relation gives a functor of non-symmetric compact closed categories, and hence an arrow in\nNSCCC(FQP(G),FP(G)). We then have a family of arrows indexed by the objects of POSET, and we\nagain appeal to the universal properties implied by freeness to demonstrate that these are indeed the\ncomponents of a natural transformation.\n4\nGraphical properties of pregroups\nMany of the categorical properties of pregroups are best illustrated graphically, using the conventions\nof overscores and underscores described above. Free pregroups (but not pregroups generally) then sat-\nisfy three core properties of undirectedness, planarity and acyclicity. By construction, the underscores\nand overscores are undirected. The lack of commutativity in the deﬁnition corresponds to planarity in\nan obvious way; in the free setting two distinct underscores (resp. overscores) may not overlap. The\nﬁnal property, acyclicity, means that in proper pregroups it is not possible to form closed loops using\nunder- / over- scores. Although intuitively obvious, we provide a proof below to demonstrate how this is\nclosely related to the construction of free pregroups given in Section 3, and thus how the free pregroup\nconstruction relates to the Yanking axiom from the categorical description.\nTheorem 19. Let w ∈P be an arbitrary non-empty word in a fully proper pregroup. Then no pattern of\nexpansions / contractions on w can contain a closed cycle.\nProof. We show that the only word satisfying 1 ≤w ≤1 is the empty word (i.e. the identity); the general\nresult follows by induction.\nAssume some word w ̸= 1 ∈P satisfying 1 ≤w ≤1. Then there exists a set of expansions demon-\nstrating w ≤1 and a set of contractions demonstrating 1 ≤w. Thus every symbol in w is part of both a\ncontraction and an expansion. We drawing these as under- / over- scores, and connect the ends of these\nwith their corresponding symbols. This gives a set of (possibly nested) Jordan curves in the plane:\nFigure 1: Every symbol is part of an expansion & a contraction\nThe symbols of w lie on the intersection of the curve, and a bisecting line – note that each symbol\nuniquely determines, and is uniquely determined by, its neighbours on the Jordan curve. Now choose\nsome symbol x of the word w together with a direction to traverse the curve on which it lies. Trivially,\neach symbol encountered is some adjoint of x; which one is uniquely determined by the direction of\n18\nInformation Flow in Natural Languages\nmovement and whether the current arc lies above or below the bisecting line:\nxr\nx\n+1\n\u0002\nx\n−1\n\u001e\nxl\nxl\nx\n−1\n[\nx,\n+1\nCxr\nWe label these directed arcs by ‘weights’ as shown, and take the sum around this closed loop.\nConsider a sub-arc of this Jordan curve that starts & ﬁnishes with a contraction, on which the direc-\ntion of movement (with respect to the bisecting line) does not change, as shown below:\nNo matter the length of this arc, the sum of weights along it is always 1 when traversed from left to right,\nand −1 otherwise. We therefore replace each such arc by a single arc beneath the bisecting line, labeled\nby either 1, or −1, depending on the direction of traversal, as follows:\nCategory theorists will, of course, notice the implicit appeal to the Yanking axioms!\nWe do a similar replacement (with a similar interpretation) to repeated unidirectional subarcs that\nstart / ﬁnish with an expansion, leaving a closed loop where the direction of movement ( with respect to\nthe bisecting line) changes every time this line is crossed.\nOn the original Jordan curve, a contraction can never be followed by another contraction, nor can an\nexpansion be followed by another expansion, and the same holds in our simpliﬁed diagram. Thus, the\ntotal number of contractions in the Jordan curve is equal to the total number of expansions. Therefore, as\nwe change direction at each crossing of the bisecting line, the sum the weights around the entire closed\nloop is either 2 or −2. This implies the existence of some element whose left and right adjoints are\nidentical, contradicting the assumption that P is a fully proper pregroup.\nCorollary 20. Let w be a word of length ≥2 in a free pregroup. Then every pattern of expansions /\ncontractions in will leave at least two symbols that are either not part of a contraction, or of an expansion.\nCorollary 21. Let Q be a (not necessarily free) pregroup. Then acyclicity fails precisely when ql = qr\nfor some q ̸= 1 ∈Q.\nP. Hines\n19\n5\nGrammatical interpretations\nGrammatical interpretations are traditionally carried out using free pregroups on sets or posets1. For\npurely linguistic applications, grammatical types are modeled by words in a free pregroup, and the partial\nordering is interpreted as an information ordering; x ≤y expresses that ‘x is a special case of y’. We are of\ncourse interested in pregroup words that are a special case of the ‘sentence’ type, and seek to demonstrate\nthis by a suitable pattern of reductions / underscores. This is best illustrated by a concrete example:\n5.1\nA worked example\nAs a concrete example, we demonstrate how pregroups may be used to describe grammatical structure\nin a language where the noun phrase type is not primitive, due to mutations of nouns following either\npossessives or prepositions. We give a pregroup analysis of the grammaticality of the modern Welsh\nsentence: “Dyma fy nghath i” (See Figure 2 for an explanation of the grammatical constituents).\nOur generating poset is {n,s,dpt,c1,np : np ≤n,dpt ≤s}, with the following interpretation:\ns\nsentence\ndpt\ndeclarative present tense sentence\nn\nnoun phrase\nnp\nnoun (1st person possessive form)\nc1\n1st person conﬁrming pronoun\nFigure 2: An example Welsh sentence\ndeclarative p.t. sentence\ndpt ≤s\nDyma\n“here is”\ndptnl\nfy\n“my”\nncl\n1nl\np\nnghath\n“cat” [pos.]\nnp ≤n\ni\n[conﬁrming pronoun]\nc1\n(ﬁrst person)\nThe pattern of contractions\ndptnl ncl\n1nl\np np c1\nthen demonstrates that dptnlncl\n1nl\npnpc1 ≤dpt ≤s, and so the given sentence is a special case of a declar-\native present tense sentence, which is itself a special case of a sentence.\n1There appears to be little discussion in the literature of the relative merits of using free vs. non-free pregroups within\nlinguistics. We therefore observe that the constructions and results of this paper are equally applicable in the non-free case, but\nuse free pregroups for our concrete examples.\n20\nInformation Flow in Natural Languages\n5.2\nFormalizing grammaticality, and the ‘language bracketing’\nThe informal description of Section 5 may be recast in monoid theoretic and categorical terms.\nDeﬁnition 22. Assume some set T of grammatical types that contains a distinguished sentence type,\nsuch as\nT = {SENTENCE,TRANSITIVE VERB,CONFIRMING PRONOUN,...}\nA pregroup model is simply a function µ : T →P that assigns elements of a pregroup P to each gram-\nmatical type. When the pregroup is the free pregroup on some generating poset G (as is standard for\nnatural language models), the interpretation of the sentence type is presumed to be a generator, com-\nmonly denoted s ∈G. (We are unaware of any linguistic argument for or against the assumption that the\nsentence type must be modeled by a generator).\nWe formalize this categorically, based on the free monoid functor ( )∗: Set →Mon from the category\nof sets to the category of monoids, and the well-known monadicity of the free monoid / underlying set\npairing.\nDeﬁnition 23. Given arbitrary M ∈Ob(Mon), we denote the ﬂattening associated with the above monad\nby ( )♭\nM ∈Mon(M∗,M). A pregroup model µ : T →P extends to a monoid homomorphism by the free\nmonoid functor, giving µ∗∈Mon(T ∗,P∗). We refer to this as the language bracketing for reasons we\nexplain below.\nIt is then usual to work with the composite of the language bracketing and the ﬂattening functor,\ngiving the pregroup interpretation µ◦: T ∗→P, by the following diagram in Mon\nT ∗\nµ∗\n/\nµ◦\n!\nP∗\n( )♭\n\u000f\nP\nRemembering the pregroup structure on P, a word of grammatical types w ∈T ∗is then a grammatically\ncorrect sentence iff µ◦(w) ≤µ◦(SENTENCE).\n(We will, as is traditional, abuse terminology and talk about the pregroup interpretation of the sen-\ntence within a language, as well as the pregroup interpretation of a formal string of grammatical types).\nAt least in the free setting, if not generally, the language bracketing may be thought of as taking the\npregroup interpretation of a sentence, and bracketing it (implicitly, giving a word in the free monoid over\nthe relevant pregroup). For the interpretation of the sentence, “Dyma fy ngath i”, we have the following:\n( dptnl ) (ncl\n1nl\np) ( np ) (c1 )\nThis is an element of the free monoid (PG)∗, rather than the free pregroup PG. However, applying the\nﬂattening homomorphism will result in a word of PG that is beneath the sentence type, as required.\n6\nContractions, expansions, and the categorical hypothesis\nIn the demonstration of Section 2, only contractions are used to demonstrate that the given sentence is a\nspecial case of the sentence type. This is indeed a general principle [17]; expansions currently play no\nrole in grammatical applications of pregroups (as opposed to Natural Language Processing applications).\nP. Hines\n21\nFrom Lambek onwards [17], authors have sought to account for the fact that the formalism has both\nexpansions and contractions.\nThe justiﬁcation given by J. Lambek is that they nevertheless determine the structure of pregroups,\neven though they play no rˆole in grammatical applications. A related but stronger viewpoint is implicit\nor explicit in more recent categorically motivated approaches such as [5, 6, 8]. This claim is that:\nBoth underscores and overscores model the interaction, or ﬂow of informa-\ntion, between components of a (grammatically correct) sentence\nWe refer to this as the categorical hypothesis. It is motivated by previous applications of compact closed\ncategories such as logical models [1, 9], lambda calculus [3], Turing machines [9, 11], quantum protocols\n[2], and causal structures [16], where the interpretation of units and co-units as modeling information\nﬂow or causal connection is by now well-established.\nOur claim is that this hypothesis is not simply a convenient viewpoint, but makes concrete predictions\nabout pregroup models of natural languages (as opposed to the formal languages that may be expressed\nin pregroup terms), and thus about grammatical structures for natural language generally.\n6.1\nThe importance of information ﬂow\nImplicit in the categorical hypothesis is the claim that there is actual interaction, or information ﬂow\nbetween all the individual words in a sentence: words are brought together to form a sentence because\nthere is non-trivial interaction between them all. Although this may seem a triviality, it is not enforced\nby the pregroup axioms, and we may build example grammars where this is not the case.\nConsider a language with grammatical types T = {SENTENCE,FOO,BAR,DOG,DUCK}, together\nwith the free pregroup over the discretely ordered set G = {s,a,b,c}, and the pregroup model\nµ(SENTENCE) = s , µ(FOO) = sacl , mu(BAR) = car , µ(DOG) = arbl , µ(DUCK) = barr\nThe pregroup interpretation of FOO.BAR.DOG.DUCK is saclcararblbarr, so this identiﬁed as a gram-\nmatically correct sentence by the following pattern of contractions:\ns a cl c ar ar bl b arr\nWhen considering the overscores, we observe that there are in fact no expansions at all! Thus FOO.BAR\nand DOG.DUCK are connected by neither underscores nor overscores. Assuming the correctness of\nthe categorical hypothesis, we conclude that there is no causal interaction / information ﬂow between\nFOO.BAR and DOG.DUCK in any (grammatically correct) sentence with this typing. We therefore\nwish to rule it out as an appropriate typing for a meaningful sentence.\nRemark 24. The above objection to this typing as appropriate for a meaningful sentence in a natural\nlanguage is of course reminiscent of the usual undergraduate objection to the ‘implication’ of boolean\nlogic — that there is no causal connection between the antecedent and the consequent. Although the\nuse of logical connectives in natural language is not always a good match for their formal logical inter-\npretations, in Section 8 we consider a restricted fragment that rules out such possibilities, and consider\nconsequences of the categorical hypothesis within this restricted setting.\n7\nCausal connections in sentences and grammars\nWe wished to rule out the FOO.BAR.DOG.DUCK example of Section 6.1 due to a failure of ‘connect-\nedness’, which interprets under the categorical hypothesis as a lack of information ﬂow between the\nconstituents of the sentence. We formalize this as follows\n22\nInformation Flow in Natural Languages\nDeﬁnition 25. We assume a set of grammatical types T, a pregroup P, and a grammatical model µ :\nT →PG. Given a word w ∈T ∗, we deﬁne its causal graph Cw to be the following undirected graph:\nNodes These are elements of the language bracketing µ∗(w) ∈P∗\nG.\nEdges There is an edge between two nodes for each underscore / overscore connecting them, in the\nimage of the language bracketing under the under the ‘ﬂattening’ homomorphism.\nIf Cw is connected, we say that w is a causally connected word of T ∗. If this condition holds for all words\nof T ∗that reduce to a given grammatical type S ∈T, we say that the grammatical model µ : T →PG is\nS-connected.\nExample 26. By way of illustration, we contrast the causal graph for our modern Welsh sentence, “Dyma\nfy ngath i” with that of the formal example “FOO.BAR.DOG.DUCK”. As we are working within free\npregroups, we may simply superimpose the language bracketing with the underscores and overscores.\nFor the Welsh example, we derive:\n(dptnl) (ncl\n1nl\np) (np) (c1)\nDoing the same for the formal example, we derive the following two causal graphs:\nc1\nDyma fy nghath i\nsnl\nncl\n1nl\np\nnp\nsacl\nFOO BAR DOG DUCK\narbl\nbarr\ncar\nAlthough the pattern of underscores & overscores is almost identical for these two examples, the language brack-\neting differs signiﬁcantly. This leads to substantially different causal graphs; one of which we ﬁnd reasonable for\na natural language, and the other we do not.\n7.1\nA hypothesis on natural vs formal Languages\nHypothesis 27 (The Connectedness Hypothesis).\nWe conjecture, based on the categorical hypothesis, that pregroup models of natural lan-\nguages are SENTENCE-connected.\nRemark 28. The above hypothesis is not a conjecture in the mathematical / logical sense. It is not\namenable to a formal proof but may perhaps be disproved by a convincing counterexample. It shares\nsome common features with the notion of a scientiﬁc theory within K. Popper’s ‘critical rationalism’,\nwhere it is claimed that, “Every ‘good’ scientiﬁc theory is a prohibition: it forbids certain things to\nhappen. The more a theory forbids, the better it is” [18]. However, it is somewhat exceptional in that it\nP. Hines\n23\ndoes not seek to relate a man-made model (with predictive power) to natural phenomena; rather, it could\nbe seen as a scientiﬁc theory about models.\nThis of course, raises the question of what a ‘convincing counterexample’ would look like, and\nwhether a pregroup model of natural language that was not SENTENCE-connected would instead be\ntaken as evidence that the model, rather than the hypothesis, was incorrect.\nIt is also worthwhile to note the underlying assumptions. It is clearly derived from the categorical\nhypothesis, and so is underpinned by the assumption that there is indeed ‘information ﬂow’ between\nthe words in a natural language sentence. This in turn assumes that the purpose of natural language\ngrammar is to connect words in a meaningful way, and is not simply a game played with a possibly\narbitrary set of rules.\nShould the above hypothesis be disproved by a convincing counterexample, we would, of course,\nsimply move on to analysing differences between sentences which are, and are not, causally connected.\nHowever, it is more likely that degeneracies at the level of the models result in us seeing spurious causal\nconnectedness traces of information ﬂow that does not actually take place. If we take the FOO-BAR\nDOG-DUCK example, and identify al = a = ar, we see exactly this via the resulting pattern of over-\nscores. The linguists convention of using free (and hence fully proper) pregroup models should go some\nway towards eliminating this, but we will observe similar phenomena in ‘degenerate’ models, where\nwhat should be distinct grammatical types are mapped to the same pregroup words.\n8\nA simpliﬁed class of sentences\nWe now deﬁne a simpliﬁed fragment of the languages identiﬁed as grammatically correct by a pregroup\nmodel. The original motivation for this was applications to Compositional Distributional Semantics [5],\nwhich aims to combine both meaning and grammar into a single concrete setting. The intention was to\nrule out grammatical constructs that unavoidably lead to rather structurally complex models.\nDeﬁnition 29. Given a set G = A ∪{s}, we say that a word w ≤s ∈PG simply reduces to s iff it is of\nthe form usv where u,v ≤1 ∈PA ≤PG. We call the set of all such words the simply s-reducing words of\nPG. Given a set T of grammatical types, we say that a pregroup model µ : T →PG is simply reducing\niff all elements of µ◦(T ∗) that reduce to s also simply reduce to s.\nRemark 30 (The original motivation for simply-reducing words). In [12], a strong case is made that\n‘logical’ connectives in natural language (such as ‘and’, ‘or’, etc.) must be polymorphically typed (in a\nsimilar manner to [7]), & as a consequence of this any concrete models must be self-dual idempotents of\na compact closed category. This would of course imply that they are reﬂexive [10], and there is a direct\nlink from there to models of pure untyped lambda calculus [3]. In terms of computational tractability,\nthis is not where we wish to go!\nSuch structures are also not expressible within the vector space models commonly used in distribu-\ntional semantics; it is only the subcategory of ﬁnite-dimensional vector spaces that is compact closed,\nand this of course contains no non-trivial idempotent objects.\nThe intention of the above deﬁnition was to rule out such grammatical constructs that take as ‘input’\nan arbitrary sentence or family of sentences and then ‘output’ a larger grammatically correct sentence –\nthe motivation being that unrestricted use of such constructs leads in short order to irreducibly complex\nmodels2. Thus, as well as ruling out logical connectives, this also eliminates the possibility of modeling\n2A somewhat facetious example being the ‘Encyclopædia Britannica’ problem, where we take any particularly large text\nand replace every full stop by conjunction. This leads to the question of exactly how large and complex our concrete ‘sentence\ntype’ should be when we allow for naive unrestricted conjunctions.\n24\nInformation Flow in Natural Languages\nepistemic constructs such as “He knows that ...”, “I believe ...”, and indeed, meta-statements “It is not\nprovable that ...”.\nLogicians – including the author – may consider these restrictions to be taking all the fun out of\nmodels of meaning. However, they do lead to fragments of language that are amenable to vector space\nstyle models, and also provide a secure setting for formalizing some consequences of the categorical\nhypothesis. We hope to show that they also lead to some interesting non-trivial algebra.\n9\nP{n,s} — a toy example\nAlthough deciding whether an individual sentence is causally connected seems to be a simple task (Ex-\nample 26), deciding whether an entire pregroup model is causally connected appears to be a harder task,\neven in the simply reducing case.\nWe take some small steps in this direction, within a ‘toy example’ based on the free pregroup over the\nset {n,s}, equipped with the discrete partial order. This is commonly used as an illustration of pregroup\nmodels, with the two basic types correspond to ‘noun’ and ‘sentence’ respectively.\nLet us assume a set of grammatical types\nT = {SENTENCE , transVERB , intVERB , NOUN , attADJ , NONCE}\n(Types Example)\nThese all, excluding the type NONCE, have the obvious intended meaning, so we may go some way\ntowards constructing a simply reducing pregroup model µ : T →P{n,s} by deﬁning\nµ(transVERB) = nrsnl , µ(intVERB) = nrs , µ(attADJ) = nnr\n(Partial Model)\nWe now demonstrate how to give a pregroup typing to NONCE ∈T that leads to a simply reducing\npregroup model µ : T →P{n,s} that is not SENTENCE-connected. We thus exhibit a class of pregroup\nmodels that, by Hypothesis 27, we do not expect to model any real-world natural language.\nDeﬁnition 31. In a pregroup P, we deﬁne the down-closure of the identity [1P]↓to be the submonoid\nof words that are below the identity. This contains the identity and closure under composition is ensured\nby compatibility of composition and partial order (categorically, the interchange law for a monoidal\ntensor).\nRemark 32. From a linguistic point of view, we should be suspicious of any pregroup model where\nwhere the interpretation of some type or string of types falls within this monoid; this would correspond\nto a series of words that could be arbitrarily added to either side of any grammatically correct sentence\nto give another grammatically correct sentence.\nTheorem 33. Let T and µ : T −{NONCE} →P{n,s} be as deﬁned in Types Example and Partial Model\nrespectively. Let us also denote the sub-pregroup of P{n,s} generated by {n} ⊆{n,s} by P{n} ≤P{n,s}.\nThen for arbitrary w ∈\n\u0002\nP{n}\n\u0003\n↓, completing µ to a globally deﬁned function by taking µ(NONCE) = w\nwill give a pregroup typing that is:\n1. simply reducing, and\n2. not SENTENCE-connected.\nProof. It is almost immediate that µ is indeed simply reducing, as µ(NONCE) contains no occurrences\nof s ∈P{n,s}.\nP. Hines\n25\nBy compatibility of partial order and composition, µ◦(NOUN.intVERB.NONCE) ≤s.1 = s so any\nsentence of this type is also a grammatical sentence. However, µ◦(NOUN.intVERB.NONCE) is of\nthe form nnrsw, where w ∈\n\u0002\nP{n}\n\u0003\n↓. By considering the word bracketing, there is no causal connection\nbetween (nrs) and µ◦(NONCE). Thus µ : T →P{n,s} is not causally connected.\nRemark 34. Mathematically, the above toy example is based on elements within the down-closure of the\nidentity in the free pregroup generated by a singleton. Such elements are amenable to a simple graphical\ndescription, based on the proof of Theorem 19 – the more general case appears to be signiﬁcantly harder.\n10\nComments on the assumption of freeness\nLinguistically, it is standard to base grammatical models on free pregroups. Even in areas where non-\nproper pregroups are used (such as natural language processing and compositional distributional seman-\ntics), the grammatical typing is commonly thought of as a mapping into a free pregroup, followed by\nsome appropriate quotient.\nDue to this linguistic convention, much of this paper has used free pregroups for illustrative purposes;\nhowever, concepts such as language bracketing, causal connectedness and the categorical and connect-\nedness hypotheses are deﬁned generally, rather than in simply in the free pregroup setting (notably, as\nobserved in Section 5.2, the language bracketing is signiﬁcantly more subtle in the non-free case).\n11\nConclusions & future directions\nThis paper is of course very preliminary work. Our overall thesis is that the categorical hypothesis of\nCompositional Distributional Semantics makes predictions about the forms of grammar we do and do\nnot expect to ﬁnd in natural languages. We have attempted to formalize this to the point where concrete\npredictions can be made and potentially tested.\nWe do still require a better algebraic and categorical understanding of the implications of these re-\nstrictions, including characterizing the grammars that do and do not satisfy these conditions.\n11.1\nOther forms of connectedness, and the deﬁnition of types\nThe motivation for the claim that natural languages should be sentence-connected is clear, and it is natural\nto wonder whether the same applies to other grammatical types. Given a series of natural language words\nthat come together to make up something of, say, the NOUN PHRASE type, the same intuition would\nsuggest that pregroup models of natural language should be NOUN PHRASE connected.\nBased on this intuition, it is then hard to conceive of a grammatical type U where we would not\nexpect U-connectedness of pregroup models (with the possible exception of the constructions outlined in\nSection 8). We could then consider a much stronger form of Hypothesis 27, and speculate that pregroup\nmodels of natural language should be T-connected, for all grammatical types T.\nThis raises the question of what should be considered as a ‘grammatical type’? The discussion\nof Section 5.2 presents them as an a priori given, but it is worthwhile to consider their origins. In a\nlanguage with VERB−SUBJECT −OBJECT ordering, we are happy to call VERB or VERB PHRASE\na grammatical type, but do not refer to the SUBJECT −OBJECT pair of noun-phrases as a single type.\nOf course, a pair of NOUN PHRASE types will not in general be connected, but will rely on the verb\n26\nInformation Flow in Natural Languages\nphrase to establish connections between them in a complete sentence. We may conjecture that causal\nconnection or information ﬂow is what distinguishes types from more arbitrary collections of words.\nAcknowledgements\nI am grateful to the anonymous referees of CAPNS for perceptive comments highlighting where clariﬁ-\ncations or revisions were needed. I am also grateful for many discussions on linguistics, NLP, algebra,\nand category theory with the usual suspects, including Steve Clarke, Bob Coecke, Chris Heunen, Suresh\nManandhar, Mehrnoosh Sadrzadeh, and Phil Scott.\nReferences\n[1] S. Abramsky (1996): Retracing some paths in Process algebra. In: CONCUR 96, Springer-Verlag Lecture\nNotes in Computer Science, pp. 1–17, DOI: 10.1007/3-540-61604-7_44.\n[2] S. Abramsky & B. Coecke (2004): A categorical semantics of quantum protocols. In: Proc. 19th Annual\nIEEE Symp. on Logic in Computer Science (LICS 2004), IEEE Computer Soc. Press, pp. 415–425, DOI:\n10.1109/LICS.2004.1319636.\n[3] S. Abramsky, E. Haghverdi & P. Scott (2002): Geometry of interaction and linear combinatory algebras.\nMathematical Structures in Computer Science 12 (5), DOI: 10.1017/S0960129502003730.\n[4] W. Buszkowski (2001): Lambek Grammars Based on Pregroups. In P. de Groote, G. Morrill & C. Retor´e,\neditors: Logical Aspects of Computational Linguistics, Springer Berlin Heidelberg, pp. 95–109, DOI: 10.\n1007/3-540-48199-0_6.\n[5] S. Clark, B. Coecke & M. Sadrzadeh (2008): A Compositional Distributional Model of Meaning. In P. D.\nBruza, W. Lawless, K. Van Rijsbergen, D. Sofge, B. Coecke, G. Chen, L. Kauffman & S. Lamonaco, edi-\ntors: Quantum Interaction: Proceedings of the Second Quantum Interaction Symposium - Qi-2008, College\nPublications.\n[6] B. Coecke, E. Grefenstette & M. Sadrzadeh (2013): Lambek vs. Lambek: Functorial vector space semantics\nand string diagrams for Lambek calculus. Annals of Pure and Applied Logic 164(11), pp. 1079 – 1100, DOI:\n10.1016/j.apal.2013.05.009. Special issue on Seventh Workshop on Games for Logic and Program-\nming Languages (GaLoP VII).\n[7] J.-Y. Girard (1986): The System F of Variable Types, Fifteen Years Later. Theor. Comput. Sci. 45(C), pp.\n159–192, DOI: 10.1016/0304-3975(86)90044-7.\n[8] Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen Clark, Bob Coecke & Stephen Pulman (2014): Con-\ncrete Sentence Spaces for Compositional Distributional Models of Meaning, pp. 71–86. Springer Nether-\nlands, DOI: 10.1007/978-94-007-7284-7_5.\n[9] P. Hines (1998): The algebra of self-similarity. Ph.D. thesis, University of Wales, Bangor, DOI: 10.13140/\nRG.2.2.21834.21447.\n[10] P. Hines (1999): The categorical theory of self-similarity. Theory and Applications of Categories 6, pp.\n33–46, DOI: 10.5281/zenodo.1436477.\n[11] P. Hines (2008): Machine Semantics. Theoretical Computer Science 409, pp. 1–23, DOI: 10.1016/j.tcs.\n2008.07.015.\n[12] Peter Hines (2013): Types and Forgetfulness in Categorical Linguistics and Quantum Mechanics. In Chris\nHeunen, Mehrnoosh Sadrzadeh & Edward Grefenstette, editors: Quantum Physics and Linguistics: A\nCompositional, Diagrammatic Discourse, Oxford University Press, pp. 217–251, DOI: 10.1093/acprof:\noso/9780199646296.003.0008.\nP. Hines\n27\n[13] A. Joyal (1991): The Geometry of Tensor Calculus (II). Advances in Mathematics 88, DOI: 10.1016/\n0001-8708(91)90003-P.\n[14] A. Joyal, R. Street & D. Verity (1996): Traced monoidal categories.\nMathematical Proceedings of the\nCambridge Philosophical Society 119, pp. 447–468, DOI: 10.1017/S0305004100074338.\n[15] M. Kelly & M. Laplaza (1980): Coherence for Compact Closed Categories. Journal of Pure and Applied\nAlgebra 19, pp. 193–213, DOI: 10.1016/0022-4049(80)90101-2.\n[16] A. Kissinger & S. Uijlen (2017): A categorical semantics for causal structure. In: 32nd Annual ACM/IEEE\nSymposium on Logic in Computer Science, LICS, pp. 1–12, DOI: 10.1109/LICS.2017.8005095.\n[17] J. Lambek (1999): Type Grammar Revisited. In A. Lecomte, F. Lamarche & Guy Perrier, editors: Logical As-\npects of Computational Linguistics, Springer Berlin Heidelberg, pp. 1–27, DOI: 10.1007/3-540-48975-4_\n1.\n[18] K. Popper (1965): Conjectures and Refutations. The Growth of Scientiﬁc Knowledge. Isis 56(1), pp. 88–88,\nDOI: 10.4324/9780203538074.\n[19] P. Selinger (2009): A survey of graphical langauges for monoidal categories. In B. Coecke, editor: New\nStructures for Physics, Springer, DOI: 10.1007/978-3-642-12821-9_4.\n",
  "categories": [
    "cs.CL",
    "cs.FL"
  ],
  "published": "2018-11-08",
  "updated": "2018-11-08"
}