{
  "id": "http://arxiv.org/abs/1811.04784v1",
  "title": "Improving Generalization for Abstract Reasoning Tasks Using Disentangled Feature Representations",
  "authors": [
    "Xander Steenbrugge",
    "Sam Leroux",
    "Tim Verbelen",
    "Bart Dhoedt"
  ],
  "abstract": "In this work we explore the generalization characteristics of unsupervised\nrepresentation learning by leveraging disentangled VAE's to learn a useful\nlatent space on a set of relational reasoning problems derived from Raven\nProgressive Matrices. We show that the latent representations, learned by\nunsupervised training using the right objective function, significantly\noutperform the same architectures trained with purely supervised learning,\nespecially when it comes to generalization.",
  "text": "Improving Generalization for Abstract Reasoning\nTasks Using Disentangled Feature Representations\nXander Steenbrugge\nML6 & IDlab, Ghent University - imec\nxander.steenbrugge@ml6.eu\nSam Leroux, Tim Verbelen, Bart Dhoedt\nIDlab, Ghent University - imec\nfirstname.lastname@ugent.be\nAbstract\nIn this work we explore the generalization characteristics of unsupervised represen-\ntation learning by leveraging disentangled VAE’s to learn a useful latent space on\na set of relational reasoning problems derived from Raven Progressive Matrices.\nWe show that the latent representations, learned by unsupervised training using the\nright objective function, signiﬁcantly outperform the same architectures trained\nwith purely supervised learning, especially when it comes to generalization.\n1\nIntroduction\nReasoning about abstract concepts has been a long standing challenge in machine learning. Recent\nwork by Barret et al. [1] introduces a concrete problem setting for testing generalization in the form\nof a relational reasoning problem derived from Raven Progressive Matrices that are often used in\nhuman IQ-tests. The problem exists of a grid of 3-by-3 related images where the bottom right\none is missing and a set of 8 possible answers, of which exactly one is correct. Two examples\nare shown in Figure 1. In this work we use the same dataset, which can be downloaded from\nhttps://github.com/deepmind/abstract-reasoning-matrices.\nFigure 1: Two example PGM problems: a grid of 3-by-3 related images where the bottom right one\nis missing and a set of 8 possible answers. The correct choice panels are A and C respectively.\nTo create a Procedurally Generated Matrices (PGM) dataset, a set of properties is ﬁrst randomly\nsampled from the following primitive sets:\n• Relation types: (R, with elements r): progression, XOR, OR, AND, consistent union\n• Attribute types: (A, with elements a): size, type, colour, position, number\n• Object types:\n(O, with elements o): shape, line\nThe structure S of a PGM then, is a set of triples, S = {[r, o, a] : r ∈R, o ∈O, a ∈A}. These\ntriples determine the challenge posed by one particular matrix problem. In the used dataset, up to 4\ntriples can be present in a single problem: 1 ≤|S| ≤4.\n32nd Conference on Neural Information Processing Systems (NIPS 2018), Workshop on Relational Representa-\ntion Learning, Montréal, Canada.\narXiv:1811.04784v1  [cs.LG]  12 Nov 2018\nFigure 2: WReN model from [1]: A CNN processes each context panel and an individual answer\nchoice panel independently to produce 9 vector embeddings. This set of embeddings is then passed\nto an RN network [2], whose output is a single sigmoid unit encoding the “score” for the associated\nanswer choice panel. 8 such passes are made through this network (here we only depict 2 for clarity),\none for each answer choice, and the scores are put through a softmax function to determine the\nmodel’s predicted answer.\nTo solve a PGM problem, Barrett et al. propose a Wild Relation Network (WReN) architecture [2]\nas shown in Figure 2. In this architecture, all given images (the 8 context panels and the 8 choice\npanels, all represented as 80x80 grayscale images) are ﬁrst processed by a small convolutional neural\nnetwork (CNN), resulting in 16 feature embeddings (one per panel). The 8 context embeddings are\nthen sequentially combined with each option embedding, yielding a total of 8 stacks of 9 embeddings.\nThese are ﬁnally processed by the WReN network, yielding a single scalar value for each choice\npanel, indicating its ‘matching-score’ with the given problem. The entire pipeline is then trained to\nproduce the label of the correct missing panel as an output answer by optimizing a cross entropy loss\nusing stochastic gradient descent. To include spatial information in the panel embeddings, each CNN\nembedding is also concatenated with a one-hot label indicating the panels position, followed by a\nlinear projection.\nAlthough WReNs achieve reasonable performance (62.6% classiﬁcation accuracy) on a randomly\nheld-out test set of the complete training data (which includes triples S from all possible primitives\n[r, o, a]), the generalization performance on new reasoning problems S (containing primitives not\nseen during training) is signiﬁcantly worse. This shows that while the model manages to ﬁt the\ntraining distribution reasonably well, it fails to generalize in a meaningful way. One of the reasons\nfor this lack of strong generalization is that there is no explicit pressure for the model to discover the\ngenerative, latent factors of the problem domain. In fact (as can be seen in Figure 8 of the appendix),\nthe learned CNN embeddings seem to completely disregard the underlying causal structure of the\nproblem domain. In this paper, we aim to improve the generalization performance of WReNs, by ﬁrst\nlearning a disentangled latent space that encodes the PGM panels, and then learning to reason within\nthis space using the RN architecture.\n2\nUnsupervised representation learning with Variational Autoencoders\nBecause the structure of the Raven problems depend explicitly upon a set of generative factors\n(such as shape, size, colour, ...), recovering these variables in a suitable latent space should prove\nbeneﬁcial for solving the relational reasoning problem. To test this hypothesis, we leverage Variational\nAutoencoders [3] to learn an unsupervised mapping from high-dimensional pixel space to a lower\ndimensional and more structured latent space that is subsequently used by the WReN model to\ncomplete the relational reasoning task.\nThe behavior of these models has been widely studied [4, 5, 6, 7] and a clear trade-off between\ndesirable latent space properties (such as disentanglement of generative factors, linearity & sparsity)\nand reconstruction quality is usually present. The effect of these constraints on the generalization\nstrength of the resulting latent space, however, has not been widely studied.\n2\nAs such, our setup replaces the CNN-encoder of [1] with a disentangled ‘β-VAE’ that is trained\nseparately from the WReN model using the modiﬁed ELBO optimization objective as described\nin [6]:\nL(θ, φ; x, z, β) = Eqφ(z|x) [log pθ(x|z)] −βDKL (qφ(z|x)∥p(z)) .\n(1)\nIn this case, different β-values control the trade-off between reconstruction quality and latent variable\ndisentanglement.\nOne common problem with the β-VAE setting is that high β-factors often constrain the latent space to\nsuch an extent that encoding small, visual details (which only have a marginal effect on reconstruction\nerror) does not outweigh the KL-penalty that follows from the corresponding divergence from the\nimposed prior distribution, even though these details are often task-critical. To solve this problem\nwe apply a variable-β training regime as described in Appendix D. We ended up using the objective\nfunction from (1) with a gradually increasing β: 0.5 →4.0. The effects of this training regime can\nbe seen in Figure 3.\nFigure 3: Effect of β on reconstruction quality. Three sets of reconstructions are shown using the\nsame VAE trained with different β training regimes. In the variable-β scenario the latent space ﬁrst\nlearns to capture small visual details and only later receives pressure to disentangle them.\nWe trained various models with z = 64 latent dimensions on the PGM dataset using different β\nfactors and annealing schemes. Details of our VAE encoder and decoder architecture and training\nscheme are discussed in Appendices A and D. After training we can visualize that the disentangled\nlatent space indeed captures many of the underlying generative factors of the problem domain (see\nFigure 4). A more extended set of latent space interpolations is shown in Appendix B.\n3\nLeveraging the learned latent space for relational reasoning\nHere we test the generalization properties of the learned latent space by using the learned image\nembeddings in the PGM problem setting. To start the WReN training process, we freeze the\npretrained VAE-encoder graph and use it to initialize the encoder step of the WReN architecture.\nIn order to create a fair comparison, our VAE architecture uses the exact same encoder network\nas the CNN-embedder in [1]. But, since we use a latent space of z = 64 latent dimensions, the\n512 convolutional-features from the encoder are passed through two FC-layers mapping onto 64-\ndimensional vectors representing the means and log(variances) of a factorized, Gaussian distribution.\nAt training time we randomly sample from this posterior to get the latent representations used in\nfurther processing. At test time, we simply use the mean vector. Apart from this difference in input\nfeature dimensionality (64 in our VAE case vs 512 in the default CNN-encoder case), the entire\nWReN architecture is identical to the one used in [1].\nFinally, the WReN model is trained for 6 epochs using a ﬁxed encoder and then ﬁnetuned end-to-end\nfor another 2 epochs to get the results displayed in Table 1. Notice that while we outperform the\ndefault WReN model trained with purely supervised learning on various generalization regimes as\nintended, surprisingly we also do better on two of the validation sets, indicating that the disentangled\nlatent space does in fact make the relational reasoning problem more tractable for the RN network.\n3\nFigure 4: Latent space visualization obtained by encoding the input images (left) and interpolating\nbetween the support boundaries of the posterior distribution for z[1] and z[63], while keeping all other\nlatents constant before rendering the resulting z through the decoder network. Notice how the VAE’s\nlatent space (β=4.00) clearly disentangles multiple generative factors such as the colour/presence of\nthe diamond-shaped raster (top rows) or the shape of an object in a single position (bottom rows).\nModel-type\nCNN-WReN [1]\nVAE-WReN (β=4.00)\nGeneralization regime\nVal (%)\nTest (%)\nTest (kappa)\nVal (%)\nTest (%)\nTest (kappa)\nNeutral\n63.0\n62.6\n0.573\n64.8\n64.2\n0.591\nH.O. Triple Pairs\n63.9\n41.9\n0.336\n64.6\n43.6\n0.355\nH.O. Attribute Pairs\n46.7\n27.2\n0.168\n70.1\n36.8\n0.278\nH.O. Triples\n63.4\n19.0\n0.074\n59.5\n24.6\n0.138\nTable 1: Relational Reasoning Results. Each data regime consists of 1.2M training images, a held-out\nvalidation set of 20K images drawn from the same problem distribution and a generalization test set\nof 200K images containing new problem sets S = {r, o, a} not seen during training. Regimes are\nsorted according to the degree of generalization required to solve them (more info in [1]). We also\nlist Cohen’s Kappa values which vary linearly from 0 (random guessing) to 1 (oracle).\n4\nConclusion\nIn this paper, we show that disentangled variational autoencoders can be leveraged to learn a mapping\nfrom high-dimensional pixel space to a low-dimensional and more structured latent space without\nany explicit supervision. This disentangled latent space can subsequently be leveraged for solving a\nnon-trivial relational reasoning problem and by doing so, outperforms the same architecture trained\nusing a fully supervised approach.\n4.1\nFuture work\nFuture work will focus on further investigating the desirable characteristics of a generally useful\nlatent space (disentanglement, linearity, sparsity, ...) and explore new objective functions that can\nbe leveraged for unsupervised representation learning, such as various representation losses [8, 9],\nGAN-inspired discriminator networks [10] and predictive capacity [11, 12]. Additionally, the current\nWReN setup does not leverage the intrinsic variational properties of the learned latent space.\nBibliography\n[1] D. G. T. Barrett, F. Hill, A. Santoro, A. S. Morcos, and T. Lillicrap. Measuring abstract reasoning\nin neural networks. International Conference on Machine Learning, 2018.\n4\n[2] A. Santoro, D. Raposo, D. G. T. Barrett, M. Malinowski, R. Pascanu, P. Battaglia, and T. Lil-\nlicrap. A simple neural network module for relational reasoning. Conference on Neural\nInformation Processing Systems (NIPS), 2017.\n[3] D. P Kingma and M. Welling. Auto-Encoding Variational Bayes. Proceedings of the 2nd\nInternational Conference on Learning Representations (ICLR), 2014.\n[4] H. Kim and A. Mnih. Disentangling by Factorising. Conference on Neural Information Pro-\ncessing Systems (NIPS), Learning Disentangled Representations: From Perception to Control\nWorkshop, 2017.\n[5] D. Jimenez Rezende and F. Viola. Taming VAEs. ArXiv e-print arXiv:1810.00597, 2018.\n[6] C. P. Burgess, I. Higgins, A. Pal, L. Matthey, N. Watters, G. Desjardins, and A. Lerchner.\nUnderstanding disentangling in β-VAE. ArXiv e-print arXiv:1804.03599, April 2018.\n[7] I. Higgins, L. Matthey, X. Glorot, A. Pal, B. Uria, C. Blundell, S. Mohamed, and A. Ler-\nchner. Early Visual Concept Learning with Unsupervised Deep Learning. ArXiv e-print\narXiv:1606.05579, June 2016.\n[8] I. Higgins, A. Pal, A. A. Rusu, L. Matthey, C. P Burgess, A. Pritzel, M. Botvinick, C. Blun-\ndell, and A. Lerchner. DARLA: Improving Zero-Shot Transfer in Reinforcement Learning.\nProceedings of the 34rd International Conference on Machine Learning (ICML), 2017.\n[9] A. Boesen Lindbo Larsen, S. Kaae Sønderby, H. Larochelle, and O. Winther. Autoencoding be-\nyond pixels using a learned similarity metric. Proceedings of the 33rd International Conference\non International Conference on Machine Learning (ICML), 2016.\n[10] J. Bao, D. Chen, F. Wen, H. Li, and G. Hua. CVAE-GAN: Fine-Grained Image Generation\nthrough Asymmetric Training. IEEE International Conference on Computer Vision (ICCV),\n2017.\n[11] Y. Bengio. The Consciousness Prior. ArXiv e-print arXiv:1709.08568, September 2017.\n[12] A. van den Oord, Y. Li, and O. Vinyals. Representation Learning with Contrastive Predictive\nCoding. ArXiv e-print arXiv:1807.03748, July 2018.\nAppendix\nA\nArchitecture details\nOur VAE setup uses a convolutional encoder-decoder architecture and a N(0, 1)-Gaussian distribution\nas prior for the latent variables. The encoder has 4 conv2d() layers intermitted with BatchNorm2d()\nlayers. The decoder uses the same architecture with ConvTranspose2d() layers. Every conv-layer\nuses 32 kernels of size 3 and a stride of 2. We did not use any pooling layers.\nThe VAE bottleneck has 64 latent variables which are parameterized through their respective means\nand log(variances). We use the common reparameterization trick from [3] for backpropagating\nthrough the non-deterministic bottleneck.\nThe VAE was trained using the ADAM optimizer with a learning rate of .0003 and a batch size of 32\nPGM problems per batch ( = 512 panel images).\nIn the WReN network, we use dropout (p = 0.5) in the penultimate layer. All models were\nimplemented in PyTorch and trained on a single Tesla K80 GPU.\nB\nAdditional latent space visualizations\nTo further aid the insight of the reader, we provide a bunch of additional visualizations we found\nhelpful to understand the latent space behavior of disentangled VAEs. See Figures 5, 6, 7.\nC\nFailure modes of the WReN CNN\nOne of the problems with the purely supervised CNN approach is that the model receives no explicit\npressure to discover the generative, latent structure of the problem domain. This can be clearly seen\nin Figure 8.\n5\nFigure 5: Latent space interpolations. VAE with β=0,01 - avg-MSE = 6,06 and avg-KL-divergence\n= 120. Many latent traversals change a variety of generative factors simultaneously. Note that the\nnumerical bounds of the interpolations for each latent variable are clipped using the support from the\nposterior distribution (generated by passing 5000 random train images through the encoder network).\nUnfortunately, the used VAE approach based on a pixel-reconstruction loss also has its own drawbacks\nas can be seen in Figure 9. This lends support to the widely held assumption (see eg. [8, 9]) that simple,\npixel-based reconstruction metrics are not the ideal optimization objectives for visual representation\nlearning.\nD\nImpact of annealing β\nOne common problem with β-VAEs is that imposing a large disentanglement constraint often causes\nthe latent space to collapse to supporting only the most salient modes of the visual input domain,\nfailing to capture more ﬁne-grained visual information that is often task critical.\nTo tackle this problem we start training the VAE with a low β-factor and gradually increase the\ndisentanglement constraint until a desirable state is reached. By primarily focusing on the visual\nreconstruction error in the beginning of training, the latent space learns to capture most of the visual\ninformation before the disentanglement constraint begins dominating the training objective, leading\nto a much better ﬁnal representation model.\n6\nFigure 6: Latent space interpolations. VAE with β=4,00 - avg-MSE=20,2 and avg-KL-divergence=22.\nMost latent traversals correspond to a single, clear generative factor. Again, the interpolation bounds\nare clipped using the support from the posterior distribution.\nFigure 7: Visualization of the latent distribution for the two variables with highest average KL-\ndivergence from a disentangled VAE with β=4.0. We run 5000 randomly sampled images through\nthe encoder network and plot the resulting distributions of µ and σ as well as 5000 random samples\ndrawn from each of the resulting Gaussian distributions. Notice that latent variables with a very high\nKL-divergence from the N(0, 1)-prior (eg. top row where σ is always close to zero) can still result in\na near-Gaussian distribution over sampled z-values.\n7\nFigure 8: Input images (top) and reconstructions (bottom) obtained by training a convolutional-\ndecoder on the panel features extracted by the CNN embedder from [1]. As can be seen, the\ngenerative factors deﬁning the problem domain are not contained within the learned embedding space,\nlending support to the claim that the CNN simply overﬁts on speciﬁc visual features in the training\nexamples instead of discovering useful latent structure.\nFigure 9: When forcing the VAE to trade-off reconstruction quality for smaller KL-divergence from\nthe latent prior, small, grayscale objects will be sacriﬁced ﬁrst since they correspond to the smallest\nincrease in MSE penalty in pixel space. Larger and darker objects will maintain good reconstruction\nquality until much higher β values are imposed. (Shown here on a custom dataset we generated for\ntesting purposes.)\n8\n",
  "categories": [
    "cs.LG",
    "cs.NE",
    "stat.ML"
  ],
  "published": "2018-11-12",
  "updated": "2018-11-12"
}