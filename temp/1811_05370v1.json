{
  "id": "http://arxiv.org/abs/1811.05370v1",
  "title": "Unsupervised Transfer Learning for Spoken Language Understanding in Intelligent Agents",
  "authors": [
    "Aditya Siddhant",
    "Anuj Goyal",
    "Angeliki Metallinou"
  ],
  "abstract": "User interaction with voice-powered agents generates large amounts of\nunlabeled utterances. In this paper, we explore techniques to efficiently\ntransfer the knowledge from these unlabeled utterances to improve model\nperformance on Spoken Language Understanding (SLU) tasks. We use Embeddings\nfrom Language Model (ELMo) to take advantage of unlabeled data by learning\ncontextualized word representations. Additionally, we propose ELMo-Light\n(ELMoL), a faster and simpler unsupervised pre-training method for SLU. Our\nfindings suggest unsupervised pre-training on a large corpora of unlabeled\nutterances leads to significantly better SLU performance compared to training\nfrom scratch and it can even outperform conventional supervised transfer.\nAdditionally, we show that the gains from unsupervised transfer techniques can\nbe further improved by supervised transfer. The improvements are more\npronounced in low resource settings and when using only 1000 labeled in-domain\nsamples, our techniques match the performance of training from scratch on\n10-15x more labeled in-domain data.",
  "text": "Unsupervised Transfer Learning for Spoken\nLanguage Understanding in Intelligent Agents\nAditya Siddhant 1, Anuj Kumar Goyal 2, Angeliki Metallinou 2\nasiddhan@cs.cmu.edu, anujgoya@amazon.com, ametalli@amazon.com\n1 Carnegie Mellon University, 2 Amazon Alexa AI\nAbstract\nUser interaction with voice-powered agents generates large\namounts of unlabeled utterances. In this paper, we explore\ntechniques to efﬁciently transfer the knowledge from these\nunlabeled utterances to improve model performance on Spo-\nken Language Understanding (SLU) tasks. We use Embed-\ndings from Language Model (ELMo) to take advantage of\nunlabeled data by learning contextualized word representa-\ntions. Additionally, we propose ELMo-Light (ELMoL), a\nfaster and simpler unsupervised pre-training method for SLU.\nOur ﬁndings suggest unsupervised pre-training on a large cor-\npora of unlabeled utterances leads to signiﬁcantly better SLU\nperformance compared to training from scratch and it can\neven outperform conventional supervised transfer. Addition-\nally, we show that the gains from unsupervised transfer tech-\nniques can be further improved by supervised transfer. The\nimprovements are more pronounced in low resource settings\nand when using only 1000 labeled in-domain samples, our\ntechniques match the performance of training from scratch\non 10-15x more labeled in-domain data.\nIntroduction\nVoice-powered artiﬁcial virtual agents have become popu-\nlar amongst consumer devices, as they enable their users to\nperform everyday tasks through intuitive and natural user in-\nterfaces. SLU tasks such as intent classiﬁcation and entity\ntagging are critical functionalities of these agents. Fast ex-\npansion of these functionalities to new domains is important\nfor achieving engaging and informative interactions, as it in-\ncreases the range of capabilities that their users enjoy.\nFor SLU tasks, most of the current methods use su-\npervised learning, which relies on manually labeled data\nfor building high quality models. The supervised learn-\ning paradigm is therefore costly, time-consuming and does\nnot scale well for cases where the label space is continu-\nously expanding as new functionality is added to an agent.\nAlso, user interaction with voice-powered agents generates\nlarge amounts of unlabeled text, produced by the Automatic\nSpeech Recognition (ASR) engine. This ASR output text\nis a large and valuable resource of conversational data that\nis available in practically unlimited quantities and could be\nused to improve the agent’s SLU accuracy. Thus, the ability\nCopyright c⃝2019, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nto learn effectively from unlabeled text is crucial to alleviat-\ning the bottlenecks of supervised learning.\nThe machine learning community is actively exploring\ntransfer learning and unsupervised learning for low resource\ntasks. Goyal, Metallinou, and Matsoukas (2018) explored\ntransfer learning from existing annotated SLU domains for\nbuilding models for related, low-resource domains for ar-\ntiﬁcial agents. However, such transfer learning techniques\nrely on large annotated resources from a related function-\nality. Recent work has used language modeling (LM) as a\nproxy task for learning context dependent word embeddings\nfrom large unlabeled text corpora (Peters et al. 2018). These\nembeddings allow for unsupervised knowledge transfer and\nhave been shown to bring performance gains for various\ndownstream natural language processing (NLP) tasks.\nIn this work, we propose an unsupervised transfer learn-\ning technique inspired from ELMo and Universal Language\nModel Fine Tuning (ULMFiT) to leverage unlabeled text for\nbuilding SLU models (Peters et al. 2018; Howard and Ruder\n2018). We also explore the combination of unsupervised and\nsupervised knowledge transfer for SLU. We evaluate our\nmethods on various tasks and datasets, including data from\na popular commercial intelligent agent. Our results show\nthat unsupervised transfer using unlabeled utterances can\noutperform both training from scratch and supervised pre-\ntraining. Additionally, the gains from unsupervised transfer\ncan further be improved by supervised transfer. These im-\nprovements are more pronounced in low resource setting and\nwhen only 1K labeled in-domain samples are available, the\nproposed techniques match the performance of training from\nscratch on 10-15x more labeled data. Concretely, our contri-\nbutions are:\n• We apply ELMo embeddings for unsupervised knowledge\ntransfer from raw ASR text and show SLU accuracy gains.\n• We propose ELMo-Light (ELMoL), a light-weight ELMo\nalternative that is well-suited for commercial settings,\nwith comparable accuracy to ELMo for most SLU tasks.\n• We combine unsupervised and supervised transfer learn-\ning, and show the additive effect of the two techniques.\n• We extensively evaluate our methods on benchmark SLU\ndatasets and data from a commercial agent, across various\nresource conditions.\narXiv:1811.05370v1  [cs.CL]  13 Nov 2018\nThe rest of paper is organized as follows. To provide a bit\nof background, we discuss related work and neural architec-\ntures for SLU and then introduce the methods we use for un-\nsupervised transfer including the proposed ELMoL. Finally,\nwe describe the datasets, experimental setup, results and end\nwith directions for future work. Table 1 summarizes some of\nthe frequently used abbreviations throughout the paper.\nAbbr.\nDescription\nUT\nUnsupervised Transfer\nST\nSupervised Transfer\nIC\nIntent Classiﬁcation\nET\nEntity Tagging\nLM\nLanguage Model\nguf\nGradual Unfreezing\ntlr\nTriangular Learning Rate\ndiscr\nDiscriminative Fine Tuning\nTable 1: Abbreviation Table\nRelated Work\nDeep learning models using CNNs and LSTMs are state\nof the art for many NLP tasks. Examples include apply-\ning LSTMs for sentence classiﬁcation (Liu et al. 2015;\nSocher et al. 2013), LSTM with Conditional Random Field\n(CRF) decoder for sequence labeling (Chiu and Nichols\n2016) and CNN-LSTM combinations for LM (Jozefowicz\net al. 2016). LSTMs with attention have also been used for\nSLU tasks including Entity tagging (ET) and intent classiﬁ-\ncation (IC) (Liu and Lane 2016).\nTo enable robust training of deep learning models in low\nresource settings, the community is actively exploring semi-\nsupervised, transfer and multi-task learning techniques. In\nthe multi-task paradigm a network is jointly trained to op-\ntimize multiple related tasks, exploiting beneﬁcial correla-\ntions across tasks (Liu and Lane 2016; Collobert and Weston\n2008). Liu et al. (2018) used language models (LMs) as an\nauxiliary task in a multi-task setting to improve sequence la-\nbeling performance. Transfer learning addresses the transfer\nof knowledge from data-rich source tasks to under-resourced\ntarget tasks. Neural transfer learning has been successfully\napplied in computer vision where lower network layers are\ntrained in high-resource supervised datasets like ImageNet\nto learn generic features (Krizhevsky, Sutskever, and Hin-\nton 2012), and are then ﬁne-tuned on target tasks, leading\nto impressive results for image classiﬁcation and object de-\ntection (Donahue et al. 2014; Sharif Razavian et al. 2014).\nIn NLP, such supervised transfer learning was successfully\napplied for SLU tasks, by learning IC and ET models on\nhigh resource SLU domains, and then ﬁne-tuning the net-\nwork on under resourced domains (Goyal, Metallinou, and\nMatsoukas 2018). Similar ideas have also been explored for\nPOS tagging using for cross-lingual transfer learning (Kim\net al. 2017).\nUnsupervised methods for knowledge transfer include\ncomputing word and phrase representations from large un-\nlabeled text corpora. Examples include Word2Vec and Fast-\nText, where context independent word representations are\nlearnt based on LM-related objectives (Mikolov et al. 2013;\nBojanowski et al. 2017). Unsupervised sentence representa-\ntions have been computed via predicting sentence sequences\nlike skip-thought (Kiros et al. 2015), and through a combi-\nnation of auxiliary supervised and unsupervised tasks (Cer\net al. 2018). Recent work has introduced LM-based word\nembeddings, ELMo, that are dependent on sentence context\nand are shown to lead to signiﬁcant accuracy gains for var-\nious downstream NLP tasks (Peters et al. 2018). Unsuper-\nvised pre-training has also been used as a form of knowl-\nedge transfer by ﬁrst training a network using an LM objec-\ntive and then ﬁne-tuning it on supervised NLP tasks. This\nhas been shown to be efﬁcient for sentence classiﬁcation\n(Howard and Ruder 2018; Dai and Le 2015) and (Radford et\nal. 2018) for textual entailment and question answering. Our\nwork, building upon transfer learning ideas such as super-\nvised model pre-training (Goyal, Metallinou, and Matsoukas\n2018), LM-ﬁne tuning (Howard and Ruder 2018) and con-\ntext dependent word embeddings (Peters et al. 2018), intro-\nduces a light-weight ELMo extension and combines those\nmethods for improving SLU performance in a commercial\nagent.\nNeural Architectures for SLU\nWe focus on SLU for voice powered artiﬁcial agents, specif-\nically on intent classiﬁcation (IC) and Entity tagging (ET)\nmodels which are essential for such agents. Given a user re-\nquest like ‘how to make kadhai chicken’, the IC model clas-\nsiﬁes the intention of the user, such as ‘GetRecipe’ while the\nET model tags the entities of interest in the utterance, such\nas ‘Dish’=‘kadhai chicken’.\nWe use a multi-task deep neural network architecture for\njointly learning the IC and ET models, hence exploring ben-\neﬁcial correlations between the two tasks. Our architecture\nis illustrated in Figure 1. It consists of a bottom shared bidi-\nrectional LSTM (bi-LSTM) layer on top of which we train\na bi-LSTM-CRF for ET and a bi-LSTM for IC. The two\ntop layers are optimized separately for ET and IC, while the\ncommon bottom layer is optimized for both tasks. The ob-\njective function for the multi-task network combines the IC\nand ET objectives.\nSpeciﬁcally let rc\nt denote the common representation\ncomputed by the bottom-most bi-LSTM for each word in-\nput at word t. The ET forward LSTM layer learns a repre-\nsentation rET,f\nt\n= φ(rc\nt, rET\nt−1), where φ denotes the LSTM\noperation. The IC forward LSTM layer learns rIC,f\nt\n=\nφ(rc\nt, rIC\nt−1). Similarly, the backward LSTM layers learn\nrET,b\nt\nand rIC,b\nt\n.\nTo obtain the entity tagging decision, we feed the ET bi-\nLSTM layer’s output per step, denoted as [rentity\nt\n]T\nt=1 into\nthe CRF layer, and produce a entity label sequence [ ˆSt]T\nt=1.\nFor the intent decision, we concatenate the last step from the\nforward LSTM with the ﬁrst step of the backward LSTM to\nget the intent representation rintent, and feed it into a soft-\nmax layer for classiﬁcation:\nFigure 1: Multi-task architecture for IC and ET\nrentity\nt\n= rET,f\nt\n⊕rET,b\nt\n, rintent = rIC,f\nT\n⊕rIC,b\n1\n[ ˆSt]T\nt=1 = CRF([rentity\nt\n]T\nt=1)\nˆI = softmax(WIrintent + bI)\nwhere ⊕denotes concatenation. WI, bI are the weights and\nbiases for the intent softmax layer and CRF(·) denotes the\nCRF layer. ˆSt is the predicted entity tag per step, and ˆI is\nthe predicted intent label for the utterance.\nMethods for Unsupervised Transfer\nWhen building ET and IC models for a target new SLU\ndomain like recipes, we typically have between few hun-\ndreds and few hundred thousand labeled utterances available\nfor supervised training. We also have manually labeled ET\nand IC data from existing source SLU domains, like music,\nweather etc, which is a few million combined. Finally, we\nhave billions of unlabeled utterances produced by the ASR\nengine from live user-agent interaction in our system. In this\nsection, we describe Unsupervised Transfer (UT) learning\ntechniques that leverage this large corpus of unlabeled ASR\ntext to improve the accuracy of the IC and ET tasks. We also\ncombine UT with supervised transfer learning from the ex-\nisting labeled data from source domains, to obtain additional\ngains.\nEmbeddings from Language Model (ELMo)\nPeters et al. (2018) introduced contextualized word embed-\ndings, called ELMo, computed by ﬁrst training an LM us-\ning a state-of-the-art architecture with multiple CNNs over\ncharacters and L bi-LSTM layers on top to model the sen-\ntence (the network is also referred to as CNN-BIG-LSTM\n(Jozefowicz et al. 2016)). After the model is trained on a\nlarge corpus, the outputs of the different layers are used as\nembeddings for downstream tasks. Speciﬁcally, let’s denote\nwith xt the context-independent word representations from\nthe CNN layer at word t and with hLM\nt,i\nthe contextual word\nrepresentations obtained by concatenating the forward and\nbackward LSTM activations at word t and layer i. The con-\ntextual ELMo embeddings at t are computed through linear\ncombination as:\nELMot = γ(s0 · xt + ΣL\ni=1si · hLM\nt,i )\nwhere γ is a scalar parameter for scaling the ELMo vector\nWe trained ELMo embeddings using L = 2 bi-LSTM\nlayers on ASR text, and used them as input to the down-\nstream SLU architecture of Figure 1. ELMo parameters were\nkept frozen, as in Peters et al. (2018), and parameters si and\nγ were jointly optimized on the ET and IC tasks. An ad-\nvantage of this unsupervised pre-training is that the CNN-\nBIG-LSTM weights do not experience catastrophic forget-\nting, therefore the SLU architecture can be trained without\nlosing the knowledge gained from unlabeled data. However,\ncomputing ELMo embeddings at runtime introduces many\nadditional parameters (e.g., weights from the large CNN-\nBIG-LSTM). In practice, we observed that using ELMo in-\ncreases the inference time by 1.6x and triples the memory\nrequirement at runtime because of additional parameters,\nwhich makes this solution less attractive for a commercial\nSLU system.\nELMo-Light (ELMoL) for SLU tasks\nWe introduce ELMo-Light (ELMoL) which is a light-weight\nalternative for computing contextualized word embeddings\nwithin our SLU architecture. We base our model on two ob-\nservations. First, typical conversational requests to artiﬁcial\nagents use brief and simple language therefore a smaller LM\narchitecture may be sufﬁcient. Second, the purpose of the\nlower shared layer of our SLU is to learn generic represen-\ntations for IC and ET, and could be additionally pre-trained\nwith an LM objective on a larger corpus.\nSpeciﬁcally, we train an LM network consisting of char-\nacter level CNNs and a single bi-LSTM layer on unlabeled\nASR text to compute contextual word representation hLM\nt,i\nat\nword t, as shown in Fig 2(a). Then, we combine these con-\ntextual embeddings with non-contextual word embeddings\nxt, and feed them as input to our upper SLU model layers\nfor IC and ET, optimizing the linear weights on the down-\nstream tasks, similarly to ELMo:\nELMoLt = γ(s0 · xt + s1 · hLM\n1,i )\nThis model is illustrated in Fig 2(b). Compared to our\noriginal architecture of Fig 1(a), the proposed ELMoL\nmodel introduces only three additional trainable parameters\n(γ, s0, s1). This is because the single bi-LSTM LM weights\nare re-purposed as the lower layer our of SLU architecture\nand are further ﬁne-tuned on the in-domain data along with\nthe rest of the SLU network. The character level CNNs of the\nLM are removed from the ﬁnal architecture and their output\nis used as ﬁxed non-trainable embeddings. Therefore, EL-\nMoL embeddings do not incur additional latency or memory\ncosts and are well suited for our SLU system.\nTechniques for effectively training ELMoL\nIn contrast\nto ELMo, for the ELMoL model we do not freeze the\nweights of lower bi-LSTM layer. Freezing them would only\nleave a single trainable bi-LSTM layer for each of the IC and\nST tasks during the supervised training stage, which empiri-\ncally negatively impacts the ﬁnal accuracy. Instead, after un-\nsupervised LM pre-training is complete for the lower layers,\nwe update all layers of the network using the target domain\nlabeled IC and ET data.\n(a) ELMoL LM Pre-training\n(b) ELMoL embeddings used in the SLU multi-task model\nFigure 2: Left: ELMoL embeddings from simpliﬁed LM. Right: ELMoL embeddings used in the SLU model\nWe empirically noticed that making the lower layer train-\nable causes catastrophic forgetting, e.g., the knowledge\ntransferred through pre-trained weights is erased by the gra-\ndient updates during supervised training. To counter this\nissue, we apply a combination of techniques from the lit-\nerature (Howard and Ruder 2018), speciﬁcally gradual\nunfreezing (guf), discriminative ﬁne-tuning (discr) and\nslanted triangular learning rates (tlr). guf means updat-\ning only the top layer for a few epochs keeping lower layers\nfrozen, and then progressively updating bottom layers. It can\nbe combined with discr, e.g, using different learning rates\nacross network layers. Here, we use a lower learning rate\nfor the bottom layer to avoid large updates in the transferred\nknowledge of the lower layer weights. tlr refer to learning\nrates that are initially slow which discourages drastic up-\ndates in early stages of learning, then it rapidly increase al-\nlowing more exploration of the parameter space and then\nslowly decrease enabling learned parameters to stabilize.\nCombining Supervised Transfer (ST) with UT\nGoyal, Metallinou, and Matsoukas (2018) used supervised\ntransfer learning in a similar SLU model by pre-training\na multi-task network on labeled data from source domains\nand ﬁne-tuning the weights on ET and IC tasks of an under-\nresourced target domain, updating the top layers to reﬂect\nthe labels of the new domain. We combine this supervised\npre-training technique with unsupervised ELMo and our\nproposed ELMoL embeddings. We explore the following\ntwo variations for combining unsupervised pre-training with\nsupervised transfer learning.\nELMo+ST\nThis method is similar to Goyal, Metallinou,\nand Matsoukas (2018), the only difference being that instead\nof FastText embeddings, we use ELMo embeddings while\npre-training the network on labeled data from source do-\nmain(s). This method, therefore, becomes a 3 step process.\nThe ﬁrst step involves training the external LM network us-\ning unlabeled data. In the second step, we use the embed-\ndings from LM, trained in the ﬁrst step, to pre-train our\nmulti-task IC/ET architecture using labeled data from source\ndomains. Finally, in the third step, we ﬁne tune the weights\nin multi-task IC/ET architecture on labeled data from target\ndomain. The LM weights are frozen in steps 2 and 3.\nELMoL+ST\nThis is also a three step method where in the\nﬁrst step, the lower layer of multi-task IC/ET architecture\nis pre-trained using a language model objective. This step\nis essentially the same as the ﬁrst step of ELMoL training\nwithout ST. In the second step, we train the entire architec-\nture on labeled source data using guf and discr. This step,\ntherefore, becomes a two step process itself in which we ﬁrst\ntrain the upper layers of the architecture keeping the lower\nlayer frozen with weights from step 1. Once the upper lay-\ners stabilize, we train the entire network with labeled source\ndata, keeping the lower layer learning rate much lower than\nthe higher layer (discr). Finally, in the third step of the pro-\ncess, we ﬁne tune all the layers on labeled data from target\ndomain using all 3 techniques guf, discr and tlr.\nWhile we combine ST with UT to see if it can provide\nan additive effect and bump performance even further, we\nwould like to point out that the main focus of this paper is\nunsupervised transfer learning and therefore, comparison of\nST+UT with other semi-supervised techniques do not form\na part of our experiments.\nDatasets\nInternal Labeled and Unlabeled SLU datasets\nWe use two internal SLU domains as our target domains,\ndenoted as Domain A (5 intents, 36 entities) and Domain B\n(22 intents, 43 entities). We have a total of 43K and 100K la-\nbeled training samples for Domains A and B respectively. In\naddition to these two target labeled datasets, we pool labeled\ndatasets from tens of other domains of our SLU system. In\ntotal, we use around 4 million utterances which are labeled\nin terms of the intents and entities of their respective do-\nmains. We refer to this labeled dataset as Internal Labeled\nSource Dataset (ILSD). ILSD spans data from a range of\nfunctionalities like playing media (music, movies, books),\nquestion answering, performing tasks like setting calendar\nevents, alarms and notiﬁcations, etc. The total number of in-\ntents and entities in this dataset are of the order of hundreds\neach. We also collect unlabeled ASR text from the live ut-\nterances of users interacting with our agent. Speciﬁcally, we\nuse a large set of 250 million de-duplicated tokens collected\nover a year. We refer to this dataset as Internal Unlabeled\nDataset (IUD).\nPublic Labeled and Unlabeled datasets\nFor benchmarking our proposed methods we use two la-\nbeled public SLU datasets: ATIS and SNIPS. ATIS is a\ncommon SLU benchmark from the travel planning domain\nwhich contains 5K utterances (Hemphill, Godfrey, and Dod-\ndington 1990). SNIPS is a more recent SLU benchmark\ncreated by the company snips.ai for benchmarking com-\nmercial NLU engines offered by companies like Google,\nAmazon, etc (Coucke et al. 2017). SNIPS includes 7 com-\nmonly used intents like asking for the weather, playing mu-\nsic, booking restaurants and contains 13K training utter-\nances. We also used two unlabeled datasets, the 1B Word\nBenchmark (1BWB) and the 1M SLU Benchmark data\n(1MSLU). The 1BWB is a common text benchmark dataset\nused in the LM literature (Chelba et al. 2014), it is extracted\nfrom News Crawl and contains 750 million tokens. We ad-\nditionally collected an SLU text dataset by combining vari-\nous public SLU corpora, including training splits from ATIS,\nSNIPS, DSTC2 (Henderson, Thomson, and Williams 2014),\nand others. We call this dataset 1M SLU Benchmark data\n(1MSLU) and it contains 1 million tokens in total. These\ndatasets were used as additional resources to the internal\nASR data for training language models for unsupervised\nknowledge transfer. Statistics of the target datasets are in Ta-\nble 2.\n#Training\nSamples\n#Dev\nSamples\n#Test\nSamples\nVocab\nSize\n#Int.\n#Ent.\nDomain-A\n43168\n3680\n4752\n62600\n5\n36\nDomain-B\n100000\n8227\n8695\n62600\n22\n43\nATIS\n4478\n500\n893\n868\n26\n79\nSNIPS\n13084\n700\n700\n11823\n7\n39\nTable 2: Internal and public target datasets statistics\nExperiments and Results\nBaselines\nOur ﬁrst baseline uses no form of unsupervised pre-training\nand works only with the target data at hand. We refer to\nthis baseline as NoUT. Our second baseline uses pre-trained\nFastText word embeddings (Bojanowski et al. 2017). We\nchose this baseline because pre-trained word embeddings\nis a very popular means of unsupervised pre-training in the\nNLP literature. This baseline is referenced as Fasttext.\nExperimental Setup\nNetwork Hyper-parameters\nThe experimental setup was\nkept same for all 4 datasets with minor adjustments in hyper-\nparameters. We use 200 hidden units for all three LSTM lay-\ners in our multi-task architecture. We use Adam optimizer\n(Kinmga and Ba 2015) with initial learning rate 0.0001\nfor internal datasets (Domain-A and Domain-B) and 0.0005\nfor ATIS and SNIPS. Both IC and ET losses are weighted\nequally in the total loss. Training is done upto 25 epochs\nwith early-stopping based on sum of IC and ET scores on\ndevelopment set. Dropout probability is 0.5 for ATIS and\nSNIPS and 0.2 for internal datasets and we use L2 regular-\nization on all weights with lambda=0.0001.\nNetwork Input Embeddings\nThe embedding layer di-\nmension depends on the type of embedding being used.\nFor NoUT and FastText, the embedding dimension is 400.\nNoUT has all 400 trainable while FastText has 300 pre-\ntrained and 100 trainable. ELMo uses 1024 dimension em-\nbedding while ELMoL has 200 dimension embedding. EL-\nMoL embedding is reduced to 200 in order to keep the num-\nber of parameters same as NoUT or FastText. This is because\nto linearly combine embedding and LSTM output, the em-\nbedding dimension has to be the same as the forward and\nbackward LSTM output. First 100 dimensions of those em-\nbeddings are word-level trainable embeddings and the rest\nare ﬁxed output of character-level CNN.\nLM Training for ELMo\nTo train the LM for ELMo, we\nfollow Peters et al. (2018) very closely. For internal datasets,\nwe train a CNN-BIG-LSTM LM, halving the embedding\nand hidden layer dimensions, on IUD from scratch for 10\nepochs. As an alternative, we also ﬁne-tune the LM already\ntrained on 1B Word Benchmark on IUD for 5 epochs. The\nLM to be used for embeddings is decided based on perplex-\nity on a heldout set (1% of IUD). For ATIS and SNIPS, we\nuse 1M SLU Benchmark to both train from scratch and ﬁne-\ntune the LM trained on 1B Word Benchmark for 25 and 10\nepochs respectively.\nLM Training for ELMoL\nELMoL language model train-\ning is carried out similar to ELMo. The main difference is\nin the LM architecture being used. Keeping it similar to the\nlower layer of multi-task model, there is only 1 LSTM layer\nwith 200 hidden units. Context independent CNN layer has\n10, 20, 20, 20, 20 and 10 channels of width 1,2,3,4,5 and 6\nrespectively. For internal domains, the model was trained on\nIUD with a batch size of 128 for 25 epochs. For ATIS and\nSNIPS, we train the on 1M SLU Benchmark with batch size\nof 32 for 50 epochs. While using guf, there are two impor-\ntant hyper-parameters, learning rate for ﬁrst round of train-\ning (when lower layer is frozen) which we keep as 0.0001\nfor internal datasets and 0.0005 for public datasets. The sec-\nond important hyper-parameter is the number of epochs after\nwhich lower layer is unfrozen. This is usually between 10-\n15 epochs. The learning rate for second round of training\n(with all layers unfrozen) is decided based on the dev per-\nformance and varies across datasets. We also use discr and\ntlr in second round of training. For discr, we use the setting\nfollowed by Howard and Ruder (2018) and keep the lower\nlayer learning rate 2.5 times lesser than the upper layer. For\ntlr, we keep the starting learning rate 10 times lesser than\nthe peak learning rate and this peak is located at 1/8th of the\ntotal number of updates.\nSimulating Low Resource Data Settings\nWe simulate\nlow resource settings by carrying out experiments on smaller\ntraining sets sampled from all four target datasets (Domain\nA, Domain B, ATIS, SNIPS). Samples of size 100, 200,\n500, 1000, 2000, 5000 and 10000 are drawn from the target\ntraining sets. We then compare the different methods using\nsmaller training sets. These sets are drawn ﬁve times and the\nperformance is averaged out.\nEvaluation Metrics\nWe use three metrics for evaluation:\n• Intent Classiﬁcation Accuracy (ICA): For IC we use accu-\nracy, which is simply the fraction of utterances with cor-\nrectly predicted intent.\n• Entity F1 (EF1): For ET, we use standard F1 score calcu-\nlated by the CoNLL-2003 evaluation script.\n• Sentence Error Rate (SER): We use SER as a metric for\njoint evaluation of IC and ET to reﬂect overall erroneous\noutputs. It is the fraction of utterances with either IC error\nor at least one ET error (lower is better).\n• For LM evaluation, we use the standard perplexity score.\nSigniﬁcance Test\nWe use t-test to establish the statistical\nsigniﬁcance of a method over another (p-value = 0.05).\nLanguage Modeling Results\nTable 3 shows the perplexity numbers for language models\nof ELMo and ELMoL trained on different datasets. The up-\nper part of the table shows performance on Internal Datasets,\nwith LMs trained on IUD, 1BWB and their combination\nand evaluated on held out development sets. Overall, ELMo\nLM achieves the best performance when ﬁrst trained on\n1BWB and then ﬁne-tuned on either IUD or IMSLU. The\nELMoL LM perplexity is higher than ELMo, which is ex-\npected because of the much smaller ELMoL LM architec-\nture. However, we ﬁnd that this difference in perplexity be-\ntween ELMo and ELMoL does not result in a large SLU per-\nformance difference when used in downstream tasks which\nwe discuss in next section.\nInternal Datasets\nIUD\nHoldout\nDomain-A\nDev\nDomain-B\nDev\nELMo\nIUD\n45.2\n19.4\n35.8\n1BWB+\nIUD\n44.9\n19.1\n34.9\nELMoL\nIUD\n56.7\n24.2\n43.2\nPublic Datasets\n1MSLU\nHoldout\nATIS\nDev\nSNIPS\nDev\nELMo\n1MSLU\n38.0\n16.6\n60.7\n1BWB+\n1MSLU\n31.0\n15.2\n50.8\nELMoL\n1MSLU\n40.6\n17.7\n60.2\nTable 3: Perplexity of ELMo and ELMoL language model\ntrained on different datasets.\nUT Results for Internal SLU tasks\nThe results of Unsupervised Transfer Learning (UT) on in-\nternal datasets, Domain A and Domain B are shown in 4.\nBoth ELMo and ELMoL beat the NoUT and FastText base-\nline signiﬁcantly on both Domain A and Domain B (p<0.05,\nwhen comparing ELMo and ELMoL vs NoUT and Fast-\nText). ELMo gives a performance improvement of ∼2 ab-\nsolute SER points on Domain A and ∼1.5 absolute SER\npoints on Domain B over Fasttext. With much smaller archi-\ntecture and 1.6x faster inference, ELMoL does not perform\nsigniﬁcantly worse compared to ELMo for three out of the\nfour datasets we examined (ATIS, Domains A and B) except\nSNIPS where ELMo signiﬁcantly outperforms ELMoL. It is\nshort of ELMo by just ∼0.3 SER points on Domain A and\n∼only 0.5 SER points on Domain B.\nIn Figures 3b and 3e, we plot the SER performance as\na function of training data size using smaller training sets\nfor Domain A and Domain B. In both plots, we observe\nthe trend that SER performance using 10000 samples with\nNoUT approximately matches performance using 5000 sam-\nples with Fasttext, using 2000 samples with ELMoL and\nusing 1000 samples with ELMo. Hence, unsupervised pre-\ntraining reduces the number of labeled target samples re-\nquired for achieving same level of performance as Fasttext\nby 5 times using ELMo and 3 times using ELMoL. This en-\nables building accurate models for a new SLU domain faster\nby reducing labeling time and effort through leveraging UT.\nDomain-A\nDomain-B\nICA\nEF1\nSER\nICA\nEF1\nSER\nNoUT\n91.96\n76.58\n39.79\n91.68\n74.70\n30.93\nFasttext\n92.34\n77.74\n38.76\n92.27\n76.59\n29.30\nELMo\n93.71\n78.59\n36.58\n92.46\n78.43\n27.95\nELMoL\n93.23\n78.23\n36.86\n92.35\n78.34\n28.42\nTable 4: Performance of UT methods on internal datasets.\nTraining set sizes are 43K and 100K for domains A and B.\nUT Results for Benchmark SLU Tasks\nTable 5 shows the performance of Unsupervised Transfer\non ATIS and SNIPS and Figure 3a and 3d plot the SER\nnumbers using smaller training sets for these two datasets\n(low resource simulation). Similar to the trends we observed\non internal datasets, we notice that both ELMo and EL-\nMoL signiﬁcantly outperform the baselines (p<0.05, when\ncomparing ELMo and ELMoL vs NoUT and FastText) and\nELMoL’s performance is only slightly lower than that of\nELMo. State-of-the-art (SOTA) for each of the two datasets\nis presented in the last row. For ATIS, Liu and Lane (2016)\nachieve the SOTA by using more complex attention-based\nmodels without transfer learning. We are able to get close\nto their accuracy with simpler models using UT. For SNIPS,\nwe outperform the SOTA performance (Coucke et al. 2017).\nATIS\nSNIPS\nICA\nEF1\nSER\nICA\nEF1\nSER\nNoUT\n95.41\n94.30\n17.13\n98.43\n88.78\n24.14\nFasttext\n96.75\n95.35\n15.01\n98.57\n91.78\n19.57\nELMo\n97.42\n95.62\n12.65\n99.29\n93.90\n14.57\nELMoL\n97.30\n95.42\n13.38\n98.83\n93.29\n15.62\nSOTA\n98.43\n95.87\n-\n-\n93\n-\nTable 5: Performance of UT methods on benchmark\ndatasets. SOTA indicates state-of-the-art referring to Liu and\nLane (2016) for ATIS and Coucke et al. (2017) for SNIPS.\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 3: Performance of different methods of UT using varying amounts of training data on ATIS (a), SNIPS (d), Domain-A\n(b), Domain-B (e). Performance on Domain A and B using UT+ST is presented in (c) and (f). The y-scale of (b) and (e) is kept\nsame as (c) and (f) respectively for comparison. Best viewed in color.\nELMoL Training: Ablation Study\nIn Table 6, we evaluate the gains provided by various train-\ning strategies employed for ELMoL to prevent catastrophic\nforgetting. Vanilla refers to the base ELMoL method, while\nguf, discr and tlr are training techniques we described in\nSection . Improvements from each of these techniques are\nconsistent across datasets.\nATIS\nSNIPS\nDomain-A\nDomain-B\nvanilla\n14.46\n17.14\n37.59\n28.78\nguf\n13.79\n16.39\n37.26\n28.72\nguf+\ndiscr+tlr\n13.38\n15.62\n36.86\n28.42\nTable 6: Performance of training techniques in ELMoL. All\nthe above numbers are SER scores.\nUT+ST Results for SLU\nTable 7 shows the performance when combining each of the\nUT methods presented earlier along with supervised transfer\n(ST). Comparing with the performance of Table 4, we see a\nconsistent improvement of all UT methods when combining\nwith ST. Speciﬁcally, ELMo+ST achieves SER 35.86 and\n27.46 compared to 36.58 and 27.95 for ELMo for Domains\nA and B respectively. Similarly, ELMoL+ST achieves SER\n36.38 and 28.11 compared to 36.86 and 28.42 for ELMoL\nfor Domains A and B respectively. This indicates that gains\nfrom UT and ST are additive, and transferring knowledge\nfrom all available data, both labeled and unlabeled, is bene-\nﬁcial for the ﬁnal SLU accuracy.\nDomain-A\nDomain-B\nICA\nEF1\nSER\nICA\nEF1\nSER\nNoUT+ST\n92.80\n76.85\n38.97\n91.57\n75.53\n30.32\nFasttext+ST\n93.15\n77.27\n37.58\n92.36\n76.73\n28.95\nELMo+ST\n93.79\n78.76\n35.86\n92.88\n78.49\n27.46\nELMoL+ST\n93.53\n78.56\n36.38\n92.84\n77.03\n28.11\nTable 7: Performance when ST is combined with UT.\nELMo vs ELMoL: Performance Comparison\nThe proposed ELMoL model is faster in terms of both train-\ning time and inference time, and smaller in total model\nsize compared to standard ELMo. Assuming ELMoL takes\n1 unit of time, ELMo would take 1.8x more time to train\nand 1.6x more time during inference. Fast training and in-\nference times are critical in deployed SLU systems as they\nenable rapid domain development and timely response to\nthe users request during runtime. Also, ELMoL requires 4x\nlesser memory than ELMo (37 million parameters in EL-\nMoL compared to 140 million in model with ELMo), which\nis advantageous for SLU systems deployed in embedded de-\nvices or other resource constrained systems.\nConclusions and Future Work\nWe describe unsupervised transfer (UT) learning techniques\nfor leveraging unlabeled text data for SLU tasks. Speciﬁ-\ncally, we showed that the recently proposed ELMo embed-\ndings (Peters et al. 2018) improve IC and ET accuracy in\nmulti-task setting. We also proposed a light-weight alterna-\ntive to ELMo, called ELMo-Light (ELMoL). We showed\nthat ELMoL performance is comparable to ELMo, and it\nis faster at runtime and better suited for practical SLU sys-\ntems. Our UT techniques achieved large gains over using\nonly labeled in-domain data for training, with gains being\nmore pronounced for low resource settings. Finally, we also\nshowed that gains from ST and UT are additive. In future,\nwe plan to apply the transfer techniques across different lan-\nguages. We would also like to experiment with alternative\narchitectures such as transformer and adversarial networks.\nReferences\nBojanowski, P.; Grave, E.; Joulin, A.; and Mikolov, T. 2017.\nEnriching word vectors with subword information. Trans-\nactions of the Association of Computational Linguistics\n(TACL) 5(1):135–146.\nCer, D.; Yang, Y.; Kong, S.-Y.; Hua, N.; Limtiaco, N.; John,\nR. S.; Constant, N.; Guajardo-Cespedes, M.; Yuan, S.; Tar,\nC.; Sung, Y.-H.; Strope, B.; and Kurzweil, R. 2018. Univer-\nsal sentence encoder. arXiv preprint arXiv:1803.11175.\nChelba, C.; Mikolov, T.; Schuster, M.; Ge, Q.; Brants, T.;\nKoehn, P.; and Robinson, T. 2014. One billion word bench-\nmark for measuring progress in statistical language model-\ning. In Interspeech.\nChiu, J. P., and Nichols, E. 2016. Named entity recognition\nwith bidirectional lstm-cnns. Transactions of the Associa-\ntion for Computational Linguistics (TACL) 4:357–370.\nCollobert, R., and Weston, J. 2008. A uniﬁed architecture\nfor natural language processing: Deep neural networks with\nmultitask learning. In International Conference of Machine\nLearning (ICML).\nCoucke, A.; Ball, A.; Delpuech, C.; Doumouro, C.; Ray-\nbaud, S.; Gisselbrecht, T.; and Dureau, J.\n2017.\nBench-\nmarking natural language understanding systems: Google,\nfacebook, microsoft, amazon and snips.\nDai, A. M., and Le, Q. V. 2015. Semi-supervised sequence\nlearning. In Neural Information Processing Systems (NIPS).\nDonahue, J.; Jia, Y.; Vinyals, O.; Hoffman, J.; Zhang, N.;\nTzeng, E.; and Darrell, T. 2014. Decaf: A deep convolu-\ntional activation feature for generic visual recognition. In\nInternational Conference on Machine Learning (ICML).\nGoyal, A.; Metallinou, A.; and Matsoukas, S. 2018. Fast and\nscalable expansion of natural language understanding func-\ntionality for intelligent agents. In North American Chapter\nof the Association for Computational Linguistics: Human\nLanguage Technologies (NAACL-HLT).\nHemphill, C. T.; Godfrey, J. J.; and Doddington, G. R. 1990.\nThe atis spoken language systems pilot corpus. In Speech\nand Natural Language Workshop.\nHenderson, M.; Thomson, B.; and Williams, J. D. 2014. The\nsecond dialog state tracking challenge. In Special Interest\nGroup on Discourse and Dialogue (SIGDIAL).\nHoward, J., and Ruder, S. 2018. Universal language model\nﬁne-tuning for text classiﬁcation. In Association for Com-\nputational Linguistics(ACL).\nJozefowicz, R.; Vinyals, O.; Schuster, M.; Shazeer, N.; and\nWu, Y. 2016. Exploring the limits of language modeling.\narXiv preprint arXiv:1602.02410.\nKim, J.-K.; Kim, Y.-B.; Sarikaya, R.; and Fosler-Lussier, E.\n2017. Cross-lingual transfer learning for pos tagging with-\nout cross-lingual resources. In Empirical Methods in Natu-\nral Language Processing (EMNLP).\nKinmga, D., and Ba, J. 2015. A method for stochastic op-\ntimization. In International Conference on Learning Repre-\nsentations (ICLR).\nKiros, R.; Zhu, Y.; Salakhutdinov, R. R.; Zemel, R.; Urtasun,\nR.; Torralba, A.; and Fidler, S. 2015. Skip-thought vectors.\nIn Neural Information Processing Systems (NIPS).\nKrizhevsky, A.; Sutskever, I.; and Hinton, G. E.\n2012.\nImagenet classiﬁcation with deep convolutional neural net-\nworks. In Neural Information Processing Systems (NIPS).\nLiu, B., and Lane, I. 2016. Attention-based recurrent neural\nnetwork models for joint intent detection and slot ﬁlling. In\nInterspeech.\nLiu, P.; Qiu, X.; Chen, X.; Wu, S.; and Huang., X. 2015.\nMulti-timescale long short-term memory neural network for\nmodelling sentences and documents. In Empirical Methods\nfor Natural Language Processing (EMNLP).\nLiu, L.; Shang, J.; Xu, F.; Ren, X.; Gui, H.; Peng, J.; and\nHan, J. 2018. Empower sequence labeling with task-aware\nneural language model. In Association for the Advancement\nof Artiﬁcial Intelligence (AAAI).\nMikolov, T.; Sutskever, I.; Chen, K.; Corrado, G.; and Dean,\nJ. 2013. Distributed representations of words and phrases\nand their compositionality. In Neural Information Process-\ning Systems (NIPS).\nPeters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark,\nC.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized\nword representations. In North American Chapter of the As-\nsociation for Computational Linguistics: Human Language\nTechnologies (NAACL-HLT).\nRadford, A.; Narasimhan, K.; Salimans, T.; and Sutskever,\nI. 2018. Improving language understanding by generative\npre-training. Preprint.\nSharif Razavian, A.; Azizpour, H.; Sullivan, J.; and Carls-\nson, S.\n2014.\nCnn features off-the-shelf: an astounding\nbaseline for recognition. In Computer Vision and Pattern\nRecognition (CVPR) Workshop.\nSocher, R.; Perelygin, A.; Wu, J. Y.; Chuang, J.; Manning,\nC. D.; Ng, A. Y.; and Potts, C.\n2013.\nRecursive deep\nmodels for semantic compositionality over a sentiment tree-\nbank. In Empirical Methods for Natural Language Process-\ning (EMNLP).\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2018-11-13",
  "updated": "2018-11-13"
}