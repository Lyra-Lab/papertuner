{
  "id": "http://arxiv.org/abs/1811.06622v1",
  "title": "Concept-Oriented Deep Learning: Generative Concept Representations",
  "authors": [
    "Daniel T. Chang"
  ],
  "abstract": "Generative concept representations have three major advantages over\ndiscriminative ones: they can represent uncertainty, they support integration\nof learning and reasoning, and they are good for unsupervised and\nsemi-supervised learning. We discuss probabilistic and generative deep\nlearning, which generative concept representations are based on, and the use of\nvariational autoencoders and generative adversarial networks for learning\ngenerative concept representations, particularly for concepts whose data are\nsequences, structured data or graphs.",
  "text": "Concept-Oriented Deep Learning: Generative Concept Representations \n \nDaniel T. Chang (Âº†ÈÅµ) \n \nIBM (Retired) dtchang43@gmail.com \nAbstract:  Generative concept representations have three major advantages over discriminative ones: they can represent \nuncertainty, they support integration of learning and reasoning, and they are good for unsupervised and semi-supervised \nlearning. We discuss probabilistic and generative deep learning, which generative concept representations are based on, and \nthe use of variational autoencoders and generative adversarial networks for learning generative concept representations, \nparticularly for concepts whose data are sequences, structured data or graphs.  \n1 Introduction \nConcept-oriented deep learning (CODL) [1] extends deep learning with concept representations and conceptual \nunderstanding capability. The concept representations discussed in [1] are deterministic and discriminative in nature. In this \npaper we discuss generative concept representations, which are probabilistic and generative in nature, for CODL. Generative \nconcept representations have three major advantages: they can represent uncertainty, they support integration of deep \nlearning and reasoning, and they are good for unsupervised and semi-supervised learning. \n1.1 Uncertainty \nUncertainty [2] arises because of limitations in our ability to observe the world, limitations in our ability to model it, and \npossibly inherent non-determinism (e.g., quantum phenomena). From the perspective of deep learning, there are two types of \nuncertainty: model uncertainty and data uncertainty [3, 4]. Model uncertainty accounts for uncertainty in the model structure \nand model parameters. This is important to consider for safety-critical applications and small datasets. Data uncertainty \naccounts for out-of-distribution data and noisy data. This is important to consider for real-time applications and large \ndatasets.  \nUncertainty, therefore, is an inescapable aspect of most real-world data-driven applications [2]. Because of this, we need \nto allow our learning system to represent uncertainty and our reasoning system to consider different possibilities due to \nuncertainty. Further, to obtain meaningful conclusions, we need to reason not just about what is possible, but also about what \nis probable. Probability theory provides us with a formal framework for representing uncertainty and for considering multiple \npossible outcomes and their likelihood. \n2 \n \n1.2 Integration of Deep Learning and Reasoning \nDeep learning has achieved significant success in many perception and learning tasks. However, a real AI system must \nadditionally possess the ability of reasoning to be useful. To achieve integrated intelligence that involves perception, learning \nand reasoning, it is desirable to tightly integrate deep learning (for learning and perception tasks) and probabilistic graphical \nmodels (for reasoning tasks) within a principled probabilistic framework [5].  \nTo ensure efficient and effective information exchange between the learning and perception component and the \nreasoning component, a natural way is to represent the learning and perception component as a probabilistic graphical model \nand seamlessly connect it to the reasoning probabilistic graphical model. Such integration has been successfully used for \nmolecular design [6].  \n1.3 Unsupervised and Semi-supervised Learning \nA shortcoming of the current state of the art of supervised deep learning is that it requires large amounts of labeled data \nto achieve good accuracy. Reducing the amount of labeled data necessary for deep learning to work well and be applicable \nacross a broad range of tasks requires unsupervised learning or semi-supervised learning. However, a central cause of the \ndifficulties with unsupervised and semi-supervised learning is the high dimensionality of the random variables to be \nconsidered. This brings two distinct challenges [7]: a statistical challenge and a computational challenge. The statistical \nchallenge regards generalization; the computational challenge arises because many algorithms involve intractable \ncomputations. \n Deep generative models have been proposed to avoid these intractable computations by design [7]. \n2 Probabilistic and Generative Deep Learning \nWhereas concept representations are based on traditional deep learning, which is deterministic and discriminative in \nnature, generative concept representations are based instead on probabilistic and generative deep learning, which is \ndiscussed in this section. \n2.1 Probabilistic Machine Learning \nThe key idea behind probabilistic machine learning [8] is that learning can be thought of as inferring probabilistic \nmodels to explain observed data. A machine can use such models to make predictions about future data, and take actions that \n3 \n \nare rational given these predictions. Uncertainty plays a fundamental part in all of this and probability theory provides a \nframework for modeling uncertainty, as mentioned previously. \nThe probabilistic approach to modeling uses probability theory to represent all forms of uncertainty: \n- Probability distributions are used to represent all uncertain elements in a model (including structural, parametric, out-\nof-distribution and noise-related) and how they relate to the data. \n- The basic rules of probability theory are used to infer the uncertain elements given the observed data. \n- Learning from data occurs through the transformation of the prior distributions (defined before observing the data) into \nposterior distributions (after observing the data). The application of probability theory to learning from data is called \nBayesian learning [8, 9]. \nSimple probability distributions over single or a few random variables can be composed to form the building blocks of \nlarger, more complex models. The compositionality of probabilistic models means that the behavior of these building blocks \nin the context of the larger model is often much easier to understand. The dominant paradigm for representing such \ncompositional probabilistic models is probabilistic graphical models, which will be discussed in the next subsection. \nThere are two main challenges [8] for probabilistic machine learning. The main modeling challenge is that the model \nshould be flexible enough to capture all the properties of the data required to achieve the prediction task of interest. (The key \nstatistical concept underlying flexible models that grow in complexity with the data is non-parametrics [9].) Computationally, \nthe main challenge is that learning involves marginalizing (summing out) all the variables in the model except for the \nvariables of interest. Such high-dimensional sums and integrals are generally computationally hard. \n2.2 Probabilistic Graphical Models \nProbabilistic graphical models (PGMs) [2, 7, 10] are structured probabilistic models. A PGM is a way of describing a \nprobability distribution of random variables, using a graph to describe which variables in the probability distribution directly \ninteract with each other, with each node represents a variable and each edge represents a direct interaction. This allows the \nmodels to have significantly fewer parameters which can in turn be estimated reliably from less data. These smaller models \nalso have dramatically reduced computational cost in terms of storing the model, performing inference in the model, and \ndrawing samples from the model. \n4 \n \nUsually a PGM comes with both a graphical representation of the model and a generative process to depict how the \nrandom variables are generated. Due to its Bayesian nature, PGM is easy to extend to incorporate other information or to \nperform other tasks. \nDirected PGMs \nThere are essentially two types of PGM [2]: directed PGM (also known as Bayesian networks) and undirected PGM \n(also known as Markov random fields). Directed PGMs are based on directed graphs (the direction of the arrow indicating \nwhich variable‚Äôs probability distribution is defined in terms of the other‚Äôs) and are useful because both the structure and the \nparameters provide a natural representation for many types of real-world domains. Undirected PGMs, on the other hand, are \nuseful in modeling a variety of phenomena where one cannot naturally ascribe directionality to the interaction between \nvariables. We focus on directed PGMs. \nA directed PGM defined on random variables x is defined [7] by a directed acyclic graph G whose vertices are the \nrandom variables in the model, and a set of local conditional probability distributions p(xi | PaG(xi)) where PaG(xi) gives the \nparents of xi in G. The probability distribution over x is given by \np(x) = Œ†i p(xi | PaG(xi)). \nSo long as each variable has few parents in the graph, the distribution can be represented with very few parameters. \nSome restrictions on the graph structure, such as requiring it to be a tree, can also guarantee that operations like computing \nmarginal or conditional distributions over subsets of variables are efficient. The directed PGM syntax does not place any \nconstraint on how we define our conditional distributions. It only defines which variables they are allowed to take in as \narguments. \nLatent Variables \nA good PGM needs to accurately capture the distribution over the observed variables x. Often the different elements of x \nare highly dependent on each other. The approach most commonly used to model these dependencies is to introduce several \nlatent variables z [7]. The model can then capture dependencies between any pair of variables xi and xj indirectly, via direct \ndependencies between xi and z, and direct dependencies between z and xj. \n5 \n \nLatent variables have advantages beyond their role in efficiently capturing p(x). The latent variables z also provide an \nalternative representation for x. Many approaches accomplish representation learning by learning latent variables. \nThe Deep Learning Approach \nThe deep learning approach [7] tends to use different model structures, learning algorithms and inference procedures \nthan are commonly used by traditional probabilistic graphical modeling. Deep learning PGMs typically have more latent \nvariables than observed variables. Complicated nonlinear interactions between observed variables are accomplished via \nindirect connections that flow through multiple latent variables. Also, the deep learning approach does not intend for the \nlatent variables to take on any specific semantics ahead of time‚Äîthe training algorithm is free to invent the ‚Äúconcepts‚Äù it \nneeds to model a particular dataset. The latent variables are usually not very easy to interpret. \nIn the context of PGM, we can define the depth of a model in terms of the graphical model depth rather than the \ncomputational graph depth. We can think of a latent variable zi as being at depth j if the shortest path from zi to an observed \nvariable is j steps. We usually describe the depth of the model as being the greatest depth of any such zi. We focus on deep \nPGMs, which are PGMs with multiple layers of latent variables. \n2.3 Deep Generative Models \nUnsupervised and semi-supervised learning methods are often based on generative models [11-13], which are \nprobabilistic models that express hypotheses about the way in which data may have been generated. PGMs have emerged as a \nbroadly useful approach to specifying generative models. The elegant marriage of graph theory and probability theory makes \nit possible to take a fully probabilistic approach to unsupervised and semi-supervised learning in which efficient algorithms \nare available to update a prior generative model into a posterior generative model once data have been observed. \nAll of the generative models [7] represent probability distributions over multiple variables in some way. Some allow the \nprobability distribution function to be evaluated explicitly. Others do not allow the evaluation of the probability distribution \nfunction, but support operations that require knowledge of it, such as drawing samples from the distribution. Some of these \nmodels are PGMs. We focus on deep generative models (DGMs), which are deep PGMs and/or employ deep neural networks \nfor parameterizing the models. \nPrescribed and Implicit DGMs \n6 \n \nAs mentioned above, there are two types of DGMs [12]: prescribed DGMs and implicit DGMs. Prescribed DGMs are \nthose that provide an explicit parametric specification of the probability distribution of the observed variable x, specifying the \nlikelihood function pŒ∏(x) with parameter Œ∏. Implicit DGMs are defined by a stochastic procedure (a simulation process) that \ngenerates data - we can sample data from its generative process, but we may not have access to calculate its probability \ndistribution, and thus are referred to as likelihood-free. \n2.4 Differentiable Generator Networks \nMany DGMs are based on the idea of using a differentiable generator network [7]. The model transforms samples of \nlatent variables z to observed variables x or to distributions over x using a differentiable function g(z; Œ∏(g)) which is typically \nrepresented by a neural network. This model class includes \n- variational autoencoders which pair the differentiable generator network with an inference network, and \n- generative adversarial networks which pair the differentiable generator network with a discriminator network. \nIn each case, the paired networks are jointly trained. \nGenerator networks are essentially parameterized computational procedures for generating samples, where the \narchitecture provides the family of possible distributions to sample from and the parameters select a distribution from within \nthat family. To generate samples from complicated distributions that are difficult to specify directly, difficult to integrate \nover, or whose resulting integrals are difficult to invert, one uses a neural network to represent a parametric family of \nnonlinear functions g(z; Œ∏(g)), and uses training data to infer the parameters selecting the desired function. \nApproaches based on differentiable generator networks are motivated by the success of gradient descent applied to \ndifferentiable neural networks for classification and regression. However, generative modeling is more difficult than \nclassification or regression because the learning process requires optimizing intractable criteria. In the case of generative \nmodeling, the learning procedure needs to determine how to arrange z space in a useful way and additionally how to map \nfrom z to x. \n7 \n \n2.5 Generative Concept Representations \nWe use DGMs for generative concept representations. Generative concept representations are PGMs and, therefore, can \nrepresent uncertainty as well as support integration of deep learning and reasoning. Further, generative concept \nrepresentations are generative models and, therefore, are good for unsupervised and semi-supervised learning. \nIn the next two sections, we discuss learning generative concept representations using variational autoencoders and \ngenerative adversarial networks, two widely-used architectures for DGMs, particularly for concepts whose data are \nsequences, structured data or graphs. \n3 Variational Autoencoders for Generative Concept Representations \n3.1 Autoencoders \nAn autoencoder [7] is a neural network that is trained to copy its input to its output. The network may be viewed as \nconsisting of two parts: an encoder function h = f(x) and a decoder that produces a reconstruction x = g(h). Usually they are \nrestricted in ways that allow them to copy only approximately. An autoencoder whose code dimension is less than the input \ndimension is called undercomplete. Learning an undercomplete representation forces the autoencoder to capture the most \nsalient features of the training data. \nModern autoencoders have generalized to stochastic mappings pencoder(h | x) and pdecoder(x | h). Theoretical connections \nbetween autoencoders and latent variable models have brought autoencoders to the forefront of generative modeling, as \ndiscussed next. \n3.2 Variational Autoencoders \nThe variational autoencoder (VAE) [7, 11-16] consists of an encoder network and a decoder network which encodes a \ndata example to a latent representation and generates samples from the latent space, respectively. The decoder network is a \ndifferentiable generator network and the encoder network is an auxiliary inference network. Both networks are jointly trained \nusing variational learning. Variational learning is mainly applied to prescribed DGMs and uses the variational lower bound \nof the marginal log-likelihood as the single objective function to optimize both the generator network and the auxiliary \ninference network. \nThe likelihood function of a prescribed DGM can be written as [12]: \n8 \n \npŒ∏(x) ‚â° pŒ∏(z) pŒ∏(x | z) \nIt is usually intractable to directly evaluate and maximize the marginal log-likelihood log(pŒ∏(x)). Following the variational \ninference approach, one introduces an auxiliary inference model qœÜ(z | x) with parameters œÜ, which serves as an \napproximation to the exact posterior pŒ∏(z | x). \nThe variational lower bound can then be derived using the Jenson Inequality: \nlog(pŒ∏(x)) = log(‚àëpŒ∏(ùê±, ùê≥)\n‡Ø≠\n) = log(‚àëqœÜ(ùê≥ | ùê±)\n‡Ø≠\n \npŒ∏(ùê±,ùê≥)\nqœÜ(ùê≥ | ùê±)) ‚â• ‚àëqœÜ(ùê≥ | ùê±)\n‡Ø≠\n log( pŒ∏(ùê±,ùê≥)\nqœÜ(ùê≥ | ùê±)) ‚â° L(x; Œ∏, œÜ) \nIt can further be rewritten as [7, 12]: \n \nL(x; Œ∏, œÜ) = ‚àëqœÜ(ùê≥ | ùê±)\n‡Ø≠\nlog(pŒ∏(x | z)) ‚Äì DKL(qœÜ(z | x) || pŒ∏(z)) \nThe first term is the expected reconstruction quality, requiring that pŒ∏(x | z) is high for samples of z from qœÜ(z | x), while the \nsecond term (the KL divergence between the approximate posterior and the prior) acts as a regularizer, ensuring we can \ngenerate realistic data by sampling latent variables from pŒ∏(z). \n \nVariational learning is to maximize the variational lower bound over the training data. It performs something like the \nautoencoder, with qœÜ(z | x) as the encoder and pŒ∏(x | z) as the decoder. As such, the VAE introduces the constraint on the \nautoencoder that the latent variable z is distributed according to a prior p(z). The encoder qœÜ(z | x) approximates the posterior \np(z | x), and the decoder pŒ∏(x | z) parameterizes the likelihood p(x | z). The generation model is then z ~ p(z); x ~ p(x | z). \nThe VAE framework is very straightforward to extend to a wide range of model architectures. This is a key advantage. \nVAEs also work very well with a diverse family of differentiable operators. \nConcept representations (latent representations) learned using VAE generally are explicit, continuous and with \nmeaningful structure. As such, they can be directly manipulated to generate new concepts with desired attributes. An example \nis given in [11] in which concept vectors are used to edit images. Further examples will be mentioned later. \n9 \n \n3.3 Sequence VAEs \n \nSequence data include text (understood as sequences of words), timeseries, molecular structures (represented as SMILES \nstrings), and music (transcribed in MIDI files). They are generally learned using recurrent neural networks. A recurrent \nneural network (RNN) [11, 19] processes sequences by iterating through the sequence elements and maintaining a state \ncontaining information relative to what it has seen so far. The state of the RNN is reset between processing two different, \nindependent sequences. \nLearning with RNNs can be especially challenging due to the difficulty of learning long-term dependencies. The \nproblems of vanishing and exploding gradients occur when back-propagating errors across many time steps. To resolve the \nproblems, the Long-Short Term Memory (LSTM) architecture [19] introduces the memory cell, a unit of computation that \nreplaces traditional nodes in the hidden layer of an RNN. Each memory cell contains a node with a self-connected recurrent \nedge of fixed weight one, ensuring that the gradient can pass across many time steps without vanishing or exploding. \nThe CVAE [6, 20-21] adapts the VAE to sequence by using single-layer LSTM RNNs for both the encoder and the \ndecoder. Concept representations learned using the CVAE contain latent representations of entire sequences and can be used \nto generate coherent new sequences that interpolate between known sequences. Example usage can be found for text [20], \nmolecular structures [6] and music [21].  \nThere are two main issues with the CVAE. First, the decoder, being an RNN, is itself sufficiently powerful to produce an \neffective model of the data, and therefore the decoder can completely ignore the latent variables. Second, the model must \ncompress the entire sequence to a single latent vector. This begins to fall apart as the sequence length increases. These issues \ncan be overcome by using a hierarchical RNN for the decoder, as done in MusicVAE [21]. \n3.4 Grammar VAEs \nStructured data such as symbolic expressions, computer programs, and molecular structures (represented as SMILES \nstrings) have syntax and semantic formalisms. For such data, sequence VAEs will often lead to invalid outputs because of the \nlack of formalization of syntax and semantics serving as the constraints of the structured data.  \nContext-free grammars can be used to incorporate syntax constraints in the generative model. To do so, structured data \nare represented as parse trees from a context-free grammar. The grammar variational autoencoder (GVAE) [24] encodes and \ndecodes directly from and to these parse trees, respectively, ensuring the generated outputs are always valid based on the \n10 \n \ngrammar. It also learns a more coherent latent space in which nearby points decode to similar outputs. GVAE has been \napplied to symbolic regression and molecular synthesis. Although the context-free grammar provides a mechanism for \ngenerating syntactically valid outputs, it is incapable to constraint the model for generating semantically valid outputs. \nAttribute grammars or syntax-directed definitions allow one to attach semantics to a parse tree generated by context-free \ngrammar. The syntax-direct variational autoencoder (SD-VAE) [25] incorporates attribute grammar in the VAE such that it \naddresses both syntactic and semantic constraints and generates outputs that are syntactically valid and semantically \nreasonable. SD-VAE has been applied to reconstruction and optimization of computer programs and molecules. \n3.5 Graph VAEs \nGraph data include knowledge graphs (relations between entities), social networks (relations between people), physical \nnetworks (relations between objects), and molecular graphs (representing molecular structures). The Variational Graph \nAutoencoder (VGAE) [27] is a framework for learning graph data. The encoder is a graph convolutional network and the \ndecoder is an inner product decoder. It makes use of latent variables and is capable of learning interpretable latent \nrepresentations for undirected graphs. \nThe Adversarially Regularized Variational Graph Autoencoder (ARVGA) [28] is an adversarial embedding framework \nfor graph data. The framework extends the above-mentioned graph convolutional network to encode the topological structure \nand node content in a graph to a latent representation, on which a decoder is trained to reconstruct the graph structure. \nFurthermore, the latent representation is enforced to match a prior distribution via an adversarial training scheme. The \nvariational graph encoder learning and adversarial regularization are jointly optimized in a unified framework so that each \ncan be beneficial to the other and lead to a better graph embedding. \n4 Generative Adversarial Networks for Generative Concept Representations \nThe generative adversarial network (GAN) [7, 11-13, 17-18] consists of a differentiable generator network and an \nauxiliary discriminator network which generates samples from latent representations and discriminates between data samples \nand generated samples, respectively. Both networks are jointly trained using adversarial learning. Adversarial learning is \nmainly applied to implicit DGMs and optimizes the generator network so that the generated samples are statistically almost \nindistinguishable from the data samples. \n11 \n \nAdversarial learning is based on a game theoretic scenario in which the generator network competes against an adversary \n[7]: \n- \nThe generator network produces samples x = g(z; Œ∏(g)). \n- \nIts adversary, the discriminator network, attempts to distinguish between samples drawn from the training data and \nsamples generated by the generator network. The discriminator emits a probability value given by d(x; Œ∏(d)), \nindicating the probability that x is a real data sample rather than a generated sample. \nThe main motivation for the design of GANs is that the learning process requires neither approximate inference nor \napproximation of a partition function gradient. Unfortunately, learning in GANs can be difficult in practice. Also, \nstabilization of GAN learning remains an open problem. \nConcept representations (latent representations) learned using GAN generally are implicit, discrete and without \nmeaningful structure. As such, they may not be suited for certain applications. \n4.1 Sequence GANs \nApplying GAN to sequence data (see 3.3 Sequence VAEs) has two problems. First, the generated data from the \ngenerator network is based on discrete tokens. The ‚Äúslight change‚Äù guidance from the discriminator network makes little \nsense because there is probably no corresponding token for such slight change. Second, GAN can only give the score for an \nentire sequence when it has been generated; for a partially generated sequence, it is non-trivial to balance its current score and \nthe future one once the entire sequence has been generated. \n The sequence generation framework (SeqGAN) [22] solves the problems by treating the generator network as an agent \nof reinforcement learning; the state is the generated tokens so far and the action is the next token to be generated. To give the \nreward, it employs the discriminator network to evaluate the sequence and feedback the evaluation to guide the learning of \nthe generator network. It models the generator network as a stochastic parameterized policy and directly trains the policy via \npolicy gradient, which avoids the differentiation difficulty for discrete data in a conventional GAN. SeqGAN has been \napplied to poem, speech language and music generation. \nA common problem in reinforcement learning is sparse reward, that the non-zero reward is only observed at the last time \nstep. To deal with sparse reward, the SeqGAN is trained with a stepwise evaluation method, Monte Carlo tree search, which \n12 \n \nstabilizes the training but is computationally intractable when dealing with large dataset. To improve the computational costs, \nthe stepwise GAN (StepGAN) [23] uses an alternative stepwise evaluation method to replace Monte Carlo tree search, which \nautomatically assign scores quantifying the goodness of each subsequence at every generation step. \n4.2 Grammar GANs \nFor structured data (see 3.4 Grammar VAEs), as is the case of sequence VAEs, sequence GANs will often lead to \ninvalid outputs because of the lack of formalization of syntax and semantics serving as the constraints of the structured data. \nContext-free grammars can be used to incorporate syntax constraints in the generative model. The TreeGAN [26] incorporate \na given context-free grammar into the sequence generation process. In TreeGAN, the generator network employs an RNN to \nconstruct a parse tree. The discriminator network uses a tree-structured RNN to distinguish the generated parse trees from \nsample parse trees. Although the context-free grammar provides a mechanism for generating syntactically valid outputs, as is \nthe case of GVAE, it is incapable to constraint the model for generating semantically valid outputs. \nIn principle, attribute grammars, instead of context-free grammars, could be incorporated in the GAN to generate \noutputs that are syntactically valid and semantically reasonable, as in SD-VAE. This awaits future work. \n4.3 Graph GANs \nFor graph data (see 3.5 Graph VAEs) we can denote a given graph as G = (V; E), where V = {v1, ‚Ä¶, vV} represents \nthe set of vertices (or nodes) and E = {eij}V\ni;j=1 represents the set of edges. For a given vertex vc, we define N(vc) as the set of \nvertices directly connected to vc. We further denote the underlying connectivity distribution for vertex vc as conditional \nprobability p(v | vc). N(vc) can then be seen as a set of observed samples drawn from p(v | vc).  \nThe GraphGAN [29] is a graph representation learning framework using the node embeddings approach. In GraphGAN, \nadversarial learning is based on the following scenario in which the generator network competes against an adversary: \n- \nThe generator network g(v | vc; Œ∏(g)) tries to approximate the underlying connectivity distribution p(v | vc) and \ngenerates the most likely vertices to be connected with vc from the vertex set V. (A graph softmax is used to \novercome the limitations of the traditional softmax.) \n- \nThe discriminator network D(v, vc; Œ∏(d)) aims to discriminate the connectivity for the vertex pair (v, vc). It outputs a \nsingle scalar representing the probability of an edge existing between v and vc. \n13 \n \nUsing node embedding approaches for generating entire graphs, however, can produce samples that don‚Äôt preserve any of the \npatterns inherent to real graphs.  \nThe NetGAN [30] architecture has the associated goals of learning from a single graph, generating discrete outputs, and \nstaying invariant under node reordering.  It generates graphs via random walks. NetGAN consists of two main components - \na generator network and a discriminator network. The generator network is defined as a stochastic neural network with \ndiscrete outputs, whose goal is to generate synthetic random walks that are plausible in the input graph. At the same time, the \ndiscriminator network learns to distinguish the synthetic random walks from the real ones that come from the input graph. \nNetGAN preserves important topological properties, without having to explicitly specifying them in the model definition. \nMoreover, it can be used for generating new graphs with continuously varying characteristics using latent space \ninterpolation. \n5 Summary and Conclusion \nGenerative concept representations are probabilistic and generative in nature and can represent uncertainty, support \nintegration of learning and reasoning, and are good for unsupervised and semi-supervised learning. They adopt the paradigm, \nand are the outcome, of probabilistic and generative deep learning. \nProbabilistic and generative deep learning is based on probabilistic machine learning and probabilistic graphical models, \nwhile adopting the deep learning approach. Its core is deep generative models which utilizes latent variables and \ndifferentiable generator networks. \nVariational autoencoders and generative adversarial networks can be used for learning generative concept \nrepresentations. Both types of architectures have been enhanced or extended to handle data that are sequences, structured data \nor graphs. \nGenerative concept representations can be directly manipulated to generate new concepts with desired attributes. They \nare the foundation of creative applications. \nReferences \n \n[1] Daniel T. Chang, ‚ÄúConcept-Oriented Deep Learning,‚Äù arXiv preprint arXiv:1806.01756 (2018). \n[2] Daphne Koller and Nir Friedman, Probabilistic Graphical Models: Principles and Techniques (MIT Press, 2009). \n[3] Alex Kendall and Yarin Gal, ‚ÄúWhat Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?‚Äù arXiv \npreprint arXiv:1703.04977 (2017). \n14 \n \n[4] Yarin Gal, Uncertainty in Deep Learning (PhD thesis, University of Cambridge, 2016). \n[5] Hao Wang and Dit-Yan Yeung, ‚ÄúTowards Bayesian Deep Learning: A Survey,‚Äù arXiv preprint arXiv:1604.01662 (2016).  \n[6] R. G√≥mez-Bombarelli, J. N. Wei, D. Duvenaud, J. Miguel Hern√°ndez-Lobato, B. S√°nchez-Lengeling, D. Sheberla, J. \nAguilera-Iparraguirre, T. D. Hirzel, R. P. Adams, and A. Aspuru-Guzik, ‚ÄúAutomatic Chemical Design Using a Data-Driven \nContinuous Representation of Molecules,‚Äù ACS Cent. Sci. 2018, 4, 268‚àí276.  \n[7] Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning (MIT Press, 2016). \n[8] Zoubin Ghahramani, ‚ÄúProbabilistic Machine Learning and Artificial Intelligence,‚Äù Nature, 521(7553), 2015. \n[9] Zoubin Ghahramani, ‚ÄúBayesian Nonparametrics and the Probabilistic Approach to Modeling,‚Äù Phil. Trans. R. Soc. A 371, \n20110553 (2013). \n[10] Kiran R Karkera, Building Probabilistic Graphical Models with Python (Packt Publishing, 2014). \n[11] Francois Chollet, Deep Learning with Python (Manning, 2018). \n[12] Zhijian Ou, ‚ÄúA Review of Learning with Deep Generative Models from Perspective of Graphical Modeling,‚Äù arXiv \npreprint arXiv:1808:01630 (2018). \n[13] Z. Hu, Z. Yang, R. Salakhutdinov, and E. P. Xing, ‚ÄúOn Unifying Deep Generative Models,‚Äù ICLR 2018. \n[14] Diederik P Kingma and Max Welling, ‚ÄúAuto-encoding Variational Bayes,‚Äù ICLR, 2014. \n[15] Danilo Jimenez Rezende, Shakir Mohamed, and DaanWierstra, ‚ÄúStochastic Backpropagation and Approximate Inference \nin Deep Generative Models,‚Äù ICML, 2014. \n[16] Carl Doersch, ‚ÄúTutorial on Variational Autoencoders,‚Äù arXiv preprint arXiv:1606.05908 (2016). \n[17] K. Wang, C. Gou, Y. Duan, Y. Lin, X. Zheng, and F.-Y. Wang, ‚ÄúGenerative Adversarial Networks: Introduction and \nOutlook,‚Äù IEEE/CAA Journal of Automatica Sinica, vol. 4, no. 4, pp. 588‚Äì598, Sep. 2017. \n[18] Saifuddin Hitawala, ‚ÄúComparative Study on Generative Adversarial Networks,‚Äù arXiv preprint arXiv:1801.04271 \n(2018). \n[19] Zachary C. Lipton, John Berkowitz, and Charles Elkan, ‚ÄúA Critical Review of Recurrent Neural Networks for Sequence \nLearning,‚Äù arXiv preprint arXiv:1506.00019 (2015). \n[20] S. R. Bowman, L. Vilnis, O. Vinyals, A.M. Dai, R. Jozefowicz, and S. Bengio, ‚ÄúGenerating Sentences from a \nContinuous Space,‚Äù arXiv preprint arXiv:1511.06349 (2015). \n[21] Adam Roberts, Jesse Engel, Colin Raffel, Curtis Hawthorne, and Douglas Eck, ‚ÄúA Hierarchical Latent Vector Model for \nLearning Long-term Structure in Music,‚Äù arXiv preprint arXiv:1803.05428 (2018).  \n[22] L. Yu, W. Zhang, J. Wang, and Y. Yu, ‚ÄúSeqGAN: Sequence Generative Adversarial Nets with Policy Gradient,‚Äù in \nAAAI, pages 2852‚Äì2858, 2017. \n[23] Yi-Lin Tuan and Hung-Yi Lee, ‚ÄúImproving Conditional Sequence Generative Adversarial Networks by Stepwise \nEvaluation,‚Äù arXiv preprint arXiv:1808.05599 (2018). \n[24] Matt J. Kusner, Brooks Paige, and Jos√© Miguel Hern√°ndez-Lobato, ‚ÄúGrammar Variational Autoencoder,‚Äù arXiv \npreprint arXiv:1703.01925 (2017). \n[25] H. Dai, Y. Tian, B. Dai, S. Skiena, and L. Song, ‚ÄúSntax-directed Variational Autoencoder for Structured Data,‚Äù ICLR, \n2018. \n[26] Xinyue Liu, Xiangnan Kong, Lei Liu, and Kuorong Chiang, ‚ÄúTreeGAN: Syntax-Aware Sequence Generation with \nGenerative Adversarial Networks,‚Äù arXiv preprint arXiv:1808.07582 (2018). \n[27] T. N. Kipf and M. Welling, ‚ÄúVariational Graph Autoencoders,‚Äù arXiv preprint arXiv:1611.07308 (2016). \n[28] S. Pan, R. Hu, G. Long, J. Jiang, L. Yao, and C. Zhang, ‚ÄúAdversarially Regularized Graph Autoencoder,‚Äù arXiv preprint \narXiv:1802.04407 (2018). \n[29] Hongwei Wang, Jia Wang, Jialin Wang, Miao Zhao, Weinan Zhang, Fuzheng Zhang, Xing Xie, and Minyi Guo, \n‚ÄúGraphGAN: Graph Representation Learning with Generative Adversarial Nets,‚Äù in AAAI Conference on Artificial \nIntelligence, 2018. \n[30] A. Bojchevski, O. Shchur, D. Z√ºgner, and S. G√ºnnemann, ‚ÄúNetGAN: Generating Graphs via Random Walks,‚Äù arXiv \npreprint arXiv:1803.00816 (2018). \n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2018-11-15",
  "updated": "2018-11-15"
}