{
  "id": "http://arxiv.org/abs/1811.07211v2",
  "title": "Classifiers Based on Deep Sparse Coding Architectures are Robust to Deep Learning Transferable Examples",
  "authors": [
    "Jacob M. Springer",
    "Charles S. Strauss",
    "Austin M. Thresher",
    "Edward Kim",
    "Garrett T. Kenyon"
  ],
  "abstract": "Although deep learning has shown great success in recent years, researchers\nhave discovered a critical flaw where small, imperceptible changes in the input\nto the system can drastically change the output classification. These attacks\nare exploitable in nearly all of the existing deep learning classification\nframeworks. However, the susceptibility of deep sparse coding models to\nadversarial examples has not been examined. Here, we show that classifiers\nbased on a deep sparse coding model whose classification accuracy is\ncompetitive with a variety of deep neural network models are robust to\nadversarial examples that effectively fool those same deep learning models. We\ndemonstrate both quantitatively and qualitatively that the robustness of deep\nsparse coding models to adversarial examples arises from two key properties.\nFirst, because deep sparse coding models learn general features corresponding\nto generators of the dataset as a whole, rather than highly discriminative\nfeatures for distinguishing specific classes, the resulting classifiers are\nless dependent on idiosyncratic features that might be more easily exploited.\nSecond, because deep sparse coding models utilize fixed point attractor\ndynamics with top-down feedback, it is more difficult to find small changes to\nthe input that drive the resulting representations out of the correct attractor\nbasin.",
  "text": "Classiﬁers Based on Deep Sparse Coding Architectures are Robust to Deep\nLearning Transferable Examples\nJacob M. Springer1,2, Charles S. Strauss3, Austin M. Thresher3, Edward Kim4,\nGarrett T. Kenyon1\nAbstract\nAlthough deep learning has shown great success in re-\ncent years, researchers have discovered a critical ﬂaw\nwhere small, imperceptible changes in the input to the sys-\ntem can drastically change the output classiﬁcation. These\nattacks are exploitable in nearly all of the existing deep\nlearning classiﬁcation frameworks. However, the suscep-\ntibility of deep sparse coding models to adversarial exam-\nples has not been examined. Here, we show that classiﬁers\nbased on a deep sparse coding model whose classiﬁcation\naccuracy is competitive with a variety of deep neural net-\nwork models are robust to adversarial examples that effec-\ntively fool those same deep learning models. We demon-\nstrate both quantitatively and qualitatively that the robust-\nness of deep sparse coding models to adversarial examples\narises from two key properties. First, because deep sparse\ncoding models learn general features corresponding to gen-\nerators of the dataset as a whole, rather than highly dis-\ncriminative features for distinguishing speciﬁc classes, the\nresulting classiﬁers are less dependent on idiosyncratic fea-\ntures that might be more easily exploited. Second, because\ndeep sparse coding models utilize ﬁxed point attractor dy-\nnamics with top-down feedback, it is more difﬁcult to ﬁnd\nsmall changes to the input that drive the resulting represen-\ntations out of the correct attractor basin.\n1. Introduction\nWith their strong and steadily improving performance\non image classiﬁcation tasks, deep feed-forward convolu-\ntional networks (DCNs) are now ubiquitous in virtually all\ncomputer vision and machine learning challenges. How-\never, overwhelming evidence shows that DCNs have at least\none major ﬂaw: they are susceptible to adversarial attacks.\nResearchers are able to make specially crafted “adversar-\nial perturbations” that when added to a correctly-classiﬁed\n1Los Alamos National Laboratory, 2Swarthmore College, 3New Mexico\nConsortium, 4Villanova University\nimage, produce an image that appears semantically indis-\ntinguishable from the original yet are conﬁdently misclassi-\nﬁed by DCN models [1, 2]. In many cases, these adversar-\nial perturbations can be added to any image within a class\nto cause the DCN to misclassify the image. Even more\nalarming, adversarial perturbations are commonly transfer-\nable across DCN model architectures [1–3]. In an age in\nwhich autonomous cars are ever closer to entering the con-\nsumer market, in which deep learning is common in medical\napplications, and in which facial recognition is used for se-\ncurity purposes, the ﬂaw posed by adversarial examples can\nhave devastating effects if used maliciously. Moreover, at-\ntempts to mitigate the problem of adversarial examples have\nlargely failed or render established models signiﬁcantly less\neffective [4,5]. In fact, in practice, increasing robustness to\nadversarial examples in DCN models necessarily reduces\nclassiﬁcation performance [4,5].\nIn our work, we sought to develop a framework that\nwould be immune to adversarial attack while maintaining\na high level of classiﬁcation performance.\nIn particular,\nour goal was to mitigate the risk of a neural network to\nthe transfer of adversarial example attacks and low-pass\nﬁltering attacks that have been shown to be effective ad-\nversaries against deep feedforward networks [1, 2, 4]. We\nbelieve that demonstrating robustness is an important step\nforward in computer vision and machine learning research\nand illustrate a thorough process of testing and experimen-\ntation in the body of this paper. As a result of our explo-\nration and analysis, we found that every model we tested\nwas susceptible to transferable adversarial examples, except\none. This model, deep sparse coding (DSC) [6], takes a\nnovel, biologically-inspired approach to machine learning,\nand was immune to these transferable adversarial examples\ngenerated to attack other deep learning architectures. Fur-\nthermore, we found, and illustrate in our results, that the\nDSC model’s representation of input is invariant to small\nperturbations. This robustness stems from the model’s ten-\ndency to observe semantically meaningful features of an im-\nage, in contrast with the high frequency features that have\nlittle or no semantic meaning to humans used by the con-\narXiv:1811.07211v2  [cs.LG]  20 Nov 2018\nventional DCN classiﬁers.\n2. Background\n2.1. Single-layer sparse coding\nTo introduce the concept of deep sparse coding, we\nﬁrst consider a single-layer of sparse coding. Intuitively,\na single-layer of sparse coding requires learning a set of\nrelatively low-dimensional, nearly orthogonal bases, with\nthe set of all such bases forming an overcomplete dictio-\nnary that has been optimized for parsimoniously encoding\nany input drawn from the same distribution on which the\nmodel has been trained. The fully trained system acts as an\nautoencoder that is able to infer a sparse representation of\nany given input x which can in turn be used to reconstruct\nx. From a learned dictionary, the sparse coding algorithm\nattempts to infer the optimal basis in which to project the\ninput by maximizing reconstruction ﬁdelity while using as\nfew basis vectors as possible.\nMore formally, we can pose the sparse coding problem\nas follows: given a set of inputs from some input distribu-\ntion x1, . . . , xn ∼X with associated sparse representations\na1, . . . , an, which we call activation vectors, sparse coding\nattempts to minimize the average mean squared error of the\nreconstructions of xi using ai and a dictionary Φ. Addi-\ntionally, the ai must be sparse. Mathematically, inferring a\nsparse representations is a minimization problem:\nmin\nΦ\nn\nX\ni=1\nmin\nai\n1\n2 ∥xi −Φai∥2\n2 + λ ∥ai∥0\n(1)\nwhere λ is the sparsity penalty parameter. Note that Φai\nrepresents the reconstruction of xi and ∥·∥k represents the\nℓk norm.\nThe way in which we solve for an appropriate activation\nvector for each x is inspired by neural biological processes.\nWe represent the internal state of the model with a mem-\nbrane potential vector u associated with the activations. We\ncan compute a by a simple thresholding function:\nam =\n(\num −c\num ≥c\n0\notherwise.\n(2)\nwhere c is the thresholding constant.\nTo minimize reconstruction error while maintaining a\ndesired sparsity, we use a technique known as the locally\ncompetitive algorithm [7], allowing our sparse representa-\ntion to evolve over time as a dynamical system. We wish to\nminimize the energy J of our system, which we deﬁne as\nthe mean squared error of the reconstruction and a sparsity\ncost penalty ∥a∥1. Formally:\nJ = 1\n2 ∥x −Φa∥2\n2 + λ ∥a∥1 .\n(3)\nNote that we use an ℓ1 norm as an approximation for the ℓ0\nnorm of the activation vector to make the system differen-\ntiable. We can express the time evolution rule as a differen-\ntial equation obtained by taking the negative gradient of the\nenergy J with respect to a,\n˙u = −u + ΦT x −[ΦT Φa −a]\n(4)\nwhich can be described as the combination of three intuitive\ncomponents. The −u term acts as the decay component\nof the system, thus slowly decreasing the potential of each\nneuron over time. If an active neuron is not continuously\nexcited, then it will quickly fall below threshold and deacti-\nvate. The ΦT x term “charges up” each of the neurons, ex-\nciting neurons whose features effectively match the input.\nThe [ΦT Φa −a] term acts as an inhibitory signal, caus-\ning neurons that explain a similar component of the data to\ncompete, thus allowing only the neurons that “best” explain\nthe data to remain active. Rozell et al. [7] proves that this\ndynamical system converges to the optimal solution to the\nenergy function.\n2.2. Multi-layer (deep) sparse coding\nAnalogous to deep feed-forward models, deep sparse\ncoding simply adds more hidden layers between the input\nand output layers, where each hidden layer attempts to re-\nconstruct the layer immediately below it. However, in con-\ntrast to conventional deep learning models, the data con-\nnections are not simply unidirectional, forward connections\nfrom one layer to the next. Instead, deep sparse coding inte-\ngrates multiple highly recurrent connections at every level,\ni.e., bottom-up input, lateral, and top-down feedback sig-\nnals. The top-down feedback connections reinforce a stable\nand self-consistent representation of the data at each layer.\nTo illustrate the necessity of top-down feedback, con-\nsider that a typical feed-forward network might activate\nmultiple “good” explanations of the data. For example, the\nneurons in the early layers of a model only perceive a small\npatch of the input due to their local and limited receptive\nﬁelds. These neurons can easily observe cat-like features\nand dog-like on the same input.\nA feed-forward model\nmight encode this input by activating both neurons that re-\nspond to cat-like features and those that respond to dog-like\nfeatures. This is because it is nearly impossible for bottom-\nup and lateral processes to “know” which low-level feature\nshould be assigned to one shape vs. another. Thus, feedback\nprojections from higher-level regions would be necessary to\naccount for our visual abilities in all but the simplest of cir-\ncumstances [8]. On the other hand, consider the case when\na higher-level neuron that activates on images of dogs be-\ncomes active in the DSC model. If the model had learned\nthat observing a dog reduces the probability of observing a\ncat, then the top-down feedback from the higher-level dog\nneuron will inhibit the neurons that respond to cat-like fea-\n2\ntures and would reinforce the neurons that respond to dog-\nlike features.\nThus, top-down feedback provides an effective method\nto converge on a conﬁdent and self-consistent representa-\ntion of the input.\nThe DSC model formulates top-down\nfeedback as follows: Let uL represent the internal state of\nlayer L. Let the residual rL+1 of layer L + 1’s reconstruc-\ntion of layer L be deﬁned:\nrL = uL −ΦL+1aL+1\n(5)\nThe uL term is forced to “conform” to layer L + 1’s re-\nconstruction of it by modifying our time evolution function\nof uL to continually reduce the residual of the higher-level\nreconstruction:\n˙uL = −uL + ΦT x −[ΦT ΦaL −aL] −rL\n(6)\n3. Methodology\nIn this paper, we begin by deﬁning a binary classiﬁca-\ntion task that will be attacked using adversarial examples.\nWe chose the binary classiﬁcation task of face identiﬁcation\nas this is an important domain where a machine learning\nmodel should be robust. The face ID application will be ap-\nplied to a set of deep feed-forward convolutional networks\nas well as on the deep sparse coding framework. Earlier\nwork on the deep sparse coding framework used the exam-\nple of “Halle Berry” face detection [6] and thus we con-\ntinue on with this theme. We ﬁrst demonstrate that we can\ntrain a number of DCNs that can accurately distinguish be-\ntween images of Halle Berry from our dataset and images\nthat are not of Halle Berry. Next, we exploit the learned\n“Halle Berry neuron” in the DSC model, i.e., a neuron anal-\nogous to the neuron in the human brain identiﬁed by [9] that\nresponds to the multimodal “concept” of Halle Berry. After\ntraining the DCN models, we show that we can generate ad-\nversarial examples from our holdout set that transfer across\nDCN architectures using established techniques. Finally,\nwe show that these adversarial examples fail to transfer to\nthe DSC model.\n3.1. Dataset\nWe use a subset of the Labeled Faces in the Wild (LFW)\ndataset [10] augmented with images of Halle Berry. The\ndataset consists of photographs of faces from the web de-\ntected by the Viola-Jones face detector. In total, there are\n370 images of Halle Berry, scraped from IMDB, and a total\nof 5,763 images consisting of 2,189 unique people. Each\nimage is 64 × 64 pixels, each with 3 color channels. We\nsegment the data into 4,519 training images, 1094 valida-\ntion images, and 150 holdout images for evaluating our ﬁ-\nnal models. We use 150 holdout images which includes\n75 images of Halle Berry and 75 images of other people.\nEach of these holdout images are altered to make the vari-\nants of the dataset as described in later sections, including\nthe adversarial dataset, the low-pass ﬁltered dataset, and the\nGaussian noisy dataset. See Figure 1 for a sample of the\ndataset and its variants. As a note, all ﬁgures in this paper\nare best viewed in color.\n3.2. Deep feed-forward convolutional network mod-\nels\nDeep convolutional neural networks (DCNs) that have\nbeen pre-trained on large image datasets such as ImageNet\n[11] and then ﬁne-tuned on the target dataset have been\nshown to have impressive classiﬁcation performance on\nseveral datasets of limited size [12, 13]. Thus, we initial-\nized several state-of-the-art DCN architectures (ResNet50\n[14], InceptionV3 [15], VGG16 [16], MobileNetV2 [17],\nDenseNet121 [18]) with weights trained on ImageNet [14–\n20] and then ﬁne-tuned these DCNs to distinguish between\nimages of Halle Berry and images of “other” faces. We\nused the Keras [20] framework on top of TensorFlow [19]\nto build the models.\n3.3. Deep sparse coding model\nWe use the DSC model built and trained by Kim et al.\ndescribed in [6]. The model attempts to reconstruct two\nmodalities using a single shared top-level hidden layer. The\ntwo modalities are a 64 × 64 color image of a person’s face\nand a 128 × 16 grayscale image of the person’s name, ren-\ndered in 20 point Arial font. The bi-modality of the model\nwith a shared P1 layer (see Figure 2) encourages the model\nto encode an image of a person and an image of their name\nwith the same representation. We show that this feature en-\ncourages the model to learn high-level features including a\n“Halle Berry neuron”, i.e., a single neuron which appears\nto encode both images of Halle Berry and of her name.\nThe model is implemented using PetaVision [21], an open-\nsource accelerated neuromorphic computing framework.\nThe full model is depicted in Figure 2. The model con-\nsists of two vision-speciﬁc layers, V1 and VP1, and one\ntext-speciﬁc layer, T1. These recurrent layers are connected\nthrough the top-level layer, P1. For a more detailed descrip-\ntion of the model, see [6].\nThe DSC model does not perform classiﬁcation explic-\nitly. Rather, it generates a sparse representation of the input\nin its P1 layer. For classiﬁcation, we train a linear support\nvector machine (SVM) to take input from the P1 layer acti-\nvations associated with a given image and output a label.\n3.4. Transferable adversarial examples\nWe demonstrate that transferable adversarial examples,\ni.e., adversarial examples that generalize across multiple\nDCN architectures, do not generalize to the DSC model.\nTo generate an adversarial example for input x on classiﬁer\n3\nFigure 1. Images from the holdout dataset and the corresponding alterations. In each row, the left three faces are of “other” (non-Halle\nBerry), and the right three faces are of Halle Berry. From top to bottom: original, adversarial, Gaussian noise, low-pass ﬁlter. The noise to\nthe left of each non-original image shows the difference between that image and the corresponding original. The noise has been translated\nso that a pixel with no change is gray with a value 0.5. The noise is scaled by a factor of 10 in order to increase visibility. While upon\nclose inspection the alternations are visible, each altered image is nearly indistinguishable identical to the original image.\nFigure 2. Deep sparse coding (DSC) model from Kim et al. [6]\nˆk, we wish to ﬁnd perturbation η such that ˆk(x) ̸= ˆk(x+η)\nsubject to the constraint that ∥η∥∞is sufﬁciently small. We\ngenerate adversarial examples using an iterative version of\nthe fast gradient sign method [2] (IFGSM). Let J(x, y) be\nthe classiﬁcation loss function given input x and ground-\ntruth label y. Beginning from the original input x∗\n0 = x, we\ndescribe the algorithm as follows:\nx∗\ni+1 = x∗\ni + ε sign ∇xJ(x∗\ni , y)\n(7)\nLow-pass ﬁltering has been shown to reduce classiﬁca-\ntion performance on DCN architectures [4]. To demonstrate\nthat the DSC model does not rely on high-frequency noise\nto classify images, we run each holdout image through a\nlow-pass ﬁlter to generate a fourth dataset.\nSamples of these datasets are shown in Figure 1.\n4. Experiments and results\nWe ﬁrst illustrate the responses of the Halle Berry neu-\nron. To demonstrate the DSC model’s resistance to trans-\nferable adversarial examples, we evaluate the DSC model\nagainst analogous DCN models by comparing Halle-Berry-\nidentiﬁcation performance on original images, adversarial\nimages, Gaussian noisy images, and low-pass ﬁltered ver-\nsions. We examine the internal state of the DSC model to\nidentify the properties of the DSC model that provide its\nrobustness.\n4.1. Representation of Halle Berry\nWe observe a single “Halle Berry neuron”, which we call\nN-326 due to its index in the DSC model. To demonstrate\nthe selectivity of N-326, we compare the average activation\nof all P1 neurons over all images in the training set, and over\nimages of Halle Berry in the training set. We plot these radi-\nally in Figure 3. The average activation of N-326 on images\nof Halle Berry is signiﬁcantly stronger than the average ac-\ntivation produced by other faces, suggesting that this neuron\nactivates strongly and selectively in response to an image of\nHalle Berry. In fact, thresholding N-326 yields a classiﬁ-\ncation performance of nearly 85% on the holdout dataset.\nThus, N-326 acts as a somewhat effective indicator of im-\nages of Halle Berry. Additionally, after training an SVM on\nthe full P1 layer to distinguish images of Halle Berry from\nother faces, we examine the weights, which are plotted in\nFigure 4. The weight associated with N-326 is nearly twice\n4\nFigure 3. Average activations of P1 layer over the training set.\nStandard deviations are plotted with a dotted black line. In red:\naverage response over all images of Halle Berry. In blue: aver-\nage response over all other faces. On average, the P1 layer has a\nstrong spike on N-326 for Halle Berry images and comparatively\nweak activations to tther faces. The average activations over all\nother faces is relatively uniform. Both graphs are plotted on the\nsame scale.\nthe second-highest weight, further suggesting that N-326 is\nthe strongest linear indicator of Halle Berry.\n4.2. Comparison of models\nWe compare robustness by measuring the performance\nof each model on the holdout datasets. We entertain the\npossibility that any distortion of an image might cause a\nmodel to be unable to identify any particular features that\ncorrelate strongly with images of Halle Berry or of other\nfaces, which would force the network to guess randomly.\nThus, we control for the effects of the distortion by run-\nning the model on the input with added Gaussian noise with\nFigure 4. Positive weights of an SVM trained to classify images\nas either “Halle Berry” or “other” from the sparse P1 layer rep-\nresentation of each image.\nImportantly, the strongest positive\nweight corresponds to N-326, indicating that this neuron acts as\nthe strongest linear indicator of Halle Berry. Since none of the\nnegative weights have a magnitude close to that of the weight that\ncorresponds to N-326, we omit them to improve the clarity of the\ndiagram.\nFigure 5. Accuracy scores of the various models on the in blue:\noriginal images, in green: Gaussian-noisy images, in orange: low-\npass ﬁltered images, and in red: adversarial images. Note that we\nuse the DenseNet121 model to generate the adversarial examples,\nwhich is why its classiﬁcation accuracy on the adversarial exam-\nples is zero. We see that the performance of all DCN models drops\nsigniﬁcantly on adversarial examples whereas the performance of\nthe DSC model is invariant to the described minor alterations.\n5\nthe same mean and standard distribution as the adversarial\nnoise. The comparative accuracies are plotted in Figure 5.\nNote that the performance of the DenseNet121 model on the\nadversarial examples is shown for reference, however, since\nthe DenseNet121 model is used as a whitebox with which\nto generate adversarial examples, the performance on ad-\nversarial examples is expected to be near-zero and orders of\nmagnitude smaller than the performance of the other DCN\narchitectures on the transferred adversarial examples, cor-\nresponding to a blackbox attack.\nThe classiﬁcation performances of the respective models\ndemonstrate the robustness of the DSC model to transfer-\nable adversarial examples. The four applicable DCN mod-\nels (ResNet50, InceptionV3, VGG16, and MobileNetV2)\nperform signiﬁcantly worse on the transferred adversarial\nexamples than on the original and Gaussian noisy images,\nyet the difference in the performance of the DSC model is\nnegligible between the original and adversarial inputs.\nFurthermore, the classiﬁcation performance of the DCN\nmodels decreased on images after processing them with a\nlow-pass ﬁlter. However, as shown by Figure 5 the perfor-\nmance of the DSC model is invariant to a low-pass ﬁlter.\n4.3. Invariance of representation\nResNet\nInception\nVGG\nMobileNet\nDSC\nδα\n0.44\n0.08\n0.59\n0.68\n0.05\nδη\n0.34\n0.08\n0.42\n0.69\n0.04\nδℓ\n0.39\n0.13\n0.44\n0.46\n0.04\nTable 1. Distance metric between hidden-layer representation of\noriginal images and adversarial images (δα), Gaussian noisy im-\nages (δη), and low-pass ﬁlter images (δℓ). The exact metric is\ndescribed below. Deep sparse coding representations of adversar-\nial, noisy, and low-pass ﬁlter images are signiﬁcantly more similar\nthan the corresponding representations by DCN models.\nTo show that the DSC model is resistant to minor high-\nfrequency perturbations, we extract and plot the activation\nof the strongest linear indicator of Halle Berry, N-326, on\nthe 75 holdout images of Halle Berry and the 75 holdout im-\nages of other. We provide three graphs, placed adjacently,\nthat show the N-326 activation on the original images, the\nadversarial images, and the low-pass ﬁlter images, respec-\ntively. These graphs are shown in Figure 6. Adversarial\nperturbations and low-pass ﬁltering have little effect on the\nactivation of N-326. It follows that any linear classiﬁer that\nrelies heavily on N-326, such as the SVM we use to classify\nthe images, should be resistant to these minor perturbations,\nincluding alterations that prove adversarial in feedforward\nDCNs.\nWe further examine the P1 representation of images\nacross the original images, adversarial perturbations, and a\nlow-pass ﬁlter. In Figure 8, we plot the P1 activations of a\nrepresentative image of Halle Berry (top) and of other (bot-\ntom). The representation of each image is nearly identical\nacross the alterations. In other words, these minor perturba-\ntions do not appear to affect the entire P1 representation of\nan image.\nThese observations generalize to the entire holdout\ndataset. We compare the top-level hidden-layer represen-\ntation of each original input image with the corresponding\nrepresentation of the adversarial, Gaussian noisy, and low-\npass ﬁlter images. As a measure, we compute the similarity\nbetween the neuron activations in the hidden layer imme-\ndiately below the output layer in all networks. In the DSC\nmodel, we observe the P1 layer activations when computing\nthe similarity metric. To control for the various distributions\nof neuron activations across networks when measuring sim-\nilarity, we compute the average difference between the neu-\nron activations over the average standard deviation of the\nneuron activations. Shown in Table 1, we demonstrate that\nthe DSC model has a signiﬁcantly more similar represen-\ntations across the original, adversarial, and low-pass ﬁlter\nimages than the DCN models we trained. Thus, minor per-\nturbations to the input do not affect the internal representa-\ntion of the image in our P1 layer.\nFor completeness, we include representative examples of\nthe reconstructions of the inputs from the three datasets:\noriginal, adversarial, and low-pass ﬁlter. As illustrated by\nFigure 7, there is little, if any, noticeable difference across\nthe reconstructions of the images in each dataset.\n4.4. Limitations\nWe believe that deep sparse coding makes an important\nstep forward towards the development of machine learning\nmodels that are fully robust to adversarial examples. How-\never, as with any novel approach, deep sparse coding has\nnot yet been tested on many datasets.\nWhile we show that transferable adversarial examples\ngenerated using the iterative fast gradient sign method do\nnot transfer to the DSC model, we do acknowledge that the\nDSC model is susceptible to adversarial attacks. This is\nno surprise since all visual recognition systems (including\nbiological ones) will eventually change their classiﬁcation\ngiven a strong enough perturbation to the input signal. Since\nobserving the gradient of the DSC model is non-trivial, we\ndo not perform a direct gradient-based attack on the DSC\nmodel, despite the possibility that an effective one might\nexist. However, we are able to estimate the gradient of the\nmodel by making single-pixel changes to a given image and\naggregating them to construct a ﬁrst-order estimation of the\ngradient. These examples prove effective against the DSC\nmodel and their properties should be examined in the fu-\nture. This paper, however, still presents an important result:\nthe DSC model is not susceptible to transferable adversarial\nexamples that effectively target DCN models.\n6\nFigure 6. The activation strength of the N-326 neuron on each holdout image, compared across the variants of the datasets. The left graph\nshows the N-326 activation on each in blue: original image of Halle Berry and in orange: of people other than Halle Berry. The center\nand right graphs show the same, but on the adversarial copies and on the low-pass ﬁlter copies, respectively. N-326 activates strongly on\nimages of Halle Berry and weakly on images of other. The N-326 activation remains invariant across adversarial perturbations and low-pass\nﬁltering.\nFigure 7. Representative examples of reconstructions of Halle\nBerry faces and corresponding alterations. The images provided\nas input to the model are shown on the top row. The bottom row\nfeatures the model’s reconstruction of each image. For each face,\nfrom left to right: original, adversarial, and low-pass ﬁlter.\n5. Conclusion\nWe show that the deep sparse coding model is resistant\nto transferable adversarial attacks that harshly drop perfor-\nmance of state-of-the-art DCN models.\nTo demonstrate\nthis, we build state-of-the-art DCN classiﬁers that apply\ntransfer learning to separate images into the categories of\nHalle Berry and other. We apply the iterative fast gradient\nsign method to the DenseNet model to generate adversarial\nexamples, and then show that these adversarial examples\ntransfer more effectively than random-noise perturbations\nto other DCN models. We then show that the DSC model\nachieves the same classiﬁcation rate on adversarial exam-\nples and low-pass ﬁlter images as the original unperturbed\nimages. Additionally, we show that the internal DSC repre-\nsentation of each image is invariant to minor perturbations.\nFinally, we acknowledge that adversarial examples exist for\nthe DSC model, but that traditional transfer attacks fail on\nthe DSC model.\nAt a high level, we demonstrate that the deep sparse cod-\ning model relies on features that are different than those\nrelied upon by DCN models, which has important impli-\ncations for the ﬁeld of adversarial machine learning. De-\nspite the fact that the DSC model is vulnerable to direct ad-\nversarial attacks, if an attacker does not have access to the\nspeciﬁc model, an attack may be difﬁcult to perform. For\nexample, an adversary attempting to fool a self-driving car\nwithout access to the model would not be able to apply a\nstandard adversarial example transfer attack. A malicious\ndoctor could not apply a transfer attack to fool the model\ninto identifying a disease from an X-ray that is not present.\nWe show that the DSC model is signiﬁcantly more resis-\ntant to high-frequency alterations of the data (such as Gaus-\nsian noise and DCN adversarial examples) than traditional\nDCN models. As such, the model may be especially ef-\nfective in environments in which high-frequency noise oc-\ncurs frequently. Furthermore, we show that a low-pass ﬁlter\ndoes not affect the DSC model’s internal representation of\nthe data, suggesting that the DSC model relies on lower-\nfrequency features than DCN models.\nIn our future work, we plan to further investigate deep\nsparse coding models, especially to answer questions such\nas: what particular attacks are effective against DSC mod-\nels, do there exist universal adversarial examples as found\nin DCN models [3], and do adversarial examples effectively\ntransfer across separate DSC models?\nIn conclusion, we believe that the DSC model provides\nan important step forward in the ﬁeld of computer vision\nand machine learning, and warrants further research. Our\nproject addresses a major ﬂaw in the traditional deep learn-\ning methodologies and offers an effective alternative. Our\nhope is to see further research on neurally inspired models\nand their applications in the pattern recognition community.\nReferences\n[1] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,\nJoan Bruna, Dumitru Erhan, Ian Goodfellow, and\nRob Fergus. Intriguing properties of neural networks.\narXiv preprint arXiv:1312.6199, 2013. 1\n[2] Ian J Goodfellow, Jonathon Shlens, and Christian\nSzegedy. Explaining and harnessing adversarial ex-\namples. corr (2015). 1, 4\n7\nFigure 8. P1 layer activations on Halle Berry and not Halle Berry\nacross perturbations. The left column shows the activations of all\n512 neurons in the P1 layer of the DSC model on an arbitrarily\nselected image of Halle Berry from our holdout set. The right\ncolumn shows the activations of the P1 layer on an arbitrarily se-\nlected image of a person’s face who is not Halle Berry. The top\nrow shows activations on the original version of the image; the\ncenter row shows activations on the adversarial version of the im-\nage; the bottom row shows the activations on the original image\nafter being processed by a low-pass ﬁlter.\n[3] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi,\nOmar Fawzi, and Pascal Frossard. Universal adver-\nsarial perturbations. arXiv preprint, 2017. 1, 7\n[4] Philip HS Torr et al. With friends like these, who needs\nadversaries? arXiv preprint arXiv:1807.04200, 2018.\n1, 4\n[5] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom,\nAlexander Turner, and Aleksander Madry.\nRobust-\nness may be at odds with accuracy.\narXiv preprint\narXiv:1805.12152, 2018. 1\n[6] Edward Kim, Darryl Hannan, and Garrett Kenyon.\nDeep sparse coding for invariant multimodal halle\nberry neurons.\narXiv preprint arXiv:1711.07998,\n2017. 1, 3, 4\n[7] Christopher J Rozell, Don H Johnson, Richard G\nBaraniuk, and Bruno A Olshausen. Sparse coding via\nthresholding and local competition in neural circuits.\nNeural computation, 20(10):2526–2563, 2008. 2\n[8] Kestutis Kveraga, Avniel S Ghuman, and Moshe Bar.\nTop-down predictions in the cognitive brain.\nBrain\nand cognition, 65(2):145–168, 2007. 2\n[9] R Quian Quiroga, Leila Reddy, Gabriel Kreiman,\nChristof Koch, and Itzhak Fried. Invariant visual rep-\nresentation by single neurons in the human brain. Na-\nture, 435(7045):1102, 2005. 3\n[10] Gary B. Huang, Marwan Mattar, Honglak Lee, and\nErik Learned-Miller. Learning to align from scratch.\nIn NIPS, 2012. 3\n[11] Olga Russakovsky, Jia Deng, Hao Su, Jonathan\nKrause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein,\net al.\nImagenet large scale visual recognition chal-\nlenge.\nInternational Journal of Computer Vision,\n115(3):211–252, 2015. 3\n[12] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoff-\nman, Ning Zhang, Eric Tzeng, and Trevor Darrell.\nDecaf: A deep convolutional activation feature for\ngeneric visual recognition.\nIn International confer-\nence on machine learning, pages 647–655, 2014. 3\n[13] Ali Sharif Razavian, Hossein Azizpour, Josephine\nSullivan, and Stefan Carlsson. Cnn features off-the-\nshelf: an astounding baseline for recognition. In Pro-\nceedings of the IEEE conference on computer vision\nand pattern recognition workshops, pages 806–813,\n2014. 3\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun.\nDeep residual learning for image recognition.\nIn Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 770–778, 2016.\n3\n[15] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJon Shlens, and Zbigniew Wojna. Rethinking the in-\nception architecture for computer vision. In Proceed-\nings of the IEEE conference on computer vision and\npattern recognition, pages 2818–2826, 2016. 3\n[16] Karen Simonyan and Andrew Zisserman. Very deep\nconvolutional networks for large-scale image recogni-\ntion. arXiv preprint arXiv:1409.1556, 2014. 3\n8\n[17] Mark Sandler, Andrew Howard, Menglong Zhu, An-\ndrey Zhmoginov, and Liang-Chieh Chen.\nMo-\nbilenetv2: Inverted residuals and linear bottlenecks.\nIn Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 4510–4520,\n2018. 3\n[18] Gao Huang, Zhuang Liu, Laurens Van Der Maaten,\nand Kilian Q Weinberger. Densely connected convo-\nlutional networks. In CVPR, volume 1, page 3, 2017.\n3\n[19] Mart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene\nBrevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado,\nAndy Davis, Jeffrey Dean, Matthieu Devin, Sanjay\nGhemawat, Ian Goodfellow, Andrew Harp, Geoffrey\nIrving, Michael Isard, Yangqing Jia, Rafal Jozefow-\nicz, Lukasz Kaiser, Manjunath Kudlur, Josh Leven-\nberg, Dan Man´e, Rajat Monga, Sherry Moore, Derek\nMurray, Chris Olah, Mike Schuster, Jonathon Shlens,\nBenoit Steiner, Ilya Sutskever, Kunal Talwar, Paul\nTucker, Vincent Vanhoucke, Vijay Vasudevan, Fer-\nnanda Vi´egas, Oriol Vinyals, Pete Warden, Martin\nWattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang\nZheng.\nTensorFlow: Large-scale machine learning\non heterogeneous systems, 2015. Software available\nfrom tensorﬂow.org. 3\n[20] Franc¸ois Chollet et al. Keras. keras.io, 2015. 3\n[21] Garret\nKenyon\net\nal.\nPetaVision.\nhttps://github.com/PetaVision/OpenPV. 3\n9\n",
  "categories": [
    "cs.LG",
    "cs.CR",
    "cs.CV",
    "cs.NE",
    "stat.ML"
  ],
  "published": "2018-11-17",
  "updated": "2018-11-20"
}