{
  "id": "http://arxiv.org/abs/1811.07253v1",
  "title": "Quantifying Uncertainties in Natural Language Processing Tasks",
  "authors": [
    "Yijun Xiao",
    "William Yang Wang"
  ],
  "abstract": "Reliable uncertainty quantification is a first step towards building\nexplainable, transparent, and accountable artificial intelligent systems.\nRecent progress in Bayesian deep learning has made such quantification\nrealizable. In this paper, we propose novel methods to study the benefits of\ncharacterizing model and data uncertainties for natural language processing\n(NLP) tasks. With empirical experiments on sentiment analysis, named entity\nrecognition, and language modeling using convolutional and recurrent neural\nnetwork models, we show that explicitly modeling uncertainties is not only\nnecessary to measure output confidence levels, but also useful at enhancing\nmodel performances in various NLP tasks.",
  "text": "Quantifying Uncertainties in Natural Language Processing Tasks\nYijun Xiao and William Yang Wang\nUniversity of California, Santa Barbara\n{yijunxiao,william}@cs.ucsb.edu\nAbstract\nReliable uncertainty quantiﬁcation is a ﬁrst step towards\nbuilding explainable, transparent, and accountable artiﬁcial\nintelligent systems. Recent progress in Bayesian deep learn-\ning has made such quantiﬁcation realizable. In this paper, we\npropose novel methods to study the beneﬁts of characterizing\nmodel and data uncertainties for natural language processing\n(NLP) tasks. With empirical experiments on sentiment anal-\nysis, named entity recognition, and language modeling using\nconvolutional and recurrent neural network models, we show\nthat explicitly modeling uncertainties is not only necessary to\nmeasure output conﬁdence levels, but also useful at enhanc-\ning model performances in various NLP tasks.\nIntroduction\nWith advancement of modern machine learning algorithms\nand systems, they are applied in various applications that, in\nsome scenarios, impact human wellbeing. Many of such al-\ngorithms learn black-box mappings between input and out-\nput. If the overall performance is satisfactory, these learned\nmappings are assumed to be correct and are used in real-life\napplications. It is hard to quantify how conﬁdent a certain\nmapping is with respect to different inputs. These deﬁcien-\ncies cause many AI safety and social bias issues with the\nmost notable example being failures of auto-piloting sys-\ntems. We need systems that can not only learn accurate map-\npings, but also quantify conﬁdence levels or uncertainties\nof their predictions. With uncertainty information available,\nmany issues mentioned above can be effectively handled.\nThere are many situations where uncertainties arise when\napplying machine learning models. First, we are uncer-\ntain about whether the structure choice and model param-\neters can best describe the data distribution. This is re-\nferred to as model uncertainty, also known as epistemic un-\ncertainty. Bayesian neural networks (BNN) (Buntine and\nWeigend 1991; Denker and Lecun 1991; MacKay 1992;\n1995; Neal 2012) is one approach to quantify uncertainty as-\nsociated with model parameters. BNNs represent all model\nweights as probability distributions over possible values in-\nstead of ﬁxed scalars. In this setting, learned mapping of\na BNN model must be robust under different samples of\nCopyright c⃝2019, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nweights. We can easily quantify model uncertainties with\nBNNs by, for example, sampling weights and forward in-\nputs through the network multiple times. Quantifying model\nuncertainty using a BNN learns potentially better representa-\ntions and predictions due to the ensemble natural of BNNs.\nIt is also showed in (Blundell et al. 2015) that it is beneﬁ-\ncial for exploration in reinforcement learning (RL) problems\nsuch as contextual bandits.\nAnother situation where uncertainty arises is when col-\nlected data is noisy. This is often the case when we rely\non observations and measurements to obtain the data. Even\nwhen the observations and measurements are precise, noises\nmight exist within the data generation process. Such uncer-\ntainties are referred to as data uncertainties in this paper\nand is also called aleatoric uncertainty (Der Kiureghian and\nDitlevsen 2009). Depending on whether the uncertainty is\ninput independent, data uncertainty is further divided into\nhomoscedastic uncertainty and heteroscedastic uncertainty.\nHomoscedastic uncertainty is the same across the input\nspace which can be caused by systematic observation noise.\nHeteroscedastic uncertainty, on the contrary, is dependent on\nthe input. For example, when predicting the sentiment of a\nYelp review, single-word review “good” is possible to have\n3, 4 or 5-star ratings while a lengthened review with strong\npositive emotion phrases is deﬁnitely a 5-star rating. In the\nrest of the paper, we also refer to heteroscedastic uncertainty\nas input-dependent data uncertainty.\nRecently, there are increasing number of studies investi-\ngating the effects of quantifying uncertainties in different\napplications (Kendall, Badrinarayanan, and Cipolla 2015;\nGal and Ghahramani 2016b; Kendall and Gal 2017; Zhu and\nLaptev 2017). In this paper, we focus on exploring the ben-\neﬁts of quantifying both model and data uncertainties in the\ncontext of various natural language processing (NLP) tasks.\nSpeciﬁcally, we study the effects of quantifying model un-\ncertainty and input-dependent data uncertainty in sentiment\nanalysis, named entity recognition, and language modeling\ntasks. We show that there is a potential performance increase\nwhen including both uncertainties in the model. We also an-\nalyze the characteristics of the quantiﬁed uncertainties.\nThe main contributions of this work are:\n1. We mathematically deﬁne model and data uncertainties\nvia the law of total variance;\narXiv:1811.07253v1  [cs.CL]  18 Nov 2018\n2. Our empirical experiments show that by accounting for\nmodel and data uncertainties, we observe signiﬁcant im-\nprovements in three important NLP tasks;\n3. We show that our model outputs higher data uncertainties\nfor more difﬁcult predictions in sentiment analysis and\nnamed entity recognition tasks.\nRelated Work\nBayesian Neural Networks\nModern neural networks are parameterized by a set of\nmodel weights W. In the supervised setting, for a dataset\nD = {(x1, yi)}N\ni=1, a point estimate for W is obtained by\nmaximizing certain objective function. Bayesian neural net-\nworks (Buntine and Weigend 1991; Denker and Lecun 1991;\nMacKay 1992; 1995; Neal 2012) introduce model uncer-\ntainties by putting a prior on the network parameters p(W).\nBayesian inference is adopted in training aiming to ﬁnd the\nposterior distribution of the parameters p(W|D) instead of\na point estimate. This posterior distribution describes possi-\nble values for the model weights given the dataset. Predic-\ntive function f W(x) is used to predict the corresponding y\nvalue. Given the posterior distribution for W, the function\nis marginalized over W to obtain the expected prediction.\nExact inference for BNNs is rarely available given the\ncomplex nonlinear structures and high dimension of model\nparameters W of modern neural networks. Various ap-\nproximate inference methods are proposed (Graves 2011;\nHern´andez-Lobato and Adams 2015; Blundell et al. 2015;\nGal and Ghahramani 2016a). In particular, Monte Carlo\ndropout (MC dropout) (Gal and Ghahramani 2016a) requires\nminimum modiﬁcation to the original model. Dropouts are\napplied between nonlinearity layers in the network and\nare activated at test time which is different from a regu-\nlar dropout. They showed that this process is equivalent to\nvariational Bayesian approximation where the approximat-\ning distribution is a mixture of a zero mean Gaussian and\na Gaussian with small variances. When sampling dropout\nmasks, model outputs can be seen as samples from the pos-\nterior predictive function f c\nW(x) where c\nW ∼p(W|D). As\na result, model uncertainty can be approximately evaluated\nby ﬁnding the variance of the model outputs from multiple\nforward passes.\nUncertainty Quantiﬁcation\nModel uncertainty can be quantiﬁed using BNNs which\ncaptures uncertainty about model parameters. Data uncer-\ntainty describes noises within the data distribution. When\nsuch noises are homogeneous across the input space, it\ncan be modeled as a parameter. In the cases where such\nnoises are input-dependent, i.e. observation noise varies with\ninput x, heteroscedastic models (Nix and Weigend 1994;\nLe, Smola, and Canu 2005) are more suitable.\nRecently, quantiﬁcations of model and data uncertainties\nare gaining researchers’ attentions. Probabilistic pixel-wise\nsemantic segmentation has been studied in (Kendall, Badri-\nnarayanan, and Cipolla 2015); Gal and Ghahramani (2016b)\nstudied model uncertainty in recurrent neural networks in\nthe context of language modeling and sentiment analysis;\nKendall and Gal (2017) researched both model and data un-\ncertainties in various vision tasks and achieved higher per-\nformances; Zhu and Laptev (2017) used similar approaches\nto perform time series prediction and anomaly detection\nwith Uber trip data. This study focuses on the beneﬁts of\nquantifying model and data uncertainties with popular neu-\nral network structures on various NLP tasks.\nMethods\nFirst of all, we start with the law of total variance. Given a\ninput variable x and its corresponding output variable y, the\nvariance in y can be decomposed as:\nVar(y) = Var (E[y|x]) + E [Var(y|x)]\n(1)\nWe mathematically deﬁne model uncertainty and data un-\ncertainty as:\nUm(y|x) = Var (E[y|x])\n(2)\nUd(y|x) = E [Var(y|x)]\n(3)\nwhere Um and Ud are model and data uncertainties respec-\ntively. We can see that both uncertainties partially explain\nthe variance in the observation. In particular, model un-\ncertainty explains the part related to the mapping process\nE[y|x] and data uncertainty describes the variance inherent\nto the conditional distribution Var(y|x). By quantifying both\nuncertainties, we essentially are trying to explain different\nparts of the observation noise in y.\nIn the following sections, we introduce the methods em-\nployed in this study to quantify uncertainties.\nModel Uncertainty\nRecall that Bayesian neural networks aim to ﬁnd the poste-\nrior distribution of W given the dataset D = {(x1, yi)}N\ni=1.\nWe also specify the data generating process in the regression\ncase as:\ny|W ∼N(f W(x), σ2)\n(4)\nWith the posterior distribution p(W|D), given a new in-\nput vector x∗, the prediction is obtained by marginalizing\nover the posterior:\np(y∗|x∗, D) =\nZ\nW\np\n\u0000y∗|f W(x∗)\n\u0001\np(W|D)dW\n(5)\nAs exact inference is intractable in this case, we can\nuse variational inference approach to ﬁnd an approximation\nqθ(W) to the true posterior p(W|D) parameterized by a dif-\nferent set of weights θ where the Kullback-Leibler (KL) di-\nvergence of the two distributions is minimized.\nThere are several variational inference methods pro-\nposed for Bayesian neural networks (Hern´andez-Lobato and\nAdams 2015; Blundell et al. 2015; Gal and Ghahramani\n2016a). In particular, dropout variational inference method\n(Gal and Ghahramani 2016a), when applied to models with\ndropout layers, requires no retraining and can be applied\nwith minimum changes. The only requirement is dropouts\nhave to be added between nonlinear layers. At test time,\nFigure 1: Illustration of the evaluation process of predicted output and both model uncertainty and data uncertainty. E(y∗|x∗)\ndenotes the expected value of model prediction; Um(y∗) is the model uncertainty with respect to the output; Ud(y∗) is the\ninput-dependent data uncertainty. Dotted arrows represent sampling processes.\ndropouts are activated to allow sampling from the approx-\nimate posterior. We use MC dropout in this study to evaluate\nmodel uncertainty.\nAt test time, we have the optimized approximated pos-\nterior q(W). Prediction distribution can be approximated\nby switching p(W|D) to q(W) in Equation 5 and perform\nMonte Carlo integration as follows:\nE(y∗|x∗) ≈1\nM\nM\nX\nj=1\nf\nc\nWj(x∗)\n(6)\nPredictive variance can also be approximated as:\nVar (y∗) ≈1\nM\nM\nX\nj=1\nf\nc\nWj(x∗)2 −E(y∗|x∗)2 + σ2\n(7)\nwhere c\nWj is sampled from q(W).\nNote here σ2 is the inherent noise associated with the in-\nputs which is homogeneous across the input space. This is\noften considered by adding a weight decay term in the loss\nfunction. We will discuss the modeling of input-dependent\ndata uncertainty in the next section. The rest part of the vari-\nance arises because of the uncertainty about the model pa-\nrameters W. We use this to quantify model uncertainty in\nthe study, i.e.:\nUm(y∗|x∗) = 1\nM\nM\nX\nj=1\nf\nc\nWj(x∗)2 −E(y∗|x∗)2\n(8)\nData Uncertainty\nData uncertainty can be either modeled homogeneous across\ninput space or input-dependent. We take the second option\nand make the assumption that data uncertainty is dependent\non the input. To achieve this, we need to have a model that\nnot only predicts the output values, but also estimates the\noutput variances given some input. In other words, the model\nneeds to give an estimation of Var(y|x) mentioned in Equa-\ntion 3.\nDenote µ(x) and σ(x) as functions parameterized by W\nthat calculate output mean and standard deviation for input\nx (in practice, logarithm of the variance is calculated for an\nimprovement on stability). We make the following assump-\ntion on the data generating process:\ny ∼N\n\u0000µ(x), σ(x)2\u0001\n(9)\nGiven the setting and the assumption, the negative data\nlog likelihood can be written as follows:\nLrgs(W) = −1\nN\nN\nX\ni=1\nlog p(yi|µ(xi), σ(xi))\n= 1\nN\nN\nX\ni=1\n \n1\n2\n\f\f\f\f\nyi −µ(xi)\nσ(xi)\n\f\f\f\f\n2\n+\n1\n2 log σ(xi)2 + 1\n2 log 2π\n\u0013\n(10)\nComparing Equation 10 to a standard mean squared loss\nused in regression, we can see that the model encourages\nhigher variances estimated for inputs where the predicted\nmean µ(xi) is more deviated from the true observation yi.\nOn the other hand, a regularization term on the σ(xi) pre-\nvents the model from estimating meaninglessly high vari-\nances for all inputs. Equation 10 is referred to as learned\nloss attenuation in (Kendall and Gal 2017).\nWhile Equation 10 works desirably for regression, it is\nbased on the assumption that y ∼N(µ(x), σ(x)2). This\nassumption clearly does not hold in the classiﬁcation con-\ntext. We can however adapt the same formulation in the logit\nspace. In detail, deﬁne µ(x) and σ(x) as functions that maps\ninput x to the logit space. Logit vector is sampled and there-\nafter transformed into probabilities using softmax operation.\nThis process can be described as:\nu ∼N\n\u0000µ(x), diag(σ(x)2)\n\u0001\n(11)\np = softmax(u)\n(12)\ny ∼Categorical(p)\n(13)\nCorpus\nSize\nAverage Tokens\n|V |\nClasses\nClass Distribution\nYelp 2013\n335,018\n151.6\n211,245\n5\n.09/.09/.14/.33/.36\nYelp 2014\n1,125,457\n156.9\n476,191\n5\n.10/.09/.15/.30/.36\nYelp 2015\n1,569,264\n151.9\n612,636\n5\n.10/.09/.14/.30/.37\nIMDB\n348,415\n325.6\n115,831\n10\n.07/.04/.05/.05/.08/.11/.15/.17/.12/.18\nTable 1: Summaries of Yelp 2013/2014/2015 and IMDB datasets. |V | represents the vocabulary size.\nwhere diag() function takes a vector and output a diagonal\nmatrix by putting the elements on the main diagonal. Note\nhere in Equation 13, y is a single label. This formulation can\nbe easily extended to multi-way Categorical labels.\nDuring training, we seek to maximize the expected data\nlikelihood. Here we approximate the expected distribution\nfor p using Monte Carlo approximation as follows:\nu(k) ∼N\n\u0000µ(x), diag(σ(x)2)\n\u0001\n(14)\nE[p] ≈1\nK\nK\nX\nk=1\nsoftmax(u(k))\n(15)\nThe negative log-likelihood for the dataset can be written\nas:\nLclf(W) = 1\nN\nN\nX\ni=1\nlog\nK\nX\nk=1\nexp\n \nu(k)\ni,yi −log\nX\nc\nexp u(k)\ni,c\n!\n−log K\n(16)\nwhere ui,c is the c-th element in ui.\nAfter the model is optimized, we use σ(x∗)2 to estimate\nthe data uncertainty given input x∗in the regression case:\nUd(y∗|x∗) = σ(x∗)2\n(17)\nFor classiﬁcation, we use the average variance of the log-\nits as a surrogate to quantify the data uncertainty. This does\nnot directly measures data uncertainty in the output space\nbut can reﬂect to a certain extent the variance caused by the\ninput.\nCombining Both Uncertainties\nTo simultaneously quantify both uncertainties, we can sim-\nply use Equation 10,16 in the training stage and adopt MC\ndropout during evaluation as described in the model uncer-\ntainty section.\nTake the regression setting as an example, prediction can\nbe approximated as:\nE(y∗|x∗) ≈1\nM\nM\nX\nj=1\nµ\nc\nWj(x∗)\n(18)\nModel uncertainty can be measured with:\nUm(y∗|x∗) = 1\nM\nM\nX\nj=1\nµ\nc\nWj(x∗)2 −E(y∗|x∗)2\n(19)\nand data uncertainty is quantiﬁed with:\nUd(y∗|x∗) = 1\nM\nM\nX\nj=1\nσ\nc\nWj(x∗)2\n(20)\nwhere again c\nWj is sampled from q(W). Figure 1 is an il-\nlustration of the evaluation process of predictive value and\ndifferent uncertainty measures.\nExperiments and Results\nWe conduct experiments on three different NLP tasks: sen-\ntiment analysis, named entity recognition, and language\nmodeling. In the following sections, we will introduce the\ndatasets, experiment setups, evaluation metrics for each task,\nand experimental results.\nSentiment Analysis\nConventionally, sentiment analysis is done with classiﬁca-\ntion. In this study, to explore the effect of quantifying un-\ncertainties, we consider both regression and classiﬁcation\nsettings for sentiment analysis. In the regression setting, we\ntreat the class labels as numerical values and aim to predict\nthe real value score given a review document. We introduce\nthe datasets and setups in both settings in this section.\nDatasets\nWe use four large scale datasets containing doc-\nument reviews as in (Tang, Qin, and Liu 2015). Speciﬁcally,\nwe use IMDB movie review data (Diao et al. 2014) and Yelp\nrestaurant review datasets from Yelp Dataset Challenge in\n2013, 2014 and 2015. Summaries of the four datasets are\ngiven in Table 1. Data splits are the same as in (Tang, Qin,\nand Liu 2015; Diao et al. 2014).\nExperiment Setup\nWe implement convolutional neural\nnetwork (CNN) baselines in both regression and classiﬁca-\ntion settings. CNN model structure follows (Kim 2014). We\nuse a maximum vocabulary size of 20,000; embedding size\nis set to 300; three different kernel sizes are used in all mod-\nels and they are chosen from [(1,2,3), (2,3,4), (3,4,5)]; num-\nber of feature maps for each kernel is 100; dropout (Srivas-\ntava et al. 2014) is applied between layers and dropout rate\nis 0.5. To evaluate model uncertainty and input uncertainty,\n10 samples are drawn from the approximated posterior to\nestimate the output mean and variance.\nAdam (Kingma and Ba 2014) is adopted in all experi-\nments with learning rate chosen from [3e-4, 1e-3, 3e-3] and\nweight decay from [3e-5, 1e-4, 3e-4]. Batch size is set to\n32 and training runs for 48 epochs with 2,000 iterations per\nModel\nYelp 2013\nYelp 2014\nYelp 2015\nIMDB\n(RGS MSE)\nBaseline\n0.71\n0.72\n0.72\n3.62\nBaseline + MU\n0.57\n0.55\n0.55\n3.20\nBaseline + DU\n0.84\n0.75\n0.73\n3.74\nBaseline + both\n0.57\n0.54\n0.53\n3.13\nRelative Improvement (%)\n19.7\n25.0\n26.4\n13.5\nTable 2: Test set mean squared error of CNN regressors trained on four sentiment analysis datasets. RGS MSE represents\nregression MSE. Baseline is the baseline CNN model (Kim 2014); MU and DU denote model uncertainty and data uncertainty\nrespectively. Classiﬁcation results have a similar pattern but the improvements are less obvious.\nU.N.       official     Ekeus       heads         for      Baghdad        .\nB-ORG        O         B-PER        O             O        B-LOC         O\nFigure 2: An illustration of the bidirectional LSTM model\nused for named entity recognition. Two dropout layers in-\ndependently sample their masks while masks are the same\nacross time steps.\nepoch for Yelp 2013 and IMDB, and 5,000 iterations per\nepoch for Yelp 2014 and 2015. Model with best performance\non the validation set is chosen to be evaluated on the test set.\nEvaluation\nWe use accuracy in the classiﬁcation setting\nand mean squared error (MSE) in the regression setting to\nevaluate model performances. Accuracy is a standard metric\nto measure classiﬁcation performance. MSE measures the\naverage deviation of the predicted scores from the true rat-\nings and is deﬁned as:\nMSE =\nPN\ni=1(goldi −predictedi)2\nN\n(21)\nResults\nExperiment results are shown in Table 2. We can\nsee that BNN models (i.e. model w/ MU and w/ both) out-\nperform non-Bayesian models. Quantifying both model and\ndata uncertainties boosts performances by 13.5%-26.4% in\nthe regression setting. Most of the performance gain is from\nquantifying model uncertainty. Modeling input-dependent\nuncertainty alone marginally hurts prediction performances.\nThe performances for classiﬁcation increase marginally with\nadded uncertainty measures. We conjecture that this might\nbe due to the limited output space in the classiﬁcation set-\nting.\nModel\nCoNLL 2003\n(F1 SCORE)\nBaseline\n77.5\nBaseline + MU\n76.5\nBaseline + DU\n79.6\nBaseline + both\n78.5\nRelative Improvement (%)\n2.7\nTable 3: Test set F1 scores (%) of bidirectional LSTM tag-\ngers trained on CoNLL 2003 dataset. Baseline is the base-\nline bidirectional LSTM model; MU and DU denote model\nuncertainty and data uncertainty respectively. Modeling data\nuncertainty boosts performances\nNamed Entity Recognition\nWe conduct experiments on named entity recognition (NER)\ntask which essentially is a sequence tagging problem. We\nadopt a bidirectional long-short term memory (LSTM)\n(Hochreiter and Schmidhuber 1997) neural network as the\nbaseline model and measure the effects of quantifying model\nand input-dependent uncertainties on the test performances.\nDatasets\nFor the NER experiments, we use the CoNLL\n2003 dataset (Tjong Kim Sang and De Meulder 2003). This\ncorpus consists of news articles from the Reuters RCV1\ncorpus annotated with four types of named entities: loca-\ntion, organization, person, and miscellaneous. The annota-\ntion scheme is IOB (which stands for inside, outside, begin,\nindicating the position of the token in an entity). The orig-\ninal dataset includes annotations for part of speech (POS)\ntags and chunking results, we do not include these features\nin the training and use only the text information to train the\nNER model.\nExperiment Setup\nOur baseline model is a bidirectional\nLSTM with dropout applied after the embedding layer and\nbefore the output layer. We apply dropout with the same\nmask for all time steps following (Gal and Ghahramani\n2016b). An illustration of the model is shown in Figure 2.\nNote that the dropout mask is the same across time steps.\nDifferent examples in the same mini-batch have different\ndropout masks.\nWord embedding size is 200 and hidden size in each direc-\ntion is 200; dropout probability is ﬁxed at 0.5; other hyper-\nModel\nPTB\n(PPL)\nBaseline\n82.7\nBaseline + MU\n81.3\nBaseline + DU\n80.5\nBaseline + both\n79.2\nRelative Improvement (%)\n4.2\nTable 4: Test set perplexities of LSTM language models\ntrained on PTB dataset. PPL represents perplexity. Baseline\nis the baseline medium two-layer LSTM model in (Zaremba,\nSutskever, and Vinyals 2014); MU and DU denote model un-\ncertainty and data uncertainty respectively.\nparameters related to quantifying uncertainties are the same\nwith previous experiment setups.\nFor training, we use Adam optimizer (Kingma and Ba\n2014). Learn rate is selected from [3e-4, 1e-3, 3e-4] and\nweight decay is chosen from [0, 1e-5, 1e-4]. Training runs\nfor 100 epochs with each epoch consisting of 2,000 ran-\ndomly sampled mini-batches. Batch size is 32.\nEvaluation\nThe performances of the taggers are measured\nwith F1 score:\nF1 = 2 · precision · recall\nprecision + recall\n(22)\nwhere precision is the percentage of entities tagged by the\nmodel that are correct; recall is the percentage of entities in\nthe gold annotation that are tagged by the model. A named\nentity is correct only if it is an exact match of the corre-\nsponding entity in the data.\nResults\nTest set performances of the models trained with\nand without uncertainties are listed in Table 3. We observe\nthat much different from the sentiment analysis case, mod-\nels that quantify data uncertainty improves performances by\n2.7% in F1 score. Quantifying model uncertainty, on the\nother hand, under-performs by approximately 1% absolute\nF1 score. One possible explanation for worse results with\nmodel uncertainty is due to the use of MC dropout and chunk\nbased evaluation. More speciﬁcally, predicted tag at each\ntime step is taken to be the argmax of the average tag prob-\nability across multiple passes with the same inputs. This op-\neration might break some temporal dynamics captured with\na single pass of the inputs.\nLanguage Modeling\nWe introduce the experiments conducted on the language\nmodeling task.\nDatasets\nWe use the standard Penn Treebank (PTB),\na standard benchmark in the ﬁeld. The dataset contains\n887,521 tokens (words) in total.\nHigh DU\nshould game automatic doors !\ni ’ve bought tires from discount tire for years at dif-\nferent locations and have had a good experience , but\nthis location was different . i went in to get some new\ntires with my ﬁanc . john the sales guy pushed a certain\nbrand , speciﬁcally because they were running a rebate\nspecial . tires are tires , especially on a prius (the rest\n134 tokens not shown here due to space)\nLow DU\ngreat sports bar ! brian always goes out of his way to\nmake sure we are good to go ! great people , great\nfood , great music ! great bartenders and even great\nbouncers ! always accommodating ! all the best unk !\ngreat unk burger ! amazing service ! brilliant interior\n! the burger was delicious but it was a little big . it ’s a\ngreat restaurant good for any occasion .\nTable 5: Examples of inputs in Yelp 2013 dataset with high\nand low data uncertainties. They are taken from the top and\nbottom 10 examples with respect to measured data uncer-\ntainty. High DU is around 0.80 and low is around 0.52. Italic\ntokens are highly indicative tokens for higher ratings.\nExperiment Setting\nWe follow the medium model setting\nin (Zaremba, Sutskever, and Vinyals 2014). The model is\na two-layer LSTM with hidden size 650. Dropout rate is\nﬁxed at 0.5. Dropout is applied after the embedding layer,\nbefore the output layer, and between two LSTM layers. Sim-\nilar to the NER setting, dropout mask is the same across time\nsteps. Unlike (Gal and Ghahramani 2016b), we do not apply\ndropout between time steps. Weight tying is also not applied\nin our experiments. Number of samples for MC dropout is\nset to 50.\nEvaluation\nWe use the standard perplexity to evaluate the\ntrained language models.\nResults\nThe results are shown in Table 4. We can ob-\nserve performance improvements when quantifying either\nmodel uncertainty or data uncertainty. We observe less per-\nformance improvements compared to (Gal and Ghahramani\n2016b) possibly due to the fact that we use simpler dropout\nformulation that only applies dropout between layers.\nSummary of Results\nWe can observe from the results that accounting for un-\ncertainties improves model performances in all three NLP\ntasks. In detail, for the sentiment analysis setting with CNN\nmodels, quantifying both uncertainties gives the best per-\nformance and improves upon baseline by up to 26.4%. For\nnamed entity recognition, input-dependent data uncertainty\nimproves F1 scores by 2.7% in CoNLL 2003. For language\nmodeling, perplexity improves 4.2% when both uncertain-\nties are quantiﬁed.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nentropy\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\ndata uncertainty\nFigure 3: Scatter plot of evaluated data uncertainty against\nentropy of annotated NER tag distribution for all tokens in\nCoNLL 2003 dataset. Higher input-dependent data uncer-\ntainties are estimated for input tokens that have higher tag\nentropies.\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nF1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\naverage data uncertainty\nO\nB-LOC\nB-PER\nI-LOC\nI-PER\nB-ORG\nB-MISC\nI-MISC\nI-ORG\nFigure 4: Scatter plot of average evaluated data uncertainty\nagainst test set F1 score for different tags. Higher data un-\ncertainties are observed when predicting tags with lower F1\nscore.\nAnalysis\nIn the previous section, we empirically show that by model-\ning uncertainties we could get better performances for vari-\nous NLP tasks. In this section, we turn to analyze the uncer-\ntainties quantiﬁed by our approach. We mainly focus on the\nanalysis of data uncertainty. For model uncertainty, we have\nsimilar observations to (Kendall and Gal 2017).\nWhat Does Data Uncertainty Measure\nIn Equation 3, we deﬁne data uncertainty as the proportion\nof observation noise or variance that is caused by the inputs.\nConceptually, input-dependent data uncertainty is high if it\nis hard to predict its corresponding output given an input. We\nexplore in both sentiment analysis and named entity recogni-\ntion tasks and analyze the characteristics of inputs with high\nand low data uncertainties measured by our model.\nTable 5 shows examples with high and low data uncertain-\nties taken from the Yelp 2013 test set. Due to space limit,\nwe only show four typical examples. Examples with high\ndata uncertainties are either short or very long with exten-\nsive descriptions of actions instead of opinions. On the other\nhand, examples with low data uncertainties are of relatively\nmedium length and contain large amount of strong opinion\ntokens. These observations are consistent with our intuition.\nFor the CoNLL 2003 dataset, we take all tokens and mea-\nsure their average quantiﬁed data uncertainty. We use the\nfollowing strategy to measure how difﬁcult the prediction\nfor each token is: 1. calculate the distribution of NER tags\nthe token is annotated in the training data; 2. use entropy to\nmeasure the difﬁculty level of the prediction deﬁned as:\nH(p1, p2, · · · , pm) = −\nm\nX\ni=1\npi log pi\n(23)\nwhere p1, p2, · · · , pm is the distribution of NER tags as-\nsigned to a particular token in the training set. The higher\nthe entropy, the more tags a token can be assigned and the\nmore even these possibilities are. For example, in the train-\ning data, the token Hong has been annotated with tag B-\nLOC (ﬁrst token in Hong Kong), B-ORG, B-PER, B-MISC.\nTherefore Hong has a high entropy with respect to its tag\ndistribution. In contrast, the token defended has only been\nassigned tag O representing outside of any named entities.\nTherefore defended has a low entropy of 0.\nWe plot the relationship between the average quantiﬁed\ndata uncertainty and NER tag distribution entropy for the\ntokens in Figure 3. It is clear that for tokens with higher\nentropy values, data uncertainties measured by our model\nare indeed higher.\nWe also analyze the data uncertainty differences among\nNER tags. For each NER tag, we evaluate its test set F1\nscore and average data uncertainty quantiﬁed by our model.\nThe relationship is shown in Figure 4. We observe that when\npredicting more difﬁcult tags, higher average data uncertain-\nties are estimated by the model. These observations indicate\nthat data uncertainty quantiﬁed by our model is highly cor-\nrelated with prediction conﬁdence.\nConclusion\nIn this work, we evaluate the beneﬁts of quantifying un-\ncertainties in modern neural network models applied in the\ncontext of three different natural language processing tasks.\nWe conduct experiments on sentiment analysis, named en-\ntity recognition, and language modeling tasks with convolu-\ntional and recurrent neural network models. We show that\nby quantifying both uncertainties, model performances are\nimproved across the three tasks. We further investigate the\ncharacteristics of inputs with high and low data uncertainty\nmeasures in Yelp 2013 and CoNLL 2003 datasets. For both\ndatasets, our model estimates higher data uncertainties for\nmore difﬁcult predictions. Future research directions include\npossible ways to fully utilize the estimated uncertainties.\nReferences\nBlundell, C.; Cornebise, J.; Kavukcuoglu, K.; and Wierstra,\nD. 2015. Weight uncertainty in neural networks. arXiv\npreprint arXiv:1505.05424.\nBuntine, W. L., and Weigend, A. S. 1991. Bayesian back-\npropagation. Complex systems 5(6):603–643.\nDenker, J. S., and Lecun, Y. 1991. Transforming neural-\nnet output levels to probability distributions. In Advances in\nneural information processing systems, 853–859.\nDer Kiureghian, A., and Ditlevsen, O. 2009. Aleatory or\nepistemic? does it matter? Structural Safety 31(2):105–112.\nDiao, Q.; Qiu, M.; Wu, C.-Y.; Smola, A. J.; Jiang, J.; and\nWang, C. 2014. Jointly modeling aspects, ratings and senti-\nments for movie recommendation (jmars). In Proceedings of\nthe 20th ACM SIGKDD international conference on Knowl-\nedge discovery and data mining, 193–202. ACM.\nGal, Y., and Ghahramani, Z. 2016a. Dropout as a bayesian\napproximation: Representing model uncertainty in deep\nlearning. In international conference on machine learning,\n1050–1059.\nGal, Y., and Ghahramani, Z.\n2016b.\nA theoretically\ngrounded application of dropout in recurrent neural net-\nworks. In Advances in neural information processing sys-\ntems, 1019–1027.\nGraves, A. 2011. Practical variational inference for neu-\nral networks. In Advances in neural information processing\nsystems, 2348–2356.\nHern´andez-Lobato, J. M., and Adams, R. 2015. Probabilis-\ntic backpropagation for scalable learning of bayesian neural\nnetworks. In International Conference on Machine Learn-\ning, 1861–1869.\nHochreiter, S., and Schmidhuber, J. 1997. Long short-term\nmemory. Neural computation 9(8):1735–1780.\nKendall, A., and Gal, Y. 2017. What uncertainties do we\nneed in bayesian deep learning for computer vision?\nIn\nAdvances in neural information processing systems, 5574–\n5584.\nKendall, A.; Badrinarayanan, V.; and Cipolla, R.\n2015.\nBayesian segnet: Model uncertainty in deep convolu-\ntional encoder-decoder architectures for scene understand-\ning. arXiv preprint arXiv:1511.02680.\nKim, Y. 2014. Convolutional neural networks for sentence\nclassiﬁcation. arXiv preprint arXiv:1408.5882.\nKingma, D. P., and Ba, J.\n2014.\nAdam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980.\nLe, Q. V.; Smola, A. J.; and Canu, S. 2005. Heteroscedas-\ntic gaussian process regression. In Proceedings of the 22nd\ninternational conference on Machine learning, 489–496.\nACM.\nMacKay, D. J. 1992. A practical bayesian framework for\nbackpropagation networks. Neural computation 4(3):448–\n472.\nMacKay, D. J. 1995. Probable networks and plausible pre-\ndictionsa review of practical bayesian methods for super-\nvised neural networks.\nNetwork: Computation in Neural\nSystems 6(3):469–505.\nNeal, R. M. 2012. Bayesian learning for neural networks,\nvolume 118. Springer Science & Business Media.\nNix, D. A., and Weigend, A. S. 1994. Estimating the mean\nand variance of the target probability distribution. In Neural\nNetworks, 1994. IEEE World Congress on Computational\nIntelligence., 1994 IEEE International Conference On, vol-\nume 1, 55–60. IEEE.\nSrivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; and\nSalakhutdinov, R. 2014. Dropout: a simple way to prevent\nneural networks from overﬁtting. The Journal of Machine\nLearning Research 15(1):1929–1958.\nTang, D.; Qin, B.; and Liu, T. 2015. Document modeling\nwith gated recurrent neural network for sentiment classiﬁ-\ncation. In Proceedings of the 2015 conference on empirical\nmethods in natural language processing, 1422–1432.\nTjong Kim Sang, E. F., and De Meulder, F. 2003. Introduc-\ntion to the conll-2003 shared task: Language-independent\nnamed entity recognition. In Proceedings of the seventh con-\nference on Natural language learning at HLT-NAACL 2003-\nVolume 4, 142–147. Association for Computational Linguis-\ntics.\nZaremba, W.; Sutskever, I.; and Vinyals, O.\n2014.\nRe-\ncurrent neural network regularization.\narXiv preprint\narXiv:1409.2329.\nZhu, L., and Laptev, N.\n2017.\nDeep and conﬁdent pre-\ndiction for time series at uber. In Data Mining Workshops\n(ICDMW), 2017 IEEE International Conference on, 103–\n110. IEEE.\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG",
    "cs.NE"
  ],
  "published": "2018-11-18",
  "updated": "2018-11-18"
}