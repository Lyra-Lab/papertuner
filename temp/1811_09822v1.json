{
  "id": "http://arxiv.org/abs/1811.09822v1",
  "title": "Object Detection based Deep Unsupervised Hashing",
  "authors": [
    "Rong-Cheng Tu",
    "Xian-Ling Mao",
    "Bo-Si Feng",
    "Bing-Bing Bian",
    "Yu-shu Ying"
  ],
  "abstract": "Recently, similarity-preserving hashing methods have been extensively studied\nfor large-scale image retrieval. Compared with unsupervised hashing, supervised\nhashing methods for labeled data have usually better performance by utilizing\nsemantic label information. Intuitively, for unlabeled data, it will improve\nthe performance of unsupervised hashing methods if we can first mine some\nsupervised semantic 'label information' from unlabeled data and then\nincorporate the 'label information' into the training process. Thus, in this\npaper, we propose a novel Object Detection based Deep Unsupervised Hashing\nmethod (ODDUH). Specifically, a pre-trained object detection model is utilized\nto mining supervised 'label information', which is used to guide the learning\nprocess to generate high-quality hash codes.Extensive experiments on two public\ndatasets demonstrate that the proposed method outperforms the state-of-the-art\nunsupervised hashing methods in the image retrieval task.",
  "text": "Object Detection based Deep Unsupervised\nHashing\nRong-Cheng Tu, Xian-Ling Mao, Bo-Si Feng, Bing-Bing Bian, Yu-shu Ying\nSchool of Computer Science, Beijing Institute of Technology\nNovember 27, 2019\nAbstract\nRecently, similarity-preserving hashing methods have been extensively studied\nfor large-scale image retrieval. Compared with unsupervised hashing, supervised\nhashing methods for labeled data have usually better performance by utilizing\nsemantic label information. Intuitively, for unlabeled data, it will improve the\nperformance of unsupervised hashing methods if we can Ô¨Årst mine some super-\nvised semantic ‚Äôlabel information‚Äô from unlabeled data and then incorporate the\n‚Äôlabel information‚Äô into the training process. Thus, in this paper, we propose a\nnovel Object Detection based Deep Unsupervised Hashing method (ODDUH).\nSpeciÔ¨Åcally, a pre-trained object detection model is utilized to mining supervised\n‚Äôlabel information‚Äô, which is used to guide the learning process to generate high-\nquality hash codes.Extensive experiments on two public datasets demonstrate\nthat the proposed method outperforms the state-of-the-art unsupervised hash-\ning methods in the image retrieval task.\nIntroduction\nWith the rapid growth of image data, approximate nearest neighbour (ANN)\nsearch have attracted more and more attention from researchers in the large\nscale image search area. Among the existing ANN search techniques, similarity-\npreserving hashing methods are advantageous due to their high retrieval eÔ¨É-\nciency and low storage cost. The main idea of hashing methods are to transform\nhigh dimensional data points into a set of compact binary codes, meanwhile,\nmaintain similarity of the original data points. Since the original data points\nare represented by binary codes instead of real valued features, the time and\nmemory cost of searching can be dramatically reduced.\nIn general, data-dependent hashing can be divided into unsupervised [8, 10,\n5, 18] and supervised [33, 21, 32] methods. The unsupervised hashing methods\nmainly utilize the features of images to generate similarity-preserving binary\n1\narXiv:1811.09822v1  [cs.CV]  24 Nov 2018\nPseudo-label: car \nPseudo-label: car \n(b) Unsupervised Hashing + pseudo-label \nHash \nfunction \nHand-crafted or  \nlearnt features \nDissimilar \nlearnt features \nHash \ncode \nSimilar \n(a) Classical Unsupervised Hashing \nùêºùëè \nùêºùëé \nHash \ncode \nHash \nfunction \nObject  \ndetection \nObject  \ndetection \nFigure 1: High-quality similarity-preserving hashing code can be produced by\nutilizing the pseudo-labels mined from images. The block (a) is the workÔ¨Çow\nof the existing unsupervised hashing methods which do not mine the pseudo-\nlabels from images, it is hard for them to judge that the two images Ia and Ib\nare similar. However, in block (b), we use pseudo-labels mined from images to\ntrain hashing models which can easily judge the two image are similar.\ncodes without any supervised imformation. Compared with unsupervised hash-\ning, supervised hashing methods incorporate sematic labels of training data\ninto training process, thus they can perform more remarkably in generating\nsimilarity-preserving binary code. However, in many real applications, there are\nno semantic labels of images that can be used as supervised information. Hence,\nwe just can use the unsupervised hashing methods to tackle the large scale image\nretrieval task in these case.\nIntuitively, if we can detect the objects in images and use their classes as the\npseudo-labels of the images, then we can use the pseudo-labels as ‚Äôsupervised\ninformation‚Äô to guide hash codes learning to obtain batter performance. An\nillustrative example is shown in Figure 1, the block (a) is the procedure of\nexisting unsupervised hashing methods. They use the hand-crafted or learnt\nfeatures as inputs. And they directly use the Euclidean distance between images\nor the similarity between one image and its rotated image to guide the hash\ntraining, which will make the existing unsupervised hashing models to judge\nthat the images Ia and Ib are dissimilar with high possibility. Actually, the\nimages Ia and Ib are similar, that the two images are belong to the class ‚Äôcar‚Äô.\nOn the contrary, in the block (b), if we use an object detection model to get the\npseudo-labels of the two images by detecting objects inside an image and classify\neach object into one of many diÔ¨Äerent classes. And by utilizing the pseudo-labels\nthat both classes of images are ‚Äôcar‚Äô, we can construct pair-wise similarity to\ntrain hashing models and make the hashing models to judge that the two images\nIa and Ib are similar with high possibility.\n2\nInspired by this idea, we propose a novel Object Detection based Deep Un-\nsupervised Hashing model, called ODDUH. In particular, an object detection\nmodel is Ô¨Årst pre-trained on a large database which contains all the tags be-\nlonging to the hashing dataset. Then, we utilize the object detection model to\nmine latent semantic ‚Äôlabel information‚Äô( i.e., pseudo-labels) from images. And\nby taking use of pseudo-labels learned from the pre-trained object detection\nmodel, we deÔ¨Åne a novel similarity criterion called pair-wise percentage similar-\nity inspired by ISDH [32] . Moreover, a shared CNN is introduced to capture\nthe feature representations of images. Finally, we combine the pair-wise per-\ncentage similarity and the learnt feature representations of images to learn hash\nfunctions and generate high-quality similarity-preserving hash codes.\nExtensive experiments on two real-world public datasets illustrate that our\nmethod outperforms the state-of-the-art unsupervised hash methods in image\nretrieval tasks. Our main contributions are outlined as follows:\n‚Ä¢ We propose a novel unsupervised hashing architecture by introducing an\npre-trained object detection model to mining semantic ‚Äôlabels information‚Äô\nfrom images.\n‚Ä¢ Pair-wise percentage similarity is the Ô¨Årst used in unsupervised hash-\ning methods. With the guiding of pair-wise percentage similarity, we can\ngreatly take use of the power of deep models to learn high-quality similarity-\npreserving binary codes. Moreover, the binary codes can preserve ranking\nimformation.\n‚Ä¢ Experiments have shown that the proposed method can perform batter\nthan the existing unsupervised hashing methods in large-scale image re-\ntrieval tasks.\n2.Related Work\nSimilarity-preserving Hashing\nGenerally, existing hashing methods can be divided into data-independent hash-\ning and data-dependent hashing. For data-independent hashing methods, the\nhashing functions are typically randomly generated without any training data.\nThe representative data-independent methods include Locality Sensitivity Hash-\ning (LSH) [6] and its variants [2, 15]. For data-dependent hashing methods, they\ncan achieve better accuracy with shorter codes by learning hash functions from\ntraining data. Futhermore, data-dependent can be further classiÔ¨Åed into two\ncategories: supervised [28, 20, 26] and unsupervised [6, 12, 5] methods. The\nsupervised methods can achieve remarkable performance by utilizing labeled\ndata to learn hashing functions. And the label information in supervised hash-\ning methods can be used in the following three ways: point-wise label, pair-\nwise labels and triplet. Representative point-wise label based methods include:\nsupervised discrete hashing (SDH) [24]. Representative pair-wise labels based\ndeep hashing methods include: Deep Supervised Hashing with Pairwise (DPSH)\n3\n[17], Deep Supervised Discrete Hashing (DSDH) [16], Supervised Hierarchical\nDeep Hashing (SHDH) [28]. Representative triplet based deep hashing methods\ninclude: Deep Semantic-preserving and Ranking-based Hashing (DSRH) [31],\nDeep Semantic Hashing with GANs (DSH-GANs) [21].\nThe unsupervised hashing can be divided into traditional unsupervised hash-\ning methods and deep unsupervised hashing methods. The traditional unsuper-\nvised hashing methods used hand-crafted features and shallow hash functions\nto obtain binary hash code. Lots of algorithms in this category have been pro-\nposed, including Spectral Hashing (SH) [30], Iterative Quantization (ITQ) [8].\nHowever, limited by the hand-crafted features and shallow hash functions, it is\nhard for them to deal with complex and high dimensional real-world data and\nkeep the semantic similarity between original data in the binary hash codes.\nThe deep unsupervised hashing methods utilize deep architecture to learn hash\ncode. Among the deep unsupervised hashing methods, Deepbit [18] get rota-\ntion invariant and balanced binary hash codes by deÔ¨Åned a quantization loss.\nUnsupervised triplet hashing (UTH) [10] employs an unsupervised triplet loss\nto get balanced hash codes. HashGAN [5] generate compact hash codes by a\ngenerative adversarial hashing network.\nHowever, few existing unsupervised hashing methods take a good use of the\nlatent sematic ‚Äôlabel information‚Äô in the images. Thus, in this paper, we propose\na novel deep unsupervised hashing model based on object detection to gener-\nate high-quality hash codes by mining the latent semantic ‚Äôlabel information‚Äô\ncontained in images and incorporating the ‚Äôlabel information‚Äô into the training\nprocess.\nObject Detection\nObject detection has been studied widely for locating an object inside the image\nand classifying the object into one of many diÔ¨Äerent categories. And object de-\ntection can be simple categorized into two categories: classical models and deep\nlearning models. In the classical models, one of the most popular is Viola-Jones\nframework [13], and it works by generating diÔ¨Äerent (possibly thousands) simple\nbinary classiÔ¨Åers using Haar features. However, with the growing success of deep\nlearning, deep learning models are now state of the art in object detection, and\nmany studies have been published about deep object detection. In this category,\nsome models are based on region proposal which can solve the sliding windows\nproblem, such as R-CNN [7]; and some models are based on regression which\ncan have a fast detection speed, e.g., YOLO [22], SSD [19].\nMost of the above approaches can have a good detection eÔ¨Äect, and can\nmine the latent semantic ‚Äôlabel information‚Äô in images, which is exactly what\nour hashing architecture need. Thus, we can chose one of state-of-the-art object\ndetection methods such as YOLOv2 [23] as a part of our hashing architecture.\n4\n‚Ä¶ \n‚Ä¶ \nFeature Learning \nWeight Sharing \nObject detection model \n4096 \n4096 \n5 CNN layers \nFully-connected layers \nHash Function Learning \nHashing layer \nPseudo-label vector \nPercentage \nSimilarity \nMatrix \nObject \nFunction \nLearning \nMining Latent Semantic  ‚ÄòLabel Information‚Äô  \nk \nFigure 2: The ODDUH learning framework. The Mining Latent Semantic ‚ÄôLabel\nInformation‚Äô is a pre-trained object detection model. It is used to get the pseudo-\nlabels. A shared CNN is implemented for learning image fearture representations\nin the Feature Learning part. In Hash Function Learning part, a pair-wise loss\nfunction with Percentage Similarity Matrix is minimized to get the optimal hash\nfunction\nObject Detection based Deep Unsupervised Hash-\ning Network\nIn this section, we will present the proposed Object Detection based Deep Un-\nsupervised Hashing Network (ODDUH) in detail.\nNotation\nSuppose a dataset has n images X = {xi}n\ni=1, and the ith image is xi. The goal\nof similarity-preserving hashing is to learn a mapping H : xi ‚Üíbi ‚àà{‚àí1, 1}k,\nwhere k is the length of hashing codes, such that an input image xi will be\nencoded into a k-bit binary code bi.\nThe Architecture of ODDUH\nAs shown in Figure 2, our architecture consists of three parts: mining latent\nsemantic ‚Äôlabel information‚Äô, feature learning and hash function learning.\nIn the mining latent semantic ‚Äôlabel information‚Äô part, the ODDUH uses a\npre-trained object detection model named YOLOv2 [23] to mining the latent\nsemantic ‚Äôlabel information‚Äô in images. Note that other state-of-the-art object\ndetection models can also be used such as SSD [19] and Mask R-CNN [9]\nThe feature learning part includes a convolutional neural network (CNN)\ncomponent which has Ô¨Åve convolutional layers and two fully-connected layers.\nWhat‚Äôs more, all the seven layers are the same as those of CNN-F network in\n5\nAlexnet [14].Note that other CNN architectures can also be used here, such as\nVGG [25] and GoogLeNet [27].\nThe hash function learning part is a hashing layer which has k units. Fur-\nthermore, k is the length of hash code, and the hashing functions are learnt by\nthe hashing layer. Eventually, we use element-wise sign function sgn(¬∑), which\nreturns 1 if the element is positive and returns ‚àí1 otherwise, to process the\noutputs of the hashing layer and get the binary code b.\nSimilarity DeÔ¨Ånition\nIn ODDUH, we use an object detection model to mine the objects in images\nand get their classes (i.e., pseudo-labels). And for many images, more than one\npesudo-labels will be mined. Thus, the unlabeled training dataset will become\na ‚Äômutil-label‚Äô dataset. In oder to take a good use of the mined semantic ‚Äôlable\ninformation‚Äô, inspired by ISDH [32], the pair-wise percentage similarity is deÔ¨Åned\nas:\nsij =\n‚ü®li, lj‚ü©\n‚à•li‚à•2‚à•lj‚à•2\n(1)\nwhere ‚ü®li, lj‚ü©calculate the inner product and li ‚àà{0, 1}c is the pseudo-label\nvector of xi, where c is the total number of classes that pseudo-labels belong to.\nIf ith image xi has the jth pseudo-label, then lij = 1, else lij = 0.\nBy incorporating the pair-wise percentage similarity into the training pro-\ncess, the learnt binary codes B = {bi}n\ni=1 can preserve the similarity in S =\n{sij|i, j ‚àà{1, 2, . . . , n}, sij ‚àà[0, 1]}. More speciÔ¨Åcally, if sij = 0, the binary\ncodes bi and bj should have large Hamming distance; if sij = 1, the binary\ncodes bi and bj should have a small Hamming distance; otherwise, the binary\ncodes bi and bj should have a suitable Hamming distance complying with the\nsimilarity sij.\nObjective Fuction\nGiven the binary codes B = {bi}n\ni=1 for all the images, we can deÔ¨Åne the\nlikelihood of the pair-wise percentage similarity sij as:\np(sij|B) =\nÔ£±\nÔ£≤\nÔ£≥\nœÉ(Œ®ij),\nsij = 1,\n1 ‚àíœÉ(Œ®ij),\nsij = 0,\n1 ‚àí(sij ‚àíœÉ(Œ®ij)),\notherwise.\n(2)\nwhere Œ®ij = 1\n2bT\ni bj, and œÉ(Œ®ij) =\n1\n1+e‚àíŒ®ij . When sij = 0 or 1, we take the\nnegative log-likelihood of the observed pair-wise labels in S to measure the pair-\nwise similarity loss , and when 0 < sij < 1, we take mean square error to measure\nthe pair-wise similarity loss. Thus, the pair-wise similarity loss function can be\n6\ndeÔ¨Åned as:\nL1 =\nX\nsij‚ààS\n[‚àíŒ± ¬∑ Iij(sijlog(œÉ(Œ®ij))\n+ (1 ‚àísij)log(1 ‚àíœÉ(Œ®ij)))\n+ (1 ‚àíIij)(sij ‚àíœÉ(Œ®ij))2]\n=\nX\nsij‚ààS\n[Œ± ¬∑ Iij(log(1 + eŒ®ij) ‚àísijŒ®ij)\n+ (1 ‚àíIij)(sij ‚àíœÉ(Œ®ij))2]\n(3)\nwhere Œ± is a hyper-parameter. Iij is used to denote two conditions, Iij = 1 when\nthe pseudo-labels of ith image and the pseudo-labels of jth image are completely\nsimilar or dissimilar, i.e., sij = 1 or 0, and Iij = 0 when the pseudo-labels of ith\nimage and the pseudo-labels of jth image are partly similar, i.e., 0 < sij < 1.\nBy minimizing Eq. (3), we can make the hamming distance between two\ncompletely similar points as small as possible, and simultaneously make the\nhamming distance between two dissimilar points as large as possible. Meanwhile,\nwe can make the partly similar image xi and image xj have the suitable hamming\ndistance complying with the similarity sij.\nHowever, Eq. (3) is a discrete optimization problem, which is diÔ¨Écult to\nsolve. Following previous work [17], we reformulated Eq. (3) as:\nL2 =\nX\nsij‚ààS\n[Œ± ¬∑ Iij(log(1 + eŒòij) ‚àísijŒòij)\n+ (1 ‚àíIij)(sij ‚àíœÉ(Œòij))2]\n(4)\nwhere Œòij = 1\n2uT\ni uj. ui ‚ààRk is the outputs of hashing layer: ui = WT F(xi; Œ∏)+\nv, where the mapping F : Rd ‚ÜíR4096 is parameterized by Œ∏ and Œ∏ represents\nthe parameters of the seven layers of network in the feature learning part. W ‚àà\nR4096√ók is the weight matrix to be learnt at the hashing layer, v ‚ààRk is the\nbias. Due to the ui is not the binary codes, we used a quantization loss to make\nui to be close to binary codes. The quantization loss is deÔ¨Åned as:\nLq =\nn\nX\ni\n||bi ‚àíui||2\n2\n(5)\nThen, by connecting the pseudo-label pair-wise similarity loss and quantization\nloss, the Ô¨Ånal objective function can be deÔ¨Åned as:\nL = L2 + Œ≤Lq\n(6)\nwhere Œ≤ is a hyper-parameter.\nLearning\nIn our method, the parameters containing B, W, Œ∏, v need to be learnt, during\nthe training phase. A mini-batch gradient descent method is used for learning.\n7\nMoreover, we design an alternating method for learning. More speciÔ¨Åcally, we\noptimize B with W, Œ∏, v Ô¨Åxed and optimize W, Œ∏, v with B Ô¨Åxed.\nThe bi can be directly optimized as follows:\nbi = sgn(ui) = sgn(WT F(xi; Œ∏) + v)\n(7)\nFor the other parameters W, Œ∏, v, standard back-propagation algorithm is\nused for learning. Especially, we are able to compute the derivatives of the loss\nfunction about ui as follows:\n‚àÇL\n‚àÇui\n=\nX\nj:sij‚ààS\n[1\n2Œ± ¬∑ Iij(œÉ(Œòij) ‚àísij)\n+ (1 ‚àíIij)œÉ(Œòij)(1 ‚àíœÉ(Œòij))(sij ‚àíœÉ(Œòij))]uj\n+\nX\nj:sji‚ààS\n[1\n2Œ± ¬∑ Iji(œÉ(Œòji) ‚àísji)\n+ (1 ‚àíIji)œÉ(Œòji)(1 ‚àíœÉ(Œòji))(sji ‚àíœÉ(Œòji))]uj\n+ 2Œ≤(ui ‚àíbi)\n(8)\nThen, we can use the standard back-propagation algorithm to update W, Œ∏ and\nv with Eq. (8):\n‚àÇL\n‚àÇW = F(xi; Œ∏)( ‚àÇL\n‚àÇui\n)T\n(9)\n‚àÇL\n‚àÇF(xi; Œ∏) = W ‚àÇL\n‚àÇui\n(10)\n‚àÇL\n‚àÇv = ‚àÇL\n‚àÇui\n(11)\nThe outline of the proposed method is described in Algorithm 1.\nExperiments\nDataset and Baseline\nWe conduct experiments on two public benchmark datasets: Pascal VOC 2007 1\n[4] and BMVC 2009 2 [1]. Pascal VOC 2007 consists of 9,963 multi-label images.\nThere are 20 object classes in this dataset. On average, each image is annotated\nwith 1.5 labels. BMVC 2009 contains 96,378 images collected from Flickr. Each\nimage in the dataset is associated with one or multiple labels in 20 semantic\nconcepts.\nOur proposed method is an unsupervised method, thus we compare our\nmethod with eight calssical and state-of-the-art unsupervised hashing methods\nincluding: LSH [6], ITQ [8], SH [30], PCAH [29], SGH [11], UH BDNN [3],\n1http://host.robots.ox.ac.uk/pascal/VOC/voc2007/\n2http://pascal.inrialpes.fr/data2/flickr-bmvc2009/\n8\nAlgorithm 1 Learning algorithm for ODDUH\nRequire: Training images X = {xi}n\ni=1, the max iterative count E, the size of\nmini-batch(default 128), the length of hash codes K.\nEnsure: The hash codes for of images.\n1: Initialize the weights and bias of the Hashing model.\n2: Initialize all the hyper-parametersŒ±, Œ≤ and learning rate r as 2, 100, 0.01\nrespectively.\n3: Utilize pre-trained object detection model to get pseudo label vectors of all\nthe images.\n4: S ‚Üêusing Eq. (1), S ‚ààRN√óN\n5: repeat\n6:\nUpdate r ‚Üêr/10 every 50 iterations empirically.\n7:\nRandomly sample a mini-batch of images from X, and for each image xi\n, perform as follows:\n8:\nCalculate ui = WT F(xi; Œ∏) + v;\n9:\nCalculate the hash code with Eq. (7);\n10:\nUpdate the parameters {W, Œ∏, v} by back propagation with Eq. (9), Eq.\n(10) and Eq. (11), respectively.\n11: until Up to E\n12: Use Eq. (7) calculate the hash codes of all the images.\nUTH [10], HashGAN [5], where LSH, SH, ITQ, PCAH and SGH are traditional\nunsupervised methods and the other three are deep unsupervised methods. Note\nthat the Ô¨Åve traditional unsupervised hashing methods use hand-crafted features\nas inputs. And each image in Pascal VOC 2007 and BMVC 2009 is represented\nby a 512-dimensional GIST vector. For the deep unsupervised hashing method\nUH BDNN, it use the outputs of fc7 layer in AlexNet as image representation.\nAnd for the other two deep unsupervised hashing methods and our proposed\nmethod, we resize all the images to be 224 √ó 224 pixels and then directly use\nthe raw image pixels as input. When carry out experiments on the two datasets\nrespectively, we randomly select 2,000 images as test set and the left images\nas training dataset. Moreover, we also conduct the experiments by using the\noutputs of fc7 layer in AlexNet [14] as image representation in the Ô¨Åve traditional\nhashing approaches and denote them as LSH+CNN, SH+CNN, ITQ+CNN and\nPCAH+CNN, respectively.\nImplementation details\nFor the object detection component, we choose YOLOv2 [23]. And it is pre-\ntrained in COCO 2014 dataset which contains 81 object classes. Please note\nthat all the object classes contained in Pascal VOC 2007 and BMVC 2009 are\nsubdet of the 81 object classes. For the hash code learning model formed by the\nfeature learning part and hash function learning part, all the weights and bias\nare learned via back-propagation algorithm. Furthermore, the weights and bias\n9\nin the feature learning part are initialized as the values pre-trained in Alexnet\n[14]. We adopt SGD with a mini-batch size of 128 as our optimization algorithm.\nThe learning rate is initialized as 0.01. The hyper-parameters Œ±, Œ≤ in ODDUH\nare empirically set as 2 and 100, respectively. We will discuss the eÔ¨Äect of Œ±, Œ≤\nin the followed subsection. And the learning rate is adjusted to one tenth of the\ncurrent learning rate every one third of epoches.\nEvaluation criterion\nTo verify the eÔ¨Äectiveness of hash code, we evaluate the image retrieval qual-\nity for diÔ¨Äerent methods by Average Cumulative Gains (ACG), Normalized\nDiscounted Cumulative Gains (NDCG), Mean Average Precision (MAP) and\nWeighted Mean Average Precision (W-MAP).\nACG@n represents the average of similarities between the query image and\nthe top n retrieved images, which can be calculated as:\nACG@n =\nn\nX\nj=1\nr(j)\nn\n(12)\nwhere r(j) is deÔ¨Åned as the number of shared labels between the query image\nand the jth retrieved image.\nNDCG is a widely used evaluation metric in information retrieval. Given a\nquery image, the DCG score of top n retrieved images is deÔ¨Åned as:\nDCG@n =\nn\nX\nj=1\n2r(j) ‚àí1\nlog(i + 1)\n(13)\nThen, the normalized DCG (NDCG) score at the position n can be calculated\nbyNDCG@n = DCG@n\nZn\n, where Zn is the maximum value of DCG@n, constrain-\ning the value of NDCG in the range [0,1].\nMAP, a standard evaluation metric for information retrieval, is the mean of\naverage precision for each query. It is deÔ¨Åned as:\nMAP =\nn\nX\nj=1\nPj\np(j)\nN\n(14)\nwhere Pj = R(j)\nj , R(j) represents the number of relevant images within the top j\nimages. p(j) is a indicator function, if the image at the position j shares at least\none label with the query image, p(j) is 1; otherwise p(j) is 0. N represents the\ntotal number of relevant images, i.e., it shares at least one label with the query\nimage.\nThe deÔ¨Ånition of W-MAP is similar with MAP, deÔ¨Åned as:\nWMAP =\nn\nX\nj=1\nACG@j p(j)\nN\n(15)\n10\nTable 1: Results on the Pascal VOC 2007. The ranking results are measured by\nNDCG, ACG, WMAP, and MAP@N (N=1000, i.e., the values are calculated\nbased on the top 1000 returned neighbors). The best results for each category\nare shown in boldface\nMethods\nMAP@1000\nWMAP@1000\nNDCG@1000\nACG@1000\n12bits\n24bits\n36bits\n48bits\n12bits\n24bits\n36bits\n48bits\n12bits\n24bits\n36bits\n48bits\n12bits\n24bits\n36bits\n48bits\nLSH\n0.2676\n0.2875\n0.2916\n0.2877\n0.2881\n0.3115\n0.3168\n0.3123\n0.2230\n0.2379\n0.2436\n0.2407\n0.2821\n0.2998\n0.3009\n0.2968\nSH\n0.3071\n0.3021\n0.3028\n0.3023\n0.3337\n0.3287\n0.3299\n0.3299\n0.2568\n0.2514\n0.2527\n0.2530\n0.3131\n0.3074\n0.3071\n0.3055\nPCAH\n0.2884\n0.2802\n0.2783\n0.2778\n0.3124\n0.3039\n0.3018\n0.3013\n0.2384\n0.2320\n0.2307\n0.23.5\n0.2982\n0.2883\n0.2849\n0.2837\nITQ\n0.2879\n0.3086\n0.3137\n0.3223\n0.3110\n0.3345\n0.3404\n0.3509\n0.2366\n0.2584\n0.2620\n0.2718\n0.2924\n0.3191\n0.3258\n0.3358\nSGH\n0.3028\n0.3081\n0.3073\n0.3107\n0.3288\n0.3358\n0.3350\n0.3395\n0.2559\n0.2611\n0.2614\n0.2644\n0.3052\n0.3082\n0.3065\n0.3089\nLSH+CNN\n0.2924\n0.3351\n0.3611\n0.3694\n0.3226\n0.3737\n0.4001\n0.4189\n0.2502\n0.2875\n0.3030\n0.3142\n0.2993\n0.3314\n0.3505\n0.3542\nSH+CNN\n0.4497\n0.4454\n0.4585\n0.4587\n0.5122\n0.5033\n0.5162\n0.5160\n0.3927\n0.3757\n0.3780\n0.3731\n0.4065\n0.3837\n0.3853\n0.3800\nPCAH+CNN\n0.4892\n0.4914\n0.4890\n0.4848\n0.5515\n0.5514\n0.5486\n0.5439\n0.4337\n0.4185\n0.4066\n0.3961\n0.4454\n0.4207\n0.4067\n0.3962\nITQ+CNN\n0.5606\n0.5886\n0.6006\n0.6070\n0.6429\n0.6777\n0.6927\n0.6996\n0.5137\n0.5266\n0.5323\n0.5368\n0.5328\n0.5362\n0.5366\n0.5391\nSGH+CNN\n0.2575\n0.2653\n0.2730\n0.2839\n0.2773\n0.2871\n0.2955\n0.3083\n0.2129\n0.2198\n0.2254\n0.2339\n0.2675\n0.2718\n0.2748\n0.2789\nUH BDNN\n0.5572\n0.5795\n0.5851\n0.5915\n0.6388\n0.6639\n0.6700\n0.6781\n0.5080\n0.5132\n0.5110\n0.5115\n0.5188\n0.5168\n0.5101\n0.5067\nUTH\n0.5389\n0.5468\n0.5561\n0.5634\n0.6192\n0.6286\n0.6427\n0.6451\n0.4856\n0.4921\n0.4994\n0.5012\n0.4961\n0.4979\n0.5006\n0.5013\nHashGAN\n0.4606\n0.4672\n0.4711\n0.4783\n0.5114\n0.5201\n0.5263\n0.5310\n0.4115\n0.4183\n0.4214\n0.4240\n0.4197\n0.4246\n0.4293\n0.4303\nODUDH\n0.6946\n0.7335\n0.7538\n0.7604\n0.7449\n0.7871\n0.8083\n0.8321\n0.5979\n0.6162\n0.6249\n0.6441\n0.6206\n0.6284\n0.6324\n0.6512\nTable 2: Results on the BMVC 2009. The ranking results are measured by\nNDCG, ACG, WMAP, and MAP@N (N=5000, i.e., the values are calculated\nbased on the top 5000 returned neighbors). The best results for each category\nare shown in boldface\nMethods\nMAP@5000\nWMAP@5000\nNDCG@5000\nACG@5000\n12bits\n24bits\n36bits\n48bits\n12bits\n24bits\n36bits\n48bits\n12bits\n24bits\n36bits\n48bits\n12bits\n24bits\n36bits\n48bits\nLSH\n0.1393\n0.1494\n0.1539\n0.1494\n0.1495\n0.1602\n0.1652\n0.1604\n0.1070\n0.1136\n0.1174\n0.1143\n0.1459\n0.1543\n0.1582\n0.1536\nSH\n0.1656\n0.1629\n0.1641\n0.1668\n0.1785\n0.1756\n0.1768\n0.1796\n0.1247\n0.1221\n0.1235\n0.1252\n0.1706\n0.1664\n0.1677\n0.1694\nPCAH\n0.1452\n0.1464\n0.1477\n0.1499\n0.1562\n0.1575\n0.1588\n0.1614\n0.1108\n0.1110\n0.1116\n0.1125\n0.1513\n0.1516\n0.1517\n0.1533\nITQ\n0.1356\n0.1423\n0.1599\n0.1618\n0.1431\n0.1508\n0.1719\n0.1738\n0.0996\n0.1081\n0.1206\n0.1237\n0.1326\n0.1439\n0.1608\n0.1656\nSGH\n0.1681\n0.1698\n0.1715\n0.1724\n0.1807\n0.1825\n0.1841\n0.1850\n0.1280\n0.1287\n0.1298\n0.1305\n0.1696\n0.1700\n0.1700\n0.1701\nLSH+CNN\n0.1621\n0.1925\n0.1954\n0.2133\n0.1750\n0.2078\n0.2112\n0.2304\n0.1233\n0.1425\n0.1435\n0.1544\n0.1657\n0.1887\n0.1878\n0.1976\nSH+CNN\n0.2667\n0.2805\n0.2798\n0.2877\n0.2880\n0.3041\n0.3032\n0.3122\n0.1845\n0.1981\n0.1931\n0.1971\n0.2452\n0.2460\n0.2379\n0.2429\nPCAH+CNN\n0.2991\n0.3076\n0.3063\n0.3090\n0.3236\n0.3336\n0.3321\n0.3354\n0.2215\n0.2187\n0.2122\n0.2105\n0.2803\n0.2720\n0.2616\n0.2584\nITQ+CNN\n0.3330\n0.3627\n0.3712\n0.3781\n0.3617\n0.3942\n0.4034\n0.4123\n0.2527\n0.2684\n0.2726\n0.2765\n0.3223\n0.3339\n0.3370\n0.3415\nSGH+CNN\n0.1344\n0.1423\n0.1493\n0.1575\n0.1444\n0.1530\n0.1606\n0.1697\n0.1033\n0.1079\n0.1117\n0.1152\n0.1413\n0.1460\n0.1496\n0.1527\nUH BDNN\n0.3442\n0.3736\n0.3828\n0.3960\n0.3737\n0.4049\n0.4148\n0.4289\n0.2605\n0.2768\n0.2811\n0.2876\n0.3262\n0.3405\n0.3439\n0.3500\nUTH\n0.3011\n0.3083\n0.3102\n0.3138\n0.3375\n0.3417\n0.3481\n0.3495\n0.2276\n0.2292\n0.2317\n0.2343\n0.2743\n0.2835\n0.2891\n0.2931\nHashGAN\n0.2711\n0.2790\n0.2866\n0.2935\n0.2930\n0.3052\n0.3121\n0.3167\n0.2028\n0.2091\n0.2131\n0.2177\n0.2496\n0.2528\n0.2589\n0.2635\nODUDH\n0.4091\n0.4228\n0.4254\n0.4313\n0.4428\n0.4570\n0.4582\n0.4651\n0.3205\n0.3259\n0.3274\n0.3306\n0.4047\n0.4085\n0.4093\n0.4130\nFurthermore, the three evaluation criterions WMAP, NDCG and ACG are\nusually used to measure the ranking quality of hashing models. Beacuse the\ngreater value of WMAP or NDCG means that the related items in the retrieved\nresult list have higher ranks. And the larger value of ACG indicates that the\nimages in the retrieved result list are more similar to the query image.\nExperimental results\nTable 1 summarizes the comparative results of diÔ¨Äerent hashing methods over\nPascal VOC 2007. And table 2 shows the performance comparison of diÔ¨Äerent\nhashing methods over BMVC 2009. In general, It can be found that our proposed\nmethod substantially outperforms the other unsupervised hashing methods for\ndiÔ¨Äerent length of hash code. In particular, on Pascal VOC 2007, comparing with\nthe best traditional competitor ITQ+CNN on 48-bits, the results of ODDUH\nhave a relative increase of 25.3% on MAP, 18.9% on WMAP, 20.0% on NDCG,\n20.8% on ACG, and comparing with the best deep unsupervised competitor\nUH BDNN on 48-bits, the results of ODDUH have a relative increase of 25.9%\non MAP, 22.7% on WMAP, 25.9% on NDCG, 28.5% on ACG. Moreover, on\nBMVC 2009, comparing with the competitor UH BDNN on 48-bits, the results\nof ODDUH have a relative increase of 8.9% on MAP, 8.4% on WMAP, 15.0%\n11\n12\n24\n36\n48\n# Hash bits\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\nPrecision@n(n=1000)\nPascal VOC 2007\nODDUH\nLSH\nSH\nPCAH\nITQ\nSGH\nLSH+CNN\n+SHCNN\nPCAH+CNN\nITQ+CNN\nSGH+CNN\nUH_BNNH\nUTH\nHashGAN\n(a)\n12\n24\n36\n48\n# Hash bits\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nPrecision@n(n=5000)\nBMVC 2009\nODDUH\nLSH\nSH\nPCAH\nITQ\nSGH\nLSH+CNN\n+SHCNN\nPCAH+CNN\nITQ+CNN\nSGH+CNN\nUH_BNNH\nUTH\nHashGAN\n(b)\nFigure 3: Precision@n over (a) Pascal VOC 2007 and (b) BMVC 2009\n1.0\n1.5\n2.0\n2.5\n3.0\n5.0\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nMAP@n\nPascal VOC 2007\nBMVC 2009\n(a) Sensitivity to hyper-parameter Œ± over two\ndataset with Œ≤ = 100\n50\n80\n100\n120\n150\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nMAP@n\nPascal VOC 2007\nBMVC 2009\n(b) Sensitivity to hyper-parameter Œ≤ over Pas-\ncal VOC 2007 with Œ± = 2\nFigure 4: Sensitivity to hyper-parameters\non NDCG, 18.0% on ACG. And as the growth of the three evaluation criterions\nWMAP, NDCG, ACG illustrate that the ODDUH can preserve more ranking\ninformation in the binary codes than the baselines. Furthermore,the results ob-\nviously indicates that pseudo-labels mined from images are more advantageous\nthan the Euclidean distance between images or the similarity between one im-\nage and its rotated images to generate similarity-preserving hashing codes. In\naddition, most traditional unsupervised methods with image representations ex-\ntracted from deep CNN architecture perform better than these methods with\nGIST features, which proves that the learnt representations by deep network\nfrom raw images are more superior than hand-crafted features in hash learning\nprocedure.\nThe precision@n ( the precision value is calculated based on the top n re-\nturned neighbors) curves at diÔ¨Äerent length of hash code are also showed in\nFigure 3. Figure 3(a) and Figure 3(b) exhibit the precision@n curves over Pas-\ncal VOC 2007 and BMVC 2009, respectively. And both Figure 3(a) and Figure\n12\n3(b) show that our ODDUH model perform batter than baselines. Also, the pre-\ncisions for most of the baselines drop when the length of hash codes increases.\nContrarily, the precision of ODDUH is still growing, which means ODDUH is\nmore stable.\nSensitivity to Hyper-Parameters\nFigure 4(a) shows the eÔ¨Äect of the hyper-parameter Œ± on 48 bits over Pascal\nVOC 2007 and BMVC 2009. It can be found that ODDUH is not sensitive to Œ±\non both datasets. For instance, ODDUH can achieve good performance on both\ndatasets with 1 ‚â§Œ± ‚â§5. Figure 4(b) shows the eÔ¨Äect of the hyper-parameter Œ≤\non 48 bits over Pascal VOC 2007 and BMVC 2009. Also, ODDUH is not sensitive\nto Œ≤ in a large range. For example, ODDUH can achieve good performance on\nboth datasets with 80 ‚â§Œ≤ ‚â§150. And we can also obtain similar conclusion on\nother length of hash codes for both hyper-Parameters Œ± and Œ≤.\nConclusion\nIn this paper, we have proposed a novel Object Detection based Deep Unsuper-\nvised Hashing method, called, for unlabeled data. To the best of our knowledge,\nODDUH is the Ô¨Årst method which utilize object detection model to mine se-\nmantic ‚Äôlabel information‚Äô from images. By incorporating the semantic ‚Äôlabel\ninformation‚Äô into the training process, the learnt hashing functions can gen-\nerat high-quality similarity-preserving hash codes. Extensive experiments on\ntwo real-world public datasets have shown that the proposed ODDUH method\ncan outperform other methods to achieve the state-of-the-art performance in\nimage retrieval applications.\nReferences\n[1] Moray Allan and Jakob Verbeek. Ranking user-annotated images for multi-\nple query terms. In BMVC 2009-British Machine Vision Conference, pages\n20‚Äì1. BMVA Press, 2009.\n[2] Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S Mirrokni.\nLocality-sensitive hashing scheme based on p-stable distributions. In Pro-\nceedings of the twentieth annual symposium on Computational geometry,\npages 253‚Äì262. ACM, 2004.\n[3] Thanh-Toan Do, Anh-Dzung Doan, and Ngai-Man Cheung. Learning to\nhash with binary deep neural network. In European Conference on Com-\nputer Vision, pages 219‚Äì234. Springer, 2016.\n[4] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn,\nand Andrew Zisserman. The pascal visual object classes (voc) challenge.\nInternational journal of computer vision, 88(2):303‚Äì338, 2010.\n13\n[5] Kamran Ghasedi Dizaji, Feng Zheng, Najmeh Sadoughi, Yanhua Yang,\nCheng Deng, and Heng Huang. Unsupervised deep generative adversarial\nhashing network.\nIn Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 3664‚Äì3673, 2018.\n[6] Aristides Gionis, Piotr Indyk, Rajeev Motwani, et al. Similarity search in\nhigh dimensions via hashing. In Vldb, volume 99, pages 518‚Äì529, 1999.\n[7] Ross Girshick, JeÔ¨ÄDonahue, Trevor Darrell, and Jitendra Malik.\nRich\nfeature hierarchies for accurate object detection and semantic segmenta-\ntion. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 580‚Äì587, 2014.\n[8] Yunchao Gong and Svetlana Lazebnik.\nIterative quantization: A pro-\ncrustean approach to learning binary codes.\nIn Computer Vision and\nPattern Recognition (CVPR), 2011 IEEE Conference on, pages 817‚Äì824.\nIEEE, 2011.\n[9] Kaiming He, Georgia Gkioxari, Piotr Doll¬¥ar, and Ross Girshick. Mask r-\ncnn. In Computer Vision (ICCV), 2017 IEEE International Conference\non, pages 2980‚Äì2988. IEEE, 2017.\n[10] Shanshan Huang, Yichao Xiong, Ya Zhang, and Jia Wang. Unsupervised\ntriplet hashing for fast image retrieval. In Proceedings of the on Thematic\nWorkshops of ACM Multimedia 2017, pages 84‚Äì92. ACM, 2017.\n[11] Qing-Yuan Jiang and Wu-Jun Li.\nScalable graph hashing with feature\ntransformation. In IJCAI, pages 2248‚Äì2254, 2015.\n[12] Zhongming Jin, Cheng Li, Yue Lin, and Deng Cai. Density sensitive hash-\ning. IEEE Trans. Cybernetics, 44(8):1362‚Äì1371, 2014.\n[13] Michael J Jones and Paul Viola. Robust real-time object detection. In\nWorkshop on statistical and computational theories of vision, volume 266,\npage 56, 2001.\n[14] Alex Krizhevsky, Ilya Sutskever, and GeoÔ¨Ärey E Hinton. Imagenet clas-\nsiÔ¨Åcation with deep convolutional neural networks. In Advances in neural\ninformation processing systems, pages 1097‚Äì1105, 2012.\n[15] Brian Kulis, Prateek Jain, and Kristen Grauman. Fast similarity search\nfor learned metrics. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 31(12):2143‚Äì2157, 2009.\n[16] Qi Li, Zhenan Sun, Ran He, and Tieniu Tan. Deep supervised discrete\nhashing.\nIn Advances in Neural Information Processing Systems, pages\n2482‚Äì2491, 2017.\n[17] Wu-Jun Li, Sheng Wang, and Wang-Cheng Kang.\nFeature learning\nbased deep supervised hashing with pairwise labels.\narXiv preprint\narXiv:1511.03855, 2015.\n14\n[18] Kevin Lin, Jiwen Lu, Chu-Song Chen, and Jie Zhou. Learning compact bi-\nnary descriptors with unsupervised deep neural networks. In Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition, pages\n1183‚Äì1192, 2016.\n[19] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott\nReed, Cheng-Yang Fu, and Alexander C Berg.\nSsd: Single shot multi-\nbox detector. In European conference on computer vision, pages 21‚Äì37.\nSpringer, 2016.\n[20] Wei Liu, Jun Wang, Rongrong Ji, Yu-Gang Jiang, and Shih-Fu Chang. Su-\npervised hashing with kernels. In Computer Vision and Pattern Recognition\n(CVPR), 2012 IEEE Conference on, pages 2074‚Äì2081. IEEE, 2012.\n[21] Zhaofan Qiu, Yingwei Pan, Ting Yao, and Tao Mei. Deep semantic hashing\nwith generative adversarial networks. In Proceedings of the 40th Interna-\ntional ACM SIGIR Conference on Research and Development in Informa-\ntion Retrieval, pages 225‚Äì234. ACM, 2017.\n[22] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You\nonly look once: UniÔ¨Åed, real-time object detection. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 779‚Äì\n788, 2016.\n[23] Joseph Redmon and Ali Farhadi. Yolo9000: better, faster, stronger. arXiv\npreprint, 2017.\n[24] Fumin Shen, Chunhua Shen, Wei Liu, and Heng Tao Shen.\nSupervised\ndiscrete hashing. In Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 37‚Äì45, 2015.\n[25] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks\nfor large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n[26] Dongjin Song, Wei Liu, Rongrong Ji, David A Meyer, and John R Smith.\nTop rank supervised binary coding for visual search.\nIn Proceedings of\nthe IEEE International Conference on Computer Vision, pages 1922‚Äì1930,\n2015.\n[27] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\nDragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Ra-\nbinovich.\nGoing deeper with convolutions.\nIn Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 1‚Äì9, 2015.\n[28] Dan Wang, Heyan Huang, Chi Lu, Bo-Si Feng, Liqiang Nie, Guihua Wen,\nand Xian-Ling Mao. Supervised deep hashing for hierarchical labeled data.\npages 7388‚Äì7395, 2018.\n[29] Jun Wang, Sanjiv Kumar, and Shih-Fu Chang. Semi-supervised hashing\nfor scalable image retrieval. 2010.\n15\n[30] Yair Weiss, Antonio Torralba, and Rob Fergus. Spectral hashing. In Ad-\nvances in neural information processing systems, pages 1753‚Äì1760, 2009.\n[31] Ting Yao, Fuchen Long, Tao Mei, and Yong Rui. Deep semantic-preserving\nand ranking-based hashing for image retrieval. In IJCAI, pages 3931‚Äì3937,\n2016.\n[32] Zheng Zhang, Qin Zou, Qian Wang, Yuewei Lin, and Qingquan Li. Instance\nsimilarity deep hashing for multi-label image retrieval.\narXiv preprint\narXiv:1803.02987, 2018.\n[33] Han Zhu, Mingsheng Long, Jianmin Wang, and Yue Cao. Deep hashing\nnetwork for eÔ¨Écient similarity retrieval. In AAAI, pages 2415‚Äì2421, 2016.\n16\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2018-11-24",
  "updated": "2018-11-24"
}