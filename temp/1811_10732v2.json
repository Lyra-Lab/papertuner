{
  "id": "http://arxiv.org/abs/1811.10732v2",
  "title": "Environments for Lifelong Reinforcement Learning",
  "authors": [
    "Khimya Khetarpal",
    "Shagun Sodhani",
    "Sarath Chandar",
    "Doina Precup"
  ],
  "abstract": "To achieve general artificial intelligence, reinforcement learning (RL)\nagents should learn not only to optimize returns for one specific task but also\nto constantly build more complex skills and scaffold their knowledge about the\nworld, without forgetting what has already been learned. In this paper, we\ndiscuss the desired characteristics of environments that can support the\ntraining and evaluation of lifelong reinforcement learning agents, review\nexisting environments from this perspective, and propose recommendations for\ndevising suitable environments in the future.",
  "text": "arXiv:1811.10732v2  [cs.AI]  6 Dec 2018\nEnvironments for Lifelong Reinforcement Learning\nKhimya Khetarpal∗\nMila, McGill University, Montreal, Canada\nkhimya.khetarpal@mail.mcgill.ca\nShagun Sodhani∗\nMila, Université de Montréal, Canada\nsshagunsodhani@gmail.com\nSarath Chandar\nMila, Université de Montréal, Canada\nsarathcse2008@gmail.com\nDoina Precup\nMila, McGill University, Montreal, Canada\ndprecup@cs.mcgill.ca\nAbstract\nTo achieve general artiﬁcial intelligence, reinforcement learning (RL) agents\nshould learn not only to optimize returns for one speciﬁc task but also to con-\nstantly build more complex skills and scaffold their knowledge about the world,\nwithout forgetting what has already been learned. In this paper, we discuss the de-\nsired characteristics of environments that can support the training and evaluation\nof lifelong reinforcement learning agents, review existing environments from this\nperspective, and propose recommendations for devising suitable environments in\nthe future.\n1\nIntroduction\nHumans acquire skills and build on them to solve increasingly complex tasks. For instance, consider\na child learning to play basketball. This involves learning to hold the ball properly, to throw and\ncatch, then learning to pass and dribble. These skills are then combined to learn more complex skills.\nFor example, a lay-up is a composition of dribbling while running and a throw, while ensuring that\nthe sequence of steps is not too long and that the throw lands the ball in the hoop. The entire task\nof playing a game relies on various subtasks and requires developing skills of increasing complexity.\nMore importantly, skills need to work in different games and against different opponents.\nIn contrast, reinforcement learning (RL) agents, while able to achieve human-level performance in\ncomplex games like Go, usually focus on becoming really proﬁcient at one task, and train from\nscratch in each new problem they face.\nLifelong learning agents can learn from a stream of experience spanning many tasks (possibly of\ndifferent nature) over its lifetime [Silver et al., 2013, Ring, 1997, Thrun, 1996]. The early work of\nRing [Ring, 1997] describes a continual learning agent as an autonomous agent trained on a sequence\nof tasks with no ﬁnal task. This process bears different names in the literature - incremental and\ncontinual learning [Solomonoff, 1989], never-ending learning [Carlson et al., 2010], etc.\nIn this work, we focus on lifelong learning performed in the context of reinforcement learning. In\naddition to optimizing returns, a lifelong RL agent should be able to:\n• Learn behaviors, skills, and predictions about the environment while solving given tasks.\n• Learn incrementally throughout its lifetime\n• Combine previously learned skills and build on top of them to solve increasingly complex\ntasks\n∗Equal contribution\n2nd Continual Learning Workshop, Neural Information Processing Systems (NeurIPS 2018), Montréal,\nCanada.\n• Plan for short-term and long-term goals\nWe argue that virtual-embodiment is perhaps the most natural setup for training and evaluating\nlifelong reinforcement learning agents. We review existing environments to understand if they are\nappropriate for training and evaluating a lifelong learning agent. Based on this comparative analysis,\nwe propose recommendations for RL environments suitable for lifelong learning.\n2\nBackground and Motivation\nRecent breakthroughs in RL research [Silver et al., 2016, Mnih et al., 2015] have been powered in\npart by advances in deep learning and the availability of diverse simulation environments to train\nRL agents. The Arcade Learning Environment (ALE) [Bellemare et al., 2013], originally proposed\nin 2013, is a suite of Atari 2600 games which provides dozens of problems in which to train and\nevaluate RL agents. More recently, OpenAI’s Gym [Brockman et al., 2016] was developed and\noffers a broader variety of environments ranging from toy text and grid world problems to continuous\ncontrol tasks and Atari-based games.\nOne of the shortcomings of the originally proposed ALE platform is the deterministic nature of the\nenvironments, which can result in memorization of state-action sequences as opposed to general-\nization. A more recent version of ALE [Machado et al., 2018] supports multiple game modes and\nprovides a form of stochasticity.\nMoving towards a more realistic setting, frameworks such as DeepMind Lab [Beattie et al., 2016]\nand VizDoom [Kempka et al., 2016] offer 3D ﬁrst-person-view environments. Both VizDoom and\nDeepMind Lab support stylized labyrinths catered to navigation tasks. However, they lack natural-\nistic appeal in terms of their layout, appearance and objects. Another platform that the RL commu-\nnity has explored in recent years is Minecraft [Duncan, 2011], which offers a highly complex envi-\nronment with characteristics that could be potentially leveraged for lifelong learning [Tessler et al.,\n2017].\nSimulators such as Gazebo [Koenig and Howard, 2004] have been intensively used in robotics re-\nsearch and are tailored towards learning agents which train in simulation but are then evaluated on\nphysical robots. Aligned with recent efforts toward reproducible research, [Mahmood et al., 2018]\nintroduced benchmark tasks for physical robots which allow experiments to be reproduced in dif-\nferent locations and under diverse conditions. While these tasks are quite challenging and push RL\nagents’ limits, for lifelong learning they may be too difﬁcult at the moment, given that robotics plat-\nforms have limitations in terms of their ability to support many different tasks. However, they do\nhave a feature which we ﬁnd very useful for continual learning: embodiment. We will discuss next\nwhat we mean by embodiment and why it is useful for lifelong learning.\n3\nLearning in Embodied Agents\nTheories of embodied cognition [Kiefer and Trumpp, 2012] suggest that cognition is grounded in\nperception and action. Embodied learning allows agents to actively interact with the environment\nand utilize a rich, multi-modal sensori-motor stream of data. However, training physically embod-\nied agents can be slow, expensive and sometimes impractical. Therefore, virtual embodiment can\nbe seen as an alternative approach. More importantly, virtual embodiment is closer to how humans\nlearn through interaction with their environment via multiple sensors and effectors of different types.\nInteraction modalities include (3D) vision, audio signals, natural language, experiencing and exert-\ning physical forces etc. Environments that support a multitude of these modalities are generally\nmore difﬁcult to solve, but they provide diversity and richness that can be very useful in order to\nbuild good generalizations.\nVirtual embodiment has several other advantages such as i) curriculum learning - virtual environ-\nments are easy to modify in terms of complexity thereby making it easier to train agents in progres-\nsive fashion, ii) short-term and long-term goals - these are equivalent respectively with skills and\ncomposition of skills, iii) mimic agents in real-world scenarios - embodied learning tries to encap-\nsulate the real world dynamics in a simulated environment as faithfully as possible, which creates\nmore realistic domains and iv) cause-and-effect learning - rich, multi-modal data streams can help\n2\nagents to understand the causality relationships of various events and opportunities associated with\neach object, through actions that are afforded by these objects.\nSo far, virtual embodiment environments have been used for tasks like navigation [Wu et al., 2018,\nAnderson et al., 2018], visual question answering [Das et al., 2018], teaching to execute instructions\nor programs [Puig et al., 2018]. We will now summarize the characteristics of these environments,\nwhich we believe would provide good testbeds for training and evaluating lifelong learning algo-\nrithms.\n4\nVirtual Embodiment Environments: A Short Review\n4.1\nHouse 3D\nHouse3D [Wu et al., 2018] is a realistic and extensible environment built on top of the SUNCG\ndataset [Song et al., 2017] (a large dataset of over 45,000 human designed 3D house layouts). The\nenvironment supports rendering photo-realistic 3D visuals with support for diverse 3D objects and\nlayouts. Each scene is annotated with 3D coordinates and other meta-data like room and object type.\nThe paper introduced the room navigation task where a set of episodic environments E = E1, .., En\nand a set of semantic concepts I = I1, .., Im are pre-deﬁned. During each episode, the agent\ninteracts with one of the environments e ∈E and is given a concept i ∈I. The agent starts at\na random position in e and at each time-step, receives a visual signal xt corresponding to the ﬁrst\nperson view. The agent needs to navigate to reach a target destination.\nWhile the environment can be customized for deﬁning new tasks and can be used to load other 3D\nscene datasets (like [Chang et al., 2017], [Armeni et al., 2017] etc), the environment itself does not\nallow deﬁning varying difﬁculty tasks. In particular, once a layout is selected, we can not make the\nlayout “harder” or “easier” for the agent. For instance, one can not “add” obstacles that the agent\nneeds to overcome before the task is considered complete.\n4.2\nHoME: Household Multimodal Environment\nHoME[Brodeur et al., 2017] is similar to House3D as it is also built on top of the SUNCG dataset.\nAlong with 3D visual rendering and semantic image segmentation, HoME also provides natural lan-\nguage descriptions of objects and audio rendering. It further supports rigid body dynamics (through\na physics engine) and external forces like gravity. This environment supports many more modalities\ncompared to the other virtual embodiment environments and also supports “adding” or “removing”\nobjects. This makes it a suitable candidate for training and evaluating lifelong learning agents in\nRL.\n4.3\nMINOS: Multimodal Indoor Simulator for Navigation in Complex Environments\nMINOS[Savva et al., 2017] is a simulation framework speciﬁcally designed for multi-sensory nav-\nigation models. It can use layouts from both SUNCG and Matterplot3D. While it is not as rich as\nsome other environments (like HoME or House3D), it allows for easy customization. In particular,\nMINOS supports material variation (for texture and colors), object clutter variation (where a set of\nspeciﬁed categories of objects can be removed), navigation goal speciﬁcation (where goals can be\nat arbitrary points in space or can be arbitrary instances of a category), task speciﬁcation (where the\ntask can be speciﬁed through an arbitrary Python function which returns reward signals and episode\nsuccess or failure at each state).\nThese characteristics make MINOS a good candidate for lifelong learning. It can be easily used to\ndesign tasks with different levels of complexity or to design tasks that require the use of just one skill\nor composition of skills. These beneﬁts also set it apart from other environments like AI2-THOR\n[Zhu et al., 2017] which supports only 32 single-room environments or CHALET [Yan et al., 2018]\nin which only a small set of discrete actions are supported.\n4.4\nVirtualHome: Simulating Household Activities via Programs\nVirtualHome[Puig et al., 2018] crowdsourced a dataset of “programs” for performing different ac-\ntivities in a house. Most common and atomic (inter)actions were identiﬁed and implemented in the\n3\nUnity3D game engine. The programs and the simulated environment can be used to train an agent\nto perform household tasks based on language instructions. If we think of each atomic action as a\nskill, then each program, which is a sequence of atomic actions, can be seen as a composition of\nskills. This is a major advantage of using VirtualHome - the ready availability of the program dataset\n(which can be seen as a composition of skills). A disadvantage of this environment, however, is that\nit does not allow for creating variations of a scene, which is important for designing tasks of varying\ncomplexity.\nBefore wrapping up this section, we also consider some of the prominent RGB-D datasets which are\nuseful building blocks for developing environments for virtual embodied agents. The environments\ndiscussed earlier use one or more of these datasets.\n4.5\nMatterport3D: Learning from RGB-D Data in Indoor Environments\nMatterport3D[Chang et al., 2017] introduced a large RGB-D dataset of indoors scenes (10,800\npanoramic views from 194,400 RGB-D images of 90 building-scale scenes). It includes annota-\ntions for surface reconstruction, camera poses and 2D and 3D semantics segmentation. Even though\nan embodied learning agent is not introduced as part of the task setup, many follow-up works like\n[Anderson et al., 2017] and [Savva et al., 2017] used this work as the starting point for deﬁning a\nplethora of tasks related to computer vision.\n4.6\nSUNCG: A Large 3D Model Repository for Indoor Scenes\nSUNCG [Song et al., 2017] is a large scale dataset of richly annotated scenes. The dataset con-\ntains over 45,000 manually created room and object layouts along with semantic annotations. The\ndataset was created to learn semantic scene completion, where given a single-view depth map obser-\nvation, a complete 3D representation, along with semantic labels is generated. Follow-up works like\n[Das et al., 2018] and [Wu et al., 2018] used it for training embodied agents.\nIn the following section, we discuss what is missing in these frameworks in order to perform lifelong\nlearning.\n5\nRecommendations for a lifelong learning testbed\nTo conclude, we discuss features that, in our view, would be important to support by any framework\nfor training and evaluation of lifelong learning agents.\n1: The proposed testbed should support a multitude of tasks of different difﬁculty. The lowest\nlevel tasks should require only one skill to solve, while tasks more complex tasks should require\na composition of skills learned in the previous levels. Tasks could for example be structured in a\nhierarchy, such that task complexity would increase as the learning agent moves up. Solving a task\nin the i + 1th level should require the agent to solve the task at level i and also learn to compose\npreviously acquired skills. The environment should be able to present the agent with tasks that test\nits ability to make compositions.\n2: A lifelong learning framework would ideally provide easy addition or removal of objects from\na scene or a variety of scenes. Such a framework would facilitate the incremental expansion of the\ndata that the agent sees over time. This could be achieved by providing the ﬂexibility to scale up the\nsize of the environment, the quantity of objects with which the agent can interact, etc.\n3: Tasks should be generated in the environment in a way that requires the agent to do both short-\nterm and long-term planning. Dealing with many goals that span different time scales would test an\nagent’s capacity to learn different types of knowledge and to generalize across time scales.\n4: As the learning agent moves to more complex tasks, the environment should continue to challenge\nit with previously seen tasks, in order to assess whether the agent can resist catastrophic forgetting.\nAs discussed above, some of the already existing environments align well with these desiderata.\nHowever, more work can be done to expand the set of environments used in lifelong learning. More\nimportantly, assessing the performance of lifelong learning agents and deﬁning the objective that\nthey should optimize is still a problem that has not been tackled much, and is critical for progress in\n4\nour ﬁeld. We hope that this paper is at least useful in outlining some of the intuitive desiderata for\nlifelong learning, and that as a community we can all work to make these more formal and precise.\nReferences\nP. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. Sünderhauf, I. Reid, S. Gould, and A. van den\nHengel. Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real\nenvironments. ArXiv e-prints, November 2017.\nP. Anderson, A. Chang, D. Singh Chaplot, A. Dosovitskiy, S. Gupta, V. Koltun, J. Kosecka, J. Malik, R. Mot-\ntaghi, M. Savva, and A. R. Zamir. On Evaluation of Embodied Navigation Agents. ArXiv e-prints, July\n2018.\nI. Armeni, A. Sax, A. R. Zamir, and S. Savarese. Joint 2D-3D-Semantic Data for Indoor Scene Understanding.\nArXiv e-prints, February 2017.\nCharles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Küttler, Andrew\nLefrancq, Simon Green, Víctor Valdés, Amir Sadik, et al. Deepmind lab. arXiv preprint arXiv:1612.03801,\n2016.\nMarc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An\nevaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279, 2013.\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech\nZaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\nS. Brodeur, E. Perez, A. Anand, F. Golemo, L. Celotti, F. Strub, J. Rouat, H. Larochelle, and A. Courville.\nHoME: a Household Multimodal Environment. ArXiv e-prints, November 2017.\nAndrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R Hruschka Jr, and Tom M Mitchell.\nToward an architecture for never-ending language learning. In AAAI, volume 5, page 3. Atlanta, 2010.\nA. Chang, A. Dai, T. Funkhouser, M. Halber, M. Nießner, M. Savva, S. Song, A. Zeng, and Y. Zhang. Matter-\nport3D: Learning from RGB-D Data in Indoor Environments. ArXiv e-prints, September 2017.\nAbhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. Embodied Ques-\ntion Answering.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2018.\nSean C Duncan. Minecraft, beyond construction and survival. Well Played: a journal on video games, value\nand meaning, 1(1):1–22, 2011.\nMichał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Ja´skowski. Vizdoom: A\ndoom-based ai research platform for visual reinforcement learning.\nIn Computational Intelligence and\nGames (CIG), 2016 IEEE Conference on, pages 1–8. IEEE, 2016.\nMarkus\nKiefer\nand\nNatalie\nM.\nTrumpp.\nEmbodiment\ntheory\nand\neducation:\nThe\nfounda-\ntions of cognition in perception and action.\nTrends in Neuroscience\nand Education,\n1(1):\n15 – 20,\n2012.\nISSN 2211-9493.\ndoi:\nhttps://doi.org/10.1016/j.tine.2012.07.002.\nURL\nhttp://www.sciencedirect.com/science/article/pii/S221194931200004X.\nNathan P Koenig and Andrew Howard. Design and use paradigms for gazebo, an open-source multi-robot\nsimulator. In IROS, volume 4, pages 2149–2154. Citeseer, 2004.\nMarlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and Michael Bowling.\nRevisiting the arcade learning environment: Evaluation protocols and open problems for general agents.\nJournal of Artiﬁcial Intelligence Research, 61:523–562, 2018.\nA Rupam Mahmood, Dmytro Korenkevych, Gautham Vasan, William Ma, and James Bergstra. Benchmarking\nreinforcement learning algorithms on real-world robots. arXiv preprint arXiv:1809.07731, 2018.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex\nGraves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep\nreinforcement learning. Nature, 518(7540):529, 2015.\nX. Puig, K. Ra, M. Boben, J. Li, T. Wang, S. Fidler, and A. Torralba. VirtualHome: Simulating Household\nActivities via Programs. ArXiv e-prints, June 2018.\n5\nXavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. Virtual-\nhome: Simulating household activities via programs. In CVPR, 2018.\nMark B Ring. Child: A ﬁrst step towards continual learning. Machine Learning, 28(1):77–104, 1997.\nM. Savva, A. X. Chang, A. Dosovitskiy, T. Funkhouser, and V. Koltun. MINOS: Multimodal Indoor Simulator\nfor Navigation in Complex Environments. ArXiv e-prints, December 2017.\nDaniel L Silver, Qiang Yang, and Lianghao Li. Lifelong machine learning systems: Beyond learning algorithms.\nIn AAAI Spring Symposium: Lifelong Machine Learning, volume 13, page 05, 2013.\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian\nSchrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go\nwith deep neural networks and tree search. nature, 529(7587):484, 2016.\nRay J Solomonoff. A system for incremental learning based on algorithmic probability. In Proceedings of the\nSixth Israeli Conference on Artiﬁcial Intelligence, Computer Vision and Pattern Recognition, pages 515–527,\n1989.\nShuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Manolis Savva, and Thomas Funkhouser. Semantic scene\ncompletion from a single depth image. IEEE Conference on Computer Vision and Pattern Recognition, 2017.\nChen Tessler, Shahar Givony, Tom Zahavy, Daniel J Mankowitz, and Shie Mannor. A deep hierarchical ap-\nproach to lifelong learning in minecraft. In AAAI, volume 3, page 6, 2017.\nSebastian Thrun.\nExplanation-based neural network learning: A lifelong learning approach, volume 357.\nSpringer Science & Business Media, 1996.\nY. Wu, Y. Wu, G. Gkioxari, and Y. Tian. Building Generalizable Agents with a Realistic and Rich 3D Environ-\nment. ArXiv e-prints, January 2018.\nC. Yan, D. Misra, A. Bennnett, A. Walsman, Y. Bisk, and Y. Artzi. CHALET: Cornell House Agent Learning\nEnvironment. ArXiv e-prints, January 2018.\nYuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi. Target-\ndriven visual navigation in indoor scenes using deep reinforcement learning. In Robotics and Automation\n(ICRA), 2017 IEEE International Conference on, pages 3357–3364. IEEE, 2017.\n6\n",
  "categories": [
    "cs.AI",
    "cs.LG"
  ],
  "published": "2018-11-26",
  "updated": "2018-12-06"
}