{
  "id": "http://arxiv.org/abs/1811.11819v2",
  "title": "Unsupervised Meta-Learning For Few-Shot Image Classification",
  "authors": [
    "Siavash Khodadadeh",
    "Ladislau B√∂l√∂ni",
    "Mubarak Shah"
  ],
  "abstract": "Few-shot or one-shot learning of classifiers requires a significant inductive\nbias towards the type of task to be learned. One way to acquire this is by\nmeta-learning on tasks similar to the target task. In this paper, we propose\nUMTRA, an algorithm that performs unsupervised, model-agnostic meta-learning\nfor classification tasks. The meta-learning step of UMTRA is performed on a\nflat collection of unlabeled images. While we assume that these images can be\ngrouped into a diverse set of classes and are relevant to the target task, no\nexplicit information about the classes or any labels are needed. UMTRA uses\nrandom sampling and augmentation to create synthetic training tasks for\nmeta-learning phase. Labels are only needed at the final target task learning\nstep, and they can be as little as one sample per class. On the Omniglot and\nMini-Imagenet few-shot learning benchmarks, UMTRA outperforms every tested\napproach based on unsupervised learning of representations, while alternating\nfor the best performance with the recent CACTUs algorithm. Compared to\nsupervised model-agnostic meta-learning approaches, UMTRA trades off some\nclassification accuracy for a reduction in the required labels of several\norders of magnitude.",
  "text": "UNSUPERVISED META-LEARNING FOR FEW-SHOT IMAGE\nCLASSIFICATION\nSiavash Khodadadeh, Ladislau B√∂l√∂ni\nDept. of Computer Science\nUniversity of Central Florida\nsiavash.khodadadeh@knights.ucf.edu, lboloni@cs.ucf.edu\nMubarak Shah\nCenter for Research in Computer Vision\nUniversity of Central Florida\nshah@crcv.ucf.edu\nABSTRACT\nFew-shot or one-shot learning of classiÔ¨Åers requires a signiÔ¨Åcant inductive bias towards the type\nof task to be learned. One way to acquire this is by meta-learning on tasks similar to the target\ntask. In this paper, we propose UMTRA, an algorithm that performs unsupervised, model-agnostic\nmeta-learning for classiÔ¨Åcation tasks.\nThe meta-learning step of UMTRA is performed on a Ô¨Çat collection of unlabeled images. While\nwe assume that these images can be grouped into a diverse set of classes and are relevant to the\ntarget task, no explicit information about the classes or any labels are needed. UMTRA uses random\nsampling and augmentation to create synthetic training tasks for meta-learning phase. Labels are only\nneeded at the Ô¨Ånal target task learning step, and they can be as little as one sample per class.\nOn the Omniglot and Mini-Imagenet few-shot learning benchmarks, UMTRA outperforms every\ntested approach based on unsupervised learning of representations, while alternating for the best\nperformance with the recent CACTUs algorithm. Compared to supervised model-agnostic meta-\nlearning approaches, UMTRA trades off some classiÔ¨Åcation accuracy for a reduction in the required\nlabels of several orders of magnitude.\n1\nIntroduction\nMeta-learning or ‚Äúlearning-to-learn‚Äù approaches have been proposed in the neural networks literature since the\n1980s [37, 4]. The general idea is to prepare the network through several learning tasks T1 . . . Tn, in a meta-learning\nphase such that when presented with the target task Tn+1, the network will be ready to learn it as efÔ¨Åciently as possible.\nRecently proposed model-agnostic meta-learning approaches [11, 31] can be applied to any differentiable network.\nWhen used for classiÔ¨Åcation, the target learning phase consists of several gradient descent steps on a backpropagated\nsupervised classiÔ¨Åcation loss. Unfortunately, these approaches require the learning tasks Ti to have the same supervised\nlearning format as the target task. Acquiring labeled data for a large number of tasks is not only a problem of cost and\nconvenience but also puts conceptual limits on the type of problems that can be solved through meta-learning. If we\nneed to have labeled training data for tasks T1 . . . Tn in order to learn task Tn+1, this limits us to task types that are\nvariations of tasks known and solved (at least by humans).\nIn this paper, we propose an algorithm called Unsupervised Meta-learning with Tasks constructed by Random sampling\nand Augmentation (UMTRA) that performs meta-learning of one-shot or few-shot classiÔ¨Åers in an unsupervised manner\non an unlabeled dataset. Instead of starting from a collection of labeled tasks, {. . . Ti . . .}, UMTRA starts with a\ncollection of unlabeled data U = {. . . xi . . .}. We have only a set of relatively easy-to-satisfy requirements towards U:\nIts objects have to be drawn from the same distribution as the objects classiÔ¨Åed in the target task and it must have a set\nof classes signiÔ¨Åcantly larger than the number of classes of the Ô¨Ånal classiÔ¨Åer. Starting from this unlabeled dataset,\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\narXiv:1811.11819v2  [cs.CV]  7 Nov 2019\nSupervised MAML\nùë•1 ùë•2\nùë•3\n...\nùë•n\nCL1\n.\nUpdated \nModel\nC1\nC2\nC3\nC1200\n...\nSample N classes\n..\n1\n2\n3\n...\nN\n‚Ñí\nUpdate \nmodel \nparameters\nbased on \nouter loss\n1\n2\n3\n...\nN\nSample 2 data points from each class\n‚Ñí\nModel\nC\nC\nC\nL2\nL3\nLN\nùë•‚Ä≤1 ùë•‚Ä≤2 ùë•‚Ä≤3\n... ùë•‚Ä≤n\nUMTRA\nùë•‚Ä≤1 ùë•‚Ä≤2 ùë•‚Ä≤3\n... ùë•‚Ä≤n\n       (    )\n     (    )\n     (    )     (    )     (    )\nùë•1 ùë•2\nùë•3\n...\nùë•n\nUpdated \nModel\n1\n2\n3\n...\nN\n‚Ñí\nUpdate \nmodel \nparameters\nbased on \nouter loss\n1\n2\n3\n...\nN\n‚Ñí\nModel\nSample N data points\nùë•‚Ä≤i =    (ùë•i)\nFigure 1: The process of creation of the training and validation data of the meta-training task T . (top) Supervised\nMAML: We start from a dataset where the samples are labeled with their class. The training data is created by sampling\nN distinct classes CLi, and choosing a random sample xi from each. The validation data is created by choosing a\ndifferent sample x‚Ä≤\ni from the same class. (bottom) UMTRA: We start from a dataset of unlabeled data. The training\ndata is created by randomly choosing N samples xi from the dataset. The validation data is created by applying the\naugmentation function A to each sample from the training data. For both MAML and UMTRA, artiÔ¨Åcial temporary\nlabels 1, 2 . . . N are used.\nUMTRA uses statistical diversity properties and domain speciÔ¨Åc augmentations to generate the training and validation\ndata for a collection of synthetic tasks, {. . . T ‚Ä≤\ni . . .}. These tasks are then used in the meta-learning process based on a\nmodiÔ¨Åed classiÔ¨Åcation variant of the MAML algorithm [11]. Figure 1 summarizes the differences between the original\nsupervised MAML model and the process of generating synthetic tasks from unsupervised data in UMTRA.\nThe contributions of this paper can be summarized as follows:\n‚Ä¢ We describe a novel algorithm that allows unsupervised, model-agnostic meta-learning for few-shot classiÔ¨Åca-\ntion by generating synthetic meta-learning data with artiÔ¨Åcial labels.\n‚Ä¢ From a theoretical point of view, we demonstrate a relationship between generalization error and the loss\nbackpropagated from the validation set in MAML. Our intuition is that we can generate unsupervised validation\ntasks which can perform effectively if we are able to span the space of the classes by generating useful samples\nwith augmentation.\n‚Ä¢ On all the Omniglot and Mini-Imagenet few-shot learning benchmarks, UMTRA outperforms every tested\napproach based on unsupervised learning of representations. It also achieves a signiÔ¨Åcant percentage of the\naccuracy of the supervised MAML approach, while requiring vastly fewer labels. For instance, for 5-way\n5-shot classiÔ¨Åcation on the Omniglot dataset UMTRA obtains a 95.43% accuracy with only 25 labels, while\nsupervised MAML obtains 98.83% with 24025. Compared with recent unsupervised meta-learning approaches\nbuilding on top of stock MAML, UMTRA alternates for the best performance with the CACTUs algorithm.\n2\n2\nRelated Work\nFew-shot or one-shot learning of classiÔ¨Åers has signiÔ¨Åcant practical applications. Unfortunately, the few-shot learning\nmodel is not a good Ô¨Åt to the traditional training approaches of deep neural networks, which work best with large\namounts of data. In recent years, signiÔ¨Åcant research targeted approaches to allow deep neural networks to work in\nfew-shot learning settings. One possibility is to perform transfer learning, but it was found that the accuracy decreases if\nthe target task diverges from the trained task. One solution to mitigate this is through the use of an adversarial loss [25].\nA large class of approaches aim to enable few-shot learning by meta-learning - the general idea being that the meta-\nlearning prepares the network to learn from the small amount of training data available in the few-shot learning setting.\nNote that meta-learning can be also used in other computer vision applications, such as fast adaptation for tracking in\nvideo [33]. The mechanisms through which meta-learning is implemented can be loosely classiÔ¨Åed in two groups. One\nclass of approaches use a custom network architecture for encoding the information acquired during the meta-learning\nphase, for instance in fast weights [3], neural plasticity values [29], custom update rules [28], the state of temporal\nconvolutions [30] or in the memory of an LSTM [35]. The advantage of this approach is that it allows us to Ô¨Åne-tune the\narchitecture for the efÔ¨Åcient encoding of the meta-learning information. A disadvantage, however, is that it constrains\nthe type of network architectures we can use; innovations in network architectures do not automatically transfer into the\nmeta-learning approach. In a custom network architecture meta-learning model, the target learning phase is not the\ncustomary network learning, as it needs to take advantage of the custom encoding.\nA second, model-agnostic class of approaches aim to be usable for any differentiable network architecture. Examples of\nthese algorithms are MAML [11] or Reptile [31], where the aim is to encode the meta-learning in the weights of the\nnetwork, such that the network performs the target learning phase with efÔ¨Åcient gradients. Approaches that customize\nthe learning rates [27] during meta-training can also be grouped in this class. For this type of approaches, the target\nlearning phase uses the well-established learning algorithms that would be used if learning from scratch (albeit it\nmight use speciÔ¨Åc hyperparameter settings, such as higher learning rates). We need to point out, however, that the\nmeta-learning phase uses custom algorithms in these approaches as well (although they might use the standard learning\nalgorithm in the inner loop, such as in the case of MAML). A recent work similar in spirit to ours is the CACTUs\nunsupervised meta-learning model described in [15].\nIn this paper, we perform unsupervised meta-learning. Our approach generates tasks from unlabeled data which will\nhelp it to understand the structures of the relevant supervised tasks in the future. One should note that these relevant\nsupervised tasks in the future do not have any intersection with the tasks which are used during the meta-learning.\nFor instance, Wu et al. perform unsupervised learning by recognizing a certain internal structure between dataset\nclasses [42]. By learning this structure, the approach can be extended to semi-supervised learning. In addition, Pathak et\nal. propose a method which learns object features in an interesting unsupervised way by detecting movement patterns of\nsegmented objects [34]. These approaches are orthogonal to ours. We do not make assumptions that the unsupervised\ndata shares classes with the target learning (in fact, we explicitly forbid it). Finally, [14] deÔ¨Åne unsupervised meta-\nlearning in reinforcement learning context. The authors study how to generate tasks with synthetic reward functions\n(without supervision) such that when the policy network is meta trained on them, they can learn real tasks with manually\ndeÔ¨Åned reward functions (with supervision) much more quickly and with fewer samples.\n3\nThe UMTRA algorithm\n3.1\nPreliminaries\nWe consider the task of classifying samples x drawn from a domain X into classes yi ‚ààY = {C1, . . . , CN}. The\nclasses are encoded as one-hot vectors of dimensionality N. We are interested in learning a classiÔ¨Åer fŒ∏ that outputs\na probability distribution over the classes. It is common to envision f as a deep neural network parameterized by Œ∏,\nalthough this is not the only possible choice.\nWe package a certain supervised learning task, T , of type (N, K), that is with N classes of K training samples each, as\nfollows. The training data will have the form (xi, yi), where i = 1 . . . N √ó K, xi ‚ààX and yi ‚ààY , with exactly K\nsamples for each value of yi. In the recent meta-learning literature, it is often assumed that the task T has K samples of\neach class for training and (separately), K samples for validation (xv\nj, yv\nj ).\nIn supervised meta-learning, we have access to a collection of tasks T1 . . . Tn drawn from a speciÔ¨Åc distribution, with\nboth supervised training and validation data. The meta-learning phase uses this collection of tasks, while the target\nlearning uses a new task T with supervised learning data but no validation data.\n3\nAlgorithm 1: Unsupervised Meta-learning with Tasks constructed by Random sampling and Augmentation (UMTRA)\nrequire :N: class-count, NMB: meta-batch size, NU : no. of updates\nrequire :U = {. . . xi . . .} unlabeled dataset\nrequire :Œ±, Œ≤: step size hyperparameters\nrequire :A: augmentation function\n1 randomly initialize Œ∏;\n2 while not done do\n3\nfor i in 1 . . . NMB do\n4\nSample N data points x1 . . . xN from U;\n5\nTi ‚Üê{x1, . . . xN};\n6\nend\n7\nforeach Ti do\n8\nGenerate training set Di = {(x1, 1), . . . , (xN, N)};\n9\nŒ∏‚Ä≤\ni = Œ∏;\n10\nfor j in 1 . . . NU do\n11\nEvaluate ‚àáŒ∏‚Ä≤\niLTi(fŒ∏‚Ä≤\ni);\n12\nCompute adapted parameters with gradient descent: Œ∏‚Ä≤\ni = Œ∏‚Ä≤\ni ‚àíŒ±‚àáŒ∏‚Ä≤\niLTi(fŒ∏‚Ä≤\ni);\n13\nend\n14\nGenerate validation set for the meta-update D‚Ä≤\ni = {(A(x1), 1), . . . , (A(xN), N)}\n15\nend\n16\nUpdate Œ∏ ‚ÜêŒ∏ ‚àíŒ≤‚àáŒ∏\nP\nTi LTi(fŒ∏‚Ä≤\ni) using each D‚Ä≤\ni;\n17 end\n3.2\nModel\nUnsupervised meta-learning retains the goal of meta-learning by preparing a learning system for the rapid learning\nof the target task T . However, instead of the collection of tasks T1 . . . Tn and their associated labeled training data,\nwe only have an unlabeled dataset U = {. . . xi . . .}, with samples drawn from the same distribution as the target task.\nWe assume that every element of this dataset is associated with a natural class C1 . . . Cc, ‚àÄxi ‚àÉj such that xi ‚ààCj.\nWe will assume that N ‚â™c, that is, the number of natural classes in the unsupervised dataset is much higher than the\nnumber of classes in the target task. These requirements are much easier to satisfy than the construction of the tasks for\nsupervised meta-learning - for instance, simply stripping the labels from datasets such as Omniglot and Mini-ImageNet\nsatisÔ¨Åes them.\nThe pseudo-code of the UMTRA algorithm is described in Algorithm 1. In the following, we describe the various parts\nof the algorithm in detail. In order to be able to run the UMTRA algorithm on unsupervised data, we need to create\ntasks Ti from the unsupervised data that can serve the same role as the meta-learning tasks serve in the full MAML\nalgorithm. For such a task, we need to create both the training data D and the validation data D‚Ä≤.\nCreating the training data: In the original form of the MAML algorithm, the training data of the task T must have\nthe form (x, y), and we need N √ó K of them. The exact labels used during the meta-training step are not relevant, as\nthey are discarded during the meta-training phase. They can be thus replaced with artiÔ¨Åcial labels, by setting them\ny ‚àà{1, ...N}. It is however, important that the labels maintain class distinctions: if two data points have the same label,\nthey should also have the same artiÔ¨Åcial labels, while if they have different labels, they should have different artiÔ¨Åcial\nlabels.\nThe Ô¨Årst difference between UMTRA and MAML is that during the meta-training phases, we always perform one-shot\nlearning, with K = 1. Note that during the target learning phase we can still set values of K different from 1. The\ntraining data is created as the set Di = {(x1, 1), . . . (xN, N)}, with xi sampled randomly from U.\nLet us see how this training data construction satisfy the class distinction conditions. The Ô¨Årst condition is satisÔ¨Åed\nbecause there is only one sample for each label. The second condition is satisÔ¨Åed statistically by the fact that N ‚â™c,\nwhere c is the total number of classes in the dataset. If the number of samples is signiÔ¨Åcantly smaller than the number\nof classes, it is likely that all the samples will be drawn from different classes. If we assume that the samples are equally\ndistributed among the classes (e.g. m samples for each class), the probability that all samples are in a different class is\nequal to\nP = (c ¬∑ m) ¬∑ ((c ‚àí1) ¬∑ m)...((c ‚àíN + 1) ¬∑ m)\n(c ¬∑ m) ¬∑ (c ¬∑ m ‚àí1)...(c ¬∑ m ‚àíN + 1)\n= c! ¬∑ mN ¬∑ (c ¬∑ m ‚àíN)!\n(c ‚àíN)! ¬∑ (c ¬∑ m)!\n(1)\n4\nTranslation + Zeroing Pixels\nSame\nFlip Grayscale\nAuto Augment\nRotate\nFigure 2:\nAugmentation techniques on Omniglot (left) and Mini-Imagenet (right). Top row: Original images in\ntraining data. Bottom: augmented images for the validation set, transformed with an augmentation function A. Auto\nAugment [8] applies augmentations from a learned policy based on combinations of translation, rotation, or shearing.\nTo illustrate this, the probability for 5-way classiÔ¨Åcation on the Omniglot dataset used with each of the 1200 characters\nis a separate class (c = 1200, N = 5) is 99.21%. For Mini-ImageNet (c = 64), the probability is 85.23%, while for the\nfull ImageNet it would be about 99%.\nCreating the validation data: For the MAML approach, the validation data of the meta-training tasks is actually\ntraining data in the outer loop. It is thus required that we create a validation dataset D‚Ä≤\ni = {(x‚Ä≤\n1, 1), . . . (x‚Ä≤\nN, N)} for\neach task Ti. Thus we need to create appropriate validation data for the synthetic task. A minimum requirement for the\nvalidation data is to be correctly labeled in the given context. This means that the synthetic numerical label should map\nin both cases to the same class in the unlabeled dataset: ‚àÑC such that xi, x‚Ä≤\ni ‚ààC.\nIn the original MAML model, these x‚Ä≤\ni values are labeled examples part of the supervised dataset. In our case,\npicking such x‚Ä≤\ni values is non-trivial, as we don‚Äôt have access to the actual class. Instead, we propose to create such\na sample by augmenting the sample used in the training data using an augmentation function x‚Ä≤\ni = A(xi) which is\na hyperparameter of the UMTRA algorithm. A requirement towards the augmentation function is to maintain class\nmembership x ‚ààC ‚áíA(x) ‚ààC. We should aim to construct the augmentation function to verify this property for the\ngiven dataset U, based on what we know about the domain described by the dataset. However, as we do not have access\nto the classes, such a veriÔ¨Åcation is not practically possible on a concrete dataset.\nAnother choice for the augmentation function A is to apply some kind of domain-speciÔ¨Åc change to the images or\nvideos. Examples of these include setting some of the pixel values to zero in the image (Figure 2, left), or translating\nthe pixels of the training image by some amount (eg. between -6 and 6).\nThe overall process of generating the training data from the unlabeled dataset in UMTRA and the differences from the\nsupervised MAML approach is illustrated in Figure 1.\n3.3\nSome theoretical considerations\nWhile a full formal model of the learning ability of the UMTRA algorithm is beyond the scope of this paper, we can\ninvestigate some aspects of its behavior that shed light into why the algorithm is working, and why augmentation\nimproves its performance. Let us denote our network with a parameterized function fŒ∏. As we want to learn a few-shot\nclassiÔ¨Åcation task, T we are searching for the corresponding function fT , to which we do not have access. To learn this\nfunction, we use the training dataset, DT = {(xi, yi)}n√ók\ni=1 . For this particular task, we update our parameters (to Œ∏‚Ä≤)\nto Ô¨Åt this task‚Äôs training dataset. In other words, we want fŒ∏‚Ä≤ to be a good approximation of fT . Finding Œ∏‚Ä≤ such that,\nŒ∏‚Ä≤ = argmin\nŒ∏\nX\n(xi,yi)‚ààDT\nL(yi, fŒ∏(xi)) is ill-deÔ¨Åned because there are more than one solution for it. In meta-learning,\nwe search for the Œ∏‚Ä≤ value that gives us the minimum generalization error, the measure of how accurately an algorithm\nis able to predict outcome values for unseen data [1]. We can estimate the generalization error based on sampled data\npoints from the same task. Without loss of generality, let us consider a sampled data point (x0, y0). We can estimate\ngeneralization error on this point as L(y0, fŒ∏‚Ä≤(x0)). In case of mean squared error, and by accepting irreducible error\nœµ ‚àºN(0, œÉ), we can decompose the expected generalization error as follows [18, 12]:\nE [L(y0, fŒ∏‚Ä≤(x0))] =\n\u0000E[fŒ∏‚Ä≤(x0)] ‚àífT (x0)\n\u00012 + E\nh\n(fŒ∏‚Ä≤(x0))2i\n‚àíE [fŒ∏‚Ä≤(x0)]2 + œÉ2\n(2)\nIn this equation, when (x0, y0) /‚ààDT we have E[(fŒ∏‚Ä≤(x0))2] ‚àíE[fŒ∏‚Ä≤(x0)]2 = 0, which means that the estimation\nof the generalization error on these samples will be as unbiased as possible (only biased by œÉ2). On the other hand,\n5\nTable 1: The inÔ¨Çuence of augmentation function on the accuracy of UMTRA for 5-way one-shot classiÔ¨Åcation on the\n(Left: Omniglot dataset, Right: Mini-Imagenet dataset). For all cases, we use meta-batch size NMB = 4 and number of\nupdates NU = 5, except the ones with best hyperparameters.\nAugmentation Function A\nAccuracy\nTraining from scratch\n52.50\nA = 1\n52.93\nA = randomly zeroed pixels\n56.23\nA = randomly zeroed pixels\n(with best hyperparameters)\n67.00\nA = randomly zeroed pixels\n+ random shift (with best\nhyperparameters)\n83.80\nSupervised MAML\n98.7\nAugmentation Function A\nAccuracy\nTraining from scratch\n24.17\nA = 1\n26.49\nA = Shift + random Ô¨Çip\n30.16\nA = Shift + random Ô¨Çip + ran-\ndomly change to grayscale\n32.80\nA = Shift + random Ô¨Çip + ran-\ndom rotation + color distortions\n35.09\nA = Auto Augment [8]\n39.93\nSupervised MAML\n46.81\nif (x0, y0) ‚ààDT , the estimation of the error is going to be highly biased. We conjecture that similar results will be\nobserved for other loss functions as well with the estimate of the loss function being more biased if the samples are\nfrom the training data rather than outside it. As in the outer loop of MAML estimates the generalization error on a\nvalidation set for each task in a batch of tasks, it is important to keep the validation set separate from the training set, as\nthis estimate will be eventually applied to the starter network.\nIn contrast, if we pick our validation set as points in DT , our algorithm is going to learn to minimize a biased estimation\nof the generalization error. Our experiments also show that if we choose the same data for train and test (A(x) = x),\nwe will end up with an accuracy almost the same as training from scratch. UMTRA, however, tries to improve the\nestimation of generalization error with augmentation techniques. Our experiments show that by applying UMTRA with\ngood choice of function for augmentation, we can achieve comparable results with supervised meta-learning algorithms.\nIn our supplementary material, we show that UMTRA is able to adapt very quickly with just few iterations to a new\ntask. Last but not least, in comparison with CACTUs algorithm which applies advanced clustering algorithms such as\nDeepCluster [6], ACAI [5], and BiGAN [10] to generate train and validation set for each task, our method does not\nrequire clustering.\n4\nExperiments\n4.1\nUMTRA on the Omniglot dataset\nOmniglot [23] is a dataset of handwritten characters frequently used to compare few-shot learning algorithms. It\ncomprises 1623 characters from 50 different alphabets. Every character in Omniglot has 20 different instances each was\nwritten by a different person. To allow comparisons with other published results, in our experiments we follow the\nexperimental protocol described in [36]: 1200 characters were used for training, 100 characters were used for validation\nand 323 characters were used for testing.\nUMTRA, like the supervised MAML algorithm, is model-agnostic, that is, it does not impose conditions on the actual\nnetwork architecture used in the learning. This does not, of course, mean that the algorithm performs identically for\nevery network structure and dataset. In order to separate the performance of the architecture and the meta-learner, we\nrun our experiments using an architecture originally proposed in [41]. This classiÔ¨Åer uses four 3 x 3 convolutional\nmodules with 64 Ô¨Ålters each, followed by batch normalization [16], a ReLU nonlinearity and 2 x 2 max-pooling. On the\nresulting feature embedding, the classiÔ¨Åer is implemented as a fully connected layer followed by a softmax layer.\nUMTRA has a relatively large hyperparameter space that includes the augmentation function. As pointed out in a recent\nstudy involving performance comparisons in semi-supervised systems [32], excessive tuning of hyperparameters can\neasily lead to an overestimation of the performance of an approach compared to simpler approaches. Thus, for the\ncomparison in the remainder of this paper, we keep a relatively small budget for hyperparameter search: beyond basic\nsanity checks, we only tested 5-10 hyperparameter combinations per dataset, without specializing them to the N or K\nparameters of the target task. Table 1, left, shows several choices for the augmentation function for the 5-way one-shot\nclassiÔ¨Åcation on Omniglot. Based on this table, in comparing with other approaches, we use an augmentation function\nconsisting of randomly zeroed pixels and random shift.\nIn our experiments, we realized two of the most important hyperparameters in meta-learning are meta-batch size, NMB,\nand number of updates, NU. In table 2, we study the effects of these hyperparameters on the accuracy of the network for\nthe randomly zeroed pixels and random shift augmentation. Based on this experiment, we decide to Ô¨Åx the meta-batch\nsize to 25 and number of updates to 1.\n6\nTable 2: The effect of hyperparameters meta-batch size, NMB, and number of updates, NU on accuracy. Omniglot\n5-way one shot.\n# Updates\nNMB\n1\n2\n4\n8\n16\n25\n1\n67.08\n79.04\n80.72\n81.60\n82.72\n83.80\n5\n76.08\n76.68\n77.20\n79.56\n81.12\n83.32\n10\n79.20\n79.24\n80.92\n80.68\n83.52\n83.26\nTable 3: The effect of the augmentation level on UMTRA‚Äôs accuracy on the Omniglot dataset. In all of the experiments\nwe use random pixel zeroing with meta-batch size NMB = 25 and number of updates NU = 1.\nTranslation Range (Pixels)\n0\n0-3\n3-6\n0-6\n6-9\n9-12\n0-9\nAccuracy %\n67.0\n82.8\n80.4\n83.8\n79.8\n77\n80.4\nIn order to Ô¨Ånd out the relationship between the level of the augmentation and accuracy, we apply different levels of\naugmentation on images. If the generated samples are different from current observation but within the same class\nmanifold, UMTRA performs well. The results of this experiment are shown in table 3.\nThe second consideration is what sort of baseline we should use when evaluating our approach on a few-shot learning\ntask? Clearly, supervised meta-learning approaches such as an original MAML [11] are expected to outperform our\napproach, as they use a labeled training set. A simple baseline is to use the same network architecture being trained\nfrom scratch with only the Ô¨Ånal few-shot labeled set. If our algorithm takes advantage of the unsupervised training set\nU, as expected, it should outperform this baseline.\nA more competitive comparison can be made against networks that are Ô¨Årst trained to obtain a favorable embedding\nusing unsupervised learning on U, with the resulting embedding used on the few-shot learning task. These baselines are\nnot meta-learning approaches, however, we can train them with the same target task training set as UMTRA. Similar\nto [15], we compare the following unsupervised pre-training approaches: ACAI [5], BiGAN [10], DeepCluster [6]\nand InfoGAN [7]. These up-to-date approaches cover a wide range of the recent advances in the area of unsupervised\nfeature learning. Finally, we also compare against the CACTUs unsupervised meta-learning algorithm proposed in the\n[15], combined with MAML and ProtoNets [38]. As a note, another unsupervised meta-learning approach related to\nUMTRA and CACTUs is AAL [2]. However, as [2] doesn‚Äôt compare against stock MAML, the results are not directly\ncomparable.\nTable 4, columns three to six, shows the results of the experiments. For the UMTRA approach we trained for 6000\nmeta-iterations for the 5-way, and 36,000 meta-iterations for the 20-way classiÔ¨Åcations. Our approach, with the\nproposed hyperparameter settings outperforms, with large margins, training from scratch and the approaches based on\nunsupervised representation learning. UMTRA also outperforms, with a smaller margin, the CACTUs approach on all\nmetrics, and in combination with both MAML and ProtoNets.\nAs expected, the supervised meta-learning baselines perform better than UMTRA. To put this value in perspective,\nwe need to take into consideration the vast difference in the number of labels needed for these approaches. In 5-way\none-shot classiÔ¨Åcation, UMTRA obtains a 83.80% accuracy with only 5 labels, while supervised MAML obtains\n94.46% but requires 24005 labels. For 5-way 5-shot classiÔ¨Åcation UMTRA obtains a 95.43% accuracy with only 25\nlabels, while supervised MAML obtains 98.83% with 24025.\n4.2\nUMTRA on the Mini-Imagenet dataset\nThe Mini-Imagenet dataset was introduced by [35] as a subset of the ImageNet dataset [9], suitable as a benchmark for\nfew-shot learning algorithms. The dataset is limited to 100 classes, each with 600 images. We divide our dataset into\ntrain, validation and test subsets according to the experimental protocol proposed by [41]. The classiÔ¨Åer network is\nsimilar to the one used in [11].\nSince Mini-Imagenet is a dataset with larger images and more complex classes compared to Omniglot, we need to\nchoose augmentation functions suitable to the model. We had investigated several simple choices involving random\nÔ¨Çips, shifts, rotation, and color changes. In addition to these hand-crafted algorithms, we also investigated the learned\nauto-augmentation method proposed in [8]. Table 1, right, shows the accuracy results for the tested augmentation\nfunctions. We found that auto-augmentation provided the best results, thus this approach was used in the remainder of\nthe experiments.\n7\nTable 4: Accuracy in % of N-way K-shot (N,K) learning methods on the Omniglot and Mini-Imagenet datasets. The\nACAI / DC label means ACAI Clustering on Omniglot and DeepCluster on Mini-Imagenet. The source of non-UMTRA\nvalues is [15].\nOmniglot\nMini-Imagenet\nAlgorithm (N, K)\nClustering\n(5,1)\n(5,5)\n(20,1)\n(20,5)\n(5,1)\n(5,5)\n(5,20)\n(5,50)\nTraining from scratch\nN/A\n52.50\n74.78\n24.91\n47.62\n27.59\n38.48\n51.53\n59.63\nknn-nearest neighbors\nBiGAN\n49.55\n68.06\n27.37\n46.70\n25.56\n31.10\n37.31\n43.60\nlinear classiÔ¨Åer\nBiGAN\n48.28\n68.72\n27.80\n45.82\n27.08\n33.91\n44.00\n50.41\nMLP with dropout\nBiGAN\n40.54\n62.56\n19.92\n40.71\n22.91\n29.06\n40.06\n48.36\ncluster matching\nBiGAN\n43.96\n58.62\n21.54\n31.06\n24.63\n29.49\n33.89\n36.13\nCACTUs-MAML\nBiGAN\n58.18\n78.66\n35.56\n58.62\n36.24\n51.28\n61.33\n66.91\nCACTUs-ProtoNets\nBiGAN\n54.74\n71.69\n33.40\n50.62\n36.62\n50.16\n59.56\n63.27\nknn-nearest neighbors\nACAI / DC\n57.46\n81.16\n39.73\n66.38\n28.90\n42.25\n56.44\n63.90\nlinear classiÔ¨Åer\nACAI / DC\n61.08\n81.82\n43.20\n66.33\n29.44\n39.79\n56.19\n65.28\nMLP with dropout\nACAI / DC\n51.95\n77.20\n30.65\n58.62\n29.03\n39.67\n52.71\n60.95\ncluster matching\nACAI / DC\n54.94\n71.09\n32.19\n45.93\n22.20\n23.50\n24.97\n26.87\nCACTUs-MAML\nACAI / DC\n68.84\n87.78\n48.09\n73.36\n39.90\n53.97\n63.84\n69.64\nCACTUs-ProtoNets\nACAI / DC\n68.12\n83.58\n47.75\n66.27\n39.18\n53.36\n61.54\n63.55\nUMTRA (ours)\nN/A\n83.80\n95.43\n74.25\n92.12\n39.93\n50.73\n61.11\n67.15\nMAML (Supervised)\nN/A\n94.46\n98.83\n84.60\n96.29\n46.81\n62.13\n71.03\n75.54\nProtoNets (Supervised)\nN/A\n98.35\n99.58\n95.31\n98.81\n46.56\n62.29\n70.05\n72.04\nThe last four columns of Table 4 lists the experimental results for few-shot classiÔ¨Åcation learning on the Mini-Imagenet\ndataset. Similar to the Omniglot dataset, UMTRA performs better than learning from scratch and all the approaches that\nuse unsupervised representation learning. It performs weaker than supervised meta-learning approaches that use labeled\ndata. Compared to the various combinations involving the CACTUs unsupervised meta-learning algorithm, UMTRA\nperforms better on 5-way one-shot classiÔ¨Åcation, while it is outperformed by the CACTUs-MAML with DeepCluster\ncombination for the 5, 20 and 50 shot classiÔ¨Åcation.\nA possible question might be raised whether the improvements we see are due to the meta-learning process or due to\nthe augmentation enriching the few shot dataset. To investigate this, we performed several experiments on Omniglot\nand Mini-Imagenet by training the target tasks from scratch on the augmented target dataset. For 5-way, 1-shot\nlearning on Omniglot the accuracy was: training from scratch 52.5%, training from scratch with augmentation 55.8%,\nUMTRA 83.8%. For MiniImagenet the numbers were: from scratch without augmentation 27.6%, from scratch with\naugmentation 28.8%, UMTRA 39.93%. We conclude that while augmentation does provide a (minor) improvement on\nthe target training by itself, the majority of the improvement shown by UMTRA is due to the meta-learning process.\nThe results on Omniglot and Mini-Imagenet allow us to draw the preliminary conclusions that unsupervised meta-\nlearning approaches like UMTRA and CACTUs, which generate meta tasks Ti from the unsupervised training data tend\nto outperform other approaches for a given unsupervised training set U. UMTRA and CACTUs use different, orthogonal\napproaches for building T . UMTRA uses the statistical likelihood of picking different classes for the training data of Ti\nin case of K = 1 and large number of classes, and an augmentation function T for the validation data. CACTUs relies\non an unsupervised clustering algorithm to provide a statistical likelihood of difference and sameness in the training and\nvalidation data of Ti. Except in the case of UMTRA with A = 1, both approaches require domain speciÔ¨Åc knowledge.\nThe choice of the right augmentation function for UMTRA, the right clustering approach for CACTUs, and the other\nhyperparameters (for both approaches) have a strong impact on the performance.\n5\nConclusions\nIn this paper, we described the UMTRA algorithm for few-shot and one-shot learning of classiÔ¨Åers. UMTRA performs\nmeta-learning on an unlabeled dataset in an unsupervised fashion, without putting any constraint on the classiÔ¨Åer\nnetwork architecture. Experimental studies over the few-shot learning image benchmarks Omniglot and Mini-Imagenet\nshow that UMTRA outperforms learning-from-scratch approaches and approaches based on unsupervised representation\nlearning. It alternated in obtaining by best result with the recently proposed CACTUs algorithm that takes a different\napproach to unsupervised meta-learning by applying clustering on an unlabeled dataset. The statistical sampling and\naugmentation performed by UMTRA can be seen as a cheaper alternative to the dataset-wide clustering performed\nby CACTUs. The results also open the possibility that these approaches might be orthogonal, and in combination\nmight yield an even better performance. For all experiments, UMTRA performed worse than the equivalent supervised\n8\nmeta-learning approach - but requiring 3-4 orders of magnitude less labeled data. The supplemental material shows that\nUMTRA is not limited to image classiÔ¨Åcation but it can be applied to other tasks as well, such as video classiÔ¨Åcation.\nAcknowledgements: This research is based upon work supported in parts by the National Science Foundation under\nGrant numbers IIS-1409823 and IIS-1741431 and OfÔ¨Åce of the Director of National Intelligence (ODNI), Intelligence\nAdvanced Research Projects Activity (IARPA), via IARPA R&D Contract No. D17PC00345. The views, Ô¨Åndings,\nopinions, and conclusions or recommendations contained herein are those of the authors and should not be interpreted\nas necessarily representing the ofÔ¨Åcial policies or endorsements, either expressed or implied, of the NSF, ODNI, IARPA,\nor the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental\npurposes notwithstanding any copyright annotation thereon.\nReferences\n[1] Yaser S Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin. Learning from data, volume 4. AMLBook\nNew York, NY, USA, 2012.\n[2] Antreas Antoniou and Amos Storkey. Assume, augment and learn: Unsupervised few-shot meta-learning via\nrandom labels and data augmentation. arXiv preprint arXiv:1902.09884, 2019.\n[3] Jimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. Using fast weights to attend\nto the recent past. In Proc. of Advances in Neural Information Processing Systems (NIPS), pages 4331‚Äì4339,\n2016.\n[4] Yoshua Bengio, Samy Bengio, and Jocelyn Cloutier. Learning a synaptic learning rule. Universit√© de Montr√©al,\nD√©partement d‚Äôinformatique et de recherche op√©rationnelle, 1990.\n[5] David Berthelot, Colin Raffel, Aurko Roy, and Ian Goodfellow. Understanding and improving interpolation in\nautoencoders via an adversarial regularizer. arXiv preprint arXiv:1807.07543, 2018.\n[6] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning\nof visual features. In Proc. of the European Conf. on Computer Vision (ECCV), pages 132‚Äì149, 2018.\n[7] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. InfoGAN: Interpretable\nrepresentation learning by information maximizing generative adversarial nets. In Proc. of Advances in Neural\nInformation Processing Systems (NIPS), pages 2172‚Äì2180, 2016.\n[8] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning\naugmentation policies from data. arXiv preprint arXiv:1805.09501, 2018.\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 248‚Äì255. Ieee, 2009.\n[10] Jeff Donahue, Philipp Kr√§henb√ºhl, and Trevor Darrell.\nAdversarial feature learning.\narXiv preprint\narXiv:1605.09782, 2016.\n[11] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep\nnetworks. Proc. of Int‚Äôl Conf. on Machine Leanring (ICML), 2017.\n[12] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning, volume 1. Springer\nseries in statistics New York, 2001.\n[13] Federico Girosi, Michael Jones, and Tomaso Poggio. Regularization theory and neural networks architectures.\nNeural computation, 7(2):219‚Äì269, 1995.\n[14] Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn, and Sergey Levine. Unsupervised meta-learning for\nreinforcement learning. arXiv preprint arXiv:1806.04640, 2018.\n[15] Kyle Hsu, Sergey Levine, and Chelsea Finn.\nUnsupervised learning via meta-learning.\narXiv preprint\narXiv:1810.02334, 2018.\n[16] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal\ncovariate shift. arXiv preprint arXiv:1502.03167, 2015.\n9\n[17] Mihir Jain, Jan C van Gemert, Thomas Mensink, and Cees GM Snoek. Objects2action: Classifying and localizing\nactions without any video example. In Proc. of the IEEE Int‚Äôl Conf. on computer vision, pages 4588‚Äì4596, 2015.\n[18] Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An introduction to statistical learning,\nvolume 112. Springer, 2013.\n[19] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama,\nand Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding. In Proc. of the 22nd ACM Int‚Äôlal\nConf. on Multimedia, pages 675‚Äì678. ACM, 2014.\n[20] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale\nvideo classiÔ¨Åcation with convolutional neural networks. In Proc. of the IEEE Conf. on Computer Vision and\nPattern Recognition (CVPR), pages 1725‚Äì1732, 2014.\n[21] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale\nvideo classiÔ¨Åcation with convolutional neural networks. In CVPR, 2014.\n[22] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio\nViola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint\narXiv:1705.06950, 2017.\n[23] Brenden Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua Tenenbaum. One shot learning of simple visual\nconcepts. In Proc. of the Annual Meeting of the Cognitive Science Society, volume 33, 2011.\n[24] Ziwei Liu, Ping Luo, Xinaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proc. of\nInt‚Äôlal Conf. on Computer Vision (ICCV), December 2015.\n[25] Zelun Luo, Yuliang Zou, Judy Hoffman, and Li F Fei-Fei. Label efÔ¨Åcient learning of transferable representations\nacross domains and tasks. In Proc. of Advances in Neural Information Processing Systems (NIPS), pages 165‚Äì177,\n2017.\n[26] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research,\n9(Nov):2579‚Äì2605, 2008.\n[27] Franziska Meier, Daniel Kappler, and Stefan Schaal. Online learning of a memory for learning rates. In Proc. of\nIEEE Int‚Äôl Conf. on Robotics and Automation (ICRA), pages 2425‚Äì2432, 2018.\n[28] Luke Metz, Niru Maheswaranathan, Brian Cheung, and Jascha Sohl-Dickstein. Learning unsupervised learning\nrules. arXiv preprint arXiv:1804.00222, 2018.\n[29] Thomas Miconi, Jeff Clune, and Kenneth O Stanley. Differentiable plasticity: training plastic neural networks\nwith backpropagation. arXiv preprint arXiv:1804.02464, 2018.\n[30] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-learner. arXiv\npreprint arXiv:1707.03141, 2018.\n[31] Alex Nichol and John Schulman. Reptile: a scalable metalearning algorithm. arXiv preprint arXiv:1803.02999,\n2018.\n[32] Avital Oliver, Augustus Odena, Colin A Raffel, Ekin Dogus Cubuk, and Ian Goodfellow. Realistic evaluation of\ndeep semi-supervised learning algorithms. In Advances in Neural Information Processing Systems (NIPS), pages\n3235‚Äì3246, 2018.\n[33] Eunbyung Park and Alexander C Berg. Meta-tracker: Fast and robust online adaptation for visual object trackers.\nIn Proc. of the European Conf. on Computer Vision (ECCV), pages 569‚Äì585, 2018.\n[34] Deepak Pathak, Ross Girshick, Piotr Doll√°r, Trevor Darrell, and Bharath Hariharan. Learning features by watching\nobjects move. In Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition(CVPR), pages 2701‚Äì2710,\n2017.\n[35] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. Proc. of Int‚Äôl Conf. on Learning\nRepresentations (ICLR), 2016.\n10\n[36] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-learning with\nmemory-augmented neural networks. In Proc. of Int‚Äôlal Conf. on Machine Learning (ICML), pages 1842‚Äì1850,\n2016.\n[37] J√ºrgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn: the\nmeta-meta-... hook. PhD thesis, Technische Universit√§t M√ºnchen, 1987.\n[38] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In Proc. of Advances\nin Neural Information Processing Systems (NIPS), pages 4077‚Äì4087, 2017.\n[39] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. UCF101: A dataset of 101 human actions classes\nfrom videos in the wild. arXiv preprint arXiv:1212.0402, 2012.\n[40] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal\nfeatures with 3d convolutional networks. In Proc. of the IEEE Int‚Äôl Conf. on Computer Vision (ICCV), pages\n4489‚Äì4497, 2015.\n[41] Oriol Vinyals, Charles Blundell, Tim Lillicrap, and Daan Wierstra. Matching networks for one shot learning. In\nProc. of Advances in Neural Information Processing Systems (NIPS), pages 3630‚Äì3638, 2016.\n[42] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric\ninstance discrimination. In Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition(CVPR), pages\n3733‚Äì3742, 2018.\n[43] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning\nrequires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.\n11\nSupplementary Material for Unsupervised Meta-Learning for Few-Shot Image ClassiÔ¨Åcation\nEvolution of accuracy during training\nIn these series of experiments we study the evolution of the accuracy obtained after a speciÔ¨Åc number of gradient\ntraining steps during the target learning phase. The results for Omniglot are shown in Figure 3 (with K=1), while those\nfor Mini-Imagenet in Figure 4 with K values of 1, 5 and 20. For both datasets, we compare learning from scratch,\nUMTRA and supervised MAML. As expected, both MAML and UMTRA reach their accuracy plateau very quickly\nduring target training, while learning from scratch takes a larger number of training steps. Further training does not\nappear to provide any beneÔ¨Åt for either approach. The results are averaged among 1000 tasks. This demonstrates\nthat UMTRA has the capacity to learn to adapt to novel tasks by just looking at unlabeled data and generating tasks\nfrom that dataset in an unsupervised manner. An interesting phenomena happens with K = 5 and K = 20 values for\nMini-Imagenet: the accuracy curve of UMTRA dips after the Ô¨Årst iteration, and it takes several iterations to recover. We\nconjecture that this is a result of the fact that UMTRA sets K = 1 during meta-learning, thus the resulting network is\nbest optimized to learn from one sample per class.\n0\n10\n20\n30\n40\n50\nNumber of Iterations\n20\n30\n40\n50\n60\n70\n80\n90\nAccuracy (%)\nUnsupervised UMTRA\nSupervised MAML\nTraining From Scratch\nFigure 3: The accuracy curves during the target training task on the Omniglot dataset for K = 1. The band around\nlines denotes a 95% conÔ¨Ådence interval.\nFeature Representations\nTo compare generalization of training from scratch, UMTRA and supervised MAML, we visualize the activations\nof the last hidden layer of the network on Omniglot dataset by t-SNE. We compare all of the methods on the same\ntarget training task which is constructed by sampling Ô¨Åve characters from test data and selecting one image from each\ncharacter class randomly. Each character has 20 different instances. Figure 5 shows the t-SNE visualization of raw\npixel values of these 100 images. Instances which are sampled for the one-shot learning task are connected to each\nother by dotted lines. Figure 6 shows the visualization of the last hidden layer activations for the same task. UMTRA as\nwell as MAML can adapt quickly to a feature space which has a better generalization than training from scratch.\nVideo Domain\nIn this section, we show how the UMTRA can be applied to video action recognition, a domain signiÔ¨Åcantly more\ncomplex and data intensive than the one used in the few-shot learning benchmarks such as Omniglot and Mini-Imagenet.\nTo the best of our knowledge, we are the Ô¨Årst to apply meta-learning to video action recognition. We perform our\ncomparisons using one of the standard video action recognition datasets, UCF-101 . UCF-101 includes 101 action\nclasses divided into Ô¨Åve types: Human-Object Interaction, Body-Motion Only, Human Human Interaction, Playing\nMusical Instruments and Sports. The dataset is composed of snippets of Youtube videos. Many videos have poor\nlighting, cluttered background and severe camera motion. As the classiÔ¨Åer on which to apply the meta-learning process,\nwe use a 3D convolution network, C3D .\nPerforming unsupervised meta-learning on video data, requires several adjustments to the UMTRA workÔ¨Çow, with\nregards to the initialization of the classiÔ¨Åer, the split between meta-learning data and testing data, and the augmentation\nfunction.\nFirst, networks of the complexity of C3D cannot be learned from scratch using the limited amount of data available in\nfew-shot learning. In the video action recognition research, it is common practice to start with a network that had been\npre-trained on a large dataset, such as Sports-1M dataset , an approach we also use in all our experiments.\n12\nK = 1\nK = 5\n0\n10\n20\n30\n40\n50\n20\n25\n30\n35\n40\n45\nAccuracy (%)\n0\n10\n20\n30\n40\n50\n20\n25\n30\n35\n40\n45\n50\n55\nAccuracy (%)\nK = 20\n0\n10\n20\n30\n40\n50\nNumber of Iterations\n20\n30\n40\n50\n60\nAccuracy (%)\nUnsupervised UMTRA\nSupervised MAML\nTraining From Scratch\nFigure 4: The accuracy curves during the target training task on the Mini-Imagenet dataset. Accuracy curves are shown\nfor K = 1 (Top left), K = 5 (Top right), and K = 20 (Bottom). The band around lines denotes a 95% conÔ¨Ådence\ninterval.\nClass 0 Training\nClass 0\nClass 1 Training\nClass 1\nClass 2 Training\nClass 2\nClass 3 Training\nClass 3\nClass 4 Training\nClass 4\nFigure 5: t-SNE on the Omniglot raw pixel values.\nSecond, we make the choice to use two different datasets for the meta-learning phase (Kinetics) and for the few-shot\nlearning and evaluation (UCF-101 ). This gives us a larger dataset for training since Kinetics contains 400 actions, but it\nintroduces an additional challenge of domain-shift: the network is pre-trained on Sports-1M, meta-trained on Kinetics\nand few-shot trained on UCF-101. This approach, however, closely resembles the practical setup when we need to do\nfew-shot learning on a novel domain. When using the Kinetics dataset, we limit it to 20 instances per class.\nFor the augmentation function A, working in the video domain opens a new possibility, of creating an augmented sample\nby choosing a temporally shifted video fragment from the same video. In other words, we can use self supervision in\nvideo domain: The augmentation is to sample another part of the same video clip. Figure 7 shows some samples of these\naugmentations. In our experiments, we have experimented both with UMTRA (using a Kinetics dataset stripped from\nlabels), and supervised meta-learning (retaining the labels on Kinetics). This supervised meta-learning experiment is\nalso signiÔ¨Åcant because, to the best of our knowledge, meta-learning has never been applied to human action recognition\nfrom videos.\n13\nTraining from Scratch\nClass 0 Training\nClass 0\nClass 1 Training\nClass 1\nClass 2 Training\nClass 2\nClass 3 Training\nClass 3\nClass 4 Training\nClass 4\nClass 0 Training\nClass 0\nClass 1 Training\nClass 1\nClass 2 Training\nClass 2\nClass 3 Training\nClass 3\nClass 4 Training\nClass 4\nUMTRA\nClass 0 Training\nClass 0\nClass 1 Training\nClass 1\nClass 2 Training\nClass 2\nClass 3 Training\nClass 3\nClass 4 Training\nClass 4\nClass 0 Training\nClass 0\nClass 1 Training\nClass 1\nClass 2 Training\nClass 2\nClass 3 Training\nClass 3\nClass 4 Training\nClass 4\nMAML\nClass 0 Training\nClass 0\nClass 1 Training\nClass 1\nClass 2 Training\nClass 2\nClass 3 Training\nClass 3\nClass 4 Training\nClass 4\nClass 0 Training\nClass 0\nClass 1 Training\nClass 1\nClass 2 Training\nClass 2\nClass 3 Training\nClass 3\nClass 4 Training\nClass 4\nFigure 6: Visualization of the last hidden layer activation values by t-SNE on the Omniglot dataset before target task\ntraining (Left), and after target task training (Right). Visualized features are shown for training from scratch (Top),\nUMTRA (Middle), and MAML (Bottom). Each class is shown by a different color and shape. From each class one\ninstance is used for target task training. Training instances are denoted by larger and lighter symbols and are connected\nto each other by dotted lines\n14\nùë• π\nùë•\nùë• π\nùë•\nùë• π\nùë•\nFigure 7: Example of the training data and the augmentation function A for video. The training data x is a 16 frame\nsegment starting from a random time in the video sample (Here we show three frames of a sample at each column). The\nvalidation data x‚Ä≤ = A(x) is also a 16 frame segment, starting from a different, randomly selected time from the same\nvideo sample.\nTable 5: Accuracy and F1-Score for a 5-way, one-shot classiÔ¨Åer trained and evaluated on classes sampled from UCF-101.\nAll training (even for ‚Äútraining from scratch‚Äù), employ a C3D network pre-trained on Sports-1M. For all approaches,\nnone of the UCF-101 classes was seen during pre- or meta-learning.\nAlgorithm\nTest Accuracy / F1-Score\nTraining from scratch\n29.30 / 20.48\nPre-trained on Kinetics\n45.51 / 42.49\nUMTRA on unlabeled Kinetics (ours)\n60.33 / 58.47\nSupervised MAML on Kinetics\n71.08 / 69.44\nIn our evaluation, we perform 30 different experiments. At each experiment we sample 5 classes from UCF-101,\nperform the one-shot learning, and evaluate the classiÔ¨Åer on all the examples for the 5 classes from UCF-101. As the\nnumber of samples per class are not the same for all classes, in Table 5 we report both the accuracy and F1-score.\nThe results allow us to draw several conclusions. The relative accuracy ranking between training from scratch, pre-\ntraining and unsupervised meta-learning and supervised meta-learning remained unchanged. Supervised meta-learning\nhad proven feasible for one-shot classiÔ¨Åer training for video action recognition. UMTRA performs better than other\napproaches that use unsupervised data. Finally, we found that the domain shift from Kinetics to UCF-101 was successful.\n15\n",
  "categories": [
    "cs.CV",
    "cs.LG"
  ],
  "published": "2018-11-28",
  "updated": "2019-11-07"
}