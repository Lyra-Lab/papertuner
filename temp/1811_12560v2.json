{
  "id": "http://arxiv.org/abs/1811.12560v2",
  "title": "An Introduction to Deep Reinforcement Learning",
  "authors": [
    "Vincent Francois-Lavet",
    "Peter Henderson",
    "Riashat Islam",
    "Marc G. Bellemare",
    "Joelle Pineau"
  ],
  "abstract": "Deep reinforcement learning is the combination of reinforcement learning (RL)\nand deep learning. This field of research has been able to solve a wide range\nof complex decision-making tasks that were previously out of reach for a\nmachine. Thus, deep RL opens up many new applications in domains such as\nhealthcare, robotics, smart grids, finance, and many more. This manuscript\nprovides an introduction to deep reinforcement learning models, algorithms and\ntechniques. Particular focus is on the aspects related to generalization and\nhow deep RL can be used for practical applications. We assume the reader is\nfamiliar with basic machine learning concepts.",
  "text": "An Introduction to Deep\nReinforcement Learning\nVincent François-Lavet, Peter Henderson, Riashat Islam, Marc G. Bellemare and Joelle\nPineau (2018), “An Introduction to Deep Reinforcement Learning”, Foundations and\nTrends in Machine Learning: Vol. 11, No. 3-4. DOI: 10.1561/2200000071.\nVincent François-Lavet\nMcGill University\nvincent.francois-lavet@mcgill.ca\nPeter Henderson\nMcGill University\npeter.henderson@mail.mcgill.ca\nRiashat Islam\nMcGill University\nriashat.islam@mail.mcgill.ca\nMarc G. Bellemare\nGoogle Brain\nbellemare@google.com\nJoelle Pineau\nFacebook, McGill University\njpineau@cs.mcgill.ca\nBoston — Delft\narXiv:1811.12560v2  [cs.LG]  3 Dec 2018\nContents\n1\nIntroduction\n2\n1.1\nMotivation . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n1.2\nOutline . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n2\nMachine learning and deep learning\n6\n2.1\nSupervised learning and the concepts of bias and overﬁtting\n7\n2.2\nUnsupervised learning . . . . . . . . . . . . . . . . . . . .\n9\n2.3\nThe deep learning approach . . . . . . . . . . . . . . . . .\n10\n3\nIntroduction to reinforcement learning\n15\n3.1\nFormal framework . . . . . . . . . . . . . . . . . . . . . .\n16\n3.2\nDiﬀerent components to learn a policy . . . . . . . . . . .\n20\n3.3\nDiﬀerent settings to learn a policy from data . . . . . . . .\n21\n4\nValue-based methods for deep RL\n24\n4.1\nQ-learning . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n4.2\nFitted Q-learning\n. . . . . . . . . . . . . . . . . . . . . .\n25\n4.3\nDeep Q-networks\n. . . . . . . . . . . . . . . . . . . . . .\n27\n4.4\nDouble DQN . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n4.5\nDueling network architecture . . . . . . . . . . . . . . . .\n29\n4.6\nDistributional DQN . . . . . . . . . . . . . . . . . . . . .\n31\n4.7\nMulti-step learning . . . . . . . . . . . . . . . . . . . . . .\n32\n4.8\nCombination of all DQN improvements and variants of DQN 34\n5\nPolicy gradient methods for deep RL\n36\n5.1\nStochastic Policy Gradient\n. . . . . . . . . . . . . . . . .\n37\n5.2\nDeterministic Policy Gradient . . . . . . . . . . . . . . . .\n39\n5.3\nActor-Critic Methods\n. . . . . . . . . . . . . . . . . . . .\n40\n5.4\nNatural Policy Gradients\n. . . . . . . . . . . . . . . . . .\n42\n5.5\nTrust Region Optimization\n. . . . . . . . . . . . . . . . .\n43\n5.6\nCombining policy gradient and Q-learning . . . . . . . . .\n44\n6\nModel-based methods for deep RL\n46\n6.1\nPure model-based methods . . . . . . . . . . . . . . . . .\n46\n6.2\nIntegrating model-free and model-based methods\n. . . . .\n49\n7\nThe concept of generalization\n53\n7.1\nFeature selection . . . . . . . . . . . . . . . . . . . . . . .\n58\n7.2\nChoice of the learning algorithm and function approximator\nselection . . . . . . . . . . . . . . . . . . . . . . . . . . .\n59\n7.3\nModifying the objective function . . . . . . . . . . . . . .\n61\n7.4\nHierarchical learning . . . . . . . . . . . . . . . . . . . . .\n62\n7.5\nHow to obtain the best bias-overﬁtting tradeoﬀ. . . . . .\n63\n8\nParticular challenges in the online setting\n66\n8.1\nExploration/Exploitation dilemma . . . . . . . . . . . . . .\n66\n8.2\nManaging experience replay . . . . . . . . . . . . . . . . .\n71\n9\nBenchmarking Deep RL\n73\n9.1\nBenchmark Environments . . . . . . . . . . . . . . . . . .\n73\n9.2\nBest practices to benchmark deep RL\n. . . . . . . . . . .\n78\n9.3\nOpen-source software for Deep RL . . . . . . . . . . . . .\n80\n10 Deep reinforcement learning beyond MDPs\n81\n10.1 Partial observability and the distribution of (related) MDPs\n81\n10.2 Transfer learning . . . . . . . . . . . . . . . . . . . . . . .\n86\n10.3 Learning without explicit reward function . . . . . . . . . .\n89\n10.4 Multi-agent systems . . . . . . . . . . . . . . . . . . . . .\n91\n11 Perspectives on deep reinforcement learning\n94\n11.1 Successes of deep reinforcement learning . . . . . . . . . .\n94\n11.2 Challenges of applying reinforcement learning to real-world\nproblems . . . . . . . . . . . . . . . . . . . . . . . . . . .\n95\n11.3 Relations between deep RL and neuroscience . . . . . . . .\n96\n12 Conclusion\n99\n12.1 Future development of deep RL . . . . . . . . . . . . . . .\n99\n12.2 Applications and societal impact of deep RL . . . . . . . . 100\nAppendices\n103\nReferences\n106\nAn Introduction to Deep\nReinforcement Learning\nVincent François-Lavet1, Peter Henderson2, Riashat Islam3, Marc\nG. Bellemare4 and Joelle Pineau5\n1McGill University; vincent.francois-lavet@mcgill.ca\n2McGill University; peter.henderson@mail.mcgill.ca\n3McGill University; riashat.islam@mail.mcgill.ca\n4Google Brain; bellemare@google.com\n5Facebook, McGill University; jpineau@cs.mcgill.ca\nABSTRACT\nDeep reinforcement learning is the combination of reinforce-\nment learning (RL) and deep learning. This ﬁeld of research\nhas been able to solve a wide range of complex decision-\nmaking tasks that were previously out of reach for a machine.\nThus, deep RL opens up many new applications in domains\nsuch as healthcare, robotics, smart grids, ﬁnance, and many\nmore. This manuscript provides an introduction to deep\nreinforcement learning models, algorithms and techniques.\nParticular focus is on the aspects related to generalization\nand how deep RL can be used for practical applications. We\nassume the reader is familiar with basic machine learning\nconcepts.\n1\nIntroduction\n1.1\nMotivation\nA core topic in machine learning is that of sequential decision-making.\nThis is the task of deciding, from experience, the sequence of actions\nto perform in an uncertain environment in order to achieve some\ngoals. Sequential decision-making tasks cover a wide range of possible\napplications with the potential to impact many domains, such as\nrobotics, healthcare, smart grids, ﬁnance, self-driving cars, and many\nmore.\nInspired by behavioral psychology (see e.g., Sutton, 1984), rein-\nforcement learning (RL) proposes a formal framework to this problem.\nThe main idea is that an artiﬁcial agent may learn by interacting with\nits environment, similarly to a biological agent. Using the experience\ngathered, the artiﬁcial agent should be able to optimize some objectives\ngiven in the form of cumulative rewards. This approach applies in prin-\nciple to any type of sequential decision-making problem relying on past\nexperience. The environment may be stochastic, the agent may only\nobserve partial information about the current state, the observations\nmay be high-dimensional (e.g., frames and time series), the agent may\nfreely gather experience in the environment or, on the contrary, the data\n2\n1.2. Outline\n3\nmay be may be constrained (e.g., not access to an accurate simulator\nor limited data).\nOver the past few years, RL has become increasingly popular due\nto its success in addressing challenging sequential decision-making\nproblems. Several of these achievements are due to the combination of\nRL with deep learning techniques (LeCun et al., 2015; Schmidhuber,\n2015; Goodfellow et al., 2016). This combination, called deep RL, is\nmost useful in problems with high dimensional state-space. Previous RL\napproaches had a diﬃcult design issue in the choice of features (Munos\nand Moore, 2002; Bellemare et al., 2013). However, deep RL has been\nsuccessful in complicated tasks with lower prior knowledge thanks to its\nability to learn diﬀerent levels of abstractions from data. For instance,\na deep RL agent can successfully learn from visual perceptual inputs\nmade up of thousands of pixels (Mnih et al., 2015). This opens up the\npossibility to mimic some human problem solving capabilities, even in\nhigh-dimensional space — which, only a few years ago, was diﬃcult to\nconceive.\nSeveral notable works using deep RL in games have stood out for\nattaining super-human level in playing Atari games from the pixels\n(Mnih et al., 2015), mastering Go (Silver et al., 2016a) or beating the\nworld’s top professionals at the game of Poker (Brown and Sandholm,\n2017; Moravčik et al., 2017). Deep RL also has potential for real-world\napplications such as robotics (Levine et al., 2016; Gandhi et al., 2017;\nPinto et al., 2017), self-driving cars (You et al., 2017), ﬁnance (Deng\net al., 2017) and smart grids (François-Lavet, 2017), to name a few.\nNonetheless, several challenges arise in applying deep RL algorithms.\nAmong others, exploring the environment eﬃciently or being able\nto generalize a good behavior in a slightly diﬀerent context are not\nstraightforward. Thus, a large array of algorithms have been proposed\nfor the deep RL framework, depending on a variety of settings of the\nsequential decision-making tasks.\n1.2\nOutline\nThe goal of this introduction to deep RL is to guide the reader towards\neﬀective use and understanding of core methods, as well as provide\n4\nIntroduction\nreferences for further reading. After reading this introduction, the reader\nshould be able to understand the key diﬀerent deep RL approaches and\nalgorithms and should be able to apply them. The reader should also\nhave enough background to investigate the scientiﬁc literature further\nand pursue research on deep RL.\nIn Chapter 2, we introduce the ﬁeld of machine learning and the deep\nlearning approach. The goal is to provide the general technical context\nand explain brieﬂy where deep learning is situated in the broader ﬁeld\nof machine learning. We assume the reader is familiar with basic notions\nof supervised and unsupervised learning; however, we brieﬂy review the\nessentials.\nIn Chapter 3, we provide the general RL framework along with\nthe case of a Markov Decision Process (MDP). In that context, we\nexamine the diﬀerent methodologies that can be used to train a deep\nRL agent. On the one hand, learning a value function (Chapter 4)\nand/or a direct representation of the policy (Chapter 5) belong to the\nso-called model-free approaches. On the other hand, planning algorithms\nthat can make use of a learned model of the environment belong to the\nso-called model-based approaches (Chapter 6).\nWe dedicate Chapter 7 to the notion of generalization in RL.\nWithin either a model-based or a model-free approach, we discuss the\nimportance of diﬀerent elements: (i) feature selection, (ii) function\napproximator selection, (iii) modifying the objective function and\n(iv) hierarchical learning. In Chapter 8, we present the main challenges of\nusing RL in the online setting. In particular, we discuss the exploration-\nexploitation dilemma and the use of a replay memory.\nIn Chapter 9, we provide an overview of diﬀerent existing bench-\nmarks for evaluation of RL algorithms. Furthermore, we present a set\nof best practices to ensure consistency and reproducibility of the results\nobtained on the diﬀerent benchmarks.\nIn Chapter 10, we discuss more general settings than MDPs: (i) the\nPartially Observable Markov Decision Process (POMDP), (ii) the\ndistribution of MDPs (instead of a given MDP) along with the notion\nof transfer learning, (iii) learning without explicit reward function and\n(iv) multi-agent systems. We provide descriptions of how deep RL can\nbe used in these settings.\n1.2. Outline\n5\nIn Chapter 11, we present broader perspectives on deep RL. This\nincludes a discussion on applications of deep RL in various domains,\nalong with the successes achieved and remaining challenges (e.g. robotics,\nself driving cars, smart grids, healthcare, etc.). This also includes a brief\ndiscussion on the relationship between deep RL and neuroscience.\nFinally, we provide a conclusion in Chapter 12 with an outlook on\nthe future development of deep RL techniques, their future applications,\nas well as the societal impact of deep RL and artiﬁcial intelligence.\n2\nMachine learning and deep learning\nMachine learning provides automated methods that can detect patterns\nin data and use them to achieve some tasks (Christopher, 2006; Murphy,\n2012). Three types of machine learning tasks can be considered:\n• Supervised learning is the task of inferring a classiﬁcation or\nregression from labeled training data.\n• Unsupervised learning is the task of drawing inferences from\ndatasets consisting of input data without labeled responses.\n• Reinforcement learning (RL) is the task of learning how agents\nought to take sequences of actions in an environment in order to\nmaximize cumulative rewards.\nTo solve these machine learning tasks, the idea of function\napproximators is at the heart of machine learning. There exist many\ndiﬀerent types of function approximators: linear models (Anderson et al.,\n1958), SVMs (Cortes and Vapnik, 1995), decisions tree (Liaw, Wiener,\net al., 2002; Geurts et al., 2006), Gaussian processes (Rasmussen, 2004),\ndeep learning (LeCun et al., 2015; Schmidhuber, 2015; Goodfellow et al.,\n2016), etc.\n6\n2.1. Supervised learning and the concepts of bias and overﬁtting\n7\nIn recent years, mainly due to recent developments in deep learning,\nmachine learning has undergone dramatic improvements when learning\nfrom high-dimensional data such as time series, images and videos. These\nimprovements can be linked to the following aspects: (i) an exponential\nincrease of computational power with the use of GPUs and distributed\ncomputing (Krizhevsky et al., 2012), (ii) methodological breakthroughs\nin deep learning (Srivastava et al., 2014; Ioﬀe and Szegedy, 2015; He\net al., 2016; Szegedy et al., 2016; Klambauer et al., 2017), (iii) a growing\neco-system of softwares such as Tensorﬂow (Abadi et al., 2016) and\ndatasets such as ImageNet (Russakovsky et al., 2015). All these aspects\nare complementary and, in the last few years, they have lead to a\nvirtuous circle for the development of deep learning.\nIn this chapter, we discuss the supervised learning setting along\nwith the key concepts of bias and overﬁtting. We brieﬂy discuss the\nunsupervised setting with tasks such as data compression and generative\nmodels. We also introduce the deep learning approach that has become\nkey to the whole ﬁeld of machine learning. Using the concepts introduced\nin this chapter, we cover the reinforcement learning setting in later\nchapters.\n2.1\nSupervised learning and the concepts of bias and overﬁtting\nIn its most abstract form, supervised learning consists in ﬁnding a\nfunction f : X →Y that takes as input x ∈X and gives as output\ny ∈Y (X and Y depend on the application):\ny = f(x).\n(2.1)\nA supervised learning algorithm can be viewed as a function that\nmaps a dataset DLS of learning samples (x, y) i.i.d.\n∼(X, Y ) into a model.\nThe prediction of such a model at a point x ∈X of the input space\nis denoted by f(x | DLS). Assuming a random sampling scheme, s.t.\nDLS ∼DLS, f(x | DLS) is a random variable, and so is its average error\nover the input space. The expected value of this quantity is given by:\nI[f] = E\nX\nE\nDLS\nE\nY |XL (Y, f(X | DLS)),\n(2.2)\n8\nMachine learning and deep learning\nwhere L(·, ·) is the loss function. If L(y, ˆy) = (y −ˆy)2, the error\ndecomposes naturally into a sum of a bias term and a variance term1.\nThis bias-variance decomposition can be useful because it highlights a\ntradeoﬀbetween an error due to erroneous assumptions in the model\nselection/learning algorithm (the bias) and an error due to the fact that\nonly a ﬁnite set of data is available to learn that model (the parametric\nvariance). Note that the parametric variance is also called the overﬁtting\nerror2. Even though there is no such direct decomposition for other loss\nfunctions (James, 2003), there is always a tradeoﬀbetween a suﬃciently\nrich model (to reduce the model bias, which is present even when the\namount of data would be unlimited) and a model not too complex (so as\nto avoid overﬁtting to the limited amount of data). Figure 2.1 provides\nan illustration.\nWithout knowing the joint probability distribution, it is impossible\nto compute I[f]. Instead, we can compute the empirical error on a\nsample of data. Given n data points (xi, yi), the empirical error is\nIS[f] = 1\nn\nn\nX\ni=1\nL(yi, f(xi)).\nThe generalization error is the diﬀerence between the error on a\nsample set (used for training) and the error on the underlying joint\nprobability distribution. It is deﬁned as\nG = I[f] −IS[f].\n1 The bias-variance decomposition (Geman et al., 1992) is given by:\nE\nDLS\nE\nY |X(Y −f(X | DLS))2 = σ2(x) + bias2(x),\n(2.3)\nwhere\nbias2(x) ≜\u0000EY |x(Y ) −EDLSf(x | DLS)\u00012 ,\nσ2(x) ≜EY |x\n\u0000Y −EY |x(Y )\u00012\n|\n{z\n}\nInternal variance\n+ EDLS\n\u0010\nf(x | DLS) −EDLSf(x | DLS)\n\u00112\n|\n{z\n}\nParametric variance\n,\n(2.4)\n2For any given model, the parametric variance goes to zero with an arbitrary\nlarge dataset by considering the strong law of convergence.\n2.2. Unsupervised learning\n9\nx\ny\nPolynomial of degree 1\n for the model\nModel\nTrue function\nTraining samples\nx\ny\nPolynomial of degree 4\n for the model\nModel\nTrue function\nTraining samples\nx\ny\nPolynomial of degree 10\n for the model\nModel\nTrue function\nTraining samples\nFigure 2.1: Illustration of overﬁtting and underﬁtting for a simple 1D regression task\nin supervised learning (based on one example from the library scikit-learn (Pedregosa\net al., 2011)). In this illustration, the data points (x, y) are noisy samples from a\ntrue function represented in green. In the left ﬁgure, the degree 1 approximation is\nunderﬁtting, which means that it is not a good model, even for the training samples;\non the right, the degree 10 approximation is a very good model for the training\nsamples but is overly complex and fails to provide a good generalization.\nIn machine learning, the complexity of the function approximator\nprovides upper bounds on the generalization error. The generalization\nerror can be bounded by making use of complexity measures, such as\nthe Rademacher complexity (Bartlett and Mendelson, 2002), or the\nVC-dimension (Vapnik, 1998). However, even though it lacks strong\ntheoretical foundations, it has become clear in practice that the strength\nof deep neural networks is their generalization capability, even with\na high number of parameters (hence a potentially high complexity)\n(Zhang et al., 2016).\n2.2\nUnsupervised learning\nUnsupervised learning is a branch of machine learning that learns from\ndata that do not have any label. It relates to using and identifying\npatterns in the data for tasks such as data compression or generative\nmodels.\nData compression or dimensionality reduction involve encoding\ninformation using a smaller representation (e.g., fewer bits) than the\noriginal representation. For instance, an auto-encoder consists of an\nencoder and a decoder. The encoder maps the original image xi ∈RM\n10\nMachine learning and deep learning\nonto a low-dimensional representation zi = e(xi; θe) ∈Rm, where\nm << M; the decoder maps these features back to a high-dimensional\nrepresentation d(zi; θd) ≈e−1(zi; θe). Auto-encoders can be trained\nby optimizing for the reconstruction of the input through supervised\nlearning objectives.\nGenerative models aim at approximating the true data distribution\nof a training set so as to generate new data points from the distribution.\nGenerative adversarial networks (Goodfellow et al., 2014) use an\nadversarial process, in which two models are trained simulatenously: a\ngenerative model G captures the data distribution, while a discriminative\nmodel D estimates whether a sample comes from the training data rather\nthan G. The training procedure corresponds to a minimax two-player\ngame.\n2.3\nThe deep learning approach\nDeep learning relies on a function f : X →Y parameterized with\nθ ∈Rnθ (nθ ∈N):\ny = f(x; θ).\n(2.5)\nA deep neural network is characterized by a succession of multiple\nprocessing layers. Each layer consists in a non-linear transformation\nand the sequence of these transformations leads to learning diﬀerent\nlevels of abstraction (Erhan et al., 2009; Olah et al., 2017).\nFirst, let us describe a very simple neural network with one fully-\nconnected hidden layer (see Fig 2.2). The ﬁrst layer is given the input\nvalues (i.e., the input features) x in the form of a column vector of size\nnx (nx ∈N). The values of the next hidden layer are a transformation\nof these values by a non-linear parametric function, which is a matrix\nmultiplication by W1 of size nh × nx (nh ∈N), plus a bias term b1 of\nsize nh, followed by a non-linear transformation:\nh = A(W1 · x + b1),\n(2.6)\nwhere A is the activation function. This non-linear activation function is\nwhat makes the transformation at each layer non-linear, which ultimately\nprovides the expressivity of the neural network. The hidden layer h of\n2.3. The deep learning approach\n11\nsize nh can in turn be transformed to other sets of values up to the last\ntransformation that provides the output values y. In this case:\ny = (W2 · h + b2),\n(2.7)\nwhere W2 is of size ny × nh and b2 is of size ny (ny ∈N).\nHidden\nlayer\nh\nInputs\nx\nOutput\nlayer\ny\nFigure 2.2: Example of a neural network with one hidden layer.\nAll these layers are trained to minimize the empirical error IS[f].\nThe most common method for optimizing the parameters of a neural\nnetwork is based on gradient descent via the backpropagation algorithm\n(Rumelhart et al., 1988). In the simplest case, at every iteration, the\nalgorithm changes its internal parameters θ so as to ﬁt the desired\nfunction:\nθ ←θ −α∇θIS[f],\n(2.8)\nwhere α is the learning rate.\nIn current applications, many diﬀerent types of neural network\nlayers have appeared beyond the simple feedforward networks just\nintroduced. Each variation provides speciﬁc advantages, depending on\nthe application (e.g., good tradeoﬀbetween bias and overﬁtting in\na supervised learning setting). In addition, within one given neural\nnetwork, an arbitrarily large number of layers is possible, and the trend\n12\nMachine learning and deep learning\nin the last few years is to have an ever-growing number of layers, with\nmore than 100 in some supervised learning tasks (Szegedy et al., 2017).\nWe merely describe here two types of layers that are of particular interest\nin deep RL (and in many other tasks).\nConvolutional layers (LeCun, Bengio, et al., 1995) are particularly\nwell suited for images and sequential data (see Fig 2.3), mainly due\nto their translation invariance property. The layer’s parameters consist\nof a set of learnable ﬁlters (or kernels), which have a small receptive\nﬁeld and which apply a convolution operation to the input, passing\nthe result to the next layer. As a result, the network learns ﬁlters that\nactivate when it detects some speciﬁc features. In image classiﬁcation,\nthe ﬁrst layers learn how to detect edges, textures and patterns; then\nthe following layers are able to detect parts of objects and whole objects\n(Erhan et al., 2009; Olah et al., 2017). In fact, a convolutional layer is\na particular kind of feedforward layer, with the speciﬁcity that many\nweights are set to 0 (not learnable) and that other weights are shared.\ninput image\nor input feature map\nﬁlter\n0 1\n1 0\n2\n0\noutput feature maps\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\nFigure 2.3: Illustration of a convolutional layer with one input feature map that is\nconvolved by diﬀerent ﬁlters to yield the output feature maps. The parameters that\nare learned for this type of layer are those of the ﬁlters. For illustration purposes,\nsome results are displayed for one of the output feature maps with a given ﬁlter (in\npractice, that operation is followed by a non-linear activation function).\n2.3. The deep learning approach\n13\nyt\nh\nxt\nxt−1\nxt−2\nyt−1\nyt−2\n=\ny1\ny2\ny3\nh\nh\nh\nx1\nx2\nx3\n. . .\nyt−1\nh\nxt−1\nyt\nh\nxt\nFigure 2.4: Illustration of a simple recurrent neural network. The layer denoted\nby \"h\" may represent any non linear function that takes two inputs and provides\ntwo outputs. On the left is the simpliﬁed view of a recurrent neural network that\nis applied recursively to (xt, yt) for increasing values of t and where the blue line\npresents a delay of one time step. On the right, the neural network is unfolded with\nthe implicit requirement of presenting all inputs and outputs simultaneously.\nRecurrent layers are particularly well suited for sequential data (see\nFig 2.4). Several diﬀerent variants provide particular beneﬁts in diﬀerent\nsettings. One such example is the long short-term memory network\n(LSTM) (Hochreiter and Schmidhuber, 1997), which is able to encode\ninformation from long sequences, unlike a basic recurrent neural network.\nNeural Turing Machines (NTMs) (Graves et al., 2014) are another such\nexample. In such systems, a diﬀerentiable \"external memory\" is used\nfor inferring even longer-term dependencies than LSTMs with low\ndegradation.\nSeveral other speciﬁc neural network architectures have also been\nstudied to improve generalization in deep learning. For instance, it is\npossible to design an architecture in such a way that it automatically\nfocuses on only some parts of the inputs with a mechanism called\nattention (Xu et al., 2015; Vaswani et al., 2017). Other approaches aim\nto work with symbolic rules by learning to create programs (Reed and\nDe Freitas, 2015; Neelakantan et al., 2015; Johnson et al., 2017; Chen\net al., 2017).\nTo be able to actually apply the deep RL methods described in the\nlater chapters, the reader should have practical knowledge of applying\ndeep learning methods in simple supervised learning settings (e.g.,\nMNIST classiﬁcation). For information on topics such as the importance\n14\nMachine learning and deep learning\nof input normalizations, weight initialization techniques, regularization\ntechniques and the diﬀerent variants of gradient descent techniques,\nthe reader can refer to several reviews on the subject (LeCun et al.,\n2015; Schmidhuber, 2015; Goodfellow et al., 2016) as well as references\ntherein.\nIn the following chapters, the focus is on reinforcement learning,\nin particular on methods that scale to deep neural network function\napproximators. These methods allows for learning a wide variety of\nchallenging sequential decision-making tasks directly from rich high-\ndimensional inputs.\n3\nIntroduction to reinforcement learning\nReinforcement learning (RL) is the area of machine learning that deals\nwith sequential decision-making. In this chapter, we describe how the\nRL problem can be formalized as an agent that has to make decisions\nin an environment to optimize a given notion of cumulative rewards.\nIt will become clear that this formalization applies to a wide variety\nof tasks and captures many essential features of artiﬁcial intelligence\nsuch as a sense of cause and eﬀect as well as a sense of uncertainty and\nnondeterminism. This chapter also introduces the diﬀerent approaches\nto learning sequential decision-making tasks and how deep RL can be\nuseful.\nA key aspect of RL is that an agent learns a good behavior. This\nmeans that it modiﬁes or acquires new behaviors and skills incrementally.\nAnother important aspect of RL is that it uses trial-and-error experience\n(as opposed to e.g., dynamic programming that assumes full knowledge\nof the environment a priori). Thus, the RL agent does not require\ncomplete knowledge or control of the environment; it only needs to be\nable to interact with the environment and collect information. In an\noﬄine setting, the experience is acquired a priori, then it is used as a\nbatch for learning (hence the oﬄine setting is also called batch RL).\n15\n16\nIntroduction to reinforcement learning\nThis is in contrast to the online setting where data becomes available\nin a sequential order and is used to progressively update the behavior\nof the agent. In both cases, the core learning algorithms are essentially\nthe same but the main diﬀerence is that in an online setting, the agent\ncan inﬂuence how it gathers experience so that it is the most useful for\nlearning. This is an additional challenge mainly because the agent has to\ndeal with the exploration/exploitation dilemma while learning (see §8.1\nfor a detailed discussion). But learning in the online setting can also be\nan advantage since the agent is able to gather information speciﬁcally\non the most interesting part of the environment. For that reason, even\nwhen the environment is fully known, RL approaches may provide the\nmost computationally eﬃcient approach in practice as compared to\nsome dynamic programming methods that would be ineﬃcient due to\nthis lack of speciﬁcity.\n3.1\nFormal framework\nThe reinforcement learning setting\nThe general RL problem is formalized as a discrete time stochastic\ncontrol process where an agent interacts with its environment in the\nfollowing way: the agent starts, in a given state within its environment\ns0 ∈S, by gathering an initial observation ω0 ∈Ω. At each time step t,\nthe agent has to take an action at ∈A. As illustrated in Figure 3.1, it\nfollows three consequences: (i) the agent obtains a reward rt ∈R, (ii) the\nstate transitions to st+1 ∈S, and (iii) the agent obtains an observation\nωt+1 ∈Ω. This control setting was ﬁrst proposed by Bellman, 1957b\nand later extended to learning by Barto et al., 1983. Comprehensive\ntreatment of RL fundamentals are provided by Sutton and Barto, 2017.\nHere, we review the main elements of RL before delving into deep RL\nin the following chapters.\nThe Markov property\nFor the sake of simplicity, let us consider ﬁrst the case of Markovian\nstochastic control processes (Norris, 1998).\n3.1. Formal framework\n17\nAgent\nEnvironment\nst →st+1\nat\nωt+1\nrt\nFigure 3.1: Agent-environment interaction in RL.\nDeﬁnition 3.1. A discrete time stochastic control process is Markovian\n(i.e., it has the Markov property) if\n• P(ωt+1 | ωt, at) = P(ωt+1 | ωt, at, . . . , , ω0, a0), and\n• P(rt | ωt, at) = P(rt | ωt, at, . . . , , ω0, a0).\nThe Markov property means that the future of the process only\ndepends on the current observation, and the agent has no interest in\nlooking at the full history.\nA Markov Decision Process (MDP) (Bellman, 1957a) is a discrete\ntime stochastic control process deﬁned as follows:\nDeﬁnition 3.2. An MDP is a 5-tuple (S, A, T, R, γ) where:\n• S is the state space,\n• A is the action space,\n• T : S ×A×S →[0, 1] is the transition function (set of conditional\ntransition probabilities between states),\n• R : S ×A×S →R is the reward function, where R is a continuous\nset of possible rewards in a range Rmax ∈R+ (e.g., [0, Rmax]),\n• γ ∈[0, 1) is the discount factor.\nThe system is fully observable in an MDP, which means that the\nobservation is the same as the state of the environment: ωt = st. At\neach time step t, the probability of moving to st+1 is given by the state\n18\nIntroduction to reinforcement learning\ntransition function T(st, at, st+1) and the reward is given by a bounded\nreward function R(st, at, st+1) ∈R. This is illustrated in Figure 3.2.\nNote that more general cases than MDPs are introduced in Chapter 10.\ns0\ns1\ns2\na0\na1\nr0\nr1\n. . .\nPolicy\nReward\nfunction\nR(s0, a0, s1)\nTransition\nfunction\nT (s0, a0, s1)\nPolicy\nReward\nfunction\nR(s1, a1, s2)\nTransition\nfunction\nT (s1, a1, s2)\nFigure 3.2: Illustration of a MDP. At each step, the agent takes an action that\nchanges its state in the environment and provides a reward.\nDiﬀerent categories of policies\nA policy deﬁnes how an agent selects actions. Policies can be categorized\nunder the criterion of being either stationary or non-stationary. A non-\nstationary policy depends on the time-step and is useful for the ﬁnite-\nhorizon context where the cumulative rewards that the agent seeks to\noptimize are limited to a ﬁnite number of future time steps (Bertsekas\net al., 1995). In this introduction to deep RL, inﬁnite horizons are\nconsidered and the policies are stationary1.\nPolicies can also be categorized under a second criterion of being\neither deterministic or stochastic:\n• In the deterministic case, the policy is described by π(s) : S →A.\n• In the stochastic case, the policy is described by π(s, a) : S ×A →\n[0, 1] where π(s, a) denotes the probability that action a may be\nchosen in state s.\n1The formalism can be directly extended to the ﬁnite horizon context. In that\ncase, the policy and the cumulative expected returns should be time-dependent.\n3.1. Formal framework\n19\nThe expected return\nThroughout this survey, we consider the case of an RL agent whose\ngoal is to ﬁnd a policy π(s, a) ∈Π, so as to optimize an expected return\nV π(s) : S →R (also called V-value function) such that\nV π(s) = E\nhX∞\nk=0 γkrt+k | st = s, π\ni\n,\n(3.1)\nwhere:\n• rt =\nE\na∼π(st,·)R\n\u0000st, a, st+1\n\u0001,\n• P\n\u0000st+1|st, at\n\u0001 = T(st, at, st+1) with at ∼π(st, ·),\nFrom the deﬁnition of the expected return, the optimal expected return\ncan be deﬁned as:\nV ∗(s) = max\nπ∈Π V π(s).\n(3.2)\nIn addition to the V-value function, a few other functions of interest\ncan be introduced. The Q-value function Qπ(s, a) : S ×A →R is deﬁned\nas follows:\nQπ(s, a) = E\nhX∞\nk=0 γkrt+k | st = s, at = a, π\ni\n.\n(3.3)\nThis equation can be rewritten recursively in the case of an MDP using\nBellman’s equation:\nQπ(s, a) =\nX\ns′∈S\nT(s, a, s′)\n\u0000R(s, a, s′) + γQπ(s′, a = π(s′))\n\u0001 .\n(3.4)\nSimilarly to the V-value function, the optimal Q-value function Q∗(s, a)\ncan also be deﬁned as\nQ∗(s, a) = max\nπ∈Π Qπ(s, a).\n(3.5)\nThe particularity of the Q-value function as compared to the V-value\nfunction is that the optimal policy can be obtained directly from Q∗(s, a):\nπ∗(s) = argmax\na∈A\nQ∗(s, a).\n(3.6)\nThe optimal V-value function V ∗(s) is the expected discounted reward\nwhen in a given state s while following the policy π∗thereafter. The\n20\nIntroduction to reinforcement learning\noptimal Q-value Q∗(s, a) is the expected discounted return when in\na given state s and for a given action a while following the policy π∗\nthereafter.\nIt is also possible to deﬁne the advantage function\nAπ(s, a) = Qπ(s, a) −V π(s).\n(3.7)\nThis quantity describes how good the action a is, as compared to the\nexpected return when following directly policy π.\nNote that one straightforward way to obtain estimates of either\nV π(s), Qπ(s, a) or Aπ(s, a) is to use Monte Carlo methods, i.e. deﬁning\nan estimate by performing several simulations from s while following\npolicy π. In practice, we will see that this may not be possible in the case\nof limited data. In addition, even when it is possible, we will see that\nother methods should usually be preferred for computational eﬃciency.\n3.2\nDiﬀerent components to learn a policy\nAn RL agent includes one or more of the following components:\n• a representation of a value function that provides a prediction of\nhow good each state or each state/action pair is,\n• a direct representation of the policy π(s) or π(s, a), or\n• a model of the environment (the estimated transition function and\nthe estimated reward function) in conjunction with a planning\nalgorithm.\nThe ﬁrst two components are related to what is called model-free RL\nand are discussed in Chapters 4, 5. When the latter component is used,\nthe algorithm is referred to as model-based RL, which is discussed in\nChapter 6. A combination of both and why using the combination can\nbe useful is discussed in §6.2. A schema with all possible approaches is\nprovided in Figure 3.3.\nFor most problems approaching real-world complexity, the state\nspace is high-dimensional (and possibly continuous). In order to learn\nan estimate of the model, the value function or the policy, there are\ntwo main advantages for RL algorithms to rely on deep learning:\n3.3. Diﬀerent settings to learn a policy from data\n21\nExperience\nValue/policy\nModel\nActing\nModel\nlearning\nPlanning\nModel-free\nRL\nFigure 3.3: General schema of the diﬀerent methods for RL. The direct approach\nuses a representation of either a value function or a policy to act in the environment.\nThe indirect approach makes use of a model of the environment.\n• Neural networks are well suited for dealing with high-dimensional\nsensory inputs (such as times series, frames, etc.) and, in practice,\nthey do not require an exponential increase of data when adding\nextra dimensions to the state or action space (see Chapter 2).\n• In addition, they can be trained incrementally and make use of\nadditional samples obtained as learning happens.\n3.3\nDiﬀerent settings to learn a policy from data\nWe now describe key settings that can be tackled with RL.\n3.3.1\nOﬄine and online learning\nLearning a sequential decision-making task appears in two cases: (i) in\nthe oﬄine learning case where only limited data on a given environment\nis available and (ii) in an online learning case where, in parallel to\nlearning, the agent gradually gathers experience in the environment. In\nboth cases, the core learning algorithms introduced in Chapters 4 to 6\nare essentially the same. The speciﬁcity of the batch setting is that the\nagent has to learn from limited data without the possibility of interacting\nfurther with the environment. In that case, the idea of generalization\nintroduced in Chapter 7 is the main focus. In the online setting, the\nlearning problem is more intricate and learning without requiring a\nlarge amount of data (sample eﬃciency) is not only inﬂuenced by the\n22\nIntroduction to reinforcement learning\ncapability of the learning algorithm to generalize well from the limited\nexperience. Indeed, the agent has the possibility to gather experience\nvia an exploration/exploitation strategy. In addition, it can use a replay\nmemory to store its experience so that it can be reprocessed at a later\ntime. Both the exploration and the replay memory will be discussed in\nChapter 8). In both the batch and the online settings, a supplementary\nconsideration is also the computational eﬃciency, which, among other\nthings, depends on the eﬃciency of a given gradient descent step. All\nthese elements will be introduced with more details in the following\nchapters. A general schema of the diﬀerent elements that can be found\nin most deep RL algorithms is provided in Figure 3.4.\nPolicies\nExploration/Exploitation\ndilemma\nControllers\n• train/validation\nand test phases\n• hyperparameters\nmanagement\nReplay memory\nLearning\nalgorithms\nValue-based RL\nPolicy-based RL\nModel-based RL\nFunction\nApproximators\n• Convolutions\n• Recurrent cells\n• ...\nENVIRONMENT\nAGENT\nFigure 3.4: General schema of deep RL methods.\n3.3.2\nOﬀ-policy and on-policy learning\nAccording to Sutton and Barto, 2017, « on-policy methods attempt to\nevaluate or improve the policy that is used to make decisions, whereas\n3.3. Diﬀerent settings to learn a policy from data\n23\noﬀ-policy methods evaluate or improve a policy diﬀerent from that\nused to generate the data ». In oﬀ-policy based methods, learning is\nstraightforward when using trajectories that are not necessarily obtained\nunder the current policy, but from a diﬀerent behavior policy β(s, a).\nIn those cases, experience replay allows reusing samples from a diﬀerent\nbehavior policy. On the contrary, on-policy based methods usually\nintroduce a bias when used with a replay buﬀer as the trajectories\nare usually not obtained solely under the current policy π. As will be\ndiscussed in the following chapters, this makes oﬀ-policy methods sample\neﬃcient as they are able to make use of any experience; in contrast,\non-policy methods would, if speciﬁc care is not taken, introduce a bias\nwhen using oﬀ-policy trajectories.\n4\nValue-based methods for deep RL\nThe value-based class of algorithms aims to build a value function,\nwhich subsequently lets us deﬁne a policy. We discuss hereafter one\nof the simplest and most popular value-based algorithms, the Q-\nlearning algorithm (Watkins, 1989) and its variant, the ﬁtted Q-learning,\nthat uses parameterized function approximators (Gordon, 1996). We\nalso speciﬁcally discuss the main elements of the deep Q-network\n(DQN) algorithm (Mnih et al., 2015) which has achieved superhuman-\nlevel control when playing ATARI games from the pixels by using\nneural networks as function approximators. We then review various\nimprovements of the DQN algorithm and provide resources for further\ndetails. At the end of this chapter and in the next chapter, we discuss the\nintimate link between value-based methods and policy-based methods.\n4.1\nQ-learning\nThe basic version of Q-learning keeps a lookup table of values Q(s, a)\n(Equation 3.3) with one entry for every state-action pair. In order to\nlearn the optimal Q-value function, the Q-learning algorithm makes use\n24\n4.2. Fitted Q-learning\n25\nof the Bellman equation for the Q-value function (Bellman and Dreyfus,\n1962) whose unique solution is Q∗(s, a):\nQ∗(s, a) = (BQ∗)(s, a),\n(4.1)\nwhere B is the Bellman operator mapping any function K : S × A →R\ninto another function S × A →R and is deﬁned as follows:\n(BK)(s, a) =\nX\ns′∈S\nT(s, a, s′)\n\u0012\nR(s, a, s′) + γ max\na′∈A K(s′, a′)\n\u0013\n.\n(4.2)\nBy Banach’s theorem, the ﬁxed point of the Bellman operator B exists\nsince it is a contraction mapping1. In practice, one general proof of\nconvergence to the optimal value function is available (Watkins and\nDayan, 1992) under the conditions that:\n• the state-action pairs are represented discretely, and\n• all actions are repeatedly sampled in all states (which ensures\nsuﬃcient exploration, hence not requiring access to the transition\nmodel).\nThis simple setting is often inapplicable due to the high-dimensional\n(possibly continuous) state-action space. In that context, a param-\neterized value function Q(s, a; θ) is needed, where θ refers to some\nparameters that deﬁne the Q-values.\n4.2\nFitted Q-learning\nExperiences are gathered in a given dataset D in the form of tuples\n< s, a, r, s′ > where the state at the next time-step s′ is drawn from\nT(s, a, ·) and the reward r is given by R(s, a, s′). In ﬁtted Q-learning\n(Gordon, 1996), the algorithm starts with some random initialization of\nthe Q-values Q(s, a; θ0) where θ0 refers to the initial parameters (usually\n1The Bellman operator is a contraction mapping because it can be shown that\nfor any pair of bounded functions K, K′ : S × A →R, the following condition is\nrespected:\n∥TK −TK′∥∞≤γ∥K −K′∥∞.\n26\nValue-based methods for deep RL\nsuch that the initial Q-values should be relatively close to 0 so as to\navoid slow learning). Then, an approximation of the Q-values at the\nkth iteration Q(s, a; θk) is updated towards the target value\nY Q\nk = r + γ max\na′∈A Q(s′, a′; θk),\n(4.3)\nwhere θk refers to some parameters that deﬁne the Q-values at the kth\niteration.\nIn neural ﬁtted Q-learning (NFQ) (Riedmiller, 2005), the state can\nbe provided as an input to the Q-network and a diﬀerent output is given\nfor each of the possible actions. This provides an eﬃcient structure that\nhas the advantage of obtaining the computation of maxa′∈A Q(s′, a′; θk)\nin a single forward pass in the neural network for a given s′. The Q-\nvalues are parameterized with a neural network Q(s, a; θk) where the\nparameters θk are updated by stochastic gradient descent (or a variant)\nby minimizing the square loss:\nLDQN =\n\u0010\nQ(s, a; θk) −Y Q\nk\n\u00112 .\n(4.4)\nThus, the Q-learning update amounts in updating the parameters:\nθk+1 = θk + α\n\u0010\nY Q\nk −Q(s, a; θk)\n\u0011\n∇θkQ(s, a; θk),\n(4.5)\nwhere α is a scalar step size called the learning rate. Note that using the\nsquare loss is not arbitrary. Indeed, it ensures that Q(s, a; θk) should\ntend without bias to the expected value of the random variable Y Q\nk\n2.\nHence, it ensures that Q(s, a; θk) should tend to Q∗(s, a) after many\niterations in the hypothesis that the neural network is well-suited for\nthe task and that the experience gathered in the dataset D is suﬃcient\n(more details will be given in Chapter 7).\nWhen updating the weights, one also changes the target. Due to\nthe generalization and extrapolation abilities of neural networks, this\napproach can build large errors at diﬀerent places in the state-action\nspace3. Therefore, the contraction mapping property of the Bellman\n2The minimum of E[(Z −c)2] occurs when the constant c equals the expected\nvalue of the random variable Z.\n3Note that even ﬁtted value iteration with linear regression can diverge (Boyan\nand Moore, 1995). However, this drawback does not happen when using linear\n4.3. Deep Q-networks\n27\noperator in Equation 4.2 is not enough to guarantee convergence. It\nis veriﬁed experimentally that these errors may propagate with this\nupdate rule and, as a consequence, convergence may be slow or even\nunstable (Baird, 1995; Tsitsiklis and Van Roy, 1997; Gordon, 1999;\nRiedmiller, 2005). Another related damaging side-eﬀect of using function\napproximators is the fact that Q-values tend to be overestimated due to\nthe max operator (Van Hasselt et al., 2016). Because of the instabilities\nand the risk of overestimation, speciﬁc care has be taken to ensure\nproper learning.\n4.3\nDeep Q-networks\nLeveraging ideas from NFQ, the deep Q-network (DQN) algorithm\nintroduced by Mnih et al. (2015) is able to obtain strong performance\nin an online setting for a variety of ATARI games, directly by learning\nfrom the pixels. It uses two heuristics to limit the instabilities:\n• The target Q-network in Equation 4.3 is replaced by Q(s′, a′; θ−\nk )\nwhere its parameters θ−\nk are updated only every C ∈N iterations\nwith the following assignment: θ−\nk\n= θk. This prevents the\ninstabilities to propagate quickly and it reduces the risk of\ndivergence as the target values Y Q\nk\nare kept ﬁxed for C iterations.\nThe idea of target networks can be seen as an instantiation of ﬁtted\nQ-learning, where each period between target network updates\ncorresponds to a single ﬁtted Q-iteration.\n• In an online setting, the replay memory (Lin, 1992) keeps all\ninformation for the last Nreplay ∈N time steps, where the\nexperience is collected by following an ϵ-greedy policy4. The\nupdates are then made on a set of tuples < s, a, r, s′ > (called\nmini-batch) selected randomly within the replay memory. This\nfunction approximators that only have interpolation abilities such as kernel-based\nregressors (k-nearest neighbors, linear and multilinear interpolation, etc.) (Gordon,\n1999) or tree-based ensemble methods (Ernst et al., 2005). However, these methods\nhave not proved able to handle successfully high-dimensional inputs.\n4It takes a random action with probability ϵ and follows the policy given by\nargmaxa∈A Q(s, a; θk) with probability 1 −ϵ.\n28\nValue-based methods for deep RL\ntechnique allows for updates that cover a wide range of the state-\naction space. In addition, one mini-batch update has less variance\ncompared to a single tuple update. Consequently, it provides the\npossibility to make a larger update of the parameters, while having\nan eﬃcient parallelization of the algorithm.\nA sketch of the algorithm is given in Figure 4.1.\nIn addition to the target Q-network and the replay memory, DQN\nuses other important heuristics. To keep the target values in a reasonable\nscale and to ensure proper learning in practice, rewards are clipped\nbetween -1 and +1. Clipping the rewards limits the scale of the error\nderivatives and makes it easier to use the same learning rate across\nmultiple games (however, it introduces a bias). In games where the\nplayer has multiple lives, one trick is also to associate a terminal state\nto the loss of a life such that the agent avoids these terminal states (in\na terminal state the discount factor is set to 0).\nIn DQN, many deep learning speciﬁc techniques are also used. In\nparticular, a preprocessing step of the inputs is used to reduce the\ninput dimensionality, to normalize inputs (it scales pixels value into\n[-1,1]) and to deal with some speciﬁcities of the task. In addition,\nconvolutional layers are used for the ﬁrst layers of the neural network\nfunction approximator and the optimization is performed using a variant\nof stochastic gradient descent called RMSprop (Tieleman, 2012).\n4.4\nDouble DQN\nThe max operation in Q-learning (Equations 4.2, 4.3) uses the same\nvalues both to select and to evaluate an action. This makes it more likely\nto select overestimated values in case of inaccuracies or noise, resulting\nin overoptimistic value estimates. Therefore, the DQN algorithm induces\nan upward bias. The double estimator method uses two estimates for\neach variable, which allows for the selection of an estimator and its value\nto be uncoupled (Hasselt, 2010). Thus, regardless of whether errors\nin the estimated Q-values are due to stochasticity in the environment,\nfunction approximation, non-stationarity, or any other source, this\nallows for the removal of the positive bias in estimating the action\n4.5. Dueling network architecture\n29\nUpdate\nQ(s, a; θk)\nEvery C:\nθ−\nk := θk\nr1, . . . , rNreplay\ns1, . . . , sNreplay, a1, . . . , aNreplay\ns1+1, . . . , sNreplay+1\nrt + γmax\na′∈A(Q(st+1, a′; θ−\nk ))\nPolicy\nEnvironment\nFigure 4.1: Sketch of the DQN algorithm. Q(s, a; θk) is initialized to random values\n(close to 0) everywhere in its domain and the replay memory is initially empty;\nthe target Q-network parameters θ−\nk are only updated every C iterations with the\nQ-network parameters θk and are held ﬁxed between updates; the update uses a\nmini-batch (e.g., 32 elements) of tuples < s, a > taken randomly in the replay\nmemory along with the corresponding mini-batch of target values for the tuples.\nvalues. In Double DQN, or DDQN (Van Hasselt et al., 2016), the target\nvalue Y Q\nk\nis replaced by\nY DDQN\nk\n= r + γQ(s′, argmax\na∈A\nQ(s′, a; θk); θ−\nk ),\n(4.6)\nwhich leads to less overestimation of the Q-learning values, as well as\nimproved stability, hence improved performance. As compared to DQN,\nthe target network with weights θ−\nt are used for the evaluation of the\ncurrent greedy action. Note that the policy is still chosen according to\nthe values obtained by the current weights θ.\n4.5\nDueling network architecture\nIn (Wang et al., 2015), the neural network architecture decouples the\nvalue and advantage function Aπ(s, a) (Equation 3.7), which leads to\nimproved performance. The Q-value function is given by\nQ(s, a; θ(1), θ(2), θ(3)) = V\n\u0010\ns; θ(1), θ(3)\u0011\n+\n\u0012\nA\n\u0010\ns, a; θ(1), θ(2)\u0011\n−max\na′∈A A\n\u0010\ns, a′; θ(1), θ(2)\u0011\u0013\n.\n(4.7)\nNow, for a∗\n=\nargmaxa′∈A Q(s, a′; θ(1), θ(2), θ(3)), we obtain\nQ(s, a∗; θ(1), θ(2), θ(3)) = V (s; θ(1), θ(3)). As illustrated in Figure 4.2,\n30\nValue-based methods for deep RL\nthe stream V (s; θ(1), θ(3)) provides an estimate of the value function,\nwhile the other stream produces an estimate of the advantage function.\nThe learning update is done as in DQN and it is only the structure of\nthe neural network that is modiﬁed.\n. . .\nQ(s, a)\nA(s, a)\nV (s)\nθ(3)\nθ(2)\nθ(1)\nFigure 4.2: Illustration of the dueling network architecture with the two streams\nthat separately estimate the value V (s) and the advantages A(s, a). The boxes\nrepresent layers of a neural network and the grey output implements equation 4.7 to\ncombine V (s) and A(s, a).\nIn fact, even though it loses the original semantics of V and A, a\nslightly diﬀerent approach is preferred in practice because it increases\nthe stability of the optimization:\nQ(s, a; θ(1), θ(2), θ(3)) = V\n\u0010\ns; θ(1), θ(3)\u0011\n+\n\nA\n\u0010\ns, a; θ(1), θ(2)\u0011\n−1\n|A|\nX\na′∈A\nA\n\u0010\ns, a′; θ(1), θ(2)\u0011\n\n.\n(4.8)\nIn that case, the advantages only need to change as fast as the mean,\nwhich appears to work better in practice (Wang et al., 2015).\n4.6. Distributional DQN\n31\n4.6\nDistributional DQN\nThe approaches described so far in this chapter all directly approximate\nthe expected return in a value function. Another approach is to aim for\na richer representation through a value distribution, i.e. the distribution\nof possible cumulative returns (Jaquette et al., 1973; Morimura et al.,\n2010). This value distribution provides more complete information of\nthe intrinsic randomness of the rewards and transitions of the agent\nwithin its environment (note that it is not a measure of the agent’s\nuncertainty about the environment).\nThe value distribution Zπ is a mapping from state-action pairs to\ndistributions of returns when following policy π. It has an expectation\nequal to Qπ:\nQπ(s, a) = EZπ(s, a).\nThis random return is also described by a recursive equation, but one\nof a distributional nature:\nZπ(s, a) = R(s, a, S′) + γZπ(S′, A′),\n(4.9)\nwhere we use capital letters to emphasize the random nature of the\nnext state-action pair (S′, A′) and A′ ∼π(·|S′). The distributional\nBellman equation states that the distribution of Z is characterized by\nthe interaction of three random variables: the reward R(s, a, S′), the\nnext state-action (S′, A′), and its random return Zπ(S′, A′).\nIt has been shown that such a distributional Bellman equation can\nbe used in practice, with deep learning as the function approximator\n(Bellemare et al., 2017; Dabney et al., 2017; Rowland et al., 2018). This\napproach has the following advantages:\n• It is possible to implement risk-aware behavior (see e.g., Morimura\net al., 2010).\n• It leads to more performant learning in practice. This may appear\nsurprising since both DQN and the distributional DQN aim to\nmaximize the expected return (as illustrated in Figure 4.3). One of\nthe main elements is that the distributional perspective naturally\nprovides a richer set of training signals than a scalar value function\n32\nValue-based methods for deep RL\nQ(s, a). These training signals that are not a priori necessary\nfor optimizing the expected return are known as auxiliary tasks\n(Jaderberg et al., 2016) and lead to an improved learning (this is\ndiscussed in §7.2.1).\n(s, a)\nπ1\nπ2\ns(1)\ns(2)\ns(3)\nP = 1\nP = 0.2\nP = 0.8\nRmax\n5\n0\nRmax\n(a) Example MDP.\nˆZπ1\nˆZπ2\n0\nRmax\n1−γ\nˆQπ1≈ˆQπ2\n(b) Sketch (in an idealized version) of\nthe estimate of resulting value distribu-\ntion ˆZπ1 and ˆZπ2 as well as the esti-\nmate of the Q-values ˆQπ1, ˆQπ2.\nFigure 4.3: For two policies illustrated on Fig (a), the illustration on Fig (b) gives\nthe value distribution Z(π)(s, a) as compared to the expected value Qπ(s, a). On\nthe left ﬁgure, one can see that π1 moves with certainty to an absorbing state with\nreward at every step Rmax\n5\n, while π2 moves with probability 0.2 and 0.8 to absorbing\nstates with respectively rewards at every step Rmax and 0. From the pair (s, a), the\npolicies π1 and π2 have the same expected return but diﬀerent value distributions.\n4.7\nMulti-step learning\nIn DQN, the target value used to update the Q-network parameters\n(given in Equation 4.3) is estimated as the sum of the immediate reward\nand a contribution of the following steps in the return. That contribution\nis estimated based on its own value estimate at the next time-step. For\nthat reason, the learning algorithm is said to bootstrap as it recursively\nuses its own value estimates (Sutton, 1988).\nThis method of estimating a target value is not the only possibility.\nNon-bootstrapping methods learn directly from returns (Monte Carlo)\n4.7. Multi-step learning\n33\nand an intermediate solution is to use a multi-step target (Sutton, 1988;\nWatkins, 1989; Peng and Williams, 1994; Singh and Sutton, 1996). Such\na variant in the case of DQN can be obtained by using the n-step target\nvalue given by:\nY Q,n\nk\n=\nn−1\nX\nt=0\nγtrt + γn max\na′∈A Q(sn, a′; θk)\n(4.10)\nwhere (s0, a0, r0, · · · , sn−1, an−1, rn−1, sn) is any trajectory of n+1 time\nsteps with s = s0 and a = a0. A combination of diﬀerent multi-steps\ntargets can also be used:\nY Q,n\nk\n=\nn−1\nX\ni=0\nλi\n i\nX\nt=0\nγtrt + γi+1 max\na′∈A Q\n\u0000si+1, a′; θk\n\u0001\n!\n(4.11)\nwith Pn−1\ni=0 λi = 1. In the method called TD(λ) (Sutton, 1988), n →∞\nand λi follow a geometric law: λi ∝λi where 0 ≤λ ≤1.\nTo bootstrap or not to bootstrap?\nBootstrapping has both advan-\ntages and disadvantages. On the negative side, using pure bootstrapping\nmethods (such as in DQN) are prone to instabilities when combined\nwith function approximation because they make recursive use of their\nown value estimate at the next time-step. On the contrary, methods\nsuch as n-step Q-learning rely less on their own value estimate because\nthe estimate used is decayed by γn for the nth step backup. In addition,\nmethods that rely less on bootstrapping can propagate information\nmore quickly from delayed rewards as they learn directly from returns\n(Sutton, 1996). Hence they might be more computationally eﬃcient.\nBootstrapping also has advantages. The main advantage is that\nusing value bootstrap allows learning from oﬀ-policy samples. Indeed,\nmethods that do not use pure bootstrapping, such as n-step Q-learning\nwith n > 1 or TD(λ), are in principle on-policy based methods that\nwould introduce a bias when used with trajectories that are not obtained\nsolely under the behavior policy µ (e.g., stored in a replay buﬀer).\nThe conditions required to learn eﬃciently and safely with eligibility\ntraces from oﬀ-policy experience are provided by Munos et al., 2016;\nHarutyunyan et al., 2016. In the control setting, the retrace operator\n(Munos et al., 2016) considers a sequence of target policies π that depend\n34\nValue-based methods for deep RL\non the sequence of Q-functions (such as ϵ-greedy policies), and seek to\napproximate Q∗(if π is greedy or becomes increasingly greedy w.r.t.\nthe Q estimates). It leads to the following target:\nY = Q(s, a) +\n\nX\nt≥0\nγt\n \ntY\ncs=1\ncs\n!\n(rt + γEπQ(st+1, a′) −Q(st, at))\n\n\n(4.12)\nwhere cs = λ min\n\u0010\n1, π(s,a)\nµ(s,a)\n\u0011\nwith 0 ≤λ ≤1 and µ is the behavior\npolicy (estimated from observed samples). This way of updating the\nQ-network has guaranteed convergence, does not suﬀer from a high\nvariance and it does not cut the traces unnecessarily when π and µ\nare close. Nonetheless, one can note that estimating the target is more\nexpansive to compute as compared to the one-step target (such as in\nDQN) because the Q-value function has to be estimated on more states.\n4.8\nCombination of all DQN improvements and variants of DQN\nThe original DQN algorithm can combine the diﬀerent variants discussed\nin §4.4 to §4.7 (as well as some discussed in Chapter 8.1) and that\nhas been studied by Hessel et al., 2017. Their experiments show that\nthe combination of all the previously mentioned extensions to DQN\nprovides state-of-the-art performance on the Atari 2600 benchmarks,\nboth in terms of sample eﬃciency and ﬁnal performance. Overall, a\nlarge majority of Atari games can be solved such that the deep RL\nagents surpass the human level performance.\nSome limitations remain with DQN-based approaches. Among others,\nthese types of algorithms are not well-suited to deal with large and/or\ncontinuous action spaces. In addition, they cannot explicitly learn\nstochastic policies. Modiﬁcations that address these limitations will be\ndiscussed in the following Chapter 5, where we discuss policy-based\napproaches. Actually, the next section will also show that value-based\nand policy-based approaches can be seen as two facets of the same\nmodel-free approach. Therefore, the limitations of discrete action spaces\nand deterministic policies are only related to DQN.\nOne can also note that value-based or policy-based approaches do\nnot make use of any model of the environment, which limits their sample\n4.8. Combination of all DQN improvements and variants of DQN\n35\neﬃciency. Ways to combine model-free and model-based approaches\nwill be discussed in Chapter 6.\n5\nPolicy gradient methods for deep RL\nThis section focuses on a particular family of reinforcement learning\nalgorithms that use policy gradient methods. These methods optimize\na performance objective (typically the expected cumulative reward)\nby ﬁnding a good policy (e.g a neural network parameterized policy)\nthanks to variants of stochastic gradient ascent with respect to the policy\nparameters. Note that policy gradient methods belong to a broader\nclass of policy-based methods that includes, among others, evolution\nstrategies. These methods use a learning signal derived from sampling\ninstantiations of policy parameters and the set of policies is developed\ntowards policies that achieve better returns (e.g., Salimans et al., 2017).\nIn this chapter, we introduce the stochastic and deterministic\ngradient theorems that provide gradients on the policy parameters in\norder to optimize the performance objective. Then, we present diﬀerent\nRL algorithms that make use of these theorems.\n36\n5.1. Stochastic Policy Gradient\n37\n5.1\nStochastic Policy Gradient\nThe expected return of a stochastic policy π starting from a given state\ns0 from Equation 3.1 can be written as (Sutton et al., 2000):\nV π(s0) =\nZ\nS\nρπ(s)\nZ\nA\nπ(s, a)R′(s, a)dads,\n(5.1)\nwhere R′(s, a) =\nR\ns′∈S T(s, a, s′)R(s, a, s′) and ρπ(s) is the discounted\nstate distribution deﬁned as\nρπ(s) =\n∞\nX\nt=0\nγtPr{st = s|s0, π}.\nFor a diﬀerentiable policy πw, the fundamental result underlying\nthese algorithms is the policy gradient theorem (Sutton et al., 2000):\n∇wV πw(s0) =\nZ\nS\nρπw(s)\nZ\nA\n∇wπw(s, a)Qπw(s, a)dads.\n(5.2)\nThis result allows us to adapt the policy parameters w: ∆w ∝\n∇wV πw(s0) from experience. This result is particularly interesting\nsince the policy gradient does not depend on the gradient of the state\ndistribution (even though one might have expected it to). The simplest\nway to derive the policy gradient estimator (i.e., estimating ∇wV πw(s0)\nfrom experience) is to use a score function gradient estimator, commonly\nknown as the REINFORCE algorithm (Williams, 1992). The likelihood\nratio trick can be exploited as follows to derive a general method of\nestimating gradients from expectations:\n∇wπw(s, a) = πw(s, a)∇wπw(s, a)\nπw(s, a)\n= πw(s, a)∇w log(πw(s, a)).\n(5.3)\nConsidering Equation 5.3, it follows that\n∇wV πw(s0) = Es∼ρπw,a∼πw [∇w (log πw(s, a)) Qπw (s, a)] .\n(5.4)\n38\nPolicy gradient methods for deep RL\nNote that, in practice, most policy gradient methods eﬀectively use\nundiscounted state distributions, without hurting their performance\n(Thomas, 2014).\nSo far, we have shown that policy gradient methods should include\na policy evaluation followed by a policy improvement. On the one\nhand, the policy evaluation estimates Qπw. On the other hand, the\npolicy improvement takes a gradient step to optimize the policy πw(s, a)\nwith respect to the value function estimation. Intuitively, the policy\nimprovement step increases the probability of the actions proportionally\nto their expected return.\nThe question that remains is how the agent can perform the policy\nevaluation step, i.e., how to obtain an estimate of Qπw(s, a). The\nsimplest approach to estimating gradients is to replace the Q function\nestimator with a cumulative return from entire trajectories. In the Monte-\nCarlo policy gradient, we estimate the Qπw(s, a) from rollouts on the\nenvironment while following policy πw. The Monte-Carlo estimator is an\nunbiased well-behaved estimate when used in conjunction with the back-\npropagation of a neural network policy, as it estimates returns until the\nend of the trajectories (without instabilities induced by bootstrapping).\nHowever, the main drawback is that the estimate requires on-policy\nrollouts and can exhibit high variance. Several rollouts are typically\nneeded to obtain a good estimate of the return. A more eﬃcient approach\nis to instead use an estimate of the return given by a value-based\napproach, as in actor-critic methods discussed in §5.3.\nWe make two additional remarks. First, to prevent the policy from\nbecoming deterministic, it is common to add an entropy regularizer\nto the gradient. With this regularizer, the learnt policy can remain\nstochastic. This ensures that the policy keeps exploring.\nSecond, instead of using the value function Qπw in Eq. 5.4, an\nadvantage value function Aπw can also be used. While Qπw(s, a)\nsummarizes the performance of each action for a given state under policy\nπw, the advantage function Aπw(s, a) provides a measure of comparison\nfor each action to the expected return at the state s, given by V πw(s).\nUsing Aπw(s, a) = Qπw(s, a) −V πw(s) has usually lower magnitudes\nthan Qπw(s, a). This helps reduce the variance of the gradient estimator\n∇wV πw(s0) in the policy improvement step, while not modifying the\n5.2. Deterministic Policy Gradient\n39\nexpectation1. In other words, the value function V πw(s) can be seen as\na baseline or control variate for the gradient estimator. When updating\nthe neural network that ﬁts the policy, using such a baseline allows for\nimproved numerical eﬃciency – i.e. reaching a given performance with\nfewer updates – because the learning rate can be bigger.\n5.2\nDeterministic Policy Gradient\nThe policy gradient methods may be extended to deterministic policies.\nThe Neural Fitted Q Iteration with Continuous Actions (NFQCA)\n(Hafner and Riedmiller, 2011) and the Deep Deterministic Policy\nGradient (DDPG) (Silver et al., 2014; Lillicrap et al., 2015) algorithms\nintroduce the direct representation of a policy in such a way that it can\nextend the NFQ and DQN algorithms to overcome the restriction of\ndiscrete actions.\nLet us denote by π(s) the deterministic policy: π(s) : S →A. In\ndiscrete action spaces, a direct approach is to build the policy iteratively\nwith:\nπk+1(s) = argmax\na∈A\nQπk(s, a),\n(5.5)\nwhere πk is the policy at the kth iteration. In continuous action\nspaces, a greedy policy improvement becomes problematic, requiring a\nglobal maximisation at every step. Instead, let us denote by πw(s)\na diﬀerentiable deterministic policy. In that case, a simple and\ncomputationally attractive alternative is to move the policy in the\ndirection of the gradient of Q, which leads to the Deep Deterministic\nPolicy Gradient (DDPG) algorithm (Lillicrap et al., 2015):\n∇wV πw(s0) = Es∼ρπw\nh\n∇w (πw) ∇a (Qπw(s, a)) |a=πw(s)\ni\n.\n(5.6)\nThis equation implies relying on ∇a (Qπw(s, a)) (in addition to ∇wπw),\nwhich usually requires using actor-critic methods (see §5.3).\n1Indeed, subtracting a baseline that only depends on s to Qπw(s, a) in Eq. 5.2\ndoes not change the gradient estimator because ∀s, R\nA ∇wπw(s, a)da = 0.\n40\nPolicy gradient methods for deep RL\n5.3\nActor-Critic Methods\nAs we have seen in §5.1 and §5.2, a policy represented by a neural\nnetwork can be updated by gradient ascent for both the deterministic\nand the stochastic case. In both cases, the policy gradient typically\nrequires an estimate of a value function for the current policy. One\ncommon approach is to use an actor-critic architecture that consists of\ntwo parts: an actor and a critic (Konda and Tsitsiklis, 2000). The actor\nrefers to the policy and the critic to the estimate of a value function (e.g.,\nthe Q-value function). In deep RL, both the actor and the critic can be\nrepresented by non-linear neural network function approximators (Mnih\net al., 2016). The actor uses gradients derived from the policy gradient\ntheorem and adjusts the policy parameters w. The critic, parameterized\nby θ, estimates the approximate value function for the current policy π:\nQ(s, a; θ) ≈Qπ(s, a).\nThe critic\nFrom a (set of) tuples < s, a, r, s′ >, possibly taken from a replay\nmemory, the simplest oﬀ-policy approach to estimating the critic is to\nuse a pure bootstrapping algorithm TD(0) where, at every iteration,\nthe current value Q(s, a; θ) is updated towards a target value:\nY Q\nk = r + γQ(s′, a = π(s′); θ)\n(5.7)\nThis approach has the advantage of being simple, yet it is not\ncomputationally eﬃcient as it uses a pure bootstrapping technique that\nis prone to instabilities and has a slow reward propagation backwards\nin time (Sutton, 1996). This is similar to the elements discussed in the\nvalue-based methods in §4.7.\nThe ideal is to have an architecture that is\n• sample-eﬃcient such that it should be able to make use of both\noﬀ-policy and and on-policy trajectories (i.e., it should be able to\nuse a replay memory), and\n• computationally eﬃcient: it should be able to proﬁt from the\nstability and the fast reward propagation of on-policy methods\nfor samples collected from near on-policy behavior policies.\n5.3. Actor-Critic Methods\n41\nThere are many methods that combine on- and oﬀ-policy data for\npolicy evaluation (Precup, 2000). The algorithm Retrace(λ) (Munos\net al., 2016) has the advantages that (i) it can make use of samples\ncollected from any behavior policy without introducing a bias and\n(ii) it is eﬃcient as it makes the best use of samples collected from\nnear on-policy behavior policies. That approach was used in actor-critic\narchitectures described by Wang et al. (2016b) and Gruslys et al. (2017).\nThese architectures are sample-eﬃcient thanks to the use of a replay\nmemory, and computationally eﬃcient since they use multi-step returns\nwhich improves the stability of learning and increases the speed of\nreward propagation backwards in time.\nThe actor\nFrom Equation 5.4, the oﬀ-policy gradient in the policy improvement\nphase for the stochastic case is given as:\n∇wV πw(s0) = Es∼ρπβ ,a∼πβ [∇θ (log πw(s, a)) Qπw (s, a)] .\n(5.8)\nwhere β is a behavior policy generally diﬀerent than π, which makes the\ngradient generally biased. This approach usually behaves properly in\npractice but the use of a biased policy gradient estimator makes diﬃcult\nthe analysis of its convergence without the GLIE assumption (Munos\net al., 2016; Gruslys et al., 2017)2.\nIn the case of actor-critic methods, an approach to perform the policy\ngradient on-policy without experience replay has been investigated with\nthe use of asynchronous methods, where multiple agents are executed\nin parallel and the actor-learners are trained asynchronously (Mnih\net al., 2016). The parallelization of agents also ensures that each agent\nexperiences diﬀerent parts of the environment at a given time step.\nIn that case, n-step returns can be used without introducing a bias.\nThis simple idea can be applied to any learning algorithm that requires\n2Greedy in the Limit with Inﬁnite Exploration (GLIE) means that the behavior\npolicies are required to become greedy (no exploration) in the limit of an online\nlearning setting where the agent has gathered an inﬁnite amount of experience. It\nis required that « (i) each action is executed inﬁnitely often in every state that is\nvisited inﬁnitely often, and (ii) in the limit, the learning policy is greedy with respect\nto the Q-value function with probability 1 »(Singh et al., 2000).\n42\nPolicy gradient methods for deep RL\non-policy data and it removes the need to maintain a replay buﬀer.\nHowever, this asynchronous trick is not sample eﬃcient.\nAn alternative is to combine oﬀ-policy and on-policy samples to\ntrade-oﬀboth the sample eﬃciency of oﬀ-policy methods and the\nstability of on-policy gradient estimates. For instance, Q-Prop (Gu\net al., 2017b) uses a Monte Carlo on-policy gradient estimator, while\nreducing the variance of the gradient estimator by using an oﬀ-policy\ncritic as a control variate. One limitation of Q-Prop is that it requires\nusing on-policy samples for estimating the policy gradient.\n5.4\nNatural Policy Gradients\nNatural policy gradients are inspired by the idea of natural gradients\nfor the updates of the policy. Natural gradients can be traced back to\nthe work of Amari, 1998 and has been later adapted to reinforcement\nlearning (Kakade, 2001).\nNatural policy gradient methods use the steepest direction given\nby the Fisher information metric, which uses the manifold of the\nobjective function. In the simplest form of steepest ascent for an\nobjective function J(w), the update is of the form △w ∝∇wJ(w).\nIn other words, the update follows the direction that maximizes\n(J(w) −J(w + △w)) under a constraint on || △w||2. In the hypothesis\nthat the constraint on △w is deﬁned with another metric than L2, the\nﬁrst-order solution to the constrained optimization problem typically\nhas the form △w ∝B−1∇wJ(w) where B is an nw × nw matrix. In\nnatural gradients, the norm uses the Fisher information metric, given by\na local quadratic approximation to the KL divergence DKL(πw||πw+∆w).\nThe natural gradient ascent for improving the policy πw is given by\n△w ∝F −1\nw ∇wV πw(·),\n(5.9)\nwhere Fw is the Fisher information matrix given by\nFw = Eπw[∇w log πw(s, ·)(∇w log πw(s, ·))T ].\n(5.10)\nPolicy gradients following ∇wV πw(·) are often slow because they are\nprone to getting stuck in local plateaus. Natural gradients, however, do\nnot follow the usual steepest direction in the parameter space, but the\n5.5. Trust Region Optimization\n43\nsteepest direction with respect to the Fisher metric. Note that, as the\nangle between natural and ordinary gradient is never larger than ninety\ndegrees, convergence is also guaranteed when using natural gradients.\nThe caveat with natural gradients is that, in the case of neural\nnetworks and their large number of parameters, it is usually impractical\nto compute, invert, and store the Fisher information matrix (Schulman\net al., 2015). This is the reason why natural policy gradients are usually\nnot used in practice for deep RL; however alternatives inspired by this\nidea have been found and they are discussed in the following section.\n5.5\nTrust Region Optimization\nAs a modiﬁcation to the natural gradient method, policy optimization\nmethods based on a trust region aim at improving the policy while\nchanging it in a controlled way. These constraint-based policy optimiza-\ntion methods focus on restricting the changes in a policy using the KL\ndivergence between the action distributions. By bounding the size of\nthe policy update, trust region methods also bound the changes in state\ndistributions guaranteeing improvements in policy.\nTRPO (Schulman et al., 2015) uses constrained updates and\nadvantage function estimation to perform the update, resulting in\nthe reformulated optimization given by\nmax\n△w Es∼ρπw,a∼πw\n\u0014πw+△w(s, a)\nπw(s, a)\nAπw(s, a)\n\u0015\n(5.11)\nsubject to E DKL (πw(s, ·)||πw+△w(s, ·)) ≤δ, where δ\n∈R is a\nhyperparameter. From empirical data, TRPO uses a conjugate gradient\nwith KL constraint to optimize the objective function.\nProximal Policy Optimization (PPO) (Schulman et al., 2017b) is a\nvariant of the TRPO algorithm, which formulates the constraint as a\npenalty or a clipping objective, instead of using the KL constraint. Unlike\nTRPO, PPO considers modifying the objective function to penalize\nchanges to the policy that move rt(w) = πw+△w(s,a)\nπw(s,a)\naway from 1. The\nclipping objective that PPO maximizes is given by\nE\ns∼ρπw,a∼πw\n\u0014\nmin\n\u0010\nrt(w)Aπw(s, a), clip\n\u0000rt(w), 1 −ϵ, 1 + ϵ\n\u0001Aπw(s, a)\n\u0011\u0015\n(5.12)\n44\nPolicy gradient methods for deep RL\nwhere ϵ ∈R is a hyperparameter. This objective function clips\nthe probability ratio to constrain the changes of rt in the interval\n[1 −ϵ, 1 + ϵ].\n5.6\nCombining policy gradient and Q-learning\nPolicy gradient is an eﬃcient technique for improving a policy in a\nreinforcement learning setting. As we have seen, this typically requires\nan estimate of a value function for the current policy and a sample\neﬃcient approach is to use an actor-critic architecture that can work\nwith oﬀ-policy data.\nThese algorithms have the following properties unlike the methods\nbased on DQN discussed in Chapter 4:\n• They are able to work with continuous action spaces. This is\nparticularly interesting in applications such as robotics, where\nforces and torques can take a continuum of values.\n• They can represent stochastic policies, which is useful for building\npolicies that can explicitly explore. This is also useful in settings\nwhere the optimal policy is a stochastic policy (e.g., in a multi-\nagent setting where the Nash equilibrium is a stochastic policy).\nHowever, another approach is to combine policy gradient methods\ndirectly with oﬀ-policy Q-learning (O’Donoghue et al., 2016). In some\nspeciﬁc settings, depending on the loss function and the entropy\nregularization used, value-based methods and policy-based methods are\nequivalent (Fox et al., 2015; O’Donoghue et al., 2016; Haarnoja et al.,\n2017; Schulman et al., 2017a). For instance, when adding an entropy\nregularization, Eq. 5.4 can be written as\n∇wV πw(s0) = Es,a [∇w (log πw(s, a)) Qπw (s, a)] + αEs∇wHπw(s).\n(5.13)\nwhere Hπ(s) = −P\na π(s, a) log π(s, a). From this, one can note\nthat an optimum is satisﬁed by the following policy: πw(s, a) =\nexp(Aπw(s, a)/α−Hπw(s)). Therefore, we can use the policy to derive an\nestimate of the advantage function: ˜Aπw(s, a) = α(log πw(s, a)+Hπ(s)).\n5.6. Combining policy gradient and Q-learning\n45\nWe can thus think of all model-free methods as diﬀerent facets of the\nsame approach.\nOne remaining limitation is that both value-based and policy-based\nmethods are model-free and they do not make use of any model of the\nenvironment. The next chapter describes algorithms with a model-based\napproach.\n6\nModel-based methods for deep RL\nIn Chapters 4 and 5, we have discussed the model-free approach that\nrely either on a value-based or a policy-based method. In this chapter,\nwe introduce the model-based approach that relies on a model of the\nenvironment (dynamics and reward function) in conjunction with a\nplanning algorithm. In §6.2, the respective strengths of the model-based\nversus the model-free approaches are discussed, along with how the two\napproaches can be integrated.\n6.1\nPure model-based methods\nA model of the environment is either explicitly given (e.g., in the game\nof Go for which all the rules are known a priori) or learned from\nexperience. To learn the model, yet again function approximators bring\nsigniﬁcant advantages in high-dimensional (possibly partially observable)\nenvironments (Oh et al., 2015; Mathieu et al., 2015; Finn et al., 2016a;\nKalchbrenner et al., 2016; Duchesne et al., 2017; Nagabandi et al., 2018).\nThe model can then act as a proxy for the actual environment.\nWhen a model of the environment is available, planning consists\nin interacting with the model to recommend an action. In the case\nof discrete actions, lookahead search is usually done by generating\n46\n6.1. Pure model-based methods\n47\npotential trajectories. In the case of a continuous action space, trajectory\noptimization with a variety of controllers can be used.\n6.1.1\nLookahead search\nA lookahead search in an MDP iteratively builds a decision tree where\nthe current state is the root node. It stores the obtained returns in the\nnodes and focuses attention on promising potential trajectories. The\nmain diﬃculty in sampling trajectories is to balance exploration and\nexploitation. On the one hand, the purpose of exploration is to gather\nmore information on the part of the search tree where few simulations\nhave been performed (i.e., where the expected value has a high variance).\nOn the other hand, the purpose of exploitation is to reﬁne the expected\nvalue of the most promising moves.\nMonte-Carlo tree search (MCTS) techniques (Browne et al., 2012)\nare popular approaches to lookahead search. Among others, they have\ngained popularity thanks to proliﬁc achievements in the challenging\ntask of computer Go (Brügmann, 1993; Gelly et al., 2006; Silver et al.,\n2016a). The idea is to sample multiple trajectories from the current state\nuntil a terminal condition is reached (e.g., a given maximum depth)\n(see Figure 6.1 for an illustration). From those simulation steps, the\nMCTS algorithm then recommends an action to take.\nRecent works have developed strategies to directly learn end-to-end\nthe model, along with how to make the best use of it, without relying on\nexplicit tree search techniques (Pascanu et al., 2017). These approaches\nshow improved sample eﬃciency, performance, and robustness to model\nmisspeciﬁcation compared to the separated approach (simply learning\nthe model and then relying on it during planning).\n6.1.2\nTrajectory optimization\nLookahead search techniques are limited to discrete actions, and\nalternative techniques have to be used for the case of continuous actions.\nIf the model is diﬀerentiable, one can directly compute an analytic policy\ngradient by backpropagation of rewards along trajectories (Nguyen\nand Widrow, 1990). For instance, PILCO (Deisenroth and Rasmussen,\n2011) uses Gaussian processes to learn a probabilistic model of the\n48\nModel-based methods for deep RL\nst\na(i) ∈A\nMonte-carlo\nsimulation\nEnd\nstate\nt\nt + 1\nt + 2\na(i) ∈A\nFigure 6.1: Illustration of how a MCTS algorithm performs a Monte-Carlo\nsimulation and builds a tree by updating the statistics of the diﬀerent nodes. Based\non the statistics gathered for the current node st, the MCTS algorithm chooses an\naction to perform on the actual environment.\ndynamics. It can then explicitly use the uncertainty for planning and\npolicy evaluation in order to achieve a good sample eﬃciency. However,\nthe gaussian processes have not been able to scale reliably to high-\ndimensional problems.\nOne approach to scale planning to higher dimensions is to aim at\nleveraging the generalization capabilities of deep learning. For instance,\nWahlström et al. (2015) uses a deep learning model of the dynamics\n(with an auto-encoder) along with a model in a latent state space.\nModel-predictive control (Morari and Lee, 1999) can then be used to\nﬁnd the policy by repeatedly solving a ﬁnite-horizon optimal control\nproblem in the latent space. It is also possible to build a probabilistic\ngenerative model in a latent space with the objective that it possesses\na locally linear dynamics, which allows control to be performed more\neﬃciently (Watter et al., 2015). Another approach is to use the trajectory\noptimizer as a teacher rather than a demonstrator: guided policy\nsearch (Levine and Koltun, 2013) takes a few sequences of actions\nsuggested by another controller. iIt then learns to adjust the policy from\nthese sequences. Methods that leverage trajectory optimization have\n6.2. Integrating model-free and model-based methods\n49\ndemonstrated many capabilities, for instance in the case of simulated\n3D bipeds and quadrupeds (e.g., Mordatch et al., 2015).\n6.2\nIntegrating model-free and model-based methods\nThe respective strengths of the model-free versus model-based ap-\nproaches depend on diﬀerent factors. First, the best suited approach\ndepends on whether the agent has access to a model of the environment.\nIf that’s not the case, the learned model usually has some inaccuracies\nthat should be taken into account. Note that learning the model can\nshare the hidden-state representations with a value-based approach by\nsharing neural network parameters (Li et al., 2015).\nSecond, a model-based approach requires working in conjunction\nwith a planning algorithm (or controller), which is often computationally\ndemanding. The time constraints for computing the policy π(s) via\nplanning must therefore be taken into account (e.g., for applications\nwith real-time decision-making or simply due to resource limitations).\nThird, for some tasks, the structure of the policy (or value function) is\nthe easiest one to learn, but for other tasks, the model of the environment\nmay be learned more eﬃciently due to the particular structure of the\ntask (less complex or with more regularity). Thus, the most performant\napproach depends on the structure of the model, policy, and value\nfunction (see the coming Chapter 7 for more details on generalization).\nLet us consider two examples to better understand this key consideration.\nIn a labyrinth where the agent has full observability, it is clear how\nactions aﬀect the next state and the dynamics of the model may easily be\ngeneralized by the agent from only a few tuples (for instance, the agent\nis blocked when trying to cross a wall of the labyrinth). Once the model\nis known, a planning algorithm can then be used with high performance.\nLet us now discuss another example where, on the contrary, planning\nis more diﬃcult: an agent has to cross a road with random events\nhappening everywhere on the road. Let us suppose that the best policy\nis simply to move forward except when an object has just appeared\nin front of the agent. In that case, the optimal policy may easily be\ncaptured by a model-free approach, while a model-based approach would\nbe more diﬃcult (mainly due to the stochasticity of the model which\n50\nModel-based methods for deep RL\nleads to many diﬀerent possible situations, even for one given sequence\nof actions).\nModel-based\nRL\nValue-based\nRL\nPolicy-based\nRL\nFigure 6.2: Venn diagram in the space of possible RL algorithms.\nWe now describe how it is possible to obtain advantages from both\nworlds by integrating learning and planning into one end-to-end training\nprocedure so as to obtain an eﬃcient algorithm both in performance\n(sample eﬃcient) and in computation time. A Venn diagram of the\ndiﬀerent combinations is given in Figure 6.2.\nWhen the model is available, one direct approach is to use tree\nsearch techniques that make use of both value and policy networks\n(e.g., Silver et al., 2016a). When the model is not available and under\nthe assumption that the agent has only access to a limited number of\ntrajectories, the key property is to have an algorithm that generalizes\nwell (see Chapter 7 for a discussion on generalization). One possibility\nis to build a model that is used to generate additional samples for a\nmodel-free reinforcement learning algorithm (Gu et al., 2016b). Another\npossibility is to use a model-based approach along with a controller\nsuch as MPC to perform basic tasks and use model-free ﬁne-tuning in\norder to achieve task success (Nagabandi et al., 2017).\nOther approaches build neural network architectures that combine\nboth model-free and model-based elements. For instance, it is possible\nto combine a value function with steps of back-propagation through a\nmodel (Heess et al., 2015). The VIN architecture (Tamar et al., 2016)\n6.2. Integrating model-free and model-based methods\n51\nis a fully diﬀerentiable neural network with a planning module that\nlearns to plan from model-free objectives (given by a value function). It\nworks well for tasks that involve planning-based reasoning (navigation\ntasks) from one initial position to one goal position and it demonstrates\nstrong generalization in a few diﬀerent domains.\nIn the same spirit, the predictron (Silver et al., 2016b) is aimed at\ndeveloping a more generally applicable algorithm that is eﬀective in the\ncontext of planning. It works by implicitly learning an internal model\nin an abstract state space, which is used for policy evaluation. The\npredictron is trained end-to-end to learn, from the abstract state space,\n(i) the immediate reward and (ii) value functions over multiple planning\ndepths. The predictron architecture is limited to policy evaluation, but\nthe idea was extended to an algorithm that can learn an optimal policy\nin an architecture called VPN (Oh et al., 2017). Since VPN relies on\nn-step Q-learning, it requires however on-policy data.\nOther works have proposed architectures that combine model-based\nand model-free approaches. Schema Networks (Kansky et al., 2017) learn\nthe dynamics of an environment directly from data by enforcing some\nrelational structure. The idea is to use a richly structured architecture\nsuch that it provides robust generalization thanks to an object-oriented\napproach for the model.\nI2As (Weber et al., 2017) does not use the model to directly perform\nplanning but it uses the predictions as additional context in deep policy\nnetworks. The proposed idea is that I2As could learn to interpret\npredictions from the learned model to construct implicit plans.\nTreeQN (Farquhar et al., 2017) constructs a tree by recursively\napplying an implicit transition model in an implicitly learned abstract\nstate space, built by estimating Q-values. Farquhar et al. (2017) also\npropose ATreeC, which is an actor-critic variant that augments TreeQN\nwith a softmax layer to form a stochastic policy network.\nThe CRAR agent explicitly learns both a value function and a model\nvia a shared low-dimensional learned encoding of the environment,\nwhich is meant to capture summarized abstractions and allow for\neﬃcient planning (François-Lavet et al., 2018). By forcing an expressive\nrepresentation, the CRAR approach creates an interpretable low-\n52\nModel-based methods for deep RL\ndimensional representation of the environment, even far temporally\nfrom any rewards or in the absence of model-free objectives.\nImproving the combination of model-free and model-based ideas\nis one key area of research for the future development of deep RL\nalgorithms. We therefore expect to see smarter and richer structures in\nthat domain.\n7\nThe concept of generalization\nGeneralization is a central concept in the ﬁeld of machine learning, and\nreinforcement learning is no exception. In an RL algorithm (model-free\nor model-based), generalization refers to either\n• the capacity to achieve good performance in an environment where\nlimited data has been gathered, or\n• the capacity to obtain good performance in a related environment.\nIn the former case, the agent must learn how to behave in a test\nenvironment that is identical to the one it has been trained on. In\nthat case, the idea of generalization is directly related to the notion\nof sample eﬃciency (e.g., when the state-action space is too large to\nbe fully visited). In the latter case, the test environment has common\npatterns with the training environment but can diﬀer in the dynamics\nand the rewards. For instance, the underlying dynamics may be the\nsame but a transformation on the observations may have happened\n(e.g., noise, shift in the features, etc.). That case is related to the idea\nof transfer learning (discussed in §10.2) and meta-learning (discussed\nin §10.1.2).\n53\n54\nThe concept of generalization\nNote that, in the online setting, one mini-batch gradient update\nis usually done at every step. In that case, the community has also\nused the term sample eﬃciency to refer to how fast the algorithm\nlearns, which is measured in terms of performance for a given number\nof steps (number of learning steps=number of transitions observed).\nHowever, in that context, the result depends on many diﬀerent elements.\nIt depends on the learning algorithm and it is, for instance, inﬂuenced\nby the possible variance of the target in a model-free setting. It also\ndepends on the exploration/exploitation, which will be discussed in\n§8.1 (e.g, instabilities may be good). Finally, it depends on the actual\ngeneralization capabilities.\nIn this chapter, the goal is to study speciﬁcally the aspect of\ngeneralization. We are not interested in the number of mini-batch\ngradient descent steps that are required but rather in the performance\nthat a deep RL algorithm can have in the oﬄine case where the agent\nhas to learn from limited data. Let us consider the case of a ﬁnite\ndataset D obtained on the exact same task as the test environment.\nFormally, a dataset available to the agent D ∼D can be deﬁned as a\nset of four-tuples < s, a, r, s′ >∈S × A × R × S gathered by sampling\nindependently and identically (i.i.d.)1\n• a given number of state-action pairs (s, a) from some ﬁxed\ndistribution with P(s, a) > 0, ∀(s, a) ∈S × A,\n• a next state s′ ∼T(s, a, ·), and\n• a reward r = R(s, a, s′).\nWe denote by D∞the particular case of a dataset D where the number\nof tuples tends to inﬁnity.\nA learning algorithm can be seen as a mapping of a dataset D into\na policy πD (independently of whether the learning algorithm has a\n1That i.i.d. assumption can, for instance, be obtained from a given distribution\nof initial states by following a stochastic sampling policy that ensures a non-zero\nprobability of taking any action in any given state. That sampling policy should be\nfollowed during at least H time steps with the assumption that all states of the MDP\ncan be reached in a number of steps smaller than H from the given distribution of\ninitial states.\n55\nmodel-based or a model-free approach). In that case, we can decompose\nthe suboptimality of the expected return as follows:\nE\nD∼D\nh\nV π∗(s) −V πD(s)\ni\n=\nE\nD∼D\nh\nV π∗(s) −V πD∞(s) + V πD∞(s) −V πD(s)\ni\n= (V π∗(s) −V πD∞(s))\n|\n{z\n}\nasymptotic bias\n+\nE\nD∼D\nh\nV πD∞(s) −V πD(s)\ni\n|\n{z\n}\nerror due to ﬁnite size of the dataset D\n.\n(7.1)\nThis decomposition highlights two diﬀerent terms: (i) an asymptotic\nbias which is independent of the quantity of data and (ii) an overﬁtting\nterm directly related to the fact that the amount of data is limited.\nThe goal of building a policy πD from a dataset D is to obtain the\nlowest overall suboptimality. To do so, the RL algorithm should be well\nadapted to the task (or the set of tasks).\nIn the previous section, two diﬀerent types of approaches (model-\nbased and model-free) have been discussed, as well as how to combine\nthem. We have discussed the algorithms that can be used for diﬀerent\napproaches but we have in fact left out many important elements that\nhave an inﬂuence on the bias-overﬁtting tradeoﬀ(e.g., Zhang et al.,\n2018c; Zhang et al., 2018a for illustrations of overﬁtting in deep RL).\nAs illustrated in Figure 7.1, improving generalization can be seen as\na tradeoﬀbetween (i) an error due to the fact that the algorithm trusts\ncompletely the frequentist assumption (i.e., discards any uncertainty\non the limited data distribution) and (ii) an error due to the bias\nintroduced to reduce the risk of overﬁtting. For instance, the function\napproximator can be seen as a form of structure introduced to force some\ngeneralization, at the risk of introducing a bias. When the quality of the\ndataset is low, the learning algorithm should favor more robust policies\n(i.e., consider a smaller class of policies with stronger generalization\ncapabilities). When the quality of the dataset increases, the risk of\noverﬁtting is lower and the learning algorithm can trust the data more,\nhence reducing the asymptotic bias.\n56\nThe concept of generalization\nData\nPolicy\nclass\n% of the\nerror\ndue to\noverﬁtting\n% of the\nerror due to\nasymptotic\nbias\nFigure 7.1: Schematic representation of the bias-overﬁtting tradeoﬀ.\nAs we will see, for many algorithmic choices, there is in practice a\ntradeoﬀto be made between asymptotic bias and overﬁtting that we\nsimply call \"bias-overﬁtting tradeoﬀ\". In this section, we discuss the\nfollowing key elements that are at stake when one wants to improve\ngeneralization in deep RL:\n• the state representation,\n• the learning algorithm (type of function approximator and model-\nfree vs model-based),\n• the objective function (e.g., reward shaping, tuning the training\ndiscount factor), and\n• using hierarchical learning.\nThroughout those discussions, a simple example is considered. This\nexample is, by no means, representative of the complexity of real-world\nproblems but it is enlightening to simply illustrate the concepts that\nwill be discussed. Let us consider an MDP with NS states, NS = 11\nand NA actions, NA = 4. Let us suppose that the main part of the\nenvironment is a square 3 × 3 grid world (each represented by a tuple\n(x, y) with x = {0, 1, 2}, y = {0, 1, 2}), such as illustrated in Figure 7.2.\nThe agent starts in the central state (1, 1). In every state, it selects one\nof the 4 actions corresponding to 4 cardinal directions (up, down, left\nand right), which leads the agent to transition deterministically in a\nstate immediately next to it, except when it tries to move out of the\ndomain. On the upper part and lower part of the domain, the agent is\n57\nstuck in the same state if it tries to move out of the domain. On the\nleft, the agent transitions deterministically to a given state, which will\nprovide a reward of 0.6 for any action at the next time step. On the\nright side of the square, the agent transitions with a probability 25% to\nanother state that will provide, at the next time step, a reward of 1 for\nany action (the rewards are 0 for all other states). When a reward is\nobtained, the agent transitions back to the central state.\ny\nx\nP = 1\nP = 0.25\nr = 0.6\nr = 1\nFigure 7.2: Representation of a simple MDP that illustrates the need of\ngeneralization.\nIn this example, if the agent has perfect knowledge of its environment,\nthe best expected cumulative reward (for a discount factor close to 1)\nwould be to always go to the left direction and repeatedly gather a\nreward of 0.6 every 3 steps (as compared to gathering a reward of 1 every\n6 steps on average). Let us now suppose that only limited information\nhas been obtained on the MDP with only one tuple of experience\n< s, a, r, s′ > for each couple < s, a >. According to the limited data in\nthe frequentist assumption, there is a rather high probability (∼58%)\nthat at least one transition from the right side seems to provide a\ndeterministic access to r = 1. In those cases and for either a model-\nbased or a model-free approach, if the learning algorithm comes up\nwith the optimal policy in an empirical MDP built from the frequentist\nstatistics, it would actually suﬀer from poor generalization as it would\nchoose to try to obtain the reward r = 1.\nWe discuss hereafter the diﬀerent aspects that can be used to avoid\noverﬁtting to limited data; we show that it is done by favoring robust\n58\nThe concept of generalization\npolicies within the policy class, usually at the expense of introducing\nsome bias. At the end, we also discuss how the bias-overﬁtting tradeoﬀ\ncan be used in practice to obtain the best performance from limited\ndata.\n7.1\nFeature selection\nThe idea of selecting the right features for the task at hand is key in the\nwhole ﬁeld of machine learning and also highly prevalent in reinforcement\nlearning (see e.g., Munos and Moore, 2002; Ravindran and Barto, 2004;\nLeﬄer et al., 2007; Kroon and Whiteson, 2009; Dinculescu and Precup,\n2010; Li et al., 2011; Ortner et al., 2014; Mandel et al., 2014; Jiang\net al., 2015a; Guo and Brunskill, 2017; François-Lavet et al., 2017). The\nappropriate level of abstraction plays a key role in the bias-overﬁtting\ntradeoﬀand one of the key advantages of using a small but rich abstract\nrepresentation is to allow for improved generalization.\nOverﬁtting\nWhen considering many features on which to base the\npolicy (in the example the y-coordinate of the state as illustrated in\nFigure 7.3), an RL algorithm may take into consideration spurious\ncorrelations, which leads to overﬁtting (in the example, the agent may\ninfer that the y-coordinate changes something to the expected return\nbecause of the limited data).\ns(6)\ns(3)\ns(0)\ns(7)\ns(4)\ns(1)\ns(8)\ns(5)\ns(2)\ny\nx\nEnvironment\n(0, 2)\n(0, 1)\n(0, 0)\n(1, 2)\n(1, 1)\n(1, 0)\n(2, 2)\n(2, 1)\n(2, 0)\nStates\nrepresentation\nwith a set of\nfeatures (x, y)\n(0)\n(0)\n(0)\n(1)\n(1)\n(1)\n(2)\n(2)\n(2)\nFeature\nselection where\nonly the x-coordinate\nhas been kept\nFigure 7.3: Illustration of the state representation and feature selection process. In\nthis case, after the feature selection process, all states with the same x-coordinate\nare considered as indistinguishable.\n7.2. Choice of the learning algorithm and function approximator selection\n59\nAsymptotic bias\nRemoving features that discriminate states with a\nvery diﬀerent role in the dynamics introduces an asymptotic bias. Indeed,\nthe same policy would be enforced on undistinguishable states, hence\nleading to a sub-optimal policy.\nIn deep RL, one approach is to ﬁrst infer a factorized set of\ngenerative factors from the observations. This can be done for instance\nwith an encoder-decoder architecture variant (Higgins et al., 2017;\nZhang et al., 2018b). These features can then be used as inputs to a\nreinforcement learning algorithm. The learned representation can, in\nsome contexts, greatly help for generalization as it provides a more\nsuccinct representation that is less prone to overﬁtting. However, an\nauto-encoder is often too strong of a constraint. On the one hand, some\nfeatures may be kept in the abstract representation because they are\nimportant for the reconstruction of the observations, though they are\notherwise irrelevant for the task at hand (e.g., the color of the cars in a\nself-driving car context). On the other hand, crucial information about\nthe scene may also be discarded in the latent representation, particularly\nif that information takes up a small proportion of the observations x in\npixel space (Higgins et al., 2017). Note that in the deep RL setting, the\nabstraction representation is intertwined with the use of deep learning.\nThis is discussed in detail in the following section.\n7.2\nChoice of the learning algorithm and function approximator\nselection\nThe function approximator in deep learning characterizes how the\nfeatures will be treated into higher levels of abstraction (a fortiori it can\nthus give more or less weight to some features). If there is, for instance,\nan attention mechanism in the ﬁrst layers of a deep neural network, the\nmapping made up of those ﬁrst layers can be seen as a feature selection\nmechanism.\nOn the one hand, if the function approximator used for the value\nfunction and/or the policy and/or the model is too simple, an asymptotic\nbias may appear. On the other hand, when the function approximator\nhas poor generalization, there will be a large error due to the ﬁnite size\n60\nThe concept of generalization\nof the dataset (overﬁtting). In the example above, a particularly good\nchoice of a model-based or model-free approach associated with a good\nchoice of a function approximator could infer that the y-coordinate of\nthe state is less important than the x-coordinate, and generalize that\nto the policy.\nDepending on the task, ﬁnding a performant function approximator\nis easier in either a model-free or a model-based approach. The choice of\nrelying more on one or the other approach is thus also a crucial element\nto improve generalization, as discussed in §6.2.\nOne approach to mitigate non-informative features is to force the\nagent to acquire a set of symbolic rules adapted to the task and to\nreason on a more abstract level. This abstract level reasoning and the\nimproved generalization have the potential to induce high-level cognitive\nfunctions such as transfer learning and analogical reasoning (Garnelo\net al., 2016). For instance, the function approximator may embed a\nrelational learning structure (Santoro et al., 2017) and thus build on\nthe idea of relational reinforcement learning (Džeroski et al., 2001).\n7.2.1\nAuxiliary tasks\nIn the context of deep reinforcement learning, it was shown by Jaderberg\net al. (2016) that augmenting a deep reinforcement learning agent with\nauxiliary tasks within a jointly learned representation can drastically\nimprove sample eﬃciency in learning. This is done by maximizing\nsimultaneously many pseudo-reward functions such as immediate reward\nprediction (γ = 0), predicting pixel changes in the next observation, or\npredicting activation of some hidden unit of the agent’s neural network.\nThe argument is that learning related tasks introduces an inductive\nbias that causes a model to build features in the neural network that\nare useful for the range of tasks (Ruder, 2017). Hence, this formation of\nmore signiﬁcant features leads to less overﬁtting.\nIn deep RL, it is possible to build an abstract state such that it\nprovides suﬃcient information for simultaneously ﬁtting an internal\nmeaningful dynamics as well as the estimation of the expected value\nof an optimal policy. By explicitly learning both the model-free and\nmodel-based components through the state representation, along with an\n7.3. Modifying the objective function\n61\napproximate entropy maximization penalty, the CRAR agent (François-\nLavet et al., 2018) shows how it is possible to learn a low-dimensional\nrepresentation of the task. In addition, this approach can directly make\nuse of a combination of model-free and model-based, with planning\nhappening in a smaller latent state space.\n7.3\nModifying the objective function\nIn order to improve the policy learned by a deep RL algorithm, one can\noptimize an objective function that diverts from the actual objective.\nBy doing so, a bias is usually introduced but this can in some cases\nhelp with generalization. The main approaches to modify the objective\nfunction are either (i) to modify the reward of the task to ease learning\n(reward shaping), or (ii) tune the discount factor at training time.\n7.3.1\nReward shaping\nReward shaping is a heuristic for faster learning. In practice, reward\nshaping uses prior knowledge by giving intermediate rewards for actions\nthat lead to desired outcome. It is usually formalized as a function\nF(s, a, s′) added to the original reward function R(s, a, s′) of the original\nMDP (Ng et al., 1999). This technique is often used in deep reinforcement\nlearning to improve the learning process in settings with sparse and\ndelayed rewards (e.g., Lample and Chaplot, 2017).\n7.3.2\nDiscount factor\nWhen the model available to the agent is estimated from data, the\npolicy found using a shorter planning horizon can actually be better\nthan a policy learned with the true horizon (Petrik and Scherrer, 2009;\nJiang et al., 2015b). On the one hand, artiﬁcially reducing the planning\nhorizon leads to a bias since the objective function is modiﬁed. On the\nother hand, if a long planning horizon is targeted (the discount factor\nγ is close to 1), there is a higher risk of overﬁtting. This overﬁtting can\nintuitively be understood as linked to the accumulation of the errors\nin the transitions and rewards estimated from data as compared to\nthe actual transition and reward probabilities. In the example above\n62\nThe concept of generalization\n(Figure 7.2), in the case where the upper right or lower right states\nwould seem to lead deterministically to r = 1 from the limited data,\none may take into account that it requires more steps and thus more\nuncertainty on the transitions (and rewards). In that context, a low\ntraining discount factor would reduce the impact of rewards that are\ntemporally distant. In the example, a discount factor close to 0 would\ndiscount the estimated rewards at three time steps much more strongly\nthan the rewards two time steps away, hence practically discarding the\npotential rewards that can be obtained by going through the corners as\ncompared to the ones that only require moving along the x-axis.\nIn addition to the bias-overﬁtting tradeoﬀ, a high discount factor\nalso requires speciﬁc care in value iteration algorithms as it can lead to\ninstabilities in convergence. This eﬀect is due to the mappings used in\nthe value iteration algorithms with bootstrapping (e.g., Equation 4.2\nfor the Q-learning algorithm) that propagate errors more strongly with\na high discount factor. This issue is discussed by Gordon (1999) with\nthe notion of non-expansion/expansion mappings. When bootstrapping\nis used in a deep RL value iteration algorithm, the risk of instabilities\nand overestimation of the value function is empirically stronger for a\ndiscount factor close to one (François-Lavet et al., 2015).\n7.4\nHierarchical learning\nThe possibility of learning temporally extended actions (as opposed\nto atomic actions that last for one time-step) has been formalized\nunder the name of options (Sutton et al., 1999). Similar ideas have\nalso been denoted in the literature as macro-actions (McGovern et\nal., 1997) or abstract actions (Hauskrecht et al., 1998). The usage of\noptions is an important challenge in RL because it is essential when\nthe task at hand requires working on long time scales while developing\ngeneralization capabilities and easier transfer learning between the\nstrategies. A few recent works have brought interesting results in\nthe context of fully diﬀerentiable (hence learnable in the context of\ndeep RL) options discovery. In the work of Bacon et al., 2016, an\noption-critic architecture is presented with the capability of learning\nsimultaneously the internal policies and the termination conditions of\n7.5. How to obtain the best bias-overﬁtting tradeoﬀ\n63\noptions, as well as the policy over options. In the work of Vezhnevets\net al., 2016, the deep recurrent neural network is made up of two main\nelements. The ﬁrst module generates an action-plan (stochastic plan\nof future actions) while the second module maintains a commitment-\nplan which determines when the action-plan has to be updated or\nterminated. Many variations of these approaches are also of interest\n(e.g., Kulkarni et al., 2016; Mankowitz et al., 2016). Overall, building\na learning algorithm that is able to do hierarchical learning can be a\ngood way of constraining/favoring some policies that have interesting\nproperties and thus improving generalization.\n7.5\nHow to obtain the best bias-overﬁtting tradeoﬀ\nFrom the previous sections, it is clear that there is a large variety\nof algorithmic choices and parameters that have an inﬂuence on the\nbias-overﬁtting tradeoﬀ(including the choice of approach between model-\nbased and model-free). An overall combination of all these elements\nprovides a low overall sub-optimality.\nFor a given algorithmic parameter setting and keeping all other\nthings equal, the right level of complexity is the one at which the increase\nin bias is equivalent to the reduction of overﬁtting (or the increase in\noverﬁtting is equivalent to the reduction of bias). However, in practice,\nthere is usually not an analytical way to ﬁnd the right tradeoﬀs to be\nmade between all the algorithmic choices and parameters. Still, there\nare a variety of practical strategies that can be used. We now discuss\nthem for the batch setting case and the online setting case.\n7.5.1\nBatch setting\nIn the batch setting case, the selection of the policy parameters to\neﬀectively balance the bias-overﬁtting tradeoﬀcan be done similarly\nto that in supervised learning (e.g., cross-validation) as long as the\nperformance criterion can be estimated from a subset of the trajectories\nfrom the dataset D not used during training (i.e., a validation set).\nOne approach is to ﬁt an MDP model to the data via regression (or\nsimply use the frequentist statistics for ﬁnite state and action space).\n64\nThe concept of generalization\nThe empirical MDP can then be used to evaluate the policy. This purely\nmodel-based estimator has alternatives that do not require ﬁtting a\nmodel. One possibility is to use a policy evaluation step obtained\nby generating artiﬁcial trajectories from the data, without explicitly\nreferring to a model, thus designing a Model-free Monte Carlo-like\n(MFMC) estimator (Fonteneau et al., 2013). Another approach is to\nuse the idea of importance sampling that lets us obtain an estimate of\nV π(s) from trajectories that come from a behavior policy β ̸= π, where\nβ is assumed to be known (Precup, 2000). That approach is unbiased\nbut the variance usually grows exponentially in horizon, which renders\nthe method unsuitable when the amount of data is low. A mix of the\nregression-based approach and the importance sampling approach is\nalso possible (Jiang and Li, 2016; Thomas and Brunskill, 2016), and\nthe idea is to use a doubly-robust estimator that is both unbiased and\nwith a lower variance than the importance sampling estimators.\nNote that there exists a particular case where the environment’s\ndynamics are known to the agent, but contain a dependence on\nan exogenous time series (e.g., trading in energy markets, weather-\ndependent dynamics) for which the agent only has ﬁnite data. In that\ncase, the exogenous signal can be broken down in training time series\nand validation time series (François-Lavet et al., 2016b). This allows\ntraining on the environment with the training time series and this\nallows estimating any policy on the environment with the validation\ntime series.\n7.5.2\nOnline setting\nIn the online setting, the agent continuously gathers new experience.\nThe bias-overﬁtting tradeoﬀstill plays a key role at each stage of the\nlearning process in order to achieve good sampling eﬃciency. Indeed,\na performant policy from given data is part of the solution to an\neﬃcient exploration/exploitation tradeoﬀ. For that reason, progressively\nﬁtting a function approximator as more data becomes available can in\nfact be understood as a way to obtain a good bias-overﬁtting tradeoﬀ\nthroughout learning. With the same logic, progressively increasing the\ndiscount factor allows optimizing the bias-overﬁtting tradeoﬀthrough\n7.5. How to obtain the best bias-overﬁtting tradeoﬀ\n65\nlearning (François-Lavet et al., 2015). Besides, optimizing the bias-\noverﬁtting tradeoﬀalso suggests the possibility to dynamically adapt\nthe feature space and/or the function approximator. For example, this\ncan be done through ad hoc regularization, or by adapting the neural\nnetwork architecture, using for instance the NET2NET transformation\n(Chen et al., 2015).\n8\nParticular challenges in the online setting\nAs discussed in the introduction, reinforcement learning can be used in\ntwo main settings: (i) the batch setting (also called oﬄine setting), and\n(ii) the online setting. In a batch setting, the whole set of transitions\n(s, a, r, s′) to learn the task is ﬁxed. This is in contrast to the online\nsetting where the agent can gather new experience gradually. In the\nonline setting, two speciﬁc elements have not yet been discussed in\ndepth. First, the agent can inﬂuence how to gather experience so that\nit is the most useful for learning. This is the exploration/exploitation\ndilemma that we discuss in Section 8.1. Second, the agent has the\npossibility to use a replay memory (Lin, 1992) that allows for a good\ndata-eﬃciency. We discuss in Section 8.2 what experience to store and\nhow to reprocess that experience.\n8.1\nExploration/Exploitation dilemma\nThe exploration-exploitation dilemma is a well-studied tradeoﬀin RL\n(e.g., Thrun, 1992). Exploration is about obtaining information about the\nenvironment (transition model and reward function) while exploitation\nis about maximizing the expected return given the current knowledge.\nAs an agent starts accumulating knowledge about its environment, it\n66\n8.1. Exploration/Exploitation dilemma\n67\nhas to make a tradeoﬀbetween learning more about its environment\n(exploration) or pursuing what seems to be the most promising strategy\nwith the experience gathered so far (exploitation).\n8.1.1\nDiﬀerent settings in the exploration/exploitation dilemma\nThere exist mainly two diﬀerent settings. In the ﬁrst setting, the agent\nis expected to perform well without a separate training phase. Thus,\nan explicit tradeoﬀbetween exploration versus exploitation appears so\nthat the agent should explore only when the learning opportunities are\nvaluable enough for the future to compensate what direct exploitation\ncan provide. The sub-optimality E\ns0 V ∗(s0) −V π(s0) of an algorithm\nobtained in this context is known as the cumulative regret 1. The deep\nRL community is usually not focused on this case, except when explicitly\nstated such as in the works of Wang et al. (2016a) and Duan et al.\n(2016b).\nIn the more common setting, the agent is allowed to follow a training\npolicy during a ﬁrst phase of interactions with the environment so as to\naccumulate training data and hence learn a test policy. In the training\nphase, exploration is only constrained by the interactions it can make\nwith the environment (e.g., a given number of interactions). The test\npolicy should then be able to maximize a cumulative sum of rewards in\na separate phase of interaction. The sub-optimality E\ns0 V ∗(s0) −V π(s0)\nobtained in this case of setting is known as the simple regret. Note\nthat an implicit exploration/exploitation is still important. On the\none hand, the agent has to ensure that the lesser-known parts of the\nenvironment are not promising (exploration). On the other hand, the\nagent is interested in gathering experience in the most promising parts of\nthe environment (which relates to exploitation) to reﬁne the knowledge\nof the dynamics. For instance, in the bandit task provided in Figure 8.1,\nit should be clear with only a few samples that the option on the right\nis less promising and the agent should gather experience mainly on the\ntwo most promising arms to be able to discriminate the best one.\n1This term is mainly used in the bandit community where the agent is in only\none state and where a distribution of rewards is associated to each action; see e.g.,\nBubeck et al., 2011.\n68\nParticular challenges in the online setting\n0\nRmax r\nP(·)\n0\nRmax r\nP(·)\n0\nRmax r\nP(·)\nFigure 8.1: Illustration of the reward probabilities of 3 arms in a multi-armed\nbandit problem.\n8.1.2\nDiﬀerent approaches to exploration\nThe exploration techniques are split into two main categories: (i) directed\nexploration and (ii) undirected exploration (Thrun, 1992).\nIn the undirected exploration techniques, the agent does not rely on\nany exploration speciﬁc knowledge of the environment (Thrun, 1992).\nFor instance, the technique called ϵ-greedy takes a random action with\nprobability ϵ and follows the policy that is believed to be optimal with\nprobability 1−ϵ. Other variants such as softmax exploration (also called\nBoltzmann exploration) takes an action with a probability that depends\non the associated expected return.\nContrary to the undirected exploration, directed exploration\ntechniques make use of a memory of the past interactions with the\nenvironment. For MDPs, directed exploration can scale polynomially\nwith the size of the state space while undirected exploration scales\nin general exponentially with the size of the state space (e.g., E3 by\nKearns and Singh, 2002; R-max by Brafman and Tennenholtz, 2003; ...).\nInspired by the Bayesian setting, directed exploration can be done via\nheuristics of exploration bonus (Kolter and Ng, 2009) or by maximizing\nShannon information gains (e.g., Sun et al., 2011).\nDirected exploration is, however, not trivially applicable in high-\ndimensional state spaces (e.g., Kakade et al., 2003). With the develop-\nment of the generalization capabilities of deep learning, some possibil-\nities have been investigated. The key challenge is to handle, for high-\ndimensional spaces, the exploration/exploitation tradeoﬀin a principled\nway – with the idea to encourage the exploration of the environment\n8.1. Exploration/Exploitation dilemma\n69\nwhere the uncertainty due to limited data is the highest. When rewards\nare not sparse, a measure of the uncertainty on the value function can\nbe used to drive the exploration (Dearden et al., 1998; Dearden et al.,\n1999). When rewards are sparse, this is even more challenging and\nexploration should in addition be driven by some novelty measures on\nthe observations (or states in a Markov setting).\nBefore discussing the diﬀerent techniques that have been proposed\nin the deep RL setting, one can note that the success of the ﬁrst deep\nRL algorithms such as DQN also come from the exploration that arises\nnaturally. Indeed, following a simple ϵ-greedy scheme online often proves\nto be already relatively eﬃcient thanks to the natural instability of the\nQ-network that drives exploration (see Chapter 4 for why there are\ninstabilities when using bootstrapping in a ﬁtted Q-learning algorithm\nwith neural networks).\nDiﬀerent improvements are directly built on that observation. For\ninstance, the method of \"Bootstrapped DQN\" (Osband et al., 2016)\nmakes an explicit use of randomized value functions. Along similar lines,\neﬃcient exploration has been obtained by the induced stochasticity\nof uncertainty estimates given by a dropout Q-network (Gal and\nGhahramani, 2016) or parametric noise added to its weights (Lipton\net al., 2016; Plappert et al., 2017; Fortunato et al., 2017). One speciﬁcity\nof the work done by Fortunato et al., 2017 is that, similarly to Bayesian\ndeep learning, the variance parameters are learned by gradient descent\nfrom the reinforcement learning loss function.\nAnother common approach is to have a directed scheme thanks\nto exploration rewards given to the agent via heuristics that estimate\nnovelty (Schmidhuber, 2010; Stadie et al., 2015; Houthooft et al., 2016).\nIn (Bellemare et al., 2016; Ostrovski et al., 2017), an algorithm provides\nthe notion of novelty through a pseudo-count from an arbitrary density\nmodel that provides an estimate of how many times an action has been\ntaken in similar states. This has shown good results on one of the most\ndiﬃcult Atari 2600 games, Montezuma’s Revenge.\nIn (Florensa et al., 2017), useful skills are learned in pre-training\nenvironments, which can then be utilized in the actual environment\nto improve exploration and train a high-level policy over these skills.\nSimilarly, an agent that learns a set of auxiliary tasks may use them to\n70\nParticular challenges in the online setting\neﬃciently explore its environment (Riedmiller et al., 2018). These ideas\nare also related to the creation of options studied in (Machado et al.,\n2017a), where it is suggested that exploration may be tackled by learning\noptions that lead to speciﬁc modiﬁcations in the state representation\nderived from proto-value functions.\nExploration strategies can also make use of a model of the\nenvironment along with planning. In that case, a strategy investigated\nin (Salge et al., 2014; Mohamed and Rezende, 2015; Gregor et al., 2016;\nChiappa et al., 2017) is to have the agent choose a sequence of actions by\nplanning that leads to a representation of state as diﬀerent as possible\nto the current state. In (Pathak et al., 2017; Haber et al., 2018), the\nagent optimizes both a model of its environment and a separate model\nthat predicts the error/uncertainty of its own model. The agent can\nthus seek to take actions that adversarially challenge its knowledge of\nthe environment (Savinov et al., 2018).\nBy providing rewards on unfamiliar states, it is also possible to\nexplore eﬃciently the environments. To determine the bonus, the current\nobservation can be compared with the observations in memory. One\napproach is to deﬁne the rewards based on how many environment steps\nit takes to reach the current observation from those in memory (Savinov\net al., 2018). Another approach is to use a bonus positively correlated\nto the error of predicting features from the observations (e.g., features\ngiven by a ﬁxed randomly initialized neural network) (Burda et al.,\n2018).\nOther approaches require either demonstrations or guidance from\nhuman demonstrators. One line of work suggests using natural\nlanguage to guide the agent by providing exploration bonuses when an\ninstruction is correctly executed (Kaplan et al., 2017). In the case where\ndemonstrations from expert agents are available, another strategy for\nguiding exploration in these domains is to imitate good trajectories. In\nsome cases, it is possible to use demonstrations from experts even when\nthey are given in an environment setup that is not exactly the same\n(Aytar et al., 2018).\n8.2. Managing experience replay\n71\n8.2\nManaging experience replay\nIn online learning, the agent has the possibility to use a replay\nmemory (Lin, 1992) that allows for data-eﬃciency by storing the past\nexperience of the agent in order to have the opportunity to reprocess\nit later. In addition, a replay memory also ensures that the mini-\nbatch updates are done from a reasonably stable data distribution\nkept in the replay memory (for Nreplay suﬃciently large) which helps\nfor convergence/stability. This approach is particularly well-suited\nin the case of oﬀ-policy learning as using experience from past (i.e.\ndiﬀerent) policies does not introduce any bias (usually it is even good\nfor exploration). In that context, methods based for instance on a DQN\nlearning algorithm or model-based learning can safely and eﬃciently\nmake use of a replay memory. In an online setting, the replay memory\nkeeps all information for the last Nreplay ∈N time steps, where Nreplay\nis constrained by the amount of memory available.\nWhile a replay memory allows processing the transitions in a diﬀerent\norder than they are experienced, there is also the possibility to use\nprioritized replay. This allows for consideration of the transitions with\na diﬀerent frequency than they are experienced depending on their\nsigniﬁcance (that could be which experience to store and which ones\nto replay). In (Schaul et al., 2015b), the prioritization increases with\nthe magnitude of the transitions’ TD error, with the aim that the\n\"unexpected\" transitions are replayed more often.\nA disadvantage of prioritized replay is that, in general, it also\nintroduces a bias; indeed, by modifying the apparent probabilities of\ntransitions and rewards, the expected return gets biased. This can\nreadily be understood by considering the simple example illustrated\nin Figure 8.2 where an agent tries to estimate the expected return for\na given tuple < s, a >. In that example, a cumulative return of 0 is\nobtained with probability 1−ϵ (from next state s(1)) while a cumulative\nreturn of C > 0 is obtained with probability ϵ (from next state s(2)).\nIn that case, using prioritized experience replay will bias the expected\nreturn towards a value higher than ϵC since any transition leading to\ns(2) will be replayed with a probability higher than ϵ.\n72\nParticular challenges in the online setting\ns\ns(1)\ns(2)\nT(s, a, s(1)) = 1 −ϵ\nT(s, a, s(2)) = ϵ\nV π(s(1)) = 0\nV π(s(2)) = C > 0\nFigure 8.2: Illustration of a state s where for a given action a, the value of Qπ(s, a; θ)\nwould be biased if prioritized experience replay is used (ϵ << 1).\nNote that this bias can be partly or completely corrected using\nweighted importance sampling, and this correction is important near\nconvergence at the end of training (Schaul et al., 2015b).\n9\nBenchmarking Deep RL\nComparing deep learning algorithms is a challenging problem due\nto the stochastic nature of the learning process and the narrow\nscope of the datasets examined during algorithm comparisons. This\nproblem is exacerbated in deep reinforcement learning. Indeed, deep\nRL involves both stochasticity in the environment and stochasticity\ninherent to model learning, which makes ensuring fair comparisons\nand reproducibility especially diﬃcult. To this end, simulations of\nmany sequential decision-making tasks have been created to serve\nas benchmarks. In this section, we present several such benchmarks.\nNext, we give key elements to ensure consistency and reproducibility\nof experimental results. Finally, we also discuss some open-source\nimplementations for deep RL algorithms.\n9.1\nBenchmark Environments\n9.1.1\nClassic control problems\nSeveral classic control problems have long been used to evaluate\nreinforcement learning algorithms. These include balancing a pole\non a cart (Cartpole) (Barto et al., 1983), trying to get a car\n73\n74\nBenchmarking Deep RL\nup a mountain using momentum (Mountain Car) (Moore, 1990),\nswinging a pole up using momentum and subsequently balancing\nit (Acrobot) (Sutton and Barto, 1998). These problems have been\ncommonly used as benchmarks for tabular RL and RL algorithms using\nlinear function approximators (Whiteson et al., 2011). Nonetheless,\nthese simple environments are still sometimes used to benchmark deep\nRL algorithms (Ho and Ermon, 2016; Duan et al., 2016a; Lillicrap et al.,\n2015).\n9.1.2\nGames\nBoard-games have also been used for evaluating artiﬁcial intelligence\nmethods for decades (Shannon, 1950; Turing, 1953; Samuel, 1959; Sutton,\n1988; Littman, 1994; Schraudolph et al., 1994; Tesauro, 1995; Campbell\net al., 2002). In recent years, several notable works have stood out in\nusing deep RL for mastering Go (Silver et al., 2016a) or Poker (Brown\nand Sandholm, 2017; Moravčik et al., 2017).\nIn parallel to the achievements in board games, video games have\nalso been used to further investigate reinforcement learning algorithms.\nIn particular,\n• many of these games have large observation space and/or large\naction space;\n• they are often non-Markovian, which require speciﬁc care (see\n§10.1);\n• they also usually require very long planning horizons (e.g., due to\nsparse rewards).\nSeveral platforms based on video games have been popularized.\nThe Arcade Learning Environment (ALE) (Bellemare et al., 2013)\nwas developed to test reinforcement algorithms across a wide range of\ndiﬀerent tasks. The system encompasses a suite of iconic Atari games,\nincluding Pong, Asteroids, Montezuma’s Revenge, etc. Figure 9.1 shows\nsample frames from some of these games. On most of the Atari games,\ndeep RL algorithms have reached super-human level (Mnih et al., 2015).\nDue to the similarity in state and action spaces between diﬀerent Atari\n9.1. Benchmark Environments\n75\ngames or diﬀerent variants of the same game, they are also a good test-\nbed for evaluating generalization of reinforcement learning algorithms\n(Machado et al., 2017b), multi-task learning (Parisotto et al., 2015) and\nfor transfer learning (Rusu et al., 2015).\n(a) Space invaders\n(b) Seaquest\n(c) Breakout\nFigure 9.1: Illustration of three Atari games.\nThe General Video Game AI (GVGAI) competition framework\n(Perez-Liebana et al., 2016) was created and released with the purpose\nof providing researchers a platform for testing and comparing their\nalgorithms on a large variety of games and under diﬀerent constraints.\nThe agents are required to either play multiple unknown games with\nor without access to game simulations, or to design new game levels or\nrules.\nVizDoom (Kempka et al., 2016) implements the Doom video game as\na simulated environment for reinforcement learning. VizDoom has been\nused as a platform for investigations of reward shaping (Lample and\nChaplot, 2017), curriculum learning (Wu and Tian, 2016), predictive\nplanning (Dosovitskiy and Koltun, 2016), and meta-reinforcement\nlearning (Duan et al., 2016b).\nThe open-world nature of Minecraft also provides a convenient\nplatform for exploring reinforcement learning and artiﬁcial intelligence.\nProject Malmo (Johnson et al., 2016) is a framework that provides easy\naccess to the Minecraft video game. The environment and framework\nprovide layers of abstraction that facilitate tasks ranging from simple\nnavigation to collaborative problem solving. Due to the nature of\nthe simulation, several works have also investigated lifelong-learning,\ncurriculum learning, and hierarchical planning using Minecraft as a\n76\nBenchmarking Deep RL\nplatform (Tessler et al., 2017; Matiisen et al., 2017; Branavan et al.,\n2012; Oh et al., 2016).\nSimilarly, Deepmind Lab (Beattie et al., 2016) provides a 3D\nplatform adapted from the Quake video game. The Labyrinth maze\nenvironments provided with the framework have been used in work on\nhierarchical, lifelong and curriculum learning (Jaderberg et al., 2016;\nMirowski et al., 2016; Teh et al., 2017).\nFinally, “StarCraft II\" (Vinyals et al., 2017) and “Starcraft:\nBroodwar\" (Wender and Watson, 2012; Synnaeve et al., 2016) provide\nsimilar beneﬁts in exploring lifelong-learning, curriculum learning, and\nother related hierarchical approaches. In addition, real-time strategy\n(RTS) games – as with the Starcraft series – are also an ideal testbed\nfor multi-agent systems. Consequently, several works have investigated\nthese aspects in the Starcraft framework (Foerster et al., 2017b; Peng\net al., 2017a; Brys et al., 2014).\n9.1.3\nContinuous control systems and robotics domains\nWhile games provide a convenient platform for reinforcement learning,\nthe majority of those environments investigate discrete action decisions.\nIn many real-world systems, as in robotics, it is necessary to provide\nframeworks for continuous control.\nIn that setting, the MuJoCo (Todorov et al., 2012) simulation\nframework is used to provide several locomotion benchmark tasks.\nThese tasks typically involve learning a gait to move a simulated robotic\nagent as fast as possible. The action space is the amount of torque to\napply to motors on the agents’ joints, while the observations provided\nare typically the joint angles and positions in the 3D space. Several\nframeworks have built on top of these locomotion tasks to provide\nhierarchical task environments (Duan et al., 2016a) and multi-task\nlearning platforms (Henderson et al., 2017a).\nBecause the MuJoCo simulator is closed-source and requires a license,\nan open-source initiative called Roboschool (Schulman et al., 2017b)\nprovides the same locomotion tasks along with more complex tasks\ninvolving humanoid robot simulations (such as learning to run and\nchase a moving ﬂag while being hit by obstacles impeding progress).\n9.1. Benchmark Environments\n77\nFigure 9.2: Screenshots from MuJoCo locomotion benchmark environments\nprovided by OpenAI Gym.\nThese tasks allow for evaluation of complex planning in reinforcement\nlearning algorithms.\nPhysics engines have also been used to investigate transfer learning\nto real-world applications. For instance, the Bullet physics engine\n(Coumans, Bai, et al., 2016) has been used to learn locomotion skills\nin simulation, for character animation in games (Peng et al., 2017b)\nor for being transferred to real robots (Tan et al., 2018). This also\nincludes manipulation tasks (Rusu et al., 2016; Duan et al., 2017) where\na robotic arm stacks cubes in a given order. Several works integrate\nRobot Operating System (ROS) with physics engines (such as ODE,\nor Bullet) to provide RL-compatible access to near real-world robotic\nsimulations (Zamora et al., 2016; Ueno et al., 2017). Most of them can\nalso be run on real robotic systems using the same software.\nThere exists also a toolkit that leverages the Unity platform for\ncreating simulation environments (Juliani et al., 2018). This toolkit\nenables the development of learning environments that are rich in\nsensory and physical complexity and supports the multi-agent setting.\n9.1.4\nFrameworks\nMost of the previously cited benchmarks have open-source code available.\nThere also exists easy-to-use wrappers for accessing many diﬀerent\nbenchmarks. One such example is OpenAI Gym (Brockman et al., 2016).\nThis wrapper provides ready access to environments such as algorithmic,\nAtari, board games, Box2d games, classical control problems, MuJoCo\nrobotics simulations, toy text problems, and others. Gym Retro1 is\na wrapper similar to OpenAI Gym and it provides over 1,000 games\n1https://github.com/openai/retro\n78\nBenchmarking Deep RL\nacross a variety of backing emulators. The goal is to study the ability of\ndeep RL agents to generalize between games that have similar concepts\nbut diﬀerent appearances. Other frameworks such as µniverse2 and\nSerpentAI3 also provide wrappers for speciﬁc games or simulations.\n9.2\nBest practices to benchmark deep RL\nEnsuring best practices in scientiﬁc experiments is crucial to continued\nscientiﬁc progress. Across various ﬁelds, investigations in reproducibility\nhave found problems in numerous publications, resulting in several works\nproviding experimental guidelines in proper scientiﬁc practices (Sandve\net al., 2013; Baker, 2016; Halsey et al., 2015; Casadevall and Fang,\n2010). To this end, several works have investigated proper metrics and\nexperimental practices when comparing deep RL algorithms (Henderson\net al., 2017b; Islam et al., 2017; Machado et al., 2017b; Whiteson et al.,\n2011).\nNumber of Trials, Random Seeds and Signiﬁcance Testing\nStochasticity plays a large role in deep RL, both from randomness within\ninitializations of neural networks and stochasticity in environments.\nResults may vary signiﬁcantly simply by changing the random seed.\nWhen comparing the performance of algorithms, it is therefore important\nto run many trials across diﬀerent random seeds.\nIn deep RL, it has become common to simply test an algorithm’s\neﬀectiveness with an average across a few learning trials. While this is\na reasonable benchmark strategy, techniques derived from signiﬁcance\ntesting (Demšar, 2006; Bouckaert and Frank, 2004; Bouckaert, 2003;\nDietterich, 1998) have the advantage of providing statistically grounded\narguments in favor of a given hypothesis. In practice for deep RL, signif-\nicance testing can be used to take into account the standard deviation\nacross several trials with diﬀerent random seeds and environment condi-\ntions. For instance, a simple 2-sample t-test can give an idea of whether\nperformance gains are signiﬁcantly due to the algorithm performance\n2https://github.com/unixpickle/muniverse\n3https://github.com/SerpentAI/SerpentAI\n9.2. Best practices to benchmark deep RL\n79\nor to noisy results in highly stochastic settings. In particular, while\nseveral works have used the top-K trials and simply presented those\nas performance gains, this has been argued to be inadequate for fair\ncomparisons (Machado et al., 2017b; Henderson et al., 2017b).\nIn addition, one should be careful not to over-interpret the results. It\nis possible that a hypothesis can be shown to hold for one or several given\nenvironments and under one or several given set of hyperparameters,\nbut fail in other settings.\nHyperparameter Tuning and Ablation Comparisons\nAnother important consideration is ensuring a fair comparison between\nlearning algorithms. In this case, an ablation analysis compares\nalternate conﬁgurations across several trials with diﬀerent random\nseeds. It is especially important to tune hyperparameters to the greatest\nextent possible for baseline algorithms. Poorly chosen hyperparameters\ncan lead to an unfair comparison between a novel and a baseline\nalgorithm. In particular, network architecture, learning rate, reward\nscale, training discount factor, and many other parameters can\naﬀect results signiﬁcantly. Ensuring that a novel algorithm is indeed\nperforming much better requires proper scientiﬁc procedure when\nchoosing such hyperparameters (Henderson et al., 2017b).\nReporting Results, Benchmark Environments, and Metrics\nAverage returns (or cumulative reward) across evaluation trajectories are\noften reported as a comparison metric. While some literature (Gu et al.,\n2016a; Gu et al., 2017c) has also used metrics such as average maximum\nreturn or maximum return within Z samples, these may be biased to\nmake results for highly unstable algorithms appear more signiﬁcant.\nFor example, if an algorithm reaches a high maximum return quickly,\nbut then diverges, such metrics would ensure this algorithm appears\nsuccessful. When choosing metrics to report, it is important to select\nthose that provide a fair comparison. If the algorithm performs better in\naverage maximum return, but worse by using an average return metric,\nit is important to highlight both results and describe the beneﬁts and\nshortcomings of such an algorithm (Henderson et al., 2017b).\n80\nBenchmarking Deep RL\nThis is also applicable to the selection of which benchmark\nenvironments to report during evaluation. Ideally, empirical results\nshould cover a large mixture of environments to determine in which\nsettings an algorithm performs well and in which settings it does not.\nThis is vital for determining real-world performance applications and\ncapabilities.\n9.3\nOpen-source software for Deep RL\nA deep RL agent is composed of a learning algorithm (model-based or\nmodel-free) along with speciﬁc structure(s) of function approximator(s).\nIn the online setting (more details are given in Chapter 8), the agent\nfollows a speciﬁc exploration/exploitation strategy and typically uses a\nmemory of its previous experience for sample eﬃciency.\nWhile many papers release implementations of various deep RL\nalgorithms, there also exist some frameworks built to facilitate the\ndevelopment of new deep RL algorithms or to apply existing algorithms\nto a variety of environments. We provide a list of some of the existing\nframeworks in Appendix A.\n10\nDeep reinforcement learning beyond MDPs\nWe have so far mainly discussed how an agent is able to learn how\nto behave in a given Markovian environment where all the interesting\ninformation (the state st ∈S) is obtained at every time step t. In\nthis chapter, we discuss more general settings with (i) non-Markovian\nenvironments, (ii) transfer learning and (iii) multi-agent systems.\n10.1\nPartial observability and the distribution of (related) MDPs\nIn domains where the Markov hypothesis holds, it is straightforward to\nshow that the policy need not depend on what happened at previous time\nsteps to recommend an action (by deﬁnition of the Markov hypothesis).\nThis section describes two diﬀerent cases that complicate the Markov\nsetting: the partially observable environments and the distribution of\n(related) environments.\nThose two settings are at ﬁrst sight quite diﬀerent conceptually.\nHowever, in both settings, at each step in the sequential decision process,\nthe agent may beneﬁt from taking into account its whole observable\nhistory up to the current time step t when deciding what action to\nperform. In other words, a history of observations can be used as a\npseudo-state (pseudo-state because that refers to a diﬀerent and abstract\n81\n82\nDeep reinforcement learning beyond MDPs\nstochastic control process). Any missing information in the history of\nobservations (potentially long before time t) can introduce a bias in\nthe RL algorithm (as described in Chapter 7 when some features are\ndiscarded).\n10.1.1\nThe partially observable scenario\nIn this setting, the agent only receives, at each time step, an observation\nof its environment that does not allow it to identify the state with\ncertainty. A Partially Observable Markov Decision Process (POMDP)\n(Sondik, 1978; Kaelbling et al., 1998) is a discrete time stochastic control\nprocess deﬁned as follows:\nDeﬁnition 10.1. A POMDP is a 7-tuple (S, A, T, R, Ω, O, γ) where:\n• S is a ﬁnite set of states {1, . . . , NS},\n• A is a ﬁnite set of actions {1, . . . , NA},\n• T : S ×A×S →[0, 1] is the transition function (set of conditional\ntransition probabilities between states),\n• R : S ×A×S →R is the reward function, where R is a continuous\nset of possible rewards in a range Rmax ∈R+ (e.g., [0, Rmax]\nwithout loss of generality),\n• Ωis a ﬁnite set of observations {1, . . . , NΩ},\n• O : S × Ω→[0, 1] is a set of conditional observation probabilities,\nand\n• γ ∈[0, 1) is the discount factor.\nThe environment starts in a distribution of initial states b(s0). At\neach time step t ∈N0, the environment is in a state st ∈S. At the\nsame time, the agent receives an observation ωt ∈Ωthat depends on\nthe state of the environment with probability O(st, ωt), after which the\nagent chooses an action at ∈A. Then, the environment transitions to\nstate st+1 ∈S with probability T(st, at, st+1) and the agent receives a\nreward rt ∈R equal to R(st, at, st+1).\n10.1. Partial observability and the distribution of (related) MDPs\n83\nWhen the full model (T, R and O) are known, methods such as\nPoint-Based Value Iteration (PBVI) algorithm (Pineau et al., 2003) for\nPOMDP planning can be used to solve the problem. If the full POMDP\nmodel is not available, other reinforcement learning techniques have to\nbe used.\nA naive approach to building a space of candidate policies is to\nconsider the set of mappings taking only the very last observation(s) as\ninput. However, in a POMDP setting, this leads to candidate policies\nthat are typically not rich enough to capture the system dynamics,\nthus suboptimal. In that case, the best achievable policy is stochastic\n(Singh et al., 1994), and it can be obtained using policy gradient. The\nalternative is to use a history of previously observed features to better\nestimate the hidden state dynamics. We denote by Ht = Ω×(A×R×Ω)t\nthe set of histories observed up to time t for t ∈N0 (see Fig. 10.1), and\nby H =\n∞\nS\nt=0\nHt the space of all possible observable histories.\ns0\ns1\ns2\nω0\nω1\nω2\nH0\nH1\nH2\na0\na1\na2\nr0\nr1\n. . .\nPolicy\nPolicy\nHidden\ndynamics\nPolicy\nFigure 10.1: Illustration of a POMDP. The actual dynamics of the POMDP is\ndepicted in dark while the information that the agent can use to select the action at\neach step is the whole history Ht depicted in blue.\nA straightforward approach is to take the whole history Ht ∈H\nas input (Braziunas, 2003). However, increasing the size of the set of\ncandidate optimal policies generally implies: (i) more computation to\nsearch within this set (Singh et al., 1994; McCallum, 1996) and, (ii) an\nincreased risk of including candidate policies suﬀering from overﬁtting\n84\nDeep reinforcement learning beyond MDPs\ndue to lack of suﬃcient data, which thus leads to a bias-overﬁtting\ntradeoﬀwhen learning policies from data (François-Lavet et al., 2017).\nIn the case of deep RL, the architectures used usually have a smaller\nnumber of parameters and layers than in supervised learning due to\nthe more complex RL setting, but the trend of using ever smarter\nand complex architectures in deep RL happens similarly to supervised\nlearning tasks. Architectures such as convolutional layers or recurrency\nare particularly well-suited to deal with a large input space because they\noﬀer interesting generalization properties. A few empirical successes\non large scale POMDPs make use of convolutional layers (Mnih et al.,\n2015) and/or recurrent layers (Hausknecht and Stone, 2015), such as\nLSTMs (Hochreiter and Schmidhuber, 1997).\n10.1.2\nThe distribution of (related) environments\nIn this setting, the environment of the agent is a distribution of diﬀerent\n(yet related) tasks that diﬀer for instance in the reward function or in\nthe probabilities of transitions from one state to another. Each task\nTi ∼T can be deﬁned by the observations ωt ∈Ω(which are equal\nto st if the environments are Markov), the rewards rt ∈R, as well as\nthe eﬀect of the actions at ∈A taken at each step. Similarly to the\npartially observable context, we denote the history of observations by\nHt, where Ht ∈Ht = Ω× (A × R × Ω)t. The agent aims at ﬁnding a\npolicy π(at|Ht; θ) with the objective of maximizing its expected return,\ndeﬁned (in the discounted setting) as\nE\nTi∼T\nhX∞\nk=0 γkrt+k | Ht, π\ni\n.\nAn illustration of the general setting of meta learning on non-Markov\nenvironments is given in Figure 10.2.\nDiﬀerent approaches have been investigated in the literature. The\nBayesian approach aims at explicitly modeling the distribution of the\ndiﬀerent environments, if a prior is available (Ghavamzadeh et al.,\n2015). However, it is often intractable to compute the Bayesian-optimal\nstrategy and one has to rely on more practical approaches that do\nnot require an explicit model of the distribution. The concept of meta-\nlearning or learning to learn aims at discovering, from experience, how\n10.1. Partial observability and the distribution of (related) MDPs\n85\nDistribution of tasks\nRL algorithm\nTraining\non a set\nof tasks\nTesting\non related\ntasks\nFigure 10.2: Illustration of the general setting of meta learning on POMDPs for a\nset of labyrinth tasks. In this illustration, it is supposed that the agent only sees the\nnature of the environment just one time step away from him.\nto behave in a range of tasks and how to negotiate the exploration-\nexploitation tradeoﬀ(Hochreiter et al., 2001). In that case, deep RL\ntechniques have been investigated by, e.g., Wang et al., 2016a; Duan\net al., 2016b with the idea of using recurrent networks trained on a set\nof environments drawn i.i.d. from the distribution.\nSome other approaches have also been investigated. One possibility\nis to train a neural network to imitate the behavior of known optimal\npolicies on MDPs drawn from the distribution (Castronovo et al., 2017).\nThe parameters of the model can also be explicitly trained such that a\nsmall number of gradient steps in a new task from the distribution will\nproduce fast learning on that task (Finn et al., 2017).\n86\nDeep reinforcement learning beyond MDPs\nThere exists diﬀerent denominations for this setting with a\ndistribution of environments. For instance, the denomination of \"multi-\ntask setting\" is usually used in settings where a short history of\nobservations is suﬃcient to clearly distinguish the tasks. As an example\nof a multi-task setting, a deep RL agent can exceed median human\nperformance on the set of 57 Atari games with a single set of weights\n(Hessel et al., 2018). Other related denominations are the concepts\nof \"contextual\" policies (Da Silva et al., 2012) and \"universal\"/\"goal\nconditioned\" value functions (Schaul et al., 2015a) that refer to learning\npolicies or value function within the same dynamics for multiple goals\n(multiple reward functions).\n10.2\nTransfer learning\nTransfer learning is the task of eﬃciently using previous knowledge\nfrom a source environment to achieve a good performance in a target\nenvironment. In a transfer learning setting, the target environment\nshould not be in the distribution of the source tasks. However, in\npractice, the concept of transfer learning is sometimes closely related to\nmeta learning, as we discuss hereafter.\n10.2.1\nZero-shot learning\nThe idea of zero-shot learning is that an agent should be able to\nact appropriately in a new task directly from experience acquired on\nother similar tasks. For instance, one use case is to learn a policy\nin a simulation environment and then use it in a real-world context\nwhere gathering experience is not possible or severely constrained (see\n§11.2). To achieve this, the agent must either (i) develop generalization\ncapacities described in Chapter 7 or (ii) use speciﬁc transfer strategies\nthat explicitly retrain or replace some of its components to adjust to\nnew tasks.\nTo develop generalization capacities, one approach is to use an idea\nsimilar to data augmentation in supervised learning so as to make sense\nof variations that were not encountered in the training data. Exactly\nas in the meta-learning setting (§10.1.2), the actual (unseen) task may\n10.2. Transfer learning\n87\nappear to the agent as just another variation if there is enough data\naugmentation on the training data. For instance, the agent can be\ntrained with deep RL techniques on diﬀerent tasks simultaneously, and\nit is shown by Parisotto et al., 2015 that it can generalize to new related\ndomains where the exact state representation has never been observed.\nSimilarly, the agent can be trained in a simulated environment while\nbeing provided with diﬀerent renderings of the observations. In that case,\nthe learned policy can transfer well to real images (Sadeghi and Levine,\n2016; Tobin et al., 2017). The underlying reason for these successes\nis the ability of the deep learning architecture to generalize between\nstates that have similar high-level representations and should therefore\nhave the same value function/policy in diﬀerent domains. Rather than\nmanually tuning the randomization of simulations, one can also adapt\nthe simulation parameters by matching the policy behavior in simulation\nto the real world (Chebotar et al., 2018). Another approach to zero-shot\ntransfer is to use algorithms that enforce states that relate to the same\nunderlying task but have diﬀerent renderings to be mapped into an\nabstract state that is close (Tzeng et al., 2015; François-Lavet et al.,\n2018).\n10.2.2\nLifelong learning or continual learning\nA speciﬁc way of achieving transfer learning is to aim at lifelong learning\nor continual learning. According to Silver et al., 2013, lifelong machine\nlearning relates to the capability of a system to learn many tasks over\na lifetime from one or more domains.\nIn general, deep learning architectures can generalize knowledge\nacross multiple tasks by sharing network parameters. A direct approach\nis thus to train function approximators (e.g. policy, value function,\nmodel, etc.) sequentially in diﬀerent environments. The diﬃculty of\nthis approach is to ﬁnd methods that enable the agent to retain\nknowledge in order to more eﬃciently learn new tasks. The problem\nof retaining knowledge in deep reinforcement learning is complicated\nby the phenomenon of catastrophic forgetting, where generalization to\npreviously seen data is lost at later stages of learning.\n88\nDeep reinforcement learning beyond MDPs\nThe straightforward approach is to either (i) use experience replay\nfrom all previous experience (as discussed in §8.2), or (ii) retrain\noccasionally on previous tasks similar to the meta-learning setting\n(as discussed in §10.1.2).\nWhen these two options are not available, or as a complement to the\ntwo previous approaches, one can use deep learning techniques that are\nrobust to forgetting, such as progressive networks (Rusu et al., 2016).\nThe idea is to leverage prior knowledge by adding, for each new task,\nlateral connections to previously learned features (that are kept ﬁxed).\nOther approaches to limiting catastrophic forgetting include slowing\ndown learning on the weights important for previous tasks (Kirkpatrick\net al., 2016) and decomposing learning into skill hierarchies (Stone and\nVeloso, 2000; Tessler et al., 2017).\nTask 0\nTask 1\nTask 2\nTask 3\nTask 4\n. . .\nAgent\nSequence\nof tasks\nFigure 10.3: Illustration of the continual learning setting where an agent has to\ninteract sequentially with related (but diﬀerent) tasks.\n10.2.3\nCurriculum learning\nA particular setting of continual learning is curriculum learning. Here,\nthe goal is to explicitly design a sequence of source tasks for an agent to\ntrain on such that the ﬁnal performance or learning speed is improved\non a target task. The idea is to start by learning small and easy aspects\nof the target task and then to gradually increase the diﬃculty level\n(Bengio et al., 2009; Narvekar et al., 2016). For instance, Florensa et al.\n(2018) use generative adversarial training to automatically generate\ngoals for a contextual policy such that they are always at the appropriate\nlevel of diﬃculty. As the diﬃculty and number of tasks increase, one\n10.3. Learning without explicit reward function\n89\npossibility to satisfy the bias-overﬁtting tradeoﬀis to consider network\ntransformations through learning.\n10.3\nLearning without explicit reward function\nIn reinforcement learning, the reward function deﬁnes the goals to be\nachieved by the agent (for a given environment and a given discount\nfactor). Due to the complexity of environments in practical applications,\ndeﬁning a reward function can turn out to be rather complicated. There\nare two other possibilities: (i) given demonstrations of the desired task,\nwe can use imitation learning or extract a reward function using inverse\nreinforcement learning; (ii) a human may provide feedback on the agent’s\nbehavior in order to deﬁne the task.\n10.3.1\nLearning from demonstrations\nIn some circumstances, the agent is only provided with trajectories of\nan expert agent (also called the teacher), without rewards. Given an\nobserved behavior, the goal is to have the agent perform similarly. Two\napproaches are possible:\n• Imitation learning uses supervised learning to map states to\nactions from the observations of the expert’s behavior (e.g., Giusti\net al., 2016). Among other applications, this approach has been\nused for self-driving cars to map raw pixels directly to steering\ncommands thanks to a deep neural network (Bojarski et al., 2016).\n• Inverse reinforcement learning (IRL) determines a possible re-\nward function given observations of optimal behavior. When the\nsystem dynamics is known (except the reward function), this is\nan appealing approach particularly when the reward function\nprovides the most generalizable deﬁnition of the task (Ng, Russell,\net al., 2000; Abbeel and Ng, 2004). For example, let us consider a\nlarge MDP for which the expert always ends up transitioning to\nthe same state. In that context, one may be able to easily infer,\nfrom only a few trajectories, what the probable goal of the task\nis (a reward function that explains the behavior of the teacher),\n90\nDeep reinforcement learning beyond MDPs\nas opposed to directly learning the policy via imitation learning,\nwhich is much less eﬃcient. Note that recent works bypass the\nrequirement of the knowledge of the system dynamics (Boularias\net al., 2011; Kalakrishnan et al., 2013; Finn et al., 2016b) by using\ntechniques based on the principle of maximum causal entropy\n(Ziebart, 2010).\nA combination of the two approaches has also been investigated by\nNeu and Szepesvári, 2012; Ho and Ermon, 2016. In particular, Ho and\nErmon, 2016 use adversarial methods to learn a discriminator (i.e.,\na reward function) such that the policy matches the distribution of\ndemonstrative samples.\nIt is important to note that in many real-world applications, the\nteacher is not exactly in the same context as the agent. Thus, transfer\nlearning may also be of crucial importance (Schulman et al., 2016; Liu\net al., 2017).\nAnother setting requires the agent to learn directly from a sequence\nof observations without corresponding actions (and possibly in a slightly\ndiﬀerent context). This may be done in a meta-learning setting by\nproviding positive reward to the agent when it performs as it is expected\nbased on the demonstration of the teacher. The agent can then act\nbased on new unseen trajectories of the teacher, with the objective that\nis can generalize suﬃciently well to perform new tasks (Paine et al.,\n2018).\n10.3.2\nLearning from direct feedback\nLearning from feedback investigates how an agent can interactively learn\nbehaviors from a human teacher who provides positive and negative\nfeedback signals. In order to learn complex behavior, human trainer\nfeedbacks has the potential to be more performant than a reward\nfunction deﬁned a priori (MacGlashan et al., 2017; Warnell et al., 2017).\nThis setting can be related to the idea of curriculum learning discussed\nin §10.2.3.\nIn the work of Hadﬁeld-Menell et al., 2016, the cooperative inverse\nreinforcement learning framework considers a two-player game between\na human and a robot interacting with an environment with the purpose\n10.4. Multi-agent systems\n91\nof maximizing the human’s reward function. In the work of Christiano\net al., 2017, it is shown how learning a separate reward model using\nsupervised learning lets us signiﬁcantly reduce the amount of feedback\nrequired from a human teacher. They also present the ﬁrst practical\napplications of using human feedback in the context of deep RL to solve\ntasks with a high dimensional observation space.\n10.4\nMulti-agent systems\nA multi-agent system is composed of multiple interacting agents within\nan environment (Littman, 1994).\nDeﬁnition 10.2. A multi-agent POMDP with N agents is a tuple\n(S, AN, . . . , AN, T, R1, . . . , RN, Ω, O1, . . . , ON, γ) where:\n• S is a ﬁnite set of states {1, . . . , NS} (describing the possible\nconﬁgurations of all agents),\n• A = A1 × . . . × An is a ﬁnite set of actions {1, . . . , NA},\n• T : S ×A×S →[0, 1] is the transition function (set of conditional\ntransition probabilities between states),\n• ∀i, Ri : S × Ai × S →R is the reward function for agent i, where\nR is a continuous set of possible rewards in a range Rmax ∈R+\n(e.g., [0, Rmax] without loss of generality),\n• Ωis a ﬁnite set of observations {1, . . . , NΩ},\n• ∀i, Oi : S × Ω→[0, 1] is a set of conditional observation\nprobabilities, and\n• γ ∈[0, 1) is the discount factor.\nFor this type of system, many diﬀerent settings can be considered.\n• Collaborative versus non-collaborative setting. In a pure collabo-\nrative setting, agents have a shared reward measurement (Ri =\nRj, ∀i, j ∈[1, . . . , N]). In a mixed or non-collaborative (possibly\nadversarial) setting each agent obtains diﬀerent rewards. In both\n92\nDeep reinforcement learning beyond MDPs\ncases, each agent i aims to maximize a discounted sum of its\nrewards PH\nt=0 γtr(i)\nt .\n• Decentralized versus centralized setting. In a decentralized set-\nting, each agent selects its own action conditioned only on its\nlocal information. When collaboration is beneﬁcial, this decentral-\nized setting can lead to the emergence of communication between\nagents in order to share information (e.g., Sukhbaatar et al., 2016).\nIn a centralized setting, the RL algorithm has access to all\nobservations w(i) and all rewards r(i). The problem can be\nreduced to a single-agent RL problem on the condition that\na single objective can be deﬁned (in a purely collaborative\nsetting, the unique objective is straightforward). Note that even\nwhen a centralized approach can be considered (depending on\nthe problem), an architecture that does not make use of the\nmulti-agent structure usually leads to sub-optimal learning (e.g.,\nSunehag et al., 2017).\nIn general, multi-agent systems are challenging because agents\nare independently updating their policies as learning progresses, and\ntherefore the environment appears non-stationary to any particular\nagent. For training one particular agent, one approach is to select\nrandomly the policies of all other agents from a pool of previously\nlearned policies. This can stabilize training of the agent that is learning\nand prevent overﬁtting to the current policy of the other agents (Silver\net al., 2016a).\nIn addition, from the perspective of a given agent, the environment is\nusually strongly stochastic even with a known, ﬁxed policy for all other\nagents. Indeed, any given agent does not know how the other agents will\nact and consequently, it doesn’t know how its own actions contribute\nto the rewards it obtains. This can be partly explained due to partial\nobservability, and partly due to the intrinsic stochasticity of the policies\nfollowed by other agents (e.g., when there is a high level of exploration).\nFor these reasons, a high variance of the expected global return is\nobserved, which makes learning challenging (particularly when used in\nconjunction with bootstrapping). In the context of the collaborative\n10.4. Multi-agent systems\n93\nsetting, a common approach is to use an actor-critic architecture with a\ncentralized critic during learning and decentralized actor (the agents can\nbe deployed independently). These topics have already been investigated\nin works by Foerster et al., 2017a; Sunehag et al., 2017; Lowe et al.,\n2017 as well as in the related work discussed in these papers. Other\nworks have shown how it is possible to take into account a term that\neither considers the learning of the other agent (Foerster et al., 2018)\nor an internal model that predicts the actions of other agents (Jaques\net al., 2018).\nDeep RL agents are able to achieve human-level performance in 3D\nmulti-player ﬁrst-person video games such as Quake III Arena Capture\nthe Flag (Jaderberg et al., 2018). Thus, techniques from deep RL have\na large potential for many real-world tasks that require multiple agents\nto cooperate in domains such as robotics, self-driving cars, etc.\n11\nPerspectives on deep reinforcement learning\nIn this section, we ﬁrst mention some of the main successes of deep\nRL. Then, we describe some of the main challenges for tackling an even\nwider range of real-world problems. Finally, we discuss some parallels\nthat can be found between deep RL and neuroscience.\n11.1\nSuccesses of deep reinforcement learning\nDeep RL techniques have demonstrated their ability to tackle a wide\nrange of problems that were previously unsolved. Some of the most\nrenowned achievements are\n• beating previous computer programs in the game of backgammon\n(Tesauro, 1995),\n• attaining superhuman-level performance in playing Atari games\nfrom the pixels (Mnih et al., 2015),\n• mastering the game of Go (Silver et al., 2016a), as well as\n• beating professional poker players in the game of heads up no-\nlimit Texas hold’em: Libratus (Brown and Sandholm, 2017) and\nDeepstack (Moravčik et al., 2017).\n94\n11.2. Challenges of applying reinforcement learning to real-world problems\n95\nThese achievements in popular games are important because they\nshow the potential of deep RL in a variety of complex and diverse\ntasks that require working with high-dimensional inputs. Deep RL has\nalso shown lots of potential for real-world applications such as robotics\n(Kalashnikov et al., 2018), self-driving cars (You et al., 2017), ﬁnance\n(Deng et al., 2017), smart grids (François-Lavet et al., 2016b), dialogue\nsystems (Fazel-Zarandi et al., 2017), etc. In fact, Deep RL systems are\nalready in production environments. For example, Gauci et al. (2018)\ndescribe how Facebook uses Deep RL such as for pushing notiﬁcations\nand for faster video loading with smart prefetching.\nRL is also applicable to ﬁelds where one could think that supervised\nlearning alone is suﬃcient, such as sequence prediction (Ranzato et al.,\n2015; Bahdanau et al., 2016). Designing the right neural architecture for\nsupervised learning tasks has also been cast as an RL problem (Zoph\nand Le, 2016). Note that those types of tasks can also be tackled with\nevolutionary strategies (Miikkulainen et al., 2017; Real et al., 2017).\nFinally, it should be mentioned that deep RL has applications in\nclassic and fundamental algorithmic problems in the ﬁeld of computer\nscience, such as the travelling salesman problem (Bello et al., 2016).\nThis is an NP-complete problem and the possibility to tackle it with\ndeep RL shows the potential impact that it could have on several other\nNP-complete problems, on the condition that the structure of these\nproblems can be exploited.\n11.2\nChallenges of applying reinforcement learning to real-world\nproblems\nThe algorithms discussed in this introduction to deep RL can, in\nprinciple, be used to solve many diﬀerent types of real-world problems. In\npractice, even in the case where the task is well deﬁned (explicit reward\nfunction), there is one fundamental diﬃculty: it is often not possible to\nlet an agent interact freely and suﬃciently in the actual environment\n(or set of environments), due to either safety, cost or time constraints.\nWe can distinguish two main cases in real-world applications:\n1. The agent may not be able to interact with the true environment\nbut only with an inaccurate simulation of it. This scenario occurs\n96\nPerspectives on deep reinforcement learning\nfor instance in robotics (Zhu et al., 2016; Gu et al., 2017a). When\nﬁrst learning in a simulation, the diﬀerence with the real-world\ndomain is known as the reality gap (see e.g. Jakobi et al., 1995).\n2. The acquisition of new observations may not be possible anymore\n(e.g., the batch setting). This scenario happens for instance in\nmedical trials, in tasks with dependence on weather conditions or\nin trading markets (e.g., energy markets and stock markets).\nNote that a combination of the two scenarios is also possible in the case\nwhere the dynamics of the environment may be simulated but where\nthere is a dependence on an exogenous time series that is only accessible\nvia limited data (François-Lavet et al., 2016b).\nIn order to deal with these limitations, diﬀerent elements are\nimportant:\n• One can aim to develop a simulator that is as accurate as possible.\n• One can design the learning algorithm so as to improve general-\nization and/or use transfer learning methods (see Chapter 7).\n11.3\nRelations between deep RL and neuroscience\nOne of the interesting aspects of deep RL is its relations to neuroscience.\nDuring the development of algorithms able to solve challenging\nsequential decision-making tasks, biological plausibility was not a\nrequirement from an engineering standpoint. However, biological\nintelligence has been a key inspiration for many of the most successful\nalgorithms. Indeed, even the ideas of reinforcement and deep learning\nhave strong links with neuroscience and biological intelligence.\nReinforcement\nIn general, RL has had a rich conceptual relationship\nto neuroscience. RL has used neuroscience as an inspiration and it\nhas also been a tool to explain neuroscience phenomena (Niv, 2009).\nRL models have also been used as a tool in the related ﬁeld of\nneuroeconomics (Camerer et al., 2005), which uses models of human\ndecision-making to inform economic analyses.\n11.3. Relations between deep RL and neuroscience\n97\nThe idea of reinforcement (or at least the term) can be traced back\nto the work of Pavlov (1927) in the context of animal behavior. In\nthe Pavlovian conditioning model, reinforcement is described as the\nstrengthening/weakening eﬀect of a behavior whenever that behavior is\npreceded by a speciﬁc stimulus. The Pavlovian conditioning model led\nto the development of the Rescorla-Wagner Theory (Rescorla, Wagner,\net al., 1972), which assumed that learning is driven by the error between\npredicted and received reward, among other prediction models. In\ncomputational RL, those concepts have been at the heart of many\ndiﬀerent algorithms, such as in the development of temporal-diﬀerence\n(TD) methods (Sutton, 1984; Schultz et al., 1997; Russek et al., 2017).\nThese connections were further strengthened when it was found that\nthe dopamine neurons in the brain act in a similar manner to TD-like\nupdates to direct learning in the brain (Schultz et al., 1997).\nDriven by such connections, many aspects of reinforcement learning\nhave also been investigated directly to explain certain phenomena in\nthe brain. For instance, computational models have been an inspiration\nto explain cognitive phenomena such as exploration (Cohen et al., 2007)\nand temporal discounting of rewards (Story et al., 2014). In cognitive\nscience, Kahneman (2011) has also described that there is a dichotomy\nbetween two modes of thoughts: a \"System 1\" that is fast and instinctive\nand a \"System 2\" that is slower and more logical. In deep reinforcement,\na similar dichotomy can be observed when we consider the model-free\nand the model-based approaches. As another example, the idea of having\na meaningful abstract representation in deep RL can also be related to\nhow animals (including humans) think. Indeed, a conscious thought at\na particular time instant can be seen as a low-dimensional combination\nof a few concepts in order to take decisions (Bengio, 2017).\nThere is a dense and rich literature about the connections between\nRL and neuroscience and, as such, the reader is referred to the work of\nSutton and Barto (2017), Niv (2009), Lee et al. (2012), Holroyd and\nColes (2002), Dayan and Niv (2008), Dayan and Daw (2008), Montague\n(2013), and Niv and Montague (2009) for an in-depth history of the\ndevelopment of reinforcement learning and its relations to neuroscience.\n98\nPerspectives on deep reinforcement learning\nDeep learning\nDeep learning also ﬁnds its origin in models of neural\nprocessing in the brain of biological entities. However, subsequent de-\nvelopments are such that deep learning has become partly incompatible\nwith current knowledge of neurobiology (Bengio et al., 2015). There\nexists nonetheless many parallels. One such example is the convolutional\nstructure used in deep learning that is inspired by the organization of\nthe animal visual cortex (Fukushima and Miyake, 1982; LeCun et al.,\n1998).\nMuch work is still needed to bridge the gap between machine learning\nand general intelligence of humans (or even animals). Looking back at all\nthe achievements obtained by taking inspiration from neuroscience, it is\nnatural to believe that further understanding of biological brains could\nplay a vital role in building more powerful algorithms and conversely.\nIn particular, we refer the reader to the survey by Hassabis et al., 2017\nwhere the bidirectional inﬂuence between deep RL and neuroscience is\ndiscussed.\n12\nConclusion\nSequential decision-making remains an active ﬁeld of research with many\ntheoretical, methodological and experimental challenges still open. The\nimportant developments in the ﬁeld of deep learning have contributed to\nmany new avenues where RL methods and deep learning are combined.\nIn particular, deep learning has brought important generalization\ncapabilities, which opens new possibilities to work with large, high-\ndimensional state and/or action spaces. There is every reason to think\nthat this development will continue in the coming years with more\neﬃcient algorithms and many new applications.\n12.1\nFuture development of deep RL\nIn deep RL, we have emphasized in this manuscript that one of the\ncentral questions is the concept of generalization. To this end, the new\ndevelopments in the ﬁeld of deep RL will surely develop the current\ntrends of taking explicit algorithms and making them diﬀerentiable\nso that they can be embedded in a speciﬁc form of neural network\nand can be trained end-to-end. This can bring algorithms with richer\nand smarter structures that would be better suited for reasoning on a\nmore abstract level, which would allow to tackle an even wider range of\n99\n100\nConclusion\napplications than they currently do today. Smart architectures could\nalso be used for hierarchical learning where much progress is still needed\nin the domain of temporal abstraction.\nWe also expect to see deep RL algorithms going in the direction of\nmeta-learning and lifelong learning where previous knowledge (e.g., in\nthe form of pre-trained networks) can be embedded so as to increase\nperformance and training time. Another key challenge is to improve\ncurrent transfer learning abilities between simulations and real-world\ncases. This would allow learning complex decision-making problems in\nsimulations (with the possibility to gather samples in a ﬂexible way), and\nthen use the learned skills in real-world environments, with applications\nin robotics, self-driving cars, etc.\nFinally, we expect deep RL techniques to develop improved curiosity\ndriven abilities to be able to better discover by themselves their\nenvironment.\n12.2\nApplications and societal impact of deep RL and artiﬁcial\nintelligence in general\nIn terms of applications, many areas are likely to be impacted by the\npossibilities brought by deep RL. It is always diﬃcult to predict the\ntimelines for the diﬀerent developments, but the current interest in deep\nRL could be the beginning of profound transformations in information\nand communication technologies, with applications in clinical decision\nsupport, marketing, ﬁnance, resource management, autonomous driving,\nrobotics, smart grids, and more.\nCurrent developments in artiﬁcial intelligence (both for deep RL or\nin general for machine learning) follows the development of many tools\nbrought by information and communications technologies. As with all\nnew technologies, this comes with diﬀerent potential opportunities and\nchallenges for our society.\nOn the positive side, algorithms based on (deep) reinforcement\nlearning promise great value to people and society. They have the\npotential to enhance the quality of life by automating tedious and\nexhausting tasks with robots (Levine et al., 2016; Gandhi et al., 2017;\nPinto et al., 2017). They may improve education by providing adaptive\n12.2. Applications and societal impact of deep RL\n101\ncontent and keeping students engaged (Mandel et al., 2014). They can\nimprove public health with, for instance, intelligent clinical decision-\nmaking (Fonteneau et al., 2008; Bennett and Hauser, 2013). They may\nprovide robust solutions to some of the self-driving cars challenges\n(Bojarski et al., 2016; You et al., 2017). They also have the possibility\nto help managing ecological resources (Dietterich, 2009) or reducing\ngreenhouse gas emissions by, e.g., optimizing traﬃc (Li et al., 2016).\nThey have applications in computer graphics, such as for character\nanimation (Peng et al., 2017b). They also have applications in ﬁnance\n(Deng et al., 2017), smart grids (François-Lavet, 2017), etc.\nHowever, we need to be careful that deep RL algorithms are safe,\nreliable and predictable (Amodei et al., 2016; Bostrom, 2017). As a\nsimple example, to capture what we want an agent to do in deep\nRL, we frequently end up, in practice, designing the reward function,\nsomewhat arbitrarily. Often this works well, but sometimes it produces\nunexpected, and potentially catastrophic behaviors. For instance, to\nremove a certain invasive species from an environment, one may design\nan agent that obtains a reward every time it removes one of these\norganisms. However, it is likely that to obtain the maximum cumulative\nrewards, the agent will learn to let that invasive species develop and\nonly then would eliminate many of the invasive organisms, which is of\ncourse not the intended behavior. All aspects related to safe exploration\nare also potential concerns in the hypothesis that deep RL algorithms\nare deployed in real-life settings.\nIn addition, as with all powerful tools, deep RL algorithms also\nbring societal and ethical challenges (Brundage et al., 2018), raising\nthe question of how they can be used for the beneﬁt of all. Even tough\ndiﬀerent interpretations can come into play when one discusses human\nsciences, we mention in this conclusion some of the potential issues that\nmay need further investigation.\nThe ethical use of artiﬁcial intelligence is a broad concern. The\nspeciﬁcity of RL as compared to supervised learning techniques is that\nit can naturally deal with sequences of interactions, which is ideal for\nchatbots, smart assistants, etc. As it is the case with most technologies,\nregulation should, at some point, ensure a positive impact of its usage.\n102\nConclusion\nIn addition, machine learning and deep RL algorithms will likely\nyield to further automation and robotisation than it is currently possible.\nThis is clearly a concern in the context of autonomous weapons, for\ninstance (Walsh, 2017). Automation also inﬂuences the economy, the\njob market and our society as a whole. A key challenge for humanity\nis to make sure that the future technological developments in artiﬁcial\nintelligence do not create an ecological crisis (Harari, 2014) or deepen the\ninequalities in our society with potential social and economic instabilities\n(Piketty, 2013).\nWe are still at the very ﬁrst steps of deep RL and artiﬁcial intelligence\nin general. The future is hard to predict; however, it is key that the\npotential issues related to the use of these algorithms are progressively\ntaken into consideration in public policies. If that is the case, these new\nalgorithms can have a positive impact on our society.\nAppendices\nA\nDeep RL frameworks\nHere is a list of some well-known frameworks used for deep RL:\n• DeeR (François-Lavet et al., 2016a) is focused on being (i) easily\naccessible and (ii) modular for researchers.\n• Dopamine (Bellemare et al., 2018) provides standard algorithms\nalong with baselines for the ATARI games.\n• ELF (Tian et al., 2017) is a research platform for deep RL, aimed\nmainly to real-time strategy games.\n• OpenAI baselines (Dhariwal et al., 2017) is a set of popular deep\nRL algorithms, including DDPG, TRPO, PPO, ACKTR. The\nfocus of this framework is to provide implementations of baselines.\n• PyBrain (Schaul et al., 2010) is a machine learning library with\nsome RL support.\n• rllab (Duan et al., 2016a) provides a set of benchmarked\nimplementations of deep RL algorithms.\n• TensorForce (Schaarschmidt et al., 2017) is a framework for deep\nRL built around Tensorﬂow with several algorithm implemen-\ntations. It aims at moving reinforcement computations into the\nTensorﬂow graph for performance gains and eﬃciency. As such, it\nis heavily tied to the Tensorﬂow deep learning library. It provides\nmany algorithm implementations including TRPO, DQN, PPO,\nand A3C.\nEven though, they are not tailored speciﬁcally for deep RL, we can\nalso cite the two following frameworks for reinforcement learning:\n• RL-Glue (Tanner and White, 2009) provides a standard interface\nthat allows to connect RL agents, environments, and experiment\nprograms together.\n• RLPy (Geramifard et al., 2015) is a framework focused on value-\nbased RL using linear function approximators with discrete\nactions.\nTable 1 provides a summary of some properties of the aforementioned\nlibraries.\nFramework\nDeep RL\nPython\ninterface\nAutomatic\nGPU support\nDeeR\nyes\nyes\nyes\nDopamine\nyes\nyes\nyes\nELF\nyes\nno\nyes\nOpenAI baselines\nyes\nyes\nyes\nPyBrain\nyes\nyes\nno\nRL-Glue\nno\nyes\nno\nRLPy\nno\nyes\nno\nrllab\nyes\nyes\nyes\nTensorForce\nyes\nyes\nyes\nTable 1: Summary of some characteristics for a few existing RL frameworks.\nReferences\nAbadi, M., A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro,\nG. S. Corrado, A. Davis, J. Dean, M. Devin, et al. 2016. “Tensor-\nFlow: Large-Scale Machine Learning on Heterogeneous Distributed\nSystems”. arXiv preprint arXiv:1603.04467.\nAbbeel, P. and A. Y. Ng. 2004. “Apprenticeship learning via inverse rein-\nforcement learning”. In: Proceedings of the twenty-ﬁrst international\nconference on Machine learning. ACM. 1.\nAmari, S. 1998. “Natural Gradient Works Eﬃciently in Learning”.\nNeural Computation. 10(2): 251–276.\nAmodei, D., C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and\nD. Mané. 2016. “Concrete problems in AI safety”. arXiv preprint\narXiv:1606.06565.\nAnderson, T. W., T. W. Anderson, T. W. Anderson, T. W. Anderson,\nand E.-U. Mathématicien. 1958. An introduction to multivariate\nstatistical analysis. Vol. 2. Wiley New York.\nAytar, Y., T. Pfaﬀ, D. Budden, T. L. Paine, Z. Wang, and N. de Freitas.\n2018. “Playing hard exploration games by watching YouTube”. arXiv\npreprint arXiv:1805.11592.\nBacon, P.-L., J. Harb, and D. Precup. 2016. “The option-critic\narchitecture”. arXiv preprint arXiv:1609.05140.\n106\nBahdanau, D., P. Brakel, K. Xu, A. Goyal, R. Lowe, J. Pineau, A.\nCourville, and Y. Bengio. 2016. “An actor-critic algorithm for\nsequence prediction”. arXiv preprint arXiv:1607.07086.\nBaird, L. 1995. “Residual algorithms: Reinforcement learning with\nfunction approximation”. In: ICML. 30–37.\nBaker, M. 2016. “1,500 scientists lift the lid on reproducibility”. Nature\nNews. 533(7604): 452.\nBartlett, P. L. and S. Mendelson. 2002. “Rademacher and Gaussian\ncomplexities: Risk bounds and structural results”. Journal of\nMachine Learning Research. 3(Nov): 463–482.\nBarto, A. G., R. S. Sutton, and C. W. Anderson. 1983. “Neuronlike\nadaptive elements that can solve diﬃcult learning control problems”.\nIEEE transactions on systems, man, and cybernetics. (5): 834–846.\nBeattie, C., J. Z. Leibo, D. Teplyashin, T. Ward, M. Wainwright, H.\nKüttler, A. Lefrancq, S. Green, V. Valdés, A. Sadik, et al. 2016.\n“DeepMind Lab”. arXiv preprint arXiv:1612.03801.\nBellemare, M. G., P. S. Castro, C. Gelada, K. Saurabh, and S. Moitra.\n2018. “Dopamine”. https://github.com/google/dopamine.\nBellemare, M. G., W. Dabney, and R. Munos. 2017. “A distri-\nbutional perspective on reinforcement learning”. arXiv preprint\narXiv:1707.06887.\nBellemare, M. G., Y. Naddaf, J. Veness, and M. Bowling. 2013. “The\nArcade Learning Environment: An evaluation platform for general\nagents.” Journal of Artiﬁcial Intelligence Research. 47: 253–279.\nBellemare, M. G., S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and\nR. Munos. 2016. “Unifying Count-Based Exploration and Intrinsic\nMotivation”. arXiv preprint arXiv:1606.01868.\nBellman, R. 1957a. “A Markovian decision process”. Journal of\nMathematics and Mechanics: 679–684.\nBellman, R. 1957b. “Dynamic Programming”.\nBellman, R. E. and S. E. Dreyfus. 1962. “Applied dynamic program-\nming”.\nBello, I., H. Pham, Q. V. Le, M. Norouzi, and S. Bengio. 2016. “Neural\nCombinatorial Optimization with Reinforcement Learning”. arXiv\npreprint arXiv:1611.09940.\nBengio,\nY.\n2017.\n“The\nConsciousness\nPrior”.\narXiv\npreprint\narXiv:1709.08568.\nBengio, Y., D.-H. Lee, J. Bornschein, T. Mesnard, and Z. Lin. 2015.\n“Towards biologically plausible deep learning”. arXiv preprint\narXiv:1502.04156.\nBengio, Y., J. Louradour, R. Collobert, and J. Weston. 2009. “Cur-\nriculum learning”. In: Proceedings of the 26th annual international\nconference on machine learning. ACM. 41–48.\nBennett, C. C. and K. Hauser. 2013. “Artiﬁcial intelligence framework\nfor simulating clinical decision-making: A Markov decision process\napproach”. Artiﬁcial intelligence in medicine. 57(1): 9–19.\nBertsekas, D. P., D. P. Bertsekas, D. P. Bertsekas, and D. P. Bertsekas.\n1995. Dynamic programming and optimal control. Vol. 1. No. 2.\nAthena scientiﬁc Belmont, MA.\nBojarski, M., D. Del Testa, D. Dworakowski, B. Firner, B. Flepp,\nP. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang, et al.\n2016. “End to end learning for self-driving cars”. arXiv preprint\narXiv:1604.07316.\nBostrom, N. 2017. Superintelligence. Dunod.\nBouckaert, R. R. 2003. “Choosing between two learning algorithms\nbased on calibrated tests”. In: Proceedings of the 20th International\nConference on Machine Learning (ICML-03). 51–58.\nBouckaert, R. R. and E. Frank. 2004. “Evaluating the replicability of\nsigniﬁcance tests for comparing learning algorithms”. In: PAKDD.\nSpringer. 3–12.\nBoularias, A., J. Kober, and J. Peters. 2011. “Relative Entropy Inverse\nReinforcement Learning.” In: AISTATS. 182–189.\nBoyan, J. A. and A. W. Moore. 1995. “Generalization in reinforcement\nlearning: Safely approximating the value function”. In: Advances in\nneural information processing systems. 369–376.\nBrafman, R. I. and M. Tennenholtz. 2003. “R-max-a general polynomial\ntime algorithm for near-optimal reinforcement learning”. The\nJournal of Machine Learning Research. 3: 213–231.\nBranavan, S., N. Kushman, T. Lei, and R. Barzilay. 2012. “Learning\nhigh-level planning from text”. In: Proceedings of the 50th Annual\nMeeting of the Association for Computational Linguistics: Long\nPapers-Volume 1. Association for Computational Linguistics. 126–\n135.\nBraziunas, D. 2003. “POMDP solution methods”. University of Toronto,\nTech. Rep.\nBrockman, G., V. Cheung, L. Pettersson, J. Schneider, J. Schulman,\nJ. Tang, and W. Zaremba. 2016. “OpenAI Gym”.\nBrown, N. and T. Sandholm. 2017. “Libratus: The Superhuman AI\nfor No-Limit Poker”. International Joint Conference on Artiﬁcial\nIntelligence (IJCAI-17).\nBrowne, C. B., E. Powley, D. Whitehouse, S. M. Lucas, P. I. Cowling,\nP. Rohlfshagen, S. Tavener, D. Perez, S. Samothrakis, and S.\nColton. 2012. “A survey of monte carlo tree search methods”. IEEE\nTransactions on Computational Intelligence and AI in games. 4(1):\n1–43.\nBrügmann, B. 1993. “Monte carlo go”. Tech. rep. Citeseer.\nBrundage, M., S. Avin, J. Clark, H. Toner, P. Eckersley, B. Garﬁnkel,\nA. Dafoe, P. Scharre, T. Zeitzoﬀ, B. Filar, et al. 2018. “The\nMalicious Use of Artiﬁcial Intelligence: Forecasting, Prevention,\nand Mitigation”. arXiv preprint arXiv:1802.07228.\nBrys, T., A. Harutyunyan, P. Vrancx, M. E. Taylor, D. Kudenko, and\nA. Nowé. 2014. “Multi-objectivization of reinforcement learning\nproblems by reward shaping”. In: Neural Networks (IJCNN), 2014\nInternational Joint Conference on. IEEE. 2315–2322.\nBubeck, S., R. Munos, and G. Stoltz. 2011. “Pure exploration in\nﬁnitely-armed and continuous-armed bandits”. Theoretical Computer\nScience. 412(19): 1832–1852.\nBurda, Y., H. Edwards, A. Storkey, and O. Klimov. 2018. “Exploration\nby Random Network Distillation”. arXiv preprint arXiv:1810.12894.\nCamerer, C., G. Loewenstein, and D. Prelec. 2005. “Neuroeconomics:\nHow neuroscience can inform economics”. Journal of economic\nLiterature. 43(1): 9–64.\nCampbell, M., A. J. Hoane, and F.-h. Hsu. 2002. “Deep blue”. Artiﬁcial\nintelligence. 134(1-2): 57–83.\nCasadevall, A. and F. C. Fang. 2010. “Reproducible science”.\nCastronovo, M., V. François-Lavet, R. Fonteneau, D. Ernst, and A.\nCouëtoux. 2017. “Approximate Bayes Optimal Policy Search using\nNeural Networks”. In: 9th International Conference on Agents and\nArtiﬁcial Intelligence (ICAART 2017).\nChebotar, Y., A. Handa, V. Makoviychuk, M. Macklin, J. Issac, N.\nRatliﬀ, and D. Fox. 2018. “Closing the Sim-to-Real Loop: Adapting\nSimulation Randomization with Real World Experience”. arXiv\npreprint arXiv:1810.05687.\nChen, T., I. Goodfellow, and J. Shlens. 2015. “Net2net: Accelerating\nlearning via knowledge transfer”. arXiv preprint arXiv:1511.05641.\nChen, X., C. Liu, and D. Song. 2017. “Learning Neural Programs To\nParse Programs”. arXiv preprint arXiv:1706.01284.\nChiappa, S., S. Racaniere, D. Wierstra, and S. Mohamed. 2017. “Recur-\nrent Environment Simulators”. arXiv preprint arXiv:1704.02254.\nChristiano, P., J. Leike, T. B. Brown, M. Martic, S. Legg, and D. Amodei.\n2017. “Deep reinforcement learning from human preferences”. arXiv\npreprint arXiv:1706.03741.\nChristopher, M. B. 2006. Pattern recognition and machine learning.\nSpringer.\nCohen, J. D., S. M. McClure, and J. Y. Angela. 2007. “Should I stay or\nshould I go? How the human brain manages the trade-oﬀbetween\nexploitation and exploration”. Philosophical Transactions of the\nRoyal Society of London B: Biological Sciences. 362(1481): 933–942.\nCortes, C. and V. Vapnik. 1995. “Support-vector networks”. Machine\nlearning. 20(3): 273–297.\nCoumans, E., Y. Bai, et al. 2016. “Bullet”. http://pybullet.org/.\nDa Silva, B., G. Konidaris, and A. Barto. 2012. “Learning parameterized\nskills”. arXiv preprint arXiv:1206.6398.\nDabney, W., M. Rowland, M. G. Bellemare, and R. Munos. 2017.\n“Distributional Reinforcement Learning with Quantile Regression”.\narXiv preprint arXiv:1710.10044.\nDayan, P. and N. D. Daw. 2008. “Decision theory, reinforcement learning,\nand the brain”. Cognitive, Aﬀective, & Behavioral Neuroscience.\n8(4): 429–453.\nDayan, P. and Y. Niv. 2008. “Reinforcement learning: the good, the\nbad and the ugly”. Current opinion in neurobiology. 18(2): 185–196.\nDearden, R., N. Friedman, and D. Andre. 1999. “Model based\nBayesian exploration”. In: Proceedings of the Fifteenth conference on\nUncertainty in artiﬁcial intelligence. Morgan Kaufmann Publishers\nInc. 150–159.\nDearden, R., N. Friedman, and S. Russell. 1998. “Bayesian Q-learning”.\nDeisenroth, M. and C. E. Rasmussen. 2011. “PILCO: A model-based\nand data-eﬃcient approach to policy search”. In: Proceedings of\nthe 28th International Conference on machine learning (ICML-11).\n465–472.\nDemšar, J. 2006. “Statistical comparisons of classiﬁers over multiple\ndata sets”. Journal of Machine learning research. 7(Jan): 1–30.\nDeng, Y., F. Bao, Y. Kong, Z. Ren, and Q. Dai. 2017. “Deep\ndirect reinforcement learning for ﬁnancial signal representation\nand trading”. IEEE transactions on neural networks and learning\nsystems. 28(3): 653–664.\nDhariwal, P., C. Hesse, M. Plappert, A. Radford, J. Schulman, S. Sidor,\nand Y. Wu. 2017. “OpenAI Baselines”.\nDietterich, T. G. 1998. “Approximate statistical tests for comparing\nsupervised classiﬁcation learning algorithms”. Neural computation.\n10(7): 1895–1923.\nDietterich, T. G. 2009. “Machine learning and ecosystem informatics:\nchallenges and opportunities”. In: Asian Conference on Machine\nLearning. Springer. 1–5.\nDinculescu, M. and D. Precup. 2010. “Approximate predictive represen-\ntations of partially observable systems”. In: Proceedings of the 27th\nInternational Conference on Machine Learning (ICML-10). 895–902.\nDosovitskiy, A. and V. Koltun. 2016. “Learning to act by predicting\nthe future”. arXiv preprint arXiv:1611.01779.\nDuan, Y., M. Andrychowicz, B. Stadie, J. Ho, J. Schneider, I. Sutskever,\nP. Abbeel, and W. Zaremba. 2017. “One-Shot Imitation Learning”.\narXiv preprint arXiv:1703.07326.\nDuan, Y., X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. 2016a.\n“Benchmarking deep reinforcement learning for continuous control”.\nIn: International Conference on Machine Learning. 1329–1338.\nDuan, Y., J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and\nP. Abbeel. 2016b. “RLˆ2: Fast Reinforcement Learning via Slow\nReinforcement Learning”. arXiv preprint arXiv:1611.02779.\nDuchesne, L., E. Karangelos, and L. Wehenkel. 2017. “Machine learning\nof real-time power systems reliability management response”.\nPowerTech Manchester 2017 Proceedings.\nDžeroski, S., L. De Raedt, and K. Driessens. 2001. “Relational\nreinforcement learning”. Machine learning. 43(1-2): 7–52.\nErhan, D., Y. Bengio, A. Courville, and P. Vincent. 2009. “Visualizing\nhigher-layer features of a deep network”. University of Montreal.\n1341(3): 1.\nErnst, D., P. Geurts, and L. Wehenkel. 2005. “Tree-based batch mode\nreinforcement learning”. In: Journal of Machine Learning Research.\n503–556.\nFarquhar, G., T. Rocktäschel, M. Igl, and S. Whiteson. 2017. “TreeQN\nand ATreeC: Diﬀerentiable Tree Planning for Deep Reinforcement\nLearning”. arXiv preprint arXiv:1710.11417.\nFazel-Zarandi, M., S.-W. Li, J. Cao, J. Casale, P. Henderson, D. Whitney,\nand A. Geramifard. 2017. “Learning Robust Dialog Policies in Noisy\nEnvironments”. arXiv preprint arXiv:1712.04034.\nFinn, C., P. Abbeel, and S. Levine. 2017. “Model-agnostic meta-\nlearning for fast adaptation of deep networks”. arXiv preprint\narXiv:1703.03400.\nFinn, C., I. Goodfellow, and S. Levine. 2016a. “Unsupervised learning\nfor physical interaction through video prediction”. In: Advances In\nNeural Information Processing Systems. 64–72.\nFinn, C., S. Levine, and P. Abbeel. 2016b. “Guided cost learning: Deep\ninverse optimal control via policy optimization”. In: Proceedings of\nthe 33rd International Conference on Machine Learning. Vol. 48.\nFlorensa, C., Y. Duan, and P. Abbeel. 2017. “Stochastic neural\nnetworks for hierarchical reinforcement learning”. arXiv preprint\narXiv:1704.03012.\nFlorensa, C., D. Held, X. Geng, and P. Abbeel. 2018. “Automatic\ngoal generation for reinforcement learning agents”. In: International\nConference on Machine Learning. 1514–1523.\nFoerster, J., R. Y. Chen, M. Al-Shedivat, S. Whiteson, P. Abbeel, and\nI. Mordatch. 2018. “Learning with opponent-learning awareness”. In:\nProceedings of the 17th International Conference on Autonomous\nAgents and MultiAgent Systems. International Foundation for\nAutonomous Agents and Multiagent Systems. 122–130.\nFoerster, J., G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson.\n2017a. “Counterfactual Multi-Agent Policy Gradients”. arXiv\npreprint arXiv:1705.08926.\nFoerster, J., N. Nardelli, G. Farquhar, P. Torr, P. Kohli, S. Whiteson,\net al. 2017b. “Stabilising experience replay for deep multi-agent\nreinforcement learning”. arXiv preprint arXiv:1702.08887.\nFonteneau, R., S. A. Murphy, L. Wehenkel, and D. Ernst. 2013. “Batch\nmode reinforcement learning based on the synthesis of artiﬁcial\ntrajectories”. Annals of operations research. 208(1): 383–416.\nFonteneau, R., L. Wehenkel, and D. Ernst. 2008. “Variable selection for\ndynamic treatment regimes: a reinforcement learning approach”.\nFortunato, M., M. G. Azar, B. Piot, J. Menick, I. Osband, A. Graves,\nV. Mnih, R. Munos, D. Hassabis, O. Pietquin, et al. 2017. “Noisy\nnetworks for exploration”. arXiv preprint arXiv:1706.10295.\nFox, R., A. Pakman, and N. Tishby. 2015. “Taming the noise in reinforce-\nment learning via soft updates”. arXiv preprint arXiv:1512.08562.\nFrançois-Lavet, V. et al. 2016a. “DeeR”. https://deer.readthedocs.io/.\nFrançois-Lavet, V. 2017. “Contributions to deep reinforcement learning\nand its applications in smartgrids”. PhD thesis. University of Liege,\nBelgium.\nFrançois-Lavet, V., Y. Bengio, D. Precup, and J. Pineau. 2018.\n“Combined Reinforcement Learning via Abstract Representations”.\narXiv preprint arXiv:1809.04506.\nFrançois-Lavet, V., D. Ernst, and F. Raphael. 2017. “On overﬁtting\nand asymptotic bias in batch reinforcement learning with partial\nobservability”. arXiv preprint arXiv:1709.07796.\nFrançois-Lavet, V., R. Fonteneau, and D. Ernst. 2015. “How to Discount\nDeep Reinforcement Learning: Towards New Dynamic Strategies”.\narXiv preprint arXiv:1512.02011.\nFrançois-Lavet, V., D. Taralla, D. Ernst, and R. Fonteneau. 2016b.\n“Deep Reinforcement Learning Solutions for Energy Microgrids\nManagement”. In: European Workshop on Reinforcement Learning.\nFukushima, K. and S. Miyake. 1982. “Neocognitron: A self-organizing\nneural network model for a mechanism of visual pattern recognition”.\nIn: Competition and cooperation in neural nets. Springer. 267–285.\nGal, Y. and Z. Ghahramani. 2016. “Dropout as a Bayesian Approx-\nimation: Representing Model Uncertainty in Deep Learning”. In:\nProceedings of the 33nd International Conference on Machine Learn-\ning, ICML 2016, New York City, NY, USA, June 19-24, 2016. 1050–\n1059.\nGandhi, D., L. Pinto, and A. Gupta. 2017. “Learning to Fly by Crashing”.\narXiv preprint arXiv:1704.05588.\nGarnelo, M., K. Arulkumaran, and M. Shanahan. 2016. “To-\nwards Deep Symbolic Reinforcement Learning”. arXiv preprint\narXiv:1609.05518.\nGauci, J., E. Conti, Y. Liang, K. Virochsiri, Y. He, Z. Kaden,\nV. Narayanan, and X. Ye. 2018. “Horizon: Facebook’s Open\nSource Applied Reinforcement Learning Platform”. arXiv preprint\narXiv:1811.00260.\nGelly, S., Y. Wang, R. Munos, and O. Teytaud. 2006. “Modiﬁcation of\nUCT with patterns in Monte-Carlo Go”.\nGeman, S., E. Bienenstock, and R. Doursat. 1992. “Neural networks\nand the bias/variance dilemma”. Neural computation. 4(1): 1–58.\nGeramifard, A., C. Dann, R. H. Klein, W. Dabney, and J. P. How. 2015.\n“RLPy: A Value-Function-Based Reinforcement Learning Framework\nfor Education and Research”. Journal of Machine Learning Research.\n16: 1573–1578.\nGeurts, P., D. Ernst, and L. Wehenkel. 2006. “Extremely randomized\ntrees”. Machine learning. 63(1): 3–42.\nGhavamzadeh, M., S. Mannor, J. Pineau, A. Tamar, et al. 2015.\n“Bayesian reinforcement learning: A survey”. Foundations and\nTrends® in Machine Learning. 8(5-6): 359–483.\nGiusti, A., J. Guzzi, D. C. Cireşan, F.-L. He, J. P. Rodriguez, F. Fontana,\nM. Faessler, C. Forster, J. Schmidhuber, G. Di Caro, et al. 2016.\n“A machine learning approach to visual perception of forest trails\nfor mobile robots”. IEEE Robotics and Automation Letters. 1(2):\n661–667.\nGoodfellow, I., Y. Bengio, and A. Courville. 2016. Deep learning. MIT\nPress.\nGoodfellow, I., J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio. 2014. “Generative adversarial\nnets”. In: Advances in neural information processing systems. 2672–\n2680.\nGordon, G. J. 1996. “Stable ﬁtted reinforcement learning”. In: Advances\nin neural information processing systems. 1052–1058.\nGordon, G. J. 1999. “Approximate solutions to Markov decision\nprocesses”. Robotics Institute: 228.\nGraves, A., G. Wayne, and I. Danihelka. 2014. “Neural turing machines”.\narXiv preprint arXiv:1410.5401.\nGregor, K., D. J. Rezende, and D. Wierstra. 2016. “Variational Intrinsic\nControl”. arXiv preprint arXiv:1611.07507.\nGruslys, A., M. G. Azar, M. G. Bellemare, and R. Munos. 2017.\n“The Reactor: A Sample-Eﬃcient Actor-Critic Architecture”. arXiv\npreprint arXiv:1704.04651.\nGu, S., E. Holly, T. Lillicrap, and S. Levine. 2017a. “Deep reinforcement\nlearning for robotic manipulation with asynchronous oﬀ-policy\nupdates”. In: Robotics and Automation (ICRA), 2017 IEEE\nInternational Conference on. IEEE. 3389–3396.\nGu, S., T. Lillicrap, Z. Ghahramani, R. E. Turner, and S. Levine. 2016a.\n“Q-prop: Sample-eﬃcient policy gradient with an oﬀ-policy critic”.\narXiv preprint arXiv:1611.02247.\nGu, S., T. Lillicrap, Z. Ghahramani, R. E. Turner, and S. Levine. 2017b.\n“Q-Prop: Sample-Eﬃcient Policy Gradient with An Oﬀ-Policy Critic”.\nIn: 5th International Conference on Learning Representations (ICLR\n2017).\nGu, S., T. Lillicrap, Z. Ghahramani, R. E. Turner, B. Schölkopf, and S.\nLevine. 2017c. “Interpolated Policy Gradient: Merging On-Policy and\nOﬀ-Policy Gradient Estimation for Deep Reinforcement Learning”.\narXiv preprint arXiv:1706.00387.\nGu, S., T. Lillicrap, I. Sutskever, and S. Levine. 2016b. “Continuous\nDeep Q-Learning with Model-based Acceleration”. arXiv preprint\narXiv:1603.00748.\nGuo, Z. D. and E. Brunskill. 2017. “Sample eﬃcient feature selection\nfor factored mdps”. arXiv preprint arXiv:1703.03454.\nHaarnoja, T., H. Tang, P. Abbeel, and S. Levine. 2017. “Reinforce-\nment learning with deep energy-based policies”. arXiv preprint\narXiv:1702.08165.\nHaber, N., D. Mrowca, L. Fei-Fei, and D. L. Yamins. 2018. “Learning to\nPlay with Intrinsically-Motivated Self-Aware Agents”. arXiv preprint\narXiv:1802.07442.\nHadﬁeld-Menell, D., S. J. Russell, P. Abbeel, and A. Dragan. 2016.\n“Cooperative inverse reinforcement learning”. In: Advances in neural\ninformation processing systems. 3909–3917.\nHafner, R. and M. Riedmiller. 2011. “Reinforcement learning in feedback\ncontrol”. Machine learning. 84(1-2): 137–169.\nHalsey, L. G., D. Curran-Everett, S. L. Vowler, and G. B. Drummond.\n2015. “The ﬁckle P value generates irreproducible results”. Nature\nmethods. 12(3): 179–185.\nHarari, Y. N. 2014. Sapiens: A brief history of humankind.\nHarutyunyan, A., M. G. Bellemare, T. Stepleton, and R. Munos.\n2016. “Q (\\lambda) with Oﬀ-Policy Corrections”. In: International\nConference on Algorithmic Learning Theory. Springer. 305–320.\nHassabis, D., D. Kumaran, C. Summerﬁeld, and M. Botvinick. 2017.\n“Neuroscience-inspired artiﬁcial intelligence”. Neuron. 95(2): 245–\n258.\nHasselt, H. V. 2010. “Double Q-learning”. In: Advances in Neural\nInformation Processing Systems. 2613–2621.\nHausknecht, M. and P. Stone. 2015. “Deep recurrent Q-learning for\npartially observable MDPs”. arXiv preprint arXiv:1507.06527.\nHauskrecht, M., N. Meuleau, L. P. Kaelbling, T. Dean, and C. Boutilier.\n1998. “Hierarchical solution of Markov decision processes using\nmacro-actions”. In: Proceedings of the Fourteenth conference on\nUncertainty in artiﬁcial intelligence. Morgan Kaufmann Publishers\nInc. 220–229.\nHe, K., X. Zhang, S. Ren, and J. Sun. 2016. “Deep residual learning\nfor image recognition”. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition. 770–778.\nHeess, N., G. Wayne, D. Silver, T. Lillicrap, T. Erez, and Y. Tassa. 2015.\n“Learning continuous control policies by stochastic value gradients”.\nIn: Advances in Neural Information Processing Systems. 2944–2952.\nHenderson, P., W.-D. Chang, F. Shkurti, J. Hansen, D. Meger, and G.\nDudek. 2017a. “Benchmark Environments for Multitask Learning in\nContinuous Domains”. ICML Lifelong Learning: A Reinforcement\nLearning Approach Workshop.\nHenderson, P., R. Islam, P. Bachman, J. Pineau, D. Precup, and D.\nMeger. 2017b. “Deep Reinforcement Learning that Matters”. arXiv\npreprint arXiv:1709.06560.\nHessel, M., J. Modayil, H. van Hasselt, T. Schaul, G. Ostrovski, W.\nDabney, D. Horgan, B. Piot, M. Azar, and D. Silver. 2017. “Rainbow:\nCombining Improvements in Deep Reinforcement Learning”. arXiv\npreprint arXiv:1710.02298.\nHessel, M., H. Soyer, L. Espeholt, W. Czarnecki, S. Schmitt, and H.\nvan Hasselt. 2018. “Multi-task Deep Reinforcement Learning with\nPopArt”. arXiv preprint arXiv:1809.04474.\nHiggins, I., A. Pal, A. A. Rusu, L. Matthey, C. P. Burgess, A.\nPritzel, M. Botvinick, C. Blundell, and A. Lerchner. 2017. “Darla:\nImproving zero-shot transfer in reinforcement learning”. arXiv\npreprint arXiv:1707.08475.\nHo, J. and S. Ermon. 2016. “Generative adversarial imitation learning”.\nIn: Advances in Neural Information Processing Systems. 4565–4573.\nHochreiter, S. and J. Schmidhuber. 1997. “Long short-term memory”.\nNeural computation. 9(8): 1735–1780.\nHochreiter, S., A. S. Younger, and P. R. Conwell. 2001. “Learning\nto learn using gradient descent”. In: International Conference on\nArtiﬁcial Neural Networks. Springer. 87–94.\nHolroyd, C. B. and M. G. Coles. 2002. “The neural basis of human error\nprocessing: reinforcement learning, dopamine, and the error-related\nnegativity.” Psychological review. 109(4): 679.\nHouthooft, R., X. Chen, Y. Duan, J. Schulman, F. De Turck,\nand P. Abbeel. 2016. “Vime: Variational information maximizing\nexploration”. In: Advances in Neural Information Processing Systems.\n1109–1117.\nIoﬀe, S. and C. Szegedy. 2015. “Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift”. arXiv preprint\narXiv:1502.03167.\nIslam, R., P. Henderson, M. Gomrokchi, and D. Precup. 2017.\n“Reproducibility of Benchmarked Deep Reinforcement Learning\nTasks for Continuous Control”. ICML Reproducibility in Machine\nLearning Workshop.\nJaderberg, M., V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D.\nSilver, and K. Kavukcuoglu. 2016. “Reinforcement learning with\nunsupervised auxiliary tasks”. arXiv preprint arXiv:1611.05397.\nJaderberg, M., W. M. Czarnecki, I. Dunning, L. Marris, G. Lever,\nA. G. Castaneda, C. Beattie, N. C. Rabinowitz, A. S. Morcos,\nA. Ruderman, et al. 2018. “Human-level performance in ﬁrst-\nperson multiplayer games with population-based deep reinforcement\nlearning”. arXiv preprint arXiv:1807.01281.\nJakobi, N., P. Husbands, and I. Harvey. 1995. “Noise and the reality\ngap: The use of simulation in evolutionary robotics”. In: European\nConference on Artiﬁcial Life. Springer. 704–720.\nJames, G. M. 2003. “Variance and bias for general loss functions”.\nMachine Learning. 51(2): 115–135.\nJaques, N., A. Lazaridou, E. Hughes, C. Gulcehre, P. A. Ortega, D.\nStrouse, J. Z. Leibo, and N. de Freitas. 2018. “Intrinsic Social\nMotivation via Causal Inﬂuence in Multi-Agent RL”. arXiv preprint\narXiv:1810.08647.\nJaquette, S. C. et al. 1973. “Markov decision processes with a new\noptimality criterion: Discrete time”. The Annals of Statistics. 1(3):\n496–505.\nJiang, N., A. Kulesza, and S. Singh. 2015a. “Abstraction selection in\nmodel-based reinforcement learning”. In: Proceedings of the 32nd\nInternational Conference on Machine Learning (ICML-15). 179–188.\nJiang, N., A. Kulesza, S. Singh, and R. Lewis. 2015b. “The Dependence\nof Eﬀective Planning Horizon on Model Accuracy”. In: Proceedings\nof the 2015 International Conference on Autonomous Agents and\nMultiagent Systems. International Foundation for Autonomous\nAgents and Multiagent Systems. 1181–1189.\nJiang, N. and L. Li. 2016. “Doubly robust oﬀ-policy value evaluation for\nreinforcement learning”. In: Proceedings of The 33rd International\nConference on Machine Learning. 652–661.\nJohnson, J., B. Hariharan, L. van der Maaten, J. Hoﬀman, L. Fei-\nFei, C. L. Zitnick, and R. Girshick. 2017. “Inferring and Executing\nPrograms for Visual Reasoning”. arXiv preprint arXiv:1705.03633.\nJohnson, M., K. Hofmann, T. Hutton, and D. Bignell. 2016. “The\nMalmo Platform for Artiﬁcial Intelligence Experimentation.” In:\nIJCAI. 4246–4247.\nJuliani, A., V.-P. Berges, E. Vckay, Y. Gao, H. Henry, M. Mattar, and\nD. Lange. 2018. “Unity: A General Platform for Intelligent Agents”.\narXiv preprint arXiv:1809.02627.\nKaelbling, L. P., M. L. Littman, and A. R. Cassandra. 1998. “Planning\nand acting in partially observable stochastic domains”. Artiﬁcial\nintelligence. 101(1): 99–134.\nKahneman, D. 2011. Thinking, fast and slow. Macmillan.\nKakade, S. 2001. “A Natural Policy Gradient”. In: Advances in Neural\nInformation Processing Systems 14 [Neural Information Processing\nSystems: Natural and Synthetic, NIPS 2001, December 3-8, 2001,\nVancouver, British Columbia, Canada]. 1531–1538.\nKakade, S., M. Kearns, and J. Langford. 2003. “Exploration in metric\nstate spaces”. In: ICML. Vol. 3. 306–312.\nKalakrishnan, M., P. Pastor, L. Righetti, and S. Schaal. 2013. “Learning\nobjective functions for manipulation”. In: Robotics and Automation\n(ICRA), 2013 IEEE International Conference on. IEEE. 1331–1336.\nKalashnikov, D., A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D.\nQuillen, E. Holly, M. Kalakrishnan, V. Vanhoucke, and S. Levine.\n2018. “Qt-opt: Scalable deep reinforcement learning for vision-based\nrobotic manipulation”. arXiv preprint arXiv:1806.10293.\nKalchbrenner, N., A. v. d. Oord, K. Simonyan, I. Danihelka, O. Vinyals,\nA. Graves, and K. Kavukcuoglu. 2016. “Video pixel networks”. arXiv\npreprint arXiv:1610.00527.\nKansky, K., T. Silver, D. A. Mély, M. Eldawy, M. Lázaro-Gredilla,\nX. Lou, N. Dorfman, S. Sidor, S. Phoenix, and D. George. 2017.\n“Schema Networks: Zero-shot Transfer with a Generative Causal\nModel of Intuitive Physics”. arXiv preprint arXiv:1706.04317.\nKaplan, R., C. Sauer, and A. Sosa. 2017. “Beating Atari with\nNatural Language Guided Reinforcement Learning”. arXiv preprint\narXiv:1704.05539.\nKearns, M. and S. Singh. 2002. “Near-optimal reinforcement learning\nin polynomial time”. Machine Learning. 49(2-3): 209–232.\nKempka, M., M. Wydmuch, G. Runc, J. Toczek, and W. Jaśkowski.\n2016. “Vizdoom: A doom-based ai research platform for visual\nreinforcement learning”. In: Computational Intelligence and Games\n(CIG), 2016 IEEE Conference on. IEEE. 1–8.\nKirkpatrick, J., R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins,\nA. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska,\net al. 2016. “Overcoming catastrophic forgetting in neural networks”.\narXiv preprint arXiv:1612.00796.\nKlambauer, G., T. Unterthiner, A. Mayr, and S. Hochreiter. 2017. “Self-\nNormalizing Neural Networks”. arXiv preprint arXiv:1706.02515.\nKolter, J. Z. and A. Y. Ng. 2009. “Near-Bayesian exploration in\npolynomial time”. In: Proceedings of the 26th Annual International\nConference on Machine Learning. ACM. 513–520.\nKonda, V. R. and J. N. Tsitsiklis. 2000. “Actor-critic algorithms”. In:\nAdvances in neural information processing systems. 1008–1014.\nKrizhevsky, A., I. Sutskever, and G. E. Hinton. 2012. “Imagenet\nclassiﬁcation with deep convolutional neural networks”. In: Advances\nin neural information processing systems. 1097–1105.\nKroon, M. and S. Whiteson. 2009. “Automatic feature selection\nfor model-based reinforcement learning in factored MDPs”. In:\nMachine Learning and Applications, 2009. ICMLA’09. International\nConference on. IEEE. 324–330.\nKulkarni, T. D., K. Narasimhan, A. Saeedi, and J. Tenenbaum. 2016.\n“Hierarchical deep reinforcement learning: Integrating temporal\nabstraction and intrinsic motivation”. In: Advances in Neural\nInformation Processing Systems. 3675–3683.\nLample, G. and D. S. Chaplot. 2017. “Playing FPS Games with Deep\nReinforcement Learning.” In: AAAI. 2140–2146.\nLeCun, Y., Y. Bengio, and G. Hinton. 2015. “Deep learning”. Nature.\n521(7553): 436–444.\nLeCun, Y., L. Bottou, Y. Bengio, and P. Haﬀner. 1998. “Gradient-based\nlearning applied to document recognition”. Proceedings of the IEEE.\n86(11): 2278–2324.\nLeCun, Y., Y. Bengio, et al. 1995. “Convolutional networks for images,\nspeech, and time series”. The handbook of brain theory and neural\nnetworks. 3361(10): 1995.\nLee, D., H. Seo, and M. W. Jung. 2012. “Neural basis of reinforcement\nlearning and decision making”. Annual review of neuroscience. 35:\n287–308.\nLeﬄer, B. R., M. L. Littman, and T. Edmunds. 2007. “Eﬃcient\nreinforcement learning with relocatable action models”. In: AAAI.\nVol. 7. 572–577.\nLevine, S., C. Finn, T. Darrell, and P. Abbeel. 2016. “End-to-end\ntraining of deep visuomotor policies”. Journal of Machine Learning\nResearch. 17(39): 1–40.\nLevine, S. and V. Koltun. 2013. “Guided policy search”. In: International\nConference on Machine Learning. 1–9.\nLi, L., Y. Lv, and F.-Y. Wang. 2016. “Traﬃc signal timing via deep\nreinforcement learning”. IEEE/CAA Journal of Automatica Sinica.\n3(3): 247–254.\nLi, L., W. Chu, J. Langford, and X. Wang. 2011. “Unbiased oﬄine\nevaluation of contextual-bandit-based news article recommendation\nalgorithms”. In: Proceedings of the fourth ACM international\nconference on Web search and data mining. ACM. 297–306.\nLi, X., L. Li, J. Gao, X. He, J. Chen, L. Deng, and J. He. 2015.\n“Recurrent reinforcement learning: a hybrid approach”. arXiv\npreprint arXiv:1509.03044.\nLiaw, A., M. Wiener, et al. 2002. “Classiﬁcation and regression by\nrandomForest”. R news. 2(3): 18–22.\nLillicrap, T. P., J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,\nD. Silver, and D. Wierstra. 2015. “Continuous control with deep\nreinforcement learning”. arXiv preprint arXiv:1509.02971.\nLin, L.-J. 1992. “Self-improving reactive agents based on reinforcement\nlearning, planning and teaching”. Machine learning. 8(3-4): 293–321.\nLipton, Z. C., J. Gao, L. Li, X. Li, F. Ahmed, and L. Deng. 2016.\n“Eﬃcient exploration for dialogue policy learning with BBQ networks\n& replay buﬀer spiking”. arXiv preprint arXiv:1608.05081.\nLittman, M. L. 1994. “Markov games as a framework for multi-agent\nreinforcement learning”. In: Proceedings of the eleventh international\nconference on machine learning. Vol. 157. 157–163.\nLiu, Y., A. Gupta, P. Abbeel, and S. Levine. 2017. “Imitation from\nObservation: Learning to Imitate Behaviors from Raw Video via\nContext Translation”. arXiv preprint arXiv:1707.03374.\nLowe, R., Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch.\n2017. “Multi-Agent Actor-Critic for Mixed Cooperative-Competitive\nEnvironments”. arXiv preprint arXiv:1706.02275.\nMacGlashan, J., M. K. Ho, R. Loftin, B. Peng, D. Roberts, M. E.\nTaylor, and M. L. Littman. 2017. “Interactive Learning from Policy-\nDependent Human Feedback”. arXiv preprint arXiv:1701.06049.\nMachado, M. C., M. G. Bellemare, and M. Bowling. 2017a. “A Laplacian\nFramework for Option Discovery in Reinforcement Learning”. arXiv\npreprint arXiv:1703.00956.\nMachado, M. C., M. G. Bellemare, E. Talvitie, J. Veness, M. Hausknecht,\nand M. Bowling. 2017b. “Revisiting the Arcade Learning Environ-\nment: Evaluation Protocols and Open Problems for General Agents”.\narXiv preprint arXiv:1709.06009.\nMandel, T., Y.-E. Liu, S. Levine, E. Brunskill, and Z. Popovic. 2014.\n“Oﬄine policy evaluation across representations with applications\nto educational games”. In: Proceedings of the 2014 international\nconference on Autonomous agents and multi-agent systems. Interna-\ntional Foundation for Autonomous Agents and Multiagent Systems.\n1077–1084.\nMankowitz, D. J., T. A. Mann, and S. Mannor. 2016. “Adaptive Skills\nAdaptive Partitions (ASAP)”. In: Advances in Neural Information\nProcessing Systems. 1588–1596.\nMathieu, M., C. Couprie, and Y. LeCun. 2015. “Deep multi-scale\nvideo prediction beyond mean square error”. arXiv preprint\narXiv:1511.05440.\nMatiisen, T., A. Oliver, T. Cohen, and J. Schulman. 2017. “Teacher-\nStudent Curriculum Learning”. arXiv preprint arXiv:1707.00183.\nMcCallum, A. K. 1996. “Reinforcement learning with selective percep-\ntion and hidden state”. PhD thesis. University of Rochester.\nMcGovern, A., R. S. Sutton, and A. H. Fagg. 1997. “Roles of macro-\nactions in accelerating reinforcement learning”. In: Grace Hopper\ncelebration of women in computing. Vol. 1317.\nMiikkulainen, R., J. Liang, E. Meyerson, A. Rawal, D. Fink, O. Francon,\nB. Raju, A. Navruzyan, N. Duﬀy, and B. Hodjat. 2017. “Evolving\nDeep Neural Networks”. arXiv preprint arXiv:1703.00548.\nMirowski, P., R. Pascanu, F. Viola, H. Soyer, A. Ballard, A. Banino,\nM. Denil, R. Goroshin, L. Sifre, K. Kavukcuoglu, et al. 2016.\n“Learning to navigate in complex environments”. arXiv preprint\narXiv:1611.03673.\nMnih, V., A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D.\nSilver, and K. Kavukcuoglu. 2016. “Asynchronous methods for deep\nreinforcement learning”. In: International Conference on Machine\nLearning.\nMnih, V., K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,\net al. 2015. “Human-level control through deep reinforcement\nlearning”. Nature. 518(7540): 529–533.\nMohamed, S. and D. J. Rezende. 2015. “Variational information\nmaximisation for intrinsically motivated reinforcement learning”. In:\nAdvances in neural information processing systems. 2125–2133.\nMontague, P. R. 2013. “Reinforcement Learning Models Then-and-\nNow: From Single Cells to Modern Neuroimaging”. In: 20 Years of\nComputational Neuroscience. Springer. 271–277.\nMoore, A. W. 1990. “Eﬃcient memory-based learning for robot control”.\nMorari, M. and J. H. Lee. 1999. “Model predictive control: past, present\nand future”. Computers & Chemical Engineering. 23(4-5): 667–682.\nMoravčik, M., M. Schmid, N. Burch, V. Lisy, D. Morrill, N. Bard, T.\nDavis, K. Waugh, M. Johanson, and M. Bowling. 2017. “DeepStack:\nExpert-level artiﬁcial intelligence in heads-up no-limit poker”.\nScience. 356(6337): 508–513.\nMordatch, I., K. Lowrey, G. Andrew, Z. Popovic, and E. V. Todorov.\n2015. “Interactive control of diverse complex characters with neural\nnetworks”. In: Advances in Neural Information Processing Systems.\n3132–3140.\nMorimura, T., M. Sugiyama, H. Kashima, H. Hachiya, and T.\nTanaka. 2010. “Nonparametric return distribution approximation\nfor reinforcement learning”. In: Proceedings of the 27th International\nConference on Machine Learning (ICML-10). 799–806.\nMunos, R. and A. Moore. 2002. “Variable resolution discretization in\noptimal control”. Machine learning. 49(2): 291–323.\nMunos, R., T. Stepleton, A. Harutyunyan, and M. Bellemare. 2016.\n“Safe and eﬃcient oﬀ-policy reinforcement learning”. In: Advances\nin Neural Information Processing Systems. 1046–1054.\nMurphy, K. P. 2012. “Machine Learning: A Probabilistic Perspective.”\nNagabandi, A., G. Kahn, R. S. Fearing, and S. Levine. 2017. “Neural\nnetwork dynamics for model-based deep reinforcement learning with\nmodel-free ﬁne-tuning”. arXiv preprint arXiv:1708.02596.\nNagabandi, A., G. Kahn, R. S. Fearing, and S. Levine. 2018. “Neural\nnetwork dynamics for model-based deep reinforcement learning with\nmodel-free ﬁne-tuning”. In: 2018 IEEE International Conference on\nRobotics and Automation (ICRA). IEEE. 7559–7566.\nNarvekar, S., J. Sinapov, M. Leonetti, and P. Stone. 2016. “Source\ntask creation for curriculum learning”. In: Proceedings of the 2016\nInternational Conference on Autonomous Agents & Multiagent\nSystems. International Foundation for Autonomous Agents and\nMultiagent Systems. 566–574.\nNeelakantan, A., Q. V. Le, and I. Sutskever. 2015. “Neural programmer:\nInducing latent programs with gradient descent”. arXiv preprint\narXiv:1511.04834.\nNeu, G. and C. Szepesvári. 2012. “Apprenticeship learning using\ninverse reinforcement learning and gradient methods”. arXiv preprint\narXiv:1206.5264.\nNg, A. Y., D. Harada, and S. Russell. 1999. “Policy invariance under\nreward transformations: Theory and application to reward shaping”.\nIn: ICML. Vol. 99. 278–287.\nNg, A. Y., S. J. Russell, et al. 2000. “Algorithms for inverse reinforcement\nlearning.” In: Icml. 663–670.\nNguyen, D. H. and B. Widrow. 1990. “Neural networks for self-learning\ncontrol systems”. IEEE Control systems magazine. 10(3): 18–23.\nNiv, Y. 2009. “Reinforcement learning in the brain”. Journal of\nMathematical Psychology. 53(3): 139–154.\nNiv, Y. and P. R. Montague. 2009. “Theoretical and empirical studies\nof learning”. In: Neuroeconomics. Elsevier. 331–351.\nNorris, J. R. 1998. Markov chains. No. 2. Cambridge university press.\nO’Donoghue, B., R. Munos, K. Kavukcuoglu, and V. Mnih. 2016.\n“PGQ: Combining policy gradient and Q-learning”. arXiv preprint\narXiv:1611.01626.\nOh, J., V. Chockalingam, S. Singh, and H. Lee. 2016. “Control of\nMemory, Active Perception, and Action in Minecraft”. arXiv preprint\narXiv:1605.09128.\nOh, J., X. Guo, H. Lee, R. L. Lewis, and S. Singh. 2015. “Action-\nconditional video prediction using deep networks in atari games”.\nIn: Advances in Neural Information Processing Systems. 2863–2871.\nOh, J., S. Singh, and H. Lee. 2017. “Value Prediction Network”. arXiv\npreprint arXiv:1707.03497.\nOlah, C., A. Mordvintsev, and L. Schubert. 2017. “Feature Visualiza-\ntion”. Distill. https://distill.pub/2017/feature-visualization.\nOrtner, R., O.-A. Maillard, and D. Ryabko. 2014. “Selecting near-\noptimal approximate state representations in reinforcement learning”.\nIn: International Conference on Algorithmic Learning Theory.\nSpringer. 140–154.\nOsband, I., C. Blundell, A. Pritzel, and B. Van Roy. 2016. “Deep Explo-\nration via Bootstrapped DQN”. arXiv preprint arXiv:1602.04621.\nOstrovski, G., M. G. Bellemare, A. v. d. Oord, and R. Munos. 2017.\n“Count-based exploration with neural density models”. arXiv preprint\narXiv:1703.01310.\nPaine, T. L., S. G. Colmenarejo, Z. Wang, S. Reed, Y. Aytar, T. Pfaﬀ,\nM. W. Hoﬀman, G. Barth-Maron, S. Cabi, D. Budden, et al. 2018.\n“One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets\nwith RL”. arXiv preprint arXiv:1810.05017.\nParisotto, E., J. L. Ba, and R. Salakhutdinov. 2015. “Actor-mimic:\nDeep multitask and transfer reinforcement learning”. arXiv preprint\narXiv:1511.06342.\nPascanu, R., Y. Li, O. Vinyals, N. Heess, L. Buesing, S. Racanière,\nD. Reichert, T. Weber, D. Wierstra, and P. Battaglia. 2017.\n“Learning model-based planning from scratch”. arXiv preprint\narXiv:1707.06170.\nPathak, D., P. Agrawal, A. A. Efros, and T. Darrell. 2017. “Curiosity-\ndriven exploration by self-supervised prediction”. In: International\nConference on Machine Learning (ICML). Vol. 2017.\nPavlov, I. P. 1927. Conditioned reﬂexes. Oxford University Press.\nPedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,\nO. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, et\nal. 2011. “Scikit-learn: Machine learning in Python”. Journal of\nMachine Learning Research. 12(Oct): 2825–2830.\nPeng, J. and R. J. Williams. 1994. “Incremental multi-step Q-learning”.\nIn: Machine Learning Proceedings 1994. Elsevier. 226–232.\nPeng, P., Q. Yuan, Y. Wen, Y. Yang, Z. Tang, H. Long, and J. Wang.\n2017a. “Multiagent Bidirectionally-Coordinated Nets for Learning to\nPlay StarCraft Combat Games”. arXiv preprint arXiv:1703.10069.\nPeng, X. B., G. Berseth, K. Yin, and M. van de Panne. 2017b.\n“DeepLoco: Dynamic Locomotion Skills Using Hierarchical Deep\nReinforcement Learning”. ACM Transactions on Graphics (Proc.\nSIGGRAPH 2017). 36(4).\nPerez-Liebana, D., S. Samothrakis, J. Togelius, T. Schaul, S. M. Lucas,\nA. Couëtoux, J. Lee, C.-U. Lim, and T. Thompson. 2016. “The 2014\ngeneral video game playing competition”. IEEE Transactions on\nComputational Intelligence and AI in Games. 8(3): 229–243.\nPetrik, M. and B. Scherrer. 2009. “Biasing approximate dynamic\nprogramming with a lower discount factor”. In: Advances in neural\ninformation processing systems. 1265–1272.\nPiketty, T. 2013. “Capital in the Twenty-First Century”.\nPineau, J., G. Gordon, S. Thrun, et al. 2003. “Point-based value iteration:\nAn anytime algorithm for POMDPs”. In: IJCAI. Vol. 3. 1025–1032.\nPinto, L., M. Andrychowicz, P. Welinder, W. Zaremba, and P. Abbeel.\n2017. “Asymmetric Actor Critic for Image-Based Robot Learning”.\narXiv preprint arXiv:1710.06542.\nPlappert, M., R. Houthooft, P. Dhariwal, S. Sidor, R. Y. Chen, X. Chen,\nT. Asfour, P. Abbeel, and M. Andrychowicz. 2017. “Parameter Space\nNoise for Exploration”. arXiv preprint arXiv:1706.01905.\nPrecup, D. 2000. “Eligibility traces for oﬀ-policy policy evaluation”.\nComputer Science Department Faculty Publication Series: 80.\nRanzato, M., S. Chopra, M. Auli, and W. Zaremba. 2015. “Sequence\nlevel training with recurrent neural networks”. arXiv preprint\narXiv:1511.06732.\nRasmussen, C. E. 2004. “Gaussian processes in machine learning”. In:\nAdvanced lectures on machine learning. Springer. 63–71.\nRavindran, B. and A. G. Barto. 2004. “An algebraic approach to\nabstraction in reinforcement learning”. PhD thesis. University of\nMassachusetts at Amherst.\nReal, E., S. Moore, A. Selle, S. Saxena, Y. L. Suematsu, Q. Le, and A.\nKurakin. 2017. “Large-Scale Evolution of Image Classiﬁers”. arXiv\npreprint arXiv:1703.01041.\nReed, S. and N. De Freitas. 2015. “Neural programmer-interpreters”.\narXiv preprint arXiv:1511.06279.\nRescorla, R. A., A. R. Wagner, et al. 1972. “A theory of Pavlovian\nconditioning: Variations in the eﬀectiveness of reinforcement and\nnonreinforcement”. Classical conditioning II: Current research and\ntheory. 2: 64–99.\nRiedmiller, M. 2005. “Neural ﬁtted Q iteration–ﬁrst experiences with a\ndata eﬃcient neural reinforcement learning method”. In: Machine\nLearning: ECML 2005. Springer. 317–328.\nRiedmiller, M., R. Hafner, T. Lampe, M. Neunert, J. Degrave, T. Van de\nWiele, V. Mnih, N. Heess, and J. T. Springenberg. 2018. “Learning\nby Playing - Solving Sparse Reward Tasks from Scratch”. arXiv\npreprint arXiv:1802.10567.\nRowland, M., M. G. Bellemare, W. Dabney, R. Munos, and Y. W. Teh.\n2018. “An Analysis of Categorical Distributional Reinforcement\nLearning”. arXiv preprint arXiv:1802.08163.\nRuder, S. 2017. “An overview of multi-task learning in deep neural\nnetworks”. arXiv preprint arXiv:1706.05098.\nRumelhart, D. E., G. E. Hinton, and R. J. Williams. 1988. “Learning\nrepresentations by back-propagating errors”. Cognitive modeling.\n5(3): 1.\nRussakovsky, O., J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z.\nHuang, A. Karpathy, A. Khosla, M. Bernstein, et al. 2015. “Imagenet\nlarge scale visual recognition challenge”. International Journal of\nComputer Vision. 115(3): 211–252.\nRussek, E. M., I. Momennejad, M. M. Botvinick, S. J. Gershman, and\nN. D. Daw. 2017. “Predictive representations can link model-based\nreinforcement learning to model-free mechanisms”. bioRxiv: 083857.\nRusu, A. A., S. G. Colmenarejo, C. Gulcehre, G. Desjardins, J.\nKirkpatrick, R. Pascanu, V. Mnih, K. Kavukcuoglu, and R. Hadsell.\n2015. “Policy distillation”. arXiv preprint arXiv:1511.06295.\nRusu, A. A., M. Vecerik, T. Rothörl, N. Heess, R. Pascanu, and\nR. Hadsell. 2016. “Sim-to-real robot learning from pixels with\nprogressive nets”. arXiv preprint arXiv:1610.04286.\nSadeghi, F. and S. Levine. 2016. “CAD2RL: Real single-image ﬂight\nwithout a single real image”. arXiv preprint arXiv:1611.04201.\nSalge, C., C. Glackin, and D. Polani. 2014. “Changing the environment\nbased on empowerment as intrinsic motivation”. Entropy. 16(5):\n2789–2819.\nSalimans, T., J. Ho, X. Chen, and I. Sutskever. 2017. “Evolution\nStrategies as a Scalable Alternative to Reinforcement Learning”.\narXiv preprint arXiv:1703.03864.\nSamuel, A. L. 1959. “Some studies in machine learning using the game of\ncheckers”. IBM Journal of research and development. 3(3): 210–229.\nSandve, G. K., A. Nekrutenko, J. Taylor, and E. Hovig. 2013.\n“Ten simple rules for reproducible computational research”. PLoS\ncomputational biology. 9(10): e1003285.\nSantoro, A., D. Raposo, D. G. Barrett, M. Malinowski, R. Pascanu, P.\nBattaglia, and T. Lillicrap. 2017. “A simple neural network module\nfor relational reasoning”. arXiv preprint arXiv:1706.01427.\nSavinov, N., A. Raichuk, R. Marinier, D. Vincent, M. Pollefeys,\nT. Lillicrap, and S. Gelly. 2018. “Episodic Curiosity through\nReachability”. arXiv preprint arXiv:1810.02274.\nSchaarschmidt, M., A. Kuhnle, and K. Fricke. 2017. “TensorForce: A\nTensorFlow library for applied reinforcement learning”.\nSchaul, T., J. Bayer, D. Wierstra, Y. Sun, M. Felder, F. Sehnke, T.\nRückstieß, and J. Schmidhuber. 2010. “PyBrain”. The Journal of\nMachine Learning Research. 11: 743–746.\nSchaul, T., D. Horgan, K. Gregor, and D. Silver. 2015a. “Universal value\nfunction approximators”. In: Proceedings of the 32nd International\nConference on Machine Learning (ICML-15). 1312–1320.\nSchaul, T., J. Quan, I. Antonoglou, and D. Silver. 2015b. “Prioritized\nExperience Replay”. arXiv preprint arXiv:1511.05952.\nSchmidhuber, J. 2010. “Formal theory of creativity, fun, and intrinsic\nmotivation (1990–2010)”. IEEE Transactions on Autonomous\nMental Development. 2(3): 230–247.\nSchmidhuber, J. 2015. “Deep learning in neural networks: An overview”.\nNeural Networks. 61: 85–117.\nSchraudolph, N. N., P. Dayan, and T. J. Sejnowski. 1994. “Temporal\ndiﬀerence learning of position evaluation in the game of Go”. In:\nAdvances in Neural Information Processing Systems. 817–824.\nSchulman, J., P. Abbeel, and X. Chen. 2017a. “Equivalence Be-\ntween Policy Gradients and Soft Q-Learning”. arXiv preprint\narXiv:1704.06440.\nSchulman, J., J. Ho, C. Lee, and P. Abbeel. 2016. “Learning from\ndemonstrations through the use of non-rigid registration”. In:\nRobotics Research. Springer. 339–354.\nSchulman, J., S. Levine, P. Abbeel, M. I. Jordan, and P. Moritz. 2015.\n“Trust Region Policy Optimization”. In: ICML. 1889–1897.\nSchulman, J., F. Wolski, P. Dhariwal, A. Radford, and O. Klimov.\n2017b. “Proximal policy optimization algorithms”. arXiv preprint\narXiv:1707.06347.\nSchultz, W., P. Dayan, and P. R. Montague. 1997. “A neural substrate\nof prediction and reward”. Science. 275(5306): 1593–1599.\nShannon, C. 1950. “Programming a Computer for Playing Chess”.\nPhilosophical Magazine. 41(314).\nSilver, D. L., Q. Yang, and L. Li. 2013. “Lifelong Machine Learning Sys-\ntems: Beyond Learning Algorithms.” In: AAAI Spring Symposium:\nLifelong Machine Learning. Vol. 13. 05.\nSilver, D., G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller.\n2014. “Deterministic Policy Gradient Algorithms”. In: ICML.\nSilver, D., A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den\nDriessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M.\nLanctot, et al. 2016a. “Mastering the game of Go with deep neural\nnetworks and tree search”. Nature. 529(7587): 484–489.\nSilver, D., H. van Hasselt, M. Hessel, T. Schaul, A. Guez, T. Harley,\nG. Dulac-Arnold, D. Reichert, N. Rabinowitz, A. Barreto, et al.\n2016b. “The predictron: End-to-end learning and planning”. arXiv\npreprint arXiv:1612.08810.\nSingh, S. P., T. S. Jaakkola, and M. I. Jordan. 1994. “Learning Without\nState-Estimation in Partially Observable Markovian Decision\nProcesses.” In: ICML. 284–292.\nSingh, S. P. and R. S. Sutton. 1996. “Reinforcement learning with\nreplacing eligibility traces”. Machine learning. 22(1-3): 123–158.\nSingh, S., T. Jaakkola, M. L. Littman, and C. Szepesvári. 2000.\n“Convergence results for single-step on-policy reinforcement-learning\nalgorithms”. Machine learning. 38(3): 287–308.\nSondik, E. J. 1978. “The optimal control of partially observable Markov\nprocesses over the inﬁnite horizon: Discounted costs”. Operations\nresearch. 26(2): 282–304.\nSrivastava, N., G. E. Hinton, A. Krizhevsky, I. Sutskever, and R.\nSalakhutdinov. 2014. “Dropout: a simple way to prevent neural\nnetworks from overﬁtting.” Journal of Machine Learning Research.\n15(1): 1929–1958.\nStadie, B. C., S. Levine, and P. Abbeel. 2015. “Incentivizing Exploration\nIn Reinforcement Learning With Deep Predictive Models”. arXiv\npreprint arXiv:1507.00814.\nStone, P. and M. Veloso. 2000. “Layered learning”. Machine Learning:\nECML 2000: 369–381.\nStory, G., I. Vlaev, B. Seymour, A. Darzi, and R. Dolan. 2014. “Does\ntemporal discounting explain unhealthy behavior? A systematic re-\nview and reinforcement learning perspective”. Frontiers in behavioral\nneuroscience. 8: 76.\nSukhbaatar, S., A. Szlam, and R. Fergus. 2016. “Learning multiagent\ncommunication with backpropagation”. In: Advances in Neural\nInformation Processing Systems. 2244–2252.\nSun, Y., F. Gomez, and J. Schmidhuber. 2011. “Planning to be\nsurprised: Optimal bayesian exploration in dynamic environments”.\nIn: Artiﬁcial General Intelligence. Springer. 41–51.\nSunehag, P., G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi, M.\nJaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls, et al.\n2017. “Value-Decomposition Networks For Cooperative Multi-Agent\nLearning”. arXiv preprint arXiv:1706.05296.\nSutton, R. S. 1988. “Learning to predict by the methods of temporal\ndiﬀerences”. Machine learning. 3(1): 9–44.\nSutton, R. S. 1996. “Generalization in reinforcement learning: Suc-\ncessful examples using sparse coarse coding”. Advances in neural\ninformation processing systems: 1038–1044.\nSutton, R. S. and A. G. Barto. 1998. Reinforcement learning: An\nintroduction. Vol. 1. No. 1. MIT press Cambridge.\nSutton, R. S. and A. G. Barto. 2017. Reinforcement Learning: An\nIntroduction (2nd Edition, in progress). MIT Press.\nSutton, R. S., D. A. McAllester, S. P. Singh, and Y. Mansour. 2000.\n“Policy gradient methods for reinforcement learning with function\napproximation”. In: Advances in neural information processing\nsystems. 1057–1063.\nSutton, R. S., D. Precup, and S. Singh. 1999. “Between MDPs and\nsemi-MDPs: A framework for temporal abstraction in reinforcement\nlearning”. Artiﬁcial intelligence. 112(1-2): 181–211.\nSutton, R. S. 1984. “Temporal credit assignment in reinforcement\nlearning”.\nSynnaeve, G., N. Nardelli, A. Auvolat, S. Chintala, T. Lacroix, Z. Lin, F.\nRichoux, and N. Usunier. 2016. “TorchCraft: a Library for Machine\nLearning Research on Real-Time Strategy Games”. arXiv preprint\narXiv:1611.00625.\nSzegedy, C., S. Ioﬀe, V. Vanhoucke, and A. Alemi. 2016. “Inception-v4,\ninception-resnet and the impact of residual connections on learning”.\narXiv preprint arXiv:1602.07261.\nSzegedy, C., S. Ioﬀe, V. Vanhoucke, and A. A. Alemi. 2017. “Inception-\nv4, inception-resnet and the impact of residual connections on\nlearning.” In: AAAI. Vol. 4. 12.\nTamar, A., S. Levine, P. Abbeel, Y. WU, and G. Thomas. 2016. “Value\niteration networks”. In: Advances in Neural Information Processing\nSystems. 2146–2154.\nTan, J., T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner, S. Bohez,\nand V. Vanhoucke. 2018. “Sim-to-Real: Learning Agile Locomotion\nFor Quadruped Robots”. arXiv preprint arXiv:1804.10332.\nTanner, B. and A. White. 2009. “RL-Glue: Language-independent\nsoftware for reinforcement-learning experiments”. The Journal of\nMachine Learning Research. 10: 2133–2136.\nTeh, Y. W., V. Bapst, W. M. Czarnecki, J. Quan, J. Kirkpatrick, R.\nHadsell, N. Heess, and R. Pascanu. 2017. “Distral: Robust Multitask\nReinforcement Learning”. arXiv preprint arXiv:1707.04175.\nTesauro, G. 1995. “Temporal diﬀerence learning and TD-Gammon”.\nCommunications of the ACM. 38(3): 58–68.\nTessler, C., S. Givony, T. Zahavy, D. J. Mankowitz, and S. Mannor. 2017.\n“A Deep Hierarchical Approach to Lifelong Learning in Minecraft.”\nIn: AAAI. 1553–1561.\nThomas, P. 2014. “Bias in natural actor-critic algorithms”. In: Interna-\ntional Conference on Machine Learning. 441–448.\nThomas, P. S. and E. Brunskill. 2016. “Data-eﬃcient oﬀ-policy policy\nevaluation for reinforcement learning”. In: International Conference\non Machine Learning.\nThrun, S. B. 1992. “Eﬃcient exploration in reinforcement learning”.\nTian, Y., Q. Gong, W. Shang, Y. Wu, and C. L. Zitnick. 2017. “ELF:\nAn Extensive, Lightweight and Flexible Research Platform for Real-\ntime Strategy Games”. Advances in Neural Information Processing\nSystems (NIPS).\nTieleman, H. 2012. “Lecture 6.5-rmsprop: Divide the gradient by a\nrunning average of its recent magnitude”. COURSERA: Neural\nNetworks for Machine Learning.\nTobin, J., R. Fong, A. Ray, J. Schneider, W. Zaremba, and P.\nAbbeel. 2017. “Domain Randomization for Transferring Deep Neural\nNetworks from Simulation to the Real World”. arXiv preprint\narXiv:1703.06907.\nTodorov, E., T. Erez, and Y. Tassa. 2012. “MuJoCo: A physics engine\nfor model-based control”. In: Intelligent Robots and Systems (IROS),\n2012 IEEE/RSJ International Conference on. IEEE. 5026–5033.\nTsitsiklis, J. N. and B. Van Roy. 1997. “An analysis of temporal-\ndiﬀerence learning with function approximation”. Automatic Control,\nIEEE Transactions on. 42(5): 674–690.\nTuring, A. M. 1953. “Digital computers applied to games”. Faster than\nthought.\nTzeng, E., C. Devin, J. Hoﬀman, C. Finn, P. Abbeel, S. Levine,\nK. Saenko, and T. Darrell. 2015. “Adapting deep visuomotor\nrepresentations with weak pairwise constraints”. arXiv preprint\narXiv:1511.07111.\nUeno, S., M. Osawa, M. Imai, T. Kato, and H. Yamakawa. 2017.\n““Re: ROS”: Prototyping of Reinforcement Learning Environment\nfor Asynchronous Cognitive Architecture”. In: First International\nEarly Research Career Enhancement School on Biologically Inspired\nCognitive Architectures. Springer. 198–203.\nVan Hasselt, H., A. Guez, and D. Silver. 2016. “Deep Reinforcement\nLearning with Double Q-Learning.” In: AAAI. 2094–2100.\nVapnik, V. N. 1998. “Statistical learning theory. Adaptive and learning\nsystems for signal processing, communications, and control”.\nVaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin. 2017. “Attention Is All You Need”.\narXiv preprint arXiv:1706.03762.\nVezhnevets, A., V. Mnih, S. Osindero, A. Graves, O. Vinyals, J. Agapiou,\net al. 2016. “Strategic attentive writer for learning macro-actions”.\nIn: Advances in Neural Information Processing Systems. 3486–3494.\nVinyals, O., T. Ewalds, S. Bartunov, P. Georgiev, A. S. Vezhnevets,\nM. Yeo, A. Makhzani, H. Küttler, J. Agapiou, J. Schrittwieser, et al.\n2017. “StarCraft II: A New Challenge for Reinforcement Learning”.\narXiv preprint arXiv:1708.04782.\nWahlström, N., T. B. Schön, and M. P. Deisenroth. 2015. “From pixels\nto torques: Policy learning with deep dynamical models”. arXiv\npreprint arXiv:1502.02251.\nWalsh, T. 2017. It’s Alive!: Artiﬁcial Intelligence from the Logic Piano\nto Killer Robots. La Trobe University Press.\nWang, J. X., Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo,\nR. Munos, C. Blundell, D. Kumaran, and M. Botvinick. 2016a.\n“Learning to reinforcement learn”. arXiv preprint arXiv:1611.05763.\nWang, Z., V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu, and\nN. de Freitas. 2016b. “Sample eﬃcient actor-critic with experience\nreplay”. arXiv preprint arXiv:1611.01224.\nWang, Z., N. de Freitas, and M. Lanctot. 2015. “Dueling network\narchitectures for deep reinforcement learning”. arXiv preprint\narXiv:1511.06581.\nWarnell, G., N. Waytowich, V. Lawhern, and P. Stone. 2017. “Deep\nTAMER: Interactive Agent Shaping in High-Dimensional State\nSpaces”. arXiv preprint arXiv:1709.10163.\nWatkins, C. J. and P. Dayan. 1992. “Q-learning”. Machine learning.\n8(3-4): 279–292.\nWatkins, C. J. C. H. 1989. “Learning from delayed rewards”. PhD thesis.\nKing’s College, Cambridge.\nWatter, M., J. Springenberg, J. Boedecker, and M. Riedmiller. 2015.\n“Embed to control: A locally linear latent dynamics model for control\nfrom raw images”. In: Advances in neural information processing\nsystems. 2746–2754.\nWeber, T., S. Racanière, D. P. Reichert, L. Buesing, A. Guez, D. J.\nRezende, A. P. Badia, O. Vinyals, N. Heess, Y. Li, et al. 2017.\n“Imagination-Augmented Agents for Deep Reinforcement Learning”.\narXiv preprint arXiv:1707.06203.\nWender, S. and I. Watson. 2012. “Applying reinforcement learning\nto small scale combat in the real-time strategy game StarCraft:\nBroodwar”. In: Computational Intelligence and Games (CIG), 2012\nIEEE Conference on. IEEE. 402–408.\nWhiteson, S., B. Tanner, M. E. Taylor, and P. Stone. 2011. “Protecting\nagainst evaluation overﬁtting in empirical reinforcement learning”.\nIn: Adaptive Dynamic Programming And Reinforcement Learning\n(ADPRL), 2011 IEEE Symposium on. IEEE. 120–127.\nWilliams, R. J. 1992. “Simple statistical gradient-following algorithms\nfor connectionist reinforcement learning”. Machine learning. 8(3-4):\n229–256.\nWu, Y. and Y. Tian. 2016. “Training agent for ﬁrst-person shooter game\nwith actor-critic curriculum learning”.\nXu, K., J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel,\nand Y. Bengio. 2015. “Show, attend and tell: Neural image caption\ngeneration with visual attention”. In: International Conference on\nMachine Learning. 2048–2057.\nYou, Y., X. Pan, Z. Wang, and C. Lu. 2017. “Virtual to Real\nReinforcement Learning for Autonomous Driving”. arXiv preprint\narXiv:1704.03952.\nZamora, I., N. G. Lopez, V. M. Vilches, and A. H. Cordero. 2016.\n“Extending the OpenAI Gym for robotics: a toolkit for reinforcement\nlearning using ROS and Gazebo”. arXiv preprint arXiv:1608.05742.\nZhang, A., N. Ballas, and J. Pineau. 2018a. “A Dissection of Overﬁtting\nand Generalization in Continuous Reinforcement Learning”. arXiv\npreprint arXiv:1806.07937.\nZhang, A., H. Satija, and J. Pineau. 2018b. “Decoupling Dynamics and\nReward for Transfer Learning”. arXiv preprint arXiv:1804.10689.\nZhang, C., O. Vinyals, R. Munos, and S. Bengio. 2018c. “A Study\non Overﬁtting in Deep Reinforcement Learning”. arXiv preprint\narXiv:1804.06893.\nZhang, C., S. Bengio, M. Hardt, B. Recht, and O. Vinyals. 2016.\n“Understanding deep learning requires rethinking generalization”.\narXiv preprint arXiv:1611.03530.\nZhu, Y., R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-\nFei, and A. Farhadi. 2016. “Target-driven visual navigation in\nindoor scenes using deep reinforcement learning”. arXiv preprint\narXiv:1609.05143.\nZiebart, B. D. 2010. Modeling purposeful adaptive behavior with the\nprinciple of maximum causal entropy. Carnegie Mellon University.\nZoph, B. and Q. V. Le. 2016. “Neural architecture search with\nreinforcement learning”. arXiv preprint arXiv:1611.01578.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2018-11-30",
  "updated": "2018-12-03"
}