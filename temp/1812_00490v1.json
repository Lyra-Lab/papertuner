{
  "id": "http://arxiv.org/abs/1812.00490v1",
  "title": "Improving Clinical Predictions through Unsupervised Time Series Representation Learning",
  "authors": [
    "Xinrui Lyu",
    "Matthias Hueser",
    "Stephanie L. Hyland",
    "George Zerveas",
    "Gunnar Raetsch"
  ],
  "abstract": "In this work, we investigate unsupervised representation learning on medical\ntime series, which bears the promise of leveraging copious amounts of existing\nunlabeled data in order to eventually assist clinical decision making. By\nevaluating on the prediction of clinically relevant outcomes, we show that in a\npractical setting, unsupervised representation learning can offer clear\nperformance benefits over end-to-end supervised architectures. We experiment\nwith using sequence-to-sequence (Seq2Seq) models in two different ways, as an\nautoencoder and as a forecaster, and show that the best performance is achieved\nby a forecasting Seq2Seq model with an integrated attention mechanism, proposed\nhere for the first time in the setting of unsupervised learning for medical\ntime series.",
  "text": "Improving Clinical Predictions through Unsupervised\nTime Series Representation Learning\nXinrui Lyu1, Matthias Hüser1, Stephanie L. Hyland1, George Zerveas2, Gunnar Rätsch1\n1 Biomedical Informatics Group, Dept. of Computer Science, ETH Zürich\n2 AI Lab, Center for Biomedical Informatics, Brown University\nAbstract\nIn this work, we investigate unsupervised representation learning on medical time\nseries, which bears the promise of leveraging copious amounts of existing unlabeled\ndata in order to eventually assist clinical decision making. By evaluating on the\nprediction of clinically relevant outcomes, we show that in a practical setting,\nunsupervised representation learning can offer clear performance beneﬁts over end-\nto-end supervised architectures. We experiment with using sequence-to-sequence\n(Seq2Seq) models in two different ways, as an autoencoder and as a forecaster, and\nshow that the best performance is achieved by a forecasting Seq2Seq model with\nan integrated attention mechanism, proposed here for the ﬁrst time in the setting of\nunsupervised learning for medical time series.\n1\nIntroduction\nPatient representation learning is one of the popular topics in the ﬁeld of machine learning for\nhealthcare. The generality of supervised representations is usually constrained by the amount of\nlabeled data, while unsupervised representations can leverage information from all data, labeled or\nnot. Hence, unsupervised learning can produce representations of general utility [1–4], which can be\nuseful in case downstream tasks are not known a priori.\nConditions like the ones described above are especially true in the medical domain. Routine medical\npractice generates a wealth of patient-related time series, while data annotation often requires medical\nexperts, whose time is very limited. Additionally, new tasks of interest emerge, and different hospitals\nor health systems often deﬁne tasks in different ways. Thus, generally useful representations,\nproviding good performance over a broad range of downstream tasks, are highly desired.\nIn this work, we investigate unsupervised representation learning on medical time series, which\nremains relatively unexplored. We propose adapted and novel models well suited for this objective\nand elucidate under which conditions they provide a performance beneﬁt over end-to-end supervised\nlearning with respect to predicting clinically relevant outcomes.\n2\nRelated Work\nThe unsupervised learning approaches studied in this paper are rooted in the autoencoding princi-\nple [5]. The basic autoencoding architecture has been extended in several ways, such as denois-\ning [6], variational [7], convolutional [8], or contractive [9] autoencoders. Sequence-to-sequence\n(Seq2Seq) [10] architectures have been used successfully in translation [11], and on text and im-\nages [12, 13]. Seq2Seq models have also been pre-trained in an unsupervised way [14] and ﬁne-tuned\nwith labeled data.\nSeveral models for unsupervised representation learning have been successfully employed in medical\napplications [4, 15–18]. While in many cases representations were obtained with both descriptive as\nwell as predictive utility, the optimal reconstruction principles and loss functions leading to accurate\nclinical outcome prediction have not been widely studied.\nMachine Learning for Health (ML4H) Workshop at NeurIPS 2018.\narXiv:1812.00490v1  [cs.LG]  2 Dec 2018\nAttention mechanisms can improve performance and interpretability and have enjoyed wide use across\ndomains [19–22]. Although attention has been used in the context of unsupervised representation\nlearning of natural language [23], attention architectures in the medical domain have been so far\nexclusively focused on predicting speciﬁc supervised tasks.\n3\nRepresentation Learning Models\n3.1\nBaselines: Autoencoders\nAutoencoding consists of two steps: encoding maps the input data space Rd to an representation\nspace Rm, where typically m < d, while decoding maps in the reverse direction to reconstruct the\ndata from representations. The objective of autoencoding is to minimize the reconstruction error\nbetween the input data and the reconstructions.\nNon-Sequential models\nPrincipal Component analysis (PCA) and its inverse together can be\nconsidered as a simple autoencoding process, where the encoding is a learned linear projection. An\nautoencoder (AE) is a neural network composed of an encoder and a decoder, each implemented as\na multi-layer perceptron; it encodes the data in a non-linear way. Our goal is to encode temporal\nsequences of physiological signal vectors, but the inherent architecture of PCA and AE does not\nallow them to exploit the temporal structure in time series. To make data compatible with the input\nformat of PCA and AE, we ﬂatten a (T, d)-dimensional time series (i.e. T time samples, each of d\ndimensions) into a (Td)-dimensional vector.\nSeq2Seq model\nWhile Seq2Seq models are often used in supervised training settings in natu-\nral language processing [10, 11, 14], we use it in an unsupervised way by minimizing the input\nreconstruction error as an objective; we refer to such a model as a S2S-AE. Figure 1 shows the\nstructure of a S2S-AE model. A Long Short-Term Memory (LSTM) cell is used for both encoder and\ndecoder recurrent neural network (RNN) units, because it can retain information over more time-steps\ncompared to simple RNN cells [24, 25].\nAt time t, the encoder receives a sequence of signal vectors Xt := {xτ}t\nτ=t−T +1 from a time window\nof size T as input and produces a representation et := ht, where ht is the last hidden state of the\nencoder. The decoder, given et, outputs a sequence of reconstructions {ˆxτ}t\nτ=t−T +1 for the same\nwindow. Let fθ and gφ denote the encoder and decoder respectively, with parameters θ and φ. Then\nthe S2S-AE model can be formulated like\nfθ(xt−T +1, xt−T +2, . . . , xt) = et,\ngφ(et) = ˆxt−T +1, ˆxt−T +2, . . . , ˆxt,\n(1)\nL(Xt) = 1\nT\nXT −1\nτ=0 ∥xt−τ −ˆxt−τ∥2,\nL = 1\nN\nXN\ni=1\n1\nLi\nXLi−1\nk=0 L(XT +k),\n(2)\nwhere L(Xt) is the average reconstruction error for one window of a single patient’s input signals\nfrom t −T + 1 until the current time t. The loss for patient i is then the average error over their Li\nwindows, indexed by k, sliding with stride 1. To train the S2S-AE model we average the patient-wise\nloss over all N patients. The representation et from a S2S-AE model summarizes a ﬁxed length of\nthe medical history of a patient up to time T, which reﬂects the current state of the patient.\n3.2\nSequential forecasting model (S2S-F)\nWe hypothesize that the requirement to forecast future time points in the patient’s signal would force\nthe encoding LSTM to extract meaningful representations of the past time series. For this purpose,\nwe design another Seq2Seq-based variant, S2S-F (“F” for forecasting), where the decoder predicts\nEncoder\nDecoder\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLinear \nlayer\nLinear \nlayer\nLinear \nlayer\nLinear \nlayer\nFigure 1: Sequence-to-sequence autoencoder (S2S-AE). T is the length of history the we want to encode in the\nrepresentations. If we want to encode the patient’s history from admission to time t, then T = t.\n2\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLinear \nlayer\nLinear \nlayer\nLinear \nlayer\nLinear \nlayer\nAttention model\nEncoder\nDecoder\nFigure 2: Sequence-to-Sequence Forecaster with Attention (S2S-F-A).\nthe future time series instead of reconstructing the past time series in the input. In this way, the\nrepresentations still reﬂect the current patient state but are also optimized to predict the future patient\nstate. We modify (1) and (2) to get the decoder function and the loss function for S2S-F:\ng′\nφ(et) = ˆxt+1, ˆxt+2, . . . , ˆxt+T ,\nL′(Xt) = (1/T)\nXT\nτ=1 ∥xt+τ −ˆxt+τ∥2.\n3.3\nForecasting with attention (S2S-F-A)\nThe idea behind applying attention mechanisms to time series forecasting is to enable the decoder\nto preferentially “attend” to speciﬁc parts of the input sequence during decoding. This allows for\nparticularly relevant events (e.g. drastic changes in heart rate), to contribute more to the generation\nof different points in the output sequence. Since autoencoding with attention is trivial (an effective\nattention mechanism would learn to only point to the corresponding input at each time point), we only\naugment S2S-F with the attention mechanism, calling the architecture S2S-F-A (shown in Figure 2).\nFormally, at time τ ∈{t+1, . . . , t+T} during decoding, the objective is to produce a context vector\ncτ which is a weighted combination of the hidden states hj of the decoder: cτ = PT\nj=1 ατjhj. The\nweights ατj are softmax-normalized versions of weights γτj computed by the attention mechanism\nF, which considers both the current state of the decoder h′τ and each state of the encoder hj in\nturn: γτj = F(h′τ, hj) and ατj = exp(γτj)/ P\ni exp(γτi). To implement F, we use a single-\nlayer perceptron with a tanh activation function and scalar output, following [26]: F(h′τ, hj) =\nβT tanh(Wdh′τ + Wehj).\nEach ατj reﬂects the importance of time point j in the input sequence for decoding time point τ in\nthe output. The context vector cτ is thus an explicit resummarization of the input data in light of the\ncurrent decoding task. The context vector is concatenated to the usual input fed to the decoder at\nτ + 1, which is ˆxτ (see Figure 2).\nThe attention mechanism breaks the \"bottleneck\" principle of usual Seq2Seq models, and it is not\nobvious how to choose a self-contained representation. Following our practice for S2S-AE and S2S-F,\nwe take the ﬁnal state of the encoder, ht as the representation. Although we experimented with\nadditionally including context vectors as part of the representation, an interesting ﬁnding was that\nsimply taking ht was sufﬁcient in the prediction of downstream tasks. Table A2 summarizes the\ncharacteristics of the unsupervised representation models we analyze.\n4\nExperiments and results\nData\nThe eICU Collaborative Research Database v1.2 [27] was used for all experiments described\nin this paper. 94 time series variables including periodic and aperiodic vital signs and irregularly\nmeasured lab tests were extracted. The data was resampled to be hourly, with implausible data\nrejection and imputation performed online; see Appendix A.1 for more details. Overall, the dataset\nconsists of 20,878 patients with 72-720 hours of history, extending from ICU admission to dispatch.\nWe use a window size of 12 hours (i.e. 12 time points) and representation dimension 94.\nReconstructing past and predicting future\nWe aim to evaluate the ability of representations\nto reconstruct past and future data. Some representations are obtained from models optimized to\nreconstruct past data (PCA, AE and S2S-AE), while others from models optimized to predict future\ndata (S2S-F and S2S-F-A). To produce a fair comparison independent of a speciﬁc decoder, we use\nthe representations themselves as input features to a 1-layer LSTM trained to either reconstruct the\npast 12 hours, or predict the next 12. The performance for each set of representations are shown in\n3\nTable 1, evaluated using mean-squared error (MSE). Not surprisingly, representations from forecaster\nmodels perform better in future prediction and the attention mechanism further improves performance.\nHowever, the extent to which attention helps is surprising.\nTable 1: Performance of representations used as input features to a 1-layer LSTM trained to either reconstruct\nthe past 12 hours, or predict the next 12 hours. (The best results are in bold and second are best marked with *)\nMSE\nPCA\nAE\nS2S-AE\nS2S-F\nS2S-F-A\nReconstruction\n0.0743 ± 0.002\n0.0403 ± 0.001\n0.0505 ± 0.001\n0.0500 ± 0.001\n0.0474 ± 0.003∗\nPrediction\n0.149 ± 0.003\n0.114 ± 0.002∗\n0.121 ± 0.003\n0.119 ± 0.003\n0.0982 ± 0.003\nPredicting mortality and discharge status within the next 24 hours\nBesides evaluating the\nability of representations in past/future signal prediction, we are also interested in whether we can\nuse them to predict future clinical events. Here we focus on predicting whether patients will be\ndischarged from the ICU in a stable state (“24h Discharge”), or die within the next 24 hours (“24h\nMortality”). We trained 1-layer LSTM classiﬁers (LSTM-1) using representations as input to predict\nthese two events and report the area under ROC curve (AUROC) and the area under precision-recall\ncurve (AUPRC) in Table 2. In addition, we also include the performance of a 3-layer LSTM classiﬁer\n(LSTM-3), a “deeper” model, trained on the original input signals as a baseline.\nTable 2: Prediction of discharge/mortality status within the next 24 hours using unsupervised representations or\nraw signals. The prevalence of the discharge and mortality positive labels is 0.197 and 0.021 respectively. (The\nbest results are in bold and second are best marked with *)\n24h Discharge\n24h Mortality\nAUPRC\nAUROC\nAUPRC\nAUROC\nLSTM-1 +\nPCA rep.\n0.436 ± 0.01\n0.811 ± 0.004\n0.0975 ± 0.007\n0.78 ± 0.007\nAE rep.\n0.471 ± 0.005\n0.824 ± 0.002\n0.203 ± 0.01\n0.889 ± 0.008∗\nS2S-AE rep.\n0.474 ± 0.006\n0.824 ± 0.003\n0.196 ± 0.02\n0.887 ± 0.004\nS2S-F rep.\n0.477 ± 0.006∗\n0.825 ± 0.003∗\n0.193 ± 0.01\n0.886 ± 0.005\nS2S-F-A rep.\n0.48 ± 0.007\n0.825 ± 0.003∗\n0.201 ± 0.01∗\n0.89 ± 0.009\nLSTM-3 +\nraw signals\n0.438 ± 0.01\n0.834 ± 0.002\n0.181 ± 0.01\n0.89 ± 0.01\nImproved performance in limited data setting\nHere we evaluate how unsupervised representa-\ntions help boost prediction performance in the limited labeled data scenario.\nWe simulate this setting by reducing the quantity of labeled data available for the classiﬁcation\nproblems described in the previous section, with as few as 1% (N = 75 patients) training examples.\nThe results under this varying data scarcity are shown in Figure 3, for the different representation-\nlearning approaches. We also include the prediction performance of classiﬁers, namely LSTM-1 and\nLSTM-3, trained in an end-to-end supervised fashion on the available labeled data, as baselines.\n1%\n5%\n10%\n25%\n50%\n100%\n% of labeled data\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\n0.225\nAUPRC\n24h Mortality\nSupervised (LSTM-3)\nSupervised (LSTM-1)\nLSTM-1 + PCA rep.\nLSTM-1 + AE rep.\nLSTM-1 + S2S-AE rep.\nLSTM-1 + S2S-F rep.\nLSTM-1 + S2S-F-A rep.\n1%\n5%\n10%\n25%\n50%\n100%\n% of labeled data\n0.25\n0.30\n0.35\n0.40\n0.45\nAUPRC\n24h Discharge\nFigure 3: 24h discharge and mortality prediction performance of LSTM-1 using unsupervised representations,\nas well as supervised learning with LSTM-1 and LSTM-3. (There are only 75 labeled patients for training in the\n1% labeled data setting.)\nWe observe from Figure 3 that when labels are scarce, the model trained using time-series repre-\nsentations as input features outperforms the end-to-end supervised model, conﬁrming the beneﬁt of\nunsupervised representation learning in limited data settings. Even when we use all labeled samples\nat our disposal to train a more complex classiﬁer, the best unsupervised representations still lead to a\nbetter performance than supervised representations. For all models, however, performance does not\nsaturate when increasing the training set size, which indicates that the entire regime examined here is\nthe data scarcity regime. Given more data, the purely supervised models might eventually surpass the\nones using learned representations.\n4\n5\nConclusion\nWe have studied the performance of several methods for learning unsupervised representations of\npatient time series, and proposed a new architecture, S2S-F-A, which is optimized for forecasting\nusing an attention mechanism. We empirically showed that in scenarios where labeled medical time\nseries data is scarce, training classiﬁers on unsupervised representations provides performance gains\nover end-to-end supervised learning using raw input signals, thus making effective use of information\navailable in a separate, unlabeled training set. The proposed model, explored for the ﬁrst time in the\ncontext of unsupervised patient representation learning, produces representations with the highest\nperformance in future signal prediction and clinical outcome prediction, exceeding several baselines.\nReferences\n[1] Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discriminative\nunsupervised feature learning with convolutional neural networks. In Z. Ghahramani, M. Welling, C. Cortes,\nN. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27,\npages 766–774. Curran Associates, Inc., 2014.\n[2] Quoc V. Le and Tomas Mikolov.\nDistributed representations of sentences and documents.\nCoRR,\nabs/1405.4053, 2014.\n[3] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed representations of\nwords and phrases and their compositionality. CoRR, abs/1310.4546, 2013.\n[4] Riccardo Miotto, Li Li, Brian A Kidd, and Joel T Dudley. Deep patient: An unsupervised representation to\npredict the future of patients from the electronic health records. Scientiﬁc reports, 6, 2016.\n[5] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new\nperspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798–1828, 2013.\n[6] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked\ndenoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.\nJournal of Machine Learning Research, 11(Dec):3371–3408, 2010.\n[7] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,\n2013.\n[8] Jonathan Masci, Ueli Meier, Dan Cire¸san, and Jürgen Schmidhuber. Stacked convolutional auto-encoders\nfor hierarchical feature extraction. In International Conference on Artiﬁcial Neural Networks, pages 52–59.\nSpringer, 2011.\n[9] Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive auto-encoders:\nExplicit invariance during feature extraction. In Proceedings of the 28th International Conference on\nInternational Conference on Machine Learning, pages 833–840. Omnipress, 2011.\n[10] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In\nAdvances in neural information processing systems, pages 3104–3112, 2014.\n[11] Ron J Weiss, Jan Chorowski, Navdeep Jaitly, Yonghui Wu, and Zhifeng Chen. Sequence-to-sequence\nmodels can directly transcribe foreign speech. arXiv preprint arXiv:1703.08581, 2017.\n[12] Xinlei Chen and C Lawrence Zitnick. Mind’s eye: A recurrent visual representation for image caption\ngeneration. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages\n2422–2431, 2015.\n[13] Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, and Daan Wierstra. Draw: A\nrecurrent neural network for image generation. arXiv preprint arXiv:1502.04623, 2015.\n[14] Prajit Ramachandran, Peter J Liu, and Quoc V Le. Unsupervised pretraining for sequence to sequence\nlearning. arXiv preprint arXiv:1611.02683, 2016.\n[15] Rimma Pivovarov, Adler J Perotte, Edouard Grave, John Angiolillo, Chris H Wiggins, and Noémie Elhadad.\nLearning probabilistic phenotypes from heterogeneous ehr data. Journal of biomedical informatics, 58:156–\n165, 2015.\n[16] Harini Suresh, Peter Szolovits, and Marzyeh Ghassemi. The use of autoencoders for discovering patient\nphenotypes. arXiv preprint arXiv:1703.07004, 2017.\n[17] Corinne L Jones, Sham M Kakade, Lucas W Thornblade, David R Flum, and Abraham D Flaxman. Canoni-\ncal correlation analysis for analyzing sequences of medical billing codes. arXiv preprint arXiv:1612.00516,\n2016.\n5\n[18] Edward Choi, Mohammad Taha Bahadori, Elizabeth Searles, Catherine Coffey, Michael Thompson, James\nBost, Javier Tejedor-Sojo, and Jimeng Sun. Multi-layer representation learning for medical concepts. In\nProceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining, pages 1495–1504. ACM, 2016.\n[19] Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio. Attention-\nbased models for speech recognition. In Advances in neural information processing systems, pages\n577–585, 2015.\n[20] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel,\nand Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In\nInternational Conference on Machine Learning, pages 2048–2057, 2015.\n[21] Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong,\nRomain Paulus, and Richard Socher. Ask me anything: Dynamic memory networks for natural language\nprocessing. In International Conference on Machine Learning, pages 1378–1387, 2016.\n[22] Edward Choi, Mohammad Taha Bahadori, Jimeng Sun, Joshua Kulas, Andy Schuetz, and Walter Stewart.\nRetain: An interpretable predictive model for healthcare using reverse time attention mechanism. In\nAdvances in Neural Information Processing Systems, pages 3504–3512, 2016.\n[23] Myeongjun Jang, Seungwan Seo, and Pilsung Kang. Recurrent neural network-based semantic variational\nautoencoder for sequence-to-sequence learning. CoRR, abs/1802.03238, 2018.\n[24] Sepp Hochreiter. The vanishing gradient problem during learning recurrent neural nets and problem\nsolutions. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6(02):107–116,\n1998.\n[25] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, Jürgen Schmidhuber, et al. Gradient ﬂow in recurrent\nnets: the difﬁculty of learning long-term dependencies, 2001.\n[26] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based\nneural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n[27] Ary L Goldberger, Luis AN Amaral, Leon Glass, Jeffrey M Hausdorff, Plamen Ch Ivanov, Roger G Mark,\nJoseph E Mietus, George B Moody, Chung-Kang Peng, and H Eugene Stanley. Physiobank, physiotoolkit,\nand physionet. Circulation, 101(23):e215–e220, 2000.\n6\nA\nAppendix\nA.1\nData\nThe eICU Collaborative Research Database v1.2 [27] was used for all experiments described in this paper. 94\ntime series variables (shown in Table A1) including periodic and aperiodic vital signs and irregularly measured\nlab tests were extracted from the raw database. A variable was included in our analysis if at least 10% of patients\nin the cohort had at least one record for this variable. As preprocessing, the raw data was resampled to a regular\ntime-grid format with an interval size of 60 minutes, extending from admission to the ICU to dispatch from\nthe unit. During computation of the time grid, rejection of implausible data and imputation were performed\nwith an online algorithm. An observation was rejected if it is a statistical outlier with respect to pre-computed\n5th/95th dataset percentiles. Values on the regular time grid were imputed using a combination of forward ﬁlling,\npersonalized history mean ﬁlling and population median ﬁlling. Forward ﬁlling was used if the last value was\nrecorded no earlier than 1 hour (periodic vital signs), 5 hours (aperiodic vital signs) or 1 day (lab tests) prior to\nthe grid point, respectively. Otherwise, if there have been previous observations of that variable, the mean of all\nsuch observations was used to ﬁll in the time grid point. If there were no observations in a patient’s history, the\ngrid value was ﬁlled with the population median for that variable.\nOverall, the dataset consists of 20878 patients with 72-240 hours of history.\nTable A1: List of 94 selected variables.\neICU Table\nVariables\nvitalPeriodic\ncvp, heartrate, respiration, sao2, st1, st2, st3, systemicdiastolic, systemicmean, systemicsystolic, temperature\nvitalAperiodic\nnoninvasivediastolic, noninvasivemean, noninvasivesystolic\nLab\n-bands, -basos, -eos, -lymphs, -monos, -polys, ALT (SGPT), AST (SGOT), BNP, BUN, Base Deﬁcit, Base Excess,\nCPK, CPK-MB, CPK-MB index, Carboxyhemoglobin, Fe, Ferritin, FiO2, HCO3, HDL, Hct, Hgb, LDL, LPM\nO2, MCH, MCHC, MCV, MPV, Methemoglobin, O2 Content, O2 Sat (%), PT, PT - INR, PTT, RBC, RDW,\nRespiratory Rate, TIBC, TSH, TV, Total CO2, Vancomycin - trough, Vent Rate, Vitamin B12, WBC x 1000,\nWBC’s in urine, albumin, alkaline phos., ammonia, anion gap, bedside glucose, bicarbonate, calcium, chloride,\ncreatinine, direct bilirubin, ﬁbrinogen, glucose, ionized calcium, lactate, lipase, magnesium, pH, paCO2, paO2,\npeep, phosphate, platelets x 1000, potassium, sodium, temporature, total bilirubin, total cholesterol, total protein,\ntriglycerides, troponin - I, troponin - T, urinary sodium, urinary speciﬁc gravity\nA.1.1\nCohort selection\nAmong the >200,000 ICU stays available in the dataset, we included only patients with one stay, such that data\nsplits do not have to be stratiﬁed with respect to patient ID. In the second ﬁltering step, ICU stays shorter than 3\ndays or longer than 10 days were excluded. The ﬁltering yielded a set of 20878 patients/ICU stays.\nA.1.2\nData splits\nFrom the pre-ﬁltered dataset we created 5 replicates of random partitions into train, validation and 2 test sets,\nwith respect to patients, i.e. the entire data of a patient was contained in exactly one of the 4 sets. Size ratios of\n40:40:10:10 for train/validation/test1/test2 sets were used. The training set was used to train the representations,\nthe validation set was used to tune free hyperparameters of the representation method (if any). The classiﬁers\nwere trained on the patient representations obtained from the validation set, optimized its hyperparameters on the\nrepresentations from the ﬁrst test set, and its predictive performance was evaluated on the unseen representations\nfrom the second test set. 5 independent experiments have been performed on the replicates.\nA.2\nRepresentation learning\nFor each representation learning method, representations were extracted from the training set. Feature columns\nwere standard-scaled (subtracting mean / dividing by the standard deviation) before training the models to obtain\nrepresentations. The validation set was used to implement an early stopping heuristic for the training process, in\nthe case of the deep learning models. At this point, all trained representations were saved to disk. For the deep\nlearning models, we used grid search to ﬁnd the best set of hyper-parameters.\nFor basic autoencoders, we train with a mini-batch of 512 randomly sampled records, and for the recurrent\nautoencoders we train with a mini-batch of 4 patients with full history. We use early stopping based on the\nvalidation set loss to avoid overﬁtting, i.e. we stop training if we observe that validation set loss is non-decreasing\nfor 10 consecutive epochs. We additionally use the validation set to perform hyperparameter optimization over\nthe optimal learning rate and activation functions.\n7\nA.3\nRepresentation evaluation\nFor evaluating the future signal and task prediction performance, representations of the ﬁrst 12 hours of a\npatient recording were excluded. In this way the results are not affected by the model-speciﬁc ways of handling\nincomplete histories, which occur at the beginning of the patient stay.\nA.4\nModel complexity\nTable A2 shows the traits of the unsupervised learning models used in the paper. An advantage of Seq2Seq-based\nmodels is that the number of parameters they use does not depend on the length of the input time series to be\ncompressed.\nTable A2: Comparison of used unsupervised representation learning models. T refers to the length of the time\nseries to be encoded (12 in our experiments), d is the dimension of the input data, and m is the dimension of the\nhidden state of the LSTM in the S2S-based models, which is the same as the representation dimension.\nname\nnonlinear\ntemporal\ndecoder output\nattention\nnumber of parameters\nPCA\npast\nO(T md)\nAE\n✓\npast\nO(T md)\nS2S-AE\n✓\n✓\npast\nO(m2 + md)\nS2S-F\n✓\n✓\nfuture\nO(m2 + md)\nS2S-F-A\n✓\n✓\nfuture\n✓\nO(m2 + md)\nx\nA.5\nImpact of representation dimension\nIn this section we investigate the relationship between the dimensionality of representations and their performance\nacross tasks. In the previously described experiments, we used a representation dimension of m = 94, implying\na compression factor of 12 (as the windows consist of 12 hourly measurements of 94 variables). Here we vary\nthe value of m to explore how much compression is possible while retaining prediction performance.\nTable A3 shows the AUROC values using S2S-F-A representations for prediction. Compared with the AUROC\nscores corresponding to using raw features in Table 2, even the S2S-F-A representations with very low dimension\nstill obtain reasonable performance.\nTable A3: AUROC scores of predictions using LSTM-1 classiﬁers on S2S-F-A representations with different\ndimensions.\nAUROC\nS2S-F-A (m=2)\nS2S-F-A (m=50)\nS2S-F-A (m=94)\n24h Discharge\n0.7985 ± 0.003\n0.8028 ± 0.004\n0.825 ± 0.003\n24h Mortality\n0.8398 ± 0.01\n0.8696 ± 0.006\n0.89 ± 0.009\n8\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2018-12-02",
  "updated": "2018-12-02"
}