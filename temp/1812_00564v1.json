{
  "id": "http://arxiv.org/abs/1812.00564v1",
  "title": "Split learning for health: Distributed deep learning without sharing raw patient data",
  "authors": [
    "Praneeth Vepakomma",
    "Otkrist Gupta",
    "Tristan Swedish",
    "Ramesh Raskar"
  ],
  "abstract": "Can health entities collaboratively train deep learning models without\nsharing sensitive raw data? This paper proposes several configurations of a\ndistributed deep learning method called SplitNN to facilitate such\ncollaborations. SplitNN does not share raw data or model details with\ncollaborating institutions. The proposed configurations of splitNN cater to\npractical settings of i) entities holding different modalities of patient data,\nii) centralized and local health entities collaborating on multiple tasks and\niii) learning without sharing labels. We compare performance and resource\nefficiency trade-offs of splitNN and other distributed deep learning methods\nlike federated learning, large batch synchronous stochastic gradient descent\nand show highly encouraging results for splitNN.",
  "text": "Split learning for health: Distributed deep learning\nwithout sharing raw patient data\nPraneeth Vepakomma∗\nMassachusetts Institute of Technology\nCambridge, MA 02139\nOtkrist Gupta\nMassachusetts Institute of Technology\nCambridge, MA 02139\nTristan Swedish\nMassachusetts Institute of Technology\nCambridge, MA 02139\nRamesh Raskar\nMassachusetts Institute of Technology\nCambridge, MA 02139\nAbstract\nCan health entities collaboratively train deep learning models without sharing sen-\nsitive raw data? This paper proposes several conﬁgurations of a distributed deep\nlearning method called SplitNN to facilitate such collaborations. SplitNN does\nnot share raw data or model details with collaborating institutions. The proposed\nconﬁgurations of splitNN cater to practical settings of i) entities holding different\nmodalities of patient data, ii) centralized and local health entities collaborating on\nmultiple tasks and iii) learning without sharing labels. We compare performance\nand resource efﬁciency trade-offs of splitNN and other distributed deep learning\nmethods like federated learning, large batch synchronous stochastic gradient de-\nscent and show highly encouraging results for splitNN.\n1\nIntroduction\nCollaboration in health is heavily impeded by lack of trust, data sharing regulations such as HIPAA\n[3, 4, 5, 6, 7] and limited consent of patients. In settings where different institutions hold differ-\nent modalities of patient data in the form of electronic health records (EHR), picture archiving and\ncommunication systems (PACS) for radiology and other imaging data, pathology test results, or\nother sensitive data such as genetic markers for disease, collaborative training of distributed ma-\nchine learning models without any data sharing is desired. Deep learning methods in general have\nfound a pervasive suite of applications in biology, clinical medicine, genomics and public health as\nsurveyed in [24, 23, 25, 29, 30, 31]. Training of distributed deep learning models without sharing\nmodel architectures and parameters in addition to not sharing raw data is needed to prevent undesir-\nable scrutiny by other entities. As a concrete health example, consider the use case of training a deep\nlearning model for patient diagnosis via collaboration of two entities holding pathology test results\nand radiology data respectively. These entities are unable to share their raw data with each other\ndue to the concerns noted above. That said, diagnostic performance of the distributed deep learning\nmodel is highly contingent on being able to use data from both the institutions for its training. In ad-\ndition to such multi-modal settings, this problem also manifests in settings with entities holding data\nof the same modality as shown in Fig 1 below. As illustrated, local hospitals or tele-health screening\ncenters do not acquire an enormous number of diagnostic images on their own. These entitites may\nalso be limited by diagnostic manpower. A distributed machine learning method for diagnosis in\nthis setting would enable each individual center to contribute data to an aggregate model without\nsharing any raw data. This conﬁguration can achieve high accuracy while using signiﬁcantly lower\n∗Corresponding author, e-mail: vepakom@mit.edu\n32nd Conference on Neural Information Processing Systems (NIPS 2018), Montr´eal, Canada.\narXiv:1812.00564v1  [cs.LG]  3 Dec 2018\n(a) Non-cooperating health units\n(b) Distributed learning without raw data sharing\nFigure 1: Distributed learning over retinopathy images (or undetected fast moving threats) over slow\nbit-rate (snail-pace), to detect the emerging threat by pooling their images but without exchanging\nraw patient data.\ncomputational resources and communication bandwidth than previously proposed approaches. This\nenables smaller hospitals to effectively serve those in need while also beneﬁting the distributed train-\ning network as a whole. In this paper, we build upon splitNN introduced in [32] to propose speciﬁc\nconﬁgurations that cater to practical health settings such as these and furthermore as described in\nthe sections below.\n1.1\nRelated work:\nIn addition to splitNN [32], techniques of federated deep learning [1] and large batch synchronous\nstochastic gradient descent (SGD)[19] are currently available approaches for distributed deep learn-\ning. There has been no work as yet on federated deep learning and large batch synchronous SGD\nmethods with regards to their applicability to useful non-vanilla settings of distributed deep learn-\ning studied in rest of this paper such as a) distributed deep learning with vertically partitioned data,\nb) distributed deep learning without label sharing, c) distributed semi-supervised learning and d)\ndistributed multi-task learning. That said, with regards to non-neural network based federated learn-\ning techniques, the work in [27] shows their applicability to vertically partitioned distributed data\n[33, 34, 26, 35] shows applicability to multi-task learning in distributed settings. We now propose\nconﬁgurations of splitNN for all these useful settings in the rest of this paper.\n2\nSplitNN conﬁgurations for health\nIn this section we propose several conﬁgurations of splitNN for various practical health settings:\nSimple vanilla conﬁguration for split learning: This is the simplest of splitNN conﬁgura-\ntions as shown in Fig 2a. In this setting each client, (for example, radiology center) trains a partial\ndeep network up to a speciﬁc layer known as the cut layer. The outputs at the cut layer are sent to a\nserver which completes the rest of the training without looking at raw data (radiology images) from\nclients. This completes a round of forward propagation without sharing raw data. The gradients\nare now back propagated at the server from its last layer until the cut layer. The gradients at the\ncut layer (and only these gradients) are sent back to radiology client centers. The rest of back\npropagation is now completed at the radiology client centers. This process is continued until the\ndistributed split learning network is trained without looking at each others raw data.\nU-shaped conﬁgurations for split learning without label sharing: The other two conﬁgu-\nrations described in this section involve sharing of labels although they do not share any raw input\ndata with each other. We can completely mitigate this problem by a U-shaped conﬁguration that\ndoes not require any label sharing by clients. In this setup we wrap the network around at end layers\nof servers network and send the outputs back to client entities as seen in Fig.2b. While the server\nstill retains a majority of its layers, the clients generate the gradients from the end layers and use\n2\n(a) Simple vanilla split learning\n(b) Split learning without label\nsharing\n(c) Split learning for vertically\npartitioned data\nFigure 2: Split learning conﬁgurations for health shows raw data is not transferred between the\nclient and server health entities for training and inference of distributed deep learning models with\nSplitNN.\nthem for backpropagation without sharing the corresponding labels. In cases where labels include\nhighly sensitive information like the disease status of patients, this setup is ideal for distributed deep\nlearning.\nVertically partitioned data for split learning: This conﬁguration allows for multiple insti-\ntutions holding different modalities of patient data [27, 33, 34] to learn distributed models without\ndata sharing. In Fig. 2c, we show an example conﬁgurations of splitNN suitable for such multi-\nmodal multi-institutional collaboration. As a concrete example we walkthrough the case where\nradiology centers collaborate with pathology test centers and a server for disease diagnosis. As\nshown in Fig. 2c radiology centers holding imaging data modalities train a partial model upto the\ncut layer. In the same way the pathology test center having patient test results trains a partial model\nupto its own cut layer. The outputs at the cut layer from both these centers are then concatenated\nand sent to the disease diagnosis server that trains the rest of the model. This process is continued\nback and forth to complete the forward and backward propagations in order to train the distributed\ndeep learning model without sharing each others raw data. We would like to note that although\nthese example conﬁgurations show some versatile applications for splitNN, they are by no means\nthe only possible conﬁgurations.\n3\nResults about resource efﬁciency\nWe share a comparison from [32] of validation accuracy and required client computational resources\nin Figure 3 for the three techniques of federated learning, large batch synchronous SGD and splitNN\nas they are tailored for distributed deep learning. As seen in this ﬁgure, the comparisons were done\non the CIFAR 10 and CIFAR 100 datasets using VGG and Resnet-50 architectures for 100 and 500\nclient based setups respectively. In this distributed learning experiment we clearly see that SplitNN\noutperforms the techniques of federated learning and large batch synchronous SGD in terms of\nhigher accuracies with drastically lower computational requirements on the side of clients. In tables\n1 and 2 we share more comparisons from [32] on computing resources in TFlops and communica-\ntion bandwidth in GB required by these techniques. SplitNN again has a drastic improvement of\ncomputational resource efﬁciency on the client side. In the case with a relatively smaller number\nof clients the communication bandwidth required by federated learning is less than splitNN. These\nimprovements on the client side resource efﬁciency are even more dramatic due to the presence of a\nsmaller number of parameters in earlier layers of convolutional neural networks (CNNs) like VGG\nand Resnet in addition to the fact that computation is split due to the cut layers. This uneven distri-\n3\n(a) Accuracy vs client-side ﬂops on 100 clients with\nVGG on CIFAR 10\n(b) Accuracy vs client-side ﬂops on 500 clients with\nResnet-50 on CIFAR 100\nFigure 3: We show dramatic reduction in computational burden (in tﬂops) while maintaining higher\naccuracies when training over large number of clients with splitNN. Blue line denotes distributed\ndeep learning using splitNN, red line indicate federated averaging and green line indicates large\nbatch SGD.\nbution of network parameters holds for the vast majority of modern CNNs, a property that SplitNN\ncan effectively exploit.\nMethod\n100 Clients\n500 Clients\nLarge Batch SGD\n29.4 TFlops\n5.89 TFlops\nFederated Learning\n29.4 TFlops\n5.89 TFlops\nSplitNN\n0.1548 TFlops\n0.03 TFlops\nTable 1: Computation resources consumed per client when training CIFAR 10 over VGG (in ter-\naﬂops) are drastically lower for SplitNN than Large Batch SGD and Federated Learning.\nMethod\n100 Clients\n500 Clients\nLarge Batch SGD\n13 GB\n14 GB\nFederated Learning\n3 GB\n2.4 GB\nSplitNN\n6 GB\n1.2 GB\nTable 2: Computation bandwidth required per client when training CIFAR 100 over ResNet (in\ngigabytes) is lower for splitNN than large batch SGD and federated learning with a large number of\nclients. For setups with a smaller number of clients, federated learning requires a lower bandwidth\nthan splitNN. Large batch SGD methods popular in data centers use a heavy bandwidth in both\nsettings.\n4\nConclusion and future work\nSimple conﬁgurations of distributed deep learning do not sufﬁce for various practical setups of col-\nlaboration across health entities. We propose novel conﬁgurations of a recently proposed distributed\ndeep learning technique called splitNN that is dramatically resource efﬁcient in comparison to cur-\nrently available distributed deep learning methods of federated learning and large batch synchronous\nSGD. SplitNN is versatile in allowing for many plug and play conﬁgurations based on the required\napplication. Generaton of such novel conﬁgurations in health and beyond is a good avenue for future\n4\nwork. SplitNN is also scalable to large-scale settings and can use any state of the art deep learning\narchitectures. In addition, the boundaries of resource efﬁciency can be pushed further in distributed\ndeep learning by combining splitNN with neural network compression methods [36, 37, 38] for\nseamless distributed learning with edge devices.\n5\nSupplementary material:\n5.1\nAdditional conﬁgurations\nIn this supplementary section we propose some more split learning conﬁgurations of splitNN for\nversatile collaborations in health to train and infer from distributed deep learning models without\nsharing raw patient data.\nExtended vanilla split learning: As shown in Fig. 4a we give another modiﬁcation of vanilla\nsplit learning where the result of concatenated outputs is further processed at another client before\npassing it to the server.\nConﬁgurations for multi-task split learning:\nAs shown in Fig.\n4b, in this conﬁguration\nmulti-modal data from different clients is used to train partial networks up to their corresponding\ncut layers. The outputs from each of these cut layers are concatenated and then sent over to multiple\nservers. These are used by each server to train multiple models that solve different supervised\nlearning tasks.\nTor [28] like conﬁguration for multi-hop split learning:\nThis conﬁguration is an analo-\ngous extension of the vanilla conﬁguration. In this setting multiple clients train partial networks in\nsequence where each client trains up to a cut layer and transfers its outputs to the next client. This\nprocess is continued as shown in Fig. 4c as the ﬁnal client sends its activations from its cut layer to\na server to complete the training.\n(a) Extended vanilla split learning\n(b) Split learning for multi-task\noutput with vertically partitioned\ninput\n(c) ’Tor’[28] like multi-hop split\nlearning\nFigure 4: Split learning conﬁgurations for health shows raw data is not transferred between the\nclient and server health entities for training and inference of distributed deep learning models with\nSplitNN.\nReferences\n[1] McMahan, H. B. , Moore, E., Ramage, D., Hampson, S. and Aguera y Arcas, B.,\nCommunication-efﬁcient learning of deep networks from decentralized data, 20’th Interna-\ntional Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2017.\n[2] Swedish, T. and Raskar, R., Deep Visual Teach and Repeat on Path Networks, The IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2018.\n5\n[3] Annas, George J. , HIPAA regulations-a new era of medical-record privacy?, New England\nJournal of Medicine, Vol.348 (15), pp.1486–1490, 2003.\n[4] U.S. Centers for Disease Control and Prevention , HIPAA privacy rule and public health. Guid-\nance from CDC and the US Department of Health and Human Services, MMWR: Morbidity\nand mortality weekly report, Vol.52 (1), pp.1–17, 2003.\n[5] Mercuri, Rebecca T. , The HIPAA-potamus in health care data security, Communications of\nthe ACM, Vol.47 (7), pp.25–28, 2004.\n[6] Gostin, Lawrence O., Levit, Laura A. and Nass, Sharyl J. , Beyond the HIPAA privacy rule:\nenhancing privacy, improving health through research, National Academies Press, 2009.\n[7] Luxton, David D and Kayl, Robert A and Mishkind, Matthew C. , mHealth data security: The\nneed for HIPAA-compliant standardization, Telemedicine and e-Health, Vol.18(4), pp. 284–\n288, 2012.\n[8] Koneˇcny, Jakub and McMahan, H Brendan and Yu, Felix X and Richt´arik, Peter and Suresh,\nAnanda Theertha and Bacon, D. , Federated learning: Strategies for improving communication\nefﬁciency, arXiv preprint arXiv:1610.05492, 2016.\n[9] Hynes, Nick and Cheng, Raymond and Song, Dawn , Efﬁcient Deep Learning on Multi-Source\nPrivate Data, arXiv preprint arXiv:1807.06689, 2018.\n[10] Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov,\nIlya and Talwar, Kunal and Zhang, Li, Deep learning with differential privacy, Proceedings of\nthe 2016 ACM SIGSAC Conference on Computer and Communications Security, pp.308–318,\n2016.\n[11] Shokri, Reza and Shmatikov, Vitaly, Privacy-preserving deep learning, Proceedings of the 22nd\nACM SIGSAC conference on computer and communications security, pp.1310–1321 2015.\n[12] Papernot, Nicolas and Abadi, Mart´ın and Erlingsson, Ulfar and Goodfellow, Ian and Talwar,\nKunal, Semi-supervised knowledge transfer for deep learning from private training data, arXiv\npreprint arXiv:1610.05755, 2016.\n[13] Geyer, Robin C and Klein, Tassilo and Nabi, Moin, Differentially private federated learning:\nA client level perspective, arXiv preprint arXiv:1712.07557, 2017.\n[14] Rouhani, Bita Darvish and Riazi, M Sadegh and Koushanfar, Farinaz, Deepsecure: Scalable\nprovably-secure deep learning, arXiv preprint arXiv:1705.08963, 2017.\n[15] Rouhani, Bita Darvish and Riazi, M Sadegh and Koushanfar, Farinaz, SecureML: A system for\nscalable privacy-preserving machine learning, 38th IEEE Symposium on Security and Privacy\n(SP), 2017.\n[16] Hesamifard, Ehsan and Takabi, Hassan and Ghasemi, Mehdi, CryptoDL: Deep Neural Net-\nworks over Encrypted Data, arXiv preprint arXiv:1711.05189, 2017.\n[17] Hardy, Stephen and Henecka, Wilko and Ivey-Law, Hamish and Nock, Richard and Patrini,\nGiorgio and Smith, Guillaume and Thorne, Brian, Private federated learning on vertically\npartitioned data via entity resolution and additively homomorphic encryption, arXiv preprint\narXiv:1711.10677, 2017.\n[18] Aono, Yoshinori and Hayashi, Takuya and Wang, Lihua and Moriai, Shiho, Privacy-preserving\ndeep learning via additively homomorphic encryptionn, IEEE Transactions on Information\nForensics and Security, Vol.13(5), pp.1333–1345arXiv preprint arXiv:1711.10677, 2018.\n[19] Chen, Jianmin and Pan, Xinghao and Monga, Rajat and Bengio, Samy and Jozefowicz, Rafal,\nRevisiting distributed synchronous SGD, IEEE Transactions on Information Forensics and Se-\ncurity, Vol.13(5), arXiv preprint arXiv:1604.00981, 2016.\n[20] Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMa-\nhan, H Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn,\nPractical secure aggregation for privacy-preserving machine learning, Proceedings of the 2017\nACM SIGSAC Conference on Computer and Communications Security, pp.1175–1191, 2017.\n[21] Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMa-\nhan, H Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn,\nPractical secure aggregation for privacy-preserving machine learning, Proceedings of the 2017\nACM SIGSAC Conference on Computer and Communications Security, pp.1175–1191, 2017.\n6\n[22] Ben-Nun, Tal and Hoeﬂer, Torsten, Demystifying Parallel and Distributed Deep Learning: An\nIn-Depth Concurrency Analysis, arXiv preprint arXiv:1802.09941, 2018.\n[23] Shickel, Benjamin and Tighe, Patrick James and Bihorac, Azra and Rashidi, Parisa, Deep\nEHR: A survey of recent advances in deep learning techniques for electronic health record\n(EHR) analysis, IEEE journal of biomedical and health informatics, Vol.22(5) pp.1589–1604,\n2018.\n[24] Ching, Travers and Himmelstein, Daniel S and Beaulieu-Jones, Brett K and Kalinin, Alexandr\nA and Do, Brian T and Way, Gregory P and Ferrero, Enrico and Agapow, Paul-Michael and\nZietz, Michael and Hoffman, Michael M , Opportunities and obstacles for deep learning in\nbiology and medicine, Journal of The Royal Society Interface, Vol.15(141), 2018.\n[25] Miotto, Riccardo and Wang, Fei and Wang, Shuang and Jiang, Xiaoqian and Dudley, Joel T.,\nDeep learning for healthcare: review, opportunities and challenges, Brieﬁngs in bioinformatics,\n2017.\n[26] Smith, Virginia and Chiang, Chao-Kai and Sanjabi, Maziar and Talwalkar, Ameet S., Advances\nin Neural Information Processing Systems, pp.4424–4434, 2017.\n[27] Hardy, Stephen and Henecka, Wilko and Ivey-Law, Hamish and Nock, Richard and Patrini,\nGiorgio and Smith, Guillaume and Thorne, Brian, Private federated learning on vertically\npartitioned data via entity resolution and additively homomorphic encryption, arXiv preprint\narXiv:1711.10677, 2017.\n[28] Syverson, Paul and Dingledine, R and Mathewson, N, Tor: The second generation onion\nrouter,Usenix Security, 2004.\n[29] Ravı, Daniele and Wong, Charence and Deligianni, Fani and Berthelot, Melissa and Andreu-\nPerez, Javier and Lo, Benny and Yang, Guang-Zhong, Deep learning for health informatics,\nIEEE journal of biomedical and health informatics, Vol.21(1), pp.4–21, 2017.\n[30] Alipanahi, Babak and Delong, Andrew and Weirauch, Matthew T and Frey, Brendan J, Pre-\ndicting the sequence speciﬁcities of DNA-and RNA-binding proteins by deep learning, Nature\nbiotechnology, Vol.33(8), 2015.\n[31] Litjens, Geert and Kooi, Thijs and Bejnordi, Babak Ehteshami and Setio, Arnaud Arindra\nAdiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and van der Laak, Jeroen AWM\nand Van Ginneken, Bram and S´anchez, Clara I, A survey on deep learning in medical image\nanalysis, Medical image analysis, Vol.42, pp.60–88, 2017.\n[32] Gupta, Otkrist and Raskar, Ramesh, Distributed learning of deep neural network over multiple\nagents, Journal of Network and Computer Applications, Vol.116, pp.1–8, 2018.\n[33] Navathe, Shamkant and Ceri, Stefano and Wiederhold, Gio and Dou, Jinglie, Vertical partition-\ning algorithms for database design, ACM Transactions on Database Systems (TODS), Vol.9(4),\npp.680–710, 1984.\n[34] Agrawal, Sanjay and Narasayya, Vivek and Yang, Beverly, Integrating vertical and horizontal\npartitioning into automated physical database design, Proceedings of the 2004 ACM SIGMOD\ninternational conference on Management of data, pp.359–370, 2004.\n[35] Abadi, Daniel J and Marcus, Adam and Madden, Samuel R and Hollenbach, Kate, Scalable se-\nmantic web data management using vertical partitioning, Proceedings of the 33rd international\nconference on Very large data bases, pp.411–422, 2007.\n[36] Lin, Yujun and Han, Song and Mao, Huizi and Wang, Yu and Dally, William J, Deep gradient\ncompression: Reducing the communication bandwidth for distributed training, arXiv preprint\narXiv:1712.01887, 2017.\n[37] Louizos, Christos and Ullrich, Karen and Welling, Max, Bayesian compression for deep learn-\ning, Advances in Neural Information Processing Systems, pp.3288–3298, 2017.\n[38] Han, Song and Mao, Huizi and Dally, William J, Deep compression: Compressing deep\nneural networks with pruning, trained quantization and huffman coding, arXiv preprint\narXiv:1510.00149, 2015.\n[39] Fung, Clement and Yoon, Chris JM and Beschastnikh, Ivan, Mitigating Sybils in Federated\nLearning Poisoning, arXiv preprint arXiv:1808.04866, 2018.\n7\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2018-12-03",
  "updated": "2018-12-03"
}