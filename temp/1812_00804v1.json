{
  "id": "http://arxiv.org/abs/1812.00804v1",
  "title": "Deep Inverse Optimization",
  "authors": [
    "Yingcong Tan",
    "Andrew Delong",
    "Daria Terekhov"
  ],
  "abstract": "Given a set of observations generated by an optimization process, the goal of\ninverse optimization is to determine likely parameters of that process. We cast\ninverse optimization as a form of deep learning. Our method, called deep\ninverse optimization, is to unroll an iterative optimization process and then\nuse backpropagation to learn parameters that generate the observations. We\ndemonstrate that by backpropagating through the interior point algorithm we can\nlearn the coefficients determining the cost vector and the constraints,\nindependently or jointly, for both non-parametric and parametric linear\nprograms, starting from one or multiple observations. With this approach,\ninverse optimization can leverage concepts and algorithms from deep learning.",
  "text": "Deep Inverse Optimization\nYingcong Tan1, Andrew Delong2, and Daria Terekhov1\n1 Department of Mechanical, Industrial and Aerospace Engineering,\nConcordia University\nt yingco@encs.concordia.ca\ndaria.terekhov@concordia.ca\n2 andrew.delong@gmail.com\nAbstract. Given a set of observations generated by an optimization pro-\ncess, the goal of inverse optimization is to determine likely parameters\nof that process. We cast inverse optimization as a form of deep learning.\nOur method, called deep inverse optimization, is to unroll an iterative\noptimization process and then use backpropagation to learn parameters\nthat generate the observations. We demonstrate that by backpropagating\nthrough the interior point algorithm we can learn the coeﬃcients deter-\nmining the cost vector and the constraints, independently or jointly, for\nboth non-parametric and parametric linear programs, starting from one\nor multiple observations. With this approach, inverse optimization can\nleverage concepts and algorithms from deep learning.\nKeywords: inverse optimization · deep learning · interior point\n1\nIntroduction\nThe potential for synergy between optimization and machine learning is well-\nrecognized [6], with recent examples including [8,18,26]. Our work uses machine\nlearning for inverse optimization. Consider a parametric linear optimization\nproblem, PLP(u, w):\nminimize\nx\nc(u, w)′x\nsubject to\nA(u, w)x\n≤b(u, w),\n(1)\nwhere x ∈Rd and c(u, w) ∈Rd, A(u, w) ∈Rd×m and b(u, w) ∈Rm are\nall functions of features u and weights w. Let xn\ntru be an optimal solution to\nPLP(un, wtru). Given a set of observed optimal solutions, {x1\ntru, x2\ntru, . . . , xN\ntru},\nfor observed conditions {u1, u2, . . . , uN}, the goal of inverse optimization (IO)\nis to determine values of optimization process parameters w that generated the\nobserved optimal solutions. Applications of IO range from medicine (e.g., imput-\ning the importance of treatment sub-objectives from clinically-approved radio-\ntherapy plans [11]) to energy (e.g., predicting the behaviour of price-responsive\ncustomers [31]).\nFundamentally, IO problems are learning problems: each un is a feature vec-\ntor and xn\ntru is its corresponding target; the goal is to learn model parameters\narXiv:1812.00804v1  [cs.LG]  3 Dec 2018\n2\nY. Tan et al.\nw that minimize some loss function. In this paper, we cast inverse optimization\nas a form of deep learning. Our method, called deep inverse optimization, is to\nunroll an iterative optimization process and then use backpropagation to learn\nmodel parameters that generate the observations/targets.\nc(u, wtru)\nTrue parametric LP\nInitial parametric LP\nLearned parametric LP\n(i) Learning c only\ncini\nclrn\nctru\nxtru\nc(u, wini)\nc(u, wlrn)\n(ii) Learning c, A, b jointly\nxlrn\nxini\nx(u , wtru)\nloss = 0.45\nloss = 0.00\nclrn\nctru\ncini\nxtru\n(iii) Learning weights w of a parametric LP from multiple points\nFig. 1. Three IO learning tasks in non-parametric and parametric linear programs.\nFigure 1 shows the actual result of applying our deep IO method to three\ninverse optimization learning tasks. The top panel illustrates the non-parametric,\nsingle-point variant of model (1) — the case when exactly one xtru is given — a\nclassical problem in IO (see [1,12]). In Figure 1 (i), only c needs to be learned:\nstarting from an initial cost vector cini, our method ﬁnds clrn which makes xtru an\noptimal solution of the LP by minimizing ∥xtru−xlrn∥2. In Figure 1 (ii), starting\nfrom cini, Aini and bini, our approach ﬁnds clrn, Alrn and blrn which make xtru\nan optimal solution of the learned LP through minimizing ∥xtru −xlrn∥2.\nFigure 1 (iii) shows learning w = [w0, w1] for the parametric problem instance\nminimize\nx\ncos(w0 + w1u)x1 + sin(w0 + w1u)x2\nsubject to\n−x1 ≤0.2w0u,\n−x2 ≤−0.2w1u,\nw0x1 + (1 + 1\n3w1u)x2 ≤w0 + 0.1u.\n(2)\nDeep Inverse Optimization\n3\nStarting from wini = [0.2, 0.4] with a loss (mean squared error) of 0.45, our\nmethod is able to ﬁnd wlrn = [1.0, 1.0] with a loss of zero, thereby making xn\ntru\noptimal solutions of (2) for u values {−1.5, −0.5, 0.5, 1.5}. Given newly observed\nu values, in this example wlrn would predict correct decisions. In other words,\nthe learned model generalizes well.\nThe contributions of this paper are as follows. We propose a general frame-\nwork for inverse optimization based on deep learning. This framework is applica-\nble to learning coeﬃcients of the objective function and constraints, individually\nor jointly; minimizing a general loss function; learning from a single or multiple\nobservations; and solving both non-parametric and parametric problems. As a\nproof of concept, we demonstrate that our method obtains eﬀectively zero loss\non many randomly generated linear programs for all three types of learning tasks\nshown in Figure 1, and always improves the loss signiﬁcantly. Such a numerical\nstudy on randomly generated non-parameteric and parametric linear programs\nwith multiple learnable parameters has not previously been published for any\nIO method in the literature. Finally, to the best of our knowledge, we are the\nﬁrst to use unrolling and backpropagation for constrained inverse optimization.\nWe explain how our approach diﬀers from methods in inverse optimization\nand machine learning in Section 2. We present our deep IO framework in Sec-\ntion 3 and our experimental results in Section 4. Section 5 discusses both the\ngenerality and the limitations of our work, and Section 6 concludes the paper.\n2\nRelated Work\nThe goal of our paper is to develop a general-purpose IO approach that is ap-\nplicable to problems for which theoretical guarantees or eﬃcient exact opti-\nmization approaches are diﬃcult or impossible to develop. Naturally, such a\ngeneral-purpose approach will not be the method of choice for all classes of IO\nproblems. In particular, for non-parametric linear programs, closed-form solu-\ntions for learning the c vector (Figure 1 (i)) and for learning the constraint\ncoeﬃcients have been derived by Chan et al. [12,14] and Chan and Kaw [13],\nrespectively. However, learning objective and constraint coeﬃcients jointly (Fig-\nure 1 (ii)) has, to date, received little attention. To the best of our knowledge,\nthis task has been investigated only by Troutt et al. [36,37], who referred to it\nas linear system identiﬁcation, using a maximum likelihood approach. However,\ntheir approach was limited to two dimensions [37] or required the coeﬃcients to\nbe non-negative [36].\nIn the parametric optimization setting, Keshavarz et al. [22] develop an opti-\nmization model that encodes KKT optimality conditions for imputing objective\nfunction coeﬃcients of a convex optimization problem. Aswani et al. [3] focus\non the same problem under the assumption of noisy measurements, developing\na bilevel problem and two algorithms which are shown to maintain statistical\nconsistency. Saez-Gallego and Morales [31] address the case of learning c and\nb jointly in a parametric setting where the b vector is assumed to be an aﬃne\nfunction of a regressor. The general case of learning the weights of a parametric\n4\nY. Tan et al.\nlinear optimization problem (1) where c, A and b are functions of u (Figure 1\n(iii)) has not been addressed in the literature.\nRecent work in machine learning [4,5,16] views inverse optimization through\nthe lens of online learning, where new observations appear over time rather than\nas one batch. Our approach may be applicable in online settings, but we focus\non generality in the batch setting and do not investigate real-time cases.\nMethodologically, our unrolling strategy is similar to McLaurin et al. [24]\nwho directly optimize the hyperparameters of a neural network training pro-\ncedure with gradient descent. Conceptually, the closest papers to our work are\nby Amos and Kolter [2] and Donti, Amos and Kolter [17]. However, these pa-\npers are written independently of the inverse optimization literature. Amos and\nKolter [2] present the OptNet framework, which integrates a quadratic optimiza-\ntion layer in a deep neural network. The gradients for updating the coeﬃcients\nof the optimization problem are derived through implicit diﬀerentiation. This\napproach involves taking matrix diﬀerentials of the KKT conditions for the op-\ntimization problem in question, while our strategy is based on allowing a deep\nlearning framework to unroll an existing optimization procedure. Their method\nhas eﬃciency advantages, while our unrolling approach is easily applicable, in-\ncluding to processes for which the KKT conditions may not hold or are diﬃcult\nto implicitly diﬀerentiate. We include a more in-depth discussion in Section 5.\n3\nDeep Learning Framework for Inverse Optimization\nThe problems studied in inverse optimization are learning problems: given fea-\ntures un and corresponding targets xn\ntru, the goal is to learn parameters of a\nforward optimization model that generate xn\ntru as its optimal solutions. A com-\nplementary view is that inverse optimization is a learning technique specialized\nto the case when the observed data is coming from an optimization process.\nGiven this perspective on inverse optimization and motivated by the success of\ndeep learning for a variety of learning tasks in recent years (see [23]), this paper\ndevelops a deep learning framework for inverse optimization problems.\nDeep learning is a set techniques for training the parameters of a sequence\nof transformations (layers) chained together. The more intermediate layers, the\n‘deeper’ the architecture. We refer the reader to the textbook by Goodfellow,\nBengio and Courville [19] for additional details about deep learning. The features\nof the intermediate layers can be trained/learned through backpropagation, an\nautomatic diﬀerentiation technique that computes the gradient of an output with\nrespect to its input through the layers of a neural network, starting from the ﬁnal\nlayer all the way to the initial one. This method eﬃciently computes an update\nto the weights of the model [30]. Importantly, current machine learning libraries\nsuch as PyTorch provide built-in backpropagation capabilities [28] that allow\nfor wider use of deep learning. Thus, our deep inverse optimization framework\niterates between solving the forward optimization problem using an iterative\noptimization algorithm and backpropagating through the steps (layers) of that\nDeep Inverse Optimization\n5\nalgorithm to improve the estimates of learnable parameters (weights) of the\nforward process.\nAlgorithm 1 Deep inverse optimization framework.\nInput: wini; (un, xn\ntru) for n = 1, .. N,\nOutput: wlrn\n1: w ←wini\n2: for s in 1 .. max steps do\n3:\n∆w ←0\n4:\nfor n in 1 .. N do\n5:\nx ←FO(un, w)\n▷Solve forward problem\n6:\nℓ←L(x, xn\ntru)\n▷Compute loss\n7:\n∆w ←∆w + ∂ℓ\n∂w\n▷Accumulate gradient by backprop\n8:\nend for\n9:\nβ ←line search(w, α · ∆w\nN )\n▷Find safe step size\n10:\nw ←w −βα · ∆w\nN\n▷Update weights\n11: end for\n12: Return w\nOur approach, shown in Algorithm 1, takes the pairs (un, xn\ntru), n = 1, .., N,\nas input, and starts by initializing w = wini. For each n, the forward optimization\nproblem (FO) is solved with the current weights (line 5), and the loss between\nthe resulting optimal solution x and xtru is computed (line 6). The gradient of\nthe loss function with respect to w is computed by backpropagation through\nthe layers of the forward process. In line 9, line search is used to determine\nthe step size, β, for updating the weights: β is reduced by half if infeasibility\nor unboundedness is encountered until a value is found that will lead to loss\nreduction or β < 10−8, in which case early algorithm termination is triggered.\nFinally, in line 10, the weights are updated using the average gradient, step size\nβ, and α, a vector representing the component-wise learning rates for w.\n(i) IPM forward process\nc(u,w)\nx(u,wtru)\nx(1)\nx(u,w)\nx(2)\nu\nw\nc(u,w)\nA(u,w)\nb(u,w)\nx(1)\nx(2)\nx(u,w)\n(ii) Deep inverse optimization through IPM\nx(u,wtru)\nloss(u,w)\n...\nNewton step\nforward instance\nfeasible point\nfeatures\nparams\ngradient ∆w\ntarget\nbackprop\nFig. 2. Deep inverse optimization framework.\nImportantly, our framework is applicable in the context of any diﬀerentiable,\niterative forward optimization procedure. In principle, parameter gradients are\n6\nY. Tan et al.\nautomatically computable even with non-linear constraints or non-linear objec-\ntives, so long as they can be expressed through standard diﬀerentiable primitives.\nOur particular implementation uses the barrier interior point method (IPM) as\ndescribed by Boyd and Vandenberghe [9], as our forward optimization solver.\nThe IPM forward process is illustrated in Figure 2 (i): the central path taken\nby IPM is illustrated for the current u and w, which deﬁne both the current\nfeasible region and the current c(u, w). As shown in Figure 2 (ii), backpropaga-\ntion starts from the computation of the loss function between a (near) optimal\nforward optimization solution x(u, w) and the target x(u, wtru) and proceeds\nbackward through all the steps of IPM, i.e., x(u, w) to x(1), the starting point of\nIPM, to the forward instance parameters and ﬁnally w to compute ∆w. In prac-\ntice, backpropagating all the way to x(1) may not be necessary for computing\naccurate gradients; see Section 5.\nThe framework requires setting three main hyperparameters: wini, the initial\nweight vector; max steps, the total number of steps allotted to the training;\nand α, the learning rates for the diﬀerent components of w. The number of\nadditional hyperparameters depends on the forward optimization process.\n4\nExperimental Results\nIn this section, we demonstrate the application of our framework on randomly-\ngenerated LPs for the three types of problems shown in Figure 1: learning c in\nthe non-parametric case; learning c, A and b together in the non-parametric\ncase; and learning w in the parametric case.\nImplementation Our framework is implemented in Python, using PyTorch\nversion 0.4.1 and its built-in backpropagation capabilities [28]. All numerical op-\nerations are carried out with PyTorch tensors and standard PyTorch primitives,\nincluding the matrix inversion at the heart of the Newton step.\nHyperparameters We limit learning to max steps = 200 in all experiments.\nFour additional hyperparameters are set in each experiment:\n– ϵ, which controls the precision and termination of IPM;\n– t(0): the initial value of the barrier IPM sharpness parameter t;\n– µ: the factor by which t is increased along the IPM central path;\n– α: the vector of per-parameter learning rates, which in some experiments is\nbroken down into αc and αAb.\nIn all experiments, the ϵ hyperparameter is either a constant 10−5 or decays\nexponentially from 0.1 to 10−5 during learning. The decay is a form of graduated\noptimization [7], and tends to help performance when using the MSE loss.\nDeep Inverse Optimization\n7\nBaseline LPs To generate problem instances, we ﬁrst create a set of baseline\nLPs with d variables and m constraints by sampling at least d random points\nfrom N(0, 1), and then construct the convex hull via the scipy.spatial.convexhull\npackage [29]. We generate 50 LP instances for each of the following six problem\nsizes: d = 2 and m ∈{4, 8, 16} and d = 10, m ∈{20, 36, 80}. Our experi-\nments focus on inequality constraints. We observed that our method can work\nfor equality constrained instances, but we did not systematically evaluate equal-\nity constraints and we leave that for future work.\n(i) Learning c only\n(ii) Learning c, A, b jointly\nabsolute duality gap\nsquared error\nnumber of constraints\nnumber of constraints\n2D instances\n10D instances\n2D instances\n10D instances\nini\nlrn\n4\n8\n16\n20\n36\n80\n4\n8\n16\n20\n36\n80\ntarget at vertex\ntarget strictly feasible\ntarget infeasible\nini\nlrn\nini\nlrn\nini\nlrn\nFig. 3. Learning in non-parametric IO problems.\n8\nY. Tan et al.\n4.1\nNon-Parametric\nWe ﬁrst demonstrate the performance of our method for learning c only, and\nlearning c, A and b jointly, on the single-point variant of model (1), i.e., when\na single optimal target xtru is given, a classical problem in IO [1]. We use two\nloss functions, absolute duality gap (ADG) and squared error (SE), deﬁned as\nfollows:\nADG = c′\nlrn|xtru −xlrn|,\n(3)\nSE = ∥xtru −xlrn∥2\n2,\n(4)\nthe ﬁrst of which is a classical performance metric in IO [11] and the second is\na standard metric in machine learning.\nLearning c only To complete instance generation for this experiment, we ran-\ndomly select one vertex of the convex hull to be xtru for each of the 50 baseline\nLP instances and for each of the six (m, d) combinations.\nInitialization is done by sampling each parameter of cini from N(0, 1). We\nimplement a randomized grid search by sampling 20 random combinations of the\nfollowing three hyperparameter sets: t(0) ∈{0.5, 1, 5, 10}, µ ∈{1.5, 2, 5, 10, 20},\nand αc ∈{1, 10, 100, 1000}. As in other applications of deep learning, it is not\nclear which hyperparameters will work best for a particular problem instance.\nFor each instance we run our algorithm with the same 20 hyperparameter com-\nbinations, reporting the best ﬁnal error values.\nFigure 3 (i) shows the results of this experiment for ADG and SE loss. In\nboth cases, our method is able to reliably learn c: in fact, for all instances, the\nﬁnal error is under 10−4, while the majority of initial errors are above 10−1.\nThere is no clear pattern in the performance of the method as m and d change\nfor ADG; for SE, the ﬁnal loss is slightly bigger for higher d.\nLearning c, A, b jointly Our approach to instance generation here is to start\nwith each baseline LP and generate a strictly feasible or infeasible target within\nsome reasonable proximity of an existing vertex. The algorithm is then forced\nto learn a new c, A, b that generate the target, which is not an optimum for the\ninitial LP. To make this task more challenging, we also perturb c so that it is\nnot initialized too close to the optimal direction.\nFor each of the 50 baseline LP feasible regions, we generate a c ∼N(0, 1)\nand compute its optimal solution x∗. To generate an infeasible target we set\nxtru = x∗+η where η ∼U[−0.2, 0.2]. We similarly generate a challenging cini by\ncorrupting c with noise from U[−0.2, 0.2]. To generate a strictly feasible target\nnear x∗, we set xtru = 0.9x∗+0.1x′ where x′ is a uniformly random point within\nthe feasible region generated by Dirichlet-weighted combination of all vertices;\nthis method was used because adding noise in 10 dimensions almost always\nresults in an infeasible target.\nIn summary, we generate new LP instances with the same feasible region as\nthe baseline LPs but a corrupted cini and one feasible and one infeasible target.\nDeep Inverse Optimization\n9\nThe goal is to demonstrate the ability of our algorithm to detect the change and\nalso move the constraints and objective so that the feasible/infeasible target\nbecomes a vertex optimum. For each of the six problem sizes, we randomly split\nthe 50 instances into two subsets, one with feasible and the other with infeasible\ntargets. For ADG loss we set ϵ = 10−5 and for SE we use the ϵ decay strategy.\nIn practice, this decay strategy is similar to putting emphasis on learning c in\nthe initial iterations and ending with emphasis on constraint learning.\nThe values of hyperparameters αc and αAb are independently selected from\n{0.1, 1, 10} and concatenated into one learning rate vector α. We generate 20\ndiﬀerent hyperparameter combinations. We run our algorithm on each instance\nwith all hyperparameter combinations and record the value of the best trial.\nFigure 3 (ii) shows the results of this experiment for ADG and SE loss. In\nboth cases, our method is able to learn model parameters that result in median\nloss of under 10−4. For ADG, our method performs equally well for all problem\nsizes, and there is not much diﬀerence in the ﬁnal loss for feasible and infeasible\ntargets. For SE, however, the ﬁnal loss is larger for higher d but decreases as\nm increases. Furthermore, there is a visible diﬀerence in performance of the\nmethod on feasible and infeasible points for 10-dimensional instances: learning\nfrom infeasible targets becomes a more diﬃcult task.\n4.2\nParametric\nSeveral aspects of the experiment for parametric LPs are diﬀerent from the non-\nparametric case. First, we train by minimizing MSE(w), deﬁned as\nMSE(w) = 1\nN\nN\nX\nn=1\n∥x(un, wtru) −x(un, w)∥2\n2.\n(5)\nWe chose the mean of SE loss instead of the mean of ADG loss for the parametric\nexperiments because it is only zero if the targets are all feasible, which is not\nnecessarily required for ADG to be zero. This makes the SE loss more diﬃcult\nfrom a learning point of view, but also leads to more intuitive notion of success.\nSee Section 5 for discussion. In the parametric case, we also assess how well the\nlearned PLP generalizes, by evaluating its MSE(wlrn) on a held-out test set.\nTo generate parametric problem instances, we again started from the baseline\nLP feasible regions. To generate a true PLP, we used six weights to deﬁne linear\nfunctions of u for all elements of c, all elements of b, and one random element in\neach row of A. For example, for 2-dimensional problems with four constraints,\nour instances have the following form:\nminimize\nx\n(c1 + w1 + w2u)x1 + (c2 + w1 + w2u)x2\nsubject to\n\n\na11\na12 + w3 + w4u\na21\na22 + w3 + w4u\na31 + w3 + w4u\na32\na41\na42 + w3 + w4u\n\n≤\n\n\nb1 + w5 + w6u\nb2 + w5 + w6u\nb3 + w5 + w6u\nb4 + w5 + w6u\n\n\n(6)\n10\nY. Tan et al.\nSpeciﬁcally, the “true PLP” instances are generated by setting w1, w3, w5 = 0\nand w2, w4, w6 ∼N(0, 0.2). This ensures that when u = 0 the feasible region\nof the true PLP matches the baseline LP. For each true PLP, we ﬁnd a range\n[umin, umax] ⊆[−1, 1] over which the resulting PLP remains bounded and fea-\nsible. To ﬁnd this ‘safe’ range we evaluate u at increasingly large values and\ntry to solve the corresponding LP, expanding [umin, umax] if successful. For each\ntrue PLP, we generate 20 equally spaced training points spanning [umin, umax].\nWe also sample 20 test points u sampled uniformly from [umin, umax]. We then\ninitialize learning from a corrupted PLP by setting wini = wtru + η where each\nelement of η ∼U[−0.2, 0.2].\nHyperparameters are sampled as t(0) ∈{0.5, 1, 5, 10}, µ ∈{1.5, 2, 5, 10, 20}\nand αAb ∈{1, 10}, and αc is then chosen to be a factor of {0.01, 1, 100} times\nαAb, i.e., a relative learning rate. Here, αc and αAb control the learning rate\nof parameters within w that determine c and (A, b), respectively. In total, we\ngenerate 20 diﬀerent hyperparameter combinations. We run our algorithm on\neach instance with all hyperparameter combinations and record the best ﬁnal\nerror value. A constant value of ϵ = 10−5 is used.\ninitial\nlearned\n(i) 2D PLP instances\n    (8 constraints)\nsquared error\nini\nlrn\ntest\n(ii) 10D PLP instances\n    (36 constraints)\nini\nlrn\ntest\n(iii) 2D PLP instance example\nloss = 0.44\nloss = 0.00\nxtru\nn\nFig. 4. Learning in non-parametric IO problems.\nWe demonstrate the performance of our method on learning parametric LPs\nof the form shown in (6) with d = 2, m = 8, and d = 10, m = 36. In Figure\n4, we report two metrics evaluated on the training set, namely MSE(wini) and\nMSE(wlrn), and one metric for the test set, MSE(wlrn). Figure 4 (iii) shows an\nexample of an instance with d = 2, m = 8 from the training set. We see that,\noverall, our deep learning method works well on 2-dimensional problems with the\ntraining and testing error both being much smaller than the initial error. In the\nvast majority of cases the test error is also comparable to training error, though\nthere are a few cases where it is worse, which indicates a failure to generalize\nwell. For 10D instances, the algorithm signiﬁcantly improves MSE(wlrn) over the\nDeep Inverse Optimization\n11\ninitialization MSE(wini), but in most cases fails to drive the loss to zero, either\ndue to local minima or slow convergence. Again, performance on the test set is\nsimilar to that on training set.\n5\nDiscussion\nThe conceptual message that we wish to reinforce is that inverse optimization\nshould be viewed as a form of deep learning, and that unrolling gives easy ac-\ncess to the gradients of any parameter used directly or indirectly in the forward\noptimization process. There are many aspects to this view that merit further\nexploration. What kind of forward optimization processes can be inversely opti-\nmized this way? Which ideas and algorithms from the deep learning community\nwill help? Are there aspects of IO that make gradient-based learning more chal-\nlenging than in deep learning at large? Conclusive answers are beyond the scope\nof this paper, but we discuss these and other questions below.\nGenerality and applicability. As a proof of concept, this paper uses linear\nprogramming for the forward problems and IPM with barrier method as the\nforward optimization process. In principle, the framework is applicable to any\nforward process for which automatic diﬀerentiation can be applied. This obser-\nvation does not mean that ours is the best approach for a specialized IO problem,\nsuch as learning c from a single point [12] or multiple points within the same\nfeasible region [14], but it provides a new strategy.\nThe practical message of our paper is that, when faced with novel classes\nor novel parameterizations of IO problems, the unrolling strategy provides con-\nvenient access to a suite of general-purpose gradient-based algorithms for solv-\ning the IO problem at hand. This strategy is made especially easy by deep\nlearning libraries that support dynamic ‘computation graphs’ such as PyTorch.\nResearchers working within this framework can rapidly apply IO to many diﬀer-\nentiable forward optimization processes, without having to derive the algorithm\nfor each case. Automatic diﬀerentiation and backpropagation have enabled a\nnew level of productivity for deep learning research, and they may do the same\nfor inverse optimization research. Applying deep inverse optimization does not\nrequire expertise in deep learning itself.\nWe chose IPM as a forward process because the inner Newton step is dif-\nferentiable and because we expected the gradient to temperature parameter t\nto have a stabilizing eﬀect on the gradient. For non-diﬀerentiable optimization\nprocesses, it may still be possible to develop diﬀerentiable versions. In deep\nlearning, many advances have been made by developing diﬀerentiable versions\nof traditionally discrete operations, such as memory addressing [20] or sampling\nfrom a discrete distribution [25]. We believe the scope of diﬀerentiable forward\noptimization processes may similarly be expanded over time.\nLimitations and possible improvements. Deep IO inherits the limitations\nof most gradient-based methods. If learning is initialized to the right “basin\nof attraction”, it can proceed to a global optimum. Even then, the choice of\n12\nY. Tan et al.\nlearning algorithm may be crucial. When implemented within a steepest descent\nframework, as we have here, the learning procedure can get trapped in local\nminima or exhibit very slow convergence. Such eﬀects are why most instances in\nFigure 4 (ii) failed to achieve zero loss.\nIn deep learning with neural networks, poor local minima become exponen-\ntially rare as the dimension of the learning problem increases [15,33]. A typical\nstrategy for training neural networks is therefore to over-parameterize (use a high\nsearch dimension) and then use regularization to avoid over-ﬁtting to the data.\nIn deep IO, natural parameterizations of the forward process may not permit an\nincrease in dimension, or there may not be enough observations for regularization\nto compensate, so local minima remain a potential obstacle. We believe train-\ning and regularization methods specialized to low-dimensional learning problems\nsuch as by Sahoo et al. [32] may be applicable here.\nWe expect that other techniques from deep learning, and from gradient-based\noptimization in general, will translate to deep IO. For example, optimization\ntechniques with second-order aspects such as momentum [35] and L-BFGS [10]\nare readily available in deep learning frameworks. Other deep learning ‘tricks’\nmay be applicable to stabilizing deep IO. For example, we observe that, when\nc is normal to a constraint, the gradient with respect to c can suddenly grow\nvery large. We stabilized this behaviour with line search, but a similar ‘explod-\ning gradient’ phenomenon exists when training deep recurrent neural networks,\nand gradient clipping [27] is a popular way to stabilize training. A detailed in-\nvestigation of applicable deep learning techniques is outside the scope of this\npaper.\nDeep IO may be more successful when the loss with respect to the forward\nprocess can be annealed or ‘smoothed’ in a manner akin to graduated non-\nconvexity [7]. Our ϵ-decay strategy is an example of this, as discussed below.\nFinally, it may be possible to develop hybrid approaches, combining gradient-\nbased learning with closed-form solutions or combinatorial algorithms.\nLoss function and metric of success. One advantage of the deep inverse\noptimization approach is that it is can accommodate various loss functions, or\ncombinations of loss functions, without special development or analysis. For\nexample one could substitute other p-norms, or losses that are robust to outliers,\nand the gradient will be automatically available. This ﬂexibility may be valuable.\nSpecial loss functions have been important in machine learning, especially for\nstructured output problems [21]. The decision variables of optimization processes\nare likewise a form of structured output.\nIn this study we chose two classical loss functions: absolute duality gap and\nsquared error. The behaviour of our algorithm varied depending on the loss func-\ntion used. Looking at Figure 3 (ii) it appears that deep IO performs better with\nADG loss than with SE loss when learning c, A, b jointly. However, this perfor-\nmance is due to the theoretical property that ADG can be zero even when the\nobserved target point is arbitrarily infeasible [12]. With ADG, all the IO solver\nneeds to do is adjust c, A, b so that xlrn−xtru is orthogonal to c, which in no way\nrequires the learned model to be capable of generating xtru as an optimum. In\nDeep Inverse Optimization\n13\nother words, ADG is meaningful mainly when the true feasible region is known,\nas in Figure 3 (i). When the true region is unknown, SE prioritizes solutions that\ndirectly generate the observations xn\ntru, and may therefore be a more meaningful\nloss function. That is why we used it for our parametric experiments depicted\nin Figure 4.\n(i) ADG loss surface\n(ii) SE loss surface\nabsolute duality gap\nsquared error\nangle of c vector\nangle of c vector\n¼\n2¼\n0\n¼\n2¼\n0\neps = 0.00001\neps = 0.01\neps = 0.00001\neps = 0.1\nFig. 5. Loss surfaces for the feasible region and target shown in Figure 1 (i).\nMinimizing the SE loss also appears to be more challenging for steepest\ndescent. To get a sense for the characteristics of ADG versus SE from the point\nof view of varying c, consider Figure 5, which depicts the loss for the IO problem\nin Figure 1 (i) using both high precision (ϵ = 10−5) and low precision (ϵ =\n0.1, 0.01) for IPM. Because the ADG loss is directly dependent on c, the loss\nvaries smoothly even as the corresponding optimum x∗stays ﬁxed. The SE\nloss, in contrast, is piece-wise constant; an instantaneous perturbation of c will\nalmost never change the SE loss in the limit of ϵ →0. Note that the gradients\nderived by implicit diﬀerentiation [2] indicate ∂ℓ\n∂c = 0 everywhere in the linear\ncase, which would mean c cannot be learned by gradient descent. IPM can\nlearn c nonetheless because the barrier sharpness parameter t smooths the loss,\nespecially at low values. The precision parameter ϵ limits the maximal sharpness\nduring forward optimization, and so the gradient\n∂ℓ\n∂c is not zero in practice,\nespecially when ϵ is weak. Notice that the SE loss surface becomes qualitatively\nsmoother, whereas ADG is not fundamentally changed. Also notice that when c\nis normal to a constraint (when the optimal point is about to transition from one\npoint to another) the gradient ∂ℓ\n∂c explodes even when the problem is smoothed.\nComputational eﬃciency. Our paper is conceptual and focuses on ﬂexibility\nand the likelihood of success, rather than computational eﬃciency. Many appli-\ncations of IO are not real-time, and so we expect methods with running times\non the order of seconds or minutes to be of practical use. Still, we believe the\nframework can be both ﬂexible and fast.\n14\nY. Tan et al.\nDeep learning frameworks are GPU accelerated and scale well with the size\nof an individual forward problem, so large instances are not a concern. A bigger\nissue for GPUs is solving many small or moderate instances eﬃciently. Amos\nand Kolter [2] developed a batch-mode GPU forward solver to address this.\nWhat is more concerning for the unrolling strategy is that forward opti-\nmization processes can be very deep, with hundreds or thousands of iterations.\nBackpropagation requires keeping all the intermediate values of the forward pass\nresident in memory, for later use in the backward pass. The computational cost\nof backpropagation is comparable to that of the forward process, so there is no\nasymptotic advantage to skipping the backwards pass. Although memory usage\nwas small in our instances, if the memory usage is linear with depth, then at\nsome depth the unrolling strategy will cease to be practical compared to Amos\nand Kolter’s [2] implicit diﬀerentiation approach. However, we observed that for\nIPM most of the gradient contribution comes from the ﬁnal ten Newton steps\nbefore termination. In other words, there is a vanishing gradient with depth,\nwhich means the gradient can be well-approximated in practice with truncated\nbackpropagation through time (see [34] for review), which uses a small constant\npool of memory regardless of depth.\nIn practice, we suggest that the unrolling approach is convenient during the\ndevelopment and exploration phase of IO research. Once an IO model is proven\nto work, it can potentially be made more eﬃcient by deriving the implicit gra-\ndients [2] and comparing them to the unrolled implementation as a reference.\nStill, more important than improving any of these constants is to use asymptot-\nically faster learning algorithms actively being developed in the deep learning\ncommunity.\n6\nConclusion\nWe developed a deep learning framework for inverse optimization based on back-\npropagation through an iterative forward optimization process. We illustrate the\npotential of this framework via an implementation where the forward process is\nthe interior point barrier method. Our results on linear non-parametric and para-\nmetric problems show promising performance. To the best of our knowledge, this\npaper is the ﬁrst to explicitly connect deep learning and inverse optimization.\nReferences\n1. Ahuja, R.K., Orlin, J.B.: Inverse optimization. Operations Research 49(5), 771–783\n(2001)\n2. Amos, B., Kolter, J.Z.: OptNet: Diﬀerentiable optimization as a layer in neural net-\nworks. In: Proceedings of the 34th International Conference on Machine Learning,\nPMLR 70. pp. 136–145 (2017)\n3. Aswani, A., Shen, Z.J., Siddiq, A.: Inverse optimization with noisy data. Operations\nResearch 63(3) (2018)\n4. B¨armann, A., Martin, A., Pokutta, S., Schneider, O.: An online-learning approach\nto inverse optimization. arXiv preprint arXiv:1810.12997 (2018)\nDeep Inverse Optimization\n15\n5. B¨armann, A., Pokutta, S., Schneider, O.: Emulating the expert: Inverse optimiza-\ntion through online learning. In: International Conference on Machine Learning.\npp. 400–410 (2017)\n6. Bengio, Y., Lodi, A., Prouvost, A.: Machine learning for combinatorial optimiza-\ntion: a methodological tour d’horizon. arXiv preprint arXiv:1811.06128 (2018)\n7. Blake, A., Zisserman, A.: Visual Reconstruction. MIT Press, Cambridge, MA, USA\n(1987)\n8. Bonami, P., Lodi, A., Zarpellon, G.: Learning a classiﬁcation of mixed-integer\nquadratic programming problems. In: International Conference on the Integration\nof Constraint Programming, Artiﬁcial Intelligence, and Operations Research. pp.\n595–604. Springer (2018)\n9. Boyd, S., Vandenberghe, L.: Convex optimization. Cambridge university press\n(2004)\n10. Byrd, R.H., Lu, P., Nocedal, J., Zhu, C.: A limited memory algorithm for bound\nconstrained optimization. SIAM Journal on Scientiﬁc Computing 16(5), 1190–1208\n(1995)\n11. Chan, T.C.Y., Craig, T., Lee, T., Sharpe, M.B.: Generalized inverse multi-objective\noptimization with application to cancer therapy. Operations Research 62(3), 680–\n695 (2014)\n12. Chan, T.C.Y., Lee, T., Terekhov, D.: Goodness of ﬁt in inverse optimization. Man-\nagement Science (2018)\n13. Chan, T.C.Y., Kaw, N.: Inverse optimization for the recovery of constraint param-\neters. arXiv preprint arXiv:1811.00726 (2018)\n14. Chan, T.C.Y., Lee, T., Mahmood, R., Terekhov, D.: Multiple observations and\ngoodness of ﬁt in generalized inverse optimization. arXiv preprint arXiv:1804.04576\n(2018)\n15. Dauphin, Y.N., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., Bengio, Y.: Iden-\ntifying and attacking the saddle point problem in high-dimensional non-convex\noptimization. In: Advances in neural information processing systems. pp. 2933–\n2941 (2014)\n16. Dong, C., Chen, Y., Zeng, B.: Generalized inverse optimization through online\nlearning. In: Advances in Neural Information Processing Systems. pp. 86–95 (2018)\n17. Donti, P., Amos, B., Kolter, J.Z.: Task-based end-to-end model learning in stochas-\ntic optimization. In: Advances in Neural Information Processing Systems. pp. 5484–\n5494 (2017)\n18. Fischetti, M., Jo, J.: Deep neural networks and mixed integer linear optimization.\nConstraints pp. 1–14 (2018)\n19. Goodfellow, I., Bengio, Y., Courville, A.: Deep Learning. MIT Press (2016), http:\n//www.deeplearningbook.org\n20. Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-\nBarwi´nska, A., Colmenarejo, S.G., Grefenstette, E., Ramalho, T., Agapiou, J.,\net al.: Hybrid computing using a neural network with dynamic external memory.\nNature 538(7626), 471 (2016)\n21. Hazan, T., Keshet, J., McAllester, D.A.: Direct loss minimization for structured\nprediction. In: Advances in Neural Information Processing Systems. pp. 1594–1602\n(2010)\n22. Keshavarz, A., Wang, Y., Boyd, S.: Imputing a convex objective function. In: 2011\nIEEE International Symposium on Intelligent Control. pp. 613–619. IEEE (2011)\n23. LeCun, Y., Bengio, Y., Hinton, G.: Deep learning. Nature 521, 436–444 (2015)\n16\nY. Tan et al.\n24. Maclaurin, D., Duvenaud, D., Adams, R.: Gradient-based hyperparameter op-\ntimization through reversible learning. In: International Conference on Machine\nLearning. pp. 2113–2122 (2015)\n25. Maddison, C.J., Mnih, A., Teh, Y.W.: The concrete distribution: A continuous\nrelaxation of discrete random variables. arXiv preprint arXiv:1611.00712 (2016)\n26. Mahmood, R., Babier, A., McNiven, A., Diamant, A., Chan, T.C.Y.: Automated\ntreatment planning in radiation therapy using generative adversarial networks.\narXiv preprint arXiv:1807.06489 (2018)\n27. Pascanu, R., Mikolov, T., Bengio, Y.: Understanding the exploding gradient prob-\nlem. CoRR, abs/1211.5063 (2012)\n28. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,\nDesmaison, A., Antiga, L., Lerer, A.: Automatic diﬀerentiation in PyTorch. In:\nNeurIPS AutoDiﬀWorkshop (2017)\n29. QHull\nLibrary:\n(2018),\nhttps://docs.scipy.org/doc/scipy-0.19.0/\nreference/generated/scipy.spatial.ConvexHull.html,\n[Online;\naccessed\nNovember 2, 2018]\n30. Rumelhart, D.E., Hinton, G.E., Williams, R.J.: Learning representations by back-\npropagating errors. Nature 323(6088), 533–536 (1986)\n31. Saez-Gallego, J., Morales, J.M.: Short-term forecasting of price-responsive loads\nusing inverse optimization. IEEE Transactions on Smart Grid (2017)\n32. Sahoo, S.S., Lampert, C.H., Martius, G.: Learning equations for extrapolation and\ncontrol. In: International Conference on Machine Learning (ICML) (2018)\n33. Soudry, D., Hoﬀer, E.: Exponentially vanishing sub-optimal local minima in mul-\ntilayer neural networks. arXiv preprint arXiv:1702.05777 (2017)\n34. Sutskever, I.: Training recurrent neural networks. University of Toronto Toronto,\nOntario, Canada (2013)\n35. Sutskever, I., Martens, J., Dahl, G., Hinton, G.: On the importance of initializa-\ntion and momentum in deep learning. In: International Conference on Machine\nLearning. pp. 1139–1147 (2013)\n36. Troutt, M.D., Brandyberry, A.A., Sohn, C., Tadisina, S.K.: Linear programming\nsystem identiﬁcation: The general nonnegative parameters case. European Journal\nof Operational Research 185(1), 63–75 (2008)\n37. Troutt, M.D., Tadisina, S.K., Sohn, C., Brandyberry, A.A.: Linear programming\nsystem identiﬁcation. European Journal of Operational Research 161(3), 663–672\n(2005)\n",
  "categories": [
    "cs.LG",
    "math.OC",
    "stat.ML"
  ],
  "published": "2018-12-03",
  "updated": "2018-12-03"
}