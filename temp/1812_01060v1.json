{
  "id": "http://arxiv.org/abs/1812.01060v1",
  "title": "Bach2Bach: Generating Music Using A Deep Reinforcement Learning Approach",
  "authors": [
    "Nikhil Kotecha"
  ],
  "abstract": "A model of music needs to have the ability to recall past details and have a\nclear, coherent understanding of musical structure. Detailed in the paper is a\ndeep reinforcement learning architecture that predicts and generates polyphonic\nmusic aligned with musical rules. The probabilistic model presented is a\nBi-axial LSTM trained with a pseudo-kernel reminiscent of a convolutional\nkernel. To encourage exploration and impose greater global coherence on the\ngenerated music, a deep reinforcement learning approach DQN is adopted. When\nanalyzed quantitatively and qualitatively, this approach performs well in\ncomposing polyphonic music.",
  "text": " \nBach2Bach: Generating Music Using A Deep Reinforcement Learning Approach \nNikhil Kotecha \nColumbia University \n \nAbstract \nA model of music needs to have the ability to recall past details and have a clear, coherent\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \nunderstanding of musical structure. Detailed in the paper is a deep reinforcement learning\n \n \n \n \n \n \n \n \n  \n \n \n \narchitecture that predicts and generates polyphonic music aligned with musical rules. The\n \n \n \n \n \n \n \n \n \n \n \n \nprobabilistic model presented is a Bi-axial LSTM trained with a “kernel” reminiscent of a\n \n \n \n  \n \n \n \n  \n \n \n  \nconvolutional kernel. To encourage exploration and impose greater global coherence on the\n \n \n \n \n \n \n \n \n \n \n \n \ngenerated music, a deep reinforcement learning (DQN) approach is adopted. When analyzed\n \n  \n \n \n \n \n  \n \n \n \nquantitatively and qualitatively, this approach performs well in composing polyphonic music. \n  \n1. Introduction \nThis paper describes an algorithmic approach to the generation of music. The key goal is to\n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \nmodel and learn musical styles, then generate new musical content. This is challenging to model\n \n \n \n \n \n \n \n \n \n \n  \n  \n \nbecause it requires the function to be able to recall past information to project in the future.\n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFurther, the model has to learn the original subject and transform it. This first condition of\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nmemory is a necessary and non-trivial task. The second necessary condition is cohesion: the next\n   \n \n \n \n \n \n \n \n  \n \n \n \nchallenge is to understand the underlying substructure of the piece so that it performs the piece\n  \n \n \n \n \n \n \n \n \n \n  \n \n \n \ncohesively. It is easier to create small, non-connected subunits that do not contribute to sense of a\n   \n  \n \n \n \n \n \n \n \n  \n \n  \ncoherent piece. A third non-necessary, but sufficient condition is musical, aesthetic value and\n \n \n \n \n \n \n \n  \n \n \n \n \nnovelty. The model should not be recycling learned content in a static, thoughtless way. Rather,\n \n \n \n \n \n \n \n \n \n  \n \n \n \n \nthe model should optimize under uncertainty, exploring the potential options for note sequences\n \n \n \n \n \n \n \n \n \n \n \n \n \nand select the highest valued path.  \nIn the following subsections of the introduction, some musical language will be introduced,\n \n \n \n \n \n \n \n \n \n \n \n \n \nfollowed by the necessary conditions and then the sufficient conditions. An outline of solutions\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nwill be also provided. In this paper, two different papers will be re-implemented and used as\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nbenchmarks, changes will be described. In subsequent sections, the following will be covered:\n \n \n \n \n \n \n \n \n \n \n \n \n \nmethodology,\nobjectives\nand\ntechnical\nchallenges,\nproblem\nformulation\nand\ndesign,\n \n \n \n \n \n \n \n \n \nimplementation, results and conclusion.  \n \n1.1 Music as a sequence generation task \nOne method to algorithmically generate music is to train a probabilistic model. Model the\n \n \n \n \n \n  \n \n  \n \n \n \n \nmusic as a probability distribution, mapping measures, or sequences of notes based on likelihood\n \n  \n \n \n \n \n \n \n \n \n \n \n \nof appearance in the corpus of training music. These probabilities are learnt from the input data\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nwithout prior specification of particular musical rules. The algorithm uncovers patterns from the\n \n \n \n \n \n \n \n \n \n \n \n \n \nmusic alone. After the model is trained, new music is generated in sequences. This generated\n \n \n \n \n  \n \n \n  \n \n \n \n \n \nmusic comes from a sampling of the learned probability distribution. This approach is\n \n \n \n \n \n \n \n \n \n \n \n \n \ncomplicated by the structure of music. Structurally, most music contains a melody, or a key\n \n \n \n \n \n \n \n \n \n  \n \n  \n \nsequence of notes with a single instrument or vocal theme. This melody can be monodic,\n \n \n \n  \n \n \n \n \n \n \n \n \n \n \nmeaning at most one note per time step. The melody can also be polyphonic, meaning greater\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nthan one note per time step[2]. In the case of Bach's chorales, they have a polyphony, or multiple\n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \nvoices producing a polyphonic melody. These melodies can also have an accompaniment. This\n \n  \n \n \n \n \n \n \n \n \n \n \ncan be counterpoint, composed of one or more melodies or voices[3]. A form of accompaniment\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \ncan also be a sequence of chords that provide an associated term called a harmony. The input has\n \n \n  \n \n \n \n \n \n \n \n \n  \n \n \n \n \ngreat bearing on the nature of the output generated. \nThese musical details are relevant because training a probabilistic model is complicated by the\n \n \n \n \n \n \n  \n \n  \n \n \n \nmultidimensionality of polyphonic music. For instance, within a single time step multiple notes\n \n \n \n \n \n \n  \n \n \n \n \n \ncan occur creating harmonic intervals. These notes can also be patterns across multiple time\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nsteps in sequence. Further, musical notes are expressed by octave, or by interval between musical\n  \n \n \n \n \n \n \n \n \n \n \n \n \n \npitches. Pitches one or more octaves apart are by assumption musically equivalent, creating the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nidea of pitch circularity. Pitch is therefore viewed as having two dimensions: height, which refers\n \n \n \n \n  \n \n \n \n \n \n \n \n \n \nto the absolute physical frequency of the note (e.g. 440 Hz); and pitch class, which refers to\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nrelative position within the octave. Therefore, when music is moved up or down a key the\n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \nabsolute frequency of the note is different but the fundamental linkages between notes is\n \n \n \n \n \n \n \n \n \n \n \n \n \n \npreserved. This is a necessary feature of a model. Chen et al[4] offered an early paper on deep\n \n   \n \n \n  \n \n  \n \n \n \n \n \n \n \nlearning generated music with a limited macro structure to the entire piece. The model created\n \n \n \n  \n \n \n \n \n \n \n \n \n \n \nsmall, non-connected subunits that did not contribute to a sense of a coherent composition. To\n \n \n \n \n \n \n \n  \n \n  \n \n \n \neffectively model music, attention needs to be paid to the structure of the music. \n \n1.2 Necessary Conditions: Memory and Cohesion  \nA model of music needs to have the ability to recall past details and understand the underlying\n \n \n \n \n  \n \n \n  \n \n \n \n \n \n \n \nsub-structure to create a coherent piece in line with musical structure. Recurrent neural networks\n \n \n  \n \n  \n \n \n \n \n \n \n \n(RNN), and in particular long short-term memory networks (LSTM), are successful in capturing\n \n \n \n \n \n \n \n \n \n \n  \n \npatterns occurring over time. To capture the complexity of musical structure vis a vis harmonic\n \n \n \n \n \n \n \n \n \n \n \n  \n \n \nand melodic structure, notes at each time step should be modeled as a joint probability\n \n \n \n \n \n \n \n \n \n \n \n  \n \n \ndistribution. Musically speaking, there should be an understanding of time signature, or the\n \n \n \n \n \n \n \n \n \n \n \n \n \nnumber of notes in a measure. Further, the RNN-based architecture allows the network to\n \n \n \n  \n \n \n \n \n \n \n \n \n \ngenerate notes identical for each time step indefinitely, making the songs time-invariant. Prior\n \n \n \n \n \n \n \n \n \n \n \n \n \nwork with music generation using deep learning [5] [6] have used RNNs to learn to predict the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nnext note in a monophonic melody with success.  \n \nTo account for octaves and pitch circularity, greater context is needed. Following the\n \n \n \n \n \n \n \n \n \n \n \n \n \nconvolutional neural network (CNN) architecture, a solution is to employ a kernel or a window\n \n \n \n \n  \n  \n \n  \n \n  \n \nof notes and sliding that kernel or convolving across surrounding notes. Convolutional layers\n \n \n \n \n \n \n \n \n \n \n \n \n \nhave a rich history linked to the mammalian visual system. For instance, “a deep convolutional\n  \n \n \n \n \n \n \n \n \n \n \n \n \n \nnetwork… uses hierarchical layers of tiled convolutional filters to mimic the effects of receptive\n \n \n \n \n \n \n \n  \n \n \n \n \n \nfields occurring in early visual cortical development[7].” Much in the same way vision requires\n \n \n \n \n \n \n \n \n \n \n \n \n \n \ninvariance in multiple dimensions, CNNs offer a way to develop hierarchical representations of\n \n \n \n \n \n  \n \n \n \n \n \n \nfeatures giving invariance to a network. In effect, this allows the model to take advantage of\n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n“local\nspatial\ncorrelations”\nbetween\nmeasures,\nand\nbuilding\nin\nrobustness\nto\nnatural\n \n \n \n \n \n \n \n \n \n \n \ntransformations.\nMusically\nspeaking,\nnotes\ncan\nbe\ntransposed\nup\nand\ndown\nstaying\n \n \n \n \n \n \n \n \n \n \n \nfundamentally the same. To account for this pitch circularity, the network should be roughly\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nidentical for each note. Further, multiple notes can be played simultaneously - the idea of\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \npolyphony - and the network should account for the selection of coherent chords. RNNs are\n  \n \n \n \n \n \n \n \n \n \n \n \n \n \ninvariant in time, but are not invariant in note. A specific output node represents each note.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nMoving, therefore, up a whole step produces a different output. For music, relative relationships\n \n \n  \n \n \n  \n \n \n \n \n \n \nnot absolute relationships are key. E major sounds more like a G major than an E minor chord,\n \n \n \n \n  \n \n \n \n   \n \n \n  \n \n \ndespite the fact that E minor has closer absolute similarity in terms of note\nposition.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nConvolutional networks offer this necessary invariance across multiple dimensions. Inspired by\n \n \n \n \n \n \n \n \n \n \n \nDaniel Johnson’s[8] Bi-axial LSTM model, I describe a neural network architecture that\n \n \n \n \n  \n  \n \n \n \n \ngenerates music. The probabilistic model described is a stacked recurrent network with a\n \n \n \n \n \n \n  \n \n \n \n  \nstructure employing a convolution-esque kernel.  \n \nThe model described thus far learns past information so that it can project in the future.\n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \nThe solution described is to use a LSTM network. There are limitations to using an RNN model.\n \n \n  \n \n  \n \n \n \n \n  \n \n \n \n \nAs stated initially, an essential feature of an algorithmic approach to music is an understanding\n \n \n \n \n \n \n \n \n \n \n \n  \n \n \nof the underlying substructure of the piece so that it performs the piece cohesively. This\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nnecessitates remembering past details and creating global coherence. An RNN solves this\n \n \n \n \n \n \n \n \n \n \n \n \nproblem generally: by design an RNN generates the next note by sampling from the model’s\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \noutput distribution, producing the next note. However, this form of model suffers from excessive\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nrepetition of the same note, or produces sequences of notes that lack global coherent structure.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nThe work can therefore sound meandering or without general pattern.  \n \n1.4 Sufficient Condition: Musical Novelty \nThe next challenge is to understand the underlying substructure so that it performs\n \n \n \n \n \n \n \n \n \n \n \n \n \ncohesively. The convolutional-esque kernel offers greater context needed for musical coherence,\n \n \n \n \n \n \n \n \n \n \n \nbut is insufficient. A third non-necessary, but sufficient condition of generating music is\n \n \n \n \n \n \n \n \n \n \n \n \n \naesthetic value, increased coherence, and novelty. This third condition is difficult to model due\n \n \n \n \n \n \n \n \n  \n \n \n \n \nto the subjective nature of what makes a song sound “good.” A way to solve a related problem is\n \n \n \n \n \n \n  \n \n \n  \n  \n  \n \n  \nto allow for exploration. Instead of sampling from a static learned distribution as in the case of a\n \n \n \n \n \n \n \n  \n \n \n \n  \n \n \n  \npure deep learning approach, the reinforcement learning (RL) algorithm can be cast as a class of\n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \nMarkov Decision Processes (MDP). An MDP is a decision making framework in which\n \n \n \n \n \n \n  \n \n \n \n \n \noutcomes are partly random and partly under the control of a decision maker. (More precise\n \n \n \n \n \n \n \n \n \n  \n \n \n \n \ndetails on reinforcement learning and MDPs in the methodology section.) At each time step, an\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nagent can visit a finite number of states. From every state there are subsequent states that can be\n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nreached by means of actions. When a state is visited, a reward is collected. Positive rewards\n \n \n \n \n \n  \n  \n  \n  \n \n \n \nrepresent gain and negative rewards represent punishment. The value of a given state is the\n \n \n \n \n \n \n \n \n \n  \n \n \n \n \naveraged future reward which can be accumulated by selecting actions from the particular state.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nActions are selected according to a policy which can also change. The goal of an RL algorithm is\n \n \n \n   \n \n \n \n \n \n \n \n \n \n \n  \nto select actions that maximize the expected cumulative reward (the return) of the agent. The\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \napproach will be described in greater detail in the methodology section.  \nIn the context of music, the necessary and sufficient conditions described above combine\n \n \n \n \n \n \n \n \n \n \n \n \n \nto create a sequence learning and generation task. RL is used to impose structure on an Bi-axial\n \n  \n \n \n \n \n \n  \n \n \n \n \n \n \n \nLSTM with a covolutional-esque kernel trained on data. The reward function is a combination of\n \n  \n \n \n \n \n \n \n \n   \n \n \nrewards associated with following hard-coded musical theory rules and a reward associated with\n \n \n \n \n \n \n \n \n  \n \n \n \nthe probability of a given action learned by the LSTM network. This enables an accurate\n \n \n  \n \n \n \n \n \n \n \n \n \n \n \nrepresentation of the source probability distribution learned from Bach’s music, while still\n \n \n \n \n \n \n \n \n \n \n \n \nretaining musical constructs - pitch, harmony, etc - to bound the samples within reasonable,\n \n \n  \n \n \n  \n \n \n \n \n \n \nheuristic musical rules. This mix of reward learned from data and task specific reward combined\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ninto a general reward function provides a better metric tailored to the specific task of generating\n  \n \n \n \n  \n \n \n \n \n \n \n \n \n \nmusic. Different from previous approaches [9][10][11][12] and following the lead of [13], the\n \n \n \n \n \n \n \n \n \n \n \n \n \nmodel mainly relies on information learned from data with the RL component improving the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nstructure of the output through the imposition of musical, structural rules. \nOverall inspired by Daniel Johnson’s[8] Bi-axial LSTM model and Natasha Jacques’[13]\n \n \n \n \n \n \n \n \n \n \n \nReinforcement Learning model, I describe a deep neural network with reinforcement learning\n \n \n  \n  \n \n \n \n \n \n \narchitecture that generates music. The probabilistic model described is a stacked recurrent\n \n \n \n \n \n \n \n \n  \n \n \n \nnetwork with a structure employing a convolution-esque kernel, refined by a RL component.\n \n  \n \n  \n \n \n \n  \n \n \nPresented is the model, the approach to training, and generation. \n2. Background \n2.1 Deep Q-Learning  \nA song can be discretized and interpreted as a series of finite measures of notes concatenated into\n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \na full song. Given the state of the environment at time ,\n, the agent takes an action according\n \n \n \n \n \n \n \n \n  \n  \n \n \n \n \n \n \n \nto its policy\n, receives a reward\nand the environment transitions to a new state,\n \n \n \n \n  \n \n \n \n \n \n   \n \n \n. The agent’s goal is to maximize reward over a sequence of actions, with a discount factor\n \n \n \n  \n \n \n \n  \n \n \n \n  \n \n \nof\napplied to future rewards. Casting the problem in this framework, yields traction from a\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \nreinforcement learning and dynamic programming approach.  \nA dynamic programming and RL approach splits the multi-period planning problem, as\n \n \n \n \n \n \n \n \n \n \n \n \nin the case of music, into easier sub-problems at different points in time. Information describing\n \n \n \n \n \n \n \n  \n \n  \n \n \n \nthe evolution of the decision problem over time is therefore necessary. The LSTM approach\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nsolves this problem and encodes this information in the forget and input gate mechanism\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n(described further in the subsequent background section). The information necessary about the\n \n \n \n \n \n \n \n \n \n \n \n \ncurrent situation needed to make the “correct” decision, that which maximizes the expected\n \n \n \n \n \n \n \n \n \n \n \n \n \nreward is achieved through an RL or dynamic programming approach. In general, RL methods\n  \n \n \n \n \n \n \n \n \n \n \n \n \nare used to solve two related problems: Prediction Problems and Control Problems. In prediction\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nproblems, RL is used to learn the value function for the policy followed. At the end of learning,\n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nthe learned value function describes for every visited state how much future reward can be\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nexpected when performing actions starting at this state. Control problems takes this a step\n \n \n \n \n \n \n \n \n \n \n \n  \n \nfurther. Interaction with the environment offers a chance to find a policy that maximizes reward.\n \n \n \n \n \n  \n  \n  \n \n \n \n \n \nBy traveling through state space, the agent is learning the optimal policy[14]: a rule that\n \n \n \n \n \n \n \n \n \n \n \n  \n \n \ndetermines a decision given the available information in the current state. After sufficient\n  \n \n \n \n \n \n \n \n \n \n \n \ntraveling, the agent obtains an optimal policy which allows for planning of actions and optimal\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ncontrol. If the control problem is reframed as a predictive type of control, the solution to the\n \n \n \n \n  \n \n  \n \n \n \n \n \n \n \n \ncontrol problem appears to require a solution to the prediction problem as well.  \nFrom Richard Bellman [15], an optimal policy has the property that whatever the initial\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nstate and initial decision are, the remaining decisions must constitute an optimal policy with\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nregard to the state resulting from the first decision. The problems that can be broken apart like\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nthis have in the world of computer science “optimal substructure”, which is analogous to the idea\n \n  \n \n \n \n \n \n \n \n  \n  \n \n \nof “subgame perfect equilibria” from game theory. (Notation used is consistent with Jacques\n \n \n \n \n \n \n \n \n  \n \n \n \npaper.) The optimal deterministic policy\nis known to satisfy the Bellman optimality equation\n \n \n \n \n \n  \n  \n \n \n \n \n \n[15]:  \n(1)\n \nWhere\nis the Q function of a policy\n. The Bellman\n \n \n \n \n \n \n  \n \n \n \n \nequation shows that a dynamic optimization problem in discrete time can be expressed\n \n \n \n \n \n \n \n \n \n \n \n \n \nrecursively by relating the value function in one period relative to the next. The optimal policy in\n \n \n \n \n \n  \n \n \n  \n \n \n \n \n  \nthe last time period is specified in advance as a function of the state variable’s value at that time.\n \n \n \n  \n \n \n \n  \n \n \n \n \n \n  \n \n \nThe following optimal value objective function can be then expressed in terms of that state\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nvariable. This continues, maximizing the sum of the period’s time specific objective function.\n \n \n \n \n \n \n \n \n \n \n \n \n \nUsing recursion, the first period decision rule can be derived as a function of the initial state\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \nvariable value by optimizing the sum of the first period specific objective functions and the one\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nstep look ahead, which captures the value for all future periods. Therefore, each period’s\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ndecision is made by acknowledging that all future decisions will be optimally made. Practically,\n  \n \n \n \n \n \n \n \n \n \n \n \n \nsince Bellman’s equation is a functional equation solving the Bellman equation solves for the\n \n \n   \n \n \n \n \n \n \n \n \n \nunknown value function. The value function is a function of the state and characterizes the best\n \n \n \n \n \n   \n \n \n \n \n \n \n \n \npossible value of the objective. By calculating the value function, the function describing the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \noptimal action as a function of the state is also found, called the policy function. \nThis is useful in the context of music generation because the Bellman equation offers a\n  \n \n \n \n \n \n \n \n \n \n \n \n  \nmethod to solve stochastic optimal control problems, like a Markov Decision Process. A Markov\n \n \n \n \n \n \n \n  \n \n \n  \n \nDecision Process (MDP) is a discrete time stochastic control process. At each time step, the\n \n \n \n  \n \n \n \n \n \n \n \n \n \n \nprocess is in some state, and the decision maker may choose an action in said state. The process\n  \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \nresponds by transitioning to a new state and giving the decision maker a corresponding reward.\n \n \n \n  \n \n \n \n \n \n \n  \n \n \nThe probability that the process moves into its new state is influenced by the agent’s chosen\n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \naction, characterized by the state transition function. Given the state and action, state transitions\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nare conditionally independent of all previous states and actions. The Markov property holds:\n \n \n \n \n \n \n \n \n \n \n \n \n \nrecent past, not distant past influence the next state. The central goal of MDPs is to find a policy\n \n \n \n \n \n \n \n \n \n \n \n \n \n   \n  \n \nfunction, or the action that the agent takes given the state. The desire is to select a policy that\n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n  \n \n \nmaximizes the cumulative function of the random rewards, usually the expected discounted sum\n \n \n \n \n \n \n \n \n \n \n \n \n \nover a potentially infinite horizon (termed an infinite horizon MDP). This entails a discount\n  \n \n \n \n \n \n \n \n \n \n  \n \nfactor multiplied the reward which is a function of the state summed over the infinite horizon. A\n \n \n \n \n   \n \n \n \n \n \n \n \n \n  \nMDP can also be thought of as a one-player stochastic game[16]. If the probabilities or rewards\n \n \n \n \n \n \n  \n \n \n  \n \n \n \n \nare unknown, this becomes a reinforcement learning problem. \nAt a high level, there exist several different ways of finding the optimal value function\n  \n \n \n \n \n \n \n \n \n \n \n \n \n \nand/or the optimal policy. If the state transition function\n, which characterizes the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \ntransition probability in going from state\nto\nwhen performing action\n. And if the reward\n \n \n \n \n \n  \n \n \n \n \n \n \n  \n \n \nfunction\nwhich determines how much reward is obtained at a state, then algorithms\n \n \n \n \n \n \n \n \n \n  \n \n \n \nwhich can be modeled are called model-based algorithms. They can be used to acquire the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \noptimal value function and/or the optimal policy. Of note are value iteration and policy iteration,\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nwhich both come from Dynamic Programming [15], not RL. These two approaches are beyond\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nthe scope of the methodology, but the names are included for completeness. If the model of the\n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \nprocess, namely the transition function and the reward function are unknown ex ante, then this\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nbecomes a RL problem. In the language of control theory, an adaptive process of the optimal\n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nvalue function and/or the optimal policy will need to be learned. Notable algorithms include:\n \n \n \n \n \n \n \n \n \n \n \n \n \n \ntemporal difference learning (TD) which in isolation is used for value function learning;\n \n \n \n \n \n \n \n \n \n \n \n \n \nAdaptive Actor-Critics, which is an adaptive policy iteration algorithm used to approximate the\n \n \n  \n \n \n \n \n \n \n \n \n \nmodel of the value function by TD where the TD error is used for the actor and the critic; and\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \nmost relevant for this paper is Q-learning, which allows for concurrent value function and policy\n \n \n \n \n  \n \n \n \n \n \n \n \n \n \noptimization.  \nIn Q learning, an action is taken, and given uncertainty over the transition probabilities or\n  \n \n \n  \n \n \n \n \n \n \n \n \n \nrewards the agent continues optimally given the current policy. Experience during learning\n \n \n \n \n \n \n \n \n \n \n \n \nfollows: given the current state and the taken action, a new state emerges. Q-learning techniques\n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n[17] [18] learn this optimal Q function by iteratively minimizing the Bellman residual. The\n \n \n \n \n \n \n \n \n \n \n \n \n \n \noptimal policy is given by:\n. Deep Q-learning [19] uses a neural\n \n  \n \n \n \n \n \n \n  \n \nnetwork called the deep Q-network (DQN) to approximate the Q function,\n. This\n \n \n \n \n \n \n \n \n \n \n \n \n \nnaive approach has some major flaws, namely the Q function can diverge when a non-linear\n \n \n \n \n \n \n \n \n \n \n \n \n  \n \nfunction approximator, such as a neural network is used [20]. Solutions proposed by Mnih et al.\n \n \n \n  \n \n  \n \n \n \n \n \n  \n \n \n[19] use a method termed experience replay that “randomizes over data, removing correlations in\n \n  \n \n \n \n \n \n \n \n \n \n  \nthe observation sequence and smoothing over changes in the data distribution.” Mnih et al also\n \n \n \n \n \n \n \n \n \n \n \n   \n \npropose an iterative update that adjusts the action values toward target values that are only\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nperiodically updated. In effect, the network parameters\nare learned by applying stochastic\n \n \n \n \n \n \n \n \n \n \n \n \n \ngradient descent (SGD) updates with respect to the following loss function, \n(2) \n \nwhere\nis the exploration policy, and\nis the parameter of the Target Q-network [19] that is\n \n  \n \n \n \n \n  \n \n \n \n \n \n \n \n  \nheld fixed during the gradient computation. The moving average of\nis used as\nas proposed\n \n \n \n \n \n \n \n \n \n   \n \n \n \n \n \nin [21]. Exploration can be performed with either the epsilon-greedy method or Boltzmann\n \n \n \n \n \n \n \n \n \n \n \n \n \nsampling. Additional standard techniques such as replay memory [20] mentioned above and\n \n \n \n \n \n \n \n \n \n \n \n \nDeep Double Q-learning [22] are used to stabilize and improve learning. In game theoretic\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nlanguage, the agent is exploring the potential sub-game equilibria and finding the corresponding\n \n \n  \n \n \n \n \n \n \n \n \n \npolicy functions to approximately solve the infinite horizon MDP through the Bellman equation,\n \n \n \n \n \n \n \n \n \n \n \n \n \npractically through the DQN. \n2.2 LSTMs \nRecurrent networks encounter a serious problem caused by difficulty in estimating gradients. In\n \n \n  \n \n \n \n \n \n \n \n \n \nbackpropagation through time (BPTT), recurrence passes multiplications in repetition. This can\n \n \n \n \n \n \n \n \n \n \n \nlead to diminishingly small or increasingly large effects, respectively called the vanishing or\n \n \n \n \n \n \n \n \n \n \n \n \n \nexploding gradient problem. To resolve this problem, Hochreiter and Schmidhuber[23] designed\n \n \n \n \n \n \n \n \n \n \n \nLong short-term memory (LSTM) networks. The LSTM is designed to secure information in\n \n \n \n \n \n \n  \n \n \n \n \n \nmemory cells, separate and protected from the standard information flow of a recurrent network.\n \n \n \n \n \n \n \n \n \n \n  \n \n \nTo pass, read or forget information is performed by opening or closing the input gate, output\n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \ngate, or forget gate. This process of is akin to a neuron firing. Input gates and output gates\n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \ncontrol flows of information in and out of the cell. The forget gate controls if information in the\n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n  \n \ncell should be reset. LSTMs are better at learning long-term dependencies in data, and readjust to\n \n \n \n \n \n \n  \n \n \n  \n \n \n  \ndata in a fast manner[24]. LSTMs are an effective combination with the softmax function. A\n \n  \n \n \n \n \n \n \n \n \n \n \n \n \nsoftmax function, a generalization of a logistic function, squashes arbitrary real values to values\n \n  \n \n  \n \n \n \n \n \n  \n \nin the range (0,1) and that add up to 1. This property makes the softmax function effective at\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \nrepresenting a categorical distribution. In music generation, the softmax function takes as input\n  \n \n \n \n \n \n \n \n \n \n \n \nthe network output of the LSTM and outputs probability values assigned to different notes\n \n \n \n \n \n \n \n \n \n \n \n \n \n \navailable to play. LSTM gates are modulated by weight that is differentiable, allowing for\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nbackpropagation through time (BPTT). Softmax cross-entropy loss is used to train the model in\n \n \n \n \n \n \n  \n \n \n \n \n \n \ntypical neural network learning fashion, through BPTT [25]. To reiterate from a previous section,\n \n \n \n \n \n \n \n \n \n \n  \n \n \nsongs generated using an only deep learning approach lack global structure. An RL approach can\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nimprove this model.  \n \n2. Methodology \nIn this section, presented is Daniel’s Johnson’s original model followed by extensions to the\n \n \n \n  \n \n \n \n \n \n \n \n \n \nmodel. In the original paper there are a few models attempted to generate music. Here the best\n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \nperforming model is selected, replicated, and the model is extended. Additionally, presented is\n \n  \n \n \n \n \n  \n \n \n  \nNatasha Jacques et. al’s deep reinforcement learning approach primed with my extended model.  \n  \n2.1. Objectives and Technical Challenges \nOne key challenge with modeling music is selecting the data representation. Possible\n \n \n \n \n \n \n \n \n \n \n \n \nrepresentations are signal, transformed signal, MIDI, text, etc. In general, musical content for\n \n \n \n \n \n \n \n \n \n \n \n \n \n \ncomputers is first represented as an audio signal. It can be raw audio (waveform), or an audio\n  \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \nspectrum processed as a Fourier transform. A relevant issue is the end destination of the\n \n \n  \n \n \n \n \n \n \n \n \n \n \n \ngenerated music content[2]. The format destination could be a human user, in which case the\n \n \n \n \n \n \n \n  \n \n \n \n \n \n \noutput would need to be human readable, for instance a musical score. In the case of this paper,\n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \nthe destination is a computer. The final output format is therefore readable by a computer, which\n \n   \n \n \n \n \n  \n \n \n  \n \n \nin this case is a MIDI file (musical instrument digital interface). The MIDI representation was\n \n \n   \n \n \n \n \n \n \n \n \n \n \nselected because it offers a particularly rich representation in two senses: first it carries\n \n \n \n  \n \n \n \n \n \n \n \n \n \ncharacteristics of the music in the metadata of the file, like time steps. Second it is a common\n \n \n \n \n \n \n \n \n \n \n \n \n \n    \n \ndigital representation which allowed access to freely and widely available data. In this model, the\n \n \n \n \n  \n \n \n \n \n \n \n \n \n \ncriteria optimized for are: computer readable, information about characteristics of the music, and\n \n \n \n \n \n \n \n \n \n \n \n \n \navailability to a wide selection of Bach’s work. More detail will be provided about each of these\n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nchoices.  \nThe choice of the MIDI file format has substantial bearing on the model. There is a\n \n \n \n \n \n \n \n \n \n \n \n \n \n   \nquestion of how much richness to have in the objective characteristics of the music, or the sense\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nof musical structure. Richer musical representation affords greater precision in the potential\n \n \n \n \n \n \n \n \n \n \n \n \nplaying, but it also creates a more supervised approach to the generation. One must for instance\n \n  \n \n  \n \n \n \n \n \n \n \n \n \n \nknow the musical theory in a deep way to understand the different characteristics of a musical\n \n \n \n \n  \n \n \n \n \n \n \n \n  \n \nscore. The level of musical detail beyond the waveform therefore represents a choice of how\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \nmuch or how little to include. As stated above, the MIDI file format offered the greatest balance\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nbetween objective characteristics of the music and the raw waveform and was thus selected for\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ntraining. Other considered data sets are worth mentioning because they offer consideration for\n \n \n \n \n \n \n \n \n \n \n \n \n \ndifferent definitions for the “generation” of music.  \n \nOne data set is called Bach Digital [26]. The population covered is 90 percent of Bach’s\n \n \n  \n \n \n \n \n \n \n  \n \n \n \n \ncompositions in high resolution scans of his work. The collection data is variable, but in some\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \nways is irrelevant because the scores were produced several hundred years ago. The topics\n \n \n \n \n \n \n \n \n \n \n \n \n \n \ncovered are a major portion of Bach’s oeuvre. This is a useful dataset because it has the clearest\n \n  \n \n \n \n \n \n   \n \n \n  \n \n \n \nrepresentation of how Bach wanted his music. Granted there is a lot of entropy in interpretation,\n \n \n \n \n \n \n \n \n   \n \n \n  \n \nbut this is as close to the man as we can get. In the sense of objective music characteristics\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n(accents, fermatas, loudness, etc) , this is the optimal data set. The challenge with this dataset is\n \n \n \n  \n  \n \n \n \n \n \n \n \n \n  \ngetting the information in a computer readable format. In many ways it is easier to go from the\n \n \n \n  \n \n \n \n \n \n   \n \n \n \n \n \nraw waveform and have the machine learn from the sound straight. But the loss of fundamental\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nmusical information is significant. The midi format (dataset selected) offers a hybrid of\n \n \n \n \n \n \n \n \n \n  \n \n \nwaveform and objective characteristics but still it is a crude representation of the music. The\n \n \n \n \n \n    \n \n \n \n \n \n \nchallenge of converting the musical score to a waveform is a non-trivial task beyond the scope of\n \n \n \n \n \n   \n   \n \n \n \n \n \n \nthis paper. The deeper principle here is the level of supervision in the generation of the output.\n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \nAt one extreme is complete autonomy and automation with no human supervision. Or it could be\n \n \n  \n \n \n \n \n \n \n \n \n  \n \n \nmore interactive, with early stopping built into the model to supervise the music creation process.\n \n \n \n \n \n \n \n \n  \n \n \n \n \n \nThe pure neural network approach employed by this paper is by design non-interactive. The\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nMIDI file format optimized for this dimension as well because it offers a complete end product\n \n \n \n \n \n \n \n \n \n  \n  \n \n \n \nthat is machine readable without human intervention. The level of autonomy is an interesting\n \n \n \n \n \n \n \n \n \n \n  \n \n \npotential development for actual musicians who can interrupt the model in the middle of content\n \n \n \n \n \n \n \n \n \n  \n \n \n \n \ngeneration: have a human guide the process of generation. While beyond the scope of the paper,\n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \nfeedback throughout the process can lead to suggestions that are superior, or more aesthetically\n \n \n \n \n \n \n \n \n \n \n \n \n \n \npleasing musical compositions. \n \nAnother data set is called Bach Choral Harmony data set [27]. The population is\n \n \n \n \n \n \n \n \n \n \n \n \n \n \ncomposed of 60 chorales by Bach. The dataset donated in 2014 comes from a free resource\n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \ncalled the UCI machine learning repository. This data set is useful because it offers a textual\n \n \n \n \n \n \n \n \n  \n \n  \n  \n \nrepresentation of Bach’s work. It contains pitch classes extracted from midi sources, meter\n \n \n \n \n \n \n \n \n \n \n \n \n \ninformation, and chord labels (human annotated). In this way it offers an additional pairing to the\n \n \n \n \n \n \n \n \n  \n \n \n \n  \n \nraw waveform. The challenge with this dataset is if it is used in isolation. It is tough to go from\n \n \n \n \n \n \n     \n \n \n   \n  \n \n \nthe text to an audio representation, and therefore as such does not provide a closed form solution\n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \nto the original posed question. This data set offers an interesting addition to the other data sets\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nbecause it gives information that Bach digital data set has encoded, information straight from the\n  \n \n \n \n \n \n \n \n \n \n \n \n \n \nscores. These objective characteristics are valuable because they give a sense of what Bach\n \n \n \n \n \n \n \n \n  \n \n \n \n \nhimself was trying to create. Performing a pattern recognition on the source limits the potential\n \n \n \n \n \n  \n \n \n \n \n \n \n \n \nloss of information. Some general information on how textual representation works. A melody\n \n \n \n \n \n \n \n \n \n \n  \n \ncan be processed as text. One common example in folk music is the ABC notation. The first 6\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n  \nlines provide metadata – e.g. T: title of the music, M: Meter (time signature is a more accurate\n \n \n  \n \n \n \n \n \n \n \n \n \n   \n \n \nterm), L: default length, K: Key, etc. This is then followed by the main text which codifies the\n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \nmelody. A key thing captured is the pitch class of a note is encoded by association with its\n \n \n \n \n \n \n \n \n \n  \n  \n \n \n \n \n \nEnglish notation. The pitch is represented by with either an upper or lower case. Upper case A\n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n  \nfor instance represents A above middle C (A440 Stuttgart Pitch). a represents one octave up and\n \n \n  \n \n  \n \n \n  \n \n \n \n \n \na’ represents two octaves up. A further consideration with this data set is the focus on Bach’s\n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \nchorales. The specific choice of what selection of music to train on is a judgement that has great\n \n \n \n \n \n \n \n \n \n \n \n   \n \n \n \n \nbearing on the resulting output. Interpretation is a significant source of artistry and generation.\n \n \n \n \n \n   \n \n \n \n \n \n \nThe breadth of the sample offers a wide potential for probability distributions. The breadth of the\n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \nsample also limits the ability to bore deeply into one song. If instead of a wide slice, I selected a\n \n \n \n \n \n \n \n \n \n \n  \n \n  \n \n  \n  \ndata set with one song – for instance, a personal preference of Bach’s music Partitia no. 2 in D\n \n \n \n \n  \n \n  \n \n \n \n \n \n \n  \n  \nminor – but with several artists I would have a learned manifold with a rich and deep\n \n \n \n \n \n  \n \n  \n \n \n  \n \n \n \nunderstanding of one piece. Understanding how Joshua Heifetz, Itzhak Pearlman, Ivry Gitlis, and\n \n \n \n \n \n \n \n \n \n \n \n \n \nsay Hilary Hahn all play the same song differently could yield great depth of understanding, a\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \ncomplex manifold in its own right. If Bach’s oeuvre is the state space for the model, the selection\n \n  \n \n \n  \n \n  \n \n \n \n \n \n \n \n \nof which songs and how many songs seems to suggest a generalist versus specialist tradeoff in\n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \nthe output of the model. In the training of this model I selected a more generalist approach.  \nThe last dataset considered is Bach’s sheet music [28]. This is misleading because the\n \n \n \n  \n \n \n \n \n  \n \n \n \nlisted scores are actually lead sheets (mostly). The population covered is a significant portion of\n \n \n \n \n \n \n \n \n \n   \n \n \n \nBach’s violin compositions. Lead sheets are an important representation because they convey in\n \n \n \n \n \n \n \n \n \n \n \n  \na single or few pages the key ideas of a piece: the score of a melody with annotations specifying\n \n \n \n \n \n \n \n \n  \n \n \n \n  \n \n \n \n \nharmony (chord labels). Also given are composer, musical style (e.g. detache, legato, staccato),\n \n \n \n \n \n \n \n \n \n \n \n \n \nand tempo (allegro for instance). The salient details are given in a data rich and concise format\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \neasy to add to the music directly (by hand). For edification, lead sheets are often used in Blues\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nfor improvisation (where my familiarity comes from). This data set is by design a form of lossy\n \n \n \n \n \n \n \n \n \n  \n \n  \n \n \n \ncompression. It takes the data set and represents it in a less memory intensive, inexact\n \n \n \n \n \n \n \n \n \n  \n \n \n \n \napproximation, partial data format. Therefore there is a lot of information lost in this form. This\n \n \n \n \n \n   \n \n \n \n  \n \n \n \nis good for the purposes of adding a few objective characteristics but is an inefficient estimator\n \n \n \n \n \n \n  \n \n \n \n  \n \n \n \nbecause it fails to capture the full source of Bach’s music.  \n \nAs stated originally, the MIDI file format was selected for its balance of musical\n \n \n \n \n \n \n \n \n \n \n \n \n \n \ninformation with raw waveform, a wide availability of Bach’s oeuvre, and its computer readable\n \n \n \n  \n \n \n \n \n \n \n \n \n \nformat. Next the model details will be explored.  \n \n2.2. Problem Formulation and Design \n2.2a Deep Learning Network \nTo capture the harmonic and melodic structure between notes, the model uses a two-layered\n \n \n \n \n \n \n \n \n \n \n \n  \n \nLSTM RNN architecture with recurrent connections along the note axis. By having one LSTM\n \n \n \n \n \n \n \n \n \n \n \n \n \n \non the time axis and another on the note axis, the model takes on, to borrow Daniel Johnson’s\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nlanguage[8]: a “bi-axial” configuration.  \nThe note-axis LSTM receives as input a concatenation of final output of the note-axis\n \n \n \n \n \n  \n \n \n \n \n \n \n \nLSTM for the previous note window and the activations of the last time-axis LSTM layer for the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nparticular note. The output of the final activations of the note-axis LSTM are then fed into a\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \nsoftmax layer to convert to a probability. The loss corresponds to the cross entropy error of the\n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \npredictions at each time step compared to the played note at each time step. Each note therefore\n  \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \nhas a time component from the time-axis LSTM. This allows for understanding the temporal\n  \n \n \n \n \n \n \n \n \n \n \n \n \nrelationships for the particular note and for modeling the joint distribution of notes in the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nparticular time step. By joining the information from an LSTM focused on the time component\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nand an LSTM focused on the note-component, the relationships within and between notes is\n \n \n \n \n \n \n \n \n \n \n \n \n  \ncaptured for each timestep. By using this approach on each note in sequence, the full conditional\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \ndistribution for each time step can be learned. Further, another key piece of functionality is\n \n \n \n \n \n \n \n \n \n \n \n \n \n  \nbuilding into the model a window that slides over sequences of notes. This architecture enables\n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \nthe model to learn the harmonic and melodic structure of the notes accounting for pitch\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ncircularity. \nExtending beyond Daniel Johnson’s model, the model presented here is designed and\n \n \n \n \n \n \n \n \n \n \n \n \nimplemented to be flexible, general, and to take advantage of parallelization in code. A primary\n \n \n \n \n \n \n \n \n \n \n \n \n  \n \ngoal was flexibility in user input. The architecture is general: the user can set various hyper\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nparameters, such as the number of layers, hidden unit size, sequence length, time steps, batch\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nsize, optimization method, and learning rate. The model is parameterized so users can also set the\n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \nlength of the window of notes fed into the note-axis LSTM model and the length of time steps\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nfed into the time-axis LSTM. The size of the window and length of the time steps are a relevant\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \nfeatures because music is highly variable based on genre and artist. Designing the system to be\n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \ngeneral allows the user to tailor the model to his/her specific needs. In terms of functionality and\n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \nmodel design, a primary goal for the model was parallelization in code. The code was written at\n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n  \na high level to do everything in efficient matrix format, minimizing the use of ‘for’ loops. This\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nallows for speed gains in computational time. \n 2.2b Reinforcement Learning Framework \n \nWith the trained modified Bi-axial model described above (henceforth “Biaxial model”),\n \n \n \n \n \n \n \n \n \n \n \nthe next part of the process is to have the model learn musical theory concepts. To achieve this, I\n \n \n \n \n \n   \n \n \n \n \n \n \n \n \n \n  \nfollow Natasha Jacques et al.’s model [13] of a RL tuner. The LSTM trained on data (the Biaxial\n \n \n  \n \n \n \n  \n \n \n \n \n \n \n \n \n \nmodel) primes the three networks in the RL model: the Q-network and Target Q-network in the\n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \nDQN algorithm described in section 2.1, and a Biaxial Reward. The Q-network is initialized with\n \n \n  \n \n \n  \n \n \n \n  \n \n \nan LSTM model with architecture identical to that of the Biaxial model. The Biaxial Reward\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nfurnishes part of the reward value used to train the model and is fixed while training. In order to\n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n  \n \ncast musical sequence generation in the RL framework, each subsequent note in the sequence or\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \nmelody is seen as taking an action. The state, ​s​, is the previous note and the internal state of the\n  \n \n \n \n \n \n \n   \n \n \n \n \n \n \n \n \n \nLSTM cells of both the Q-network and the Biaxial Reward. Q(a,s) can be calculated by\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ninitializing the Q-network with the suitable memory cell contents, running it one time step using\n \n \n \n \n \n \n \n \n \n  \n \n \n \n \nthe previous note, and evaluating the output value for the action a. The next action can be\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nselected with either a Boltzmann sampling or epsilon-greedy exploration strategy.  \nA key point is whether RL can be used to offer bounds on a sequence learner such that\n \n \n  \n \n \n \n \n \n \n \n \n  \n \n \n \n \nthe sequences it generates conform to a desired, specified structure. To test this idea, I codified\n \n  \n \n \n  \n \n \n \n \n \n \n  \n \nmusical rules consistent with some notions from texts on musical composition [29] [30] [31] [32]\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n[33]. These musical rules are heuristic: I am not a professional musician and as such have chosen\n \n \n \n \n \n  \n \n  \n \n \n \n \n \n \n \nrules with what I thought most salient. The goal of these rules is to steer the model toward more\n \n \n  \n \n \n \n \n \n \n \n   \n \n \n \n \n \ntraditional melodic composition. The codified musical rules are a sufficient addition to the\n \n \n \n \n \n \n \n  \n \n \n \n \nnecessary Biaxial LSTM model, which learns directly from the data.  \n \n3. Implementation \nIn this section, the process of training the network and the generation of new musical\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ncompositions will be explained. Experiments were performed on Google Cloud Platform with\n \n \n \n \n \n \n \n \n \n \n \n \ndeep learning implementation done in TensorFlow. Sources of material that helped guide the\n \n \n \n \n \n \n \n \n \n \n \n \n \nimplementation: Daniel Johnson’s code [34]. For loading the data into the appropriate format\n \n \n \n \n \n \n \n \n \n \n \n \n \n[35]. \n  \n3.1. Deep Learning Network \nThe model is applied to a polyphonic music prediction task. The network is trained to model\n \n  \n \n  \n \n \n \n \n \n  \n \n \n \nthe conditional probability distribution of the notes played in a given time step, conditioned on\n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \nthe notes in previous time steps. The output of the network can be read as at time step t, the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \nprobability of playing a note at time step t, conditioned on prior note choices. Therefore, the\n \n \n  \n \n \n \n  \n \n \n \n \n \n \n \nmodel is maximizing the log-likelihood of each training sequence under the conditional\n \n \n \n \n \n \n \n \n \n \n \n \ndistribution. \nThe time-axis LSTM depends on chosen notes, not on the specific output of the note axis\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nlayers. The rationale is that all notes at all timesteps are known so training can be expedited. The\n \n \n  \n \n \n  \n \n \n \n \n \n \n \n \n \n \ntime gain comes from processing the input, then feeding the pre-processed input through the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nLSTM time-axis in parallel for all notes. Next, the LSTM note-axis layer computes the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nprobabilities across all time steps. This provides a significant speed up when using a GPU to\n \n \n \n \n \n \n  \n \n \n \n \n  \n \n \nperform parallel computing. \nNow that the probability distribution is learned, sampling from this distribution offers a way to\n \n \n \n \n  \n \n \n \n \n \n  \n  \ngenerate new sequences. Sequences are not known in advance. The network must project one\n \n \n \n \n \n \n \n \n \n \n \n \n \n \ntime step in the future at a time. The input for each timestep is used to advance the LSTM\n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \ntime-axis layers one step at at a time to compose the note in the next period. First a sample must\n \n \n \n    \n \n \n \n \n  \n \n \n \n  \n \n \nbe taken while the distribution is being created. Each note is drawn from a Bernoulli distribution.\n \n \n \n \n  \n \n \n \n  \n \n  \n \n \nThis drawn value is then used for the input to the next note. This process is repeated for all notes,\n \n \n  \n \n \n \n \n  \n \n \n \n \n  \n \n \n \n \nafter which the model moves to the next time step. \nThe model was tested on a selection of Bach’s works from [17] as well as the classical piano\n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \nfiles from [18]. Input was in the form of MIDI files. \nAfter training the Bi-axial LSTM, the model was used to create new musical compositions. A\n \n \n \n \n \n \n \n \n \n \n \n \n \n  \nlarger and diverse dataset with different note and structural patterns was used during training.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nThe goal here was to expose the model during training to a wide variety of patterns so as to\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \nencourage as much diversity in output as possible. The MIDI file format enables the use of a\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \ntemporal position in the music. A time component was an important feature to build into the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ndataset so that the model could learn patterns over time relative to different note sequences.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFollowing the guide of Johnson[8], an additional dimension was added to the note vectors fed\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ninto the model: a binary, 0 or 1 to indicate if a note was articulated or sustained at a particular\n \n \n  \n  \n  \n \n   \n \n \n \n \n   \n \ntime step. From Johnson, for instance, the first time step for playing a note is represented as 11.\n \n \n \n \n \n \n \n \n \n \n \n  \n  \n \n \n \nSustaining a previous note is represented as 10, and resting is represented as 00. This added\n  \n \n \n \n \n \n \n \n  \n \n \n \n \n \ndimension allows the model to play the same note multiple times in succession. From the input\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nperspective, the articulation dimension or bit is processed beforehand. This processing is done in\n \n \n \n \n \n  \n \n \n \n  \n  \nparallel with the playing dimension, which together are then fed into to the time-axis LSTM.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFrom the output perspective, the note-axis LSTM gives a probability of playing a note and a\n \n \n \n \n \n \n \n  \n \n \n  \n \n  \nprobability of articulating the same note. When computing the cost function, articulating a\n \n \n \n \n \n \n \n \n \n \n \n  \nplayed note incorrectly is penalized. The articulation output for notes that should not be played is\n \n \n  \n \n \n \n \n \n \n \n \n \n \n  \nignored. It makes little sense to penalize for articulation if a note is not played. \nUsing Moon et al[38] as a suggested guide, Dropout of .75 was applied to each LSTM layer.\n \n  \n \n  \n \n \n \n \n \n \n \n \n \n \n \nThe optimizer selected was ADADELTA[39]. The learning rate selected was 1.0. The Biaxial\n \n \n \n \n \n \n \n \n \n \n \n \n \nmodels were evaluated in two dimensions.  \n3.2 Reinforcement Learning \nFollowing from section 2.2b, music generation can be cast as an RL problem if placement\n \n \n \n \n \n \n \n \n \n \n \n \n  \n \nof the next note in the song is treated like an action. The state is the environment consisting of\n \n \n \n \n \n \n  \n \n \n \n \n \n  \n \n \n \n \nthe previous note and the internal state of the Q network and the Biaxial Reward. Given the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \naction, reward can be evaluated by joining the Biaxial model’s prediction probability of the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \naccurate note learned from the data guided with the hard coded musical theory rules. The reward,\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nin short, can be seen as a combination of the best predicted note as learned from data and from\n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \nmusical theory rules. Described at a high level in section 2.2b, musical theory rules are defined\n \n \n \n   \n \n \n \n \n \n \n \n \n \n \nto offer bounds on the song that the model is composing through the reward\n. If a\n \n \n \n \n \n \n \n \n  \n \n \n \n \n   \nnote, for instance, is in the wrong key, then the model will receive a punishment or a negative\n \n \n  \n \n \n \n \n \n \n \n \n  \n \n  \n \nreward. There is a tradeoff present in this decision to incorporate a model learned from data and\n \n   \n \n \n \n \n \n \n  \n \n \n \n \n \na model constrained exogenously: more constraint or tighter bounds offer narrower searching of\n \n \n \n \n \n \n \n \n \n \n \n \n \nthe state space resulting in more similar actions and more uniform melodic composition, but a\n \n \n \n \n \n \n \n \n \n \n \n \n \n  \nconsistent sounding melody. On the other side, fewer constraints offer greater melodic\n \n \n \n \n \n \n \n \n \n \n \n \n“creativity” in the sense that the model will move beyond creating a simple melody that exploits\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \nthe sure-fire rewards and satisfy the initial sufficient condition of novelty. To accomplish this,\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nthe Biaxial Reward is used to compute\n, the log probability of a note\ngiven a\n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n  \nmelody , and incorporate this into the reward function. \nThe total reward given at time t is: \n (3) \nThe constant c controls the weight given to the musical theory. From the DQN loss function in\n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nequation 2 and the above reward function in equation 3, the modified loss function and learned\n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \npolicy are as follows: \n \n \n (4)  \n \n (5) \n \n \nThe loss function encourages the model to value actions in accordance with the specified musical\n \n \n \n \n \n  \n \n  \n \n \n \n \n \nrules and from the source material, selecting notes with high probability of matching the learned\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ndistribution. \nTo understand the specific musical rules that should be encoded into the reward, I read\n \n \n \n \n \n \n \n \n \n \n \n \n  \n \nseveral music theory books [29] [30] [31] [32] [33]. I tried to characterize which principles were\n \n \n \n \n \n \n \n \n  \n  \n \n \n \n \npreserved in multiple books, most relevant to general composition, agreed with the described\n \n \n \n \n \n \n \n \n \n \n \n \n \nrules in Jacques et al’s paper, and were mathematically interpretable. The music reward function\n \n \n  \n \n \n \n \n \n \n \n \n \n \nwas foster the following characteristics. Generated notes should belong to the same\n \n \n \n \n \n \n \n \n \n \n \n \n \nkey, with the melody starting and finishing with same tonic note of the key. For instance, if the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \nkey is in D-major, the rewarded note would be a middle D. The produced note should also occur\n  \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \nin the first beat and the last four beats of the melody. There should be a designation if a rest is\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n   \n  \nintroduced or a note is held (see discussion above in section 3.1, last paragraph on articulation).\n \n  \n  \n \n \n \n \n \n \n \n \n \n \n \nOutside the cases of a rest or a held note, repetition of the same sound should be minimized.\n \n \n \n  \n \n  \n \n \n \n \n \n \n \n \n \n \nSpecifically, a single tone should not be repeated more than four times in a row. These\n  \n \n \n \n \n \n \n \n \n \n \n  \n \n \nspecifications offer bounds on the generated music. To satisfy the sufficient condition of novelty,\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nthe model receives a negative reward if the model is too similar with itself from previous time\n \n \n  \n \n  \n \n  \n \n \n \n \n \n \n \nsteps. Repeated patterns can be identified by looking at similarity between observations as a\n \n \n \n \n \n \n \n \n \n \n \n \n  \nfunction of time lag, termed autocorrelation. Practically, autocorrelation measures the correlation\n \n \n \n \n \n \n \n \n \n \n \nof a signal and its copy as a function of time. To prevent excessive local periodicity,\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nautocorrelation is used to look at time domain signals. Signal processing has a rich history of\n  \n \n \n \n \n \n \n \n \n \n  \n \n \n \nusing autocorrelation [40]. In the context of music, this can be done by measuring the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nautocorrelation at a time lag of one, two, or three beats. Specifically, the negative reward is\n \n  \n \n \n \n \n \n \n \n \n \n \n \n  \n \napplied when the autocorrelation coefficient is larger than .15. Further, the melody should follow\n \n \n \n \n  \n \n \n \n \n \n \n \n \ntraditional musical intervals. The size of an interval between two notes is the ratio of their\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nmusical frequencies. The size of the main intervals can be expressed by integer ratios. For\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ninstance, some key intervals: one-to-one is called the unison; two-to-one is called an octave;\n \n \n \n \n \n \n \n \n \n  \n \n \n \nthree-to-two is called the perfect fifth; four-to-three is called the perfect fourth; five-to-four is\n  \n \n \n \n \n  \n \n \n \n \n  \ncalled the major third; six-to-five is called the minor third. Pitch increments following the same\n \n \n \n \n  \n \n \n \n \n \n \n \n \n \ninterval produce an exponential increase of frequency, despite the fact that people physically\n \n \n \n \n \n \n \n \n \n \n \n \n \nperceive this increase as a linear increase in pitch. This results in the fascinating idea that people\n \n \n \n  \n \n  \n \n \n  \n \n \n \n \n \nperceive logarithmically [41]. An interval can also be described as horizontal consonance or\n \n \n \n \n \n \n \n \n \n \n \n \n \ndissonance, commonly called melodic if successive or prior notes in a temporal sense have\n \n \n \n  \n \n \n \n \n  \n \n \n \nadjacent pitches (right to left, left to right). An interval can also be thought of in a vertical\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \nconsonance or dissonance sense, commonly called harmonics if concurrent notes have adjacent\n \n \n \n \n \n \n  \n \n \n \n \npitches while interacting in a spatial sense (up and down, down and up). Melodies should avoid\n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \nnon-traditional intervals, intervals occuring outside the described increments. As an example, the\n \n \n \n \n \n \n \n \n \n \n \n \nchord made up two major thirds, called an augmented triad should fall within traditional\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nincrements avoiding clumsy intervals like augmented sevenths. Large jumps of greater than an\n \n \n \n \n \n \n \n \n \n \n \n \n \noctave should also receive a negative reward. Gauldin [29] and Rimsky-Korsakov [32]\n \n \n \n \n \n \n \n \n \n \n \n \nrecommend good composition balance continuity and novelty: most often moving in small steps\n \n \n \n \n \n \n \n \n \n  \n \n \nand occasionally large harmonic intervals. The reward function aligns with this rule. Following\n \n \n \n \n \n \n \n \n \n \n \n \n \nSchoenberg’s Fundamentals of Musical Composition [31], melodic movement should follow a\n \n \n \n \n \n \n \n \n \n  \nsense of mean reversion. If the melody moves a fifth or more in a direction, actions that return\n \n \n \n  \n \n \n  \n \n \n \n  \n \n \n \n \nthe agent with a leap or a gradual movement in the opposite direction are rewarded. Significant\n \n \n  \n \n  \n \n \n \n \n \n \n \n \n \n \nmelodic movement in the same direction is disincentivized. highest and lowest notes should not\n \n \n \n \n \n  \n \n \n \n \n \n \n \nbe repeated. From Tchaikovsky’s Guide to the Practical Study of Harmony [33], motifs or\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nsequences of notes capturing an idea are important for sound composition. Consistency within\n \n \n \n \n \n \n \n \n \n \n \n \n \nthree or more distinct notes, repeated throughout the piece are positively rewarded. \n \n3.3. Software Design \n3.3a Deep Learning  \nThe featured data vector in this neural network is referred to as the ‘Note State Matrix’\n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \nshown in Figure 1. This represents the ‘play’ and ‘articulate’ state of each note over the range of\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nMidi values and for each time step over a specified period of time (i.e. 8 measures at 16 time\n \n \n \n \n \n \n \n  \n \n \n \n \n  \n  \n \n \nsteps per measure). The model takes as input a single batch of these feature data vectors, a single\n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n  \n \n4D tensor referred to as the ‘Note_State_Batch’. The original raw musical data in the form of\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n.MIDI files are first preprocessed to generate each Note_State_Batch using the Python-Midi\n \n \n \n \n \n \n \n \n \n \n \n \npackage extracted from [42].\nIn this work, I used this package only to import MIDI file\n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \nsegments as Note_State_Batches, as well as to create MIDI files from the Generated Samples. It\n \n \n \n \n \n  \n \n \n \n \n \n \n \n \nmay be of interest in further work to enhance the processed feature data vectors to include other\n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \nmusical features such as volume.  \nNote State Matrix​ = \n  a batch of which constitute a ‘Note_State_Batch.’ \nFig. 1: Note State Matrix is the processed, feature vector for the neural network. N is the # Midi note states\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \nextracted from the songs, T is the # time steps in the batch, ‘p’ indicates the binary value of the note being played,\n \n \n \n   \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nand ‘a’ indicates the binary value of the corresponding articulation. \n \n \nThe overall structure of the code was broken into two main tasks: training/validating the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nmodel numerically and then using the trained model to generate new .MIDI files for qualitative\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nevaluation. Both functions use the same Model Graph in different contexts: The training task,\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nshown at a high level in Figure 2, iteratively inputs a Note_State_Batch into the model, runs the\n   \n \n \n \n \n \n \n  \n \n \n \n \n \n \nmodel through all of the corresponding time steps and notes present in the batch, and then\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \noutputs a tensor of corresponding ‘Logits’, or inverse sigmoid probability that a given note at a\n  \n \n \n \n \n \n \n \n \n  \n \n   \ngiven time step is played/articulated. The log likelihood of the input data is interpreted as the\n \n \n  \n \n \n \n \n \n \n \n  \n \n \n \nability of the model to take as input a vector of notes at a given time step and to predict the set of\n \n \n \n  \n \n \n  \n \n \n   \n \n \n \n  \n \n \n \n \nnotes at the subsequent time step. The Loss Function, pseudo-code of which is shown in Figure\n  \n \n \n \n \n \n \n \n \n \n  \n  \n \n3, calculates cross-entropy between the generated Logits and the Note_State_Batch (after lining\n \n \n \n \n \n \n \n \n \n \n \n \nup the Logits to the Note_State_Batch elements corresponding to one time step in the future). \n \nFig. 2: High-level view of the Training Graph  \n \n \n \nFig. 3: Pseudo-code for Loss definition and calculation  \n \nDuring the music generation task, presented in Fig. 4, the model is iteratively run through\n \n \n \n \n \n  \n  \n \n  \n \n \n \none time step, every time feeding back the Generated Samples as the Note_State_Batch input for\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nthe subsequent time step. This samples are accumulate, and this produces a tensor of Generated\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \nSamples in the form of the Note_State_Batchof arbitrary time length. The Generated Samples\n \n \n \n \n \n \n \n \n \n \n \n \n \nare then converted to .MIDI files using post-processing functions from [42] for qualitative\n \n \n \n \n \n \n \n \n \n \n \n \n \nevaluation. \n \nFig. 4: High-level view of music generation graph.  \nA functional breakdown of the Model Graph, itself, is shown in Figure 5. Pseudo code\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nof the first stage of the model, referred to as the ‘Input Kernel’’, is shown in Figure 6. The Input\n \n \n \n \n \n \n \n  \n \n \n \n  \n  \n  \n \n \nKernel takes a Note_State_Batch as its input and for each note/articulation pair, generates an\n \n  \n \n \n \n \n \n \n \n \n \n \n \n \nexpanded vector that consists of: 1) the Midi note number, 2) a one hot vector of the note’s pitch\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \nclass, 3) window of the play/articulation values relative to the\n‘n’th note (the effective\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nconvolutional kernel aspect of the model), 4) a vector of the sum of all played notes in each pitch\n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n  \n \n \nclass, and 5) a binary-valued vector representing the 16 value position of the note within a\n \n \n  \n \n \n \n \n \n \n \n \n \n \n  \nmeasure.  \n \n \nFig. 5: Breakdown of Model Graph \n  \n \n \n \nFig. 6: Pseudo code for Input Kernel.  This code is performed in parallel note-wise, time-wise, and sample-wise.  \n \n \nThe second stage is referred to as the Timewise LSTM stage, pseudo-code for which is\n \n \n  \n \n \n \n \n \n \n \n \n \n  \nshown in Figure 7. In this block, an LSTM cell is run along the time axis for the length of the\n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \nbatch time dimension. This operation is performed on the Note_State_Expand vector for every\n \n \n \n \n  \n \n \n \n \n \n \n \nnote in parallel with tied weights. This part of the graph captures the sequential patterns of the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nmusic and, in combination with the Input Kernel, preserves translation invariance due to the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \ninput window of relative notes and the tied LSTM weights across all notes. Due to these tied\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nweights, the computations can be run in parallel across notes and across Note State Matrix\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nsamples as separate effective batches. The only required sequential aspect is along the time axis.\n \n \n \n \n \n \n \n \n \n  \n \n \n \n \nAn arbitrary number of cascaded LSTM cells can be run, and a dropout mask is applied after\n \n \n \n \n \n \n \n \n \n \n  \n \n  \n \n \neach cell. \n  \n \nFig. 7: Pseudo code for Timewise LSTM stage.  This code is performed in parallel note-wise and sample-wise. \n \nThe final stage in the Model Graph as described in the block diagram is the Notewise\n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \nLSTM stage, pseudo-code for which is shown in Figure 8.\nThis is a potentially one or\n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \nmulti-layered LSTM stage like the Timewise LSTM, also with dropout after each layer.\n \n \n \n \n \n \n \n \n \n \n \n \n \nHowever, instead of running sequentially along the time axis, this stage runs sequentially along\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nthe note axis. Furthermore, this section includes the ‘local’ feedback of generated samples into\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nits input. After each ‘note step’, the LSTM cell produces a pair of logits representing the inverse\n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \nsigmoid of the probability of generating a play/articulation for that note.\nNext, a play and\n \n \n \n \n \n  \n \n \n \n \n  \n \n \narticulation sample are drawn from this Bernoulli distribution. If the play sample is a ‘0’ for ‘not\n \n \n \n \n \n \n \n \n \n \n \n   \n \n \n \nplayed’, the articulation sample is forced to ‘0’, as well, to avoid the generation of any values not\n \n \n \n  \n  \n \n \n  \n \n \n \n \n \n \n \npresent in the input data. The generated sampled pair at note (n-1), concatenated with the input\n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \nof the timewise LSTM stage at note (n), is fed back into the input of the notewise LSTM for step\n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n(n).\nThis feedback creates a conditional probability for each note based on the actual values\n \n \n \n  \n \n \n \n \n \n \n \n \n \n \ngenerated for lower notes. This helps prevent dissonant simultaneous notes from being played.\n \n \n \n \n \n \n \n \n \n \n \n \n \nThe final output tensors of the Model Graph are the batch of Logits and corresponding Generated\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nSamples that are used for training and music generation, respectively. \n \nFig. 8: Pseudo Code for Notewise LSTM Stage.  This code is performed in parallel time-wise and sample-wise. \n \n3.3b Reinforcement Learning  \nTo capture the musical rules described in the last paragraph of section 3.2, the musical\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ncharacteristics present in the MIDI file were used. Taking a cue from Natasha Jacques’\n \n \n \n \n \n \n \n \n  \n \n \n \n \ncharacterization, three octaves of pitches starting from MIDI pitch forty eight are encoded as:\n \n \n \n \n \n \n \n \n \n \n \n \n \n \ntwo = C2, three =C#3, four = D3, etc, thirty-seven = B5. For instance, the sequence {3, 1, 0,1}\n  \n \n \n \n  \n \n \n  \n \n \n \n \n \n  \n \nencodes an eighth note with pitch C sharp, followed by an eighth note rest. The sequence {2, 4,\n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n  \n6, 7} encodes a melody of four sixteenth notes: C3, D3, E3, F3. A length 38 one-hot encoding of\n \n \n  \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \nthese values is used for both network input and network output.  \nThe learned weights of the Biaxial Model were used to prime the three sub-networks in\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nthe RL model. The overview of the RL model is shown in Figure 9.  \n \nFig. 9:  The Biaxial model is trained, and initializes the target Q network, Q network, and the Biaxial Reward.  \nThis paper’s RL model was trained for 50,000 iterations. For consistency with the Biaxial model,\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nthe optimizer used was ADADELTA. The learning rate selected was 1.0. Batch size of 32 was\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nused. In Jacques’ model, the optimizer used was Adam [43]. Gradients were clipped and the the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nlearning rate was set to .5, with a momentum of .85 to decay the learning rate every 1000 steps.\n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \nJacques’ model was trained for 1,000,000 iterations. Due to time and computational resource\n \n \n \n \n \n \n \n \n \n \n \n \n \nconstraints, I elected not to use Jacques’ model specifications for the number of iterations. The\n  \n \n \n \n \n \n \n \n \n \n \n \n \n \nRL model used in this paper did take a cue from Jacques’ for other aspects of the RL model: the\n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \nreward discount factor of γ=.5 was used. The Target Q-network’s weights θ were updated\n \n \n \n \n \n \n \n \n \n \n  \n \n \nincrementally to be effectively similar to the Q-network’s θ according to the formula (1 −η)θ −\n \n \n \n \n \n \n \n  \n  \n \n \n  \n  \n+ ηθ, where η = .01 is the Target-Q-network update rate. Once again, due to limited time and\n \n \n   \n  \n \n \n \n \n \n \n \n \n \n \n \ncomputational resources a minimum number of experiments were run. The modifying constant\n \n  \n \n \n \n \n \n \n \n \n \nof the musical theory reward\n, c, was set to equal .5. Boltzmann exploration and\n \n \n \n \n rMT  \n \n \n \n \n \n \n \n \n \nepsilon-greedy were coded, but only epsilon-greedy was run. Additionally, a Q-learning method\n \n \n \n \n \n \n \n \n  \n \n \nwith eligibility traces was coded but due to limited time and computational resources was not\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nrun. The RL model used Q-learning with the policy from the Biaxial model used as the cross\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nentropy reward. Next section will contain results.  \n  \n4. Results \n4.1. Quantitative Analysis \nTable 1 shows the log-likelihood performance of Daniel Johnson’s models, as well as that\n  \n \n \n \n \n \n \n \n \n \n \n \n \nof the pure Biaxial LSTM model implemented in this work. The results obtained in this work\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nwere, in general, on par with the survey of models reported by the Daniel Johnson but somewhat\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ninferior to the corresponding model. However, limitation of training time (due to time and\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nresource constraints) most likely plays the largest role in this discrepancy. In his blog, Daniel\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nJohnson estimated roughly 24-48 hours of training to capture the quality of his music samples,\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nwhereas the training for this work consisted of about 13 hours on a limited set of 1-2 dozen of\n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \nBach’s fugues from [36]. In addition, the author’s paper reported optimization using RMSprop\n \n \n \n \n \n \n \n \n \n \n \n \n \nwhereas his blog, which seemed to represent the latest of his progress, reported Adadelta. This\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nwork started with the latter, but more experimentation needs to be done to fine tune such hyper\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nparameters. \n \nModel \nLog-Likelihood \nHours \nTrained \nRandom \n-61 \n-- \nTP-LSTM-NAD\nE \n-5.44, -5.49 \n24 - 48 \nBALSTM \n-4.90, -5.00 \n24 - 48 \nBALSTM \n(this work) \n-6.27, -7.93 (test) \n-5.16, -6.59 \n(train) \n16 \n  \nTable 1: The top 3 rows represent the Log-likelihood performance reported by the original author for random \nweighted, LSTM-NADE, and  Bi-axial LSTM networks tested on the Piano-Midi.de data set.  The two values \nrepresent the best and median performance across 5 trials.  For the bottom row, the two values represent best and \nmedian across 100 trials for the BALSTM in this work, scaled by 88/78 to normalize it to the number of notes used \nby Daniel Johnson. \n \nTable 2 shows the performance of the models on the hard-coded music theory rules.\n  \n \n \n \n \n \n \n \n \n \n \n \n \nResults of this paper’s Biaxial model are in column 2 while Natasha Jacques’ priming model\n \n \n \n \n \n \n \n \n  \n \n \n \n \n \ncalled Note RNN are in column 3. In column 4, is this paper’s RL component primed with the\n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \nBiaxial model while column 5 is Natasha Jaques’ complete RL model. The statistics were\n \n \n \n  \n \n \n \n \n \n \n \n \n \ncomputed by randomly generating 1,000 songs from each model.  \nThe results show that RL helps improve almost in every column in the metrics column.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nOf note is the Biaxial model outperforms the Note RNN model in almost every metric that\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nrequires context: repeated notes and nearly all the auto-correlation measures. This follows from\n \n \n \n \n \n \n \n \n \n \n \n \n \nthe addition of the convolutional-esque kernel which slides over notes providing greater links\n \n \n \n \n \n \n \n \n \n \n \n \n \nbetween notes. This follows from the fact that convolution and autocorrelation are operations\n \n \n \n \n \n \n \n \n \n \n \n \n \nwith minor differences. In terms of the addition of the RL component, \nThis paper’s RL model and Jacques’ RL model perform in a pattern similar to the pure\n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \ndeep learning model. This paper’s RL model outperforms in contextual operations, like\n \n \n \n \n \n \n \n \n \n \n \n \nautocorrelation and is outperformed in every other musical theory category. This follows again\n \n  \n \n \n \n \n \n \n \n \n \n \nfrom the incorporation of a convolutional operation. The outperformance follows from the\n \n \n \n  \n \n \n \n \n \n \n \nsignificant gap in training iterations: 1,000,000 iterations for Natasha Jacques’ model and 50,000\n \n \n \n \n \n \n \n \n \n \n \n \n \niterations for this paper’s model. Given the discrepancy in training time, the incorporation of the\n \n \n \n \n \n \n \n  \n \n \n \n \n \n \nconvolutional operation indicates a significant increase in performance and a fruitful addition to\n \n \n  \n \n \n \n \n  \n \n  \nthe priming model. The addition of RL clearly indicates an improvement: learning play notes in\n \n \n \n \n \n \n \n \n \n \n \n \n \n  \nkey, autocorrelation decreases, more unique high and low notes, fewer leaps, and more motifs.  \nIn terms of the numbers themselves, the increase in performance on the metrics is tied to the\n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \nstrength of the reward signal for the specific behavior. For instance, playing a note frequently\n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \nreceived a serious negative reward. Rewards can be adjusted to alter performance on desired\n  \n \n \n \n \n \n \n \n \n \n \n \n \nmetrics.  \n \nMetric \nBiaxial Model (this \npaper) \nNote RNN \n(benchmark) \nQ (this paper) \nQ (benchmark) \nNotes repeated \n47.7% \n63.3% \n17.0% \n0.0% \nMean \nAutocorrelation – \nlag 1 \n0.13 \n-.16 \n-0.05 \n-.11 \nMean \nAutocorrelation – \nlag 2 \n-0.23 \n.14 \n0.09 \n.03 \nMean \nAutocorrelation – \nlag 3 \n-0.12 \n-.13 \n0.04 \n.03 \nNotes not in key \n0.4% \n.1% \n2.7% \n1.00% \nMelody starting with \ntonic \n0.1% \n.9% \n19.2% \n28.8% \nLeaps resolved \n69.1% \n77.2% \n79.3% \n91.1% \nMelodies with \nunique highest note \n42.9% \n64.7% \n34.7% \n56.4% \n \nMelodies unique \nlowest note \n55.7% \n49.4% \n48.9% \n51.9% \nNotes in motif \n3.2% \n5.9% \n49.3% \n75.7% \nNotes in repeated \nmotif \n.04% \n0.007% \n.07% \n.11% \n \nTable 2- Comparison of model performance on music theory rules. Rows starting with “Notes not in Key” and \nabove (inclusive), lower percentages are better. Rows starting with “Melody starting with tonic and below \n(inclusive), higher percentages are better.  \n ​4.2. Qualitative Analysis \nTo go beyond my untrained ear, I asked Professor Jospeh Dubiel of Columbia University, \nformer Chair of the Music department and the Chair of the Music Theory Area Committee, for \nhis thoughts on the piece. Professor Dubiel generously answered in detail. We communicated \nover email, so I will quote him fully.   \n\"Generated\" can mean a lot of things, especially in computer music. Sometimes it refers to the synthesis of \nthe sounds themselves, in contrast to what I assume in happening in your piece, sampling from a bank of \nprerecorded \"piano\" sounds.​ ​What really put me in a questioning mood was the sense of the sample you \nsent as losing its way every few beats: cohering for a very short time--from a few beats to a few \nseconds--then making an apparently pointless change, to something that might have a local connection, but \nthat is substantially different in direction (when there's direction at all), pacing, and sometimes even style. \nThis unevenness of continuity made me wonder whether the unit of selection was a single note, or \nsomething longer, perhaps a short figure drawn from a preexistent repertoire of such figures, or modeled on \nfigures in such a repertoire. \n \nYour sample doesn't quite sound like an intentional jump-cut piece, but might come off a little more \nlike human beginner's effort to make one of those than like a continuous composition in a traditional sense. \nListening to it one more time, I realize that I may have exaggerated the sense of cutting in order to try to \nbring the sample close to ​some​ kind of actual music. If I do my best to listen to it as a single succession, it's \nthe ​rhythmic​ discontinuity, the constant stopping and starting, that makes this unbelievable. The pauses just \ncome whenever, not when any motivic, harmonic, or phraseological action seems to have been completed \n(or at moments that would make sense as dramatic interruptions). The final stop is definitely one of these: I \ncome to the end of the sample with absolutely no sense that it was a composition, as opposed to an arbitrary \nselection from a stream of indefinite length. I can imagine how this might happen if the models used to \nproduce it are as narrowly focused on pitch as these may have been.” \nQualitatively, the samples produced by this model when listened to by a professional clearly \nsuggest limitations. The music breaks down in its ability to create clear transitions between \nlarger ideas in the piece as a whole. There is no deeper structure. The sample also makes poor \nuse of negative space, few pauses are present in the work. Due to the lack of global structure, the \nmusic has a mechanical feel. An important note is the length of training time. When the model is \ntrained for 30 minutes, the music generated is sparse and significantly less consistent and \ncoherent. When trained for 2 hours, the difference is dramatic.  Clear relationships between \ngenerated music and the corresponding training files developed.  \n4.3. Discussion of Insights Gained \nIt became clear how the variability and complexity of music on which the model was trained\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \naffected the outcome.\nTraining a newly initialized model on a large data set consisting of\n \n \n \n  \n \n \n \n  \n \n \n \n \n \nsignificant variability in music segments (i.e. fast monodic and slow polyphonic) tended to create\n \n  \n \n \n \n \n \n \n \n \n  \n \na model that seemed to be confused at first. Trying to learn such a range of features requires a\n \n \n \n \n \n \n  \n \n \n \n \n  \n \n \n \n  \ncomplex function needing long training times.\nTraining on a set consisting of 22 of Bach’s\n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \nfugues from [36] obtained better results more quickly than training on the 120 Piano-de-Midi for\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nmodest training times < 2 hours. However, it became evident that very long training times were\n \n \n   \n \n  \n \n \n \n \n \n \n \n \nrequired, in general to produce decent music. It was clear the music was gradually learning\n \n \n \n \n \n \n  \n \n \n \n \n \n \n \nrhythm and chord structures, however it sounded as if a human were learning to play piano by\n \n \n \n \n  \n \n   \n \n \n \n \n \n \n \ntrying to play songs that were too difficult. One possible training strategy may be to train on a\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \nsuccession of increasingly difficult songs, graduating the model manually, or perhaps in an\n \n \n \n \n \n \n \n \n \n \n \n \n \nautomated fashion once a certain ability or log likelihood was achieved. In addition to songs of\n \n \n  \n \n \n \n \n \n \n \n \n  \n \n \ndifferent ‘level-of-difficulty’, training could begin on very short time segments and increase to\n \n \n \n \n \n \n \n \n \n \n \n \n \nvery long segments to allow the model to learn basic structure in addition to longer musical\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nform. This approach to learning is in the vein of curriculum learning  [44]. \nIn terms of future work, it would be fruitful to add to the bi-axial LSTM a component that\n \n \n \n \n  \n \n \n  \n  \n \n \n  \n \n \nfocused on structure alone. There has been good work showing the merits of using Restricted\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nBoltzmann Machines to model chord progressions and other forms of harmonic and melodic\n \n \n \n \n \n \n \n \n \n \n \n \n \nstructure. Additionally, an effective model could incorporate genetic algorithms. The line of\n \n \n \n \n \n \n \n \n \n \n \n \nthinking would be to train the model on some simple music and set the fitness score as a proxy\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \nfor novelty, and allow the algorithm to generate mutations to add complexity to the piece over\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ntime. Other dynamic optimization techniques like ant colony optimization could also prove to be\n \n \n \n \n \n \n \n \n \n \n \n  \n \neffective. Another model design that would be effective would be Generative Adversarial\n \n \n \n \n \n \n \n \n \n \n \n \nNetworks (GANs) [45] which have achieved remarkable progress in generating photo-realistic\n \n \n \n \n \n \n \n \n \n \n \nimages and as such should provide effective musical generation. Moving beyond the deep\n \n \n \n \n \n \n \n \n \n \n \n \n \nlearning priming model, a more innovative approach is to rely on reinforcement learning and\n \n \n  \n \n \n  \n \n \n \n \n \n \nincorporate a more refined sense of exploration in the music generation. A potential refinement\n  \n \n \n \n \n \n \n \n \n  \n \n \n \ncan enter in the sampling the action space: paths or musical measures explored may have\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ndifferent rewards associated with the distribution. I​n a similar vein, I suggest exploration can\n \n \n \n \n \n \n  \n \n  \n \n \n \nenter by building into the model a sense of choosing the action that maximizes the expected\n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \nreward with respect to a randomly drawn belief. This method is called Thompson Sampling [46]\n \n \n \n  \n \n \n \n \n  \n \n \n \n \nand draws from the Multi-armed bandit literature. Another idea is to move beyond the one-hot\n \n \n \n \n \n \n \n \n  \n \n \n \n \n \nencoding of notes and look at a vectorized representation musical chord embeddings, akin to\n \n \n \n \n \n  \n \n \n \n \n \n \n \nword2vec for music. There has been work that creates a chord2vec tool [47], which can improve\n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \nperformance. Further, incorporating a hierarchical training strategy could prove to be effective:\n \n \n  \n \n \n \n \n \n \n \n \ntaking an idea from the image segmentation literature, and separate out different instruments and\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nhave different models focus on representing with fidelity the sound of say the cello. This could\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nimprove performance and sound quality in the produced samples. Additionally, an issue with\n \n \n \n \n \n \n \n \n \n \n \n \n \ncreating global coherence in the music is having the algorithm learn when to shift between ideas\n \n \n \n \n \n  \n \n \n \n \n  \n \n \n \nin the composition. Having clear labels for transitions between motifs via a spatio-temporal (note\n \n \n \n \n \n \n \n \n \n \n  \n \n \naxis and time axis) labeling mechanism could improve global coherence. Additionally, currently\n \n \n \n \n \n \n \n \n \n \n \n \nthe music is formulated as a discrete time problem: the song is sliced into discrete chunks and\n \n  \n \n  \n \n \n \n \n  \n \n \n \n \n \nsolved recursively via Q learning through Bellman’s Equation. There has been work to create\n \n \n \n \n \n \n \n \n \n \n \n \n \n \ncontinuous deep Q learning with model based acceleration, which would limit loss of\n \n \n \n \n \n \n \n \n \n \n \n \n \ninformation in connecting the local subproblems, or linking the discrete notes [48]. The\n \n \n \n \n \n \n \n \n \n \n \n \n \nHamilton Jacobi Bellman equation could prove to be useful. Computational time and resources\n \n \n \n \n \n \n \n \n \n \n \n \n \nscale at an unpalatable rate when using more and more complex models, called the “Curse of\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nDimensionality”. Working in the spectral domain, affords potentially significant increases in\n \n \n \n \n \n \n \n \n \n \n \nspeed. This seems to be a natural extension given the raw waveform of the file [49]. There is\n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n  \n \nmuch theoretical work that can be done to link the process of exploring the action space to\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nstochastic optimal control [50]. This can be extended with the Fenyman-Kac formula [49] which\n \n \n \n \n \n \n \n \n \n \n \n \n \n \noffers a link between partial differential equations and stochastic processes. The gist of the idea\n  \n \n \n \n \n \n \n \n \n \n \n \n \n \nis to consider the process of music selection as an interacting particle system. This affords a nice\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \nformulation in terms of game theory: mean field games [50] and randomized equilibria [51] map\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \non nicely to selecting different sub-equilibria of high reward actions. Convergence and stability\n \n \n \n \n \n \n \n \n \n \n \n \n \ncan be shown in discrete time by solving the Hamiltonian, making a fixed point argument and\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \nfinding an equilibrium concept. Beyond the reinforcement learning, further work can be done on\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nthe musical side by incorporating expert rules for musical creation.  \n  \n5. Conclusion \nIn this paper, a two layer LSTM model with a convolutional-esque kernel capable of learning\n \n \n  \n \n \n \n \n  \n \n \n \n \n \nharmonic and melodic rhythmic probabilities from polyphonic MIDI files of Bach was created.\n \n \n \n \n \n \n \n \n \n \n \n \n \nThe model design was explained, with an eye to key functional principles of flexibility and\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ngeneralizability. The model was extended with a reinforcement learning (DQN) approach. The\n \n \n \n \n \n  \n \n \n \n \n \nunderlying logic and method of training and generation of algorithmic music were presented.\n \n \n \n \n \n \n \n \n \n \n \n \n \nFurther, the outputs of the model were analyzed in a quantitative and qualitative fashion. Some\n \n \n \n \n \n \n \n \n  \n \n \n \n \n \nsuggestions were then put forward for future work. \n  \n6. Acknowledgement \nThe opportunity to apply powerful tools to fascinating problems all within a supportive\n \n \n \n \n \n \n \n \n \n \n  \n \nenvironment is a tremendous gift. My sincere gratitude to: my thesis advisor Professor Dan\n \n  \n \n \n \n \n \n \n \n \n \n \n \nRusso for his clarity and warmth; Professor Joseph Dubiel for his insights and time; Professor\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nZoran Kostic for teaching me deep learning; Paul for our many talks; QMSS and Professor Elena\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nKrumova for a structure that encourages curiosity and open inquiry; Professor Greg Eirich for his\n \n  \n \n \n \n \n \n \n \n \n \n \n \n \nwisdom and generosity of spirit; Mom, Dad, and Krishen for their love and humor; Andrew for\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nhis friendship; Aryeh for his inspiration; and Dana for our time on the water.  \n \n \n \n7. References \n[1]​Briot, Jean-Pierre, Gaëtan Hadjeres, and François Pachet. \"Deep Learning Techniques for Music Generation-A\n \n \n \n \n \n \n \n \n \n \n \n \n \nSurvey.\" ​arXiv preprint arXiv:1709.01620​ (2017). \n[2] ​Glenn Gould is a notable example of someone who excelled in interpreting the counterpoint in Bach’s Goldberg\n \n \n   \n \n \n \n \n \n  \n \n \n  \n \n \nVariations. \n[3]​Chen, C-CJ, and Risto Miikkulainen. \"Creating melodies with evolving recurrent neural networks.\" In ​Neural\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nNetworks, 2001. Proceedings. IJCNN'01. International Joint Conference on​, vol. 3, pp. 2241-2246. IEEE, 2001. \n[4] Eck, Douglas, and Juergen Schmidhuber. \"A first look at music composition using lstm recurrent neural\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nnetworks.\" ​Istituto Dalle Molle Di Studi Sull Intelligenza Artificiale​ 103 (2002). \n[5] Sturm, Bob L., et al. \"Music transcription modelling and composition using deep learning.\" ​arXiv preprint\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \narXiv:1604.08723​ (2016). \n[6] Hubel, D. H. & Wiesel, T. N. Shape and arrangement of columns in cat’s striate cortex. J. Physiol. 165, 559–568\n \n \n \n  \n \n \n \n \n \n \n \n  \n \n \n  \n \n \n \n(1963) \n[7]Johnson, Daniel D. \"Generating Polyphonic Music Using Tied Parallel Networks.\" In ​International Conference\n \n \n \n \n \n \n \n \n \n \n \n \n \non Evolutionary and Biologically Inspired Music and Art​, pp. 128-143. Springer, Cham, 2017. \n[8] Ranzato, Marc'Aurelio, et al. \"Sequence level training with recurrent neural networks.\" ​arXiv preprint\n \n \n \n \n \n \n \n \n \n \n \n \n \n \narXiv:1511.06732​(2015). \n[9] ​Bahdanau et al. An actor-critic algorithm for sequence prediction. arXiv preprint:1607.07086, 2016 \n[10] Wu, Yonghui, et al. \"Google's neural machine translation system: Bridging the gap between human and\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nmachine translation.\" ​arXiv preprint arXiv:1609.08144​ (2016). \n[11] ​Jiwei Li, Will Monroe, Alan Ritter, and Dan Jurafsky. Deep reinforcement learning for dialogue generation.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \narXiv preprint arXiv:1606.01541, 2016. \n[12] Jaques, Natasha, et al. \"Tuning recurrent neural networks with reinforcement learning.\" (2017). \n[13] Powell, Warren B. ​Approximate Dynamic Programming: Solving the curses of dimensionality​. Vol. 703. John\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nWiley & Sons, 2007. \n[14] Bellman, Richard. ​Dynamic programming​. Courier Corporation, 2013. \n[15] Littman, Michael L. \"Markov games as a framework for multi-agent reinforcement learning.\" ​Machine\n \n \n \n \n \n \n  \n \n \n \n \n \n \nLearning Proceedings 1994​. 1994. 157-163. \n[16] Watkins, Christopher JCH, and Peter Dayan. \"Q-learning.\" ​Machine learning​ 8.3-4 (1992): 279-292. \n[17] Sutton, Richard S., Doina Precup, and Satinder Singh. \"Between MDPs and semi-MDPs: A framework for\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ntemporal abstraction in reinforcement learning.\" ​Artificial intelligence​112.1-2 (1999): 181-211. \n[18] Mnih, Volodymyr, et al. \"Playing atari with deep reinforcement learning.\" ​arXiv preprint arXiv:1312.5602\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n(2013). \n[19] Mnih, Volodymyr, et al. \"Human-level control through deep reinforcement learning.\" ​Nature 518.7540 (2015):\n \n \n  \n \n \n \n \n \n \n \n \n \n \n529. \n[20] Lillicrap, Timothy P., et al. \"Continuous control with deep reinforcement learning.\" ​arXiv preprint\n \n \n \n \n \n \n \n \n \n \n \n \n \n \narXiv:1509.02971​(2015). \n[21] Van Hasselt, Hado, Arthur Guez, and David Silver. \"Deep Reinforcement Learning with Double Q-Learning.\"\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nAAAI​. Vol. 16. 2016. \n[22]Hochreiter, Sepp, and Jürgen Schmidhuber. \"Long short-term memory.\" ​Neural computation 9, no. 8 (1997):\n \n \n \n \n \n \n \n \n \n \n \n  \n \n1735-1780. \n[23] Graves, Alex. \"Generating sequences with recurrent neural networks.\" ​arXiv preprint arXiv:1308.0850​ (2013). \n[24] Graves, Alex, and Jürgen Schmidhuber. \"Framewise phoneme classification with bidirectional LSTM and other\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nneural network architectures.\" ​Neural Networks​ 18.5-6 (2005): 602-610. \n \n[25] To access the data set, go to the following link. Search for the desired songs and the musical score will pop up. :\n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \nhttps://www.bach-digital.de/content/index.xed​. \n[26]\n​To\naccess\nthe\ndataset,\ngo\nto\nthe\nlink\nand\nclick\ndata\nfolder\nand\ndownload\nthe\nzip.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nhttps://archive.ics.uci.edu/ml/datasets/Bach+Choral+Harmony \n[29] ​To access the dataset, go to the link and click the pdfs and download the corresponding sheet music:\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nhttp://www.theviolinsite.com/sheet_music/bach/index.html​. \n[30] Gauldin, Robert. ​A Practical Approach to 18th Century Counterpoint: Revised Edition​. Waveland Press, 2013. \n[31] Fux, Johann Joseph. ​The study of counterpoint from Johann Joseph Fux's Gradus ad Parnassum​. No. 277. WW\n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \nNorton & Company, 1965. \n[32] Schoenberg, Arnold, Leonard Stein, and Gerald Strang. ​Fundamentals of musical composition​. London: Faber\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n& Faber, 1967. \n[33] Rimsky-Korsakov, Nikolay. ​Principles of orchestration​. Courier Corporation, 1964. \n[34] Tchaikovsky, Peter Ilich. ​Guide to the practical study of harmony​. Courier Corporation, 1900. \n[35]​https://github.com/hexahedria/biaxial-rnn-music-composition \n[36]​https://github.com/dshieble/Musical_Matrices \n[37] Bach Data:​ ​http://www.bachcentral.com/midiindexcomplete.html \n[38] Other piano data source:​ ​http://www.piano-midi.de/midi_files.htm \n[39]Moon, T., Choi, H., Lee, H., Song, I.: RnnDrop: a novel dropout for RNNs in ASR. In: Automatic Speech\n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \nRecognition and Understanding (ASRU) (2015). \n[40]​Zeiler, Matthew D. \"ADADELTA: an adaptive learning rate method.\" ​arXiv preprint arXiv:1212.5701​ (2012). \n[41] Krim, Hamid, and Mats Viberg. \"Two decades of array signal processing research: the parametric approach.\"\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nIEEE signal processing magazine​ 13.4 (1996): 67-94. \n[42]​ Varshney, Lav R., and John Z. Sun. \"Why do we perceive logarithmically?.\" ​Significance​ 10.1 (2013): 28-31. \n[43] https://github.com/vishnubob/python-midi \n[44] Kingma, Diederik P., and Jimmy Ba. \"Adam: A method for stochastic optimization.\" ​arXiv preprint\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \narXiv:1412.6980​(2014). \n[45] Bengio, Yoshua, et al. \"Curriculum learning.\" Proceedings of the 26th annual international conference on\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nmachine learning. ACM, 2009. \n[46]​ Goodfellow, Ian, et al. \"Generative adversarial nets.\" ​Advances in neural information processing systems​. 2014. \n[47]Agrawal, Shipra, and Navin Goyal. \"Analysis of Thompson sampling for the multi-armed bandit problem.\" In\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nConference on Learning Theory​, pp. 39-1. 2012. \n[48] Madjiheurem, Sephora, Lizhen Qu, and Christian Walder. \"Chord2Vec: Learning musical chord embeddings.\"\n \n \n \n \n \n \n \n \n \n \n \n \n \nProceedings of the Constructive Machine Learning Workshop at 30th Conference on Neural Information Processing\n \n \n \n \n \n \n  \n \n \n \n \n \n \nSystems (NIPS’2016), Barcelona, Spain. 2016. \n[49] Gu, Shixiang, et al. \"Continuous deep q-learning with model-based acceleration.\" International Conference on\n \n \n  \n \n \n \n \n \n \n \n \n \n \nMachine Learning. 2016. \n[50] Rippel, Oren, Jasper Snoek, and Ryan P. Adams. \"Spectral representations for convolutional neural networks.\"\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nAdvances in neural information processing systems​. 2015. \n[51] Dimitri P. Bertsekas. ​Dynamic programming and optimal control​. Vol. 1, no. 2. Belmont, MA: Athena\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nscientific, 1995. \n[52] Del Moral, Pierre, and Laurent Miclo. \"Branching and interacting particle systems approximations of\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFeynman-Kac formulae with applications to non-linear filtering.\" ​Séminaire de probabilités XXXIV​. Springer,\n \n \n \n \n \n \n \n \n \n \n \n \nBerlin, Heidelberg, 2000. 1-145. \n[53] ​Yang, Yaodong, et al. \"Mean Field Multi-Agent Reinforcement Learning.\" ​arXiv preprint arXiv:1802.05438\n \n \n \n \n \n \n \n \n \n \n \n \n \n(2018). \n[54] Carmona, René, François Delarue, and Daniel Lacker. \"Mean field games with common noise.\" ​The Annals of\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \nProbability​44.6 (2016): 3740-3803. \n",
  "categories": [
    "cs.SD",
    "cs.LG",
    "eess.AS",
    "stat.ML"
  ],
  "published": "2018-12-03",
  "updated": "2018-12-03"
}