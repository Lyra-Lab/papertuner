{
  "id": "http://arxiv.org/abs/1812.01193v2",
  "title": "e-SNLI: Natural Language Inference with Natural Language Explanations",
  "authors": [
    "Oana-Maria Camburu",
    "Tim Rocktäschel",
    "Thomas Lukasiewicz",
    "Phil Blunsom"
  ],
  "abstract": "In order for machine learning to garner widespread public adoption, models\nmust be able to provide interpretable and robust explanations for their\ndecisions, as well as learn from human-provided explanations at train time. In\nthis work, we extend the Stanford Natural Language Inference dataset with an\nadditional layer of human-annotated natural language explanations of the\nentailment relations. We further implement models that incorporate these\nexplanations into their training process and output them at test time. We show\nhow our corpus of explanations, which we call e-SNLI, can be used for various\ngoals, such as obtaining full sentence justifications of a model's decisions,\nimproving universal sentence representations and transferring to out-of-domain\nNLI datasets. Our dataset thus opens up a range of research directions for\nusing natural language explanations, both for improving models and for\nasserting their trust.",
  "text": "arXiv:1812.01193v2  [cs.CL]  6 Dec 2018\ne-SNLI: Natural Language Inference with\nNatural Language Explanations\nOana-Maria Camburu1 Tim Rocktäschel2 Thomas Lukasiewicz1,3 Phil Blunsom1,4\n{oana-maria.camburu, thomas.lukasiewicz, phil.blunsom}@cs.ox.ac.uk\nt.rocktaschel@ucl.ac.uk\n1Department of Computer Science, University of Oxford\n2Department of Computer Science, University College London\n3Alan Turing Institute, London, UK\n4DeepMind, London, UK\nAbstract\nIn order for machine learning to garner widespread public adoption, models must\nbe able to provide interpretable and robust explanations for their decisions, as\nwell as learn from human-provided explanations at train time. In this work, we\nextend the Stanford Natural Language Inference dataset with an additional layer\nof human-annotated natural language explanations of the entailment relations. We\nfurther implement models that incorporate these explanations into their training\nprocess and output them at test time. We show how our corpus of explanations,\nwhich we call e-SNLI, can be used for various goals, such as obtaining full sen-\ntence justiﬁcations of a model’s decisions, improving universal sentence represen-\ntations and transferring to out-of-domain NLI datasets. Our dataset1 thus opens\nup a range of research directions for using natural language explanations, both for\nimproving models and for asserting their trust.\n1\nIntroduction\nHumans do not learn solely from labeled examples supplied by a teacher. Instead, they seek a\nconceptual understanding of a task through both demonstrations and explanations. Machine learning\nmodels trained simply to obtain high accuracy on held-out sets often learn to rely heavily on shallow\ninput statistics, resulting in brittle models susceptible to adversarial attacks. For example, Ribeiro\net al. [24] present a document classiﬁer that distinguishes between Christianity and Atheism with an\naccuracy of 94%. However, on close inspection, the model spuriously separates classes based on\nwords contained in the headers, such as Posting, Host, and Re.\nIn this work, we introduce a new dataset and models for exploiting and generating explanations for\nthe task of recognizing textual entailment. We argue for free-form natural language explanations, as\nopposed to formal language, for a series of reasons. First, natural language is readily comprehensi-\nble to an end-user who needs to assert a model’s reliability. Secondly, it is also easiest for humans\nto provide free-form language, eliminating the additional effort of learning to produce formal lan-\nguage, thus making it simpler to collect such datasets. Lastly, natural language justiﬁcations might\neventually be mined from existing large-scale free-form text.\nDespite the potential for free-form justiﬁcations to improve both learning and transparency, there\nis currently a lack of such datasets in the machine learning community. To address this deﬁciency,\n1https://github.com/OanaMariaCamburu/e-SNLI\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.\nPremise: An adult dressed in black holds a stick.\nHypothesis: An adult is walking away, empty-handed.\nLabel: contradiction\nExplanation: Holds a stick implies using hands so it is not empty-handed.\nPremise: A child in a yellow plastic safety swing is laughing as a dark-haired woman\nin pink and coral pants stands behind her.\nHypothesis: A young mother is playing with her daughter in a swing.\nLabel: neutral\nExplanation: Child does not imply daughter and woman does not imply mother.\nPremise: A man in an orange vest leans over a pickup truck.\nHypothesis: A man is touching a truck.\nLabel: entailment\nExplanation: Man leans over a pickup truck implies that he is touching it.\nFigure 1: Examples from e-SNLI. Annotators were given the premise, hypothesis, and label. They\nhighlighted the words that they considered essential for the label and provided the explanations.\nwe have collected a large corpus of human-annotated explanations for the Stanford Natural Lan-\nguage Inference (SNLI) dataset [3]. We chose SNLI because it constitutes an inﬂuential corpus for\nnatural language understanding that requires deep assimilation of ﬁne-grained nuances of common-\nsense knowledge. We call our explanation-augmented dataset e-SNLI, which we collected to enable\nresearch in the direction of training with and generation of free-form textual justiﬁcations.\nIn order to demonstrate the efﬁcacy of the e-SNLI dataset, we ﬁrst show that it is much more difﬁcult\nto produce correct explanations based on spurious correlations than to produce correct labels. We\nthen implement models that, given a premise and a hypothesis, predict a label and an explanation.\nWe also investigate how the additional signal from explanations received at train time can guide\nmodels into learning better sentence representations. Finally, we look into the transfer capabilities\nof our model to out-of-domain NLI datasets.\n2\nBackground\nThe task of recognizing textual entailment is a critical natural language understanding task. Given\na pair of sentences, called the premise and hypothesis, the task consists of classifying their relation\nas either (a) entailment, if the premise entails the hypothesis, (b) contradiction, if the hypothesis\ncontradicts the premise, or (c) neutral, if neither entailment nor contradiction hold. The SNLI\ndataset [3], containing 570K data points of human-generated triples (premise, hypothesis, label),\nhas driven the development of a large number of neural network models [25, 21, 22, 6, 19, 5, 7].\nConneau et al. [7] showed that training universal sentence representations on SNLI is both more ef-\nﬁcient and more accurate than the traditional training approaches on orders of magnitude larger, but\nunsupervised, datasets [17, 14]. We take this approach one step further and show that an additional\nlayer of explanations on top of the label supervision brings further improvement.\nRecently, Gururangan et al. [13] cast doubt on whether models trained on SNLI are learning to\nunderstand language, or are largely ﬁxating on spurious correlations, also called artifacts. For ex-\nample, speciﬁc words in the hypothesis tend to be strong indicators of the label, e.g., friends, old\nappear very often in neutral hypotheses, animal, outdoors appear most of the time in entailment\nhypotheses, while nobody, sleeping appear mostly in contradiction hypothesis. They show that a\npremise-agnostic model, i.e., a model that only takes as input the hypothesis and outputs the label,\nobtains 67% test accuracy. In section 4.1 we show that it is much more difﬁcult to rely on artifacts\nto generate explanations than to generate labels.\n3\nCollecting explanations\nWe present our collection methodology for e-SNLI, for which we used Amazon Mechanical Turk.\nThe main question that we want our dataset to answer is: Why is a pair of sentences in a relation of\n2\nentailment, neutrality, or contradiction? We encouraged the annotators to focus on the non-obvious\nelements that induce the given relation, and not on the parts of the premise that are repeated identi-\ncally in the hypothesis. For entailment, we required justiﬁcations of all the parts of the hypothesis\nthat do not appear in the premise. For neutral and contradictory pairs, while we encouraged stating\nall the elements that contribute to the relation, we consider an explanation correct, if at least one el-\nement is stated. Finally, we asked the annotators to provide self-contained explanations, as opposed\nto sentences that would make sense only after reading the premise and hypothesis. For example,\nwe prefer an explanation of the form “Anyone can knit, not just women.”, rather than “It cannot be\ninferred they are women.”\nIn crowd-sourcing, it is difﬁcult to control the quality of free-form annotations. Thus, we aimed\nto preemptively block the submission of obviously incorrect answers. We did in-browser checks\nto ensure that each explanation contained at least three tokens and that it was not a copy of the\npremise or hypothesis. We further guided the annotators to provide adequate answers by asking\nthem to proceed in two steps. First, we require them to highlight words from the premise and/or\nhypothesis that they consider essential for the given relation. Secondly, annotators had to formulate\nthe explanation using the words that they highlighted. However, using exact spelling might push\nannotators to formulate grammatically incorrect sentences, therefore we only required half of the\nhighlighted words to be used with the same spelling. For entailment pairs, we required at least one\nword in the premise to be highlighted. For contradiction pairs, we required highlighting at least\none word in both the premise and the hypothesis. For neutral pairs, we only allowed highlighting\nwords in the hypothesis, in order to strongly emphasize the asymmetry in this relation and to prevent\nworkers from confusing the premise with the hypothesis. We believe these label-speciﬁc constraints\nhelped in putting the annotator into the correct mindset, and additionally gave us a means to ﬁlter\nincorrect explanations. Finally, we also checked that the annotators used other words that were\nnot highlighted, as we believe a correct explanation would need to articulate a link between the\nkeywords.\nWe collected one explanation for each pair in the training set and three explanations for each pair in\nthe validation and test sets. Figure 1 shows examples of collected explanations. There were 6325\nworkers with an average of 860 explanations per worker and a standard deviation of 403.\nAnalysis and reﬁnement of the collected dataset In order to measure the quality of our collected\nexplanations, we selected a random sample of 1000 examples and manually graded their correctness\nbetween 0 (incorrect) and 1 (correct), giving partial scores of k/n if only k out of n required argu-\nments were mentioned. We also considered an explanation as incorrect if it was uninformative, that\nis, if the explanation was template-like, extensively repeating details from the premise/hypothesis\nthat are not directly useful for justifying the relation between the two sentences. We observed a few\nre-occurring templates such as: “Just because [entire premise] doesn’t mean [entire hypothesis]” for\nneutral pairs, “[entire premise] implies [entire hypothesis]” for entailment pairs, and “It can either\nbe [entire premise] or [entire hypothesis]” for contradiction pairs. We assembled a list of templates,\nwhich can be found in Appendix A, that we used for ﬁltering the dataset of such uninformative\nexplanations. Speciﬁcally, we ﬁltered an explanation if its edit distance to one of the templates\nwas less than 10. We ran this template detection on the entire dataset and reannotated the detected\nexplanations (11% in total).\nOur ﬁnal counts show a total error rate of 9.62%, with 19.55% on entailment, 7.26% on neutral,\nand 9.38% on contradiction. We notice that entailment pairs were by far the most difﬁcult to obtain\nproper explanations for. This is ﬁrstly due to partial explanations, as annotators had an incentive to\nprovide shorter inputs, so they often only mentioned one argument. A second reason is that many of\nthe entailment pairs have the hypothesis as almost a subset of the premise, prompting the annotators\nto just repeat that as a statement.\n4\nExperiments\nWe ﬁrst present an experiment which demonstrates that a model which can easily rely on artifacts in\nSNLI to provide correct labels would not be able to provide correct explanations as easily. We refer\nto it as PREMISEAGNOSTIC.\nWe then present a series of experiments to elucidate whether models trained on e-SNLI are able to:\n(i) predict a label and generate an explanation for the predicted label (referred to as PREDICTAND-\n3\nEXPLAIN), (ii) generate an explanation then predict the label given only the generated explanation\n(EXPLAINTHENPREDICT), (iii) learn better universal sentence representations (REPRESENT), and\n(iv) transfer to out-of-domain NLI datasets (TRANSFER).\nThroughout our experiments, our models follow the architecture presented in Conneau et al. [7], as\nwe build directly on top of their code2. Therefore, our encoders are 2048-bidirectional-LSTMs [15]\nwith max-pooling, resulting in a sentence representation dimension of 4096. Our label classiﬁers\nare 3-layers MLPs with 512 internal size and without non-linearities. For our explanation decoders,\nwe used a simple one-layer LSTM, for which we tried internal sizes of 512, 1024, 2048, and 4096.\nIn order to reduce the vocabulary size for explanation generation, we replaced words that appeared\nless than 15 times3 with <UNK>. We obtain an output vocabulary of approximately 12K words. The\npreprocessing and optimization were kept the same as in [7].\nWhenever appropriate, we run our models with ﬁve seeds and provide the average performance with\nthe standard deviation in parenthesis. If no standard deviation is reported, the results are from one\nexperiment with seed 1234.\n4.1\nPREMISEAGNOSTIC: Generate an explanation given only the hypothesis\nGururangan et al. [13] show that a neural network that only has access to the hypothesis can predict\nthe correct label 67% of the times. We are therefore interested in evaluating how well our explana-\ntions can be predicted from hypotheses alone.\nModel We train a 2048-bidirectional-LSTM with max-pooling for encoding the hypothesis, fol-\nlowed by a one-layer LSTM for decoding the explanation. The initial state of the decoder is the\nvector embedding of the hypothesis, which is also concatenated at every timestep of the decoder, to\navoid forgetting.\nSelection We consider internal sizes of the decoder of 512, 1024, 2048 and 4096. We pick the model\nthat gives the best perplexity on the validation set. We notice that the perplexity strictly decreases\nwhen we increase the decoder size. However, for practical reasons, we do not increase the decoder\nsize beyond 4096.\nResults We then manually look at the ﬁrst 100 test examples and obtain that only 6.834 were\ncorrect. We also separately train the same hypothesis-only encoder for label prediction alone and\nobtain 66 correct labels in the same ﬁrst 100 test examples. This validates our intuition that it is\nmuch more difﬁcult (approx. 10x for this architecture) to rely on spurious correlations to predict\ncorrect explanations than to predict correct labels.\n4.2\nPREDICTANDEXPLAIN: Jointly predict a label and generate an explanation for the\npredicted label\nIn this experiment, we investigate how the typical architecture employed on SNLI can be enhanced\nwith a module that aims to justify the decisions of the entire network.\nModel We employ the InferSent [7] architecture, where a bidirectional-LSTM with max-pooling\nseparately encodes the premise, u, and hypothesis, v. The vector of features f = [u, v, |u−v|, u⊙\nv] is then passed to the MLP classiﬁer that outputs a distribution over the 3 labels. We add a one-\nlayer LSTM decoder for explanations, which takes the feature vector f both as an initial state and\nconcatenated to the word embedding at each time step.\nIn order to condition the explanation also on the label, we prepend the label as a word (entailment,\ncontradiction, neutral) at the beginning of the explanation. At training time, the gold label is pro-\nvided, while at test time, we use the label predicted by the classiﬁer. This architecture is depicted in\nFigure 2.\nLoss We use negative log-likelihood for both classiﬁcation and explanation losses. The explanation\nloss is much larger in magnitude than the classiﬁcation loss, due to the summation of negative log-\n2https://github.com/facebookresearch/InferSent. We ﬁxed the issue raised in https://github.com/ facebookre-\nsearch/InferSent/issues/51 that the max-pooling was taken over paddings.\n3Counted among premises, hypothesis, and explanations.\n4Partial scoring as explained in Section 3.\n4\nlikelihoods over the words in the explanations. To account for this difference during training, we\nuse a weighting coefﬁcient α ∈[0, 1]. Hence, our overall loss is:\nLtotal = αLlabel + (1 −α)Lexplanation\n(1)\nPremise\nHypothesis\nf\nExplanation\nLabel\nFigure 2: Overview of the e-INFERSENT architec-\nture.\nSelection We consider α values from 0.1 to 0.9\nwith a step of 0.1 and decoder internal sizes of\n512, 1024, 2048, and 4096. For this experiment,\nwe choose as model selection criterion the ac-\ncuracy on the SNLI validation set, because we\nwant to investigate how well a model can gen-\nerate justiﬁcations without sacriﬁcing accuracy.\nAs future work, one can inspect different trade-\noffs between accuracy and explanation genera-\ntion. We found α = 0.6 and the decoder size of\n512 to produce the best validation accuracy, of\n84.37%, while InferSent with no explanations\nproduced 84.30% validation accuracy. We call\nour model e-INFERSENT, since it freezes the In-\nferSent architecture and training procedure, and\nonly adds the explanations decoder.\nResults The average test accuracy that we obtain when training InferSent[7] on SNLI with ﬁve seeds\nis 84.01% (0.25). Our e-INFERSENT model obtains essentially the same test accuracy, of 83.96%\n(0.26), which shows that one can get additional justiﬁcations without sacriﬁcing label accuracy. For\nthe generated explanations, we obtain a perplexity of 10.58(0.4) and a BLEU-score of 22.40(0.7).\nSince we collected 3 explanations for each example in the validation and test sets, we compute\nthe inter-annotator BLEU-score of the third explanation with respect to the ﬁrst two, and obtain\n22.51. For consistency, we used the same two explanations as the only references when computing\nthe BLEU-score for the predicted explanations. Given the low inter-annotator score and the fact that\ngenerated explanations almost match the inter-annotator BLEU-score, we conclude that this measure\nis not reliable for our task, and we further rely on human evaluation. Therefore, we manually\nannotated the ﬁrst 100 datapoints in the test set (we used the same partial scoring as in Section 3).\nSince the explanation is conditioned on the predicted label, for incorrect labels, the model would not\nproduce a correct explanation. Therefore, we provide as correctness score the percentage of correct\nexplanations in the subset of the ﬁrst 100 examples where the predicted label was correct (80 in this\nexperiment). We obtain a percentage of 34.68% correct explanations. While this percentage is low,\nwe keep in mind that the selection criteria was only the accuracy of the label classiﬁer and not the\nperplexity of the explanation. In the next experiment, we show how training (and selecting) only for\ngenerating explanations results in higher quality explanations.\n4.3\nEXPLAINTHENPREDICT: Generate an explanation then predict a label\nIn PREDICTANDEXPLAIN, we conditioned the explanation on the label predicted by the MLP, be-\ncause we wanted to see how the typical architecture used on SNLI can be adapted to justify its\ndecisions in natural language. However, a more natural approach for solving inference is to think of\nthe explanation ﬁrst and based on the explanation to decide a label. Therefore, in this experiment, we\nﬁrst train a network to generate an explanation given a pair of (premise, hypothesis), and, separately,\nwe train a network to provide a label given an explanation. This is a sensible decomposition for our\ndataset, due to the following key observation: In our dataset, in the large majority of the cases, one\ncan easily detect for which label an explanation has been provided. We highlight that this is not the\ncase in general, as the same explanation can be correctly arguing for different labels, depending on\nthe premise and hypothesis. For example, the explanation \"A woman is a person\" would be a correct\nexplanation for the entailment pair (\"A woman is in the park\", \"A person is in the park\") as well\nfor the contradiction pair (\"A woman is in the park\", \"There is no person in the park\"). However,\nthere are multiple ways of formulating an explanation. In our example, for the contradiction pair,\none could also explain that \"There cannot be no person in the park if a woman is in the park\", which\nread alone would allow one to infer that the pair was a contradiction. To support our observation, we\ntrain a neural network that given only an explanation predicts a label. We use the same bidirectional\nencoder and MLP-classiﬁer as above. We obtain an accuracy of 96.83% on the test set of SNLI.\n5\nModels For predicting an explanation given a pair of (premise, hypothesis), we ﬁrst train a simple\nseq2seq model that we call EXPLAINTHENPREDICTSEQ2SEQ. Essentially, we keep the architec-\nture in e-INFERSENT, where we eliminate the classiﬁer by setting α = 0, and we decode the expla-\nnation without prepending the label. Secondly, we train an attention model, which we refer to as\nEXPLAINTHENPREDICTATTENTION. Attention mechanisms in neural networks brought consistent\nimprovements over the non-attention counter-parts in various areas, such as computer vision [27],\nspeech [4], or natural language processing [12, 2]. We use the same encoder and decoder as in EX-\nPLAINTHENPREDICTSEQ2SEQ, and we add two identical but separate attention modules, over the\ntokens in the premise and hypothesis. For details of the attention modules, see Appendix B.\nSelection Our only hyper-parameter is internal sizes for the decoder of 512, 1024, 2048, and 4096.\nOur model selection criterion is the perplexity on the validation set of SNLI. We obtain the best con-\nﬁguration for both EXPLAINTHENPREDICTSEQ2SEQ and EXPLAINTHENPREDICTATTENTION to\nhave an internal size of 1024.\nResults With the described setup, the SNLI test accuracy drops from 83.96% (0.26) in PREDIC-\nTANDEXPLAIN to 81.59% (0.45) in EXPLAINTHENPREDICTSEQ2SEQ and 81.71%(0.36) in EX-\nPLAINTHENPREDICTATTENTION. However, when we again manually annotate the ﬁrst 100 gen-\nerated explanations in the test set, we obtain signiﬁcantly higher percentages of correct explana-\ntions: 49.8% for EXPLAINTHENPREDICTSEQ2SEQ and 64.27% for EXPLAINTHENPREDICTAT-\nTENTION. We note that the attention mechanism indeed signiﬁcantly increases the quality of the\nexplanations. The perplexity and BLEU-score are 8.95(0.03) and 24.14(0.58) for EXPLAINTHEN-\nPREDICTSEQ2SEQ, and 6.1(0) and 27.58(0.47) for EXPLAINTHENPREDICTATTENTION. Our ex-\nperiment shows that, while sacriﬁcing a bit of performance, we get a better trust that when EX-\nPLAINTHENPREDICT predicts a correct label, it does so for the right reasons.\nQualitative analysis of explanations In Table 1, we provide examples of generated explanations\nfrom the test set from (a) PREDICTANDEXPLAIN, (b) EXPLAINTHENPREDICTSEQ2SEQ, and (c)\nEXPLAINTHENPREDICTATTENTION. At the end of each explanation, we give in brackets the score\nthat we manually allocated as explained in section 3. We notice that the explanations are mainly on\ntopic for all the three models, with minor exceptions, such as the mention of \"camouﬂage\" in (1c).\nWe also notice that even when incorrect, they are sometimes frustratingly close to being correct, for\nexample, explanation (2b) is only one word (out of its 20 words) away from being correct. It is also\ninteresting to inspect the explanations provided when the predicted label is incorrect. For example,\nin (1a), we see that the network omitted the information of \"facing the camera\" in the premise and\ntherefore classiﬁed the pair as neutral, which is backed up by an otherwise correct explanation in\nitself. We also see that model EXPLAINTHENPREDICTSEQ2SEQ correctly classiﬁes this pair as\nentailment, however, it only motivates 1 out the 3 reasons why it is so, and it also picks arguably the\neasiest reason. Interestingly, the attention model (1c) points to the correct evidence but argues that\n\"standing\" and \"facing a camera\" is not enough to conclude \"posing for a picture\".\n4.4\nREPRESENT: Universal sentence representations\nObtaining universal sentence representations is the task of learning an encoder to provide semanti-\ncally meaningful ﬁxed-length representations of phrases/sentences. These representations are fur-\nther used as features in other downstream tasks, particularly for tasks where labeled training data\nare scarce. In computer vision, pretrained ImageNet-based encoders [10] provide standard image\nfeature extractors for other downstream tasks. However, in natural language processing, there is\nstill no consensus on general-purpose sentence encoders. It remains an open question on which\ntask and dataset should such an encoder be trained. Traditional approaches make use of very large\nunsupervised datasets [17, 14], taking weeks to train. Conneau et al. [7] showed that training only\non NLI is both more accurate and more time-efﬁcient than training on orders of magnitude larger\nbut unsupervised datasets. Their results constitute a previous state-of-the-art for universal sentence\nrepresentations and encourage the idea that supervision can be more beneﬁcial than larger but un-\nsupervised datasets. We hypothesize that an additional layer of supervision in the form of natural\nlanguage explanations should further improve learning of universal sentence representations.\nModel We use our e-INFERSENT model already trained in PREDICTANDEXPLAIN. While we com-\npare our model with InferSent that has not been trained on explanations, we want to ensure that\neventual improvements are not purely due to the addition of a language model in the decoder net-\nwork. We therefore introduce a second baseline, INFERSENTAUTOENC, where instead of decoding\n6\nTable 1: Examples of predicted labels and generated explanations from (a) PREDICTANDEXPLAIN,\n(b) EXPLAINTHENPREDICTSEQ2SEQ, and (c) EXPLAINTHENPREDICTATTENTION. In brackets\nour scores of correctness.\n(1) Premise: 3 young man in hoods standing in the middle of a quiet street facing the camera.\nHypothesis: Three hood wearing people pose for a picture.\nGold label: entailment\n(a) Predicted label: neutral\nExplanation: Just because the men are in the\nmiddle of a street doesn’t mean\nthey are posing for a picture. [0]\n(b) Predicted label: entailment\nExplanation: three young men\nare people. [0.33]\n(c) Predicted label: neutral\nExplanation: Just because three young man\nin camouﬂage standing in the middle of\na quiet street facing the camera\ndoes not mean they pose for a picture. [0]\n(2) Premise: Three ﬁreﬁghter come out of subway station.\nHypothesis: Three ﬁreﬁghters putting out a ﬁre inside of a subway station.\nGold label: neutral\n(a) Predicted label: contradiction\nExplanation: The ﬁreﬁghters\ncan not be putting out a ﬁre station and\nputting out a ﬁre at the same time. [0]\n(b) Predicted label: neutral\nExplanation: The fact that three\nﬁremen are putting out of a subway\nstation doesn’t imply that they\nare putting out a ﬁre. [0]\n(c) Predicted label: neutral\nExplanation: The ﬁreﬁghters\nmay not be putting out a ﬁre\ninside of the subway station. [1]\n(3) Premise: A blond-haired doctor and her African American assistant looking threw new medical manuals.\nHypothesis: A man is eating pb and j.\nGold label: contradiction\n(a) Predicted label: contradiction\nExplanation: A man is not a woman. [1]\n(b) Predicted label: contradiction\nExplanation: One can not be looking\nand eating simultaneously. [0]\n(c) Predicted label: contradiction\nExplanation: A person can not be looking\nat a medical and a book at the same time. [0]\nTable 2: Transfer results on downstream tasks. For MRPC we report accuracy/F1 score, for STS14\nwe report the Person/Spearman correlations, for SICK-R the Person correlation, and for all the rest\ntheir accuracies. Results are the average of 5 runs with different seeds. The standard deviations\nis shown in brackets, and the best result for every task is indicated in bold. * indicates signiﬁcant\ndifference at level 0.05 with respect to the InferSent baseline.\nModel\nMR\nCR\nSUBJ\nMPQA\nSST2\nTREC\nMRPC\nSICK-E\nSICK-R\nSTS14\nInferSent-SNLI-ours\n78.18\n81.28\n92.46\n88.46\n82.12\n89.32\n74.82 / 82.74\n85.96\n0.887\n0.65 / 0.63\n(0.25)\n(0.15)\n(0.15)\n(0.21)\n(0.22)\n(0.5)\n(0.66 / 0.27)\n(0.32)\n(0.002)\n(0 / 0)\nINFERSENTAUTOENC\n75.94*\n79.26*\n91.72*\n88.16\n80.9*\n90.52*\n76.2* / 82.48\n85.58\n0.88*\n0.5* / 0.5*\n(0.18)\n(0.36)\n(0.28)\n(0.26)\n(0.48)\n(0.52)\n(0.93 / 1.23)\n(0.33)\n(0)\n(0.02 / 0.02)\ne-INFERSENT\n77.76\n81.3\n92.14*\n88.78*\n81.84\n90\n75.56 / 83.24*\n85.92\n0.89*\n0.68 / 0.65*\n(0.44)\n(0.16)\n(0.21)\n(0.22)\n(0.4)\n(0.51)\n(0.62 / 0.24)\n(0.52)\n(0)\n(0.01 / 0.01)\nexplanations, we decode the premise and hypothesis separately from each sentence representation\nusing one shared decoder.\nEvaluation metrics Typically, sentence representations are evaluated by using them as ﬁxed fea-\ntures on top of which shallow classiﬁers are trained for a series of downstream tasks. Conneau et al.\n[7] provide an excellent tool, called SentEval, for evaluating sentence representations on 10 diverse\ntasks: movie reviews (MR), product reviews (CR), subjectivity/objectivity (SUBJ), opinion polar-\nity (MPQA), question-type (TREC), sentiment analysis (SST), semantic textual similarity (STS),\nparaphrase detection (MRPC), entailment (SICK-E), and semantic relatedness (SICK-R). We re-\nfer to their work for a more detailed description of each of these tasks and of SentEval, which we\nuse for comparing the quality of the sentence embeddings obtained by additionally providing our\nexplanations on top of the label supervision.\nResults In Table 2, we report the average results and standard deviations of e-INFERSENT, our\nretrained InferSent model, and the additional INFERSENTAUTOENC baseline on the downstream\ntasks mentioned above. To test if the differences in performance of INFERSENTAUTOENC and e-\nINFERSENT relative to the InferSent baseline are signiﬁcant, we performed Welch’s t-test.5 We\nmark with * the results that appeared signiﬁcant under the signiﬁcance level of 0.05.\n5Using the implementation in scipy.stats.ttest_ind with equal_var=False.\n7\nWe notice that INFERSENTAUTOENC is performing signiﬁcantly worse than InferSent on 6 tasks\nand signiﬁcantly outperforms this baseline on only 2 tasks. This indicates that just adding a language\ngenerator can harm performance. Instead, e-INFERSENT signiﬁcantly outperforms InferSent on 4\ntasks, while it is signiﬁcantly outperformed only on 1 task. Therefore, we conclude that training\nwith explanations helps the model to learn overall better sentence representations.\n4.5\nTRANSFER: Transfer without ﬁne-tuning to out-of-domain NLI\nTransfer without ﬁne-tuning to out-of-domain entailment datasets is known to exhibit poor perfor-\nmance. For example, Bowman et al. [3] obtained an accuracy of only 46.7% when training on SNLI\nand evaluating on SICK-E [20]. We test how our explanations affect the direct transfer in both label\nprediction and explanation generation by looking at SICK-E [20] and MultiNLI [26]. The latter\nincludes a diverse range of genres of written and spoken English, as well as test sets for cross-genre\ntransfer.\nModel We again use our already trained e-INFERSENT model from PREDICTANDEXPLAIN.\nResults In Table 3, we present the performance of e-INFERSENT and our 2 baselines when evaluated\nwithout ﬁne-tuning on SICK-E and MultiNLI. We notice that the accuracy improvements obtained\nwith e-INFERSENT are very small. However, e-INFERSENT additionally provides explanations,\nwhich could bring insight into the inner workings of the model. We manually annotated the ﬁrst 100\nexplanations of the test sets. The percentage of correct explanations in the subset where the label\nwas predicted correctly was 30.64% for SICK-E and only 1.92% for MultiNLI. We also noticed\nthat the explanations in SICK-E, even when wrong, were generally on-topic and valid statements,\nwhile the ones in MultiNLI were generally nonsense or off-topic. Therefore, transfer learning for\ngenerating explanations in out-of-domain NLI would constitute challenging future work.\n5\nRelated work\nTable 3: The average performance over 5 seeds\nof e-INFERSENT and the 2 baselines on SICK-E\nand MultiNLI with no ﬁne-tuning. Standard devi-\nations are in parenthesis.\nModel\nSICK-E\nMultiNLI\nInferSent-SNLI-ours\n53.27 (1.65)\n57 (0.41)\nINFERSENTAUTOENC\n52.9 (1.77)\n55.38 (0.9)\ne-INFERSENT\n53.54 (1.43)\n57.16 (0.51)\nInterpretability One main direction in inter-\npretability for neural networks is providing ex-\ntractive justiﬁcations, i.e., explanations consist-\ning of subsets of the raw input, such as words\nor image patches.\nExtractive techniques can\nbe divided into post-hoc (applied after train-\ning) and architecture-incorporated (guiding the\ntraining). For example, Ribeiro et al. [24] in-\ntroduce a post-hoc extractive technique, LIME,\nthat explains the prediction of any classiﬁer via\na local linear approximation around the predic-\ntion. Alvarez-Melis and Jaakkola [1] introduces\na similar approach but for structured prediction, where a variational autoencoder provides relevant\nperturbations of the inputs that are then used to infer pairs of input-output tokens that are causally\nrelated. While these models provide valuable insight for detecting biases, further model and dataset\nreﬁnements would have to be made on a case-by-case basis. For example, Gururangan et al. [13]\nidentiﬁed a set of biases in SNLI, but noted that their attempts to remove them would give rise to\nother biases.\nAttention-based models, such as [2, 25], offer some degree of interpretability and have been shown\nto also improve performance on downstream tasks. However, soft attention, the most prominent\nattention model, often does not learn to single out human-interpretable inputs.\nNeither extractive nor attention-based techniques can provide full-sentence explanations of a\nmodel’s decisions. Moreover, they cannot capture ﬁne-grained relations and asymmetries, especially\nin a task like recognizing textual entailment. For example, if the words person, woman, mountain,\noutdoors are extracted as justiﬁcation, one may not know whether the model correctly learned that\nA woman is a person and not that A person is a woman, let alone that the model correctly paired\n(woman, person) and (mountain, outside).\nNatural language explanations In our work, we have taken a step further and built a neural network\nthat is able to directly provide full-sentence natural language justiﬁcations. There has been little\n8\nwork on incorporating and outputting natural language free-form explanations, mostly due to the\nlack of appropriate datasets. In this direction, and very similar to our approach, is the recent work\nby Park et al. [23], who introduce two datasets of natural language explanations for the tasks of\nvisual question-answering and activity recognition. Another work in this direction is that of Ling\net al. [18], who introduced a dataset of textual justiﬁcations for solving math problems and formulate\nthe task in terms of program execution. Nonetheless, their setup is speciﬁc to the task of solving\nmath problems, and thus hard to transfer to more general natural understanding tasks. Jansen et al.\n[16] provided a dataset of natural language explanation graphs for elementary science questions.\nHowever, with only 1, 680 pairs of questions and explanations, their corpus is orders of magnitude\nsmaller than e-SNLI.\nBreaking natural language inference Recently, an increasing amount of analysis has been carried\nout on the SNLI dataset and on the inner workings of different models trained on it. For example,\nDasgupta et al. [9] assembled a dataset to test whether inference models actually capture composi-\ntionality beyond word level. They showed that InferSent sentence embeddings [7] indeed do not\nexhibit signiﬁcant compositionality and that downstream models using these sentence representa-\ntions largely rely on simple heuristics that are ecologically valid in the SNLI corpus. For example,\nhigh overlap in words between premise and hypothesis usually predicts entailment, while most con-\ntradictory sentence pairs have no or very little overlap of words. Negation words would also strongly\nindicate a contradiction. Glockner et al. [11] introduce a toy dataset, BreakingNLI, to test whether\nnatural language inference models capture world knowledge and generalize beyond statistical regu-\nlarities. To construct BreakingSNLI, they modiﬁed some of the original SNLI sentences such that\nthey differ by at most one word from the sentences in the training set. Glockner et al. show that mod-\nels achieving high accuracies on SNLI, such as [21, 22, 6], show dramatically reduced performance\non this simpler dataset, while the model of Chen et al. [5] is more robust due to incorporating exter-\nnal knowledge. As the explanations in e-SNLI are mostly self-contained, our dataset provides the\nprecise external knowledge that one requires in order to solve the SNLI inference task. It is therefore\na perfect testbed for developing models that incorporate external knowledge from free-form natural\nlanguage.\n6\nConclusions and future work\nWe introduced e-SNLI, a large dataset of natural language explanations for an inﬂuential task of\nrecognizing textual entailment. To demonstrate the usefulness of e-SNLI, we experimented with\nvarious ways of using these explanations for outputting human-interpretable full-sentence justiﬁca-\ntions of classiﬁcation decisions. We also investigated the usefulness of these explanations as an\nadditional training signal for learning better universal sentence representations and the transfer ca-\npabilities to out-of-domain NLI datasets. In this work, we established a series of baselines using\nstraight-forward recurrent neural network architectures for incorporating and generating natural lan-\nguage explanations. We hope that e-SNLI will be valuable for future research on more advanced\nmodels that would outperform our baselines.\nFinally, we hope that the community will explore the dataset in other directions. For example, we\nalso recorded the highlighted words, which we release with the dataset. Similar to the evaluation\nperformed for visual question answering in Das et al. [8], our highlighted words could provide a\nsource of supervision and evaluation for attention models [25, 22] or post-hoc explanation models\nwhere the explanation consists of a subset of the input.\nReferences\n[1] Alvarez-Melis, D. and Jaakkola, T. S. (2017). A causal framework for explaining the predictions\nof black-box sequence-to-sequence models. CoRR, abs/1707.01943.\n[2] Bahdanau, D., Cho, K., and Bengio, Y. (2014). Neural machine translation by jointly learning\nto align and translate. CoRR, abs/1409.0473.\n[3] Bowman, S. R., Angeli, G., Potts, C., and Manning, C. D. (2015). A large annotated corpus for\nlearning natural language inference. CoRR, abs/1508.05326.\n[4] Chan, W., Jaitly, N., Le, Q. V., and Vinyals, O. (2015).\nListen, attend and spell.\nCoRR,\nabs/1508.01211.\n9\n[5] Chen, Q., Zhu, X., Ling, Z., Inkpen, D., and Wei, S. (2017). Natural language inference with\nexternal knowledge. CoRR, abs/1711.04289.\n[6] Chen, Q., Zhu, X., Ling, Z., Wei, S., and Jiang, H. (2016). Enhancing and combining sequential\nand tree LSTM for natural language inference. CoRR, abs/1609.06038.\n[7] Conneau, A., Kiela, D., Schwenk, H., Barrault, L., and Bordes, A. (2017).\nSupervised\nlearning of universal sentence representations from natural language inference data.\nCoRR,\nabs/1705.02364.\n[8] Das, A., Agrawal, H., Zitnick, C. L., Parikh, D., and Batra, D. (2016). Human attention in\nvisual question answering: Do humans and deep networks look at the same regions?\nCoRR,\nabs/1606.03556.\n[9] Dasgupta, I., Guo, D., Stuhlmüller, A., Gershman, S. J., and Goodman, N. D. (2018). Evaluating\ncompositionality in sentence embeddings.\n[10] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). ImageNet: A large-\nscale hierarchical image database. In Proc. of CVPR.\n[11] Glockner, M., Shwartz, V., and Goldberg, Y. (2018). Breaking NLI systems with sentences\nthat require simple lexical inferences. In Proc. of ACL.\n[12] Gong, Y., Luo, H., and Zhang, J. (2017). Natural language inference over interaction space.\nCoRR, abs/1709.04348.\n[13] Gururangan, S., Swayamdipta, S., Levy, O., Schwartz, R., Bowman, S., and Smith, N. A.\n(2018). Annotation artifacts in natural language inference data. In Proc. of NAACL.\n[14] Hill, F., Cho, K., and Korhonen, A. (2016). Learning distributed representations of sentences\nfrom unlabelled data. CoRR, abs/1602.03483.\n[15] Hochreiter, S. and Schmidhuber, J. (1997).\nLong short-term memory.\nNeural Comput.,\n9(8):1735–1780.\n[16] Jansen, P. A., Wainwright, E., Marmorstein, S., and Morrison, C. T. (2018). Worldtree: A\ncorpus of explanation graphs for elementary science questions supporting multi-hop inference.\nCoRR, abs/1802.03052.\n[17] Kiros, R., Zhu, Y., Salakhutdinov, R., Zemel, R. S., Torralba, A., Urtasun, R., and Fidler, S.\n(2015). Skip-thought vectors. CoRR, abs/1506.06726.\n[18] Ling, W., Yogatama, D., Dyer, C., and Blunsom, P. (2017). Program induction by rationale\ngeneration: Learning to solve and explain algebraic word problems. CoRR, abs/1705.04146.\n[19] Liu, P., Qiu, X., and Huang, X. (2016). Modelling interaction of sentence pair with coupled-\nlstms. CoRR, abs/1605.05573.\n[20] Marelli, M., Menini, S., Baroni, M., Bentivogli, L., bernardi, R., and Zamparelli, R. (2014). A\nsick cure for the evaluation of compositional distributional semantic models.\n[21] Nie, Y. and Bansal, M. (2017). Shortcut-stacked sentence encoders for multi-domain inference.\nIn Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for NLP, pages\n41–45. Association for Computational Linguistics.\n[22] Parikh, A. P., Täckström, O., Das, D., and Uszkoreit, J. (2016). A decomposable attention\nmodel for natural language inference. CoRR, abs/1606.01933.\n[23] Park, D. H., Hendricks, L. A., Akata, Z., Rohrbach, A., Schiele, B., Darrell, T., and Rohrbach,\nM. (2018). Multimodal explanations: Justifying decisions and pointing to the evidence. CoRR,\nabs/1802.08129.\n[24] Ribeiro, M. T., Singh, S., and Guestrin, C. (2016). \"why should I trust you?\": Explaining the\npredictions of any classiﬁer. CoRR, abs/1602.04938.\n10\n[25] Rocktäschel, T., Grefenstette, E., Hermann, K. M., Kociský, T., and Blunsom, P. (2015). Rea-\nsoning about entailment with neural attention. CoRR, abs/1509.06664.\n[26] Williams, A., Nangia, N., and Bowman, S. R. (2017). A broad-coverage challenge corpus for\nsentence understanding through inference. CoRR, abs/1704.05426.\n[27] Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A. C., Salakhutdinov, R., Zemel, R. S., and\nBengio, Y. (2015). Show, attend and tell: Neural image caption generation with visual attention.\nCoRR, abs/1502.03044.\nAppendices\nA\nList of templates to ﬁlter uninformative explanations\nGeneral templates\n\"<premise>\"\n\"<hypothesis>\"\n\"<hypothesis> <premise>\"\n\"<premise> <hypothesis>\"\n\"Sentence 1 states <premise>. Sentence 2 is stating <hypothesis>\"\n\"Sentence 2 states <hypothesis>. Sentence 1 is stating <premise>\"\n\"There is <hypothesis>\"\n\"There is <premise>\"\nEntailment templates\n\"<premise> implies <hypothesis>\"\n\"If <premise> then <hypothesis>\"\n\"<premise> would imply <hypothesis>\"\n\"<hypothesis> is a rephrasing of <premise>\"\n\"<premise> is a rephrasing of <hypothesis>\"\n\"In both sentences <hypothesis>\"\n\"<premise> would be <hypothesis>\"\n\"<premise> can also be said as <hypothesis>\"\n\"<hypothesis> can also be said as <premise>\"\n\"<hypothesis> is a less speciﬁc rephrasing of <premise>\"\n\"This clariﬁes that <hypothesis>\"\n\"If <premise> it means <hypothesis>\"\n\"<hypothesis> in both sentences\"\n\"<hypothesis> in both\"\n\"<hypothesis> is same as <premise>\"\n\"<premise> is same as <hypothesis>\"\n\"<premise> is a synonym of <hypothesis>\"\n\"<hypothesis> is a synonym of <premise>\".\n11\nNeutral templates\n\"Just because <premise> doesn’t mean <hypothesis>\"\n\"Cannot infer the <hypothesis>\"\n\"One cannot assume <hypothesis>\"\n\"One cannot infer that <hypothesis>\"\n\"Cannot assume <hypothesis>\"\n\"<premise> does not mean <hypothesis>\"\n\"We don’t know that <hypothesis>\"\n\"The fact that <premise> doesn’t mean <hypothesis>\"\n\"The fact that <premise> does not imply <hypothesis>\"\n\"The fact that <premise> does not always mean <hypothesis>\"\n\"The fact that <premise> doesn’t always imply<hypothesis>\".\nContradiction templates\n\"In sentence 1 <premise> while in sentence 2 <hypothesis>\"\n\"It can either be <premise> or <hypothesis>\"\n\"It cannot be <hypothesis> if <premise>\"\n\"Either <premise> or <hypothesis>\"\n\"Either <hypothesis> or <premise>\"\n\"<premise> and other <hypothesis>\"\n\"<hypothesis> and other <premise>\"\n\"<hypothesis> after <premise>\"\n\"<premise> is not the same as <hypothesis>\"\n\"<hypothesis> is not the same as <premise>\"\n\"<premise> is contradictory to <hypothesis>\"\n\"<hypothesis> is contradictory to <premise>\"\n\"<premise> contradicts <hypothesis>\"\n\"<hypothesis> contradicts <premise>\"\n\"<premise> cannot also be <hypothesis>\"\n\"<hypothesis> cannot also be <premise>\"\n\"either <premise> or <hypothesis>\"\n\"either <premise> or <hypothesis> not both at the same time\"\n\"<premise> or <hypothesis> not both at the same time\".\nB\nArchitecture of EXPLAINTHENPREDICTATTENTION\nOur attention model EXPLAINTHENPREDICTATTENTION is composed of two identical but separate\nmodules for premise and hypothesis. We ﬁx the number of attended tokens at 84, the maximum\nlength of a sentence in SNLI. We denote by hp\nt and hh\nt the bidirectional embeddings of the premise\nand hypothesis at timestep t. We denote by hdec\nτ\nthe decoder hidden state at timestep τ, which we\nrefer to as the context of the attention.\nWe use 3 couples of linear projections followed by tanh non-linearities as follows:\n12\nWe project each timestep of the encoder for premise and hypothesis:\nproj1p\nt = tanh(W 1\np hp\nt + b1\np)\nproj1h\nt = tanh(W 1\nhhh\nt + b1\nh).\nWe separately project the context vector, that is, the hidden vector of the decoder at each timestep,\nbefore doing its dot product with the tokens of the premise and hypothesis:\nprojc,p\nτ\n= tanh(W c\nphdec\nτ\n+ bc\np)\nprojc,h\nτ\n= tanh(W c\nhhdec\nτ\n+ bc\nh).\nAt each decoding timestep τ, we do the dot product between the projections of the context with all\nthe timesteps of the premise and hypothesis, respectively:\nf\nwt\np,τ =< projc,p\nτ , proj1p\nt >\nf\nwt\nh,τ =< projc,h\nτ\n, proj1h\nt > .\nThe ﬁnal attention weights are computed from a softmax over the non-normalized weights:\nwp,τ\nt\n= Softmax(f\nwt\np,τ)\nwh,τ\nt\n= Softmax(f\nwt\nh,τ).\nWe use another couple of projections for the embeddings of the tokens of premise and hypothesis,\nbefore we apply the weighted sum.\nproj2p\nt = tanh(W 2\np hp\nt + b2\np)\nproj2h\nt = tanh(W 2\nhhh\nt + b2\nh).\nFinally, we compute the weighted sums for premise and hypothesis:\npτ =\nX\nt\nwp,τ\nt\nproj2p\nt\nhτ =\nX\nt\nwh,τ\nt\nproj2h\nt .\nAt each timestept τ, we concatenate pτ and hτ with the word embedding from the previous timestep\nτ −1 and give as input to our decoder.\n13\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2018-12-04",
  "updated": "2018-12-06"
}