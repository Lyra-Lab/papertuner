{
  "id": "http://arxiv.org/abs/1812.01410v1",
  "title": "Compressive Classification (Machine Learning without learning)",
  "authors": [
    "Vincent Schellekens",
    "Laurent Jacques"
  ],
  "abstract": "Compressive learning is a framework where (so far unsupervised) learning\ntasks use not the entire dataset but a compressed summary (sketch) of it. We\npropose a compressive learning classification method, and a novel sketch\nfunction for images.",
  "text": "Compressive Classiﬁcation (Machine Learning without learning)\nVincent Schellekens∗and Laurent Jacques∗\nAbstract—Compressive learning is a framework where (so far\nunsupervised) learning tasks use not the entire dataset but a com-\npressed summary (sketch) of it. We propose a compressive learn-\ning classiﬁcation method, and a novel sketch function for images.\n1\nIntroduction and background\nMachine Learning (ML)—inferring models from datasets of\nnumerous learning examples—recently showed unparalleled\nsuccess on a wide variety of problems. However, modern mas-\nsive datasets necessitate a long training time and large memory\nstorage. The recent Compressive Learning (CL) framework al-\nleviates those drawbacks by computing a compressed summary\nof the dataset—its sketch—prior to any learning [1]. The sketch\nis easily computed in a single parallelizable pass, and its re-\nquired size (to capture enough information for successful learn-\ning) does not grow with the number of examples: CLs time and\nmemory requirements are thus unaffected by the dataset size.\nSo far, CL focused on unsupervised ML tasks, where learn-\ning examples don’t belong to a (known) class [1, 2, 3]. We\nshow that CL easily extends to supervised ML tasks by propos-\ning (Sec. 2) and experimentally validating (Sec. 3) a ﬁrst simple\ncompressive classiﬁcation method using only a sketch of the la-\nbeled dataset (Fig. 1). We also introduce a sketch feature func-\ntion leveraging a random convolutional neural network to bet-\nter capture information in images. While not as accurate as ML\nmethods learning from the full dataset, this compressive clas-\nsiﬁcation scheme still attains remarkable accuracy considering\nits unlearned nature. Our method also enjoys from a nice geo-\nmetric interpretation, i.e., Maximum A Posteriori classiﬁcation\nperformed in the Reproducible Kernel Hilbert Space associated\nwith the sketch.\n(Unsupervised) Compressive Learning: Unsupervised ML\nusually amount to estimate parameters of a distribution P,\nfrom a dataset X := {xi ∼iid P}N\ni=1 ⊂Rn of examples—\nassociated to an empirical distribution ˆPX :=\n1\nN\nP\nxi∈X δxi,\nwith δu the Dirac measure at u. While most unsupervised ML\nalgorithms require (often multiple times) access to the entire\ndataset X, CL algorithms require only access to the sketch: a\nsingle vector zX ∈Cm summarizing X. This dataset sketch\nzX actually serves as a proxy for the true distribution sketch\nA(P), i.e., a linear embedding of the “inﬁnite-dimensional”\nprobability distribution P into Cm, a space of lower dimension:\nA(P) := E\nx∼P f(x) ≃zX := A( ˆPX ) = 1\nN\nP\nxi∈X\nf(xi),\n(1)\nwhere f is a random nonlinear feature map to Cm. This map\ndeﬁnes a positive deﬁnite kernel κ(u, v) := E⟨f(u), f(v)⟩,\nand κ in turn provides a Reproducible Kernel Hilbert Space\n(RKHS) Hκ to embed distributions; A indirectly maps P to its\nMean Map κ(·, P) := Ex∼P κ(·, x) ∈Hκ [4, 5, 6]. Existing\n∗E-mail: {vincent.schellekens, laurent.jacques}@uclouvain.be. ISPGroup,\nELEN/ICTEAM, UCLouvain (UCL), B1348 Louvain-la-Neuve, Belgium. VS\nand LJ are funded by Belgian National Science Foundation (F.R.S.-FNRS).\nRn\nf(·)\n...\n...\nxi\n...\nf\nf\nRn\ny0 =?\nx0\n...\nf(·)\nargmax\nClassiﬁcation phase\nObservation phase\nmemory\nzx0\nh·, zX1i\nh·, zXKi\nzXK\nzX1\naverage\n1\nN1\nX\nxi2X1\nf(xi)\naverage\n1\nNK\nX\nxi2XK\nf(xi)\nk⇤' arg max\nk\npk (x0, Pk)\nzxi\nˆp1\nˆpK\nFigure 1: Observation phase: we record only a summary of the dataset X as\nthe K class sketches zXk: the class average of non-linear maps zxi = f(xi)\nof the examples xi. Classiﬁcation phase: a new sample x′ gets the class label\nk∗that maximizes the correlation between its sketch zx′ and the stored class\nsketches; this can be interpreted as a MAP classiﬁer in a RKHS Hκ.\nmethods [2, 3] use Random Fourier Features [7] as map f:\nfRFF(x) =\n\u0002\nexp(i ωT\nj x)\n\u0003m\nj=1\nwith\nωj ∼iid Λ,\n(2)\nand κ is then shift-invariant and the Fourier transform of the\ndistribution Λ: κ(x, x′) = ϖ(x −x′) := (FΛ)(x −x′) [8].\nCL is promising because the sketch zX retains sufﬁcient infor-\nmation (to compete with traditional ML) whenever its size m\nexceeds some value independent on the number of examples\nN, yielding algorithms that scale well when N increases.\nRandom Convolutional Neural Networks (CNN): Shift-\ninvariant kernels are not that relevant when dealing with im-\nages (they are sensitive to image translations for example).\nRecent studies have shown that the last layer of a randomly\nweighted (convolutional) neural network CNN (combining con-\nvolutions with random weights, nonlinear activations, and\npooling operations) captures surprisingly meaningful image\nfeatures [9, 10, 11, 12].\nWe thus propose the feature map\nfCNN(x) = CNN(x) ∈Rm as sketch map f for images: the\nassociated kernel κ is (for a fully connected network) an arc-\ncosine kernel, that surpasses shift-invariant kernels for solving\nimage classiﬁcation tasks with kernel methods [12].\n2\nCompressive learning classiﬁcation\nObservation phase: Supervised ML infers a mathematical\nmodel from a labeled dataset X := {(xi, yi)}N\ni=1 where each\nsignal xi ∈Rn belongs to a class Ck as designated by its class\nlabel yi ∈[K]. Denoting pk := P(x ∈Ck) = P(y = k), the\nsignals are assumed drawn from an unknown density P:\nxi ∼iid P = PK\nk=1 pk p(x| x ∈Ck) =: P\nk pkPk(x).\n(3)\nAs illustrated in Fig. 1(top), our supervised compressive learn-\ning framework considers that X is not explicitly available but\ncompressed as a collection of K class sketches zXk deﬁned as:\nzXk = A( ˆPXk)\nwhere\nXk := {xi ∈Ck}.\n(4)\narXiv:1812.01410v1  [cs.LG]  4 Dec 2018\nWe can also require approximated a priori class probabilities\nˆpk, e.g., ˆpk = Nk\nN if we count the class occurrences Nk = |Xk|,\nor setting an uniform prior ˆpk = 1\nK otherwise.\nClassiﬁcation phase: Under (3), the optimal classiﬁer (min-\nimal error probability) for a test example x′ is the Maximum\nA Posteriori (MAP) estimator kMAP := arg maxk pkPk(x′),\nwhere Pk is generally hard to estimate. In our CL framework,\nwe classify x′ from zXk and ˆpk only (Fig. 1, bottom): we ac-\nquire its sketch zx′ = f(x′) and maximize the correlation with\nthe class sketch weighted by ˆpk, i.e., we assign to x′ the label\nk∗:= arg maxk ˆpk⟨zx′, zXk⟩\n(CC)\nNote that this Compressive Classiﬁer (CC) does not require pa-\nrameter tuning. Interestingly, under a few approximations, this\nprocedure can be seen as a MAP estimator in the RKHS Hκ.\nIndeed, we ﬁrst note that if m is large, the law of large numbers\n(LLN) provides the kernel approximation (KA)\n⟨f(u), f(v)⟩≃κ(u, v),\n∀u, v ∈Rn.\n(KA)\nAssuming Nk is also large, another use of the LLN gives the\nmean map approximation (MMA): we have both ˆpk ≃pk and\n⟨zu, zXk⟩=\n1\nNk\nP\nxi∈Xk\n⟨f(u), f(xi)⟩\n≃\n(KA)\n1\nNk\nP\nxi∈Xk\nκ(u, xi)\n≃Ex∼Pk κ(u, x) =: κ(u, Pk)\n∀u ∈Rn.\n(MMA)\nConsequently, under the KA and MMA approximations,\nk∗≃arg maxk pk κ(x′, Pk),\n(5)\nor in other words, we replace Pk in the MAP estimator by\nits Mean Map κ(·, Pk)—its embedding in Hκ—such that CC\ncomputes a MAP estimation inside the RKHS Hκ.\nIn all\ngenerality κ(·, Pk) is not a probability density function, but\ncan be interpreted as a smoothing of Pk by convolution with\nϖ(u) := κ(u, 0) if κ is a properly scaled shift-invariant\nkernel.\nAlternatively, (5) can be seen as a Parzen-windows\nclassiﬁer—a nonparametric Support Vector Machine (with-\nout weights learning)—evaluated compressively thanks to the\nsketch [13, 14].\n3\nExperimental proof of concept\nSynthetic datasets: We build two datasets that are not linearly\nseparable (Fig. 2 left), and sketch them using f = fRFF with\nΛ ∼N(0,\nIn\nσ−2 ): therefore κ(u, v) ∝exp(−∥u−v∥2\n2σ2\n). As\nshown Fig. 2(right), the test accuracy of CC improves with m\nuntil reaching—when the KA is good enough—a constant ﬂoor\ndepending on the compatibility between κ and P. Accuracy is\nalmost optimal when κ is close to the constituents of P (e.g.,\n1st dataset, σ = 0.1), but degrades when the kernel scale and/or\nshape mismatches the data clusters (e.g., 1st dataset, σ = 10;\nor 2nd dataset). CC thus reaches good accuracy provided m is\nlarge enough and κ is well adapted to the task.\nStandard datasets: We also test CC on some well-known\n“real-life” datasets from the UCI ML Repository [15]. Table 1\ncompares the error rates of CC and SVM, a fully learned ap-\nproach. Although worse than SVM, CC is surprisingly accurate\nconsidering its compressive nature, low computational cost (es-\npecially when m = 50), and that κ is a basic, non-tuned kernel.\nImage classiﬁcation: More challenging are image classiﬁca-\ntion datasets: handwritten digit recognition (MNIST [16]) and\nvehicle/animal recognition (CIFAR-10 [17]). We use f = fCNN\n(the default architecture provided by [18]) because it yielded\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\nlog10(m)\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTest accuracy\nvar = 10e-3\nvar = 0.01\nvar = 0.1\nvar = 1\nvar = 10\nOptimal (MAP)\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\nlog10(m)\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTest accuracy\nvar = 10e-4\nvar = 10e-3\nvar = 0.1\nvar = 0.1\nvar = 1\nOptimal (MAP)\n-4\n-2\n0\n2\n4\n-4\n-2\n0\n2\n4\n-2\n-1\n0\n1\n2\n-2\n-1\n0\n1\n2\nFigure 2: Left: synthetic 2-d datasets of N = 104 examples from K = 3\nequiprobable classes, separated into 2/3 for “training” (observation phase) and\n1/3 for testing (classiﬁcation phase). Right: testing accuracy (average over 10\ntrials) of our compressive classiﬁcation method for different values of σ (noted\nvar) and increasing m (solid), compared to MAP classiﬁcation (dashed).\nN\nn\nK\nSVM\nm = 50\nm = 1000\n2.00\n6.51 ± 1.81\n5.51 ± 1.23\nIris\n150\n4\n3\n4.00\n8.22 ± 3.25\n6.18 ± 2.40\n0.84\n4.56 ± 2.34\n2.43 ± 0.72\nWine\n178\n13\n3\n1.69\n13.75 ± 4.09\n8.19 ± 1.29\n3.67\n7.00 ± 1.40\n3.93 ± 0.39\nBreast cancer\n569\n30\n2\n2.13\n9.22 ± 2.33\n6.23 ± 0.69\n21.03\n23.88 ± 4.37\n23.11 ± 1.05\nAdult (3 attr.)\n30718\n3\n2\n21.06\n36.09 ± 6.67\n35.04 ± 1.63\nTable 1: Standard datasets: train set (white, 2/3 of data) and test set (gray)\naverage error rates ± standard deviation (in %, 100 repetitions), for SVM and\nCC with m ∈{50, 1000}, and with σ = 2 (data re-scaled inside [−1, +1]n).\nbetter accuracy than fRFF, and compare CC to the same CNN\narchitecture with a classiﬁcation layer, with all weights learned\nin one pass over X for fairness. Again CC is outperformed by\nthe learned approach, but still achieves reasonable, non-trivial\naccuracy. Surprisingly, CC performs here better on the test set\nthan on the training set.\nN\nn\nCNN\nm = 250\nm = 5000\n60000\n1.60 ± 0.12\n17.73 ± 1.43\n16.60 ± 1.54\nMNIST\n10000\n28 × 28 × 1\n1.63 ± 0.11\n16.83 ± 1.39\n15.80 ± 1.61\n50000\n39.08 ± 1.48\n71.76 ± 1.85\n72.83 ± 2.00\nCIFAR10\n10000\n32 × 32 × 3\n40.28 ± 1.36\n71.12 ± 1.72\n72.02 ± 1.85\nTable 2: Image datasets: train (white) and test (gray) average error rates ± stan-\ndard deviation (in %, 10 repetitions), for SVM and CC with m ∈{250, 5000}.\n4\nDiscussion and conclusion\nWe proposed a very simple and ﬂexible compressive classiﬁca-\ntion method, relying only on class sketches: accumulated ran-\ndom nonlinear signatures f(·) of the learning examples. This\nclassiﬁer is cheap to evaluate (e.g., in low-power hardware, fol-\nlowing ideas from [19]), involves no parameter tuning, and\nhas an interesting interpretation: a MAP estimator inside the\nRKHS Hκ associated with the kernel κ deﬁned by f. Prelimi-\nnary experimental results, relying on a basic Gaussian κ, are an\nencouraging proof of concept, but indicate room for improve-\nment if the mapping f (and associated kernel κ) are optimized\naccording to the true data distribution; for example, image clas-\nsiﬁcation accuracy improves when f is a random CNN (deﬁn-\ning a shift-variant κ). Intuitively, κ should be such that the\nMean Maps κ(·, Pk) ∈Hκ of different classes k are “well\nseparated” (ideally as much separated as the initial, unknown\ndensities Pk). This could be done by adding some a priori as-\nsumptions on the densities Pk, or by ﬁrst getting a rough esti-\nmation of them through a form of distilled sensing [20]. To be\nreliable, compressive classiﬁcation also requires precise, non-\nasymptotic guarantees, e.g., using results from [5] and [7].\nReferences\n[1] R. Gribonval, G. Blanchard, N. Keriven, and Y. Traonmilin,\n“Compressive Statistical Learning with Random Feature Mo-\nments,” ArXiv e-prints, Jun. 2017.\n[2] N. Keriven, N. Tremblay, Y. Traonmilin, and R. Gribonval,\n“Compressive K-means,” ICASSP 2017 - IEEE International\nConference on Acoustics, Speech and Signal Processing, 2017.\n[3] N. Keriven, A. Bourrier, R. Gribonval, and P. Pérez, “Sketching\nfor Large-Scale Learning of Mixture Models,” Information and\nInference: A Journal of the IMA, 2017.\n[4] N. Aronszajn, “Theory of reproducing kernels,” Transactions\nof the Amererican Mathematical Sociecty, no. 68, pp. 337–404,\n1950.\n[5] A. Smola, A. Gretton, L. Song, B. Scholkopf, “A Hilbert space\nembedding for distributions”, International Conference on Al-\ngorithmic Learning Theory, Springer, Berlin, Heidelberg, 2007.\n[6] B. K. Sriperumbudur, A. Gretton, K. Fukumizu, B. Schölkopf,\nand G. R. Lanckriet, “Hilbert Space Embeddings and Metrics on\nProbability Measures,” Journal of Machine Learning Research,\nvol. 11, pp. 1517–1561, Aug. 2010.\n[7] A. Rahimi and B. Recht, “Random Features for Large-Scale Ker-\nnel Machines,” in Advances in Neural Information Processing\nSystems 20, J. C. Platt, D. Koller, Y. Singer, and S. T. Roweis,\nEds.\nCurran Associates, Inc., 2008, pp. 1177–1184.\n[8] W. Rudin, Fourier Analysis on Groups. Interscience Publishers,\n1962.\n[9] D. Ulyanov, A. Vedaldi, V. Lempitsky, \"Deep Image Prior,\"\narXiv preprint, 2017.\n[10] R. Giryes, G. Sapiro, A.M. Bronstein \"Deep Neural Networks\nwith Random Gaussian Weights:\nA Universal Classiﬁcation\nStrategy?,\" IEEE Transactions on Signal Processing, vol. 64, no.\n13, pp. 3444-3457, Jul. 2016.\n[11] A. Rosenfeld, J.K. Tsotsos, \"Intriguing Properties of Randomly\nWeighted Networks: Generalizing While Learning Next to Noth-\ning,\" arXiv preprint, 2018.\n[12] Y. Cho, L.K. Saul, \"Kernel methods for deep learning,\" Advances\nin neural information processing systems, 2009.\n[13] R. O. Duda, P. E. Hart “Pattern Classiﬁcation and Scene Analy-\nsis”, Wiley Interscience, 1973.\n[14] B. Scholkopf, A. J. Smola, “Learning with kernels: support vec-\ntor machines, regularization, optimization, and beyond”, MIT\npress, 2001.\n[15] A. Asuncion, D.J. Newman, “UC Irvine Machine Learn-\ning Repository,” http://archive.ics.uci.edu/ml/\nindex.php, Accessed: 2018-05-15.\n[16] Y. LeCun, C. Cortes, and C. J. Burges, “The MNIST database\nof handwritten digits,” http://yann.lecun.com/exdb/\nmnist/, Accessed: 2018-05-15.\n[17] A. Krizhevsky, “The CIFAR-10 dataset,” https://www.cs.\ntoronto.edu/~kriz/cifar.html, Accessed: 2018-05-\n15.\n[18] The MatConvNet Team, “MatConvNet:\nCNNs for MAT-\nLAB,” http://www.vlfeat.org/matconvnet/,\nAc-\ncessed: 2018-06-13.\n[19] V. Schellekens, and L. Jacques, \"Quantized Compressive K-\nMeans,\" arXiv preprint, arXiv:1804.10109, 2018.\n[20] J. Haupt, R.M. Castro, and R. Nowak, \"Distilled sensing: Adap-\ntive sampling for sparse detection and estimation,\" IEEE Trans-\nactions on Information Theory, vol. 57, no. 9, 2011, pp. 6222-\n6235.\n",
  "categories": [
    "cs.LG",
    "cs.CV",
    "stat.ML"
  ],
  "published": "2018-12-04",
  "updated": "2018-12-04"
}