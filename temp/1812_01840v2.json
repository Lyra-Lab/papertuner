{
  "id": "http://arxiv.org/abs/1812.01840v2",
  "title": "Attention Boosted Sequential Inference Model",
  "authors": [
    "Guanyu Li",
    "Pengfei Zhang",
    "Caiyan Jia"
  ],
  "abstract": "Attention mechanism has been proven effective on natural language processing.\nThis paper proposes an attention boosted natural language inference model named\naESIM by adding word attention and adaptive direction-oriented attention\nmechanisms to the traditional Bi-LSTM layer of natural language inference\nmodels, e.g. ESIM. This makes the inference model aESIM has the ability to\neffectively learn the representation of words and model the local subsentential\ninference between pairs of premise and hypothesis. The empirical studies on the\nSNLI, MultiNLI and Quora benchmarks manifest that aESIM is superior to the\noriginal ESIM model.",
  "text": "Atention Boosted Sequential Inference Model\nGuanyu Li\nSchool of Computer and\nInformation Technology & Beijing\nKey Lab of Traﬃc Data Analysis\nand Mining, Beijing Jiaotong\nUniversity\nBeijing, China 100044\n17120379@bjtu.edu.cn\nPengfei Zhang\nSchool of Computer and\nInformation Technology & Beijing\nKey Lab of Traﬃc Data Analysis\nand Mining, Beijing Jiaotong\nUniversity\nBeijing, China 100044\n18120448@bjtu.edu.cn\nCaiyan Jia*\nSchool of Computer and\nInformation Technology & Beijing\nKey Lab of Traﬃc Data Analysis\nand Mining, Beijing Jiaotong\nUniversity\nBeijing, China 100044\ncyjia@bjtu.edu.cn\nABSTRACT\nAtention mechanism has been proven eﬀective on natu-\nral language processing. Tis paper proposes an atention\nboosted natural language inference model named aESIM by\nadding word atention and adaptive direction-oriented aten-\ntion mechanisms to the traditional Bi-LSTM layer of natural\nlanguage inference models, e.g. ESIM. Tis makes the in-\nference model aESIM has the ability to eﬀectively learn the\nrepresentation of words and model the local subsentential\ninference between pairs of premise and hypothesis. Te em-\npirical studies on the SNLI, MultiNLI and Qora benchmarks\nmanifest that aESIM is superior to the original ESIM model.\nKEYWORDS\nnatural language processing, deep learning, natural language\ninference, Bi-LSTM\n1\nINTRODUCTION\nNatural language inference (NLI) is an important and signiﬁ-\ncant task in natural language processing (NLP). It concerns\nwhether a hypothesis can be inferred from a premise, requir-\ning understanding of the semantic similarity between the\nhypothesis and the premise to discriminate their relation [1].\nTable 1 shows several samples of natural language inference\nfrom SNLI (Stanford Natural Language Inference) corpus [2].\nIn the literature, the task of NLI is usually viewed as a re-\nlation classiﬁcation. It learns the relation between a premise\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for proﬁt or commercial advantage and that\ncopies bear this notice and the full citation on the ﬁrst page. Copyrights\nfor components of this work owned by others than the author(s) must\nbe honored. Abstracting with credit is permited. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior speciﬁc\npermission and/or a fee. Request permissions from permissions@acm.org.\nDAPA ’19, Melbourne, Australia\n© 2019 Copyright held by the owner/author(s). Publication rights licensed\nto ACM. .\nDOI:\nand a hypothesis in a large training set, then predicts the rela-\ntion between a new pair of premise and hypothesis. Te exist-\ning methods of NLI can be roughly partitioned into two cat-\negories: feature-based models [2] and neural network-based\nmodels [3, 4]. Feature-based models represent a premise\nand a hypothesis by their unlexicalized and lexicalized fea-\ntures, such as n-gram length and the real-valued feature of\nlength diﬀerence, then train a classiﬁer to perform relation\nclassiﬁcation. Recently, end-to-end neural network-based\nmodels have drawn worldwide atention since they have\ndemonstrated excellent performance on quite a few NLP\ntasks including machine translation, natural language infer-\nence, etc.\npremise\nhypothesis\nrelationship\nWet brown dog\nswims towards\nA dog is playing fetch\nin a pond.\nneutral\ncamera.\nA dog is in the water.\nentailment\nTe dog is sleeping in\nhis bed.\ncontradiction\nTable 1: Samples from the SNLI corpus\nOn the basis of their model structures, we can divide neural\nnetwork-based models for NLI into two classes [1], sentence\nencoding models and sentence interaction-aggregation mod-\nels. Te architectures of the two types of models are shown\nin Figure 1.\nSentence encoding models [5–8] (their main architecture\nis shown in Figure 1.a) independently encode a pair of sen-\ntences, a premise and a hypothesis using pre-trained word\nembedding vectors, then learn semantic relation between\ntwo sentences with a multi-layer perceptron (MLP). In these\nmodels, LSTM (Long Short-Term Memory networks) [9], its\nvariants GRU (Gated Recurrent Units) [10] and Bi-LSTM,\nare usually utilized to encode the sentences since they were\ncapable of learning long-term dependencies inside sentences.\nFor example, Conneau et al. proposed a generic NLI training\narXiv:1812.01840v2  [cs.CL]  6 Dec 2018\nDAPA ’19, February 15th, 2019, Melbourne, Australia\nGuanyu Li et al.\n(a) sentence encoding model\n(b) sentence interaction-aggregation model\nFigure 1: Two types of neural network-based models\nscheme and compared several sentence encoding architec-\ntures: LSTM or GRU, Bi-LSTM with mean/max pooling, self-\natention network and hierarchical convolutional networks\n[5]. Te experimental results demonstrated that the Bi-LSTM\nwith max pooling achieved the best performance. Talman et\nal. designed a hierarchical Bi-LSTM max pooling (HBMP)\nmodel to encode sentences [6]. Tis model applied parame-\nters of one Bi-LSTM to initialize the next Bi-LSTM to convey\ninformation, which shown beter results than the model\nwith a single Bi-LSTM. Besides LSTM, atention mechanisms\ncould also be used to boost the eﬀectiveness of sentence\nencoding. Te model developed by Ghaeini et al. added self-\natention to LSTM model, and achieved beter performance\n[11].\nSentence interaction-aggregation models [1, 12–14] (their\nmain architecture is shown in Figure 1.b) learn vector repre-\nsentations of pairs of sentences in the way similar to sentence\nencoding models and calculate pairwise word interaction\nmatrix between two sentences using the newly updated word\nvectors, and then the matching results are aggregated into a\nvector to make the ﬁnal decision. Compared with sentence\nencoding model, sentence interaction-aggregation models\naggregate word similarities between a pair of sentences, are\ncapable of capturing the relevant information between two\nsentences, a premise and a hypothesis. Bahdanau et al. trans-\nlated and aligned text simultaneously in machine translation\ntask [15], innovatively introducing atention mechanism to\nnatural language process (NLP). He et al. designed a pairwise\nword interaction model (PWIM) [16], which made full use\nof word-level ﬁne-grained information. Wang et al. put for-\nward a bilateral multi-perspective matching (BiMPM) model\n[13], focusing on various matching strategies that could be\nseen as diﬀerent types of atention. Te empirical studies\nof Lan et al. [1] and Chen et al. [4] concluded that sentence\ninteration-aggregation models, especially ESIM (Enhanced\nSequential Inference Model), a carefully designed sequential\ninference model based on chain LSTMs, outperformed all\nprevious sentence encoding models.\nAlthough ESIM has achieved excellent achievements, this\nmodel doesn’t consider the atention along the words in a\nsentence in its Bi-LSTM layer. Word atention can charac-\nterize the diﬀerent contribution of each word. Terefore, it\nwill be beneﬁcial to put word atention into the Bi-LTSM\nlayer. Moreover, the orientation of the words represents the\ndirection of the information ﬂow, either forward or back-\nward, should not be ignored. In traditional Bi-LSTM model,\nthe forward and the backward vectors learnt by Bi-LSTM\nare simply jointed. It’s necessary to consider whether each\norientation (forward or backward) has diﬀerent importance\non word encoding, thus adaptively joint the two orientation\nvectors together with diﬀerent weights. Terefore, in this\nstudy, using ESIM model as the baseline, we add an aten-\ntion layer behind each Bi-LSTM layer, then use an adaptive\norientation embedding layer to jointly represent the forward\nand backward vectors. We name this atention boosted Bi-\nLSTM as Bi-aLSTM, and denote the modiﬁed ESIM as aESIM.\nExperimental results on SNLI, MultiNLI [17] and Qora [13]\nbenchmarks have demonstrated beter performance of aESIM\nmodel than that of the baseline ESIM and the other state-of-\nthe-art models. We believe that the architecture of Bi-aLSTM\nhas potentially to be used in other NLP tasks such as text\nclassiﬁcation, machine translation and so on.\nTis paper is organized as follows. We introduce the gen-\neral frameworks of ESIM and aESIM in Section 2. We describe\nthe datasets and the experiment setings, and analyze our\nexperimental results in Section 3. We then draw conclusions\nin Section 4.\n2\nATTENTION BOOSTED SEQUENTIAL\nINFERENCE MODEL\nSupposed that we have two sentences p = (p1, · · · ,plp) and\nq = (q1, · · · ,qlq), where p represents premise and q repre-\nsents hypothesis. Te goal is to predict the label y meaning\nfor their relation.\nAtention Boosted Sequential Inference Model\nDAPA ’19, February 15th, 2019, Melbourne, Australia\n2.1\nESIM model\nEnhanced Sequential Inference Model (ESIM) [9] is com-\nposed of four main components: input encoding layer, local\ninference modeling layer, inference composition layer and\nclassiﬁcation layer.\nIn the input encoding layer, ESIM ﬁrst uses Bi-LSTM layer\nto encode input sentence pairs (Equations 1-2), which can\nbe initialized using pre-trained word embeddings (e.g. Glove\n840B vectors [18]), where (p,i) is the word embedding vector\nof the i-th word in p, (q,i) is that of word in q.\npi = Bi-LSTM(p,i), ∀i ∈[1, · · · ,lp]\n(1)\nqj = Bi-LSTM(q, j), ∀j ∈[1, · · · ,lq]\n(2)\nSecondly, ESIM implements the local inference layer for\nenhancing the sentence information. First it calculates a\nsimilarity matrix M based on p and q.\nM = pTq\n(3)\nIt then gets the new expression for p and q with the equation\nbelow:\nepi =\nlq\nÕ\nj=1\nexp(Mij)\nlqÍ\nk=1\nexp(Mik)\nqj, ∀i ∈[1, · · · ,lp]\n(4)\neqj =\nlp\nÕ\ni=1\nexp(Mij)\nÍlp\nk=1 exp(Mkj)\npi, ∀j ∈[1, · · · ,lq]\n(5)\nwhere ep and eq represent the weighted summation of p and q.\nIt further enhances the local inference information collected\nas below.\nmp = [p; ep;p −ep;p ⊙ep]\n(6)\nmq = [q;eq;q −eq;q ⊙eq]\n(7)\nAfer the enhancement of local inference, another Bi-\nLSTM layer is used to capture local inference information\nand their context for inference composition.\nInstead of summation adopted by Parikh et al. [12], ESIM\nproposes to compute both max and average pooling and feeds\nthe concatenate ﬁxed length vector to the ﬁnal classiﬁer: a\nfully connected multi-layer perceptron.\nFigure 2 shows a high-level view of the ESIM architecture,\nwhere the botom LSTM1 layer of Figure 2 is the input en-\ncoding layer, the middle part with LSTM2 layer is the local\ninference layer, the upper part is the inference composition\nlayer.\n2.2\naESIM model\nTe overall architecture of our newly proposed atention\nboosted sequential inference model (named aESIM) based\non ESIM is similar to ESIM. In detail, aESIM also consists of\nfour main parts: encoding layer, local inference modeling\nFigure 2: ESIM and aESIM model architectures\nlayer, decoding layer and classiﬁcation layer. Te only dif-\nference between ESIM and aESIM is that we substitute the\ntwo Bi-LSTM layers (LSTM1 and LSTM2) in ESIM with two\nBi-aLSTM layers in aESIM. Terefore, as illustrated in Figure\n2, the layers with red-doted circles in ESIM will be replaced\nby the Bi-aLSTM layers shown in the right upper corner of\nthe Figure 2 and the details of Bi-aLSTM can be found in\nFigure 3.\nGiven the word vector xil,l ∈[1,T] of thel-th word in sen-\ntence i, which can be obtained by pre-trained word embed-\ndings such as Glove 840B vectors [18] in the ﬁrst Bi-aLSTM\nlayer or obtained from the local inference modeling layer\nin the second Bi-aLSTM layer. We utilize a forward LSTM\nlayer and a backward LSTM layer to collect both direction\ninformation −→f and ←−f .\n−→f il = −−−−→\nLSTM(xil),l ∈[0,T]\n(8)\n←−f il = ←−−−−\nLSTM(xil),l ∈[0,T]\n(9)\nAs described in introduction section, in the following newly\nproposed Bi-aLSTM, we add word atention and additive\noperation on both orientations of traditional Bi-LSTM layer.\nWord attention layer\nIt’s obvious that not all words contribute equally to the\nrepresentation of a sentence. Atention mechanism, which is\nintroduced in [3], is extremely eﬀective to extract vital words\nfrom the whole sentence, and is particularly beneﬁcial to\ngenerate the sentence vector. Terefore, we use the following\natention mechanism afer we get −→f and ←−f .\nSuppose fil ∈{−→f il, ←−f il }, we then have\nuil = tanh(W fil + b)\n(10)\nαil =\nexp(uT\niluw)\nÍ\nl\nexp(uT\niluw)\n(11)\nDAPA ’19, February 15th, 2019, Melbourne, Australia\nGuanyu Li et al.\nFigure 3: Te structure of Bi-aLSTM including input\nlayer, word attention layer and adaptive word direc-\ntion layer.\nsil = αil ∗fil\n(12)\nwhere uil is obtained afer one-layer MLP for the input fil,\nαil is the importance of word l, is calculated by the SofMax\nunit on the context vector uw of the sentence i which is\nrandomly initialized and modiﬁed during the training, sil\nis the atention enhanced vector through multiplying the\nweight αil and original vector fil, where sil ∈{−→\nsil, ←−\nsil } cor-\nrespond to the forward vector −→f il and the backward vector\n←−f il, respectively.\nAdaptive word direction layer\nIn traditional Bi-LSTM model, the forward and the back-\nward vectors of a word are considered to have equal im-\nportance on the word representation. Te model simply\nconnects the forward and backward vectors head and tail\nwithout weighing their importance. For a word in diﬀer-\nent direction or orientation, the former and the later words\nare reversed. Tus, diﬀerent direction vectors of a word\nmake diﬀerent contribution to the representation, especially\nthe words in a long sentence. Terefore, we propose a new\nadaptive direction layer to learn the contribution of diﬀerent\ndirections for a single word.\nFormally, given two direction word vectors −→\nsil and ←−\nsil, the\nwhole word vector can be expressed as:\nsil = д[(WF ∗−→\nsil + bF ) \u0005 (WB ∗←−\nsil + bB)]\n(13)\nwhere,W∗andb∗denote weight matrix and the bias,д denotes\nthe nonlinear function, [\u0005] denotes the concentration. All the\nparameters can be learned during training. Ten we can get\nthe whole sentence vector as below:\npi = Bi-aLSTM(sil), ∀i ∈[1, · · · ,lp]\n(14)\nqj = Bi-aLSTM(sjl), ∀j ∈[1, · · · ,lq]\n(15)\nTis word and orientation enhanced Bi-LSTM is called Bi-\naLSTM. Its whole architecture is shown in the Figure 3, is\napplied in ESIM model to replace the two Bi-LSTM layers\nfor the task of natural language inference. Besides, this Bi-\naLSTM can be used to other natural language processing\ntasks and our preliminary experiments have demonstrated\nthat Bi-aLSTM is capable of improving the performance of\nBi-LSTM models on sentimental classiﬁcation task (for space\nlimitation, this results will not be shown in the paper).\n3\nEXPERIMENT SETUP\n3.1\nDatasets\nWe evaluated our model on three datasets: the Stanford\nNatural Language Inference (SNLI) corpus, the Multi-Genre\nNatural Language Inference (MultiNLI) corpus, and Qora\nduplicate question dataset. We selected these three relatively\nlarge corpora out of eight corpora in [1] since deep learning\nmodels usually show beter generalization ability on large\ntraining sets and produce more convincing results than on\nsmall training sets.\nSNLI Te Stanford Natural Language Inference (SNLI) cor-\npus contains 570,152 sentence pairs, including 549K training\npairs, 10K validation pairs and 10K testing pairs. Each pair\nhas one of relation classes (entailment, neutral, contradiction\nand ‘-’). Te ‘-’ class indicates there is no conclusion between\nthe two sentences. Consequently, we remove all pairs with\nrelation ’-’ during training, validating and testing processes.\nMultiNLI Tis corpus is a crowd-sourced collection of\n433K sentence pairs annotated with textual entailment in-\nformation. Te corpus is modeled on the SNLI corpus, but\ndiﬀers in that covers a range of genres of spoken and writ-\nten text, and supports a distinctive cross-genre generation\nevaluation.\nQora Te Qora dataset contains 400,000 question pairs.\nTe task of this corpus is to judge whether the two sentences\nmeans the same aﬀair.\n3.2\nSetting\nWe use the validation set to select models for testing. Te\nhyper-parameters of aESIM model are listed as follows. We\nuse the Adam method [19] for optimization. Te ﬁrst mo-\nmentum is set to be 0.9 and the second 0.999. Te initial\nlearning rate is set to 0.0005, and the batch size is 128. Te\ndimensions of all hidden states of Bi-aLSTM and word em-\nbedding are 300. We employ non-linearity function f = selu\n[20] replacing rectiﬁed linear unit ReLU on account of its\nfaster convergence rate. Dropout rate is set to 0.2 during\ntraining. We use pre-trained 300-D Glove 840B vectors [18]\nto initialize word embeddings. Out-of-vocabulary (OOV)\nwords are initialized randomly with Gaussian samples. All\nvectors are updated during training.\nAtention Boosted Sequential Inference Model\nDAPA ’19, February 15th, 2019, Melbourne, Australia\n(a) contradiction pair\n(b) entailment pair\n(c) neutral pair\nFigure 4: Attention visualization\n3.3\nExperiment results\nExcept for comparing our method aESIM with ESIM, we\nlisted the experimental results of methods with their refer-\nences in Table 2 on SNIL. In Table 2, the method in the ﬁrst\nblock is a traditional feature engineering method, those in\nthe second are the sentence vector-based models, those in the\nthird are atention-based models, and ESIM and our aESIM\nare shown in the fourth block. Where the results of ESIM and\naESIM are implemented by ourselves on Keras, the results\nof the others are taken from their original publications. We\nthen compare the baseline models, CBOW, Bi-LSTM with\nESIM and our aESIM on MultiNLI corpus shown In Table 3,\nwhere the results of the baselines are taken from [17]. Fi-\nnally,we compare several types of CNN and RNN models on\nQroa corpus shown in Table 4, the results of theses CNN\nand RNN models are taken from [13]. Te accuracy (ACC) of\neach method is measured by the commonly used precision\nscore 1, and the methods with the best accuracy are marked\nin bold.\nAccording to the results in Tables 2-4, aESIM model achieved\n88.1% on SNLI corpus, elevating 0.8 percent higher than ESIM\nmodel. It promoted almost 0.5 percent accuracy and outper-\nformed the baselines on MultiNLI. It also achieved 88.01%\non Qora. Terefore, we concluded that aESIM with further\nword atention and word orientation operation was superior\nto ESIM model.\n3.4\nAttention visualization\nWe selected three types of sentence pairs from a premise\nand its three hypothesis sentences in the test set of SNLI\ncorpus as shown in Figure 4, where the premise sentence is\n‘A woman with a green headscarf, blue shirt and a very big\ngrin’, and three hypothesis sentences are ‘the woman has\n1htps://nlp.stanford.edu/projects/snli/\nModels\nAcc\nUnlexicalized + Unigram and bigram features [2]\n78.2\n300D LSTM encoders [2]\n80.6\n300D NTI-SLSTM-LSTM encoders [21]\n83.4\n4096D Bi-LSTM with max-pooling [5]\n84.5\n300D Gumbel TreeLSTM encoders [22]\n85.6\n512D Dynamic Meta-Embeddings [23]\n86.7\n100D DF-LSTM17 [24]\n84.6\n300D LSTMN with deep atention fusion [9]\n85.7\nBiMPM [13]\n87.5\nESIM\n87.3\naESIM\n88.1\nTable 2: Te accuracy (%) of the methods on SNLI\nModels\nAccuracy (%)\nMatched\nMismatched\nCBOW\n64.8\n64.5\nBi-LSTM\n66.9\n66.9\nESIM\n73.4\n73.5\naESIM\n73.9\n73.9\nTable 3: Te accuracy (%) of the methods on MultiNLI\nbeen shot’, ‘the woman is very happy’ and ‘the woman is\nyoung’ with relation labels ‘contradiction’, ‘entailment’, and\n‘neutral’, respectively. Each pair of sentences has their key\nword pairs: grin-shot, grin-happy and grin-young, which\ndetermines whether the premise can entail the hypothesis.\nFigures 4.a-4.c are the visualization of the atention layer be-\ntween sentence pairs afer the Bi-LSTM layer in ESIM model\nand that afer Bi-aLSTM layer in aESIM model for contrast-\ning ESIM and aESIM. By doing so, we could understand how\nthe models judge the relation between two sentences.\nDAPA ’19, February 15th, 2019, Melbourne, Australia\nGuanyu Li et al.\nModels\nAccuracy (%)\nSiamese-CNN\n79.60\nMulti-perspective-CNN\n81.38\nSiamese-LSTM\n82.58\nMulti-Perspective-LSTM\n83.21\nL.D.C\n85.55\nESIM\n86.98\naESIM\n88.01\nTable 4: Te accuracy (%) of the methods on Qora\nIn each Figure, the brighter the color, the higher the weight\nis. We could conclude that our aESIM model had the higher\nweight than ESIM model on each key word pair, especially in\nFigure 4.b, where the similarity of ‘happy’ and ‘grin’ in aESIM\nmodel is much higher than that in ESIM model. Terefore,\nour aESIM model was able to capture the most important\nword pair in each pair of sentences.\n4\nCONCLUSION\nIn this study, we propose an improved version of ESIM named\naESIM for NLI. It modiﬁes the Bi-LSTM layer to collect more\ninformation. We evaluate our aESIM model on three NLI cor-\npora. Experimental results show that aESIM model achieves\nbeter performance than ESIM model. In the future, we will\nevaluate how atention mechanisms can be applied on other\ntasks and explore a way to use less time and space with\nguaranteed accuracy.\nACKNOWLEDGEMENT\nTis work is supported in part by the National Nature Science\nFoundation of China (No. 61876016 and No. 61632004), the\nFundamental Research Funds for the Central Universities\n(No. 2018JBZ006).\nREFERENCES\n[1] W. Lan and W. Xu, “Neural network models for paraphrase identiﬁ-\ncation, semantic textual similarity, natural language inference, and\nquestion answering,” in Proceedings of COLING 2018, 2018.\n[2] S. R. Bowman, G. Angeli, C. Pots, and C. D. Manning, “A large anno-\ntated corpus for learning natural language inference,” arXiv preprint\narXiv:1508.05326, 2015.\n[3] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy, “Hierarchical\natention networks for document classiﬁcation,” in Proceedings of the\n2016 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, pp. 1480–\n1489, 2016.\n[4] Q. Chen, X. Zhu, Z. Ling, S. Wei, H. Jiang, and D. Inkpen, “Enhanced\nlstm for natural language inference,” arXiv preprint arXiv:1609.06038,\n2016.\n[5] A. Conneau, D. Kiela, H. Schwenk, L. Barrault, and A. Bordes, “Su-\npervised learning of universal sentence representations from natural\nlanguage inference data,” arXiv preprint arXiv:1705.02364, 2017.\n[6] A. Talman, A. Yli-Jyr¨a, and J. Tiedemann, “Natural language inference\nwith hierarchical bilstm max pooling architecture,” arXiv preprint\narXiv:1808.08762, 2018.\n[7] J. Im and S. Cho, “Distance-based self-atention network for natural\nlanguage inference,” arXiv preprint arXiv:1712.02047, 2017.\n[8] T. Shen, T. Zhou, G. Long, J. Jiang, S. Wang, and C. Zhang, “Reinforced\nself-atention network: a hybrid of hard and sof atention for sequence\nmodeling,” arXiv preprint arXiv:1801.10296, 2018.\n[9] J. Cheng, L. Dong, and M. Lapata, “Long short-term memory-networks\nfor machine reading,” arXiv preprint arXiv:1601.06733, 2016.\n[10] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation\nof gated recurrent neural networks on sequence modeling,” arXiv\npreprint arXiv:1412.3555, 2014.\n[11] R. Ghaeini, S. A. Hasan, V. Datla, J. Liu, K. Lee, A. Qadir, Y. Ling,\nA. Prakash, X. Z. Fern, and O. Farri, “Dr-bilstm: Dependent read-\ning bidirectional lstm for natural language inference,” arXiv preprint\narXiv:1802.05577, 2018.\n[12] A. P. Parikh, O. T¨ackstr¨om, D. Das, and J. Uszkoreit, “A decompos-\nable atention model for natural language inference,” arXiv preprint\narXiv:1606.01933, 2016.\n[13] Z. Wang, W. Hamza, and R. Florian, “Bilateral multi-perspective match-\ning for natural language sentences,” arXiv preprint arXiv:1702.03814,\n2017.\n[14] S. Kim, J.-H. Hong, I. Kang, and N. Kwak, “Semantic sentence matching\nwith densely-connected recurrent and co-atentive information,” arXiv\npreprint arXiv:1805.11360, 2018.\n[15] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by\njointly learning to align and translate,” arXiv preprint arXiv:1409.0473,\n2014.\n[16] H. He and J. Lin, “Pairwise word interaction modeling with deep neural\nnetworks for semantic similarity measurement,” in Proceedings of the\n2016 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, pp. 937–948,\n2016.\n[17] A. Williams, N. Nangia, and S. R. Bowman, “A broad-coverage chal-\nlenge corpus for sentence understanding through inference,” arXiv\npreprint arXiv:1704.05426, 2017.\n[18] J. Pennington, R. Socher, and C. Manning, “Glove: Global vectors for\nword representation,” in Proceedings of the 2014 conference on empirical\nmethods in natural language processing (EMNLP), pp. 1532–1543, 2014.\n[19] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\narXiv preprint arXiv:1412.6980, 2014.\n[20] G. Klambauer, T. Unterthiner, A. Mayr, and S. Hochreiter, “Self-\nnormalizing neural networks,” in Advances in Neural Information Pro-\ncessing Systems, pp. 971–980, 2017.\n[21] T. Munkhdalai and H. Yu, “Neural tree indexers for text understand-\ning,” in Proceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics, vol. 1, p. 11, NIH Public Access, 2017.\n[22] J. Choi, K. M. Yoo, and S.-g. Lee, “Learning to compose task-speciﬁc\ntree structures,” in Proceedings of the 2018 Association for the Advance-\nment of Artiﬁcial Intelligence (AAAI). and the 7th International Joint\nConference on Natural Language Processing (ACL-IJCNLP), 2018.\n[23] D. Kiela, C. Wang, and K. Cho, “Dynamic meta-embeddings for im-\nproved sentence representations,” in Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Processing, pp. 1466–1477,\n2018.\n[24] P. Liu, X. Qiu, J. Chen, and X. Huang, “Deep fusion lstms for text\nsemantic matching,” in Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics, vol. 1, pp. 1034–1043, 2016.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2018-12-05",
  "updated": "2018-12-06"
}