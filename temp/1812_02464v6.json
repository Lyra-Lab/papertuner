{
  "id": "http://arxiv.org/abs/1812.02464v6",
  "title": "Pseudo-Rehearsal: Achieving Deep Reinforcement Learning without Catastrophic Forgetting",
  "authors": [
    "Craig Atkinson",
    "Brendan McCane",
    "Lech Szymanski",
    "Anthony Robins"
  ],
  "abstract": "Neural networks can achieve excellent results in a wide variety of\napplications. However, when they attempt to sequentially learn, they tend to\nlearn the new task while catastrophically forgetting previous ones. We propose\na model that overcomes catastrophic forgetting in sequential reinforcement\nlearning by combining ideas from continual learning in both the image\nclassification domain and the reinforcement learning domain. This model\nfeatures a dual memory system which separates continual learning from\nreinforcement learning and a pseudo-rehearsal system that \"recalls\" items\nrepresentative of previous tasks via a deep generative network. Our model\nsequentially learns Atari 2600 games without demonstrating catastrophic\nforgetting and continues to perform above human level on all three games. This\nresult is achieved without: demanding additional storage requirements as the\nnumber of tasks increases, storing raw data or revisiting past tasks. In\ncomparison, previous state-of-the-art solutions are substantially more\nvulnerable to forgetting on these complex deep reinforcement learning tasks.",
  "text": "Pseudo-Rehearsal: Achieving Deep Reinforcement\nLearning without Catastrophic Forgetting\nCraig Atkinson1,∗, Brendan McCane1, Lech Szymanski1, Anthony Robins1,\nAbstract\nNeural networks can achieve excellent results in a wide variety of applica-\ntions. However, when they attempt to sequentially learn, they tend to learn\nthe new task while catastrophically forgetting previous ones. We propose\na model that overcomes catastrophic forgetting in sequential reinforcement\nlearning by combining ideas from continual learning in both the image classi-\nﬁcation domain and the reinforcement learning domain. This model features\na dual memory system which separates continual learning from reinforcement\nlearning and a pseudo-rehearsal system that “recalls” items representative of\nprevious tasks via a deep generative network. Our model sequentially learns\nAtari 2600 games without demonstrating catastrophic forgetting and contin-\nues to perform above human level on all three games. This result is achieved\nwithout: demanding additional storage requirements as the number of tasks\nincreases, storing raw data or revisiting past tasks. In comparison, previous\nstate-of-the-art solutions are substantially more vulnerable to forgetting on\nthese complex deep reinforcement learning tasks.\nKeywords:\nDeep Reinforcement Learning, Pseudo-Rehearsal, Catastrophic\nForgetting, Generative Adversarial Network\n∗Corresponding author.\nEmail address: atkcr398@student.otago.ac.nz (Craig Atkinson)\n1Department of Computer Science, University of Otago, 133 Union Street East,\nDunedin, New Zealand\nAccepted for publication in Neurocomputing (https://doi.org/10.1016/j.neucom.\n2020.11.050). © 2020. This manuscript version is made available under the CC-BY-\nNC-ND 4.0 license http://creativecommons.org/licenses/by-nc-nd/4.0/.\narXiv:1812.02464v6  [cs.LG]  16 Dec 2020\nAccepted for publication in Neurocomputing\n1. Introduction\nThere has been enormous growth in research around reinforcement learn-\ning since the development of Deep Q-Networks (DQNs) [1]. DQNs apply\nQ-learning to deep networks so that complicated reinforcement tasks can be\nlearnt. However, as with most distributed models, DQNs can suﬀer from\nCatastrophic Forgetting (CF) [2, 3]. This is where a model has the tendency\nto forget previous knowledge as it learns new knowledge. Pseudo-rehearsal\nis a method for overcoming CF by rehearsing randomly generated exam-\nples of previous tasks, while learning on real data from a new task. Although\npseudo-rehearsal methods have been widely used in image classiﬁcation, they\nhave been virtually unexplored in reinforcement learning. Solving CF in the\nreinforcement learning domain is essential if we want to achieve artiﬁcial\nagents that can continuously learn.\nContinual learning is important to neural networks because CF limits\ntheir potential in numerous ways. For example, imagine a previously trained\nnetwork whose function needs to be extended or partially changed. The typ-\nical solution would be to train the neural network on all of the previously\nlearnt data (that was still relevant) along with the data to learn the new func-\ntion. This can be an expensive operation because previous datasets (which\ntend to be very large in deep learning) would need to be stored and retrained.\nHowever, if a neural network could adequately perform continual learning,\nit would only be necessary for it to directly learn on data representing the\nnew function. Furthermore, continual learning is also desirable because it\nallows the solution for multiple tasks to be compressed into a single network\nwhere weights common to both tasks may be shared. This can also beneﬁt\nthe speed at which new tasks are learnt because useful features may already\nbe present in the network.\nOur Reinforcement-Pseudo-Rehearsal model (RePR2) achieves continual\nlearning in the reinforcement domain. It does so by utilising a dual memory\nsystem where a freshly initialised DQN is trained on the new task and then\nknowledge from this short-term network is transferred to a separate DQN\ncontaining long-term knowledge of all previously learnt tasks. A generative\nnetwork is used to produce states (short sequences of data) representative of\nprevious tasks which can be rehearsed while transferring knowledge of the\nnew task. For each new task, the generative network is trained on pseudo-\n2Read as “reaper”.\n2\nAccepted for publication in Neurocomputing\nitems produced by the previous generative network, alongside data from the\nnew task. Therefore, the system can prevent CF without the need for a large\nmemory store holding data from previously encountered training examples.\nThe reinforcement tasks learnt by RePR are Atari 2600 games. These\ngames are considered complex because the input space of the games is large\nwhich currently requires reinforcement learning to use deep neural networks\n(i.e. deep reinforcement learning). Applying pseudo-rehearsal methods to\ndeep reinforcement learning is challenging because these reinforcement learn-\ning methods are notoriously unstable compared to image classiﬁcation (due\nto the deadly triad [4]). In part, this is because target values are consistently\nchanging during learning. We have found that using pseudo-rehearsal while\nlearning these non-stationary targets is diﬃcult because it increases the inter-\nference between new and old tasks. Furthermore, generative models struggle\nto produce high quality data resembling these reinforcement learning tasks,\nwhich can prevent important task knowledge from being learnt for the ﬁrst\ntime, as well as relearnt once it is forgotten.\nOur RePR model applies pseudo-rehearsal to the diﬃcult domain of deep\nreinforcement learning. RePR introduces a dual memory model suitable for\nreinforcement learning. This model is novel compared to previously used\ndual memory pseudo-rehearsal models in two important aspects.\nFirstly,\nthe model isolates reinforcement learning to the short-term system, so that\nthe long-term system can use supervised learning (i.e. mean squared error)\nwith ﬁxed target values (converged on by the short-term network), prevent-\ning non-stationary target values from increasing the interference between\nnew and old tasks. Importantly, this diﬀers from previous applications of\npseudo-rehearsal, where both the short-term and long-term systems learn\nwith the same cross-entropy loss function. Secondly, RePR transfers knowl-\nedge between the dual memory system using real samples, rather than those\nproduced by a generative model. This allows tasks to be learnt and retained\nto a higher performance in reinforcement learning. The source code for RePR\ncan be found at https://bitbucket.org/catk1ns0n/repr_public/.\nA summary of the main contributions of this paper are:\n• the ﬁrst successful application of pseudo-rehearsal methods to complex\ndeep reinforcement learning tasks;\n• above state-of-the-art performance when sequentially learning complex\nreinforcement tasks, without storing any raw data from previously\nlearnt tasks;\n3\nAccepted for publication in Neurocomputing\n• empirical evidence demonstrating the need for a dual memory system\nas it facilitates new learning by separating the reinforcement learning\nsystem from the continual learning system.\n2. Background\n2.1. Deep Q-Learning\nIn deep Q-learning [1], the neural network is taught to predict the dis-\ncounted reward that would be received from taking each one of the possible\nactions given the current state. More speciﬁcally, it minimises the following\nloss function:\nLDQN = E(st,at,rt,dt,st+1)∼U(D)\n\"\u0012\nyt −Q(st, at; ψt)\n\u00132#\n,\n(1)\nyt =\n(\nrt,\nif terminal at t + 1\nrt + γ max\nat+1 Q(st+1, at+1; ψ−\nt ),\notherwise\n(2)\nwhere there exist two Q functions, a deep predictor network and a deep\ntarget network. The predictor’s parameters ψt are updated continuously by\nstochastic gradient descent and the target’s parameters ψ−\nt are infrequently\nupdated with the values of ψt. The tuple (st, at, rt, dt, st+1) ∼U(D) consists\nof the state, action, reward, terminal and next state for a given time step t\ndrawn uniformly from a large record of previous experiences, known as an\nexperience replay.\n2.2. Pseudo-Rehearsal\nThe simplest way of solving the CF problem is to use a rehearsal strat-\negy, where previously learnt items are practised alongside the learning of new\nitems. Researchers have proposed extensions to rehearsal [5, 6, 7]. However,\nall these rehearsal methods still have disadvantages, such as requiring exces-\nsive amounts of data to be stored from previously seen tasks3. Furthermore,\nin certain applications (e.g. the medical ﬁeld), storing real data for rehearsal\nmight not be possible due to privacy regulations. Additionally, rehearsal is\n3An experience replay diﬀers from rehearsal because it only stores recently seen data\nfrom the current task and therefore, learning data from the experience replay does not\nprevent forgetting of previously seen tasks.\n4\nAccepted for publication in Neurocomputing\nnot biologically plausible - mammalian brains don’t retain raw sensory infor-\nmation over the course of their lives. Hence, cognitive research might be a\ngood inspiration for tackling the CF problem.\nPseudo-rehearsal was proposed as a solution to CF which does not require\nstoring large amounts of past data and thus, overcomes the previously men-\ntioned shortcomings of rehearsal [8]. Originally, pseudo-rehearsal involved\nconstructing a pseudo-dataset by generating random inputs (i.e.\nfrom a\nrandom number generator), passing them through the original network and\nrecording their output. This meant that when a new dataset was learnt,\nthe pseudo-dataset could be rehearsed alongside it, resulting in the network\nlearning the data with minimal changes to the previously modelled function.\nThere is psychological research that suggests that mammal brains use an\nanalogous method to pseudo-rehearsal to prevent CF in memory consolida-\ntion. Memory consolidation is the process of transferring memory from the\nhippocampus, which is responsible for short-term knowledge, to the cortex\nfor long-term storage. The hippocampus and sleep have both been linked as\nimportant components for retaining previously learnt information [9]. The\nhippocampus has been observed to replay patterns of activation that occurred\nduring the day while sleeping [10], similar to the way that pseudo-rehearsal\ngenerates previous experiences. Therefore, we believe that pseudo-rehearsal\nbased mechanisms are neurologically plausible and could be useful as an ap-\nproach to solving the CF problem in deep reinforcement learning.\nPlain pseudo-rehearsal does not scale well to data with large input spaces\nsuch as images [11]. This is because the probability of a population of ran-\ndomly generated inputs sampling the space of possible input images with suf-\nﬁcient density is essentially zero. This is where Deep Generative Replay [12]\nand Pseudo-Recursal [11] have leveraged Generative Adversarial Networks\n(GANs) [13] to randomly generate pseudo-items representative of previously\nlearnt items.\nA GAN has two components; a generator and a discriminator.\nThe\ndiscriminator is trained to distinguish between real and generated images,\nwhereas the generator is trained to produce images which fool the discrim-\ninator. When a GAN is used alongside pseudo-rehearsal, the GAN is also\ntrained on the task so that its generator learns to produce items represen-\ntative of the task’s input items.\nThen, when a second task needs to be\nlearnt, pseudo-items can be generated randomly from the GAN’s genera-\ntor and used in pseudo-rehearsal. More speciﬁcally, the minimised cost for\n5\nAccepted for publication in Neurocomputing\npseudo-rehearsal is:\nJ = L(h (xi; θi), yi) +\ni−1\nX\nj=1\nL (h(exj; θi), eyj) ,\n(3)\nwhere L is a loss function, such as cross-entropy, and h is a neural network\nwith weights θi while learning task i. The tuple xi, yi is the input-output pair\nfor the current task. The tuple exj, eyj is the input-output pair for a pseudo-\nitem, where exj is the input generated so that it represents the previous task\nj and eyj is the target output calculated by eyj = h(exj; θi−1).\nThis technique can be applied to multiple tasks using only a single GAN\nby doing pseudo-rehearsal on the GAN as well. Thus, the GAN learns to\ngenerate items representative of the new task while still remembering to\ngenerate items representative of the previous tasks (by rehearsing the pseudo-\nitems it generates). This technique has been shown to be very eﬀective for\nremembering a chain of multiple image classiﬁcation tasks [12, 11].\n3. The RePR Model\nRePR is a dual memory model which uses pseudo-rehearsal with a gener-\native network to achieve sequential learning in reinforcement tasks. The ﬁrst\npart of our dual memory model is the short-term memory (STM) system4,\nwhich serves a role analogous to that of the hippocampus in learning and is\nused to learn the current task. The STM system contains two components;\na DQN that learns the current task and an experience replay containing\ndata only from the current task. The second part is the long-term memory\n(LTM) system, which serves a role analogous to that of the cortex. The LTM\nsystem also has two components; a DQN containing knowledge of all tasks\nlearnt and a GAN which can generate states representative of these tasks.\nDuring consolidation, the LTM system retains previous knowledge through\npseudo-rehearsal, while being taught by the STM system how to respond on\nthe current task.\nTransferring knowledge between these two systems is achieved through\nknowledge distillation [14], where a student network is optimised so that it\n4Note that the interpretation of the term “short-term memory” (STM) as used here\nand in related literature varies somewhat from the use of the same term in the general\npsychological literature.\n6\nAccepted for publication in Neurocomputing\noutputs similar values to a teacher network. In the RePR model, the student\nnetwork is the long-term DQN and the teacher network is the short-term\nDQN. The key diﬀerence between distillation and pseudo-rehearsal is that\ndistillation uses real items to teach new knowledge, whereas pseudo-rehearsal\nuses generated items to retain previously learnt knowledge.\nIn reinforcement learning (deep Q-learning), the target Q-values are from\na non-stationary distribution because they are produced by the target net-\nwork which is being updated during training. Our dual memory system in\nRePR is important because it allows the diﬃcult job of reinforcement learning\nto be isolated to the short-term DQN, so that no other learning can interfere\nwith it. Consequently, this reduces the diﬃculty of the problem for the long-\nterm DQN as it does not need to learn this non-stationary distribution while\nalso trying to retain knowledge of previous tasks. Instead, the long-term\nDQN can use supervised learning to learn the ﬁnal stationary distribution\nQ-values, while rehearsing previous knowledge through pseudo-rehearsal.\n3.1. Training Procedure\nThe training procedure can be broken down into three steps: short-term\nDQN training, long-term DQN training and long-term GAN training. This\nprocess could be repeated for any number of tasks until the DQN or GAN\nrun out of capacity to perform the role suﬃciently.\n3.1.1. Training the Short-Term DQN\nWhen there is a new task to be learnt, the short-term DQN is reini-\ntialised and trained solely on the task using the standard DQN loss function\n(Equation 1). A summary of the information ﬂow while training the STM\nsystem can be found in Fig. 1 and the related pseudo-code can be found in\nthe appendices as Algorithm 1.\n3.1.2. Training the Long-Term DQN\nKnowledge is transferred to the long-term DQN by teaching it to produce\nsimilar outputs to the short-term DQN on examples from its experience re-\nplay. Alongside this, the long-term DQN is constrained with pseudo-rehearsal\nto produce similar output values to the previous long-term DQN on states\ngenerated from the long-term GAN. More speciﬁcally, the loss functions used\nare:\nLLTM = 1\nN\nN\nX\nj=1\nαLDj + (1 −α)LPRj,\n(4)\n7\nAccepted for publication in Neurocomputing\nPlay\nTrain short-term DQN\nEnvironment\nExperience \nreplay\nDQN\nSTM\nExperience \nreplay\nDQN\nSTM\n𝐿!\"#\n𝑠$\n𝑎$\n𝑠$, 𝑟$, 𝑑$\n𝑎$, 𝑟$, 𝑑$, 𝑠$%&\n𝑄\n𝑠$\n𝑎$\n𝜓$\n𝜓$\nFigure 1: Flow of information while training the STM system. The model plays the game\nwhile simultaneously training the STM system.\nLDj =\nA\nX\na\n(Q(sj, a; θi) −Q(sj, a; ψi))2,\n(5)\nLPRj =\nA\nX\na\n(Q(esj, a; θi) −Q(esj, a; θi−1))2,\n(6)\nwhere sj is a state drawn from the current task’s experience replay, N is the\nmini-batch size, A is the set of possible actions, θi is the long-term DQN’s\nweights on the current task, ψi is the short-term DQN’s weights after learning\nthe current task and θi−1 is the long-term DQN’s weights after learning the\nprevious task. Pseudo-items’ inputs esj are generated from a GAN and are\n8\nAccepted for publication in Neurocomputing\nExperience \nreplay\nDQN\nSTM\nTrain long-term DQN\nDQN\nGAN\nDQN\nGAN\nLTM\n𝐿!\n𝐿\"#\ñ𝑠$\n𝑠$\n𝑠$\n𝑄\n𝑄\n𝑄\n𝑄\nExperience \nreplay\nDQN\nDQN\nGAN\nDQN\nGAN\n𝐿%&'\n𝑠$\n%𝑥\nSTM\nLTM\nTrain long-term GAN\n𝑠$\nCopy of LTM before training on the task\nCopy of LTM before training on the task\ñ𝑠$\ñ𝑠$\ñ𝑠$\n𝜃()*\n𝑥\n𝜃()*\n𝜃(\n𝜃(\n𝜓(\n𝜓(\nFigure 2: Flow of information while training the LTM system. Solid lines represent the\ninformation ﬂow for learning the new task and dashed lines represent the information ﬂow\nfor retaining previous tasks with pseudo-rehearsal. The DQN and GAN are both trained\nat independent times.\nrepresentative of states in previously learnt games. The trade oﬀbetween\nlearning the new tasks and retaining the previous ones is controlled by a\nscaling factor (0 < α < 1). A summary of the information ﬂow while training\nthe long-term DQN can be found in Fig. 2 and the related pseudo-code can\nbe found in the appendices as Algorithm 2.\n3.1.3. Training the Long-Term GAN\nThe GAN is reinitialised and trained to produce both states that are\nrepresentative of states the previous GAN outputs and states drawn from\nthe current task’s experience replay. More speciﬁcally, the states the GAN\nlearns are drawn such that:\nx =\n(\nsj,\nif r < 1\nT\nesj,\notherwise\n(7)\n9\nAccepted for publication in Neurocomputing\nwhere r is a random number uniformly drawn from [0, 1) and sj is a randomly\nselected state in the current task’s experience replay. T is the number of tasks\nlearnt and esj is a randomly generated state from the long-term GAN before\ntraining on task i. This state is a pseudo-item which ideally resembles a state\nfrom one of the prior tasks (task 1 to i −1). The GAN in our experiments\nis trained with the WGAN-GP loss function [15] with a drift term [16]. The\nspeciﬁc loss function can be found in Appendix\nB.2. A summary of the\ninformation ﬂow while training the long-term GAN can be found in Fig. 2\nand the related pseudo-code can be found in the appendices as Algorithm 3.\n3.2. Requirements of a Continual Learning Agent\nWe seek to operate within the constraints that apply to biological agents.\nAs such, we seek a continual learning agent capable of learning multiple tasks\nsequentially: without revisiting them; without substantially forgetting pre-\nvious tasks; with a consistent memory size that does not grow as the number\nof tasks increases; and without storing raw data from previous tasks. The\nresults in Section 6 demonstrate that our RePR model can sequentially learn\nmultiple tasks, without revisiting them or substantially forgetting previous\ntasks. Furthermore, our model does not store raw data because applying\npseudo-rehearsal methods to a single GAN allows it to learn to generate data\nrepresentative of all previous tasks, without growing in size as the number\nof tasks increases.\n4. Related Work\nThis section will focus on methods for preventing CF in reinforcement\nlearning and will generally concentrate on how to learn a new policy without\nforgetting those previously learnt for diﬀerent tasks. There is a lot of related\nresearch outside of this domain (see [17] for a broad review), predominantly\naround continual learning in image classiﬁcation. However, because these\nmethods cannot be directly applied to complex reinforcement learning tasks,\nwe have excluded them from this review.\nThere are two main strategies for avoiding CF. The ﬁrst of these strategies\nis restricting how the network is optimised. Some models, such as Progressive\nneural networks [18], introduce a large number of units, or even separate\nnetworks, which are restrictively trained only on a particular task. Although\nthese methods can share some weights, they still have a large number of\n10\nAccepted for publication in Neurocomputing\ntask speciﬁc ones and thus, the model size needs to grow substantially as it\ncontinually learns.\nAnother way which the network’s optimisation can be restricted is through\nweight constraints. These methods amend the loss function so that weights\ndo not change considerably when learning a new task. The most popular of\nthese methods is Elastic Weight Consolidation (EWC) [3], which augments a\nnetwork’s loss function with a constraint that forces the network’s weights to\nyield similar values to previous networks. Weights that are more important\nto the previous task/s are constrained more so that less important weights\ncan be used to learn the new task. EWC has been paired with a DQN to\nlearn numerous Atari 2600 games. One undesirable requirement of EWC\nis that the network’s weights after learning each task must be stored along\nwith either the Fisher information matrix for each task or examples from past\ntasks (so that the matrix can be calculated when needed). Other variations\nfor constraining the weights have also been proposed [19, 20]. However, these\nvariations have only been applied to relatively simple reinforcement tasks.\nProgress and Compress [21] learnt multiple Atari games by ﬁrstly learning\nthe game in STM and then using distillation to transfer it to LTM. The LTM\nsystem held all previously learnt tasks, counteracting CF using a modiﬁed\nversion of EWC called online-EWC. This modiﬁed version does not scale\nin memory requirements as the number of tasks increases. This is because\nthe algorithm stores only the weights of the network after learning the most\nrecent task, along with the discounted sum of previous Fisher information\nmatrices to use when constraining weights. In Progress and Compress, there\nare also layer-wise connections between the two systems to encourage the\nshort-term network to learn using features already learnt by the long-term\nnetwork.\nThe second main strategy for avoiding CF is to amend the training data\nto be more representative of previous tasks. This category includes both\nrehearsal and pseudo-rehearsal, as these methods add either real or gener-\nated samples to the training dataset. One example of a rehearsal method\nin reinforcement learning is PLAID [22]. It uses distillation to merge a net-\nwork that performs the new task with a network whose policy performs all\npreviously learnt tasks. Distillation methods have also been applied to Atari\n2600 games [23, 24]. However, these were in multi-task learning where CF\nis not an issue. The major disadvantage with all rehearsal methods is that\nthey require either having continuous access to previous environments or\nstoring a large amount of previous training data. For other recent examples\n11\nAccepted for publication in Neurocomputing\nof rehearsal in reinforcement learning see [25, 26, 27].\nTo our knowledge, pseudo-rehearsal has only been applied by [28] to se-\nquentially learn reinforcement tasks. This was achieved by extending the\nDeep Generative Replay algorithm from image classiﬁcation to reinforcement\nlearning. Pseudo-rehearsal was combined with a Variational Auto-Encoder\nso that two very simple reinforcement tasks could be sequentially learnt by\nState Representation Learning without CF occurring. These tasks involved\na 2D world where the agent’s input was a small 64 × 3 grid representing the\ncolour of objects it could see in front of it. The only thing that changed\nbetween tasks was the colour of the objects the agent must collect. This\nsequential learning environment is simple compared to the one we use and\nthus, there are a number of important diﬀerences in our RePR model. Our\ncomplex reinforcement learning tasks are relatively diﬀerent from one another\nand have a large input space. This requires RePR to use deep convolutional\nnetworks, speciﬁcally DQNs and GANs, to learn and generate plausible input\nitems. Furthermore, our model incorporates a dual memory system which\nisolates reinforcement learning to the STM system, improving the acquisition\nof the new task.\nWithout an experience replay, CF can occur while learning even a single\ntask as the network forgets how to act in previously seen states. Pseudo-\nrehearsal has also been applied to this problem by rehearsing randomly gener-\nated input items from basic distributions (e.g. uniform distribution) [29, 30],\nwith a similar idea accomplished in actor-critic networks [31]. However, all\nthese methods were applied to simple reinforcement tasks and did not utilise\ndeep generative structures for producing pseudo-items or convolutional net-\nwork architectures. Since our work, pseudo-rehearsal has been used to over-\ncome CF in models which have learnt to generate states from previously\nseen environments [32, 33]. But in both these cases, pseudo-rehearsal was\nnot applied to the learning agent to prevent CF.\nThe model which most closely resembles RePR is the Deep Generative\nDual Memory Network (DGDMN) [34] used in sequential image classiﬁca-\ntion. The DGDMN extends Deep Generative Replay by introducing a dual\nmemory system similar to RePR. The STM system comprises one or more\nmodules made up of a classiﬁer paired with a generative network (i.e.\na\nVariational Auto-Encoder). A separate module is used to learn each of the\nrecent tasks. The LTM system comprises a separate classiﬁer and generative\nnetwork. When consolidation occurs, knowledge from the short-term mod-\nules is transferred to the LTM system while the long-term generator is used\n12\nAccepted for publication in Neurocomputing\nto produce pseudo-items for pseudo-rehearsal.\nThe primary diﬀerence between DGDMN and RePR relates to the data\nused to teach the new task to the LTM system. The short-term modules in\nDGDMN each contain a generative network which learns to produce input\nexamples representative of the new task. These examples are labelled by their\nrespective classiﬁer and then taught to the LTM system. However, in RePR,\nthe STM system does not contain a generative network and instead uses data\nfrom its experience replay to train the LTM system. This experience replay\ndoes not grow in memory as it only contains a limited amount of data from\nthe most recent task and as this data is more accurate than generated data,\nit is more eﬀective in teaching the LTM system the new task. There are also\nkey diﬀerences in the loss functions used for DGDMN and RePR. DGDMN\nwas developed for sequential image classiﬁcation and so, it uses the same\ncross-entropy loss function for learning new tasks in the STM system as in\nthe LTM system. However, in RePR the learning process is separated. The\nmore diﬃcult reinforcement learning (deep Q-learning) is accomplished in\nthe STM system. This is isolated from the LTM system, which can learn\nand retain through supervised learning (i.e. mean squared error).\nTo our knowledge, a dual memory system has not previously been used\nto separate diﬀerent types of learning in pseudo-rehearsal algorithms. Al-\nthough a similar dual memory system to RePR was used in Progress and\nCompress [21], unlike in RePR, authors of Progress and Compress did not\nﬁnd conclusive evidence that it assisted sequential reinforcement learning.\nIn real neuronal circuits it is a matter of debate whether memory is re-\ntained through synaptic stability, synaptic plasticity, or a mixture of mech-\nanisms [35, 36, 37]. The synaptic stability hypothesis states that memory\nis retained through ﬁxing the weights between units that encode it. The\nsynaptic plasticity hypothesis states that the weights between the units can\nchange as long as the output units still produce the correct output pattern.\nEWC and Progress and Compress are both methods that constrain the net-\nwork’s weights and therefore, align with the synaptic stability hypothesis.\nPseudo-rehearsal methods amend the training dataset, pressuring the net-\nwork’s outputs to remain the same, without constraining the weights and\ntherefore, align with the synaptic plasticity hypothesis.\nPseudo-rehearsal\nmethods have not yet been successfully applied to complex reinforcement\nlearning tasks. The major advantage of methods that align with the synap-\ntic plasticity hypothesis is that, when consolidating new knowledge, they\nallow the network to restructure its weights and compress previous repre-\n13\nAccepted for publication in Neurocomputing\nsentations to make room for new ones. This suggests that, in the long run,\npseudo-rehearsal will outperform state-of-the-art weight constraint methods\nin reinforcement learning, since it allows neural networks to internally recon-\nﬁgure in order to consolidate new knowledge.\nIn summary, RePR is the ﬁrst variation of pseudo-rehearsal to be suc-\ncessfully applied to continual learning with complex reinforcement tasks.\n5. Method\nOur current research applies pseudo-rehearsal to deep Q-learning so that a\nDQN can be used to learn multiple Atari 2600 games5 in sequence. All agents\nselect between 18 possible actions representing diﬀerent combinations of joy-\nstick movements and pressing the ﬁre button. Our DQN is based upon [1]\nwith a few minor changes which we found helped the network to learn the\nindividual tasks quicker. The speciﬁcs of these changes can be found in Ap-\npendix B.1. Our initial experiments aim to comprehensively evaluate RePR\nby comparing it to competing methods. In these experiments, conditions are\ntaught Road Runner, Boxing and James Bond. These were chosen as they\nwere three conceptually diﬀerent games in which a DQN outperforms humans\nby a wide margin [1]. The tasks were learnt in the order speciﬁed above. The\nﬁnal experiment aims to test the capacity of RePR and analyse whether the\nmodel fails gracefully or catastrophically when pushed beyond its capacity.\nIn this condition we extend the sequence learnt with the Atari games Pong,\nAtlantis and Qbert. Due to the exceptionally long computational time re-\nquired for running conditions in this experiment, only RePR and rehearsal\nmethods were tested with a single seed on this extended sequence. The pro-\ncedure for this extended experiment is identical to the procedure described\nbelow for the initial experiments, except for two diﬀerences. Firstly, learning\nis slowed down by reducing α and doubling the LTM system’s training time.\nSecondly, a larger DQN and GAN is used to ensure the initial size of the\nnetworks is large enough to learn some of the extended sequence of tasks6.\nSpeciﬁc details for the extended experiment can be found in Appendix B.5.\n5A brief summary of the Atari 2600 games learnt in this paper can be found in Appendix\nA.\n6Importantly, even when using larger networks, RePR is still pushed beyond its capacity\nas the results from this experiment clearly demonstrate forgetting.\n14\nAccepted for publication in Neurocomputing\nTable 1: Details of the experimental conditions.\nCondition\nDescription\nno-reh\nFor each new task, the task is learnt in the STM system using deep Q-learning (Equation 1).\nThe task is then transferred to the LTM system using distillation (Equation 5). This\ncondition does not attempt to retain previously learnt tasks.\nreh\nFor each new task, the task is learnt in the STM system using deep Q-learning (Equation 1).\nThe task is then transferred to the LTM system using distillation, while previously learnt\ntasks are retained with rehearsal (Equation 4, where the generated items esj are replaced with\nreal items from previous tasks).\nRePR\nFor each new task, the task is learnt in the STM system using deep Q-learning (Equation 1).\nThe task is then transferred to the long-term DQN using distillation, while previously learnt\ntasks are retained with pseudo-rehearsal (Equation 4). The long-term GAN is then taught to\ngenerate data from the new task and previously learnt tasks (Equation 7) using the loss\nfunctions deﬁned in Equation B.1 and Equation B.2 in Appendix B.2.\newc\nFor each new task, the task is learnt in the STM system using deep Q-learning (Equation 1).\nThe task is then transferred to the LTM system using distillation, while previously learnt\ntasks are retained with EWC (Equation B.3 in Appendix B.3).\nonline-ewc\nFor each new task, the task is learnt in the STM system using deep Q-learning (Equation 1).\nThe task is then transferred to the LTM system using distillation (Equation 5), while\npreviously learnt tasks are retained with online-EWC (Equation B.6 in Appendix B.3).\nPR\nFor each new task, the task is learnt in a DQN using deep Q-learning (Equation 1), while\npreviously learnt tasks are retained with pseudo-rehearsal (Equation 6, where the weights θi\nare the same weights ψt used in deep Q-learning). The GAN is then taught to generate data\nfrom the new task and previously learnt tasks (Equation 7) using the loss functions deﬁned in\nEquation B.1 and Equation B.2 in Appendix B.2.\nDGDMN\nFor each new task, the task is learnt in the short-term DQN using deep Q-learning\n(Equation 1). The short-term GAN is then taught to generate data from the new task using\nthe loss functions deﬁned in Equation B.1 and Equation B.2 in Appendix B.2. The task is\nthen transferred to the long-term DQN using distillation with generated items (Equation 5,\nwhere real items sj are replaced with items generated by the short-term GAN), while\npreviously learnt tasks are retained with pseudo-rehearsal (Equation 6). The long-term GAN\nis then taught to generate data from the new task and previously learnt tasks (Equation 7,\nwhere the real items from the new task sj are replaced with generated items from the\nshort-term GAN) using the same loss functions as the short-term GAN.\nreh-limit\nThis condition is identical to the reh condition above, except that the number of items stored\nand used in rehearsal is limited to the number of uncompressed items (i.e. 600) which could\nbe stored using the same memory allocation size as used by RePR’s generative network.\nreh-limit-comp\nThis condition is identical to the reh condition above, except that the number of items stored\nand used in rehearsal is limited to the number of compressed items (i.e. 17000) which could\nbe stored using the same memory allocation size as used by RePR’s generative network.\nreh-extend\nThis condition is identical to the reh condition above, except the model and its\nhyper-parameters are set up to learn the extended task sequence (see Appendix B.5 for more\ndetails).\nRePR-extend\nThis condition is identical to the RePR condition above, except the model and its\nhyper-parameters are set up to learn the extended task sequence (see Appendix B.5 for more\ndetails).\nreh-extend-policy\nThis condition is identical to the reh condition above, except the model and its\nhyper-parameters are set up to learn the extended task sequence (see Appendix B.5 for more\ndetails). Furthermore, the policy is transferred and retained in the long-term agent, rather\nthan Q-values.\nRePR-extend-policy\nThis condition is identical to the RePR condition above, except the model and its\nhyper-parameters are set up to learn the extended task sequence (see Appendix B.5 for more\ndetails). Furthermore, the policy is transferred and retained in the long-term agent, rather\nthan Q-values.\n15\nAccepted for publication in Neurocomputing\nThe details of the experimental conditions used in this paper can be found\nin Table 1.\nThe architecture of the DQNs is kept consistent across all experimental\nconditions that train on the same sequence of tasks. Details of the network\narchitectures (including the GAN) and training parameters used throughout\nthe experiments can be found in Appendix B. A hyper-parameter search was\nused in conditions that used a variant of EWC. Details of this search can also\nbe found in this appendix along with other speciﬁc implementation details\nfor these conditions. The online-EWC implementation does not include con-\nnections from the LTM system to the STM system which try and encourage\nweight sharing when learning the new task. This was not included because\nauthors of the Progress and Compress method found online-EWC alone was\ncompetitive with the Progress and Compress method (which included these\nconnections) and it kept the architecture of the agents’ dual memory system\nconsistent with other conditions.\nIn some of the conditions listed in Table 1, the policy is transferred and\nretained in the long-term agent, rather than Q-values. Transfer is achieved\nby giving samples from the current task to the short-term agent and one-\nhot encoding the action with the largest associated Q-value. This one-hot\nencoding is then taught to the long-term agent using the cross-entropy loss\nfunction. Similarly, when the policy is being retained in RePR’s long-term\nagent, generated samples are given to the previous long-term agent and the\nsoftmax values produced are then retained using the cross-entropy loss func-\ntion. More speciﬁc details on learning and retaining the policy (including\nloss functions) can be found in Appendix B.4.\nEach game was learnt by the STM system for 20 million frames and then\ntaught to the LTM system for 20m frames. The only exception was for the\nﬁrst long-term DQN which had the short-term DQN’s weights copied directly\nover to it. This means that our experimental conditions diﬀer only after the\nsecond task (Boxing) was introduced. The GAN had its discriminator and\ngenerator loss function alternatively optimised for 200,000 steps.\nWhen pseudo-rehearsal was applied to the long-term DQN agent or GAN,\npseudo-items were drawn from a temporary array of 250,000 states generated\nby the previous GAN. The ﬁnal weights for the short-term DQN are those\nthat produce the largest average score over 250,000 observed frames. The\nﬁnal weights for the long-term DQN are those that produced the lowest\nerror over 250,000 observed frames.\nIn the initial experiments α = 0.55.\nHowever, we have also tested RePR with α = 0.35 and α = 0.75, both of\n16\nAccepted for publication in Neurocomputing\nwhich produced very similar results, with the ﬁnal agent performing at least\nequivalently to the original DQN’s results [1] for all tasks.\nOur evaluation procedure is similar to [1] in that our network plays each\ntask for 30 episodes and an episode terminates when all lives are lost. Ac-\ntions are selected from the network using an ε-greedy policy with ε = 0.05.\nFinal network results are also reported using this procedure and standard\ndeviations are calculated over these 30 episodes. Unless stated otherwise,\neach condition is trained three times using the same set of seeds between\nconditions and all reported results are averaged across these seeds.\n6. Results\n6.1. RePR Performance on CF\nThe ﬁrst experiment investigates how well RePR compares to a lower and\nupper baseline. The no-reh condition is the lower baseline because it does\nnot contain a component to assist in retaining the previously learnt tasks.\nThe reh condition is the upper baseline for RePR because it rehearses real\nitems from previously learnt tasks and thus, demonstrates how RePR would\nperform if its GAN could perfectly generate states from previous tasks to\nrehearse alongside learning the new task7.\nThe results of RePR can be found in Fig. 3, alongside other conditions’\nresults. All of the mentioned conditions outperform the no-reh condition\nwhich severely forgets previous tasks.\nRePR was found to perform very\nclosely to the reh condition, besides a slight degradation of performance on\nRoad Runner, which was likely due to the GAN performing pseudo-rehearsal\nto retain states representative of Road Runner. These results suggest that\nRePR can prevent CF without any need for extra task speciﬁc parameters\nor directly storing examples from previously learnt tasks.\nWe also investigated whether a similar set of weights are important to the\nRePR agent’s output on all of the learnt tasks or whether the network learns\nthe tasks by dedicating certain weights as important to each individual task.\nWhen observing the overlap in the network’s Fisher information matrices for\neach of the games (see Appendix C for implementation details and speciﬁc\nresults), we found that the network did share weights between tasks, with\n7The reh condition is not doing typical rehearsal although the diﬀerence is subtle. It\nrelearns previous tasks using targets produced by the previous network (as in RePR),\nrather than targets on which the original long-term DQN was taught.\n17\nAccepted for publication in Neurocomputing\n0\n10\n20\n30\n40\n50\n60\nNumber of training frames seen across all games (millions)\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\nTest score\nRoad Runner test scores during training\nshared\nno-reh\nreh\nRePR\newc\nonline-ewc\nPR\nDGDMN\nreh-limit\nreh-limit-comp\n0\n10\n20\n30\n40\n50\n60\nNumber of training frames seen across all games (millions)\n100\n50\n0\n50\n100\nTest score\nBoxing test scores during training\nno-reh\nreh\nRePR\newc\nonline-ewc\nPR\nDGDMN\nreh-limit\nreh-limit-comp\n0\n10\n20\n30\n40\n50\n60\nNumber of training frames seen across all games (millions)\n0\n100\n200\n300\n400\n500\n600\n700\n800\nTest score\nJames Bond test scores during training\nno-reh\nreh\nRePR\newc\nonline-ewc\nPR\nDGDMN\nreh-limit\nreh-limit-comp\nFigure 3: Results of our RePR model compared to the no-reh, reh, ewc, online-ewc, PR,\nDGDMN, reh-limit and reh-limit-comp conditions. After every 1 million observable\ntraining frames, the long-term agent is evaluated on the current task and all previously\nlearnt tasks. Task switches occur at the dashed lines, in the order Road Runner, Boxing\nand then James Bond.\n18\nAccepted for publication in Neurocomputing\nReal Images\nGenerated Images\nFigure 4: Images drawn from previous tasks’ experience replays (real) and images gener-\nated from a GAN sequentially taught to produce sequences from Road Runner, Boxing\nand then James Bond. Images shown are the ﬁrst image of each four frame sequence.\nEach row contains images from one of the three tasks.\nsimilar tasks sharing a larger proportion of important weights. Overall, these\npositive results show that RePR is a useful strategy for overcoming CF.\n6.2. Quality of Generated Items\nIn RePR, all pseudo-items generated by the GAN are used in pseudo-\nrehearsal. Furthermore, we have found that balancing the proportion of new\nitems and pseudo-items by the number of tasks learnt (with Equation 7) is\nsuﬃcient to ensure the GAN generates items evenly from each of the tasks.\nFig. 4 shows some GAN generated images after learning all three tasks, along-\nside real images from the games. This ﬁgure shows that although the GAN is\nsuccessful at generating images similar to the previous games there are clear\nvisual diﬀerences between them.\nA further experiment was conducted to analytically assess the quality\nof pseudo-items. This experiment uses three networks. The ﬁrst is a DQN\ntrained on a task (or sequence of tasks) and is the teacher network. The\nsecond is a GAN trained on the same task/s as the ﬁrst network. The ﬁnal\nnetwork is a freshly initialised DQN and is the student network. The student\n19\nAccepted for publication in Neurocomputing\nTable 2: Final long-term network scores (and standard deviations) attained after training\na student DQN using either real or generated items. Results are collected using a single\nconsistent seed with the average scores and standard deviations calculated by testing the\nﬁnal network on 30 episodes.\nCondition\nRoad Runner\nBoxing\nJames Bond\nreal road\n27133 (±5772)\ngan road\n15343 (±5885)\nreal boxing\n84 (±17)\ngan boxing\n70 (±21)\nreal james\n482 (±139)\ngan james\n247 (±98)\nreal 3T\n23660 (±5112)\n79 (±20)\n342 (±178)\ngan 3T\n747 (±739)\n−6 (±12)\n142 (±100)\nnetwork is taught how to play the task/s by the teacher network (using the\ndistillation loss function in Equation 5). The data used to teach the task/s\nis either real items from the task/s (real) or pseudo-items generated by the\nGAN (gan).\nTherefore, the score which the student network can attain,\nafter training with real or generated data, reﬂects the quality of the training\ndata. The student network was either taught to play one of the games: Road\nRunner (road), Boxing (boxing) or James Bond (james), or taught to play\nall three of these games at once (3T).\nTable 2 shows a clear diﬀerence between the quality of real and generated\nitems. When learning a single task with items generated by the GAN, the\nstudent network cannot learn the task to the same score as it can with real\nitems. When the GAN has been taught multiple tasks, the gan 3T condition\nshows that the quality of generated items is severely lower than real items\nand cannot be used to learn all three tasks to a reasonable standard. This\ncan be considered a positive result for RePR as it demonstrates that pseudo-\nitems can still be used to eﬀectively prevent CF even when the pseudo-items\nare considerably poorer quality than real items.\n6.3. RePR Versus EWC\nWe further investigate the eﬀectiveness of RePR by comparing its perfor-\nmance to the leading EWC variants. The results of both the EWC conditions\nare also included in Fig. 3. These results clearly show that RePR outperforms\nboth EWC and online-EWC under these conditions. We ﬁnd that EWC re-\ntains past experiences better than online-EWC and due to this, online-EWC\nwas more eﬀective at learning the new task.\n20\nAccepted for publication in Neurocomputing\nThe poor results displayed by the EWC variants contrast substantially\nfrom those originally reported by authors [3, 21]. This can be explained by\nthe diﬀerences in our training scheme. More speciﬁcally, we do not allow\ntasks to be revisited, whereas both EWC and Progress and Compress visited\ntasks several times. Furthermore, we do not allow the networks we tested\nto grow in capacity when a new task is learnt, whereas the training scheme\nin [3] allowed EWC to have two task speciﬁc weights per neuron. Finally, the\nnetworks we have tested so far are retaining Q-values in their LTM system,\nwhereas Progress and Compress [21] retained only the policy of an Actor-\nCritic network in its LTM system.\nIn Appendix\nB.3, we test RePR and the EWC variants on a diﬀerent\ntraining scheme where only the policy is retained in the LTM system and\nthe total training time of the LTM system is reduced so that tasks need only\nto be retained for a shorter period of time. Under this diﬀerent training\nscheme, the EWC variants perform comparatively to RePR and thus, the\ntraining scheme less comprehensively explores the capabilities of the models.\n6.4. Further Evaluating RePR\nIn this section, RePR is further evaluated through a number of condi-\ntions. Firstly, the PR condition represents an ablation study investigating\nthe importance of the dual memory system. Consequently, the PR condition\nis identical to the RePR condition without a dual memory system. In Fig. 3,\nthe PR condition demonstrates poorer results compared to the RePR con-\ndition along with slower convergence times for learning the new task. This\nshows that combining pseudo-rehearsal with a dual memory model, as we\nhave done in RePR, is beneﬁcial for learning the new task.\nThe DGDMN condition represents an implementation of the DGDMN.\nThis network was proposed for sequential image classiﬁcation and therefore,\nsigniﬁcant changes were necessary to allow the model to learn reinforcement\ntasks. These changes made our DGDMN implementation similar to RePR,\nexcept for the presence of a separate GAN in the STM system which DGDMN\nteaches to generate data representative of the new task. This GAN gener-\nates the data used to train the LTM system on the new task. The DGDMN\ncondition also demonstrates poorer results compared to the RePR condi-\ntion. The most evident diﬀerence between these conditions was DGDMN’s\ninability to completely transfer new tasks to its long-term DQN using items\ngenerated by its STM system. More speciﬁcally, the DGDMN’s long-term\nDQN could learn the new tasks Boxing and James Bond to approximately\n21\nAccepted for publication in Neurocomputing\nTable 3: Final long-term network scores for each of the conditions, along with their storage\nrequirements. The ﬁnal three rows contain the scores which the same DQN can attain\nafter training solely on the speciﬁed task. Results are collected using three consistent seeds\nwith the average scores and standard deviations calculated by testing each of the seed’s\nﬁnal networks on 30 episodes.\nCondition\nFinal network’s average score (std)\nMemory space*\nRoad Runner\nBoxing\nJames Bond\nno-reh\n0 (±0)\n−13 (±9)\n619 (±165)\n0.007 GB\nreh\n26486 (±6245)\n85 (±10)\n548 (±156)\n7.063 GB\nRePR\n22042 (±5375)\n82 (±12)\n468 (±155)\n0.023 GB\newc\n581 (±525)\n−2 (±10)\n162 (±97)\n0.042 GB\nonline-ewc\n130 (±159)\n−10 (±10)\n381 (±165)\n0.014 GB\nPR\n21804 (±6042)\n33 (±17)\n342 (±99)\n0.023 GB\nDGDMN\n22841 (±4211)\n36 (±24)\n184 (±108)\n0.023 GB\nreh-limit\n151 (±329)\n29 (±26)\n598 (±246)\n0.024 GB\nreh-limit-comp\n16348 (±6980)\n80 (±14)\n511 (±178)\n0.024 GB\nRoad Runner\n29463 (±7864)\nn/a\nBoxing\n88 (±8)\nn/a\nJames Bond\n645 (±161)\nn/a\n*Memory allocation size required for long-term storage.\nhalf the performance of RePR. To be consistent with the other conditions\ntested, the STM system’s networks were copied to the LTM system after\nlearning the ﬁrst task. Because of this, the LTM system learnt the ﬁrst task\nwith real data (not data generated from the short-term GAN) and thus, did\nnot struggle to learn the ﬁrst task Road Runner.\nThe memory eﬃciency of RePR is investigated with the reh-limit and\nreh-limit-comp conditions. These conditions only store a small number of\neither uncompressed or compressed real items for rehearsal, limited by the\nmemory allocation size of RePR’s generative network. The reh-limit condi-\ntion shows substantially more forgetting compared to RePR. On Road Run-\nner, the reh-limit condition quickly forgets everything it has learnt about the\ntask and thus, performs similarly to the no-reh condition, which makes no ef-\nfort to retain the task. On Boxing, forgetting causes the reh-limit condition\nto retain roughly half of its performance on the task. The reh-limit-comp\ncondition only displays forgetting on Road Runner, where, compared to\nRePR, the condition retains noticeably less knowledge of Road Runner through-\nout the learning sequence.\nThe scores for the ﬁnal long-term networks8 in each of the conditions are\n8For the PR condition, as no dual memory system is used, we refer to the DQN which\n22\nAccepted for publication in Neurocomputing\nshown in Table 3 along with the long-term storage space required by the\nmodels to be able to continue learning. Additionally, the table includes the\nscores which can be attained by training three DQNs individually on each\nof the tasks. Similar to previous results, this table shows that the reh and\nRePR conditions are the most successful at learning and retaining this se-\nquence of tasks, with reh achieving on average 90% of the scores that could\nbe attained from individually learning the tasks and RePR achieving 80%.\nThe scores attained by RePR are found to be well above human expert per-\nformance levels (7845, 4, 407) [1]. The table also shows that the long-term\nmemory allocation used by the RePR condition was smaller than the reh\ncondition by orders of magnitude9. Although, we did not attempt to optimise\nthis size for either of the conditions, the reh-limit and reh-limit-comp con-\nditions show that RePR outperforms rehearsal when rehearsal is constrained\nto approximately the same memory size as the RePR condition.\nFinally, Fig. 5 displays the results for the ﬁnal experiment which compares\nRePR to rehearsal in the extended sequence of Atari 2600 tasks. Overall, the\nreh-extend, reh-extend-policy and RePR-extend-policy conditions retained\nconsiderable knowledge of previous tasks, with reh-extend-policy retaining\nnoticeably more knowledge of Road Runner. The RePR-extend condition\nunderperformed compared to the other conditions, showing mainly a gradual\ndecline in performance of previously learnt tasks.\n7. Discussion\nOur experiments have demonstrated RePR to be an eﬀective solution to\nCF when sequentially learning multiple tasks. To our knowledge, pseudo-\nrehearsal has not been used until now to successfully prevent CF on complex\nreinforcement learning tasks. RePR has advantages over popular weight con-\nstraint methods, such as EWC, because it does not constrain the network to\nretain similar weights when learning a new task. This allows the internal lay-\ners of the network to change according to new knowledge, giving the model\nthe freedom to restructure itself when incorporating new information. Exper-\nimentally, we have veriﬁed that RePR does outperform these state-of-the-art\nEWC methods on a sequential learning task.\nlearns new tasks while retaining previous tasks as the ﬁnal long-term network.\n9When the reh condition is storing compressed items, the size of its long-term memory\nallocation is still an order of magnitude greater than RePR at approximately 0.250 GB.\n23\nAccepted for publication in Neurocomputing\n0\n50\n100\n150\n200\nNumber of training frames seen across all games (millions)\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\nTest score\nRoad Runner test scores during training\nshared\nshared-policy\nreh-extend\nRePR-extend\nreh-extend-policy\nRePR-extend-policy\n0\n50\n100\n150\n200\nNumber of training frames seen across all games (millions)\n100\n50\n0\n50\n100\nTest score\nBoxing test scores during training\nreh-extend\nRePR-extend\nreh-extend-policy\nRePR-extend-policy\n0\n50\n100\n150\n200\nNumber of training frames seen across all games (millions)\n0\n200\n400\n600\n800\nTest score\nJames Bond test scores during training\nreh-extend\nRePR-extend\nreh-extend-policy\nRePR-extend-policy\n0\n50\n100\n150\n200\nNumber of training frames seen across all games (millions)\n20\n10\n0\n10\n20\nTest score\nPong test scores during training\nreh-extend\nRePR-extend\nreh-extend-policy\nRePR-extend-policy\n0\n50\n100\n150\n200\nNumber of training frames seen across all games (millions)\n0\n10000\n20000\n30000\n40000\n50000\n60000\n70000\n80000\nTest score\nAtlantis test scores during training\nreh-extend\nRePR-extend\nreh-extend-policy\nRePR-extend-policy\n0\n50\n100\n150\n200\nNumber of training frames seen across all games (millions)\n0\n2000\n4000\n6000\n8000\n10000\nTest score\nQbert test scores during training\nreh-extend\nRePR-extend\nreh-extend-policy\nRePR-extend-policy\nFigure\n5:\nResults\nof\nthe\nreh-extend,\nRePR-extend,\nreh-extend-policy\nand\nRePR-extend-policy conditions on an extended sequence of tasks.\nAfter every 1 mil-\nlion observable training frames, the long-term agent is evaluated on the current task and\nall previously learnt tasks. Task switches occur at the dashed lines, in the order Road\nRunner, Boxing, James Bond, Pong, Atlantis and then Qbert. Results were produced\nusing a single seed.\n24\nAccepted for publication in Neurocomputing\n7.1. Dual Memory\nThe PR condition omits using the dual memory system to analyse the sys-\ntem’s importance. This results in the condition trying to learn non-stationary\ntarget Q-values (for the new task) through reinforcement learning, while also\nretaining previous tasks through pseudo-rehearsal. The results for the PR\ncondition showed that convergence times were longer and the ﬁnal perfor-\nmance reached was lower on new tasks compared to RePR. Therefore, this\ndemonstrates that omitting the dual memory system increases the interfer-\nence between new knowledge and knowledge of previous tasks. In Appendix\nD the extent of this interference is explored and it is found that to learn\nBoxing to a similar standard to RePR, while also omitting the dual mem-\nory system, the deep Q-learning loss function must be weighted substantially\nhigher than the pseudo-rehearsal loss function, such that the model consid-\nerably suﬀers from CF. Overall, these results are strong evidence that the\ndual memory system is beneﬁcial in sequential reinforcement learning tasks.\nThis is particularly interesting because the authors of Progress and Com-\npress [21] did not ﬁnd clear evidence that their algorithm beneﬁted from a\nsimilar dual memory system in their experimental conditions and addition-\nally, the dual memory system in RePR showed more substantial beneﬁts than\nthe dual memory system used by DGDMN in image classiﬁcation with the\nsame number of tasks [34].\nOur results show that the dual memory system used by DGDMN per-\nforms substantially worse than RePR in reinforcement learning. This is be-\ncause DGDMN relies on short-term generative networks to provide the data\nto teach the LTM system. When these tasks are diﬃcult for the generative\nnetwork to learn, this data is not accurate enough to eﬀectively teach new\ntasks to the LTM system. Although this was not an issue for the DGDMN\nin image classiﬁcation [34], in Section 6.2 we show that, in these complex\nreinforcement tasks, the GAN does struggle such that it is much more eﬀec-\ntive to teach a new task to a freshly initialised DQN using real data than\ngenerated data. However, our results also show that generated items that are\nnot of high enough quality for STM to LTM transfer, can be used eﬀectively\nin pseudo-rehearsal.\nTo overcome the limited quality of generative items, RePR’s dual memory\nsystem assumes real data from the most recent task can be retained in an\nexperience replay so that it can be used to successfully teach the LTM system.\nWe do not believe this assumption is unrealistic because the experience replay\nis a temporary buﬀer, with a ﬁxed size, and once the most recent task has\n25\nAccepted for publication in Neurocomputing\nbeen consolidated into LTM, it is cleared to make room for data from the\nnext task.\n7.2. Scalability\nOne main advantage of RePR compared to many other continual learn-\ning algorithms is that it is very scalable. Applying pseudo-rehearsal to both\nthe agent and generative model means that the network does not need to\nuse any task speciﬁc weights to accomplish continual learning and thus, the\nmodel’s memory requirements do not grow as the number of tasks increases.\nRePR’s generative network also means that it does not retain knowledge by\nstoring raw data from previous tasks.\nThis is advantageous in situations\nwhere storing raw data is not allowed because of e.g. privacy reasons, biolog-\nical plausibility, etc. Furthermore, results show RePR outperforms rehearsal\nwhen the number of rehearsal items are limited by the storage space required\nfor RePR’s generative network (reh-limit and reh-limit-comp). Therefore,\nRePR can also prevent CF more eﬀectively than rehearsal when memory is\nlimited.\nIn theory, RePR could be used to learn any number of tasks as long as\nthe agent’s network (e.g. DQN) and GAN have the capacity to successfully\nlearn the collection and generate states that are representative of previously\nlearnt tasks. However, in practice the capacity of RePR’s components could\nbe exceeded and we explored how this aﬀects RePR by investigating it on\nan extended sequence of tasks. In this test, RePR demonstrated a modest\namount of forgetting when required to retain only the policy in its LTM sys-\ntem. However, when RePR is required to retain Q-values in its LTM system,\nit shows substantial forgetting compared to rehearsal. In this case, RePR’s\nforgetting was generally gradual, such that learning the new tasks only dis-\nrupts retention after millions of training frames. However, the reh-extend\ncondition does not notably suﬀer from this gradual forgetting. The only dif-\nference between these conditions is that the reh-extend condition uses real\nitems to rehearse previous tasks. This is important as it identiﬁes that it is\nthe GAN which is struggling to learn the extended sequence of tasks, result-\ning in the observed forgetting. Therefore, future research improving GANs’\ncapacity and stability (vanishing gradients and mode collapse [38]) will di-\nrectly improve RePR’s ability to prevent CF in these challenging conditions.\n26\nAccepted for publication in Neurocomputing\n7.3. Limitations and Future Research\nOne limitation of rehearsal methods (including RePR) is that they be-\ncome infeasible when the number of tasks goes to inﬁnity. This is because\nthe items rehearsed in each training iteration will only cover a very small se-\nlection of previously seen tasks. However, research in sweep rehearsal [39, 8]\nshows that rehearsal methods can still be beneﬁcial when rehearsing a com-\nparably small number of items per task, suggesting that this limitation will\nonly become severe in long task sequences. Another limitation of our model\nis that it currently assumes the agent knows when a task switch occurs.\nHowever, in some use cases this might not be true and thus, a task detection\nmethod would need to be combined with RePR.\nIn these experiments we use the same task sequence for all conditions\nwhich could bias the results either positively or negatively. However, there is\nwork [40] which suggests that for a given set of tasks, the mean performance\nof the ﬁnal model is not aﬀected by the order in which those tasks are learnt.\nCurrently, our RePR model has been designed to prevent the CF that\noccurs while learning a sequence of tasks, without using real data. However,\nour implementation still stores a limited amount of real data from the current\ntask in an experience replay. This data is used to prevent the CF that occurs\nwhile learning a single task. We believe this was acceptable as our goal in\nthis research was to prevent the CF that occurs from sequential task learning\n(without using real data) and consequently, this real data was never used to\nprevent this form of CF. In future work we wish to extend our model by\nfurther modifying pseudo-rehearsal so that it can also prevent the CF that\noccurs while learning a single reinforcement task.\nIn this paper, we chose to apply our RePR model to DQNs. However, this\ncan be easily extended to other state-of-the-art deep reinforcement learning\nalgorithms, such as actor-critic networks, by adding a similar constraint to\nthe loss function. We chose DQNs for this research because their Q-values\ncontain both policy and value information whereas Actor-Critics produce\npolicy and value information separately. Although our research suggests that\nit is easier for the LTM system to retain solely the policy, the policy does not\ncontain information about the expected discounted rewards (i.e. the value)\nassociated with each state which is necessary to continue learning an already\nseen task. Future research could investigate whether the expected discounted\nrewards can be quickly relearnt with access to the retained policy, or whether\nit should also be retained by the LTM system to continue reinforcement\nlearning without disruption.\n27\nAccepted for publication in Neurocomputing\n8. Conclusion\nIn conclusion, pseudo-rehearsal can be used with deep reinforcement\nlearning methods to achieve continual learning. We have shown that our\nRePR model can be used to sequentially learn a number of complex reinforce-\nment tasks, without scaling in complexity as the number of tasks increases\nand without revisiting or storing raw data from past tasks. Pseudo-rehearsal\nhas major beneﬁts over weight constraint methods as it is less restrictive on\nthe network and this is supported by our experimental evidence. We also\nfound compelling evidence that the addition of our dual memory system is\nnecessary for continual reinforcement learning to be eﬀective. As the power\nof generative models increases, it will have a direct impact on what can be\nachieved with RePR and our goal of having an agent which can continuously\nlearn in its environment, without being challenged by CF.\nAppendix A. Details on the Atari 2600 Games\nRoad Runner is a game where the agent must outrun another character\nby moving toward the left of the screen while collecting items and avoiding\nobstacles. To achieve high performance the agent must also learn to lead\nits opponent into certain obstacles to slow it down. Boxing is a game where\nthe agent must learn to move its character around a 2D boxing ring and\nthrow punches aimed at the face of the opponent to score points, while also\navoiding taking punches to the face. James Bond has the agent learn to\ncontrol a vehicle, while avoiding obstacles and shooting various objects. In\nPong, the agent learns to hit a ball back to its opponent by moving a paddle.\nA point is scored by the agent when the opponent does not successfully\nhit the ball back and the opponent scores a point when the agent does not\nsuccessfully hit back the ball. For Atlantis, the agent learns to control three\nstationary cannons and must shoot down enemy ships moving horizontally\nacross the sky. After a ship passes 4 times it will take out one of the city’s\nbases, starting with the central cannon. The agent scores points for shooting\ndown ships and loses once all bases have been destroyed. Finally, in Qbert the\nagent learns to control a character which jumps diagonally around a pyramid\nof cubes, changing the cubes’ colours. Once all the cubes have been changed\nto a particular colour the level is cleared. In this game, the agent must also\nlearn to avoid various enemies.\n28\nAccepted for publication in Neurocomputing\nTable B.4: DQN hyper-parameters.\nHyper-parameter\nValue\nDescription\nmini-batch size\n32\nNumber of examples drawn for calculating the stochastic gradient\ndescent update.\nreplay memory size\n200,000\nNumber of frames in experience replay which samples from the\ncurrent game are drawn from.\nhistory length\n4\nNumber of recent frames given to the agent as an input sequence.\ntarget network update frequency\n5,000\nNumber of frames which are observed from the environment before\nthe target network is updated.\ndiscount factor\n0.99\nDiscount factor (γ) for each future reward.\naction repeat\n4\nNumber of times the agent’s selected action is repeated before\nanother frame is observed.\nupdate frequency\n4\nFrequency of observed frames which updates to the current network\noccur on.\nlearning rate\n0.00025\nLearning rate used by Tensorﬂow’s RMSProp optimiser.\nmomentum\n0.0\nMomentum used by Tensorﬂow’s RMSProp optimiser.\ndecay\n0.99\nDecay used by Tensorﬂow’s RMSProp optimiser.\nepsilon\n1e−6\n1e−6\n1e−6\nEpsilon used by Tensorﬂow’s RMSProp optimiser.\ninitial exploration\n1.0\nInitial ε-greedy exploration rate.\nﬁnal exploration\n0.1\nFinal ε-greedy exploration rate.\nﬁnal exploration frame\n1,000,000\nNumber of frames seen by the agent before the linear decay of the\nexploration rate reaches its ﬁnal value.\nreplay start size\n50,000\nThe number of frames which the experience replay is initially ﬁlled\nwith (using a uniform random policy).\nno-op max\n30\nMaximum number of “do nothing” actions performed at the start of\nan episode (U[1, no-op max]).\nAppendix B. Further Implementation Details\nAppendix B.1. DQN\nThe main diﬀerence between our DQN and [1] is that we used Ten-\nsorFlow’s RMSProp optimiser (without centering) with global norm gra-\ndient clipping compared to the original paper’s RMSProp optimiser which\nclipped gradients between [−1, 1]. Our network architecture remained the\nsame. However, our biases were set to 0.01 and weights were initialised with\nN(0, 0.01), where all values that were more than two standard deviations\nfrom the mean were re-drawn. The remaining changes were to the hyper-\nparameters of the learning algorithm which can be seen in bold in Table B.4.\nThe architecture of our network can be found in Table B.5, where all layers\nuse the ReLU activation function except the last linear layer.\n29\nAccepted for publication in Neurocomputing\nTable B.5: DQN architecture used in all experiments except the extended task sequence\nexperiment, where CONV is a convolutional layer and FC is a fully connected layer.\nDQN\nInput: 4 × 84 × 84\nlayer\n# units/ﬁlters\nﬁlter shape\nﬁlter stride\nCONV\n32\n8 × 8\n4 × 4\nCONV\n64\n4 × 4\n2 × 2\nCONV\n64\n3 × 3\n1 × 1\nFC\n512\nFC\n18\nAppendix B.2. GAN\nThe GAN is trained with the WGAN-GP loss function [15] with a drift\nterm [16]. The drift term is applied to the discriminator’s output for real and\nfake inputs, stopping the output from drifting too far away from zero. More\nspeciﬁcally, the loss functions used for updating the discriminator (Ldisc) and\ngenerator (Lgen) are:\nLdisc = D(ex; φ) −D(x; φ) + λ(∥∇ˆxD(ˆx; φ)∥2 −1)2\n+ϵdriftD(x; φ)2 + ϵdriftD(ex; φ)2,\n(B.1)\nLgen = −D(ex; φ),\n(B.2)\nwhere D and G are the discriminator and generator networks with the pa-\nrameters φ and ϕ. x is an input item drawn from either the current task’s\nexperience replay or the previous long-term GAN (as speciﬁed in Equation 7).\nex is an item produced by the current generative model (ex = G(z; ϕ)) and\nˆx = ϵx + (1 −ϵ)ex. ϵ is a random number ϵ ∼U(0, 1), z is an array of la-\ntent variables z = U(−1, 1), λ = 10 and ϵdrift = 1e−6. The discriminator\nand generator networks’ weights are updated on alternating steps using their\ncorresponding loss function.\nThe GAN is trained with the Adam optimiser (α = 0.001, β1 = 0.0,\nβ2 = 0.99 and ϵ = 1e−8 as per [16]) where the networks are trained with\na mini-batch size of 100. The architecture of the networks is illustrated in\nTable B.6. All layers of the discriminator use the Leaky ReLU activation\nfunction (with the leakage value set to 0.2), except the last linear layer.\nAll layers of the generator use batch normalisation (momentum = 0.9 and\nϵ = 1e−5) and the ReLU activation function, except the last layer which has\nno batch normalisation and uses the Tanh activation function. This is to\n30\nAccepted for publication in Neurocomputing\nTable B.6: GAN architecture used in all experiments except the extended task sequence\nexperiment, where FC is a fully connected layer, DECONV is a deconvolutional layer and\nCONV is a convolutional layer.\nGenerator\nDiscriminator\nInput: 100 latent variables\nInput: 4 × 84 × 84\nlayer\n# units/ﬁlters\nﬁlter shape\nﬁlter stride\nlayer\n# units/ﬁlters\nﬁlter shape\nﬁlter stride\nFC\n256 × 7 × 7\nCONV\n64\n5 × 5\n3 × 3\nDECONV\n256\n5 × 5\n3 × 3\nCONV\n128\n5 × 5\n2 × 2\nDECONV\n128\n5 × 5\n2 × 2\nCONV\n256\n5 × 5\n2 × 2\nDECONV\n64\n5 × 5\n2 × 2\nFC\n1\nDECONV\n4\n5 × 5\n1 × 1\nmake the generated images’ output space the same as the real images which\nare rescaled between −1 and 1 by applying f(x) = 2( x\n255 −0.5) to each raw\npixel value. We also decreased the convergence time of our GAN by applying\nrandom noise U(−10, 10) to real and generated images before rescaling and\ngiving them to the discriminator.\nAppendix B.3. EWC\nThe EWC constraint is implemented as per [3], where the loss function\nis amended so that:\nLLTM EWC = 1\nN\nN\nX\nj=1\nLDj + λ\n2LEWC,\n(B.3)\nLEWC =\nX\ni\nFi(θi −θ∗\ni )2,\n(B.4)\nwhere LD is the distillation loss for learning the current task (as speciﬁed in\nEquation 5) and N is the batch-size. λ is a scaling factor determining how\nmuch importance the constraint should be given, θ is the current long-term\nnetwork’s parameters, θ∗is the ﬁnal long-term network’s parameters after\nlearning the previous task and i iterates over each of the parameters in the\nnetwork. Fi is an approximation of the diagonal elements in a Fisher informa-\ntion matrix, where each element represents the importance each parameter\nhas on the output of the network.\nThe Fisher information matrix is calculated as in [3], by approximating\nthe posterior as a Gaussian distribution with the mean given by the optimal\n31\nAccepted for publication in Neurocomputing\nparameters after learning a previous task θ∗\ni and a standard deviation β = 1.\nMore speciﬁcally, the calculation follows [41]:\nF = β2Es∼U(D)[J T\ny J y],\n(B.5)\nwhere an expectation is calculated by uniformly drawing states from the\nexperience replay (s ∼U(D)). J y is the Jacobian matrix ∂y\n∂θ for the output\nlayer y.\nWhen the standard EWC implementation is extended to a third task, a\nseparate penalty is added. This means the current parameters of the network\nare constrained to be similar to both the parameters after learning the ﬁrst\ntask and the parameters after further learning the second task.\nOnline-EWC further extends EWC so that only the previous network’s\nparameters and a single Fisher information matrix is stored. As per [21], this\nresults in the LEWC constraint being replaced by:\nLOEWC =\nX\ni\nF ∗\ni (θi −θ∗\ni,t−1)2,\n(B.6)\nwhere the single Fisher information matrix F ∗is updated by:\nF ∗= γF ∗\nt−1 + Ft,\n(B.7)\nwhere γ < 1 is a discount parameter and t represents the index of the current\ntask. In online-EWC, Fisher information matrices are normalised using min-\nmax normalisation so that the tasks’ diﬀerent reward scales do not aﬀect the\nrelative importance of parameters between tasks.\nFor the ewc condition, we applied a grid search over λ = [50, 100, 150, 200,\n250, 300, 350, 400] and for our online-ewc condition we performed a grid\nsearch over λ = [25, 75, 125, 175] and γ = [0.95, 0.99]. The best parameters\nfound during the grid searches are in bold.\nIn all conditions, the Fisher\ninformation matrix is calculated by sampling 100 batches from each task.\nThe ﬁnal network’s test scores for each of the tasks were min-max normalised\nand the network with the best average score was selected. The minimum and\nmaximum is found across all testing episodes played during the learning of\nthe task in the STM system.\nAn additional experiment was run to conﬁrm our EWC variants could suc-\ncessfully retain previous task knowledge under a diﬀerent training scheme.\nIn these conditions, the LTM system retained the agent’s policy (taught by\n32\nAccepted for publication in Neurocomputing\nminimising the cross-entropy) and new tasks were learnt in the LTM sys-\ntem for 5m frames each. Fig. B.6 displays results for the EWC, online-EWC\nand RePR implementations tested under these conditions (ewc-policy-short,\nonline-ewc-policy-short and RePR-policy-short respectively).\nAll condi-\ntions performed similarly and could successfully learn new tasks while re-\ntaining knowledge of previous tasks.\nAppendix B.4. Transferring and Retaining the Policy in the LTM System\nFirstly, the policy is extracted from the short-term DQN by giving sam-\nples from the current task to the DQN and calculating the Q-values it has\nlearnt to associate with those samples. For each sample, the action with\nthe largest Q-value is then one-hot encoded. After this policy has been ex-\ntracted, distillation is used to transfer it to the long-term agent, using the\nloss function:\nLD policyj = CE(π(sj; θi), h(Q(sj; ψi))),\n(B.8)\nwhere sj is a state drawn from the current task’s experience replay. π is\nthe long-term agent which has a softmax output layer and Q is the short-\nterm DQN agent which has a linear output layer. θi is the long-term agent’s\nweights on the current task and ψi is the short-term agent’s weights after\nlearning the current task. CE is the standard cross-entropy loss function\nand h(q) is a function that one-hot encodes q.\nFor EWC variants, the long-term agent retains the policy through the\nsame methods it retains Q-values (i.e. using a weight constraint). However,\nin rehearsal based methods (including RePR) it is necessary to adapt the re-\ntention loss function to use cross-entropy. More speciﬁcally, the loss function\nfor RePR is changed to:\nLPR policyj = CE(π(esj; θi), π(esj; θi−1)),\n(B.9)\nwhere pseudo-items’ inputs esj are generated from a GAN and are representa-\ntive of states in previously learnt games. θi−1 is the long-term agent’s weights\nafter learning the previous task.\nAppendix B.5. Extended Task Sequence Experiment\nThe experiment investigating RePR with an extended task sequence is\nnearly identical to the ﬁrst experiment’s details from the main text. One of\nthe main diﬀerences is that learning is slowed down by initially setting α to\n0.05 and after learning the fourth task reducing this to 0.01. Consequently,\n33\nAccepted for publication in Neurocomputing\n0\n2\n4\n6\n8\n10\n12\n14\nNumber of training frames seen across all games (millions)\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\nTest score\nRoad Runner test scores during training\nshared\newc-policy-short\nonline-ewc-policy-short\nRePR-policy-short\n0\n2\n4\n6\n8\n10\n12\n14\nNumber of training frames seen across all games (millions)\n100\n50\n0\n50\n100\nTest score\nBoxing test scores during training\newc-policy-short\nonline-ewc-policy-short\nRePR-policy-short\n0\n2\n4\n6\n8\n10\n12\n14\nNumber of training frames seen across all games (millions)\n0\n100\n200\n300\n400\n500\n600\n700\nTest score\nJames Bond test scores during training\newc-policy-short\nonline-ewc-policy-short\nRePR-policy-short\nFigure B.6: Results of our EWC, online-EWC and RePR implementations tested under a\ndiﬀerent training scheme. After every 1 million observable training frames, the long-term\nagent is evaluated on the current task and all previously learnt tasks. Task switches occur\nat the dashed lines, in the order Road Runner, Boxing and then James Bond. Results\nwere produced using a single seed.\n34\nAccepted for publication in Neurocomputing\ntraining time is increased to 40m frames for the long-term DQN. Secondly,\nboth the DQN and generative networks are enlarged by doubling the number\nof ﬁlters and units in hidden layers.\nMore speciﬁcally, the convolutional\nlayers in the DQN use 64, 128 and 128 ﬁlters respectively and then the fully\nconnected layer (before the output layer) uses 1024 units. For the GAN, the\ngenerative network is enlarged by increasing the ﬁrst fully connected layer to\n512 × 7 × 7 units and increasing the following three deconvolutional layers so\nthat they use 512, 256 and 128 ﬁlters respectively. The training time for the\nGAN is also increased to 400,000 steps.\nThe reh-extend and RePR-extend conditions learn the extended se-\nquence of tasks with either rehearsal or RePR. In these conditions we stan-\ndardise the short-term DQN’s Q-values when they are being taught to the\nlong-term DQN. This was beneﬁcial because it reduced the interference be-\ntween the games’ substantially diﬀerent reward functions and thus, evened\nout the importance of retaining each of the games. In the reh-extend-policy\nand RePR-extend-policy conditions only the policy was retained in the LTM\nsystem. This was achieved by using cross-entropy to learn and retain each\ngame’s policy, as described above.\nAppendix C. How well does RePR Share Weights?\nTo investigate whether an agent’s DQN uses similar parameters for deter-\nmining its output across multiple tasks, [3] suggest that the degree of overlap\nbetween two tasks’ Fisher information matrices can be analysed. This Fisher\noverlap score is bounded between 0 and 1, where a high score represents high\noverlap and indicates that many of the weights that are important for cal-\nculating the desired action in one task are also important in the other task.\nMore speciﬁcally, the Fisher overlap is calculated by 1 −d2, where:\nd2( ˆF1, ˆF2) = 1\n2tr\n\u0010\nˆF1 + ˆF2 −2( ˆF1 ˆF2)\n1\n2\n\u0011\n,\n(C.1)\ngiven ˆF1 and ˆF2 are the two tasks’ Fisher information matrices which have\nbeen normalised so that they each have a unit trace. Fisher information\nmatrices are approximated by Equation B.5 using 100 batches of samples\ndrawn from each tasks’ experience replay.\nWe compared RePR’s Fisher information matrices for each task using\nthe Fisher overlap calculation. When RePR had learnt the tasks in the order\nRoad Runner, Boxing and then James Bond (as in the RePR condition from\n35\nAccepted for publication in Neurocomputing\nTable C.7: Fisher overlap scores between task pairs.\nCondition\nRoad Runner & Boxing\nRoad Runner & James Bond\nBoxing & James Bond\nRePR\n0.691\n0.233\n0.198\nRePR-rev\n0.753\n0.192\n0.110\nSection 6.1), the Fisher overlap score was high between the ﬁrst two tasks\nlearnt but relatively low between other task pairs. This suggests that there\nare more similarities between Road Runner and Boxing than other task pairs.\nWe conﬁrm this by calculating the Fisher overlap for each of the task pairs\nwhen the RePR model had successfully learnt the tasks in the reverse order\n(ie. James Bond, Boxing and then Road Runner). In this case, a higher\noverlap value remains between Road Runner and Boxing, regardless of the\norder they were learnt in. This demonstrates that the network attempts to\nshare the computation across a similar set of important weights, where the\nmore similar the tasks are the more eﬀective they are at sharing weights.\nThe precise Fisher overlap values for both of these conditions can be found\nin Table C.7.\nAppendix D. Evaluating the Importance of Dual Memory\nTo further evaluate the importance of the dual memory system, the PR\nand RePR conditions from the main text are trained with varying importance\nvalues (α) on the sequence Road Runner, then Boxing. The PR condition\ndoes not use a dual memory system and thus, its α value weights the impor-\ntance of learning new tasks with deep Q-learning (Equation 1) vs. retaining\nprevious tasks with pseudo-rehearsal (Equation 6).\nThe RePR condition\nhas a dual memory system and thus, its α value weights the importance of\nlearning new tasks with distillation (Equation 5) vs. retaining previous tasks\nwith pseudo-rehearsal (Equation 6). The aim of this experiment was to in-\nvestigate the extent to which omitting the dual memory system increases the\ninterference between new knowledge and knowledge of old tasks. This was\ninvestigated by ﬁnding the α value the PR condition needed to learn Boxing\nto the same standard as RePR (i.e. to an approximate score of 80) and what\neﬀect these varying α values had on the two models’ retention.\nThe results in Table D.8 show that without a dual memory system, α\nmust be extremely high (α = 0.95) for the model to successfully learn Box-\ning to a similar standard as RePR. However, both models show that high α\n36\nAccepted for publication in Neurocomputing\nTable D.8: Final long-term network scores (and standard deviations) for the RePR and\nPR models after learning Road Runner and Boxing with varying importance values (α).\nResults are collected using a single consistent seed with the average scores and standard\ndeviations calculated by testing the ﬁnal network on 30 episodes.\nPR\nRePR\nα\nRoad Runner\nBoxing\nRoad Runner\nBoxing\n0.65\n30700 (±6463)\n32 (±17)\n28543 (±4699)\n84 (±9)\n0.75\n30630 (±5420)\n32 (±21)\n23603 (±6124)\n85 (±8)\n0.85\n26310 (±8257)\n46 (±14)\n13977 (±8814)\n85 (±10)\n0.95\n12090 (±5894)\n80 (±10)\n7617 (±5696)\n83 (±10)\nvalues result in considerable forgetting of Road Runner. Importantly, this\nmeans that without a dual memory system there does not exist an α value\nthat results in both successful learning of the new task and acceptable reten-\ntion of the previous task. When using RePR’s dual memory system, learning\nthe new task is considerably easier and thus, comparably lower α values will\nresult in both successfully learning the new task and retaining the previous\ntask. Overall, these results suggest that omitting the dual memory system\ndramatically increases the interference between new knowledge and knowl-\nedge of previous tasks.\nAcknowledgment\nWe gratefully acknowledge the support of NVIDIA Corporation with the\ndonation of the TITAN X GPU used for this research. We also wish to ac-\nknowledge the use of New Zealand eScience Infrastructure (NeSI) high per-\nformance computing facilities. New Zealand’s national facilities are provided\nby NeSI and funded jointly by NeSI’s collaborator institutions and through\nthe Ministry of Business, Innovation & Employment’s Research Infrastruc-\nture programme. URL https://www.nesi.org.nz.\nDeclaration of Competing Interest\nThe authors declare that they have no known competing ﬁnancial inter-\nests or personal relationships that could have appeared to inﬂuence the work\nreported in this paper.\n37\nAccepted for publication in Neurocomputing\ndef train stm(env, exp):\nstm agent = init stm agent()\nexp.clear()\nwhile training stm agent do\nexp.add(env.step(stm agent.pred net.calc action()))\nat, rt, dt, st, st+1 = exp.sample()\nif dt then\nyt = rt\nelseyt =\nrt + γ max\nat+1 stm agent.target net.get outputs(st+1)[at+1]\nloss = (yt −stm agent.pred net.get outputs(st)[at])2\nstm agent.pred net.SGD step(loss)\nif iter divisible by update target freq then\nstm agent.target net = copy(stm agent.pred net)\nreturn stm agent\nAlgorithm 1: Simpliﬁed pseudo-code demonstrating the training proce-\ndure of the short-term agent in RePR. This pseudo-code implements the\nloss function deﬁned in Equation 1. The stm agent contains a predictor\nnetwork and a target network; env is the environment currently being\nlearnt; exp is an experience replay; iter is the current training iteration;\nupdate target freq is the frequency at which the target network is up-\ndated. In practice, the loss function is calculated by averaging over a\nbatch of samples.\n38\nAccepted for publication in Neurocomputing\ndef train ltm(env, exp, stm agent, ltm agent, gan):\nexp.clear()\nprev ltm agent = copy(ltm agent)\nwhile training ltm agent do\nexp.add(env.step(ltm agent.pred net.calc action()))\nat, rt, dt, st, st+1 = exp.sample()\nz = uniform sample(−1, 1, n latents)\nes = gan.gen.get outputs(z)\nLD = sum((ltm agent.pred net.get outputs(s) −\nstm agent.pred net.get outputs(s))2)\nLPR = sum((ltm agent.pred net.get outputs(es) −\nprev ltm agent.pred net.get outputs(es))2)\nloss = αLD + (1 −α)LPR\nltm agent.pred net.SGD step(loss)\nreturn ltm agent\nAlgorithm 2: Simpliﬁed pseudo-code demonstrating the training proce-\ndure of the long-term agent in RePR. This pseudo-code implements the\nloss function deﬁned in Equation 4. The stm agent contains a predictor\nnetwork; the ltm agent contains a predictor network; the gan contains a\ngenerator network; env is the environment currently being learnt; exp is\nan experience replay; α weights the importance of learning the new task\nvs. retaining previously learnt tasks; n latents is the number of latent\ninput variables used by the GAN. In practice, the loss function is calcu-\nlated by averaging over a batch of samples and pseudo-items are sampled\nfrom a large array of items generated by the GAN before training the\nagent.\n39\nAccepted for publication in Neurocomputing\ndef train gan(exp, gan):\nnew gan = initialise gan()\nwhile training new gan do\nif iter is even then\nif True with probability of 1/T then\nat, rt, dt, st, st+1 = exp.sample()\nx = st\nelse\nz = uniform sample(−1, 1, n latents)\nes = gan.gen.get outputs(z)\nx = es\nz = uniform sample(−1, 1, n latents)\nϵ = uniform sample(0, 1)\nex = new gan.gen.get outputs(z)\nˆx = ϵx + (1 −ϵ)ex\ndisc real = new gan.disc.get outputs(x)\ndisc fake = new gan.disc.get outputs(ex)\ndisc xhat = new gan.disc.get outputs(ˆx)\ngradient penalty = λ(∥grads(disc xhat, ˆx)∥2 −1)2\nloss = disc fake −disc real + gradient penalty +\nϵdriftdisc real2 + ϵdriftdisc fake2\nnew gan.disc.SGD step(loss)\nelse\nz = uniform sample(−1, 1, n latents)\nex = new gan.gen.get outputs(z)\nloss = −new gan.disc.get outputs(ex)\nnew gan.gen.SGD step(loss)\nreturn new gan\nAlgorithm 3: Simpliﬁed pseudo-code demonstrating the training pro-\ncedure of the long-term GAN in RePR. This pseudo-code implements\nthe training data selection procedure deﬁned in Equation 7 and the loss\nfunctions deﬁned in Equation B.1 and Equation B.2. The gan contains\na generator network; the new gan contains both a discriminator network\nand a generator network; exp is an experience replay containing samples\nfrom the current environment; iter is the current training iteration; T is\nthe number of tasks seen; n latents is the number of latent input vari-\nables used by the GAN. In practice, the loss functions are calculated by\naveraging over a batch of samples and pseudo-items are sampled from a\nlarge array of items generated by the previous GAN before training the\nnew GAN.\n40\nAccepted for publication in Neurocomputing\nReferences\n[1] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,\net al., Human-level control through deep reinforcement learning, Nature\n518 (7540) (2015) 529–533.\n[2] M. McCloskey, N. J. Cohen, Catastrophic interference in connectionist\nnetworks: The sequential learning problem, in: Psychology of Learning\nand Motivation, Vol. 24, Elsevier, 1989, pp. 109–165.\n[3] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins,\nA. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska,\net al., Overcoming catastrophic forgetting in neural networks, Proceed-\nings of the National Academy of Sciences 114 (13) (2017) 3521–3526.\n[4] R. S. Sutton, A. G. Barto, Reinforcement learning: An introduction\n(2nd edition), complete draft (2017).\n[5] D. Lopez-Paz, M. Ranzato, Gradient episodic memory for contin-\nual learning, in: Advances in Neural Information Processing Systems,\nVol. 30, Curran Associates, Inc., 2017, pp. 6467–6476.\n[6] S.-A. Rebuﬃ, A. Kolesnikov, G. Sperl, C. H. Lampert, iCaRL: Incre-\nmental classiﬁer and representation learning, in: IEEE Conference on\nComputer Vision and Pattern Recognition, 2017, pp. 5533–5542.\n[7] M. Riemer, M. Franceschini, D. Bouneﬀouf, Generative knowledge dis-\ntillation for general purpose function compression, in: Neural Informa-\ntion Processing Systems Workshop on Teaching Machines, Robots, and\nHumans, 2017.\n[8] A. Robins, Catastrophic forgetting, rehearsal and pseudorehearsal, Con-\nnection Science 7 (2) (1995) 123–146.\n[9] S. Gais, G. Albouy, M. Boly, T. T. Dang-Vu, A. Darsaud, M. Desseilles,\nG. Rauchs, M. Schabus, V. Sterpenich, G. Vandewalle, et al., Sleep\ntransforms the cerebral trace of declarative memories, Proceedings of\nthe National Academy of Sciences 104 (47) (2007) 18778–18783.\n41\nAccepted for publication in Neurocomputing\n[10] K. Louie, M. A. Wilson, Temporally structured replay of awake hip-\npocampal ensemble activity during rapid eye movement sleep, Neuron\n29 (1) (2001) 145–156.\n[11] C. Atkinson, B. McCane, L. Szymanski, A. Robins, Pseudo-recursal:\nSolving the catastrophic forgetting problem in deep neural networks,\narXiv e-prints (Unpublished results). arXiv:1802.03875.\n[12] H. Shin, J. K. Lee, J. Kim, J. Kim, Continual learning with deep gen-\nerative replay, in: Advances in Neural Information Processing Systems,\nVol. 30, Curran Associates, Inc., 2017, pp. 2990–2999.\n[13] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, Y. Bengio, Generative adversarial nets, in: Ad-\nvances in Neural Information Processing Systems, Vol. 27, Curran As-\nsociates, Inc., 2014, pp. 2672–2680.\n[14] G. Hinton, O. Vinyals, J. Dean, Distilling the knowledge in a neural\nnetwork, arXiv e-prints (Unpublished results). arXiv:1503.02531.\n[15] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, A. C. Courville,\nImproved training of Wasserstein GANs, in: Advances in Neural Infor-\nmation Processing Systems, Vol. 30, Curran Associates, Inc., 2017, pp.\n5767–5777.\n[16] T. Karras, T. Aila, S. Laine, J. Lehtinen, Progressive growing of GANs\nfor improved quality, stability, and variation, in: International Confer-\nence on Learning Representations, 2018.\n[17] G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, S. Wermter, Continual\nlifelong learning with neural networks: A review, Neural Networks 113\n(2019) 54–71.\n[18] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick,\nK. Kavukcuoglu, R. Pascanu, R. Hadsell, Progressive neural networks,\narXiv e-prints (Unpublished results). arXiv:1606.04671.\n[19] T. Kobayashi, Check regularization: Combining modularity and elastic-\nity for memory consolidation, in: Artiﬁcial Neural Networks and Ma-\nchine Learning, Springer International Publishing, 2018, pp. 315–325.\n42\nAccepted for publication in Neurocomputing\n[20] C. Kaplanis, M. Shanahan, C. Clopath, Continual reinforcement learn-\ning with complex synapses, in: Proceedings of the 35th International\nConference on Machine Learning, Vol. 80 of Proceedings of Machine\nLearning Research, PMLR, 2018, pp. 2497–2506.\n[21] J. Schwarz, J. Luketina, W. M. Czarnecki, A. Grabska-Barwinska, Y. W.\nTeh, R. Pascanu, R. Hadsell, Progress & compress: A scalable frame-\nwork for continual learning, in: Proceedings of the 35th International\nConference on Machine Learning, Vol. 80 of Proceedings of Machine\nLearning Research, PMLR, 2018, pp. 4528–4537.\n[22] G. Berseth, C. Xie, P. Cernek, M. Van de Panne, Progressive reinforce-\nment learning with distillation for multi-skilled motion control, in: In-\nternational Conference on Learning Representations, 2018.\n[23] A. A. Rusu, S. G. Colmenarejo, C. Gulcehre, G. Desjardins, J. Kirk-\npatrick, R. Pascanu, V. Mnih, K. Kavukcuoglu, R. Hadsell, Policy distil-\nlation, in: International Conference on Learning Representations, 2016.\n[24] E. Parisotto, J. L. Ba, R. Salakhutdinov, Actor-mimic: Deep multitask\nand transfer reinforcement learning, in: International Conference on\nLearning Representations, 2016.\n[25] D. Rolnick, A. Ahuja, J. Schwarz, T. P. Lillicrap, G. Wayne, Experi-\nence replay for continual learning, in: Advances in Neural Information\nProcessing Systems, Vol. 32, Curran Associates, Inc., 2019, pp. 350–360.\n[26] D. Isele, A. Cosgun, Selective experience replay for lifelong learning, in:\nAAAI Conference on Artiﬁcial Intelligence, 2018.\n[27] M. Riemer, I. Cases, R. Ajemian, M. Liu, I. Rish, Y. Tu, G. Tesauro,\nLearning to learn without forgetting by maximizing transfer and mini-\nmizing interference, in: International Conference on Learning Represen-\ntations, 2019.\n[28] H. Caselles-Dupr´e, M. Garcia-Ortiz, D. Filliat, Continual state represen-\ntation learning for reinforcement learning using generative replay, arXiv\ne-prints (Unpublished results). arXiv:1810.03880.\n43\nAccepted for publication in Neurocomputing\n[29] V. Marochko, L. Johard, M. Mazzara, Pseudorehearsal in value function\napproximation, in: Agent and Multi-Agent Systems: Technology and\nApplications, Springer International Publishing, 2017, pp. 178–189.\n[30] B. Baddeley, Reinforcement learning in continuous time and space: In-\nterference and not ill conditioning is the main problem when using dis-\ntributed function approximators, IEEE Transactions on Systems, Man,\nand Cybernetics, Part B (Cybernetics) 38 (4) (2008) 950–956.\n[31] V. Marochko, L. Johard, M. Mazzara, L. Longo, Pseudorehearsal in\nactor-critic agents with neural network function approximation, in:\nIEEE 32nd International Conference on Advanced Information Network-\ning and Applications, 2018, pp. 644–650.\n[32] H. Caselles-Dupr´e, M. Garcia-Ortiz, D. Filliat, S-TRIGGER: Continual\nstate representation learning via self-triggered generative replay, arXiv\ne-prints (Unpublished results). arXiv:1902.09434.\n[33] N. Ketz, S. Kolouri, P. Pilly, Continual learning using world models for\npseudo-rehearsal, arXiv e-prints (Unpublished results).\narXiv:1903.\n02647.\n[34] N. Kamra, U. Gupta, Y. Liu, Deep generative dual memory network\nfor continual learning, arXiv e-prints (Unpublished results).\narXiv:\n1710.10368.\n[35] W. C. Abraham, A. Robins, Memory retention - the synaptic stability\nversus plasticity dilemma, Trends in Neurosciences 28 (2) (2005) 73–78.\n[36] C. R. Gallistel, L. D. Matzel, The neuroscience of learning: Beyond the\nhebbian synapse, Annual Review of Psychology 64 (1) (2013) 169–200.\n[37] W. C. Abraham, O. D. Jones, D. L. Glanzman, Is plasticity of synapses\nthe mechanism of long-term memory storage?, npj Science of Learning\n4 (1) (2019).\n[38] J. Li, A. Madry, J. Peebles, L. Schmidt, On the limitations of ﬁrst-\norder approximation in GAN dynamics, in: Proceedings of the 35th\nInternational Conference on Machine Learning, Vol. 80 of Proceedings\nof Machine Learning Research, PMLR, 2018, pp. 3005–3013.\n44\nAccepted for publication in Neurocomputing\n[39] D. L. Silver, G. Mason, L. Eljabu, Consolidation using sweep task re-\nhearsal: Overcoming the stability-plasticity problem, in: Advances in\nArtiﬁcial Intelligence, Springer International Publishing, 2015, pp. 307–\n322.\n[40] R. Poirier, D. L. Silver, Eﬀect of curriculum on the consolidation of\nneural network task knowledge, in: IEEE International Joint Conference\non Neural Networks, Vol. 4, 2005, pp. 2123–2128.\n[41] R. Pascanu, Y. Bengio, Revisiting natural gradient for deep networks,\nin: International Conference on Learning Representations, 2014.\nCraig Atkinson received his B.Sc.\n(Hons.)\nfrom\nthe University of Otago, Dunedin, New Zealand, in 2017.\nHe has just completed his doctorate in Computer Sci-\nence at the University of Otago.\nHis research interests\ninclude deep reinforcement learning and continual learn-\ning.\nBrendan McCane received the B.Sc.\n(Hons.)\nand\nPh.D. degrees from the James Cook University of North\nQueensland, Townsville City, Australia, in 1991 and 1996,\nrespectively. He joined the Computer Science Department,\nUniversity of Otago, Otago, New Zealand, in 1997. He served\nas the Head of the Department from 2007 to 2012.\nHis\ncurrent research interests include computer vision, pattern\nrecognition, machine learning, and medical and biological\nimaging. He also enjoys reading, swimming, ﬁshing and long\nwalks on the beach with his dogs.\nLech Szymanski received the B.A.Sc. (Hons.) degree\nin computer engineering and the M.A.Sc.\ndegree in elec-\ntrical engineering from the University of Ottawa, Ottawa,\nON, Canada, in 2001 and 2005, respectively, and the Ph.D.\ndegree in computer science from the University of Otago,\nOtago, New Zealand, in 2012.\nHe is currently a Lecturer\n45\nAccepted for publication in Neurocomputing\nat the Computer Science Department at the University of\nOtago. His research interests include machine learning, arti-\nﬁcial neural networks, and deep architectures.\nAnthony Robins completed his doctorate in cognitive\nscience at the University of Sussex (UK) in 1989. He is cur-\nrently a Professor of Computer Science at the University of\nOtago, New Zealand. His research interests include artiﬁ-\ncial neural networks, computational models of memory, and\ncomputer science education.\n46\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CV"
  ],
  "published": "2018-12-06",
  "updated": "2020-12-16"
}