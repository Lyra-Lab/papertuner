{
  "id": "http://arxiv.org/abs/1812.02900v3",
  "title": "Off-Policy Deep Reinforcement Learning without Exploration",
  "authors": [
    "Scott Fujimoto",
    "David Meger",
    "Doina Precup"
  ],
  "abstract": "Many practical applications of reinforcement learning constrain agents to\nlearn from a fixed batch of data which has already been gathered, without\noffering further possibility for data collection. In this paper, we demonstrate\nthat due to errors introduced by extrapolation, standard off-policy deep\nreinforcement learning algorithms, such as DQN and DDPG, are incapable of\nlearning with data uncorrelated to the distribution under the current policy,\nmaking them ineffective for this fixed batch setting. We introduce a novel\nclass of off-policy algorithms, batch-constrained reinforcement learning, which\nrestricts the action space in order to force the agent towards behaving close\nto on-policy with respect to a subset of the given data. We present the first\ncontinuous control deep reinforcement learning algorithm which can learn\neffectively from arbitrary, fixed batch data, and empirically demonstrate the\nquality of its behavior in several tasks.",
  "text": "Off-Policy Deep Reinforcement Learning without Exploration\nScott Fujimoto 1 2 David Meger 1 2 Doina Precup 1 2\nAbstract\nMany practical applications of reinforcement\nlearning constrain agents to learn from a ﬁxed\nbatch of data which has already been gathered,\nwithout offering further possibility for data col-\nlection. In this paper, we demonstrate that due to\nerrors introduced by extrapolation, standard off-\npolicy deep reinforcement learning algorithms,\nsuch as DQN and DDPG, are incapable of learn-\ning without data correlated to the distribution un-\nder the current policy, making them ineffective\nfor this ﬁxed batch setting. We introduce a novel\nclass of off-policy algorithms, batch-constrained\nreinforcement learning, which restricts the action\nspace in order to force the agent towards behaving\nclose to on-policy with respect to a subset of the\ngiven data. We present the ﬁrst continuous con-\ntrol deep reinforcement learning algorithm which\ncan learn effectively from arbitrary, ﬁxed batch\ndata, and empirically demonstrate the quality of\nits behavior in several tasks.\n1. Introduction\nBatch reinforcement learning, the task of learning from a\nﬁxed dataset without further interactions with the environ-\nment, is a crucial requirement for scaling reinforcement\nlearning to tasks where the data collection procedure is\ncostly, risky, or time-consuming. Off-policy batch reinforce-\nment learning has important implications for many practical\napplications. It is often preferable for data collection to\nbe performed by some secondary controlled process, such\nas a human operator or a carefully monitored program. If\nassumptions on the quality of the behavioral policy can\nbe made, imitation learning can be used to produce strong\npolicies. However, most imitation learning algorithms are\nknown to fail when exposed to suboptimal trajectories, or\n1Department of Computer Science, McGill University, Mon-\ntreal, Canada 2Mila Qu´ebec AI Institute. Correspondence to: Scott\nFujimoto <scott.fujimoto@mail.mcgill.ca>.\nProceedings of the 36 th International Conference on Machine\nLearning, Long Beach, California, PMLR 97, 2019. Copyright\n2019 by the author(s).\nrequire further interactions with the environment to com-\npensate (Hester et al., 2017; Sun et al., 2018; Cheng et al.,\n2018). On the other hand, batch reinforcement learning of-\nfers a mechanism for learning from a ﬁxed dataset without\nrestrictions on the quality of the data.\nMost modern off-policy deep reinforcement learning al-\ngorithms fall into the category of growing batch learn-\ning (Lange et al., 2012), in which data is collected and\nstored into an experience replay dataset (Lin, 1992), which\nis used to train the agent before further data collection oc-\ncurs. However, we ﬁnd that these “off-policy” algorithms\ncan fail in the batch setting, becoming unsuccessful if the\ndataset is uncorrelated to the true distribution under the cur-\nrent policy. Our most surprising result shows that off-policy\nagents perform dramatically worse than the behavioral agent\nwhen trained with the same algorithm on the same dataset.\nThis inability to learn truly off-policy is due to a funda-\nmental problem with off-policy reinforcement learning we\ndenote extrapolation error, a phenomenon in which unseen\nstate-action pairs are erroneously estimated to have unre-\nalistic values. Extrapolation error can be attributed to a\nmismatch in the distribution of data induced by the policy\nand the distribution of data contained in the batch. As a\nresult, it may be impossible to learn a value function for a\npolicy which selects actions not contained in the batch.\nTo overcome extrapolation error in off-policy learning, we\nintroduce batch-constrained reinforcement learning, where\nagents are trained to maximize reward while minimizing\nthe mismatch between the state-action visitation of the pol-\nicy and the state-action pairs contained in the batch. Our\ndeep reinforcement learning algorithm, Batch-Constrained\ndeep Q-learning (BCQ), uses a state-conditioned generative\nmodel to produce only previously seen actions. This gen-\nerative model is combined with a Q-network, to select the\nhighest valued action which is similar to the data in the batch.\nUnder mild assumptions, we prove this batch-constrained\nparadigm is necessary for unbiased value estimation from\nincomplete datasets for ﬁnite deterministic MDPs.\nUnlike any previous continuous control deep reinforcement\nlearning algorithms, BCQ is able to learn successfully with-\nout interacting with the environment by considering ex-\ntrapolation error. Our algorithm is evaluated on a series\nof batch reinforcement learning tasks in MuJoCo environ-\narXiv:1812.02900v3  [cs.LG]  10 Aug 2019\nOff-Policy Deep Reinforcement Learning without Exploration\nments (Todorov et al., 2012; Brockman et al., 2016), where\nextrapolation error is particularly problematic due to the\nhigh-dimensional continuous action space, which is impos-\nsible to sample exhaustively. Our algorithm offers a uniﬁed\nview on imitation and off-policy learning, and is capable\nof learning from purely expert demonstrations, as well as\nfrom ﬁnite batches of suboptimal data, without further ex-\nploration. We remark that BCQ is only one way to approach\nbatch-constrained reinforcement learning in a deep setting,\nand we hope that it will be serve as a foundation for future\nalgorithms. To ensure reproducibility, we provide precise ex-\nperimental and implementation details, and our code is made\navailable (https://github.com/sfujim/BCQ).\n2. Background\nIn reinforcement learning, an agent interacts with its envi-\nronment, typically assumed to be a Markov decision pro-\ncess (MDP) (S, A, pM, r, γ), with state space S, action\nspace A, and transition dynamics pM(s′|s, a). At each dis-\ncrete time step, the agent receives a reward r(s, a, s′) ∈R\nfor performing action a in state s and arriving at the state\ns′. The goal of the agent is to maximize the expectation\nof the sum of discounted rewards, known as the return\nRt = P∞\ni=t+1 γir(si, ai, si+1), which weighs future re-\nwards with respect to the discount factor γ ∈[0, 1).\nThe agent selects actions with respect to a policy π : S →A,\nwhich exhibits a distribution µπ(s) over the states s ∈S\nvisited by the policy. Each policy π has a corresponding\nvalue function Qπ(s, a) = Eπ[Rt|s, a], the expected return\nwhen following the policy after taking action a in state s.\nFor a given policy π, the value function can be computed\nthrough the Bellman operator T π:\nT πQ(s, a) = Es′[r + γQ(s′, π(s′))].\n(1)\nThe Bellman operator is a contraction for γ ∈[0, 1) with\nunique ﬁxed point Qπ(s, a) (Bertsekas & Tsitsiklis, 1996).\nQ∗(s, a) = maxπ Qπ(s, a) is known as the optimal value\nfunction, which has a corresponding optimal policy obtained\nthrough greedy action choices. For large or continuous\nstate and action spaces, the value can be approximated with\nneural networks, e.g. using the Deep Q-Network algorithm\n(DQN) (Mnih et al., 2015). In DQN, the value function Qθ\nis updated using the target:\nr + γQθ′(s′, π(s′)),\nπ(s′) = argmaxa Qθ′(s′, a), (2)\nQ-learning is an off-policy algorithm (Sutton & Barto, 1998),\nmeaning the target can be computed without consideration\nof how the experience was generated. In principle, off-\npolicy reinforcement learning algorithms are able to learn\nfrom data collected by any behavioral policy. Typically, the\nloss is minimized over mini-batches of tuples of the agent’s\npast data, (s, a, r, s′) ∈B, sampled from an experience\nreplay dataset B (Lin, 1992). For shorthand, we often write\ns ∈B if there exists a transition tuple containing s in the\nbatch B, and similarly for (s, a) or (s, a, s′) ∈B. In batch\nreinforcement learning, we assume B is ﬁxed and no further\ninteraction with the environment occurs. To further stabilize\nlearning, a target network with frozen parameters Qθ′, is\nused in the learning target. The parameters of the target\nnetwork θ′ are updated to the current network parameters\nθ after a ﬁxed number of time steps, or by averaging θ′ ←\nτθ + (1 −τ)θ′ for some small τ (Lillicrap et al., 2015).\nIn a continuous action space, the analytic maximum of\nEquation (2) is intractable. In this case, actor-critic methods\nare commonly used, where action selection is performed\nthrough a separate policy network πφ, known as the actor,\nand updated with respect to a value estimate, known as the\ncritic (Sutton & Barto, 1998; Konda & Tsitsiklis, 2003).\nThis policy can be updated following the deterministic pol-\nicy gradient theorem (Silver et al., 2014):\nφ ←argmaxφ Es∈B[Qθ(s, πφ(s))],\n(3)\nwhich corresponds to learning an approximation to the max-\nimum of Qθ, by propagating the gradient through both πφ\nand Qθ. When combined with off-policy deep Q-learning to\nlearn Qθ, this algorithm is referred to as Deep Deterministic\nPolicy Gradients (DDPG) (Lillicrap et al., 2015).\n3. Extrapolation Error\nExtrapolation error is an error in off-policy value learning\nwhich is introduced by the mismatch between the dataset\nand true state-action visitation of the current policy. The\nvalue estimate Q(s, a) is affected by extrapolation error\nduring a value update where the target policy selects an\nunfamiliar action a′ at the next state s′ in the backed-up\nvalue estimate, such that (s′, a′) is unlikely, or not contained,\nin the dataset. The cause of extrapolation error can be\nattributed to several related factors:\nAbsent Data. If any state-action pair (s, a) is unavailable,\nthen error is introduced as some function of the amount\nof similar data and approximation error. This means that\nthe estimate of Qθ(s′, π(s′)) may be arbitrarily bad without\nsufﬁcient data near (s′, π(s′)).\nModel Bias. When performing off-policy Q-learning with\na batch B, the Bellman operator T π is approximated by\nsampling transitions tuples (s, a, r, s′) from B to estimate\nthe expectation over s′. However, for a stochastic MDP,\nwithout inﬁnite state-action visitation, this produces a biased\nestimate of the transition dynamics:\nT πQ(s, a) ≈Es′∼B[r + γQ(s′, π(s′))],\n(4)\nwhere the expectation is with respect to transitions in the\nbatch B, rather than the true MDP.\nOff-Policy Deep Reinforcement Learning without Exploration\nTraining Mismatch. Even with sufﬁcient data, in deep Q-\nlearning systems, transitions are sampled uniformly from the\ndataset, giving a loss weighted with respect to the likelihood\nof data in the batch:\n≈1\n|B|\nX\n(s,a,r,s′)∈B\n||r + γQθ′(s′, π(s′)) −Qθ(s, a)||2. (5)\nIf the distribution of data in the batch does not correspond\nwith the distribution under the current policy, the value\nfunction may be a poor estimate of actions selected by the\ncurrent policy, due to the mismatch in training.\nWe remark that re-weighting the loss in Equation (5) with\nrespect to the likelihood under the current policy can still\nresult in poor estimates if state-action pairs with high like-\nlihood under the current policy are not found in the batch.\nThis means only a subset of possible policies can be evalu-\nated accurately. As a result, learning a value estimate with\noff-policy data can result in large amounts of extrapolation\nerror if the policy selects actions which are not similar to the\ndata found in the batch. In the following section, we discuss\nhow state of the art off-policy deep reinforcement learning\nalgorithms fail to address the concern of extrapolation error,\nand demonstrate the implications in practical examples.\n3.1. Extrapolation Error in Deep Reinforcement\nLearning\nDeep Q-learning algorithms (Mnih et al., 2015) have been\nlabeled as off-policy due to their connection to off-policy Q-\nlearning (Watkins, 1989). However, these algorithms tend\nto use near-on-policy exploratory policies, such as ϵ-greedy,\nin conjunction with a replay buffer (Lin, 1992). As a result,\nthe generated dataset tends to be heavily correlated to the\ncurrent policy. In this section, we examine how these off-\npolicy algorithms perform when learning with uncorrelated\ndatasets. Our results demonstrate that the performance of\na state of the art deep actor-critic algorithm, DDPG (Lil-\nlicrap et al., 2015), deteriorates rapidly when the data is\nuncorrelated and the value estimate produced by the deep\nQ-network diverges. These results suggest that off-policy\ndeep reinforcement learning algorithms are ineffective when\nlearning truly off-policy.\nOur practical experiments examine three different batch set-\ntings in OpenAI gym’s Hopper-v1 environment (Todorov\net al., 2012; Brockman et al., 2016), which we use to train\nan off-policy DDPG agent with no interaction with the en-\nvironment. Experiments with additional environments and\nspeciﬁc details can be found in the Supplementary Material.\nBatch 1 (Final buffer). We train a DDPG agent for 1 mil-\nlion time steps, adding N(0, 0.5) Gaussian noise to actions\nfor high exploration, and store all experienced transitions.\nThis collection procedure creates a dataset with a diverse\nset of states and actions, with the aim of sufﬁcient coverage.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nAverage Return\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nHopper-v1\n(a) Final buffer\nperformance\n(b) Concurrent\nperformance\n(c) Imitation\nperformance\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n−4\n−2\n0\n2\n4\n6\nEstimated Value\n×104\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n500\n1000\n1500\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n1\n2\n3\n4\n5\n6\n7\n×104\nHopper-v1\n(d) Final buffer\nvalue estimate\n(e) Concurrent\nvalue estimate\n(f) Imitation\nvalue estimate\nFigure 1. We examine the performance (top row) and correspond-\ning value estimates (bottom row) of DDPG in three batch tasks on\nHopper-v1. Each individual trial is plotted with a thin line, with\nthe mean in bold (evaluated without exploration noise). Straight\nlines represent the average return of episodes contained in the\nbatch (with exploration noise). An estimate of the true value of the\noff-policy agent, evaluated by Monte Carlo returns, is marked by\na dotted line. In all three experiments, we observe a large gap in\nthe performance between the behavioral and off-policy agent, even\nwhen learning from the same dataset (concurrent). Furthermore,\nthe value estimates are unstable or divergent across all tasks.\nBatch 2 (Concurrent). We concurrently train the off-policy\nand behavioral DDPG agents, for 1 million time steps. To\nensure sufﬁcient exploration, a standard N(0, 0.1) Gaus-\nsian noise is added to actions taken by the behavioral pol-\nicy. Each transition experienced by the behavioral policy is\nstored in a buffer replay, which both agents learn from. As\na result, both agents are trained with the identical dataset.\nBatch 3 (Imitation). A trained DDPG agent acts as an ex-\npert, and is used to collect a dataset of 1 million transitions.\nIn Figure 1, we graph the performance of the agents as\nthey train with each batch, as well as their value estimates.\nStraight lines represent the average return of episodes con-\ntained in the batch. Additionally, we graph the learning\nperformance of the behavioral agent for the relevant tasks.\nOur experiments demonstrate several surprising facts about\noff-policy deep reinforcement learning agents. In each task,\nthe off-policy agent performances signiﬁcantly worse than\nthe behavioral agent. Even in the concurrent experiment,\nwhere both agents are trained with the same dataset, there\nis a large gap in performance in every single trial. This\nresult suggests that differences in the state distribution un-\nder the initial policies is enough for extrapolation error to\ndrastically offset the performance of the off-policy agent.\nAdditionally, the corresponding value estimate exhibits di-\nOff-Policy Deep Reinforcement Learning without Exploration\nvergent behavior, while the value estimate of the behavioral\nagent is highly stable. In the ﬁnal buffer experiment, the\noff-policy agent is provided with a large and diverse dataset,\nwith the aim of providing sufﬁcient coverage of the initial\npolicy. Even in this instance, the value estimate is highly un-\nstable, and the performance suffers. In the imitation setting,\nthe agent is provided with expert data. However, the agent\nquickly learns to take non-expert actions, under the guise\nof optimistic extrapolation. As a result, the value estimates\nrapidly diverge and the agent fails to learn.\nAlthough extrapolation error is not necessarily positively\nbiased, when combined with maximization in reinforcement\nlearning algorithms, extrapolation error provides a source of\nnoise that can induce a persistent overestimation bias (Thrun\n& Schwartz, 1993; Van Hasselt et al., 2016; Fujimoto et al.,\n2018). In an on-policy setting, extrapolation error may be a\nsource of beneﬁcial exploration through an implicit “opti-\nmism in the face of uncertainty” strategy (Lai & Robbins,\n1985; Jaksch et al., 2010). In this case, if the value func-\ntion overestimates an unknown state-action pair, the policy\nwill collect data in the region of uncertainty, and the value\nestimate will be corrected. However, when learning off-\npolicy, or in a batch setting, extrapolation error will never\nbe corrected due to the inability to collect new data.\nThese experiments show extrapolation error can be highly\ndetrimental to learning off-policy in a batch reinforcement\nlearning setting. While the continuous state space and multi-\ndimensional action space in MuJoCo environments are con-\ntributing factors to extrapolation error, the scale of these\ntasks is small compared to real world settings. As a result,\neven with a sufﬁcient amount of data collection, extrapola-\ntion error may still occur due to the concern of catastrophic\nforgetting (McCloskey & Cohen, 1989; Goodfellow et al.,\n2013). Consequently, off-policy reinforcement learning\nalgorithms used in the real-world will require practical guar-\nantees without exhaustive amounts of data.\n4. Batch-Constrained Reinforcement\nLearning\nCurrent off-policy deep reinforcement learning algorithms\nfail to address extrapolation error by selecting actions with\nrespect to a learned value estimate, without consideration\nof the accuracy of the estimate. As a result, certain out-\nof-distribution actions can be erroneously extrapolated to\nhigher values. However, the value of an off-policy agent can\nbe accurately evaluated in regions where data is available.\nWe propose a conceptually simple idea: to avoid extrapo-\nlation error a policy should induce a similar state-action\nvisitation to the batch. We denote policies which satisfy\nthis notion as batch-constrained. To optimize off-policy\nlearning for a given batch, batch-constrained policies are\ntrained to select actions with respect to three objectives:\n(1) Minimize the distance of selected actions to the data in\nthe batch.\n(2) Lead to states where familiar data can be observed.\n(3) Maximize the value function.\nWe note the importance of objective (1) above the others, as\nthe value function and estimates of future states may be arbi-\ntrarily poor without access to the corresponding transitions.\nThat is, we cannot correctly estimate (2) and (3) unless (1) is\nsufﬁciently satisﬁed. As a result, we propose optimizing the\nvalue function, along with some measure of future certainty,\nwith a constraint limiting the distance of selected actions\nto the batch. This is achieved in our deep reinforcement\nlearning algorithm through a state-conditioned generative\nmodel, to produce likely actions under the batch. This gen-\nerative model is combined with a network which aims to\noptimally perturb the generated actions in a small range,\nalong with a Q-network, used to select the highest valued\naction. Finally, we train a pair of Q-networks, and take the\nminimum of their estimates during the value update. This\nupdate penalizes states which are unfamiliar, and pushes the\npolicy to select actions which lead to certain data.\nWe begin by analyzing the theoretical properties of batch-\nconstrained policies in a ﬁnite MDP setting, where we are\nable to quantify extrapolation error precisely. We then in-\ntroduce our deep reinforcement learning algorithm in detail,\nBatch-Constrained deep Q-learning (BCQ) by drawing in-\nspiration from the tabular analogue.\n4.1. Addressing Extrapolation Error in Finite MDPs\nIn the ﬁnite MDP setting, extrapolation error can be de-\nscribed by the bias from the mismatch between the tran-\nsitions contained in the buffer and the true MDP. We ﬁnd\nthat by inducing a data distribution that is contained en-\ntirely within the batch, batch-constrained policies can elim-\ninate extrapolation error entirely for deterministic MDPs.\nIn addition, we show that the batch-constrained variant of\nQ-learning converges to the optimal policy under the same\nconditions as the standard form of Q-learning. Moreover,\nwe prove that for a deterministic MDP, batch-constrained\nQ-learning is guaranteed to match, or outperform, the be-\nhavioral policy when starting from any state contained in\nthe batch. All of the proofs for this section can be found in\nthe Supplementary Material.\nA value estimate Q can be learned using an experience\nreplay buffer B. This involves sampling transition tuples\n(s, a, r, s′) with uniform probability, and applying the tem-\nporal difference update (Sutton, 1988; Watkins, 1989):\nQ(s, a) ←(1 −α)Q(s, a) + α (r + γQ(s′, π(s′))) . (6)\nIf π(s′) = argmaxa′ Q(s′, a′), this is known as Q-learning.\nAssuming a non-zero probability of sampling any possi-\nble transition tuple from the buffer and inﬁnite updates,\nOff-Policy Deep Reinforcement Learning without Exploration\nQ-learning converges to the optimal value function.\nWe begin by showing that the value function Q learned with\nthe batch B corresponds to the value function for an alter-\nnate MDP MB. From the true MDP M and initial values\nQ(s, a), we deﬁne the new MDP MB with the same action\nand state space as M, along with an additional terminal\nstate sinit. MB has transition probabilities pB(s′|s, a) =\nN(s,a,s′)\nP\n˜s N(s,a,˜s), where N(s, a, s′) is the number of times the\ntuple (s, a, s′) is observed in B. If P\n˜s N(s, a, ˜s) = 0, then\npB(sinit|s, a) = 1, where r(s, a, sinit) is set to the initialized\nvalue of Q(s, a).\nTheorem 1. Performing Q-learning by sampling from a\nbatch B converges to the optimal value function under the\nMDP MB.\nWe deﬁne ϵMDP as the tabular extrapolation error, which\naccounts for the discrepancy between the value function\nQπ\nB computed with the batch B and the value function Qπ\ncomputed with the true MDP M:\nϵMDP(s, a) = Qπ(s, a) −Qπ\nB(s, a).\n(7)\nFor any policy π, the exact form of ϵMDP(s, a) can be com-\nputed through a Bellman-like equation:\nϵMDP(s, a) =\nX\ns′\n(pM(s′|s, a) −pB(s′|s, a))\n \nr(s, a, s′) + γ\nX\na′\nπ(a′|s′)Qπ\nB(s′, a′)\n!\n+ pM(s′|s, a)γ\nX\na′\nπ(a′|s′)ϵMDP(s′, a′).\n(8)\nThis means extrapolation error is a function of divergence\nin the transition distributions, weighted by value, along with\nthe error at succeeding states. If the policy is chosen care-\nfully, the error between value functions can be minimized by\nvisiting regions where the transition distributions are similar.\nFor simplicity, we denote\nϵπ\nMDP =\nX\ns\nµπ(s)\nX\na\nπ(a|s)|ϵMDP(s, a)|.\n(9)\nTo evaluate a policy π exactly at relevant state-action pairs,\nonly ϵπ\nMDP = 0 is required. We can then determine the\ncondition required to evaluate the exact expected return of a\npolicy without extrapolation error.\nLemma 1. For all reward functions, ϵπ\nMDP = 0 if and only\nif pB(s′|s, a) = pM(s′|s, a) for all s′ ∈S and (s, a) such\nthat µπ(s) > 0 and π(a|s) > 0.\nLemma 1 states that if MB and M exhibit the same tran-\nsition probabilities in regions of relevance, the policy can\nbe accurately evaluated. For a stochastic MDP this may\nrequire an inﬁnite number of samples to converge to the true\ndistribution, however, for a deterministic MDP this requires\nonly a single transition. This means a policy which only\ntraverses transitions contained in the batch, can be evaluated\nwithout error. More formally, we denote a policy π ∈ΠB\nas batch-constrained if for all (s, a) where µπ(s) > 0 and\nπ(a|s) > 0 then (s, a) ∈B. Additionally, we deﬁne a batch\nB as coherent if for all (s, a, s′) ∈B then s′ ∈B unless\ns′ is a terminal state. This condition is trivially satisﬁed if\nthe data is collected in trajectories, or if all possible states\nare contained in the batch. With a coherent batch, we can\nguarantee the existence of a batch-constrained policy.\nTheorem 2.\nFor a deterministic MDP and all reward\nfunctions, ϵπ\nMDP = 0 if and only if the policy π is batch-\nconstrained. Furthermore, if B is coherent, then such a\npolicy must exist if the start state s0 ∈B.\nBatch-constrained policies can be used in conjunction with\nQ-learning to form batch-constrained Q-learning (BCQL),\nwhich follows the standard tabular Q-learning update while\nconstraining the possible actions with respect to the batch:\nQ(s, a) ←(1−α)Q(s, a)+α(r+γ\nmax\na′s.t.(s′,a′)∈B Q(s′, a′)).\n(10)\nBCQL converges under the same conditions as the standard\nform of Q-learning, noting the batch-constraint is nonrestric-\ntive given inﬁnite state-action visitation.\nTheorem 3. Given the Robbins-Monro stochastic conver-\ngence conditions on the learning rate α, and standard sam-\npling requirements from the environment, BCQL converges\nto the optimal value function Q∗.\nThe more interesting property of BCQL is that for a deter-\nministic MDP and any coherent batch B, BCQL converges\nto the optimal batch-constrained policy π∗∈ΠB such that\nQπ∗(s, a) ≥Qπ(s, a) for all π ∈ΠB and (s, a) ∈B.\nTheorem 4. Given a deterministic MDP and coherent batch\nB, along with the Robbins-Monro stochastic convergence\nconditions on the learning rate α and standard sampling\nrequirements on the batch B, BCQL converges to Qπ\nB(s, a)\nwhere π∗(s) = argmaxa s.t.(s,a)∈B Qπ\nB(s, a) is the optimal\nbatch-constrained policy.\nThis means that BCQL is guaranteed to outperform any\nbehavioral policy when starting from any state contained\nin the batch, effectively outperforming imitation learning.\nUnlike standard Q-learning, there is no condition on state-\naction visitation, other than coherency in the batch.\n4.2. Batch-Constrained Deep Reinforcement Learning\nWe introduce our approach to off-policy batch reinforce-\nment learning, Batch-Constrained deep Q-learning (BCQ).\nBCQ approaches the notion of batch-constrained through a\ngenerative model. For a given state, BCQ generates plau-\nOff-Policy Deep Reinforcement Learning without Exploration\nsible candidate actions with high similarity to the batch,\nand then selects the highest valued action through a learned\nQ-network. Furthermore, we bias this value estimate to\npenalize rare, or unseen, states through a modiﬁcation to\nClipped Double Q-learning (Fujimoto et al., 2018). As a\nresult, BCQ learns a policy with a similar state-action visi-\ntation to the data in the batch, as inspired by the theoretical\nbeneﬁts of its tabular counterpart.\nTo maintain the notion of batch-constraint, we deﬁne a sim-\nilarity metric by making the assumption that for a given\nstate s, the similarity between (s, a) and the state-action\npairs in the batch B can be modelled using a learned state-\nconditioned marginal likelihood P G\nB (a|s). In this case, it\nfollows that the policy maximizing P G\nB (a|s) would min-\nimize the error induced by extrapolation from distant, or\nunseen, state-action pairs, by only selecting the most likely\nactions in the batch with respect to a given state. Given\nthe difﬁculty of estimating P G\nB (a|s) in high-dimensional\ncontinuous spaces, we instead train a parametric generative\nmodel of the batch Gω(s), which we can sample actions\nfrom, as a reasonable approximation to argmaxa P G\nB (a|s).\nFor our generative model we use a conditional variational\nauto-encoder (VAE) (Kingma & Welling, 2013; Sohn et al.,\n2015), which models the distribution by transforming an un-\nderlying latent space1. The generative model Gω, alongside\nthe value function Qθ, can be used as a policy by sampling n\nactions from Gω and selecting the highest valued action ac-\ncording to the value estimate Qθ. To increase the diversity of\nseen actions, we introduce a perturbation model ξφ(s, a, Φ),\nwhich outputs an adjustment to an action a in the range\n[−Φ, Φ]. This enables access to actions in a constrained\nregion, without having to sample from the generative model\na prohibitive number of times. This results in the policy π:\nπ(s) =\nargmax\nai+ξφ(s,ai,Φ)\nQθ(s, ai + ξφ(s, ai, Φ)),\n{ai ∼Gω(s)}n\ni=1.\n(11)\nThe choice of n and Φ creates a trade-off between an im-\nitation learning and reinforcement learning algorithm. If\nΦ = 0, and the number of sampled actions n = 1, then the\npolicy resembles behavioral cloning and as Φ →amax−amin\nand n →∞, then the algorithm approaches Q-learning, as\nthe policy begins to greedily maximize the value function\nover the entire action space.\nThe perturbation model ξφ can be trained to maximize\nQθ(s, a) through the deterministic policy gradient algorithm\n(Silver et al., 2014) by sampling a ∼Gω(s):\nφ ←argmax\nφ\nX\n(s,a)∈B\nQθ(s, a + ξφ(s, a, Φ)).\n(12)\nTo penalize uncertainty over future states, we modify\n1See the Supplementary Material for an introduction to VAEs.\nAlgorithm 1 BCQ\nInput: Batch B, horizon T, target network update rate\nτ, mini-batch size N, max perturbation Φ, number of\nsampled actions n, minimum weighting λ.\nInitialize Q-networks Qθ1, Qθ2, perturbation network ξφ,\nand VAE Gω = {Eω1, Dω2}, with random parameters θ1,\nθ2, φ, ω, and target networks Qθ′\n1, Qθ′\n2, ξφ′ with θ′\n1 ←\nθ1, θ′\n2 ←θ2, φ′ ←φ.\nfor t = 1 to T do\nSample mini-batch of N transitions (s, a, r, s′) from B\nµ, σ = Eω1(s, a),\n˜a = Dω2(s, z),\nz ∼N(µ, σ)\nω ←argminω\nP(a −˜a)2 + DKL(N(µ, σ)||N(0, 1))\nSample n actions: {ai ∼Gω(s′)}n\ni=1\nPerturb each action: {ai = ai + ξφ(s′, ai, Φ)}n\ni=1\nSet value target y (Eqn. 13)\nθ ←argminθ\nP(y −Qθ(s, a))2\nφ ←argmaxφ\nP Qθ1(s, a + ξφ(s, a, Φ)), a ∼Gω(s)\nUpdate target networks: θ′\ni ←τθ + (1 −τ)θ′\ni\nφ′ ←τφ + (1 −τ)φ′\nend for\nClipped Double Q-learning (Fujimoto et al., 2018), which\nestimates the value by taking the minimum between two Q-\nnetworks {Qθ1, Qθ2}. Although originally used as a coun-\ntermeasure to overestimation bias (Thrun & Schwartz, 1993;\nVan Hasselt, 2010), the minimum operator also penalizes\nhigh variance estimates in regions of uncertainty, and pushes\nthe policy to favor actions which lead to states contained in\nthe batch. In particular, we take a convex combination of\nthe two values, with a higher weight on the minimum, to\nform a learning target which is used by both Q-networks:\nr+γ max\nai\n\u0014\nλ min\nj=1,2 Qθ′\nj(s′, ai) + (1 −λ) max\nj=1,2 Qθ′\nj(s′, ai)\n\u0015\n(13)\nwhere ai corresponds to the perturbed actions, sampled\nfrom the generative model. If we set λ = 1, this update\ncorresponds to Clipped Double Q-learning. We use this\nweighted minimum as the constrained updates produces less\noverestimation bias than a purely greedy policy update, and\nenables control over how heavily uncertainty at future time\nsteps is penalized through the choice of λ.\nThis forms Batch-Constrained deep Q-learning (BCQ),\nwhich maintains four parametrized networks: a generative\nmodel Gω(s), a perturbation model ξφ(s, a), and two Q-\nnetworks Qθ1(s, a), Qθ2(s, a). We summarize BCQ in Al-\ngorithm 1. In the following section, we demonstrate BCQ\nresults in stable value learning and a strong performance in\nthe batch setting. Furthermore, we ﬁnd that only a single\nchoice of hyper-parameters is necessary for a wide range of\ntasks and environments.\nOff-Policy Deep Reinforcement Learning without Exploration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n2000\n4000\n6000\n8000\nAverage Return\nHalfCheetah-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n500\n1000\n1500\nHopper-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n500\n1000\n1500\n2000\n2500\nWalker2d-v1\n(a) Final buffer performance\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n2000\n4000\n6000\n8000\n10000\nAverage Return\nHalfCheetah-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n500\n1000\n1500\n2000\nHopper-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n500\n1000\n1500\n2000\n2500\n3000\nWalker2d-v1\n(b) Concurrent performance\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n2000\n4000\n6000\n8000\n10000\nAverage Return\nHalfCheetah-v1\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nHopper-v1\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n1000\n2000\n3000\nWalker2d-v1\n(c) Imitation performance\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n2000\n4000\n6000\nAverage Return\nHalfCheetah-v1\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n500\n1000\n1500\n2000\n2500\n3000\nHopper-v1\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n500\n1000\n1500\n2000\n2500\nWalker2d-v1\n(d) Imperfect demonstrations performance\nFigure 2. We evaluate BCQ and several baselines on the experi-\nments from Section 3.1, as well as the imperfect demonstrations\ntask. The shaded area represents half a standard deviation. The\nbold black line measures the average return of episodes contained\nin the batch. Only BCQ matches or outperforms the performance\nof the behavioral policy in all tasks.\n5. Experiments\nTo evaluate the effectiveness of Batch-Constrained deep\nQ-learning (BCQ) in a high-dimensional setting, we focus\non MuJoCo environments in OpenAI gym (Todorov et al.,\n2012; Brockman et al., 2016). For reproducibility, we make\nno modiﬁcations to the original environments or reward\nfunctions. We compare our method with DDPG (Lillicrap\net al., 2015), DQN (Mnih et al., 2015) using an indepen-\ndently discretized action space, a feed-forward behavioral\ncloning method (BC), and a variant with a VAE (VAE-BC),\nusing Gω(s) from BCQ. Exact implementation and experi-\nmental details are provided in the Supplementary Material.\nWe evaluate each method following the three experiments\ndeﬁned in Section 3.1. In ﬁnal buffer the off-policy agents\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n200\n400\n600\n800\n1000\nEstimated Value\nHopper-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\n(a) Final Buffer\n(b) Concurrent\n(c) Imitation\nFigure 3. We examine the value estimates of BCQ, along with\nDDPG and DQN on the experiments from Section 3.1 in the\nHopper-v1 environment. Each individual trial is plotted, with\nthe mean in bold. An estimate of the true value of BCQ, evaluated\nby Monte Carlo returns, is marked by a dotted line. Unlike the state\nof the art baselines, BCQ exhibits a highly stable value function\nin each task. Graphs for the other environments and imperfect\ndemonstrations task can be found in the Supplementary Material.\nlearn from the ﬁnal replay buffer gathered by training a\nDDPG agent over a million time steps. In concurrent the\noff-policy agents learn concurrently, with the same replay\nbuffer, as the behavioral DDPG policy, and in imitation, the\nagents learn from a dataset collected by an expert policy.\nAdditionally, to study the robustness of BCQ to noisy and\nmulti-modal data, we include an imperfect demonstrations\ntask, in which the agents are trained with a batch of 100k\ntransitions collected by an expert policy, with two sources of\nnoise. The behavioral policy selects actions randomly with\nprobability 0.3 and with high exploratory noise N(0, 0.3)\nadded to the remaining actions. The experimental results\nfor these tasks are reported in Figure 2. Furthermore, the\nestimated values of BCQ, DDPG and DQN, and the true\nvalue of BCQ are displayed in Figure 3.\nOur approach, BCQ, is the only algorithm which succeeds\nat all tasks, matching or outperforming the behavioral policy\nin each instance, and outperforming all other agents, besides\nin the imitation learning task where behavioral cloning un-\nsurprisingly performs the best. These results demonstrate\nthat our algorithm can be used as a single approach for both\nimitation learning and off-policy reinforcement learning,\nwith a single set of ﬁxed hyper-parameters. Furthermore,\nunlike the deep reinforcement learning algorithms, DDPG\nand DQN, BCQ exhibits a highly stable value function in\nthe presence of off-policy samples, suggesting extrapolation\nerror has been successfully mitigated through the batch-\nconstraint. In the imperfect demonstrations task, we ﬁnd\nthat both deep reinforcement learning and imitation learn-\ning algorithms perform poorly. BCQ, however, is able to\nstrongly outperform the noisy demonstrator, disentangling\npoor and expert actions. Furthermore, compared to current\ndeep reinforcement learning algorithms, which can require\nmillions of time steps (Duan et al., 2016; Henderson et al.,\n2017), BCQ attains a high performance in remarkably few\niterations. This suggests our approach effectively leverages\nexpert transitions, even in the presence of noise.\nOff-Policy Deep Reinforcement Learning without Exploration\n6. Related Work\nBatch Reinforcement Learning. While batch reinforce-\nment learning algorithms have been shown to be conver-\ngent with non-parametric function approximators such as\naveragers (Gordon, 1995) and kernel methods (Ormoneit\n& Sen, 2002), they make no guarantees on the quality of\nthe policy without inﬁnite data. Other batch algorithms,\nsuch as ﬁtted Q-iteration, have used other function approx-\nimators, including decision trees (Ernst et al., 2005) and\nneural networks (Riedmiller, 2005), but come without con-\nvergence guarantees. Unlike many previous approaches\nto off-policy policy evaluation (Peshkin & Shelton, 2002;\nThomas et al., 2015; Liu et al., 2018), our work focuses on\nconstraining the policy to a subset of policies which can\nbe adequately evaluated, rather than the process of evalua-\ntion itself. Additionally, off-policy algorithms which rely\non importance sampling (Precup et al., 2001; Jiang & Li,\n2016; Munos et al., 2016) may not be applicable in a batch\nsetting, requiring access to the action probabilities under\nthe behavioral policy, and scale poorly to multi-dimensional\naction spaces. Reinforcement learning with a replay buffer\n(Lin, 1992) can be considered a form of batch reinforcement\nlearning, and is a standard tool for off-policy deep reinforce-\nment learning algorithms (Mnih et al., 2015). It has been\nobserved that a large replay buffer can be detrimental to\nperformance (de Bruin et al., 2015; Zhang & Sutton, 2017)\nand the diversity of states in the buffer is an important factor\nfor performance (de Bruin et al., 2016). Isele & Cosgun\n(2018) observed the performance of an agent was strongest\nwhen the distribution of data in the replay buffer matched\nthe test distribution. These results defend the notion that\nextrapolation error is an important factor in the performance\noff-policy reinforcement learning.\nImitation Learning. Imitation learning and its variants are\nwell studied problems (Schaal, 1999; Argall et al., 2009;\nHussein et al., 2017). Imitation has been combined with\nreinforcement learning, via learning from demonstrations\nmethods (Kim et al., 2013; Piot et al., 2014; Chemali &\nLazaric, 2015), with deep reinforcement learning extensions\n(Hester et al., 2017; Veˇcer´ık et al., 2017), and modiﬁed pol-\nicy gradient approaches (Ho et al., 2016; Sun et al., 2017;\nCheng et al., 2018; Sun et al., 2018). While effective, these\ninteractive methods are inadequate for batch reinforcement\nlearning as they require either an explicit distinction be-\ntween expert and non-expert data, further on-policy data\ncollection or access to an oracle. Research in imitation, and\ninverse reinforcement learning, with robustness to noise is\nan emerging area (Evans, 2016; Nair et al., 2018), but relies\non some form of expert data. Gao et al. (2018) introduced an\nimitation learning algorithm which learned from imperfect\ndemonstrations, by favoring seen actions, but is limited to\ndiscrete actions. Our work also connects to residual policy\nlearning (Johannink et al., 2018; Silver et al., 2018), where\nthe initial policy is the generative model, rather than an\nexpert or feedback controller.\nUncertainty in Reinforcement Learning. Uncertainty es-\ntimates in deep reinforcement learning have generally been\nused to encourage exploration (Dearden et al., 1998; Strehl\n& Littman, 2008; O’Donoghue et al., 2018; Azizzadenesheli\net al., 2018). Other methods have examined approximat-\ning the Bayesian posterior of the value function (Osband\net al., 2016; 2018; Touati et al., 2018), again using the vari-\nance to encourage exploration to unseen regions of the state\nspace. In model-based reinforcement learning, uncertainty\nhas been used for exploration, but also for the opposite\neffect–to push the policy towards regions of certainty in the\nmodel. This is used to combat the well-known problems\nwith compounding model errors, and is present in policy\nsearch methods (Deisenroth & Rasmussen, 2011; Gal et al.,\n2016; Higuera et al., 2018; Xu et al., 2018), or combined\nwith trajectory optimization (Chua et al., 2018) or value-\nbased methods (Buckman et al., 2018). Our work connects\nto policy methods with conservative updates (Kakade &\nLangford, 2002), such as trust region (Schulman et al., 2015;\nAchiam et al., 2017; Pham et al., 2018) and information-\ntheoretic methods (Peters & M¨ulling, 2010; Van Hoof et al.,\n2017), which aim to keep the updated policy similar to the\nprevious policy. These methods avoid explicit uncertainty\nestimates, and rather force policy updates into a constrained\nrange before collecting new data, limiting errors introduced\nby large changes in the policy. Similarly, our approach can\nbe thought of as an off-policy variant, where the policy aims\nto be kept close, in output space, to any combination of the\nprevious policies which performed data collection.\n7. Conclusion\nIn this work, we demonstrate a critical problem in off-\npolicy reinforcement learning with ﬁnite data, where the\nvalue target introduces error by including an estimate of\nunseen state-action pairs. This phenomenon, which we\ndenote extrapolation error, has important implications for\noff-policy and batch reinforcement learning, as it is gen-\nerally implausible to have complete state-action coverage\nin any practical setting. We present batch-constrained rein-\nforcement learning–acting close to on-policy with respect\nto the available data, as an answer to extrapolation error.\nWhen extended to a deep reinforcement learning setting, our\nalgorithm, Batch-Constrained deep Q-learning (BCQ), is\nthe ﬁrst continuous control algorithm capable of learning\nfrom arbitrary batch data, without exploration. Due to the\nimportance of batch reinforcement learning for practical\napplications, we believe BCQ will be a strong foothold for\nfuture algorithms to build on, while furthering our under-\nstanding of the systematic risks in Q-learning (Thrun &\nSchwartz, 1993; Lu et al., 2018).\nOff-Policy Deep Reinforcement Learning without Exploration\nReferences\nAchiam, J., Held, D., Tamar, A., and Abbeel, P. Constrained\npolicy optimization. In International Conference on Ma-\nchine Learning, pp. 22–31, 2017.\nArgall, B. D., Chernova, S., Veloso, M., and Browning, B.\nA survey of robot learning from demonstration. Robotics\nand Autonomous Systems, 57(5):469–483, 2009.\nAzizzadenesheli, K., Brunskill, E., and Anandkumar, A.\nEfﬁcient exploration through bayesian deep q-networks.\narXiv preprint arXiv:1802.04412, 2018.\nBertsekas, D. P. and Tsitsiklis, J. N. Neuro-Dynamic Pro-\ngramming. Athena scientiﬁc Belmont, MA, 1996.\nBrockman, G., Cheung, V., Pettersson, L., Schneider, J.,\nSchulman, J., Tang, J., and Zaremba, W. Openai gym,\n2016.\nBuckman, J., Hafner, D., Tucker, G., Brevdo, E., and Lee,\nH. Sample-efﬁcient reinforcement learning with stochas-\ntic ensemble value expansion. In Advances in Neural\nInformation Processing Systems, pp. 8234–8244, 2018.\nChemali, J. and Lazaric, A. Direct policy iteration with\ndemonstrations. In Proceedings of the Twenty-Fourth\nInternational Joint Conference on Artiﬁcial Intelligence,\n2015.\nCheng, C.-A., Yan, X., Wagener, N., and Boots, B. Fast pol-\nicy learning through imitation and reinforcement. arXiv\npreprint arXiv:1805.10413, 2018.\nChua, K., Calandra, R., McAllister, R., and Levine, S. Deep\nreinforcement learning in a handful of trials using proba-\nbilistic dynamics models. In Advances in Neural Infor-\nmation Processing Systems 31, pp. 4759–4770, 2018.\nDayan, P. and Watkins, C. J. C. H. Q-learning. Machine\nlearning, 8(3):279–292, 1992.\nde Bruin, T., Kober, J., Tuyls, K., and Babuˇska, R. The\nimportance of experience replay database composition\nin deep reinforcement learning. In Deep Reinforcement\nLearning Workshop, NIPS, 2015.\nde Bruin, T., Kober, J., Tuyls, K., and Babuˇska, R. Im-\nproved deep reinforcement learning for robotics through\ndistribution-based experience retention. In IEEE/RSJ In-\nternational Conference on Intelligent Robots and Systems\n(IROS), pp. 3947–3952. IEEE, 2016.\nDearden, R., Friedman, N., and Russell, S. Bayesian q-\nlearning. In AAAI/IAAI, pp. 761–768, 1998.\nDeisenroth, M. and Rasmussen, C. E. Pilco: A model-\nbased and data-efﬁcient approach to policy search. In\nInternational Conference on Machine Learning, pp. 465–\n472, 2011.\nDuan, Y., Chen, X., Houthooft, R., Schulman, J., and\nAbbeel, P. Benchmarking deep reinforcement learning\nfor continuous control. In International Conference on\nMachine Learning, pp. 1329–1338, 2016.\nErnst, D., Geurts, P., and Wehenkel, L. Tree-based batch\nmode reinforcement learning. Journal of Machine Learn-\ning Research, 6(Apr):503–556, 2005.\nEvans, O. Learning the preferences of ignorant, inconsistent\nagents. In AAAI, pp. 323–329, 2016.\nFujimoto, S., van Hoof, H., and Meger, D. Addressing func-\ntion approximation error in actor-critic methods. In Inter-\nnational Conference on Machine Learning, volume 80,\npp. 1587–1596. PMLR, 2018.\nGal, Y., McAllister, R., and Rasmussen, C. E. Improving\npilco with bayesian neural network dynamics models.\nIn Data-Efﬁcient Machine Learning workshop, Interna-\ntional Conference on Machine Learning, 2016.\nGao, Y., Lin, J., Yu, F., Levine, S., and Darrell, T. Rein-\nforcement learning from imperfect demonstrations. arXiv\npreprint arXiv:1802.05313, 2018.\nGoodfellow, I. J., Mirza, M., Xiao, D., Courville, A., and\nBengio, Y. An empirical investigation of catastrophic for-\ngetting in gradient-based neural networks. arXiv preprint\narXiv:1312.6211, 2013.\nGordon, G. J. Stable function approximation in dynamic\nprogramming. In Machine Learning Proceedings 1995,\npp. 261–268. Elsevier, 1995.\nHenderson, P., Islam, R., Bachman, P., Pineau, J., Precup,\nD., and Meger, D. Deep Reinforcement Learning that\nMatters. arXiv preprint arXiv:1709.06560, 2017.\nHester, T., Vecerik, M., Pietquin, O., Lanctot, M., Schaul,\nT., Piot, B., Horgan, D., Quan, J., Sendonaris, A., Dulac-\nArnold, G., et al. Deep q-learning from demonstrations.\narXiv preprint arXiv:1704.03732, 2017.\nHiguera, J. C. G., Meger, D., and Dudek, G. Synthesizing\nneural network controllers with probabilistic model based\nreinforcement learning. arXiv preprint arXiv:1803.02291,\n2018.\nHo, J., Gupta, J., and Ermon, S. Model-free imitation learn-\ning with policy optimization. In International Conference\non Machine Learning, pp. 2760–2769, 2016.\nOff-Policy Deep Reinforcement Learning without Exploration\nHussein, A., Gaber, M. M., Elyan, E., and Jayne, C. Im-\nitation learning: A survey of learning methods. ACM\nComputing Surveys (CSUR), 50(2):21, 2017.\nIsele, D. and Cosgun, A. Selective experience replay for\nlifelong learning. arXiv preprint arXiv:1802.10269, 2018.\nJaksch, T., Ortner, R., and Auer, P. Near-optimal regret\nbounds for reinforcement learning. Journal of Machine\nLearning Research, 11(Apr):1563–1600, 2010.\nJiang, N. and Li, L. Doubly robust off-policy value evalua-\ntion for reinforcement learning. In International Confer-\nence on Machine Learning, pp. 652–661, 2016.\nJohannink, T., Bahl, S., Nair, A., Luo, J., Kumar, A.,\nLoskyll, M., Ojea, J. A., Solowjow, E., and Levine, S.\nResidual reinforcement learning for robot control. arXiv\npreprint arXiv:1812.03201, 2018.\nKakade, S. and Langford, J. Approximately optimal approxi-\nmate reinforcement learning. In International Conference\non Machine Learning, volume 2, pp. 267–274, 2002.\nKim, B., Farahmand, A.-m., Pineau, J., and Precup, D.\nLearning from limited demonstrations. In Advances in\nNeural Information Processing Systems, pp. 2859–2867,\n2013.\nKingma, D. and Ba, J. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980, 2014.\nKingma, D. P. and Welling, M. Auto-encoding variational\nbayes. arXiv preprint arXiv:1312.6114, 2013.\nKonda, V. R. and Tsitsiklis, J. N. On actor-critic algorithms.\nSIAM journal on Control and Optimization, 42(4):1143–\n1166, 2003.\nLai, T. L. and Robbins, H. Asymptotically efﬁcient adaptive\nallocation rules. Advances in Applied Mathematics, 6(1):\n4–22, 1985.\nLange, S., Gabel, T., and Riedmiller, M. Batch reinforce-\nment learning. In Reinforcement learning, pp. 45–73.\nSpringer, 2012.\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,\nT., Tassa, Y., Silver, D., and Wierstra, D. Continuous\ncontrol with deep reinforcement learning. arXiv preprint\narXiv:1509.02971, 2015.\nLin, L.-J. Self-improving reactive agents based on reinforce-\nment learning, planning and teaching. Machine learning,\n8(3-4):293–321, 1992.\nLiu, Y., Gottesman, O., Raghu, A., Komorowski, M., Faisal,\nA. A., Doshi-Velez, F., and Brunskill, E. Representa-\ntion balancing mdps for off-policy policy evaluation. In\nAdvances in Neural Information Processing Systems, pp.\n2644–2653, 2018.\nLu, T., Schuurmans, D., and Boutilier, C. Non-delusional\nq-learning and value-iteration. In Advances in Neural\nInformation Processing Systems, pp. 9971–9981, 2018.\nMcCloskey, M. and Cohen, N. J. Catastrophic interfer-\nence in connectionist networks: The sequential learning\nproblem. In Psychology of Learning and Motivation,\nvolume 24, pp. 109–165. Elsevier, 1989.\nMelo, F. S. Convergence of q-learning: A simple proof.\nInstitute Of Systems and Robotics, Tech. Rep, pp. 1–4,\n2001.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,\nJ., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-\nland, A. K., Ostrovski, G., et al. Human-level control\nthrough deep reinforcement learning. Nature, 518(7540):\n529–533, 2015.\nMunos, R., Stepleton, T., Harutyunyan, A., and Bellemare,\nM. Safe and efﬁcient off-policy reinforcement learning.\nIn Advances in Neural Information Processing Systems,\npp. 1054–1062, 2016.\nNair, A., McGrew, B., Andrychowicz, M., Zaremba, W.,\nand Abbeel, P. Overcoming exploration in reinforcement\nlearning with demonstrations. In 2018 IEEE Interna-\ntional Conference on Robotics and Automation (ICRA),\npp. 6292–6299. IEEE, 2018.\nO’Donoghue, B., Osband, I., Munos, R., and Mnih, V. The\nuncertainty Bellman equation and exploration. In Inter-\nnational Conference on Machine Learning, volume 80,\npp. 3839–3848. PMLR, 2018.\nOrmoneit, D. and Sen, ´S. Kernel-based reinforcement learn-\ning. Machine learning, 49(2-3):161–178, 2002.\nOsband, I., Blundell, C., Pritzel, A., and Van Roy, B. Deep\nexploration via bootstrapped dqn. In Advances in Neural\nInformation Processing Systems, pp. 4026–4034, 2016.\nOsband, I., Aslanides, J., and Cassirer, A. Randomized prior\nfunctions for deep reinforcement learning. In Advances\nin Neural Information Processing Systems 31, pp. 8626–\n8638, 2018.\nPeshkin, L. and Shelton, C. R. Learning from scarce experi-\nence. In International Conference on Machine Learning,\npp. 498–505, 2002.\nPeters, J. and M¨ulling, K. Relative entropy policy search.\nIn AAAI, pp. 1607–1612, 2010.\nOff-Policy Deep Reinforcement Learning without Exploration\nPham, T.-H., De Magistris, G., Agravante, D. J., Chaud-\nhury, S., Munawar, A., and Tachibana, R. Constrained\nexploration and recovery from experience shaping. arXiv\npreprint arXiv:1809.08925, 2018.\nPiot, B., Geist, M., and Pietquin, O. Boosted bellman resid-\nual minimization handling expert demonstrations. In Joint\nEuropean Conference on Machine Learning and Knowl-\nedge Discovery in Databases, pp. 549–564. Springer,\n2014.\nPrecup, D., Sutton, R. S., and Dasgupta, S. Off-policy\ntemporal-difference learning with function approxima-\ntion. In International Conference on Machine Learning,\npp. 417–424, 2001.\nRezende, D. J., Mohamed, S., and Wierstra, D. Stochastic\nbackpropagation and approximate inference in deep gen-\nerative models. arXiv preprint arXiv:1401.4082, 2014.\nRiedmiller, M. Neural ﬁtted q iteration–ﬁrst experiences\nwith a data efﬁcient neural reinforcement learning method.\nIn European Conference on Machine Learning, pp. 317–\n328. Springer, 2005.\nSchaal, S.\nIs imitation learning the route to humanoid\nrobots?\nTrends in Cognitive Sciences, 3(6):233–242,\n1999.\nSchulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz,\nP. Trust region policy optimization. In International\nConference on Machine Learning, pp. 1889–1897, 2015.\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and\nRiedmiller, M. Deterministic policy gradient algorithms.\nIn International Conference on Machine Learning, pp.\n387–395, 2014.\nSilver, T., Allen, K., Tenenbaum, J., and Kaelbling, L. Resid-\nual policy learning. arXiv preprint arXiv:1812.06298,\n2018.\nSingh, S., Jaakkola, T., Littman, M. L., and Szepesv´ari,\nC.\nConvergence results for single-step on-policy\nreinforcement-learning algorithms. Machine learning,\n38(3):287–308, 2000.\nSohn, K., Lee, H., and Yan, X. Learning structured output\nrepresentation using deep conditional generative models.\nIn Advances in Neural Information Processing Systems,\npp. 3483–3491, 2015.\nStrehl, A. L. and Littman, M. L. An analysis of model-\nbased interval estimation for markov decision processes.\nJournal of Computer and System Sciences, 74(8):1309–\n1331, 2008.\nSun, W., Venkatraman, A., Gordon, G. J., Boots, B., and\nBagnell, J. A. Deeply aggrevated: Differentiable imita-\ntion learning for sequential prediction. In International\nConference on Machine Learning, pp. 3309–3318, 2017.\nSun, W., Bagnell, J. A., and Boots, B. Truncated horizon\npolicy search: Combining reinforcement learning & imi-\ntation learning. arXiv preprint arXiv:1805.11240, 2018.\nSutton, R. S. Learning to predict by the methods of temporal\ndifferences. Machine learning, 3(1):9–44, 1988.\nSutton, R. S. and Barto, A. G. Reinforcement learning: An\nintroduction, volume 1. MIT press Cambridge, 1998.\nThomas, P., Theocharous, G., and Ghavamzadeh, M. High\nconﬁdence policy improvement. In International Confer-\nence on Machine Learning, pp. 2380–2388, 2015.\nThrun, S. and Schwartz, A. Issues in using function approx-\nimation for reinforcement learning. In Proceedings of the\n1993 Connectionist Models Summer School Hillsdale, NJ.\nLawrence Erlbaum, 1993.\nTodorov, E., Erez, T., and Tassa, Y. Mujoco: A physics en-\ngine for model-based control. In IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS), pp.\n5026–5033. IEEE, 2012.\nTouati, A., Satija, H., Romoff, J., Pineau, J., and Vincent, P.\nRandomized value functions via multiplicative normaliz-\ning ﬂows. arXiv preprint arXiv:1806.02315, 2018.\nVan Hasselt, H. Double q-learning. In Advances in Neural\nInformation Processing Systems, pp. 2613–2621, 2010.\nVan Hasselt, H., Guez, A., and Silver, D. Deep reinforce-\nment learning with double q-learning. In AAAI, pp. 2094–\n2100, 2016.\nVan Hoof, H., Neumann, G., and Peters, J. Non-parametric\npolicy search with limited information loss. The Journal\nof Machine Learning Research, 18(1):2472–2517, 2017.\nVeˇcer´ık, M., Hester, T., Scholz, J., Wang, F., Pietquin, O.,\nPiot, B., Heess, N., Roth¨orl, T., Lampe, T., and Riedmiller,\nM. Leveraging demonstrations for deep reinforcement\nlearning on robotics problems with sparse rewards. arXiv\npreprint arXiv:1707.08817, 2017.\nWatkins, C. J. C. H. Learning from delayed rewards. PhD\nthesis, King’s College, Cambridge, 1989.\nXu, H., Li, Y., Tian, Y., Darrell, T., and Ma, T. Algorithmic\nframework for model-based reinforcement learning with\ntheoretical guarantees. arXiv preprint arXiv:1807.03858,\n2018.\nZhang, S. and Sutton, R. S. A deeper look at experience\nreplay. arXiv preprint arXiv:1712.01275, 2017.\nOff-Policy Deep Reinforcement Learning without Exploration:\nSupplementary Material\nA. Missing Proofs\nA.1. Proofs and Details from Section 4.1\nDeﬁnition 1. We deﬁne a coherent batch B as a batch such that if (s, a, s′) ∈B then s′ ∈B unless s′ is a terminal state.\nDeﬁnition 2. We deﬁne ϵMDP(s, a) = Qπ(s, a) −Qπ\nB(s, a) as the error between the true value of a policy π in the MDP M\nand the value of π when learned with a batch B.\nDeﬁnition 3. For simplicity in notation, we denote\nϵπ\nMDP =\nX\ns\nµπ(s)\nX\na\nπ(a|s)|ϵMDP(s, a)|.\n(14)\nTo evaluate a policy π exactly at relevant state-action pairs, only ϵπ\nMDP = 0 is required.\nDeﬁnition 4. We deﬁne the optimal batch-constrained policy π∗∈ΠB such that Qπ∗(s, a) ≥Qπ(s, a) for all π ∈ΠB and\n(s, a) ∈B.\nAlgorithm 1. Batch-Constrained Q-learning (BCQL) maintains a tabular value function Q(s, a) for each possible state-\naction pair (s, a). A transition tuple (s, a, r, s′) is sampled from the batch B with uniform probability and the following\nupdate rule is applied, with learning rate α:\nQ(s, a) ←(1 −α)Q(s, a) + α(r + γ\nmax\na′s.t.(s′,a′)∈B Q(s′, a′)).\n(15)\nTheorem 1. Performing Q-learning by sampling from a batch B converges to the optimal value function under the MDP\nMB.\nProof. Again, the MDP MB is deﬁned by the same action and state space as M, with an additional terminal state sinit.\nMB has transition probabilities pB(s′|s, a) =\nN(s,a,s′)\nP\n˜s N(s,a,˜s), where N(s, a, s′) is the number of times the tuple (s, a, s′) is\nobserved in B. If P\n˜s N(s, a, ˜s) = 0, then pB(sinit|s, a) = 1, where r(sinit, s, a) is to the initialized value of Q(s, a).\nFor any given MDP Q-learning converges to the optimal value function given inﬁnite state-action visitation and some\nstandard assumptions (see Section A.2). Now note that sampling under a batch B with uniform probability satisﬁes the\ninﬁnite state-action visitation assumptions of the MDP MB, where given (s, a), the probability of sampling (s, a, s′)\ncorresponds to p(s′|s, a) =\nN(s,a,s′)\nP\n˜s N(s,a,˜s) in the limit. We remark that for (s, a) /∈B, Q(s, a) will never be updated, and will\nreturn the initialized value, which corresponds to the terminal transition sinit. It follows that sampling from B is equivalent\nto sampling from the MDP MB, and Q-learning converges to the optimal value function under MB.\nRemark 1. For any policy π and state-action pair (s, a), the error term ϵMDP(s, a) satisﬁes the following Bellman-like\nequation:\nϵMDP(s, a) =\nX\ns′\n(pM(s′|s, a) −pB(s′|s, a))\n \nr(s, a, s′) + γ\nX\na′\nπ(a′|s′) (Qπ\nB(s′, a′))\n!\n+ pM(s′|s, a)γ\nX\na′\nπ(a′|s′)ϵMDP(s′, a′).\n(16)\nOff-Policy Deep Reinforcement Learning without Exploration: Supplementary Material\nProof. Proof follows by expanding each Q, rearranging terms and then simplifying the expression.\nϵMDP(s, a) = Qπ(s, a) −Qπ\nB(s, a)\n=\nX\ns′\npM(s′|s, a)\n \nr(s, a, s′) + γ\nX\na′\nπ(a′|s′)Qπ(s′, a′)\n!\n−Qπ\nB(s, a)\n=\nX\ns′\npM(s′|s, a)\n \nr(s, a, s′) + γ\nX\na′\nπ(a′|s′)Qπ(s′, a′)\n!\n−\n X\ns′\npB(s′|s, a)\n \nr(s, a, s′) + γ\nX\na′\nπ(a′|s′)Qπ\nB(s′, a′)\n!!\n=\nX\ns′\n(pM(s′|s, a) −pB(s′|s, a)) r(s, a, s′) + pM(s′|s, a)γ\nX\na′\nπ(a′|s′) (Qπ\nB(s′, a′) + ϵMDP(s′, a′))\n−pB(s′|s, a)γ\nX\na′\nπ(a′|s′)Qπ\nB(s′, a′)\n=\nX\ns′\n(pM(s′|s, a) −pB(s′|s, a)) r(s, a, s′) + pM(s′|s, a)γ\nX\na′\nπ(a′|s′) (Qπ\nB(s′, a′) + ϵMDP(s′, a′))\n+ pM(s′|s, a)γ\nX\na′\nπ(a′|s′) (ϵMDP(s′, a′) −ϵMDP(s′, a′)) −pB(s′|s, a)γ\nX\na′\nπ(a′|s′)Qπ\nB(s′, a′)\n=\nX\ns′\n(pM(s′|s, a) −pB(s′|s, a))\n \nr(s, a, s′) + γ\nX\na′\nπ(a′|s′)Qπ\nB(s′, a′)\n!\n+ pM(s′|s, a)γ\nX\na′\nπ(a′|s′)ϵMDP(s′, a′)\n(17)\nLemma 1. For all reward functions, ϵπ\nMDP = 0 if and only if pB(s′|s, a) = pM(s′|s, a) for all s′ ∈S and (s, a) such that\nµπ(s) > 0 and π(a|s) > 0.\nProof. From Remark 1, we note that the form of ϵMDP(s, a), since no assumptions can be made on the reward function and\ntherefore the expression r(s, a, s′) + γ P\na′ π(a′|s′)Qπ\nB(s′, a′), we have that ϵMDP(s, a) = 0 if and only if pB(s′|s, a) =\npM(s′|s, a) for all s′ ∈S and pM(s′|s, a)γ P\na′ π(a′|s′)ϵMDP(s′, a′) = 0.\n(⇒) Now we note that if ϵMDP(s, a) = 0 then pM(s′|s, a)γ P\na′ π(a′|s′)ϵMDP(s′, a′) = 0 by the relationship deﬁned by\nRemark 1 and the condition on the reward function. It follows that we must have pB(s′|s, a) = pM(s′|s, a) for all s′ ∈S.\n(⇐) If we have P\ns′ |pM(s′|s, a) −pB(s′|s, a)| = 0 for all (s, a) such that µπ(s) > 0 and π(a|s) > 0, then for any (s, a)\nunder the given conditions, we have ϵ(s, a) = P\ns′ pM(s′|s, a)γ P\na′ π(a′|s′)ϵ(s′, a′). Recursively expanding the ϵ term,\nwe arrive at ϵ(s, a) = 0 + γ0 + γ20 + ... = 0.\nTheorem 2. For a deterministic MDP and all reward functions, ϵπ\nMDP = 0 if and only if the policy π is batch-constrained.\nFurthermore, if B is coherent, then such a policy must exist if the start state s0 ∈B.\nProof. The ﬁrst part of the Theorem follows from Lemma 1, noting that for a deterministic policy π, if (s, a) ∈B then we\nmust have pB(s′|s, a) = pM(s′|s, a) for all s′ ∈S.\nWe can construct the batch-constrained policy by selecting a in the state s ∈B, such that (s, a) ∈B. Since the MDP\nis deterministic and the batch is coherent, when starting from s0, we must be able to follow at least one trajectory until\ntermination.\nTheorem 3. Given the Robbins-Monro stochastic convergence conditions on the learning rate α, and standard sampling\nrequirements from the environment, BCQL converges to the optimal value function Q∗.\nProof. Follows from proof of convergence of Q-learning (see Section A.2), noting the batch-constraint is non-restrictive\nwith a batch which contains all possible transitions.\nOff-Policy Deep Reinforcement Learning without Exploration: Supplementary Material\nTheorem 4. Given a deterministic MDP and coherent batch B, along with the Robbins-Monro stochastic convergence\nconditions on the learning rate α and standard sampling requirements on the batch B, BCQL converges to Qπ\nB(s, a) where\nπ∗(s) = argmaxa s.t.(s,a)∈B Qπ\nB(s, a) is the optimal batch-constrained policy.\nProof. Results follows from Theorem 1, which states Q-learning learns the optimal value for the MDP MB for state-action\npairs in (s, a). However, for a deterministic MDP MB corresponds to the true MDP in all seen state-action pairs. Noting\nthat batch-constrained policies operate only on state-action pairs where MB corresponds to the true MDP, it follows that π∗\nwill be the optimal batch-constrained policy from the optimality of Q-learning.\nA.2. Sketch of the Proof of Convergence of Q-Learning\nThe proof of convergence of Q-learning relies large on the following lemma (Singh et al., 2000):\nLemma 2. Consider a stochastic process (ζt, ∆t, Ft), t ≥0 where ζt, ∆t, Ft : X →R satisfy the equation:\n∆t+1(xt) = (1 −ζt(xt))∆t(xt) + ζt(xt)Ft(xt),\n(18)\nwhere xt ∈X and t = 0, 1, 2, .... Let Pt be a sequence of increasing σ-ﬁelds such that ζ0 and ∆0 are P0-measurable and\nζt, ∆t and Ft−1 are Pt-measurable, t = 1, 2, .... Assume that the following hold:\n1. The set X is ﬁnite.\n2. ζt(xt) ∈[0, 1], P\nt ζt(xt) = ∞, P\nt(ζt(xt))2 < ∞with probability 1 and ∀x ̸= xt : ζ(x) = 0.\n3. ||E [Ft|Pt] || ≤κ||∆t|| + ct where κ ∈[0, 1) and ct converges to 0 with probability 1.\n4. Var[Ft(xt)|Pt] ≤K(1 + κ||∆t||)2, where K is some constant\nWhere || · || denotes the maximum norm. Then ∆t converges to 0 with probability 1.\nSketch of Proof of Convergence of Q-Learning. We set ∆t = Qt(s, a) −Q∗(s, a). Then convergence follows by\nsatisfying the conditions of Lemma 2. Condition 1 is satisﬁed by the ﬁnite MDP, setting X = S × A. Condition 2 is\nsatisﬁed by the assumption of Robbins-Monro stochastic convergence conditions on the learning rate αt, setting ζt = αt.\nCondition 4 is satisﬁed by the bounded reward function, where Ft(s, a) = r(s, a, s′) + γ maxa′ Q(s′, a′) −Q∗(s, a), and\nthe sequence Pt = {Q0, s0, a0, α0, r1, s1, ...st, at}. Finally, Condition 3 follows from the contraction of the Bellman\nOperator T , requiring inﬁnite state-action visitation, inﬁnite updates and γ < 1.\nAdditional and more complete details can be found in numerous resources (Dayan & Watkins, 1992; Singh et al., 2000;\nMelo, 2001).\nOff-Policy Deep Reinforcement Learning without Exploration: Supplementary Material\nB. Missing Graphs\nB.1. Extrapolation Error in Deep Reinforcement Learning\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n2000\n4000\n6000\n8000\nAverage Return\nHalfCheetah-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nHopper-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n500\n1000\n1500\n2000\n2500\n3000\nWalker2d-v1\n(a) Final buffer performance\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n200\n400\n600\n800\n1000\nEstimated Value\nHalfCheetah-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n−40000\n−20000\n0\n20000\n40000\n60000\nHopper-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n200\n400\n600\n800\n1000\n1200\n1400\nWalker2d-v1\n(b) Final buffer value estimates\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n2000\n4000\n6000\n8000\n10000\nAverage Return\nHalfCheetah-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nHopper-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n500\n1000\n1500\n2000\n2500\n3000\nWalker2d-v1\n(c) Concurrent performance\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nEstimated Value\n×105 HalfCheetah-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n500\n1000\n1500\nHopper-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0.0\n0.2\n0.4\n0.6\n0.8\n×104\nWalker2d-v1\n(d) Concurrent value estimates\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n2000\n4000\n6000\n8000\n10000\n12000\nAverage Return\nHalfCheetah-v1\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n1000\n2000\n3000\nHopper-v1\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n1000\n2000\n3000\n4000\nWalker2d-v1\n(e) Imitation performance\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n500\n1000\n1500\n2000\nEstimated Value\nHalfCheetah-v1\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n×107\nHopper-v1\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0.0\n0.2\n0.4\n0.6\n0.8\n×106\nWalker2d-v1\n(f) Imitation value estimates\nFigure 4. We examine the performance of DDPG in three batch tasks. Each individual trial is plotted with a thin line, with the mean in\nbold (evaluated without exploration noise). Straight lines represent the average return of episodes contained in the batch (with exploration\nnoise). An estimate of the true value of the off-policy agent, evaluated by Monte Carlo returns, is marked by a dotted line. In the ﬁnal\nbuffer experiment, the off-policy agent learns from a large, diverse dataset, but exhibits poor learning behavior and value estimation. In the\nconcurrent setting the agent learns alongside a behavioral agent, with access to the same data, but suffers in performance. In the imitation\nsetting, the agent receives data from an expert policy but is unable to learn, and exhibits highly divergent value estimates.\nOff-Policy Deep Reinforcement Learning without Exploration: Supplementary Material\nB.2. Complete Experimental Results\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n2000\n4000\n6000\n8000\nAverage Return\nHalfCheetah-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n500\n1000\n1500\nHopper-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n500\n1000\n1500\n2000\n2500\nWalker2d-v1\n(a) Final buffer performance\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n200\n400\n600\n800\n1000\nEstimated Value\nHalfCheetah-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n200\n400\n600\n800\n1000\nHopper-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n200\n400\n600\n800\n1000\nWalker2d-v1\n(b) Final buffer value estimates\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n2000\n4000\n6000\n8000\n10000\nAverage Return\nHalfCheetah-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n500\n1000\n1500\n2000\nHopper-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n500\n1000\n1500\n2000\n2500\n3000\nWalker2d-v1\n(c) Concurrent performance\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nEstimated Value\nHalfCheetah-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nHopper-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime steps (1e6)\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nWalker2d-v1\n(d) Concurrent value estimates\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n2000\n4000\n6000\n8000\n10000\nAverage Return\nHalfCheetah-v1\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nHopper-v1\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n1000\n2000\n3000\nWalker2d-v1\n(e) Imitation performance\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nEstimated Value\nHalfCheetah-v1\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nHopper-v1\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nWalker2d-v1\n(f) Imitation value estimates\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n2000\n4000\n6000\nAverage Return\nHalfCheetah-v1\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n500\n1000\n1500\n2000\n2500\n3000\nHopper-v1\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n500\n1000\n1500\n2000\n2500\nWalker2d-v1\n(g) Imperfect demonstrations performance\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nEstimated Value\nHalfCheetah-v1\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nHopper-v1\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nWalker2d-v1\n(h) Imperfect demonstrations value estimates\nFigure 5. We evaluate BCQ and several baselines on the experiments from Section 3.1, as well as a new imperfect demonstration task.\nPerformance is graphed on the left, and value estimates are graphed on the right. The shaded area represents half a standard deviation.\nThe bold black line measures the average return of episodes contained in the batch. For the value estimates, each individual trial is plotted,\nwith the mean in bold. An estimate of the true value of BCQ, evaluated by Monte Carlo returns, is marked by a dotted line. Only BCQ\nmatches or outperforms the performance of the behavioral policy in all tasks, while exhibiting a highly stable value function in each task.\nWe present the complete set of results across each task and environment in Figure 5. These results show that BCQ\nsuccessful mitigates extrapolation error and learns from a variety of ﬁxed batch settings. Although BCQ with our current\nhyper-parameters was never found to fail, we noted with slight changes to hyper-parameters, BCQ failed periodically on\nthe concurrent learning task in the HalfCheetah-v1 environment, exhibiting instability in the value function after 750, 000\nor more iterations on some seeds. We hypothesize that this instability could occur if the generative model failed to output\nin-distribution actions, and could be corrected through additional training or improvements to the vanilla VAE. Interestingly,\nBCQ still performs well in these instances, due to the behavioral cloning-like elements in the algorithm.\nOff-Policy Deep Reinforcement Learning without Exploration: Supplementary Material\nC. Extrapolation Error in Kernel-Based Reinforcement Learning\nThis problem of extrapolation persists in traditional batch reinforcement learning algorithms, such as kernel-based rein-\nforcement learning (KBRL) (Ormoneit & Sen, 2002). For a given batch B of transitions (s, a, r, s′), non-negative density\nfunction φ : R+ →R+, hyper-parameter τ ∈R, and norm || · ||, KBRL evaluates the value of a state-action pair (s,a) as\nfollows:\nQ(s, a) =\nX\n(sa\nB,a,r,s′\nB)∈B\nκa\nτ(s, sa\nB)[r + γV (s′\nB)],\n(19)\nκa\nτ(s, sa\nB) =\nkτ(s, sa\nB)\nP\n˜sa\nB kτ(s, ˜sa\nB),\nkτ(s, sa\nB) = φ\n\u0012||s −sa\nB||\nτ\n\u0013\n,\n(20)\nwhere sa\nB\n∈\nS represents states corresponding to the action a for some tuple (sB, a)\n∈\nB, and V (s′\nB)\n=\nmaxa s.t.(s′\nB,a)∈B Q(s′\nB, a). At each iteration, KBRL updates the estimates of Q(sB, aB) for all (sB, aB) ∈B follow-\ning Equation (19), then updates V (s′\nB) by evaluating Q(s′\nB, a) for all sB ∈B and a ∈A.\ns0\ns1\na1, r = 1\na0, r = 0\na0, r = 0\na1, r = 0\nFigure 6. Toy MDP with two states s0 and s1, and two actions a0 and a1. Agent receives reward of 1 for selecting a1 at s0 and 0\notherwise.\nGiven access to the entire deterministic MDP, KBRL will provable converge to the optimal value, however when limited to\nonly a subset, we ﬁnd the value estimation susceptible to extrapolation. In Figure 6, we provide a deterministic two state,\ntwo action MDP in which KBRL fails to learn the optimal policy when provided with state-action pairs from the optimal\npolicy. Given the batch {(s0, a1, r = 1, s1), (s1, a0, r = 0, s0)}, corresponding to the optimal behavior, and noting that\nthere is only one example of each action, Equation (19) provides the following:\nQ(·, a1) = 1 + γV (s1) = 1 + γQ(s1, a0),\nQ(·, a0) = γV (s0) = γQ(s0, a1).\n(21)\nAfter sufﬁcient iterations KBRL will converge correctly to Q(s0, a1) =\n1\n1−γ2 , Q(s1, a0) =\nγ\n1−γ2 . However, when\nevaluating actions, KBRL erroneously extrapolates the values of each action Q(·, a1) =\n1\n1−γ2 , Q(·, a0) =\nγ\n1−γ2 , and its\nbehavior, argmaxa Q(s, a), will result in the degenerate policy of continually selecting a1. KBRL fails this example by\nestimating the values of unseen state-action pairs. In methods where the extrapolated estimates can be included into the\nlearning update, such ﬁtted Q-iteration or DQN (Ernst et al., 2005; Mnih et al., 2015), this could cause an unbounded\nsequence of value estimates, as demonstrated by our results in Section 3.1.\nOff-Policy Deep Reinforcement Learning without Exploration: Supplementary Material\nD. Additional Experiments\nD.1. Ablation Study of Perturbation Model\nBCQ includes a perturbation model ξθ(s, a, Φ) which outputs a small residual update to the actions sampled by the generative\nmodel in the range [−Φ, Φ]. This enables the policy to select actions which may not have been sampled by the generative\nmodel. If Φ = amax −amin, then all actions can be plausibly selected by the model, similar to standard deep reinforcement\nlearning algorithms, such as DQN and DDPG (Mnih et al., 2015; Lillicrap et al., 2015). In Figure 7 we examine the\nperformance and value estimates of BCQ when varying the hyper-parameter Φ, which corresponds to how much the model\nis able to move away from the actions sampled by the generative model.\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n2000\n4000\n6000\n8000\n10000\nAverage Return\nHalfcheetah-v1\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nHopper-v1\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nWalker2d-v1\n(a) Imitation performance\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n200\n400\n600\n800\n1000\n1200\n1400\nEstimated Value\nHalfCheetah-v1\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n500\n1000\n1500\n2000\nHopper-v1\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n200\n400\n600\n800\nWalker2d-v1\n(b) Imitation value estimates\nFigure 7. We perform an ablation study on the perturbation model of BCQ, on the imitation task from Section 3.1. Performance is graphed\non the left, and value estimates are graphed on the right. The shaded area represents half a standard deviation. The bold black line\nmeasures the average return of episodes contained in the batch. For the value estimates, each individual trial is plotted, with the mean in\nbold.\nWe observe a clear drop in performance with the increase of Φ, along with an increase in instability in the value function.\nGiven that the data is based on expert performance, this is consistent with our understanding of extrapolation error. With\nlarger Φ the agent learns to take actions that are further away from the data in the batch after erroneously overestimating the\nvalue of suboptimal actions. This suggests the ideal value of Φ should be small enough to stay close to the generated actions,\nbut large enough such that learning can be performed when exploratory actions are included in the dataset.\nOff-Policy Deep Reinforcement Learning without Exploration: Supplementary Material\nD.2. Uncertainty Estimation for Batch-Constrained Reinforcement Learning\nSection 4.2 proposes using generation as a method for constraining the output of the policy π to eliminate actions which are\nunlikely under the batch. However, a more natural approach would be through approximate uncertainty-based methods\n(Osband et al., 2016; Gal et al., 2016; Azizzadenesheli et al., 2018). These methods are well-known to be effective for\nexploration, however we examine their properties for the exploitation where we would like to avoid uncertain actions.\nTo measure the uncertainty of the value network, we use ensemble-based methods, with an ensemble of size 4 and 10 to\nmimic the models used by Buckman et al. (2018) and Osband et al. (2016) respectively. Each network is trained with\nseparate mini-batches, following the standard deep Q-learning update with a target network. These networks use the default\narchitecture and hyper-parameter choices as deﬁned in Section G. The policy πφ is trained to minimize the standard deviation\nσ across the ensemble:\nφ ←argmin\nφ\nX\n(s,a)∈B\nσ\n\u0000{Qθi(s, a)}N\ni=1\n\u0001\n.\n(22)\nIf the ensembles were a perfect estimate of the uncertainty, the policy would learn to select the most certain action for a\ngiven state, minimizing the extrapolation error and effectively imitating the data in the batch.\nTo test these uncertainty-based methods, we examine their performance on the imitation task in the Hopper-v1 environment\n(Todorov et al., 2012; Brockman et al., 2016). In which a dataset of 1 million expert transitions are provided to the agents.\nAdditional experimental details can be found in Section F. The performance, alongside the value estimates of the agents are\ndisplayed in Figure 8.\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nAverage Return\n0.0\n0.1\n0.2\n0.3\nTime steps (1e6)\n0\n100\n200\n300\n400\n500\nEstimated Value\nBCQ\nDDPG\nEnsemble-4\nEnsemble-10\n(a) Imitation performance\n(b) Imitation value estimates\nFigure 8. A comparison with uncertainty-based methods on the Hopper-v1 environment from OpenAI gym, on the imitation task. Although\nensemble based methods are able to generate stable value functions (right) under these conditions, they fail to constrain the action space to\nthe demonstrated expert actions and suffer in performance compared to our approach, BCQ (left).\nWe ﬁnd that neither ensemble method is sufﬁcient to constrain the action space to only the expert actions. However, the\nvalue function is stabilized, suggesting that ensembles are an effective strategy for eliminating outliers or large deviations\nin the value from erroneous extrapolation. Unsurprisingly, the large ensemble provides a more accurate estimate of the\nuncertainty. While scaling the size of the ensemble to larger values could possibly enable an effective batch-constraint,\nincreasing the size of the ensemble induces a large computational cost. Finally, in this task, where only expert data is\nprovided, the policy can attempt to imitate the data without consideration of the value, however in other tasks, a weighting\nbetween value and uncertainty would need to be carefully tuned. On the other hand, BCQ offers a computational cheap\napproach without requirements for difﬁcult hyper-parameter tuning.\nOff-Policy Deep Reinforcement Learning without Exploration: Supplementary Material\nD.3. Random Behavioral Policy Study\nThe experiments in Section 3.1 and 5 use a learned, or partially learned, behavioral policy for data collection. This is a\nnecessary requirement for learning meaningful behavior, as a random policy generally fails to provide sufﬁcient coverage\nover the state space. However, in simple toy problems, such as the pendulum swing-up task and the reaching task with\na two-joint arm from OpenAI gym (Brockman et al., 2016), a random policy can sufﬁciently explore the environment,\nenabling us to examine the properties of algorithms with entirely non-expert data.\nIn Figure 9, we examine the performance of our algorithm, BCQ, as well as DDPG (Lillicrap et al., 2015), on these two\ntoy problems, when learning off-policy from a small batch of 5000 time steps, collected entirely by a random policy. We\nﬁnd that both BCQ and DDPG are able to learn successfully in these off-policy tasks. These results suggest that BCQ is\nless restrictive than imitation learning algorithms, which require expert data to learn. We also ﬁnd that unlike previous\nenvironments, given the small scale of the state and action space, the random policy is able to provide sufﬁcient coverage for\nDDPG to learn successfully.\n0\n5\n10\n15\n20\nTime steps (1e3)\n−1750\n−1500\n−1250\n−1000\n−750\n−500\n−250\nAverage Return\nPendulum-v0\n0\n5\n10\n15\n20\nTime steps (1e3)\n−100\n−80\n−60\n−40\n−20\nReacher-v1\n0\n5\n10\n15\n20\nTime steps (1e3)\n−600\n−500\n−400\n−300\n−200\n−100\n0\n100\nEstimated Value\nPendulum-v0\nBCQ\nDDPG\nTrue BCQ\nTrue DDPG\n0\n5\n10\n15\n20\nTime steps (1e3)\n−20\n−15\n−10\n−5\n0\nReacher-v1\n(a) Random behavioral performance\n(b) Random behavioral value estimates\nFigure 9. We evaluate BCQ and DDPG on a batch collected by a random behavioral policy. The shaded area represents half a standard\ndeviation. Value estimates include a plot of each trial, with the mean in bold. An estimate of the true value of BCQ and DDPG, evaluated\nby Monte Carlo returns, is marked by a dotted line. While both BCQ and DDPG perform well, BCQ exhibits more stability in the value\nfunction for the Pendulum task, and outperforms DDPG in the Reacher task. These tasks demonstrate examples where any imitation\nlearning would fail as the data collection procedure is entirely random.\nE. Missing Background\nE.1. Variational Auto-Encoder\nA variational auto-encoder (VAE) (Kingma & Welling, 2013) is a generative model which aims to maximize the marginal\nlog-likelihood log p(X) = PN\ni=1 log p(xi) where X = {x1, ..., xN}, the dataset. While computing the marginal likelihood\nis intractable in nature, we can instead optimize the variational lower-bound:\nlog p(X) ≥Eq(X|z)[log p(X|z)] + DKL(q(z|X)||p(z)),\n(23)\nwhere p(z) is chosen a prior, generally the multivariate normal distribution N(0, I). We deﬁne the posterior q(z|X) =\nN(z|µ(X), σ2(X)I) as the encoder and p(X|z) as the decoder. Simply put, this means a VAE is an auto-encoder, where a\ngiven sample x is passed through the encoder to produce a random latent vector z, which is given to the decoder to reconstruct\nthe original sample x. The VAE is trained on a reconstruction loss, and a KL-divergence term according to the distribution\nof the latent vectors. To perform gradient descent on the variational lower bound we can use the re-parametrization trick\n(Kingma & Welling, 2013; Rezende et al., 2014):\nEz∼N(µ,σ)[f(z)] = Eϵ∼N(0,I)[f(µ + σϵ)].\n(24)\nThis formulation allows for back-propagation through stochastic nodes, by noting µ and σ can be represented by deterministic\nfunctions. During inference, random values of z are sampled from the multivariate normal and passed through the decoder\nto produce samples x.\nOff-Policy Deep Reinforcement Learning without Exploration: Supplementary Material\nF. Experimental Details\nEach environment is run for 1 million time steps, unless stated otherwise, with evaluations every 5000 time steps, where\nan evaluation measures the average reward from 10 episodes with no exploration noise. Our results are reported over 5\nrandom seeds of the behavioral policy, OpenAI Gym simulator and network initialization. Value estimates are averaged over\nmini-batches of 100 and sampled every 2500 iterations. The true value is estimated by sampling 100 state-action pairs from\nthe buffer replay and computing the discounted return by running the episode until completion while following the current\npolicy.\nEach agent is trained after each episode by applying one training iteration per each time step in the episode. The agent is\ntrained with transition tuples (s, a, r, s′) sampled from an experience replay that is deﬁned by each experiment. We deﬁne\nfour possible experiments. Unless stated otherwise, default implementation settings, as deﬁned in Section G, are used.\nBatch 1 (Final buffer). We train a DDPG (Lillicrap et al., 2015) agent for 1 million time steps, adding large amounts of\nGaussian noise (N(0, 0.5)) to induce exploration, and store all experienced transitions in a buffer replay. This training\nprocedure creates a buffer replay with a diverse set of states and actions. A second, randomly initialized agent is trained\nusing the 1 million stored transitions.\nBatch 2 (Concurrent learning). We simultaneously train two agents for 1 million time steps, the ﬁrst DDPG agent,\nperforms data collection and each transition is stored in a buffer replay which both agents learn from. This means the\nbehavioral agent learns from the standard training regime for most off-policy deep reinforcement learning algorithms, and\nthe second agent is learning off-policy, as the data is collected without direct relationship to its current policy. Batch 2\ndiffers from Batch 1 as the agents are trained with the same version of the buffer, while in Batch 1 the agent learns from the\nﬁnal buffer replay after the behavioral agent has ﬁnished training.\nBatch 3 (Imitation). A DDPG agent is trained for 1 million time steps. The trained agent then acts as an expert policy, and\nis used to collect a dataset of 1 million transitions. This dataset is used to train a second, randomly initialized agent. In\nparticular, we train DDPG across 15 seeds, and select the 5 top performing seeds as the expert policies.\nBatch 4 (Imperfect demonstrations). The expert policies from Batch 3 are used to collect a dataset of 100k transitions,\nwhile selecting actions randomly with probability 0.3 and adding Gaussian noise N(0, 0.3) to the remaining actions. This\ndataset is used to train a second, randomly initialized agent.\nG. Implementation Details\nAcross all methods and experiments, for fair comparison, each network generally uses the same hyper-parameters and\narchitecture, which are deﬁned in Table 1 and Figure 10 respectively. The value functions follow the standard practice (Mnih\net al., 2015) in which the Bellman update differs for terminal transitions. When the episode ends by reaching some terminal\nstate, the value is set to 0 in the learning target y:\ny =\n(\nr\nif terminal s′\nr + γQθ′(s′, a′)\nelse\n(25)\nWhere the termination signal from time-limited environments is ignored, thus we only consider a state st terminal if t <\nmax horizon.\nTable 1. Default hyper-parameters.\nHyper-parameter\nValue\nOptimizer\nAdam (Kingma & Ba, 2014)\nLearning Rate\n10−3\nBatch Size\n100\nNormalized Observations\nFalse\nGradient Clipping\nFalse\nDiscount Factor\n0.99\nTarget Update Rate (τ)\n0.005\nExploration Policy\nN(0, 0.1)\nOff-Policy Deep Reinforcement Learning without Exploration: Supplementary Material\n(input dimension, 400)\nReLU\n(400, 300)\nRelU\n(300, output dimension)\nFigure 10. Default network architecture, based on DDPG (Lillicrap et al., 2015). All actor networks are followed by a tanh · max\naction size\nBCQ. BCQ uses four main networks: a perturbation model ξφ(s, a), a state-conditioned VAE Gω(s) and a pair of value\nnetworks Qθ1(s, a), Qθ2(s, a). Along with three corresponding target networks ξφ′(s, a), Qθ′\n1(s, a), Qθ′\n2(s, a). Each\nnetwork, other than the VAE follows the default architecture (Figure 10) and the default hyper-parameters (Table 1). For\nξφ(s, a, Φ), the constraint Φ is implemented through a tanh activation multiplied by I · Φ following the ﬁnal layer.\nAs suggested by Fujimoto et al. (2018), the perturbation model is trained only with respect to Qθ1, following the deterministic\npolicy gradient algorithm (Silver et al., 2014), by performing a residual update on a single action sampled from the generative\nmodel:\nφ ←argmax\nφ\nX\n(s,a)∈B\nQθ1(s, a + ξφ(s, a, Φ)),\na ∼Gω(s).\n(26)\nTo penalize uncertainty over future states, we train a pair of value estimates {Qθ1, Qθ2} and take a weighted minimum\nbetween the two values as a learning target y for both networks. First n actions are sampled with respect to the generative\nmodel, and then adjusted by the target perturbation model, before passed to each target Q-network:\ny = r + γ max\nai\n\u0014\nλ min\nj=1,2 Qθ′\nj(s′, ˜ai) + (1 −λ) max\nj=1,2 Qθ′\nj(s′, ˜ai)\n\u0015\n,\n{˜ai = ai + ξφ′(s, ai, Φ)}n\ni=0,\n{ai ∼Gω(s′)}n\ni=0,\nLvalue,i =\nX\n(s,a,r,s′)∈B\n(y −Qθi(s, a))2.\n(27)\nBoth networks are trained with the same target y, where λ = 0.75.\nThe VAE Gω is deﬁned by two networks, an encoder Eω1(s, a) and decoder Dω2(s, z), where ω = {ω1, ω2}. The encoder\ntakes a state-action pair and outputs the mean µ and standard deviation σ of a Gaussian distribution N(µ, σ). The state s,\nalong with a latent vector z is sampled from the Gaussian, is passed to the decoder Dω2(s, z) which outputs an action. Each\nnetwork follows the default architecture (Figure 10), with two hidden layers of size 750, rather than 400 and 300. The VAE\nis trained with respect to the mean squared error of the reconstruction along with a KL regularization term:\nLreconstruction =\nX\n(s,a)∈B\n(Dω2(s, z) −a)2,\nz = µ + σ · ϵ,\nϵ ∼N(0, 1),\n(28)\nLKL = DKL(N(µ, σ)||N(0, 1)),\n(29)\nLVAE = Lreconstruction + λLKL.\n(30)\nNoting the Gaussian form of both distributions, the KL divergence term can be simpliﬁed (Kingma & Welling, 2013):\nDKL(N(µ, σ)||N(0, 1)) = −1\n2\nJ\nX\nj=1\n(1 + log(σ2\nj ) −µ2\nj −σ2\nj ),\n(31)\nwhere J denotes the dimensionality of z. For each experiment, J is set to twice the dimensionality of the action space.\nThe KL divergence term in LVAE is normalized across experiments by setting λ =\n1\n2J . During inference with the VAE, the\nlatent vector z is clipped to a range of [−0.5, 0.5] to limit generalization beyond previously seen actions. For the small scale\nexperiments in Supplementary Material D.3, L2 regularization with weight 10−3 was used for the VAE to compensate for\nOff-Policy Deep Reinforcement Learning without Exploration: Supplementary Material\nthe small number of samples. The other networks remain unchanged. In the value network update (Equation 27), the VAE is\nsampled multiple times and passed to both the perturbation model and each Q-network. For each state in the mini-batch\nthe VAE is sampled n = 10 times. This can be implemented efﬁciently by passing a latent vector with batch size 10 ·\nbatch size, effectively 1000, to the VAE and treating the output as a new mini-batch for the perturbation model and\neach Q-network. When running the agent in the environment, we sample from the VAE 10 times, perturb each action with\nξφ and sample the highest valued action with respect to Qθ1.\nDDPG. Our DDPG implementation deviates from some of the default architecture and hyper-parameters to mimic the\noriginal implementation more closely (Lillicrap et al., 2015). In particular, the action is only passed to the critic at the second\nlayer (Figure 11), the critic uses L2 regularization with weight 10−2, and the actor uses a reduced learning rate of 10−4.\n(state dimension, 400)\nReLU\n(400 + action dimension, 300)\nRelU\n(300, 1)\nFigure 11. DDPG Critic Network Architecture.\nAs done by Fujimoto et al. (2018), our DDPG agent randomly selects actions for the ﬁrst 10k time steps for HalfCheetah-v1,\nand 1k time steps for Hopper-v1 and Walker2d-v1. This was found to improve performance and reduce the likelihood of\nlocal minima in the policy during early iterations.\nDQN. Given the high dimensional nature of the action space of the experimental environments, our DQN implementation\nselects actions over an independently discretized action space. Each action dimension is discretized separately into 10\npossible actions, giving 10J possible actions, where J is the dimensionality of the action-space. A given state-action\npair (s, a) then corresponds to a set of state-sub-action pairs (s, aij), where i ∈{1, ..., 10} bins and j = {1, ..., J}\ndimensions. In each DQN update, all state-sub-action pairs (s, aij) are updated with respect to the average value of the\ntarget state-sub-action pairs (s′, a′\nij). The learning update of the discretized DQN is as follows:\ny = r + γ\nn\nJ\nX\nj=1\nmax\ni\nQθ′(s′, a′\nij),\n(32)\nθ ←argmin\nθ\nX\n(s,a,r,s′)∈B\nJ\nX\nj=1\n(y −Qθ(s, aij))2 ,\n(33)\nWhere aij is chosen by selecting the corresponding bin i in the discretized action space for each dimension j. For clarity,\nwe provide the exact DQN network architecture in Figure 12.\n(state dimension, 400)\nReLU\n(400, 300)\nRelU\n(300, 10 action dimension)\nFigure 12. DQN Network Architecture.\nBehavioral Cloning. We use two behavioral cloning methods, VAE-BC and BC. VAE-BC is implemented and trained\nexactly as Gω(s) deﬁned for BCQ. BC uses a feed-forward network with the default architecture and hyper-parameters, and\ntrained with a mean-squared error reconstruction loss.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2018-12-07",
  "updated": "2019-08-10"
}