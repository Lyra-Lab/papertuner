{
  "id": "http://arxiv.org/abs/1812.03794v3",
  "title": "Unsupervised Deep Learning for Structured Shape Matching",
  "authors": [
    "Jean-Michel Roufosse",
    "Abhishek Sharma",
    "Maks Ovsjanikov"
  ],
  "abstract": "We present a novel method for computing correspondences across 3D shapes\nusing unsupervised learning. Our method computes a non-linear transformation of\ngiven descriptor functions, while optimizing for global structural properties\nof the resulting maps, such as their bijectivity or approximate isometry. To\nthis end, we use the functional maps framework, and build upon the recent FMNet\narchitecture for descriptor learning. Unlike that approach, however, we show\nthat learning can be done in a purely \\emph{unsupervised setting}, without\nhaving access to any ground truth correspondences. This results in a very\ngeneral shape matching method that we call SURFMNet for Spectral Unsupervised\nFMNet, and which can be used to establish correspondences within 3D shape\ncollections without any prior information. We demonstrate on a wide range of\nchallenging benchmarks, that our approach leads to state-of-the-art results\ncompared to the existing unsupervised methods and achieves results that are\ncomparable even to the supervised learning techniques. Moreover, our framework\nis an order of magnitude faster, and does not rely on geodesic distance\ncomputation or expensive post-processing.",
  "text": "Unsupervised Deep Learning for Structured Shape Matching\nJean-Michel Roufosse\nLIX, ´Ecole Polytechnique\njm.roufosse@gmail.com\nAbhishek Sharma\nLIX, ´Ecole Polytechnique\nkein.iitian@gmail.com\nMaks Ovsjanikov\nLIX, ´Ecole Polytechnique\nmaks@lix.polytechnique.fr\nAbstract\nWe present a novel method for computing correspon-\ndences across 3D shapes using unsupervised learning. Our\nmethod computes a non-linear transformation of given de-\nscriptor functions, while optimizing for global structural\nproperties of the resulting maps, such as their bijectivity\nor approximate isometry.\nTo this end, we use the func-\ntional maps framework, and build upon the recent FMNet\narchitecture for descriptor learning. Unlike that approach,\nhowever, we show that learning can be done in a purely\nunsupervised setting, without having access to any ground\ntruth correspondences. This results in a very general shape\nmatching method that we call SURFMNet for Spectral Un-\nsupervised FMNet, and which can be used to establish cor-\nrespondences within 3D shape collections without any prior\ninformation. We demonstrate on a wide range of challeng-\ning benchmarks, that our approach leads to state-of-the-art\nresults compared to the existing unsupervised methods and\nachieves results that are comparable even to the supervised\nlearning techniques. Moreover, our framework is an order\nof magnitude faster, and does not rely on geodesic distance\ncomputation or expensive post-processing.\n1. Introduction\nShape matching is a fundamental problem in computer\nvision and geometric data analysis, with applications in de-\nformation transfer [42] and statistical shape modeling [6]\namong other domains.\nDuring the past decades, a large\nnumber of techniques have been proposed for both rigid\nand non-rigid shape matching [44]. The latter case is both\nmore general and more challenging since the shapes can\npotentially undergo arbitrary deformations (See Figure 1),\nwhich are not easy to characterize by purely axiomatic ap-\nproaches. As a result, several recent learning-based tech-\nniques have been proposed for addressing the shape cor-\nrespondence problem, e.g. [10, 25, 26, 51] among many\nothers.\nMost of these approaches are based on the idea\nthat the underlying correspondence model can be learned\nfrom data, typically given in the form of ground truth corre-\nSource descriptor before\nTarget descriptor before\nSource descriptor after\nTarget descriptor after\nFigure 1: Given a pair of shapes with noisy descriptors\n(top), our approach makes them more consistent (bottom)\nwithout the knowledge of the underlying map, and auto-\nmatically computes an accurate pointwise correspondence.\nspondences between some shape pairs. In the simplest case,\nthis can be formulated as a labeling problem, where differ-\nent points, e.g., in a template shape, correspond to labels\nto be predicted [51, 27]. More recently, several methods\nhave been proposed for structured map prediction, aiming\nto infer an entire map, rather than labeling each point in-\n1\narXiv:1812.03794v3  [cs.GR]  22 Aug 2019\ndependently [10, 23]. These techniques are based on learn-\ning pointwise descriptors, but, crucially, impose a penalty\non the entire map, obtained using these descriptors, result-\ning in higher quality, globally consistent correspondences.\nNevertheless, while learning-based methods have achieved\nimpressive performance, their utility is severely limited by\nrequiring the presence of high-quality ground truth maps\nbetween a sufﬁcient number of training examples.\nThis\nmakes it difﬁcult to apply such approaches to new shape\nclasses for which ground truth data is not available.\nIn our paper, we show that this limitation can be lifted\nand propose a purely unsupervised strategy, which com-\nbines the accuracy of learning-based methods with the gen-\nerality of axiomatic techniques for shape correspondence.\nThe key to our approach is a bi-level optimization scheme,\nwhich optimizes for descriptors on the shapes, but imposes\na penalty on the entire map, inferred from them. For this, we\nuse the recently proposed FMNet architecture [23], which\nexploits the functional map representation [30]. However,\nrather than penalizing the deviation of the map from the\nground truth, we enforce structural properties on the map,\nsuch as its bijectivity or approximate isometry. This results\nin a shape matching method that achieves state-of-the-art\naccuracy among unsupervised methods and, perhaps sur-\nprisingly, achieves comparable performance even to super-\nvised techniques.\n2. Related Work\nComputing correspondences between 3D shapes is a\nvery well-studied area of computer vision and computer\ngraphics. Below we only review the most closely related\nmethods and refer the interested readers to recent surveys\nincluding [46, 44, 5] for more in-depth discussions.\nFunctional Maps Our method is built on the functional\nmap representation, which was originally introduced in\n[30] for solving non-rigid shape matching problems, and\nthen extended signiﬁcantly in follow-up works, including\n[2, 21, 20, 9, 15, 36] among many others (see also [31] for\na recent overview).\nOne of the key beneﬁts of this framework is that it al-\nlows us to represent maps between shapes as small matri-\nces, which encode relations between basis functions deﬁned\non the shapes. Moreover, as observed by several works in\nthis domain [30, 40, 21, 36, 9], many natural properties on\nthe underlying pointwise correspondences can be expressed\nas objectives on functional maps. This includes orthonor-\nmality of functional maps, which corresponds to the lo-\ncal area-preservation nature of pointwise correspondences\n[30, 21, 40]; commutativity with the Laplacian operators,\nwhich corresponds to intrinsic isometries [30], preservation\nof inner products of gradients of functions, which corre-\nsponds to conformal maps [40, 9, 50]; preservation of point-\nwise products of functions, which corresponds to functional\nmaps arising from point-to-point correspondences [29, 28];\nand slanted diagonal structure of functional map in the con-\ntext of partial shapes [36, 24] among others.\nSimilarly, several other regularizers have been proposed,\nincluding exploiting the relation between functional maps\nin different directions [14], the map adjoint [18], and pow-\nerful cycle-consistency constraints [17] in shape collec-\ntions to name a few. More recently constraints on func-\ntional maps have been introduced to promote map continu-\nity [35, 34] and kernel-based techniques for extracting more\ninformation from given descriptors [49] among others. All\nthese methods, however, are based on combining ﬁrst-order\npenalties that arise from enforcing descriptor preservation\nconstraints with these additional desirable structural prop-\nerties of functional maps. As a result, any artefact or in-\nconsistency in the pre-computed descriptors will inevitably\nlead to severe map estimation errors. Several methods have\nbeen suggested to use robust norms [21, 20], which can help\nreduce the inﬂuence of certain descriptors but still does not\ncontrol the global map consistency properties.\nMost recently, a powerful technique BCICP, for map op-\ntimization, was introduced in [35] that combines a large\nnumber of functional constraints with sophisticated post-\nprocessing, and careful descriptor selection. As we show\nbelow our method is simpler, more efﬁcient and achieves\nsuperior accuracy even to this recent approach.\nLearning-based Methods To overcome the inherent dif-\nﬁculty of axiomatic techniques, several methods have been\nintroduced to learn the correct deformation model from data\nwith learning-based methods. Some early approaches in\nthis direction were used to learn either optimal parame-\nters of spectral descriptors [25] or exploited random forests\n[38] or metric learning [11] for learning optimal constraints\ngiven some ground truth matches.\nMore recently, with the advent of deep learning methods,\nseveral approaches have been proposed to learn transforma-\ntions in the context of non-rigid shape matching. Most of\nthe proposed methods either use Convolutional Neural Net-\nworks (CNNs) on depth maps, e.g. for dense human body\ncorrespondence [51] or exploit extensions of CNNs directly\nto curved surfaces, either using the link between convolu-\ntion and multiplication in the spectral domain [7, 12], or di-\nrectly deﬁning local parametrizations, for example via the\nexponential map, which allows convolution in the tangent\nplane of a point, e.g. [26, 8, 27, 33] among others.\nThese methods have been applied to non-rigid shape\nmatching, in most cases modeling it as a label prediction\nproblem, with points corresponding to different labels. Al-\nthough successful in the presence of sufﬁcient training data,\nsuch approaches typically do not impose global consistency,\nand can lead to artefacts, such as outliers, requiring post-\nprocessing to achieve high-quality maps.\nLearning for Structured Prediction Most closely related\nto our approach are recent works that apply learning for\nstructured map prediction [10, 23]. These methods learn a\ntransformation of given input descriptors, while optimizing\nfor the deviation of the map computed from them using the\nfunctional map framework, from ground truth correspon-\ndences. By imposing a penalty on entire maps, and thus\nevaluating the ultimate use of the descriptors, these methods\nhave led to signiﬁcant accuracy improvements in practice.\nWe note that concurrent to our work, Halimi et al. [16] also\nproposed an unsupervised deep learning method that com-\nputes correspondences without using the ground truth. This\napproach is similar to ours, but is based on computation of\ngeodesic distances, while our method operates purely in the\nspectral domain making it extremely efﬁcient.\nContribution Unlike these existing methods, we propose\nan unsupervised learning-based approach that transforms\ngiven input descriptors, while optimizing for structural map\nproperties, without any knowledge of the ground truth or\ngeodesic distances. Our method, which can be seen as a bi-\nlevel optimization strategy, allows to explicitly control the\ninteraction between pointwise descriptors and global map\nconsistency, computed via the functional map framework.\nAs a result, our technique is scalable with respect to shape\ncomplexity, leads to signiﬁcant improvement compared to\nthe standard unsupervised methods, and achieves compara-\nble performance even to supervised approaches.\n3. Background & Motivation\n3.1. Shape Matching and Functional Maps\nOur work is based on the functional map framework and\nrepresentation. For completeness, we brieﬂy review the ba-\nsic notions and pipeline for estimating functional maps, and\nrefer the interested reader to a recent course [31] for a more\nin-depth discussion.\nBasic Pipeline Given a pair of shapes, S1, S2 represented\nas triangle meshes, and containing, respectively, n1 and n2\nvertices, the basic pipeline for computing a map between\nthem using the functional map framework, consists of the\nfollowing main steps (see Chapter 2 in [31]) :\n1. Compute a small set of k1, k2 of basis functions on\neach shape, e.g. by taking the ﬁrst few eigenfunctions\nof the respective Laplace-Beltrami operators.\n2. Compute a set of descriptor functions on each shape\nthat are expected to be approximately preserved by\nthe unknown map. For example, a descriptor function\ncan correspond to a particular dimension (e.g. choice\nof time parameter of the Heat Kernel Signature [43])\ncomputed at every point. Store their coefﬁcients in the\nrespective bases as columns of matrices A1, A2.\n3. Compute the optimal functional map C by solving the\nfollowing optimization problem:\nCopt = arg min\nC12\nEdesc\n\u0000C12\n\u0001\n+ αEreg\n\u0000C12\n\u0001\n,\n(1)\nwhere the ﬁrst term aims at the descriptor preservation:\nEdesc\n\u0000C12\n\u0001\n=\n\r\rC12A1 −A2\n\r\r2, whereas the second\nterm regularizes the map by promoting the correctness\nof its overall structural properties. The simplest ap-\nproach penalizes the failure of the unknown functional\nmap to commute with the Laplace-Beltrami operators:\nEreg(C12) =\n\r\rC12Λ1 −Λ2C12\n\r\r2\n(2)\nwhere Λ1 and Λ2 are diagonal matrices of the Laplace-\nBeltrami eigenvalues on the two shapes.\n4. Convert the functional map C to a point-to-point map,\nfor example using nearest neighbor search in the spec-\ntral embedding, or using other more advanced tech-\nniques [37, 15].\nOne of the strengths of this pipeline is that typically\nEq. (1) leads to a simple (e.g., least squares) problem with\nk1k2 unknowns, independent of the number of points on the\nshapes. This formulation has been extended using e.g. man-\nifold optimization [22], descriptor preservation constraints\nvia commutativity [29] and, more recently, with kerneliza-\ntion [49] among many others (see also Chapter 3 in [31]).\n3.2. Deep Functional Maps\nDespite its simplicity and efﬁciency, the functional map\nestimation pipeline described above is fundamentally de-\npendent on the initial choice of descriptor functions. To al-\nleviate this dependence, several approaches have been pro-\nposed to learn the optimal descriptors from data [10, 23].\nIn our work, we build upon a recent deep learning-based\nframework, called FMNet, introduced by Litany et al. [23]\nthat aims to transform a given set of descriptors so that the\noptimal map computed using them is as close as possible to\nsome ground truth map given during training.\nSpeciﬁcally, the approach proposed in [23] assumes, as\ninput, a set of shape pairs for which ground truth point-wise\nmaps are known, and aims to solve the following problem:\nmin\nT\nX\n(S1,S2)∈Train\nlF (Soft(Copt), GT(S1,S2)), where\n(3)\nCopt = arg min\nC\n∥CAT (D1) −AT (D2)∥.\n(4)\nHere T is a non-linear transformation, in the form of a neu-\nral network, to be applied to some input descriptor functions\nD, Train is the set of training pairs for which ground truth\ncorrespondence GT(S1,S2) is known, lF is the soft error\nloss, which penalizes the deviation of the computed func-\ntional map Copt, after converting it to a soft map Soft(Copt)\nFM\nNet\nFM\nNet\nFigure 2: Overview of our SURFMNet approach: given a\npair of shapes and their descriptors D1, D2, we optimize for\na non-linear transformation T using the FMNet architecture\nso that the transformed descriptors lead to functional maps\nthat best satisfy the structural constraints.\nfrom the ground truth correspondence, and AT (D1) de-\nnotes the transformed descriptors D1 written in the basis of\nshape 1. In other words, the FMNet framework [23] aims\nto learn a transformation T of descriptors, so that the trans-\nformed descriptors T(D1), T(D2), when used within the\nfunctional map pipeline result in a soft map that is as close\nas possible to some known ground truth correspondence.\nUnlike methods based on formulating shape matching as\na labeling problem this approach evaluates the quality of\nthe entire map, obtained using the transformed descriptors,\nwhich as shown in [23] leads to signiﬁcant improvement\ncompared to several strong baselines.\nMotivation Similarly to other supervised learning methods,\nalthough FMNet [23] can result in highly accurate corre-\nspondences, its applicability is limited to shape classes for\nwhich high-quality ground truth maps are available. More-\nover, perhaps less crucially, the soft map loss in FMNet is\nbased on the knowledge of geodesic distances between all\npairs of points, making it computationally expensive. Our\ngoal, therefore, is to show that a similar approach can be\nused more widely, without any training data, while working\npurely in the spectral domain.\n4. SURFMNet\n4.1. Overview\nIn this paper, we introduce a novel approach, which we\ncall SURFMNet for Spectral Unsupervised FMNet. Our\nmethod aims to optimize for non-linear transformations of\ndescriptors, in order to obtain high-quality functional, and\nthus pointwise maps. For this, we follow the general strat-\negy proposed in FMNet [23]. However, crucially, rather\nthan penalizing the deviation of the computed map from\nthe known ground truth correspondence, we evaluate the\nstructural properties of the inferred functional maps, such\nas their bijectivity or orthogonality. Importantly, we express\nall these desired properties, and thus the penalties during\noptimization, purely in the spectral domain, which allows\nus to avoid the conversion of functional maps to soft maps\nduring optimization as was done in [23]. Thus, in addition\nto being purely unsupervised, our approach is also more ef-\nﬁcient since it does not require pre-computation of geodesic\ndistance matrices or expensive manipulation of large soft\nmap matrices during training.\nTo achieve these goals, we build on the FMNet model,\ndescribed in Eq. (3) and (4) in several ways: ﬁrst, we pro-\npose to consider functional maps in both directions, i.e. by\ntreating the two shapes as both source and target; second,\nwe remove the conversion from functional to soft maps;\nand, most importantly, third, we replace the soft map loss\nwith respect to ground truth with a set of penalties on the\ncomputed functional maps, which are described in detail\nbelow. Our optimization problem can be written as:\nmin\nT\nX\n(S1,S2)\nX\ni∈penalties\nwiEi(C12, C21), where\n(5)\nC12 = arg min\nC\n∥CAT (D1) −AT (D2)∥,\n(6)\nC21 = arg min\nC\n∥CAT (D2) −AT (D1)∥.\n(7)\nHere, similarly to Eq. (3) above, T denotes a non-linear\ntransformation in the form of a neural network, (S1, S2) is\na set of pairs of shapes in a given collection, wi are scalar\nweights, and Ei are the penalties, described below. Thus,\nwe aim to optimize for a non-linear transformation of in-\nput descriptor functions, such that functional maps com-\nputed from transformed descriptors possess certain desir-\nable structural properties and are expressed via penalty min-\nimization. Figure 2 illustrates our proposed method where\nwe denote the total sum of all penalty terms in Eq. (5) as\nEglobal and back-propagation via grey dashed lines.\nWhen deriving the penalties used in our approach, we\nexploit the links between properties of functional maps and\nassociated pointwise maps, that have been established in\nseveral previous works [30, 40, 14, 29]. Unlike all these\nmethods, however, we decouple the descriptor preservation\nconstraints from structural map properties. This allows us\nto optimize for descriptor functions, and thus, gain a very\nstrong resilience in the presence of noisy or uninformative\ndescriptors, while still exploiting the compactness and efﬁ-\nciency of the functional map representation.\n4.2. Deep Functional Map Regularization\nIn our work, we propose to use four regularization terms,\nby including them as a penalties in the objective function,\nall inspired by desirable map properties.\nBijectivity Given a pair of shapes and the functional maps\nin both directions, perhaps the simplest requirement is for\nthem to be inverses of each other, which can be enforced by\npenalizing the difference between their composition and the\nidentity map. This penalty, used for functional map estima-\ntion in [14], can be written, simply as:\nE1 = ∥C12C21 −I∥2 + ∥C21C12 −I∥2\n(8)\nOrthogonality As observed in several works [30, 40] a\npoint-to-point map is locally area preserving if and only\nif the corresponding functional map is orthonormal. Thus,\nfor shape pairs, approximately satisfying this assumption, a\nnatural penalty in our unsupervised pipeline is:\nE2 = ∥C⊤\n12C12 −I∥2 + ∥C⊤\n21C21 −I∥2\n(9)\nLaplacian commutativity Similarly, it is well-known that\na pointwise map is an intrinsic isometry if and only if\nthe associated functional map commutes with the Laplace-\nBeltrami operator [39, 30]. This has motivated using the\nlack of commutativity as a regularizer for functional map\ncomputations, as mentioned in Eq. (2). In our work, we use\nit to introduce the following penalty:\nE3 =\n\r\rC12Λ1 −Λ2C12\n\r\r2 +\n\r\rC21Λ2 −Λ1C21\n\r\r2 (10)\nwhere Λ1 and Λ2 are diagonal matrices of the Laplace-\nBeltrami eigenvalues on the two shapes.\nDescriptor preservation via commutativity The previ-\nous three penalties capture desirable properties of point-\nwise correspondences when expressed as functional maps.\nOur last penalty promotes functional maps that arise from\npoint-to-point maps, rather than more general soft corre-\nspondences. To achieve this, we follow the approach pro-\nposed in [29] based on preservation of pointwise products\nof functions. Namely, it is known that a non-trivial linear\ntransformation T across function spaces corresponds to a\npoint-to-point map if and only if T (f ⊙h) = T (f) ⊙T (h)\nfor any pair of functions f, h. Here ⊙denotes the point-\nwise product between functions [41], i.e. (f ⊙h)(x) =\nf(x)h(x). When f is a descriptor function on the source\nand g is the corresponding descriptor on the target, the au-\nthors of [29] demonstrate that this condition can be rewrit-\nten in the reduced basis as follows: CMf = MgC, where\nMf = Φ+Diag(f)Φ, and Mg = Ψ+Diag(g)Ψ. This leads\nto the following penalty, in our setting:\nE4 =\nX\n(fi,gi)∈Descriptors\n||C12Mfi −MgiC12||2\n+||C21Mgi −MfiC21||2,\nMfi = Φ+Diag(fi)Φ, Mgi = Ψ+Diag(gi)Ψ.\n(11)\nIn this expression, fi and gi are the optimized descriptors\non source and target shape, obtained by the neural network,\nand expressed in the full (hat basis), whereas Φ, Ψ are the\nﬁxed basis functions on the two shapes, and + denotes the\nMoore-Penrose pseudoinverse.\n4.3. Optimization\nAs mentioned in Section 4.1, we incorporate these four\npenalties into the energy in Eq. (5). Importantly, the only\nunknowns in this optimization are the parameters of the\nneural network applied to the descriptor functions.\nThe\nfunctional maps C12 and C21 are fully determined by the\noptimized descriptors via the solution of the optimization\nproblems in Eq. (6) and Eq. (7). Note that although stated\nas optimization problems, both Eq. (6) and Eq. (7) reduce\nto solving a linear system of equations. This is easily differ-\nentiable using the well-known closed-form expression for\nderivatives of matrix inverses [32]. Moreover, the function-\nality of differentiating a linear system of equations is im-\nplemented in TensorFlow [1] and we use it directly, in the\nsame way as it was used in the original FMNet work. Fi-\nnally, all of the penalties E1, E2, E3, E4 are differentiable\nwith respect to the functional maps C12, C21. This means\nthat the gradient of the total energy can be back-propagated\nto the neural network T in Eq. (5), allowing us to optimize\nfor the descriptors while penalizing the structural properties\nof the functional maps.\n5. Implementation & Parameters\nImplementation details We implemented 1 our method in\nTensorFlow [1] by adapting the open-source implementa-\ntion of FMNet [23].\nThus, the neural network T used\nfor transforming descriptors in our approach, in Eq. (5) is\nexactly identical to that used in FMNet, as mentioned in\nEq. (3). Namely, this network is based on a residual archi-\ntecture, consisting of 7 fully connected residual layers with\nexponential linear units, without dimensionality reduction.\nPlease see Section 5 in [23] for more details.\nFollowing the approach of FMNet [23], we also sub-sample\na random set of 1500 points at each training step, for ef-\nﬁciency. However, unlike their method, sub-sampling is\ndone independently on each shape, without enforcing con-\nsistency. Remark that our network is fully connected on\nthe dimensions of the descriptors, not across vertices them-\nselves. For example, the ﬁrst layer has 352 × 352 weights\n(not 1500×352 weights) where 352 and 1500 are the dimen-\nsions of the SHOT descriptors, and no. of sampled vertices\nrespectively. Indeed, in exactly the same way as in FM-\nNet, our network is applied on the descriptors of each point\nindependently, using the same (learned) weights, and differ-\nent points on the shape only communicate through the func-\ntional map estimation layer, and not in the MLP layers. This\n1Code\navailable\nat\nhttps://github.com/\nLIX-shape-analysis/SURFMNet.\nMethods\nE1+E2+E3+E4\nE3\nE1\nE2\nE4\nGeodesic Error\n0.020\n0.073\n0.083\n0.152\n0.252\nTable 1: Ablation study of penalty terms in our method on\nthe FAUST benchmark.\nensures invariance to permutation of shape vertices. We also\nrandomly sub-sample 20% of the optimized descriptors for\nour penalty E4 at each training step to avoid manipulating a\nlarge set of operators. We observed that this sub-sampling\nnot only helps to gain speed but also robustness during opti-\nmization. Importantly, we do not form large diagonal matri-\nces explicitly, but rather deﬁne the multiplicative operators\nM in objective E4 directly via pointwise products and sum-\nmation using contraction between tensors.\nFinally, we convert functional maps to pointwise ones\nwith nearest neighbor search in the spectral domain, fol-\nlowing the original approach [30].\nParameters Our method takes two types of inputs: the in-\nput descriptors, and the scalar weights wi in Eq. (5). In\nall experiments below, we used the same SHOT [45] de-\nscriptors as in FMNet [23] with the same parameters, which\nleads to a 352-dimensional vector per point, or equivalently,\n352\ndescriptor functions on each shape.\nFor the scalar\nweights, wi, we used the same four ﬁxed values for all ex-\nperiments below (namely, w1 = 103, w2 = 103, w3 = 1\nand w4 = 105), which were obtained by examining the rel-\native penalty values obtained throughout the optimization\non a small set of shapes, and setting the weights inversely\nproportionally to those values. We train our network with a\nbatch size of 10 for 10 000 iterations using a learning rate\nof 0.001 and ADAM optimizer [13].\n6. Results\nDatasets We evaluate our method on the following datasets:\nthe original FAUST dataset [6] containing 100 human\nshapes in 1-1 correspondence and the remeshed versions of\nSCAPE [3] and FAUST [6] datasets, made publicly avail-\nable recently by Ren et al. [35]. These datasets were ob-\ntained by independently re-meshing each shape to approx-\nimately 5000 vertices using the LRVD re-meshing method\n[52], while keeping track of the ground truth maps within\neach collection. This results in meshes that are no longer\nin 1-1 correspondence, and indeed can have different num-\nber of vertices. The re-meshed datasets therefore offer sig-\nniﬁcantly more variability in terms of shape structures, in-\ncluding e.g.\npoint sampling density, making them more\nchallenging for existing algorithms. Let us note also that\nthe SCAPE dataset is slightly more challenging since the\nshapes are less regular (e.g., there are often reconstruction\nartefacts on hands and feet) and have fewer features than\nthose in FAUST.\nWe stress that although we also evaluated on the origi-\nnal FAUST dataset, we view the remeshed datasets as more\nrealistic, providing a more faithful representation of the ac-\ncuracy and generalization power of different techniques.\nAblation study We ﬁrst evaluated the relative importance\nof the different penalties in our method on the FAUST\nshape dataset [6]. We evaluated the average correspondence\ngeodesic error with respect to the ground truth maps.\nTable 1 summarizes the quality of the computed corre-\nspondences between shapes in the test set, using different\ncombination of penalties. We observe that the combination\nof all four penalties signiﬁcantly out-performs any other\nsubsets. Besides, among individual penalties used indepen-\ndently, the Laplacian commutativity gives the best result.\nFor more combinations of penalty terms, we refer to a more\ndetailed ablation study in the supplementary material.\nBaselines We compared our method to several techniques,\nboth supervised and fully automatic. For conciseness, we\nrefer to SURFMNet as Ours in the following text. For a\nfair comparison with FMNet, we evaluate our method in\ntwo settings: Ours-sub and Ours-all. For Ours-sub, we\nsplit each dataset into training and test sets containing 80\nand 20 shapes respectively, as done in [23]. For Ours-all,\nwe optimize over all the dataset and apply the optimized\nnetwork on the same test set as before. We stress that unlike\nFMNet, our method does not use any ground truth in either\nsetting. We use the notation Ours-sub only to emphasize the\nsplit of dataset into train and test since the “training set” was\nonly used for descriptor optimization with the functional\nmap penalties introduced above without any ground truth.\nSince the original FMNet work [23] already showed very\nstrong improvement compared to existing supervised learn-\ning methods we primarily compare to this approach. For\nreference, we also compare to the Geodesic Convolutional\nNeural Networks (GCNN) method of [26] on the remeshed\ndatasets, which were not considered in [23]. GCNN is a rep-\nresentative supervised method based on local shape param-\neterization, and as FMNet assumes, as input, ground truth\nmaps between a subset of the training shapes. For super-\nvised methods, we always split the datasets into 80 (resp.\n60) shapes for training and 20 (resp. 10) for testing in the\nFAUST and SCAPE datasets respectively.\nAmong fully automatic methods, we use the Product\nManifold Filter method with the Gaussian kernel [48] (PMF\nGauss) and its variant with the Heat kernel [47] (PMF Heat).\nWe also compare to the recently proposed BCICP [35],\nwhich achieved state-of-the-art results among axiomatic\nmethods. With a slight abuse of notation, we denote these\nnon-learning methods as Unsupervised in Figure 4 since\nnone of these methods use ground truth. Finally, we also\nevaluated the basic functional map approach, based on di-\nrectly optimizing the functional maps as outlined in Sec-\nFigure 3: Quantitative evaluation of pointwise correspondences comparing our method with Supervised Methods.\nFigure 4: Quantitative evaluation of pointwise correspondences comparing our method with Unsupervised Methods.\ntion 3.1, but using all four of our energies for regulariza-\ntion. This method, which we call “Fmap Basic” can be\nviewed as a combination of the approaches of [14] and [28],\nas it incorporates functional map coupling (via energy E1)\nand descriptor commutativity (via E4). Unlike our tech-\nnique, however, it operates on ﬁxed descriptor functions,\nand uses descriptor preservation constraints with the origi-\nnal and noisy descriptors.\nFor fairness of comparison, we used SHOT descriptors\n[45] as input to all methods, except BCICP [35], which uses\ncarefully curated WKS [4] descriptors. Furthermore, we\nconsider the results of FMNet [23] before and after applying\nthe PMF-based post-processing as suggested in the original\narticle. We also report results with ICP post-processing in-\ntroduced in [30]. Besides the accuracy plots shown in Fig-\nures 3 and 4, we also include statistics such as maximum\nand 95th percentile in supplementary material.\n6.1. Evaluation and Results\nFigure 3 summarizes the accuracy obtained by super-\nvised methods on the three datasets whereas Figure 4 com-\npares with unsupervised methods, using the evaluation pro-\ntocol introduced in [19]. Note that in all cases, our network\nSURFMNet, (Ours-all), when optimized on all shapes\nachieves the best results even compared to the recent state-\nof-the-art method in [35]. Furthermore, our method is com-\nparable even to supervised learning techniques, GCNN [7]\nand FMNet [23] despite being purely unsupervised.\nRemark that the remeshed datasets are signiﬁcantly\nharder for both supervised and unsupervised methods, since\nthe shapes are no longer identically meshed and in 1-1 cor-\nrespondence. We have observed this difﬁculty also while\ntraining supervised FMNet and GCNN techniques with\nvery slow convergence during training. On both of these\ndatasets, our approach achieves the lowest average error, re-\nported in Figure 3 and 4. Note that on the remeshed FAUST\ndataset, as shown in Figure 3, only GCNN [7] produces a\nsimilarly large fraction of correspondences with a small er-\nror. However, this method is supervised. On the remeshed\nSCAPE dataset, our method leads to the best results across\nall measures, despite being purely unsupervised.\nPostprocessing Results As shown in Figures 3 and 4 our\nmethod can often obtain high quality results even without\nany post-processing. Nevertheless, in the challenging cases\nsuch as the SCAPE remeshed dataset, when trained on a\nsubset of shapes, it can also beneﬁt from an efﬁcient ICP-\nbased reﬁnement. This reﬁnement, does not require com-\nputing geodesic distances and does not require the shapes\nto have the same number of points, thus maintaining the\nﬂexibility and efﬁciency of our pipeline.\nCorrelation with actual Geodesic loss We further investi-\ngated if there is a correlation between the value of our loss\nand the quality of correspondence. Speciﬁcally, whether\nminimizing our loss function, mainly consisting of regular-\nization terms on estimated functional maps, corresponds to\nSource\nGround-Truth\nSURFMNet\nBCICP\nPMF (heat)\nPMF (gauss)\nFigure 5: Comparison of our method with Unsupervised methods for texture transfer on the SCAPE remeshed dataset. Note\nthat BCICP is roughly 7 times slower than our method and its shortcomings are marked with red circles.\nSource\nGround-Truth\nSURFMNet+ICP\nSURFMNet\nFMNet\nFMNet + PMF\nGCNN\nFigure 6: Comparison of our method with Supervised method for texture transfer on the SCAPE remeshed dataset.\nRuntime\nMethods\nPre-processing\nTraining\nTesting\nPost-processing\nTotal\nFMNet\n60s\n1500s\n0.3s\nN/As\n1650s\nFMNet + PMF\n60s\n1500s\n0.3s\n30s\n1680s\nFmap Basic\n10s\nN/A\n60s\nN/A\n120s\nBCICP\nN/A\nN/A\n60s\n180s\n240s\nSURFMNet\n10s\n25s\n0.3s\nN/A\n35s\nSURFMNet + ICP\n10s\n25s\n0.3s\n10s\n45s\nTable 2: Runtime of different methods averaged over 190\nshape pairs.\nminimizing the geodesic loss with respect to the unknown\nground truth map. We found strong correlation between the\ntwo and share a plot in the supplementary material.\nQualitative and Runtime Comparison Figures 5 and 6\nshow examples shape pairs and maps obtained between\nthem using different methods, visualized via texture trans-\nfer. Note the continuity and quality of the maps obtained\nusing our method, compared to other techniques (more re-\nsults in supplementary material). One further advantage of\nour method is its efﬁciency, since we do not rely on the\ncomputation of geodesic matrices and operate entirely in\nthe spectral domain. Table 2 compares the run-time of the\nbest performing methods on an Intel Xeon 2.10GHz ma-\nchine with an NVIDIA Titan X GPU. Note that our method\nis over an order of magnitude faster than FMNet and signif-\nicantly faster than the currently best unsupervised BCICP.\n7. Conclusion & Future Work\nWe presented an unsupervised method for computing\ncorrespondences between shapes. Key to our approach is\na bi-level optimization formulation, aimed to optimize de-\nscriptor functions, while promoting the structural properties\nof the entire map, obtained from them via the functional\nmaps framework. Remarkably, our approach achieves sim-\nilar, and in some cases superior performance even to super-\nvised correspondence techniques.\nIn the future, we plan to incorporate other penalties on\nfunctional maps, e.g., those arising from recently-proposed\nkernalization approaches [49], or for promoting orientation\npreserving maps[35] and also incorporate cycle consistency\nconstraints [17]. Finally, it would be interesting to extend\nour method to partial and non-isometric shapes and match-\ning other modalities, such as images or point clouds, since it\nopens the door to linking the properties of local descriptors\nto global map consistency.\nAcknowledgements Parts of this work were supported by\nthe ERC Starting Grant StG-2017-758800 (EXPROTEA),\nKAUST OSR Award No. CRG-2017-3426, and a gift from\nNvidia. We are grateful to Jing Ren, Or Litany, Emanuele\nRodol`a and Adrien Poulenard for their help in performing\nquantitative comparisons and producing qualitative results.\nReferences\n[1] Mart´ın Abadi, Ashish Agarwal, and Paul Barham et\nal. TensorFlow: Large-scale machine learning on het-\nerogeneous systems, 2015. Software available from\ntensorﬂow.org. 5\n[2] Yonathan Aﬂalo, Anastasia Dubrovina, and Ron Kim-\nmel.\nSpectral generalized multi-dimensional scal-\ning.\nInternational Journal of Computer Vision,\n118(3):380–392, 2016. 2\n[3] Dragomir Anguelov, Praveen Srinivasan, Daphne\nKoller, Sebastian Thrun, Jim Rodgers, and James\nDavis. SCAPE: Shape Completion and Animation of\nPeople.\nIn ACM Transactions on Graphics (TOG),\nvolume 24, pages 408–416. ACM, 2005. 6\n[4] Mathieu Aubry, Ulrich Schlickewei, and Daniel Cre-\nmers. The wave kernel signature: A quantum mechan-\nical approach to shape analysis. 31(4), Nov. 2011. 7\n[5] Silvia Biasotti, Andrea Cerri, A Bronstein, and M\nBronstein. Recent trends, applications, and perspec-\ntives in 3d shape similarity assessment. In Computer\nGraphics Forum, volume 35, pages 87–119, 2016. 2\n[6] Federica Bogo, Javier Romero, Matthew Loper, and\nMichael J. Black.\nFAUST: Dataset and evaluation\nfor 3D mesh registration. In Proceedings IEEE Conf.\non Computer Vision and Pattern Recognition (CVPR),\nPiscataway, NJ, USA, June 2014. IEEE. 1, 6\n[7] Davide Boscaini, Jonathan Masci, Simone Melzi,\nMichael M Bronstein, Umberto Castellani, and Pierre\nVandergheynst. Learning class-speciﬁc descriptors for\ndeformable shapes using localized spectral convolu-\ntional networks. In Computer Graphics Forum, vol-\nume 34, pages 13–23. Wiley Online Library, 2015. 2,\n7\n[8] Davide Boscaini, Jonathan Masci, Emanuele Rodola,\nand Michael M. Bronstein. Learning shape correspon-\ndence with anisotropic convolutional neural networks.\nIn Proc. NIPS, pages 3189–3197, 2016. 2\n[9] Oliver Burghard, Alexander Dieckmann, and Rein-\nhard Klein. Embedding shapes with Green’s functions\nfor global shape matching. Computers & Graphics,\n68:1–10, 2017. 2\n[10] Etienne Corman, Maks Ovsjanikov, and Antonin\nChambolle. Supervised descriptor learning for non-\nrigid shape matching.\nIn Proc. ECCV Workshops\n(NORDIA), 2014. 1, 2, 3\n[11] Luca Cosmo, Emanuele Rodola, Jonathan Masci, An-\ndrea Torsello, and Michael M Bronstein. Matching\ndeformable objects in clutter.\nIn 3D Vision (3DV),\n2016 Fourth International Conference on, pages 1–10.\nIEEE, 2016. 2\n[12] Micha¨el Defferrard, Xavier Bresson, and Pierre Van-\ndergheynst. Convolutional neural networks on graphs\nwith fast localized spectral ﬁltering. In Advances in\nNeural Information Processing Systems, pages 3844–\n3852, 2016. 2\n[13] J. Ba D.P. Kingma. Adam: A method for stochastic\noptimization. In ICLR, 2015. 6\n[14] Davide Eynard, Emanuele Rodola, Klaus Glashoff,\nand Michael M Bronstein. Coupled functional maps.\nIn 3D Vision (3DV), pages 399–407. IEEE, 2016. 2, 4,\n5, 7\n[15] Danielle Ezuz and Mirela Ben-Chen. Deblurring and\ndenoising of maps between shapes.\nIn Computer\nGraphics Forum, volume 36, pages 165–174. Wiley\nOnline Library, 2017. 2, 3\n[16] Oshri Halimi, Or Litany, Emanuele Rodol‘a, Alex\nBronstein, and Ron Kimmel. Unsupervised learning\nof dense shape correspondence. In CVPR, 2019. 3\n[17] Qixing Huang, Fan Wang, and Leonidas Guibas.\nFunctional map networks for analyzing and exploring\nlarge shape collections. ACM Transactions on Graph-\nics (TOG), 33(4):36, 2014. 2, 8\n[18] Ruqi Huang and Maks Ovsjanikov. Adjoint map rep-\nresentation for shape analysis and matching. In Com-\nputer Graphics Forum, volume 36, pages 151–163.\nWiley Online Library, 2017. 2\n[19] Vladimir G Kim,\nYaron Lipman,\nand Thomas\nFunkhouser. Blended intrinsic maps. In ACM Trans-\nactions on Graphics (TOG), volume 30, page 79.\nACM, 2011. 7\n[20] Artiom Kovnatsky, Michael M Bronstein, Xavier\nBresson, and Pierre Vandergheynst. Functional cor-\nrespondence by matrix completion. In Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition, pages 905–914, 2015. 2\n[21] Artiom Kovnatsky, Michael M Bronstein, Alexan-\nder M Bronstein, Klaus Glashoff, and Ron Kimmel.\nCoupled quasi-harmonic bases. In Computer Graph-\nics Forum, volume 32, pages 439–448, 2013. 2\n[22] Artiom Kovnatsky, Klaus Glashoff, and Michael M\nBronstein.\nMADMM: a generic algorithm for non-\nsmooth optimization on manifolds. In Proc. ECCV,\npages 680–696. Springer, 2016. 3\n[23] Or Litany, Tal Remez, Emanuele Rodol`a, Alexan-\nder M. Bronstein, and Michael M. Bronstein. Deep\nfunctional maps:\nStructured prediction for dense\nshape correspondence. 2017 IEEE International Con-\nference on Computer Vision (ICCV), pages 5660–\n5668, 2017. 2, 3, 4, 5, 6, 7\n[24] Or Litany, Emanuele Rodol`a, Alex M Bronstein, and\nMichael M Bronstein.\nFully spectral partial shape\nmatching. In Computer Graphics Forum, volume 36,\npages 247–258. Wiley Online Library, 2017. 2\n[25] Roee Litman and Alexander M Bronstein. Learning\nspectral descriptors for deformable shape correspon-\ndence. IEEE transactions on pattern analysis and ma-\nchine intelligence, 36(1):171–180, 2014. 1, 2\n[26] Jonathan Masci, Davide Boscaini, Michael Bronstein,\nand Pierre Vandergheynst.\nGeodesic convolutional\nneural networks on riemannian manifolds. In Proceed-\nings of the IEEE international conference on computer\nvision workshops, pages 37–45, 2015. 1, 2, 6\n[27] Federico Monti, Davide Boscaini, Jonathan Masci,\nEmanuele Rodol`a, Jan Svoboda, and Michael M.\nBronstein.\nGeometric deep learning on graphs and\nmanifolds using mixture model cnns. In CVPR, pages\n5425–5434. IEEE Computer Society, 2017. 1, 2\n[28] Dorian Nogneng, Simone Melzi, Emanuele Rodol`a,\nUmberto Castellani, M Bronstein, and Maks Ovs-\njanikov.\nImproved functional mappings via prod-\nuct preservation. In Computer Graphics Forum, vol-\nume 37, pages 179–190. Wiley Online Library, 2018.\n2, 7\n[29] Dorian Nogneng and Maks Ovsjanikov. Informative\ndescriptor preservation via commutativity for shape\nmatching.\nComputer Graphics Forum, 36(2):259–\n267, 2017. 2, 3, 4, 5\n[30] Maks Ovsjanikov, Mirela Ben-Chen, Justin Solomon,\nAdrian Butscher, and Leonidas Guibas.\nFunctional\nMaps: A Flexible Representation of Maps Between\nShapes.\nACM Transactions on Graphics (TOG),\n31(4):30, 2012. 2, 4, 5, 6, 7\n[31] Maks Ovsjanikov, Etienne Corman, Michael Bron-\nstein, Emanuele Rodol`a, Mirela Ben-Chen, Leonidas\nGuibas, Frederic Chazal, and Alex Bronstein. Com-\nputing and processing correspondences with func-\ntional maps. In ACM SIGGRAPH 2017 Courses, SIG-\nGRAPH ’17, pages 5:1–5:62, 2017. 2, 3\n[32] K. B. Petersen and M. S. Pedersen. The matrix cook-\nbook, 2012. 5\n[33] Adrien Poulenard and Maks Ovsjanikov.\nMulti-\ndirectional geodesic neural networks via equivariant\nconvolution. arXiv preprint arXiv:1810.02303, 2018.\n2\n[34] Adrien Poulenard, Primoz Skraba, and Maks Ovs-\njanikov. Topological function optimization for contin-\nuous shape matching. In Computer Graphics Forum,\nvolume 37, pages 13–25. Wiley Online Library, 2018.\n2\n[35] Jing Ren, Adrien Poulenard, Peter Wonka, and Maks\nOvsjanikov.\nContinuous and orientation-preserving\ncorrespondences via functional maps. ACM Transac-\ntions on Graphics (TOG), 37(6), 2018. 2, 6, 7, 8\n[36] Emanuele Rodol`a, Luca Cosmo, Michael M Bron-\nstein, Andrea Torsello, and Daniel Cremers. Partial\nfunctional correspondence. In Computer Graphics Fo-\nrum, volume 36, pages 222–236. Wiley Online Li-\nbrary, 2017. 2\n[37] Emanuele Rodol`a, M Moeller, and Daniel Cremers.\nPoint-wise map recovery and reﬁnement from func-\ntional correspondence. In Proc. Vision, Modeling and\nVisualization (VMV), 2015. 3\n[38] Emanuele Rodol`a, Samuel Rota Bulo, Thomas Wind-\nheuser, Matthias Vestner, and Daniel Cremers. Dense\nnon-rigid shape correspondence using random forests.\nIn Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 4177–4184,\n2014. 2\n[39] Steven Rosenberg. The Laplacian on a Riemannian\nmanifold: an introduction to analysis on manifolds,\nvolume 31. Cambridge University Press, 1997. 5\n[40] Raif Rustamov, Maks Ovsjanikov, Omri Azencot,\nMirela Ben-Chen,\nFrederic Chazal,\nand Leonid\nGuibas.\nMap-based exploration of intrinsic shape\ndifferences and variability.\nACM Trans. Graphics,\n32(4):72:1–72:12, July 2013. 2, 4, 5\n[41] R K Singh and J S Manhas. Composition Operators\non Function Spaces. ELSEVIER, 1993. 5\n[42] Robert W Sumner and Jovan Popovi´c. Deformation\ntransfer for triangle meshes. In ACM Transactions on\nGraphics (TOG), volume 23, pages 399–405. ACM,\n2004. 1\n[43] Jian Sun, Maks Ovsjanikov, and Leonidas Guibas. A\nConcise and Provably Informative Multi-Scale Signa-\nture Based on Heat Diffusion. In Computer graphics\nforum, volume 28, pages 1383–1392, 2009. 3\n[44] Gary KL Tam, Zhi-Quan Cheng, Yu-Kun Lai, Frank C\nLangbein, Yonghuai Liu, David Marshall, Ralph R\nMartin, Xian-Fang Sun, and Paul L Rosin. Registra-\ntion of 3d point clouds and meshes: a survey from\nrigid to nonrigid. IEEE transactions on visualization\nand computer graphics, 19(7):1199–1217, 2013. 1, 2\n[45] Federico Tombari, Samuele Salti, and Luigi Di Ste-\nfano. Unique signatures of histograms for local sur-\nface description. In International Conference on Com-\nputer Vision (ICCV), pages 356–369, 2010. 6, 7\n[46] Oliver Van Kaick, Hao Zhang, Ghassan Hamarneh,\nand Daniel Cohen-Or. A survey on shape correspon-\ndence.\nIn Computer Graphics Forum, volume 30,\npages 1681–1707, 2011. 2\n[47] Matthias Vestner, Zorah L¨ahner, Amit Boyarski, Or\nLitany, Ron Slossberg, Tal Remez, Emanuele Rodola,\nAlex Bronstein, Michael Bronstein, Ron Kimmel, and\nDaniel Cremers.\nEfﬁcient deformable shape corre-\nspondence via kernel matching. In Proc. 3DV, 2017.\n6\n[48] M. Vestner, R. Litman, E. Rodol`a, A. Bronstein, and\nD. Cremers. Product manifold ﬁlter: Non-rigid shape\ncorrespondence via kernel density estimation in the\nproduct space.\nIn Proc. CVPR, pages 6681–6690,\n2017. 6\n[49] Larry Wang, Anne Gehre, Michael M Bronstein, and\nJustin Solomon. Kernel functional maps. In Computer\nGraphics Forum, volume 37, pages 27–36. Wiley On-\nline Library, 2018. 2, 3, 8\n[50] Yuexuan Wang, B Liu, K Zhou, and Yu Tong. Vec-\ntor ﬁeld map representation for near conformal sur-\nface correspondence. In Computer Graphics Forum,\nvolume 37, pages 72–83. Wiley Online Library, 2018.\n2\n[51] Lingyu Wei, Qixing Huang, Duygu Ceylan, Etienne\nVouga, and Hao Li. Dense human body correspon-\ndences using convolutional networks. In Proceedings\nof the IEEE Conference on Computer Vision and Pat-\ntern Recognition, pages 1544–1553, 2016. 1, 2\n[52] Dong-Ming Yan, Guanbo Bao, Xiaopeng Zhang, and\nPeter Wonka. Low-resolution remeshing using the lo-\ncalized restricted voronoi diagram. IEEE transactions\non visualization and computer graphics, 20(10):1418–\n1427, 2014. 6\n8. Supplement\nA. Correlation with actual geodesic loss\nTo support the claim made in the subsection ’Evaluation\nand Results’, we include a plot here to visualize the cor-\nrelation between our loss and the actual geodesic loss. As\nevident in Figure 7, there is a strong correlation between our\nloss value and the quality of correspondence as measured by\naverage geodesic error.\nB. Detailed Tabular Quantitative Comparison\nBesides the average geodesic error reported for quanti-\ntative comparison in Figures 3 and 4, we provide detailed\nstatistics in Table 3. Note that Table 3 also includes ’Fmap\nOurs Opt’ which is equivalent to “Fmap Basic” but uses\nthe learned descriptors instead of original ones. Its compet-\nitive performance across all datasets proves quantitatively\nthe utility of learning descriptors. Figures 13 and 14 illus-\ntrate this further. For completeness, in Table 4, we also pro-\nvide a detailed ablation study with different combinations\nof penalties.\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n8000\n9000\n10000\nNumber of Mini-Batch Iterations\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.1\n0.11\nAverage Geodesic Distance Error\n1\n1.2\n1.4\n1.6\n1.8\n2\n2.2\nUnsupervised Loss\n10 4\nUnsupervised Loss vs Average Geodesic Error\nAverage Geodesic Distance Error\nUnsupervised Loss\nFigure 7: Correlation with average geodesic loss computed\nfrom ground truth correspondences.\nC. Sensitivity to number of basis functions\nFigure 8 shows the sensitivity of our network SURFM-\nNet on the SCAPE remeshed dataset as the number of eigen\nfunctions are varied from 20 to 150. We train the network\neach time with 10000 mini batch steps. As evident, we\nobtain best result using 120. However, when trained on\nan individual dataset and tested on a different one, we see\nover-ﬁtting when using a large eigen-basis. We attribute\nthis phenomenon to the initialization of our descriptors with\nSHOT which is a very local descriptor and is not robust to\nvery strong mesh variability. However, over-ﬁtting is mini-\nmal when we train together on a relatively larger subset of\nSCAPE and FAUST and test on a different subset of shapes\nfrom both datasets, with smaller eigen basis.\n20\n40\n60\n80\n100\n120\n140\n160\nNumber of Eigenvectors\n0\n0.05\n0.1\n0.15\nAverage Geodesic Distance Error\nSCAPE remeshed\nFigure 8: Accuracy of our method on the SCAPE remeshed\ndataset as the number of eigenfunctions is varied from 20 to\n150.\n(Results are ×10−3)\nFAUST 7k\nFAUST 5k\nSCAPE 5k\nSupervised Methods\nMean\n95th Percentile\nMaximum\nMean\n95th Percentile\nMaximum\nMean\n95th Percentile\nMaximum\nFMNet\n25.01\n63.11\n1207.8\n112.8\n451.8\n1280.6\n172.6\n543.8\n1399.6\nSURFMNet Subset\n19.83\n52.11\n1204.0\n92.09\n493.6\n1279.4\n60.32\n329.8\n1068.7\nFMNet + PMF\n2.98\n14.10\n1222.7\n83.61\n395.7\n1576.4\n63.00\n159.8\n1561.5\nSURFMNet-sub + PMF\n5.33\n22.90\n1302.4\n74.80\n408.5\n1619.3\n51.03\n111.5\n1555.6\nFMNet + ICP\n11.16\n27.91\n1206.8\n47.53\n237.3\n1348.6\n81.76\n341.4\n1226.5\nSURFMNet-sub + ICP\n11.79\n35.76\n1088.4\n30.47\n95.64\n1277.3\n23.00\n54.76\n73.18\nGCNN\n50.49\n206.3\n1578.2\n71.85\n374.2\n1523.7\nUnsupervised Methods\nBCICP\n15.46\n53.27\n572.4\n31.08\n64.51\n1149.9\n22.28\n50.60\n107.5\nPMF (Gaussian Kernel)\n29.42\n83.80\n1168.1\n75.13\n236.9\n1632.7\n54.68\n156.9\n465.1\nPMF (Heat Kernel)\n17.26\n25.06\n1168.1\n31.08\n64.51\n1150.0\n47.23\n133.4\n802.1\nFmap Basic\n457.56\n1171.4\n1568.4\n366.2\n1159.0\n1549.1\n383.0\n1043.7\n1280.3\nFmap Ours Opt\n9.75\n30.02\n420.2\n20.19\n53.24\n1169.5\n13.98\n31.16\n86.45\nSURFMNet-all\n7.89\n26.01\n572.4\n18.56\n50.25\n1156.3\n17.50\n42.50\n228.8\nTable 3: Quantitative comparison on all three benchmark datasets for shape correspondence problem.\nMethods\nE1+E2+E3+E4\nE3\nE1+E2+E3\nE1+E3+E4\nE1\nE2+E3+E4\nE1+E2+E4\nE2\nE4\nFMNet\nOurs-Sub\nOurs-all\nMean Geodesic Error\n0.044\n0.073\n0.081\n0.077\n0.111\n0.079\n0.126\n0.135\n0.330\n0.025\n0.020\n0.008\nTable 4: Ablation study of penalty terms in our method and comparison with the supervised FMNet on the FAUST benchmark.\nSource\nGround-Truth\nSUFMNet + ICP\nSUFMNet\nFMNet\nFMNet + ICP\nGCNN\nFigure 9: Comparison of our method with Supervised methods for texture transfer on the FAUST remeshed dataset.\nD. More Qualitative Comparison\nIn Figures 9 and 12 , we provide more qualitative com-\nparisons of SURFMNet on the FAUST remeshed datasets\nwhereas Figures 10 and 11 provide a comparison on the\nSCAPE remeshed dataset. In all cases, our method pro-\nduces the highest quality maps.\nSource\nGround-Truth\nSURFMNet-all\nBCICP\nPMF (heat)\nPMF (gauss)\nFigure 10: Comparison of our method with Unsupervised methods for texture transfer on the SCAPE remeshed dataset.\nSource\nGround-Truth\nSURFMNet + ICP\nSURFMNet\nFMNet\nFMNet + ICP\nGCNN\nFigure 11: Comparison of our method with Supervised methods for texture transfer on the SCAPE remeshed dataset.\nSource\nGround-Truth\nSURFMNet\nBCICP\nPMF (heat)\nPMF (gauss)\nFigure 12: Comparison of our method with Unsupervised methods for texture transfer on the FAUST remeshed dataset. Note\nthat BCICP is roughly 7 times slower when compared to our method. We highlight the shortcomings of BCICP matching\nwith red circles.\n0\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.1\nGeodesic error\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nFraction of correspondences\nSupervised Methods on FAUST Original\nFMNet : 0.0250\n FMNet + PMF : 0.0030\nFMNet + ICP: 0.0112\nOurs-sub : 0.0198\nOurs-sub + PMF : 0.0053\nOurs-sub + ICP : 0.0118\nFmap Basic : 0.1453\nFmap Ours Opt: 0.0181\n0\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.1\nGeodesic error\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nFraction of correspondences\nSupervised Methods on FAUST Remesh\nFMNet : 0.1128\nFMNet + PMF : 0.0836\nFMNet + ICP : 0.0475\nOurs-sub : 0.0921\nOurs-sub + PMF : 0.0748\n Ours-sub + ICP : 0.0305\nGCNN : 0.0505\nFmap Basic : 0.2199\nFmap Ours Opt: 0.0610\n0\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.1\nGeodesic error\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nFraction of correspondences\nSupervised Methods on SCAPE Remesh\nFMNet : 0.1726\nFMNet + PMF : 0.0630\nFMNet + ICP : 0.0818\nOurs-sub : 0.0603\nOurs-sub + PMF : 0.0510\n Ours-sub + ICP : 0.0230\nGCNN : 0.0718\nFmap Basic : 0.2291\nFmap Ours Opt: 0.0433\nFigure 13: Quantitative evaluation of pointwise correspondences comparing our method with Supervised Methods.\n0\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.1\nGeodesic error\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nFraction of correspondences\nUnsupervised Methods on FAUST Original\nPMF Gauss : 0.0294\nPMF Heat : 0.0173\nBCICP : 0.0155\n Ours-all : 0.0079\nFmap Basic : 0.4576\nFmap Ours Opt : 0.0098\n0\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.1\nGeodesic error\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nFraction of correspondences\nUnsupervised Methods on FAUST Remesh\nPMF Gauss : 0.0389\nPMF Heat : 0.0381\nBCICP : 0.0501\n Ours-all: 0.0185\nFmap Basic : 0.3662\nFmap Ours Opt  : 0.02019\n0\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.1\nGeodesic error\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nFraction of correspondences\nUnsupervised Methods on SCAPE Remesh\nPMF Gauss : 0.0547\nPMF Heat : 0.0472\nBCICP : 0.0223\nOurs-all : 0.0175\nFmap Basic : 0.3830\n Fmap Ours Opt : 0.0140\nFigure 14: Quantitative evaluation of pointwise correspondences comparing our method with Unsupervised Methods.\n",
  "categories": [
    "cs.GR",
    "cs.CV"
  ],
  "published": "2018-12-10",
  "updated": "2019-08-22"
}