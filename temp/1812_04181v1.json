{
  "id": "http://arxiv.org/abs/1812.04181v1",
  "title": "KF-LAX: Kronecker-factored curvature estimation for control variate optimization in reinforcement learning",
  "authors": [
    "Mohammad Firouzi"
  ],
  "abstract": "A key challenge for gradient based optimization methods in model-free\nreinforcement learning is to develop an approach that is sample efficient and\nhas low variance. In this work, we apply Kronecker-factored curvature\nestimation technique (KFAC) to a recently proposed gradient estimator for\ncontrol variate optimization, RELAX, to increase the sample efficiency of using\nthis gradient estimation method in reinforcement learning. The performance of\nthe proposed method is demonstrated on a synthetic problem and a set of three\ndiscrete control task Atari games.",
  "text": "KF-LAX: Kronecker-factored curvature estimation\nfor control variate optimization in reinforcement\nlearning\nMohammad Firouzi\nUniversity of Toronto\nfirouzi@cs.toronto.edu\nAbstract\nA key challenge for gradient based optimization methods in model-free reinforce-\nment learning is to develop an approach that is sample efﬁcient and has low vari-\nance. In this work, we apply Kronecker-factored curvature estimation technique\n(KFAC) to a recently proposed gradient estimator for control variate optimiza-\ntion, RELAX, to increase the sample efﬁciency of using this gradient estimation\nmethod in reinforcement learning. The performance of the proposed method is\ndemonstrated on a synthetic problem and a set of three discrete control task Atari\ngames.\n1\nIntroduction\nLatent variable models are widely used in machine learning. Speciﬁcally, discrete latent variable\nmodels have been studied in many areas such as reinforcement learning (RL) and natural language\nprocessing. In the RL applications, for the optimization of both continuous and discrete variable\nmodels, policy gradient methods (Williams, 1992; Glynn, 1990) have gained high attention and have\nbeen applied in a wide variety of tasks. However, they mostly suffer from having a high variance for\nthe gradient estimation.\nVarious works have focused on reducing the variance of the estimated gradient in the policy gradient\nmethods. A subset of these approaches try to reduce the variance through use of a well designed\ncontrol variate or a baseline. For example, Q-prop (Gu et al., 2016) and MuProp (Gu et al., 2015) use\ncontrol variate based on the ﬁrst order Taylor expansion of the target function around a ﬁxed point,\nand (Wu et al., 2018) uses a factorized action-dependent baseline. In many applications, despite the\nestimated gradient being unbiased, it still suffers from high variance even when it is aligned with a\ncontrol variate. In cases that the target function is continuous, reparameterization trick (Kingma and\nWelling, 2013; Rezende et al., 2014) could be used as a way of obtaining a low variance gradient\nestimator.\nParticularly, in discrete domains, concrete relaxation (Maddison et al., 2016; Jang et al., 2016) uses\nthis technique to obtain a low variance but biased gradient estimator. More recent gradient estima-\ntors such as REBAR (Tucker et al., 2017) and RELAX (Grathwohl et al., 2017) leverage both the\ncontrol variate and the reparameterization trick to obtain a low variance and unbiased estimator. To\nestimate the gradient and apply the reparameterization trick, REBAR uses two different uniform ran-\ndom variables. In general, REBAR is an unbiased gradient estimator. RELAX gradient estimation\napproach is further discussed in the next sections.\nNatural gradient descent is an approach that leverages the local curvature structures of a target func-\ntion and can provide gradient updates invariant to the parameterization. In problems that parameter\nIn Neurips 2018 Workshop on Deep Reinforcement Learning\n1\narXiv:1812.04181v1  [cs.LG]  11 Dec 2018\nspace has an underlying structures, the ordinary gradient does not represent the steepest descent di-\nrection, but the natural gradient does (Amari, 1998). Therefore, an efﬁcient estimation of the natural\ngradient can provide a practical way to build a faster training procedure. In particular, in reinforce-\nment learning, natural policy gradient method (Kakade, 2002) was introduced over a decade ago,\nand works such as (Wu et al., 2017) and (Schulman et al., 2015) successfully applied it in practice\nfor discrete and continuous control tasks. As an another practical usage of the natural gradient, au-\nthors in (Hoffman et al., 2013) used a noisy computation of natural gradient to update variational\nparameters in mean ﬁeld variational inference, and applied it for topic modeling of large data sets.\nKFAC (Martens and Grosse, 2015) is a technique to approximate the local curvature structure of a\ngiven function with an efﬁcient way of using mini-batch samples of data. Sample efﬁciency and fast\noptimization procedure are critical challenges in deep RL. As a second order optimization technique,\nleveraging KFAC to estimate the natural gradient can be helpful to train an agent faster. In this\npaper, inspired by KFAC and a recently introduced gradient estimator, RELAX (Grathwohl et al.,\n2017), we develop a gradient optimization procedure for black box functions which approximates\nthe natural gradient for a differentiable surrogate function as a control variate. Black box functions\nare particularly important in RL where the reward function is not necessarily known. Section 2\ndescribes the preliminaries. Section 3 is devoted for the proposed method. Section 4 provides\na summary of related works. The proposed method is applied to a synthetic problem and a few\ndiscrete control task Atari games in section 5, and the conclusion and future work are provided in\nthe last sections.\n2\nPreliminaries\nMany loss functions in machine learning such as variational objective in variational inference, ex-\npected reward in reinforcement learning, and empirical risk in supervised learning can be written\nas an expectation of a function over the data distribution. Consider the optimization problem of a\nfunction in the form of Ep(x|θ)[f(x)] with respect to θ. Some of the popular techniques to obtain\na gradient estimator for the parameters of this loss function are described in sections 2.1-2.3. In\nsection 2.4 the natural gradient and KFAC are explained in more details.\n2.1\nScore function estimator\nA general approach to estimate the gradient of Ep(x|θ)[f(x)] with respect to θ is known as the score\nfunction estimator (also known as REINFORCE, and likelihood ratio estimator) (Williams, 1992;\nGlynn, 1990) which uses the derivatives of log p(x|θ) to estimate the gradient as,\nf(x)∇θ log p(x|θ)\nWhere x is sampled from p(x|θ). A straight forward calculation shows that the score function\nis an unbiased gradient estimator (Williams, 1992). Moreover, this estimator assumes no special\nrestriction for the function f and is generally applicable for black box functions. In situations that f\ndirectly depends on θ and the dependency is known, the derivatives of f can be added to the above\nestimator to come up with an unbiased estimator.\n2.2\nReparameterization trick\nReparameterization trick (Kingma and Welling, 2013; Rezende et al., 2014) is a technique for cases\nthat f is a continuous and differentiable function, and x can be written as a continuous function of\nθ and a random noise ϵ with a known distribution. The reparameterized gradient estimator has the\nform,\n∂f\n∂T\n∂T\n∂θ ,\nx = T(ϵ, θ), ϵ ∼p(ϵ)\nReparameterization gradient estimate is generally unbiased and has low variance. In practice, the\nstandard Gaussian and Gumbel distributions are popular distributions for reparameterizing the con-\ntinuous and discrete x values, respectively (Maddison et al., 2016; Rezende et al., 2014; Kingma\nand Welling, 2013; Jang et al., 2016).\n2\nAlgorithm 1 KF-LAX\nRequire: f(.), log p(x|θ), x = T(θ, ϵ), p(ϵ), a neural network surrogate cφ(.) with weights Wl for\nlayer l, step sizes α1, α2,\nwhile not converged do\nϵ ∼p(ϵ)\nx ←T(θ, ϵ)\nˆgθ ←[f(x) −cφ(x)] ∂\n∂θ log p(x|θ) + ∂\n∂θcφ(x)\nfor each layer l of cφ do\nEstimate the matrices A and S for the layer l using KFAC.\nWl ←Wl −α2A−1∇Wlˆg2\nθS−1\nend for\nθ ←θ −α1ˆgθ\nend while\nreturn θ\n2.3\nControl variates\nFor a given gradient estimator ˆg, using a well designed function c(x) which Ep(x|θ)[c(x)] can be\ncalculated analytically can result a lower variance gradient estimator without changing the bias of\nthe initial estimator as follow,\nˆgnew(x) = ˆg(x) −c(x) + Ep(x|θ)[c(x)]\nStronger positive correlation between c(x) and ˆg(x) results more reduction in the variance of the\nnew estimator.\n2.4\nNatural gradient and Kronecker-factored curvature approximation\nConsider the problem of minimizing J(θ) with respect to θ ∈Θ. A Conventional gradient descent\napproach uses Euclidean distance over the parameter space Θ. However, Euclidean distance does\nnot beneﬁt from the curvature information of the function and may result a slower convergence.\nNatural gradient (Amari, 1998) uses particular Mahalanobis distance as a local metric and gives an\nupdate rule of the form ∆θ ∝−F −1∇θJ, where F is the Fisher information matrix which is a\nfunction of θ.\nModern neural networks could have millions of parameters. In cases that the optimization is with\nrespect to a neural network parameters, computing the Fisher information matrix and its inverse\ncan be computationally impractical and inefﬁcient. For example, for a neural network with only one\nfully-connected layer with 1000 input and 1000 outputs, direct estimation of the entries of the Fisher\ninformation matrix which is a square matrix with a million rows might be inefﬁcient. Kronecker-\nfactored approximate curvature (KFAC) (Martens and Grosse, 2015) leverages the properties of\nKronecker product to approximate the Fisher information matrix and its inverse efﬁciently. Consider\na multi-layer neural network. Let the output of the neural network be p(y|x) where x denotes\nthe input, and let L = log p(y|x) indicates the log likelihood. For the l-th layer of the neural\nnetwork, let din, dout denote the input and output size of the layer (the index l is dropped for\nsimplicity), Wl ∈Rdin×dout the weights, a ∈Rdin input vector to the layer, and s = Wla as\nthe pre-activation vector for the next layer. We note that ∇WlL = (∇sL)aT . KFAC uses this\nrelationship to approximate the Fisher information matrix in a block diagonal manner, where each\nblock corresponds to a layer. For the layer l, denote the corresponding block as Fl. The approximate\nFisher information for this layer can be written as,\nFl = E[vec{∇WlL}vec{∇WlL}T ] = E[aaT ⊗∇sL(∇sL)T ]\n≈E[aaT ] ⊗E[∇sL(∇sL)T ] := A ⊗S := ˆFl\nWhere A := E[aaT ], and S := E[∇sL(∇sL)T ]. A and S can be approximated during training\nusing mini-batches of the data, or through the states and actions resulting from the agent experience\nof different trajectories. Having these approximations, the inverse of the block could be obtained\nusing the following property of Kronecker product,\n(X ⊗Y )−1 = X−1 ⊗Y −1\n3\nFigure 1: Loss and variance curve of the REINFORCE, RELAX and KF-RELAX gradient estima-\ntors in the synthetic problem with t = 0.499. The learning rate for optimizing θ is set to a common\nvalue for all the estimators. The surrogate function learning rate is set to a same value for both the\nRELAX and KF-RELAX.\nTherefore, the natural gradient update for the layer l can be approximated as,\n∆Wl = ˆFl\n−1∇WlJ = A−1∇WlJS−1\nFor more details about the derivation refer to the (Martens and Grosse, 2015).\n3\nProposed Method\nThis section uses KFAC for the LAX/RELAX (Grathwohl et al., 2017) gradient estimators which\ncan be applied in both continuous and discrete domains. LAX (continuous cases) and RELAX\n(discrete cases) leverage the score function and the reparameterization trick to obtain a low variance\nestimator without adding bias to the initial gradient estimator.\nLet x be a continuous random variable and consider the optimization of Ep(x|θ)[f(x)] with respect\nto θ. The LAX estimator (Grathwohl et al., 2017) is given by,\nˆgLAX = ˆgREINF ORCE[f] −ˆgREINF ORCE[cφ] + ˆgREP ARAM[cφ]\n= [f(x) −cφ(x)] ∂\n∂θ log p(x|θ) + ∂\n∂θcφ(x),\nx = T(θ, ϵ), ϵ ∼p(ϵ)\nwhere cφ is the surrogate function. Parameters of the surrogate function are trained in order to\nminimize the variance of the gradient estimator. In case that that cφ is equal to f, LAX and the repa-\nrameterization gradient estimator are the same. By applying KFAC to estimate the natural gradient\nfor the surrogate function parameters, KF-LAX procedure is obtained. The procedure is described\nin the algorithm 1.\nSimilarly, in case that x is a discrete random variable, RELAX can be written as,\nˆgRELAX = [f(x) −cφ(˜z)] ∂\n∂θ log p(x|θ) + ∂\n∂θcφ(z) −∂\n∂θcφ(˜z),\nx = H(z), z ∼p(z|θ), ˜z ∼p(z|x, θ)\nwhere H represents the heaviside function2. Following the same approach for the LAX, we get\nanother optimization algorithm for RELAX which we call KF-RELAX. This algorithm is provided\nin Appendix A.\n4\nExperiments\n4.1\nSynthetic problem\nKF-RELAX is applied to a synthetic problem. Consider the minimization of Ep(b|σ(θ))[(b −t)2]\nwhere t ∈(0, 1) is a constant and b is a binary random variable sampled from a Bernoulli distribution\n2H(z) = 1 if z ≥0 and H(z) = 0 if z < 0\n4\nwith parameter σ(θ), where σ(.) indicates the sigmoid function. According to the selection of t,\nthe optimal distribution is a deterministic Bernoulli distribution with parameter 0 or 1. Figure 1\ncompares the convergence of REINFORCE, KF-RELAX and RELAX for t = 0.499, using only\na single sample of x per iteration. Also, in appendix B, it is empirically shown that all gradient\nestimators are unbiased.\n4.2\nReinforcement learning\nConsider a ﬁnite discrete-time Markov Decision Process (MDP) which is a tuple (χ, ρ0, γ, A, r, P).\nAt time t = 0, an agent is in the state s0 which is sampled from the initial distribution ρ0 : χ →R.\nAt each time t, the agent chooses an action at ∈A from its policy distribution πθ(at|st), gets a\nreward according to the reward function r(st, at) and transitions to the next state ss+1 according\nto the Markov transition probabilities P(st+1|st, at). Final goal of the agent is to maximize its\nexpected reward J(θ) = Es0,a0,τ[P∞\nt=0 γtr(st, at)] = Es∼ρπ(s),a,τ[P∞\nt=0 γtr(st, at)], where τ\nindicate the trajectories, γ is the discount factor and ρπ = P∞\nt=0 γtP(st = s) is the unnormalaized\nstate visitation frequency. Using the policy gradient theorem (Sutton et al., 1998), the gradient of\nJ(θ) can be written in the following expectation form,\n∇J(θ) = Es∼ρπ(s),a,τ[Qπ(s, a)∇θ log πθ(a|s)] = Es∼ρπ(s),a,τ[Aπ(s, a)∇θ log πθ(a|s)]\nWhere, Qπ(s, a) is the state-action value function, V π(s) is the value function, and Aπ(s, a) =\nQπ(s, a)−V π(s) is the advantage function. Due to simplicity of the selected tasks, same as (Grath-\nwohl et al., 2017), we use the Q function approximation, instead of directly estimating the advantage\nfunction.\n4.2.1\nDiscrete Control Tasks\nKF-RELAX and RELAX performance are compared in three discrete control RL tasks. According\nto appendix C of the (Grathwohl et al., 2017), using a sampled trajectory, the RELAX gradient\nestimator can be written as,\nˆgRL\nRELAX =\nT\nX\nt=1\n∂\n∂θ log π(at|st, θ)[ ˆQπ(at, st) −cφ(˜zt, st)] −∂\n∂θcφ(˜zt, st) + ∂\n∂θcφ(zt, st)\nKF-RELAX gradient estimator has a similar form with a difference that at each step, the natural\ngradient of the surrogate function is estimated.\nThe learning rate for both the agent and the surrogate function were chosen from the set\n{0.03, 0.01, 10−3, 10−4}. cφ is chosen as a three fully-connected layer neural network. For the\nRELAX, all models were trained using Adam (Kingma and Ba, 2014). Entropy regularization\nwith a weight of 0.01 was used to increase the exploration, and the discount factor was set to\n0.99. Tikhonov damping technique described by (Martens and Grosse, 2015) with the value in\n{0.1, 0.01, 10−3, 5 × 10−4} was used to further control the training stability. The same value of\nmini-batch size was used for both gradient estimators. In addition, as in (Wu et al., 2017), a trust re-\ngion approach was used for KF-RELAX, whereby the parameter updates capped at a speciﬁc upper\nbound to prevent the agent from converging to a near-deterministic policy during the early training\nstages. The value of the upper bound was chosen from {10−3, 10−4, 10−5, 10−6}. The experi-\nments are done on Cart-pole, Lunar-Lander, and Acrobot which were selected from the OpenAI\ngym environments (Brockman et al., 2016). The comparison of KF-RELAX and RELAX in these\nenvironments are shown in the ﬁgure 2.\n5\nRelated Works\nNatural policy gradient method was ﬁrst introduced in (Kakade, 2002). In particular case where the\nstate-value function is approximated with a compatible function approximator, the natural gradient\ntends toward the optimal greedy policy improvement (Appendix C shows the same fact holds for the\nadvantage function which is frequently used for policy optimization). Natural gradient was used for\n5\n(a) Cart pole\n(b) Lunar lander\n(c) Acrobot\nFigure 2: Training curve for KF-RELAX and RELAX on three discrete task Atari games.\nactor-critic methods by (Peters and Schaal, 2008). More recently, Trust Region Policy Optimization\n(TRPO) (Schulman et al., 2015) performs an optimization for each step of the policy update and the\nnatural gradient policy update is a particular case of TRPO. However, TRPO does not scale to neural\nnetworks with larger architectures.\nKronecker-factored curvature approximation (KFAC) technique is also applied in practice to im-\nprove the training convergence speed. For example, (Grosse and Martens, 2016) used KFAC to\nestimate the Fisher information of the convolutional layers in neural networks. Recently, (Martens\net al., 2018) extended KFAC to recurrent neural networks. (Wu et al., 2017) applies KFAC to op-\ntimize both the actor and the critic of an actor-critic model for deep RL and proposed the ACKTR\nmethod. Inspired by ACKTR, this work uses the KFAC technique for the surrogate function in\nRELAX. In the continuous deep RL applications, a vanilla Gauss-Newton method can be used to\nleverage KFAC for the agent as well (Wu et al., 2017; Nocedal and Wright, 2006).\n6\nConclusion and Future Work\nEven though the KF-LAX (KF-RELAX) could provide stronger update rules for optimization and\nreduce the number of samples for convergence, tuning the hyperparameters for KFAC can be chal-\nlenging in the deep RL framework as the estimation of the gradient may exhibit a high variance.\nIll speciﬁcation of parameters like the damping factor, and the scaling factor may cause a training\ncollapse or no improvement in sample size needed for training. Fortunately, these hyperparameters\nare studied for practical usage and these studied are beneﬁcial for the tuning process (Martens and\nGrosse, 2015; Wu et al., 2017).\nChoosing a complicated architecture for the surrogate function can negatively inﬂuence the training\nprogress or even add to the variance of the gradient estimation, hence degrading the convergence. In\nour experiments, a surrogate function with more than three layers increases the gradient estimation\nvariance. This is considered a limitation of using this approach. Stabilizing the training process for\nmore complicated surrogate functions is considered as a future work.\nIn this work we leveraged two recently introduced optimization techniques, KFAC and RELAX, to\nget more powerful gradient steps. We experimented the proposed approach on a few discrete control\ntasks in reinforcement learning. We believe this approach can be promising in improving the sample\nefﬁciency of RELAX in reinforcement learning. Further experiments for discrete and continuous\ntasks are considered as the future works.\nReferences\nShun-Ichi Amari. Natural gradient works efﬁciently in learning. Neural computation, 10(2):251–\n276, 1998.\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\nWojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\nPeter W Glynn. Likelihood ratio gradient estimation for stochastic systems. Communications of the\nACM, 33(10):75–84, 1990.\n6\nWill Grathwohl, Dami Choi, Yuhuai Wu, Geoff Roeder, and David Duvenaud. Backpropagation\nthrough the void: Optimizing control variates for black-box gradient estimation. arXiv preprint\narXiv:1711.00123, 2017.\nRoger Grosse and James Martens. A kronecker-factored approximate ﬁsher matrix for convolution\nlayers. In International Conference on Machine Learning, pages 573–582, 2016.\nShixiang Gu, Sergey Levine, Ilya Sutskever, and Andriy Mnih. Muprop: Unbiased backpropagation\nfor stochastic neural networks. arXiv preprint arXiv:1511.05176, 2015.\nShixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E Turner, and Sergey Levine. Q-prop:\nSample-efﬁcient policy gradient with an off-policy critic. arXiv preprint arXiv:1611.02247, 2016.\nMatthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational infer-\nence. The Journal of Machine Learning Research, 14(1):1303–1347, 2013.\nEric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv\npreprint arXiv:1611.01144, 2016.\nSham M Kakade. A natural policy gradient. In Advances in neural information processing systems,\npages 1531–1538, 2002.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\nDiederik P Kingma and Max Welling.\nAuto-encoding variational bayes.\narXiv preprint\narXiv:1312.6114, 2013.\nChris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous\nrelaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.\nJames Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate\ncurvature. In International conference on machine learning, pages 2408–2417, 2015.\nJames Martens, Jimmy Ba, and Matt Johnson. Kronecker-factored curvature approximations for\nrecurrent neural networks. 2018.\nJorge Nocedal and Stephen J Wright. Nonlinear Equations. Springer, 2006.\nJan Peters and Stefan Schaal. Reinforcement learning of motor skills with policy gradients. Neural\nnetworks, 21(4):682–697, 2008.\nDanilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and\napproximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.\nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region\npolicy optimization. In International Conference on Machine Learning, pages 1889–1897, 2015.\nRichard S Sutton, Andrew G Barto, et al. Reinforcement learning: An introduction. 1998.\nGeorge Tucker, Andriy Mnih, Chris J Maddison, John Lawson, and Jascha Sohl-Dickstein. Rebar:\nLow-variance, unbiased gradient estimates for discrete latent variable models. In Advances in\nNeural Information Processing Systems, pages 2624–2633, 2017.\nRonald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforce-\nment learning.\nMach. Learn., 8(3-4):229–256, May 1992. ISSN 0885-6125. doi: 10.1007/\nBF00992696. URL https://doi.org/10.1007/BF00992696.\nCathy Wu, Aravind Rajeswaran, Yan Duan, Vikash Kumar, Alexandre M Bayen, Sham Kakade,\nIgor Mordatch, and Pieter Abbeel. Variance reduction for policy gradient with action-dependent\nfactorized baselines. arXiv preprint arXiv:1803.07246, 2018.\nYuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable trust-region\nmethod for deep reinforcement learning using kronecker-factored approximation. In Advances in\nneural information processing systems, pages 5285–5294, 2017.\nA\nKF-RELAX\nUsing the KFAC technique for the RELAX gradient estimator results the KF-RELAX procedure for\ndiscrete cases which is explained in the Algorithm 2.\n7\nAlgorithm 2 KF-RELAX\nRequire: f(.), log p(x|θ), x = T(θ, ϵ), p(ϵ), a neural network surrogate cφ(.) with weights Wl for\neach layer l, z = S(θ, ϵ), ˜z = S(θ, ϵ|x) step sizes α1, α2,\n1: while not converged do\n2:\nϵ, ˜ϵ ∼p(ϵ)\n3:\nz ←S(θ, ϵ)\n4:\nx ←H(z)\n5:\n˜z ←S(θ, ϵ|x)\n6:\nˆgθ ←[f(x) −cφ(˜z)]∇θ log p(x|θ) + ∇θcφ(z) −∇θcφ(˜z)\n7:\nfor each layer l of cφ do\n8:\nestimate the matrices A and S for the layer l using KFAC.\n9:\nWl ←Wl −α2A−1∇Wlˆg2\nθS−1\n10:\nend for\n11:\nθ ←θ −α1ˆgθ\n12: end while\n13: return θ\nB\nRELAX and KF-RELAX are unbiased\nFigure 3 shows that same as the REIFNORCE, RELAX and KF-RELAX are unbiased and converge\nto the true optimal solution in the synthetic problem which is described in section 4.1. Since for t =\n0.499 the REINFORCE gradient estimator does not necessarily converge, a simpler case t = 0.49\nis used in ﬁgure 3.\nFigure 3: Convergence of REINFORCE, RELAX, and KF-RELAX to the true solution in the syn-\nthetic problem with the target value t = 0.49.\nC\nUse of the advantage function in the natural policy gradient\nIn this section we rewrite a similar results to (Kakade, 2002) that with an approximation of the\nadvantage function by some compatible function approximator, the natural gradient update rule\ntends to move toward the best action.\nLemma: Suppose an approximation of the advantage function with a function f π(s, a; w) is desired.\nLet f π be in the form of wT ∇log π(a|s, θ) and let ˜w be the minimizer of the square error ϵ(w, π) =\nEs∼ρπ,a[(Aπ(s, a) −f π(s, a; w))2]. Then\n˜w = ˜∇J(θ)\nwhere J(θ) = Es,a,τ[P∞\nt=0 γtrt], and ˜∇indicates the natural gradient.\nProof: Since ˜w minimizes the ϵ(w, π), we have\n∂ϵ\n∂wi = 0. For simplicity in the notation we set\nψ(s, a) = ∇log π(a|s, θ). Therefore,\n8\nX\ns,a\nρπ(s)π(a|s, θ)ψ(s, a)(ψ(s, a)T ˜w −Aπ(s, a)) = 0\nRearranging this equation,\nX\ns,a\nρπ(s)π(a|s, θ)ψ(s, a)ψ(s, a)T ˜w =\nX\ns,a\nρπ(s)π(a|s, θ)ψ(s, a)Aπ(s, a)\nThe policy gradient theorem (Sutton et al., 1998) states that\n∇J(θ) = Es∼ρπ(s),a,τ[Qπ(s, a)∇θ log πθ(a|s)] = Es∼ρπ(s),a,τ[A(s, a)∇log π(a|s)]\nAlso, by using the fact that ∇π(a|s, θ) = π(a|s, θ)ψπ(s, a) and the deﬁnition of the Fisher infor-\nmation matrix F(θ) = P\ns,a ρπ(s)π(a|s, θ)ψ(s, a)ψ(s, a)T both sides can be rewritten as,\nF(θ) ˜w = ∇J(θ)\nSolving for ˜w gives ˜w = F −1(θ)∇J(θ) which is the deﬁnition of the natural gradient.\n□\nThe following lemma states that based on our approximation of the Aπ(a|s, θ), the natural gradient\nlocally tends toward the best action. An action a is best if a ∈argmaxa′f π(s, a′; ˜w). Intuitively,\nthe best action is deﬁned by performing greedy policy with regard to our approximation of the\nadvantage function.\nLemma: Let θ′ = θ + ∆θ be the natural gradient update of the policy parameters where ∆θ =\nα ˜∇J(θ) = α ˜w, then\nπ(a|s, θ′) = π(a|s, θ)(1 + f π(s, a; w)) + O(α2)\nProof: To the ﬁrst order Taylor approximation,\nπ(a|s, θ′) = π(a|s, θ) + ∂π(a|s, θ)\n∂θ\n∆θ + O(∆θ2)\n= π(a|s, θ)(1 + αψ(s, a)T ˜w) + O(∆θ2)\n= π(a|s, θ)(1 + αf π(s, a; w)) + O(α2)\n□\n9\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2018-12-11",
  "updated": "2018-12-11"
}