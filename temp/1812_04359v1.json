{
  "id": "http://arxiv.org/abs/1812.04359v1",
  "title": "Efficient Model-Free Reinforcement Learning Using Gaussian Process",
  "authors": [
    "Ying Fan",
    "Letian Chen",
    "Yizhou Wang"
  ],
  "abstract": "Efficient Reinforcement Learning usually takes advantage of demonstration or\ngood exploration strategy. By applying posterior sampling in model-free RL\nunder the hypothesis of GP, we propose Gaussian Process Posterior Sampling\nReinforcement Learning(GPPSTD) algorithm in continuous state space, giving\ntheoretical justifications and empirical results. We also provide theoretical\nand empirical results that various demonstration could lower expected\nuncertainty and benefit posterior sampling exploration. In this way, we\ncombined the demonstration and exploration process together to achieve a more\nefficient reinforcement learning.",
  "text": "Efﬁcient Model-Free Reinforcement Learning Using Gaussian Process\nYing Fan 1 Letian Chen 1 Yizhou Wang 1\nAbstract\nEfﬁcient Reinforcement Learning usually takes\nadvantage of demonstration or good exploration\nstrategy.\nBy applying posterior sampling in\nmodel-free RL under the hypothesis of GP, we\npropose GPPSTD algorithm in continuous state\nspace, giving theoretical justiﬁcations and empiri-\ncal results. We also provide theoretical and empir-\nical results that various demonstration could lower\nexpected uncertainty and beneﬁt posterior sam-\npling exploration. In this way, we combined the\ndemonstration and exploration process together to\nachieve a more efﬁcient reinforcement learning.\n1. Introduction\nOver the past years, Reinforcement Learning (RL) has\nachieved a great success in tasks such as Atari Games (Mnih\net al., 2015), Go (Silver et al., 2016), robot control (Levine\net al., 2016) and high-level decisions (Silver et al., 2013).\nBut in general, the conventional RL approaches can hardly\nobtain a good performance before a large number of ex-\nperiences are collected. Therefore, two types of methods\nhave been proposed to realize sample efﬁcient learning, i.e.\nleveraging human demonstration (e.g. inverse RL (Ng et al.,\n2000)) and designing better exploration strategies. Although\nthe literature has plenty of interesting studies on either one,\nthere seems lack of work combining them to our best knowl-\nedge. In this paper we propose a new model-free exploration\nstrategy which leverages all kinds of demonstrations (even\nincluding unsuccessful ones) to improve learning efﬁciency.\nExisting works on learning from demonstration are mainly\nfocused on inferring the underlying reward function (in IRL)\nor imitating of the expert demonstrations (Ng et al., 2000;\nAbbeel & Ng, 2004; Ho & Ermon, 2016; Hester et al., 2017).\nHence, most methods can only exploit demonstrations that\nare optimal. However, the very optimal demonstrations are\n*Equal contribution 1Nat’l Engineering Laboratory for Video\nTechnology Cooperative Medianet Innovation Center Key Lab-\noratory of Machine Perception (MoE) Sch’l of EECS, Peking\nUniversity, Beijing, 100871, China. Correspondence to: Yizhou\nWang <yizhou.wang@pku.edu.cn>.\nhard to obtain in practice since it is known that humans\noften perform suboptimal behaviors. Therefore, mediocre\nand unsuccessful demonstrations have long been neglected\nor even expelled in RL. In this paper, we show how to make\nuse of seemingly-useless demonstrations in the exploration\nprocess to improve sample efﬁciency.\nSpeaking of efﬁcient exploration strategy, it expects an agent\nto balance between exploring poorly-understood state-action\npairs to get better performance in the future and exploiting\nexisting knowledge to get better performance now. The\nexploration vs exploitation problem also has two families of\nmethods: model-based and model-free. Model-based means\nthe agent explicitly model the Markov Decision Process\n(MDP) environment, then does planning over the model. In\ncontrast, model-free methods maintain no such environment\nmodel. Typical model-free exploration approaches include ϵ-\ngreedy(Sutton & Barto, 1998), optimistic initialization(Ross\net al., 2011), and more sophisticated ones such as noisy\nnetwork (Fortunato et al., 2017) and curiosity(Pathak et al.,\n2017). These model-free exploration strategies usually are\ncapable to handle large scale real problems, however, they\ndo not have a theoretic guarantee. Whereas, the model-\nbased explorations are more systematic, thus often have the-\noretic bounds, such as Optimism in the Face of Uncertainty\n(OFU)(Jaksch et al., 2010) and Posterior Sampling (PS) Re-\ninforcement Learning (PSRL)(Osband et al., 2013). Despite\nthe beautiful theoretical guarantees, the model-based meth-\nods suffer from signiﬁcant computation complexity when\nstate-action space is large, hence usually not suitable for\nlarge scale real problem.\nHow can we combine the advantage of both demonstra-\ntion and exploration strategy to gain an even more efﬁcient\nlearning for RL? In this paper, we propose a model-free RL\nexploration algorithm GPPSTD using posterior sampling\non joint Gaussian value function, and provide theoretical\nanalysis about its efﬁciency in the meantime. We also make\nuse of various demonstrations to decrease the expectation\nuncertainty of Q value model, and then leverages this advan-\ntage in implementing PS on Q values to gain more efﬁcient\nexploration.\nIn summary our contributions include:\n• Show that posterior sampling based on model-free\narXiv:1812.04359v1  [cs.LG]  11 Dec 2018\nEfﬁcient Model-Free Reinforcement Learning Using Gaussian Process\nGaussian Process could achieve a BayesRegret Bound\nof\n˜O(\n√\nHT) with deterministic environment and\nbayesian cumulative error of estimation bound for a\nsingle state of ˜O(\nq\n⌈T\nH ⌉).\n• Propose the GPPSTD algorithm to leverage posterior\nsampling together with various demonstration to im-\nprove the learning efﬁciency of RL.\n• Prove that making use of various demonstrations could\ndecrease the expectation of GP uncertainty.\n• Show empirical results for GPPSTD exploration efﬁ-\nciency and an even more efﬁcient learning when using\nvarious demonstrations.\n2. Related Work\nTwo typical methods of learning from demonstration, are\ninverse reinforcement learning (IRL) and imitation learn-\ning (IL). Inverse reinforcement learning was introduced\nin Ng et al. 2000. Its goal is to infer the underlying re-\nward function given the optimal demonstration behavior.\nFurther IRL algorithm includes Bayesian IRL (Ramachan-\ndran & Amir, 2007; Michini & How, 2012), Maximum\nEntropy IRL (Ziebart et al., 2008; Audiffren et al., 2015),\nRepeated IRL (Amin et al., 2017), etc. But IRL can be\nintractable when problem scale is large. Earlier imitation\nlearning indicates behavior cloning, which could fail when\nagent encounters untrained states. Later representative IL\nalgorithm includes Data Aggregation (DAgger) (Ross et al.,\n2011), Generative Adversarial Imitation Learning (GAIL)\n(Ho & Ermon, 2016), etc. However, their work focuses on\nimitating optimal demonstration, regarding mediocre and\nfailed demonstration unusable. They also never consider\nexploration problem after imitating.\nAs for the exploration problem, two intuitive methods, ϵ-\ngreedy(Sutton & Barto, 1998) and Optimistic Initializa-\ntion(Grze´s & Kudenko, 2009), are the earliest way to tackle\nthis problem. ϵ-greedy is to explore with a probability ϵ.\nOptimistic Initialization initializes all Q values to rmax\n1−γ ,\nmaking RL visit each state at least some times. Model\nbased method Optimism in the Face of Uncertainty (OFU)\nis to assign each state-action pair a biased estimate of future\nvalue and selects the action with highest estimate (Jaksch\net al., 2010). Posterior sampling method has been proposed\nsince (Strens, 2000), involving sampling a set of values from\nposterior estimation and selecting the action with maximal\nsampled value. PSRL proposed by Osband et al.(2013)\ndoes PS on the Markov Decision Process (MDP): in every\nepisode, PSRL sample a MDP , run model-based planning\nalgorithm and acts as if it is the true optimal policy. For\nﬁnite horizon algorithms, regret bound of O(HS\n√\nAT) is\nachieved by PSRL (Osband et al., 2013), and O(H\n√\nSAT)\nby GPSRL(Osband & Van Roy, 2017). It is notable that\nthese methods are all model-based with ﬁnite SA space,\nwhich can be a considerable limitation in application.\nHowever, since PSRL is a model-based algorithm, it suffers\nfrom signiﬁcant computation complexity for planning when\nstate and action space are large. Therefore, in this paper we\nbuilt model on value function based on Gaussian Process\n(GP), making it model-free, and to achieve both exploration\nefﬁciency and tractable computation complexity.\nPrevious model-free algorithms have also been proposed\nusing GP in RL. GP-SARSA (Engel et al., 2005) used GP\nto update posterior estimation of value function by temporal\ndifference method. iGP-SARSA proposed informative ex-\nploration but lacks theoretical analysis (Chung et al., 2013).\nGPQ for both on-line and batch settings aims at learning Q\nfunction which could actually converge as T →∞(Chowd-\nhary et al., 2014) but lacks efﬁcient exploration. DGPQ\nemployed delayed update of Q function to achieve PAC-\nMDP(Grande et al., 2014) but still lacks efﬁcient explo-\nration.\nFor regret bounds under GP hypothesis, Srinivas et al.(2012)\nused GP to analyze the regret bound using information gain\nin bandit problems, while posterior sampling using GP and\nrelated analysis of regret bounds had not been explored yet,\nwhich would be discussed in this paper.\n3. Theoretical Analysis\nIn this section, we will show that how to choose demonstra-\ntions to achieve lower expected estimation variance, analyze\nrelated bounds of posterior sampling in RL under the hy-\npothesis of GP for both deterministic and non-deterministic\nMDPs, and ﬁnally relate the choice of demonstrations and\nposterior sampling for efﬁciency improvement.\n3.1. Expectation of variance conditioned on data in GP\nWe choose joint Gaussian distribution on value function\n– more speciﬁcally, Gaussian Process (GP) – because GP\nprovides a principled, practical, probabilistic approach to\nlearn in kernel machines(Rasmussen & Williams, 2006).\nWe assume that the values in the value function are joint\nnormal distributed. Under the GP assumption, the posterior\ndistribution are given by\nf ∗|X∗, X, f ∽N(K(X∗, X)K(X, X)−1f,\nK(X∗, X∗) −K(X∗, X)K(X, X)−1K(X, X∗)),\n(1)\nwhere f is the value of the state vector X, and we wish to\nobtain value estimation f ∗over the new observation X∗. f\nand X come from history or what we call experiences. We\ndeﬁne p(x) as the distribution of test points, i.e. the states\nwhich occur in RL. In the framework of RL, x is every sin-\ngle state and its visiting distribution p(x) is determined by\nEfﬁcient Model-Free Reinforcement Learning Using Gaussian Process\ncurrent policy µ and the MDP (Markov Decision Process).\nWe will start by a theorem that is quite obvious from intu-\nition but hasn’t been proved yet.\nTheorem 1 When a set (X′, f) is used to estimate f(x∗)\nin GP, the expectation of variance on test points x∗with\ndistribution p(x) conditioned on all possible training set\n(X’, f) set would not be less than what conditioned on the\ntraining set X sampled from distribution p(x), if the size\nof sample set is large enough to ignore the approximation\nerror.\nZ\n{K(x∗, x∗) −K(x∗, X)K(X, X)−1K(X, x∗))}p(x)dx\n≤\nZ\n{K(x∗, x∗) −K(x∗, X′)K(X′, X′)−1K(X′, x∗))}p(x)dx\n(2)\nProof\nGiven a kernel K, together with a distribution p(x),\nthere is a corresponding series of eigenfunctions φ(x), s.t.\nR\nk(x, x′)φ(x)dµ(x) =\nR\nk(x, x′)φ(x)p(x)dx = λφ(x′),\nand ∀i,\nR\nφi(x)φ∗\nj(x)p(x)dx = δij(* here means conjugate\ntranspose) (Rasmussen & Williams, 2006).\nWe consider the expectation of posterior variance over\nthe distribution p(x) given any X′ as\nR\n{K(x∗, x∗) −\nK(x∗, X′)K(X′, X′)−1K(X′, x∗))}p(x)dx.\nSince\nR\nK(x∗, x∗)p(x)dx\nhas\nno\nrelation\nwith\nX′,\nwe\njust\nfocus\non\nthe\nlatter\nsubtracted\npart\nR\nK(x, X′)K(X′, X′)−1K(X′, x)p(x)d(x).\nAccord-\ning to Mercer’s theorem, K(x, x′) = Σ∞\ni=1λiφi(x)φ∗\ni (x′).\nZ\nK(x, X′)K(X′, X′)−1K(X′, x)p(x)d(x)\n=\nZ\n{(Σ∞\ni=1λiφi(x)φ∗\ni (X′))K(X′, X′)−1\n(Σ∞\nj=1λjφj(X′)φ∗\nj(x))}p(x)dx.\n(3)\nIf i does not equals to j, the integral would be 0.\nSo\nZ\n{Σ∞\ni=1λiφi(x)φ∗\ni (X′)K(X′, X′)−1λiφi(X′)φ∗\ni (x)}p(x)dx\n= Σ∞\ni=1λ2\ni φ∗\ni (X′)K(X′, X′)−1φi(X′).\nFor each i, focus on φ∗\ni (X′)K(X′, X′)−1φi(X′).\nUsing numerical approximation of eigenfunctions (Ras-\nmussen & Williams, 2006), when each xl is sampled from\nthe distribution p(x), λiφi(x) =\nR\nk(x, x′)p(x)φi(x) ⋍\n1\nnΣn\nl=1k(xl, x′)φi(xl).\nPlugging in x′\n= xl, we get\nK(X, X)ui = λmat\ni\nui, where X = [xl] and Ki,j =\nk(xi, xj),and each ui and λmat\ni\nis the eigenvector and\neigenvalue of matrix K(X, X), with the approximation\nφi(X) ⋍√nui, 1\nnλmat\ni\n⋍λi.\nGiven a random set of X′, and a sampled set of X, al-\nthough we do not know φ(x) exactly, we can use X to\nestimate the value of eigenfunctions of X′: φi(X′) ⋍\n√n\nλmat\ni\nK(X′, X)ui.\nNow that\nφ∗\ni (X′)K(X′, X′)−1φi(X′)\n⋍\nn\n(λmat\ni\n)2 uT\ni K(X, X′)K(X′, X′)−1K(X′, X)ui\n, when n →∞we could regard all above estimations as\nasymptotic unbiased estimations, and here we suppose n\nis large enough to ignore the approximation error so the\napproximate equations can be seen as equations.\nApplying matrix decomposition to symmetric non-negative\ndeﬁnite matrix K(X′, X′) ,\nK(X′, X′)−1 = Σn\nj=1\n1\nλ′mat\nj\nvjv∗\nj .\nSo\nn\n(λmat\ni\n)2 u∗\ni K(X, X′)K(X′, X′)−1K(X′, X)ui\n=\nΣj\nn\n(λmat\ni\n)2λ′mat\nj\n||u∗\ni K(X, X′)vj||2.\nOn each\nn\n(λmat\ni\n)2λ′mat\nj\n||u∗\ni K(X, X′)vj||2, we have:\nK(X, X′) = Σφ(X)φ(X′)∗= ψ(X)ψ(X′)∗\n(4)\nuiK(X, X)u∗\ni = λmat\ni\n= uiψ(X)ψ(X)∗u∗\ni = ||uiψ(X)||2\n(5)\nvjK(X′, X′)v∗\nj = λ\n′mat\nj\n= vjψ(X′)ψ(X′)T v∗\nj = ||viψ(X′)||2\n(6)\nAccording to Cauchy-Schwartz inequality,\n|uiψ(X)ψ(X′)∗v∗\nj |\n⩽\n||uiψ(X)||\n||viψ(X′)||\n=\nq\nλmat\ni\nλ\n′mat\nj\n.\nSo\nn\n(λmat\ni\n)2λ\n′mat\nj\n||uT\ni K(X, X′)vj||2 ⩽\nn\n(λmat\ni\n)2λ\n′mat\nj\nλmat\ni\nλ\n′mat\nj\n=\nn\nλmat\ni\n= 1\nλi ,\n(7)\nand when λ\n′mat\nj\nequals to λmat\ni\nthe result can reach its\nlargest, and the lowest expectation of overall conditional\nvariance is\nR\nk(x, x)p(x)dx −Σ∞\ni=1λi. Especially, when\nRBF kernel is selected, under any p(x) the lowest expec-\ntation would be 1 −Σ∞\ni=1λi = 1 −limn→∞\nΣn\ni=1λmat\ni\nn\n=\n1 −limn→∞\ntrace(KXX)\nn\n= 0.\nEfﬁcient Model-Free Reinforcement Learning Using Gaussian Process\nMoreover, if the kernel contains noise as below:\nf ∗|X∗, X, f ∽N(K(X∗, X)[K(X, X) + σ2I]−1f,\nK(X∗, X∗) −K(X∗, X)[K(X, X) + σ2I]−1K(X, X∗)),\n(8)\nK(X, X) + σ2I would still be a symmetric non-negative\ndeﬁnite matrix, so the eigenvalues in previous analysis\nwould all be added with σ2, and the eigenvectors remain the\nsame. Obviously the conclusion still remains the same. □\nNotice that during a learning process of RL, if the agent\nhas not learned how to perform perfectly yet, under present\npolicy the states which the agent would come across would\nnot be those of highest real value. So non-perfect demonstra-\ntions are necessary to lower the expectation of uncertainty\nduring exploration.\n3.2. BayesRegret of GP-based Posterior Sampling\n3.2.1. DETERMINISTIC MDP\nWe start with a simple case where transitions are determin-\nistic in MDP.\nWe model each MDP M = {S, A, RM, P M, H} ∼φ, with\npotentially inﬁnite sets of states S and actions A. H is\nthe length of a single episode. At timestep t of an episode,\nthe agent observe st ∈S, select at ∈A, receive a reward\nrM\nst,at ∼RM(st, at) and transition st+1 = P M(st, at).\n¯rM\nst,at = E[rM\nst,at|rM\nst,at ∼RM(st, at)].\nµ is the policy function of state, and value function:\nVµ,M(s) = E[Σ∞\ni=0γi¯rM\nsi+1,ai+1|si+1 = P M(si, ai), ai =\nµ(si)], where γ is the rate of discount and satisﬁes 0 < γ ≤\n1.\nM k\nis\nthe\nposterior\nsample\nof\nunknown\ntrue\nMDP\nM ∗\ngiven\nhistory\nHkt,\nHkt\n=\n{s1,1, a1,1, r1,1, s1,2, ......sk,t−1, ak,t−1, rk−1,t−1}.\nµMis\nthe optimistic policy under M, µk ∈argmaxµVµ,M k(s),\nand particularly, µk, µ∗is the optimistic policy under\nM k, M ∗separately. π indicates the learning algorithm\nwhich choose a policy µ for the agent to perform.\nWe assume that given MDP VµM,M(s) is joint normal on the\nset of state S with optimal policy µM in M, which contains\nthe assumption of the model using a model-free method.\nDeﬁne expected cumulative reward of the kth episode:\nSµ,M\nk\n= E[ΣHk\nt=H(k−1)+1¯rM\nst,at\n|s(t + 1) = P M(st, at), ai = µ(si)].\n(9)\nRegret :\nRegret(T, π, M ∗) = Σ\n⌈T\nH ⌉\nk=1 (Sµ∗,M∗\nk\n−Sµk,M∗\nk\n).\n(10)\nThe regret of every episode is random due to the unknown\ntrue MDP M ∗, the learning algorithm π, the sampling M k\nof the present episode and previous sampling through history\nHk1. Notice that in our algorithm we do not directly sample\nM k from the posterior distribution φ(·|Hk1) and we use the\nposterior distribution of the value to realize our sampling.\nBut for convenience we would use sampled M k to refer to\nour way of sampling in practice.\nAnd Bayesian regret:\nBayesRegret(T, π, φ) =\nE[Σ\n⌈T\nH ⌉\nk=1 (Sµ∗,M∗\nk\n−Sµk,M∗\nk\n)|M ∗∼φ].\n(11)\nwhich is actually the same with the regret deﬁned by Osband\n& Van Roy(2017). Since we have different deﬁnition of the\nvalue function, we use other notations to avoid confusion.\nWe separate this BayesRegret by episodes, where each\nepisode k conditioned on the previous history Hk1 then\ntaking expectation again in order to achieve the expecta-\ntion on M ∗. We discuss the relation between BayesRe-\ngret and the conditional regret, which is different from\nthe method of previous work (Osband & Van Roy, 2017).\nThe conditional regret is E[Sµ∗,M ∗\nk\n−Sµk,M ∗\nk\n|M ∗\n∼\nφ(·|Hk1)], and Σ\n⌈T\nH ⌉\nk=1 E[Sµ∗,M ∗\nk\n−Sµk,M ∗\nk\n|M ∗∼φ] =\nE[Σ\n⌈T\nH ⌉\nk=1 {E[Sµ∗,M ∗\nk\n−Sµk,M ∗\nk\n|M ∗∼φ(·|Hk1)]|Hk1 ∼\nPreviousSampling}]. It is obvious that each Hk1 here\ncontains previous history and not independent. So when\nwe take expectation of a series of Hk1, we actually take\nexpectation on whole history H.\nSince we will use the stochastic property of M k to analyze\nthe bound, another thing to notice is that since H is actually\nproduced by every sampled M k, taking expectation of H\nwould not disturb the distribution of M k. So if we can bound\nconditional regret (as described above) on every possible\nM k from its distribution, then taking the expectation would\nalso bound BayesRegret.\nTheorem 2 Let M ∗be the true MDP with deterministic\ntransitions according to prior φ with values under GP\nhypothesis.\nThen the regret for GPPSTD is bounded:\nBayesRegret(T, πGP P ST D, φ) = ˜O(\n√\nHT).\nProof\nDecomposition:\nSµ∗,M∗\nk\n−Sµk,M∗\nk\n= (Sµ∗,M∗\nk\n−Sµk,Mk\nk\n)+(Sµk,Mk\nk\n−Sµk,M∗\nk\n).\n(12)\nFirst we focus on the difference we can observe by the policy\nEfﬁcient Model-Free Reinforcement Learning Using Gaussian Process\nµk that the agent actually follows, i.e. Sµk,M k\nk\n−Sµk,M ∗\nk\nin\n(12).\nReferring to previous deﬁnation (9),\nEM∗[Sµk,Mk\nk\n−Sµk,M∗\nk\n|Hk1] = Vµk,Mk(s1)\n+ ΣH−1\nt=2 (1 −γ)Vµk,Mk(st) −γVµk,Mk(sH) −EM∗[Vµk,M∗(s1)\n+ ΣH−1\nt=2 (1 −γ)Vµk,M∗(s′\nt) −γVµk,M∗(s′\nH)|Hk1],\n(13)\nwhere EM ∗[\n|Hk1] means taking expectation on M ∗∼\nφ(·|Hk1). Recall our assumption of V in the deﬁnition part.\nAlthough Vµk,M ∗does not satisfy joint normal distribution\nsince its policy is not optimistic of its MDP, M k is still sam-\npled from the posterior distribution of M ∗, which means\nthat given history Hk1, the posterior sample Rk, P k and\nunknown true R∗, P ∗are identically distributed. So the\nexpectation of (13) (on M k while performing posterior sam-\npling) is zero. So EM ∗[Sµk,M k\nk\n−Sµk,M ∗\nk\n|Hk1] is totally\nzero-mean, and is a sum of a series of joint normal variables.\nWe would focus on the variance next.\nLemma 1 (Transformation of Joint Normal Variables).\nIf X ∼Np(µ, Σ), A is a matrix of l × p and rank(A) = l,\nY = AX + b, Then\nY ∼Nl(Aµ + b, AΣAT ).\nTo calculate the sum , let A be a vector ﬁlled with 1, so we\nhave Σn\ni=1Xi ∼(N(Σn\ni=1µi, Σn\ni=1Σn\nj=1Cov(Xi, Xj)))\nNoticing that Cov(X, Y ) = E[(X −E[X])(Y −E[Y ])]\n≤\np\nE[(X −E[X])2]E[(Y −E[Y ])2] = σ1 ∗σ2 ≤max σ2.\nSo we have proved that given history Hk1, EM ∗[Sµk,M k\nk\n−\nSµk,M ∗\nk\n|Hk1] is normally distributed with expectation of\n0 and variance ≤H2\nmax(k)σ2, where max(k)σ2 is the max\nvariance of every state in episode k.\nNow back to the ﬁrst difference of (12).\nLemma 2 (Posterior Sampling).\nIf φ is the distribution of M ∗, then for any σ(Hk1)-\nmeasurable function g,\nE[g(M ∗)|Hk1] = E[g(M k)|Hk1].\nUsing the posterior lemma, the cumulative reward SµM,M\nk\nis σ(Hk1)-measurable, so E[Sµ∗,M ∗\nk\n−Sµk,M k\nk\n|Hk1] = 0\n(Osband et al., 2013).\nRecall that Sµ,M\nk\nis the sum of joint normal variables, so sim-\nilar to previous analysis, each EM ∗[Sµk,M k\nk\n−Sµ∗,M ∗\nk\n|Hk1]\nis normally distributed with zero-mean and variance ≤\nH2\nmax(k)σ2.\nSo EM ∗[(Sµ∗,M ∗\nk\n−Sµk,M k\nk\n) + (Sµk,M k\nk\n−Sµk,M ∗\nk\n)|Hk1]\nhas zero-mean and variance ≤4H2\nmaxσ2 by analyzing co-\nvariance as previous part.\nFor normal distribution X ∼N(0, σ2), and for any 1 >\nδ > 0, P(X ≤\np\n−2σ2logδ) ≥1 −δ, which means there\nis a probability of 1 −δ that X ≤\np\n−2σ2logδ.\nSo noticing the independence of sampling between episodes,\ncalculate EH[Σ\n⌈T\nH ⌉\nk=1 (Sµ∗,M ∗\nk\n−Sµk,M k\nk\n)|Hk1] as analyzed\nbefore, where EH means taking expectation on H. Set\nδ as\n1\nT , and let maxσ2 be the max variance of all states\nin all episodes (just for worst case bound), and there is a\nprobability of 1 −1\nT that:\nE[Σ\n⌈T\nH ⌉\nk=1 (Sµk,M ∗\nk\n−Sµk,M ∗\nk\n)|M ∗∼φ]\n≤2\np\n2maxσ2(HT + H)logT.\n□\nIn general cases (like RBF and Matern), σ2 is bounded\n(in a few cases like dot-product kernels, covariance cannot\nbe bounded only in inﬁnite spaces, while most continuous\nspaces in RL has borders), so this could be a sub-linear\nbound which means the agent would actually learn the real\nMDP in the end. Notice that we use maxσ2 only for a\nworst case bound in brief, while the true regret is related\nwith each variance and covariance of the state. This result\nis better than previous posterior sampling analysis (PSRL\nbounds\n√\nHSAT empirically but H\n√\nSAT theoretically).\nAs GP gets more information of the environment during\nexploration, the variance would decay, so actually the bound\ncould be even better.\n3.2.2. NON-DETERMINISTIC MDP\nTrue MDP M ∗= {S, A, RM, P M, H, ρ} ∼φ, other nota-\ntions are just the same as 3.2.1, except that P M is a stochas-\ntic transition in M, ρ is the distribution of initial states.\nSince the transition is not deterministic and the states are\ncontinuous, the cumulative reward could be related to count-\nless states of values. Since we do not have assumptions on\nstochastic transition function, which is necessary for regret\nanalysis in non-deterministic environment, we focus on the\ncumulative estimation error for any single state during the\nlearning process.\nCumError(T, π, M ∗, s) = Σ\n⌈T\nH ⌉\nk=1 (V µ∗,M∗\nk\n(s) −V µk,M∗\nk\n(s)).\n(14)\nEfﬁcient Model-Free Reinforcement Learning Using Gaussian Process\nWe would show that CumError can also lead to the con-\nvergence of estimation as described below. We put the proof\nof Theorem 3 in Appendix A.\nTheorem 3 Let\nM ∗\nbe\nthe\ntrue\nMDP\nwith\nnon-\ndeterministic transitions according to prior φ with with\nvalues under GP hypothesis, we have the bayesian\ncumulative error of estimation of any single state s:\nE[CumError(T, π, M ∗, s)|M ∗∼φ] = ˜O(\nq\n⌈T\nH ⌉). And\nlet M be any family of MDPs with non-zero probability\nunder the prior φ. Then for any ϵ ≥0:\nP( CumError(T,π,M ∗,s)\nT\n≥ϵ|M ∗∈M) →0.\n3.3. Demonstrations for Posterior Samping\nNow back to our reason to make use of demonstrations.\nConsider the expected variance of all states with distribution\np(s) of our estimate of value function, where p(s) is deter-\nmined by posterior distribution of value function and the\npresent policy. The analysis in 3.2.1&3.2.2 use maxσ2 only\nfor a worst bound, while the real situation is determined\nby every single σ2. So if we get lower expected variance,\nlower regret would be achieved with a high probability by\nMarkov’s inequality: P(σ2 ≥a) ≤E[σ2]\na\n. That is, with the\nsame parameter a, the lower the expectation is, there is a\nlower probability that σ2 would be larger than a.\nAbove analysis requires that we use sample set X which\nfrom distribution p(x) as demonstrations, while in fact we\ndo not know the exact p(x). So as a compromise, we could\nimprove the efﬁciency of our learning process by demon-\nstrations that contains similar situations to present episode,\nwhich is rational from intuition, and also produce better\nresult in practice in Section 6.\n4. Gaussian Process for Posterior Sampling\n4.1. Gaussian Process Temporal Difference\nGPTD was ﬁrstly introduced in Engel et al. 2003, then\nimproved in Engel et al. 2005. We’ll brieﬂy explain its\noverview framework here since our algorithm is closely\nrelated to it.\nGPTD proposes a generative model for the sequence of\nrewards corresponding to the trajectory x1, x2, · · · , xt:\nR(xi, xi+1) = V (xi) −γV (xi+1) + N(xi, xi+1)\n(15)\nwhere R is the reward process observed in experience, V is\nthe value Gaussian process, and N is a noise process.\nDeﬁne\nHt =\n\n\n1\n−γ\n0\n· · ·\n0\n0\n1\n−γ\n· · ·\n0\n...\n...\n0\n0\n· · ·\n1\n−γ\n\n\n(16)\nWe may rewrite (15) using (16) as\nRt−1 = HtVt + Nt\n(17)\nIn order to complete the probabilistic generative model con-\nnecting reward observations and values, we may impose\na Gaussian prior over V , i.e. V ∼N(0, k(·, ·)), in which\nk is the kernel chosen to reﬂect our prior beliefs concern-\ning the correlations between the values. We also need to\ndeﬁne Nt ∼N(0, Σt) with Σt = σ2HtHT\nt and σ is the\nobservation noise level(Engel et al., 2005).\nSince both the value prior and the observation noise are\nGaussian, the posterior distribution of the value conditioned\non observation sequence rt−1 = (r0, · · · , rt−1)T are also\nGaussian and given by\nˆvt(x) = kt(x)T αt\npt(x) = k(x, x) −kt(x)T Ctkt(x)\nwhere\nkt(x) = (k(x0, x), · · · , k(xt, x))T\nKt =\n\n\nk(x0, x0)\nk(x0, x1)\n· · ·\nk(x0, xt)\nk(x1, x0)\nk(x1, x1)\n· · ·\nk(x1, xt)\n...\n...\n...\nk(xt, x0)\nk(xt, x1)\n· · ·\nk(xt, xt)\n\n\nαt = HT\nt (HtKtHT\nt + Σt)\n−1rt−1\nCt = HT\nt (HtKtHT\nt + Σt)\n−1Ht\n(18)\n4.2. GPPSTD\nNow we are ready to present Gaussian Process Posterior\nSampling Temporal Difference (GPPSTD) algorithm, de-\nscribed in Algorithm 1. We adopt the GPTD framework to\ngain the posterior Q value distribution of state action pair\nconditioned on all reward experiences by Equation 18. We\nnote that similar to GPSARSA method(Engel et al., 2005),\nwe treat state action pair as xt, therefore model Q value of\nstate action pair rather than V value of state in GP. We also\nuse episodic algorithm with ﬁxed episode length as required\nby the analysis.\nAs analyzed before, we only update GP model after one\nepisode ends. Posterior sampling should depend on the joint\ndistribution of all the state-action pair in one episode. But\nduring the exploration, the agent would not know exactly\nwhat state-action pair it would come across in the following\nsteps within the episode. We overcome this problem by us-\ning conditional distribution of joint variables as the analysis\nbelow.\nWe\napplied\nposterior\nsampling\nmethod\nby\na\n=\narg maxa Qsampled(st, a).\nDenote the already sampled\nQi = Q(si, ai)(i = 1, 2, · · · , t). In a single episode, when\nEfﬁcient Model-Free Reinforcement Learning Using Gaussian Process\nAlgorithm 1 GPPSTD\nInitialize GP model M\nrepeat\nInitialize initial state s1, Memory of the episode\nfor timestep t = 1 to H do\nObtain µ(st, ·),Σ from M using 18\nSample n(st, ·) according to 21\nPerform a = arg maxa(µ(st, a) + n(st, a))\nObserve st+1,r\nMemory.add((st−1, at−1, r, st, at)\nend for\nGPTD.Update(M, Memory)\nuntil M convergence requirement satisﬁed\nQ1, Q2, · · · , Qt−1 have been sampled, posterior Qt and all\nprevious Q are joint Gaussian distributed,\n\u0014\nQ1···t−1\nQ(st, ·)\n\u0015\n∼N(\n\u0014\nµ1···t−1\nµt(st, ·)\n\u0015\n,\n\u0014\nΣxx\nΣx∗x\nΣxx∗\nΣx∗x∗\n\u0015\n)\n(19)\nin which Q(st, ·) stands for Q values of all actions possibili-\nties in st, µ(st, ·) stands for their posterior means, and Σxx,\nΣx∗x, Σx∗x∗stands for posterior covariance matrix given\nby GP. Using standard multivariate Gaussian conditional\nresults, we gain posterior sampling\nQ(st, ·) ∼N(µt(st, ·) + Σx∗xΣ−1\nxx (Q1···t−1 −µ1···t−1),\nΣx∗x∗−Σx∗xΣ−1\nxx Σxx∗)\n(20)\nBy subtracting µt in (20), we have each conditional noise\nn(st, at) ∼N(Σx∗xΣ−1\nxx n1···t−1,\nΣx∗x∗−Σx∗xΣ−1\nxx Σxx∗))\n(21)\nSo at each timestep t, we perform action selection by sam-\npling a noise from conditional distribution, add it to pos-\nterior mean of Q and choose the best action according to\nthe noised Q. At the end of the episode we use collected\nobservation sequence to update our GP model by updat-\ning Kt, αt, Ct in (18)(for more detail, we refer readers to\nEngel et al. 2005). This exploration is bounded by Theo-\nrem 2 (in deterministic environments) or Theorem 3 (with\nnon-deterministic environments).\nIt is worth mentioning that because our policy remain un-\nchanged during one episode, it achieves deep exploration.\n(Russo et al., 2017).\n4.3. Pretrain\nNow let’s see how we can make use of various demonstra-\ntions to make GPPSTD more efﬁcient. The way we pretrain\nGP model M is exactly the same as training. For RL, the\n”test” point distribution p(x) is the experience collected in\nenvironment, which is determined by its current knowledge\n(in our case, value) and exploration strategy. According to\nanalysis in Section 3.1, a training set sampled from p(x)\ncould give the lowest expected uncertainty, then help to\navoid GPPSTD algorithm from meaningless exploration,\nresulting in the efﬁciency bound in Section 3.2.\nIntuitively, we could regard the various-pretrain as an sketch\noverview of the Q value over state action space, and this\nsketch helps RL agent explore smartly. Though we just pre-\ntrain data with training method, we note that it is extremely\nhard for the agent to obtain the sketch alone, since a large\nproportion of space can’t be accessed by RL agent itself for\nlack of systematic information especially in the beginning\nof the training.\n5. Gaussian Process and Bayesian Neural\nNetwork\nNow we’ll be ready to discuss the general relationship be-\ntween GP and bayesian neural networks, expanding our\nideas to BNN. Neal (1996) had shown that Bayesian neural\nnetworks with inﬁnitely many hidden units converged to\nGaussian Process with a particular kernel (covariance) func-\ntion. Recently, Jaehoon Lee (2018) has proposed NNGP\nto perform Bayesian prediction with a deep neural net-\nwork which could outperform standard neural networks\ntrained with stochastic gradient descent. Alexander G. de\nG. Matthews (2018) exhibited situations where existing\nBayesian deep networks are close to Gaussian Processes.\nSo based on earlier work, we could expect that our the-\nory about efﬁcient exploration and making use of demon-\nstrations in RL could extend to Bayesian deep networks.\nRelated work had been done by Kamyar Azizzadenesheli\n(2018). They proposed Bayesian Deep Q-Network (BDQN),\na practical Thompson sampling based RL Algorithm us-\ning Bayesian regression to estimate the posterior over Q-\nfunctions, and has achieved impressive results while lacks\ntheoretical analysis. We think this paper could provide a pos-\nsible theoretical justiﬁcation for BDQN, meanwhile making\nuse of demonstrations remains future work.\n6. Experiments\nOur empirical experiment is done in the CartPole Task, a\nclassic control problem in OpenAI Gym (Brockman et al.,\n2016). The task is to push a car left or right to balance a\nstick on the car. In each timestep, the RL algorithm receives\na 4-dimensional state, takes one of two actions (left or right),\nand receives a reward of 1 if the stick’s deviation angle from\nvertical line is within a range. If not, the episode will end.\nThe maximum length of an episode is 200 steps, and we\nEfﬁcient Model-Free Reinforcement Learning Using Gaussian Process\nFigure 1. Performance and Variance comparison between no-pretrain, optimal-pretrain and various-pretrain settings. The results are the\naverage of 5 experiments\nFigure 2. Performance comparison between GPPSTD, GPTD with\nϵ-greedy and Deep Q with ϵ-greedy. The results are the average of\n5 experiments\ncould view the steps after failure as reward = 0, therefore\nmaking it a ﬁxed length task.\nFirstly, we compare the performance of GPPSTD algo-\nrithm, GPTD using ϵ-greedy and deep-q learning using\nϵ-greedy on CartPole in Fig. 2. We choose squared ex-\nponential kernel k(xi, xj) = c × exp(−1\n2d(xi/l, xj/l)2)\nfor GPPSTD and GPTD method, with length scale l =\n[0.1, 0.02, 0.1, 0.02, 0.001] and variance c = 10. Since we\nregard state-action pair as x in GP, our length scale is a 5-\ndimensional vector. We note that because we believe there\nare no value correlations in action, we give it an length\nscale of 0.001, which in turn will cause k(xi, xj) = 0\nwhen action is different. Result Fig. 2 shows that GPPSTD\nsigniﬁcantly outperform other two algorithms. It demon-\nstrates GPPSTD’s exploration process to be both efﬁcient\nand robust, since ϵ-greedy methods ﬂuctuate a lot relative to\nGPPSTD. We also see that GP may be a better model than\nneural network in this task.\nIn the second experiment, we show that when combined\nwith demonstration, GPPSTD could achieve an even bet-\nter results. In the optimal demonstration pretrain setting\nwe use 10 episodes of optimal demonstration (200-score\nepisodes) while in the various-pretrain setting, 5 episodes\nof optimal demonstration and 5 episodes of unsuccessful\ndemonstration (score between 10-60) are used for pretrain.\nAs shown in Fig. 1), various-pretrain outperforms optimal-\npretrain and no-pretrain. We notice that optimal-pretrain\nsuffers ﬂuctuate performance compared to various-pretrain,\nwhich veriﬁes our belief. It is because that optimal demon-\nstration only can not provide agent with the information\noutside optimal trajectory, which leads to higher variance\nof estimations, whereas various demonstration has lower\nvariance of estimation during exploration, thus lead to better\nregret as our analysis in Section 3.2. Moreover, as in Fig.\n1, various-pretrain has the lowest action uncertainty (mea-\nsured by posterior variance) at the beginning, reﬂected our\nanalysis on expected uncertainty analysis in Section 3.1.\n7. Conclusions\nIn this paper, we discuss how to make use of various demon-\nstrations to improve exploration efﬁciency in RL and make\na statistical proof from the view of GP. What is equally im-\nportant is that we propose a new algorithm GPPSTD, which\nimplements a model-free method in continuous space with\nefﬁcient exploration by posterior sampling under GP hypoth-\nesis, and also behaves impressively in practice. Both two\nmethods aim at efﬁcient exploration in RL. More impres-\nsively, combining both could further improve the efﬁciency\nfrom a Bayesian view. The property of Gaussian Process\nhas been discussed to extend these methods to neural net-\nwork, and we expect faster computation and even better\nresults using our model-free posterior sampling methods on\nBayesian Neural Network.\nEfﬁcient Model-Free Reinforcement Learning Using Gaussian Process\nReferences\nAbbeel, Pieter and Ng, Andrew Y. Apprenticeship learn-\ning via inverse reinforcement learning. In Proceedings\nof the twenty-ﬁrst international conference on Machine\nlearning, pp. 1. ACM, 2004.\nAlexander G. de G. Matthews, Jiri Hron, Mark Rowland\nRichard E. Turner Zoubin Ghahramani. Gaussian process\nbehaviour in wide deep neural networks. International\nConference on Learning Representations, 2018.\nAmin, Kareem, Jiang, Nan, and Singh, Satinder. Repeated\ninverse reinforcement learning. In Advances in Neural\nInformation Processing Systems, pp. 1813–1822, 2017.\nAudiffren, Julien, Valko, Michal, Lazaric, Alessandro, and\nGhavamzadeh, Mohammad. Maximum entropy semi-\nsupervised inverse reinforcement learning. In IJCAI, pp.\n3315–3321, 2015.\nBrockman, Greg, Cheung, Vicki, Pettersson, Ludwig,\nSchneider, Jonas, Schulman, John, Tang, Jie, and\nZaremba, Wojciech. Openai gym, 2016.\nChowdhary, Girish, Liu, Miao, Grande, Robert, Walsh,\nThomas, How, Jonathan, and Carin, Lawrence.\nOff-\npolicy reinforcement learning with gaussian processes.\nIEEE/CAA Journal of Automatica Sinica, 1(3):227–238,\n2014.\nChung, Jen Jen, Lawrance, Nicholas RJ, and Sukkarieh,\nSalah. Gaussian processes for informative exploration\nin reinforcement learning.\nIn Robotics and Automa-\ntion (ICRA), 2013 IEEE International Conference on,\npp. 2633–2639. IEEE, 2013.\nEngel, Yaakov, Mannor, Shie, and Meir, Ron. Bayes meets\nbellman: The gaussian process approach to temporal dif-\nference learning. In Proceedings of the 20th International\nConference on Machine Learning (ICML-03), pp. 154–\n161, 2003.\nEngel, Yaakov, Mannor, Shie, and Meir, Ron. Reinforce-\nment learning with gaussian processes. In Proceedings of\nthe 22Nd International Conference on Machine Learning,\npp. 201–208, New York, NY, USA, 2005. ACM.\nFortunato, Meire, Azar, Mohammad Gheshlaghi, Piot, Bi-\nlal, Menick, Jacob, Osband, Ian, Graves, Alex, Mnih,\nVlad, Munos, Remi, Hassabis, Demis, Pietquin, Olivier,\net al. Noisy networks for exploration. arXiv preprint\narXiv:1706.10295, 2017.\nGrande, R.C., Walsh, T.J., and How, Jonathan. Sample\nefﬁcient reinforcement learning with gaussian processes.\nIn 31st International Conference on Machine Learning,\nICML 2014, volume 4, pp. 3136–3150, 01 2014.\nGrze´s, Marek and Kudenko, Daniel. Improving optimistic\nexploration in model-free reinforcement learning.\nIn\nInternational Conference on Adaptive and Natural Com-\nputing Algorithms, pp. 360–369. Springer, 2009.\nHester, Todd, Vecerik, Matej, Pietquin, Olivier, Lanctot,\nMarc, Schaul, Tom, Piot, Bilal, Sendonaris, Andrew,\nDulac-Arnold, Gabriel, Osband, Ian, Agapiou, John, et al.\nLearning from demonstrations for real world reinforce-\nment learning. arXiv preprint arXiv:1704.03732, 2017.\nHo, Jonathan and Ermon, Stefano. Generative adversarial\nimitation learning. In Advances in Neural Information\nProcessing Systems, pp. 4565–4573, 2016.\nJaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington Ro-\nman Novak Sam Schoenholz Yasaman Bahri. Deep neural\nnetworks as gaussian processes. International Conference\non Learning Representations, 2018.\nJaksch, Thomas, Ortner, Ronald, and Auer, Peter. Near-\noptimal regret bounds for reinforcement learning. Jour-\nnal of Machine Learning Research, 11(Apr):1563–1600,\n2010.\nKamyar\nAzizzadenesheli,\nEmma\nBrunskill,\nAni-\nmashree Anandkumar.\nEfﬁcient exploration through\nbayesian deep q-networks, 2018.\nLevine, Sergey, Finn, Chelsea, Darrell, Trevor, and Abbeel,\nPieter. End-to-end training of deep visuomotor policies.\nThe Journal of Machine Learning Research, 17(1):1334–\n1373, 2016.\nMichini, Bernard and How, Jonathan P. Improving the ef-\nﬁciency of bayesian inverse reinforcement learning. In\nRobotics and Automation (ICRA), 2012 IEEE Interna-\ntional Conference on, pp. 3651–3656. IEEE, 2012.\nMnih, Volodymyr, Kavukcuoglu, Koray, Silver, David,\nRusu, Andrei A, Veness, Joel, Bellemare, Marc G, Graves,\nAlex, Riedmiller, Martin, Fidjeland, Andreas K, Ostro-\nvski, Georg, et al. Human-level control through deep\nreinforcement learning. Nature, 518(7540):529, 2015.\nNeal, Radford M. Bayesian Learning for Neural Networks.\nSpringer-Verlag New York, Inc., Secaucus, NJ, USA,\n1996. ISBN 0387947248.\nNg, Andrew Y, Russell, Stuart J, et al. Algorithms for\ninverse reinforcement learning. In Icml, pp. 663–670,\n2000.\nOsband, Ian and Van Roy, Benjamin. Why is posterior sam-\npling better than optimism for reinforcement learning? In\nPrecup, Doina and Teh, Yee Whye (eds.), Proceedings of\nthe 34th International Conference on Machine Learning,\npp. 2701–2710, International Convention Centre, Sydney,\nAustralia, 2017. PMLR.\nEfﬁcient Model-Free Reinforcement Learning Using Gaussian Process\nOsband, Ian, Benjamin, Van Roy, and Daniel, Russo. (More)\nefﬁcient reinforcement learning via posterior sampling. In\nProceedings of the 26th International Conference on Neu-\nral Information Processing Systems - Volume 2, NIPS’13,\npp. 3003–3011, USA, 2013. Curran Associates Inc.\nPathak, Deepak, Agrawal, Pulkit, Efros, Alexei A, and\nDarrell, Trevor.\nCuriosity-driven exploration by self-\nsupervised prediction. In International Conference on\nMachine Learning (ICML), volume 2017, 2017.\nRamachandran, Deepak and Amir, Eyal. Bayesian inverse\nreinforcement learning. Urbana, 51(61801):1–4, 2007.\nRasmussen, Carl Edward and Williams, Christopher K. I.\nCovariance functions. In Gaussian Processes for Ma-\nchine Learning, chapter 4, pp. 96–99. The MIT Press,\nMassachusetts Institute of Technology, 2006.\nRoss, St´ephane, Gordon, Geoffrey, and Bagnell, Drew. A\nreduction of imitation learning and structured prediction\nto no-regret online learning. In Proceedings of the four-\nteenth international conference on artiﬁcial intelligence\nand statistics, pp. 627–635, 2011.\nRusso, Daniel, Van Roy, Benjamin, Kazerouni, Abbas, and\nOsband, Ian. A tutorial on thompson sampling. arXiv\npreprint arXiv:1707.02038, 2017.\nSilver, David, Newnham, Leonard, Barker, David, Weller,\nSuzanne, and McFall, Jason. Concurrent reinforcement\nlearning from customer interactions. In International\nConference on Machine Learning, pp. 924–932, 2013.\nSilver, David, Huang, Aja, Maddison, Chris J, Guez, Arthur,\nSifre, Laurent, Van Den Driessche, George, Schrittwieser,\nJulian, Antonoglou, Ioannis, Panneershelvam, Veda,\nLanctot, Marc, et al. Mastering the game of go with\ndeep neural networks and tree search. nature, 529(7587):\n484–489, 2016.\nSrinivas, Niranjan, Krause, Andreas, Kakade, Sham M, and\nSeeger, Matthias W. Information-theoretic regret bounds\nfor gaussian process optimization in the bandit setting.\nIEEE Transactions on Information Theory, 58(5):3250–\n3265, 2012.\nStrens, Malcolm. A bayesian framework for reinforcement\nlearning. In In Proceedings of the Seventeenth Interna-\ntional Conference on Machine Learning, pp. 943–950.\nICML, 2000.\nSutton, Richard S and Barto, Andrew G. Reinforcement\nlearning: An introduction, volume 1. MIT press Cam-\nbridge, 1998.\nZiebart, Brian D, Maas, Andrew L, Bagnell, J Andrew, and\nDey, Anind K. Maximum entropy inverse reinforcement\nlearning. In AAAI, volume 8, pp. 1433–1438. Chicago,\nIL, USA, 2008.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2018-12-11",
  "updated": "2018-12-11"
}