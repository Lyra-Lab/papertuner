{
  "id": "http://arxiv.org/abs/1812.05642v2",
  "title": "SIGNet: Semantic Instance Aided Unsupervised 3D Geometry Perception",
  "authors": [
    "Yue Meng",
    "Yongxi Lu",
    "Aman Raj",
    "Samuel Sunarjo",
    "Rui Guo",
    "Tara Javidi",
    "Gaurav Bansal",
    "Dinesh Bharadia"
  ],
  "abstract": "Unsupervised learning for geometric perception (depth, optical flow, etc.) is\nof great interest to autonomous systems. Recent works on unsupervised learning\nhave made considerable progress on perceiving geometry; however, they usually\nignore the coherence of objects and perform poorly under scenarios with dark\nand noisy environments. In contrast, supervised learning algorithms, which are\nrobust, require large labeled geometric dataset. This paper introduces SIGNet,\na novel framework that provides robust geometry perception without requiring\ngeometrically informative labels. Specifically, SIGNet integrates semantic\ninformation to make depth and flow predictions consistent with objects and\nrobust to low lighting conditions. SIGNet is shown to improve upon the\nstate-of-the-art unsupervised learning for depth prediction by 30% (in squared\nrelative error). In particular, SIGNet improves the dynamic object class\nperformance by 39% in depth prediction and 29% in flow prediction. Our code\nwill be made available at https://github.com/mengyuest/SIGNet",
  "text": "SIGNet: Semantic Instance Aided Unsupervised 3D Geometry Perception\nYue Meng1\nYongxi Lu1\nAman Raj1\nSamuel Sunarjo1\nRui Guo2\nTara Javidi1\nGaurav Bansal2\nDinesh Bharadia1\n1UC San Diego\n2Toyota InfoTechnology Center\n{yum107, yol070, amraj, ssunarjo, tjavidi, dineshb}@ucsd.edu\nrguo@us.toyota-itc.com\ngauravbs@gmail.com\nAbstract\nUnsupervised learning for geometric perception (depth,\noptical ﬂow, etc.) is of great interest to autonomous sys-\ntems. Recent works on unsupervised learning have made\nconsiderable progress on perceiving geometry; however,\nthey usually ignore the coherence of objects and perform\npoorly under scenarios with dark and noisy environments.\nIn contrast, supervised learning algorithms, which are ro-\nbust, require large labeled geometric dataset. This paper\nintroduces SIGNet, a novel framework that provides ro-\nbust geometry perception without requiring geometrically\ninformative labels. Speciﬁcally, SIGNet integrates seman-\ntic information to make depth and ﬂow predictions con-\nsistent with objects and robust to low lighting conditions.\nSIGNet is shown to improve upon the state-of-the-art unsu-\npervised learning for depth prediction by 30% (in squared\nrelative error). In particular, SIGNet improves the dynamic\nobject class performance by 39% in depth prediction and\n29% in ﬂow prediction. Our code will be made available at\nhttps://github.com/mengyuest/SIGNet\n1. Introduction\nVisual perception of 3D scene geometry using a monoc-\nular camera is a fundamental problem with numerous appli-\ncations, like autonomous driving and space exploration. We\nfocus on the ability to infer accurate geometry (depth and\nﬂow) of static and moving objects in a 3D scene. Supervised\ndeep learning models have been proposed for geometry pre-\ndictions, yielding “robust” and favorable results against the\ntraditional approaches (SfM) [38, 39, 10, 2, 1, 26]. How-\never, supervised models require a dataset labeled with ge-\nometrically informative annotations, which is extremely\nchallenging as the collection of geometrically annotated\nground truth (e.g. depth, ﬂow) requires expensive equip-\nment (e.g. LIDAR) and careful calibration procedures.\nOurs\nState-of-the-art unsupervised\nXs\nXu\nXt\nYs\nYt\nYu\nft →u\nFrame s\nFrame t\nFrame u\nft →s\nft → u\nft →s\nFigure 1: On the right, state-of-the-art unsupervised learn-\ning approach relies on pixel-wise information only, while\nSIGNet on the left utilizes the semantic information to en-\ncode the spatial constraints hence further enhances the ge-\nometry prediction.\nRecent works combine the geometric-based SfM meth-\nods with end-to-end unsupervised trainable deep models\nto utilize abundantly available unlabeled monocular cam-\nera data.\nIn [54, 41, 51, 9] deep models predict depth\nand ﬂow per pixel simultaneously from a short sequence of\nimages and typically use photo-metric reconstruction loss\nof a target scene from neighboring scenes as the surro-\ngate task. However, these solutions often fail when dealing\nwith dynamic objects1. Furthermore, the prediction quality\nis negatively affected by the imperfections like Lambertian\nreﬂectance and varying intensity which occur in the real\nworld. In short, no robust solution is known.\nIn Fig 1, we highlight the innovation of our system (on\nthe left) comparing to the existing unsupervised frameworks\n(on the right) for geometry perception. Traditional unsu-\npervised models learn from the pixel-level feedback (i.e.\n1Section 5 presents empirical results that explicitly illustrate this short-\ncoming of state-of-the-art unsupervised approaches.\narXiv:1812.05642v2  [cs.CV]  5 Apr 2019\nphoto-metric reconstruction loss), whereas SIGNet relies on\nthe key observation that inherent spatial constraints exist in\nthe visual perception problem as shown in Fig 1. Speciﬁ-\ncally, we exploit the fact that pixels belonging to the same\nobject have additional constraints for the depth and ﬂow\nprediction.\nHow can those spatial constraints of the pixels be en-\ncoded? We leverage the semantic information as seen in\nFig 1 for unsupervised frameworks.\nIntuitively, seman-\ntic information can be interpreted as deﬁning boundaries\naround a group of pixels whose geometry is closely related.\nThe knowledge of semantic information between different\nsegments of a scene could allow us to easily learn which\npixels are correlated, while the object edges could imply\nsharp depth transition. Furthermore, note that this learn-\ning paradigm is practical 2 as annotations for semantic pre-\ndiction tasks such as semantic segmentation are relatively\ncheaper and easier to acquire. To the best of our knowl-\nedge, our work is the ﬁrst to utilize semantic information in\nthe context of unsupervised learning for geometry percep-\ntion.\nA natural question is how do we combine semantic in-\nformation with an unsupervised geometric prediction? Our\napproach to combine the semantic information with RGB\ninput is two-fold: First, we propose a novel way to augment\nRGB images with semantic information. Second, we pro-\npose new loss functions, architecture, and training method.\nThe two-fold approach precisely accounts for spatial con-\nstraints in making geometric predictions:\nFeature Augmentation: We concatenate the RGB input\ndata with both per-pixel class predictions and instance-level\npredictions. We use per pixel class predictions to deﬁne se-\nmantic mask which serves as a guidance signal that eases\nunsupervised geometric predictions. Moreover, we use the\ninstance-level prediction and split them into two inputs, in-\nstance edges and object masks. Instance edges and object\nmasks enable the network to learn the object edges and\nsharp depth transitions.\nLoss Function Augmentation: Second, we augment the\nloss function to include various semantic losses, which re-\nduces the reliance on semantic features in the evaluation\nphase. This is crucial when the environment contains less\ncommon contextual elements (like in dessert navigation or\nmining exploitation). We design and experiment with var-\nious semantic losses, such as semantic warp loss, masked\nreconstruction loss, and semantic-aware edge smoothness\nloss.\nHowever, manually designing a loss term which\ncan improve the performance over the feature augmenta-\ntion technique turns out to be very difﬁcult.\nThe chal-\n2Semantic labels can be easily curated on demand on unlabeled data.\nOn the contrary, geometrically informative labels such as ﬂow and depth\nrequire additional sensors and careful annotation at the data collection\nstage.\nlenge comes from the lack of understanding of error dis-\ntributions because we are generally biased towards simple,\ninterpretable loss functions that can be sub-optimal in un-\nsupervised learning. Hence, we propose an alternative ap-\nproach of incorporating a transfer network that learns how\nto predict semantic mask via a semantic reconstruction loss\nand provides feedback to improve the depth and pose esti-\nmations, which shows considerable improvements in depth\nand ﬂow prediction.\nWe empirically evaluate the feature and loss func-\ntion augmentations on KITTI dataset [14] and compare\nthem with the state-of-the-art unsupervised learning frame-\nwork [51]. In our experiments we use class-level predic-\ntions from DeepLabv3+ [4] trained on Cityscapes [6] and\nMask R-CNN [18] trained on MSCOCO [27]. Our key ﬁnd-\nings:\n• By using semantic segmentation for both feature and\nloss augmentation, our proposed algorithms improves\nsquared relative error in depth estimation by 28% com-\npared to the strong baseline set by state-of-the-art un-\nsupervised GeoNet [51].\n• Feature augmentation alone, combining semantic with\ninstance-level information,\nleads to larger gains.\nWith both class-level and instance-level features, the\nsquared relative error of the depth predictions im-\nproves by 30% compared to the baseline.\n• Finally,\nas for common dynamic object classes\n(e.g. vehicles) SIGNet shows 39% improvement (in\nsquared relative error) for depth predictions and 29%\nimprovement in the ﬂow prediction, thereby showing\nthat semantic information is very useful for improving\nthe performance in the dynamic categories of objects.\nFurthermore, SIGNet is robust to noise in image\nintensity compared to the baseline.\n2. Related Work\nDeep Models for Understanding Geometry: Deep mod-\nels have been widely used in supervised depth estimation\n[8, 29, 36, 53, 5, 49, 50, 11, 46], tracking, and pose es-\ntimation [43, 47, 2, 17] , as well as optical ﬂow predic-\ntions [7, 20, 25, 40]. These models have demonstrated su-\nperior accuracy and typically faster speed in modern hard-\nware platforms (especially in the case of optical ﬂow esti-\nmation) compared to traditional methods. However, achiev-\ning good performance with supervised learning requires a\nlarge amount of geometry-related labels. The current work\naddresses this challenge by adopting an unsupervised learn-\ning framework for depth, pose, and optical ﬂow estimations.\nDeep Models for Semantic Predictions: Deep models are\nwidely applied in semantic prediction tasks, such as image\nclassiﬁcation [24], semantic segmentation [4], and instance\nsegmentation [18]. In this work, we utilize the effectiveness\nof the semantic predictions provided by DeepLab v3+ [4]\nand Mask R-CNN [18] in encoding spatial constraints to ac-\ncurately predict geometric attributes such as depth and ﬂow.\nWhile we particularly choose [4] and [18] for our SIGNet,\nsimilar gains can be obtained by using other state-of-the-art\nsemantic prediction methods.\nUnsupervised Deep Models for Understanding Geome-\ntry: Several recent methods propose to use unsupervised\nlearning for geometry understanding. In particular, Garg\net al. [13] uses a warping method based on Taylor expan-\nsion. In the context of unsupervised ﬂow prediction, Yu et\nal. [21] and Ren et al. [37] introduce image reconstruction\nloss with spatial smoothness constraints. Similar methods\nare used in Zhou et al. [54] for learning depth and camera\nego-motions by ignoring object motions. This is partially\naddressed by Vijayanarasimhan et al. [41], despite the fact,\nwe note, that the modeling of motion is difﬁcult without\nintroducing semantic information. This framework is fur-\nther improved with better modeling of the geometry. Ge-\nometric consistency loss is introduced to handle occluded\nregions, in binocular depth learning [16], ﬂow prediction\n[32] and joint depth, ego-motion and optical ﬂow learning\n[51]. Mahjourian et al. [31] focuses on improved geometric\nconstraints, Godard et al. [15] proposes several architectural\nand loss innovations, while Zhan et al. [52] uses reconstruc-\ntion in the feature space rather than the image space. In con-\ntrast, the current work explores using semantic information\nto resolve ambiguities that are difﬁcult for pure geometric\nmodeling. Methods proposed in the current work are com-\nplementary to these recent methods, but we choose to vali-\ndate our approach on a state-of-the-art framework known as\nGeoNet [51].\nMulti-Task Learning for Semantic and Depth: Multi-\ntask learning [3] achieves better generalization by allowing\nthe system to learn features that are robust across different\ntasks. Recent methods focus on designing efﬁcient archi-\ntectures that can predict related tasks using shared features\nwhile avoiding negative transfers [35, 19, 30, 34, 23, 12].\nIn this context, several prior works report promising results\ncombining scene geometry with semantics. For instance,\nsimilar to our method Liu et al. [28] uses semantic predic-\ntions to provide depth. However, this work is fully super-\nvised and only uses sub-optimal traditional methods. Wang\net al. [44], Cross-Stitching [35], UberNet [23] and NDDR-\nCNN [12] all report improved performance over single-task\nbaselines. But they have not addressed outdoor scenes and\nunsupervised geometry understanding. Our work is also re-\nlated to PAD-Net [48]. PAD-Net reports improvements by\ncombining intermediate tasks as inputs to ﬁnal depth and\nsegmentation tasks. Our method of using semantic input\nsimilarly introduces an intermediate prediction task as input\nto the depth and pose predictions, but we tackle the problem\nsetting where depth labels are not provided.\n3. State-of-the-art Unsupervised Geometry\nPrediction\nPrior to presenting our technical approach, we provide\na brief overview of state-of-the-art unsupervised depth and\nmotion estimation framework, which is based on image re-\nconstruction from geometric predictions [54, 51]. It trains\nthe geometric prediction models through the reconstruc-\ntions of a target image from source images. The target and\nsource images are neighboring frames in a video sequence.\nNote that such a reconstruction is possible only when cer-\ntain elements of the 3D geometry of the scene are under-\nstood: (1) The relative 3D location (and thus the distance)\nbetween the camera and each pixel. (2) The camera ego-\nmotion. (3) The motion of pixels. Thus this framework can\nbe used to train a depth estimator and an ego-motion esti-\nmator, as well as a optical ﬂow predictor.\nTechnically, each training sample I = {Ii}n\ni=1 consists\nof n contiguous video frames Ii ∈RH×W ×3 where the cen-\nter frame It is the “target frame” and the other frames serve\nas the “source frame”. In training, a differentiable warping\nfunction ft→s is constructed from the geometry predictions.\nThe warping function is used to reconstruct the target frame\n˜Is ∈RH×W ×3 from source frame Is via bilinear sampling.\nThe level of success in this reconstruction provides training\nsignals through backpropagation to the various ConvNets in\nthe system. A standard loss function to measure reconstruc-\ntion success is as follows:\nLrw = α1 −SSIM(It, ˜Is)\n2\n+ (1 −α)||It −˜Is||1\n(1)\nwhere SSIM denotes the structural similarity index [45] and\nα is set to 0.85 in [51].\nTo ﬁlter out erroneous predictions while preserving\nsharp details, the standard practice is to include an edge-\naware depth smoothness loss Lds weighted by image gradi-\nents\nLds =\nX\npt\n|∇D(pt)| · (e−|∇I(pt)|)T\n(2)\nwhere |·| denotes element-wise absolute operation, ∇is the\nvector differential operator, and T denotes transpose of gra-\ndients. These losses are usually computed from a pyramid\nof multi-scale predictions. The sum is used as the training\ntarget.\nWhile the reconstruction of RGB images is an effective\nsurrogate task for unsupervised learning, it is limited by the\nlack of semantic information as supervision signals. For ex-\nample, the system cannot learn the difference between the\ncar and the road if they have similar colors or two neighbor-\ning cars with similar colors. When object motion is consid-\nered in the models, the learning can mistakenly assign mo-\ntion to non-moving objects as the geometric constraints are\nRGB Frames\nDepthNet\nPoseNet\nCamera Motion\nDepth Map\nSemantic Frames\nInstance Edges\nInstance Frames\nResFlowNet\nOptical Flow\n⊕\nConcat\nConcat\nConcat\nFigure 2: Our unsupervised architecture contains DepthNet, PoseNet and ResFlowNet to predict depth, poses and motion\nusing semantic-level and instance-level segmentation concatenated along the input channel dimension.\nill-posed. We augment and improve this system by leverag-\ning semantic information.\n4. Methods\nIn this section, we present solutions to enhance geome-\ntry predictions with semantic information. Semantic labels\ncan provide rich information on 3D scene geometry. Impor-\ntant details such as 3D location of pixels and their move-\nments can be inferred from a dense representation of the\nscene semantics. The proposed methods are applicable to\na wide variety of recently proposed unsupervised geometry\nlearning frameworks based on photometric reconstruction\n[54, 16, 51] represented by our baseline framework intro-\nduced in Section 3. Our complemented pipeline in test time\nis illustrated in Fig 2.\n4.1. Semantic Input Augmentation\nSemantic predictions can improve geometry prediction\nmodels when serving as input features. Unlike RGB im-\nages, semantic predictions mark objects and contiguous\nstructures with consistent blobs, which provide important\ninformation for the learning problem. However, it is un-\ncertain that using semantic labels as input could indeed im-\nprove depth and ﬂow predictions since training labels are\nnot available. Semantic information could be lost or dis-\ntorted, which would end up being a noisy training signal.\nAn important ﬁnding of our work is that using semantic\npredictions as inputs signiﬁcantly improves the accuracy in\ngeometry predictions, despite the presence of noisy training\nsignal. Input representation and the type of semantic la-\nbels have a large impact on the performance of the system.\nWe further illustrate this by Fig 3, where we show various\nsemantic labels (semantic segmentation, instance segmen-\ntation, and instance edge) that we use to augment the input.\nThis imposes additional constraints such as depth of the pix-\nels belonging to a particular object (e.g. a vehicle) which\nhelps the learning process. Furthermore, sudden changes\nRGB Image\nSemantic \nsegmentation\nInstance class \nsegmentation\nInstance \nedge map\nFigure 3: Top to bottom: RGB image, semantic segmen-\ntation, instance class segmentation and instance edge map.\nThey are used for the full prediction architecture. The se-\nmantic segmentation provides accurate segments grouped\nby classes, but it fails to differentiate neighboring cars.\nin the depth predictions can be inferred from the boundary\nof vehicles. The semantic labels of the pixels can provide\nimportant information to associate pixels across frames.\nEncoding Pixel-wise Class Labels: We explored two in-\nput encoding techniques for class labels: dense encoding\nand one-hot encoding. In dense encoding, dense class la-\nbels are concatenated along the input channel dimension.\nThe added semantic features are centralized to the range of\n[−1, 1] to be consistent with RGB inputs. In the case of\none-hot encoding, the class-level semantic predictions are\nﬁrst expanded to one-hot encoding and then concatenated\nalong the input channel dimension. The labels are repre-\nsented as one-hot sparse vectors. In this variant, semantic\nfeatures are not normalized since they have similar value\nrange as the RGB inputs,\nEncoding Instance-level Semantic Information:\nBoth\ndense and one-hot encoding are natural to class-level se-\nmantic prediction, where each pixel is only assigned a class\nlabel rather than an instance label. Our conjecture is that\ninstance-level semantic information is particular well-suited\nto improve unsupervised geometric predictions, as it pro-\nvides accurate information on the boundary between indi-\nvidual objects of the same type. Unlike class-level label, the\ninstance label itself does not have a well-deﬁned meaning.\nAcross different frames, the same label could refer to differ-\nent object instances. To efﬁciently represent the instance-\nlevel information, we compute the gradient map of a dense\ninstance map and use it as an additional feature channel con-\ncatenating to the class label input (dense/one-hot encoding).\nDirect Input versus Residual Correction: Complemen-\ntary to the choice of encoding, we also experiment with dif-\nferent architectures to feed semantic information to the ge-\nometry prediction model. In particular, we make a residual\nprediction using a separate branch that takes in only seman-\ntic inputs. Notably, using residual depth prediction leads\nto further improvement on top of the gains from the direct\ninput methods.\n4.2. Semantic Guided Loss Functions\nThe information from semantic predictions could be di-\nminished due to noisy semantic labels and very deep archi-\ntectures. Hence, we design training loss functions that are\nguided by semantic information. In such design, the se-\nmantic predictions provide additional loss constraints to the\nnetwork. In this subsection, we introduce a set of seman-\ntic guided loss functions to improve depth and ﬂow predic-\ntions.\nSemantic Warp Loss: Semantic predictions can help learn\nscenarios where reconstruction of the RGB image is cor-\nrect in terms of pixel values but violates obvious semantic\ncorrespondences, e.g. matching pixels to incorrect seman-\ntic classes and/or instances. In light of this, we propose to\nreconstruct the semantic predictions in addition of doing so\nfor RGB images. We call this “semantic warping loss” as it\nis based on warping of the semantic predictions from source\nframes to the target frame. Let Ss be the source frame se-\nmantic prediction and ˜Srig\ns\nbe the warped semantic image,\nwe deﬁne semantic warp loss as:\nLsem = || ˜Srig\ns\n−St||2\n(3)\nThe warped loss is added to the baseline framework using a\nhyper-tuned value of the weight w.\nMasking of Reconstruction Loss via Semantics: As de-\nscribed in Section 3, the ambiguity in object motion can\nlead to sub-optimal learning.\nSemantic labels can par-\ntially resolve this by separating each class of region. Moti-\nvated by this observation, we mask the foreground region\nout to form a set of new images Jk\nt,c = It,c ⊙St,k for\nc = 0, 1, 2 and k = 0, ..., K −1 where c represents the\nRGB-channel index, ⊙is the element-wise multiplication\noperator and Ss,k is the k-th channel of the binary semantic\nsegmentation (K classes in total). Similarly we can obtain\n˜Jrig,k\ns,c\n= ˜Irig\ns,c ⊙St,k for c = 0, 1, 2 and k = 0, ..., K −1.\nFinally, the image similarity loss is deﬁned as:\nL′\nrw =\nK−1\nX\nk=0\nα1 −SSIM(Jk\nt , ˜Jrig,k\ns\n)\n2\n+(1−α)||Jk\nt −˜Jrig,k\ns\n||1\n(4)\nDepthNet\nPoseNet\nVGG16\nImage Loss\ndepth\npose\nSemantic \nMaps\n*Optional\nRGB\nTotal Loss\n⊕\nSemantic \nLoss\npredicted semantic \nmaps\nConcat\nFigure 4: Infer semantic labels from depth predictions. The\ntransfer function uses RGB and predicted depth as input.\nWe experimented the variants with and without semantic\ninput.\nSemantic-Aware Edge Smoothness Loss: Equation 2 uses\nRGB to infer edge locations when enforcing smooth re-\ngions of depth. This could be improved by including an\nedge map computed from semantic predictions. Given a se-\nmantic segmentation result St, we deﬁne a weight matrix\nMt ∈[0, 1]H×W where the weight is low (close to zero)\non class boundary regions and high (close to one) on other\nregions. We propose a new image similarity loss as:\nL′′\nrw =\nK−1\nX\nk=0\nα1 −SSIM(It ⊙Mt, ˜Irig\ns\n⊙Mt)\n2\n+ (1 −α)||It ⊙Mt −˜Irig\ns\n⊙Mt||1\n(5)\nSemantic Loss by Transfer Network: Motivated by the\nobservation that high-quality depth maps usually depict ob-\nject classes and background region, we designed a novel\ntransfer network architecture. As shown in Fig 4 the trans-\nfer network block receives predicted depth maps along\nwith the original RGB images and outputs semantic labels.\nThe transfer network introduces a semantic reconstruction\nloss term to the objective function to force the predicted\ndepth maps to be richer in contextual sense, hence reﬁnes\nthe depth estimation. For implementation, we choose the\nResNet-50 as the backbone and alter the dimensions for the\ninput and output convolutional layers to be consistent with\nthe segmentation task. The network generates one-hot en-\ncoded heatmaps and use cross-entropy as the semantic sim-\nilarity measure.\n5. Experiments\nTo quantify the beneﬁts that semantic information brings\nto geometry-based learning, we designed experiments sim-\nilar to [51]. First, we showed our model’s depth predic-\ntion performance on KITTI dataset [14], which outper-\nformed state-of-the-art unsupervised and supervised mod-\nels. Then we designed ablation studies to analyze each in-\ndividual component’s contribution. Finally, we presented\nimprovements in ﬂow predictions and revisited the perfor-\nmance gains using a category-speciﬁc evaluation.\n5.1. Implementation Details\nTo make a fair comparison with state-of-the-art models\n[8, 54, 51], we divided KITTI 2015 dataset into train set\n(40238 images) and test set (697 images) according to the\nrules from Eigen et al [8]. We used DeepLabv3+ [4] (pre-\ntrained on [6]) for semantic segmentation and Mask-RCNN\n[18] (pretrained on [27]) for instance segmentation. Similar\nto the hyper-parameter settings in [51], we used Adam opti-\nmizer [22] with initial learning rate as 2e-4, set batch size to\n4 per GPU and trained our modiﬁed DepthNet and PoseNet\nmodules for 250000 iterations with random shufﬂing and\ndata augmentation (random scaling, cropping and RGB per-\nturbation). The training took 10 hours on two GTX1080Ti.\n5.2. Monocular Depth Evaluation on KITTI\nWe augmented the image sequences with corresponding\nsemantic and instance segmentation sequences and adopted\nthe scale normalization suggested in [42]. In the evalua-\ntion stage, the ground truth depth maps were generated by\nprojecting 3D Velodyne LiDAR points to the image plane.\nFollowed by [51], we clipped our depth predictions within\n0.001m to 80m and calibrated the scale by the medium num-\nber of the ground truth. The evaluation results are shown in\nTable 1, where all the metrics are introduced in [8]. Our\nmodel beneﬁts signiﬁcantly from feature augmentation and\nsurpasses the state-of-the-art methods substantially in both\nsupervised and unsupervised ﬁelds.\nMoreover, we found a correlation between the improve-\nment region and object classes. We visualized the absolute\nrelative error (AbsRel) among image plane from our model\nand from the baseline. As shown in Fig 5, most of the im-\nprovements come from regions containing objects. This in-\ndicates that the network is able to learn the concept of ob-\njects to improve the depth prediction by rendering extra se-\nmantic information.\nFigure 5: Comparisons of depth evaluations on KITTI. Top\nto bottom: Input RGB image, AbsRel error map of [51],\nAbsRel error map of ours, and improvements of ours on Ab-\nsRel map compared to [51]. The ground truth is interpolated\nto enhance visualization. Lighter color in those heatmaps\ncorresponds to larger errors or improvements.\n5.3. Ablation Studies\nHere we took a deeper look of our model, testiﬁed its ro-\nbustness under noise from observations, and presented vari-\nations of our framework to show promising explorations for\nfuture researchers. In the following experiments, we kept all\nthe other parameters the same in [51] and applied the same\ntraining/evaluation strategies mentioned in Section 5.2\nHow much gain from various feature augmentation?\nWe\ntried\nout\ndifferent\ncombinations\nand\nforms\nof\nsemantic/instance-level inputs based on “Yin et al” [51]\nwith scale normalization. From Table 2, our ﬁrst conclusion\nis that any meaningful form of extra input can ameliorate the\nmodel, which is straightforward. Secondly, when we use\n“Semantic” and “Instance class” for feature augmentation,\none-hot encoding tends to outperform the dense map form.\nConceivably one-hot encoding stores richer information in\nits structural formation, whereas dense map only contains\ndiscrete labels which may be more difﬁcult for learning.\nMoreover, using both “Semantic” and “Instance class” can\nprovide further gain, possibly due to the different label dis-\ntributions of the two datasets. Labels from Cityscape [6]\ncover both background and foreground concepts, while the\nCOCO dataset [27] focuses more on objects. At last, when\nwe combined one-hot encoded “Semantic” and “Instance\nclass” along with “Instance id” edge features, the network\nexploited the most from scene understanding, hence greatly\nenhanced the performance.\nCan our model survive under low lighting conditions?\nTo testify our model’s robustness for varied lighting condi-\ntions, we multiplied a scalar between 0 and 1 to RGB inputs\nMethod\nSupervised\nError-related metrics\nAccuracy-related metrics\nAbs Rel\nSq Rel\nRSME\nRSME log\nδ < 1.25\nδ < 1.252\nδ < 1.253\nEigen et al. [8] Coarse\nDepth\n0.214\n1.605\n6.653\n0.292\n0.673\n0.884\n0.957\nEigen et al. [8] Fine\nDepth\n0.203\n1.548\n6.307\n0.282\n0.702\n0.890\n0.957\nLiu et al. [29]\nDepth\n0.202\n1.614\n6.523\n0.275\n0.678\n0.895\n0.965\nGodard et al. [16]\nPose\n0.148\n1.344\n5.927\n0.247\n0.803\n0.922\n0.964\nZhou et al. [54] updated\nNo\n0.183\n1.595\n6.709\n0.270\n0.734\n0.902\n0.959\nYin et al. [51]\nNo\n0.155\n1.296\n5.857\n0.233\n0.793\n0.931\n0.973\nOurs\nNo\n0.133\n0.905\n5.181\n0.208\n0.825\n0.947\n0.981\n(improved by)\n14.04%\n30.19%\n11.55%\n10.85%\n3.14%\n1.53%\n0.80%\nTable 1: Monocular depth results on KITTI 2015 [33] by the split of Eigen et al. [8] (Our model used scale normalization.)\nSemantic\nInstance\nInstance\nError-related metrics\nAccuracy-related metrics\nclass\nid\nAbs Rel\nSq Rel\nRSME\nRSME log\nδ < 1.25\nδ < 1.252\nδ < 1.253\n0.149\n1.060\n5.567\n0.226\n0.796\n0.935\n0.975\nDense\n0.142\n0.991\n5.309\n0.216\n0.814\n0.943\n0.980\nOne-hot\n0.139\n0.949\n5.227\n0.214\n0.818\n0.945\n0.980\nDense\n0.142\n0.986\n5.325\n0.218\n0.812\n0.943\n0.978\nOne-hot\n0.141\n0.976\n5.272\n0.215\n0.811\n0.942\n0.979\nEdge\n0.145\n1.037\n5.314\n0.217\n0.807\n0.943\n0.978\nDense\nEdge\n0.142\n0.969\n5.447\n0.219\n0.808\n0.941\n0.978\nOne-hot\nOne-hot\nEdge\n0.133\n0.905\n5.181\n0.208\n0.825\n0.947\n0.981\nTable 2: Depth prediction performance gains due to different semantic sources and forms. (Scale normalization was used.)\nin the evaluation. Fig 6 showed that our model still holds\nequal performance to [51] when the intensity drops to 30%.\n(a) Observations under decreased light condition (left to right)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nDarkness = 1 - Intensity\n1\n2\n3\n Square Relative Error\nTest under Varied Light Conditions\nYin et al\nOurs\n(b) Robustness under decreased light condition\nFigure 6: The abs errs change as lighting condition drops.\nOur model can still be better than baseline even if the light-\ning intensity drops to 0.30 of the original ones.\nWhich module needs extra information the most?\nWe fed semantics to only DepthNet or PoseNet to see the\ndifference in their performance gain. From Table 3 we can\nsee that compared to DepthNet, PoseNet learns little from\nthe semantics to help depth prediction. Therefore we tried\nto feed the semantics to a new PoseNet with the same struc-\nture as the original one and compute the predicted poses by\ntaking the sum from two different PoseNets, which led to\nperformance gain; however, performance gain was not ob-\nserved from applying the same method to DepthNet.\nHow to be “semantic-free” in evaluation?\nThough semantic helps depth prediction, this idea relies on\nsemantic features during the evaluation phase. If semantic\nis only utilized in the loss, it would not be needed in evalua-\ntion. We attempted to introduce a handcrafted semantic loss\nterm as a weight guidance among image plane but it didn’t\nwork well. Also we designed a transfer network which uses\nthe predicted depth to predict semantic maps along with a\nreconstruction error to help in the training stage. The result\nin Table 4 shows a better result can be obtained by training\nfrom pretrained models.\n5.4. Optical Flow Estimation on KITTI\nUsing our best model for DepthNet and PoseNet in Sec-\ntion 5.2, we conducted rigid ﬂow and full ﬂow evaluation\non KITTI [14]. We generated the rigid ﬂow from estimated\ndepth and pose, and compared with [51]. Our model per-\nformed better in all the metrics shown in Table 5.\nDepthNet\nPoseNet\nError-related metrics\nAccuracy-related metrics\nAbs Rel\nSq Rel\nRSME\nRSME log\nδ < 1.25\nδ < 1.252\nδ < 1.253\n0.149\n1.060\n5.567\n0.226\n0.796\n0.935\n0.975\nChannel\n0.145\n0.957\n5.291\n0.216\n0.805\n0.943\n0.980\nChannel\n0.147\n1.076\n5.385\n0.223\n0.808\n0.938\n0.975\nChannel\nChannel\n0.139\n0.949\n5.227\n0.214\n0.818\n0.945\n0.980\nExtra Net\nChannel\n0.147\n1.036\n5.593\n0.226\n0.803\n0.937\n0.975\nChannel\nExtra Net\n0.135\n0.932\n5.241\n0.211\n0.821\n0.945\n0.980\nTable 3: Each module’s contribution toward performance gain from semantics. (Scale normalization was used.)\nCheckpoint\nTransfer\nError-related metrics\nAccuracy-related metrics\nNetwork\nAbs Rel\nSq Rel\nRSME\nRSME log\nδ < 1.25\nδ < 1.252\nδ < 1.253\nYin et al. [51]\n0.155\n1.296\n5.857\n0.233\n0.793\n0.931\n0.973\nYin et al. [51]\nYes\n0.150\n1.141\n5.709\n0.231\n0.792\n0.934\n0.974\nYin et al. [51] +sn\n0.149\n1.060\n5.567\n0.226\n0.796\n0.935\n0.975\nYin et al. [51] +sn\nYes\n0.145\n0.994\n5.422\n0.222\n0.806\n0.939\n0.976\nTable 4: Gains in depth prediction using our proposed Transfer Network. (+sn: “using scale normalization”.)\nMethod\nEnd Point Error\nAccuracy\nNoc\nAll\nNoc\nAll\nYin et al. [51]\n23.5683\n29.2295\n0.2345\n0.2237\nOurs\n22.3819\n26.8465\n0.2519\n0.2376\nTable 5: Rigid ﬂow prediction from ﬁrst stage on KITTI on\nnon-occluded regions(Noc) and overall regions(All).\nMethod\nEnd Point Error\nNoc\nAll\nDirFlowNetS\n6.77\n12.21\nYin et al. [51]\n8.05\n10.81\nOurs\n7.66\n13.91\nTable 6:\nFull ﬂow prediction on KITTI 2015 on non-\noccluded regions(Noc) and overall regions(All).\nResults\nfrom DirFlowNetS are shown in [51]\nWe further appended the semantic warping loss intro-\nduced in Section 4.2 to ResFlowNet in [51] and trained our\nmodel on KITTI stereo for 1600000 iterations. As demon-\nstrated in Table 6, ﬂow prediction got improved in non-\noccluded region compared to [51] and our model produced\ncomparable results in overall regions.\n5.5. Category-Speciﬁc Metrics Evaluation\nThis section will present the improvements by seman-\ntic categories. As shown in the bar-chart in Fig 7, most\nimprovements were shown in “Vehicle” and “Dynamic”\nclasses3, where errors are generally large. Our network did\nnot improve much for other less frequent categories, such\n3For “Dynamic” classes, we choose “person”, “rider”, “car”, “truck”,\n“bus”, “train”, “motorcycle” and “bicycle” classes as deﬁned in [6]\nCar\nMotorcycle Dynamic\nClasses\n0\n2\n4\n6\nSquare Relative Error\n5.42\n0.16\n5.59\n3.72\n0.21\n3.39\nDepth Prediction Comparison\nYin et al\nOurs\nCar\nMotorcycle Dynamic\nClasses\n0\n5\n10\n15\n20\nEndpoint Error\n19.62\n5.45\n19.50\n13.60\n6.40\n13.86\nFlow Prediction Comparison\nYin et al\nOurs\nFigure 7: Performance gains in depth (left) and ﬂow (right)\namong different classes of dynamic objects.\nas “Motorcycle”, which are generally more difﬁcult to seg-\nment in images.\n6. Conclusion\nIn SIGNet, we strive to achieve robust performance\nfor depth and ﬂow perception without using geometric\nlabels.\nTo achieve this goal, SIGNet utilizes semantic\nand instance segmentation to create spatial constraints on\nthe geometric attributes of the pixels.\nWe present novel\nmethods of feature augmentation and loss augmentation to\ninclude semantic labels in the geometry predictions. This\nwork presents a ﬁrst of a kind approach which moves away\nfrom pixel-level to object-level depth and ﬂow predictions.\nMost notably, our method signiﬁcantly surpasses the\nstate-of-the-art solution for monocular depth estimation. In\nthe future, we would like to extend our SIGNet to various\nsensor modalities (IMU, LiDAR or thermal).\nAcknowledgement: This work was supported by UCSD\nfaculty startup (Prof.\nBharadia), Toyota InfoTechnology\nCenter and Center for Wireless communication at UCSD.\nReferences\n[1] Michael Bloesch, Jan Czarnowski, Ronald Clark, Stefan\nLeutenegger, and Andrew J Davison. Codeslam-learning a\ncompact, optimisable representation for dense visual slam.\narXiv preprint arXiv:1804.00874, 2018. 1\n[2] Arunkumar Byravan and Dieter Fox.\nSe3-nets: Learning\nrigid body motion using deep neural networks. In Robotics\nand Automation (ICRA), 2017 IEEE International Confer-\nence on, pages 173–180. IEEE, 2017. 1, 2\n[3] Rich Caruana. Multitask learning. Mach. Learn., 28(1):41–\n75, July 1997. 3\n[4] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Flo-\nrian Schroff, and Hartwig Adam.\nEncoder-decoder with\natrous separable convolution for semantic image segmenta-\ntion. arXiv preprint arXiv:1802.02611, 2018. 2, 3, 6\n[5] Weifeng Chen, Zhao Fu, Dawei Yang, and Jia Deng. Single-\nimage depth perception in the wild. In Proceedings of the\n30th International Conference on Neural Information Pro-\ncessing Systems, NIPS’16, pages 730–738, USA, 2016. Cur-\nran Associates Inc. 2\n[6] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\nRehfeld,\nMarkus Enzweiler,\nRodrigo Benenson,\nUwe\nFranke, Stefan Roth, and Bernt Schiele.\nThe cityscapes\ndataset for semantic urban scene understanding. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 3213–3223, 2016. 2, 6, 8\n[7] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip\nHausser, Caner Hazirbas, Vladimir Golkov, Patrick van der\nSmagt, Daniel Cremers, and Thomas Brox. Flownet: Learn-\ning optical ﬂow with convolutional networks. In The IEEE\nInternational Conference on Computer Vision (ICCV), De-\ncember 2015. 2\n[8] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map\nprediction from a single image using a multi-scale deep net-\nwork. In Advances in neural information processing systems,\npages 2366–2374, 2014. 2, 6, 7\n[9] Xiaohan Fei, Alex Wang, and Stefano Soatto.\nGeo-\nsupervised\nvisual\ndepth\nprediction.\narXiv\npreprint\narXiv:1807.11130, 2018. 1\n[10] Philipp Fischer, Alexey Dosovitskiy, Eddy Ilg, Philip\nH¨ausser, Caner Hazırbas¸, Vladimir Golkov, Patrick Van der\nSmagt, Daniel Cremers, and Thomas Brox. Flownet: Learn-\ning optical ﬂow with convolutional networks. arXiv preprint\narXiv:1504.06852, 2015. 1\n[11] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat-\nmanghelich, and Dacheng Tao. Deep ordinal regression net-\nwork for monocular depth estimation. In The IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\nJune 2018. 2\n[12] Yuan Gao, Qi She, Jiayi Ma, Mingbo Zhao, Wei Liu, and\nAlan L. Yuille.\nNddr-cnn: Layer-wise feature fusing in\nmulti-task cnn by neural discriminative dimensionality re-\nduction. CoRR, abs/1801.08297, 2018. 3\n[13] Ravi Garg, G VijayKumarB., and Ian D. Reid. Unsupervised\ncnn for single view depth estimation: Geometry to the res-\ncue. In ECCV, 2016. 3, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22\n[14] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we\nready for autonomous driving? the kitti vision benchmark\nsuite. In Computer Vision and Pattern Recognition (CVPR),\n2012 IEEE Conference on, pages 3354–3361. IEEE, 2012.\n2, 6, 7\n[15] Cl´ement Godard, Oisin Mac Aodha, and Gabriel J. Bros-\ntow. Digging into self-supervised monocular depth estima-\ntion. CoRR, abs/1806.01260, 2018. 3\n[16] Cl´ement Godard, Oisin Mac Aodha, and Gabriel J Bros-\ntow.\nUnsupervised monocular depth estimation with left-\nright consistency.\nIn CVPR, volume 2, page 7, 2017.\n3,\n4, 7\n[17] Daniel Gordon, Ali Farhadi, and Dieter Fox. Re 3: Real-time\nrecurrent regression networks for visual tracking of generic\nobjects. IEEE Robotics and Automation Letters, 3(2):788–\n795, 2018. 2\n[18] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Gir-\nshick. Mask r-cnn. In Computer Vision (ICCV), 2017 IEEE\nInternational Conference on, pages 2980–2988. IEEE, 2017.\n2, 3, 6\n[19] Keke He, Zhanxiong Wang, Yanwei Fu, Rui Feng, Yu-Gang\nJiang, and Xiangyang Xue. Adaptively weighted multi-task\ndeep network for person attribute classiﬁcation. In Proceed-\nings of the 25th ACM International Conference on Multime-\ndia, MM ’17, pages 1636–1644, New York, NY, USA, 2017.\nACM. 3\n[20] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper,\nAlexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evo-\nlution of optical ﬂow estimation with deep networks. In The\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), July 2017. 2\n[21] J Yu Jason, Adam W Harley, and Konstantinos G Derpanis.\nBack to basics: Unsupervised learning of optical ﬂow via\nbrightness constancy and motion smoothness. In European\nConference on Computer Vision, pages 3–10. Springer, 2016.\n3\n[22] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 6\n[23] I. Kokkinos.\nUbernet: Training a universal convolutional\nneural network for low-, mid-, and high-level vision using\ndiverse datasets and limited memory. In 2017 IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\npages 5454–5463, July 2017. 3\n[24] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.\nImagenet classiﬁcation with deep convolutional neural net-\nworks. Commun. ACM, 60(6):84–90, May 2017. 2\n[25] Wei-Sheng Lai, Jia-Bin Huang, and Ming-Hsuan Yang.\nSemi-supervised learning for optical ﬂow with generative ad-\nversarial networks. In NIPS, 2017. 2\n[26] Konstantinos-Nektarios Lianos, Johannes L Sch¨onberger,\nMarc Pollefeys, and Torsten Sattler. Vso: Visual semantic\nodometry. In Proceedings of the European Conference on\nComputer Vision (ECCV), pages 234–250, 2018. 1\n[27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nEuropean conference on computer vision, pages 740–755.\nSpringer, 2014. 2, 6\n[28] Beyang Liu, Stephen Gould, and Daphne Koller. Single im-\nage depth estimation from predicted semantic labels. 2010\nIEEE Computer Society Conference on Computer Vision and\nPattern Recognition, pages 1253–1260, 2010. 3\n[29] Fayao Liu, Chunhua Shen, Guosheng Lin, and Ian D Reid.\nLearning depth from single monocular images using deep\nconvolutional neural ﬁelds. IEEE Trans. Pattern Anal. Mach.\nIntell., 38(10):2024–2039, 2016. 2, 7\n[30] Yongxi Lu, Abhishek Kumar, Shuangfei Zhai, Yu Cheng,\nTara Javidi, and Rogerio Feris. Fully-adaptive feature shar-\ning in multi-task networks with applications in person at-\ntribute classiﬁcation. In The IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), July 2017. 3\n[31] Reza Mahjourian, Martin Wicke, and Anelia Angelova. Un-\nsupervised learning of depth and ego-motion from monoc-\nular video using 3d geometric constraints.\nIn The IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), June 2018. 3\n[32] Simon Meister, Junhwa Hur, and Stefan Roth. UnFlow: Un-\nsupervised learning of optical ﬂow with a bidirectional cen-\nsus loss. In AAAI, New Orleans, Louisiana, Feb. 2018. 3\n[33] Moritz Menze and Andreas Geiger. Object scene ﬂow for au-\ntonomous vehicles. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 3061–\n3070, 2015. 7\n[34] Elliot Meyerson and Risto Miikkulainen. Beyond shared hi-\nerarchies: Deep multitask learning through soft layer order-\ning. CoRR, abs/1711.00108, 2017. 3\n[35] Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Mar-\ntial Hebert. Cross-stitch networks for multi-task learning.\n2016 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 3994–4003, 2016. 3\n[36] N.Mayer,\nE.Ilg,\nP.H¨ausser,\nP.Fischer,\nD.Cremers,\nA.Dosovitskiy, and T.Brox.\nA large dataset to train\nconvolutional networks for disparity, optical ﬂow, and scene\nﬂow estimation.\nIn IEEE International Conference on\nComputer Vision and Pattern Recognition (CVPR), 2016.\narXiv:1512.02134. 2\n[37] Zhe Ren, Junchi Yan, Bingbing Ni, Bin Liu, Xiaokang Yang,\nand Hongyuan Zha. Unsupervised deep learning for optical\nﬂow estimation. In AAAI, volume 3, page 7, 2017. 3\n[38] Ashutosh Saxena, Sung H Chung, and Andrew Y Ng. Learn-\ning depth from single monocular images. In Advances in\nneural information processing systems, pages 1161–1168,\n2006. 1\n[39] Ashutosh Saxena, Min Sun, and Andrew Y Ng. Make3d:\nLearning 3d scene structure from a single still image. IEEE\ntransactions on pattern analysis and machine intelligence,\n31(5):824–840, 2009. 1\n[40] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz.\nPwc-net: Cnns for optical ﬂow using pyramid, warping, and\ncost volume. In The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), June 2018. 2\n[41] Sudheendra Vijayanarasimhan, Susanna Ricco, Cordelia\nSchmid, Rahul Sukthankar, and Katerina Fragkiadaki. Sfm-\nnet: Learning of structure and motion from video.\narXiv\npreprint arXiv:1704.07804, 2017. 1, 3\n[42] Chaoyang Wang, Jos´e Miguel Buenaposada, Rui Zhu, and\nSimon Lucey. Learning depth from monocular videos us-\ning direct methods. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 2022–\n2030, 2018. 6\n[43] Lijun Wang, Wanli Ouyang, Xiaogang Wang, and Huchuan\nLu. Visual tracking with fully convolutional networks. In\nProceedings of the IEEE international conference on com-\nputer vision, pages 3119–3127, 2015. 2\n[44] Peng Wang, Xiaohui Shen, Zhe Lin, Scott Cohen, Brian\nPrice, and Alan L. Yuille. Towards uniﬁed depth and seman-\ntic prediction from a single image. In The IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), June\n2015. 3\n[45] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-\nmoncelli. Image quality assessment: from error visibility to\nstructural similarity. IEEE transactions on image processing,\n13(4):600–612, 2004. 3\n[46] Ke Xian, Chunhua Shen, Zhiguo Cao, Hao Lu, Yang Xiao,\nRuibo Li, and Zhenbo Luo. Monocular relative depth percep-\ntion with web stereo data supervision. In The IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\nJune 2018. 2\n[47] Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and\nDieter Fox. Posecnn: A convolutional neural network for\n6d object pose estimation in cluttered scenes. arXiv preprint\narXiv:1711.00199, 2017. 2\n[48] Dan Xu, Wanli Ouyang, Xiaogang Wang, and Nicu Sebe.\nPad-net: Multi-tasks guided prediction-and-distillation net-\nwork for simultaneous depth estimation and scene parsing.\nCoRR, abs/1805.04409, 2018. 3\n[49] Dan Xu, Elisa Ricci, Wanli Ouyang, Xiaogang Wang, and\nNicu Sebe. Multi-scale continuous crfs as sequential deep\nnetworks for monocular depth estimation.\nIn The IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), July 2017. 2\n[50] Dan Xu, Wei Wang, Hao Tang, Hong Liu, Nicu Sebe, and\nElisa Ricci. Structured attention guided convolutional neural\nﬁelds for monocular depth estimation. In The IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\nJune 2018. 2\n[51] Zhichao Yin and Jianping Shi. Geonet: Unsupervised learn-\ning of dense depth, optical ﬂow and camera pose. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), volume 2, 2018. 1, 2, 3, 4, 6,\n7, 8, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22\n[52] Huangying Zhan, Ravi Garg, Chamara Saroj Weerasekera,\nKejie Li, Harsh Agarwal, and Ian Reid.\nUnsupervised\nlearning of monocular depth estimation and visual odometry\nwith deep feature reconstruction. In The IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), June\n2018. 3\n[53] Ziyu Zhang, Alexander G. Schwing, Sanja Fidler, and\nRaquel Urtasun.\nMonocular object instance segmentation\nand depth ordering with cnns.\n2015 IEEE International\nConference on Computer Vision (ICCV), pages 2614–2622,\n2015. 2\n[54] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G\nLowe. Unsupervised learning of depth and ego-motion from\nvideo. In CVPR, volume 2, page 7, 2017. 1, 3, 4, 6, 7\nSupplementary Material for SIGNet\nAdditional ablation studies on loss augmentations: As mentioned in our paper, the heuristic loss functions are not effective\neven after careful hyper-parameter tuning. This motivated us to design a learnable loss function (transfer network), which\ndoes improve upon the baseline as shown in Table 4 of our paper.\nMethod\nError metrics\nAccuracy (δ <)\nAbsRel\nSqRel\nRSME\nRSMElog\n1.251\n1.252\n1.253\nYin et al.[51]\n0.155\n1.296\n5.857\n0.233\n0.793\n0.931\n0.973\nWarp Loss\n0.169\n1.246\n6.233\n0.254\n0.750\n0.917\n0.968\nMask Loss\n0.165\n1.204\n5.593\n0.232\n0.769\n0.926\n0.974\nEdge Loss\n0.163\n1.230\n5.961\n0.243\n0.774\n0.924\n0.970\nTransfer\n0.150\n1.141\n5.709\n0.231\n0.792\n0.934\n0.974\nTable 1: Depth predictions for different loss augmentations (without using scale normalization). Here Warp Loss, Mask Loss\nand Edge Loss are on par or not as good as the baseline, whereas Transfer Network surpasses the baseline in almost all the\nmetrics.\nWhy does ExtraNet only work for PoseNet? In the ablation studies in Section 5.3, we tested the contribution of semantic\ninformation in each module.\nThe result suggests that vanilla PoseNet beneﬁts from semantics only marginally, which\nmight due to its simple structure. By adding Extra Network (ExtraNet) to PoseNet, our model gained further improvement.\nExtraNet does not beneﬁt DepthNet because the latter has already had a complicated structure as shown in Figure 1.\n \n \nConv\nUpconv\nMaxpool\nConcat\nUpsample+Concat\nPrediction\nResblock\nInput\nDepthNet\nPoseNet\nFigure 1: Network structures for DepthNet and PoseNet\nMore visualization results for depth estimation: In the rest of the supplementary material, we will present extra visual-\nization results to help readers understand where our semantic-aided model improved the most. We compared the prediction\nresult from our best model in Table 1 with Yin et al. [51] and ground truth. We followed [13] to plot the prediction result\nusing disparity heatmaps. The following results show that our model can gain improvement from regions belonging to cars\nand other dynamic classes.\nFigure 2: Top to bottom: input image, semantic segmentation, instance segmentation, ground truth disparity map, disparity\nprediction from baseline(Yin et al. [51]) , disparity prediction from ours, AbsRel error map of baseline models, AbsRel\nerror map of ours and the improvement region compared to baseline. For the purpose of visualization, disparity maps are\ninterpolated and cropped[13]. For all heatmaps, darker means smaller value (disparity, error or improvement). Typical image\nregions where we do better include cars, pedestrians and other common dynamic objects\nFigure 3: Top to bottom: input image, semantic segmentation, instance segmentation, ground truth disparity map, disparity\nprediction from baseline(Yin et al. [51]) , disparity prediction from ours, AbsRel error map of baseline models, AbsRel\nerror map of ours and the improvement region compared to baseline. For the purpose of visualization, disparity maps are\ninterpolated and cropped[13]. For all heatmaps, darker means smaller value (disparity, error or improvement). Typical image\nregions where we do better include cars, pedestrians and other common dynamic objects\nFigure 4: Top to bottom: input image, semantic segmentation, instance segmentation, ground truth disparity map, disparity\nprediction from baseline(Yin et al. [51]) , disparity prediction from ours, AbsRel error map of baseline models, AbsRel\nerror map of ours and the improvement region compared to baseline. For the purpose of visualization, disparity maps are\ninterpolated and cropped[13]. For all heatmaps, darker means smaller value (disparity, error or improvement). Typical image\nregions where we do better include cars, pedestrians and other common dynamic objects\nFigure 5: Top to bottom: input image, semantic segmentation, instance segmentation, ground truth disparity map, disparity\nprediction from baseline(Yin et al. [51]) , disparity prediction from ours, AbsRel error map of baseline models, AbsRel\nerror map of ours and the improvement region compared to baseline. For the purpose of visualization, disparity maps are\ninterpolated and cropped[13]. For all heatmaps, darker means smaller value (disparity, error or improvement). Typical image\nregions where we do better include cars, pedestrians and other common dynamic objects\nFigure 6: Top to bottom: input image, semantic segmentation, instance segmentation, ground truth disparity map, disparity\nprediction from baseline(Yin et al. [51]) , disparity prediction from ours, AbsRel error map of baseline models, AbsRel\nerror map of ours and the improvement region compared to baseline. For the purpose of visualization, disparity maps are\ninterpolated and cropped[13]. For all heatmaps, darker means smaller value (disparity, error or improvement). Typical image\nregions where we do better include cars, pedestrians and other common dynamic objects\nFigure 7: Top to bottom: input image, semantic segmentation, instance segmentation, ground truth disparity map, disparity\nprediction from baseline(Yin et al. [51]) , disparity prediction from ours, AbsRel error map of baseline models, AbsRel\nerror map of ours and the improvement region compared to baseline. For the purpose of visualization, disparity maps are\ninterpolated and cropped[13]. For all heatmaps, darker means smaller value (disparity, error or improvement). Typical image\nregions where we do better include cars, pedestrians and other common dynamic objects\nFigure 8: Top to bottom: input image, semantic segmentation, instance segmentation, ground truth disparity map, disparity\nprediction from baseline(Yin et al. [51]) , disparity prediction from ours, AbsRel error map of baseline models, AbsRel\nerror map of ours and the improvement region compared to baseline. For the purpose of visualization, disparity maps are\ninterpolated and cropped[13]. For all heatmaps, darker means smaller value (disparity, error or improvement). Typical image\nregions where we do better include cars, pedestrians and other common dynamic objects\nFigure 9: Top to bottom: input image, semantic segmentation, instance segmentation, ground truth disparity map, disparity\nprediction from baseline(Yin et al. [51]) , disparity prediction from ours, AbsRel error map of baseline models, AbsRel\nerror map of ours and the improvement region compared to baseline. For the purpose of visualization, disparity maps are\ninterpolated and cropped[13]. For all heatmaps, darker means smaller value (disparity, error or improvement). Typical image\nregions where we do better include cars, pedestrians and other common dynamic objects\nFigure 10: Top to bottom: input image, semantic segmentation, instance segmentation, ground truth disparity map, disparity\nprediction from baseline(Yin et al. [51]) , disparity prediction from ours, AbsRel error map of baseline models, AbsRel\nerror map of ours and the improvement region compared to baseline. For the purpose of visualization, disparity maps are\ninterpolated and cropped[13]. For all heatmaps, darker means smaller value (disparity, error or improvement). Typical image\nregions where we do better include cars, pedestrians and other common dynamic objects\nFigure 11: Top to bottom: input image, semantic segmentation, instance segmentation, ground truth disparity map, disparity\nprediction from baseline(Yin et al. [51]) , disparity prediction from ours, AbsRel error map of baseline models, AbsRel\nerror map of ours and the improvement region compared to baseline. For the purpose of visualization, disparity maps are\ninterpolated and cropped[13]. For all heatmaps, darker means smaller value (disparity, error or improvement). Typical image\nregions where we do better include cars, pedestrians and other common dynamic objects\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2018-12-13",
  "updated": "2019-04-05"
}