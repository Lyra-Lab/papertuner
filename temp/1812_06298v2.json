{
  "id": "http://arxiv.org/abs/1812.06298v2",
  "title": "Residual Policy Learning",
  "authors": [
    "Tom Silver",
    "Kelsey Allen",
    "Josh Tenenbaum",
    "Leslie Kaelbling"
  ],
  "abstract": "We present Residual Policy Learning (RPL): a simple method for improving\nnondifferentiable policies using model-free deep reinforcement learning. RPL\nthrives in complex robotic manipulation tasks where good but imperfect\ncontrollers are available. In these tasks, reinforcement learning from scratch\nremains data-inefficient or intractable, but learning a residual on top of the\ninitial controller can yield substantial improvements. We study RPL in six\nchallenging MuJoCo tasks involving partial observability, sensor noise, model\nmisspecification, and controller miscalibration. For initial controllers, we\nconsider both hand-designed policies and model-predictive controllers with\nknown or learned transition models. By combining learning with control\nalgorithms, RPL can perform long-horizon, sparse-reward tasks for which\nreinforcement learning alone fails. Moreover, we find that RPL consistently and\nsubstantially improves on the initial controllers. We argue that RPL is a\npromising approach for combining the complementary strengths of deep\nreinforcement learning and robotic control, pushing the boundaries of what\neither can achieve independently. Video and code at\nhttps://k-r-allen.github.io/residual-policy-learning/.",
  "text": "Residual Policy Learning\nTom Silver∗, Kelsey Allen∗, Josh Tenenbaum, Leslie Kaelbling\nMIT\n{tslvr, krallen, jbt}@mit.edu, lpk@csail.mit.edu\nAbstract—We present Residual Policy Learning (RPL): a\nsimple method for improving nondifferentiable policies using\nmodel-free deep reinforcement learning. RPL thrives in complex\nrobotic manipulation tasks where good but imperfect controllers\nare available. In these tasks, reinforcement learning from scratch\nremains data-inefﬁcient or intractable, but learning a residual on\ntop of the initial controller can yield substantial improvements.\nWe study RPL in six challenging MuJoCo tasks involving partial\nobservability, sensor noise, model misspeciﬁcation, and controller\nmiscalibration. For initial controllers, we consider both hand-\ndesigned policies and model-predictive controllers with known\nor learned transition models. By combining learning with control\nalgorithms, RPL can perform long-horizon, sparse-reward tasks\nfor which reinforcement learning alone fails. Moreover, we ﬁnd\nthat RPL consistently and substantially improves on the initial\ncontrollers. We argue that RPL is a promising approach for\ncombining the complementary strengths of deep reinforcement\nlearning and robotic control, pushing the boundaries of what\neither can achieve independently1.\nI. INTRODUCTION\nDeep reinforcement learning (RL) methods are increasingly\ncommon and increasingly successful in robotic manipulation\ndomains like grasping and pushing [1, 2, 3, 4, 5]. But for most\ncomplex problems of interest, learning from scratch remains\nintractable. For example, consider the task illustrated in Figure\n1a. A simulated Fetch robot must pick up and use a hook\nto drag an out-of-reach block to a target location. The only\nreward offered is a positive signal once the block reaches\nthe target. This long-horizon, sparse-reward problem remains\nout of reach for current deep RL methods. In contrast, it is\nrelatively straightforward to hand-design a policy that accom-\nplishes this hook task perfectly in simulation (see Section\nV-B).\nWhile a hand-designed policy may be robust to variations\nin the initial block position and target, it will likely break\ndown with more dramatic variations in the task. For example,\nconsider the task variation illustrated in Figure 1b. The robot\nmust now move a more complex rigid object to the goal. The\ntask is further complicated by static “bumps” on the table that\nmay impede the movement of the hook and object. Moreover,\nthe robot’s state includes no information about the bumps,\nwhich randomly regenerate at each trial, nor information about\nthe object’s shape, which is randomly selected from a library\nof 100 diverse objects. The policy designed for the original\ntask sometimes succeeds in this setup, but more often fails.\nWhat should be done when a policy — be it a hand-designed\npolicy, a model-predictive controller, or any other controller\n∗Equal contribution.\n1Video and code at https://k-r-allen.github.io/residual-policy-learning/.\nFig. 1: (a) A simulated Fetch robot must use a hook to move\na block to a target (red sphere). A hand-designed policy can\naccomplish this task perfectly. (b) The same policy often fails\nin a more difﬁcult task where the block is replaced by a\ncomplex object and the table top contains large “bumps.”\nResidual Policy Learning (RPL) augments the policy π with\na residual fθ, which can learn to accomplish the latter task.\nmapping states to actions — performs below par? One path\nforward is to manually tweak the policy. This option, while\npotentially laborious, may sufﬁce for some problems. But for\nother problems like the complex hook task described above,\nit is unclear how to even begin improving the policy by hand.\nIn this work, we propose Residual Policy Learning (RPL):\na method for improving policies using deep reinforcement\nlearning. Our main idea is to augment arbitrary initial policies\nby learning residuals on top of them. Given an initial policy\nπ : S →A with states s ∈S and actions a ∈A ⊆Rd, we\nlearn a residual function fθ(s) : S →A so that we have a\nresidual policy πθ : S →A given by\nπθ(s) = π(s) + fθ(s) .\nObserve that ∇θπθ(s) = ∇θfθ(s), that is, the gradient of\nthe policy does not depend on the initial policy π. We can\ntherefore use policy gradient methods to learn πθ even if the\ninitial policy π is not differentiable.\nThere are two ways to see the role of the residual. If the\ninitial policy is nearly perfect, the residual fθ may be viewed\nas a corrective term. But if the initial policy is far from\nideal, we may interpret the outputs of π as merely “hints”\nto guide exploration. In practice, these two interpretations of\nthe residual represent ends of a spectrum. We study problems\nall along this spectrum in this paper.\nWe present experimental results on several complex manipu-\nlation tasks that feature issues central to robotics and controller\ndesign: partial observability, sensor noise, model misspeci-\narXiv:1812.06298v2  [cs.RO]  3 Jan 2019\nﬁcation, and controller miscalibration. Our experiments are\ndesigned to investigate when and to what extent the following\ntwo claims hold:\n1) RPL improves on initial policies; and\n2) RPL is more data-efﬁcient than learning from scratch.\nWe examine two common sources of initial policies: hand-\ndesigned policies and model-predictive controllers (MPC). We\nconsider MPC with both known and learned transition models.\nIn the latter case, we use Probabilistic Ensembles with Tra-\njectory Sampling (PETS), a state-of-the-art method for model-\nbased RL, to derive the initial controller [6]. In all cases, RPL\nis able to substantially improve on the original policies, while\nrequiring far less data than learning from scratch to achieve\nthe same performance. Furthermore, in complex manipulation\ntasks like that in Figure 1b, RPL succeeds where learning\nfrom scratch is intractable and hand-designing perfect policies\nis unrealistic.\nII. RELATED WORK\nRPL be seen as tackling two separate but related questions:\nhow to improve imperfect controllers, and how to make deep\nreinforcement learning methods more data efﬁcient and able\nto handle longer horizon planning.\nThere has been a substantial body of work on improv-\ning the data efﬁciency of deep RL by combining model-\nfree and model-based approaches. These methods often ﬁrst\nlearn a dynamics model and then use this dynamics model\nto simulate experience [7, 8, 9] or compute gradients for\nmodel-free updates [10, 11]. Another set of approaches uses\nthe learned dynamics model (or inverse dynamics model) to\nperform trajectory optimization or model-predictive control\n[6, 12]. Further work uses such model-based methods to guide\na model-free learner in a DAGGER-style imitation strategy\n[13]. More recent work has shown an equivalence between\nmodel-free and model-based RL with goal-conditioned value\nfunctions [14], and used this to improve model-free RL data\nefﬁciency. RPL can be seen as an extension of this line of\nwork, as it provides a new means for combining the beneﬁts of\nmodel-based and model-free RL. We show in experiments that\nthe model-based method proposed by Chua et al. [6] can be\nimproved upon with RPL. However, RPL is also more general;\nit can be used to improve upon arbitrary policies, including\nbut not limited to model-based ones.\nRPL can also be seen as a form of imitation learning.\nThis set of approaches considers an expert that provides\ndemonstrations of a task to a learner. Most approaches then\nattempt to copy the expert’s strategy [13, 15], or to use inverse\nreinforcement learning to infer goals and subgoals of the\nexpert agent [16, 17]. Underpinning most of these approaches\nis the supposition that the expert is perfect. If the expert is\nindeed perfect, then RPL will be immediately perfect as well\ndue to our initialization strategy (see Section IV-A). But if the\nexpert is imperfect and is only meant to provide “hints,” RPL\nlearns to improve nonetheless.\nFrom robotics, many methods exist for learning different\naspects of the perception, control, execution pipeline. Focusing\non control speciﬁcally, Bayesian optimization approaches are\npopular for learning controllers based on Gaussian process\nmodels of objective functions to be optimized [18, 19, 20, 21,\n22]. Learning an accurate dynamics model is another central\nfocus for robotics (termed system identiﬁcation), and has been\napproached using analytic gradients [23, 24], ﬁnite differences\n[25] or Bayesian Optimization [8]. In contrast, RPL does not\npresuppose which aspect of the controller needs correction.\nThis is particularly valuable in partially observable settings,\nwhere it is unclear how to learn a good dynamics model or\ndesign a better objective function.\nIn the case of dynamics learning, our work is inspired by\nAjay et al. [26] and Kloss et al. [27] who learn a correction to\nan analytical physics model in order to perform better model-\npredictive control. RPL is more general in that it can learn\nto correct the model implicitly by correcting the policy, but\ncan also provide corrections which could not be provided by\ndynamics corrections (such as partially observable or noisy\ndomains).\nConcurrent work by Johannink et al. [28] also proposes\nresidual reinforcement learning, and focuses on showing the\nvalue of the approach for real robots in a task of block\ninsertion, investigating the effects of variation in the initial\nstate, control noise, and the transfer from sim to real. Here\nwe aim to show the power of residual policies for a variety\nof different tasks that disentangle several sources of difﬁculty:\npartial observability, sensor noise, model misspeciﬁcation, and\ncontroller miscalibration. We also empirically analyze the root\ncause of RPL’s success by introducing a baseline that uses the\ninitial policy only as an “expert” to guide exploration.\nIII. BACKGROUND\nRPL operates within a standard (Partially Observable)\nMarkov Decision Process (MDP) framework. An MDP is a\ntuple M = (S, A, R, T, γ) where s ∈S are states, a ∈A\nare actions, R(s, a) ∈R is the reward for taking action\na in state s, T(s, a, s′) = Pr(s′|s, a) is the probability of\ntransitioning to state s′ following state s and action a, and\n0 ≤γ ≤1 is a temporal discount factor. We assume all\ntrajectories or episodes sampled from the MDP have a ﬁnite\nnumber of actions (horizon) h. In all of the experiments\ndescribed in this paper, states and actions are real-valued\nvectors. A policy π : S →A maps states to actions. Given an\ninitial state s0, the reinforcement learning problem is to ﬁnd\na policy π that maximizes expected rewards discounted over\ntime J = Est∼M[Ph\nt=0 γtR(st, π(st))].\nLet Qπ : S × A →R be the action-value function that\ngives the expected future discounted rewards following policy\nπ. Many reinforcement learning methods make use of the\nBellman equation for the action-value function\nQπ(s, a) = Es′∼T (s,a,·)[R(s, a) + γQπ(s′, π(s′))]\nActor-critic methods learn both a parameterized policy πθ\n(the actor) and a parameterized action-value function Qθ (the\ncritic). The critic is trained with a loss function derived from\nthe Bellman equation above and the actor is trained to produce\nFig. 2: Illustration of the original ReactivePush policy and RPL on the SlipperyPush task. The original policy was\ndesigned so that the robot pushes the block to the target (red sphere) when the block has high friction. When the block has\nlower friction than anticipated, the block is pushed off the table (top row). RPL, our proposed method, learns to correct the\nfaulty policy and accomplish the task after 1 million simulator steps (bottom row). Given the same number of time steps,\nreinforcement learning from scratch results in a policy where the robot does not touch the block at all (not shown).\nactions that maximize the critic. This approach is typically\nmore stable than training the actor alone.\nFor the experiments in this work, we use Deep Deterministic\nPolicy Gradients (DDPG) [29], an actor-critic method that\nworks well in domains with continuous states and actions\n(though any RL method could be used with RPL in principle).\nIn DDPG, the actor is updated following the deterministic\npolicy gradient\n∇θJ ≈Est∼M[∇aQθ(s, a)|s=st,a=πθ(st)∇θπθ(s)|s=st]\nDDPG makes use of experience replay, in which transitions\n(st, at, rt, st+1) sampled from the environment are stored in\na replay buffer. During training, transitions are then randomly\ndrawn from the replay buffer in an effort to break the corre-\nlation between consecutive transitions.\nHindsight Experience Replay (HER) [5] extends experience\nreplay to dramatically improve data efﬁciency in domains\nwith sparse binary rewards (goals) like those we consider in\nour experiments. In HER, the reward function and policy are\nadditionally parameterized by a goal g so that they become\nR(s, a, g) and π(s, g) respectively. For our purposes, the goal\ng is a subvector of the ﬁnal state of an episode. During\ntraining, each transition added to the replay buffer includes\na goal gt that was achieved “in hindsight,” i.e. the goal\nthat was actually reached at the end of the training episode.\nGiven a sampled transition (st, at, rt, st+1, gt), the policy\nis then updated according to the reward R(st, at, gt). This\ntrick is especially useful early in training when the chance\nof achieving nonzero rewards is low. We combine HER and\nDDPG for all of the experiments presented in this work.\nIV. RESIDUAL POLICY LEARNING (RPL)\nIn Residual Policy Learning (RPL), we begin with an initial\npolicy π : S →A. Our goal is to learn a residual fθ to create\nan improved ﬁnal policy πθ(s) = π(s) + fθ(s).\nObserve that a ﬁxed initial policy π together with an MDP\nM\n= (S, A, R, T, γ) induces a residual MDP M (π)\n=\n(S, A, R, T (π), γ) where\nT (π)(s, a, s′) = T(s, π(s) + a, s′)\nIf we view M (π) as an MDP like any other, we see that the\nresidual that we wish to learn, fθ, is a policy in this MDP. We\ncan thus apply standard reinforcement learning techniques to\nlearn the residual. In this work, we parameterize fθ as a neural\nnetwork and use model-free deep RL methods for learning.\nRPL is as simple as that: given an initial policy, create a\nresidual policy and proceed with deep RL. We now describe a\nfew minor extensions that can improve performance and data\nefﬁciency in practice.\nA. Initializing the Residual\nA desirable property of RPL is that it should never make\na good initial policy worse. In the extreme case, if an initial\npolicy is perfect, then we would like the residual policy to have\nno inﬂuence. We therefore endeavor to initialize the residual\nfunction so that fθ(s) = 0¯ for all s ∈S. We do this by\ninitializing the last layer of the network to be zero.\nB. RPL with Actor-Critic Methods\nRPL learns a residual on the output of an initial policy.\nActor-critic methods like DDPG involve not only a policy\nbut also a learned action-value function. If we begin with a\nperfect initial policy and a poor critic, the policy performance\nmay degrade, since it is trained with reference to the critic.\nWe therefore propose to train the critic alone for a “burn in”\nperiod while leaving the policy ﬁxed. We can determine an\nappropriate burn in length automatically by monitoring the\ncritic loss function and waiting for it to dip below a threshold\nβ, which becomes a hyperparameter of our method. We use\nβ = 1.0 for all experiments in this paper.\nC. Recurrent RPL for POMDPs\nRPL can also be extended to handle Partially Observable\nMarkov Decision Processes (POMDPs). Generally, this is done\nin deep reinforcement learning by making πθ(s) recurrent.\nIn practice, this is challenging for DDPG [30], and so we\npresent an approximation by simply considering a ”history” of\nprevious states. This is equivalent to writing s = {st−n∥n ∈\n0, ..., N} with t being the current time-step, and N the history\nlength. While the history length could take on any value,\nwe found that a history length of just 1 (meaning the policy\nconsiders the current state and previous state) to be effective.\nWe take advantage of this extension in our NoisyHook\nexperiment in which observation noise obscures the input to\nthe policy.\nV. EXPERIMENTS\nHere we investigate to what extent RPL improves on initial\npolicies, learns faster than model-free RL alone, and succeeds\nin tasks where model-free RL is intractable.\nA. Tasks\nWe study six simulated manipulation tasks. All environ-\nments are implemented in MuJoCo [31]. To provide direct\ncomparison with previous work, we begin with a Push task\nand a PickAndPlace task, both taken from Plappert et al.\n[32]. We then present three more difﬁcult tasks that have not\nbeen previously considered: SlipperyPush, NoisyHook,\nand ComplexHook. These ﬁrst ﬁve tasks all involve a Fetch\nrobot positioned in front of a table top. In the ﬁnal task, we\nuse the “7-DOF Pusher” environment from Chua et al. [6].\nOur focus in the last task is model-based RL, so we call this\nenvironment MBRLPusher.\nIn the ﬁrst ﬁve tasks, following previous work, we parame-\nterize the action space in terms of changes to the end effector’s\nxyz position in world coordinates [32]. A fourth action coor-\ndinate modulates the gripper’s two ﬁngers symmetrically. (In\nthe push tasks, the gripper is locked, and the fourth action\ncoordinate has no effect.) In the sixth task, actions actuate\nthe joints of the 7-DOF robot arm directly. All actions are\nnormalized so that the resulting action space is A = [−1, 1]D,\nwhere D = 4 for the ﬁrst ﬁve tasks and D = 7 for the last.\nThe state spaces and rewards vary per task, as described next.\n1) Push: This task is taken directly from [32]. The objec-\ntive is to push an object (a cube) to a target location on the\ntable surface. The initial position of the object and the target\nlocation are randomized. The state space includes:\n• Gripper xyz position and velocity (6 dims)\n• Object xyz position, ypr rotation, velocities (12 dims)\n• Object position relative to the gripper (3 dims)\n• Gripper ﬁnger joint states and velocities (4 dims)\nfor a total dimensionality of 25. To use Hindsight Experience\nReplay, we must also specify achieved and desired goals. Here\nthe achieved goal is the three-dimensional ﬁnal position of the\nobject and the desired goal is the target location. Rewards are\nsparse and binary: a reward of 1 is given when the object\nis within a small radius around the target location and 0\notherwise. The episode is counted as a success if the last\nreward is 1, i.e. the goal is achieved. Episode lengths are 50\nand do not terminate early.\n2) SlipperyPush: Here we present a slight modiﬁcation\nto the original Push environment. In the original environment,\nthe object has a sliding friction coefﬁcient of 1.0. In this\nSlipperyPush environment, the same coefﬁcient is set\nto 0.18. The initial state randomization, state space, goals,\nrewards, and horizon are otherwise identical to Push.\n3) PickAndPlace: This task is taken directly from [32].\nAs in the previous tasks, the objective is to move an object\n(a cube) to a target location. However, the target location may\nnow be either on the table top or in the air above the table.\nAt the beginning of each episode, the xy position for the\ntarget location is randomly sampled as before. Then with 0.5\nprobability, the z location is set to be on the table surface;\notherwise, the z location is randomly sampled to be above\nthe table surface. As mentioned above, the gripper is now\nunlocked so that the ﬁngers open and close following the\nfourth action dimension. All other environment details are\nunchanged with respect to Push.\n4) NoisyHook: In this task, the robot cannot initially\nreach the block with its gripper. A new hook object is intro-\nduced and positioned to the right of the robot (see Figure 1a).\nThe objective is still to move the cube to a target location, but\nnow the robot must use the hook to manipulate the cube. The\ntarget location is randomly initialized so that it lies between\nthe cube and the robot. In addition to the 25 state dimensions\nincluded in the previous tasks, the state space now includes\ninformation about the hook:\n• Hook xyz position, rpy rotation, velocities (12 dims)\n• Hook position relative to the gripper (3 dims)\nfor a total of 15 + 25 = 40 dimensions. Rewards and goals\nare the same as in previous tasks; we provide no additional\nshaping rewards.\nThis NoisyHook task is further complicated with the\naddition of observation noise. We suppose that the robot has\nprecise proprioception but has signiﬁcant uncertainty about the\npositions of the hook and cube. At each time step, we add\nIID diagonal Gaussian noise (µ = 0.0, σ2 = 0.025) to the xy\nposition of the block and the xyz position of the hook, as well\nas the rotation of both objects. Since the achieved goals are\nderived from the state, they too are affected. Here we double\nthe episode length for a total of 100 frames.\n5) ComplexHook: This task again features a hook and\nan object that must be moved to a target location. There is no\nlonger noise added to the state. There is, however, signiﬁcant\nuncertainty of two different, structured kinds. We ﬁrst replace\nthe simple cube from previous tasks with complex objects that\nvary in mass, friction, and shape. We use 100 objects taken\nfrom previous work by Finn et al. [1]. The object meshes\nwere originally downloaded from thingiverse.com and include\nbowls, teddy bears, and small chairs among many other shapes.\nNo information about the object shape or physical parameters\nis included in the state. To accomplish this task robustly, a\npolicy must work across all possible objects.\nTo introduce a second source of structured uncertainty, we\nsimulate large “bumps” on the table. A bump is a rigid box that\nis ﬁxed to the table top. The width, length, height, position, and\ncount of the bumps are randomly selected. See Figures 1b and\n5 for two examples. Note crucially that no information about\nthe bumps are included in the state space. Thus the complete\nstate space and other task parameters remain unchanged.\n6) MBRLPusher: This ﬁnal task is taken directly from\n[6]. A 7-DOF robot arm (not Fetch, but a simpler model) is\npositioned in front of a table with a tall cylinder and a target\narea. The objective is to push the cylinder to the target. The\ncylinder position and initial arm velocity are randomized per\ntrial but the goal is ﬁxed. The state space includes:\n• The robot joint positions and velocities (14 dims)\n• The cylinder center of mass (3 dims)\n• The gripper center of mass (3 dims)\nfor a total of 20 dimensions. Goals are the same as in previous\ntasks. Rewards are weighted sums of three terms: negative L1\nnorm between the cylinder and the goal, negative L1 norm\nbetween the gripper and the cylinder, and negative L2 norm\nof the action, with weights (1.25, 0.1, 0.5) respectively. The\ntask horizon here is 150 frames.\nB. Initial Policies\nIn Residual Policy Learning (RPL), we begin with an\nenvironment and an initial policy π : S →A and we learn to\nimprove on that initial policy. The initial policies that we use\nfor our experiments are:\n1) DiscreteMPCPush: a model-predictive controller\nwith discrete actions and heuristics speciﬁc to Push.\n2) ReactivePush: a reactive policy designed to work\nperfectly in the original Push task.\n3) ReactivePickAndPlace: a reactive policy for the\nPickAndPlace task with miscalibrated gains.\n4) ReactiveHook: a reactive policy designed to work\nperfectly in the noiseless hook task (Figure 1a).\n5) CachedPETS: a model-predictive controller with a\nlearned transition model [6]. To make this controller fast,\nwe cache the output actions for 500 input states. Given\na new state, the ﬁnal CachedPETS controller ﬁnds the\nnearest state in the cache and outputs the corresponding\nstored action. The number 500 was selected based on a\nsmall performance analysis (see appendix).\nSee the appendix for details on all policies.\nC. Architectures and Training Details\nRPL is indifferent to the deep RL method applied or\narchitecture used. However, for consistency, we use the same\nactor-critic architecture with Deep Deterministic Policy Gra-\ndients [29] and Hindsight Experience Replay [5] across our\nexperiments. The network consists of 3 fully connected layers\nof 256 units each, with ReLU non-linearities (not on the output\nlayer). We use the same hyperparameters as in [32], given in\nthe appendix. Our only substantial modiﬁcation is to initialize\nthe last layer of the network to zeros, so that the policy starts\nwith the base controller (as described in section IV-A).\nWhen training in noisy environments, we use a history of\n1 (see Section IV-C). We considered two variants. In the ﬁrst\nvariant, the states are concatenated and fed to the network:\nfθ(s1, s2). In the second variant, we consider the average of\nthe features obtained for the states: 0.5 (fθ(s1) + fθ(s2)). In\npractice, we found the second variant to work better, and so\nuse it for all noisy environments.\nD. Baselines\nWe consider three baselines for all experiments. First, in all\nexperiments, we show the result of running the initial policy\nwithout learning. Second, we show the result of learning from\nscratch with DDPG and HER.\nOur third baseline is designed to disentangle the causes of\nRPL’s success. One hypothesis for why residual learning might\nbe helpful is that the initial policy provides a smart means for\nexploration. The baseline, “Expert Explore”, uses the initial\n(“expert”) policy for exploration only. Actions are selected by\nselecting z = rand(0, 1) and proceeding as follows:\na =\n\n\n\n\n\nπ(s) if z < ϵα\nrand() if z < ϵ(1 −α)\nfθ(s) if z > ϵ\nwhere ϵ and α are hyperparameters that we selected with a\nsmall grid search. Thus the agent acts (1 −ϵ)% of the time\naccording to the learned policy, (ϵ × α)% according to the\nexpert, and the rest of the time takes random actions. This\nbaseline is similar to a policy-reuse method [33].\nE. Results\nHere we present empirical and qualitative results for RPL\nacross the six complex manipulation tasks described in Section\nV-A. For each task, we show RPL’s superior data efﬁciency\nand performance compared to the three baselines described in\nSection V-D. All empirical results are presented with mean\nand standard deviation across ﬁve random seeds.\nFig. 3: RPL and baseline results for the Push, SlipperyPush, and PickAndPlace tasks. As described in the text, the\n“Initial” policies are DiscreteMPCPush, ReactivePush, and ReactivePickAndPlace respectively. In the ﬁrst two\ntasks, RPL converges to perfect performance in roughly the same number of simulation steps as learning from scratch with\nDDPG and HER but outperforms the baseline before convergence. In the third task, RPL converges with roughly 10x fewer\ntraining samples. In all cases, RPL substantially improves on the initial policies.\n1) DiscreteMPCPush in Push: In this experiment, we\nexamine whether RPL can overcome the limitations of an\nMPC controller that makes coarse approximations in an effort\nto trade performance for speed. In particular, we use the\nDiscreteMPCPush as our initial policy for the Push task.\nWe graph the success rates of RPL and the baselines in\nFigure 3a. The success rate of DiscreteMPCPush starts\naround 0.5. We noticed three common sources of suboptimal-\nity for this initial policy. First, the limited node expansions\nper MPC call, which is necessitated by the speed bottleneck of\nquerying the MPC’s model, means that a good action sequence\nis not always found. Second, the discreteness of the actions\nsometimes leads to circuitous executions in which the episode\nends before the object reaches the target. Third, the heuristic\nused to guide the MPC’s search, while very informative, can\nalso be misleading in rare cases. These failure modes are\nespecially common when the gripper must move from one side\nof the cube to the other, since the cube acts as an obstacle in\nthis context.\nWe conﬁrm the results reported in previous work [32] that\nlearning from scratch with DDPG and HER works well in\nthis domain, converging to a success rate of nearly 1.0 after\nroughly 2 million simulator steps. The performance of RPL\nbefore convergence greatly surpasses both the initial policy\nand learning from scratch, while still converging to a perfect\nsuccess rate. For example, RPL takes an order of magnitude\nfewer training samples to reach an average success rate of 0.9\nversus the learning from scratch baseline.\nNote that the performance of RPL drops early in training\nbefore quickly recovering and surpassing the baselines. We\nsee this pattern in the following experiments as well. This is a\nmanifestation of the issue discussed in Section IV-B whereby\nthe critic is initialized poorly with respect to the actor. We\nfound that decreasing the burn-in parameter β mitigated the\ndrop but did not signiﬁcantly affect the time to convergence.\nWe thus left the results as they are for the beneﬁt of discussion.\nTo analyze the source of RPL’s superior data efﬁciency,\nwe turn to the performance of the Expert Explore baseline.\nWe ﬁnd that this baseline also improves on learning from\nscratch, but that RPL converges slightly faster. This suggests\nthat RPL’s advantage in this Push task derives in large part\nfrom more efﬁcient exploration, but also from the residual\nparameterization and initialization.\n2) ReactivePush in SlipperyPush: Our second ex-\nperiment examines model misspeciﬁcation. We tuned the\nReactivePush policy to achieve near perfect performance\nin the original Push task. We now transfer this policy to the\nSlipperyPush task in which the sliding friction coefﬁcient\nof the cube is 5x smaller.\nThe success rates of RPL and the baselines on the\nSlipperyPush task are shown in Figure 3b. As expected,\nthe ReactivePush policy is not perfect, achieving a success\nrate of around 0.45. The most common failure mode of this\ninitial policy is when the gripper pushes the slippery cube\ntoo hard and the cube slides off the table. In other cases, the\ncube does not fall off, but is pushed back and forth across the\ngoal without converging. A representative trial is illustrated in\nFigure 2 (top row). As in the ﬁrst experiment, we ﬁnd that\nRPL is far better before convergence and converges to the\nsame perfect success rate as model-free learning from scratch.\n3) ReactivePickAndPlace in PickAndPlace: In\nthis experiment, we consider an example of a poorly calibrated\ninitial policy that leads to detrimental oscillatory behavior.\nSuch oscillations are a common issue in stateless robotic\ncontrol when gains are improperly tuned. To create a represen-\ntative scenario, we start with the ReactivePickAndPlace\npolicy and artiﬁcially increase the gains. Oscillations quickly\narise, e.g. when the gripper overshoots the waypoints implicit\nin the design of the policy. These oscillations cause the success\nrate of the ReactivePickAndPlace to drop to roughly\n0.5, as seen in Figure 3c.\nAs reported in previous work [32], learning from scratch\nwith DDPG and HER requires far more data to reach a success\nrate of 1.0 in PickAndPlace versus Push. Here we ﬁnd\nthe data efﬁciency of RPL to be substantially better. RPL\nconverges to a success rate of 1.0 after roughly 1 million\nsimulator steps, which represents a nearly 10x improvement\nover learning from scratch. Comparing with the Expert Ex-\nplore baseline, we ﬁnd that not all of the advantage can be\nexplained by improved exploration; the good parameterization\nand initialization of the policy is also to credit.\nIt was not a priori obvious that the initial policy would aid\nRPL here as much as it apparently does. By design, we know\nthat the policy is close in “gain space” to a near optimal one,\nbut that does not guarantee that the policy is similarly close in\n“residual weight space.” Fortunately, it seems the two notions\ncoincide here.\n4) ReactiveHook in NoisyHook: Now we turn to\nanother prevalent problem in robotic control — sensor noise\n— and investigate whether RPL can improve the robustness\nof a sensitive initial policy. As discussed in Section V-A,\nthe NoisyHook task features Gaussian noise applied to the\npositions and rotations of the block and hook. While the\nReactiveHook policy is nearly perfect in a noiseless ver-\nsion of the same task, the policy proves to be quite sensitive to\nthe sensor noise. We observe diverse failure modes throughout\nthe course of execution: the gripper often moves to a wrong\nposition, sometimes fails to pick up the hook, and other times\ndrops the hook. As shown in Figure 4a, the success rate of the\ninitial policy is roughly 0.15, far lower than in our previous\nexperiments.\nIn this experiment, we make use of the two frame policy\narchitecture described in Section IV-C to cope with sensor\nnoise. We use the same architecture for all three learning\nmethods for comparison.\nLearning from scratch with DDPG and HER fails in this\ntask, never achieving a nontrivial success rate. This failure is\nnot surprising given the long horizon and sparse rewards in the\ntask. The Expert Explore baseline also performs quite poorly,\nonly beginning to reach nontrivial success rates after 5 million\nsimulator steps. We speculate that this failure is due to the fact\nthat the hook is so often dropped by the initial policy.\nIn contrast, we see that RPL quickly converges to a success\nrate of roughly 0.8. This represents the ﬁrst instance of RPL\nobtaining strong performance in a task that is both out of reach\nfor current deep RL methods and nontrivial for robotic control\nalone. Moreover, the results suggest that RPL is a promising\nmethod for overcoming the common challenge of sensor noise.\n5) ReactiveHook in ComplexHook: In this experi-\nment, we study structured uncertainty inspired by the common\nmismatch between physics simulators and real robotics tasks.\nAs described in Section V-A, the ComplexHook task con-\ntains two challenging innovations over the noiseless hook task:\nbumps are randomly scattered across the table surface; and the\nobject takes on a variety of shapes, masses, and coefﬁcients\nof friction. We observed that each of these two innovations\nindependently cause the ReactiveHook policy performance\nto drop by roughly 20%. With both changes present, the initial\npolicy success rate drops to 0.55, as shown in Figure 4b.\nA random or null policy is occasionally successful in this\ntask due to the scene randomization. With this in mind, we\nsee that learning from scratch with DDPG and HER does\nnot obtain any nontrivial success rate, as in the previous\nexperiment. We again ﬁnd that the policy never causes the\ngripper to touch the hook, let alone move it to reach the object.\nInterestingly, the Expert Explore baseline does achieve\na nontrivial success rate, eventually slightly surpassing the\nsuccess rate of the initial policy. This task is easier than\nNoisyHook from the perspective of the expert baseline if\nonly because the initial success rate is much higher.\nFinally, RPL learns a robust policy with strong data ef-\nﬁciency, converging at a success rate just below 0.8. The\nfact that RPL is able to achieve this success rate is fairly\nremarkable given the diversity in the objects and obstacles,\nand the fact that the state contains no information about this\ndiversity. RPL has apparently learned a “conformant” policy\nthat works for most objects and obstacles without discretion.\nWe show one intriguing example of RPL succeeding where\nthe initial policy fails in Figure 5.\n6) CachedPETS in MBRLPusher: In this ﬁnal experi-\nment, we examine whether RPL can improve on a model-based\nRL method while converging faster than model-free RL. As\ndescribed in Section V-A, to derive the initial controller, we\nbegin by learning a transition model. Following Chua et al.\n[6], we train for 15000 simulator steps in their MBRLPusher\nenvironment. We then query the environment for an additional\n500 steps to construct our CachedPETS controller. (We thus\noffset our plotted results by 15500 relative to the baseline in\nFigure 4c.) The performance of PETS, averaged over 10 trials,\nis plotted as a dashed line in Figure 4c. We see that the drop in\nperformance due to the caching approximation is fairly small.\nWe ﬁnd that RPL improves substantially not only on\nthe initial CachedPETS controller, but also on the original\nPETS controller. Furthermore, RPL converges faster than\nDDPG+HER, indicating that the initial controller was beneﬁ-\ncial. It is worth emphasizing that no domain knowledge was\nused to design the initial policy here; this same combination\nof MBRL and RPL could be applied immediately to a new\ndomain. These results suggest that RPL may be seen as a\ngeneral RL method that marries the data efﬁciency of MBRL\nwith the superior asymptotic performance of model-free RL.\nVI. DISCUSSION AND CONCLUSION\nWe have described Residual Policy Learning (RPL), a\nsimple method that combines the strengths of deep RL and\nrobotic control. Our experimental results suggest that RPL is\na powerful approach to deal with pervasive issues in complex\nmanipulation tasks such as sensor noise, model misspeci-\nﬁcation, controller miscalibration, and partial observability.\nWe ﬁnd that RPL consistently improves on initial policies\nand achieves better data efﬁciency than model-free RL alone.\nFurthermore, RPL can improve on initial policies for long-\nhorizon, sparse-reward problems where model-free RL fails.\nWe have also seen the promise of combining RPL with\nmodel-based RL [6]. MBRL is often more data-efﬁcient\nwhereas model-free RL can be faster to run and asymptotically\nsuperior. RPL offers a simple mechanism for combining the\nstrengths of both. We ﬁnd empirically that learning a residual\nFig. 4: RPL and baseline results for the NoisyPush, ComplexPush and MBRLPusher tasks. For the ﬁrst two tasks, the\n“Initial” policy is ReactiveHook; for the third, it is CachedPETS. In the ﬁrst task, RPL quickly and substantially improves\non the initial policy while the other two learning methods fail. In the second task, RPL again improves on the initial policy.\nSee also Figures 1b and 5 for illustrations of this task. In the third, RPL improves on the initial policy, converges faster than\nDDPG+HER, and outperforms the model-based reinforcement learning method PETS (“MBRL”). The dashed line marks the\naverage performance of PETS over 10 trials.\n.\nFig. 5: Illustration of the original policy and RPL on the ComplexHook task. The original policy was designed to work when\nthe object is a simple cube and the table has no obstacles (see Figure 1a). The same policy pushes a larger complex object off\nthe table rather than to the target (red sphere) as required (top row). RPL, our proposed method, learns to improve the policy\nthat pulls the object to the target (bottom row). The learned policy exhibits interesting behavior that qualitatively resembles\nlifting the hook to avoid obstacles and reaching around at a wider angle than originally programmed.\non top of MBRL improves on MBRL alone, converging to the\nsame performance as model-free RL with less data.\nWe postulate three main causes for the success of RPL.\nFirst, as described in Section IV, we take care to initialize the\nresidual policy so that its output at ﬁrst matches the initial\npolicy. When the initial policy is strong, this initialization\ngives RPL a clear boost. The second cause of RPL’s success\nis improved exploration early on during training. In learning\nfrom scratch with sparse rewards and long horizons, the ﬁrst\nsuccessful trajectory must be discovered by chance. Hindsight\nExperience Replay is designed to face this challenge, but\nRPL offers a more direct solution. RPL can discover suc-\ncessful trajectories immediately if the initial policy produces\nthem with nontrivial frequency. To measure the impact of\nthis exploration advantage, we introduced the Expert Explore\nbaseline described in Section V-D. Empirically we ﬁnd this\nbaseline performance to lie midway between RPL and learning\nfrom scratch. A third likely cause of RPL’s success is that\nthe residual reinforcement learning problem induced by the\ninitial policy may be easier than the original problem. This\ncause may best explain the superior performance of RPL in\nthe NoisyHook task, where both the initial policy and the\n“Expert Explore” baseline are empirically poor.\nThough the six case studies we have presented all involve\nrobotic manipulation with DDPG and HER, RPL is far more\ngeneral than any speciﬁc task domain or deep RL method. The\nmethod we have described can be immediately applied in any\ndomain with continuous actions and with any gradient-based\nlearning method. However, RPL is especially well suited for\ncomplex manipulation because of the availability of good but\nimperfect initial policies and the long-horizon, sparse-reward\ntasks that naturally arise.\nIn recent years, complex manipulation problems have been\nat the forefront of research in robotics and deep RL. Both\nﬁelds have made signiﬁcant strides in often complementary\ndirections. RPL should be viewed as one piece of a larger\neffort to combine the strengths of both approaches. We con-\njecture that solving the hardest open problems in manipulation\nwill require such a synthesis.\nACKNOWLEDGMENTS\nWe thank Evan Shelhamer for helpful discussions. We\ngratefully acknowledge support from NSF grants 1523767 and\n1723381; from ONR grant N00014-13-1-0333; from AFOSR\ngrant FA9550-17-1-0165; from ONR grant N00014-18-1-\n2847; from Honda Research; and from the Center for Brains,\nMinds and Machines (CBMM), funded by NSF STC award\nCCF-1231216. KA acknowledges support from NSERC. Any\nopinions, ﬁndings, and conclusions or recommendations ex-\npressed in this material are those of the authors and do not\nnecessarily reﬂect the views of our sponsors.\nREFERENCES\n[1] Chelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel,\nand Sergey Levine. One-shot visual imitation learning\nvia meta-learning.\nIn Conference on Robot Learning,\npages 357–368, 2017.\n[2] Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian\nIbarz, Alexander Herzog, Eric Jang, Deirdre Quillen,\nEthan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke,\net al. Scalable deep reinforcement learning for vision-\nbased robotic manipulation.\nIn Conference on Robot\nLearning, pages 651–673, 2018.\n[3] Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey\nLevine. Deep reinforcement learning for robotic manip-\nulation with asynchronous off-policy updates. In IEEE\nInternational Conference on Robotics and Automation\n(ICRA), pages 3389–3396. IEEE, 2017.\n[4] Andy Zeng, Shuran Song, Stefan Welker, Johnny Lee,\nAlberto Rodriguez, and Thomas Funkhouser.\nLearn-\ning synergies between pushing and grasping with self-\nsupervised deep reinforcement learning. arXiv preprint\narXiv:1803.09956, 2018.\n[5] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas\nSchneider, Rachel Fong, Peter Welinder, Bob Mc-\nGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech\nZaremba.\nHindsight experience replay.\nIn Advances\nin Neural Information Processing Systems, pages 5048–\n5058, 2017.\n[6] Kurtland Chua, Roberto Calandra, Rowan McAllister,\nand Sergey Levine.\nDeep reinforcement learning in a\nhandful of trials using probabilistic dynamics models.\narXiv preprint arXiv:1805.12114, 2018.\n[7] Richard S Sutton. Integrated architectures for learning,\nplanning, and reacting based on approximating dynamic\nprogramming. In Machine Learning Proceedings 1990,\npages 216–224. Elsevier, 1990.\n[8] Marc Peter Deisenroth, Dieter Fox, and Carl Edward Ras-\nmussen. Gaussian processes for data-efﬁcient learning\nin robotics and control. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 37(2):408–423, 2015.\n[9] Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and\nSergey Levine. Continuous deep Q-learning with model-\nbased acceleration. In International Conference on Ma-\nchine Learning, pages 2829–2838, 2016.\n[10] Nicolas Heess, Gregory Wayne, David Silver, Timothy\nLillicrap, Tom Erez, and Yuval Tassa.\nLearning con-\ntinuous control policies by stochastic value gradients.\nIn Advances in Neural Information Processing Systems,\npages 2944–2952, 2015.\n[11] Derrick H Nguyen and Bernard Widrow. Neural networks\nfor self-learning control systems. IEEE Control Systems\nMagazine, 10(3):18–23, 1990.\n[12] Igor Mordatch, Nikhil Mishra, Clemens Eppner, and\nPieter Abbeel.\nCombining model-based policy search\nwith online model learning for control of physical hu-\nmanoids. In IEEE International Conference on Robotics\nand Automation (ICRA), pages 242–248. IEEE, 2016.\n[13] Anusha Nagabandi, Gregory Kahn, Ronald S Fearing,\nand Sergey Levine. Neural network dynamics for model-\nbased deep reinforcement learning with model-free ﬁne-\ntuning. In IEEE International Conference on Robotics\nand Automation (ICRA), pages 7559–7566. IEEE, 2018.\n[14] Vitchyr Pong, Shixiang Gu, Murtaza Dalal, and Sergey\nLevine.\nTemporal difference models: Model-free deep\nrl for model-based control. International Conference on\nLearning Representations, 2018.\n[15] St´ephane Ross, Geoffrey Gordon, and Drew Bagnell. A\nreduction of imitation learning and structured prediction\nto no-regret online learning. In Proceedings of the Four-\nteenth International Conference on Artiﬁcial Intelligence\nand Statistics, pages 627–635, 2011.\n[16] Pieter Abbeel and Andrew Y Ng. Apprenticeship learn-\ning via inverse reinforcement learning. In Proceedings\nof the Twenty-ﬁrst International Conference on Machine\nLearning, page 1. ACM, 2004.\n[17] Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and\nAnind K Dey. Maximum entropy inverse reinforcement\nlearning. In AAAI Conference on Artiﬁcial Intelligence,\nvolume 8, pages 1433–1438. Chicago, IL, USA, 2008.\n[18] Zi Wang, Caelan Reed Garrett, Leslie Pack Kaelbling,\nand Tom´as Lozano-P´erez.\nActive model learning and\ndiverse action sampling for task and motion planning.\nIEEE/RSJ International Conference on Intelligent Robots\nand Systems (IROS), 2018.\n[19] Alonso Marco, Felix Berkenkamp, Philipp Hennig, An-\ngela P Schoellig, Andreas Krause, Stefan Schaal, and Se-\nbastian Trimpe. Virtual vs. real: Trading off simulations\nand physical experiments in reinforcement learning with\nbayesian optimization. In IEEE International Conference\non Robotics and Automation (ICRA), pages 1557–1563.\nIEEE, 2017.\n[20] Daniel J Lizotte, Tao Wang, Michael H Bowling, and\nDale Schuurmans.\nAutomatic gait optimization with\nGaussian process regression.\nIn International Joint\nConference on Artiﬁcial Intelligence (IJCAI), pages 944–\n949, 2007.\n[21] Matthew Tesch, Jeff Schneider, and Howie Choset. Using\nresponse surfaces and expected improvement to optimize\nsnake robot gait parameters. In IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS),\npages 1069–1074. IEEE, 2011.\n[22] Alonso Marco, Philipp Hennig, Jeannette Bohg, Stefan\nSchaal, and Sebastian Trimpe. Automatic LQR tuning\nbased on Gaussian process global optimization. In IEEE\nInternational Conference on Robotics and Automation\n(ICRA), pages 270–277. IEEE, 2016.\n[23] Eric A Wan and Rudolph Van Der Merwe. The unscented\nkalman ﬁlter for nonlinear estimation.\nIn Adaptive\nSystems for Signal Processing, Communications, and\nControl Symposium, pages 153–158. Ieee, 2000.\n[24] Filipe de Avila Belbute-Peres, Kevin Smith, Kelsey\nAllen, Josh Tenenbaum, and J Zico Kolter.\nEnd-to-\nend differentiable physics for learning and control. In\nAdvances in Neural Information Processing Systems,\npages 7176–7187, 2018.\n[25] Svetoslav Kolev and Emanuel Todorov.\nPhysically\nconsistent state estimation and system identiﬁcation for\ncontacts.\nIn IEEE-RAS 15th International Conference\non Humanoid Robots (Humanoids), pages 1036–1043.\nIEEE, 2015.\n[26] Anurag Ajay, Jiajun Wu, Nima Fazeli, Maria Bauza,\nLeslie P. Kaelbling, Joshua B. Tenenbaum, and Al-\nberto Rodriguez. Augmenting physical simulators with\nstochastic neural networks: Case study of planar pushing\nand bouncing. arXiv preprint arXiv:1808.03246, 2018.\n[27] Alina Kloss, Stefan Schaal, and Jeannette Bohg. Com-\nbining learned and analytical models for predicting action\neffects. arXiv preprint arXiv:1710.04102, 2017.\n[28] Tobias Johannink, Shikhar Bahl, Ashvin Nair, Jianlan\nLuo, Avinash Kumar, Matthias Loskyll, Juan Aparicio\nOjea, Eugen Solowjow, and Sergey Levine.\nResidual\nreinforcement learning for robot control. arXiv preprint\narxIV:1812.03201, 2018.\n[29] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel,\nNicolas Heess, Tom Erez, Yuval Tassa, David Silver, and\nDaan Wierstra. Continuous control with deep reinforce-\nment learning. 2016.\n[30] Steven Kapturowski, Georg Ostrovski, Will Dabney, John\nQuan, and Remi Munos.\nRecurrent experience replay\nin distributed reinforcement learning.\nIn International\nConference on Learning Representations, 2019.\nURL\nhttps://openreview.net/forum?id=r1lyTjAqYX.\n[31] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco:\nA physics engine for model-based control. In IEEE/RSJ\nInternational Conference on Intelligent Robots and Sys-\ntems (IROS), pages 5026–5033. IEEE, 2012.\n[32] Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob\nMcGrew, Bowen Baker, Glenn Powell, Jonas Schnei-\nder, Josh Tobin, Maciek Chociej, Peter Welinder, et al.\nMulti-goal reinforcement learning: Challenging robotics\nenvironments and request for research.\narXiv preprint\narXiv:1802.09464, 2018.\n[33] Fernando Fern´andez and Manuela Veloso. Probabilistic\npolicy reuse in a reinforcement learning agent.\nIn\nProceedings of the Fifth International Joint Conference\non Autonomous Agents and Multiagent Systems, pages\n720–727. ACM, 2006.\n[34] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980,\n2014.\nVII. APPENDIX\nA. Initial Policies\nHere we describe in detail the initial policies used for our\nexperiments.\n1) DiscreteMPCPush: Suppose we have a learned or\nknown transition model Pr(s′|s, a) that can be queried to\npredict the state trajectories and rewards that may result from\na sequence of actions taken from an initial state. In Model-\nPredictive Control (MPC), we use this transition model to\nselect each action taken by the policy π. More speciﬁcally,\ngiven the current state st, an MPC policy will internally\nconsider multiple sequences of actions at, at+1, ..., ah and\ncompute the expected rewards accrued for each sequence. The\nﬁrst action in the best sequence is then the output of π(st). To\ndesign an MPC policy, we must therefore specify the model\nand a procedure for selecting possible action sequences.\nIn high-dimensional tasks with long horizons, sparse re-\nwards, and continuous states and actions, MPC is intractable\nwithout an efﬁcient mechanism for selecting action sequences.\nHere we opt to discretize the action space as a means to\nsimplify the search. In particular, rather than consider the\ninﬁnite number of possible gripper movements, we consider\nonly six, one per cardinal direction. We can then use a discrete\ngraph search to explore possible action sequences.\nWe develop a discrete MPC policy for the Push task.\nThe model is a perfect copy of the environment (i.e. a\nseparate instance of MuJoCo). We further improve the policy\nby introducing an informative heuristic to guide the discrete\nsearch. The heuristic is a tuple (d1, d2) where d1 is the distance\nbetween the object and the target location and d2 is the\ndistance between the gripper and the “push location.” The push\nlocation is meant to be the desired position of the gripper for\npushing the block to the target; it is approximated by extending\nthe vector difference between the object and target location\nby a small amount corresponding to the radius of a sphere\ncircumscribed around the object. The second entry d2 of the\nheuristic is only used to break ties when the ﬁrst entry d1\nmatches. We use this heuristic to perform a best-ﬁrst search\nwith 10 node expansions per environment step. At the end of\nthe search, we ﬁnd the node with the best heuristic value and\ntake the corresponding ﬁrst action. (If the root has the best\nheuristic value, we take a noop action.)\n2) ReactivePush: Our second policy is designed for\npushing an object to a target location. While this policy works\nnearly perfectly in the original Push task, its performance\ndrops dramatically when the sliding friction on the block is\nreduced as in the SlipperyPush task. Given an input state,\nthe policy checks the following conditional statements in order\nuntil one holds and proceeds accordingly.\n1) If the object is already at the target location, do nothing.\n2) If the block is between the gripper and the target\nlocation, move the gripper towards the target location.\n3) If the gripper is above the push location (see deﬁnition\nin DiscreteMPCPush), move the gripper down to\nprepare to push.\n4) Move the gripper to above the push location.\nTo determine whether the object or gripper is “at” a location,\nwe measure the distance and check if it is below a global\nthreshold. The other key hyperparameter is a gain that de-\ntermines how far the gripper moves at each time step. We\nmanually tuned this gain to achieve near optimal performance\non the original Push task.\n3) ReactivePickAndPlace: Our third policy is de-\nsigned to pick up a cube and bring it to a target location on\nor above the table. Given an input state, the policy checks the\nfollowing conditional statements in order until one holds and\nproceeds accordingly.\n1) If the object is already at the target location, do nothing.\n2) If the gripper is grasping the object, move towards the\ntarget location.\n3) If the object is between the gripper ﬁngers (but not\ngrasped), close the gripper.\n4) If the gripper is above the object\n5)\n... and the gripper is closed, open the gripper.\n6)\n... and the gripper is open, move the gripper down.\n7) Move the gripper towards the location above the object.\nTo determine whether the gripper is grasping the object, we\ncheck that the object location is between the two ﬁngers and\nthat the ﬁngers are not more than the block width apart. We\nagain use the distance threshold and gain hyperparameters\ndescribed above.\n4) ReactiveHook: Our fourth policy is designed to pick\nup a hook, move it behind and to the right of an object, and\npush and pull the object towards a target location. The policy\nworks nearly perfectly when the object is a cube, the table is\nclear of obstacles, and the observations are noise-free (see Fig-\nure1a). However, the policy performance drops substantially\nwhen transferred to the NoisyHook and ComplexHook\ntasks. Given an input state, the policy checks the following\nconditional statements in order until one holds and proceeds\naccordingly.\n1) If the object is already at the target location, do nothing.\n2) If the hook is not grasped and lifted above the table,\ngrasp and lift the hook.\n3) If the hook is not beyond and to the right of the object,\nmove forward or rightward accordingly.\n4) Move the gripper following the vector difference be-\ntween the object and the target location.\nThe grasp position is ﬁxed so that the robot always attempts\nto pick up the same part of the hook (near the bottom). In\naddition to the global threshold and gain hyperparameters,\nwe use knowledge of the length and width of the hook to\ndetermine gripper movements as a function of desired hook\nmovements.\n5) CachedPETS: Our ﬁnal policy uses a model-predictive\ncontroller (MPC) with a learned transition model. We take\nthe recently proposed Probabilistic Ensembles with Trajectory\nSampling (PETS) as our method for model-based reinforce-\nment learning [6]. PETS learns a transition model in the\nform of an ensemble of probabilistic neural networks. During\nplanning, a sequence of actions is sampled with reference\nto previously high-reward action sequences using the cross-\nentropy method. To predict the subsequent states and rewards\nusing the learned transition model, a ﬁnite collection of\nparticles are propagated forward in time. The action that leads\nto the highest expected reward is selected, and planning repeats\nafter each environment step. We use the PETS implementation\nmade available by Chua et al. [6] without modiﬁcation.\nMPC methods are generally much slower than model-free\ncounterparts. Indeed, we found PETS alone to be intractably\nslow as an initial policy for RPL. We therefore create a\n“cached” version that stores the action produced by PETS for\n500 input states. The number 500 was selected based on a\nsmall performance analysis (see later in the appendix). We\nselect these 500 input states by sampling trajectories from the\nenvironment on-policy. At test time, given a new state, we\nﬁnd the nearest state in the cache (as measured by Euclidean\ndistance) and take the corresponding action. Though quite\nsimple and coarse as an approximation of the full MPC, the\nﬁnal CachedPETS controller performs only slightly worse\nthan the original PETS (see Figure 4c) with a 2-3 order of\nmagnitude speed up.\nB. Model Hyperparameters\nAll experiments in this paper use the following hyperpa-\nrameters, which are taken from [32].\n• Actor and critic networks: 3 layers with 256 units each\nand ReLU non-linearities\n• Adam optimizer [34] with 1·10−3 for training both actor\nand critic\n• Buffer size: 106 transitions\n• Polyak-averaging coefﬁcient: 0.95\n• Action L2 norm coefﬁcient: 1.0\n• Observation clipping: [−200, 200]\n• Batch size: 256\n• Cycles per epoch: 50\n• Batches per cycle: 40\n• Test rollouts per epoch: 50\n• Probability of random actions: 0.3\n• Scale of additive Gaussian noise: 0.2 (0.1 for hooks)\nFig. 6: To select the size of the cache used for the\nCachedPETS\ncontroller,\nwe\nplot\nperformance\non\nthe\nMBRLPusher task for sizes ranging from 1 to 1000. We run\n25 trials for each size and plot the mean and standard deviation\nhere. Based on this analysis, we selected a database of size\n500.\n• Probability of HER experience replay: 0.8\n• Normalized clipping: [−5, 5]\nFor the Push and PickAndPlace experiments, we use 19\nMPI workers with a rollout batch size of 2 to match the\nprevious work. For the Hook experiments, we use 1 MPI\nworker and a rollout batch size of 4 to save on compute\nresources. We determined the ”Expert Explore” baseline hy-\nperparameters ϵ = 0.6 and α = 0.8 with a small grid search.\nFor all experiments, we used a burn-in threshold of β = 1.0.\nWe did not optimize this hyperparameter and believe RPL’s\nperformance could be further improved in doing so.\n",
  "categories": [
    "cs.RO",
    "cs.LG"
  ],
  "published": "2018-12-15",
  "updated": "2019-01-03"
}