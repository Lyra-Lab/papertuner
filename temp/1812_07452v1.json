{
  "id": "http://arxiv.org/abs/1812.07452v1",
  "title": "Domain Adaptation for Reinforcement Learning on the Atari",
  "authors": [
    "Thomas Carr",
    "Maria Chli",
    "George Vogiatzis"
  ],
  "abstract": "Deep reinforcement learning agents have recently been successful across a\nvariety of discrete and continuous control tasks; however, they can be slow to\ntrain and require a large number of interactions with the environment to learn\na suitable policy. This is borne out by the fact that a reinforcement learning\nagent has no prior knowledge of the world, no pre-existing data to depend on\nand so must devote considerable time to exploration. Transfer learning can\nalleviate some of the problems by leveraging learning done on some source task\nto help learning on some target task. Our work presents an algorithm for\ninitialising the hidden feature representation of the target task. We propose a\ndomain adaptation method to transfer state representations and demonstrate\ntransfer across domains, tasks and action spaces. We utilise adversarial domain\nadaptation ideas combined with an adversarial autoencoder architecture. We\nalign our new policies' representation space with a pre-trained source policy,\ntaking target task data generated from a random policy. We demonstrate that\nthis initialisation step provides significant improvement when learning a new\nreinforcement learning task, which highlights the wide applicability of\nadversarial adaptation methods; even as the task and label/action space also\nchanges.",
  "text": "DOMAIN ADAPTATION FOR REINFORCEMENT LEARNING ON\nTHE ATARI\nThomas Carr\nAston University\nBirmingham, United Kingdom\ncarrtp@aston.ac.uk\nMaria Chli\nAston University\nBirmingham, United Kingdom\nm.chli@aston.ac.uk\nGeorge Vogiatzis\nAston University\nBirmingham, United Kingdom\ng.vogiatzis@aston.ac.uk\nABSTRACT\nDeep reinforcement learning agents have recently been successful across a variety of discrete and\ncontinuous control tasks; however, they can be slow to train and require a large number of interactions\nwith the environment to learn a suitable policy. This is borne out by the fact that a reinforcement\nlearning agent has no prior knowledge of the world, no pre-existing data to depend on and so must\ndevote considerable time to exploration. Transfer learning can alleviate some of the problems by\nleveraging learning done on some source task to help learning on some target task. Our work presents\nan algorithm for initialising the hidden feature representation of the target task. We propose a domain\nadaptation method to transfer state representations and demonstrate transfer across domains, tasks\nand action spaces. We utilise adversarial domain adaptation ideas combined with an adversarial\nautoencoder architecture. We align our new policies’ representation space with a pre-trained source\npolicy, taking target task data generated from a random policy. We demonstrate that this initialisation\nstep provides signiﬁcant improvement when learning a new reinforcement learning task, which\nhighlights the wide applicability of adversarial adaptation methods; even as the task and label/action\nspace also changes.\nKeywords Deep Learning · Reinforcement Learning · Domain Adaptation;\n1\nIntroduction\nDeep Reinforcement Learning (DRL) is a successful paradigm for learning sophisticated policies through interacting\nwith a complex environment. DRL allows for end-to-end learning from images or raw sensor data. RL agents generally\nstart learning tabula rasa with randomly parameterised neural networks and this contributes to the problem of these\nalgorithms requiring large numbers of interactions with the environment to learn a suitable policy. In this context,\nTransfer Learning is a viable route to reducing the sample complexity of these algorithms. Many problems share similar\nfeatures; an algorithm which enables an agent to identify these features quickly will, through transfer, beneﬁt over an\nalgorithm which has to learn these representations from scratch.\nOne way to view learning within DRL is to consider the hidden layers as learning a state representation on top of which\na policy can be learned. This view has held true in the standard deep learning paradigm and the learned features are\noften re-purposed through direct transfer with or without ﬁne-tuning for a subsequent task [1]. This type of approach\ncan also work under a domain shift, however direct transfer with relatively small domain shifts can be quite detrimental\nto performance [2]. When the tasks to be learned are split over different input domains, the representation to be learned\nshould be capable of describing both domains, i.e. the representation should be shared. This representation within RL\ncan be considered the state space. Domain Adaptation methods in this context are about learning to construct that state\nspace for corresponding inputs in the target domain. This, however, raises the question of how to align observations\nfrom the two domains.\nOur method uses adversarial regularization as a mechanism to impose the source task’s embedding structure on our target\nproblem encoding space in an unsupervised manner. If the tasks are related we expect that similar states will require\nsimilar embeddings and so we regularise our target state representation towards that of our source task. Adversarial\narXiv:1812.07452v1  [cs.LG]  18 Dec 2018\nFigure 1: Knowledge transfer in ATARI. Transferring skills from Pong (left) to Breakout (middle) and Tennis (right)\nis beyond current state of the art due to non-aligned environments and actions/rewards.\nAuto-Encoders (AAE) [3], which combine an auto-encoder (AE) with a generative adversarial network (GAN) [4],\nprovide a method to impose this regularisation in an unsupervised manner, allowing us to transfer knowledge from a\nsource to target domain.\nOur approach imposes this regularization at a distribution level and as such frees itself from the problem of aligning the\ndata points that should share a common representation by passing this problem to the dynamics of the discriminator-\ngenerator learning problem. Our architecture is depicted in Figure 2 and shows how a target task encoder is trained to\nboth reconstruct its input and lead a discriminator network into believing it was sampled from the source task encoder.\nWe show how this approach can be successfully applied to the reinforcement learning problem across very disparate\ndomains to provide an early performance boost over random initialization of the neural network.\n2\nBackground\nHere we present the concepts and review recent work in Reinforcement Learning, Domain Adaptation and Adversarial\nAutoEncoders - which are the main building blocks of our approach.\n2.1\nRL\nRL is a machine learning paradigm centred on learning how to act in an environment. The problem is modelled as\nsolving a Markov Decision Process (MDP), deﬁned by the tuple: (S, A, T, R, γ) where S is a set of states, A is a set of\nactions, T is the transition function T : S × A →S, R is the reward function R : S × A × S →R with discount factor\nγ with 0 ≤γ ≤1. The aim is to learn a policy π : S →A that maximizes the discounted expected reward where the\ndiscounted expected reward is :\nEπ[ΣT\nt=0γtR(st, at)].\n(1)\nIn our work we make use of the A2C [5, 6] algorithm. A2C is a variant of the A3C [7] algorithm that performs\nsynchronous updates. A2C is an actor-critic algorithm so a state value function and action distribution are learned\njointly; with the value function critiquing the policy. Actor-Critic algorithms are versatile algorithms, their relationship\nto generative adversarial networks has been explored by [8] and recent work has applied them to the problem of\ndistributed multi-task learning [9].\n2.2\nAutoEncoding\nAuto-Encoders (AE) are a powerful unsupervised representation learning tool. There are many variants, however\nthe vanilla AE works by learning to reproduce its input, usually by passing through a lower-dimensional, bottleneck,\nrepresentation. This can be done with a straightforward squared error loss function\nL(X) = 1\nN\nN\nX\ni=1\n(x −AE(x))2.\n(2)\nAEs have been used successfully in a number of RL algorithms as part of multi-step training pipelines and end-to-end\nparadigms. The most obvious use of the AE is to learn a low-dimensional state representation [10, 11, 12, 13]. Many\nRL algorithms are designed for low-dimensional state spaces and AEs provide a powerful unsupervised method to learn\nsuch low-dimensional representations, rather than building hand-crafted features. On the other hand, AEs also serve as\na method for initialising the weights of the RL network, in the convolutional case by learning complex ﬁlters which\nmay be hard to learn via the reward function.\n2\nVanilla AE embedding spaces are not perfect, as there can be undeﬁned regions of embedding space that lead to poor\nregenerations and non-smooth transitions between regeneration targets. This problem can be alleviated through the\nimposition of a prior, such as a Gaussian, on the embedding space [14].\n2.3\nGenerative Adversarial Networks\nGenerative Adversarial Networks (GANs) are a powerful method for learning a generative model of a given domain.\nThey work by pitting two networks against each-other. The Generator (G) is seeking to learn a generative model and\nthe Discriminator (D) learns to distinguish between samples from the Generator’s distribution and the True distribution.\nThis is shown in the loss function:\nmin\nG max\nD V (D, G) =\nE\nx∼Pdata(x)[log D(x)] +\nE\nz∼Pz(z)[1 −log(D(G(z)))].\n(3)\nwhere D(x) is the Discriminator’s classiﬁcation of real samples and D(G(z)) is its classiﬁcation of fake samples\ngenerated by G(z) where x is a sample from the true distribution and z is a sample from some other distribution.\nMany extentions to the GAN framework have been proposed. In particular, for this work we utilize the Wasserstein\nGAN (WGAN) [15]. The WGAN minimizes the earth-mover distance and the discriminator becomes a critic predicting\ncontinuous values for the samples rather than classifying them as real or fake. The discriminator loss to achieve this is\ngiven by:\nmin\nG max\nD V (D, G) =\nE\nx∼Pdata(x)[D(x)] −\nE\nz∼Pz(z)[D(G(z))].\n(4)\n2.4\nAdversarial AutoEncoders\nCombining the AE and the GAN gives us the Adversarial Autoencoder (AAE) [3]. This model treats the encoder of the\nAE as the generator of the GAN framework; its encodings are trained to mimic the imposed prior distribution and to\nencode the essential domain information. This is done by passing the encoding through both the GAN discriminator\nand the AE decoder. The network is trained by the two loss functions of the AE and GAN, by interleaving steps of both\noptimizations. In this way the GAN can be seen as regularizing the encoding of the AE.\n2.5\nDomain Adaptation\nDomain Adaptation is a transfer learning approach that seeks to align knowledge gained on a supervised source task\nwith an unlabelled (or limited availability of labels) target dataset from a different domain. These datasets usually share\nprediction labels so it is only the representation of the information that has changed.\nMany methods seek to align the representation vector of the source and target data. This can be done in a supervised\nor unsupervised manner, depending on labelling assumptions, however target data is usually limited if it does exist.\nAligning representations can be done in many ways; perhaps the simplest is to impose a constraint between the predicted\nrepresentation of source and target data.\nThis alignment is often pursued through regularization of the embedding space as in [16, 17]. These approaches show\nmuch promise, however, aligning samples from source and target must be considered. There are various approaches to\nthis, from using supervised data [18], to assumed correspondences [16], to unsupervised approaches [19, 17].\n3\nArchitecture and Method\nMany RL methods require vast numbers of samples to learn an acceptable policy and this is caused, in part, by\nreliance on a reward function, which may be sparse, and by the need to explore the state space characterized by the\nexploration-exploitation dilemma. Alleviating this will require many innovations; extracting as much information from\nthe sampled data as possible is crucial. Doing so in an unsupervised fashion will also be necessary as we do not have\nlabels for samples gathered as the agent acts. Another issue is that solving the problem from scratch is not a natural\napproach; for humans problems are often solved by drawing on knowledge already learned. The question for RL agents\nthen is how can an algorithm, in an unsupervised fashion, identify similarities between tasks and use that knowledge to\nenhance learning of subsequent problems.\n3\nAction\nSource Task\nTarget task\nEncoder\nDecoder\nEncoder\nDiscriminator\nPolicy\nEmbeddings\nFigure 2: Our agent’s architecture for domain adaptation, combining adversarially discriminative domain adaptation\nand adversarial autoencoder architectures. A fully trained source task is depicted on top, while on the bottom is the\ntarget task autoencoder.\nOur method addresses this question by initialising the state representation of our target task to look similar to our source\ntask. We view the hidden representation layer of our network, that is passed to the Policy layers and Value Layers, as\nthe state representation that contains the information needed to learn how to interact with the world. With this view\nwe also make the observation that related game domains should require similar state features; games with a ball may\nfor instance have a representation which contains the balls position and velocity. Such state information is clearly\nsomething which could be shared across many input domains.\nDomain adaptation is one way to address the problem of knowledge transfer. In this approach we already have a model\nwhich is capable of solving a source domain and we want to adapt that knowledge so that it is applicable to a target\ndomain. Consider for example digit classiﬁcation and the problem of solving MNIST [20] having already solved SVHN\n[21]. We should be able to leverage the SVHN model to solve MNIST without extra labeled data. The Adversarial\nDiscriminative Domain Adaptation (ADDA) [17] approach addresses this issue by passing the new domain data through\na network intialised from the source network and regularising the generated representations back towards those of the\nsource task through the use of an adversarial domain classiﬁer.\nFor SVHN to MNIST this approach seems appropriate, but as domains become more dissimilar such an obvious\nalignment may not be enough. Our method will be applied to transfer between Atari games where the visual domains\nare vastly different when compared to the varied number domains. Our approach therefore makes use of the Adversarial\nAutoencoder architecture. This architecture allows us to learn a a feature space that captures the target domain while\naligning it with the source domain.\nOur proposed approach seeks to transfer knowledge in this way as a pre-training phase which aligns the representations\nof a trained agent with that of an initialization network that is trained to efﬁciently encode the target task. The alignment\ncan be achieved in an unsupervised manner by utilising an AAE style architecture and is depicted in Figure 2. The\ninitialization network will then be used as the starting weights for training an RL agent using standard deep RL\napproaches.\nOur algorithm is outlined in Algorithm 1 and explained in the subsequent paragraphs.\nTo combine the ADDA approach with that of the AAE requires that we replace the regularising Gaussian distribution of\nthe AAE with a pre-trained source embedding model. In this work that model comes from the source policy which\nwas trained using the A2C algorithm. Other algorithms could be substituted here, however we leave an exploration of\nthe best algorithm state-space for adaptation to future work. Since the source network is ﬁxed during training of our\nadaptation method and to make our embedding samples more I.I.D, we run our source agent ahead of time and generate\na data-set of embeddings which we can sample from. We also generate a data-set of target task observations using the\nrandom policy; this is again done so that we can draw samples in a more I.I.D manner.\n4\nAlgorithm 1\nRequire: Trained Source Policy πθs(a|s)\nRequire: Untrained Adversarial Auto-Encoder\n1: for i = 1 →numEpochs do\n2:\nfor j = 1 →numBatches do\n3:\nsample target observations: x ∼Ot\n4:\nTrain AutoEncoder on batch\n5:\nfor k = 1 →N do\n6:\nsample source s: d ∼πθs(as|ss)\n7:\nsample target: x ∼πrandom(at|st)\n8:\nTrain Discriminator\n9:\nsample target observations: x ∼πrandom(at|st)\n10:\nTrain Generator on batch\n11:\n12: return Encoder/Generator\nWe can now train our target embedding network. This is done following the training procedure for an AAE. Our training\nprocedure has three update steps per batch. First, the AE is updated to reproduce target domain images. Second the\nDiscriminator is updated to separate source and target domain embeddings, and third, the Generator is updated to fool\nthe Discriminator.\nFinally we use the trained target embedding network to initialise the target model and train it using A2C. This deviates\nfrom the standard application of Adversarial Domain Adaptation methods where the source task classiﬁer layers can be\ndirectly used on the adapted body as the source and target task remain the same and share target labels. In this work, we\ndo not map action spaces or policies and treat the policy and value function as functions that must be learned from\nscratch.\n4\nExperiments\nOur experiments investigate the applicability of Adversarial Domain Adaptation to the reinforcement learning problem.\nWe use the Arcade Learning Environment (ALE) [22] to provide a set of arcade games which are commonly used as\nbenchmarks for Deep RL. We interface with ALE through the OpenAi Gym platform [23]. Our experiments focus on\ntransfer between pairs of games. We begin by selecting pairs of games which we deem to be related in order to verify\nthe effectiveness of our approach; we leave automatically identifying appropriate source tasks for future work. In this\nwork we focus on three games: Pong, Breakout and Tennis.\nPong is the virtualisation of Ping-Pong and is played with the ball moving sideways across the screen and the player’s\npaddle moving along the vertical axis. Breakout is a brick-breaking game where the player bounces a ball up to break\nthe bricks while moving a paddle side-to-side at the bottom of the screen. Breakout and Pong use a very similar\nmechanic, however, the image domain, action space, transition function and reward structure are different.\nTennis is a more difﬁcult task than Pong or Breakout and many deep RL algorithms have reported mediocre scores\n[7]; although human performance reported in [24] tends to be worse. This game also focuses on returning a ball. The\naction space for Tennis is much larger; in full it is an 18-dimensional action space, as the agent can move forward and\nbackward to cover the whole court. Tennis also switches the agent’s sides on the court, multiple times per set, so the\ncontrol of the agent becomes more complicated as the algorithm must identify which of the two agents it is playing as.\nThis leads to an exploration problem where the agent tends not to score well for a signiﬁcant number of games.\nWe start with transfer from Pong (source) to Breakout (target). We describe the experiment procedure in terms of these\ngames, however, we follow the same procedure for the other experiments; averaging the results over ﬁve trials.\n1. Train agent to play Pong in the standard RL fashion; we use A2C.\n2. Run the agent through a few games of Pong capturing the ﬁnal hidden layer output until we have collected\n100k samples. This represents the embedding space.\n3. Run a random policy through our target game, Breakout, and capture 100k frames. No learning happens at this\nstage.\n4. Using these two data sets train the AAE.\n5\n0\n1\n2\n3\nSteps\n1e7\n20\n10\n0\n10\n20\n100 episode average score\nPong baseline\nTransfer from Breakout\n0\n5000\n10000\n15000\n20000\nnumber of games\n20\n10\n0\n10\n20\n100 episode average score\nPong baseline\nTransfer from Breakout\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nSteps\n1e7\n1000\n1500\n2000\n2500\n3000\n3500\n(100 ep)Average frames per game\nPong baseline\nTransfer from Breakout\nFigure 3: TopLeft: Learning Curve for Pong, showing average score per game, against number of batches, each batch\nis 80 frames. TopRight: Plots the Average score against number of games completed. Bottom: 100 episode average\nnumber of frames in a game. Blue lines represent transfer and black represent random initialization, the ﬁlled regions\nrepresent 1 standard deviation. Source Task: Pong\n5. Take the weights of the Generator of our AAE, to initialise the weights of a new Policy network for the target\ntask. The policy and value function layer weights are randomly initialised.\n6. Train the policy on the target task.\n5\nResults\n5.1\nBetween Pong and Breakout\nTransferring from Breakout to Pong we can clearly see improvement from training using randomly initialised weights\nin Figure 4. Our agent reaches a solution faster than the baseline, converging to a solution approximately 800000\nsamples faster. We also note that while the adapted policy is learned faster it converges lower than the baseline. This\nis an example of negative transfer and highlights how evaluating transfer is a multi-faceted problem in its own right;\nbalancing improved learning speeds, improved initial performance and improved asymptotic performance.\nAnother way to look at this result is to observe how the number of frames in a game changes over time. We show\nthe 100 episode moving average. What we expect is that the agent will learn to score, and thus each game will last\nlonger as more points will be played per game. As the agent improves, the opponent will score fewer points and the\nnumber of frames will start reducing again. For the baselines this is what we observe, with high peaks of around 3500\nframes in a game. Our transfer experiments by comparison peak a lot lower at 2500. This observation is interesting as it\nsuggests the agent goes straight to learning a winning policy, without passing through a sort of intermediate stage. We\ncan also take this into account to observe that not only does our agent need fewer games to learn, each of those games is\non average shorter than the source games. Further investigation of the policy differences around these changes could\nprovide interesting insights; the negative transfer observed, for instance, may be explained by the discovery of a local\nmaximum winning policy, as despite the lower convergence our agent is still winning by 19 points which is a signiﬁcant\nadvantage.\nTransferring from Pong to Breakout we can also clearly see in Figure 4 a speed-up in the acquisition of Breakout,\nneeding signiﬁcantly fewer games (in the order of thousands) to achieve a decent score. As the agents become better\ntheir performance evens out again and they ﬁnish at comparable levels.\n6\n0\n1\n2\n3\n4\nSteps\n1e7\n0\n100\n200\n300\n400\n100 episode average score\nBreakout baseline\nTranfer from Pong\n0\n10000\n20000\n30000\nnumber of games\n0\n100\n200\n300\n400\n100 episode average score\nBreakout baseline\nTranfer from Pong\n0\n1\n2\n3\n4\nSteps\n1e7\n0\n1000\n2000\n3000\n(100 ep)Average frames per game\nBreakout baseline\nTranfer from Pong\nFigure 4: TopLeft: Learning Curve for Breakout, showing average score per game, against number of batches, each\nbatch is 80 frames. TopRight: Plots the Average score against number of games completed. Bottom: 100 episode\naverage number of frames in a game.Blue lines represent transfer and black random initialization,the ﬁlled regions\nrepresent 1 standard deviation. Source Task: Breakout\nFigure 4 also shows the 100 episode moving average of game length. We can clearly see that episode length increases\nfaster in the transfer case, meaning that the agent is better at hitting the ball back, and that the variance is less than the\nbaseline.\n5.2\nPong and Breakout to Tennis\nTennis provides a challenging domain to learn in; the opponent is a strong player and so positive rewards are sparse,\neven if our agent does hit the ball as the opponent is able to return it. The large action space also creates an exploration\nproblem. These difﬁculties lead us to run our algorithm for 80 million time-steps in the Tennis environment, signiﬁcantly\nmore than the 40 million used for Pong and Breakout.\nFigure 5 shows the result of transferring to Tennis from Pong or Breakout. It is clearly difﬁcult to learn a policy for\nTennis as all our initialisations require many training steps before a scoring trend emerges. Initialising with knowledge\nof Pong provides beneﬁt by reducing the number of games the agent needs to play.\nInitialising with knowledge of Breakout appears to have a different effect, slowing the agent earlier on, before taking a\nlarge learning step. The variance of the breakout initialisations is also smaller than transfer from Pong.\n5.3\nWhat About the Policy?\nOur proposed method focuses on learning an initial state representation utilising transferred knowledge but does\nnot provide any policy initialisation. Initialising the policy in conjunction with our approach could provide further\nimprovement. As a proof of concept demonstration of this we run a further experiment, transferring from Pong to\nTennis. In this experiment we use the same procedure as before, however, we now take the policy and value function\nlayer from a trained Tennis model and use those to initialise the corresponding layers in the transfer model.\nFigure 6 shows the beneﬁt of adding this extra policy information. We see signiﬁcant improvement, which demonstrates\nhow our algorithm can be integrated into a larger transfer system which also predicts policy initialisations.\n7\nFigure 5: TopLeft: Learning Curve for Tennis, showing 100 episode average score per game, against number of batches,\neach batch is 80 frames. TopRight: Plots the Average score against number of games completed. Bottom: 100 episode\naverage number of frames in a game. Blue lines represent transfer from Pong, green lines transfer from Breakout and\nblack random initialization baseline, the ﬁlled regions represent 1 standard deviation. Source Task: Breakout/Pong\n0\n10000\n20000\n30000\n40000\nnumber of games\n25\n20\n15\n10\n5\n100 episode average score\nTransfer From Pong\nTransfer From Breakout\nBaseline\nTransfer From Pong + PreTrained Tennis Policy\nFigure 6: Learning Curve for Tennis, showing 100 episode average score per game, against number of games. Showing\ntransfer from Pong in red, transfer from breakout in blue and transfer from Pong, with a pre-trained policy layer and\nvalue-function layer appended.\n8\n6\nRelated Work\nOur method is closely related to a number of AE transfer learning methods that seek to align the encoding distribution\nof two data-domains. This is usually done with reference to the Kullback-Leibler (KL)-divergence between the two\nembedding domains which can be implicitly or explicitly minimized. In [18] an AE with encoding and decoding\nweights shared between source and target domains is trained and the KL-divergence between their embeddings is\nexplicitly minimised in the cost function. Their cost function also directly incorporates source label information as a\nregularization term by using the embedding layer as input for a softmax prediction layer.\nIn the RL context AEs can also be used as part of the initialisation process. The approach taken in [25] uses Variational\nAEs to align real and synthetic images, for a robotics application, where the decoder is shared. This allows them to\ntransform one image domain into the other. They then train an object detector on top of these reconstructed images.\nThe work in [26] imposes a Kullback-Liebler divergence constraint on the encoding layer of two AEs trained in parallel.\nThis approach estimates an alignment through time, as two differently actuated robots solve the same task. This\napproach is successful but requires estimating some form of alignment between samples. Our method differs from\nthose methods by aligning encoding distributions in an unsupervised manner via Discriminator network feedback as in\nthe AAE method. This method utilises the Generative Adversarial Network learning algorithm to incorporate source\ndomain information into our target domain embedding. Our approach extends the use of AE-based transfer learning in\nreinforcement learning by utilising methods developed for adversarial adaptation.\nThe adversarial approach we use is related to a variety of Adversarial Regularization methods. The most related on\nis the AAE of [3] where the discriminator is used as a regularisation term on the AE embedding. We adapt this by\nintroducing a regularisation space that transfers information from a source task. The AAE itself is closely related to a\nnumber of approaches such as variational autoencoders (VAE) [14]. A similar approach using adversarial regularization\nhas also been used for generating discrete data, such as text, in [27]. Adversarial approaches have also been used more\nexplicitly for domain adaptation and [17] provides a general framework that describes the various parts that can be\nchanged, and what has so far been attempted.\nA recent approach [28] that uses a GAN to transfer between two problems in the RL setting. They do this by using the\nGAN loss for True or False samples as an extra reward variable within their RL training. The approach explores how\nthis reward augmentation can be used to achieve learning when the environment reward is missing or uninformative.\nThe domain, however, is much more aligned as they focus on transfer for robotic agents moving from simulation\nenvironments to real environments.\n6.1\nTransfer and the Atari\nTransfer learning has been attempted in various forms on the Atari. An early approach following the DQN [24] was\nthe actor-mimic [29] where a multi-task atari agent was trained trying to match expert actions and representations.\nThis network can then be used to intialise the weights of an agent for some target task. This pre-training mechanism\ninitialises the ﬁlters of the target agent allowing it to start with a useful state-space. A similar mechanic is employed in\n[30] where instead of building all relevant ﬁlters into a single network, a series of networks are joined-together feeding\ninto the layers of each successor, adding more feature detectors. If games share similarities then these methods allow\npreviously learned ﬁlters to contribute to the state-space description of the new task. The progressive neural network\napplied to transfer learning on the atari has a number of interesting success and failure cases and the approach comes\nwith the down-side of continually increasing the size of the network. A recent work (progress and compress) by [31]\nhas looked at limiting the growth of progressive networks and their approach shows interesting results.\n7\nConclusion\nWe have presented an adversarial method for knowledge transfer in reinforcement learning. We have demonstrated how\nthis approach can be used for domain adaptation to improve performance on the difﬁcult task of learning to play Atari\ngames. We demonstrate how we can also change the action space and the reward function and derive beneﬁt from the\ntransfer approach.\nAdversarial Adaptation methods offer a powerful framework for domain adaptation. We have demonstrated how a\nsimple application of the technique can help learning in the reinforcement learning case, even when the ﬁnal layer needs\nto be relearned as the action space and task have changed. This shows that the technique is not limited to simpler or\nmore closely-related domains as previously explored such as MNIST to USPS, or SVHN to MNIST. Recent work has\nsought to expand and improve the application of the technique and future work would seek to holistically integrate this\ntechnique within the RL paradigm.\n9\nAs noted in related work other authors have attempted Transfer-Learning for RL and applied it in the context of the\nAtari. Many of those approaches take a multi-task or lifelong learning view of the problem [29, 32] and so have not\nbeen directly compared with. Future work would seek to more fully explore the comparison between methods as well\nas understand where they fail or succeed.\nTransfer for RL is a challenging domain and many recent approaches that align domains use the alignment to generate an\nextra reward signal for the target task agent. Our approach demonstrates transfer across different worlds with different\ntargets through representation alignment without any additional reward information encoded in the target task reward\nfunction. This demonstrates the power of adversarial domain adaptation methods and provides for a variety of future\nresearch directions.\nReferences\n[1] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural\nnetworks? In Advances in neural information processing systems, pages 3320–3328, 2014.\n[2] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. arXiv preprint\narXiv:1409.7495, 2014.\n[3] Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial autoencoders.\narXiv preprint arXiv:1511.05644, 2015.\n[4] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville,\nand Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages\n2672–2680, 2014.\n[5] a2c announcement. https://blog.openai.com/baselines-acktr-a2c/, 2018. accessed 09/03/2018.\n[6] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell,\nDharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv preprint arXiv:1611.05763,\n2016.\n[7] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David\nSilver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International\nConference on Machine Learning, pages 1928–1937, 2016.\n[8] David Pfau and Oriol Vinyals. Connecting generative adversarial networks and actor-critic methods. arXiv\npreprint arXiv:1610.01945, 2016.\n[9] Sergio Valcarcel Macua, Aleksi Tukiainen, Daniel Garcıa-Ocana Hernandez, David Baldazo, Enrique Munoz de\nCote, and Zazo Santiago. Diff-dac: Distributed actor-critic for multitask deep reinforcement learning. arXiv\npreprint arXiv:1710.10363, 2017.\n[10] Sascha Lange and Martin Riedmiller. Deep auto-encoder neural networks in reinforcement learning. In Neural\nNetworks (IJCNN), The 2010 International Joint Conference on, pages 1–8. IEEE, 2010.\n[11] Herke van Hoof, Nutan Chen, Maximilian Karl, Patrick van der Smagt, and Jan Peters. Stable reinforcement\nlearning with autoencoders for tactile and visual data. In Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ\nInternational Conference on, pages 3928–3934. IEEE, 2016.\n[12] Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, and Pieter Abbeel. Deep spatial autoencoders\nfor visuomotor learning. In Robotics and Automation (ICRA), 2016 IEEE International Conference on, pages\n512–519. IEEE, 2016.\n[13] Gabriel Barth-Maron. Learning deep state representations with convolutional autoencoders. PhD thesis, Master’s\nthesis, Brown University, 2015.\n[14] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\n[15] Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.\n[16] Abhishek Gupta, Coline Devin, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Learning invariant feature spaces\nto transfer skills with reinforcement learning. arXiv preprint arXiv:1703.02949, 2017.\n10\n[17] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In\nComputer Vision and Pattern Recognition (CVPR), volume 1,2, page 4, 2017.\n[18] Fuzhen Zhuang, Xiaohu Cheng, Ping Luo, Sinno Jialin Pan, and Qing He. Supervised representation learning:\nTransfer learning with deep autoencoders. In IJCAI, pages 4119–4125, 2015.\n[19] Haitham Bou Ammar, Eric Eaton, Paul Ruvolo, and Matthew E Taylor. Unsupervised cross-domain transfer in\npolicy gradient reinforcement learning via manifold alignment. In Proc. of AAAI, 2015.\n[20] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document\nrecognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n[21] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in\nnatural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature\nlearning, volume 2011,2, page 5, 2011.\n[22] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation platform\nfor general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279, jun 2013.\n[23] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech\nZaremba. Openai gym, 2016.\n[24] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex\nGraves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep\nreinforcement learning. Nature, 518(7540):529, 2015.\n[25] Tadanobu Inoue, Subhajit Chaudhury, Giovanni De Magistris, and Sakyasingha Dasgupta. Transfer learning from\nsynthetic to real images using variational autoencoders for robotic applications. arXiv preprint arXiv:1709.06762,\n2017.\n[26] Abhishek Gupta, Coline Devin, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Learning invariant feature spaces\nto transfer skills with reinforcement learning, 03 2017.\n[27] Yoon Kim, Kelly Zhang, Alexander M Rush, Yann LeCun, et al. Adversarially regularized autoencoders for\ngenerating discrete structures. arXiv preprint arXiv:1706.04223, 2017.\n[28] Markus Wulfmeier, Ingmar Posner, and Pieter Abbeel. Mutual alignment transfer learning. arXiv preprint\narXiv:1707.07907, 2017.\n[29] Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and transfer reinforce-\nment learning. arXiv preprint arXiv:1511.06342, 2015.\n[30] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu,\nRazvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.\n[31] Jonathan Schwarz, Jelena Luketina, Wojciech M Czarnecki, Agnieszka Grabska-Barwinska, Yee Whye Teh,\nRazvan Pascanu, and Raia Hadsell. Progress & compress: A scalable framework for continual learning. arXiv\npreprint arXiv:1805.06370, 2018.\n[32] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran\nMilan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in\nneural networks. Proceedings of the national academy of sciences, page 201611835, 2017.\n11\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2018-12-18",
  "updated": "2018-12-18"
}