{
  "id": "http://arxiv.org/abs/1812.07715v1",
  "title": "A Tour of Unsupervised Deep Learning for Medical Image Analysis",
  "authors": [
    "Khalid Raza",
    "Nripendra Kumar Singh"
  ],
  "abstract": "Interpretation of medical images for diagnosis and treatment of complex\ndisease from high-dimensional and heterogeneous data remains a key challenge in\ntransforming healthcare. In the last few years, both supervised and\nunsupervised deep learning achieved promising results in the area of medical\nimaging and image analysis. Unlike supervised learning which is biased towards\nhow it is being supervised and manual efforts to create class label for the\nalgorithm, unsupervised learning derive insights directly from the data itself,\ngroup the data and help to make data driven decisions without any external\nbias. This review systematically presents various unsupervised models applied\nto medical image analysis, including autoencoders and its several variants,\nRestricted Boltzmann machines, Deep belief networks, Deep Boltzmann machine and\nGenerative adversarial network. Future research opportunities and challenges of\nunsupervised techniques for medical image analysis have also been discussed.",
  "text": "1 \n \nA Tour of Unsupervised Deep Learning for \nMedical Image Analysis \nKhalid Raza* and Nripendra Kumar Singh  \nDepartment of Computer Science, Jamia Millia Islamia, New Delhi \nkraza@jmi.ac.in \nDecember 13, 2018 \nAbstract \nInterpretation of medical images for diagnosis and treatment of complex disease from high-\ndimensional and heterogeneous data remains a key challenge in transforming healthcare. In the last \nfew years, both supervised and unsupervised deep learning achieved promising results in the area of \nmedical imaging and image analysis. Unlike supervised learning which is biased towards how it is \nbeing supervised and manual efforts to create class label for the algorithm, unsupervised learning \nderive insights directly from the data itself, group the data and help to make data driven decisions \nwithout any external bias. This review systematically presents  various unsupervised models applied \nto medical image analysis, including autoencoders and its several variants, Restricted Boltzmann \nmachines, Deep belief networks, Deep Boltzmann machine and Generative adversarial network. \nFuture research opportunities and challenges of unsupervised techniques for medical image analysis \nhave also been discussed. \n \nKeywords: Unsupervised learning; medical image analysis; autoencoders; restricted Boltzmann \nmachine; Deep belief network \n1. Introduction \nMedical imaging techniques, including magnetic resonance imaging (MRI), positron \nemission tomography (PET), computed tomography (CT), mammography, ultrasound, X-ray \nand digital pathology images, are frequently used diagnostic system for the early detection, \ndiagnosis, and treatment of various complex diseases (Wani & Raza, 2018). In the clinics, the \nimages are mostly interpreted by human experts such as radiologists and physicians. Because \nof major variations in pathology and the potential fatigue of human experts, scientists  and \ndoctors have started using computer-assisted interventions. The advancement in machine \nlearning techniques, including deep learning, and availability of computing infrastructure \nthrough cloud computing, have given fuel to the field of computer-assisted medical image \nanalysis and computer-assisted diagnosis (CAD). Deep learning is about learning \nrepresentations, i.e, learning intermediate concept or features which are important to capture \ndependencies from input variables to output variables in supervised learning, or between \nsubsets of variables in unsupervised learning. Both supervised and unsupervised machine \nlearning approaches are widely applied in medical image analysis; each of them has their own \npros and cons. Some of widely used supervised (deep) learning algorithms are Feedforward \nNeural Network (FFNN), Recurrent Neural Network (RNN), Convolutional Neural Network \n(CNN), Support Vector Machine (SVM) and so on (Jabeen et al., 2018). There are many \n2 \n \nscenarios where human supervisions are unavailable, inadequate or biased and therefore, \nsupervised learning algorithm cannot be directly used. Unsupervised learning algorithms, \nincluding its deep architecture, give a big hope with lots of advantages and have been widely \napplied in several areas of medical and engineering problems including medical image \nanalysis. \nThis chapter presents unsupervised deep learning models, its applications to medical image \nanalysis, list of software tools/packages and benchmark datasets; and discusses opportunities \nand future challenges in the area. \n2. Why Unsupervised Learning? \nIn the majority of machine learning projects, the workflow is designed in a supervised way, \nwhere the algorithm is guided by us what to do and what not to! In such supervised \narchitecture the potential of the algorithms are limited in three ways, (i) A huge manual effort \nto create labels and (ii) Biases related to how it is being supervised, which probabilities the \nalgorithm to think for other corner cases during problem solving, and (iii) Reduce the \nscalability of target function at hand.  \nTo intelligently solve these issues, unsupervised machine learning algorithm can be used. \nUnsupervised machine learning algorithms not only derives insights directly from the data \nand group the data, but also uses these insights for data-driven decisions making. Also, \nunsupervised models are more robust in the sense that they act as a base for several different \ncomplex tasks where these can be utilized as the holy grail of learning and classification. In \nfact, the classification is not the only task that we do; rather, other tasks such as compression, \ndimensionality reduction, denoising, super resolution and some degree of decision making \nare also performed. Therefore, it is rather more useful to construct a model without knowing \nwhat tasks will be at hand and we will use representation (or model) for. In a nutshell, we can \nthink of unsupervised learning as preparation (preprocessing) step for supervised learning \ntasks, where unsupervised learning of representation may allow better generalization of a \nclassifier (Jabeen et al., 2018). \n \n3. Taxonomy of Unsupervised Learning Tasks \nIn unsupervised learning, we group the unlabeled data set on the basis of underlying hidden \nfeatures. By grouping data through unsupervised learning, at least we learn something about \nraw data. \n3.1 Density estimation \nDensity estimation is one of the popular categories of unsupervised learning which discovers \nthe intrinsic feature and structure of large and complex unlabeled data set via another non-\nprobabilistic approach. Density estimation is a non-parametric method which doesn’t possess \nmuch restriction and distributional assumption unlike parametric estimation. \n3 \n \nEstimation of univariate or multivariate density function without any prior functional \nassumptions get almost limitless function from data. There are some widely used non-\nparametric methods of estimation. \n3.1.1 Kernel density estimation \nKernel density estimation (KDE) uses statistical model to produce a probabilistic distribution \nthat resembles an observed variable as a random variable. Basically, KDE is used for data \nsmoothing, exploratory data analysis and visualization. A large number of kernels have been \nproposed, namely normal Gaussian mixture model and multivariate Gaussian mixture model. \nSome of the advantages of Kernel density estimation are: \no No need for model specification (data speaks itself). \no Estimation converges to any density, shape with sufficient sample. \no Easily generalizes to higher dimension. \no Densities are multivariate and multimodal with irregular cluster shape. \n3.1.2 Histogram density estimation \nHistogram based technique mainly adds smoothness of the density curve of reconstruction \nwhich can be optimized by kernel parameters and closely related to KNN density estimation \nalgorithm (Bishop et al., 2006). \n3.2 Dimensionality reduction \nWhy dimensionality reduction? There has been a tremendous increase in deployment of \nsensors and various imaging technique’s which are being used in industry and medical \ndiagnosis continuously record data and store it to be analyzed later. Lots of redundancy or \nnoises are present initially when data are captured. For example, let us take a case of a patient \nhaving bone fracture. Initially doctors suggested for X-ray images which is a 2D/3D imaging, \nbut when they do not find it helpful in diagnosis, then a CT scan and/or MRI (magnetic \nresonance imaging) may be taken which gives more detailed results for further right \ndiagnosis. Now assume that an analyst sits with all this data to analyze and identified all \nimportant variables/dimensions which have most significant information’s and left all \nunwanted parts of data. This is the problem of high unwanted dimension removal and needs \ntreatment of dimension reduction. Dimension reduction is the process of reducing higher \ndimension data set into a lesser dimension, ensuring that final reduced data must convey \nequivalent information concisely. \nLet’s look at figure shown below. It shows two-dimensional x and y which are measurement \nof several objects in cm (x1) and inches (y1), if you continue to use both dimensions in \nmachine learning problems it will introduce lots of noise in the system. So, it is better to just \nuse one-dimension (z1) and they will convey similar information. \n4 \n \n \nFig 1. Representation of data in two dimensional and one dimensional space \nThere are some common methods to perform dimensionality reduction: \nS.1.2 \nFactor analysis \nSome variables in given data are highly correlated. These variables can be grouped on the \nbasis of their correlations. This means a particular group can have highly correlated variable, \nbut have low correlation with variables of other groups. Each group represents single inherent \nconstruct or factor. As compared to data having large number of dimensions, these factors are \nsmall in number, while these factors are difficult to find. There are two methods for doing \nfactor analysis: (i) Exploratory Factor Analysis, (ii) Confirmatory Factor Analysis \n \n3.2.2 Principal component analysis \nA set of variables, which are linear combination of the original set of variables, performs \nhigher dimensional space mapped to lower dimensions in such a way that variance of data in \nlower dimensional space is maximized. These new set of variables is known as principle \ncomponents.  \nLet’s consider a situation of two-dimensional data set, there can be only two principal \ncomponents, first principal component is the most possible variation of original data and \nsecond principal component is orthogonal to the first principal component, as shown in Fig. \n2. \n5 \n \n \nFig. 2 Principle components on a two-dimensional data \nIn practice, a simple principal component analysis (PCA) can be used to construct the \ncovariance or correlation matrix of the data and compute the eigenvectors. The eigenvectors \ncorrespond to the largest eigenvalues (principal component) are used to reconstruct a large \nfraction of variance of original data. As a result, it is left with lesser number of eigenvector \nand original space has been reduced. There might be chances of loss of data, but it is retained \nby most important eigenvectors. \nConsider a matrix U(m) which stored empirical mean of input matrix R, \n ( )   \n \n ∑\n (   )          (              )                     ( )\n \n   \n \n \nCalculate a normalized matrix X,  X = (R – Ue), where e is a unitary vector matrix of size N. \nFinally, the mean square error (E2) is calculated in which smallest eigenvalues are removed, \n        (   ( ))   ∑\n  \n \n   \n  ∑\n  \n \n     \n                         (2) \nThe trace(A) is the sum of all eigenvalues. Simple PCA is not capable of constructing \nnonlinear mapping, however, can implement nonlinear classification by using kernel \ntechniques. \n3.2.3 Kernel PCA  \nKernel PCA is a nonlinear extension of conventional PCA, which is designed for \ndimensionality reduction of nonlinear subspaces depending on magnitude of input data and \nproblem setup. In medical image analysis, hybrid kernel PCA is frequently used to get better \nresults in unsupervised deep learning training model. Fischer et al. (2017) proposed an \nunsupervised deep learning illumination invariant kernel PCA, which is applied to each patch \nof respiratory signal extraction from X-ray fluoroscopy images leading to a set of low-\ndimensional embedding.  \n6 \n \nA kernel PCA comprised a kernel matrix K and kernel function k(.) is a Mercer kernel \n(Minh et al. 2006), defined as Ki j  = k(x(i) , x(j)),such that k(.) return dot product of feature \nspace. Now mapping of an eigenvalue of the kernel matrix, the Eigen decomposition and \nrespected eigenvectors are computed as, \nλ{i}e{i} = K e{i}  \n                                                 (3) \n ( )  \n \n * + ∑\n * + ( ( )  )\n \n   \n \n \n \n(4) \nwhere λ{i} is eigenvalues and e{i} is eigenvectors of K; T is the number of training sample x to \nthe principal component “i”. Fischer et al. (2017) analyzed different methods like PCA, \nKPCA and Multi-Resolution PCA to Diaphragm tracking correlation coefficient between \ndifferent versions of the same sequence and agreed that Multi-Resolution PCA produce the \nbest result among most of the parameters. Principal component analysis network (PCANet) is \na simple network architecture and one of the benchmark frameworks (Chan et al. 2015) for \nthe unsupervised deep learning in recent time. However, Shi et al. (2017) propose an \nencoding as C-RBH-PCANet which is improved PCANet to effectively integrate the color \npattern extraction and random binary hashing method for learning feature from color \nhistopathological images. \n3.3  Clustering \nClustering is an unsupervised classification of unlabeled data (patterns, data item or feature \nvectors) into similar groups (clusters) (Fig. 3). Cluster analysis is explanatory in nature to \nfind structure in data (Jain, 2008). Some model of clustering includes semi-supervised \nclustering, ensemble clustering, simultaneous feature selection and large-scale data clustering \nwere emerging as a hybrid-clustering. It involves analysis of multivariate data and applied in \nvarious scientific domains where clustering technique is utilized, such as machine learning, \nimage analysis, bioinformatics, pattern recognition, computer vision and so on. \n \nFig. 3 An illustration of data clustering \n7 \n \nClustering algorithm is broadly divided into two groups: hierarchal clustering and partitional \nclustering, as described below. \n3.3.1 Hierarchical clustering \nHierarchical clustering algorithms find clusters recursively (using previously established \ncluster). These algorithms can be either in the agglomerative mode (bottom-up) in which \nbegin with each element as a separate cluster, merge the most similar pair of clusters \nsuccessively into large clusters, or in divisive (top-down) mode which begin with all elements \nin one cluster, recursively divide into smaller clusters. A hierarchical clustering algorithm \nyields a dendrogram representing group of patterns and similarity level (Jain et al., 1999). A \ndetailed discussion can be found in Jain et al. (1999). \n3.3.2 Partitional (k-means) clustering \nOne of the most popular partitioning clustering algorithms is k-means. In spite of several \nclustering algorithms published in over 50 years, k-means is still widely used (Jain, 2010). \nThe most frequently used functions in partitional clustering is squared error criterion, which \napplied to isolate and compact clusters. Let X = {xi: i = 1, 2, 3, …. N} be the set of n d-\ndimensional elements clustered into set of K clusters as C = {ck : k = 1, 2, 3,…K}. To find \npartitions, squared error between empirical mean of a cluster and elements in the cluster is \nminimized. Let µk be the mean of the cluster (ck), the squared error between mean and \nelements in a cluster is defined as: \n (  )  ∑‖     ‖ \n     \n                                                     ( ) \nThe main objective of K-means is to minimize the sum of squared error for all k clusters \n(Drineas et al., 1999). \n ( )  ∑∑‖     ‖ \n     \n \n   \n                                               ( ) \nMinimizing objective function is  an NP-hard problem even for k = 2. Thus, k-means is a \ngreedy algorithm and it can only be expected to converge to local minima. \n4. Unsupervised deep learning models \nThis section  introduces a formal introduction  of unsupervised deep learning concepts, \nmodels and architectures that are used in medical image analysis. The unsupervised deep \nlearning models can be roughly classified as shown in Fig. 4. \n8 \n \n \nFig. 4 Unsupervised deep learning models \n4.1 Auto-encoders and its variants \nIn the literature, autoencoders and its several variants are reported and are being extensively \napplied in medical image analysis.  \n4.1.1 Autoencoders and Stacked autoencoder \nAutoencoders (AEs) (Bourlard et al., 1988) are simple unsupervised learning model consist \nsingle-layer neural network that transforms the input into a latent or compressed \nrepresentation by minimizing the reconstruction errors between input and output values of the \nnetwork. By constraining the dimension of latent representation (may be from different input) \nit is possible to discover relevant pattern from the data. AEs framework defines a feature to \nextract function with specific parameters (Bengio et al., 2013). Basically, AEs are trained \nwith specific function fƟ is called encoder and h = fƟ(x) is feature vector or representation \nfrom input x, another parameterized function gƟ called decoder, producing input space back \nfrom feature space. In short, basic AEs are trained to minimize reconstruction error in finding \na value of parameter  , given by, \n   ( )   ∑ .    〖( 〗 ( ))/                              ( ) \nThis minimization optionally followed by a non-linearity (most commonly used for encoder \nand decoder) as given by, \nUnsupervised \ndeep learning \nmodels \nAutoencoders \n(AEs) & \nVariants \nRestricted \nBoltzmann \nMachines \n(RBMs) \nDeep Belief \nNetworks \n(DBNs) \nDeep \nBolzmann \nMachines \n(DBMs) \nGenerative \nadversarial \nnetworks \n(GANs) \n9 \n \n  ( )     (    )                                                     ( ) \n  ( )     (     )                                                 ( ) \nwhere Sf and Sg are encoder and decoder activation function (normally, sigmoid,  hyperbolic \ntangent or an identity function), respectively; parameters of model   = {W, b, W’, d}, where \nW and W’ are encoder decoder weight matrices, and b and d are encoder and decoder bias \nvector, respectively. Moreover, regularization or sparsity constraints may be applied in order \nto boost the discovery process.  In case, hidden layer has the same input as the input layer, \nand no any non-linearity is added, the model would simply learn an identity function. Fig. \n5(a) illustrates the basic structure of AE.  \nStacked autoencoders (SAEs) are constructed by organizing AEs on top of each other also \nknown as deep AEs. SAEs consist of multiple AEs stacked into multiple layers where the \noutput of each layer is wired to the inputs of the successive layers Fig. 5(b). To obtain good \nparameters, SAE uses greedy layer-wise training. The benefit of SAE is that it can enjoy the \nbenefits of deep network, which has greater expressive power. Furthermore, it usually \ncaptures a useful hierarchical grouping of the input (Shin et al., 2013). \n4.1.2 Denoising autoencoder \nDenoising autoencoder (DAEs) is another variant of the auto-encoder. Denoising investigated \nas a training criterion for learning to constitute better higher-level representation and extract \nuseful features (Vincent et al. 2010). DAEs prevent the model from learning a trivial solution \n(Litjens G. et al., 2017) where the model is trained to reconstruct a clean input from the \ncorrupted version from noise or another corruption.  his is done by corrupting the initial \ninput x into x  by using a stochastic function x  ~ qD x   x . The corrupted input    is then \nmapped to a hidden representation y = fƟ(x ) = s(Wx  + b) and reconstruct z = gƟ’ (y). A \nschematic representation of DAE is shown in Fig.5(c). Parameter   and    are initialized \nrandomly and trained using stochastic gradient descent in order to minimize average \nreconstruction error. The denoising autoencoders continue minimizing same reconstruction \nloss between clean X and reconstruction from Y. This continues maximizing a lower bound \non the mutual information between input x and representation y, and difference is obtained by \napplying mapping fƟ to a corrupted input. Hence, such learning is cleverer than the identity, \nand it extracts features useful for denoising. \nStack denoising autoencoder (SDAE) is a deep network utilizing the power of DAE (Bengio \net al., 2007; Vincent et al., 2010) and RBMs in the deep belief network (Hinton & \nSalakhutdinov, 2006; Hinton et al., 2006).  \n4.1.3 Sparse autoencoder \nThe limitation of autoencoders to have only small numbers of hidden units can be overcome \nby adding a sparsity constraint, where a large number of hidden units can be introduced \nusually more than one input. The aim of sparse autoencoder (SAE) is to make a large number \nof neurons to have low average output so that neurons may be inactive most of the time. \n10 \n \nSparsity can be achieved by introducing a loss function during training or manually zeroing \nfew strongest hidden unit activations. A schematic representation of SAE is shown in Fig. \n5(d). \nIf the activation function of hidden neurons is aj, the average activation function of each \nhidden neuron j is given by  \n     \n ∑[    ]\n \n   \n                                                           (  ) \nThe objective of sparsity constraints is to minimize    so that    , where   is a sparsity \nconstraint very close to 0 such as 0.05. \nTo enforce sparsity constraints, a penalty term is added to cost function which penalizes ̂ , \nde-weighting significantly from  . The penalty term is the Kullback-Leibler (KL) divergence \nbetween Bernoulli random variables, can be calculated as (Ng, 2013; Makhzani & Frey, \n2013), \n              ∑  (    ̂ )\n  \n   \n                                           (  ) \nwhere    is number of neurons in the hidden layers,and index   is summing over the hidden \nunits in the network. \n  (    ̂ )        \n ̂ \n  (   )      \n   ̂ \n                             (  ) \nThe property of penalty function is that   (      ̂ )   , if     ̂ , otherwise it increases \ngradually as  ̂ diverses for  . \nThe k-sparse autoencoder (Makhzani & Frey 2013) is a form of sparse AE where k neurons \nhaving the highest activation function are chosen and the rest is ignored. The advantage of k-\nsparse AE is that they allow better exploration on a data set in terms of percentage activation \nof the network. The advantage of SAE is the sparsity constraints which penalize the cost \nfunction and as a result degrees of freedom is reduced. Hence, it regularizes and maintains \nthe complexity of the network by preventing over-fitting. \n4.1.4 Convolutional autoencoder  \nThe most popular and widely used network model in deep unsupervised architecture is \nstacked AE. Stacked AE requires layer-wise pre-training. When layers go deeper during the \npre-training process, it may be time consuming and tedious because of stacked AE is built \nwith fully connected layers. Li et al. (2017) propose first trial to train convolutional directly \nan end-to-end manner without pre-training. Guo et al. (2017) suggested convolutional \nautoencoder (CAE) that is beneficial to learn feature for images and preserving the local \nstructure of data and avoid distortion of feature space. A general architecture of CAE is \ndepicted in Fig. 5(c). \n11 \n \n \n \nFig. 5 (a)-(g) Diagrams showing networks of autoencoders and its different variants \n12 \n \n4.1.5 Variational autoencoder \nAnother variant of autoencoder, called variational autoencoder (VAE), was introduced as a \ngenerative model (Kingma &Welling, 2013). A general architecture of VAE is given in Fig. 4(f). \nVAEs utilize the strategy of deriving a lower bond estimator from the directed graphical models \nwith continuous distribution of latent variables.  he generative parameter θ in the decoder \n(generative model) assist the learning process of the variational parameter, ϕ as encoder in the \nvariational approximation model. VAEs apply the variational approach to latent representation, \nlearning as additional loss component training estimators, known as stochastic gradient \nvariational Bayes (SGVB) and Autoencoding variational Bayes (AEVB) (Kingma & Welling, \n2013). It Optimizes the parameter ϕ and θ for probabilistic encoder qϕ(z|x), which is an \napproximation to the generative model pθ(x, z), where z is latent variable and x is continuous or \ndiscrete variable. Its aim is to maximize the probability of each x in the training data set under \nentire generative process. However, alternative configuration of generative latent variable \nmodeling rises to give deep generative models (DGMs) instead of existing assumption of \nsymmetric Gaussian posterior (Partaourides at el., 2017). \n4.1.6 Contractive autoencoder  \nRifai (2011) presented a novel approach for training deterministic autoencoder. Contractive \nautoencoder is additional of explicit regularizer in the objective function that enables the model to \nlearn a function having slight variations of input values. This additional regularizer corresponds \nto the squared Forbenius norm of the Jacobian matrix of given activation with respect to the \ninput. The contractive autoencoder is obtained with the regularization term in following equation \nyield final objective function, \n    ( )  ∑. (   ( ( ))    ‖   ( )‖\n \n /\n    \n                             (  ) \nThe difference between contractive AE and DAE stated by (Vincent et al., 2010) as contractive \nAE explicitly encourage robustness of representation, whereas DAE stressed on the robustness of \nreconstruction this property make sense of contractive AE a better choice than DAEs to learn \nuseful feature extraction. Table 1 presents a summary of autoencoders and its variants, and Table \n2 presents its applications for medical image analysis.  \nTable 1.Summary of autoencoders and its variants \nTypes \nDescriptions \nReferences \nAutoencoder  \nOne of the simplest form which aims to learn \na representation (encoding) for a set of data. \nBallard (1987);   \nBourlard & Kamp (1988) \nStacking autoencoder \nAn autoencoder having multiple layers where \nthe outputs of each layers are given as inputs \nof the successive layer. \nZabalza et al. (2016) \nSparse autoencoder \nEncourages hidden units to be zero or near to \nzero \nGoodfellow et al. (2009) \nDenosing autoencoder \nCapable to predict true inputs from noisy \ndata \nLeCun & Gallinari, (1987); \nVincent et al. (2008) \nConvolutional autoencoder \nLearn feature, preserve the local structure of \ndata and avoid distortion of feature space \nGuo et al. (2017) \nVariational autoencoder \nA generative model utilizing strategy of \nderiving a lower bond estimator from \ndirected graphical models with continuous \ndistribution of latent variables. \nKingma & Welling (2013) \nContractive autoencoder \nForces encoder to take small derivatives \nRifai et al. (2011) \n13 \n \nTable 2 Applications of autoencoders and its variants for medical image analysis. \n[Abbreviations: H&E: hematoxylin and eosin staining; AD: Alzheimer’s disease; MCI: Mild cognitive \nimpairment; fMRI: Functional magnetic resonance imaging; sMRI: Structural magnetic resonance imaging; rs-\nfMRI: Resting-state fMRI; DBN: Deep belief network; RBM: Restricted Boltzmann machine] \nMethod \nTask \nImage type \nRemarks \nReferences \nSAE \nAD/MCI classification \nMRI \nSAE accompanied by supervised fine \ntuning \nSuk & Shen (2013) \nSAE \nAD/MCI/HC \nclassification \nMRI & PET \nExtraction of latent features on a huge \nset of  features obtained from MRI and \nPET images using SAE \nSuk et al. (2013a) \nSAE \nAD/MCI/HC \nclassification \nMRI \nSAE used to pre-train 3D CNN \nPayan \n& \nMontana \n(2015) \nSAE \nMCI/HC classification  \nfMRI \nSAE used for feature extraction, HMM \nas a generative model on top \nSuk et al. (2016) \nSAE \nHippocampus \nsegmentation \nMRI \nSAE used for representation learning \nand measure target/atlas patch  \nGuo et al. (2014) \nSAE \nVisual \npathway \nsegmentation \nMRI \nSAE used to learn appearance features \nto \nsteer \nthe \nshape \nmodel \nfor \nsegmentation \nMansoor et al. (2016) \nSAE \nDenoising DCE-MRI \nMRI \nUses an ensemble of denoising SAE \n(pre-trained with RBMs). Denoising \ncontrast-enhanced MRI sequences using \nexpert DNNs (pre trained with RBMs) \nBenou et al. (2016) \nSSAE \nNucleus detection \nDigital \npathology \nimage \ndetection of nuclei on breast cancer \ndigital histopathological images. \nXu et al. (2016) \nSAE \nStain normalization \nDigital \npathology \nimage \nSAE is applied to classify tissues and \ntheir subsequent histogram \nMatching \nJanowczyk \net \nal. \n(2017) \nSAE \nDensity classification \nMammography \nUnsupervised CNN  with SAE to learn \nfeatures from unlabeled data  for breast \ntexture and  density classification \nKallenberg \net \nal. \n(2016) \nSAE \nLesion classification \nMRI \nLearn to extract features from multi-\nparametric MRI data, subsequently \ncreates a hierarchical classification to \ndetect prostate cancer. \nZhu et al. (2017) \n \nSAE \nDetection \nof \nHeart, \nkidney \nand \nliver \nlocation  \nMRI \nSAE used for acquisition of spatio-\ntemporal features on 2D along with time \nDCE-MRI \nShin et al. (2013) \nSAE \nCell segmentation \nDigital \npathology \nimage \nLearning spatial relationships \nHatipoglu, N. 2017 \nSAE \nSegmentation \nright \nventricle \nin \ncardiac \nMRI  \nMRI \nSAE applied to obtain an initial right \nventricle  segmentation. \nAvendi, M. 2017 \nSDAE \nCell segmentation \nDigital \npathology \nimage \nThe SDAE trained with data and their \nstructured labels for cell segmentation \nSu. H. at el 2018 \nSSAE \nAD \nMRI \nSSAE \nfor \nearly \ndetection \nof \nAlzheimer’s disease from brain MRI  \nLiu et al. (2014) \nSDAE \nBreast lesion \nUltrasound and \nCT \nStacked Denoising AE for Diagnosis of \nbreast nodules and lesions \nCheng et al. (2016) \nSDAE \nPatient clinical events \nPatient clinical \nhistory \nSDAE for an unsupervised  early \nprediction  of patients e future clinical \nevents and disease. \nMiotto et al. (2016) \nSDAE \n-- \nCT/MRI \nMulti-modal SDAE used to pre-train the \nDNN. \nCheng et al. (2018) \nDCAE \nModeling task fMRI \ntfMRI \nDeep Convolutional AE to model \ntfMRI. \nHuang et al. (2018) \n \nCAE \nAD/MCI/HC \nclassification \nfMRI \nCAE used to pre-train 3D CNN. \nHosseini-Asl \net \nal. \n(2016) \nCAE \nNucleus detection \nDigital \npathology \nimage \nSparse CAE to detect and encode nuclei \nand feature extraction from tissue \nsection images. \nHou et al. (2019) \n14 \n \n4.2. \nRestricted Boltzmann Machines  \nRestricted Boltzmann Machines (RBMs) are a variant of Markov Random Field (MRF), \nconstitute of single layer undirected graphical model with an input layer or visible layer x = (x1, \nx2...... xN) and a hidden layer h = {h1, h2, …. HM}. The connection between nodes/units are \nbidirectional, so each given input vector x can take the latent feature representation h and vice-\nversa. An RBM is a generative model which learns probability distribution over the given input \nspace and generates new data point (Yoo, et al. 2014). Illustration of a typical RBM  is shown in \nFig. 6(a). In fact, RBMs are restricted version of Boltzmann machines where neurons must form \nan arrangement of bipartite graphs. Due to this restriction, pairs of nodes belonging to each of the \nvisible and hidden nodes have a symmetric connection between them, and nodes within a group \nhave no internal connections.. This restriction makes RBM more efficient training algorithm than \nthe general case of Boltzmann machine. Hinton et al. (2010) proposed a practical guide to train \nRBMs.  \nRBMs have been utilized in various aspects of medical image analysis such as detection of \nvariations in Alzheimer disease (Brosch, et al. 2013), image segmentation (Yoo et al. 2014), \ndimensionality reduction (Cheng et al. 2016), feature learning (Pereira et al. 2018) and so on. A \nbrief account for the application of RMBs in medical image analysis is shown in Table 3. \nTable 3. Applications of RBM for medical image analysis \nMethod \nTask \nImage type \nRemarks \nReferences \nRBM \nAD \nMRI \nUses a large dataset of MRI to rule \nout the mode of variations in AD \nbrains. \nBrosch et al. (2013) \nRBM \nMultiple \nsclerosis \nlesions \n3DMRI \nUses multi-channel 3D MR images \nof multiple sclerosis (MS) lesion  \nfor MS segmentation \nYoo et al. (2014) \nRBM \nAD/MCI/HC \nclassification \nMRI, PET \nDBMs on multimodal images from \nMRI and PET scans for disease \nclassification. \nSuk et al. (2014) \nRBM \nMass detection in breast \ncancer \nMammography \nRBM \nbased \nmethod \nfor \noversampling and semi-supervised \nlearning to solve classification of \nimbalanced data with a few labeled \nsamples \nCao et al. (2015) \nRBM \nfMRI \nblind \nsource \nseparation \nfMRI \nRBM used for both internal and \nfunctional interaction-induced latent \nsource detection \nHuang et al. (2016) \nRBM \nVertebrae localization \nCT, MRI \nRBMs to locate the exact position of \nthe vertebrae. \nCai et al. (2016b) \nRBM \nBenign/Malignant \nclassification \nUltrasound \nShear wave elsatrography for class \nindication of benign and malignant \nmammary gland tumors using RBM. \nZhang et al. (2016a) \nRBM \nTongue \ncontour \nextraction \nUltrasound \nAnalysis of tongue motion during \nspeech, using auto encoders in \ncombination with RBM. \nJaumard-Hakoun et al. \n(2016) \nCRBM \nLung \ntissue \nclassification \nand \nairway detection \nCT \nDiscriminative \nand \ngenerative \nlearning by CRBM to develop filters \nfor \ndata \ntraining \nas \nwell \nclassification. \nVan Tulder & de Bruijne \n(2016) \nRBM \nCardiac \narrhythmia \nclassification \nECG \nAchieves \naverage \nrecognition \naccuracy \nfor \nventricular \nand \nsupraventricular \nectopic \nbeats \n(93.63% and 95.57%, respectively) \nfor \nCardiac \narrhythmia \nclassification. \nMathews et al. (2018) \nRBM \nBrain \nlesion \nsegmentation \nMRI \nRBM is used for feature learning, \nand a Random Forest as a classifier. \nPereira et al. (2018) \n \n15 \n \n \nFig. 6 (a)-(d) Diagrams showing various unsupervised network models \n4.3. \nDeep Belief Networks \nDeep Belief Networks (DBN) is a kind of neural network proposed by Bengio (2009). It is a \ngreedy layer-wise unsupervised learning algorithm with several layers of hidden variables \n(Hinton et al., 2016). Layer-wise unsupervised training (Bengio 2007) help the optimization \nand weight initialization for better generalization. In fact, DBN is a hybrid single \nprobabilistic generative model, like a typical RBM. In order to construct a deep architecture \nlike SAEs where AEs layers are replaced by RBMs, DBN has one lowest  visible layer v, \nrepresenting state of input data vector and a series of hidden layers h1, h2, h3, . . . hL. When \nmultiple RBMs are stacked hierarchically,  an undirected generative model is formed by top \ntwo layers and  directed generative model is formed by lower layers. Fig. 6(b), illustrates the \n16 \n \nstructure of DBN. The following function in DBN represents the joint distribution of visible \nunit v, hidden layers hl (l = 1, 2…. L : \nP(               )   (∏\n (       ) (      ( ))\n   \n   \n)             (14) \nHinton at el. (2006a) applied layer-wise training procedure, where lower layers learns low-\nlevel features and subsequently higher layers learns high-level features (Hinton at el. 1995). \nDBN are used to extract features from fMRI images (Plis et al., 2014), temporal ultrasound \n(Azizi et al. 2016), classify Autism spectrum disorders (Aghdam et al. 2018), and so on. \nSome of the applications of DBNs are presented in Table 4. \nTable 4. Applications of DBNs for medical image analysis \nMethod \nTask \nImage type \nRemarks \nReferences \nDBN \nAD/HC classification \nMRI \nDBNs with convolutional RBMs \nfor manifold learning \nBrosch & Tam (2013) \nDBN, \nConvolutional \nRBM \nManifold Learning \nMRI \nDBM along with convolutional \nRBM layers to efficiently train \nDBMs \nin \norder \nto \ndetect \nmorphological changes in brain in \nnormal \nas \nwell \nas \ndisease \nconditions \nBrosch et al. (2014) \nDBN \n \nMRI \nEvaluation of DBN to estimate \nbrain networks in neurocognitive \ndisorders \nlike \n \nHuntington’s \ndisease and Schizophrenia  \nPlis et al. (2014) \nDBN \nAD/MCI/HC \nclassification \nMRI \nA \ngroup \nof \nvoting \nschemes \nclubbed using an SVM to better \nclassify AD and MCI from brain’s \n3D gray mater images. \nOrtiz et al. (2016) \nDBN \nLeft \nventricle \nsegmentation \nUltrasound \nDBN assisted system exploiting \nnon-rigid registration, landmarks \nand patches to maneuver multi \natlas segmentation. \nCarneiro et al. (2012); \nCarneiro \n& \nNascimento (2013) \nDBN \nSchizophrenia/NH \nclassification \nMRI \nCharacterizing \ndifferences \nin \nmorphology \nof \nvarious \nbrain \nregions in schizophrenia using \nDBN and supervised fine tuning. \nPinaya et al. (2016) \nDBN \nLesion classification \nUltrasound \nTraining DBN to extract features \nfrom \nprostate \nultrasonography \nimages to classify benign and \nmalignant lesions. \nAzizi et al. (2016)  \nDBN \nLeft \nventricle \nsegmentation \nMRI \nThe combination of DBN and level \nset method to yield automated \nsegmentation of the left ventricle \nfrom cardiac cine MRI  \nNgo et al. (2017) \nDBN \nCardiac \narrhythmia \nclassification \nECG \nAchieves \naverage \nrecognition \naccuracy \nof \nventricular \nand \nsupraventricular \nectopic \nbeats \n(93.63% and 95.57%, respectively) \nfor \ncardiac \narrhythmia \nclassification. \nMathews et al. (2018) \nDBN \nAutism \nspectrum \ndisorders classification \nrs-fMRI, \nsMRI \nClassifies \nAutism \nspectrum \ndisorders (ASDs) in children using \nrs-fMRI and sMRI data on the \nbasis of Random Neural Network \nclustering. \nAghdam et al. (2018) \n \n \n17 \n \n4.4. \nDeep Boltzmann Machine \nDeep Boltzmann machine (DBM) is a robust deep learning model proposed by Salakhutdinov \net al. (2009) and Salakhutdinov et al. (2012). They stacked multiple RBMs in a hierarchal \nmanner to handle ambiguous input robustly. Fig. 6(c) represents the architecture of DBM as a \ncomposite model of RBMs which clearly shows how DBM differ from DBN.  Unlike DBNs,  \nDBMs form undirected generative model combining information from both lower and upper \nlayers which improves the representation power of DBMs. Training of layer-wise greedy \nalgorithm for DBM (Salakhutdinov et al., 2015; Goodfellow et al., 2013b) is calculated by \nmodifying in procedure of DBN. \nRecently, a three-layer DBM was presented by Salakhutdinov et al. (2015) and Dinggang et \nal. (2017). In this three-layer DBM, to learn parameters    *     +  the values of \nneighbour layer(s) and probability of visible and hidden units are computed using logistic \nsigmoidal function.  The derivative of log likelihood of the observation ( ) with respect to \nthe model parameter( ) is computed as, \n \n  ( )    (   )        ,    (  )  -         ,    (  ) -                           (  ) \n Where      [.] denote data-dependent obtained from visible units and       [.] denote \ndata-independence obtained from the model. Some of the applications of DBMs are shown in \nTable 5. \nTable 5. Applications of DBMs for medical image analysis \nMethod \nTask \nImage type \nRemarks \nReferences \nDBM \nHeart motion tracking \n \nMRI \nUsing \nthree-layered \nDeep \nBoltzmann Machine to guide \nframe-by-frame \nheart \nsegmentation during radiation \ntherapy of cancer patient on cine \nMRI images. \nWu, et al., (2018) \nDBN \nAD/HC classification \nMRI \nDBN \ncombined \nwith \nconvolutional \nRBMs \nfor \nmanifold learning. \nBrosch \n& \nTam \n(2013) \nRBM \nBreast-image \nclassification \n \nMRI \nRestricted Boltzmann machine \nwith backpropagation have been \nused for histopathological breast-\nimage classification \nNahid et al., (2018) \nDBM \nMedical image retrieval \n \nMulti digital \nimage \nDBM based multi model learning \nto learn joint density model. \n \nCao, et al., (2014) \n4.5. \nGenerative Adversarial Network (GAN) \nGenerative Adversarial Network (GAN) (Goodfellow, et al. 2014) is one of recent promising \ntechnique for building flexible deep generative unsupervised architecture. Goodfellow et al. \n(2014) proposed two models generative model G and Discriminative model D, where G \ncapture data distribution (pg)over real data t, and D estimates the probability of a sample \ncoming from training data (m) not from G. In every iteration, backpropagation generator and \ndiscriminator competing with each other. The training procedure the probability of D is \n18 \n \nmaximized. This framework functions like a mini-max two-player game. The value function \nV(G, D) establishes following two-player mini-max game is given by, \n   \n⏟\n \n   \n⏟\n \n  (   )           ,    ( )-        ( ) 0    .   ( ( ))/1           (  ) \nWhere D(t) represents the probability of t from data m and pdata is distribution of real-world \ndata. This model seems to be stable and improved as pg = pdata. A typical architecture of GAN \nis depicted in Fig. 6(d). In fact, these two adversaries, Generator and Discriminator, \ncontinuously battle during the processing of training. GAN have been applied to generate \nsamples of photorealistic images to visualize new designs. Some of the applications of GAN \nfor medical image analysis are presented in Table 6. \nTable 6. Applications of GAN for medical image analysis \nMethod \nTask \nImage \ntype \nRemarks \nReferences \nGAN \nSynthesis of retinal \nimages \nRetinal \nimages \n \nMI-GAN generates precise \nsegmented images for the \napplication of supervised \nlearning of retinal images. \nIqbal & Ali (2018) \n \nGAN \nChest X-ray \nX-ray \nGAN used to produce \nphotorealistic images which \nretain pathological quality \nCanas, et al, (2018) \n \nDual \nGAN-\nFCN \nSegmentation \nof \nregions of interest \n(ROIs) \n --- \nImprove  GAN using dual-\npath adversarial learning for \nFully Convolutional Network \nbased image segmentation \nBi, et al., (2018) \nGAN \nSimulation \nof \nB-\nmode \nultrasound \nimages \n \nUltrasound \n \nConditional generative \nadversarial networks used to \nsimulate ultrasound images at \ngiven 3D spatial locations. \nHu et al., (2017) \nGAN \nTreatment \nof \nlymphomas and lung \ncancer \nPET \nMulti-channel generative \nadversarial networks used to \nsynthesize PET data. \nBi, et al., (2017) \n \n5. List of software tools/packages and benchmark datasets \nA plethora of software tools and packages implementing unsupervised learning models (as \ndiscussed in the paper) has been developed and made available to the research community \nand data analysts. Some of the tools/packages and medical images benchmark datasets are \nlisted in Table 7 and Table 8, respectively.  \n \n \n19 \n \nTable 7. List of software tools/packages for unsupervised learning models \nS. No. \nTools/ Packages \nName \nModels/ \nMethods \nDescription \nLanguage \n/Technology \nURL \n1. deeplearning4\nj \nAutoencoders \nDeep learning APIs for Java \nhaving an implementation of \nseveral deep learning techniques. \nJava \nhttps://deeplearning4j.org/  \n2. unsup under \ntorch7 \nAutoencoder, \netc.  \nA scientific computing \nframework with good support for \nmachine learning algorithms that \nputs GPUs first. Unsup package \nprovides few unsupervised \nlearning algorithms such as \nautoencoders, clustering, etc. \nLua \nhttps://github.com/torch/to\nrch7 \n3. DeepPy \nAutoencoders \nMIT licensed deep learning \nframework that runs on CPU or \nGPUs and implements \nautoencoders, in addition to other \nsupervised learning algorithms. \nPython \nhttps://github.com/andersb\nll/deeppy \nhttp://andersbll.github.io/d\neeppy-website/ \n4. SAENET.train \nStacked \nautoencoder \nBuild a stacked autoencoder in R \nenvironment for pre-training of \nfeed-forward NN and dimension \nreduction of features. \nR package \nhttps://rdrr.io/cran/SAEN\nET/man/SAENET.train.ht\nml  \n5. kdsb17 \nConvolutional \nautoencoder \nGaussian Mixture Convolutional \nAutoencoder (GMCAE) used for \nCT lung scan using \nKeras/TensorFlow \nPython, \nKeras, \nTensor-flow-\ngpu \nhttps://github.com/alegonz\n/kdsb17  \n \n6. autoencoder \nDeep \nautoencoder \nTraining a deep autoencoder for \nMNIST digits datasets \nMatlab \nhttp://www.cs.toronto.edu/\n~hinton/code/Autoencoder\n_Code.tar  \n7. H2O \nDeep \nautoencoder \nParallelized implementations of \nmany supervised and \nunsupervised machine learning \nalgorithms, including GLM, \nGBM, RF, DNN, K-Means, \nPCA,Deep AE, etc. \nR package \nhttps://cran.r-\nproject.org/web/packages/\nh2o/ \n8. dbn \nDBN \nDeep belief network pre-train in \nunsupervised manner with stacks \nof RBM, which in return fine-\ntuned DBN. \nR package \nhttps://rdrr.io/github/Timo\nMatzen/RBM/src/R/DBN.\nR  \n9. darch \nDBN, RBM \nRestricted Boltzmann machine, \ndeep belief network \nimplementation \nR package \nhttps://github.com/maddin\n79/darch  \n10. deepnet \nDBN, RBM, \ndeep \nautoencoders \nImplementation of RBM, DBN, \ndeep stacked autoencoders \nR package \nhttps://cran.r-\nproject.org/web/packages/\ndeepnet/  \n11. Vulpes \nDBN \nDBN and other deep learning \nimplementation in F#. \nVisual \nStudio \nhttps://github.com/fsproje\ncts/Vulpes \n12. pydbm \nDBM/ RBM \nRBM/DBM are implemented in \npython for pre-learning or \ndimension reduction \nPython \nhttps://pypi.org/project/py\ndbm/  \n13. RBM \nRBM \nSimple RBM implementation in \nPython \nPython \nhttps://github.com/echen/r\nestricted-boltzmann-\nmachines  \n14. xRBM \nRBM and its \nvariants \nImplementation of RBM and its \nvariants in Tensorflow \nPython \nhttps://github.com/omimo/\nxRBM  \n15. DCGAN.torch \nGAN \nUnsupervised representation \nlearning using Deep \nConvolutional GAN \nLua \nhttps://github.com/soumit\nh/dcgan.torch  \n16. pix2pix \n \nGAN \nConditional Adversarial Networks \nfor Image-to-image translation \nsynthesizing from the image. \nLinux Shell \nScript \nhttps://github.com/phillipi/\npix2pix  \n17. ebgan \nGAN \nEnergy-based GAN equivalent to \nprobabilistic GANs produces high \nresolution images. \nPython \nhttps://github.com/eriklind\nernoren/PyTorch-\nGAN/tree/master/impleme\nntations/ebgan  \n20 \n \nTable 8. List of benchmark medical image datasets \n[Abbreviations. ADNI: Alzheimer’s Disease Neuroimaging Initiative; ABIDE: Autism Brain Imaging Data Exchange; DICOM: Digital \nImaging and Communications in Medicine; BCDR: Breast Cancer Digital Repository; CIVM: Center for in Vivo Microscopy; DDSM: \nDigital Database for Screening Mammography; DRIVE: Digital Retinal Images for Vessel Extraction; IDA: Image & Data Archive; ISDIS: \nInternational Society for Digital Imaging of the Skin; NBIA: National Biomedical Imaging Archive; OASIS: Open Access Series of \nImaging Studies; TCGA: The Cancer Genome Atlas; TCIA: The Cancer Imaging Archive] \nS. No. \nData set \nModalities \nMedical \ncondition \nAccessibility \nURL \n1. \nABIDE \nMRI \nAutism spectrum \ndisorder  \nOpen access \n \nhttp://fcon_1000.projects.nitrc.org/indi/abide/ \n2. \nADNI \nMRI \nAlzheimer’s \ndisease \nPaid  \nhttp://adni.loni.usc.edu/data-samples/access-\ndata/ \n3. \nBCDR \nMammography \nBreast cancer \nOpen access \n \nhttps://bcdr.eu/ \n4. \nCIVM \n3D-MRM \nHistology of the \nEmbryonic and \nNeonatal Mouse \nLimited \naccess \nhttp://www.civm.duhs.duke.edu/devatlas/ \n \n5. \nDDSM \nMammography \nBreast cancer \nOpen access \nhttp://marathon.csee.usf.edu/Mammography/\nDatabase.html \n6. \nDermNet \nPhoto \ndermatology \nA huge database \nof various skin \ndiseases \nLimited \naccess \nhttp://www.dermnet.com/ \n7. \nDICOM  \nMRI, CT, etc. \nA variety of \nmedical images, \nvideos and signal \nfiles \nOpen access \nhttps://www.dicomlibrary.com  \n8. \nDRIVE \n2D color \nimages of \nretina \nRetinal blood \nvessel \nsegmentation to \nstudy diabetic \nretinopathy \nOpen access \n \nhttp://www.isi.uu.nl/Research/Databases/DRI\nVE/download.php \n \n9. \nIDA \n \nAn online \nresource for \nneuroscience \nimages \nOpen access \nhttps://ida.loni.usc.edu/  \n10. ISDIS \nDermoscopy, \ntelemedicine, \nspectroscopy \netc. \nSkin disease \nPaid \nhttps://isdis.org/ \n11. MedPix \nVariety of \nimaging data  \nOnline database \nof medical \nimages, teaching \ncases, and clinical \ntopics \nOpen access \nhttps://medpix.nlm.nih.gov  \n12. NBIA \nCT, PT, MRI, \netc. \nA database of the \nNational Cancer \nInstitute proving \nmedical images of \nvarious conditions \nand anatomical \nsites. \nLimited/ \nopen access \nhttps://imaging.nci.nih.gov/  \n13. OASIS \nMRI and PET \nNormal aging or \nmild to moderate \nAlzheimer's \nDisease  \nOpen access \n \nhttp://www.oasis-brains.org/ \n14. TCIA \nCollection of \nMRI, CT etc. \nMultimodal image \narchive for \nvarious types of \ncancer  \nLimited/ \nopen access \nhttp://www.cancerimagingarchive.net/ \n15. TCGA \nHistopathology \nslide images \nHistopathology \nslide images from \nsample portions of \nvarious types of \ncancers \nOpen \nhttps://cancergenome.nih.gov/  \n21 \n \n6. Discussion, opportunities and challenges \nMedical imaging and diagnostic techniques are one of the most widely used for early \ndetection, diagnosis and treatment of complex diseases. After significant advancement in \nmachine learning and deep learning (both supervised and unsupervised), there is a paradigm \nshift from the manual interpretation of medical images by human experts such as radiologists \nand physicians to an automated analysis and interpretation, called computer-assisted \ndiagnosis (CAD). As unsupervised learning algorithms can derive insights directly from data, \nuse them for data-driven decisions making, and are more robust, hence they can be utilized as \nthe holy grail of learning and classification problems. Furthermore, these models are also \nutilized for other important tasks including compression, dimensionality reduction, denoising, \nsuper resolution and some degree of decision making.  \nUnsupervised learning and CAD, both being in its infancy, researchers and practitioners have \nmuch opportunity in this area. Some of them are: (i) Allow us to perform exploratory analysis \nof data (ii) Allow to be used as preprocessing for supervised algorithm, when it is used to \ngenerate a new representation of data which ensure learning accuracy and reduces memory \ntime overheads. (iii) Recent development of cloud computing, GPU-based computing, \nparallel computing and its cheaper cost allow big data processing, image analysis and execute \ncomplex deep learning algorithms very easily.  \nSome of the challenges and research directions are: \n(i) Difficult to evaluate whether algorithm has learned anything useful: Due to lack of \nlabel in unsupervised learning, it is nearly impossible to quantify its accuracy. For instance, \nhow can we access whether K-means algorithm found the right clusters? In this direction, \nthere is a need to develop algorithms which can give an objective performance measure in \nunsupervised learning. \n(ii) Difficult to select right algorithm and hardware: Selection of right algorithm for a \nparticular type of medical image analysis is not a trivial task because performances of the \nalgorithm are highly dependent on the types of data. Similarly, hardware requirement also \nvaries from problem to problem.  \n(iii) Will unsupervised learning work for me? It is mostly asked question, but its answer \ntotally depends on the problem at hand. In image segmentation problem, clustering algorithm \nwill only work if the images do fit into naturals groups. \n(iv) Not a common choice for medical image analysis: Unsupervised learning is not a \ncommon choice for medical image analysis. However, from literature it is revealed that these \n(autoencoders and its variants, DBN, RBM, etc.) are mostly used to learn the hierarchy level \nof features for classification tasks. It is expected that unsupervised learning will play pivotal \nrole in solving complex medical imaging problems which are not only scalable to large \namount of unlabeled data, but also suitable for performing unsupervised and supervised \nlearning tasks simultaneously (Yi et al., 2018). \n22 \n \n(v) Development of patient-specific anatomical and organ model: Anatomical skeletons \nplay crucial role in understanding diseases and pathology. Patient-specific anatomical model \nis frequently used for surgery and interventions. They help to plan procedure, perform \nmeasurement for device surging, and predict the outcome of post-surgery complexities. \nHence, the algorithm needs to be developed to construct patient-specific anatomical and \norgan model from medical images. \n(vi) Heterogeneous image data: In the last two to three decades, more emphasis was given \nto well-defined medical image analysis applications, where developed algorithms were \nvalidated on well-defined types of images with well-defined acquisition protocol. The \nalgorithms are required, which can work on more heterogeneous data. \n(vii) Semantic segmentation of images: Semantic segmentation is task of complete scene \nunderstanding, leading to knowledge inference from imagery. Scene understanding is a core \nof computer vision problems which has several applications, including human-computer \ninteraction, self-driving vehicles, virtual reality, and medical image analysis. The semantic \nsegment of medical images with acceptable accuracy is still challenging.  \n(viii) Medical video transmission: Enabling 3D video in recently adopted telemedicine and \nU-healthcare applications result in more natural viewing conditions and better diagnosis. \nAlso, remote surgery can be benefited from 3D video because of additional dimensions of \ndepth. However, it is crucial to transmit data-hungry 3D medical video stream in real-time \nthrough limited bandwidth channels. Hence, efficient encoding and decoding techniques for \n3D video data transmission is required.  \n(ix) Need extensive inter-organizational collaborations: Inter-professional and inter-\norganizational collaboration is important for better functioning of the health care system, \neliminating some of the pitfalls such as limited resources, lack of expertise, aging \npopulations, and combat chronic diseases  (Karam et al., 2017). Medical image based CAD \nneeds extensive inter-organizational collaborations among doctors, radiologists, medical \nimage analysts, and computational data analysts.   \n(x) Need to capitalize big medical imaging market: According to IHS Markit report \n(https://technology.ihs.com.), medical imaging market has total global revenue of $21.2 \nbillion in 2016, which is forecasted to touch $24.0 billion by 2020. According to WHO, \nglobal population will rise from 12% to 22% from 2015 to 2050. Population aging lead to \nincreased rate of chronic diseases globally and hence there is a need to capitalize a big \nmedical imaging market worldwide.  \n(xi) Black-box and its acceptance by health professionals: Machine learning algorithms \nare boon which solves the problems earlier thought to be unsolvable, however, it suffers from \nbeing “black-box”, i.e., how output arrives from the model is very complicated to interpret. \nParticularly, deep learning models are almost non-interpretable and but still being used for \ncomplex medical image analysis. Hence, its acceptance by health professionals is still \nquestionable.  \n23 \n \n(xii) Will technology replace radiologists? For the processing of medical images, deep \nlearning algorithms help select and extract important features and construct new ones, leading \nto new representation of images, not seen before. For image interpretation side, deep learning \nhelps identify, classify, quantify disease patterns, allow measure predictive targets, and make \npredictive models, and so on. So, will technology “replace radiologists”, or migrate to \n“virtual radiologist assistant” in near future? Hence, following slogan is quite relevant in this \ncontext: “Embrace it, it will make you stronger; reject it, it may make you irrelevant”. \nIn a nutshell, unsupervised learning is very much open topic where researchers can make \ncontributions by developing a new unsupervised method to train how network (e.g. Solve a \npuzzle, generate image patterns, image patch comparison, etc.) and re-thinking of creating a \ngreat unsupervised feature representation, (e.g. What is the object and what is the \nbackground?), nearly analogous to the human visual system.   \n7. Conclusion \nMedical imaging is one of the important techniques for early detection, diagnosis and \ntreatment of complex diseases. Interpretation of medical images is usually performed by \nhuman experts such as radiologists and physicians. After the success of machine learning \ntechniques, including deep learning, availability of cheap computing infrastructure through \ncloud computing, there has been a paradigm shift in the field of computer-assisted diagnosis \n(CAD). Both supervised and unsupervised machine learning approaches are widely applied in \nmedical image analysis, each of them with their own pros and cons. Due to the fact that \nhuman supervisions are not always available or inadequate or biased, therefore, unsupervised \nlearning algorithms, including its deep architecture, give a big hope with lots of advantages.  \nUnsupervised learning algorithms derive insights directly from data, and use them for data-\ndriven decisions making. Unsupervised models are more robust and they can be utilized as \nthe holy grail of learning and classification problems. These models are also used for other \ntasks including compression, dimensionality reduction, denoising, super resolution and some \ndegree of decision making. Therefore, it is better to construct a model without knowing what \ntasks will be at hand and we would use representation (or model) for. In a nutshell, we can \nthink of unsupervised learning as preparation (preprocessing) step for supervised learning \ntasks, where unsupervised learning of representation may allow better generalization of a \nclassifier. \nAcknowledgements \nAuthors would like to thank Ms. Sahar Qazi, Ms. Almas Jabeen, and Mr. Nisar Wani for \nnecessary support. \nConflict of Interest Statement \nAuthors declare that there is no any conflict of interest in the publication of this manuscript. \n \n \n24 \n \nReferences \nAghdam, M. A., Sharifi, A., & Pedram, M. M. (2018). Combination of rs-fMRI and sMRI Data to Discriminate \nAutism Spectrum Disorders in Young Children Using Deep Belief Network. Journal of digital imaging, \n1-9. https://doi.org/10.1007/s10278-018-0093-8  \nAvendi, M. R., Kheradvar, A., &Jafarkhani, H. (2017). Automatic segmentation of the right ventricle from \ncardiac MRI using a learning-based approach. Magnetic Resonance in Medicine, 78(6), 2439–2448. \nhttps://doi.org/10.1002/mrm.26631  \nAzizi, S., Imani, F., Ghavidel, S., Tahmasebi, A., Kwak, J.T., Xu, S., Turkbey, B., Choyke, P., Pinto, P., Wood, \nB., Mousavi, P., Abolmaesumi, P. (2016). Detection of prostate cancer using temporal sequences of \nultrasound data: a large clinical feasibility study. Int. J. Comput. Assist. Radiol. Surg. 11 (6), 947–956. \nhttps://doi.org/10.1007/s11548-016-1395-2  \nBallard, D. H. (1987). Modular Learning in Neural Networks. In AAAI (pp. 279-284). \nBengio Y, Courville A, Vincent P. (2013).  Representation learning: a review and new perspectives. IEEE Trans \nPattern Anal Mach Intell. 35:1798–828. https://doi.org/10.1109/TPAMI.2013.50  \nBengio, Y. (2009). Learning Deep Architectures for AI. Foundations and Trends in Machine Learning, 2(1), 1-\n127. https://doi.org/10.1561/2200000006  \nBengio, Y., Lamblin, P. , Popovici, D. , Larochelle, H.  (2007). Greedy layer-wise training of deep networks. In: \nProceedings of the Advances in Neural Information Processing Systems, pp. 153–160. \nBenou, A., Veksler, R. , Friedman, A. , Raviv, T.R. (2016). De-noising of contrast-enhanced MRI sequences by \nan ensemble of expert deep neural networks. In: Deep Learning and Data Labeling for Medical \nApplications (pp. 95-110). Springer, Cham. https://doi.org/10.1007/978-3-319-46976-8_11  \nBi, L., Feng, D., Kim, J. (2018). Dual-Path Adversarial Learning for Fully Convolutional Network (FCN)-Based \nMedical Image Segmentation, Visual Computer, 34(6-8), 1043-1052. https://doi.org/10.1007/s00371-\n018-1519-5  \nBi, L., Kim, J., Kumar, A., Feng, D., Fulham, M. (2017). Synthesis of positron emission tomography (PET) \nimages via multi-channel generative adversarial networks (GANs). Lecture Notes in Computer Science, \n10555, pp. 43-51. https://doi.org/10.1007/978-3-319-67564-0_5  \nBishop CM. (2006). Pattern Recognition and Machine Learning (Information Science and Statistics), 1st edn. \nSpringer, New York.. \nBourlard H, Kamp Y. (1988). Auto-association by multilayer perceptrons and singular value decomposition. \nBiological Cybernetics, 59, 291–94. https://doi.org/10.1007/BF00332918  \nBrosch, T., Tam, R. (2013). Manifold learning of brain MRIs by deep learning. In: Proceedings of the Medical \nImage Computing and Computer-Assisted Interven- tion. In: Lecture Notes in Computer Science, 8150, \npp. 633–640. https://doi.org/10.1007/978- 3- 642-40763-5_78  \nBrosch, T., Yoo, Y., Li, D. K. B., Traboulsee, A., Tam, R. (2014). Modeling the variability in brain morphology \nand lesion distribution in multiple sclerosis by deep learning. In: Med Image Comput Comput Assist \nInterv. Lecture Notes in Computer Science, 8674 (pp. 462–469).  \nCai, Y., Landis, M., Laidley, D. T., Kornecki, A., Lum, A., Li, S. (2016b). Multi-modal vertebrae recognition \nusing transformed deep convolution network. Comput Med Imaging Graph, 51, 11–19. \nCanas, K., Liu, X., Ubiera, B., Liu, Y. (2018). Scalable biomedical image synthesis with GAN. ACM \nInternational Conference Proceeding Series, art. no. a95. https://doi.org/10.1145/3219104.3229261  \nCao, P., Liu, X., Bao, H., Yang, J., & Zhao, D. (2015). Restricted Boltzmann machines based oversampling and \nsemi-supervised learning for false positive reduction in breast CAD. Bio-medical materials and \nengineering, 26(s1), S1541-S1547. https://doi.org/10.3233/BME-151453  \nCao, Y., Steffey, S., He, J., Xiao, D., Tao, C., Chen, P., Müller, H. (2014). Medical image retrieval: A \nmultimodal approach. Cancer Informatics, 125-136. https://doi.org/10.4137/CIN.S14053  \n25 \n \nCarneiro, G., Nascimento, J.C. (2013). Combining multiple dynamic models and deep learning architectures for \ntracking the left ventricle endocardium in ultrasound data. IEEE Trans. Pattern Anal. Mach. Intell. 35, \n2592–2607. https://doi.org/10.1109/TPAMI. 2013.96  \nCarneiro, G., Nascimento, J.C., Freitas, A. (2012). The segmentation of the left ven- tricle of the heart from \nultrasound data using deep learning architectures and derivative-based search methods. IEEE \nTransactions on Image Processing, 21(3), 968–982. https://doi.org/10.1109/TIP.2011.2169273   \nChan, T. H., Jia, K., Gao, S., Lu, J., Zeng, Z., & Ma, Y. (2015). PCANet: A simple deep learning baseline for \nimage \nclassification?. \nIEEE \nTransactions \non \nImage \nProcessing, \n24(12), \n5017-5032. \nhttps://doi.org/10.1109/TIP.2015.2475625  \nCheng J-Z, Ni D, Chou Y-H, et al. (2016). Computer-aided diagnosis with deep learning architecture: \napplications to breast lesions in US images and pulmonary nodules in CT scans. Scientific Reports, 6, \n24454. https://doi.org/10.1038/srep24454  \nCheng, Li Zhang & Yefeng Zheng. (2018). Deep similarity learning for multimodal medical images, Computer \nMethods in Biomechanics and Biomedical Engineering: Imaging & Visualization, 6:3, 248-252. \nhttps://doi.org/10.1080/21681163.2015.1135299  \nDinggang S. & Wu, Guorong& Suk, Heung-Il. (2017). Deep Learning in Medical Image Analysis. Annual \nreview of biomedical engineering, 19. https://doi.org/10.1146/annurev-bioeng-071516-044442  \nDrineas, Petros & Frieze, Alan & Kannan, Ravindran & Vempala, Santosh & Vinay, V. (1999). Clustering in \nLarge Graphs and Matrices. In Proceedings of the 10th ACM-SIAM Symposium on Discrete \nAlgorithms(pp. 291-299). \nFischer, P. Pohl, T. Faranesh, A., Maier, A. and Hornegger, J. (2017). Unsupervised Learning  for Robust \nRespiratory Signal Estimation From X-Ray Fluoroscopy, IEEE Transactions on Medical Imaging, 36(4), \n865-877. https://doi.org/10.1109/TMI.2016.2609888  \nGallinari, Y. LeCun, S. Thiria, and F. Fogelman-Soulie. Memoires associatives distribuees. In Proceedings of \nCOGNITIVA 87, Paris, La Villette, 1987 \nGoodfellow, I. J., Mirza, M., Courville, A., and Bengio, Y. (2013b). Multi-prediction deep Boltzmann \nmachines. In NIPS’2013. \nGoodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, SherjilOzair, Aaron Courville, \nand YoshuaBengio. (2014). Generative adversarial nets. In Advances in Neural Information Processing \nSystems. Curran Associates, 2672–2680. \nGoodfellow, Quoc Le, Andrew Saxe, and Andrew Ng. (2009). Measuring invariances in deep networks. In \nYoshua Bengio, Dale Schuurmans, Christopher Williams, John Lafferty, and Aron Culotta, editors, \nAdvances in Neural Information Processing Systems 22  NIPS’09 , pages 646–654. \nGuo, X., Liu, X., Zhu, E., & Yin, J. (2017, November). Deep clustering with convolutional autoencoders. \nIn International Conference on Neural Information Processing (pp. 373-382). Springer, Cham. \nGuo, Y., Wu, G., Commander, L. A., Szary, S., Jewells, V., Lin, W., & Shent, D. (2014). Segmenting \nhippocampus from infant brains by sparse patch matching with deep-learned features. Medical image \ncomputing and computer-assisted intervention : MICCAI ... International Conference on Medical Image \nComputing and Computer-Assisted Intervention, 17(Pt 2), 308-15. \nHatipoglu, N., &Bilgin, G. (2017). Cell segmentation in histopathological images with deep learning algorithms \nby utilizing spatial relationships. Medical & Biological Engineering & Computing, 55(10), 1829–1848. \nhttps://doi.org/10.1007/s11517-017-1630-1  \nHinton G, Dayan P, Frey B,NealR. (1995).  he “wake–sleep” algorithm for unsupervised neural networks. \nScience, 268:1158–61. https://doi.org/10.1126/science.7761831  \nHinton GE, Salakhutdinov RR. (2006). Reducing the dimensionality of data with neural networks. Science \n313:504–7. https://doi.org/10.1126/science.1127647  \nHinton, G. , 2010. A practical guide to training restricted boltzmann machines. Momentum 9 (1), 926 \n26 \n \nHinton, G.E., Osindero, S., Teh, Y.-W., 2006a. A fast learning algorithm for deep belief nets. Neural Comput. \n18, 1527–1554. https://doi.org/10.1162/neco.2006.18.7.1527  \nHosseini-Asl, E., Gimel’farb, G., El-Baz, A. (2016). Alzheimer’s disease diagnostics by a deeply supervised \nadaptable 3D convolutional network. arxiv: 1607.00556 . \nHou, L., Nguyen, V., Kanevsky, A. B., Samaras, D., Kurc, T. M., Zhao, T., ... & Saltz, J. H. (2019). Sparse \nAutoencoder for Unsupervised Nucleus Detection and Representation in Histopathology Images. Pattern \nRecognition, 86: 188-200. https://doi.org/10.1016/j.patcog.2018.09.007  \nHu, Y., Gibson, E., Lee, L.-L., Xie, W., Barratt, D.C., Vercauteren, T., Noble, J.A. (2017). Freehand ultrasound \nimage simulation with spatially-conditioned generative adversarial networks. Lecture Notes in Computer \nScience, 10555 (pp. 105-115). https://doi.org/10.1007/978-3-319-67564-0_11  \nHuang, H., Hu, X., Han, J., Lv, J., Liu, N., Guo, L., Liu, T., 2016. Latent source mining in FMRI data via deep \nneural network. In: Proceedings of the IEEE International Symposium on Biomedical Imaging, pp. 638–\n641. https://doi.org/10.1109/ISBI.2016.7493348   \nHuang, H., Hu, X., Zhao, Y., Makkie, M., Dong, Q., Zhao, S., ... & Liu, T. (2018). Modeling task fMRI data via \ndeep convolutional autoencoder. IEEE transactions on medical imaging, 37(7), 1551-1561. \nIqbal, T., Ali, H. Generative Adversarial Network for Medical Images (MI-GAN) (2018) Journal of Medical \nSystems, 42 (11), art. no. 231. https://doi.org/10.1007/s10916-018-1072-9  \nJabeen, A., Ahmad, N., & Raza, K. (2018). Machine Learning-Based State-of-the-Art Methods for the \nClassification of RNA-Seq Data. In Classification in BioApps (pp. 133-172). Springer, Cham. \nhttps://doi.org/10.1007/978-3-319-65981-7_6  \nJain K. (2010). Data clustering: 50 years beyond K-means. Pattern Recogn. Lett. 31, 8 (June 2010), 651-\n666. https://doi.org/10.1016/j.patrec.2009.09.011  \nJain K., Karthik Nandakumar, and Abhishek Nagar. (2008). Biometric Template Security. EURASIP Journal on \nAdvances \nin \nSignal \nProcessing \nVolume \n2008, \nArticle \nID \n579416, \n17 \npages. \nhttps://doi.org/10.1155/2008/579416  \nJain K., Murty, M & J. Flynn, Patrick. (1999). Data clustering: a review. ACM Comput Surv. ACM Comput. \nSurv.. 31. 264-323. https://doi.org/10.1145/331499.331504  \nJanowczyk, A. , Basavanhally, A. , Madabhushi, A. (2017). Stain normalization using sparse autoencoders \n(STANOSA): application to digital pathology. Comput. Med. Imaging Graph 57, 50–61. \nhttps://doi.org/10.1016/j.compmedimag.2016.05.003  \nJaumard-Hakoun, A., Xu, K., Roussel-Ragot, P., Dreyfus, G., Denby, B. (2016). Tongue contour extraction \nfrom ultrasound images based on deep neural network. arxiv: 1605.05912 . \nJunbo Zhao, Michael Mathieu and Yann LeCun, (2017). Energy-Based Generative Adversarial Networks. ICLR \n2017, arXiv:1609.03126v4 \nKallenberg, M., Petersen, K., Nielsen, M., Ng, A., Diao, P., Igel, C., Vachon, C., Hol- land, K., Karssemeijer, \nN., Lillholm, M., 2016. Unsupervised deep learning ap- plied to breast density segmentation and \nmammographic \nrisk \nscoring. \nIEEE \nTrans. \nMed. \nImaging \n35, \n1322–1331. \nhttps://doi.org/10.1109/TMI.2016.2532122  \nKaram, M., Brault, I., Van Durme, T., & Macq, J. (2017). Comparing interprofessional and interorganizational \ncollaboration in healthcare: A systematic review of the qualitative research. International journal of \nnursing studies. \nKarim Armanious, Chenming Yang, Marc Fischer, Thomas K¨ustner, Konstantin Nikolaou, Sergios Gatidis, and \nBin Yang. MedGAN: Medical Image Translation using GANs. Journal Of Latex Class Files, Vol. 14, No. \n8, August 2015. \nKingma and Max Welling. 2013. Auto-encoding variational bayes. CoRRabs/1312.6114 (2013). Retrieved from \nhttp://arxiv.org/abs/1312.6114  \n27 \n \nLi, F., Qiao, H., Zhang, B., Xi, X. (2017). Discriminatively boosted image clustering with fully convolutional \nauto-encoders. arXiv preprint arXiv:1703.07980. \nLitjens, G., Kooi, T., Bejnordi, B. E., Setio, A. A. A., Ciompi, F., Ghafoorian, M., ... & Sánchez, C. I. (2017). A \nsurvey on deep learning in medical image analysis. Medical image analysis, 42, 60-88.. \nhttps://doi.org/10.1016/j.media.2017.07.005  \nLiu S, Liu S, Cai W, et al. Early diagnosis of Alzheimer’s disease with deep learning. In: International \nSymposium on Biomedical Imaging, Beijing, China 2014, 1015–18. \nMakhzani, A. & Frey, B. (2013). k-Sparse Autoencoders. arxiv: preprint: 1312.5663. \nMansoor, A., Cerrolaza, J., Idrees, R., Biggs, E., Alsharid, M., Avery, R., Linguraru, M.G., 2016. Deep learning \nguided partitioned shape model for anterior visual path- way segmentation. IEEE Trans. Med. Imaging \n35 (8), 1856–1865. https://doi.org/10.1109/TMI.2016.2535222   \nMathews, S. M., Kambhamettu, C., & Barner, K. E. (2018). A novel application of deep learning for single-lead \nECG \nclassification. Computers \nin \nbiology \nand \nmedicine, \n99:53-62. \nhttps://doi.org/10.1016/j.compbiomed.2018.05.013  \nMinh H.Q., Niyogi P., Yao Y. (2006). Mercer’s  heorem, Feature Maps, and Smoothing. In: Lugosi G., Simon \nH.U. (eds) Learning Theory. COLT 2006. Lecture Notes in Computer Science, vol 4005. Springer, \nBerlin, Heidelberg. https://doi.org/10.1007/11776420_14  \nMiotto R, Li L, Kidd BA, et al. (2016). Deep patient: an unsupervised representation to predict the future of \npatients \nfrom \nthe \nelectronic \nhealth \nrecords. \nScientific \nReports, \n6:26094. \nhttps://doi.org/10.1038/srep26094  \nNahid, A.-A., Mikaelian, A., Kong, Y. (2018). Histopathological breast-image classification with restricted \nBoltzmann machine along with backpropagation. Biomedical Research, 29(10), 2068-2077. \nhttps://doi.org/10.4066/biomedicalresearch.29-17-3903  \nNg, A. (2013). Sparse autoencoder lecture notes. Source: web.stanford.edu/class/cs294a/sparseAutoencoder.pdf \nNgo, T.A., Lu, Z., Carneiro, G. (2017). Combining deep learning and level set for the au- tomated segmentation \nof the left ventricle of the heart from cardiac cine mag- netic resonance. Med. Image Anal. 35, 159–171. \nhttps://doi.org/10.1016/j.media.2016.05.009  \nOrtiz, A., Munilla, J., Górriz, J.M., Ramírez, J. (2016). Ensembles of deep learning architectures for the early \ndiagnosis of the Alzheimer’s disease. International Journal of Neural Systems, 26(7), 1650025. \nhttps://doi.org/10.1142/S0129065716500258  \nPartaourides, Harris; Chatzis, Sotirios P. (2017). Asymmetric deep generative models. Neurocomputing, 241, \n90. https://doi.org/10.1016/j.neucom.2017.02.028   \nPayan, A., Montana, G. (2015). Predicting Alzheimer’s disease: a neuroimaging study with 3D convolutional \nneural networks. arXiv preprint arXiv:1502.02506. \nPereira, S., Meier, R., McKinley, R., Wiest, R., Alves, V., Silva, C. A., & Reyes, M. (2018). Enhancing \ninterpretability of automatically extracted machine learning features: application to a RBM-Random \nForest \nsystem \non \nbrain \nlesion \nsegmentation. Medical \nimage \nanalysis, 44, \n228-244. \nhttps://doi.org/10.1016/j.media.2017.12.009  \nPinaya, W.H.L., Gadelha, A., Doyle, O.M., Noto, C., Zugman, A., Cordeiro, Q., Jack- owski, A.P., Bressan, \nR.A., Sato, J.R., 2016. Using deep belief network modelling to characterize differences in brain \nmorphometry in schizophrenia. Nat. Sci. Rep. 6, 38897. https://doi.org/10.1038/srep38897  \nPlis, S.M., Hjelm, D.R., Salakhutdinov, R., Allen, E.A., Bockholt, H.J., Long, J.D., John- son, H.J., Paulsen, \nJ.S., Turner, J.A., Calhoun, V.D., 2014. Deep learning for neu- roimaging: a validation study. Front. \nNeurosci. https://doi.org/10.3389/fnins.2014.00229  \nRifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and YoshuaBengio. 2011. Contractive auto-encoders: \nexplicit invariance during feature extraction. In Proceedings of the 28th International Conference on \n28 \n \nInternational Conference on Machine Learning (ICML'11), LiseGetoor and Tobias Scheffer (Eds.). \nOmnipress, USA, 833-840. \nSalakhutdinov R, and Geoffrey Hinton. 2009. Deep Boltzmann machines. In Artificial Intelligence and \nStatistics.PMLR, 448–455. \nSalakhutdinov R, and Geoffrey Hinton. 2012. An efficient learning procedure for deep Boltzmann machines. \nNeural Computation 24, 8 (2012), 1967–2006. \nSalakhutdinov R. 2015. Learning deep generative models. Annu. Rev. Stat. Appl. 2:361–85 \nShi, J. Wu, Y. Li, Q. Zhang and S. Ying, \"Histopathological Image Classification WithColor Pattern Random \nBinary Hashing-Based PCANet and Matrix-Form Classifier,\" in IEEE Journal of Biomedical and Health \nInformatics, vol. 21, no. 5, pp. 1327-1337, Sept. 2017. https://doi.org/10.1109/JBHI.2016.2602823  \nShin, H. C., Orton, M. R., Collins, D. J., Doran, S. J., & Leach, M. O. (2013). Stacked autoencoders for \nunsupervised feature learning and multiple organ detection in a pilot study using 4D patient data. IEEE \ntransactions \non \npattern \nanalysis \nand \nmachine \nintelligence, 35(8), \n1930-1943. \nhttps://doi.org/10.1109/TPAMI.2012.277  \nSu H., Xing F., Kong X., Xie Y., Zhang S., Yang L. (2018). Robust Cell Detection and Segmentation in \nHistopathological Images Using Sparse Reconstruction and Stacked Denoising Autoencoders. Lecture \nNotes in Computer Science, 9351. Springer, Cham. https://doi.org/10.1007/978-3-319-24574-4_46   \nSuk, H. I., Lee, S. W., Shen, D., (2013a). Latent feature representation with stacked auto-encoder for AD/MCI \ndiagnosis. Brain structure & function, 220(2), 841-59. https://doi.org/10.1007/s00429-013-0687-3  \nSuk, H.-I., Lee, S.-W., Shen, D. (2014). Hierarchical feature representation and multi- modal fusion with deep \nlearning for AD/MCI diagnosis. Neuroimage 101, 569–582. \nhttps://doi.org/10.1016/j.neuroimage.2014.06.077  \nSuk, H.-I., Shen, D. (2013). Deep learning-based feature representation for AD/MCI classification. In: \nProceedings of the Medical Image Computing and Computer-Assisted Intervention. In: Lecture Notes in \nComputer Science, 8150 (pp. 583–590). https://doi.org/10.1007/978-3-642-40763-5_72  \nSuk, H.-I., Wee, C.-Y., Lee, S.-W., Shen, D. (2016). State-space model with deep learn- ing for functional \ndynamics \nestimation \nin \nresting-state \nFMRI. \nNeuroimage, \n129, \n292–307. \nhttps://doi.org/10.1016/j.neuroimage.2016.01.005  \nVan Tulder, G., & de Bruijne, M. (2016). Combining generative and discriminative representation learning for \nlung CT analysis with convolutional restricted boltzmann machines. IEEE transactions on medical \nimaging, 35(5), 1262-1272. https://doi.org/10.1109/TMI.2016.2526687  \nVincent P, Larochelle H, Lajoie I, (2010) Stacked denoising autoencoders: learning useful representations in a \ndeep network with a local denoising criterion. Journal of Machine Learning Research, 11, 3371-3408 \nVincent, H. Larochelle, Y. Bengio, and P.A. Manzagol. Extracting and composing robust features with \ndenoising autoencoders. In W.W. Cohen, A. McCallum, and S.T. Roweis, editors, Proceedings of the \nTwenty-fifth International Conference on Machine Learning  ICML’08 , pages 1096–1103. ACM, 2008. \nWani, N., & Raza, K. (2018). Multiple Kernel-Learning Approach for Medical Image Analysis. In Soft \nComputing Based Medical Image Analysis (pp. 31-47). https://doi.org/10.1016/B978-0-12-813087-\n2.00002-6  \nWu, J., Ruan, S., Mazur, T.R., Daniel, N., Lashmett, H., Ochoa, L., Zoberi, I., Lian, C., Gach, H.M., Mutic, S., \nThomas, M., Anastasio, M.A., Li, H. (2018). Heart motion tracking on cine MRI based on a deep \nBoltzmann machine-driven level set method. In Proceedings of International Symposium on Biomedical \nImaging (pp. 1153-1156). https://doi.org/10.1109/ISBI.2018.8363775  \nXu, J., Xiang, L., Liu, Q., Gilmore, H., Wu, J., Tang, J., Madabhushi, A. (2016). Stacked sparse autoencoder \n(SSAE) for nuclei detection on breast cancer histopathology images. IEEE Trans. Med. Imaging 35, 119–\n130. https://doi.org/10.1109/TMI.2015.2458702  \n29 \n \nYi, W., Tsang, K. K., Lam, S. K., Bai, X., Crowell, J. A., & Flores, E. A. (2018). Biological plausibility and \nstochasticity in scalable VO2 active memristor neurons. Nature Communications, 9(1), 4661. \nhttps://doi.org/10.1038/s41467-018-07052-w  \nYoo, Y., Brosch, T., Traboulsee, A., Li, D. K., & Tam, R. (2014). Deep learning of image features from \nunlabeled data for multiple sclerosis lesion segmentation. In International Workshop on Machine \nLearning in Medical Imaging (pp. 117-124). Springer, Cham. https://doi.org/10.1007/978-3-319-10581-\n9_15  \nZabalza, J., Ren, J., Zheng, J., Zhao, H., Qing, C., Yang, Z., ... & Marshall, S. (2016). Novel segmented stacked \nautoencoder for effective dimensionality reduction and feature extraction in hyperspectral \nimaging. Neurocomputing, 185, 1-10. https://doi.org/10.1016/j.neucom.2015.11.044  \nZhang, Q., Xiao, Y., Dai, W., Suo, J., Wang, C., Shi, J., Zheng, H., (2016a). Deep learning based classification \nof \nbreast \ntumors \nwith \nshear-wave \nelastography. \nUltrasonics \n72, \n150–157. \nhttps://doi.org/10.1016/j.ultras.2016.08.004  \nZhao, Wei & Jia, Zuchen & Wei, Xiaosong & Wang, Hai. (2018). An FPGA Implementation of a Convolutional \nAuto-Encoder. Applied Sciences. 8. 504. https://doi.org/10.3390/app8040504  \nZhu, Y., Wang, L., Liu, M., Qian, C., Yousuf, A., Oto, A., Shen, D. (2017). MRI Based prostate cancer \ndetection with high-level representation and hierarchical classification. Med. Phys. 44 (3), 1028–1039. \nhttps://doi.org/10.1002/mp.12116  \n \n",
  "categories": [
    "eess.IV",
    "cs.CV",
    "cs.LG"
  ],
  "published": "2018-12-19",
  "updated": "2018-12-19"
}