{
  "id": "http://arxiv.org/abs/1812.08904v1",
  "title": "Pre-training with Non-expert Human Demonstration for Deep Reinforcement Learning",
  "authors": [
    "Gabriel V. de la Cruz",
    "Yunshu Du",
    "Matthew E. Taylor"
  ],
  "abstract": "Deep reinforcement learning (deep RL) has achieved superior performance in\ncomplex sequential tasks by using deep neural networks as function\napproximators to learn directly from raw input images. However, learning\ndirectly from raw images is data inefficient. The agent must learn feature\nrepresentation of complex states in addition to learning a policy. As a result,\ndeep RL typically suffers from slow learning speeds and often requires a\nprohibitively large amount of training time and data to reach reasonable\nperformance, making it inapplicable to real-world settings where data is\nexpensive. In this work, we improve data efficiency in deep RL by addressing\none of the two learning goals, feature learning. We leverage supervised\nlearning to pre-train on a small set of non-expert human demonstrations and\nempirically evaluate our approach using the asynchronous advantage actor-critic\nalgorithms (A3C) in the Atari domain. Our results show significant improvements\nin learning speed, even when the provided demonstration is noisy and of low\nquality.",
  "text": "PRE-TRAINING WITH NON-EXPERT HUMAN DEMONSTRATION\nFOR DEEP REINFORCEMENT LEARNING\nA PREPRINT\nGabriel V. de la Cruz Jr., Yunshu Du and Matthew E. Taylor\nSchool of Electrical Engineering and Computer Science\nWashington State University\nPullman, WA 99164-2752\n{gabriel.delacruz,yunshu.du,matthew.e.taylor}@wsu.edu\nDecember 24, 2018\nABSTRACT\nDeep reinforcement learning (deep RL) has achieved superior performance in complex sequential\ntasks by using deep neural networks as function approximators to learn directly from raw input\nimages. However, learning directly from raw images is data inefﬁcient. The agent must learn feature\nrepresentation of complex states in addition to learning a policy. As a result, deep RL typically suffers\nfrom slow learning speeds and often requires a prohibitively large amount of training time and data to\nreach reasonable performance, making it inapplicable to real-world settings where data is expensive.\nIn this work, we improve data efﬁciency in deep RL by addressing one of the two learning goals,\nfeature learning. We leverage supervised learning to pre-train on a small set of non-expert human\ndemonstrations and empirically evaluate our approach using the asynchronous advantage actor-critic\nalgorithms (A3C) in the Atari domain. Our results show signiﬁcant improvements in learning speed,\neven when the provided demonstration is noisy and of low quality.\n1\nIntroduction\nThe widespread successes of deep Reinforcement Learning (deep RL) have brought a resurgence in using deep neural\nnetworks for RL tasks. Using a deep neural network as its function approximator, deep RL can learn state representations\ndirectly from raw input pixels and has achieved state-of-the-art results in various domains (Mnih et al. 2015, Silver et al.\n2016, Kempka et al. 2016, Lillicrap et al. 2016, Duan et al. 2016, Silver et al. 2018). However, despite this impressive\nability, deep RL remains data inefﬁcient and often requires a long training time to achieve reasonable performance.\nThis drawback has made deep RL impractical in real-world applications where data is expensive to collect, such as in\nrobotics, self-driving cars, ﬁnance, or medical applications (Bojarski et al. 2017, Deng et al. 2017, Miotto et al. 2017).\nSimilar to classic RL algorithms, deep RL suffers from poor initial performance since it learns tabula rasa (Sutton\n& Barto 2018). Inherently, deep RL takes even longer to learn because, unlike in classic RL where hand-engineered\nfeatures were used, deep RL has to learn features directly from raw observations, in addition to policy learning.\nTherefore, one can speed up deep RL by addressing its two learning components: feature learning and policy learning.\nIn this work, we tackle the feature learning problem and show that faster learning can be achieved by addressing one of\nthe two problems.\nThere have been many techniques proposed to speed up feature learning in deep RL, from transfer learning (Taylor &\nStone 2009, Pan et al. 2010), to reward shaping (Ng et al. 1999, Brys et al. 2015), to using auxiliary tasks (Zhang et al.\n2016, Jaderberg et al. 2017, Mirowski et al. 2017, Papoudakis et al. 2018, Du et al. 2018). Learning from demonstrations\n(Argall et al. 2009) is yet another way to help speed up learning in deep RL and has recently gained traction due to\nits ability to bootstrap the agent at the beginning of training (Kurin et al. 2017, Vinyals et al. 2017, Hester et al. 2018,\nPohlen et al. 2018). In this work, we make use of human demonstration data by ﬁrst using supervised learning to\npre-train a neural network to learn the underlying state features and then transfer the learned features to an RL agent\n(Erhan et al. 2009, 2010, Yosinski et al. 2014). We evaluate our approach using a recently-developed deep RL algorithm,\narXiv:1812.08904v1  [cs.LG]  21 Dec 2018\nA PREPRINT - DECEMBER 24, 2018\nthe Asynchronous Advantage Actor-Critic (A3C) (Mnih et al. 2016) algorithm in six Atari games (Bellemare et al.\n2013). Unlike previous work where a large amount of expert human data is required to achieve good initial performance\nboost, our approach shows signiﬁcant learning speed improvements on all experiments with only a relatively small\namount of noisy, non-expert demonstration data. The simplicity of our approach has made it generally adaptable to\nother deep RL algorithms and potentially to other domains since the collection of demonstration data becomes easy. In\naddition, we apply Gradient-weighted Class Activation Mapping (Grad-CAM) (Selvaraju et al. 2017) on learned feature\nmaps for both the human data and the agent data, providing a detailed analysis on why pre-training helps to speed up\nlearning. Our work makes the following contributions:\n1. We show that pre-training on a small amount of non-expert human demonstration data is sufﬁcient to achieve\nsigniﬁcant performance improvements.\n2. We are the ﬁrst to apply the transformed Bellman (TB) operator (Pohlen et al. 2018) in the A3C algorithm\n(Mnih et al. 2016) and further improve A3C’s performance on both baseline and pre-training methods.\n3. We propose a modiﬁed version of the Grad-CAM method (Selvaraju et al. 2017), which we are the ﬁrst to\nprovide empirical analysis on what features are learned from pre-training, indicating why pre-training on\nhuman demonstration data helps.\n4. We release our code and all collected human demonstration data at https://github.com/gabrieledcjr/\nDeepRL.\nThis article is organized as the following. In the next section, we review some of the related work in using pre-training\nto improve data efﬁciency. Section 3 provides background on deep RL algorithms and the transformed Bellman\noperator. In Section 4, we propose our pre-training methods for deep RL. Followed by Section 5 where we describe\nthe experimental designs. Results and analysis are presented in Section 6. We conclude this article in Section 7 with\ndiscussions and future works.\n2\nRelated Work\nOur work is closely related to transfer learning (Taylor & Stone 2009, Pan et al. 2010) where learned knowledge from\nsource task(s) is transferred to target task(s) such that the target task does not need to learn features and/or policies\nfrom scratch, thus obtaining faster learning. In supervised learning, transferring parameters from a model pre-trained\non ImageNet (Russakovsky et al. 2015) has shown to be an effective way of speeding up image classiﬁcation in a\nnew dataset, especially when the source dataset is similar to the target dataset (Yosinski et al. 2014). In deep RL,\nthe performance of a target agent can be improved by making use of the knowledge learned in one or more similar\nsource agents (Du et al. 2016, Glatt et al. 2016, Parisotto et al. 2016, Rusu et al. 2016, Teh et al. 2017). All works\nmentioned above perform pre-train and transfer under the same problem settings. That is, pre-train in supervised\nlearning and transfer to supervised learning, or pre-train in RL and transfer to RL. In this work, we consider a different\nmechanism that transfers between different problem settings. We pre-train a supervised classiﬁcation model on the\nhuman demonstration data, then transfer the learned features to an RL agent.\nExisting work has shown that pre-training can effectively speed up learning in RL. For example, Abtahi & Fasel (2011)\nconsidered learning latent features with unsupervised learning through Deep Belief Networks as a pre-training method;\nAnderson et al. (2015) pre-train hidden units of a Q-network by learning to predict state dynamics. These works show\nlearning speed up in relatively easy RL domains such as Mountain Car, Puddle World, and Cart-Pole. Our approach\nis in spirit to these earlier works and differs in that we consider a much complex domain, Atari, and learn features\ndirectly from raw input images instead of using hand-engineered features in a simpler domain. Learning directly from\nraw inputs is extremely challenging to an RL agent as it has to learn both the feature representations and the policy\nsimultaneously.\nLeveraging human knowledge via learning from demonstration (LfD) is another effective way to pre-train an RL agent\nand has shown to be successful in robotics problems (Argall et al. 2009). LfD has recently been applied widely in\ndeep RL. Christiano et al. (2017) uses human feedback to learn a reward function; Hester et al. (2018) pre-train a Deep\nQ-network (DQN) (Mnih et al. 2015) with human demonstrations by combining a large margin supervised loss with\ntemporal difference loss, such that the agent closely imitates the demonstrator’s policy at the beginning and later on\nlearn to surpass the demonstrator. Our work similarly leverages a supervised loss as pre-training but differs in that we\nconsider only the cross-entropy supervised loss as a feature learner, but not to imitate the policy. Pohlen et al. (2018)\nbuilds upon Hester et al. (2018) and proposes a reward-invariant update rule that uses the transformed Bellman (TB)\noperator in the DQN algorithm which better leverages expert demonstrations. In our work, we instead apply the TB\noperator in the A3C algorithm. The work of Silver et al. (2016) also trains human demonstrations in supervised learning\nthen uses the supervised learner’s network to initialize RL’s policy network. However, their work uses a vast amount\n2\nA PREPRINT - DECEMBER 24, 2018\nA3C Only\nConv1 (Kernel: 8x8 Stride: 4x4 Channels: 32)\nConv2 (Kernel: 4x4 Stride: 2x2 \nChannels: 64)\nConv3 (Kernel: 3x3 Stride: 1x1 \nChannels: 64)\nFullyConnected fc1\n (Units: 512)\nFullyConnected fc2\n(Units: # of actions)\nFullyConnected fc3 \n(Units: 1)\nReLU\nReLU\nReLU\nReLU\nvalue of \nstate\npolicy\n(mapping from state \nto action/class)\nFigure 1: Network architecture for each parallel actor in the A3C agent. We follow the same architecture as in Mnih\net al. (2015) where there are three convolutional layers (conv1, conv2, and conv3), followed with two fully-connected\nlayers (fc1 and fc2), and a third fully-connected layer (fc3) for learning the state value function as was done in Mnih\net al. (2016).\nof expert demonstration data to train the supervised learner, while ours only uses a small amount of non-expert data.\nOur work is also the ﬁrst to provide a comparative analysis on how pre-training with human data impacts learning in\ndeep RL algorithms, as well as how our approach complements existing deep RL algorithms when (a small amount of)\nhuman demonstrations are available.\n3\nBackground: Deep Reinforcement Learning\nAn RL problem is typically modeled using a Markov Decision Process (MDP) that is represented by a 5-tuple\n⟨S, A, P, R, γ⟩. At each time step t, an RL agent receives some state representation St ∈S and explores an unknown\nenvironment by taking an action At ∈A(s). A reward Rt+1 ∈R ⊂R is given based on the action the agent\ntook and the next state St+1 it reaches. The goal of an RL agent is to learn to maximize the expected return value\nGt = P∞\nk=0 γkRt+k+1 for each state at time t. The discount factor γ ∈[0, 1] determines the relative importance of\nfuture and immediate rewards (Sutton & Barto 2018).\nThe ﬁrst successful deep RL method, deep Q-network (DQN), learns to play 49 Atari games directly from screen pixels\nby combining Q-learning with a deep convolutional neural network (Mnih et al. 2015). In classic Q-learning, an agent\nlearns a state-action value function Qπ(s, a) = Es′[r + γ maxa′ Qπ(s′, a′)|s, a], which is the expected discounted\nreward determined by performing action a in state s and thereafter following policy π (Watkins & Dayan 1992). The\noptimal Q∗can be deduced by following actions that have the maximum Q value, Q∗(s, a) = argmaxπQπ(s, a).\nDirectly computing the Q value is not feasible when the state space is large or continuous. The DQN algorithm uses a\nconvolutional neural network as a function approximator to estimate Q(s, a; θ) ≈Q∗(s, a), where θ is the network’s\nweight parameters. For each iteration i, DQN is trained to minimize\nLi(θi) = Es,a,r,s′\nh\n(y −Q(s, a; θi))2i\nwhere y = r + γmaxa′Q(s′, a′; θ−\ni ) is a target network parameterized as θ−\ni that was generated from previous\niterations. {s, a, r, s′} are state-action samples drawn from an experience replay memory, which is used to store the\nagent’s experiences. At each time step, a batch of 32 samples (or minibatch) is drawn from the experience replay\nmemory to perform an update—this off-policy method could break the correlation between data (i.e., the sampled data\nis i.i.d.) which stabilizes the learning. All rewards are clipped to [−1, 1] to cope with different reward scale in games.\nThe use of a target network, an experience replay memory, and the reward clipping are essential to stabilizing learning.\nThe ϵ-greedy policy is used by the agent to obtain sufﬁcient exploration of the state space; for a probability of ϵ, the\nagent selects a random action to explore.\n3\nA PREPRINT - DECEMBER 24, 2018\n3.1\nAsynchronous Advantage Actor-Critic (A3C)\nThe DQN algorithm suffers from two drawbacks: long training times and high computational requirements for memory\nand GPU resources. The A3C algorithm, in contrast, trains faster without the need of a GPU. In this work, we choose to\nuse the A3C algorithm for all experiments.\nA3C combines the actor-critic algorithm with deep RL. It differs from value-based algorithms where only a value\nfunction is learned. An actor-critic algorithm is policy-based and maintains both a policy function π(at|st; θ) and a\nvalue function V π(st; θv). The policy function is called the actor, which takes actions based on the current policy π.\nThe value function is called the critic, which serves as a baseline to evaluate the quality of the action using the state value\nV π(st; θv). The network architecture of the A3C algorithm is shown in Figure 1. There are three convolutional layers\n(conv1, conv2, and conv3), one fully connected layer of size 512 (fc1), followed by two branches of fully connected\nlayer: fc2 is the policy function output layer which is of the same size as the number of actions and fc3 is the value\nfunction output layer of size 1.\nIn A3C, k actor-learners run in parallel with their own copies of the environment and parameters for the policy and\nvalue function, which enables exploration of different parts of the environment and therefore observations will not be\ncorrelated. Each actor-learner performs a parameter update every tmax actions, or when a terminal state is reached—this\nis similar to using minibatch update as was done in DQN. Updates are synchronized to a master learner that maintains a\ncentral policy and value function, which will be the ﬁnal policy upon the completion of training.\nThe policy network is directly parameterized and improved via policy-gradient (Sutton & Barto 2018). To reduce the\nvariance in policy gradient, an advantage function is used and calculated as A(st, at; θ, θv) = Q(n)(st, at; θ, θv) −\nV (st; θv). The Q(n) function is deﬁned as\nQ(n)(st, at; θ, θv) =\nn−1\nX\nk=0\nγkrt+k + γnV (st+n; θv)\n(1)\nwhere n is upper-bounded by tmax. The loss function for the policy network is then deﬁned as\nL(θ) = ∇θ log π(at|st; θ)A(at, st; θ, θv) + β∇θH(π(st; θ))\nwhere H is the entropy of policy π that encourages exploration therefore helps prevent premature convergence to\nsub-optimal policies. The value network is updated using the loss function\nL(θv) = ∇θv\nh\n(Q(n)(st, at; θ, θv) −V (st; θv))2i\n3.2\nTransformed Bellman Operator\nReward clipping is introduced in DQN and is also used in A3C to cope with different reward scales among Atari games\n(Mnih et al. 2015, 2016). However, this is problematic because the RL agent will not be able to distinguish between\nstates with high rewards versus those with low rewards, resulting in learning a suboptimal policy. The poor performance\nin some Atari games has been attributed to reward clipping (Hester et al. 2018).\nIn this work, we apply the transformed Bellman operator (Pohlen et al. 2018) to the A3C algorithm to overcome the\nproblem of reward clipping. We use the raw rewards instead of clipping them to the scale of [−1, 1]. A function\nh : R →R is used to reduce the scale of Q(n)(st, at; θ, θv) (Equation 1) and is transformed as\nQ(n)(st, at; θ, θv) =\nn−1\nX\nk=0\nh\n\u0000γkrt+k + γnh−1 (V (st+n; θv))\n\u0001\n(2)\nh : z 7→sign(z)\n\u0010p\n|z| + 1 −1\n\u0011\n+ εz\n(3)\nh−1 : x 7→sign(x)\n\n\n p\n1 + 4ε(|x| + 1 + ε) −1\n2ε\n!2\n−1\n\n\n(4)\nwhere εz is for regularization that ensures h−1 is Lipschitz continuous and a closed form inverse.\n4\nA PREPRINT - DECEMBER 24, 2018\n4\nSupervised Pre-Training for Deep RL\nDeep reinforcement learning can be divided into two sub-tasks: feature learning and policy learning. Although deep RL\nin itself has succeeded in learning both tasks simultaneously, it still suffers from long training time and slow learning.\nWe believe that by addressing feature learning, we can jumpstart the performance in an RL agent since it will be able to\nfocus more on policy learning, which in turn speeds up the entire learning process.\nIn this article, we propose to use supervised pre-training on human demonstration data as a way to address feature\nlearning. We train a multiclass-classiﬁcation network over a set of non-expert human demonstrations, where actions\ndemonstrated by the human were used as the ground truth labels for a given input game frame. The network uses the\nsame architecture as in A3C (shown in Figure 1) where the fc2 layer is used as the classiﬁcation output layer. Note\nthat we exclude the fc3 layer for the classiﬁcation task. The network classiﬁer minimizes a softmax cross entropy loss\nusing the RMSProp (Tieleman & Hinton 2012) optimizer with a set of hyperparameters shown in Table 1. We also use\ngradient clipping and L2 regularization for more stable training.\nHowever, we identify two problems when using non-expert human demonstration for pre-training. First, we assume the\nactions provided by the human are the correct labels—the low quality of our data shown in Table 2 indicates that we are\npre-training with noisy data. In this work, we empirically study if the noise in the data would still allow us to learn\nimportant features. Second, the collected human data is highly imbalanced. For example, in the game of Breakout,\nafter hitting the ball, the human usually do nothing until the ball bounces and starts falling back to the paddle, which\nresults in most collected actions being the “NOOP” action. In some games, the human demonstrator tends to use the\nsimpler actions like “LEFT” instead of the compound actions like “LEFTFIRE”. The class imbalance problem plagues\nall six games used in our experiments. To cope with this, we use proportional sampling. During minibatch sampling,\nwe randomly sample over all available actions based on their proportion to the entire demonstration data; doing so\nensures that each minibatch includes a relatively balanced set of classes. We pre-train a classiﬁcation model for each\nAtari game for 750,000 training iterations.\nAfter pre-training, the learned weights and biases from the classiﬁer are then used to initialize the A3C’s network\n(instead of random initialization). Note that when using all layers’ parameters from the pre-trained model (including\nthe output fc2 layer), normalizing the output layer’s weights is necessary to achieve a positive result; we empirically\nobserve that the values of the output layer tend to explode without normalization. To normalize the output layer, we\nkeep track of the maximum value of the output layer during training, which is then used as the divisor to all weights and\nbiases. We refer to our pre-training method as the pre-trained model for A3C (PMfA3C). We also apply pre-training to\nthe transformed Bellman operator variant of the A3C algorithm, and we refer to it as PMfA3C-TB.\n5\nExperimental Design\nWe evaluate our approach in six Atari games: Asterix, Breakout, MsPacman, NameThisGame, Pong, and SpaceInvaders.\nWe use the deterministic version four of the Atari 2600 environment from OpenAI Gym (Brockman et al. 2016). We\nfollow the Atari settings described in the DQN algorithm and OpenAI baselines Atari wrapper (Mnih et al. 2015,\nDhariwal et al. 2017). Here are the Atari settings used:\n• At the beginning of a game, the agent executes a random x (0 ≤x ≤30) number of “NOOP” actions.\n• Take an action for games (e.g., Breakout) that remains static unless pressing “FIRE.”\n• Apply max pooling over the game frames to remove ﬂickering effects of the game.\n• Consider loss of life as the end of an episode or as a terminal state, but only do a hard-reset on the game\nenvironment (i.e., reset back to the initial game state) when losing all lives.\n• Use a frame skip of four, meaning that an action is repeated for four frames before a new action is selected.\nThe network architecture for A3C is shown in Figure 1. The four most recent game frames are used as input to the\nnetwork, each frame is converted to grayscale and resized to 84 × 84 without cropping. We use the same set of\nhyperparameters for all games (except for Pong). We summarize the hyperparameter values in Table 1. Gradient\nclipping is also used in A3C. For all experiments, we train a total of 50 million steps, distributed over 16 parallel A3C\nactors. Each step consists four game frames (since we use frame skip of four) thus all experiments run a total of 200\nmillion game frames.\n5\nA PREPRINT - DECEMBER 24, 2018\nTable 1: All games use the same set of hyperparameters except for Pong, where we found setting RMSProp epsilon to\n1 × 10−4 gives a much more stable learning.\nParameter\nValue\nRMSProp learning rate\n7 × 10−4\nRMSProp epsilon\n1 × 10−5\nRMSProp decay\n0.99\nRMSProp momentum\n0\nMaximum gradient norm\n0.5\nParameters unique to supervised pre-training\nNumber of mini-batch updates\n750,000\nBatch size\n32\nL2 regularization weight\n1 × 10−4\nParameters unique to A3C only\nk parallel actors\n16\ntmax\n20\ntransformed Bellman operator ε\n10−2\n5.1\nCollection of Human Demonstration\nWe use the keyboard interface from OpenAI Gym (Brockman et al. 2016) to enable interactions between the human\ndemonstrator and the Atari environment. For each game, the demonstrator is provided with game rules and a set of valid\nactions with their corresponding keyboard keys. The frame skip is set to one to provide smoother game transitions during\nhuman plays (whereas we reset it to four during agent training). To simulate frame skipping during the demonstration,\nwe collect every fourth frame of the game. At each collection step, we save: 1) the game image (i.e., the state), 2)\nthe action taken by the demonstrator, 3) the reward received, and 4) if the current state is a terminal state. For each\nepisode, we allow a maximum of 20 minutes of playing time for the human demonstrator. The demonstration ends\nwhen the game reaches the time limit or when the game ends—whichever comes ﬁrst. Table 2 provides a breakdown of\nthe demonstration size and quality for all games.\n5.2\nEvaluation Procedures\nFor all experiments, we perform policy evaluation on the RL agent at every one million training steps. We get the\naverage testing score over 125,000 testing steps and report the average over four trials. We report the highest average\nreward of the RL agent and also measure the learning speed improvement using three metrics adapted from Taylor &\nStone (2009):\n1. Best reward: the highest average reward attained by the agent from over four trials.\n2. Final performance: the ﬁnal learned performance of the agent. We use the reward obtained at step 50 million\nas the value for the ﬁnal performance.\n3. Total reward: the total reward accumulated (i.e., the area under the learning curve (AUC)) by the agent. We\napproximate the AUC using the trapezoidal rule: AUC ≈PT\nt=1\nf(xt−1)+f(xt)\n2\n∆xt, where f(xt) is the reward\nvalue at time t and ∆xt = xt −xt−1 = 106 is the evaluation frequency. Note that for readability, we scale\ndown all calculation results by one million and consider ∆xt = 1, this does not affect the comparison results.\n4. Reward improvement: the ratio of the total reward improvement of the pre-trained agent compared to the\nbaseline agent. We calculate it as %Ratio = AUCpre-trained−AUCbaseline\nAUCbaseline\n× 100\n6\nResults\nFirst, we present the performance of the baseline A3C and the transformed Bellman operator variant A3C (A3C-TB).\nFigure 2 shows that A3C-TB outperforms the baseline A3C in ﬁve out of the six games. Although Pong in A3C-TB has\na low performance,1 we still consider the results as consistent with the ﬁndings in Hester et al. (2018) that using reward\n1In the game of Pong, its rewards r ∈{−1, 0, 1} are already at the same reward scale as applying the reward clipping in A3C at\nr ∈[−1, 1]. Thus, reward clipping has no effect in Pong’s performance. However, applying the transformed Bellman operator scales\n6\nA PREPRINT - DECEMBER 24, 2018\nTable 2: Human demonstration size and quality. The data is of a small amount and the demonstrator is a non-expert,\ncompared to the best human demonstration score in the state-of-the-art Ape-X DQfD algorithm (Pohlen et al. 2018)\n(Ape-X DQfD did not collect human demonstration for SpaceInvaders).\nGame\nWorst score\nBest score\nBest score\n# of states\n# of episodes\n(Ape-X DQfD)\nAsterix\n6250\n14300\n18100\n12870\n5\nBreakout\n26\n59\n79\n10190\n10\nMsPacman\n4020\n18241\n55021\n14504\n8\nNameThisGame\n2510\n4840\n19380\n17113\n4\nPong\n-13\n5\n0\n21674\n6\nSpaceInvaders\n545\n1840\n-\n16807\n8\n0\n10\n20\n30\n40\n50\nSteps (in millions)\n0\n2000\n4000\n6000\n8000\n10000\nReward\nAsterix\nA3C\nA3C-TB\n0\n10\n20\n30\n40\n50\nSteps (in millions)\n0\n100\n200\n300\n400\nReward\nBreakout\nA3C\nA3C-TB\n0\n10\n20\n30\n40\n50\nSteps (in millions)\n500\n1000\n1500\n2000\n2500\nReward\nMsPacman\nA3C\nA3C-TB\n0\n10\n20\n30\n40\n50\nSteps (in millions)\n2000\n3000\n4000\n5000\n6000\n7000\n8000\nReward\nNameThisGame\nA3C\nA3C-TB\n0\n10\n20\n30\n40\n50\nSteps (in millions)\n20\n10\n0\n10\n20\nReward\nPong\nA3C\nA3C-TB\n0\n10\n20\n30\n40\n50\nSteps (in millions)\n200\n400\n600\n800\n1000\nReward\nSpaceInvaders\nA3C\nA3C-TB\nFigure 2: Performance of the baseline A3C and the transformed Bellman operator variant A3C (A3C-TB). The x-axis is\nthe total number of training steps (among all 16 actors), where each step consists of four game frames (we use frame\nskip of four). The y-axis is the average testing score over four trials where the shaded regions correspond to the standard\ndeviation.\nclipping leads to an agent learning a suboptimal policy. As we discussed in Section 3.2, A3C-TB enables using raw\nreward signals such that the RL agent can distinguish between low and high rewarding states, which leads to better\npolicy learning. This is even more important to address when learning from demonstrations for games that have distinct\nreward signals. For example in the game of MsPacman, human tends to take actions that move towards states with high\nrewards (e.g., eating an edible ghost is more rewarding than eating the dots). However, in a baseline A3C agent where\nthe rewards are clipped, it sees all rewards as equal and might not be able to leverage the human knowledge of “eating\nan edible ghost.”\nNext, we present and discuss results for our pre-training approaches, PMfA3C and PMfA3C-TB. Note that we do not\ncompare our results to recent algorithms in Hester et al. (2018) and Pohlen et al. (2018) since our training steps are\nmuch shorter and are not directly comparable to those that were trained on large-scale.\n6.1\nPre-Training Methods\nWe apply pre-training methods as described in Section 4 to both PMfA3C and PMfA3C-TB. A multiclass-classiﬁcation\nnetwork is pre-trained using the human demonstration dataset. All weights and biases from the classiﬁer are used to\ndown Pong’s reward values, which slows down the reward propagation. We believe this is the reason why Pong’s performance is\nnegatively affected in A3C-TB.\n7\nA PREPRINT - DECEMBER 24, 2018\n0\n10\n20\n30\n40\n50\nSteps (in millions)\n0\n2000\n4000\n6000\n8000\n10000\nReward\nAsterix\nA3C\nPMfA3C\nPMfA3C-TB\n0\n10\n20\n30\n40\n50\nSteps (in millions)\n0\n100\n200\n300\n400\n500\nReward\nBreakout\nA3C\nPMfA3C\nPMfA3C-TB\n0\n10\n20\n30\n40\n50\nSteps (in millions)\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\nReward\nMsPacman\nA3C\nPMfA3C\nPMfA3C-TB\n0\n10\n20\n30\n40\n50\nSteps (in millions)\n2000\n3000\n4000\n5000\n6000\n7000\n8000\nReward\nNameThisGame\nA3C\nPMfA3C\nPMfA3C-TB\n0\n10\n20\n30\n40\n50\nSteps (in millions)\n20\n10\n0\n10\n20\nReward\nPong\nA3C\nPMfA3C\nPMfA3C-TB\n0\n10\n20\n30\n40\n50\nSteps (in millions)\n200\n400\n600\n800\n1000\n1200\nReward\nSpaceInvaders\nA3C\nPMfA3C\nPMfA3C-TB\nFigure 3: Performance of baseline and pre-training using A3C. The x-axis is the total number of training steps (among\nall 16 actors), where each step consists of four game frames (we use frame skip of four). The y-axis is the average\ntesting score over four trials where the shaded regions correspond to the standard deviation.\ninitialize A3C’s network layers, conv1, conv2, conv3, fc1, and fc2; the fc3 layer is initialized randomly. Figure 3 shows\nthe learning curve of both PMfA3C and PMfA3C-TB. Compared to the baseline A3C, PMfA3C outperforms in three\nout of six games; PMfA3C-TB shows a much stronger performance and exceeds the baseline in ﬁve out of six games.\nTable 3 shows the quantitative performance improvements of our pre-training methods over the baseline. PMfA3C and\nPMfA3C-TB achieve the best performance in the game of MsPacman among all experiments with a remarkable total\nreward improvement of 67.36%. This veriﬁes the importance of being able to learn from raw reward signals (instead of\nclipped rewards) in games with various reward scales. SpaceInvaders and Pong also show good reward improvement\nratios in both pre-training methods compared to the baseline. While the performance increase in Breakout might not\nseem obvious visually from the learning curves, their total rewards indicate a 10.64% and an 8.06% improvement\nfor PMfA3C and PMfA3C-TB respectively. Contrarily, we note that PMfA3C did not help in NameThisGame and\nAsterix. The former shows comparable results as the baseline A3C with a negligible improvement of 0.91%. When\nusing PMfA3C for Asterix, it shows the worst performance among all experiments with an 18.01% performance drop\ncompared to the baseline. However, we point out that when applying the TB operator, A3C-TB and PMfA3C-TB,\nAsterix outperforms the baseline by 21.08% and 15.38% respectively.\nIn summary, our experiment results show that: 1) using the TB operator is important for games with various reward\nscales and 2) pre-training is beneﬁcial for training in the RL agent.\n6.2\nAblation Studies\nIn reusing networks for image classiﬁcation, Yosinski et al. (2014) pointed out that the lower layers of a network tend\nto learn more general features while upper layers tend to learn more speciﬁc features towards the task. This is what\ninspired us to use all layers from the pre-trained network since both networks are trained on data collected from the\nsame game.\nTo understand how reusing just a subset of the whole pre-trained network affects the performance speedup and how it\ncompares to our best approach PMfA3C-TB, we conduct the following ablation studies:\n• PMfA3C-TB: reuse all layers (as was done in Section 6.1).\n• PMfA3C-TB fc1: reuse conv1, conv2, conv3, and fc1.\n• PMfA3c-TB conv3: reuse conv1, conv2, and conv3.\n• PMfA3c-TB conv2: reuse conv1 and conv2.\n8\nA PREPRINT - DECEMBER 24, 2018\nTable 3: Quantitative evaluation of pre-training methods using four metrics: best reward, ﬁnal performance, total reward,\nand reward improvement.\nGame\nBest reward\nFinal performance\nTotal reward\nReward\nimprovement\nAsterix\nA3C\n6398.44\n6398.43 ± 1072.22\n187702.21 ± 26249.69\n-\nA3C-TB\n7500.58\n7345.85 ± 1258.83\n223942.78 ± 47077.93\n21.08%\nPMfA3C\n5201.08\n5022.76 ± 1134.26\n153889.07 ± 15016.30\n−18.01%\nPMfA3C-TB\n8566.49\n8566.49 ± 724.55\n216571.05 ± 18629.16\n15.38%\nBreakout\nA3C\n413.88\n400.40 ± 8.04\n14198.48 ± 606.64\n-\nA3C-TB\n405.03\n391.23 ± 60.39\n14197.93 ± 181.07\n−0.29%\nPMfA3C\n427.56\n406.03 ± 7.83\n15709.87 ± 171.10\n10.64%\nPMfA3C-TB\n419.28\n398.13 ± 16.98\n15343.39 ± 472.31\n8.06%\nMsPacman\nA3C\n2052.07\n1980.46 ± 231.12\n78419.18 ± 6027.52\n-\nA3C-TB\n2172.01\n2071.19 ± 405.62\n80959.67 ± 6773.66\n3.24%\nPMfA3C\n2514.61\n2514.61 ± 247.60\n103950.65 ± 8529.05\n32.56%\nPMfA3C-TB\n3539.0\n3493.90 ± 510.68\n131239.08 ± 9851.32\n67.36%\nNameThisGame\nA3C\n5942.29\n5775.73 ± 435.45\n264140.63 ± 9830.70\n-\nA3C-TB\n7276.32\n7276.32 ± 799.27\n282540.94 ± 14059.33\n6.97%\nPMfA3C\n5952.68\n5868.82 ± 164.64\n266544.47 ± 5827.28\n0.91%\nPMfA3C-TB\n7869.28\n7869.28 ± 99.12\n306014.77 ± 4438.13\n15.85%\nPong\nA3C\n19.89\n19.79 ± 0.67\n791.71 ± 32.08\n-\nA3C-TB\n19.84\n19.84 ± 0.23\n604.60 ± 46.49\n−23.63%\nPMfA3C\n20.67\n20.67 ± 0.20\n948.38 ± 8.21\n19.79%\nPMfA3C-TB\n20.44\n19.96 ± 0.37\n937.61 ± 19.44\n18.43%\nSpaceInvaders\nA3C\n832.98\n805.96 ± 123.04\n30533.94 ± 1413.44\n-\nA3C-TB\n975.27\n948.26 ± 70.67\n33293.51 ± 1506.37\n9.04%\nPMfA3C\n952.71\n908.50 ± 50.73\n35163.27 ± 697.12\n15.16%\nPMfA3C-TB\n1081.21\n1032.48 ± 111.07\n36902.61 ± 2529.46\n20.86%\n• PMfA3c-TB conv1: reuse conv1 only.\nFigure 4 shows the results for our ablation study. We found the most intuitive results in Pong where the more pre-trained\nlayers are reused, the better the results are. In Breakout and SpaceInvaders, reusing different layers show no obvious\ndistinctions in performance compared to reusing all layers. In Asterix, reusing all pre-trained layers still has the\nbest performance; when reusing only a part of the pre-trained layers, the performance is similar to the baseline A3C.\nMsPacman and NameThisGame have the most varying results when different layers are reused. It is interesting to\nnote that they show inverted performance for different reusing strategies—PMfA3C-TB conv1 has the worst result\nin MsPacman among all reusing strategies but shows the best result in NameThisGame. Despite this distinction, all\npre-training methods in these two games outperform the baseline, regardless of the choice of reuse layers.\nIn summary, our ablation study shows that reusing the entire pre-trained network consistently performs well—it either\nachieves the best results or is comparable to reusing other layers. We point out that reusing layers that are closer to the\noutput layer (PMfA3C-TB fc1 and conv3) also shows competitive results in four out of six games. This is consistent\nwith the ﬁndings in Yosinski et al. (2014) that lower layers learn general features while higher layers learn task-speciﬁc\nfeatures, and transferring general features are more beneﬁcial when the training and testing data are different. In our\ncase, however, since the classiﬁcation model and the A3C network are trained on data that are collected from the same\ngame, leveraging task-speciﬁc features can be more helpful than reusing only general features.\n6.3\nWhat is really learned from pre-training?\nIn order to understand the beneﬁt of our pre-training method, we visualize the feature maps in the last convolutional layer\nto assess if an RL agent can learn meaningful high-level information about the game through pre-training. We adopt\nthe Gradient-weighted Class Activation Mapping (Grad-CAM) as our visualization method since it is model-agnostic\nand can be used in any convolutional-based network without needing to change architectures (Selvaraju et al. 2017).\nUsing Grad-CAM, we analyze how different regions in feature maps are activated by corresponding actions under three\nsettings: 1) a randomly initialized RL agent, 2) a pre-trained classiﬁcation model, and 3) a ﬁnal learned RL agent in\nPMfA3C-TB. By comparing feature map patterns among the three scenarios, we will be able to obtain an intuitive\n9\nA PREPRINT - DECEMBER 24, 2018\n0\n10\n20\n30\n40\n50\nSteps (in millions)\n0\n2000\n4000\n6000\n8000\n10000\nReward\nAsterix\nA3C\nPMfA3C-TB conv1\nPMfA3C-TB conv2\nPMfA3C-TB conv3\nPMfA3C-TB fc1\nPMfA3C-TB\n0\n10\n20\n30\n40\n50\nSteps (in millions)\n0\n100\n200\n300\n400\n500\nReward\nBreakout\nA3C\nPMfA3C-TB conv1\nPMfA3C-TB conv2\nPMfA3C-TB conv3\nPMfA3C-TB fc1\nPMfA3C-TB\n0\n10\n20\n30\n40\n50\nSteps (in millions)\n0\n1000\n2000\n3000\n4000\n5000\nReward\nMsPacman\nA3C\nPMfA3C-TB conv1\nPMfA3C-TB conv2\nPMfA3C-TB conv3\nPMfA3C-TB fc1\nPMfA3C-TB\n0\n10\n20\n30\n40\n50\nSteps (in millions)\n2000\n3000\n4000\n5000\n6000\n7000\n8000\n9000\nReward\nNameThisGame\nA3C\nPMfA3C-TB conv1\nPMfA3C-TB conv2\nPMfA3C-TB conv3\nPMfA3C-TB fc1\nPMfA3C-TB\n0\n10\n20\n30\n40\n50\nSteps (in millions)\n20\n10\n0\n10\n20\nReward\nPong\nA3C\nPMfA3C-TB conv1\nPMfA3C-TB conv2\nPMfA3C-TB conv3\nPMfA3C-TB fc1\nPMfA3C-TB\n0\n10\n20\n30\n40\n50\nSteps (in millions)\n200\n400\n600\n800\n1000\n1200\nReward\nSpaceInvaders\nA3C\nPMfA3C-TB conv1\nPMfA3C-TB conv2\nPMfA3C-TB conv3\nPMfA3C-TB fc1\nPMfA3C-TB\nFigure 4: Performance of baseline and pre-training using A3C. The x-axis is the total number of training steps (among\nall 16 actors), where each step consists of four game frames (we use frame skip of four). The y-axis is the average\ntesting score over four trials where the shaded regions correspond to the standard deviation.\nunderstanding of what is learned through pre-training and how an RL agent uses the pre-trained knowledge during its\nlearning.\nGrad-CAM is a visualization tool designed to increase the interpretability of prediction results of a deep neural\nnetwork (Selvaraju et al. 2017). Speciﬁcally, Grad-CAM computes the gradient of the target logits (output values before\napplying softmax) with respect to feature maps of a convolutional layer. The importance weight of each neuron can then\nbe obtained by performing global averaging pooling (Lin et al. 2013) over the gradients. For a classiﬁcation task, the\ntarget logits refer to the score of a target class, while in the context of RL, the target class can be considered as the action\na taken based on the current policy π. Given any game state s, the score ya of an action a is deﬁned as ya = π(a|s; θ),\nwhere θ parameterizes the policy network in A3C. We then compute gradients for ya with respect to the feature maps of\nthe last convolutional layer (denoted as M k).2 We compute the importance weight (denoted as αa\nk) in A3C as\nαa\nk =\nglobal average pooling\nz\n}|\n{\n1\nZ\nX\ni\nX\nj\n∂ya\n∂M k\nij\n| {z }\ngradients via backprop\nSelvaraju et al. (2017) also combines forward activation maps and pass through a rectiﬁer linear units (ReLU) to show\nthe features that has a positive effect on the action of interest. This gives us the action-discriminative saliency map as\nLa\nGrad-CAM = ReLU\n X\nk\nαa\nkM k\n!\nHowever, the output of the discriminative saliency map did not provide an interpretable visualization under A3C.\nAccording to (Selvaraju et al. 2017), the weight αa\nk captures the importance of the feature map k. Since αa\nk can have\nboth positive and negative values, we choose to emphasize only on positive weights (i.e., captures the most important\nfeatures). Thus, we instead apply ReLU directly to αa\nk, transforming La\nGrad-CAM as\nLa\nGrad-CAM =\nX\nk\nReLU\n\u0000αa\nk\n\u0001\nM k\n2In Selvaraju et al. (2017) the feature maps are denoted as Ak, we change this notation to avoid confusion with the notation of the\naction a in the context of RL.\n10\nA PREPRINT - DECEMBER 24, 2018\nWe present Grad-CAM results for two games, Pong and Breakout, as the running example in this section for analyzing\nwhat features are learned. Video clips for all six games’ Grad-CAM results are available online at https://sites.\ngoogle.com/view/pretrain-deeprl. Figure 5 shows example Grad-CAM results for a sequence of ﬁve frames\nin Pong and Breakout and each frame is used as the input image to the network. We run a forward pass to compute\nthe target logits ya and output an action a based on the current policy π. Note that actions are not executed in the\nenvironment but only used to compute La\nGrad-CAM—we do not perform gradient updates to the network. To ensure we\ncompare the three models (random initialized, pre-trained, and ﬁnal learned RL) with the same set of game inputs, we\nperform one episode of evaluation (until the game ends or reaches 5,000 testing steps, whichever comes ﬁrst) using\nthe ﬁnal learned RL agent and save all game images it encounters, then use these images as a sequential state input to\ncalculate Grad-CAM for the random initialized agent and the pre-trained model. Grad-CAM is visualized as a heatmap\nof scale [0, 1]: red (value close to 1) indicates regions that are important for a corresponding action while blue (value\nclose to 0) indicates unimportant regions. The top row shows the feature map for a randomly initialized agent; the\nsecond row is the feature map of a pre-trained classiﬁer; the third row shows a ﬁnal learned RL agent’s feature map; the\nbottom row shows the original game image.\nAt the beginning of training when the network weights are randomly initialized (top row of Figure 5), both Pong and\nBreakout agents act randomly and overall consider the entire game state to be important. In particular, Pong agent sees\nthe top part (where the score is located shows more red color) and somehow the bottom part to be more important than\nthe middle part (orange color) of the game image; Breakout agent also considers the top part to be important—with\nmore focus on the location of bricks (red-orange color) than the location of the score (yellow-green color)—but the\nbottom part of the image is shown to be not important at all (blue color).\nThe important regions change notably when the network reuses weights from pre-trained models (second row of Figure\n5). Interestingly, in both games, the agent learns to pay attention to the paddle movements. In Pong, both the opponent’s\npaddle (left) and the agent’s paddle (right) are identiﬁed to be important; in Breakout, the paddle at the bottom becomes\nthe most important (whereas under random initialization the bottom part of the image is considered to be the least). The\nresult is intuitive for the pre-trained model as it ﬁnds the object that follows the cardinal directions of the actions taken\nto be the most important feature—when the demonstrator takes an action, the object that correlates the most to this\naction is the paddle.\nWe see a further change of important regions in the ﬁnal learned RL policy (third row of Figure 5). Instead of paying\nattention to the paddle, both games learn to track the movement of the ball. This is particularly clear in Pong: the\nimportant features evolve following roughly the same trajectory as the movement of the ball, from the bottom-left to the\ntop-right of the game image.\nWhile the Grad-CAM examples in Figure 5 show that the pre-trained model picks up different important regions than\nthe ﬁnal RL policy, we also observe that some pre-training features are carried on to the ﬁnal RL policy. For example in\nPong, although the RL policy has more focus on the ball, the regions around paddles are still activated with a lower\nimportance—an inheritance from the pre-train model. In addition, in the Grad-CAM video clips where we show a\ncomplete run on all six games, it is more clear that important features in the pre-training are also identiﬁed in the ﬁnal\nRL policy. We refer the readers to the video clips at https://sites.google.com/view/pretrain-deeprl.\n7\nConclusion and Discussion\nThe goal of this article is not to defeat the state-of-the-art results for Atari. Instead, we want to address the problem\nof slow learning. Other work focuses on increasing computational resources to speed up the training, while our work\nfocuses more on improving learning speed, which is the ability to learn better policy with a smaller amount of game\nenvironment interactions. We attain the speedup in learning by supervised pre-training of the deep RL’s network using\nnon-expert human demonstrations.\nWe used the transformed Bellman operator (Pohlen et al. 2018) in A3C, which addresses reward clipping that allows\nthe RL agent to differentiate high and low rewarding states. The transformed Bellman operator helps our pre-training\napproach achieve improvements in all six games that we evaluated our approach. Our pre-training approach improves\nthe reward in MsPacman with 67.36%. In addition, A3C in MsPacman reach the highest average reward of 2,052\nat 49 million training steps, while PMfA3C-TB in MsPacman only takes 11 million training steps to surpass A3C’s\nhighest average reward. This is quite a signiﬁcant speedup in the learning speed especially when we only pre-trained\nPMfA3C-TB’s network on 14,504 game states from a non-expert human.\nWe also have a much better understanding of what features are being considered during pre-training by using our\nmodiﬁed Grad-CAM. One future direction for pre-training in deep RL is to drive the activation mapping towards the\nobjects in the game state image. We believe it would be easier for non-expert humans to simply identify the important\n11\nA PREPRINT - DECEMBER 24, 2018\nOrthogonal \ninitializer\nInitialize\nfrom\n pre-trained\n network\nFinal policy\nState\nGrad CAM\nPong\nBreakout\n1.0\n0.0\nFigure 5: Gradient-weighted Class Activation Mapping (Grad-CAM) visualization for Pong and Breakout. Top row:\nrandom initialization; second row: pre-trained model; third row: ﬁnal learned RL policy; bottom row: the original game\nimage.\nobjects in the game relative to actually playing the game—this would be another way of using humans to improve\nlearning.\nAs we investigate further ways to improve our approach, we know there is a limit to how much improvement pre-training\ncan provide without addressing policy learning. In our approach, we have already trained a model with a policy that\ntries to imitate the human demonstrator, and thus we can extend this work by using the pre-trained model’s policy to\nprovide advice to the agent (Wang & Taylor 2017).\nTo summarize, learning both features and policy directly from raw images through deep neural networks is a major factor\nwhy learning is slow in deep RL. This article has demonstrated the following: 1) we have shown through Grad-CAM\nthat using supervised pre-training with non-expert human demonstration data can be used for feature learning, and\n2) that our method of initializing deep RL’s network with a supervised pre-trained model can signiﬁcantly speed up\nlearning in deep RL.\nAcknowledgements\nThe A3C implementation was a modiﬁcation of https://github.com/miyosuda/async_deep_reinforce. The\nauthors thank Sahil Sharma and Kory Matthewson for providing very useful insights on the actor-critic method. We\nalso thank NVidia for donating a graphics card used in these experiments. This research used resources of Kamiak,\nWashington State University’s high performance computing cluster, where we ran all our experiments.\nReferences\nAbtahi, F. & Fasel, I. (2011), ‘Deep belief nets as function approximators for reinforcement learning’, Restricted\nBoltzmann Machine (RBM) 2, h3.\nAnderson, C. W., Lee, M. & Elliott, D. L. (2015), Faster reinforcement learning after pretraining deep networks to\npredict state dynamics, in ‘Neural Networks (IJCNN), 2015 International Joint Conference on’, IEEE, pp. 1–7.\nArgall, B. D., Chernova, S., Veloso, M. & Browning, B. (2009), ‘A survey of robot learning from demonstration’,\nRobotics and autonomous systems 57(5), 469–483.\nBellemare, M. G., Naddaf, Y., Veness, J. & Bowling, M. (2013), ‘The arcade learning environment: An evaluation\nplatform for general agents’, Journal of Artiﬁcial Intelligence Research 47, 253–279.\nBojarski, M., Yeres, P., Choromanska, A., Choromanski, K., Firner, B., Jackel, L. & Muller, U. (2017), ‘Explaining how\na deep neural network trained with end-to-end learning steers a car’, arXiv preprint arXiv:1704.07911 .\nBrockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J. & Zaremba, W. (2016), ‘Openai gym’.\n12\nA PREPRINT - DECEMBER 24, 2018\nBrys, T., Harutyunyan, A., Taylor, M. E. & Nowé, A. (2015), Policy transfer using reward shaping, in ‘Proceedings of\nthe 2015 International Conference on Autonomous Agents and Multiagent Systems’, International Foundation for\nAutonomous Agents and Multiagent Systems, pp. 181–188.\nChristiano, P., Leike, J., Brown, T. B., Martic, M., Legg, S. & Amodei, D. (2017), Deep reinforcement learning from\nhuman preferences, in ‘NIPS’.\nDeng, Y., Bao, F., Kong, Y., Ren, Z. & Dai, Q. (2017), ‘Deep direct reinforcement learning for ﬁnancial signal\nrepresentation and trading’, IEEE transactions on neural networks and learning systems 28(3), 653–664.\nDhariwal, P., Hesse, C., Klimov, O., Nichol, A., Plappert, M., Radford, A., Schulman, J., Sidor, S., Wu, Y. & Zhokhov,\nP. (2017), ‘Openai baselines’, https://github.com/openai/baselines.\nDu, Y., Czarnecki, W. M., Jayakumar, S. M., Pascanu, R. & Lakshminarayanan, B. (2018), ‘Adapting auxiliary losses\nusing gradient similarity’, arXiv preprint arXiv:1812.02224 .\nDu, Y., de la Cruz, Jr., G. V., Irwin, J. & Taylor, M. E. (2016), Initial progress in transfer for deep reinforcement learning\nalgorithms, in ‘In Proceedings of the Deep Reinforcement Learning: Frontiers and Challenges (DeepRL) workshop\n(at IJCAI 2016)’.\nDuan, Y., Chen, X., Houthooft, R., Schulman, J. & Abbeel, P. (2016), Benchmarking deep reinforcement learning for\ncontinuous control, in ‘International Conference on Machine Learning’, pp. 1329–1338.\nErhan, D., Bengio, Y., Courville, A., Manzagol, P.-A., Vincent, P. & Bengio, S. (2010), ‘Why does unsupervised\npre-training help deep learning?’, J. Mach. Learn. Res. 11, 625–660.\nErhan, D., Manzagol, P.-A., Bengio, Y., Bengio, S. & Vincent, P. (2009), The difﬁculty of training deep architectures and\nthe effect of unsupervised pre-training, in ‘Twelfth International Conference on Artiﬁcial Intelligence and Statistics\n(AISTATS)’, pp. 153–160.\nGlatt, R., d. Silva, F. L. & Costa, A. H. R. (2016), Towards knowledge transfer in deep reinforcement learning, in ‘2016\n5th Brazilian Conference on Intelligent Systems (BRACIS)’, pp. 91–96.\nHester, T., Vecerik, M., Pietquin, O., Lanctot, M., Schaul, T., Piot, B., Sendonaris, A., Dulac-Arnold, G., Osband, I.,\nAgapiou, J., Leibo, J. Z. & Gruslys, A. (2018), Deep q-learning from demonstrations, in ‘Proceedings of the 32nd\nAAAI Conference on Artiﬁcial Intelligence’.\nJaderberg, M., Mnih, V., Czarnecki, W. M., Schaul, T., Leibo, J. Z., Silver, D. & Kavukcuoglu, K. (2017), Reinforcement\nlearning with unsupervised auxiliary tasks, in ‘ICLR’.\nKempka, M., Wydmuch, M., Runc, G., Toczek, J. & Ja´skowski, W. (2016), Vizdoom: A Doom-based AI research\nplatform for visual reinforcement learning, in ‘Computational Intelligence and Games (CIG), 2016 IEEE Conference\non’, IEEE, pp. 1–8.\nKurin, V., Nowozin, S., Hofmann, K., Beyer, L. & Leibe, B. (2017), ‘The Atari grand challenge dataset’, arXiv preprint\narXiv:1705.10998 .\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D. & Wierstra, D. (2016), ‘Continuous\ncontrol with deep reinforcement learning’, ICLR .\nLin, M., Chen, Q. & Yan, S. (2013), ‘Network in network’, arXiv preprint arXiv:1312.4400 .\nMiotto, R., Wang, F., Wang, S., Jiang, X. & Dudley, J. T. (2017), ‘Deep learning for healthcare: review, opportunities\nand challenges’, Brieﬁngs in bioinformatics .\nMirowski, P., Pascanu, R., Viola, F., Soyer, H., Ballard, A. J., Banino, A., Denil, M., Goroshin, R., Sifre, L.,\nKavukcuoglu, K. et al. (2017), Learning to navigate in complex environments, in ‘ICLR’.\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D. & Kavukcuoglu, K. (2016),\nAsynchronous methods for deep reinforcement learning, in ‘International Conference on Machine Learning’,\npp. 1928–1937.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M.,\nFidjeland, A. K., Ostrovski, G. et al. (2015), ‘Human-level control through deep reinforcement learning’, Nature\n518(7540), 529–533.\n13\nA PREPRINT - DECEMBER 24, 2018\nNg, A. Y., Harada, D. & Russell, S. (1999), Policy invariance under reward transformations: Theory and application to\nreward shaping, in ‘ICML’, Vol. 99, pp. 278–287.\nPan, S. J., Yang, Q. et al. (2010), ‘A survey on transfer learning’, IEEE Transactions on knowledge and data engineering\n22(10), 1345–1359.\nPapoudakis, G., Chatzidimitriou, K. C. & Mitkas, P. A. (2018), ‘Deep reinforcement learning for Doom using\nunsupervised auxiliary tasks’, CoRR abs/1807.01960.\nParisotto, E., Ba, J. L. & Salakhutdinov, R. (2016), ‘Actor-mimic: Deep multitask and transfer reinforcement learning’,\nICLR .\nPohlen, T., Piot, B., Hester, T., Azar, M. G., Horgan, D., Budden, D., Barth-Maron, G., van Hasselt, H., Quan, J.,\nVeˇcerík, M. et al. (2018), ‘Observe and look further: Achieving consistent performance on Atari’, arXiv preprint\narXiv:1805.11593 .\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein,\nM. et al. (2015), ‘Imagenet large scale visual recognition challenge’, International Journal of Computer Vision\n115(3), 211–252.\nRusu, A. A., Colmenarejo, S. G., Gulcehre, C., Desjardins, G., Kirkpatrick, J., Pascanu, R., Mnih, V., Kavukcuoglu, K.\n& Hadsell, R. (2016), ‘Policy distillation’, ICLR .\nSelvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D. et al. (2017), Grad-cam: Visual explanations\nfrom deep networks via gradient-based localization., in ‘ICCV’, pp. 618–626.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I.,\nPanneershelvam, V., Lanctot, M. et al. (2016), ‘Mastering the game of go with deep neural networks and tree search’,\nNature 529(7587), 484–489.\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel,\nT. et al. (2018), ‘A general reinforcement learning algorithm that masters chess, shogi, and go through self-play’,\nScience 362(6419), 1140–1144.\nSutton, R. S. & Barto, A. G. (2018), Reinforcement learning: An introduction, MIT press.\nTaylor, M. E. & Stone, P. (2009), ‘Transfer learning for reinforcement learning domains: A survey’, Journal of Machine\nLearning Research 10(Jul), 1633–1685.\nTeh, Y., Bapst, V., Czarnecki, W. M., Quan, J., Kirkpatrick, J., Hadsell, R., Heess, N. & Pascanu, R. (2017), Distral:\nRobust multitask reinforcement learning, in ‘Advances in Neural Information Processing Systems’, pp. 4496–4506.\nTieleman, T. & Hinton, G. (2012), ‘Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent\nmagnitude’, COURSERA: Neural networks for machine learning 4(2), 26–31.\nVinyals, O., Ewalds, T., Bartunov, S., Georgiev, P., Vezhnevets, A. S., Yeo, M., Makhzani, A., Küttler, H., Aga-\npiou, J., Schrittwieser, J. et al. (2017), ‘Starcraft II: A new challenge for reinforcement learning’, arXiv preprint\narXiv:1708.04782 .\nWang, Z. & Taylor, M. E. (2017), Improving reinforcement learning with conﬁdence-based demonstrations, in\n‘Proceedings of the 26th International Conference on Artiﬁcial Intelligence (IJCAI)’.\nWatkins, C. J. & Dayan, P. (1992), ‘Q-learning’, Machine Learning 8(3-4), 279–292.\nYosinski, J., Clune, J., Bengio, Y. & Lipson, H. (2014), How transferable are features in deep neural networks?, in\n‘Advances in neural information processing systems’, pp. 3320–3328.\nZhang, Y., Lee, K. & Lee, H. (2016), Augmenting supervised neural networks with unsupervised objectives for\nlarge-scale image classiﬁcation, in ‘ICML’.\n14\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2018-12-21",
  "updated": "2018-12-21"
}