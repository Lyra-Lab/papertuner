{
  "id": "http://arxiv.org/abs/1812.09968v1",
  "title": "VMAV-C: A Deep Attention-based Reinforcement Learning Algorithm for Model-based Control",
  "authors": [
    "Xingxing Liang",
    "Qi Wang",
    "Yanghe Feng",
    "Zhong Liu",
    "Jincai Huang"
  ],
  "abstract": "Recent breakthroughs in Go play and strategic games have witnessed the great\npotential of reinforcement learning in intelligently scheduling in uncertain\nenvironment, but some bottlenecks are also encountered when we generalize this\nparadigm to universal complex tasks. Among them, the low efficiency of data\nutilization in model-free reinforcement algorithms is of great concern. In\ncontrast, the model-based reinforcement learning algorithms can reveal\nunderlying dynamics in learning environments and seldom suffer the data\nutilization problem. To address the problem, a model-based reinforcement\nlearning algorithm with attention mechanism embedded is proposed as an\nextension of World Models in this paper. We learn the environment model through\nMixture Density Network Recurrent Network(MDN-RNN) for agents to interact, with\ncombinations of variational auto-encoder(VAE) and attention incorporated in\nstate value estimates during the process of learning policy. In this way, agent\ncan learn optimal policies through less interactions with actual environment,\nand final experiments demonstrate the effectiveness of our model in control\nproblem.",
  "text": "VMAV-C: A Deep Attention-based Reinforcement Learning \nAlgorithm for Model-based Control \nXingxing Liang1#, Qi Wang2,1#, Yanghe Feng1*#, Zhong Liu1, Jincai Huang1 \n1College of Systems Engineering, University of Defense Technology, Changsha, China \n2Institute for Advanced Study, University of Amsterdam, Amsterdam, The Netherlands \n*Correspondence Author: fengyanghe@yeah.net \n#These Authors Contributed Equally in this Work. \n \nAbstract \nRecent breakthroughs in Go play and strategic games have witnessed the great potential of \nreinforcement learning in intelligently scheduling in uncertain environment, but some bottlenecks \nare also encountered when we generalize this paradigm to universal complex tasks. Among them, \nthe low efficiency of data utilization in model-free reinforcement algorithms is of great concern. In \ncontrast, the model-based reinforcement learning algorithms can reveal underlying dynamics in \nlearning environments and seldom suffer the data utilization problem. To address the problem, a \nmodel-based reinforcement learning algorithm with attention mechanism embedded is proposed as \nan extension of World Models in this paper. We learn the environment model through Mixture \nDensity Network Recurrent Network(MDN-RNN) for agents to interact, with combinations of \nvariational auto-encoder(VAE) and attention incorporated in state value estimates during the \nprocess of learning policy. In this way, agent can learn optimal policies through less interactions \nwith actual environment, and final experiments demonstrate the effectiveness of our model in \ncontrol problem. \n \nKey Words：Attention Mechanism，Deep Reinforcement Learning，Actor-Critic Algorithm, Variational \nAuto-Encoder(VAE)，Mixture Density Network Recurrent Neural Network(MDN-RNN), Discrete \nControl \n \n1. Introduction \nReinforcement learning is a promising paradigm in complex scheduling and control tasks, and it \nhas contributed a lot in domains, ranging from strategic game play [1, 2], unmanned aerial vehicle \ncontrol [3], autonomous driving [4] to robots cooperation [5]. These remarkable achievements \nhave not only demonstrated the plausibility and effectiveness of reinforcement learning for \ncombating uncertainty from both environments and decision process but inspired the ever-lasting \nresearch interest in this domain as well [6]. One of the indispensable factors accounting for the \nsuccess in applications is the agent’s ability of successively interacting with environment. More \nspecifically, the interactions relieve the extent of uncertainty, reveal the dynamics in environment \nand drive the agent to perform task-beneficial actions after learning from delayed rewards. Thus, \nsuch learning paradigm is also referred to as learning from error-trial signals and believed to pave \nthe way to general artificial intelligence.  \nRoughly speaking, there exist two families in reinforcement learning. One is called as model-free \nbased reinforcement learning, which merely takes advantage of rewards but neglects other \ninherent potential environment information, which would possibly promote the learning \nperformance. With no dependence on implicit environment information suggesting properties of \ntransitions or reward signals, model-free based learning enjoys its popularity in practice. However, \na long-standing challenge for this type of learning is that millions of instances or experience need \nto be continually collected for policy evaluation and improvement. Meanwhile, the low efficiency \nof data utilization in model-free based learning is a waste of resource and restricts the power of \nreinforcement learning being applied to more universal real-life problems. On the contrary, \nanother family, called as model-based reinforcement learning algorithms, does not strictly rely on \nsampling, enables characterization of potential dynamics in environment and mostly reveals the \ndiscipline of task. Once the model of Markov decision process(MDP) is approximately explored, \nthere is no need to create additional experience through interactions with environment, and \noptimal policy can be directly derived based on the model. As evidenced in former works [7-9], \nmodel-based algorithms tend to maintain higher efficiency in some scenarios. Still, the bottleneck \nof model-based algorithms comes from weak capability in modifications and strongly dependence \non precisely modeling, resulting in less adaptability and flexibility to dynamic Zero-shot or noisy \nenvironments.  \nAs already noticed, model-free algorithms can achieve better performance, while model-based \nalgorithms can well exploit dynamic properties of environment. Though some works have already \ntried to combine these two families in the past few decades, including synthetic experience \ngeneration [10, 11] and partially model-based backpropagation [7, 12, 13], the way to perfectly \nestablish connections between two families remains limited. From the intuition, human’s \nexperience in life and work indicates the accommodation to complex environments may not \ndemand so many instances to learn and summarize. Information we perceive every day with \nsenses is quite limited, but human beings can easily generalize knowledge or skills regardless of \ndiversity of scenarios. That is, we can conceptualize things from limited sensory information and \ngeneralize decision-making in various scenarios. One of possible reasons explaining this \nphenomenon can be the ability of abstraction, and once concepts as well as relationships between \nconcepts are built with limited accessible data, we can establish some abstract model to represent \nthe environment and partially predict dynamic variations of environment given some actions, \nincluding state transitions and reward signals. Similar interesting viewpoints refer to existing \nworks [14, 15] on neural network models, insisting human beings tend to build world physical \nmodel with finite cognitions, and perform decision-makings based on mentally constructed model. \nInstead of learning new models, our brain can make decisions more frequently with the formerly \nself-constructed physical model in our mind [16, 17]. By predicting the future scenario after \ninstant action at some state, we can promptly react and avoid potential danger with previously \nbuilt model [18, 19]. A recent trial can be seen in World Model(WM) [15], and D. Ha & J. \nSchmidhuber [15] has put it in practice, in which environment model is built with limited real \nworld experiences and policy learning efficiency is proved to be advanced. One of significant \nbenefits of such framework in modeling is that heavy workloads of collecting transitions in \nenvironment as well as concerning expense would be brought in reduction through learning a \nvirtual environment model. For an instance, unimaginable massive images in a variety of real \nscenarios iteratively perceived by sensors are required to feed an autonomous driving system, and \nthe generalization capability of such system is theoretically positively correlated to manual and \nfinancial expense in accessing environmental data. In some sense, these consuming prediction or \ncontrol tasks can be partially addressed through interactions with well-trained virtual environment.  \nIn this paper, we make an extension of the former work [15] and study the approach to \naggregating both model-free algorithm and model-based control to explore abundant environment \ninformation buried in experience and further guide the optimal policy search. More technically, we \nalso make use of neural networks in state embedding representations, sequence data prediction and \nimprove WM through attention-based policy learning. The remainders are arranged as follows. \nSection 2 summarizes some related works and express our intentions in research. Basic knowledge \nabout reinforcement learning is included in Section 3, which would contribute to optimization \nprocess in our model. Section 4 would pay a revisit to WM and elaborate framework of our \nproposed model, including components in the model, training procedures and technical details. \nSection 5 is about the experiments in some classical control problem, and we also analyze the \nperformance and sensitivity to parameters. Finally, some conclusions are drawn and some future \nworks with respect to this domain are highlighted. \n2. Literature Review \nThe simulation environment is helpful in developing and testing new reinforcement learning \nalgorithms, and OpenAI Gym [20] provides a series of virtual environments to carry out \nexperiments, allowing the comparison and validation of algorithm performance. These include \nsome traditional problems of control, among which end-to-end tasks are more practical but \nchallenging. The end-to-end tasks push the agent to directly receive original input such as images \nof some scenarios as signals to make decisions, including Cart-Pole control and Car-Racing. The \ninherent high dimensionality of images poses great difficulty in learning process and inspires the \napplication of representation learning to reinforcement learning [21]. Encoding a complicated \ninstance into a vector of low dimensions, deep neural networks can extract compact \nrepresentations for the input of high dimensions, including but not limited to images. The \nrepresentation learning with deep neural networks makes it possible to train a reinforcement \nlearning model in dealing with complicated tasks. Another advantage of using deep learning lies in \nbetter generalization, and both of DQN [1] and AlphaGo Zero [2] have benefited from the \nconvolutional neural network’s representations of states and achieved state of art performance in \npolicy learning. \nThough powerful representation model and increasingly computational power can satisfy the basic \ndemands of solving complicated control problems with reinforcement learning, the access to the \ndataset from real environment is still the bottleneck in this domain and the algorithm is hungry for \nthis resource in some sense. Honestly speaking, interactions with environment are decisive to the \nsuccessful application of reinforcement learning, and to achieve ideal performance, a wealth of \nresources such as human labor, time and money are consumed to collect transitions and rewards \nfrom environments. Especially for model-free based reinforcement learning algorithms, the \ncircumstance is more obvious, and the lower efficiency of data utilization accompanied with \nneglection of structure information in environment is another urgent concern. Such dilemma has \ncaught increasingly attention in domains and inspired some interesting thoughts to tackle \nproblems.  \nLearning the environment is extremely crucial in this study, and there exist mainly two paradigms \nfor capturing the properties of environments and relieving bias in modeling. One is to learn \nsamples indicating properties of generated environment in the form of some probability \ndistribution and explore the policy as well. Earlier works on simultaneously learning environment \nmodel and policy are not stable, while expectation maximization(EM) [22] can separably capture \nthe environment model, disentangle the parameters from control model and just learn limited \ncontrol parameters to accelerate the rate of convergence. As a breakthrough in learning \nenvironment model, WM [15] can automatically reveal dynamics environments and its motivation \nfrom cognition science has been mentioned in Introduction [15]. A. Piergiovanni et al. [23] \nconstructed deep neural networks to encode states and predict future scenario as a simulation of \nenvironment model, and demonstrated the robot can learn plausible policy to act in real world \nthrough interacting with such dreaming environment. Noticing the high complexity and cost to \nhandle image observations in visual based reinforcement learning [24], A. V. Nair et al. developed \na reinforcement learning algorithm with imaged goals, which combined variational \nauto-encoder(VAE) with off-policy goal-conditioned reinforcement learning. To address planning \nproblem in uncertain environments, a recurrent state space model was trained to capture dynamics \nof environment in pixel level and the constructed agent called Deep Planning Network can learn \npolicies to control [25]. Additionally, the original image is seldom used in environment modelling, \ne.g. World Models [15] and PA [26], and auto-encoder is mostly introduced to represent the state \nin low dimensions, further advancing the training efficiency and bringing reductions on the scale \nof parameters in control. And another paradigm is motivated by meta-learning, which seeks \nmultiple dynamic models learned from various environments and integrates the characteristics of \nthese models to describe the uncertainty in environment [22, 27, 28]. \n3. Background \nReinforcement learning is a traditional learning paradigm to deal with prediction and control \nproblems in uncertain environments. The main goal of reinforcement learning is to capture some \npolicy to maximize the cumulative rewards, which means selecting proper action given some \nstates. Generally, it can be described with Markov Decision Process(MDP), which is formulated in \na tuple of five elements {S, A, R, P, γ}. And elements in tuple respectively represent the set of \nstates in environment S = {𝑠(𝑖)|i = 1,2, . . n}, the set of available actions in environment A =\n{𝑎(𝑖)|i = 1,2, . . , m}, the reward function conditioned on state transition in environment and some \naction R = {r(st+1 = s|st = 𝑠′, at = a)|a ∈A; s, s′ ∈S}, the transition probability between states \ngiven some action P = {p(st+1 = s|st = 𝑠′, at = a)|a ∈A; s, s′ ∈S} and the discount of step \nreward in long-run experience γ.  \nMathematically, the cumulative rewards with discount factor γ and initial state s under some \npolicy π is R = Eπ[Σt=0\n𝑇𝛾𝑡𝑟𝑡|𝑠0 = 𝑠], where {rt|t = 0,1,2. . , T} is the set of reward signals in \neach time of state transition after some action.  \nAnd the policy to learn is a map from state space to action space as  \nπ: S ∗A →[0,1] \nπ(a|s) = p(at = a|st = s) \nThe learning process is to interact with environment, collect some experiences with state \ntransitions and rewards information and evaluate and improve policies. \nProximal Policy Optimization(PPO) \nIn cases of nonconvex optimization, gradient can be computed with numerical or sampling \nmethods, but a propriate learning rate in iterations is hard to determine and it need to vary with \ntime to ensure better performance. Earlier works on reinforcement learning also encounter such \ndilemma when using gradient based optimization technique, and simulated annealing algorithm is \nwidely used to determine learning rate with annealing factor during optimization process, \ngradually decreasing the step width of learning rate. However, it is still tough for policy \ngradient-based reinforcement learning algorithms to modify learning rate especially when training \nneural network.  \nTo circumvent the bottleneck, Schulman et al. [29] proposed Trust Region Policy \nOptimization(TRPO) algorithm to deal with random policy, in which Kullback-Leibler(KL) \ndivergence between old policy and updated policy is considered in objective function, and the KL \ndivergence in each state point can be bounded as well. The approach jumps out of modifying \nlearning rate, enforces the process of policy improvements more stable and is theoretically proved \nto monotonically increase the cumulative rewards. Considering the complexity of second order \nHessian matrix computations in TRPO, Schulman et al. [30] further developed one order \nderivative proximal policy optimization(PPO) algorithm.   \nThe surrogate loss function in original TRPO can be formulated as \nmax\nθ\n𝐿𝐶𝑃𝐼(𝜃) = max 𝐸𝑡\n~[𝑟𝑡(𝜃)𝐴𝑡\n~], rt(𝜃) =\n𝜋𝜃(𝑎𝑡|𝑠𝑡)\n𝜋𝜃𝑜𝑙𝑑(𝑎𝑡|𝑠𝑡) . \nWhere π is some stochastic policy, πθold is the parameters in policy in last time, and At\n~ \nestimates the advantage function of performing at conditioned on the state st at time step t. The \nobjective as the expectation is the empirical average over instances in mini-batch. \nThrough pruning the above surrogate loss function, we can obtain loss function in PPO as \nLCLP(𝜃) = 𝐸𝑡\n~[min(𝑟𝑡(𝜃)𝐴𝑡\n~, 𝑐𝑙𝑖𝑝(𝑟𝑡(𝜃), 1 −𝜖, 1 + 𝜖) 𝐴𝑡\n~]. \nWhere the function clip is  \nclip(x, xMIN, xMAX) = {\n𝑥, 𝑖𝑓 𝑥𝑀𝐼𝑁≤𝑥≤𝑥𝑀𝐴𝑋\n𝑥𝑀𝐼𝑁, 𝑖𝑓 𝑥< 𝑥𝑀𝐼𝑁\n𝑥𝑀𝐴𝑋, 𝑖𝑓 𝑥> 𝑥𝑀𝐴𝑋\n \nAnd the objective can be optimized with stochastic gradient descent, minimizing the KL \ndivergence and reducing workloads of modification.  \n4. Methodology \nIn the former sections, some basic knowledge about reinforcement learning have been introduced, \nand concerning challenges are presented. In this section, we would elaborate our proposed model, \nreferred to as VMAV-C, in decision-making process. Here, VMAV-C corresponds to a \ncombination of Variational Auto-Encoder(VAE), Mixture Density Network-Recurrent Neural \nNetwork(MDN-RNN), Attention-based Value Function(AVF) and Controller Model. Different \nfrom the covariation matrix adaptation evolution strategy utilized to optimize Controller Model in \nWM [15], we make use of PPO based Actor Critic(AC) algorithm [31] in Controller, and attention \nmechanism is considered in critic network for estimates of state value function. In AC algorithm, \ncritic network generally works in prior to actor network, since precise estimation of value function \ncan better accelerate the policy learning. \nAt first, some fundamental components in original framework VM-C [15], including VAE, \nMDN-RNN and Controller, are detailed as the background. And then the attention-based value \nfunction is highlighted, and the way to combine with Critic network is core of the framework. To \nunderstand how VMAV-C works, training process would be separately discussed. \n4.1 Outline of VM-C Model \n \nFig1. Framework of VM-C used in World Models.  \n \nThe Fig1 reveals the relationship between VAE, MDN-RNN and Controller model, and answers \nthe question on how VMC dynamically reacts to the environment. The lines in Fig1 indicate the \ninformation flow and control operations in given environment. The specific procedure in platform \nof OpenAI gym can be described as follows. \n \nDecision Process with VMC： \nobs=env.reset() \nh=rnn.initial_state() //initialize Recurrent Neural Networks \ndone=Flase \ncumulative_reward=0 //initialize the cumulative rewards when interacting with environment \nWhile not done: \nz=vae.encode(obs) //encode the observation of state in latent representation \na=controller(z,h) //input latent representation of observation and hidden information in \n//RNN \nnext_obs, reward, done, _=env.step(a) //perform action and receive responses \ncumulative_reward +=reward //cumulative reward in each step \nh=rnn.forward(a, z, h) //compute next time hidden information in RNN \nobs=next_obs \nreturn cumulative_reward \n \n \n4.1.1 VAE \nFormulated as the information compression technique, the auto-encoder (AE) attempts to encode \nthe original input into a vector of fixed length and then to decode such latent representation to \nreconstruct the input. As one of the commonly used AEs, variational auto-encoder (VAE) is used \nin learning the latent representation of some complex datasets or manifolds through variational \napproximations and reproducing some synthetic instances through sampling from latent space[32]. \nThe ideology of VAE is that some complicated dataset can be probabilistically generated from \nsome latent variables through a series of transformations. To enable the computation process \ntractable and efficient, VAE assumes variables z in latent space obey some multi-dimensional \nGaussian distribution z~N(0, I). Though the distribution of latent space can be more universal \naccording to specific hypothesis, the continuity of Gaussian distribution and the differentiation of \nneural network connections makes it viable to utilize the back propagation.  \nMore specifically, given some instance x, we wish to uncover its relationship with latent \nvariables z, so some neural network q(z|x) as encoder is used to approximate actual latent \nvariable distribution p(z|x). With the help of Bayes theorem, the Kullback-Leibler divergence is \ncomputed as the difference between two distributions \nDKL(𝑞(𝑧|𝑥)||𝑝(𝑧|𝑥)) = 𝐸𝑧~𝑞(𝑧|𝑥) log 𝑞(𝑧|𝑥) −𝐸𝑧~𝑞(𝑧|𝑥) log 𝑝(𝑧|𝑥) = 𝐸𝑧~𝑞(𝑧|𝑥) log 𝑞(𝑧|𝑥) −\n𝐸𝑧~𝑞(𝑧|𝑥) log 𝑝(𝑥|𝑧) −𝐸𝑧~𝑞(𝑧|𝑥) log 𝑝(𝑧) + log 𝑝(𝑥). \nEquivalently,  \nlog 𝑝(𝑥) = 𝐸𝑧~𝑞(𝑧|𝑥) log 𝑝(𝑥|𝑧) −DKL(𝑞(𝑧|𝑥)||𝑝(𝑧)) + 𝐷𝐾𝐿(𝑞(𝑧|𝑥)||𝑝(𝑧|𝑥)) ≥\n𝐸𝑧~𝑞(𝑧|𝑥) log 𝑝(𝑥|𝑧) −DKL(𝑞(𝑧|𝑥)||𝑝(𝑧)). \nThe left-hand side of inequality, which we wish to maximize, is computationally intractable, so the \nright-hand side term also referred to as evidence lower bound(ELBO) is taken as the objective, \nand the first term of ELBO is decoder in form of neural network structure. Hence, VAE makes a \ntrade-off between two loss functions, respectively as the construction error in decoder and the KL \ndivergence in encoder, and conceptually connects observed space to latent space probabilistically \nwith some differentiable neural network.  \nRecognized as a typical generative model, once VAE is well trained through back propagation, it \ncan draw some samples of compressed representations from latent space distribution p(z) and \nthen generate novel instances through decoder network p(x|z).  \n \nFig2. VAE in Observation of CartPole-V0. Encoder and Decoder are two neural networks, and mean \nvector and logarithm variance vector are latent representation for some state. \n \nAs displayed in Fig2, the input of VAE in our experiments is the observation of environment, \nnamely scenario image of CartPole-V0, and we compress this observation into some low \ndimension vector as the latent representation. \n4.1.2 MDN-RNN \nWith the mixture density model and conventional neural network aggregated, mixture density \nnetwork can approximate arbitrary conditional probability distributions, especially those with \ncontinuous input, and solve the inverse problems in practice. Meanwhile, recurrent neural \nnetwork(RNN) is proved to be efficient in capturing the dependencies in sequence datasets and \nperceiving the trend of sequence in some sense. Hence, some works focus on the combination of \nthese two techniques and present some variants of RNN, referred to as MDN-RNNs, in dealing \nwith real life problems [15, 33], and a recent interesting research is about applying MDN-RNN to \nsketch generation in drawings [33]. \nFor general purpose, RNN is used to model the conditional probability distribution \np(zt+1|at, zt, ht), where at, zt, ht respectively correspond to action, latent representation of state \nand sequence hidden information at the time step t, and zt+1 is the predicted state in time step \nt + 1 conditioned on at, zt, ht . When RNN meets reinforcement learning tasks, some \nmodifications are required for probability representations, since information whether the episode \nends in process need to be marked. Thus, with the involvement of additional variable dt+1to \nindicate whether the episode ends at time step t + 1, the probability can be parameterized in the \nform of p(zt+1, dt+1|at, zt, ht), which has been illustrated in Fig3. Especially, dt+1 is a binary \nvariable to predict, and the episode is predicted to end when its value turns to one instead of zero.  \nGaussian mixture model(GMM) is a mixture probability model, and take the bivariate response \nvariable as an example, GMM with m components of Gaussian distribution can be linearly \nrepresented as follows: \np(x, y) = Σj=1\n𝑀𝜃𝑗𝑁(𝑥, 𝑦|𝜇𝑥,𝑗, 𝜇𝑦,𝑗, 𝜎𝑥,𝑗, 𝜎𝑦,𝑗, 𝜌𝑥𝑦,𝑗) \nwhere Σj=1\n𝑀𝜃𝑗= 1 and {𝜇𝑥,𝑗, 𝜇𝑦,𝑗, 𝜎𝑥,𝑗, 𝜎𝑦,𝑗, 𝜌𝑥𝑦,𝑗} is respectively means, standard variances and \ncorrelation coefficients for Gaussian distribution indexed with j. In the mixture density network, \nexponential function, hyperbolic tangent function and softmax function are respectively employed \nto normalize the variance, correlation coefficient, and prior weight σ = exp 𝜎~ , 𝜌=\ntanh 𝜌~ , 𝜃𝑘=\nexp(𝜃𝑘\n~)\nΣ𝑗=1\n𝑀\nexp(𝜃𝑗\n~). To control the randomness of sampling in Gaussian distribution, the \ntemperature parameter τ is used to adjust the scale of prior weights and variances 𝜃𝑘\n~ →\n𝜃𝑘\n~\n𝜏, 𝜎2 →𝜏𝜎2. \n \n \n \nFig3. MDN-RNN. Each Box of LSTM Networks contains three LSTM units.  \n \nIn our learning task, we also encode the discrete action as f(at)1 and combine it with latent \nrepresentation of state zt and hidden information ht at time step t to guide the prediction of \nfuture state in environment as p(zt+1, dt+1|f(at), zt, ht). Additionally, the loss function of \nMDN-RNN comprises two components: the prediction error in next state Ls and the prediction \nerror in mark of ending state Lp. \nLs = −1\n𝑁Σ𝑖=1\n𝑁\nlog(Σj=1\n𝑀𝜃𝑗𝑁(𝑥, 𝑦|𝜇𝑥,𝑗, 𝜇𝑦,𝑗, 𝜎𝑥,𝑗, 𝜎𝑦,𝑗, 𝜌𝑥𝑦,𝑗)) \nLp = −\n1\n𝑁Σ𝑖=1\n𝑁(𝛼𝑑(𝑡+1)𝑖log 𝑞𝑖+ (1 −𝑑(𝑡+1)𝑖) log(1 −𝑞𝑖))), \nwhere 1 −qi is the predicted probability when mini-sequence ends at time step i. \nAs the proportion of ending states is quite limited, we place more weights of penalty on these \ninstances through enlarging the value of α > 1 in Lp. \nFinally, the total loss function is the weighted sum of two loss functions: \nLtotal = 𝛽1 ∗𝐿𝑠+ 𝛽2 ∗𝐿𝑝, \nwhere {𝛽1, 𝛽2} is the set of weights in loss terms. \nFig3 details the structure of MDN-RNN in our model and indicates dependencies between action, \nlatent representation of state, hidden information in mini-sequences of epoch and ending state of \nmini-sequence. \n4.1.3 Controller Model \nFunctioned as a decision maker, Controller Model in Fig4 plays a critical role and is expected to \nseek optimal action given specific state in each time step to maximize cumulative rewards. That is, \nMDN-RNN produces former well-encoded hidden state information as well as current state \ninformation for Controller, and the latter determines which action to select as at~𝜋(𝑎|𝑧𝑡, ℎ𝑡). \nSpecifically, the former mentioned state-of-art algorithm PPO is taken in our paper for policy \nlearning. \n \n                                                        \n1 In our task, discrete action is initialized as one-hot encoding and through a two-layer fully connected neural \nnetwork a 32-dimension vector is learned as the f(at). \n \nFig4. Controller Model. ht comes from hidden information in MDN-RNN, zt is latent \nrepresentation of current state, and action is conditioned on both information. Fully connected(FC) \nnetwork is used here. \n \n4.2 Involvement of AVF \nRecently, attention mechanism is attracting increasingly attention and frequently accompanied \nwith sequence learning, due to its fancy power in performance promotion in comparison to \nordinary sequence models. The operation of attention mechanism is to probabilistically assign \nvarious weights to hidden information of historical sequence and then aggregate them to form \ncontext vector for some predicting time steps, and the hidden information more related to \npredicting time step would receive more attention and be assigned more weight. That is, given \nhidden information of some t-length sequence H = [h1, h2, . . , ht], the context vector v for \npredicting time step serves as the embedding information for historical sequence and is computed \nas the weighted sum of hidden information in such time step v = Σi=1\n𝑡\n𝛼𝑖ℎ𝑖.  \nDuring the training process with reinforcement learning algorithm, we incorporate the attention \nmechanism in estimation of state value function, which suggests historical hidden information \nmay contribute to the estimation of current state value in varying weights.  \n \n \nFig5. Attention-based Value Function Representation. Four recent units containing hidden \ninformation from MDN-RNN contribute to state value estimation, and attention layer is to compute \nimportance of these information. \n \nIn Critic network of AC algorithm, hidden information of each time illustrated in Fig5 comes from \nMDN-RNN rout layer. And historical information in former n time steps is utilized for current \nstate value estimation. To ensure the initial state can also satisfy structure of attention, absent \nprevious hidden information such as {ℎ−3, h−2, h−1} in [ℎ−3, h−2, h−1, ℎ0] is initialized with \nzeros (Here take the case in Fig5 as an example).  \nThus, the context vector with attention can be computed as \nct = Σ𝑖=1\n𝑛𝛼𝑖ℎ𝑡−𝑖 \n𝛼𝑖=\nexp(𝛼𝑖\n~)\nΣ𝑗=1\n𝑛\nexp(𝛼𝑗\n~) \nαi\n~ = 𝑊[ℎ𝑡−𝑖, 𝑧𝑡] + 𝑏 \nWhere w, bi are the parameter in RNNs and 𝛼𝑖 reflect the strength of impact of historical \ninformation indexed with i in context vector ct. 𝑧𝑡 as the input in predicting time step is the next \nstep of state information. For the state value function estimation, it is required to combine both \ncontext information 𝑐𝑡 derived from previously hidden information {ht−1, ht−2, . . , ht−n} and \ncurrent state information as \nV(st) = Wv[𝑧𝑡, 𝑐𝑡] + 𝑏𝑣. \nWhere zt is the latent representation of state in time step t, ct is the context vector with \nattention, [. , . ] is the concatenation of vectors and {W, b, Wv, bv} is the set of parameters to \nlearn in attention-based value neural network. Fig5 reveals the learning process of state value, and \nthis structure is also the specific setting in our experiments. \n4.3 Training Details for VMAV-C RL \nEnvironment\nOriginal \nEpisode\n1\n1\n1\n(\n,\n,\n,\n,\n)\nt\nt\nt\nt\nt\nx a x\nr\nd\n\n\n\nRandom \nPolicy\nVAE Training \nDataset\nStates \nExtraction\nVAE\nMDN-RNN \nTraining Dataset\nEpisode \nExtraction\nAVF Model\n1\n1\n1\n(\n,\n,\n)\nt\nt\nt\nt\nh\nz\na\nh\n\n\n\n\ntz\ntx\nta\nEpisodes in \nIteration\n1\n1\n1\n1\n( , , \n, \n, \n, \n)\nt\nt\nt\nt\nt\nt\nz\na\nz\nd\nr\nh\n\n\n\n\nMAVC Training \nDataset\n1. VAE \nTraining\n2. MAV \nTraining\n4. Execution\n3. MAV-C \nTraining\n0. Episode \nCollection\nMDN-RNN\nController \nModel\nFig6. VMAV-C Reinforcement Learning Training Framework. Arrows suggest information flow in \nmodules. \n \nThe observations, which agent receives or recognizes from actual environment are raw 2-D \nimages with high dimensions, and the role of VAE in experiments is to compress images in lower \ndimensions. As illustrated in Fig2, VAE captures the latent representation of observation from the \nhidden layer and reconstructs the observation. Note that this preprocessing of states can be trained \nindependently from the whole model training.  \nSince the latent representations of observations in environment are in time series with the episode \ngoing, the correlation information as well as transition characteristics in environment is buried in \nthe latent space as well. MDN-RNN makes attempt to generate latent representation of \nobservation zt+1 and predict the ending state of sequence dt+1 in next time step conditioned on \ncurrent state zt and hidden sequence information ht after some specific action at. Of course, \nthe predicted latent representation of observation can be decoded for visualization through decoder \nlayers of VAE to understand the next observation. In the light of uncertainty in transitions of \nenvironment, the probability function p(z)  is utilized to estimate possible embedded \nrepresentations of states in the future. As formerly mentioned, GMM is trained to approximate the \nprobability distribution of embedded representation of state information of environment in next \ntime as p(zt+1, dt+1|at, zt, ht). Similar to the work [33], the temperature parameter τ is also \nincluded in sampling process to combat uncertainty. Intuitively, MDN-RNN would learn a model \nsimulating actual environment, which suggests how the environment varies with time conditioned \non some state after performing some specific action. And in our model, MDN-RNN would be \npretrained as initialization for training process.  \nThe Controller Model directly takes part in in training process and execution process. The reason \nwhy we introduce attention in Critic network in Controller model is due to different importance of \nhistorical hidden information in sequence for state value function estimation, and the following \nexperiments would demonstrate varying weights of importance on former sequences do bring \nbetter generalization. \nMore concrete illustration on training process is in Fig6, and the formerly induced components or \nsubset are tied in five modules, respectively as Episode Collection(Step 0), VAE Training(Step 1), \nMAV Training(Step 2), MAV-C Training(Step 3) and Execution(Step 4). And dependencies \nbetween components in different modules are also summarized in dotted boxes in Fig6. Notice \nthat these steps are in order of training and testing model, and detail roles of modules are \ndescribed in following sections. \n4.3.1 Pretraining Details \nThe purpose of VAE, MDN-RNN, AVF and Controller is to learn representations of states and the \ndynamic transitions in the environment at the same time, but massive parameters and complexity \nof network structures make it tough and time-consuming to train VMAV-C. Hence, synchronously \npretraining VMAV is the required step in our experiments. To achieve this aim, we collect 2000 \nepisodes with random policy strategy through a series of interactions with actual environment as \n{episode = {(xt, at, xt+1, rt+1, dt+1)}} in Step 0. These rollouts/screenshots of the environment \nserve as the training dataset for VAE, and we assume the sampling has approximately \nencompassed the dynamic information of the environment, especially the state representations and \nconcerning transitions.  \nFurther in Step 1, the whole dataset of states as the input for VAE are randomly partitioned into \ntwo sections, and 75% are for the training process while the rest are for testing the reconstruction \nperformance. During this process, the latent space for environment in the form of image frames is \nefficiently explored by monitoring the reconstruction error in testing dataset. Here, the images are \nreduced into 64 dimensions in latent space, since the dimension of multivariate normal distribution \nof latent space z is assumed as 32. Once the training process of VAE is completed, the collected \nimages can be encoded in lower dimensional vectors as partial inputs for MDN-RNN model.  \nAfter that, we can access to latent representations of states in formerly collected episodes, which \nare embedded with VAE. And these episodes are firstly merged into one long sequence according \nto order of time and then sliced into mini-sequences of fixed length as dataset to learn MDN-RNN. \nIn our experiments, the length of each mini-sequence is 32 steps in state transitions, and the \nmini-sequence would include some time step when epoch ends. After a few iterations as \ninitialization for MDN-RNN, AVF is also simultaneously included in pre-training process. Some \nmodification on ending time step is performed during AVF training as the random initialization of \nhistorical hidden information ht. In this way, AVF is preliminarily learned through pre-training, \nand a virtual environment buried in MDN-RNN is derived. These are the goals in Step 2 to \nachieve.  \n \nAlgorithm1. Pretraining VMAV-C Model \nInput: Initialized VAE, MDN-RNN, Controller and Attention Value Model initialized with random \npolicy \nOutput: Trained VAE, Pretrained MDN-RNN, Pretrained Attention Value Model \n(1) Rollout to actual environment N times with random policy. Save all actions, observations, \nrewards and ending indicators {episode = {(xt, at, xt+1, rt+1, dt+1)}}  during rollouts to \nstorage device \n(2) Collect observations of states {xt} to train VAE \nWhile VAE has not converged Do: \n  \nSample mini-batch of states in images \nlossvae =\n1\n𝑁Σ𝑖=1\n𝑁[(𝑉𝐴𝐸(𝑥𝑖) −𝑥𝑖)2 +\n1\n2 Σ𝑗=1\n𝑘(𝜇(𝑗)\n𝑥𝑖\n2 + 𝜎(𝑗)\n𝑥𝑖\n2 −ln 𝜎(𝑗)\n𝑥𝑖\n2 −𝑘)\n//The \n//number of examples in mini-batch N, the dimension variable z as latent space in VAE k  \n  \nUpdate VAE through Back Propagation // RMSProp as the default Optimizer \n(3) Collect MDN-RNN training dataset \nFor episode in storage: \n  \nTransform the episode to some fixed length sequence with L time steps \n  \nFor each time step: \n  \n \nFormulate transition as (zt = VAEEnco(xt), at, zt+1 = VAEEnco(xt+1), rt+1, dt+1)  \n  \nStore these mini-sequences of time steps in memory as MMDN−RNN \n(4) Train MDN-RNN \nWhile MDN-RNN has not converged Do: \n  \nSample batch from MMDN−RNN \n  \nCompute total loss function Ltotal = 𝛽1 ∗𝐿𝑠+ 𝛽2 ∗𝐿𝑝 in Section 4.1.2 \n  \nUpdate MDN-RNN through Back Propagation //Adam as the default Optimizer \n(5) Train AVF in AC algorithm \nWhile AVF has not converged Do: \n  \nSample mini-batch from MMDN−RNN in step (3) \n  \nGenerate rout and formulate training dataset as Fig5 \n  \nUse n-step return: \n  \n \nV~ = {Σ𝑡=0\n𝑇−1𝜆𝑡𝑟+ 𝐴𝑉𝐹(𝑧𝑇, ℎ𝑇), 𝑖𝑓 𝑑𝑇= 0\nΣ𝑡=0\n𝑇−1𝜆𝑡𝑟+ 0, 𝑖𝑓 𝑑𝑇= 1\n \nlossAVF = −𝐸(𝑉~ −𝐴𝑉𝐹(𝑧, ℎ))\n2  //T is the number of maximum of steps return \n//rewards, here 32 is the maximal for T because of length of mini-sequences.  \nUpdate AVF through Back Propagation //Adam as the default Optimizer \nNotation: Here {𝛽1, 𝛽2} is the set of hyper parameters to adjust the learning goals in different \nstages. \n \n4.3.2 Training Details \nIn Step 2, we have learned a virtual environment model based on MDN-RNN, which can \ntheoretically reveal transitions of states and reward signals. Hence, the following procedure would \nbe running reinforcement learning in such environment. In Step 3, AVF and Controller are trained \nthrough interactions with the virtual environment, and we utilize PPO algorithm to optimize \nController Model. We expect the virtual environment learned in pre-training as MDN-RNN has \nincluded dynamical properties of environment as precisely as possible and can function as the \nactual environment here.  \nIn Step 4, VAE, MDN-RNN and well-trained Controller from Step 3 are used in making decisions \nin the actual environment, since Controller is conditioned on information from latent state \nrepresentation z produced by VAE and historical hidden information h produced by MDN-RNN. \nNotice that apart from the use of the virtual environment information in policy learning, Step 4 in \nsome sense utilizes the actual environment information from sequential reward signals to improve \npolicy as well. \n \nAlgorithm2. MVA-C Model Training with PPO \nInput: Trained VAE, Well-trained MDN-RNN, Well-trained Attention Value Model, and Controller \nModel  \n(1) Environment initialization to obtain initial state \n(2) For i = 0,1, . . , K : \nDrive the agent to interact with virtual environment (MDN-RNN in pre-training process) \nto collect hidden information of RNN h, latent representations z embedded with VAE, \naction a and feedbacked reward signal r. \nOptimize the policy through PPO: \n \nLCLP(𝜃) = 𝐸𝑡\n~[min(𝑟𝑡(𝜃)𝐴𝑡\n~, 𝑐𝑙𝑖𝑝(𝑟𝑡(𝜃), 1 −𝜖, 1 + 𝜖) 𝐴𝑡\n~] \nUpdate state value network with minimization of mean square error(MSE): \nmin\nw 𝐸(Σ𝑡=1\n𝑇\n𝜆𝑡−1𝑟𝑡+ 𝐴𝑉𝐹(ℎ𝑇, 𝑧𝑇, 𝑤) −𝐴𝑉𝐹(ℎ, 𝑧), 𝑤)2 //w is the State Value Network \n//Parameter \n(3) End For \n \n5. Experiments and Performance Analysis \nIn this section, we would carry out experiments to accomplish a representative end-to-end \nreinforcement task, CartPole-V0 control based on the OpenAI Gym platform [20]. In this \nbackground, a cart is attached and un-actuated joined with a pole. The control decisions include an \nimplementation of a force of +1 or -1 to the car. The pendulum is initialized upright, and the \nprolonging the time of keep upright during the process is the target in the task. The end of the \nepisode is when the cart moves away from the ranges of 2.4 units in the center or the pole is over \nthe 15 degrees from the vertical. For the reward, the +1 signal is returned every timestep in the \ncondition the pole remains upright. \nBased on this end-to-end control problem, we would carry out a series of experiments with \nVMAV-C to test performance. \n5.1 Baseline Algorithms \nTo demonstrate the effectiveness of our proposed model VMAV-C, two baseline algorithms are \nintroduced for comparison. They are respectively Contractive PPO(CP) algorithm and MDN-RNN \nPPO(MRP) algorithm. \n1. Contractive PPO algorithm. This algorithm receives raw images from actual environment and \nthrow them into VAE to obtain encoded representations as inputs for agent to make decisions. \nAnd the agent directly interacts with the actual environment, and Actor Critic algorithm with \nPPO is also used in learning policy. That is, the process of learning model of virtual \nenvironment is not required here. \n2. MDN-RNN PPO algorithm. This algorithm is a variant of method in Ha D et al.’s work[15], \nbut the PPO is used in Controller model to learn policies instead of evolutionary algorithms. \nAnd the main difference from our model is the lack of attention mechanism in AC. \nThe settings are detailed in Section 5.2, and the performance of running three algorithms are \nalready summarized in Section 5.3. \nAdditionally, the scenario when the agent is completely trained to make decisions in virtual \nenvironment is also included in these experiments, which is specifically highlighted in Section 5.4. \nThat is, we would randomly sample some state from actual environment as the initial, run \nMDN-RNN in virtual environment, automatically generate future states given actions, feedback \nrewards and improve policy. And such learned policy is used in the actual environment to test the \naccumulative rewards without discount. This operation is different from Step 4, since the actual \nenvironment does not offer sequential reward signals in policy improvement but is merely for \ntesting policy learned from virtual environment every fixed number of epochs. So Controller as \nthe agent directly learns the policy through interactions with the virtual environment, and PPO is \nused for policy learning. \n5.2 Experimental Settings \nThe original image size in environment of CartPole-V0 is 400*600, and we reduce the size into \n160*320 at the center of cart position and further resize them into 40*80. To formulate dataset of \nenvironment states, we run 2000 episodes and record raw images of states, instant action, next \nframe of image, related reward and indicator of whether the episode ends through successively \ninteracting with actual environment. The concrete structure of VAE in our model is demonstrated \nin Fig7, and the training process refers to Section 4.3.1 in Algorithm1. \n \nkernel size=4x4\nstride=(2,2)\nkernel size=3x3\nstride=(2,3)\nkernel size=3x3\nstride=(2,2)\nkernel size=4x4\nstride=(2,2)\n\n\n(0,1)\nz\nN\n\n\n\n\nkernel size=4x6\nstride=(2,2)\nkernel size=3x3\nstride=(2,3)\nkernel size=3x3\nstride=(2,3)\nkernel size=4x4\nstride=(2,2)\nZ\nrelu conv 19x39x32\nrelu conv 9x13x64\nrelu conv 4x6x128\nrelu conv 1x2x512\ninput image 40x80x3\nfully connect \n32\n1024\n\n1x1024\nrelu deconv 4x6x128\nrelu deconv 9x13x64\nrelu deconv 19x39x32\nsigmod deconv 19x39x32\nEncoder\nDecoder\n \nFig7. The Network Structure of VAE in our Experiments. In experiments, the latent variable z \nobeys 32 dimensional multivariate normal distribution. \n \nOnce VAE is trained, we make use of it to encode the whole of formerly collected dataset of states \ninto latent representations, which are employed as partial inputs for MDN-RNN training. And then \nabout 1500 episodes of experience are merged as the training dataset for MDN-RNN, and we in \norder slice these into several mini-sequences to feed MDN-RNN at the step width 32. The rest of \ndataset, containing 500 episodes, is sliced in the same way to formulate testing dataset. However, \nthis manipulation also brings some uncertainty about context information, since the context \ninformation for initial state in mini-sequence theoretically depends on last mini-sequence but the \nmodel fails to include. In the pretraining process, the parameters for MDN-RNN are set as β1 =\nβ2 = 1, and the mini-batch size is 256, and Adam is chosen as the default optimizer with learning \nrate 4.77e-5, and the parameter τ to control randomness is 1. For the loss Lp in total loss of \nMDN-RNN, parameter α is tuned as 2 for mini-sequences with ending mark of state. Through a \nnetwork of three layers long short term memory(LSTM) units, MDN-RNN encodes the context \ninformation of mini-sequence into vectors and then learns the means and logarithm variances of \nfive Gaussian distribution as well as corresponding prior weights in GMMs to make predictions \ntowards next state of latent representation zt+1 ending state dt+1. \n5.3 Policy Learning in Actual Environment \nIn the CartPole-V0 control problem, the mentioned three models, CP Model, MRP Model and \nVMAV-C Model, are employed and compared in performance. We would respectively analyze \naccumulative rewards without discount and loss of state value network separately and uncover \npotential reasons behind phenomenon in Fig8.  \nThe number of iterations in training for these three models is 60000 and each iteration means the a \nsequence of up to 32 step transitions in interactions, and shadow curves which fluctuate fiercely \nare real cumulative values in results. To better display the results, Tensorboard [34] is used to \nsmoothen these results into dark colored curves. \n \n \nFig8. Cumulative Rewards and Value Network Losses with Three Models in the Actual \nEnvironment. Smooth rates in test rewards and value network losses are respectively set as 0.9 and \n0.95 in Tensorboard. \n \nLearning with CP Model \nAs mentioned before, CP Model also makes use of VAE to preprocess states and takes latent \nrepresentations of original images as the input. And the model directly interacts with the actual \nenvironments.  \nFrom Fig8, we can notice that with value network loss function deceasing, agent can gradually \nimprove policies, but this process requires massive experience and the learning speed is not ideal \nas well.  \nLearning with MRP Model \nApart from state representations in low dimensions with VAE, MDN-RNN can learn the \nenvironment model to perceive possible transitions and rewards in the future. It can be observed in \nFig8 that MRP Model benefits from additional information provided by the learned environment \nmodel, and the required volume of experiences is smaller but with more satisfying rewards in \ncomparison to CP Model. Meanwhile, there is a significant sharp declining tendency in value \nnetwork losses. \nLearning with Attention-based World Model \nIn Fig8, we can notice the involvement of attention mechanism in World Model does advance the \nperformance. Based on the observation of curves, VMAV-C converges to stable results of rewards \nin high level more quickly, and workload of sampling from actual environments to achieve \ncomparable stable optimal cumulative rewards is even less, suggesting the varying attention on \nhistorical information for prediction is reasonable in practice. The loss tendency in value network \nalso tells VMAV-C can help state value network reach the optimal more easily than other two \nalgorithms. \n5.4 Policy Learning in Complete Virtual Environment \nIn this section, we would make use of MDN-RNN model trained after 50 epochs, which means \ncycling the process of Algorithm2 in 50 times, and sample initial state to directly train agent in \nthe virtual environment. Details on model configurations has been described in Section 5.2. Each \niteration corresponds to a mini-sequence of up to 32 time step transitions’ learning in the virtual \nenvironment. Meanwhile, some testing on learned policies would be performed in actual \nenvironment every 1000 iterations in training. To further investigate the characteristics in virtual \nenvironment, we operate parameter sensitivity analysis on VMAV-C model, including the length \nof mini-sequences used in RNN and randomness parameter τ of GMM’s influence towards \nVMAV-C’s performance. Notice, there is no process of sequentially interacting with the actual \nenvironments as Step 4, and the actual environment no longer provides information for policy \nimprovement but functions as a testing bed for goodness of policy learned from the virtual \nenvironment every 1000 iterations. \nInfluence of Length in Mini-sequences \nHere, four scenarios corresponding to values of randomness parameter τ in GMMs in the list \n{0.6,0.8,1.0,1.2} are investigated in experiments. With randomness parameter fixed as some level, \nwe compare the performance of two structures of VMAV-Cs. One is to learn dataset of \nmini-sequence in 32 time steps, and another is to learn that in 16 time steps. Thus, the structure of \nAVF varies with the length of mini-sequence. Results in Fig9 show that though the virtual \nenvironment learned from 16 time steps mini-sequences enables agent to receive ideal cumulative \nrewards, it is still worse than that learned from mini-sequences of 32 time steps, and such gap \nincreases with iteration number in four levels of τ. With respect to the prediction accuracy for \nending state dt, VMAV-C learned from mini-sequences of 32 time steps achieves average \naccuracy over 0.2% higher than that with 16 recurrent units after convergence in four scenarios. \nElse, the distribution of 1500 episodes, which were sampled for the virtual environment training, \nare analyzed here, and we find episodes, whose length is less than 16 time steps, occupy a \nproportion of 38% and only 16% episodes are with length of over 32 time steps. As it turns out, \nlonger mini-sequences for MDN-RNN training can capture more dynamics in transitions, so \nVMAV-C trained from mini-sequences of 32 time steps is able to approach actual environment \nmore precisely, and this is not restricted to the volume of learning dataset. \n \n \n \n \n \n \nFig9. Performance Comparison of Two Referred Lengths of Mini-sequences. Figures in the left are \nnon-discount cumulative rewards, while the right ones are accuracies of predicting ending state in \nepochs. Smooth rates in Tensorboard are respectively 0.8 and 0.95 in tested rewards and label \naccuracies. \n \n \n \nFig10. Distribution of Number of Time Steps in Initially Collected Episodes. \n \nInfluence of Randomness Parameter in GMMs \nThe investigation on the length of mini-sequences indicates the length is crucial in ending state of \nepoch prediction, and in this subsection the influence of randomness parameter τ in GMM \ntowards VMAV-C performance is also studied. We map the collected results in Fig9 to another \nview, focusing on the randomness parameter τ in GMMs. Here, the smooth rate in Tensorboard is \nchosen as 0.95 for illustration in Fig11. For the case of VMAV-C learned from mini-sequence in \n16 time steps, we find higher τ values results in better cumulative rewards. For the case of \nVMAV-C learned from mini-sequence in 32 time steps, the optimal τ value for optimal \ncumulative rewards is in the range between 0.8 and 1. Similar to former analysis, it can be inferred \nthat higher τ value aggravates the uncertainty of environment. Hence, for the VMAV-C learned \nfrom mini-sequence in 32 time steps, which can well learn the environment model, it would \nencounter overfitting with higher τ values. However, smaller τ value would leverage the \nrobustness of model in some sense. So, the conclusion is the value of randomness parameter in \nGMMs should vary with power of learned environment model. \n \n \n \n \n \n \n \n \nFig11. Performance Comparison in Various 𝛕 Values of GMMs. The smooth rates in \nTensorboard are respectively set as 0.9 and 0.95 for tested rewards and accuracies of ending states. \n \n6. Conclussion \nIn this paper, we make modifications on former works about World Models and focused on \nimproving policy learning procedure, including incorporating attention mechanism in estimates of \nstate value and optimizing policy learning with PPO based AC algorithm. The experiment results \ndemonstrate the effectiveness of these improvements, and further prove that limited experiences \ncan still build task-beneficial virtual environment model with combination of VAE and \nMDN-RNN. In real environment, VMAV-C’s performance is superior to former works, and agent \ntrained in the virtual environment is capable of learning effective policy as well. Sensitivity \nanalysis suggests that with appropriate parameter selection of MDN-RNN, the learned \nenvironment model can approach real environment model more precisely. In the future, we would \nexplore methods to constructing more environment model and pay more attention to multi-agent \nsystem to boost simulation performance and efficiency. \nAcknowledgement \nThe authors declare no conflict of interest in this work and thanks to the sponsorship from \nNational Science Foundation No.71701205 and No.71701206. Meanwhile, Dr. Qi Wang gratefully \nacknowledges the financial support from China Scholarship Council and supervision from Prof. \nPeter M. A. Sloot during his study in the Netherlands. \n \nReference \n[1] \nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. \nRiedmiller, A. K. Fidjeland, and G. Ostrovski, “Human-level control through deep \nreinforcement learning,” Nature, vol. 518, no. 7540, pp. 529, 2015. \n[2] \nD. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. \nBaker, M. Lai, and A. Bolton, “Mastering the game of Go without human knowledge,” Nature, \nvol. 550, no. 7676, pp. 354, 2017. \n[3] \nR. Conde, J. R. Llata, and C. Torre-Ferrero, “Time-varying formation controllers for \nunmanned \naerial \nvehicles \nusing \ndeep \nreinforcement \nlearning,” \narXiv \npreprint \narXiv:1706.01384, 2017. \n[4] \nS. Shalev-Shwartz, S. Shammah, and A. Shashua, “Safe, multi-agent, reinforcement learning \nfor autonomous driving,” arXiv preprint arXiv:1610.03295, 2016. \n[5] \nP.-H. Su, M. Gasic, N. Mrksic, L. Rojas-Barahona, S. Ultes, D. Vandyke, T.-H. Wen, and S. \nYoung, “On-line active reward learning for policy optimisation in spoken dialogue systems,” \narXiv preprint arXiv:1605.07669, 2016. \n[6] \nQ. Wang, X. Zhao, J. Huang, Y. Feng, J. Su, and Z. Luo, “Addressing Complexities of \nMachine Learning in Big Data: Principles, Trends and Challenges from Systematical \nPerspectives,” 2017. \n[7] \nV. Pong, S. Gu, M. Dalal, and S. Levine, “Temporal difference models: Model-free deep rl for \nmodel-based control,” arXiv preprint arXiv:1802.09081, 2018. \n[8] \nA. Nagabandi, G. Kahn, R. S. Fearing, and S. Levine, \"Neural network dynamics for \nmodel-based deep reinforcement learning with model-free fine-tuning.\" pp. 7559-7566. \n[9] \nS. Kamthe, and M. P. Deisenroth, “Data-efficient reinforcement learning with probabilistic \nmodel predictive control,” arXiv preprint arXiv:1706.06491, 2017. \n[10] \nR. S. Sutton, \"Integrated architectures for learning, planning, and reacting based on \napproximating dynamic programming,\" Machine Learning Proceedings 1990, pp. 216-224: \nElsevier, 1990. \n[11] \nA. Kumar, A. Biswas, and S. Sanyal, “eCommerceGAN: A Generative Adversarial Network \nfor E-commerce,” arXiv preprint arXiv:1801.03244, 2018. \n[12] \nN. Heess, G. Wayne, D. Silver, T. Lillicrap, T. Erez, and Y. Tassa, \"Learning continuous \ncontrol policies by stochastic value gradients.\" pp. 2944-2952. \n[13] \nY. Chebotar, K. Hausman, M. Zhang, G. Sukhatme, S. Schaal, and S. Levine, “Combining \nmodel-based and model-free updates for trajectory-centric reinforcement learning,” arXiv \npreprint arXiv:1703.03078, 2017. \n[14] \nJ. W. Forrester, “Counterintuitive behavior of social systems,” Technological Forecasting and \nSocial Change, vol. 3, pp. 1-22, 1971. \n[15] \nD. Ha, and J. Schmidhuber, “World Models,” 2018. \n[16] \nL. Chang, and D. Y. Tsao, “The code for facial identity in the primate brain,” Cell, vol. 169, no. \n6, pp. 1013-1028. e14, 2017. \n[17] \nN. Nortmann, S. Rekauzke, S. Onat, P. König, and D. Jancke, “Primary visual cortex \nrepresents the difference between past and present,” Cerebral Cortex, vol. 25, no. 6, pp. \n1427-1440, 2013. \n[18] \nM. Leinweber, D. R. Ward, J. M. Sobczak, A. Attinger, and G. B. Keller, “A sensorimotor \ncircuit in mouse cortex for visual flow predictions,” Neuron, vol. 95, no. 6, pp. 1420-1432. e5, \n2017. \n[19] \nD. Mobbs, C. C. Hagan, T. Dalgleish, B. Silston, and C. Prévost, “The ecology of human fear: \nsurvival optimization and the nervous system,” Frontiers in neuroscience, vol. 9, pp. 55, 2015. \n[20] \nG. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba, \n“Openai gym,” arXiv preprint arXiv:1606.01540, 2016. \n[21] \nR. Sun, D. Silver, G. Tesauro, and G.-B. Huang, \"Introduction to the special issue on deep \nreinforcement learning: An editorial,\" 2018. \n[22] \nT. Kurutach, I. Clavera, Y. Duan, A. Tamar, and P. Abbeel, “Model-Ensemble Trust-Region \nPolicy Optimization,” arXiv preprint arXiv:1802.10592, 2018. \n[23] \nA. Piergiovanni, A. Wu, and M. S. Ryoo, “Learning Real-World Robot Policies by Dreaming,” \narXiv preprint arXiv:1805.07813, 2018. \n[24] \nA. V. Nair, V. Pong, M. Dalal, S. Bahl, S. Lin, and S. Levine, \"Visual reinforcement learning \nwith imagined goals.\" pp. 9208-9219. \n[25] \nD. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and J. Davidson, “Learning \nLatent Dynamics for Planning from Pixels,” arXiv preprint arXiv:1811.04551, 2018. \n[26] \nG. Cuccu, J. Togelius, and P. Cudre-Mauroux, “Playing Atari with Six Neurons,” arXiv \npreprint arXiv:1806.01363, 2018. \n[27] \nI. Clavera, J. Rothfuss, J. Schulman, Y. Fujita, T. Asfour, and P. Abbeel, “Model-based \nreinforcement learning via meta-policy optimization,” arXiv preprint arXiv:1809.05214, 2018. \n[28] \nA. Rajeswaran, S. Ghotra, B. Ravindran, and S. Levine, “Epopt: Learning robust neural \nnetwork policies using model ensembles,” arXiv preprint arXiv:1610.01283, 2016. \n[29] \nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, \"Trust region policy \noptimization.\" pp. 1889-1897. \n[30] \nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal policy \noptimization algorithms,” arXiv preprint arXiv:1707.06347, 2017. \n[31] \nV. R. Konda, and J. N. Tsitsiklis, \"Actor-critic algorithms.\" pp. 1008-1014. \n[32] \nD. P. Kingma, and M. Welling, “Auto-encoding variational bayes,” arXiv preprint \narXiv:1312.6114, 2013. \n[33] \nD. Ha, and D. Eck, “A neural representation of sketch drawings,” arXiv preprint \narXiv:1704.03477, 2017. \n[34] \nM. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, \nand M. Isard, \"Tensorflow: a system for large-scale machine learning.\" pp. 265-283. \n \n \n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.NE"
  ],
  "published": "2018-12-24",
  "updated": "2018-12-24"
}