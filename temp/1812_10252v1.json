{
  "id": "http://arxiv.org/abs/1812.10252v1",
  "title": "Optimizing Market Making using Multi-Agent Reinforcement Learning",
  "authors": [
    "Yagna Patel"
  ],
  "abstract": "In this paper, reinforcement learning is applied to the problem of optimizing\nmarket making. A multi-agent reinforcement learning framework is used to\noptimally place limit orders that lead to successful trades. The framework\nconsists of two agents. The macro-agent optimizes on making the decision to\nbuy, sell, or hold an asset. The micro-agent optimizes on placing limit orders\nwithin the limit order book. For the context of this paper, the proposed\nframework is applied and studied on the Bitcoin cryptocurrency market. The goal\nof this paper is to show that reinforcement learning is a viable strategy that\ncan be applied to complex problems (with complex environments) such as market\nmaking.",
  "text": "Optimizing Market Making using Multi-Agent Reinforcement Learning\nYagna Patel\nyagna.patel@berkeley.edu\nAbstract— In this paper, reinforcement learning is applied\nto the problem of optimizing market making. A multi-agent\nreinforcement learning framework is used to optimally place\nlimit orders that lead to successful trades. The framework\nconsists of two agents. The macro-agent optimizes on making\nthe decision to buy, sell, or hold an asset. The micro-agent\noptimizes on placing limit orders within the limit order book.\nFor the context of this paper, the proposed framework is applied\nand studied on the Bitcoin cryptocurrency market. The goal of\nthis paper is to show that reinforcement learning is a viable\nstrategy that can be applied to complex problems (with complex\nenvironments) such as market making.\nI. INTRODUCTION\nAlgorithmic trading, and in particular high-frequency al-\ngorithmic trading (HFT), has gained immense popularity in\nthe recent decade. With advances in hardware and software,\nalgorithmic trading has rapidly become the norm. The in-\ncreasing popularity of machine learning has slowly made\nits way to ﬁnancial markets [1], where it is primarily used\nto predict price movements of assets. However, there are\na number of challenges that these classic machine learning\ntechniques entail:\n1) Prediction time: In machine learning, model complex-\nity can have an impact on prediction time. Many times,\nin the supervised learning setting, neural networks are\nused to make predictions. Due to the computational\ncomplexity that comes with these models, as the model\ncomplexity increases, the decision time also increases\n[2]. In the HFT setting, by the time the model makes\na prediction, it may already be too late to take the\npredicted action. The problem then becomes, how can\nthese added latency costs be incorporated into our\nprediction?\n2) Prediction accuracy:\nFinancial markets are second-\norder chaotic systems, i.e. they are highly unpre-\ndictable since markets can respond to predictions. The\ngeneral rule of thumb in ﬁnance is that the historical\nperformance of an asset does not predict the future\nperformance of the asset, i.e. forecasting and predict-\ning the market from historical performance alone is\nvirtually impossible. In fact, markets are sometimes\ncompared to random walk processes [10]. Therefore,\napproaches that solely rely on the historical perfor-\nmance of an asset are likely to have low prediction\naccuracy.\n3) Policy optimization: Suppose that our model predicted\na 55% chance of increase and a 45% chance of\ndecrease for an asset. How can our model’s prediction\nbe converted into an action? An optimized policy and\ncertain decision thresholds are needed to turn the pre-\ndiction into an action. However, coming up with such\na policy is usually done by hand. The development\nand optimization of these policies are commonly done\nby mix of humans and computers. Therefore, if the\nmarket suddenly shifted, these policies would likely\nnot be able to adapt.\nII. RELATED WORK\nThe concept of reinforcement learning is relatively new in\nﬁnance. There has been little research done into whether or\nnot reinforcement learning is a viable approach for market\nmaking. The optimization problem of market making is a\ncomplex problem [11], and reinforcement learning is not a\ncommon approach used to solve it. Multi-agent approaches\nto stock trading have been taken previously [9]. However,\nthey do not account for placing limit orders. The general\nconcept of the micro-agent (see section 3) has been shown\nby Nevmyvaka et al. [3], and further extended by Juchli [4].\nThese papers present the problem of optimally buying (or\nselling) an asset over a ﬁxed time horizon. This is similar to\nthe idea of the micro-agent. The work for the micro-agent is\nsubstantially based, and builds upon the ideas presented in\nthese two papers.\nIII. PROBLEM FORMULATION\nIn this paper, a less common approach of reinforcement\nlearning is utilized to create an optimal market maker. More\nspeciﬁcally, a multi-agent approach (with two agents) is used:\n1) Macro-agent: The macro-agent is given minute tick\ndata (data at a macro-level) and makes the decision to\nbuy, sell, or hold the asset.\n2) Micro-agent: The micro-agent is given order book data\n(data at a micro-level) and makes the decision of where\nto place the order within the limit order book.\nFig. 1.\nTime-steps where agents take actions. The Macro-agent takes\nactions at every bold black line, while the Micro-agent takes actions between\nthe dashed lines.\nAt the start of every minute, the macro-agent makes the\ndecision to buy, sell, or hold the asset. A running count\nof how many buys it chooses to make is kept. The total\nnumber of buys before the sell is fed into the micro-agent,\n1\narXiv:1812.10252v1  [q-fin.TR]  26 Dec 2018\ni.e. all collected assets are exhausted on each decision to\nsell. Additionally, on each decision to sell, the count is reset.\nWithin the 60 second time horizon that follows, the micro-\nagent attempts to exhaust its held assets by only placing limit\norders. The agent may place multiple limit orders, however,\nthe it is limited to a single order placement every 10 seconds,\nas depicted in Fig. 1.\nA. Data Collection\nHistorical order book and minute tick data for markets is\nnot readily available. The ﬁrst step is to collect this data. The\nBitcoin cryptocurrency market is used in this study for the\nreason that its data is readily available. All trade, bid, and ask\ndata is collected by subscribing to Bittrex’s WebSocket in the\ndate range of November 2nd, 2018 to November 17th, 2018.\nWith data provided by the WebSocket, a historical order book\nis constructed, i.e. a sequence of historical order book states\nover the time period. A minute tick dataset is then created\nusing the trade data collected. In total, 41830629 trade, bid,\nand ask data points, and 10945 minute tick data points (Fig.\n2) were collected for this study. However, the entire dataset\nwas not used. Instead, only the most recent few days were\nchosen.\nFig. 2.\nBitcoin Minute tick data collected over 2018-11-02 to 2018-11-17\nIV. BACKGROUND: BRIEF OVERVIEW OF\nREINFORCEMENT LEARNING\nReinforcement learning (RL) is a learning technique\nwithin sequential decision making where an agent learns\nto take actions optimally in an environment. Unlike super-\nvised learning, in RL, the agent learns to take actions that\nmaximize the reward it receives from the environment. At\neach time-step, the agent observes its state and takes an\naction based on the observation. The environment provides\nfeedback on how well the action performed in the form of\na reward. Many times these rewards can be delayed. For\nexample, the decision to hold an asset may not yield an\ninstant reward.\nThe process of an agent interacting with the environment\nis formalized as a Markov Decision Process (MDP). MDPs\nare used to describe the environment in the context of an RL\nproblem. The importance behind MDPs in this problem is\nthe Markov property that MDPs assume: the effects of some\naction taken in some state depends only on that state and not\non prior states encountered. Market prices are Markovian in\nnature, i.e. the probability distribution of the price of an asset\ndepends only on the current price and not on the prior history.\nHence, it makes sense to formulate this problem as an MDP\nproblem with the goal being to solve this MDP by ﬁnding\nan optimal policy. For each agent, the tuple (S, A, P, r, γ)\ndescribes the problem:\n• S is the set of states.\n• A is the set of actions.\n• P : S × A × S →R is the transition probability distri-\nbution which determines how the environment changes\nbased on the state and actions taken by the agent.\n• r : S →R is the reward function.\n• γ ∈(0, 1) is the discount factor which determines the\nimportance of future rewards.\nLet Rt denote the sum of future rewards, i.e.\nRt = rt+1 + rt+2 + · · · + rT\nThen, at time-step t, the future discounted reward is then\ngiven by\nR′\nt = Rt+1 + γ · Rt+2 + γ2 · Rt+3 + . . . + γT · RT\n=\nT\nX\ni=t\nγi · Rt+i+1\n(4.1)\nwhere T is the last time-step, potentially inﬁnity.\nStates in the context of market making are much more\ncomplex than typical RL problems. In fact, the states en-\ncountered are most often not the complete states since there\nexist many other traders in the environment. Thus, the market\nmaking problem is a Partially Observable Markov Decision\nProcess (POMDP). The state the agent observes, S′, is some\nderivation of the true state, S, i.e. S′ ∼O(S). Nevertheless,\ngiven a proper simulation of the environment, the agent\nshould be able to optimize well against the unknowns in\nthe environment.\nAs previously mentioned, an RL agent continuously goes\nthrough a cycle of states by interacting with the environment.\nFor this problem, the interaction between the agents and the\nenvironment is depicted in Fig. 3.\nFig. 3.\nMulti-agent Reinforcement Learning Framework\nBoth agents encounter states and output actions. The\naction determined by the macro-agent is fed into the micro-\nagent, which collectively turns into a single action: placing\n2\nan order in the limit order book. The environment takes this\naction and outputs a reward and a next state. As noted in\nthe problem statement, discrete time-steps are chosen (rather\nthan continuous time-steps) for the reason that continuous\ntime-steps would not be possible in the real world since the\nWebSocket data itself arrives at discrete time-steps.\nA. Q-Learning\nThe speciﬁc reinforcement learning algorithm used for\nboth agents is deep Q-learning. Recall that the goal is to ﬁnd\nan optimal policy π : S →A, i.e. a function mapping be-\ntween states and actions such that it maximizes the expected\nreward. Q-learning is a model-free value-based approach,\nwhich uses the action-value function Q(s, a), where s ∈S\nand a ∈A, to estimate the expected reward given some state-\naction pair. In other words, under policy π, the true value of\nbeing in state s and performing action a is given by\nQπ(s, a) = Eπ [Rt | St = s, At = a, π]\n(4.2)\nTypically the Q-values are updated using the bellman equa-\ntion, which are derived by expanding (4.2):\nQπ(s, a) = Eπ [Rt | St = s, At = a, π]\n= Eπ\n\u0002\nRt+1 + γR′\nt+1 | St = s, At = a\n\u0003\n=\nX\ns′\nX\nr\nP(s′, r | s, a)\n\u0002\nr + γEπ\n\u0002\nR′\nt+1 | St+1 = s′\u0003\u0003\n=\nX\ns′,r\nP(s′, r | s, a) [r + γQπ(s′, πt(s′))]\nBased on this expansion, the update rule is given by:\nQt+1(st, at) =\nX\ns′,r\nP(s′, r | s, a) [r + γ · Qπ\nt (s′, πt(s′))]\nAlthough this update rule would lead to an optimal policy, it\nis unfeasible for this problem since P(s′, r | s, a) is unknown,\ni.e. the data collected does not provide a complete picture of\nthe market environment. Instead we use the following update\nrule:\nQt+1(st, at)\n= (1 −α) · Qt(st, at) + α\nh\nrt+1 + γ max\na\nQt(st+1, a)\ni\nwhere α is a learning rate.\nIn this problem, a neural network is used to approximate\nthe Q-value function where the input is the state (instead of\nstate-action pairs), and the output are the Q-values for each\nof the actions. Thus, the Q-value function is parameterized\nby weights θ, i.e. we assume that\nQ(s, a; θ) ≈Q∗(s, a)\nB. Exploration-Exploitation Trade-off\nIn reinforcement learning, there is a trade-off between\nexploration and exploitation: how often do we want our agent\nto continue taking actions with the policy it has determined\nversus how often do we want our agent to explore taking\nnew actions. To get a good balance between both worlds,\na decaying ϵ-greedy approach is used. In this approach, the\nagent takes random actions with probability ϵ and takes the\npolicy action with probability 1 −ϵ, while ϵ decays over\ntime. This strategy encourages the agent to explore more at\nthe beginning, and exploit more at the end, i.e. stick to the\npolicy it has computed.\nC. Experience Replay\nAnother concept that is used is experience replay. In online\nlearning, we receive data sequentially. Due to the nature\nof markets, there is always a concern for market shifts. If\nthe market suddenly shifted, the agent may forget its past\nexperiences as it aims to get a better advantage in the new\nstates. By storing the agent’s experiences in a memory buffer,\nand having the agent relive randomly sampled batches of\nexperiences often, the high temporal correlation in the data\nis reduced. Additionally, training on these randomly sampled\nbatches gives the feel of training on iid data. Therefore, as\nin supervised learning problems, this would yield a better\nconvergence for the function approximator to the optimal\npolicy. Furthermore, this allows to update the parameters of\nthe function approximator with a stochastic gradient descent\napproach.\nV. DETAILS OF THE MACRO-AGENT\nThe Macro-agent is responsible for making a discrete\ndecision to buy, sell, or hold an asset by looking at the\ndata from a macro level. The data used is minute tick data.\nAlthough the data is collected over a span of multiple days,\nthe entire data-set is not used for the reason that the agent\nmay under-explore the more recent and relevant states, and\nover-explore other states. Since most of the data had little\nvolatility, while the more recent data was much more volatile,\nthe agent would not have been able to perform well in volatile\nstates due to exploring the less volatile states more often.\nTherefore, the recent minute tick data spanning the course of\nthe last few days is used: November 15th, 2018 to November\n17th, 2018. Fig. 4 shows the portion of the data that was\nchosen.\nFig. 4.\nBitcoin Minute tick data (2018-11-15 00:00 to 2018-11-17 17:06)\nTo train the agent, the dataset is split into training and test\nsets. The training set consists of the time-steps prior to\nNovember 16th, 2018.\n3\nA. State\nThe state space at a certain time-step t consists of histor-\nical price data in the range t −h to t, where h denotes how\nfar back in history the agent looks. Furthermore, featurized\ndata of various momentum and reversion indicators are also\nincorporated into the state space. These indicator features are\ncomputed in the following ways:\nMarket Indicator Features\nOne commonly used feature in market trading is the\nz-score indicator, which determines how far (in standard\ndeviations σ) a data point x is from the mean µ:\nzx = x −µ\nσ\n(5.1)\nIn a way, the z-score indicator reveals anomalies in data. It\nhas also been proven to be a useful indicator in determining\nfuture trends [5]. Equation (5.1) can be applied by using\nan expanding window approach with a window size of n\ntime-steps. Let pt denote the closing price at time-step\nt, let SMA(pt−n,t) denote simple moving average (SMA)\nof closing prices over n time-steps prior to t, and let\nSTDDEV(pt−n,t) denote the standard deviation of closing\nprices over n time-steps prior to t. Thus, the z-score at time-\nstep t for a window size of n is\nzn\nt = pt −SMA(pt−n,t)\nSTDDEV(pt−n,t)\nThe z-score indicator can be extended to volume as well.\nThus, the following are the features that are extracted:\n• Price Level: To determine price levels, the z-scores are\ncalculated for the prices. This essentially expresses how\nfar each of the prices in the time period are from the\naverage price in the time period.\n• Price Change: To determine price changes, the current\nprice is compared to the average of a window of prices\nprior to it, i.e. calculated for a window size n at time-\nstep t\nPCn\nt =\npt\nSMA(pt−n,t) −1\nand take the z-score of the result.\n• Volume Level: To determine volume levels, the z-scores\nare calculated for the traded volumes. As with price\nlevels, this expresses how far the volume at each time\nstep is from the volume in the time period.\n• Volume Change: Similar to price change, volume\nchange for a window size n at time-step t is calculated:\nPCn\nt =\nvt\nSMA(vt−n,t) −1\nand the z-score of the result is taken.\n• Volatility: To determine volatility, exponential moving\naverages (EMA) are used to determine the rate of price\nchange over a span of n days. The reasoning behind\nusing EMA instead of SMA is that EMA gives more\nweight to the recent time-steps, whereas in SMA all\ntime-steps are weighted equally. Let pt be the current\nprice, and let n be the window size. The EMA at time-\nstep t is given by\nEMAn\nt =\n2pt\nn + 1 +\nPt\nj=t−n pj\nn\n\u0012\n100 −\n2\nn + 1\n\u0013\nThus, the volatility over m days is given by\nVolatilitym\nt = EMAt,n −EMAt−m,n\nEMAt−m,n\nAlong with these market indicator features and the price\ndata, there is an additional state parameter which plays an\nessential role in determining rewards. Each time the agent\nchooses to buy, the price at which the agent buys the asset\n(the current open price) is stored into a current assets list.\nThis list of prices is stored as part of the state as well. Each\ntime the agent chooses to sell, all of the bought assets are\nexhausted (sold). Thus, the historical prices, market indicator\nfeatures, and current assets list form the state.\nB. Action\nThere are three actions that the agent can choose:\n1) Buy: If the decision to buy is chosen, then the agent is\nchoosing to buy 1 bitcoin at the current opening price.\nAs mentioned previously, when the agent chooses to\nbuy, the price gets appended to the current assets list.\nNote that the decision to buy 1 bitcoin is purely a\ndesign choice. The proposed framework can be applied\nto any quantity of bitcoins.\n2) Sell: If the decision to sell is chosen, then the agent is\nchoosing to sell all of its accrued assets at the current\nopening price.\n3) Hold: If the decision to hold is chosen, then the agent\ndoes not do anything.\nC. Reward\nUpon taking an action, the agent receives a reward from\nthe environment. Algorithm 1 depicts how rewards are han-\ndled.\nAlgorithm 1: Macro-agent Reward Function\n1 if action = hold then\n2\nreward ←0\n3 else if action = buy then\n4\nAppend current open price to current assets list\n5\nreward ←0\n6 else if action = sell then\n7\nif there are no assets to sell then\n8\nreward ←−1\n9\nelse\n10\nreward ←sell off all assets from current assets\nlist and determine proﬁt based on current open\nprice\n11\nend\n12 Clip Rewards\nNote that the rewards are clipped to {−1, 0, 1} based on\nnegative, zero, or positive rewards, respectively. This is done\n4\nin order to have the agent deal with different types of market\nenvironments, e.g. high/low volatility, as well as reduce the\nimpact of anomalous market shifts. Clipping rewards has\nproven to be a useful method when dealing with different\nscales of rewards when in various Atari games [6].\nD. Deep Q-Network Architecture & Training\nTo parameterize the value function, a deep Q-network is\nused. A simple multilayer perceptron with two hidden layers\n(with ReLu activation functions) is chosen. Furthermore, the\nAdam optimizer is used for training. The Q-network takes\nin as inputs the price data, market indicator data, and the\ncurrent assets list, and outputs 3 Q-values which are then\nused to determine the optimal action. In Fig. 5 a depiction\nof the training framework for the macro-agent is provided.\nFig. 5.\nMacro-agent Training Framework\nThe details of how training happens within this framework\nare outlined in Algorithm 2. Note that for each epoch the\nagent is trained on the entire training set.\nE. Results\nIt is interesting to see how the macro-agent would perform\non its own. Although the closing and opening prices, and\nthe prices used to calculate proﬁts for the macro-agent are\nusually not indicative of the true prices where trades may\nhappen, they can hypothetically be assumed so to evaluate\nthe performance of the agent. For these results, the agent\nwas trained on the training set and tested on the test set, as\ndeﬁned previously, for 500 epochs.\nTo understand the potential of the macro-agent, its perfor-\nmance is compared to two common investment strategies:\n• Buy and Hold investing: Buy and Hold investing is a\nnaive long-term investment strategy where an investor\nbuys an asset at the start of a time period and holds\nthe asset in the hopes that it will accrue value over\ntime. From the looks of Fig. 4 it is not expected that\nthis strategy will yield any positive proﬁt as the general\ntrend in the test set is downwards. In this strategy, 10\nBitcoins are chosen to be bought and held over the time\nperiod. In order to simulate the Buy and Hold investing\nstrategy, at each time-step the proﬁt is plotted assuming\nthat the investor decided to sell at each time-step, i.e. if\nAlgorithm 2: Macro-agent Deep Q-Learning Training\n1 Initialize replay memory M\n2 Initialize ϵ, and discount factor γ\n3 Initialize Q network with random weights\n4 for epoch = 1 to E do\n5\nst ←Reset environment (environment outputs\ninitial state)\n6\nwhile not done do\n7\nat ←Select random action with probability ϵ or\nchoose action maxa Q(s, a; θ)\n8\nst+1, rt, done ←Act based on a (environment\noutputs new state, reward, and done)\n9\nif replay memory M is full then\n10\nRemove the ﬁrst element from M\n11\nAppend (st, at, rt, st+1, done) to replay\nmemory M\n12\nb ←Sample mini-batch from replay memory\nM at random\n13\nq ←Initialize empty array of size |b|\n14\nforeach (si, ai, ri, si+1) ∈b do\n15\nqi =\n(\nri + γ · maxa Q(si+1, ai; θ)\nnot done\nri\ndone\n16\nend\n17\nApply gradient descent with loss\nEs,a,r,s′ \u0002\n(qi −Q(si, ai; θ))2\u0003\n18\nDecay ϵ\n19\nend\n20 end\np0 is the price the asset was initially bought at, at each\ntime-step the difference pt −p0 is plotted.\n• Momentum investing: Momentum investing is where an\ninvestor chooses to buy or sell an asset at a certain\ntime-step given the performance of the asset in the last\nn steps. To implement this, a simple moving average of\nthe prices with a window size of n = 20 is computed,\ni.e. 20 minutes prior. At each time-step, if the current\nopen price is less than the average price in the minutes\nprior, 1 Bitcoin is bought. If the current open price is\ngreater than the average price in the minutes prior, all\nBitcoins bought thus far are sold. In the case they are\nequal, all assets are held.\nThese investment strategies serve as benchmarks and are\ncompared with the macro-agent. The macro-agent is similar\nto momentum investing with the only difference being that\nthe policy of when to buy, sell, or hold is different. Similar\nto momentum investing, the macro-agent buys one bitcoin\non a decision to buy and exhausts all assets on a decision\nto sell. Fig. 6 depicts the performance comparisons between\nthe different strategies.\nFig. 6 compares the strategies in terms of proﬁt. The accu-\nmulated proﬁt is plotted for the momentum investing strategy\nand the macro-agent. The macro-agent’s accumulated proﬁt\n5\nFig. 6. Performances of Various Investment Strategies Realized-PNL Graph\ngrowth is not very volatile, indicating that the macro-agent\nstrategy is stable as well.\nVI. DETAILS OF THE MIRCO-AGENT\nIn the previous section, the results indicated that the\nmacro-agent performed well compared to two known invest-\nment strategies. However, one issue with the macro-agent is\nthat it assumes that the trade occurs at the opening prices. In\nreal market environments a trade happening at the opening\nprice is most often not the case. Therefore, there lies this\nuncertainty in the framework thus far of what price the order\nshould be set at. This motivates the purpose for the micro-\nagent. Many of the ideas used here are derived from [3] and\n[4].\nA. Introduction\nIn ﬁnancial markets, there is an order book, a data structure\nthat holds all bid and ask price/volume values. Fig. 7 depicts\na snapshot of an order book state.\nFig. 7.\nExample of an Order Book State\nThe order book is separated into two parts: bids and asks. The\nbid section contains all the prices and quantities buyers are\nwilling to buy at. The ask section contains all the prices and\nquantities sellers are willing to sell at. There are two main\nways that a trader can place an order in the order book:\n1) Market Order: When a trader places a market order,\nthey are placing an order at the market price, i.e. the\nbest price of the opposing side of the order book.\nMost often, this order is immediately ﬁlled. However,\ndepending on the quantity the order was placed for,\nthe order might not be executed at the same prices, as\nit depends on how much quantity is available at mar-\nket price. Another disadvantage is that market orders\nusually come with additional trading fees. Depending\non the state, this may even yield negative proﬁt.\n2) Limit Order: When a trader places a limit order, they\nare essentially demanding the order price (or a better\nprice). A limit order guarantees the trader the order\nprice, however, it is possible for the order to never\nbe ﬁlled. When a trader places a limit order, they\nstand behind all the traders that have placed an order\nat that price. An advantage to placing limit orders\nis that since limit orders provide liquidity to other\nmarket participants, exchanges incentivize limit orders\nby removing additional trading fees.\nOnce a trader places an order, a match engine attempts to\nmatch the trader’s order to orders on the opposing order\nbook side. Only when there is a matching order does a trade\nhappen.\nThe agent should be able to optimally place orders in the\norder book. As noted previously, the agent has two order\noptions: market orders or limit orders. However, there is a\ntrade-off between placing market orders and placing limit\norders. Therefore, the goal of the agent is to optimize on\nthis trade-off: is there an action better than simply placing a\nmarket order at the beginning of the time horizon?\nThis is a challenging optimization problem since there are\nmany unknown variables here:\n• Other Market Participants: This agent is not the only\none trading on the market. There are many other market\nparticipants that the agent interacts with.\n• Adversarialness: Among these market participants,\nthere are also algorithmic traders, some of which might\nbe playing adversarially, e.g. Bitcoin whales.\nThe agent must also optimize over these unknowns as well.\nIn order to do that, the match engine needs to accurately\nsimulate the market. Since this problem is posed as an\nRL problem, such a simulator is essential, since the agent\nrequires reward feedback. An open-source matching engine\n[7] is used to help simulate the exchange match engine.\nHowever, since the data is limited to the time periods it was\ncollected for, the match engine simulator is only indicative\nto that time period. Therefore, there is still a disadvantage\nto using an artiﬁcial match engine.\nB. Environment\nIn reinforcement learning problems, the environment is\nessential since it provides the agent with subsequent states,\n6\nas well as reward feedback. The environment for this problem\nis vastly different than the macro-agent’s environment. The\nenvironment takes in as input the order side, i.e. buy or sell\nfrom the macro-agent.\nThe two major components of the environment are the\nmatching engine and the order book. The order book plays\na key role in the environment. Bid, ask, and trade data are\ncollected from the WebSocket. Additionally, at the start of\ndata collection, a snapshot of the current order book state\nfor the ﬁrst 20 levels on each side are taken. Based on this\nsnapshot, and the bid and ask data alone, the entire historical\norder book is created.\nThe data is again split into a training and test set similar\nto the macro-agent. During training, the environment starts\nat a random time-step within the training set (at the start\nof a minute). The environment lasts until the end of the\nminute. Within the minute, the agent chooses to place an\norder which the matching engine attempts to match. If for\nsome reason the order has not been matched until the last\ntime-step, the environment forces a market order for all\nremaining assets. The environment provides reward feedback\nonly when a trade has occurred, or when the time horizon\nhas been exhausted. This single interaction is deﬁned as one\nepoch.\nC. State\nAs part of the state, the concept of private and market\nvariables is used:\n• Private Variables: Private variables contain two fea-\ntures: quantity remaining, and time remaining.\n• Market Variables: Market variables contain order book\nstates and trade data from the past 30 time-steps. An\norder book state consists of the bid and ask prices and\nquantities up to 20 levels. Trade data consists of the\nprice and quantity, and order side at which the last trade\noccurred.\nD. Action\nThe action the agent decides is the price at which to place\na limit order. More speciﬁcally, the agent chooses an integer\naction a ∈[−50, 50], such that the price, pt, at time-step t\nis\npt = pmt + 0.10a\nwhere pmt is the market price. Thus, there are a total of 101\npossible discrete actions the agent can choose from.\nE. Reward\nRecall that the goal of the micro-agent is to optimally\nplace limit orders. Therefore, the agent is rewarded accord-\ningly. The reward function is the difference between what\nthe agent was able to sell at and the market price before\nthe order was placed. The Volume Weighted Average Price\n(VWAP) is used to determine the aggregated price the agent\nwas able to trade at. Thus, the reward function is\nRt = pmt −\n1\nP\ni vi\nX\np∈P\nvp · p\nwhere pmt is the market price, P\ni vi is the total volume of\nassets, and P is all the prices the agent ordered at.\nF. Deep Q-Network Architecture & Training\nFor this agent, a similar training framework to the macro-\nagent is used. However, a different network architecture is\nused.\nDueling Deep Q-Network\nFor many states, the actions a on the edge of the order\nbook may have little value to the states. Therefore, for some\nstates, it may be unnecessary to estimate the action values\nfor all actions. Dueling Deep Q-Networks (DDQNs) provide\na solution to this. First applied to Atari games, DDQNs not\nonly help to speed up training, but also yield more reliable\nQ-values [8].\nFig. 8.\nDueling Deep Q-Network Framework\nRecall that a Q-value can be decomposed into two parts:\nV (s), the value of being in a particular state s, and A(s, a),\nthe advantage of taking action a in state s, i.e. the Q-value\nis decomposed as\nQ(s, a) = V (s) + A(s, a)\nDDQNs decouple the prediction into two network streams\n(Fig. 8): value stream, which estimates V (s), and advantage\nstream, which estimates A(s, a). This decoupling allows the\nagent to learn which states might not be valuable enough\nto explore every action. After this decoupling, there is an\naggregation layer which aggregates the two streams by the\nfollowing:\nQ(s, a; θ, α, β)\n= ˆV (s; θ, β) +\n \nˆA(s, a; θ, α) −1\n|A|\nX\na′\nˆA(s, a′; θ, α)\n!\nwhere θ are the weights for the MLP, α are the weights for\nthe advantage stream, and β are the weight for the value\nstream.\nThe concept of dueling deep Q-networks is applied to\nthe micro-agent. Besides the dueling concept, the rest of\nthe training framework remains the same (see Fig. 3). The\nmodiﬁcations to Algorithm 2 are detailed in Algorithm 3.\n7\nAlgorithm 3: Micro-agent Deep Q-Learning Training\n1 Initialize replay memory M\n2 Initialize ϵ, discount factor γ\n3 Initialize Q network with random weights\n4 for epoch = 1 to E do\n5\nst ←Reset environment (environment outputs\nrandom state and gets inputs from macro-agent)\n6\nwhile t ⩽60 seconds do\n7\nat ←Select random action with probability ϵ or\nchoose action maxa Q(s, a; θ)\n8\nst+1, rt, done ←Act based on a (environment\noutputs new state, reward, and done, based on\nmatch engine output)\n9\nif replay memory M is full then\n10\nRemove the ﬁrst element from M\n11\nAppend (st, at, rt, st+1, done) to replay\nmemory M\n12\nb ←Sample mini-batch from replay memory\nM at random\n13\nq ←Initialize empty array of size |b|\n14\nforeach (si, ai, ri, si+1) ∈b do\n15\nqi =\n(\nri + γ · maxa Q(si+1, ai; θ)\nnot done\nri\ndone\n16\nend\n17\nApply gradient descent with loss\nEs,a,r,s′ \u0002\n(qi −Q(si, ai; θ))2\u0003\n18\nDecay ϵ\n19\nend\n20 end\nG. Results\nIn this section, the performance of the micro-agent strategy\nis evaluated. The basic goal behind this agent is to optimally\nplace limit orders to buy or sell an asset within the allotted\ntime horizon. Here, the agent is tested through two scenarios,\neach evaluating its performance on ideal and un-ideal states.\n1) Market Trending Downwards: In this scenario a time-\nstep is chosen within the test set where the market is\ngenerally trending downwards. Here, the ideal case is to buy\nan asset, since the price is generally decreasing.\nFig. 9 depicts the snapshot chosen to test the agent against\na downward trend. More speciﬁcally, the agent is tested on\nthe 07:36 to 07:37 time horizon.\nBuying an Asset\nWhen buying an asset on an upward trend, the agent\ndecided to take an action of −11. This meant setting a limit\norder to buy at $1.10 lower than market price. This resulted\nin immediate execution. This was the right choice to make,\nsince the market was trending downward.\nSelling an Asset\nWhen selling an asset on a downward trend, the agent\ndecided to take an action of 29. This amounted to a limit\nFig. 9.\nOrder Book Visualization for Downward Trend\norder price of $2.90 greater than the market price. Although\nthis may not seem like the optimal decision, given that the\nagent was able to recognize the price drop, it decided to\nimmediately sell off its asset. At such a price it is almost\nguaranteed for a trade to occur, as was the case here. The\nagent was able to sell off 1 bitcoin in the ﬁrst step (before\nthe market dropped).\n2) Market Trending Upwards: In this scenario a time-step\nwithin the test set is chosen where the market is generally\ntrending upwards. Here, selling an asset is the ideal case,\nsince the price is generally increasing.\nFig. 10.\nOrder Book Visualization for Upward Trend\nFig. 10 depicts the snapshot chosen to test the agent on\nan upward trend. More speciﬁcally, agent was tested on the\n10:09 to 10:10 time horizon.\nBuying an Asset\nWhen buying an asset on an upward trend, the agent\ndecided to take an action of 18. This meant setting a limit\norder to buy at $1.80 higher than market price. This resulted\n8\nFig. 11.\nCombined Multi-agent End-to-End Pipeline\nin immediate execution. This was the right choice to make,\nsince the market was trending upwards.\nSelling an Asset\nWhen selling on an upward trend, the agent decided to take\nan action of −21. This meant setting a limit order to sell at\n$2.10 lower than market price. This was a risky decision,\nsince the order may not have been matched. In fact, it took\ntwo steps for the agent to sell 1 bitcoin, i.e. it had to place 2\nlimit orders (each partially executed). In this case a negative\naction was the right choice to make since the price was\nincreasing.\nBased on these special cases, the micro-agent looks to be\nable to make optimal decisions. When the price is expected\nto drop, the agent makes the optimal decision to sell off\nall assets immediately (or limit buy assets at a lower price\nthan the market on the bid side). On the other hand, when\nthe price is expected to grow, the agent makes the optimal\ndecision to buy assets immediately (or sell assets at a lower\nprice than the market on the ask side).\nVII. COMBINING THE MACRO AND MICRO AGENTS\nIn Sections 5 and 6, an in-depth overview of the frame-\nworks for both the Macro and the Micro agents was given.\nNow, the two agents are combined to reﬂect the multi-\nagent framework deﬁned in Fig. 3. The end-to-end pipeline\ncontains 4 components, depicted in Fig. 11, which is brieﬂy\ndetailed as follows:\n1) WebSocket: Raw trade, bid, and ask data arrives\nthrough a WebSocket that is connected to the Bittrex\nexchange. As soon as data arrives, it is sent to the\nfeaturizers.\n2) Featurizers: There are two featurizers, one for each\nagent. The Macro-agent featurizer computes the market\nindicator features and the minute data based on the raw\ntrade data collected from the WebSocket. The Micro-\nagent featurizer determines the new state of the order\nbook based on the raw data.\n3) Macro-agent: As detailed in Section 5, the macro-agent\ndetermines the action to buy, sell, or hold the asset.\nThe macro-agent keeps a running count of how many\nassets it currently has. If the decision to sell is chosen,\nthen this count (total number of assets) is sent. If the\ndecision to buy is chosen, then a quantity of 1 is sent.\n4) Micro-agent: As detailed in Section 6, the micro-agent\ndetermines the action of where to place the order in the\nlimit book. The side of the order book and the quantity\nis determined by the data provided by the macro-\nagent. Once the micro-agent has decided an action, this\naction gets fed into the match engine which attempts\nto match the order. This decision is relayed back to\nthe environment for the agent to decide to cancel the\norder and create a new order. If the order is matched,\ni.e. turns into a trade, then the order book is updated.\nNote that in practice the match engine is the actual\nexchange.\nA. Final Results\nThe end-to-end pipeline is evaluated using the test set\ndeﬁned in Section 5. Similar to the evaluation of the macro-\nagent, the multi-agent strategy is compared to the Buy and\nHold investing and Momentum investing strategies.\nIn Fig. 12 the performance comparisons between the Buy\nand Hold investment, Momentum investment, Macro-agent,\nand Multi-agent (Macro-agent and Micro-agent) strategies\nare given. It is interesting to see that the multi-agent approach\nunder-performs in comparison to the macro-agent. Previ-\nously, in the evaluation of the micro-agent, it was noticed\nthat often times the agent would decide that the right decision\nwas to place an order towards the opposing side of the order\nbook, i.e. a price slightly worse than market price. Although\nthis resulted in immediate execution, this resulted in a drop\nin cumulative proﬁt.\nIn terms of total orders placed by the micro-agent in\nthe multi-agent setting, 91% of all orders placed were limit\norders. Thus, the micro-agent was able to optimize placing\nlimit orders. Note that in the other strategies hypothetical\n9\nFig. 12.\nPerformances of Various Investment Strategies PNL-Realized\nGraph (including Multi-agent)\nprices were assumed for which market orders would have\nbeen placed for. This would also result in additional exchange\nfees.\nOverall, the proposed multi-agent framework is able to\noptimize well. Additionally, the multi-agent approach is also\na stable strategy since the cumulative proﬁt growth is not\nvolatile.\nVIII. IMPROVEMENTS & FUTURE WORK\nAlthough considerable progress was made in creating an\nend-to-end framework for optimizing market making, there\nare still some areas of improvement:\n• Risk: One assumption made throughout this study was\nthat the agent was allowed to buy an unlimited num-\nber of assets. There is high risk associated with this\ndecision, since the agent is in control of how many\nassets to buy. When testing the multi-agent strategy, it\nwas found that there was a case when the agent was\nselling 84 bitcoins with relatively low proﬁt. The risk\nassociated with this is also high. The next step would\nbe to optimize on this by adding further constraints that\nlimit the number of buy actions the macro-agent takes.\nHowever, this requires the agent to better learn when to\nhold an asset. This gives rise to the next concern.\n• Reward Engineering: Reward engineering is an im-\nportant problem in reinforcement learning. In the case\nof the macro-agent, it was found that the decision to\nhold an asset was very sparse. This may have resulted\nfrom the agent receiving a reward of 0 for the decision\nto hold. Perhaps setting the constraints as previously\nmentioned along with a reward function that is able\nto provide better reward feedback for holds, may help\nthe agent learn to hold assets for a longer time. Fur-\nthermore, incorporating exchange fees in the reward\nfunction will lead to an agent that can successfully\ninteract with the exchange environment.\n• Market Simulator: Although the simulator used in this\nframework did provide a decent reﬂection of the true\nmarket environment, it still lacked many variables.\nSpeciﬁcally the effect of other traders. The assumption\nthat the micro-agent was the only trader interacting\nwith the order book was also made. One option here\nis to create a generative model that can create synthetic\norders that simulate various traders.\n• Corrupted Data: Another concern is corrupted data. On\nmultiple instances, it was found that data would arrive\nout of order, or only partial (missing) data was received\ndue to network issues. In practice, it is critical that the\nstates match the actual exchange. Therefore, a solution\nto this is needed before this framework can be deployed\non an actual exchange.\nIX. CONCLUSION\nA multi-agent reinforcement learning framework was pro-\nvided to solve the problem of optimizing market making.\nThe results showed that the policy these agents were able\nto learn led to a stable trading strategy which resulted in a\nlow-volatile linear growth in proﬁt. Applying reinforcement\nlearning to such a problem also shows that it has the ability\nto perform well in complex environments, and is a viable\ntool that can be used for market making.\nREFERENCES\n[1] Matthew F Dixon. “A High Frequency Trade Execution Model for\nSupervised Learning.” arXiv preprint arXiv:1710.03870 (2017).\n[2] Prakhar Ganesh, Puneet Rakheja. “Deep Reinforcement Learning in\nHigh Frequency Trading.” arXiv preprint arXiv:1809.01506 (2018).\n[3] Yuriy\nNevmyvaka,\nYi\nFeng,\nMichael\nKearns\n“Rein-\nforcement\nLearning\nfor\nOptimized\nTrade\nExecution.”\nhttps://www.cis.upenn.edu/ mkearns/papers/rlexec.pdf (2006).\n[4] Marc\nJuchli\n“Limit\norder\nplacement\nopti-\nmization\nwith\nDeep\nReinforcement\nLearning”\nhttps://repository.tudelft.nl/islandora/object/uuid:e2e99579-541b-\n4b5a-8cbb-36ea17a4a93a?collection=education (2018).\n[5] “Use\nZ-Scores\nTo\nMaximize\nYour\nPortfolio’s\nPotential”\nhttps://seekingalpha.com/article/3224666-use-z-scores-to-maximize-\nyour-portfolios-potential (2015).\n[6] Hado van Hasselt, et al. “Learning values across many orders of\nmagnitude” arXiv preprint arXiv:1602.07714 (2016).\n[7] Gavin\nChan.\n“Light\nMatching\nEngine”\nhttps://github.com/gavincyi/LightMatchingEngine (2017).\n[8] Ziyu Wang, et al. “Dueling Network Architectures for Deep Rein-\nforcement Learning” arXiv preprint arXiv:1511.06581 (2016).\n[9] Jae\nWon\nLee,\nJangmin\nO.\n“A\nMulti-agent\nQ-learning\nFramework\nfor\nOptimizing\nStock\nTrading\nSystems”\nftp://ftp.cse.buffalo.edu/users/azhang/disc/springer/0558/papers/2453/\n24530153.pdf (2002).\n[10] Investopedia\n“Random\nWalk\nTheory”\nhttps://www.investopedia.com/terms/r/randomwalktheory.asp (2018).\n[11] Olivier\nGueant.\n“Optimal\nmarket\nmaking”\narXiv\npreprint\narXiv:1605.01862 (2017).\n10\n",
  "categories": [
    "q-fin.TR",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2018-12-26",
  "updated": "2018-12-26"
}