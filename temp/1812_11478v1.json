{
  "id": "http://arxiv.org/abs/1812.11478v1",
  "title": "DART: Domain-Adversarial Residual-Transfer Networks for Unsupervised Cross-Domain Image Classification",
  "authors": [
    "Xianghong Fang",
    "Haoli Bai",
    "Ziyi Guo",
    "Bin Shen",
    "Steven Hoi",
    "Zenglin Xu"
  ],
  "abstract": "The accuracy of deep learning (e.g., convolutional neural networks) for an\nimage classification task critically relies on the amount of labeled training\ndata. Aiming to solve an image classification task on a new domain that lacks\nlabeled data but gains access to cheaply available unlabeled data, unsupervised\ndomain adaptation is a promising technique to boost the performance without\nincurring extra labeling cost, by assuming images from different domains share\nsome invariant characteristics. In this paper, we propose a new unsupervised\ndomain adaptation method named Domain-Adversarial Residual-Transfer (DART)\nlearning of Deep Neural Networks to tackle cross-domain image classification\ntasks. In contrast to the existing unsupervised domain adaption approaches, the\nproposed DART not only learns domain-invariant features via adversarial\ntraining, but also achieves robust domain-adaptive classification via a\nresidual-transfer strategy, all in an end-to-end training framework. We\nevaluate the performance of the proposed method for cross-domain image\nclassification tasks on several well-known benchmark data sets, in which our\nmethod clearly outperforms the state-of-the-art approaches.",
  "text": "DART: DOMAIN-ADVERSARIAL RESIDUAL-TRANSFER\nNETWORKS FOR UNSUPERVISED CROSS-DOMAIN IMAGE\nCLASSIFICATION\nA PREPRINT\nXianghong Fang\nSMILE Lab\nUniv. of Electronic Sci & Tech.of China\nChengdu, Sichuan 611731 China\nHaoli Bai\nDept Computer Science and Engineering\nThe Chinese University of Hong Kong\nShatin, N. T. Hong Kong, China\nZiyi Guo\nSMILE Lab, Sch. of Computer Science and Engineering\nUniversity of Electronic Science and Technology of China\nChengdu, Sichuan 611731 China\nBin Shen\nGoogle Inc.\nSteven Hoi\nSchool of Information Systems (SIS)\nSingapore Management University\nZenglin Xu∗\nSMILE Lab, Sch. of Computer Science and Engineering\nUniversity of Electronic Science and Technology of China\nChengdu, Sichuan 611731 China\nzenglin@gmail.com\nJanuary 1, 2019\nABSTRACT\nThe accuracy of deep learning (e.g., convolutional neural networks) for an image classiﬁcation task\ncritically relies on the amount of labeled training data. Aiming to solve an image classiﬁcation task on\na new domain that lacks labeled data but gains access to cheaply available unlabeled data, unsupervised\ndomain adaptation is a promising technique to boost the performance without incurring extra labeling\ncost, by assuming images from different domains share some invariant characteristics. In this paper,\nwe propose a new unsupervised domain adaptation method named Domain-Adversarial Residual-\nTransfer (DART) learning of Deep Neural Networks to tackle cross-domain image classiﬁcation\ntasks. In contrast to the existing unsupervised domain adaption approaches, the proposed DART\n∗Corresponding author.\narXiv:1812.11478v1  [cs.CV]  30 Dec 2018\nA PREPRINT - JANUARY 1, 2019\nnot only learns domain-invariant features via adversarial training, but also achieves robust domain-\nadaptive classiﬁcation via a residual-transfer strategy, all in an end-to-end training framework. We\nevaluate the performance of the proposed method for cross-domain image classiﬁcation tasks on\nseveral well-known benchmark data sets, in which our method clearly outperforms the state-of-the-art\napproaches.\nKeywords Transfer Learning · Residue Network · Adversarial Domain Adaptation\n1\nIntroduction\nRecent years have witnessed remarkable successes of deep learning methods, especially the Deep Convolutional Neural\nNetworks (CNN), for various image classiﬁcation and visual recognition tasks in multimedia and computer vision\ndomains. The successes of deep neural networks for image classiﬁcation tasks critically rely on large amounts of\nlabeled data, usually leading to networks with millions of parameters [Lin et al., 2014, Russakovsky et al., 2015]. In\npractice, when solving an image categorization task in a new domain, collecting a large amount of labeled data is often\ndifﬁcult or very expensive, yet a large amount of unlabeled data is cheaply available. How to leverage the rich amount\nof unlabeled data in the target domain and to resort to an existing classiﬁcation task from a source domain has become\nan important research topic, which is often known as unsupervised domain adaption or transfer learning [Pan and Yang,\n2010, Pan et al., 2017, Wang and Deng, 2018]. The goal of this work is to explore new unsupervised domain adaption\ntechniques for cross-domain image classiﬁcation tasks.\nUnsupervised domain adaptation has been actively studied in literature. One of the dominating approaches seeks to\nbridge source domain and target domain through learning a domain-invariant representation, and then training an\nadaptive classiﬁer on the target domain by exploiting knowledge from the source domain. Following such kind of\nprinciple, several previous works have proposed to learn transferable features with deep neural networks [Long et al.,\n2015, Tzeng et al., 2014, Long et al., 2016, 2017, Zhuo et al., 2017], by minimizing a distance metric of domain\ndiscrepancy, such as Maximum Mean Discrepancy (MMD) [Gretton et al., 2006]. Recently, inspired by Generative\nAdversarial Networks (GANs) [Goodfellow et al., 2014], a surge of emerging studies proposed to apply adversarial\nlearning for unsupervised domain adaptation [Tzeng et al., 2017, Ganin and Lempitsky, 2015, Liu and Tuzel, 2016, Liu\net al., 2017, Taigman et al., 2016, Bousmalis et al., 2016, Ganin et al., 2016], validating the advantages of adversarial\nlearning over traditional approaches in minimizing domain discrepancy and obtained new state-of-the-art results on\nbenchmark datasets for unsupervised domain adaptation.\nAmong the emerging GAN-inspired approaches, DANN [Ganin et al., 2016] represents an important milestone. Based\non the common low-dimensional features shared by both source and target domains, DANN introduces a domain\nclassiﬁer borrowing the idea from GAN to help learn transferable features. The domain classiﬁer and the feature\nrepresentation learner are trained adversarially, where the former strives to discriminate the source domain from the\ntarget domain, while the latter tries to learn domain indistinguishable features from both domains. Then a label classiﬁer\nis deployed to predict the labels of samples from both domains with the learned domain-invariant features.\nDespite the success of DANN, it has two major limitations. First, it assumes that the image class label classiﬁer of\nthe source domain can be directly applied to the target domain. However, in practice there could be some small shifts\nacross the label classiﬁers in two domains, since the image classiﬁcation tasks on both two domains can be quite\ndifferent. An intuitive example is illustrated in the ﬁrst column of Figure 1, where the source classiﬁer fails to correctly\nclassify data from the target domain. Second, the label information of labeled training data in the source domain is\nnot exploited when learning the domain-invariant features. In other words, minimizing the discrepancy of marginal\ndistributions (i.e., without exploiting label information) may only lead to some restrictive representations lacking strong\nclass discriminative ability.\nIn this paper, we propose a new unsupervised domain adaptation approach named Domain-Adversarial Residual-Transfer\n(DART) learning for training Deep Neural Networks to tackle cross-domain image classiﬁcation tasks. Speciﬁcally, the\nproposed DART architecture consists of three key components: image feature extractors, image label classiﬁers, and a\ndomain classiﬁer, which will be discussed in detail in Section 2. DART inherits all the advantages of DANN-based\nnetwork architectures for domain-adversarial learning, but makes the following two important improvements.\nFirst, in order to model the shifts across the label classiﬁers in different domains, we introduce a perturbation function\nacross the label classiﬁers, and insert the ResNet [He et al., 2016] into the source label classiﬁer to learn the perturbation\nfunction, since ResNet has demonstrated superior advantages in modelling the perturbation via a shortcut connection\nas shown in [Long et al., 2016]. With the learned perturbation function, the label classiﬁer could be more robust\nand accurate. An intuitive illustration is shown in Figure 1, where the target classiﬁer with the perturbation function\ncorrectly classiﬁes image samples from the target domain.\n2\nA PREPRINT - JANUARY 1, 2019\nSource Classifier\nTarget Classifier\nPerturbation Function\nFigure 1: A well-trained source classiﬁer may fail to classify images in the target domain correctly. By adding a\nperturbation function, the target classiﬁer corrects the mistakes made by the source classiﬁer on the target domain. Here\nthe red dots and green triangles denote image samples from the target domain.\nSecond, to learn representations that are more discriminative and robust, we exploit the joint distributions of image\nfeatures and class labels to align the source domain and the target domain. Speciﬁcally, our model is established based\non a relaxed but more general assumption in that the joint distribution of both image data and class labels in the source\ndomain is different from that joint distribution in the target domain. Therefore, our model seeks to reduce their joint\ndiscrepancy when learning the common feature space. Notice that class labels in the target domain are replaced by\npseudo labels predicted by existing classiﬁers since these labels are unavailable during training in unsupervised domain\nadaption. Instead of minimizing the discrepancy on marginal distributions, minimizing the joint discrepancy learns\nmore discriminative domain-invariant features, as the joint distributions leverage the additional label information. We\nfurther regularize the model by minimizing the entropy of the predicted labels of the target domain, which ensures\nclassiﬁcation predictions from the target classiﬁer stay away from low-density regions.\nAs a summary, this work makes the following major contributions:\n1. Our work incorporates the learning of the perturbation function across label classiﬁers into the adversarial\ntransfer network. This makes the transfer learning more adaptable to real-world image domain adaptation\ntasks.\n2. Our work focuses on minimizing the discrepancy of the joint distributions of both image samples and class\nlabels in the adversarial learning scheme, thus is able to yield domain-invariant features which are more\ndiscriminative .\n3. We conduct extensive evaluation of cross-domain image classiﬁcation on several benchmarks, in which DART\nclearly outperforms the state-of-the-art methods on most cases.\n2\nCross-Domain Image Classiﬁcation\n2.1\nProblem Setting\nTable 1: Notations\nxs\ni, xt\ni\nimage samples from source & target domain\nys\ni , yt\ni\nlabels from source & target domain\nDs, Dt\nData of source and target domain\nds\ni, dt\ni\ndomain labels for source & target domain\nGf(·), Gd(·)\nfeature extractor & domain classiﬁer\nθ\nparameters for certain function\nσ(·)\nsoftmax function\nWe consider a cross-domain image classiﬁcation task by following a common setting of unsupervised domain adaption.\nSpeciﬁcally, consider a collection of Ns image samples and class labels from a source domain Ds = {(xs\ni, ys\ni )}Ns\ni=1,\nand Nt unlabeled image samples from a target domain Dt = {xt\ni}Nt\ni=1. The goal of unsupervised cross-domain image\nclassiﬁcation is to adapt a classiﬁer trained using only labeled data from the source domain and unlabeled data from the\ntarget domain, such that the domain-adaptive classiﬁer can correctly predict the labels yt of images from the target\ndomain.\n3\nA PREPRINT - JANUARY 1, 2019\nIt is important to note that a major challenge of this problem is the absence of labeled data in the target domain,\nmaking it different and much more challenging from many existing supervised transfer learning problems. Another key\nchallenge of unsupervised domain adaption is that the source image classiﬁer trained on the source domain Ds cannot\nbe directly applied to solve the image classiﬁcation tasks in the target domain Dt, because the image data between the\nsource domain and the target domain can have large discrepancy, and their joint and marginal distributions are different,\ni.e. p(xt, yt) ̸= p(xs, ys) and p(xt) ̸= p(xs), where yt is the true underlying target class labels.\n2.2\nOverview of Proposed DART\nTo minimize the discrepancy of the source domain and the target domain effectively, we propose the Domain-Adversarial\nResidual-Transfer learning (DART) of training Deep Neural Networks for unsupervised domain adaptation, as shown in\nFigure 2. The proposed DART method is based on two assumptions.\nFirst of all, DART assumes the joint distributions of labels and high-level features of data should be similar. The\nhigh-level features are extracted by a feature extractor Gf(·) parameterized by θf. Then a Kronecker product is applied\non high-level features and the label information to obtain joint representations. Finally, these joint representations are\ncollectively embedded into a domain classiﬁer Gd(·) parameterized by θd to ensure p(Gf(xt), ˆyt) ≈p(Gf(xs), ys),\nwhere ˆyt represents the predicted label.\nFigure 2: The architectures of Domain-Adversarial Residual-Transfer Networks (DART), which consists of a deep\nfeature extractor (yellow), a deep label classiﬁer (blue) and a domain classiﬁer (purple). ⊕means the plus operator\nwhile ⊗represents the Kronecker product. The crossing lines with black points indicate connections, while crossing\nlines without black points are independent and not connected. The domain classiﬁer is connected to the feature extractor\nvia a gradient reversal layer. During the training process, we minimize the label prediction loss (source examples for\nLY and target examples for LH) and the domain classiﬁcation loss (for all samples). Gradient reversal ensures that the\njoint distributions over the two domains are made similar, thus resulting in the domain-invariant features.\nSecond, DART assumes the label classiﬁer for the target domain differs from that of the source domain, and the\ndifference between the two classiﬁers can be modeled by the following\np(yt|xt) = p(ys|xs) + ϵ,\nwhere ϵ is a perturbation function across the source label classiﬁer Gs(·) and the target label classiﬁer Gt(·). In order\nto learn the perturbation function, one way is to introduce residual layers [He et al., 2016] parameterized by θr by\ninserting into the source label classiﬁer. Note that the predicted labels are ˆyt = Gt(Gf(xt)). In the following sections,\nwe introduce each module in detail.\n2.3\nDomain Adversarial Training for Joint Distribution\nMinimizing the domain discrepancy is crucial in learning domain-invariant features. A number of previous work seek\nto minimize over a metric, i.e., MMD [Gretton et al., 2006]. These methods have been proved experimentally effective,\nhowever, they suffer from large amount of hyper-parameters and thereon difﬁculty in training. A more elegant way is to\n4\nA PREPRINT - JANUARY 1, 2019\noperate on the architecture of the neural network. DANN [Ganin et al., 2016] is a representative work in which domain\ndiscrepancy is reduced in an adversarial way, leading to an easier and faster training process.\nInspired by DANN, we devise an adversarial transfer network to collectively distinguish the joint distribution\np(Gf(xs), ys) and p(Gf(xt), ˆyt). Speciﬁcally, for each domain, we fuse the high-level features from the feature\nextractor and labels together via a Kronecker product, i.e. Gf(xs\ni) ⊗ys\ni and Gf(xt\ni) ⊗ˆyt\ni, and then embed them into the\ndomain classiﬁer Gd(·) so as to minimize the discrepancy of their joint distributions in an adversarial way. The feature\nextractor seeks to learn indistinguishable features from the source domain and the target domain, while the domain clas-\nsiﬁer is trained to discriminate the domain of features correctly. In the domain classiﬁer, we manually label the source\ndomain as ds\ni = 1, and the target domain as dt\ni = 0. We introduce the gradient reversal layer (GRL) as proposed\nin [Ganin et al., 2016] for a more feasible adversarial training scheme. Given a hyper-parameter λ and a function f(v)\nand , the GRL can be viewed as the function g(f(v); λ) = f(v) with its gradient\nd\ndvg(f(v); λ) = −λ d\ndvf(v). With\nGRL, we could minimize over θf, θd directly by the standard back propagation. The output ˆds\ni and ˆdt\ni of the domain\nclassiﬁer can be written as follows:\nˆds\ni = Gd(g(Gf(xs\ni) ⊗ys\ni )),\n(1)\nˆdt\ni = Gd(g(Gf(xt\ni) ⊗ˆyt\ni)).\nTo attain precise domain prediction of joint representations, we deﬁne the loss of domain classiﬁer as follows:\nLD = LDs + LDt\n= −1\nNs\nNs\nX\ni=1\nds\ni log ˆds\ni −1\nNt\nNt\nX\ni=1\n(1 −dt\ni) log (1 −ˆdt\ni).\n(2)\n2.4\nResidual Transfer Learning for Label Classiﬁer Perturbation\nA key point in transfer learning is to predict labels in the target domain based on the domain-invariant features. A\ncommon assumption for label classiﬁers is that given the domain-invariant features, the conditional distribution of labels\nare the equal, i.e., p(yt|xt) = p(ys|xs). However, this might be insufﬁcient to capture the underlying perturbation of\nlabel classiﬁers across different domains. Hence, we assume p(yt|xt) = p(ys|xs) + ϵ, and consider the modelling\nof the perturbation between label classiﬁers. The residual layers [He et al., 2016] has shown its superior advantages\nin modelling the perturbation via a shortcut connection in RTN [Long et al., 2016], and it can be concluded by\nfs(x; θr) = ft(x) + ∆f(x; θr), where ∆f(x) is the perturbation function parameterized by θr and ft(·) is the identity\nfunction. Residual layers ensure the output to satisfy |∆f(x)| ≪|ft(x)| ≈|fs(x)|, as veriﬁed in [Long et al., 2016].\nWe set Gs(Gf(xs)) ≜σ(fs(Gf(xs))) and Gt(Gf(xt)) ≜σ(ft(Gf(xt))) = σ(Gf(xt)), where σ(·) is the softmax\nfunction to give speciﬁc predicted probabilities.\nFor the source label classiﬁer, the loss function can be easily computed as\nLY = −1\nNs\nNs\nX\ni=1\nL(Gs(Gf(xs\ni)), ys\ni )\n= −\nNs\nX\ni=1\n{ys\ni log Gs(Gf(xs\ni))}.\n(3)\nFor the target label classiﬁer, the learned label classiﬁer may fail in ﬁtting the possibilities of ground truth target\nlabels well. To tackle this problem, following [Grandvalet and Bengio, 2004], we further minimize the entropy of\nclass-conditional distribution p(yt\ni = j|xt\ni) as\nLH = −1\nNt\nNt\nX\ni=1\nc\nX\nj=1\np(yt\ni = j|xt\ni) log p(yt\ni = j|xt\ni)).\n(4)\nwhere c represents the number of classes, and p(yt\ni = j|xt\ni) can be obtained by p(yt\ni|xt\ni) = Gt(Gf(xt\ni)). By minimizing\nthe entropy penalty, the target classiﬁer Gt would adjust itself to enlarge the difference of possibilities among the\npredictions, and thereon predict more indicative labels.\nFinally, the overall objective function of our model is\nL = LY + αLH + βLD.\n(5)\n5\nA PREPRINT - JANUARY 1, 2019\nAlgorithm 1 The algorithm description for DART.\nRequire:\n• source samples and labels (xs\ni, ys\ni ) and target samples xt\ni\n• domain classiﬁer label ds\ni = 1, dt\ni = 0\n• trade-off parameter α and β for entropy penalty and domain classiﬁcation respectively and hyper-parameter\nλ(t) for the gradient reversal layer function g(·; λ(t)).\n• the feature extractor Gf(·); θf) with parameters θf,\n• the domain classiﬁer Gd(·) with parameters θd\n• the source classiﬁer Gs(·; θr) with parameters θr and target classiﬁer Gt(·)\nfor t in [1, training_steps] do\nfor minibatch A, B do\nExtract features from source samples and target samples:\nf s\ni = Gf(xs\ni∈A; θf, t), f t\ni = Gf(xt\ni∈B; θf, t)\nObtain source label prediction and target label prediction:\nˆys\ni = Gs(f s\ni ; θr, t), ˆyt\ni = Gt(f t\ni ; t)\nFuse features and labels with a Kronecker product to represent joint representation:\nzs\ni = f s\ni ⊗ys\ni , zt\ni = f t\ni ⊗ˆyt\ni\nEmbed joint representation into domain classiﬁer:\nds\ni = Gd(g(zs\ni ; λ(t)); θd, t), dt\ni = Gd(g(zt\ni; λ(t)); θd, t)\nUpdate loss function of domain classiﬁer:\nLD = −1\n|A|\nP\ni∈A ds\ni log ˆds\ni\n−1\n|B|\nP\ni∈B(1 −dt\ni) log (1 −ˆdt\ni)\nUpdate loss function of source classiﬁer:\nLY = −1\n|A|\nP\ni∈A ys\ni log ˆys\ni\nUpdate loss function of target classiﬁer:\nLH = −1\n|B|\nP\ni∈B ˆyt\ni log ˆyt\ni\nUpdate overall objective loss function:\nL = LY + αLH + βLD\nUpdate θf,θd and θr using SGD optimizer\nend for\nend for\nReturn ˆθf, θd, θr\nwhere α, β are the trade-off regularizers for the entropy penalty and the domain classiﬁcation. Our goal is to ﬁnd the\noptimal parameters θ∗\nf, θ∗\nd, θ∗\nr by minimizing Equation 5.\nTo clearly present our DART model, we represent the pseudo-code in Algorithm 1.\n3\nExperiment\nWe evaluate the proposed DART against several state-of-the-art baselines on unsupervised domain adaptation problems.\nCodes and datasets will be released.\n3.1\nDatasets and Baselines\nUSPS ↔MNIST : MNIST [LeCun et al., 1998] contains 60000 training digit images and 10000 test digit images, and\nUSPS [Denker et al., 1988] contains 7291 training images and 2007 test images. For the transfer task from MNIST to\nUSPS, we use the labeled MNIST dataset as the source domain and use unlabeled USPS dataset as the target domain,\nand vice versa for the transfer task from USPS to MNIST.\nOfﬁce-312 [Saenko et al., 2010] is a standard benchmark for unsupervised domain adaptation. It consists of 4652\nimages and 31 common categories collected from three different domains: Amazon (A) which contains 2817 images\nfrom amazon.com, DSLR (D) which contains 498 images from digital SLR camera and Webcam (W) which contains\n795 images from the web camera. We evaluate all methods on the following six transfer tasks: A →W, D →W,\nW →D, A →D, D →A, W →A, as done in [Long et al., 2017, 2016].\n2http://ofﬁce31.com.my\n6\nA PREPRINT - JANUARY 1, 2019\nImageCLEF-DA3 is a benchmark dataset for the ImageCLEF 2014 domain adaptation challenge, consisting of three\npublic domains: Caltech-256(C), ImageNet ILSVRC 2012(I), and Pascal VOC 2012(P). Each domain contains 12\ncategories and each category has 50 images. We also consider all the possible six transfer tasks: I →P, P →I, I →C,\nC →I, C →P, P →C, as done in [Long et al., 2017].\nFor MNIST to USPS and USPS to MNIST, we compare with three recent unsupervised domain adaptation algorithms:\nCoGAN [Liu and Tuzel, 2016], pixelDA [Bousmalis et al., 2017], and UNIT [Liu et al., 2017]. CoGAN and UNIT seek\nto learn the indistinguishable features from the discriminator. PixelDA is an effective method in unsupervised domain\nadaptation in which a generator is used to map data from the source domain to the target domain. We choose these\nGANs-based baselines since it has been illustrated in [Goodfellow et al., 2014] that GANs have more advantages over\nconventional kernel method (e.g., MMD) on reducing distribution discrepancy of domains.\nFor Ofﬁce-31 and ImageCLEF-DA datasets, in order to have a fair comparison with the latest algorithms in unsupervised\ndomain adaptatoin, we choose the same baselines as reported in Joint Adaptation Network (JAN) [Long et al., 2017].\nAside from JAN, other baselines include Transfer Component Analysis (TCA) [Pan et al., 2009], Geodesic Flow\nKernel (GFK) [Gong et al., 2012], ResNet [He et al., 2016], Deep Domain Confusion (DDC) [Tzeng et al., 2014],\nDeep Adaptation Network (DAN) [Long et al., 2015], Residual Transfer Network (RTN) [Long et al., 2016], Domain-\nAdversarial Training of Neural Networks (DANN) [Ganin and Lempitsky, 2015].\n(a) USPS and MNIST samples\n(b) Amazon and Webcam samples\n(c) Pascal and Imagenet samples\nFigure 3: Experiment datasets: The ﬁrst two rows in (a) (b) (c) represent examples from the USPS, Amazon and Pascal\ndatasets, and the last two rows represent examples from the MNIST,Webcam and Imagenet datasets respectively.\n3.2\nExperiment Setup\nOur method is implemented based on Tensorﬂow. We use the stochastic gradient descent (SGD) optimizer and\nset the learning rate η = η0 ∗γ[\np\n3000 ], where p is the training step varying from 0 to 30000, set γ to 0.92, and set\nη0 ∈{0.005, 0.01, 0.02, 0.03} for all transfer tasks. We ﬁxed the trade-off regularizer weight α = 0.6 and domain\nadaptation regularizer weight β = 1.0 in all experiments. In order to suppress noisy signal from the domain classiﬁer at\nearly stages during training, we change the hyper-parameter λ of domain classiﬁcation using the following schedule:\nλ = λ0 ∗(\n2\n1+exp(−γ∗q) −1), where q changes from 0 to 1 during progress, and λ0, γ are sensitive to different datasets,\nas discussed in the following paragraphs.\n3http://imageclef.org/2014/adaptation\n7\nA PREPRINT - JANUARY 1, 2019\n(a) DANN: Source domain: A\n(b) DANN: Target domain: W (c) DART: Source domain: A\n(d) DART: Target domain W\nFigure 4: The t-SNE visualization of network activations by DANN in (a), (b) and DART in (c), (d) respectively.\nSpeciﬁcally, for experiments on Ofﬁce-31 and ImageCLEF-DA datasets, due to the limited data in the source domain,\nwe ﬁne-tune our model using the pre-trained model of Resnet4 (50 layers) on the Imagenet [Russakovsky et al., 2015]\ndataset. Following the notation in ResNet, we ﬁx the convolutional layers of conv1, conv2_x, and conv3_x, and ﬁne-tune\nthe rest conv4_x and conv5_x. Then we train the logits layer of ResNet, the label classiﬁers and the domain classiﬁer\nfrom scratch with learning rate 10 times larger than the ﬁne-tuning part. We set γ = 10, and λ0 ∈{1.3, 1.5, 1.8, 2.0}\nfor different tasks in Ofﬁce-31 while λ0 = 1.0 as a ﬁxed value for tasks in ImageCLEF-DA. This progressive strategy\nsigniﬁcantly stabilizes parameter sensitivity and eases model selection for DART.\nFor USPS and MNIST datasets, we replace the Resnet with several CNN layers without ﬁne-tuning. We set λ0 = 1.0 ,\nand γ = 2.5. Note that for all transfer tasks, we run our method for three times and report the average classiﬁcation\naccuracy and the standard error for comparisons.\n3.3\nResults\nFor MNIST and USPS datasets, the classiﬁcation results are shown in Table 2. The reported results of CoGAN, pixelDA,\nand UNIT are from their corresponding papers [Liu and Tuzel, 2016, Bousmalis et al., 2017, Liu et al., 2017]. As can\nbe easily observed, the proposed DART outperforms all baselines on both tasks. Especially on USPS to MNIST, our\nmodel improves the accuracy by a large margin, i.e., about 6%, indicating the effectiveness of DART.\nTable 2: Classiﬁcation accuracy (%) on USPS and MNIST datasets.\nModel\nMNIST to USPS\nUSPS to MNIST\nCoGAN\n95.65\n93.15\npixelDA\n95.9\n-\nUNIT\n95.97\n93.58\nDART\n98.20\n99.40\nSimilarly, the results on Ofﬁce-31 and ImageCLEF-DA datasets are shown in Tables 3-4. We report the results of\nResnet, TCA, GFK, DDC, DAN, RTN, DANN and JAN from [Long et al., 2017], in which all the above algorithms are\nre-implemented. For the results of Ofﬁce-31 dataset, the proposed DART exceeds the state of the art results around\n1.6% in average accuracy. In particular, on A →D, DART achieves a large margin of improvement, i.e., more than\n6.0% over the best baseline.\nIn terms of ImageCLEF-DA, the results in Table 4 again demonstrate that our model could achieve higher accuracy than\nthe rest baselines on all transfer tasks. Compared with JAN, our average performance exceeds 1.3%.\nThe DART model outperforms all previous methods and sets new prediction records on most tasks, indicating its\nsuperiority of both effectiveness and robustness. DART is different from previous methods, since it adapts the joint\ndistribution of high-level features and labels instead of marginal distributions as those in DAN, RTN and DANN, and\nlearns the perturbation function between the label classiﬁers. These modiﬁcations can be the key to the improvement of\nthe prediction performance.\n4https://github.com/tensorﬂow/models/tree/master/research/slim\n8\nA PREPRINT - JANUARY 1, 2019\nTable 3: Classiﬁcation accuracy (%) on Ofﬁce-31 dataset for unsupervised domain adaptation. DART-c denotes the\nDART network without joint distribution alignment, and DART-s denotes the DART network without label classiﬁer\nperturbation. Results of the competitive methods are copied from the original literature.\nMethod\nA →W\nD →W\nW →D\nA →D\nD →A\nW →A\nAvg\nResnet\n68.4 ± 0.2\n96.7 ± 0.1\n99.3 ± 0.1\n68.9 ± 0.2\n62.5 ± 0.3\n60.7 ± 0.3\n76.1\nTCA\n72.7 ± 0.0\n96.7 ± 0.0\n99.6 ± 0.0\n74.1 ± 0.0\n61.7 ± 0.0\n60.9 ± 0.0\n77.6\nGFK\n72.8 ± 0.0\n95.0 ± 0.0\n98.2 ± 0.0\n74.5 ± 0.0\n63.4 ± 0.0\n61.0 ± 0.0\n77.5\nDDC\n75.6 ± 0.2\n96.0 ± 0.2\n98.2 ± 0.1\n76.5 ± 0.3\n62.4 ± 0.4\n61.5 ± 0.5\n78.3\nDAN\n80.5 ± 0.4\n97.1 ± 0.1\n99.6 ± 0.1\n78.6 ± 0.2\n63.6 ± 0.3\n62.8 ± 0.2\n80.4\nRTN\n84.5 ± 0.2\n96.8 ± 0.1\n99.4 ± 0.1\n77.5 ± 0.3\n66.2 ± 0.2\n64.8 ± 0.3\n81.6\nDANN\n82.0 ± 0.4\n96.9 ± 0.2\n99.1 ± 0.1\n79.7 ± 0.4\n68.2 ± 0.4\n67.4 ± 0.5\n82.2\nJAN\n85.4 ± 0.3\n97.4 ± 0.2\n99.8 ± 0.2\n84.7 ± 0.3\n68.6 ± 0.3\n70.0 ± 0.4\n84.3\nJAN-A\n86.0 ± 0.4\n96.7 ± 0.3\n99.7 ± 0.1\n85.1 ± 0.4\n69.2 ± 0.2\n70.7 ± 0.5\n84.6\nDART-c\n84.5 ± 0.2\n95.6 ± 0.1\n98.4 ± 0.1\n82.1 ± 0.2\n42.4 ± 0.2\n50.5 ± 0.3\n75.5\nDART-s\n85.3 ± 0.2\n97.6 ± 0.1\n99.9 ± 0.1\n86.0 ± 0.1\n46.7 ± 0.2\n54.3 ± 0.3\n78.3\nDART\n87.3 ± 0.1\n98.4 ± 0.1\n99.9 ± 0.1\n91.6 ± 0.1\n70.3 ± 0.1\n69.7 ± 0.1\n86.2\nTable 4: Classiﬁcation accuracy (%) on ImageCLEF dataset for unsupervised domain adaptation. Results of the\ncompetitive methods are copied from the original literature.\nMethod\nI →P\nP →I\nI →C\nC →I\nC →P\nP →C\nAvg\nResnet\n74.8 ± 0.3\n83.9 ± 0.1\n91.5 ± 0.3\n78.0 ± 0.2\n65.5 ± 0.2\n91.2 ± 0.3\n80.7\nDAN\n74.5 ± 0.4\n82.2 ± 0.2\n92.8 ± 0.2\n86.3 ± 0.4\n69.2 ± 0.4\n89.8 ± 0.4\n82.5\nRTN\n74.6 ± 0.3\n85.8 ± 0.1\n94.3 ± 0.1\n85.9 ± 0.3\n71.7 ± 0.3\n91.2 ± 0.4\n83.9\nJAN\n76.8 ± 0.4\n88.0 ± 0.2\n94.7 ± 0.2\n89.5 ± 0.3\n74.2 ± 0.3\n91.7 ± 0.3\n85.8\nDART\n78.3 ± 0.1\n89.3 ± 0.1\n95.3 ± 0.1\n91.0 ± 0.1\n75.2 ± 0.1\n93.5 ± 0.1\n87.1\n3.4\nResults Analysis\nPredictions Visualization: To further visualize our results, we embed the label predictions of DANN and DART using\nt-SNE [Donahue et al., 2014] on the example task A →W, and the results are shown in Figure 4(a)-4(d) respectively.\nThe embeddings of DART show larger margin than those of DANN, indicating better classiﬁcation performance of the\ntarget classiﬁer of DART. Now it can be observed that the adaptation of joint distribution of features and labels is an\neffective approach to unsupervised domain adaptation and the modelling of the perturbation between label classiﬁers is\na reasonable extension to previous deep feature adaptation methods.\n1\n2\nTransfer Task\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n1.6\n1.8\nA-Distance\nDANN\nDART\nW2D\nA2W\n(a) A-distance\n50\n100\n150\n200\n250\n300\nNumber of Iterations ( # 100)\n0\n0.1\n0.2\n0.3\n0.4\nTest Error\nA2W\nW2D\n(b) Convergence\n(c) Accuracy change on α\n(d) Accuracy change on β\nFigure 5: (a) A-distance of DANN and DART; (b) The convergence performance of DART; (c) Accuracy of our model\non different choice of α, with β ﬁxed to 1.0; (d) Accuracy of the model on different choice of β, with α ﬁxed to 0.6.\nDistribution Discrepancy: We use A-distance [Ben-David et al., 2009, Mansour et al., 2009] as an alternative mea-\nsurement to visualize the joint discrepancy attained by DANN and DART. A-distance is deﬁned as dA = 2(1 −2ϵ),\nwhere ϵ is a generalization error on some binary problems. A-distance could bound the target risk when source risk is\nlimited, i.e., Rt(Gt) < Rs(Gs) + dA, where Rt(Gt) = E(x,y)∼p(xt,yt)[Gt(Gf(x)) ̸= y] represents the target risk, and\n9\nA PREPRINT - JANUARY 1, 2019\nRs(Gs) = E(x,y)∼p(xs,ys)[Gs(Gf(x)) ̸= y] represents the source risk. Figure 5(a) shows dA values on tasks A →W\nand W →D attained by DANN and DART, respectively. We observe that dA of DART is much smaller than that of\nDANN, which suggests that the joint representation of features and labels in DART can bridge different domains more\neffectively than DANN.\nConvergence Performance: To demonstrate the robustness and stability of DART, we visualize the convergence perfor-\nmance of our model in Figure 5(b) on two example tasks: A →W and W →D. Test error of two tasks converges fast\nin the early 8000 steps and stabilizes in the remaining training process, which testiﬁes the effectiveness and stability of\nour model.\nParameter Sensitivity Analysis: We testify the sensitivity of our model to parameters α and β, i.e. the hyper-parameters\nfor label-classiﬁcation loss and domain cross-entropy loss. As an illustration, we use task A →W to report the\ntransfer accuracy of DART with different choice of α and β. Figure 5(c) reports the accuracy of varying α ∈\n{0.0, 0.2, 0.6, 0.8, 1.0} while ﬁxing β = 1.0. We observe that DART achieves the best performance when α is set from\n[0.2,0.6], although there is a vibration when α = 0.4. The vibration may be caused by the small number of samples. As\nfor β, the best accuracy can be obtained when setting β from [0.5, 1.5], illustrating a bell-shaped curve as showed in\nFigure 5(d).\nTable 5: Categories of Unsupervised Domain Adaptation Methods\nModel\nDiscrepancy measure\nDistribution\nassumption\nClassiﬁer Per-\nturbation\nDDC [Tzeng et al., 2014]\nMMD\nMarginal\nno\nDAN [Long et al., 2015]\nMMD\nMarginal\nno\nDANN [Ganin et al., 2016]\nDomain Adversarial Loss\nMarginal\nno\nDSN [Bousmalis et al., 2016]\nMMD/Domain Adversarial Loss\nMarginal\nno\nUNIT [Liu et al., 2017]\nGAN\nMarginal\nno\nCoGAN [Liu and Tuzel, 2016]\nGAN\nMarginal\nno\nDTN [Taigman et al., 2016]\nGAN\nMarginal\nno\npixelDA [Bousmalis et al., 2017]\nGAN\nMarginal\nno\nRTN [Long et al., 2016]\nMMD\nMarginal\nyes\nJDA [Long et al., 2013]\nMMD\nJoint\nno\nJAN [Long et al., 2017]\nJMMD\nJoint\nno\nDART(proposed)\nDomain Adversarial Loss\nJoint\nyes\n4\nRelated work\nUnsupervised domain adaptation based on deep learning architectures can bridge different domains or tasks and mitigate\nthe burden of manual labeling. The goal of domain adaptation is to reduce the domain discrepancy measured in various\nprobability distributions of different domains. In the following, we review different deep domain adaptions methods\nfrom three perspectives: (1) domain discrepancy measures; (2) distributions used to measure the discrepancy; (3)\ndifferences between label classiﬁers of both domains.\nIn unsupervised domain adaptation, most methods try to learn domain-invariant features, such as DDC [Tzeng et al.,\n2014], DAN [Long et al., 2015], DANN [Ganin et al., 2016], Conditional Adversarial Domain Adaptation [Long et al.,\n2018], etc. That means p(Gs(xt)) ≈p(Gt(xs)). Recent approaches assume that both the source domain and the target\ndomain should share a joint distribution of both features and labels. That means the joint distributions of extracted\nfeatures and labels are shared, i.e, p(Gs(xt), yt) ≈p(Gt(xs), ys). Such work include JDA [Long et al., 2013] and\nJAN [Long et al., 2017]. Our work follows the idea of JDA to model the joint distribution, and use the Kronecker\nproject to generate feature and label maps.\nIn terms of measuring the distribution alignment between the source domain and the target domain, most of previous\nmethods have utilized probabilistic measures, such as the Maximum Mean Discrepancy (MMD) and [Gretton et al.,\n2006], the correlation alignment[Sun and Saenko, 2016]. DDC [Tzeng et al., 2014] and DAN[Long et al., 2015]\nminimizes the discrepancy such that a representation that is both semantically meaningful and domain-invariant can be\nlearned. Recent methods have utilized the idea of adversarial learning to implicitly measure the distribution alignment\nbetween the source domain and the target domain. Among these methods, UNIT [Liu et al., 2017] and CoGAN [Liu\nand Tuzel, 2016] adopt GANs in their architectures to generate domain translated images and evaluate whether the\ntranslated images are realistic for each domain; while DTN [Taigman et al., 2016] and pixelDA [Bousmalis et al., 2017]\nmap data in the source domain to the target domain by a generator. DANN [Ganin et al., 2016] introduces a domain\n10\nA PREPRINT - JANUARY 1, 2019\nclassiﬁer borrowing the idea from adversarial training to help learn transferable features. The proposed DART follows\nthe idea of DANN to discriminate features from the source domain and the target domain.\nIn terms of the relation between the source classiﬁer and the target classiﬁer, previous unsupervised domain adaptation\nmethods mostly assume that the same conditional distribution is shared between the target domain and the source\ndomain, i.e., p(yt|xt) = p(ys|xs). Different from this category of approaches, the second category relaxes the rather\nstrong assumption. Instead, it considers a more general scenario in practical applications and assumes that the source\nclassiﬁer and the target classiﬁer differ by a small perturbation function. RTN [Long et al., 2016] learns an adaptive\nclassiﬁer by adding residual modules into the source label classiﬁer, fusing the features from multiple layers with a\nKronecker product, and then minimizing its discrepancy.\nTaking the advantages of successful unsupervised domain adaptation methods, we design our DART by introducing\nperturbation to the source classiﬁer and the target classiﬁer to increase ﬂexibility, measuring the joint distributions of\nfeatures and labels, and introducing the domain adversarial loss to discriminate two domains.\n5\nConclusion\nThis paper presents a novel approach to the unsupervised domain adaptation in deep networks, which enables the\nend-to-end learning of adaptive classiﬁers and transferable features. Unlike previous methods that match the marginal\ndistributions of features across different domains, the proposed approach reduces the discrepancy of domains using the\njoint distribution of both high-level features and labels. In addition, the proposed approach also learns the perturbation\nfunction across the label classiﬁers via the residual modules, bridging the source classiﬁer and target classiﬁer together\nto produce more robust outputs. The approach can be trained by standard back-propagation, which is scalable and can\nbe implemented by most deep learning packages. We conduct extensive experiments on several benchmark datasets,\nvalidating the effectiveness and robustness of our model. Future work constitutes semi-supervised domain adaptation\nextensions.\nReferences\nShai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A\ntheory of learning from different domains. Machine Learning, 79:151–175, 2009.\nKonstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan. Domain separation\nnetworks. In NIPS, 2016.\nKonstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, and Dilip Krishnan. Unsupervised pixel-\nlevel domain adaptation with generative adversarial networks. In 2017 IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 95–104, 2017.\nJohn S. Denker, W. R. Gardner, Hans Peter Graf, Donnie Henderson, Richard E. Howard, Wayne E. Hubbard,\nLawrence D. Jackel, Henry S. Baird, and Isabelle Guyon. Neural network recognizer for hand-written zip code digits.\nIn NIPS, 1988.\nJeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep\nconvolutional activation feature for generic visual recognition. In ICML, 2014.\nYaroslav Ganin and Victor S. Lempitsky. Unsupervised domain adaptation by backpropagation. In ICML, 2015.\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario\nMarchand, and Victor Lempitsky. Domain-adversarial training of neural networks. Journal of Machine Learning\nResearch, 17(59):1–35, 2016.\nBoqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. Geodesic ﬂow kernel for unsupervised domain adaptation.\n2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 2066–2073, 2012.\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville,\nand Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.\nYves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In NIPS, 2004.\nArthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, and Bernhard. A kernel method for the two-sample-problem. In\nNIPS, 2006.\n11\nA PREPRINT - JANUARY 1, 2019\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2016 IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016.\nYann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document\nrecognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\nTsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and\nC. Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.\nMing-Yu Liu and Oncel Tuzel. Coupled generative adversarial networks. In NIPS, 2016.\nMing-Yu Liu, Thomas Breuel, and Jan Kautz.\nUnsupervised image-to-image translation networks.\nCoRR,\nabs/1703.00848, 2017.\nMingsheng Long, Jianmin Wang, Guiguang Ding, Jia-Guang Sun, and Philip S. Yu. Transfer feature learning with joint\ndistribution adaptation. 2013 IEEE International Conference on Computer Vision, pages 2200–2207, 2013.\nMingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. Learning transferable features with deep adaptation\nnetworks. In ICML, 2015.\nMingsheng Long, Han Zhu, Jianmin Wang, and Michael I. Jordan. Unsupervised domain adaptation with residual\ntransfer networks. In NIPS, 2016.\nMingsheng Long, Jianmin Wang, and Michael I. Jordan. Deep transfer learning with joint adaptation networks. In\nICML, 2017.\nMingsheng Long, ZHANGJIE CAO, Jianmin Wang, and Michael I Jordan. Conditional adversarial domain adaptation.\nIn S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural\nInformation Processing Systems 31, pages 1647–1657. 2018.\nYishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and algorithms. In\nIn Proceedings of COLT. Citeseer, 2009.\nSinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on Knowledge and Data\nEngineering, 22:1345–1359, 2010.\nSinno Jialin Pan, Ivor W. Tsang, James T. Kwok, and Qiang Yang. Domain adaptation via transfer component analysis.\nIEEE Transactions on Neural Networks, 22:199–210, 2009.\nYingwei Pan, Ting Yao, Houqiang Li, and Tao Mei. Video captioning with transferred semantic attributes. In 2017\nIEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017,\npages 984–992, 2017.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition\nchallenge. International Journal of Computer Vision, 115:211–252, 2015.\nKate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new domains. In ECCV,\n2010.\nBaochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In ECCV Workshops,\n2016.\nYaniv Taigman, Adam Polyak, and Lior Wolf. Unsupervised cross-domain image generation. CoRR, abs/1611.02200,\n2016.\nEric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion: Maximizing for\ndomain invariance. CoRR, abs/1412.3474, 2014.\nEric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. CoRR,\nabs/1702.05464, 2017.\nMei Wang and Weihong Deng. Deep visual domain adaptation: A survey. Neurocomputing, 312:135–153, 2018.\nJunbao Zhuo, Shuhui Wang, Weigang Zhang, and Qingming Huang. Deep unsupervised convolutional domain\nadaptation. In Proceedings of the 2017 ACM on Multimedia Conference, MM ’17, pages 261–269, 2017. ISBN\n978-1-4503-4906-2.\n12\n",
  "categories": [
    "cs.CV",
    "cs.LG"
  ],
  "published": "2018-12-30",
  "updated": "2018-12-30"
}