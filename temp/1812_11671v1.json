{
  "id": "http://arxiv.org/abs/1812.11671v1",
  "title": "Unsupervised monocular stereo matching",
  "authors": [
    "Zhimin Zhang",
    "Jianzhong Qiao",
    "Shukuan Lin"
  ],
  "abstract": "At present, deep learning has been applied more and more in monocular image\ndepth estimation and has shown promising results. The current more ideal method\nfor monocular depth estimation is the supervised learning based on ground truth\ndepth, but this method requires an abundance of expensive ground truth depth as\nthe supervised labels. Therefore, researchers began to work on unsupervised\ndepth estimation methods. Although the accuracy of unsupervised depth\nestimation method is still lower than that of supervised method, it is a\npromising research direction.\n  In this paper, Based on the experimental results that the stereo matching\nmodels outperforms monocular depth estimation models under the same\nunsupervised depth estimation model, we proposed an unsupervised monocular\nvision stereo matching method. In order to achieve the monocular stereo\nmatching, we constructed two unsupervised deep convolution network models, one\nwas to reconstruct the right view from the left view, and the other was to\nestimate the depth map using the reconstructed right view and the original left\nview. The two network models are piped together during the test phase. The\noutput results of this method outperforms the current mainstream unsupervised\ndepth estimation method in the challenging KITTI dataset.",
  "text": "Unsupervised monocular stereo matching\nZhimin Zhanga, Jianzhong Qiaoa, Shukuan Lina\naNortheastern University, Hunnan Campus, Computer Science and Engineering, No.195, Chong San Road,\nHunnan district, Shenyang, China, 110169\nAbstract. At present, deep learning has been applied more and more in monocular image depth estimation and\nhas shown promising results. The current more ideal method for monocular depth estimation is the supervised\nlearning based on ground truth depth, but this method requires an abundance of expensive ground truth depth\nas the supervised labels. Therefore, researchers began to work on unsupervised depth estimation methods. Al-\nthough the accuracy of unsupervised depth estimation method is still lower than that of supervised method, it is a\npromising research direction.\nIn this paper, Based on the experimental results that the stereo matching models outperforms monocular\ndepth estimation models under the same unsupervised depth estimation model, we proposed an unsupervised\nmonocular vision stereo matching method. In order to achieve the monocular stereo matching , we constructed\ntwo unsupervised deep convolution network models, one was to reconstruct the right view from the left view,\nand the other was to estimate the depth map using the reconstructed right view and the original left view. The\ntwo network models are piped together during the test phase. The output results of this method outperforms the\ncurrent mainstream unsupervised depth estimation method in the challenging KITTI dataset.\nKeywords: depth estimation,unsupervised learning,synthesis view,stereo matching,monocular vision, Kitti dataset,\n.\n*Fourth author name, zhangzhimin@stumail.neu.edu.cn\n1 Introduction\nWith the development of virtual reality and self-driving car etc, depth estimation has be-\ncome very hot research. It is also the fundamental problems of the computer vision . At present,\nthe research on depth estimation has achieved very good results. However, most of the research\nare based on multi-view or binocular of the scene1 . Such as stereo matching2 , structure from\nmotion3 .\nIn recent years, with the widespread application of machine learning or deep learning in\nthe ﬁeld of computer vision, researchers began to apply these methods to the ﬁeld of depth\nprediction with a single image4–6 or stereo pairs2,7,8 . Although there have been a number\nof stereo matching articles based on depth learning in recent years, and fruitful results have\nbeen achieved both in industry and academia, stereo matching requires costly special-purpose\nstereo camera rigs. To overcome this problem, researchers began working on depth estima-\ntion based on monocular vision. In theory, monocular depth estimation, which does not take\ninto account ground truth depth, is an ill-posed approach for geometric clues, because people\nwho are sensitive to three-dimensional world perception still need two eyes to locate objects\nin nature. Therefore, it is rather difﬁcult to estimate the depth of the three-dimensional space\nthrough a single picture, and the learning model must be a very complicated function.\nAt present, supervised monocular depth estimation9–11 has undoubtedly become one of the\nresearch hotspots of computer vision, and some exciting research results have appeared. The\nmethod of these articles is to directly predict the depth value of each pixel of a single image\nby using deep model, which is ofﬂine trained by the input of single view under the supervi-\nsion of large number of ground truth depth. Although these studies are fruitful, they need to\n1\narXiv:1812.11671v1  [cs.CV]  31 Dec 2018\n \nFig 1 Design ideas of our method on monocular stereo matching. We can synthesize right view from single left\nview by the unsupervised right view synthesis network and then use unsupervised stereo matching network to\nestimate disparity map from the concatenation input of both left and right views.\nobtain a large amount of ground truth depths that matching strictly with monocular images\nthrough expensive 3D sensors such as LIDAR. Therefore, supervised Monocular depth estima-\ntion is a challenging task for collecting vast quantities of corresponding ground truth depth data\nto training the models. Researchers began to focus on unsupervised depth estimates12–14 that\ndoes not require vast quantities of corresponding ground truth depth data for training. Most\nof these methods estimate an accurate disparity map by only supervising on the image align-\nment loss. They rely more on large amounts of high-quality data and effective learning to make\ndeep estimates and require certain geometric constraint. Although this process is difﬁcult to\nunderstand and produces suboptimal results, it is also a promising research direction\nMotivated by Luo et al.15 , our paper proposes an unsupervised stereo matching depth esti-\nmation model based on monocular vision. Similarly, we also consider the monocular depth\nestimation problem as two sub-problems, namely: 1)right view synthesis process; 2) stereo\nmatching process. Unlike Refs.15 , which uses semi-supervised, our models purely uses un-\nsupervised depth estimation without ground truth depth and the network architecture that\nperforms end-to-end depth estimation with deep learning network. In order to obtain better\ndepth estimation results, in the training stage, the two models were separately trained, while\nin the test stage, we connected the two models through pipes and directly estimated the depth\nvalue from a single view. A model diagram is shown in Fig. 1.\nIn short, we make the following contributions: 1)We proposed an unsupervised depth esti-\nmation method from single image. 2) We constructed two deep convolutional network model to\nachieve our method. Our method is better than the mainstream unsupervised depth evaluation\nmethod, and even better than some supervised methods.\n2 Related Work\nDue to the rise of robotics and virtual reality, depth evaluation has undoubtedly become\none of the most popular research points at present. Because machine learning or deep learning\nhas better performance than traditional methods, more and more researchers has applied this\nmethods to depth evaluation and some research results have been achieved. Here we would\nfocus on works related to stereo matching8 and monocular depth evaluation16 with machine\n2\nlearning or depth learning, and no assumptions about the scene geometry or types of objects\npresent are made.\n2.1 Stereo Matching\nThe traditional stereo matching algorithm determines the pole by searching the polar ge-\nometric line on the stereo pairs. This polar constraint is the basic principle of stereo vision\nand motion analysis. The binocular views in the stereo matching algorithm are the calibrated\nimages, so the matching problem in 3D space can be transformed into the search problem in\n1D space, and obtain the geometrical relation between the depth and the disparity, namely d =\nfb/z, where the d is the disparity of views, z is the scene depth, f is the camera focal length, the\nbaseline B is the distance of the camera.\nRecently, the vast experiment results show that the stereo matching method based on deep\nlearning outperform using hand deﬁned similarity measures. The methods7,17,18 is to learn-\ning the matching function through the process of ﬁnding pixels points consistent with the left\nview from the right view of stereo pairs. Luo et al.8 proposed a faster and more accurate depth\nestimation network architecture. The architecture consists of two Siamese network and prod-\nuct layer that computes the inner product of feature vectors from two Siamese network. This\nmethod is treating disparity estimation as a multi-classiﬁcation problem, that is ,every possible\ndisparity as a class. L’ubor et al.2 proposed stereo matching architecture based on convolutional\nneural network with the ground truth disparity to constructing a binary classiﬁcation dataset.\nThe approach focuses on the matching cost computation by learning a similarity measure on\nsmall image patches. Mayer et al.19 presented an novel deep CNN network with fully convo-\nlutional20 to achieve end-to-end training process using synthetic stereo pairs, called DispNet.\nThe network architecture of FlowNet21 is similar to DispNet19 , which are also applied to optical\nﬂow estimation. Pang et al.17 proposed a cascade residual convolutional neural network archi-\ntecture composing of two stages. The two stages, which can generate residual signals across\nmultiple scales, include improved DispNet19 by add additional up-convolution modules, and\nthe network of explicitly rectifying the disparity. Although the above methods based on learn-\ning outperformed traditional stereo matching methods, they rely on vast accurate ground truth\ndisparity data and stereo image pairs at training time.\n2.2 Monocular Depth Estimation\nThe stereo matching method has certain requirements for binocular camera, which is not\nsuitable for the actual single camera equipment. Therefore, researchers began to study the\nmonocular depth estimation and has obtained a series of research results. For supervised learn-\ning depth estimation, Saxena et al.16 proposed ﬁrst supervised learning approach to resolve the\nproblem of depth estimation from monocular images. The model adopted a discriminatively-\ntrained MRF with multi-scale local and global image features, and models the depth of each\npoint and the depth relation of different points. With the widely application of CNN in com-\nputer vision, researchers began to apply the deep learning method to monocular depth estima-\ntion. Eigen et al.22 was the ﬁrst article that attempted to solve the monocular depth estimation\nproblem using CNN architecture by employing two network models that one network model\nmakes coarse global prediction for entire image and another network models reﬁnes this pre-\ndiction locally. The loss function of this model is adapted a scale-invariant error. Subsequently,\n3\nthe authors improved the network and generated a new multi-scale CNN network architec-\nture5 with fully convolutional up-sampling network20 , which can complete three visual tasks,\nincluding depth prediction, surface normal estimation, and semantic labeling. Laina et al.11\nproposed a fully convolutional residual network23 to model the mapping relation between a\nsingle images and ground truth depths. The architecture adapted a novel up-sampling model\ncalled up-projected to improve the output resolution and introduced the reverse Huber loss to\noptimize the network. Liu et al.24 proposed a deep learning model based on deep CNN and con-\ntinuous CRF for estimating monocular depth. On the basis of this, the author further proposes\nan equally effective model based on FCN and a new superpixel pooling method to speedup the\npatch-wise convolutions in the estimation model.\nAlthough supervised learning can achieve well results of depth estimation, this learning\nmethod requires vast ground truth depth data, which are difﬁcult to obtain for practical ap-\nplication. To overcome this problem, researchers began to focus on unsupervised depth es-\ntimation. Xie et al.25 proposed a unsupervised transformation method of 2D to 3D of ﬁlms,\nwhich is essentially a method of reconstructing the right view based on single left view by ex-\ntracting stereo pairs from existing 3D ﬁlms as supervision training. This model predicted a\nprobabilistic disparity-like map and combined it with left view to reconstructed right view. Garg\net al.12 proposed a unsupervised deep model based on polar geometry to implement end-to-\nend monocular depth estimation by only supervising on the image alignment loss. However,\nthe author adapted Taylor expansion to linearize the not fully differentiable loss function. Go-\ndard et al.13 proposed a novel unsupervised depth estimation model based on proposed the\nmodel by12 and adapted a new fully differentiable appearance matching loss and left-right dis-\nparity consistency loss. Due to the sparsity of ground truth depth data acquired by radar, the\nsupervised learning cannot accurately estimate the image depth, so Kuznietsov et al.26 pro-\nposed semi-supervised depth estimation model, which can make unsupervised learning on\ndense correspondence ﬁeld and use sparse radar depth data for further supervised learning.\nFor explicitly imposing geometrical constraint, Luo et al.15 decomposed the monocular depth\nestimation into two sub-problems for the ﬁrst time that one is view synthesis procedure and an-\nother is stereo matching. Similar to the semi-supervised method, the stereo matching network\nalso needs sparse ground truth disparity data for the supervised learning.\nWe can see the comparison results from Table 1, that the stereo matching models outper-\nforms monocular depth estimation models under the same unsupervised depth estimation\nmodel. So inspired by Luo et al.15 , we proposed a unsupervised monocular image stereo\nmatching model that composed by the view synthesis network and stereo matching network.\nFor these two network, we was suggested from Refs.13 , that constructed an unsupervised end-\nto-end convolutional network model with similar structure. We can synthesize right view from\nleft view through view synthesis network that was trained by the loss of consistency between\nthe predicted view and the original image. Then we input the concatenation of both original\nleft and synthesized right views into stereo matching networks for depth estimation . The im-\nplementation procedure of our unsupervised monocular vision stereo matching is illustrated\nin Fig. 2.\n4\nTable 1 Comparison of effects between monocular and binocular inputs from the same unsupervised network\nmodel. Where K is the KITTI dataset. The experimental results show that the stereo matching models outperforms\nmonocular depth estimation models under the same unsupervised depth estimation model.\nMethod\nDataset Type RMSE RMSE(log) ARD\nSRD δ < 1.25 δ < 1.252 δ < 1.253\n–lower is better–\n–higher is better–\nours with VGG16\nK\nMono 6.125\n0.217\n0.1235 1.3882\n0.841\n0.936\n0.975\nours with ResNet50\nMono 5.764\n0.203\n0.114\n1.246\n0.854\n0.947\n0.979\nours with VGG16\nBino 4.434\n0.146\n0.0669 0.899\n0.947\n0.978\n0.988\nours with ResNet50\nBino 4.593\n0.150\n0.0701 1.0391\n0.946\n0.977\n0.988\n \nFig 2 The implementation procedure of our unsupervised monocular stereo matching .The network model con-\nsists of two parts, namely right view synthesis network and stereo matching network. The input original left view\nis ﬁrst processed by CNN based on Resnet50 and FCN of upsampling to reconstruct right view .Then the concate-\nnation of both synthetic right view and original left view input the stereo matching network composed by vgg16\nand upsampling FCN to estimate an accurate disparity.\n3 Approach\nThis section describes our unsupervised monocular stereo matching model. We describe\nthe model in two parts that one is the right view synthesis network and the other is the stereo\nmatching network. In the training stage, in order to get better output results for each model,\nwe trained the two models separately, but stereo matching network need synthetic right view as\nthe training data. In the testing stage, we loaded two models successively and the output results\nof the former network are transferred to the latter network as input data through the pipeline.\n3.1 Unsupervised Monocular Depth Estimation\nDepth map is the actual distance between each pixel in a 2D image and the camera that\ntook the image. The so-called monocular depth estimation is that given a 2D imageI, we use a\nfunction f to predict the depth z corresponding to each pixel in the image. The process can be\ndescribed as:z = f (I). The current monocular depth estimation method based on supervised\nlearning uses the single RGB image as the input, and the ground truth depth data as labels\ntraining the neural network to construct the ﬁtting function f , so that the scene depth infor-\nmation can be obtained according to the single image. However, this method needs to know\n5\nthe expensive ground truth depth data corresponding to the input images as the reference for\ntraining. For unsupervised monocular depth estimation, it is generally to estimate the disparity\nmap from the input left view through the depth CNN network, and then the generated disparity\nmap and the original view are used to reconstruct the left and right views. In the training stage,\nwe obtain the depth estimation model by optimizing the original view and the image alignment\nloss function of the reconstructed view or other additional loss functions. In the test stage, the\nimage depth can be estimated by directly inputting a single view based on the trained model.\nOur method uses the same design idea with other unsupervised model. But the experimental\nresults show that the stereo matching models outperform monocular depth estimation models\nunder the same unsupervised depth estimation model. So we transformed the depth estima-\ntion problem into the process of image synthesis and stereo matching based on unsupervised\nlearning. We use the image synthesis network to learning the function that it can reconstructed\nthe right view from left view. Then the unsupervised stereo matching network can train the con-\nvolutional network to estimate disparity from the input of combination of left and right views.\nThe stereo matching model refers to that the input data of this model is the concatenation of\nboth left views and right views. In order to verify which training method can get better results\nfor stereo matching model, we adapted three ways to train that it can be see from experimental\nsection.\nAs we can see from Fig. 2, at training time of view synthesis network, we have access to a\nlot of pairs of calibrated stereo pairsI l and I r , with the left view I l as the input data and the\nstereo pairs I l andI r as the supervision labels. The input left view I l is processed by convolu-\ntional neural network to ﬁnd the dense correspondence ﬁeld dr that was the disparity map of\nthe right view relative to left view. Then, we can reconstructed right view ¯I r by the bilinear sam-\npling function I l(dr ). We can training the loss function by supervising on the image alignment\nloss between the original right view I r and synthesis right view ¯I r to generate view synthesis\nnetwork model. Similarly, we can also use the convolutional neural network to process input\nleft view I l for obtaining the dense correspondence ﬁeld dl that was the disparity map of the\nleft view relative to right view. Then,we can synthesize the left view given the right view by the\nbilinear sampling formula ¯I l = I r (dl) and optimize the alignment loss function between the\noriginal view and the synthetic view. During testing, we just need to input the left view into the\ntrained network model to estimate the right view, instead of stereo pairs like the input in train-\ning stage. Then we can input the concatenation of both left and synthetic right views into the\nstereo matching network to estimate depth with similar principles as described above.\n3.2 Network Architecture\nIn order to select a more suitable encoding architecture, we have done relevant experiments\non view synthesis network and stereo matching network with vgg16 and ResNet50 networks\nrespectively as Table 1.\nThe experimental results show that the evaluation metrics of ResNet5023 outperform that\nof VGG1627 network in view synthesis process. Therefore, we use ResNet50 network as the en-\ncoder part of the view synthesis network, and The full convolution network is used to replace\nthe full connection layer as the decoding part of the network. View synthesis network architec-\nture is shown in Table 2. Our input image is the RGB left view with 256*512 resolution size. In\nthe coding part, 2048 feature images with a resolution of 4*8 are extracted after 50 convolution\n6\noperations. In order to obtain accurate depth information, we adapted skip connections to con-\ncatenate both different scales feature map of encoder parts and same resolution feature maps\nof decoder parts for up-sampling, which can get disparity map by simple computation. Finally,\nthe right view is synthesized based on the polar geometry by estimating the depth information.\nAs can be seen from Table 1, for the stereo matching network, VGG16 as the encoding part\nis better than the encoding result of ResNet50. Fig. 3 is the network architecture diagram of our\nstereo matching network. The input is the concatenation of both original left views and synthe-\nsized right views with 256*512 resolution size. After the coding of VGG16 network, 512 feature\nmaps with a resolution of 2*4 were ﬁnally obtained. In the same way, the feature map of seven\nscales is up-sampled by the method of skip connection. Then the disparity map is calculated\nby selecting the feature graph of four high resolution scales as the model optimization factor.\nTable 2 View synthesize network encoder-decoder architecture. The res_convx_3 refers to the convolution block\nof our deep residual network, which include three convolution process each block. The upconv_block was the\nupsampling convolution block which the input was the concatenation both different scales feature map of encoder\nparts and same resolution feature maps of decoder parts.\nLayer\nOutput(resolution*channels)\nInputs\nconv7\n2\n128*256*64\nInput(RGB)\nmax_pool3\n2\n64*128*64\nconv\nres_conv1_3\n64*128*64\nmax_pool3\n2\nres_conv2_3\n64*128*64\nres_conv1_3\nres_conv3_3\n32*64*256\nres_conv2_3\nres_conv4_3\n32*64*128\nres_conv3_3\nres_conv5_3\n32*64*128\nres_conv4_3\nres_conv6_3\n32*64*128\nres_conv5_3\nres_conv7_3\n16*32*512\nres_conv6_3\nres_conv8_3\n16*32*256\nres_conv7_3\nres_conv9_3\n16*32*256\nres_conv8_3\nres_conv10_3\n16*32*256\nres_conv9_3\nres_conv11_3\n16*32*256\nres_conv10_3\nres_conv12_3\n16*32*256\nres_conv11_3\nres_conv13_3\n8*16*1024\nres_conv12_3\nres_conv14_3\n8*16*512\nres_conv13_3\nres_conv15_3\n8*16*512*\nres_conv14_3\nres_conv16_3\n4*8*2048*\nres_conv15_3\nupconv_block1\n8*16*512\nres_conv16_3\nupconv_block2\n16*32*256\nupconv_block1\nres_conv13_3\nupconv_block3\n32*64*128\nupconv_block2\nres_conv7_3\nupconv_block4\n64*128*64\nupconv_block3\nres_conv3_3\nupconv_block5\n128*256*32\nupconv_block4\nmax_pool3\n2\nupconv_block6\n256*512*16\nupconv_block5\nconv7\n2\n7\n \nFig 3 Stereo matching network encoder-decoder architecture. The input of model was the concatenation both\noriginal left views and synthesized right views. We input the output of encoder network at each resolution to de-\ncoder network at the same scales layers by skip connections. We selected four scales high resolution up-sampling\noutputs to calculate disparity map.\n3.3 Loss Function\nWe adopted the same unsupervised optimization method for view synthesis network and\nstereo matching network as shown in Fig. 2, that is, we acquire the disparity map by learning the\ndeep network, and reconstruct the left and right views according to the geometric relationship\nbetween the disparity map and the stereo pairs. Therefore, we use the same loss function for\nthe two network models.\n3.3.1 Related Formula\nIn order to obtain the formula of loss function, we explain the derivation process of the\nrelated formula based on the previous theory. We use the rectiﬁed stereo pairs as training data.\nGiven the camera’s internal and external parameters, we assume photo-consistency between\nthe left view and the right view. According to the geometric properties and polar constraints of\nthe binocular camera, the corresponding formula can be obtained.\n¯I l(i, j) = I r (i, j −dl\ni,j)\n¯I r (i, j) = I l(i, j −dr\ni,j)\n(1)\nWhere(i, j) ∈Ω, Ωis the image space of I, and i,j refer to the horizontal and vertical coordi-\nnates of the pixel position of the image. We can also get the depth estimation Z = b f\nd by given\nthe baseline distance b between the left and right cameras, the cameras focal length f and the\ndisparity map.\n3.3.2 Loss Function of Model\nOur network model adopts the unsupervised network model, which takes the original left\nimage or the concatenation of the original left image and the composite right image as inputs,\nand there is no ground truth depth data as the supervised label in the training stage. Therefore,\nwe are inspired by Refs 13 to estimates the disparity map by optimizing image alignment loss,\nleft-right disparity consistency loss and spatial smoothness loss. We deﬁne a loss Lθ as the total\nloss of the different constraint forms for two network model.\nLθ = αLia +βLss +γLdc\n(2)\n8\nWhere Lal was the image alignment loss of reconstructed view and original view, Lsm was\nthe regularization term on the spatial smoothness of disparity values. Each loss terms contains\ntwo loss formulas that was the respective image alignment loss between the reconstructed left\nand right views and the original views. α,β,γ are used as the adjustment parameter of speciﬁc\ngravity of each loss function.\nImage alignment loss: From article Refs.28 , we can see that the optimization results of the\ncombined features of MS_SSIM29 and L1 loss functions were better than the single function of\nthese two loss functions. In order to optimize the quality of the reconstructed views, we use\nthe mixed loss function came from L1 and MS_SSMI as our photometric image reconstruction\ncost function Lia, which calculates the alignment loss between original stereo pairs I l,I r and\nreconstructed stereo pairs ¯I l, ¯I r .\nLia = 1\nN\nX\nn∈(l,r)\nX\ni,j\nγLMS_SSIM(I n\ni,j, ¯I n\ni,j)+(1−γ)Ll1GσM\nG (I n\ni,j, ¯I n\ni,j)\n(3)\nwhere\nLMS_SSIM(I n\ni,j, ¯I n\ni,j) = 1−MS_SSIM(I n\ni,j, ¯I n\ni,j),\nLl1(I n\ni,j, ¯I n\ni,j) = |I n\ni,j −¯I n\ni,j|\n(4)\nHere,we selected Gaussian smoothing kernelGσM\nG with normal distribution of σ = 1px for Ll1\nand set the parameter γ to 1\nSpatial smoothness loss: It is well known that the disparity estimation problem is ill-posed\nin homogeneous regions of the scene without ground truth disparity and depth discontinuities\noften occur at image gradients. Thus as suggested in this paper26 , we add the edge edge pre-\nserving regularizer as part of the loss function using the image gradients ∂I,with the n ∈l,r.\nLsm = 1\nN\nX\nn\nX\ni,j\n|∂xdn\ni j|e−|∂xI n\ni j | +|∂ydn\ni j|e−|∂y I n\ni j |\n(5)\nLeft-Right disparity consistency loss: We refer to the loss of the article13 , which is based on\nthe geometric constraint of stereo to able to make the left-right disparity map convert to align-\nment loss. To ensure consistency, we adopted a simple L1 loss as left-right disparity consistency\nloss Lds. By optimizing the loss function, we can get more accurate disparity maps.\nLds = 1\nN (\nX\ni,j\n|dl\ni,j −dr\ni,j+dl\ni,j\n|+\nX\ni,j\n|dr\ni,j −dl\ni,j+dr\ni,j |)\n(6)\n4 Experiments\nCurrently, the commonly used data sets reconstructed in 3D scenes include indoor data\nNYU Depth Dataset30 , outdoor Dataset Make3D6 and KITTI Dataset31 in self-driving scenes.\nOur research objective is to study the automatic driving of cars under complex road conditions.\nTherefore, this section shows our experiments and results that we compare the performance\nbetween our approach and current state-of-the-art monocular depth estimation method on the\npopular KITTI dataset. In this section, we also discover and prove the theoretical correctness of\nour method.\n9\n4.1 Dataset\nKitti dataset is the most widely used image dataset in the ﬁeld of autonomous driving. The\ndataset32 records the six-hour trafﬁc scenarios by a series of sensors, including high-resolution\ncolor and gray stereo cameras, a 3D laser scanner, and a high-precision GPS/IMU inertial navi-\ngation. The scenarios are captured by driving around inner city of Karlsruhe city on high-speed,\nin rural areas, with many static and dynamic objects. This dataset is calibrated, synchronized\nand timestamped, and we provide the rectiﬁed and raw image sequences.\nWe evaluate our method with rectiﬁed stereo pairs from 61 scenarios of the KITTI dataset\nwhich include the categories \"city\",\"residential\" and \"road\". In order to better show the com-\nparison results of our method and other methods, our experiment referred to the data allocation\nscheme proposed by Eigen et al.22 . We randomly selected 28 scenes from all 61 scenarios, and\nthen randomly selected 697 images from them as test data. The remaining 33 scenes contained\na total of 30159 images, of which 29,000 were used for training data and the rest for veriﬁcation\ndata.\n4.2 Implementation Details\nTo better training our model, we implement our model in Tensorﬂow33 on the experimen-\ntal platform of 32GB E5-2620v4 with 12GB NVIDIA GTX 1080Ti. Since the input of the stereo\nmatching network needs to have a high-quality input view from the concatenate of original left\nview and synthesized right view, in order to get a better reconstructed right view, we trained the\ntwo networks separately.\nView synthesis network: For the training of view synthesis network, we adopt ResNet5023\nas encoder network and use fully convolutional network with bilinear sampler as decoder net-\nwork. we initialize the weights of the encoder network using the trained ResNet50 model from\nImageNet and other weights using random initialization from the gaussian distribution with a\nstandard deviation of 0.01. The network model contains 48 million trainable parameters with\nthe input resolution 512x256. We set the default batch size to 10 and the default epochs num-\nber to 60. In order to make the model converge quickly, we set the initial learning rate to 0.0001,\nwhich for the ﬁrst 40 epochs, we kept the constant learning rate, and then reduce it by a factor\nof 2 after every 10 epochs until the end. During optimization, we refer to the paper13 that we set\nthe parameters α = 1 ,β = 1 and γ = 0.1 of the loss function and constrain the output disparity to\nbe between 0 and dmax, where dmax is assigned 0.3 multiply by the resolution width of output\nimage. In order to estimate the disparity of images at different scales, we obtain four different\nscales of images by down-sampling of a factor of two.\nStereo matching network: Our stereo matching network architecture is similar to the view\nsynthesis network architecture and the input data is the concatenation of both left view and\nsynthesis right view, where the synthesis right view has the same resolution as the original left\nview by the view synthesis network’s up-sampling. Therefore, we refer to the basic setup of the\nview synthesis network to set stereo matching network. However, due to certain errors in the\nreconstructed right view, in order to avoid large shocks and overﬁtting, we reduced the learning\nrate to 0.00001 and increased the number of epochs to 80. We ﬁnd that for stereo matching\nalgorithm, VGG1634 network model is better than ResNet50 through experimental veriﬁcation\nfrom Table ??, so we adopt the VGG16 network model as the encoding part, and the decod-\ning part is still the full convolution network with bilinear interpolation sampling. We initialize\n10\n \nFig 4 The optimization process of the loss function of our view synthesis network(a) and stereo matching net-\nwork(b) in training\nthe weights of the encoder network using the trained VGG16 model from ImageNet and other\nweights using random initialization from the gaussian distribution with a standard deviation of\n0.01. Although we use the synthesized right view as part of the input data of the stereo matching\nmodel, the supervised image of the image alignment loss still use the original stereo pairs, and\nmore accurate depth estimates can be obtained.\nWe abandoned the addition of batch regularization in the two network models because ex-\nperiments showed that the structure did not play an important role in the experimental results.\nWe augment the image data during data loading. We ﬂip and swap every stereo pairs in equal\nprobability, and make sure the both images are in the right position relative to each other. At the\nsame time, we also adjust the brightness, contrast and color of the stereo pairs by making lin-\near changes to the pixel from uniform distribution in the range [0.8,1.2] for each color channel,\n[0.8,1.2] for gamma, [0.5,2.0] for brightness.\nTraining loss: Fig. 4 shows the optimization process of the loss function of our view syn-\nthesis network and stereo matching network in training. In order to better show the training\ntrend of the loss function as a whole, we select a loss value for every 100 iterations and make an\naverage value for every ﬁve iterations to show in the ﬁgure. As can be seen from the ﬁgure, the\nloss value ﬂuctuates slightly in the training process, but the overall trend is gradually decreas-\ning. The loss value of view synthesis network decreased from the original 1.57 to the ﬁnal 0.35,\nwhile the loss value of stereo matching network decreased from the initial 1.13 to the ﬁnal 0.26.\n4.3 Evaluation Metrics\nWe set the following parameters as the estimation metrics of the model and they demon-\nstrate the error and performance of our method on depth evaluation using the ground truth\n11\ndepth data. The estimation metrics are used by Eigen et al.22 .\nRMSE(linear) =\nv\nu\nu\nt 1\nN\nN\nX\ni=1\n||Zi −Z g t\ni ||2\nRMSE(log) =\nv\nu\nu\nt 1\nN\nN\nX\ni=1\n||log(Zi)−log(Z g t\ni )||2\nAccuracy = %Zi : max( Zi\nZ g t\ni\n,\nZ g t\ni\nZi\n) = δ < thr\nAbs Relative di f f erence(ARD) = 1\nN\nN\nX\ni=1\n|Zi −Z g t\ni |\nZ g t\ni\nSquared Relative di f f erence(SRD) = 1\nN\nN\nX\ni=1\n||Zi −Z g t\ni ||2\nZ g t\ni\n(7)\nWhere N is the number of pixels about the ground truth depth map Z g t and evaluation\ndepth map Z.\nTo compare our method with current state-of-the-art methods of unsupervised monocular\ndepth estimates and partial supervised monocular depth estimates,we crop our image resolu-\ntion to match these models. Because these methods cap the evaluated depth in different ranges\nthat Eigen et.al.22 and Godard et al.13 is 0-80m and Garg et.al12 is 1-50m, we respectively pro-\nvide comparative results of the both depth distance. If the estimated depth value is outside the\ndepth range, we set the depth value to be the lowest or highest value of the depth range.\n4.4 Results\nComparison of view synthesis network model: The quality of the synthesized right view is\nvery important for the depth estimation accuracy of the stereo matching network, so we set the\nevaluation formula for the output of the synthesized view network. We compute mean absolute\nerror(MAE) between synthesized right view and original right view.\nMean Absolute Error(M AE) = 1\nN\nN\nX\ni=1\n|I r\ni −¯I r\ni |\n(8)\nWhere N is the number of pixels that was product of the image width and height. I r\ni is the\noriginal right view and ¯I r\ni is the synthesized right view. To evaluate our accuracy of view synthe-\nsis network model, we compare to a variant of this method based on the original unsupervised\nDeep3D25 model and an improved one with adding smoothness constraint or modiﬁed L1 loss.\nTable 3 shows the comparative results of different model.\nAs can be seen from Table 3, for deep3D model, the image reconstruction accuracy is greatly\nimproved by adding smoothing constraints and L1+SSMI loss functions. Our model with addi-\ntional Left-Right Disparity Consistency Loss outperform improved deep3D model 0.1 levels in\nMAE metrics for reconstructed right view. As the reconstruction process of right view is the\n12\nTable 3 Comparison of different view synthesis network models. For the Deep3D model,we uses the original model\nand an improved deep3d model with an added smoothness constraint(SC) and loss of L1+SSMI. The last row is the\nevaluation index of our model that outperform other model.\nMethod\nDataset MAE RMSE RMSE(log) ARD SRD δ < 1.25 δ < 1.252 δ < 1.253\n–lower is better–\n–higher is better–\nDeep3D25\nK\n6.87 13.693\n0.512\n0.412 16.37\n0.690\n0.833\n0.891\nDeep3D25 with SC and L1+SSMI\n3.12 6.211\n0.220\n0.123 1.321\n0.841\n0.936\n0.973\nOurs\n3.02 6.096\n0.214\n0.120 1.300 0.846\n0.939\n0.980\n \nFig 5 The disparity, reconstruction error and reconstruction right view of the three models outputting, where the\ndeep3D w/sc was the deep3d model with adding smoothing constraints and L1+SSMI loss functions\nresult of the joint action of original left view and disparity value, disparity value plays an impor-\ntant role in view reconstruction. In order to compare our model with deep3D series model more\naccurately, the comparative results of model outputting disparity value according to sec. 4.3\nevaluation metrics is shown in table 2. As seen from the table, our model is better than Deep3D\nseries model. Fig. ?? visually shows the disparity, reconstruction error and reconstruction right\nview of the three models outputting.\nComparisons with depth estimation: Table 4 shows the comparison results of estimated\ndepth value between our model and the current state-of-the-art unsupervised monocular depth\nevaluation method on the test dataset of the KITTI benchmark. As can be seen from the Table\n4, compared with other models, our model method has better results. In order to adapt to depth\ncaps of various models, we conducted experiments on depth caps of 80m and 50m respectively\nfor our models and compared them with other corresponding models. For the evaluation depth\ncap of 80m, our evaluation metrics outperform the unsupervised model of Godard et al.13 . In\nparticular, the root mean square error(RMSE) and the square relative deviation(SRD) are better\nthan the model 0.081m and 0.0774 respectively. However, our method is slightly inferior to this\nmodel in accuracy by 0.001m. When we compared our model with other method at an evalua-\ntion depth caps of 50m, we can see from Table 4 that our model, in depth evaluation metrics, are\nsuperior to the results reported by Godard et al.13 in almost every indicators and we can get the\nsame performance as this model on the accuracy of the ﬁrst power. However, compared with\nthe results reported by Grag et al.12 model, we lost 0.242 in the root mean square error(RMSE),\n13\n \nFig 6 Qualitative results about different methods on test dataset.This ﬁgure shows the depth estimation result\nfrom partial supervised model, current state-of-the-art unsupervised model and ground truth depth by the form\nof a visual image.\nbut we won by a large margin in other evaluation metrics. In general,the estimation depth caps\nof 50m is better than that of 80m in terms of the evaluation metrics.\nWe also compare our unsupervised model with the state-of-the-art unsupervised methods\nin terms of the output visual quality as Fig. 6. As we can see from the ﬁgure, the model of Godard\net al.13 has been able to extract the depth map of car, person and trafﬁc sign in the scene, but\nour method can present the details of the depth map more clearly and smoothly. There is no\ndoubt that the ground truth depth data obtained by radar can present a better depth map.\nTable 4 Metrics evaluation results of our model and the current mainstream depth estimation model on the test of\nKITTI dataset using the split of Eigen et al.22 This table shows two different caps of 50m and 80m between ground\ntruth and estimated depth. where the SSD means that both the input data and the supervised data contain the\nsynthesized right view, the OOD means that original right view as these two kinds of data, and that the SOD was\nthat input data contain synthesized right view and the supervised data include original right view. However, in the\ntest stage, mixed stereo pair is used as the input of the training models. We bold out the best results.\nMethod\nDataset Supervised Cap RMSE RMSE(log) ARD\nSRD δ < 1.25 δ < 1.252 δ < 1.253\n–lower is better–\n–higher is better–\nEigen et al.22 Coarse\nK\nY\n0-80m 6.215\n0.271\n0.204 1.598\n0.695\n0.897\n0.960\nEigen et al.22 Fine\nY\n0-80m 6.138\n0.265\n0.195 1.531\n0.734\n0.904\n0.966\nGodard et al.13\nN\n0-80m 5.764\n0.203\n0.114 1.246\n0.854\n0.947\n0.979\nOurs (SSD)\nN\n0-80m 5.725\n0.203\n0.113 1.240\n0.850\n0.946\n0.979\nOurs (OOD)\nN\n0-80m 5.710\n0.202\n0.1127 1.166\n0.850\n0.946\n0.980\nOurs (SOD)\nN\n0-80m 5.683\n0.201\n0.111 1.1686 0.854\n0.948\n0.980\nGarg et al.12\nK\nN\n0-50m 5.104\n0.273\n0.169 1.080\n0.740\n0.904\n0.962\nGodard et al.13\nN\n0-50m 5.431\n0.199\n0.110 1.034\n0.854\n0.949\n0.980\nOurs (SSD)\nN\n0-50m 5.413\n0.199\n0.109 0.975\n0.850\n0.949\n0.980\nOurs (OOD)\nN\n0-50m 5.404\n0.198\n0.110 0.973\n0.850\n0.948\n0.981\nOurs (SOD)\nN\n0-50m 5.346\n0.196\n0.108 0.963\n0.854\n0.950\n0.981\nThe inﬂuence of input data on stereo matching network in training stage: As there is an\nerror between the synthetic right view and the original right view, it can be seen from Table 1\nand Table 4 that the network model trained based on the synthetic right view is inferior to the\nnetwork model trained based on the original in terms of estimation metrics. In order to verify\nthe inﬂuence of training data on stereo matching network, we use three different input data to\ntrain the network model,which are respectively represented as the SSD, the OOD and the SOD\n14\n \nFig 7 The visual comparative result from different input data of stereo matching network at the training stage.\nwhere the SSD means that both the input data and the supervised data contain the synthesized right view, the\nOOD means that original right view as these two kinds of data, and that the SOD was that input data contain\nsynthesized right view and the supervised data include original right view.\nin Table 4. Where, in the training phase, the SSD refers to the original left view and synthesized\nright view as the stereo pairs, the OOD means that we use the original stereo pairs as as the\nstereo pairs, and the SOD refers to the original left view and synthesized right view as the stereo\npairs. However, in the testing phase, mixed stereo pairs is used as the input of the three training\nmodels. As can be seen from table 3, no matter the evaluation caps is 50m or 80m, the training\nmode of the SOD is better than the other two training modes, and the evaluation metrics of\nthe SSD training mode is the worst, even lower than monocular depth estimation methods of\nGodard et al.13 . At the depth cap of 80m, the performance of the OOD and the SOD are not\ndifferent, even exceeding 0.003m in terms of the SRD metrics. When the depth cap is 50m,\nalthough the SOD is not much better than the OOD, it is clearly better than the OOD.\nFig. 7 shows the visual output results of the stereo matching models feeding in three input\ndata. We can see from ﬁgure that since the synthetic right view is used as the supervision label\nfor the training method of SSD, the depth map generated a large error in the edge part of the\nimage. Although the OOD’s result is better than that of SSD, the method does not refer to the\nsynthetic right view during training, so the depth map generated is not optimal. For the method\nof SOD, We adapt mixed stereo pairs including synthetic right view as the input data and original\nstereo pairs as the supervision label,that can better optimize our network model.\n5 Conclusions\nIn this paper, we propose an unsupervised stereo matching model based on monocular vi-\nsion. The experimental results show that the stereo matching models outperforms monocular\ndepth estimation models under the same unsupervised depth estimation model. Therefore, we\nﬁrst proposed a deep neural network to synthesize the right view from a single left view, and\nthen used the reconstructed right view and original left view as the input of the unsupervised\nstereo matching network to estimate the depth. Both the view synthesis network and the stereo\nmatching network are the unsupervised methods by only supervising on the image alignment\nloss from synthesized view and original view without the ground truth depth. Experimental\nresults show that our method is superior to the current mainstream method of unsupervised\ndepth estimation.\nAlthough our method has certain advantages for some unsupervised methods, it cannot\nsurpass the state-of-the-art supervised methods with the ground truth depth and the unsuper-\nvised methods with the original stereo pairs as the stereo matching method input. So you can\n15\nsee that our stereo matching network relies on a high-quality input data, we need to make fur-\nther improvements to the view synthesis network to reconstruct the higher-quality right view.\nFinally, we may need a more appropriate network model to address the inconsistent depth of\nthe problem caused by specular and transparent surfaces.\nReferences\n1 Y. Furukawa, Multi-View Stereo: A Tutorial, Now Publishers Inc. (2015).\n2 L. Ladický, H. Christian, and M. Pollefeys, “Learning the matching function,” Computer\nScience (2015).\n3 P. Sturm and B. Triggs, “A factorization based algorithm for multi-image projective struc-\nture and motion,” Proceedings Eccv96 London 1065(3), 709–720 (1996).\n4 W. Chen, Z. Fu, D. Yang, et al., “Single-image depth perception in the wild,” (2016).\n5 D. Eigen and R. Fergus, “Predicting depth, surface normals and semantic labels with\na common multi-scale convolutional architecture,” in IEEE International Conference on\nComputer Vision, 2650–2658 (2015).\n6 A. Saxena, M. Sun, and A. Y. Ng, “Make3d: Learning 3d scene structure from a single still\nimage,” IEEE Trans Pattern Anal Mach Intell 31(5), 824–840 (2009).\n7 Y. Lecun, Stereo matching by training a convolutional neural network to compare image\npatches, JMLR.org (2016).\n8 W. Luo, A. G. Schwing, and R. Urtasun, “Efﬁcient deep learning for stereo matching,” in\nComputer Vision and Pattern Recognition, 5695–5703 (2016).\n9 L. Ladicky, J. Shi, and M. Pollefeys, “Pulling things out of perspective,” in IEEE Conference\non Computer Vision and Pattern Recognition, 89–96 (2014).\n10 B. Li, C. Shen, Y. Dai, et al., “Depth and surface normal estimation from monocular images\nusing regression on deep features and hierarchical crfs,” in IEEE Conference on Computer\nVision and Pattern Recognition, 1119–1127 (2015).\n11 I. Laina, C. Rupprecht, V. Belagiannis, et al., “Deeper depth prediction with fully convolu-\ntional residual networks,” 239–248 (2016).\n12 R. Garg, K. B. G. Vijay, G. Carneiro, et al., “Unsupervised cnn for single view depth estima-\ntion: Geometry to the rescue,” 740–756 (2016).\n13 C. Godard, O. M. Aodha, and G. J. Brostow, “Unsupervised monocular depth estima-\ntion with left-right consistency,” in Computer Vision and Pattern Recognition, 6602–6611\n(2017).\n14 T. Zhou, M. Brown, N. Snavely, et al., “Unsupervised learning of depth and ego-motion\nfrom video,” 6612–6619 (2017).\n15 Y. Luo, J. Ren, M. Lin, et al., “Single view stereo matching,” (2018).\n16 A. Saxena, S. H. Chung, and A. Y. Ng, “3-d depth reconstruction from a single still image,”\nInternational Journal of Computer Vision 76(1), 53–69 (2008).\n17 J. Pang, W. Sun, J. S. Ren, et al., “Cascade residual learning: A two-stage convolutional\nneural network for stereo matching,” 878–886 (2017).\n18 S. Zagoruyko and N. Komodakis, “Learning to compare image patches via convolutional\nneural networks,” in IEEE Conference on Computer Vision and Pattern Recognition, 4353–\n4361 (2015).\n16\n19 N. Mayer, E. Ilg, P. Häusser, et al., “A large dataset to train convolutional networks for dis-\nparity, optical ﬂow, and scene ﬂow estimation,” in Computer Vision and Pattern Recogni-\ntion, 4040–4048 (2016).\n20 J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for semantic segmen-\ntation,” in IEEE Conference on Computer Vision and Pattern Recognition, 3431–3440 (2015).\n21 A. Dosovitskiy, P. Fischery, E. Ilg, et al., “Flownet: Learning optical ﬂow with convolutional\nnetworks,” in IEEE International Conference on Computer Vision, 2758–2766 (2015).\n22 D. Eigen, C. Puhrsch, and R. Fergus, “Depth map prediction from a single image using a\nmulti-scale deep network,” in International Conference on Neural Information Processing\nSystems, 2366–2374 (2014).\n23 K. He, X. Zhang, S. Ren, et al., “Deep residual learning for image recognition,” 770–778\n(2015).\n24 F. Liu, C. Shen, G. Lin, et al., “Learning depth from single monocular images using deep\nconvolutional neural ﬁelds,” IEEE Transactions on Pattern Analysis & Machine Intelligence\n38(10), 2024–2039 (2016).\n25 J. Xie, R. Girshick, and A. Farhadi, “Deep3d: Fully automatic 2d-to-3d video conversion\nwith deep convolutional neural networks,” (2016).\n26 Y. Kuznietsov, J. St´l´zckler, and B. Leibe, “Semi-supervised deep learning for monocular\ndepth map prediction,” 2215–2223 (2017).\n27 K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image\nrecognition,” arXiv preprint arXiv:1409.1556 (2014).\n28 H. Zhao, O. Gallo, I. Frosio, et al., “Is l2 a good loss function for neural networks for image\nprocessing?,” Computer Science (2015).\n29 Z. Wang, A. Bovik, H. Sheikh, et al., “Image quality assessment: from error visibility to\nstructural similarity,” IEEE Trans Image Process 13(4), 600–612 (2004).\n30 P. K. Nathan Silberman, Derek Hoiem and R. Fergus, “Indoor segmentation and support\ninference from rgbd images,” in ECCV, (2012).\n31 A. Geiger, “Are we ready for autonomous driving? the kitti vision benchmark suite,” in\nComputer Vision and Pattern Recognition, 3354–3361 (2012).\n32 A. Geiger, P. Lenz, C. Stiller, et al., “Vision meets robotics: The kitti dataset,” International\nJournal of Robotics Research 32(11), 1231–1237 (2013).\n33 M. Abadi, A. Agarwal, P. Barham, et al., “Tensorﬂow: Large-scale machine learning on het-\nerogeneous distributed systems,” (2016).\n34 K. Simonyan and Z. Andrew, “Very deep convolutional networks for large-scale image\nrecognition,” 1409–1556 (2014).\n17\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2018-12-31",
  "updated": "2018-12-31"
}