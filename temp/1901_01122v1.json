{
  "id": "http://arxiv.org/abs/1901.01122v1",
  "title": "Machine Translation: A Literature Review",
  "authors": [
    "Ankush Garg",
    "Mayank Agarwal"
  ],
  "abstract": "Machine translation (MT) plays an important role in benefiting linguists,\nsociologists, computer scientists, etc. by processing natural language to\ntranslate it into some other natural language. And this demand has grown\nexponentially over past couple of years, considering the enormous exchange of\ninformation between different regions with different regional languages.\nMachine Translation poses numerous challenges, some of which are: a) Not all\nwords in one language has equivalent word in another language b) Two given\nlanguages may have completely different structures c) Words can have more than\none meaning. Owing to these challenges, along with many others, MT has been\nactive area of research for more than five decades. Numerous methods have been\nproposed in the past which either aim at improving the quality of the\ntranslations generated by them, or study the robustness of these systems by\nmeasuring their performance on many different languages. In this literature\nreview, we discuss statistical approaches (in particular word-based and\nphrase-based) and neural approaches which have gained widespread prominence\nowing to their state-of-the-art results across multiple major languages.",
  "text": "Machine Translation: A Literature Review\nAnkush Garg, Mayank Agarwal\nDepartment of Computer Science\nUniversity of Massachusetts Amherst\n{agarg,mayankagarwa}@cs.umass.edu\nAbstract\nMachine translation (MT) plays an important role in beneﬁting linguists, sociolo-\ngists, computer scientists, etc. by processing natural language to translate it into\nsome other natural language. And this demand has grown exponentially over past\ncouple of years, considering the enormous exchange of information between differ-\nent regions with different regional languages. Machine Translation poses numerous\nchallenges, some of which are: a) Not all words in one language has equivalent\nword in another language b) Two given languages may have completely different\nstructures c) Words can have more than one meaning. Owing to these challenges,\nalong with many others, MT has been active area of research for more than ﬁve\ndecades. Numerous methods have been proposed in the past which either aim at im-\nproving the quality of the translations generated by them, or study the robustness of\nthese systems by measuring their performance on many different languages. In this\nliterature review, we discuss statistical approaches (in particular word-based and\nphrase-based) and neural approaches which have gained widespread prominence\nowing to their state-of-the-art results across multiple major languages.\n1\nIntroduction\nMachine Translation is a sub-ﬁeld of computational linguistics that aims to automatically translate\ntext from one language to another using a computing device. To the best of our knowledge, Petr\nPetrovich Troyanskii was the ﬁrst person to formally introduce machine translation [22]. In 1939,\nPetr approached the Academy of Sciences with proposals for mechanical translation, but barring\npreliminary discussions these proposals were never worked upon. Thereafter, in 1949, Warren\nWeaver [46] proposed using computers to solve the task of machine translation. Since then, machine\ntranslation has been studied extensively under different paradigms over the years. Earlier research\nfocused on rule-based systems, which gave way to example-based systems in the 1980s. Statistical\nmachine translation gained prominence starting late 1980s, and different word-based and phrase-based\ntechniques requiring little to no linguistic information were introduced. With the advent of deep\nneural networks in 2012, application of these neural networks in machine translation systems became\na major area of research. Recently, researchers announced achieving human parity on automatic\nchinese to english news translation [18] using neural machine translation. While early machine\ntranslation systems were primarily used to translate scientiﬁc and technical documents, contemporary\napplications are varied. These include various online translation systems for exchange of bilingual\ninformation, teaching systems, and many others.\nIn this literature review, we survey two major sub-ﬁelds of machine translation: statistical machine\ntranslation, and neural machine translation. The rest of the review is structured as follows: Section\n2 brieﬂy discusses the early work in machine translation. Section 3 reviews Statistical machine\ntranslation, focusing on word-based and phrase-based translation techniques. Section 4 elaborates\non neural machine translation techniques where we also discuss different attention mechanisms and\narchitectures with special purposes. Section 5 brieﬂy describes the current research in the ﬁeld.\nFinally, we conclude the report with Section 6.\narXiv:1901.01122v1  [cs.CL]  28 Dec 2018\n2\nEarly Work\nIn the 1970’s, Rule-based Machine Translation (RBMT) was the primary focus of research. Such\nsystems fall into one of the following three categories: Direct systems (these map input sentence\ndirectly to the output sentence), Transfer RBMT systems (these use morphological and syntactic\nanalysis to translate sentences), and Interlingual RBMT systems (these transformed the input sentence\nto an abstract representation and mapped this abstract representation to the ﬁnal output). One such\nwork in Interlingual RBMT system is by Carbonell et al. in 1978 [7]. The proposed approach\ntranslates text by: 1) Converting the source text to a language-free conceptual representation, 2)\nAugmenting this representation with information that was implicit in the source text, and 3) Converting\nthis augmented representation to the target language. The authors argue that translation requires\ndetailed understanding of the source text which semantic rules are inadequate to capture and therefore\nneed to be augmented with detailed domain knowledge as well.\nRule-based MT is complicated for certain languages (ex: English-Japan) owing to different structures\nof the languages. In 1984, Nagao [33] proposed a translation system that works by analogy principle.\nTitled \"machine translation by example-guided inference\", the system relies on a big dataset of\nexample sentences and their translations to learn the correspondence between English-Japanese words\nand also the structure of the language. The authors describe different approaches to build such a\nsystem and also discuss ways to curate the data required for such a system. This paper, to the best\nof our knowledge, is the ﬁrst paper to introduce example-based learning and paved way for further\nresearch in building machine translation systems that do not rely on manually curated rules and\nexceptions.\n3\nStatistical Machine Translation\nStatistical Machine Translation (SMT), as introduced by Brown et al. [5], takes the view that every\nsentence S in a source language has a possible translation T in the target language. Building on\ntop of this fundamental assumption, SMT based approaches assign to each (S, T) sentence pair the\nprobability P(T|S), which is interpreted as the probability that sentence T is the translated equivalent\nin the target language of the sentence S in the source language. Accordingly, statistical approaches\ndeﬁne the problem of Machine Translation as:\nT = arg max\nT\nP(T|S)\n(1)\n= arg max\nT\nP(T)P(S|T)\n(2)\nThe components P(T) and P(S|T) in the equation above are referred to as the Language Model\nof the target language, and the Translation Model respectively. Hereafter, we refer to the language\nmodel of the target language as the language model itself. Together, the language model and the\ntranslation model compute the joint probability of the sentences S and T. The argmax operation over\nall sentences in the target language denotes the search problem and is referred to as the decoder. The\ndecoder performs the actual translation - given a sentence S, it searches for a sentence T in the target\nlanguage with the highest probability P(T|S).\nSince the current formulation requires a translation model for target language to the source language,\nan important question arises. Why can’t the process to build this translation model be utilized to\nbuild the model that computes P(T|S). This would eliminate the need for the language model\nof the target language, and can be used in conjunction with the decoder to get the translation of\nthe original sentence. Brown et al. [6] state this to be a means to get a well-formed sentence. To\nmodel P(T|S) and use this for translation would require the probabilities to be concentrated over\nwell-formed sentences in the target language domain. Rather, this is achieved through the joint usage\nof the language model and the translation model. Sentences which are not well-formed are expected\nto have a lower language model probability which offsets the necessity for the translation model to\nhave their probabilities concentrated over well-formed sentences.\nIn the following sections, we ﬁrst brieﬂy review language models since they are modelled independent\nof the translation model and typically remain consistent across works in SMT. Thereafter, we review\nthe research work in SMT categorized into two sections: Word-based SMT, and Phrase-based SMT.\n2\n3.1\nLanguage models\nGiven a target string T of length m and consisting of words t1, t2, · · · , tm, we can write the language\nmodel probability P(T) as:\nP(T) = P(t1, t2, · · · , tm) = P(t1)\nm\nY\ni=2\nP(ti|t1:i−1)\n(3)\nThis converts the language modelling problem into one that requires computing probabilities for a\nword given its history. However, computing these probabilities is infeasible since there could be\ntoo many histories for a word. Thus, this requirement is relaxed by truncating the dependence of\nthe current word on a ﬁxed subset of its history. In an n-gram model, it is assumed that the current\nword depends only on the previous (n-1) words. For example, in a trigram model, P(wi|w1:i−1) =\nP(wi|wi−1, wi−2). These probabilities can now be computed through counting to get a Maximum\nLikelihood Estimate (MLE). There are other formulations of language models - ones that make use\nof neural networks, or formulate the problem as a maximum entropy language model. However,\nwe won’t delve deeper into language models since the majority of research in SMT is focused on\ndifferent formulations of the translation model, but the reader can refer to the following resources for\nmore information [16] [37] [3].\n3.2\nWord-based SMT\nPost Warren Weaver’s proposal in 1949 [46] to use statistical techniques from the then nascent ﬁeld of\ncommunication theory to the task of using computers to translate text from one language to another,\nresearch in the area lay dormant for a while. It wasn’t until 1988 that Brown et al. in [4] outlined\nan approach to use statistical inference tools to solve the task. The authors argued that translation\nought to be based on a complex glossary of correspondences of ﬁxed locations. This glossary would\nmap words as well as phrases (contiguous and non-contiguous) to corresponding translations. For\nexample, the following could be the contents of a glossary mapping english words/phrases to their\nfrench counterpart: [word = mot], [not = ne pas], [seat belt = ceinture de sécurité]. The authors\nbase their approach on the following decomposition of the task: 1) Partition the source text into a\nset of ﬁxed locations, 2) Use the glossary and contextual information to select the corresponding\nset of ﬁxed locations in the target language, and 3) Arrange the words of the target ﬁxed location\ninto a sequence that forms the target language. The proposed glossary in the paper is based on a\nmodel of the translation process P(T|S), and comes to the critical conclusion that a probabilistic\nmethod is required to identify the corresponding words in the target and source sentence. To learn the\nparameters of this glossary, the paper introduces a concept of \"generation pattern\" which as we will\nsee later is similar to the critical concept of alignment in machine translation. Since the authors were\nexperimenting with English-French language pairs - languages with similar word order and therefore\nthe translation being quite local - the fact that the proposed glossary did not incorporate this property\nmotivated them to propose another formulation of the glossary - one that models the locality of the\nlanguage pairs through distortion probabilities P(k|h, m, n), where k refers to the kth word in T, h\nrefers to the hth word in S, and m and n are the lengths of T and S. To the best of our knowledge, this\nwork was the ﬁrst to formalize the ﬁeld of statistical machine translation, and though it provided only\nsome intermediate results and not the translation examples it stimulated interest in the application of\nstatistical methods to machine translation.\nTwo years later, in 1990, Brown et al. in [5] provided ﬁrst experimental results for a statistical machine\ntranslation technique translating sentences in French to English. The proposed method translates\n5% of the sentences exactly to their actual translation, but if alternate and different translations are\nconsidered reasonable translations, the model’s accuracy rises to 48%. The authors further argue that\nthis system reduces the manual work of translation by about 60% as measured in units of key strokes.\nThe translation model proposed by Brown et al. in [5] introduces the critical concept of alignment.\nAs deﬁned by the authors, alignment between a pair of strings (S, T) indicates the origin in T of each\nword in S. One such alignment for the sentence pairs \"Le programme a été mis en application\" (S)\nand \"And the program has been implemented\" (T) is shown in ﬁgure 1. This particular alignment\nstates that the origin of the word \"the\" in the english sentence lies in the word \"Le\", for \"program\" its\n\"programme\", and similarly \"implemented\" originates from the words \"mis en application\". Closely\n3\nrelated to this concept of alignment is fertility. Fertility is the number of words in S, that each word\nin T produces. Thus, for the same example, word \"And\" has fertility 0 (since it’s not aligned with any\nword in french translation), \"the\" has fertility 1, and \"implemented\" has fertility 3.\nFigure 1: Alignment between two sentences.\nBuilding on top of their previous work, Brown et al. in [6] describe a set of ﬁve statistical models,\neach with a different model of the alignment probability distribution. Speciﬁcally, they modify their\ntranslation model to include the alignment variable A.\nP(S|T) =\nX\nA\nP(S, A|T)\n(4)\nFor Models 1 and 2, the authors decompose P(S, A|T) into three probability distributions: 1)\ndistribution over the length of the target sentence, 2) alignment model deﬁning a distribution over the\nalignment conﬁguration, and 3) the translation probabilities over target sentence, given the alignment\nconﬁguration and the source sentence. The main distinction between Models 1 and 2 is in their\nmodelling of the alignment probabilities. Model 1 assumes a uniform distribution over all alignments\nfor a sentence pair, while Model 2 uses a zero-order alignment model where alignments at different\npositions are independent of each other. Additionally, the trained parameters of Model 1 are used to\ninitialize Model 2.\nFor Models 3, 4, and 5, the authors decompose P(S, A|T) differently and parameterize fertilities\ndirectly. The generative process is broken into two parts: 1) Given T, compute the fertility of each\nword in T and a set of words in S which connect to it. This is called the tableau τ, and 2) Permute the\nwords in the tableau to form the source sentence S. This permutation is denoted as π. Accordingly,\nP(S, A|T) is decomposed as:\nP(S, A|T) =\nX\nτ,π∈(S,A)\nP(τ, π|T)\n(5)\nP(τ, π|T) is further decomposed to result in the following parameters for Model 3 and 4: fertility\nprobabilities, translation probabilities, and distortion probabilities. The main distinction between\nModel 3 and 4 lies in modelling of the distortion probabilities. Model 3 uses a zero-order distortion\nprobabilities where the distortion for a particular position depends only on its current position and the\nlengths of T and S. Model 4 on the other hand, parameterizes these distortion probabilities by two\nsets of parameters: one to place the head of each word/phrase, and the other to place the rest of the\nwords. This was done because Model 3 did not account well for the tendency of phrases to move\naround as a unit.\nBoth of these models (Model 3 and 4) are however deﬁcient. The authors deﬁne a model to be\ndeﬁcient when it does not concentrate its probability over events of interest but rather distributes it\nover generalized strings. Model 5, which is the ﬁnal of the proposed models, aims to avoid deﬁciency\nand does so by reformulating Model 4 by a suitably reﬁned alignment model. Since each of the 5\nproposed models have a particular decomposition of the translation model, the authors have tried to\ngain insights into the capabilities of these individual distributions as well as the ﬁnal model itself. It\nis found that while the individual distributions model the particular events well, there is room for\nimprovement in the model’s capacity to translate.\nThe fundamental basis of the ﬁve models presented by Brown et al. [6] was the introduction of a\nhidden alignment variable in the translation model. These alignment probabilities were then modelled\ndifferently in different models. Vogel et al. [44] propose a new alignment model that’s based on\n4\nHidden Markov Models (HMM) and aims to effectively model the strong localization effect when\ntranslating between certain languages (ex: for language pairs from Indoeuropean languages). The\ntranslation model P(S|T) is accordingly broken down into two components: the HMM alignment\nprobabilities and the translation probabilities. The key component of this approach is that it makes\nthe alignment probabilities depend on the relative position of the word alignment rather than the\nabsolute position. This HMM model is shown to result in smaller perplexities as compared to Model\n2 by Brown et al. [6] and also produces smoother alignments.\nWith the increased focus on research in alignment models, Och and Ney [35] present an annotation\nand evaluation scheme for word-alignment models. The proposed annotation scheme made it possible\nto explicitly annotate the ambiguous alignments along with the sure alignments. This provided an\nextra degree of freedom to the human annotators to generate reference alignments. To evaluate the\nperformance of a word alignment model the authors propose an Alignment Error Rate which depends\non the sure and ambiguous reference alignments, and the alignment produced by the model.\n3.3\nPhrase-based SMT\nDespite the revolutionary nature of word-based systems, they still failed to deal with cases, gender,\nand homonymy. Every single word was translated in a single-true way, according to the machine.\nIn phrase-based translation system there is no restriction of translating source sentence into target\nsentence word-by-word. This was a signiﬁcant departure from word-based models - IBM models. In\nPhrase-based systems, a lexical unit is a sequence of words (of any length) as opposed to a single\nword in IBM models. Each pair of units (one each from source and target language) has a score or a\n‘weight’ associated with it. For example, a lexical entry could look like:\n(le chien, the dog, 0.002)\nDeﬁnition More formally, a phrase-based lexicon L is a set of lexical entries where each lexical\nentry is a tuple (f,e,g) where:\n• f is a sequence of one or more foreign language words\n• e is a sequence of one or more source language words\n• g is a ‘score’ of the lexical entry which is a real number.\nPhrase-based translation models improved the translation quality over IBM models and many re-\nsearches tried to advance the state-of-the-art with these models. Och et al.[35] alignment template\nmodel can be reframed as a phrase translation system; Yamada and Knight[48] use phrase translation\nin a syntax based translation system; Marcu et al.[31] introduced a joint-probability model for\nphrase translation. At its core, phrase-based translation system has a phrase translation probability\ntable (deﬁned above) to map phrases in source language to phrases in target language. The phrase\ntranslation table is learnt from word alignment models using bilingual corpus. We don’t delve into\nthe details of learning phrase lexicons from word alignments and encourage the reader to refer [34],\n[35] for details. We, instead, focus our discussions on modelling aspect of phrase-based systems and\nvariations among different models.\nPhrase-based systems decompose the translation probability deﬁned in equation-2 as follows:\nP(S|T) = P(¯s1|¯t1)\n(6)\n=\nIY\ni=1\nφ(¯si|¯ti)d(ai −bi−1)\n(7)\n¯s1 is the sequence of phrases in source sentence, ¯f1 is the sequence of phrases in target sentence, I\nis the number of sequences (in source sentence). d(ai −bi−1) is the relative distortion probability\ndistribution, where ai denotes the start position of the source language phrase that was translated\ninto the ith target language phrase, and bi−1 denotes the end position of the source language phrase\n5\ntranslated into the (i-1)th target language phrase. φ(¯si|¯ti) is the phrase translation probabilities (or\nequivalently phrase translation table) learnt from bilingual corpus and the distortion probability is\neither learnt or could be as simple as α|ai−bi−1−1|. The distortion probability distribution accounts\nfor reordering of phrases in target language after they have been translated individually. Once all the\nfactors (phrase translation tables, distortion distribution, language model) are learnt, the decoding\noperation (equation-1) generates translated sentences. The reader is encouraged to look at [14], [15],\n[45] for more information on design of decoders and their nuances.\nMarcu et al.[31] present a different formulation of phrase-based model to learn the phrase transition\ntable and distortion distribution. They argue that lexical correspondences can be established not only\nat the word level but also at the phrase level. They model the translation task as a joint probability\nmodel where the translations between phrases are learnt directly without using word alignment\nmodels. Their joint probability model is deﬁned as:\np(E, F) =\nX\nC∈C|L(E,F,C)\nY\nci∈C\n[t(¯ei, ¯fi) ∗\n| ¯fi|\nY\nk=1\nd(pos( ¯f k\ni ), poscm(¯ei))]\n(8)\nThe generative story of this model is as follows:\n1. Generate of bag of concepts C where each concept ci is the hidden variable.\n2. For each concept ci ∈C, generate a pair of phrases (¯ei, ¯fi) according to the distribution\nt(¯ei, ¯fi) where ¯ei and ¯fi each contain atleast one word.\n3. Order the phrases generated in each language so as to create two linear sequence of phrases;\nthese sequences correspond to the sentence pair in bilingual corpus. This is modelled with\nd(.) distribution.\nA set of concepts can be linearized into a sentence pair (E, F) if E and F can be obtained by\npermuting the phrases ¯ei and ¯fi that characterize all concepts ci ∈C.\nTo learn this model, they also propose a heuristics based learning algorithm. The model couldn’t be\nlearnt with the EM algorithm exhaustively as there are exponential number of alignments that can\ngenerate the sentence pair (E,F). They use French-English parallel corpus of 100,000 sentence pairs\nfrom the Hansard corpus to train their model. Their model achieves boost in the BLEU score by 6\npoints compared to the IBM model 4 (with BLEU score of 22).\nOch et al.[36] proposed a maximum entropy models for phrase-based translation where the translation\nprobability is formulated as conditional log-linear model. The conditional probability of a sentence\nin target language given sentence in source language is:\nPr(¯eI\n1| ¯f J\n1 ) = pλM\n1 (¯eI\n1| ¯f J\n1 )\n(9)\n=\nexp[PM\nm=1 λmhm(¯eI\n1, ¯f J\n1 )]\nP\n¯e′I\n1 exp[PM\nm=1 λmhm(¯e′I\n1, ¯f ′J\n1 )]\n(10)\nIn this framework, there is set of M feature functions hm(¯eI\n1, ¯f J\n1 ). For each feature function, there\nexists a model parameter λm, m = 1, ...., M. The model is trained with the GIS (global iterative\nsearch) algorithm[12]. Since the normalization constant is intractable, it is approximated with highly-\nprobable n sentences. The list of highly probable n sentences is computed by extended version from\nused search algorithm (Och et al. [35]) which approximately computes n-best list of translations.\nThe main advantage of the maximum entropy model is that any feature function can be added easily\n(for eg., language model, distortion model, word penalty, phrase translation model) and the weights\nof these individual feature functions (models) can be learnt jointly. They experiment with various\nfeature functions including language model, word penalty, phrase translation dictionary and achieve\nstate-of-the-art results on VERBMOBIL task which is a speech translation task in the domain of\nappointment scheduling, travel planning and hotel reservation.\n6\n4\nNeural Machine Translation\nIn most statistical approaches to machine translation, the most crucial component of the system is\nthe phrase transition model. It is either the joint probability of co-occurence of source and target\nlanguage phrases, P(ei, fi) or the conditional probability of generating a target language phrase\ngiven the source language phrase P(ei|fi). Such models consider the phrases which are distinct\non the surface as distinct units. Although these distinct phrases share many properties, linguistic\nor otherwise, they rarely share parameters of the model while predicting translations. There is no\nconcrete notion of ‘phrase similarity’ in such models. Besides ignoring phrase similarities, this leads\nto a very common problem of sparsity. It gets difﬁcult for model to adapt itself to unseen phrases at\ntest time. Finally, this makes it difﬁcult to adapt such models to other similar domains.\nContinuous representations of linguistic units, be it character, word, sentence or document have shown\npromising results on various language processing tasks. One of early works which introduced this idea\nwas proposed by Bengio et al. [3]. They model words with continuous ﬁxed dimension word vectors\nusing neural network and achieve state-of-the-art results on language modelling task. It has also shown\npromising results in dealing with sparsity issue. Collobert et al. [10] have shown that continuous\nrepresentations for words are able to capture the syntactic, semantic and morphological properties of\nthe words. Continuous representations for characters have also shown notable results in language\nmodelling task as proposed by Sutskever et al. [42]. Recently, continuous representations have been\nproposed for phrases and sentences and have been shown to carry task-dependent information to help\ndownstream language processing tasks (Grefenstette et al. [17], Socher et al. [40], Hermann et al.\n[19]).\nThe approaches discussed above make use of neural networks to model continuous representations\nof linguistic units. Deep neural networks have shown tremendous progress in computer vision\n(eg., Krizhevsky et al. [26]) and speech recognition (eg., Hinton et al. [20] and Dahl et al. [11])\ntasks. Since then, they have also been successfully applied to solve many NLP tasks like paraphrase\ndetection (Socher et al. [41]) and word embedding extraction (Mikolov et al. [32]). Neural networks\nhave also been applied to advance the state-of-the-art in statistical machine translation. Schwenk [38]\nsummarizes usage of feedforward neural networks in the framework of phrase-based SMT system.\n4.1\nPreliminary\nIn the next two sections (4.1.1 and 4.1.2), we discuss some background work which is common to\nalmost all the neural machine translation systems.\n4.1.1\nRecurrent Language Model\nA recurrent neural network (RNN) is a neural network that consists of a hidden state h and an optional\noutput y which operates on a variable length sequence x = (x1... xT ). At each time step t, the hidden\nstate ht of the RNN is updated by:\nht = f(ht−1, xt)\n(11)\nwhere the f is non-linear activation function which is usually implemented with LSTM cell (Hochreiter\nand Schmidhuber [21]). Using softmax function with vocabulary size V, an RNN can be trained to\npredict the distribution over xt given the history of words (xt−1, xt−2, ...x1) at each time step t. By\ncombining the probabilities at each time step, we can compute the probability of the sequence x (eg:\ntarget language sentence) using\np(x) =\nT\nY\nt=1\np(xt|xt−1, ..., x1)\n(12)\nwhich is called the Recurrent Language Model (RLM).\n4.1.2\nEncoder-Decoder Architecture\nThough RNN Encoder-Decoder architecture was proposed by Cho et al. [9] for a machine translation\ntask, it remains the base model for most of the NLP sequence-to-sequence models (and especially\nmachine translation). We discuss this model in its general form here, and delve into details of different\nneural architectures in next section.\n7\nAn encoder-decoder neural model (ﬁgure 2), from a probabilistic perspective, is a general method\nto learn the conditional distribution over variable length sequence given yet another variable length\nsequence, e.g. p(y1, ..., yT ′|x1, ..., xT ). An encoder is an RNN which reads each symbol in input\nsequence (x) one word at a time till it encounters end-of-sequence symbol. The hidden state of the\nRNN at the last time step is the summary c of the whole input sequence. The decoder operates very\nsimilar to RLM discussed previously except that the hidden state of the decoder ht now depends on\nthe summary c too. Hence the hidden state of the decoder at time step t is calculated by\nht = f(ht−1, yt−1, c)\n(13)\nand the conditional distribution of the next symbol (for e.g. next word in target language sentence\ngiven source language sentence) is\np(yt|y1, ..., yt−1) = g(ht, yt−1, c)\n(14)\nThe two components of the encoder-decoder model are jointly trained to maximize the conditional\nlog-likelihood\n1\nN\nN\nX\nn=1\nlog pθ(yn|xn)\n(15)\nFigure 2: An encoder-decoder architecture\nWe now describe some of the Neural Machine Translation (NMT) methods proposed recently.\n4.2\nNMT Methods\nMotivated from success of deep neural networks and their ability to represent a linguistic unit with a\ncontinuous representation, Kalchbrenner et al. [25] propose a class of probabilistic translation models,\nRecurrent Continuous Translation Model (RCTM) for machine translation. The RCTM model has\na generation aspect and a conditional aspect. The generation of a sentence in target language is\nmodelled with target Recurrent language model. The conditioning on the source sentence is modelled\nwith a Convolutional Neural Network (CNN). In their model, CNN takes a sentence as input and\ngenerates a ﬁxed size representation of this source sentence. This representation of source sentence is\npresented to the Recurrent Language Model to produce the translation in target language. The entire\nmodel (CNN and RNN) is trained jointly with back-propagation.\nTo the best of our knowledge, this is the ﬁrst work which explores the idea of modelling the task\nof machine translation entirely with neural networks, with no component from statistical machine\ntranslation systems. They propose two CNN architectures to map source sentence into ﬁxed size\ncontinuous representation. Though CNN architectures have shown tremendous success in image\nspace, these architectures were ﬁrst explored extensively in text space in this paper. We, therefore,\ndiscuss these architectures in detail here.\nThe Convolutional Sentence Model (CSM) creates a representation for a sentence that is progressively\nbuilt up from representations of the n-grams in the sentence. The CSM architecture embodies a\nhierarchical structure, similar to parse trees, to create a sentence representation. The lower layers\nin the CNN architecture operate locally on n-grams and the upper layers act increasingly globally\non the entire sentence. The lack of need of parse tree makes it easy to apply these models to\nlanguages for which parsers are not available. Also, generation of the sentence in target language is\nnot dependent on one particular parse tree. Similar to CSM, the authors propose another CNN model\ncalled Convolutional n-gram model (CGM). The CGM is obtained by truncating the CSM at the level\nwhere n-grams are represented for the chosen value of n. The CGM can also be inverted (icgm) to\n8\nobtain a representation for a sentence from the representation of its n-grams. The transformation\nicgm unfolds the n-gram representation onto a representation of a target sentence with m target\nwords (where m is also predicted by the network according to the Poisson distribution). The pictorial\nrepresentation of two models is shown in ﬁgure 3.\nFigure 3: A graphical depiction of the two RCTMs. Arrows represent full matrix transformations\nwhile lines are vector transformations corresponding to columns of weight matrices.\nThe experimentation is performed on a bilingual corpus of 144953 pairs of sentences less than 80\nwords in length from the news commentary section of the Eighth Workshop on Machine Translation\n(WMT) 2013 training data. The source language is English and the target language is French. A low\nperplexity value achieved by RCTMs on test set as compared to IBM models (model 1-4) suggests\nthat continuous representations and the transformations between them make up well for the lack of\nexplicit alignments. To make sure that RCTM architecture (with CGM) doesn’t just take bag-of-words\napproach, they change the ordering of the words in the source sentence and train their model. This\nmodel achieves much lower perplexity values which proves that the model is indeed sensitive to\nsource sentence structure. They also compare the performance of the RCTM model with cdec system.\ncdec employs 12 engineered features including, among others, 5 translation models, 2 language\nmodel features and a word penalty feature (WP). RCTM models achieve comparable performance\n(marginally better) than the cded system on BLEU score. The results indicate that the RCTMs are\nable to learn both translation and language modelling distributions without explicitly modelling them.\nCho et al. [9] propose a RNN encoder-decoder architecture very similar to the one proposed above\nbut with one major difference. While Kalchbrenner et al. [25] use CNN to map a source sentence\ninto a ﬁxed-sized continuous representation, Cho et al. [9] use an encoder RNN to map source\nsequence into a vector. However, they use this architecture to learn phrase translation probabilities.\nThe training is done on phrase translation pairs extracted in the phrase-based translation system. The\nmodel re-scores all the phrase-pairs probabilities which are used as additional features in log-linear\nphrase based translation system. They use WMT’14 translation task to build English/French SMT\nsystem coupled with features from encoder-decoder model. With quantitative analysis of the system\n(on BLEU score), they show that baseline SMT system’s performance was improved when RLM\nwas used. Additionally, adding features from proposed Encoder-Decoder architecture increased the\nperformance further suggesting that signals from multiple neural systems indeed add up and are\nnot redundant. They later perform qualitative analysis of their proposed model to investigate the\nquality of the target phrases generated by model. The target phrases (given a source phrase) proposed\nby model look more visually appealing than the top target phrases from translation table. They\nalso plot the phrase representations (after dimensionality reduction) on 2-d plane and show that the\nsyntactically and semantically similar phrases are clustered together.\nWhile Cho et al. [9] proposed an end-to-end RNN architecture, they use it only to get additional\nphrase translation table to be eventually used in the SMT based system. Sutskever et al. [43] gave a\nmore formal introduction to the sequence-to-sequence RNN encoder-decoder architecture. Though\ntheir motivation was to investigate the ability of very-deep neural networks at solving seq-to-seq\n9\nproblem, they run their experiments on machine translation task to achieve their goals. They proposed\nthe architecture very similar to Cho et al. [9] with three major architecture changes: 1) they used\nLSTM cells in encoder and decoder RNN, 2) they trained their system on complete sentence pairs\nand not just phrases, 3) they used stacked LSTMs (with 4-6 layers) in both decoder and encoder.\nFinally, they also reverse the source sentences in the training data and train their system on reversed\nsource sentences (keeping the target language sentences in their original order). They don’t provide a\nclear motivation of why they did so, but informally, reversing the source sentence helps in capturing\nlocal dependencies around the word from either direction. Their experimentation results (on WMT-14\nEnglish/French MT dataset) show that reversing the source sentence achieves higher BLEU score on\ntest set than the model where no reversing was done. Though their model doesn’t beat the state-of-\nthe-art MT system, it achieves performance very close to the latter. Their model doesn’t employ any\nattention methods or bi-directional RNN (which is used by the state-of-the-art system). This suggests\nthat deep models indeed help in seq-to-seq learning with RNN encoder-decoder architecture.\nNeural machine translation has shown very promising results for many language pairs. Despite that,\nit has only been applied to only formal texts like WMT shared task. Luong et al. [28] study the\neffectiveness of NMT systems in spoken language domains by using IWSLT 2015 dataset. They\nexplore two scenarios: NMT adaptation and NMT for low resource translation. For NMT adaptation\ntask, they take an existing state-of-the-art English-German system[29], which consists of 8 individual\nmodels trained on WMT data with mostly formal texts (4.5M sentence pairs. They further train on\nthe English-German spoken language data provided by IWSLT 2015 (200K sentence pairs). They\nshow that NMT adaptation is very effective: models trained on a large amount of data in one domain\ncan be ﬁnetuned on a small amount of data in another domain. This boosts the performance of an\nEnglish-German NMT system by 3.8 BLEU points. For NMT low resource translation task, they use\nthe provided English-Vietnamese parallel data (133K sentence pairs). At such a small scale of data,\nthey could not train deep LSTMs with 4 layers as in the English-German case. Instead, they opt for\n2-layer LSTM models with 500-dimensional embeddings and LSTM cells. Though their system is\nlittle behind the IWSLT baseline (baseline’s BLEU score is 27.0 and their model’s BLEU score is\n26.4), it still shows that NMT systems are quite effective in other domains too, and not just formal\ntexts.\n4.2.1\nAttention Mechanisms\nA potential issue with this encoder–decoder approach is that a neural network needs to be able to\ncompress all the necessary information of a source sentence into a ﬁxed-length vector. This may\nmake it difﬁcult for the neural network to cope with long sentences, especially those that are longer\nthan the sentences in the training corpus. Cho et al. [8] showed that indeed the performance of a\nbasic encoder–decoder deteriorates rapidly as the length of an input sentence increases. Bahdanau et\nal. [2] proposed an attention mechanism to deal with this issue. They propose a model where the\nsource sentence is not encoded into one ﬁxed length vector. Instead, the encoder maps the source\nsentence into sequence of vectors and decoder chooses a subset of these vectors at each time step to\ngenerate tokens in the target language. We now discuss this model more formally. In the proposed\narchitecture, the conditional probability is deﬁned as:\np(yi|y1, ...yi−1, x) = g(yi−1, si, ci)\n(16)\nwhere si is an RNN hidden state for time i, computed by:\nsi = f(si−1, yi−1, ci)\n(17)\nThe context vector ci depends on a sequence of annotations (h1, ...hTx) to which an encoder maps the\ninput sentence. The context vector ci is, then, computed as a weighted sum of these annotations hi:\nci =\nTx\nX\nj=1\nαijhj\n(18)\nThe weight αij of each annotation is computed by\nαij =\nexp(eij)\nPTx\nk=1 exp(eik)\n(19)\nwhere eij = a(si−1, hj) is an alignment model, implemented with feed-forward neural network.\nThe alignment scores how well the inputs around position j and output at position i match. We can\n10\nunderstand the approach of taking a weighted sum of all the annotations as computing an expected\nannotation, where the expectation is over possible alignments. They use bidirectional RNN for\nencoder and a unidirectional RNN for decoder, and use the bilingual, parallel corpora provided\nby ACL WMT ’14 task (English/French). With their experiments on source sentences of different\nlengths, they show that the performance of the conventional encoder-decoder drops quickly when the\nsentence length increases beyond 30. On the other hand, the proposed model remains less volatile to\nthe sentence length and continues to achieve good performance on long sentences too. They also\nplot alignment visualizations for each target word produced by decoder. The visualizations show that\nEnglish and French languages are highly monotonic, which indeed is the case with these languages.\nLuong et al. [29] propose two attention approaches: a global approach which always attends to all\nsource words and a local one that only looks at a subset of source words at a time. The global approach\nis very similar to the one proposed by Bahdanau et al. [2] but is architecturally simpler than the latter.\nThe local attention can be viewed as a blend between soft and hard alignment approaches proposed\nby Xu et al. [47]. The local attention model is computationally less expensive and differentiable,\nmaking it easier to implement and train. We don’t discuss the global attention model here as it is\nvery similar to the one proposed by Bahdanau et al. [2], and direct the reader to the original paper\nfor details and comparisons. We focus our discussions on local attention model which is the major\ncontribution of this work. In local attention, the model ﬁrst generates the aligned position pt for each\ntarget word at time t. The context vector ct is then derived as a weighted average over the set of\nsource hidden states within the window [pt−D, pt+D];D is empirically selected. Unlike the global\napproach, the local alignment vector at is now ﬁxed-dimensional, i.e., ∈R2D+1. The model predicts\nthe aligned position pt as follows:\npt = S.sigmoid(vT\np tanh(Wpht))\n(20)\nWp and vp are the model parameters which will be learned to predict positions. S is the source\nsentence length. To favor alignment points near pt, Gaussian distribution is used and centered around\npt. The alignment weights are now deﬁned as:\nat(s) = align(ht, ¯hs) exp\n\u0012\n−(s −pt)2\n2σ2\n\u0013\n(21)\nThe align function can be as simple as a dot product or can be learned with feed-forward neural net.\nThe standard deviation is empirically set as σ = D\n2 and s is an integer within the window centered\naround pt. They evaluate the effectiveness of the model on the WMT translation tasks between\nEnglish and German in both directions, and use WMT’14 training data, and newstest2014 (2737\nsentences) and newstest2015 (2169 sentences) as their test data. Apart from achieving higher BLEU\nscores (even on longer sentences) than the baseline system NMT system (without attention) and\nother conventional SMT approaches, they also visualize the quality of the alignments produced by\nthe model during decoding. After learning, they extract only one-to-one alignments by selecting the\nsource word with the highest alignment weight per target word and compare it with gold alignment\ndata provided by RWTH for 508 English-German Europarl sentences. They use the alignment error\nrate (AER) to measure the alignment quality of the model. The results show that they were able to\nachieve AER scores comparable to the one-to-many alignments obtained by the Berkeley aligner\n(Liang et al. [27]).\n4.2.2\nAddressing OOV\nA signiﬁcant weakness in conventional NMT systems is their inability to correctly translate very\nrare words: end-to-end NMTs tend to have relatively small vocabularies with a single symbol that\nrepresents unk every possible out-of-vocabulary (OOV) word. Standard phrase-based systems, on the\nother hand, do not suffer from this problem to same extent as NMTs as they make use of explicit\nalignments and phrase tables which allows them to memorize the translations of extremely rare\nwords.\nJean et al. [23] propose a method based on importance sampling that allows them to use large target\nlanguage vocabulary without increasing training complexity. They divide the training set into multiple\nindividual sets, each having its own target vocabulary V’. More concretely, before training begins,\neach target sentence is sequentially examined and unique words are accumulated till number of unique\nwords reach predeﬁned threshold τ. The accumulated vocabulary will be used for this partition of\nthe corpus during training. The process is repeated until the end of the training set is reached. An\n11\nintrigued reader can refer the original paper for more formal description of the model and the training\nprocedure. The proposed approach is evaluated on English to French and English to German task.\nBilingual parallel corpora from WMT’14 is used for training the model. Apart from showing the\nefﬁciency of the proposed model through comparable BLEU scores with the state-of-the-art WMT’14\nsubmitted model, they propose heuristic-based changes to the traditional NMT decoder to make it\nsample efﬁciently from extremely large target vocabulary.\nYet another method was proposed recently to address the OOV problem. Luong et al. [30] train an\nNMT system on data that is augmented by the output of a word alignment algorithm, allowing the\nNMT system to emit, for each OOV word in the target sentence, the position of its corresponding\nword in the source sentence. This information is later utilized in a post-processing step that translates\nevery OOV word using a dictionary. Experiments on the WMT’14 English to French translation task\nshow that this method provides improvement of up to 2.8 BLEU points over an equivalent NMT\nsystem that does not use this technique.\n5\nCurrent Research\nKaiser et al. [24] recently proposed an interesting neural network architecture - ‘One Model to learn\nthem all’. Its a Multi-Model architecture that can simultaneously learn many tasks across domains.\nAt its core, it has four components - modality nets, encoder, IO mixer and decoder. Modality nets\n(one each for all types of data - text, speech, audio, image) map input into a representation. Encoder\ntakes this representation and processes it with attention blocks and mixture-of-experts[39]. Decoder,\nin a similar fashion, produces output representation which is given to the respective modality net to\nproduce the output. Both encoder and decoder are built with convolutional blocks. Their experiments\non various tasks (including machine translation) show that the model performs, if not at par yet,\nbut close to the state-of-the-art systems on individual tasks. They also show that attention and\nmixture-of-experts blocks, designed for textual data (especially machine translation) doesn’t hurt the\nperformance of other completed unrelated tasks like classiﬁcation on ImageNet[13].\nAnother area of current research is related to the requirement of a large parallel corpus to train\nNMT systems. The lack of such corpus for low-resource languages (e.g. Basque) as well as for\ncombinations of major languages (e.g. German-Russian) poses a challenge for such systems. Artetxe\net al. [1] propose an unsupervised approach to neural machine translation which relies solely on a\nmonolingual corpus. The system architecture is a standard encoder-decoder setup with the encoder\nshared across the two decoders along with attention. The encoder contains pre-trained cross-lingual\nword embeddings which are kept ﬁxed during training. Ideally, this architecture can be trained to\nencode a given sentence using the shared encoder and decode it using the appropriate decoder, but it\nis prone to learn trivial copying task. To circumvent this, the authors propose using denoising and\non-the-ﬂy backtranslation. Denoising randomizes the order of the words to force the network to\nlearn meaningful information about the language, and the on-the-ﬂy backtranslation translates text\nfrom the available monolingual corpus to the other language to get a pseudo-parallel sentence pair.\nThis architecture improves over the baseline scores by at least 40% on both German-English and\nFrench-English translation. This unsupervised approach is also shown to improve with the availability\nof small parallel corpus.\n6\nConclusion\nMachine translation has been an active area of research within the ﬁeld of AI for many years.\nStatistical machine translation, with the advent of IBM models (model 1-4) paved way for advanced\napproaches based on phrase-based and syntax-based models. These methods have shown tremendous\nprogress in many language pairs and have been successfully deployed in large scale systems, like\nGoogle translate (up until 2014). Over the past couple of years, Neural Machine Translation has\ntaken the front seat in this task. Owing to their ease of learning, their ability to model complex\nfeature functions and their striking performance in translating major languages of the world, NMT\nsystems have become natural choice for researchers to study their behavior, the feature space they\nlearn, and the effect of variations in architectures. Despite that, much needs to be done both from\nmodelling perspective and architecture changes. We believe that a uniﬁed architecture similar to\nthe one proposed in ‘One model to learn them all’ holds potential in beneﬁting from multiple tasks\nlearnt simultaneously. Finally, we are also seeing unsupervised methods being applied to learn MT\n12\nsystems from just one language. This research area is especially important considering the number of\nlanguages in the world and the limited amount of labelled data available for them.\n13\nBibliography\n[1] Mikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. Unsupervised neural\nmachine translation. arXiv preprint arXiv:1710.11041, 2017.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint arXiv:1409.0473, 2014.\n[3] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic\nlanguage model. Journal of machine learning research, 3(Feb):1137–1155, 2003.\n[4] Peter Brown, John Cocke, S Della Pietra, V Della Pietra, Frederick Jelinek, Robert Mercer,\nand Paul Roossin. A statistical approach to language translation. In Proceedings of the 12th\nconference on Computational linguistics-Volume 1, pages 71–76. Association for Computational\nLinguistics, 1988.\n[5] Peter F Brown, John Cocke, Stephen A Della Pietra, Vincent J Della Pietra, Fredrick Jelinek,\nJohn D Lafferty, Robert L Mercer, and Paul S Roossin. A statistical approach to machine\ntranslation. Computational linguistics, 16(2):79–85, 1990.\n[6] Peter F Brown, Vincent J Della Pietra, Stephen A Della Pietra, and Robert L Mercer. The\nmathematics of statistical machine translation: Parameter estimation. Computational linguistics,\n19(2):263–311, 1993.\n[7] Jaime G Carbonell, Richard E Cullinford, and Anatole V Gershman. Knowledge-based machine\ntranslation. Technical report, Yale University, Department of Computer Science, 1978.\n[8] KyungHyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the\nproperties of neural machine translation: Encoder-decoder approaches. CoRR, abs/1409.1259,\n2014. URL http://arxiv.org/abs/1409.1259.\n[9] Kyunghyun Cho, Bart van Merriënboer, Ça˘glar Gülçehre, Dzmitry Bahdanau, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder–\ndecoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), pages 1724–1734, Doha, Qatar, Oc-\ntober 2014. Association for Computational Linguistics. URL http://www.aclweb.org/\nanthology/D14-1179.\n[10] Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: Deep\nneural networks with multitask learning. In Proceedings of the 25th International Conference\non Machine Learning, ICML ’08, pages 160–167, New York, NY, USA, 2008. ACM. ISBN\n978-1-60558-205-4. doi: 10.1145/1390156.1390177. URL http://doi.acm.org/10.1145/\n1390156.1390177.\n[11] G. E. Dahl, Dong Yu, Li Deng, and A. Acero. Context-dependent pre-trained deep neural\nnetworks for large-vocabulary speech recognition. Trans. Audio, Speech and Lang. Proc.,\n20(1):30–42, January 2012. ISSN 1558-7916. doi: 10.1109/TASL.2011.2134090. URL\nhttp://dx.doi.org/10.1109/TASL.2011.2134090.\n[12] J. N. Darroch and D. Ratcliff. Generalized iterative scaling for log-linear models. In The Annals\nof Mathematical Statistics, volume 43, 1972.\n[13] Jia Deng, Wei Dong, Richard Socher, Li jia Li, Kai Li, and Li Fei-fei. Imagenet: A large-scale\nhierarchical image database. In In CVPR, 2009.\n[14] Ulrich Germann. Greedy decoding for statistical machine translation in almost linear time.\nIn Proceedings of the 2003 Conference of the North American Chapter of the Association for\nComputational Linguistics on Human Language Technology-Volume 1, pages 1–8. Association\nfor Computational Linguistics, 2003.\n[15] Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu, and Kenji Yamada. Fast and\noptimal decoding for machine translation. Artiﬁcial Intelligence, 154(1-2):127–143, 2004.\n14\n[16] Joshua T Goodman. A bit of progress in language modeling. Computer Speech & Language,\n15(4):403–434, 2001.\n[17] Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen Clark, Bob Coecke, and Stephen Pul-\nman. Concrete sentence spaces for compositional distributional models of meaning. CoRR,\nabs/1101.0309, 2011. URL http://arxiv.org/abs/1101.0309.\n[18] Hany Hassan, Anthony Aue, Chang Chen, Vishal Chowdhary, Jonathan Clark, Christian Feder-\nmann, Xuedong Huang, Marcin Junczys-Dowmunt, William Lewis, Mu Li, et al. Achieving\nhuman parity on automatic chinese to english news translation. arXiv preprint arXiv:1803.05567,\n2018.\n[19] Karl Moritz Hermann and Phil Blunsom. A simple model for learning multilingual composi-\ntional semantics. CoRR, abs/1312.6173, 2013. URL http://arxiv.org/abs/1312.6173.\n[20] Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel rahman Mohamed, Navdeep Jaitly,\nAndrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, and Brian Kingsbury. Deep\nneural networks for acoustic modeling in speech recognition. Signal Processing Magazine,\n2012.\n[21] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Comput., 9\n(8):1735–1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL\nhttp://dx.doi.org/10.1162/neco.1997.9.8.1735.\n[22] John Hutchins and Evgenii Lovtskii. Petr petrovich troyanskii (1894–1950): A forgotten pioneer\nof mechanical translation. Machine translation, 15(3):187–221, 2000.\n[23] Sébastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. On using very\nlarge target vocabulary for neural machine translation. CoRR, abs/1412.2007, 2014. URL\nhttp://arxiv.org/abs/1412.2007.\n[24] Lukasz Kaiser, Aidan N. Gomez, Noam Shazeer, Ashish Vaswani, Niki Parmar, Llion Jones,\nand Jakob Uszkoreit. One model to learn them all. CoRR, abs/1706.05137, 2017. URL\nhttp://arxiv.org/abs/1706.05137.\n[25] Nal Kalchbrenner and Phil Blunsom. Recurrent continuous translation models. Seattle, October\n2013. Association for Computational Linguistics.\n[26] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep\nconvolutional neural networks. In Proceedings of the 25th International Conference on Neural\nInformation Processing Systems - Volume 1, NIPS’12, pages 1097–1105, USA, 2012. Curran\nAssociates Inc. URL http://dl.acm.org/citation.cfm?id=2999134.2999257.\n[27] Percy Liang, Ben Taskar, and Dan Klein. Alignment by agreement. In Proceedings of the Main\nConference on Human Language Technology Conference of the North American Chapter of the\nAssociation of Computational Linguistics, HLT-NAACL ’06, pages 104–111, Stroudsburg, PA,\nUSA, 2006. Association for Computational Linguistics. doi: 10.3115/1220835.1220849. URL\nhttps://doi.org/10.3115/1220835.1220849.\n[28] Minh-Thang Luong and Christopher D. Manning. Stanford neural machine translation systems\nfor spoken language domains. 2015.\n[29] Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-\nbased neural machine translation. CoRR, abs/1508.04025, 2015. URL http://arxiv.org/\nabs/1508.04025.\n[30] Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Wojciech Zaremba. Addressing\nthe rare word problem in neural machine translation. CoRR, abs/1410.8206, 2014. URL\nhttp://arxiv.org/abs/1410.8206.\n[31] Daniel Marcu and William Wong. A phrase-based, joint probability model for statistical\nmachine translation. In Proceedings of the ACL-02 Conference on Empirical Methods in\nNatural Language Processing - Volume 10, EMNLP ’02, pages 133–139, Stroudsburg, PA,\nUSA, 2002. Association for Computational Linguistics. doi: 10.3115/1118693.1118711. URL\nhttps://doi.org/10.3115/1118693.1118711.\n15\n[32] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed\nrepresentations of words and phrases and their compositionality. CoRR, abs/1310.4546, 2013.\nURL http://arxiv.org/abs/1310.4546.\n[33] Makoto Nagao. A framework of a mechanical translation between japanese and english by\nanalogy principle. Artiﬁcial and human intelligence, pages 351–354, 1984.\n[34] Franz Josef Och. Statistical machine translation: from single-word models to alignment\ntemplates. PhD thesis, Bibliothek der RWTH Aachen, 2002.\n[35] Franz Josef Och and Hermann Ney. Improved statistical alignment models. In Proceedings\nof the 38th Annual Meeting on Association for Computational Linguistics, pages 440–447.\nAssociation for Computational Linguistics, 2000.\n[36] Franz Josef Och and Hermann Ney. Discriminative training and maximum entropy models for\nstatistical machine translation. In Proceedings of the 40th Annual Meeting on Association for\nComputational Linguistics, ACL ’02, pages 295–302, Stroudsburg, PA, USA, 2002. Association\nfor Computational Linguistics. doi: 10.3115/1073083.1073133. URL https://doi.org/10.\n3115/1073083.1073133.\n[37] Ronald Rosenfeld. Two decades of statistical language modeling: Where do we go from here?\nProceedings of the IEEE, 88(8):1270–1278, 2000.\n[38] Holger Schwenk. Continuous space translation models for phrase-based statistical machine\ntranslation. In COLING, 2012.\n[39] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E.\nHinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-\nexperts layer. CoRR, abs/1701.06538, 2017. URL http://arxiv.org/abs/1701.06538.\n[40] Richard Socher, Christopher D. Manning, and Andrew Y. Ng. Learning continuous phrase\nrepresentations and syntactic parsing with recursive neural networks. In In Proceedings of the\nNIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop, 2010.\n[41] Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y. Ng, and Christopher D. Manning.\nDynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Proceedings\nof the 24th International Conference on Neural Information Processing Systems, NIPS’11,\npages 801–809, USA, 2011. Curran Associates Inc. ISBN 978-1-61839-599-3. URL http:\n//dl.acm.org/citation.cfm?id=2986459.2986549.\n[42] Ilya Sutskever, James Martens, and Geoffrey Hinton. Generating text with recurrent neural\nnetworks. In Proceedings of the 28th International Conference on International Conference on\nMachine Learning, ICML’11, pages 1017–1024, USA, 2011. Omnipress. ISBN 978-1-4503-\n0619-5. URL http://dl.acm.org/citation.cfm?id=3104482.3104610.\n[43] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural net-\nworks. In Proceedings of the 27th International Conference on Neural Information Processing\nSystems - Volume 2, NIPS’14, pages 3104–3112, Cambridge, MA, USA, 2014. MIT Press. URL\nhttp://dl.acm.org/citation.cfm?id=2969033.2969173.\n[44] Stephan Vogel, Hermann Ney, and Christoph Tillmann. Hmm-based word alignment in statistical\ntranslation. In Proceedings of the 16th conference on Computational linguistics-Volume 2,\npages 836–841. Association for Computational Linguistics, 1996.\n[45] Ye-Yi Wang and Alex Waibel. Decoding algorithm in statistical machine translation. In\nProceedings of the eighth conference on European chapter of the Association for Computational\nLinguistics, pages 366–372. Association for Computational Linguistics, 1997.\n[46] Warren Weaver. Translation. Machine translation of languages, 14:15–23, 1955.\n[47] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov,\nRichard S. Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation\nwith visual attention. CoRR, abs/1502.03044, 2015. URL http://arxiv.org/abs/1502.\n03044.\n16\n[48] Kenji Yamada and Kevin Knight. A syntax-based statistical translation model. In Proceedings\nof the 39th Annual Meeting on Association for Computational Linguistics, pages 523–530.\nAssociation for Computational Linguistics, 2001.\n17\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2018-12-28",
  "updated": "2018-12-28"
}