{
  "id": "http://arxiv.org/abs/1901.01492v1",
  "title": "What Should I Do Now? Marrying Reinforcement Learning and Symbolic Planning",
  "authors": [
    "Daniel Gordon",
    "Dieter Fox",
    "Ali Farhadi"
  ],
  "abstract": "Long-term planning poses a major difficulty to many reinforcement learning\nalgorithms. This problem becomes even more pronounced in dynamic visual\nenvironments. In this work we propose Hierarchical Planning and Reinforcement\nLearning (HIP-RL), a method for merging the benefits and capabilities of\nSymbolic Planning with the learning abilities of Deep Reinforcement Learning.\nWe apply HIPRL to the complex visual tasks of interactive question answering\nand visual semantic planning and achieve state-of-the-art results on three\nchallenging datasets all while taking fewer steps at test time and training in\nfewer iterations. Sample results can be found at youtu.be/0TtWJ_0mPfI",
  "text": "What Should I Do Now?\nMarrying Reinforcement Learning and Symbolic Planning\nDaniel Gordon1\nDieter Fox1,2\nAli Farhadi1,3\n1Paul G. Allen School of Computer Science, University of Washington\n2Nvidia\n3Allen Institute for ArtiÔ¨Åcial Intelligence\nAbstract\nLong-term planning poses a major difÔ¨Åculty to many re-\ninforcement learning algorithms.\nThis problem becomes\neven more pronounced in dynamic visual environments. In\nthis work we propose Hierarchical Planning and Reinforce-\nment Learning (HIP-RL), a method for merging the bene-\nÔ¨Åts and capabilities of Symbolic Planning with the learning\nabilities of Deep Reinforcement Learning. We apply HIP-\nRL to the complex visual tasks of interactive question an-\nswering and visual semantic planning and achieve state-\nof-the-art results on three challenging datasets all while\ntaking fewer steps at test time and training in fewer itera-\ntions. Sample results can be found at youtu.be/0TtWJ_\n0mPfI1\n1. Introduction\nAn important goal in developing systems with visual un-\nderstanding is to create agents that interact intelligently with\nthe world. Teaching these agents about the world requires\nseveral steps. An agent must initially learn simple behav-\niors such as navigation and object affordances. Then, it can\ncombine several actions together to accomplish longer term\ngoals. As the task complexity increases, the agent must plan\nfarther in the future.\nIn recent years, researchers have predominantly trained\ninteractive agents using either deep learning techniques on\nraw visual data or using planning algorithms on symbolic\nstate representations. Deep learning has proven to be a very\nuseful tool at learning to extract meaningful features from\nlarge sources of raw data. Deep Reinforcement Learning\n(Deep RL) has gained signiÔ¨Åcant traction in the vision com-\nmunity for simple tasks like playing video games [26, 27].\nYet as task complexity increases, and longer-term planning\nis required, these systems can no longer learn good reactive\npolicies due to the exponentially branching state space.\nConversely, many robotics systems still favor planning\nand search techniques such as RRTs and A* over the re-\nactive Deep RL counterparts [9, 29]. The traditional plan-\nning algorithms offer better generalization when sensor data\n1The full dataset and code will be open sourced soon.\nFigure 1: Sample Visual Semantic Planning task execution.\nThe agent is asked to put a bowl in the microwave. At t=0,\nHIP-RL has not observed any locations where bowls can be,\nso it explores the room. At t=1, the Meta-Controller invokes\nthe Planner which creates an efÔ¨Åcient plan to check all the\ncabinets. It also sees the microwave and saves this informa-\ntion for later. At t=2 a bowl is found, and the Planner up-\ndates its plan to Ô¨Ånish the task. Since HIP-RL already saw\nthe microwave, it saves time by not needing an additional\nsearch. At t=3 the Planner puts the bowl in the microwave,\nreturning control to the Meta-Controller which Ô¨Ånishes the\nepisode. In contrast, the Pure RL system spends much more\ntime exploring the room, does not open any drawers, and\nends the episode after many more steps, failing the task.\nis clean, and many provide optimality guarantees which\nare beneÔ¨Åcial for ensuring safety in potentially dangerous\nrobotics environments. However most planning algorithms\n1\narXiv:1901.01492v1  [cs.CV]  6 Jan 2019\nassume either perfect or fairly accurate state estimation and\ncannot operate on high dimensional raw sensor input. This\nis especially true for task planning algorithms which use bi-\nnary values as state representations (e.g. the apple is in the\nfridge) like Planning Domain DeÔ¨Ånition Language (PDDL)\nsolvers.\nAs such, deep learning and task planning have comple-\nmentary beneÔ¨Åts and drawbacks: deep learning techniques\ncan extract information from raw (pixel) data but fail at\nlong-term planning, and task planning require preprocessed\ndata but can use it to construct multi-step plans which sat-\nisfy complex goals.\nMuch of the recent work in this area has treated learning\nand planning as separate problems. We present Hierarchical\nPlanning and Reinforcement Learning (HIP-RL), a method\nfor merging these techniques to beneÔ¨Åt from the strengths of\nboth. HIP-RL has several advantages over traditional plan-\nning as well as over current Deep RL techniques. 1) Due\nto the correctness/completeness guarantees of the planner,\nHIP-RL increases the accuracy and effectiveness over com-\nparable pure RL systems. 2) By relying on a planner to\ncreate sequences of actions, HIP-RL signiÔ¨Åcantly reduces\nthe number of steps that an agent takes at test time. 3) By\nsimplifying the learning procedure and shortening the path\nlengths, we are able to train our algorithm using an order\nof magnitude fewer training examples. 4) HIP-RL can also\nlearn to account for noisy sensor data which may otherwise\nhinder a symbolic planner.\nTo evaluate the usefulness and effectiveness of our al-\ngorithm, we apply HIP-RL to a variety of tasks.\nFirst,\nwe apply HIP-RL to the task of Interactive Question An-\nswering (IQA), an extension of Visual Question Answering\n(VQA) where an agent dynamically navigates in and inter-\nacts with an environment in order to answer questions. We\napply HIP-RL to IQUAD V1 [13] and EQA V1 [11] and\nachieve state-of-the-art results on both tasks. We addition-\nally show that HIP-RL is able to perform complex Visual\nSemantic Planning (VSP) tasks such as Find a bowl\nand put it in the microwave, dramatically out-\nperforming both learning-only and planning-only baselines.\nIn general, we Ô¨Ånd that using planning and learning together\nresults in higher accuracy, more efÔ¨Åcient exploration, and\nfaster training than other methods.\n2. Related Work\n2.1. Planning and Learning\nAlthough both learning and planning have signiÔ¨Åcant\namounts of prior work, there have been relatively few at-\ntempts at merging the two.\nMany recent reinforcement-\nlearning based algorithms fail when long-term planning is\nrequired; most algorithms trained on ATARI fail on the\nMontezuma‚Äôs Revenge game due to its sparse rewards and\nlong episodes [26]. Yet when planning and learning are\ncombined, the results are often greater than either could do\nalone. One example of successfully merging planning and\nlearning is the AlphaZero family of algorithms which com-\nbine Deep RL for board state evaluation and Monte Carlo\nTree Search (MCTS) for planning and Ô¨Ånding high-value\nfuture board positions [28]. Rather than using MCTS to\nintelligently explore future states, which is not feasible in\na partially observed visually dynamic environment, we use\nthe Metric-FF Planner [16] to plan a single trajectory to the\ngoal state. This chains actions together in order to shorten\nthe number of hierarchical decisions and reduce the size\nof the action space. Planning in stochastic environments\nis often solved using Partially Observable Markov Deci-\nsion Processes. Although they are frequently used to great\nsuccess [1, 6], POMDPs often assume a known noise and\ntransition model, which is not readily applicable for algo-\nrithms which use deep feature extraction. We avoid this is-\nsue by using both planning and learning; although our plan-\nner operates under the assumption of perfect and complete\ninformation, the Meta-Controller can divert control to the\nlearning-based methods in the event of a planner failure or\nto gather more information.\n2.2. Hierarchical Reinforcement Learning (HRL)\nHRL seeks to solve several problems with standard re-\ninforcement learning such as handling very long episodes\nwith sparse rewards.\nThe design of these systems typi-\ncally has one hierarchical meta-controller which invokes\none of several low-level controllers. Each low-level con-\ntroller is trained to accomplish a simpler task.\nIn many\ncases [11, 12, 13, 22, 30] both the meta-controller and all\nlow-level controllers are learned, and in some cases [22, 30]\nthe tasks of the sub-controllers are also learned purely\nfrom interactions during training episodes rather than being\nhuman-engineered. This allows these systems to generalize\nwell to unseen tasks with only a few training examples for\nthe new tasks. In contrast, we use some learned low-level\ncontrollers, and some which use planning algorithms to di-\nrectly solve subtasks. This allows our system to train qickly\nand still generalize well to new tasks with only moderate\nadditional goal-state speciÔ¨Åcation.\n2.3. Deep RL in Virtual Visual Environments\nIn the past few years, many different virtual platforms\nhave been created in order to facilitate better Deep RL.\nVirtual environments provide limitless labeled data and are\neasily parallelizable. Some of the most popular are Ope-\nnAI Gym [5] and VizDoom [19] which both build on ex-\nisting video games, and MuJoCo [32] which implements\nmore realistic contact physics.\nMore recently, multiple\nenvironments have been created which offer near-photo-\nrealistic and physically accurate interaction such as AI2-\nTHOR [20], Gibson [34], and CHALET [35]. Other envi-\nronments forgo photo-realism for increased rendering speed\nsuch as House3D [33], and DeepMind Lab [4].\nAdditionally, there have been many advancements in\nlearning from interactions with a virtual visual environ-\nment.\nRecent works have used virtual environments for\nQuestion Answering [11, 12, 13], visually-driven naviga-\ntion [14, 25, 37], and semantic navigation [3, 7]. How-\never, much of the focus has been on improving navigation\ntechniques in these environments. We are interested in ex-\npanding beyond navigation to more complex visual tasks\nwhich require both navigation and long term planning. In\nthis work, we use the existing IQUAD V12 dataset pre-\nsented in [13] which is built on the AI2-THOR [20] envi-\nronment, and EQA V13 [11] which uses the House3D envi-\nronment [33]. Additionally, we construct a new dataset for\nVisual Semantic Planning built on AI2-THOR, explained\nmore in section 4.2.\n2.4. Task and Motion Planning (TaMP)\nTask and motion planning is the problem of accomplish-\ning goals at a high level by planning the exact motion trajec-\ntories for a robot. Much of the work in TaMP uses hierarchi-\ncal algorithms to plan high level actions for long-term goals\nand use motion planning algorithms for Ô¨Åne-grained motor\nmanipulation. [18] outlines a detailed hierarchy for plan-\nning and executing simple manipulation tasks. [21] attempt\nto learn symbolic representations by interacting with an en-\nvironment and observing the effects of actions. [8] learn\nheuristics to shorten the search procedure for their TaMP\nalgorithm.\nWe differ from these approaches in that we\nassume perfect manipulators, which simpliÔ¨Åes our control\nsetup, but use pixel-level inputs from near-photo-realistic\n3D simulation environments. We also do not assume com-\nplete state information is given to the controller. Finally, we\nuse reinforcement learning to direct our hierarchical con-\ntroller, whereas most methods treat the entire state as fully\nobservable and therefore plan and execute complete motion\ntrajectories from the initial state.\n3. Method\nIn order to accomplish complex tasks, a system must be\nable to plan long action trajectories which satisfy the task\ngoals. To operate in a visual world, a system must learn to\nunderstand a dynamic visual environment. Thus, to learn\nto plan in a visual environment, we combine Deep RL with\nSymbolic Planning, handing control of the agent back and\nforth between the two methods. In this section we outline\nthe individual components of HIP-RL as well as how they\nwork together to solve complex learning and planning tasks.\n3.1. Hierarchical Planning and Reinforcement\nLearning (HIP-RL)\nHIP-RL consists of a hierarchical Meta-Controller, sev-\neral direct (low-level) controllers, and a shared knowledge\nstate (Figure 2). The knowledge state contains all percep-\ntual and interactive information such as navigable locations,\nobject positions, and which cabinets have previously been\n2https://github.com/danielgordon10/thor-iqa-cvpr-2018\n3https://github.com/facebookresearch/EmbodiedQA\nMeta-Controller\nDirect Controllers\n(Planner, Explorer, Stopper, Etc.)\nEnvironment\nMap/State Knowledge\nGoal/Task \nEmbedding\nFigure 2: Diagram of the HIP-RL framework. Each direct\ncontroller interacts with the environment based off of com-\nmands given by the Meta-Controller. All controllers share a\nknowledge state which is updated by the various controllers\nduring the episode based on perceptual and interactive ob-\nservations.\nopened, as well as the goal representation. For example, in\nFigure 1 the knowledge state in image 1 contains the posi-\ntions of the drawers, the microwave, the red plate, and the\nfact that none of the drawers have been checked. Each con-\ntroller can read all of the state knowledge, and can update a\nportion of the knowledge based on its perception; the Navi-\ngator can update the world map, and the Object Detector can\nupdate the object locations but not vice versa. At the start of\nan episode, the Meta-Controller chooses which direct con-\ntroller should be used to advance the current state towards\nthe goal state, invoking that direct controller with a subtask.\nThe direct controller attempts to accomplish this subtask\nand returns control back to the Meta-Controller upon com-\npletion. This process is repeated until the Meta-Controller\ndecides the full task has been accomplished and calls the\nStopper to end the episode.\n3.2. Hierarchical Meta-Controller\nThe Meta-Controller receives the goal and decides which\nof the direct controllers to invoke. It learns to trade off be-\ntween the length of an episode and the reward/penalty re-\nceived for successfully or unsuccessfully ending an episode.\nWe train this behavior using the A3C algorithm [26] to re-\nward successful episodes, penalize unsuccessful episodes,\nand add a time penalty for each hierarchical step. We vi-\nsualize the Meta-Controller‚Äôs architecture in the context of\nInteractive Question Answering in Figure 3. It takes as in-\nput a top-down spatial map of object locations, the ques-\ntion representation, and the current image from the envi-\nDetected Object Map\nCurrent Image\nQuestion\nSpatial Attention Map\n*\nùö∫\nùö∫\nP(Explore)\nP(Scan)\nP(Plan)\nP(Answer)\nValue\nP(Answer0)\nP(Answer1)\nP(Answer2)\n‚Ä¶\nTiled Question Embedding\nIs there an apple in the fridge?\nConvolution\nConcatenation\nFully Connected\nGRU\nSpatial\nSpatial\nFigure 3: Overview of the network architecture for the hierarchical Meta-Controller (and answerer) used for IQA tasks.\nThe network takes as input the full detected object map, the question, and the current image. The question is embedded\nand spatially tiled. We concatenate the object map features with the question embedding, perform several convolutions, and\nspatially sum the output. Similarly, we concatenate the image features with the question embedding, and use an attention\nmechanism conditioned on the question to spatially sum the features. We do not use an attention mechanism on the detected\nobject map as this makes counting questions difÔ¨Åcult. The image features additionally use a GRU [10] to add temporal\ncontext. Since the detected object map is purely additive (the Ô¨Ånal map contains at least as much information as the previous\nones) no temporal context is necessary. The map and image features are concatenated and fed into a fully-connected layer.\nThe network outputs a probability distribution over the actions, a value estimate for the state, and a distribution over the\nanswers for the question.\nronment. The question embedding is concatenated with the\nspatial map and visual features to condition the features on\nthe question. The network outputs a probability distribu-\ntion over the action space of direct controllers as well as a\nvalue for the current state. The Meta-Controller architecture\nfor Visual Semantic Planning is exactly the same except the\nquestion embedding is replaced with the semantic task em-\nbedding, and there is no answer branch in the output.\n3.3. Planner\nThe Planner is tasked with returning a sequence of ac-\ntions which would accomplish the goal. It operates on log-\nical primitives using the Planning Domain DeÔ¨Ånition Lan-\nguage (PDDL) and is guaranteed to return a correct set of\nhigh-level planning actions given the observations are ac-\ncurate. PDDL speciÔ¨Åes states using ‚ÄúÔ¨Çuents‚Äù which can be\nboolean values (cabinet 1 is open, an apple is in the fridge)\nor numeric (location A and B are 10 steps apart). Actions\nconsist of templatized preconditions necessary for the ac-\ntion to be possible, and effects, which modify the values of\nthe Ô¨Çuents caused by executing that action. For example, the\nOpen action takes the preconditions that an object must be\nopenable but closed and that the agent must be near the ob-\nject, and has the effect of setting the object‚Äôs closed Ô¨Çuent\nto False. Goals specify a set of criteria necessary for com-\npleting some task. Even complex tasks which take hundreds\nof steps like Put all the mugs into the sink.\nmay be easily speciÔ¨Åed. When using the Planner‚Äôs output,\nHIP-RL sequentially takes the next Planner action, runs the\nObject Detector and updates the knowledge representation,\nand replans. This allows the Planner to easily recover from\nincorrect initial detections or false negatives which may\nbe corrected over time. The Planner returns control either\nwhen the goal state has been reached, when it determines\nthe goal is impossible, or after a Ô¨Åxed number of steps.\nAn example Planner sequence for the question Is\nthere a bowl in a drawer?\nis as follows.\nThe\nMeta-Controller invokes the Planner with the goal All\ndrawers have been checked or a bowl is\nin a drawer.\nThe knowledge state contains three\ndrawers and the microwave. The Planner outputs a plan to\ncheck each drawer. The Ô¨Årst drawer is empty, so the plan\ncontinues. In the second drawer, the bowl is found, and the\nPlanner returns control to the Meta-Controller.\nIn a different example for the same task, the Planner\nchecks each drawer and the bowl is in none of them. Af-\nter all drawers are checked, the Planner returns control to\nthe Meta-Controller.\nGrouping the multi-step output from the Planner into a\nsingle decision made by the hierarchical controller gives\nHIP-RL several advantages over pure RL solutions. Pri-\nmarily, this signiÔ¨Åcantly reduces our action space and the\nnumber of high-level steps in a single episode. Furthermore,\nbecause the Planner guarantees that the goal will be reached\n(or ruled impossible), HIP-RL can be more thorough and ef-\nÔ¨Åcient in its exploration. In the examples above, if the agent\nhad only checked a single drawer and moved on, it would\nhave missed the opportunity to know the answer was ‚Äúyes‚Äù\nor be more conÔ¨Ådent that the answer was ‚Äúno.‚Äù\nIn this work we use the Metric-FF PDDL solver [16],\none of the most popular planning algorithms for operat-\ning on PDDL instances. Metric-FF extends the origin FF\nPlanning algorithm [17] to both boolean and numerical val-\nues. Metric-FF uses hill-climbing on relaxed plans (plans in\nwhich contradictions are ignored) as a heuristic to estimate\nthe distance to the goal. For numeric values, the relaxation\ntakes the form of ignoring any non-monotonicly increas-\ning effects that an action may have. Finding the absolute\nshortest solution to PDDL problems is NP-Complete, but in\npractice, Metric-FF usually returns nearly optimal plans in\naround 100 milliseconds. We include a sample PDDL state\nand action domain and corresponding Planner output in the\nsupplemental material.\n3.4. Object Detector\nThe Object Detector must detect objects from the cur-\nrent image, but it must also track what it has detected in\nthe past. In this work, we assume perfect camera location\nknowledge which simpliÔ¨Åes this process. The Object De-\ntector predicts object masks as well as pixelwise depths, and\nthe objects are projected into a global coordinate frame and\nmerged with prior detections. In our experiments we use\nMask-RCNN [15] for the detection masks and the FRCN\ndepth estimation network [23] which are both Ô¨Ånetuned on\nthe training environments. We merge detections by joining\nthe bounding cube around two detections if their 3D inter-\nsection is above a certain threshold (in practice 0.3). A more\nsophisticated strategy with more frequent detections such as\nFusion++[24] could further improve our method (however\nFusion++ requires a depth camera). In Figure 3, the De-\ntected Object Map represents the previously detected ob-\njects and their spatial locations.\n3.5. Navigator\nWe use the Navigator from [13] as it shows reliable per-\nformance for going to locations speciÔ¨Åed in a relative co-\nordinate frame by a hierarchical controller. The Navigator\npredicts edge weights for a small region in front of the agent\nand uses an Egocentric Spatial GRU to update the memory\nstate. Then it chooses the next action based on A* search to\nthe target location. For more details, see [13]. To improve\nthe overall execution speed of our method, our algorithm\nonly calls the Object Detector once the navigation has Ô¨Ån-\nished rather than at every intermediate location.\n3.6. Stopper\nThe Stopper is tasked with Ô¨Ånishing an episode.\nFor\nVSP, the Stopper simply terminates the episode.\nHow-\never for IQA, the Stopper must provide an answer to the\nposed question. For this, we train a network which takes\nthe question, the entire memory state, and the current im-\nage features as input and outputs an answer. For questions\nfrom IQUAD V1, we use state information from the Ob-\nject Detector to improve our accuracy. For example, for the\nquestion Is there bread in the room? since we\ntrack whether we have detected bread to be able to end plan-\nning upon detection, we can provide this information to the\nStopper as well. For EQA V1, since the questions can be\nanswered from a single image, we provided an additional\nimage channel representing a detection mask of the ques-\ntion‚Äôs subject. The Stopper is only trained based on the last\nstate from a sequence via cross entropy over the possible\nanswers. In practice, the Stopper shares a network with the\nMeta-Controller, as shown in Figure 3 which encourages\nthe learned features to be semantically grounded.\n3.7. Explorer\nTo gather more information about an environment, the\nExplorer Ô¨Ånds a location which has not been visited and in-\nvokes the Navigator with that location. In this work, the\nExplorer is not learned; instead, it tracks where the agent\nhas been and picks the location and orientation which max-\nimizes new views while minimizing the distance from the\ncurrent agent location. Note that the Explorer still operates\non the Navigator‚Äôs learned free-space map.\n3.8. Scanner\nThe Scanner issues a predeÔ¨Åned sequence of commands\nto the environment to obtain a 360¬∞ view of its surround-\nings. SpeciÔ¨Åcally, it performs three full rotations at +30,\n0, and -30 degrees with the horizon, stopping every 90 de-\ngrees to run the Object Detector. It is often useful to call the\nScanner after calling the Explorer, but we leave this up to\nthe hierarchical controller to learn.\n4. Tasks and Environments\nWe focus on two tasks (Interactive Question Answer-\ning and Visual Semantic Planning) in two virtual environ-\nments (AI2-THOR [20] and House3D [33] for IQA, and\nAI2-THOR for VSP). Both tasks require complex visual\nand spatial reasoning as well as multi-step planning.\n4.1. Interactive Question Answering (IQA)\nWe evaluate our agent on both IQUAD V1 [13] and\nEQA V1 [11]. IQUAD V1 provides 75,600 training ques-\ntions in 25 training rooms, and 1920 test questions in 5 un-\nseen rooms. Additionally, IQUAD V1 provides 2400 test\nquestions in seen rooms which helps factor out errors due to\nobject detection noise. For EQA V1, we use the published\ntrain/val splits which consist of 7129 training questions in\n648 train houses and 853 validation questions in 68 unseen\nhouses. [11, 12] show results with the agent starting 10, 30,\nor 50 steps away from the goal, yet only their results for 10\nsteps outperform the language baselines of [31]. As such,\nwe limit our experiments to starting 10 steps away.\nIn IQUAD V1, the agent must interact with kitchen\nscenes (opening drawers, exploring the room) in order\nto gather information to answer questions about the ob-\njects in the room such as Is there a mug in the\nmicrowave? EQA V1 places an agent in a house a certain\ndistance away from the object of interest and asks questions\nlike What color is the ottoman? Both datasets\nuse templated language. For more detail on these datasets,\nsee [11, 13].\nFor IQUAD V1, we specify the goal state for the Plan-\nner as either checking all locations where the question‚Äôs\nsubject could be, or knowing where the question‚Äôs sub-\nject is. For example, for the question Is there a mug\nin the microwave? the Planner must only check the\nmicrowave, but for the question Is there a mug in\nthe room? the Planner must check the microwave, the\nfridge, and all the drawers until it Ô¨Ånds at least one mug.\nFor EQA V1, we state the goal as ‚Äúthe agent is looking at\nthe object of interest if it knows where it is, or it has looked\nthrough all the doorways that it has seen.‚Äù This encour-\nages the Planner to explore various different rooms in the\nenvironment in the case where the object of interest is not\nlocated in the starting room.\n4.2. Visual Semantic Planning (VSP)\nWe also use the AI2-THOR environment [20] for seman-\ntic planning tasks such as Put the apple in the\nsink. These tasks are similar to those often done in Task\nand Motion Planning. Each task speciÔ¨Åes one of 13 small\nobjects (such as mug, fork, potato), and one of 6 target lo-\ncations (such as fridge, cabinet, sink), but unlike in [36],\nwe train one network to accomplish all tasks rather than\none for each pair of object-locations. Additionally, we in-\nclude navigation as a subtask of planning which [36] omits.\nCompared with IQA, VSP contains only a single task type\nbut uses a larger action space to facilitate picking up and\nputting down objects. Although we only test a single task\ntype, we include more complex tasks in the supplemental\nmaterial. We do not perform Visual Semantic Planning in\nHouse3D [33] as the environments are static. We use the\nsame train/test split of environments as in IQUAD V1, and\nconstruct 25,200 training tasks, 640 test tasks in unseen\nrooms, and 800 tasks in seen rooms. When constructing\nthe tasks, we verify that each task is possible, yet cannot\nbe completed by the empty plan, e.g. for the task Put a\nmug in a cabinet. there exists at least one mug and\nat least one cabinet, but no mugs start in cabinets. This data\nwill be made available upon publication.\n5. Experiments\nWe compare HIP-RL across the datasets outlined in Sec-\ntion 4 using existing state-of-the-art methods as baselines as\nwell as the unimodal baselines from [31] and pure planning\nbaselines. On each dataset, we record the accuracy/success\nof our method as well as the episode length. Except in the\ngeneralization experiment, all tests are done on unseen en-\nvironments. [2] proposes the Success weighted by (normal-\nized inverse) Path Length (SPL) which combines accuracy\nand episode length into a single metric for evaluating em-\nbodied agents on pure navigation tasks. SPL is deÔ¨Åned as\nSPL = 1\nN\nN\nX\ni=1\nSi\n‚Ñìi\nmax(pi, ‚Ñìi)\n(1)\nwhere Si is a success indicator for episode i, pi is the path\nlength, and ‚Ñìi is the shortest path length. SPL is not sufÔ¨Å-\ncient for question answering as an agent which never moves\ncould still be very successful depending on the difÔ¨Åculty of\nthe questions4. To address this issue, we propose the Shifted\nSPL (SSPL) metric which is deÔ¨Åned as\nSSPL = ¬µ ‚àíb\n1 ‚àíb ‚àóSPL\n(2)\nwhere ¬µ is the average accuracy of the method and b is\nthe average accuracy of a baseline agent which is forced to\nend/answer immediately after beginning an episode. Note\nthat SSPL directly accounts for dataset biases by subtract-\ning the accuracy of a learned baseline rather than simply\nthe most common answer or random chance accuracy. For\nthe VSP experiments SPL is exactly equal to SSPL, as a\nbaseline which cannot move will achieve 0% success. For\nthe IQA experiments we use the ‚ÄúLanguage Only‚Äù baselines\npresented in [31] as b.\n5.1. Baselines\nOn all datasets we include (at least) one pure-learning\nand one pure-planning baseline. The ‚ÄúPlanner Only‚Äù base-\nline uses the same Plan/Act/Observe/Replan loop as HIP-\nRL but does not include any hierarchical decision making.\nAdditionally, if at the start of the episode the plan is empty\n(for example if the agent starts looking at a wall), we ro-\ntate the agent until the plan is not empty. We also use the\n‚ÄúLanguage Only‚Äù baselines from [31] which attempt to an-\nswer the question without making any actions, effectively\nlearning the language bias of the dataset. For VSP, we intro-\nduce a ‚ÄúLearning Only‚Äù baseline which removes the Planner\nfrom HIP-RL and adds reward shaping to encourage certain\ninteractions like looking at and picking up the object of the\ntask. Even after signiÔ¨Åcant training time, this method fails\nto learn a working policy.\n4[31] shows signiÔ¨Åcant bias exists in EQA V1 [11] and in Matter-\nPort3D [3]\nIQUAD V1\nEQA V1\nVSP\nAccuracy\nEpisode Length\nSSPL\nAccuracy\nEpisode Length\nSSPL\nSuccess\nEpisode Length\nSSPL\nPlanner With Global Information\n(Shortest Path Estimate)\n100%\n88.710\n1\n100%\n10\n1\n100%\n87.477\n1\nState-of-the-art for IQUAD V1 [13] and EQA V1[12]\n52.52%\n586.890\n0.015\n53.58%\n-\n-\n-\n-\n-\nPlanner Only\n56.91%\n138.644\n0.105\n49.53%\n44.424\n0.002\n11.41%\n105.559\n0.059\nHIP-RL\n65.99%\n357.690\n0.086\n58.41%\n154.781\n0.007\n46.01%\n427.784\n0.189\n[13] with GT Detections\n64.27%\n531.840\n0.042\n-\n-\n-\n-\n-\n-\nPlanner Only with GT Detections\n74.53%\n169.773\n0.251\n54.15%\n22.780\n0.025\n43.44%\n161.998\n0.245\nHIP-RL With GT Detections\n81.25%\n297.238\n0.177\n65.28%\n127.011\n0.017\n73.75%\n254.367\n0.362\n[13] with GT Detections and Nav\n69.85%\n655.100\n0.046\n-\n-\n-\n-\n-\n-\nPlanner Only with GT Detections and Nav\n68.07%\n56.961\n0.091\n49.64%\n18.784\n0.004\n31.72%\n48.095\n0.268\nHIP-RL with GT Detections and Nav\n83.30%\n182.191\n0.325\n65.52%\n65.237\n0.033\n81.88%\n161.013\n0.549\nTable 1: Comparison of accuracy and episode length with varying levels of ground truth (GT) information on unseen environ-\nments. In some cases, the ‚ÄúPlanner Only‚Äù is fast enough to outperform HIP-RL on the SSPL metric, indicating there is still\nsigniÔ¨Åcant progress to be made on speeding up HIP-RL. Interestingly, using the navigation system instead of GT navigation\nhelps the ‚ÄúPlanner Only‚Äù method by giving it more varied locations to run Object Detector. The shortest path estimate for\nIQUAD V1 and VSP is equivalent to the ‚ÄúPlanner Only with GT Detections and Nav‚Äù method except that it is additionally\ngiven the positions of all large objects (fridges, cabinets, etc.). In IQUAD V1 Ô¨Ånding true shortest paths is an instance of the\ntraveling salesman problem. For EQA V1 the shortest path is found using an oracle with preexisting knowledge of the loca-\ntion of the target object. In all experiments we use either the FRCN [23] depth network in conjunction with Mask-RCNN [15]\nor ground truth detection masks and depth.\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nMost Frequent Answer\nLanguage Only [31]\nHIMN [13] (Learner Only)\nPlanner Only\nHIP-RL\nMost Frequent Answer\nLanguage Only [31]\nNMC [12] (Learner Only)\nPlanner Only\nHIP-RL\nLearner Only\nPlanner Only\nHIP-RL\nIQUAD V1\nEQA V1\nVSP\nAccuracy/Success\nAccuracy/Success on the IQA and VSP tasks\nin Unseen Environments\nFigure 4: Accuracy of various methods on each of the tasks.\nIn all cases, HIP-RL achieves state-of-the-art performance.\nWe include ‚ÄúLearner Only‚Äù and ‚ÄúPlanner Only‚Äù results for\neach experiment to show that combining both their strengths\nis better than either alone.\n5.2. Results\nWe test our system for accuracy on IQUAD V1,\nEQA V1, and the new VSP dataset, achieving state-of-the-\nart performance on all tasks. The results are shown in Fig-\nure 4. In IQUAD V1 and VSP the ‚ÄúPlanner Only‚Äù base-\nline outperforms the ‚ÄúLearning Only‚Äù baseline which co-\nincides with the fact that the ground-truth trajectories are\nsigniÔ¨Åcantly longer and contain more necessary interactions\nthan in EQA V1. A fundamental issue with reinforcement\nlearning is that it must ‚Äúluck into‚Äù good solutions randomly\nbefore it can improve, which can be very rare in complex\nmulti-step tasks. Planning simpliÔ¨Åes this by directly solv-\ning objectives rather than making guesses and observing\nrewards or penalties. Yet pure planning suffers from my-\nopia in that (in our system) it assumes perfect and complete\nglobal information, leading it to ignore unobserved parts of\nthe environment. This is most apparent in VSP where the\nplanner assumes a task is impossible if it has not observed\na location where the object can be.\nBy combining both\nstrategies, HIP-RL achieves the exploration tendencies of\nRL along with the goal-oriented direct problem solving of\nplanning.\n5.3. Ablation\nWe further explore the effect that various sources of in-\naccuracy have on HIP-RL by substituting the Object Detec-\ntor and the Navigator with Ground Truth (GT) information,\nshown in Table 1. Adding GT detections greatly improves\nour accuracy across the board. This is due to all the tasks\nbeing very object-centric, so if the object is misidentiÔ¨Åed or\nnot detected at all, the Answerer/Planner has no means of\nÔ¨Åxing the mistake. In contrast, using GT Navigation does\nnot improve performance dramatically, but the path lengths\ndo nearly halve. In practice we observe this is frequently\nnot from the navigation agent wandering randomly but is\ninstead usually from the beginning of the episodes where\nthe map starts empty and the navigator unknowingly goes\ndown dead ends or takes otherwise inefÔ¨Åcient paths.\n5.4. Episode EfÔ¨Åciency\nTable 1 also lists episode lengths and SSPL scores for\neach method. Note that episode lengths include every inter-\naction with the environment (turn left, open, move ahead\neach count as one action), not just hierarchical actions.\nWhile HIP-RL dramatically improves over [13], there is\nstill a large gap between the shortest path estimate. Some\ninefÔ¨Åciency due to exploration is unavoidable, but there are\nalso cases where the agent explores even after it could an-\nIQUAD V1 Unseen\nIQUAD V1 Seen\nAccuracy\nLength\nSSPL\nAccuracy\nLength\nSSPL\nHIP-RL\n65.99%\n357.690\n0.086\n77.75%\n265.668\n0.182\nHIP-RL + GT Det\n81.25%\n297.238\n0.177\n87.04%\n277.538\n0.278\nVSP Unseen\nVSP Seen\nSuccess\nLength\nSSPL\nSuccess\nLength\nSSPL\nHIP-RL\n46.01%\n427.784\n0.189\n70.88%\n245.301\n0.384\nHIP-RL + GT Det\n73.75%\n254.367\n0.362\n82.48%\n170.22\n0.504\nTable 2: Comparison of accuracy/success on seen and un-\nseen environments. HIP-RL is the full method, and HIP-RL\n+ GT Det uses the ground truth detections.\nswer.\nThis generally occurs in IQUAD V1 on counting\nquestions where the agent is not sure that it has sufÔ¨Åciently\nchecked everywhere where the object could be.\n5.5. Generalization\nOne beneÔ¨Åt of hierarchical models is they tend to gen-\neralize better as they force certain structure to be consis-\ntent between seen and unseen environments.\nYet if the\nhierarchical models depend on the performance of indi-\nvidual components, then the generalization performance of\nthe constituent models directly affects the overall perfor-\nmance. In Table 2 we explore the generalization of HIP-\nRL on IQUAD V1 and VSP by comparing performance\non rooms seen during training time with never-before-seen\nrooms (EQA V1 only provides test questions for unseen en-\nvironments). With ground truth detections, HIP-RL gener-\nalizes quite well, losing less than 10% raw performance in\nboth cases and staying nearly as efÔ¨Åcient step-wise. With\nMask-RCNN [15] detections and FRCN depth [23], per-\nformance is still reasonably similar, but there is a larger\ngap. Mask-RCNN produces high-quality results on large\ndatasets, yet in the case of AI2-THOR [20], there are fre-\nquently only 25 training and 5 testing samples of a partic-\nular class. Thus, Mask-RCNN struggles to detect the cabi-\nnets and drawers in unseen environments from AI2-THOR\n(as there are many cabinets and drawers per scene but none\nrepeat in multiple rooms), so frequently many areas remain\nunchecked. We believe that in scenarios with many more\ntraining examples, HIP-RL with detection would approach\nthe same level of generalization performance as without.\n5.6. Learning Speed\nWe compare the convergence speed of HIP-RL on the\nIQUAD V1 task with the previous state-of-the-art model,\nHIMN [13].\nAfter only 26,000 hierarchical steps, HIP-\nRL with ground truth information matches the Ô¨Ånal perfor-\nmance of HIMN with ground truth at 8 million hierarchical\nsteps5. After 120,000 hierarchical steps, HIP-RL (without\nground truth) converges to its maximum performance com-\npared to HIMN which takes 500,000 iterations and achieves\nsigniÔ¨Åcantly worse performance. HIP-RL trains orders of\n5 We use the number of hierarchical steps rather than the total number\nof steps in the environments as hierarchical steps are of variable length\nand do not provide gradients to the hierarchical controller until the Ô¨Ånal\nlow-level action.\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1,000\n10,000\n100,000\n1,000,000\n10,000,000\nAccuracy\nNumber of Training  Hierarchical Steps (log scale)\nUnseen Environment Performance Over Time\nHIP-RL with GT Detection and Nav\nHIP-RL\nHIMN [13] with GT Detection and Nav\nHIMN [13]\nFigure 5: Learning speed of HIP-RL and HIMN [13] with\nand without ground truth information. HIMN‚Äôs Answerer\nis additionally pretrained on fully observed rooms whereas\nHIP-RL does not require any pretraining.\nmagnitude faster than traditional RL algorithms because the\nplanner simpliÔ¨Åes much of the learning by being able to im-\nmediately (upon initialization) return good, thorough tra-\njectories. Being thorough early on is especially useful for\nquestion answering where an algorithm may get confusing\nfeedback if it answers too soon; for example, for the ques-\ntion Is there a bowl in the room? if an agent\ndoes not see a bowl and answers ‚Äúno‚Äù but the correct an-\nswer is ‚Äúyes,‚Äù the network will receive contradictory learn-\ning signals. By using a planner, we ensure more thorough\nexploration so this case is much less likely to occur, even at\nthe beginning of training.\n6. Conclusion\nIn this work, we presented Hierarchical Planning and Re-\ninforcement Learning, a method for combining the bene-\nÔ¨Åts of Deep Reinforcement Learning and Symbolic Plan-\nning. We demonstrate its effectiveness at increasing accu-\nracy while simultaneously decreasing episode length and\ntraining time. Though this exact implementation may not\nbe applicable to many other tasks, we believe the high level\nidea of learning to invoke various direct controllers, some of\nwhich explicitly plan, could be applied to a broader array of\ntasks such as Task and Motion Planning. In general, we ob-\nserve that using planning algorithms to assist in performing\n‚Äúgood‚Äù actions results in improved accuracy, test-time efÔ¨Å-\nciency, and training speed. Still, HIP-RL could be improved\nto explore more efÔ¨Åciently using priors based on likely lo-\ncations of an object. Additionally, we could learn the PDDL\npreconditions and effects directly so as to limit the need for\nhuman labels. We are excited about the potential impacts\nof visual agents and their ability to learn to interact more\nintelligently with the world around them.\n7. Acknowledgements\nThis work was funded in part by the National Science\nFoundation under contract number NSF-NRI-1637479,\nNSF-IIS-1338054,\nNSF-1652052,\nONR N00014-13-1-\n0720, the Allen Distinguished Investigator Award, and the\nAllen Institute for ArtiÔ¨Åcial Intelligence. We would like to\nthank NVIDIA for generously providing a DGX used for\nthis research via the UW NVIDIA AI Lab (NVAIL).\nReferences\n[1] C. Amato, G. Konidaris, A. Anders, G. Cruz, J. P. How, and\nL. P. Kaelbling. Policy search for multi-robot coordination\nunder uncertainty. The International Journal of Robotics Re-\nsearch, 35(14):1760‚Äì1778, 2016. 2\n[2] P. Anderson, A. Chang, D. S. Chaplot, A. Dosovitskiy,\nS. Gupta, V. Koltun, J. Kosecka, J. Malik, R. Mottaghi,\nM. Savva, and A. R. Zamir. On evaluation of embodied nav-\nigation agents. arXiv preprint arXiv:1807.06757, 2018. 6\n[3] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson,\nN. S¬®underhauf, I. Reid, S. Gould, and A. van den Hen-\ngel. Vision-and-Language Navigation: Interpreting visually-\ngrounded navigation instructions in real environments. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2018. 3, 6\n[4] C. Beattie, J. Z. Leibo, D. Teplyashin, T. Ward, M. Wain-\nwright, H. K¬®uttler, A. Lefrancq, S. Green, V. Vald¬¥es,\nA. Sadik,\net al.\nDeepmind lab.\narXiv preprint\narXiv:1612.03801, 2016. 2\n[5] G. Brockman, V. Cheung, L. Pettersson, J. Schneider,\nJ. Schulman, J. Tang, and W. Zaremba. Openai gym, 2016.\n2\n[6] P. Cai, Y. Luo, D. Hsu, and W. S. Lee. Hyp-despot: A hy-\nbrid parallel algorithm for online planning under uncertainty.\nRobotics Science and Systems (RSS) 2018, 11, 2018. 2\n[7] D. S. Chaplot, K. M. Sathyendra, R. K. Pasumarthi, D. Ra-\njagopal, and R. Salakhutdinov. Gated-attention architectures\nfor task-oriented language grounding. AAAI-18 Conference\non ArtiÔ¨Åcial Intelligence, 2018. 3\n[8] R. Chitnis, D. HadÔ¨Åeld-Menell, A. Gupta, S. Srivastava,\nE. Groshev, C. Lin, and P. Abbeel. Guided search for task and\nmotion plans using learned heuristics. In Robotics and Au-\ntomation (ICRA), 2016 IEEE International Conference on,\npages 447‚Äì454. IEEE, 2016. 3\n[9] S. Chitta, I. Sucan, and S. Cousins.\nMoveit![ros topics].\nIEEE Robotics & Automation Magazine, 19(1):18‚Äì19, 2012.\n1\n[10] K. Cho, B. van Merri¬®enboer, C¬∏ . G¬®ulc¬∏ehre, D. Bahdanau,\nF. Bougares, H. Schwenk, and Y. Bengio. Learning phrase\nrepresentations using rnn encoder‚Äìdecoder for statistical ma-\nchine translation.\nIn Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language Processing\n(EMNLP), pages 1724‚Äì1734, Doha, Qatar, Oct. 2014. Asso-\nciation for Computational Linguistics. 4\n[11] A. Das, S. Datta, G. Gkioxari, S. Lee, D. Parikh, and D. Ba-\ntra. Embodied question answering. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), volume 5, page 6, 2018. 2, 3, 5, 6\n[12] A. Das, G. Gkioxari, S. Lee, D. Parikh, and D. Batra. Neu-\nral Modular Control for Embodied Question Answering. In\nProceedings of the Conference on Robot Learning (CoRL),\n2018. 2, 3, 6, 7\n[13] D. Gordon, A. Kembhavi, M. Rastegari, J. Redmon, D. Fox,\nand A. Farhadi. Iqa: Visual question answering in interac-\ntive environments. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 4089‚Äì\n4098, 2018. 2, 3, 5, 6, 7, 8\n[14] S. Gupta, J. Davidson, S. Levine, R. Sukthankar, and J. Ma-\nlik. Cognitive mapping and planning for visual navigation.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 2616‚Äì2625, 2017. 3\n[15] K. He, G. Gkioxari, P. Doll¬¥ar, and R. Girshick. Mask r-cnn.\nIn Computer Vision (ICCV), 2017 IEEE International Con-\nference on, pages 2980‚Äì2988. IEEE, 2017. 5, 7, 8\n[16] J. Hoffmann.\nThe metric-ff planning system:\nTranslat-\ning‚Äúignoring delete lists‚Äùto numeric state variables. Journal\nof ArtiÔ¨Åcial Intelligence Research, 20:291‚Äì341, 2003. 2, 5\n[17] J. Hoffmann and B. Nebel. The ff planning system: Fast\nplan generation through heuristic search. Journal of ArtiÔ¨Åcial\nIntelligence Research, 14:253‚Äì302, 2001. 5\n[18] L. P. Kaelbling and T. Lozano-P¬¥erez.\nHierarchical task\nand motion planning in the now. In Robotics and Automa-\ntion (ICRA), 2011 IEEE International Conference on, pages\n1470‚Äì1477. IEEE, 2011. 3\n[19] M. Kempka, M. Wydmuch, G. Runc, J. Toczek, and\nW. Ja¬¥skowski. Vizdoom: A doom-based ai research platform\nfor visual reinforcement learning. In Computational Intelli-\ngence and Games (CIG), 2016 IEEE Conference on, pages\n1‚Äì8. IEEE, 2016. 2\n[20] E. Kolve, R. Mottaghi, D. Gordon, Y. Zhu, A. Gupta, and\nA. Farhadi. AI2-THOR: An Interactive 3D Environment for\nVisual AI. arXiv, 2017. 2, 3, 5, 6, 8\n[21] G. Konidaris, L. P. Kaelbling, and T. Lozano-Perez. From\nskills to symbols: Learning symbolic representations for ab-\nstract high-level planning. Journal of ArtiÔ¨Åcial Intelligence\nResearch, 61:215‚Äì289, 2018. 3\n[22] T. D. Kulkarni, K. Narasimhan, A. Saeedi, and J. Tenen-\nbaum. Hierarchical deep reinforcement learning: Integrating\ntemporal abstraction and intrinsic motivation. In Advances\nin neural information processing systems, pages 3675‚Äì3683,\n2016. 2\n[23] I. Laina, C. Rupprecht, V. Belagiannis, F. Tombari, and\nN. Navab. Deeper depth prediction with fully convolutional\nresidual networks. In 3D Vision (3DV), 2016 Fourth Interna-\ntional Conference on, 2016. 5, 7, 8\n[24] J. McCormac, R. Clark, M. Bloesch, A. Davison, and\nS. Leutenegger. Fusion++: Volumetric object-level slam. In\n2018 International Conference on 3D Vision (3DV), pages\n32‚Äì41. IEEE, 2018. 5\n[25] P. W. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. J. Ballard,\nA. Banino, M. Denil, R. Goroshin, L. Sifre, K. Kavukcuoglu,\nD. Kumaran, and R. Hadsell. Learning to navigate in com-\nplex environments. International Conference on Learning\nRepresentations (ICLR), 2017. 3\n[26] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap,\nT. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous\nmethods for deep reinforcement learning. In International\nConference on Machine Learning, 2016. 1, 2, 3\n[27] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiosity-\ndriven exploration by self-supervised prediction. In Inter-\nnational Conference on Machine Learning (ICML), volume\n2017, 2017. 1\n[28] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou,\nA. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton,\net al. Mastering the game of go without human knowledge.\nNature, 550(7676):354, 2017. 2\n[29] I. A. Sucan, M. Moll, and L. E. Kavraki. The open motion\nplanning library. IEEE Robotics & Automation Magazine,\n19(4):72‚Äì82, 2012. 1\n[30] C. Tessler, S. Givony, T. Zahavy, D. J. Mankowitz, and\nS. Mannor. A deep hierarchical approach to lifelong learning\nin minecraft. In AAAI, volume 3, page 6, 2017. 2\n[31] J. Thomason, D. Gordon, and Y. Bisk. Shifting the base-\nline: Single modality performance on visual navigation &\nqa. arXiv preprint arXiv:1811.00613, 2018. 6\n[32] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine\nfor model-based control. In Intelligent Robots and Systems\n(IROS), 2012 IEEE/RSJ International Conference on, pages\n5026‚Äì5033. IEEE, 2012. 2\n[33] Y. Wu, Y. Wu, G. Gkioxari, and Y. Tian. Building general-\nizable agents with a realistic and rich 3d environment. arXiv\npreprint arXiv:1801.02209, 2018. 2, 3, 5, 6\n[34] F. Xia, A. R. Zamir, Z. He, A. Sax, J. Malik, and S. Savarese.\nGibson env: Real-world perception for embodied agents. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 9068‚Äì9079, 2018. 2\n[35] C. Yan, D. Misra, A. Bennnett, A. Walsman, Y. Bisk, and\nY. Artzi. Chalet: Cornell house agent learning environment.\narXiv preprint arXiv:1801.07357, 2018. 2\n[36] Y. Zhu, D. Gordon, E. Kolve, D. Fox, L. Fei-Fei, A. Gupta,\nR. Mottaghi, and A. Farhadi. Visual semantic planning using\ndeep successor representations. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pages 483‚Äì\n492, 2017. 6\n[37] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-\nFei, and A. Farhadi. Target-driven visual navigation in in-\ndoor scenes using deep reinforcement learning. In Robotics\nand Automation (ICRA), 2017 IEEE International Confer-\nence on, pages 3357‚Äì3364. IEEE, 2017. 3\nAppendix A. PDDL Domain\nBelow is the full PDDL Domain for question answering and\nvisual semantic planning.\n( d e f i n e\n( domain\nq a v s p t a s k )\n( : r e q u i r e m e n t s\n: adl\n)\n( : types\nagent\nl o c a t i o n\nr e c e p t a c l e\no b j e c t\nr t y p e\notype\n)\n( : p r e d i c a t e s\n( a t L o c a t i o n\n? a ‚àíagent\n? l ‚àíl o c a t i o n )\n( r e c e p t a c l e A t L o c a t i o n\n? r ‚àír e c e p t a c l e\n? l ‚àíl o c a t i o n )\n( o b j e c t A t L o c a t i o n\n?o ‚àío b j e c t\n? l ‚àíl o c a t i o n )\n( openable\n? r ‚àír e c e p t a c l e )\n( opened ? r ‚àír e c e p t a c l e )\n( i n R e c e p t a c l e\n?o ‚àío b j e c t\n? r ‚àír e c e p t a c l e )\n( checked ? r ‚àír e c e p t a c l e )\n( r e c e p t a c l e T y p e\n? r ‚àír e c e p t a c l e\n? t ‚àír t y p e )\n( objectType\n?o ‚àío b j e c t\n? t ‚àíotype )\n( canContain\n? t ‚àír t y p e\n?o ‚àíotype )\n( holds\n? a ‚àíagent\n?o ‚àío b j e c t )\n( holdsAny ? a ‚àíagent )\n( f u l l\n? r ‚àír e c e p t a c l e )\n)\n( : f u n c t i o n s\n( d i s t a n c e\n? from ? to )\n( t o t a l C o s t )\n)\n; ;\nagent\ngoes\nto\nr e c e p t a c l e\n( : a c t i o n\nGotoLocation\n: parameters\n(? a ‚àíagent\n? l S t a r t ‚àíl o c a t i o n\n? lEnd ‚àíl o c a t i o n )\n: p r e c o n d i t i o n\n( a t L o c a t i o n\n? a ? l S t a r t )\n: e f f e c t\n( and\n( a t L o c a t i o n\n? a ? lEnd )\n( not\n( a t L o c a t i o n\n? a ? l S t a r t ) )\n( f o r a l l\n(? r ‚àír e c e p t a c l e )\n( when ( and\n( r e c e p t a c l e A t L o c a t i o n\n? r\n? lEnd )\n( or\n( not\n( openable\n? r ) )\n( opened ? r ) ) )\n( checked ? r )\n)\n)\n( i n c r e a s e\n( t o t a l C o s t )\n( d i s t a n c e\n? l S t a r t\n? lEnd ) )\n)\n)\n; ;\nagent\nopens\nr e c e p t a c l e\n( : a c t i o n\nOpenObject\n: parameters\n(? a ‚àíagent\n? l ‚àíl o c a t i o n\n? r ‚àír e c e p t a c l e )\n: p r e c o n d i t i o n\n( and\n( a t L o c a t i o n\n? a ? l )\n( r e c e p t a c l e A t L o c a t i o n\n? r\n? l )\n( openable\n? r )\n( f o r a l l\n(? re ‚àír e c e p t a c l e )\n( not\n( opened ? re ) ) )\n)\n: e f f e c t\n( and\n( opened ? r )\n( checked ? r )\n( i n c r e a s e\n( t o t a l C o s t )\n1)\n)\n)\n; ;\nagent\nc l o s e s\nr e c e p t a c l e\n( : a c t i o n\nCloseObject\n: parameters\n(? a ‚àíagent\n? l ‚àíl o c a t i o n\n? r ‚àír e c e p t a c l e )\n: p r e c o n d i t i o n\n( and\n( a t L o c a t i o n\n? a ? l )\n( r e c e p t a c l e A t L o c a t i o n\n? r\n? l )\n( openable\n? r )\n( opened ? r )\n)\n: e f f e c t\n( and\n( not\n( opened ? r ) )\n( i n c r e a s e\n( t o t a l C o s t )\n1)\n)\n)\n; ;\nagent\npicks\nup\no b j e c t\n( : a c t i o n\nPickupObject\n: parameters\n(? a ‚àíagent\n? l ‚àíl o c a t i o n\n?o ‚àío b j e c t\n? r ‚àír e c e p t a c l e )\n: p r e c o n d i t i o n\n( and\n( a t L o c a t i o n\n? a ? l )\n( o b j e c t A t L o c a t i o n\n?o ? l )\n( or\n( not\n( openable\n? r ) )\n( opened ? r ) )\n( i n R e c e p t a c l e\n?o ? r )\n( not\n( holdsAny ? a ) )\n)\n: e f f e c t\n( and\n( not\n( i n R e c e p t a c l e\n?o ? r ) )\n( holds\n? a ?o )\n( holdsAny ? a )\n( i n c r e a s e\n( t o t a l C o s t )\n1)\n)\n)\n; ;\nagent\nputs down\no b j e c t\n( : a c t i o n\nPutObject\n: parameters\n(? a ‚àíagent\n? l ‚àíl o c a t i o n\n? ot ‚àíotype\n?o ‚àío b j e c t\n? r ‚àír e c e p t a c l e )\n: p r e c o n d i t i o n\n( and\n( a t L o c a t i o n\n? a ? l )\n( r e c e p t a c l e A t L o c a t i o n\n? r\n? l )\n( or\n( not\n( openable\n? r ) )\n( opened ? r ) )\n( not\n( f u l l\n? r ) )\n( objectType\n?o ? ot )\n( holds\n? a ?o )\n)\n: e f f e c t\n( and\n( i n R e c e p t a c l e\n?o ? r )\n( f u l l\n? r )\n( not\n( holds\n? a ?o ) )\n( not\n( holdsAny ? a ) )\n( i n c r e a s e\n( t o t a l C o s t )\n1)\n)\n)\n)\nAppendix B. PDDL Goal Example\nBelow is the goal speciÔ¨Åcation for the question Is there\na mug in the room?.\n( : goal\n( or\n( e x i s t s\n(? o ‚àío b j e c t )\n( objectType\n?o MugType ) )\n( and\n( f o r a l l\n(? t ‚àír t y p e )\n( f o r a l l\n(? r ‚àír e c e p t a c l e )\n( or\n( not\n( and\n( canContain\n? t MugType )\n( r e c e p t a c l e T y p e\n? r\n? t ) ) )\n( checked ? r )\n)\n)\n)\n( f o r a l l\n(? re ‚àír e c e p t a c l e )\n( not\n( opened ? re ) ) )\n)\n)\n)\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2019-01-06",
  "updated": "2019-01-06"
}