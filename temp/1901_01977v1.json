{
  "id": "http://arxiv.org/abs/1901.01977v1",
  "title": "Accelerating Goal-Directed Reinforcement Learning by Model Characterization",
  "authors": [
    "Shoubhik Debnath",
    "Gaurav Sukhatme",
    "Lantao Liu"
  ],
  "abstract": "We propose a hybrid approach aimed at improving the sample efficiency in\ngoal-directed reinforcement learning. We do this via a two-step mechanism where\nfirstly, we approximate a model from Model-Free reinforcement learning. Then,\nwe leverage this approximate model along with a notion of reachability using\nMean First Passage Times to perform Model-Based reinforcement learning. Built\non such a novel observation, we design two new algorithms - Mean First Passage\nTime based Q-Learning (MFPT-Q) and Mean First Passage Time based DYNA\n(MFPT-DYNA), that have been fundamentally modified from the state-of-the-art\nreinforcement learning techniques. Preliminary results have shown that our\nhybrid approaches converge with much fewer iterations than their corresponding\nstate-of-the-art counterparts and therefore requiring much fewer samples and\nmuch fewer training trials to converge.",
  "text": "Accelerating Goal-Directed Reinforcement Learning by Model\nCharacterization\nShoubhik Debnath1, Gaurav Sukhatme2, Lantao Liu3\nAbstract— We propose a hybrid approach aimed at im-\nproving the sample efﬁciency in goal-directed reinforcement\nlearning. We do this via a two-step mechanism where ﬁrstly, we\napproximate a model from Model-Free reinforcement learning.\nThen, we leverage this approximate model along with a notion\nof reachability using Mean First Passage Times to perform\nModel-Based reinforcement learning. Built on such a novel\nobservation, we design two new algorithms - Mean First\nPassage Time based Q-Learning (MFPT-Q) and Mean First\nPassage Time based DYNA (MFPT-DYNA), that have been\nfundamentally modiﬁed from the state-of-the-art reinforcement\nlearning techniques. Preliminary results have shown that our\nhybrid approaches converge with much fewer iterations than\ntheir corresponding state-of-the-art counterparts and therefore\nrequiring much fewer samples and much fewer training trials\nto converge.\nI. INTRODUCTION\nReinforcement Learning (RL) has been successfully applied\nto numerous challenging problems for autonomous agents to\nbehave intelligently in unstructured real-world environment.\nOne interesting area of research in RL which motivates\nthis work is goal-directed reinforcement learning problem\n(GDRLP) [1] [2]. In GDRLP, the learning process takes place\nin two stages. The ﬁrst stage focuses on solving the goal-\ndirected exploration problem (GDEP) which allows an agent\nto determine a viable path from an initial state to a goal state\nin an unknown or only partially known state space. The path\nfound in this stage need not be the optimal one. In the second\nstage, the agent takes advantage of the previously learned\nknowledge to optimize the path to the goal state. The two\nstages iterate in order to converge to the action policy.\nRL methods are generally divided into Model-Free (MF)\nand Model-Based (MB) approaches. MF methods can learn\ncomplex policies with minimal feature and policy engineering\nwork. However, the convergence of such methods might\nrequire millions of trials and hence they are sample inefﬁcient\n[3] [4]. MB methods require much smaller number of real-\nworld trials to converge but need an accurate model of real-\nworld physical system and the environment which might be\nchallenging to obtain [5]. Also, relying on an accurate model\n1Shoubhik Debnath is with NVIDIA Corporation, Santa Clara, CA 95051,\nUSA. E-mail: sdebnath@nvidia.com\n2Gaurav Sukhatme is with the Department of Computer Science at the\nUniversity of Southern California, Los Angeles, CA 90089, USA. E-mail:\ngaurav@usc.edu\n3Lantao Liu is with the Intelligent Systems Engineering Department at\nIndiana University - Bloomington, Bloomington, IN 47408, USA. E-mail:\nlantao@iu.edu\nThe paper was published in 2018 IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS).\ncan be problematic because small glitches on the model may\nlead to catastrophic consequences.\nIn this paper, we leverage the beneﬁts of both approaches\n- MB and MF, with an aim to improve the sample efﬁciency\nduring the training process. We achieve this via a two-step\niterative learning mechanism. In the ﬁrst step, we learn an\napproximate model of the physical system using an MF\nscheme. Note that, our approach does not need to construct\nan accurate model, instead a rough model with little cost is\nsufﬁcient which is used to characterize the structure of the\nproblem. In the second step, we leverage this approximate\nmodel along with the notion of reachability using Mean First\nPassage Times (MFPT) where the result is used to guide\nfollowing MF exploration and learning.\nContribution: The main contributions of our work are:\n• We propose a hybrid RL approach that introduces a\nmodel-based characterization into the state-of-the-art RL\nalgorithms to improve sample-efﬁciency.\n• The model-based characterization is achieved via a\nmodel-based RL algorithm that is robust to an approxi-\nmate model for learning complex policies.\nWe demonstrate our proposed method on two tasks related\nto the path-planning domain in 2D and 3D simulation\nenvironments respectively. The goal of the agent in both\ntasks is to learn an optimal policy to reach a goal state from\na given start location. Our results show that the proposed\nhybrid algorithms with model-based characterization were\nable to learn the optimal policy in very few trials, thereby\nimproving the sample efﬁciency and accelerating the learning\nprocess.\nII. RELATED WORK\nEarlier works have demonstrated various approaches to\nsolve goal-directed reinforcement learning problem. Braga\net al. presented a solution to solve GDRLP for an indoor\nunknown environment. Firstly, using temporal difference\nlearning method, they ﬁnd an initial solution to reach the goal\nand then improve upon the initial solution by employing a\nvariable learning rate [1]. Several earlier works also focused\non goal-directed exploration as it provides insight into the\ncorresponding RL problems. It has been proved that goal-\ndirected exploration with RL methods can be intractable,\ntherefore demonstrating that solving RL problems can be\nintractable as well [6]. This work also presented that the\nbehavior of an agent followed a random walk until it reached\nthe goal for the ﬁrst time. Koenig et al. also studied the\neffect of representation and knowledge on the tractability of\ngoal-directed exploration with RL algorithms [2]. However,\narXiv:1901.01977v1  [cs.LG]  4 Jan 2019\nin our work, the primary focus is on the second stage of\nGDRLP, where the aim is to accelerate the convergence to\noptimal policy via model characeterization.\nMF approaches in reinforcement learning can learn complex\npolicies but requires many trials to converge. The most\nwidely used model-free reinforcement learning might be\nthe Q-Learning [7] which is detailed in the next section.\nAnother model-free learning algorithm similar to Q-Learning\nis SARSA [8]. The main difference between SARSA and Q-\nLearning is that SARSA agent learns the action-value function\nby following the policy it learned, while Q-Learning agent\nlearns the action-value function by following an exploitation\npolicy owing to the exploration/exploitation trade-off. On the\nother hand, MB approaches can generalize to new tasks and\nenvironments in fewer trials, however, an accurate model is\nnecessary. We recently also investigated reachability heuristics\nand showed that computational performance for standard and\naccurate MDP models can be improved [9].\nAnother interesting research direction focused on reducing\nthe size of problem space in MB approaches. Boutilier et al.\nproposed structured reachability analysis of MDPs in order to\nremove variables from problem description, thereby reducing\nthe size of MDP and eventually making it easier to solve [10].\nIt is therefore very intuitive to investigate approaches that\ncombine the advantages of MF and MB methods [11] [12].\nThere have also been multiple previous works that combined\nthe two paradigms. The primary objective of such methods\nwere to speed-up the learning process for MF reinforcement\nlearning approaches. A broad area of research including the\nDYNA framework [13] [14] leveraged a learned model to\ngenerate synthetic experience for MF learning.\nAlong similar direction, several prior works focused on\ndevising a model as an initialization for MF component\n[15] [16]. One of the challenges that this leads to is the\ninaccuracies in the model which cause the issue of model\nbias. A suggested solution to overcome model bias is to\ndirectly train the dynamics in a goal-oriented manner [17]\n[18]. Our work is also motivated from this approach in order\nto deal with model bias.\nUnlike prior works on combining MB and MF reinforce-\nment learning methods, we integrate the beneﬁts of both\napproaches - MB and MF, with an aim to improve the sample\nefﬁciency during the training process. The primary objective\nin this work is to incorporate a model-based characterization\nusing MFPT into a reinforcement learning algorithm (model-\nfree approaches like Q-Learning or RL frameworks like\nDYNA), so that the characterization result can be used to\nguide following MF exploration and learning. Our approach\ndiffers from existing hybrid models in that, the method does\nnot need to construct an accurate model, and a rough model\nwith little cost is enough for capturing high-level features.\nBy comparing with state-of-the-art baseline approaches, our\nevaluations reveal that the proposed hybrid algorithms are able\nto learn optimal policy in very few trials with high sample\nefﬁciency, and have signiﬁcantly accelerated the practical\nlearning process.\nIII. PRELIMINARIES\nA. Model-Based Reinforcement Learning\nModel-Based reinforcement learning needs to ﬁrst build\na model, and then use it to derive a policy. The underlying\nmechanism is Markov Decision Process (MDP), which is a\ntuple M = (S, A, T, R), where S = {s1, · · · , sn} is a set of\nstates and A = {a1, · · · , an} is a set of actions. The state\ntransition function T : S × A × S →[0, 1] is a probability\nfunction such that Ta(s1, s2) is the probability that action\na in state s1 will lead to state s2, and R : S × A →R\nis a reward function where Ra(s, s′) returns the immediate\nreward received on taking action a in state s that will lead\nto state s′. A policy is of the form π = {s1 →a1, s2 →\na2, · · · , sn →an}. We denote π[s] as the action associated\nto state s. If the policy of a MDP is ﬁxed, then the MDP\nbehaves as a Markov chain [19].\nTo solve an MDP, the most widely used approach should\nbe value iteration (VI). The VI is an iterative procedure that\ncalculates the value (or utility in some literature) of each\nstate s based on the values of the neighbouring states that\ns can directly transit to. The value V (s) of state s at each\niteration can be calculated by the Bellman equation shown\nbelow\nV (s) = max\na∈A\nX\ns′∈S\nTa(s, s′)\n\u0010\nRa(s, s′) + γV (s′)\n\u0011\n,\n(1)\nwhere γ is a reward discounting parameter. The stopping\ncriteria for the algorithm is when the values calculated on two\nconsecutive iterations are close enough, i.e., maxs∈S |V (s)−\nV ′(s)| ≤ϵ, where ϵ is an optimization tolerance/threshold\nvalue, which determines the level of convergence accuracy.\nRelevant to this work, the prioritized sweeping mechanism\nis an important heuristic-based approach for efﬁciently solving\nMDPs in order to further speed up the value iteration process\n[20]. This heuristic evaluates each state and obtains a score\nbased on the state’s contribution to the convergence, and then\nprioritizes/sorts all states based on their scores (e.g., those\nstates with larger difference in value between two consecutive\niterations will get higher scores) [21], [22]. Then immediately\nin the next dynamic programming iteration, evaluating the\nvalue of states follow the newly prioritized order.\nGiven a model, methods proposed for solving MDPs can be\neasily extended to the context of MB learning methods [23].\nThe model-based characterization in our proposed approach\nis also built on top of this notion.\nB. Model-Free Reinforcement Learning\nModel-free reinforcement learning aims at learning a policy\nwithout learning a model. The most widely used model-\nfree reinforcement learning might be the Q-Learning [7],\nwhich is a special algorithm of the Temporal Difference\n(TD) learning [24]. This approach is able to compare the\nexpected utility of the available actions at a given state without\nrequiring a model of the environment. To learn the expected\nutility of taking a given action a in a given state s, it learns\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFig. 1. (a) Demonstration of simulation environment, with the agent’s\ninitial state (blue) and the goal state (red). Grey blocks are obstacles;\n(b) Converged optimal policy (red arrows) and a trajectory completed\nby the agent to reach the goal.; (c)-(f) Evolution of reachability\nlandscapes.\na action-value function Q(s, a). The Q-Learning rule is\nQ(s, a) = Q(s, a) + α(r + γ max\na′∈A Q(s′, a′) −Q(s, a)), (2)\nwhere, <s, a, r, s′> is an experience tuple, α is the learning\nrate, and γ is the discount factor. After the action-value\nfunction is learned, the optimal policy can be constructed by\ngreedily selecting the action with the highest action-value in\neach state.\nC. Synthesis of Model-based and Model-free\nThere are several frameworks that integrate the model-\nbased and model-free paradigms, with the most well known\narchitecture probably being DYNA [13], [14]. DYNA exploits\na middle ground, yielding strategies that are both more\neffective than model-free learning and more computationally\nefﬁcient than the certainty-equivalence approach. DYNA\narchitecture comprises of two phases. In the ﬁrst phase, the\nagent carries out actions in the environment and performs\nregular reinforcement learning to learn value function and\nadjust the policy. It also uses the real experience to explicitly\nbuild up the transition model T and/or the reward function R\nassociated with the environment. The second phase involves\nplanning updates where simulated experiences are used to\nupdate policy and value function.\nD. Mean First Passage Times\nBefore we elaborate on what we mean by model charac-\nterization in our work, we will describe a key concept called\nMean First Passage Times (MFPT).\nThe ﬁrst passage time (FPT), Tij, is deﬁned as the number\nof state transitions involved in reaching states sj when started\nin state si for the ﬁrst time. The mean ﬁrst passage time\n(MFPT), µij from state si to sj is the expected number of\nhopping steps for reaching state sj given initially it was in\nstate si [25]. The MFPT analysis is built on the Markov chain,\nand has nothing to do with the agent’s actions. Remember\nthat, when a MDP is associated to a ﬁxed policy, it then\nbehaves as a Markov chain [19].\nFormally, let us deﬁne a Markov chain with n states and\ntransition probability matrix, p ∈IRn,n. If the transition\nprobability matrix is regular, then each MFPT, µij = E(Tij),\nsatisﬁes the below conditional expectation formula:\nE(Tij) =\nX\nk\nE(Tij|Bk)pik,\n(3)\nwhere, pik represents the transition probability from state\nsi to sk, and Bk is an event where the ﬁrst state transition\nhappens from state si to sk. From the deﬁnition of mean ﬁrst\npassage times, we have, E(Tij|Bk) = 1 + E(Tkj). So, we\ncan rewrite Eq. (3) as follows\nE(Tij) =\nX\nk\npik +\nX\nk̸=j\nE(Tkj)pik.\n(4)\nSince, P\nk pik = 1, Eq. (4) can be formulated as per the\nbelow equation:\nµij = 1+\nX\nk̸=j\npik∗µkj\n⇒\nX\nk̸=j\npik∗µkj−µij = −1, (5)\nSolving all MFPT variables can be viewed as solving a system\nof linear equations\n\n\n\n\n\n\np11 −1\np12\n..\n..\np1n\np21\np22 −1\n..\n..\np2n\n..\n..\n..\n..\n..\n..\n..\n..\n..\n..\npn1\npn2\n..\n..\npnn −1\n\n\n\n\n\n\n\n\n\n\n\n\nµ1j\nµ2j\n..\n..\nµnj\n\n\n\n\n\n\n=\n\n\n\n\n\n\n−1\n−1\n..\n..\n−1\n\n\n\n\n\n\n.\n(6)\nThe values µ1j, µ2j, ...., µnj represents the MFPTs calculated\nfor state transitions from states s1, s2, ...., sn to sj and\nµjj = 0. To solve above equation, efﬁcient decomposition\nmethods [26] may help to avoid a direct matrix inversion.\nIV. TECHNICAL APPROACH\nIn this work, we are interested in goal-directed autonomy,\nwhere the agent is speciﬁed with a goal or terminal state to\narrive. Note, a Markov system is deﬁned as absorbing if from\nevery non-terminal state it is possible to eventually enter a\ngoal state [27]. We restrict our attention to absorbing Markov\nsystems so that the agent ﬁnally terminates at a goal.\nA. Reachability Characterization using Mean First Passage\nTimes\nThe notion of MFPT allows us to deﬁne the reachability\nof a state. By “reachability of a state\" we mean that based\non current ﬁxed policy, how hard it is for the agent to transit\nfrom the current state to the given goal/absorbing state. With\nall MFPT values µij obtained, we can construct a reachability\nlandscape which is essentially a “map\" measuring the degree\nof difﬁculty of all states transiting to the goal.\nFig. 1 shows a series of landscapes represented in heatmap\nin our simulated environment. The values in the heatmap\nrange from 0 (cold color) to 600 (warm color). In order to\nbetter visualize the low MFPT spectrum that we are most\ninterested, any value greater than 600 has been clipped to 600.\nFig. 1(c)-1(f) show the change of landscapes as the learning\nalgorithm proceeds. Initially, all states except the goal state\nare initialized as unreachable, as shown by the high MFPT\ncolor in Fig. 1(c).\nWe observe that the reachability landscape conveys very\nuseful information on potential impacts of different states.\nMore speciﬁcally, a state with a better reachability\n(smaller MFPT value) is more likely to make a larger\nchange during the MDP convergence procedure, leading\nto a bigger convergence step. With such observation and\nthe new metric, we can design state prioritization schemes for\nValue Iteration that we will use in our proposed approaches.\nAlgorithm 1: Mean First Passage Time based Q-Learning\n(MFPT-Q)\n1 Given states S, actions A, discount factor γ, learning rate α\nand goal state s∗, calculate the optimal policy π\n2 while true do\n3\nSelect state s at random\n4\nChoose action a based on ϵ-greedy\n5\nExecute a at state s and get s′, r\n6\nPerform one-step tabular Q-Learning:\nQ(s, a) = Q(s, a) + α[r + γ maxa′ Q(s′, a′) −Q(s, a)]\n7\nUpdate model details: transition probability Ta(s, s′) and\nreward Ra(s, s′) based on count statistics\n8\nwhile true do\n9\nV = V ′\n10\nCalculate MFPT values µ1s∗, µ2s∗, · · · , µ|S|s∗by\nsolving the linear system as shown in Eq. (6)\n11\nList L := Sorted states with increasing order of MFPT\nvalues\n12\nforeach state s in L do\n13\nCompute value update at each state s given policy\nπ: Q(s, a) =\nP\n∀s′∈S Ta(s, s′)\n\u0010\nRa(s, s′) + γV (s′)\n\u0011\nV ′(s) = maxa∈A Q(s, a)\n14\nif maxsi |V (si) −V ′(si)| ≤ϵ then\n15\nbreak\nB. Mean First Passage Time based Q-Learning (MFPT-Q)\nClassic Q-Learning converges to the optimal solution\nthrough the Q-Learning rule as shown in Eq. (2), which\nessentially learns the state-action value function Q(s, a) that\nrepresents the expected utility of the available actions at a\ngiven state.\nOur proposed hybrid algorithm, MFPT-Q, performs two\nmain operations involving a model-free and a model-based up-\ndate every iteration. Firstly, given an experience <s, a, r, s′>,\nit builds an approximate model by updating the transition\nfunction Ta(s, s′) and reward function Ra(s, s′). To update\nTa(s, s′), it uses the count statistics which basically refers to\nthe count of transitions occurring from state s to s′ followed\nby normalization between 0 and 1.\nThe second step leverages the approximate model computed\nearlier to perform a model-based update using MFPT-VI\nalgorithm (See lines 8-15 of Alg. 1). The MFPT-VI method\nis built on a metric using the reachability (MFPT values)\nsince, the reachability characterization of each state reﬂects\nthe potential impact/contribution of this state. Hence, such\ncharacterization provides a natural basis for state prioritization\nwhile performing a version of value-iteration for Q values\nduring model-based update.\nNote that, since the MFPT computation is relatively\nexpensive, and the purpose of using MFPT is to characterize\nglobal and general (instead of ﬁne) features of all states, thus\nit is not necessary to compute the MFPT at every iteration, but\nrather after every few iterations. For those iterations between\ntwo adjacent MFPT updates, the value of all states converge\nfrom a “local reﬁnement\" perspective. The computational\nprocess of MFPT-Q is pseudo-coded in Alg. 1.\nAlgorithm 2: Mean First Passage Time based DYNA\n(MFPT-DYNA)\n1 Given states S, actions A, discount factor γ, learning rate α\nand goal state s∗, calculate the optimal policy π\n2 while true do\n3\nSelect state s at random\n4\nChoose action a based on ϵ-greedy\n5\nExecute a at state s and get s′, r (real experience)\n6\nPerform one-step tabular Q-Learning:\nQ(s, a) = Q(s, a) + α[r + γ maxa′ Q(s′, a′) −Q(s, a)]\n7\nUpdate model details: transition probability Ta(s, s′) and\nreward Ra(s, s′) based on count statistics\n8\ni →0\n9\nwhile i < N do\n10\ns is a randomly allocated previously observed state\n11\na is a random action previously carried out in s\n12\nProduce simulated experience to get s′, r\n13\nQ(s, a) = Q(s, a)+α[r+γ maxa′ Q(s′, a′)−Q(s, a)]\n14\nwhile true do\n15\nV = V ′\n16\nCalculate MFPT values µ1s∗, µ2s∗, · · · , µ|S|s∗by\nsolving the linear system as shown in Eq. (6)\n17\nList L := Sorted states with increasing order of MFPT\nvalues\n18\nforeach state s in L do\n19\nCompute value update at each state s given policy\nπ: Q(s, a) =\nP\n∀s′∈S Ta(s, s′)\n\u0010\nRa(s, s′) + γV (s′)\n\u0011\nV ′(s) = maxa∈A Q(s, a)\n20\nif maxsi |V (si) −V ′(si)| ≤ϵ then\n21\nbreak\nC. Mean First Passage Time based DYNA (MFPT-DYNA)\nClassic DYNA architecture balances between real and\nsimulated experience to speed up the training process. As\nmentioned earlier, the agent learns a value function and\nupdates the policy using both sets of experiences. In addition,\nthe agent also learns a model of the environment using real\nexperiences. This notion of learned model in DYNA makes\nit intuitive and easier to integrate our proposed model-based\ncharacterization. Here we further extend this framework and\npropose an upgraded hybrid algorithm - Mean First Passage\nTime based DYNA (MFPT-DYNA).\nRemember that the classic DYNA algorithm includes two\nsteps involving real experience and simulated experience.\nFor the simulated experience, we employ the classic DYNA\nprocedure where the simulation performs N additional\nupdates, i.e., it chooses N state-action pairs at random and\nupdate state-action values according to the rule mentioned in\nEq. (2). (See lines 9-13 of Alg. 2.)\nDifferent from the standard DYNA mechanism, our MFPT-\nDYNA utilizes the real experience <s, a, r, s′> to build the\napproximate model by updating state-action values according\nto the rule mentioned in Eq. (2). Again, for updating Ta(s, s′),\nit increments the count statistics for the transitions occurring\nfrom state s to s′ followed by normalization between 0 and\n1. It updates the Ra(s, s′) based on the reward r received\nfor taking action a in state s.\nAnalogous to the one mentioned in case of MFPT-Q, we\npropose a model-based update using MFPT-VI algorithm.\nThe MFPT-VI algorithm leverages the approximate model\n(represented by the transition and reward function) computed\nearlier to perform a version of the value-iteration update for\nQ values as shown in lines 14-21 of Alg. 2.\nOne advantage of this framework is the introduction of\nmodel-based characterization by MFPT-VI. MFPT-VI very\nwell assesses the importance of states based on their MFPT\nvalues and thus provides a natural basis for effectively\nprioritizing states while updating state-action Q values. This\nmechanism towards updating state-action Q values allow the\nalgorithm to converge with a very small number of iterations,\nwhich practically decrease the overall training time by a\nsigniﬁcant margin. The evaluation details are presented in\nSection V.\nD. Time Complexity Analysis\nQ-Learning and DYNA have a sample complexity\nO(|S| log |S|), where |S| denotes the number of states in\norder to obtain a policy arbitrarily close to the optimal one\nwith high probability [28] [29]. Calculation of the MFPT\nneeds to solve a linear system that involves matrix inversion\n(the matrix decomposition has a time complexity of O(|S|2.3)\nif state-of-the-art algorithms are employed [26], given that\nthe size of matrix is the number of states |S|). Therefore, for\neach iteration, the worst-case time complexity for both the\nMFPT-Q and MFPT-DYNA algorithms is O(|S|2.3). Note,\nin practical applications, since the expensive MFPT is used\nfor summerizing global features, this part is usually used\nsparsely (less frequently) which also saves much time for\ncomputation.\nV. EXPERIMENTAL RESULTS\nIn this section, we compare the performance of our pro-\nposed MFPT-Q and MFPT-DYNA with their corresponding\nbaseline methods - Q-Learning and DYNA respectively. More\nimportantly, through the comparison, we wish to demonstrate\nthat our proposed feature characterization mechanism can be\n(a)\n(b)\nFig. 2.\n(a) Demonstration of simulation environment, with the\nagent’s initial state (pink) and the goal state (red). Green blocks are\nobstacles; (b) A trajectory (blue) completed by the agent to reach\nthe goal.\nused as a module to existing other popular frameworks (not\nlimited to Q or DYNA) in order to further speed up their\npractical learning processes.\nA. Experimental Setting\nTask Details: We validated our method through numerical\nevaluations with two types of simulation suites running on a\nLinux machine.\nFor the ﬁrst task, we developed a simulator in C++ using\nOpenGL. To obtain the discrete MDP states, we tessellate\nthe agent’s workspace into a 2D grid map, and represent\nthe center of each grid as a state. In this way, the state\nhopping between two states represents the corresponding\nmotion in the workspace. Each non-boundary state has a total\nof nine actions, i.e., a non-boundary state can move in the\ndirections of N, NE, E, SE, S, SW, W, NW, plus an idle\naction returning to itself. A demonstration is shown in Fig. 1.\nSuch environmental setting allows us to better visualize the\ncharacterized reachability landscape, with a small number of\nstates in 2D space.\nFor the second task, we developed a simulator in C++ using\nROS and Rviz [30]. The agent’s workspace is partitioned into\na 3D voxel map where the center of each voxel denotes a MDP\nstate. Each non-boundary state has a total of seven actions, i.e.,\na non-boundary state can move in the directions of N, E, S, W,\nTOP, BOTTOM plus an idle action returning to itself. Such\na 3D environment setting is more complex compared to the\n2D setting. Moreover, it can be leveraged to simulate various\nrobotic path-planning application scenarios like quadrotor\ntrajectory planning and demonstrate the practicality of our\nproposed algorithms for such tasks. A demonstration of the\nagent ﬂying in the 3D simulation environment is shown in\nFig. 2.\nIn both tasks, the objective of the agent is to reach a goal\nstate from an initial start location in the presence of obstacles.\nThe reward function for both setups is represented as high\npositive reward at the goal state and -1 for other states. All\nexperiments were performed on a laptop computer with 8GB\nRAM and 2.6 GHz quad-core Intel i7 processor.\n(a)\n(b)\n(c)\n(d)\nFig. 3.\nStatistics of computational time and number of iterations required to converge between the baseline methods and our proposed\nalgorithms, with changing numbers of states (x-axis) in the 2D simulation environment. (a) and (c) Variants of Q-Learning methods. (b)\nand (d) DYNA variants. The thick green curve in two ﬁgures is the result of MFPT-based hybrid models.\nEvaluation Methods: We are concerned about both com-\nputational performance and real-world training performance.\nThus, we designed two evaluation metrics:\n• For the ﬁrst metric, we look into the computational\ncosts of the proposed and baseline approaches where we\ninvestigate the number of iterations required to converge\nas well as the computational runtime used to generate\nthe result.\n• For the second evaluation metric, we evaluate and\ncompare the actual time used for training and completing\na task. We do this because the robot needs to interact\nwith the real world, and the time spent on training\nwith the real world experience can be much more than\ncomputational time cost. Unsurprisingly, such saving\nalso extends to other costs such as energy if the task\ncan be done more quickly.\nB. 2D Grid Setup\nIn this setup, we compare our proposed algorithms with\ntheir corresponding baseline algorithms in terms of their\ncomputational runtimes as well as the numbers of iterations\nrequired to converge.\nComputational Time Cost: We compare the compu-\ntational time taken by the algorithms as the number of\nstates change. The time taken by Q-Learning and MFPT-\nQ algorithms to converge to the optimal solution (with the\nsame convergence error threshold) are shown in Fig. 3(a).\nThe results reveal that time taken by our proposed MFPT-Q to\nconverge is faster than Q-Learning when the number of states\nare less than 5000. When the number of states exceed 5000,\nQ-Learning takes lesser time to converge than our proposed\nMFPT-Q. Fig. 3(b) compares the time taken by DYNA and\nMFPT-DYNA. Here, we observe that our proposed MFPT-\nDYNA takes more time to converge in comparison to DYNA.\nThe reason is due to the increased time taken for MFPT\ncalculation which dominates the time taken to converge in\nthe execution of MFPT-DYNA.\nNumber of Iterations: We then evaluate the number of\niterations taken by the algorithms to converge to the optimal\npolicy since it very well reﬂects the sample efﬁciency.\nFig. 3(c) compares the number of iterations taken by Q-\nLearning and MFPT-Q, respectively. The plot clearly shows\nthat MFPT-Q takes much fewer iterations to converge in\ncomparison to the standard Q-Learning. Fig. 3(d) compares\nthe number of iterations taken by DYNA and MFPT-DYNA,\nfrom which we can observe that MFPT-DYNA takes much\nfewer itertations to converge than DYNA.\nThis also implies the remarkable merit of model-based\ncharacterization via MFPT as means for faster convergence\nin signiﬁcantly fewer iterations.\nC. 3D Grid Setup\nTo evaluate with larger number of states as well as more\ncomplex environments, we compare our proposed algorithms:\nMFPT-Q and MFPT-DYNA in the 3D simulation environment.\nComputational Time Cost: Fig. 4(a) presents the compu-\ntational time taken by Q-Learning and MFPT-Q algorithms\nto converge to the optimal solution. We can see that the\ncomputational time cost trends are very similar to that of\nthe 2D case, where for a large number of states, the MFPT\nvariants take longer time than the baseline methods. We\nattribute this to the fact that as the number of states increase,\n(a)\n(b)\n(c)\n(d)\nFig. 4.\nComparisons of computational time and number of iterations required to converge between the baseline methods and our proposed\nalgorithms in the 3D simulation environments. (a) and (c) Variants of Q-Learning methods. (b) and (d) DYNA variants.\n(a)\n(b)\n(c)\n(d)\nFig. 5.\nAgents progress during the training process via qlearning. (a) Policy learned after 500 trials; (b) Policy learned after 5000 trials;\n(c) Policy learned after 50000 trials; (d) Optimal policy learned by the agent. A trajectory (blue) completed by the agent to reach the goal.\nthe time taken for MFPT calculation also increase which\ndominates the computational time cost in the MFPT variants.\nNumber of Iterations: Similarly, we also compare the\nnumber of iterations taken by Q-Learning and MFPT-Q, re-\nspectively. As shown in Fig. 4(c), our proposed MFPT variant\nconverges in fewer trials compared to Q-Learning. Next, we\ncompare the number of iterations taken by DYNA and MFPT-\nDYNA. Again, the advantage of our proposed hybrid RL\napproach that introduces a model-based characterization into\nDYNA, is clearly visible in Fig. 4(d), as the results show that\nthe MFPT-DYNA requires much smaller number of iterations\nto converge compared to DYNA.\nD. Training Runtime Performance\nAs previously discussed, the objective of RL for an agent\nis to learn an optimal policy in a given environment in order\nto reach a goal state from a given starting location. Here\nwe present our second evaluation metric that considers the\ntotal time involved during the agent’s training process in the\nsimulation environment.\nFig. 5 shows the progress of an agent in the 3D environment\nusing Q-Learning. During the initial stages of the learning\nprocess, the agent could hardly overcome the ﬁrst obstacle\nas shown in Fig. 5(a). After 5000 trials, the agent could\novercome the ﬁrst obstacle, however was unable to overcome\nthe next one. At the end of the training process, the agent\nlearned the optimal policy using which it could overcome all\nobstacles and reach the goal as shown in Fig. 5(d).\nWe trained an agent in the 3D environment under varying\nvoxel sizes. Fig. 6(a) shows that the agent takes much less\noverall time to learn the optimal policy when MFPT-Q was\nemployed in comparison to classical Q-Learning. Similarly,\nthe agent takes much less time to learn and complete the task\nusing MFPT-DYNA in comparison to DYNA algorithm as\nshown in Fig. 6(b).\nSince, practically the training and learning efforts are\nmuch more expensive and important than the computational\ntime cost, thus, these results reestablish the beneﬁts of our\n(a)\n(b)\nFig. 6.\nTime required for agent’s training in the 3D environment.\n(a) Variants of Q-Learning methods. (b) DYNA variants. The dark\nblue bar in two ﬁgures is the result of MFPT-based hybrid models.\nhybrid algorithms towards improving sample efﬁciency in\ngoal-directed reinforcement learning. Such faster convergence\nand lesser training time is owing to the underlying mechanism\nof model-based characterization via MFPT introduced to the\nexisting reinforcement learning schemes.\nVI. CONCLUSIONS\nIn this paper, we propose a hybrid approach where we\nintroduced a new model-based characterization that can be\nextended to reinforcement learning techniques in order to im-\nprove its sample efﬁciency. We achieved this by synthesizing\nthe advantages of both model-free and model-based reinforce-\nment learning paradigms. The proposed hybrid framework can\nfurther accelerate reinforcement learning approaches, via an\nintegration of the MFPT feature characterization mechanism.\nThe experimental results show the remarkable merit of model-\nbased characterization in our hybrid algorithms which learn\nmuch faster with fewer samples in comparison to their state-\nof-the-art reinforcement learning counterparts.\nREFERENCES\n[1] Arthur P. de S. Braga and Aluizio F. R. Araújo.\nGoal-directed\nreinforcement learning using variable learning rate. In Flávio Moreira\nde Oliveira, editor, Advances in Artiﬁcial Intelligence, pages 131–140,\nBerlin, Heidelberg, 1998. Springer Berlin Heidelberg.\n[2] Sven Koenig and Reid G. Simmons. The effect of representation and\nknowledge on goal-directed exploration with reinforcement-learning\nalgorithms. Machine Learning, 22(1):227–250, Mar 1996.\n[3] Jens Kober, J. Andrew Bagnell, and Jan Peters. Reinforcement learning\nin robotics: A survey. The International Journal of Robotics Research,\n32(11):1238–1274, 2013.\n[4] J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel. Trust\nRegion Policy Optimization. ArXiv e-prints, February 2015.\n[5] Marc Peter Deisenroth, Gerhard Neumann, and Jan Peters. A survey on\npolicy search for robotics. Found. Trends Robot, 2(1&#8211;2):1–142,\nAugust 2013.\n[6] Steven D. Whitehead and Dana H. Ballard. Learning to perceive and\nact by trial and error. Machine Learning, 7(1):45–83, Jul 1991.\n[7] Christopher J. C. H. Watkins and Peter Dayan. Q-learning. In Machine\nLearning, pages 279–292, 1992.\n[8] G. A. Rummery and M. Niranjan. On-line q-learning using connec-\ntionist systems. Technical report, 1994.\n[9] Shoubhik Debnath, Lantao Liu, and Gaurav Sukhatme. Reachability\nand differential based heuristics for solving markov decision processes.\nIn Proceedings of 2017 International Symposium on Robotics Research.\nforthcoming.\n[10] Craig Boutilier, Ronen I. Brafman, and Christopher Geib. Structured\nreachability analysis for markov decision processes. In Proceedings\nof the Fourteenth Conference on Uncertainty in Artiﬁcial Intelligence,\nUAI’98, pages 24–32, San Francisco, CA, USA, 1998. Morgan\nKaufmann Publishers Inc.\n[11] Y. Chebotar, K. Hausman, M. Zhang, G. Sukhatme, S. Schaal, and\nS. Levine.\nCombining Model-Based and Model-Free Updates for\nTrajectory-Centric Reinforcement Learning. ArXiv e-prints, March\n2017.\n[12] S. Bansal, R. Calandra, K. Chua, S. Levine, and C. Tomlin. MBMF:\nModel-Based Priors for Model-Free Reinforcement Learning. ArXiv\ne-prints, September 2017.\n[13] Richard S. Sutton.\nIntegrated architectures for learning, planning,\nand reacting based on approximating dynamic programming. In In\nProceedings of the Seventh International Conference on Machine\nLearning, pages 216–224. Morgan Kaufmann, 1990.\n[14] Richard S. Sutton. Planning by incremental dynamic programming.\nIn In Proceedings of the Eighth International Workshop on Machine\nLearning, pages 353–357. Morgan Kaufmann, 1991.\n[15] F. Farshidian, M. Neunert, and J. Buchli. Learning of closed-loop\nmotion control.\nIn 2014 IEEE/RSJ International Conference on\nIntelligent Robots and Systems, pages 1441–1446, Sept 2014.\n[16] A. Nagabandi, G. Kahn, R. S. Fearing, and S. Levine. Neural Network\nDynamics for Model-Based Deep Reinforcement Learning with Model-\nFree Fine-Tuning. ArXiv e-prints, August 2017.\n[17] P. L. Donti, B. Amos, and J. Zico Kolter. Task-based End-to-end\nModel Learning in Stochastic Optimization. ArXiv e-prints, March\n2017.\n[18] S. Bansal, R. Calandra, T. Xiao, S. Levine, and C. J. Tomlin. Goal-\nDriven Dynamics Learning via Bayesian Optimization. ArXiv e-prints,\nMarch 2017.\n[19] John G Kemeny, Hazleton Mirkill, J Laurie Snell, and Gerald L\nThompson. Finite mathematical structures. Prentice-Hall, 1959.\n[20] Andrew W. Moore and Christopher G. Atkeson. Prioritized sweeping:\nReinforcement learning with less data and less time.\nIn Machine\nLearning, pages 103–130, 1993.\n[21] David Andre, Nir Friedman, and Ronald Parr. Generalized prioritized\nsweeping. Advances in Neural Information Processing Systems, 1998.\n[22] David Wingate and Kevin D Seppi.\nPrioritization methods for\naccelerating mdp solvers. Journal of Machine Learning Research,\n6(May):851–881, 2005.\n[23] Leslie Pack Kaelbling, Michael L. Littman, and Andrew W. Moore.\nReinforcement learning: A survey. J. Artif. Int. Res., 4(1):237–285,\nMay 1996.\n[24] Richard S. Sutton. Learning to predict by the methods of temporal\ndifferences. Machine Learning, 3(1):9–44, Aug 1988.\n[25] David Assaf, Moshe Shared, and J. George Shanthikumar. First-passage\ntimes with pfr densities. Journal of Applied Probability, 22(1):185–196,\n1985.\n[26] Gene H. Golub and Charles F. Van Loan. Matrix Computations (3rd\nEd.). Johns Hopkins University Press, Baltimore, MD, USA, 1996.\n[27] Craig Boutilier, Richard Dearden, and Moisés Goldszmidt. Stochas-\ntic dynamic programming with factored representations.\nArtiﬁcial\nintelligence, 121(1):49–107, 2000.\n[28] Michael Kearns and Satinder Singh. Finite-sample convergence rates\nfor q-learning and indirect algorithms. In Proceedings of the 1998\nConference on Advances in Neural Information Processing Systems II,\npages 996–1002, Cambridge, MA, USA, 1999. MIT Press.\n[29] Luis C. Cobo, Charles L. Isbell, and Andrea L. Thomaz.\nObject\nfocused q-learning for autonomous agents.\nIn Proceedings of the\n2013 International Conference on Autonomous Agents and Multi-\nagent Systems, AAMAS ’13, pages 1061–1068, Richland, SC, 2013.\nInternational Foundation for Autonomous Agents and Multiagent\nSystems.\n[30] Rviz: 3d visualization tool for ros. http://wiki.ros.org/rviz.\nAccessed: 2018-02-27.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2019-01-04",
  "updated": "2019-01-04"
}