{
  "id": "http://arxiv.org/abs/1901.02144v1",
  "title": "Guidelines and Benchmarks for Deployment of Deep Learning Models on Smartphones as Real-Time Apps",
  "authors": [
    "Abhishek Sehgal",
    "Nasser Kehtarnavaz"
  ],
  "abstract": "Deep learning solutions are being increasingly used in mobile applications.\nAlthough there are many open-source software tools for the development of deep\nlearning solutions, there are no guidelines in one place in a unified manner\nfor using these tools towards real-time deployment of these solutions on\nsmartphones. From the variety of available deep learning tools, the most suited\nones are used in this paper to enable real-time deployment of deep learning\ninference networks on smartphones. A uniform flow of implementation is devised\nfor both Android and iOS smartphones. The advantage of using multi-threading to\nachieve or improve real-time throughputs is also showcased. A benchmarking\nframework consisting of accuracy, CPU/GPU consumption and real-time throughput\nis considered for validation purposes. The developed deployment approach allows\ndeep learning models to be turned into real-time smartphone apps with ease\nbased on publicly available deep learning and smartphone software tools. This\napproach is applied to six popular or representative convolutional neural\nnetwork models and the validation results based on the benchmarking metrics are\nreported.",
  "text": " \n1 \n  \nAbstract— Deep learning solutions are being increasingly used \nin mobile applications. Although there are many open-source \nsoftware tools for the development of deep learning solutions, \nthere are no guidelines in one place in a unified manner for using \nthese tools towards real-time deployment of these solutions on \nsmartphones. From the variety of available deep learning tools, the \nmost suited ones are used in this paper to enable real-time \ndeployment of deep learning inference networks on smartphones. \nA uniform flow of implementation is devised for both Android and \niOS smartphones. The advantage of using multi-threading to \nachieve or improve real-time throughputs is also showcased. A \nbenchmarking framework consisting of accuracy, CPU/GPU \nconsumption and real-time throughput is considered for \nvalidation purposes. The developed deployment approach allows \ndeep learning models to be turned into real-time smartphone apps \nwith ease based on publicly available deep learning and \nsmartphone software tools. This approach is applied to six popular \nor representative convolutional neural network models and the \nvalidation results based on the benchmarking metrics are \nreported.  \nIndex Terms— Deployment of deep learning models on \nsmartphones, real-time smartphone apps of deep learning models, \nbenchmarking deep learning apps on smartphones \nI. INTRODUCTION \nDeep learning has had a dramatic impact on advancing the \nfield of machine learning [1]. It has pushed the state-of-the-art \nbeyond what conventional approaches have achieved in various \napplications such as object detection [2], object localization [3], \nand speech recognition [4]. The expansion in the use of deep \nlearning has been fueled by increases in the computational \npower of processors, in particular graphics processing units \n(GPUs), and the availability of large datasets for training. \nDeep learning involves Deep Neural Networks (DNNs) \nconsisting of a cascade of non-linear processing units arranged \nin layers by which an increasing level of data abstraction is \nenabled at deeper layers. This is of particular importance in \nclassification and regression tasks due to the fact that raw or \nminimally processed data can get processed without the need to \nperform feature extraction as compared to conventional \napproaches that normally require obtaining hand-crafted \nfeatures first. DNNs are able to learn optimal features \n \nA. Sehgal and N. Kehtarnavaz are with the Department of Electrical and \nComputer Engineering, University of Texas at Dallas, Richardson, TX 75080, \nUSA. E-mail: {abhishek.sehgal, kehtar}@utdallas.edu. \nthemselves for a particular task and have provided state-of-the-\nart accuracies in computer vision, speech recognition, natural \nlanguage processing applications among others. \nIn terms of implementation platforms, smartphones have \nemerged as a ubiquitous and mobile computing device with \nmore than 2.5 billion people worldwide owning them [5]. Apart \nfrom being equipped with multi-core CPUs and GPUs, \nsmartphones contain a plethora of sensors which do not require \ninterfacing hardware as compared to other popular platforms \nsuch as Arduino [6] and Raspberry Pi [7]. In addition, there \nexist well developed and supported Application Programming \nInterfaces (APIs) for smartphones which have been optimized \nfor performance. \nSmartphones constitute the highest users of deep learning-\nbased solutions spanning various applications such as voice \nassistants, automatic text prediction, and augmented reality. \nAlso, they are used as research platforms to run deep learning \nsolutions involving different applications such as concussion \ndetection [8], jaundice diagnosis [9], schizophrenia recognition \n[10] and voice activity detection for hearing studies [11] among \nothers. These solutions either rely on server-side processing or \nperform offline simulations on the data that are previously \ncollected using smartphones. Real-time deployment of such \ndeep learning solutions on smartphones has been fairly limited \nin the literature. Manual coding of DNNs to run in real-time on \nsmartphones is cumbersome and time consuming. This paper \nmakes the process of deployment of deep learning algorithms \non smartphones easy by providing in one place the steps needed \nto bridge the gap between development and deployment based \non the publicly available software tools. \nThere have been some works in the literature on the \ndevelopment of deep learning solutions that are aimed \nspecifically at mobile implementation [12],[13],[14],[15]. On-\ndevice deep learning engines are also finding their way into \nsmartphones. For example, Apple has introduced a neural \nengine as part of the A11 Bionic chip and Huawei has \nintroduced the Kirin 970 neural processing unit (NPU). The \nsmartphone industry is also working toward dedicated \nprocessors to speed up on-device deep learning in contrast to \ncloud servers in order to cope with real-time implementation \nissues and the need for internet connection. In addition, on-\nGuidelines and Benchmarks for \nDeployment of Deep Learning Models on \nSmartphones as Real-Time Apps \nAbhishek Sehgal and Nasser Kehtarnavaz \n \n2 \ndevice deep learning helps to alleviate security or privacy \nconcerns due to data storage on servers. Considering that \nsmartphones \nare \nequipped \nwith \nmulti-core \nCPUs, \nmultithreading is used here to reduce computation time towards \nachieving real-time throughputs. \nFurthermore, the open-source deep learning software tools \nhave reached a maturation point in terms of libraries for on-\ndevice deep learning deployment. However, there exists a steep \nlearning curve associated with the deployment of these software \ntools and an absence of benchmarking guidelines for \nsmartphones. Although previous works have addressed \nefficient processing techniques for the purpose of running DNN \nmodels on smartphones, thus far no step-by-step guidelines or \nbenchmarks have been provided regarding the real-time \ndeployment of DNN models on smartphones. This paper aims \nat bringing such information into one place or under one \numbrella, thus providing a unified approach to easily deploy \ntrained deep learning models as apps on Android and iOS \nsmartphones with a focus on their real-time operation. This \nwork enables smartphones to be used as a portable research \nplatform for deep learning studies. \nTowards this objective, the rest of the paper is organized as \nfollows: Section II describes the most suited deep learning \nlibraries for smartphone deployment at the time of this writing, \ndeployment steps based on the smartphone operating system, \nthe software tools used to build deep learning apps, and the \nsmartphone devices used to showcase a number of \nrepresentative deep learning models. In Section III, the DNN \nmodels and the benchmarking criteria used for validation are \ndiscussed. The use of multi-core CPUs on the smartphones to \nachieve or improve real-time throughputs through multi-\nthreading is also discussed in this section. Section IV provides \nthe validation results and their discussion. Finally, the paper is \nconcluded in Section V. \nII. DEPLOYMENT OF DNN MODELS ON SMARTPHONES \nThis section discusses how to deploy DNN models on \nsmartphones using publicly and freely available software tools. \nThe steps discussed is aimed at turning DNN models into apps \nfor both Android and iOS smartphones in a unified manner. \nA. Deep Learning Software Tools \nThe rise of deep learning has been accelerated by the \nintroduction of various publicly and freely available libraries. \nThe main libraries that are widely used include Caffe [16] \n(developed by Berkeley AI Research), TensorFlow [17] \n(developed by Google), PyTorch [18] (developed by \nFacebook), and CNTK [19] (developed by Microsoft). These \nlibraries support Python for the purpose of training and \nprototyping models. Even though such a wide collection of \nprototyping tools is available to train and develop deep learning \nmodels, researchers often wonder where to begin. Here, three \npublicly available libraries most suited for the task under \nconsideration are selected from the available deep learning \nlibraries. These libraries are selected based on (i) their easy \nportability to mobile devices and (ii) active support by their \ndevelopers. In what follows, these libraries and their \nframeworks are briefly described. \nTensorFlow is a dataflow programming library. It expresses \ncomputations as stateful dataflow graphs, enabling users to \ndefine a neural network as a graph of operations that can be \nexecuted on input data streams. Data are represented as \nmultidimensional arrays or “tensors” thus the name \nTensorFlow. The underlying benefit of defining a neural \nnetwork as a graph is that the computations and memory usage \nare highly optimized and they can be parallelized using multiple \nCPUs and GPUs and implemented across a variety of hardware \nplatforms. As TensorFlow and Android are both developed by \nGoogle, TensorFlow models can be integrated into the Android \nsoftware environment with ease by adding the “TensorFlow for \nMobile” library module as a dependency. A similar tool named \n“TensorFlow Lite” is also available, but currently it remains \nexperimental and does not support as many operations as \nTensorFlow for Mobile. Furthermore, it is worth noting that the \ndeployment flow for TensorFlow Lite is similar to TensorFlow \nfor Mobile. \nAnother widely used library is called Keras [20], which is a \nhigher-level library written in Python where TensorFlow or \nCNTK can be used as its backend. Keras makes development \nof models easier and faster by providing the building blocks for \ncommon-use DNN layers, a simple coding syntax, and tools to \neasily preprocess data. As Keras can use TensorFlow as its \nbackend, the model trained using Keras is essentially a \nTensorFlow model which can be extracted and used in Android \napps. \nCoreML [21] is a software framework developed by Apple \nto run machine learning models on iOS devices. It has a Python-\nbased tool called CoreMLTools [22] which allows one to \ntranslate existing machine learning models into CoreML \nsupported models. This conversion capability allows the \nconversion of Keras models into CoreML models which can \nthen be implemented as an app on iOS devices or iPhones. A \nconverter developed by TensorFlow (tf-coreml) [23] also \nallows converting TensorFlow models into CoreML models. \nThe latest version of CoreMLTools at the time of this writing \nis 0.8, which supports Keras version 2.1.3 and TensorFlow \nversion 1.5. These versions are utilized here for the results \nreported in the paper. \nB. Deployment Steps \nThe steps that are needed for deployment of deep learning \nmodels on smartphones are showcased in Fig. 1. Keras can be \nconsidered to be the primary prototyping library as it can be \neasily converted to CoreML models for iOS and the underlying \nTensorFlow backend model can be extracted from it for \nAndroid. In case of TensorFlow models, a secondary path \n(marked in red in Fig. 1) is also provided by using the converter \ntf-coreml \nto \nconvert the \nmodels into \nCoreML \nfor \nimplementation on iOS smartphones. In case of models trained \nusing different libraries, several publicly available converter \ntools [24] are available to convert them to Keras or TensorFlow. \nCoreML also provides conversion tools for models trained by \nthe deep-learning libraries other than Keras. \nModels are stored as inference only and all training related \nlayers are removed to allow only the feed-forward path of a \nnetwork model to execute. Since layers are stored as \ncomputational graphs, this allows their optimization for the \nplatform they are going to run on. The flowcharts provided in \n \n3 \nFigs. 2 and 3 depict the steps needed to convert a trained Keras \nor TensorFlow model to a model for deployment on Android \nand iOS smartphones. The flowcharts appearing in Fig. 4 show \nthe steps needed to create an Android or iOS app from a \nconverted model. \n \n1) Model Generation – iOS \nTo create a CoreML model, a Keras model needs to be \ntrained first or a pre-trained model needs to be considered first. \nKeras models are usually saved as a .h5 file which denotes the \nHierarchical Data Format (HDF). This allows using the same \ntrained model across different backends. This file format stores \nthe architecture of the graph and the weights of the graph \ntensors as numerical arrays. After the model is loaded into \nPython, the CoreMLTools python library can be used to convert \nthe Keras model to a CoreML model. The converter provides \nthe option to specify the input as an image or as a multi-\ndimensional array. In case of image inputs, the converter \nprovides the option to define the pre-processing parameters \nused for that model. As the pre-processing varies across \ndifferent models, this option is highly useful since it allows a \nmodel to be used on raw-images and easy switching without \nexplicitly implementing pre-processing for each different \nmodel. \nThe converted CoreML model is stored as a .mlmodel file. \nThis file encapsulates a MLModel class which can be directly \ninstantiated allowing the model to be used as a plug-and-play \nmodel. The CoreML API handles all the underlying DNN \ncomputation removing the overhead required for coding a \nneural network from scratch. As a result, the user can focus on \ndeploying and testing the model rather than implementing the \nneural network. \n \n2) Model Generation – Android \nTensorFlow for Mobile provides a Gradle build dependency \n[25]. This allows using predefined functions for inference via \njust a trained model. A TensorFlow model gets stored as a .pb \nfile which denotes the Protocol Buffer file format. Similar to \nHDF, this format also stores the architecture of a model and its \ntrained weights. The TensorFlow for Mobile inference interface \ncreates a model based on a .pb file which can then be executed \non Android smartphones using the predefined functions \nincluded in the dependency. \nTo extract a TensorFlow model from a Keras model, first the \nvariables in the model need to be converted to constants. \nVariable tensors are only required for training as they get \nupdated based on the back-propagation input. For inference \nonly models, these tensors need to be constant. This can be done \nby the graph utility sub-module (graph_util) in TensorFlow. To \nconvert a graph, first the underlying TensorFlow session created \nby Keras needs to be accessed. This can be achieved by using \nthe backend module in Keras. Then, the session graph can be \ninputted to the graph_util constant converter function to obtain \nthe graph with constant weights. This constant graph can be \nsaved as a .pb file using the graph input/output sub-module \n(graph_io), which can get imported into an Android app using \nthe TensorFlow inference interface. This interface creates a \nsession similar to the TensorFlow session in Python and handles \nall the required DNN computations. The input image or data \ncan then be fed into the session to extract the output of the \nmodel from the session. Compared with CoreML, one needs to \nexplicitly pre-process the image in this method. However, an \nadvantage that TensorFlow for Mobile has is that the output of \nany of the intermediate layers can be extracted, whereas in \nCoreML only the output of the model can be extracted. Unlike \nCoreML, one needs to manually set up the model by explicitly \nfeeding the .pb file, the input and output node names, and the \nsize of the input.  \nTreating TensorFlow as the primary framework for DNN \ndevelopment, the converter tf-coreml can be used to convert a \n.pb model into a .mlmodel file. This converter tool provides a \nreduced set of computations and this set can be seen on the \nGitHub page of the converter for the purpose of altering the \nmodel if any unsupported computations are seen. \nC. Smartphone Software Tools \nTo demonstrate on-device deep learning inference, both \nAndroid and iOS smartphones are considered in this work to \n \n \nFig. 1.  Diagram illustrating the utilization of publicly available software tools for deployment of deep learning solutions on smartphones – the blue lines \nillustrate the path when Keras is used as the primary framework for training deep learning models and the red lines illustrate the path when TensorFlow is \nused as the primary framework for training deep learning models. \n \n4 \nform a unified approach. These two operating systems have a \ncombined market share of 96% of smartphones worldwide [26]. \nAdditionally, the developer tools for both of these smartphones \noperating systems are available online for free and are well \nmaintained by their respective organizations.  \nTo develop Android apps, the Android Studio IDE [27] is \nused which is available for all operating systems. The language \nof choice for Android development is Java, which is used here \nto develop apps to run DNN models on Android smartphones. \nAndroid apps can also be packaged as executable Android \nApplication Package (APK) files for deployment on any \nAndroid smartphone.  \niOS apps can be developed and deployed on an iOS device \nor iPhone only via a macOS machine running the Xcode IDE \n[28]. iOS apps are developed using the Swift or Objective-C \nprogramming language. To deploy iOS apps on an iPhone, one \nneeds to be registered as an Apple Developer. \nD. Smartphone Processors \nFor running DNN models, two modern smartphones of Pixel \n2 and iPhone 8 are used in this work as sample Android and iOS \nsmartphones, respectively.  \nPixel 2 is developed by Google. It runs Android operating \nsystem and possesses a Qualcomm Snapdragon 835 64-bit \nARM-based octa-core system on a chip (SoC). Its CPU clock \nspeed varies between 1.9-2.35 GHz depending on the core \nbeing used. The internal memory of this smartphone is 4 GB \nLPDDR4x RAM. It also possesses an Adreno 540 GPU. Note \nthat TensorFlow for Mobile does not utilize this GPU. The Pixel \n2 smartphone used here runs the latest Android version 8.1.0 at \nthe time of this writing. \nOn the iOS side, iPhone 8 is used here which incorporates \nthe Apple A11 Bionic 64-bit ARM-based hexa-core SoC with a \nmaximum CPU clock rate of 2.39 GHz. The internal memory \nof iPhone 8 is 2 GB of LPDDR4x RAM. The A11 chip also \ncontains a dedicated neural engine which can be used to run \nmachine learning models more efficiently than using plain \nGPU. The neural engine is capable of performing up to 6000 \nbillion operations per second. The iPhone 8 smartphone used \nhere runs the latest iOS version 11.4 at the time of this writing. \nIII. DNN SMARTPHONE APPS AND BENCHMARKING METRICS \nA. DNN Models \nIn this section, the steps involved in turning DNN models to \nsmartphone apps are applied to six popular Convolutional \nNeural Networks (CNNs) and a benchmarking framework of \nthese models is discussed. CNNs are a class of DNNs where the \nprimary computation involves convolution. The convolution \nlayers (CONV) in CNNs are able to provide a higher level of \nabstraction by creating feature extraction kernels similar to \nthose used in image processing. By stacking convolution layers, \na CNN model is able to extract unique information related to a \nparticular image or matrix input. CNN models currently provide \nthe state-of-the-art solutions in many image processing, \n \n \nFig. 2.  Flowchart depicting the steps needed to convert a Keras model \ninto a smartphone deployable model for Android and iOS smartphones. \n \n \nFig. 3.  Flowchart depicting the steps needed to convert a TensorFlow \nmodel into a smartphone deployable model for Android and iOS \nsmartphones.  \n \n5 \ncomputer vision, speech and audio processing applications. In \nthis paper, six popular or representative CNNs are benchmarked \nbased on the MNIST [29] dataset, which is considered to be a \ngateway dataset for exploring deep learning, and the widely \nused ImageNet Large Scale Visual Recognition Competition \n(ILSVRC) [30] dataset. These networks are briefly described \nbelow for the sake of completeness.  \nLeNet was introduced in [31] for digit classification trained \non the MNIST dataset. The summary of the model is provided \nin Table I. It is designed to classify grayscale images of \ndimensions 28 × 28 into single digits using 2 CONV layers \nfollowed by 3 fully-connected (FC) layers. To reduce the \ndimensions of the intermediate feature maps, 2 × 2 average \npooling is utilized. The activation function used in LeNet is \nsigmoid.  \nResNet [32] or Residual Network is a CNN model that \ninclude so called “skip” or “shortcut” connections which allow \nbypassing the weight layers using identity mappings. The \noutput of the weight layers and the identity mapping are then \nadded together. The skip connections of ResNet are critical in \npreventing the gradient from vanishing in deep layers, as the \nidentity mapping prevents the backpropagation error from \nshrinking. The weight layers in ResNet usually consist of two \n3 × 3 CONV layers. To reduce the number of parameters in \neach weight layer, ResNet also uses so called “bottleneck” \nlayers by using 1 × 1 filters. The bottleneck layers replace the \ntwo layers with three layers of 1 × 1, 3 × 3 and 1 × 1 filters. \nThe 1 × 1 filters are used to decrease and then increase the \nnumber of weights. ResNet-152 was selected as the winner of \nthe ILSVRC 2015 challenge, surpassing human level accuracy \nwith a top-5 accuracy of 3.57% on the test set provided. Here, \nthe ResNet-50 model is used which has 1% less accuracy than \nResNet-152, but with 2.5 times fewer parameters (approx. 25.6 \nmillion). It consists of a CONV layer followed by 16 bottleneck \nlayers and a FC layer. \nInceptionV3 [33] is the extension of GoogLeNet [34] that \nwas selected as the winner of the ILSVRC 2014 challenge. \nGoogLeNet is based on the inception module which consists of \n4 parallel CONV layers of 1 × 1 CONV, 3 × 3 and 1 ×\n1 CONV, 5 × 5 and 1 × 1 CONV, and 1 × 1 CONV followed \nby 3 × 3 max-pooling. GoogLeNet achieved a top-5 error of \n6.65% on the ILSVRC challenge test set. InceptionV3 includes \nthe following 3 new inception modules: (i) In the first module, \nthe 5 × 5 CONV is replaced with two 3 × 3 CONV to reduce \nthe number of parameters in the module. (ii) In the second \nmodule, the n×n CONV layer is replaced with an n×1 CONV \nfollowed by a 1×n. This reduces the number of weights and thus \nthe computational cost. For the results reported in the next \nsection, n is considered to be 7. (iii) The third module separates \nthe initial 3 × 3 CONV in the first inception module into two \nparallel 3 × 1 and 1 × 3 CONV layers. This module is used to \npromote high dimensional sparse representations and is placed \nlast after the other two modules. \nInceptionV3 consists of 6 CONV layers followed by 3 of the \nfirst modified inception modules, 5 of the second modified \ninception modules, 2 of the third modified inception modules \nand a FC layer. It also contains an auxiliary classifier [34], \nwhich is an inception module on the output of the second \nmodified inception module with a batch-normalized [35] FC \nlayer to increase accuracy. InceptionV3 achieves a top-5 \naccuracy similar to ResNet-152 while having only 23.8 million \n \n(a) \n \n(b) \nFig. 4.  Flowcharts depicting the steps needed for creating an (a) Android, \nand (b) iOS app from a converted model. \nTABLE I  \nSUMMARY OF LENET MODEL TRAINED ON MNIST DATASET \nLayer Information \nLeNet \nInput-Size \n28x28x1 \nCONV Layer \n \n# of CONV Layers \n2 \nDepth \n2 \nKernel Size \n5 \nStrides \n1 \n# of Channels \n1, 16 \n# of Filters \n16, 32 \nFC Layer \n \n# of FC Layers \n3 \n# of Channels \n128 - 3200 \n# of Filters \n10 - 256 \nParameters \n866K \nFLOPs \n28M \nModel Storage Memory \n3.5MB \nAccuracy \n99.43 \n \n6 \nparameters (similar to ResNet-50). \nSqueezeNet [36] is a CNN model designed for limited-\nmemory systems. It provides the accuracy of AlexNet [2]. This \nmodel was selected as the winner of the ILSVRC challenge \n2012 with a top-5 error of 9.8% with 50 times fewer parameters. \nSqueezeNet is built using “fire” modules which consist of two \nstacked layers: squeeze layer, and expand layer. The squeeze \nlayer is composed exclusively of 1 × 1 filters, which reduce the \nnumber of channels of the input to the module. The expand \nlayer is a mix of 1 × 1 and 3 × 3 filters, the outputs of which \nare concatenated after activation and fed into the next module. \nThe sparing use of 1 × 1 reduces the number of parameters of \nthe CNN model considerably, while still maintaining the \nbaseline accuracy. SqueezeNet has been compressed even \nfurther using Deep Compression [15], with the reduction in size \nof SqueezeNet being 510 times that of AlexNet with no loss in \naccuracy. The downside to this is that the compressed model \ncannot be used using the existing deep learning software tools. \nSqueezeNet consists of a CONV layer followed by 8 so-called \nfire modules and a CONV layer in the end. FC layers are not \nused as they have much higher number of parameters as \ncompared to CONV layers. \nMobileNet [14] is a CNN model that has been specifically \ndesigned for mobile and embedded vision applications. \nMobileNet modules reduce computations and memory by \ndividing a normal CONV layer into two parts: depthwise \nconvolution and pointwise convolution. These two parts \ntogether are called Depthwise Separable Convolution. In \ndepthwise convolution, the channel width of the filter is kept as \n1. The pointwise convolution uses 1 × 1 filters to expand the \nchannels of the output of the depthwise convolution. \nMobileNets can be modified by using width and resolution \nmultipliers, called model shrinking hyperparameters. The width \nmultiplier is used to reduce the channels of the network \nuniformly at each layer, thereby reducing the overall number of \nparameters of the layer. The resolution multiplier is implicitly \nset by changing the input resolution of the model, which \nreduces the computational cost of the model. Here, MobileNet \nis considered with a width multiplier of 1.0 and an input \nresolution of 224 × 224. It consists of a CONV layer followed \nby 13 depthwise separable convolution layers and a FC layer. \nDenseNet [37] or Densely Connected Networks is a CNN \nthat extends the residual learning framework introduced by \nResNets. In a DenseNet block, every layer is connected to all \nthe subsequent layers of equal feature map dimensions. Instead \nof additions, as done in ResNet, the input is concatenated. The \nCONV architecture used in DenseNet is similar to the \nbottleneck architecture in ResNet, where a 1 × 1 filter is used \nto reduce the number of the channels of the input before feeding \nit into the 3 × 3 CONV layer. Between every DenseNet block, \na compression/transition layer is used to reduce the number of \nfeature maps into the next DenseNet block. DenseNet requires \nconsiderably fewer number of parameters than ResNet to \nachieve similar accuracy. Here, DenseNet-121 is used, which \ngives 1% less accuracy than ResNet50 with 3 times fewer \nparameters. It consists of a CONV layer followed by 4 \nDenseNet Blocks with 3 transition layers between them \nfollowed by a FC layer. \nTable II shows a summary of the CNN models trained on the \nILSVRC challenge dataset. A comparison of the models based \non their top-5 accuracy, model-size and FLOPs is also displayed \nin Fig. 5. The depth and total number of CONV layers for \nSqueezeNet and Inception are different as they consist of \nparallel CONV layers in their modules. The number of floating-\npoint operations (FLOPs) represents how computationally \nexpensive a CNN model is. The FLOPs of the model are \ncomputed using the TensorFlow built-in profiler. The top-5 \naccuracy is computed using the pre-trained models in Keras \nwith the ILSVRC-2012 validation set, which consists of 50,000 \nimages. The accuracy reported here is taken based on a single \ncrop of the images. Accuracies reported in the literature usually \nuse multiple-crops. However, for real-time operation, a single \nTABLE II  \nSUMMARY OF POPULAR CNNS TRAINED ON THE ILSVRC CHALLENGE DATASET \nLayer Information \nResNet50 \nInceptionV3 \nSqueezeNet \nMobileNet \nDenseNet \nInput-Size \n224x224x3 \n299x299x3 \n227x227x3 \n224x224x3 \n224x224x3 \nCONV Layer \n \n \n \n \n \n# of CONV Layers \n49 \n95 \n26 \n27 \n120 \nDepth \n49 \n46 \n18 \n27 \n120 \nKernel Size \n1,3,7 \n1,3,5,7 \n1,3 \n1,3 \n1,3,7 \nStrides \n1,2 \n1,2 \n1,2 \n1,2 \n1,2 \n# of Channels \n3 - 2048 \n3 - 2048 \n3 - 1000 \n3 - 1024 \n3 - 1024 \n# of Filters \n64 - 2048 \n32 - 2048 \n16 - 1000 \n32 - 1024 \n32 - 1024 \nFC Layer \n \n \n \n \n \n# of FC Layers \n1 \n1 \n0 \n1 \n1 \n# of Channels \n2048 \n2048 \n0 \n1024 \n1024 \n# of Filters \n1000 \n1000 \n0 \n1000 \n1000 \nParameters \n25.6 M \n23.8 M \n1.2 M \n4.3 M \n8 M \nFLOPs \n7.7 B \n11.5 B \n714 M \n1.1 B \n5.7 B \nModel Storage Memory \n102 MB \n96 MB \n5 MB \n17MB \n33MB \nTop-5 Accuracy  \n(Single Crop) \n92.1% \n93.8% \n78.4% \n86.2% \n91.8% \n \n7 \ncrop accuracy is regarded as more realistic. One can see that \neven though InceptionV3 has fewer number of parameters \ncompared to ResNet, the number of FLOPs for the network is \nhigher due to a greater number of CONV layers. All of these \nmodels are available pre-trained via Keras and can be extended \nto various applications by using transfer-learning [38]. \nB. Multithreading \nDNNs are computationally very expensive. For real-time \noperation on smartphones, executing the model on the main \nthread causes delay in capturing of frames and thus reduces the \napp throughput. This would also lead to a reduced number of \nFPS (frames per second). For applications where a DNN model \ncan operate at slower rate than the frame rate of the app, \nmultithreading needs to be adopted. Noting that multi-core \nprocessors are used in modern smartphones, a DNN model can \nbe run on a secondary thread to create the needed computational \nbandwidth on the main thread to run the app at a desired FPS. \nThis technique was used previously in [11] to allow a DNN \nmodel to run on a parallel thread by removing the computation \nburden from the main audio thread and thus preventing any \naudio frames to get skipped. \nC. Benchmarking Metrics \n1) Accuracy \nThe ILSVRC-2012 validation dataset is used to validate the \nCNN models. This validation set consists of 50,000 images of \n1000 object categories. The DNN models are validated on the \nPC and smartphone platforms using the top-5 accuracy metric \non a single crop. It should be noted that the accuracy reported \nin the literature involves multiple crops with an ensemble of \nclassifiers which improves the accuracy. This is not possible for \nreal-time apps running on smartphones. In other words, a single \ncrop is more appropriate to consider when operating in real-\ntime. For LeNet, the MNIST test set is used for validation. This \nset consists of 10,000 images of handwritten digits. \n \n2) CPU/GPU Consumption \nFor smartphone apps, CPU/GPU consumption is critical as \nthis has a direct impact on the battery utilization. A higher \nconsumption metric has a higher impact on the battery \nutilization. As TensorFlow for Mobile currently only supports \nrunning on the CPU, the CPU consumption of the Android app \nis measured here. The CoreML API utilizes the neural engine \nof iPhone 8, which in turn utilizes the GPU for the parallel \ncomputations of a CNN model. The GPU consumption of the \niOS app is measured here as this is the way the majority of the \ncomputation is handled. \nAs far as LeNet is concerned, it is benchmarked as a non \nreal-time app, or the CPU/GPU consumption is not monitored \nlike the other real-time models. \n \n3) Real-Time Throughput \nTo evaluate the throughput of an app, the number of \nconsecutive frames is displayed and measured per second on \nthe screen (FPS). This is necessary for video-based apps that \ndemand smooth visual perception of video data with an FPS of \n24 or more. \nThe number of frames processed per second is also \nmeasured. This metric is highly dependent on the number of \nFLOPs in a model, as a higher number of FLOPs is directly \nproportional to a reduced number of frames processed per \nsecond and vice-versa. This metric is important to decipher \nwhich model is efficient to use. A model with a higher number \nof frames processed per second would be required for \napplications where throughput is critical. As can be seen in Fig. \n5, models with high FLOPs have a higher accuracy. Therefore, \nin applications where accuracy is critical, number of frames \nprocessed per second can be reduced. When the model is run at \nframe rate, the FPS and frames processed per second are the \nsame. When using multithreading, the number of frames \nprocessed per second is different than the FPS.  \nAs LeNet is generally not used in real-time, the time taken \nper image to be classified is considered here as its throughput \nmetric. \nIV. RESULTS AND DISCUSSION \nInitially, LeNet was first implemented as Android and iOS \napps using the developed approach. The accuracy of the \nvalidation set on the smartphone platforms was found to be \n99.43%, the same as the PC platform, with a processing time of \n5.66ms and 5.20ms per image on Pixel 2 and iPhone 8, \nrespectively. \nThen, the ILSVRC models were implemented as Android \nand iOS apps and the accuracy for the validation set consisting \nof 50,000 images of the ImageNet challenge dataset was \nexamined. As illustrated in Fig. 6, the accuracy for the \nimplemented apps was found to be practically the same, or \nwithin 0.5% difference. This slight difference is due to the fact \nthat the precision of floating-point numbers is handled \ndifferently in ARM and Intel-based processors.  \nNext, to examine the CPU/GPU consumption and \nthroughput, the models were run in real-time on the \nsmartphones. For Android, the CPU consumption was \ncomputed using the Android Profiler [39] of the Android Studio \nIDE and the GPU consumption for iOS was computed using the \n \n \nFig. 5.  Benchmarking metrics: Top-5 accuracy, FLOPs (in billions), and \nmemory size of the DNN models; circle sizes represent model sizes in \nTable II. \n75\n80\n85\n90\n95\n100\n0\n5\n10\n15\nTop-5 Accuracy\nFLOPs (in Billions)\nCNN Model Parameters\nResNet50\nInceptionV3\nSqueezeNet\nMobileNet\nDenseNet\n \n8 \nInstruments [40] performance analysis tool in the Xcode IDE. \nThe FPS on Android was measured using the OpenCV camera \nAPI [41] and on iOS was measured using the Instruments \nperformance analysis tool. The frames processed per second \nwere measured for the multithreading approach by periodically \ncomputing it in the apps. \nFig. 7 provides the CPU/GPU consumption results that were \nobtained. From this figure, one can see that the best processing \nrate for Android was achieved by SqueezeNet which ran at 11 \nFPS. The CPU consumption of the Android apps was seen to be \nproportional to the number of FLOPs used by the apps. For iOS, \nonly SqueezeNet was able to achieve greater than 24 FPS. As \ncan be seen from this figure, the iOS apps benefited from access \nto the GPU leading to higher throughputs than their Android \ncounterparts. The GPU consumption was seen to be \nproportional to the number of FLOPs used by the apps. It should \nbe noted that a DNN app with a higher number of FLOPs \ndrained the battery faster due to higher processor consumption. \nWhen deploying multithreading, as illustrated in Fig. 8, the \nFPS remained constant at 30 FPS for all the Android apps but \nthe number of frames processed per second was different for the \napps. Multi-threading allowed running the GUI for a natural \nvisual perception of 30 FPS while running the model in parallel \nat a lower rate. For the iOS versions of the apps, it can be seen \nthat higher throughputs in both FPS and frames processed per \nsecond were achieved due to the use of the GPU. Obviously, the \nCPU and GPU consumptions increased when using multi-\nthreading due to the use of concurrency while getting the benefit \nof being able to see video streams as they occurred. \nA comparison of the apps implementing the DNN models \nindicates that SqueezeNet provides high energy efficiency as \nwell as high throughputs on iOS smartphones but is not as \naccurate as the other models. It constitutes the model of choice \nfor high throughput applications. MobileNets provides high \naccuracy in a multi-threaded setting at the expense of a lower \nenergy efficiency and throughput. For applications where \naccuracy is the key requirement, one can use InceptionV3 in a \nmulti-threaded setting as the model of choice. ResNet50 and \nDenseNet121 are also good choices providing relatively higher \nenergy efficiency and throughput at the expense of 1-2% loss in \naccuracy.  \nV. CONCLUSION AND FUTURE EXTENSIONS \nThis paper has presented in one place the steps needed in \norder to deploy deep learning inference networks on Android \nand iOS smartphones. It has been shown how to use the publicly \navailable deep learning software tools to turn deep learning \nmodels into smartphone apps. In addition, it has been discussed \nhow to enable real-time operation of such apps on smartphones. \nA benchmarking framework involving accuracy, CPU/GPU \nconsumption, and real-time throughput has been devised to \nexamine these models. The steps discussed have been validated \nusing the benchmarking framework by considering six popular \nconvolutional neural network models that are extensively used \nin deep learning applications. The benchmarking results have \nshown that the deep learning models are implemented without \nany significant loss in accuracy. It has also been shown that the \nuse of multi-threading leads to achieving real-time throughputs. \nIn summary, this paper has provided the guidelines and \nbenchmarks for deploying deep learning inference models on \nsmartphones as real-time apps.  \nIt is worth mentioning here that the step-by-step guidelines \nprovided in this paper can get extended through ONNX (Open \nNeural Network Exchange), which is a community project \nstarted by Facebook and Microsoft to provide a unified \ncomputational dataflow graph for deep neural networks. Such \nan extension would allow a model trained in ONNX supported \nframeworks to be used interchangeably. Currently, converters \nfor CoreML and TensorFlow for other frameworks such as \nCaffe2, CNTK, PyTorch, etc., are being developed by ONNX, \nwhich would lead to seamless integration of models from \ndifferent frameworks into the deployment approach presented \nin this paper. \nAnother extension involves the use of neural network \ncompression methods. These methods involve reducing the size \nof models and thus the computation time by converting the \nweights of a model from floating-point numbers to integers with \nlower bits (quantization), by performing matrix decomposition, \nby pruning connections, etc. Once these methods are fully \ndeveloped, supported by the smartphone hardware, and made \navailable in the public domain, they can be incorporated \nseamlessly into the deployment approach presented in this \npaper. \nREFERENCES \n[1] \nY. Lecun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521, \nno. 7553, pp. 436–444, 2015.  \n[2] \nA. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet Classification \nwith Deep Convolutional Neural Networks,” Adv. Neural Inf. Process. \nSyst., pp. 1–9, 2012.  \n[3] \nS. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards Real-\nTime Object Detection with Region Proposal Networks,” IEEE Trans. \nPattern Anal. Mach. Intell., vol. 39, no. 6, pp. 1137–1149, 2017.  \n[4] \nL. Deng et al., “Recent advances in deep learning for speech research at \nMicrosoft,” Proc. IEEE Int. Conf. Acoust. Speech Signal Process., pp. \n8604–8608, 2013.  \n[5] \n“Number of smartphone users worldwide 2014-2020.”  [Online]. \nAvailable: \nhttps://www.statista.com/statistics/330695/number-of-\nsmartphone-users-worldwide/. [Accessed: 15-Sep-2018] \n[6] \n“Arduino.”  [Online]. Available: https://www.arduino.cc/. [Accessed: 11-\n \nFig. 6.  Single crop top-5 accuracy of the ILSVRC challenge validation \ndataset on PC and smartphone platforms. \n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\nResNet50\nInceptionV3\nSqueezeNet\nMobileNet\nDenseNet\nTop-5 Accuracy\nPC\nAndroid\niOS\n \n9 \nJul-2018] \n[7] \n“Raspberry Pi.”  [Online]. Available: https://www.raspberrypi.org/. \n[Accessed: 11-Jul-2018] \n[8] \nA. Mariakakis et al., “PupilScreen: Using Smartphones to Assess \nTraumatic Brain Injury,” Proc. ACM Interactive, Mobile, Wearable \nUbiquitous Technol., vol. 1, no. 3, pp. 1–27, Sep. 2017.  \n [9] A. Mariakakis, M. A. Banks, L. Phillipi, L. Yu, J. Taylor, and S. N. Patel, \n“BiliScreen: Scleral Jaundice Detection with a Smartphone,” Proc. ACM \nInteractive, Mobile, Wearable Ubiquitous Technol., vol. 1, no. 2, pp. 1–\n26, Jun. 2017.  \n[10] L. Chu, R. Qiu, H. Liu, Z. Ling, T. Zhang, and J. Wang, “Individual \nRecognition in Schizophrenia using Deep Learning Methods with \nRandom Forest and Voting Classifiers: Insights from Resting State EEG \nStreams,” Jun. 2017 [Online]. Available: http://arxiv.org/abs/1707.03467 \n[11] A. Sehgal and N. Kehtarnavaz, “A Convolutional Neural Network \nSmartphone App for Real-Time Voice Activity Detection,” IEEE Access, \nvol. 6, pp. 9017–9026, 2018.  \n[12] N. D. Lane et al., “DeepX: A Software Accelerator for Low-Power Deep \nLearning Inference on Mobile Devices,” Proc. 15th ACM/IEEE Int. Conf. \nInf. Process. Sens. Networks, no. 1, 2016.  \n[13] N. D. Lane, S. Bhattacharya, A. Mathur, P. Georgiev, C. Forlivesi, and F. \nKawsar, “Squeezing Deep Learning into Mobile and Embedded Devices,” \nIEEE Pervasive Comput., vol. 16, no. 3, pp. 82–88, 2017.  \n[14] A. G. Howard et al., “MobileNets: Efficient Convolutional Neural \nNetworks for Mobile Vision Applications,” Apr. 2017 [Online]. \nAvailable: http://arxiv.org/abs/1704.04861 \n[15] S. Han, H. Mao, and W. J. Dally, “Deep Compression: Compressing Deep \nNeural Networks with Pruning, Trained Quantization and Huffman \nCoding,” Oct. 2015 [Online]. Available: http://arxiv.org/abs/1510.00149 \n[16] Y. Jia et al., “Caffe: Convolutional Architecture for Fast Feature \nEmbedding,” \nJun. \n2014 \n[Online]. \nAvailable: \nhttp://arxiv.org/abs/1408.5093 \n[17] M. Abadi et al., “TensorFlow: Large-Scale Machine Learning on \nHeterogeneous Distributed Systems,” Mar. 2016 [Online]. Available: \nhttp://arxiv.org/abs/1603.04467 \n[18] “PyTorch.”  [Online]. Available: https://pytorch.org/. [Accessed: 11-Jul-\n2018] \n[19] “Microsoft \nCognitive \nToolkit.” \n \n[Online]. \nAvailable: \nhttps://www.microsoft.com/en-us/cognitive-toolkit/. [Accessed: 11-Jul-\n2018] \n[20] “Keras.”  [Online]. Available: https://keras.io/. [Accessed: 11-Jul-2018] \n[21] “Core \nML.” \n \n[Online]. \nAvailable: \nhttps://developer.apple.com/documentation/coreml. [Accessed: 11-Jul-\n2018] \n \nFig. 7.  (a) Average frames per second of the DNN Android apps are shown on the primary y-axis (left) and their average CPU consumption on the secondary \ny-axis (right), and (b) average frame per second of the DNN iOS apps are shown on the primary y-axis (left) and their average GPU consumption on the \nsecondary y-axis (right). Note the number of frames processed per second is the same as FPS. \n \nFig. 8.  (a) Average frames per second and frames processed per second of the DNN Android apps are shown on the primary y-axis (left) and their average \nCPU consumption on the secondary y-axis when using multithreading, and (b) average frames per second and frames processed per second of the DNN iOS \napps are shown on the primary y-axis (left) and their average CPU consumption on the secondary y-axis (right) when using multithreading. Note the number \nof frames processed per second is different than FPS. \n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\n0\n5\n10\n15\n20\n25\n30\n35\nNo Model\nResNet50\nInceptionV3\nSqueezeNet\nMobileNet\nDenseNet\nAverage CPU Consumption\nFrames per second\n(a) Android\nFPS\nCPU Consumption\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\n0\n5\n10\n15\n20\n25\n30\n35\nNo Model\nResNet50\nInceptionV3\nSqueezeNet\nMobileNet\nDenseNet\nAverage GPU Consumption\nFrames per second\n(b) iOS\nFPS\nGPU Consumption\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\n0\n5\n10\n15\n20\n25\n30\n35\nResNet50\nInceptionV3\nSqueezeNet\nMobileNet\nDenseNet\nAverage CPU Consumption\nFrames Per Second / \nFrames Processed Per Second\n(a) Android\nFPS\nFrames Processed Per Second\nCPU Consumption\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\nResNet50\nInceptionV3\nSqueezeNet\nMobileNet\nDenseNet\nAverage GPU Consumption\nFrames Per Second / \nFrames Processed Per Second\n(b) iOS\nFPS\nFrames Processed Per Second\nGPU Consumption\n \n10 \n[22] “coremltools.”  [Online]. Available: https://apple.github.io/coremltools/. \n[Accessed: 11-Jul-2018] \n[23] “tf-coreml: TensorFlow to CoreML Converter.”  [Online]. Available: \nhttps://github.com/tf-coreml/tf-coreml. [Accessed: 12-Jul-2018] \n[24] “deep-learning-model-convertor: The convertor/conversion of deep \nlearning models for different deep learning frameworks/softwares.”  \n[Online]. Available: https://github.com/ysh329/deep-learning-model-\nconvertor. [Accessed: 12-Jul-2018] \n[25] “Gradle Build Dependencies for Android Studio.”  [Online]. Available: \nhttps://developer.android.com/studio/build/dependencies. [Accessed: 11-\nAug-2018] \n[26] “Mobile Operating System Market Share,” 2018.  [Online]. Available: \nhttp://gs.statcounter.com/os-market-share/mobile/worldwide. [Accessed: \n11-Jul-2018] \n[27] “Android \nStudio.” \n \n[Online]. \nAvailable: \nhttps://developer.android.com/studio/. [Accessed: 11-Jul-2018] \n[28] “Xcode.”  [Online]. Available: https://developer.apple.com/xcode/. \n[Accessed: 11-Jul-2018] \n[29] “MNIST \nhandwritten \ndigit \ndatabase.” \n \n[Online]. \nAvailable: \nhttp://yann.lecun.com/exdb/mnist/. [Accessed: 11-Jul-2018] \n[30] O. Russakovsky et al., “ImageNet Large Scale Visual Recognition \nChallenge,” \nSep. \n2014 \n[Online]. \nAvailable: \nhttps://arxiv.org/abs/1409.0575v3 \n[31] Y. Le Cun et al., “Handwritten digit recognition: applications of neural \nnetwork chips and automatic learning,” IEEE Commun. Mag., vol. 27, no. \n11, pp. 41–46, Nov. 1989.  \n[32] K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for Image \nRecognition,” \nDec. \n2015 \n[Online]. \nAvailable: \nhttp://arxiv.org/abs/1512.03385 \n[33] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking \nthe Inception Architecture for Computer Vision,” Dec. 2015 [Online]. \nAvailable: http://arxiv.org/abs/1512.00567 \n[34] C. Szegedy et al., “Going deeper with convolutions,” in Proc. IEEE Conf. \nComput. Vis. Pattern Recognit., 2015, pp. 1–9.  \n[35] S. Ioffe and C. Szegedy, “Batch Normalization: Accelerating Deep \nNetwork Training by Reducing Internal Covariate Shift,” Feb. 2015 \n[Online]. Available: http://arxiv.org/abs/1502.03167 \n[36] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and K. \nKeutzer, “SqueezeNet: AlexNet-level accuracy with 50x fewer \nparameters and <0.5 MB model size,” Feb. 2016 [Online]. Available: \nhttp://arxiv.org/abs/1602.07360 \n[37] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger, “Densely \nConnected Convolutional Networks,” Aug. 2016 [Online]. Available: \nhttp://arxiv.org/abs/1608.06993 \n[38] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, “How transferable are \nfeatures in deep neural networks?,” Nov. 2014 [Online]. Available: \nhttp://arxiv.org/abs/1411.1792 \n[39] “Android Profiler for Android Studio.”  [Online]. Available: \nhttps://developer.android.com/studio/profile/android-profiler. [Accessed: \n11-Aug-2018] \n[40] “Instruments \nfor \nXcode \nIDE.” \n \n[Online]. \nAvailable: \nhttps://help.apple.com/instruments/mac/current/. [Accessed: 11-Aug-\n2018] \n[41] “Android \n- \nOpenCV \nlibrary.” \n \n[Online]. \nAvailable: \nhttps://opencv.org/platforms/android/. [Accessed: 11-Aug-2018] \n \n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-01-08",
  "updated": "2019-01-08"
}