{
  "id": "http://arxiv.org/abs/1901.02219v1",
  "title": "Uncertainty-Based Out-of-Distribution Detection in Deep Reinforcement Learning",
  "authors": [
    "Andreas Sedlmeier",
    "Thomas Gabor",
    "Thomy Phan",
    "Lenz Belzner",
    "Claudia Linnhoff-Popien"
  ],
  "abstract": "We consider the problem of detecting out-of-distribution (OOD) samples in\ndeep reinforcement learning. In a value based reinforcement learning setting,\nwe propose to use uncertainty estimation techniques directly on the agent's\nvalue estimating neural network to detect OOD samples. The focus of our work\nlies in analyzing the suitability of approximate Bayesian inference methods and\nrelated ensembling techniques that generate uncertainty estimates. Although\nprior work has shown that dropout-based variational inference techniques and\nbootstrap-based approaches can be used to model epistemic uncertainty, the\nsuitability for detecting OOD samples in deep reinforcement learning remains an\nopen question. Our results show that uncertainty estimation can be used to\ndifferentiate in- from out-of-distribution samples. Over the complete training\nprocess of the reinforcement learning agents, bootstrap-based approaches tend\nto produce more reliable epistemic uncertainty estimates, when compared to\ndropout-based approaches.",
  "text": "Uncertainty-Based Out-of-Distribution Detection\nin Deep Reinforcement Learning\nAndreas Sedlmeier∗\nLMU Munich\nMunich, Germany\nThomas Gabor\nLMU Munich\nMunich, Germany\nThomy Phan\nLMU Munich\nMunich, Germany\nLenz Belzner\nMaibornWolff\nMunich, Germany\nClaudia Linnhoff-Popien\nLMU Munich\nMunich, Germany\nAbstract\nWe consider the problem of detecting out-of-distribution (OOD) samples in deep\nreinforcement learning. In a value based reinforcement learning setting, we propose\nto use uncertainty estimation techniques directly on the agent’s value estimating\nneural network to detect OOD samples. The focus of our work lies in analyzing\nthe suitability of approximate Bayesian inference methods and related ensembling\ntechniques that generate uncertainty estimates. Although prior work has shown that\ndropout-based variational inference techniques and bootstrap-based approaches can\nbe used to model epistemic uncertainty, the suitability for detecting OOD samples\nin deep reinforcement learning remains an open question. Our results show that\nuncertainty estimation can be used to differentiate in- from out-of-distribution\nsamples. Over the complete training process of the reinforcement learning agents,\nbootstrap-based approaches tend to produce more reliable epistemic uncertainty\nestimates, when compared to dropout-based approaches.\n1\nIntroduction\nOne of the main impediments to the deployment of machine learning systems in the real world is\nthe difﬁculty to show that the system will continue to reliably produce correct predictions in all\nthe situations it encounters in production use. One of the possible reasons for failure is so called\nout-of-distribution (OOD) data, i.e. data which deviates substantially from the data encountered\nduring training. As the fundamental problem of limited training data seems unsolvable for most\ncases, especially in sequential decision making tasks like reinforcement learning, a possible ﬁrst\nstep towards a solution is to detect and report the occurance of OOD data. This can prevent silent\nfailures caused by wrong predictions of the machine learning system, for example by handing control\nover to a human supervisor [1]. In this paper, we propose to use uncertainty estimation techniques\nin combination with value-based reinforcement learning to detect OOD samples. We focus on deep\nQ-Learning [20], integrating directly with the agent’s value-estimating neural network.\nWhen considering to use uncertainty estimation in order to detect OOD samples, it is important to\ndifferentiate two types of uncertainty: aleatoric and epistemic uncertainty. The ﬁrst type, aleatoric\nuncertainty models the inherent stochasticity in the system and consequently cannot be reduced by\ncapturing more data. Epistemic uncertainty by contrast arises out of a lack of sufﬁcient data to exactly\ninfer the underlying system’s data generating function. Consequently, epistemic uncertainty tends\nto be higher in areas of low data density. Qazaz [25], who in turn refers to Bishop [2] for the initial\n∗Corresponding author: Andreas Sedlmeier <andreas.sedlmeier@iﬁ.lmu.de>\nPreprint. Work in progress.\narXiv:1901.02219v1  [cs.LG]  8 Jan 2019\nconjecture, showed that the epistemic uncertainty σepis(x) is approximately inversely proportional\nto the density p(x) of the input data, for the case of generalized linear regression models as well as\nmulti-layer neural networks: σepis(x) ∝p−1(x)\nThis also forms the basis of our proposed method: to use this inverse relation between epistemic\nuncertainty and data density in order to differentiate in- from out-of-distribution samples.\n2\nRelated Work\nA systematic way to deal with uncertainty is via Bayesian inference. Its combination with neural\nnetworks in the form of Bayesian neural networks is realised by placing a probability distribution\nover the weight-values of the network [19]. As calculating the exact Bayesian posterior quickly\nbecomes computationally intractable for deep models, a popular solution are approximate inference\nmethods [9, 12, 3, 7, 13, 17, 8]. Another option is the construction of model ensembles, e.g., based\non the idea of the statistical bootstrap [6]. The resulting distribution of the ensemble predictions can\nthen be used to approximate the uncertainty [22, 15]. Both approaches have been used for tasks as\ndiverse as machine vision [14], disease detection [16], or decision making [5, 22].\nFor the case of low-dimensional feature spaces, OOD detection (also called novelty detection) is a\nwell-researched problem. For a survey on the topic, see e.g. Pimentel et al. [24], who distinguish\nbetween probabilistic, distance-based, reconstruction-based, domain-based and information theoretic\nmethods. During the last years, several new methods based on deep neural networks were proposed for\nhigh-dimensional cases, mostly focusing on classiﬁcation tasks, e.g. image classiﬁcation. Hendrycks\nand Gimpel [11] propose a baseline for detecting OOD examples in neural networks, based on the\npredicted class probabilities of a softmax classiﬁer. Liang et al. [18] improve upon this baseline by\nusing temperature scaling and by adding perturbations to the input. These methods are not directly\napplicable to our focus, value-based reinforcement learning, where neural networks are used for\nregression tasks. Other methods, especially generative-neural-network-based techniques [26] could\nprovide a solution, but at the cost of adding separate, additional components. Our approach has the\nbeneﬁt of not needing additional components, as it directly integrates with the neural network used\nfor value estimation.\n3\nExperimental Setup\nOne of the problems in researching OOD detection for reinforcement learning is the lack of datasets\nor environments which can be used for generating and assessing OOD samples in a controlled and\nreproducible way. By contrast to the ﬁeld of image classiﬁcation, where benchmark datasets like\nnotMNIST [4] exist that contain OOD samples, there are no equivalent sets for reinforcement learning.\nAs a ﬁrst step, we developed a simple gridworld environment, which allows modiﬁcations after the\ntraining process, thus producing OOD states during evaluation.\nFor our experiments, we focus on a simple gridworld pathﬁnding environment. During training,\nthe agent starts every episode at a random position in the left half of the 12 × 4 grid space. Its\ngoal is to reach a speciﬁc target position in the right half of the grid, which also varies randomly\nevery episode, by choosing one of the four possible actions: {up,down,left,right}. A vertical set of\nwalls separates the two halves of the environment, acting as static obstacles. Each step of the agent\nincurs a cost of −1 except the target-reaching action, which is rewarded with +100 and ends the\nepisode. This conﬁguration of the environment is called the train environment. For evaluating the\nOOD detection performance, we ﬂip the start and goal positions, i.e. the agent starts in the right half\nof the environment and has to reach a goal position in the left half. This so called mirror environment\nproduces states which the agent has not encountered during training. Consequently, we expect higher\nepistemic uncertainty values for these OOD states. Note that training is solely performed in the train\nenvironment. Evaluation runs are executed independently of the training process, based on model\nsnapshots generated at the respective training episodes. Data collected during evaluation runs is not\nused for training. The state of the environment is represented as a stack of three W × H feature\nplanes (W being the width, H the height of the grid layout) with each plane representing the spatial\npositions of all environment objects of a speciﬁc type: agent, target or wall.\nWe compare different neural network architectures and their effect on the reported uncertainty values\nas the networks are being used by the RL agent for value estimation. The Monte-Carlo Dropout\n2\nnetwork (MCD) uses dropout variational inference as described by [14]. Our implementation consists\nof two fully-connected hidden layers with 64 neurons each, followed by two separate neurons in the\noutput layer representing µ and σ of a normal distribution. Before every weight layer in the model, a\ndropout layer with p = 0.95 is added, specifying the probability that a neuron stays active. Model\nloss is calculated by minimizing the negative log-likelihood of the predicted output distribution.\nEpistemic uncertainty as part of the total predictive uncertainty is then calculated according to the\nfollowing formula: Varep(y) ≈1\nT\nPT\nt=1 ˆy2\nt −( 1\nT\nPT\nt=1 ˆyt)2 with T outputs ˆyt of the Monte-Carlo\nsampling.\nGal et al. [8] suggested an improvement to the default Monte-Carlo dropout method called Concrete\nDropout which does not require a pre-speciﬁed dropout rate and instead learns individual dropout\nrates per layer. This method is of special interest when used in the context of reinforcement learning,\nas here the available data change during the training process, rendering a manual optimization of the\ndropout rate hyperparameter even more difﬁcult. Our implementation of the Monte-Carlo Concrete\nDropout network (MCCD) is identical to the MCD network with the exception that every normal\ndropout layer is replaced by a concrete dropout layer. For both the MCD and MCCD networks, 10\nMonte-Carlo forward passes are performed.\nThe Bootstrap neural network (BOOT) is based on the architecture described by [22]. It represents\nan efﬁcient implementation of the bootstrap principle by sharing a set of hidden layers between all\nmembers of the ensemble. Our implementation consists of two fully-connected hidden layers with 64\nneurons each, which are shared between all heads, followed by an output layer of K = 10 bootstrap\nheads. For each datapoint, a Boolean mask of length equal to the number of heads is generated, which\ndetermines the heads this datapoint is visible to. The mask’s values are set by drawing K times from\na Bernoulli distribution with p = 0.2. The Bootstrap-Prior neural network (BOOTP) is based on the\nextension presented in [21]. It has the same basic architecture as the BOOT network but with the\naddition of a so-called random Prior Network. Predictions are generated by adding the output of\nthis untrainable prior network to the output of the different bootstrap heads before calculating the\nloss. Osband et al. [21] conjecture that the addition of this randomized prior function outperforms\nensemble-based methods without explicit priors, as for the latter, the initial weights have to act both\nas prior and training initializer.\n4\nResults\nFigure 1 presents the average uncertainty of the chosen actions over 10000 training episodes of the\ndifferent network architectures. As there is a certain amount of randomness in the evaluation runs,\ncaused by the random placement of start and goal positions, the plots show averages of 30 evaluation\nruns.\nAccording to the concept of epistemic uncertainty, we would expect a decline in the absolute value of\nreported epistemic uncertainty in the train environment over the training process, as the agent collects\nmore data. Interestingly, only the bootstrap-based methods (BOOT 1c and BOOTP 1d) reliably show\nthis behaviour. The dropout-based methods do not show consistent behaviour in this regard. For these\nmethods, the predicted uncertainty sometimes even increases along the training process as can be\nseen in Figure 1b. Regarding the OOD detection performance, the methods are required to predict\nhigher epistemic uncertainty values for OOD samples than for in-distribution samples. Here also,\nthe bootstrap-based methods outperform the dropout-based ones. For all bootstrap methods, over\nthe complete training process, the predicted uncertainty values in the “out-of-distribution” mirror\nenvironment are higher than the values in the train environment. Consequently, it would be possible\nto detect the OOD samples reliably, for example by setting a threshold based on the lower uncertainty\nvalues predicted during training. Figure 1d shows that the addition of a prior has a positive effect on\nthe separation of in- and out-of-distribution samples, as the distance between the predicted uncertainty\nvalues increases.\nOur results for the dropout-based techniques are not as positive. As can be seen in Figure 1a and 1b,\nneither of the tested Monte-Carlo dropout methods consistenly outputs higher uncertainty values for\nthe OOD states of the mirror environment over the complete training process. Although there are\nepisodes, especially in the beginning, where the mirror environment’s uncertainty values are higher,\nthere is a reversal during the training process. As a consequence, it would not be possible to reliably\ndifferentiate between in- and out-of-distribution samples at every point in time.\n3\n0\n2000\n4000\n6000\n8000\n10000\nepisode\n10\n3\n10\n2\n10\n1\n100\n101\n102\nuncertainty\ntrain\nmirror\n(a) MCD\n0\n2000\n4000\n6000\n8000\n10000\nepisode\n10\n3\n10\n2\n10\n1\n100\n101\n102\nuncertainty\ntrain\nmirror\n(b) MCCD\n0\n2000\n4000\n6000\n8000\n10000\nepisode\n10\n3\n10\n2\n10\n1\n100\n101\n102\nuncertainty\ntrain\nmirror\n(c) BOOT\n0\n2000\n4000\n6000\n8000\n10000\nepisode\n10\n3\n10\n2\n10\n1\n100\n101\n102\nuncertainty\ntrain\nmirror\n(d) BOOTP\nFigure 1: Per-episode mean uncertainty of chosen actions, averages of 30 runs (y-axis log-scaled).\n5\nDiscussion and Future Work\nThe results we obtained from the bootstrap-based methods show the general feasibility of our\napproach, as they allow for a reliable differentiation between in- and out-of-distribution samples in\nthe evaluated environments. Declining uncertainty values over the training process also conform to the\nexpectation that epistemic uncertainty can be reduced by collecting more data. For the dropout-based\ntechniques, it remains to be seen if our results show a general problem of these methods in sequential\ndecision problems, or whether the results are a consequence of our speciﬁc environments. According\nto Osband et al. [21] the observed behaviour is to be expected for the basic Monte-Carlo dropout\nmethod (MCD) as the dropout distribution does not concentrate with observed data. Consequently,\nwe expected different results from the concrete dropout method (MCCD) as it should be able to\nadapt to the training data. Nevertheless, this did not lead to decreasing uncertainty estimates over the\ntraining process or a reliable prediction of higher uncertainty for OOD samples. We are currently\nworking on extending our evaluations to more environments in order to evaluate if these results\ngeneralize. This will include stochastic domains, where it is necessary to differentiate between\naleatoric and epistemic uncertainty in order to correctly detect OOD samples. It will also be very\ninteresting to compare the performance of the proposed uncertainty based methods to methods based\non generative models. Another interesting aspect which could further improve the OOD detection\nperformance of the ensemble methods is the choice of prior [10] and a newly proposed method\ncalled Bayesian Ensembling [23], which could bridge the gap between fully Bayesian methods and\nensembling methods.\n4\nReferences\n[1] D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. Mané. Concrete Problems\nin AI Safety. ArXiv e-prints, June 2016.\n[2] C. M. Bishop. Novelty detection and neural network validation. IEE Proceedings - Vision, Image\nand Signal Processing, 141(4):217–222, Aug 1994. ISSN 1350-245X. doi: 10.1049/ip-vis:\n19941330.\n[3] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra. Weight Uncertainty in Neural\nNetworks. ArXiv e-prints, May 2015.\n[4] Yaroslav Bulatov. Notmnist dataset. Google (Books/OCR), Tech. Rep.[Online]. Available:\nhttp://yaroslavvb. blogspot. it/2011/09/notmnist-dataset. html, 2011.\n[5] S. Depeweg, J. M. Hernández-Lobato, F. Doshi-Velez, and S. Udluft. Learning and Policy\nSearch in Stochastic Dynamical Systems with Bayesian Neural Networks. ArXiv e-prints, May\n2016.\n[6] Bradley Efron. Bootstrap Methods: Another Look at the Jackknife, pages 569–593. Springer\nNew York, New York, NY, 1992. ISBN 978-1-4612-4380-9. doi: 10.1007/978-1-4612-4380-9_\n41.\n[7] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model\nuncertainty in deep learning. In international conference on machine learning, pages 1050–1059,\n2016.\n[8] Yarin Gal, Jiri Hron, and Alex Kendall. Concrete dropout. In I. Guyon, U. V. Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural\nInformation Processing Systems 30, pages 3581–3590. Curran Associates, Inc., 2017.\n[9] Alex Graves. Practical variational inference for neural networks. In J. Shawe-Taylor, R. S.\nZemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger, editors, Advances in Neural Information\nProcessing Systems 24, pages 2348–2356. Curran Associates, Inc., 2011.\n[10] D. Hafner, D. Tran, A. Irpan, T. Lillicrap, and J. Davidson. Reliable Uncertainty Estimates in\nDeep Neural Networks using Noise Contrastive Priors. ArXiv e-prints, July 2018.\n[11] D. Hendrycks and K. Gimpel. A Baseline for Detecting Misclassiﬁed and Out-of-Distribution\nExamples in Neural Networks. ArXiv e-prints, October 2016.\n[12] J. M. Hernández-Lobato and R. P. Adams. Probabilistic Backpropagation for Scalable Learning\nof Bayesian Neural Networks. ArXiv e-prints, February 2015.\n[13] JM Hernández-Lobato, Y Li, M Rowland, D Hernández-Lobato, TD Bui, and RE Ttarner.\nBlack-box α-divergence minimization. In 33rd International Conference on Machine Learning,\nICML 2016, volume 4, pages 2256–2273, 2016.\n[14] Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for\ncomputer vision? In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-\nwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30,\npages 5574–5584. Curran Associates, Inc., 2017.\n[15] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable pre-\ndictive uncertainty estimation using deep ensembles. In I. Guyon, U. V. Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Informa-\ntion Processing Systems 30, pages 6402–6413. Curran Associates, Inc., 2017.\n[16] Christian Leibig, Vaneeda Allken, Murat Seçkin Ayhan, Philipp Berens, and Siegfried Wahl.\nLeveraging uncertainty information from deep neural networks for disease detection. Scientiﬁc\nreports, 7(1):17816, 2017.\n[17] Y. Li and Y. Gal. Dropout Inference in Bayesian Neural Networks with Alpha-divergences.\nArXiv e-prints, March 2017.\n5\n[18] S. Liang, Y. Li, and R. Srikant. Enhancing The Reliability of Out-of-distribution Image\nDetection in Neural Networks. ArXiv e-prints, June 2017.\n[19] David J. C. MacKay. A practical bayesian framework for backpropagation networks. Neural\nComputation, 4:448–472, 1992.\n[20] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller.\nPlaying Atari with Deep Reinforcement Learning. ArXiv e-prints, December 2013.\n[21] I. Osband, J. Aslanides, and A. Cassirer. Randomized Prior Functions for Deep Reinforcement\nLearning. ArXiv e-prints, June 2018.\n[22] Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via\nbootstrapped dqn. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 29, pages 4026–4034. Curran Associates,\nInc., 2016.\n[23] T. Pearce, M. Zaki, A. Brintrup, and A. Neel. Uncertainty in Neural Networks: Bayesian\nEnsembling. ArXiv e-prints, October 2018.\n[24] Marco A.F. Pimentel, David A. Clifton, Lei Clifton, and Lionel Tarassenko. A review of\nnovelty detection. Signal Processing, 99:215 – 249, 2014. ISSN 0165-1684. doi: https:\n//doi.org/10.1016/j.sigpro.2013.12.026.\n[25] Cazhaow S. Qazaz. Bayesian error bars for regression. PhD thesis, Aston University, 1996.\n[26] Thomas Schlegl, Philipp Seeböck, Sebastian M. Waldstein, Ursula Schmidt-Erfurth, and Georg\nLangs. Unsupervised anomaly detection with generative adversarial networks to guide marker\ndiscovery. In IPMI, 2017.\n6\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2019-01-08",
  "updated": "2019-01-08"
}