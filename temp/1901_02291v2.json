{
  "id": "http://arxiv.org/abs/1901.02291v2",
  "title": "Spectral Clustering via Ensemble Deep Autoencoder Learning (SC-EDAE)",
  "authors": [
    "Severine Affeldt",
    "Lazhar Labiod",
    "Mohamed Nadif"
  ],
  "abstract": "Recently, a number of works have studied clustering strategies that combine\nclassical clustering algorithms and deep learning methods. These approaches\nfollow either a sequential way, where a deep representation is learned using a\ndeep autoencoder before obtaining clusters with k-means, or a simultaneous way,\nwhere deep representation and clusters are learned jointly by optimizing a\nsingle objective function. Both strategies improve clustering performance,\nhowever the robustness of these approaches is impeded by several deep\nautoencoder setting issues, among which the weights initialization, the width\nand number of layers or the number of epochs. To alleviate the impact of such\nhyperparameters setting on the clustering performance, we propose a new model\nwhich combines the spectral clustering and deep autoencoder strengths in an\nensemble learning framework. Extensive experiments on various benchmark\ndatasets demonstrate the potential and robustness of our approach compared to\nstate-of-the-art deep clustering methods.",
  "text": "Spectral Clustering via Ensemble Deep Autoencoder\nLearning (SC-EDAE)\nS´everine Aﬀeldta,∗, Lazhar Labioda, Mohamed Nadifa\naUniversity of Paris Descartes,\nMathematics and Computer Science,\n45 rue des Saints P`eres,\n75006 Paris, France\nAbstract\nRecently, a number of works have studied clustering strategies that combine\nclassical clustering algorithms and deep learning methods. These approaches\nfollow either a sequential way, where a deep representation is learned using a\ndeep autoencoder before obtaining clusters with k-means, or a simultaneous way,\nwhere deep representation and clusters are learned jointly by optimizing a single\nobjective function. Both strategies improve clustering performance, however the\nrobustness of these approaches is impeded by several deep autoencoder setting\nissues, among which the weights initialization, the width and number of layers\nor the number of epochs.\nTo alleviate the impact of such hyperparameters\nsetting on the clustering performance, we propose a new model which combines\nthe spectral clustering and deep autoencoder strengths in an ensemble learning\nframework. Extensive experiments on various benchmark datasets demonstrate\nthe potential and robustness of our approach compared to state-of-the-art deep\nclustering methods.\nKeywords:\nspectral clustering, unsupervised ensemble learning, autoencoder\n∗Corresponding author\nEmail addresses: severine.affeldt@parisdescartes.fr (S´everine Aﬀeldt),\nlazhar.labiod@parisdescartes.fr (Lazhar Labiod), mohamed.nadif@mi.parisdescartes.fr\n(Mohamed Nadif)\nPreprint submitted to Journal of LATEX Templates\nJune 13, 2019\narXiv:1901.02291v2  [cs.LG]  12 Jun 2019\n1. Introduction\nLearning from large amount of data is a very challenging task.\nSeveral\ndimensionality reduction and clustering techniques that are well studied in the\nliterature aim to learn a suitable and simpliﬁed data representation from original\ndataset; see for instance [1, 2, 3]. While many approaches have been proposed to\naddress the dimensionality reduction and clustering tasks, deep learning-based\nmethods recently demonstrate promising results. Motivated by the keen interest\nin deep learning, many authors tackle the objective of data representation and\npartitioning using jointly the autoencoders [4] and clustering approaches.\n1.1. Deep Autoencoder: challenges and issues\nDeep learning is a machine learning method that works with multi-level\nlearning of data representations [5] where one passes from low level features to\nhigher level features through the diﬀerent layers. These deep architectures can\nautomatically learn important features from images, sound or text data and\nhave made signiﬁcant progress in the ﬁeld of computer vision. The autoencoder\n(AE) algorithm and its deep version (DAE), like the traditional methods of\ndimensionality reduction, has been a great success in recent years.\nAn autoencoder [4, 6, 7] is a neural network which is trained to replicate its\ninput at its output. Training an autoencoder is unsupervised in the sense that no\nlabeled data is needed. The training process is still based on the optimization\nof a cost function.\nAutoencoders can be used as tools to train deep neural\nnetworks [8].\nFor the purpose of dimensionality reduction, an autoencoder can learn a\nrepresentation (or encoding) for a set of data. If linear activations are used, or\nonly a single sigmoid hidden layer, then the optimal solution to an autoencoder\nis strongly related to Principal Component Analysis (PCA). With appropriate\ndimensionality and sparsity constraints, autoencoders can learn data projections\nthat are more interesting than other basic techniques such as PCA which only\nallows linear transformation of data vectors. By contrast, the autoencoders are\n2\nnon-linear by nature, and can learn more complex relations between visible and\nhidden units. Moreover, they can be stacked, which makes them even more\npowerful.\nRecently, a number of works have studied clustering strategies that combine\nclassical clustering algorithms and deep learning methods. These approaches\nfollow either a sequential way, where a deep representation is learned using a\ndeep autoencoder before obtaining clusters using a clustering technique (e.g.\nk-means) [9, 10, 11, 12, 13, 14, 15, 16, 17], or a simultaneous way, where deep\nrepresentation and clusters are learned jointly by optimizing a single objective\nfunction [18, 19, 20]. Both strategies improve clustering performance. However,\nwhen dealing with real-world data, existing clustering algorithms based on deep\nautoencoders suﬀer from diﬀerent issues which impede their robustness and\nease-to-use, such as,\n• the weights initialization, as mentioned in [21], the training of a Deep\nNeural Network (DNN) still suﬀers from two major drawbacks, among\nwhich the weights initialization. Indeed, initializing the weights with ran-\ndom values clearly adds randomness to the obtained results. The DNN\npretraining [22], which is strongly related to the initialization issue, has\nbeen used in an increasing number of studies [18, 23, 24]. While pretrain-\ning helps to improve clustering performance, it is usually computationally\nintensive and thus raises supplementary training issues.\n• the architecture (or structure), the architecture (i.e., number of layers\nand their width) forces the network to seek a diﬀerent representation of\nthe data while preserving the important information. However, we observe\nthat in almost all recent papers on deep clustering [18, 19, 20, 15, 16, 17,\n25], a diﬀerent structure is recommended by the authors for each studied\ndataset. In some studies, the DAE architecture can even lack of technical\nrationales. Most importantly, the clustering performance of the proposed\nmethods usually strongly depends on a particular DAE structure.\n3\n1.2. Our paper’s contribution and structure\nTo address the above mentioned challenging issues, we propose a Spectral\nClustering via Ensemble Deep Autoencoder’s algorithm (SC-EDAE) which com-\nbines the advantages and strengths of spectral clustering, deep embedding mod-\nels and ensemble paradigm. Ensemble learning has been considered in diﬀerent\nmachine learning context where it generally helps in improving results by com-\nbining several models. The ensemble approach allows a better predictive perfor-\nmance and a more robust clustering as compared to the results obtained with\na single model. Following the ensemble paradigm, we ﬁrst used several DAE\nwith diﬀerent hyperparameters settings to generate m encodings. In a second\nstep, each encoding is projected in a higher features space based on the anchors\nstrategy [26, 27] to construct m graph aﬃnity matrices. Finally, we apply spec-\ntral clustering on an ensemble graph aﬃnity matrix to have the common space\nshared by all the m encodings, before we run k-means in this common subspace\nto produce the ﬁnal clustering (see Fig. 1 for a summary diagram).\nThe outline of the paper is as follows. In Section 2 we present the related\nwork. In Section 3, some notations and preliminaries are given. In Section 4, we\npresent and discuss our approach in full details. In Section 5, the evaluations of\nthe proposed method and comparisons with several related approaches available\nin the literature are presented. The conclusion of the paper is given in Section 6.\n2. Related Work\nDespite their success, most existing clustering methods are severely chal-\nlenged by the data generated with modern applications, which are typically\nhigh-dimensional, noisy, heterogeneous and sparse. This has driven many re-\nsearchers to investigate new clustering models to overcome these diﬃculties.\nOne promising category of such models relies on data embedding.\nWithin this framework, classical dimensionality reduction approaches, e.g.,\nPrincipal Component Analysis (PCA), have been widely considered for the em-\nbedding task. However, the linear nature of such techniques makes it challenging\n4\nto infer faithful representations of real-world data, which typically lie on highly\nnon-linear manifolds. This motivates the investigation of deep learning models\n(e.g., autoencoders, convolutional neural networks), which have been shown so\nfar to be successful in extracting highly non-linear features from complex data,\nsuch as text, images or graphs [4, 6, 7].\nThe deep autoencoders (DAE) have proven to be useful for dimensionality\nreduction [4] and image denoising. In particular, the autoencoders (AE) can\nnon-linearly transform data into a latent space. When this latent space has\nlower dimension than the original one [4], this can be viewed as a form of non-\nlinear PCA. An autoencoder typically consists of an encoder stage, that can\nprovide an encoding of the original data in lower dimension, and a decoder part,\nto deﬁne the data reconstruction cost. In clustering context, the general idea\nis to embed the data into a low dimensional latent space and then perform\nclustering in this new space. The goal of the embedding here is to learn new\nrepresentations of the objects of interest (e.g., images) that encode only the most\nrelevant information characterizing the original data, which would for example\nreduce noise and sparsity.\nSeveral interesting works have recently combined embedding learning and\nclustering. The proposed methods generally conduct both clustering and deep\nembedding in two diﬀerent ways. First, some works proposed to combine deep\nembedding and clustering in a sequential way. In [10] the authors use a stacked\nautoencoder to learn a representation of the aﬃnity graph, and then run k-\nmeans on the learned representations to obtain the clusters.\nIn [24], it has\nbeen proposed to train a deep network by iteratively minimizing a Kullback-\nLeibler (KL) divergence between a centroid based probability distribution and\nan auxiliary target distribution.\nMore recently, in [28] the authors propose to incorporate an autoencoder\ninto the Deep Embedded Clustering (DEC) framework [24]. Then, the proposed\nframework can jointly perform clustering and learn representative features with\nlocal structure preservation. A novel non-linear reconstruction method which\nadopt deep neural networks for representation based community detection has\n5\nbeen proposed in [20]. The work presented in [25] combines deep learning with\nsubspace clustering such that the network is designed to directly learn the aﬃni-\nties matrix. Finally, a novel algorithm was introduced in [15] that uses land-\nmarks and deep autoencoders, to perform eﬃcient spectral clustering.\nSince the embedding process is not guaranteed to infer representations that\nare suitable for the clustering task, several authors recommend to perform both\ntasks jointly so as to let clustering govern feature extraction and vice-versa. In\n[19], the authors propose a general framework, so-called DeepCluster, to inte-\ngrate the traditional clustering methods into deep learning models and adopt\nAlternating Direction of Multiplier Method to optimize it. In [18], a joint dimen-\nsionality reduction and k-means clustering approach in which dimensionality\nreduction is accomplished via learning a deep neural network is proposed.\nBeyond the joint and sequential ways to combine clustering and deep em-\nbedding, it appears that the connection between autoencoder and ensemble\nlearning paradigm has not been explored yet. In this paper, we aim to ﬁll the\ngap between ensemble deep autoencoders and spectral clustering in order to\npropose a robust approach that takes simultaneously advantage of several deep\nmodels with various hyperparameter settings. In particular, we apply spectral\nclustering on an ensemble of fused encodings obtained from m diﬀerent deep\nautoencoders. To our knowledge, the adoption of deep learning in an ensemble\nlearning paradigm has not been adequately investigated yet. The goal of this\nwork is to conduct investigations along this direction.\n3. Preliminaries\n3.1. Notation\nThroughout the paper, we use bold uppercase characters to denote matrices,\nbold lowercase characters to denote vectors. For any matrix M, mj denotes the\nj-th column vector of M, yi means the i-th row vector of Y, mij denotes the\n(i, j)−element of M and Tr[M] is the trace of M whether M is a square matrix;\nM⊤denotes the transpose matrix of M. We consider the Frobenius norm of a\n6\nmatrix M ∈Rn×d: ||M||2 = Pn\ni=1\nPd\nj=1 m2\nij = Tr[M⊤M]. Furthermore, let I\nbe the identity matrix with appropriate size.\n3.2. Spectral clustering\nSpectral clustering is a popular clustering method that uses eigenvectors\nof a symmetric matrix derived from the distance between datapoints. Several\nalgorithms have been proposed in the literature [29, 30], each using the eigenvec-\ntors in slightly diﬀerent ways [31, 32, 33]. The partition of the n datapoints of\nX ∈Rn×d into k disjoint clusters is based on an objective function that favors\nlow similarity between clusters and high similarity within clusters. In its nor-\nmalized version, the spectral clustering algorithm exploits the top k eigenvectors\nof the normalized graph Laplacian L that are the relaxations of the indicator\nvectors which provide assignments of each datapoint to a cluster. In particular,\nit amounts to maximize the following relaxed normalized association,\nmax\nB∈Rn×k Tr(B⊤SB)\ns.t.\nB⊤B = I\n(1)\nwith S = D−1/2KD−1/2\n∈Rn×n is the normalized similarity matrix where\nK\n∈Rn×n is the similarity matrix and\nD ∈Rn×n is the diagonal matrix\nwhose (i, i)-element of X is the sum of X’s i-th row. The solution of (1) is to set\nthe matrix B ∈Rn×k equal to the k eigenvectors corresponding to the largest\nk eigenvalues of S. After renormalization of each row of B, a k-means assigns\neach datapoint xi of X to the cluster that the row bi of B is assigned to.\nAs opposed to several other clustering algorithms (e.g. k-means), spectral\nclustering performs well on arbitrary shaped clusters. However, a limitation\nof this method is the diﬃculty to handle large-scale datasets due to the high\ncomplexity of the graph Laplacian construction and the eigendecomposition.\nRecently, a scalable spectral clustering approach, referred to as Landmark-\nbased Spectral Clustering (LSC) [34] or AnchorGraph [26], has been proposed.\nThis approach allows to eﬃciently construct the graph Laplacian and compute\nthe eigendecomposition. Speciﬁcally, each datapoint is represented by a linear\ncombination of p representative datapoints (or landmarks), with p ≪n. The\n7\nobtained representation matrix ˆZ ∈Rp×n, for which the aﬃnity is calculated\nbetween n datapoints and the p landmarks, is sparse which in turn ensures a\nmore eﬃcient eigendecomposition as compare to the above mentioned eigende-\ncomposition of S (Eq. 1).\n3.3. Deep autoencoders\nAn autoencoder [35] is a neural network that implements an unsupervised\nlearning algorithm in which the parameters are learned in such a way that the\noutput values tend to copy the input training sample. The internal hidden layer\nof an autoencoder can be used to represent the input in a lower dimensional\nspace by capturing the most salient features.\nSpeciﬁcally, we can decompose an autoencoder in two parts, namely an en-\ncoder, fθ, followed by a decoder, gψ. The ﬁrst part allows the computation of\na feature vector yi = fθ(xi) for each input training sample, thus providing the\nencoding Y of the input dataset. The decoder part aims at transforming back\nthe encoding into its original representation, ˆxi = gψ(yi).\nThe sets of parameters for the encoder fθ and the decoder gψ are learned\nsimultaneously during the reconstruction task while minimizing the loss, referred\nto as J , where L is a cost function for measuring the divergence between the\ninput training sample and the reconstructed data,\nJAE(θ, ψ) =\nn\nX\ni=1\nL(xi, gψ(fθ(xi))).\n(2)\nThe encoder and decoder parts can have several shallow layers, yielding a deep\nautoencoder (DAE) that enables to learn higher order features. The network\narchitecture of these two parts usually mirrors each other.\nIt is remarkable that PCA can be interpreted as a linear AE with a single\nlayer [4]. In particular, PCA can be seen as a linear autoencoder with W ∈Rd×k\nwhere k ≤d. Taking fθ(X) = XW and gψ ◦fθ(X) = XWW⊤we ﬁnd the\nobjective function ||X −XWW⊤||2 optimized by PCA.\n8\n4.\nSpectral Clustering via Ensemble DAE\n4.1. Problem formulation\nGiven an n×d data matrix X, the goal is to ﬁrst obtain a set of m encodings\n{Yℓ}ℓ∈[1,m] using m DAE trained with diﬀerent hyperparameters settings. In\na second step, we construct a graph matrix Sℓassociated to each embedding\nYℓ, and then fuse the m graph matrices in an ensemble graph matrix S which\ncontains information provided by the m embeddings. Finally, to beneﬁt from\nthe common subspace shared by the m deep embeddings, spectral clustering is\napplied to S. The challenges of the problem are threefold,\n1. generate m deep embeddings,\n2. integrate the clustering in an ensemble learning framework,\n3. solve the clustering task in a highly eﬃcient way.\nEach of the above mentioned issues is discussed in the separate subsec-\ntions 4.2, 4.3 and 4.4 respectively. Most importantly, the SC-EDAE approach\nis provided with an ensemble optimization which is detailed in subsection 4.5.\n4.2. Deep embeddings generation\nThe cost function of an autoencoder, with an encoder fθ and a decoder gψ,\nmeasures the error between the input x ∈Rd×1 and its reconstruction at the\noutput ˆx ∈Rd×1. The encoder fθ and decoder gψ can have multiple layers of\ndiﬀerent widths. To generate m deep representations or encodings {Yℓ}ℓ∈[1,m],\nthe DAE is trained with diﬀerent hyperparameter settings (e.g., initialization,\nlayer widths) by optimizing the following cost function.\n||X −gψℓ◦fθℓ(X)||2\n(3)\nwhere gψℓand fθℓare learned with the hyperparameter setting ℓ, and Yℓ=\nfθℓ(X) (Fig. 1, (a)).\n9\n4.3. Graph matrix construction\nTo construct the graph matrix Sℓ, we use an idea similar to that of Landmark\nSpectral Clustering [27] and the Anchor-Graphs [26], where a smaller and sparser\nrepresentation matrix Zℓ∈Rn×p that approximates a full n × n aﬃnity matrix\nis built between the landmarks {uℓ\nj}j∈[1,p] and the encoded points {yℓ\ni}i∈[1,n]\n(Fig. 1, (a)). Speciﬁcally, a set of p points (p ≪n) are obtained through a k-\nmeans clustering on the embedding matrix Yℓ. These points are the landmarks\nwhich approximate the neighborhood structure.\nThen a non-linear mapping\nfrom data to landmark is computed as follows,\nzℓ\nij = Φ(yℓ\ni) =\nK(yℓ\ni, uℓ\nj)\nP\nj′∈N(i) K(yℓ\ni, uℓ\nj′)\n(4)\nwhere N(i) indicates the r (r < p) nearest landmarks around yℓ\ni. As proposed\nin [27], we set zℓ\nij to zero when the landmark uℓ\nj is not among the nearest neigh-\nbor of yℓ\ni, leading to a sparse aﬃnity matrix Zℓ. The function K(.) is used\nto measure the similarity between data yℓ\ni and anchor uℓ\nj with L2 distance in\nGaussian kernel space K(xi, xj) = exp(−||xi −xj||2/2σ2), and σ is the band-\nwidth parameter. The normalized matrix ˆZℓ∈Rn×p is then utilized to obtain\na low-rank graph matrix,\nSℓ∈Rn×n,\nSℓ= ZℓΣ−1Z⊤\nℓwhere Σ = diag(Z⊤\nℓ1).\nAs the Σ−1 normalizes the constructed matrix, Sℓis bi-stochastic, i.e.\nthe\nsummation of each column and row equal to one, and the graph Laplacian\nbecomes,\nSℓ= ˆZℓˆZ⊤\nℓ\nwhere ˆZℓ= ZℓΣ−1/2.\n(5)\n4.4. Ensemble of aﬃnity matrices\nGiven a set of m encodings {Yℓ}ℓ∈[1,m] obtained using m DAE trained with\ndiﬀerent hyperparameters setting ℓ, the goal is to merge the m graph similarity\nmatrices Sℓin an ensemble similarity matrix which contains information pro-\nvided by the m embeddings. To aggregate the diﬀerent similarity matrices, we\n10\nuse an Ensemble Clustering idea analogous to that proposed in [36, 37] where\na co-association matrix is ﬁrst built as the summation of all basic similarity\nmatrices, and where each basic partition matrix can be represented as a block\ndiagonal matrix. Thus, the SC-EDAE ensemble aﬃnity matrix is built as the\nsummation of the m basics similarity matrices using the following formula,\n¯S = 1\nm\nm\nX\nℓ=1\nSℓ.\n(6)\nNote that the obtained matrix ¯S is bi-stochastic, as Sℓ(Eq. 6).\nFor many\nnatural problems, ¯S is approximately block stochastic matrix, and hence the\nﬁrst k eigenvectors of ¯S are approximately piecewise constant over the k almost\ninvariant rows subsets [38].\nIn the sequel, we aim to compute, at lower cost, B that is shared by the m\ngraph matrices Sℓ, and obtained by optimizing the following trace maximization\nproblem\nmax\nB Tr(B⊤¯SB)\ns.t.\nB⊤B = I.\n(7)\n4.5. Proposed optimization and algorithm\nThe solution of Eq. 7 is to set the matrix B equal to the k eigenvectors\ncorresponding to the largest k eigenvalues of ¯S. However, as the computation of\nthe eigendecomposition of ¯S of size (n × n) is O(n3), relying on proposition 4.1,\nwe propose instead to compute the k left singular vectors of the concatenated\nmatrix,\n¯Z =\n1\n√m[ˆZ1| . . . |ˆZj| . . . |ˆZm].\n(8)\nUsing the sparse matrix ¯Z ∈Rn×Pm\nj=1 ℓj with Pm\nj=1 ℓj ≪n, instead of ¯S, which\nhas a larger dimension, naturally induces an improvement in the computational\ncost of B (Fig. 1, (b)).\nProposition 4.1. Given a set of m similarity matrices Sℓ, such that each\nmatrix Sℓcan be expressed as ZℓZ⊤\nℓ. Let ¯Z ∈Rn×Pm\nj=1 ℓj, where Pm\nj=1 ℓj ≪\nn, denoted as\n1\n√m[Z1| . . . |Zj| . . . |Zm], be the concatenation of the Zℓ’s, ℓ=\n11\n1, . . . , m. We ﬁrst have,\nmax\nB⊤B=I Tr(B⊤¯SB) ⇔\nmin\nB⊤B=I,M ||¯Z −BM⊤||2\nF .\n(9)\nThen, given SVD(¯Z), ¯Z = UΣV⊤and the optimal solution B∗is equal to U.\nProof. From the second term of Eq. 9, one can easily show that M∗= ¯Z⊤B.\nPlugging now the expression of M∗in Eq. 9, the following equivalences hold\nmin\nB⊤B=I,M ||¯Z −BM⊤||2\nF\n⇔\nmin\nB⊤B=I ||¯Z −BB⊤¯Z||2\nF\n⇔\nmax\nB⊤B=I Tr(B⊤¯Z¯Z⊤B)\n⇔\nmax\nB⊤B=I Tr(B⊤¯SB).\nOn the other hand, SVD(¯Z) leads to ¯Z = UΣV⊤(with U⊤U = I, V⊤V = I)\nand therefore to the eigendecomposition of ¯S as follows:\n¯S = ¯Z¯Z⊤\n=\n(UΣV⊤)(UΣV⊤)⊤\n=\nUΣ(V⊤V)ΣU⊤\n=\nUΣ2U⊤.\nThereby the left singular vectors of ¯Z are the same as the eigenvectors of ¯S.\nThe steps of our SC-EDAE algorithm are summarized in Algorithm 1 and il-\nlustrated by Figure 1. The SC-EDAE approach proposes a unique way to combine\nDAE encodings with clustering. It also directly beneﬁts from the low complex-\nity of the anchors strategy for both the graph aﬃnity matrix construction and\nthe eigendecomposition.\nSpeciﬁcally, the computational cost for the construction of each Zℓaﬃnity\nmatrix amounts to O(npℓe(t + 1)) (Alg. 1, step (b)) , where n is the number of\ndatapoints, pℓis the number of landmarks for the ℓth DAE (pℓ≪n), e is the\nsize of the DAE encoding Yℓ(e ≪n) and t is the number of iterations for the k-\nmeans that is used to select the landmarks. The computation of the Zℓmatrices\ncan be easily parallelized over multiple cores, leading to an eﬃcient computation\nof the ensemble aﬃnity matrix ¯Z. Furthermore, the eigendecomposition of the\n12\nsparse ensemble aﬃnity matrix ¯Z, which leads to the B embeddings (Alg. 1, step\n(c)), induces a computational complexity of O(p′3+p′2n), where p′ is the sum of\nall landmarks numbers for the concatenated Zℓmatrices, i.e. p′ = Pm\nj=1 ℓj ≪n.\nFinally, we need additional O(nctk) for the last k-means on B ∈Rn×k (Alg. 1,\noutput), where c is the number of centro¨ıds, usually equal to k the number of\neigenvectors, leading to O(ntk2).\nAlgorithm 1 : SC-EDAE algorithm\nInput: data matrix X;\nInitialize: m DAE with diﬀerent hyperparameters setting;\nDo:\n(a) Generate m deep embedding {Yℓ}l∈[1,m]\n(Eq. 3)\n(b) Construct the ensemble sparse aﬃnity matrix ¯Z ∈Rn×Pm\nj=1 ℓj\n(Eq. 4, 8)\n(c) Compute B∗∈Rn×k by performing sparse SVD on ¯Z\n(Eq. 9)\nOutput: Run k-means on B∗to get the ﬁnal clustering\nThe originality and eﬃciency of our ensemble method hinges on the replace-\nment of a costly eigendecomposition on ¯S ∈Rn×n by an eigendecomposition on\na low-dimensional and sparse matrix ¯Z ∈Rn×Pm\nj=1 ℓj, with Pm\nj=1 ℓj ≪n (Alg. 1,\nstep (c)). In particular, the sparsity of ¯Z enables the use of fast iterative and\npartial eigenvalue decomposition.\n5. Experiments\n5.1. Deep autoencoders settings\nFor our experiments, we trained fully connected autoencoders with an en-\ncoder fθ of three hidden layers of size 50, 75 or 100 for synthetic datasets\n(Tetra, Chainlink and Lsun; Section 5.3), and three hidden layers of size 500,\n750 or 1000 for real datasets (MNIST, PenDigits and USPS; Section 5.4), as sug-\ngested by Bengio et al. [8], in all possible orders. The decoder part gψ mirrors\nthe encoder stage fθ. For each DAE architecture (e.g., {750 −500 −1000},\n{100 −50 −75}), 5 encodings were generated with 50, 100, 150, 200 and 250\n13\nxi\nEncoder fθ1\nY1\nEncoding\n¯xi\nDecoder gψ1\nˆZ1 = Φ(Y1)\nxi\nEncoder fθℓ\nYℓ\nEncoding\n¯xi\nDecoder gψℓ\nˆZℓ= Φ(Yℓ)\n¯Z =\n1\n√m\n\u0002ˆZ1|...|ˆZℓ|...|ˆZm\n\u0003\nxi\nEncoder fθm\nYm\nEncoding\n¯xi\nDecoder gψm\nˆZm = Φ(Ym)\n(a) Produce m encodings with\ndiﬀerent DAE settings\n(b) Construct the concatenated\nsparse matrix ¯Z (Proposition IV.1)\n(c) Spectral clustering on m\nencodings common subspace\nFigure 1: Scheme of SC-EDAE. The SC-EDAE algorithm computes ﬁrst m encodings from\nDAE with diﬀerent hyperparameters settings (a), then generates m sparse aﬃnity matrix,\n{ˆZℓ}ℓ∈[1,m], that are concatenated in ¯Z (b), and ﬁnally performs a SVD on the ensemble graph\naﬃnity matrix ¯Z (c).\nepochs for real datasets and 200 epochs for synthetic datasets. The weights\ninitialization follows the Glorot’s approach [39] and all encoder/decoder pairs\nused rectiﬁed linears units (ReLUs), except for the output layer which requires\na sigmoid function. The autoencoder data are systematically L2 normalized.\nWe conﬁgure the autoencoders using the Keras tensorflow Python package,\nand compile the neural network with binary cross-entropy loss and Adam opti-\nmizer [40] with the default Keras parameters.\n5.2. SC-EDAE ensemble strategy\nThe ensemble strategy of SC-EDAE exploits the encodings {Yℓ}ℓ∈[1,m]vwhich\nare generated with either (i) m diﬀerent DAE initializations or m diﬀerent DAE\nepochs number in association with one DAE structure (e.g. d–500–1000–750–\ne, with d and e the input and encoding layers width resp.), or (ii) m DAE\nwith diﬀerent structures for the same number of landmarks and epochs.\nIn\n14\nboth cases, the SC-EDAE strategy enables to compute the m diﬀerent sparse\naﬃnity matrices {ˆZℓ}ℓ∈[1,m] (Eq. 4) and, following Proposition 4.1, generate\nthe ensemble aﬃnity matrix ¯Z (Eq. 8).\n5.3. Synthetic datasets\nAs a ﬁrst step, we focus on synthetic datasets to illustrate the SC-EDAE algo-\nrithm and show the class-separability information embedded in the left singular\nvectors matrix of ¯Z, noted as B⋆(Prop. 4.1 and Alg.1). We used generated\nsynthetic data sets selected from the Fundamental Clustering Problem Suite\n(FCPS)1. FCPS yields some hard clustering problems, a short description of\nTetra, Chainlink and Lsun FCPS data sets and the inherent problems related\nto clustering are given in Table 1. Following the experiments on synthetic data\nproposed by Yang et al. [18], we transformed the low-dimensional FCPS data,\nhi ∈R2 or R3, in high-dimensional datapoints, xi ∈R100. Speciﬁcally, the xi\nare transformed based on the following equation,\nxi = σ(Uσ(Whi))\n(10)\nwhere the entries of matrices W ∈R10×2 and U ∈R100×10 follow the zero-\nmean unit-variance i.i.d. Gaussian distribution, and the sigmoid function σ(.)\nintroduces nonlinearity.\nTable 1: Description of the used FCPS data sets.\nData sets\nCharacteristics\nSamples\nFeatures\nClusters\nMain Problem\nTetra\n400\n3\n4\ninner vs inter cluster distances\nChainlink\n1000\n3\n2\nnot linear separable\nLsun\n400\n2\n3\ndiﬀerent variances\n5.4. Real datasets\nOur SC-EDAE algorithm (Alg.1) is fully evaluated on three image datasets,\nnamely MNIST (Modiﬁed National Institute of Standards and Technology) [41],\n1The suite can be downloaded from the website of the author:\nhttp://www.uni-\nmarburg.de/fb12/datenbionik/data\n15\n(a) Tetra\n(b) Chainlink\n(c) Lsun\nOriginal\ndata\nSC-EDAE\nEmbeddings\nSC-EDAE acc = 1.00\nSC-EDAE acc = 1.00\nSC-EDAE acc = 0.90\nFigure 2: Visualization of the SC-EDAE embeddings on Tetra, Chainlink and Lsun datasets The\ntwo ﬁrst components of B (Alg.1, step (c)) gives a visualization of the datapoints separability\nwith the SC-EDAE method. Colors indicate the predicted labels.\nPenDigits (Pen-Based Recognition of Handwritten Digits) [42] and USPS (U.S.\nPostal Service) [43] and their DAE encodings (see Section 5.1 for details on DAE\nstructure).\nMNIST [41] The database is loaded from the Keras Python package. The\ntraining and testing sets contain respectively 60, 000 and 10, 000 images\nof size 28 × 28 of the integers in range 0 −9. The images are of grayscale\nlevels rescaled within [0, 1] by dividing by 255.\nPenDigits [42] The training and testing sets contain respectively 7, 494\nand 3, 498 images of size 16×16 of the integers in range 0−9. The images\nwith 16 numeric attributes rescaled within [0, 1] by dividing by 100.\nUSPS [43] The database is prepared as proposed in [23] and contains\n9, 298 images of size 16 × 16 pixels of the 10- digits (integers in range\n0 −9) rescaled within [0, 1].\n16\nTable 2: Class distribution for MNIST, PenDigits and USPS datasets.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nMNIST\n5923\n6742\n5958\n6131\n5842\n5421\n5918\n6265\n5851\n5949\nPenDigits\n780\n779\n780\n719\n780\n720\n720\n778\n719\n719\nUSPS\n1194\n1005\n731\n658\n652\n556\n664\n645\n542\n644\nThe classes distribution for each dataset is given in Table 2.\nMNIST and\nPenDigits appear as balanced-class datasets while USPS has an imbalanced\ndistribution.\n5.5. Experiment results\n5.5.1. Evaluation on synthetic data\nSynthetic data enable us to easily explore the separability capacity of the\nembeddings matrix B. For the experiments related to synthetic data, SC-EDAE\nis used in its ensemble structure version, with m = 6 encodings from diﬀerent\nstructures, and the number of landmarks is set to 100. Applying SC-EDAE on\nthe data sets Tetra, Chainlink and Lsun, we note that the 2D representations\nof the obtained clusters reﬂect the real cluster structure (Fig. 2 a, b, c; projec-\ntion on the two ﬁrst components of the matrix B as computed in Alg.1, step\nc). The SC-EDAE accuracy is of 1.00 for Tetra and Chainlink, and 0.90 for\nLsun. The colored labels correspond to the predicted clusters. Complementary\ntests with diﬀerent transformation functions conﬁrm this trend (see annexes,\nSection Appendix A.1).\n5.5.2. Baseline evaluations on real data\nAs baseline, we ﬁrst evaluate k-means and LSC [27] on the three real\ndatasets. The kmeans++ approach corresponds to the scikit-learn Python\npackage k-means implementation with the default parameters and kmeans++\ninitialization scheme [44]. We implemented the LSC method in Python, fol-\nlowing the Matlab implementation proposed in [27], and kept the same default\nparameters.\nThe LSC landmarks initialization is done with k-means, which\nhas been shown to provide better accuracy results than the random initializa-\ntion [27, 15]. We consider landmarks number within 100 and 1000, by step of\n17\n100. The evaluations are done either on the original datasets (Table 3, columns\nLSC and kmeans++ or on the encodings (Table 3, columns DAE-LSC and DAE-\nkmeans++). The accuracy reported for LSC and k-means++ corresponds to the\nmean over 10 clustering replicates on the original datasets, over all epoch and\nlandmark numbers. The accuracy reported for DAE-LSC and DAE-kmeans++\ncorresponds to an average over 50 replicates (10 replicates on each of the 5 en-\ncodings per DAE structure), over all epoch and landmark numbers (see annexes\nfor complementary results per DAE structure, Section Appendix A.2).\nAs can be seen from Table 3 and already reported in [27], LSC outperforms\nkmeans++ for the clustering task on the three datasets (bold values, columns\nLSC and kmeans++), yet with larger standard deviations. The same trend is\nobserved when applying LSC and kmeans++ on encodings, with standard devi-\nations of similar magnitude for both clustering methods (bold values, columns\nDAE-LSC and DAE-kmeans++).\nTable 3: Mean clustering accuracy for LSC and k-means on original real datasets\nand encodings: Evaluations on MNIST, PenDigits, USPS data and their encodings. Bold\nvalues highlight the higher accuracy values.\nData\nLSC\nkmeans++ DAE structure DAE-LSC DAE-kmeans++\nMNIST\n68.55 ±2.25 55.13 ±0.05\n500–750–1000\n87.06 ±8.27\n76.33 ±7.69\n500–1000–750\n90.48 ±5.20\n79.22 ±5.93\n750–500–1000\n88.31 ±5.46\n77.71 ±6.03\n750–1000–500\n90.30 ±4.89\n79.45 ±5.81\n1000–500–750\n91.54 ±3.06\n79.98 ±5.98\n1000–750–500\n90.96 ±3.98\n77.70 ±5.09\nPenDigits 80.17 ±3.76 73.89 ±3.97\n500–750–1000\n85.59 ±2.34\n73.64 ±4.00\n500–1000–750\n85.11 ±3.15\n74.67 ±3.43\n750–500–1000\n85.36 ±2.91\n73.47 ±3.89\n750–1000–500\n85.27 ±2.92\n74.64 ±4.01\n1000–500–750\n85.02 ±2.72\n74.20 ±3.84\n1000–750–500\n84.39 ±3.04\n73.78 ±3.55\nUSPS\n77.20 ±1.49 68.36 ±0.08\n500–750–1000\n81.78 ±8.08\n72.85 ±3.52\n500–1000–750\n83.47 ±7.40\n73.44 ±3.70\n750–500–1000\n79.72 ±6.21\n72.46 ±2.78\n750–1000–500\n80.29 ±5.70\n73.80 ±3.51\n1000–500–750\n81.39 ±4.46\n74.07 ±3.07\n1000–750–500\n83.08 ±5.64\n72.41 ±3.06\n18\nThe results from Table 3 demonstrate that the simple combination of DAE\nand LSC or k-means already reaches higher accuracy and smaller standard\ndeviations than without the autoencoder step. These results also show the ad-\nvantage of associating the DAE encodings with the landmark-based representa-\ntion over the k-means approach for the clustering task (columns DAE-LSC and\nDAE-kmeans++). In particular, the average accuracy for the MNIST and USPS\ndatasets varies within [87.06; 91.54] and [79.72; 83.47] respectively for DAE-LSC\nand within [77.70; 79.98] and [72.41; 74.07] respectively for DAE-kmeans++.\nAlthough the encodings generated by the deep autoencoder improve the clus-\ntering accuracy, ﬁnding a priori the most appropriate DAE structure remains a\nchallenging task. The accuracy may also vary for diﬀerent landmark and epoch\nnumbers (see Table 5 and annexes Tables A.7 & A.8). As will be seen in the\nfollowing sections, the ensemble strategy of SC-EDAE provides a straightforward\nway to alleviate these issues and avoid arbitrary DAE hyperparameters setting.\n5.5.3. SC-EDAE ensemble evaluations\nThe Table 4 summarizes the performance of our LSC-based ensemble ap-\nproach in the two cases detailed in section 5.2. Speciﬁcally, the columns Ens.Init.\nand Ens.Ep. indicate the clustering accuracy for the case (i) with an ensemble\napproach on the DAE weights initialization (Ens.Init., m = 5) and the DAE\ntraining epoch numbers (Ens.Ep., m = 5). The clustering accuracy values for\nthe ensemble approach on various DAE structures, i.e. case (ii), is provided in\nthe column Ens.Struct. (m = 6).\nThe SC-EDAE ensemble strategy provides higher clustering accuracy as com-\npare to the baseline evaluations (Table 3). In particular, the mean accuracy\nvalues obtained with the ensemble strategy for MNIST, PenDigits and USPS can\nreach, 95.33 ± 0.07, 87.28 ± 0.48 and 85.22 ± 2.14 respectively, vs. 91.54 ± 3.06,\n85.59 ± 2.34 and 83.47 ± 7.40 (Table 3).\nThe SC-EDAE ensemble approach on the DAE structures (Ens.Struct.) en-\nables also to reach higher accuracy as compare to the baseline evaluations\nfor MNIST (93.23 ± 0.28 vs.\n91.54 ± 3.06) and PenDigits (86.44 ± 1.42 vs.\n19\n85.59 ± 2.34), but with the added beneﬁt of avoiding the arbitrary choice of\na particular DAE structure. The SC-EDAE results for USPS with an ensemble\non several structures are lower than our reference evaluations (81.78 ± 3.61 vs.\n83.47 ± 7.40), yet the accuracy value remains fairly high with lower standard\ndeviation.\nTable 4: Mean clustering accuracy for SC-EDAE, ensemble on initializations, epochs\nnumber and structures: Bold values highlight the higher accuracy values.\nDataset\nDAE structure\nEns.Init.\nEns.Ep.\nEns.Struct.\nMNIST\n500–750–1000\n89.19 ±0.41\n85.54 ±4.30\n93.23 ±2.84\n500–1000–750\n95.33 ±0.07\n94.34 ±2.68\n750–500–1000\n92.15 ±0.25\n92.03 ±3.87\n750–1000–500\n92.65 ±0.13\n92.26 ±3.71\n1000–500–750\n94.28 ±0.20\n94.57 ±1.48\n1000–750–500\n93.87 ±0.38\n95.25 ±0.59\nPenDigits\n500-750-1000\n86.80 ±0.74\n87.08 ±1.10\n86.44 ±1.42\n500–1000–750\n85.95 ±0.73\n86.69 ±1.33\n750–500–1000\n86.69 ±0.87\n87.27 ±0.60\n750–1000–500\n86.48 ±1.09\n86.91 ±1.01\n1000–500–750\n86.75 ±6.40\n86.96 ±8.10\n1000–750–500\n86.66 ±9.50\n87.28 ±0.48\nUSPS\n500–750–1000\n80.07 ±1.95\n81.36 ±5.09\n81.78 ±3.61\n500–1000–750\n80.54 ±0.77\n82.06 ±3.54\n750–500–1000\n79.49 ±1.19\n81.10 ±3.86\n750–1000–500\n79.29 ±1.05\n79.88 ±2.69\n1000–500–750\n84.12 ±1.80\n81.89 ±3.21\n1000–750–500\n85.22 ±2.14\n84.96 ±3.29\nWhile the SC-EDAE method aims at providing an ensemble strategy for the\ndeep architecture settings (Ens.Init., Ens.Ep. and Ens.Struct., Table 4), it relies\nalso on the LSC idea which depends on the number of landmarks. We studied\nthe possibility of an ensemble on the number of landmarks (m = 5). As can be\nseen from Table 5, which provides mean accuracy on 10 replicates, the ensemble\nstrategy enables again to reach high accuracy values as compared to our baseline\nevaluations. The results still remain dependent from the DAE structure type,\nin particular for MNIST and USPS, and we would therefore recommend to use\nSC-EDAE in its ensemble structure version (ie., Ens.Struct.).\n20\nTable 5: Mean clustering accuracy for SC-EDAE, ensemble on landmarks:\nBold values\nhighlight the higher accuracy values.\nDAE structure\nMNIST\nPenDigits\nUSPS\n500–750–1000\n88.84 ±1.22\n87.31 ±1.13\n82.17 ±3.79\n500–1000–750\n95.35 ±0.20\n87.21 ±0.36\n81.96 ±2.74\n750–500–1000\n92.48 ±1.27\n87.16 ±0.99\n80.61 ±3.46\n750–1000–500\n92.53 ±0.76\n87.09 ±0.95\n80.30 ±1.26\n1000–500–750\n93.76±1.14\n86.67 ±1.40\n86.35 ±2.62\n1000–750–500\n95.08 ±0.17\n87.13 ±1.26\n87.32 ±4.85\n5.6. Evaluation in terms of NMI and ARI\nEvaluating clustering results is not a trivial task. The clustering accuracy is\nnot always a reliable measure when the clusters are not balanced and the number\nof clusters is high. To better appreciate the quality of our approach, in the sequel\nwe retain two widely used measures to assess the quality of clustering, namely\nthe Normalized Mutual Information [36] and the Adjusted Rand Index [45].\nIntuitively, NMI quantiﬁes how much the estimated clustering is informative\nabout the true clustering, while the ARI measures the degree of agreement\nbetween an estimated clustering and a reference clustering. Higher NMI/ARI\nis better.\nWe report in Figure 3 the ARI and NMI values for the three real datasets\n(MNIST, PenDigits and USPS). The ARI and NMI values are given for the\nbaseline evaluations (DAE-kmeans++ and DAE-LSC; average results over 10\nruns), and the various ensemble versions of SC-EDAE (Ens.Init, Ens.Ep. and\nEns.Struct.; average results over 10 runs for each of the 5 diﬀerent encodings).\nThe ensemble paradigm of SC-EDAE ensures high ARI and NMI results with low\nstandard deviations for all real datasets, even for USPS which is an imbalanced-\nclass dataset (Fig. 3, green boxplots).\nWe also detail the ARI and NMI evaluations per DAE structure in annexes,\nTables A.7 & A.8. These supplementary results highlight the strong inﬂuence\nof a particular DAE structure on the ARI and NMI values. As an example, the\nARI minimal and maximal values for DAE-LSC are 73.66 and 77.75 respectively\nfor USPS, a diﬀerence of 4.09 (Table A.7). Another striking example can be\n21\nMNIST\nPenDigits\nUSPS\nFigure 3:\nComparison of Adjusted Rand Index (ARI) and Normalized Mutual Information\n(NMI) for our SC-EDAE approach (ensemble on initialization, epochs and structures; 10 runs)\nand baseline methods (combination of deep autoencoders and k-means or LSC; 10 runs for\neach of the 5 encodings).\nfound for the SC-EDAE in its ensemble initialization version (Ens.Init.) applied\nto MNIST, where the ARI values ﬂuctuate within a [81.87; 90.17] (Table A.8).\nBased on these evaluations, and as already mentioned (Section 5.2), we would\nrecommend to use SC-EDAE in its ensemble structure version (i.e., Ens.Struct.)\nto alleviate the issue of the DAE structure choice.\n5.7. Comparison to deep k-means variants\nSeveral strategies that use deep learning algorithm and k-means approaches,\nsequentially or jointly, have demonstrated accuracy improvement on the clus-\ntering task. Among these methods, two approaches can now be considered as\nstate-of-the-art methods, namely IDEC (Improved Deep Embedded Cluster-\ning) [28] and DCN (Deep Clustering Network) [18]. Very recently, the DKM\n(Deep k-means) algorithm, which applies a k-means in an AE embedding space,\noutperformed these approaches [46].\n22\nTable 6: Mean clustering accuracy and NMI comparison with deep k-means vari-\nants: Mean accuracy and NMI for MNIST and USPS over 10 replicates with SC-EDAE and\ncomparison to baselines and state-of-the-art approaches.\nBold values highlight the higher\naccuracy values.\nModel\nMNIST\nUSPS\nACC\nNMI\nACC\nNMI\nbaselines\nkmeans++\n55.13 ±0.05\n52.89 ±0.02\n68.36 ±0.08\n65.67 ±0.10\nLSC\n68.55 ±2.25\n70.54 ±0.83\n77.20 ±1.49\n79.48 ±0.90\nDAE+kmeans++\n78.40 ±6.09\n71.97 ±4.13\n73.17 ±3.27\n70.48 ±1.84\nDAE+LSC\n89.78 ±5.14\n83.06 ±4.38\n81.62 ±6.25\n80.44 ±3.39\nno pretraining required\nSC-EDAE Ens.Init.\n92.91 ±0.24\n87.65 ±0.18\n81.46 ±1.48\n82.88 ±0.59\nSC-EDAE Ens.Ep.\n92.33 ±2.77\n87.72 ±2.42\n81.88 ±3.62\n83.03 ±1.88\nSC-EDAE Ens.Struct.\n93.23 ±2.84\n87.93 ±2.27\n81.78 ±3.61\n83.17 ±1.96\nDeep clustering approaches without pretraining (Fard et al. 2018) [46]\nDCNnp\n34.8 ±3.0\n18.1 ±1.0\n36.4 ±3.5\n16.9 ±1.3\nIDECnp\n61.8 ±3.0\n62.2 ±1.6\n53.9 ±5.1\n50.0 ±3.8\nDKMa\n82.3 ±3.2\n78.0 ±1.9\n75.5 ±6.8\n73.0 ±2.3\nDeep clustering approaches with pretraining (Fard et al. 2018) [46]\nDCNp\n81.1 ±1.9\n75.7 ±1.1\n73.0 ±0.8\n71.9 ±1.2\nIDECp\n85.7 ±2.4\n86.4 ±1.0\n75.2 ±0.5\n74.9 ±0.6\nDKMp\n84.0 ±2.2\n79.6 ±0.9\n75.7 ±1.3\n77.6 ±1.1\nWe compare SC-EDAE to these three methods and summaries these evalua-\ntions in Table 6. The last six rows of Table 6 are directly extracted from the\nDKM authors study [46]. The accuracy and NMI values of these six rows are an\naverage over 10 runs. The other values correspond to our evaluations. Specif-\nically, baseline results are given in the ﬁrst four rows, and correspond to the\nclustering task via k-means++ or LSC (average results over 10 runs), and via\na combination of DAE and k-means or LSC (average results over 10 runs for\neach of the 5 diﬀerent encodings). The SC-EDAE rows gives the accuracy and\nNMI results for our ensemble method, with an ensemble over several initial-\nizations (SC-EDAE Ens.Init.), epoch numbers (SC-EDAE Ens.Ep.) and DAE\narchitectures (SC-EDAE Ens.Struct.).\nAs can be seen from Table 6, while our SC-EDAE approach does not require\nany pretraining, it outperforms the DCN and IDEC methods in there pretrained\n23\nversion (Table 6, DCNp and IDECp results). The DKM method performs well\nwith and without pretraining. Yet, our SC-EDAE approach reaches higher accu-\nracy and NMI results than the DKM approach with and without pretraining.\n5.8. Visualization of latent space\nWe investigate the quality of the representation learned with SC-EDAE and\nin particular the positive inﬂuence of the left singular vectors matrix of ¯Z, B\n(Alg.1, step c), on the clustering task. Speciﬁcally, we visualize the datapoints\nnearest-neighbor from the B matrix using the t-SNE visualization tool [47] that\ncan project embeddings into two components (TSNE Python version from the\nsklearn package ). The results are given in Figure 4. The t-SNE hyperparam-\neters perplexity, learning rate and number of iterations are set to 40, 200 and\n500 for MNIST, and 25, 100 and 400 for PenDigits and USPS, following the rec-\nommendations and experimental setup of Maaten et al. [47]. For each dataset,\nMNIST\nPenDigits\nUSPS\nFigure 4:\nt-SNE Vizualization of the embeddings B from the SC-EDAE approach on MNIST,\nPenDigits and USPS datasets. The t-SNE approach provides clustering visualization of the\ndatapoints from the B embeddings. Colors indicate the ground truth labels corresponding to\nthe digits from 0 to 9.\nwe can observe clearly separated clusters. The ground truth labels nicely match\nthe t-SNE datapoints gathering, highlighting the ability of SC-EDAE to separate\ndata according to the underlying classes. As already noticed in [47], the t-SNE\nresults obtained from the SC-EDAE ensemble aﬃnity matrix reﬂects the local\nstructure of the data, such as the orientation of the ones, by showing elongated\nclusters (e.g., Fig. 4, red cluster).\n24\n6. Conclusion\nWe report in this paper a novel clustering method that combines the advan-\ntages of deep learning, spectral clustering and ensemble strategy. Several studies\nhave proposed to associate, either sequentially or jointly, deep architecture and\nclassical clustering methods to improve the partitioning of large datasets. How-\never, these methods are usually confronted to important issues related to well\nknown challenges with neural networks, such as weight initialization or struc-\nture settings. Our SC-EDAE approach alleviates these issues by exploiting an\nensemble procedure to combine several deep models before applying a spectral\nclustering; it is quite simple and can be framed in three steps:\n• generate m deep embeddings from the original data,\n• construct a sparse and low-dimensional ensemble aﬃnity matrix based on\nanchors strategy,\n• apply spectral clustering on the common space shared by the m encoding.\nThe experiments on real and synthetic datasets demonstrate the robustness\nand high performance of SC-EDAE on image datasets. SC-EDAE can be used in\ndiﬀerent versions with an ensemble on weights initialization, epoch numbers\nor deep architectures. These variants provide higher accuracy, ARI and NMI\nresults than state-of-the art methods. Most importantly, the high performance\nof SC-EDAE is obtained without any deep models pretraining.\nThe proposed method also beneﬁts from the anchors strategy. The anchors\nprovide a sparse and low-dimensional ensemble aﬃnity matrix that ensures an\neﬃcient spectral clustering. As a complementary improvement, one could easily\nimplements the parallelization of the m encodings computation in the ﬁrst step\nof the SC-EDAE procedure. Our experiments show that few diﬀerent encodings\nalready lead to signiﬁcant performance improvement, yet more complex datasets\ncould require larger amount of various encodings, and such parallelization would\nfacilitate the SC-EDAE use.\n25\nAppendix A. Appendix\nAppendix A.1. Supplementary experiments on synthetic data\nAs proposed in [18], we provide two complementary examples of clustering\nwith SC-EDAE that demonstrate the ability of the B embeddings to correctly\nrecover the underlying classes of a given dataset. We ﬁrst consider the following\ntwo transformations, xi = σ(σ(Whi))2 and xi = tan(σ(Whi)). The Figure A.5\nshows the two ﬁrst embeddings of B obtained with the transformed data. This\nrepresentation highlights the separability power of SC-EDAE. The correspond-\ning accuracy is 1.00 for Tetra, Chainlink and Lsun. For both supplementary\ntransformation, we can observe patterns that are similar to clusters presented\nin the main text (Fig. 2).\nTetra\nChainlink\nLsun\nxi=σ(σ(Whi))2\nxi=tan(σ(Whi))\nFigure A.5:\nEmbeddings B from SC-EDAE on Tetra,\nChainlink and Lsun high-\ndimensional datasets:\nColors indicate the predicted labels.\n26\nAppendix A.2. Complementary experiments on real data\nAppendix A.2.1. Baseline evaluations\nThe Table A.7 provides complementary results for the baseline evaluations\non real datasets. Speciﬁcally, it gives the mean Adjusted Rand Index (ARI) and\nthe Normalized Mutual Information (NMI) for LSC and kmeans++. The mean\nis taken over 10 replicates on the original datasets, over all epoch and landmark\nnumbers. The results for DAE-LSC and DAE-kmeans++ are averaged over 50\nreplicates (10 replicates on each of the 5 encodings per DAE structure type),\nover all epoch and landmark numbers. These results follow the same trend as\nthe accuracy results detailed in main text.\nTable A.7: Mean clustering Adjusted Rand Index (ARI) and Normalized Mutual\nInformation (NMI) for LSC and k-means on original real datasets and encodings.\nEvaluations on MNIST, PenDigits, USPS data and their encodings. Bold values highlight the\nhigher results\nARI\nNMI\nARI\nNMI\nData LSC kmeans++ LSC kmeans++ DAE structure\nDAE-LSC\nDAE-kmeans++\nDAE-LSC\nDAE-kmeans++\nMNIST\n54.86 ±1.69\n39.98 ±0.03\n70.54 ±0.83\n52.89 ±0.02\n500–750–1000 78.16 ±10.26 63.58 ±8.14 80.88 ±6.58 70.29 ±5.38\n500–1000–750 82.84 ±1.20 67.66 ±6.36 84.04 ±1.20 73.21 ±4.02\n750–500–1000 79.20 ±8.42 65.32 ±7.36 81.57 ±5.55 71.50 ±4.64\n750–1000–500 82.23 ±6.33 66.75 ±6.48 83.52 ±4.13 72.32 ±4.10\n1000–500–750 83.66 ±4.23 67.48 ±5.80 84.29 ±2.80 72.80 ±3.52\n1000–750–500 83.15 ±4.81 65.28 ±7.48 84.07 ±2.99 71.69 ±3.09\nPenDigits\n68.58 ±3.79\n57.58 ±2.61\n79.78 ±1.42\n69.72 ±0.58\n500–750–1000 74.12 ±2.53 59.62 ±3.79 81.06 ±1.43 69.33 ±2.11\n500–1000–750 73.18 ±3.55 58.97 ±3.41 80.46 ±1.85 69.14 ±1.99\n750–500–1000 73.47 ±3.12 58.23 ±3.73 80.55 ±1.48 68.56 ±2.25\n750–1000–500 73.30 ±3.17 58.82 ±3.73 80.38 ±1.62 68.74 ±2.07\n1000–500–750 73.07 ±2.97 58.92 ±3.35 80.23 ±1.79 69.53 ±2.23\n1000–750–500 73.40 ±3.17 58.16 ±3.12 80.66 ±1.60 68.83 ±1.90\nUSPS\n77.09 ±1.52\n57.70 ±0.12\n79.48 ±0.90\n65.67 ±0.10\n500–750–1000 76.12 ±8.45 63.62 ±3.02 80.32 ±4.89 70.35 ±2.25\n500–1000–750 77.34 ±7.71 64.22 ±3.34 80.69 ±4.30 70.37 ±2.16\n750–500–1000 73.66 ±6.38 63.34 ±2.67 78.77 ±3.81 70.11 ±1.77\n750–1000–500 75.17 ±5.23 64.87 ±2.66 80.13 ±3.11 70.94 ±2.03\n1000–500–750 76.15 ±4.29 64.63 ±2.02 80.98 ±2.12 70.80 ±1.36\n1000–750–500 77.75 ±5.02 63.88 ±2.07 81.74 ±2.12 70.33 ±1.45\n27\nAppendix A.2.2. SC-EDAE ensemble evaluations\nThe Table A.8 provides complementary results for the ensemble evaluations\non real datasets. Speciﬁcally, it gives the mean Adjusted Rand Index (ARI) and\nthe Normalized Mutual Information (NMI) for SC-EDAE. The mean is taken over\n10 replicates on the encodings. The columns Ens.Init. and Ens.Ep. indicate the\nresults for an ensemble approach on the DAE weight initializations (Ens.Init.,\nm = 5) and the DAE training epoch numbers (Ens.Ep., m = 5). The column\nEns.Struct. provides the evaluations for an ensemble approach on various DAE\nstructure types (m = 6).\nTable A.8: Mean clustering Adjusted Rank Index (ARI) and Normalized Mutual\nInformation (NMI) for the SC-EDAE algorithm. The ensemble is done on initializations,\nepochs number and structures. Bold values highlight the higher results.\nARI\nNMI\nData DAE structure\nEns.Init.\nEns.Ep.\nEns.Struct.\nEns.Init.\nEns.Ep.\nEns.Struct.\nMNIST\n500–750–1000\n81.87 ±0.49 83.22 ±7.07\n87.25 ±3.88\n84.69 ±0.28 85.44 ±4.22\n87.93 ±2.27\n500–1000–750\n90.17 ±0.14 88.84 ±3.93\n89.59 ±0.10 88.87 ±2.32\n750–500–1000\n84.66 ±1.71 85.29 ±5.18\n86.86 ±0.21 86.68 ±3.06\n750–1000–500\n86.18 ±0.20 85.86 ±4.89\n87.44 ±0.13 87.17 ±2.66\n1000–500–750\n88.47 ±0.27 88.86 ±2.49\n88.53 ±0.17 88.71 ±1.45\n1000–750–500\n88.59 ±0.38 90.02 ±1.13\n88.81 ±0.18 89.44 ±0.81\nPenDigits\n500–750–1000\n75.67 ±0.80 76.15 ±1.23\n74.88 ±1.57\n82.33 ±0.48 82.73 ±0.78\n81.87 ±0.84\n500–1000–750\n74.00 ±0.88 74.83 ±1.82\n80.96 ±0.43 81.68 ±0.99\n750–500–1000\n75.13 ±0.95 75.71 ±0.81\n81.89 ±0.48 82.19 ±0.51\n750–1000–500\n74.98 ±1.20 75.38 ±1.18\n81.88 ±0.61 81.99 ±0.73\n1000–500–750\n75.07 ±0.69 75.63 ±0.89\n81.86 ±0.39 82.14 ±0.70\n1000–750–500\n75.16 ±0.10 75.60 ±0.76\n81.97 ±0.52 82.13 ±0.57\nUSPS\n500–750–1000\n75.68 ±1.80 76.85 ±5.37\n77.61 ±3.69\n81.93 ±0.81 82.39 ±3.04\n83.17 ±1.96\n500–1000–750\n76.12 ±0.71 77.67 ±3.63\n82.29 ±0.39 83.03 ±1.93\n750–500–1000\n74.32 ±1.02 76.53 ±3.74\n81.69 ±0.48 82.07 ±1.87\n750–1000–500\n75.33 ±0.93 75.70 ±2.82\n82.34 ±0.43 82.35 ±1.59\n1000–500–750\n79.93 ±1.64 77.73 ±3.00\n84.28 ±0.65 83.58 ±1.37\n1000–750–500\n80.96 ±1.99 80.79 ±3.21\n84.75 ±0.78 84.75 ±1.47\n28\nReferences\nReferences\n[1] M. Yamamoto, H. Hwang, A general formulation of cluster analysis with di-\nmension reduction and subspace separation, Behaviormetrika 41 (1) (2014)\n115–129.\n[2] K. Allab, L. Labiod, M. Nadif, A Semi-NMF-PCA uniﬁed framework for\ndata clustering, IEEE Trans. Knowl. Data Eng. 29 (1) (2017) 2–16.\n[3] K. Allab, L. Labiod, M. Nadif, Simultaneous spectral data embedding and\nclustering, IEEE Trans. Neural Netw. Learning Syst. 29 (12) (2018) 6396–\n6401.\n[4] G. E. Hinton, R. Salakhutdinov, Reducing the Dimensionality of Data with\nNeural Networks, Science 313 (2006) 504–507.\n[5] Y. Bengio, et al., Learning deep architectures for ai, Foundations and\ntrends R⃝in Machine Learning 2 (1) (2009) 1–127.\n[6] P. Baldi, Autoencoders, unsupervised learning, and deep architectures, in:\nUnsupervised and Transfer Learning - Workshop held at ICML 2011, 2012,\npp. 37–50.\n[7] Y. Bengio, L. Yao, G. Alain, P. Vincent, Generalized denoising auto-\nencoders as generative models, in: NIPS 2013, 2013, pp. 899–907.\n[8] Y. Bengio, P. Lamblin, D. Popovici, H. Larochelle, Greedy layer-wise train-\ning of deep networks, in: Advances in neural information processing sys-\ntems, 2007, pp. 153–160.\n[9] M. Shao, S. Li, Z. Ding, Y. Fu, Deep linear coding for fast graph clustering,\nin: IJCAI 2015, 2015, pp. 3798–3804.\n[10] F. Tian, B. Gao, Q. Cui, E. Chen, T. Liu, Learning deep representations\nfor graph clustering, in: AAAI 2014, 2014, pp. 1293–1299.\n29\n[11] W. Wang, Y. Huang, Y. Wang, L. Wang, Generalized autoencoder: A\nneural network framework for dimensionality reduction, in: IEEE CVPR\nWorkshops 2014, 2014, pp. 496–503.\n[12] P. Huang, Y. Huang, W. Wang, L. Wang, Deep embedding network for\nclustering, in: ICPR 2014, 2014, pp. 1532–1537.\n[13] M. Leyli-Abadi, L. Labiod, M. Nadif, Denoising autoencoder as an eﬀective\ndimensionality reduction and clustering of text data, in: PAKDD 2017,\n2017, pp. 801–813.\n[14] L. Yang, X. Cao, D. He, C. Wang, X. Wang, W. Zhang, Modularity based\ncommunity detection with deep learning, in: IJCAI 2016, 2016, pp. 2252–\n2258.\n[15] E. Banijamali, A. Ghodsi, Fast spectral clustering using autoencoders and\nlandmarks, in: ICIAR 2017, 2017, pp. 380–388.\n[16] S. Wang, Z. Ding, Y. Fu, Feature selection guided auto-encoder, in: AAAI\n2017, 2017, pp. 2725–2731.\n[17] J. Xie, R. B. Girshick, A. Farhadi, Unsupervised deep embedding for clus-\ntering analysis, in: ICML, 2016, pp. 478–487.\n[18] B. Yang, X. Fu, N. D. Sidiropoulos, M. Hong, Towards k-means-friendly\nspaces: Simultaneous deep learning and clustering, in: Proceedings of the\n34th International Conference on Machine Learning, ICML 2017, Sydney,\nNSW, Australia, 6-11 August 2017, 2017, pp. 3861–3870.\n[19] K. Tian, S. Zhou, J. Guan, Deepcluster:\nA general clustering frame-\nwork based on deep learning, in: M. Ceci, J. Hollm´en, L. Todorovski,\nC. Vens, S. Dˇzeroski (Eds.), Machine Learning and Knowledge Discovery\nin Databases, 2017.\n[20] L. Yang, X. Cao, D. He, C. Wang, X. Wang, W. Zhang, Modularity based\ncommunity detection with deep learning, in: Proceedings of the Twenty-\n30\nFifth International Joint Conference on Artiﬁcial Intelligence, IJCAI’16,\n2016.\n[21] M. Seuret, M. Alberti, M. Liwicki, R. Ingold, Pca-initialized deep neural\nnetworks applied to document image analysis, in: 14th IAPR International\nConference on Document Analysis and Recognition, ICDAR 2017, Kyoto,\nJapan, November 9-15, 2017, 2017, pp. 877–882.\n[22] D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol, P. Vincent, S. Ben-\ngio, Why does unsupervised pre-training help deep learning?, Journal of\nMachine Learning Research 11 (Feb) (2010) 625–660.\n[23] X. Guo, L. Gao, X. Liu, J. Yin, Improved deep embedded clustering with\nlocal structure preservation, in: International Joint Conference on Artiﬁcial\nIntelligence (IJCAI-17), 2017, pp. 1753–1759.\n[24] J. Xie, R. Girshick, A. Farhadi, Unsupervised deep embedding for clustering\nanalysis, in: International conference on machine learning, 2016, pp. 478–\n487.\n[25] P. Ji, T. Zhang, H. Li, M. Salzmann, I. Reid, Deep subspace clustering\nnetworks, in: I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fer-\ngus, S. Vishwanathan, R. Garnett (Eds.), Advances in Neural Information\nProcessing Systems 30, Curran Associates, Inc., 2017, pp. 24–33.\n[26] W. Liu, J. He, S.-F. Chang, Large graph construction for scalable semi-\nsupervised learning, in: Proceedings of the 27th International Conference\non International Conference on Machine Learning, ICML’10, 2010.\n[27] X. Chen, D. Cai, Large scale spectral clustering with landmark-based\nrepresentation, in:\nTwenty-Fifth Conference on Artiﬁcial Intelligence\n(AAAI’11), 2011.\n[28] X. Guo, L. Gao, X. Liu, J. Yin, Improved deep embedded clustering with\nlocal structure preservation, in: Proceedings of the 26th International Joint\nConference on Artiﬁcial Intelligence, IJCAI’17, 2017.\n31\n[29] D. Verma, M. Meila, A comparison of spectral clustering algorithms, Uni-\nversity of Washington Tech Rep UWCSE030501 1 (2003) 1–18.\n[30] U. Von Luxburg, A tutorial on spectral clustering, Statistics and computing\n17 (4) (2007) 395–416.\n[31] J. Shi, J. Malik, Normalized cuts and image segmentation, IEEE Transac-\ntions on pattern analysis and machine intelligence 22 (8) (2000) 888–905.\n[32] A. Y. Ng, M. I. Jordan, Y. Weiss, On spectral clustering: Analysis and an\nalgorithm, in: Advances in neural information processing systems, 2002,\npp. 849–856.\n[33] M. Meila, J. Shi, Learning segmentation by random walks, in: Advances in\nneural information processing systems, 2001, pp. 873–879.\n[34] X. Chen, D. Cai, Large scale spectral clustering with landmark-based rep-\nresentation., in: AAAI, Vol. 5, 2011, p. 14.\n[35] G. E. Hinton, R. S. Zemel, Autoencoders, minimum description length\nand helmholtz free energy, in: Advances in neural information processing\nsystems, 1994, pp. 3–10.\n[36] A. Strehl, J. Ghosh, Cluster ensembles — a knowledge reuse framework for\ncombining multiple partitions, J. Mach. Learn. Res. 3 (2003) 583–617.\n[37] S. Vega-Pons, J. Ruiz-Shulcloper, A survey of clustering ensemble algo-\nrithms, International Journal of Pattern Recognition and Artiﬁcial Intelli-\ngence 25 (03) (2011) 337–372.\n[38] M. Maila, J. Shi, A random walks view of spectral segmentation, in: AI\nand STATISTICS (AISTATS), 2001.\n[39] X. Glorot, Y. Bengio, Understanding the diﬃculty of training deep feed-\nforward neural networks, in: Proceedings of the thirteenth international\nconference on artiﬁcial intelligence and statistics, 2010, pp. 249–256.\n32\n[40] S. J. Reddi, S. Kale, S. Kumar, On the convergence of adam and beyond,\nin: International Conference on Learning Representations, 2018.\n[41] Y. LeCun, L. Bottou, Y. Bengio, P. Haﬀner, Gradient-based learning ap-\nplied to document recognition, Proceedings of the IEEE 86 (11) (1998)\n2278–2324.\n[42] F. Alimoglu, E. Alpaydin, Methods of combining multiple classiﬁers based\non diﬀerent representations for pen-based handwritten digit recognition,\nin: Proceedings of the Fifth Turkish Artiﬁcial Intelligence and Artiﬁcial\nNeural Networks Symposium, Citeseer, 1996.\n[43] V. Vapnik, Statistical learning theory. 1998, Wiley, New York, 1998.\n[44] D. Arthur, S. Vassilvitskii, k-means++: The advantages of careful seed-\ning, in: Proceedings of the eighteenth annual ACM-SIAM symposium on\nDiscrete algorithms, Society for Industrial and Applied Mathematics, 2007,\npp. 1027–1035.\n[45] D. Steinley, Properties of the hubert-arable adjusted rand index., Psycho-\nlogical methods 9 (3) (2004) 386.\n[46] M. M. Fard, T. Thonet, E. Gaussier, Deep k-means: Jointly clustering with\nk-means and learning representations, arXiv preprint arXiv:1806.10069.\n[47] L. v. d. Maaten, G. Hinton, Visualizing data using t-sne, Journal of machine\nlearning research 9 (Nov) (2008) 2579–2605.\n33\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-01-08",
  "updated": "2019-06-12"
}