{
  "id": "http://arxiv.org/abs/1901.02452v1",
  "title": "Face Recognition System",
  "authors": [
    "Yang Li",
    "Sangwhan Cha"
  ],
  "abstract": "Deep learning is one of the new and important branches in machine learning.\nDeep learning refers to a set of algorithms that solve various problems such as\nimages and texts by using various machine learning algorithms in multi-layer\nneural networks. Deep learning can be classified as a neural network from the\ngeneral category, but there are many changes in the concrete realization. At\nthe core of deep learning is feature learning, which is designed to obtain\nhierarchical information through hierarchical networks, so as to solve the\nimportant problems that previously required artificial design features. Deep\nLearning is a framework that contains several important algorithms. For\ndifferent applications (images, voice, text), you need to use different network\nmodels to achieve better results. With the development of deep learning and the\nintroduction of deep convolutional neural networks, the accuracy and speed of\nface recognition have made great strides. However, as we said above, the\nresults from different networks and models are very different. In this paper,\nfacial features are extracted by merging and comparing multiple models, and\nthen a deep neural network is constructed to train and construct the combined\nfeatures. In this way, the advantages of multiple models can be combined to\nmention the recognition accuracy. After getting a model with high accuracy, we\nbuild a product model. This article compares the pure-client model with the\nserver-client model, analyzes the pros and cons of the two models, and analyzes\nthe various commercial products that are required for the server-client model.",
  "text": " \n \n \n \n FACE RECOGNITION SYSTEM \n \nBY: \nYang Li \nYLi@my.harrisburgu.edu \n \nSangwhan Cha, PhD. \nAssistant Professor of Computer Science \nscha@harrisburgu.edu \n \n \n \n \n \n1 \n \nTable of Contents   \n \nAbstract .......................................................................................................................................... 3 \nKeywords: ................................................................................................................................... 4 \nChapter 1 Face Recognition System ............................................................................................ 5 \n1.1 \nINTRODUCTION .......................................................................................................... 5 \n1.2 \nAPPLICATION OF THIS RESEARCH ..................................................................... 6 \n1.3 \nANALYSIS OF PROBLEM STATEMENT ................................................................ 7 \nChapter 2 Theoretical Background ............................................................................................. 9 \n2.1 \nANALYSIS OF RELATED WORK ............................................................................. 9 \n2.2 \nTHEORETICAL IDEA OF PROPOSED WORK .................................................... 12 \nChapter 3 Building Face Recognition Model with Neural Network ...................................... 14 \n3.1 \nINTRODUCTION TO NEURAL NETWORK ......................................................... 14 \n3.2 \nCONVOLUTIONAL NEURAL NETWORK ........................................................... 15 \n3.3 \nBUILD FACE RECOGNITION MODEL WITH CNN ........................................... 16 \nChapter 4 Building Robust Face Recognition System ............................................................. 28 \n4.1 INTRODUCTION............................................................................................................. 28 \n4.2 SYSTEM ARCHITECTURE .......................................................................................... 29 \nChapter 5 Conclusion and Future Work .................................................................................. 44 \n5.1 CONCLUSION ................................................................................................................. 44 \n \n2 \n \n5.2 FUTURE WORK .............................................................................................................. 45 \nREFERENCES ............................................................................................................................ 46 \n \n   \n  \n \n \n \n3 \n \n \nAbstract \nHow to accurately and effectively identify people has always been an interesting topic, both \nin research and in industry. With the rapid development of artificial intelligence in recent years, \nfacial recognition gains more and more attention. Compared with the traditional card recognition, \nfingerprint recognition and iris recognition, face recognition has many advantages, including but \nlimit to non-contact, high concurrency, and user friendly. It has high potential to be used in \ngovernment, public facilities, security, e-commerce, retailing, education and many other fields. \nDeep learning is one of the new and important branches in machine learning. Deep learning \nrefers to a set of algorithms that solve various problems such as images and texts by using various \nmachine learning algorithms in multi-layer neural networks. Deep learning can be classified as a \nneural network from the general category, but there are many changes in the concrete realization. \nAt the core of deep learning is feature learning, which is designed to obtain hierarchical \ninformation through hierarchical networks, so as to solve the important problems that previously \nrequired artificial design features. Deep Learning is a framework that contains several important \nalgorithms. For different applications (images, voice, text), you need to use different network \nmodels to achieve better results. \nWith the development of deep learning and the introduction of deep convolutional neural \nnetworks, the accuracy and speed of face recognition have made great strides. However, as we \nsaid above, the results from different networks and models are very different. In this paper, facial \nfeatures are extracted by merging and comparing multiple models, and then a deep neural network \nis constructed to train and construct the combined features. In this way, the advantages of multiple \n \n4 \n \nmodels can be combined to mention the recognition accuracy. After getting a model with high \naccuracy, we build a product model. This article compares the pure-client model with the server-\nclient model, analyzes the pros and cons of the two models, and analyzes the various commercial \nproducts that are required for the server-client model. \n \nKeywords:  \nDeep neural network, face recognition, server-client model, business model, deep multi-model \nfusion, convolutional neural network。 \n \n \n \n \n \n \n5 \n \nChapter 1 \nFace Recognition System \n \n1.1 INTRODUCTION \n \nEver since IBM introduced first personal computer on 1981, to the .com era in the early \n2000s, to the online shopping trend in last 10 years, and the Internet of Things today, computers \nand information technologies are rapidly integrating into everyday human life. As the digital world \nand real world merge more and more together, how to accurately and effectively identify users and \nimprove information security has become an important research topic.  \n \nNot only in the civil area, in particular, since the 9-11 terrorist attacks, governments all \nover the world have made urgent demands on this issue, prompting the development of emerging \nidentification methods. Traditional identity recognition technology mainly rely on the individual’s \nown memory (password, username, etc.) or foreign objects (ID card, key, etc.). However, whether \nby virtue of foreign objects or their own memory, there are serious security risks. It is not only \ndifficult to regain the original identity material, but also the identity information is easily acquired \nby others if the identification items that prove their identity are stolen or forgotten. As a result, if \nthe identity is impersonated by others, then there will be serious consequences. \n \nDifferent from the traditional identity recognition technology, biometrics is the use of the \ninherent characteristics of the body for identification, such as fingerprints, irises, face and so on. \n \n6 \n \nCompared with the traditional identity recognition technology, biological features have many \nadvantages, as: 1. Reproducibility, biological characteristics are born with, cannot be changed, so \nit is impossible to copy other people's biological characteristics. 2. Availability, biological features \nas part of the human body, readily available, and will never be forgotten. 3. Easy to use. Many \nbiological characteristics will not require individuals to corporate with the examine device. Based \non the above advantages, biometrics has attracted the attention of major corporations and research \ninstitutes and has successfully replaced traditional recognition technologies in many fields. And \nwith the rapid development of computer and artificial intelligence, biometrics technology is easy \nto cooperate with computers and networks to realize automation management, and is rapidly \nintegrating into people's daily life. \n \nWhen comparing the differences between different biometrics, we can see that the cost of \nfacial recognition is low, the acceptance from user is easy, and the acquisition of information is \neasy. Facial recognition is the use of computer vision technology and related algorithms, from the \npictures or videos to find faces, and then analysis of the identity. In addition, further analysis of \nthe acquired face, may conduct some additional attributes of the individual, such as gender, age, \nemotion, and etc. \n \n1.2  APPLICATION OF THIS RESEARCH \n \nFace recognition can be traced back to the sixties and seventies of the last century, and \nafter decades of twists and turns of development has matured. The traditional face detection \nmethod relies mainly on the structural features of the face and the color characteristics of the face. \n \n7 \n \nSome traditional face recognition algorithms identify facial features by extracting landmarks, or \nfeatures, from an image of the subject's face. For example, as shown in Figure 1.1, an algorithm \nmay analyze the relative position, size, and/or shape of the eyes, nose, cheekbones, and jaw. These \nfeatures are then used to search for other images with matching features. These kinds of algorithms \ncan be complicated, require lots of compute power, hence could be slow in performance. And they \ncan also be inaccurate when the faces show clear emotional expressions, since the size and position \nof the landmarks can be altered significantly in such circumstance. \n \nFigure 1.1 Abstract humane face into features [1] \n \n1.3 ANALYSIS OF PROBLEM STATEMENT \n \nA complete face recognition system includes face detection, face preprocessing and face \nrecognition processes. Therefore, it is necessary to extract the face region from the face detection \nprocess and separate the face from the background pattern, which provides the basis for the \nsubsequent extraction of the face difference features. The recent rise of the face based on the depth \n \n8 \n \nof learning detection methods, compared to the traditional method not only shorten the time, and \nthe accuracy is effectively improved. Face recognition of the separated faces is a process of feature \nextraction and contrast identification of the normalized face images in order to obtain the identity \nof human faces in the images. \n \nIn this paper, we will first summarize and analyze the present research results of face \nrecognition technology, and studies a face recognition algorithm based on feature fusion. The \nalgorithm flow consists of face image preprocessing, combination feature construction and \ncombination feature training. \n \n \n \n9 \n \nChapter 2 \nTheoretical Background \n \n2.1 ANALYSIS OF RELATED WORK \n \nIn Chapter 1, we introduced the facial recognition, discussed the use case and bright future \nof this technology. A tremendous amount of research and effort from many major company and \nuniversities and been dedicated to this field. In the first part of this chapter, we will review the \nmost significant work in the facial recognition field. \n \n2.1.1 FACE DETECTION AND FACE TRACKING \n \nThis article Robust Real-time Object Detection [2] is the most frequently cited article in a \nseries of articles by Viola that makes face detection truly workable. We can learn about several \nface detection methods and algorithm from this publication. The article Fast rotation invariant \nmulti-view face detection based on real Adaboost [3] for the first time real adaboost applied to \nobject detection, and proposed a more mature and practical multi-face detection framework, the \nnest structure mentioned on the cascade structure improvements also have good results. The article \nTracking in Low Frame Rate Video: A Cascade Particle Filter with Discriminative Observers of \nDifferent Life Spans [4] is a good combination of face detection model and tracking, offline model \nand online model, and obtained the CVPR 2007 Best Student Paper. \n \n \n10 \n \nThe above 3 papers discussed about the face detection and face tracking problems. \nAccording to the research result in these papers, we can make real time face detection systems. \nThe main purpose is to find the position and size of each face in the image or video, but for \ntracking, it is also necessary to determine the correspondence between different faces in the frame. \n \n2.1.2 FACE POSITIONING AND ALIGNMENT \n \nEarlier localization of facial feature points focused on two or three key points, such as \nlocating the center of the eyeball and the center of the mouth, but later introduced more points and \nadded mutual restraint to improve the accuracy and stability of positioning Sex. The article Active \nShape Models-Their Training and Application [5] is a model of dozens of facial feature points and \ntexture and positional relationship constraints considered together for calculation. Although ASM \nhas more articles to improve, it is worth mentioning that the AMM model, but also another \nimportant idea is to improve the original article based on the edge of the texture model. The \nregression-based approach presented in the paper Boosted Regression Active Shape Models [6] is \nbetter than the one based on the categorical apparent model. The article Face Alignment by Explicit \nShape Regression [7] is another aspect of ASM improvement and an improvement on the shape \nmodel itself. Is based on the linear combination of training samples to constrain the shape, the \neffect of alignment is currently seen the best. \n \nThe purpose of the facial feature point positioning is to further determine facial feature \npoints (eyes, mouth center points, eyes, mouth contour points, organ contour points, etc.) on the \nbasis of the face area detected by the face detection / tracking, s position. These 3 articles show \n \n11 \n \nthe methods for face positioning and face alignment. The basic idea of locating the face feature \npoints is to combine the texture features of the face locals and the position constraints of the organ \nfeature points. \n \n2.1.3 FACE FEATURE EXTRACTION \n \nPCA-based eigenfaces [8] are one of the most classic algorithms for face recognition. \nAlthough today's PCA is more used in dimensionality reduction in real systems than classification, \nsuch a classic approach deserves our attention. The article Local Gabor Binary Pattern Histogram \nSequence (LGBPHS): A Novel Non-Statistical Model for Face Representation and Recognition [9] \nis close to many mature commercial systems. In many practical systems, a framework for \nextracting authentication information is PCA and LDA. Using PDA to reduce matrix to avoid the \nmatrix singularity problem of LDA solving, then using LDA to extract the features suitable for \nclassification, To further identify the various original features extracted after the decision-level \nfusion. Although some of the LFW test protocols are not reasonable, there is indeed a face \nrecognition library that is closest to the actual data. In this article, Blessing Dimensionality: High-\ndimensional Feature and Its Efficient Compression for Face Verification [10], the use of precise \npositioning point as a reference to face multi-scale, multi-regional representation of the idea is \nworth learning, can be combined with a variety of representation. \n \nThe above 3 papers discussed about facial feature positioning/alignment. Facial feature \nextraction is a face image into a string of fixed-length numerical process. This string of numbers \nis called the \"Face Feature\" and has the ability to characterize this face. Human face to mention \n \n12 \n \nthe characteristics of the process of input is \"a face map\" and \"facial features key points \ncoordinates\", the output is the corresponding face of a numerical string (feature). Face to face \nfeature algorithm will be based on facial features of the key point coordinates of the human face \npre-determined mode, and then calculate the features. In recent years, the deep learning method \nbasically ruled the face lift feature algorithm, In the articles mentioned above, they showed the \nprogress of research in this area. These algorithms are fixed time length algorithm. Earlier face \nfeature models are larger, slow, only used in the background service. However, some recent studies \ncan optimize the model size and operation speed to be available to the mobile terminal under the \npremise of the basic guarantee algorithm effect. \n \n2.2 THEORETICAL IDEA OF PROPOSED WORK \n \n       \nFace recognition is essentially pattern recognition, and the purpose is to abstract real things \ninto numbers that computers can understand. If a picture is a 256 bit-color image, then each pixel \nof the image is a value between 0 and 255, so we can convert an image into a matrix. How to \nidentify the patterns in this matrix? One way is to use a relatively small matrix to sweep from left \nto right and top to bottom in this large matrix. Within each small matrix block, we can count the \nnumber of occurrences of each color from 0 to 255. So we can express the characteristics of this \nblock.  \n \n      \nThrough this scan, we get another matrix consisting of many small matrix block features. \nAnd this matrix is smaller than the original matrix. Then, for this smaller matrix, perform the above \nsteps again to perform a feature \"concentration\". In another sense, it is abstracted. Finally, after \n \n13 \n \nmany abstractions, we will turn the original matrix into a 1 dimension by 1 dimension matrix, \nwhich is a number. Different pictures, such as a cat, or a dog, a bear, will eventually get abstracted \nto different numbers. Similarly, faces, expressions, ages, these principles are similar, but the initial \nsample size will be large, and ultimately the specific image is abstracted into numbers through the \nmatrix. Then by calculating the difference between the matrixes, we can achieve the goal of \ncomparing faces. \n \n \n \n14 \n \nChapter 3 \nBuilding Face Recognition Model with Neural Network \n \n3.1 INTRODUCTION TO NEURAL NETWORK \n \nArtificial Neural Network (ANN) is a research hotspot in the field of artificial intelligence \nsince the 1980s. It abstracts the human brain neuron network from the perspective of information \nprocessing, establishes a simple model, and forms different networks according to different \nconnection methods. It is also often referred to as neural network or neural network in engineering \nand academia. A neural network is an operational model consisting of a large number of nodes (or \nneurons) connected to each other. Each node represents a specific output function called an \nactivation function. The connection between every two nodes represents a weighting value for \npassing the connection signal, called weight, which is equivalent to the memory of the artificial \nneural network. The output of the network varies depending on the connection method of the \nnetwork, the weight value and the excitation function. The network itself is usually an \napproximation of an algorithm or function in nature, or it may be an expression of a logic strategy. \n \nIn the past ten years, the research work of artificial neural networks has been deepened, \nand great progress has been made. It has successfully solved many problems in the fields of pattern \nrecognition, intelligent robots, automatic control, predictive estimation, biology, medicine, and \neconomy. Practical problems that are difficult to solve in modern computers, showing good \nintelligence. \n \n15 \n \nThe artificial neural network model mainly considers the topology of the network \nconnection, the characteristics of the neurons, and the learning rules. At present, there are nearly \n40 kinds of neural network models, including back propagation network, perceptron, self-\norganizing map, Hopfield network, Boltzmann machine, adaptive resonance theory and so on. \nAccording to the topology of the connection, the neural network model can be divided into: \nFeedforward network and Feedback network. \n \nFeedforward network: Each neuron in the network accepts the input of the previous stage \nand outputs it to the next stage. There is no feedback in the network, and it can be represented by \na directed loop-free graph. This kind of network realizes the transformation of signals from input \nspace to output space, and its information processing capability comes from multiple \nrecombination of simple nonlinear functions. The network structure is simple and easy to \nimplement. The backhaul network is a typical forward network. \n \nFeedback network: There is feedback between neurons in the network, which can be \nrepresented by an undirected complete graph. The information processing of this neural network \nis a state transition that can be handled by dynamic system theory. The stability of the system is \nclosely related to the associative memory function. Both the Hopfield network and the Boltzmann \nmachine belong to this type. \n \n3.2 CONVOLUTIONAL NEURAL NETWORK \n \n \n16 \n \nConvolutional neural network (CNN) is a deformation of multi-layer perceptron inspired \nby biological vision and the most simplified preprocessing operation. It is essentially a forward \nfeedback neural network. The biggest difference between convolutional neural network and multi-\nlayer perceptron is network. The first few layers are composed of a convolutional layer and a \npooled layer alternately cascaded to simulate a simple cascade of cells and complex cells for high-\nlevel feature extraction in the visual cortex. \n \n \nFigure 2. Typical convolutional neural network (CNN) structure [11] \n \nThe convolutional neurons respond to a portion of the input from the previous layer (called \nthe local receptive field, with overlap between the regions), extracting higher-level features of the \ninput; the neurons of the pooled layer are input to the previous layer. A portion of the area (no \noverlap between the areas) is averaged or maximized to resist slight deformation or displacement \nof the input. The latter layers of the convolutional neural network are typically an output layer of \na number of fully connected layers and a classifier. \n \n3.3 BUILD FACE RECOGNITION MODEL WITH CNN \n \n17 \n \nAt present, face recognition algorithms can be roughly divided into two categories: \n(1) Representation-based methods. The basic idea is to convert two-dimensional face input \ninto another space, and then use statistical methods to analyze face patterns, such as Eigenface, \nFisherface, and SVM. \n(2) A feature-based method generally extracts local or global features and then sends a \nclassifier for face recognition, such as recognition based on set features and HMM. \nConvolutional neural network for face recognition can be considered as a feature-based \nmethod. It is different from traditional artificial feature extraction and high-performance classifier \ndesign for features. Its advantage is that feature extraction is performed by layer-by-layer \nconvolution dimension reduction, and then through multi-layer nonlinear mapping, the network \ncan automatically learn from the unprocessed training samples to form a feature extractor and \nclassifier that adapts to the recognition task. This method reduces the requirements on the training \nsamples, and the number of layers of the network. The more it learns, the more global the features \nare. \n \n \n3.3.1 THEORY \nConvolutional neural network is a deformation of multi-layer perceptron inspired by \nbiological vision and the most simplified preprocessing operation. It is essentially a forward \nfeedback neural network. The biggest difference between convolutional neural network and multi-\nlayer perceptron is network. The first few layers are composed of a convolutional layer and a \npooled layer alternately cascaded to simulate a simple cascade of cells and complex cells for high-\nlevel feature extraction in the visual cortex. \n \n18 \n \n \nThe convolutional neurons respond to a portion of the input from the previous layer (called \nthe local receptive field, with overlap between the regions), extracting higher-level features of the \ninput; the neurons of the pooled layer are input to the previous layer. A portion of the area (no \noverlap between the areas) is averaged or maximized to resist slight deformation or displacement \nof the input. The latter layers of the convolutional neural network are typically an output layer of \na number of fully connected layers and a classifier. \n \n \nFigure 3. Classical LeNet-5 CNN for face recognition [12] \n \nIn Figure 3, it shows how a classical LeNet-5 CNN works for face recognition. The network \nwas proposed by LeCun et al [13], and it is composed by below layers: \n \nConvolution layer: The convolutional layer simulates the process of extracting some \nprimary visual features by using simple methods of local connection and weight sharing to \nsimulate simple cells with local receptive fields. Local connection means that each neuron on the \nconvolutional layer is connected with the neurons in the fixed area in the previous feature map; \n \n19 \n \nweight sharing means that the neurons in the same feature map use the same connection strength \nand the previous layer. Connection, can reduce the network training parameters, the same set of \nconnection strength is a feature extractor, which is realized as a convolution kernel in the process \nof calculation, and the convolution kernel value is randomly initialized first, and finally determined \nby network training. \n \nThe pooling/sampling layer: The pooled layer simulates complex cells as a process of \nscreening and combining primary visual features into more advanced, abstract visual features. It \nis implemented by sampling in the network. After sampling by the pooling layer, the number of \noutput feature maps is unchanged, but the size of the feature map becomes smaller, which has the \neffect of reducing the computational complexity and resisting small displacement changes. \nThe pooling layer proposed in this paper adopts large-value sampling, and the sampling size is \n2*2, that is, the input feature map is divided into non-overlapping 2*2 rectangles, and the \nmaximum value is taken for each rectangle, so the output feature map is output. Both the length \nand the width are half of the input feature map. The neurons in the pooled layer defined in this \npaper do not have the learning function. \n \nThe fully connected layer: In order to enhance the nonlinearity of the network and limit the \nsize of the network, the network extracts features from the four feature extraction layers and \naccesses a fully connected layer. Each neuron of the layer is interconnected with all neurons of the \nprevious layer. The same layer of neurons are not connected. \n \n3.3.2 BUILD SIAMESE NETWORK WITH CNN \n \n20 \n \n \nAfter comparing different neural networks and their characteristics, we used Siamese \nnetwork to resolve the problem. The Siamese network is neural network for measuring of \nsimilarity. It can be used for category identification, classification, etc., in the scenario when there \nare many categories, but the number of samples per category is small. The traditional classification \nmethod for distinguishing is to know exactly which class each sample belongs to and need to have \nan exact label for each sample. And the relative number of tags is not too much. These methods \nare less applicable when the number of categories is too large and the number of samples per \ncategory is relatively small. In fact, it is also very well understood. For the entire data set, our data \nvolume is available, but for each category, there can be only a few samples, then using the \nclassification algorithm to do it, because each category of samples is too Less, we can't train any \ngood results at all, so we can only find a new way to train this data set, thus proposing the Siamese \nnetwork, as showing in Figure 4. \n \nFigure 4. Siamese Network Work Flow [14] \n \n21 \n \n \nThe Siamese network learns a similarity measure from the data and uses the learned metric \nto compare and match the samples of the new unknown category. This method can be applied to \nclassification problems where the number of classes is large, or the entire training sample cannot \nbe used for previous method training. \n \nThe machine we used for this article is on Ubuntu 18 operating system. The CPU is Intel(R) \nCore(TM) i5-7300HQ CPU @ 2.50GHz with 4 cores. The memory is dual channel DDR 4 8GB \nSDRAM. We train our neural network using NVIDIA GPU GeForce GTX 1050 with 4GB GPU \nRAM. The GPU driver version is 390.48. We use Compute Unified Device Architecture (CUDA) \nversion 9.0, and NVIDIA CUDA Deep Neural Network (cuDNN) version 7.0 for CUDA 9.0. We \nbuild the Siamese network with below parameters: \nSiameseNetwork( \n  (cnn1): Sequential( \n    (0): ReflectionPad2d((1, 1, 1, 1)) \n    (1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1)) \n    (2): ReLU(inplace) \n    (3): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_\nrunning_stats=True) \n    (4): ReflectionPad2d((1, 1, 1, 1)) \n    (5): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1)) \n    (6): ReLU(inplace) \n    (7): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_\nrunning_stats=True) \n    (8): ReflectionPad2d((1, 1, 1, 1)) \n    (9): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1)) \n    (10): ReLU(inplace) \n    (11): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track\n_running_stats=True) \n  ) \n  (fc1): Sequential( \n    (0): Linear(in_features=80000, out_features=500, bias=True) \n    (1): ReLU(inplace) \n    (2): Linear(in_features=500, out_features=500, bias=True) \n    (3): ReLU(inplace) \n    (4): Linear(in_features=500, out_features=5, bias=True) \n  ) \n \n22 \n \n) \n \n \n3.3.3 TRAIN THE NEURAL NETWORK \n \nThe face database we choose is ORL [15]. The ORL face database consists of 400 pictures \nof 40 people, that is, 10 pictures per person. The face has expressions, tiny gestures and so on. The \ntraining processing is performed on the two databases, and 90% of the faces in the library are \nrandomly selected as the training set, and the remaining 10% of the faces are used as test sets, and \nthen the faces in the two sets are standardized. The training process was using GPU, as shown in \nFigure 5 and Figure 6. We can see during training, the CPU usage went to 100%, and the working \ntemperature increased dramatically. \n \n \nFigure 5. GPU Usage before training. \n \n23 \n \n \nFigure 6. GPU usage during training. \n \nWe trained the model with 100 epochs. A sample training loss is showed below, we can \nsee the training loss goes down significantly during the early epochs, and converged to 0.0067 at \nlast. Figure 8 shows clearly how the trend of training loss goes down as the epoch increases. \nEpoch number 0 \n Current loss 2.055403470993042 \n \nEpoch number 1 \n Current loss 0.9863696694374084 \n \nEpoch number 2 \n Current loss 0.8137061595916748 \n \n… \n… \n… \n \nEpoch number 6 \n Current loss 0.5110232830047607 \n \nEpoch number 7 \n Current loss 0.36760351061820984 \n \n24 \n \n \nEpoch number 8 \n Current loss 0.346961110830307 \n \nEpoch number 9 \n Current loss 0.23092612624168396 \n… \n… \n… \n \nEpoch number 33 \n Current loss 0.019308971241116524 \n \nEpoch number 34 \n Current loss 0.02861899696290493 \n \nEpoch number 35 \n Current loss 0.047534599900245667 \n \n… \n… \n… \n \nEpoch number 70 \n Current loss 0.027325695380568504 \n \nEpoch number 71 \n Current loss 0.019941458478569984 \n \n… \n… \n… \n \nEpoch number 97 \n Current loss 0.00932073313742876 \n \nEpoch number 98 \n Current loss 0.00708159850910306 \n \nEpoch number 99 \n Current loss 0.006693363655358553 \n \n25 \n \n \nFigure 8. Training loss vs. Epoch \n \n3.3.4 MODEL VERIFICATION \n \nThe input of the neural network is an image of human face, and the output of the neural \nnetwork is a vector of 5 dimensions. A sample output looks like below: \nVector of Face 1: Variable containing: \n 1.7350  0.2165  1.0214  1.5764  2.2253 \n[torch.cuda.FloatTensor of size 1x5 (GPU 0)] \n \nTo calculate if the faces on two images come from the same person, we need to calculate \nthe similarity of the two images, aka, the Euclidean distance between two vectors. Below is an \nexample output of different people identified by our model, and the images are shown in Figure 9: \nVector of Face 1: Variable containing: \n 1.7350  0.2165  1.0214  1.5764  2.2253 \n[torch.cuda.FloatTensor of size 1x5 (GPU 0)] \nVector of Face 2: Variable containing: \n-0.7570  1.5081  0.3380  1.5524 -0.0977 \n \n26 \n \n[torch.cuda.FloatTensor of size 1x5 (GPU 0)] \nDistance between Face1 Vector and Face2 Vector: Variable containing: \n 3.7070 \n[torch.cuda.FloatTensor of size 1x1 (GPU 0)] \n \nFigure 9. Different people with high Euclidean distance \nBelow is an example output of same person with different pose, identified by our model, \nand the images are shown in Figure 10: \nVector of Face 1: Variable containing: \n 1.7350  0.2165  1.0214  1.5764  2.2253 \n[torch.cuda.FloatTensor of size 1x5 (GPU 0)] \nVector of Face 2: Variable containing: \n 1.6301  0.7585  1.1658  1.6345  2.2486 \n[torch.cuda.FloatTensor of size 1x5 (GPU 0)] \nDistance between Face1 Vector and Face2 Vector: Variable containing: \n 0.5741 \n[torch.cuda.FloatTensor of size 1x1 (GPU 0)] \n \n27 \n \n \nFigure 10. Same person, even with different pose, has small Euclidean distance \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n28 \n \nChapter 4 \nBuilding Robust Face Recognition System \n \n4.1 INTRODUCTION \n \nHigh accuracy face recognition models have been reported in scientific researches by giant \ntechnology companies and research institutions, as shown in Figure 11. But all these ground-\nbreaking result still stays in the laboratory. Applications of face recognition in the real world is \nhard to see. For example, you cannot see it be used in DMV to void fake id, you cannot see it in a \ndaycare to make sure the right person pick up the right kid, you cannot see it in a fitness club to \nmake customer check-in more pleasant. \n \n \nFigure 11. Face recognition models and their accuracy [16]. \n \n \n29 \n \nThe key gap between face recognition research and industrial usage is the application. You \ncan see different algorithm companies provided different APIs, but how to turn the API into the \nreal product is a tough problem laying in front of potential industrial users.  \n \n4.2 SYSTEM ARCHITECTURE \n \nMany aspects need to be take into consideration when building towards a commercial \nusable system. We are targeting at a system architecture that is high performance, scalable, agile, \nand low cost. The high performance means the system will give out result at milliseconds level, \nand have high threshold with high concurrency. Scalable means the system can scale well as the \nneeds increases, from a single node machine to a multi node cluster. Agile means the system \nshould be easily modifiable, and be able to apply to different domain with ease. Low cost not \nonly means the deployment cost is low, also the development cast and maintenance cost. To \nachieve all the goals, we need to resolve many practical problems. \n \n4.2.1 CHOOSE BETWEEN CPU AND GPU \n \nAlthough GPU was not designed for neural network initially, but it has the features which \nput it into a better position than CPU for the neural network calculations: 1. Provides the \ninfrastructure of multi-core parallel computing, and has a large number of cores, which can \nsupport parallel computing of large amounts of data. Parallel computing or parallel computing is \n \n30 \n \nrelative to serial computing. It is an algorithm that can execute multiple instructions at a time, \nwith the goal of increasing the speed of calculations and solving large and complex \ncomputational problems by expanding the problem solving scale. 2. Has a higher speed of \nmemory access. 3. Has higher floating point computing power. Floating-point computing power \nis an important indicator of multimedia and 3D graphics processing related to processors. In \ntoday's computer technology, due to the application of a large number of multimedia \ntechnologies, the calculation of floating point numbers has been greatly increased, such as the \nrendering of 3D graphics, so the ability of floating point computing is an important indicator to \nexamine the computing power of the processor. \nA test done by V Chu [17] shows that, depending on the GPU you choose, the \nperformance gain of a GPU over CPU on neural network calculation can be between 43 times to \n167 times, as shown in Figure 12. So, to achieve the high performance, our system will need to \nuse GPU for the neural network related processes. \n \nFigure 12, Tensorflow Performance comparison between GPUs and CPU [17] \n \n4.2.2 CHOOSE BETWEEN ALL-IN-ONE AND CLIENT-SERVER ARCHITECTURE \n \n31 \n \nThe demos from many face recognition research institutions are using all-in-one \narchitecture. In these examples [18], all processing unit are inside same system. This is very \ngood for prototyping, but has issues on industrial applications. The first problem to be considered \nis the size, cost and power consumption of a graphic card. Figure 13 shows a RTX 2080 Ti \ngraphic card held by NVIDIA CEO Jensen Huang. The size is so big that it cannot be put into \nsmall size appliances. The power consumption is 280W, with huge amount of heats generated \nwhen running at full potential. The price is no less than $999 for 1 pieces, which limit bulk \ndeployment of it.  \n \nFigure 13. A RTX 2080 Ti graphic card held by NVIDIA CEO Jensen Huang [19] \nDespite all the limitations of the graphic card, its computational power is superior. In our \nstress test with a GTX 1080 Ti graphic card, it can extract facial features from 200 pictures at same \n \n32 \n \ntime, with an average of 25 milliseconds processing time per picture. All the limitations and \nfeatures are implicating that the server-client architecture will perform well. \n \n4.2.3 THE CLIENT-SERVER ARCHITECTURE WITH GPU \n \nFigure 14 shows the client-server architecture with GPU we designed to fulfill the \nrequirement. The whole system has 2 sub-systems: user registration system and real time \nrecognition system, and these 2 sub-systems share compute-heavy components to reduce the \ncomplexity and cost. The client side of user registration system will take user picture and user \ninput, i.e. employee id. Then the picture and user input is sent to the server, where GPU resides. \nThe GPU will process the picture, first detect human face and then extract the picture into vector. \nThe vector is then stored into database with the user input. \n \n \n33 \n \n \nFigure 14. Client-server architecture with GPU \n \n \nIn the real time recognition system, the client camera will be used to capture real time \nvideos. In our first design, the client will send video feed to the server no matter what was captured. \nDuring the stress test, we found this method will put too much load into the server. So we added a \nmotion detection component to the client. This component can be done with opencv libraries, and \ncan be deployed on both desk-top level clients, and mobile clients. The motion detection will \nsignificantly reduce the amount of image which need to be processed for face detection. \n \nAfter a motion is detected, the video feed will be processed and only send n frames of \npictures to the server. The server’s face detection components will try to detect the face from the \nimage, it will be able to detect multiple faces from single image. If there is no face detected, the \n \n34 \n \nimage will be discarded. If one or multiple faces were detected, each face will be cropped from the \noriginal image, aligned, resized, and marked with a unique ID and send to the vector extraction \nprocesser. The vector extraction processer will read the pre-processed human face image and \nextract each face into a vector. The vector will be combined with the unique request ID, and start \na query into the face feature database. The Euclidean distance will be calculated against the current \nface vector and the face vectors retrieved from the database. This step can be done by CPU. After \nall the distances have been calculated, they will be sorted and generated the list n distances. A \nmessage similar to below format will be generated:  \n{ \nrequest_id:<x>,  \n{ \nuser_id[1]:<x> \ndistance:<x> \n}, \n{ \nuser_id[2]:<x> \ndistance:<x> \n}, \n{ \nuser_id[3]:<x> \ndistance:<x> \n} \n} \n \nThis message will be sent back to the client, and on the client, the customer can have their \nown rules on what will happen, i.e. allow employee badge-in, tell if an I.D. card is fake and etc. In \nthe meanwhile, the message can be sent to a different database, which logs the activities, and it \ncan hook up with customers’ ERP system. The images can also be saved for further training, to \nimprove the model performance on this particular group of people. \n \n \n35 \n \n4.2.4 DATABASE DESIGN \n \nTo preserve data, this system will use its own database. We will use MySql relational \ndatabase management system as an example in this article. An obvious way to for storing the \nhuman face picture into the database is to use blob (binary large object). But if the image are stored \nin the database, every time to do the face compare, the GPU need to re-extract the vector from the \nimages. For large dataset, the wait will be long. To avoid this issues, we will first extract the vectors \nfrom human face image, and only save the vectors into the database. And we also need to associate \nthe vector with an identification property. User_id/employee_id/customer_id will be good choice \nfor this identification property. So, depending on how many vectors will the model extract, the \nmain table structure in the database will look like below: \nID \nVector1\nVector2\nVector3\n… \n… \n… \nVector<n-1> \nVector<n> \n \nAccording to database normalization rules, the database belongs to this system shouldn’t \nstore anything other than the ID and face vector. Since many user information can change rapidly \ninside on organization, bring any other column into this database will be redundant and require \nadditional work to maintain data integrity. The database can retrieve/send information to other \nenterprise database like ERP, Clarify and etc. with just ID column, like showing in Figure 15. \n \n \n36 \n \n \nFigure 15. Database linking to each other. \n \nAccording to our experiment, one tweak in the database design is to not make the ID \ncolumn a unique key or primary key column. By doing this, the performance can be significantly \nimproved. For example, after the system is online for 10 days, you gathered one face with ID=9 \n10 times, with slightly different vectors every time. You can save all these 10 vectors into this \nmain table. One the 11th day, when person with ID=10 go through the camera, the picture is \nprocessed, and the top 3 candidates from the processing engine will show ID=10 more than 1 \ntimes. This result will give client more confidence to justify the person is the one with ID=10. \n \n4.2.5 NODE FRAMEWORK AS SERVER \nNode.js is a JavaScript runtime environment, released in May 2009 by Ryan Dahl, which \nessentially encapsulates the Chrome V8 engine. Node.js is neither a JavaScript framework, nor a \nbrowser-side library. Node.js is a development platform that lets JavaScript run on the server side, \n \n37 \n \nmaking JavaScript a scripting language that is on par with server-side languages like PHP, Python, \nPerl, and Ruby.  \n \nThe V8 engine itself uses some of the latest compilation techniques. This allows code \nwritten in a scripting language such as JavaScript to run at a much faster speed and saves \ndevelopment costs. The demand for performance is a key factor in Node. JavaScript is an event-\ndriven language, and Node takes advantage of this to write highly scalable servers. Node uses an \narchitecture called an \"event loop\" that makes writing a highly scalable server easy and secure. \nThere are many different techniques for improving server performance. Node chose an architecture \nthat improves performance while reducing development complexity. This is a very important \nfeature. Concurrent programming is often complex and full of mines. Node bypasses these, but \nstill provides good performance. \n \nNode uses a series of \"non-blocking\" libraries to support the way the event loops. \nEssentially, it provides an interface for resources such as file systems and databases. When a \nrequest is sent to the file system, there is no need to wait for the hard disk (addressing and retrieving \nthe file), and the non-blocking interface notifies Node when the hard disk is ready. The model \nsimplifies access to slow resources in an extensible way, intuitive and easy to understand. \nEspecially for users who are familiar with DOM events such as onmouseover and onclick, there is \na feeling of deja vu. \n \nAlthough running JavaScript on the server side is not unique to Node, it is a powerful \nfeature. I have to admit that the browser environment limits our freedom to choose a programming \n \n38 \n \nlanguage. The desire to share code between any server and an increasingly complex browser client \napplication can only be achieved through JavaScript. Although there are other platforms that \nsupport JavaScript running on the server side, because of the above characteristics, Node has \ndeveloped rapidly and become top choice for many developers.  \n \n \nFigure 16. Callback in Node.js [20] \n \nFigure 16 is a diagram of how callback works in Node. Although in the face recognition \nsystem, the I/O is not the bottle neck, but Node still is our best choice. Because for GPU to detect \nface and extract vector from face image, and average of 25 milliseconds will be consumed. With \nthe non-block feature of Node, the whole process will not just wait there for 25 seconds each time \na request comes. The Node sever will send the request to GPU, then keeps handling new requests. \nAfter GPU finished its work, it will run the call back function, and the server will pick up what’s \nleft on the request and move one. This increased the concurrency of the system significantly. \n \nFigure 17 shows how the node server react with the neural network model in our system. \nWhen request comes in, the node sever will send the request to the neural network model. The \n \n39 \n \nneural network, which can be deployed in any format, will take the task and run it on GPU. While \nthis workload is running, node server will not wait for it to finish, but keeps accepting request from \nthe caller. After the neural network model retuned the calculation result, node server will package \nit with other necessary information, and send the feedback to the caller. \n \n \nFigure 17. Node Server work with Neural Network Model. \n \nBelow is our Node Server deployment with quest queueing: \nvar blockList = []; \nvar waitList = []; \nvar list1 = [];  //test list \n \napp.post('/postUid', function (req, res) { \n  var uid1 = req.body.uid1; \n  var uid2 = req.body.uid2; \n  var uid3 = req.body.uid3; \n \n  var value1 = req.body.value1; \n \n40 \n \n  var value2 = req.body.value2; \n  var value3 = req.body.value3; \n \n  function timeOutBlockList(listElement){ \n    setTimeout(function(){ \n        var listPos = blockList.indexOf(listElement); \n        console.log(new Date(),'BlockList: To be removed From block list:',li\nstElement); \n        // blockList[listElement] = 'False';    // to set value to this eleme\nnt. \n        blockList.splice(listPos,1);   // to remove this element. \n        //console.log(new Date(),'New block list',blockList); \n    }, blockTime); \n  } \n \n  function timeOutDisplayList(listPos,uid,waitTime){ \n    setTimeout(function(){ \n        console.log(new Date(),'DisplayList: To be emptyed: position: ', list\nPos); \n        if (displayList[listPos].uid === uid ) { \n          if (displayList[listPos].stat === 'Replaceable') { \n            displayList[listPos].stat = 'Empty'; \n          } \n          request.post({url:'http://localhost:3000/postUinfo', form: {slot:di\nsplayList[listPos].slot,uid:'', imgSrc:displayList[listPos].imgSrc,username:d\nisplayList[listPos].username, title:displayList[listPos].title}}, function (e\nrr,response, body) { \n            if (err) { \n              return console.error(err); \n            } else { \n              console.log(new Date(),\"Timeout DL, post to SSE sucesss\",listPo\ns,uid); \n            } \n          }); \n              // to set value to this element. \n          //displayList.splice(listPos,1);   // to remove this element. \n          //console.log(new Date(),'New display list',displayList); \n \n        } else { \n          console.log(new Date(),uid,\" no longer on the display list\") \n        } \n    }, waitTime ); \n  } \n \n41 \n \n \n  function addToDL(listPos,uid){ \n    displayList[listPos].stat = 'OnScreen'; \n    displayList[listPos].uid = uid; \n    // query the user info with UID \n    var sqlStmt = 'SELECT uid, username, title from client_users where uid = \n\\\"' + uid + '\\\";'; \n    //console.log(new Date(),sqlStmt); \n    connection.query(sqlStmt, function (err, rows) { \n      if (err) { \n        // throw err; \n        console.log(new Date(),err); \n      } \n      else { \n        if (rows.length > 0) { \n          console.log(new Date(),'DB query result: ', rows[0].uid, rows[0].us\nername, rows[0].title); \n          displayList[i] = { \n            uid:rows[0].uid, \n            slot : i+1, \n            imgSrc:'userimg/'+rows[0].uid + '.jpg', \n            username:rows[0].username, \n            title:rows[0].title \n          }; \n \n          request.post({url:'http://localhost:3000/postUinfo', form: {slot:di\nsplayList[i].slot,uid:displayList[i].uid, imgSrc:displayList[i].imgSrc,userna\nme:displayList[i].username, title:displayList[i].title}}, function (err,respo\nnse, body) { \n            if (err) { \n              return console.error(err); \n            } else { \n              console.log(new Date(),\"Add to DL: Post to SSE sucesss\",listPo\ns,uid); \n            } \n          }); \n          // below is the one without handle exception where sse is down. \n          //request.post('http://localhost:3000/postUinfo').form({slot:displa\nyList[i].slot,uid:displayList[i].uid, imgSrc:displayList[i].imgSrc,username:d\nisplayList[i].username, title:displayList[i].title}); \n \n        } else { \n          console.log(new Date(),'User not in database: ', uid1); \n \n42 \n \n        } \n      } \n    }); \n \n    oneSecFunc(listPos,uid); \n  } \n \n  function oneSecFunc(listPos,uid) { \n    setTimeout(function(){ \n      if (waitList.length > 0) { \n        var wl1 = waitList.shift(); \n        console.log(new Date(),listPos, \" is picking from waitlist for \",wl\n1); \n        // play out animation, about 500 \n        displayList[listPos].stat = 'pickedUpfromWL'; \n        timeOutDisplayList(listPos,uid,0); \n        //-- wrong --setTimeout(addToDL(listPos,wl1),500); \n        console.log(new Date(),wl1,' will be added to DL'); \n        setTimeout(function (){addToDL(listPos,wl1);},500); \n      } else { \n        displayList[listPos].stat = 'Replaceable'; \n        timeOutDisplayList(listPos,uid,displayTime - 1000); \n      } \n    } , 1000); \n  } \n \n \n  // blockList.push(uid1); //test \n  if (value1 > yz1) { \n    if (blockList.includes(uid1)) { \n      console.log(new Date(),\"Uid1 already in BlockList. Uid1 is: \", uid1); \n    } else { \n      console.log(new Date(),\"Uid1 not in BlockList. Uid1 is: \", uid1); \n \n      // add this user to block list \n      blockList.push(uid1); \n      timeOutBlockList(uid1); \n \n      // add this user to display list \n      var displayed = false; \n      for (var i=0; i<displayList.length; i++) { \n        if (displayList[i].stat === 'Empty') { \n          console.log(new Date(),\"Dispaly on Display list index: \",i); \n \n43 \n \n          addToDL(i,uid1); \n          displayed = true; \n          break; \n        } \n      } \n \n      // Add a waitDisplayList to handle displayed = false \n      console.log(new Date(),'Displayed?: ', displayed); \n      if ( displayed == false ) { \n        var pushed = false; \n        for (var i=0; i<displayList.length; i++) { \n          if (displayList[i].stat === 'Replaceable') { \n            console.log(new Date(),\"Push to Display list index: \",i); \n            timeOutDisplayList(i,displayList[i].uid,0); \n            displayList[i].stat = \"waitForPush\" \n            setTimeout(function (){addToDL(i,uid1);},500); \n            pushed = true; \n            break; \n          } \n        } \n        console.log(new Date(),'Pushed?: ', pushed); \n        if ( pushed == false ) { \n          waitList.push(uid1); \n          console.log(new Date(),'Current wait list',waitList); \n        } \n      } \n      //console.log(new Date(),displayList) \n    } \n  } else { \n    console.log(new Date(),\"Largest value lower than \",yz1); \n  } \n \n  res.send('Got a POST request:: ' + JSON.stringify(req.body) ); \n}) \n \napp.get('/getUinfo', (req, res) => res.send(JSON.stringify(displayList))) \n \n  \n \n \n \n44 \n \nChapter 5 \nConclusion and Future Work \n \n5.1 CONCLUSION \n \nWe proposed to build a high performance, scalable, agile, and low cost face recognition \nsystem. We divide the proposed approach into several small sub projects. First, we studied neural \nnetwork and convolutional neural network. Based on the theory of deep learning, we built the \nSiamese network which will train the neural network based on similarities. Then we examine and \ncompare the available open source data set, we chose ORL dataset and trained the model with \nGPU. The model will take a human face image and extract it into a vector. Then the distance \nbetween vectors are compared to determine if two faces on different picture belongs to the same \nperson.  \n \nThen we did the study, compare, design and build a system to work with the neural network \nmodel. The system uses client-server architecture. GPU is used on the server side to provide high \nperformance. We also de-coupled the main components of the system to make it flexible and \nscalable. We used the non-block and asynchronies features of Node.JS to increase the system’s \nconcurrency. Since the entire system is modularized, it can be used in different domains, thus \nreduced the development cost.  \n \n \n45 \n \n5.2 FUTURE WORK \n \nWhen build the neural network model, there are many parameters which can be tuned to \nincrease the model performance. We can keep tuning our models to increase its accuracy.  \nAlso, for a trained base model, we can re-train it using a specific dataset. So another way \nto increase the whole system’s performance is to capture the specific people’s images and re-train \nthe model based on this small dataset. For example, if an organization with 3000 people uses this \nsystem, the model can be trained to be very accurate on these 3000 people. We can employ and \nautomate this feature into the system. \n \n \n \n46 \n \nREFERENCES \n \n[1] \"cookbook.fortinet.com,\" \n10 \n10 \n2018. \n[Online]. \nAvailable: \nhttps://cookbook.fortinet.com/face-recognition-configuration-in-forticentral/. [Accessed 10 \n10 2018]. \n[2] M. J. Paul Viola, \"Robust Real-time Object Detection,\" International Journal of Computer \nVision, pp. 137-154, 2004.  \n[3] H. A. C. H. a. S. L. Bo Wu, \"Fast rotation invariant multi-view face detection based on real \nAdaboost,\" Sixth IEEE International Conference on Automatic Face and Gesture \nRecognition, pp. 79-84, 2004.  \n[4] H. A. Y. T. S. L. Yuan Li, \"Tracking in Low Frame Rate Video: A Cascade Particle Filter \nwith Discriminative Observers of Different Life Spans,\" IEEE Transactions on Pattern \nAnalysis and Machine Intelligence, pp. 1728-1740, 2008.  \n[5] C. J. T. D. H. C. A. J. G. T. F. COOTES, \"Active Shape Models-Their Training and \nApplication,\" COMPUTER VISION AND IMAGE UNDERSTANDING, pp. 38-59, 1995.  \n[6] D. C. a. T. Cootes, Boosted Regression Active Shape Models, BMVC, 2007.  \n[7] Y. W. F. W. a. J. S. X. Cao, \"Face alignment by Explicit Shape Regression,\" in 2012 IEEE \nConference on Computer Vision and Pattern Recognition, Providence, RI, 2012.  \n[8] M. T. a. A. Pentland, \"Eigenfaces for recognition,\" Journal of Cognitive Neuroscience, pp. \n71-86, 1991.  \n \n47 \n \n[9] S. S. W. G. X. C. a. H. Z. Wenchao Zhang, \"Local Gabor binary pattern histogram sequence \n(LGBPHS): a novel non-statistical model for face representation and recognition,\" Tenth \nIEEE International Conference on Computer Vision, pp. 786-791, 2005.  \n[10] X. C. F. W. a. J. S. D. Chen, \"Blessing of Dimensionality: High-Dimensional Feature and Its \nEfficient Compression for Face Verification,\" in 2013 IEEE Conference on Computer Vision \nand Pattern Recognition, Portland, OR, 2013.  \n[11] \"Convolutional_neural_network,\" \n2017. \n[Online]. \nAvailable: \nhttps://en.wikipedia.org/wiki/Convolutional_neural_network. \n[12] S. \nS. \nLiew, \n\"Research \nGate,\" \n1 \n3 \n2016. \n[Online]. \nAvailable: \nhttps://www.researchgate.net/figure/Architecture-of-the-classical-LeNet-5-\nCNN_fig2_299593011. [Accessed 10 10 2018]. \n[13] L. B. Y. B. a. P. H. Y. LeCun, \"Gradient-based learning applied to document recognition,\" \nProceedings of the IEEE, pp. 1-45, 11 1998.  \n[14] xlvector, \n\"JIANSHU,\" \n25 \n7 \n2016. \n[Online]. \nAvailable: \nhttps://www.jianshu.com/p/70a66c8f73d3. [Accessed 18 9 2018]. \n[15] F. S. Samaria, Face recognition using Hidden Markov Models, Doctoral thesis, 1995.  \n[16] E. H. G. R. A. L. H. Learned-Miller, \" Labeled Faces in the Wild: A Survey,\" Advances in \nFace Detection and Facial Image Analysis, pp. 189-248, 2016.  \n[17] V. Chu, \"Medium.com,\" 20 4 2017. [Online]. Available: https://medium.com/initialized-\ncapital/benchmarking-tensorflow-performance-and-cost-across-different-gpu-options-\n69bd85fe5d58. [Accessed 19 9 2018]. \n \n48 \n \n[18] B. L. M. S. B. Amos, \"Openface: A general-purpose face recognition library with mobile \napplications,\" CMU School of Computer Science, Tech. Rep., 2016. \n[19] B. \nHill, \n\"HOTHARDWARE,\" \n20 \n8 \n2018. \n[Online]. \nAvailable: \nhttps://hothardware.com/news/nvidia-geforce-rtx-1080-rtx-1080-ti-799-1199-september-\n20th. [Accessed 10 9 2018]. \n[20] 4psa, \"4psa.com,\" 28 6 2013. [Online]. Available: https://blog.4psa.com/the-callback-\nsyndrome-in-node-js/. [Accessed 11 8 2018]. \n[21] W.-S. Chu, \"Component-Based Constraint Mutual Subspace Method,\" 2017. [Online]. \nAvailable: http://www.contrib.andrew.cmu.edu/~wschu/project_fr.html. \n[22] W. Hwang, \"Face Recognition System Using Multiple Face Model of Hybrid Fourier Feature \nunder \nUncontrolled \nIllumination \nVariation,\" \n2017. \n[Online]. \nAvailable: \nhttp://ispl.korea.ac.kr/~wjhwang/project/2010/TIP.html. \n \n \n \n \n \n",
  "categories": [
    "cs.CV"
  ],
  "published": "2019-01-08",
  "updated": "2019-01-08"
}