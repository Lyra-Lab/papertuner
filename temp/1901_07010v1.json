{
  "id": "http://arxiv.org/abs/1901.07010v1",
  "title": "A Short Survey on Probabilistic Reinforcement Learning",
  "authors": [
    "Reazul Hasan Russel"
  ],
  "abstract": "A reinforcement learning agent tries to maximize its cumulative payoff by\ninteracting in an unknown environment. It is important for the agent to explore\nsuboptimal actions as well as to pick actions with highest known rewards. Yet,\nin sensitive domains, collecting more data with exploration is not always\npossible, but it is important to find a policy with a certain performance\nguaranty. In this paper, we present a brief survey of methods available in the\nliterature for balancing exploration-exploitation trade off and computing\nrobust solutions from fixed samples in reinforcement learning.",
  "text": "A Short Survey on Probabilistic Reinforcement\nLearning\nReazul Hasan Russel\nDepartment of Computer Science\nUniversity of New Hampshire\nDurham, NH-03824 USA\nrrussel@cs.unh.edu\nAbstract\nA reinforcement learning agent tries to maximize its cumulative payoff by in-\nteracting in an unknown environment. It is important for the agent to explore\nsuboptimal actions as well as to pick actions with highest known rewards. Yet, in\nsensitive domains, collecting more data with exploration is not always possible,\nbut it is important to ﬁnd a policy with a certain performance guaranty. In this\npaper, we present a brief survey of methods available in the literature for balanc-\ning exploration-exploitation trade off and computing robust solutions from ﬁxed\nsamples in reinforcement learning.\n1\nIntroduction\nReinforcement Learning is learning to map situations to actions that maximize a long term objec-\ntive [Sutton and Barto, 1998; Puterman, 2005; Szepesvari, 2010]. The actions are not labeled for\ntraining, rather the agent needs to learn about most rewarding actions by trying them. An action\naffects both the immediate reward and the next state yielding short and long term consequences. An\nimportant element of some reinforcement learning systems is a model of the environment, which\nmimics its behavior.\nMarkov Decision Process (MDP) is a mathematical framework to build the model of the environment\nfor reinforcement learning [Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998; Puterman, 2005].\nAn MDP is a tuple, M = (S, A, R, P, H, p0), where S = {1, . . . , S} is the state space, A =\n{1, . . . , A} is the action space, H is the horizon and p0 is the initial state distribution. At each time\nperiod h = 1, . . . , H within an episode, the agent observes a state sh ∈S, takes action ah ∈A,\nreceives a reward rh ∈R(sh, ah) and transitions to a new state sh+1 ∼P(sh, ah). A reward signal\nr ∈R is a scalar deﬁning the desirability of an event, which the agent wants to maximize over long\nrun. A policy π is a mapping from a state s ∈S to an action a ∈A. A value function Vπ(s) provides\nan estimate of the expected amount of total reward the agent can accumulate over the future when\nfollowing a policy π, starting from any particular state s. The return ρ(s, π) of an MDP is the total\naccumulated reward for executing a policy π starting from a state s.\nMDPs usually assume that transition probabilities are known precisely, but this is rarely the case in\nreality. Corrupted data set, noisy data collection process, missing assumptions, overlooked factors\netc. are common reasons for errors in transition probabilities [Petrik and Subramanian, 2014; Wiese-\nmann, 2013; Iyengar, 2005]. Also, a ﬁnite set of samples may not exactly represent the underlying\ntrue transition model. This uncertainty often leads to policies that fail in real-world deployments.\nOne of the main challenges in learning the transition probabilities is the trade-off between explo-\nration and exploitation [Sutton and Barto, 1998]. The agent needs to prefer actions yielding higher\nrewards, but also needs to discover such actions. Different actions need to be considered to collect\nstatistically signiﬁcant evidence and then favor those with higher rewards. Optimism in the face of\narXiv:1901.07010v1  [cs.LG]  21 Jan 2019\nuncertainty promotes exploration by encouraging the agent to try less certain but high potential ac-\ntions [Brafman and Tennenholtz, 2001; Auer, 2006; Dann and Brunskill, 2015; Cserna et al., 2017;\nOsband et al., 2017].\nOn the other hand, in a batch RL setup, a ﬁxed training set based on the historic interaction with\nthe environment is provided. More data cannot be collected at will in this situation because of the\nsensitivity of high stake domains, but a solution with certain performance guaranty is still important\nto obtain. For example, it can reduce the chance of an unpleasant surprise when the policy is de-\nployed and can be used to justify the need to collect more data [Petrik et al., 2016; Lim et al., 2013;\nHanasusanto and Kuhn, 2013]. If the lower bound on the return is smaller than the return of the\ncurrently-deployed policy, then the currently deployed policy would not be replaced.\nIn this paper, we review different approaches proposed in the literature for dealing with uncertainty\nin reinforcement learning. We analyze both the optimistic and robust formulations of the problem.\nWe discuss different proposed methods and their advantages/disadvantages. The paper is organized\nas follows: in Section 2, we present different methods proposed for exploration-exploitation balanc-\ning. Section 3 discusses methods available for computing robust policies in batch RL setup. Finally,\nwe draw the concluding remarks in Section 4\n2\nExploration\nA reinforcement learning agent faces a fundamental trade-off between exploration and exploitation.\nThe question of exploration is ”how can learning time be minimized?” and the question of ex-\nploitation is ”how can rewards be maximized?”. Pure exploration does not minimize learning time,\nbecause the agent may attempt sub-optimal actions. Similarly, pure exploitation cannot maximize\nthe reward, because the agent may keep choosing a sub-optimal action [Thrun, 1992a]. A reinforce-\nment learning agent tries to ﬁnd an optimal policy while minimizing both learning time and cost,\nwhich boils down to balancing exploration and exploitation. This is a very active research area in\nreinforcement learning and has received signiﬁcant attention.\n2.1\nDirected/Undirected Exploration\nThrun [1992b] states two broad categories of exploration techniques: 1) Undirected exploration, and\n2) Directed exploration.\nUndirected exploration Undirected exploration always picks an action randomly without consid-\nering the previous experience or the underlying learning process: P(a) =\n1\nA. This doesn’t learn\nanything and is the worst possible exploration strategy in terms of cost. A simple alternative is to\nexplore uniformly with a small probability ϵ, and act greedily rest of the time [Sutton and Barto,\n1998; Tokic, 2010].\nP(a) =\n\u001a 1 −ϵ,\nif a maximizes return\n1\nA,\notherwise\nThis method is known as ϵ-greedy. In the limit, as the number of steps increases, ϵ-greedy con-\nverges with the guarantee that each action is sampled an inﬁnite number of times [Sutton and Barto,\n2006]. The ϵ-greedy method is simple and effective, but it keeps choosing a sub-optimal action with\nsmall probability in the long run. Decaying ϵ over time can minimize this sub-optimal exploration,\nbut it is still non-trivial to set this decay-rate reasonably due to its non-adaptive nature.\nDirected exploration Directed exploration considers the underlying learning process and utilizes\nthe learned knowledge to guide exploration. A basic directed exploration strategy is counter based\nexploration where the number of visits to each state and/or action is maintained.\nA policy is\nevaluated with exploitation value and exploration bonus based on collected statistics. Dyna algo-\nrithm [Sutton, 1991] maintains visitation statistics for each transition from state s to s′ with action\na and obtaining reward r. It updates the policy at a state s with the latest model as:\nQ(s, a) = ˆR(s, a) + γ\nX\ns′\nˆP(s, a, s′) max\na′ Q(s′, a′)\n(1)\n2\nHere, Q(s, a) is the state-action value for taking an action a in a state s and γ is the discount factor.\nDyna performs k additional random updates following the same update model. This adds more\ncomputational cost for Dyna than general Q-learning [Watkins, 1992], but Dyna requires an order\nof magnitude fewer steps to converge to an optimal policy.\nKocsis and Szepesvri [2006] proposes a roll-out based Monte-Carlo planning algorithm UCT (Up-\nper Conﬁdence Bound applied to trees). UCT incrementally builds a lookahead tree by repeatedly\nsampling state-action-reward triplets and returns the action with highest average observed reward.\nGelly and Silver [2007] proposes three different variations of UCT and applies them in MoGo Go\nprogram. A simple count-based technique for deep-RL is described in Tang et al. [2017]; they map\nstates with a hash function and derive an exploration bonus using count statistics.\nHowever, it is not a surprise that directed exploration techniques usually outperform undirected\nexploration techniques in most problem setups.\n2.2\nExploration with Different Objectives\nAnother main consideration in classifying exploration techniques is the underlying objective of the\noptimization problem. Researchers usually consider three different objectives to optimize:\n1. Probably Approximately Correct-MDPs (PAC-MDPs)\n2. Knows What It Knows (KWIK)\n3. Regret Bounds\nPAC-MDPs For optimal exploration, an RL algorithm needs to obtain the maximum possible ex-\npected discounted reward E[P∞\nt=1 γt−1rt]. This is a very hard problem and only tractable for a\nspecial class of MDPs known as K-armed bandits [Gittins, 1979]. A relaxed but still challenging\nproblem is to act near-optimally in terms of two additional positive real parameters ϵ and δ [Kakade,\n2003; Strehl, 2007]. Here ϵ deﬁnes the quality of the solution obtained by the algorithm and δ is\na measure of conﬁdence on the obtained solution. For any ϵ > 0 and 0 < δ < 1, the complexity\nof an efﬁcient PAC-MDP algorithm is bounded by polynomials in terms of (S, A, 1\nϵ , 1\nδ ,\n1\n1−γ ) with\nprobability at least 1−δ [Strehl, 2007; Strehl and Littman, 2009]. One important notion in analyzing\nPAC-optimal algorithms is the ϵ-return mixing time T for a policy π, which is the smallest number\nT of steps required to guarantee that the distribution of states after t steps of π is within ϵ of the\nstationary distribution induced by π [Kearns and Singh, 1998].\nA model-based algorithm Explicit Explore or Exploit (E3) proposed by Kearns and Singh [1998]\nestimates the MDP parameters from data and ﬁnds a policy based on the estimated parameters. E3\ndivides the states into two sets: known and unknown. States those are visited at least m times\n(for some number m) having an accurate estimate of transition probabilities and rewards with high\nprobability are labeled as known. E3 maintains two estimated MDPs, ˆ\nM and ˆ\nM ′ based on the\nestimated parameters for known states. In ˆ\nM ′, the unknown states are merged into a recurrent state\nwith maximum reward Rmax, encouraging exploration. In an unknown state, E3 picks the action\nthat was chosen the fewest number of times (balanced wandering). When a known state appears\nduring balanced wandering, E3 exploits if the exploitation policy ˆπ for ˆ\nM achieves a return that is\nat most ϵ\n2 away from the true return. Otherwise, it executes the exploration policy ˆπ′ derived from\nˆ\nM ′. E3 returns a policy that is within ϵ of the true policy in terms of return with probability at least\n1 −δ and complexity polynomial in ( 1\nϵ , 1\nδ , S,\n1\n1−α, Rmax).\nBrafman and Tennenholtz [2001] proposes R-MAX, a model-based RL algorithm with near-optimal\naverage reward in polynomial time. R-MAX initializes the model optimistically by assigning maxi-\nmum possible rewards for each state-action. It maintains an empirical maximum-likelihood estimate\nfor reward and transition distributions of each state-action pair and updates them based on the agent’s\nobservation. Action-selection step always picks the action that maximizes the state-action value\nQ(s, .) and the update step solves the Bellman equation in 1. R-MAX achieves an expected return\nwithin 2ϵ of true return with probability at least 1 −δ and complexity polynomial in (N, k, T, 1\nϵ , 1\nδ ).\nHere, N is the number of states, k is the number of actions, T is the ϵ-return mixing time of an\noptimal policy.\n3\nKnows What It Knows (KWIK) Li et al. [2011] proposes the KWIK learning framework of su-\npervised learning for problems where active exploration affects the generated training samples. The\nmain motivation of KWIK is that, a sample that is independent from the available samples can con-\ntribute more in the learning process and can be a good candidate for exploration. In KWIK setup,\nthe algorithm gives a prediction when its accuracy is not more than ϵ away from the true label.\nOtherwise, the learning algorithm commits ”don’t know” and requests the true label to update the\nmodel. KWIK algorithms do not bound the number of mistakes for performance measure, rather it\nbounds the number of label requests it can make. This independence of performance measure from\nthe underlying distribution makes KWIK algorithms lucrative for problem domains where the input\ndistribution is dependent in complex ways. Li et al. [2011] presents a different class of algorithms\nand examples within KWIK framework for deterministic and probabilistic observations along with\nmethods for composing complex hypothesis classes.\nRegret Bounds Algorithms considering regret bounds as an objective usually care about the online\nperformance of the algorithm over some ﬁnite episodes K. For a speciﬁc episode k ∈K and a\npolicy π, the regret with respect to the true MDP M ∗is deﬁned as:\n∆k =\nX\ns∈S\np0(s)(V M ∗\nπ∗(s) −V M ∗\nπk (s))\nThe total regret over T episodes is computed as:\nRegret(T, π, M ∗) =\nT\nX\nk=1\n∆k\nAuer [2006] presents UCRL2, an algorithm that considers the total regret and behaves optimistically\nin the face of uncertainty (OFU). UCRL2 uses samples available so far to compute the maximum\nlikelihood estimates of transition probabilities ˆp and rewards ˆr. It then deﬁnes a set of statistically\nplausible MDPs M in terms of conﬁdence regions d from ˆp and ˆr. It picks the most optimistic MDP\n˜\nM ∈M and computes the near optimal policy ˜π for ˜\nM using extended value iteration:\nvi+1(s) = max\na∈A\n\u001a\n˜r(s, a) +\nmax\np∈P(s,a)\nn X\ns′∈S\np(s′)vi(s′)\no\u001b\nHere, ˜r(s, a) and P(s, a) are the maximum plausible estimates of reward and transition probabilities\nwithin the conﬁdence region d. Computation of ˜π happens only at the beginning of the episode and\nexecution of ˜π generates samples to be used in the next episode. The regret of UCRL2 is bounded\nby 34DS\nq\nAT log( T\nδ ) with probability at least 1 −δ.\nStrens [2000]; Osband et al. [2013]; Osband and Van Roy [2017] propose a Bayesian stochastically\noptimistic algorithm Posterior Sampling Reinforcement Learning (PSRL) inspired by the idea of\nThompson Sampling [Thompson, 1933]. PSRL proceeds in episodes. At the start of each episode\nk, it updates a prior distribution over MDPs with states S, actions A and horizon τ. It then samples\na single statistically plausible MDP Mk from the posterior and selects a policy µk to maximize\nvalue for that MDP. PSRL follows this policy µk over episode k and collects samples for the next\nepisode. PSRL is conecptually simple and computationally efﬁcient. The Bayesian nature of the\nalgorithm allows it to incorporate any prior knowledge. OFU algorithms like UCRL2 are often\noverly conservative because they bound worst-case scenarios simultaneously for every state-action\npair [Osband and Van Roy, 2017; Petrik and Subramanian, 2014; Wiesemann, 2013; Osband et\nal., 2013]. But PSRL doesn’t need to explicitly construct the statistically complicated conﬁdence\nbounds. PSRL rather selects policies with proportional probabilities of being optimal, quantifying\nthe uncertainty efﬁciently with posterior distribution. Therefore, PSRL outperforms OFU algorithms\nby a signiﬁcant margin. The Bayesian regret of PSRL algorithm is bounded by H\n√\nSAT.\n3\nUncertainty and Robustness\nUncertainty is inherent in many real world problems. Often it is important to ensure reasonable\nperformance guarantee during the process of learning or deployment in those problems. Robustness\n4\ncares about learning a policy that maximizes the expectation of return with respect to worst-case\nscenario [Ben-Tal et al., 2009; Petrik and Subramanian, 2014]:\nmax\nπ∈Π min\nw∈Ωπ Eπ,w\n\u0012 ∞\nX\nt=0\nγtrt\n\u0013\n(2)\nHere, Ωπ is a set of trajectories (s0, a0, s1, a1, . . .) under policy π, Eπ,w(·) is the expected return\nunder policy π and transition probabilities inferred from trajectory w. This objective mitigates\nthe effects of stochastic nature of the problem and uncertainty about problem parameters [Nilim\nand El Ghaoui, 2005; Tamar et al.; Garc and Andez, 2018]. One popular approach to optimize\nthis objective is to build an approximate model of the environment ﬁrst and then apply dynamic\nprogramming algorithms to learn the optimal policy. Robust MDP (RMDP) is a framework to do\njust that. RMDPs are identical to normal MDPs as described before, except that the transition\nprobability is not known exactly, rather comes from a set of possible transition matrices P(s, a)\nknown as an ambiguity set [Iyengar, 2005; Wiesemann, 2013; Petrik and Subramanian, 2014]. Once\nan action a ∈A is chosen in state s ∈S, a transition probability p(·|s, a) ∈P(s, a) leads to a next\nstate s′ ∈S yielding an immediate reward ˆr. The optimal value function of RMDP must satisfy the\nrobust Bellman equation:\nV (s) = max\na∈A\n\u0012\nˆr(s, a) +\nmin\np∈P(s,a)\n\u0010 X\ns′∈S\np(s′)V (s′)\n\u0011\u0013\n(3)\nThe quality of the solution on an RMDP depends on the size and shape of the ambiguity set. One\nimportant notion about constructing ambiguity sets is the underlying rectangularity assumption.\nRectangularity simply indicates the independence of the transition probabilities for different states\ns ∈S or state-action pairs (s, a) ∈S × A. the optimization problem in 3 is NP-hard in general,\nbut solvable in polynomial time when P is s, a-rectangular or s-rectangular and convex [Nilim and\nGhaoui, 2004; Iyengar, 2005; Wiesemann, 2013; Ho et al., 2018]. In s, a-rectangular RMDPs, am-\nbiguity sets are deﬁned independently for each state s and action a: ps,a ∈Ps,a. The most common\nmethod for deﬁning ambiguity sets is to use norm-bounded distance from a nominal probability\ndistribution ¯p as:\nPs,a = {p ∈∆s : ∥p −¯ps,a∥≤ψs,a}\nfor a given ψs,a ≥0, a nominal point ¯ps,a and a norm ∥·∥. Here, ∆s denotes the probability\nsimplex in RS\n+. The nominal point is often the most likely transition probability, but needs not be.\nIn s-rectangular RMDPs, the ambiguity set is deﬁned independently for each state s: ps ∈Ps as:\nPs =\nn\np1 ∈∆s, . . . , pA ∈∆s :\nX\na∈A\n∥¯ps,a −pa∥≤ψs\no\nfor a given ψs ≥0 [Wiesemann, 2013; Ho et al., 2018].\nBased on different optimization objectives, robust RL techniques can be classiﬁed into three broad\ncategories: 1) Robust objective 2) Robust Baseline Regret, and 3) Regret based Robust\nRobust objective The general idea of robustness is to compute a policy with the best worst-case\nperformance guarantee. Nilim and Ghaoui [2004] considers a robust MDP with uncertain tran-\nsition probabilities. Their proposed ways to construct ambiguity sets include: log likelihood re-\ngion based ellipsoidal or interval model, maximum a posteriori model that can incorporate prior\ninformation and entropy bound model based on Kullback-Leibler divergence between two distribu-\ntions. They show that these ambiguity sets are statistically accurate and computationally tractable.\nThey prove that perfect duality holds in the robust control problem: maxπ∈Π minp∈P V (π, p) =\nminp∈P maxπ∈Π V (π, p). They solve a robust problem equivalent to equation (3) and argues that\nthe computational cost is asymptotically same as of the classical recursion. Iyengar [2005] builds\non top of the ideas presented in Nilim and Ghaoui [2004] to build ambiguity sets and proposes a\nunifying framework for robust Dynamic Programming. They present robust value iteration, robust\npolicy iteration algorithms to compute ϵ-optimal robust solutions and independently veriﬁes that the\ncomputational cost to solve robust DP is just modestly higher than the non-robust DP.\nXu and Mannor [2012] proposes a distributionally robust criterion where the optimal policy maxi-\nmizes the expected total reward under the most plausible adversarial probability distribution. They\n5\nformulate nested uncertainty sets C1\ns ⊆C1\ns ⊆. . . C1\ns implying n different levels of estimated pa-\nrameters where Cis are state-wise independent. The policies then can be ranked based on their\nperformance under most adversarial distributions. They reduce this problem into standard RMDP\nwith a single ambiguity set, show that the optimal policy satisﬁes a Bellman type equation and can\nbe solved in polynomial time.\nWiesemann [2013] uses the observation history (s0, a0, s1, a1, . . . , sn, an) ∈(S ×A)n to construct\nan ambiguity set that contains the MDP’s true transition probabilities with the probability at least\n1−δ. The worst-case total expected reward under any policy π over this ambiguity set then provides\na valid lower bound on the expected total reward with the conﬁdence at least 1 −δ. They develop\nalgorithms to solve the robust policy evaluation and improvement problems with s, a-reactangular\nand s-reactangular ambiguity sets in polynomial time. They prove that these algorithms are not\ntractable for non-rectangular ambiguity sets unless P = NP.\nRobust Approximations for Aggregated MDPs (RAAM) [Petrik and Subramanian, 2014] proposes\nvalue function approximation with state aggregation for RL problems with very large state spaces.\nThey consider state importance weights determining the relative importance of approximation errors\nover states. They describe a linear time algorithm to construct L1 constrained rectangular uncertainty\nsets. RAAM reduces the robust optimization problem as an RMDP and obtains desired robustness\nguaranty in the aggregated model by assuming bounded worst-case importance weights.\nRobust Baseline Regret Building an accurate dynamics of the model is difﬁcult when the data is\nlimited. It’s often the case that a baseline policy πB with certain performance guaranty is deployed\non an inaccurate model and it’s only possible to replace that policy when a new policy π is guar-\nanteed to outperform πB. Baseline regret is deﬁned as the difference between these two policies\nin terms of returns: ρ(π, P ∗) −ρ(πB, P ∗), where ρ(.) is the return and P ∗is the true transition\nprobability. Robust regret is the regret obtained in the worst plausible scenario and robust baseline\nregret optimization technique tries to maximize that:\nπR = arg max\nπ∈Π min\nξ∈Ξ\n\u0012\nρ(π, ξ) −ρ(πB, ξ)\n\u0013\nHere, Ξ is the model uncertainty set.\nPetrik et al. [2016] proposes a model based approach built on top of Robust MDPs to address this\nproblem. They argue that the general technique of computing the robust policy and then replacing\nthe baseline policy only if the robust policy outperform baseline is overly conservative. They show\nthat optimizing the robust baseline regret with randomized policies is NP-hard. They propose one\napproximate algorithm based on the assumption that there is no error on the transition probabilities\nof the baseline policy. They also propose to construct and solve an RMDP to compute a better\nworst-case policy and replace πB only if it outperforms the robust performance of πB.\nRegret based Robust Parameter uncertainty in an MDP is common for real problems. Parametric\nregret of an uncertain MDP for a policy π is the gap between the performance of π and that of the\noptimal policy π∗:\nmax\nπ∈Π{ρ(π∗) −ρ(π)}\nThe goal is to minimize this maximum regret, hence deriving the term MiniMax regret. In an MDP\nwith known parameters, minimizing regret is equivalent to maximizing the performance, but that’s\nnot the case in an uncertain MDP. In uncertain MDPs, both regret and performance of a strategy are\nfunctions of parameter realizations and they may not coincide.\nXu and Mannor [2009] considers uncertain MDPs where the transition laws are known, but the\nreward parameters are not known in advance (this is a common case in state aggregation where\nstates are grouped together into hyper-states to build a reduced MDP. The transition law between\nhyper states is known in general, but the reward is uncertain due to the internal transitions inside\neach hyper-state). They ﬁnd a MiniMax Regret (MMR) strategy by solving:\nπMMR = arg min\nπ∈Π max\nr∈R{ρ(π∗, r) −ρ(π, r)}\nHere, R is the ambiguity set with admissible reward parameters and Π is the set of admissible\npolicies. They propose a Mixed Integer Programming (MIP) based subgradient method and show\n6\nthat solving this problem is NP-Hard. They reduce the problem to Linear Programs for problems\nwith small number of vertices in R and small number of candidate policies in Π, which is then\nsolvable in polynomial time.\nAhmed et al. [2017] proposes more general methods to handle both reward and transition uncer-\ntainties in uncertain MDPs. They present a Mixed Integer Linear Programming (MILP) formulation\nto approximate the minimax regret policy for a given set of samples. They consider different scal-\nable variants of regrets, namely Cumulative Expected Myopic Regret (CEMR) and One Step Regret\n(OSR). CEMR for a policy π is the cumulation of expected maximum possible regret in each state\nfor that policy:\ncemr(s, πt) =\nX\na∈A\nπt(s, a) · [R∗,t(s) −Rt(s, a) + γ\nX\ns′∈S\nT t(s, a, s′) · cemrt+1(s′, πt+1)]\nHere T t(s, a, s′) is the transition probability of going to state s′ by taking action a in state s. OSR\nfor a given policy π is the minimum regret over all the policies that are obtained by making changes\nto the policy in at most one time step:\nosr(π) = min\nˆπ [v0(π∗) −v0(ˆπ)] where, ∃ˆt s.t. ∀s, a, tˆπt(s, a) = πt(s, a), if t ̸= ˆt\nThey show the optimal substructure property of CEMR and propose a dynamic programming based\napproach to minimize CEMR. They argue that CEMR is computationally expensive for dependent\nuncertainties across states and sparse rewards. They show that OSR overcomes these drawbacks\nand also present a scalable policy iteration based algorithm to optimize OSR. The novel and much\nsimpler OSR approach consistently outperforms other regret based approaches on different practical\nproblems in a wide margin.\n4\nConclusion\nIn this paper we have presented a brief survey on probabilistic reinforcement learning. We focused\non two major aspects of developing RL algorithms: i) balancing the exploration-exploitation trade\noff and ii) computing a robust policy with limited data. We considered the underlying optimization\nobjective as the main factor to categorize different algorithms. We presented brief descriptions along\nwith their advantages/disadvantages and results of different methods.\nReferences\nAsrar Ahmed, Pradeep Varakantham, Meghna Lowalekar, Yossiri Adulyasak, and Patrick Jaillet.\nSampling Based Approaches for Minimizing Regret in Uncertain Markov Decision Processes\n(MDPs). Journal of Artiﬁcial Intelligence Research, 59:229–264, 2017.\nPeter Auer. Logarithmic Online Regret Bounds for Undiscounted Reinforcement Learning. Ad-\nvances in Neural Information Processing Systems (NIPS), 2006.\nAharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski.\nRobust Optimization.\nPrinceton\nUniversity Press, 2009.\nDimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic programming. Athena Scientiﬁc, 1996.\nRonen I. Brafman and Moshe Tennenholtz. R-MAX - A general polynomial time algorithm for near-\noptimal reinforcement learning. International Joint Conference on Artiﬁcial Intelligence (IJCAI),\n2001.\nBence Cserna, Marek Petrik, Reazul Hasan Russel, and Wheeler Ruml. Value Directed Exploration\nin Multi-Armed Bandits with Structured Priors. Uncertainty in Artiﬁcial Intelligence (UAI), 2017.\nChristoph Dann and Emma Brunskill. Sample Complexity of Episodic Fixed-Horizon Reinforce-\nment Learning. Advances in Neural Information Processing Systems, 2015.\nJavier Garc and a Fernando Fern Andez. A Comprehensive Survey on Safe Reinforcement Learning.\nIEEE Transactions on Neural Networks and Learning Systems, 29(4):1069–1081, 2018.\n7\nSylvain Gelly and David Silver. Combining Online and Ofﬂine Knowledge in UCT. International\nConference on Machine Learning (ICML), 2007.\nJ. C. Gittins. Bandit Processes and Dynamic Allocation Indices. Journal of the Royal Statistical\nSociety, 41(2):148–177, 1979.\nGA Hanasusanto and Daniel Kuhn.\nRobust Data-Driven Dynamic Programming.\nAdvances in\nNeural Information Processing Systems (NIPS), 2013.\nChin Pang Ho, Marek Petrik, and Wolfram Wiesemann. Fast Bellman Updates for Robust MDPs.\nInternational Conference on Machine Learning (ICML), 2018.\nGarud N. Iyengar. Robust dynamic programming. Mathematics of Operations Research, 30(2):257–\n280, 2005.\nSham Machandranath Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis,\n2003.\nMichael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. In-\nternational Conference on Machine Learning (ICML), 1998.\nLevente Kocsis and Csaba Szepesvri. Bandit based Monte-Carlo Planning. European Conference\non Machine Learning (ECML), 2006.\nLihong Li, Michael L Littman, Thomas J Walsh, and Alexander L Strehl. Knows what it knows: a\nframework for self-aware learning. Machine Learning, 82:399–443, 2011.\nShiau Hong Lim, Huan Xu, and Shie Mannor. Reinforcement Learning in Robust Markov Decision\nProcesses. Advances in Neural Information Processing Systems (NIPS), 2013.\nArnab Nilim and Laurent El Ghaoui. Robust Control of Markov Decision Processes with Uncertain\nTransition Matrices. Mathematics of Operations Research, 53(5):780–798, 2005.\nArnab Nilim and Laurent El Ghaoui. Robust solutions to Markov decision problems with uncertain\ntransition matrices. Mathematics of Operations Research, 53(5):780–788, 2004.\nIan Osband and Benjamin Van Roy. Why is Posterior Sampling Better than Optimism for Rein-\nforcement Learning? International Conference on Machine Learning (ICML), 2017.\nIan Osband, Daniel Russo, and Benjamin Van Roy. (More) Efﬁcient Reinforcement Learning via\nPosterior Sampling? Advances in Neural Information Processing Systems (NIPS), 2013.\nIan Osband, Benjamin Van Roy, Daniel Russo, and Zheng Wen. Deep Exploration via Randomized\nValue Functions. PhD thesis, 2017.\nMarek Petrik and Dharmashankar Subramanian. RAAM: The beneﬁts of robustness in approxi-\nmating aggregated MDPs in reinforcement learning. Advances in Neural Information Processing\nSystems (NIPS), 2014.\nMarek Petrik, Mohammad Ghavamzadeh, and Yinlam Chow. Safe Policy Improvement by Min-\nimizing Robust Baseline Regret. Advances in Neural Information Processing Systems (NIPS),\n2016.\nMartin L Puterman. Markov decision processes: Discrete stochastic dynamic programming. John\nWiley & Sons, Inc., 2005.\nAlexander L Strehl and Michael L Littman. Reinforcement Learning in Finite MDPs: PAC Analysis.\nJournal of Machine Learning Research (JMLR), 10:2413–2444, 2009.\nAlexander L Strehl. Probably Approximately Corrct ( PAC ) Exploration in Reinforcement Learning.\nPhD thesis, 2007.\nMalcolm Strens. A Bayesian Framework for Reinforcement Learning. International Conference on\nMachine Learning (ICML), 2000.\n8\nRichard S Sutton and Andrew Barto. Reinforcement learning: An Introduction. MIT Press, 1998.\nRichard S Sutton and Andrew Barto. Reinforcement Learning: An Introduction (Second Edition).\nMIT Press, 2006.\nRichard S. Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM\nSIGART Bulletin, 2(4):160–163, 1991.\nCsaba Szepesvari. Algorithms for reinforcement learning. Morgan and Claypool Publisher, pages\n1–98, 2010.\nAviv Tamar, Dotan Di Castro, and Shie Mannor. Temporal Difference Methods for the Variance of\nthe Reward To Go. International Conference on Machine Learning (ICML).\nHaoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, and Xi Chen. Exploration: A Study\nof Count-Based Exploration for Deep Reinforcement Learning. Advances in Neural Information\nProcessing Systems (NIPS), 2017.\nW.R. Thompson. On the Likelihood that One Unknown Probability Exceeds Another in View of the\nEvidence of Two Samples. Oxford University Press, 25(3):285–294, 1933.\nSebastian Thrun. The role of exploration in learning control. Handbook of Intelligent Control:\nNeural, Fuzzy and Adaptive Approaches, 1992.\nSebastian B. Thrun. Efﬁcient Exploration In Reinforcement Learning. Technical Report, 1992.\nMichel Tokic. Adaptive ϵ-greedy exploration in reinforcement learning based on value differences.\nAnnual German Conference on Advances in Artiﬁcial Intelligence, 2010.\nChristopher J C H Watkins. Q-Learning. Machine Learning, 1992.\nWolfram Wiesemann. Robust Markov decision processes. Mathematics of Operations Research,\n38(1):153–183, 2013.\nHuan Xu and Shie Mannor. Parametric regret in uncertain Markov decision processes. IEEE Con-\nference on Decision and Control, 2009.\nHuan Xu and Shie Mannor. Distributionally Robust Markov Decision Processes. Mathematics of\nOperations Research, 37(2):288–300, 2012.\n9\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-01-21",
  "updated": "2019-01-21"
}