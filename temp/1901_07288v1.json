{
  "id": "http://arxiv.org/abs/1901.07288v1",
  "title": "Unsupervised Learning-based Depth Estimation aided Visual SLAM Approach",
  "authors": [
    "Mingyang Geng",
    "Suning Shang",
    "Bo Ding",
    "Huaimin Wang",
    "Pengfei Zhang",
    "Lei Zhang"
  ],
  "abstract": "The RGB-D camera maintains a limited range for working and is hard to\naccurately measure the depth information in a far distance. Besides, the RGB-D\ncamera will easily be influenced by strong lighting and other external factors,\nwhich will lead to a poor accuracy on the acquired environmental depth\ninformation. Recently, deep learning technologies have achieved great success\nin the visual SLAM area, which can directly learn high-level features from the\nvisual inputs and improve the estimation accuracy of the depth information.\nTherefore, deep learning technologies maintain the potential to extend the\nsource of the depth information and improve the performance of the SLAM system.\nHowever, the existing deep learning-based methods are mainly supervised and\nrequire a large amount of ground-truth depth data, which is hard to acquire\nbecause of the realistic constraints. In this paper, we first present an\nunsupervised learning framework, which not only uses image reconstruction for\nsupervising but also exploits the pose estimation method to enhance the\nsupervised signal and add training constraints for the task of monocular depth\nand camera motion estimation. Furthermore, we successfully exploit our\nunsupervised learning framework to assist the traditional ORB-SLAM system when\nthe initialization module of ORB-SLAM method could not match enough features.\nQualitative and quantitative experiments have shown that our unsupervised\nlearning framework performs the depth estimation task comparable to the\nsupervised methods and outperforms the previous state-of-the-art approach by\n$13.5\\%$ on KITTI dataset. Besides, our unsupervised learning framework could\nsignificantly accelerate the initialization process of ORB-SLAM system and\neffectively improve the accuracy on environmental mapping in strong lighting\nand weak texture scenes.",
  "text": "Unsupervised Learning-based Depth Estimation\naided Visual SLAM Approach\nMingyang Geng 1, Suning Shang 1, Bo Ding 1∗, Huaimin Wang 1, Pengfei\nZhang 1, and Lei Zhang 2\nNational Key Laboratory of Parallel and Distributed Processing,\nCollege of Computer, National University of Defense Technology, China 1\nNational Key Laboratory of Integrated Automation of Process Industry,\nNortheastern University, China 2\nAbstract. Existing visual-based SLAM systems mainly utilize the three-\ndimensional environmental depth information from RGB-D cameras to\ncomplete the robotic synchronization localization and map construction\ntask. However, the RGB-D camera maintains a limited range for work-\ning and is hard to accurately measure the depth information in a far\ndistance. Besides, the RGB-D camera will easily be inﬂuenced by strong\nlighting and other external factors, which will lead to a poor accuracy\non the acquired environmental depth information. Recently, deep learn-\ning technologies have achieved great success in the visual SLAM area,\nwhich can directly learn high-level features from the visual inputs and\nimprove the estimation accuracy of the depth information. Therefore,\ndeep learning technologies maintain the potential to extend the source\nof the depth information and improve the performance of the SLAM\nsystem. However, the existing deep learning-based methods are mainly\nsupervised and require a large amount of ground-truth depth data, which\nis hard to acquire because of the realistic constraints. In this paper, we\nﬁrst present an unsupervised learning framework, which not only uses im-\nage reconstruction for supervising but also exploits the pose estimation\nmethod to enhance the supervised signal and add training constraints\nfor the task of monocular depth and camera motion estimation. Further-\nmore, we successfully exploit our unsupervised learning framework to\nassist the traditional ORB-SLAM system when the initialization mod-\nule of ORB-SLAM method could not match enough features. Qualitative\nand quantitative experiments have shown that our unsupervised learning\nframework performs the depth estimation task comparably to the super-\nvised methods and outperforms the previous state-of-the-art approach by\n13.5% on KITTI dataset. Besides, our unsupervised learning framework\ncould signiﬁcantly accelerate the initialization process of ORB-SLAM\nsystem and eﬀectively improve the accuracy on environmental mapping\nin strong lighting and weak texture scenes.\nKeywords: Robotic visual SLAM, monocular depth estimation, pose\nestimation, unsupervised learning\narXiv:1901.07288v1  [cs.CV]  22 Jan 2019\n2\nMingyang Geng 1, Suning Shang 1, Bo Ding 1∗et al.\n1\nIntroduction\nSimultaneous Localization and Mapping (SLAM) has attracted increasing at-\ntention in the robotic areas. SLAM technologies have wide applications in area\nsuch as autonomous driving, localization and navigation. The goal of a SLAM\nsystem is to construct the map of an unknown environment incrementally based\non the perception information, i.e., scene information acquired by a radar or\ndepth sensor when the robot is performing a complex task and confronted with\nan unknown environment. In order to achieve a satisfying performance in the\nvisual SLAM tasks, the quality of the perception on the environmental depth,\ni.e., the distance of the objects in the environment, will play an indispensable\nrole. Therefore, how to extract valuable depth information from the visual inputs\nis an important problem in the visual SLAM systems.\nExisting visual-based SLAM systems mainly utilize the three-dimensional\nenvironmental depth information from RGB-D cameras. However, the RGB-D\ncamera maintains a limited range for working and is hard to accurately measure\nthe depth information in a far distance. Besides, in some special scenes, i.e.,\nstrong lighting and weak texture environments, robotic visual SLAM always\nfaces the problems of scale drift or scale error because of the inaccurate accuracy\non the acquired depth information. The reason of obtaining the imprecise depth\ninformation is that most of the existing visual SLAM algorithms design sparse\nimage features manually, while the manually designed features often contain\ncertain assumptions about the environment, i.e., suﬃcient illumination, material\ndetermination, which will lead to a poor performance on environmental depth\nestimation when the environmental factors change.\nFig. 1: Illustration of our learning framework. The input to our system consists\nsolely of unlabeled video clips. Our learning framework estimates the depth in\nthe ﬁrst image and the camera motion.\nLecture Notes in Computer Science: Authors’ Instructions\n3\nDeep learning technologies have recently emerged as a powerful tool for im-\nproving the accuracy on monocular depth estimation [10,15,23,34]. One of the\nadvantages of deep learning technologies over common alternatives is that fea-\ntures are learned directly from data, and do not have to be chosen or designed\nby the algorithm developers for the speciﬁc problem on which they are applied.\nHowever, existing methods [10, 15, 23] are mainly supervised and need a large\namount of ground-truth data, which is hard to acquire because of the expen-\nsive radar sensors and the limited working range. A promising branch in the\ndepth estimation ﬁeld is unsupervised learning [34], which exploits image re-\nconstruction as supervision signal and signiﬁcantly reduce the burden to collect\nhigh-quality depth training data in advance. However, the existing unsupervised\nmethod [34] does not fully exploit the heuristic knowledge during the image ac-\nquisition process, which can strengthen the supervised signal and further improve\nthe accuracy on depth estimation. Therefore, how to enhance the supervised sig-\nnal and utilize the unsupervised learning technologies to assist the traditional\nvisual SLAM systems remains a great challenge.\nIn this paper, we ﬁrst propose a novel unsupervised learning framework which\nexploit the pose estimation method to enhance the supervised signals and fur-\nther promote the accuracy of extracting the depth information from monocular\nimage sequences. In concrete, we utilize a large number of scene image sequences\nto train a model for camera motion prediction and scene structure prediction\n(shown in Fig. 1). In the pose estimation stage, we set up a continuous frame\nwindow and exploit the pose transformation relationships to construct the pose\ngraph, which can partially eliminate the cumulative error. Furthermore, we suc-\ncessfully exploit our unsupervised learning framework to assist the traditional\nORB-SLAM system, a widely used visual SLAM system, when the initializa-\ntion module of ORB-SLAM method could not match enough features. Extensive\nexperiments have shown that our method can signiﬁcantly accelerate the initial-\nization process of ORB-SLAM system and eﬀectively improve the accuracy on\nenvironmental mapping in strong lighting and weak texture scenes.\nThe rest of this paper is organized as follows. Section 2 introduces the back-\nground and the highly related work. Section 3 describes the methodology of our\nwork as well as the architecture designed for training and prediction. Section\n4 describes the details of our unsupervised learning-based depth estimation-\naided visual SLAM system. The validation and evaluation of our work based\non diﬀerent public datasets are described in Section 5. Section 6 presents the\nexperimental results implemented in the real-world settings. We conclude and\nprovide our future direction in Section 7.\n2\nRelated Work\nOur method covers three research areas, including depth estimation optimiza-\ntion, monocular depth estimation and motion estimation from images.\n4\nMingyang Geng 1, Suning Shang 1, Bo Ding 1∗et al.\n2.1\nDepth Estimation optimization\nThe existing mechanism of depth estimation optimization can be mainly divided\ninto three categories based on how to utilize the deep learning technologies.\nThe ﬁrst kind of methods directly substitute the depth information acqui-\nsition module with deep learning technologies [12]. This method can eﬀectively\nsuit for the environments which are hard for traditional visual SLAM methods\nto deal with, i.e., strong lighting and weak texture environments. However, this\nmethod will require a more complex system with a stronger computing and pro-\ncessing ability. In addition, deep learning technologies are easily over-ﬁt to the\ndataset and will lead to a poor performance in the unfamiliar environments. Last\nbut not least, there does not exist a method which fully substitute the depth\ninformation acquisition module with deep learning technologies in realistic ap-\nplications. Therefore, this kind of method needs further validation.\nThe second kind of methods exploit the environmental depth information\nfrom the deep learning technologies and the traditional SLAM system simulta-\nneously [22]. This method can optimize the environmental depth information\nused by the visual SLAM system and indirectly improve the accuracy on map-\nping and localization. However, this method needs to implement the two depth\ninformation acquisition methods simultaneously and require a stronger ability\nof computing and processing. In addition, the complex evaluation process to se-\nlect the optimal depth information needs to be accurately implemented, which\nis hard to satisfy the quality of service.\nThe third kind of methods complement the drawbacks of the deep learning\ntechnologies and the depth information acquisition module of traditional SlAM\nalgorithm [32]. In concrete, deep learning technologies is only applied when the\ntraditional SLAM algorithms can not obtain high-accuracy depth information.\nThis method can eﬀectively improve the accuracy of the depth estimation while\nmaintaining the ability to guarantee the QoS requirement. There are very limited\nworks which exploit the deep learning technologies to acquire the environmen-\ntal depth estimation. A deep learning-aided LSD-SLAM algorithm is proposed\nin [11], which achieves a better result than the traditional LSD-SLAM algo-\nrithm and a stronger adaptability to the strong lighting and weak texture en-\nvironments. We choose ORB-SLAM as our baseline method, which has wide\napplications in visual SLAM area but performs an unsatisﬁed performance in\nstrong lighting and weak texture environments. To the best of our knowledge,\nthis is the ﬁrst work which combines the deep learning technologies and the\nORB-SLAM system.\n2.2\nMonocular Depth Estimation\nMonocular depth estimation is a basic low-level challenge problem which has\nbeen studied for decades. Early works on depth estimation using RGB images\nusually relied on hand-crafted features and probabilistic graphical models. [16]\nintroduced photo pop-up, a fully automatic method for creating a basic 3D\nmodel from a single photograph. In [18], the authors design Depth Transfer, a\nLecture Notes in Computer Science: Authors’ Instructions\n5\nnon-parametric approach where the depth of an input image is reconstructed by\ntransferring the depth of multiple similar images and then applying some warping\nand optimizing procedures. Delage et al. in [7] proposed a dynamic Bayesian\nframework for recovering 3D information from indoor scenes. A discriminatively-\ntrained multi-scale Markov Random Field (MRF) was introduced in [29], in order\nto optimally fuse local and global features. Depth estimation is considered as an\ninference problem in a discrete-continuous CRF in [24].\nMore recent approaches for depth estimation are based on convolutional neu-\nral network(CNN). As a pioneer work, Eigen et al. proposed a multi-scale ap-\nproach for depth prediction in [10]. It considers two deep networks, one perform-\ning a coarse global prediction based on the entire image, and the other reﬁning\npredictions locally. This approach was extended in [9] to handle multiple tasks\n(e.g. semantic segmentation, surface normal estimation). In [23], authors com-\nbine a deep CNN and a continuous conditional random ﬁeld, and attain visually\nsharper transitions and local details. In [21], a deep residual network is devel-\noped, based on the ResNet and achieved higher accuracy than [23]. Unlike our\napproach, these methods require explicit depth for training. Unsupervised learn-\ning setups have also been explored for disparity image prediction. For instance,\nGodard et al. formulate disparity estimation as an image reconstruction prob-\nlem in [15], where neural networks are trained to warp left images for matching\nthe right one. Though these methods show similarity with ours, which are un-\nsupervised without requiring ground-truth depth data for training, they assume\ncamera poses known in advance, which is treated a large simpliﬁcation. Our work\nis inspired by that of [34], which proposes to use view synthesis as the supervi-\nsory signal. However, the further advantage of our approach which demonstrated\nin the following evaluations is that, the idea of continuous frame window used\nin traditional SLAM approach is applied to enhance the supervisory signal and\ncapture more constraints which can guide the training process for more accuracy\nresults.\n2.3\nMotion Estimation from Images\nThe motion estimation has a long history in computer vision. The underlying\n3D geometry is a consolidated ﬁeld. They consist of a long pipeline of meth-\nods, start from descriptor matching for ﬁnding a sparse set of correspondences\nbetween images [26], to estimating the essential matrix to determine the cam-\nera motion. Bundle adjustment [33] is used in the pipeline of method to reﬁne\nthe ﬁnal structure and camera position. The bundle adjustment minimizes the\nreprojection error of the three-dimensional point in the two-dimensional image\nsequence by Levenberg-Marquardt (LM) nonlinear algorithm to get the optimal\nmotion model [25]. The accuracy of the bundle adjustment method is related to\nthe number of frames of the image. The more the number of image frames, the\nmore accurate the camera motion parameters can obtain.\nRecent works [8] propose learning frame-to-frame motion ﬁelds with deep\nneural networks supervised with ground-truth motion obtained from simulation\nor synthetic movies. This enables eﬃcient motion estimation that learns to deal\n6\nMingyang Geng 1, Suning Shang 1, Bo Ding 1∗et al.\nwith lack of texture using training examples rather than relying only on smooth-\nness constraints of the motion ﬁeld, as previous optimization methods [31]. Our\napproach draws on the respective advantages of the geometry-based motion esti-\nmation in SLAM and the learning-based motion estimation. Multiview pose net-\nwork is used to estimate pose transformation matrix between adjacent frames.\nWe set up a continuous frame window to construct the pose graph and use the\npose transform relationship to calculate more pose transform matrices which\nperfect the pose graph.\n3\nPose Estimation-based Monocular Depth Estimation\nMethod\nThe accuracy of existing monocular depth estimation methods is hard to satisfy\nthe requirement realistic applications. Therefore, it is meaningful to improve the\naccuracy of monocular depth estimation. In this section, we introduce our pose\nestimation-based monocular depth estimation method from the following three\naspects: the basic framework, learning and geometry-based pose estimation and\nimage recovery-based training method.\n3.1\nFramework\nGiven a single image frame I, the goal of our method is to provide two functions\nf1 and f2 which can predict the per-pixel scene depth d\n′ = f1I and the camera\npose p\n′ = f2I. We design two deep neural networks (depth estimation neural\nnetwork and pose estimation neural network) to learn these two functions. Most\nexisting methods treat the learning task as a supervised learning problem, where\nthe color input images, the corresponding target depth and pose values are pro-\nvided. However, it is not practical to acquire such large amount of ground-truth\ndepth and pose data in various scenes because of the expensive lidar sensor\nand the limited working range. Besides, existing methods always neglect the\ntraditional pose estimation methods and do not take the prior knowledge from\ntraditional algorithms into account.\nWe propose an unsupervised learning method, which exploits the pose es-\ntimation approach in traditional SLAM algorithms to augment the supervised\nsignals by image reconstruction. In concrete, based on a short image sequences\nIi, Ii+1, Ii+2captured by a moving camera, we can reconstruct the image I\n′\ni by\nthe predicted the depth image Di and the predicted pose estimation matrix.\nThe diﬀerence between the image Ii and the reconstructed image I\n′\ni can be used\nas the supervised signals to train the depth estimation and the pose estima-\ntion neural networks. The framework of our unsupervised method is illustrated\nin Fig. 2. The monocular video sequences are used for training the single-view\ndepth estimation and the multi-view pose estimation networks. The output of\nthe single-view depth estimation network is the depth map of the input image.\nFor the pose estimation part, a continuous frame window are used as the input\nand the output of the multi-view pose estimation network is the pose transform\nLecture Notes in Computer Science: Authors’ Instructions\n7\nFig. 2: The overview of the training pipeline based on image reconstruction.\nThe depth network takes only the ﬁrst image Ii as input and outputs a per-\npixel depth map ˆDx. The continuous frame window takes images (e.g., Ii, Ii+1,\nIi+2) as input, through the pose network, outputs the relative camera pose\nmatrices of adjacent frames ( ˆTi→i+1, ˆTi+1→i+2) and we can use the camera pose\nmatrices to calculate more camera poses ( ˆTi+1→i+2). The outputs of both models\nhave then used to inverse warp images to reconstruct the target image, and the\nphotometric reconstruction loss is used for training the CNNs. By utilizing image\nreconstruction as supervision, we are able to train the entire framework in an\nunsupervised manner from videos.\n8\nMingyang Geng 1, Suning Shang 1, Bo Ding 1∗et al.\nFig. 3: Illustration of the camera pose estimation pipeline. For example, we main-\ntain a continuous frame window whose length is 3 frames for estimating the\ncamera pose transform matrix. For each sequence images (e.g., Ii, Ii+1, Ii+2), we\nwill get the adjacent camera pose transform matrix ( ˆTi→i+1, ˆTi+1→i+2). We can\nuse the camera pose transformation to get more camera poses ˆTi→i+2.\nmatrices between all adjacent frames in the continuous frame window. We then\noptimize the pose graph in the continuous frame window by calculating more\nnonadjacent pose transform matrices using a pose transform relationship. Then,\nwe can reconstruct the image by the depth map and the pose transform matri-\nces and train the two neural networks by calculating the diﬀerence between the\ninput and the reconstructed images.\n3.2\nEstimation based on Learning and Geometry Pose Graph\nGiven a frame Ii, the single-view depth estimation network can directly predict\nthe corresponding depth map ˆDi. For the pose estimation part, our method is\nbased both on the unsupervised learning technologies and the traditional geome-\ntry pose graph. A continuous frame window is set up before the multi-view pose\nestimation network in order to make the network learn the pose relationship\nbetween the continuous images < I1, I2, · · · , In > simultaneously. The length of\nthe continuous frame window stands for the number of input images in a train-\ning episode. In other words, the continuous frame window sequentially reads n\nimages from the training set and then sends the training data to the multi-view\npose estimation network for further processing. During the training process, the\nmulti-view pose estimation network will sequentially predict the transformation\nmatrix between two adjacent frames in the continuous frame window (shown in\nFig. 3). For a better illustration, denote the the input image sequences in the\ncontinuous frame window as < I1, I2, · · · , In >, the output of the multi-view pose\nestimation network is ˆTi→i+1. Then, we can build a preliminary pose graph us-\nLecture Notes in Computer Science: Authors’ Instructions\n9\ning the pose transformation matrices between the adjacent frames. However, the\npreliminary pose graph lacks the pose transform matrices between non-adjacent\nframes, so we calculate the non-adjacent pose transformation relationships by\nusing the following function:\nˆTi→i+1 × ˆTi+1→i+2 = ˆTi→i+2\n(1)\nSimilarly, we can increase the length of the frame window to get more non-\nadjacent pose transformation relationships (e.g., ˆTi→i+5) and improve the pose\ngraph. The acquired pose transformation relationships maintain the following\ntwo advantages. First, the cumulative error is partially eliminated. In the con-\ntinuous frame window, the error of pose estimation between adjacent frames will\naccumulate gradually. But if we use the calculated pose matrix to reconstruct\nthe image, we can sequentially adjust the parameters of the pose network and\npartially eliminate the cumulative error. The second advantage is that this mech-\nanism can avoid the estimation errors between the frames which are far apart in\nthe frame sequence. Experiments [34] have shown that learning-based methods\ncould not predict a satisfying relationship between the two frames which main-\ntain a far distance in the frame sequence. Our method solve this problem by\ncalculating the far apart pose relationships based on the pose estimation of the\nadjacent frames.\n3.3\nGeometry-based Image Reconstruction\nFig. 4: Illustration of image reconstruction based on camera pose matrix. For\neach point (e.g., x1) in the ﬁrst image, we project it onto the other image base\non the predicted depth and camera pose and then use bilinear interpolation to\nobtain the value of the warped image (ˆI2) at location (x1).\n10\nMingyang Geng 1, Suning Shang 1, Bo Ding 1∗et al.\nImage reconstruction through the means of warps and camera projection is\nan important application of geometric scene understanding. The goal of image\nreconstruction is to reconstruct a new viewpoints image from other viewpoints\nthrough warps and camera projection. In our learning framework, we reconstruct\nthe target image It by sampling pixels from the other images Ir based on the\npredicted depth map ˆDt and the predicted 4 × 4 camera pose transformation\nmatrix ˆTt→r.\nOur camera model is the pinhole model. Denote K as the camera intrinsic\nmatrix, I1 and I2, as the ﬁrst and the second image in a training episodes.\nThe transformation matrices of the two images to the world coordinates are\nrepresented as T1→w and T2→w, and the homogeneous coordinates of a pixel in\nthe ﬁrst image is represented as x1. We can acquire the projected coordinates of\nx1 onto the second image x2 by x2 ∼KT2→wT −1\n2→wK−1 ˆD1(x1)x1. Notice that\nthe camera transformation matrix between I1 and I2 is equal to T2→wT −1\n1→w, i.e.,\nˆT1→2 = T2→wT −1\n1→. We substitute T1→wT −1\n2→w with ˆT1→2 so the formula becomes\nx2 ∼K ˆT1→2K−1 ˆD1(x1)x1. Furthermore, when we get a short image sequences\n< I1, I2, · · · , In > at the training time, denote It as the target view image, and\nIr as the other images. The pixel project procedure can be formulated as:\nxr ∼K ˆTt→rK−1 ˆDt(xt)xt.\n(2)\nBased on Eq.2, we can project the pixels on the target image It onto other\nimages Ir. After that, our image reconstruction model uses the image sampler\nfrom the spatial transformer network (STN) to sample the projected image Ir.\nThe STN uses bilinear sampling where the output pixel is the weighted sum\nof the four pixel neighbors x(1)\nr , x(2)\nr , x(3)\nr , x(4)\nr\nof xr, i.e., ˆIr(xt) = Ir(xr) =\nP4\ni=1 w(i)Ir(xi\nr), where w(i) is linearly proportional to the spatial proximity\nbetween xr and xi\nr, and P4\ni=1 w(i) = 1 (shown in Fig. 4). Contrast with the\nalternative approaches [13], the bilinear sampler is locally fully diﬀerentiable and\nintegrates seamlessly into our fully convolutional network, which means that we\ndo not require any simpliﬁcation or approximation of our cost function.\nFinally, we use the predicted depth map ˆDt and the predicted 4 × 4 camera\npose transformation matrix ˆTt→r of the previous step to reconstruct the target\nimage through projection and the diﬀerentiable bilinear sampling mechanism.\n3.4\nImage Reconstruction as Supervision\nImage reconstruction has been used to learn end-to-end unsupervised optical\nﬂow [17], disparity ﬂow in a stereo rig [15] and video prediction [28]. These\nmethods reconstruct the images by transforming the input based on depth maps\nor ﬂow ﬁelds. Our work considers dense structure estimation and uses monocular\nvideos to obtain the necessary self-supervision, instead of static images. The\ndepth information could also be predicted from a single image supervised by\nphotometric error [13]. However, the methods above do not infer camera pose\ntransform or object motion and require stereo pairs with known baseline in the\ntraining process. Our work estimates the camera motion between frames, which\nLecture Notes in Computer Science: Authors’ Instructions\n11\neﬀectively removes the constraint that the ground-truth pose of the camera is\nknown in the training process.\nIn concrete, denote the ﬁrst and the second images acquired from a moving\nmonocular camera in chronological order as I1 and I2. Instead of directly pre-\ndicting the depth and the camera pose of the ﬁrst view image I1, we use the\nsecond image I2 to reconstruct I1, which is illustrated in the previous section.\nEvery pixel point x in I2 coordinates is warped to the target coordinate image.\nLet x indexes over the pixel coordinate, and denote ˆI2 as the second image I2\nwarped to the ﬁrst coordinate frame based on the image reconstruction process.\nWhen we use image reconstruction as a supervised signal, the diﬀerence\nbetween the reconstructed image and the target image can be calculated by\nC = P\nx |I1(x) −I2(x)|. Similarly, denote the short image sequences as <\nI1, I2, · · · , In > in the training process, It as the target view image, and Ir\nas the other images. The image reconstructs procedure can be formulated as:\nCvr =\nX\nr\nX\nx\n|It(x) −ˆIr(x)|,\n(3)\nwhere the supervisory signal Cvr adjust the parameters of our depth estima-\ntion and pose estimation networks.\n3.5\nNetwork Architecture and Training Loss\nFig. 5: Network architecture for our depth and pose prediction modules. (a) For\nsingle-view depth network, we adopt the DispNet architecture with multi-scale\nside predictions. All conv layers are followed by ReLU activation except for the\nprediction layers. (b) For pose network, the input is sequences of consecutive\nframes. All conv layers are followed by ReLU except for the last layer where no\nnonlinear activation is applied.\nAs shown in Fig. 5, the architecture consists of two diﬀerent networks: the\nsingle-view depth network and the pose network. The single-view depth network\nis inspired by DispNet but several important modiﬁcations are made to enable\nthe training process without ground-truth depth data. The single-view network\nis mainly composed of two parts: the encoder (from cnv1 to cnv7b) and the\ndecoder (from deconv7). The decoder uses skip connection layers to connect\n12\nMingyang Geng 1, Suning Shang 1, Bo Ding 1∗et al.\nthe activation blocks of the encoder, which enables the ability to obtain more\nrepresentative features. The disparity are predicted at four diﬀerent scales (from\ndisp4 to disp1). The function of the pose network is to predict the relative poses\nbetween the target image and other input images, which are accurately described\nby the 6-dimensional camera pose transform matrix (3-dimensional euler angles\nand 3-dimensional translation).\nIn our learning framework, the gradients are mainly derived from the pixel\nintensity diﬀerence between the four-pixel neighbors of xr and xt. Therefore,\nthe training process will be inhibited when the correct xr (projected using the\nground-truth depth and pose) is located in a low-texture region or far from the\ncurrent estimation. We solve this problem by using multi-scale and smoothness\nloss which allows gradients to be derived from larger spatial regions directly.\nDenote the loss at each output scale as Cs, so the total loss can be represented\nas C = P4\ns=1 Cs. Our loss module calculates Cs as a combination of the two\nmain terms:\nCs = Cvr + λCsmooth,\n(4)\nwhich encourages the reconstructed image to appear similar to the corre-\nsponding training input, indexes the minimized norm of the second-order gra-\ndients for the predicted depth maps. λ denotes the weighting for the depth\nsmoothness loss.\n4\nVisual SLAM System with the Assistance of\nUnsupervised Learning-based Depth Estimation\nExisting visual SLAM system always acquires the depth information from the\ndepth sensor. The depth sensor can directly obtain the environmental depth\ninformation within a certain distance. However, the depth sensors suﬀer from\nthe limited working range and are sensitive to the interference, which will de-\ncrease the acquired accuracy. In Section 3, we propose an image depth estimation\nmethod based on camera pose transformation relationship, which can directly\nobtain the depth information from the image through unsupervised learning. In\nthis section, we introduce our method which extends the source of the three-\ndimensional environmental depth information and improves the accuracy of the\ndepth information on the basis of ORB-SLAM algorithm.\n4.1\nTraditional ORB-SLAM Algorithm\nThe ORB-SLAM algorithm is mainly composed of the following three parts: the\ntracking module, the local map building module and the closed-loop detection [3]\nmodule. The tracking module aims to locate the camera through each frame and\ndetermines whether the frame should join the key-frame set. The tracking task\nﬁrst extracts the features of the frame, then initializes the camera pose and\nmap and tracks the local map, ﬁnally determines the key frame. The local map\nLecture Notes in Computer Science: Authors’ Instructions\n13\nbuilding module aims to process the new key-frames and rebuild the map through\nlocal BA [2]. The closed-loop detection module mainly judges the newly added\nkey-frame and determines whether the scene has been encountered before.\nIn the initialization phase of ORB-SLAM, the monocular camera ﬁrst reads\nthe RGB image, and the depth sensor acquires the depth data. Then, feature\npoint matching process is performed on the two consecutive frames in the time\nseries to determine the motion of the camera. The image and depth acquisition\nprocess is implemented until the number of matched feature points on the two\nconsecutive frames reaches a speciﬁed threshold. Then, the ORB-SLAM algo-\nrithm utilizes the existing geometric relationship between the matched feature\npoints to calculate the current pose of the camera, and further creates an initial\nmap or updates the local map.\n4.2\nDepth Estimation Optimizing Mechanism\nBy analyzing the ORB-SLAM algorithm, we decide to optimize the depth in-\nformation acquisition process in the initialization phase. In concrete, our unsu-\npervised learning-based depth estimation mechanism is applied only when the\nORB-SLAM system does not match enough feature points. In this way, the\naccuracy of the ORB-SLAM will be eﬀectively optimized while maintaining a\nreasonable computing ability and satisfying the QoS requirement.\nThe initialization phase of ORB-SLAM algorithm is optimized as follows.\nWhen there are not enough matched feature points between two consecutive\nframes, the RGB image will be transmitted to our monocular depth estimation\nnetwork instead of making the system read the next frame. Then, our depth\nestimation network will re-estimate the environmental depth information of the\ninput image and further implement the feature point matching process. The\ncomplete depth information optimization mechanism on the basis of ORB-SLAM\nalgorithm is illustrated below:\n4.3\nImplement of Depth Estimation Assisted Visual SLAM System\nbased on Unsupervised Learning\nWe now introduce the details to realize the optimized ORB-SLAM system us-\ning our unsupervised learning-based depth estimation method. For the training\nprocess of our monocular depth estimation networks, the training dataset in-\ncludes samples from various scenes (i.e., indoor scenes, outdoor scenes) as well\nas samples of weak texture and strong light. After the training process, we de-\ncoupled the trained network because the optimization mechanism only uses the\ndepth estimation network. In addition, we establish the transmission channel\nof the ORB-SLAM initialization module and the trained depth estimation net-\nwork. When the ORB-SLAM algorithm does not match enough feature points,\nthe RGB image is transmitted to the depth estimation network, which keeps the\nrunning state to guarantee a immediate output.\n14\nMingyang Geng 1, Suning Shang 1, Bo Ding 1∗et al.\nAlgorithm 1 The initialization process of the optimized ORB-SLAM algorithm\nusing our monocular depth estimation network.\nRequire: The threshold M, which stands for the number of the matched feature\npoints required by the ORB-SLAM algorithm.\n1: The monocular camera reads the RGB image and the depth sensor acquires\nthe depth information.\n2: Feature point matching process is implemented on the two consecutive\nframes in time series based on the RGB image and the depth information.\n3: if the number of matched feature points is less than the threshold M then\n4:\nTransmit the current image to the monocular depth estimation network\nand get the new depth image.\n5:\nImplement the feature point matching process on the two consecutive\nframes in time series based on the RGB image and the new depth image.\n6:\nif the matched feature points still less than M then\n7:\nRead the next RGB image and get the corresponding depth informa-\ntion.\n8:\nelse\n9:\nGet the pose of the camera by utilizing the geometric relationship\nbetween the matched feature points.\n10:\nend if\n11: else\n12:\nGet the pose of the camera by utilizing the geometric relationship be-\ntween the matched feature points.\n13: end if\n14: Create the initial map or update the local map.\n5\nEvaluation on the Testing Set\nIn this section, we evaluate the performance of our approach and make compar-\nison with the existing methods on single-view depth and ego-motion estimation.\nWe choose KITTI dataset as the test benchmark. To evaluate the cross-dataset\ngeneralization ability of our approach and demonstrate the superiority on the\nstrong lighting and weak texture environments, we also use the Make3D dataset\nfor a better illustration.\n5.1\nTraining Details\nWe implement our system in TensorFlow [1]. In all experiments, the value of λ\nis set to 0.5. During the training process, we use the Adam optimizer [19] with\nβ1 of 0.9, learning rate of 0.0002 and mini-batch size of 4. All the experiments\nare performed with image sequences captured with a monocular camera and the\nimages are resized to 128×416 during training. In the test phase, the depth and\npose networks can be applied independently for images of arbitrary size.\nLecture Notes in Computer Science: Authors’ Instructions\n15\n5.2\nSingle-view Depth Estimation\nWe present results for the KITTI dataset [14] using two diﬀerent test splits, to\nenable comparison to existing works. In its raw form, the dataset contains 42382\nimages from 61 scenes. The length of the continuous frame window is set to 3.\nFig. 6: The sample prediction on Cityscapes dataset using our approach trained\non Cityscapes only.\nTo the best of our knowledge, among the methods which learn single-view\ndepth estimation from monocular videos using unsupervised mechanism, state-\nof-the-art performance is achieved in Zhou [34]. We also make comparison with\nmethods using supervised mechanism (depth ground-truth with depth supervi-\nsion or calibrated stereo images with pose supervision) for training. Our method\nuses a scale factor to deﬁne the predicted depth so in the test phase, we multi-\nply the predicted depth maps with a scalar which matches the median with the\nground-truth data. Fig. 6 illustrates the predictions of our approach training on\nCityscapes dataset [6]. We also make comparison with Godard [15] by taking\nthe same training strategy which ﬁrst pre-train the system on the Cityscapes\ndataset and then ﬁne-tune on the KITTI dataset.\n16\nMingyang Geng 1, Suning Shang 1, Bo Ding 1∗et al.\nTable 1: Single-view depth results on the KITTI dataset and Cityscapes dataset.\nMethod\nDataset Supervision\nError metric\nAccuracy metric\nDepth Pose Abs Rel Sq Rel RMSE RMSE log δ < 1.25 δ < 1.252 δ < 1.253\nTrain set mean\nK\n✓\n0.403\n5.530\n8.709\n0.403\n0.593\n0.776\n0.878\nEigen et al. Coarse\nK\n✓\n0.214\n1.605\n6.563\n0.292\n0.673\n0.884\n0.957\nEigen et al. Fine\nK\n✓\n0.203\n1.548\n6.307\n0.282\n0.702\n0.890\n0.958\nLiu et al.\nK\n✓\n0.202\n1.614\n6.523\n0.275\n0.678\n0.895\n0.965\nGodard et al.\nK\n✓\n0.148\n1.344\n5.927\n0.247\n0.803\n0.922\n0.964\nGodard et al.\nCS+K\n✓\n0.124\n1.076\n5.311\n0.219\n0.847\n0.942\n0.973\nZhou et al.\nK\n0.208\n1.768\n6.856\n0.283\n0.678\n0.885\n0.957\nZhou et al.\nCS+K\n0.198\n1.836\n6.565\n0.275\n0.718\n0.901\n0.960\nOurs\nK\n0.180 1.510 6.349\n0.256\n0.741\n0.906\n0.966\nOurs\nCS\n0.236 2.476 7.249\n0.307\n0.645\n0.861\n0.946\nOurs\nCS+K\n0.170 1.429 6.082\n0.245\n0.786\n0.927\n0.969\nKITTI We follow the experimental settings proposed by [10] with the the test\nset of 697 images covering 29 scenes. Table 1 shows the performance comparison\nbetween our method and the baseline methods. Here, K stands for the KITTI\ndataset and CS stands for the Cityscapes dataset. Compared with the meth-\nods using depth supervision [10,23], our method performs better. However, our\nunsupervised method performs a little worse than the methods using pose su-\npervision mechanism [13, 15]. [15] uses calibrated stereo images with left-right\ncycle consistency loss for training. In future work, we will apply the similar cycle\nconsistency loss to our framework.\nCompared with previous state-of-the-art method [34] using unsupervised\nmechanism, our method decreases the depth estimation error of nearly 13.5%.\nThis validates that our learning framework can eﬀectively take advantage of the\nknowledge gained from the camera pose in traditional SLAM algorithms. We\nfurther compare the depth images obtained by our method with the baseline\nmethods. From Fig. 7, we can see that our results have no explicit diﬀerence\nwith those of the supervised approaches. Furthermore, our method can even\nbetter represent the depth of the boundary information in some special scenes.\nFig. 8 visualizes the testing results of our method using the strategy of [15] (ﬁrst\npre-train on the Cityscapes dataset and then ﬁne-tune the model on the KITTI\ndataset).\nIn order to show the relationship between the length of the continuous frame\nwindow and the performance of our system, we set the length of the window\nto 5 and repeat the experiments above. As shown in Table 2, the performance\nof 5-frame version version is better than that of the 3-frame version on all the\nmetrics, which is consistent with our suppose. The beneﬁt is attributed to the\nabundant transformation matrices between frames, which can further augment\nthe supervised signals.\nLecture Notes in Computer Science: Authors’ Instructions\n17\nFig. 7: Comparison of single-view depth estimation between Eigen et al. [10]\n(with ground-truth depth supervision), Garg et al. [34] (with ground-truth pose\nsupervision), Zhou et al. [34] (unsupervised), and ours (unsupervised). The\nground-truth depth map is interpolated from sparse measurements for visual-\nization purpose.\nFig. 8: Comparison of single-view depth predictions on the KITTI dataset by\nour initial Cityscapes model and the ﬁnal model (pretrained on Cityscapes and\nthen ﬁne-tuned on KITTI). The Cityscapes model sometimes ignores structural\nmistakes (e.g. roadside billboards and lamp posts) likely due to the domain gap\nbetween the two datasets.\n18\nMingyang Geng 1, Suning Shang 1, Bo Ding 1∗et al.\nTable 2: Results of diﬀerent continuous frame window length versions of our\nsystem.\nMethod\nThe length of the\nError metric\ncontinuous frame window\nAbs Rel\nSq Rel\nRMSE\nRMSE log\nZhou et al.\n3\n0.208\n1.768\n6.856\n0.283\nOurs\n3\n0.180\n1.510\n6.349\n0.256\nOurs\n5\n0.176\n1.455\n5.940\n0.248\nFig. 9: Our sample predictions on the Make3D dataset. Note that our model is\ntrained on KITTI+Cityscapes only, and directly tested on Make3D.\nLecture Notes in Computer Science: Authors’ Instructions\n19\nMake3D In order to evaluate the generalization ability and the adaptability\nof our proposed method on the strong lighting and weak texture environments,\nwe choose Make3D [30] dataset for further experiments. In concrete, we pre-\ntrain our network on Cityscapes dataset and ﬁne-tune on KITTI dataset. Then,\nwe evaluate our model on Make3D dataset, which contains abundant strong\nlighting and weak texture samples and maintains an explicit diﬀerence between\nthe other two datasets. As shown in Table 3, the results on Make3D dataset are\nsimilar to those of KITTI dataset. In concrete, compared with the methods using\ndepth supervision mechanism, our method achieves a better performance than\n[10,23], but a little worse than [15], which indicates that our method maintains a\nsatisfying generalization ability. Compared with the unsupervised method [34],\nour method still achieves a better performance. We further visualize the sample\npredictions of our method in Fig. 9 for a better illustration.\nTable 3: Results on the Make3D dataset.\nMethod\nSupervision\nError metric\nDepth\nPose\nAbs Rel\nSq Rel\nRMSE\nRMSE log\nTrain set mean\n✓\n0.876\n13.98\n12.27\n0.307\nKarsch et al.\n✓\n0.428\n5.079\n8.389\n0.149\nLiu et al.\n✓\n0.475\n6.562\n10.05\n0.165\nLaina et al.\n✓\n0.204\n1.840\n5.683\n0.084\nGodard et al.\n✓\n0.544\n10.94\n11.76\n0.193\nZhou et al.\n0.383\n5.321\n10.47\n0.478\nOurs\n0.343\n4.739\n8.201\n0.455\n5.3\nPose Estimation\nWe choose ORB-SLAM [27] as the baseline method to illustrate the eﬀectiveness\nof our proposed pose estimation network. We follow the experimental settings\nin [34] and use the oﬃcial KITTI odometry split method to guarantee a fair\ncomparison. The odometry benchmark is composed of 11 driving sequences with\nground-truth odometry. We choose the ﬁrst 9 driving sequences (00-08) for train-\ning and the last 2 driving sequences (09-10) for testing. The ground-truth odom-\netry is used to evaluate our ego-motion estimation performance and the length\nof the frame window is set to 5. We compare our ego-motion estimation with two\nvariants of monocular ORB-SLAM algorithm. The ﬁrst one is ORB-SLAM (full)\nwhich uses all frames of the driving sequence to recover odometry. The second\none is ORB-SLAM (short) which is lack of the loop closure and re-localization\nmodules and maintains the same input setting as our system (5-frame snippets).\nBesides, the unsupervised method [34] is also selected as the baseline.\nNotably, due to the reason that diﬀerent methods have diﬀerent scales, we\noptimize the scaling factor for the predictions made by each method to make all\n20\nMingyang Geng 1, Suning Shang 1, Bo Ding 1∗et al.\nFig. 10: Pose estimation trajectories comparison.\nthe scaling factors consistent with the ground-truth. The Absolute Trajectory\nError (ATE) of the ground-truth and the estimated trajectory is chosen for\nevaluation. All the methods are computed on 5-frame snippets except for ORB-\nSLAM (full). For the ORB-SLAM (full) method, we break down the trajectory\nof the full sequence into 5-frame snippets by adjusting the reference coordinate\nframe to the central frame of each snippet.\nTable 4: Absolute Trajectory Error (ATE) on the KITTI odometry (lower is\nbetter).\nMethod\nSeq.09\nSeq.10\nORB-SLAM(full)\n0.014±0.008\n0.012±0.011\nORB-SLAM(short)\n0.064 ± 0.141\n0.064 ± 0.130\nMean Odom\n0.032 ± 0.026\n0.028 ± 0.023\nORB-SLAM(short)\n0.064 ± 0.141\n0.064 ± 0.130\nZhou et al.\n0.021 ± 0.017\n0.021 ± 0.017\nOurs\n0.017±0.008\n0.015±0.017\nAs shown in Table 4, our approach performs comparably with the ORB-\nSLAM (full) method, which utilizes the whole image sequences for loop closure\nand re-localization to improve the pose estimation accuracy. The ATE value\nLecture Notes in Computer Science: Authors’ Instructions\n21\nof our approach is about a quarter of that acquired by ORB-SLAM (short).\nIn future, it would be interesting to use our learned ego-motion instead of the\nlocal estimation modules in monocular SLAM systems. Meanwhile, our pose\nestimation outperforms the previous state-of-the-art unsupervised method [34],\nwhich is conceptually similar to ours. Fig. 10 illustrates the pose estimation\ntrajectories comparison between our method and the baseline methods.\n6\nEvaluation in the Realistic Settings\nIn this section, we ﬁrst introduce the details on our platform for implementing the\nunsupervised learning-based depth estimation aided ORB-SLAM system using\ncloud robotic infrastructure. Then, we present the testing results of our system\nin various scenes.\n6.1\nExperimental Platform\nOur system is composed of two parts: the robot and the server from the per-\nspective of hardware.\nThe robot is deployed with multiple sensors, i.e., camera, ultrasonic radar\nand sound sensor. In our system, the robot interacts with the real-world settings\nby collecting the images from the RGB-D sensor and transmitting them to the\nserver. The server mainly deals with the data saving and data processing tasks. In\nconcrete, for our system, the depth estimation network and the pose estimation\nnetworks are both deployed in the server and the server will also processes the\ncomputing task and the simultaneous localization and mapping task. Fig. 11\nillustrates the sparse point cloud image acquired by our system and the green\nlines stand for the pose trajectory predicted by our system.\nFig. 11: The sparse point cloud image acquired by our system on a indoor desk\nscene.\nThe depth estimation network and the pose estimation network deployed in\nthe server are both well trained on KITTI, Cityscapes and Make3D datasets.\nWe choose TUM RGBD dataset to evaluate the performance and the server uses\nNVIDA GeForce 1080P GPU for the processing tasks.\n22\nMingyang Geng 1, Suning Shang 1, Bo Ding 1∗et al.\n6.2\nExperimental Results in Strong Lighting and Weak Texture\nEnvironments\nFig. 12: Illustration of pose estimation in a normal scene (desk).\nFig. 13: Illustration of pose estimation in a weak texture scene Fr3/nst.\nFig. 14: Illustration of pose estimation in a strong lighting scene Fr3/stf.\nThe standard for evaluating the performance of our system and the ORB-\nSLAM system is ATE, which is widely used for testing in the SLAM area. We\ncompare the performances of the two methods in various scenes of TUM RGBD\ndataset. The results are reported in Table 5, the ﬁrst 5 scenes are in the normal\nenvironments and the last 3 scenes are in the strong lighting or weak texture\nenvironments. We can see that in normal scenes, our system performs compa-\nrably to the traditional ORB-SLAM system. In the strong lighting and weak\nLecture Notes in Computer Science: Authors’ Instructions\n23\ntexture environments, our system achieves a better performance than that of\nthe traditional ORB-SLAM system. For a better illustration, we report the pose\nestimation trajectories in normal, weak texture and strong lighting environments\nrespectively (shown in Fig. 12, Fig. 13, and Fig. 14).\nTable 5: ATE Comparison of our method and traditional ORB-SLAM system in\ndiﬀerent scenes.\nScene\nORB-SLAM\nOurs\nFr1/desk\n0.018490\n0.017181\nFr1/desk2\n0.021034\n0.021564\nFr2/desk\n0.011296\n0.010978\nFr1/room\n0.061536\n0.062283\nFr2/oﬃce\n0.010999\n0.010901\nFr2/stf\n0.013178\n0.012105\nFr2/stn\n0.012706\n0.012294\nFr2/nst\n0.023507\n0.022203\n6.3\nTesting on the Speed for Initialization\nDue to the reason that our method optimizes the initialization process of the\ntraditional ORB-SLAM system, we design a series of experiments to test the\ninitialization speed of the two systems by recording the number of images used\nfor initialization.\nFig. 15: Initialization speed comparison in a strong lighting scene.\nFig. 15 and Fig. 16 report the testing results in a strong lighting and weak\ntexture environments of the TUM RGBD dataset respectively. The lines in the\nred square stand for the estimated trajectories in the initialization process. We\ncan see that the estimated trajectory of our method is longer than that of the\ntraditional ORB-SLAM system in both the two situations, which indicates that\nour method can complete the initialization process and start to build the map in a\n24\nMingyang Geng 1, Suning Shang 1, Bo Ding 1∗et al.\nFig. 16: Initialization speed comparison in a weak texture scene.\nfaster speed. Table 6 reports the comparison on the number of images used in the\ninitialization process. For the ﬁrst 5 normal scenes, our system uses comparable\nnumber of images to the traditional ORB-SLAM system. However, towards the\nlast 3 scenes in the strong lighting and weak texture environments, the number\nof images used by our system is explicitly less than that of the traditional ORB-\nSLAM system, which indicates the eﬀectiveness of our approach.\nTable 6: Comparison on the number of images used in the initialization process.\nScene\nORB-SLAM\nOurs\nFr1/desk\n4\n2\nFr1/desk2\n5\n2\nFr2/desk\n4\n4\nFr1/room\n7\n2\nFr2/oﬃce\n6\n5\nFr2/stf\n38\n10\nFr2/stn\n145\n84\n7\nConclusion\nWe present an unsupervised learning framework for single-view depth and ego-\nmotion estimation. The proposed method exploits the pose estimation method\nto enhance the supervised signal and add training constraints for the task of\nmonocular depth and camera motion estimation. The system is trained on unla-\nbeled videos and performs comparably to approaches that require ground-truth\ndepth or pose for training. Furthermore, our method outperforms the previous\nstate-of-the-art unsupervised learning method by 13.5% on KITTI dataset. Fi-\nnally, we successfully exploit our unsupervised learning framework to assist the\ntraditional ORB-SLAM system when the initialization module of ORB-SLAM\nmethod could not match enough features. Experiments have shown that our\nmethod can signiﬁcantly accelerate the initialization process of traditional ORB-\nLecture Notes in Computer Science: Authors’ Instructions\n25\nSLAM system and eﬀectively improve the accuracy on environmental mapping\nin strong lighting and weak texture scenes.\nAcknowledgments. This work was supported by the National Natural Sci-\nence Foundation of China (grant numbers 61751208, 61502510, and 61773390),\nthe Outstanding Natural Science Foundation of Hunan Province (grant number\n2017JJ1001).\nReferences\n1. Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghe-\nmawat, S., Irving, G., Isard, M., et al.: Tensorﬂow: a system for large-scale machine\nlearning. In: OSDI. vol. 16, pp. 265–283 (2016)\n2. Agarwal, S., Snavely, N., Seitz, S.M., Szeliski, R.: Bundle adjustment in the large.\nIn: European conference on computer vision. pp. 29–42. Springer (2010)\n3. Canutescu, A.A., Dunbrack Jr, R.L.: Cyclic coordinate descent: A robotics algo-\nrithm for protein loop closure. Protein science 12(5), 963–972 (2003)\n4. Ciresan, D., Giusti, A., Gambardella, L.M., Schmidhuber, J.: Deep neural networks\nsegment neuronal membranes in electron microscopy images. In: Advances in neural\ninformation processing systems. pp. 2843–2851 (2012)\n5. Cire¸san, D., Meier, U., Schmidhuber, J.: Multi-column deep neural networks for\nimage classiﬁcation. arXiv preprint arXiv:1202.2745 (2012)\n6. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R.,\nFranke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene\nunderstanding. In: Proceedings of the IEEE conference on computer vision and\npattern recognition. pp. 3213–3223 (2016)\n7. Delage, E., Lee, H., Ng, A.Y.: A dynamic bayesian network model for autonomous\n3d reconstruction from a single indoor image. In: Computer Vision and Pattern\nRecognition, 2006 IEEE Computer Society Conference on. vol. 2, pp. 2418–2428.\nIEEE (2006)\n8. Dosovitskiy, A., Fischer, P., Ilg, E., Hausser, P., Hazirbas, C., Golkov, V., Van\nDer Smagt, P., Cremers, D., Brox, T.: Flownet: Learning optical ﬂow with convo-\nlutional networks. In: Proceedings of the IEEE International Conference on Com-\nputer Vision. pp. 2758–2766 (2015)\n9. Eigen, D., Fergus, R.: Predicting depth, surface normals and semantic labels with\na common multi-scale convolutional architecture. In: Proceedings of the IEEE In-\nternational Conference on Computer Vision. pp. 2650–2658 (2015)\n10. Eigen, D., Puhrsch, C., Fergus, R.: Depth map prediction from a single image using\na multi-scale deep network. In: Advances in neural information processing systems.\npp. 2366–2374 (2014)\n11. Engel, J., Sch¨ops, T., Cremers, D.: Lsd-slam: Large-scale direct monocular slam.\nIn: European Conference on Computer Vision. pp. 834–849. Springer (2014)\n12. Gao, X., Zhang, T.: Unsupervised learning to detect loops using deep neural net-\nworks for visual slam system. Autonomous robots 41(1), 1–18 (2017)\n13. Garg, R., BG, V.K., Carneiro, G., Reid, I.: Unsupervised cnn for single view depth\nestimation: Geometry to the rescue. In: European Conference on Computer Vision.\npp. 740–756. Springer (2016)\n26\nMingyang Geng 1, Suning Shang 1, Bo Ding 1∗et al.\n14. Geiger, A., Lenz, P., Urtasun, R.: Are we ready for autonomous driving? the kitti\nvision benchmark suite. In: Computer Vision and Pattern Recognition (CVPR),\n2012 IEEE Conference on. pp. 3354–3361. IEEE (2012)\n15. Godard, C., Mac Aodha, O., Brostow, G.J.: Unsupervised monocular depth esti-\nmation with left-right consistency. In: 2017 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR). pp. 6602–6611. IEEE (2017)\n16. Hoiem, D., Efros, A.A., Hebert, M.: Automatic photo pop-up. In: ACM transac-\ntions on graphics (TOG). vol. 24, pp. 577–584. ACM (2005)\n17. Jason, J.Y., Harley, A.W., Derpanis, K.G.: Back to basics: Unsupervised learning\nof optical ﬂow via brightness constancy and motion smoothness. In: European\nConference on Computer Vision. pp. 3–10. Springer (2016)\n18. Karsch, K., Liu, C., Kang, S.B.: Depth transfer: Depth extraction from videos\nusing nonparametric sampling. In: Dense Image Correspondences for Computer\nVision, pp. 173–205. Springer (2016)\n19. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 (2014)\n20. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep con-\nvolutional neural networks. In: Advances in neural information processing systems.\npp. 1097–1105 (2012)\n21. Laina, I., Rupprecht, C., Belagiannis, V., Tombari, F., Navab, N.: Deeper depth\nprediction with fully convolutional residual networks. In: 3D Vision (3DV), 2016\nFourth International Conference on. pp. 239–248. IEEE (2016)\n22. Li, R., Wang, S., Long, Z., Gu, D.: Undeepvo: Monocular visual odometry through\nunsupervised deep learning. In: 2018 IEEE International Conference on Robotics\nand Automation (ICRA). pp. 7286–7291. IEEE (2018)\n23. Liu, F., Shen, C., Lin, G., Reid, I.D.: Learning depth from single monocular images\nusing deep convolutional neural ﬁelds. IEEE Trans. Pattern Anal. Mach. Intell.\n38(10), 2024–2039 (2016)\n24. Liu, M., Salzmann, M., He, X.: Discrete-continuous depth estimation from a single\nimage. In: Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition. pp. 716–723 (2014)\n25. Lourakis, M.I.: A brief description of the levenberg-marquardt algorithm imple-\nmented by levmar. Foundation of Research and Technology 4(1), 1–6 (2005)\n26. Lowe, D.G.: Distinctive image features from scale-invariant keypoints. Interna-\ntional journal of computer vision 60(2), 91–110 (2004)\n27. Mur-Artal, R., Montiel, J.M.M., Tardos, J.D.: Orb-slam: a versatile and accurate\nmonocular slam system. IEEE Transactions on Robotics 31(5), 1147–1163 (2015)\n28. Patraucean, V., Handa, A., Cipolla, R.: Spatio-temporal video autoencoder with\ndiﬀerentiable memory. arXiv preprint arXiv:1511.06309 (2015)\n29. Saxena, A., Chung, S.H., Ng, A.Y.: 3-d depth reconstruction from a single still\nimage. International journal of computer vision 76(1), 53–69 (2008)\n30. Saxena, A., Sun, M., Ng, A.Y.: Make3d: Learning 3d scene structure from a single\nstill image. IEEE transactions on pattern analysis and machine intelligence 31(5),\n824–840 (2009)\n31. Sun, D., Roth, S., Black, M.J.: Secrets of optical ﬂow estimation and their princi-\nples. In: Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Confer-\nence on. pp. 2432–2439. IEEE (2010)\n32. Tateno, K., Tombari, F., Laina, I., Navab, N.: Cnn-slam: Real-time dense monoc-\nular slam with learned depth prediction. In: Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR). vol. 2 (2017)\nLecture Notes in Computer Science: Authors’ Instructions\n27\n33. Triggs, B., McLauchlan, P.F., Hartley, R.I., Fitzgibbon, A.W.: Bundle adjustmenta\nmodern synthesis. In: International workshop on vision algorithms. pp. 298–372.\nSpringer (1999)\n34. Zhou, T., Brown, M., Snavely, N., Lowe, D.G.: Unsupervised learning of depth and\nego-motion from video. In: CVPR. vol. 2, p. 7 (2017)\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2019-01-22",
  "updated": "2019-01-22"
}