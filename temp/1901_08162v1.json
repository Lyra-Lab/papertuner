{
  "id": "http://arxiv.org/abs/1901.08162v1",
  "title": "Causal Reasoning from Meta-reinforcement Learning",
  "authors": [
    "Ishita Dasgupta",
    "Jane Wang",
    "Silvia Chiappa",
    "Jovana Mitrovic",
    "Pedro Ortega",
    "David Raposo",
    "Edward Hughes",
    "Peter Battaglia",
    "Matthew Botvinick",
    "Zeb Kurth-Nelson"
  ],
  "abstract": "Discovering and exploiting the causal structure in the environment is a\ncrucial challenge for intelligent agents. Here we explore whether causal\nreasoning can emerge via meta-reinforcement learning. We train a recurrent\nnetwork with model-free reinforcement learning to solve a range of problems\nthat each contain causal structure. We find that the trained agent can perform\ncausal reasoning in novel situations in order to obtain rewards. The agent can\nselect informative interventions, draw causal inferences from observational\ndata, and make counterfactual predictions. Although established formal causal\nreasoning algorithms also exist, in this paper we show that such reasoning can\narise from model-free reinforcement learning, and suggest that causal reasoning\nin complex settings may benefit from the more end-to-end learning-based\napproaches presented here. This work also offers new strategies for structured\nexploration in reinforcement learning, by providing agents with the ability to\nperform -- and interpret -- experiments.",
  "text": "Causal Reasoning from Meta-reinforcement Learning\nIshita Dasgupta∗1,4 , Jane Wang1,\nSilvia Chiappa1, Jovana Mitrovic1, Pedro Ortega1,\nDavid Raposo1, Edward Hughes1, Peter Battaglia1,\nMatthew Botvinick1,3, Zeb Kurth-Nelson1,2\n1DeepMind, UK\n2MPS-UCL Centre for Computational Psychiatry, UCL, UK\n3Gatsby Computational Neuroscience Unit, UCL, UK\n4Department of Physics and Center for Brain Science, Harvard University, USA\nAbstract\nDiscovering and exploiting the causal structure in\nthe environment is a crucial challenge for intelligent\nagents. Here we explore whether causal reasoning\ncan\nemerge\nvia\nmeta-reinforcement\nlearning.\nWe train a recurrent network with model-free\nreinforcement learning to solve a range of problems\nthat each contain causal structure. We find that\nthe trained agent can perform causal reasoning in\nnovel situations in order to obtain rewards. The\nagent can select informative interventions, draw\ncausal inferences from observational data, and make\ncounterfactual predictions. Although established\nformal causal reasoning algorithms also exist, in this\npaper we show that such reasoning can arise from\nmodel-free reinforcement learning, and suggest that\ncausal reasoning in complex settings may benefit\nfrom the more end-to-end learning-based approaches\npresented here. This work also offers new strategies\nfor structured exploration in reinforcement learning,\nby providing agents with the ability to perform—and\ninterpret—experiments.\n1. Introduction\nMany machine learning algorithms are rooted in discovering\npatterns of correlation in data. While this has been sufficient to\nexcel in several areas (Krizhevsky et al., 2012; Cho et al., 2014),\nsometimes the problems we are interested in are intrinsically\ncausal. Answering questions such as “Does smoking cause\ncancer?”\nor “Was this person denied a job due to racial\ndiscrimination?” or “Did this marketing campaign cause sales\nto go up?” require an ability to reason about causes and effects.\nCausal reasoning may be an essential component of natural\n∗Corresponding author: ishitadasgupta@g.harvard.edu\nintelligence and is present in human babies, rats, and even birds\n(Leslie, 1982; Gopnik et al., 2001; 2004; Blaisdell et al., 2006;\nLagnado et al., 2013).\nThere is a rich literature on formal approaches for defining and\nperforming causal reasoning (Pearl, 2000; Spirtes et al., 2000;\nDawid, 2007; Pearl et al., 2016). We investigate whether such\nreasoning can be achieved by meta-learning. The approach of\nmeta-learning is to learn the learning (or inference/estimation)\nprocedure itself, directly from data. Analogous (Grant et al.,\n2018) models that learn causal structure directly from the\nenvironment, rather than having a pre-conceived formal theory,\nhave also been implicated in human intelligence (Goodman\net al., 2011).\nWe specifically adopt the “meta-reinforcement learning”\nmethod introduced previously (Duan et al., 2016; Wang et al.,\n2016), in which a recurrent neural network (RNN)-based\nagent is trained with model-free reinforcement learning (RL).\nThrough training on a large family of structured tasks, the\nRNN becomes a learning algorithm which generalizes to new\ntasks drawn from a similar distribution. In our case, we train on\na distribution of tasks that are each underpinned by a different\ncausal structure. We focus on abstract tasks that best isolate\nthe question of interest: whether meta-learning can produce an\nagent capable of causal reasoning, when no notion of causality\nis explicitly given to the agent.\nMeta-learning offers advantages of scalability by amortizing\ncomputations and, by learning end-to-end, the algorithm has\nthe potential to find the internal representations of causal struc-\nture best suited for the types of causal inference required\n(Andrychowicz et al., 2016; Wang et al., 2016; Finn et al., 2017).\nWe chose to focus on the RL approach because we are interested\nin agents that can learn about causes and effects not only from\npassive observations but also from active interactions with the\nenvironment (Hyttinen et al., 2013; Shanmugam et al., 2015).\n1\narXiv:1901.08162v1  [cs.LG]  23 Jan 2019\nG\nA\nE\nH\np(A)\np(E|A)\np(H|A,E)\n(a)\nG→E=e\nA\nE\nH\np(A)\nδ(E−e)\np(H|A,E)\n(b)\nFigure 1. (a): A CBN G with a confounder for the effect of exercise (E)\non heath (H) given by age (A). (b): Intervened CBN G→E=e resulting\nfrom modifying G by replacing p(E|A) with a delta distribution δ(E−\ne) and leaving the remaining conditional distributions p(H|E,A) and\np(A) unaltered.\n2. Problem Specification and Approach\nWe examine three distinct data settings – observational,\ninterventional, and counterfactual – which test different kinds\nof reasoning.\n• In the observational setting (Experiment 1), the agent can\nonly obtain passive observations from the environment.\nThis type of data allows an agent to infer correlations\n(associative reasoning) and, depending on the structure of\nthe environment, causal effects (cause-effect reasoning).\n• In the interventional setting (Experiment 2), the agent can\nact in the environment by setting the values of some vari-\nables and observing the consequences on other variables.\nThis type of data facilitates estimating causal effects.\n• In the counterfactual setting (Experiment 3), the agent\nfirst has an opportunity to learn about the causal structure\nof the environment through interventions. At the last step\nof the episode, it must answer a counterfactual question\nof the form “What would have happened if a different\nintervention had been made in the previous timestep?”.\nNext we formalize these three settings and the patterns\nof reasoning possible in each, using the graphical model\nframework (Pearl, 2000; Spirtes et al., 2000; Dawid, 2007).\nRandom variables will be denoted by capital letters (e.g., E)\nand their values by small letters (e.g., e).\n2.1. Causal Reasoning\nCausal relationships among random variables can be expressed\nusing causal Bayesian networks (CBNs) (see the Supple-\nmentary Material). A CBN is a directed acyclic graphical\nmodel that captures both independence and causal relations.\nEach node Xi corresponds to a random variable, and the\njoint distribution p(X1,...,XN) is given by the product of\nconditional distributions of each node Xi given its parent nodes\npa(Xi), i.e. p(X1:N ≡X1,...,XN)=QN\ni=1p(Xi|pa(Xi)).\nEdges carry causal semantics: if there exists a directed path\nfrom Xi to Xj, then Xi is a potential cause of Xj. Directed\npaths are also called causal paths. The causal effect of Xi on\nXj is the conditional distribution of Xj given Xi restricted to\nonly causal paths.\nAn example of CBN G is given in Fig. 1a, where E represents\nhours of exercise in a week, H cardiac health, and A age. The\ncausal effect of E on H is the conditional distribution restricted\nto the path E →H, i.e. excluding the path E ←A→H. The\nvariable A is called a confounder, as it confounds the causal\neffect with non-causal statistical influence. Simply observing\ncardiac health conditioning on exercise level from p(H|E)\n(associative reasoning) cannot answer if change in exercise\nlevels cause changes in cardiac health (cause-effect reasoning),\nsince there is always the possibility that correlation between\nthe two is because of the common confounder of age.\nCause-effect Reasoning. The causal effect of E = e can be\nseen as the conditional distribution p→E=e(H|E=e)1 on the\nintervened CBN G→E=e resulting from replacing p(E|A) with\na delta distribution δ(E−e) (thereby removing the link from\nA to E) and leaving the remaining conditional distributions\np(H|E,A) and p(A) unaltered (Fig. 1b). The rules of do-\ncalculus (Pearl, 2000; Pearl et al., 2016) tell us how to compute\np→E=e(H|E = e) using observations from G. In this case\np→E=e(H|E =e)=P\nAp(H|E =e,A)p(A)2. Therefore, do-\ncalculus enables us to reason in the intervened graph G→E=e\neven if our observations are from G. This is the scenario\ncaptured by our observational data setting outlined above.\nSuch inferences are always possible if the confounders are\nobserved, but in the presence of unobserved confounders, for\nmany CBN structures the only way to compute causal effects\nis by collecting observations directly from the intervened graph,\ne.g. from G→E=e by fixing the value of the variable E = e\nand observing the remaining variables—we call this process\nperforming an actual intervention in the environment. In our\ninterventional data setting, outlined above, the agent has access\nto such interventions.\nCounterfactual Reasoning. Cause-effect reasoning can be\nused to correctly answer predictive questions of the type\n“Does exercising improve cardiac health?” by accounting for\ncausal structure and confounding. However, it cannot answer\nretrospective questions about what would have happened. For\nexample, given an individual i who has died of a heart attack,\nthis method would not be able to answer questions of the type\n“What would the cardiac health of this individual have been\nhad she done more exercise?”. This type of question requires\nreasoning about a counterfactual world (that did not happen).\nTo do this, we can first use the observations from the factual\nworld and knowledge about the CBN to get an estimate of\n1In the causality literature, this distribution would most often be\nindicated with p(H|do(E=e)). We prefer to use p→E=e(H|E=e)\nto highlight that intervening on E results in changing the original\ndistribution p, by structurally altering the CBN.\n2Notice that conditioning on E=e would instead give p(H|E=\ne)=P\nAp(H|E=e,A)p(A|E=e).\nthe specific latent randomness in the makeup of individual i\n(for example information about this specific patient’s blood\npressure and other variables as inferred by her having had\na heart attack). Then, we can use this estimate to compute\ncardiac health under intervention on exercise. This procedure\nis explained in more detail in the Supplementary Material.\n2.2. Meta-learning\nMeta-learning refers to a broad range of approaches in which\naspects of the learning algorithm itself are learned from the\ndata. Many individual components of deep learning algorithms\nhave been successfully meta-learned, including the optimizer\n(Andrychowicz et al., 2016), initial parameter settings (Finn\net al., 2017), a metric space (Vinyals et al., 2016), and use of\nexternal memory (Santoro et al., 2016).\nFollowing the approach of (Duan et al., 2016; Wang et al.,\n2016), we parameterize the entire learning algorithm as a\nrecurrent neural network (RNN), and we train the weights of\nthe RNN with model-free reinforcement learning (RL). The\nRNN is trained on a broad distribution of problems which each\nrequire learning. When trained in this way, the RNN is able to\nimplement a learning algorithm capable of efficiently solving\nnovel learning problems in or near the training distribution (see\nthe Supplementary Material for further details).\nLearning the weights of the RNN by model-free RL can be\nthought of as the “outer loop” of learning. The outer loop shapes\nthe weights of the RNN into an “inner loop” learning algorithm.\nThis inner loop algorithm plays out in the activation dynamics\nof the RNN and can continue learning even when the weights\nof the network are frozen. The inner loop algorithm can also\nhave very different properties from the outer loop algorithm\nused to train it. For example, in previous work this approach\nwas used to negotiate the exploration-exploitation tradeoff in\nmulti-armed bandits (Duan et al., 2016) and learn algorithms\nwhich dynamically adjust their own learning rates (Wang et al.,\n2016; 2018). In the present work we explore the possibility of\nobtaining a causally-aware inner-loop learning algorithm.\n3. Task Setup and Agent Architecture\nIn our experiments, in each episode the agent interacted with\na different CBN G, defined over a set of N variables. The\nstructure of G was drawn randomly from the space of possible\nacyclic graphs under the constraints given in the next subsection.\nEach episode consisted of T steps, which were divided into\ntwo phases: an information phase and a quiz phase. The\ninformation phase, corresponding to the first T −1 steps,\nallowed the agent to collect information by interacting with\nor passively observing samples from G. The agent could\npotentially use this information to infer the connectivity and\nweights of G. The quiz phase, corresponding to the final step T,\nrequired the agent to exploit the causal knowledge it collected\nin the information phase, to select the node with the highest\nvalue under a random external intervention.\nCausal Graphs, Observations, and Actions\nWe generated all graphs on N = 5 nodes, with edges only\nin the upper triangular of the adjacency matrix (this guaran-\ntees that all the graphs obtained are acyclic). Edge weights\nwji were uniformly sampled from {−1,0,1}. This yielded\n3N(N−1)/2=59049 unique graphs. These can be divided into\nequivalence classes: sets of graphs that are structurally identical\nbut differ in the permutation of the node labels. Our held-out\ntest set consisted of 12 random graphs plus all other graphs in\nthe equivalence classes of these 12. Thus, all graphs in the test\nset had never been seen (and no equivalent graphs had been\nseen) during training. There were 408 total graphs in the test set.\nEach node, Xi∈R, was a Gaussian random variable. Parentless\nnodes had distribution N(µ=0.0,σ =0.1). A node Xi with\nparents pa(Xi) had conditional distribution p(Xi|pa(Xi)) =\nN(µ=P\njwjiXj,σ=0.1), where Xj ∈pa(Xi)3.\nA root node of G was always hidden (unobservable), to allow\nfor the presence of an unobserved confounder and the agent\ncould therefore only observe the values of the other 4 nodes.\nThe concatenated values of the nodes, vt, and and a one-hot\nvector indicating the external intervention during the quiz\nphase, mt, (explained below) formed the observation vector\nprovided to the agent at step t, ot=[vt,mt].\nIn both phases, on each step t, the agent could choose to take 1\nof 2(N−1) actions, the first N−1 of which were information\nactions, and the second of which were quiz actions. Both\ninformation and quiz actions were associated with selecting\nthe N−1 observable nodes, but could only be legally used in\nthe appropriate phase of the task. If used in the wrong phase,\na penalty was applied and the action produced no effect.\nInformation Phase. In the information phase, an information\naction at=i caused an intervention on the i-th node, setting the\nvalue of Xat =Xi =5 (the value 5 was chosen to be outside\nthe likely range of sampled observations, to facilitate learning\nthe causal graph). The node values vt were then obtained\nby sampling from p→Xi=5(X1:N\\i|Xi = 5) (where X1:N\\i\nindicates the set of all nodes except Xi), namely from the inter-\nvened CBN G→Xat=5 resulting from removing the incoming\nedges to Xat from G, and using the intervened value Xat =5\nfor conditioning its children’s values. If a quiz action was\nchosen during the information phase, it was ignored; namely,\nthe node values were sampled from G as if no intervention had\nbeen made, and the agent was given a penalty of rt =−10 in\norder to encourage it to take quiz actions during the quiz phase.\nThere was no other reward during the information phase.\n3We also tested graphs with non-linear causal effects, and larger\ngraphs of size N = 6 (see the Supplementary Material).\nThe default length an episode for fixed to be T = N = 5,\nresulting in this phase being fixed to a length of T −1=4. This\nwas because in the noise-free limit, a minimum of N −1=4\ninterventions, one on each observable node, are required in\ngeneral to resolve the causal structure and score perfectly on\nthe test phase.\nQuiz Phase. In the quiz phase, one non-hidden node Xj was\nselected at random to be intervened on by the environment.\nIts value was set to −5. We chose −5 to disallow the agent\nfrom memorizing the results of interventions in the information\nphase (which were fixed to +5) in order to perform well on the\nquiz phase. The agent was informed which node received this\nexternal intervention via the one-hot vector mt as part of the\nobservation from the the final pre-quiz phase timestep, T −1.\nFor steps t<T −1, mt was the zero vector. The agent’s reward\non this step was the sampled value of the node it selected\nduring the quiz phase. In other words, rT =Xi=XaT −(N−1)\nif the action selected was a quiz action (otherwise, the agent\nwas given a penalty of rT =−10).\nActive vs Random Agents. Our agents had to perform two\ndistinct tasks during the information phase: a) actively choose\nwhich nodes to set values on, and b) infer the CBN from its\nobservations. We refer to this setup as the “active” condition.\nTo better understand the role of (a), we include comparisons\nwith a baseline agent in the “random” condition, whose policy\nis to choose randomly which observable node it will set values\nfor, at each step of the information phase.\nTwo Kinds of Learning. The “inner loop” of learning (see\nSection 2.2) occurs within each episode where the agent is\nlearning from the evidence it gathers during the information\nphase in order to perform well in the quiz phase. The same\nagent then enters a new episode, where it has to repeat the task\non a different CBN. Test performance is reported on CBNs that\nthe agent has never previously seen, after all the weights of the\nRNN have been fixed. Hence, the only transfer from training\nto test (or the “outer loop” of learning) is the ability to discover\ncausal dependencies based on observations in the information\nphase, and to perform causal inference in the quiz phase.\nAgent Architecture and Training\nWe used a long short-term memory (LSTM) network (Hochre-\niter and Schmidhuber, 1997) (with 192 hidden units) that, at\neach time-step t, receives a concatenated vector containing\n[ot,at−1,rt−1] as input, where ot is the observation4, at−1 is\nthe previous action (as a one-hot vector) and rt−1 the reward\n(as a single real-value)5.\n4’Observation’ ot refers to the reinforcement learning term, i.e. the\ninput from the environment to the agent. This is distinct from observa-\ntions in the causal sense (referred to as observational data) i.e. samples\nfrom a causal structure where no interventions have been carried out.\n5These are both set to zero for the first step in an episode.\nThe outputs, calculated as linear projections of the LSTM’s\nhidden state, are a set of policy logits (with dimensionality\nequal to the number of available actions), plus a scalar baseline.\nThe policy logits are transformed by a softmax function, and\nthen sampled to give a selected action.\nLearning was by asynchronous advantage actor-critic (Mnih\net al., 2016). In this framework, the loss function consists\nof three terms – the policy gradient, the baseline cost and\nan entropy cost. The baseline cost was weighted by 0.05\nrelative to the policy gradient cost. The weighting of the\nentropy cost was annealed over the course of training from\n0.25 to 0. Optimization was done by RMSProp with ϵ=10−5,\nmomentum = 0.9 and decay = 0.95.\nLearning rate was\nannealed from 9×10−6 to 0, with a discount of 0.93. Unless\notherwise stated, training was done for 1 × 107 steps using\nbatched environments with a batch size of 1024.\nFor all experiments, after training, the agent was tested with\nthe learning rate set to zero, on a held-out test set.\n4. Experiments\nOur three experiments (observational, interventional, and\ncounterfactual data settings) differed in the properties of the\nvt that was observed by the agent during the information\nphase, and thereby limited the extent of causal reasoning\npossible within each data setting. Our measure of performance\nis the reward earned in the quiz phase for held-out CBNs.\nChoosing a random node in the quiz phase results in a reward\nof −5/4 = −1.25, since one node (the externally intervened\nnode) always has value −5 and the others have on average 0\nvalue. By learning to simply avoid the externally intervened\nnode, the agent can earn on average 0 reward. Consistently\npicking the node with the highest value in the quiz phase\nrequires the agent to perform causal reasoning. For each agent,\nwe took the average reward earned across 1632 episodes (408\nheld-out test CBNs, with 4 possible external interventions). We\ntrained 8 copies of each agent and reported the average reward\nearned by these, with error bars showing 95% confidence\nintervals. The p values based on the appropriate t-test are\nprovided in cases where the compared values are close.\n4.1. Experiment 1: Observational Setting\nIn Experiment 1, the agent could neither intervene to set the\nvalue of variables in the environment, nor observe any external\ninterventions. In other words, it only received observations\nfrom G, not G→Xj (where Xj is a node that has been intervened\non). This limits the extent of causal inference possible. In this\nexperiment, we tested five agents, four of which were learned:\n“Observational”, “Long Observational”, “Active Conditional”,\n“Random Conditional”,\nand the “Optimal Associative\nBaseline” (not learned). We also ran two other standard RL\nbaselines—see the Supplementary Material for details.\n-3.3\n-0.8\n-1.4\n-5.0\n-5.0\n0.0\n-2.5\n-2.5\nCond. (Orphan)\nAssoc.(Orphan)\nCond. (Parent)\nAssoc. (Parent)\n   Active-Cond.\nOptimal-Assoc.\nObs.\nLong-Obs.\nAvg. Reward\n0.0\n1.0\nAvg. Reward\n0.0\n1.0\n2.0\n(a)\n(b)\n(c)\nOptimal-Assoc. Agent\nActive-Cond. Agent\nFigure 2. Experiment 1. Agents do cause-effect reasoning from observational data. a) Average reward earned by the agents tested in this\nexperiment. See main text for details. b) Performance split by the presence or absence of at least one parent (Parent and Orphan respectively) on\nthe externally intervened node. c) Quiz phase for a test CBN. Green (red) edges indicate a weight of +1 (−1). Black represents the intervened\nnode, green (red) nodes indicate a positive (negative) value at that node, white indicates a zero value. The blue circles indicate the agent’s choice.\nLeft panel: The undirected version of G and the nodes taking the mean values prescribed by p(X1:N\\j|Xj =−5), including backward inference\nto the intervened node’s parent. We see that the Optimal Associative Baseline’s choice is consistent with maximizing these (incorrect) node values.\nRight panel: G→Xj=−5 and the nodes taking the mean values prescribed by p→Xj=−5(X1:N\\j|Xj =−5). We see that the Active-Conditional\nAgent’s choice is consistent with maximizing these (correct) node values.\nObservational Agents: In the information phase, the actions\nof the agent were ignored6, and the observational agent always\nreceived the values of the observable nodes as sampled from\nthe joint distribution associated with G. In addition to the\ndefault T =5 episode length, we also trained this agent with\n4× longer episode length (Long Observational Agent), to\nmeasure performance increase with more observational data.\nConditional Agents: The information phase actions corre-\nsponded to observing a world in which the selected node Xj\nis equal to Xj =5, and the remaining nodes are sampled from\nthe conditional distribution p(X1:N\\j|Xj = 5). This differs\nfrom intervening on the variable Xj by setting it to the value\nXj =5, since here we take a conditional sample from G rather\nthan from G→Xj=5 (or from p→Xj=5(X1:N\\j|Xj = 5)), and\ninference about the corresponding node’s parents is possible.\nTherefore, this agent still has access to only observational\ndata, as with the observational agents. However, on average\nit receives more diagnostic information about the relation\nbetween the random variables in G, since it can observe\nsamples where a node takes a value far outside the likely range\nof sampled observations. We run active and random versions\nof this agent as described in Section 3.\nOptimal Associative Baseline: This baseline receives the true\njoint distribution p(X1:N) implied by the CBN in that episode\nand therefore has full knowledge of the correlation structure of\nthe environment7. It can therefore do exact associative reason-\ning of the form p(Xj|Xi=x), but cannot do any cause-effect\nreasoning of the form p→Xi=x(Xj|Xi=x). In the quiz phase,\nthis baseline chooses the node that has the maximum value\naccording to the true p(Xj|Xi=x) in that episode, where Xi\nis the node externally intervened upon, and x = −5. This is\nthe best possible performance using only associative reasoning.\n6These agents also did not receive the out-of-phase action penalties\nduring the information phase since their actions are totally ignored.\n7Notice that the agent does not know the graphical structure, i.e. it\ndoes not know which nodes are parents of which other nodes.\nResults\nWe focus on two questions in this experiment.\n(i) Most centrally, do the agents learn to perform cause-effect\nreasoning using observational data? The Optimal Associative\nBaseline tracks the greatest reward that can be achieved using\nonly knowledge of correlations – without causal knowledge.\nCompared to this baseline, the Active-Conditional Agent\n(which is allowed to select highly informative observations)\nearns significantly more reward (p = 6×10−5, Fig. 2a). To\nbetter understand why the agent outperforms the associative\nbaseline, we divided episodes according to whether or not the\nnode that was intervened on in the quiz phase has a parent\n(Fig. 2b). If the intervened node Xj has no parents, then\nG=G→Xj, and cause-effect reasoning has no advantage over\nassociative reasoning. Indeed, the Active-Conditional Agent\nperforms better than the Optimal Associative Baseline only\nwhen the intervened node has parents (hatched bars in Fig. 2b).\nWe also show the quiz phase for an example test CBN in Fig. 2c,\nwhere the Optimal Associative Baseline chooses according to\nthe node values predicted by G, whereas the Active-Conditional\nAgent chooses according the node values predicted by G→Xj.\nRandom\nActive\nFigure 4. Active and Random Condi-\ntional Agents\nThese analyses allow\nus to conclude that\nthe agent can perform\ncause-effect reasoning,\nusing\nobservational\ndata alone – analogous\nto the formal use of\ndo-calculus.\n(ii) Do\nthe agents learn to select useful observations? We find that the\nActive-Conditional Agent’s performance is significantly greater\nthan the Random-Conditional Agent (Fig. 4). This indicates\nthat the agent has learned to choose useful data to observe.\nFor completeness we also included agents that received uncon-\nditional observations from G, i.e. the Observational Agents\n(’Obs’ and ’Long-Obs’ in Fig. 2a). As expected, these agents\nActive-Int.\nOptimal Assoc.\nOptimal C-E\nActive-Cond.\n0.0\n1.0\nCond. (Unconf.)\nInt. (Unconf.)\nCond. (Conf.)\nInt. (Conf.)\n0.0\n1.1\n(a)\n(b)\n(c)\nActive-Cond. Agent\nActive-Int. Agent\nAvg. Reward\nAvg. Reward\nFigure 3. Experiment 2. Agents do cause-effect reasoning from interventional data. a) Average reward earned by the agents tested in this\nexperiment. See main text for details. b) Performance split by the presence or absence of unobserved confounders (abbreviated as Conf. and\nUnconf. respectively) on the externally intervened node. c) Quiz phase for a test CBN. See Fig. 2 for a legend. Here, the left panel shows the\nfull G and the nodes taking the mean values prescribed by p(X1:N\\j|Xj = −5). We see that the Active-Cond Agent’s choice is consistent\nwith choosing based on these (incorrect) node values. The right panel shows G→Xj=−5 and the nodes taking the mean values prescribed by\np→Xj=−5(X1:N\\j|Xj =−5). We see that the Active-Int. Agent’s choice is consistent with maximizing on these (correct) node value.\nperformed worse than the Active-Conditional Agent, because\nthey received less diagnostic information during the information\nphase. However, they were still able to acquire some informa-\ntion from unconditional samples, and also made use of the\nincreased information available from longer episodes.\n4.2. Experiment 2: Interventional Setting\nIn Experiment 2, the agent receives interventional data in the in-\nformation phase – it can choose to intervene on any observable\nnode, Xj, and observe a sample from the resulting graph G→Xj.\nAs discussed in Section 2.1, access to interventional data per-\nmits cause-effect reasoning even in the presence of unobserved\nconfounders, a feat which is in general impossible with access\nonly to observational data. In this experiment, we test three new\nagents, two of which were learned: “Active Interventional”,\n“Random Interventional”, and “Optimal Cause-Effect Baseline”\n(not learned).\nInterventional Agents: The information phase actions corre-\nspond to performing an intervention on the selected node Xj\nand sampling from G→Xj (see Section 3 for details). We run\nactive and random versions of this agent as described in Sec-\ntion 3.\nOptimal Cause-Effect Baseline: This baseline receives the\ntrue CBN, G. In the quiz phase, it chooses the node that has\nthe maximum value according to G→Xj, where Xj is the node\nexternally intervened upon. This is the maximum possible\nperformance on this task.\nResults\nWe focus on two questions in this experiment.\n(i) Do our agents learn to perform cause-effect reasoning from\ninterventional data? The Active-Interventional Agent’s per-\nformance is marginally better than the Active-Conditional\nAgent (p = 0.06, Fig. 3a).\nTo better highlight the cru-\ncial role of interventional data in doing cause-effect reason-\ning, we compare the agent performances split by whether\nthe node that was intervened on in the quiz phase of the\nepisode had unobserved confounders with other variables in\nthe graph (Fig. 3b). In confounded cases, as described in\nSection 2.1, cause-effect reasoning is impossible with only\nobservational data. We see that the performance of the Active-\nInterventional Agent is significantly higher (p = 10−5) than\nthat of the Active-Conditional Agent in the confounded cases.\nThis indicates that the Active-Interventional Agent (that had\naccess to interventional data) is able to perform additional\ncause-effect reasoning in the presence of confounders that\nthe Active-Conditional Agent (that had access to only ob-\nservational data) cannot do. This is highlighted by Fig. 3c,\nwhich shows the quiz phase for an example CBN, where the\nActive-Conditional Agent is unable to resolve the unobserved\nconfounder, but the Active-Interventional Agent is able to.\nRandom\nActive\nFigure 5. Active and Random Interven-\ntional Agents\n(ii) Do our agents\nlearn to make useful\ninterventions?\nThe\nActive-Interventional\nAgent’s performance\nis significantly greater\nthan\nthe\nRandom-\nInterventional Agent’s\n(Fig. 5). This indicates that when the agent is allowed to choose\nits actions, it makes tailored, non-random choices about the\ninterventions it makes and the data it wants to observe.\n4.3. Experiment 3: Counterfactual Setting\nIn Experiment 3, the agent was again allowed to make in-\nterventions as in Experiment 2, but in this case the quiz\nphase task entailed answering a counterfactual question. We\nexplain here what a counterfactual question in this domain\nlooks like. Assume Xi = P\nj wjiXj + ϵi where ϵi is dis-\ntributed as N(0.0, 0.1) (giving the conditional distribution\np(Xi|pa(Xi))=N(P\njwjiXj,0.1) as described in Section 3).\nAfter observing the nodes X2:N (X1 is hidden) in the CBN in\none sample, we can infer this latent randomness ϵi for each\nobservable node Xi (i.e. abduction as described in the Sup-\nActive-Int.\n0.0\n1.0\n2.0\nActive-CF\nOptimal C-E\n0.0\n1.0\nCF (Dist.)\nInt. (Dist.)\nCF (Deg.)\nInt. (Deg.)\n(a)\n(b)\n(c)\nPassive-Int. Agent\nPassive-CF Agent\nAvg. Reward\nAvg. Reward\nOptimal CF\nFigure 6. Experiment 3. Agents do counterfactual reasoning. a) Average reward earned by the agents tested in this experiment. See main\ntext for details. b) Performance split by if the maximum node value in the quiz phase is degenerate (Deg.) or distinct (Dist.). c) Quiz phase\nfor an example test-CBN. See Fig. 2 for a legend. Here, the left panel shows G→Xj=−5 and the nodes taking the mean values prescribed by\np→Xj=−5(X1:N\\j|Xj =−5). We see that the Active-Int. Agent’s choice is consistent with maximizing on these node values, where it makes a\nrandom choice between two nodes with the same value. The right panel panel shows G→Xj=−5 and the nodes taking the exact values prescribed\nby the means of p→Xj=−5(X1:N\\j|Xj = −5), combined with the specific randomness inferred from the previous time step. As a result of\naccounting for the randomness, the two previously degenerate maximum values are now distinct. We see that the Active-CF. agent’s choice is\nconsistent with maximizing on these node values.\nplementary Material) and answer counterfactual questions like\n“What would the values of the nodes be, had Xi instead taken\non a different value than what we observed?”, for any of the ob-\nservable nodes Xi. We test three new agents, two of which are\nlearned: “Active Counterfactual”, “Random Counterfactual”,\nand “Optimal Counterfactual Baseline” (not learned).\nCounterfactual Agents: This agent is exactly analogous to the\nInterventional agent, with the addition that the latent random-\nness in the last information phase step t = T −1 (where say\nsome Xp=+5), is stored and the same randomness is used in\nthe quiz phase step t=T (where say some Xf =−5). While\nthe question our agents have had to answer correctly so far in\norder to maximize their reward in the quiz phase was “Which\nof the nodes X2:N will have the highest value when Xf is set\nto −5?”, in this setting, we ask “Which of the nodes X2:N\nwould have had the highest value in the last step of the infor-\nmation phase, if instead of having the intervention Xp =+5,\nwe had the intervention Xf =−5?”. We run active and random\nversions of this agent as described in Section 3.\nOptimal Counterfactual Baseline: This baseline receives the\ntrue CBN and does exact abduction of the latent randomness\nbased on observations from the penultimate step of the infor-\nmation phase, and combines this correctly with the appropriate\ninterventional inference on the true CBN in the quiz phase.\nResults\nWe focus on two key questions in this experiment.\n(i) Do our agents learn to do counterfactual inference? The\nActive-Counterfactual Agent achieves higher reward than the\nActive-Interventional Agent (p = 2×10−5).\nTo evaluate\nwhether this difference results from the agent’s use of abduction\n(see the Supplementary Material for details), we split the test\nset into two groups, depending on whether or not the decision\nfor which node will have the highest value in the quiz phase is\naffected by the latent randomness, i.e. whether or not the node\nwith the maximum value in the quiz phase changes if the noise\nis resampled. This is most prevalent in cases where the maxi-\nmum expected reward is degenerate, i.e. where several nodes\ngive the same maximum reward (denoted by hatched bars in\nFigure 6b). Here, agents with no access to the randomness have\nno basis for choosing one over the other, but different noise sam-\nples can give rise to significant differences in the actual values\nthat these degenerate nodes have. We see indeed that there is no\ndifference in the rewards received by the Active-Counterfactual\nand Active-Interventional Agents in the cases where the max-\nimum values are distinct, however the Active-Counterfactual\nAgent significantly outperforms the Active-Interventional\nAgent in cases where there are degenerate maximum values.\nRandom\nActive\nFigure 7. Active and Random Counter-\nfactual Agents\n(ii) Do our agents\nlearn to make useful\ninterventions in the\nservice\nof\na\ncoun-\nterfactual task?\nThe\nActive-Counterfactual\nAgent’s performance\nis significantly greater\nthan the Random-Counterfactual Agent’s (Fig. 5).\nThis\nindicates that when the agent is allowed to choose its actions, it\nmakes tailored, non-random choices about the interventions it\nmakes and the data it wants to observe – even in the service of\na counterfactual objective.\n5. Summary of Results\nIn this paper we used the meta-learning to train a recurrent net-\nwork – using model-free reinforcement learning – to implement\nan algorithm capable of causal reasoning. Agents trained in\nthis manner performed causal reasoning in three data settings:\nobservational, interventional, and counterfactual. Crucially, our\napproach did not require explicit encoding of formal principles\nof causal inference. Rather, by optimizing an agent to perform\na task that depended on causal structure, the agent learned im-\nplicit strategies to generate and use different kinds of available\ndata for causal reasoning, including drawing causal inferences\nfrom passive observation, actively intervening, and making\ncounterfactual predictions, all on held out causal CBNs that the\nagents had never previously seen.\nA consistent result in all three data settings was that our agents\nlearned to perform good experiment design or active learning.\nThat is, they learned a non-random data collection policy where\nthey actively chose which nodes to intervene (or condition) on\nin the information phase, and thus could control the kinds of\ndata they saw, leading to higher performance in the quiz phase\nthan that from an agent with a random data collection policy.\nBelow, we summarize the other keys results from each of the\nthree experiments.\nIn Section 4.1 and Fig. 2, we showed that agents learned to per-\nform do-calculus. In Fig. 2a we saw that, the trained agent with\naccess to only observational data received more reward than the\nhighest possible reward achievable without causal knowledge.\nWe further observed in Fig. 2b that this performance increase\noccurred selectively in cases where do-calculus made a predic-\ntion distinguishable from the predictions based on correlations –\ni.e. where the externally intervened node had a parent, meaning\nthat the intervention resulted in a different graph.\nIn Section 4.2 and Fig. 3, we showed that agents learned to\nresolve unobserved confounders using interventions (which\nis impossible with only observational data). In Fig. 3b we\nsaw that agents with access to interventional data performed\nbetter than agents with access to only observational data only in\ncases where the intervened node shared an unobserved parent\n(a confounder) with other variables in the graph.\nIn Section 4.3 and Fig. 6, we showed that agents learned to\nuse counterfactuals. In Fig. 6a we saw that agents with ad-\nditional access to the specific randomness in the test phase\nperformed better than agents with access to only interventional\ndata. In Fig. 6b, we found that the increased performance was\nobserved only in cases where the maximum mean value in the\ngraph was degenerate, and optimal choice was affected by the\nlatent randomness – i.e. where multiple nodes had the same\nvalue on average and the specific randomness could be used to\ndistinguish their actual values in that specific case.\n6. Discussion and Future Work\nTo our knowledge, this is the first direct demonstration\nthat causal reasoning can arise out of model-free reinforce-\nment learning. Our paper lays the groundwork for a meta-\nreinforcement learning approach to causal reasoning that poten-\ntially offers several advantages over formal methods for causal\ninference in complex real world settings.\nFirst, traditional formal approaches usually decouple the prob-\nlems of causal induction (inferring the structure of the underly-\ning model from data) and causal inference (estimating causal\neffects based on a known model). Despite advances in both (Or-\ntega and Stocker, 2015; Lattimore et al., 2016; Bramley et al.,\n2017; Forney et al., 2017; Sen et al., 2017; Parida et al., 2018),\ninducing models is expensive and typically requires simplifying\nassumptions. When induction and inference are decoupled,\nthe assumptions used at the induction step are not fully opti-\nmized for the inference that will be performed downstream. By\ncontrast, our model learns to perform induction and inference\nend-to-end, and can potentially find representations of causal\nstructure best tuned for the required causal inferences. Meta-\nlearning can sometimes even leverage structure in the problem\ndomain that may be too complex to specify when inducing a\nmodel (Duan et al., 2016; Santoro et al., 2016; Wang et al.,\n2016), allowing more efficient and accurate causal reasoning\nthan would be possible without representing and exploiting this\nstructure.\nSecond, since both the induction and inference steps are costly,\nformal methods can be very slow at run-time when faced with\na new query. Meta-learning shifts most of the compute burden\nfrom inference time to training time. This is advantageous when\ntraining time is ample but fast answers are needed at run-time.\nFinally, by using an RL framework, our agents can learn to take\nactions that produce useful information—i.e. perform active\nlearning. Our agents’ active intervention policy performed\nsignificantly better than a random intervention policy, which\ndemonstrates the promise of learning an experimentation policy\nend-to-end with the causal reasoning built on the resulting\nobservations.\nOur work focused on a simple domain because our aim was to\ntest in the most direct way possible whether causal reasoning\ncan emerge from meta-learning. Follow-up work should focus\non scaling up our approach to larger environments, with more\ncomplex causal structure and a more diverse range of tasks. This\nopens up possibilities for agents that perform active experiments\nto support structured exploration in RL, and learning optimal\nexperiment design in complex domains where large numbers of\nrandom interventions are prohibitive. The results here are a first\nstep in this direction, obtained using relatively standard deep\nRL components – our approach will likely benefit from more\nadvanced architectures (e.g. Hester et al., 2017; Hessel et al.,\n2018; Espeholt et al., 2018) that allow us to train on longer more\ncomplex episodes, as well as models which are more explicitly\ncompositional (e.g. Andreas et al., 2016; Battaglia et al., 2018)\nor have richer semantics (e.g. Ganin et al., 2018) that can more\nexplicitly leverage symmetries in the environment and improve\ngeneralization and training efficiency.\n7. Acknowledgements\nThe authors would like to thank the following people for helpful\ndiscussions and comments: Neil Rabinowitz, Neil Bramley,\nTobias Gerstenberg, Andrea Tacchetti, Victor Bapst, Samuel\nGershman.\nReferences\nJ. Andreas, M. Rohrbach, T. Darrell, and D. Klein. Neural\nmodule networks. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 39–48,\n2016.\nM. Andrychowicz, M. Denil, S. Gomez, M. W. Hoffman,\nD. Pfau, T. Schaul, B. Shillingford, and N. De Freitas. Learn-\ning to learn by gradient descent by gradient descent. In\nAdvances in Neural Information Processing Systems, pages\n3981–3989, 2016.\nP. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez,\nV. Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo,\nA. Santoro, R. Faulkner, et al.\nRelational inductive bi-\nases, deep learning, and graph networks. arXiv preprint\narXiv:1806.01261, 2018.\nA. P. Blaisdell, K. Sawa, K. J. Leising, and M. R. Waldmann.\nCausal reasoning in rats. Science, 311(5763):1020–1022,\n2006.\nN. R. Bramley, P. Dayan, T. L. Griffiths, and D. A. Lagnado.\nFormalizing neuraths ship: Approximate algorithms for on-\nline causal learning. Psychological review, 124(3):301, 2017.\nK. Cho, B. Van Merri¨enboer, C. Gulcehre, D. Bahdanau,\nF. Bougares, H. Schwenk, and Y. Bengio. Learning phrase\nrepresentations using rnn encoder-decoder for statistical ma-\nchine translation. arXiv preprint arXiv:1406.1078, 2014.\nP. Dawid. Fundamentals of statistical causality. Technical\nreport, University Colledge London, 2007.\nY. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever,\nand P. Abbeel. rl2: Fast reinforcement learning via slow\nreinforcement learning. arXiv preprint arXiv:1611.02779,\n2016.\nL. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih,\nT. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, et al. Im-\npala: Scalable distributed deep-rl with importance weighted\nactor-learner architectures. arXiv preprint arXiv:1802.01561,\n2018.\nC. Finn, P. Abbeel, and S. Levine.\nModel-agnostic meta-\nlearning for fast adaptation of deep networks. arXiv preprint\narXiv:1703.03400, 2017.\nA. Forney, J. Pearl, and E. Bareinboim. Counterfactual data-\nfusion for online reinforcement learners. In International\nConference on Machine Learning, pages 1156–1164, 2017.\nY. Ganin, T. Kulkarni, I. Babuschkin, S. M. Eslami,\nand O. Vinyals.\nSynthesizing programs for images\nusing reinforced adversarial learning.\narXiv preprint\narXiv:1804.01118, 2018.\nN. D. Goodman, T. D. Ullman, and J. B. Tenenbaum. Learning\na theory of causality. Psychological review, 118(1):110,\n2011.\nA. Gopnik, D. M. Sobel, L. E. Schulz, and C. Glymour. Causal\nlearning mechanisms in very young children: two-, three-,\nand four-year-olds infer causal relations from patterns of\nvariation and covariation. Developmental psychology, 37(5):\n620, 2001.\nA. Gopnik, C. Glymour, D. M. Sobel, L. E. Schulz, T. Kushnir,\nand D. Danks. A theory of causal learning in children: causal\nmaps and bayes nets. Psychological review, 111(1):3, 2004.\nE. Grant, C. Finn, S. Levine, T. Darrell, and T. Griffiths. Re-\ncasting gradient-based meta-learning as hierarchical bayes.\narXiv preprint arXiv:1801.08930, 2018.\nM. Hessel, H. Soyer, L. Espeholt, W. Czarnecki, S. Schmitt,\nand H. van Hasselt. Multi-task deep reinforcement learning\nwith popart. arXiv preprint arXiv:1809.04474, 2018.\nT. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul,\nB. Piot, D. Horgan, J. Quan, A. Sendonaris, G. Dulac-Arnold,\net al. Deep q-learning from demonstrations. arXiv preprint\narXiv:1704.03732, 2017.\nS. Hochreiter and J. Schmidhuber. Long short-term memory.\nNeural computation, 9(8):1735–1780, 1997.\nA. Hyttinen, F. Eberhardt, and P. O. Hoyer. Experiment selec-\ntion for causal discovery. The Journal of Machine Learning\nResearch, 14(1):3041–3071, 2013.\nA. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classifi-\ncation with deep convolutional neural networks. In Advances\nin neural information processing systems, pages 1097–1105,\n2012.\nD. A. Lagnado, T. Gerstenberg, and R. Zultan. Causal re-\nsponsibility and counterfactuals. Cognitive science, 37(6):\n1036–1073, 2013.\nF. Lattimore, T. Lattimore, and M. D. Reid. Causal bandits:\nLearning good interventions via causal inference. In Ad-\nvances in Neural Information Processing Systems, pages\n1181–1189, 2016.\nA. M. Leslie. The perception of causality in infants. Perception,\n11(2):173–186, 1982.\nV. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lilli-\ncrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asyn-\nchronous methods for deep reinforcement learning. CoRR,\nabs/1602.01783, 2016.\nURL http://arxiv.org/\nabs/1602.01783.\nP. A. Ortega and D. D. Lee A. A. Stocker. Causal reasoning in\na prediction task with hidden causes. 37th Annual Cognitive\nScience Society Meeting CogSci, 2015.\nP. K. Parida, T. Marwala, and S. Chakraverty. A multivariate\nadditive noise model for complete causal discovery. Neural\nNetworks, 103:44–54, 2018.\nJ. Pearl. Causality: Models, Reasoning, and Inference. Cam-\nbridge University Press, 2000.\nJ. Pearl, M. Glymour, and N. P. Jewell. Causal inference in\nstatistics: a primer. John Wiley & Sons, 2016.\nA. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and T. Lil-\nlicrap. Meta-learning with memory-augmented neural net-\nworks. In International conference on machine learning,\npages 1842–1850, 2016.\nR. Sen, K. Shanmugam, A. G. Dimakis, and S. Shakkottai.\nIdentifying best interventions through online importance\nsampling. arXiv preprint arXiv:1701.02789, 2017.\nK. Shanmugam, M. Kocaoglu, A. G. Dimakis, and S. Vish-\nwanath. Learning causal graphs with small interventions. In\nAdvances in Neural Information Processing Systems, pages\n3195–3203, 2015.\nP. Spirtes, C. N. Glymour, R. Scheines, D. Heckerman, C. Meek,\nG. Cooper, and T. Richardson. Causation, prediction, and\nsearch. MIT press, 2000.\nO. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra, et al. Match-\ning networks for one shot learning. In Advances in Neural\nInformation Processing Systems, pages 3630–3638, 2016.\nJ. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo,\nR. Munos, C. Blundell, D. Kumaran, and M. Botvinick.\nLearning to reinforcement learn. CoRR, abs/1611.05763,\n2016. URL http://arxiv.org/abs/1611.05763.\nJ. X. Wang, Z. Kurth-Nelson, D. Kumaran, D. Tirumala,\nH. Soyer, J. Z. Leibo, D. Hassabis, and M. Botvinick. Pre-\nfrontal cortex as a meta-reinforcement learning system. Na-\nture neuroscience, 21(6):860, 2018.\nSupplementary to Causal Reasoning from Meta-reinforcement Learning\n1. Additional Baselines\n6\n4\n2\n0\n2\n4\n6\nReward Earned\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nPercentage of DAGs\nQ-total\nQ-episode\nOptimal\nFigure 1: Reward distribution\nWe\ncan\nalso\ncom-\npare the performance\nof\nthese\nagents\nto\ntwo standard model-\nfree RL baselines. The\nQ-total Agent learns a\nQ-value for each ac-\ntion across all steps\nfor all the episodes.\nThe Q-episode Agent\nlearns a Q-value for\neach action conditioned on the input at each time step\n[ot,at−1,rt−1], but with no LSTM memory to store previous\nactions and observations. Since the relationship between action\nand reward is random between episodes, Q-total was equivalent\nto selecting actions randomly, resulting in a considerably nega-\ntive reward (−1.247±2.940). The Q-episode agent essentially\nmakes sure to not choose the arm that is indicated by mt to be\nthe external intervention (which is assured to be equal to −5),\nand essentially chooses randomly otherwise, giving a reward\nclose to 0 (0.080±2.077).\n2. Formalism for Memory-based Meta-learning\nConsider a distribution D over Markov Decision Processes\n(MDPs). We train an agent with memory (in our case an\nRNN-based agent) on this distribution. In each episode, we\nsample a task m ∼D. At each step t within an episode,\nthe agent sees an observation ot, executes an action at, and\nreceives a reward rt. Both at−1 and rt−1 are given as ad-\nditional inputs to the network. Thus, via the recurrence of\nthe network, each action is a function of the entire trajectory\nHt = {o0,a0,r0,...,ot−1,at−1,rt−1,ot} of the episode. Be-\ncause this function is parameterized by the neural network, its\ncomplexity is limited only by the size of the network.\n3. Abduction-Action-Prediction Method for\nCounterfactual Reasoning\nPearl et al. (2016)’s “abduction-action-prediction” method pre-\nscribes one way to answer counterfactual queries of the type\n“What would the cardiac health of individual i have been had\nshe done more exercise?”, by estimating the specific latent ran-\ndomness in the unobserved makeup of the individual and by\ntransferring it to the counterfactual world. Assume, for exam-\nple, the following model for G of Section 2.1: E=wAEA+η,\nH =wAHA+wEHE+ϵ, where the weights wij represent the\nknown causal effects in G and ϵ and η are terms of (e.g.) Gaus-\nsian noise that represent the latent randomness in the makeup\nof each individual1. Suppose that for individual i we observe:\nA=ai, E=ei, H =hi. We can answer the counterfactual ques-\ntion of “What if individual i had done more exercise, i.e. E=e′,\ninstead?” by: a) Abduction: estimate the individual’s specific\nmakeup with ϵi = hi−wAHai−wEHei, b) Action: set E to\nmore exercise e′, c) Prediction: predict a new value for cardiac\nhealth as h′=wAHai+wEHe′+ϵi.\n4. Additional Experiments\nThe purview of the previous experiments was to show a proof\nof concept on a simple tractable system, demonstrating that\ncausal induction and inference can be learned and implemented\nvia a meta-learned agent. In the following, we scale up to more\ncomplex systems in two new experiments.\n4.1. Experiment 4: Non-linear Causal Graphs\n0.0\n2.0\nAvg. Reward\n(a)\n(b)\nActive-Int.\nObs.\nLong-Obs.\n0.0\n2.0\nAvg. Reward\nRandom-Int.\nActive-Int.\nOptimal C-E\nFigure 2: Results for non-linear graphs. (a) Comparing agent\nperformances with different data. (b) Comparing information\nphase intervention policies.\nIn this experiment, we generalize some of our results to non-\nlinear, non-Gaussian causal graphs which are more typical of\nreal-world causal graphs and to demonstrate that our results\nhold without loss of generality on such systems.\nHere we investigate causal Bayesian networks (CBNs) with\na quadratic dependence on the parents by changing the con-\nditional distribution to p(Xi|pa(Xi)) = N( 1\nNi\nP\nj wji(Xj +\nX2\nj ),σ). Here, although each node is normally distributed\ngiven its parents, the joint distribution is not multivariate Gaus-\n1These are zero in expectation, so without access to their value for\nan individual we simply use G: E =wAEA, H =wAHA+wEHE\nto make causal predictions.\n1\narXiv:1901.08162v1  [cs.LG]  23 Jan 2019\nsian due to the non-linearity in how the means are determined.\nWe find that the Long-Observational Agent achieves more re-\nward than the Observational Agent indicating that the agent is\nin fact learning the statistical dependencies between the nodes,\nwithin an episode2. We also find that the Active-Interventional\nAgent achieves reward well above the best agent with access\nto only observational data (Long-Observational in this case)\nindicating an ability to reason from interventions. We also see\nthat the Active-Interventional Agent performs better than the\nRandom-Interventional Agent indicating an ability to choose\ninformative interventions.\n4.2. Experiment 5: Larger Causal Graphs\n0.0\n1.0\nAvg. Reward\nLong-Obs.\nObs.\nActive-Cond.\nActive-Int.\nActive-CF\n(a)\n(b)\n0.0\n1.0\nAvg. Reward\nRandom-Int.\nActive-Int.\nOptimal C-E\nFigure 3: Results for N = 6 graphs. (a) Comparing agent\nperformances with different data. (b) Comparing information\nphase intervention policies.\nIn this experiment we scaled up to larger graphs with N = 6\nnodes, which afforded considerably more unique CBNs than\nwith N =5 (1.4×107 vs 5.9×104). As shown in Fig. 3a, we\nfind the same pattern of behavior noted in the main text where\nthe rewards earned are ordered such that Observational agent\n< Active-Conditional agent < Active-Interventional agent <\nActive-Counterfactual agent. We see additionally in Fig. 3b that\nthe Active-Interventional agent performs significantly better\nthan the baseline Random-Interventional agent, indicating an\nability to choose non-random, informative interventions.\n5. Causal Bayesian Networks\nBy combining graph theory and probability theory, the causal\nBayesian network framework provides us with a graphical\ntool to formalize and test different levels of causal reasoning.\nThis section introduces the main definitions underlying this\nframework and explains how to visually test for statistical in-\ndependence (Pearl, 1988; Bishop, 2006; Koller and Friedman,\n2009; Barber, 2012; Murphy, 2012).\nA graph is a collection of nodes and links connecting pairs of\nnodes. The links may be directed or undirected, giving rise to\ndirected or undirected graphs respectively.\nA path from node Xi to node Xj is a sequence of linked nodes\nstarting at Xi and ending at Xj. A directed path is a path\nwhose links are directed and pointing from preceding towards\nfollowing nodes in the sequence.\n2The conditional distribution p(X1:N\\j|Xj = 5), and therefore\nConditional Agents, were non-trivial to calculate for the quadratic\ncase.\nX2\nX1\nX3\nX4\n(a)\nX2\nX1\nX3\nX4\n(b)\nFigure 4: (a): Directed acyclic graph. The node X3 is a collider\non the path X1 →X3 ←X2 and a non-collider on the path\nX2→X3→X4. (b): Cyclic graph obtained from (a) by adding\na link from X4 to X1.\nA directed acyclic graph is a directed graph with no directed\npaths starting and ending at the same node. For example, the\ndirected graph in Fig. 4(a) is acyclic. The addition of a link\nfrom X4 to X1 gives rise to a cyclic graph (Fig. 4(b)).\nA node Xi with a directed link to Xj is called parent of Xj.\nIn this case, Xj is called child of Xi.\nA node is a collider on a specified path if it has (at least) two\nparents on that path. Notice that a node can be a collider on\na path and a non-collider on another path. For example, in\nFig. 4(a) X3 is a collider on the path X1 →X3 ←X2 and a\nnon-collider on the path X2→X3→X4.\nA node Xi is an ancestor of a node Xj if there exists a directed\npath from Xi to Xj. In this case, Xj is a descendant of Xi.\nA graphical model is a graph in which nodes represent random\nvariables and links express statistical relationships between the\nvariables.\nA Bayesian network is a directed acyclic graphical model in\nwhich each node Xi is associated with the conditional distri-\nbution p(Xi|pa(Xi)), where pa(Xi) indicates the parents of\nXi. The joint distribution of all nodes in the graph, p(X1:N),\nis given by the product of all conditional distributions, i.e.\np(X1:N)=QN\ni=1p(Xi|pa(Xi)).\nWhen equipped with causal semantic, namely when describing\nthe process underlying the data generation, a Bayesian net-\nwork expresses both causal and statistical relationships among\nrandom variables—in such a case the network is called causal.\nAssessing statistical independence in Bayesian networks.\nGiven the sets of random variables X,Y and Z, X and Y\nare statistically independent given Z if all paths from any ele-\nment of X to any element of Y are closed (or blocked). A path\nis closed if at least one of the following conditions is satisfied:\n(i) There is a non-collider on the path which belongs to the\nconditioning set Z.\n(ii) There is a collider on the path such that neither the collider\nnor any of its descendants belong to Z.\nReferences\nD. Barber. Bayesian reasoning and machine learning. Cam-\nbridge University Press, 2012.\nC. M. Bishop. Pattern Recognition and Machine Learning.\nSpringer, 2006.\nD. Koller and N. Friedman. Probabilistic Graphical Models:\nPrinciples and Techniques. MIT Press, 2009.\nK. P. Murphy. Machine Learning: a Probabilistic Perspective.\nMIT Press, 2012.\nJ. Pearl. Probabilistic Reasoning in Intelligent Systems: Net-\nworks of Plausible Inference. Morgan Kaufmann, 1988.\nJ. Pearl, M. Glymour, and N. P. Jewell. Causal inference in\nstatistics: a primer. John Wiley & Sons, 2016.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2019-01-23",
  "updated": "2019-01-23"
}