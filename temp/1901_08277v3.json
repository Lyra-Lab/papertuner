{
  "id": "http://arxiv.org/abs/1901.08277v3",
  "title": "Federated Deep Reinforcement Learning",
  "authors": [
    "Hankz Hankui Zhuo",
    "Wenfeng Feng",
    "Yufeng Lin",
    "Qian Xu",
    "Qiang Yang"
  ],
  "abstract": "In deep reinforcement learning, building policies of high-quality is\nchallenging when the feature space of states is small and the training data is\nlimited. Despite the success of previous transfer learning approaches in deep\nreinforcement learning, directly transferring data or models from an agent to\nanother agent is often not allowed due to the privacy of data and/or models in\nmany privacy-aware applications. In this paper, we propose a novel deep\nreinforcement learning framework to federatively build models of high-quality\nfor agents with consideration of their privacies, namely Federated deep\nReinforcement Learning (FedRL). To protect the privacy of data and models, we\nexploit Gausian differentials on the information shared with each other when\nupdating their local models. In the experiment, we evaluate our FedRL framework\nin two diverse domains, Grid-world and Text2Action domains, by comparing to\nvarious baselines.",
  "text": "Federated Deep Reinforcement Learning\nHankz Hankui Zhuo 1 Wenfeng Feng 1 Yufeng Lin 1 Qian Xu 2 Qiang Yang 2\nAbstract\nIn deep reinforcement learning, building policies\nof high-quality is challenging when the feature\nspace of states is small and the training data is lim-\nited. Despite the success of previous transfer learn-\ning approaches in deep reinforcement learning,\ndirectly transferring data or models from an agent\nto another agent is often not allowed due to the pri-\nvacy of data and/or models in many privacy-aware\napplications. In this paper, we propose a novel\ndeep reinforcement learning framework to federa-\ntively build models of high-quality for agents with\nconsideration of their privacies, namely Federated\ndeep Reinforcement Learning (FedRL). To pro-\ntect the privacy of data and models, we exploit\nGausian differentials on the information shared\nwith each other when updating their local mod-\nels. In the experiment, we evaluate our FedRL\nframework in two diverse domains, Grid-world\nand Text2Action domains, by comparing to vari-\nous baselines.\n1. Introduction\nIn deep reinforcement learning, building policies of high-\nquality is challenging when the feature space of states is\nsmall and the training data is limited. In many real-world\napplications, however, datasets from clients are often privacy\nsensitive (Duchi et al., 2012) and it is often difﬁcult for\nsuch a data center to guarantee building models of high-\nquality. To deal with the issue, Konecny et al. propose a new\nlearning setting, namely federated learning, whose goal is to\ntrain a classiﬁcation or clustering model with training data\ninvolving texts, images or videos distributed over a large\nnumber of clients (Konecn´y et al., 2016; McMahan et al.,\n2017). Different from previous federated learning setting\n(c.f. (Yang et al., 2019)), we propose a novel federated\n1zhuohank@mail.sysu.edu.cn,\nfengwf2014@outlook.com,\nlinyf23@mail2.sysu.edu.cn, Sun Yat-Sen University, Guangzhou,\nChina; 2{qianxu,qiangyang}@webank.com, WeBank, Shenzhen,\nChina.\nCorrespondence\nto:\nHankz\nHankui\nZhuo\n<zhuo-\nhank@mail.sysu.edu.cn>.\nlearning framework based on reinforcement learning (Sutton\n& Barto, 1998; Mnih et al., 2015; Co-Reyes et al., 2018), i.e.,\nFederated deep Reinforcement Learning (FedRL), which\naims to learn a private Q-network policy for each agent by\nsharing limited information (i.e., output of the Q-network)\namong agents. The information is “encoded” when it is sent\nto others and “decoded” when it is received by others. We\nassume that some agents have rewards corresponding to\nstates and actions, while others have only observed states\nwithout rewards. Without rewards, those agents are unable\nto build decision policies on their own information. We\nclaim that all agents beneﬁt from joining the federation in\nbuilding decision policies.\nThere are many applications regarding federated reinforce-\nment learning. For example, in the manufacturing industry,\nproducing products may involve various factories which\nproduce different components of the products. Factories’\ndecision policies are private and will not be shared with\neach other. On the other hand, building individual decision\npolicies of high-quality on their own is often difﬁcult due\nto their limited businesses and lack of rewards (for some\nfactories). It is thus helpful for them to learn decision po-\nlices federatively under the condition that private data is not\ngiven away. Another example is building medical treatment\npolicies to patients for hospitals. Patients may be treated in\nsome hospitals and never give feedbacks to the treatments,\nwhich indicates these hospitals are unable to collect rewards\nbased on the treatments given to the patients and build treat-\nment decision policies for patients. In addition, data records\nabout patients are private and may not be shared among\nhospitals. It is thus necessitated to learn treatment policies\nfor hospitals federatively.\nOur FedRL framework is different from multi-agent rein-\nforcement learning, which is concerned with a set of au-\ntonomous agents that observe global states (or partial states\nwhich are directly shared to make “global” states), select\nan individual action and receive a team reward (or each\nagent receives an individual reward but shares it with other\nagents) (Tampuu et al., 2015; Leibo et al., 2017; Foerster\net al., 2016). FedRL assumes agents do not share their par-\ntial observations and some agents are unable to receive re-\nwards. Our FedRL framework is also different from transfer\nlearning in reinforcement learning, which aims to transfer\nexperience gained in learning to perform one task to help\narXiv:1901.08277v3  [cs.LG]  9 Feb 2020\nFederated Collaborative Reinforcement Learning\nimprove learning performance in a related but different task\nor agent, assuming observations are shared with each other\n(Taylor & Stone, 2009; Tirinzoni et al., 2018), while FedRL\nassumes states cannot be shared among agents.\nOur FedRL framework functions in three phases. Initially,\neach agent collects output values of Q-networks from other\nagents, which are “encrypted” with Gausian differentials.\nFurthermore, it builds a shared value network, e.g., MLP\n(multilayer perceptron), to compute a global Q-network\noutput with its own Q-network output and the encrypted\nvalues as input. Finally, it updates both the shared value\nnetwork and its own Q-network based on the global Q-\nnetwork output. Note that MLP is shared among agents\nwhile agents’ own Q-networks are unknown to others and\nshould not be inferred based on the encrypted Q-network\noutput shared in the training process.\nIn the remainder of the paper, we ﬁrst review previous work\nrelated to our FedRL framework, and then present the prob-\nlem formulation of FedRL. After that, we introduce our\nFedRL framework in detail. Finally we evaluate our FedRL\nframework in the Grid-World domain with various sizes and\nthe Text2Actions domain.\n2. Related Work\nThe nascent ﬁeld of federated learning considers training\nstatistical models directly on devices (Konecn´y et al., 2015;\nMcMahan et al., 2017). The aim in federated learning is to\nﬁt a model to data generated by distributed nodes. Each node\ncollects data in a non-IID manner across the network, with\ndata on each node being generated by a distinct distribution.\nThere are typically a large number of nodes in the network,\nand communication is often a signiﬁcant bottleneck. Dif-\nferent from previous work that train a single global model\nacross the network (Konecn´y et al., 2015; 2016; McMahan\net al., 2017), Smith et al. propose to learn separate mod-\nels for each node which is naturally captured through a\nmulti-task learning (MTL) framework, where the goal is to\nconsider ﬁtting separate but related models simultaneously\n(Smith et al., 2017). Different from those federated learning\napproaches, we consider federated settings in reinforcement\nlearning.\nOur work is also related to multi-agent reinforcement learn-\ning (MARL) which involves a set of agents in a shared\nenvironment. A straightforward way to MARL is to ex-\ntend the single-agent RL approaches. Q-learning has been\nextended to cooperative multi-agent settings, namely Inde-\npendent Q-learning (IQL), in which each agent observes\nthe global state, selects an individual action and receives a\nteam reward (Tampuu et al., 2015; Leibo et al., 2017). One\nchallenging of MARL is that multi-agent domains are non-\nstationary from agent’s perspectives, due to other agents’\ninteractions in the shared environment. To address this issue,\n(Omidshaﬁei et al., 2017) propose to explore Concurrent\nExperience Replay Trajectories (CERTs) structures, which\nstore different agents’ histories, and align them together\nbased on the episode indices and time steps. Due to the ac-\ntion space growing exponentially with the number of agents,\nlearning becomes very difﬁcult due to partial observability\nof limited communication when the number of agents is\nlarge. (Lowe et al., 2017) thus propose to solve the MARL\nproblem through a Centralized Critic and a Decentralized\nActor, and (Rashid et al., 2018) propose to exploit a linear\ndecomposition of the joint value function across agents. Dif-\nferent from MARL, our FedRL framework assumes agents\ndo not share their partial observations and some agents are\nunable to receive rewards, instead of assuming observations\nare sharable and all agents are able to receive rewards.\n3. Problem Deﬁnition\nA Markov Decision Process (MDP) can be deﬁned by\n⟨S, A, T, r⟩, where S is the state space, and A is the ac-\ntion space. T is the transition function: S × A →S, i.e.,\nT(s, a, s′) = P(s′|s, a), specifying the probability of next\nstate s′ ∈S given current state s ∈S and a ∈A that ap-\nplies on s. r is the reward function: S →R, where R is the\nspace of real numbers. Given a policy π : S →A, the value\nfunction V π(s) and the Q-function Qπ(s, a) at step t + 1,\ncan be updated by their t step:\nV π\nt+1(s) = r(s) +\nX\ns′∈S\nT(s, π(s), s′)V π\nt (s′),\nand\nQπ\nt+1(s, a) = r(s) +\nX\ns′∈S\nT(s, a, s′)V π\nt (s′),\nfor t ∈{0, . . . , K −1}. The solution to an MDP prob-\nlem is the best policy π∗such that V π∗(s) = maxπ V π(s)\nor Qπ∗(s, π∗(s)) = maxπ Qπ(s, π(s)). In DQN (Deep Q-\nNetwork), given transition function T is unknown, the Q-\nfunction is represented by a Q-network Q(s, a; θ) with θ as\nparameters of the network, and updated by\nQt+1(s, a; θ) = Es′\nn\nr(s) + γ max\na′∈A Qt(s′, a′; θ)|s, a\no\n,\nas done by (Mnih et al., 2015). To learn the parameters θ,\none way is to store transitions ⟨s, a, s′, r⟩in replay mem-\nories Ωand exploit a mini-batch sampling to repeatedly\nupdate θ (Mnih et al., 2015). Once θ is learnt, the policy π∗\ncan be extracted from Q(s, a; θ),\nπ∗(s) = arg max\na∈A Q(s, a; θ).\nWe deﬁne our federated deep reinforcement learning prob-\nlem by: given transitions Dα = {⟨sα, aα, s′\nα, rα⟩} col-\nlected by agent α, and pairs of states and actions Dβ =\nFederated Collaborative Reinforcement Learning\n{⟨sβ, aβ⟩} collected by agent β, we aim to federatively\nbuild policies π∗\nα and π∗\nβ for agents α and β, respectively.\nNote that in this paper we consider the federation with two\nmembers for simplicity. The setting can be extended to many\nagents by exploiting the same federated mechanism between\neach two agents. We denote states, actions, Q-functions,\nand policies with respect to agents α and β, by “sα ∈Sα,\naα ∈Aα, Qα, π∗\nα” and “sβ ∈Sβ, aβ ∈Aβ, Qβ, π∗\nβ”,\nrespectively.\nIn our federated deep reinforcement learning problem, we\nassume:\nA1: The feature spaces of states sα and sβ are different\nbetween agents α and β. For example, a state sα denotes a\npatient’s cardiogram in hospital α, while another state sβ\ndenotes the same patient’s electroencephalogram in hospital\nβ, indicating the feature spaces of sα and sβ are different.\nA2: Transitions Dα and Dβ cannot be shared directly be-\ntween α and β when they learning their own models. The\ncorrespondences between transitions from Dα and Dβ are,\nhowever, known to each other. In other words, agent α can\nsend the “ID” of a transition to agent β, and agent β can\nuse that “ID” to ﬁnd its corresponding transition in Dβ.\nFor example, in hospital, “ID” can correspond to a speciﬁc\npatient.\nA3: The output of functions Qα and Qβ can be shared with\neach other under the condition that they are protected by\nsome privacy protection mechanism.\nBased on A1-A3, we aim to learn policies π∗\nα and π∗\nβ of\nhigh-quality for agents α and β by preserving privacies of\ntheir own data and models.\n4. Our FedRL Approach\nIn this section we present our FedRL framework in detail.\nAn overview of FedRL is shown in Figure 1, where the left\npart is the model of agent α, and the right part is the model\nof agent β. Each model is composed of a local Q-network\nwith parameters θα for agent α, θβ for agent β, and a global\nMLP module with parameters θg for both α and β.\nBasic Q-networks\nWe build two Q-networks for agents\nα and β, denoted by Qα(sα, aα; θα) and Qβ(sβ, aβ; θβ),\nrespectively, where θα and θg are parameters of the Q-\nnetworks. The outputs of these two basic Q-networks are\nnot directly used to predict the actions, but taken as input of\nthe MLP module.\nGaussian differential privacy\nTo avoid agents “induc-\ning” models of each other according to repeatedly received\noutputs of other’s Q-network during training, we consider\nusing differential privacy (Dwork & Roth, 2014) to pro-\n𝜃\"\n𝐶$\n𝑄& = [𝑞*\n&, …, 𝑞-.\n& ]\n⨁\n𝑁(0, 𝜎5)\n𝑄7 = [𝑞*\n7,… , 𝑞-.\n7 ]\n𝜃&\n𝑠&\n9\nAgent 𝛼\n𝜃\"\n𝐶&\n𝑄$ = [𝑞*\n$,… , 𝑞-.\n$ ]\n⨁\n𝑁(0, 𝜎5)\n𝑄7 = [𝑞*\n7,… , 𝑞-.\n7 ]\n𝜃$\n𝑠$\n9\nAgent 𝛽\nFigure 1. The networks of agents α and β\ntect the output of agent’s Q-network. There are various\nmechanisms of differential privacy such as the Gaussian\nmechanism (Abadi et al., 2016) and Binomial mechanism\n(Agarwal et al., 2018). In this paper, we exploit Gaussian\nmechanism since the output of the MLP with Gaussian input\nis Gaussian itself. In previous federated learning settings,\nthe mechanism is applied to the gradients of agents (clients)\nbefore being sent to the server or other agents (clients). In\nour FedRL framework, we send the output of one agent’s\nQ-network to another, so the Gaussian noise is added to the\noutput rather than the gradients. The mechanism is deﬁned\nby\nˆQα(sα, aα; θα) = Qα(sα, aα; θα) + N(0, σ2)\n(1)\nˆQβ(sβ, aβ; θβ) = Qβ(sβ, aβ; θβ) + N(0, σ2)\n(2)\nwhere N(0, σ2) is the Gaussian distribution with mean 0\nand standard deviation σ.\nFederated Q-network\nWe build a new Q-network,\nnamely federated Q-network denoted by Qf, to leverage the\noutputs of the two basic Q-networks with Gaussian noise\nbased on MLP, which is deﬁned by\nQf(·; θα, θβ, θg) =\nMLP([ ˆQα(sα, aα; θα)| ˆQβ(sβ, aβ; θβ)]; θg) (3)\nwhere θg is the parameters of MLP and [·|·] indicates the\nconcatenation operation. Note that parameters of an MLP\ncan be shared between agents. Once MLP is updated, the\nupdated parameters are shared with the other agent.\nWith respect to agents α and β, we deﬁne each agent’s\nfederated Q-networks by viewing the other agent’s basic\nQ-network (with Gaussian noise) as a ﬁxed constant when\nupdating its own basic Q-network, as shown below,\nQα\nf (·, Cβ; θα, θg) = MLP([ ˆQα(·; θα)|Cβ]; θg)\n(4)\nQβ\nf(·, Cα; θβ, θg) = MLP([Cα| ˆQβ(·; θβ)]; θg)\n(5)\nwhere Cα = ˆQα(sα, aα; θα) and Cβ = ˆQβ(sβ, aβ; θβ) are\nﬁxed constants when updating agent β’s basic Q-network\nand agent α’s basic Q-network, respectively.\nFederated Collaborative Reinforcement Learning\nThe Q-networks are trained by minimizing the square error\nloss Lj\nα(θα, θg) and Lj\nβ(θβ, θg) of agents α and β,\nLj\nα(θα, θg) = E\n\u0014\n(Y j −Qα\nf (sj\nα, aj\nα, Cβ; θα, θg))2\n\u0015\n(6)\nLj\nβ(θβ, θg) = E\n\u0014\n(Y j −Qβ\nf(sj\nβ, aj\nβ, Cα; θβ, θg))2\n\u0015\n(7)\nwhere Y j = rj + γ max\na\nQα\nf (sj\nα, a, Cβ; θα, θg). Note that\nagent β is unable to compute Y j since it does not have\nreward r. Y j is computed by agent α and shared with agent\nβ.\n𝐶\"\n𝐶#\nCompute \n𝑄%\n#\nCompute\n𝐶#\nAgent 𝛼\nCompute\n𝑄%\n\"\nAgent 𝛽\n(II) testing\n𝑠#\n𝑠\"\nCompute\n𝐶\"\n𝑎\n𝑎\n𝐶\"\n𝜃+\n𝜃+\n𝑌-\n𝐶#\n Update \n𝜃# and  𝜃+\nCompute\n𝑌-\nCompute\n𝐶#\nAgent 𝛼\nCompute\n𝐶\"\nUpdate\n𝜃\" and  𝜃+\nAgent 𝛽\n(I) training\n𝑌-\n𝜃# \n𝜃\"\nFigure 2. The training and testing procedures\nOverview of training and testing\nThe training and test-\ning procedures of agents α and β are shown in Figure 2.\nWhen training, agent α computes Y j and sends it to agent\nβ. Agent β updates θβ and θg, computes Cβ, and sends\nθg and Cβ to agent α. After that, agent α updates θα and\nθg, computes Cα, and sends θg and Cα to agent β. When\ntesting, both agents compute and send Cα and Cβ to each\nother for computing Qα\nf and Qβ\nf.\nThe detailed training procedure can be seen from Algorithms\n1 and 2. In Steps 1 and 2 of Algorithm 1, we initialize the\nbasic Q-network and replay memory of agent α and the\nMLP module. In Step 3 of Algorithm 1, we call the function\nfrom Algorithm 2 to initialize the Q-network and replay\nmemory of agent β. In Step 6 of Algorithm 1, we obtain\nobservation of agent α’s state st\nα. In Step 7 of Algorithm 1,\nwe call the function in Algorithm 2 to calculate the output\nof basic Q-network of agent β and obtain observation of\nagent β and select the corresponding action, as shown in\nSteps from 6 to 10 of Algorithm 2. In Steps from 8 and 11\nof Algorithm 1, we perform the ϵ-greedy exploration and\nexploitation, obtain new observations and store the transi-\ntions to the replay memory. In Steps 12 and 13 of Algorithm\n1, we sample a record j in the memory and call the function\nComputeQBeta(j) of Algorithm 2 to calculate the output\nof basic Q-network of agent β based on the index j. In Steps\n14 and 15 of Algorithm 1, we update parameters θα and θg.\nIn Steps 16 and 17 of Algorithm 1, we compute the output\nof basic Q-network of agent α and pass it to agent β and call\nthe function UpdateQ of agent β to update basic Q-network\nof agent β and MLP, as shown in Steps from 19 to 21 of\nAlgorithm 2. Note that Algorithms 1 and 2 are executed by\nagents α and β separately.\nAlgorithm 1 FedRL-ALPHA\nInput: state space Sα, action space Aα, rewards r\nOutput: θα,θg\n1: Initialize Qα, Qf with random values for θα, θg\n2: Initialize replay memory Dα\n3: Call FedRL-BETA.Init()\n4: for episode = 1: M do\n5:\nrepeat\n6:\nObserve st\nα\n7:\nCall Cβ=FedRL-BETA.ComputeQBeta()\n8:\nSelect action at with probability ϵ\n9:\nOtherwise at = arg max\na\nQα\nf (st\nα, a, Cβ; θα, θg)\n10:\nExecute action at, obtain reward rt and state st+1\n11:\nObserve st+1\nα\n, store (st\nα, at, rt, st+1\nα\n) in Dα\n12:\nSample (sj\nα, aj, rj, sj+1\nα\n) from Dα\n13:\nCall Cβ=FedRL-BETA.ComputeQBeta(j)\n14:\nY j = rj + γ max\na\nQα\nf (sj\nα, a, Cβ; θα, θg)\n15:\nUpdate θα, θg according to Eq. (4), (6)\n16:\nCα = ˆQα(sj\nα, a; θα)\n17:\nCall θg=FedRL-BETA.UpdateQ(Y j, j, Cα, θg)\n18:\nuntil terminal t\n19: end for\nAlgorithm 2 FedRL-BETA\nInput: state space Sβ, action space Aβ\nOutput: θβ, θg\n1: function Init()\n2:\nInitialize Qβ with random values for θβ\n3:\nInitialize replay memory Dβ\n4: end function\n5: function ComputeQBeta()\n6:\nObserve sβ\n7:\nSelect aβ ∈Aβ with probability ϵ\n8:\nOtherwise aβ = arg max\naβ Qβ(sβ, aβ; θβ)\n9:\nStore (sβ, aβ) in Dβ\n10:\nLet Cβ = ˆQβ(sβ, a; θβ)\n11:\nreturn Cβ\n12: end function\n13: function ComputeQBeta(j)\n14:\nSelect (sβ, aβ) from Dβ based on index j\n15:\nLet Cβ = ˆQβ(sβ, aβ; θβ)\n16:\nreturn Cβ\n17: end function\n18: function UpdateQ(Y j, j, Cα, θg)\n19:\nSelect (oj\nβ, aj\nβ) from Dβ based on index j\n20:\nUpdate θβ, θg based on Eq. (5), (7)\n21:\nreturn θg\n22: end function\nFederated Collaborative Reinforcement Learning\n5. Experiments\nIn the experiment, we evaluate FedRL1 in the following\ntwo aspects. Firstly, we would like to see if agent α can\nlearn better policies by joining the federation than without\njoining the federation (agent α can build policies by itself\nsince it has complete transitions {⟨s, a, s′, r⟩}, while agent\nβ cannot build policies without joining the federation since\nit has only pairs of states and actions {⟨s, a⟩}). Secondly, we\nwould like to see if our FedRL approach can learn policies\nof high-quality which are close to the ones learnt by directly\ncombining data of both agent α and β, neglecting the data-\nprivacy issue between α and β. To do this, we compared our\nFedRL approach to the following baselines:\n• DQN-alpha: which is a deep Q-network (Mnih et al.,\n2015) trained with agent α’s data only. It takes obser-\nvations sα as input and outputs actions corresponding\nto sα.\n• DQN-full: which is a deep Q-network trained by di-\nrectly putting data together from both agents α and β,\ni.e., neglecting data privacy between agents α and β.\n• FCN-alpha: which is a fully convolutional network\n(FCN) trained with agent α’s data only, similar to DQN-\nalpha. FCN-alpha aims to build policies via supervised\nlearning (Furuta et al., 2019), i.e., viewing states as\ninput and actions as labels.\n• FCN-full: which is a convolutional network trained\nwith all data of agent α and β put together directly,\nsimilar to DQN-full.\nIn our experiment, we used the kernel of size 3 × 3 in CNN\nand two fully-connected layers with the size of 32 × 4 in\nMLP. We set the standard deviation in Gaussian differential\nprivacy σ to be 1. We adopted the Adam optimizer with\nlearning rate 0.001 and the ReLU activation for all models.\nWe evaluated our FedRL with comparison to baselines in\ntwo domains, i.e., Grid-World and Text2Action.\n5.1. Evaluation in Grid-World Domain\nWe ﬁrst conducted experiments in Grid-World domain with\nrandomly placed obstacles (Tamar et al., 2016). The two\nagents α and β were randomly put in the environment as\nwell, as shown in Figure 3. The two agents aim to navigate\nthrough optimal pathes (i.e. the shortest pathes without hit-\nting obstacles) to meet with each other. In this domain, we\ndeﬁne states, actions and rewords as follows.\n• States: The domain is represented by a Ng×Ng binary-\nvalued matrix (0 for obstacle, 1 otherwise), where Ng\n1The\nsource\ncode\nand\ndatasets\nare\navailable\nfrom\nhttps://github.com/FRL2019/FRL\n5\n10\n15\n20\n25\n30\n5\n10\n15\n20\n25\n30\nalpha\nbeta\nFigure 3. A 32 × 32 grid-world domain with agents α and β. The\nrectangles in blue are boundaries of observations.\nis the size of the domain. We evaluated our approach\nwith respect to different sizes, i.e., Ng = 8, 16, 32. The\nobserved state of agent α, denoted by sα, was set to be\na 3 × 3 matrix with the current position of agent α in\nthe center of the matrix. The observed state of agent β,\ndenoted by sβ, was set to be a 5 × 5 matrix with the\ncurrent position of agent β in the center of the matrix.\n• Actions: There are 4 actions for each agent,\ni.e.\ngoing\ntowards\n4\ndirections,\ndenoted\nby\n{east, south, west, north}.\n• Rewards: The reward is composed of two parts, i.e., lo-\ncal reward rl and global reward rg, respectively. When\nan agent hits an obstacle, rl is set to be -10; when an\nagent meets the other agent, rl is set to be +50; other-\nwise, rl is set to be -1. Considering the goal of the task\nis to make the two agents eventually meet each other,\nwe exploited an additional reward regarding the dis-\ntance between two agents, namely global reward, i.e.,\nrg = c/md(α, β), where md(α, β) is the Manhattan\ndistance between the two agents, and c is a regular-\nization factor which is set to be the dimension of the\ndomain, i.e. c = Ng. The ﬁnal reward r is calculated\nby r = rl + rg.\n• Dataset: We generated 8000 different maps (or matri-\nces) for each size of 8 × 8, 16 × 16 and 32 × 32. In\neach map, we randomly chose two positions for the\ntwo agents, and computed the optimal path (shortest\npath) which was compared to the paths predicted by\nour FedRL and baselines. We randomly split the 8000\nmaps into 6400 for training, 800 for validation, and\n800 for testing.\n• Criteria: In each training episode, we ﬁrst took the\ninitial state and predicted an action with a model. We\nthen got a new state and took it as the new input of\nthe model at the second time step. We repeated the\nprocedure until the two agents met each other, which\nis indicated as a successful episode, or it exceeded the\nmaximum time step Tm, which is indicated as a failure\nepisode. We set Tm to be twice the length of the longest\noptimal path, i.e. Tm = 38, 86, 178 for each size of\nFederated Collaborative Reinforcement Learning\n8 × 8, 16 × 16 and 32 × 32, respectively. We ﬁnally\ncomputed the measures of successful rate SuccRate,\nand average reward AvgRwd, i.e.,\nSuccRate = #SuccessfulEpisodes\n#TotalEpisodes\n,\nand\nAvgRwd = TotalCumulativeReward\n#TotalEpisodes\n,\nwhere #SuccessfulEpisodes indicates the number\nof successful episodes, #TotalEpisodes indicates the\ntotal number of episodes, TotalCumulativeReward\nindicates the total reward of all episodes, respectively.\nTable 1. Comparison with baselines in Grid-World\nMetric\nMethod\nDomain w.r.t. various sizes\n8 × 8\n16 × 16\n32 × 32\nSuccRate\nFCN-alpha\n69.73%\n48.04%\n41.73%\nDQN-alpha\n88.27%\n76.20%\n71.41%\nFedRL-1\n92.52%\n79.83%\n77.88%\nFedRL-2\n95.06%\n84.31%\n82.02%\nFCN-full\n72.16%\n56.44%\n50.15%\nDQN-full\n93.69%\n83.40%\n79.73%\nAvgRwd\nDQN-alpha\n13.781\n-112.084\n-285.946\nFedRL-1\n18.152\n-94.193\n-226.583\nFedRL-2\n19.101\n-84.139\n-189.756\nDQN-full\n31.286\n-38.114\n-52.72\nExperimental Results w.r.t. Domain Sizes\nWe ran our\nFedRL and baselines ﬁve times and calculated an average\nof SuccRate (as well as AvgRwd). We calculated two re-\nsults of our FedRL, denoted by FedRL-1 and FedRL-2,\nwhich correspond to “adding Gausian differential privacy on\nthe local Q-network” and ”not adding Gausian differential\nprivacy on the local Q-network” in the testing data, respec-\ntively. The results are shown in Table 1. From Table 1, we\ncan see that SuccRate of both FedRL-1 and FedRL-2 is\nmuch better than DQN-alpha and CNN-alpha in all three dif-\nferent sizes of domains, which indicates agent α can indeed\nget help from agent β via learning federatively with agent β.\nComparing to DQN-full, we can see that the SuccRate of\nboth FedRL-1 and FedRL-2 is close to DQN-full, which\nindicates our federated learning framework can indeed take\nadvantage of both training data from agents α and β, even\nthough they are protected locally with Gausian differential\nprivacy (the reason why FedRL-2 is slightly better than\nDQN-full is that the hierarchical structure of our federated\nlearning framework with three components may be more\nsuitable for this domain than a unique DQN framework).\nIn addition, we can also see that the SuccRate generally\ndecreases when the size of the domain increases, which is\nconsistent with our intuition, since the larger the domain is,\nthe more difﬁcult the task is (which requires more training\ndata to build models of high-quality).\nFrom the metric of AvgRwd, we can see that both FedRL-1\nand FedRL-2 are better than DQN-alpha, which indicates\nagent α can indeed get help from agent β in gaining re-\nwards via federated learning with agent β. We can also\nﬁnd that FedRL-2 outperforms FedRL-1 in both AvgRwd\nand SuccRate. This is because our model is trained based\non Gausian differential privacy, indicating SuccRate on\ntesting data with Gausian differential privacy, as done by\nFedRL-2, should be better than on testing data without\nGausian differential privacy, as done by FedRL-1.\nExperimental Results w.r.t. History Length\nTo study\nwhen FedRL works, we consider the amount of informa-\ntion that an agent uses. The observation input at each time\nis a sequence of observations, i.e. observation history. In-\ntuitively, the longer the length of the observation history,\nthe more information we have, and the more complicated\nthe neural network is. We ﬁxed the structures of all models\nand only changed the length of the history observations. We\ntested the length of history from 2 to 32 for all domains. The\nresults are shown in Figure 4. In the ﬁrst row of Figure 4,\nwe can observe is that the success rates are improving with\nthe increment of the history length. In 8 × 8 and 16 × 16\ndomains, the results converge when history length is longer\nthan 16, while in 32 × 32 domain, they have not converged\neven at the history length of 32. The reason is that 32 × 32\ndomain is more complicated than the other two domains, so\nit needs more information (i.e. H > 32) to learn a model of\nhigh-quality. We can also ﬁnd that when the history length is\nshort (i.e. H = 2 for 16×16 and H ≤4 for 32×32), DQN-\nalpha, DQN-full and FedRL-1 perform poorly. FedRL-1\nand DQN-full do not show their advantages although they\ntake as input both sα and sβ directly or indirectly. How-\never, FedRL-2, which applies differential privacy to both\ntraining and testing samples, shows its great scalability even\nwith limited amount of history. In small domains, such as\n8 × 8, a few steps are enough to explore the whole environ-\nment. Therefore, the DQN-alpha which takes only single\nobservation as input can performs as well as DQN-alpha\nand FedRL models.\n5.2. Text2Actions Domain\nIn this experiment, we evaluated our FedRL in another\ndomain, i.e., Text2Action (Feng et al., 2018), which aims to\nextract action sequences from texts. For example, consider\na text “Cook the rice the day before, or use leftover rice\nin the refrigerator. The important thing to remember is not\nto heat up the rice, but keep it cold.”, which addresses the\nprocedure of making egg ﬁred rice. The task is to extract\nwords composing an action sequence “cook(rice), keep(rice,\ncold)”, or “use(leftover rice), keep(rice, cold)”. We deﬁne\nFederated Collaborative Reinforcement Learning\n0.6 \n0.7 \n0.8 \n0.9 \n1 \n2 \n4 \n8 \n16 \n32 \nsucc. rate \nhistory length \n8x8 \n0.4 \n0.5 \n0.6 \n0.7 \n0.8 \n0.9 \n1 \n2 \n4 \n8 \n16 \n32 \nsucc. rate \nhistory length \n16x16 \n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n2\n4\n8\n16\n32\nsucc. rate\nhistory length\n32x32\nDQN-alpha\nDQN-full\nFedRL-1\nFedRL-2\nFCN-alpha\nFCN-full\n0 \n10 \n20 \n30 \n40 \n50 \n2 \n4 \n8 \n16 \n32 \navg. rwd. \nhistory length \n8x8 \n-200 \n-150 \n-100 \n-50 \n0 \n2 \n4 \n8 \n16 \n32 \navg. rwd. \nhistory length \n16x16 \n-450\n-400\n-350\n-300\n-250\n-200\n-150\n-100\n-50\n0\n2\n4\n8\n16\n32\navg. rwd.\nhistory length\n32x32\nDQN-alpha\nDQN-full\nFedRL-1\nFedRL-2\nFigure 4. Results about the impact of history length.\nstates, actions and rewords as follows.\nTable 2. Comparison with baselines in Text2Action\nMetric\nMethod\nDataset\nWHS\nCT\nWHG\nF1\nFCN-alpha\n86.19%\n65.44%\n55.18%\nDQN-alpha\n92.11%\n74.64%\n66.37%\nFedRL-1\n93.76%\n85.05%\n75.64%\nFedRL-2\n94.41%\n84.92%\n75.85%\nFCN-full\n91.42%\n79.03%\n68.93%\nDQN-full\n94.55%\n83.39%\n74.63%\nAvgRwd\nDQN-alpha\n54.762\n47.375\n46.510\nFedRL-1\n55.623\n50.472\n48.359\nFedRL-2\n55.894\n50.452\n48.373\nDQN-full\n56.192\n50.307\n48.154\n• States: sα ∈RNw×K1 is real-valued matrix that de-\nscribes the part-of-speech of words, and sβ ∈RNw×K2\nis real-valued matrix that describes the embedding of\nwords. Nw is the number of words in the text, K1 is\nthe dimension of a part-of-speech vector, and K2 is\nthe dimension of word vectors. In our experiments,\npart-of-speech vectors are randomly initialized and\ntrained together with the Q-network, while word vec-\ntors are generated from the pre-trained word embed-\ndings and will not be changed during the training of\nthe Q-network.\n• Actions: There are two actions for each agent, i.e.,\n{select, neglect}, indicating selecting a word as an\naction name (or a parameter), or neglecting a word\n(which means the corresponding word is neither an\naction name nor a parameter).\n• Rewards: The instant reward includes a basic reward\nand an additional reward, where the basic reward indi-\ncates whether the agent selects a word correctly or not,\nand the additional reward encodes the priori knowl-\nedge of the domain, i.e., the proportion of words that\nare related to action sequences (Feng et al., 2018). We\nassume that only agent α knows the rewards, while\nagent β does not know them.\n• Dataset: We conducted experiments on three datasets,\ni.e., “Microsoft Windows Help and Support” (WHS)\ndocuments (Branavan et al., 2009), and two datasets\ncollected from “WikiHow Home and Garden”2 (WHG)\nand “CookingTutorial”3 (CT), respectively. In CT,\nthere are 116 labeled texts and 134,000 words, with\n10.37% being action names and 7.44% being action\narguments. In WHG, there are 150 labeled texts and\n34,000,000 words, with 7.61% being action names and\n6.3% being action arguments. In WHS, there are 154 la-\nbeled texts and 1,500 words, with 19.47% being action\nnames and 15.45% being action arguments.\n• Criteria: For evaluation, we ﬁrst fed texts to each\nmodel to obtain selected words. We then compared\nthe outputs to their corresponding ground truth and\ncalculated #TotalTruth (total ground truth words),\n#TotalRight (total correctly selected words), and\nTotalSelected (total selected words). After that\nwe computed metrics precision =\n#T otalRight\n#T otalSelected,\n2https://www.wikihow.com/Category:Home-and-Garden\n3http://cookingtutorials.com/\nFederated Collaborative Reinforcement Learning\n0.35\n0.45\n0.55\n0.65\n0.75\n0.85\n0.95\n2\n4\n8\n16\n32\nF1\nnumber of filters\nWHS\n0.35\n0.45\n0.55\n0.65\n0.75\n0.85\n0.95\n2\n4\n8\n16\n32\nF1\nnumber of filters\nCT\n0.35\n0.45\n0.55\n0.65\n0.75\n0.85\n0.95\n2\n4\n8\n16\n32\nF1\nnumber of filters\nWHG\nEASDRL-alpha\nEASDRL-full\nFCN-alpha\nFCN-full\nFedRL-1\nFedRL-2\nFigure 5. Results about the impact of the number of convolutional kernels.\nrecall = #T otalRight\n#T otalT ruth, and\nF1 = 2 × precision × recall\nprecision + recall\n.\nWe used F1-metric for all baselines. For reinforcement\nlearning methods, we also computed the average cu-\nmulative rewards\nAvgRwd = TotalCumulativeReward\n#TotalTimeSteps\n,\nwhere TotalCumulativeReward indicates the to-\ntal cumulative reward of all testing texts and\n#TotalTimeSteps indicates the total number of steps\nof all testing texts.\nWe adopted the same TextCNN structure as (Feng et al.,\n2018). Four convolutional layers corresponding to bigram,\ntri-gram, four-gram and ﬁve-gram with 32 kernels of size\nn × m, where n refers to the n-gram and m = 50. Each\nconvolutional layer is followed by a max-pooling layer with\nsize of (Nw −n + 1, 1), where Nw −n + 1 is the ﬁrst\ndimension of the outputs of the n-gram convolutional layer.\nThe max-pooling outputs are concatenated and fed to two\nfully connected layers with size 128 × 2, where 128 equals\nto 4 × 32, indicating 4 types of n-grams and 32 kernels\nfor each n-grams, and 2 is the size of the action space. The\nMLP module of FedRL is composed of two fully-connected\nlayers with size 4 × 2. The standard deviation of Gaussian\ndifferential privacy σ was set to be 1. For all models, we\nadopted the Adam optimizer with learning rate 0.001 and\nReLU activation4.\nExperimental Results\nWe ran our FedRL and base-\nlines ﬁve times to calculate an average of F1 (as well as\nAvgRwd). The results are shown in Table 2. From Table\n2 we can see that both FedRL-1 and FedRL-2 outperform\n4Detailed setting can be found from the source code:\nhttps://github.com/FRL2019/FRL\nboth FCN-alpha and DQN-alpha in all three datasets under\nboth F1 and AverRwd metrics, which indicates agent α can\nlearn better policies via federated learning with agent β than\nlearning by itself. Comparing our FedRL-1 and FedRL-2\nwith DQN-full in both F1 and AvgRwd, we can see that\ntheir performances are close to each other, suggesting our\nfederated learning framework performs as good as the DQN\nmodel directly built from all training data from both agents\nα and β.\nTo see the impact of the model complexity, we varied the\nnumber of convolutional kernels from 2 to 32 with other\nparameters ﬁxed. The results are shown in Figure 5. We can\nsee that all models generally perform better when the num-\nber of convolutional kernels (ﬁlters) increases, and become\nstable after 8 kernels. We can also see that our FedRL mod-\nels outperform DQN-alpha, FCN-alpha and FCN-full with\nrespect to the number of kernels, which indicates agent α\ncan indeed learns better policies via federated learning with\nagent β. Both FedRL-1 and FedRL-2 are close to DQN-\nfull with respect to the number of ﬁlters, suggesting that our\nfederated learning framework is effective even though the\ndata of agents α and β are not shared with each other.\n6. Conclusion\nwe propose a novel reinforcement learning approach to con-\nsidering privacies and federatively building Q-network for\neach agent with the help of other agents, namely Federated\ndeep Reinforcement Learning (FedRL). To protect the pri-\nvacy of data and models, we exploit Gausian differentials on\nthe information shared with each other when updating their\nlocal models. We demonstrate that the proposed FedRL\napproach is effective in building high-quality policies for\nagents under the condition that training data are not shared\nbetween agents. In the future, it would be interesting to study\nmore members in the federation with background knowl-\nedge represented by (probably incomplete) action models\n(Zhuo et al., 2014; Zhuo & Yang, 2014; Zhuo & Kambham-\npati, 2017).\nFederated Collaborative Reinforcement Learning\nReferences\nAbadi, M., Chu, A., Goodfellow, I. J., McMahan, H. B.,\nMironov, I., Talwar, K., and Zhang, L. Deep learning\nwith differential privacy. In ACM SIGSAC, pp. 308–318,\n2016.\nAgarwal, N., Suresh, A. T., Yu, F. X., Kumar, S., and\nMcMahan, B.\ncpsgd: Communication-efﬁcient and\ndifferentially-private distributed SGD. In NeurIPS, pp.\n7575–7586, 2018.\nBranavan, S. R. K., Chen, H., Zettlemoyer, L. S., and Barzi-\nlay, R. Reinforcement learning for mapping instructions\nto actions. In ACL, 2009.\nCo-Reyes, J., Liu, Y., Gupta, A., Eysenbach, B., Abbeel,\nP., and Levine, S. Self-consistent trajectory autoencoder:\nHierarchical reinforcement learning with trajectory em-\nbeddings. In ICML, pp. 1009–1018, 2018.\nDuchi, J. C., Jordan, M. I., and Wainwright, M. J. Privacy\naware learning. In NIPS, pp. 1439–1447, 2012.\nDwork, C. and Roth, A. The algorithmic foundations of dif-\nferential privacy. Foundations and Trends in Theoretical\nComputer Science, 9(3-4):211–407, 2014.\nFeng, W., Zhuo, H. H., and Kambhampati, S. Extracting\naction sequences from texts based on deep reinforcement\nlearning. In IJCAI, pp. 4064–4070, 2018.\nFoerster, J. N., Assael, Y. M., de Freitas, N., and White-\nson, S. Learning to communicate with deep multi-agent\nreinforcement learning. In NIPS, pp. 2137–2145, 2016.\nFuruta, R., Inoue, N., and Yamasaki, T. Fully convolutional\nnetwork with multi-step reinforcement learning for image\nprocessing. In AAAI, pp. 3598–3605, 2019.\nKonecn´y, J., McMahan, B., and Ramage, D. Federated opti-\nmization: Distributed optimization beyond the datacenter.\nCoRR, abs/1511.03575, 2015.\nKonecn´y, J., McMahan, H. B., Yu, F. X., Richt´arik, P.,\nSuresh, A. T., and Bacon, D. Federated learning: Strate-\ngies for improving communication efﬁciency. CoRR,\nabs/1610.05492, 2016.\nLeibo, J. Z., Zambaldi, V. F., Lanctot, M., Marecki, J., and\nGraepel, T. Multi-agent reinforcement learning in sequen-\ntial social dilemmas. In AAMAS, pp. 464–473, 2017.\nLowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, P., and Mor-\ndatch, I. Multi-agent actor-critic for mixed cooperative-\ncompetitive environments.\nIn NIPS, pp. 6382–6393,\n2017.\nMcMahan, B., Moore, E., Ramage, D., Hampson, S., and\ny Arcas, B. A. Communication-efﬁcient learning of deep\nnetworks from decentralized data. In AISTATS, pp. 1273–\n1282, 2017.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Ve-\nness, J., Bellemare, M. G., Graves, A., Riedmiller, M.,\nFidjeland, A. K., and Ostrovski, G. Human-level control\nthrough deep reinforcement learning. Nature, 518(7540):\n529–33, 2015.\nOmidshaﬁei, S., Pazis, J., Amato, C., How, J. P., and Vian, J.\nDeep decentralized multi-task multi-agent reinforcement\nlearning under partial observability. In ICML, pp. 2681–\n2690, 2017.\nRashid, T., Samvelyan, M., de Witt, C. S., Farquhar, G.,\nFoerster, J. N., and Whiteson, S. QMIX: monotonic value\nfunction factorisation for deep multi-agent reinforcement\nlearning. In ICML, pp. 4292–4301, 2018.\nSmith, V., Chiang, C., Sanjabi, M., and Talwalkar, A. S.\nFederated multi-task learning. In NIPS, pp. 4427–4437,\n2017.\nSutton, R. S. and Barto, A. G. Reinforcement learning\n- an introduction. Adaptive computation and machine\nlearning. MIT Press, 1998. ISBN 0262193981.\nTamar, A., Levine, S., Abbeel, P., Wu, Y., and Thomas, G.\nValue iteration networks. In NIPS, pp. 2146–2154, 2016.\nTampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus,\nK., Aru, J., Aru, J., and Vicente, R. Multiagent cooper-\nation and competition with deep reinforcement learning.\nCoRR, abs/1511.08779, 2015.\nTaylor, M. E. and Stone, P. Transfer learning for reinforce-\nment learning domains: A survey. J. Mach. Learn. Res.,\n10:1633–1685, December 2009. ISSN 1532-4435.\nTirinzoni, A., Rodriguez Sanchez, R., and Restelli, M. Trans-\nfer of value functions via variational methods. In NIPS,\npp. 6182–6192. 2018.\nYang, Q., Liu, Y., Chen, T., and Tong, Y. Federated machine\nlearning: Concept and applications. ACM TIST, 10(2):\n12:1–12:19, 2019.\nZhuo, H. H. and Kambhampati, S. Model-lite planning:\nCase-based vs. model-based approaches. Artif. Intell.,\n246:1–21, 2017.\nZhuo, H. H. and Yang, Q. Action-model acquisition for\nplanning via transfer learning. Artif. Intell., 212:80–103,\n2014.\nZhuo, H. H., Mu˜noz-Avila, H., and Yang, Q. Learning hi-\nerarchical task network domains from partially observed\nplan traces. Artif. Intell., 212:134–157, 2014.\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2019-01-24",
  "updated": "2020-02-09"
}