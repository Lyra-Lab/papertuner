{
  "id": "http://arxiv.org/abs/1901.09388v2",
  "title": "Moving Deep Learning into Web Browser: How Far Can We Go?",
  "authors": [
    "Yun Ma",
    "Dongwei Xiang",
    "Shuyu Zheng",
    "Deyu Tian",
    "Xuanzhe Liu"
  ],
  "abstract": "Recently, several JavaScript-based deep learning frameworks have emerged,\nmaking it possible to perform deep learning tasks directly in browsers.\nHowever, little is known on what and how well we can do with these frameworks\nfor deep learning in browsers. To bridge the knowledge gap, in this paper, we\nconduct the first empirical study of deep learning in browsers. We survey 7\nmost popular JavaScript-based deep learning frameworks, investigating to what\nextent deep learning tasks have been supported in browsers so far. Then we\nmeasure the performance of different frameworks when running different deep\nlearning tasks. Finally, we dig out the performance gap between deep learning\nin browsers and on native platforms by comparing the performance of\nTensorFlow.js and TensorFlow in Python. Our findings could help application\ndevelopers, deep-learning framework vendors and browser vendors to improve the\nefficiency of deep learning in browsers.",
  "text": "Moving Deep Learning into Web Browser: How Far Can We Go?\nYun Ma1,2, Dongwei Xiang1, Shuyu Zheng1, Deyu Tian1, Xuanzhe Liu1\n1Key Lab of High-Confidence Software Technology, MoE (Peking University), Beijing, China\n2Tsinghua University, Beijing, China\n{mayun,xdw,zhengshuyu,tiandeyu,xzl}@pku.edu.cn\nABSTRACT\nRecently, several JavaScript-based deep learning frameworks have\nemerged, making it possible to perform deep learning tasks directly\nin browsers. However, little is known on what and how well we can\ndo with these frameworks for deep learning in browsers. To bridge\nthe knowledge gap, in this paper, we conduct the first empirical\nstudy of deep learning in browsers. We survey 7 most popular\nJavaScript-based deep learning frameworks, investigating to what\nextent deep learning tasks have been supported in browsers so\nfar. Then we measure the performance of different frameworks\nwhen running different deep learning tasks. Finally, we dig out\nthe performance gap between deep learning in browsers and on\nnative platforms by comparing the performance of TensorFlow.js\nand TensorFlow in Python. Our findings could help application\ndevelopers, deep-learning framework vendors and browser vendors\nto improve the efficiency of deep learning in browsers.\nKEYWORDS\nDeep learning; Web browser; Web applications; Measurement\n1\nINTRODUCTION\nIn the past decade, the advance of deep learning (DL) technique\nhas significantly promoted the artificial intelligence (AI). Numer-\nous AI applications, e.g., image processing, object tracking, speech\nrecognition, and natural language processing, have raised urgent\nrequirements to adopt the DL. As a result, various libraries and\nframeworks, such as TensorFlow [18], Caffe [2], and CNTK [3],\nhave been proposed and applied in practice.\nHowever, developing AI applications powered by the popular\nDL frameworks and libraries is a non-trivial task. Usually, these\nframeworks and libraries are leveraged by native applications that\ncan run on heterogeneous development environments such as Win-\ndows, Linux, MacOS/iOS, and Android. The applications are devel-\noped by various imperative programming languages, i.e., C/C++ on\nWindows, Objective-C on iOS and MacOS, and Java on Android.\nDeveloping AI applications that is portable to multiple platforms is\nindeed not easy. The development is particularly complicated for\nmobile applications, as the app vendors usually need to develop\nand maintain both iOS and Android versions. In addition, the de-\nployment is also non-trivial, as most current platforms come with\nan appstore, some of which require manual testing of submitted\napplications by the appstore provider before being published-a pro-\ncess that can take several weeks-and applications can be rejected\nfor seemingly arbitrary reasons.\nWWW ’19, May 13–17, 2019, San Francisco, CA, USA\n2019. ACM ISBN 978-1-4503-6674-8/19/05.\nhttps://doi.org/10.1145/3308558.3313639\nCompared to the native applications, Web applications can in-\ndeed make the cross-platform portability issues much simpler. The\nsame implementation of a DL-powered Web application can be de-\nployed in the browser on all platforms regardless of the underlying\nhardware device types (PC, smartphones, and wearable devices)\nand the operating systems (Windows, Mac, iOS, and Android). Ad-\nvancements in HTML5, CSS3, and especially JavaScript language,\nstarted to enable the creation of DL-powered Web applications that\noffer a comparable experience to native applications, especially for\nthe popular Web game applications [38][34]. In particular, bene-\nfited from the development of WebGL [32][19][20], current major\nbrowsers such as Google Chrome, Mozilla FireFox, and Apple Sa-\nfari, can better utilize the integrated graphics card to accelerate DL\ntasks, without the need of standalone graphics card like NVIDIA\nwhich is required by native DL frameworks.\nRunning DL-powered Web applications in browsers has drawn\nthe attention from various research communities including AI, soft-\nware engineering, Web browsers, and even computer architecture.\nAs a result, various JavaScript-based DL development frameworks\nand libraries have been published. In 2015, Karpathy presented the\nConvNetJS [4], known as the first JavaScript library for DL in Web\nbrowsers to date. Other efforts such as WebDNN [15], Keras.js [5],\nand Mind [7], were proposed to support DL in browsers. In early\n2018, Google released the TensorFlow.js [14], which is a significant\nstep for promoting the DL in browsers.\nAlthough the preceding efforts along with some on-going efforts\nseem to make running DL tasks in browsers possible, little is known\non what DL tasks we can do and how well DL works in browsers.\nMore importantly, considering the long debate of performance of\nWeb applications compared with that of native applications, the\nsame issue also exists in developing DL-powered Web applications.\nHence, it is urgent to address such a knowledge gap in terms of the\nfeasibility and usability for running DL in Web browsers.\nIn this paper, we make the first empirical study of DL in browsers\nby answering the following research questions.\n• RQ1: What features do existing frameworks provide to im-\nplement various kinds of DL tasks in the browser?\n• RQ2: How well do existing frameworks perform over differ-\nent DL tasks?\n• RQ3: How big is the performance gap between running DL\nin the browser and on the native platform?\nWe select 7 popular JavaScript-based frameworks that support\nrunning DL in browsers, and conduct a characteristic study over\nthem. We develop a browser extension to measure the performance\nas well as the utilization of system resources when running different\nDL tasks. We choose the TensorFlow.js and native TensorFlow in\nPython to compare the performance of DL in browsers with that\non native platforms.\narXiv:1901.09388v2  [cs.SE]  24 Mar 2019\nThe key findings of our study includes:\n• DL in browsers is still at dawn. Most frameworks of DL in\nbrowsers support only a specific subset of DL tasks. Among all the\nframeworks, TensorFlow.js provides the most number of function-\nalities to realize various kinds of DL tasks.\n• Support of training in browsers is not fledged. In most frame-\nworks, inference has drawn more attention compared with training.\nFor training tasks, the number of neurons per layer dominates the\nperformance variation considering the complexity of DL models\nsince the browser is limited in complex matrix calculation.\n• Model loading dominates the computation for inference\ntasks. Loading and warming up the DL model costs more time\nthan running the inference task itself. The CPU backend performs\nbetter than the GPU backend when the browser run inference tasks\nfor small-size models.\n• Integrated graphics card helps browsers to beat native plat-\nforms when standalone graphics card is not available. For\npopular pre-trained models like MobileNet and Inception, Tensor-\nFlow.js is just 1x to 2x slower than native TensorFlow in Python\nwhen running inference tasks on the standalone graphics card. Ten-\nsorFlow.js on the integrated graphics card outperforms the native\nTensorFlow on CPU when running the same inference task.\n• System resources can be further exploited for in-browser\nDL tasks. For TensorFlow.js, the CPU is not fully utilized (about\n80%) when DL tasks run on the CPU backend. The memory allocated\nto WebGL is limited by the browser, leading to the crash of some\nDL tasks.\nBased on the findings, We have drawn some practical recommen-\ndations for application developers as well as DL-framework and\nbrowser vendors. Application developers who aim to develop DL-\npowered Web applications, need to better control the the number\nof neurons per layer in DL models, pre-load the model file in ad-\nvance, and employ the CPU backend rather than the GPU backend\nwhen running inference tasks on small DL models. DL-framework\nvendors should consider encoding the model file in binary format\nrather than JSON to reduce the file size as well as improve the\ncomputation time, and leverage compiler optimization techniques\nto reduce the call stack. Browser vendors should consider support-\ning multi-process and scheduling over multi-core in the JavaScript\nengines.\nThe remainder of this paper is organized as follows. Section 2\nshows some background knowledge of deep learning in browsers.\nSections 3 to 5 describe the results, including the analysis of frame-\nwork functionality, performance measurement, and comparison\nwith native DL frameworks. Section 6 presents the implications\nand recommendations drawn from the findings. Section 7 surveys\nrelated work and Section 8 concludes the paper with future work.\n2\nBACKGROUND\nIn this section, we give some background of deep learning and then\ndiscuss how browsers support deep learning tasks.\n2.1\nDeep Learning\nDeep learning (DL) is a class of machine learning algorithms that\nuse a cascade of multiple layers of nonlinear processing units (called\nneurons) for feature extraction and transformation. Each successive\nlayer uses the output from the preceding layer as input. In recent\nyears, DL has gained great success in many areas such as computer\nvision, speech recognition and natural language processing.\nThere are many types of DL models, among which deep neural\nnetwork (DNN) [23], convolutional neural network (CNN) [28],\nand recurrent neural network (RNN) [35] are three basic structures.\nDNN is a typically feedforward network with multiple layers be-\ntween the input and output layers, in which data flows from the\ninput layer to the output layer without looping back. CNN uses a\nvariation of multi-layer perceptrons designed to require minimal\npreprocessing, usually applied to analyzing visual imagery. RNN\nhas connections between nodes forming a directed graph along a\ntemporal sequence, allowing it to exhibit temporal behaviors.\nDL consists of two phases: training phase where the input data\nare used to calculate the parameters of the model, and inference\nphase where the trained model outputs the value given a specific\ninput sample.\n2.2\nDeep Learning in Browsers\nRecently, there is a trend that applications perform DL tasks directly\non the clients for better privacy and timely response. As a cross-\nplatform client-side computation target, Web browsers have drawn\nthe attention to AI communities to support client-side DL. Sev-\neral applications of in-browser DL are implemented and published,\nsuch as 1) TensorFlow playground [13], which is an interactive\nplatform to learn the principle of DL; 2) Teachable Machine [12],\nwhich gives the users an experience of teaching the machine how\nto response when they pose a gesture, using camera in the browser;\n3) MLitB [33], which is capable of performing distributed learning\nwith heterogeneous classes of devices using Web browsers; 4) Mor-\nphCast [9], which combines interactive video and face recognition\nwith emotion, gender and age analysis to create adaptive-media.\nDL in browsers is implemented by JavaScript and rely on the\nbrowser engine to execute. Fortunately, the advancement of latest\nbrowsers provides APIs to access GPU, which can be used to ac-\ncelerate matrix calculations of DL. These APIs are: 1) WebGL [16],\nwhich is a JavaScript API for rendering interactive 2D and 3D graph-\nics within any compatible Web browser; 2) WebGPU [17], which is\nthe fastest among existing JavaScript APIs for accelerating graphics\nand computation. Currently, WebGPU API is supported only in\nSafari Technology Preview. We should mention that WebGL and\nWebGPU can run on both integrated graphics cards and standalone\ngraphics cards.\n3\nSUPPORTED FEATURES OF DEEP\nLEARNING IN BROWSERS\nIn this section, we make a characteristic study to answer the first\nresearch question, i.e., what features do existing frameworks pro-\nvide to implement various kinds of DL tasks in the browser? We\nfirst introduce the frameworks selected for the study. Then we com-\npare the features of these frameworks from two aspects: provided\nfunctionality and developer support. For provided functionality,\nwe mainly examine whether each framework supports some basic\nfunctionalities that are commonly used in the development of DL\napplications. For developer support, we take a look at some factors\nwhich may affect the efficiency of developing and deploying DL\napplications. Table 1 summarizes all the results as of Nov. 2018.\n3.1\nSelected Frameworks\nTo select the state-of-the-art frameworks of supporting DL in browsers,\nwe search on the GitHub with the keyword “deep learning frame-\nwork” and filter the results in JavaScript language. Then we choose\nthe top 7 frameworks of which the number of stars exceeds 1,000\non GitHub. We introduce each framework as follows.\nTensorFlow.js [14], released by Google in Mar. 2018, is an in-\nbrowser machine learning library that supports defining, training,\nand running models entirely in the browser using JavaScript. It is\nthe successor to deeplearn.js which is now called TensorFlow.js\nCore. TensorFlow.js is powered by WebGL and provides high-level\nAPIs for defining models. TensorFlow.js support all the Keras layers\n(including Dense, CNN, LSTM, and so on). Therefore, it is easy to\nimport models pre-trained by the native TensorFlow and Keras into\nthe browser and run with Tensorflow.js.\nConvNetJS [4] is a Javascript library originally written by Andrej\nKarpathy at Stanford. The entire library is based on transforming\n3-dimensional volumes of numbers. ConvNetJS currently supports\ncommon neural network models and cost functions for classifica-\ntion and regression. Furthermore, it supports convolutional net-\nworks, and an experimental reinforcement learning. Unfortunately,\nalthough ConvNetJS might be the most famous framework before\nTensorFlow.js, it is no longer maintained after Nov. 2016.\nKeras.js [5] abstracts away a number of frameworks as backends\nincluding TensorFlow, CNTK, etc. It supports importing models\npre-trained by Keras for inference. In the GPU mode, computation\nis performed by WebGL. However, this project is no longer active.\nWebDNN [15], released by the University of Tokyo, claims to be\nthe fastest DNN execution framework in browsers. It supports only\nthe inference tasks. The framework supports 4 execution backends:\nWebGPU, WebGL, WebAssembly, and fallback pure JavaScript im-\nplementation. WebDNN optimizes DNN models by compressing\nthe model data to accelerate the execution. Empirical evaluations\nshowed that it achieved more than 200x acceleration [6].\nbrain.js [1] is a JavaScript library for neural networks replacing\nthe deprecated “brain” library. It provides DNN, RNN, LSTM and\nGRU for training tasks. The library supports serializing and loading\nthe state of a trained DL model with JSON.\nsynaptic [11] is a JavaScript architecture-free neural network li-\nbrary, supporting basically any type of first order or even second\norder RNN. This library also includes a few built-in DL architectures,\nincluding multi-layer perceptrons, LSTM, liquid state machines and\nHopfield networks.\nMind [7] is a flexible neural network library. The core framework\nhas only 247 lines of code, which uses a matrix implementation\nto process training data. It supports customization of the network\ntopology and plugins to configure pre-trained models created by\nthe mind community. However, this framework is no longer active.\n3.2\nProvided Functionality\nSupport for training. Most frameworks support training and in-\nference tasks in the browser. However, Keras.js and WebDNN do not\nsupport training DL models in browsers. They support only load-\ning pre-trained models to perform inference tasks. Therefore, the\nnumber is not available for the types of layer/activation/optimizer\nsupported by Keras.js and WebDNN in Table 1.\nSupported network types. Some frameworks are not for general-\npurpose DL tasks, so they differ in the supported network types.\nSpecifically, TensorFlow.js, Keras.js and WebDNN support three\nnetwork types: DNN, CNN and RNN. However, ConvNetJS mainly\nsupports CNN tasks and does not support RNN. brain.js and synap-\ntic mainly support RNN tasks, and do not support convolution and\npooling operations used in CNN networks. Mind supports only the\nbasic DNN.\nSupported layer types. All frameworks support building neural\nnetworks using units of layers. The layer API of TensorFlow.js\nsupports 49 different layers, including dense, convolution, pooling,\nRNN, normalization, and so on. Other frameworks support a smaller\nvariety of layers, which are also related to the network types they\nsupport. It should be noted that the core API of TensorFlow.js is\nimplemented in a way similar to the native TensorFlow which com-\nbines various operations to build computational graphs. synaptic is\nan architecture-free framework that supports building any type of\nfirst order or even second order RNN networks.\nSupported activation/optimizer types. In general, TensorFlow.js\nprovides developers with the most kinds of choices. For activation\nfunctions, other frameworks support only basic sigmoid or ReLU.\nFor optimizers, other frameworks mainly support basic stochastic\ngradient descent (SGD).\nSupport for GPU acceleration (WebGL). TensorFlow.js is the\nonly framework that supports GPU-accelerated training tasks. Ten-\nsorFlow.js, Keras.js, and WebDNN support using GPU to accelerate\ninference tasks. WebDNN also supports a more advanced tech-\nnology, WebGPU, but WebGPU has been supported by only the\ntechnology preview version of Safari.\n3.3\nDeveloper Support\nDocuments. Documents provided by TensorFlow.js, ConvNetJS,\nWebDNN and synaptic are completed and in detail. The document\nof Keras.js is not complete and brain.js has only a few tutorials.\nDemos. All the frameworks provide demos for developers to get\nstart. TensorFlow.js offers the richest demos covering a wide range\nof use cases.\nImporting models from other frameworks. TensorFlow.js, Keras.js\nand WebDNN support importing models from native DL frame-\nworks in Python and all of them provide Python scripts for convert-\ning models. TensorFlow.js supports models trained by TensorFlow\nand Keras. Keras.js supports Keras models. WebDNN supports im-\nporting models from TensorFlow, Keras, Caffe and Pytorch. With\nthe support of using pre-trained models from other DL frameworks,\nthe development effort can be significantly reduced.\nAPI to save/load model. All frameworks that support training\ntasks in the browser have APIs for saving models. All frameworks\nhave APIs for loading models.\nSupport for server side (Node.js). All frameworks are supported\nfor Node.js. Such a feature makes it possible to offload computation\ninside browsers onto remote servers.\nTable 1: Characteristics of JavaScript-based frameworks that support deep learning in browsers.\nTensorFlow.js\nConvNetJS\nKeras.js\nWebDNN\nbrain.js\nsynaptic\nMind\nBasic Information\nGithub Stars\n9453\n9364\n4348\n1464\n6366\n6315\n1333\nMain Contributor\nGoogle\nStanford\nUniversity\nLeon Chen\nThe University\nof Tokyo\nRobert\nPlummer\nJuan\nCazala\nSteven\nMiller\nLast Commit Date\nOct 30, 2018\nNov 25, 2016\nAug 17, 2018\nOct 25, 2018\nNov 5, 2018\nMar 25, 2018\nJul 7, 2017\nStatus\nActive\nNot Active\nNot Active\nActive\nActive\nActive\nNot Active\nFunctionality\nSupport for Training\nY\nY\nN\nN\nY\nY\nY\nSupported\nNetwork Types\nDNN\nY\nY\nY\nY\nY\nY\nY\nCNN\nY\nY\nY\nY\nN\nN\nN\nRNN\nY\nN\nY\nY\nY\nY\nN\nSupported Layer Types\n49\n7\nNA\nNA\n7\n1\n1\nSupported Activation Types\n16\n4\nNA\nNA\n4\n5\n2\nSupported Optimizer Types\n7\n3\nNA\nNA\n1\nNA\nNA\nSupport for GPU Accelaration (WebGL)\nY\nN\nY\nY\nN\nN\nN\nDeveloper Support\nDocuments\nY\nY\nNot finished\nY\nOnly tutorials\nY\nY\nDemos\n20\n10\n9\n8\n7\n7\n4\nImporting Models from\nOther Frameworks\nTensorFlow\nY\nN\nN\nY\nN\nN\nN\nKeras\nY\nN\nY\nY\nN\nN\nN\nCaffe&Pytorch\nN\nN\nN\nY\nN\nN\nN\nAPI to Save/Load Model\nSave\nY\nY\nN\nN\nY\nY\nY\nLoad\nY\nY\nY\nY\nY\nY\nY\nSupport for Server Side (Node.js)\nY\nY\nY\nY\nY\nY\nY\nLibrary Size\n732KB\n33KB\n650KB\n130KB\n819KB\n106KB\nNA\nLibrary size. We list the size of the library files that need to be\nloaded into browsers. ConvNetJS is the smallest, which is just 33KB.\nTensorFlow.js and brain.js have very large size of files, which are\n732KB and 819KB, respectively. Small-size libraries are better for\nloading applications in browsers since all the files have to be down-\nloaded on demand.\n4\nPERFORMANCE OF DEEP LEARNING IN\nBROWSERS\nIn this section, we conduct a measurement study to investigate the\nsecond research question, i.e., how well do existing frameworks\nperform over different DL tasks? We study the influence of model\ncomplexity and backend processor (CPU or GPU) on the perfor-\nmance when the browser runs training and inference tasks.\n4.1\nExperiment Setup\nDL model. As explained before, the network types supported by\ndifferent frameworks are not the same. So we adopt the most basic\nfully connected neural network as the model in the experiment. For\nthe dataset to run the DL tasks, we use the classic MNIST handwrit-\nten digit recognition database [8]. The model to be trained has 784\ninput nodes and 10 output nodes. To study the influences of model\ncomplexity on the performance, we choose different configurations\nof the model. The parameters include 1) the number of the hidden\nlayers (depth) of the neural network, which ranges in [1, 2, 4, 8],\nand 2) the number of neurons (width) in each hidden layer, which\nranges in [64, 128, 256]. The range of depth and width is set based\non the assumption that client-side DL models should be of small\nsize, being able to run on the client. In the training process, the\nbatch size is always set to 64.\nHardware. In order to study the performance difference between\nCPU and GPU backend, we use a Hasee T97E laptop computer,\nwhich has a standalone graphics card, Nvidia 1070 Max-Q (with\n8GB GPU memory). The CPU is Intel i7-8750H, which includes an\nIntel HD Graphics 630, enabling us to measure the performance\nusing integrated graphics card. In the following, we use nGPU and\niGPU to denote the GPU backend on the standalone Nvidia graphics\ncard and the integrated Intel graphics card, respectively.\nSoftware. All the experiments run on the Chrome browser (version:\n71.0.3578.10 dev 64-bit) on Ubuntu 18.04.01 LTS (64-bit). For the\nframeworks, we use their latest published version.\nPerformance measurement. For each DL task, we implement a\nWeb page where the configurations of DL models can be varied\nthrough the parameters in the URL. We run each DL task on the\nChrome browser, and measure the time spent on finishing the task.\nSince each experiment usually requires running dozens of tasks\nunder different configurations, we developed a Chrome extension\nto iterate all the pages and change the configuration after one\ntask is performed. This browser extension is also responsible for\nmonitoring the system resource usage of the Web page. At the same\ntime, a local server records the experimental statistics uploaded by\nthe extension.\n4.2\nTraining Performance\nWe select four JavaScript frameworks, brain.js, ConvNetJS, synaptic,\nand TensorFlow.js, which support training in browsers, to compare\nFigure 1: Average training time (ms) on one batch under different model complexities. The y-axis is on log scale.\ntheir performance of running training tasks. All the four frame-\nworks can train models on the CPU backend except that Tensor-\nFlow.js is also able to use the GPU backend via WebGL. We train the\ndefined model using each framework and obtain the average time\nspent on training one batch. Figure 1 shows the results under differ-\nent model complexities. Since the training time of synaptic is about\ntens to hundreds of times longer than that of other frameworks, we\nomit the result of synaptic in the figure for better presentation but\nthe findings are similar to other frameworks.\nIn general, the training time increases with the increase of the\nnetwork size since more computation is needed to complete the\ntraining process for larger networks. Comparing the training time\nof different frameworks on the CPU backend, we can see that Con-\nvNetJS is the fastest among all the frameworks for all network\nconfigurations. The possible reason may be that ConvNetJS is de-\nsigned to be simpler, which can be reflected by its small library file\nsize. Brain.js is closely behind, with a performance gap of about two\ntimes (2x) with ConvNetJS. Tensorflow.js has a performance gap\nof two to three times (2x-3x) with ConvNetJS. When comparing\nthe training time ratio of ConvNetJS over TensorFlow.js, we find\nthat the performance gap is gradually reduced when the depth and\nwidth increase, indicating that compared with ConvNetJS, Tensor-\nFlow.js has relatively large overhead beyond calculation. In addition,\nthe performance gap is larger as the network width increases than\nas the network depth increases, implying that TensorFlow.js deals\nbetter with large-scale matrix calculation than ConvNetJS.\nGPU benefits. The training time on the CPU backend becomes\nlonger with the increase of network size, but the results on the\nGPU backend are not the same. For both the iGPU with weaker\ncomputation power and the nGPU which can satisfy larger-scale\nmatrix calculations, the training time does not increase significantly.\nBut in the process from (4 hidden layers, 128 neurons per layer) to\n(8 hidden layers, 256 neurons per layer), the training time of iGPU\nincreases significantly. The reason may be that under the network\nsize set in this experiment, the training process does not reach\nthe GPU’s capability bottleneck. Although the matrix computation\ncapability of nGPU is better than that of iGPU, the training time\non nGPU is even longer than iGPU. Such a result is caused by the\nexcessive time overhead to call the WebGL for accessing GPU. The\nreal computation time of GPU should be much shorter.\nSystem resource utilization. We show the statistics of CPU uti-\nlization of each framework during the training process in Table 2.\n110% is the upper bound of CPU utilization. The capability of multi-\ncore processor cannot be used since the JavaScript engine is single-\nthreaded. As a result, it can only maximize the usage of a single\nTable 2: CPU utilization (%) in the training process.\nFramework\nBackend\nMax\nMin\nAverage\nbrain.js\nCPU\n104.0\n99.9\n101.2\nConvNetJS\nCPU\n108.0\n101.9\n104.1\nsynaptic\nCPU\n113.9\n88.7\n102.8\nTensorFlow.js\nCPU\n108.0\n61.0\n82.1\niGPU\n82.0\n54.9\n65.9\nnGPU\n75.9\n48.0\n60.0\ncore. The reason why the CPU utilization is over 100% is that other\nkernel and user space components occasionally run simultaneously\nin other threads.\nOn the CPU backend, TensorFlow.js sometimes cannot maximize\nthe utilization of a single core and its average CPU utilization is\nonly 82.1%. Meanwhile, we can find that when running training\ntasks on the GPU backend, CPU is not fully utilized since most\ncomputation is on the GPU. Training on iGPU has about 5-7%\nhigher CPU utilization than that on nGPU.\n4.3\nInference Performance\nWe select 6 JavaScript frameworks to compare their performance\nof running inference tasks. TensorFlow.js, Keras.js, and WebDNN\nsupport using GPU for acceleration, but brain.js, ConvNetJS, and\nsynaptic support using only CPU for inference. In terms of model\nusage, brain.js, ConvNetJS, synaptic and TensorFlow.js support sav-\ning their own trained models, while Keras.js and WebDNN only\nsupport importing pre-trained models from other deep learning\nframeworks. Therefore, for brain.js, ConvNetJS, synaptic and Ten-\nsorFlow.js, we use the models saved by the frameworks themselves.\nFor Keras.js and WebDNN, we use the models trained by Keras and\nthen convert the models to the corresponding format. Theoretically,\nthe parameter values of the trained DL models should be different,\nbut the absolute value does not affect the inference time. So we\njust assign the same parameter values to all the models of different\nframeworks.\nThe inference task involves loading a pre-trained model and\nthen given a sample input, the model outputs the result. In addition,\non the GPU backend, there is a warmup process where the first\nsample for inference is usually used to activate the GPU processor.\nTherefore, we break down the inference process into three phases:\nmodel loading, warming up, and inference, and study the fine-\ngrained performance. Due to the space limitation, we omit the\nresults where the model depth is 8 in the following analysis because\nthe trend is similar as the depth increases. Besides, since the model\nFigure 2: Model loading time (ms) under different model complexities. The y-axis is on log scale.\nTable 3: Size of model files (MB).\nDepth\nWidth\nbrain.js\nConvNetJS\nsynaptic\nTensorFlow.js\n1\n64\n1.4\n1.3\n3.4\n0.2\n128\n2.7\n2.7\n6.7\n0.4\n256\n5.5\n5.4\n13.3\n0.8\n2\n64\n1.5\n1.5\n3.7\n0.2\n128\n3.2\n3.1\n7.8\n0.5\n256\n7.2\n7.1\n17.7\n1.1\n4\n64\n1.7\n1.7\n4.2\n0.3\n128\n4.0\n4.0\n10.1\n0.6\n256\n10.7\n10.5\n26.5\n1.6\nloading time and inference time of synaptic are still much longer\nthan those of other frameworks, we do not depict the results of\nsynaptic in the figures for better presentation.\nModel file size. We first investigate the size of the model file used\nby different frameworks. As models for inference usually should\nbe downloaded from the remote server, smaller size of model files\nmeans shorter downloading time. Table 3 shows the size of model\nfiles that are used in all inference experiments. ConvNetJS and\nbrain.js use similar JSON encoding, so the size of their model files\nare nearly the same. The model file of synaptic uses JSON encoding\nas well but its size is the largest among all the frameworks. As\nthe model files used by TensorFlow.js, Keras.js and WebDNN are\nall converted from Keras models, their model files are of the same\nsize. So we just show TensorFlow.js in the table. Since the model\nconverted from Keras is compressed and saved as a binary file, the\nsize can be greatly reduced, just about 1/7 of the model file in JSON.\nModel loading time. We then compare the time spent on loading\nthe model of different frameworks, as shown in Figure 2. For the\nCPU backend, the loading time of different models of the same\nframework is proportional to the size of the model files described in\nTable 3. However, the model loading time of different frameworks\nis significantly different. ConvNetJS is the fastest. Model loading\ntime of brain.js, TensorFlow.js and Keras.js are consistent in terms\nof magnitude. Interestingly, the increase of loading time of Con-\nvNetJS, brain.js and synaptic is particularly noticeable when the\nwidth increases. The result is caused by their choice of using JSON\nto encode models. The model loading time of synaptic is slowest\namong all the frameworks, which are more than 100x to 1000x\nlonger than ConvNetJS. The model loading time of TensorFlow.js\nis almost unchanged regardless of the model size.\nThe loading time on the GPU backend does not change much un-\nder different model complexities. However, the difference is still sig-\nnificant between different frameworks. TensorFlow.js is the fastest.\nCompared with loading models on the CPU backend, Keras.js speeds\nup loading large models, but the loading time of WebDNN is longer.\nIn addition, it can be seen that there is no difference in the model\nloading time between iGPU and nGPU.\nWarmup time. Next, we examine the difference of warmup time\non the GPU backend. As shown in Figure 3, Keras.js is still far ahead,\nand can complete the warmup in 3ms on all tasks. Tensorflow.js is\nthe second, and WebDNN is the worst. On the whole, the warmup\ntime on iGPU backend is shorter than that on nGPU.\nInference time. Figure 4 shows the average time of doing inference\non one sample. Almost all the inference tasks can finish within 1.5ms\n(except synaptic, of which the shortest is 6.68ms). In the range\nof the model sizes we set, the powerful computation capability\nof GPU does not make a difference. Among all the model sizes,\nConvNetJS occupies all the first place, followed by WebDNN on\nthe CPU backend. The inference time of WebDNN on the GPU\nbackend is longer than the inference time on the CPU backend.\nAs for TensorFlow.js, running on the CPU backend is faster for\ninference on smaller models, while the GPU backend is faster for\ninference on larger models. Inference times of Keras.js on the CPU\nand GPU backend are basically the same.\nWe can observe that for all the frameworks on the CPU backend,\nthe inference time increases when the model becomes complex. In\nparticular, when the width increases, the time increases sharply\n(about two times as the model width doubles). Similar to the train-\ning tasks, such a result also reflects that these frameworks do not\noptimize the large-scale matrix operations in the process of forward\npropagation on the CPU backend. TensorFlow.js and WebDNN on\nFigure 3: Model warmup time (ms) on GPU under different model complexities. The y-axis is on log scale.\nFigure 4: Average inference time (ms) on one sample under different model complexities.\nthe GPU backend do not exhibit this problem, but Keras.js on the\nGPU still suffers from this problem.\n4.4\nTakeaway\nBased on the above results, we can see that in small-scale fully-\nconnected neural network which the browser is capable of, Con-\nvNetJS performs the best for both training and inference. However,\nsince ConvNetJS is no longer maintained and has fewer functional-\nities, developers may need to choose some alternatives.\nTensorflow.js is the only framework that can take advantage\nof GPU to accelerate training processes. It is feature-rich and has\ncomparable performance with ConvNetJS. So TensorFlow.js is a\ngood choice for both training and inference. We do not recommend\nusing GPU as the backend on small models because the advantage\nof GPU’s computation power is not fully exploited.\nFinally, we are interested in why ConvNetJS has the best perfor-\nmance for all the tasks among these frameworks. Given the same\nmodel of which the process logic is the same, the performance\ndifference is likely to be accounted by the different implementation\ndetails. To this end, we compare the function call stack of ConvNetJS\nwith that of TensorFlow.js when doing the same training task. It is\nsurprising to find that the depth of the call stack of ConvNetJS is\nonly 3 while TensorFlow.js is 48! Such a result suggests that one\npossible reason for the performance difference among different\nframeworks is the deep call stack that costs a lot of computation\nresources.\n5\nCOMPARISON WITH NATIVE\nFRAMEWORK\nIn this section, we study the third research question, i.e., how big\nis the performance gap between running DL in the browser and\non the native platform? To this end, we compare the performance\nof TensorFlow.js and the native TensorFlow in Python, both of\nwhich are released and maintained by Google and have similar\nAPIs, making the comparison fair enough.\nWe study the performance gap from two aspects. On one hand,\nwe leverage well-known pre-trained models to compare the perfor-\nmance when running inference tasks on TensorFlow.js and native\nTensorFlow. On the other hand, we use decision tree analysis to\ndistinguish the factors contributing to the performance gap. We\nuse the same laptop as the one used in the experiments of the last\nsection. We install the latest TensorFlow in Python (version 1.11.0)\non the laptop.\n5.1\nInference Based on Pre-Trained Models\nWe use the pre-trained models officially provided by the Keras to\nmeasure the performance of TensorFlow.js and native TensorFlow\nwhen doing inference tasks on these classical models.\n5.1.1\nLimitations of TensorFlow.js and browser constraints. Keras\nofficially provides 11 pre-trained models. Although these models\ncan work using native TensorFlow, we encountered a series of errors\nwhen we run them using TensorFlow.js in the browser. These errors\nTable 4: Selected Keras pre-trained models.\nModel Name\nPre-trained\nModel Size\nTrainable\nParameters\nComputation\n(FLOPs)\nMobileNetV2\n14MB\n3.5M\n7.2M\nDenseNet121\n33MB\n8.0M\n16.3M\nXception\n88MB\n22.9M\n46.0M\nInceptionV3\n92MB\n23.8M\n47.8M\nResNet50\n99MB\n25.6M\n51.4M\nFigure 5: Inference time on pre-trained Keras models. The\ny-axis is on log scale.\nimply the limitations of TensorFlow.js itself as well as constraints\nimposed by the browser.\nFor the model of NasNet Large, the browser throws out the error\nmessage “truncatedNormal is not a valid Distribution”. For the\nmodel of ResNet V3, the browser throws out the error message\n“Unknown layer: Lambda”. The reason for these two errors is that\nTensorFlow.js is still under development and so far has offered only\na limited number of support for the converted model. Many user-\ndefined operations are not supported by TensorFlow.js, e.g., models\nwith control flow operations in RNNs are not yet supported.\nWhen we try to use VGG16 or VGG19, the browser throws out\nthe error message “GL OUT OF MEMORY”, meaning that the GPU\nmemory is overfilled. The reason is that the VGG16 model applies\nfor more than 1GB GPU memory. However, it should not be an\nissue since the GPU memory of our experiment laptop is 8GB. As a\nresult, such an error is due to the browser constraints.\nAfter trying all the models, we finally have 5 models that can\nbe correctly converted and run on the browser. The information\nof these models are listed in Table 4. The number of trainable pa-\nrameters is obtained by the build-in summary() method of tensor-\nflow.keras, and the computation complexity (Floating Operations)\nare obtained by tensorflow.propfiler.profile() method.\n5.1.2\nResults. Figure 5 shows the inference time for each model.\nIt can be seen that the inference time of TensorFlow.js on nGPU\nis comparable (1x-2x slower) to native TensorFlow’s. The most en-\ncouraging result is that the performance of TensorFlow.js on the\niGPU backend is better than that of native TensorFlow on the CPU\nbackend. This result is not surprising considering the computation\ncapability of integrated graphics card and CPU. However, since\ntraditional native DL frameworks do not support integrated graph-\nics card for acceleration, DL tasks can benefit a lot from browsers\nin such a case with the help of integrated graphics card that is\ncommon on current devices.\nUnder the real-time requirement of client-side DL, if users want\nto achieve the experience of 10FPS (frame per second), they need\nTable 5: Contributing factors to the performance gap.\nNetwork Type\nFactor\nRange\nDNN\nBackend\nCPU, GPU\nTask Type\ntraining, inference\nDepth\n1, 2, 4, 8, 16\nWidth\n64, 128, 256, 512\nCNN\nBackend\nCPU, GPU\nTask Type\ntraining, inference\nDepth\n6, 9, 15, 27\nWidth\n200, 400, 800\nRNN\nBackend\nCPU, GPU\nTask Type\ntraining, inference\nDepth\n1, 2, 3\nWidth\n4, 8, 16, 32, 64, 256\nto consider using a more powerful standalone graphics card. The\nMobile Net model accelerated by iGPU can also meet the require-\nment. If the requirement is 1FPS, iGPU is also fully capable. But if\nonly CPU can be used, then these common models are too heavy\nto run in browsers.\n5.2\nDecision Tree Analysis\nIn order to deeply reveal how different factors of DL tasks influ-\nence the performance gap between DL in browsers and on native\nframeworks, we build a predictive model based on decision tree\nanalysis to study the factor importance.\n5.2.1\nExperiment Setup. We consider 4 factors that influence the\nperformance gap between DL in browsers and on native platforms\nas shown in Table 5, including backend (CPU or GPU), task type\n(training or inference), as well as depth and width that represent\nthe model complexity. In the DNN and RNN models, width refers\nto the number of neurons of each layer. In the CNN models, width\nrefers to the number of kernels used in the convolution layer. For\neach of DNN, CNN and RNN, we choose one model from the Ten-\nsorflow.js official examples. The DNN and CNN models are used to\nrecognize handwritten digits on the MNIST dataset, and the RNN\nmodel is to perform text generation from Nietzsche’s writings. The\nrange of depth and width is selected according to the values set in\nTensorflow.js official examples.\nIn our experiment, we build and run the DNN, CNN and RNN\nmodels under different configurations using TensorFlow.js and na-\ntive TensorFlow, respectively. Each configuration is a combination\nof values for the factors above. We measure the execution time of\neach configuration as the average time per batch for training tasks\nand average time per sample for inference tasks on two platforms.\nWe use the ratio of the execution time on TensorFlow.js over that\non native TensorFlow to quantify the performance gap.\n5.2.2\nMethodology. We run the decision tree algorithm with sklearn [10]\nto predict the ratio of execution time between TensorFlow.js and\nnative TensorFlow. The decision tree depicts the relative impor-\ntance of contributing factors. Intuitively, factors close to the root\nof the decision tree affect the time ratio more than those near the\nleaves. This is because the decision tree chooses to do the splitting\nof the nodes according to the Entropy-Information Gain criterion.\n>192\n<=3\n<=12\n>12\nCPU\ntraining\n<=192\n<=192\n<=12\n>192\n<=192\n>3\n<=1.5 >1.5\n<=384\n>384\n<=12\n>12\n>12\n>192\ntraining\ninference\ninference\nGPU\n5.2\n35.3\n20.9\n28.3\n4.8\n7.6\n3.7\n4.4\n1.6\n2.1\n3.1\n3.4\n44.7\nBackend\nTask\nWidth\nWidth\nWidth\nDepth\nDepth\nWidth\nTask\nDepth\nDepth\nDepth\nDepth\n(a) DNN\n>600\n>7.5\n>12\n<=7.5\n<=7.5 >7.5\nCPU\ntraining\n<=600\n<=600\n<=600\n<=21\n>7.5\n<=12\n<=7.5\n>21\n<=12\n<=600\n>600\n>600\n>12\n>600\ntraining\ninference\ninference\nGPU\n7.3\nDepth\n609.2\n334.0\n2268.4\n81.6\n4.9\n53.9\n6.0\n1.5\n1.0\n1.7\n5.5\n4.8\n14.6\nBackend\nTask\nWidth\nWidth\nDepth\nWidth\nDepth\nWidth\nDepth\nTask\nDepth\nDepth\n(b) CNN\n>96\n>1.5\n>1.5\n<=192\nCPU\ntraining\n\u0003\u0004\u0002\u0001\n<=48\n<=1.5\n<=96\n>192\n<=1.5\n<=1.5\n>96\n<=1.5\n>1.5\n>1.5\n>48\ntraining\ninference\ninference\nGPU\n4.9\n119.7\n30.8\n42.4\n7.8\n4.5\n2.3\n2.6\n3.3\n2.0\n2.9\n7.6\nBackend\nTask\nWidth\nWidth\nWidth\nDepth\nDepth\nWidth\nTask\nDepth\nDepth\nDepth\nDepth\n(c) RNN\nFigure 6: Decision tree to analyze the time ratio of TensorFlow.js over native TensorFlow on DNN, CNN, and RNN Models.\nIn other words, the decision tree places the important factors near\nthe root to gain the best prediction of splits.\nBased on the results, we first produce a fully grown and unpruned\ndecision tree for all the factors. In this way, each leaf contains only\none configuration. Then we set the depth of the tree to the number\nof factors, in order to prevent using a factor several times on one\npath. Figure 6 shows the decision trees for DNN, CNN, and RNN.\n5.2.3\nResults. The execution time of TensorFlow.js is longer than\nnative TensorFlow in almost every configuration.\nBackend is the most important factor contributing to the per-\nformance gap. The ratio of execution time on the CPU backend is\nmuch higher than that on the GPU backend. For example, the ratio\ndecreases from 44.7 to 4.4 for training tasks when the DNN model\nwith depth over 3 and width over 192 runs on the GPU backend\ninstead of on the CPU backend. The extreme case happens on the\nCNN. On the CPU backend, there is a wide range of the ratio from\nbelow 5 to over 2200 (when depth is less than 7.5 and width is over\n600). However, when doing inference task on the GPU backend\nwith depth over 12 and width over 600, TensorFlow.js performs\nas fast as native TensorFlow. This is because CNN makes use of\nthe powerful computation capability of GPU when the model is\nlarge enough, yet not exceeding the upper bound of the browser\nmemory.\nThe second most important factor is task type for all the three\nmodels. Performing training tasks exhibits a higher ratio, while the\nperformance gap on inference tasks is small. For example, for the\nDNN model on the CPU backend, training tasks of TensorFlow.js\nperform 33.9 times slower than native TensorFlow on average, and\ninference tasks of TensorFlow.js perform 5.8 times slower than\nnative TensorFlow on average.\nThe decision trees of DNN and RNN both suggest that the im-\nportance of depth and width depends on which backend the task\nis taken on. On the CPU backend, the importance of width out-\nweighs that of depth, while depth plays a more important role on\nthe GPU backend. However, in the case of CNN, width plays a more\nimportant role to the performance gap than depth for training tasks.\n6\nIMPLICATIONS\nTable 6 summarizes the findings and implications of our study.\nSpecifically, we draw implications for three stakeholders of DL\nin browsers: application developers, DL-framework vendors, and\nbrowser vendors. For application developers, we give recommen-\ndations on how to choose frameworks for DL in browsers, how\nto optimize the model, as well as how to select the backend. For\nDL-framework vendors, we present some advice on encoding of\nmodel files and optimizing the call stack. For browser vendors, we\nsuggest on the utilization of system resources.\n7\nRELATED WORK\nTo the best of our knowledge, this paper is the first study to char-\nacterize the DL in browsers. So we survey related work on general\nclient-side DL and performance measurement of DL systems.\n7.1\nClient-side Deep Learning\nWith the emphasis on privacy, personalization and timely response,\nit is a trend to conduct DL directly on the clients where the data\nis generated, especially on mobile devices. Lane et al. [27] studied\nthe feasibility of using DL for typical mobile sensing tasks, such\nas activity recognition. Yao et al. [40] proposed DeepSense, a uni-\nfied DL framework for processing time-series mobile sensing data.\nDespite of the increasing computation power of mobile devices,\ntypical DL tasks are still of heavy workload for these resource-\nconstraint devices. Several optimization methods were proposed\nto improve the performance of client-side DL. One line of work\nfocuses on the DL models. Han et al. [22] proposed deep compres-\nsion to compress the DNN through a three-stage method: pruning,\ntrained quantization and Huffman coding, which showed a consid-\nerable reduction in terms of the storage requirements of DNNs. The\nother line of work leverages the cloud and edge environment to\noffload the computation-intensive tasks to powerful computation\nnodes [29]. Kang et al. [26] proposed Neurosurgeon, a lightweight\nscheduler to automatically partition DNN computation between\nmobile devices and data centers at the granularity of neural net-\nwork layers. Wang et al. [39] designed Arden, a cloud-based deep\nlearning framework for mobile devices. The framework partitions\nthe DNN and offloads the resource-hungry training and complex\ninferences tasks to the cloud.\nAnther usage scenario of client-side DL is to support the dis-\ntributed deep learning. Teerapittayanon et al. [37] proposed dis-\ntributed deep neural networks (DDNNs) over distributed computing\nhierarchies, consisting of the cloud, the edge (fog) and end devices.\nIchinose et al. [24] proposed a pipelined method for distributed\nDL processing between mobile devices and the cloud to reduce\nthe amount of data sent to the cloud and protect the privacy of\nusers. Meeds et al. [33] designed MLitB, a prototype DL frame-\nwork capable of performing large-scale distributed computing with\nheterogeneous classes of devices using Web browsers.\nTable 6: Major findings and implications of DL in browsers.\nNo.\nName\nFinding\nImplication\nStakeholder\n1\nSpecific DL Tasks\nSupport\nFrameworks supporting DL in browsers are emerging and being actively\nmaintained. Most of them are not for general purpose and support only a\nspecific subset of DL tasks.\nIt is better for developers to use general-purpose DL\nframeworks like TensorFlow.js to implement their DL-\npowered Web applications.\nApplication\nDeveloper\n2\nModel Complex-\nity\nThe width of DL models dominates the performance variation of both\ntraining and inference tasks considering the complexity of DL models.\nDevelopers should pay attention to the width of their\nmodels, and balance the width and required perfor-\nmance if possible.\nApplication\nDeveloper\n3\nModel Loading\nFor inference tasks, loading and warming up the DL model accounts for\nmuch longer time than running the inference task itself. The warmup\ntime on the integrated graphics card is generally shorter than that on the\nstandalone graphics card.\nDevelopers should pre-load and warm up the model\nbefore using it for inference.\nApplication\nDeveloper\n4\nBenefits\nfrom\nGPU\nFor popular pre-trained models like MobileNet and Inception, TensorFlow.js\nhas comparable performance with native TensorFlow when running infer-\nence on the standalone graphics card.\nIt is possible to develop Web applications rather than\nnative applications for these tasks.\nApplication\nDeveloper\n5\nBenefits from In-\ntegared Graphics\nCard\nTensorFlow.js running on the integrated graphics card works better than\nnative TensorFlow running on CPU backend.\nFor devices without standalone GPUs, developers can\nuse the browser for DL tasks, leveraging integrated\ngraphics card for acceleration.\nApplication\nDeveloper\n6\nModel File Encod-\ning and Size\nModel file encoded in JSON is much bigger (7x) in size than that encoded\nin binary, and significantly increases the model loading time.\nIt is better to encode DL models in binary files.\nDL-\nFramework\nVendor\n7\nFramework Call\nStack\nThe call stack of TensorFlow.js is much deeper than that of ConvNetJS,\npulling down the performance.\nFramework vendors could leverage compiler optimiza-\ntion techniques to reduce the call stack when the DL\nmodels are used in the production environment.\nDL-\nFramework\nVendor\n8\nSystem Resource\nUtilization\nThe capability of multi-core CPU cannot be utilized when running DL tasks\non the CPU backend in browsers since the JavaScript program is single-\nthreaded. GPU memory usage is limited in 1GB, failing to load and run\nlarger models.\nJavaScript engine should take into account the support\nof multi-process or scheduling among multi cores for\nbetter performance of DL tasks in browsers. The GPU\nmemory should be configurable for DL tasks.\nBrowser\nVendor\n7.2\nPerformance Measurement of Deep\nLearning\nIn recent years, researchers have conducted studies to measure the\nperformance for various kinds of deep learning tasks. Liu et al. [30]\nevaluated the performance of leading DL methods for object detec-\ntion. Guignard et al. [21] presented detailed characterization results\nof a set of archetypal state-of-the-art DL workloads to identify the\nperformance bottlenecks and to guide the design of prospective\nacceleration platforms in a more effective manner. Shi et al. [36]\nevaluated the performance of four state-of-the-art distributed DL\nframeworks over different GPU hardware environments. They built\nperformance models of standard processes in training DNNs with\nSGD, and then benchmark the performance of the frameworks with\nthree neural networks (i.e., AlexNet, GoogleNet and ResNet-50). As\nfor DL on mobile devices, Ignatov et al. [25] studied state-of-the-art\nDL in the Android ecosystem and described available frameworks,\nprogramming models and the limitations of running AI on smart-\nphones.\nAlthough many JavaScript-based frameworks have been pub-\nlished to support DL in browsers, there is no comprehensive study to\nunderstand their characteristics and performance. Some researchers\nfocus on the possibility of supporting DL in browsers by measur-\ning the low-level browser capabilities. Malle et al. [31] presented a\ncomparison study between native code and different browser-based\nimplementations: JavaScript, ASM.js as well as WebAssembly on a\nrepresentative mix of algorithms. However, these algorithms are not\nDL tasks. Their goal is just to show that the browsers performance\nis now comparable to and even exceeds native binary performance.\n8\nCONCLUSION\nThis paper made the first study on understanding the feasibility and\nperformance of deep learning in Web browsers. We chose 7 recently\nemerging JavaScript-based DL frameworks and comprehensively\nrevealed which type of DL tasks have been supported. We measured\nthe performance of different frameworks when doing different DL\ntasks in browsers, and compared with the native DL framework to\ninvestigate the performance gap. Although the in-browser DL is\nstill at the early stage, some interesting findings, e.g., the compara-\nble performance of JavaScript frameworks to that of native ones on\nsome types of DL tasks and the benefits gained from the integrated\ngraphics card, can be useful and help guide the DL-powered Web\napplications. Additionally, we have also found that there are some\npotential space of improvement for currently in-browser DL frame-\nworks, and plan to realize some practical solutions. We believe that\nour work can shed a light on the future of Web applications in the\nAI era.\nACKNOWLEDGMENTS\nThis work was supported by the National Key R&D Program of\nChina under the grant number 2018YFB1004800, the National Natu-\nral Science Foundation of China under the grant number 61725201,\nthe Beijing Municipal Science and Technology Project under the\ngrant number Z171100005117002, and China Postdoctoral Science\nFoundation.\nREFERENCES\n[1] 2018. brain.js. https://github.com/BrainJS.\n[2] 2018. Caffe. http://caffe.berkeleyvision.org/.\n[3] 2018. CNTK. https://www.microsoft.com/en-us/cognitive-toolkit/.\n[4] 2018. ConvNetJS. https://cs.stanford.edu/people/karpathy/convnetjs/.\n[5] 2018. Keras.js. https://github.com/transcranial/keras-js.\n[6] 2018.\nMIL WebDNN Benchmark.\nhttps://mil-tokyo.github.io/webdnn/\n#benchmar.\n[7] 2018. Mind. https://github.com/stevenmiller888/mind.\n[8] 2018. The MNIST database of handwritten digits. http://yann.lecun.com/exdb/\nmnist/.\n[9] 2018. MorphCast. https://www.morphcast.com/.\n[10] 2018. Sklearn DecisionTreeRegressor. https://scikit-learn.org/stable/modules/\ngenerated/sklearn.tree.DecisionTreeRegressor.html.\n[11] 2018. synaptic.js. https://github.com/cazala/synaptic.\n[12] 2018. Teachable Machine. https://teachablemachine.withgoogle.com/.\n[13] 2018. TensorFlow Playgournd. http://playground.tensorflow.org.\n[14] 2018. TensorFlow.js. https://js.tensorflow.org/.\n[15] 2018. WebDNN. https://github.com/mil-tokyo/webdnn.\n[16] 2018. WebGL. https://www.khronos.org/webgl/.\n[17] 2018. WebGPU. https://www.w3.org/community/gpu/.\n[18] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey\nDean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manju-\nnath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray,\nBenoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke,\nYuan Yu, and Xiaoqiang Zheng. 2016. TensorFlow: A System for Large-Scale\nMachine Learning. In Proceedings of the 12th USENIX Symposium on Operating\nSystems Design and Implementation, (OSDI). 265–283.\n[19] Michael Auer. 2012. Real-time Web GIS Analysis Using WebGL. International\nJournal of 3-D Information Modeling (IJ3DIM) 1, 3 (2012), 49–61.\n[20] Bijin Chen and Zhiqi Xu. 2011. A Framework for Browser-Based Multiplayer\nOnline Games Using WebGL and WebSocket. In Proceedings of 2011 International\nConference on Multimedia Technology (ICMT). 471–474.\n[21] Mauricio Guignard, Marcelo Schild, Carlos S. Bederián, Nicolás Wolovick, and\nAugusto J. Vega. 2018. Performance Characterization of State-Of-The-Art Deep\nLearning Workloads on an IBM \"Minsky\" Platform. In Proceedings of the 51st\nHawaii International Conference on System Sciences, HICSS 2018.\n[22] Song Han, Huizi Mao, and William J. Dally. 2015. Deep Compression: Compress-\ning Deep Neural Network with Pruning, Trained Quantization and Huffman\nCoding. CoRR abs/1510.00149 (2015). http://arxiv.org/abs/1510.00149\n[23] Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. 2006. A Fast Learning\nAlgorithm for Deep Belief Nets. Neural computation 18, 7 (2006), 1527–1554.\n[24] Ayae Ichinose, Atsuko Takefusa, Hidemoto Nakada, and Masato Oguchi. 2018.\nPerformance Evaluation of Pipeline-Based Processing for the Caffe Deep Learning\nFramework. IEICE Transactions 101-D (2018), 1042–1052.\n[25] Andrey Ignatov, Radu Timofte, William Chou, Ke Wang, Max Wu, Tim Hartley,\nand Luc Van Gool. 2018. AI Benchmark: Running Deep Neural Networks on\nAndroid Smartphones. In Computer Vision - ECCV 2018 Workshops. 288–314.\n[26] Yiping Kang, Johann Hauswald, Cao Gao, Austin Rovinski, Trevor N. Mudge,\nJason Mars, and Lingjia Tang. 2017. Neurosurgeon: Collaborative Intelligence\nBetween the Cloud and Mobile Edge. In Proceedings of the Twenty-Second Inter-\nnational Conference on Architectural Support for Programming Languages and\nOperating Systems, (ASPLOS). 615–629.\n[27] Nicholas D Lane and Petko Georgiev. 2015. Can Deep Learning Revolutionize\nMobile Sensing?. In Proceedings of the 16th International Workshop on Mobile\nComputing Systems and Applications (HotMobile). 117–122.\n[28] Yann LeCun et al. 1989. Generalization and Network Design Strategies. Connec-\ntionism in perspective (1989), 143–155.\n[29] Weiqing Liu, Jiannong Cao, Lei Yang, Lin Xu, Xuanjia Qiu, and Jing Li. 2017.\nAppBooster: Boosting the Performance of Interactive Mobile Applications with\nComputation Offloading and Parameter Tuning. IEEE Transactions on Parallel\nand Distributed Systems 28, 6 (2017), 1593–1606.\n[30] Y. Liu, P. Sun, M. R. Highsmith, N. M. Wergeles, J. Sartwell, A. Raedeke, M.\nMitchell, H. Hagy, A. D. Gilbert, B. Lubinski, and Y. Shang. 2018. Performance\nComparison of Deep Learning Techniques for Recognizing Birds in Aerial Images.\nIn 2018 IEEE Third International Conference on Data Science in Cyberspace (DSC).\n317–324.\n[31] Bernd Malle, Nicola Giuliani, Peter Kieseberg, and Andreas Holzinger. 2018.\nThe Need for Speed of AI Applications: Performance Comparison of Native\nvs. Browser-based Algorithm Implementations. CoRR abs/1802.03707 (2018).\nhttp://arxiv.org/abs/1802.03707\n[32] Chris Marrin. 2011. WebGL specification. Khronos WebGL Working Group (2011).\n[33] Edward Meeds, Remco Hendriks, Said Al Faraby, Magiel Bruntink, and Max\nWelling. 2015. MLitB: machine learning in the browser. PeerJ Computer Science 1\n(2015), e11.\n[34] Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L. Lewis, and Satinder P. Singh.\n2015. Action-Conditional Video Prediction using Deep Networks in Atari Games.\nIn NIPS.\n[35] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1986. Learning\nrepresentations by back-propagating errors. nature 323, 6088 (1986), 533.\n[36] Shaohuai Shi, Qiang Wang, and Xiaowen Chu. 2018. Performance Modeling and\nEvaluation of Distributed Deep Learning Frameworks on GPUs. In Proceedings of\nIEEE DASC/PiCom/DataCom/CyberSciTech 2018. 949–957.\n[37] Surat Teerapittayanon, Bradley McDanel, and HT Kung. 2017. Distributed Deep\nNeural Networks over the Cloud, the Edge and End Devices. In Proceedings of\nIEEE 37th International Conference on Distributed Computing Systems (ICDCS).\n328–339.\n[38] Aaron Tucker, Adam Gleave, and Stuart Russell. 2018. Inverse Reinforcement\nLearning for Video Games. arXiv preprint arXiv:1810.10593 (2018).\n[39] Ji Wang, Jianguo Zhang, Weidong Bao, Xiaomin Zhu, Bokai Cao, and Philip S.\nYu. 2018. Not Just Privacy: Improving Performance of Private Deep Learning in\nMobile Cloud. In Proceedings of the 24th ACM SIGKDD International Conference\non Knowledge Discovery & Data Mining, (KDD). 2407–2416.\n[40] Shuochao Yao, Shaohan Hu, Yiran Zhao, Aston Zhang, and Tarek Abdelzaher.\n2017. DeepSense: A Unified Deep Learning Framework for Time-Series Mobile\nSensing Data Processing. In Proceedings of the 26th International Conference on\nWorld Wide Web. 351–360.\n",
  "categories": [
    "cs.SE"
  ],
  "published": "2019-01-27",
  "updated": "2019-03-24"
}