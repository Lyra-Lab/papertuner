{
  "id": "http://arxiv.org/abs/1901.09837v1",
  "title": "Designing a Multi-Objective Reward Function for Creating Teams of Robotic Bodyguards Using Deep Reinforcement Learning",
  "authors": [
    "Hassam Ullah Sheikh",
    "Ladislau Bölöni"
  ],
  "abstract": "We are considering a scenario where a team of bodyguard robots provides\nphysical protection to a VIP in a crowded public space. We use deep\nreinforcement learning to learn the policy to be followed by the robots. As the\nrobot bodyguards need to follow several difficult-to-reconcile goals, we study\nseveral primitive and composite reward functions and their impact on the\noverall behavior of the robotic bodyguards.",
  "text": "Designing a Multi-Objective Reward Function for Creating Teams of Robotic\nBodyguards Using Deep Reinforcement Learning\nHassam Ullah Sheikh 1 Ladislau B¨ol¨oni 1\nAbstract\nWe are considering a scenario where a team of\nbodyguard robots provides physical protection to\na VIP in a crowded public space. We use deep\nreinforcement learning to learn the policy to be\nfollowed by the robots. As the robot bodyguards\nneed to follow several difﬁcult-to-reconcile goals,\nwe study several primitive and composite reward\nfunctions and their impact on the overall behavior\nof the robotic bodyguards.\n1. Introduction\nRecent progress in the ﬁeld of autonomous robots makes it\nfeasible for robots to interact with multiple humans in public\nspaces. In this paper, we are considering a human VIP\nmoving in a crowded market environment who is protected\nfrom physical assault by a team of bodyguard robots. The\nrobots must take into account the position and movement of\nthe VIP, the bystanders and other robots. Previous work in\nsimilar problems relied on explicitly programmed behaviors.\nRecent research in deep reinforcement learning (Levine\net al., 2016; Silver et al., 2016) and imitation learn-\ning (Rahmatizadeh et al., 2018) applied to the robotics has\nraised the possibility that learning algorithms might lead to\nbetter algorithms than explicit programming. In this paper\nwe explore deep reinforcement learning approaches for our\nscenario. We aim to simultaneously learn communication\nand coordination techniques between the agent and the task\noriented behavior.\nWe aim to develop a general task framework, which can\ngeneralize to other types of desired behaviors, beyond body-\nguard protection.\nIn order to achieve these goals, we need to specify: the envi-\nronment in which the agents will perform, the environment\n1Department of Computer Science, University of Central\nFlorida, Orlando, Florida, United States. Correspondence to: Has-\nsam Ullah Sheikh <hassam.sheikh@knights.ucf.edu>.\nAccepted at the 1st Workshop on Goal Speciﬁcations for Reinforce-\nment Learning, FAIM 2018, Stockholm, Sweden, 2018. Copyright\n2018 by the author(s).\nrepresentation in the robot that forms the basis of learning,\nthe reward function that describes the desired behavior, and\nthe reinforcement algorithms deployed. For the bodyguard\ntask, the design of the reward function is especially chal-\nlenging, because it is task speciﬁc, and it needs to reconcile\nmultiple conﬂicting objectives the maximum protection,\nwhile minimizing interference with the crowd and being un-\nobtrusive. In Section 4.2 we discuss several different reward\nfunctions that reﬂect the different aspects of the desired\nbehavior.\nWe describe several experiments using the Multi-agent Deep\nDeterministic Policy Gradient (MADDPG) algorithms over\nseveral choices of reward functions. We found that commu-\nnication penalization reward functions captures better the\ncollaborative nature of the scenario, and thus it performs\nbetter in experiments.\n2. Related Work\nThe use of robots as a bodyguards is related to several dif-\nferent area of research and has received signiﬁcant attention\nlately. Several different studies such as (Richard Klima,\n2016; Yasuyuki et al., 2015) considered using robots and\nmulti-agent reinforcement learning for security related tasks\nsuch as patrolling and team coordination by placing check-\npoints to provide protection against imminent adversaries.\nA multi-robot patrolling framework was proposed by (Khan\net al., 2012) that analyzes the behavior pattern of the sol-\ndiers and the robot and generates a patrolling schedule. The\ncontrol of robots for providing maximal protection to a VIP\nwas well studied in (Bhatia et al., 2016) where they intro-\nduced the concept of threat vector resolution and quadrant\nload balancing.\n3. Background\nWe consider the problem of providing maximal physical pro-\ntection as a standard reinforcement learning setup with N\nagent interacting with the environment in E discrete steps us-\ning real valued continuous actions at such that at ∈Rd. At\neach timestep t, the agents receive an observation xt, takes\nthe action at, and receive a scalar reward rt. Generally, the\nenvironment can be partially observable i.e we may need the\narXiv:1901.09837v1  [cs.MA]  28 Jan 2019\nMulti-Objective Reward for Teams of Robotic Bodyguards\nentire past history of observations, action pairs to represent\nthe current state such that st= (x1, a1, x2, a2, . . . , xt). For\nour problem, we have assumed that the environment is fully\nobservable so we will represent st = xt\nThe behavior of each agent is represented by its own policy\nπ that takes the state st as an input and outputs a probability\ndistribution over all the actions i.e π (s) →P (A). Since\nthe environment is stochastic, we will model it as Markov\nDecision Process with a state space S, action space A, re-\nward function R (st, at, st+1) and the transition dynamics\np(st+1|st, at).\nThe return Gt from state s at timestep t is deﬁned as the\ndiscounted cummulative reward that the agent accumulates\nstarting from state s at timestep t and represented as Gt =\nT\nX\ni=t\nγi−tR (si, ai, si+1) where γ is the discounting factor\nγ ∈[0, 1]. The goal of the reinforcement learning is ﬁnd\nan optimum policy π∗that maximizes the expected return\nstarting from state s. We denote the trajectory for state\nvisitation for the policy π as ρπ.\n3.1. Policy Gradients\nPolicy gradient methods have been shown to learn the opti-\nmal policy in variety of reinforcement learning tasks. The\nmain idea behind policy gradient methods is instead of pa-\nrameterizing the Q-function to extract the policy, we param-\neterize the policy using the parameters θ to maximize the\nobjective which is represented as J (θ) = E [Gt] by taking a\nstep in the direction of ∇J (θ) where ∇J (θ) is deﬁned as:\n∇J (θ) = E [∇θ log πθ (a|s) Qπ (s, a)]\nThe policy gradient methods are prone to high variance\nproblem. Several different methods such as (Wu et al.,\n2018; Schulman et al., 2017) have been shown to reduce\nthe variability in policy gradient methods by introducing a\ncritic which is a Q-function that tells about the goodness of\na reward by working as a baseline.\n3.2. Deep Deterministic Policy Gradients\nIn (Silver et al., 2014) has shown that it is possible to extend\npolicy gradient framework to deterministic policies i.e. πθ :\nS →A. In particular we can write ∇J (θ) as\n∇J (θ) = E\n\u0002\n∇θπ (a|s) ∇aQπ (s, a) |a=π(s)\n\u0003\nDeep Deterministic Policy Gradients (Lillicrap et al., 2015)\nis an off-policy algorithm and is a modiﬁcation of the DPG\nmethod introduced in (Silver et al., 2014) in which the pol-\nicy π and the critic Qπ is approximated using deep neural\nnetworks. DDPG also uses an experience replay buffer\nalongside with a target network to stabilize the training.\n3.3. Multi-Agent Deep Deterministic Policy Gradients\nMulti-agent deep deterministic policy gradients (Lowe et al.,\n2017) is the extension of the DDPG for the multi-agent\nsetting where each agent has it’s own policy. The gradient\n∇J (θi) of each policy is written as\n∇J (θi) = E [∇θi log πi (ai|si) Qπ\ni (si, a1, . . . , aN)]\nwhere Qπ\ni (si, a1, . . . , aN) is a centralized action-value\nfunction that takes the actions of all the agents in addition\nto the state of the agent to estimate the Q-value for agent\ni. Since every agent has it’s own Q-function, its allows the\nagents to have different action space and reward functions.\nThe primary motivation behind MADDPG is that know-\ning all the actions of other agents makes the environment\nstationary, even though their policy changes.\n3.4. Policy Gradients\nPolicy gradient methods have been shown to learn the opti-\nmal policy in variety of reinforcement learning tasks. The\nmain idea behind policy gradient methods is instead of pa-\nrameterizing the Q-function to extract the policy, we param-\neterize the policy using the parameters θ to maximize the\nobjective which is represented as J (θ) = E [Gt] by taking a\nstep in the direction of ∇J (θ) where ∇J (θ) is deﬁned as:\n∇J (θ) = E [∇θ log πθ (a|s) Qπ (s, a)]\nThe policy gradient methods are prone to high variance\nproblem. Several different methods such as (Wu et al.,\n2018; Schulman et al., 2017) have been shown to reduce\nthe variability in policy gradient methods by introducing a\ncritic which is a Q-function that tells about the goodness of\na reward by working as a baseline.\n3.5. Deep Deterministic Policy Gradients\nIn (Silver et al., 2014) has shown that it is possible to extend\npolicy gradient framework to deterministic policies i.e. πθ :\nS →A. In particular we can write ∇J (θ) as\n∇J (θ) = E\n\u0002\n∇θπ (a|s) ∇aQπ (s, a) |a=π(s)\n\u0003\nDeep Deterministic Policy Gradients (Lillicrap et al., 2015)\nis an off-policy algorithm and is a modiﬁcation of the DPG\nmethod introduced in (Silver et al., 2014) in which the pol-\nicy π and the critic Qπ is approximated using deep neural\nnetworks. DDPG also uses an experience replay buffer\nalongside with a target network to stabilize the training.\n3.6. Multi-Agent Deep Deterministic Policy Gradients\nMulti-agent deep deterministic policy gradients (Lowe et al.,\n2017) is the extension of the DDPG for the multi-agent\nsetting where each agent has it’s own policy. The gradient\nMulti-Objective Reward for Teams of Robotic Bodyguards\n∇J (θi) of each policy is written as\n∇J (θi) = E [∇θi log πi (ai|si) Qπ\ni (si, a1, . . . , aN)]\nwhere Qπ\ni (si, a1, . . . , aN) is a centralized action-value\nfunction that takes the actions of all the agents in addition\nto the state of the agent to estimate the Q-value for agent\ni. Since every agent has it’s own Q-function, its allows the\nagents to have different action space and reward functions.\nThe primary motivation behind MADDPG is that know-\ning all the actions of other agents makes the environment\nstationary, even though their policy changes.\n4. Problem Formulation\nThe setting we are considering for providing maximal phys-\nical protection to the VIP in crowded environment is a co-\noperative Markov game which becomes a natural extension\nof the single agent MDP for multi-agent systems. A multi-\nagent MDP is deﬁned as state space S that decribes all the\nconﬁgurations of all the agents, an action space A that de-\nscribes the action space of every agent A1, . . . , AN. The\ntransitions are deﬁned as T = S1 × A1 × . . . × SN × AN.\nFor each agent i, the reward function is deﬁned as ri =\nR (Si, Ai).\nIn this problem, we are assuming that all bodyguards have\nsame state space and they are following an identical policy.\nFor this problem we are considering a ﬁnite horizon problem\nwhere each episode is terminated after T steps. Since this\nproblem is a cooperative setting, the goal of all the agents is\nto ﬁnd individual policies that increase the collected payoff.\n4.1. The Environment Model\nFor the emergence of interesting behaviors in a multi-agent\nsetting, grounded communication in a physical environ-\nment is considered to be a crucial component. For per-\nforming the experiments, we used Multi-Agent Particle\nEnvironment (Mordatch & Abbeel, 2017) which is a two-\ndimensional physically simulated environment in a discrete\ntime and continuous space. The environment consists of N\nagent and M landmarks, both possessing physical attributes\nsuch as location, velocity and size etc. Agents can act and\nmove independently with their own policies.\nIn addition to the ability of performing physical actions in\nthe environment, the agents also have the ability to utter\nverbal symbols over the communication channel at every\ntimestamp. The utterances are symbolic in nature and does\nnot carry any meaning. At each timestamp, every agent\nutters a categorical variable that is observed by every other\nagent and it is up to the agents to infer a meaning of these\nsymbols during the training time. Every utterance carry a\nsmall penalty and the agent can decide not to utter at every\ntimestamp. We denote the utterance which is a one-hot\nvector by c.\nThe complete observation of the environment is given by\no = [x1,...N+M, c1,...N] ∈O. The state of each agent is\nthe physical state of all the entities in the environment and\nverbal utterances of all the agents. Formally, the state of\neach agent is deﬁned as si = [xj,...N+M, ck,...N] where xj\nis the observation of the entity j from the perspective of\nagent i and ck is the verbal utterance of the agent k.\n4.2. Reward Functions\nIn (Bhatia et al., 2016) has deﬁned a metric that quantiﬁes\nthe threat to the VIP from each crowd member bi at each\ntimestep t. This metric can be extended to a reward function.\nSince the threat level metric gives a probability. We can\nconclude that when the distance between the VIP and the\ncrowd member is 0, the threat to the VIP is maximum, i.e\n1, conversely when the distance between the VIP and the\ncrowd member bi is more than the safe distance, the threat\nto the VIP is 0. We can model this phenomenon as an\nexponential decay. Thus the fundamental reward function\ncan be deﬁned as\nRt (B, VIP) = −1 +\nk\nY\ni=1\n(1 −TL (VIP, bi))\n(1)\nwhere\nTL (VIP, bi) = exp−A(Dist(VIP,bi))/B\n(2)\nIn the following we will derive reward functions and ex-\nplain the motivation behind them that were derived from\nequation 1.\nThe baseline Threat-Only Reward Function penalizes each\nagent with the threat to the VIP at each time step as men-\ntioned in (Bhatia et al., 2016).\nRt (B, VIP) = −1 +\nk\nY\ni=1\n(1 −TL (VIP, bi))\n(3)\nThe Binary Threat Reward Function penalizes each agent\nfor the threat with a negative binary reward, in addition,\neach agent is also penalized for not maintaining a suitable\ndistance from the VIP:\nL (VIP, B) =\n\n\n\n\n\n−1\nif −1 +\nk\nY\ni=1\n(1 −TL (VIP, bi)) ̸= 0\n0\notherwise\n(4)\nD (VIP, xi) =\n\n\n\n\n\n0\nm ≤∥xi −VIP∥2 ≤d\n−1\notherwise\n(5)\nMulti-Objective Reward for Teams of Robotic Bodyguards\nwhere m is the minimum distance the bodyguard has to\nmaintain from VIP and d is the safe distance. The ﬁnal\nreward function is represented as\nRt (B, VIP, xi) = L (VIP, B) + D (VIP, xi)\n(6)\nThe Composite Reward Function is the composition of the\nthreat only reward function and the penalty for not maintain-\ning a suitable distance from the VIP\nRt (B, VIP, xi) = −1 +\nk\nY\ni=1\n(1 −TL (VIP, bi))\n+ D (VIP, xi)\n(7)\nThe Communication Penalization Reward Function aug-\nments the composite reward by adding a small penalty p\nevery time the bodyguard performs an utterance, as recom-\nmended in (Mordatch & Abbeel, 2017).\n5. Experiments\nWe performed our experiments using the Multi-Agent Parti-\ncle Environment (MPE) (Mordatch & Abbeel, 2017). The\nperformance was measured using the threat metric deﬁned\nin (Bhatia et al., 2016) over one episode. The experiments\nwere performed with 2-4 bodyguards ranging from 2-4, and\na constant number of 10 bystanders. For all of the experi-\nments, we have trained the agents for 10,000 episodes and\nlimiting the length of the episode to 25 steps.\nFigure 1 shows examples of the resulting bodyguard behav-\nior for the composite reward function (left) and the threat\nonly reward function (right). Notice that for the threat only\nbehavior, the bodyguards are not in the close proximity of\nthe VIP - they have found ways to keep the threat low by\n“attacking” the crowd.\nFigure 1. The emerging collaborative bodyguard behavior, using\nthe composite reward function from Equation 7 (left) and the\nthreat-only function from Equation 3 (right). The VIP is brown,\nbodyguards blue, bystanders red and landmarks grey.\nFigure 2 shows the threat levels obtained by different reward\nfunctions. The communication penalty function appears the\nFigure 2. The overall threat level achieved by agents trained using\n4 different reward functions (threat only, composite, binary threat\nand communication penalty). The scenario involved 4 agents and\n10 bystanders.\nclear winner, with the lowest threat level obtained over the\ncourse of the scenario.\nAcknowledgement: This research was sponsored by the\nArmy Research Laboratory and was accomplished under\nCooperative Agreement Number W911NF-10-2-0016. The\nviews and conclusions contained in this document are those\nof the authors and should not be interpreted as represent-\ning the ofﬁcial policies, either expressed or implied, of the\nArmy Research Laboratory or the U.S. Government. The\nU.S. Government is authorized to reproduce and distribute\nreprints for Government purposes notwithstanding any copy-\nright notation herein.\nReferences\nBhatia, T.S., Solmaz, G., Turgut, D., and B¨ol¨oni, L. Con-\ntrolling the movement of robotic bodyguards for maximal\nphysical protection. In Proc of the 29th International\nFLAIRS Conference, pp. 380–385, May 2016.\nKhan, S. A., Bhatia, T.S., Parker, S., and B¨ol¨oni, L. Mod-\neling human-robot interaction for a market patrol task.\nIn Proc. of 25th International FLAIRS Conference, pp.\n50–55, May 2012.\nLevine, Sergey, Finn, Chelsea, Darrell, Trevor, and Abbeel,\nPieter. End-to-end training of deep visuomotor policies.\nJournal of Machine Learning Research, 17(1):1334–1373,\nJanuary 2016.\nLillicrap, Timothy P., Hunt, Jonathan J., Pritzel, Alexander,\nHeess, Nicolas, Erez, Tom, Tassa, Yuval, Silver, David,\nand Wierstra, Daan. Continuous control with deep rein-\nforcement learning. In Proceedings of the International\nConference on Learning Representations (ICLR), 2015.\nMulti-Objective Reward for Teams of Robotic Bodyguards\nLowe, Ryan, Wu, Yi, Tamar, Aviv, Harb, Jean, Abbeel,\nPieter, and Mordatch, Igor. Multi-agent actor-critic for\nmixed cooperative-competitive environments. Neural\nInformation Processing Systems (NIPS), 2017.\nMordatch, Igor and Abbeel, Pieter. Emergence of grounded\ncompositional language in multi-agent populations. arXiv\npreprint arXiv:1703.04908, 2017.\nRahmatizadeh, Rouhollah, Abolghasemi, Pooya, B¨ol¨oni,\nLadislau, and Levine, Sergey. Vision-based multi-task\nmanipulation for inexpensive robots using end-to-end\nlearning from demonstration. In International Conference\non Robotics and Automation (ICRA), 2018.\nRichard Klima, Karl Tuyls, Frans Oliehoek. Markov se-\ncurity games: Learning in spatial security problems. In\nThe NIPS 2016 The Learning, Inference and Control of\nMulti-Agent System, 2016.\nSchulman, John, Wolski, Filip, Dhariwal, Prafulla, Radford,\nAlec, and Klimov, Oleg. Proximal policy optimization\nalgorithms. CoRR, abs/1707.06347, 2017.\nSilver, David, Lever, Guy, Heess, Nicolas, Degris, Thomas,\nWierstra, Daan, and Riedmiller, Martin. Deterministic\npolicy gradient algorithms. In Proceedings of the 31st\nInternational Conference on Machine Learning, 2014.\nSilver, David, Huang, Aja, Maddison, Christopher J., Guez,\nArthur, Sifre, Laurent, van den Driessche, George, Schrit-\ntwieser, Julian, Antonoglou, Ioannis, Panneershelvam,\nVeda, Lanctot, Marc, Dieleman, Sander, Grewe, Dominik,\nNham, John, Kalchbrenner, Nal, Sutskever, Ilya, Lillicrap,\nTimothy, Leach, Madeleine, Kavukcuoglu, Koray, Grae-\npel, Thore, and Hassabis, Demis. Mastering the game of\nGo with deep neural networks and tree search. Nature,\n529:484–503, 2016.\nWu, Cathy, Rajeswaran, Aravind, Duan, Yan, Kumar,\nVikash, Bayen, Alexandre M, Kakade, Sham, Mordatch,\nIgor, and Abbeel, Pieter. Variance reduction for policy\ngradient with action-dependent factorized baselines. In\nInternational Conference on Learning Representations,\n2018.\nYasuyuki, S., Hirofumi, O., Tadashi, M., and Maya, H.\nCooperative capture by multi-agent using reinforcement\nlearning application for security patrol systems. In 2015\n10th Asian Control Conference (ASCC), pp. 1–6, May\n2015.\n",
  "categories": [
    "cs.MA",
    "cs.LG"
  ],
  "published": "2019-01-28",
  "updated": "2019-01-28"
}