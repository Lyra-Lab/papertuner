{
  "id": "http://arxiv.org/abs/1901.10177v1",
  "title": "Unsupervised Person Re-identification by Deep Asymmetric Metric Embedding",
  "authors": [
    "Hong-Xing Yu",
    "Ancong Wu",
    "Wei-Shi Zheng"
  ],
  "abstract": "Person re-identification (Re-ID) aims to match identities across\nnon-overlapping camera views. Researchers have proposed many supervised Re-ID\nmodels which require quantities of cross-view pairwise labelled data. This\nlimits their scalabilities to many applications where a large amount of data\nfrom multiple disjoint camera views is available but unlabelled. Although some\nunsupervised Re-ID models have been proposed to address the scalability\nproblem, they often suffer from the view-specific bias problem which is caused\nby dramatic variances across different camera views, e.g., different\nillumination, viewpoints and occlusion. The dramatic variances induce specific\nfeature distortions in different camera views, which can be very disturbing in\nfinding cross-view discriminative information for Re-ID in the unsupervised\nscenarios, since no label information is available to help alleviate the bias.\nWe propose to explicitly address this problem by learning an unsupervised\nasymmetric distance metric based on cross-view clustering. The asymmetric\ndistance metric allows specific feature transformations for each camera view to\ntackle the specific feature distortions. We then design a novel unsupervised\nloss function to embed the asymmetric metric into a deep neural network, and\ntherefore develop a novel unsupervised deep framework named the DEep\nClustering-based Asymmetric MEtric Learning (DECAMEL). In such a way, DECAMEL\njointly learns the feature representation and the unsupervised asymmetric\nmetric. DECAMEL learns a compact cross-view cluster structure of Re-ID data,\nand thus help alleviate the view-specific bias and facilitate mining the\npotential cross-view discriminative information for unsupervised Re-ID.\nExtensive experiments on seven benchmark datasets whose sizes span several\norders show the effectiveness of our framework.",
  "text": "1\nUnsupervised Person Re-identiﬁcation by Deep\nAsymmetric Metric Embedding\nHong-Xing Yu, Ancong Wu, Wei-Shi Zheng\nDemo code is available at:\nhttps://github.com/KovenYu/DECAMEL\nFor reference of this work, please cite:\nHong-Xing\nYu,\nAncong\nWu,\nWei-Shi\nZheng.\n“Unsupervised\nPer-\nson Re-identiﬁcation by Deep Asymmetric Metric Embedding.” IEEE\nTransactions\non\nPattern\nAnalysis\nand\nMachine\nIntelligence.\n(DOI\n10.1109/TPAMI.2018.2886878).\nBib:\n@article{yu2018unsupervised,\ntitle={Unsupervised Person Re-identiﬁcation by Deep Asymmetric Metric\nEmbedding},\nauthor={Yu, Hong-Xing and Wu, Ancong and Zheng, Wei-Shi},\njournal={IEEE Transactions on Pattern Analysis and Machine Intelligence\n(DOI 10.1109/TPAMI.2018.2886878)},\n}\narXiv:1901.10177v1  [cs.CV]  29 Jan 2019\nCITATION INFORMATION: DOI 10.1109/TPAMI.2018.2886878, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n2\nUnsupervised Person Re-identiﬁcation by\nDeep Asymmetric Metric Embedding\nHong-Xing Yu, Ancong Wu, Wei-Shi Zheng\nAbstract—Person re-identiﬁcation (Re-ID) aims to match identities across non-overlapping camera views. Researchers have proposed\nmany supervised Re-ID models which require quantities of cross-view pairwise labelled data. This limits their scalabilities to many\napplications where a large amount of data from multiple disjoint camera views is available but unlabelled. Although some unsupervised\nRe-ID models have been proposed to address the scalability problem, they often suffer from the view-speciﬁc bias problem which is\ncaused by dramatic variances across different camera views, e.g., different illumination, viewpoints and occlusion. The dramatic\nvariances induce speciﬁc feature distortions in different camera views, which can be very disturbing in ﬁnding cross-view discriminative\ninformation for Re-ID in the unsupervised scenarios, since no label information is available to help alleviate the bias. We propose to\nexplicitly address this problem by learning an unsupervised asymmetric distance metric based on cross-view clustering. The\nasymmetric distance metric allows speciﬁc feature transformations for each camera view to tackle the speciﬁc feature distortions. We\nthen design a novel unsupervised loss function to embed the asymmetric metric into a deep neural network, and therefore develop a\nnovel unsupervised deep framework named the DEep Clustering-based Asymmetric MEtric Learning (DECAMEL). In such a way,\nDECAMEL jointly learns the feature representation and the unsupervised asymmetric metric. DECAMEL learns a compact cross-view\ncluster structure of Re-ID data, and thus help alleviate the view-speciﬁc bias and facilitate mining the potential cross-view discriminative\ninformation for unsupervised Re-ID. Extensive experiments on seven benchmark datasets whose sizes span several orders show the\neffectiveness of our framework.\nIndex Terms—Unsupervised person re-identiﬁcation, unsupervised metric learning, unsupervised deep learning, cross-view clustering,\ndeep clustering.\n!\n1\nINTRODUCTION\nAlong with the extensive deployment of visual surveillance\nnetworks, considerable visual surveillance data is emerging every-\nday within the networks. A basic problem of analyzing and ex-\nploiting the data is to ﬁnd target persons who had been previously\nobserved like missing children and suspects, also known as person\nre-identiﬁcation (Re-ID) [1], [2]. Typically, Re-ID focuses on\npedestrian matching and ranking across multiple non-overlapping\ncamera views.\nRe-ID remains an open problem although it has received\nincreasing explorations in recent years, principally because of\ndramatic intra-person appearance variation across views and high\ninter-person similarity. They mainly focus on learning robust and\ndiscriminative feature representations [3], [4], [5], [6], [7], [8],\n[9], [10], [11], [12], [13] and distance metrics [14], [15], [16],\n[17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28].\nRecently, deep learning has been adopted to the Re-ID community\nand has achieved promising performances [29], [30], [31], [32],\n[33], [34], [35], [36], [37], [38], [39], [40]. They have contributed\n•\nHong-Xing Yu is with School of Data and Computer Science, Sun Yat-\nsen University, Guangzhou 510275, China, and is also with the Guang-\ndong Province Key Laboratory of Information Security, China. E-mail:\nxKoven@gmail.com.\n•\nAncong Wu is with School of Electronics and Information Technology,\nSun Yat-sen University, Guangzhou 510275, China, and is also with the\nCollaborative Innovation Center of High Performance Computing, NUDT,\nChina. E-mail: wuancong@mail2.sysu.edu.cn.\n•\nWei-Shi Zheng is with School of Data and Computer Science, Sun Yat-sen\nUniversity, Guangzhou 510275, China, and is also with the Key Laboratory\nof Machine Intelligence and Advanced Computing, Ministry of Education,\nChina E-mail: wszheng@ieee.org / zhwshi@mail.sysu.edu.cn.\n(Corresponding author: Wei-Shi Zheng.)\na lot to the Re-ID community.\nHowever, supervised models are intrinsically limited because\nthey rely on a large amount of correctly labelled cross-view\ntraining data, which is very expensive [41]. In the context of Re-\nID, this limitation is even pronounced because (1) manual labelling\nmay not be fully reliable when a huge number of images to be\nchecked across multiple camera views, and more importantly (2)\nthe astronomical cost of time and money is prohibitive to label the\noverwhelming amount of data. In many scenarios, there is a large\namount of data available but unlabelled, so that the supervised\nmethods would be restricted or not applicable.\nTo directly make full use of the cheap and valuable unlabelled\ndata, some efforts on unsupervised Re-ID have been made [42],\n[43], [44], [45], [46], [47], [48], [49], [50], which typically learn\ngeneral/universal feature projections for person images from every\ncamera view, but the performances are still not satisfactory. One\nof the main reasons is that without the supervision of cross-view\npairwise labelled data, it is very difﬁcult for a universal feature\nprojection to capture the cross-view discriminative information\nunder dramatic cross-view person appearance variations caused\nby view-speciﬁc conditions (Figure 1(a)). For example, a person\nin white may appear wearing gray in one camera view where\nillumination is darker, while he may appear snow-white in another\nview where illumination is brighter. Without the pairwise super-\nvision guidance, it is very hard for a universal feature projection\nto map such drastically different cross-view image features of the\nsame person to very close points in the subspace. More generally,\nthe view-speciﬁc conditions introduce the view-speciﬁc bias, i.e.,\nsome speciﬁc feature distortions in different camera views, which\nCITATION INFORMATION: DOI 10.1109/TPAMI.2018.2886878, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n3\n(a) View-speciﬁc conditions\n(b) View-speciﬁc bias in feature space\n(c) View-speciﬁc bias alleviated\nFig. 1. Cause and effect of the view-speciﬁc bias. (a) The images from different camera views suffer from dramatic viewing condition variations\nacross camera views. Images in the same colored box belong to the same person. (b) The speciﬁc viewing conditions lead to the view-speciﬁc\nfeature distortions/bias, making unsupervised Re-ID more challenging. A visualization of the view-speciﬁc bias can be found in Figure 3(a). For\nexample, given a probe image (in blue box), the correct match falls to the third rank due to the view-speciﬁc bias. (c) If we can tackle the view-\nspeciﬁc feature distortion (i.e., alleviate the view-speciﬁc bias), we may reach a better cross-view matching performance. Further visualization and\ndiscussion can be found in Figure 3 and Figure 4. (Best viewed in color).\ncan be very disturbing in ﬁnding what is more distinguishable in\nmatching people across views. We show a toy example to illustrate\nthis disturbing effect in Figure 1. In Figure 1(a), the color feature\nof a person’s arm may be located in the central position of images\nfrom Camera-1 since Camera-1 typically captures the proﬁle of a\nperson, while the corresponding color feature may appear at the\nboundary of images from Camera-2 since it captures the back of\nthe person. Thus, the view-speciﬁc feature distortion can make\nthe cross-view matching even harder as shown in Figure 1(b). In\nparticular, most existing unsupervised models treat the samples\nfrom different views in the same manner, and thus could suffer\nfrom the effect of the view-speciﬁc bias.\nIn this work, we propose to explicitly deal with the view-\nspeciﬁc bias problem in unsupervised Re-ID by formulating it\nas an unsupervised asymmetric distance metric learning problem.\nWe brieﬂy introduce the idea in the following. Given a pair of\nsample feature representations xi and xj, a conventionally learned\ndistance metric between them is:\ndl(xi, xj) =\np\n(xi −xj)TM(xi −xj) = ∥U Txi −U Txj∥2, (1)\nwhere M = UU T is a positive semi-deﬁned matrix, and U\nis a transformation matrix. Learning such a metric is equivalent\nto ﬁnding a space “shared” by samples from each camera view\n[51]. This shared space is found by projecting all samples with\na view-generic universal transformation U. However, different\ncamera views may induce different feature characteristics, e.g., the\nside-view of persons in Camera-1 vs. the back-view in Camera-\n2 as shown in Figure 1(a). Intuitively, it is important to perform\nview-speciﬁc transformations for acquiring common features to\nmatch person images across camera views (e.g., selecting the\ncorresponding color features at different locations of images from\ndifferent camera views). Therefore, it can be hard for a univer-\nsal transformation to implicitly tackle the view-speciﬁc feature\ndistortions from different camera views, especially when we lack\nlabel information to guide it. This motivates us to explicitly take\nthe view-speciﬁc feature distortion into account. Inspired by the\nsupervised asymmetric distance model [28], [3], we propose to\nembed the asymmetric metric learning into our unsupervised Re-\nID modelling, and thus consider the modiﬁcation of Eq. (1), i.e.,\nthe asymmetric form:\ndl({xi, vi}, {xj, vj}) = ∥U T\nvixi −U T\nvjxj∥2,\n(2)\nwhere vi denotes which camera view the i-th sample comes from,\nand Uvi is the corresponding view-speciﬁc transformation. Such\nan asymmetric metric allows speciﬁc transformation for each view\nto tackle the view-speciﬁc feature distortions.\nSince no label information is provided to strictly distinguish\nevery visually similar person in unsupervised Re-ID scenarios, we\nencourage the asymmetric metric to condense the visually similar\ncross-view person image clusters, and thus better distinguish them\nfrom other dissimilar clusters. With the asymmetric metric in the\nclustering procedure, we can explicitly address the view-speciﬁc\nbias and learn a better cross-view cluster structure of the Re-ID\ndata in the shared space.\nIn the following, we refer to the clustering procedure which\nuses an asymmetric metric as the asymmetric metric clustering.\nBased on the asymmetric metric clustering, we ﬁrst develop a\nlinear metric learning model named Clustering-based Asymmetric\nMEtric Learning (CAMEL). CAMEL jointly learns an asymmetric\nmetric and a cluster separation. Then, based on CAMEL, we\nfurther propose a novel unsupervised deep framework named\nDEep CAMEL (DECAMEL), which jointly learns the feature\nrepresentation and the unsupervised asymmetric metric end to end.\nDECAMEL can address the sub-optimality due to the separation\nof feature and metric learning. By learning a better cross-view\ncluster structure in the shared space, DECAMEL attempts to mine\nthe underlying cross-view discriminative information to achieve a\nbetter cross-view matching performance.\nMore speciﬁcally, DECAMEL consists of a feature extractor\nnetwork and an asymmetric metric layer. We propose a novel\nunsupervised loss function for the whole deep framework. In\nthe optimization of DECAMEL, the asymmetric metric layer is\ninitialized by CAMEL, which can alleviate the view-speciﬁc bias\nand learn a preliminary cross-view cluster separation. Then, the\nCITATION INFORMATION: DOI 10.1109/TPAMI.2018.2886878, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n4\nasymmetric metric is embedded into the whole network by the\njoint learning. The term “embedded” refers to the fact that, during\nthe joint learning, the asymmetric metric is back-propagated to\nthe whole network, so that ﬁnally the view-speciﬁc bias in the\nfeature space is also alleviated. This is empirically observed\nin Figure 4. Through this joint learning procedure, DECAMEL\nlearns a compact cross-view cluster structure of Re-ID data. And\nthereby, it attempts to mine the potential cross-view discriminative\ninformation which will be qualitatively illustrated in Sec. 4 and\nquantitatively validated in Sec. 5.\nIn summary, our main contributions in this work are:\n(1) We formulate unsupervised Re-ID as a joint learning prob-\nlem which consists of learning the feature representation and an\nasymmetric distance metric, together with a cluster separation. To\nour best knowledge, this is the ﬁrst work formulating unsupervised\nRe-ID as a joint learning problem of the feature and the metric.\n(2) We propose a novel unsupervised deep framework named\nDECAMEL. Different from previous works in unsupervised Re-\nID, DECAMEL jointly learns the feature representation and the\nunsupervised asymmetric metric. DECAMEL can learn a compact\ncross-view clustering structure for mining underlying cross-view\ndiscriminative information. We also propose a novel unsupervised\nasymmetric metric learning model, i.e. CAMEL, for the metric\ninitialization in DECAMEL. CAMEL allows to explicitly model\nthe view-speciﬁc conditions in unsupervised Re-ID [52], and\nthus the useful information of the cross-view person appearance\nvariations can be exploited for the feature learning in DECAMEL.\n(3) For large-scale view-extendable scenarios, we develop a\nmethod named View Clustering (VC) for better generalizability\nand scalability. VC provides a ﬂexible control on the generalizabil-\nity versus ability to precisely model the view-speciﬁc conditions.\nTo evaluate our framework, we conduct extensive experiments on\nseven size-varying benchmark datasets. Experimental results show\nthat our model outperforms the state-of-the-art with noticeable\nmargins, indicating that the asymmetric modelling is effective in\nunsupervised Re-ID.\n2\nRELATED WORK\n2.1\nUnsupervised Re-ID Models\nWhile there are a lot of great works in supervised Re-ID [3-40],\nunsupervised Re-ID still remains under-studied. Existing attempts\nin unsupervised Re-ID can be classiﬁed into two categories:\nfeature representation learning [42], [43], [44], [45], [46], [47]\nand dictionary learning [48], [49], [50].\nFeature representation learning. This category of models mainly\nfocuses on designing or learning discriminative and invariant fea-\ntures. Farenzena et.al. [42] proposed to extract features containing\nthree complementary parts of human appearance, including color,\nspatial arrangement of colors and texture patches. Cheng et.al. [43]\nproposed to exploit the part-to-part correspondence. They evaluate\nthe modiﬁed HSV characterization for each part and found the\nmaximally stable color regions. Zhao et.al. [44] proposed to exploit\nsalience information by building dense correspondence and unsu-\npervised salience learning. Wang et.al. [45] proposed to localize\nperson foreground saliency and remove busy background clutters.\nLisanti et.al. [46] proposed to extract the weighted histogram of\noverlapping stripes (WHOS) feature, and then applied the Iterative\nRe-Weighted Sparse Ranking (ISR) algorithm to generate the\nranked list of gallery individuals. Wang et.al. [47] proposed a\nCCA-based model to learn a feature subspace where within-view\nsimilar persons are distant from each other and cross-view similar\npersons are close to each other. Recently, Fan et.al. [53] attempted\nto learn feature representation with a deep clustering network.\nDictionary learning. Dictionary learning aims to learn a dictio-\nnary with its atoms corresponding to some semantic elements. The\nlearned dictionary will be used to produce the new features which\nminimize the reconstruction error. Kodirov et.al. [48] proposed\nto formulate unsupervised Re-ID as a sparse dictionary learning\nproblem. To regularize the learned dictionary, they learned it with\ngraph Laplacian regularization, and iteratively updated the graph\nLaplacian matrix. Then they took a further step [50]. They propose\nto introduce an l1-norm graph Laplacian to jointly learn the graph\nand the dictionary, resulting in alleviation of the effects of data\noutliers [50]. Peng et.al. [49] proposed to learn a dictionary by\nunsupervised multi-task learning. Different from [48] and [50], the\nlearned dictionary consists of task-shared, task-speciﬁc and unique\ncomponents.\nOur model is very different from them in that: (1) Ours\nexplicitly addresses the view-speciﬁc bias by learning an asym-\nmetric metric, i.e., learning view-speciﬁc transformation for each\ncamera view. Note that the RKSL [47] is based on Canonical\nCorrelation Analysis (CCA) and also learns different projections\nfor different views. However, it does not address the view-speciﬁc\nbias problem, because the CCA-based model RKSL learns two\nprojections separately and inconsistently. RKSL focuses on the\ncorrelations of cross-view samples by maximizing the correlation\ncoefﬁcients. In the context of unsupervised learning (given no\ncross-view labelled pairs), not like ours, RKSL is not able to\ndiscover the cross-view cluster structure and thus not suitable\nto mine the potential discriminative information across views. In\naddition, no inherent consistency between two speciﬁc feature\nprojections is considered, while our model preserves cross-view\nconsistency by learning an asymmetric metric under a cross-view\nconsistency regularization. (2) Ours can jointly learn the feature\nrepresentation and the asymmetric metric end to end.\nThis work is based on our preliminary work which presented\nCAMEL [52]. In addition to giving a more detailed description\nand analysis of the proposed linear model CAMEL, the major\ndifferences are as follow: (1) We present a novel unsupervised\ndeep framework DECAMEL. Compared to the linear metric\nlearning model CAMEL, we propose the novel loss function,\narchitecture and learning algorithm, which together enable DE-\nCAMEL to perform joint learning of feature representation and\nmetric in an unsupervised way. By this means, compared to\nCAMEL, DECAMEL can learn a more compact cross-view cluster\nstructure, potentially facilitating mining the underlying cross-view\ndiscriminative information. (2) We propose the view clustering that\ncan signiﬁcantly improve the generalizability and scalability of\nDECAMEL. (3) We present more in-depth discussion and analysis\non the proposed framework, including a series of visual results\nwhich show intuitively and progressively how DECAMEL works.\nMoreover, we conduct more extensive evaluations for comparisons\nand analysis.\nCITATION INFORMATION: DOI 10.1109/TPAMI.2018.2886878, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n5\nFig. 2. Illustration of our framework DECAMEL. We follow the brown arrows to inspect it. We extract features for person images by a deep network.\nDue to view-speciﬁc conditions, the initial feature space has severe view-speciﬁc bias: the red triangles (data points from Camera 1) and blue circles\n(from Camera 2) are far apart. We perform CAMEL to learn an initial asymmetric metric. In the shared space produced by the asymmetric metric, the\nview-speciﬁc bias is alleviated. By optimizing the proposed unsupervised loss, DECAMEL jointly learns the feature representation and asymmetric\nmetric in an end-to-end manner. During testing, pairwise distance can be computed by Eq. (2). (Best viewed in color).\n2.2\nUnsupervised Metric Learning\nAlthough unsupervised metric learning has not been exploited in\nRe-ID, there are a few works that exploit unsupervised metric\nlearning in other ﬁelds [54], [55], [56], [57]. Ye et.al. [54] proposed\nto jointly learn a metric and a clustering separation for better\nclustering results. Qin et.al. [55] proposed to learn a metric based\non regularized neighborhood component analysis for clustering\nanalysis. Cinbis et.al. [56] proposed to mine video information to\nﬁnd positive and negative pairs of faces, and thus learning a metric\nfor video face recognition. Jiang et.al. [57] proposed a diffusion-\nbased approach to improve an input similarity metric for vision\ntasks like image segmentation and clustering.\nSpeciﬁcally, our model is closely related to the Adaptive Metric\nLearning (AML) [54]. AML learns a transformation G to project\nthe training data x onto a low dimensional space: ˆx = GTx. It\nminimizes the sum of squared error (SSE) in the subspace:\nSSE =\nX\nˆx\ndM(ˆx, ˆc)2,\n(3)\nwhere dM is the Mahalanobis distance, and ˆc denotes the cluster\ncentroid to which ˆx belongs.\nOur model is different from AML in that ours performs cross-\nview clustering and explicitly models the view-speciﬁc bias in the\ncontext of unsupervised Re-ID. We propose to alleviate the bias\nby learning an asymmetric metric. Furthermore, we propose to\nembed the metric learning into the deep neural networks for a\njoint learning of feature representation and the asymmetric metric.\nThe asymmetric modelling is also used in cross-modal retrieval\n[58], [59], [60], where Canonical Correlation Analysis (CCA)\n[61] and Partial Least Squares (PLS) [62] based methods are\narguably the most popular. CCA and PLS also learn different\nprojections for different modalities to induce a latent space, where\nthe correlation/covariance of pairwise cross-modal training sam-\nples is maximized. However, they do not address the view-speciﬁc\nbias problem in person Re-ID since the projections are learned\nseparately and inconsistently. Moreover, they require sufﬁcient\npairwise labelled data is required by these methods. There are\nalso some works on supervised deep metric learning [63], [64],\n[65], [66], [67], [68]. However, these models require substantial\nlabelled data for training. In contrast, our model can directly learn\nfrom unlabelled data and thus it is free from requiring large amount\nof expensive labelled data.\n2.3\nUnsupervised Deep Clustering Embedding\nOur model is also related to the unsupervised deep clustering\nembedding technique [69], [70], [71]. Xie et.al. [69] proposed\nto jointly learn cluster membership and deep representation by\nminimizing the KL divergence between original data distribution\nand a target distribution. Yang et.al. [70] proposed to jointly\nlearn image clusters and deep representation using a convolutional\nneural network. Wang et.al. [71] proposed to learn a task-speciﬁc\ndeep architecture based on sparse coding for clustering.\nDECAMEL is different from all these models in that it joins\nunsupervised asymmetric metric learning with cross-view cluster-\ning, which is speciﬁcally designed for cross-view matching in Re-\nID, while others use standard symmetric metrics in clustering.\n3\nAPPROACH\nIn this section, we progressively develop our unsupervised Re-ID\nframework, i.e., the DEep Clustering-based Asymmetric MEtric\nLearning (DECAMEL). Speciﬁcally, our framework ﬁrst learns\nan initial asymmetric metric by a linear unsupervised model\n(CAMEL) and then embeds the metric into a deep network by\njointly learning feature and metric. These two steps are based\non the asymmetric metric clustering, so we ﬁrst introduce the\nasymmetric metric clustering and develop the linear model. Then,\nwe introduce a novel loss function for the unsupervised deep joint\nlearning. We show an overview of our framework in Figure 2.\n3.1\nUnsupervised Asymmetric Metric Learning\nNow let us dive into the details and develop the model step by step.\nUnder a general unsupervised Re-ID setting, we have V camera\nviews. From each of them, we have collected Nv (v = 1, · · · , V )\nimages, and thus we have N = PV\nv=1 Nv unlabelled images\nfor training. Here we assume that feature representation for the\ntraining images are given, and the training set is denoted as\nX = {xi, vi}N\ni=1, where xi is the feature vector and vi denotes\nwhich camera view it comes from. Note that in visual surveillance,\nthe camera view “label” vi is naturally available for each image,\nCITATION INFORMATION: DOI 10.1109/TPAMI.2018.2886878, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n6\nsince its straightforward to know by which camera an image\nis captured in a camera network. Here we follow a popular\nassumption that during training and testing, the camera views are\nthe same [3, 5, 6, 8, 9, 11, 14, 15, 18-40, 44, 47-50]. We will also\ndiscuss a novel view-extendable setting in Sec. 3.4\nWe are looking for some transformations to map the data into a\nshared space, where we can better separate the images of different\npersons. A natural idea is to decrease the intra-class (here, a class\nis a person) discrepancy and enlarge the inter-class discrepancy.\nIn an unsupervised scenario, however, we have no labelled data\nto strictly separate visually similar persons. Therefore, we relax\nthe original idea: we focus on gathering similar person images\ntogether, and thereby separating relatively dissimilar ones. We\nformulate it by an objective like that of k-means clustering [72]:\nmin\nU,{ck}K\nk=1\nfintra = 1\nN\nK\nX\nk=1\nX\ni∈Ck\n∥U Txi −ck∥2,\n(4)\nwhere U is the view-generic universal feature transformation, K is\nthe number of clusters, ck denotes the centroid of the k-th cluster\nand Ck = {i|U Txi ∈k-th cluster}.\nHowever, clustering can be largely affected by the view-\nspeciﬁc bias when applied in the cross-view problems. In Re-\nID, the variances across camera views like different lighting\nconditions, human pose variations and occlusions [28] can be very\ndramatic. They are disturbing or even dominating in searching the\nsimilar person images across views during clustering procedure.\nTo address this problem, we learn speciﬁc transformations for each\nview rather than a generic one, to explicitly take the view-speciﬁc\nfeature distortion into account and to alleviate the view-speciﬁc\nbias. As discussed in Eq. (2), the idea can be further formulated by\nmin\n{Uv}V\nv=1,{ck}K\nk=1\nfintra = 1\nN\nK\nX\nk=1\nX\ni∈Ck\n∥U T\nvixi −ck∥2\ns.t.\nU T\nv ΣvUv = I\n(v = 1, · · · , V ),\n(5)\nwhere Uvi is the speciﬁc feature transformation for the vi-th\ncamera view, vi denotes which camera view xi comes from,\nΣv = P\nxt:vt=v xtxT\nt /Nv + I is a covariance matrix, and I\nrepresents the identity matrix which is used to avoid singularity\nof the covariance matrix. The transformation for each instance\nxi is determined by vi. The quasi-orthogonal constraints on Uv\nensure that the model will not simply give zero matrices. By\njointly learning the asymmetric metric and cross-view clustering,\nwe actually realize an asymmetric metric clustering on Re-ID data\nacross camera views.\nMathematically, if we minimize this objective function, every\nUv will largely depend on the data distribution from the v-th\nview. Since there is view-speciﬁc bias on each view, any pair\nof transformations—Uv and Uw—could be arbitrarily different\naccording to the biases. However, large inconsistencies among\nthe learned transformations are not what we expect, since these\ntransformations are with respect to person images from different\nviews. Although under different conditions, the subjects are human\nbeings, and thus they are inherently correlated and not heteroge-\nneous. Therefore, largely different projection basis pairs would fail\nto capture the discriminative nature of the person images.\nHence, to strike a balance between the ability to preserve the\ncross-view consistency and the ability to alleviate view-speciﬁc\nbias, we add a cross-view consistency regularization term to\nour objective function. The cross-view consistency regularization\npenalizes the discrepancy between any pair of correlated transfor-\nmation basis uc\nv and uc\nw, where uc\nv is the c-th column of Uv. Thus,\nwe formulate it as:\nfconsistency =\nX\nv,w\nX\nc\n∥uc\nv −uc\nw∥2\n2 =\nX\nv,w\n∥Uv −Uw∥2\nF ,\n(6)\nwhere ∥·∥is the Frobenius norm of a matrix. And then, our\noptimization task is given by\nmin\n{Uv}V\nv=1,{ck}K\nk=1\nfobj = fintra + λfconsistency\n= 1\nN\nK\nX\nk=1\nX\ni∈Ck\n∥U T\nvixi −ck∥2 + λ\nX\nv,w\n∥Uv −Uw∥2\nF\ns.t.\nU T\nv ΣvUv = I\n(v = 1, · · · , V ),\n(7)\nwhere λ is the cross-view regularizer. We call the above model the\nClustering-based Asymmetric MEtric Learning (CAMEL).\nWe will show by an illustration that the asymmetric metric can\nalleviate the view-speciﬁc bias in the Re-ID problem in Sec. 4.1,\nand show that the cross-view consistency regularization contributes\nmuch in our framework in Sec. 5.4.2.\nRemark 1: Cross-view Consistency Regularization for\nthe Metric. We note that although asymmetric metric learn-\ning has been successfully applied in supervised Re-ID [3],\n[28], it is a pseudo metric rather than a strict metric [28],\nbecause it may not meet the coincidence property: given\ntwo identical feature vectors xi and xj (xi = xj) from\ndifferent camera views vi and vj, the asymmetric metric may\nnot guarantee d(xi, xj) = 0. In this aspect, the cross-view\nconsistency regularization plays a role to control an upper\nbound of coincidence discrepancy. In fact, according to the\nCauchy Inequality, we have\nd(xi, xj) = ∥U T\nvixi −U T\nvjxj∥2 = ∥U T\nvixi −U T\nvjxi∥2\n≤∥xi∥2 · ∥Uvi −Uvj∥F .\n(8)\nThe cross-view consistency regularization controls ∥Uvi −\nUvj∥F which is a scaled upper bound of the coincidence\ndiscrepancy. Thus, it makes the learned asymmetric metric\nmore mathematically principled and rigorous.\n3.2\nDeep Unsupervised Asymmetric Metric Embedding\nBased on CAMEL and the foregoing analysis, we can fur-\nther propose our framework, named the DEep Clustering-based\nAsymmetric MEtric Learning (DECAMEL). DECAMEL embeds\nthe unsupervised asymmetric metric into the network by jointly\nlearning the feature representation and the asymmetric metric.\nIn the last subsection we assume that we have extracted\nfeatures for the training images. Here, we specify the feature\nextractor function f as a deep convolutional network, which is\nparameterized by Θ. So, the feature representation is given by\nxi = f(Mi; Θ), where M is an image from our training image\nset M = {Mi, vi}N\ni=1. Our goal is to develop an end-to-end\nframework, g(Mi, vi; Θ, {Uv}V\nv=1), which includes the feature\nextraction and the unsupervised asymmetric metric learning.\nRecall the objective in Eq. (7), where in CAMEL we aim to\nminimize it under constraints. To avoid trivial implementation and\nbetter adapt to the back-propagating algorithm in the optimization\nprocedure, we develop a soft version of constraints, by replacing\nCITATION INFORMATION: DOI 10.1109/TPAMI.2018.2886878, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n7\nthe constraints with a regularization term fconstraint similar to\nfconsistency:\nfconstraint =\nV\nX\nv=1\n∥U T\nv ΣvUv −I∥2\nF .\n(9)\nThis technology is widely used in the machine learning problems.\nFor example, the well-known Tikhonov regularization is a soft\nversion of the Ivanov regularization [73]. By this way, we have\nour loss function of DECAMEL as follow:\nfloss = fintra + λfconsistency + γfconstraint\n= 1\nN\nK\nX\nk=1\nX\ni∈Ck\n∥U T\nvixi −ck∥2 + λ\nX\nu,w\n∥Uv −Uw∥2\nF\n+ γ\nV\nX\nv=1\n∥U T\nv ΣvUv −I∥2\nF ,\n(10)\nwhere γ is the soft-constraint parameter. In our experiments we\nset γ = 10 to basically ensure the constraints. We empirically ﬁnd\nthis hyper-parameter needs no exhaustive tuning across datasets.\nCAMEL is a linear metric learning model. Intrinsically, it can-\nnot discover the underlying non-linear cross-view cluster structure\nof the feature representation. In addition, the feature extraction\nis separated and independent from metric in CAMEL. This leads\nto sub-optimality because the feature representation might have\nthe capacity to be further improved according to the metric. In\ncontrast, DECAMEL addresses these problems naturally. It jointly\nlearns the feature representation and the asymmetric metric with\nnon-linearity capacity. Thus, it learns a better cross-view cluster\nstructure.\n3.3\nOptimization and Algorithm\nBefore performing the gradient descent to train DECAMEL,\nwe ﬁrst learn an unsupervised asymmetric metric {Uv}V\nv=1 by\nCAMEL to initialize the metric layer of DECAMEL, as well as\nthe cluster results {ck}K\nk=1. So we ﬁrst introduce how to optimize\nthe objective of CAMEL. We will see that the metric initialization\nby CAMEL contributes a lot to the whole framework in Sec 5.4.4.\n3.3.1\nMetric Initialization by CAMEL\nFor convenience, let yi = U T\nvixi and Y = [y1, · · · , yN]. We\nrewrite our objective function using trace instead of sum. The ﬁrst\nterm fintra can be rewritten as [74]\nK\nX\nk=1\nX\ni∈Ck\n∥U T\nvixi −ck∥2 = [Tr(Y TY ) −Tr(HTY TY H)],\n(11)\nwhere\nH =\n\u0002h1, ..., hK\n\u0003\n,\nhT\nk hl =\n(\n0\nk ̸= l\n1\nk = l ,\n(12)\nand\nhk =\n\u00020, · · · , 0, 1, · · · , 1, 0, · · · , 0, 1, · · ·\u0003T /√nk\n(13)\nis an indicator vector with the i-th entry corresponding to the\ninstance yi, indicating that yi is in the k-th cluster if the cor-\nresponding entry is non-zero.\nNow we construct an assignment function ξ : X →RV d\nwhere d denotes the feature dimension:\nξ({x, v}) =\n\u0002\n0T, · · · , 0T, xT, 0T, · · · , 0T\u0003T ,\n(14)\nwhere 0 is a zero column vector which has the same size as x,\nand x is placed in the v-th “entry”. Then we can construct f\nX =\n[ex1, · · · , exN] ∈RV d×N, where exi = ξ({xi, vi}). Besides, we\nconstruct\neU =\n\u0002\nU T\n1 , · · · , U T\nV\n\u0003T ,\n(15)\nso that\nY = eU T f\nX.\n(16)\nSubstitute Eq. (16) into Eq. (11) and thus fintra becomes\nfintra = 1\nN Tr(f\nXT eU eU T f\nX) −1\nN Tr(HT f\nXT eU eU T f\nXH).\n(17)\nAs for the second term, we can also rewrite fconsistency as\nfconsistency =\nX\nv,w\n∥Uv −Uw∥2\nF = Tr( eU TD eU),\n(18)\nwhere\nD =\n\n\n(V −1)I\n−I\n−I\n· · ·\n−I\n−I\n(V −1)I\n−I\n· · ·\n−I\n...\n...\n...\n...\n...\n−I\n−I\n−I\n· · ·\n(V −1)I\n\n∈RV d×V d\n(19)\nhas V × V block entries Then, it is reasonable to relax the\nconstraints\nU T\nv ΣvUv = I\n(v = 1, · · · , V )\n(20)\nto\nV\nX\nv=1\nU T\nv ΣvUv = eU T eΣ eU = V I,\n(21)\nwhere eΣ = diag(Σ1, · · · , ΣV ), because what we expect is to\nprevent each Uv from shrinking to a zero matrix. The relaxed\nversion of constraints is able to satisfy such a need, and it\nallows more elegant optimization. By now we can rewrite our\noptimization task as follows:\nmin\ne\nU,H\nfobj = 1\nN Tr(f\nXT eU eU T f\nX) + λTr( eU TD eU)\n−1\nN Tr(HT f\nXT eU eU T f\nXH)\ns.t.\neU T eΣ eU = V I.\n(22)\nWe can easily ﬁnd that our objective function is non-convex.\nFortunately, in the form of Eq. (22), we can ﬁnd that once H\nis ﬁxed, Lagrange’s method can be applied to our optimization\ntask. And from Eq. (11), we can ﬁnd that it is exactly the objective\nof k-means clustering [72] with respect to yi once eU is ﬁxed.\nThus, we can adopt an EM-like alternating algorithm to solve the\noptimization problem.\nFix H and optimize eU. After ﬁxing H and applying the method\nof Lagrange multiplier, our optimization task (22) is transformed\ninto an eigen-decomposition problem as follow:\nGu = ηu,\n(23)\nwhere η is the Lagrange multiplier (and also is the eigenvalue here)\nand\nG = eΣ−1(λD + 1\nN\nf\nX f\nXT −1\nN\nf\nXHHT f\nXT).\n(24)\neU can be obtained by solving this eigen-decomposition problem.\nFix eU and optimize H. As for the optimization of H, we can\nsimply ﬁx eU and conduct k-means clustering in the learned space.\nEach column of H, hk, is thus constructed by Eq. (13) according\nto the k-means clustering result.\nCITATION INFORMATION: DOI 10.1109/TPAMI.2018.2886878, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n8\n(a) Feature representation distribution\n(b) Using symmetric metric\n(c) Using asymmetric metric\nFig. 3. Illustration of asymmetric metric alleviating view-speciﬁc bias. The data is randomly sampled from the SYSU dataset [28]. We performed\nPCA for visualization. Blue circles and red triangles represent data points from two camera views. (a) Cross-view data distribution in original\nfeature representation space. View-speciﬁc bias is severe here, since one can easily draw a boundary to roughly separate the circles and triangles.\n(b) Distribution in the shared space after projected by the learned view-generic transformation (symmetric metric). The bias is not alleviated. (c)\nDistribution in the shared space after projected by the learned view-speciﬁc transformations (asymmetric metric). The bias is largely alleviated. (Best\nviewed in color).\nFig. 4. Illustration of DECAMEL learning the better cross-view cluster structure via jointly learning the feature representation and the asymmetric\nmetric. We perform PCA for visualization. Images of an identity are indicated by a speciﬁc color (e.g. all red triangles and circles are images of the\nﬁrst identity in the feature space). The numbers in the upper left of each ﬁgure indicates different stages, from initial to convergence. The two ﬁgures\nin each column are synchronous and corresponding to each other. Data points are only a subset from those in Figure 3 for clarity. Speciﬁcally, the\ninitial stages (the leftmost column) are subsets of Figure 3(a) and Figure 3(c), respectively. (Best viewed in color and please refer to the text in Sec.\n4.2 for more analysis. Please zoom in for better visual quality).\n3.3.2\nOptimizing DECAMEL by Gradient Descent\nAfter obtaining the initial {Uv}V\nv=1 and {ck}K\nk=1, we can opti-\nmize DECAMEL. We adopt the stochastic gradient descent (SGD)\nto optimize DECAMEL. The gradients are\n∂floss\n∂U T\nv x = 2(U T\nv x −ck)\n(25)\nand\n∂floss\n∂Uv\n= 2(xxTUv −xck),\n(26)\n∂floss\n∂eU\n= 2λD eU + 4γ eΣ eU( eU T eΣ eU −V I).\n(27)\nWe note that the gradient in Eq. (25) is with respect to each\nsample and the gradients in Eq. (26) and Eq. (27) are with respect\nto the asymmetric metric. Thus, we refer to Eq. (25) as the\nsample gradient and refer to Eq. (26) and Eq. (27) as the metric\ngradient in the following Remark 2. We show the main algorithm\nof DECAMEL in Algorithm 1. Note that fobj is guaranteed to\nCITATION INFORMATION: DOI 10.1109/TPAMI.2018.2886878, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n9\nRemark 2: Explanation for Deep Metric Embedding. We\ncan see from Eq. (25), (26) and (27) that, the sample gradient\nﬂows over the whole network, while the metric gradient only\nﬂows to the metric. However, the metric is actually embedded\ninto the sample gradient: according to the chain rule, the\nsample gradient for the feature extractor parameter Θ is\n∂floss\n∂Θ\n=\n∂floss\n∂f(M; Θ)\n∂f(M; Θ)\n∂Θ\n= ∂floss\n∂x\n∂f(M; Θ)\n∂Θ\n= ∂floss\n∂U T\nv x\n∂U T\nv x\n∂x\n∂f(M; Θ)\n∂Θ\n= 2(U T\nv x −ck)U T\nv · ∂f(M; Θ)\n∂Θ\n.\n(28)\nThus, the metric U T\nv is back-propagated to the whole net-\nwork. As we will see in Sec. 4.2, the jointly learned feature\nbears resemblance to the metric. Furthermore, we will also\nsee in Sec. 5.4.3 that this improves the cross-view discrim-\ninability of the feature. These observations seem like the\nmetric is being “embedded” into the feature, and thus we\nrefer to it as the “deep metric embedding”.\nAlgorithm 1: DECAMEL\nInput : The training images M, the deep feature extractor f(·; Θ)\n1 Training:\n2 Metric initialization:\n3 Extract feature representations using f to obtain the initial feature set X.\n4 Conduct k-means clustering in X to obtain {ck}K\nk=1 and to initialize H\naccording to Eq. (12) and (13).\n5 Fix H and solve the eigen-decomposition problem described by Eq. (23) and\n(24) to construct e\nU.\n6 t ←1 where t denotes each step in the following loop.\n7 while {f t\nobj} not converged do\n• Alternate ﬁxing e\nU and H while optimizing the other.\n• t ←t + 1.\n8 end\n9 Decompose e\nU to obtain {Uv}V\nv=1.\n10 Initialize the deep framework g(·, ·; Θ, {Uv}V\nv=1) using Θ and {Uv}V\nv=1.\n11 End-to-end joint learning:\n12 Update {ck}K\nk=1 from H according to Eq. (12) and (13).\n13 while maximum iteration not reached do\n• Update Θ and {Uv}V\nv=1 by performing SGD using the gradients in\nEq. (25), (26) and (27).\n• Update {ck}K\nk=1 while ﬁxing Θ and {Uv}V\nv=1.\n14 end\n15 Testing:\n16 Given two testing images {Mi, vi} and {Mj, vj}, the distance/dissimilarity is\ncomputed by ||g(Mi, vi) −g(Mj, vj)||2.\nconverge as proved in [52]. It typically reaches convergence within\n20 iterations.\n3.4\nView Clustering: Generalizing to unseen views\nIn the beginning of this section we follow the conventional Re-\nID setting that assumes the training and testing camera views are\nthe same. However, some realistic large-scale applications might\nneed to be view-extendable, i.e. new unseen camera views might\nbe added to the surveillance network after training. In this case,\nthe generalizability to new views becomes important. We propose\na general method toward view-extendable scenarios for achieving\nbetter generalizability. The proposed method does not need to re-\ntrain the model when new views are added after training.\nThe main idea is that instead of learning feature transforma-\ntions for each camera view, we can learn transformations for some\nAlgorithm 2: DECAMEL with View Clustering\nInput: The training images M, the deep feature extractor f(·; Θ)\n1 Training:\n2 Compute the view representations {wv}V\nv=1 by Eq. (30) .\n3 Conduct k-means clustering in {wv}V\nv=1 to obtain the cluster separation\nBj = {v|wv ∈j-th cluster}.\n4 For each image Mi in the training set, reassign a view label v′\ni ←j where\nvi ∈Bj to it. So we now have 1 ≤v′\ni ≤J (the number of view clusters).\n5 Feed M′ = {Mi, v′\ni} to Algorithm 1 to train a deep framework\ng(·, ·; Θ, {Uj}J\nj=1).\n6 Use the learned feature extractor g(·; Θ) to compute view prototypes/centroids\n{bj}J\nj=1.\n7 Testing for an unseen view u:\n8 Extract the view representation wu using the learned feature extractor g(·; Θ).\n9 Assign this view to a view prototype j where j = arg minj∥wu −bj∥2.\n10 Assign a view label j to all testing images from this view.\n11 Follow the testing procedure in Algorithm 1.\ngeneralizable view prototypes, which shall cover the most typical\nview-speciﬁc conditions. Then, if a new, unseen view is added\nafter training, we can assign the new view to a view prototype and\nthus use the corresponding feature transformation. We elaborate\nour strategy in the following.\nTo better motivate our method, we start from deﬁning pairwise\ndissimilarity/distance dV (·, ·) of two camera views. As our model\naddresses view-speciﬁc bias in an overall view level, a straight-\nforward idea is to deﬁne dV (·, ·) as the distance between dis-\ntributions of images from the two views. We adopt the simpliﬁed\n2-Wasserstein distance [75], [76]1, which has been shown effective\nand easy-to-compute in many vision tasks [76], [77], deﬁned as:\ndV (V iewu, V iewv)2 = 1\n2(∥mu −mv∥2\n2 +∥σu −σv∥2\n2), (29)\nwhere mv is the mean vector of all training sample features\n(extracted by the feature network) from V iewv, σv is the cor-\nresponding standard deviation vector, and mu, σu are similarly\ndeﬁned. From Eq. (29) we can deﬁne the view representation of\nV iewv as:\nwv = [mT\nv , σT\nv ]T,\n(30)\nso that the L2 distance of the view representations is now\nequivalent to the distributional distance, i.e. 1\n2∥wu −wv∥2\n2 =\ndV (V iewu, V iewv)2.\nWith the view representation, we can model the generalizable\nview prototype as a cluster of views. View clusters can be gener-\nalizable and robust, since slight deviation of viewing condition\nis allowed in a view cluster, and hence each view cluster can\ncover and deal with the potential deviations of a new unseen view.\nFor example, in a shopping mall, a newly added camera view\nfacing a passageway may probably ﬁnd a view cluster consisting\nof several other passageways views that share similar viewing\nconditions. One can use any clustering algorithm according to\nspeciﬁc requirements, and in our method we adopt the simplest\nK-means clustering, as formulated by:\nmin\n{bj}J\nj=1\nfvc = 1\nV\nJ\nX\nj=1\nX\nv∈Bj\n∥wv −bj∥2\n2,\n(31)\nwhere V is the number of views, J is the number of view clusters,\nbj denotes the centroid of the j-th cluster and Bj = {v|wv ∈j-th\ncluster}.\n1. This simpliﬁed 2-Wasserstein distance makes a Gaussian assumption over\nthe sample features, which is also empirically observed in our experiments. We\nrefer the reader to [76] for further justiﬁcation.\nCITATION INFORMATION: DOI 10.1109/TPAMI.2018.2886878, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n10\nAfter obtaining J view clusters, we regard the view clusters as\nthe views for training DECAMEL. After training, if a new view\ncomes, we ﬁrst assign this view to the most similar view cluster\nand use its feature transformation. We refer to this method as DE-\nCAMEL with View Clustering (DECAMELV C). We summarize\nDECAMELV C in Algorithm 2. By DECAMELV C, the learned\nfeature transformations can naturally generalize to unseen views.\nScalability. Solving Eq. (23) requires eigen decomposition whose\ncomputational complexity is O((V d)3) where V is the number of\nviews and d is the feature dimension which is constant. In the view-\nextendable setting, we can pre-deﬁne the number of generalizable\nview prototypes J (J ≪V ), leading to a constant computational\ncomplexity of the eigen decomposition in Eq. (23). In the con-\nventional Re-ID setting where training views and testing views\ncome from the same pool, we can perform view clustering (and\npre-deﬁne J) only for CAMEL (i.e. metric initialization). Then,\nthe i-th (1 ≤i ≤J) learned transformation is used to initialize\nall the view-speciﬁc transformations that belong to the views in\nthe i-th view cluster (so that in the joint learning of DECAMEL\nwe still learn V transformations). We refer to this method as\nDECAMEL Initialized with View Clustering (DECAMELIV C).\nWe ﬁnd that using a relatively small J for DECAMELIV C could\nachieve very close performance as DECAMEL. The experimental\nresult is shown in the supplementary material.\nFinally we also note that DECAMELV C is a generalization of\nboth DECAMEL and its symmetric version in the conventional Re-\nID setting. If we set J = V , the method is equivalent to original\nDECAMEL. On the other hand, if we set J = 1, the method de-\ngrade to learning a universal feature transformation. In this sense,\nDECAMELV C allows us to ﬂexibly control the generalizability\nvs. ability to accurately model each speciﬁc viewing conditions,\naccording to the given application scenario.\n4\nINSIGHT UNDERSTANDING\nIn this section, we show visual results to provide intuitive per-\nceptions on the mechanism of our framework. We ﬁrst illustrate\nthat the learned asymmetric metric can alleviate the view-speciﬁc\nbias in the original/initialized feature space more easily than a\nsymmetric one. Then, we illustrate how DECAMEL progressively\nlearns a better cross-view cluster structure based on the learned\nasymmetric metric by joint learning, and how it mines the potential\ncross-view discriminative information.\n4.1\nAsymmetric Metric Alleviates Views-speciﬁc Bias\nIn Figure 3, we show three cross-view data distributions, which are\nin the original/initialized feature space, the shared space learned by\na symmetric version of CAMEL and the shared space learned by\nCAMEL, respectively.\nWe can see from Figure 3(a) that in the original feature space,\nthere is severe view-speciﬁc bias. One can easily draw a borderline\nto separate the data from different views. Then, in Figure 3(b), the\nview-speciﬁc bias is still severe in the shared space found by the\nlearned symmetric metric (view-generic transformation), mainly\nbecause only some view-generic rotations and translations are\ntaken for both views. In contrast, in Figure 3(c), in the shared space\nlearned by CAMEL, the view-speciﬁc bias is alleviated so that\ndata points from two views are much more overlapped with each\nother. One of the main reasons is that the learned view-speciﬁc\ntransformations provide more ﬂexibility to facilitate alleviating the\nbias.\n4.2\nDECAMEL Learns Cross-view Cluster Structure\nIn Figure 4, we further show the cross-view distributions in both\nthe feature space and the shared space learned by DECAMEL in\ndifferent stages. By examining the distributions through stages, we\ncan obtain an intuitive understanding for DECAMEL, in terms of\nhow it progressively learns a better cross-view cluster structure.\nInitialization. The metric initialization (i.e., CAMEL) learns a\npreliminary cross-view cluster structure. We ﬁrst look at the\nleftmost column in Figure 4. We can see that in the original feature\nspace (the leftmost ﬁgure in the upper row), the view-speciﬁc bias\nis severe as we have seen in the last subsection (data points are a\nsubset of Figure 3(a)). In contrast, after the initial metric layer (the\nleftmost ﬁgure in the lower row), the bias is alleviated (data points\nare a subset of Figure 3(c)). Then, by comparing them, we can see\nthat CAMEL learns a preliminary cross-view cluster structure, i.e.,\nthe cross-view data points representing the same identity (color)\nroughly get closer to each other.\nJoint learning. The joint learning facilitates learning a better\ncross-view cluster structure. The upper row in Figure 4 shows\nthe feature distributions in several stages. Through them, we can\nﬁnd that the feature extractor network is guided by the learned\ncross-view cluster structure to improve the feature representation,\nand in the convergence stage the view-speciﬁc bias is alleviated.\nThis shows that the asymmetric metric is embedded into the whole\nnetwork, as we discussed in Remark 2. This is mutually helpful in\nlearning the asymmetric metric. The lower row in Figure 4 shows\nthe corresponding distributions after asymmetric metric layer.\nThrough them, we can ﬁnd that the asymmetric metric gradually\nlearns a better cross-view cluster structure. Take the purple and\nblue identities for example. In the initial shared space (the leftmost\nﬁgure in the lower row) there is around 1/3 data points of them\noverlapped with each other. However, along with the joint learning\nprocedure, they are getting more and more compact, and ﬁnally\nthere is nearly no overlap between them in the convergence stage.\nMining potential discriminative information. Finally, by com-\nparing the initial feature distribution (the leftmost ﬁgure in the\nupper row) with the convergence distribution (the rightmost ﬁgure\nin the lower row), we can ﬁnd that through the joint learning\nof DECAMEL, a better cross-view cluster structure is learned.\nSince the view-speciﬁc bias has been alleviated by the asym-\nmetric metric, the cross-view data points belonging to the same\nidentity can get closer to each other in the convergence stage,\nrather than entangling and overlapping with other identities in the\ninitialized feature space. Therefore, during this learning procedure\nDECAMEL is attempting to mine the potential cross-view discrim-\ninative information. We will further show a quantitative result on\nevaluating the learned cross-view cluster structures in Sec. 5.4.1,\nwhich experimentally validates that DECAMEL learns a better\ncross-view cluster structure compared to using a symmetric metric.\n5\nEXPERIMENTS\nIn this section we compare the performance of DECAMEL with\nother related unsupervised models to show the effectiveness. Then,\nwe perform experimental validations and analysis to provide fur-\nther comprehensive understanding of our framework.\n5.1\nDatasets\nWe conduct our comparisons on seven datasets, whose scales\nvary from hundreds to hundreds of thousands. In particular, since\nCITATION INFORMATION: DOI 10.1109/TPAMI.2018.2886878, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n11\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\nFig. 5. Samples of the datasets. Every two images in a column are from\none identity across two disjoint camera views. (a) VIPeR (b) CUHK01 (c)\nCUHK03 (d) SYSU (e) Market (f) ExMarket (g) MSMT17.\nTABLE 1\nOverview of dataset scales. “#” means “the number of”.\nDataset\nVIPeR\nCUHK01 CUHK03 SYSU\nMarket\nExMarket MSMT17\n# Samples\n1,264\n3,884\n13,164\n24,448\n32,668\n236,696\n126,441\n# Views\n2\n2\n6\n2\n6\n6\n15\nunsupervised models are more meaningful when the scale of\nproblem is larger, our experiments are conducted on relatively\nbigger datasets except VIPeR [78] which is small but widely used.\nVarious degrees of viewing condition variation can be observed in\nall these datasets (see Figure 5). A brief overview of the dataset\nscales can be found in Table 1.\nThe VIPeR dataset [78] contains 1264 images, where every two\nimages are captured for each identity from two camera views.\nThe CUHK01 dataset [79] contains 3, 884 images of 971 identi-\nties captured from two disjoint views.\nThe CUHK03 dataset [29] contains 13, 164 images of 1,360\npedestrians captured from six surveillance camera views. Pedes-\ntrian images were detected by a state-of-the-art pedestrian detector.\nThe SYSU dataset [28] includes 24, 448 RGB images of 502\npersons under two surveillance cameras. One camera view mainly\ncaptured the frontal or back views of persons, while the other\nobserved mostly the side views.\nThe Market-1501 dataset [80] (Market) contains 32, 668 images\nof 1, 501 pedestrians, each of which was captured by at most\nsix cameras. All of the images were cropped by a pedestrian\ndetector. There are some badly-detected samples in this datasets\nas distractors as well.\nThe ExMarket dataset. Unsupervised models are more mean-\ningful when the problem scale is larger due to the difﬁculty in\nlabelling substantial cross-view data. In order to evaluate unsuper-\nvised Re-ID methods on an even larger scale, we further combined\nthe MARS dataset [81] with Market. MARS is a video-based Re-\nID dataset which contains 20, 715 tracklets of 1, 261 pedestrians.\nAll the identities from MARS are of a subset of those from Market.\nWe then took 20% frames (each one in every ﬁve successive\nframes) from the tracklets and combined them with Market to\nobtain an extended version of Market (ExMarket). The imbalance\nbetween the numbers of samples from the 1, 261 persons and other\n240 persons makes this dataset more challenging and realistic.\nThere are 236, 696 images in ExMarket in total, and 112, 351\nimages of them are of training set.\nThe MSMT17 dataset [82] is currently the most large-scale\ndataset which contains 126, 441 images of 4, 101 persons cap-\ntured from 15 camera views during four days. Extreme lighting\nvariations can be observed across camera views.\n5.2\nSettings\nExperimental Protocols. We follow a widely adopted protocol on\nVIPeR [19], i.e., randomly dividing the images into two halves,\none of which is used as training set and the other as test set. This\nprocedure is repeated 10 times to offer an average performance.\nThis dataset only allows single-shot experiments.\nThe experimental protocol for CUHK01 was the same as that\nin [19]. We randomly selected images from 485 persons to form\nthe training set and images from the rest 486 persons formed the\ntesting set. The evaluating procedure is repeated 10 times. We\nperform both multi-shot and single-shot experiments. That is, in\nthe single-shot setting only one image of each gallery person is\nused for evaluation, whereas in the multi-shot setting all the images\nof each gallery person are used. In both settings, all probe images\nare used.\nThe CUHK03 dataset is provided together with its recom-\nmended evaluating protocol [29]. We follow the provided protocol,\nwhere images of 1, 260 persons are chosen as the training set, and\nthe remainders as testing set. This procedure is repeated 10 times.\nBoth multi-shot and single-shot experiments are conducted.\nAs for the SYSU dataset, we randomly pick half pedestrians\nas training set and the others as testing set. In the testing stage,\nwe basically follow the protocol as in [28]. That is, we randomly\nchoose one and three images of each pedestrian as gallery for\nsingle-shot and multi-shot experiments, respectively. We repeat the\ntesting procedure by 10 times.\nMarket is somewhat different from others. The evaluation\nprotocol is also provided along with the dataset [80]. Since images\nof one person come from at most six views, the provided protocol\ndoes not adopt the single-shot setting. Instead, the protocol adopts\nthe multi-shot setting and requires both the cumulative matching\ncharacteristic (CMC) and the mean average precision (MAP) [80].\nThe protocol of ExMarket is identical to Market since the identities\nfrom both datasets are completely the same as we mentioned\nabove. For MSMT17 [82] we also use the provided protocol.\nImplementation Details. We adopt the 56-layer ResNet [84] as\nthe feature extractor network where the dimension is 64. The\nnetwork is pre-trained using the JSTL pre-training technique\nproposed in [32], which used softmax loss with a concatenation\nof several Re-ID datasets including CUHK03 [29], CUHK01 [79],\nPRID [85], VIPeR [78], 3DPeS [86], i-LIDS [87] and Shinpuhkan\n[88]. We do not exploit ﬁne-tuning or the domain guided dropout\nproposed in [32]. Note that we do not use any label of the\ntarget dataset. For example, when pre-training the feature extractor\nnetwork for the CUHK03 dataset, we exclude all the training\nsamples from the CUHK03 dataset, to guarantee an unsupervised\nsetting [52]. After pre-training, we remove the last fully-connected\nlayer and take the output of the second-last global average pooling\nlayer [84] as our feature. Also note that for MSMT17 [82], since\nit is highly challenging for unsupervised setting due to extreme\nlighting variations, we add Market-1501, SYSU and the Duke\ndataset [89], [90] to its pre-training set to improve the baseline\nfeature.\nWe set λ = 0.01, K = 500 and ﬁx them for all the datasets in\nthe following comparisons. We also show a parameter evaluation\non these major hyper parameters which are corresponding to\ncertain characteristics of DECAMEL. We typically set the SGD\niterations to 10, 000 and the learning rate to 0.005 which is divided\nby 5 after 5, 000 iterations. No weight decay (L2 regularization)\nCITATION INFORMATION: DOI 10.1109/TPAMI.2018.2886878, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n12\nTABLE 2\nComparison with related unsupervised models: single-shot (“single”) and multi-shot (“multi”) rank-1 matching rate and MAP in percentage. In each\ncolumn, the best is indicated in red and the second in blue.\nDataset\nVIPeR\nCUHK01\nCUHK01\nCUHK03\nCUHK03\nSYSU\nSYSU\nMarket\nExMarket\nMSMT17\nMeasure\nsingle\nsingle\nmulti\nsingle\nmulti\nsingle\nmulti\nmulti(MAP)\nmulti(MAP)\nmulti(MAP)\nDIC [48]\n29.94\n49.31\n52.85\n27.38\n36.51\n21.28\n28.56\n50.21(22.68)\n52.18(21.19)\n22.81(7.01)\nISR [46]\n27.53\n53.17\n55.66\n31.13\n38.50\n23.16\n33.77\n40.32(14.27)\n42.99(15.74)\n21.50(6.10)\nRKSL [47]\n25.76\n45.41\n50.13\n25.79\n34.75\n17.64\n23.01\n33.97(11.03)\n34.86(10.40)\n15.41(4.30)\nSAE [83]\n20.70\n45.33\n49.94\n21.18\n30.51\n18.02\n24.15\n42.40(16.23)\n43.97(15.10)\n19.29(5.50)\nJSTL [32]\n25.73\n46.26\n50.61\n24.66\n33.15\n19.92\n25.59\n44.69(18.36)\n46.41(16.68)\n21.24(6.05)\nAML [54]\n23.10\n46.78\n51.14\n22.19\n31.41\n20.88\n26.39\n44.71(18.36)\n46.20(16.22)\n21.16(6.08)\nUsNCA [55]\n24.27\n47.01\n51.70\n19.76\n29.59\n21.07\n27.18\n45.22(18.91)\n47.03(16.91)\n22.01(6.53)\nDECAMEL\n34.15\n65.81\n69.00\n38.27\n45.82\n36.14\n43.90\n60.24(32.44)\n62.98(33.28)\n30.34(11.13)\n(a) CUHK01\n(b) CUHK03\n(c) SYSU\n(d) Market\n(e) ExMarket\n(f) MSMT17\nFig. 6. CMC curves for comparisons with related unsupervised models. In each legend, the ﬁgure beside the model name is the rank-1 matching\nrate. For clarity, we omit VIPeR and show the single-shot results for CUHK01, CUHK03 and SYSU.\nis applied. The batch size is 216. To guarantee that each batch\ncontains all views, we ﬁrst compute the distribution of the numbers\nof samples in each view in the training set (say we have two views\nand the distribution is [0.4, 0.6]), and then we randomly sample\n0.4*216=86 images in the ﬁrst view and 0.6*216=130 images in\nthe second view. We note that we can also use standard random\nsampling and this empirically does not affect the performance,\nas shown in the supplementary material.\nThe framework is\nimplemented based on the MatConvNet [91].\n5.3\nComparison to Related Unsupervised Models\nComparison to related unsupervised Re-ID models. We ﬁrst\ncompare DECAMEL with the unsupervised Re-ID models. For a\nmore fair and comprehensive comparison, we conduct experiments\non the seven datasets for DECAMEL and the code-available re-\nlated models. The compared models in the following comparisons\nadopt the same baseline JSTL feature which is used for initializa-\ntion in DECAMEL. We have tuned the hyper parameters for the\ncompared models to adapt to the JSTL feature, and thus report\neven better results than the original in literatures [48], [46], [47]\n(the performances are worse than the original without this tuning\nprocedure). We use the available code for the sparse dictionary\nlearning model (denoted as DIC) [48], the sparse iterative re-\nranking model ISR [46], the CCA-based kernel subspace learning\nmodel RKSL [47], and sparse auto-encoder (SAE) [83]. We also\ncompare our model with the baseline JSTL feature [32], which\nadopts the Euclidean distance as its metric. The comparative results\nare measured by the cumulative characteristic curve (CMC) and\nthe rank-1 matching rate of CMC. We show the matching rate in\nTable 2, and show the CMC in Figure 6.\nAs reported in Table 2, our model outperforms other models\non all the datasets in both settings. In addition, from Figure 6,\nour model outperforms other models by large margins at any\nrank. This is partly because DECAMEL explicitly deals with\nthe view-speciﬁc bias problem by learning an asymmetric metric.\nNote that the improvements are notably signiﬁcant on CUHK01\nand SYSU. We can see that the cross-view condition variations\nare particularly severe on these two datasets as shown in Figure\n5(b) and Figure 5(d) intuitively. For example, the changes of\nillumination are extremely severe in Figure 5(b) and Figure 5(d),\nand the differences between features from the two views may\nbe caused more by illumination than by identity under such a\nsituation. In particular, although the CCA-based model RKSL\nalso produces speciﬁc feature projections for different views, it\nCITATION INFORMATION: DOI 10.1109/TPAMI.2018.2886878, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n13\nTABLE 3\nComparison with the state-of-the-art unsupervised Re-ID models\nreported in literature. The performance is measured by rank-1 matching\nrate (%) in single-shot setting. “-” means no reported result. In each\nrow, the best is indicated in red and the second in blue\nModel\nSDALF\nUDML\nGL\nSDC\nGTS\nDECAMEL\n[42]\n[49]\n[50]\n[44]\n[45]\nVIPeR\n19.9\n31.5\n33.5\n26.7\n25.2\n34.2\nCUHK01\n9.9\n27.1\n41.0\n26.6\n-\n65.8\nCUHK03\n4.9\n-\n30.4\n7.7\n-\n38.3\nlearns the speciﬁc feature projections inconsistently, and thus does\nnot deal with the view-speciﬁc bias. Apart from this, DECAMEL\n(and CAMEL as reported in Table 5) learns a compact cross-view\ncluster structure for mining potential discriminative information\nwhile RKSL does not.\nComparison to published state-of-the-art results. Now we com-\npare our model with the reported results in published literatures,\nincluding the transfer learning model UDML [49], the hand-crafted\nfeature model SDALF [42], the graph learning model GL [50]\nand the saliency learning model GTS [45] and SDC [44]. We\nshow the comparative results in Table 3. Note that these models\nhave not been evaluated on SYSU, Market and ExMarket, so we\ncan only compare with their reported results on VIPeR, CUHK01\nand CUHK03 (all single-shot). As shown in Table 3, DECAMEL\noutperforms these models.\nComparison to clustering-based metric models. We also com-\npare with a typical clustering-based metric learning model AML\n[54], and a recently proposed one UsNCA [55]. As reported in\nTable 2 and Figure 6, DECAMEL can also achieve notable im-\nprovements over them. A main reason should be that DECAMEL\nlearns an asymmetric metric to address the view-speciﬁc bias\nproblem so as to learn a better cross-view cluster structure. In\ncontrast, the compared clustering-based models do not take into\nconsideration this issue which is particularly important for Re-ID.\n5.4\nFurther Analysis of DECAMEL\nIn the following, we provide some further experimental validations\nand analysis to make a more comprehensive understanding for the\nframework components and show some signiﬁcant properties.\n5.4.1\nAsymmetric vs. Symmetric Modelling\nWe ﬁrst evaluate the asymmetric modelling in our framework.\nTo this end, we develop a symmetric version of DECAMEL and\ndenote it as DECMEL. The only difference between them is that\nDECMEL learns (and embeds) a symmetric metric instead of an\nasymmetric one. We show the comparative results of performances\nin the upper part of Table 4. We can see that DECAMEL achieves\nmuch higher performances than DECMEL.\nTo further explore the differences between DECAMEL and\nDECMEL, we also develop a natural, reasonable measure to eval-\nuate the learned cross-view cluster structure in terms of mining the\ncross-view discriminative information. The measure is formulated\nas\nS = inter/intra\n= (\n1\nP(P −1)\nP\nX\np̸=q\n∥dp −dq∥2)/( 1\nT\nP\nX\np=1\nX\ni∈Dp\n∥xi −dp∥2), (32)\nwhere P denotes the number of persons, Dp is a set containing the\nindexes of all the cross-view images of the p-th person, dp denotes\nTABLE 4\nEvaluation of the asymmetric modelling in our framework. “DECMEL”\ndenotes the symmetric version of DECAMEL. The S value is deﬁned in\nEq. (32). The performance is measured by single-shot (“single”) and\nmulti-shot (“multi”) rank-1 matching rate and MAP in percentage. For\nclarity, we drop VIPeR and the multi-shot results of CUHK01, CUHK03\nand SYSU, which follow a similar pattern to single-shot results.\nDataset\nCUHK01 CUHK03\nSYSU\nMarket\nExMarket\nMSMT17\nMeasure\nsingle\nsingle\nsingle\nmulti(MAP)\nmulti(MAP)\nmulti(MAP)\nDECMEL\n55.95\n27.86\n25.38\n49.94(23.15) 53.00(26.53) 23.88(8.01)\nDECAMEL\n65.81\n38.27\n36.14\n60.24(32.44) 62.98(33.28) 30.34(11.13)\nMeasure\nS\nS\nS\nS\nS\nS\nDECMEL\n2.13\n1.44\n1.61\n1.79\n1.82\n1.33\nDECAMEL\n2.36\n1.87\n1.88\n1.93\n1.97\n1.42\nthe centroid of the p-th person, and T denotes the total number of\ncross-view person images. In Eq. (32), the numerator measures the\ninter-person discrepancy and the denominator measures the intra-\nperson discrepancy. We note that we only use the label information\nto form the measure to evaluate how discriminative the learned\ncross-view cluster structure is for Re-ID, i.e., the higher the S\nvalue is, the more easily we can distinguish different persons. We\nshow the comparative results in the lower part of Table 4. We can\nsee in Table 4 that DECAMEL has higher S values on all datasets,\nindicating that the asymmetric modelling in our framework helps\nlearn a better cross-view cluster structure to facilitate mining the\npotential cross-view discriminative information. This can be one\nof the main reasons why DECAMEL outperforms DECMEL. In\nthe following parameter evaluation we will further explore the\nbehavior of the proposed asymmetric modelling.\n5.4.2\nParameter Evaluation\nIn the comparisons we ﬁx the major hyper parameters, and\nhere we discuss the behaviors of the major hyper parameters to\nhave a better understanding of our framework. The cores of our\nframework are asymmetric modelling and cross-view clustering.\nThey are characterized mainly by λ (the cross-view consistency\nregularizer) and K (the number of clusters), respectively.\nEvaluation of λ: Characteristic of Asymmetric Modelling.\nThe cross-view consistency regularizer, λ, controls the degree\nof asymmetric modelling. When λ is larger, the larger penalty\nenforces the discrepancy between any pair of projection basis\nto be smaller, and thus the asymmetric modelling will become\nmore symmetric. We show the matching rate as a function of λ\nin Figures 7(a), 7(b) and 7(c). When λ is in a median range, the\nperformance is relatively stable. When λ is too large, the matching\nrate will drop. In fact, in the extreme case when λ goes to inﬁnity,\nit is equivalent to the symmetric version. This shows that the\nasymmetric modelling is very signiﬁcant in our framework.\nOn the other hand, when λ is too small, the matching rate also\ndrops. To understand the reason behind, we examine the extreme\ncase when λ = 0. This is equivalent to taking out the cross-view\nconsistency regularization in Eq. (7) and Eq. (10). In this extreme\ncase, the model actually fails to learn. To reveal the underlying\nreason, we show the cross-view data distribution in the learned\nshared space without the regularization in Figure 7(d). We ﬁnd\nthat the distribution collapses roughly to two lines. This is because\nthe intrinsic consistency across the distributions of different views\nis not preserved by the arbitrarily different transformations. Thus,\nwithout the cross-view consistency regularization, the learned\ntransformations become extreme to minimize the objective and\nproduces the collapsed cross-view distribution. Clearly, such a\nCITATION INFORMATION: DOI 10.1109/TPAMI.2018.2886878, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n14\n(a) CUHK03\n(b) SYSU\n(c) ExMarket\n(d) Collapsed distribution\nFig. 7. (a)-(c) Matching rate vs. λ on the three large-scale datasets. Similar observations can be made on other datasets. (d) The cross-view\ndistribution without cross-view consistency regularization. Data is from the SYSU dataset, and we performed PCA for visualization as in Figure 3.\nFig. 8. Matching rate as a function of K on CUHK01. K (blue parts)\nis linearly spaced from 100 to 1000. We show two extreme cases (red\nparts) when K = 1 and K = 1940, where 1940 is the number of total\ntraining samples. Similar observations can be made on other datasets.\ndistribution loses the discriminative information. This observation\nshows that the cross-view consistency regularization averts learn-\ning a shared space with collapsed cross-view distribution.\nEvaluation of K: Characteristic of Asymmetric Metric Clus-\ntering. K is the number of clusters in DECAMEL. We show the\nmatching rate as a function of K in Figure 8. In the middle blue\nparts in Figure 8, when K is set in a median range, e.g., 300 - 700,\nthe bars are tightly close to each other. This shows that to a mild\nextent, our framework is robust to K.\nTo further explore the reason behind, we show in Figure 9 the\nnumber of clusters which contains more than one person (i.e., > 1\npersons) when K varies. From Figure 9, it is found that (1) despite\nK is varying, there is always a number of clusters containing more\nthan one person in the initialization stage, i.e., the cluster results\nare in fact far from perfect. And (2), in the convergence stage the\nnumbers are consistently decreased compared to the initialization\nstage. This indicates that for K in a median range, the cluster\nresults are improved consistently. This can be a reason for the\nmild robustness.\nHowever, on the other hand, we can also ﬁnd that in Figure 8,\nthe two extreme cases (red parts) where K = 1 and K = 1940\nlead to performance drop. In the case when K = 1, the model fails\nto learn, somewhat similar to the situation shown in 7(d), because\nK = 1 leads to a collapsed distribution where all the data points\nare pulled towards a single centroid.\n5.4.3\nQuantitative Component-wise Evaluation\nNow we discuss how framework components contribute to DE-\nCAMEL. We ﬁrst set the learning rate to 0 for the metric layer to\nfreeze it. By this way, the framework only learns the feature rep-\nresentation. We denote this derived model as DECAMELf. Simi-\nlarly, we freeze the feature extractor, so we have DECAMELm. We\nFig. 9. Number of clusters containing more than one person at the initial\nstage (red solid line) and at the convergence stage (blue dashed line)\nwhen K varies on CUHK01. Similar observations can be made on other\ndatasets.\nTABLE 5\nComponent-wise evaluation. Featinit denotes the initialized feature.\nFeatembd denotes the feature learned by metric embedding.\nDECAMELf denotes the model when asymmetric metric layer is frozen\nwhile DECAMELm denotes when feature extractor is frozen.\nPerformance is measured by single-shot (“single”) and multi-shot\n(“multi”) rank-1 matching rate and MAP in percentage. For clarity, we\ndrop VIPeR and the multi-shot results of CUHK01, CUHK03 and SYSU,\nwhich follow a similar pattern to single-shot results.\nDataset\nCUHK01 CUHK03\nSYSU\nMarket\nExMarket\nMSMT17\nMeasure\nsingle\nsingle\nsingle\nmulti(MAP)\nmulti(MAP)\nmulti(MAP)\nFeatinit\n46.26\n24.66\n19.92\n44.69(18.36) 46.41(16.68) 21.24(6.05)\nFeatembd\n53.67\n33.20\n28.29\n51.90(25.56) 58.37(28.21) 25.31(8.49)\nCAMEL\n57.30\n31.89\n30.76\n54.45(26.31) 55.88(23.88) 27.06(9.14)\nDECAMELf 64.17\n36.71\n34.74\n55.97(29.30) 60.01(31.11) 27.20(9.78)\nDECAMELm 54.88\n30.15\n26.06\n51.19(23.65) 54.81(23.09) 26.74(8.86)\nDECAMEL\n65.81\n38.27\n36.14\n60.24(32.44) 62.98(33.28) 30.34(11.13)\nalso compare DECAMEL with the initialization stage (CAMEL).\nBesides, we compare the initialized feature representation (denoted\nas Featinit) with the feature learned by metric embedding, i.e., we\ntake the feature alone out of DECAMEL for comparison (denoted\nas Featembd). We show the results in Table 5.\nFrom Table 5 we can make three observations: (1) by com-\nparing Featinit with Featembd, we ﬁnd that Featembd consistently\noutperforms Featinit. This shows that the deep asymmetric metric\nembedding also learns some underlying cross-view discriminative\ninformation for the feature representation, as have been illustrated\nin Sec. 4.2. (2) By comparing DECAMELf and DECAMELm\nwith DECAMEL, we ﬁnd that any single component cannot\nachieve improvements as DECAMEL. This shows that the two\ncomponents of DECAMEL are intrinsically joint and cooperative,\nand their effects are mutually promoting each other, rather than\nsimply linearly superposed. (3) By comparing CAMEL with DE-\nCAMEL, we can ﬁnd that DECAMEL further provides noticeable\nCITATION INFORMATION: DOI 10.1109/TPAMI.2018.2886878, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n15\nTABLE 6\nEvaluation of different initialization strategies: single-shot (“single”) and\nmulti-shot (“multi”) rank-1 matching rate and MAP in percentage.\n“DECAMELi” and “DECAMELr” denote DECAMEL initialized by identity\nmatrix and random matrix [92], respectively, rather than CAMEL. For\nclarity, we drop VIPeR and the multi-shot results of CUHK01, CUHK03\nand SYSU, which follow a similar pattern to single-shot results.\nDataset\nCUHK01 CUHK03\nSYSU\nMarket\nExMarket\nMSMT17\nMeasure\nsingle\nsingle\nsingle\nmulti(MAP)\nmulti(MAP)\nmulti(MAP)\nDECAMELi\n48.33\n28.59\n18.71\n40.14(15.89) 41.06(14.40) 25.65(8.32)\nDECAMELr failed\nfailed\nfailed\nfailed\nfailed\nfailed\nDECAMEL\n65.81\n38.27\n36.14\n60.24(32.44) 62.97(33.28) 30.34(11.13)\nTABLE 7\nEvaluation when given label information to a small proportion of training\nsamples on Market. Similar observations can be made on other\ndatasets. “Acc.” is the rank-1 matching rate measured in %.\nProportion\n0%\n10%\n20%\n30%\nAcc. (MAP)\n60.24(32.44)\n64.55(38.26)\n67.01(40.33)\n69.98(43.20)\nTABLE 8\nEvaluation when the training samples size grows on the largest dataset\nExMarket. “#” means the number of. “Acc.” is the rank-1 matching rate\nmeasured in %.\n# Training samples\n1,000\n10,000\n100,000\n112,351(all)\nAcc. (MAP)\n52.52(22.33)\n60.04(30.17)\n62.86(33.21)\n62.98(33.28)\nimprovements over CAMEL. This suggests the potential of jointly\nlearning feature and metric in unsupervised Re-ID.\n5.4.4\nEffect of Metric Initialization\nAs Remark 2 reveals, the joint learning of DECAMEL is partially\nguided by the metric. We compare our proposed metric initializa-\ntion (CAMEL) to two standard initialization strategies, i.e., identity\nmatrix (in the view of distance metric) and randomly initialized\nmatrix using the Xavier initialization [92] (in the view of fully-\nconnected layer), and we denote their results as DECAMELi and\nDECAMELr, respectively. As shown in Table 6, DECAMEL out-\nperforms both of them. This is because in our learning algorithm,\nthe metric initialization method CAMEL can learn an asymmetric\ndistance metric which captures the cross-view person appearance\nvariations, providing a cross-view discriminative initialization suit-\nable for the unsupervised feature learning, and therefore achieve\nsuperior performance.\n5.4.5\nBeneﬁting from Extra Labelled Data\nWe evaluate a signiﬁcant property of our framework: the ability\nto beneﬁt from a little extra labelled data. This is a natural way\nto further boost the performance for an unsupervised application\nscenario. We give label information to a small proportion of\ntraining samples, i.e., 10%, 20% and 30%. These labelled samples\nare separated from the unlabelled samples and form extra clusters\naccording to their labels. We show in Table 7 the results on Market\nwhich is very representative and similar observations can be made\non other datasets. We can see that when given a little extra label\ninformation, the accuracy is improved as well as MAP.\n5.4.6\nBeneﬁting from More Unlabelled Data\nIn typical unsupervised Re-ID scenarios, e.g., public surveillance,\nthe available data increases with time. Therefore, it is signiﬁcant\nfor an unsupervised Re-ID model to beneﬁt from more unlabelled\nsamples. We evaluate this property by varying the training set size\non the largest dataset ExMarket, which uniquely provides over\n100, 000 samples. We show the results in Table 8. We can see\nthat when the training set size grows, the accuracy and MAP are\nimproved signiﬁcantly.\nTABLE 9\nRunning time on the Market-1501 dataset. All methods are\nimplemented in MATLAB R2017a on a Linux server. Note that ISR does\nnot need a training procedure. Training time of DECAMEL includes all\nsteps in Algorithm 1. Testing time is the average time of each probe\nimage searching over the gallery list.\nMethod\nDic [48]\nISR [46]\nDECAMEL\nTraining/Testing Time\n35.2h/0.02s\n-/98.0s\n5.6h/0.02s\nTABLE 10\nComparative results in the view-extendable setting. The performances\nof Rank-1 accuracy (MAP) are only of all the unseen views (see the\ntext in Sec. 5.5).\nMethod\nAML\nDic\nDECAMELV C\nMarket-1501\n41.57(15.01)\n43.82(18.19)\n56.50(29.99)\nMSMT17\n19.77(5.87)\n21.04(6.00)\n26.42(8.75)\nFig. 10. Performances of DECAMELV C in the view-extendable setting\nin the MSMT17 dataset. We ﬁx J = 10.\n5.4.7\nRunning Time\nWe report the running time of our model on the Market-1501\ndataset in Table 9, compared to the most competitive models\nDic [48] and ISR [46] which together take all the second places\nin Table 2. A GTX Titan X GPU is used for deep learning in\nDECAMEL. We can see that the testing procedure (i.e. usage after\ndeployment) of DECAMEL is efﬁcient.\n5.5\nEvaluation on the View Clustering\nIn the following we evaluate the View Clustering (VC) proposed\nin Sec. 3.4 for view-extendable (i.e. requiring to add new views\nduring testing stage) and large-scale applications.\nThe View-extendable Re-ID setting. To evaluate the performance\nin the view-extendable scenario instead of the conventional Re-\nID setting, we divide the 15 views of the MSMT17 dataset to\ntwo sets: a set of 2/3 of all the views, i.e. 10 views as training\nviews, and a set of the other 1/3, i.e. 5 views as testing views. In\nthe training stage, we only use the training images that are from\nthe training views, but discard all the training images from the\ntesting views. More speciﬁcally, we randomly divided all views\ninto 3 subsets each of which contains 1/3 of all the views, and we\nalternatively select one subset as testing views and the other two\nas training views, repeat the above procedure for three different\nsubset divisions (i.e. in total we train and test for 3*3=9 times) and\nreport the overall averaged results. In the testing stage, we only\nuse the probe images from the testing views for computing the\nquantitative results (i.e. Rank-1 accuracy and MAP). In this view-\nextendable setting, for comparability to the conventional setting,\nwe remain the gallery set to contain images from all views, so\nthat the only difference between these two settings is whether\nthe models have seen training samples from the testing views in\nthe training stage. We also report the results on the Market-1501\ndataset on which 2 views are for testing views.\nCITATION INFORMATION: DOI 10.1109/TPAMI.2018.2886878, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n16\nComparative results. Following the above view-extendable set-\nting, we report the comparative results with the most competitive\nmethod (i.e. Dic) as well as a clustering-based model AML on\nthe Market-1501 and MSMT17 dataset in Table 10. In our method\nDECAMEL with View Clustering (denoted as DECAMELV C and\nintroduced in Sec. 3.4) we set the number of view clusters to 10\non MSMT17 and 4 on Market-1501.\nFrom Table 10, two observations can be made: 1) Our method\nalso outperforms the compared methods with a clear margin in\nthis view-extendable setting. 2) In the view-extendable setting, the\nperformances are lower than those in the conventional setting, but\nthis is reasonable since the training samples from the testing views\nare not available.\nGeneralizability of the View Clustering. We further investigate\nthe generalizability of VC when more training views are available.\nTo this end, we now take n views (n = 5, 4, 3, 2, 1) on the\nMSMT17 dataset as testing views, so that we have (15−n) training\nviews. In the testing stage, similarly to the above setting, we report\nthe averaged results of the unseen views. We ﬁx the number of\nview prototypes J = 10 and show the results in Figure 10.\nFrom the above results, we can make two observations: 1)\nWhen there are more training views, the performance increases.\nThis is because when there are more available training views, the\nview prototypes can cover a wider range of typical view-speciﬁc\nconditions, and thus more generalizable. 2) the performances of\nDECAMELV C are close to DECAMEL, showing that although the\ntesting views are not seen in the training stage, the learned projec-\ntions of view prototypes can generalize to the unseen views. Note\nthat when the number of training views is 14 DECAMEL with\nView Clustering (DECAMELV C) still has a gap of 1.5% com-\npared to DECAMEL. While this observation also indicates that\neach view has its speciﬁc condition, DECAMELV C thus strikes\na balance where the ability to precisely model the view-speciﬁc\ncondition is compromised for better generalizability. Moreover,\nwe could infer from the above two observations that in real-world\nlarge-scale problems where there would be much more available\ncamera views, the curve could be further extrapolated and we can\nexpect that the performance of DECAMELV C could further ap-\nproximate DECAMEL, i.e. the generalizability of DECAMELV C\nin large-scale applications shall be further improved.\n6\nCONCLUSION AND DISCUSSION\nIn this work, we present a novel approach for unsupervised Re-ID\nby formulating it as an unsupervised asymmetric metric learning\nproblem. We propose a novel unsupervised loss function to pro-\nduce a deep framework DECAMEL, which learns the asymmetric\nmetric and embeds it into a deep feature learning network by\nend-to-end learning. The experiments show that our model can\noutperform the related unsupervised Re-ID models.\nThe analysis and experimental results suggest the effectiveness\nof the asymmetric modelling in unsupervised Re-ID. We note that\nthe asymmetric modelling could be extensively embedded into\nother modelling strategies in unsupervised Re-ID, e.g., designing\nview-speciﬁc features, learning unsupervised asymmetric metrics\nand learning view-speciﬁc dictionaries. Our work also suggests\nthe potential of unsupervised metric learning in Re-ID, especially\nthat based on cross-view clustering. A future direction could be\nexploring the behaviour of asymmetric modelling in front of a\nview-imbalance problem, where the number of samples in each\ncamera view is largely imbalanced, which could be necessary.\nDeriving theoretical guarantees on the robustness against the view-\nimbalance problem could further perfect the asymmetric modelling\ntheoretically.\nACKNOWLEDGMENT\nThis work was supported partially by the National Key Re-\nsearch and Development Program of China (2016YFB1001002),\nNSFC(61522115, 61661130157, 61472456, U1611461), Guang-\ndong Province Science and Technology Innovation Leading Tal-\nents (2016TX03X157), and the Royal Society Newton Advanced\nFellowship (NA150459).\nThis paper has supplementary downloadable material available\nat http://ieeexplore.ieee.org, provided by the author. The material\nincludes a document of more experiments and analysis. Contact\nxKoven@gmail.com for further questions about this work.\nREFERENCES\n[1]\nA. Bedagkar-Gala and S. K. Shah, “A survey of approaches and trends in\nperson re-identiﬁcation,” IVC, 2014.\n[2]\nR. Vezzani, D. Baltieri, and R. Cucchiara, “People reidentiﬁcation in\nsurveillance and forensics: A survey,” CSUR, 2013.\n[3]\nY.-C. Chen, X. Zhu, W.-S. Zheng, and J.-H. Lai, “Person re-identiﬁcation\nby camera correlation aware feature augmentation,” TPAMI, 2017.\n[4]\nC. Liu, S. Gong, C. Loy, and X. Lin, “Person re-identiﬁcation: What\nfeatures are important?” in ECCV, 2012.\n[5]\nC. Su, F. Yang, S. Zhang, Q. Tian, L. S. Davis, and W. Gao, “Multi-task\nlearning with low rank attribute embedding for person re-identiﬁcation,”\nin ICCV, 2015.\n[6]\nZ. Shi, T. M. Hospedales, and T. Xiang, “Transferring a semantic\nrepresentation for person re-identiﬁcation and search,” in ICCV, 2015.\n[7]\nI. Kviatkovsky, A. Adam, and E. Rivlin, “Color invariants for person\nreidentiﬁcation,” TPAMI, 2013.\n[8]\nR. Zhao, W. Oyang, and X. Wang, “Person re-identiﬁcation by saliency\nlearning,” TPAMI, 2017.\n[9]\nD. Chen, Z. Yuan, G. Hua, N. Zheng, and J. Wang, “Similarity learning\non an explicit polynomial kernel feature map for person re-identiﬁcation,”\nin CVPR, 2015.\n[10] T. Matsukawa, T. Okabe, E. Suzuki, and Y. Sato, “Hierarchical gaussian\ndescriptor for person re-identiﬁcation,” in CVPR, 2016.\n[11] Y. Shen, W. Lin, J. Yan, M. Xu, J. Wu, and J. Wang, “Person re-\nidentiﬁcation with correspondence structure learning,” in ICCV, 2015.\n[12] A. Das, A. Chakraborty, and A. K. Roy-Chowdhury, “Consistent re-\nidentiﬁcation in a camera network,” in ECCV, 2014.\n[13] B. Ma, Y. Su, and F. Jurie, “Bicov: a novel image representation for\nperson re-identiﬁcation and face veriﬁcation,” in BMVC, 2012.\n[14] A. Mignon and F. Jurie, “Pcca: A new approach for distance learning\nfrom sparse pairwise constraints,” in CVPR, 2012.\n[15] W.-S. Zheng, S. Gong, and T. Xiang, “Reidentiﬁcation by relative\ndistance comparison,” TPAMI, 2013.\n[16] Z. Li, S. Chang, F. Liang, T. S. Huang, L. Cao, and J. R. Smith, “Learning\nlocally-adaptive decision functions for person veriﬁcation,” in CVPR,\n2013.\n[17] S. Pedagadi, J. Orwell, S. Velastin, and B. Boghossian, “Local ﬁsher\ndiscriminant analysis for pedestrian re-identiﬁcation,” in CVPR, 2013.\n[18] F. Xiong, M. Gou, O. Camps, and M. Sznaier, “Person re-identiﬁcation\nusing kernel-based metric learning methods,” in ECCV, 2014.\n[19] S. Liao, Y. Hu, X. Zhu, and S. Z. Li, “Person re-identiﬁcation by local\nmaximal occurrence representation and metric learning,” in CVPR, 2015.\n[20] M. Koestinger, M. Hirzer, P. Wohlhart, P. M. Roth, and H. Bischof, “Large\nscale metric learning from equivalence constraints,” in CVPR, 2012.\n[21] S. Paisitkriangkrai, C. Shen, and A. van den Hengel, “Learning to rank in\nperson re-identiﬁcation with metric ensembles,” in CVPR, 2015.\n[22] L. Zhang, T. Xiang, and S. Gong, “Learning a discriminative null space\nfor person re-identiﬁcation,” in CVPR, 2016.\n[23] D. Chen, Z. Yuan, B. Chen, and N. Zheng, “Similarity learning with\nspatial constraints for person re-identiﬁcation,” in CVPR, 2016.\n[24] T. Wang, S. Gong, X. Zhu, and S. Wang, “Person re-identiﬁcation by\ndiscriminative selection in video ranking,” TPAMI, 2016.\n[25] H. Wang, S. Gong, X. Zhu, and T. Xiang, “Human-in-the-loop person\nre-identiﬁcation,” in ECCV, 2016.\nCITATION INFORMATION: DOI 10.1109/TPAMI.2018.2886878, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n17\n[26] L. An, M. Kafai, S. Yang, and B. Bhanu, “Person reidentiﬁcation with\nreference descriptor,” TCSVT, 2016.\n[27] M. Hirzer, P. Roth, M. K¨ostinger, and H. Bischof, “Relaxed pairwise\nlearned metric for person re-identiﬁcation,” ECCV, 2012.\n[28] Y.-C. Chen, W.-S. Zheng, J.-H. Lai, and P. Yuen, “An asymmetric dis-\ntance model for cross-view feature mapping in person re-identiﬁcation,”\nTCSVT, 2016.\n[29] W. Li, R. Zhao, T. Xiao, and X. Wang, “Deepreid: Deep ﬁlter pairing\nneural network for person re-identiﬁcation,” in CVPR, 2014.\n[30] L. Wu, C. Shen, and A. van den Hengel, “Deep linear discriminant analy-\nsis on ﬁsher networks: A hybrid architecture for person re-identiﬁcation,”\nPR, 2017.\n[31] E. Ahmed, M. Jones, and T. K. Marks, “An improved deep learning\narchitecture for person re-identiﬁcation,” in CVPR, 2015.\n[32] T. Xiao, H. Li, W. Ouyang, and X. Wang, “Learning deep feature\nrepresentations with domain guided dropout for person re-identiﬁcation,”\nin CVPR, 2016.\n[33] D. Yi, Z. Lei, S. Liao, and S. Z. Li, “Deep metric learning for person\nre-identiﬁcation,” in ICPR, 2014.\n[34] S. Wu, Y.-C. Chen, X. Li, A.-C. Wu, J.-J. You, and W.-S. Zheng, “An\nenhanced deep feature representation for person re-identiﬁcation,” in\nWACV, 2016.\n[35] S.-Z. Chen, C.-C. Guo, and J.-H. Lai, “Deep ranking for person re-\nidentiﬁcation via joint representation learning,” TIP, 2016.\n[36] F. Wang, W. Zuo, L. Lin, D. Zhang, and L. Zhang, “Joint learn-\ning of single-image and cross-image representations for person re-\nidentiﬁcation,” in CVPR, 2016.\n[37] D. Cheng, Y. Gong, S. Zhou, J. Wang, and N. Zheng, “Person re-\nidentiﬁcation by multi-channel parts-based cnn with improved triplet loss\nfunction,” in CVPR, 2016.\n[38] R. R. Varior, M. Haloi, and G. Wang, “Gated siamese convolutional neural\nnetwork architecture for human re-identiﬁcation,” in ECCV, 2016.\n[39] H. Liu, J. Feng, M. Qi, J. Jiang, and S. Yan, “End-to-end comparative\nattention networks for person re-identiﬁcation,” TIP, 2017.\n[40] C. Su, S. Zhang, J. Xing, W. Gao, and Q. Tian, “Deep attributes driven\nmulti-camera person re-identiﬁcation,” in ECCV, 2016.\n[41] X. Wang, W. S. Zheng, X. Li, and J. Zhang, “Cross-scenario transfer\nperson reidentiﬁcation,” TCSVT, 2015.\n[42] M. Farenzena, L. Bazzani, A. Perina, V. Murino, and M. Cristani, “Person\nre-identiﬁcation by symmetry-driven accumulation of local features,” in\nCVPR, 2010.\n[43] D. S. Cheng, M. Cristani, M. Stoppa, L. Bazzani, and V. Murino, “Custom\npictorial structures for re-identiﬁcation.” in BMVC, 2011.\n[44] R. Zhao, W. Ouyang, and X. Wang, “Unsupervised salience learning for\nperson re-identiﬁcation,” in CVPR, 2013.\n[45] H. Wang, S. Gong, and T. Xiang, “Unsupervised learning of generative\ntopic saliency for person re-identiﬁcation,” in BMVC, 2014.\n[46] G. Lisanti, I. Masi, A. D. Bagdanov, and A. Del Bimbo, “Person re-\nidentiﬁcation by iterative re-weighted sparse ranking,” TPAMI, 2015.\n[47] H. Wang, X. Zhu, T. Xiang, and S. Gong, “Towards unsupervised open-\nset person re-identiﬁcation,” in ICIP, 2016.\n[48] E. Kodirov, T. Xiang, and S. Gong, “Dictionary learning with iterative\nlaplacian regularisation for unsupervised person re-identiﬁcation.” in\nBMVC, 2015.\n[49] P. Peng, T. Xiang, Y. Wang, M. Pontil, S. Gong, T. Huang, and\nY. Tian, “Unsupervised cross-dataset transfer learning for person re-\nidentiﬁcation,” in CVPR, 2016.\n[50] E. Kodirov, T. Xiang, Z. Fu, and S. Gong, “Person re-identiﬁcation by\nunsupervised l1 graph learning,” in ECCV, 2016.\n[51] B. Kulis et al., “Metric learning: A survey,” Foundations and Trends R⃝in\nMachine Learning, 2013.\n[52] H.-X. Yu, A. Wu, and W.-S. Zheng, “Cross-view asymmetric metric\nlearning for unsupervised person re-identiﬁcation,” in ICCV, 2017.\n[53] H. Fan, L. Zheng, and Y. Yang, “Unsupervised person re-identiﬁcation:\nClustering and ﬁne-tuning,” arXiv preprint arXiv:1705.10444, 2017.\n[54] J. Ye, Z. Zhao, and H. Liu, “Adaptive distance metric learning for\nclustering,” in CVPR, 2007.\n[55] C. Qin, S. Song, G. Huang, and L. Zhu, “Unsupervised neighborhood\ncomponent analysis for clustering,” Neurocomputing, 2015.\n[56] R. G. Cinbis, J. Verbeek, and C. Schmid, “Unsupervised metric learning\nfor face identiﬁcation in tv video,” in ICCV, 2011.\n[57] J. Jiang, B. Wang, and Z. Tu, “Unsupervised metric learning by self-\nsmoothing operator,” in ICCV, 2011.\n[58] C. Kang, S. Xiang, S. Liao, C. Xu, and C. Pan, “Learning consistent\nfeature representation for cross-modal multimedia retrieval,” ToM, 2015.\n[59] Y. Gong, Q. Ke, M. Isard, and S. Lazebnik, “A multi-view embedding\nspace for modeling internet images, tags, and their semantics,” IJCV,\n2014.\n[60] N. Rasiwasia, J. Costa Pereira, E. Coviello, G. Doyle, G. R. Lanckriet,\nR. Levy, and N. Vasconcelos, “A new approach to cross-modal multime-\ndia retrieval,” in ACMMM, 2010.\n[61] D. R. Hardoon, S. Szedmak, and J. Shawe-Taylor, “Canonical correlation\nanalysis: An overview with application to learning methods,” Neural\ncomputation, 2004.\n[62] R. Rosipal and N. Kr¨amer, “Overview and recent advances in partial least\nsquares,” in SLSFS, 2005.\n[63] D. Yi, Z. Lei, S. Liao, and S. Z. Li, “Deep metric learning for person\nre-identiﬁcation,” in ICPR, 2014.\n[64] J. Hu, J. Lu, and Y.-P. Tan, “Discriminative deep metric learning for face\nveriﬁcation in the wild,” in CVPR, 2014.\n[65] E. Hoffer and N. Ailon, “Deep metric learning using triplet network,” in\nInternational Workshop on Similarity-Based Pattern Recognition, 2015.\n[66] H. Oh Song, Y. Xiang, S. Jegelka, and S. Savarese, “Deep metric learning\nvia lifted structured feature embedding,” in CVPR, 2016.\n[67] J. Lu, G. Wang, W. Deng, P. Moulin, and J. Zhou, “Multi-manifold deep\nmetric learning for image set classiﬁcation,” in CVPR, 2015.\n[68] J. Yu, X. Yang, F. Gao, and D. Tao, “Deep multimodal distance metric\nlearning using click constraints for image ranking,” IEEE Trans on\nCybernetics, 2016.\n[69] J. Xie, R. Girshick, and A. Farhadi, “Unsupervised deep embedding for\nclustering analysis,” in ICML, 2016.\n[70] J. Yang, D. Parikh, and D. Batra, “Joint unsupervised learning of deep\nrepresentations and image clusters,” in CVPR, 2016.\n[71] Z. Wang, S. Chang, J. Zhou, M. Wang, and T. S. Huang, “Learning a\ntask-speciﬁc deep architecture for clustering,” in SDM, 2016.\n[72] J. A. Hartigan, Clustering algorithms.\nJohn Wiley & Sons Inc, 1975.\n[73] V. N. Vapnik and V. Vapnik, Statistical learning theory.\nWiley New\nYork, 1998.\n[74] C. H. Q. Ding and X. He, “On the equivalence of nonnegative matrix\nfactorization and spectral clustering.” in ICDM, 2005.\n[75] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein generative adver-\nsarial networks,” in ICML, 2017.\n[76] D. Berthelot, T. Schumm, and L. Metz, “Began: boundary equilibrium\ngenerative adversarial networks,” arXiv preprint arXiv:1703.10717, 2017.\n[77] R. He, X. Wu, Z. Sun, and T. Tan, “Wasserstein cnn: Learning invariant\nfeatures for nir-vis face recognition,” TPAMI, 2018.\n[78] D. Gray, S. Brennan, and H. Tao, “Evaluating appearance models for\nrecognition, reacquisition, and tracking,” in PETS, 2007.\n[79] W. Li, R. Zhao, and X. Wang, “Human reidentiﬁcation with transferred\nmetric learning,” in ACCV, 2012.\n[80] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian, “Scalable\nperson re-identiﬁcation: A benchmark,” in ICCV, 2015.\n[81] L. Zheng, Z. Bie, Y. Sun, J. Wang, C. Su, S. Wang, and Q. Tian, “Mars:\nA video benchmark for large-scale person re-identiﬁcation,” in ECCV,\n2016.\n[82] L. Wei, S. Zhang, W. Gao, and Q. Tian, “Person transfer gan to bridge\ndomain gap for person re-identiﬁcation,” CVPR, 2018.\n[83] H. Lee, C. Ekanadham, and A. Y. Ng, “Sparse deep belief net model for\nvisual area v2,” in NIPS, 2008.\n[84] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in CVPR, 2016.\n[85] M. Hirzer, C. Beleznai, P. M. Roth, and H. Bischof, “Person re-\nidentiﬁcation by descriptive and discriminative classiﬁcation,” in Scan-\ndinavian Conference on Image Analysis, 2011.\n[86] D. Baltieri, R. Vezzani, and R. Cucchiara, “3dpes: 3d people dataset\nfor surveillance and forensics,” in Proceedings of the 2011 joint ACM\nworkshop on Human gesture and behavior understanding, 2011.\n[87] W. S. Zheng, S. Gong, and T. Xiang, “Associating groups of people,” in\nBMVC, 2009.\n[88] Y. Kawanishi, Y. Wu, M. Mukunoki, and M. Minoh, “Shinpuhkan2014:\nA multi-camera pedestrian dataset for tracking people across multiple\ncameras,” in 20th Korea-Japan Joint Workshop on Frontiers of Computer\nVision, 2014.\n[89] Z. Zheng, L. Zheng, and Y. Yang, “Unlabeled samples generated by gan\nimprove the person re-identiﬁcation baseline in vitro,” in Proceedings of\nthe IEEE International Conference on Computer Vision, 2017.\n[90] E. Ristani, F. Solera, R. Zou, R. Cucchiara, and C. Tomasi, “Performance\nmeasures and a data set for multi-target, multi-camera tracking,” in\nEuropean Conference on Computer Vision workshop on Benchmarking\nMulti-Target Tracking, 2016.\nCITATION INFORMATION: DOI 10.1109/TPAMI.2018.2886878, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n18\n[91] A. Vedaldi and K. Lenc, “Matconvnet: Convolutional neural networks for\nmatlab,” in ACMMM, 2015.\n[92] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\nS. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture for\nfast feature embedding,” in ACMMM, 2014.\nHong-Xing Yu received the bachelor’s degree\nin communication engineering from Sun Yat-Sen\nUniversity in 2017. He is now a M.S. student in\nthe School of Data and Computer Science in Sun\nYat-Sen University. His research interest lies in\ncomputer vision and machine learning.\nHomepage: http://isee.sysu.edu.cn/∼yuhx/.\nAncong Wu received the bachelor’s degree in\nintelligence science and technology from Sun\nYat-Sen University in 2015. He is pursuing PhD\ndegree with the School of Electronics and In-\nformation Technology in Sun Yat-sen University.\nHis research interests are computer vision and\nmachine learning. He is currently focusing on the\ntopic of person re-identiﬁcation.\nHomepage:http://isee.sysu.edu.cn/∼wuancong/.\nWei-Shi Zheng is now a professor at Sun Yat-\nsen University. His research interests include\nperson association and activity understanding in\nvisual surveillance. He has now published more\nthan 100 papers, including more than 70 publica-\ntions in main journals (TPAMI,TNN,TIP,PR) and\ntop conferences (ICCV, CVPR,IJCAI,AAAI). He\nserved as an area chair for AVSS 2012, ICPR\n2018 and BMVC 2018, and a Senior PC for\nIJCAI 2019. He has joined Microsoft Research\nAsia Young Faculty Visiting Programme. He is\na recipient of Excellent Young Scientists Fund of the National Natural\nScience Foundation of China, and a recipient of Royal Society-Newton\nAdvanced Fellowship, United Kingdom. He is an associate editor of the\nPattern Recognition Journal.\nHomepage: http://isee.sysu.edu.cn/∼zhwshi/.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2019-01-29",
  "updated": "2019-01-29"
}