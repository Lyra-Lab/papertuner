{
  "id": "http://arxiv.org/abs/1902.00566v1",
  "title": "Visual Rationalizations in Deep Reinforcement Learning for Atari Games",
  "authors": [
    "Laurens Weitkamp",
    "Elise van der Pol",
    "Zeynep Akata"
  ],
  "abstract": "Due to the capability of deep learning to perform well in high dimensional\nproblems, deep reinforcement learning agents perform well in challenging tasks\nsuch as Atari 2600 games. However, clearly explaining why a certain action is\ntaken by the agent can be as important as the decision itself. Deep\nreinforcement learning models, as other deep learning models, tend to be opaque\nin their decision-making process. In this work, we propose to make deep\nreinforcement learning more transparent by visualizing the evidence on which\nthe agent bases its decision. In this work, we emphasize the importance of\nproducing a justification for an observed action, which could be applied to a\nblack-box decision agent.",
  "text": "Visual Rationalizations in Deep Reinforcement\nLearning for Atari Games\nLaurens Weitkamp1, Elise van der Pol2, and Zeynep Akata2\n1 Informatics Institute, University of Amsterdam, the Netherlands\n2 UvA-Bosch Delta Lab, University of Amsterdam, the Netherlands\nAbstract. Due to the capability of deep learning to perform well in high\ndimensional problems, deep reinforcement learning agents perform well in\nchallenging tasks such as Atari 2600 games. However, clearly explaining\nwhy a certain action is taken by the agent can be as important as the\ndecision itself. Deep reinforcement learning models, as other deep learning\nmodels, tend to be opaque in their decision-making process. In this work,\nwe propose to make deep reinforcement learning more transparent by\nvisualizing the evidence on which the agent bases its decision. In this\nwork, we emphasize the importance of producing a justiﬁcation for an\nobserved action, which could be applied to a black-box decision agent.\nKeywords: Explainable AI · Reinforcement Learning · Deep Learning.\n1\nIntroduction\nDue to strong results on challenging benchmarks over the last few years, en-\nabled by the use of deep neural networks as function approximators [6,11,17]\ndeep reinforcement learning has become an increasingly active ﬁeld of research.\nWhile neural networks allow reinforcement learning methods to scale to complex\nproblems with large state spaces, their decision-making is opaque and they can\nfail in non-obvious ways, for example, if the network fails to generalize well and\nchooses an action based on the wrong feature. Moreover, recent work [7] has\nshown that these methods can lack robustness, with large diﬀerences in perfor-\nmance when varying hyperparameters, deep learning libraries or even the random\nseed. Gaining insight into the decision-making process of reinforcement learning\n(RL) agents can provide new intuitions about why and how they fail. Moreover,\nagents that can justify with visual elements why a prediction is consistent to a\nuser are more likely to be trusted [18]. Generating such post-hoc explanations,\nalso referred to as rationalizations, does not only increase trust, but also it is a\nkey component for understanding and interacting with them [3]. Motivated by\nexplainability as a means to make the black-box neural networks transparent,\nwe propose to visualize the decision process of a reinforcement learning agent by\nusing Grad-CAM [15].\nGrad-CAM creates an activation map that shows prominent spaces of activa-\ntion given an input image and class, typically in an image classiﬁcation task. The\nactivation map is calculated through a combination of the convolutional neural\narXiv:1902.00566v1  [cs.LG]  1 Feb 2019\n2\nWeitkamp et al.\nnetwork weights and the gradient activations created during a forward pass of\nthe input image and class in the neural network.\nApplying this method instead to a reinforcement learning agent, it wil be\nused to construct action-speciﬁc activation maps that highlight the regions of\nthe image that are the most important evidences, for the predicted action of\nthe RL agent. We evaluate these visualizations on three Atari 2600 games using\nthe OpenAI Gym wrapper, created precisely to tackle diﬃcult problems in deep\nreinforcement learning. The range of games in the wrapper are diverse in diﬃculty:\nthey have diﬀerent long-term reward mechanics and a diﬀerent action space per\ngame. These diﬃculties are of interest when looking to explain why the agent\ntakes a speciﬁc action given a state.\nThis paper is structured as follows: The next section, 2, discusses related\nworks in both reinforcement learning and explainable AI. Section 3 presents the\nvisual rationalization model and explains how it is adapted to reinforcement\nlearning tasks. Following after that is section 4 which provides the setup required\nfor experiments. This section also provides the results for the rationalization\nmodel, including where the model fails. The last section, section 5, provides an\nconclusion to the experiments.\n2\nRelated Work\nIn this section, we discuss previous works relevant to reinforcement learning and\nexplainable artiﬁcial intelligence.\n2.1\nDeep Reinforcement Learning\nIn general, there are two main methods in deep reinforcement learning. The ﬁrst\nmethod uses a neural network to approximate the value function that estimates\nthe value of state, action pairs as to infer a policy. One such value function\nestimation model is called the Deep-Q Network (DQN), which has had garnered\nmuch attention to the ﬁeld of deep reinforcement learning due to impressive\nresults on challenging benchmarks such as Atari 2600 games [11]. Since the release\nof this model, a range of modiﬁcations have been proposed that have improved\nthis model such as the Deep Recurrent-Q model, the Double DQN model and\nthe Rainbow DQN model [5,9,19].\nThe second method used in deep reinforcement learning approximates the\npolicy directly, by parameterizing the policy and using the gradient of these pa-\nrameters to calculate an optimal policy. This method is called the policy gradient\nmethod, and a much cited example of such a method is known as the REIN-\nFORCE line of algorithms [20]. More recent examples of policy gradient methods\ninclude Trust Region Policy Optimization and Proximal Policy Optimization\n[13,14].\nA hybrid that combines value function methods and policy gradient methods\nis known as the actor-critic method. In this method, the actor is trying to infer a\npolicy using a state, action pair and the critic is assigning a value to the current\nVisual Rationalizations in Deep Reinforcement Learning for Atari Games\n3\nstate of the actor. In this paper we use the Asynchronous Advantage Actor-Critic\n(A3C) model which has been used to achieve human level performance on a wide\nrange of Atari 2600 games [10].\n2.2\nExplainable AI\nGenerating visual or textual explanations of deep network predictions is a re-\nsearch direction that has recently gained much interest [1,8,12,23]. Following the\nconvention described by Park et al. in [12], we focus on post-hoc explanations,\nnamely rationalizations where a deep network is trained to explain a black box\ndecision maker which is useful in increasing trust for the end user.\nTextual rationalizations are explored in Hendricks et al. [8] which proposes\na loss function based on sampling and reinforcement learning that learns to\ngenerate sentences that realize a global sentence property, such as class speciﬁcity.\nAndreas et al. [1] composes collections of jointly-trained neural modules into deep\nnetworks for question answering by decomposing questions into their linguistic\nsubstructures, and using these structures to dynamically instantiate modular\nnetworks with reusable components.\nAs for visual rationalizations, Zintgraf et al. [23] propose to apply prediction\ndiﬀerence analysis to a speciﬁc input. [12] utilizes a visual attention module\nthat justiﬁes the predictions of deep networks for visual question answering and\nactivity recognition. In [4] the authors propose to use a perturbation method\nthat selectively blurs regions to calculate the impact on an RL agent’s policy.\nAlthough this method demonstrates important regions for the agent’s decision\nmaking, the method used in this paper highlights important regions without the\nneed for such a perturbation method.\nGrad-CAM [15] uses the gradients of any target concept, i.e. predicted action,\nﬂowing into the ﬁnal convolutional layer to produce a coarse localization map\nhighlighting the important regions in the image for predicting the concept. It\nhas been demonstrated on image classiﬁcation and captioning. In this work, we\nadapt it to two reinforcement learning tasks to visually rationalize the predicted\naction.\n3\nVisual Rationalization Model\nIn reinforcement learning, an agent interacting with an environment over a series\nof discrete time steps observes a state3 st ∈S, takes an action at ∈A and\nreceives a reward rt and observes the next state st+1 ∈S. The agent is tasked\nwith ﬁnding a policy π : S × A →[0, 1], a function mapping states and actions\nto probabilities whose goal is to maximize the discounted sum of rewards:\nRt =\n∞\nX\nk=0\nγkrt+k+1\n(1)\n3 Here we assume problems where partial observability can be addressed by representing\na state as a small number of past observations\n4\nWeitkamp et al.\nwhich is the return with discount factor γ ∈[0, 1].\n3.1\nAsynchronous Advantage Actor Critic Learning\nGradient based actor-critic methods split the agent in two components: an actor\nthat interacts with the environment using a policy π(a|s; θ), and a critic that\nassigns values to these actions using the value function V (s; θ). Both the policy\nand the value function are directly parameterized by θ. Updating the policy and\nvalue function is done through gradient descent\nθt+1 = θt + ∇θt log π(at|st; θt)At.\n(2)\nWith At = Rt−V (st; θt), an estimation of the advantage function [21]. In [10],\nthe policy gradient actor-critic uses a series of asynchronous actors that all send\npolicy-gradient updates to a single model that keeps track of the parameters θ.\nIn our implementation the actor output is a softmax vector of size |A|, the total\nnumber of actions the agent can take in the speciﬁc environment. Because our\nvisual rationalization model uses the actor output only, the scalar critic output\nwill be ignored for the purposed of this paper. However, in future work, exploring\nthe critic’s explanations could be of interest. To ensure exploration early on an\nentropy regularization term H is introduced with respect to the policy gradient,\nθt+1 = θt + ∇θt log π(at|st; θt)At + β∇θtH(π(st; θt)),\n(3)\nwhere β is a hyper parameter discounting the entropy regularization.\n3.2\nVisual Rationalization\nOur visual rationalization is based on Grad-CAM [15], and constitutes of comput-\ning a class-discriminative localization map Ls\nGradCAM ∈Ru×v using the gradient\nof any target class. These gradients are global-average-pooled to obtain the\nneuron importance weights ac\nk for class c, for activation layer k in the CNN4:\nαc\nk = 1\nZ\nX\ni\nX\nj\n∂yc\n∂Lk\nij\n.\n(4)\nAdapting this method in particular to the A3C actor output, let ha be the\nscore for action a before the softmax, αa\nk now represents the importance weight\nfor state a in activation layer k:\nαa\nk = 1\nZ\nX\ni\nX\nj\n∂ha\n∂Lk\nij\n,\n(5)\n4 k is usually chosen to be the last convolutional layer in the CNN.\nVisual Rationalizations in Deep Reinforcement Learning for Atari Games\n5\nFig. 1. The model takes as input a state, calculates the state-action π(a|s; θ) policy and\nthen produces a gradient-based activation map based on the state, action pair. This\nactivation map can then be overlayed on the original state to indicate evidence that\nthe agent has to take the action. In this Figure, the agent chooses to take the action\nLEFTFIRE which would make the agent go one step to the left and then shoot up. The\nactivation map is highlighting the agent (bottom), incoming debris (upper-right) and\nan incoming enemy (upper-mid).\nwith |h| = |A|, the total amount of actions the agent can take. The gradient\nthen gets weighted by the forward-pass activations Lk and passes an ELU\nactivation5 to produce a weighted class activation map:\nLa\nGradCAM = ELU(\nK\nX\nk=1\nαa\nkLk).\n(6)\nThis activation map has values in the range [0, 1] with higher weights corre-\nsponding to a stronger response to the input state. This can be applied to the\ncritic output in the same fashion. The resulting activation map can bilinearly\nextrapolated to the size of the input state and can then be overlayed on top of\nthis state to produce a high-quality heatmap that indicate regions that motivate\nthe agent to take action a. A visual representation of this process is depicted in\nﬁgure 1.\n5 the Exponential Linear Unit has been chosen in favor of the ReLU used in the original\nGrad-CAM paper due to the dying ReLU eﬀect described in [22].\n6\nWeitkamp et al.\nFig. 2. A detailed explanation of the Pong, BeamRider and Seaquest game frames,\nrespectively from Atari 2600 games [2]. The agent is situated in an environment with\n(multiple) moving enemies, other moving objects and semi-static objects (for example\nthe torpedoes left in BeamRider and the oxygen bar in Seaquest).\n4\nExperiments\nIn this section, we ﬁrst provide the details of our experimentation setup. We\nthen show qualitative examples evaluating how our model performs in three of\nthe Atari games. Throughout this section, red bounding boxes and red arrows\nindicate important regions of a state.\n4.1\nSetup\nThe Atari 2600 game environment is provided through the Arcade Learning\nEnvironment wrapper in the OpenAI Gym framework [2]. The framework has\nmultiple version of each game but for the purpose of this paper the NoFrameskip-\nv4 environment will be used(OpenAI considers NoFrameskip the canonical Atari\nenvironment in gym and v4 is the latest version). Each state is represented as a\n210×160×3 pixel image with a 128-colour palette, and each state is preprocessed\nto a 84 × 84 × 1 image as input to the network. A side-eﬀect of this preprocessing\nis that the visual score will be removed from the state in most games, but the\nagent still gets the reward per state implicitly through the environment.\nIn our experiments, we use three Atari games, namely Pong, BeamRider and\nSeaquest, all depicted in Figure 2. All three games have a diﬀerent action space\n(see Table 1), and a diﬀerent long term-reward system for the agent to learn.\nPong. Pong has six actions with three of the six being redundant (FIRE is equal\nto NOOP, LEFT is equal to LEFTFIRE and RIGHT is equal to RIGHTFIRE).\nThe agent is displayed on the right and the enemy on the left and the ﬁrst player\nto score 21 goals wins.\nVisual Rationalizations in Deep Reinforcement Learning for Atari Games\n7\nNOOP FIRE UP LEFT RIGHT DOWN LEFT\nFIRE\nRIGHT\nFIRE\nUP\nLEFT\nUP\nRIGHT\nUP\nFIRE\nDOWN\nLEFT\nDOWN\nRIGHT\nDOWN\nFIRE\nUP\nLEFT\nFIRE\nUP\nRIGHT\nFIRE\nDOWN\nLEFT\nFIRE\nDOWN\nRIGHT\nFIRE\nPong\nx\nx\nx\nx\nx\nx\nBeamRider x\nx\nx\nx\nx\nx\nx\nx\nx\nSeaquest\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nTable 1. Action space of Pong, BeamRider and Seaquest in the Atari 2600 OpenAI\nwrapper. Each agent from top to bottom has an increasing amount of actions.\nBeamRider. In BeamRider the agent is displayed at the bottom and the agent\nhas to traverse a series of sectors where each sector contains 15 enemies (remaining\nenemies is displayed at the top-left) and a boss at the end. The agent has three\ntorpedoes that can be used speciﬁcally to kill the sector boss, but these can also\nbe used to destroy debris that appear in later sectors. Learning how to use the\ntorpedoes correctly is not necessary to succeed in the game, but it provides for\nlong-term rewards in the form of bonus points.\nSeaquest. In Seaquest the agent is dependent on a limited amount of oxygen,\ndepicted at the bottom of the state. The agent can ascend to the surface which\nwill reﬁll the oxygen bar and it drops oﬀany swimmers that the agent has picked\nup along the way for a bonus reward. Resurfacing requires learning a long-term\nreward dependency which is not easily learned [16]. Surfacing is not just used to\nreﬁll the oxygen bar but also to drop-oﬀany swimmer that the agent has found\nunderwater which results in additional points. A diﬀerent way to get a positive\nreward is to kill sharks.\n4.2\nLearning A Policy\nTraining an agent to gain human-like or superhuman-like performance in a\ncomplex environment can take millions of input frames. In this section we take\nthe same approach as Greydanus et al. [4], in which the authors argue that deep\nRL agents, during training, discard policies in favor for better ones. Seeing how an\nagent is reacting to diﬀerent situations at diﬀerent times of training might make\nit clear how an agent is trying to maximize long-term rewards. To demonstrate\nthis, two agents have been trained for a diﬀerent number of frames. The ﬁrst\nmodel which will be called the Full Agent has been trained using (at least) 40\nmillion frames. The second agent which will be called the Half Agent has been\ntrained using 20 million frames, except for the case of Pong where it has been\ntrained using 500,000 frames, due to the fact that Pong is an easier game to learn.\nThe mean score and variance can be found in Table 2. For both games a sequence\nof states were manually sampled, after which both agents have evaluated6 the\nstate to learn spatial-temporal information. States were manually sampled by\nhaving a person (one of the authors) play one episode of each game. The states\n6 evaluated in this case means having forwarded each state that has been manually\nsampled through the model.\n8\nWeitkamp et al.\nwere sampled manually to make sure the samples were not biased towards one\nagents’ policy.\nFig. 3. Manually sampled states from the game Pong, combined with the Full Agent\nand the Half Agent’s actions Grad-CAM outputs based on these states. Indicated in\nthe red boxes is the tracking behavior exhibited by the Half Agent. Best viewed in high\nresolution in color.\nPong. For Pong, the Full Agent has learned to shoot the ball in such a way\nthat it scores by hitting the ball only once each round. The initial round might\ndiﬀer, but after that all rounds are the same: the Full Agent shoots the ball\nup high which makes the ball bounce oﬀthe wall all the way down over the\nopponents side, at which point the agent retreats to the lower right corner. This\nwould indicate that the Full Agent is not reacting to the ball most of the time,\nbut is waiting to exploit a working strategy that allows it to win each round.\nIn contrast, he Half Agent is actively tracking the ball at each step and could\npotentially be losing some rounds because of this. The tracking behavior of the\nHalf Agent is demonstrated in Figure 3 at frames 50, 51 and 53 indicated with a\nred box. In these frames the Half Agent’s attention is focused on the ball and\nthe corresponding action is to go up to match it.\nVisual Rationalizations in Deep Reinforcement Learning for Atari Games\n9\nFull Agent Mean Full Agent Variance Half Agent Mean Half Agent Variance\nPong\n21.00\n0.00\n14.99\n0.09\nBeamRider\n4659.04\n1932.58\n1597.40\n1202.00\nSeaquest\n1749.00\n11.44\nN/A\nN/A\nTable 2. The mean and variance of both the Full Agent (trained on at least 40 million\nframes) and the Half Agent (500,000 frames in Pong and 20 million frames in BeamRider)\nafter playing 100 episodes using a greedy strategy. Seaquest’s Half Agent is omitted\nbecause the Full Agent could not learn how to surface for water.\nBeamRider. For BeamRider, both agents have learned to hit enemies but the\nFull Agent has a higher average return. Looking at ﬁgure 4, both agents have\na measure of attention on the two white enemy saucers, but the intensity of\nattention diﬀers; the Full Agent has high attention on the enemies, in comparison\nwith the Half Agent which has low attention on the enemies. The Half Agent\nis either going right which is essentially a NOOP in that area or it could be\nshooting at the incoming enemy. More interesting are the last two frames: 175\nand 176. The attention of the Full Agent turns from the directly approaching\nenemy saucer to the enemy saucer on the left of it, and the agent would try to\nmove into its direction (LEFTFIRE). the Full Agent’s attention in frame 176 is\nplaced in a medium degree at the trajectory of its own laser that will hit the\nenemy saucer in the next frame. This could indicate that the Full Agent knows\nit will hit the target and is thus moving away from it, to focus on the other\nremaining enemy.\nFrom the analysis of both agents another interesting result is discovered: the\nagents do not learn to properly use the torpedoes. At the beginning of each\nepisode/level both agents would ﬁre torpedoes until they are all used up and\nthen continue on as usual. In Figure 5 this phenomena is demonstrated through a\nmanually sampled conﬁguration evaluated by the Full Agent only (the results are\nthe same for the Half Agent). The torpedoes have not been used yet, on purpose,\nand there are enemies coming towards the agent at diﬀerent time-steps. Looking\nat the Grad-CAM attention map, it would appear to be highly focused on the\nremaining three torpedoes in the upper right corner indicated by a red box. This\noccurs even when the action chosen by the agent is not of the UP-variety which\nwould trigger ﬁring a torpedo.\n10\nWeitkamp et al.\nFig. 4. Manually sampled states from the game BeamRider, combined with the Full\nAgent and the Half Agent’s actions Grad-CAM outputs based on these states. The red\nboxes indicate the diﬀerence in focus of the agents and the arrow indicates the shot\nﬁred by the agent. Best viewed in high resolution in color.\n4.3\nAgent Failing\nA diﬀerent way of looking at how rationalizations aid in understanding the\nbehavior of an agent is by looking at when an agent fails at its task. In the\ncontext of BeamRider and Seaquest, this means looking at the last couple of\nframes before the agent dies.\nVisual Rationalizations in Deep Reinforcement Learning for Atari Games\n11\nFig. 5. Manually sampled states from the game BeamRider while not ﬁring torpedoes.\nCombined with the Full Agent’s actions Grad-CAM outputs based on these states. In\nthe 300 frames played it has chosen any UP-variant 219 times, LEFTFIRE 67 and other\nactions 14 times. Best viewed in high resolution in color.\nBeamRider. In the situation depicted in Figure 6 the agent is approached by a\nnumber of diﬀerent enemies, one of which only appears after sector 7: the green\nbounce craft, depicted inside a red box in the ﬁrst four frames. This is an enemy\nthat can only be destroyed by shooting a torpedo at it, and it jumps from beam\nto beam trying to hit the agent which is what kills the agent eventually in the\nlast frame. In all frames the Grad-CAM model is focused at the nearest three\nenemies, and the agent is shooting using LEFTFIRE in the direction of the green\nbounce craft. This could add extra weight to the idea that the agent does not\nknow how to use the torpedoes correctly, but perhaps also that the agent might\nnot be able to distinguish one enemy from another; the piece of green debris to\nthe left of the green bounce craft looks quite similar to it.\nSeaquest. The agent playing Seaquest has a diﬀerent problem: it has not learned\nthe long term strategy of surfacing for oxygen. An example of a death due to\nthis is depicted in Figure 7. The oxygen bar is highlighted by a red box, and\nit is noticeable that there is no direct or intense activations produced by the\nrationalization model on the oxygen bar. This could indicate that the agent\nhas never made a correlation between the oxygen bar depleting and the episode\nending. A multitude of factors could lead the agent to not learn this such as not\nhaving enough temporal knowledge or a lack of exploratory actions. A solution to\nthis could be the use of Fine-Grained Action Repetition which selects a random\naction and performs this action for a decaying number of times [16].\n12\nWeitkamp et al.\nFig. 6. Agent dies because it is hit by a green bouncecraft, highlighted by the rex box.\nThe green bouncecraft is an enemy that only appears in later sectors of the game, but\nit looks similar to an enemy which is more easily avoidable and which also appears\nmultiple times in each sector. Best viewed in high resolution in color.\n4.4\nFailure Cases of Our Model\nLooking at Figure 7 a prominent activation is depicted in the form of a vertical\nbar at the top of the state. This vertical bar might seem a bit too ambiguous\nand even hard to interpret. This type of activation map come in two varieties:\nactivations that highlight only static objects and avoid any non-static objects\nlike agents or enemies and activations that do highlight seemingly at random.\nThe activations that highlight everything except for non-static objects are\nnoticeable in Figure 8 in the case of Pong and BeamRider. For Pong, the\nactivations are not focused on the ball but on everything except for the ball which\ncould still indicate some pattern for the agent. For BeamRider the activations\nare highlighting areas directly next to the non-static agent and enemies in the\nstate. This could indicate that the agent is calculating the trajectory of enemies\nor possible safe locations for it to go to.\nThe activations that are seemingly at random are depicted in Figure 8 in the\nlast two Seaquest frames. A possible explanation for this could be that the agent\nis not provided with enough evidence and is indiﬀerent to taking any action,\nwhich is reﬂected in the ambiguous activation map.\nVisual Rationalizations in Deep Reinforcement Learning for Atari Games\n13\nFig. 7. The agent dies due to lack of oxygen depicted in the red box. Looking at the\nactivation map for the Full Agent, it is noticeable that there are no (direct) Grad-CAM\nactivations on the oxygen bar. This could indicate a lack of understanding of the oxygen\nmechanism that allows the agent to live longer and get a higher score. Best viewed in\nhigh resolution in color.\n5\nConclusion\nIn this work, we have presented a post-hoc explanation framework that visually\nrationalizes the output of a deep reinforcement learning agent. Once the agent has\nmade the decision of which action to take, the model propagates the gradients that\nlead to that action back to the image. Hence, it is able to visualize the activation\nmap of the action as a heatmap. Our experiments on three Atari 2600 games\nindicate that the visualizations successfully attend to the regions such as the agent\nand the obstacle that lead to the action. We argue that such visual rationalizations,\ni.e. post-hoc explanations, are important to enable communication between users\nand the agents. Future work will include a quantitative evaluation in the form\nof a user study or developing an automatic evaluation metric for these kind of\nvisual explanations.\n14\nWeitkamp et al.\nFig. 8. (Seemingly) ambiguous rationalization outputs. The activations depicted in\nPong highlight the agent and its enemy. The activations are also noticeable lightly on\nthe whole ﬁeld except for the bal itself. When looking more closely to the BeamRider\nactivations, it appears that there are activations surrounding important in the game\nsuch as the agent, the lives left and incoming enemies. The Seaquest activations, in\ncontrast to the other games, seem more scattered and not focused on either objects or\nspace between objects. Best viewed in high resolution in color.\nReferences\n1. Andreas, J., Rohrbach, M., Darrell, T., Klein, D.: Neural module networks. In: 2016\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016)\n2. Bellemare, M.G., Naddaf, Y., Veness, J., Bowling, M.: The arcade learning environ-\nment: An evaluation platform for general agents. CoRR abs/1207.4708 (2012),\nhttp://arxiv.org/abs/1207.4708\n3. Biran, O., McKeown, K.: Justiﬁcation narratives for individual classiﬁcations. In:\nProceedings of the AutoML workshop at ICML 2014 (2014)\n4. Greydanus, S., Koul, A., Dodge, J., Fern, A.: Visualizing and understanding atari\nagents. CoRR abs/1711.00138 (2017), http://arxiv.org/abs/1711.00138\n5. Hausknecht, M., Stone, P.: Deep recurrent q-learning for partially observable mdps.\nCoRR, abs/1507.06527 (2015)\n6. Heess, N., Sriram, S., Lemmon, J., Merel, J., Wayne, G., Tassa, Y., Erez, T., Wang,\nZ., Eslami, A., Riedmiller, M., et al.: Emergence of locomotion behaviours in rich\nenvironments. arXiv preprint arXiv:1707.02286 (2017)\n7. Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., Meger, D.: Deep\nreinforcement learning that matters. arXiv preprint arXiv:1709.06560 (2017)\n8. Hendricks, L.A., Akata, Z., Rohrbach, M., Donahue, J., Schiele, B., Darrell, T.:\nGenerating visual explanations. In: ECCV (2016)\n9. Hessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W.,\nHorgan, D., Piot, B., Azar, M., Silver, D.: Rainbow: Combining improvements in\ndeep reinforcement learning. arXiv preprint arXiv:1710.02298 (2017)\nVisual Rationalizations in Deep Reinforcement Learning for Atari Games\n15\n10. Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver,\nD., Kavukcuoglu, K.: Asynchronous methods for deep reinforcement learning. In:\nInternational Conference on Machine Learning. pp. 1928–1937 (2016)\n11. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G.,\nGraves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G., Petersen, S., Beattie, C.,\nSadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., Hassabis,\nD.: Human-level control through deep reinforcement learning. Nature 518(7540),\n529–533 (2015)\n12. Park, D.H., Hendricks, L.A., Akata, Z., Rohrbach, A., Schiele, B., Darrell, T.,\nRohrbach, M.: Multimodal explanations: Justifying decisions and pointing to the\nevidence. In: IEEE CVPR (2018)\n13. Schulman, J., Levine, S., Abbeel, P., Jordan, M., Moritz, P.: Trust region policy\noptimization. In: International Conference on Machine Learning. pp. 1889–1897\n(2015)\n14. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O.: Proximal policy\noptimization algorithms. CoRR abs/1707.06347 (2017), http://arxiv.org/abs/\n1707.06347\n15. Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D.: Grad-\ncam: Visual explanations from deep networks via gradient-based localization. In:\nIEEE ICCV (2017)\n16. Sharma, S., Lakshminarayanan, A.S., Ravindran, B.: Learning to repeat: Fine\ngrained action repetition for deep reinforcement learning. CoRR abs/1702.06054\n(2017), http://arxiv.org/abs/1702.06054\n17. Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., Van Den Driessche, G.,\nSchrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al.: Mastering\nthe game of go with deep neural networks and tree search. nature 529(7587),\n484–489 (2016)\n18. Teach, R.L., Shortliﬀe, E.H.: An analysis of physician attitudes regarding computer-\nbased clinical consultation systems. In: Use and impact of computers in clinical\nmedicine, pp. 68–85. Springer (1981)\n19. Van Hasselt, H., Guez, A., Silver, D.: Deep reinforcement learning with double\nq-learning. In: AAAI. vol. 16, pp. 2094–2100 (2016)\n20. Williams, R.J.: Simple statistical gradient-following algorithms for connectionist\nreinforcement learning. In: Reinforcement Learning, pp. 5–32. Springer (1992)\n21. Williams, R.J.: Simple statistical gradient-following algorithms for connec-\ntionist reinforcement learning. Machine Learning 8(3), 229–256 (May 1992).\nhttps://doi.org/10.1007/BF00992696, https://doi.org/10.1007/BF00992696\n22. Xu, B., Wang, N., Chen, T., Li, M.: Empirical evaluation of rectiﬁed activations in\nconvolutional network. CoRR abs/1505.00853 (2015), http://arxiv.org/abs/\n1505.00853\n23. Zintgraf, L.M., Cohen, T.S., Adel, T., Welling, M.: Visualizing deep neural network\ndecisions: Prediction diﬀerence analysis. In: ICLR (2017)\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-02-01",
  "updated": "2019-02-01"
}