{
  "id": "http://arxiv.org/abs/1902.01313v2",
  "title": "An Effective Approach to Unsupervised Machine Translation",
  "authors": [
    "Mikel Artetxe",
    "Gorka Labaka",
    "Eneko Agirre"
  ],
  "abstract": "While machine translation has traditionally relied on large amounts of\nparallel corpora, a recent research line has managed to train both Neural\nMachine Translation (NMT) and Statistical Machine Translation (SMT) systems\nusing monolingual corpora only. In this paper, we identify and address several\ndeficiencies of existing unsupervised SMT approaches by exploiting subword\ninformation, developing a theoretically well founded unsupervised tuning\nmethod, and incorporating a joint refinement procedure. Moreover, we use our\nimproved SMT system to initialize a dual NMT model, which is further fine-tuned\nthrough on-the-fly back-translation. Together, we obtain large improvements\nover the previous state-of-the-art in unsupervised machine translation. For\ninstance, we get 22.5 BLEU points in English-to-German WMT 2014, 5.5 points\nmore than the previous best unsupervised system, and 0.5 points more than the\n(supervised) shared task winner back in 2014.",
  "text": "An Effective Approach to Unsupervised Machine Translation\nMikel Artetxe, Gorka Labaka, Eneko Agirre\nIXA NLP Group\nUniversity of the Basque Country (UPV/EHU)\n{mikel.artetxe, gorka.labaka, e.agirre}@ehu.eus\nAbstract\nWhile machine translation has traditionally re-\nlied on large amounts of parallel corpora, a re-\ncent research line has managed to train both\nNeural Machine Translation (NMT) and Sta-\ntistical Machine Translation (SMT) systems\nusing monolingual corpora only. In this pa-\nper, we identify and address several deﬁcien-\ncies of existing unsupervised SMT approaches\nby exploiting subword information, develop-\ning a theoretically well founded unsupervised\ntuning method, and incorporating a joint re-\nﬁnement procedure. Moreover, we use our im-\nproved SMT system to initialize a dual NMT\nmodel, which is further ﬁne-tuned through on-\nthe-ﬂy back-translation. Together, we obtain\nlarge improvements over the previous state-\nof-the-art in unsupervised machine transla-\ntion. For instance, we get 22.5 BLEU points\nin English-to-German WMT 2014, 5.5 points\nmore than the previous best unsupervised sys-\ntem, and 0.5 points more than the (supervised)\nshared task winner back in 2014.\n1\nIntroduction\nThe recent advent of neural sequence-to-sequence\nmodeling has resulted in signiﬁcant progress in the\nﬁeld of machine translation, with large improve-\nments in standard benchmarks (Vaswani et al.,\n2017; Edunov et al., 2018) and the ﬁrst solid\nclaims of human parity in certain settings (Has-\nsan et al., 2018).\nUnfortunately, these systems\nrely on large amounts of parallel corpora, which\nare only available for a few combinations of major\nlanguages like English, German and French.\nAiming to remove this dependency on paral-\nlel data, a recent research line has managed to\ntrain unsupervised machine translation systems\nusing monolingual corpora only. The ﬁrst such\nsystems were based on Neural Machine Transla-\ntion (NMT), and combined denoising autoencod-\ning and back-translation to train a dual model ini-\ntialized with cross-lingual embeddings (Artetxe\net al., 2018c; Lample et al., 2018a).\nNeverthe-\nless, these early systems were later superseded\nby Statistical Machine Translation (SMT) based\napproaches, which induced an initial phrase-table\nthrough cross-lingual embedding mappings, com-\nbined it with an n-gram language model, and fur-\nther improved the system through iterative back-\ntranslation (Lample et al., 2018b; Artetxe et al.,\n2018b).\nIn this paper, we develop a more principled ap-\nproach to unsupervised SMT, addressing several\ndeﬁciencies of previous systems by incorporat-\ning subword information, applying a theoretically\nwell founded unsupervised tuning method, and de-\nveloping a joint reﬁnement procedure. In addition\nto that, we use our improved SMT approach to ini-\ntialize an unsupervised NMT system, which is fur-\nther improved through on-the-ﬂy back-translation.\nOur experiments on WMT 2014/2016 French-\nEnglish and German-English show the effective-\nness of our approach, as our proposed system out-\nperforms the previous state-of-the-art in unsuper-\nvised machine translation by 5-7 BLEU points\nin all these datasets and translation directions.\nOur system also outperforms the supervised WMT\n2014 shared task winner in English-to-German,\nand is around 2 BLEU points behind it in the rest\nof translation directions, suggesting that unsuper-\nvised machine translation can be a usable alterna-\ntive in practical settings.\nThe remaining of this paper is organized as fol-\nlows. Section 2 ﬁrst discusses the related work in\nthe topic. Section 3 then describes our principled\nunsupervised SMT method, while Section 4 dis-\ncusses our hybridization method with NMT. We\nthen present the experiments done and the results\nobtained in Section 5, and Section 6 concludes the\npaper.\narXiv:1902.01313v2  [cs.CL]  24 Jul 2019\n2\nRelated work\nEarly attempts to build machine translation sys-\ntems with monolingual corpora go back to statis-\ntical decipherment (Ravi and Knight, 2011; Dou\nand Knight, 2012). These methods see the source\nlanguage as ciphertext produced by a noisy chan-\nnel model that ﬁrst generates the original English\ntext and then probabilistically replaces the words\nin it. The English generative process is modeled\nusing an n-gram language model, and the chan-\nnel model parameters are estimated using either\nexpectation maximization or Bayesian inference.\nThis basic approach was later improved by incor-\nporating syntactic knowledge (Dou and Knight,\n2013) and word embeddings (Dou et al., 2015).\nNevertheless, these methods were only shown to\nwork in limited settings, being most often evalu-\nated in word-level translation.\nMore recently, the task got a renewed inter-\nest after the concurrent work of Artetxe et al.\n(2018c) and Lample et al. (2018a) on unsuper-\nvised NMT which, for the ﬁrst time, obtained\npromising results in standard machine transla-\ntion benchmarks using monolingual corpora only.\nBoth methods build upon the recent work on\nunsupervised cross-lingual embedding mappings,\nwhich independently train word embeddings in\ntwo languages and learn a linear transformation to\nmap them to a shared space through self-learning\n(Artetxe et al., 2017, 2018a) or adversarial train-\ning (Conneau et al., 2018). The resulting cross-\nlingual embeddings are used to initialize a shared\nencoder for both languages, and the entire sys-\ntem is trained using a combination of denoising\nautoencoding, back-translation and, in the case\nof Lample et al. (2018a), adversarial training.\nThis method was further improved by Yang et al.\n(2018), who use two language-speciﬁc encoders\nsharing only a subset of their parameters, and in-\ncorporate a local and a global generative adversar-\nial network. Concurrent to our work, Lample and\nConneau (2019) report strong results initializing\nan unsupervised NMT system with a cross-lingual\nlanguage model.\nFollowing the initial work on unsupervised\nNMT, it was argued that the modular architecture\nof phrase-based SMT was more suitable for this\nproblem, and Lample et al. (2018b) and Artetxe\net al. (2018b) adapted the same principles dis-\ncussed above to train an unsupervised SMT model,\nobtaining large improvements over the original\nunsupervised NMT systems.\nMore concretely,\nboth approaches learn cross-lingual n-gram em-\nbeddings from monolingual corpora based on the\nmapping method discussed earlier, and use them\nto induce an initial phrase-table that is combined\nwith an n-gram language model and a distortion\nmodel. This initial system is then reﬁned through\niterative back-translation (Sennrich et al., 2016)\nwhich, in the case of Artetxe et al. (2018b), is pre-\nceded by an unsupervised tuning step. Our work\nidentiﬁes some deﬁciencies in these previous sys-\ntems, and proposes a more principled approach to\nunsupervised SMT that incorporates subword in-\nformation, uses a theoretically better founded un-\nsupervised tuning method, and applies a joint re-\nﬁnement procedure, outperforming these previous\nsystems by a substantial margin.\nVery recently, some authors have tried to com-\nbine both SMT and NMT to build hybrid unsuper-\nvised machine translation systems. This idea was\nalready explored by Lample et al. (2018b), who\naided the training of their unsupervised NMT sys-\ntem by combining standard back-translation with\nsynthetic parallel data generated by unsupervised\nSMT. Marie and Fujita (2018) go further and use\nsynthetic parallel data from unsupervised SMT to\ntrain a conventional NMT system from scratch.\nThe resulting NMT model is then used to aug-\nment the synthetic parallel corpus through back-\ntranslation, and a new NMT model is trained on\ntop of it from scratch, repeating the process it-\neratively. Ren et al. (2019) follow a similar ap-\nproach, but use SMT as posterior regularization\nat each iteration. As shown later in our experi-\nments, our proposed NMT hybridization obtains\nsubstantially larger absolute gains than all these\nprevious approaches, even if our initial SMT sys-\ntem is stronger and thus more challenging to im-\nprove upon.\n3\nPrincipled unsupervised SMT\nPhrase-based SMT is formulated as a log-linear\ncombination of several statistical models: a trans-\nlation model, a language model, a reordering\nmodel and a word/phrase penalty. As such, build-\ning an unsupervised SMT system requires learn-\ning these different components from monolingual\ncorpora. As it turns out, this is straightforward\nfor most of them: the language model is learned\nfrom monolingual corpora by deﬁnition; the word\nand phrase penalties are parameterless; and one\ncan drop the standard lexical reordering model at a\nsmall cost and do with the distortion model alone,\nwhich is also parameterless. This way, the main\nchallenge left is learning the translation model,\nthat is, building the phrase-table.\nOur proposed method starts by building an ini-\ntial phrase-table through cross-lingual embedding\nmappings (Section 3.1). This initial phrase-table is\nthen extended by incorporating subword informa-\ntion, addressing one of the main limitations of pre-\nvious unsupervised SMT systems (Section 3.2).\nHaving done that, we adjust the weights of the un-\nderlying log-linear model through a novel unsu-\npervised tuning procedure (Section 3.3). Finally,\nwe further improve the system by jointly reﬁning\ntwo models in opposite directions (Section 3.4).\n3.1\nInitial phrase-table\nSo as to build our initial phrase-table, we follow\nArtetxe et al. (2018b) and learn n-gram embed-\ndings for each language independently, map them\nto a shared space through self-learning, and use\nthe resulting cross-lingual embeddings to extract\nand score phrase pairs.\nMore concretely, we train our n-gram embed-\ndings using phrase2vec1, a simple extension of\nskip-gram that applies the standard negative sam-\npling loss of Mikolov et al. (2013) to bigram-\ncontext and trigram-context pairs in addition to the\nusual word-context pairs.2 Having done that, we\nmap the embeddings to a cross-lingual space us-\ning VecMap3 with identical initialization (Artetxe\net al., 2018a), which builds an initial solution\nby aligning identical words and iteratively im-\nproves it through self-learning. Finally, we extract\ntranslation candidates by taking the 100 nearest-\nneighbors of each source phrase, and score them\nby applying the softmax function over their cosine\nsimilarities:\nφ( ¯f|¯e) =\nexp\n\u0000cos(¯e, ¯f)/τ\n\u0001\nP\n¯f′ exp\n\u0000cos(¯e, ¯f′)/τ\n\u0001\nwhere the temperature τ is estimated using max-\nimum likelihood estimation over a dictionary in-\nduced in the reverse direction.\nIn addition to\nthe phrase translation probabilities in both direc-\ntions, the forward and reverse lexical weightings\n1https://github.com/artetxem/\nphrase2vec\n2So as to keep the model size within a reasonable limit,\nwe restrict the vocabulary to the most frequent 200,000 uni-\ngrams, 400,000 bigrams and 400,000 trigrams.\n3https://github.com/artetxem/vecmap\nare also estimated by aligning each word in the tar-\nget phrase with the one in the source phrase most\nlikely generating it, and taking the product of their\nrespective translation probabilities. The reader is\nreferred to Artetxe et al. (2018b) for more details.\n3.2\nAdding subword information\nAn inherent limitation of existing unsupervised\nSMT systems is that words are taken as atomic\nunits, making it impossible to exploit character-\nlevel information. This is reﬂected in the known\ndifﬁculty of these models to translate named en-\ntities, as it is very challenging to discriminate\namong related proper nouns based on distribu-\ntional information alone, yielding to translation er-\nrors like “Sunday Telegraph” →“The Times of\nLondon” (Artetxe et al., 2018b).\nSo as to overcome this issue, we propose to\nincorporate subword information once the initial\nalignment is done at the word/phrase level. For\nthat purpose, we add two additional weights to the\ninitial phrase-table that are analogous to the lexi-\ncal weightings, but use a character-level similarity\nfunction instead of word translation probabilities:\nscore( ¯f|¯e) =\nY\ni\nmax\n\u0012\nϵ, max\nj\nsim( ¯fi, ¯ej)\n\u0013\nwhere ϵ = 0.3 guarantees a minimum similarity\nscore, as we want to favor translation candidates\nthat are similar at the character level without ex-\ncessively penalizing those that are not. In our case,\nwe use a simple similarity function that normal-\nizes the Levenshtein distance lev(·) (Levenshtein,\n1966) by the length of the words len(·):\nsim(f, e) = 1 −\nlev(f, e)\nmax(len(f), len(e))\nWe leave the exploration of more elaborated sim-\nilarity functions and, in particular, learnable met-\nrics (McCallum et al., 2005), for future work.\n3.3\nUnsupervised tuning\nHaving trained the underlying statistical models\nindependently, SMT tuning aims to adjust the\nweights of their resulting log-linear combination\nto optimize some evaluation metric like BLEU in a\nparallel validation corpus, which is typically done\nthrough Minimum Error Rate Training or MERT\n(Och, 2003). Needless to say, this cannot be done\nin strictly unsupervised settings, but we argue that\nit would still be desirable to optimize some un-\nsupervised criterion that is expected to correlate\nwell with test performance. Unfortunately, nei-\nther of the existing unsupervised SMT systems\ndo so: Artetxe et al. (2018b) use a heuristic that\nbuilds two initial models in opposite directions,\nuses one of them to generates a synthetic parallel\ncorpus through back-translation (Sennrich et al.,\n2016), and applies MERT to tune the model in\nthe reverse direction, iterating until convergence,\nwhereas Lample et al. (2018b) do not perform any\ntuning at all. In what follows, we propose a more\nprincipled approach to tuning that deﬁnes an unsu-\npervised criterion and an optimization procedure\nthat is guaranteed to converge to a local optimum\nof it.\nInspired by the previous work on CycleGANs\n(Zhu et al., 2017) and dual learning (He et al.,\n2016), our method takes two initial models in op-\nposite directions, and deﬁnes an unsupervised op-\ntimization objective that combines a cyclic con-\nsistency loss and a language model loss over the\ntwo monolingual corpora E and F:\nL = Lcycle(E) + Lcycle(F) + Llm(E) + Llm(F)\nThe cyclic consistency loss captures the intu-\nition that the translation of a translation should be\nclose to the original text. So as to quantify this, we\ntake a monolingual corpus in the source language,\ntranslate it to the target language and back to the\nsource language, and compute its BLEU score tak-\ning the original text as reference:\nLcycle(E) = 1 −BLEU(TF→E(TE→F (E)), E)\nAt the same time, the language model loss cap-\ntures the intuition that machine translation should\nproduce ﬂuent text in the target language. For that\npurpose, we estimate the per-word entropy in the\ntarget language corpus using an n-gram language\nmodel, and penalize higher per-word entropies in\nmachine translated text as follows:4\nLlm(E) = LP · max(0, H(F) −H(TE→F (E)))2\n4We initially tried to directly minimize the entropy of the\ngenerated text, but this worked poorly in our preliminary ex-\nperiments on English-Spanish (note that we used this lan-\nguage pair exclusively for development to be faithful to our\nunsupervised scenario at test time). More concretely, the be-\nhavior of the optimization algorithm was very unstable, as it\ntended to excessively focus on either the cyclic consistency\nloss or the language model loss at the cost of the other, and\nwe found it very difﬁcult to ﬁnd the right balance between the\ntwo factors.\nwhere the length penalty LP = LP(E) · LP(F)\npenalizes excessively long translations:5\nLP(E) = max\n\u0012\n1, len(TF→E(TE→F (E)))\nlen(E)\n\u0013\nSo as to minimize the combined loss function,\nwe adapt MERT to jointly optimize the param-\neters of the two models. In its basic form, MERT\napproximates the search space for each source\nsentence through an n-best list, and performs a\nform of coordinate descent by computing the op-\ntimal value for each parameter through an efﬁ-\ncient line search method and greedily taking the\nstep that leads to the largest gain. The process\nis repeated iteratively until convergence, augment-\ning the n-best list with the updated parameters at\neach iteration so as to obtain a better approxima-\ntion of the full search space. Given that our opti-\nmization objective combines two translation sys-\ntems TF→E(TE→F (E)), this would require gen-\nerating an n-best list for TE→F (E) ﬁrst and, for\neach entry on it, generating a new n-best list with\nTF→E, yielding a combined n-best list with N2\nentries. So as to make it more efﬁcient, we pro-\npose an alternating optimization approach where\nwe ﬁx the parameters of one model and optimize\nthe other with standard MERT. Thanks to this, we\ndo not need to expand the search space of the ﬁxed\nmodel, so we can do with an n-best list of N en-\ntries alone. Having done that, we ﬁx the parame-\nters of the opposite model and optimize the other,\niterating until convergence.\n3.4\nJoint reﬁnement\nConstrained by the lack of parallel corpora, the\nprocedure described so far makes important sim-\npliﬁcations that could compromise its potential\nperformance: its phrase-table is somewhat unnatu-\nral (e.g. the translation probabilities are estimated\nfrom cross-lingual embeddings rather than actual\nfrequency counts) and it lacks a lexical reordering\nmodel altogether. So as to overcome this issue, ex-\nisting unsupervised SMT methods generate a syn-\nthetic parallel corpus through back-translation and\nuse it to train a standard SMT system from scratch,\niterating until convergence.\n5Without this penalization, the system tended to produce\nunnecessary tokens (e.g. quotes) that looked natural in their\ncontext, which served to minimize the per-word perplexity\nof the output. Minimizing the overall perplexity instead of\nthe per-word perplexity did not solve the problem, as the op-\nposite phenomenon arose (i.e. the system tended to produce\nexcessively short translations).\nAn obvious drawback of this approach is that\nthe back-translated side will contain ungrammati-\ncal n-grams and other artifacts that will end up in\nthe induced phrase-table. One could argue that this\nshould be innocuous as long as the ungrammati-\ncal n-grams are in the source side, as they should\nnever occur in real text and their corresponding en-\ntries in the phrase-table should therefore not be\nused.\nHowever, ungrammatical source phrases\ndo ultimately affect the estimation of the back-\nward translation probabilities, including those of\ngrammatical phrases.6 We argue that, ultimately,\nthe backward probability estimations can only be\nmeaningful when all source phrases are grammati-\ncal (so the probabilities of all plausible translations\nsum to one) and, similarly, the forward probabil-\nity estimations can only be meaningful when all\ntarget phrases are grammatical.\nFollowing the above observation, we propose\nan alternative approach that jointly reﬁnes both\ntranslation directions.\nMore concretely, we use\nthe initial systems to build two synthetic corpora\nin opposite directions.7 Having done that, we in-\ndependently extract phrase pairs from each syn-\nthetic corpus, and build a phrase-table by taking\ntheir intersection. The forward probabilities are\nestimated in the parallel corpus with the synthetic\nsource side, while the backward probabilities are\nestimated in the one with the synthetic target side.\nThis does not only guarantee that the probability\nestimates are meaningful as discussed previously,\nbut it also discards the ungrammatical phrases al-\ntogether, as both the source and the target n-grams\nmust have occurred in the original monolingual\ntexts to be present in the resulting phrase-table.\nThis phrase-table is then combined with a lexical\nreordering model learned on the synthetic parallel\ncorpus in the reverse direction, and we apply the\nunsupervised tuning method described in Section\n3.3 to adjust the weights of the resulting system.\nWe repeat this process for a total of 3 iterations.8\n6For instance, let’s say that the target phrase “dos gatos”\nhas been aligned 10 times with “two cats” and 90 times with\n“two cat”. While the ungrammatical phrase-table entry two\ncat- dos gatos should never be picked, the backward proba-\nbility estimation of two cats - dos gatos is still affected by it\n(it would be 0.1 instead of 1.0 in this example).\n7For efﬁciency purposes, we restrict the size of each syn-\nthetic parallel corpus to 10 million sentence pairs.\n8For the last iteration, we do not perform any tuning and\nuse default Moses weights instead, which we found to be\nmore robust during development. Note, however, that us-\ning unsupervised tuning during the previous steps was still\nstrongly beneﬁcial.\n4\nNMT hybridization\nWhile the rigid and modular design of SMT pro-\nvides a very suitable framework for unsupervised\nmachine translation, NMT has shown to be a fairly\nsuperior paradigm in supervised settings, outper-\nforming SMT by a large margin in standard bench-\nmarks. As such, the choice of SMT over NMT\nalso imposes a hard ceiling on the potential per-\nformance of these approaches, as unsupervised\nSMT systems inherit the very same limitations\nof their supervised counterparts (e.g. the local-\nity and sparsity problems). For that reason, we\nargue that SMT provides a more appropriate ar-\nchitecture to ﬁnd an initial alignment between the\nlanguages, but NMT is ultimately a better archi-\ntecture to model the translation process.\nFollowing this observation, we propose a hybrid\napproach that uses unsupervised SMT to warm up\na dual NMT model trained through iterative back-\ntranslation.\nMore concretely, we ﬁrst train two\nSMT systems in opposite directions as described\nin Section 3, and use them to assist the training of\nanother two NMT systems in opposite directions.\nThese NMT systems are trained following an it-\nerative process where, at each iteration, we alter-\nnately update the model in each direction by per-\nforming a single pass over a synthetic parallel cor-\npus built through back-translation (Sennrich et al.,\n2016).9 In the ﬁrst iteration, the synthetic parallel\ncorpus is entirely generated by the SMT system in\nthe opposite direction but, as training progresses\nand the NMT models get better, we progressively\nswitch to a synthetic parallel corpus generated by\nthe reverse NMT model. More concretely, itera-\ntion t uses Nsmt = N · max(0, 1 −t/a) syn-\nthetic parallel sentences from the reverse SMT\nsystem, where the parameter a controls the num-\nber of transition iterations from SMT to NMT\nback-translation. The remaining N −Nsmt sen-\ntences are generated by the reverse NMT model.\nInspired by Edunov et al. (2018), we use greedy\ndecoding for half of them, which produces more\nﬂuent and predictable translations, and random\nsampling for the other half, which produces more\nvaried translations. In our experiments, we use\nN = 1, 000, 000 and a = 30, and perform a to-\ntal of 60 such iterations. At test time, we use beam\nsearch decoding with an ensemble of all check-\n9Note that we do not train a new model from scratch each\ntime, but continue training the model from the previous iter-\nation.\nWMT-14\nWMT-16\nfr-en\nen-fr\nde-en\nen-de\nde-en\nen-de\nNMT\nArtetxe et al. (2018c)\n15.6\n15.1\n10.2\n6.6\n-\n-\nLample et al. (2018a)\n14.3\n15.1\n-\n-\n13.3\n9.6\nYang et al. (2018)\n15.6\n17.0\n-\n-\n14.6\n10.9\nLample et al. (2018b)\n24.2\n25.1\n-\n-\n21.0\n17.2\nSMT\nArtetxe et al. (2018b)\n25.9\n26.2\n17.4\n14.1\n23.1\n18.2\nLample et al. (2018b)\n27.2\n28.1\n-\n-\n22.9\n17.9\nMarie and Fujita (2018)∗\n-\n-\n-\n-\n20.2\n15.5\nProposed system\n28.4\n30.1\n20.1\n15.8\n25.4\n19.7\ndetok. SacreBLEU∗\n27.9\n27.8\n19.7\n14.7\n24.8\n19.4\nSMT\n+\nNMT\nLample et al. (2018b)\n27.7\n27.6\n-\n-\n25.2\n20.2\nMarie and Fujita (2018)∗\n-\n-\n-\n-\n26.7\n20.0\nRen et al. (2019)\n28.9\n29.5\n20.4\n17.0\n26.3\n21.7\nProposed system\n33.5\n36.2\n27.0\n22.5\n34.4\n26.9\ndetok. SacreBLEU∗\n33.2\n33.6\n26.4\n21.2\n33.8\n26.4\nTable 1: Results of the proposed method in comparison to previous work (BLEU). Overall best results are in bold,\nthe best ones in each group are underlined.\n∗Detokenized BLEU equivalent to the ofﬁcial mteval-v13a.pl script. The rest use tokenized BLEU with\nmulti-bleu.perl (or similar).\npoints from every 10 iterations.\n5\nExperiments and results\nIn order to make our experiments comparable to\nprevious work, we use the French-English and\nGerman-English datasets from the WMT 2014\nshared task.\nMore concretely, our training data\nconsists of the concatenation of all News Crawl\nmonolingual corpora from 2007 to 2013, which\nmake a total of 749 million tokens in French, 1,606\nmillions in German, and 2,109 millions in English,\nfrom which we take a random subset of 2,000\nsentences for tuning (Section 3.3). Preprocessing\nis done using standard Moses tools, and involves\npunctuation normalization, tokenization with ag-\ngressive hyphen splitting, and truecasing.\nOur SMT implementation is based on Moses10,\nand we use the KenLM (Heaﬁeld et al., 2013)\ntool included in it to estimate our 5-gram language\nmodel with modiﬁed Kneser-Ney smoothing. Our\nunsupervised tuning implementation is based on\nZ-MERT (Zaidan, 2009), and we use FastAlign\n(Dyer et al., 2013) for word alignment within the\njoint reﬁnement procedure. Finally, we use the big\ntransformer implementation from fairseq11 for our\nNMT system, training with a total batch size of\n20,000 tokens across 8 GPUs with the exact same\nhyperparameters as Ott et al. (2018).\nWe use newstest2014 as our test set for\n10http://www.statmt.org/moses/\n11https://github.com/pytorch/fairseq\nFrench-English, and both newstest2014 and new-\nstest2016 (from WMT 201612) for German-\nEnglish.\nFollowing common practice, we re-\nport tokenized BLEU scores as computed by the\nmulti-bleu.perl script included in Moses.\nIn addition to that, we also report detokenized\nBLEU scores as computed by SacreBLEU13\n(Post, 2018), which is equivalent to the ofﬁcial\nmteval-v13a.pl script.\nWe next present the results of our proposed sys-\ntem in comparison to previous work in Section\n5.1. Section 5.2 then compares the obtained re-\nsults to those of different supervised systems. Fi-\nnally, Section 5.3 presents some translation exam-\nples from our system.\n5.1\nMain results\nTable 1 reports the results of the proposed sys-\ntem in comparison to previous work. As it can be\nseen, our full system obtains the best published re-\nsults in all cases, outperforming the previous state-\nof-the-art by 5-7 BLEU points in all datasets and\ntranslation directions.\nA substantial part of this improvement comes\nfrom our more principled unsupervised SMT ap-\n12Note that it is only the test set that is from WMT 2016.\nAll the training data comes from WMT 2014 News Crawl, so\nit is likely that our results could be further improved by using\nthe more extensive monolingual corpora from WMT 2016.\n13SacreBLEU signature: BLEU+case.mixed+lang.LANG\n+numrefs.1+smooth.exp+test.TEST+tok.13a+version.1.2.1\n1, with LANG ∈{fr-en, en-fr, de-en, en-de} and TEST ∈\n{wmt14/full, wmt16}\nWMT-14\nWMT-16\nfr-en\nen-fr\nde-en\nen-de\nLample et al. (2018b)\nInitial SMT\n27.2\n28.1\n22.9\n17.9\n+ NMT hybrid\n27.7 (+0.5)\n27.6 (-0.5)\n25.2 (+2.3)\n20.2 (+2.3)\nMarie and Fujita (2018)\nInitial SMT\n-\n-\n20.2\n15.5\n+ NMT hybrid\n-\n-\n26.7 (+6.5)\n20.0 (+4.5)\nProposed system\nInitial SMT\n28.4\n30.1\n25.4\n19.7\n+ NMT hybrid\n33.5 (+5.1)\n36.2 (+6.1)\n34.4 (+9.0)\n26.9 (+7.2)\nTable 2: NMT hybridization results for different unsupervised machine translation systems (BLEU).\nWMT-14\nfr-en\nen-fr\nde-en\nen-de\nUnsupervised\nProposed system\n33.5\n36.2\n27.0\n22.5\ndetok. SacreBLEU∗\n33.2\n33.6\n26.4\n21.2\nSupervised\nWMT best∗\n35.0\n35.8\n29.0\n20.6†\nVaswani et al. (2017)\n-\n41.0\n-\n28.4\nEdunov et al. (2018)\n-\n45.6\n-\n35.0\nTable 3: Results of the proposed method in comparison to different supervised systems (BLEU).\n∗Detokenized BLEU equivalent to the ofﬁcial mteval-v13a.pl script. The rest use tokenized BLEU with\nmulti-bleu.perl (or similar).\n†Results in the original test set from WMT 2014, which slightly differs from the full test set used in all subsequent\nwork. Our proposed system obtains 22.4 BLEU points (21.1 detokenized) in that same subset.\nproach, which outperforms all previous SMT-\nbased systems by around 2 BLEU points. Nev-\nertheless, it is the NMT hybridization that brings\nthe largest gains, improving the results of this ini-\ntial SMT systems by 5-9 BLEU points. As shown\nin Table 2, our absolute gains are considerably\nlarger than those of previous hybridization meth-\nods, even if our initial SMT system is substan-\ntially better and thus more difﬁcult to improve\nupon. This way, our initial SMT system is about\n4-5 BLEU points above that of Marie and Fujita\n(2018), yet our absolute gain on top of it is around\n2.5 BLEU points higher. When compared to Lam-\nple et al. (2018b), we obtain an absolute gain of 5-\n6 BLEU points in both French-English directions\nwhile they do not get any clear improvement, and\nwe obtain an improvement of 7-9 BLEU points in\nboth German-English directions, in contrast with\nthe 2.3 BLEU points they obtain.\nMore generally, it is interesting that pure SMT\nsystems perform better than pure NMT systems,\nyet the best results are obtained by initializing an\nNMT system with an SMT system. This suggests\nthat the rigid and modular architecture of SMT\nmight be more suitable to ﬁnd an initial alignment\nbetween the languages, but the ﬁnal system should\nbe ultimately based on NMT for optimal results.\n5.2\nComparison with supervised systems\nSo as to put our results into perspective, Table 3 re-\nports the results of different supervised systems in\nthe same WMT 2014 test set. More concretely, we\ninclude the best results from the shared task itself,\nwhich reﬂect the state-of-the-art in machine trans-\nlation back in 2014; those of Vaswani et al. (2017),\nwho introduced the now predominant transformer\narchitecture; and those of Edunov et al. (2018),\nwho apply back-translation at a large scale and,\nto the best of our knowledge, hold the current best\nresults in the test set.\nAs it can be seen, our unsupervised system out-\nperforms the WMT 2014 shared task winner in\nEnglish-to-German, and is around 2 BLEU points\nbehind it in the other translation directions. This\nshows that unsupervised machine translation is al-\nready competitive with the state-of-the-art in su-\npervised machine translation in 2014. While the\nﬁeld of machine translation has undergone great\nprogress in the last 5 years, and the gap between\nour unsupervised system and the current state-of-\nthe-art in supervised machine translation is still\nlarge as reﬂected by the other results, this suggests\nthat unsupervised machine translation can be a us-\nable alternative in practical settings.\nSource\nReference\nArtetxe et al. (2018b)\nProposed system\nD’autres révélations ont fait\nétat de documents divulgués\npar Snowden selon lesquels\nla NSA avait intercepté des\ndonnées et des communica-\ntions émanant du téléphone\nportable de la chancelière alle-\nmande Angela Merkel et de\nceux de 34 autres chefs d’État.\nOther revelations cited doc-\numents leaked by Snow-\nden that the NSA moni-\ntored German Chancellor\nAngela Merkel’s cellphone\nand those of up to 34 other\nworld leaders.\nOther disclosures have re-\nported documents disclosed\nby Snowden suggested the\nNSA\nhad\nintercepted\ncommunications and data\nfrom the mobile phone of\nGerman Chancellor Angela\nMerkel and those of 32\nother heads of state.\nOther\nrevelations\nhave\npointed to documents dis-\nclosed\nby\nSnowden\nthat\nthe\nNSA\nhad\nintercepted\ndata\nand\ncommunications\nemanating\nfrom\nGerman\nChancellor Angela Merkel’s\nmobile phone and those of\n34 other heads of state.\nLa NHTSA n’a pas pu ex-\naminer la lettre d’information\naux propriétaires en raison de\nl’arrêt de 16 jours des activités\ngouvernementales, ce qui a\nralenti la croissance des ventes\nde véhicules en octobre.\nNHTSA could not review\nthe owner notiﬁcation let-\nter due to the 16-day gov-\nernment shutdown, which\ntempered auto sales growth\nin October.\nThe NHTSA could not con-\nsider the letter of informa-\ntion to owners because of\nhalting 16-day government\nactivities, which slowed the\ngrowth in vehicle sales in\nOctober.\nNHTSA said it could not ex-\namine the letter of informa-\ntion to owners because of the\n16-day halt in government\noperations, which slowed ve-\nhicle sales growth in Octo-\nber.\nLe M23 est né d’une mu-\ntinerie,\nen\navril\n2012,\nd’anciens\nrebelles,\nessen-\ntiellement tutsi, intégrés dans\nl’armée en 2009 après un\naccord de paix.\nThe M23 was born of an\nApril 2012 mutiny by for-\nmer rebels, principally Tut-\nsis who were integrated\ninto the army in 2009 fol-\nlowing a peace agreement.\nM23 began as a mutiny in\nApril 2012, former rebels,\nmainly Tutsi integrated into\nthe national army in 2009\nafter a peace deal.\nThe M23 was born into a\nmutiny in April 2012, of for-\nmer rebels, mostly Tutsi, em-\nbedded in the army in 2009\nafter a peace deal.\nTunks\na\ndéclaré\nau\nSun-\nday Telegraph de Sydney que\ntoute la famille était «extrême-\nment préoccupée» du bien-\nêtre de sa ﬁlle et voulait\nqu’elle rentre en Australie.\nTunks told Sydney’s Sun-\nday Telegraph the whole\nfamily was “extremely con-\ncerned” about his daugh-\nter’s welfare and wanted\nher back in Australia.\nTunks told The Times of\nLondon from Sydney that\nthe whole family was “ex-\ntremely concerned” of the\nwelfare of her daughter and\nwanted it to go in Australia.\nTunks told the Sunday Tele-\ngraph in Sydney that the\nwhole family was “extremely\nconcerned” about her daugh-\nter’s well-being and wanted\nher to go into Australia.\nTable 4: Randomly chosen translation examples from French→English newstest2014 in comparison of those re-\nported by Artetxe et al. (2018b).\n5.3\nQualitative results\nTable 4 shows some translation examples from our\nproposed system in comparison to those reported\nby Artetxe et al. (2018b). We choose the exact\nsame sentences reported by Artetxe et al. (2018b),\nwhich were randomly taken from newstest2014,\nso they should be representative of the general be-\nhavior of both systems.\nWhile not perfect, our proposed system pro-\nduces generally ﬂuent translations that accurately\ncapture the meaning of the original text. Just in\nline with our quantitative results, this suggests that\nunsupervised machine translation can be a usable\nalternative in practical settings.\nCompared to Artetxe et al. (2018b), our transla-\ntions are generally more ﬂuent, which is not sur-\nprising given that they are produced by an NMT\nsystem rather than an SMT system. In addition to\nthat, the system of Artetxe et al. (2018b) has some\nadequacy issues when translating named entities\nand numerals (e.g. 34 →32, Sunday Telegraph →\nThe Times of London), which we do not observe\nfor our proposed system in these examples.\n6\nConclusions and future work\nIn this paper, we identify several deﬁciencies in\nprevious unsupervised SMT systems, and pro-\npose a more principled approach that addresses\nthem by incorporating subword information, us-\ning a theoretically well founded unsupervised tun-\ning method, and developing a joint reﬁnement pro-\ncedure. In addition to that, we use our improved\nSMT approach to initialize a dual NMT model\nthat is further improved through on-the-ﬂy back-\ntranslation. Our experiments show the effective-\nness of our approach, as we improve the previous\nstate-of-the-art in unsupervised machine transla-\ntion by 5-7 BLEU points in French-English and\nGerman-English WMT 2014 and 2016. Our code\nis available as an open source project at https:\n//github.com/artetxem/monoses.\nIn the future, we would like to explore learn-\nable similarity functions like the one proposed by\n(McCallum et al., 2005) to compute the character-\nlevel scores in our initial phrase-table. In addition\nto that, we would like to incorporate a language\nmodeling loss during NMT training similar to He\net al. (2016). Finally, we would like to adapt our\napproach to more relaxed scenarios with multiple\nlanguages and/or small parallel corpora.\nAcknowledgments\nThis research was partially supported by the\nSpanish MINECO (UnsupNMT TIN2017-91692-\nEXP and DOMINO PGC2018-102041-B-I00, co-\nfunded by EU FEDER), the BigKnowledge\nproject (BBVA foundation grant 2018),\nthe\nUPV/EHU (excellence research group), and the\nNVIDIA GPU grant program. Mikel Artetxe was\nsupported by a doctoral grant from the Spanish\nMECD.\nReferences\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.\nLearning bilingual word embeddings with (almost)\nno bilingual data. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 451–462,\nVancouver, Canada. Association for Computational\nLinguistics.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre.\n2018a. A robust self-learning method for fully un-\nsupervised cross-lingual mappings of word embed-\ndings. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 789–798. Association\nfor Computational Linguistics.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre.\n2018b.\nUnsupervised statistical machine transla-\ntion.\nIn Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 3632–3642, Brussels, Belgium. Associ-\nation for Computational Linguistics.\nMikel Artetxe, Gorka Labaka, Eneko Agirre, and\nKyunghyun Cho. 2018c. Unsupervised neural ma-\nchine translation. In Proceedings of the 6th Inter-\nnational Conference on Learning Representations\n(ICLR 2018).\nAlexis Conneau, Guillaume Lample, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Hervé Jégou. 2018.\nWord translation without parallel data. In Proceed-\nings of the 6th International Conference on Learning\nRepresentations (ICLR 2018).\nQing Dou and Kevin Knight. 2012. Large scale deci-\npherment for out-of-domain machine translation. In\nProceedings of the 2012 Joint Conference on Empir-\nical Methods in Natural Language Processing and\nComputational Natural Language Learning, pages\n266–275, Jeju Island, Korea. Association for Com-\nputational Linguistics.\nQing Dou and Kevin Knight. 2013. Dependency-based\ndecipherment for resource-limited machine transla-\ntion. In Proceedings of the 2013 Conference on Em-\npirical Methods in Natural Language Processing,\npages 1668–1676, Seattle, Washington, USA. Asso-\nciation for Computational Linguistics.\nQing Dou, Ashish Vaswani, Kevin Knight, and Chris\nDyer. 2015. Unifying bayesian inference and vector\nspace models for improved decipherment. In Pro-\nceedings of the 53rd Annual Meeting of the Associ-\nation for Computational Linguistics and the 7th In-\nternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 836–\n845, Beijing, China. Association for Computational\nLinguistics.\nChris Dyer, Victor Chahuneau, and Noah A. Smith.\n2013. A simple, fast, and effective reparameteriza-\ntion of ibm model 2. In Proceedings of the 2013\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 644–648, Atlanta,\nGeorgia. Association for Computational Linguistics.\nSergey Edunov, Myle Ott, Michael Auli, and David\nGrangier. 2018. Understanding back-translation at\nscale.\nIn Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 489–500, Brussels, Belgium. Association\nfor Computational Linguistics.\nHany Hassan, Anthony Aue, Chang Chen, Vishal\nChowdhary,\nJonathan\nClark,\nChristian\nFeder-\nmann, Xuedong Huang, Marcin Junczys-Dowmunt,\nWilliam Lewis, Mu Li, et al. 2018. Achieving hu-\nman parity on automatic chinese to english news\ntranslation. arXiv preprint arXiv:1803.05567.\nDi He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu,\nTie-Yan Liu, and Wei-Ying Ma. 2016. Dual learn-\ning for machine translation. In Advances in Neural\nInformation Processing Systems 29, pages 820–828.\nKenneth Heaﬁeld, Ivan Pouzyrevsky, Jonathan H.\nClark, and Philipp Koehn. 2013. Scalable modiﬁed\nkneser-ney language model estimation. In Proceed-\nings of the 51st Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 690–696, Soﬁa, Bulgaria. Association\nfor Computational Linguistics.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. arXiv preprint\narXiv:1901.07291.\nGuillaume Lample, Alexis Conneau, Ludovic De-\nnoyer, and Marc’Aurelio Ranzato. 2018a.\nUn-\nsupervised machine translation using monolingual\ncorpora only.\nIn Proceedings of the 6th Inter-\nnational Conference on Learning Representations\n(ICLR 2018).\nGuillaume Lample, Myle Ott, Alexis Conneau, Lu-\ndovic Denoyer, and Marc’Aurelio Ranzato. 2018b.\nPhrase-based & neural unsupervised machine trans-\nlation. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 5039–5049, Brussels, Belgium. Associ-\nation for Computational Linguistics.\nVladimir I Levenshtein. 1966. Binary codes capable\nof correcting deletions, insertions, and reversals. In\nSoviet physics doklady, volume 10, pages 707–710.\nBenjamin Marie and Atsushi Fujita. 2018. Unsuper-\nvised neural machine translation initialized by un-\nsupervised statistical machine translation.\narXiv\npreprint arXiv:1810.12703.\nAndrew McCallum, Kedar Bellare, and Fernando\nPereira. 2005.\nA conditional random ﬁeld for\ndiscriminatively-trained ﬁnite-state string edit dis-\ntance. In Proceedings of the Twenty-First Confer-\nence on Uncertainty in Artiﬁcial Intelligence, pages\n388–395.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111–3119.\nFranz Josef Och. 2003.\nMinimum error rate train-\ning in statistical machine translation. In Proceed-\nings of the 41st Annual Meeting of the Association\nfor Computational Linguistics, pages 160–167, Sap-\nporo, Japan. Association for Computational Linguis-\ntics.\nMyle Ott,\nSergey Edunov,\nDavid Grangier,\nand\nMichael Auli. 2018. Scaling neural machine trans-\nlation. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 1–9,\nBelgium, Brussels. Association for Computational\nLinguistics.\nMatt Post. 2018. A call for clarity in reporting bleu\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Belgium, Brussels. Association for Computa-\ntional Linguistics.\nSujith Ravi and Kevin Knight. 2011. Deciphering for-\neign language. In Proceedings of the 49th Annual\nMeeting of the Association for Computational Lin-\nguistics: Human Language Technologies, pages 12–\n21, Portland, Oregon, USA. Association for Compu-\ntational Linguistics.\nShuo Ren, Zhirui Zhang, Shujie Liu, Ming Zhou,\nand Shuai Ma. 2019.\nUnsupervised neural ma-\nchine translation with smt as posterior regulariza-\ntion. arXiv preprint arXiv:1901.04112.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Improving neural machine translation mod-\nels with monolingual data.\nIn Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n86–96, Berlin, Germany. Association for Computa-\ntional Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 6000–6010.\nZhen Yang, Wei Chen, Feng Wang, and Bo Xu.\n2018. Unsupervised neural machine translation with\nweight sharing. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 46–55. As-\nsociation for Computational Linguistics.\nOmar Zaidan. 2009. Z-mert: A fully conﬁgurable open\nsource tool for minimum error rate training of ma-\nchine translation systems. The Prague Bulletin of\nMathematical Linguistics, 91:79–88.\nJun-Yan Zhu,\nTaesung Park,\nPhillip Isola,\nand\nAlexei A. Efros. 2017.\nUnpaired image-to-image\ntranslation using cycle-consistent adversarial net-\nworks.\nIn The IEEE International Conference on\nComputer Vision (ICCV).\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2019-02-04",
  "updated": "2019-07-24"
}