{
  "id": "http://arxiv.org/abs/1902.01449v1",
  "title": "Generalization Bounds For Unsupervised and Semi-Supervised Learning With Autoencoders",
  "authors": [
    "Baruch Epstein",
    "Ron Meir"
  ],
  "abstract": "Autoencoders are widely used for unsupervised learning and as a\nregularization scheme in semi-supervised learning. However, theoretical\nunderstanding of their generalization properties and of the manner in which\nthey can assist supervised learning has been lacking. We utilize recent\nadvances in the theory of deep learning generalization, together with a novel\nreconstruction loss, to provide generalization bounds for autoencoders. To the\nbest of our knowledge, this is the first such bound. We further show that,\nunder appropriate assumptions, an autoencoder with good generalization\nproperties can improve any semi-supervised learning scheme. We support our\ntheoretical results with empirical demonstrations.",
  "text": "Proceedings of Machine Learning Research vol XX:1–14, 2019\nGeneralization Bounds For Unsupervised and Semi-Supervised\nLearning With Autoencoders\nBaruch Epstein\nBARUCH.EPSTEIN@GMAIL.COM and Ron Meir\nRMEIR@EE.TECHNION.AC.IL\nViterbi Faculty of Electrical Engineering, Technion - Israel Institute of Technology\nEditor: Under Review for COLT 2019\nAbstract\nAutoencoders are widely used for unsupervised learning and as a regularization scheme in semi-\nsupervised learning. However, theoretical understanding of their generalization properties and of\nthe manner in which they can assist supervised learning has been lacking. We utilize recent ad-\nvances in the theory of deep learning generalization, together with a novel reconstruction loss, to\nprovide generalization bounds for autoencoders. To the best of our knowledge, this is the ﬁrst such\nbound. We further show that, under appropriate assumptions, an autoencoder with good general-\nization properties can improve any semi-supervised learning scheme. We support our theoretical\nresults with empirical demonstrations.\nKeywords: Autoencoders, generalization, unsupervised learning, semi-supervised learning\n1. Introduction\nAn autoencoder (AE) (Hinton and Salakhutdinov (2006)) is a type of feedforward neural network,\naiming at reconstructing its own input through a narrow bottleneck. It typically comprises two\nparts: enc : X →R and dec : R →X with r ∈R some representation of the input x ∈X.\nUsually, r is of a smaller dimension than x. The network is trained to ﬁnd encoder and decoder\nfunctions such that some loss l (x, dec (enc (x))) is minimized. A typical choice is the square loss\n∥x −dec (enc (x))∥2\n2. In recent years, autoencoders have emerged as a standard tool for both unsu-\npervised and semi-supervised learning (SSL) (Vincent et al. (2010), Zhuang et al. (2015), Ghifary\net al. (2016), Bousmalis et al. (2016), Epstein et al. (2018)). Unfortunately, as is frequently the case\nwith deep learning approaches, the empirical practice has not been matched by parallel advances\nin theory. That is, unsupervised learning with autoencoders has not been able to beneﬁt from the\nrecent bounds for supervised deep learning (Bartlett et al. (2017), Neyshabur et al. (2018) Arora\net al. (2018), Golowich et al. (2018)). In SSL, the discrepancy is more severe still. There is a funda-\nmental tension between the goals of a supervised learner and those of an AE. One might say that an\nAE “wants to remember everything a classiﬁer wants to forget”. Indeed, given two slightly different\nimages of the digit 3, a classiﬁer would like to ignore all differences, aiming instead to see them\nas similar objects. An AE, on the other hand, aims at reconstructing precisely those nuances (e.g.\nwidth, location in the image, style) that do not matter for the classiﬁcation. Some works (Rifai et al.\n(2011)) have attempted to encourage AEs to weigh classiﬁcation-relevant features more heavily, but\nthis still dodges the basic question - why might an AE be useful for supervised learning?\nWe address the gaps above. First, we introduce a margin-based reconstruction loss which allows\na natural adaptation of existing generalization bounds for autoencoders, and show that a bounded\nloss in that sense implies a bounded loss in the standard L2 metric. Second, we demonstrate a\nmechanism by which a well-performing autoencoder is likely to assist in SSL; namely, it allows\nc⃝2019 B. Epstein & R. Meir.\narXiv:1902.01449v1  [stat.ML]  4 Feb 2019\nGENERALIZATION BOUNDS FOR AUTOENCODERS\none to reduce dimension while preserving the input structure. More formally, we show that if the\ninput distribution satisﬁes a certain clustering assumption, then the encoder part of an autoencoder\nwith a small generalization error maps most of the input to a low-dimensional distribution that itself\nsatisﬁes a reasonably-good clustering assumption. Finally, we extend Singh et al. (2008) by showing\nconditions under which any supervised learner can beneﬁt from AE-enabled SSL.\nThe remainder of the paper is organized as follows. In Section 2 we review some prior work\non AEs and SSL. In 3, we survey some recent generalization bounds for supervised learning with\ndeep networks, and the characterization by Singh et al. (2008) of conditions under which SSL is\nguaranteed to be beneﬁcial. In Section 4, we introduce our proposed reconstruction loss and use it\nto obtain generalization bounds for AEs. In Section 5, we apply these bounds to show that if an AE\ngeneralizes well, its encoder is limited in its ability to shrink the distances between most input pairs\nand discuss what this implies for semi-supervised learning and Singh et al. (2008). In Section 6,\nwe explore our bounds empirically. Finally, we discuss possible implications of this work and some\nfuture research directions.\nThe main contributions of the present work are the following. (i) We adapt recent margin-based\ngeneralization bounds for feedforward networks to autoencoders via a novel loss. (ii) We tie good\nAE reconstruction performance to a non-contractiveness property of the encoder component. (iii)\nWe show that this implies the ability to trade off separation margins between input clusters for\nreduced dimension, which is beneﬁcial for semi-supervised learning.\n2. Related Work\nA great deal of work has been devoted to dimensionality reduction since the introduction of (linear)\nprincipal component analysis (PCA) in 1901 by Karl Pearson. Nonlinear manifold-based meth-\nods introduced in Roweis and Saul (2000); Tenenbaum et al. (2000), were followed by work on\nAEs Hinton and Salakhutdinov (2006) that led to signiﬁcantly improved results for practical prob-\nlems. Later work by Vincent et al. (2010), introduced a de-noising based criterion for training AEs,\nand demonstrated its improved representation quality compared to a reconstruction based criterion,\ncontributing to better classiﬁcation performance on subsequent supervised learning tasks. Further\ndetails, and a survey of AEs, can be found in Bengio et al. (2013).\nSubsequent work directly addressed the SSL setting. Several papers demonstrated the empirical\nutility of SSL Rifai et al. (2011); Ranzato and Szummer (2008); Rasmus et al. (2015); Weston et al.\n(2012). Within the related transfer learning setting, Zhuang et al. (2015) showed how to improve\nlearning performance by combining two types of encoders, the ﬁrst, based on an unsupervised em-\nbedding from the source and target domains, and the second, based on the labels available from the\nsource data. Ghifary et al. (2016), suggested a joint encoder for both classiﬁcation of labeled data,\nand reconstruction of unlabeled data, thereby maintaining both types of information, and enhancing\nperformance in the face of scarce labels. Bousmalis et al. (2016) and Epstein et al. (2018) use AEs\nfor SSL and semi-supervised transfer learning, by explicitly learning to separate representations into\nprivate and shared components.\nWithin the framework of statistical learning theory, several recent papers have signiﬁcantly im-\nproved previous generalization bounds for deep networks by incorporating more reﬁned attributes\nof the network structure, aiming to explain the paradoxical effect of improved performance while\nover-training the network. Using covering number techniques, Bartlett et al. (2017) provide margin\nbased bounds that relate generalization error to the network’s Lipschitz constant and matrix norms\nof the weights. Neyshabur et al. (2018) establish similar matrix-norm-based margin bounds using\n2\nGENERALIZATION BOUNDS FOR AUTOENCODERS\na PAC-Bayes approach. Arora et al. (2018) present compression-based results by compressing the\nweights of a well performing network and bounding the error of the compressed network. Finally,\nGolowich et al. (2018) are able, under certain (restrictive) assumptions on matrix norms, to achieve\ngeneralization bounds that are completely independent of the network size.\nThe value of SSL has been subject to much debate. Rigollet (2007) provide a mathematical\nframework for the intuitive cluster assumption of Seeger (2000), and show that for unlabeled data to\nbe beneﬁcial, some clustering criterion is required (speciﬁcally, that the data consists of separated\nclusters with identical labels within each cluster). Based on a density level set approach, they\nprove fast rates of convergence in the SSL setting. Lafferty and Wasserman (2007) and Niyogi\n(2008)) study SSL within a minimax framework, the former work shows that, under the so-called\nmanifold assumption, optimal minimax rates of convergence may be achieved, while the second\nwork demonstrates a separation between two classes of problems. When the structure of the data\nmanifold is known, fast rates can be achieved, while without such knowledge, convergence cannot\nbe guaranteed. More directly related to our work, and building on the clustering assumption, Singh\net al. (2008) identify situations in which semi-supervised learning can improve upon supervised\nlearning. Unfortunately, their SSL bound suffers from the curse of dimensionality, and so depends\nexponentially on the dimension. Our work can be seen as allowing a trade-off between the clustering\nseparation and the dimension, suggesting how to improve the bounds in Singh et al. (2008).\nvan Rooyen and Williamson (2015) provide a principled approach to feature representation,\nand characterize the relation between the information retained by features about the input, and the\nloss incurred by a classiﬁer based on these features. They suggest the application of their results\nto SSL, but do not provide explicit conditions or generalization bounds in this setting. Recently,\nLe et al. (2018) provide such bounds for semi-supervised learning with linear AEs and a joint\nreconstruction/classiﬁcation loss, using uniform stability arguments. They also provide empirical\nresults that show that nonlinear AEs can indeed contribute to supervised learning. While their\nresults are close in spirit to ours, we are more concerned with understanding how the structure of\nthe data affects SSL, and, in particular, characterizing when, and to what extent, clustering of the\ninput contributes to performance through unsupervised learning. Moreover, we rely on bounds that\nare speciﬁc to neural networks, rather than on the looser stability based bounds.\n3. Background\n3.1. Generalization for Feed-forward Networks\nLet X be an input space, Y an output space, and DX,Y a distribution on X × Y. Throughout\nthe paper, we shall assume that X ⊂RN and that ∥x∥≤B, ∀x ∈X. Denote by D the marginal\ndistribution on the inputs. The j-th entry of a vector v is denoted by v [j]. For a collection of matrices\nw = {Wi}d\ni=1, denote by fw (x) the d-layer feedforward network Wdφ (Wd−1 (φ...φ (W1x))),\nwhere φ is a non-linearity. We shall focus on the ReLU function φ(x) = max(0, x). Let us recall\na few recent results (Bartlett et al. (2017), Neyshabur et al. (2018), Arora et al. (2018)) concerning\nsupervised K-way classiﬁcation with feedforward networks. The output of a network fw is a vector\nv ∈RK. For a pair (x, y), deﬁne the (supervised) γ-margin loss as\nls\nγ (fw(x), y) ≜\n(\n0\nfw (x) [y] −γ ≥maxj̸=y fw (x) [j]\n1\no.w.\n.\n(1)\n3\nGENERALIZATION BOUNDS FOR AUTOENCODERS\nThat is, the loss is only 0 if the correct, y-th output entry fw (x) [y] is not only the largest one - but\nthe second-largest entry is at least γ away. Given a sample of size m, Denote by ˆLs\nγ and Ls\nγ the\ncorresponding empirical and expected function losses\nˆLs\nγ (fw) ≜1\nm\nm\nX\ni=1\nls\nγ (fw(xi), yi) ,\nLs\nγ (fw) ≜E(x,y)∼DX,Yls\nγ (fw(x), y).\n(2)\nNote that Ls\n0 is the standard 0 −1 classiﬁcation loss.\nAll three aforementioned papers can be considered to suggest the same type of claim,\nLs\n0 (f) ≤ˆLs\nγ (f) + ∆(f, m, δ, γ) , w.p. ≥1 −δ,\n(3)\nwhere ∆(f, m, δ, γ) is a generalization term depending on the network parameters, failure proba-\nbility δ, sample size m and margin γ. We shall use the bound appearing in Neyshabur et al. (2018),\nas it is the simplest to state1, but the similar results appearing in the other two papers can be adapted\nfor our purpose as well.\nProposition 1 (Neyshabur et al. (2018)) For any 0 < δ < 1, 0 < γ, with probability at least 1 −δ\nover a training set of size m, for any fw of depth d and a constant C (B, fw) 2 depending only on\nthe maximal input norm B and on the structure of fw, we have\nLs\n0 (fw) ≤ˆLs\nγ (fw) + O\n\n\ns\nC (B, fw) + ln dm\nδ\nγ2m\n\n.\n(4)\n3.2. Semi-Supervised Learning - Now It Helps Now It Doesn’t\nWe brieﬂy review the necessary background from Singh et al. (2008), stating the results and ter-\nminology in a somewhat simpliﬁed manner. First, we deﬁne the clustering assumption they are\nworking under. Suppose the input distribution D is a ﬁnite mixture of smooth component densi-\nties {Dk}K\nk=1 with disjoint supports. Suppose further that each Dk is bounded away from zero and\nsupported on a unique compact and connected set Ck ⊂X with smooth boundaries:\nCk =\nn\nx ≡(x1, ..., xd) : g(1)\nk\n(x1, ..., xd−1) ≤xd ≤g(2)\nk\n(x1, ..., xd−1)\no\n,\n(5)\nwhere g(1)\nk , g(2)\nk\nare (d −1)-dimensional Lipschitz functions. Finally, assume the target label is\nconstant on each Ck3. Then we say D satisﬁes the clustering assumption with cluster-margin η4 if\neach two clusters Cj, Ck are at least η apart. More formally, for j, k ∈{1, .., K}, let\ndjk =\n\n\n\n\n\n\n\n\n\nmin\np,q∈{1,2}\n\r\r\rg(p)\nj\n−g(q)\nk\n\r\r\r\n∞\nj ̸= k\n\r\r\rg(1)\nk\n−g(2)\nk\n\r\r\r\n∞\nj = k\n.\n(6)\n1. The bound in Bartlett et al. (2017) is strictly tighter and allows for non-linearities other than ReLU, however.\n2. See Sec. 4.1 for a more detailed bound statement.\n3. Inputs with equal labels need not form a single cluster. Indeed, typically they form a number of separate clusters.\n4. The notation in the original is γ. We have changed it to avoid confusion with the γ-margin loss.\n4\nGENERALIZATION BOUNDS FOR AUTOENCODERS\nThen the cluster-margin η is simply min\nj,k djk. The support sets of the components Ck in D are called\nthe decision sets of D. Denote by H the set of all hypotheses h : X →Y.\nDeﬁnition 2 A clairvoyant supervised learner AD,n : (X ×Y)n →H is a function mapping labeled\ntraining sets of size n to hypotheses in H, with perfect knowledge of the decision sets of D. A semi-\nsupervised learner Am,n : X m × (X × Y)n →H is a function mapping unlabeled training sets of\nsize m and labeled training sets of size n to hypotheses in H.\nThe following theorem (a slightly weaker version of Corollary 1 in Singh et al. (2008)) asserts that\nunder suitable conditions, semi-supervised learning can always perform as well as any clairvoyant\nlearner.\nProposition 3 (Singh et al. (2008)) Let D satisfy the clustering assumption with cluster-margin η.\nAssume L is a bounded loss. Denote by E(A) = L(A) −L∗the excess loss of a learner A, where\nL∗is the inﬁmum loss over all possible learners. Suppose there exists a clairvoyant learner AD,n\nfor which\nE[E(AD,n)] ≤ε(n).\n(7)\nThen there exists a semi-supervised learner Am,n such that if η > C0((log m)2/m)1/N, then\nE[E(Am,n)] ≤ε(n) + O\n \n1\nm + n\n\u0012(log m)2\nm\n\u00131/N!\n.\n(8)\nThe constant C0 does not depend on η, m or n.\nRemark 4 Note the exponential dependence on the input dimension N in Prop. 3. Mapping the\ninput to a signiﬁcantly lower dimension without decreasing η too much is beneﬁcial for the bound.\n4. Generalization Bounds for Autoencoders\nLet us now turn to autoencoders and their generalization properties. We introduce a novel entry-\nwise γ-margin reconstruction loss and state a generalization bound for this loss. Furthermore, we\nshow that such a bound implies a bound for the standard L2 loss as well.\nFor simplicity, we consider X ∈{0, 1}M.5 We consider feed-forward fully-connected networks\nwith output entries in [0, 1].\n6 Given a sample x and a network fw, the reconstructed output is\nˆx = fw (x), though we will sometimes abuse the notation and simply write f (x) or f. Note\nthat while the inputs are binary, the prediction for each entry can be an intermediate value. An\nautoencoder network f is a composition of an encoder enc and a decoder dec, both fully-connected\nfeedforward networks.\n5. All the deﬁnitions and results from here on can be extended straightforwardly to support ﬁner input resolution, that\nis, to allow input values on a discrete grid {0, 1/s, 2/s, ..., (s −1)/s, 1} for some integer s. The forms of the bound\nin Prop. 6 and Thm. 9 do not change with s, though the γ-margin loss of any given autoencoder might. The 1/2\nvalue in the deﬁnition of R(r, γ) in Sec. 5 is replaced by s/2.\n6. The restriction of the output to [0, 1] can be achieved by applying a sigmoid to the output, with the beneﬁcial side\neffect of dividing the network Lipschitz constant by 4, as the Lipschitz constant of the sigmoid function is 1/4.\n5\nGENERALIZATION BOUNDS FOR AUTOENCODERS\nDeﬁnition 5 For a margin γ < 1\n2, we deﬁne the γ-margin loss to be the average amount of entries\nthat were not reconstructed with a conﬁdence of at least γ. That is,\nlγ (x, ˆx) := 1\nM\nM\nX\nj=1\n1\n\u0012\n|x [j] −ˆx [j]| > 1\n2 −γ\n\u0013\n,\n(9)\nwhere 1 is the indicator function.\nNote that the loss is bounded between 0 and 1. The corresponding expected loss and empirical\nloss on m samples are denoted Lγ and ˆLγ, respectively:\nˆLγ (fw) := 1\nm\nm\nX\ni=1\nlγ (xi, fw(xi))\nLγ (fw) := Ex∼Dlγ (x, fw(x)).\n(10)\nWe can adapt Prop. 1 in Sec. 3.1 for autoencoders and the losses deﬁned in Eq. 10.\nTheorem 6 For any positive 0 < δ < 1, 0 < γ1 < γ2 < 1/2, with probability at least 1 −δ over\na training set of size m, for any fw of depth d and a constant C (B, fw) depending only on the\nmaximal input norm B and on the structure of fw, we have\nLγ1 (fw) ≤ˆLγ2 (fw) + O\n\n\ns\nC (B, fw) + ln dm\nδ\n(γ2 −γ1)2 m\n\n.\n(11)\nThe proof is relegated to Sec. 4.1. See Fig. 2(a) for empirical corroboration of the bound above.\nA common measure of the reconstruction performance of an AE is the squared-error loss\nlSE (x, ˆx) = ∥x −ˆx∥2\n2 =\nM\nX\nj=1\n(x [j] −ˆx [j])2 .\n(12)\nWe would like to be able to bound the generalization error in terms of this loss as well. Fortunately,\nwe are able to bound lSE by a function of lγ. Let\nR (r, γ) ≜rM + (1/2 −γ)2 (1 −r) M = rM\n\u0010\n1 −(1/2 −γ)2\u0011\n+ (1/2 −γ)2 M.\n(13)\nLemma 7 Let x be an input and ˆx its reconstruction. Suppose that lγ (x, ˆx) is at most r. Then\nlSE (x, ˆx) is at most R(r, γ).\nIndeed, at most rM entries are reconstructed with accuracy less than 1/2 −γ. They contribute at\nmost 1 · rM to lSE. The remaining (1 −r) · M entries contribute at most (1/2 −γ)2 (1 −r) M to\nlSE, for a total loss at most rM + (1/2 −γ)2 (1 −r) M.\nCorollary 8 By linearity of expectation, an expected γ-margin loss Lγ (f) ≤r implies a squared-\nerror loss at most R (r, γ). Similarly, by the Jensen inequality, the expected L2 error\nµ (fw) ≜E [∥f (x) −x∥2]\n(14)\nis bounded from above by\np\nR (r, γ).\n6\nGENERALIZATION BOUNDS FOR AUTOENCODERS\nSuppose further that the reconstruction errors of the entries are distributed symmetrically around\nthe average of the possible values. That is, that the distance from the corresponding input is, on\naverage, ((1/2−γ)2+12)/2 for the rM entries with poor reconstruction, and (1/2−γ)2/2 for the (1−r)M\nremaining entries. Then\nµ (fw) ≤\ns\u0000 1\n2 −γ\n\u00012 + 12\n2\nrM +\n\u0000 1\n2 −γ\n\u00012\n2\n(1 −r) M.\n(15)\nThe empirical results in Fig. 2(b) suggest that this symmetric error assumption is reasonable.\nSubstituting the generalization bound from Thm. 6 into Corollary 8, we obtain the following\nbound for µ (fw).\nTheorem 9 For any positive 0 < δ < 1, 0 < γ1 < γ2 < 1\n2, with probability at least 1 −δ over a\ntraining set of size m, for any fw of depth d and network-related constant C (fw) independent of\nm, we have\nµ (fw) ≤\nv\nu\nu\nu\ntR\n\nˆLγ2 (fw) + O\n\n\ns\nC (fw) + ln dm\nδ\n(γ2 −γ1)2 m\n\n, γ1\n\n.\n(16)\n4.1. Proof of Theorem. 6\nWe follow the strategy appearing in Neyshabur et al. (2018).\nFirst, let us state the result in greater detail. Let fw be an autoencoder with weights w =\n{Wi}d\ni=1 and ReLU non-linearities. Let\n∥W∥2 ≜sup\nx̸=0\n∥Wx∥2\n∥x∥2\n,\n∥W∥F ≜\nv\nu\nu\nt\np\nX\ni=1\nq\nX\nj=1\n|wij|2\n(17)\nbe the spectral and Frobenius norms of a (p×q)-dimensional matrix W. Let B be the maximum L2\nnorm of an input, d the depth of f, h the upper bound on the number of output units in each layer.\nTheorem 10 (Detailed version of Thm. 6) For any B, d, h > 0 and any 0 < δ < 1, 0 < γ1 < γ2 <\n1/2, with probability at least 1 −δ over a training set of size m, for any w, we have\nLγ1 (fw) ≤ˆLγ2 (fw) + O\nv\nu\nu\ntB2d2h ln (dh) Πd\ni=1∥Wi∥2\n2\nPd\ni=1\n∥Wi∥2\nF\n∥Wi∥2\n2 + ln dm\nδ\n(γ2 −γ1)2m\n.\n(18)\nThe proof consists of three steps. Firstly, we show that a small perturbation of the weight matrices\nimplies a small perturbation of the network output (Lemma 11, Lemma 2 in Neyshabur et al. (2018)).\n7\nGENERALIZATION BOUNDS FOR AUTOENCODERS\nSecondly, for perturbations u of the network parameters such that the network output does not\nchange much relative to γ2 −γ1, we state a PAC-Bayesian bound controlling Lγ1(f) −ˆLγ2(f) by\nmeans of w + u (Lemma 12, analogous to Lemma 1 in Neyshabur et al. (2018)). Finally, we use\nLemma 11 to calculate the maximal amount of perturbation that satisﬁes the conditions of Lemma\n12. This level of perturbation, substituted into the PAC-Bayesian bound, yields the theorem.\nLemma 11 (Perturbation bound) Let u = {Ui}d\ni=1 be a perturbation such that ∥Ui∥2 ≤1\nd ∥Wi∥2.\nThen for any input x,\n|fw+u (x) −fw (x)|2 ≤eB\n d\nY\ni=1\n∥Wi∥2\n!\nd\nX\ni=1\n∥Ui∥2\n∥Wi∥2\n.\n(19)\nRecall that the Kullback-Leibler divergence between two distributions P and Q is\nKL (Q ∥P) ≜EQ\n\u0014\nln Q\nP\n\u0015\n.\n(20)\nLemma 12 (PAC-Bayesian bound) Let fw : X →X be an autoencoder, P a data-independent\ndistribution on the parameters. Then for any 0 < γ1 < γ2 < 1/2, 0 < δ < 1, w.p. at least 1 −δ,\nfor any random perturbation u s.t. Pu\n\u0014\nmax\nx∈X |fw+u (x) −fw (x)|∞< (γ2−γ1)\n4\n\u0015\n≥1/2, we have\nLγ1 (fw) ≤ˆLγ2 (fw) + 4\ns\nKL (w + u ∥P) + ln 6m\nδ\nm −1\n.\n(21)\nLet β =\n\u0010Qd\ni=1 ∥Wi∥2\n\u0011 1\nd . Consider the weights ˜Wi =\nβ\n∥Wi∥2 Wi. By the homogeneity of ReLU,\nf ˜w = fw. Also, Qd\ni=1 ∥Wi∥2 = Qd\ni=1\n\r\r\r ˜Wi\n\r\r\r\n2 and ∥Wi∥F\n∥Wi∥2 = ∥˜\nWi∥F\n∥˜\nWi∥2\n. We can, therefore, assume\nthat all weights are normalized and ∥Wi∥2 = β for all i.\nConsider u ∼N(0, σ2) and a prior distribution P of the same form. By Thm. 4.1 in Tropp\n(2012), with probability at least 1/2,\n∥Ui∥2 ≤σ\np\n2h ln(4dh).\n(22)\nBy Lemma 11, for an appropriate σ,\nmax |fw+u (x) −fw (x)| ≤eBβd X\ni\n∥Ui∥2\nβ\n≤eBβd−1σ\np\n2h ln (4dh)\n≤(γ2 −γ1)\n4\n.\n(23)\nFor such a σ, u satisﬁes the condition of Lemma 12. We can now bound the KL term in the\nPAC-Bayesian bound for the chosen P and u7,\nKL (w+u ∥P) ≤∥w∥2\n2σ2\n≤O\nv\nu\nu\ntB2d2h ln (dh) Πd\ni=1∥Wi∥2\n2\nPd\ni=1\n∥Wi∥2\nF\n∥Wi∥2\n2\n(γ2 −γ1)2\n.\n(24)\n7. We have skipped over a nuance necessary to ensure that the prior P is data-independent. See the end of the proof of\nTheorem 1 in Neyshabur et al. (2018) for the details\n8\nGENERALIZATION BOUNDS FOR AUTOENCODERS\nSubstituting Eq. 24 into Eq. 21 completes the proof.\nRemark 13 Note that the upper bound on ∥Ui∥2 given in Eq. 22 depends on the dimensions of Ui.\nThm. 10 and its proof simply use h, the largest output unit number of any layer. Assuming that the\nlayer sizes decrease exponentially approaching the bottleneck (see, e.g., Hinton and Salakhutdinov\n(2006)), there is some room for tightening the bound.\n5. Autoencoders and Semi-Supervised Learning\nIn this section we show that, under appropriate assumptions, a sufﬁciently good autoencoder can\ncontribute to the advantage of SSL over any supervised learning scheme. Speciﬁcally, we consider\nthe following strategy - ﬁrst training the AE on the unlabeled data, and then applying the bound in\nProp. 3 to the code, that is, to the output of the encoder (see Fig. 1). We stress that we do not propose\nthis strategy as an optimal empirical approach. Indeed, training to minimize both reconstruction and\nsupervised losses simultaneously has been established as a more successful approach, in practice\n(e.g., Bousmalis et al. (2016), Epstein et al. (2018).) However, the scheme we are considering\nallows for a theoretical treatment and for an explanation of the relationship between the autoencoder\nperformance and its contribution to semi-supervised learning.\nFigure 1: An autoencoder with a semi-supervised learner applied to the encoder output. The Lip-\nschitz constant of dec is denoted by C. The input distribution satisﬁes the clustering assumption\nwith cluster-margin η. The encoder distorts the input distribution, but, for a sufﬁciently good au-\ntoencoder, the distribution at the AE bottleneck still satisﬁes the clustering assumption with cluster-\nmargin η′ > 0.\nWe need some further notation, in order to state our main result in this section. Denote by\nGϵ (fw) ⊂X 8 the subset {x : ∥fw (x) −x∥2 −µ (fw) < ϵ}, that is, the inputs for which the re-\nconstruction error deviates from µ (fw) by at most ϵ. Note that by the Markov inequality, for x ∈X,\nP (∥fw (x) −x∥2 −µ (fw) > ϵ) = P (x /∈Gϵ (fw)) ≤µ (fw)\nϵ\n,\n(25)\nor in other words, the measure of Gϵ (fw) is at least 1 −µ (fw) /ϵ. Note that this allows us to\ntrade off the measure of Gϵ for the tightness of ϵ (see Fig. 2(c)). Observe, too, that by Thm. 9,\n8. We will occasionally omit f or fw and simply write Gϵ.\n9\nGENERALIZATION BOUNDS FOR AUTOENCODERS\nµ →0 as m →∞, ˆLγ2 →0 and γ1 →γ2. Thus, as the generalization error of f vanishes, so does\nthe set of “bad” inputs. Denote by DGϵ the distribution induced by D on Gϵ, and by Denc(Gϵ) the\ncorresponding distribution on enc(Gϵ).\nTheorem 14\nAssume that the input distribution D satisﬁes the clustering assumption with mar-\ngin η. Let f be an autoencoder with expected L2 reconstruction loss µ(f), bottleneck dimension\nNb < N and decoder Lipschitz constant C9. Then for any ϵ > 0, Denc(Gϵ) satisﬁes the clustering\nassumption with cluster-margin at least\nη′ = (η−2(µ(f)+ϵ))/C.\n(26)\nFurthermore, suppose there exists a clairvoyant learner ADenc(Gϵ),n for which\nE[E(ADenc(Gϵ),n)] ≤ε(n).\n(27)\nThen there exists a semi-supervised learner Am,n such that if η′ > C0((log m)2/m)1/Nb, then\nE[E(Am,n)] ≤ε(n) + O\n \n1\nm + n\n\u0012(log m)2\nm\n\u00131/Nb!\n.\n(28)\nThe constant C0 does not depend on η, m or n.\nBefore proving the theorem, we observe that whenever η/η′ < log(N −Nb), both the condition on\nm and the bound in Eq. 28 are improved relative to Prop. 3.\nNow, consider two input points x, y ∈Gϵ (f). Applying a standard “3 epsilon” argument,\n∥x −y∥2 ≤∥f (x) −x∥2 + ∥f (x) −f (y)∥2 + ∥f (y) −y∥2 ,\n≤∥f (x) −f (y)∥2 + 2 (µ (f) + ϵ) .\n(29)\nIn particular, for x, y at least η apart, ∥f (x) −f (y)∥2 is at least η −2 (µ(f) + ϵ).\nWe have established that if an autoencoder generalizes well, it does not bring two input points\nin Gϵ (f) too close together. Recall that C is the Lipschitz constant of the decoder. If, for any\npoints x, y, ∥f (x) −f (y)∥2 is at least some d, then ∥enc (x) −enc (y)∥2 cannot be less than d/C.\nThis implies that enc maps clusters at least η apart to clusters at least η′ = η−2(µ(f)+ϵ)/C apart. In\nother words, if D, the input distribution, satisﬁes the clustering assumption with margin η, then its\nrestriction DGϵ does as well, and Denc(Gϵ), the output distribution of enc, satisﬁes the clustering\nassumption with cluster-margin η′. Applying Prop. 3 to the Denc(Gϵ) completes the proof.\n6. Experiments\nAll experiments were implemented in Keras (Chollet (2015)) over Tensorﬂow (Abadi et al. (2015)).\nWe use two digit image datasets for our experiments. The MNIST dataset (LeCun et al. (1998)) is a\ncollection of 70000 grayscale 28 × 28 images of hand-written digits, split into 60,000 training and\n9. The decoder is a Lipschitz function. Indeed, C is at most the product of the spectral norms of the weight matrices in\ndec, though that is typically a very loose bound. See Arora et al. (2018) for a discussion of the behavior of C.\n10\nGENERALIZATION BOUNDS FOR AUTOENCODERS\n10,000 test samples. The SVHN dataset (Netzer et al. (2011)) is a collection of 99289 32 × 32 × 3\nRGB images of hand-written digits, split into 73,257 training and 26,032 test samples. We have the\nconverted the SVHN samples into grayscale. First, we provide evidence for the generalization bound\nin Thm. 6. For each dataset, we train an autoencoder on an increasing fraction of the training set,\nand plot the bound (divided by the constant C(B, fW ) vs. the empirically observed test error (Fig\n2(a)). The margin values we use are γ1 = 0.45, γ2 = 0.49. We see that, for both datasets, the bound\ncorrelates well with the test error. Moreover, the plot trends suggest an asymptotic convergence of\nthe bound to the test error.\nNext, we examine the control over µ(f) as a function of lγ that Corollary 8 provides. Fig. 2(b)\nplots the empirical average L2 reconstruction error over the test set vs the predicted bound. The\nworst-case bound\np\nR(lγ, γ) correlates well with the empirical L2 error, but it is overly pessimistic\nby a factor of approximately 3. The average-case bound in Eq. 15 is closer to the observed error,\nloose only by a factor of approximately 2.\nThe proof of Thm. 14 requires a restriction to Gϵ, the set of samples with small reconstruction\nerror. A reasonable concern is that such a restriction rejects a large fraction of the inputs. While Eq.\n25 provides some guarantees on the size of Gϵ for negligible µ(f)-s, Fig. 2(c) shows that, already\nfor ϵ values small relative to µ(f), most test samples are in Gϵ.\nFinally, in Table. 1 we examine the various quantities appearing in Thm. 14. The ﬁrst and\nsecond rows use a small and a large autoencoder, respectively, trained on MNIST. The third row uses\nan autoencoder trained on SVHN. The ﬁrst column, N →Nb, describes the change in dimensions\nfrom the AE input to the bottleneck. The second column, η, gives the estimated cluster-margin\nof the input distribution. η′ is the estimated cluster-margin of the bottleneck distribution. C is\nthe estimated decoder Lipschitz constant. Finally, the ﬁfth column gives the extent to which the\nlog(m)2/m\n1/Nb term in Eq. 28 improves due to the dimension reduction, for the corresponding value\nof m. We can see that the change in the cluster-margin is roughly inverse to C (though, for the given\ntraining sets, µ(f) was not small enough for Eq. 26 to yield positive values of η′).\n(a)\n(b)\n(c)\nFigure 2: (a) The (scaled-down) generalization bound in Thm. 6 correlates well with the empirical\ntest error as the sample size increases from 10% of the training set to 100%. (b) The bound for µ(f)\nas a function of Lγ given in Corollary 8 correlates well with the empirical L2 error as the the sample\nsize increases from 10% of the training set to 100%. The worst-case bound is overly pessimistic,\nbut the bound in Eq. 15 derived under the symmetric-error assumption is much closer to reality. (c)\nGϵ, the subset of samples with L2 reconstruction error at most µ(f) + ϵ, rapidly becomes almost all\nof the test set as ϵ/µ(f) increases to 1.\n11\nGENERALIZATION BOUNDS FOR AUTOENCODERS\nTable 1: Autoencoder Lipschitz Constant, Cluster-Margin and SSL Bound\nN →Nb\nη\nη′\nC\nBound improvement\nMNIST\n784 →30\n3.99\n1.60\n3.39\n1.22\nMNIST (large network)\n784 →50\n3.99\n3.41\n1.86\n1.12\nSVHN\n1024 →50\n1.58\n0.12\n24.93\n1.23\nN →Nb denotes the dimension reduction the AE encoder performs. η is the empirical input cluster-\nmargin. η′ is the empirical cluster-margin at the AE code. C is the estimated Lipschitz constant of\nthe decoder. Bound improvement refers to the (multiplicative) improvement in the log(m)2/m\n1/Nb\nterm in Eq. 28. The decrease in η′ is roughly inverse to C, as predicted by Eq. 26.\n7. Conclusion\nWe have adapted existing generalization bounds for feedforward networks, together with a novel\nreconstruction loss, to obtain a generalization bound for autoencoders. To the best of our knowledge,\nthis is the ﬁrst such bound. We went on to tie the good reconstruction performance of an autoencoder\nto a non-contractiveness property of the encoder component. This property, in turn, implies the\nability to trade off cluster-margins between input clusters for reduced dimension, which is beneﬁcial\nfor semi-supervised learning. Empirical evidence supports our theoretical results.\nThe bound we have obtained concerns only the gap between the empirical and expected losses.\nIt neither guarantees the existence of an autoencoder achieving a negligible empirical error nor\nexplains why such networks seem to exist in practice, particularly for images. We believe that\nthe answer has to do with the properties of natural images - that typical image datasets satisfy a\nmanifold hypothesis. That is, they lie on, or near, a low-dimensional manifold that is mapped to\na higher dimension where they are observed. Assuming the mapping is invertible, and both the\nmapping and its inverse can be approximated well by sufﬁciently expressive networks, this does\nimply the existence of a good autoencoder for the dataset. Such considerations lead us to believe\nthat a good generative model for the data (possibly along the lines of Ho et al. (2018)) could shed\nfurther light on unsupervised and semi-supervised learning with autoencoders.\nAn interesting and worthwhile extension of our work would be to consider more practical ap-\nproaches to SSL. Speciﬁcally, combining supervised and unsupervised losses through shared layers,\nas is often done in practice. Such approaches have been shown to be effective both in SSL and trans-\nfer learning, and the present approach could shed theoretical light on their success.\nAcknowledgments\nWe thank Ron Amit for numerous useful suggestions and corrections.\n12\nGENERALIZATION BOUNDS FOR AUTOENCODERS\nReferences\nMart´ın Abadi et al. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015.\nURL http://tensorflow.org/. Software available from tensorﬂow.org.\nSanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for\ndeep nets via a compression approach. CoRR abs/1802.05296, 2018.\nPeter L. Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for\nneural networks. Advances in Neural Information Processing Systems, 6240-6249, 2017.\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new\nperspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798–1828,\n2013.\nKonstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Er-\nhan. Domain separation networks. Advances in Neural Information Processing Systems 29 (NIPS\n2016), 2016.\nFranois Chollet. keras. https://github.com/fchollet/keras, 2015.\nBaruch Epstein, Ron Meir, and Tomer Michaeli. Joint autoencoders: a ﬂexible meta-learning frame-\nwork. ECML 2018, 2018.\nMuhammad Ghifary, W. Bastiaan Kleijn, Mengjie Zhang, David Balduzzi, and Wen Li.\nDeep\nreconstruction-classiﬁcation networks for unsupervised domain adaptation. In ECCV, pages 597–\n613. Springer, 2016.\nNoah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of\nneural networks. COLT 2018, 2018.\nG. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks.\nScience vol. 313 no. 5786 pp. 504-507 2006, 2006.\nNhat Ho, Tan Nguyen, Ankit Patel, Anima Anandkumar, Michael I. Jordan, and Richard G. Bara-\nniuk. Neural rendering model: Joint generation and prediction for semi-supervised learning. Corr\nabs/1811.02657, 2018.\nJ. Lafferty and L. Wasserman. Statistical analysis of semi-supervised regression. NIPS 2007, 2007.\nLei Le, Andrew Patterson, and Martha White. Supervised autoencoders: Improving generalization\nperformance with unsupervised regularizers. NIPS 2018, 2018.\nY. LeCun et al. Gradient-based learning applied to document recognition. Proceedings of the IEEE,\n86(11):2278-2324, 1998.\nYuval Netzer et al. Reading digits in natural images with unsupervised feature learning. NIPS\nWorkshop on Deep Learning and Unsupervised Feature Learning, 2011.\nBenham Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro.\nA pac-bayesian approach to\nspectrally-normalized margin bounds for neural networks. ICLR 2018, 2018.\n13\nGENERALIZATION BOUNDS FOR AUTOENCODERS\nP. Niyogi. Manifold regularization and semi-supervised learning: Some theoretical analyses. Tech-\nnical Report TR-2008-01, Computer Science Department, University of Chicago, 2008.\nMarc’Aurelio Ranzato and Martin Szummer. Semi-supervised learning of compact document rep-\nresentations with deep networks. In Proceedings of the 25th international conference on Machine\nlearning, pages 792–799. ACM, 2008.\nAntti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko.\nSemi-\nsupervised learning with ladder networks. In Advances in Neural Information Processing Sys-\ntems, pages 3546–3554, 2015.\nSalah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive auto-\nencoders: Explicit invariance during feature extraction. ICML 2011, 2011.\nP. Rigollet. Generalization error bounds in semi-supervised classiﬁcation under the cluster assump-\ntion. JMLR 2007 13691392, 2007.\nSam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embed-\nding. science, 290(5500):2323–2326, 2000.\nM. Seeger. Learning with labeled and unlabeled data. Technical report, Institute for ANC, Edin-\nburgh, UK, 2000.\nAarti Singh, Robert D. Nowak, and Xiaojin Zhu. Unlabeled data: Now it helps, now it doesnt. NIPS\n2008, 2008.\nJoshua B Tenenbaum, Vin De Silva, and John C Langford. A global geometric framework for\nnonlinear dimensionality reduction. science, 290(5500):2319–2323, 2000.\nJoel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational\nmathematics 2012, 389-434, 2012.\nBrendan van Rooyen and Robert C. Williamson. A theory of feature learning. Corr abs/1504.00083,\n2015.\nPascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol.\nStacked denoising autoencoders: Learning useful representations in a deep network with a local\ndenoising criterion. JMLR 2010, 2010.\nJason Weston, Fr´ed´eric Ratle, Hossein Mobahi, and Ronan Collobert. Deep learning via semi-\nsupervised embedding. In Neural Networks: Tricks of the Trade, pages 639–655. Springer, 2012.\nFuzhen Zhuang, Xiaohu Cheng, Ping Luo, Sinno Jialin Pan, and Qing H. Supervised representation\nlearning: Transfer learning with deep autoencoders. IJCAI 2015, 2015.\n14\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2019-02-04",
  "updated": "2019-02-04"
}