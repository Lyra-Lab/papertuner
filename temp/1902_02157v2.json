{
  "id": "http://arxiv.org/abs/1902.02157v2",
  "title": "When does reinforcement learning stand out in quantum control? A comparative study on state preparation",
  "authors": [
    "Xiao-Ming Zhang",
    "Zezhu Wei",
    "Raza Asad",
    "Xu-Chen Yang",
    "Xin Wang"
  ],
  "abstract": "Reinforcement learning has been widely used in many problems, including\nquantum control of qubits. However, such problems can, at the same time, be\nsolved by traditional, non-machine-learning methods, such as stochastic\ngradient descent and Krotov algorithms, and it remains unclear which one is\nmost suitable when the control has specific constraints. In this work, we\nperform a comparative study on the efficacy of three reinforcement learning\nalgorithms: tabular Q-learning, deep Q-learning, and policy gradient, as well\nas two non-machine-learning methods: stochastic gradient descent and Krotov\nalgorithms, in the problem of preparing a desired quantum state. We found that\noverall, the deep Q-learning and policy gradient algorithms outperform others\nwhen the problem is discretized, e.g. allowing discrete values of control, and\nwhen the problem scales up. The reinforcement learning algorithms can also\nadaptively reduce the complexity of the control sequences, shortening the\noperation time and improving the fidelity. Our comparison provides insights\ninto the suitability of reinforcement learning in quantum control problems.",
  "text": "When does reinforcement learning stand out in quantum control? A comparative\nstudy on state preparation\nXiao-Ming Zhang,1, 2 Zezhu Wei,1 Raza Asad,3 Xu-Chen Yang,4 and Xin Wang1, 2\n1Department of Physics, City University of Hong Kong,\nTat Chee Avenue, Kowloon, Hong Kong SAR, China\n2Shenzhen Research Institute, City University of Hong Kong, Shenzhen, Guangdong 518057, China\n3Department of Mathematics, City University of Hong Kong,\nTat Chee Avenue, Kowloon, Hong Kong SAR, China\n4Department of Physics, The University of Hong Kong, Pokfulam, Hong Kong SAR, China\nReinforcement learning has been widely used in many problems, including quantum control of\nqubits.\nHowever, such problems can, at the same time, be solved by traditional, non-machine-\nlearning methods, such as stochastic gradient descent and Krotov algorithms, and it remains unclear\nwhich one is most suitable when the control has speciﬁc constraints. In this work, we perform a\ncomparative study on the eﬃcacy of three reinforcement learning algorithms: tabular Q-learning,\ndeep Q-learning, and policy gradient, as well as two non-machine-learning methods: stochastic\ngradient descent and Krotov algorithms, in the problem of preparing a desired quantum state. We\nfound that overall, the deep Q-learning and policy gradient algorithms outperform others when the\nproblem is discretized, e.g. allowing discrete values of control, and when the problem scales up. The\nreinforcement learning algorithms can also adaptively reduce the complexity of the control sequences,\nshortening the operation time and improving the ﬁdelity. Our comparison provides insights into the\nsuitability of reinforcement learning in quantum control problems.\nI.\nINTRODUCTION\nReinforcement learning, a branch of machine learning\nin artiﬁcial intelligence, has proven to be a powerful tool\nto solve a wide range of complex problems, such as the\ngames of Go [1] and Atari [2]. Reinforcement learning\nhas also been applied to a variety of problems in quan-\ntum physics with vast success [3–24], including quantum\nstate preparation [3–8], state transfer [9], quantum gate\ndesign [10], and error correction [11].\nIn many cases,\nit outperforms commonly-used conventional algorithms,\nsuch as Krotov and Stochastic Gradient Descent (SGD)\nalgorithms [9, 10]. In the reinforcement learning algo-\nrithm, an optimization problem is converted to a set of\npolicies that governs the behavior of a computer agent,\ni.e. its choices of actions and, consequently, the reward\nit receives. By simulating sequences of actions taken by\nthe agent maximizing the reward, one ﬁnds an optimal\nsolution to the desired problem [25].\nThe development of techniques that eﬃciently opti-\nmize control protocols is key to quantum physics. While\nsome problems can be solved analytically using meth-\nods such as reverse engineering [26], in most cases nu-\nmerical solutions are required. Various numerical meth-\nods are therefore put forward, such as gradient-based\nmethods (including SGD [17], GRAPE [27] and vari-\nants [28]), the Krotov method [29], the Nelder-Mead\nmethod [30] and convex programming [31].\nRecently,\nthere is a frenetic attempt to apply reinforcement learn-\ning and other machine-learning-based algorithms [32–37]\nto a wide range of physics problems. In particular, the in-\ntroduction of reinforcement learning to quantum control\nhave revealed new interesting physics [3–8], and these\ntechniques have therefore received increasing attention.\nA fundamental question then arises: under what situa-\ntion is reinforcement learning the most suitable method?\nIn this paper, we consider problems related to quantum\ncontrol of a qubit. The goal of these problems is typically\nto steer the qubit toward a target state under certain con-\nstraints. The mismatch between the ﬁnal qubit state and\nthe target state naturally serves as the cost function used\nin the SGD or Krotov methods, and the negative cost\nfunction can serve as the reward function in the reinforce-\nment learning procedure.\nOur question then becomes:\nunder diﬀerent scenarios of constraints, which algorithm\nworks best? In this work, we compare the eﬃcacy of two\ncommonly-used traditional methods: SGD and the Kro-\ntov method, and three algorithms based on reinforcement\nlearning: tabular Q-learning (TQL) [25], deep Q-learning\n(DQL) [2], and policy gradient (PG) [38] , under situa-\ntions with diﬀerent types of control constraints.\nIn Ref. [5], the Q-learning techniques (TQL and DQL)\nhave been applied to the problem of quantum state prepa-\nration, revealing diﬀerent stages of quantum control. The\nproblem of preparing a desired quantum state from a\ngiven initial state is on one hand simple enough to be\ninvestigated in full detail, and on the other hand con-\ntains suﬃcient physics allowing for various types of con-\ntrol constraints. We therefore take quantum state prepa-\nration as the platform that our comparison of diﬀerent\nalgorithms is based on. While a detailed description of\nquantum state preparation is provided in Results, we\nbrieﬂy introduce the ﬁve algorithms we are comparing in\nthis work here. (Detailed implementations are provided\nin the Methods and Supplementary Method 1.)\nSGD is one of the simplest gradient-based optimization\narXiv:1902.02157v2  [quant-ph]  5 May 2021\n2\na\nb\nFIG. 1:\nSketch of the procedure of TQL and DQL. (a)\nIn TQL, the Q(s, a) values are stored in the Q-table. When\nthe agent is at state s(5), it reviews Q(s(5), a(i)) for all possible\nactions and chooses one with the maximum “Q-value” (which\nwe assume is a(3)). As a result, the state then evolves to s(2).\nDepending on the distance between s(2) and the target, the\nQ-values (e.g. Q(s(5), a(3))) is updated according to Eq. (8).\nThis process is then repeated at the new state s(2) and so\nforth. (b) In DQL, the Q-table is replaced by the Q-network.\nInstead of choosing an action with the maximum Q-value from\na list, this process is done by a neural network, the Q-network,\nwhich takes the input state (s) and outputs an action that it\nﬁnds most appropriate. Evaluation of the resulting state (s′)\nafter the action suggests how the neural network should be\nupdated (trained). For detailed implementation, see Methods\nand Supplementary Method 1.\nalgorithms. In each iteration, a direction in the param-\neter space is randomly chosen, along which the control\nﬁeld is updated using the gradient of the cost function\ndeﬁned as the mismatch between the evolved state and\nthe target state. Ideally, the gradient is zero when the\ncalculation has converged to the optimal solution. The\nKrotov algorithm has a diﬀerent strategy: The initial\nstate is ﬁrst propagated forward obtaining the evolved\nstate. The evolved state is then projected to the target\nstate, deﬁning a co-state encapsulating the mismatch be-\ntween the two. Then the co-state is propagated backward\nto the initial state, during which process the control ﬁelds\nare updated. When the calculation is converged, the co-\nstate is identical to the target state.\nIn Q-learning (including TQL and DQL), a computer\nagent evolves in an environment.\nAll information re-\nquired for optimization is encoded in the environment,\nwhich is allowed to be in a set of states S. In each step,\nthe agent chooses an action from a set A, bringing the\nenvironment of the agent to another state. As a conse-\nquence, the agent acquires a reward, which encapsulates\nthe desired optimization problem. Fig. 1a schematically\nshows how TQL works. At each state s ∈S, the agent\nchooses actions a ∈A according to the action-value func-\ntion Q(s, a), deﬁned as the estimated total reward start-\ning from state s and action a, forming the so-called Q-\ntable. Each time the agent takes an action, a reward r\nis generated according to the distance between the re-\nsulting state and the target, which updates the Q-table.\nAn optimal solution is found by iterating this process\nsuﬃcient times. We note that since a table has a ﬁnite\nnumber of entries, both the states and actions should be\ndiscretized. Fig. 1b shows DQL, in which the role of the\nQ-table is replaced by a neural network, called the Q-\nnetwork. The agent then chooses its action according to\nthe output of the Q-network, and the reward is used to\nupdate the network. In this case, although the allowed\nactions are typically discrete, the input state can actually\nbe continuous.\nSimilar to TQL and DQL, PG also requires the sets\nof states S, actions A, and rewards r.\nThe policy of\nthe agent is represented by a neural network. With the\nstate as the input, the network outputs the probability\nof choosing each action. After each episode, the policy\nnetwork is updated toward a direction that increases the\ntotal reward. Since the state is encoded as the input of\nthe neural network, PG can also accommodate continu-\nous input states.\nII.\nRESULTS\nA.\nSingle-qubit case\nWe start with the preparation of a single-qubit state.\nConsider the time dependent Hamiltonian\nH[J(t)] = 4J(t)σz + hσx,\n(1)\nwhere σx and σz are Pauli matrices. The Hamiltonian\nmay describe a singlet-triplet qubit [39] or a single spin\nwith energy gap h under tunable control ﬁelds [40, 41].\nIn these systems, it is diﬃcult to vary h during gate op-\nerations, and we therefore assume that h is a constant in\n3\nour work, which at the same time serves as our energy\nunit. Quantum control of the qubit is then achieved by\naltering J(t) dynamically.\nQuantum state preparation refers to the problem to\nﬁnd J(t) such that a given initial state |ψ0⟩evolves,\nwithin time T, to a ﬁnal state |ψf⟩that is as close as\npossible to the target state |φ⟩. The quality of the state\ntransfer is evaluated using the ﬁdelity, deﬁned as\nF = |⟨ψf|φ⟩|2 .\n(2)\nWe typically use the averaged ﬁdelity F over many runs\nof a given algorithm in our comparison (unless otherwise\nnoted, we average 100 runs to obtain F), because the\ninitial guesses of the control sequences are random, and\nthe reinforcement learning procedure is probabilistic.\nIn this work, we take |ψ0⟩= |0⟩, |φ⟩= |1⟩and T =\n2π unless otherwise speciﬁed. Under diﬀerent situations,\nthere are various kinds of preferences or restrictions of\ncontrol. We consider the following types of restrictions:\n(i) Assuming that control is performed with a sequence\nof piecewise constant pulses, and in this work, we further\nassume that the time duration of each piece is equal to\neach other for convenience. For this purpose, we divide\nthe total time T into N equal time steps, each of which\nhaving a step size dt = T/N, with N denoting the max-\nimum number of pieces required by the control.\nJ(t)\nis accordingly discretized, so that on the ith time step,\nJ(t) = Ji and the system evolves under H(Ji). Denot-\ning the state at the end of the ith time step as |ψi⟩,\nthe evolution at the ith step is |ψi⟩= Ui|ψi−1⟩, where\nUi = exp{−iH(Ji)dt}. In principle, the evolution time\ncan be less than T, namely the evolution may conclude\nat the if th time step with if ⩽N. (In our calculations,\nthe evolution is terminated when the ﬁdelity F ≥0.999.)\nDue to their nature, SGD and Krotov have to ﬁnish all\ntime steps, i.e. if = N. However, as we shall see below,\nQL and DQL frequently have if < N.\n(ii) We also consider the case where the magnitude of\nthe control ﬁeld is bounded, i.e. Ji ∈[Jmax, Jmin] for all\ni. The constraint can be straightforwardly satisﬁed in\nTQL and DQL, since they only operate within the given\nset of actions thus cannot exceed the bounds. For SGD\nand Krotov, updates to the control ﬁelds may exceed the\nbounds, in which case we need to enforce the bounds by\nsetting Ji as Jmax when the updated value is greater than\nJmax, and as Jmin when the updated value is smaller than\nJmin. In the case in which either of them is not restricted,\nwe simply note Jmin →−∞or Jmax →∞.\n(iii) The values of the control ﬁeld may be discretized\nin the given range, i.e., Ji ∈{Jmin, Jmin + dJ/M, Jmin +\n2dJ/M, · · · , Jmax} where dJ = (Jmax−Jmin), so that the\ncontrol ﬁeld can take M + 1 values including Jmin and\nJmax. In reality this situation may arise, for example,\nwhen decomposing a quantum operation into a set of\ngiven gates [42–44]. For a reason similar to (ii), TQL\n0\n10\n20\n30\n40\n50\nN\n0\n0.2\n0.4\n0.6\n0.8\n1\nF\nSGD \nKrotov \nTQL \nDQL \nPG\nFIG. 2:\nAverage ﬁdelities as functions of the maxi-\nmum number of control pieces. For TQL, DQL and PG,\nJi ∈{0, 1} (i.e. M = 1). For SGD and Krotov, no restriction\nis imposed on the range of Ji and M (i.e. M →∞). The ver-\ntical dashed lines correspond to results shown with respective\nM values in Fig. 4.\nand DQL only select actions within the given set so the\nconstraint is satisﬁed. For SGD and Krotov which keep\nupdating the values of the control ﬁeld during iterations,\nwe enforce the constraint by setting the value of each\ncontrol ﬁeld to the nearest allowed value at the end of\nthe execution.\nTo sum up, the number of pieces in control sequences\nN, the bounds of the control ﬁeld Jmin and Jmax, as well\nas the number of the discrete values of the control ﬁeld\nM + 1 are the main factors characterizing situations to\nprepare quantum states, based on which our comparison\nof diﬀerent algorithms is conducted. We also deﬁne N iter\nas the number of iterations performed in executing an\nalgorithm, which is typically taken as equal for diﬀerent\nalgorithms to ensure a fair comparison. Unless otherwise\nnoted, N iter = 500 in all results shown.\nIn Fig. 2 we study a situation where the maximum\nnumber of pieces in the control sequence N is given,\nand the results are shown as the averaged ﬁdelities as\nfunctions of N. Here, the quality of an algorithm is as-\nsessed by the averaged ﬁdelity of the state it prepares (as\ncompared to the target state) F, but not by the com-\nputational resources it costs. For N ⩽10, the Krotov\nmethod gives the lowest ﬁdelity, possibly due to the fact\nthat Krotov requires a reasonable level of continuity in\nthe control sequence, and one with a few pieces is unlikely\nto reach convergence. As N increases, the performance\nof Krotov is much improved, which has the highest ﬁ-\ndelity when N is large (N ⩾30 as seen in the ﬁgure).\nSGD performs better than Krotov for N ⩽10, but worse\notherwise, because as N increases, the algorithm has to\nsearch over a much larger parameter space. Within the\ngiven number of iterations (N iter = 500 as noted above),\n4\n0\n4\n8\n12\n16\n20\ni\n0.0\n0.2\n0.4\n0.6\n0.8\nJi\nx\ny\nz\n0\n4\n8\n12\n16\n20\ni\n−20\n−10\n0\n10\n20\nJi\ny\nx\nz\n0\n4\n8\n12\n16\n20\ni\n−0.04\n−0.02\n0.00\n0.02\n0.04\nJi\ny\nx\nz\nSGD\nKrotov\nTQL/DQL/PG\na\nd\nc\nf\ne\nb\nFIG. 3: Pulse proﬁles and the corresponding trajec-\ntories on the Bloch sphere. Left column: Example pulse\nproﬁles taken from results of Fig. 2 with N = 20. Right col-\numn: Evolution of the state corresponding to the respective\ncontrol sequence in the left column. TQL, DQL and PG give\nthe same optimal results and are thus shown together.\nit concludes with a lower ﬁdelity. Of course, this result\ncan be improved if more iterations are allowed, and we\nshall show relevant results in Supplementary Discussion\n2. The SGD results at N = 2 is irregular (thus the cusp\nat N = 6), due to the lack of ﬂexibility in the control\nsequence which makes it diﬃcult to achieve high ﬁdelity\nwith only two steps.\nThe ﬁdelity for TQL is higher than SGD and Krotov,\nbut is still lower than that of DQL and PG, indicating\nthe superior ability of deep learning. Nevertheless, we\nnote that the TQL may sometimes fail: it occasionally\narrives at a ﬁnal state which is completely diﬀerent than\nthe target state. On the other hand, SGD could fail by\nbeing trapped at a local minimum, but even in that case\nit is not drastically diﬀerent from the optimal solution\nin terms of the ﬁdelity. This is the reason why the TQL\nresults drop for N > 10. For larger N, the failure rate\nfor TQL is higher (possibly due to the higher dimension-\nality of the Q-table), and therefore the averaged ﬁdelity\n0.2\n0.6\n1\nF\na\nSGD\nKrotov\nTQL\nDQL\nPG\n100\n101\nM\n0.2\n0.6\n1\nF\nb\nFIG. 4:\nEﬀect of discrete control ﬁelds on the av-\neraged ﬁdelity for all ﬁve methods considered. The\nstrength of control ﬁeld is restricted to Ji ∈[0, 1], and M + 1\ndiscrete values (including 0 and 1) are allowed.\nPanel (a)\nshows the case of N = 6 while (b) N = 20, corresponding to\nthe two vertical dashed lines in Fig. 2.\nis lower. Among all ﬁve algorithms, PG is consistently\nthe best. Apart from PG, DQL gives the highest ﬁdelity\nfor N < 30, but due to its nonzero failure probability, it\nis outperformed by Krotov for N > 30. Nevertheless, the\neﬀect is moderate and the ﬁdelity is still very close to 1\n(F = 0.9988).\nTo further understand the results shown in Fig. 2, we\ntake examples from N = 20 and plot the pulse proﬁles\nand the corresponding trajectories on the Bloch sphere in\nFig. 3. We immediately realize that reinforcement learn-\ning (TQL, DQL and PG) yield very simple pulse shapes:\none only has to keep the control at zero for time T/2, and\nthe desired target state (|1⟩) will be achieved. However,\nto ﬁnd the result, the algorithm has to somehow realize\nthat one does not have to complete all N pieces, which\nimplies their ability to adaptively generating the control\nsequence. As can be seen from Fig. 3a and 3c, SGD and\nKrotov only search for pulse sequences with exactly N\npieces and therefore miss the optimal solution. Their tra-\njectories on the Bloch sphere are much more complex as\ncompared to those of reinforcement learning. In practice,\nthe complex pulse shapes and longer gate times mean\nthat they are diﬃcult to realize in the laboratory, and\npotentially introduces error to the control procedure (In\nSupplementary Discussion 4 we provide more details on\nthis issue). From Fig. 3 we also notice that reinforcement\nlearning possesses better ability to adaptively sequencing,\nwhich is particularly suitable for problems that involve\noptimization of gate time or speed, such as the quan-\n5\ntum speed limit [5, 9]. On the other hand, application of\nSGD or Krotov to the same problem requires searching\nover various diﬀerent N values before an optimal solution\ncan be found, which cost much more resources [45, 46].\nWe now study the eﬀect of restrictions on the per-\nformances of algorithms.\nNamely, the control ﬁeld is\nbounded between Jmin and Jmax, with M + 1 allowed\nvalues including the bounds. In Fig. 4, we impose the\nsame restriction J ∈[0, 1] to all ﬁve methods and vary\nM from M = 1 to M = 49.\nIt is interesting to note\nthat the averaged ﬁdelities of three reinforcement learn-\ning algorithms decreases with M, albeit not considerably.\nThis is because TQL, DQL and PG favor bounded and\nconcrete sets of actions, and more choices will only add\nburden to the searching process, rendering the algorithms\nineﬃcient. Improvements may be made by increasing the\nnumber of iterations (cf. Supplementary Discussion 2),\nand using a larger neural network with stronger represen-\ntational power. For N = 6 (Fig. 4a), TQL and DQL are\ncomparable and have overall the best performance except\nfor M > 14 in which SGD becomes slightly better. On\nthe other hand, F for PG drops rapidly for M ⩾30. For\nN = 20 (Fig. 4b), DQL and PG have the best perfor-\nmance, but for large M they are not signiﬁcantly better\nthan other methods.\nMore results involving SGD and\nKrotov are given in Supplementary Discussion 1, from\nwhich we conclude that the eﬀect of boundaries in con-\ntrol is much more obvious for Krotov method than SGD,\nsince Krotov performs much larger updates at each iter-\nation. Meanwhile, the eﬀect of discretization (decreasing\nM) are severe for both Krotov and SGD methods, indi-\ncating that successful implementations of them depend\ncrucially on the continuity of the problem.\nFinally, we note that all results obtained have the tar-\nget state being |1⟩.\nPreparing a quantum state other\nthan |1⟩may have diﬀerent results, for which an example\nis presented in Supplementary Discussion 3. Neverthe-\nless, the overall observation of the pros and cons of the\nalgorithms should remain similar.\nB.\nMulti-qubit case\nWe now consider a case preparing a multi-qubit state\nas sketched in Fig. 5a, b. Our system is described by the\nfollowing Hamiltonian:\nH(t) = C\nK−1\nX\nk=1\n\u0000Sk\nxSk+1\nx\n+ Sk\nySk+1\ny\n\u0001\n+\nK\nX\nk=1\n2Bk(t)Sk\nz , (3)\nwhere K is the total number of spins, Sk\nx, Sk\ny and Sk\nz are\nthe kth spin operator, C describes the constant nearest-\nneighbor coupling strength (set to be C = 1), and Bk(t)\nis the time-dependent local magnetic ﬁeld applied at the\nkth spin to perform control. This is essentially a task\n2\n3\n4\n5\n6\n7\n8\nNumber of spins\n0\n0.2\n0.4\n0.6\n0.8\nF\nSGD\nKrotov\nDQL\nPG\na\nc\nb\nBk(t)\nC\n0\n0.2\n0.4\n0.6\n0.8\n1\nAmplitude\nd\nSGD\ne\nKrotov\n1 2 3 4 5 6 7 8\nk\n0\n0.2\n0.4\n0.6\n0.8\nAmplitude\nf\nDQL\n1 2 3 4 5 6 7 8\nk\ng\nPG\nFIG. 5:\nSpin transfer as preparation of a multi-qubit\nstate. (a) The multi-qubit system is initialized with the left-\nmost spin being up and all others down. (b) The target state\nhas the rightmost spin being up and all others down. (c) The\naverage ﬁdelity versus the number of spins from diﬀerent al-\ngorithms; (d)-(g) The amplitudes (visualization of the ﬁnal\nprepared state) at diﬀerent spins for K = 8. The red solid\nbars correspond to results averaged over 100 runs, and the\nhollow bars enclosed by dashed line shows the results with\nthe highest ﬁdelities.\ntransferring a spin: the system is initialized to a state\nwith the leftmost spin being up and all others down, and\nthe goal is to prepare a state with the rightmost spin\nbeing up and all others down. We set the operation time\nduration to be T = (K −1)π/2, which is divided to 20\nequal time steps (i.e.\nN = 20).\nThe external ﬁeld is\nrestricted to Bk(t)/C ∈[0, 40] for SGD and Krotov, and\nBk(t)/C ∈{0, 40} for all three reinforcement learning\nalgorithms. Note that TQL fails for K ≥2 due to the\nlarge size of the Q-table, and is thus excluded in the\n6\nSGD Krotov TQL DQL PG\nPerformance vs number of time steps N\n↘\n↗\n↘\n↘\n⋆\nAbility to adaptively segment\n⋆\n⋆\n⋆\nDiscrete operation set (M small)\n⋆\n⋆\nContinuous operation set (M large)\n⋆\nScaled-up problems (multi-qubits)\n⋆\nTABLE I: Summary of the performances under diﬀerent situations. A “⋆” indicates that the algorithm performs best, while\nthe arrow “↘” (“↗”) denotes decrease (increase) of the performance versus increase of the variable concerned.\ncomparison.\nFig. 5c shows the average ﬁdelity versus the number of\nspins (K), after each algorithm is run for 500 iterations.\nAs K increases, the dimensionality of the problem in-\ncreases and therefore the performances of all algorithms\ndeteriorate. When K < 4, Krotov, DQL and PG have\ncomparable performances, while SGD has the lowest ﬁ-\ndelity. As K increases, F for PG and DQL drop much\nmore slowly as compared to Krotov. At K = 8, we have\nF = 0.0989 (Krotov), F = 0.4214 (PG), F = 0.5433\n(DQL), respectively. Here, we have not assumed a par-\nticular form of the control ﬁeld, so one has to search\nover a very large space. Specializing the control to cer-\ntain types would improve performances of the algorithms\n[47].\nIn order to visualize the ﬁnal states prepared, we de-\nﬁne the amplitude, Ak, as the absolute value of the inner\nproduct between the ﬁnal states and the state with the\nkth spin being up while all others being down. A per-\nfect transfer would be that the amplitude is 1 for the\nrightmost spin and 0 otherwise.\nTaking K = 8 as an\nexample, we show how the amplitudes distribute over\ndiﬀerent spins in Fig. 5d-g. We compare two diﬀerent\nkinds of results: one showing the averaged results over\n100 runs (shown as red solid bars), and the other the\nbest result among the 100 runs (hollow bars enclosed by\ndashed lines). We see that SGD completely fails to pre-\npare the desired state.\nThe best results from Krotov,\nDQL and PG are comparable, but considering the av-\nerage over many runs, DQL and PG have better per-\nformances. Moreover, the optimal control sequences for\ndiﬀerent algorithms are provided in Supplementary Table\n1-4.\nIII.\nDISCUSSION\nIn this paper, we have examined performances of ﬁve\nalgorithms: SGD, Krotov, TQL, DQL, and PG, on the\nproblem of quantum state preparation. From the com-\nparison, we can summarize the characteristics of the al-\ngorithms under diﬀerent situations as follows (see also\nTable. 1).\nDependence on the maximum number of pieces in the\ncontrol sequence, N: When all algorithms are executed\nwith the same number of iterations, PG has overall the\nbest performance, but the corresponding ﬁdelity still\ndrops slightly as N increases. In fact, the ﬁdelities from\nall methods decrease as N increases, except the Krotov\nmethod, for which the ﬁdelity increases when N is large.\nAbility to adaptively segment: During the optimiza-\ntion process, TQL, DQL and PG can adaptively reduce\nthe number of pieces required and can thus ﬁnd opti-\nmal solutions eﬃciently. SGD and Krotov, on the other\nhand, always work with a ﬁxed number of N and thus\nsometimes miss the optimal solution.\nDependence on restricted ranges of the strength of the\ncontrol ﬁeld: TQL, DQL and PG naturally work with\nrestricted sets of actions so they perform well when the\nstrength of the control ﬁeld is restricted. Such restriction\nreduces the eﬃciency for both SGD and Krotov method,\nbut the eﬀect is moderate for SGD because its updates\non the control ﬁeld are essentially local. However, the\nKrotov method makes signiﬁcant updates during its exe-\ncution and thus becomes severely compromised when the\nstrength of the control ﬁeld is restricted.\nAbility to work with control ﬁelds taking M + 1 dis-\ncrete values: TQL, DQL and PG again naturally work\nwith discrete values of the control ﬁeld. In fact, the ﬁdeli-\nties from them decrease as the allowed values of the con-\ntrol ﬁelds become more continuous (M increases). This\nproblem may be circumvented using more sophisticated\nalgorithms such as Actor-Critic [48, 49], and the deep de-\nterministic policy gradient method [50]. SGD is not sen-\nsitive to M because it works with a relatively small range\nof control ﬁeld and a reasonable discretization is suﬃ-\ncient. The Krotov method, on the other hand, strongly\nfavors continuous problem, i.e. M being large.\nAbility to accommodate scaled-up problems (multiple\nqubits): Except for TQL, all other algorithms can be\nstraightforwardly generalized to treat quantum control\nproblems with more than one qubit. However, SGD is\nrather ineﬃcient, and DQL generally outperforms all oth-\ners for cases considered in this work (K ≤8).\nMoreover, we have found that PG and DQL methods,\nin general, have the best performances among the ﬁve\nalgorithms considered, demonstrating the power of rein-\nforcement learning in conjunction with neural networks\nin treating complex optimization problems.\nOur direct comparison of diﬀerent methods may also\n7\nshed light on how these algorithms can be improved. For\nexample, the Krotov method strongly favors the “con-\ntinuous” problem, for which TQL, DQL and PG do not\nperform well. It should be possible that gradients in the\nKrotov method can be applied in the Q-learning pro-\ncedures and thereby improves their performances.\nWe\nhope that our work has elucidated the eﬀectiveness of re-\ninforcement learning in problems with diﬀerent types of\nconstraints, and in addition, it may provide hints on how\nthese algorithms can be improved in future studies.\nAcknowledgement This work is supported by the\nResearch\nGrants\nCouncil\nof\nthe\nHong\nKong\nSpe-\ncial Administrative Region, China (Grant Nos. CityU\n21300116, CityU 11303617, CityU 11304018), the Na-\ntional Natural Science Foundation of China (Grant\nNos. 11874312, 11604277), the Guangdong Innovative\nand Entrepreneurial Research Team Program (Grant\nNo. 2016ZT06D348), and Key R&D Program of Guang-\ndong province (Grant No. 2018B030326001).\nCode Availability The code for all algorithms used\nin\nthis\nwork\nis\navailable\non\nGitHub\nunder\nMIT\nLicense (https://github.com/93xiaoming/RL_state_\npreparation).\nData Availability The data generated during this\nstudy are available from the corresponding author upon\nreasonable request.\nMethods\nIn this section, we give a brief description of our imple-\nmentation of TQL, DQL and PG in this work. The full\nalgorithms for all methods used in this work are given in\nSupplementary Method 1.\nA.\nTQL\nFor Q-learning, the key ingredients include a set of\nallowed states S, a set of actions A, and the reward r.\nThe state of qubit can be parametrized as\n|ψ(θ, ϕ)⟩= ±\n\u0012\ncos θ\n2 |0⟩+ eiϕ sin θ\n2 |1⟩\n\u0013\n,\n(4)\nwhere (θ, φ) corresponds to a point on the Bloch sphere,\nand a possible global phase of −1 has been included. Our\nset of allowed states is deﬁned as\nS ≡{|ψ(θ, ϕ)⟩|θ ∈sθ, ϕ ∈sϕ} ,\n(5)\nwhere\nsθ =\n\u001a0π\n30 , 1π\n30 , · · · , 29π\n30\n\u001b\n,\nsϕ =\n\u001a0π\n30 , 1π\n30 , · · · , 59π\n30\n\u001b\n.\n(6)\nWe note that this is a discrete set of states, and after each\nstep in the evolution, if the resulting state is not identical\nto any of the member in the set, it will be assigned as\nthe member that is closest to the state, i.e. having the\nmaximum ﬁdelity in their overlap.\nIn the ith step of the evolution, the system is at a state\nsi = |ψi⟩∈S, and the action is given by the evolution\noperator ai = Ui = exp{−iH(Ji)dt}. All allowed values\nof the control ﬁeld Ji therefore form a set of possible\nactions A. The resulting state Ui|ψi⟩after this step is\nthen compared to the target state, and the reward is\ncalculated using the ﬁdelity between the two states as\nri =\n\n\n\n10\nF ∈(0.5, 0.9],\n100\nF ∈(0.9, 0.999],\n5000\nF ∈(0.999, 1],\n(7)\nso that the action that takes the state very close to\nthe target is strongly rewarded. In practice, the agent\nchooses its action according to the ϵ-greedy algorithm\n[25], i.e. the agent either chooses an action with the\nlargest Q(s, a) with 1 −ϵ probability, or with probability\nϵ it randomly chooses an action in the set. The introduc-\ntion of a nonzero but small ϵ ensures that the system is\nnot trapped in a poor local minimum. The elements in\nQ-tables are then updated as:\nQ(si−1, ai) ←Q(si−1, ai)+α[ri+γ max\na′ Q(si, a′)−Q(si−1, ai)],\n(8)\nwhere a′ refers to all possible ai in this step, α is the\nlearning rate, and γ is a reward discount to ensure the\nstability of the algorithm.\nB.\nDQL\nDQL stores the action-value functions with a neural\nnetwork Θ. We take qubit case as an example. Deﬁning\nan agent state as\ns = [Re (⟨0|ψ⟩) , Im (⟨0|ψ⟩) , Re (⟨1|ψ⟩) , Im (⟨1|ψ⟩)]T ,\n(9)\nthe network outputs the Q-value for each action a ∈A\nas Q(s, a; Θ). We note that in DQL, the discretization of\nstates on the Bloch sphere is no longer necessary and we\ncan deal with states that vary continuously. Otherwise\nthe deﬁnitions of the set of actions and reward are the\nsame as those in TQL.\nWe adopt the double Q-network training approach [2]:\ntwo neural networks, the evaluation network Θ and the\ntarget network Θ−, are used in training. In the memory\nwe store experiences deﬁned as ei = (si−1, ai, ri, si). In\neach training step, an experience is randomly chosen from\nthe memory, and the evaluation network is updated using\nthe outcome derived from the experience.\n8\nC.\nPG\nSimilar to DQL, PG is based on neural networks. With\nthe state s as the input vector, the network of PG out-\nputs the probability of choosing each action p = P(s; Θ),\nwhere p = [p1, p2, · · · ]T . At each time step t, the agent\nchooses its action according to p, and stores the total\nreward it has obtained vt = Pt\ni=1 γiri.\nIn each iter-\nation, the network is updated in order to increase the\ntotal reward. This is done according to the gradient of\nlog P(st; Θ)vt, the details of which can be found in Sup-\nplementary Method 1.\nWe note that unlike the case for SGD and Krotov,\nin which the ﬁdelity monotonically increases with more\ntraining in most cases, the ﬁdelity output by TQL, DQL\nand PG may experience oscillations as the algorithm can-\nnot guarantee optimal solutions in all trials. In this case,\none just has to choose outputs which have higher ﬁdelity\nas the learning outcome.\n[1] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou,\nA. Huang, A. Guez, T. Hubert, L. Baker, M. Lai,\nA. Bolton, et al., Nature 550, 354 (2017).\n[2] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Ve-\nness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K.\nFidjeland, G. Ostrovski, et al., Nature 518, 529 (2015).\n[3] C. Chen, D. Dong, H.-X. Li, J. Chu,\nand T.-J. Tarn,\nIEEE transactions on neural networks and learning sys-\ntems 25, 920 (2013).\n[4] J.-J. Chen and M. Xue, arXiv:1901.08748 (2019).\n[5] M. Bukov,\nA. G. R. Day,\nD. Sels,\nP. Weinberg,\nA. Polkovnikov, and P. Mehta, Phys. Rev. X 8, 031086\n(2018).\n[6] M. Bukov, Phys. Rev. B 98, 224305 (2018).\n[7] M. August and J. M. Hern´andez-Lobato, in International\nConference on High Performance Computing (Springer,\n2018) pp. 591–613.\n[8] F. Albarr´an-Arriagada, J. C. Retamal, E. Solano,\nand\nL. Lamata, Phys. Rev. A 98, 042315 (2018).\n[9] X.-M. Zhang, Z.-W. Cui, X. Wang,\nand M.-H. Yung,\nPhy. Rev. A 97, 052333 (2018).\n[10] M. Y. Niu, S. Boixo, V. N. Smelyanskiy, and H. Neven,\nnpj Quantum Inf. 5, 33 (2019).\n[11] T. F¨osel, P. Tighineanu, T. Weiss,\nand F. Marquardt,\nPhys. Rev. X 8, 031084 (2018).\n[12] A. A. Melnikov, H. Poulsen Nautrup, M. Krenn, V. Dun-\njko, M. Tiersch, A. Zeilinger,\nand H. J. Briegel, Proc.\nNatl. Acad. Sci. 115, 1221 (2018).\n[13] V. Dunjko, J. M. Taylor, and H. J. Briegel, Phys. Rev.\nLett. 117, 130501 (2016).\n[14] A. Hentschel and B. C. Sanders, Phys. Rev. Lett. 104,\n063603 (2010).\n[15] A. G. Day, M. Bukov, P. Weinberg, P. Mehta,\nand\nD. Sels, Phys. Rev. Lett. 122, 020601 (2019).\n[16] R.-B. Wu, B. Chu, D. H. Owens, and H. Rabitz, Phys.\nRev. A 97, 042122 (2018).\n[17] C. Ferrie, Phys. Rev. Lett. 113, 190404 (2014).\n[18] G. Reddy, A. Celani, T. J. Sejnowski, and M. Vergassola,\nProc. Natl. Acad. Sci. 113, E4877 (2016).\n[19] S. Colabrese, K. Gustavsson, A. Celani, and L. Biferale,\nPhys. Rev. Lett. 118, 158004 (2017).\n[20] H. P. Nautrup, N. Delfosse, V. Dunjko, H. J. Briegel,\nand N. Friis, Quantum 3, 215 (2019).\n[21] R. Sweke, M. S. Kesselring, E. P. van Nieuwenburg, and\nJ. Eisert, Mach. Learn. Sci. Technol. 2, 025005 (2020).\n[22] P. Andreasson,\nJ. Johansson,\nS. Liljestrand,\nand\nM. Granath, Quantum 3, 183 (2019).\n[23] J. Halverson, B. Nelson, and F. Ruehle, J. High Energy\nPhys. 2019, 1 (2019).\n[24] K.-W. Zhao, W.-H. Kao, K.-H. Wu, and Y.-J. Kao, Phys.\nRev. E 99, 062106 (2019).\n[25] R. S. Sutton and A. G. Barto, Reinforcement learning:\nAn introduction (MIT press Cambridge, 1998).\n[26] E. Barnes and S. Das Sarma, Phys. Rev. Lett. 109,\n060401 (2012).\n[27] N. Khaneja, T. Reiss, C. Kehlet, T. Schulte-Herbr¨uggen,\nand S. J. Glaser, J. Magn. Reson 172, 296 (2005).\n[28] G. J¨ager, D. M. Reich, M. H. Goerz, C. P. Koch,\nand\nU. Hohenester, Phys. Rev. A 90, 033628 (2014).\n[29] V. F. Krotov, Global methods in optimal control theory\n(Marcel Dekker Inc., New York, 1996).\n[30] J. Kelly, R. Barends, B. Campbell, Y. Chen, Z. Chen,\nB. Chiaro, A. Dunsworth, A. G. Fowler, I.-C. Hoi, E. Jef-\nfrey, A. Megrant, J. Mutus, C. Neill, P. J. J. O’Malley,\nC. Quintana, P. Roushan, D. Sank, A. Vainsencher,\nJ. Wenner, T. C. White, A. N. Cleland, and J. M. Mar-\ntinis, Phys. Rev. Lett. 112, 240504 (2014).\n[31] R. L. Kosut, M. D. Grace,\nand C. Brif, Phys. Rev. A\n88, 052326 (2013).\n[32] L. Wang, Phys. Rev. B 94, 195105 (2016).\n[33] G. Carleo and M. Troyer, Science 355, 602 (2017).\n[34] D.-L. Deng, X. Li, and S. Das Sarma, Phys. Rev. X 7,\n021021 (2017).\n[35] J. Carrasquilla and R. G. Melko, Nat. Phys. 13, 431\n(2017).\n[36] J. Li, X. Yang, X. Peng, and C.-P. Sun, Phys. Rev. Lett.\n118, 150503 (2017).\n[37] Y.-T. Hsu, X. Li, D.-L. Deng, and S. Das Sarma, Phys.\nRev. Lett. 121, 245701 (2018).\n[38] R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Man-\nsour, Advances in neural information processing systems\n, 1057 (2000).\n[39] J. R. Petta, A. C. Johnson, J. M. Taylor, E. A. Laird,\nA. Yacoby, M. D. Lukin, C. M. Marcus, M. P. Hanson,\nand A. C. Gossard, Science 309, 2180 (2005).\n[40] A. Greilich, S. E. Economou, S. Spatzek, D. Yakovlev,\nD. Reuter, A. Wieck, T. Reinecke, and M. Bayer, Nat.\nPhys. 5, 262 (2009).\n[41] E. Poem, O. Kenneth, Y. Kodriano, Y. Benny, S. Khat-\nsevich, J. E. Avron, and D. Gershoni, Phys. Rev. Lett.\n107, 087401 (2011).\n[42] A. Y. Kitaev, A. Shen,\nand M. N. Vyalyi, American\nMathematical Society (2002).\n[43] A. W. Harrow, B. Recht,\nand I. L. Chuang, J. Math.\nPhys 43, 4445 (2002).\n[44] E. T. Campbell, B. M. Terhal,\nand C. Vuillot, Nature\n549, 172 (2017).\n[45] T. Caneva, M. Murphy, T. Calarco, R. Fazio, S. Mon-\ntangero, V. Giovannetti, and G. E. Santoro, Phys. Rev.\nLett. 103, 240501 (2009).\n[46] M. Murphy,\nS. Montangero,\nV. Giovannetti,\nand\n9\nT. Calarco, Phys. Rev. A 82, 022318 (2010).\n[47] Y. Zhang and E.-A. Kim, Phys. Rev. Lett. 118, 216401\n(2017).\n[48] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap,\nT. Harley, D. Silver, and K. Kavukcuoglu, International\nConference on Machine Learning , 1928 (2016).\n[49] H. Xu, J. Li, L. Liu, Y. Wang, H. Yuan, and X. Wang,\nnpj Quantum Inf. 5, 82 (2019).\n[50] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez,\nY. Tassa, D. Silver, and D. Wierstra, arXiv:1509.02971\n(2015).\n[51] Sequences generated here have similar ﬁdelities to those\nin Fig. 5 in the main text.\nSupplementary Materials\nSupplementary Method 1: Pseudo code for the algorithms\nHere, we provide the pseudo code for all ﬁve algorithms used in the main text.\nAlgorithm 1: Stochastic Gradient Descendent (SGD).\nSet initial guess of J = [J1, J2, · · · , JN]T randomly\nfor iteration = 1, N iter\n,\nGenerate a random unit vector v\n,\nSet J+ = J + αv, J−= J −αv\n,\nCalculate the gradient g = F (J+)−F (J−)\n2α\n,\nUpdate J ←J −βg\n,\nrestrict J to the range [Jmin, Jmax]\nend for\nAlgorithm 2: Krotov algorithm.\nInitialize J arbitrarily\nCalculate and store |ψi⟩at each step i according to |ψi⟩= e−iHit|ψi−1⟩\nSet co-state at step N as |χN⟩= |φ⟩⟨φ|ψN⟩\nCalculate and store |χi⟩at each step i according to |χi−1⟩= eiHit|ψi⟩\nfor iteration = 1, N iter\n,\nfor i = 1, N\n,\nCalculate |ψi⟩according to |ψi⟩= e−iHit|ψi−1⟩\n,\nUpdate Ji ←Ji + Im⟨χi|∂JH(Ji)|ψi⟩\n,\nend for\n,\nFidelity F ←| ⟨φ|ψN⟩|2\n,\nSet co-state at step N as |χN⟩= |φ⟩⟨φ|ψN⟩\n,\nCalculate and store |χi⟩at each step i according to |χi−1⟩= eiHit|ψi⟩\n,\nRestrict J to the range [Jmin, Jmax]\nend for\nAlgorithm 3: Tabular Q-learning (TQL).\nInitialize Q(s, a) = 0 for all s ∈S, a ∈A.\nInitialize the agent state s0\nfor iteration = 1, N iter\n,\nfor i = 1, N\n,\nWith ϵ probability choose ai randomly, otherwise ai = arg max\na\nQ(si−1, a)\n,\nTake the action ai, evaluate the reward ri and |ψi⟩\n,\nSet si as the nearest point in S to |ψi⟩\n,\nUpdate Q(si−1, ai) according to Eq. (8)\n,\nBreak if 1 −F < 10−3\n,\nend for\n10\nend for\nAlgorithm 4: Deep Q-learning (DQL).\nInitialize memory R as empty\nInitialize the evaluation network Θ, and target network Θ−←Θ\nfor iteration = 1, N iter\n,\nInitialize |ψ⟩= |0⟩and s0\n,\nfor i = 1, N, do\n,\nWith ϵ probability choose ai randomly, otherwise ai = arg max\na\nQ(si−1, a; Θ)\n,\nTake the action ai, and evaluate the reward ri and state si\n,\nStore experience ei = (si−1, ai, ri, si) in memory R\n,\nif t is divisible by tlearn\n,\nSample minibatch of experiences ek\n,\nSet yk = rk + γ maxa′ ˆQ(sk, a′; Θ−)\n,\nUpdate Θ by minimizing L = [yk −Q(sk−1, ak; Θ)]2\n,\nend if\n,\nEvery C times of learning, set Θ−←Θ\n,\nBreak if 1 −F < 10−3\n,\nend for\nend for\nAlgorithm 5: Policy Gradient (PG).\nInitialize the network parameters Θ arbitrarily\nfor each episode do\n,\nfor t = 1 to T −1 do\n,\nCompute the output of the network p = P(s; Θ)\n,\nChoose action according to the probability distribution p\n,\nCompute vt = Pt\ni=1 γiri\n,\nStore st, at, vt\n,\nend for\n,\nUpdate the network via Θ ←Θ + α PT\nt=1 ∇Θ log P(st; Θ)vt\nend for\nSupplementary Discussion 1: Eﬀects of bounds and\ndiscrete control ﬁeld for SGD and Krotov methods\nHere, we consider the situation that the control ﬁeld\nis bounded for SGD and Krotov, and the results are\nshown in Fig. S1. Fig. S1a shows the results for the SGD\nmethod, with the blue line identical to that in Fig. 2 of\nthe main text (no restriction) and the black one show-\ning results after J is restricted between 0 and 1. We see\nthat imposing a restriction on the available range of the\ncontrol ﬁeld does not change the results much, because\nthe search by the SGD algorithm is essentially local: the\nalteration of J is small in each step and it is unlikely to\nbuild up a signiﬁcant variation of J in the ﬁnal results.\nThis fact can also be seen from Fig. 3a of the main text:\nthe strength of control ﬁeld is mostly within the range\nof [0, 1] so that the restriction has minimal eﬀect on the\nresults.\nThe situation is diﬀerent for the Krotov method. As\ncan be seen in Fig. S1b, for N < 30, the result from the\nKrotov method with restriction of Ji ∈[0, 1] (black line)\nhas considerably lower average ﬁdelities than that with-\nout restriction (red line, identical to the results shown\nin Fig. 2 of the main text).\nThis is because the Kro-\ntov method makes large updates on the values of the\ncontrol ﬁelds, as can be seen from Fig. 3b of the main\ntext where the magnitude of Ji can be above 20. Re-\nstricting the control ﬁeld to a much narrower range will\nseverely compromise the ability of the algorithm to ﬁnd\nsolutions with high ﬁdelities. An exception is N = 6, for\nwhich the results with restriction has higher average ﬁ-\ndelity. While the true reason remains unclear, we suspect\nthis is because that the agent happens to have found a\nrelatively good local minimum which outperforms many\nother cases. We believe that the algorithm succeeds in\nthis particular case but not in general. After all, the aver-\naged ﬁdelity is below 0.6 for both lines, with or without\nrestrictions.\nFor N > 30, the results without restric-\n11\n0.2\n0.6\n1\nF\na\n0\n10\n20\n30\n40\n50\nN\n0.2\n0.6\n1\nF\nb\n0\n10\n20\nJmax\n0.4\n0.7\n1\nF\nSupplementary Figure S1:\nEﬀect of bounds of the con-\ntrol on the average ﬁdelities as functions of N for SGD\nand Krotov methods. (a) F versus N for the SGD method,\nwithout (blue) and with (black) restriction of Ji ∈[0, 1]. (b)\nMain panel: F versus N for the Krotov method, without (red)\nand with (black) restriction of Ji ∈[0, 1]. Inset: F versus Jmax\nfor N = 20, where Ji is restricted to Ji ∈[1 −Jmax, Jmax].\ntion on the range of the control approaches almost one\n\u00001 −F < 10−7\u0001\n, and those with restriction is lower than\none but very close (for example, F = 0.9822 for N = 30).\nThis indicates that having more pieces in the control se-\nquence can greatly help the Krotov algorithm to achieve\nhigher ﬁdelities despite limited strength of control ﬁelds.\nThe inset of Fig. S1b gives information on how the\ntwo points given by the vertical dashed line at N = 20\nconnects when we expand the range of the control ﬁeld.\nThe bound is given as 1 −Jmax ≤Ji ≤Jmax. When\nJmax is increased from 0 to 20, the averaged ﬁdelity from\nthe Krotov method increases from 0.4 to above 0.8. This\nclearly demonstrates that the range of allowed values of\ncontrol ﬁelds aﬀects the outcome of the Krotov algorithm\nin a signiﬁcant way.\nWe now proceed to consider the eﬀect of discrete con-\ntrol to the averaged ﬁdelities obtained by the algorithms.\nWe start from SGD and Krotov with the range of the\ncontrol ﬁeld unrestricted, and the results are shown in\nFig. S2. In both panels shown, we see that the averaged\nﬁdelities from the SGD method ﬁrst increases for small\nM but quickly saturate. The insensitivity of the SGD\nagainst the discretization of the control ﬁeld is due to the\nfact that SGD updates the control ﬁeld moderately and\ncan ﬁnd suﬃcient control ﬁeld values as desired within a\nrelatively narrow range, even if the values are discretized.\nThis is similar to the reason why the restriction on the\nrange of control ﬁeld has little eﬀect on the results in\nFig. S1a.\nOn the other hand, the averaged ﬁdelities from the\n0.2\n0.6\n1\nF\na\nSGD\nKrotov\n100\n101\nM\n0.2\n0.6\n1\nF\nb\nSupplementary Figure S2: Eﬀect of discrete control ﬁelds\non the averaged ﬁdelity for SGD and Krotov meth-\nods. In the calculation, the strength of control ﬁeld is not\nspeciﬁcally restricted, so the Jmin and Jmax values are deter-\nmined after the algorithm has run. The values of the control\nﬁeld is then mapped to their respective closest discrete values,\nwith the total number of allowed values including Jmin and\nJmax being M + 1. Panel (a) shows the case of N = 6 while\n(b) N = 20, corresponding to the two vertical dashed lines in\nFig. 2 of the main text.\nKrotov method increase as functions of M, but the in-\ncrease is much more pronounced for N = 20 (Fig. S2b)\nthan for N = 6 (Fig. S2a). In Fig. S2b, the averaged\nﬁdelity from Krotov method exceeds that from SGD at\naround M + 1 = 15. The result indicates that successful\nimplementation of the Krotov method depends crucially\non the continuity of the problem, in terms of both the\nnumber of pieces in the control sequence as well as al-\nlowed values of the control ﬁeld. We also note that at\nthe limit M →∞, the extrapolated ﬁdelity values are\nconsistent to results shown in Fig. 2 of the main text,\nproviding a consistency check of our calculations.\nSupplementary Discussion 2: Improving the ﬁdelity\nwith more iterations\nIn Fig. 2 of the main text, we have compared ﬁve algo-\nrithms in terms of the average ﬁdelity versus the maxi-\nmum number of control pieces. To ensure a fair compar-\nison, all algorithms are requested to stop at N iter = 500,\ni.e. after 500 iterations. Here we show that by allowing\nmore iterations, the ﬁdelity of all algorithms can improve,\nand the improvement is particularly pronounced for the\nSGD method.\nFig. S3 shows the average ﬁdelity ver-\nsus the number of iterations, with all other parameters\nand constraints the same as those used in Fig. 2 of the\nmain text.\nFig. S3a shows the results at N = 20 for\n12\n101\n102\n103\n104\nN iter\n0.2\n0.4\n0.6\n0.8\n1\nF\na\n101\n102\n103\n104\nN iter\nb\nSGD\nKrotov\nTQL\nDQL\nPG\nSupplementary Figure S3:\nAverage ﬁdelity versus num-\nber of iterations N iter. (a) Maximum number of control\npieces N = 20 and (b) N = 50. F is obtained by averaging\nover 100 runs. The vertical grey dashed lines at N iter = 500\nin both panels correspond to the results shown in Fig. 2 of\nthe main text with the respective N values.\nwhich SGD has a relatively low ﬁdelity (around 0.6 at\nN iter = 500). As the iteration continues, the ﬁdelity of\nSGD improves substantially, reaching 1 at N iter ≳500.\nOn the other hand, the ﬁdelities for other methods do\nnot change much as the number of iteration is increased.\nFig. S3b shows the results at N = 50. We see that re-\nsults from the Krotov method reaches 1 at N iter ∼20,\nthose from DQL improves slightly after N iter = 50, but\nagain the increase of ﬁdelity is most pronounced for the\nSGD method, with the ﬁdelity increasing from 0.4 at\nN iter = 500 to 1 at N iter = 10000. It is also interest-\ning to note that the ﬁdelity output from TQL does not\ncarry a considerable increase, likely because the non-zero\nfailure rate cannot be decreased simply by adding more\niterations.\nWe therefore conclude that (1) The result\nfrom SGD is most sensitive to the total number of iter-\nations: the ﬁdelity can reach 1 as long as one iterates\nthe algorithm long enough. However, this process could\nbe very resource-consuming compared to other methods\nthat can have high ﬁdelity values for a much smaller num-\nber of iterations. (2) The TQL method is most insensi-\ntive to more iterations as its intrinsic failure rate cannot\nbe suppressed this way. Adding more iterations will not\nincrease its ﬁdelity output by a notable amount.\nSupplementary Discussion 3: Target state\ndependence of the learning outcome\nIn all results shown in the main text, our target state is\nalways |1⟩. In order to provide a more complete picture,\nwe take states on the equator of the Bloch sphere\n|φ⟩=\n1\n√\n2\n\u0000|0⟩+ eiϕ|1⟩\n\u0001\n,\n(S1)\nas examples of other possible target states. This set of\nstates has one sole parameter ϕ so that one may plot the\naverage ﬁdelity versus ϕ in a ﬁgure, which we show in\n0\n0.5\n1\n1.5\n'=:\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nF\nSGD\nKrotov\nTQL\nDQL\nPG\nSupplementary Figure S4: The dependence of the learn-\ning outcome (average ﬁdelity) on the target states.\nThe target state is deﬁned in Eq. (S1) with ϕ as its sole pa-\nrameter.\nFig. S4 (note that N iter = 500). First, we see that the\nKrotov method is not sensitive to the change of states,\nand the ﬁdelity is maintained at about 0.8 for all target\nstates concerned. All other three methods have outputs\nthat vary a lot with the change of target states. Taking\nDQL as an example, the average ﬁdelity reaches 1 for\nϕ = 1.5π, but is lower than 75% for ϕ = 1.25π. While\nit may not be meaningful to provide a full explanation,\nwe attribute the variance as the result of a discretized ac-\ntion space, i.e. the allowed actions are limited and cannot\ncover all points on the Bloch sphere. We note that de-\nvelopments are on-going in order to allow reinforcement\nlearning to choose action from a continuous set [48, 50].\nTheir implications on quantum physics warrant further\nstudies.\nSupplementary Discussion 4: Noise eﬀect\nHere, we brieﬂy discuss the robustness of the control\nsequences found by diﬀerent algorithms against noises.\nWe ﬁrst generate the control ﬁelds without noises, choos-\ning one for each algorithm that possesses the highest ﬁ-\ndelity from 20 runs. Then we feed noise into the evolution\nvia the control ﬁeld: Ji →Ji + δJi, where the error term\nδJi is uniformly drawn from [−ϵ, ϵ] with ϵ being the noise\nlevel. Then for each control sequence we produce a set of\n100 realizations of noises, and the resulting ﬁdelities are\naveraged. In Fig. S5 we show the results. As is clear from\nthe ﬁgure, the average ﬁdelities drop most signiﬁcantly\nfor Krotov and SGD, but only moderately for TQL and\nDQL. We believe that it is because the control sequences\nproduced by TQL and DQL are less complex, i.e. having\nshorter time durations and smaller jumps in the values\nof the control ﬁelds.\n13\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.6\n0.7\n0.8\n0.9\n1\nF\nSDG\nKrotov\nTQL/DQL/PG\nSupplementary Figure S5:\nAverage ﬁdelity versus the\nnoise level in diﬀerent algorithms. The target state is\n|1⟩, N = 20. Each point of F is an average over 100 runs.\nSupplementary Discussion 5: Multiqubit pulse\nsequence\nThe optimum pulse sequence for the 8-qubit state\ntransfer problem are shown in Table. S1-S4. The the\ncontrol sequences here have similar ﬁdelities to the opti-\nmum runs shown in Fig. 5(d)-(g) in the main text.\nStep\nB1\nB2\nB3\nB4\nB5\nB6\nB7\nB8\n1\n33.9 31.36 31.15 23.67 1.65\n5.8\n21.57 15.68\n2\n0.21 24.44 36.56 11.84 7.04 15.11 26.98 29.16\n3\n0.53 21.78 37.72 6.39 14.87 7.81 34.48 31.83\n4\n12.63 2.44 14.47 15.03 29.5 10.84 12.38 31.77\n5\n28.37 37.31 18.53 7.53 11.12 20.03 24.04 4.72\n6\n34.05 18.42 21.82 28.24 19.88 32.13 12.33 29.21\n7\n37.09 31.17 24.97 10.51 4.75 27.35 34.32 35.19\n8\n34.43 13.51 38.21 27.31 29.17 30.08 1.71\n7.2\n9\n2.12\n2.8\n0\n6.72 12.67 20.04 18.77 35.3\n10\n29.77 36.44 37.46 21.47 7.86\n0.98 21.82\n1.9\n11\n25.8\n8.82\n1.41\n0.29\n7.72 10.17 18.4 28.23\n12\n20.38 37.39 20.47 9.93 26.81 19.63 35.93 14.97\n13\n4.97 22.06 38.41\n40\n37.41 8.08 36.03 4.24\n14\n10.71 33.98 25.2 34.33 24.52 26.29 27.03 23.94\n15\n0.3\n11.93 15.24 5.52 14.06 5.94\n0.54 24.58\n16\n0.84 25.63 29.83 32.55 2.03 10.75 9.13 26.51\n17\n18.12 27.29 8.65 11.68 37.27 4.66\n7.72 25.27\n18\n28.43 33.35 18.71 13.84 14.58 18.06 32.33 30.47\n19\n7.41 14.36 13.81 16.79 0.88 27.72 26.64 0.11\n20\n20.23 21.58 15.56 30.26 2.91 28.78 38.92 26.01\nSupplementary Table S-I: Pulse proﬁle for 8-qubit state trans-\nfer problem. The result is the one with highest ﬁnal ﬁdelity\namong 100 runs of the Krotov algorithm[51].\nStep\nB1\nB2\nB3\nB4\nB5\nB6\nB7\nB8\n1\n21.72 10.92 18.89 29.21 12.55 6.21\n5.9\n10.4\n2\n30.38 23.33 2.54 14.49 17.5 38.92 35.58 21.58\n3\n14.33 18.19 16.49 5.18\n7.02\n27.2 21.83 39.74\n4\n29.09\n16\n26.28 37.61 20.51 18.65 27.32 23.52\n5\n18.66 8.07 36.99 15.61\n3.8\n20.36 5.54\n6.89\n6\n33.59 15.99 3.08\n8.13 20.27 4.34 22.21 37.9\n7\n2.71\n3.72\n2.86 37.55 1.54 31.14 8.68 19.23\n8\n33.23 28.24 29.05 1.45 33.59 17.74 33.82 35.73\n9\n15.95 32.9\n9.55 12.14 4.82\n0.39\n18.7 37.08\n10\n38.96 39.31 15.11 15.45 21.56 6.29\n4.26\n4.75\n11\n15.54 38.26 23.76 14.82 0.81 37.93 15.23 12.11\n12\n3.18 22.49 39.74 30.15 27.02 1.57 27.61 0.58\n13\n24.28 33.64 29.41 26.82 29.1 11.96 14.37 19.22\n14\n20.57 2.68 24.79 39.48 8.48\n8.06 23.21 39.6\n15\n29.03 29.28 37.98 31.63 12.09 24.96 31.67 39.74\n16\n37.28 17.21 5.65\n4.65 12.69 28.18 29.02 15.14\n17\n4.48\n2.19 22.02 19.95 24.55 24.57 17.49 15.59\n18\n19.8\n4.43\n12\n36.86 36.75 3.14 20.75 12.66\n19\n9.3\n19.9 30.61 24.88 29.13 24.43 36.7 12.03\n20\n11.02 37.63 38.67 16.97 14.76 1.47 23.02 25.28\nSupplementary Table S-II: Pulse proﬁle for 8-qubit state\ntransfer problem.\nThe result is the one with highest ﬁnal\nﬁdelity among 100 runs of the SGD algorithm[51].\nStep B1 B2 B3 B4 B5 B6 B7 B8\n1\n0\n0\n0\n0\n0\n0\n40\n0\n2\n0\n0\n0\n0\n0\n0\n40\n0\n3\n0\n40 40 40 40 40 40\n0\n4\n40 40\n0\n40 40 40 40 40\n5\n40 40\n0\n40 40 40 40 40\n6\n0\n40\n0\n0\n40\n0\n0\n40\n7\n0\n40\n0\n0\n40\n0\n0\n40\n8\n0\n40\n0\n0\n40\n0\n0\n40\n9\n0\n40\n0\n0\n40\n0\n0\n40\n10\n0\n40\n0\n0\n40\n0\n0\n40\n11\n0\n40\n0\n0\n40\n0\n0\n40\n12\n40\n0\n40 40\n0\n0\n40\n0\n13\n0\n40\n0\n0\n40\n0\n0\n40\n14\n0\n40\n0\n0\n40\n0\n0\n40\n15\n0\n40\n0\n0\n40\n0\n0\n40\n16\n0\n40\n0\n0\n40\n0\n0\n40\n17\n0\n40\n0\n0\n40\n0\n0\n40\n18\n0\n40\n0\n0\n40\n0\n0\n40\n19\n0\n40\n0\n0\n40\n0\n0\n40\n20\n0\n40\n0\n0\n40\n0\n0\n40\nSupplementary Table S-III: Pulse proﬁle for 8-qubit state\ntransfer problem.\nThe result is the one with highest ﬁnal\nﬁdelity among 100 runs of the DQL algorithm[51].\n14\nStep B1 B2 B3 B4 B5 B6 B7 B8\n1\n0\n0\n40 40\n0\n40 40 40\n2\n0\n40 40\n0\n40 40 40 40\n3\n0\n0\n40 40\n0\n40 40 40\n4\n0\n40 40 40\n0\n0\n0\n0\n5\n40\n0\n40\n0\n0\n0\n40\n0\n6\n0\n0\n0\n40\n0\n40 40\n0\n7\n0\n0\n40\n0\n0\n0\n0\n40\n8\n0\n0\n40\n0\n40\n0\n0\n0\n9\n0\n0\n40\n0\n40\n0\n0\n0\n10\n0\n0\n40 40\n0\n40 40 40\n11\n40\n0\n0\n0\n0\n0\n40\n0\n12\n0\n40 40 40\n0\n0\n0\n0\n13\n40\n0\n40 40\n0\n0\n40 40\n14\n0\n40\n0\n40\n0\n40\n0\n0\n15\n40\n0\n40 40 40\n0\n0\n40\n16\n0\n40\n0\n0\n0\n0\n40 40\n17\n0\n40 40\n0\n40 40 40 40\n18\n40\n0\n40 40 40 40\n0\n40\n19\n40\n0\n40 40 40\n0\n0\n40\n20\n0\n0\n0\n0\n0\n40 40\n0\nSupplementary Table S-IV: Pulse proﬁle for 8-qubit state\ntransfer problem.\nThe result is the one with highest ﬁnal\nﬁdelity among 100 runs of the PG algorithm[51].\n",
  "categories": [
    "quant-ph"
  ],
  "published": "2019-02-06",
  "updated": "2021-05-05"
}