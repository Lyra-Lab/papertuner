{
  "id": "http://arxiv.org/abs/1902.03765v1",
  "title": "Latent Space Reinforcement Learning for Steering Angle Prediction",
  "authors": [
    "Qadeer Khan",
    "Torsten Schön",
    "Patrick Wenzel"
  ],
  "abstract": "Model-free reinforcement learning has recently been shown to successfully\nlearn navigation policies from raw sensor data. In this work, we address the\nproblem of learning driving policies for an autonomous agent in a high-fidelity\nsimulator. Building upon recent research that applies deep reinforcement\nlearning to navigation problems, we present a modular deep reinforcement\nlearning approach to predict the steering angle of the car from raw images. The\nfirst module extracts a low-dimensional latent semantic representation of the\nimage. The control module trained with reinforcement learning takes the latent\nvector as input to predict the correct steering angle. The experimental results\nhave showed that our method is capable of learning to maneuver the car without\nany human control signals.",
  "text": "Latent Space Reinforcement Learning for Steering\nAngle Prediction\nQadeer Khan\nTUM and Artisense\nTorsten Sch¨on\nAudi Electronics Venture\nPatrick Wenzel\nTUM and Artisense\nAbstract—Model-free reinforcement learning has recently been\nshown to successfully learn navigation policies from raw sensor\ndata. In this work, we address the problem of learning driving\npolicies for an autonomous agent in a high-ﬁdelity simulator.\nBuilding upon recent research that applies deep reinforcement\nlearning to navigation problems, we present a modular deep\nreinforcement learning approach to predict the steering angle\nof the car from raw images. The ﬁrst module extracts a low-\ndimensional latent semantic representation of the image. The\ncontrol module trained with reinforcement learning takes the\nlatent vector as input to predict the correct steering angle. The\nexperimental results have showed that our method is capable of\nlearning to maneuver the car without any human control signals.\nI. INTRODUCTION\nReinforcement learning (RL) is gaining interest as a promis-\ning avenue to training end-to-end autonomous driving policies.\nThese algorithms have recently been shown to solve complex\ntasks such as navigation from raw vision-sensor modalities.\nHowever, training those algorithms require vast amounts of\ndata and interactions with the environment to cover a wide\nvariety of driving scenarios. The collection of such data if even\npossible is costly and time-consuming. Simulation engines can\nhelp to easily collect driving data at scale. By interacting\nwith a simulator, reinforcement learning can be used to train\nmodels that can map vision inputs to steering commands. This\nidea has been applied in recent works on autonomous driving\nnavigation [1], [2].\nIn this paper, we present an approach to learn driving\npolicies based on a low-dimensional state space representation.\nThe key idea is to abstract the perception system from the\ncontrol model such that the control model can be trained\nand optimized independently [3]. This is shown in Figure 1.\nThe system is organized as follows. First, a perception model\nuses raw sensor readings captured by an RGB camera placed\nat the front of the car as inputs to the system. This model\nprocesses theses inputs and produces an output map containing\na pixel-wise semantic representation of the scene. Second, the\nsemantic map of the scene is fed to the control model to\nproduce a low-dimensional vector. The advantage of using a\nsemantic representation of the scene instead of raw camera\nimages is described below:\n• Figure 2 shows how two weather conditions have dif-\nferent RGB inputs but the same semantic pixel labels.\nHence, the control model does not separately need to\nENCODER\nL \nA \nT \nE \nN \nT \nCONTROL  \nMODULE\nSteering \nAngle\nPERCEPTION \nMODULE\nDECODER\nSemantic \nLabels \nRaw\nImage\nFig. 1. The perception model is trained as an encoder-decoder architecture\nwithout any skip connections. The encoder sub-module ﬁrst embeds the raw\nimage into a low-dimensional latent representation. The decoder sub-module\nreconstructs the semantic scene from this latent vector. We directly feed the\nsemantic latent embedding as an input to the control module instead of the\nsemantic labels.\nlearn to predict the correct steering commands for each\nand every weather condition.\n• The semantic labels can precisely localize the pixels\nof important road landmarks such as trafﬁc lights and\nsigns. The status/information contained on these can then\nbe read off to take appropriate planning and control\ndecisions.\n• A high proportion of the pixels have the same label as its\nneighbours. This redundancy can be utilized to reduce the\ndimensionality of the semantic scene. Hence, the number\nof parameters required to train the control model can then\nalso be reduced.\nClear\nNoon \nRainy  \nWeather \nSemantic  \nLabels \nFig. 2.\nFor the perception model we take in raw image data as obtained\nfrom the car’s camera and output the semantic segmentation of the scene.\nNotice that irrespective of the weather condition the semantics of the scene\nremain the same. Since the perception model bears the burden of producing\nthe correct semantic labels, the control model would be agnostic to changes\nin lighting, weather, and climate conditions.\nThe perception model, which is used to produce the seman-\ntic labels of the scene from the RGB camera is trained as an\nencoder-decoder architecture. The network architecture which\nis being used is a modiﬁed version of the one proposed by [4].\nThe structure and the parameters of the model is described in\nFigure 3. The encoder ﬁrst encodes the information contained\narXiv:1902.03765v1  [cs.LG]  11 Feb 2019\nFig. 3. Encoder-decoder architecture used to train the segmentation perception\nmodel. The convolution layers numbered 15 and 16 have a kernel size of 4,\nstride of 1, and no padding. All other convolution layers have kernel size 4,\nstride of 2, and padding of 1. All the Leaky ReLU activation functions have\na negative slope of −0.2. The output of the model has 13 channels with each\ncorresponding to one of the semantic labels. The output of the last layer of the\nencoder (Layer 15) is fed to the control model to predict the correct steering\ndirection.\nin the input data to a lower dimensional latent vector. The\ndecoder takes this latent vector and attempts to reconstruct\nthe semantics of the scene. The output of the decoder is of\nthe same size as the image, but having 13 channels with\neach representing the probability of occurrence of one of the\nsemantic labels. Note that the semantic classes would be highly\nimbalanced since labels for commonly occurring entities such\nas the road would be more frequent than that for trafﬁc lights.\nTherefore, the model is trained by minimizing the weighted\nsum of the categorical cross-entropy of each pixel in the\nimage. The log-likelihood of a softmax distribution between\npredictions p and targets t is calculated as follows:\nLi =\nX\nj\nti,j log(pi,j)wj,\nwhere i denotes the pixel and j denotes the class. The\nweight wj of each semantic label is inversely proportional to\nits frequency of occurrence in the data set. Figure 4 shows\nthe relative weights of the semantic labels with their sum\nnormalized to 1.\nII. RELATED WORK\nSemantic segmentation. The visual understanding of complex\nenvironments is an enabling factor for self-driving cars. The\nauthors of [5] provide a large-scale data set with semantic ab-\nstractions of real-world urban scenes focusing on autonomous\ndriving. Semantic segmentation allows us to decompose the\nscene into a pixel-wise representation of classes relevant to\nTrafficSigns\nPedestrians\nWalls\nPoles\nOther\nVehicles\nRoadLines\nFences\nNone\nVegetation\nBuildings\nSidewalks\nRoads\nSemantic Labels\n0.0\n0.1\n0.2\n0.3\n0.4\nRelative Weights\nRelative Weights of Semantic Labels\nFig. 4. Relative weights of the semantic labels with their sum normalized to\n1. The weights are inversely proportional to the frequency of occurrence of\nthe corresponding label.\ninterpret the world. This is especially helpful in the context of\nself-driving cars, e.g. in order to discover drivable areas of the\nscene. It is therefore possible to segment a scene into different\nclasses (e.g. road and not road) and weight the different\nimportance levels of distinct classes for driving systems [6].\nImitation learning. The use of supervised learning methods\nto train driving policies for autonomous agents is a well-\nknown and common approach. The ﬁrst step towards using\nneural networks for the task of road following dates back to\nALVINN [7] in 1989. In this work, a shallow neural network\nis used to map the input images and laser range ﬁndings\ndirectly to steering actions. More recently, [8] proposed to use\nan end-to-end deep convolutional neural network for the task\nof lane following. This approach demonstrated good results\nin relatively simple real-world driving scenarios. However,\none major drawback of end-to-end learning system is the\navailability of enough labeled training data and therefore the\npossibility to generalize well to unseen scenes.\nReinforcement\nlearning.\nIn\nreinforcement\nlearning\nap-\nproaches, one crucial factor is the choice of the state space\nrepresentation. A lot of prior work on deep reinforcement\nlearning aim to learn purely from experience and discover\nthe underlying structure of the problem automatically. This\nis a challenging problem, especially for sensorimotor control\ntasks as self-driving cars [9], [10]. End-to-end vision-based\nautonomous driving models trained by reinforcement learning\nhave a high computational cost [1]. Training on a representa-\ntive lower dimensional latent space allows for a less number\nof model parameters. The reduced number of parameters\nwould allow for a signiﬁcant speedup in training time. The\nauthors of [11] proposed using variational inference to estimate\npolicy parameters, while simultaneously uncovering a low-\ndimensional latent space of controls. Similarly, the approach\nby [12] has analyzed the utility of hierarchical representations\nfor reuse in related tasks while learning latent space policies\nfor reinforcement learning.\nE\nPERCEPTION\nUTILITY \nFUNCTION \nRGB\nSTATE\nENVIRONMENT\nAGENT\nREWARD\nACTION\nCONTROL\nFig. 5. This ﬁgure shows the outline of training an agent using reinforcement\nlearning. The CARLA [1] environment yields the state (RGB image) from the\ncolor camera placed at the front of the vehicle. The perception model of the\nagent converts the RGB image into the semantics of the scene. The dotted\nblue arrow shows that a low-dimensional representation of the semantic map\nis fed to the control model which decides on the appropriate action to be taken\nbased on the current state. The agent interacts with the CARLA environment\nby executing the relevant action. This action changes the state of the car in\nthe environment and hence a new state is generated and the cycle is repeated.\nNote, that there is no human supervision in the entire loop. The control model\nis therefore, trained from the reward signals that are dependent on the utility\nfunction formulated only once at the start of the training. The orange arrow\nto the control model indicates that its weights are updated based on these\nreward signals.\nIII. BACKGROUND\nWe demonstrate our approach by training an autonomous\nagent on data obtained from a high-ﬁdelity simulator with a\nreinforcement learning algorithm. We formulate the problem\nas a partially observable Markov decision process (POMDP).\nBelow, we will cover the fundamentals of Q-learning.\nIn reinforcement learning, we assume an agent interacting\nwith an environment. At each time step t, the agent executes\nan action at ∈A from its current state st ∈S, according to\nits policy π : S →A. The received reward at time t which is\nobtained after interaction with the environment is denoted by\nrt : S ×A →R and transits to the next state st+1 according to\nthe transition probabilities of the environment. The policy is\nconsidered optimal if it maximizes the expected sum of future\nrewards.\nIV. METHOD\nOur method consists of a perception model that transforms\nimages from the front-facing camera into a semantic represen-\ntation of the scene. We are then able to train a reinforcement\nlearning algorithm on the latent embedding of this state. In RL,\ninstead of collecting supervised labels, we formulate a utility\nfunction. The car is then allowed to explore the environment\nat its discretion and accordingly learn from its experience.\nFigure 5 shows the steps involved in training an RL based\nagent.\nThe agent is provided with the state of the environment,\non the basis of which it takes an action. Actions considered\nfeasible are assigned a positive reward and adverse actions\nare assigned a negative reward. The rewards given are de-\ncided by the utility function deﬁned once before the car\nstarts exploration. With trial and error the car should learn\nhow to maneuver itself for correct decision making without\nany explicit supervision. In our case, the CARLA simulator\nprovides the RGB color image as the state. The perception\nmodel yields the semantics of the scene using this color image.\nThe semantics are fed to the control model, which acts as the\nagent to decide the most appropriate action to be taken.\nOne limitation of RL is that it requires tremendous amount\nof experimental sessions to train [13]. In the paper by [1], the\nRL algorithm on the CARLA simulator was trained for 12\ndays as opposed to 14 hours for imitation learning. Moreover,\nthe input size of the image (84 × 84) was also smaller in\ncomparison with imitation learning (200 × 88). Usually, a\nsmaller input reduces the number of parameters and thus the\nnumber of sessions required for training [11]. As depicted in\nFigure 1, we feed the latent vector containing the semantic\ninformation to the control model instead of the complete scene.\nIf we have the Markov property, then, irrespective of the\npast, the future state is only dependent on the current state.\nHence, in our case, the next state st+1 determined by the action\ndecision to be taken by the control model at time instance t\nwould only be dependent on the state st (RGB image). This\ncan mathematically be described as:\nP(st+1|st) = P(st+1|s0, s1, s2, . . . , st)\nIn the following we describe the state space s, actions a to\nbe taken by the agent, reward r received, discounting factor\nγ, as well as the transition probabilities P.\n• State, s are the set of observations furnished by the\nenvironment on the basis of which the agent takes an\naction. As illustrated in Figure 5, the CARLA simulator\nfurnishes the color images from the monocular camera\nplaced at the front of the car. The perception model\nconverts this to a quasi-state latent semantic vector of\nsize 64 i.e. s ∈R64.\n• Action, a are the actions with which the agent can interact\nwith the environment. The steering values are normalized\nbetween −1 and 1. To ﬁnd the optimal policy we shall\nbe using the off-policy based Q-learning algorithm which\nrequires a discretized action space. We therefore, quantize\nthe steering angle into 3 coarse values i.e. −0.4, 0 and,\n0.4.\n• Reward, r is the immediate reward or penalty received as\na result of the agent executing an action and transitioning\nfrom current state st to next state st+1. Since, we would\nlike the car to drive for as long as possible, a reward\nof 1 is received at every step when the car is in the\ndriving lane. If the car drives off-lane or off-road, a\nsmaller reward or penalty is received. A reward of −5\nis received either when the car is completely (100%) in\nthe other lane or (50%) off-road. The episode ends if the\ncar crashes with an obstacle, is of-road by more than 50%\nor number of steps taken is greater than 500. The reward\nfunction at timestep t can mathematically be expressed\nas:\nrt =\n(\n1,\nr = 0 & l = 0,\n1 + α ∗l + β ∗r,\nif r > 0 or l > 0,\nwhere r, l depict the percentage of the car off-road and\noff-lane, respectively. α = (Rr −1)∗l and β = 4∗(Rr −\n1) ∗r, with Rr = −5.\n• Discount factor, γ is a numerical value ∈[0, 1], which\ngives the relative importance of the immediate rewards\nin relation to future rewards it is expected to receive.\nA high value of γ(= 1) implies that immediate and\nfuture rewards are all equally important. This provides\na rather farsighted view of the problem at hand. A low\nvalue of γ(= 0) gives a myopic view as it ampliﬁes\nthe signiﬁcance of current rewards while all upcoming\nrewards can be discarded. Discounting is mathematically\nconvenient for continuous tasks with non-terminal states\nas it keeps the expected sum of rewards bounded. Even\nin episodic tasks with terminal states, it still makes sense\nto have a discount factor, due to the uncertainty in\nfuture rewards associated with the stochastic nature of the\nenvironment. The discounted return, Gt after execution\nof a sequence of state, action (and reward) pairs can be\nrepresented as:\nGt =\nN\nX\nk=0\nγkRt+k+1\nWe have used γ = 0.999 with N = 500 as the episode\nlength.\n• Transition probability, P furnishes the probability of\nthe next state being st+1, given the current state st and\nthe action taken by the agent is at i.e. P(st+1|st). A\ntransition probability tensor can be formed from each of\nthe state, action and next state tuples. Each element in\nthis tensor can be described by:\nP a\nss′ = P(st+1 = s′|st = s, at = a)\nThese state transition probabilities, provide a model of the\nenvironment in the sense that based on the actions taken,\nthe agent can anticipate the response of the environment.\nAlgorithms utilizing such prior information about the\nenvironment are referred to as model-based. In our case\nthe states are represented by a latent semantic vector in\na continuous space and obtaining transition probabilities\nfor such a large number of state-action-next state com-\nbination is not feasible even if possible. We therefore,\nresort to model-free methods, such as Q-learning. It is an\nindirect method of determining the optimal policy based\non the value functions of the state action pairs.\nQ-learning\nWe ﬁrst deﬁne the value function for a given state s as the\nexpectation of the sum of future rewards when following a\npolicy π.\nvπ(s) = E [Gt|st = s]\n= E\n\" N\nX\nk=0\nγkrt+k+1|st = s\n#\n= E\n\"\nrt +\nN\nX\nk=1\nγkrt+k+1|st = s\n#\n.\nThe above equation can be recursively represented as a\nBellman expectation equation:\nvπ(s) = E [rt + γvπ(st+1)|st = s] .\n(1)\nLoosely speaking the value of a state is the total sum of\nrewards the agent can receive if it starts from that state. Along\nsimilar lines, the Q-value for a particular state-action pair\nrepresents the expected sum of maximum future rewards. This\nis calculated by after having taken an action a from state s,\nthe agent follows the optimal policy there on-wards. Note that\nthe action a need not necessarily be the most optimal action.\nAnalogous to Equation 1, the bellman optimality for the Q-\nvalues is deﬁned by:\nQ(s, a) = rt + γ arg max\nat+1 Q(st+1, at+1).\n(2)\nIf the Q-values for each state-action pair are known, then at\ninference time we simply choose the action which yields high-\nest Q-value for the given state. Note that whereas our action\nspace is discrete, the state space is continuous. Therefore, it is\nnot possible to tabulate the Q-values for each and every state-\naction pair. Rather, we use approximation methods which are\none way of circumventing the scalability problem associated\nwith Q-tables. We use neural networks to approximate the\nQ-value of an action associated with a certain state, hence\nthe term Deep Q-Networks. This allows similar states which\nmight not even have been seen during the training to be\nassigned action values close to those of similar observed states.\nEquation 3 expresses the loss function used to update the\nweights of the control model. It is the square of the temporal\ndifference (TD) between the predicted Q-values (red) and the\ntarget Q-values (blue). The target Q-value can themselves be\ndetermined by using the same control model and applying the\nBellman equation.\nL = 1\n2[(rt + γ arg max\nat+1 Q(st+1, at+1) −Q(s, a))]2\n(3)\nTo expedite training and convergence of the deep Q-\nnetwork, we use the enhancements proposed by [14]:\n• Separate target network\n• Experience replay\nSeparate target network. Note that we are using the same\nnetwork for ﬁnding both the predicted Q-value and the target\nQ-value. This results in a potentially dangerous feedback loop\nhaving a likelihood for creating instability. This problem can\nbe addressed by using a separate network for calculating\nthe target values which is independent from the primary\nQ-network. Thus, when the weights of the Q-network are\nupdated, the target network remains ﬁxed thus bringing rel-\native stability. However, the target network also needs to\nbe improved and this can be done after a certain number\nof episodes and independent from when the Q-network is\nupdated. The target network has the same architecture as the\nprimary network and is updated by simply copying the weights\nover from the primary network.\nExperience replay. Note that the training samples we receive\nare on an episodic basis pertaining to one particular driving\nsession. These samples would strongly be correlated and\ndependent on another and thus do not fulﬁll the requirement\nof being independent and identically distributed. Hence, if\nwe train every episode independently, then the network is\nonly learning about what the agent is currently doing in\nthe environment for that episode. Therefore, when it sees a\nnew state space, the error is high and so while updating the\nnetwork it overﬁts to this current episode. The networks output\nproducing the Q-values would thus keep ﬂuctuating between\nthe episodes. This problem with training the network can be\ntackled by buffering the past experiences of the agent and\nrandomly drawing a subset of samples from this buffer. The\nrandom sampling of past experiences allows the network to\nlearn from a spectrum of different scenarios rather than just the\ncurrent episode. This allows the network to generalize better.\nV. EXPERIMENTS\nWe evaluate our approach on the CARLA simulator. The\nperception module is trained using the encoder-decoder archi-\ntecture by feeding the RGB images of size 128 × 128 × 3 to\nthe input of the encoder and reconstructing the 128×128×13\nsemantic image from the output of the decoder. We collect data\n(RGB + semantic segmentation) with the car taking random\ndriving decisions. This method of data collection with random\nexploration is representative of the state space that the agent\nwill encounter while training with reinforcement learning.\nThe control module is trained with Q-learning. The vanilla\nQ-learning algorithm requires a discrete state and action space,\nwherein the combination of state action pairs are tabulated. We\ncan do away with the requirement of the discrete state space by\nusing the control module as a functional approximator. Hence,\nthe continuous latent semantic vector produced by the output\nof the encoder can directly be fed to the control module as\na representation of the state (s ∈R64) of the environment.\nAn additional advantage of this is that every state does not\nnecessarily have to be explored. Rather, states which might not\neven have been seen but are similar to those observed during\ntraining can be assigned similar Q-values, thus reducing the\ntraining effort.\nWe constrain the action space to only 3 possible action\nvalues. The advantage of this restriction is that we can speedup\nthe training in comparison with training on a larger action\nspace. Furthermore, we found that a combination of these 3\naction values is enough to execute turns around corners. The\narchitecture of the control module (also referred to as the deep\nQ-network) is described in Table I. The output layer 11, which\nhas 3 output neurons is predicting the Q-value for each of\nthe 3 actions for a particular state represented by the latent\nsemantic vector. Since the Q-values give the expected sum\nof future rewards, it would make sense to select the action\ncorresponding to the highest Q-value, at inference time.\nTABLE I\nARCHITECTURE OF THE CONTROL MODEL. NOTE THAT THE INPUT TO THE\nCONTROL MODULE IS A VECTOR OF SIZE 64, CORRESPONDING TO THE\nSIZE OF THE LATENT VECTOR PRODUCED BY THE ENCODER OF THE\nPERCEPTION MODULE.\nLayer Number\nLayer Type\nLayer Input\nLayer Output\n1\nFully connected\n64\n100\n2\nReLU activation\n100\n100\n3\nFully connected\n100\n50\n4\nReLU activation\n50\n50\n5\nFully connected\n50\n25\n6\nReLU activation\n25\n25\n7\nFully connected\n25\n15\n8\nReLU activation\n15\n15\n9\nFully connected\n15\n8\n10\nReLU activation\n8\n8\n11\nFully connected\n8\n3\nWe use an experience buffer of size 7500 samples. All\nsamples older that this number are discarded to make room\nfor new (state, action, reward, and next state) tuples. Since\nthe aim is to keep driving the car for as long as possible, a\nreward of 1 is given for every step the car is in the driving\nlane. Reward is reduced to −5 as a continuous function,\nif the car is 100% in the other lane or 50% off-road. The\nepisode is terminated if the car crashes or has successfully\nexecuted 500 steps. The rewards are discounted by a factor\nof 0.999 at every step. The primary network is trained with\na batch size of 512 at every step and the target network is\nupdated every 256 steps. The simulation starts off with high\nexploration and gradually reduces towards exploitation. In the\nexploration phase, the actions are randomly selected whereas\nin exploitation, the action corresponding to the highest Q-value\nis chosen. The exploration starts off with a probability of 0.9\nand gradually reduces to less than 0.05 after 105 steps as\nshown in Figure 6. Table II enumerates values of some of the\nimportant parameters for training the Q-learning algorithm.\nA. Discussion\nIn order to visually check the performance of the trained\npolicies, in the following some links to videos and their\ndiscussion is given. The video1 is an example of the car\nlearning to execute a right turn with reinforcement learning.\nIt can be observed that the steering of the car has many jerks.\n1https://youtu.be/Sg3YkQEuE k\nTABLE II\nTHIS TABLE SUMMARIZES INFORMATION OF SOME OF THE IMPORTANT\nPARAMETERS USED FOR TRAINING THE Q-LEARNING ALGORITHM.\nParameter\nValue\nSize of state space s\n64\nSize of action space a\n3\nSize of experience replay buffer\n7500\nDiscount factor γ\n0.999\nSteps before updating target network\n256\nBatch Size\n512\nMaximum episode length N\n500\n0\n20000\n40000\n60000\n80000\n100000\nSteps taken\n0.2\n0.4\n0.6\n0.8\nProbability\nExploration Probability\nFig. 6.\nThis ﬁgure shows the exploration probability as a function of the\nsteps taken.\nThis is because that the action space only has 3 actions and the\ncar’s control is having to jump between the different steering\nvalues. These jerks could be avoided by using a larger action\nspace at the expense of a longer exploration time. It can also\nbe noted that, after having made the turn the car is in the\nother lane most of the time and not in its driving lane. This\nis due to the fact that most of the exploration time was spent\nfor making the turn. This can be improved by making the\nexploration probability also a function of the frequency of the\nstates visited. Therefore, states visited less should have high\nexploration. A related point to note are the Q-values which\nsometime jump drastically between 2 similar states. These\njumps are due to these states not having been explored enough.\nExperiments also revealed that the reward function is for-\nmulated to be more well suited for right than left turns having\nbarriers. Note that in case of 100% exploration, the average\nsteering value would be 0 and the car in expectation would\nmove forward. In case of right turns, from the driving lane, the\ncar eventually enters the other lane and then crashes into the\nbarrier. All this while the reward function gradually reduces\nfrom 1 to −5 in a continuous manner.\nThis is shown in Figure 7 where the car receives a reward\nof 1 while following the green trajectory (driving lane).\nThe reward smoothly transitions from +1 to −5 as it goes\ndeeper into the other non-driving lane (orange trajectory).\nRIGHT TURN\nLEFT TURN\nFig. 7. This ﬁgure shows the problem associated with executing a left turn\nusing the current reward function. Green shows the trajectory of the car in the\ndriving lane while receiving a reward of 1. Orange represents the trajectory\nof the car while in the opposite lane, wherein a reward between 1 and −5 is\nreceived as a continuous function. The exact reward is inverse relation to the\npercentage of the car in the other lane. The purple curve along the road edge,\ndepicts the presence of a barrier/fence. The red crossed marking shows the\npoint of impact of the car with the barrier in which scenario, a −5 reward is\nreceived. Note that in the case of a right turn the reward smoothly transitions\nas a continuous function from 1 to −5 at time of impact with the barrier. On\nthe contrary for the left turn, there is a discontinuity in the reward where it\nsuddenly transitions from 1 just one state before the impact to −5 at the time\nof impact. Such discontinuities in the reward function between similar states\nmake the deep q-learning either slow to converge or even diverge.\nIt eventually receives −5 reward when it collides with the\nbarrier as shown by the red crossed marking. In the case of\na left turn, the car in expectation would never move into the\nother lane, but suddenly crash into the barrier and there would\nbe a discontinuity in the rewards which would jump from 1\nto −5. Note that the deep-Q network is used as a function\napproximator, i.e. similar states are assigned similar Q-values.\nThe difference in states between time of impact of the crash\nand just one step before the crash is effectively minimal. This\nminimal difference should also be reﬂected in the rewards\nthese states receive, since the deep Q-network is trained from\nthese reward signals. This holds true for right turns but for\nleft turns there is a sudden change in rewards from 1 to −5.\nHence, the network being a function approximator would have\ndifﬁculties in assigning the appropriate Q-values to these states\nand would thus be either slow to converge or even diverge\naltogether.\nVI. CONCLUSION\nIn this paper, we have presented a framework to address the\nchallenging problem of vision-based autonomous driving using\nreinforcement learning on a low-dimensional latent space rep-\nresentation. This offers the possibility of training the control\nmodule without any labeled steering angles. We observed\nthat the car learned to execute a turn (albeit with some\nlimitations) just by simply formulating a utility function at the\nstart of the exploration phase. Hence, the need for an expert\ndriver is completely eliminated. Based on the observations we\nalso discussed some methods for further improving the RL\nalgorithm. For future work, we are interested in transferring\nthe policies trained in simulation to the real world.\nREFERENCES\n[1] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun,\n“CARLA: An Open Urban Driving Simulator,” in Conference on Robot\nLearning (CoRL), 2017.\n[2] X. Liang, T. Wang, L. Yang, and E. P. Xing, “CIRL: Controllable\nImitative Reinforcement Learning for Vision-based Self-driving,” in\nEuropean Conference on Computer Vision (ECCV), 2018.\n[3] P. Wenzel, Q. Khan, D. Cremers, and L. Leal-Taix´e, “Modular Vehicle\nControl for Transferring Semantic Information Between Weather Con-\nditions Using GANs,” in Conference on Robot Learning (CoRL), 2018.\n[4] A. B. L. Larsen, S. K. Sønderby, H. Larochelle, and O. Winther,\n“Autoencoding beyond pixels using a learned similarity metric,” in\nInternational Conference on Machine Learning (ICML), 2016.\n[5] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Be-\nnenson, U. Franke, S. Roth, and B. Schiele, “The Cityscapes Dataset\nfor Semantic Urban Scene Understanding,” in IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2016.\n[6] B.-k. Chen, C. Gong, and J. Yang, “Importance-Aware Semantic Seg-\nmentation for Autonomous Driving System,” in International Joint\nConference on Artiﬁcial Intelligence (IJCAI), 2017.\n[7] D. A. Pomerleau, “ALVINN: An Autonomous Land Vehicle in a Neural\nNetwork,” in Neural Information Processing Systems (NIPS), 1989.\n[8] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp,\nP. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang, X. Zhang,\nJ. Zhao, and K. Zieba, “End to End Learning for Self-Driving Cars,”\narXiv preprint arXiv:1604.07316, 2016.\n[9] Y. You, X. Pan, Z. Wang, and C. Lu, “Virtual to Real Reinforcement\nLearning for Autonomous Driving,” in British Machine Vision Confer-\nence (BMVC), 2017.\n[10] N. Xu, B. Tan, and B. Kong, “Autonomous Driving in Reality\nwith Reinforcement Learning and Image Translation,” arXiv preprint\narXiv:1801.05299, 2018.\n[11] K. S. Luck, J. Pajarinen, E. Berger, V. Kyrki, H. Ben Amor, and H. B.\nAmor, “Sparse Latent Space Policy Search,” in Conference on Artiﬁcial\nIntelligence (AAAI), 2016.\n[12] T. Haarnoja, K. Hartikainen, P. Abbeel, and S. Levine, “Latent Space\nPolicies for Hierarchical Reinforcement Learning,” in International\nConference on Machine Learning (ICML), 2018.\n[13] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley,\nD. Silver, and K. Kavukcuoglu, “Asynchronous Methods for Deep Rein-\nforcement Learning,” in International Conference on Machine Learning\n(ICML), 2016.\n[14] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,\nD. Silver, and D. Wierstra, “Continuous control with deep reinforcement\nlearning,” in International Conference on Machine Learning (ICML),\n2016.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CV",
    "cs.RO",
    "stat.ML"
  ],
  "published": "2019-02-11",
  "updated": "2019-02-11"
}