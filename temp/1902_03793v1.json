{
  "id": "http://arxiv.org/abs/1902.03793v1",
  "title": "Understanding over-parameterized deep networks by geometrization",
  "authors": [
    "Xiao Dong",
    "Ling Zhou"
  ],
  "abstract": "A complete understanding of the widely used over-parameterized deep networks\nis a key step for AI. In this work we try to give a geometric picture of\nover-parameterized deep networks using our geometrization scheme. We show that\nthe Riemannian geometry of network complexity plays a key role in understanding\nthe basic properties of over-parameterizaed deep networks, including the\ngeneralization, convergence and parameter sensitivity. We also point out deep\nnetworks share lots of similarities with quantum computation systems. This can\nbe regarded as a strong support of our proposal that geometrization is not only\nthe bible for physics, it is also the key idea to understand deep learning\nsystems.",
  "text": "arXiv:1902.03793v1  [cs.LG]  11 Feb 2019\n1\nUnderstanding over-parameterized deep networks by geometrization\nXiao Dong, Ling Zhou\nFaculty of Computer Science and Engineering, Southeast University, Nanjing, China\nA complete understanding of the widely used over-parameterized deep networks is a key step for AI. In this work we try to give\na geometric picture of over-parameterized deep networks using our geometrization scheme. We show that the Riemannian geometry\nof network complexity plays a key role in understanding the basic properties of over-parameterizaed deep networks, including the\ngeneralization, convergence and parameter sensitivity. We also point out deep networks share lots of similarities with quantum\ncomputation systems. This can be regarded as a strong support of our proposal that geometrization is not only the bible for physics,\nit is also the key idea to understand deep learning systems.\nIndex Terms—over-parameterization, deep networks, geometrization, physics, quantum computation, Riemannian geometry\nI. MOTIVATION\nAre all layers created equal? is a recent work which\naddressed the problem of how sensitive are the parameters\nin an over-parameterized deep network[1]. Their experiments\nshow a heterogeneous characteristics of layers, where bottom\nlayers have a higher sensitivity than top layers. This is an\nexciting observation since this is exactly what the geometry of\nquantum computation told us about deep networks one decade\nago!\nIn our former work[2], inspired by the facts that deep\nnetworks are effective descriptors for our physical world and\ndeep networks share similar geometric structures of physical\nsystems such as geometric mechanics, quantum computation,\nquantum many-body systems and even general relativity, we\nproposed a geometrization scheme to interpret deep networks\nand deep learning systems. The observation of [1] encouraged\nus to apply this scheme on over-parameterized deep networks\nto give a geometric description of such networks.\nIn the following parts of this paper, we will explore the\nsimilarities between deep networks and quantum computation\nsystems. We will transfer the rich geometric structure of\nquantum mechanics and quantum computation systems to deep\nnetworks so that we have an intuitive geometric understanding\nof the basic properties of over-parameterized deep networks,\nincluding network complexity, generalization, convergence\nand the geometry formed by deep networks.\nII. GEOMETRIZATION\nGeometrization of physics is the greatest and the most suc-\ncessful idea in understanding the rules of our physical world in\nhuman history. But why can our world be geometrized? In the\nlast decade, we saw a new trend to combine geometrization\nand quantum information processing to draw a complete new\npicture of our world. Basically this is to regard our world,\nincluding spacetime, material and the interactions among\nthem, as emergent from a complex quantum deep network.\nFrom this point of view, our world is built from deep networks\nand the geometric structure of the physical world emerges from\nthe geometric structure of the underlying deep networks. So\nthe geometrization of physics is essentially the geometrization\nof the underlying quantum deep networks. The success of\ngeometrization of physics indicates that geometrization is also\nthe key to understand deep networks.\nThe similarities between deep networks and physical sys-\ntems, including both classical geometric mechanics and quan-\ntum computation systems, have been addressed in our former\nworks[3][2]. Here for simplicity we only give a brief recap\nof key points we have learned from the geometrization of\nquantum information processing that will be involved in this\npaper.\nA. Geometry of quantum information processing\nIt’s well known that quantum mechanics has a rich geo-\nmetric structure so that we believe quantum mechanics is the\nultimate rule of our world. Quantum information processing or\nquantum computation, which explores the complex structure\nof both quantum states and quantum state evolutions, is the\nultimate tool to describe our world and the rules of quantum\ninformation processing systems can be applied to all physical\nsystems, including deep networks. So what do we know\nalready about quantum information processing systems?\nGigantic quantum state space and the corner of physical\nstates For simplicity we use the most popular model of quan-\ntum information processing, i.e. a quantum state is described\nby a n-qubit system and the quantum information processing\nis described by a quantum circuit model. The quantum state\nspace is huge since the dimension of a n-qubit pure state\nsystem is 2n and the number of possible states is O(22n). In all\nthe O(22n) states, only a tiny zero measure subset, the corner\nof physical states, is physically realizable since the states in\nthis subset can be generated with a polynomial complexity\nfrom a simple initial state such as the product state |00...0⟩.\nQuantum\ncomputational\ncomplexity\nThe\nconcept\nof\nquantum computational complexity plays a key role not\nonly in quantum computation but also in quantum grav-\nity, black hole information problem and quantum phase\ntransition[4][5][6][7][8][9][10]. Basically a quantum algorithm\non a n-qubit system is an unitary transformation U ∈U(2n)\nand its computational complexity C(U) is given by the\ngeodesic distance between the identity operation I and U,\nwhere the geodesic is deﬁned on the Riemannian manifold\nof U(2n). For more details on the geometry of quantum\n2\ncomputation, please refer to [11][7]. Accordingly the state\ncomplexity of a n-qubit quantum system |ψ⟩is deﬁned as the\nminimal complexity of all the quantum algorithms that can\ngenerate |ψ⟩from |00...0⟩, i.e. C(|ψ⟩) = min(C(U), |ψ⟩=\nU|00...0⟩). Since the DOF of a general n-qubit transformation\nU ∈U(2n) is O(2n), obviously its computational complexity\nis O(2n). This is to say, a general n-qubit algorithm can only\nbe achieved by a quantum circuit with O(2n) quantum gates,\nwhich is regarded as non-realizable. What we are interested\nare the polynomial complexity algorithms, which can be used\nto prepare the corner of physical states from the product state\n|00...0⟩.\nQuantum computational complexity and geometry Quan-\ntum computational complexity has a rich geometrical structure.\nFirstly the quantum complexity is deﬁned on the Riemannian\nstructure of the manifold U(2n). A natural question is then,\nwhat’s the curvature of the Riemannian manifold of quan-\ntum computation? It’s shown that this manifold may have\na non-positive curvature everywhere[11][7]. This is to say,\nthe geodesic on this manifold is not stable and it’s initial\nmomentum sensitive. Keen readers can immediately see that\nwe have now a connection between quantum computation\nand the observation of [1]. Secondly, the concept of quantum\ncomputational complexity builds a correspondence or a duality\nbetween quantum states and quantum algorithms. That’s to\nsay, given a quantum state |ψ⟩, we have a correspondent\noptimal quantum algorithm U(|ψ⟩) to prepare it from an\ninitial product state. If we take the quantum circuit of the\nalgorithm U(|ψ⟩) as a network of quantum operations, then\nwe have a duality between quantum states and quantum deep\nnetworks. This duality may play a key role in understanding\nthe geometry of spacetime[12][13][14]. In fact the geometry of\nspacetime is just the geometry of the quantum deep network.\nThe take-home message is, the dual quantum deep network\nof a quantum state is determined by a Riemannian geometry\nof the quantum transformation space, and a quantum deep\nnetwork also generates a Riemannian geometry. So do we have\ntwo Riemannian structures? There are signs to show, if we use\nthe Fisher-Rao metric of the deep network, then they can be\nunited and general relativity can be deduced from it[15][2].\nQuantum mechanics and geometry Finally, even we con-\nsider the most classical quantum mechanics without the fancy\nconcept of quantum complexity, we can also learn something\nthat can be applied to understanding deep networks. The ﬁrst\nobservation is the geometry of quantum state space. It’s well\nknown that quantum mechanics show a probabilistic property\nso that in a projective measurement, the probability that the\nstate falls in an eigen state of the observable is determined\nby the distance between the initial state and the ﬁnal state.\nGeometrically this means the probabilistic property of quan-\ntum mechanics is determined by the Riemannian structure of\nquantum mechanics. The second observation is the geometry\nof quantum evolution. A general quantum state evolution of\na n-qubit system can be written as a sequence of unitary\ntransformations UnUn−1...U1 with Ui ∈U(2n). Obviously\nthis can be regarded as a linear deep network. How about the\nstability of this system? It has been shown that this system\nshow a chaotic property, which means a tiny perturbation\nof the ﬁrst operation U1 will lead to a huge change of the\ncomposite operation UnUn−1...U1.\nWe will see all the afero-mentioned observations can help\nus to understand over-parameterized deep networks.\nIII. GEOMETRIZATION OF OVER-PARAMETERIZED DEEP\nNETWORKS\nA. Over-parameterized deep networks\nWe ﬁrst give a brief summary of the known facts and\narguments about over-parameterized deep networks.\nOver-parameterization By over-parameterized deep net-\nworks, we usually mean the number of network parameters\nis much larger than the number of training data. The over-\nparameterization is in both the width and the depth of deep net-\nworks. Existing works show that over-parameterization plays\na key role in the network capacity, convergency, generalization\nand even the acceleration of the optimization. But how exactly\nthe over-parameterization can affect the performance of deep\nnetworks remains not completely clear to us.\nLocal minima and convergence It’s obvious that over-\nparameterized networks have a large number of local min-\nima. In [16] it’s shown that for over-parameterized deep\nnetwork, with a high probability, all the local minima are\nalso global minima as far as the data are not degenerated.\nA similar argument in [17][18] told us that for sufﬁciently\nover-parameterized deep networks, gradient descent can reach\nlocal minima with a high probability from any initialization\npoint of the network. Of course this is because the over-\nparameterization re-shaped the loss landscape of deep net-\nworks. Can we have an intuitive geometric picture of this\npoint?\nNetwork complexity and generalization Although all the\nlocal minima can all ﬁt the training data well, we know\nthey are not equal since they have different generalization\ncapabilities and we prefer to ﬁnd out a conﬁguration with\ngood generalization performance. Generally the generalization\nof a network is related with the network complexity[19] and\na lower network complexity means a better generalization\nperformance. In [20] it’s shown that the minima that can\ngeneralize well have a larger volume of basin of attraction so\nthat they dominate over the poor ones. This is an interesting\nobservation and we will show this is essentially an analogue\nof the probabilistic characteristics of quantum mechanics and\nit has a geometrical origin.\nLoss landscape Over-parameterization changes the loss\nlandscape. [21] claimed that the locus of global minima is\nusually not discrete but rather an continuous high-dimensional\nsubmanifold of the parameter space. But how the structure of\nthis submanifold changes with the number of parameters is\nstill an open problem.\nImplicit acceleration by over-parameterization In [22] it’s\nclaimed that over-parameterization, especially in the depth\ndirection, works as an acceleration mechanism for the opti-\nmization of deep networks and also this acceleration can not\nbe achieved by a regularization. We will show maybe this is\na misunderstanding of the role of over-parameterization.\nLayers are not created equal For a multilayer deep network,\nit’s a direct question to check if all the layers are equal. The\n3\nrecent work [1] showed that layers have different sentivities\nfor either fully connected networks, convolutional networks or\nresidual networks. What’s the geometry behind this observa-\ntion? We will try to understand this point as an analogue of\nquantum information processing systems.\nB. Geometric picture of over-parameterized deep networks\nThe geometrization of deep networks has been explained\nin [3][2], where we showed that deep networks share the\nsame geometric structure of geometric mechanics and quan-\ntum computation systems. The key observation is that deep\nnetworks are curves to connect the identity transformation\nand the target transformation on the Riemannian manifold of\ndata transformations. We will now see how over-parameterized\ndeep networks can be understood in this geometrization frame-\nwork.\nOver-parameterization\nWhat’s\nthe\nrole\nof\nover-\nparameterization in deep network? How to determine if\na network is properly over-parameterized? In fact we can\nunderstand\nover-parameterization\nby\ncomparing\nit\nwith\nquantum computation systems. In quantum computation we\nhave a gigantic state space and only a zero measure subset,\nthe corner of physical states, is physically realizable. The\nduality between quantum states and quantum algorithms\nshows that this is also true for quantum algorithms. Similarly\nthe space of possible functions between the input and\noutput data of deep networks is also huge and only a small\nsubset of it is physically interesting for us, which is the\nsubset of functions that have a polynomial computational\ncomplexity. So essentially approximating a function by deep\nnetworks is to explore this subset. Compared with quantum\ncomputation systems, an universal shallow network is just a\ngeneral unitary transformation U ∈U(2n), which needs an\nexponential complexity to describe a transformation of data\nstate space. A polynomial deep network is just a polynomial\nquantum circuit that only generate the corner of physical\nstates. From this complexity point of view, deep networks\nare not really universal since they only explore a subset\nof all possible transformations. In over-parameterized deep\nnetworks, increasing the width and depth of the networks\ncan be understood as increasing the number of qubits and\nthe length of the quantum circuit to achieve a quantum\nalgorithm. A key point is that, in order to achieve a quantum\nalgorithm U the complexity of the quantum circuit, which is\nroughly proportional to the depth of the quantum circuit, has\nto exceed the quantum complexity of U.\nLocal\nminima\nand\nconvergence\nHow\nthe\nover-\nparameterization\ncan\nchange\nthe\ndistribution\nof\nlocal\nminima and convergence is not very clear yet. If we compare\ndeep networks with quantum mechanics, we can only say\nthe cost function of deep networks can be regarded as a\nfrustration free Hamiltonian and the global minima are ground\nstates of the frustration free Hamiltonian. This observation is\nclosely related with the concepts of parent Hamiltonian and\nuncle Hamiltonian. But if there is an exact correspondence\nbetween them is still under investigation.\nNetwork complexity and generalization The relationship\nbetween network complexity and generalization capability is\nstraight forward. In our former work to compare deep networks\nwith the image registration problem, we indicated that the\nnetwork complexity can be understood as the deformation\nenergy of a diffeomorphic image transformation. So a lower\nnetwork complexity means a smooth low energy deforma-\ntion. Obviously a smooth image transformation has a better\ngeneralization performance. The observation of [20] that a\nsolutions with a better generalization has a higher probability\nto be found during optimization from a random initialization\nthen has an exact correspondence in quantum mechanics. As\nmentioned in the ﬁrst section, during a projective measure-\nment, the probability of a ﬁnal quantum state |ψf⟩appears\nis related with its distance to the initial quantum state |ψi⟩.\nThis is to say, the probability p(|ψi⟩, |ψf⟩) is determined by\nthe complexity C(U(|ψi⟩, |ψf⟩)) of quantum transformation\nU that transform the initial state to the ﬁnal state so that\np(|ψi⟩, |ψf⟩) ∼e−C(U(|ψi⟩,|ψf⟩)). We see this is exactly what\nhappens in over-parameterized deep networks. Here a better\ngeneralizaiton means a lower network complexity and a higher\nprobability that this network conﬁguration is found during\noptimization. Obviously we also have a relationship p ∼e−C\nbetween the probability and the complexity. So we can claim\nthat the probability that a deep network conﬁguration is found\nby optimization is determined by the network complexity,\nwhich is geometrically the Riemannian distance between the\ntransformation achieved by this network and the identity trans-\nformation I. It’s very interesting to see classical deep networks\nshow the same probabilistic property of quantum mechanics.\nFor us it’s more interesting to check if this observation can be\nused to understand quantum mechanics from a deep network\npoint of view, because the measurement problem of quantum\nmechanics is still not fully understood. Can the commonly\nused decoherence picture of quantum measurement can be\nformulated as a training process of deep networks?\nLoss landscape It’s straight forward to see that over-\nparameterized deep network has a locus of global minima as\nan high-dimensional submanifold of the parameter space. But\nwe are not clear about the exact structure of this submani-\nfold and how it will change with the increasing number of\nnetwork parameters. For example, we have no idea if this\nhigh-dimensional submanifold is a connected or a separated\nmanifold or even has a fractal-like structure. We highly suspect\nthat the locus of global minima has a fractal structure since the\nnetwork is nonlinear and the sensitivities of different layers\nare different as will be further addressed in the following\ndiscussions.\nImplicit acceleration by over-parameterization Can the\nover-parameterization provide an implicit acceleration of the\noptimization as claimed in [22]? To clarify this, we ﬁrst restate\nthe argument of [22], in which a linear neural network is\nconsidered as follows: X\n:= Rd and Y := Rk are the\ninput and output data space. A N-layer linear network is\nused to ﬁt a training set (xi, yi)m\ni=1 ∈X × Y and the ↕2\nloss function Pm\ni=1(ˆyi −yi)2 is used, where ˆyi is the output\nof the network given the input xi. The parameters of the\ndepth-N linear network are W1, W2, ..., WN and the end-to-\nend weight matrix is given by We = WNWn−1...W1 so that\nLN(W1, W2, ..., WN) = L1(We). The gradient descent based\n4\noptimization of We can then be written as\nW (k+1)\ne\n⇐(1 −ηλN)W (k)\ne\n−η\nN\nX\nj=1\n[W (k)\ne\n(W (k)\ne\n)⊤]\nj−1\nN\n·dL1(W (k)\ne\n)\ndW\n· [(W (k)\ne\n)⊤W (k)\ne\n]\nN−j\nN\n(1)\nwhere\nthey\nassume\nW ⊤\nj+1(k)Wj+1(k)\n=\nWj(k)Wj(k)⊤, j\n=\n1, 2, ..., N −1 is fulﬁlled for the\nnetwork. [22] argued that the difference between the N-\nlayer deep network and a 1-layer network is that the gradient\ndL1(W (t)\ne\n)\ndW\nis transformed by the two items [W (k)\ne\n(W (k)\ne\n)⊤]\nj−1\nN\nand [W (k)\ne\n(W (k)\ne\n)⊤]\nN−j\nN . They interpreted the effect of\noverparameterization (replacing clasic linear model by dept-N\nlinear networks) on gradient descent as the deep network\nstructure reshapes the gradient\ndL1(W (t)\ne\n)\ndW\nby changing both\nits amplitude and direction so that this can be understood as\nintroducing some forms of momentum and adaptive learning\nrate. Also they claimed that this over-parameterization effect\ncan not be obtained by regularization.\nDo we have a geometric description of this observation\nin our geometrization scheme? In fact this can be directly\nobserved by comparing deep networks with diffeomorphic\nimage registration problem as in [3][2]. What’s more, we\ncan directly generalize the conclusion of [22] to a general\nnonlinear deep network without any further assumptions on\nthe network.\nDiffeomorphic image registration can be abstracted as a map\nG × V →V , where G is the group of image transformations\nand V\nis the vector space of images. Large deformation\ndiffeomorphic metric mapping (LDDMM)[23] generates a\ndeformation ϕ as a ﬂow ϕu\nt of a time-dependent vector ﬁeld\nut ∈Te(G) = g so that\n˙ϕu\nt = ut ◦ϕu\nt , ϕu\n0 = Id, ϕu\n1 = ϕ\n(2)\nThe diffeomorphic matching of two images I0 and I1 with\nLDDMM is to ﬁnd a vector ﬁeld ut, t ∈[0, 1] to minimize the\ncost function\nE(ut) =EK(ut) + EC(ut)\n=\nZ 1\n0\n1\n2|ut|2dt + β|I1 −Io ◦ϕu\n1|2\n(3)\nHere the regularity on ut is a kinetic energy term EK(ut) =\n1\n2\nR 1\n0 |ut|2dt with |ut| a norm on the vector ﬁeld deﬁned as\n|ut|2 = ⟨Lut, ut⟩L2. The operator L is a positive self-adjoint\ndifferential operator. Obviously the norm |ut|2 = ⟨Lut, ut⟩L2\ndeﬁnes a Riemannian metric on the manifold of the diffeo-\nmorphic transformation group Diff(Rn). The second term\nEC(ut) = β|I1 −Io ◦ϕu\n1|2 is the difference between the\ntransformed image Io ◦ϕu\n1 and the target image I1.\nA necessary condition DE(ut) = 0 to minimize the cost\nfunction is that the vector ﬁeld ut should satisfy the Euler-\nPoincar´e (E-P) equation\nLut = −ϕu\n0,tI0 ⋄ϕu\n0,tϕu\n1,0π\n(4)\nwhere ϕu\ns,t = ϕu\nt ◦ϕu\ns−1, π := β(ϕu\n0,tI0 −I1)♭∈V ∗. The\n♭operator is deﬁned as ♭: V →V ∗, ⟨u♭, v⟩V ∗×V = ⟨u, v⟩\nand ⋄: T V ∗→g∗, ⟨I ⋄π, u⟩g∗×g = ⟨π, ζu(I)⟩V ∗×V is the\nmomentum map.\nIn LDDMM framework, the curve satisfying the E-P equa-\ntion is found by a gradient descent algorithm, while the\ngradient is given by ut + Kϕu\n0,tI0 ⋄ϕu\n0,tϕu\n1,0π with K = L−1.\nA direct calculation in the LDDMM framework following [23]\nshows that the update of ϕu\n1 is given by\nϕu,(k+1)\n1\n⇐(1 −η)ϕu,(k)\n1\n−ηK ⋆\nZ 1\n0\nDϕu\nt,1 · Dϕu\nt,1 · dEC(ut)\ndϕ1\nϕu\n0,tϕu\n0,tdt\n(5)\nWe can directly see this is almost the same as the update\nrule of We given by (1). But here we are working with a\nnonlinear deep network so that we have a generalization of\nthe linear network of [22]. In fact the result of [22] can be\nregarded as a special case of LDDMM called static vector\nﬂow (SVF), which is formulated on a Lie group instead of\non a Riemannian manifold and the items [W (k)\ne\n(W (k)\ne\n)⊤]\nj−1\nN ,\n[W (k)\ne\n(W (k)\ne\n)⊤]\nN−j\nN\ncan be understood as an analogue of the\nLie exponential used in SVF framework.\nLDDMM has a beautiful geometric picture which is the\nsame as the geometric mechanics[24][25][26]. How to un-\nderstand the effect of over-parameterization in this LDDMM\nframework? LDDMM formulates a smooth image transforma-\ntion by a constrained curve described by (2). The gradient\ndescent based update of the curve is essentially a constrained\noptimal control as shown in [27]. So when we try to ap-\nproximate a function by deep networks, the structure of over-\nparameterized deep network is essentially to set constraints\non the possible solution space. The so-called acceleration\neffect of over-parameterization in [22] is nothing but a natural\nresult of the constrained optimal control formulation. Also\ntheir conclusion that this acceleration can not be obtained\nby regularization is also not exact since the constraints in\noptimal control can also be regarded as a kind of regularization\nin optimization problems[28]. The only difference is that the\nregularization is set on the structure of the network.\nLayers are not created equal We have seen that in quantum\ncomputation, for both the general sequential unitary quantum\nevolution and the quantum circuit model, we observe the\nsame initial value sensitivity property. This is to say, quantum\ninformation processing systems are playing with Riemannian\nmanifolds with negative curvatures. If we compare these with\nthe observation of [1], we ﬁnd the general quantum evolution\nsystem corresponds to the fully connect networks and the\nquantum circuit model corresponds to convolutional networks.\nSo we can say the observed non-equality of layers in [1] is just\na direct consequence of the principle of quantum computation\nsystem. But there is still one thing is missing, the residual\nnetwork. It’s observed in [1] that residual networks also show\na non-equality of layers but the pattern is different from fully\nconnected and convolutional networks. Can we also ﬁnd the\ncorrespondence of residual networks in quantum computation\nsystems? Yes, since residual networks are just differential\nequations, they are correspondent to the fundamental quantum\nmechanics rule, the Schrodinger equation. Since the ﬁnite\n5\ntime discretization of Schrodinger equation is just the general\nsequential unitary quantum evolution, we believe Schrodinger\nequation should have the same initial value sensitivity pattern.\nThis means residual networks should have a similar pattern\nas the fully connected and convolutional networks. This is\ndifferent from the observed pattern of residual networks[1].\nHow to resolve this contradiction? If we believe that quantum\nmechanics is the ultimate rule of the world and the main\nadvantage of residual networks is to build a smoother manifold\nof transformations to approximate functions, then residual\nnetworks should be related with a smooth geometry and there\nis no reason that some layers of residual networks are more\ncritical than other layers as observed in [1]. We assume this is\ndue to the artifacts of the non-uniform discretization used in\nresidual networks and noise during optimization. From another\naspect, the redistribution of the sensitivity pattern of residual\nnetworks also indicates that the strong background negative\ncurvature geometry of general deep networks is weakened\nin residual networks so that the random perturbation effects\nsurvive. This is in fact an evidence that residual networks\nare building and working on a ﬂatter manifold than fully\nconnected and convolutional networks.\nAnother problem is related with the spacetime structure.\nThere is evidence that the geometry of spacetime is emergent\nfrom quantum information processing networks. Also in [2]\nwe indicated that in deep networks, if the Fisher-Rao metric\nis used to measure the network complexity, then the interac-\ntion between data and network structures is analogue of the\ninteraction between material and spacetime geometry, i.e. the\ngeneral relativity. But if a general quantum deep network has a\nnegative curvature, how can our universe have a ﬂat (in a large\nscale) spacetime? Does the existence of our ﬂat universe is an\nevidence that there exists a subset of deep networks that can\nform a ﬂat Euclidean geometry? If such a corner of Euclidean\ndeep networks exist, then all the layers will be created equal\nin such networks. Can this help us to ﬁnd better network\nstructures? In random matrix based analysis of deep networks,\na special type of network conﬁguration with dynamic isometry\nproperty seems to fall in this subset. It has been shown that\nsuch kind of networks hold some advantages beyond normal\nnetworks such as a smooth information ﬂow in both the\nforward and backward directions. In fact geometrically the\nsmooth information ﬂow is just the inertial movement in a ﬂat\nspacetime, i.e. the ﬁrst law of Newton. Of course, just as the\ncorner of physical states in quantum mechanics, the corner\nof Euclidean deep networks is also a zero measurement set.\nSo we assume this subset may not form an universal data\nprocessing system, just as our universe may be a very special\ncase of the so-called multiverse picture.\nFinally, the negative curvature will inﬂuence the loss land-\nscape of deep networks. If a network conﬁguration has a higher\nsensitivity at the bottom layers, it can be easily ﬁgured out\nthat loss landscape is more sensitive to the bottom layers and\nmore robust to top layers. Accordingly the locus of the global\nminima will have more valleys in the bottom layers and the\nlocus may have a fractal-like complex pattern with a stronger\nover-parameterization. How exactly the over-parameterization\nwill change the loss landscape is still open.\nIV. CONCLUSIONS\nGeometrization is not only the key idea of physics, it’s\nalso a framework to understand deep networks. In this work\nwe try to understand over-parameterized deep networks by\ngeometrization. By establishing analogies between properties\nof over-parameterized deep networks and quantum informa-\ntion processing/diffeomorphic image registration systems, we\nfound they share similar geometric structures. Our key ob-\nservations are:(1)Polynomial complexity over-parameterized\ndeep networks only explore a corner of polynomial complexity\nfunctions just as quantum computation systems only explore\nthe corner of physical states in the gigantic quantum state\nspace. The network structure sets constraints on the submani-\nfold of functions that can be approximated by the network.\n(2)Over-parameterized deep networks may have a complex\nloss landscape and local minima have different generalization\ncapabilities. The generalization capability is determined by the\nnetwork complexity, which is computed as the geodesic dis-\ntance on a Riemannian manifold between the transformation\nrepresented by the network and the identity transformation.\nThe probability that a certain conﬁguration is obtained is\ndetermined by the complexity of the network. This is an\nanalogue of the measurement problem in quantum mechanics,\nwhere the probability of the ﬁnal state is determined by\nthe distance between the initial state and the ﬁnal state.\n(3)Over-parameterized deep networks have a geometry with\na negative curvature, just as quantum computation systems\nhas a Riemannian geometry with a negative curvature. All\nthese observations suggest that deep networks are closely\nrelated with physics and geometrization may provide a proper\nroadmap to interpret deep networks.\nIn this work we mainly explore the Riemannian structure\nof deep networks, for example the network complexity as the\ngeodesic distance and the sensitivity of network parameters\nas Riemannian curvature. A natural question is, can other\ngeometrical structures in physics help to understand over-\nparameterized deep networks? For example the symplectic\nstructure of geometric mechanics plays a key role in the\ndynamics of classical mechanics. Can the dynamics of deep\nnetworks also be understood in a similar way? Fibre bundle\nstructure is another key structure to understand interactions in\nphysics, also it plays a key role in the geometry of quantum\ninformation processing such as the geometry of mixed state\nand quantum entanglement. Can it be used to understand\ninteractions between subnetworks in a composite system with\nmultiple subnetworks? In [2] we have mentioned that ﬁbre\nbundles may be related with important network structures such\nas attention mechanism, Turing neural machines and differen-\ntial neural computers. There are signs that ﬁbre bundles are\nalso related with capsule networks and the recent quaternion\nneural networks. To explore the possibility to understand deep\nnetworks based on bundles will be our future work.\nREFERENCES\n[1] C. Y. Zhang, Bengio S., and Singer Y. Are all layers created equal?\narxiv:902.01996v1, 2019.\n[2] X. Dong and L. Zhou.\nGeometrization of deep networks for the\ninterpretability of deep learning systems. arxiv:1901.02354, 2019.\n6\n[3] J.S. Wu X. Dong and L. Zhou. How deep learning works –the geometry\nof deep learning. arXiv:1710.10784, 2017.\n[4] Xian Hui Ge and Bin Wang.\nQuantum computational complex-\nity, einstein’s equations and accelerated expansion of the universe.\narXiv:1708.06811v2, 2018.\n[5] H.\nHeydari.\nGeometric\nformulation\nof\nquantum\nmechanics.\narXiv:1503.00238, 2015.\n[6] Hiroaki Matsueda. Emergent general relativity from ﬁsher information\nmetric. arXiv:1310.1831v2, 2013.\n[7] M. R. Dowling and M. A. Nielsen.\nThe geometry of quantum\ncomputation. Quantum Information and Computation, 8(10):861–899,\n2008.\n[8] Leonard Susskind. The typical state paradox: diagnosing horizons with\ncomplexity. Fortschritte Der Physik, 64(1):84–91, 2016.\n[9] L. Susskind. Entanglement is not enough. arXiv:1411.0690v1, 2014.\n[10] L. Susskind and Y. Zhao.\nSwitchbacks and the bridge to nowhere.\narXiv:1408.2823v1, 2014.\n[11] M. Gu M. A. Nielsen, M. R. Dowling and A. C. Doherty. Quantum\ncomputation as geometry. Science 311,1133, 2006.\n[12] Brian Swingle. Entanglement renormalization and holography. Physical\nReview D Particles and Fields, 86(6):–, 2009.\n[13] Brian Swingle. Constructing holographic spacetimes using entanglement\nrenormalization. Physics, 2012.\n[14] X. Dong and L. Zhou. Spacetime as the optimal generative network of\nquantum states: a roadmap to qm=gr? arxiv:1804.07908, 2018.\n[15] Hiroaki Matsueda.\nDerivation of gravitational ﬁeld equation from\nentanglement entropy. arXiv:1408.5589v2, 70, 2014.\n[16] Daniel Soudry and Yair Carmon.\nNo bad local minima: Data in-\ndependent training error guarantees for multilayer neural networks.\narxiv:1605.08361v2, 2016.\n[17] Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.\nGra-\ndient descent provably optimizes over-parameterized neural networks.\narxiv:1810.02054v1, 2018.\n[18] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory\nfor deep learning via over-parameterization. arxiv:arXiv:1811.03962v2,\n2018.\n[19] Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James\nStokes. Fisher-rao metric, geometry, and complexity of neural networks.\narxiv:1711.01530, 2017.\n[20] Wu Lei, Zhanxing Zhu, and E Weinan.\nTowards understand-\ning generalization of deep learning: Perspective of loss landscapes.\narxiv:1706.10239v2, 2017.\n[21] Y. Cooper. The loss landscape of overparameterized neural networks.\narxiv:1804.10200v1, 2018.\n[22] Arora.\nS.,\nCohen\nN.,\nand\nE.\nHazan.\nOn\nthe\noptimization\nof\ndeep\nnetworks:\nimplicit\nacceleration\nby\noverparameterization.\narxiv:1802.06509v2, 2018.\n[23] Mirza Faisal Beg, Michael I. Miller, Alain Trouve, and Laurent Younes.\nComputing large deformation metric mappings via geodesic ﬂows. 2004.\n[24] M. Bruveris, F. Gay-Balmaz, D. D. Holm, and T. S. Ratiu.\nThe\nmomentum map representation of images. Journal of Nonlinear Science,\n21(1):115–150, 2011.\n[25] Martins Bruveris and Darryl D. Holm. Geometry of image registration:\nThe diffeomorphism group and momentum maps.\nFields Institute\nCommunications, 73:19–56, 2013.\n[26] Darryl D. Holm, Tanya Schmah, and Cristina Stoica.\nGeometric\nmechanics and symmetry. Oxford University Press Oxford, (2):xvi+515,\n2009.\n[27] G. L. Hart, C. Zach, and M. Niethammer. An optimal control approach\nfor deformable registration. In IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition Workshops, 2013.\n[28] Darryl D. Holm. Euler’s ﬂuid equations: Optimal control vs optimiza-\ntion. Physics Letters A, 373(47):4354–4359, 2009.\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2019-02-11",
  "updated": "2019-02-11"
}