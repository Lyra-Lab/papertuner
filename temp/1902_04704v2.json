{
  "id": "http://arxiv.org/abs/1902.04704v2",
  "title": "Neural network models and deep learning - a primer for biologists",
  "authors": [
    "Nikolaus Kriegeskorte",
    "Tal Golan"
  ],
  "abstract": "Originally inspired by neurobiology, deep neural network models have become a\npowerful tool of machine learning and artificial intelligence, where they are\nused to approximate functions and dynamics by learning from examples. Here we\ngive a brief introduction to neural network models and deep learning for\nbiologists. We introduce feedforward and recurrent networks and explain the\nexpressive power of this modeling framework and the backpropagation algorithm\nfor setting the parameters. Finally, we consider how deep neural networks might\nhelp us understand the brain's computations.",
  "text": "Neural network models and deep learning \n– a primer for biologists \n \nNikolaus Kriegeskorte1,2,3,4 and Tal Golan4 \n1Department of Psychology, 2Department of Neuroscience \n3Department of Electrical Engineering \n4Zuckerman Mind Brain Behavior Institute, Columbia University \nn.kriegeskorte@columbia.edu, tal.golan@columbia.edu \n \nOriginally inspired by neurobiology, deep neural network models have become a \npowerful tool of machine learning and artificial intelligence. They can approximate \nfunctions and dynamics by learning from examples. Here we give a brief \nintroduction to neural network models and deep learning for biologists. We \nintroduce feedforward and recurrent networks and explain the expressive power of \nthis modeling framework and the backpropagation algorithm for setting the \nparameters. Finally, we consider how deep neural network models might help us \nunderstand brain computation. \n \nNeural network models of brain function \nBrain function can be modeled at many different levels of abstraction. At one extreme, \nneuroscientists model single neurons and their dynamics in great biological detail. At the \nother extreme, cognitive scientists model brain information processing with algorithms \nthat make no reference to biological components. In between these extremes lies a model \nclass that has come to be called artificial neural network (Rumelhart et al., 1987; LeCun \net al., 2015; Schmidhuber, 2015; Goodfellow et al., 2016). \nA biological neuron receives multiple signals through the synapses contacting its \ndendrites and sends a single stream of action potentials out through its axon. The \nconversion of a complex pattern of inputs into a simple decision (to spike or not to spike) \nsuggested to early theorists that each neuron performs an elementary cognitive function: \nit reduces complexity by categorizing its input patterns (McCulloch and Pitts, 1943; \nRosenblatt, 1958). Inspired by this intuition, artificial neural network models are \ncomposed of units that combine multiple inputs and produce a single output. \nThe most common type of unit computes a weighted sum of the inputs and transforms \nthe result nonlinearly. The weighted sum can be interpreted as comparing the pattern of \ninputs to a reference pattern of weights, with the weights corresponding to the strengths \nof the incoming connections. The weighted sum is called the preactivation. The strength \nof the preactivation reflects the overall strength of the inputs and, more importantly, the \nmatch between the input pattern and the weight pattern. For a given input strength \n(measured as the sum of squared intensities), the preactivation will be maximal if the input \npattern exactly matches the weight pattern (up to a scaling factor). \nKriegeskorte & Golan (2019) Neural network models and deep learning \n \n2 \n \nThe preactivation forms the input to the unit’s nonlinear activation function. The activation \nfunction can be a threshold function (0 for negative, 1 for positive preactivations), \nindicating whether the match is sufficiently close for the unit to respond (Rosenblatt, \n1958). More typically, the activation function is a monotonically increasing function, such \nas the logistic function (Figure 1) or a rectifying nonlinearity, which outputs the \npreactivation if it is positive and zero otherwise. These latter activation functions have \nnon-zero derivatives (at least over the positive range of preactivations). As we will see \nbelow, non-zero derivatives make it easier to optimize the weights of a network. \nThe weights can be positive or negative. Inhibition, thus, need not be relayed through a \nseparate set of inhibitory units, and neural network models typically do not respect Dale’s \nlaw (which states that a neuron performs the same chemical action at all of its synaptic \nconnections to other neurons, regardless of the identity of the target cell). In addition to \nthe weights of the incoming connections, each unit has a bias parameter: the bias is \nadded to the preactivation, enabling the unit to shift its nonlinear activation function \nhorizontally, for example moving the threshold to the left or right. The bias can be \nunderstood as a weight for an imaginary additional input that is constantly 1. \n \nFigure 1 | Function approximation by a feedforward neural network. A feedforward neural \nnetwork with two input units (bottom), three hidden units (middle), and two output units (top). The \ninput patterns form a two-dimensional space. The hidden and output units here use a sigmoid \n(logistic) activation function. Surface plots on the left show the activation of each unit as a function \nof the input pattern (horizontal plane spanned by inputs x1 and x2). For the output units, the \npreactivations are shown below the output activations. For each unit, the weights (arrow \nthickness) and signs (black, positive; red, negative) of the incoming connections control the \norientation and slope of the activation function. The output units combine the nonlinear ramps \ncomputed by the hidden units. Given enough hidden units, a network of this type can approximate \nany continuous function to arbitrary precision. \n \nKriegeskorte & Golan (2019) Neural network models and deep learning \n \n3 \n \nNeural networks are universal approximators \nUnits can be assembled into networks in many different configurations. A single unit can \nserve as a linear discriminant of its input patterns. A set of units connected to the same \nset of inputs can detect multiple classes, with each unit implementing a different linear \ndiscriminant. For a network to discriminate classes that are not linearly separable in the \ninput signals, we need an intermediate layer between input and output units, called a \nhidden layer (Figure 1). \nIf the units were linear — outputting the weighted sum directly, without passing it through \na nonlinear activation function — then the output units reading out the hidden units would \ncompute weighted sums of weighted sums and would, thus, themselves be limited to \nweighted sums of the inputs. With nonlinear activation functions, a hidden layer makes \nthe network more expressive, enabling it to approximate nonlinear functions of the input, \nas illustrated in Figure 1. \nA feedforward network with a single hidden layer (Figure 1) is a flexible approximator of \nfunctions that link the inputs to the desired outputs. Typically, each hidden unit computes \na nonlinear ramp, for example sigmoid or rectified linear, over the input space. The ramp \nrises in the direction in input space that is defined by the vector of incoming weights. By \nadjusting the weights, we can rotate the ramp in the desired direction. By scaling the \nweights vector, we can squeeze or stretch the ramp to make it rise more or less steeply. \nBy adjusting the bias, we can shift the ramp forward or backward. Each hidden unit can \nbe independently adjusted in this way. \nOne level up, in the output layer, we can linearly combine the outputs of the hidden units. \nAs shown in Figure 1, a weighted sum of several nonlinear ramps produces a qualitatively \ndifferent continuous function over the input space. This is how a hidden layer of linear–\nnonlinear units enables the approximation of functions very different in shape from the \nnonlinear activation function that provides the building blocks. \nIt turns out that we can approximate any continuous function to any desired level of \nprecision by allowing a sufficient number of units in a single hidden layer (Cybenko, 1989; \nHornik et al., 1989). To gain an intuition of why this is possible, consider the left output \nunit (y1) of the network in Figure 1. By combining ramps overlapping in a single region of \nthe input space, this unit effectively selects a single compact patch. We could tile the \nentire input space with sets of hidden units that select different patches in this way. In the \noutput layer, we could then map each patch to any desired output value. As we move \nfrom one input region to another, the network would smoothly transition between the \ndifferent output values. The precision of such an approximation can always be increased \nby using more hidden units to tile the input space more finely. \nDeep networks can efficiently capture complex functions \nA feedforward neural network is called ‘deep’ when it has more than one hidden layer. \nThe term is also used in a graded sense, in which the depth denotes the number of layers. \nWe have seen above that even shallow neural networks, with a single hidden layer, are \nuniversal function approximators. What, then, is the advantage of deep neural networks? \nDeep neural networks can re-use the features computed in a given hidden layer in higher \nhidden layers. This enables a deep neural network to exploit compositional structure in a \nKriegeskorte & Golan (2019) Neural network models and deep learning \n \n4 \n \nfunction, and to approximate many natural functions with fewer weights and units (Lin et \nal., 2017; Rolnick and Tegmark, 2017). Whereas a shallow neural network must piece \ntogether the function it approximates, like a lookup table (although the pieces overlap and \nsum), a deep neural network can benefit from its hierarchical structure. A deeper \narchitecture can increase the precision with which a function can be approximated on a \nfixed budget of parameters and can improve the generalization after learning to new \nexamples. \nDeep learning refers to the automatic determination of parameters deep in a network on \nthe basis of experience (data). Neural networks with multiple hidden layers are an old \nidea and were a popular topic in engineering and cognitive science in the 1980s \n(Rumelhart et al., 1987). Although the advantages of deep architectures were understood \nin theory, the method did not realize its potential in practice, mainly because of insufficient \ncomputing power and data for learning. Shallow machine learning techniques, such as \nsupport vector machines (Cortes and Vapnik, 1995; Schölkopf and Smola, 2002), worked \nbetter in practice and also lent themselves to more rigorous mathematical analysis. The \nrecent success of deep learning has been driven by a rise in computing power — in \nparticular the advent of graphics processing units, GPUs, specialized hardware for fast \nmatrix–matrix multiplication — and web-scale data sets to learn from. In addition, \nimproved techniques for pretraining, initialization, regularization, and normalization, along \nwith the introduction of rectified linear units, have all helped to boost performance. Recent \nwork has explored a wide variety of feedforward and recurrent network architectures, \nimproving the state-of-the-art in several domains of artificial intelligence and establishing \ndeep learning as a central strand of machine learning in the last few years. \nThe function that deep neural networks are trained to approximate is often a mapping \nfrom input patterns to output patterns, for example classifying natural images according \nto categories, translating sentences from English to French, or predicting tomorrow’s \nweather from today’s measurements. When the cost minimized by training is a measure \nof the mismatch between the network’s outputs and desired outputs (that is, the ‘error’), \nfor a training set of example cases, the training is called supervised. When the cost \nminimized by training does not involve prespecified desired outputs for a set of example \ninputs, the training is called unsupervised. \nTwo examples of unsupervised learning are autoencoders and generative adversarial \nnetworks. Autoencoder networks (Rumelhart et al., 1986; Hinton and Salakhutdinov, \n2006) learn to transform input patterns into a compressed latent representation by \nexploiting inherent statistical structure. Generative adversarial networks (Goodfellow et \nal., 2014) operate in the opposite direction, transforming random patterns in a latent \nrepresentation into novel, synthetic examples of a category, such as fake images of \nbedrooms. The generator network is trained concurrently with a discriminator network that \nlearns to pick out the generator’s fakes among natural examples of the category. The two \nadversarial networks boost each other’s performance by posing increasingly difficult \nchallenges of counterfeiting and detection to each other. Deep neural networks can also \nbe trained by reinforcement (deep reinforcement learning), which has led to impressive \nperformance at playing games and robotic control (Mnih et al., 2015). \nKriegeskorte & Golan (2019) Neural network models and deep learning \n \n5 \n \nDeep learning by backpropagation \nSay we want to train a deep neural network model with supervision. How can the \nconnection weights deep in the network be automatically learned? The weights are \nrandomly initialized and then adjusted in many small steps to bring the network closer to \nthe desired behavior. A simple approach would be to consider random perturbations of \nthe weights and to apply them when they improve the behavior. This evolutionary \napproach is intuitive and has recently shown promise (Such et al., 2017; Stanley et al., \n2019), but it is not usually the most efficient solution. There may be millions of weights, \nspanning a search space of equal dimension. It takes too long in practice to find directions \nto move in such a space that improve performance. We could wiggle each weight \nseparately, and determine if behavior improves. Although this would enable us to make \nprogress, adjusting each weight would require running the entire network many times to \nassess its behavior. Again, progress with this approach is too slow for many practical \napplications. \nIn order to enable more efficient learning, neural network models are composed of \ndifferentiable operations. How a small change to a particular weight affects performance \ncan then be computed as the partial derivative of the error with respect to the weight. For \ndifferent weights in the same model, the algebraic expressions corresponding to their \npartial derivatives share many terms, enabling us to efficiently compute the partial \nderivatives for all weights. \nFor each input, we first propagate the activation forward through the network, computing \nthe activation states of all the units, including the outputs. We then compare the network’s \noutputs with the desired outputs and compute the cost function to be minimized (for \nexample, the sum of squared errors across output units). For each unit, we then compute \nhow much the cost would drop if the activation changed slightly. This is the sensitivity of \nthe cost to a change of activation of each output unit. Mathematically, it is the partial \nderivative of the cost with respect to the each activation. We then proceed backwards \nthrough the network propagating the cost derivatives (sensitivities) from the activations to \nthe preactivations and through the weights to the activations of the layer below. The \nsensitivity of the cost to each of these variables depends on the sensitivities of the cost \nto the variables downstream in the network. Backpropagating the derivatives through the \nnetwork by applying the chain rule provides an efficient algorithm for computing all the \npartial derivatives (Werbos, 1982). \nThe critical step is computing the partial derivative of the cost with respect to each weight. \nConsider the weight of a particular connection (red arrow in Figure 2). The connection \nlinks a source unit in one layer to a target unit in the next layer. The influence of the weight \non the cost for a given input pattern depends on how active the source unit is. If the source \nunit is off for the present input pattern, then the connection has no signal to transmit and \nits weight is irrelevant to the output the network produces for the current input. The \nactivation of the source unit is multiplied with the weight to determine its contribution to \nthe preactivation of the target unit, so the source activation is one factor determining the \ninfluence of the weight on the cost. The other factor is the sensitivity of the cost to the \npreactivation of the target unit. If the preactivation of the target unit had no influence on \nthe cost, then the weight would have no influence, either. The derivative of the cost with \nrespect to the weight is the product of its source unit’s activation and its target unit’s \ninfluence on the cost. \nKriegeskorte & Golan (2019) Neural network models and deep learning \n \n6 \n \n \nFigure 2 | The backpropagation algorithm. Backpropagation is an efficient algorithm for \ncomputing how small adjustments to the connection weights affect the cost function that the \nnetwork is meant to minimize. A feedforward network with two hidden layers is shown as an \nexample. First, the activations are propagated in the feedforward direction (upward). The \nactivation function (gray sigmoid) is shown in each unit (circle). In the context of a particular input \npattern (not shown), the network is in a particular activation state, indicated by the black dots in \nthe units (horizontal axis: preactivation, vertical axis: activation). Second, the derivatives of the \ncost function (squared-error cost shown on the right) are propagated in reverse (downward). In \nthe context of the present input pattern, the network can be approximated as a linear network \n(black lines indicating the slope of the activation function). The chain rule defines how the cost \n(the overall error) is affected by small changes to the activations, preactivations, and weights. The \ngoal is to compute the partial derivative of the cost with respect to each weight (bottom right). \nEach weight is then adjusted in proportion to how much its adjustment reduces the cost. The \nnotation roughly follows Nielsen (2015), but we use bold symbols for vectors and matrices. The \nsymbol ⨀ denotes element-wise multiplication (Hadamard product). \n \nWe adjust each weight in the direction that reduces the cost (the error) and by an amount \nproportional to the derivative of the cost with respect to the weight. This process is called \ngradient descent, because it amounts to moving in the direction in weight space in which \nthe cost declines most steeply. To help our intuition, let us consider two approaches we \nmight take. First, consider the approach of taking a step to reduce the cost for each \nindividual training example. Gradient descent will make a minimal and selective \nadjustments to reduce the error, which makes sense as we do not want learning from the \ncurrent example to interfere with what we’ve learned from other examples. However, our \ngoal is to reduce the overall error, which is defined as the sum of the errors across all \nexamples. So second, consider the approach of summing up the error surfaces (or, \nequivalently, the gradients) across all examples before taking a step. We can still only \ntake a small step, because the error surface is nonlinear and so the gradient will change \nas we move away from the point about which we linearized the network. \nIn practice, the best solution is to use small batches of training examples to estimate the \ngradient before taking a step. Compared to the single-example approach, this gives us a \nmore stable sense of direction. Compared to the full-training-set approach, it greatly \nKriegeskorte & Golan (2019) Neural network models and deep learning \n \n7 \n \nreduces the computations required to take a step. Although the full-training-set approach \ngives exact gradients for the training-set error, it still does not enable us to take large \nsteps, because of the nonlinearity of the error function. Using batches is a good \ncompromise between stability of the gradient estimate and computational cost. Because \nthe gradient estimate depends on the random sample of examples in the current batch, \nthe method is called stochastic gradient descent (SGD). Beyond the motivation just given, \nthe stochasticity is thought to contribute also to finding solutions that generalize well \nbeyond the training set (Poggio and Liao, 2017).  \nThe cost is not a convex function of the weights, so we might be concerned about getting \nstuck in local minima. However, the high dimensionality of weight space turns out to be a \nblessing (not a curse) for gradient descent: there are many directions to escape in, making \nit unlikely that we will ever find ourselves trapped, with the error surface rising in all \ndirections (Kawaguchi, 2016). In practice, it is saddle points (where the gradient vanishes) \nthat pose a greater challenge than local minima (Dauphin et al., 2014). Moreover, the \ncost function typically has many symmetries, with any given set of weights having many \ncomputationally equivalent twins (that is, the model computes the same overall function \nfor different parameter settings). As a result, although our solution may be one local \nminimum among many, it may not be a poor local minimum: It may be one of many \nsimilarly good solutions. \nRecurrent neural networks are universal approximators of \ndynamical systems \nSo far we have considered feedforward networks, whose directed connections do not \nform cycles. Units can also be configured in recurrent neural networks (RNNs), where \nactivity is propagated in cycles, as is the case in brains (Dayan and Abbott, 2001; \nGoodfellow et al., 2016). This enables a network to recycle its limited computational \nresources over time and perform a deeper sequence of nonlinear transformations. As a \nresult, RNNs can perform more complex computations than would be possible with a \nsingle feedforward sweep through the same number of units and connections. \nFor a given state space, a suitable RNN can map each state to any desired successor \nstate. RNNs, therefore, are universal approximators of dynamical systems (Schäfer and \nZimmermann, 2006). They provide a universal language for modeling dynamics, and one \nwhose components could plausibly be implemented with biological neurons. \nMuch like feedforward neural networks, RNNs can be trained by backpropagation. \nHowever, backpropagation must proceed through the cycles in reverse. This process is \ncalled backpropagation through time. An intuitive way to understand an RNN and \nbackpropagation through time is to ‘unfold’ the RNN into an equivalent feedforward \nnetwork (Figure 3). Each layer of the feedforward network represents a timestep of the \nRNN. The units and weights of the RNN are replicated for each layer of the feedforward \nnetwork. The feedforward network, thus, shares the same set of weights across its layers \n(the weights of the recurrent network). \n \n \nKriegeskorte & Golan (2019) Neural network models and deep learning \n \n8 \n \n \nFigure 3 | Recurrent neural networks. (a) A recurrent neural network models with two input \nunits (in blue box), three hidden units (green box), and two output units (pink box). The hidden \nunits here are fully recurrently connected: each sends its output to both other units. The arrows \nrepresent scalar weights between particular units. (b) Equivalent feedforward network. Any \nrecurrent neural network can be unfolded along time as a feedforward network. To this end, the \nunits of the recurrent neural network (blue, green, pink sets) are replicated for each time step. \nThe arrows here represent weights matrices between sets of units in the colored boxes. For the \nequivalence to hold, the feedforward network has to have a depth matching the number of time \nsteps that the recurrent network is meant to run for. Unfolding leads to a representation that is \nless concise, but easier to understand and often useful in software implementations of recurrent \nneural networks. Training of the recurrent model by backpropagation through time is equivalent \nto training of the unfolded model by backpropagation. \n \nFor tasks that operate on independent observations (for example, classifying still images), \nthe recycling of weights can enable an RNN to perform better than a feedforward network \nwith the same number of parameters (Spoerer et al., 2017). However, RNNs really shine \nin tasks that operate on streams of dependent observations. Because RNNs can maintain \nan internal state (memory) over time and produce dynamics, they lend themselves to \ntasks that require temporal patterns to be recognized or generated. These include the \nperception speech and video, cognitive tasks that require maintaining representations of \nhidden states of the agent (such as goals) or the environment (such as currently hidden \nobjects), linguistic tasks like the translation of text from one language into another, and \ncontrol tasks at the level of planning and selecting actions, as well as at the level of motor \ncontrol during execution of an action under feedback from the senses. \nDeep neural networks provide abstract process models of \nbiological neural networks \nCognitive models capture aspects of brain information processing, but do not speak to its \nbiological implementation. Detailed biological models can capture the dynamics of action \npotentials and the spatiotemporal dynamics of signal propagation in dendrites and axons. \nHowever, they have only had limited success in explaining how these processes \ncontribute to cognition. Deep neural network models, as discussed here, strike a balance, \nexplaining feats of perception, cognition, and motor control in terms of networks of units \nthat are highly abstracted, but could plausibly be implemented with biological neurons. \nFor engineers, artificial deep neural networks are a powerful tool of machine learning. For \nneuroscientists, these models offer a way of specifying mechanistic hypotheses on how \ncognitive functions may be carried out by brains (Kriegeskorte and Douglas, 2018; \nKietzmann et al., 2019; Storrs and Kriegeskorte, in press). Deep neural networks provide \nKriegeskorte & Golan (2019) Neural network models and deep learning \n \n9 \n \na powerful language for expressing information-processing functions. In certain domains, \nthey already meet or surpass human-level performance (for example, visual object \nrecognition and board games) while relying exclusively on operations that are biologically \nplausible. \nNeural network models in engineering have taken inspiration from brains, far beyond the \ngeneral notion that computations involve a network of units, each of which nonlinearly \ncombines multiple inputs to compute a single output (Kriegeskorte, 2015; Yamins and \nDiCarlo, 2016). For example, convolutional neural networks (Fukushima and Miyake, \n1982; LeCun et al., 1989), the dominant technology in computer vision, use a deep \nhierarchy of retinotopic layers whose units have restricted receptive fields. The networks \nare convolutional in that weight templates are automatically shared across image \nlocations (rendering the computation of a feature map’s preactivations equivalent to a \nconvolution of the input with the weight template). Although the convolutional aspect may \nnot capture an innate characteristic of the primate visual system, it does represent an \nidealization of the final product of development and learning in primates, where \nqualitatively similar features are extracted all over retinotopic maps at early stages of \nprocessing. Across layers, these networks transform a visuospatial representation of the \nimage into a semantic representation of its contents, successively reducing the spatial \ndetail of the maps and increasing the number of semantic dimensions (Figure 4). \nThe fact that a neural network model was inspired by some abstract features of biology \nand that it matches overall human or animal performance at a task, does not make it a \ngood model of how the human or animal brain performs the task. However, we can \ncompare neural network models to brains in terms of detailed patterns of behavior, such \nas errors and reaction times for particular stimuli. Moreover, we can compare the internal \nrepresentations in neural networks to those in brains. \nIn the ‘white-box’ approach, we evaluate a model by looking at its internal representations. \nNeural network models form the basis for predicting representations in different brain \nregions for a particular set of stimuli (Diedrichsen and Kriegeskorte, 2017). One approach \nis called encoding models. In encoding models, the brain activity pattern in some \nfunctional region is predicted using a linear transformation of the representation in some \nlayer of the model (Kay et al., 2008; Mitchell et al., 2008). In another approach, called \nrepresentational similarity analysis (Kriegeskorte et al., 2008; Nili et al., 2014; \nKriegeskorte and Diedrichsen, 2016), each representation in brain and model is \ncharacterized by a representational dissimilarity matrix. Models are evaluated according \nto their ability to explain the representational dissimilarities across pairs of stimuli. A third \napproach is pattern component modeling (Diedrichsen et al., 2011, 2017), where \nrepresentations are characterized by the second moment of the activity profiles. \nRecent results from the domain of visual object recognition indicate that deep \nconvolutional neural networks are the best available model of how the primate brain \nachieves rapid recognition at a glance, although they do not explain all of the explainable \nvariance in neuronal responses. (Cadieu et al., 2014; Khaligh-Razavi and Kriegeskorte, \n2014; Yamins et al., 2014; Güçlü and van Gerven, 2015; Cichy et al., 2016; Eickenberg \net al., 2017; Wen et al., 2017; Nayebi et al., 2018), and yet multiple functional \nincompatibilities have already been reported (Szegedy et al., 2013; Geirhos et al., 2017; \nJo and Bengio, 2017; Rajalingham et al., 2018). \nKriegeskorte & Golan (2019) Neural network models and deep learning \n \n10 \n \n \nFigure 4 | Deep convolutional feedforward neural networks. The general structure of Alexnet, \na convolutional deep neural network architecture which had a critical role in bringing deep neural \nnetworks into the spotlight. Unlike the visualization in the original report on this model, here the \ntensors' dimensions are drawn to scale, so it is easier to appreciate how the convolutional deep \nneural network gradually transforms the input image from a spatial to semantic representation. \nFor sake of simplicity, we did not visualize the pooling operations, as well as the splitting of some \nof these layers between two GPUs. The leftmost box is the input image, (a tensor of the \ndimensions 227×227×3, where 227 is the length of the square input-image edges and three is the \nnumber of color components). It is transformed by convolution into the first layer (second box from \nthe left), a tensor with smaller spatial dimensions (55×55) but a larger number of feature maps \n(96). Each feature map in this tensor is produced by a convolution of the original image with a \nparticular 11×11×3 filter. Therefore, the preactivation of each unit in this layer is a linear \ncombination of one rectangular receptive field in the image. The boundaries of such a receptive \nfield are visualized as a small box within the image tensor. In the next, second layer, the \nrepresentation is even more spatially smaller (27×27) but richer with respect of the number of \nfeature maps (256). Note that from here and onwards, each feature is not a linear combination of \npixels but a linear combination of the previous layer's features. The sixth layer (see the small \noverview inset at the top-right) combines all feature maps and locations of the fifth layer to yield \n4096 different scalar units, each with its own unrestricted input weights vector. The final eighth \nlayer has 1000 units, one for each output class. The eight images on the bottom were produced \nby gradually modifying random noise images so excite particular units in each of the eight layers \n(Erhan et al., 2009). The rightmost image was optimized to activate the output neuron related to \nthe class 'Mosque'. Importantly, these are only local solutions to the activation-maximization \nproblem. Alternative activation-maximizing images may be produced by using different starting \nconditions or optimization heuristics. \n \nIn the ‘black-box’ approach, we evaluate a model on the basis of its behavior. We can \nreject models for failing to explain detailed patterns of behavior. This has already helped \nreveal some limitations of convolutional neural networks, which appear to behave \ndifferently from humans under noisy conditions (Geirhos et al., 2017) and to show \ndifferent patterns of failures across exemplars (Rajalingham et al., 2018).  \nDeep neural networks bridge the gap between neurobiology and cognitive function, \nproviding an exciting framework for modeling brain information processing. Theories of \nhow the brain computes can now be subjected to rigorous tests by simulation. Our \nKriegeskorte & Golan (2019) Neural network models and deep learning \n \n11 \n \ntheories, and the models that implement them, will evolve as we learn to explain the rich \nmeasurements of brain activity and behavior provided by modern technologies in animals \nand humans. \n \nReferences \nCadieu CF, Hong H, Yamins DLK, Pinto N, Ardila D, Solomon EA, Majaj NJ, DiCarlo JJ \n(2014) Deep neural networks rival the representation of primate IT cortex for core \nvisual object recognition. PLoS Comput Biol 10:e1003963. \nCichy RM, Khosla A, Pantazis D, Torralba A, Oliva A (2016) Comparison of deep neural \nnetworks to spatio-temporal cortical dynamics of human visual object recognition \nreveals hierarchical correspondence. Sci Rep 6:27755. \nCortes C, Vapnik V (1995) Support-vector networks. Mach Learn 20:273–297. \nCybenko G (1989) Approximation by superpositions of a sigmoidal function. Math Control \nSignals Systems 2:303–314. \nDauphin YN, Pascanu R, Gulcehre C, Cho K, Ganguli S, Bengio Y (2014) Identifying and \nAttacking the Saddle Point Problem in High-dimensional Non-convex Optimization. \nIn: Proceedings of the 27th International Conference on Neural Information \nProcessing Systems - Volume 2, pp 2933–2941 NIPS’14. Cambridge, MA, USA: MIT \nPress. \nDayan P, Abbott LF (2001) Chapter 7.4, Recurrent Neural networks. In: Theoretical \nneuroscience. Cambridge, MA: MIT Press. \nDiedrichsen J, Kriegeskorte N (2017) Representational models: A common framework for \nunderstanding \nencoding, \npattern-component, \nand \nrepresentational-similarity \nanalysis. PLoS Comput Biol 13:e1005508. \nDiedrichsen J, Ridgway GR, Friston KJ, Wiestler T (2011) Comparing the similarity and \nspatial structure of neural representations: a pattern-component model. Neuroimage \n55:1665–1678. \nDiedrichsen J, Yokoi A, Arbuckle SA (2017) Pattern component modeling: A flexible \napproach for understanding the representational structure of brain activity patterns. \nNeuroimage Available at: http://dx.doi.org/10.1016/j.neuroimage.2017.08.051. \nEickenberg M, Gramfort A, Varoquaux G, Thirion B (2017) Seeing it all: Convolutional \nnetwork layers map the function of the human visual system. Neuroimage 152:184–\n194. \nErhan D, Bengio Y, Courville A, Vincent P (2009) Visualizing higher-layer features of a \ndeep network. University of Montreal 1341:1. \nFukushima K, Miyake S (1982) Neocognitron: A Self-Organizing Neural Network Model \nfor a Mechanism of Visual Pattern Recognition. In: Competition and Cooperation in \nNeural Nets, pp 267–285. Springer Berlin Heidelberg. \nGeirhos R, Janssen DHJ, Schütt HH, Rauber J, Bethge M, Wichmann FA (2017) \nComparing deep neural networks against humans: object recognition when the signal \ngets weaker. arXiv [csCV] Available at: http://arxiv.org/abs/1706.06969. \nKriegeskorte & Golan (2019) Neural network models and deep learning \n \n12 \n \nGoodfellow I, Bengio Y, Courville A (2016) Deep Learning. MIT Press. \nGoodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S, Courville A, \nBengio Y (2014) Generative Adversarial Nets. In: Advances in Neural Information \nProcessing Systems 27 (Ghahramani Z, Welling M, Cortes C, Lawrence ND, \nWeinberger KQ, eds), pp 2672–2680. Curran Associates, Inc. \nGüçlü U, van Gerven MAJ (2015) Deep Neural Networks Reveal a Gradient in the \nComplexity of Neural Representations across the Ventral Stream. J Neurosci \n35:10005–10014. \nHinton GE, Salakhutdinov RR (2006) Reducing the dimensionality of data with neural \nnetworks. Science 313:504–507. \nHornik K, Stinchcombe M, White H (1989) Multilayer feedforward networks are universal \napproximators. Neural Netw 2:359–366. \nJo J, Bengio Y (2017) Measuring the tendency of CNNs to Learn Surface Statistical \nRegularities. arXiv [csLG] Available at: http://arxiv.org/abs/1711.11561. \nKawaguchi K (2016) Deep Learning without Poor Local Minima. In: Advances in Neural \nInformation Processing Systems 29 (Lee DD, Sugiyama M, Luxburg UV, Guyon I, \nGarnett R, eds), pp 586–594. Curran Associates, Inc. \nKay KN, Naselaris T, Prenger RJ, Gallant JL (2008) Identifying natural images from \nhuman brain activity. Nature 452:352–355. \nKhaligh-Razavi S-M, Kriegeskorte N (2014) Deep supervised, but not unsupervised, \nmodels may explain IT cortical representation. PLoS Comput Biol 10:e1003915. \nKietzmann TC, McClure P, Kriegeskorte N (2019) Deep Neural Networks in \nComputational Neuroscience. Available at: \nhttp://oxfordre.com/neuroscience/view/10.1093/acrefore/9780190264086.001.0001/\nacrefore-9780190264086-e-46. \nKriegeskorte N (2015) Deep Neural Networks: A New Framework for Modeling Biological \nVision and Brain Information Processing. Annu Rev Vis Sci 1:417–446. \nKriegeskorte N, Diedrichsen J (2016) Inferring brain-computational mechanisms with \nmodels of activity measurements. Philos Trans R Soc Lond B Biol Sci 371 Available \nat: http://dx.doi.org/10.1098/rstb.2016.0278. \nKriegeskorte N, Douglas PK (2018) Cognitive computational neuroscience. Nat Neurosci \n21:1148–1160. \nKriegeskorte N, Mur M, Bandettini P (2008) Representational similarity analysis - \nconnecting the branches of systems neuroscience. Front Syst Neurosci 2:4. \nKrizhevsky A, Sutskever I, Hinton GE (2012) ImageNet Classification with Deep \nConvolutional Neural Networks. In: Advances in Neural Information Processing \nSystems 25 (Pereira F, Burges CJC, Bottou L, Weinberger KQ, eds), pp 1097–1105. \nCurran Associates, Inc. \nLeCun Y, Bengio Y, Hinton G (2015) Deep learning. Nature 521:436. \nLeCun Y, Boser B, Denker JS, Henderson D, Howard RE, Hubbard W, Jackel LD (1989) \nBackpropagation Applied to Handwritten Zip Code Recognition. Neural Comput \nKriegeskorte & Golan (2019) Neural network models and deep learning \n \n13 \n \n1:541–551. \nLin HW, Tegmark M, Rolnick D (2017) Why Does Deep and Cheap Learning Work So \nWell? J Stat Phys 168:1223–1247. \nMcCulloch WS, Pitts W (1943) A logical calculus of the ideas immanent in nervous \nactivity. Bull Math Biophys 5:115–133. \nMitchell TM, Shinkareva SV, Carlson A, Chang K-M, Malave VL, Mason RA, Just MA \n(2008) Predicting human brain activity associated with the meanings of nouns. \nScience 320:1191–1195. \nMnih V, Kavukcuoglu K, Silver D, Rusu AA, Veness J, Bellemare MG, Graves A, \nRiedmiller M, Fidjeland AK, Ostrovski G, Petersen S, Beattie C, Sadik A, Antonoglou \nI, King H, Kumaran D, Wierstra D, Legg S, Hassabis D (2015) Human-level control \nthrough deep reinforcement learning. Nature 518:529. \nNayebi A, Bear D, Kubilius J, Kar K, Ganguli S, Sussillo D, DiCarlo JJ, Yamins DL (2018) \nTask-Driven convolutional recurrent models of the visual system. In: Advances in \nNeural Information Processing Systems, pp 5295–5306. \nNielsen MA (2015) Neural networks and deep learning. Determination Press. \nNili H, Wingfield C, Walther A, Su L, Marslen-Wilson W, Kriegeskorte N (2014) A toolbox \nfor representational similarity analysis. PLoS Comput Biol 10:e1003553. \nPoggio T, Liao Q (2017) Theory ii: Landscape of the empirical risk in deep learning. \nRajalingham R, Issa EB, Bashivan P, Kar K, Schmidt K, DiCarlo JJ (2018) Large-scale, \nhigh-resolution comparison of the core visual object recognition behavior of humans, \nmonkeys, and state-of-the-art deep artificial neural networks. bioRxiv:240614 \nAvailable at: https://www.biorxiv.org/content/early/2018/01/01/240614 [Accessed \nApril 16, 2018]. \nRolnick D, Tegmark M (2017) The power of deeper networks for expressing natural \nfunctions. arXiv [csLG] Available at: http://arxiv.org/abs/1705.05502. \nRosenblatt F (1958) The perceptron: a probabilistic model for information storage and \norganization in the brain. Psychol Rev 65:386–408. \nRumelhart DE, Hinton GE, Williams RJ (1986) Parallel Distributed Processing: \nExplorations in the Microstructure of Cognition, Vol. 1. In (Rumelhart DE, McClelland \nJL, PDP Research Group C, eds), pp 318–362. Cambridge, MA, USA: MIT Press. \nRumelhart DE, McClelland JL, Group PR, Others (1987) Parallel distributed processing. \nMIT press Cambridge, MA. \nSchäfer AM, Zimmermann HG (2006) Recurrent Neural Networks Are Universal \nApproximators. In: Artificial Neural Networks -- ICANN 2006 (Kollias SD, Stafylopatis \nA, Duch W, Oja E, eds), pp 632–640. Berlin, Heidelberg: Springer Berlin Heidelberg. \nSchmidhuber J (2015) Deep learning in neural networks: An overview. Neural Netw \n61:85–117. \nSchölkopf B, Smola AJ (2002) Learning with Kernels: Support Vector Machines, \nRegularization, Optimization, and Beyond. MIT Press. \nSpoerer CJ, McClure P, Kriegeskorte N (2017) Recurrent Convolutional Neural Networks: \nKriegeskorte & Golan (2019) Neural network models and deep learning \n \n14 \n \nA Better Model of Biological Object Recognition. Front Psychol 8:1551. \nStanley KO, Clune J, Lehman J, Miikkulainen R (2019) Designing neural networks \nthrough neuroevolution. Nature Machine Intelligence 1:24–35. \nStorrs KR, Kriegeskorte N (in press.) Deep learning for cognitive neuroscience. In: The \nCognitive Neurosciences (6th Edition). (Gazzaniga M, ed). Boston: MIT Press. \nSuch FP, Madhavan V, Conti E, Lehman J, Stanley KO, Clune J (2017) Deep \nNeuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep \nNeural Networks for Reinforcement Learning. arXiv [csNE] Available at: \nhttp://arxiv.org/abs/1712.06567. \nSzegedy C, Zaremba W, Sutskever I, Bruna J, Erhan D, Goodfellow I, Fergus R (2013) \nIntriguing \nproperties \nof \nneural \nnetworks. \narXiv \n[csCV] \nAvailable \nat: \nhttp://arxiv.org/abs/1312.6199. \nWen H, Shi J, Zhang Y, Lu K-H, Cao J, Liu Z (2017) Neural Encoding and Decoding with \nDeep Learning for Dynamic Natural Vision. Cereb Cortex:1–25. \nWerbos PJ (1982) Applications of advances in nonlinear sensitivity analysis. In: System \nModeling and Optimization, pp 762–770. Springer Berlin Heidelberg. \nYamins DLK, DiCarlo JJ (2016) Using goal-driven deep learning models to understand \nsensory cortex. Nat Neurosci 19:356–365. \nYamins DLK, Hong H, Cadieu CF, Solomon EA, Seibert D, DiCarlo JJ (2014) \nPerformance-optimized hierarchical models predict neural responses in higher visual \ncortex. Proc Natl Acad Sci 111:8619–8624. \n",
  "categories": [
    "q-bio.NC",
    "cs.LG",
    "cs.NE"
  ],
  "published": "2019-02-13",
  "updated": "2019-03-01"
}