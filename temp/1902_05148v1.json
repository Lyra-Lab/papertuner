{
  "id": "http://arxiv.org/abs/1902.05148v1",
  "title": "Probabilistic Generative Deep Learning for Molecular Design",
  "authors": [
    "Daniel T. Chang"
  ],
  "abstract": "Probabilistic generative deep learning for molecular design involves the\ndiscovery and design of new molecules and analysis of their structure,\nproperties and activities by probabilistic generative models using the deep\nlearning approach. It leverages the existing huge databases and publications of\nexperimental results, and quantum-mechanical calculations, to learn and explore\nmolecular structure, properties and activities. We discuss the major components\nof probabilistic generative deep learning for molecular design, which include\nmolecular structure, molecular representations, deep generative models,\nmolecular latent representations and latent space, molecular structure-property\nand structure-activity relationships, molecular similarity and molecular\ndesign. We highlight significant recent work using or applicable to this new\napproach.",
  "text": " \n \nProbabilistic Generative Deep Learning for Molecular Design \n \nDaniel T. Chang (张遵) \n \nIBM (Retired) dtchang43@gmail.com \nAbstract: \n \nProbabilistic generative deep learning for molecular design involves the discovery and design of new molecules and \nanalysis of their structure, properties and activities by probabilistic generative models using the deep learning approach. It \nleverages the existing huge databases and publications of experimental results, and quantum-mechanical calculations, to learn \nand explore molecular structure, properties and activities. We discuss the major components of probabilistic generative deep \nlearning for molecular design, which include molecular structure, molecular representations, deep generative models, \nmolecular latent representations and latent space, molecular structure-property and structure-activity relationships, molecular \nsimilarity and molecular design. We highlight significant recent work using or applicable to this new approach. \n1 Introduction \nProbabilistic generative deep learning for molecular design, or probabilistic generative molecular design (PGMD) for \nshort, involves the discovery and design of new molecules and analysis of their structure, properties and activities by \nprobabilistic generative models using the deep learning approach. Probabilistic generative deep learning [1-2] has shown \ngreat promise for molecular design [4-5]. It can leverage the existing huge databases and publications of experimental results, \nand quantum-mechanical calculations, to learn and explore molecular structure, properties and activities. \nThe goal of probabilistic generative deep learning is to learn generative concept representations [1-3], which are latent \nrepresentations, using deep generative models (DGMs) [1]. A key characteristic of generative concept representations is that \nthey can be directly manipulated to generate new concepts with desired attributes. There are two widely used DGM \narchitectures: variational autoencoders and generative adversarial networks. \nThe variational autoencoder (VAE) [1] consists of an encoder network and a decoder network which encodes a data \nexample to latent representations and generates samples from the latent space, respectively. The decoder network is a \ndifferentiable generator network and the encoder network is an auxiliary inference network. Both networks are jointly trained \nusing variational learning, which is mainly applied to prescribed DGMs. Latent representations learned using VAEs \ngenerally are explicit, continuous and with meaningful structure. As such, they are suited for use as generative concept \nrepresentations which can be directly manipulated to generate new concepts with desired attributes. We focus on VAEs. \n \n2 \n \nThe generative adversarial network (GAN) [1] consists of a differentiable generator network and an auxiliary \ndiscriminator network which generates samples from latent representations and discriminates between data samples and \ngenerated samples, respectively. Both networks are jointly trained using adversarial learning, which is mainly applied to \nimplicit DGMs. Latent representations learned using GANs generally are implicit, discrete and without meaningful structure. \nAs such, they may not be suited for use as generative concept representations. \nIn this paper we discuss the major components of PGMD, which include molecular structure, molecular representations, \ndeep generative models, molecular latent representations and latent space, molecular structure-property and structure-activity \nrelationships, molecular similarity and molecular design. We highlight significant recent work using or applicable to PGMD. \n2 Background on Molecular Design \nThe following diagram shows the major components, and their interdependencies, of molecular design: \n \nwhere MSPR stands for molecular structure-property relationships and MSAR for molecular structure-activity relationships. \n2.1 Molecular Structure \nMolecules are microscopic, material bodies with more or less well-defined structure. From a statistical perspective, we \ncan define molecular structure [6] as that which distinguishes a molecule from a collection of its constituent atoms. \nMathematically, molecular structure is measured by the inter-atom probability distribution. Thus an ideal gas of atoms has \nminimal structure; a hydrogen-bonded liquid is more structured; and a crystal or molecular solid is even more so. \nMolecular \nstructure  \nMolecular \nrepresentations  \nMolecular \ndescriptors  \nMolecular \nsimilarity  \nMolecular \ndesign  \nMSPR / \nMSAR  \n \n3 \n \nElectronic structure and molecular structure are intimately related. The attractive forces exerted by the nuclei on \nelectrons impart structure to the electron distribution in a molecule. The electron density ρ(r) uniquely determines the ground \nstate wave function ψ, the ground state electronic energy and the molecular structure, and vice versa. \nThe molecular structure is determined by three elements: constitution, configuration, and conformation [7]. Constitution \nmeans a certain manner and sequence of chemical bonding of atoms and is expressed by topological descriptors, presence and \nabsence of functional groups / fragments, or other descriptors which account for the 2D features of a molecule. Configuration \nis defined by a 3D spatial arrangement of atoms, which is in turn characterized by the valence angles of all atoms that are \ndirectly linked to at least two other atoms. Configuration is expressed by shape descriptors. Finally, the conformations of a \ngiven molecule represent various thermodynamically stable 3D spatial arrangements of its atoms. For simplicity, we focus on \nconstitution since it has the greatest impact on molecular properties and activities. \nFunctional Groups \nFunctional groups are a key aspect of molecular structure. A functional group is a collection of atoms at a site within a \nmolecule with a common bonding pattern. An example is arenes which have alternating single and double C-C bonds in a \nsix-member aromatic ring. The functional group reacts in a typical way, generally independent of the rest of the molecule. \nFunctional groups give the molecule its properties, regardless of what molecule contains it; they are centers of chemical \nreactivities.  \n2.2 Molecular Representations \nMolecular representations provide machine-readable representations of molecular structure. A given molecular structure \ncan have many valid and unambiguous molecular representations. Molecular representations are crucial to PGMD since they \nare used for representing sample molecular structures as training data. \nThe most widely used molecular representations are line notations and molecular graphs. Line notations represent \nmolecular structure as a linear string of characters; molecular graphs represent molecular structure as a graph. Line notations \ninclude the Simplified Molecular-Input Line-Entry System (SMILES) and the IUPAC Chemical Identifier (InChI). Between \nthe two, we focus on SMILES since they are widely used in deep learning.  \n \n \n4 \n \nSMILES \nSMILES [8] represents a valence bond model of molecule structure, which has proved to be an incredibly useful model \nfor chemistry. A valence bond model is a way of allocating a molecule's nuclei and electrons into atoms and bonds in a way \nthat makes sense. The function of SMILES is to clearly represent a particular valence bond model, not dictate which one \nshould be used. SMILES includes specifications of the following elements / aspects of molecular structure: atoms, bonds, \nbranching, rings, disconnections, isomerism and reactions. SMILES can be used in a great variety of ways: a character string, \na list of tokens, a parse tree, or a molecular graph. The open-standards version of SMILES is defined by OpenSMILES \n(http://www.opensmil es.org/), which includes a brief description of Canonical SMILES. A molecular structure expressed in \nCanonical SMILES will always yield the same SMILES string. \nA common approach used in PGMD is to train a generative model on SMILES and then use it to generate SMILES for \nnew molecules with a desired property or activity. However, there is no guarantee that the resulting SMILES will be valid or \neven if they are, that they will represent a reasonable molecular structure. The validity problems can be broken down into \nthose related to the semantics of SMILES, versus those that involve invalid syntax. \nDeepSMILES [9] addresses two of the main causes for invalid syntax. Its syntax avoids the problem of unbalanced \nparentheses by only using close parentheses, where the number of parentheses indicates the branch length. In addition, it \navoids the problem of pairing ring closure symbols by using only a single symbol at the ring closing location, where the \nsymbol indicates the ring size. DeepSMILES can be converted to / from SMILES. \nThe extraction of molecular structures from publicly available documents such as journal articles and patent filings is an \nimportant area of data curation. Unfortunately, most publications do not provide the molecular structures in a machine-\nreadable format. Instead, they contain hand-drawn molecular structure images. Deep learning has been used to provide \nsolutions [17] for both segmenting molecular structure images from documents and for predicting molecular structures from \nthese segmented images. The method takes an image or PDF and performs segmentation using a convolutional neural \nnetwork (CNN). SMILES are then generated using a CNN in combination with a recurrent neural network (RNN) (encoder-\ndecoder) in an end-to-end fashion. \n \n \n \n5 \n \nMolecular Graph \nThe molecular graph [12-13] encodes molecular structure by a graph G = (V, E, µ, υ) where the set of nodes V encodes \nthe set of atoms and the set of edges E encodes the set of bonds. The labeling function µ(v) associates atom features (e.g., \natom type) to each atom v ϵ V and the labeling function υ(e) associates to each edge e ϵ E the corresponding bond features \n(e.g., bond type). \nMolecular graph convolutions [13] are a deep learning architecture for learning from molecular graphs, specifically for \nsmall molecules. They use a simple encoding of the molecular graph and extract meaningful molecule-level features to form \nmolecular representations that can be used in deep learning applications, such as PGMD. The first basic unit of \nrepresentation is an atom layer. The next basic unit of representation is an atom pair (bond) layer. The initial atom features \ninclude atom type, hybridization, hydrogen bonding, aromaticity, ring sizes, formal charge, partial charge and chirality. The \ninitial atom pair (bond) features include bond type, graph distance and same ring. The molecule-level features are \nconstructed only from the top-level atom features, after one final convolution on the atoms.  \n2.3 Molecular Descriptors \nMolecular descriptors [14] are numbers that capture particular features of molecular structure. Topological descriptors, \nwhich are constitutional molecular descriptors, capture 2D features of molecular structure. Numerous topological descriptors \nhave been proposed, including molecular fingerprints and molecular-graph based descriptors.  \nMolecular Fingerprints \nMolecular fingerprints [7] stand for the presence or absence of some features (e.g. fragments) within a molecule. 2D or \n3D features are encoded by setting bits (features) in a bit-string (fingerprint). Molecular fingerprints are the most widely-used \ntopological descriptors. \nExtended-connectivity fingerprints (ECFPs) [15] are topological fingerprints designed for molecular characterization, \nsimilarity searching, and structure-activity modeling. They are among the most popular molecular fingerprints. ECFPs are \nbased upon a process derived from the Morgan algorithm, one of the original methods for molecular comparison. They have \nmany useful qualities: they can be rapidly calculated; they can represent a very large number of different features (up to 4 \nbillion); features are not predefined, and so can represent variation in new structures; features can represent stereo-chemical \ninformation; and different initial atom identifiers can be used to generate different fingerprints, with different uses. \n \n6 \n \nMolecular Graph Based Descriptors \nMolecular descriptors can be generated from molecular graphs through construction of matrices and graph enumeration. \nVarious properties of the molecular graph, such as degree counts for nodes, connectivity, atom types, etc., can be used as \ndescriptors. These are referred to as topological indices (TIs) [29]. TIs can take many forms and are generally defined as \nsome function of the nodes and edges in a molecular graph. Some of the well-known TIs include the Wiener index, Randic \nconnectivity index, Kier higher-order connectivity indices, and shape index. \nA graph convolutional network (GCN) [30] is used to generate differentiable neural molecular descriptors [16] based on \nmolecular graphs. The architecture generalizes standard molecular feature extraction methods based on molecular \nfingerprints, with every non-differentiable operation replaced with a differentiable analog. Each feature of a neural molecular \ndescriptor can be activated by similar but distinct molecular fragments, making the feature representation more meaningful \nthan standard molecular fingerprints. Furthermore, the network allows end-to-end learning of molecular predictors whose \ninputs are molecular graphs. This network is of interest for their potential adaption for use in PGMD. \n2.4 Molecular Structure-Property Relationships \nThe molecular properties of interest include physical properties and chemical properties. Physical properties are any \nproperties of a molecule which can be observed and measured without changing its molecular structure. In contrast, chemical \nproperties are those that can only be observed and measured by performing a chemical reaction, thus changing the structure \nof the molecule. \nMolecular structure-property relationships [23] are indispensible to molecular similarity and molecular design which \ndepend on target molecular properties. Chemists can identify functional groups or fragments related to molecular properties. \nTherefore, it is critical to correctly identify functional groups or fragments, which determine target molecular properties, to \nlearn more accurate molecular structure-property relationships. Molecular structure-property relationships are commonly \nknown as QSPRs (Quantitative Structure-Property Relationships). \nThe most common types of QSPRs used in molecular design are group contribution (GC) methods [29]. These work \nunder the assumption that a molecule’s properties can be predicted by the number of occurrences of various molecular sub-\nstructures called (functional) groups. GC methods define the number of occurrences ng of each of the groups g, which would \nalso be associated with a coefficient cg that quantifies its “contribution” to a particular property P. Properties are calculated \n \n7 \n \nas: P = ∑cgng\ng\n. GC methods represent molecular structure in terms of its functional groups, very analogous to how chemists \ncompare and analyze molecular structure. Different GC methods to estimate different properties usually have different sets of \ngroups. \nMolecular graphs have been used to produce a large number of QSPRs. In particular, topological indices (TIs) are paired \nwith regression coefficients [29] and used to estimate properties in a similar way to GC methods. TIs can discriminate \nbetween very similar molecular structures, often in cases where GC methods cannot. However, they are not as generally \napplicable as GC methods and many of the graph properties are not always readily understandable from a chemical \nperspective. \nDeep Learning Approaches \nThe rise of deep learning offers a new viable solution to elucidate the molecular structure - property relationships \ndirectly from chemical data. The following discuss some relevant work. All are based on molecular representations, i.e., \nSMILES or molecular graphs. They are of interest because of their potential use in PGMD. \nSmiles2vec [22] is a RNN that automatically learns features from SMILES strings to predict a broad range of molecular \nproperties, including toxicity, activity, solubility and solvation energy. \nA GCN is extended with the attention and gate mechanisms [23] to automatically extract features from molecular graphs \nrelated to a target molecular property such as solubility, polarity, synthetic accessibility and photovoltaic efficiency. The \nattention mechanism can differentiate atoms in different chemical environments by considering an interaction of each atom \nwith neighbors. For example, the augmented GCN can recognize polar and nonpolar functional groups as important structural \nfeatures for molecular solubility and polarity. As a result, it can accurately predict molecular properties and place molecular \nstructures with similar properties close to each other in a well-trained latent space. \nGCNs are used for discovering functional groups in organic molecules that contribute to specific molecular properties \n[31]. Molecules are represented as molecular graphs. The GCNs are trained in a supervised way on experimentally-validated \nmolecular training sets (BBBP, BACE, and TOX21) to predict specific molecular properties, e.g., toxicity. Upon learning a \nGCN, its activation patterns are analyzed to automatically identify functional groups using four different methods: gradient-\nbased saliency maps, class activation mapping (CAM), gradient-weighted CAM, and excitation back-propagation. \n \n8 \n \nAbsorption, distribution, metabolism and excretion (ADME) studies are critical for drug discovery. Chemi-Net [24] is a \ndeep neural network architecture for ADME property prediction. It features a molecular GCN combined with the multi-task \ndeep neural networks (MT-DNNs) method to boost prediction accuracy. For Chemi-Net, input SMILES strings are first \nconverted to graph structures using a molecular conformation generator. The resultant molecular graphs are then used for \ntraining and testing. \n2.5 Molecular Structure-Activity Relationships \nThe molecular activities of interest include chemical reactivity and biological activity. Chemical reactivity is the impetus \nfor which a molecule undergoes a chemical reaction, either by itself or with other materials. Biological activity is the inherent \ncapacity of a molecule, such as a drug or toxin, to alter one or more chemical or physiological functions of a cell, tissue, \norgan, or organism. The biological activity of a molecule is determined not only by the molecule's physical and chemical \nproperties but also by its concentration and the duration of cellular exposure to it. Biological activity is driven by chemical \nreactivity. \nMolecular structure-activity relationships are indispensible to molecular similarity and molecular design which depend \non target molecular activities. Again, chemists can identify functional groups or fragments related to molecular activities. \nTherefore, it is critical to correctly identify functional groups or fragments, which determine target activities, to learn more \naccurate molecular structure-activity relationships. Molecular structure-activity relationships are commonly known as QSARs \n(Quantitative Structure-Activity Relationships) [25]. QSARs have typically been used for drug discovery and development in \nthe biological domain. \nFor simplicity, we do not discuss further molecular structure-activity relationships. However, many of the approaches \nand solutions involving molecular structure-property relationships apply equally well to molecular structure-activity \nrelationships, which are broader and more complex. \n2.6 Molecular Similarity \nThe objective of molecular similarity [7] measures is to allow assessment of molecular properties or activities. The ideal \nmeasures, therefore, should be relevant to the molecular property or activity of interest. One of the basic beliefs of chemistry \nis that similarity in molecular structure implies similarity in molecular properties or activities. Thus, molecular similarity \nmeasures have been developed based on similarity between molecular representations or molecular descriptors. All these \n \n9 \n \nmeasures attempt to describe molecular structures by a set of numerical values and define some means for comparison \nbetween them.  \nMolecular Representation Based \nWith SMILES strings, the similarity between molecular structures can be computed using SMILES-based string \nsimilarity functions [26]. Various SMILES-based similarity methods have been adapted and evaluated for drug-target \ninteraction prediction. Among these are the cosine similarity based SMILES kernels which obtain better scores than widely \nused 2D-based similarity kernels. \nGraph kernels are a promising approach for tackling the similarity and learning tasks at the same time for molecular \ngraphs. A general framework for designing graph kernels [27] utilizes the well-known message passing scheme on graphs. \nThe kernels consist of two components. The first component is a kernel between vertices, while the second component is a \nkernel between graphs. The framework is evaluated on the QM9 dataset for molecular graph prediction. Each molecule in \nthe dataset has 13 target properties to predict. The framework achieves lower mean absolute error and root mean squared \nerror values than all the baselines on 10 out of the 13 targets. This framework is of interest and has potential use in PGMD. \nMolecular Descriptor Based \nConstitutional molecular similarity assessment is based on topological descriptors. When molecular fingerprints are \nused, molecules are estimated to be structurally similar if they have many bits in the fingerprints in common.  Fingerprints \nare usually compared by the Tanimoto coefficient [7]: \nτ = \nNA&B\nNA+ NB - NA&B  \nwhere NA is the number of features (bits) in the fingerprint A, NB is the number of features in B, and NA&B is the number of \nfeatures common to A and B. \n2.7 Molecular Design \nThe objective of molecular design is to discover and design new molecules with desired molecular properties or \nactivities. The task is challenging since the chemical space is vast and often difficult to navigate. One of the basic beliefs of \nchemistry is that molecular structure largely determines molecular properties or activities. Therefore, molecular design is \n \n10 \n \nbased on molecular representations or molecular descriptors and optimized for target molecular structure-property \nrelationships or molecular structure-activity relationships.  \nOur focus is on PGMD, which is based on molecular representations, as discussed in the rest of the paper. \n3 PGMD Components and Architecture \nThe major components, and their interdependencies, of PGMD are shown below: \n \nDGMs and molecular latent representations and latent space are new and unique to PGMD. Molecular latent representations \nand latent space make it unnecessary and inappropriate to use molecular descriptors in PGMD. \n3.1 DGM Architecture for PGMD \nThe DGM architecture for PGMD is shown below [5]: \nMolecular \nstructure  \nMolecular \nrepresentations  \nMolecular latent \nrepresentations and \nlatent space \nMolecular \nsimilarity  \nDGMs \nMSPR / \nMSAR  \nMolecular \ndesign  \n \n11 \n \n \nThe DGM architecture extends the standard VAE architecture (encoder – decoder) with the predictor for molecular \nproperties / activities, with all three components jointly trained. \nSee [1-2] for discussions of various VAEs and their pros and cons, and [2] for discussions of various latent variable \nmodels (LVMs) used in VAEs and their pros and cons. \n3.2 Molecular Latent Representations and Latent Space \nThe nature and characteristics of latent representations and latent space learned in a DGM are determined by the LVM \nused in the DGM, the sample data representations, and the DGM architecture and algorithms. See [2] for discussions, \nincluding disentangled and hierarchical latent representations, latent space interpolation, latent space vectors and latent \nspace geometry, as well as their significance to generative concept representations.  \nMolecular latent representations and latent space are the center piece of PGMD. Molecular latent space supports \ninterpolation, optimization and exploration, and is the foundation of molecular structure-property and structure-activity \nrelationships, molecular similarity and molecular design. \n \nMolecular \nRepresentations \n(Input) \n \nEncoder \n(Neural  \nNetwork)\nDecoder \n(Neural  \nNetwork)\n \nInterpolation \nOptimization \nExploration  \nMolecular Latent \nRepresentations and \nLatent Space\nMolecular \nRepresentations \n(Output) \n \nPredictor \n(Neural  \nNetwork)\n \nMolecular \nProperties / \nActivities\n \n12 \n \n4 PGMD: SMILES Based \nSMILES is the most widely used molecular representation. For representing sample molecular structures as training data, \nSMILES can be used in a great variety of ways: a character string, a list of tokens, or a parse tree. \n4.1 CVAE \nSMILES strings are sequences of characters. The CVAE [1, 5, 18] adapts the VAE to sequence by using single-layer \nLong-Short Term Memory (LSTM) RNNs for both the encoder and the decoder. Latent representations learned using the \nCVAE contain entire sequences and can be used to generate coherent new sequences that interpolate between known \nsequences. \nContinuous molecular latent spaces are learned using the CVAE: one with 130K molecules from the QM9 dataset of \nmolecules with fewer than 9 heavy atoms and another with 250K drug-like molecules extracted at random from the ZINC \ndataset. Interpolating linearly between two latent points is found to be inadequate. Instead, spherical linear interpolation \n(slerp) [2] is used, which treats the interpolation as a circle path on an n-dimensional hypersphere. The CVAE is trained \njointly on the reconstruction task and an additional molecular-property prediction task. With joint training for property \nprediction, the distribution of molecules in the molecular latent space is organized by molecular property values. \nThe CVAE is trained on molecular structures (as SMILES strings) from the QM9 and ZINC datasets by using an \nencoder, a decoder, and a predictor. The encoder converts the SMILES string into a continuous molecular latent \nrepresentation, and the decoder converts these molecular latent representations back to SMILES strings. The predictor \nestimates molecular properties from the molecular latent representations. The continuous molecular latent space allows one \nto automatically generate new molecular structures by performing simple latent space operations, such as decoding random \nlatent space points, interpolating between latent space points, or perturbing known latent space vectors. To compensate for \nthe limitation of SMILES strings, the RDKit is used to validate the output molecular structures and invalid ones are \ndiscarded. \nThe continuous molecular latent space also allows the use of powerful gradient-based Bayesian optimization to \nefficiently guide the search for optimized molecular structures with desired molecular properties. In order to create a \nsmoother landscape to perform optimizations, a Gaussian process model is used to model the property predictor model. The \nGaussian process is trained to predict target properties for molecules given the molecular latent representation as an input. \n \n13 \n \n4.2 GVAE \nSMILES is structured data, not pure sequences. As a result, the CVAE will often lead to invalid outputs because of the \nlack of formalization of syntax and semantics serving as constraints. Context-free grammars can be used to incorporate \nsyntax constraints. To do so, SMILES is represented as parse trees from the grammar. The GVAE [1, 10] encodes and \ndecodes directly from and to these parse trees, respectively, ensuring the generated outputs are always valid based on the \ngrammar.  \nBy ensuring the generated outputs are always syntactically valid, the GVAE learns a more coherent molecular latent \nspace than the CVAE, in which nearby points decode to similar discrete outputs. This molecular latent space is very smooth; \nin many cases moving from one latent point to another will only change a single atom in a molecule. The training data are \n250K drug-like molecules extracted at random from the ZINC dataset. \nThe GVAE uses the same methodology used by the CVAE, and the ZINC dataset, for molecular design. Due to the use \nof SMILES parse trees which ensure the generated outputs are always syntactically valid, the GVAE generates about twice \nmore valid molecular structures than the CVAE and achieves better optimization by finding new molecular structures with \nbetter molecular properties. \n4.3 SD-VAE \nAlthough the GVAE provides the mechanism for generating syntactically valid outputs, it is incapable to constraint the \nmodel for generating semantically valid outputs. Attribute grammars allow one to attach semantics to a parse tree generated \nby context-free grammar. The SD-VAE [1, 11] incorporates attribute grammar in the VAE such that it addresses both \nsyntactic and semantic constraints and generates outputs that are syntactically valid and semantically coherent. \nThe SD-VAE makes further improvement than the GVAE as a result of generating outputs that are both syntactically \nvalid and semantically coherent. The molecular latent space learned is smoother and more discriminative. The training data \nare the same as in the GVAE. \nThe SD-VAE follows the protocols used by the GVAE for molecular design. Due to the use of SMILES annotated parse \ntrees which ensures the generated outputs are both syntactically valid and semantically coherent, the SD-VAE finds even \nbetter solution in molecular optimization and prediction tasks. \n \n14 \n \n5 PGMD: Molecular Graph Based \nSMILES-based molecular design has two critical limitations [19]. First, SMILES is not designed to capture molecular \nsimilarity. This prevents VAEs from learning smooth molecular embeddings. Second, molecule validity is easier to express \non molecular graphs than linear SMILES. Therefore, operating directly on molecular graphs improves generating valid \nmolecular structures. \n5.1 VAE with Regularization Framework \n \nGenerating semantically valid graphs is a challenging task for DGMs. A regularization framework [20] is used for \ntraining VAEs with GCNs that encourages the satisfaction of validity constraints. The approach is motivated by the \ntransformation of a constrained optimization problem to a regularized unconstrained one. It focuses on the matrix \nrepresentation of graphs and formulates penalty terms that regularize the output distribution of the decoder to encourage the \nsatisfaction of validity constraints. The penalties in effect regularize the distributions of the existence and types of the nodes \nand edges collectively. Examples of validity constraints include graph connectivity and valence in the context of molecular \ngraphs. \n5.2 CGVAE \n \nThe CGVAE [21] is a sequential generative model for molecular graphs built from a VAE with gated graph neural \nnetworks (GGNNs) [30] in the encoder and decoder. It learns latent representations of attributed nodes (atoms) instead of \nentire molecules. The decoder forms nodes and edges alternately. Decoding is performed by first initializing a set of possible \nnodes to connect. The decoder then iterates over the given nodes, performs a step of edge selection and edge labeling for the \ncurrently focused node, passes the current connected molecular graph to a GGNN for updating the node representations, and \nrepeats this process until an edge to a special stop node is selected. This entire process is repeated for a new node in the \ncurrent connected graph and terminates if there are no valid candidates. To help ensure valid molecule generation the decoder \nmakes use of a valency mask to prevent generation of additional bonds on atoms that have already been assigned the \nmaximum number of bonds for that particular atom type. \nContinuous latent spaces are learned using the CGVAE from the QM9, ZINC and CEPDB (a subset of the database \ncontaining 250K randomly sampled organic molecules) datasets. The latent representations consist of attributed atoms \n(nodes) instead of entire molecules. The CGVAE is trained jointly on the reconstruction task and an additional molecular-\nproperty prediction task. With joint training for property prediction, the distribution of atoms in the latent space is shaped by \n \n15 \n \nmolecular properties. The major drawback of the atom-embedded latent space is that one cannot perform molecule-level \nlatent space interpolation and arithmetic on the latent space. \nThe CGVAE is trained on molecular structures (as molecular graphs) from the QM9, ZINC and CEPDB datasets by \nusing an encoder, a decoder, and a predictor. The encoder converts a molecular graph into a continuous latent representation, \nand the decoder converts these latent representations back to molecular graphs. The predictor estimates molecular properties \nfrom the latent representations. The continuous latent space allows the use of the gradient-based GGNN regression model \n[28] to optimize the latent space and direct the generation towards especially interesting molecular structures. The CGVAE is \nexcellent at matching graph statistics, while generating valid, novel and unique molecular structures for all datasets \nconsidered. \n5.3 JT-VAE \nThe JT-VAE [19] generates molecular graphs in two phases. First, it generates a tree-structured object (a junction tree) \nthat represents the scaffold of subgraph components and their coarse relative arrangements. The components are valid \nchemical substructures automatically extracted from the training set using tree decomposition. Second, the subgraphs (nodes \nin the junction tree) are assembled into a coherent molecule graph using a graph message passing network. This approach \nincrementally generates the molecular graph while maintaining molecular validity at every step. The subgraph components \nare used as building blocks both when encoding a molecular graph into a latent representation as well as when decoding \nlatent representations back into valid molecular graphs. Thus, the JT-VAE encoder has two parts: graph encoder and tree \nencoder, so has the decoder: tree decoder and graph decoder. The graph and tree encoders are closely related to message \npassing neural networks (MPNNs) [28, 30]. \nThe JT-VAE is used to learn a continuous molecular latent space from the ZINC dataset. In JT-VAE, both the molecular \ngraph and its associated junction tree offer two complementary representations of molecular structure. Therefore the \nmolecular structure is encoded into a two-part continuous latent representation z = [zT, zG] where zT encodes the tree \nstructure and what the subgraph components are in the tree. zG encodes the graph to capture the fine-grained connectivity. \nThe molecular latent space learned is smoother than that of SD-VAE. \nThe JT-VAE decomposes the challenge of molecular design into two complementary subtasks: learning to represent \nmolecular graphs in a continuous manner that facilitates the prediction and optimization of their properties (encoding); and \nlearning to map an optimized continuous representation back into a molecular graph with desired properties (decoding). The \n \n16 \n \nJT-VAE generates 100% valid molecular structures when sampled from a prior distribution, outperforming the baselines \n(CVAE, GVAE and SD-VAE) by a significant margin for the ZINC dataset.  \nSimilar to the baselines, the continuous molecular latent space allows the use of powerful gradient-based Bayesian \noptimization to efficiently guide the search for optimized molecular structures. A Gaussian process is trained to predict target \nproperties for molecules given the molecular latent representation as an input. The JT-VAE excels in discovering new \nmolecular structures with desired properties, yielding a 30% relative gain over the baselines for the ZINC dataset. \n6 Summary and Conclusion \n \n \n \nPGMD involves the discovery and design of new molecules and analysis of their structure, properties and activities by \nprobabilistic generative models using the deep learning approach. It leverages the existing huge databases and publications of \nexperimental results, and quantum-mechanical calculations, to learn and explore molecular structure, properties and \nactivities. In this paper we discussed the major components of PGMD and highlighted significant recent work using or \napplicable to PGMD. \n \nPGMD is a new and promising approach to molecular design. Two areas deserve significant future work and exploration. \nThe first is molecular graphs due to their flexibility and power for representing molecular structure and the rapid advances in \ngraph neural networks. The second is molecular latent representations and latent space which are central to PGMD and are \nthe foundation of molecular structure-property and structure-activity relationships, molecular similarity and molecular design \nThere are other areas awaiting future work and exploration as well. These include hierarchical molecular latent \nrepresentations (e.g., functional groups), molecular latent space vectors (e.g., molecule vector, group vector, attribute vector), \nmolecular latent space geometry (e.g., geodesic interpolation), and molecular similarity measures based on molecular latent \nrepresentations and latent space. \nReferences \n \n[1] Daniel T. Chang, “Concept-Oriented Deep Learning: Generative Concept Representations,” arXiv preprint \narXiv:1811.06622 (2018). \n[2] Daniel T. Chang, “Latent Variable Modeling for Generative Concept Representations and Deep Generative Models,” \narXiv preprint arXiv: 1812.11856 (2018). \n[3] Daniel T. Chang, “Concept-Oriented Deep Learning,” arXiv preprint arXiv:1806.01756 (2018). \n[4] Peter B. Jørgensen, Mikkel N. Schmidt, and Ole Winther, “Deep Generative Models for Molecular Science,” Molecular \nInformatics, 37(1-2), 1700133, 2018 \n[5] R. Gómez-Bombarelli, J. N. Wei, D. Duvenaud, J. Miguel Hernández-Lobato, B. Sánchez-Lengeling, D. Sheberla, J. \nAguilera-Iparraguirre, T. D. Hirzel, R. P. Adams, and A. Aspuru-Guzik, “Automatic Chemical Design Using a Data-Driven \nContinuous Representation of Molecules,” ACS Cent. Sci. 2018, 4, 268−276. \n[6] N. Sukumar, “Molecular Similarity and Molecule Structure,” ISPC, San Francisco, Aug. 2007.  \n \n17 \n \n[7] N. Nikolova and J. Jaworska, “Approaches to Measure Chemical Similarity: A Review,” in QSAR & Combinatorial \nScience 22, 1006–1026 (2003). \n[8] D. Weininger D,  “SMILES, A Chemical Language and Information System. 1. Introduction to Methodology and \nEncoding Rules,” J Chem Inf Comput Sci 28:31–36 (1988). \n[9] Noel M. O’Boyle and Andrew Dalke, “DeepSMILES: An Adaptation of SMILES for Use in Machine-Learning of \nChemical Structures,” in ChemRxiv (2018). \n[10] Matt J. Kusner, Brooks Paige, and José Miguel Hernández-Lobato, “Grammar Variational Autoencoder,” arXiv \npreprint arXiv:1703.01925 (2017). \n[11] H. Dai, Y. Tian, B. Dai, S. Skiena, and L. Song, “Syntax-directed Variational Autoencoder for Structured Data,” ICLR, \n2018. \n[12]  Benoit Gaüzère, Luc Brun and Didier Villemin,  “A New Hypergraph Molecular Representation,”  6 ièmes \nJournées de la Chémoinformatique., Oct 2013, Nancy, France. pp.1, 2013. \n[13] Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley, “Molecular Graph Convolutions: \nMoving Beyond Fingerprints,” Journal of Computer-Aided Molecular Design, pp. 595–608, 2016. \n[14] Francesca Grisoni, “Molecular Descriptors. Theory and Tips for Real-world Applications,” ETH Zurich (2017).  \n[15] David Rogers and Mathew Hahn, “Extended-Connectivity Fingerprints,” J. Chem. Inf. Model. 2010,  50, 5, 742-754. \n[16] David K. Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik, and \nRyan P. Adams, “Convolutional Networks on Graphs for Learning Molecular Fingerprints,” in Advances in Neural \nInformation Processing Systems (NIPS), pp. 2224–2232, 2015. \n[17] Joshua Staker, Kyle Marshall, Robert Abel and Carolyn McQuaw, “Molecular Structure Extraction from Documents \nUsing Deep Learning,” arXiv preprint arXiv:1802.04903 (2018). \n[18] S. R. Bowman, L. Vilnis, O. Vinyals, A.M. Dai, R. Jozefowicz, and S. Bengio, “Generating Sentences from a \nContinuous Space,” arXiv preprint arXiv:1511.06349 (2015). \n[19] W. Jin, R. Barzilay and T. Jaakkola, “Junction Tree Variational Autoencoder for Molecular Graph Generation,” arXiv \npreprint arXiv:1802.04364 (2018). \n[20] T. Ma, J. Chen, and C. Xiao, “Constrained Generation of Semantically Valid Graphs via Regularizing Variational \nAutoencoders,” in Advances in Neural Information Processing Systems, 2018, pp. 7110–7121. \n[21] Qi Liu, Miltiadis Allamanis, Marc Brockschmidt and Alexander L. Gaunt, “Constrained Graph Variational \nAutoencoders for Molecule Design,” in Neural Information Processing Systems (NIPS), 2018. \n[22] Garrett B Goh, Nathan O Hodas, Charles Siegel, and Abhinav Vishnu, “Smiles2vec: An Interpretable General-purpose \nDeep Neural Network for Predicting Chemical Properties,” arXiv preprint arXiv:1712.02034 (2017). \n[23] Seongok Ryu, Jaechang Lim, and Woo Youn Kim, “Deeply Learning Molecular Structure-Property Relationships Using \nGraph Attention Neural Network,” arXiv preprint arXiv:1805.10988v2 (2018). \n[24] K. Liu, X. Sun, L. Jia, J. Ma, H. Xing, J. Wu, H. Gao, Y. Sun, F. Boulnois, J. Fan, “Chemi-Net: A Graph Convolutional \nNetwork for Accurate Drug Property Prediction,” arXiv preprint arXiv:1803.06236 (2018). \n[25] Tomasz Puzyn, Jerzy Leszczynski and Mark T. D. Cronin (Eds.), Recent Advances in QSAR Studies - Methods and \nApplications, Springer Science+Business Media B.V. 2010. \n[26] Hakime Ozturk, Elif Ozkirimli and Arzucan Ozgur, \"A Comparative Study of SMILES-based Compound Similarity \nFunctions for Drug-target Interaction Prediction,\" in BMC Bioinformatics 17.1 (2016): 128. \n[27] Giannis Nikolentzos and Michalis Vazirgiannis, “Message Passing Graph Kernels,” arXiv preprint arXiv:1808.02510 \n(2018). \n[28] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl, “Neural Message Passing for Quantum Chemistry,” \nin Proceedings of the 34th International Conference on Machine Learning, pages 1263–1272, 2017 \n[29] N. D. Austin, N. V. Sahinidis and D. W. Trahan, “Computer-aided Molecular Design: An Introduction and Review of \nTools, Applications, and Solution Techniques,” arXiv preprint arXiv:1701.03978 (2017). \n[30] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu, “A Comprehensive Survey on Graph Neural Networks,” arXiv \npreprint arXiv:1901.00596 (2019). \n[31] Phillip Pope, Soheil Kolouri, Mohammad Rostrami, Charles Martin and Heiko Hoffmann, “Discovering Molecular \nFunctional Groups Using Graph Convolutional Neural Networks,” arXiv preprint arXiv:1812.00265 (2018). \n \n",
  "categories": [
    "cs.LG"
  ],
  "published": "2019-02-11",
  "updated": "2019-02-11"
}