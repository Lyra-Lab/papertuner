{
  "id": "http://arxiv.org/abs/1902.05542v1",
  "title": "Unsupervised Visuomotor Control through Distributional Planning Networks",
  "authors": [
    "Tianhe Yu",
    "Gleb Shevchuk",
    "Dorsa Sadigh",
    "Chelsea Finn"
  ],
  "abstract": "While reinforcement learning (RL) has the potential to enable robots to\nautonomously acquire a wide range of skills, in practice, RL usually requires\nmanual, per-task engineering of reward functions, especially in real world\nsettings where aspects of the environment needed to compute progress are not\ndirectly accessible. To enable robots to autonomously learn skills, we instead\nconsider the problem of reinforcement learning without access to rewards. We\naim to learn an unsupervised embedding space under which the robot can measure\nprogress towards a goal for itself. Our approach explicitly optimizes for a\nmetric space under which action sequences that reach a particular state are\noptimal when the goal is the final state reached. This enables learning\neffective and control-centric representations that lead to more autonomous\nreinforcement learning algorithms. Our experiments on three simulated\nenvironments and two real-world manipulation problems show that our method can\nlearn effective goal metrics from unlabeled interaction, and use the learned\ngoal metrics for autonomous reinforcement learning.",
  "text": "Unsupervised Visuomotor Control through\nDistributional Planning Networks\nTianhe Yu, Gleb Shevchuk, Dorsa Sadigh, Chelsea Finn\nStanford University\nEmail: {tianheyu,glebs,dorsa,cbﬁnn}@stanford.edu\nAbstract—While reinforcement learning (RL) has the potential\nto enable robots to autonomously acquire a wide range of skills,\nin practice, RL usually requires manual, per-task engineering of\nreward functions, especially in real world settings where aspects\nof the environment needed to compute progress are not directly\naccessible. To enable robots to autonomously learn skills, we\ninstead consider the problem of reinforcement learning without\naccess to rewards. We aim to learn an unsupervised embedding\nspace under which the robot can measure progress towards a\ngoal for itself. Our approach explicitly optimizes for a metric\nspace under which action sequences that reach a particular state\nare optimal when the goal is the ﬁnal state reached. This enables\nlearning effective and control-centric representations that lead\nto more autonomous reinforcement learning algorithms. Our\nexperiments on three simulated environments and two real-world\nmanipulation problems show that our method can learn effective\ngoal metrics from unlabeled interaction, and use the learned goal\nmetrics for autonomous reinforcement learning.\nI. INTRODUCTION\nReinforcement learning (RL) is a promising approach for\nenabling robots to autonomously learn a breadth of visuomotor\nskills such as grasping [36, 24], object insertion and placement\ntasks [31], and non-prehensile manipulation skills [16, 15, 7].\nHowever, reinforcement learning relies heavily on a reward\nfunction or metric that indicates progress towards the goal. In\nthe case of vision-based skills, specifying such a metric is par-\nticularly difﬁcult for a number of reasons. First, object poses\nare not readily accessible and pre-trained object detectors\nstruggle without ﬁne-tuning with data collected in the robot’s\ndomain [39]. Second, even when ﬁne-tuned object detectors\nare available, the location of objects may not be a sufﬁcient\nrepresentation to identify success for some tasks, while a\nmore suitable representation would require task-speciﬁc en-\ngineering. For example, if our goal is to manipulate a rope\ninto a particular shape, the corresponding reward function\nwould need to detect the shape of the rope. In many ways,\nsuch task-speciﬁc engineering of rewards defeats the point of\nautonomous reinforcement learning in the ﬁrst place, as the\nultimate goal of RL is to eliminate such manual and task-\nspeciﬁc efforts.\nMotivated by this problem, one appealing alternative ap-\nproach is to provide an example image of a desired goal\nstate [10, 48, 15, 43, 34, 11], and derive a reward function\nusing the goal image. While such goal observations are appli-\ncable to a variety of goal-centric tasks and often easy for a user\nto provide, they do not solve the problem of rewards entirely:\nna¨ıve distances to the goal image, such as mean squared error\nFig. 1.\nGeneral overview of our method. Our method, DPN, enables au-\ntonomous reinforcement learning, without human-provided reward functions,\non vision-based manipulation problems.\nin pixel space, do not provide a suitable metric space for\nreinforcement learning as they are sensitive to small changes\nin lighting, differences in camera exposure, and distractor\nobjects. In this paper, our goal is to leverage autonomous,\nunlabeled interaction data to learn an underlying informative\nmetric that can enable the robot to achieve a variety of goals\nwith access to only a single image of the task goal. This\ncapability would enable reinforcement learning of such tasks\nto be signiﬁcantly more autonomous.\nTo approach this problem, we aim to learn an embedding\nspace that imposes a metric with respect to a goal image,\nwithout using human supervision. One natural option is to\nuse unsupervised representation learning methods [48, 15, 34].\nHowever, these models are largely trained as density esti-\nmators, meaning that they will pay attention to the most\nsalient aspects of the images rather than the ones that are\nrelevant for control. Instead, our goal is to learn a control-\ncentric representation that takes into account how a sequence\nof actions leads to a particular observation and ignores other\nchanges in the observation space that are not caused by actions.\narXiv:1902.05542v1  [cs.RO]  14 Feb 2019\nOur key insight is that any sequence of actions is optimal\nunder the binary reward function of reaching the ﬁnal state\nresulting from those actions. Further, we can use this prop-\nerty to explicitly optimize for control-centric metric spaces\nfrom unsupervised interactions. In particular, we propose to\nexplicitly optimize for a metric such that the sequences of\nactions that lead to a given goal image have high-likelihood\nwhen optimizing with respect to the metric. Our approach\ncan be viewed as a generalization of universal planning net-\nworks [43] to distributions of actions, while critically showing\nthat such models can be trained from real-world unsupervised\ninteraction rather than simluated expert demonstration data.\nOur experiments on three simulated domains and two real-\nworld domains demonstrate that our approach can effectively\nenable robots to learn reaching, object pushing, and rope\nmanipulation tasks from raw pixel observations without human\nreward feedback and with minimal engineering.\nII. RELATED WORK\nOur work aims to enable a robot to learn a variety of\nskills without human supervision, hence falling under the\ncategory of self-supervised robotic learning [36, 2, 13, 6, 32].\nWe speciﬁcally approach this problem from the perspective\nof representation learning, using the learned embedding as\na goal metric for reinforcement learning for reaching goal\nimages. Prior works have aimed to learn representations for\ncontrol through auto-encoding [27, 48, 15, 16, 34], pre-\ntrained supervised features [40], spatial structure [15, 16, 23],\nand viewpoint invariance [41]. However, unlike these works,\nwe build a metric that speciﬁcally takes into account how\nactions lead to particular states, leading to control-centric\nrepresentations that capture aspects of the observation that can\nbe controlled, while discarding other elements.\nPrevious approaches to building control-centric goal rep-\nresentations include using inverse models [2, 35] and mu-\ntual information estimation [47]. Unlike our approach, these\nmethods will not necessarily encode all of the aspects of an\nobservation needed to reach a goal. Further, inverse models are\nsusceptible to local optimum when planning greedily. Other\nmethods have built effective reward functions or goal metric\nspaces using either expert demonstration data [1, 50, 14, 43],\npre-trained goal-conditioned policies [17], or other forms of\nsupervision [8, 12]. Our approach, on the other hand, does not\nuse supervision and requires only unlabeled interaction data.\nRelated to learning goal representations, a number of prior\nworks have considered the problem of learning control-centric\nstate representations from interaction data [38, 5, 22, 30,\n45, 29], for use with planning or reinforcement learning\nunder a known reward or cost. Other works have combined\nauxiliary representation learning objectives with reinforcement\nlearning [9, 20, 42]. Unlike all of these methods, we focus\non representations that induce accurate and informative goal\nmetrics and do not assume access to any reward functions or\nmetrics on state observations.\nIII. PRELIMINARIES\nOur method builds upon universal planning networks\n(UPN) [43], which learn abstract representations for visuomo-\ntor control tasks using expert demonstration trajectories. The\nrepresentation learned from UPN provides an effective metric\nthat can be used as a reward function to specify new goal state\nfrom images in model-free reinforcement learning.\nTo learn such representations, the UPN is constructed as\na model-based planner that performs gradient-based planning\nin a latent space using a learned forward dynamics model.\nUPN encodes the initial image of a particular control task\ninto the latent space, and then iteratively executes plans to\nreach the latent representation of the goal image. The plans\nare selected via gradient descent on the latent distance between\nthe predicted terminal state and the encoding of the actual\ntarget image. Simultaneously, an outer imitation objective\nensures that the learned plans match the sequences of actions\nof the training demonstrations. Consequentially, UPNs learn\na latent distance metric by directly optimizing a plannable\nrepresentation with which gradient-based planning leads to the\ndesired actions.\nConcretely, given initial and goal observations ot and og,\ne.g., as seen in the two images in Fig. 2, the model uses an\nencoder f to encode the images into latent embeddings:\nxt = f(ot; θenc)\nxg = f(og; θenc),\nwhere f(·, θenc) is a convolutional neural network. After en-\ncoding, the features xt and xg are fed into a gradient descent\nplanner (GDP), which outputs a predicted plan ˆat:t+T to reach\nxg from xt. The GDP is composed of a forward dynamics\nmodel g with parameters θdyn where ˆxt+1 = g(xt, ˆat; θdyn).\nThe learned plan is initialized randomly from a uniform\ndistribution ˆa(0)\nt:t+T ∼U(−1, 1) and is updated iteratively via\ngradient descent as follows:\nˆa(i+1)\nt:t+T = ˆa(i)\nt:t+T −α∇ˆa(i)\nt:t+T Li\nplan,\nwhere α is the gradient descent step size and Li\nplan\n=\n∥ˆx(i)\nt+T +1 −xg∥2\n2. In practice, we ﬁnd the Huber loss is more\neffective than the ℓ2 loss for Lplan. After computing the\npredicted plan, UPN updates the planner by imitating the\nexpert actions a∗\nt:t+T in the outer loop. The imitation objective\nis computed as Limitation = ∥ˆat:t+T −a∗\nt:t+T ∥2\n2 and is used\nto update the parameters of the encoder and forward dynamics\nθ := {θenc, θdyn} respectively:\nθ ←θ −β∇θLimitation\nwhere β is the step size for the outer gradient update.\nSrinivas et al. [43] applied the learned latent metric as a\nreward function for model-free reinforcement learning to a\nrange of visuomotor control tasks in simulation and showed\nthat the robot can quickly solve new tasks with image-based\ngoals using the latent metric. However, in order to learn\neffective representations for new tasks, UPNs require access\nto optimal expert demonstrations, which are difﬁcult and time-\nconsuming to collect, making it difﬁcult to extend to a variety\nof tasks, in particular, real-world tasks. In response, we will\nshow a key extension to UPNs that can effectively learn such\nlatent representations without using expert demonstrations in\nthe next section.\nIV. UNSUPERVISED DISTRIBUTIONAL PLANNING\nNETWORKS\nOur end goal is to enable a robot to use reinforcement\nlearning to reach provided goal images, without requiring\nmanually-provided or hand-engineered rewards. To do so,\nwe will derive an approach for learning a metric space on\nimage observations using only unsupervised interaction data.\nUniversal planning networks (UPNs) [43] show how we can\nlearn such a metric from demonstration data. Further, Ghosh\net al. [17] observe that one can learn such a metric with access\nto a goal-conditioned policy by optimizing for a goal metric\nthat reﬂects the number of actions needed to reach a particular\ngoal. However, if our end-goal is to learn a policy, we are faced\nwith a “chicken-and-egg” problem: does the goal-conditioned\npolicy come ﬁrst or the goal metric? To solve this problem, we\npropose to learn both at the same time. Our key observation is\nthat a sequence of any actions, even random actions, is optimal\nunder the binary reward function of reaching the ﬁnal state\nresulting from those actions. Speciﬁcally, we can use random\ninteraction data to optimize for a metric such that the following\nis true: when we ﬁnd a sequence of actions that minimizes the\ndistance metric between the ﬁnal predicted embedding and the\nembedded goal image, the true sequence of actions has high\nprobability. Concretely, consider interaction data consisting of\nan initial image o1, a sequence of actions a1:t−1 executed by\nthe robot, and the resulting image observation ot. We can use\ndata like this to optimize for a latent space x = f(o; θenc)\nsuch that when we plan to reach xt = f(ot; θenc), we have\nhigh probability of recovering the actions taken to get there,\na1:t−1.\nSo far, this computation is precisely the same as the original\nUPN optimization, except that we perform the optimization\nover randomly sampled interaction data, rather than demon-\nstrations. In particular, we surpass the need for expert demon-\nstrations because of the observation that random interaction\ndata can also be viewed as “expert” behavior with respect to\nthe cost function of reaching a particular goal observation at\nthe last timestep of a trajectory (whereas the original UPN\nwas optimizing with respect to the cost function of reaching\na goal state with a sequence of optimally short actions). Once\nwe have a representation that can effectively measure how\nclose an observation is to a goal observation, we can use it\nas an objective that allows us to optimize for reaching a goal\nobservation quickly and efﬁciently, even though the data that\nwas used to train the network did not reach goals quickly.\nWhile not all robotic tasks can be represented as reaching a\nparticular goal observation, goal reaching is general to a wide\nrange of robotic control tasks, including object arrangement\nsuch as setting a table, deformable object manipulation such as\nfolding a towel, and goal-driven navigation, such as navigating\nto the kitchen.\nHowever, note that, unlike in the case of expert demonstra-\ntion data, we are no longer optimizing for a unique solution\nfor the sequence of actions: there are multiple sequences of\nactions that lead to the same goal. Hence, we need to model all\nof these possibilities. We do so by modeling the distribution of\naction sequences that could achieve the goal, in turn training\nthe UPN as a stochastic neural network to sample different\naction sequences. Interestingly, universal planning networks\nare already stochastic neural networks, since the initial action\nsequence is randomly sampled before each planning optimiza-\ntion. However, as described in Section III, they are trained with\na mean-squared error objective, which encourages the model\nto represent uncertainty by averaging over possible outcomes.\nTo more effectively model the multiple possible sequences of\nactions that can lead to a potential goal observation, we extend\nuniversal planning networks by enabling them to sample from\nthe distribution of potential action sequences. To do so, we\nintroduce latent variables into the UPN model and build upon\nadvances in amortized variational inference [26, 21] to train\nthe model, which we will discuss next.\nA. Distributional Planning Network Model\nTo extend universal planning networks towards modeling\ndistributions over actions, we introduce latent variables into\nthe model. We thus consider the following distribution,\np(at:t+T |ot, ot+T +1)\n=\nZ\np(at:t+T |zt:t+T , ot, ot+T +1)p(zt:t+T )dzt:t+T ,\nby introducing latent variables zt for each timestep t. We\nmodel the prior over each timestep independently, and model\neach marginal as a standard Gaussian:\np(zt:t+T ) =\nt+T\nY\nt′=t\np(zt′)\np(zt) = N(0, I)\nWe model p(at:t+T |zt:t+T , ot, ot+T +1) using a neural net-\nwork with parameters θ with two components. The ﬁrst\ncomponent is a deterministic gradient descent action planner\nwith respect to latent action representations z′\nt:t+T , with\ngradient descent initialized at zt:t+T . The second component\nis a feedforward decoder that maps from an individual latent\naction representation z′\nt to a probability distribution over the\ncorresponding action at. We will next describe these two\ncomponents in more detail before discussing how to train this\nmodel.\nConcretely, the gradient-based planner component consists of:\n(a) an encoder f(·; θenc), which encodes the current and goal\nobservation ot, og into the latent state space xt, xg,\n(b) a latent dynamics model ˆxt+1 = g(xt, z′\nt; θdyn) that now\noperates on latent actions z′\nt rather than actions at, and\n(c) a gradient descent operator on z′\nt:t+T that is initialized at a\nsample from the prior z′(0)\nt:t+T = zt:t+T , and runs np steps\nof gradient descent to produce z′(np)\nt:t+T , using learned step\nsize αi for step i = 1, ..., np.\nFig. 2.\nDiagram of our distributional planning networks model. Our model enables learning a representation x that induces a control-centric goal metric\non images o from unlabeled interaction data. It does so by explicitly training for a metric under which gradient-based planning leads to the a sequence of\nactions that reach the ﬁnal image. To effectively model the many action sequences that might lead to a goal after T timesteps, we introduce latent variables\nzt:t+T and train the model using amortized variational inference.\nLike before, the gradient descent operator computes gradients\nwith respect to the planning loss Lplan, which corresponds to\nthe Huber loss between the predicted ˆxt+T +1 and the encoded\ngoal observation, xg.\nOnce we have computed a sequence of latent actions\nz′(np)\nt:t+T using the planner, we need to decode the latent val-\nues into actions. We do so using a learned action decoder\nh(at|z′\nt; θact). This feedforward neural network outputs the\nmean of a Gaussian distribution over the action with a ﬁxed\nconstant variance. Overall, the parameters of our model are\nθ = {θenc, θdyn, θact, αi}. The architectures for each of these\ncomponents are described in detail in Appendix A. We next\ndescribe how to train this model.\nB. Distributional Planning Network Training\nSince we are training on random interaction data, there are\nmany different sequences of actions that may bring the robot\nfrom one observation to another. To effectively learn from\nthis data, we need to be able to model the distribution over\nsuch action sequences. To do so, we train the above model\nusing tools from amortized variational inference. We use an\ninference network to model the the variational distribution,\nwhich is factorized as\nqφ(zt:t+T |at:t+T ) =\nt+T\nY\nt′=t\nq(zt′|at′; φ),\nwhere q(zt|at; φ) outputs the parameters of a conditional\nGaussian distribution N(µφ(at), σφ(at)). Following Kingma\nand Welling [26], we use this estimated posterior to optimize\nthe variational lower bound on the likelihood of the data:\nLDPN(θ, φ) = −Ezt:t+T ∼qφ [log pθ(at:t+T |ot, ot+T +1)]\n+ βDKL(qφ(zt:t+T |at:t+T ) || p(zt:t+T )). (1)\nA value of β = 1 corresponds to the correct lower bound.\nAs found in a number of prior works (e.g. [19, 4]), we ﬁnd\nthat using a smaller value of β leads to better performance.\nWe compute this objective using random interaction data that\nis autonomously collected by the robot, and optimize it with\nrespect to the model parameters θ and the inference network\nparameters φ using stochastic gradient descent. Mini-batches\nare sampled by sampling a trajectories from the dataset,\no1, a1, ..., and selecting a length-T segment at random within\nthat trajectory: ot, at, ..., at+T , ot+T +1. We compute the ob-\njective using these sampled trajectory segments by ﬁrst passing\nthe executed action sequence into the inference network to\nproduce a distribution over zt:t+T . The second term in the\nobjective operates on this Gaussian distribution directly, while\nthe ﬁrst term is computed using samples from this distribution.\nIn particular, we compute the ﬁrst term by passing observations\not, ot+T +1 and the samples zt:t+T as input to the gradient\ndescent planner, running gradient descent for np timesteps,\nand decoding the result into ˆat:t+T to produce the distribution\npθ(at:t+T |ot, ot+T +1) = N(ˆat:t+T , I). See Figure 2 for a\nsummary of the model and training.\nC. RL with the Learned Goal Metric\nTraining the distributional planning network provides us\nwith several key components. Most importantly, the encoder\nf(·; θenc) of the DPN provides an embedding of images under\nwhich distances to goal images accurately reﬂect whether\nor not a sequence of actions actually reached the goal. The\ncombination of the image encoder f, latent dynamics g, and\naction decoder h serves as a policy that can optimize over a\nsequence of actions that will reach an inputted goal image.\nOne easy way to use the DPN model is directly as a goal-\nconditioned policy. In particular, consider a human-provided\ngoal image og for a desired task. We can compute a sequence\nof actions to reach this goal image ˆat:t+T by running a\nforward pass through the DPN with ot and og as input\nand gradient descent initialized at a sample from the prior\nz′(0)\nt:t+T ∼p(zt:t+T ). However, note that the model outputs a\ndistribution over all action sequences that might reach the ﬁnal\nstate after T actions. This means that we can expect the action\nsequence produced by DPN to reach the goal, but may not do\nso in a timely manner. In turn, the DPN encoder represents\na true metric of whether or not an embedded image x has\nreached the same state as another embedded image x′, as it\nis minimizes for an action sequence that reaches the correct\nimage. As a result, we can alternatively use this metric space\nwith reinforcement learning to optimize for efﬁciently reaching\na goal image.\nThus, after training the DPN model on autonomous, unla-\nbeled interaction, we discard most of the DPN model, only\nkeeping the encoder f(·; θenc): this encoder provides a goal\nmetric on images. To enable a robot to autonomously learn\nnew tasks speciﬁed by a human, we assume a human can\nprovide an image of the goal og, from the perspective of the\nrobot. We then run reinforcement learning, without hand-tuned\nreward functions, by deriving a reward function from this goal\nmetric. We derive rewards according to the following equation:\nr(ot; og) = −exp(Lδ(ot, og))\nwhere Lδ corresponds to the Huber loss:\nLδ(ot, og) = ∥dDPN(f(ot; θenc) −f(og; θenc), δ)∥1\nwhere for the i-th entry xi of some vector x,\ndDPN(x, δ)i =\n(\n1\n2x2\ni\nfor |xi| ≤δ,\nδ |xi| −1\n2δ2 otherwise.\nFollowing Srinivas et al. [43], we use δ = 0.85. We then\nuse the soft actor-critic (SAC) algorithm [18] for running\nreinforcement learning with respect to this reward function.\nV. EXPERIMENTS\nThe goal of our experiments is to test our primary hypoth-\nesis: can DPN learn an accurate and informative goal metric\nusing only unlabeled experience? We design several simulated\nand real world experiments in order to test this hypothesis with\nboth synthetic and real images in multiple domains, ranging\nfrom simple visual reaching tasks to more complex object\narrangement problems. In all cases, the objective is to reach\nthe goal state which is illustrated to the agent by a goal image.\nWe will release our code upon publication, and you can ﬁnd\nvideos of all results at the following link1.\nTo quantify the performance of DPN, we compare our\nmethod to leading prior approaches for learning goal metrics\nfrom unlabeled interaction data. In particular, we compare to\nthe following approaches:\n• We train a multi-step inverse model to predict the\nintermediate\nactions\nat:t+T\ngiven\ntwo\nobservations\not, ot+T +1. Following Agrawal et al. [2] and Pathak et al.\n[35], we use a siamese neural network that ﬁrst embeds\nthe two observations and then predicts the actions from\nthe concatenated embeddings. We include a forward-\nconsistency loss as a regularizer of the inverse model\nsuggested in Pathak et al. [35]. We use the embedding\nspace as a goal metric space.\n1The supplementary website is at https://sites.google.com/view/dpn-public\nFig. 3.\nWe conduct experiments on several different vision-based mani-\npluation domains, including simluated rope manipulation, simulated pushing,\nrobot reaching, and robot pushing in the real world.\n• We train a variational autoencoder (VAE) [26], and use\ndistances in the latent space as a goal metric, as done by\nprior work [34].\n• We lastly evaluate ℓ2 distance in pixel space as the goal\nmetric.\nAll of the above approaches are trained on the same unlabeled\ndatasets for each problem domain, except for the pixel distance\nmetric, which is not learned. Because inverse models have a\ntendency to only pay attention to the most prominent features\nof an image to identify the actions, we expect the inverse mod-\nels to work best in situations where the robot’s embodiment\nis a critical aspect of the goal, such as reaching tasks, rather\nthan tasks involving objects. However, even in situations such\nas reaching, the metric underlying the learned embedding may\nnot correspond to true distances between states in a meaningful\nway. On the other hand, because VAEs learn an embedding\nspace by reconstructing the input, we expect VAEs to work\nbest in situations where the goal involves particularly salient\nparts of the image. Finally, we do not expect pixel error to\nwork well, as matching pixels exactly is susceptible to local\nminima and sensitive to lighting variation and sharp textures.\nFinally, we use Soft Actor-Critic (SAC) [18] with default\nhyperparameters as the RL algorithm for training all experi-\nments with all four metrics respectively.\nWe provide full hyperparameters, architecture information,\nand experimental setup details in Appendix A in the supple-\nmental material.\nA. Simulation Experiments\nWe evaluate our approach starting from simulated experi-\nments using the MuJoCo physics engine [46]. For all simulated\nexperiments, the inputs ot and og are 100×100 RGB images.\nSimulated Reaching. The ﬁrst experimental domain is a\nplanar reaching task, where the goal of the task is for a 2-\nlink arm to reach a colored block. We collect 30000 videos\nFig. 4.\nQuantitative simulation results that evaluate the effectiveness of the goal metrics induced by each method by measuring the true distance to the goal\nstate when running reinforcement learning with the reward derived from the learned goal metric. Performance is averaged across multiple tasks and error bars\nindicate standard error. Each RL step requires 20 samples from the environment.\ntime\nlatent distance \n(normalized)\ngoal image\nInverse model metric\nPixel metric\nVAE metric\nDPN metric\ntrajectories of learned RL policy w.r.t. four metrics\nFig. 5.\nComparisons of normalized latent distance to the goal determined by four approaches for the simulated rope manipulation task. We evaluate each\nlatent metric on the trajectories (from a top-down view) of RL policy with respect to DPN, inverse model, VAE, and pixel space, shown above from left to\nright. Note in the leftmost plot that, though the metric learned by the inverse model achieves a lower normalized latent distance than the DPN metric, it goes\nto around 0 once the gripper moves closer to its corresponding position in the goal image without touching the rope as shown in the second and fourth plot\nfrom the left. This suggests that the inverse model metric fails to capture the actual goal of task, which is directing the rope to the right form.\nlatent distance \n(normalized)\ntrajectories of learned RL policy w.r.t. two metrics\ngoal image\ntime\nDPN metric\nInverse model metric\nFig. 6.\nComparisons of normalized latent distance to the goal determined by DPN and inverse model for the real-world pushing task. We evaluate each\nlatent metric on the trajectories of RL policy with respect to DPN and inverse model respectively, shown in the images above from left to right.\nof unlabeled physical interactions and train DPN, the inverse\nmodel, and the VAE on the random dataset. Note that since this\nis a fully observed setup with no object interaction, we do not\nuse the multi-step inverse model with recurrence and instead\nuse a one-step inverse model with feed-forward networks, as\nsuggested by Pathak et al. [35].\nWe evaluate all learned metrics along with the pixel space\nmetric by running SAC on 10 different tasks where the target\nblock is at a different position for each task. The input to the\nRL policy is the joint angles of the robot. We summarize the\ncomparison in Figure 4. As shown in the plot, our method is\nable to reach the goal within 0.05cm in 60 RL steps and gets\ncloser to 0 after 100 steps (see Figure 7). The RL policies with\nmetrics learned by VAE and the inverse model are also able to\nget to the proximity of the goal but are less accurate. This is\nreasonable since VAEs usually pay attention to the most salient\npart of the image while inverse models usually pay attention to\nobjects that correlate most with the actions, i.e. the robot arm\nin this domain. Tracking the arm movement is sufﬁcient to\nsolving reaching tasks. The pixel space distance, meanwhile,\nstruggles to ﬁnd the goal as expected since pixel-wise distance\nis susceptible to minor environmental changes.\nSimulated Rope Manipulation. The goal of the second\nexperiment is to manipulate a rope of 7 pearls into various\nshapes using a parallel-jaw gripper, where the setup is shown\nin Figure 3. In this experiment, we aim to test if our method\nFig. 7.\nRoll-outs of learned RL policy using the DPN metric of simulated\nreaching, simulated pushing, and robot reaching experiments from top to\nbottom.\ncan focus on the shape of the rope rather than the position\nof the gripper since, unlike simulated reaching, only encoding\nthe movement of the gripper into the latent space would lead\nto ignoring the actual goal of the task: manipulating the rope.\nWe collect 20000 10-frame random videos. Similar to\nsimulated reaching, we then train a one-step inverse model\nfor rope manipulation.\nWe evaluate the four metrics by running SAC on 4 tasks\nwhere the rope is displaced to a different shape in each task.\nThe input to the RL policy are the end-effector positions and\nvelocities of the gripper. For evaluation, we deﬁne the true\ndistance to goal following Xie et al. [49], measuring the aver-\nage distance of corresponding pearls in the rope. As seen from\nthe results in Figure 4, our method does substantially better\nthan the other three approaches, achieving around 0.05cm on\naverage to the shapes shown in the goal images. The other\nthree approaches fail to lead to effective RL training. To\nconduct a more direct comparison of all the latent metrics,\nwe plot the latent distance to the goal of four approaches\nwhen rolling out the trajectories of learned RL policy with all\nthe four metrics as reward functions respectively in Figure 5.\nNotice that the metrics learned by the inverse model and the\nVAE go to around 0 once the gripper goes to its corresponding\nposition in the goal image but completely disregards the rope.\nIn contrast, the DPN metric only decreases when the rope is\nmanipulated to the target shape. This experiment demonstrates\nthat DPN is able to produce a more informative metric without\ncollapsing to the most salient feature in the image.\nSimulated Pushing. For our third simulated experiment, we\nperform a simulated pushing task where a robot arm must\npush a target object to a particular goal position amid one\ndistractor. In order to make this environment more realistic,\nwe use meshes of a vase and a strainer from thingiverse.com\nwith different textures for the two objects and a marble texture\nfor the table (see Figure 3).\nFor this task, we collect 3000 random 16-frame videos and\ntrain a multi-step recurrent inverse model for comparison.\nBased on the previous two experiments, the pixel distance\ndoes not serve as a good metric, so we drop it and only eval-\nuate DPN, inverse models and VAEs. For this task, knowing\nonly the hand’s position is not sufﬁcient to solve the task.\nHence, in addition to end-effector positions and velocities of\nthe robot hand, a latent representation of the current image\nextracted from each method respectively is also provided as\nan input the RL policy. As seen in Figure 4, DPN learns to\npush both objects toward the goal position with a distance\nclose to 0cm (see Figure 7 for an example). The multi-step\ninverse model is only able to push one of the objects to goal,\nas indicated by the large standard error in Figure 4. The VAE\nmetric can not quite learn how to push both objects and RL\ntraining does not make signiﬁcant progress.\nB. Real World Robot Experiments\nIn order to ascertain how well our approach works in the\nreal world with real images, we evaluate on two robot domains.\nSimilar to the simulated tasks, a robot, a Fetch Manipulator,\nneeds to reach a goal image using images from a forward-\nfacing camera by applying continuous end effector velocities.\nBoth setups are shown in Figure 3.\nRobot Reaching. First, we evaluate our method for a simple\nenvironment, where the robot must learn to move its end-\neffector along the xy plane to a certain position above a\ntable. We collect unlabeled interaction data by having the\narm randomly move above the table. For this task, we capture\n5000 episodes, which corresponds to approximately 28 hours\nof capture.\nWe train DPN on 4000 samples and use the remaining 1000\nsamples for validation. Seeing that the inverse and VAE mod-\nels are the next best-performing in simulation experiments,\nwe also train both on this dataset using the same procedures\ndescribed earlier. We also use the one-step inverse model\nfor this experiment with the same reason discussed in the\nsimulated reaching section.\nFor the ﬁrst task, the goal image consists of the arm hover-\ning above a hole at the front-middle of the table. For the second\ntask, the goal image consists of the arm hovering at the top\nleft corner of the table. For both tasks, we run reinforcement\nlearning for approximately 300 episodes, corresponding to\naround 3 hours, for each of the DPN, the inverse model,\nand VAE metrics. To evaluate performance, we roll out the\nlearned policy 3 times at the checkpoint that achieved the\nhighest training reward, according to the learned metric, and\ncompute the average true distances to goal position at each\nﬁnal timestep (see Figure 7). As shown in Figure 8, we ﬁnd\nthat the policies that use our DPN metric and the inverse metric\nperforms fairly similarly, while the agent that used the VAE\nmetric performs considerably worse.\nAcross these two tasks, the VAE model may have performed\nworse due to slight changes in lighting and camera angle,\nas previously discussed. Furthermore, as discussed in the\nsimulated reaching example, both DPN and the inverse model\nlikely performed similarly because they both focused on the\nparts of the videos that correlate most heavily with the actions,\ni.e. the location of the arm.\nRobot Pushing. Seeing that DPN and the inverse model\nperformed similarly on the reaching task, we construct a\nharder object pushing task that requires the two metrics to\npay attention to smaller features in the environment and to\ntake longer-horizon interactions into account.\nFor this task, the robot has to maneuver a given object from\nan initial position to a goal position on the table. This task is\ncomplicated by a distractor object next to the actual object.\nWe choose two stuffed animals as our given objects and use\nan aluminum hopper to prevent them from falling off the table\nduring data collection and RL. Data collection occurs in the\nsame manner as in the reaching experiment. In total, we collect\n5000 episodes, taking approximately one day on one robot.\nSince our simulated pushing results indicated that the VAE\nmetric performed poorly, we only compare DPN with the\ninverse model for this experiment. Again, we trained an agent\nwith each of the DPN and inverse metrics for approximately\n400 episodes and roll the highest-reward policy out 3 times.\nAs shown in Figure 8, the agent trained with DPN is able to\nsuccessfully push the stuffed animal to the goal position, while\nthe agent trained with the inverse model is at best only able\nto push it to the middle of the table. The two metric curves\nshown in Figure 6 give some intuition as to why this is the\ncase. When we plot the metrics for the policy learned w.r.t.\nthe DPN metric, we see that both metrics correctly decrease,\nalthough the DPN metric is better at recognizing the similarity\nof later images, and is thus smaller. Meanwhile, when we plot\nthem for the policy learned w.r.t. the inverse model, we see\nthat the inverse metric incorrectly associates a small latent\ndistance with the earlier images and thus rewards the RL\nagent for doing nothing, making it difﬁcult for the RL agent\nto meaningfully move the object. These results seem to match\nwhat we saw in the simulated pushing experiment and suggest\nthat DPN does better at distinguishing smaller features that are\nimportant for a goal, while the inverse model ignores them and\nover-prioritizes arm placement.\nVI. DISCUSSION\nSummary. In this paper, we presented an approach for unsu-\npervised learning of a control-centric metric space on images\nthat allows a robot to evaluate its progress towards a speciﬁc\ngoal. Our approach proposes more effective and autonomous\nreinforcement learning while only having access to the goal\nimage by leveraging the learned metric. We then evaluated our\nmethod on simulated and real-world tasks, including reaching,\npushing, and rope manipulation. Our results suggest that the\nDPN metric enables RL to perform well on these robotics\ntasks while converging faster than state-of-the-art techniques\nfor unsupervised goal representations.\nLimitations and Future Work. While we are able to show\nvery good performance for an interesting set of robotics tasks,\nour method is limited to goal-reaching tasks, which leaves out\na number of other interesting RL tasks. Despite this limitation,\nwe believe learning a control-centric metric using our approach\nmay be applicable to a wider range of settings by applying the\nFig. 8.\nResults for the real world reaching and pushing tasks. Our approach\nis able to learn a metric on real images that enables successful autonomous\nRL for both reaching and object pushing, whereas prior methods do not\nconsistently lead to successful reinforcement learning.\ncost function towards tracking an entire trajectory rather than\nsimply a ﬁnal state. We are excited to take our approach be-\nyond the current tasks and consider learning control-theoretic\nmetrics in this wider range of settings.\nBeyond vanilla RL, which we study in this paper, a number\nof other methods rely heavily on effective distance metrics\nin state space, which have so far limited their application to\nnon-vision domains due to the lack of suitable metrics. This\nincludes goal relabeling for multi-goal RL [3, 37], planning\nwith learned models [28, 33], and automatic curriculum gen-\neration [44]. In future work, we hope to explore the use of\nour metric in combination with these methods.\nACKNOWLEDGMENTS\nWe thank Aravind Srinivas and Sergey Levine for helpful\ndiscussions. This work has been partially supported by JD.com\nAmerican Technologies Corporation (”JD”) under the SAIL-\nJD AI Research Initiative. This article solely reﬂects the\nopinions and conclusions of its authors and not JD or any\nentity associated with JD.com.\nREFERENCES\n[1] Pieter Abbeel and Andrew Ng. Apprenticeship learning\nvia inverse reinforcement learning.\nIn International\nConference on Machine Learning (ICML), 2004.\n[2] Pulkit Agrawal, Ashvin V Nair, Pieter Abbeel, Jitendra\nMalik, and Sergey Levine. Learning to poke by poking:\nExperiential learning of intuitive physics. In Advances\nin Neural Information Processing Systems, pages 5074–\n5082, 2016.\n[3] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas\nSchneider, Rachel Fong, Peter Welinder, Bob Mc-\nGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech\nZaremba.\nHindsight experience replay.\nIn Advances\nin Neural Information Processing Systems, pages 5048–\n5058, 2017.\n[4] Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan,\nRoy H Campbell, and Sergey Levine. Stochastic varia-\ntional video prediction. arXiv preprint arXiv:1710.11252,\n2017.\n[5] Byron Boots, Sajid M Siddiqi, and Geoffrey J Gordon.\nClosing the learning-planning loop with predictive state\nrepresentations. The International Journal of Robotics\nResearch, 30(7):954–966, 2011.\n[6] Arunkumar Byravan, Felix Leeb, Franziska Meier, and\nDieter Fox.\nSe3-pose-nets: Structured deep dynamics\nmodels for visuomotor planning and control.\narXiv\npreprint arXiv:1710.00489, 2017.\n[7] Yevgen Chebotar, Karol Hausman, Marvin Zhang, Gau-\nrav\nSukhatme,\nStefan\nSchaal,\nand\nSergey\nLevine.\nCombining model-based and model-free updates for\ntrajectory-centric reinforcement learning.\nIn Interna-\ntional Conference on Machine Learning, pages 703–711,\n2017.\n[8] Christian Daniel, Malte Viering, Jan Metz, Oliver Kroe-\nmer, and Jan Peters. Active reward learning. In Proceed-\nings of Robotics: Science and Systems, Berkeley, USA,\nJuly 2014. doi: 10.15607/RSS.2014.X.031.\n[9] Tim de Bruin, Jens Kober, Karl Tuyls, and Robert\nBabuˇska.\nIntegrating state representation learning into\ndeep reinforcement learning.\nIEEE Robotics and Au-\ntomation Letters, 3(3):1394–1401, 2018.\n[10] Koichiro Deguchi and Isao Takahashi.\nImage-based\nsimultaneous control of robot and target object motions\nby direct-image-interpretation method.\nIn Intelligent\nRobots and Systems, 1999. IROS’99. Proceedings. 1999\nIEEE/RSJ International Conference on, volume 1, pages\n375–380. IEEE, 1999.\n[11] Ashley D Edwards. Perceptual Goal Speciﬁcations for\nReinforcement Learning. PhD thesis, Georgia Institute\nof Technology, 2017.\n[12] Ashley D Edwards, Srijan Sood, and Charles L Is-\nbell Jr. Cross-domain perceptual reward functions. arXiv\npreprint arXiv:1705.09045, 2017.\n[13] Chelsea Finn and Sergey Levine. Deep visual foresight\nfor planning robot motion.\n2017 IEEE International\nConference on Robotics and Automation (ICRA), 2017.\n[14] Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided\ncost learning: Deep inverse optimal control via policy\noptimization. In International Conference on Machine\nLearning (ICML), 2016.\n[15] Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell,\nSergey Levine, and Pieter Abbeel.\nDeep spatial au-\ntoencoders for visuomotor learning.\nIn International\nConference on Robotics and Automation (ICRA), 2016.\n[16] Ali Ghadirzadeh, Atsuto Maki, Danica Kragic, and\nM˚arten Bj¨orkman. Deep predictive policy training using\nreinforcement learning. In Intelligent Robots and Systems\n(IROS), 2017 IEEE/RSJ International Conference on,\npages 2351–2358. IEEE, 2017.\n[17] Dibya Ghosh, Abhishek Gupta, and Sergey Levine.\nLearning\nactionable\nrepresentations\nwith\ngoal-\nconditioned policies. arXiv preprint arXiv:1811.07819,\n2018.\n[18] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen,\nGeorge Tucker, Sehoon Ha, Jie Tan, Vikash Kumar,\nHenry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey\nLevine.\nSoft actor-critic algorithms and applications.\narXiv preprint arXiv:1812.05905, 2018.\n[19] Irina Higgins, Loic Matthey, Arka Pal, Christopher\nBurgess, Xavier Glorot, Matthew Botvinick, Shakir Mo-\nhamed, and Alexander Lerchner.\nbeta-vae: Learning\nbasic visual concepts with a constrained variational\nframework.\nIn International Conference on Learning\nRepresentations, 2017.\n[20] Max Jaderberg, Volodymyr Mnih, Wojciech Marian\nCzarnecki, Tom Schaul, Joel Z Leibo, David Sil-\nver, and Koray Kavukcuoglu.\nReinforcement learn-\ning with unsupervised auxiliary tasks.\narXiv preprint\narXiv:1611.05397, 2016.\n[21] Matthew Johnson, David K Duvenaud, Alex Wiltschko,\nRyan P Adams, and Sandeep R Datta.\nComposing\ngraphical models with neural networks for structured\nrepresentations and fast inference. In Advances in neural\ninformation processing systems, pages 2946–2954, 2016.\n[22] Rico Jonschkowski and Oliver Brock.\nLearning state\nrepresentations with robotic priors. Autonomous Robots,\n39(3):407–428, 2015.\n[23] Rico Jonschkowski, Roland Hafner, Jonathan Scholz, and\nMartin Riedmiller. Pves: Position-velocity encoders for\nunsupervised learning of structured state representations.\narXiv preprint arXiv:1705.09805, 2017.\n[24] Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian\nIbarz, Alexander Herzog, Eric Jang, Deirdre Quillen,\nEthan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke,\net al.\nQt-opt: Scalable deep reinforcement learning\nfor vision-based robotic manipulation.\narXiv preprint\narXiv:1806.10293, 2018.\n[25] Diederik Kingma and Jimmy Ba.\nAdam: A method\nfor stochastic optimization. International Conference on\nLearning Representations (ICLR), 2015.\n[26] Diederik P Kingma and Max Welling.\nAuto-encoding\nvariational bayes. In International Conference on Learn-\ning Representations, 2014.\n[27] Sascha Lange, Martin Riedmiller, and Arne Voigtlander.\nAutonomous reinforcement learning on raw visual input\ndata in a real world application.\nIn Neural Networks\n(IJCNN), The 2012 International Joint Conference on,\npages 1–8. IEEE, 2012.\n[28] Ian Lenz, Ross A Knepper, and Ashutosh Saxena.\nDeepmpc: Learning deep latent features for model pre-\ndictive control. In Robotics: Science and Systems, 2015.\n[29] Timoth´ee Lesort, Mathieu Seurin, Xinrui Li, Natalia D´ıaz\nRodr´ıguez, and David Filliat.\nUnsupervised state rep-\nresentation learning with robotic priors: a robustness\nbenchmark. arXiv preprint arXiv:1709.05185, 2017.\n[30] Timoth´ee Lesort, Natalia D´ıaz-Rodr´ıguez, Jean-Franois\nGoudou, and David Filliat. State representation learning\nfor control: An overview. Neural Networks, 2018.\n[31] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter\nAbbeel. End-to-end training of deep visuomotor policies.\nThe Journal of Machine Learning Research, 17(1):1334–\n1373, 2016.\n[32] Sergey Levine, Peter Pastor, Alex Krizhevsky, Julian\nIbarz, and Deirdre Quillen.\nLearning hand-eye coor-\ndination for robotic grasping with deep learning and\nlarge-scale data collection. The International Journal of\nRobotics Research, 37(4-5):421–436, 2018.\n[33] Anusha Nagabandi, Gregory Kahn, Ronald S Fearing,\nand Sergey Levine. Neural network dynamics for model-\nbased deep reinforcement learning with model-free ﬁne-\ntuning.\nIn 2018 IEEE International Conference on\nRobotics and Automation (ICRA), pages 7559–7566.\nIEEE, 2018.\n[34] Ashvin Nair, Vitchyr Pong, Murtaza Dalal, Steven Bahl,\nShikhar ans Lin, and Sergey Levine. Visual reinforce-\nment learning with imagined goals. In Neural Informa-\ntion Processing Systems (NeurIPS), 2018.\n[35] Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo,\nPulkit Agrawal, Dian Chen, Yide Shentu, Evan Shel-\nhamer, Jitendra Malik, Alexei A Efros, and Trevor\nDarrell.\nZero-shot visual imitation.\nIn International\nConference on Learning Representations, 2018.\n[36] Lerrel Pinto and Abhinav Gupta.\nSupersizing self-\nsupervision: Learning to grasp from 50k tries and 700\nrobot hours. In Robotics and Automation (ICRA), 2016\nIEEE International Conference on, pages 3406–3413.\nIEEE, 2016.\n[37] Vitchyr Pong, Shixiang Gu, Murtaza Dalal, and Sergey\nLevine.\nTemporal\ndifference\nmodels:\nModel-free\ndeep rl for model-based control.\narXiv preprint\narXiv:1802.09081, 2018.\n[38] Matthew Rosencrantz, Geoff Gordon, and Sebastian\nThrun.\nLearning low dimensional predictive represen-\ntations. In Proceedings of the twenty-ﬁrst international\nconference on Machine learning, page 88. ACM, 2004.\n[39] Amir Rosenfeld, Richard Zemel, and John K Tsot-\nsos.\nThe elephant in the room.\narXiv preprint\narXiv:1808.03305, 2018.\n[40] Pierre Sermanet, Kelvin Xu, and Sergey Levine. Unsu-\npervised perceptual rewards for imitation learning. arXiv\npreprint arXiv:1612.06699, 2016.\n[41] Pierre Sermanet, Corey Lynch, Jasmine Hsu, and Sergey\nLevine.\nTime-contrastive networks: Self-supervised\nlearning from multi-view observation. arXiv:1704.06888,\n2017.\n[42] Evan Shelhamer, Parsa Mahmoudieh, Max Argus, and\nTrevor\nDarrell.\nLoss\nis\nits\nown\nreward:\nSelf-\nsupervision for reinforcement learning.\narXiv preprint\narXiv:1612.07307, 2016.\n[43] Aravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey\nLevine, and Chelsea Finn. Universal planning networks.\nInternational Conference of Machine Learning (ICML),\n2018.\n[44] Sainbayar Sukhbaatar, Zeming Lin, Ilya Kostrikov,\nGabriel Synnaeve, Arthur Szlam, and Rob Fergus. In-\ntrinsic motivation and automatic curricula via asymmetric\nself-play. arXiv preprint arXiv:1703.05407, 2017.\n[45] Valentin Thomas, Jules Pondard, Emmanuel Bengio,\nMarc Sarfati, Philippe Beaudoin, Marie-Jean Meurs,\nJoelle Pineau, Doina Precup, and Yoshua Bengio.\nIndependently controllable features.\narXiv preprint\narXiv:1708.01289, 2017.\n[46] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco:\nA physics engine for model-based control.\nIn Inter-\nnational Conference on Intelligent Robots and Systems\n(IROS), 2012.\n[47] David Warde-Farley, Tom Van de Wiele, Tejas Kulkarni,\nCatalin Ionescu, Steven Hansen, and Volodymyr Mnih.\nUnsupervised control through non-parametric discrimi-\nnative rewards. arXiv preprint arXiv:1811.11359, 2018.\n[48] Manuel Watter, Jost Springenberg, Joschka Boedecker,\nand Martin Riedmiller. Embed to control: A locally linear\nlatent dynamics model for control from raw images.\nIn Advances in neural information processing systems,\npages 2746–2754, 2015.\n[49] Annie Xie, Avi Singh, Sergey Levine, and Chelsea Finn.\nFew-shot goal inference for visuomotor learning and\nplanning. In Conference on Robot Learning, 2018.\n[50] Brian Ziebart, Andrew Maas, Andrew Bagnell, and\nAnind Dey.\nMaximum entropy inverse reinforcement\nlearning. In AAAI Conference on Artiﬁcial Intelligence\n(AAAI), 2008.\nAPPENDIX\nIn this appendix, we summarize our DPN approach in\nAlgorithm 1. We also provide additional details about our\nimplementation details and experimental setup.\nA. Architecture and Hyperparameters\nFor all approaches except the ℓ2 distance in pixel space,\nwe use a 4-layer convolutional neural networks with 64 5 × 5\nﬁlters each layer and spatial soft-argmax [15] after the last\nconvolution layer to represent the encoder f(·; θenc), which\nencodes images into latent space. For DPN, we represent\nthe latent dynamics g(xt, ˆat; θdyn) as 2-layer fully-connected\nneural networks with 128 hidden units. The inference network\nq(zt|at; φ) is modeled as a 2-layer fully-connected neural\nnetwork with hidden units 16 where the last layer has two\nheads that output the mean µφ(at) and the standard deviation\nσφ(at). The action decoder h(at|z′\nt; θact) is also a 2-layer\nneural network with 16 hidden units. We use β = 0.5 as the\nKL constraint value in LDPN. We use np = 20 as the number\nof gradient descent steps that update z′\nt:t+T and the learned\nsize αi’s are initialized with 0.05. For the inverse model, we\nrepresent the latent multi-step inverse model as a recurrent\nneural network with 128 units. For VAE, while the architecture\nof the encoder is the same across all methods as mentioned\nabove, the decoder consists of 4 deconvolutional layers with\n64 5 × 5 ﬁlters except that the last layer has 3 5 × 5 ﬁlters\nin order to reconstruct the image. All three approaches are\ntrained with Adam optimizer [25] with learning rate 0.0005.\nB. Simulated Reaching\nWe collect 3000 videos of the unlabeled interaction by\nsampling torques uniformly from [−5, 5], where each video\nconsists of 10 images. Along with each video, we also store\na sequence of torques as actions and robot joint angles. For\ntraining all methods, we use a training set that contains 28500\nvideos and a validation set that has 1500 videos.\nThe length of each RL step is 1000 and the maximum path\nlength is 50.\nC. Simulated Rope Manipulation\nFor collecting 20000 10-frame random videos, we randomly\nsample positions of gripper at each time step. We use 19000\nvideos for training and the remaining 1000 for validation.\nWe set the maximum path length to be 5 and the length of\none RL step to be 500.\nD. Simulated Pushing\nAt each time step of the random data collection, we ap-\nply a random end-effector velocity sampled uniformly from\n[−20, 20]. In this way, we collect 3000 videos, where 2850\nvideos are used for training and 150 are used for validation.\nEach RL step consists of 2000 timesteps and the maximum\npath length is 100 timesteps.\nAlgorithm 1 Distributional Planning Networks\nRequire: random dataset {(ot:t+T +1, at:t+T )}\nRequire: KL constraint value β, outer step size γ\nInitialize α0:np−1\nDeﬁne the prior p(zt:t+T ) = N(0, I)\nwhile training do\nSample a batch of random data ot, ot+T +1, at:t+T\nSample latent actions zt:t+T ∼qφ(zt:t+T |at:t+T )\nInitialize z′(0)\nt:T = zt:t+T\nfor i = 0, 1, ..., np −1 do\nEncode xt = f(ot; θenc), xt+T +1 = f(ot+T +1; θenc)\nSet ˆx(i)\nt\n= xt\nfor j = 0, 1, . . . , T do\nˆx(i)\nt+j+1 = g(ˆx(i)\nt+j, z′(i)\nt+j; θdyn)\nend for\nCompute L(i)\nplan = Lσ(ˆx(i)\nt+T +1, xt+T +1)\nUpdate z′(i+1)\nt:t+T = z′(i)\nt:t+T −αi∇z′(i)\nt:t+T Li\nplan\nend for\nCompute ˆat:t+T = h(z′(np−1)\nt:t+T\n; θact)\nCompute\nLDPN\n=\nlog pθ(at:t+T |ot, ot+T +1) +\nβDKL(qφ(zt:t+T |at:t+T ) || p(zt:t+T ))\nCompute ∇θLDPN and ∇φLDPN\nUpdate θ ←θ −γ∇θLDPN\nUpdate φ ←φ −γ∇φLDPN\nend while\nReturn θ, φ\nE. Robot Setup\nThe initial state of the robot is with its arm above the middle\nof a table. The arm is constrained to move in a square region\nthat is approximately 0.16m2 in area. In both tasks, we collect\n100 × 100 RGB images from a front facing camera alongside\njoint information, and use continuous end effector velocities\nas actions, which are normalized to be between [−1, 1]. Side\nlights are used to enable nighttime data collection.\nF. Robot Reaching\nDuring unlabeled interation data collection, each episode\nconsists of 20 timesteps starting from a ﬁxed initial state. At\neach timestep, we uniformly sample an action to apply to the\narm and keep the arm within bounds by applying an inward\nend effector velocity at table edges.\nWe use the learned metrics for reinforcement learning of\ntwo reaching tasks, where the only reward exposed to the\nreinforcement learning agent is derived from the latent metric.\nBecause we have access to the true end effector position, we\nuse ℓ2 distance between the ﬁnal position and goal position\nto evaluate ﬁnal performance. This distance is not provided to\nthe robot.\nIn order to make the ﬁrst task more challenging, we limit\nthe policy to normalized actions between -0.1 and 0.1. This\nmakes it harder for the arm to reach the goal if it initially\nchooses an incorrect direction of motion. This task, in turn, is\ndesigned to see how well each metric works at the start of an\nepisode, where the arm was farther from its goal.\nMeanwhile, in the second task, we leave the policy’s actions\nunscaled, making it more likely that the arm will violate\nbounds and be pushed back inward by the correcting inward\nvelocity once it comes close to an edge. Therefore, this task\nis meant to show how well each metric performs at the later\ntimesteps of a policy, where the arm is closer to its goal.\nThe length of each RL step is 20 and the maximum path\nlength is 20.\nG. Robot Pushing\nEach episode for random data collection consists of 15\ntimesteps. In order to vary the initial positions of the objects,\nwe use a mix of scripted shufﬂing methods and manual\nrearrangement when objects got stuck in a corner, leading to\ndata collection that is nearly entirely autonomous\nFor the reinforcement learning task, the arm had to learn\nto push the either the red or the tan stuffed animal from the\nmiddle of the table to a point at the front of the table depending\non the goal image (see Fig 8).\nThe length of each RL step is 20 and the maximum path\nlength is 20.\n",
  "categories": [
    "cs.RO",
    "cs.CV",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-02-14",
  "updated": "2019-02-14"
}