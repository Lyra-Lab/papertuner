{
  "id": "http://arxiv.org/abs/1902.05731v1",
  "title": "SVM-based Deep Stacking Networks",
  "authors": [
    "Jingyuan Wang",
    "Kai Feng",
    "Junjie Wu"
  ],
  "abstract": "The deep network model, with the majority built on neural networks, has been\nproved to be a powerful framework to represent complex data for high\nperformance machine learning. In recent years, more and more studies turn to\nnonneural network approaches to build diverse deep structures, and the Deep\nStacking Network (DSN) model is one of such approaches that uses stacked\neasy-to-learn blocks to build a parameter-training-parallelizable deep network.\nIn this paper, we propose a novel SVM-based Deep Stacking Network (SVM-DSN),\nwhich uses the DSN architecture to organize linear SVM classifiers for deep\nlearning. A BP-like layer tuning scheme is also proposed to ensure holistic and\nlocal optimizations of stacked SVMs simultaneously. Some good math properties\nof SVM, such as the convex optimization, is introduced into the DSN framework\nby our model. From a global view, SVM-DSN can iteratively extract data\nrepresentations layer by layer as a deep neural network but with\nparallelizability, and from a local view, each stacked SVM can converge to its\noptimal solution and obtain the support vectors, which compared with neural\nnetworks could lead to interesting improvements in anti-saturation and\ninterpretability. Experimental results on both image and text data sets\ndemonstrate the excellent performances of SVM-DSN compared with some\ncompetitive benchmark models.",
  "text": "SVM-based Deep Stacking Networks\nJingyuan Wangâ€ ,Â§, Kai Fengâ€ , Junjie Wuâ€¡,Â§âˆ—\nâ€  MOE Engineering Research Center of Advanced Computer Application Technology,\nSchool of Computer Science Engineering, Beihang University, Beijing 100191, China\nâ€¡ Beijing Key Laboratory of Emergency Support Simulation Technologies for City Operations,\nSchool of Economics and Management, Beihang University, Beijing 100191, China\nÂ§ Beijing Advanced Innovation Center for Big Data and Brain Computing,\nBeihang University, Beijing 100191, China\nEmail: {jywang, fengkai, wujj}@buaa.edu.cn\nAbstract\nThe deep network model, with the majority built on neural\nnetworks, has been proved to be a powerful framework to\nrepresent complex data for high performance machine learn-\ning. In recent years, more and more studies turn to non-\nneural network approaches to build diverse deep structures,\nand the Deep Stacking Network (DSN) model is one of such\napproaches that uses stacked easy-to-learn blocks to build\na parameter-training-parallelizable deep network. In this pa-\nper, we propose a novel SVM-based Deep Stacking Network\n(SVM-DSN), which uses the DSN architecture to organize\nlinear SVM classiï¬ers for deep learning. A BP-like layer tun-\ning scheme is also proposed to ensure holistic and local opti-\nmizations of stacked SVMs simultaneously. Some good math\nproperties of SVM, such as the convex optimization, is intro-\nduced into the DSN framework by our model. From a global\nview, SVM-DSN can iteratively extract data representations\nlayer by layer as a deep neural network but with paralleliz-\nability, and from a local view, each stacked SVM can con-\nverge to its optimal solution and obtain the support vectors,\nwhich compared with neural networks could lead to interest-\ning improvements in anti-saturation and interpretability. Ex-\nperimental results on both image and text data sets demon-\nstrate the excellent performances of SVM-DSN compared\nwith some competitive benchmark models.\nIntroduction\nRecent years have witnessed the tremendous interests from\nboth the academy and industries in building deep neu-\nral networks (Hinton and Salakhutdinov 2006; Bengio,\nCourville, and Vincent 2013). Many types of deep neu-\nral networks have been proposed for classiï¬cation, regres-\nsion and feature extracting tasks, such as Stacked Denois-\ning Autoencoders (SAE) (Vincent et al. 2010), Deep Be-\nlief Networks (DBN) (Hinton 2011), deep Convolutional\nNeural Networks (CNN) (Krizhevsky, Sutskever, and Hin-\nton 2012), Recurrent Neural Networks (RNN) (Medsker and\nJain 2001), and so on.\nMeanwhile, the shortcomings of neural network based\ndeep models, such as the non-convex optimization, hard-\nto-parallelizing, and lacking model interpretation, are get-\nting more and more attentions from the pertinent research\nâˆ—Corresponding author\nCopyright câƒ2019, Association for the Advancement of Artiï¬cial\nIntelligence (www.aaai.org). All rights reserved.\nsocieties. Some potential solutions have been proposed to\nbuild deep structure models using non neural network ap-\nproaches. For instance, in the literature, the PCANet build\na deep model using an unsupervised convolutional prin-\ncipal component analysis (Chan et al. 2015). The gcFor-\nest builds a tree based deep model using stacked random\nforests, which is regarded as a good alternative to deep neu-\nral networks (Zhi-Hua Zhou 2017). Deep Fisher Networks\nbuild deep networks by stacking Fisher vector encoding into\nmultiple layers (Simonyan, Vedaldi, and Zisserman 2013).\nAlong this line, in this paper, we propose a novel SVM-\nbased Deep Stacking Network (SVM-DSN) for deep ma-\nchine learning. On one hand, SVM-DSN belongs to the com-\nmunity of Deep Stacking Networks (DSN), which consist of\nmany stacked multilayer base blocks that could be trained in\na parallel way and have comparable performance with deep\nneural networks (Deng and Yu 2011; Deng, He, and Gao\n2013). In this way, SVM-DSN can gain the deep learning\nability with extra scalability. On the other hand, we replace\nthe traditional base blocks in a DSN, i.e., the perceptrons, by\nthe well known Support Vector Machine (SVM), which has\nlong been regarded as a succinct model with appealing math\nproperties such as the convexity in optimization, and was\nconsidered as a different method to model complicated data\ndistributions compared with deep neural networks (Bengio\nand others 2009). In this way, SVM-DSN can gain the abil-\nity in anti-saturation and enjoys improved interpretability,\nwhich are deemed to be the tough challenges to deep neu-\nral networks. A BP-like Layered Tuning (BLT) algorithm is\nthen proposed for SVM-DSN to conduct holistic and local\noptimizations for all base SVMs simultaneously.\nCompared with the traditional deep stacking networks and\ndeep neural networks, the SVM-DSN model has the follow-\ning advantages:\nâ€¢ The optimization of each base-SVM is convex. Using the\nproposed BLT algorithm, all base-SVMs are optimized as\na whole, and meanwhile each base-SVM can also con-\nverge to its own optimum. The ï¬nal solution of SVM-\nDSN is a group of optimized linear SVMs that are in-\ntegrated as a deep model. This advantage allows SVM-\nDSN to avoid the neuron saturation problem in deep neu-\nral networks, and thus could improve the performance.\nâ€¢ The SVM-DSN model is very easy to parallelize. The\narXiv:1902.05731v1  [cs.LG]  15 Feb 2019\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nğ‘Šğ‘Š1\nğ‘ˆğ‘ˆ1\nğ‘Šğ‘Š2\nğ‘ˆğ‘ˆ2\nğ‘ˆğ‘ˆ3\nğ‘ˆğ‘ˆ4\nğ‘Šğ‘Š3\nğ‘Šğ‘Š4\nFigure 1: An illustration of the DSN architecture (Deng, He,\nand Gao 2013). The color is used to distinguish different\nblocks in a DSN. The components in the same color belong\nto the same block.\ntraining parallelization in SVM-DSN can reach the base-\nSVM level due to the support vectors oriented property of\nSVM, but the traditional DSN can only reach the block\nlevel.\nâ€¢ The SVM-DSN model has improved interpretability. The\nsupport vectors in base-SVMs can provide some insight-\nful information about what a block learned from train-\ning data. This property empowers users to partially un-\nderstand the feature extracting process of the SVM-\nDSN model.\nExperimental results on image and sentiment classiï¬ca-\ntion tasks show that SVM-DSN model obtains respectable\nimprovements over neural networks. Moreover, compared\nwith the stacking models with strong base-learners, the\nSVM-DSN model also demonstrates signiï¬cant advantages\nin performance.\nThe SVM-DSN Model\nFramework of Deep Stacking Network\nThe Deep Stacking Network is a scalable deep machine\nlearning architecture (Deng, He, and Gao 2013; Deng and\nYu 2011) that consists of stacked easy-to-learn blocks in a\nlayer by layer manner. In the standard DSN, a block is a\nsimpliï¬ed multilayer perceptron with a single hidden layer.\nLet the inputs of a block be a vector x, the block uses a\nconnection weight matrix W to calculate the hidden layer\nvector h as\nh = Ï•\n\u0000W âŠ¤x\n\u0001\n,\n(1)\nwhere Ï•(x) = 1/(1 + exp(âˆ’x)) is a sigmoid nonlinear ac-\ntivation function. Using a weight matrix U, the objective\nfunction of the DSN block optimization is deï¬ned as\nmin\n\r\ry âˆ’U âŠ¤h\n\r\r2\nF .\n(2)\nAs shown in Fig. 1, the blocks of a DSN are stacked layer\nby layer. For the block in the input layer, the input vector x\ncontains only the raw input features. For blocks in the mid-\ndle layers, x is a concatenated vector of the raw input fea-\ntures and output representations of all previous layer blocks.\nThe training of deep stacking networks contains two\nsteps: block training and ï¬ne-tuning. In the block training\nstep, the DSN blocks are independently training as super-\nvised multilayer perceptrons. In the ï¬ne-tuning step, all the\nstacked blocks are considered as a multi-layer deep neural\nnetwork. The parameters of DSN are end-to-end trained us-\ning the error Back Propagation (BP) algorithm.\nSVM-based DSN Blocks\nIn the SVM-DSN model, we adopt support vector machines\nto implement a DSN block. A SVM classiï¬er is a hyperplane\nÏ‰âŠ¤x + b = 0 that divides the feature space of a data sample\nx into two parts â€” one for the positive and the other for the\nnegative. The parameters Ï‰ and b are optimized to maximize\nthe minimum distances from the hyperplane to a set of train-\ning samples T = {(xk, yk)|yk âˆˆ{âˆ’1, 1}, k = 1, . . . , K},\ni.e.,\nmax\nÏ‰,b\n2\nâˆ¥Ï‰âˆ¥\ns.t.\nyk(Ï‰âŠ¤xk + b) â‰¥1,\nk = 1, 2, . . . , N.\n(3)\nA training sample is called a support vector if the constraint\nin Eq. (3) turns into equality.\nFor a multi-class problem with N classes, we connect the\ninput vector x of a DSN block with N binary SVM classi-\nï¬ers â€” each for recognizing whether a sample belongs to\na corresponding class â€” to predict the label of a sample. A\nbinary SVM classiï¬er in a DSN block is called a base-SVM.\nThe N binary SVM classiï¬ers for a N classiï¬cation problem\nis called as a base-SVM group. A SVM-DSN block could\ncontains multiple base-SVM groups. In the same block, all\nbase-SVM groups share the same input vector x.\nStacking Blocks\nGiven a classiï¬cation hyperplane of a SVM, the decision\nfunction for the sample xk is expressed as\nf(xk) = sign\n\u0000Ï‰âŠ¤xk + b\n\u0001\n,\n(4)\nwhere f(xk) = 1 for the positive class and f(xk) = âˆ’1\nfor the negative. The distance from a sample to the hyper-\nplane could be considered as the conï¬dence of a classiï¬-\ncation decision. For the samples behind the support vec-\ntors, i.e.,\n\f\fÏ‰âŠ¤xk + b\n\f\f > 1, the conï¬dence is 1, otherwise\nis\n\f\fÏ‰âŠ¤xk + b\n\f\f. We therefore can express the classiï¬cation\nconï¬dence of a SVM classiï¬er for the sample xk as\ng(xk) = min\n\u00001, |Ï‰âŠ¤xk + b|\n\u0001\n.\n(5)\nWe denote the i-th base-SVM in the layer l as svm(l, i)\nand its decision function and conï¬dence as f (l,i)(Â·) and\ng(l,i)(Â·), respectively. For the base-SVM svm(l, i), we de-\nï¬ne a conï¬dence weighted output y(l,i) as\ny(l,i) = f (l,i)(x) Â· g(l,i)(x).\n(6)\nIn the layer l+1, SVM-DSN concatenates the conï¬dence\nweighted outputs of all base-SVMs in the previous layers\nand raw inputs as\nx(l+1) =\n\u0010\ny(l,1), . . . , y(l,i), . . . , y(lâˆ’1,1), . . . , y(lâˆ’1,i),\n. . . , y(1,1), . . . , y(1,i), . . . , x(1,1), . . . , x(1,i)\u0011âŠ¤\n.\n(7)\nThe base-SVMs in the layer l+1 use x(l+1) as the input to\ngenerate their conï¬dence weighted outputs y(l+1,i). In this\nway, base-SVMs are stacked and connected layer by layer.\nModel Training\nBlock Training\nSimilar to the standard deep stacking network, the training\nof the SVM-DSN model also contains a block training step\nand a ï¬ne-tuning step.\nIn the block training step, the base-SVMs in a DSN block\nare trained as regular SVM classiï¬ers. Given a set of training\nsamples T = {(xk, yk)|k = 1, . . . , K}, where yk is the\nground-truth label of xk, the objective function of a base-\nSVM group with N classiï¬cation is deï¬ned as\nJ = 1\n2 âˆ¥â„¦âˆ¥2\nF\n+ C\nK\nX\nk=1\nN\nX\ni=1\nâ„“hinge\n\u0010\ny(i)\nk (Ï‰(i)âŠ¤xk + b(i))\n\u0011\n,\n(8)\nwhere â„¦=\n\u0000Ï‰(1)âŠ¤, . . . , Ï‰(N)âŠ¤\u0001\n, and y(i)\nk\n= 1 if yk = i and\n-1 otherwise. The function â„“hinge(Â·) is a hinge loss function\ndeï¬ned as â„“hinge(z) = max (0, 1 âˆ’z). The parameter Î¸ =\n\b\n(Ï‰(i), b(i))|âˆ€i\n\t\nis inferred as Î¸ = arg min\nÎ¸\nJ (Î¸).\nIn order to increase the diversity of base-SVM groups in\na block, we adopt a bootstrap aggregating method in the\nblock training. For a block with M base-SVM groups, we\nre-sample the training data as M sets using the bootstrap\nmethod (Efron and Tibshirani 1994). Each base-SVM group\nis trained using one re-sampled data set.\nFine Tuning\nThe traditional DSN model is based on neural networks and\nuses the BP algorithm in the ï¬ne-tuning step. For the SVM-\nDSN model, we introduce SVM training into the BP algo-\nrithm framework, and propose a BP-like Layered Tuning\n(BLT) algorithm to ï¬ne-tune the model parameters.\nAlgorithm 1 gives the pseudocodes of BLT. In general,\nBLT iteratively optimizes the base-SVMs from the output\nlayer to the input layer. In each iteration, BLT optimizes\nsvm(l, i) by ï¬rstly generating a set of virtual training sam-\nples T (l,i) = {(x(l)\nk , Ëœy(l,i)\nk\n)|k = 1, . . . , K}, and then trains\na new svm(l, i) on T (l,i).\nAccording to Eq. (6) and Eq. (7), it is easy to have x(l)\nk =\n(y(lâˆ’1,1)\nk\n, y(lâˆ’1,2)\nk\n, Â· Â· Â· , y(lâˆ’1,i)\nk\n, Â· Â· Â· )âŠ¤. However, the calcu-\nlation of the virtual label Ëœy(l,i)\nk\nis not that straightforward.\nAlgorithm 1 BP-like Layered Tuning Algorithm\n1: Initialization: Initializing Ï‰(l,i), b(l,i) for all svm(l, i) as ran-\ndom values.\n2: repeat\n3:\nSelect a batch of training samples T = {(xk, yk)|k =\n1, . . . , K}.\n4:\nfor l = L, L âˆ’1, . . . , 2, 1 do\n5:\nfor i = 1, 2, . . . do\n6:\nUse Eq. (6), Eq. (7), and Eq. (9) to calculate T (l,i) =\n{(x(l)\nk , Ëœy(l,i)\nk\n)|k = 1, . . . , K}.\n7:\nUse T (l,i) to train svm(l, i) as Eq. (11).\n8:\nend for\n9:\nend for\n10: until The algorithm converges.\nSpeciï¬cally, BLT adopts a gradient descent method to cal-\nculate Ëœy(l,i)\nk\nas\nËœy(l,i)\nk\n= Ïƒ\n \ny(l,i)\nk\nâˆ’Î· âˆ‚J (o)\nâˆ‚y(l,i)\n\f\f\f\f\ny(l,i)=y(l,i)\nk\n!\n,\n(9)\nwhere J (o) is the objective function of the output layer, y(l,i)\nk\nis the output of x(l)\nk in the previous iteration, Î· is the learning\nrate, and Ïƒ(Â·) is a shaping function deï¬ned as\nÏƒ(z) =\nï£±\nï£²\nï£³\n1,\nz > 1\nz,\n|z| â‰¤1\nâˆ’1,\n< âˆ’1\n.\n(10)\nNote that since the term âˆ’Î·âˆ‚J (o)/âˆ‚y(l,i) in Eq. (9) is a neg-\native gradient direction of J (o), tuning the output y(l,i) to\nthe virtual label Ëœy(l,i) can reduce the value of the objective\nfunction J (o) in the output layer. Therefore, it could be ex-\npected that BLT can lower the overall model prediction error\niteratively by training base-SVMs on virtual training sets in\neach iteration.\nGiven the training set T (l,i)\n=\n{(x(l)\nk , Ëœy(l,i)\nk\n)|k\n=\n1, . . . K}, the objective function of training svm(l, i) is de-\nï¬ned as\nmin J (l,i) = 1\n2\n\r\r\rÏ‰(l,i)\r\r\r\n2\n+ C1\nX\nkâˆˆÎ˜\nâ„“hinge\n\u0010\nËœy(l,i)\nk\n(Ï‰(l,i)âŠ¤x(l)\nk + b(l,i))\n\u0011\n|\n{z\n}\nThe SVM Loss\n+ C2\nX\nk /âˆˆÎ˜\nâ„“Ïµ\n\u0010\nÏ‰(l,i)âŠ¤x(l)\nk + b(l,i) âˆ’Ëœy(l,i)\nk\n\u0011\n|\n{z\n}\nThe SVR Loss\n,\n(11)\nwhere Î˜ is the index set of the virtual labels\n\f\f\fËœy(l,i)\nk\n\f\f\f = 1,\nand the function â„“Ïµ(Â·) is an Ïµ-insensitive loss function in the\nform of â„“Ïµ(z) = max(|z| âˆ’Ïµ, 0).\nNote that the objective function in Eq. (11) contains two\ntypes of loss functions so as to adapt to the different condi-\ntions of Ëœy(l,i)\nk\n. When Ëœy(l,i)\nk\nâˆˆ{âˆ’1, 1}, i.e., the virtual labels\nare binary, BLT trains svm(l, i) as the standard SVM classi-\nï¬er and thus uses the hinge loss function to measure errors.\nWhen Ëœy(l,i)\nk\nâˆˆ(âˆ’1, 1), the objective function adopts a Sup-\nport Vector Regression loss term â„“Ïµ for this condition. In the\nAppendix, we prove that the problem deï¬ned in Eq. (11)\nis a quadratic convex optimization problem. The training of\nsvm(l, i) can thus reach an optimal solution by using vari-\nous quadratic programming methods such as sequential min-\nimal optimization and gradient descents.\nWe ï¬nally turn to the small problem unsolved â€” how\nto calculate the partial derivative âˆ‚J (o)/âˆ‚y(l,i) in Eq. (9).\nBased on the chain rule, the partial derivative can be recur-\nsively calculated as\nâˆ‚J\nâˆ‚y(l,i) =\nL\nX\nm=l+1\nX\nj\nâˆ‚J\nâˆ‚y(m,j)\ndy(m,j)\ndz(m,j)\nâˆ‚z(m,j)\nâˆ‚y(l,i)\n=\nL\nX\nm=l+1\nX\nj\nâˆ‚J\nâˆ‚y(m,j) yâ€² \u0010\nz(m,j)\u0011\nÏ‰(m,j)\ni\n,\n(12)\nwhere Ï‰(m,j)\ni\nis the connection weight of y(m,j) in\nsvm(m, j), and z(m,j) = Ï‰(m,j)âŠ¤x(mâˆ’1) + b(m,j). The\nterm yâ€²(z) is the derivative of the function in Eq. (6), which\nis in the form of\nyâ€²(z) =\n\u001a 0,\n|z| > 1\n1,\n|z| â‰¤1.\n(13)\nThe principle of this chain derivation is similar to the error\nback-propagation of the neural network training. The differ-\nence lies in that the BP algorithm calculates the derivative\nfor each neuron connecting weight but BLT for each base-\nSVM output. That is why we name our algorithm as BP-like\nLayered Tuning.\nModel Properties\nConnection to Neural Networks\nThe SVM-DSN model has close relations with neural\nnetworks. If we view the base-SVM output function deï¬ned\nin Eq. (6) as a neuron, the SVM-DSN model can be regarded\nas a type of neural networks. Speciï¬cally, we can rewrite the\nfunction in Eq. (6) as a neuron form as follows:\ny(l,i) = Ïƒ\n\u0010\nÏ‰\n(l,i)âŠ¤x(l) + b(l,i)\u0011\n,\n(14)\nwhere the shaping function Ïƒ(Â·) works as an activate func-\ntion, with the output Ïƒ(z) âˆˆ{1, âˆ’1} if |z| â‰¥1, and\nÏƒ(z) = z if |z| < 1. As proved in (Hornik 1991), a multi-\nlayer feedforward neural network with arbitrary bounded\nand non-constant activation function has an universal ap-\nproximation capability. As a consequence, we could expect\nthat the proposed SVM-DSN model also has the universal\napproximation capability in theory.\nNevertheless,\nthe\ndifference\nbetween\nthe\nSVM-\nDSN model and neural networks is still signiï¬cant. Indeed,\nwe have proven in the Appendix that the base-SVMs in our\nSVM-DSN model have the following property: Given a set\nof virtual training samples {(x(l)\nk , Ëœy(l,i)\nk\n)|k = 1, . . . , K} for\nsvm(l, i), to minimize the loss function deï¬ned in Eq. (11)\nis a convex optimization problem. Moreover, because the\nbase-SVMs in the same block are mutually independent, the\noptimization of the whole block is a convex problem too.\nThis implies that, in each iteration of the BLT algorithm, all\nblocks can converge to an optimal solution. In other words,\nSVM-DSN ensures that all blocks and their base-SVMs â€œdo\ntheir own bestâ€ to minimize their own objective functions\nin each iteration, which however is not the case for neural\nnetworks and MLP based deep stacking networks. It is\nalso worth noting that this â€œdo their own bestâ€ property is\ncompatible with the decrease of the overall prediction error\nmeasured by the global objective function J (o).\nAn important advantage empowered by the â€œdo their own\nbestâ€ property is the anti-saturation feature of SVM-DSN.\nIn neural network models, the BP algorithm updates the pa-\nrameter Ï‰ of a neuron as Ï‰ â†Î·âˆ‚J /âˆ‚Ï‰. Hence, the partial\nderivative for the i-th Ï‰ in the j-th neuron at the layer l is\ncalculated as\nâˆ‚J\nâˆ‚Ï‰(l,j)\ni\n=\nâˆ‚J\nâˆ‚y(l,j)\ndy(l,j)\ndz(l,j)\nâˆ‚z(l,j)\nâˆ‚Ï‰(l,j)\ni\n=\nâˆ‚J\nâˆ‚y(l,j) Â· yâ€² \u0010\nz(l,j)\u0011\nÂ· y(lâˆ’1,i),\n(15)\nwhere yâ€² \u0000z(l,j)\u0001\nis a derivative of the activation function.\nFor the sigmoid activation function, if |z| is very large then\nyâ€² becomes very small, and âˆ‚J/âˆ‚Ï‰ â†’0. In this condition,\nthe BP algorithm cannot update Ï‰ any more even if there is\nstill much room for the optimization of Ï‰. This phenomenon\nis called the â€œneuron saturationâ€ in neural network training.\nFor the ReLU activation function, similar condition appears\nwhen z < 0, where yâ€² = 0 and âˆ‚J/âˆ‚Ï‰ = 0. The neuron\nwill die when a ReLU neuron fall into this condition.\nIn the BLT algorithm of SVM-DSN model, the update of\na base-SVM is guided by âˆ‚J /âˆ‚y, with the details given in\nEq. (12). From Eq. (12), we can see that unless all base-\nSVMs in an upper layer are saturated, i.e., yâ€²(z(m,j)) = 0 for\nall z(m,j), the base-SVMs in the layer l would not fall into\nthe saturation state. Therefore, we could expect that the sat-\nuration risk of a base-SVM in SVM-DSN tends to be much\nlower than a neuron in neural networks.\nInterpretation\nIn SVM classiï¬ers, the support vector samples could pro-\nvide some interpretation information about what a classiï¬er\nlearned from data set. The SVM-DSN inherit this interpre-\ntation property of SVM. In the base-SVM output function\ndeï¬ned in Eq. (6), the conï¬dence function g(x) indicates\nwhether a sample is clearly classiï¬ed by the hyperplane of\na base-SVM. For a sample to be classiï¬ed, we can calcu-\nlate the average conï¬dence of the sample in all base-SVM\ngroups in a block. The average conï¬dence indicates whether\nthe feature extracted by the block and previous layers of-\nfer enough representations to identify label of the sample.\nBecause the samples with low conï¬dence are near to the hy-\nperplane, we can use the low conï¬dence samples to form\na â€œclassifying planeâ€ map in each blocks. Comparing the\nâ€œclassifying planeâ€ maps layer by layer, we could partly un-\nderstand the feature extracting process of the stacked block\nin a SVM-DSN model.\n2\n1\n0\n1\n2\n2.0\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n0.62\n0.64\n0.66\n0.68\n0.70\n0.72\n0.74\n(a) Layer-1\n2\n1\n0\n1\n2\n2.0\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n0.575\n0.600\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775\n(b) Layer-2\n2\n1\n0\n1\n2\n2.0\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(c) Layer-3\nFigure 2: The classifying plane maps of each layer in the SVM-DSN.\n1.0\n0.5\n0.0\n0.5\n1.0\n1.0\n0.5\n0.0\n0.5\n1.0\n1\n-1\nFigure 3: The Circle Data\nWe here give a show case to explain the interpretation\nproperty of SVM-DSN in a direct way. In this case, we gen-\nerate a circle data set containing samples of two classes, as\nshown in Fig. 3. The positive samples are generated from\na circle with radius of 0.5 plus a Gaussian noise with vari-\nance of 0.1. The negative samples are generated from a circle\nwith radius of 1 plus the same Gaussian noise. We use the\ncircle data set to train a 3-layer SVM-DSN model, where the\nmiddle layers contain 40 and 60 base-SVM groups, respec-\ntively. Because this experiment is a binary classiï¬cation, a\nbase-SVM group only contains one base-SVM.\nIn the experiment, we traverse the feature space from the\ncoordinate point (âˆ’2, âˆ’2) to (2, 2). For each coordinate\npoint, we calculate the average conï¬dence of all base-SVMs\nin each layer. Figs. 2(a) - 2(c) plot the conï¬dence distribu-\ntion maps in different layers. The samples with low conï¬-\ndence are thus near to the SVM classiï¬cation hyperplane, so\nthe low conï¬dence areas in red form the â€œclassifying planeâ€\nof a layer.\nIt is obvious that there is a clear â€œclassifying planeâ€ gen-\neration process from Figs. 2(a) to 2(c). In the layer 1, the low\nconï¬dence values concentrate in the center of the map. In the\nlayer 2, the low conï¬dence values have a vague shape as a\ncircle. In the layer 3, the low conï¬dence values distribute as\na circle shape that clearly divides the feature space into two\nparts. This process demonstrates how an SVM-DSN model\nextracts the data representations of the feature space layer\nby layer.\nParallelization\nThe deep stacking network is proposed for parallel param-\neter learning. For a DSN with L blocks, given a group of\ntraining samples, the DSN can resample the data set as L\nbatches. A DSN block only uses one batch to update its pa-\nrameter. In this way, the training of DSN parameters could\nbe deployed over L processing units. The parallelization of\nDSN training is in the block level.\nThis parallel parameter learning property could be further\nextended by using Parallel Support Vector Machines (Graf\net al. 2005). Because in a SVM classiï¬er, only the support\nvector samples are crucial, we could divide a training set as\nseveral M sub-sets, and use M virtual SVM classiï¬ers to se-\nlect support vector candidates from each sub-set. Finally, we\nuse the support vector candidates of all sub-sets to train the\nï¬nal model. In this way, the training of a SVM-DSN block\ncan be deployed over M processing units, and the all SVM-\nDSN model can be deployed over LÃ—M processors. That is\nto say the training of base-SVMs in a block is also paralleliz-\nable in SVM-DSN. The parallelization of SVM-DSN train-\ning is in the base-SVM level. The parallel degree of whole\nmodel is greatly improved. As reported in Ref (Graf et al.\n2005), the speed-up for a 5-layer Cascade SVM (16 parallel\nSVMs) to a single SVM is about 10 times for each pass and\n5 times for fully converged.\nExperiments\nImage Classiï¬cation Performance\nWe ï¬rst test the performance of SVM-DSN on the MNIST\nimage classiï¬cation database (LeCun et al. 1998). The\nMNIST database contains 60,000 handwritten digits im-\nages in the size of 28 Ã— 28 for training and validation, and\n10,000 images for testing. The images are classiï¬ed as ten\nclasses according to the digits written on them. The SVM-\nDSN model used in the experiment consists of three layers â€“\ntwo middle layers and an output layer. Both of the block in\nthe two middle layers contains 20 base-SVM groups, and\neach group contains 10 base-SVMs, i.e., 200 base-SVMs\none layer. We actually had tried neural networks with deeper\nlayers, but the classiï¬cation performance could not be im-\nproved signiï¬cantly. The benchmark models includes: i) A\n3-layer deep stacking network where the block in each mid-\ndle layer contains 200 hidden neurons; ii) As analyzed in the\nModel Properties section, the SVM-DSN could be consid-\nered as a type of neural network. Therefore, we use the BP\nalgorithm to train a same structure SVM-DSN as a bench-\nmark; iii) The 3-layer neural network models with the same\nneuron connection structure as the 3-layer SVM-DSN. In\nTable 1: MNIST Classiï¬cation Performance\nModels\nError Rate (%)\n3-layer SVM-DSN\n1.49\n3-layer DSN\n1.65\n3-layer SVM-DSN, BP\n1.62\n3-layer NN, SVM output, sigmoid\n1.74\n3-layer NN, SVM output, tanh\n1.59\n3-layer NN, SVM output, ReLU\n1.56\nHomepage benchmark (LeCun et al. 1998)\n1.53\nBagging of base-SVMs\n5.41\nthese benchmarks, we use SVM output (Tang 2013), and\nthe different activate functions in neurons; iv) The best 3-\nlayer NN benchmark listed in the homepage of MNIST â€“ 3-\nlayer NN, 500+300 hidden units, cross entropy loss, weight\ndecay (LeCun et al. 1998); v) A bagging of 41 base-SVM\ngroups, each group contains 10 base-SVMs.\nIn the SVM-DSN model ï¬ne-tuning, we have two hyper-\nparameters to set, i.e., C1 and C2 of the base-SVMâ€™s objec-\ntive function. The two hyper-parameters are used to balance\nstructural risks and empirical risks in a base-SVM. Either\ntoo big or too small for the two hyper-parameters may lead\nmodel performance degenerate. Therefore, we use the trial\nand error method to set the hyper-parameters. The learn-\ning rate Î· is the other hyper-parameter, which could be dy-\nnamic setting using elegant algorithms such as Adam. In our\nexperiment, we directly set the learning rate as a ï¬x value\nÎ· = 0.0005 to ensure the experiment fairness.\nTable 1 gives the MNIST image classiï¬cation results.\nThe SVM-DSN model achieved the best performance com-\npared with the other benchmarks, which veriï¬ed the ef-\nfectiveness of SVM-DSN. In the benchmarks, the 3-layer\nSVM-DSN+BP model has the same model structure with\nSVM-DSN but was trained by the BP algorithm. The results\nshow that SVM-DSN has a better performance than the\nSVM-DSN+BP benchmark, which indicates that the base-\nSVMs â€œdo their own bestâ€ feature of SVM-DSN is a posi-\ntive feature for model performance. In fact, the idea of BLT\nï¬ne-tuning could be extend to optimize the deep stacking\nnetworks with any derivable model as blocks, such as soft\ndecision-making tree and linear discriminant analysis.\nFeature Extractor Compatibility\nCurrently, the mainstream image classiï¬ers usually adopt\nconvolutional neural networks (CNN) as feature extractors.\nTable 2 demonstrates the MNIST classiï¬cation performance\nof SVM-DSN with a CNN feature extractor. In the exper-\niment, we connect a CNN feature extractor with a 3-layer\nSVM-DSN model. The structure of SVM-DSN is same as\nin Table 1. The CNN feature extractor contains 3 convo-\nlutional layers, and each layer consists of 24 ï¬‚itters with\nthe 5Ã—5 receptive ï¬eld. In the CNN and SVM-DSN mix-\nture model, we ï¬rst pre-trained the CNN part using BP, and\nthen uses the feature extracted by CNN as input of the SVM-\nDSN part to train the blocks. In the ï¬ne-tuning step, the\nCNN part is ï¬ne tuned by BP and the SVM-DSN part is\ntuned by BLT. The benchmark models include: i) The same\nTable 2: MNIST Classiï¬cation with CNN Feature Extractor\nModels\nError Rate (%)\nCNN + SVM-DSN\n0.51\nCNN + DSN\n0.60\nCNN + SVM-DSN, BP\n0.72\nCNN + sigmoid activation\n0.80\nCNN + tanh activation\n0.67\nCNN + ReLU activation\n0.58\nHomepage benchmark (LeCun et al. 1998)\n0.54\ngcForest (Zhi-Hua Zhou 2017)\n0.74\nTable 3: IMDB Classiï¬cation Performance\nModels\nError Rate (%)\nSVM-DSN\n10.51\nDSN\n11.15\nSVM-DSN, BP\n11.42\nRandom Forest\n14.68\nXGBoost\n14.77\nAdaBoost\n16.63\nSVM (linear kernel)\n12.43\nStacking\n11.55\nBagging of base-SVMs\n11.66\ngcForest (Zhi-Hua Zhou 2017)\n10.84\nCNN feature extractor connected with a 3-layer DSN, where\nthe 3-layer DSN has the same structure with the 3-layer\nSVM-DSN model; ii) The CNN+SVM-DSN model trained\nby the BP algorithm; iii) The neural networks consist of 3\nCNN layers and 3-layer neural network with different acti-\nvate functions, where the structures of the CNN and the neu-\nral network are same as the CNN + SVM-DSN model; iv)\nThe trainable CNN feature extractor + SVMs with afï¬ne dis-\ntortions, which is the best benchmark with the similar mod-\nels scale listed in the MNIST homepage; v) The gcForest\nwith convolutional kernels (Zhi-Hua Zhou 2017). As shown\nin Table 2, the CNN + SVM-DSN model achieved the best\nperformance, which veriï¬ed the effectiveness of our model\nagain. Whatâ€™s more, this experiment demonstrates that the\nSVM-DSN model is completely compatible to the neural\nnetwork framework. The other types of networks, such as\nRNN and LSTM, could also be used as feature extractors of\nSVM-DSN to adapt diversiï¬ed application scenarios.\nComparison with Ensemble Models\nIn this section, we compare the performance of SVM-\nDSN with several classical ensemble models. Because on\nthe MNIST data set, the performance of ensemble mod-\nels are usually not very well. For a fair comparison, we\nuse the IMDB sentiment classiï¬cation data set (Maas et\nal. 2011) in our experiment. Many tree based ensemble\nmethods achieved good performance on this data set (Zhi-\nHua Zhou 2017). The IMDB dataset contains 25,000 movie\nreviews for training and 25,000 for testing. The movie re-\nviews are represented by tf-idf features and labeled as pos-\nitives and negatives. The SVM-DSN model used in this ex-\nperiment consists of 4 middle layers, and the number of\nbase-SVMs in the middle layer are 1024-1024-512-256. The\nbenchmark models include Random Forest, XGBoost, Ad-\naBoost and SVM (linear kernel). The four benchmarks are\nalso stacked as a stacking benchmark (Perlich and Â´Swirszcz\n2011). A bagging of base-SVMs and the grForest (Zhi-\nHua Zhou 2017) are also included as competitor. A 4-layer\nDSN is used as a benchmark, where the number of hidden\nneurons in the middle layer blocks are same as the number of\nbase-SVMs in the SVM-DSN model. The SVM-DSN model\ntrained by the BP algorithm is also used as the benchmark.\nAs shown in Table 3, the SVM-DSN model achieved\nthe best performance again. Especially, the performance of\nSVM-DSN is better than the stacking benchmark, which in-\ndicates that holistic optimized multi-layer stacking of linear\nbase-learners can defeat the traditional two-layer stacking of\nstrong base-learners. In the experiment, the performance of\nthe BLT algorithm is yet better than the BP algorithm. The\nâ€œdo their bestâ€ feature of base-SVM in BLT is still effective\nin text sentiment classiï¬cation.\nRelated Works\nThis work has close relations with SVM, deep learning,\nand stacking. The support vector machine was ï¬rst pro-\nposed by Vapnik in (Vapnik 1998). Multi-layer structures\nin SVM were usually used as speedup solutions. In cas-\ncade SVM (Graf et al. 2005), a multi-layer cascade SVM\nmodel structure was used to select support vectors in a par-\nallel way. In the literature (Collobert, Bengio, and Bengio\n2002), a parallel mixture stacking structure was proposed to\nspeed up SVM training in the very large scale problems. Be-\nfore our work, some studies proposed to use SVM to replace\nthe output layer of a neural network (Wiering et al. 2013;\nTang 2013).\nIn recent years, neural network based deep models has\nachieved great success in various applications (Hinton and\nSalakhutdinov 2006). The gcFroest model (Zhi-Hua Zhou\n2017) was proposed to use the forest based deep model as\nan alternative to deep neural networks. The PCANet builds\na deep model using unsupervised convolutional principal\ncomponent analysis (Chan et al. 2015). LDANet is a super-\nvised extension of PCANet, which uses linear discriminant\nanalysis (LDA) to replace the PCA parts of PCANet (Chan\net al. 2015). Deep Fisher Networks build deep network\nthrough stacking Fisher vector encoding as multi-layers (Si-\nmonyan, Vedaldi, and Zisserman 2013).\nThe DSN framework adopted in this work is a scal-\nable deep architecture amenable to parallel parameter train-\ning, which has been adopted in various applications, such\nas information retrieval (Deng, He, and Gao 2013), image\nclassiï¬cation (Li, Chang, and Yang 2015), and speech pat-\ntern classiï¬cation (Deng and Yu 2011). T-DSN uses tensor\nblocks to incorporate higher order statistics of the hidden bi-\nnary features (Hutchinson, Deng, and Yu 2013). The CCNN\nmodel extends the DSN framework using convolutional neu-\nral networks (Zhang, Liang, and Wainwright 2016). To the\nbest of our knowledge, there are very few works introduce\nthe advantages of SVM into the DSN framework.\nStacking was introduced by Wolpert in (Wolpert 1992)\nas a scheme of combining multiple generalizers. In many\nreal-world applications, the stacking methods were used\nto integrate strong base-learners as an ensemble model\nto improve performance (Jahrer, TÂ¨oscher, and Legenstein\n2010). In the literature, most of stacking works focused\non designing elegant meta-learners and create better base-\nlearners, such as using class probabilities in stacking (Ting\nand Witten 1999), using a weighted average to combine\nstacked regression (Rooney and Patterson 2007), training\nbase-learners using cross-validations (Perlich and Â´Swirszcz\n2011), and applying ant colony optimization to conï¬gure\nbase-learners (Chen, Wong, and Li 2014). To the best of our\nknowledge, there are very few works to study how to opti-\nmize multi-layer stacked base-learners as a whole.\nConclusion\nIn this paper, we proposed an SVM-DSN model where linear\nbase-SVMs are stacked and trained in a deep stacking net-\nwork way. In the SVM-DSN model, the good mathematical\nproperty of SVMs and the ï¬‚exible model structure of deep\nstacking networks are nicely combined in a same frame-\nwork. The SVM-DSN model has many advantage proper-\nties including holistic and local optimization, parallelization\nand interpretation. The experimental results demonstrated\nthe superiority of the SVM-DSN model to some benchmark\nmethods.\nAcknowledgments\nProf. J. Wangâ€™ s work was partially supported by the Na-\ntional Key Research and Development Program of China\n(No.2016YFC1000307), the National Natural Science Foun-\ndation of China (NSFC) (61572059, 61202426), the Science\nand Technology Project of Beijing (Z181100003518001),\nand the CETC Union Fund (6141B08080401). Prof. J.\nWu was partially supported by the National Natural Sci-\nence Foundation of China (NSFC) (71531001, 71725002,\nU1636210, 71471009, 71490723).\nAppendix\nProperty: Given a set of virtual samples T (l,i)\n=\n{(x(l)\nk , Ëœy(l,i)\nk\n)|k = 1, . . . , K} for svm(l, i), to minimize the\nloss function deï¬ned in Eq. (11) is a convex optimization\nproblem.\nProof. For the sake of simplicity, we omit the superscripts\n(l) of x(l)\nk\nand Ëœy(l)\nk\nin our proof. We deï¬ne a constrained\noptimization problem in the form of\nmin\nÏ‰,b,Î¾k,Ë†Î¾k,Î¶k\n1\n2 âˆ¥Ï‰âˆ¥2 + C1\nX\nk/âˆˆÎ˜\nÎ¶k + C2\nX\nkâˆˆÎ˜\n\u0010\nÎ¾k + Ë†Î¾k\n\u0011\ns.t.\n1 âˆ’Ëœyk\n\u0000Ï‰âŠ¤xk + b\n\u0001\nâ‰¤Î¶k,\n(1)\nÎ¶k â‰¥0, k âˆˆÎ˜;\n\u0000Ï‰âŠ¤xk + b\n\u0001\nâˆ’Ëœyk â‰¤Ïµ + Î¾k,\n(2)\nËœyk âˆ’\n\u0000Ï‰âŠ¤xk + b\n\u0001\nâ‰¤Ïµ + Ë†Î¾k,\n(3)\nÎ¾k â‰¥0, Ë†Î¾k â‰¥0, k /âˆˆÎ˜.\n(16)\nWe can see the constrained optimization problem Eq. (16) is\nin a quadratic programming form as\nmin\na\n1\n2aâŠ¤Ua + câŠ¤a\ns.t.\nQa â‰¤p,\n(17)\nwhere a = (Ï‰, b, Î¾, Ë†Î¾, Î¶), and U is a positive semi-deï¬nite\ndiagonal matrix. Therefore, the constrained optimization\nproblem is a quadratic convex optimization problem (Boyd\nand Vandenberghe 2004). It is easy to prove that the con-\nstrained optimization problem deï¬ned in Eq. (16) is equiva-\nlent to the unconstrained optimization problem deï¬ned in\nEq. (11) (Zhang 2003). Therefore, the optimization prob-\nlem of base-SVM is equivalent to the problem deï¬ned in\nEq. (16). The optimization problem of base-SVM is a con-\nvex optimization problem.\nReferences\nBengio, Y., et al. 2009. Learning deep architectures for AI.\nFoundations and trends in Machine Learning 2(1):1â€“127.\nBengio, Y.; Courville, A.; and Vincent, P.\n2013.\nRepre-\nsentation learning: A review and new perspectives. IEEE\nTransactions on Pattern Analysis and Machine Intelligence\n(TPAMI) 35(8):1798â€“1828.\nBoyd, S., and Vandenberghe, L. 2004. Convex optimization.\nCambridge University.\nChan, T.-H.; Jia, K.; Gao, S.; Lu, J.; Zeng, Z.; and Ma, Y.\n2015.\nPCANet: A simple deep learning baseline for im-\nage classiï¬cation? IEEE Transactions on Image Processing\n24(12):5017â€“5032.\nChen, Y.; Wong, M.-L.; and Li, H.\n2014.\nApplying ant\ncolony optimization to conï¬guring stacking ensembles for\ndata mining. Expert Systems with Applications 41(6):2688â€“\n2702.\nCollobert, R.; Bengio, S.; and Bengio, Y. 2002. A parallel\nmixture of SVMs for very large scale problems. In NIPS,\n633â€“640.\nDeng, L., and Yu, D. 2011. Deep convex net: A scalable\narchitecture for speech pattern classiï¬cation.\nIn INTER-\nSPEECH, 2285â€“2288.\nDeng, L.; He, X.; and Gao, J. 2013. Deep stacking networks\nfor information retrieval. In ICASSP, 3153â€“3157. IEEE.\nEfron, B., and Tibshirani, R. J. 1994. An introduction to the\nbootstrap. CRC press.\nGraf, H. P.; Cosatto, E.; Bottou, L.; Dourdanovic, I.; and\nVapnik, V.\n2005.\nParallel support vector machines: The\ncascade SVM. In NIPS, 521â€“528.\nHinton, G. E., and Salakhutdinov, R. R. 2006. Reducing\nthe dimensionality of data with neural networks. Science\n313(5786):504â€“507.\nHinton, G.\n2011.\nDeep belief nets.\nIn Encyclopedia of\nMachine Learning. Springer. 267â€“269.\nHornik, K. 1991. Approximation capabilities of multilayer\nfeedforward networks. Neural networks 4(2):251â€“257.\nHutchinson, B.; Deng, L.; and Yu, D. 2013. Tensor deep\nstacking networks. IEEE Transactions on Pattern Analysis\nand Machine Intelligence (TPAMI) 35(8):1944â€“1957.\nJahrer, M.; TÂ¨oscher, A.; and Legenstein, R. 2010. Com-\nbining predictions for accurate recommender systems. In\nSIGKDD, 693â€“702. ACM.\nKrizhevsky, A.; Sutskever, I.; and Hinton, G. E.\n2012.\nImagenet classiï¬cation with deep convolutional neural\nnetworks. In NIPS, 1097â€“1105.\nLeCun, Y.; Bottou, L.; Bengio, Y.; and Haffner, P. 1998.\nGradient-based learning applied to document recognition.\nProceedings of the IEEE 86(11):2278â€“2324.\nLi, J.; Chang, H.; and Yang, J. 2015. Sparse deep stacking\nnetwork for image classiï¬cation. In AAAI, 3804â€“3810.\nMaas, A. L.; Daly, R. E.; Pham, P. T.; Huang, D.; Ng, A. Y.;\nand Potts, C. 2011. Learning word vectors for sentiment\nanalysis. In ACL, 142â€“150.\nMedsker, L., and Jain, L. 2001. Recurrent neural networks.\nDesign and Applications.\nPerlich, C., and Â´Swirszcz, G. 2011. On cross-validation and\nstacking: Building seemingly predictive models on random\ndata. ACM SIGKDD Explorations Newsletter 12(2):11â€“15.\nRooney, N., and Patterson, D. 2007. A weighted combina-\ntion of stacking and dynamic integration. Pattern Recogni-\ntion 40(4):1385â€“1388.\nSimonyan, K.; Vedaldi, A.; and Zisserman, A. 2013. Deep\nï¬sher networks for large-scale image classiï¬cation. In NIPS,\n163â€“171.\nTang, Y. 2013. Deep learning using linear support vector\nmachines. arXiv preprint arXiv:1306.0239.\nTing, K. M., and Witten, I. H.\n1999.\nIssues in stacked\ngeneralization. Journal of Artiï¬cial Intelligence Research\n10:271â€“289.\nVapnik, V. 1998. Statistical learning theory. 1998. Wiley,\nNew York.\nVincent, P.; Larochelle, H.; Lajoie, I.; Bengio, Y.; and Man-\nzagol, P.-A. 2010. Stacked denoising autoencoders: Learn-\ning useful representations in a deep network with a local de-\nnoising criterion. Journal of Machine Learning Research\n11:3371â€“3408.\nWiering, M.; Van der Ree, M.; Embrechts, M.; Stollenga,\nM.; Meijster, A.; Nolte, A.; and Schomaker, L. 2013. The\nneural support vector machine. In BNAIC.\nWolpert, D. H.\n1992.\nStacked generalization.\nNeural\nnetworks 5(2):241â€“259.\nZhang, Y.; Liang, P.; and Wainwright, M. J. 2016. Con-\nvexiï¬ed convolutional neural networks.\narXiv preprint\narXiv:1609.01000.\nZhang, T. 2003. Statistical behavior and consistency of clas-\nsiï¬cation methods based on convex risk minimization. An-\nnals of Statistics 32(1):56â€“134.\nZhi-Hua Zhou, J. F. 2017. Deep Forest: Towards an alterna-\ntive to deep neural networks. In IJCAI, 3553â€“3559.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-02-15",
  "updated": "2019-02-15"
}