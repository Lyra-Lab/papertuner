{
  "id": "http://arxiv.org/abs/1902.05731v1",
  "title": "SVM-based Deep Stacking Networks",
  "authors": [
    "Jingyuan Wang",
    "Kai Feng",
    "Junjie Wu"
  ],
  "abstract": "The deep network model, with the majority built on neural networks, has been\nproved to be a powerful framework to represent complex data for high\nperformance machine learning. In recent years, more and more studies turn to\nnonneural network approaches to build diverse deep structures, and the Deep\nStacking Network (DSN) model is one of such approaches that uses stacked\neasy-to-learn blocks to build a parameter-training-parallelizable deep network.\nIn this paper, we propose a novel SVM-based Deep Stacking Network (SVM-DSN),\nwhich uses the DSN architecture to organize linear SVM classifiers for deep\nlearning. A BP-like layer tuning scheme is also proposed to ensure holistic and\nlocal optimizations of stacked SVMs simultaneously. Some good math properties\nof SVM, such as the convex optimization, is introduced into the DSN framework\nby our model. From a global view, SVM-DSN can iteratively extract data\nrepresentations layer by layer as a deep neural network but with\nparallelizability, and from a local view, each stacked SVM can converge to its\noptimal solution and obtain the support vectors, which compared with neural\nnetworks could lead to interesting improvements in anti-saturation and\ninterpretability. Experimental results on both image and text data sets\ndemonstrate the excellent performances of SVM-DSN compared with some\ncompetitive benchmark models.",
  "text": "SVM-based Deep Stacking Networks\nJingyuan Wang‚Ä†,¬ß, Kai Feng‚Ä†, Junjie Wu‚Ä°,¬ß‚àó\n‚Ä† MOE Engineering Research Center of Advanced Computer Application Technology,\nSchool of Computer Science Engineering, Beihang University, Beijing 100191, China\n‚Ä° Beijing Key Laboratory of Emergency Support Simulation Technologies for City Operations,\nSchool of Economics and Management, Beihang University, Beijing 100191, China\n¬ß Beijing Advanced Innovation Center for Big Data and Brain Computing,\nBeihang University, Beijing 100191, China\nEmail: {jywang, fengkai, wujj}@buaa.edu.cn\nAbstract\nThe deep network model, with the majority built on neural\nnetworks, has been proved to be a powerful framework to\nrepresent complex data for high performance machine learn-\ning. In recent years, more and more studies turn to non-\nneural network approaches to build diverse deep structures,\nand the Deep Stacking Network (DSN) model is one of such\napproaches that uses stacked easy-to-learn blocks to build\na parameter-training-parallelizable deep network. In this pa-\nper, we propose a novel SVM-based Deep Stacking Network\n(SVM-DSN), which uses the DSN architecture to organize\nlinear SVM classiÔ¨Åers for deep learning. A BP-like layer tun-\ning scheme is also proposed to ensure holistic and local opti-\nmizations of stacked SVMs simultaneously. Some good math\nproperties of SVM, such as the convex optimization, is intro-\nduced into the DSN framework by our model. From a global\nview, SVM-DSN can iteratively extract data representations\nlayer by layer as a deep neural network but with paralleliz-\nability, and from a local view, each stacked SVM can con-\nverge to its optimal solution and obtain the support vectors,\nwhich compared with neural networks could lead to interest-\ning improvements in anti-saturation and interpretability. Ex-\nperimental results on both image and text data sets demon-\nstrate the excellent performances of SVM-DSN compared\nwith some competitive benchmark models.\nIntroduction\nRecent years have witnessed the tremendous interests from\nboth the academy and industries in building deep neu-\nral networks (Hinton and Salakhutdinov 2006; Bengio,\nCourville, and Vincent 2013). Many types of deep neu-\nral networks have been proposed for classiÔ¨Åcation, regres-\nsion and feature extracting tasks, such as Stacked Denois-\ning Autoencoders (SAE) (Vincent et al. 2010), Deep Be-\nlief Networks (DBN) (Hinton 2011), deep Convolutional\nNeural Networks (CNN) (Krizhevsky, Sutskever, and Hin-\nton 2012), Recurrent Neural Networks (RNN) (Medsker and\nJain 2001), and so on.\nMeanwhile, the shortcomings of neural network based\ndeep models, such as the non-convex optimization, hard-\nto-parallelizing, and lacking model interpretation, are get-\nting more and more attentions from the pertinent research\n‚àóCorresponding author\nCopyright c‚Éù2019, Association for the Advancement of ArtiÔ¨Åcial\nIntelligence (www.aaai.org). All rights reserved.\nsocieties. Some potential solutions have been proposed to\nbuild deep structure models using non neural network ap-\nproaches. For instance, in the literature, the PCANet build\na deep model using an unsupervised convolutional prin-\ncipal component analysis (Chan et al. 2015). The gcFor-\nest builds a tree based deep model using stacked random\nforests, which is regarded as a good alternative to deep neu-\nral networks (Zhi-Hua Zhou 2017). Deep Fisher Networks\nbuild deep networks by stacking Fisher vector encoding into\nmultiple layers (Simonyan, Vedaldi, and Zisserman 2013).\nAlong this line, in this paper, we propose a novel SVM-\nbased Deep Stacking Network (SVM-DSN) for deep ma-\nchine learning. On one hand, SVM-DSN belongs to the com-\nmunity of Deep Stacking Networks (DSN), which consist of\nmany stacked multilayer base blocks that could be trained in\na parallel way and have comparable performance with deep\nneural networks (Deng and Yu 2011; Deng, He, and Gao\n2013). In this way, SVM-DSN can gain the deep learning\nability with extra scalability. On the other hand, we replace\nthe traditional base blocks in a DSN, i.e., the perceptrons, by\nthe well known Support Vector Machine (SVM), which has\nlong been regarded as a succinct model with appealing math\nproperties such as the convexity in optimization, and was\nconsidered as a different method to model complicated data\ndistributions compared with deep neural networks (Bengio\nand others 2009). In this way, SVM-DSN can gain the abil-\nity in anti-saturation and enjoys improved interpretability,\nwhich are deemed to be the tough challenges to deep neu-\nral networks. A BP-like Layered Tuning (BLT) algorithm is\nthen proposed for SVM-DSN to conduct holistic and local\noptimizations for all base SVMs simultaneously.\nCompared with the traditional deep stacking networks and\ndeep neural networks, the SVM-DSN model has the follow-\ning advantages:\n‚Ä¢ The optimization of each base-SVM is convex. Using the\nproposed BLT algorithm, all base-SVMs are optimized as\na whole, and meanwhile each base-SVM can also con-\nverge to its own optimum. The Ô¨Ånal solution of SVM-\nDSN is a group of optimized linear SVMs that are in-\ntegrated as a deep model. This advantage allows SVM-\nDSN to avoid the neuron saturation problem in deep neu-\nral networks, and thus could improve the performance.\n‚Ä¢ The SVM-DSN model is very easy to parallelize. The\narXiv:1902.05731v1  [cs.LG]  15 Feb 2019\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\nùëäùëä1\nùëàùëà1\nùëäùëä2\nùëàùëà2\nùëàùëà3\nùëàùëà4\nùëäùëä3\nùëäùëä4\nFigure 1: An illustration of the DSN architecture (Deng, He,\nand Gao 2013). The color is used to distinguish different\nblocks in a DSN. The components in the same color belong\nto the same block.\ntraining parallelization in SVM-DSN can reach the base-\nSVM level due to the support vectors oriented property of\nSVM, but the traditional DSN can only reach the block\nlevel.\n‚Ä¢ The SVM-DSN model has improved interpretability. The\nsupport vectors in base-SVMs can provide some insight-\nful information about what a block learned from train-\ning data. This property empowers users to partially un-\nderstand the feature extracting process of the SVM-\nDSN model.\nExperimental results on image and sentiment classiÔ¨Åca-\ntion tasks show that SVM-DSN model obtains respectable\nimprovements over neural networks. Moreover, compared\nwith the stacking models with strong base-learners, the\nSVM-DSN model also demonstrates signiÔ¨Åcant advantages\nin performance.\nThe SVM-DSN Model\nFramework of Deep Stacking Network\nThe Deep Stacking Network is a scalable deep machine\nlearning architecture (Deng, He, and Gao 2013; Deng and\nYu 2011) that consists of stacked easy-to-learn blocks in a\nlayer by layer manner. In the standard DSN, a block is a\nsimpliÔ¨Åed multilayer perceptron with a single hidden layer.\nLet the inputs of a block be a vector x, the block uses a\nconnection weight matrix W to calculate the hidden layer\nvector h as\nh = œï\n\u0000W ‚ä§x\n\u0001\n,\n(1)\nwhere œï(x) = 1/(1 + exp(‚àíx)) is a sigmoid nonlinear ac-\ntivation function. Using a weight matrix U, the objective\nfunction of the DSN block optimization is deÔ¨Åned as\nmin\n\r\ry ‚àíU ‚ä§h\n\r\r2\nF .\n(2)\nAs shown in Fig. 1, the blocks of a DSN are stacked layer\nby layer. For the block in the input layer, the input vector x\ncontains only the raw input features. For blocks in the mid-\ndle layers, x is a concatenated vector of the raw input fea-\ntures and output representations of all previous layer blocks.\nThe training of deep stacking networks contains two\nsteps: block training and Ô¨Åne-tuning. In the block training\nstep, the DSN blocks are independently training as super-\nvised multilayer perceptrons. In the Ô¨Åne-tuning step, all the\nstacked blocks are considered as a multi-layer deep neural\nnetwork. The parameters of DSN are end-to-end trained us-\ning the error Back Propagation (BP) algorithm.\nSVM-based DSN Blocks\nIn the SVM-DSN model, we adopt support vector machines\nto implement a DSN block. A SVM classiÔ¨Åer is a hyperplane\nœâ‚ä§x + b = 0 that divides the feature space of a data sample\nx into two parts ‚Äî one for the positive and the other for the\nnegative. The parameters œâ and b are optimized to maximize\nthe minimum distances from the hyperplane to a set of train-\ning samples T = {(xk, yk)|yk ‚àà{‚àí1, 1}, k = 1, . . . , K},\ni.e.,\nmax\nœâ,b\n2\n‚à•œâ‚à•\ns.t.\nyk(œâ‚ä§xk + b) ‚â•1,\nk = 1, 2, . . . , N.\n(3)\nA training sample is called a support vector if the constraint\nin Eq. (3) turns into equality.\nFor a multi-class problem with N classes, we connect the\ninput vector x of a DSN block with N binary SVM classi-\nÔ¨Åers ‚Äî each for recognizing whether a sample belongs to\na corresponding class ‚Äî to predict the label of a sample. A\nbinary SVM classiÔ¨Åer in a DSN block is called a base-SVM.\nThe N binary SVM classiÔ¨Åers for a N classiÔ¨Åcation problem\nis called as a base-SVM group. A SVM-DSN block could\ncontains multiple base-SVM groups. In the same block, all\nbase-SVM groups share the same input vector x.\nStacking Blocks\nGiven a classiÔ¨Åcation hyperplane of a SVM, the decision\nfunction for the sample xk is expressed as\nf(xk) = sign\n\u0000œâ‚ä§xk + b\n\u0001\n,\n(4)\nwhere f(xk) = 1 for the positive class and f(xk) = ‚àí1\nfor the negative. The distance from a sample to the hyper-\nplane could be considered as the conÔ¨Ådence of a classiÔ¨Å-\ncation decision. For the samples behind the support vec-\ntors, i.e.,\n\f\fœâ‚ä§xk + b\n\f\f > 1, the conÔ¨Ådence is 1, otherwise\nis\n\f\fœâ‚ä§xk + b\n\f\f. We therefore can express the classiÔ¨Åcation\nconÔ¨Ådence of a SVM classiÔ¨Åer for the sample xk as\ng(xk) = min\n\u00001, |œâ‚ä§xk + b|\n\u0001\n.\n(5)\nWe denote the i-th base-SVM in the layer l as svm(l, i)\nand its decision function and conÔ¨Ådence as f (l,i)(¬∑) and\ng(l,i)(¬∑), respectively. For the base-SVM svm(l, i), we de-\nÔ¨Åne a conÔ¨Ådence weighted output y(l,i) as\ny(l,i) = f (l,i)(x) ¬∑ g(l,i)(x).\n(6)\nIn the layer l+1, SVM-DSN concatenates the conÔ¨Ådence\nweighted outputs of all base-SVMs in the previous layers\nand raw inputs as\nx(l+1) =\n\u0010\ny(l,1), . . . , y(l,i), . . . , y(l‚àí1,1), . . . , y(l‚àí1,i),\n. . . , y(1,1), . . . , y(1,i), . . . , x(1,1), . . . , x(1,i)\u0011‚ä§\n.\n(7)\nThe base-SVMs in the layer l+1 use x(l+1) as the input to\ngenerate their conÔ¨Ådence weighted outputs y(l+1,i). In this\nway, base-SVMs are stacked and connected layer by layer.\nModel Training\nBlock Training\nSimilar to the standard deep stacking network, the training\nof the SVM-DSN model also contains a block training step\nand a Ô¨Åne-tuning step.\nIn the block training step, the base-SVMs in a DSN block\nare trained as regular SVM classiÔ¨Åers. Given a set of training\nsamples T = {(xk, yk)|k = 1, . . . , K}, where yk is the\nground-truth label of xk, the objective function of a base-\nSVM group with N classiÔ¨Åcation is deÔ¨Åned as\nJ = 1\n2 ‚à•‚Ñ¶‚à•2\nF\n+ C\nK\nX\nk=1\nN\nX\ni=1\n‚Ñìhinge\n\u0010\ny(i)\nk (œâ(i)‚ä§xk + b(i))\n\u0011\n,\n(8)\nwhere ‚Ñ¶=\n\u0000œâ(1)‚ä§, . . . , œâ(N)‚ä§\u0001\n, and y(i)\nk\n= 1 if yk = i and\n-1 otherwise. The function ‚Ñìhinge(¬∑) is a hinge loss function\ndeÔ¨Åned as ‚Ñìhinge(z) = max (0, 1 ‚àíz). The parameter Œ∏ =\n\b\n(œâ(i), b(i))|‚àÄi\n\t\nis inferred as Œ∏ = arg min\nŒ∏\nJ (Œ∏).\nIn order to increase the diversity of base-SVM groups in\na block, we adopt a bootstrap aggregating method in the\nblock training. For a block with M base-SVM groups, we\nre-sample the training data as M sets using the bootstrap\nmethod (Efron and Tibshirani 1994). Each base-SVM group\nis trained using one re-sampled data set.\nFine Tuning\nThe traditional DSN model is based on neural networks and\nuses the BP algorithm in the Ô¨Åne-tuning step. For the SVM-\nDSN model, we introduce SVM training into the BP algo-\nrithm framework, and propose a BP-like Layered Tuning\n(BLT) algorithm to Ô¨Åne-tune the model parameters.\nAlgorithm 1 gives the pseudocodes of BLT. In general,\nBLT iteratively optimizes the base-SVMs from the output\nlayer to the input layer. In each iteration, BLT optimizes\nsvm(l, i) by Ô¨Årstly generating a set of virtual training sam-\nples T (l,i) = {(x(l)\nk , Àúy(l,i)\nk\n)|k = 1, . . . , K}, and then trains\na new svm(l, i) on T (l,i).\nAccording to Eq. (6) and Eq. (7), it is easy to have x(l)\nk =\n(y(l‚àí1,1)\nk\n, y(l‚àí1,2)\nk\n, ¬∑ ¬∑ ¬∑ , y(l‚àí1,i)\nk\n, ¬∑ ¬∑ ¬∑ )‚ä§. However, the calcu-\nlation of the virtual label Àúy(l,i)\nk\nis not that straightforward.\nAlgorithm 1 BP-like Layered Tuning Algorithm\n1: Initialization: Initializing œâ(l,i), b(l,i) for all svm(l, i) as ran-\ndom values.\n2: repeat\n3:\nSelect a batch of training samples T = {(xk, yk)|k =\n1, . . . , K}.\n4:\nfor l = L, L ‚àí1, . . . , 2, 1 do\n5:\nfor i = 1, 2, . . . do\n6:\nUse Eq. (6), Eq. (7), and Eq. (9) to calculate T (l,i) =\n{(x(l)\nk , Àúy(l,i)\nk\n)|k = 1, . . . , K}.\n7:\nUse T (l,i) to train svm(l, i) as Eq. (11).\n8:\nend for\n9:\nend for\n10: until The algorithm converges.\nSpeciÔ¨Åcally, BLT adopts a gradient descent method to cal-\nculate Àúy(l,i)\nk\nas\nÀúy(l,i)\nk\n= œÉ\n \ny(l,i)\nk\n‚àíŒ∑ ‚àÇJ (o)\n‚àÇy(l,i)\n\f\f\f\f\ny(l,i)=y(l,i)\nk\n!\n,\n(9)\nwhere J (o) is the objective function of the output layer, y(l,i)\nk\nis the output of x(l)\nk in the previous iteration, Œ∑ is the learning\nrate, and œÉ(¬∑) is a shaping function deÔ¨Åned as\nœÉ(z) =\nÔ£±\nÔ£≤\nÔ£≥\n1,\nz > 1\nz,\n|z| ‚â§1\n‚àí1,\n< ‚àí1\n.\n(10)\nNote that since the term ‚àíŒ∑‚àÇJ (o)/‚àÇy(l,i) in Eq. (9) is a neg-\native gradient direction of J (o), tuning the output y(l,i) to\nthe virtual label Àúy(l,i) can reduce the value of the objective\nfunction J (o) in the output layer. Therefore, it could be ex-\npected that BLT can lower the overall model prediction error\niteratively by training base-SVMs on virtual training sets in\neach iteration.\nGiven the training set T (l,i)\n=\n{(x(l)\nk , Àúy(l,i)\nk\n)|k\n=\n1, . . . K}, the objective function of training svm(l, i) is de-\nÔ¨Åned as\nmin J (l,i) = 1\n2\n\r\r\rœâ(l,i)\r\r\r\n2\n+ C1\nX\nk‚ààŒò\n‚Ñìhinge\n\u0010\nÀúy(l,i)\nk\n(œâ(l,i)‚ä§x(l)\nk + b(l,i))\n\u0011\n|\n{z\n}\nThe SVM Loss\n+ C2\nX\nk /‚ààŒò\n‚Ñìœµ\n\u0010\nœâ(l,i)‚ä§x(l)\nk + b(l,i) ‚àíÀúy(l,i)\nk\n\u0011\n|\n{z\n}\nThe SVR Loss\n,\n(11)\nwhere Œò is the index set of the virtual labels\n\f\f\fÀúy(l,i)\nk\n\f\f\f = 1,\nand the function ‚Ñìœµ(¬∑) is an œµ-insensitive loss function in the\nform of ‚Ñìœµ(z) = max(|z| ‚àíœµ, 0).\nNote that the objective function in Eq. (11) contains two\ntypes of loss functions so as to adapt to the different condi-\ntions of Àúy(l,i)\nk\n. When Àúy(l,i)\nk\n‚àà{‚àí1, 1}, i.e., the virtual labels\nare binary, BLT trains svm(l, i) as the standard SVM classi-\nÔ¨Åer and thus uses the hinge loss function to measure errors.\nWhen Àúy(l,i)\nk\n‚àà(‚àí1, 1), the objective function adopts a Sup-\nport Vector Regression loss term ‚Ñìœµ for this condition. In the\nAppendix, we prove that the problem deÔ¨Åned in Eq. (11)\nis a quadratic convex optimization problem. The training of\nsvm(l, i) can thus reach an optimal solution by using vari-\nous quadratic programming methods such as sequential min-\nimal optimization and gradient descents.\nWe Ô¨Ånally turn to the small problem unsolved ‚Äî how\nto calculate the partial derivative ‚àÇJ (o)/‚àÇy(l,i) in Eq. (9).\nBased on the chain rule, the partial derivative can be recur-\nsively calculated as\n‚àÇJ\n‚àÇy(l,i) =\nL\nX\nm=l+1\nX\nj\n‚àÇJ\n‚àÇy(m,j)\ndy(m,j)\ndz(m,j)\n‚àÇz(m,j)\n‚àÇy(l,i)\n=\nL\nX\nm=l+1\nX\nj\n‚àÇJ\n‚àÇy(m,j) y‚Ä≤ \u0010\nz(m,j)\u0011\nœâ(m,j)\ni\n,\n(12)\nwhere œâ(m,j)\ni\nis the connection weight of y(m,j) in\nsvm(m, j), and z(m,j) = œâ(m,j)‚ä§x(m‚àí1) + b(m,j). The\nterm y‚Ä≤(z) is the derivative of the function in Eq. (6), which\nis in the form of\ny‚Ä≤(z) =\n\u001a 0,\n|z| > 1\n1,\n|z| ‚â§1.\n(13)\nThe principle of this chain derivation is similar to the error\nback-propagation of the neural network training. The differ-\nence lies in that the BP algorithm calculates the derivative\nfor each neuron connecting weight but BLT for each base-\nSVM output. That is why we name our algorithm as BP-like\nLayered Tuning.\nModel Properties\nConnection to Neural Networks\nThe SVM-DSN model has close relations with neural\nnetworks. If we view the base-SVM output function deÔ¨Åned\nin Eq. (6) as a neuron, the SVM-DSN model can be regarded\nas a type of neural networks. SpeciÔ¨Åcally, we can rewrite the\nfunction in Eq. (6) as a neuron form as follows:\ny(l,i) = œÉ\n\u0010\nœâ\n(l,i)‚ä§x(l) + b(l,i)\u0011\n,\n(14)\nwhere the shaping function œÉ(¬∑) works as an activate func-\ntion, with the output œÉ(z) ‚àà{1, ‚àí1} if |z| ‚â•1, and\nœÉ(z) = z if |z| < 1. As proved in (Hornik 1991), a multi-\nlayer feedforward neural network with arbitrary bounded\nand non-constant activation function has an universal ap-\nproximation capability. As a consequence, we could expect\nthat the proposed SVM-DSN model also has the universal\napproximation capability in theory.\nNevertheless,\nthe\ndifference\nbetween\nthe\nSVM-\nDSN model and neural networks is still signiÔ¨Åcant. Indeed,\nwe have proven in the Appendix that the base-SVMs in our\nSVM-DSN model have the following property: Given a set\nof virtual training samples {(x(l)\nk , Àúy(l,i)\nk\n)|k = 1, . . . , K} for\nsvm(l, i), to minimize the loss function deÔ¨Åned in Eq. (11)\nis a convex optimization problem. Moreover, because the\nbase-SVMs in the same block are mutually independent, the\noptimization of the whole block is a convex problem too.\nThis implies that, in each iteration of the BLT algorithm, all\nblocks can converge to an optimal solution. In other words,\nSVM-DSN ensures that all blocks and their base-SVMs ‚Äúdo\ntheir own best‚Äù to minimize their own objective functions\nin each iteration, which however is not the case for neural\nnetworks and MLP based deep stacking networks. It is\nalso worth noting that this ‚Äúdo their own best‚Äù property is\ncompatible with the decrease of the overall prediction error\nmeasured by the global objective function J (o).\nAn important advantage empowered by the ‚Äúdo their own\nbest‚Äù property is the anti-saturation feature of SVM-DSN.\nIn neural network models, the BP algorithm updates the pa-\nrameter œâ of a neuron as œâ ‚ÜêŒ∑‚àÇJ /‚àÇœâ. Hence, the partial\nderivative for the i-th œâ in the j-th neuron at the layer l is\ncalculated as\n‚àÇJ\n‚àÇœâ(l,j)\ni\n=\n‚àÇJ\n‚àÇy(l,j)\ndy(l,j)\ndz(l,j)\n‚àÇz(l,j)\n‚àÇœâ(l,j)\ni\n=\n‚àÇJ\n‚àÇy(l,j) ¬∑ y‚Ä≤ \u0010\nz(l,j)\u0011\n¬∑ y(l‚àí1,i),\n(15)\nwhere y‚Ä≤ \u0000z(l,j)\u0001\nis a derivative of the activation function.\nFor the sigmoid activation function, if |z| is very large then\ny‚Ä≤ becomes very small, and ‚àÇJ/‚àÇœâ ‚Üí0. In this condition,\nthe BP algorithm cannot update œâ any more even if there is\nstill much room for the optimization of œâ. This phenomenon\nis called the ‚Äúneuron saturation‚Äù in neural network training.\nFor the ReLU activation function, similar condition appears\nwhen z < 0, where y‚Ä≤ = 0 and ‚àÇJ/‚àÇœâ = 0. The neuron\nwill die when a ReLU neuron fall into this condition.\nIn the BLT algorithm of SVM-DSN model, the update of\na base-SVM is guided by ‚àÇJ /‚àÇy, with the details given in\nEq. (12). From Eq. (12), we can see that unless all base-\nSVMs in an upper layer are saturated, i.e., y‚Ä≤(z(m,j)) = 0 for\nall z(m,j), the base-SVMs in the layer l would not fall into\nthe saturation state. Therefore, we could expect that the sat-\nuration risk of a base-SVM in SVM-DSN tends to be much\nlower than a neuron in neural networks.\nInterpretation\nIn SVM classiÔ¨Åers, the support vector samples could pro-\nvide some interpretation information about what a classiÔ¨Åer\nlearned from data set. The SVM-DSN inherit this interpre-\ntation property of SVM. In the base-SVM output function\ndeÔ¨Åned in Eq. (6), the conÔ¨Ådence function g(x) indicates\nwhether a sample is clearly classiÔ¨Åed by the hyperplane of\na base-SVM. For a sample to be classiÔ¨Åed, we can calcu-\nlate the average conÔ¨Ådence of the sample in all base-SVM\ngroups in a block. The average conÔ¨Ådence indicates whether\nthe feature extracted by the block and previous layers of-\nfer enough representations to identify label of the sample.\nBecause the samples with low conÔ¨Ådence are near to the hy-\nperplane, we can use the low conÔ¨Ådence samples to form\na ‚Äúclassifying plane‚Äù map in each blocks. Comparing the\n‚Äúclassifying plane‚Äù maps layer by layer, we could partly un-\nderstand the feature extracting process of the stacked block\nin a SVM-DSN model.\n2\n1\n0\n1\n2\n2.0\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n0.62\n0.64\n0.66\n0.68\n0.70\n0.72\n0.74\n(a) Layer-1\n2\n1\n0\n1\n2\n2.0\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n0.575\n0.600\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775\n(b) Layer-2\n2\n1\n0\n1\n2\n2.0\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(c) Layer-3\nFigure 2: The classifying plane maps of each layer in the SVM-DSN.\n1.0\n0.5\n0.0\n0.5\n1.0\n1.0\n0.5\n0.0\n0.5\n1.0\n1\n-1\nFigure 3: The Circle Data\nWe here give a show case to explain the interpretation\nproperty of SVM-DSN in a direct way. In this case, we gen-\nerate a circle data set containing samples of two classes, as\nshown in Fig. 3. The positive samples are generated from\na circle with radius of 0.5 plus a Gaussian noise with vari-\nance of 0.1. The negative samples are generated from a circle\nwith radius of 1 plus the same Gaussian noise. We use the\ncircle data set to train a 3-layer SVM-DSN model, where the\nmiddle layers contain 40 and 60 base-SVM groups, respec-\ntively. Because this experiment is a binary classiÔ¨Åcation, a\nbase-SVM group only contains one base-SVM.\nIn the experiment, we traverse the feature space from the\ncoordinate point (‚àí2, ‚àí2) to (2, 2). For each coordinate\npoint, we calculate the average conÔ¨Ådence of all base-SVMs\nin each layer. Figs. 2(a) - 2(c) plot the conÔ¨Ådence distribu-\ntion maps in different layers. The samples with low conÔ¨Å-\ndence are thus near to the SVM classiÔ¨Åcation hyperplane, so\nthe low conÔ¨Ådence areas in red form the ‚Äúclassifying plane‚Äù\nof a layer.\nIt is obvious that there is a clear ‚Äúclassifying plane‚Äù gen-\neration process from Figs. 2(a) to 2(c). In the layer 1, the low\nconÔ¨Ådence values concentrate in the center of the map. In the\nlayer 2, the low conÔ¨Ådence values have a vague shape as a\ncircle. In the layer 3, the low conÔ¨Ådence values distribute as\na circle shape that clearly divides the feature space into two\nparts. This process demonstrates how an SVM-DSN model\nextracts the data representations of the feature space layer\nby layer.\nParallelization\nThe deep stacking network is proposed for parallel param-\neter learning. For a DSN with L blocks, given a group of\ntraining samples, the DSN can resample the data set as L\nbatches. A DSN block only uses one batch to update its pa-\nrameter. In this way, the training of DSN parameters could\nbe deployed over L processing units. The parallelization of\nDSN training is in the block level.\nThis parallel parameter learning property could be further\nextended by using Parallel Support Vector Machines (Graf\net al. 2005). Because in a SVM classiÔ¨Åer, only the support\nvector samples are crucial, we could divide a training set as\nseveral M sub-sets, and use M virtual SVM classiÔ¨Åers to se-\nlect support vector candidates from each sub-set. Finally, we\nuse the support vector candidates of all sub-sets to train the\nÔ¨Ånal model. In this way, the training of a SVM-DSN block\ncan be deployed over M processing units, and the all SVM-\nDSN model can be deployed over L√óM processors. That is\nto say the training of base-SVMs in a block is also paralleliz-\nable in SVM-DSN. The parallelization of SVM-DSN train-\ning is in the base-SVM level. The parallel degree of whole\nmodel is greatly improved. As reported in Ref (Graf et al.\n2005), the speed-up for a 5-layer Cascade SVM (16 parallel\nSVMs) to a single SVM is about 10 times for each pass and\n5 times for fully converged.\nExperiments\nImage ClassiÔ¨Åcation Performance\nWe Ô¨Årst test the performance of SVM-DSN on the MNIST\nimage classiÔ¨Åcation database (LeCun et al. 1998). The\nMNIST database contains 60,000 handwritten digits im-\nages in the size of 28 √ó 28 for training and validation, and\n10,000 images for testing. The images are classiÔ¨Åed as ten\nclasses according to the digits written on them. The SVM-\nDSN model used in the experiment consists of three layers ‚Äì\ntwo middle layers and an output layer. Both of the block in\nthe two middle layers contains 20 base-SVM groups, and\neach group contains 10 base-SVMs, i.e., 200 base-SVMs\none layer. We actually had tried neural networks with deeper\nlayers, but the classiÔ¨Åcation performance could not be im-\nproved signiÔ¨Åcantly. The benchmark models includes: i) A\n3-layer deep stacking network where the block in each mid-\ndle layer contains 200 hidden neurons; ii) As analyzed in the\nModel Properties section, the SVM-DSN could be consid-\nered as a type of neural network. Therefore, we use the BP\nalgorithm to train a same structure SVM-DSN as a bench-\nmark; iii) The 3-layer neural network models with the same\nneuron connection structure as the 3-layer SVM-DSN. In\nTable 1: MNIST ClassiÔ¨Åcation Performance\nModels\nError Rate (%)\n3-layer SVM-DSN\n1.49\n3-layer DSN\n1.65\n3-layer SVM-DSN, BP\n1.62\n3-layer NN, SVM output, sigmoid\n1.74\n3-layer NN, SVM output, tanh\n1.59\n3-layer NN, SVM output, ReLU\n1.56\nHomepage benchmark (LeCun et al. 1998)\n1.53\nBagging of base-SVMs\n5.41\nthese benchmarks, we use SVM output (Tang 2013), and\nthe different activate functions in neurons; iv) The best 3-\nlayer NN benchmark listed in the homepage of MNIST ‚Äì 3-\nlayer NN, 500+300 hidden units, cross entropy loss, weight\ndecay (LeCun et al. 1998); v) A bagging of 41 base-SVM\ngroups, each group contains 10 base-SVMs.\nIn the SVM-DSN model Ô¨Åne-tuning, we have two hyper-\nparameters to set, i.e., C1 and C2 of the base-SVM‚Äôs objec-\ntive function. The two hyper-parameters are used to balance\nstructural risks and empirical risks in a base-SVM. Either\ntoo big or too small for the two hyper-parameters may lead\nmodel performance degenerate. Therefore, we use the trial\nand error method to set the hyper-parameters. The learn-\ning rate Œ∑ is the other hyper-parameter, which could be dy-\nnamic setting using elegant algorithms such as Adam. In our\nexperiment, we directly set the learning rate as a Ô¨Åx value\nŒ∑ = 0.0005 to ensure the experiment fairness.\nTable 1 gives the MNIST image classiÔ¨Åcation results.\nThe SVM-DSN model achieved the best performance com-\npared with the other benchmarks, which veriÔ¨Åed the ef-\nfectiveness of SVM-DSN. In the benchmarks, the 3-layer\nSVM-DSN+BP model has the same model structure with\nSVM-DSN but was trained by the BP algorithm. The results\nshow that SVM-DSN has a better performance than the\nSVM-DSN+BP benchmark, which indicates that the base-\nSVMs ‚Äúdo their own best‚Äù feature of SVM-DSN is a posi-\ntive feature for model performance. In fact, the idea of BLT\nÔ¨Åne-tuning could be extend to optimize the deep stacking\nnetworks with any derivable model as blocks, such as soft\ndecision-making tree and linear discriminant analysis.\nFeature Extractor Compatibility\nCurrently, the mainstream image classiÔ¨Åers usually adopt\nconvolutional neural networks (CNN) as feature extractors.\nTable 2 demonstrates the MNIST classiÔ¨Åcation performance\nof SVM-DSN with a CNN feature extractor. In the exper-\niment, we connect a CNN feature extractor with a 3-layer\nSVM-DSN model. The structure of SVM-DSN is same as\nin Table 1. The CNN feature extractor contains 3 convo-\nlutional layers, and each layer consists of 24 Ô¨Çitters with\nthe 5√ó5 receptive Ô¨Åeld. In the CNN and SVM-DSN mix-\nture model, we Ô¨Årst pre-trained the CNN part using BP, and\nthen uses the feature extracted by CNN as input of the SVM-\nDSN part to train the blocks. In the Ô¨Åne-tuning step, the\nCNN part is Ô¨Åne tuned by BP and the SVM-DSN part is\ntuned by BLT. The benchmark models include: i) The same\nTable 2: MNIST ClassiÔ¨Åcation with CNN Feature Extractor\nModels\nError Rate (%)\nCNN + SVM-DSN\n0.51\nCNN + DSN\n0.60\nCNN + SVM-DSN, BP\n0.72\nCNN + sigmoid activation\n0.80\nCNN + tanh activation\n0.67\nCNN + ReLU activation\n0.58\nHomepage benchmark (LeCun et al. 1998)\n0.54\ngcForest (Zhi-Hua Zhou 2017)\n0.74\nTable 3: IMDB ClassiÔ¨Åcation Performance\nModels\nError Rate (%)\nSVM-DSN\n10.51\nDSN\n11.15\nSVM-DSN, BP\n11.42\nRandom Forest\n14.68\nXGBoost\n14.77\nAdaBoost\n16.63\nSVM (linear kernel)\n12.43\nStacking\n11.55\nBagging of base-SVMs\n11.66\ngcForest (Zhi-Hua Zhou 2017)\n10.84\nCNN feature extractor connected with a 3-layer DSN, where\nthe 3-layer DSN has the same structure with the 3-layer\nSVM-DSN model; ii) The CNN+SVM-DSN model trained\nby the BP algorithm; iii) The neural networks consist of 3\nCNN layers and 3-layer neural network with different acti-\nvate functions, where the structures of the CNN and the neu-\nral network are same as the CNN + SVM-DSN model; iv)\nThe trainable CNN feature extractor + SVMs with afÔ¨Åne dis-\ntortions, which is the best benchmark with the similar mod-\nels scale listed in the MNIST homepage; v) The gcForest\nwith convolutional kernels (Zhi-Hua Zhou 2017). As shown\nin Table 2, the CNN + SVM-DSN model achieved the best\nperformance, which veriÔ¨Åed the effectiveness of our model\nagain. What‚Äôs more, this experiment demonstrates that the\nSVM-DSN model is completely compatible to the neural\nnetwork framework. The other types of networks, such as\nRNN and LSTM, could also be used as feature extractors of\nSVM-DSN to adapt diversiÔ¨Åed application scenarios.\nComparison with Ensemble Models\nIn this section, we compare the performance of SVM-\nDSN with several classical ensemble models. Because on\nthe MNIST data set, the performance of ensemble mod-\nels are usually not very well. For a fair comparison, we\nuse the IMDB sentiment classiÔ¨Åcation data set (Maas et\nal. 2011) in our experiment. Many tree based ensemble\nmethods achieved good performance on this data set (Zhi-\nHua Zhou 2017). The IMDB dataset contains 25,000 movie\nreviews for training and 25,000 for testing. The movie re-\nviews are represented by tf-idf features and labeled as pos-\nitives and negatives. The SVM-DSN model used in this ex-\nperiment consists of 4 middle layers, and the number of\nbase-SVMs in the middle layer are 1024-1024-512-256. The\nbenchmark models include Random Forest, XGBoost, Ad-\naBoost and SVM (linear kernel). The four benchmarks are\nalso stacked as a stacking benchmark (Perlich and ¬¥Swirszcz\n2011). A bagging of base-SVMs and the grForest (Zhi-\nHua Zhou 2017) are also included as competitor. A 4-layer\nDSN is used as a benchmark, where the number of hidden\nneurons in the middle layer blocks are same as the number of\nbase-SVMs in the SVM-DSN model. The SVM-DSN model\ntrained by the BP algorithm is also used as the benchmark.\nAs shown in Table 3, the SVM-DSN model achieved\nthe best performance again. Especially, the performance of\nSVM-DSN is better than the stacking benchmark, which in-\ndicates that holistic optimized multi-layer stacking of linear\nbase-learners can defeat the traditional two-layer stacking of\nstrong base-learners. In the experiment, the performance of\nthe BLT algorithm is yet better than the BP algorithm. The\n‚Äúdo their best‚Äù feature of base-SVM in BLT is still effective\nin text sentiment classiÔ¨Åcation.\nRelated Works\nThis work has close relations with SVM, deep learning,\nand stacking. The support vector machine was Ô¨Årst pro-\nposed by Vapnik in (Vapnik 1998). Multi-layer structures\nin SVM were usually used as speedup solutions. In cas-\ncade SVM (Graf et al. 2005), a multi-layer cascade SVM\nmodel structure was used to select support vectors in a par-\nallel way. In the literature (Collobert, Bengio, and Bengio\n2002), a parallel mixture stacking structure was proposed to\nspeed up SVM training in the very large scale problems. Be-\nfore our work, some studies proposed to use SVM to replace\nthe output layer of a neural network (Wiering et al. 2013;\nTang 2013).\nIn recent years, neural network based deep models has\nachieved great success in various applications (Hinton and\nSalakhutdinov 2006). The gcFroest model (Zhi-Hua Zhou\n2017) was proposed to use the forest based deep model as\nan alternative to deep neural networks. The PCANet builds\na deep model using unsupervised convolutional principal\ncomponent analysis (Chan et al. 2015). LDANet is a super-\nvised extension of PCANet, which uses linear discriminant\nanalysis (LDA) to replace the PCA parts of PCANet (Chan\net al. 2015). Deep Fisher Networks build deep network\nthrough stacking Fisher vector encoding as multi-layers (Si-\nmonyan, Vedaldi, and Zisserman 2013).\nThe DSN framework adopted in this work is a scal-\nable deep architecture amenable to parallel parameter train-\ning, which has been adopted in various applications, such\nas information retrieval (Deng, He, and Gao 2013), image\nclassiÔ¨Åcation (Li, Chang, and Yang 2015), and speech pat-\ntern classiÔ¨Åcation (Deng and Yu 2011). T-DSN uses tensor\nblocks to incorporate higher order statistics of the hidden bi-\nnary features (Hutchinson, Deng, and Yu 2013). The CCNN\nmodel extends the DSN framework using convolutional neu-\nral networks (Zhang, Liang, and Wainwright 2016). To the\nbest of our knowledge, there are very few works introduce\nthe advantages of SVM into the DSN framework.\nStacking was introduced by Wolpert in (Wolpert 1992)\nas a scheme of combining multiple generalizers. In many\nreal-world applications, the stacking methods were used\nto integrate strong base-learners as an ensemble model\nto improve performance (Jahrer, T¬®oscher, and Legenstein\n2010). In the literature, most of stacking works focused\non designing elegant meta-learners and create better base-\nlearners, such as using class probabilities in stacking (Ting\nand Witten 1999), using a weighted average to combine\nstacked regression (Rooney and Patterson 2007), training\nbase-learners using cross-validations (Perlich and ¬¥Swirszcz\n2011), and applying ant colony optimization to conÔ¨Ågure\nbase-learners (Chen, Wong, and Li 2014). To the best of our\nknowledge, there are very few works to study how to opti-\nmize multi-layer stacked base-learners as a whole.\nConclusion\nIn this paper, we proposed an SVM-DSN model where linear\nbase-SVMs are stacked and trained in a deep stacking net-\nwork way. In the SVM-DSN model, the good mathematical\nproperty of SVMs and the Ô¨Çexible model structure of deep\nstacking networks are nicely combined in a same frame-\nwork. The SVM-DSN model has many advantage proper-\nties including holistic and local optimization, parallelization\nand interpretation. The experimental results demonstrated\nthe superiority of the SVM-DSN model to some benchmark\nmethods.\nAcknowledgments\nProf. J. Wang‚Äô s work was partially supported by the Na-\ntional Key Research and Development Program of China\n(No.2016YFC1000307), the National Natural Science Foun-\ndation of China (NSFC) (61572059, 61202426), the Science\nand Technology Project of Beijing (Z181100003518001),\nand the CETC Union Fund (6141B08080401). Prof. J.\nWu was partially supported by the National Natural Sci-\nence Foundation of China (NSFC) (71531001, 71725002,\nU1636210, 71471009, 71490723).\nAppendix\nProperty: Given a set of virtual samples T (l,i)\n=\n{(x(l)\nk , Àúy(l,i)\nk\n)|k = 1, . . . , K} for svm(l, i), to minimize the\nloss function deÔ¨Åned in Eq. (11) is a convex optimization\nproblem.\nProof. For the sake of simplicity, we omit the superscripts\n(l) of x(l)\nk\nand Àúy(l)\nk\nin our proof. We deÔ¨Åne a constrained\noptimization problem in the form of\nmin\nœâ,b,Œæk,ÀÜŒæk,Œ∂k\n1\n2 ‚à•œâ‚à•2 + C1\nX\nk/‚ààŒò\nŒ∂k + C2\nX\nk‚ààŒò\n\u0010\nŒæk + ÀÜŒæk\n\u0011\ns.t.\n1 ‚àíÀúyk\n\u0000œâ‚ä§xk + b\n\u0001\n‚â§Œ∂k,\n(1)\nŒ∂k ‚â•0, k ‚ààŒò;\n\u0000œâ‚ä§xk + b\n\u0001\n‚àíÀúyk ‚â§œµ + Œæk,\n(2)\nÀúyk ‚àí\n\u0000œâ‚ä§xk + b\n\u0001\n‚â§œµ + ÀÜŒæk,\n(3)\nŒæk ‚â•0, ÀÜŒæk ‚â•0, k /‚ààŒò.\n(16)\nWe can see the constrained optimization problem Eq. (16) is\nin a quadratic programming form as\nmin\na\n1\n2a‚ä§Ua + c‚ä§a\ns.t.\nQa ‚â§p,\n(17)\nwhere a = (œâ, b, Œæ, ÀÜŒæ, Œ∂), and U is a positive semi-deÔ¨Ånite\ndiagonal matrix. Therefore, the constrained optimization\nproblem is a quadratic convex optimization problem (Boyd\nand Vandenberghe 2004). It is easy to prove that the con-\nstrained optimization problem deÔ¨Åned in Eq. (16) is equiva-\nlent to the unconstrained optimization problem deÔ¨Åned in\nEq. (11) (Zhang 2003). Therefore, the optimization prob-\nlem of base-SVM is equivalent to the problem deÔ¨Åned in\nEq. (16). The optimization problem of base-SVM is a con-\nvex optimization problem.\nReferences\nBengio, Y., et al. 2009. Learning deep architectures for AI.\nFoundations and trends in Machine Learning 2(1):1‚Äì127.\nBengio, Y.; Courville, A.; and Vincent, P.\n2013.\nRepre-\nsentation learning: A review and new perspectives. IEEE\nTransactions on Pattern Analysis and Machine Intelligence\n(TPAMI) 35(8):1798‚Äì1828.\nBoyd, S., and Vandenberghe, L. 2004. Convex optimization.\nCambridge University.\nChan, T.-H.; Jia, K.; Gao, S.; Lu, J.; Zeng, Z.; and Ma, Y.\n2015.\nPCANet: A simple deep learning baseline for im-\nage classiÔ¨Åcation? IEEE Transactions on Image Processing\n24(12):5017‚Äì5032.\nChen, Y.; Wong, M.-L.; and Li, H.\n2014.\nApplying ant\ncolony optimization to conÔ¨Åguring stacking ensembles for\ndata mining. Expert Systems with Applications 41(6):2688‚Äì\n2702.\nCollobert, R.; Bengio, S.; and Bengio, Y. 2002. A parallel\nmixture of SVMs for very large scale problems. In NIPS,\n633‚Äì640.\nDeng, L., and Yu, D. 2011. Deep convex net: A scalable\narchitecture for speech pattern classiÔ¨Åcation.\nIn INTER-\nSPEECH, 2285‚Äì2288.\nDeng, L.; He, X.; and Gao, J. 2013. Deep stacking networks\nfor information retrieval. In ICASSP, 3153‚Äì3157. IEEE.\nEfron, B., and Tibshirani, R. J. 1994. An introduction to the\nbootstrap. CRC press.\nGraf, H. P.; Cosatto, E.; Bottou, L.; Dourdanovic, I.; and\nVapnik, V.\n2005.\nParallel support vector machines: The\ncascade SVM. In NIPS, 521‚Äì528.\nHinton, G. E., and Salakhutdinov, R. R. 2006. Reducing\nthe dimensionality of data with neural networks. Science\n313(5786):504‚Äì507.\nHinton, G.\n2011.\nDeep belief nets.\nIn Encyclopedia of\nMachine Learning. Springer. 267‚Äì269.\nHornik, K. 1991. Approximation capabilities of multilayer\nfeedforward networks. Neural networks 4(2):251‚Äì257.\nHutchinson, B.; Deng, L.; and Yu, D. 2013. Tensor deep\nstacking networks. IEEE Transactions on Pattern Analysis\nand Machine Intelligence (TPAMI) 35(8):1944‚Äì1957.\nJahrer, M.; T¬®oscher, A.; and Legenstein, R. 2010. Com-\nbining predictions for accurate recommender systems. In\nSIGKDD, 693‚Äì702. ACM.\nKrizhevsky, A.; Sutskever, I.; and Hinton, G. E.\n2012.\nImagenet classiÔ¨Åcation with deep convolutional neural\nnetworks. In NIPS, 1097‚Äì1105.\nLeCun, Y.; Bottou, L.; Bengio, Y.; and Haffner, P. 1998.\nGradient-based learning applied to document recognition.\nProceedings of the IEEE 86(11):2278‚Äì2324.\nLi, J.; Chang, H.; and Yang, J. 2015. Sparse deep stacking\nnetwork for image classiÔ¨Åcation. In AAAI, 3804‚Äì3810.\nMaas, A. L.; Daly, R. E.; Pham, P. T.; Huang, D.; Ng, A. Y.;\nand Potts, C. 2011. Learning word vectors for sentiment\nanalysis. In ACL, 142‚Äì150.\nMedsker, L., and Jain, L. 2001. Recurrent neural networks.\nDesign and Applications.\nPerlich, C., and ¬¥Swirszcz, G. 2011. On cross-validation and\nstacking: Building seemingly predictive models on random\ndata. ACM SIGKDD Explorations Newsletter 12(2):11‚Äì15.\nRooney, N., and Patterson, D. 2007. A weighted combina-\ntion of stacking and dynamic integration. Pattern Recogni-\ntion 40(4):1385‚Äì1388.\nSimonyan, K.; Vedaldi, A.; and Zisserman, A. 2013. Deep\nÔ¨Åsher networks for large-scale image classiÔ¨Åcation. In NIPS,\n163‚Äì171.\nTang, Y. 2013. Deep learning using linear support vector\nmachines. arXiv preprint arXiv:1306.0239.\nTing, K. M., and Witten, I. H.\n1999.\nIssues in stacked\ngeneralization. Journal of ArtiÔ¨Åcial Intelligence Research\n10:271‚Äì289.\nVapnik, V. 1998. Statistical learning theory. 1998. Wiley,\nNew York.\nVincent, P.; Larochelle, H.; Lajoie, I.; Bengio, Y.; and Man-\nzagol, P.-A. 2010. Stacked denoising autoencoders: Learn-\ning useful representations in a deep network with a local de-\nnoising criterion. Journal of Machine Learning Research\n11:3371‚Äì3408.\nWiering, M.; Van der Ree, M.; Embrechts, M.; Stollenga,\nM.; Meijster, A.; Nolte, A.; and Schomaker, L. 2013. The\nneural support vector machine. In BNAIC.\nWolpert, D. H.\n1992.\nStacked generalization.\nNeural\nnetworks 5(2):241‚Äì259.\nZhang, Y.; Liang, P.; and Wainwright, M. J. 2016. Con-\nvexiÔ¨Åed convolutional neural networks.\narXiv preprint\narXiv:1609.01000.\nZhang, T. 2003. Statistical behavior and consistency of clas-\nsiÔ¨Åcation methods based on convex risk minimization. An-\nnals of Statistics 32(1):56‚Äì134.\nZhi-Hua Zhou, J. F. 2017. Deep Forest: Towards an alterna-\ntive to deep neural networks. In IJCAI, 3553‚Äì3559.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-02-15",
  "updated": "2019-02-15"
}