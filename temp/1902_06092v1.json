{
  "id": "http://arxiv.org/abs/1902.06092v1",
  "title": "Exploring Language Similarities with Dimensionality Reduction Technique",
  "authors": [
    "Sangarshanan Veeraraghavan"
  ],
  "abstract": "In recent years several novel models were developed to process natural\nlanguage, development of accurate language translation systems have helped us\novercome geographical barriers and communicate ideas effectively. These models\nare developed mostly for a few languages that are widely used while other\nlanguages are ignored. Most of the languages that are spoken share lexical,\nsyntactic and sematic similarity with several other languages and knowing this\ncan help us leverage the existing model to build more specific and accurate\nmodels that can be used for other languages, so here I have explored the idea\nof representing several known popular languages in a lower dimension such that\ntheir similarities can be visualized using simple 2 dimensional plots. This can\neven help us understand newly discovered languages that may not share its\nvocabulary with any of the existing languages.",
  "text": " \nEXPLORING LANGUAGE SIMILARITIES WITH \nDIMENSIONALITY REDUCTION TECHNIQUES \nSangarshanan Veeraraghavan \nFinal Year Undergraduate \nVIT Vellore  \nv.sangarshanan2015@vit.ac.in \n \n \nAbstract \nIn recent years several novel models were developed to process natural language, \ndevelopment of accurate language translation systems have helped us overcome \ngeographical barriers and communicate ideas effectively. These models are developed \nmostly for a few languages that are widely used while other languages are ignored. Most of \nthe languages that are spoken share lexical, syntactic and sematic similarity with several \nother languages and knowing this can help us leverage the existing model to build more \nspecific and accurate models that can be used for other languages, so here I have explored \nthe idea of representing several known popular languages in a lower dimension such that \ntheir similarities can be visualized using simple 2 dimensional plots. This can even help us \nunderstand newly discovered languages that may not share its vocabulary with any of the \nexisting languages. \n \n1. Introduction \nLanguage is a method of communication and ironically has long remained a \ncommunication barrier. Written representations of all languages look quite different but \ninherently share similarity between them. For example if we show a person with no \nknowledge of English alphabets a text in English and then in Spanish they might be \noblivious to the similarity between them as they look like gibberish to them. There might \nalso be several languages which may seem completely different to even experienced \nlinguists but might share a subtle hidden similarity as they is a possibility that languages \nwith no shared vocabulary might still have some similarity.  \nLexical similarity between languages are fairly easy to determine but it depends purely \non the vocabulary of the language and not how they are structured to form a sentence. \nHence this method might not be applicable to languages with no common words.  \nLanguages have evolved over several years and if we were to establish concrete \nrelationships among them it would become fairly easy to develop language specific \nmodels for lesser known languages which would be much more accurate than generic \nmodels. \n \n \n \n \n \n2. Proposed approach \n \n \n \n \n \n2.1 Text Preprocessing \nSince our corpus is made of different languages, it is quite difficult to use a single type of \npreprocessing method. The difference in preprocessing methods arise due to the \ndifferent writing systems. Languages can be classified based into three types based on \ntheir writing system  \n1. Logographic (Every symbol is a morpheme) \n2. Syllabic (Every symbol is a syllable) \n3. Alphabetic (Every symbol is a Phoneme)  \nWe can separately identify the sentences based on the writing system and remove the \nunnecessary punctuations. Stopwords should be kept intact as sometimes they give us \ncritical information on the language (Since stopwords are language specific) \nFor Tokenization we can use the standard stanford word segmenter or the weiba \nsegmentation algorithm, for Chinese whereas we can use Mecab for Japanese and \nUETsegmenter for Vietnamese. We can use nltk for Arabic and for Latin based languages \nand many Indian languages, words are space separated and hence tokenization is much \neasier  \nThe final goal is split the documents into sentences and then into characters after which \nwe can group them corresponding to their languages. \n \n2.2 Sentence Modelling \nSentence Modelling is a way to represent sentences as vectors. We compute the vector \nrepresentation of each word and find their weighted average using PCA \nCalculating the vector representation of the words in a sentence is called as Word \nembedding. There are several ways to do this and it all depends on the task and the \ndataset. We can use either a pre trained embedding or train our own one. For this task \nas I am going to working with several lesser known languages and these might not have \nan already trained vector representation so it is better to train our own embedding. \nA bag of words model can be achieved using the CountVectorizer() function provided by \nSklearn which can be used to calculate word vectors by fitting it on our data and \ntransforming it into vectors. The encoded vector is returned with a length of the entire \nvocabulary and an integer count for the number of times each word appeared in the \ndocument. \nTF-IDF or Term frequency inverse document frequency vectorizer can be used to \ncalculate the vectors corresponding to the words as it can tokenize the documents and \nlearn the vocabulary and the inverse document frequency weights which would help in \nencoding new documents.  \nWe can use TfidfVectorizer() function provided by Sklearn to calculate the vectors. We \ninitially use the fit() function to learn the vocabulary from our corpus followed by \ntransform to use the knowledge to encode our sentences corresponding to the tfidf \nvalues but the vector that we get is sparse and will contain plenty to zeros given the \nextent of our corpus, this can be handled by calling the toarray() function which converts \nthe sparse vectors into a numpy array.  \nSentence embedding can be a combination of word embedding of all words in a \nsentence. Using bag-of-words or one-hot encoding models for embedding might seem \nsimple and viable as our task does not involve complicated deep neural networks but in \norder to capture the Syntactic (Structure) and Semantic (meaning) relationship between \nthe sentences corresponding to different languages we cannot rely on these naive \nmethods. \nWord2vec is a two layer neural network that can also be used to generate robust word \nvectors and these vectors can reconstruct the linguistic context of the word by \nleveraging a skip gram model   \n \n2.3 Dimensionality Reduction  \nProjecting a higher dimensional data into two dimensions makes it easier to understand \nand visualize. This can be applied to our data so that we would be able to observe the \nrelationship between the languages by reducing the dimensions of the vector that we \nobtained from the previous step.  \nFor this task I chose Uniform Manifold Approximation and projection for dimensionality \nreduction. UMAP uses local manifold approximations and patches together their local \nfuzzy simplicial set representations which would construct a topological representation \nof the given higher dimensional data. It is superior to t-sne in terms of visualization \nquality and also preserves more of the global structure with superior runtime.  \nUMAP has a topological foundation makes it feasible for larger data sets and when we \nare handling scripts from several languages we need significantly higher visualization \nquality that can be provided by UMAP.  \nDimensionality reduction can lead to loss of information that can be represented better \nin higher dimension but with higher dimension it is very difficult to make sense of the \ndata. The two categories in in dimensionality reduction are feature selection and feature \nextraction. Feature selection is used to identify a subset of features that would lead to \nminimal loss in information whereas feature extraction refers to using techniques like \nPrincipal component analysis, linear discriminant analysis and several other methods to \ntransform the higher dimensional data into lower dimension.  \n \n \n \n \n3. Exploring the Languages \nPlotting all the existing language corpuses on a 2D graph will get clumsy and pinpointing \nlanguage similarities would become a tedious process and hence we must be careful in \npicking the languages that we are plotting. We can analyse languages based on their \nfamilies, roots, dialects, origin or several other factors that may influence similarity  \n \nFig-1 Plotting Latin and English along with Chinese and Japanese corpuses (Tfidf vectors) \n \nIn the above graph it is very easy to notice two clusters [‘Latin’,’English’] and \n[‘Chinese’,’japanese’]  \nEnglish vocabulary draws heavily from Latin even though it is a Germanic language and \nhence Latin and English are clustered together whereas Japanese and Chinese are \nclustered together due to their similar writing system called 汉字. It is called Kanji in \nJapanese and Hanzi in Chinese. Chinese characters were imported by japanese a long \ntime ago and these both language share a lot of similarity in the use of characters.  \nSuppose I replace Japanese with Arabic and consider more European languages like \nDutch and Danish to create a more interesting visualization. The Plot now includes \nEnglish, Chinese, Arabic, Dutch and Danish \n \nFig 1.1 TFIDF embedding plot of Chinese-Arabic-English-Dutch-Danish \nDutch Danish and English are present in the lower half of the graph whereas Chinese \nand Arabic occupy the upper region of the graph. English Dutch and Danish come under \nthe class of Germanic languages. Dutch and Danish are not mutually intelligible as Dutch \nis West Germanic and Danish is North Germanic but there is some commonality \nbetween these languages which also coincides with English which happens to be West \nGermanic along with Dutch and hence Dutch and English are closer in the plot along with \nsome intersection between the clusters  \n3.1 Ancient world Languages \nThere are certain languages that are considered to be the mother of several other \nlanguages and has attained antiquity. They can be called as classical languages which are \na group of ancient languages that have given birth to several other languages of the \nsame kind. A classical language should have an independent writing system that evolved \non its own without the help of any other language and also an extremely rich ancient \nliterature to prove it.  \nComparing such ancient languages can give us an idea as to how the other languages \nhave evolved. I have considered Chinese, Greek, Sanskrit, Hebrew, Tamil, Latin and \nArabic and compared them.  \n \n \n \n \n \n \n          Fig-2:   Bag of words embedding  \nA simple bag of words embedding shows us similarity between Chinese, Sanskrit and \nTamil and this might be because all three have Asian roots. Arabic and Hebrew seem to \nshare similarities with these clusters and maybe this is because of the fact that Arabic \nand Hebrew share lexical similarity (about 58.5%), grammatical correspondence, and \nmutual intelligibility. Greek and Latin form separate clusters in the 2D space  \n \n \n                                                Fig3- TFIDF embedding  \n \nEven in this cluster we can see the clear overlap between Hebrew and Arabic. Both these \nlanguages come under the class of Semitic languages. These are a branch of Afro-asiatic \nlanguages that were used in the Middle East. Arabic and Hebrew flourished with the \nhelp of Islamic and Jewish scholars. Again we can see that the rest of the languages are \nclustered separately. An odd detail here is how both Arabic and Hebrew overlap with \nChinese and the presence of Sanskrit near this group of clusters.  \nBoth TFIDF and bag of words embedding can give us an idea of language similarity that \ncan be quite easily detected by human even though it reduces the effort. If we need \nmore of a neural word embedding and so we have to move to a word2vec model that \nleverages a two layer neural network to cluster the vector groups with high similarity \ntogether in the Vectorspace. Word2vec vectors are the distributed numeric \nrepresentations of word vectors and features such as context of individual words. \nWord2vec can achieve this using just the corpus without human intervention \n \n                        Fig 4 – Classical languages with Word2vec embedding \n \nBible is used as the corpus for every language and hence the vector representations of \nsentences tend to be similar and so unlike TFIDF and count embedding word2vec \nclusters all the languages together in the vector space  \n \n \n \n \n                 HEBREW AND ARABIC  \n \n           SANSKRIT AND TAMIL \n   \n \n              CHINESE AND SANSKRIT  \n \n \nHEBREW AND LATIN \n \nThe representation of word2vec depends on the presence of sufficient data, usage and \nthe context to determine the meaning of a word based on its past occurrences.  \nIn the plots we can see that languages with no common roots such as Chinese and \nSanskrit followed by Sanskrit and Tamil are clustered into two even though the \nboundary between those clusters are hard to define. This indicates some similarity due \nto the use of similar corpus but in the case of languages like Hebrew and Arabic the \nboundary between languages doesn’t seem to exist and it appears as one huge cluster. \nThis might be because of their common origin in the Middle East. \n \n \n \n \n \n3.2 Indo Aryan and Dravidian Languages \nThe languages spoken in the Indian Peninsula comprise of Indo Aryan languages that \nare used in the northern parts of the subcontinent and Dravidian languages are used \nin the southern part of the subcontinent. The Dravidian languages have heavily \ninfluences indo Aryan languages and this is evident in the script of Rigveda which \nincludes several borrowed words from the Dravidian vocabulary. Over the years \nthere has been several interactions between Indo Aryan and Dravidian languages.  \nIndo Aryan languages refer to the languages that were spoken by the Aryan people \nwho moved to the Indian subcontinent in prehistoric times. The oldest Indo Aryan \nlanguage is Sanskrit and it was found in the Vedic scriptures which date to 1500 BCE.  \nIndo Aryan languages are spoken in North-India, Pakistan, Bangladesh, Nepal, Sri \nLanka, Myanmar and Maldives whereas Dravidian languages are spoken majorly in \nthe South-India and Sri Lanka but also in countries like Malaysia and Singapore. \n \n \n                     Bag of words embedding \n \n \n \n \n \nDravidian languages like Tamil, Kannada, Telugu and Malayalam are clustered in the \nbottom half of the graph whereas the Indo Aryan languages are clustered on top.  \nOn the bottom half of the graph we can also see a complete overlap between Telugu \nand Kannada as both of them have their writing systems derived from the Kadamba \nscript. The evolution of both Kannada and Telugu were heavily influenced by the \nChalukya dynasty.  \nOn the top half of the graph Farsi is clustered separately whereas Hindi, Nepali and \nMarathi clusters overlap with each other. The three languages clustered together follow \nthe Devanagari script which is one of the most used and adopted writing systems in the \nworld. Farsi has evolved from Arabic and Persian and hence is clustered separately from \nthe languages that evolved from the Devanagari writing system.  \n \n \n                                                        TFIDF Embedding \n \nIn TFIDF embedding Malayalam is clustered near the top. This is because Malayalam \nshares a high similarity with Sanskrit. Malayalam has borrowed several alphabets and \ngrammatical rules from Sanskrit. Hence we can say that Malayalam scripts have heavily \nborrowed from both Dravidian scripts and Indo Aryan scripts.  \n \n \n \n \n \n \n \n \n \nWord2Vec Embedding of the Languages \n \nIn word2vec embedding of the languages we can see a clear of a decision boundary \nwhen plotting Tamil and Farsi. Plotting Telugu, Kannada and Tamil shows significant \noverlap between all three languages. Same is the case for Nepalese and Hindi.  \n \n \n \n \n \n \n \n \n \nFuture Work: \nThese plots can be further extended to newly discovered languages and hieroglyphic \nscripts that were used several thousand years ago. Understanding newly discovered \nlanguages can be a challenge for linguists but is absolutely essential for archaeologists to \nconduct their research. We can also build deep learning model for one language and use \nthe same model for languages with high similarity. Comparing different languages can \nalso give us an idea of the commonly occurring stopwords with respect to every \nlanguage. Hidden similarities between seemingly dissimilar languages can be uncovered \nand deeper analysis can even help us calculate grammatical and sentence based \nsimilarities between languages that have no characters in common  \n \nConclusion: \nUnderstanding similarities between lesser known languages can help build stronger NLP \nmodels and visualizing these languages in two dimensional plots is easy to interpret. \nIdentifying hidden language similarities can be pivotal in understanding how languages \nhave evolved over the years and can help analyse new scripts. \n \nReferences: \n[1] Leland McInnes, John Healy, James Melville “UMAP: Uniform Manifold \nApproximation and Projection for Dimension Reduction” arXiv:1802.03426 , 6 Dec 2018. \n[2]  Leland McInnes  https://github.com/lmcinnes/umap,  2018 \n[3] Jay J. Jiang (University of Waterloo), David W. Conrath (McMaster University)  \n“Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy”                  \narXiv:cmp-lg/9709008 20 Sep 1997. \n[4] van der Maaten, Laurens & Postma, Eric & Herik, H. (2007). Dimensionality \nReduction: A Comparative Review. Journal of Machine Learning Research - JMLR. 10. \n[5] Polyakov, Vladimir & Anisimov, Ivan & Makarova, Elena. (2016). GRAMMAR AND \nSIMILARITY OF LANGUAGES. \n[6] Buomsoo Kim https://github.com/buomsoo-kim/Word-embedding-with-\nPython/blob/master/Sentence%20modeling, Nov 2017 \n[7] Sun Junyi ,Chinese word Segmentation algorithm https://github.com/fxsjy/jieba, \n2018 \n \n \n",
  "categories": [
    "cs.CL"
  ],
  "published": "2019-02-16",
  "updated": "2019-02-16"
}