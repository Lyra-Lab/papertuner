{
  "id": "http://arxiv.org/abs/1902.06320v1",
  "title": "Towards Improved Testing For Deep Learning",
  "authors": [
    "Jasmine Sekhon",
    "Cody Fleming"
  ],
  "abstract": "The growing use of deep neural networks in safety-critical applications makes\nit necessary to carry out adequate testing to detect and correct any incorrect\nbehavior for corner case inputs before they can be actually used. Deep neural\nnetworks lack an explicit control-flow structure, making it impossible to apply\nto them traditional software testing criteria such as code coverage. In this\npaper, we examine existing testing methods for deep neural networks, the\nopportunities for improvement and the need for a fast, scalable, generalizable\nend-to-end testing method. We also propose a coverage criterion for deep neural\nnetworks that tries to capture all possible parts of the deep neural network's\nlogic.",
  "text": "Towards Improved Testing For Deep Learning\nJasmine Sekhon\nUniversity of Virginia\nCharlottesville, VA USA\njs3cn@virginia.edu\nCody Fleming\nUniversity of Virginia\nCharlottesville, VA USA\ncf5eg@virginia.edu\nAbstract—The growing use of deep neural networks in safety-\ncritical applications makes it necessary to carry out adequate test-\ning to detect and correct any incorrect behavior for corner case\ninputs before they can be actually used. Deep neural networks\nlack an explicit control-ﬂow structure, making it impossible to\napply to them traditional software testing criteria such as code\ncoverage. In this paper, we examine existing testing methods for\ndeep neural networks, the opportunities for improvement and\nthe need for a fast, scalable, generalizable end-to-end testing\nmethod. We also propose a coverage criterion for deep neural\nnetworks that tries to capture all possible parts of the deep neural\nnetwork’s logic.\nIndex Terms—deep neural networks, whitebox testing, cover-\nage criterion\nI. INTRODUCTION\nDeep Neural Networks, or DNNs, are increasingly being\nused in diverse applications owing to their ability to match\nor exceed human level performance. The availability of large\ndatasets, fast computing methods and their ability to achieve\ngood performance has paved way for DNNs into safety-critical\navenues such as autonomous car driving, medical diagnosis,\nsecurity, etc. The safety-critical nature of such applications\nmakes it imperative to adequately test these DNNs before\ndeployment. However, unlike traditional software, DNNs do\nnot have a clear control-ﬂow structure. They learn their\ndecision policy through training on a large dataset, adjusting\nparameters gradually using several methods to achieve desired\naccuracy. Consequently, traditional software testing methods\nlike functional coverage, branch coverage, etc. cannot be\napplied to DNNs, thereby challenging their use for safety-\ncritical applications.\nA lot of recent work, discussed in III, has looked into\ndeveloping testing frameworks for DNNs. These methods\nsuffer from certain limitations, as discussed in IV. In our work,\nwe intend to make an effort to overcome these limitations and\nbuild a fast, scalable, efﬁcient, generalizable testing method for\ndeep neural networks. In V, we propose a coverage criterion\nfor feed forward deep neural networks that tries to capture the\nDNN logic to a greater extent by incorporating inter-layer and\nintra-layer relationships.\nII. BACKGROUND\nDeep neural networks are neural networks with multiple\nhidden layers between the input and output layers. Unlike\ntraditional software programs, where the program logic has\nto be manually described by the programmer, deep neural\nnetworks are capable of learning rules by training on a large\ndataset. Today, DNNs are used in easy to complex tasks, such\nas image classiﬁcation [?], medical diagnosis and end-to-end\nFig. 1: The internal logic of a deep neural network is opaque\nto humans, as opposed to the well laid out decision logic of\ntraditional software programs.\nFig. 2: A high-level representation of most existing DNN\ntesting methods.\ndriving in autonomous cars [1]. The safety-critical nature of\nsuch applications makes it important to assure correctness,\nto avoid fatally incorrect behavior and obtain performance\nbeneﬁts from DNNs safely.\nTraditional software testing methods fail when applied to\nDNNs because the code for deep neural networks holds no\ninformation about the internal decision-making logic of a\nDNN, as shown in Figure 1. DNNs learn their rules from\ntraining data and lack the control-ﬂow structure present in\ntraditional software programs. Therefore, traditional coverage\ncriterion like code coverage, branch coverage, functional cov-\nerage, etc. cannot be applied to deep neural networks. A high-\nlevel representation of most existing whitebox testing methods\nfor DNNs is shown in Figure 2. The inputs to the testing\nprocess are the DNN, the test inputs, and a coverage metric\nto ensure that all parts of the program logic have been tested.\nAn oracle decides whether the behavior of the DNN is correct\nfor the tested inputs. Further, a guided test input generation\nmethod may be used to generate test inputs that have greater\ncoverage and which uncover greater corner case behavior.\nThe output of existing testing methods is usually either a\nmeasure of system correctness or adversarial ratio.\narXiv:1902.06320v1  [cs.SE]  17 Feb 2019\nIII. PRIOR LITERATURE\nTesting methods for deep neural networks have normally\nfollowed a black-box approach, until recently, when Deep-\nXplore [7] proposed the ﬁrst white-box testing method for\nDNNs. The method proposed by [6] involves randomly search-\ning around a given input for changes that cause misclassiﬁ-\ncation. Many other approaches involve generating adversarial\nexamples by perturbing an input slightly to induce incorrect\nbehavior, which is checked for manually. However, these\nblack-box approaches are completely unguided in terms of\nthe absence of a coverage criterion and overlook the internal\nlogic of a DNN. In DeepXplore [7], the authors introduced the\nconcept of neuron coverage as a coverage metric for testing\nDNNs. They also proposed using multiple implementations\nfor the same task as an oracle to avoid manual labeling\neffort. Further, DeepCover [8] proposes several criteria for\ntesting DNNs, inspired by modiﬁed code/decision coverage\nfor software testing. Their coverage criteria take into account\nthe condition-decision dependence between neurons of con-\nsecutive layers. Another recent approach, DeepMutation [4] is\nthe ﬁrst source-level mutation testing technique that proposes\na set of model-level mutation testing operators that directly\nmutate on deep learning models without a training process.\nDeepCT [5] uses a combinatorial-testing inspired coverage\ncriterion which guides an exhaustive search for test inputs that\nactivate neurons in a layer-wise manner.\nIV. OPPORTUNITIES FOR IMPROVEMENT\nA. Why do we need a better coverage criteria?\nCoverage criteria for traditional software programs, such as\ncode coverage and branch coverage check that all parts of the\nlogic in the program have been tested by at least one test input\nand all conditions have been tested to independently affect the\nentailing decisions. On similar lines, any coverage criterion for\ndeep neural networks must be able to guarantee completeness,\nthat is, it must be able to ensure that all parts of the internal\ndecision-making structure of the DNN have been exercised by\nat least one test input.\nA typical feed-forward deep neural network contains mul-\ntiple nonlinear processing layers with each hidden layer using\nthe output of the previous hidden layer as its input. Each\nlayer consists of multiple neurons. A neuron is a computing\nunit, loosely patterned on the neurons in the human brain,\nwhich ﬁres/activates when it receives sufﬁcient stimuli or\ninput. Mathematically, if Lk−1 and Lk denote two consecutive\nlayers of this DNN (Figure 1):\nni,k = φk\n\u0010\nδi,k +\nX\n1≤j≤Nk−1\n(wj,i·nj,k−1)\n\u0011\n(1)\nwhere:\n• ni,k denotes the value of the ith neuron of kth layer,\n• φk denotes the activation function of the kth layer,\n• δi,k denotes the bias for node ni,k,\n• Nk−1 denotes the number of nodes/neurons of layer Lk−1,\nand\n• wj,i denotes the weight of the connection between the jth\nneuron of layer Lk−1 and ith neuron of layer Lk\nTherefore, along with the value of each neuron having an\nindependent effect on the activation of neurons in the next\nhidden layer, the combinations of values of neurons in the\nsame layer also affect the value of neurons in the next layer.\nAny coverage criterion for deep neural networks must be\nable to capture both of these factors. Further, the coverage\ncriterion should be scalable to larger-sized real-world DNNs\nand different network architectures. The coverage criteria\nproposed by previous works suffer from several limitations:\n• Neuron coverage [7] measures the parts of the DNN’s logic\nexercised by the test inputs based on the number of neurons\nactivated by the input. However, it is not able to thoroughly\naccount for all the possible behaviors that a DNN could\nexhibit. Experiments by [8] were able to prove that neuron\ncoverage is fairly easy to achieve and 25 random test inputs\nare able to achieve close to 100% neuron coverage. Further,\nwe observed that corner case behavior can be found beyond\n100% neuron coverage. Our experiments1 found that in cer-\ntain cases of model architecture, for instance LeNet-1 used\nfor MNIST handwritten digit classiﬁcation, 100% neuron\ncoverage can be obtained with two test inputs, because\nfor most test inputs, the neurons are always ﬁred/activated.\nTherefore, neuron coverage is a fairly coarse and insufﬁcient\ncriterion for coverage in DNNs.\n• DeepCover’s [8] coverage criteria take into consideration the\ncondition-decision dependence in adjacent layers of a DNN.\nApart from their method being tested on relatively small\nnetworks, it assumes the DNN to be a feedforward, fully-\nconnected network and cannot generalize to architectures\nlike RNNs, LSTMs, attention networks, etc. Such methods\ndo not consider the context of a neuron in its own layer,\nand the combinations of neuron outputs in the same layer.\n• DeepCT’s [5] combinatorial testing inspired coverage crite-\nrion determines the fraction of logic exercised by a test input\nin terms of the fraction of neurons activated in each layer. It\ndoes not consider the inter-layer relationships within a DNN,\nand has not been veriﬁed to scale to real-world DNNs with\ndifferent kinds of layers.\nB. Why do we need better test input generation?\nGenerating or selecting test inputs in a guided manner\nusually has two major goals - maximizing the number of un-\ncovered faults, and maximizing the coverage. [7] introduces a\njoint optimization based test input generation method, in which\nan existing test input is modiﬁed (using image manipulations)\nrecursively until a test input causing differential behavior is\nfound. [9] uses a similar greedy search technique in which\nrandom transformations are applied until an appropriate test\ninput is found. Such test input generation methods suffer from\nsome major drawbacks:\n1All results for neuron coverage were obtained by running the DeepXplore\ncode https://github.com/peikexin9/deepxplore/tree/master/MNIST for image\nmanipulation=light and best-performing parameters: λ1=1, λ2=0.1, steps=10,\ngrad iterations=1000, threshold=0\n• The iterative process of manipulating an existing test input\nuntil a test input that satisﬁes the criterion is found, has\nconsiderable time per execution.\n• The number of test inputs that actually cause an increase\nin coverage and/or an increase in the number of uncovered\ncorner case behaviors are fairly low in comparison to the\nsum of total number of tested and generated inputs.\nC. Why do we need a better oracle?\nTesting for the correctness of a DNN requires the presence\nof ground truth (oracle), that decides if the behavior is correct.\nThe existing oracles for testing DNNs suffer from several\nlimitations:\n• The most straightforward way in data-driven schemes like\nDNNs is by collecting as much real-world data as possible\nand manually labeling it to check for correctness. However,\nsuch a process requires a lot of manual effort.\n• In multiple DNN implementations [7] as an oracle, multiple\nimplementations for the same task are compared, and differ-\nential behavior is labeled as a corner case behavior. How-\never, we observed that this method erroneously classiﬁes\ncertain corner case inputs as correct behaviors because the\nlabels predicted by all the implementations are similar and\nmisclassiﬁes several correct inputs as corner-case behaviors.\nAlso, this method is only valid in applications that have sev-\neral existing high-accuracy implementations. Often, DNNs\nmay be deployed in tasks that do not have many existing\nimplementations and/or implementations may be crafted by\nthe same set of experts that are bound to have used the same\nmethods or made the same errors.\nV. PRELIMINARY APPROACH AND RESULTS\nIn this paper, we focus only on proposing a ﬁner coverage\ncriterion. An ideal coverage criterion for deep neural networks\nmust be able to guarantee completeness, i.e., all parts of the\ninternal decision logic of the DNN have been tested by at least\none input. Recall that the value of a neuron in a particular\nlayer in a DNN is computed as a nonlinear function of the\nweighted sum of neurons in the previous layer, as shown in\nEquation 1. On these lines, we propose a coverage criterion\nthat incorporates both factors- the conditional effect of each\nneuron on the value of neurons in the next layer and the\ncombinations of values of neurons in a layer [3].\nFor two consecutive layers, Lk−1 and Lk in a given\n(feed forward) deep neural network, let the neurons in these\nlayers be denoted by {n1,k−1, n2,k−1, ..., nNk−1,k−1} and\n{n1,k, n2,k, ..., nNk,k} respectively, where Nk denotes the total\nnumber of neurons in Lk. For any test input t, a neuron n\nis said to be activated if its value is greater than a certain\nthreshold, for example, 0. Formally, if φ(t, n) denotes the\nactivation of neuron n when the input to the deep neural\nnetwork is t, then if φ(t, n) >0 (or any other threshold value,\ndepending on activation function) then the neuron is said to\nbe activated or ﬁred. Therefore, for a given neuron n and test\ninput t, the condition φ(t, n) > 0 can have two values, true\nor false, depending on whether the neuron is activated or not.\nBased on these deﬁnitions, our coverage criterion is deﬁned\nas the 2-way coverage [3] for every such triplet in the DNN:\n(ni,k−1, nj,k−1, nq,k). Formally, for a given test set for n\nvariables, simple t-way combination coverage is the proportion\nof t-way combinations of n variables for which all variable-\nvalues conﬁgurations are fully covered. By ensuring 2-way\ncoverage on three such distinct neurons, we are able to cover,\n(1) the independent effect a condition (activation of neuron in\nLk−1) has on an outcome (value of neuron in the next layer,\nLk), (2) the failures that may arise because of the ‘interaction’\nor activation values of neurons in the same layer Lk−1. While\nthe ﬁrst coverage is more inspired by MC/DC [2] and other\ntraditional software-coverage criteria, the second coverage is\ninspired by combinatorial testing [3]. This kind of testing is\nbased on the fact that not every parameter contributes to every\nfailure, and empirical data suggest that nearly all failures are\ncaused by interactions between relatively fewer parameters.\nThis ﬁnding has important implications for testing because it\nsuggests that testing combinations of (fewer) parameters can\nprovide highly effective fault detection. In our scenario, since\nvalues of multiple (but not always all) neurons in the previous\nlayer contribute towards the values of neurons in the next\nlayer, such a method is able to test for multiple values that\na condition (weighted sum in Equation 1) can take, which is\nalso one of the requirements for traditional software coverage\ncriteria such as MC/DC [2].\nFor preliminary results, we approach guided test input\ngeneration via joint optimization [7]. Any triplet not having\nachieved 100% coverage is randomly chosen to determine\nwhich combination(s) of activation values has not been cov-\nered. Consider, for example, the DNN instance where ni,k-1\nis ﬁred but nj,k-1 is not activated. The decision neuron nq,k is\nﬁred. The objective becomes\nFn,t = fni,k−1(t) + fnj,k−1(t) + fnq,k(t),\n(2)\nwhere\n• fni,k−1(t) = φ(t, ni,k−1) needs to be maximized such that\nφ(t, ni,k−1) > 0 (or the decided threshold),\n• fnj,k−1(t) = φ(t, nj,k−1) needs to be minimized such that\nφ(t, nj,k−1) = 0, and\n• fnq,k(t) = φ(t, nq,k) needs to be maximized such that\nφ(t, nq,1) > 0.\nThe objective function to maximize coverage therefore be-\ncomes,\nFn,t = φ(t, ni,k−1) −φ(t, nj,k−1) + φ(t, nq,k).\n(3)\nBecause the individual terms in Fn,t are activation values\nof certain neurons in certain layers and φ(t, n) for any n is\na sequence of stacked functions, the gradient ∂F n(t)\n∂t\ncan be\ncalculated using the chain rule in calculus, i.e., by computing\nlayer-wise derivatives backwards from the layer containing\nneuron n until reaching the input layer which takes input t\n[7].Hence, the input t can be manipulated in steps to maximize\nFn,t. The oracle we use is the same as [7], so the second\nobjective is to generate differential behavior causing inputs.\nMetric\nResult\nCoverage for 10 random inputs\n8.9%\nGuided coverage for 550 test inputs\n31%\nNumber of corner case behaviors found for 550 inputs tested\n483\nAdversarial Ratio\n87.8%\nTABLE I: Evaluation of coverage metric on LeNet archi-\ntectures for MNIST dataset. All results are an average over\nLeNet-1, LeNet-4, LeNet-5.\nWe evaluated our coverage metric on three DNNs that\nclassify the MNIST dataset of handwritten digits: LeNet-1,\nLeNet-4 and LeNet-5. Since the primary goal of our work is\nto introduce and test a more ﬁne-grained coverage metric, our\ntest input generation method and oracle share the limitations\nmentioned in section IV. The metrics used for determining the\nvalidity of our coverage criterion were:\n• The coverage obtained on ten random test inputs, and\n• The ratio of number of corner cases found to the number of\ntotal test inputs.\nIdeally, the coverage for ten random test inputs (not generated\nusing a guided method) must be low, i.e., the criterion must\nbe difﬁcult to achieve for random inputs, and the adversarial\nratio must be high. We currently use multiple implementations\n[7] as an oracle, introduced in IV, and only one image\nmanipulation, brightness. The results are summarized in Table\nI2. We then compared our proposed coverage criterion with\nexisting coverage metrics. We found that for the same dataset\nand DNNs, the average neuron coverage [7] for ten random\ntest inputs over the three DNNs is 30.5% (threshold used is\n0), as opposed to 8.9% for our coverage criterion. Further,\nfor LeNet-1, 100% neuron coverage can be achieved with just\ntwo corner-case inputs and a lot of corner-case inputs can\nbe found beyond achieving 100% neuron coverage. On the\nother hand, LeNet-1 achieves close to 11.6% coverage for\nour criterion over 550 test inputs. This is because the most\ncommon activation pattern in the DNN for the given test inputs\nis all neurons being ﬁred/activated, and hence 2-way coverage\nis difﬁcult to achieve. The average neuron coverage across all\nthree DNNs using guided test input generation is 98.5% for\n550 test inputs, but is 31% for our proposed coverage criterion\nusing the same test input generation method. The maximum\nadversarial ratio obtained using DeepCover [8] for DNNs of\nsimilar size is 11%. Similarly, DeepCT [5] achieves less than\n10% adversarial ratio for a DNN of similar size, for 10,000\ntest inputs. These results conﬁrm that for testing DNNs, it is\nimportant to have a more ﬁne-grained coverage metric that not\nonly incorporates inter-layer relationships, but also the relative\nactivations of neurons in the same layer. While the large\nnumber of triplets may seem like a computational bottleneck,\nthe average time taken to update coverage for LeNet-5 with\nthe most number of triplets (651720) is 2.08 seconds.\n2Our implementation involves trying to optimize for image manipulations\nto generate differentially behaving, more coverage test inputs from all inputs,\nwhether or not they cause differential behavior when not manipulated at all.\nVI. CONCLUSION\nThe absence of a transparent decision logic makes it\nimpossible to apply traditional software testing methods to\nDNNs. This paper examines existing testing methods for deep\nneural networks and recognizes several limitations such as\ncoarse coverage criteria, open ended processes, unreliable\noracles, inefﬁcient test input generation methods, inability to\nscale to larger DNNs and different network architectures, etc.\nFurther, we propose a ﬁne-grained coverage criterion for feed\nforward DNNs that takes into account the condition-decision\nrelationships between adjacent layers and the combinations\nof values of neurons in the same layer. A set of ten random\ntest inputs could only achieve 8.9% of our coverage criterion.\nFurther, when coupled with gradient-based search techniques\nand multiple implementations oracle, it is able to achieve an\naverage 87.8% adversarial ratio over three models. The ability\nto test the internal logic of a DNN to a greater extent makes\nits performance better than existing methods. The scalability\nof the coverage method to larger-sized real-world DNNs and\nits adaptation to different network architectures is yet to be\ntested.\nVII. ACKNOWLEDGEMENTS\nThis material is based upon work supported in part by the\nNational Science Foundation under Grant No. CNS: 1650512,\nconducted in the NSF UICRC Center of Visual and Decision\nDynamics. This research was also supported by the Northrop\nGrumman Mission Systems University Research Program.\nREFERENCES\n[1] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard\nFirner, Beat Flepp, Prasoon Goyal, Lawrence D. Jackel, Mathew Monfort,\nUrs Muller, Jiakai Zhang, Xin Zhang, Jake Zhao, and Karol Zieba. End\nto End Learning for Self-Driving Cars. 2016.\n[2] Kelly J Hayhurst, John J Chilenski, and Leanna K Rierson. A Practical\nTutorial Decision Coverage on Modiﬁed Condition.\nTechnical report,\n2001.\n[3] D Richard Kuhn, Raghu N Kacker, and Yu Lei. ”Combinatorial Testing”.\nTechnical report.\n[4] Lei Ma, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Felix Juefei-\nXu, Chao Xie, Li Li, Yang Liu, Jianjun Zhao, and Yadong Wang.\nDeepMutation: Mutation Testing of Deep Learning Systems. 2018.\n[5] Lei Ma, Fuyuan Zhang, Minhui Xue, Bo Li, Yang Liu, Jianjun Zhao, and\nYadong Wang. Combinatorial Testing for Deep Learning Systems. 2018.\n[6] Nina Narodytska and Shiva Prasad Kasiviswanathan. Simple Black-Box\nAdversarial Perturbations for Deep Networks. 2016.\n[7] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. DeepXplore:\nAutomated Whitebox Testing of Deep Learning Systems. 2017.\n[8] Youcheng Sun, Xiaowei Huang, and Daniel Kroening.\nTesting Deep\nNeural Networks. 2018.\n[9] Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray.\nDeepTest:\nAutomated Testing of Deep-Neural-Network-driven Autonomous Cars.\n2017.\n",
  "categories": [
    "cs.SE",
    "cs.LG"
  ],
  "published": "2019-02-17",
  "updated": "2019-02-17"
}