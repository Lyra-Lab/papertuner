{
  "id": "http://arxiv.org/abs/1902.07068v1",
  "title": "Classifying textual data: shallow, deep and ensemble methods",
  "authors": [
    "Laura Anderlucci",
    "Lucia Guastadisegni",
    "Cinzia Viroli"
  ],
  "abstract": "This paper focuses on a comparative evaluation of the most common and modern\nmethods for text classification, including the recent deep learning strategies\nand ensemble methods. The study is motivated by a challenging real data\nproblem, characterized by high-dimensional and extremely sparse data, deriving\nfrom incoming calls to the customer care of an Italian phone company. We will\nshow that deep learning outperforms many classical (shallow) strategies but the\ncombination of shallow and deep learning methods in a unique ensemble\nclassifier may improve the robustness and the accuracy of \"single\"\nclassification methods.",
  "text": "arXiv:1902.07068v1  [cs.CL]  18 Feb 2019\nClassifying textual data: shallow, deep and\nensemble methods\nLaura Anderlucci, Lucia Guastadisegni, Cinzia Viroli\nDepartment of Statistical Sciences, University of Bologna, Italy\nFebruary 20, 2019\nAbstract\nThis paper focuses on a comparative evaluation of the most common\nand modern methods for text classiﬁcation, including the recent deep\nlearning strategies and ensemble methods.\nThe study is motivated by\na challenging real data problem, characterized by high-dimensional and\nextremely sparse data, deriving from incoming calls to the customer care of\nan Italian phone company. We will show that deep learning outperforms\nmany classical (shallow) strategies but the combination of shallow and\ndeep learning methods in a unique ensemble classiﬁer may improve the\nrobustness and the accuracy of “single” classiﬁcation methods.\nKeywords: Text classiﬁcation, Deep Learning, Ensemble methods\n1\n1\nIntroduction\nNowadays the increasing and rapid progress of technology and the availability\nof electronic documents from a variety of sources have made a huge amount\nof textual data available. Hence, one of the prominent research topics of sta-\ntistical and machine learning communities is to provide suitable and feasible\nmethods to extract high-quality information from unstructured textual data\n(Lata and Loar, 2018) for the diﬀerent purposes of clustering, classiﬁcation and\ndocument retrieval (Khan et al., 2010).\nThis work originates from an empirical problem of classiﬁcation of the con-\ntent of calls made to the customer service of an important mobile phone com-\npany in Italy. The received calls are written down by an operator and classiﬁed\ninto relevant classes (e.g. claims or request of information for speciﬁc services,\ndeals or promotions). The aim is to provide a strategy to automatically assign\nnew tickets into the pre-deﬁned classes, so as to speed up and to improve the\nassistance service.\nIn this work we compare and discuss the most common and modern classi-\nﬁcation methods for textual data. We conduct our experiments on the ticket\ndata previously described that have the peculiarity to be very short and sparse\n(after a pre-processing step, the tickets have indeed an average length of only 5\nwords and, thus, the document-term matrix contains zero almost everywhere).\nIn the scientiﬁc literature there are many studies aimed at reviewing and\ncomparing the diﬀerent strategies for text classiﬁcation (see, among the others,\nKorde and Mahender (2012); Patra and Singh (2013); Mironczuk and Protasiewicz\n(2018)). Most of these reviews focus on classical methods, called ‘shallow’ strate-\ngies in contrast to the recent deep learning strategies. Deep learning methods are\nreceiving an exponentially increasing interest in the last years as powerful tools\nto learn complex representations of data. They are basically multi-layer archi-\ntectures composed by nonlinear transformations able to predict data with high\nprecision (LeCun et al., 2015). In particular, deep neural networks (i.e. deep\nFeed Forward, Recurrent, Auto-encoder, Convolution neural networks) have\ndemonstrated to be particularly successful in supervised classiﬁcation problems\narising in several ﬁelds including textual data analysis (Schmidhuber, 2015).\nIn this work a comprehensive study comparing deep learning strategies with\nclassical classiﬁcation methods is conducted.\nWe also evaluate an ensemble\nstrategy based on voting of base methods (Mironczuk and Protasiewicz, 2018;\nOnan et al., 2016).\nTo the best of our knowledge, this is the ﬁrst empirical\nanalysis which evaluates the eﬀectiveness of ensemble learning algorithms based\non shallow and deep learning methods on very short textual data.\nThe paper is organized as follows. Section 2 presents an overview of the\nconventional classiﬁcation methods, the recent deep learning strategies and the\nensemble classiﬁers. In Section 3 the real data problem motivating this work\nis illustrated. Section 4 describes the results of the analysis. We conclude this\nwork with some ﬁnal remarks in Section 5.\n2\n2\nClassiﬁcation methods\nIn this section a description of supervised classiﬁcation methods, from the classi-\ncal classiﬁcation methods to the more recent deep learning and ensemble strate-\ngies is given.\n2.1\nClassical (shallow) methods\nNa¨ıve-Bayes\nThe Na¨ıve-Bayes (NB) classiﬁer is a well-known probabilistic\nmethod (John and Langley, 1995; Hand and Yu, 2001).\nThe model assumes\nthe conditional independence among variables. More precisely, given k classes\nand a document-term matrix of dimension n × p, the probability that a generic\ndocument d (d = 1, . . . , n) belongs to the class i (i = 1, . . . , k) is calculated as:\nP(i|d) ∝P(i)\np\nY\nj=1\nP(tj|i)\nwhere P(tj|i) is the probability that the word tj (j = 1, . . . , p) belongs to a\ndocument of class i, and P(i) is the prior probability that a text document\nbelongs to the class i.\nThe document d is ﬁnally assigned to the class i that has the maximum pos-\nterior probability (Sch¨utze et al., 2008). Depending on the probabilistic choices\nfor P(i) (uniform or weighted) and P(tj|i) (i.e. Bernoulli, Multinomial etc.)\nseveral models can be deﬁned.\nThanks to its simplicity, the Na¨ıve-Bayes classiﬁer is computationally straight-\nforward and it can be successfully trained with a small amount of data and\nhigh-dimensional complex data (Kumbhar and Mali, 2016). Even if in practice\nthe independence assumption may be unrealistic, the Na¨ıve-Bayes classiﬁer per-\nforms very well in classifying textual data, where terms are not usually highly\ncorrelated.\nk-Nearest Neighbor\nThe k-Nearest Neighbor (k-NN) is one of the sim-\nplest non-parametric method used in classiﬁcation (Kumbhar and Mali, 2016;\nCover and Hart, 1967). In k-NN with k = 1 the distance between a new doc-\nument and the set of documents of the training set is computed, then the new\ndocument is assigned to the class of the nearest document. In the general k-NN\ncase, the document is assigned to the modal class among its k nearest neigh-\nbors. The optimal value of k and the best distance to adopt (e.g. Euclidean,\nManhattan, Canberra etc.) can be selected by cross-validation. For the anal-\nysis of textual data a prominent measure of distance is based on the cosine\nsimilarity, as it considers the non-zero elements of the vectors only, allowing\nto measure the dissimilarity between two documents in terms of their subject\nmatter. Given two p-dimensional documents, say x and y, the cosine distance\nof the two corresponding frequency vectors is:\n3\nd(x, y) = 1 −\nPp\nj=1 xjyj\nqPp\nj=1 x2\nj\nqPp\nj=1 y2\nj\n,\n(1)\nwhere xj and yj denote the frequency of word j in document x and y,\nrespectively.\nThis measure is not aﬀected by the amount of zeros and is a\nnormalized synthesis of the p-variate terms of the documents. Since the elements\nof x and y are positive or null frequencies, it is easy to prove that the distance\nranges between 0 and 1.\nOne of the main disadvantage of k-Nearest Neighbor is its computational\ncost that can be very intensive when the number of features/terms is high, or\nin case of very large training set.\nComponent-wise classiﬁer\nThe component-wise classiﬁers consider the dis-\ntance of each document from a reference value of the classes, separately for each\nvariable.\nThen the variable-wise distances are simply aggregated across the\nvariables and a new document is assigned to the class from which it has the\nminimum overall dissimilarity.\nWithin this family of methods there are the\nCentroid Classiﬁer (CC), the Nearest Shrunken Centroid classiﬁer (NSC), the\nMedian Classiﬁer (MC) and the Quantile Classiﬁer (QC).\nCentroid-based classiﬁer (Tibshirani et al., 2002) considers the similarity of\na text document to the centroid of each class, where the similarity measure is\ncalculated as the ones’ complement of the Euclidean distance and the centroid\nis the vector of the average frequencies of each term across all documents of a\nspeciﬁc class. The new document is assigned to the class with the most similar\ncentroid.\nWhen applied to text classiﬁcation with TF-IDF transformed fre-\nquencies, the centroid classiﬁer is known as the Rocchio classiﬁer (Harish et al.,\n2010).\nThe Nearest Shrunken Centroid Classiﬁer (Tibshirani et al., 2003) represents\nan extension of the centroid classiﬁer, which corrects the centroid of each class\nby ‘pushing’ it to the general centroid of all the text documents. The NSC\nclassiﬁer coincides with the Centroid classiﬁer when the ‘shrinkage’ parameter\n(threshold) is equal to 0.\nThe Median Classiﬁer is a recent method proposed in order to extend the cen-\ntroid classiﬁer to problems with heavy tails (J¨ornsten, 2004; Ghosh and Chaudhuri,\n2005). Hall et al. (2009) introduced a component-wise median-based classiﬁer\nwhich behaves well in high-dimensional data.\nA new observed vector is as-\nsigned to the class having the smallest L1-distance from the class conditional\ncomponent-wise median vectors of the training set. Instead of considering the\ndistance from the ‘core’ of a distribution (mean or median) as the major source\nof the discriminatory information, Hennig and Viroli (2016) proposed to use\ngeneric quantiles depending on percentiles in [0,1].\nThe so-called Quantile\nClassiﬁer is thus deﬁned. The value of the percentile can be chosen by cross-\nvalidation.\nThe method proved to outperform the centroid and the median\n4\nclassiﬁer in high-dimensional and skew data. When the percentile is 0.5 the\nQuantile classiﬁer coincides with the Median classiﬁer.\nLinear Discriminant Analysis\nThe Linear Discriminant Analysis (LDA)\nis one of the most popular and classical statistical method for classiﬁcation.\nProposed by R.A. Fisher in 1936 (Fisher, 1936), the idea is to ﬁnd the linear\ncombination of the variables that better separates the k classes. In linear dis-\ncriminant analysis the text documents are projected onto a lower dimensional\nspace, where the classes are well separated and only features that carry infor-\nmation useful for classiﬁcation are extracted (Torkkola, 2001). LDA can suﬀer\nfrom sparsity in the data: the presence of many zeros may compromise the\nclassiﬁcation performance as the method is based on linear combinations.\nSupport Vector Machines\nSupport Vector Machines (SVM) are supervised\nlearning tools developed in the context of machine learning based on kernel\nmethods (Cortes and Vapnik, 1995; Wang et al., 2008). The SVM algorithm\nﬁnds the linear or nonlinear hyper plane that separates two classes (positive and\nnegative training set) (Harish et al., 2010). The optimal hyper plane is the one\nthat gives the maximal margin between the documents of the two classes. The\nsupport vector includes the documents that are closest to the decision surface.\nDiﬀerent kernel functions can be speciﬁed: from the simple linear to nonlin-\near alternatives, such as polynomial or sigmoid functions (Kumbhar and Mali,\n2016).\nSupport Vector Machine is accurate also in presence of a high num-\nber of features, but can be very time-consuming in training and classifying the\ndocuments (Khan et al., 2010).\nDecision Trees and Random Forests\nDecision trees are nonparametric ap-\nproaches that allow to represent a certain number of classiﬁcation rules referred\nto single variables taken one at a time with a tree structure (Breiman et al.,\n1984). A tree structure is composed by diﬀerent elements: internal nodes rep-\nresent the features, each ramiﬁcation of the structure represents the best split\nobtainable for a given feature taken as reference (according to a certain criterion)\nand leaves represent the classes. The number of branches grows exponentially\nwith the number of features considered. For this reason such method can suf-\nfer from an excessive number of features. The main advantage of decision tree\nis its ease of interpretability, even for non-professionals. Random forests are\nan ensemble method which is based on the idea of constructing a multitude\nof classiﬁcation trees at the training phase: the predicted class is the mode of\nthe classes of the individual trees (Breiman, 2001). Being an extension of the\ndecision trees, they can be computationally very high-demanding.\n2.2\nDeep learning methods\nDeep learning is a prominent research topic in machine learning and pattern\nrecognition. The exponential increasing popularity of the deep learning methods\n5\nFigure 1: Number of (relative) times the word ‘Deep Learning’ was searched by\nGoogle along the years\n(see Figure 1) derives from the fact that they have demonstrated to be powerful\nand ﬂexible tools to learn complex representations of data in several ﬁelds of\nresearch. Deep structures are a multi-layer stack of algorithms or modules able\nto gradually learn a huge number of parameters in an architecture composed\nby multiple nonlinear transformations (LeCun et al., 2015). Typically, and for\nhistorical reasons, a structure for deep learning is identiﬁed with advanced neural\nnetworks (Schmidhuber, 2015).\nNeural networks\nArtiﬁcial neural networks (ANNs) were developed to mimic\nthe human brain in arriving at a decision. The classical structure of an ANN\nis an architecture of nonlinearly connected processors called neurons (Lai et al.,\n2015). Neural networks are usually trained with back-propagation, where the\nparameters of the networks change until an accuracy measure is maximized.\nThe simplest neural networks are composed by only two layers: an input layer\nrepresenting the observed features and an output layer that gives the predicted\nclasses or values.\nBy adding layers to this hierarchy Deep Neural Networks\n(DNN) are deﬁned, and they include many relevant models: deep Feed Forward,\nRecurrent, Auto-encoder, Convolution neural networks are some eﬀective and\nvery used algorithms. Deep Neural Networks perform very well for classiﬁcation,\neven in presence of high-dimensional and noisy features (Khan et al., 2010).\n2.3\nEnsemble methods\nEnsemble strategies are procedures that combine multiple classiﬁers by consider-\ning them as a ‘committee’ of decision makers. Each classiﬁer is a voting member\nand the committee produces a ﬁnal prediction based on their votes, that could be\n6\nweighted or unweighed. Ensemble methods can adopt diﬀerent strategies. Bag-\nging, boosting, AdaBoost, stacked generalisation, mixtures of experts, and vot-\ning based methods are some relevant approaches (Mironczuk and Protasiewicz,\n2018). Ensembles are commonly applied to minimize speciﬁc drawbacks, such\nas overﬁtting and the curse of dimensionality, and they usually achieve better\nresults than those from single classiﬁers (Wang et al., 2014; Dietterich, 2000).\nEnsemble strategies have been successfully applied in textual data analysis. Re-\ncently, Onan et al. (2016) presented a comprehensive study of comparing base\nlearning algorithms with some widely utilized ensemble methods; Lochter et al.\n(2016) proposed an ensemble strategy to combine the state-of-the-art text pro-\ncessing approaches, as text normalization and semantic indexing techniques,\nwith traditional classiﬁcation methods to automatically detect opinion in short\ntext messages.\n3\nTicket data\nThe dataset contains the text of n = 2129 received calls (tickets). The tick-\nets have then been classiﬁed by independent operators to k = 5 main classes\ndescribed in Table 1.\nTable 1: Number of tickets for each class.\nClass\nDescription\nFrequencies\nACT\nActivation of SIM, ADSL, new contracts\n407\nBAL\nGeneral information about current balance, consumption, etc.\n471\nOFF\nRequest of information about new oﬀers and promotions\n376\nTOP\nTop-up\n435\nISS\nIssues with password, top-up, internet connection, etc.\n440\nRaw data were processed via lemmatization (it removes inﬂectional endings\nof a word and returns its canonic form, which is known as the lemma) and stem-\nming (it reduces inﬂected or derived words to their unique root, thus removing\nits derivational aﬃxes) (Sch¨utze et al., 2008). Then, some terms have been ﬁl-\ntered out in order to remove very common non-informative words and articles\nin the Italian language (stopwords). The stopwords can be viewed as noise since\nthey do not vary signiﬁcantly among classes (Patra and Singh, 2013). More-\nover, the terms that appear only once in the whole observed dataset have been\nremoved since considered too rare to be signiﬁcantly useful for classiﬁcation.\nAfter pre-processing, data are represented as Bag-of-words (Harish et al.,\n2010) in a ﬁnal document-term matrix of dimension n × p with p = 489 and\nn = 2129. Each cell contains the frequency of a term in a text document. The\npeculiarity and major challenge of this dataset is the limited number of words\nused, on average, for each ticket. In fact, after pre-processing, the documents\nhave an average length of 5 words. As previously observed, this determines a\nsparse matrix with many zeros, and this could aﬀect the classiﬁcation perfor-\nmance of many methods.\n7\nIn a cross-validation scheme we evaluated alternative weighting schemes\n(Sch¨utze et al., 2008) to the observed frequencies such as the absolute frequen-\ncies adjusted according to the TF-IDF transformation, or with the average fre-\nquency of the terms in the documents that contain them, the relative frequencies,\nand the relative TF-IDF transformation. For most classiﬁcation methods the\nalternative weighting schemes do not oﬀer an advantage on the classiﬁcation\nwith respect to the observed frequencies.\n4\nExperimental results on Ticket document col-\nlection\nThe classiﬁcation methods presented in Section 2 have been implemented in a 50-\nfold cross-validation study (Kohavi et al., 1995). At each step, each algorithm\nis evaluated in terms of the accuracy rate, i.e. the number of correctly classiﬁed\ndocuments over the total number of documents.\nNa¨ıve-Bayes\nThe method has been implemented with the R package quanteda.\nDiﬀerent settings have been considered: multinomial or Bernoulli conditional\ndistribution, uniform prior distribution or priors proportional to the class sizes,\nmodel with or without smoothing parameter (λ equal to 1 or 0). Table 2 con-\ntains the accuracy for the diﬀerent settings.\nClassiﬁcation performances are\ngenerally good with the exception of the Bernoulli probabilistic model without\nsmoothing.\nThe best model model is the Bernoulli Na¨ıve-Bayes with priors\nproportional to the class sizes and λ = 1.\nTable 2: Accuracy rates (multiplied by 100) of the Na¨ıve-Bayes classiﬁer. In\nbrackets cross-validation standard errors (multiplied by 100) are reported.\nProbabilistic\nPrior\nSmoothing\nAccuracy\nmodel\nParameter\nMultinomial\nuniform\nλ = 1\n98.27 (0.28)\nMultinomial\nclass document frequency\nλ = 1\n98.07 (0.31)\nMultinomial\nuniform\nλ = 0\n89.76 (0.61)\nMultinomial\nclass document frequency\nλ = 0\n89.67 (0.63)\nBernoulli\nuniform\nλ = 1\n98.50 (0.28)\nBernoulli\nclass document frequency\nλ = 1\n98.54 (0.28)\nBernoulli\nuniform\nλ = 0\n42.68 (0.79)\nBernoulli\nclass document frequency\nλ = 0\n42.68 (0.79)\nk-Nearest Neighbor\nThe k-NN has been applied for a number of neighbors\nk = 1, 2, 3, with several distance measures: the Euclidean (L2), Manhattan\n8\n(L1) and Canberra distances have been computed with the R package stats,\nthe Cosine distance via the R package skmeans. The accuracy of the method\naccording to the diﬀerent settings is shown in Table 3. As expected, due to the\nsparsity of the data, the distance that produces the best classiﬁcation is the\nCosine dissimilarity.\nTable 3: Accuracy rates (multiplied by 100) of the k-Nearest Neighbor classiﬁer\nfor diﬀerent values of k and distances. In brackets cross-validation standard\nerrors (multiplied by 100) are reported.\nk\nDistance\nAccuracy\n1-NN\nEuclidean distance\n97.33 (0.33)\n2-NN\nEuclidean distance\n94.07 (0.47)\n3-NN\nEuclidean distance\n91.32 (0.67)\n1-NN\nManhattan distance\n97.28 (0.34)\n2-NN\nManhattan distance\n94.31 (0.45)\n3-NN\nManhattan distance\n91.51 (0.67)\n1-NN\nCanberra distance\n98.09 (0.28)\n2-NN\nCanberra distance\n96.30 (0.33)\n3-NN\nCanberra distance\n95.97 (0.42)\n1-NN\nCosine distance\n98.42 (0.28)\n2-NN\nCosine distance\n96.63 (0.36)\n3-NN\nCosine distance\n96.26 (0.38)\nComponent-wise classiﬁer\nThe Centroid classiﬁer and the Nearest Shrunken\nCentroid classiﬁer (Tibshirani et al., 2002) have been implemented with the R\npackage pamr. NSC has been computed with threshold parameter equal to 0.5,\n1 and 2. The Quantile Classiﬁer has been implemented with the R package\nquantileDA for 100 diﬀerent percentiles (in Table 4 only the higher accuracy\nrate corresponding to the percentile 0.98 is reported). The Median Classiﬁer\nis obtained as special case of the Quantile one for the percentile 0.5. Results\nare shown in Table 4. Classiﬁcation results are not as good as the previous\nmethods. The component-wise classiﬁers suﬀer from the high sparsity of the\ndata more than other strategies. This is due to the fact that in most cases the\ncentroid, the median and even the extreme quantiles are zero.\nLinear Discriminant Analysis\nFisher’s LDA has been implemented by the\nR package MASS. The accuracy obtained by cross-validation is 97.56% with a\nstandard error (multiplied by 100) of 0.27.\nSupport Vector Machines\nSupport vector machines have been implemented\nwith the R package e1071 with diﬀerent types of kernels: linear, radial base,\n9\nTable 4: Accuracy rates (multiplied by 100) of diﬀerent component-wise meth-\nods. In brackets cross-validation standard errors (multiplied by 100) are re-\nported.\nMethod\nAccuracy\nCC\n95.40 (0.43)\nNSC (threshold=0.5)\n92.95 (0.49)\nNSC (threshold=1)\n90.27 (0.52)\nNSC (threshold=2)\n87.24 (0.60)\nMC\n48.05 (0.59)\nQC (percentile 0.98)\n91.34 (0.59)\npolynomial and sigmoid. As shown in Table 5, the best model is the SVM with\nsigmoid kernel.\nTable 5: Accuracy rates (multiplied by 100) of Support Vector Machines with\ndiﬀerent kernels.\nIn brackets cross-validation standard errors (multiplied by\n100) are reported.\nKernel\nAccuracy\nLinear kernel\n98.13 (0.24)\nPolynomial kernel\n58.85 (1.02)\nRadial base kernel\n91.40 (0.66)\nSigmoid kernel\n98.97 (0.20)\nDecision Trees and Random Forests\nWe implemented the classiﬁcation\ntrees by the R package rpart, with the best combination of tuning parame-\nters minsplit=(5, 10, 15, 20) and cp= (0.001, 0.01, 0.1, 0.2) chosen in cross-\nvalidation. Random Forests (Breiman, 2001) have been computed with ntree=500\nand tuning parameter mtry selected with the function tuneRF. We evaluated\nalso Random Forests for high dimensional data proposed by Xu et al. (2012),\na special algorithm that can classify very high-dimensional data with random\nforests built using small subspaces. They have been implemented with param-\neter mtry=50.\nNeural Networks and Deep Neural Networks\nNeural networks have been\napplied to the data with diﬀerent structure chosen semi-automatically by the\nsoftware with the R package nnet, with a number of nodes equal to 1 or 2. Deep\nneural networks (Lai et al., 2015) have been implemented under an R interface\nto Python called keras. We considered several architectures: 1 or 2 hidden\nlayers, combinations of diﬀerent activation nonlinear functions at the diﬀerent\n10\nTable 6: Accuracy rates (multiplied by 100) of Classiﬁcation Trees and Random\nForests. In brackets cross-validation standard errors (multiplied by 100) are\nreported.\nMethod\nAccuracy\nDecision trees\n93.24 (0.49)\nRandom Forests\n97.85 (0.27)\nRandom Forests for high dimensional data\n96.95 (0.32)\nlayers (relu, softmax, etc.), diﬀerent number of nodes and batch sizes. In the\nfollowing table, we present results for the best architectures, within the many\ncombinations of tuning parameters, in the family of classical neural networks\nand deep neural networks with 1 and 2 hidden layers. Results indicate how a\ndeep structure can largely improve the classiﬁcation. One hidden layer in this\ncase is enough compared to the case of 2 hidden layers or more. The best DNN\nmodel with 1 hidden layer corresponds to the following setting for the tuning\nparameters: unit=10, activation=‘selu’, optimizer=‘adam’, Epoch=100, batch\nsize=32, validation split=0.2.\nTable 7: Accuracy rates (multiplied by 100) of ANNs and Deep NNs. In brackets\ncross-validation standard errors (multiplied by 100) are reported.\nMethod\nAccuracy\nNeural Network (1 node)\n70.32 (1.63)\nNeural Network (2 nodes)\n90.73 (0.86)\nDeep Neural Network (1 hidden layer)\n98.82 (0.25)\nDeep Neural Network (2 hidden layers)\n98.73 (0.24)\nEnsemble of classiﬁers\nFrom the previous results it emerges that diﬀerent\nclassiﬁers produce overall good classiﬁcation. Here we consider the best classi-\nﬁers with an accuracy of about 98%, taking the best setting within each family\nof models, namely:\n1. Na¨ıve-Bayes classiﬁer with Bernoulli distribution and priors proportional\nto the class sizes (accuracy 98.54%)\n2. 1-NN with Cosine distance (accuracy 98.42%)\n3. Support Vector Machines with sigmoid kernel (accuracy 98.97%)\n4. Random Forests (accuracy 97.85%)\n5. Deep Neural Networks with 1 hidden layer (accuracy 98.82%)\n11\n0.975\n0.980\n0.985\n0.990\n0.995\nnBayes\nKNN\nSVM\nDNNs\nRFs\nensemble\nAccuracy\nFigure 2:\nAccuracy obtained by the diﬀerent strategies in 50-fold cross-\nvalidation\nThe ﬁve algorithms are considered as base learners in an ensemble strategy\nwhere the aim is to combine the diﬀerent decisions about the ticket allocation.\nHere we adopt the majority vote strategy: a new ticket is assigned to the modal\nclass of the ﬁve classiﬁcation strategies. Figure 2 depicts the plot of the average\naccuracy (with conﬁdence intervals) obtained by cross-validation by the 5 best\nmethods and the ensemble strategy. The ensemble classiﬁer has an accuracy of\n99.44% with standard error 0.14 (multiplied by 100).\nIn order to comparatively evaluate the classiﬁcation accuracy within each\nclass we computed precision, recall and the F-index. The precision is also called\npositive predictive value because it measures the fraction of correct assignments\nin a class over the predicted documents of the class. Recall is a measure of\nsensitivity, since it represents the fraction of correct assignments in a class over\nthe total true documents of the class. Values of precision and recall for the\ndiﬀerent methods and classes are reported in Table 8. The table also contains\nthe so-called F-score, a measure that combines precision and recall by their\nharmonic mean:\nF = 2 · precision · recall\nprecision + recall\nResults indicate how the ensemble strategy represents a very robust and\naccurate strategy for this classiﬁcation task, in terms of the overall accuracy\nand of the validation measures computed separately for each class.\nOne may also consider to select classiﬁers according to the diversity between\nthem, as it is recognized to be one of the desired characteristics to improve the\nperformance of an ensemble (Zouari et al., 2005). However, in the classiﬁcation\ncontext, there is no complete and agreed upon theory to explain why and how di-\nversity between individual models contribute toward overall ensemble accuracy\n(Rokach, 2010). Indeed, Kuncheva and Whitaker (2003) studied the connection\n12\nTable 8: Precision, recall and F index for the 5 classes of the base learners and\nthe ensemble classiﬁer\nPrecision\nnBayes\nKNN\nSVM\nDNNs\nRFs\nensemble\nACT\n0.99\n0.99\n0.99\n0.99\n0.99\n1.00\nBAL\n0.96\n0.97\n0.99\n0.98\n0.96\n0.99\nOFF\n0.97\n0.95\n0.97\n0.99\n0.96\n0.99\nTOP\n1.00\n0.99\n0.99\n0.99\n0.99\n1.00\nISS\n1.00\n0.99\n0.99\n0.98\n0.99\n1.00\nRecall\nnBayes\nKNN\nSVM\nDNNs\nRFs\nensemble\nACT\n0.98\n0.97\n0.99\n0.98\n0.97\n1.00\nBAL\n0.98\n0.98\n0.97\n0.99\n0.99\n0.99\nOFF\n0.99\n0.98\n0.98\n0.98\n0.94\n0.99\nTOP\n0.99\n0.98\n1.00\n0.99\n0.99\n1.00\nISS\n0.98\n0.99\n1.00\n1.00\n0.99\n1.00\nF-score\nnBayes\nKNN\nSVM\nDNNs\nRFs\nensemble\nACT\n0.98\n0.97\n0.99\n0.98\n0.97\n1.00\nBAL\n0.98\n0.98\n0.97\n0.99\n0.99\n0.99\nOFF\n0.99\n0.98\n0.98\n0.98\n0.94\n0.99\nTOP\n0.99\n0.98\n1.00\n0.99\n0.99\n1.00\nISS\n0.98\n0.99\n1.00\n1.00\n0.99\n1.00\nbetween accuracy and ten statistics which can measure diversity among binary\nclassiﬁer outputs. Although there are proven connections between diversity and\naccuracy in some special cases, their results raise some doubts about the use-\nfulness of diversity measures in building classiﬁer ensembles in real-life pattern\nrecognition problem. In addition to these considerations, as in our case there is\nno evidence of notable diversity (the smallest Yule’s Q statistic value is 0.84 for\nthe most accurate classiﬁers), there is not point in pursuing such direction.\n5\nConclusions\nThis paper gives a literature review of the most common classiﬁcation methods\nfor text classiﬁcation. Classical classiﬁcation methods, modern deep learning\nand ensemble strategies have been considered.\nThis comprehensive study has been motivated by a real textual data prob-\nlem, that is the implementation of an automatic process to allocate incoming\ncalls to their correct macro-topic in order to provide a better and fast assistance\nservice. Data have a very complex and high-dimensional structure, caused by\nthe huge number of tickets and terms used by people that call the company for a\nspeciﬁc request and by a relevant degree of sparsity. However a list of 5 classiﬁers\nhave been identiﬁed as powerful tools for the data classiﬁcation. Among these,\n13\nthe prominent deep neural networks, which many researchers consider to be the\nonly true magic recipe for prediction and classiﬁcation. The diﬀerent methods\nhave pros and cons. For instance Naive-Bayes has the great advantage to be\ncomputationally straightforward compared to the others methods and to have a\nhigh accuracy classiﬁcation rate. Deep Learning strategies are very ﬂexible and\nwork very well in all most situations at the price of demanding computational\nburden.\nThe best classiﬁcation performances are obtained by Support Vector Ma-\nchines with sigmoid kernel, Naive-Bayes classiﬁer with Bernoulli distribution,\nthe 1-Nearest Neighbor classiﬁer with cosine distance, Random Forests and Deep\nNeural Networks.\nThe ensemble strategy we proposed seems to be robust and is more accurate\nof the classiﬁcation methods separately considered. In fact, the ﬁnal ensemble\noutperforms all the base classiﬁers, even the deep learning methods, yielding\nonly a small fraction of misclassiﬁed tickets. Implementation of such strategy\nis straightforward but powerful, as it combines the good individual results and\nthe speciﬁc methodological features for an overall better performance.\nReferences\nBreiman, L. (2001, Oct). Random forests. Machine Learning 45(1), 5–32.\nBreiman, L., J. Friedman, R. Olshen, and C. Stone (1984). Classiﬁcation and\nRegression Trees. Belmont CA: Wadsworth.\nCortes, C. and V. Vapnik (1995, Sep).\nSupport-vector networks.\nMachine\nLearning 20(3), 273–297.\nCover, T. and P. Hart (1967). Nearest neighbor pattern classiﬁcation. IEEE\nTransactions on Information Theory 13, 21–27.\nDietterich, T. G. (2000). Ensemble methods in machine learning. In Multiple\nClassiﬁer Systems, Berlin, Heidelberg, pp. 1–15. Springer Berlin Heidelberg.\nFisher, R. A. (1936). The use of multiple measurements in taxonomic problems.\nAnnals of Eugenics 7(2), 179–188.\nGhosh, A. and P. Chaudhuri (2005). On Data Depth and Distribution-Free\nDiscriminant Analysis Using Separating Surfaces. Bernoulli 11, 1–27.\nHall, P., D. M. Titterington, and J.-H. Xue (2009). Median-based classiﬁers\nfor high-dimensional data. Journal of the American Statistical Society 104,\n1597–1608.\nHand, D. and K. Yu (2001). Idiot’s Bayes - Not so Stupid After All? Interna-\ntional Statistical Review 69, 385–398.\n14\nHarish, B. S., D. S. Guru, and S. Manjunath (2010). Representation and classi-\nﬁcation of text documents: A brief review. IJCA, Special Issue on RTIPPR\n(2), 110–119.\nHennig, C. and C. Viroli (2016). Quantile-based classiﬁers. Biometrika 103(2),\n435–446.\nJohn, G. and P. Langley (1995).\nEstimating Continuous Distributions in\nBayesian Classiﬁers. In Proceedings of the Eleventh Conference on Uncer-\ntainty in Artiﬁcial Intelligence, pp. 338–345.\nJ¨ornsten, R. (2004). Clustering and Classiﬁcation based on the L1 Data Depth.\nJournal of Multivariate Analysis 91, 67–89.\nKhan, A., B. Baharudin, L. H. Lee, K. Khan, and U. T. P. Tronoh (2010). A\nreview of machine learning algorithms for text-documents classiﬁcation. In\nJournal of Advances In Information Technology.\nKohavi, R. et al. (1995). A study of cross-validation and bootstrap for accu-\nracy estimation and model selection. In Proceedings of the 14th International\nJoint Conference on Artiﬁcial intelligence, Volume 2, pp. 1137–1145. Mon-\ntreal, Canada.\nKorde, V. and C. N. Mahender (2012). Text classiﬁcation and classiﬁers: A\nsurvey. International Journal of Artiﬁcial Intelligence & Applications 3(2),\n85.\nKumbhar, P. and M. Mali (2016). A survey on feature selection techniques and\nclassiﬁcation algorithms for eﬃcient text classiﬁcation. International Journal\nof Science and Research 5(5), 9.\nKuncheva, L. I. and C. J. Whitaker (2003). Measures of diversity in classi-\nﬁer ensembles and their relationship with the ensemble accuracy. Machine\nlearning 51(2), 181–207.\nLai, S., L. Xu, K. Liu, and J. Zhao (2015). Recurrent convolutional neural\nnetworks for text classiﬁcation. In Proceedings of the Twenty-Ninth AAAI\nConference on Artiﬁcial Intelligence, AAAI’15, pp. 2267–2273. AAAI Press.\nLata, S. and M. R. Loar (2018). Text clustering and classiﬁcation techniques-a\nreview. International Journal on Recent and Innovation Trends in Computing\nand Communication 6(3), 237–241.\nLeCun, Y., Y. Bengio, and G. Hinton (2015). Deep learning. Nature 521(7553),\n436–444.\nLochter, J. V., R. F. Zanetti, D. Reller, and T. A. Almeida (2016). Short text\nopinion detection using ensemble of classiﬁers and semantic indexing. Expert\nSystems with Applications 62, 243 – 249.\n15\nMironczuk, M. and J. Protasiewicz (2018, 03). A recent overview of the state-of-\nthe-art elements of text classiﬁcation. Expert Systems with Applications 106,\n36–54.\nOnan, A., S. Korukolu, and H. Bulut (2016). Ensemble of keyword extraction\nmethods and classiﬁers in text classiﬁcation. Expert Systems with Applica-\ntions 57, 232 – 247.\nPatra, A. and D. Singh (2013, August). Article: A survey report on text classi-\nﬁcation with diﬀerent term weighing methods and comparison between clas-\nsiﬁcation algorithms. International Journal of Computer Applications 75(7),\n14–18. Full text available.\nRokach, L. (2010).\nEnsemble-based classiﬁers.\nArtiﬁcial Intelligence Re-\nview 33(1-2), 1–39.\nSchmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural\nNetworks 61, 85–117.\nSch¨utze, H., C. D. Manning, and P. Raghavan (2008). Introduction to informa-\ntion retrieval, Volume 39. Cambridge University Press.\nTibshirani, R., T. Hastie, B. Narasimhan, and G. Chu (2002). Diagnosis of\nmultiple cancer types by shrunken centroids of gene expression. Proceedings\nof the National Academy of Sciences 99(10), 6567–6572.\nTibshirani, R., T. Hastie, B. Narasimhan, and G. Chu (2003). Class predic-\ntion by nearest shrunken centroids, with applications to DNA microarrays.\nStatistical Science, 104–117.\nTorkkola, K. (2001). Linear discriminant analysis in document classiﬁcation. In\nIEEE ICDM Workshop on Text Mining, pp. 800–806. Citeseer.\nWang, G., J. Sun, J. Ma, K. Xu, and J. Gu (2014). Sentiment classiﬁcation:\nThe contribution of ensemble learning. Decision Support Systems 57, 77 – 93.\nWang, L., J. Zhu, and H. Zou (2008). Hybrid Huberized Support Vector Ma-\nchines for Microarray Classiﬁcation and Gene Selection. Bioinformatics 24,\n412–419.\nXu, B., J. Z. Huang, G. Williams, Q. Wang, and Y. Ye (2012). Classifying\nvery high-dimensional data with random forests built from small subspaces.\nInternational Journal of Data Warehousing and Mining (IJDWM) 8(2), 44–\n63.\nZouari, H., L. Heutte, and Y. Lecourtier (2005). Controlling the diversity in clas-\nsiﬁer ensembles through a measure of agreement. Pattern Recognition 38(11),\n2195 – 2199.\n16\n",
  "categories": [
    "cs.CL",
    "cs.IR",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-02-18",
  "updated": "2019-02-18"
}