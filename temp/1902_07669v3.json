{
  "id": "http://arxiv.org/abs/1902.07669v3",
  "title": "ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing",
  "authors": [
    "Mark Neumann",
    "Daniel King",
    "Iz Beltagy",
    "Waleed Ammar"
  ],
  "abstract": "Despite recent advances in natural language processing, many statistical\nmodels for processing text perform extremely poorly under domain shift.\nProcessing biomedical and clinical text is a critically important application\narea of natural language processing, for which there are few robust, practical,\npublicly available models. This paper describes scispaCy, a new tool for\npractical biomedical/scientific text processing, which heavily leverages the\nspaCy library. We detail the performance of two packages of models released in\nscispaCy and demonstrate their robustness on several tasks and datasets. Models\nand code are available at https://allenai.github.io/scispacy/",
  "text": "ScispaCy: Fast and Robust Models\nfor Biomedical Natural Language Processing\nMark Neumann, Daniel King, Iz Beltagy, Waleed Ammar\nAllen Institute for Artiﬁcial Intelligence, Seattle, WA, USA\n{markn,daniel,beltagy,waleeda}@allenai.org\nAbstract\nDespite recent advances in natural language\nprocessing, many statistical models for pro-\ncessing text perform extremely poorly un-\nder domain shift. Processing biomedical and\nclinical text is a critically important applica-\ntion area of natural language processing, for\nwhich there are few robust, practical, publicly\navailable models. This paper describes scis-\npaCy, a new Python library and models for\npractical biomedical/scientiﬁc text processing,\nwhich heavily leverages the spaCy library. We\ndetail the performance of two packages of\nmodels released in scispaCy and demonstrate\ntheir robustness on several tasks and datasets.\nModels and code are available at https://\nallenai.github.io/scispacy/.\n1\nIntroduction\nThe publication rate in the medical and biomedical\nsciences is growing at an exponential rate (Born-\nmann and Mutz, 2014).\nThe information over-\nload problem is widespread across academia, but\nis particularly apparent in the biomedical sciences,\nwhere individual papers may contain speciﬁc dis-\ncoveries relating to a dizzying variety of genes,\ndrugs, and proteins. In order to cope with the sheer\nvolume of new scientiﬁc knowledge, there have\nbeen many attempts to automate the process of ex-\ntracting entities, relations, protein interactions and\nother structured knowledge from scientiﬁc papers\n(Wei et al., 2016; Ammar et al., 2018; Poon et al.,\n2014).\nAlthough there exists a wealth of tools for\nprocessing biomedical text, many focus primar-\nily on named entity recognition and disambigua-\ntion. MetaMap and MetaMapLite (Aronson, 2001;\nDemner-Fushman et al., 2017), the two most\nwidely used and supported tools for biomedical\ntext processing, support entity linking with nega-\ntion detection and acronym resolution. However,\nFigure 1: Growth of the annual number of cited ref-\nerences from 1650 to 2012 in the medical and health\nsciences (citing publications from 1980 to 2012). Fig-\nure from (Bornmann and Mutz, 2014).\ntools which cover more classical natural language\nprocessing (NLP) tasks such as the GENIA tag-\nger (Tsuruoka et al., 2005; Tsuruoka and Tsujii,\n2005), or phrase structure parsers such as those\npresented in McClosky and Charniak (2008) typi-\ncally do not make use of new research innovations\nsuch as word representations or neural networks.\nIn this paper, we introduce scispaCy, a spe-\ncialized NLP library for processing biomedical\ntexts which builds on the robust spaCy library,1\nand document its performance relative to state\nof the art models for part of speech (POS) tag-\nging, dependency parsing, named entity recogni-\ntion (NER) and sentence segmentation. Speciﬁ-\ncally, we:\n• Release a reformatted version of the GENIA\n1.0 (Kim et al., 2003) corpus converted into\nUniversal Dependencies v1.0 and aligned\nwith the original text from the PubMed ab-\nstracts.\n1spacy.io\narXiv:1902.07669v3  [cs.CL]  9 Oct 2019\nModel\nVocab Size\nVector\nCount\nMin\nWord\nFreq\nMin\nDoc\nFreq\nen core sci sm 58,338\n0\n50\n5\nen core sci md 101,678\n98,131\n20\n5\nTable 1: Vocabulary statistics for the two core packages\nin scispaCy.\n• Benchmark 9 named entity recognition mod-\nels for more speciﬁc entity extraction ap-\nplications demonstrating competitive perfor-\nmance when compared to strong baselines.\n• Release and evaluate two fast and convenient\npipelines for biomedical text, which include\ntokenization, part of speech tagging, depen-\ndency parsing and named entity recognition.\n2\nOverview of (sci)spaCy\nIn this section, we brieﬂy describe the models used\nin the spaCy library and describe how we build on\nthem in scispaCy.\nspaCy.\nThe Python-based spaCy library (Hon-\nnibal and Montani, 2017)2 provides a variety of\npractical tools for text processing in multiple lan-\nguages. Their models have emerged as the defacto\nstandard for practical NLP due to their speed, ro-\nbustness and close to state of the art performance.\nAs the spaCy models are popular and the spaCy\nAPI is widely known to many potential users, we\nchoose to build upon the spaCy library for creating\na biomedical text processing pipeline.\nscispaCy.\nOur goal is to develop scispaCy as a\nrobust, efﬁcient and performant NLP library to\nsatisfy the primary text processing needs in the\nbiomedical domain. In this release of scispaCy,\nwe retrain spaCy3 models for POS tagging, depen-\ndency parsing, and NER using datasets relevant\nto biomedical text, and enhance the tokenization\nmodule with additional rules. scispaCy contains\ntwo core released packages: en core sci sm and\nen core sci md. Models in the en core sci md\npackage have a larger vocabulary and include\nword vectors, while those in en core sci sm have\na smaller vocabulary and do not include word vec-\ntors, as shown in Table 1.\n2Source\ncode\nat\nhttps://github.com/\nexplosion/spaCy\n3scispaCy models are based on spaCy version 2.0.18\nProcessing Times Per\nSoftware Package Abstract (ms) Sentence (ms)\nNLP4J (java)\n19\n2\nGenia Tagger (c++)\n73\n3\nBiafﬁne (TF)\n272\n29\nBiafﬁne (TF + 12 CPUs)\n72\n7\njPTDP (Dynet)\n905\n97\nDexter v2.1.0\n208\n84\nMetaMapLite v3.6.2\n293\n89\nen core sci sm\n32\n4\nen core sci md\n33\n4\nTable 2: Wall clock comparison of different publicly\navailable biomedical NLP pipelines. All experiments\nrun on a single machine with 12 Intel(R) Core(TM)\ni7-6850K CPU @ 3.60GHz and 62GB RAM. For the\nBiafﬁne Parser, a pre-compiled Tensorﬂow binary with\nsupport for AVX2 instructions was used in a good faith\nattempt to optimize the implementation. Dynet does\nsupport the Intel MKL, but requires compilation from\nscratch and as such, does not represent an “off the\nshelf” system. TF is short for Tensorﬂow.\nProcessing Speed.\nTo emphasize the efﬁciency\nand practical utility of the end-to-end pipeline pro-\nvided by scispaCy packages, we perform a speed\ncomparison with several other publicly available\nprocessing pipelines for biomedical text using 10k\nrandomly selected PubMed abstracts. We report\nresults with and without segmenting the abstracts\ninto sentences since some of the libraries (e.g.,\nGENIA tagger) are designed to operate on sen-\ntences.\nAs shown in Table 2, both models released\nin scispaCy demonstrate competitive speed to\npipelines written in C++ and Java, languages de-\nsigned for production settings.\nWhilst scispaCy is not as fast as pipelines\ndesigned for purely production use-cases (e.g.,\nNLP4J), it has the beneﬁt of straightforward in-\ntegration with the large ecosystem of Python li-\nbraries for machine learning and text processing.\nAlthough the comparison in Table 2 is not an ap-\nples to apples comparison with other frameworks\n(different tasks, implementation languages etc), it\nis useful to understand scispaCy’s runtime in the\ncontext of other pipeline components. Running\nscispaCy models in addition to standard Entity\nLinking software such as MetaMap would result\nin only a marginal increase in overall runtime.\nIn the following section, we describe the POS\ntaggers and dependency parsers in scispaCy.\n3\nPOS Tagging and Dependency Parsing\nThe joint POS tagging and dependency parsing\nmodel in spaCy is an arc-eager transition-based\nparser trained with a dynamic oracle, similar to\nGoldberg and Nivre (2012).\nFeatures are CNN\nrepresentations of token features and shared across\nall pipeline models (Kiperwasser and Goldberg,\n2016; Zhang and Weiss, 2016). Next, we describe\nthe data we used to train it in scispaCy.\n3.1\nDatasets\nGENIA 1.0 Dependencies.\nTo train the depen-\ndency parser and part of speech tagger in both\nreleased models, we convert the treebank of Mc-\nClosky and Charniak (2008),4 which is based on\nthe GENIA 1.0 corpus (Kim et al., 2003), to\nUniversal Dependencies v1.0 using the Stanford\nDependency Converter (Schuster and Manning,\n2016). As this dataset has POS tags annotated, we\nuse it to train the POS tagger jointly with the de-\npendency parser in both released models.\nAs we believe the Universal Dependencies con-\nverted from the original GENIA 1.0 corpus are\ngenerally useful, we have released them as a sep-\narate contribution of this paper.5 In this data re-\nlease, we also align the converted dependency\nparses to their original text spans in the raw, un-\ntokenized abstracts from the original release,6 and\ninclude the PubMed metadata for the abstracts\nwhich was discarded in the GENIA corpus re-\nleased by McClosky and Charniak (2008).\nWe\nhope that this raw format can emerge as a resource\nfor practical evaluation in the biomedical domain\nof core NLP tasks such as tokenization, sentence\nsegmentation and joint models of syntax.\nFinally, we also retrieve from PubMed the orig-\ninal metadata associated with each abstract. This\nincludes relevant named entities linked to their\nMedical Subject Headings (MeSH terms) as well\nas chemicals and drugs linked to a variety of on-\ntologies, as well as author metadata, publication\ndates, citation statistics and journal metadata. We\nhope that the community can ﬁnd interesting prob-\nlems for which such natural supervision can be\nused.\n4https://nlp.stanford.edu/˜mcclosky/\nbiomedical.html\n5https://github.com/allenai/\ngenia-dependency-trees\n6Available at http://www.geniaproject.org/\nPackage/Model\nGENIA\nMarMoT\n98.61\njPTDP-v1\n98.66\nNLP4J-POS\n98.80\nBiLSTM-CRF\n98.44\nBiLSTM-CRF- charcnn\n98.89\nBiLSTM-CRF - char lstm\n98.85\nen core sci sm\n98.38\nen core sci md\n98.51\nTable 3: Part of Speech tagging results on the GENIA\nTest set.\nPackage/Model\nUAS\nLAS\nStanford-NNdep\n89.02\n87.56\nNLP4J-dep\n90.25\n88.87\njPTDP-v1\n91.89\n90.27\nStanford-Biafﬁne-v2\n92.64\n91.23\nStanford-Biafﬁne-v2(Gold POS)\n92.84\n91.92\nen core sci sm - SD\n90.31\n88.65\nen core sci md - SD\n90.66\n88.98\nen core sci sm\n89.69\n87.67\nen core sci md\n90.60\n88.79\nTable 4: Dependency Parsing results on the GENIA 1.0\ncorpus converted to dependencies using the Stanford\nUniversal Dependency Converter. We additionally pro-\nvide evaluations using Stanford Dependencies(SD) in\norder for comparison relative to the results reported in\n(Nguyen and Verspoor, 2018).\nOntoNotes 5.0.\nTo increase the robustness of the\ndependency parser and POS tagger to generic text,\nwe make use of the OntoNotes 5.0 corpus7 when\ntraining the dependency parser and part of speech\ntagger (Weischedel et al., 2011; Hovy et al., 2006).\nThe OntoNotes corpus consists of multiple genres\nof text, annotated with syntactic and semantic in-\nformation, but we only use POS and dependency\nparsing annotations in this work.\n3.2\nExperiments\nWe compare our models to the recent survey\nstudy of dependency parsing and POS tagging for\nbiomedical data (Nguyen and Verspoor, 2018) in\nTables 3 and 4. POS tagging results show that both\nmodels released in scispaCy are competitive with\nstate of the art systems, and can be considered of\n7Instructions for download at http://cemantix.\norg/data/ontonotes.html\nequivalent practical value. In the case of depen-\ndency parsing, we ﬁnd that the Biafﬁne parser of\nDozat and Manning (2016) outperforms the scis-\npaCy models by a margin of 2-3%. However, as\ndemonstrated in Table 2, the scispaCy models are\napproximately 9x faster due to the speed optimiza-\ntions in spaCy. 8\nRobustness to Web Data.\nA core principle of\nthe scispaCy models is that they are useful on a\nwide variety of types of text with a biomedical fo-\ncus, such as clinical notes, academic papers, clin-\nical trials reports and medical records. In order\nto make our models robust across a wider range\nof domains more generally, we experiment with\nincorporating training data from the OntoNotes\n5.0 corpus when training the dependency parser\nand POS tagger. Figure 2 demonstrates the effec-\ntiveness of adding increasing percentages of web\ndata, showing substantially improved performance\non OntoNotes, at no reduction in performance on\nbiomedical text. Note that mixing in web text dur-\ning training has been applied to previous systems\n- the GENIA Tagger (Tsuruoka et al., 2005) also\nemploys this technique.\nFigure 2: Unlabeled attachment score (UAS) perfor-\nmance for an en core sci md model trained with in-\ncreasing amounts of web data incorporated.\nTable\nshows mean of 3 random seeds.\n4\nNamed Entity Recognition\nThe NER model in spaCy is a transition-based\nsystem based on the chunking model from Lam-\nple et al. (2016).\nTokens are represented as\nhashed, embedded representations of the preﬁx,\nsufﬁx, shape and lemmatized features of individ-\n8We refer the interested reader to Nguyen and Verspoor\n(2018) for a comprehensive description of model architec-\ntures considered in this evaluation.\nual words. Next, we describe the data we used to\ntrain NER models in scispaCy.\n4.1\nDatasets\nThe main NER model in both released packages\nin scispaCy is trained on the mention spans in\nthe MedMentions dataset (Murty et al., 2018).\nSince the MedMentions dataset was originally\ndesigned for entity linking, this model recog-\nnizes a wide variety of entity types, as well as\nnon-standard syntactic phrases such as verbs\nand modiﬁers, but the model does not predict\nthe entity type.\nIn order to provide for users\nwith more speciﬁc requirements around entity\ntypes,\nwe\nrelease\nfour\nadditional\npackages\nen ner {bc5cdr|craft|jnlpba|bionlp13cg} md\nwith\nﬁner-grained\nNER\nmodels\ntrained\non\nBC5CDR (for chemicals and diseases; Li et al.,\n2016), CRAFT (for cell types, chemicals, pro-\nteins, genes; Bada et al., 2011), JNLPBA (for cell\nlines, cell types, DNAs, RNAs, proteins; Collier\nand Kim, 2004) and BioNLP13CG (for cancer\ngenetics; Pyysalo et al., 2015), respectively.\n4.2\nExperiments\nAs NER is a key task for other biomedical text pro-\ncessing tasks, we conduct a through evaluation of\nthe suitability of scispaCy to provide baseline per-\nformance across a wide variety of datasets. In par-\nticular, we retrain the spaCy NER model on each\nof the four datasets mentioned earlier (BC5CDR,\nCRAFT, JNLPBA, BioNLP13CG) as well as ﬁve\nmore datasets in Crichton et al. (2017): AnatEM,\nBC2GM, BC4CHEMD, Linnaeus, NCBI-Disease.\nThese datasets cover a wide variety of entity types\nrequired by different biomedical domains, in-\ncluding cancer genetics, disease-drug interactions,\npathway analysis and trial population extraction.\nAdditionally, they vary considerably in size and\nnumber of entities.\nFor example, BC4CHEMD\n(Krallinger et al., 2015) has 84,310 annotations\nwhile Linnaeus (Gerner et al., 2009) only has\n4,263. BioNLP13CG (Pyysalo et al., 2015) anno-\ntates 16 entity types while ﬁve of the datasets only\nannotate a single entity type.9\nTable 5 provides a thorough comparison of the\nscispaCy NER models compared to a variety of\nmodels. In particular, we compare the models to\n9For a detailed discussion of the datasets and their\ncreation, we refer the reader to https://github.com/\ncambridgeltl/MTL-Bioinformatics-2016/\nblob/master/Additional%20file%201.pdf\nstrong baselines which do not consider the use of\n1) multi-task learning across multiple datasets and\n2) semi-supervised learning via large pretrained\nlanguage models. Overall, we ﬁnd that the scis-\npaCy models are competitive baselines for 5 of the\n9 datasets.\nAdditionally, in Table 6 we evaluate the recall\nof the pipeline mention detector available in both\nscispaCy models (trained on the MedMentions\ndataset) against all 9 specialised NER datasets.\nOverall, we observe a modest drop in average re-\ncall when compared directly to the MedMentions\nresults in Table 7, but considering the diverse do-\nmains of the 9 specialised NER datasets, achieving\nthis level of recall across datasets is already non-\ntrivial.\nDataset\nsci sm\nsci md\nBC5CDR\n75.62\n78.79\nCRAFT\n58.28\n58.03\nJNLPBA\n67.33\n70.36\nBioNLP13CG\n58.93\n60.25\nAnatEM\n56.55\n57.94\nBC2GM\n54.87\n56.89\nBC4CHEMD\n60.60\n60.75\nLinnaeus\n67.48\n68.61\nNCBI-Disease\n65.76\n65.65\nAverage\n62.81\n64.14\nTable 6: Recall on the test sets of 9 specialist NER\ndatasets, when the base mention detector is trained on\nMedMentions. The base mention detector is available\nin both en core sci sm and en core sci md models.\nModel\nPrecision\nRecall\nF1\nen core sci sm\n69.22\n67.19\n68.19\nen core sci md\n70.44\n67.56\n68.97\nTable 7: Performance of the base mention detector on\nthe MedMentions Corpus.\n5\nCandidate Generation for Entity\nLinking\nIn addition to Named Entity Recognition, scis-\npaCy contains some initial groundwork needed to\nbuild an Entity Linking model designed to link to\na subset of the Uniﬁed Medical Language System\n(UMLS; Bodenreider, 2004). This reduced subset\nis comprised of sections 0, 1, 2 and 9 (SNOMED)\nof the UMLS 2017 AA release, which are publicly\ndistributable. It contains 2.78M unique concepts\nand covers 99% of the mention concepts present\nin the MedMentions dataset (Murty et al., 2018).\n5.1\nCandidate Generation\nTo generate candidate entities for linking a given\nmention, we use an approximate nearest neigh-\nbours search over our subset of UMLS concepts\nand concept aliases and output the entities asso-\nciated with the nearest K. Concepts and aliases\nare encoded using the vector of TF-IDF scores of\ncharacter 3-grams which appears in 10 or more en-\ntity names or aliases (i.e., document frequency ≥\n10). In total, all data associated with the candi-\ndate generator including cached vectors for 2.78M\nconcepts occupies 1.1GB of space on disk.\nAliases.\nCanonical concepts in UMLS have\naliases - common names of drugs, alternative\nspellings, and otherwise words or phrases that\nare often linked to a given concept. Importantly,\naliases may be shared across concepts, such as\n“cancer” for the canonical concepts of both “Lung\nCancer” and “Breast Cancer”. Since the nearest\nneighbor search is based on the surface forms, it\nreturns K string values. However, because a given\nstring may be an alias for multiple concepts, the\nlist of K nearest neighbor strings may not translate\nto a list of K candidate entities. This is the correct\nimplementation in practice, because given a pos-\nsibly ambiguous alias, it is beneﬁcial to score all\nplausible concepts, but it does mean that we can-\nnot determine the exact number of candidate enti-\nties that will be generated for a given value of K.\nIn practice, the number of retrieved candidates for\na given K is much lower than K itself, with the ex-\nception of a few long tail aliases, which are aliases\nfor a large number of concepts. For example, for\nK=100, we retrieve 54.26±12.45 candidates, with\nthe max number of candidates for a single mention\nbeing 164.\nAbbreviations.\nDuring development of the can-\ndidate generator, we noticed that abbreviated men-\ntions account for a substantial proportion of the\nfailure cases where none of the generated candi-\ndates match the correct entity. To partially rem-\nedy this, we implement the unsupervised abbrevi-\nation detection algorithm of Schwartz and Hearst\n(2002), substituting mention candidates marked as\nabbreviations for their long form deﬁnitions before\nsearching for their nearest neighbours. Figure 3\ndemonstrates the improved recall of gold concepts\nDataset\nBaseline\nSOTA\n+ Resources\nsci sm\nsci md\nBC5CDR (Li et al., 2016)\n83.87\n86.92b\n89.69bb\n78.83\n83.92\nCRAFT (Bada et al., 2011)\n79.55\n-\n-\n72.31\n76.17\nJNLPBA (Collier and Kim, 2004)\n68.95\n73.48b\n75.50bb\n71.78\n73.21\nBioNLP13CG (Pyysalo et al., 2015)\n76.74\n-\n-\n72.98\n77.60\nAnatEM (Pyysalo and Ananiadou, 2014)\n88.55\n91.61**\n-\n80.13\n84.14\nBC2GM (Smith et al., 2008)\n84.41\n80.51b\n81.69bb\n75.77\n78.30\nBC4CHEMD (Krallinger et al., 2015)\n82.32\n88.75a\n89.37aa\n82.24\n84.55\nLinnaeus (Gerner et al., 2009)\n79.33\n95.68**\n-\n79.20\n81.74\nNCBI-Disease (Dogan et al., 2014)\n77.82\n85.80b\n87.34bb\n79.50\n81.65\nbb: LM model from Sachan et al. (2017) b: LSTM model from Sachan et al. (2017)\na: Single Task model from Wang et al. (2018) aa: Multi-task model from Wang et al. (2018)\n** Evaluations use dictionaries developed without a clear train/test split.\nTable 5: Test F1 Measure on NER for the small and medium scispaCy models compared to a variety of\nstrong baselines and state of the art models. The Baseline and SOTA (State of the Art) columns include\nonly single models which do not use additional resources, such as language models, or additional sources\nof supervision, such as multi-task learning. + Resources allows any type of supervision or pretraining. All\nscispaCy results are the mean of 5 random seeds.\nFigure 3: Gold Candidate Generation Recall for differ-\nent values of K. Note that K refers to the number of\nnearest neighbour queries, and not the number of con-\nsidered candidates. Murty et al. (2018) do not report\nthis distinction, but for a given K the same amount of\nwork is done (retrieving K neighbours from the index),\nso results are comparable. For all K, the actual number\nof candidates is considerably lower on average.\nfor various values of K nearest neighbours. Our\ncandidate generator provides a 5% absolute im-\nprovement over Murty et al. (2018) despite gen-\nerating 46% fewer candidates per mention on av-\nerage.\n6\nSentence Segmentation and Citation\nHandling\nAccurate sentence segmentation is required for\nmany practical applications of natural language\nprocessing. Biomedical data presents many dif-\nﬁculties for standard sentence segmentation algo-\nrithms: abbreviated names and noun compounds\ncontaining punctuation are more common, whilst\nthe wide range of citation styles can easily be\nmisidentiﬁed as sentence boundaries.\nWe evaluate sentence segmentation using both\nsentence and full-abstract accuracy when seg-\nmenting PubMed abstracts from the raw, unto-\nkenized GENIA development set (the Sent/Ab-\nstract columns in Table 8).\nAdditionally, we examine the ability of the seg-\nmentation learned by our model to generalise to\nthe body text of PubMed articles. Body text is\ntypically more complex than abstract text, but in\nparticular, it contains citations, which are consid-\nerably less frequent in abstract text. In order to ex-\namine the effectiveness of our models in this sce-\nnario, we design the following synthetic experi-\nment. Given sentences from Cohan et al. (2019)\nwhich were originally designed for citation in-\ntent prediction, we run these sentences individu-\nally through our models. As we know that these\nsentences should be single sentences, we can sim-\nply count the frequency with which our models\nsegment the individual sentences containing cita-\ntions into multiple sentences (the Citation column\nin Table 8).\nAs demonstrated by Table 8, training the de-\npendency parser on in-domain data (both the scis-\npaCy models) completely obviates the need for\nrule-based sentence segmentation. This is a pos-\nitive result - rule based sentence segmentation is\na brittle, time consuming process, which we have\nreplaced with a domain speciﬁc version of an ex-\nisting pipeline component.\nBoth scispaCy models are released with the cus-\ntom tokeniser, but without a custom sentence seg-\nmenter by default.\nModel\nSent\nAbstract Citation\nweb-small\n88.2%\n67.5%\n74.4%\nweb-small + ct\n86.6%\n62.1%\n88.6%\nweb-small + cs\n91.9%\n77.0%\n87.5%\nweb-small + cs + ct 92.1%\n78.3%\n94.7%\nsci-small + ct\n97.2%\n81.7%\n97.9%\nsci-small + cs + ct\n97.2%\n81.7%\n98.0%\nsci-med + ct\n97.3%\n81.7%\n98.0%\nsci-med + cs + ct\n97.4%\n81.7%\n98.0%\nTable 8: Sentence segmentation performance for the\ncore spaCy and scispaCy models. cs = custom rule\nbased sentence segmenter and ct = custom rule based\ntokenizer, both designed explicitly to handle citations\nand common patterns in biomedical text.\n7\nRelated Work\nApache cTakes (Savova et al., 2010) was de-\nsigned speciﬁcally for clinical notes rather than\nthe broader biomedical domain.\nMetaMap and\nMetaMapLite (Aronson, 2001; Demner-Fushman\net al., 2017) from the National Library of\nMedicine focus speciﬁcally on entity linking using\nthe Uniﬁed Medical Language System (UMLS)\n(Bodenreider, 2004) as a knowledge base. Buyko\net al. (2006) adapt Apache OpenNLP using the\nGENIA corpus, but their system is not openly\navailable and is less suitable for modern, Python-\nbased workﬂows. The GENIA Tagger (Tsuruoka\net al., 2005) provides the closest comparison to\nscispaCy due to it’s multi-stage pipeline, inte-\ngrated research contributions and production qual-\nity runtime. We improve on the GENIA Tagger\nby adding a full dependency parser rather than\njust noun chunking, as well as improved results\nfor NER without compromising signiﬁcantly on\nspeed.\nIn more fundamental NLP research, the GENIA\ncorpus (Kim et al., 2003) has been widely used\nto evaluate transfer learning and domain adapta-\ntion. McClosky et al. (2006) demonstrate the ef-\nfectiveness of self-training and parse re-ranking\nfor domain adaptation. Rimell and Clark (2008)\nadapt a CCG parser using only POS and lexical\ncategories, while Joshi et al. (2018) extend a neu-\nral phrase structure parser trained on web text to\nthe biomedical domain with a small number of\npartially annotated examples. These papers focus\nmainly of the problem of domain adaptation it-\nself, rather than the objective of obtaining a robust,\nhigh-performance parser using existing resources.\nNLP techniques, and in particular, distant su-\npervision have been employed to assist the cu-\nration of large, structured biomedical resources.\nPoon et al. (2015) extract 1.5 million cancer path-\nway interactions from PubMed abstracts, lead-\ning to the development of Literome (Poon et al.,\n2014), a search engine for genic pathway inter-\nactions and genotype-phenotype interactions. A\nfundamental aspect of Valenzuela-Escarcega et al.\n(2018) and Poon et al. (2014) is the use of hand-\nwritten rules and triggers for events based on de-\npendency tree paths; the connection to the appli-\ncation of scispaCy is quite apparent.\n8\nConclusion\nIn this paper we presented several robust model\npipelines for a variety of natural language process-\ning tasks focused on biomedical text. The scis-\npaCy models are fast, easy to use, scalable, and\nachieve close to state of the art performance. We\nhope that the release of these models enables new\napplications in biomedical information extraction\nwhilst making it easy to leverage high quality syn-\ntactic annotation for downstream tasks. Addition-\nally, we released a reformatted GENIA 1.0 cor-\npus augmented with automatically produced Uni-\nversal Dependency annotations and recovered and\naligned original abstract metadata. Future work on\nscispaCy will include a more fully featured entity\nlinker built from the current candidate generation\nwork, as well as other pipeline components such as\nnegation detection commonly used in the clinical\nand biomedical natural language processing com-\nmunities.\nReferences\nWaleed Ammar, Dirk Groeneveld, Chandra Bhagavat-\nula, Iz Beltagy, Miles Crawford, Doug Downey, Ja-\nson Dunkelberger, Ahmed Elgohary, Sergey Feld-\nman, Vu Ha, Rodney Kinney, Sebastian Kohlmeier,\nKyle Lo, Tyler Murray, Hsu-Han Ooi, Matthew E.\nPeters, Joanna Power, Sam Skjonsberg, Lucy Lu\nWang, Chris Wilhelm, Zheng Yuan, Madeleine van\nZuylen, and Oren Etzioni. 2018.\nConstruction of\nthe literature graph in semantic scholar. In NAACL-\nHLT.\nAlan R. Aronson. 2001. Effective mapping of biomed-\nical text to the UMLS Metathesaurus: the MetaMap\nprogram.\nProceedings. AMIA Symposium, pages\n17–21.\nMichael Bada, Miriam Eckert, Donald Evans, Kristin\nGarcia, Krista Shipley, Dmitry Sitnikov, William A.\nBaumgartner, K. Bretonnel Cohen, Karin M. Ver-\nspoor, Judith A. Blake, and Lawrence Hunter. 2011.\nConcept annotation in the CRAFT corpus. In BMC\nBioinformatics.\nOlivier Bodenreider. 2004. The uniﬁed medical lan-\nguage system (UMLS): integrating biomedical ter-\nminology.\nNucleic acids research, 32 Database\nissue:D267–70.\nLutz Bornmann and R¨udiger Mutz. 2014. Growth rates\nof modern science: A bibliometric analysis. CoRR,\nabs/1402.4578.\nEkaterina Buyko, Joachim Wermter, Michael Poprat,\nand Udo Hahn. 2006.\nAutomatically adapting an\nNLP core engine to the biology domain.\nIn Pro-\nceedings of the ISMB 2006 Joint Linking Literature,\nInformation and Knowledge for Biology and the 9th\nBio-Ontologies Meeting.\nArman Cohan, Waleed Ammar, Madeleine van Zuylen,\nand Field Cady. 2019. Structural scaffolds for ci-\ntation intent classiﬁcation in scientiﬁc publications.\nCoRR, abs/1904.01608.\nNigel Collier and Jin-Dong Kim. 2004. Introduction to\nthe bio-entity recognition task at JNLPBA. In NLP-\nBA/BioNLP.\nGamal K. O. Crichton, Sampo Pyysalo, Billy Chiu, and\nAnna Korhonen. 2017. A neural network multi-task\nlearning approach to biomedical named entity recog-\nnition. In BMC Bioinformatics.\nDina Demner-Fushman, Willie J. Rogers, and Alan R.\nAronson. 2017.\nMetaMap Lite: an evaluation of\na new Java implementation of MetaMap. Journal\nof the American Medical Informatics Association :\nJAMIA, 24 4:841–844.\nRezarta Islamaj Dogan, Robert Leaman, and Zhiyong\nLu. 2014. NCBI disease corpus: A resource for dis-\nease name recognition and concept normalization.\nJournal of biomedical informatics, 47:1–10.\nTimothy Dozat and Christopher D. Manning. 2016.\nDeep biafﬁne attention for neural dependency pars-\ning. CoRR, abs/1611.01734.\nMartin\nGerner,\nGoran\nNenadic,\nand\nCasey\nM.\nBergman. 2009. LINNAEUS: A species name iden-\ntiﬁcation system for biomedical literature. In BMC\nBioinformatics.\nYoav Goldberg and Joakim Nivre. 2012. A dynamic\noracle for arc-eager dependency parsing. In Coling\n2012.\nMatthew Honnibal and Ines Montani. 2017. spaCy 2:\nNatural language understanding with bloom embed-\ndings, convolutional neural networks and incremen-\ntal parsing. To appear.\nEduard H. Hovy, Mitchell P. Marcus, Martha Palmer,\nLance A. Ramshaw, and Ralph M. Weischedel.\n2006.\nOntoNotes: The 90% solution.\nIn HLT-\nNAACL.\nVidur Joshi, Matthew Peters, and Mark Hopkins. 2018.\nExtending a parser to distant domains using a few\ndozen partially annotated examples. In ACL.\nJin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and\nJun’ichi Tsujii. 2003. GENIA corpus - a semanti-\ncally annotated corpus for bio-textmining. Bioinfor-\nmatics, 19 Suppl 1:i180–2.\nEliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-\nple and accurate dependency parsing using bidirec-\ntional lstm feature representations.\nTransactions\nof the Association for Computational Linguistics,\n4:313–327.\nMartin Krallinger, Florian Leitner, Obdulia Rabal,\nMiguel Vazquez, Julen Oyarz´abal, and Alfonso Va-\nlencia. 2015. CHEMDNER: The drugs and chemi-\ncal names extraction challenge. In J. Cheminformat-\nics.\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\nIn HLT-NAACL.\nJiao Li, Yueping Sun, Robin J. Johnson, Daniela Sci-\naky, Chih-Hsuan Wei, Robert Leaman, Allan Peter\nDavis, Carolyn J. Mattingly, Thomas C. Wiegers,\nand Zhiyong Lu. 2016.\nBioCreative V CDR task\ncorpus:\na resource for chemical disease relation\nextraction.\nDatabase : the journal of biological\ndatabases and curation, 2016.\nDavid McClosky and Eugene Charniak. 2008.\nSelf-\ntraining for biomedical parsing. In ACL.\nDavid McClosky, Eugene Charniak, and Mark John-\nson. 2006.\nReranking and self-training for parser\nadaptation. In ACL.\nShikhar Murty, Patrick Verga, Luke Vilnis, Irena\nRadovanovic, and Andrew McCallum. 2018.\nHi-\nerarchical losses and new resources for ﬁne-grained\nentity typing and linking. In ACL.\nDat Quoc Nguyen and Karin Verspoor. 2018.\nFrom\nPOS tagging to dependency parsing for biomedical\nevent extraction. arXiv preprint arXiv:1808.03731.\nHoifung Poon, Chris Quirk, Charlie DeZiel, and David\nHeckerman. 2014.\nLiterome: PubMed-scale ge-\nnomic knowledge base in the cloud. Bioinformatics,\n30 19:2840–2.\nHoifung Poon, Kristina Toutanova, and Chris Quirk.\n2015. Distant supervision for cancer pathway ex-\ntraction from text. Paciﬁc Symposium on Biocom-\nputing. Paciﬁc Symposium on Biocomputing, pages\n120–31.\nSampo\nPyysalo\nand\nSophia\nAnaniadou.\n2014.\nAnatomical entity mention recognition at literature\nscale. In Bioinformatics.\nSampo Pyysalo, Tomoko Ohta, Rafal Rak, Andrew\nRowley, Hong-Woo Chun, Sung-Jae Jung, Sung-Pil\nChoi, Jun’ichi Tsujii, and Sophia Ananiadou. 2015.\nOverview of the cancer genetics and pathway cu-\nration tasks of BioNLP shared task 2013. In BMC\nBioinformatics.\nLaura Rimell and Stephen Clark. 2008.\nAdapting a\nlexicalized-grammar parser to contrasting domains.\nIn EMNLP.\nDevendra Singh Sachan, Pengtao Xie, Mrinmaya\nSachan, and Eric P. Xing. 2017.\nEffective use of\nbidirectional language modeling for transfer learn-\ning in biomedical named entity recognition.\nIn\nMLHC.\nGuergana K. Savova, James J. Masanz, Philip V.\nOgren, Jiaping Zheng, Sunghwan Sohn, Karin Kip-\nper Schuler, and Christopher G. Chute. 2010. Mayo\nclinical text analysis and knowledge extraction sys-\ntem (cTAKES): architecture, component evaluation\nand applications. Journal of the American Medical\nInformatics Association : JAMIA, 17 5:507–13.\nSebastian Schuster and Christopher D. Manning. 2016.\nEnhanced english universal dependencies: An im-\nproved representation for natural language under-\nstanding tasks. In LREC.\nAriel S. Schwartz and Marti A. Hearst. 2002. A sim-\nple algorithm for identifying abbreviation deﬁnitions\nin biomedical text. Paciﬁc Symposium on Biocom-\nputing. Paciﬁc Symposium on Biocomputing, pages\n451–62.\nLarry Smith, Lorraine K. Tanabe, Rie Johnson nee\nAndo, Cheng-Ju Kuo, I-Fang Chung, Chun-Nan\nHsu, Y Lin, Roman Klinger, Christoph M. Friedrich,\nKuzman Ganchev, Manabu Torii, Hongfang Liu,\nBarry Haddow,\nCraig A. Struble,\nRichard J.\nPovinelli, Andreas Vlachos, William A. Baumgart-\nner, Lawrence E. Hunter, Bob Carpenter, Richard\nTzong-Han Tsai, Hong-Jie Dai, Feng Liu, Yifei\nChen, Chengjie Sun, Sophia Katrenko, Pieter Adri-\naans, Christian Blaschke, Rafael Torres, Mariana\nNeves, Preslav Nakov, Anna Divoli, Manuel Ma˜na-\nL´opez, Jacinto Mata, and W. John Wilbur. 2008.\nOverview of BioCreative II gene mention recogni-\ntion. Genome Biology, 9:S2 – S2.\nYoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,\nTomoko Ohta, John McNaught, Sophia Ananiadou,\nand Jun’ichi Tsujii. 2005. Developing a robust part-\nof-speech tagger for biomedical text. In Panhellenic\nConference on Informatics.\nYoshimasa Tsuruoka and Jun’ichi Tsujii. 2005. Bidi-\nrectional inference with the easiest-ﬁrst strategy for\ntagging sequence data. In HLT/EMNLP.\nMarco Antonio Valenzuela-Escarcega, Ozgun Babur,\nGus Hahn-Powell, Dane Bell, Thomas Hicks, En-\nrique Noriega-Atala, Xia Wang, Mihai Surdeanu,\nEmek Demir, and Clayton T. Morrison. 2018.\nLarge-scale automated machine reading discovers\nnew cancer-driving mechanisms. In Database.\nXuan Wang, Yu Zhang, Xiang Ren, Yuhao Zhang,\nMarinka Zitnik, Jingbo Shang, Curtis P. Langlotz,\nand Jiawei Han. 2018.\nCross-type biomedical\nnamed entity recognition with deep multi-task learn-\ning. Bioinformatics.\nChih-Hsuan Wei, Yifan Peng, Robert Leaman, Al-\nlan Peter Davis, Carolyn J. Mattingly, Jiao Li,\nThomas C. Wiegers, and Zhiyong Lu. 2016.\nAs-\nsessing the state of the art in biomedical relation ex-\ntraction: overview of the BioCreative V chemical-\ndisease relation (CDR) task. Database : the journal\nof biological databases and curation, 2016.\nRalph Weischedel, Eduard Hovy, Martha Palmer,\nMitch Marcus, Robert Belvin adn Sameer Prad-\nhan, Lance Ramshaw, and Nianwen Xue. 2011.\nOntoNotes: A large training corpus for enhanced\nprocessing.\nIn Joseph Olive, Caitlin Christian-\nson, and John McCary, editors, Handbook of Natu-\nral Language Processing and Machine Translation.\nSpringer.\nYuan Zhang and David I Weiss. 2016.\nStack-\npropagation: Improved representation learning for\nsyntax. CoRR, abs/1603.06598.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2019-02-20",
  "updated": "2019-10-09"
}