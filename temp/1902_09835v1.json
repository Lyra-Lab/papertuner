{
  "id": "http://arxiv.org/abs/1902.09835v1",
  "title": "Can Meta-Interpretive Learning outperform Deep Reinforcement Learning of Evaluable Game strategies?",
  "authors": [
    "Céline Hocquette",
    "Stephen H. Muggleton"
  ],
  "abstract": "World-class human players have been outperformed in a number of complex two\nperson games (Go, Chess, Checkers) by Deep Reinforcement Learning systems.\nHowever, owing to tractability considerations minimax regret of a learning\nsystem cannot be evaluated in such games. In this paper we consider simple\ngames (Noughts-and-Crosses and Hexapawn) in which minimax regret can be\nefficiently evaluated. We use these games to compare Cumulative Minimax Regret\nfor variants of both standard and deep reinforcement learning against two\nvariants of a new Meta-Interpretive Learning system called MIGO. In our\nexperiments all tested variants of both normal and deep reinforcement learning\nhave worse performance (higher cumulative minimax regret) than both variants of\nMIGO on Noughts-and-Crosses and Hexapawn. Additionally, MIGO's learned rules\nare relatively easy to comprehend, and are demonstrated to achieve significant\ntransfer learning in both directions between Noughts-and-Crosses and Hexapawn.",
  "text": "Can Meta-Interpretive Learning outperform Deep Reinforcement Learning of\nEvaluable Game strategies?\nC´eline Hocquette1 , Stephen H. Muggleton1\n1Department of Computing, Imperial College London, London, UK\n{celine.hocquette16, s.muggleton}@imperial.ac.uk\nAbstract\nWorld-class human players have been outper-\nformed in a number of complex two person games\n(Go, Chess, Checkers) by Deep Reinforcement\nLearning systems. However, owing to tractability\nconsiderations minimax regret of a learning system\ncannot be evaluated in such games. In this paper we\nconsider simple games (Noughts-and-Crosses and\nHexapawn) in which minimax regret can be efﬁ-\nciently evaluated. We use these games to compare\nCumulative Minimax Regret for variants of both\nstandard and deep reinforcement learning against\ntwo variants of a new Meta-Interpretive Learning\nsystem called MIGO. In our experiments all tested\nvariants of both normal and deep reinforcement\nlearning have worse performance (higher cumula-\ntive minimax regret) than both variants of MIGO\non Noughts-and-Crosses and Hexapawn. Addition-\nally, MIGO’s learned rules are relatively easy to\ncomprehend, and are demonstrated to achieve sig-\nniﬁcant transfer learning in both directions between\nNoughts-and-Crosses and Hexapawn.\n1\nIntroduction\nDeep Reinforcement Learning systems have been demon-\nstrated capable of mastering two-player games such as Go\n[Silver D. et al., 2016], outperforming the strongest human\nplayers. However, these systems 1) generally require a very\nlarge training set to converge toward a good strategy, 2) are\nnot easily interpretable as they provide limited explanation\nabout how decisions are made and 3) do not provide transfer-\nability of the learned strategies to other games.\nWe demonstrate in this work how machine learning strate-\ngies as logic programs can overcome these limitations. For\nexample, an applicable strategy for playing Noughts-and-\nCrosses is to create double attacks when possible. An ex-\nample of this is shown in Figure 1. Player O executes a move\nfrom board A to board B which creates the two threats repre-\nsented in green, and results in a forced win for O. The rules\nin Figure 1 describe such a strategy. A,B and C are vari-\nables representing state descriptions which encode the board\ntogether with the active player. The rules state that a move by\nX\nO\nX\nO\nO\nX\nO\nX\nO\nA\nB\nmove/2\nwin 2(A,B):-win 2 1 1(A,B),not(win 2 1 1(B,C)).\nwin 2 1 1(A,B):-move(A,B),not(win 1(B,C)).\nwin 1(A,B):- move(A,B),won(B).\nFigure 1: Noughts and Crosses: example of optimal move for O\nfrom board A to board B. For all moves of X from board B, O can\nwin in one move. This statement can be expressed with the logic\nprogram presented: O makes a move such that X cannot\nimmediately win nor make a move that blocks O.\nthe active player from A to B is a winning move if the oppo-\nnent cannot immediately win and the opponent cannot make\na move to prevent an immediate win by the active player.\nThese rules provide an understandable strategy for winning\nin two moves. Moreover, these rules are transferable to more\ncomplex games as they are generally true for describing dou-\nble attacks. We introduce in this article a new logical sys-\ntem called MIGO (Meta-Interpretive Game Ordinator)1 de-\nsigned for learning two player game optimal strategies of the\nform presented in Figure 1. It beneﬁts from a strong inductive\nbias which provides the capability to learn efﬁciently from a\nfew examples of games played. Learned hypotheses are pro-\nvided in a symbolic form, which allows their interpretation.\nMoreover, learned strategies are generally true for all two-\nplayer games, which provides straightforward transferability\nto more complex games.\nMIGO uses Meta-Interpretive Learning (MIL), a recently\ndeveloped Inductive Logic Programming (ILP) framework\nthat supports predicate invention, the learning of recursive\nprograms [Muggleton et al., 2015; Muggleton et al., 2014]\nand Abstraction [Cropper and Muggleton, 2016a]. MIGO ad-\nditionally supports Dependent Learning [Lin et al., 2014].\nThe learning operates in a staged fashion: simple deﬁnition\nare ﬁrst learned and added to the background knowledge [Lin\net al., 2014], allowing them to be reused during further learn-\ning tasks, and thus to build up more and more complex def-\ninitions. For instance, MIGO would ﬁrst learn a simple deﬁ-\n1From the children’s game-playing phrase My go! and the literal\ntranslation into English of the French word Ordinateur which means\ncomputer.\narXiv:1902.09835v1  [cs.AI]  26 Feb 2019\nnition of win 1/1 for winning in one move. Next, a predicate\nwin 2/1 describing the action of winning in two moves can be\nbuilt from win 1/1 as shown in Figure 1.\nTo evaluate performance we consider two evaluable games\n(Noughts and Crosses and Hexapawn). Our results demon-\nstrate that substantially lower Cumulative Minimax Regret\ncan be achieved by MIGO compared to variants of reinforce-\nment learning.\nOur contributions are the introduction of a system for\nlearning optimal two-player-game strategies (Section 3) and\nthe description of its implementation (Section 4). We demon-\nstrate experimentally it converges faster than reinforcement\nlearning systems and that learned strategies are transferable\nto more complex games (Section 5).\n2\nRelated Work\n2.1\nLearning game strategies\nVarious early approaches to game strategies [Shapiro and\nNiblett, 1982; J.R. Quinlan, 1983] used the decision tree\nlearner ID3 to classify minimax depth-of-win for positions\nin chess end games. These approaches used a set of care-\nfully selected board attributes as features. Conversely, MIGO\nis provided with a set of three relational primitives (move/2,\nwon/1, drawn/1) representing the minimal information a hu-\nman would expect to know before playing a two person game.\nAn ILP approach learned optimal chess endgame strategies\nat depth 0 or 1 [Bain and Muggleton, 1995]. Examples are\nboard positions taken from a database. Conversely, MIGO\nlearns from game play.\n2.2\nReinforcement Learning\nReinforcement Learning considers the task of identifying an\noptimal agent policy to maximise the cumulative reward per-\nceived by an agent. MENACE (Matchbox Educable Noughts\nAnd Crosses Engine) [Michie, 1963] was the world’s earli-\nest reinforcement learning system and was speciﬁcally de-\nsigned to learn to play Noughts-and-Crosses. An early man-\nual version of MENACE used a stack of matchboxes, one for\neach accessible board position. Each box contained coloured\nbeads representing possible moves. Moves were selected by\nrandomly drawing a bead from the current box. After having\ncompleted a game, MENACE’s punishment or reward con-\nsisted of subtracting or adding beads according to the out-\ncome of the game. This modiﬁed the probability of the se-\nlected move being played in the position [Brooks, 2017].\nHER (Hexapawn Educational Robot) [Gardner, 1962] is a\nsimilar system for the game of Hexapawn.\nMore generally, Q-learning [Watkins, 1989] addresses the\nproblem of learning an optimal policy from delayed rewards\nand by trial and error. The learned policy takes the form of\nQ-values for each actions available from a state. A guaran-\ntee of asymptotic convergence to optimal behaviour has been\nproved [Watkins and Dayan, 1992].\nDeep Q-learning [Mnih et al., 2015] is an extension that\nuses a deep convolutional neural network to approximate the\ndifferent Q-values for each actions given a particular state.\nIt provides better scalability which has been demonstrated\nthrough a diverse range of tasks from the Atari 2600 games.\nHowever, this framework generally requires the execution of\nmany games to converge. Moreover, the learned strategy is\nimplicitly encoded into the Q-value parameters, which do not\nprovide interpretability. In [Garnelo et al., 2016], a hybrid\nneural-symbolic system is described which address some of\nthese drawbacks. A neural back-end transforms images into a\nsymbolic representation and generates features. A symbolic\nfront end performs action selection. Conversely, MIGO is\nbased upon a purely symbolic approach and the number of\nprimitives considered is reduced.\n2.3\nRelational Reinforcement Learning\nRelational reinforcement learning [Dˇzeroski et al., 2001] is a\nreinforcement learning framework where states, actions and\npolicies are represented relationally. It beneﬁts from back-\nground knowledge and declarative bias. It learns a Q-function\nusing a relational regression tree algorithm. Conversely, the\nlearning framework MIGO is not based upon the identiﬁca-\ntion of Q-values but aims at deriving hypotheses describing\nan optimal strategy. Relational reinforcement learning also\nprovides the ability to carry over the policies learned in sim-\nple domains to more complex situations. However, most sys-\ntems aim at learning single agent policy and, in contrast to\nMIGO, are not designed to learn to play two person games.\n3\nTheoretical Framework\n3.1\nCredit Assignment\nOne can evaluate the success of a game by looking at its out-\ncome. However, a problem arises for assigning the reward\nto the various moves performed. Reinforcement learning sys-\ntems usually tackle this so-called Credit Assignment Problem\nby adjusting parameter values associated with the moves re-\nsponsible for the reward observed. We introduce theorems for\nidentifying moves that are necessarily positive examples for\nthe task of winning and drawing.\nWe assume the learner P1 plays against an opponent P2\nthat follows an optimal strategy and that the game starts from\na randomly chosen initial board B. We consider the following\nordering over the different outcomes for P1 and demonstrate\nthe lemma below:\nwon ≻drawn ≻loss\nLemma 3.1 The expected outcome of P1 can only decrease\nduring a game.\nProof P2 plays optimally and therefore any move of P2\nmaintains or lowers the expected outcome. Therefore P1 can-\nnot increase its outcome.\nWe demonstrate the Theorems below given these assumptions\nand Lemma 3.1:\nTheorem 3.2 If the outcome is won for P1, then every move\nof P1 is a positive example for the task of winning.\nProof Suppose there exists a move of P1 from the board B1\nto the board B2 within the game sequence that is a negative\nexample for the task of winning. Then the expected outcome\nof B1 is won and the expected outcome of B2 is strictly lower\nwith respect to the order ≻. Then, following Lemma 3.1 the\noutcome of the game is strictly lower than won, which leads\nto contradiction with the outcome observed.\nTheorem 3.3 We additionally assume an accurate strategy\nSW for winning has been learned by the learner P1. If the\noutcome of the game is drawn and if the execution of SW\nfrom B fails, then any move played by P1 or P2 is a positive\nexample for the task of drawing.\nProof The initial position does not have an expected out-\ncome of won for P2 otherwise the outcome would be won\nfor P2 since it plays optimally. The initial position is not an\nexpected outcome of win for P1 by assumption. Therefore,\nthe expected outcome of B is drawn. It follows from Lemma\n3.1 that every position reached during the game has an ex-\npected outcome of drawn and that every move of both players\nis a positive example for the task of drawing.\nTheorems 3.2 and 3.3 demonstrate that for an outcome of win\nfor P1 or drawn and because the opponent plays optimally,\nthe expected outcome is necessarily maintained as won or\ndrawn respectively. This cannot be further generalised to P2’s\nmoves: an outcome of won for P2 might be the consequence\nof a mistake of P1 who does not play optimally.\nOne should also highlight the fact that Theorems 3.2 and\n3.3 do not provide any negative examples for win/2 or draw/2,\nas these theorems do not help to evaluate moves for which the\nexpected outcome decreases. Practically, the learning system\nconsidered learns from positive examples only.\n3.2\nGame evaluation\nGiven Theorems 3.2 and 3.3, the opponent chosen is an op-\ntimal player following the minimax algorithm.\nBoth for\nNoughts-and-Crosses and Hexapawn, and more generally for\nmost fair two player games, the opponent can always ensure a\ndraw from the initial board, which leaves no opportunities for\nthe learner to win. To ensure possibilities of winning, we start\nthe game from a board randomly sampled from the set of one\nmove ahead accessible boards; this set provides different ex-\npected outcomes for the games considered. Then, the actual\noutcome relies on both the initial board and the sequence of\nmoves performed. We deﬁne the minimax regret as follows:\nDeﬁnition 3.4:\nThe minimax regret of a game is the differ-\nence between the minimax expected outcome of the initial\nboard and the actual outcome of the game.\nPractically, the minimax expected outcome of a board can\nbe evaluated from a minimax database computed beforehand.\nDeﬁnition 3.4 provides an absolute measure to evaluate the\nperformances of a learning algorithm as it does not rely on the\nchoice of initial board. Thereafter, we evaluate the cumulative\nminimax regret to compare different learning systems.\n3.3\nMeta-Interpretive Learning (MIL)\nThe system MIGO introduced in this work is a MIL system.\nMIL is a form of ILP [Muggleton et al., 2014; Muggleton\nand Lin, 2013]. The learner is given a set of examples E and\nbackground knowledge B composed of a set of Prolog deﬁ-\nnitions Bp and metarules M such that B = Bp ∪M. The aim\nis to generate a hypothesis H such that B, H |= E. The proof\nAlgorithm 1 MIGO Algorithm\nInput: Positive examples for win k and draw k\nOutput: Strategy for win k and draw k\n1: for k in [1,Depth] do\n2:\nfor each example of win k/2 do\n3:\none shot learn a rule and add it to the BK\n4:\nend for\n5:\nLearn win k/2 and add it to the BK\n6: end for\n7: for k in [1,Depth] do\n8:\nfor each example of draw k/2 do\n9:\none shot learn a rule and add it to the BK\n10:\nend for\n11:\nLearn draw k/2 and add it to the BK\n12: end for\nis based upon an adapted Prolog meta-interpreter. It ﬁrst at-\ntempts to prove the examples considered deductively. Failing\nthis, it uniﬁes the head of a metarule with the goal, and saves\nthe resulting meta-substitution. The body and then the other\nexamples are similarly proved. The meta-substitutions recov-\nered for each successful proofs are saved and can be used in\nfurther proofs by substituting them into their corresponding\nmetarules. Key features of MIL are that it supports predicate\ninvention, the learning of recursive programs and Abstraction\n[Cropper and Muggleton, 2016a]. In the following, we use\nthe MIL system Metagol [Cropper and Muggleton, 2016b].\n3.4\nMIGO algorithm\nWe present within this section details of the MIGO algorithm.\nLearning from positive examples\nTheorems 3.2 and 3.3\nprovide a way of assigning positive labels to moves. There-\nfore, the learning is based upon positive examples only. This\nis possible because of Metagol’s strong language bias and\nability to generalise from a few examples only. However, one\npitfall is the risk of over-generalisation due to the absence of\nnegative examples.\nDependent Learning\nFor successive values of k a se-\nries of inter-related deﬁnitions are learned for predicates\nwin k(A, B) and draw k(A, B).\nThese predicates deﬁne\nmaintenance of minimax win and draw in k-ply when moving\nfrom position A to B. The learning algorithm is presented as\nAlgorithm 1, each action ’learn’ represents a call to Metagol.\nThis approach is related to Dependent Learning [Lin et al.,\n2014]. The idea is to ﬁrst learn low-level predicates. They are\nderived from single examples with limited complexity. The\ndeﬁnitions are added into the background knowledge such\nthat they can be used in further deﬁnitions. The process it-\nerates until no further predicates can be learned.\nMixed Learning and Separated Learning\nTheorem 3.3\nassigns positive labels to draw/2 examples assuming a win-\nning strategy SW has already been learned. In practice, we\ndistinguish two variants of MIGO:\n1. Separated Learning: win/2 and draw/2 are learned in two\nstages. Win/2 is ﬁrst learned. When a strategy for win/2\nis stable for a given number of iterations the learner starts\nlearning draw/2.\nName\nMetarule\npostcond\nP(A, B) ←Q(A, B), R(B).\nnegation\nP(A, B) ←Q(A, B), not(R(B, C)).\nTable 1: Metarules considered: the letters P,Q and R denote exis-\ntentially quantiﬁed higher order variables. The letters A, B and C\ndenote universally quantiﬁed ﬁrst-order variables.\n2. Mixed Learning: win/2 and draw/2 are learned simultane-\nously. Examples of draw/2 are ﬁrst evaluated with the cur-\nrent strategy for win/2. If this latter is updated, examples\nof draw/2 are re-tested against the new version of win/2.\n4\nImplementation\n4.1\nRepresentation\nA board B is encoded as a 9-vector of marks from the set\n{O, X, Empty}. States s(B, M) are atoms that represent the\ncurrent board B and the active player M.\n4.2\nPrimitives and Metarules\nThe language belongs to the language class H2\n2, which is\nthe subset of Datalog logic programs with predicates of arity\nat most 2 and at most 2 literals in the body of each clause.\nLearned programs are formed of dyadic predicates, repre-\nsenting actions, and monadic predicates, representing ﬂuents.\nThe background knowledge contains a general move genera-\ntor move/2, which is an action that modiﬁes a state s(B, M)\nby executing a move on board B and updating the active\nplayer M. Move/2 only holds for valid moves; in other words,\nthe learner already knows the rules of the game. The back-\nground knowledge also contains two ﬂuents: a won classi-\nﬁer won/1 and a drawn classiﬁer drawn/1. They hold when a\nboard is respectively won or drawn.\nWe consider the metarules postcond and negation de-\nscribed in Table 1. The metarule negation expresses the log-\nical negation for primitive predicates, and is implemented as\nnegation as failure. This form of Negation does not introduce\ninvented predicates in Metagol.\n4.3\nExecution of the strategy\nFor each rule learned for win i and draw i a clause of the form\nbelow is added to the background knowledge:\nwin(A,B) :- win_i(A,B).\ndraw(A,B) :- draw_i(A,B).\nWhen executing a strategy described with a hypothesis H,\nthe move performed is the ﬁrst one consistent with H. Prac-\ntically, it ﬁrst attempts to prove win i/2 for increasing values\nfor i. Failing that, it attempts to prove draw i/2 for increasing\nvalues for i. If these proofs fail, a move is selected at random\namong the possible moves.\nThe opponent plays a deterministic minimax strategy that\nyields the best outcome in the minimum number of moves.\n4.4\nLearning a strategy\nAt the end of a game, the outcome is observed and the se-\nquence of visited boards is divided into moves. The depth\nof each board is measured as the number of full moves un-\ntil the end of the game in the observed sequence. Moves are\nFigure 2: Initial boards for Hexapawn3 and Hexapawn4\nOX\nHexapawn3\nHexapawn4\nMIGO mixed learning\n1.5.10−1\n3.0.10−3\n3.9\nMIGO separated learning\n8.9.10−2\n2.8.10−3\n3.8\nMENACE / HER\n1.5.10−3\n2.7.10−4\n/\nQ-Learning\n2.3.10−1\n1.9 .10−3\n2.7 .10−1\nDeep Q-Learning\n2.4.10−1\n1.7.10−2\n2.1 .10−1\nTable 2: Average CPU time (seconds) of one iteration\nadded to the set of positive examples for win k/2 or draw k/2\nif they satisfy Theorems 3.2 or 3.3. Strategies are relearned\nfrom scratch after each game using the MIGO algorithm pre-\nsented above. One additional constraint is added such that\ndraw/2 cannot be learned before win/2 since this would cause\nthe learner to always draw and never win.\n5\nExperiments\n5.1\nExperimental Hypothesis\nThis section describes experiments which evaluate the perfor-\nmance of MIGO for the task of learning optimal two player\ngame strategies2. We use the games of Noughts-and-Crosses\nand a variant of the game of Hexapawn [Gardner, 1962].\nMIGO is compared against the reinforcement learning sys-\ntems MENACE / HER, Q-learning and Deep Q-learning. Ac-\ncordingly, we investigate the following null hypotheses:\nNull Hypothesis 1:\nMIGO cannot converge faster than\nMENACE / HER, Q-learning and Deep Q-learning for learn-\ning optimal two-player game strategies.\nWe additionally test the ability of MIGO to transfer learned\nstrategies to more complex games, and thus verify the follow-\ning null hypothesis:\nNull Hypothesis 2:\nMIGO cannot transfer the knowledge\nlearned during a previous task to a more complex game.\n5.2\nConvergence\nMaterials and Methods\nCommon\nWe provide MIGO, Menace / HER and Q-\nlearning with the same set of initial boards randomly sampled\nfrom the set of one-full-move-ahead positions - positions that\nresult from one move of each player. The systems studied\nplay games starting from these initial boards, and they face\nthe same deterministic minimax player. Therefore, the only\nvariable in the experiments is the learning system. It is as-\nsumed the learner always starts the game. The performance\nis evaluated in terms of cumulative minimax regret.\nWe follow an implementation of Tabular Q-learning available\nfrom [Qle, 2015] and used the parameter values which were\nprovided for the Q-learning algorithm: the exploration rate\nis set to 0; the initial q-values are 1; the discount factor is\n2Code for these experiments available at\nhttps://github.com/migo19/migo.git\nγ = 0.9 and the learning rate α = 0.3.\nSimilarly, we follow an implementation of Deep Q-learning\navailable from [Dee, 2016]. The provided parameters were\nused: the discount factor is set to 0.8; the regularization\nstrength to 0.01; the target network update rate to 0.01; the\ninitial and ﬁnal exploration rate are 0.6 and 0.1 respectively.\nThe results presented here have been averaged oven 40 runs\nfor Hexapawn3 and 20 for Noughts and Crosses. Average\nrunning times are presented in Figure 2.\nNoughts-and-Crosses\nThe set of initial boards comprises\n12 boards taking into account rotations and symmetries of the\nboard. Among them 7 are expected win, and 5 are expected\ndraw. Therefore the worst case regret of a random player is\n1.58. The counter for starting learning draw/2 is set to 10.\nHexapawn\nHexapawn’s initial board is represented in Fig-\nure 2. The goal of each player is to advance one of their pawns\nto the opposite end. Pawns can move one square forward if\nthe next square is empty or capture another pawn one square\ndiagonally ahead of it [Gardner, 1962]. Rules have been mod-\niﬁed: the game is said to be drawn when the current player\nhas no legal move.Thereafter, we refer to Hexapawn3 and\nHexapawn4 for the game of Hexapawn in dimensions 3 by\n3 and 4 by 4 respectively. The set of initial boards comprises\n5 boards taking into account the vertical symmetry. Among\nthem, 3 are expected draw and 2 are expected win. There-\nfore the average worst case regret is 1.4. As the dimensions\nare smaller for Hexapawn3 than for Noughts and Crosses, the\ncounter for starting learning draw/2 is set to 5.\nResults\nResults are presented in Figure 3 and show that MIGO con-\nverges faster than MENACE / HER, Q-learning and Deep Q-\nlearning for both games, refuting null hypothesis 1. As the\nmaximum depth is larger for Noughts-and-Crosses than for\nHexapawn3, all systems require more iterations to converge.\nDeep Q-learning performs worst for Hexapawn3 as the pa-\nrameters selected are the ones tuned for Noughts and Crosses\nand might not be adapted. For both games, mixed learning\nhas lower cumulative regret than separated learning, because\nmixed learning does not waste any examples of draw/2 from\nthe initial period in which win/2 is being learned and it does\nnot stop learning win/2 after the initial period.\nRules learned by MIGO are presented in Figure 3. MIGO\nconverges toward this full set of rules when playing Noughts-\nand-Crosses.\nBecause the maximum depth of Hexapawn3\nis 2, MIGO learns up to the double line when playing\nHexapawn3. If unfolding, the ﬁrst rule can be translated into\nEnglish as: State A is won at depth 1 if there exists a move\nfrom A to B such that B is won. Similarly, winning at depth\n2 can be described with the following statement: State A is\nwon at depth 2 if there exists a move of the current player\nfrom A to B such that B is not immediately won for the op-\nponent and such that the opponent cannot make a move from\nB to C to prevent the current player from immediately win-\nning. This statement is similar to the one presented in section\n1. Finally, winning at depth 3 can be explained as: State A\nis won at depth 3 for the current player if there exists a move\nfrom A to B such that B is not won for the opponent in 1 or 2\nmoves and such that the opponent cannot make a move from\n(a) Noughts-and-Crosses\n(b) Hexapawn3\nFigure 3: Cumulative Minimax Regret versus the number of\niterations for Noughts-and-Crosses and Hexapawn3\nB to C to prevent the current player from winning in 1 or 2\nmoves. None of the other systems studied can provide similar\nexplanation about the moves chosen. Rules are built on top\non each other, the calling diagram in Figure 4 represents the\ndependencies between each learned predicates.\nDiscussion\nMENACE, HER and Q-learning encode the knowledge into\nthe parameters (number of beads or Q-values). The states and\ntheir parameters are unique for each board. This results in\na weaker generalisation ability: knowledge cannot be trans-\nferred from one state to another. Deep Q-learning can provide\nsome generalisation ability; however, it is only visible after a\nlarge number of iterations. Conversely, MIGO generalises the\nboards characteristics and each rule learned describes a set of\nstates, which considerably reduces the number of parameters\nDepth\nRule\n1\nwin 1(A,B):-win 1 1 1(A,B),won(B).\nwin 1 1 1(A,B):-move(A,B),won(B).\ndraw 1(A,B):-draw 1 1 3(A,B),not(win 1(B,C)).\ndraw 1 1 3(A,B):-move(A,B),not(win 1(B,C)).\n2\nwin 2(A,B):-win 2 1 1(A,B),not(win 2 1 1(B,C)).\nwin 2 1 1(A,B):-move(A,B),not(win 1(B,C)).\ndraw 2(A,B):-draw 2 1 1(A,B),not(win 1(B,C)).\ndraw 2 1 1(A,B):-draw 1(A,B),not(win 1(B,C)).\n3\nwin 3(A,B):-win 3 1 1(A,B),not(win 3 1 1(B,C)).\nwin 3 1 1(A,B):-win 2 1 1(A,B),not(win 2(B,C)).\ndraw 3(A,B):-draw 3 1 10(A,B),not(draw 1 1 12(B,C)).\ndraw 3 1 10(A,B):-draw 2(A,B),not(draw 1 1 12(B,C)).\n4\ndraw 4(A,B):-draw 4 1 2(A,B),not(draw 1 1 12(B,C)).\ndraw 4 1 2(A,B):-draw 3(A,B),not(draw 1 1 12(B,C)).\nTable 3: Example of rules learned for Noughts-and-Crosses (all)\nand Hexapawn3 (above the double line)\nDepth\n1\nwin_1/2\ndraw_1/2\nwin_2/2\ndraw_2/2\nwin_3/2\ndraw_3/2\n2\n3\nwin_1_1_1/2\nwin_2_1_1/2\nwin_3_1_1/2\ndraw_3_1_10/2\ndraw_2_1_1/2\ndraw_1_1_3/2\ndraw_4/2\n4\ndraw_4_1_2/2\nFigure 4: Calling diagram of Learned Strategies\nto learn and therefore the number of examples required.\nThe reinforcement learning systems tested have an implicit\nrepresentation of the problem. For instance, no geometri-\ncal concepts have been encoded. Conversely, MIGO beneﬁts\nfrom a background knowledge which describes the notion of\nwinning, and from which it can extract a notion of alignment.\nThis allows a degree of explanation.\nThe running time increases rapidly with the state dimen-\nsions for MIGO. This reﬂects the increasing execution time of\nthe learned strategy which is not efﬁcient since a deep evalu-\nation requires extensive evaluation to decide whether a move\nleads to a win.\nMENACE / HER are speciﬁcally tailored for theses games.\nConversely, Q-learning and Deep Q-learning are a general ap-\nproaches that can tackle a wide range of tasks, providing that\nparameters are tuned. MIGO beneﬁts from underlying as-\nsumptions which reduce its range of applications. However,\nthe primitives are abstract enough to allow playing a wide\nrange of games and support transferring knowledge from one\ngame to another as we will demonstrate in the next section.\n5.3\nTransferability\nMaterials and Methods\nStrategies are ﬁrst learned for\nHexapawn3 and Noughts-and-Crosses respectively. Strate-\ngies are learned with mixed learning and for 100 iterations\nfor Hexapawn3 and 200 iterations for Noughts and Crosses.\nThe resulting learned program is transferred to the next learn-\ning task, which is learning a strategy for Hexapawn4. Results\nhave been averaged over 20 runs.\nResults\nThe results presented in Figure 5 show that trans-\nferring the knowledge learned in a previous task help to con-\nverge faster, thus refuting null hypothesis 2. Since the learner\nbeneﬁts from an initial knowledge, it is substantially im-\nproved compared to an initial random player.\n6\nConclusion and Future Work\nThis article introduces a novel logical system named MIGO\nfor learning two-player-game strategies. It is based upon the\nMIL framework. This system distinguishes itself from clas-\nsical reinforcement learning by the way that it addresses the\nCredit Assignment Problem. Our experiments have demon-\nstrated that MIGO achieves lower Cumulative Minimax Re-\ngret compared to Deep and classical Q-Learning. Moreover,\nwe have demonstrated that strategies learned with MIGO are\n(a) Hexapawn3 to Noughts and Crosses\n(b) Noughts and Crosses to Hexapawn4. Similar results\nare obtained from Hexapawn3 to Hexapawn4.\nFigure 5: Transfer Learning\ntransferable to more complex games. Strategies have also\nbeen shown to be relatively easy to comprehend.\nFuture Work\nOne limitation of the system presented is the\nrisk of over-generalisation, observable in the strategy learned.\nWe will further extend the implementation to include a more\nthorough context for learning from positive examples such as\nthe one presented in [Muggleton, 1996].\nThe running time suggests that the execution time of the\nlearned strategies increases with the dimensions of the states,\nwhich limits scalability. We will further extend MIGO to op-\ntimise the execution time for hypothesised programs. Selec-\ntion of hypotheses could be performed following the idea de-\nscribed in [Cropper and Muggleton, 2018].\nAnother limitation to scalability is the restriction imposed\nby the initial assumptions. The current version of MIGO re-\nquires an optimal opponent, which is intractable in large di-\nmensions. We will further extend this system by relaxing\nTheorems 3.2 and 3.3 and weakening the optimal opponent\nassumption. A solution could be to learn from self-play.\nBecause MIGO beneﬁts from a strong declarative bias, the\nsample complexity is much improved compared to other ap-\nproaches. However, most of the examples are wasted as no\nlabels could be attributed. We plan to evaluate whether Active\nLearning could further help to reduce the sample complexity.\nThe learner could choose an initial board to start the game,\nthe choice being based upon an information gain criterion.\nAlthough learned strategies provide a certain form of ex-\nplanation, we will further study how comprehensible learned\nstrategies are. We will evaluate whether MIGO can fullﬁll\nMichie’s Machine Learning Ultra Strong criterion, which re-\nquires the learner to be able to teach the learned hypothesis to\na human [Muggleton et al., 2018].\nDespite these limitations, we believe the novel system in-\ntroduced in this work opens exciting new avenues for ma-\nchine learning game strategies.\nReferences\n[Bain and Muggleton, 1995] M. Bain and S. H. Muggleton.\nMachine intelligence 13. pages 291–309, 1995.\n[Brooks, 2017] R. Brooks.\nFoR & AI: Machine learning\nexplained, August 2017. https://rodneybrooks.com/forai-\nmachine-learning-explained/.\n[Cropper and Muggleton, 2016a] A. Cropper and S.H. Mug-\ngleton. Learning higher-order logic programs through ab-\nstraction and invention. In IJCAI 2016, pages 1418–1424,\n2016.\n[Cropper and Muggleton, 2016b] A.\nCropper\nand\nS.H.\nMuggleton.\nMetagol\nsystem.\nhttps://github.com/metagol/metagol, 2016.\n[Cropper and Muggleton, 2018] A. Cropper and S.H. Mug-\ngleton. Learning efﬁcient logic programs. Machine Learn-\ning, Apr 2018.\n[Dee, 2016] Solving tic-tac-toe using deep reinforcement\nlearning. https://github.com/yanji84/tic-tac-toe-rl, 2016.\n[Dˇzeroski et al., 2001] S. Dˇzeroski,\nL. De Raedt,\nand\nK. Driessens. Relational reinforcement learning. Machine\nLearning, 43(1):7–52, Apr 2001.\n[Gardner, 1962] M. Gardner.\nMathematical games, scien-\ntiﬁc american. reprinted in The Unexpected Hanging and\nOther Mathematical Diversions, page 93, 03 1962.\n[Garnelo et al., 2016] M. Garnelo, K. Arulkumaran, and\nM. Shanahan.\nTowards deep symbolic reinforcement\nlearning. CoRR, abs/1609.05518, 2016.\n[J.R. Quinlan, 1983] J.R. J.R. Quinlan.\nLearning Efﬁcient\nClassiﬁcation Procedures and Their Application to Chess\nEnd Games, pages 463–482. Springer Berlin Heidelberg,\nBerlin, Heidelberg, 1983.\n[Lin et al., 2014] D. Lin, E. Dechter, K. Ellis, J.B. Tenen-\nbaum, and S.H. Muggleton. Bias reformulation for one-\nshot function induction. In In Proceedings of the 23rd Eu-\nropean Conference on Artiﬁcial Intelligence (ECAI 2014),\npages 525–530,. IOS Press, 2014.\n[Michie, 1963] D. Michie. Experiments on the mechaniza-\ntion of game-learning part i. characterization of the model\nand its parameters. The Computer Journal, Volume 6, Is-\nsue 3, pages 232–236, 1963.\n[Mnih et al., 2015] V. Mnih, K. Kavukcuoglu, and D. Silver\net al.\nHuman-level control through deep reinforcement\nlearning. Nature, 518:529–533, 02 2015.\n[Muggleton and Lin, 2013] S.H. Muggleton and D. Lin.\nMeta-interpretive learning of higher-order dyadic datalog:\nPredicate invention revisited.\nIn In Proceedings of the\n23rd International Joint Conference Artiﬁcial Intelligence,\npages 1551–1557, 2013.\n[Muggleton et al., 2014] S.H Muggleton, D. Lin, N. Pahlavi,\nand A. Tamaddoni-Nezhad.\nMeta-interpretive learning:\napplication to grammatical inference. Machine Learning\n94, pages 25–49, 2014.\n[Muggleton et al., 2015] S.H.\nMuggleton,\nD.\nLin,\nand\nA. Tamaddoni-Nezhad.\nMeta-interpretive learning of\nhigher-order dyadic datalog: Predicate invention revisited.\nMachine Learning, 100(1):49–73, 2015.\n[Muggleton et al., 2018] S.H.\nMuggleton,\nU.\nSchmid,\nC. Zeller, A. Tamaddoni-Nezhad, and T. Besold. Ultra-\nstrong machine learning: comprehensibility of programs\nlearned with ilp. Machine Learning, 107(7):1119–1140,\nJuly 2018.\n[Muggleton, 1996] S.H. Muggleton. Learning from positive\ndata. In S.H. Muggleton, editor, Proceedings of the Sixth\nInternational Workshop on Inductive Logic Programming\n(Workshop-96), LNAI 1314, Springer-Verlag, pages 358–\n376, 1996.\n[Qle, 2015] Q-learning tic-tac-toe.\nhttps://gist.github.com/\nfheisler/430e70fa249ba30e707f, 2015.\n[Shapiro and Niblett, 1982] A. Shapiro and T. Niblett. Auto-\nmatic induction of classiﬁcation rules for a chess endgame.\nIn M.R.B. Clarke, editor, Advances in Computer Chess,\nvolume 3, pages 73–91. Pergammon, Oxford, 1982.\n[Silver D. et al., 2016] Huang A. Silver D., C. Maddison,\nA. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser,\nI. Antonoglou, V. Panneershelvam, M. Lanctot, S. Diele-\nman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever,\nT. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and\nD. Hassabis. Mastering the game of go with deep neural\nnetworks and tree search. 529:484–489, 01 2016.\n[Watkins and Dayan, 1992] C. Watkins and P. Dayan.\nQ-\nlearning. Machine Learning, 8(3):279–292, May 1992.\n[Watkins, 1989] C. Watkins. Learning from delayed rewards,\nphd thesis. 1989.\n",
  "categories": [
    "cs.AI",
    "cs.LG"
  ],
  "published": "2019-02-26",
  "updated": "2019-02-26"
}