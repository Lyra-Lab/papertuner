{
  "id": "http://arxiv.org/abs/1903.00535v1",
  "title": "Unsupervised Tracklet Person Re-Identification",
  "authors": [
    "Minxian Li",
    "Xiatian Zhu",
    "Shaogang Gong"
  ],
  "abstract": "Most existing person re-identification (re-id) methods rely on supervised\nmodel learning on per-camera-pair manually labelled pairwise training data.\nThis leads to poor scalability in a practical re-id deployment, due to the lack\nof exhaustive identity labelling of positive and negative image pairs for every\ncamera-pair. In this work, we present an unsupervised re-id deep learning\napproach. It is capable of incrementally discovering and exploiting the\nunderlying re-id discriminative information from automatically generated person\ntracklet data end-to-end. We formulate an Unsupervised Tracklet Association\nLearning (UTAL) framework. This is by jointly learning within-camera tracklet\ndiscrimination and cross-camera tracklet association in order to maximise the\ndiscovery of tracklet identity matching both within and across camera views.\nExtensive experiments demonstrate the superiority of the proposed model over\nthe state-of-the-art unsupervised learning and domain adaptation person re-id\nmethods on eight benchmarking datasets.",
  "text": "ACCEPTED TO APPEAR IN IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n1\nUnsupervised Tracklet Person Re-IdentiÔ¨Åcation\nMinxian Li, Xiatian Zhu, and Shaogang Gong\nAbstract‚ÄîMost existing person re-identiÔ¨Åcation (re-id) methods rely on supervised model learning on per-camera-pair manually\nlabelled pairwise training data. This leads to poor scalability in a practical re-id deployment, due to the lack of exhaustive identity\nlabelling of positive and negative image pairs for every camera-pair. In this work, we present an unsupervised re-id deep learning\napproach. It is capable of incrementally discovering and exploiting the underlying re-id discriminative information from automatically\ngenerated person tracklet data end-to-end. We formulate an Unsupervised Tracklet Association Learning (UTAL) framework. This is by\njointly learning within-camera tracklet discrimination and cross-camera tracklet association in order to maximise the discovery of\ntracklet identity matching both within and across camera views. Extensive experiments demonstrate the superiority of the proposed\nmodel over the state-of-the-art unsupervised learning and domain adaptation person re-id methods on eight benchmarking datasets.\nIndex Terms‚ÄîPerson Re-IdentiÔ¨Åcation; Unsupervised Tracklet Association; Trajectory Fragmentation; Multi-Task Deep Learning.\n!\n1\nINTRODUCTION\nP\nERSON re-identiÔ¨Åcation (re-id) aims to match the un-\nderlying identity classes of person bounding box im-\nages detected from non-overlapping camera views [1]. In\nrecent years, extensive research has been carried out on\nre-id [2,3,4,5,6,7]. Most existing person re-id methods, in\nparticular neural network deep learning models, adopt the\nsupervised learning approach. Supervised deep models as-\nsume the availability of a large number of manually labelled\ncross-view identity matching image pairs for each camera pair.\nThis enables deriving a feature representation and/or a\ndistance metric function optimised for each camera-pair.\nSuch an assumption is inherently limited for generalising a\nperson re-id model to many different camera networks. This\nis because, exhaustive manual identity (ID) labelling of pos-\nitive and negative person image pairs for every camera-pair\nis prohibitively expensive, given that there are a quadratic\nnumber of camera pairs in a surveillance network.\nIt is no surprise that person re-id by unsupervised learning\nbecome a focus in recent research. In this setting, per-\ncamera-pair ID labelled training data is no longer required\n[8,9,10,11,12,13,14,15,16]. However, existing unsupervised\nlearning re-id models are signiÔ¨Åcantly inferior in re-id accu-\nracy. This is because, lacking cross-view pairwise ID labelled\ndata deprives a model‚Äôs ability to learn strong discrimina-\ntive information. This nevertheless is critical for handling\nsigniÔ¨Åcant appearance change across cameras.\nAn alternative approach is to leverage jointly (1) unla-\nbelled data from a target domain which is freely available,\ne.g. videos of thousands of people travelling through a\ncamera view everyday in a public scene, and (2) pair-\nwise ID labelled datasets from independent source do-\nmains [17,18,19,20]. The main idea is to Ô¨Årst learn a ‚Äúview-\n‚Ä¢\nMinxian Li is with the School of Electronic Engineering and Computer\nScience, Queen Mary University of London, London E1 4NS, UK. E-mail:\nm.li@qmul.ac.uk.\n‚Ä¢\nXiatian Zhu is with Vision Semantics Limited, London E1 4NS, UK.\nE-mail: eddy@visionsemantics.com.\n‚Ä¢\nShaogang Gong is with the School of Electronic Engineering and Com-\nputer Science, Queen Mary University of London, London E1 4NS, UK.\nE-mail: s.gong@qmul.ac.uk.\ninvariant‚Äù representation from ID labelled source data, then\nadapt the pre-learned model to a target domain by using\nonly unlabelled target data. This approach makes an implicit\nassumption that, the source and target domains share some\ncommon cross-view characteristics so that a view-invariant\nrepresentation can be estimated. This is not always true.\nIn this work, we consider a pure unsupervised person\nre-id deep learning problem. That is, no ID labelled train-\ning data are assumed, neither cross-view nor within-view\nID labelling. Although this learning objective shares some\nmodelling spirit with two recent domain transfer models\n[17,19], both those models do require suitable person ID\nlabelled source domain training data, i.e. visually similar\nto the target domain. SpeciÔ¨Åcally, we consider unsupervised\nre-id model learning by jointly optimising unlabelled person\ntracklet data within-camera view to be more discriminative\nand cross-camera view to be more associative end-to-end.\nOur contributions are: We formulate a novel unsuper-\nvised person re-id deep learning method using automati-\ncally generated person tracklets. This avoids the need for\ncamera pairwise ID labelled training data, i.e. unsupervised\ntracklet re-id discriminative learning. SpeciÔ¨Åcally, we propose a\nUnsupervised Tracklet Association Learning (UTAL) model\nwith two key ideas: (1) Per-Camera Tracklet Discrimination\nLearning that optimises ‚Äúlocal‚Äù within-camera tracklet label\ndiscrimination. It aims to facilitate cross-camera tracklet\nassociation given per-camera independently created tracklet\nlabel spaces. (2) Cross-Camera Tracklet Association Learning\nthat optimises ‚Äúglobal‚Äù cross-camera tracklet matching. It\naims to Ô¨Ånd cross-view tracklet groupings that are most\nlikely of the same person identities without ID label in-\nformation. This is formulated as to jointly discriminate\nwithin-camera tracklet identity semantics and self-discover\ncross-camera tracklet pairwise matching in end-to-end deep\nlearning. Critically, the proposed UTAL method does not as-\nsume any domain-speciÔ¨Åc knowledge such as camera space-\ntime topology and cross-camera ID overlap. Therefore, it\nis scalable to arbitrary surveillance camera networks with\nunknown viewing conditions and background clutters.\nExtensive comparative experiments are conducted on\narXiv:1903.00535v1  [cs.CV]  1 Mar 2019\nACCEPTED TO APPEAR IN IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n2\nseven\nexisting\nbenchmarking\ndatasets\n(CUHK03\n[21],\nMarket-1501 [22], DukeMTMC-ReID [23,24], MSMT17 [4],\niLIDS-VID [25], PRID2011 [26], MARS [27]) and one newly\nintroduced tracklet person re-id dataset called DukeTracklet.\nThe results show the performance advantages and superior-\nity of the proposed UTAL method over the state-of-the-art\nunsupervised and domain adaptation person re-id models.\nA preliminary version of this work was reported in\n[28]. Compared with the earlier study, there are a few key\ndifferences introduced: (i) This study presents a more prin-\ncipled and scalable unsupervised tracklet learning method\nthat learns deep neural network re-id models directly from\nlarge scale raw tracklet data. The method in [28] requires a\nseparate preprocessing for domain-speciÔ¨Åc spatio-temporal\ntracklet sampling for reducing the tracklet ID duplication\nrate per camera view. This need for pre-sampling not only\nmakes model learning more complex, not-end-to-end there-\nfore suboptimal, but also loses a large number of tracklets\nwith potential rich information useful for more effective\nmodel learning. (ii) We propose in this study a new concept\nof soft tracklet labelling, which aims to explore any inherent\nspace-time visual correlation of the same person ID between\nunlabelled tracklets within each camera view. This is de-\nsigned to better address the tracklet fragmentation problem\nthrough an end-to-end model optimisation mechanism. It\nimproves person tracking within individual camera views,\nwhich is lacking in [28]. (iii) Unlike the earlier method,\nthe current model self-discovers and exploits explicit cross-\ncamera tracklet association in terms of person ID, improving\nthe capability of re-id discriminative unsupervised learning\nand leading to superior model performances. (iv) Besides\ncreating a new tracklet person re-id dataset, we further\nconduct more comprehensive evaluations and analyses for\ngiving useful and signiÔ¨Åcant insights.\n2\nRELATED WORK\nPerson Re-IdentiÔ¨Åcation. Most existing person re-id mod-\nels are built by supervised model learning on a separate set of\nper-camera-pair ID labelled training data [2,3,4,5,6,7,21,29].\nWhile having no class intersection, the training and testing\ndata are often assumed to be drawn from the same camera\nnetwork (domain). Their scalability is therefore signiÔ¨Åcantly\npoor for realistic applications when no such large training\nsets are available for every camera-pair in a test domain.\nHuman-in-the-loop re-id provides a means of reducing the\noverall amount of training label supervision by exploring\nthe beneÔ¨Åts of human-computer interaction [30,31]. But, it\nis still labour intensive and tedious. Human labellers need\nto be deployed repeatedly for conducting similar screen\nproÔ¨Åling operations whenever a new target domain exhibits.\nIt is therefore not scalable either.\nUnsupervised model learning is an intuitive solution\nto avoiding the need of exhaustively collecting a large set\nof labelled training data per application domain. How-\never, previous hand-crafted features-based unsupervised\nlearning methods offer signiÔ¨Åcantly inferior re-id matching\nperformance [8,9,10,11,12,14,15,16,32,33], when compared to\nthe supervised learning models. A trade-off between re-\nid model scalability and generalisation performance can\nbe achieved by semi-supervised learning [13,34]. But these\nmodels still assume sufÔ¨Åciently large sized cross-view pair-\nwise ID labelled data for model training.\nThere are attempts on unsupervised learning by domain\nadaptation [17,18,19,20,35,36,37]. The idea is to exploit the\nknowledge of labelled data in ‚Äúrelated‚Äù source domains\nthrough model adaptation on the unlabelled target domain\ndata. One straightforward approach is to convert the source\nID labelled training data into the target domain by appear-\nance mimicry. This enables to train a model using the do-\nmain style transformed source training data via supervised\nlearning [35,36]. Alternative techniques include semantic\nattribute knowledge transfer [17,18,38], space-time pattern\ntransfer [39], virtual ID synthesis [40], and progressive adap-\ntation [19,20]. While these models perform better than the\nearlier generation of methods (Tables 2 and 3), they require\nsimilar data distributions and viewing conditions between\nthe labelled source domain and the unlabelled target do-\nmain. This restricts their scalability to arbitrarily diverse\n(and unknown) target domains in large scale deployments.\nUnlike all existing unsupervised learning re-id methods,\nthe proposed tracklet association method in this work en-\nables unsupervised re-id deep learning from scratch at end-\nto-end. This is more scalable and general. Because there is\nno assumption on either the scene characteristic similarity\nbetween source and target domains, or the complexity of\nhandling ID label knowledge transfer. Our method directly\nlearns to discover the re-id discriminative knowledge from\nunlabelled tracklet data automatically generated.\nMoreover, the proposed method does not assume any\noverlap of person ID classes across camera views or other\ndomain-speciÔ¨Åc information. It is therefore scalable to the\nscenarios without any knowledge about camera space-time\ntopology [39]. Unlike the existing unsupervised learning\nmethod relying on extra hand-crafted features, our model\nlearns tracklet based re-id discriminative features from an\nend-to-end deep learning process. To our best knowledge,\nthis is the Ô¨Årst attempt at unsupervised tracklet association\nbased person re-id deep learning model without relying on\nany ID labelled training video or imagery data.\nMulti-Task Learning in Neural Networks.\nMulti-task\nlearning (MTL) is a machine learning strategy that learns\nseveral related tasks simultaneously for their mutual bene-\nÔ¨Åts [41]. A good MTL survey with focus on neural networks\nis provided in [42]. Deep CNNs are well suited for per-\nforming MTL. As they are inherently designed to learn joint\nfeature representations subject to multiple label objectives\nconcurrently in multi-branch architectures. Joint learning of\nmultiple related tasks has been proven to be effective in\nsolving computer vision problems [43,44].\nIn contrast to all the existing methods aiming for su-\npervised learning problems, the proposed UTAL method\nexploits differently the MTL principle to solve an unsu-\npervised learning task. Critically, our method is uniquely\ndesigned to explore the potential of MTL in correlating\nthe underlying group level semantic relationships between\ndifferent individual learning tasks1. This dramatically dif-\nfers from existing MTL based methods focusing on mining\nthe shared knowledge among tasks at the sample level.\n1. In the unsupervised tracklet person re-id context, a group corre-\nsponds to a set of categorical labels each associated with an individual\nperson tracklet drawn from a speciÔ¨Åc camera view.\nACCEPTED TO APPEAR IN IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n3\nCritically, it avoids the simultaneous labelling of multi-tasks\non each training sample. Sample-wise multi-task labels are\nnot available in the unsupervised tracklet re-id problem.\nBesides, unsupervised tracklet labels in each task (cam-\nera view) are noisy. As they are obtained without manual\nveriÔ¨Åcation. Hence, the proposed UTAL model is in effect\nperforming weakly supervised multi-task learning with noisy\nper-task labels. This makes our method fundamentally dif-\nferent from existing MTL approaches that are only interested\nin discovering discriminative cross-task common represen-\ntations by strongly supervised learning of clean and exhaustive\nsample-wise multi-task labelling.\nUnsupervised Deep Learning.\nUnsupervised learning of\nvisual data is a long standing research problem starting from\nthe auto-encoder models [45] or earlier. Recently, this prob-\nlem has regained attention in deep learning. One common\napproach is by incorporating with data clustering [46] that\njointly learns deep feature representations and image clus-\nters. Alternative unsupervised learning techniques include\nformulating generative models [35], devising a loss function\nthat preserves information Ô¨Çowing [47] or discriminates in-\nstance classes [48], exploiting the object tracking continuity\ncue [49] in unlabelled videos, and so forth.\nAs opposite to all these methods focusing on uni-domain\ndata distributions, our method is designed particularly to\nlearn visual data sampled from different camera domains\nwith unconstrained viewing settings. Conceptually, the pro-\nposed idea of soft tracklet labels is related to data cluster-\ning. As the per-camera afÔ¨Ånity matrix used for soft label\ninference is related to the underlying data cluster structure.\nHowever, our afÔ¨Ånity based method has the unique merits\nof avoiding per-domain hard clustering and having fewer\nparameters to tune (e.g. the per-camera cluster number).\n3\nMETHOD FORMULATION\nTo overcome the limitation of supervised model learning\nalgorithms for exhaustive within-camera and cross-camera\nID labelling, we propose a novel Unsupervised Tracklet As-\nsociation Learning (UTAL) method to person re-id in videos\n(or multi-shot images in general). This is achieved by ex-\nploiting person tracklet labelling obtained from existing track-\ners2 without any ID labelling either cross-camera or within-\ncamera. The UTAL learns a person re-id model end-to-end\ntherefore beneÔ¨Åting from joint overall model optimisation in\ndeep learning. In the follows, we Ô¨Årst present unsupervised\nper-camera tracklet labelling (Sec. 3.1), then describe our\nmodel design for within-camera and cross-camera tracklet\nassociation by joint unsupervised deep learning (Sec. 3.2).\n3.1\nUnsupervised Per-Camera Tracklet Formation\nGiven a large quantity of video data captured by disjoint\nsurveillance cameras, we Ô¨Årst deploy the off-the-shelf pedes-\ntrian detection and tracking models [50,51,52] to automati-\ncally extract person tracklets. We then annotate each tracklet\nS with a unique class (one-hot) label y in an unsupervised\n2. Although object tracklets can be generated by any independent\nsingle-camera-view multi-object tracking (MOT) models widely avail-\nable currently, a conventional MOT model is not end-to-end optimised\nfor cross-camera tracklet association.\nand camera-independent manner. This does not involve any\nmanual ID veriÔ¨Åcation on tracklets. By applying this tracklet\nlabelling method in each camera view separately, we can\nobtain an independent set of labelled tracklets {Si, yi} per\ncamera, where each tracklet S contains a varying number of\nperson bounding box images I as S = {I1, I2, ¬∑ ¬∑ ¬∑ }.\nChallenges. To effectively learn a person re-id model from\nsuch automatically labelled tracklet training data, we need\nto deal with two modelling challenges centred around the\nsupervision of person ID class labels: (1) Due to frequent\ntrajectory fragmentation, multiple tracklets (unknown due\nto no manual veriÔ¨Åcation) are often generated during the\nappearing period of a person under one camera view. How-\never, they are unsupervisedly assigned with different one-\nhot categorical labels. This may signiÔ¨Åcantly mislead the dis-\ncriminative learning process of a re-id model. (2) There are\nno access to positive and negative pairwise ID correspon-\ndence between tracklet labels across disjoint camera views.\nLacking cross-camera person ID supervision underpins one\nof the key challenges in unsupervised tracklet person re-id.\n3.2\nUnsupervised Tracklet Association\nGiven per-camera independent tracklets {Si, yi}, we ex-\nplore tracklet label re-id discriminative learning without person\nID labels in a deep learning classiÔ¨Åcation framework. We\nformulate an Unsupervised Tracklet Association Learning\n(UTAL) method, with the overall architecture design illus-\ntrated in Fig. 1. The UTAL contains two model components:\n(I) Per-Camera Tracklet Discrimination (PCTD) learning for op-\ntimising ‚Äúlocal‚Äù within-camera tracklet label discrimination.\nThis facilitates ‚Äúglobal‚Äù cross-camera tracklet association,\ngiven independent tracklet label spaces in different camera\nviews. (II) Cross-Camera Tracklet Association (CCTA) learn-\ning for discovering ‚Äúglobal‚Äù cross-camera tracklet identity\nmatching without ID labels.\nFor accurate cross-camera tracklet association, it is im-\nportant to formulate a robust image feature representation\nto characterise the person appearance of each tracklet. How-\never, it is sub-optimal to achieve ‚Äúlocal‚Äù per-camera tracklet\ndiscriminative learning using only per-camera independent\ntracklet labels without ‚Äúglobal‚Äù cross-camera tracklet corre-\nlations. We therefore propose to optimise jointly both PCTD\nand CCTA. The two components integrate as a whole in\na single deep learning architecture, learn jointly and mu-\ntually beneÔ¨Åt each other in incremental end-to-end model\noptimisation. Our overall idea for unsupervised learning\nof tracklet person re-id is to maximise coarse-grained latent\ngroup-level cross-camera tracklet association. This is based on\nexploring an notion of tracklet set correlation learning (Fig.\n2(b)). It differs signiÔ¨Åcantly from supervised re-id learning\nthat relies heavily on the Ô¨Åne-grained explicit instance-level\ncross-camera ID pairwise supervision (Fig. 2(a)).\n3.2.1\nPer-Camera Tracklet Discrimination Learning\nIn PCTD learning, we treat each individual camera view\nseparately. That is, optimising per-camera labelled tracklet\ndiscrimination as a classiÔ¨Åcation task with the unsupervised\nper-camera tracklet labels (not person ID labels) (Fig. 1(a)).\nGiven a surveillance network with T cameras, we hence\nhave a total of T different tracklet classiÔ¨Åcation tasks each\ncorresponding to a speciÔ¨Åc camera view.\nACCEPTED TO APPEAR IN IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n4\n0\n0.5\n‡∑ùùíö\n(c) Per-Camera Tracklet Discrimination\nCamera 2 tracklet label space \n(d) Cross-Camera Tracklet Association\n(a) Auto-detected tracklets\nwithout person ID labels\nCamera 1\n‚Ä¶\n‚Ä¶\nCamera 2\n‚Ä¶\nCamera T\n‚Ä¶\n‚Ä¶\nCamera T\n‚Ä¶\nCamera 1 tracklet label space \nùë†3\n1\nùë†2\n1\nùë†4\n1\nùë†ùëõ1\nùë†4\n2\nùë†ùëö\n2\nImage\nTracklet\nùíîùëñ\nùëê: The ùëñ-th tracklet in camera ùëê\nTracklet feature representation\nTask 1\nTask 2\nTask T\n(b) Person re-id \ndeep CNN model\nCamera-\nshared\nfeature\nùë†1\n1\nùë†3\n1\nùë†2\n1\nùë†1\n1\nCamera 1 tracklet label space \nùë†3\n1\nùë†2\n1\nùë†4\n1\nùë†ùëõ1\nùë†1\n1\nCamera 2 tracklet label space \n‚Ä¶\nCamera T\n‚Ä¶\nùë†4\n2\nùë†ùëö\n2\nùë†3\n1\nùë†2\n1\nùë†1\n1\n(e) Soft label ‡∑ùùíöof a tracklet ùíî\nùíî\nitself\nFig. 1. An overview of the proposed Unsupervised Tracklet Association Learning (UTAL) person re-identiÔ¨Åcation model. The UTAL takes as input\n(a) auto-detected tracklets from all the camera views without any person ID class labelling either within-camera or cross-camera. The objective is\nto derive (b) a person re-id discriminative feature representation model by unsupervised learning. To this end, we formulate the UTAL model for\nsimultaneous (c) Per-Camera Tracklet Discrimination (PCTD) learning and (d) Cross-Camera Tracklet Association (CCTA) learning in an end-to-\nend neural network architecture. The PCTD aims to derive the ‚Äúlocal‚Äù discrimination of per-camera tracklets in the respective tracklet label space\n(represented by soft labels (e)) by a multi-task inference process (one task for a speciÔ¨Åc camera view), whilst the CCTA to learn the ‚Äúglobal‚Äù cross-\ncamera tracklet association across independently formed tracklet label spaces. In UTAL design, the PCTD and CCTA jointly learn to optimise a re-id\nmodel for maximising their complementary contributions and advantages in a synergistic interaction and integration. Best viewed in colour.\nImportantly, we further formulate these T classiÔ¨Åcation\ntasks in a multi-branch network architecture design. All\nthe tasks share the same feature representation space (Fig.\n1(b)) whilst enjoying an individual classiÔ¨Åcation branch (Fig.\n1(c)). This is a multi-task learning [42].\nFormally, we assume Mt different tracklet labels {y}\nwith the training tracklet image frames {I} from a camera\nview t ‚àà{1, ¬∑ ¬∑ ¬∑ , T} (Sec. 3.1). We adopt the softmax Cross-\nEntropy (CE) loss function to optimise the corresponding\nclassiÔ¨Åcation task (the t-th branch). The CE loss on a training\nimage frame (I, y) is computed as:\nLce = ‚àí\nMt\nX\nj=1\n1(j = y) ¬∑ log\n\u0010\nexp(W ‚ä§\nj x)\nPMt\nk=1 exp(W ‚ä§\nk x)\n\u0011\n(1)\nwhere x speciÔ¨Åes the feature vector of I extracted from\nthe task-shared representation space and Wy the y-th class\nprediction parameters. 1(¬∑) denotes an indicator function\nthat returns 1/0 for true/false arguments. Given a training\nmini-batch, we compute the CE loss for each such training\nsample with the respective tracklet label space and utilise\ntheir average to form the PCTD learning objective as:\nLpctd =\n1\nNbs\nT\nX\nt=1\nLt\nce\n(2)\nwhere Lt\nce denotes the CE loss of all training tracklet frames\nfrom the t-th camera, and Nbs speciÔ¨Åes the batch size.\nRecall that, one of the main challenges in unsupervised\ntracklet re-id learning arises from within-camera trajectory\nfragmentation. This causes the tracklet ID duplication issue,\ni.e. the same-ID tracklets are assigned with distinct labels.\nBy treating every single tracklet label as a unique class\n(Eq (2)), misleading supervision can be resulted potentially\nhampering the model learning performance.\nSoft Labelling.\nFor gaining learning robustness against\nunconstrained trajectory fragmentation, we exploit the pair-\nwise appearance afÔ¨Ånity (similarity) information between\nwithin-camera tracklets. To this end, we propose soft tracklet\nlabels to replace the hard counterpart (one-hot labels). This\nscheme uniquely takes into account the underlying ID cor-\nrelation between tracklets in the PCTD learning (Eq (2)). It\nis based on the intuition that, multiple fragmented tracklets\nof the same person are more likely to share higher visual\nafÔ¨Ånity with each other than those describing different\npeople. Therefore, using tracklet labels involving the ap-\npearance afÔ¨Ånity (i.e. soft labels) means imposing person ID\nrelevant information into model training, from the manifold\nstructure learning perspective [53].\nFormally, we start the computation of soft tracklet la-\nbels by constructing an afÔ¨Ånity matrix of person appear-\nance At ‚ààRMt√óMt on all Mt tracklets for each camera\nt ‚àà{1, ¬∑ ¬∑ ¬∑ , T}, where each element At(i, j) speciÔ¨Åes the\nvisual appearance similarity between the tracklets i and\nj. This requires a tracklet feature representation space. We\nachieve this by cumulatively updating an external feature\nvector s for every single tracklet S with the image features\nof the constituent frames in a batch-wise manner.\nMore speciÔ¨Åcally, given a mini-batch including nt\ni image\nframes from the i-th tracklet St\ni of t-th camera view, the\ncorresponding tracklet representation st\ni is progressively\nupdated across the training iterations as:\nst\ni =\n1\n1 + Œ±\nh\nst\ni + Œ±\n\u0010 1\nnt\ni\nnt\ni\nX\nk=1\nxk\n\u0011i\n(3)\nACCEPTED TO APPEAR IN IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n5\nwhere xk is the feature vector of the k-th in-batch image\nframe of St\ni, extracted by the up-to-date model. The learning\nrate parameter Œ± controls how fast st\ni updates. This method\navoids the need of forwarding all tracklet data in each\niteration therefore computationally efÔ¨Åcient and scalable.\nGiven the tracklet feature representations, we subse-\nquently sparsify the afÔ¨Ånity matrix as:\nAt(i, j) =\n(\nexp(‚àí‚à•st\ni‚àíst\nj‚à•\n2\n2\nœÉ2\n),\nif st\nj ‚ààN(st\ni)\n0,\notherwise\n(4)\nwhere N(st\ni) are the K nearest neighbours (NN) of st\ni\ndeÔ¨Åned by the Euclidean distance in the feature space. Using\nthe sparse NN idea on the afÔ¨Ånity structure is for sup-\npressing the distracting effect of visually similar tracklets\nfrom unmatched ID classes. Computationally, each A has a\nquadratic complexity, but only to the number of per-camera\ntracklet and linear to the total number of cameras, rather\nthan quadratic to all tracklets from all the cameras. The\nuse of sparse similarity matrices signiÔ¨Åcantly reduces the\nmemory demand.\nTo incorporate the local density structure [54], we deploy\na neighbourhood structure-aware scale deÔ¨Åned as:\nœÉ2 =\n1\nMt ¬∑ K\nMt\nX\ni=1\nK\nX\nj=1\n\r\rst\ni ‚àíst\nj\n\r\r2\n2, s.t. st\nj ‚ààN(st\ni)\n(5)\nBased on the estimated neighbourhood structures, we\nÔ¨Ånally compute the soft label (Fig. 1(e)) for each tracklet St\ni\nas the L1 normalised afÔ¨Ånity measurement:\nÀÜyt\ni =\nA(i, 1 : Mt)\nPMt\nj=1\n\u0010\nA(i, j)\n\u0011\n(6)\nGiven the proposed soft tracklet labels, the CE loss\nfunction (Eq (2)) is then reformulated as:\nLsce = ‚àí\nMt\nX\nj=1\nÀÜyt\ni(j) ¬∑ log\n\u0010\nexp(W ‚ä§\nj x)\nPMt\nk=1 exp(W ‚ä§\nk x)\n\u0011\n(7)\nWe accordingly update the PCTD learning loss (Eq (2)) as:\nLpctd =\n1\nNbs\nT\nX\nt=1\nLt\nsce.\n(8)\nRemarks. In PCTD, the objective function (Eq (8)) optimises\nby supervised learning person tracklet discrimination within\neach camera view alone. It does not explicitly consider\nsupervision in cross-camera tracklet association. Interestingly,\nwhen jointly learning all the per-camera tracklet discrimi-\nnation tasks together, the learned representation model is\nimplicitly and collectively cross-view tracklet discriminative\nin a latent manner. This is due to the existence of cross-\ncamera tracklet ID class correlation. That being said, the\nshared feature representation is optimised to be tracklet\ndiscriminative concurrently for all camera views, latently\nexpanding model discriminative learning from per-camera\n(locally) to cross-camera (globally).\nApart from multi-camera multi-task learning, we exploit\nthe idea of soft tracklet labels to further improve the model\nID discrimination learning capability. This is for better ro-\nbustness against trajectory fragmentation. Fundamentally,\nthis is an indirect strategy of reÔ¨Åning fragmented tracklets.\n‚Ä¶\nùë∞ùë´ùüè\nùë∞ùë´ùüê\nùë∞ùë´ùüë\nùë∞ùë´ùüí\n(a)\nCamera 1\nCamera 2\n(b)\nUnderlying \ntracklet\nassociation\nFig. 2. Comparing (a) Fine-grained explicit instance-level cross-camera\nID labelled image pairs for supervised person re-id model learning and\n(b) Coarse-grained latent group-level cross-camera tracklet (a multi-shot\ngroup) label correlation for ID label-free (unsupervised) person re-id\nlearning using the proposed UTAL method.\nIt is based on the visual appearance afÔ¨Ånity without the\nneed of explicitly stitching tracklets. The intuition is that, the\ntracklets of the same person are possible to be assigned with\nmore similar soft labels (i.e. signatures). Consequently, this\nrenders the unsupervised tracklet labels closer to supervised\nID class labels in terms of discrimination power, therefore\nhelping re-id model optimisation.\nIn equation formulation, our PCTD objective is related\nto the Knowledge Distillation (KD) technique [55]. KD also\nutilises soft class probability labels inferred by an indepen-\ndent teacher model. Nevertheless, our method conceptually\ndiffers from KD, since we primarily aim to unveil the hidden\nÔ¨Åne-grained discriminative information of the same class\n(ID) distributed across unconstrained tracklet fragments,\nBesides, our model retains the KD‚Äôs merit of modelling\nthe inter-class similarity geometric manifold information.\nAlso, our method has no need for learning a heavy source\nknowledge teacher model, therefore, computationally more\nefÔ¨Åcient. We will evaluate the PCTD model design (Table 4).\n3.2.2\nCross-Camera Tracklet Association Learning\nThe PCTD achieves somewhat global (all the camera views)\ntracklet discrimination capability implicitly. But the result-\ning representation remains sub-optimal, due to the lack\nof explicitly optimising cross-camera tracklet association at\nthe Ô¨Åne-grained instance level. It is non-trivial to impose\ncross-camera re-id discriminative learning constraints at the\nabsence of ID labels. To address this problem, we introduce\na Cross-Camera Tracklet Association (CCTA) learning al-\ngorithm for enabling tracklet association between cameras\n(Fig. 1(d)). Conceptually, the CCTA is based on adaptively\nand incrementally self-discovering cross-view tracklet association\nin the multi-task camera-shared feature space (Fig. 1(b)).\nSpeciÔ¨Åcally, we ground the CCTA learning on cross-\ncamera nearest neighbourhoods. In re-id, the vast majority\nof cross-camera tracklet pairs are negative associations from\nunmatched ID classes. They provide no desired information\nabout how a person‚Äôs appearance varies under different\ncamera viewing conditions. The key for designing an in-\nformative CCTA loss is therefore to self-discover the cross-\ncamera positive matching pairs. This requires to search simi-\nlar samples (i.e. neighbours) which however is a challenging\ntask because: (1) Tracklet feature representations s can be\nunreliable and error-prone due to the lack of cross-camera\npair supervision (hence a catch-22 problem). (2) False pos-\nitive pairs may easily propagate the erroneous supervision\nACCEPTED TO APPEAR IN IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n6\ncumulatively through the learning process, guiding the op-\ntimisation towards poor local optima; Deep neural networks\npossess the capacity to Ô¨Åt any supervision labels [56].\nTo overcome these challenges, we introduce a model\nmatureness adaptive matching pair search mechanism. It\nprogressively Ô¨Ånds an increasing number of plausible true\nmatches across cameras as a reliable basis for the CCTA\nloss formulation. In particular, in each training epoch, we\nÔ¨Årst retrieve the reciprocal cross-camera nearest neighbour\nR(st\ni) for each tracklet st\ni. The R is obtained based on the\nmutual nearest neighbour notion [57]. Formally, let N 1(st\ni)\nbe the cross-camera NN of st\ni. The R(st\ni) is then deÔ¨Åned as:\nR(st\ni) = {s|s ‚ààN 1(st\ni) && st\ni ‚ààN 1(s)}\n(9)\nGiven such self-discovered cross-camera matching pairs,\nwe then formulate a CCTA objective loss for a tracklet st\ni as:\nLccta =\nX\ns‚ààR(st\ni)\n‚à•st\ni ‚àís ‚à•2\n(10)\nWith Eq (10), we impose a cross-camera discriminative\nlearning constraint by encouraging the model to pull the\nneighbour tracklets in Rt\ni close to st\ni. This CCTA loss applies\nonly to those tracklets s with cross-camera matches, i.e.\nR(s) is non-empty, so that it is model matureness adaptive.\nAs the training proceeds, the model is supposed to become\nmore mature, leading to more cross-camera tracklet matches\ndiscovered. We will evaluate the CCTA loss in Sec. 4.3.\nRemarks. Under the mutual nearest neighbour constraint,\nR(st\ni) are considered to be more strictly similar to st\ni than\nthe conventional nearest neighbours N(st\ni). With uncon-\ntrolled viewing condition variations across cameras, match-\ning tracklets with dramatic appearance changes may be\nexcluded from the R(st\ni) particularly in the beginning,\nrepresenting a conservative search strategy. This is designed\nso to minimise the negative effect of error propagation\nfrom false matching pairs. More matching pairs are likely\nunveiled as the training proceeds. Many previously missing\npairs can be gradually discovered when the model becomes\nmore mature and discriminative. Intuitively, easy matching\npairs are found before hard ones. Hence, the CCTA loss is in\na curriculum leaning spirit [58]. In Eq (10) we consider only\nthe positive pairs whilst ignoring the negative matches. This\nis conceptually analogue to the formulation of Canonical\nCorrelation Analysis (CCA) [59], and results in a simpler\nobjective function without the need to tune a margin hyper-\nparameter as required by the ranking losses [60].\n3.2.3\nJoint Unsupervised Tracklet Association Learning\nBy combining the CCTA and PCTD learning constraints, we\nobtain the Ô¨Ånal model objective loss function of UTAL as:\nLutal = Lpctd + ŒªLccta\n(11)\nwhere Œª is a balance weight. Note that Lpctd is an average\nloss term at the individual tracklet image level whilst Lccta\nat the tracklet group (set) level. Both are derived from the\nsame mini-batch of training data concurrently.\nRemarks. By design, the CCTA enhances model representa-\ntion learning. It imposes discriminative constraints derived\nfrom self-discovered cross-camera tracklet association. This\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\nFig. 3. Example cross-camera matching image/tracklet pairs from (a)\nCUHK03, (b) Market-1501, (c) DukeMTMC-ReID, (d) MSMT17, (e)\nPRID2011, (f) iLIDS-VID, (g) MARS, (h) DukeMTMC-SI-Tracklet.\nis based on the PCTD learning of unsupervised and per-\ncamera independent tracklet label spaces. With more dis-\ncriminative representation in the subsequent training itera-\ntions, the PCTD is then able to deploy more accurate soft\ntracklet labels. This in turn facilitates not only the following\nrepresentation learning of per-camera tracklets, but also the\ndiscovery of higher quality and more informative cross-\ncamera tracklet matching pairs. In doing so, the two learn-\ning components integrate seamlessly and optimise a per-\nson re-id model concurrently in an end-to-end batch-wise\nlearning process. Consequently, the overall UTAL method\nformulates a beneÔ¨Åt-each-other closed-loop model design.\nThis eventually leads to cumulative and complementary\nadvantages throughout training.\n3.2.4\nModel Training and Testing\nModel Training.\nTo minimise the negative effect of in-\naccurate cross-camera tracklet matching pairs, we deploy\nthe CCTA loss only during the second half training process.\nSpeciÔ¨Åcally, UTAL begins the model training with the soft\ntracklet label based PCTD loss (Eq (8)) for the Ô¨Årst half\nepochs. We then deploy the full UTAL loss (Eq (11)) for\nthe remaining epochs. To improve the training efÔ¨Åciency, we\nupdate the per-camera tracklet soft labels (Eq (6)) and cross-\ncamera tracklet matches (Eq (9)) per epoch. These updates\nprogressively enhance the re-id discrimination power of the\nUTAL objective throughout training, as we will show in our\nmodel component analysis and diagnosis in Sec. 4.3.\nModel Testing. Once a deep person re-id model is trained\nby the UTAL unsupervised learning method, we deploy the\ncamera-shared feature representations (Fig. 1(b)) for re-id\nmatching under the Euclidean distance metric.\n4\nEXPERIMENTS\n4.1\nExperimental Setting\nDatasets. To evaluate the proposed UTAL model, we tested\nboth video (iLIDS-VID [25], PRID2011 [26], MARS [27])\nand image (CUHK03 [21], Market-1501 [22], DukeMTMC-\nReID [23,24], MSMT17 [4]) person re-id datasets. In previous\nstudies, these two sets of benchmarks were usually evalu-\nated separately. We consider both sets because recent large\nimage re-id datasets were typically constructed by sampling\nperson bounding boxes from videos, so they share similar\ncharacteristics as the video re-id datasets. We adopted the\nstandard test protocols as summarised in Table 1.\nTo further test realistic model performances, we intro-\nduced a new tracklet person re-id benchmark based on\nACCEPTED TO APPEAR IN IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n7\nTABLE 1\nDataset statistics and evaluation setting.\nDataset\n# ID\n# Train\n# Test\n# Image\n# Tracklet\niLIDS-VID [25]\n300\n150\n150\n43,800\n600\nPRID2011 [26]\n178\n89\n89\n38,466\n354\nMARS [27]\n1,261\n625\n636\n1,191,003\n20,478\nDukeMTMC-SI-Tracklet\n1,788\n702\n1,086\n833,984\n12,647\nCUHK03 [21]\n1,467\n767\n700\n14,097\n0\nMarket-1501 [22]\n1,501\n751\n750\n32,668\n0\nDukeMTMC-ReID [24]\n1,812\n702\n1,110\n36,411\n0\nMSMT17 [4]\n4,101\n1,041\n3,060\n126,441\n0\nDukeMTMC [23]. It differs from all the existing DukeMTMC\nvariants [24,66,67] by uniquely providing automatically gen-\nerated tracklets. We built this new tracklet person re-id\ndataset as follows. We Ô¨Årst deployed an efÔ¨Åcient deep\nlearning tracker that leverages a COCO+PASCAL trained\nSSD [50] for pedestrian detection and an ImageNet trained\nInception [68] for person appearance matching. Applying\nthis tracker to all DukeMTMC raw videos, we generated\n19,135 person tracklets. Due to the inevitable detection and\ntracking errors caused by background clutters and visual\nambiguity, these tracklets may present typical mistakes (e.g.\nID switch) and corruptions (e.g. occlusion). We name this\ntest DukeMTMC-SI-Tracklet, abbreviated as DukeTracklet.\nThe DukeMTMC-SI-Tracklet dataset is publicly released at:\nhttps://github.com/liminxian/DukeMTMC-SI-Tracklet.\nFor benchmarking DukeTracklet, we need the ground-\ntruth person ID labels of tracklets. To this end, we used the\ncriterion of spatio-temporal average Intersection over Union\n(IoU) between detected tracklets and ground-truth trajecto-\nries available in DukeMTMC. In particular, we labelled an\nauto-generated tracklet by the ground-truth person ID asso-\nciated with a manually-generated trajectory if their average\nIoU is over 50%. Otherwise, we labelled the auto-generated\ntracklet as ‚Äúunknown ID‚Äù. To maximise the comparability\nwith existing DukeMTMC variants, we threw away those\ntracklets labelled with unknown IDs. We Ô¨Ånally obtained\n12,647 person tracklets from 1,788 unique IDs. The average\ntracklet duration is 65.9 frames. To match DukeMTMC-ReID\n[24], we set the same 702 training IDs with the remain-\ning 1,086 people for performance test (missing 14 test IDs\nagainst DukeMTMC-ReID due to tracking failures).\nTracklet Label Assignment. For each video re-id dataset,\nwe simply assigned each tracklet with a unique label in a\ncamera-independent manner (Sec. 3.1). For each multi-shot\nimage datasets, we assumed all person images per ID per\ncamera were drawn from a single pedestrian tracklet, and\nsimilarly labelled them as the video datasets.\nPerformance Metrics. We adopted the common Cumulative\nMatching Characteristic (CMC) and mean Average Precision\n(mAP) metrics [22] for model performance measurement.\nImplementation Details. We used an ImageNet pre-trained\nResNet-50 [69] as the backbone net for UTAL, along with an\nadditional 2,048-D fully-connected (FC) layer for deriving\nthe camera-shared representations. Every camera-speciÔ¨Åc\nbranch was formed by one FC classiÔ¨Åcation layer. Person\nbounding box images were resized to 256√ó128. To ensure\neach training mini-batch has person images from all cam-\neras, we set the batch size to 128 for PRID2011, iLIDS-VID\nand CUHK03, and 384 for MSMT17, Market-1501, MARS,\nDukeMTMC-ReID, and DukeTracklet. In order to balance\nthe model training speed across cameras, we randomly\nselected the same number of tracklets per camera and the\nsame number of frame images (4 images) per chosen tracklet\nwhen sampling each mini-batch. We adopted the Adam\noptimiser [70] with the learning rate of 3.5√ó10‚àí4 and the\nepoch of 200. By default, we set Œª=10 for Eq (11), Œ±=1 for\nEq (3), and K =4 for Eq (5) in the following experiments.\n4.2\nComparisons to the State-Of-The-Art Methods\nWe compared two different sets of state-of-the-art methods\non image and video re-id datasets, due to the independent\nstudies on them in the literature.\nEvaluation on Image Datasets.\nTable 2 shows the unsu-\npervised re-id performance of the proposed UTAL and 15\nstate-of-the-art methods including 3 hand-crafted feature\nbased methods (Dic [9], ISR [10], RKSL [13]) and 12 auxiliary\nknowledge (identity/attribute) transfer based models (AE\n[61], AML [63], UsNCA [64], CAMEL [20], JSTL [62], PUL\n[19], TJ-AIDL [17], CycleGAN [35], SPGAN [36], HHL [37],\nDASy [40]). The results show four observations as follows.\n(1) Among the existing methods, the knowledge transfer\nbased models are often superior due to the use of additional\nlabel information, e.g. Rank-1 39.4% by CAMEL vs 36.5%\nby Dic on CUHK03; 65.7% by DASy vs 50.2% by Dic on\nMarket-1501. To that end, CAMEL needs to beneÔ¨Åt from\nlearning on 7 different person re-id datasets of diverse\ndomains (CUHK03 [21], CUHK01 [74], PRID [26], VIPeR\n[75], i-LIDS [76]) including a total of 44,685 images and 3,791\nIDs; HHL requires to utilise labelled Market-1501 (750 IDs)\nor DukeMTMC-ReID (702 IDs) as the source training data.\nDASy needs elaborative ID synthesis and adaptation.\n(2) The proposed UTAL outperforms all competitors with\nsigniÔ¨Åcant margins. For example, the Rank-1 margin by\nUTAL over HHL is 7.0% (69.2-62.2) on Market-1501 and\n15.4% (62.3-46.9) on DukeMTMC-ReID. Also, our prelimi-\nnary method TAUDL already surpasses all previous meth-\nods. It is worth pointing out that UTAL dose not beneÔ¨Åt\nfrom any additional labelled source domain training data as\ncompared to the strong alternative HHL. Importantly, UTAL\nis potentially more scalable due to no reliance at all on the\nsimilarity constraint between source and target domains.\n(3) The UTAL is simpler to train with a simple end-to-end\nmodel learning, vs the alternated deep CNN training and\ndata clustering required by PUL, a two-stage model training\nof TJ-AIDL, high GAN training difÔ¨Åculty of HHL, and\nelaborative ID synthesis of DASy. These results show both\nthe performance advantage and model design superiority of\nUTAL over state-of-the-art re-id methods.\n(4) A large performance gap exists between unsupervised\nand supervised learning models. Further improvement is\nrequired on unsupervised learning algorithms.\nEvaluation on Video Datasets. In Table 3, we compared the\nUTAL with 8 state-of-the-art unsupervised video re-id mod-\nels (GRDL [11], UnKISS [12], SMP [16], DGM+MLAPG/IDE\n[15], DAL [72], RACE [71], DASy [40]) on the video bench-\nmarks. Unlike UTAL, all these existing models except DAL\nare not end-to-end deep learning methods using hand-\ncrafted or independently trained deep features as input.\nThe comparisons show that, our UTAL outperforms all\nexisting video person re-id models on the large scale video\nACCEPTED TO APPEAR IN IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n8\nTABLE 2\nUnsupervised person re-id on image based datasets. ‚àó: BeneÔ¨Åted from extra labelled auxiliary training data. ‚Äú-‚Äù: No reported result.\nDataset\nCUHK03 [21]\nMarket-1501 [22]\nDukeMTMC-ReID [24]\nMSMT17 [4]\nMetric (%)\nRank-1\nmAP\nRank-1\nmAP\nRank-1\nmAP\nRank-1\nmAP\nDic [9]\n36.5\n-\n50.2\n22.7\n-\n-\n-\n-\nISR [10]\n38.5\n-\n40.3\n14.3\n-\n-\n-\n-\nRKSL [13]\n34.8\n-\n34.0\n11.0\n-\n-\n-\n-\nSAE‚àó[61]\n30.5\n-\n42.4\n16.2\n-\n-\n-\n-\nJSTL‚àó[62]\n33.2\n-\n44.7\n18.4\n-\n-\n-\n-\nAML‚àó[63]\n31.4\n-\n44.7\n18.4\n-\n-\n-\n-\nUsNCA‚àó[64]\n29.6\n-\n45.2\n18.9\n-\n-\n-\n-\nCAMEL‚àó[20]\n39.4\n-\n54.5\n26.3\n-\n-\n-\n-\nPUL‚àó[19]\n-\n-\n44.7\n20.1\n30.4\n16.4\n-\n-\nTJ-AIDL‚àó[17]\n-\n-\n58.2\n26.5\n44.3\n23.0\n-\n-\nCycleGAN‚àó[35]\n-\n-\n48.1\n20.7\n38.5\n19.9\n-\n-\nSPGAN‚àó[36]\n-\n-\n51.5\n22.8\n41.1\n22.3\n-\n-\nSPGAN+LMP‚àó[36]\n-\n-\n57.7\n26.7\n46.4\n26.2\n-\n-\nHHL‚àó[37]\n-\n-\n62.2\n31.4\n46.9\n27.2\n-\n-\nDASy‚àó[40]\n-\n-\n65.7\n-\n-\n-\n-\n-\nTAUDL [28]\n44.7\n31.2\n63.7\n41.2\n61.7\n43.5\n28.4\n12.5\nUTAL\n56.3\n42.3\n69.2\n46.2\n62.3\n44.6\n31.4\n13.1\nGCS [65](Supervised)\n88.8\n97.2\n93.5\n81.6\n84.9\n69.5\n-\n-\nTABLE 3\nUnsupervised person re-id on video based datasets. ‚àó: Assume no tracking fragmentation. ‚Ä†: Use some ID labels for model initialisation.\nDataset\nPRID2011 [26]\niLIDS-VID [25]\nMARS [27]\nDukeTracklet\nMetric (%)\nRank-1 Rank-5 Rank-20\nRank-1 Rank-5 Rank-20\nRank-1 Rank-5 Rank-20 mAP\nRank-1 Rank-5 Rank-20 mAP\nGRDL [11]\n41.6\n76.4\n89.9\n25.7\n49.9\n77.6\n19.3\n33.2\n46.5\n9.6\n-\n-\n-\n-\nUnKISS [12]\n58.1\n81.9\n96.0\n35.9\n63.3\n83.4\n22.3\n37.4\n53.6\n10.6\n-\n-\n-\n-\nSMP‚àó[16]\n80.9\n95.6\n99.4\n41.7\n66.3\n80.7\n23.9\n35.8\n44.9\n10.5\n-\n-\n-\n-\nDGM+MLAPG‚Ä† [15]\n73.1\n92.5\n99.0\n37.1\n61.3\n82.0\n24.6\n42.6\n57.2\n11.8\n-\n-\n-\n-\nDGM+IDE‚Ä† [15]\n56.4\n81.3\n96.4\n36.2\n62.8\n82.7\n36.8\n54.0\n68.5\n21.3\n-\n-\n-\n-\nRACE‚Ä† [71]\n50.6\n79.4\n91.8\n19.3\n39.3\n68.7\n43.2\n57.1\n67.6\n24.5\n-\n-\n-\n-\nDASy‚àó[40]\n43.0\n-\n-\n56.5\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nDAL [72]\n85.3\n97.0\n99.6\n56.9\n80.6\n91.9\n46.8\n63.9\n77.5\n21.4\n-\n-\n-\n-\nTAUDL [28]\n49.4\n78.7\n98.9\n26.7\n51.3\n82.0\n43.8\n59.9\n72.8\n29.1\n26.1\n42.0\n57.2\n20.8\nUTAL\n54.7\n83.1\n96.2\n35.1\n59.0\n83.8\n49.9\n66.4\n77.8\n35.2\n43.8\n62.8\n76.5\n36.6\nSnippet [73](Supervised)\n93.0\n99.3\n100.0\n85.4\n96.7\n99.5\n86.3\n94.7\n98.2\n76.1\n-\n-\n-\n-\ndataset MARS, e.g. by a Rank-1 margin of 3.1% (49.9-46.8)\nand a mAP margin of 13.8% (35.2-21.4) over the best com-\npetitor DAL. However, UTAL is inferior than top existing\nmodels on the two small benchmarks iLIDS-VID (300 train-\ning tracklets) and PRID2011 (178 training tracklets), vs 8,298\ntraining tracklets in MARS. This shows that UTAL does\nneed sufÔ¨Åcient tracklet data in order to have its performance\nadvantage. As the required tracklet data are not manually\nlabelled, this requirement is not a hindrance to its scalability\non large scale deployments. Quite the contrary, UTAL works\nthe best when large unlabelled video data are available. A\nmodel would beneÔ¨Åt from pre-training using UTAL on large\nauxiliary unlabelled videos with similar viewing conditions.\n4.3\nComponent Analysis and Discussion\nWe conducted detailed UTAL model component analysis on\ntwo large tracklet re-id datasets, MARS and DukeTracklet.\nPer-Camera Tracklet Discrimination Learning. We started\nby testing the performance impact of the PCTD component.\nThis is achieved by designing a baseline that treats all cam-\neras together, that is, concatenating the per-camera track-\nlet label spaces and deploying the Cross-Entropy loss for\nlearning a Single-Task ClassiÔ¨Åcation (STC). In this analysis,\nwe did not consider the cross-camera tracklet association\ncomponent for a more focused evaluation.\nTable 4 shows that, the proposed PCTD design is sig-\nniÔ¨Åcantly superior over the STC learning algorithm, e.g.\nachieving Rank-1 gain of 27.9% (43.8-15.9), and 27.8% (31.7-\n3.9) on MARS and DukeTracklet, respectively. The results\ndemonstrate modelling advantages of PCTD in exploiting\nunsupervised tracklet labels for learning cross-view re-id\ndiscriminative features. This validates the proposed idea of\nimplicitly deriving a cross-camera shared feature represen-\ntation through a multi-camera multi-task learning strategy.\nTABLE 4\nEffect of Per-Camera Tracklet Discrimination (PCTD) learning.\nDataset\nMARS [27]\nDukeTracklet\nMetric (%)\nRank-1\nmAP\nRank-1\nmAP\nSTC\n15.9\n10.0\n3.9\n4.7\nPCTD\n43.8\n31.4\n31.7\n26.4\nRecall that we propose a soft label (Eq (6)) based Cross-\nEntropy loss (Eq (7)) for tackling the notorious trajectory\nfragmentation challenge. To test how much beneÔ¨Åt our soft\nlabelling strategy brings to unsupervised tracklet re-id, we\ncompared it against the one-hot class hard-labelling counter-\npart (Eq (1)). Table 5 shows that the proposed soft-labelling\nis signiÔ¨Åcantly superior, suggesting a clear beneÔ¨Åt in miti-\ngating the negative impact of trajectory fragmentation. This\nis due to the intrinsic capability of exploiting the appearance\npairwise afÔ¨Ånity knowledge among tracklets per camera.\nWe further examined the ID discrimination capability of\ntracklet soft labels that underpins its outperforming over\nthe corresponding hard labels. To this end, we measured the\nMean Pairwise Similarity (MPS) of soft label vectors assigned\nACCEPTED TO APPEAR IN IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n9\nTABLE 5\nSoft-labelling versus hard-labelling.\nDataset\nMARS [27]\nDukeTracklet\nMetric (%)\nRank-1\nmAP\nRank-1\nmAP\nHard-Labelling\n35.5\n20.5\n24.5\n17.3\nSoft-Labelling\n49.9\n35.2\n43.8\n36.6\nto per-camera same-ID tracklets. Figure 4 shows that, the\nMPS metric goes higher as the training epoch increases,\nparticularly after the CCTA loss is further exploited in the\nmiddle of training (at 100th epoch). This indicates explicitly\nthe evolving process of mining the discriminative knowl-\nedge among fragmented tracklets with the same person ID\nlabels in a self-supervising fashion.\nIn effect, the afÔ¨Ånity measurement (Eq (4)) used for\ncomputing soft labels can be useful for automatically merg-\ning short fragmented tracklets into long trajectories per\ncamera. We tested this tracking reÔ¨Ånement capability of our\nmethod. In particular, we built a sparse connection graph by\nthresholding the pairwise afÔ¨Ånity scores at 0.5, analysed the\nconnected tracklet components [77], and merged all tracklets\nin each component into a long trajectory.\nFig. 4. The evolving process of tracklet soft label quality over the model\ntraining epochs on MARS and DukeTracklet.\nTable 6 shows that with our per-camera tracklet afÔ¨Ån-\nity measurement, even such a simple strategy can merge\n4,389/2,527 out of 8,298/5,803 short tracklets into 1,532/982\nlong trajectories at the NMI (Normalised Mutual Informa-\ntion) rate of 0.896/0.934 on MARS/DukeTracklet. This not\nonly suggests the usefulness of UTAL in tracklet reÔ¨Ånement,\nbut also reÔ¨Çects the underlying correlation between tracking\nand person re-id. For visual quality examination, we gave\nexample cases for tracking reÔ¨Ånement in Fig. 5.\nTABLE 6\nEvaluating the tracking reÔ¨Ånement capability of soft labels.\nDataset\nMARS [27]\nDukeTracklet\nOriginal Tracklets\n8,298\n5,803\nMergable Tracklets\n4,389\n2,527\nLong Trajectories\n1,532\n928\nNMI\n0.896\n0.934\nFig. 5. Example long trajectories discovered by UTAL among unlabelled\nshort fragmented tracklets. Each row denotes a case. The tracklets\nin green/red bounding box denote the true/false matches, respectively.\nFailure tracklet merging may be due to detection and tracking errors.\nAlgorithmically, our soft label PCTD naturally inher-\nits the cross-class (ID) knowledge transfer capability from\nKnowledge Distillation [55]. It is interesting to see how much\nperformance beneÔ¨Åt this can bring to unsupervised tracklet\nre-id. To this end, we conducted a controlled experiment\nwith only one randomly selected training tracklet per ID\nper camera. Doing so enforces that no multiple per-camera\ntracklets share the same person ID, which more explicitly\nevaluates the impact of cross-ID knowledge transfer. Note,\nthis leads to probably inferior re-id model generalisation\ncapability due to less training data used in optimisation.\nTable 7 shows that the cross-ID knowledge transfer gives\nnotable performance improvements.\nTABLE 7\nEvaluating the cross-ID knowledge transfer effect of soft labels.\nDataset\nMARS [27]\nDukeTracklet\nMetric (%)\nRank-1\nmAP\nRank-1\nmAP\nHard Label\n45.1\n31.1\n28.6\n20.8\nSoft Label\n46.5\n31.2\n31.7\n24.8\nCross-Camera Tracklet Association Learning. We evalu-\nated the CCTA component by measuring the performance\ndrop once eliminating it. Table 8 shows that CCTA brings\na signiÔ¨Åcant re-id accuracy beneÔ¨Åt, e.g. a Rank-1 boost of\n6.1% (49.9-43.8) and 12.1% (43.8-31.7) on MARS and Duke-\nTracklet, respectively. This suggests the importance of cross-\ncamera ID class correlation modelling and the capability\nof our CCTA formulation in reliably associating tracklets\nacross cameras for unsupervised re-id model learning.\nTABLE 8\nEffect of Cross-Camera Tracklet Association (CCTA) learning.\nDataset\nMARS [27]\nDukeTracklet\nCCTA\nRank-1\nmAP\nRank-1\nmAP\n\u0017\n43.8\n31.4\n31.7\n26.4\n\u0013\n49.9\n35.2\n43.8\n36.6\nTo further examine why CCTA enables more discrimi-\nnative re-id model learning, we tracked the self-discovered\ncross-camera tracklet matching pairs throughout the train-\ning. Figure 6 shows that both the number and precision\nof self-discovered cross-camera tracklet pairs increase. This\nechoes the model performance superiority of UTAL.\n(a) Number of tracklet pairs.\n(b) Precision of matching pairs.\nFig. 6. The evolving process of self-discovered cross-camera tracklet\nmatching pairs in (a) number and (b) precision throughout the training.\nModel Parameters. We evaluated the performance impact\nof three UTAL hyper-parameters: (1) the tracklet feature\nupdate learning rate Œ± (Eq (3)), (2) the sparsity K of the\ntracklet afÔ¨Ånity matrix used in computing the soft labels (Eq\n(4) and (5)); (3) the loss balance weight Œª (Eq (11)). Figure 7\nshows that: (1) Œ± is not sensitive with a wide satisfactory\nrange. This suggests a stable model learning procedure.\n(2) K has an optimal value at ‚Äú4‚Äù. Too small values lose\nthe opportunities of incorporating same-ID tracklets into\nACCEPTED TO APPEAR IN IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n10\nthe soft labels whilst the opposite instead introduces dis-\ntracting/noisy neighbour information. (3) Œª is found more\ndomain dependent, with the preference values around ‚Äú10‚Äù.\nThis indicates a higher importance of cross-camera tracklet\nassociation and matching.\n(a) The sensitive of Œ± (b) The sensitive of K\n(c) The sensitive of Œª\nFig. 7. Analysis of the UTAL model parameters.\nCross-Camera Nearest Neighbours (CCNN). We evaluated\nthe effect of CCNN R (Eq (9)) used in the CCTA loss.\nWe compared two designs: Our reciprocal 2-way 1-NN vs.\ncommon 1-way 1-NN. Table 9 shows that the more strict 2-\nway 1-NN gives better overall performance whilst the 1-way\n1-NN has a slight advantage in Rank-1 on MARS (50.5%\nvs. 49.9%). By 2-way, we found using more neighbours\n(5/10-NN) degrades model performance. This is due to the\nintroduction of more false cross-camera matches.\nTABLE 9\nEffect of cross-camera nearest neighbours.\nDataset\nMARS [27]\nDukeTracklet\nMetric (%)\nRank-1\nmAP\nRank-1\nmAP\n1-way 1-NN\n50.5\n33.2\n41.9\n34.5\n2-way 1-NN\n49.9\n35.2\n43.8\n36.6\n2-way 5-NN\n47.6\n34.5\n38.0\n31.6\n2-way 10-NN\n45.5\n32.5\n33.9\n27.1\nTracklet Sampling versus All Tracklets. In our preliminary\nsolution TAUDL [28], we considered a Sparse Space-Time\nTracklet (SSTT) sampling strategy instead of unsupervised\nlearning on all tracklet data. It is useful in minimising the\nperson ID duplication rate in tracklets. However, such a data\nsampling throws away a large number of tracklets with rich\ninformation of person appearance exhibited continuously\nand dynamically over space and time. To examine this, we\ncompared the SSTT sampling with using all tracklets.\nTable 10 shows two observations: (1) In overall re-id\nperformance, our preliminary method TAUDL [28] is out-\nperformed signiÔ¨Åcantly by UTAL. For example, the Rank-1\nresults are improved by 6.1% (49.9-43.8) on MARS and by\n17.7% (43.8-26.1) on DukeTracklet. (2) When using the same\nUTAL model, the SSTT strategy leads to inferior re-id rates\nas compared with using all tracklets. For instance, the Rank-\n1 performance drop is 4.8% (49.9-45.1) on MARS, and 12.1%\n(43.8-31.7) on DukeTracklet. These performance gains are\ndue to the proposed soft label learning idea that effectively\nhandles the trajectory fragmentation problem. Overall, this\nvalidates the efÔ¨Åcacy of our model design in solving the\nSSTT‚Äôs limitation whilst more effectively tackling the ID\nduplication (due to trajectory fragmentation) problem.\nEffect of Neural Network Architecture. The model gener-\nalisation performance of UTAL may depend on the selection\nof neural network architecture. To assess this aspect, we\nevaluated one more UTAL variant using a more recent\nDenseNet-121 [78] as the backbone network, versus the\ndefault choice ResNet-50 [69]. Table 11 shows that even\nTABLE 10\nTracklet sampling versus using all tracklets.\nDataset\nMARS [27]\nDukeTracklet\nMetric (%)\nRank-1\nmAP\nRank-1\nmAP\nTAUDL [28]\n43.8\n29.1\n26.1\n20.8\nUTAL(SSTT)\n45.1\n31.1\n31.7\n24.8\nUTAL(All Tracklets)\n49.9\n35.2\n43.8\n36.6\nsuperior re-id performances can be obtained when using a\nstronger network architecture. This suggests that UTAL can\nreadily beneÔ¨Åt from the advancement of network designs.\nTABLE 11\nEffect of backbone neural network in UTAL.\nDataset\nMARS [27]\nDukeTracklet\nMetric (%)\nRank-1\nmAP\nRank-1\nmAP\nResNet-50 [69]\n49.9\n35.2\n43.8\n36.6\nDenseNet-121 [78]\n51.6\n35.9\n44.3\n36.7\nWeakly Supervised Tracklet Association Learning.\nFor\ntraining data labelling in person re-id, the most costly\nprocedure is on exhaustive manual search of cross-camera\nimage/tracklet matching pairs. It is often unknown where\nand when a speciÔ¨Åc person will appear given complex\ncamera spatio-temporal topology and unconstrained peo-\nple‚Äôs behaviours in the public spaces. Therefore, per-camera\nindependent ID labelling is more affordable. Such labelled\ndata are much weaker and less informative, due to the lack\nof cross-camera positive and negative ID pairs information.\nWe call the setting Weakly Supervised Learning (WSL).\nThe proposed UTAL model can be Ô¨Çexibly applied in\nthe WSL setting. Interestingly, this allows to test how much\nre-id performance beneÔ¨Åt such labels can provide. Unlike\nin the unsupervised learning setting, the soft label based\nPCTD loss is no longer necessary in WSL given the within-\ncamera ID information. Hence, we instead deployed the\nhard one-hot label (Eq (1)) based PCTD loss (Eq (2)). Table\n12 shows that such weak labels are informative and useful\nfor person re-id by the UTAL method. This test indicates\na wide suitability and usability of our method in practical\ndeployments under various labelling budgets.\nTABLE 12\nEvaluation of weakly supervised tracklet association learning.\nDataset\nMARS [27]\nDukeTracklet\nMetric (%)\nRank-1\nmAP\nRank-1\nmAP\nUnsupervised\n49.9\n35.2\n43.8\n36.6\nWeakly Supervised\n59.5\n51.7\n46.4\n39.0\nManual Tracking. DukeMTMC-VideoReID provides manually\nlabelled trajectories, originally introduced for one-shot per-\nson re-id [66]. We tested UTAL on this dataset without the\nassumed one-shot labelled trajectory per ID. We set K = 0\nfor Eq (5) due to no trajectory fragmentation. Table 13 shows\nthat UTAL outperforms EUG [66] even without one-shot\nID labelling. This indicates the efÔ¨Åcacy of our unsupervised\nlearning strategy in discovering re-id information.\nTABLE 13\nEvaluation on DukeMTMC-VideoReID.\nMetric (%)\nRank-1\nRank-5\nRank-20\nmAP\nEUG [66]\n72.8\n84.2\n91.5\n63.2\nUTAL\n74.5\n88.7\n96.3\n72.1\nACCEPTED TO APPEAR IN IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n11\n5\nCONCLUSIONS\nWe presented a novel Unsupervised Tracklet Association Learn-\ning (UTAL) model for unsupervised tracklet person re-\nidentiÔ¨Åcation. This model learns from person tracklet data\nautomatically extracted from videos, eliminating the expen-\nsive and exhaustive manual ID labelling. This enables UTAL\nto be more scalable to real-world applications. In contrast to\nexisting re-id methods that require exhaustively pairwise\nlabelled training data for every camera-pair or assume\nlabelled source domain training data, the proposed UTAL\nmodel performs end-to-end deep learning of a person re-id\nmodel from scratch using totally unlabelled tracklet data.\nThis is achieved by optimising jointly both a Per-Camera\nTracklet Discrimination loss function and a Cross-Camera\nTracklet Association loss function in a uniÔ¨Åed architecture.\nExtensive evaluations were conducted on eight image and\nvideo person re-id benchmarks to validate the advantages\nof the proposed UTAL model over state-of-the-art unsuper-\nvised and domain adaptation re-id methods.\nACKNOWLEDGMENTS\nThis work is partially supported by Vision Semantics Limited,\nNational Natural Science Foundation of China (Project No.\n61401212), Royal Society Newton Advanced Fellowship Pro-\ngramme (NA150459), and Innovate UK Industrial Challenge\nProject on Developing and Commercialising Intelligent Video\nAnalytics Solutions for Public Safety (98111-571149).\nREFERENCES\n[1]\nS. Gong, M. Cristani, S. Yan, and C. C. Loy, Person re-identiÔ¨Åcation.\nSpringer, 2014.\n[2]\nW. Li, X. Zhu, and S. Gong, ‚ÄúPerson re-identiÔ¨Åcation by deep joint\nlearning of multi-loss classiÔ¨Åcation,‚Äù in Proc. Int. Jo. Conf. of Artif.\nIntell., 2017.\n[3]\n‚Äî‚Äî,\n‚ÄúHarmonious\nattention\nnetwork\nfor\nperson\nre-\nidentiÔ¨Åcation,‚Äù in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,\n2018, pp. 2285‚Äì2294.\n[4]\nL. Wei, S. Zhang, W. Gao, and Q. Tian, ‚ÄúPerson transfer gan to\nbridge domain gap for person re-identiÔ¨Åcation,‚Äù in Proc. IEEE\nConf. Comput. Vis. Pattern Recognit., 2018, pp. 79‚Äì88.\n[5]\nC. Song, Y. Huang, W. Ouyang, and L. Wang, ‚ÄúMask-guided\ncontrastive attention model for person re-identiÔ¨Åcation,‚Äù in Proc.\nIEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 1179‚Äì1188.\n[6]\nX. Chang, T. M. Hospedales, and T. Xiang, ‚ÄúMulti-level factorisa-\ntion net for person re-identiÔ¨Åcation,‚Äù in Proc. IEEE Conf. Comput.\nVis. Pattern Recognit., 2018, pp. 2109‚Äì2118.\n[7]\nY. Shen, H. Li, T. Xiao, S. Yi, D. Chen, and X. Wang, ‚ÄúDeep group-\nshufÔ¨Çing random walk for person re-identiÔ¨Åcation,‚Äù in Proc. IEEE\nConf. Comput. Vis. Pattern Recognit., 2018, pp. 2265‚Äì2274.\n[8]\nH. Wang, S. Gong, and T. Xiang, ‚ÄúUnsupervised learning of\ngenerative topic saliency for person re-identiÔ¨Åcation,‚Äù in Proc. Bri.\nMach. Vis. Conf., 2014.\n[9]\nE. Kodirov, T. Xiang, and S. Gong, ‚ÄúDictionary learning with\niterative laplacian regularisation for unsupervised person re-\nidentiÔ¨Åcation,‚Äù in Proc. Bri. Mach. Vis. Conf., 2015.\n[10] G. Lisanti, I. Masi, A. D. Bagdanov, and A. Del Bimbo, ‚ÄúPerson re-\nidentiÔ¨Åcation by iterative re-weighted sparse ranking,‚Äù IEEE Trans.\nPattern Anal. Mach. Intell., vol. 37, no. 8, pp. 1629‚Äì1642, 2015.\n[11] E. Kodirov, T. Xiang, Z. Fu, and S. Gong, ‚ÄúPerson re-identiÔ¨Åcation\nby unsupervised l1 graph learning,‚Äù in Proc. Eur. Conf. Comput.\nVis., 2016, pp. 178‚Äì195.\n[12] F. M. Khan and F. Bremond, ‚ÄúUnsupervised data association\nfor metric learning in the context of multi-shot person re-\nidentiÔ¨Åcation,‚Äù in Proc. IEEE Conf. Adv. Vid. Sig. Surv., 2016, pp.\n256‚Äì262.\n[13] H. Wang, X. Zhu, T. Xiang, and S. Gong, ‚ÄúTowards unsupervised\nopen-set person re-identiÔ¨Åcation,‚Äù in IEEE Int. Conf. on Img. Proc.,\n2016, pp. 769‚Äì773.\n[14] X. Ma, X. Zhu, S. Gong, X. Xie, J. Hu, K.-M. Lam, and Y. Zhong,\n‚ÄúPerson re-identiÔ¨Åcation by unsupervised video matching,‚Äù Pat-\ntern Recognition, vol. 65, pp. 197‚Äì210, 2017.\n[15] M. Ye, A. J. Ma, L. Zheng, J. Li, and P. C. Yuen, ‚ÄúDynamic label\ngraph matching for unsupervised video re-identiÔ¨Åcation,‚Äù in Proc.\nIEEE Int. Conf. Comput. Vis., 2017, pp. 5142‚Äì5150.\n[16] Z. Liu, D. Wang, and H. Lu, ‚ÄúStepwise metric promotion for\nunsupervised video person re-identiÔ¨Åcation,‚Äù in Proc. IEEE Int.\nConf. Comput. Vis., 2017, pp. 2429‚Äì2438.\n[17] J. Wang, X. Zhu, S. Gong, and W. Li, ‚ÄúTransferable joint attribute-\nidentity deep learning for unsupervised person re-identiÔ¨Åcation,‚Äù\nin Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 2275‚Äì\n2284.\n[18] P. Peng, Y. Tian, T. Xiang, Y. Wang, M. Pontil, and T. Huang, ‚ÄúJoint\nsemantic and latent attribute modelling for cross-class transfer\nlearning,‚Äù IEEE Trans. Pattern Anal. Mach. Intell., vol. 40, no. 7,\npp. 1625‚Äì1638, 2018.\n[19] H. Fan, L. Zheng, and Y. Yang, ‚ÄúUnsupervised person re-\nidentiÔ¨Åcation: Clustering and Ô¨Åne-tuning,‚Äù arXiv:1705.10444, 2017.\n[20] H.-X. Yu, A. Wu, and W.-S. Zheng, ‚ÄúCross-view asymmetric metric\nlearning for unsupervised person re-identiÔ¨Åcation,‚Äù in Proc. IEEE\nInt. Conf. Comput. Vis., 2017, pp. 994‚Äì1002.\n[21] W. Li, R. Zhao, T. Xiao, and X. Wang, ‚ÄúDeepreid: Deep Ô¨Ålter pairing\nneural network for person re-identiÔ¨Åcation,‚Äù in Proc. IEEE Conf.\nComput. Vis. Pattern Recognit., 2014, pp. 152‚Äì159.\n[22] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian,\n‚ÄúScalable person re-identiÔ¨Åcation: A benchmark,‚Äù in Proc. IEEE\nConf. Comput. Vis. Pattern Recognit., 2015, pp. 1116‚Äì1124.\n[23] E. Ristani, F. Solera, R. Zou, R. Cucchiara, and C. Tomasi, ‚ÄúPer-\nformance measures and a data set for multi-target, multi-camera\ntracking,‚Äù in Workshop of Eur. Conf. Comput. Vis., 2016, pp. 17‚Äì35.\n[24] Z. Zheng, L. Zheng, and Y. Yang, ‚ÄúUnlabeled samples generated\nby gan improve the person re-identiÔ¨Åcation baseline in vitro,‚Äù in\nProc. IEEE Int. Conf. Comput. Vis., 2017, pp. 3754‚Äì3762.\n[25] T. Wang, S. Gong, X. Zhu, and S. Wang, ‚ÄúPerson re-identiÔ¨Åcation\nby video ranking,‚Äù in Proc. Eur. Conf. Comput. Vis., 2014, pp. 688‚Äì\n703.\n[26] M. Hirzer, C. Beleznai, P. M. Roth, and H. Bischof, ‚ÄúPerson re-\nidentiÔ¨Åcation by descriptive and discriminative classiÔ¨Åcation,‚Äù in\nScand. Conf. Img. Anal., 2011, pp. 91‚Äì102.\n[27] L. Zheng, Z. Bie, Y. Sun, J. Wang, C. Su, S. Wang, and\nQ. Tian, ‚ÄúMars: A video benchmark for large-scale person re-\nidentiÔ¨Åcation,‚Äù in Proc. Eur. Conf. Comput. Vis., 2016, pp. 868‚Äì884.\n[28] M.\nLi,\nX.\nZhu,\nand\nS.\nGong,\n‚ÄúUnsupervised\nperson\nre-\nidentiÔ¨Åcation by deep learning tracklet association,‚Äù in Proc. Eur.\nConf. Comput. Vis., 2018, pp. 737‚Äì753.\n[29] Y.-C. Chen, X. Zhu, W.-S. Zheng, and J.-H. Lai, ‚ÄúPerson re-\nidentiÔ¨Åcation by camera correlation aware feature augmentation,‚Äù\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 40, no. 2, pp. 392‚Äì408,\n2018.\n[30] H. Wang, S. Gong, X. Zhu, and T. Xiang, ‚ÄúHuman-in-the-loop\nperson re-identiÔ¨Åcation,‚Äù in Proc. Eur. Conf. Comput. Vis., 2016, pp.\n405‚Äì422.\n[31] C. Liu, C. Change Loy, S. Gong, and G. Wang, ‚ÄúPop: Person\nre-identiÔ¨Åcation post-rank optimisation,‚Äù in Proc. IEEE Int. Conf.\nComput. Vis., 2013, pp. 441‚Äì448.\n[32] M. Farenzena, L. Bazzani, A. Perina, V. Murino, and M. Cristani,\n‚ÄúPerson re-identiÔ¨Åcation by symmetry-driven accumulation of\nlocal features,‚Äù in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,\n2010, pp. 2360‚Äì2367.\n[33] R. Zhao, W. Ouyang, and X. Wang, ‚ÄúUnsupervised salience learn-\ning for person re-identiÔ¨Åcation,‚Äù in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit., 2013, pp. 3586‚Äì3593.\n[34] X. Liu, M. Song, D. Tao, X. Zhou, C. Chen, and J. Bu,\n‚ÄúSemi-supervised coupled dictionary learning for person re-\nidentiÔ¨Åcation,‚Äù in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,\n2014, pp. 3550‚Äì3557.\n[35] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, ‚ÄúUnpaired image-to-\nimage translation using cycle-consistent adversarial networks,‚Äù in\nProc. IEEE Int. Conf. Comput. Vis., 2017, pp. 2223‚Äì2232.\n[36] W. Deng, L. Zheng, Q. Ye, G. Kang, Y. Yang, and J. Jiao,\n‚ÄúImage-image domain adaptation with preserved self-similarity\nand domain-dissimilarity for person re-identiÔ¨Åcation,‚Äù in Proc.\nIEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 994‚Äì1003.\n[37] Z. Zhong, L. Zheng, S. Li, and Y. Yang, ‚ÄúGeneralizing a person\nretrieval model hetero-and homogeneously,‚Äù in Proc. Eur. Conf.\nComput. Vis., 2018, pp. 172‚Äì188.\nACCEPTED TO APPEAR IN IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n12\n[38] P. Peng, T. Xiang, Y. Wang, M. Pontil, S. Gong, T. Huang, and\nY. Tian, ‚ÄúUnsupervised cross-dataset transfer learning for person\nre-identiÔ¨Åcation,‚Äù in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,\n2016, pp. 1306‚Äì1315.\n[39] J. Lv, W. Chen, Q. Li, and C. Yang, ‚ÄúUnsupervised cross-dataset\nperson re-identiÔ¨Åcation by transfer learning of spatial-temporal\npatterns,‚Äù in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018,\npp. 7948‚Äì7956.\n[40] S. Bak, P. Carr, and J.-F. Lalonde, ‚ÄúDomain adaptation through\nsynthesis for unsupervised person re-identiÔ¨Åcation,‚Äù in Proc. Eur.\nConf. Comput. Vis., 2018, pp. 189‚Äì205.\n[41] A. Argyriou, T. Evgeniou, and M. Pontil, ‚ÄúMulti-task feature\nlearning,‚Äù in Proc. Neur. Info. Proc. Sys., 2007, pp. 41‚Äì48.\n[42] R. Caruana, ‚ÄúMultitask learning,‚Äù Mach. Learn., vol. 28, no. 1, pp.\n41‚Äì75, 1997.\n[43] Q. Dong, S. Gong, and X. Zhu, ‚ÄúMulti-task curriculum transfer\ndeep learning of clothing attributes,‚Äù in Proc. IEEE Win. Conf. App.\nof Comp. Vis., 2017, pp. 520‚Äì529.\n[44] Z. Zhang, P. Luo, C. C. Loy, and X. Tang, ‚ÄúLearning deep repre-\nsentation for face alignment with auxiliary attributes,‚Äù IEEE Trans.\nPattern Anal. Mach. Intell., vol. 38, no. 5, pp. 918‚Äì930, 2016.\n[45] B. A. Olshausen and D. J. Field, ‚ÄúSparse coding with an over-\ncomplete basis set: A strategy employed by v1?‚Äù Vision Research,\nvol. 37, no. 23, pp. 3311‚Äì3325, 1997.\n[46] J. Xie, R. Girshick, and A. Farhadi, ‚ÄúUnsupervised deep embed-\nding for clustering analysis,‚Äù in Proc. Int. Conf. Mach. Learn., 2016,\npp. 478‚Äì487.\n[47] P. Bojanowski and A. Joulin, ‚ÄúUnsupervised learning by predicting\nnoise,‚Äù in Proc. Int. Conf. Mach. Learn., 2017, pp. 517‚Äì526.\n[48] Z. Wu, Y. Xiong, X. Y. Stella, and D. Lin, ‚ÄúUnsupervised feature\nlearning via non-parametric instance discrimination,‚Äù in Proc.\nIEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 3733‚Äì3742.\n[49] X. Wang and A. Gupta, ‚ÄúUnsupervised learning of visual represen-\ntations using videos,‚Äù in Proc. IEEE Int. Conf. Comput. Vis., 2015,\npp. 2794‚Äì2802.\n[50] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and\nA. C. Berg, ‚ÄúSsd: Single shot multibox detector,‚Äù in Proc. Eur. Conf.\nComput. Vis., 2016, pp. 21‚Äì37.\n[51] S. Zhang, R. Benenson, M. Omran, J. Hosang, and B. Schiele, ‚ÄúHow\nfar are we from solving pedestrian detection?‚Äù in Proc. IEEE Conf.\nComput. Vis. Pattern Recognit., 2016, pp. 1259‚Äì1267.\n[52] L. Leal-Taix¬¥e, A. Milan, I. Reid, S. Roth, and K. Schindler,\n‚ÄúMotchallenge 2015: Towards a benchmark for multi-target track-\ning,‚Äù arXiv:1504.01942, 2015.\n[53] M. Belkin, P. Niyogi, and V. Sindhwani, ‚ÄúManifold regularization:\nA geometric framework for learning from labeled and unlabeled\nexamples,‚Äù Journ. of Mach. Learn. Res., vol. 7, pp. 2399‚Äì2434, 2006.\n[54] L. Zelnik-Manor and P. Perona, ‚ÄúSelf-tuning spectral clustering,‚Äù\nin Proc. Neur. Info. Proc. Sys., 2005, pp. 1601‚Äì1608.\n[55] G. Hinton, O. Vinyals, and J. Dean, ‚ÄúDistilling the knowledge in a\nneural network,‚Äù arXiv preprint arXiv:1503.02531, 2015.\n[56] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals, ‚ÄúUn-\nderstanding deep learning requires rethinking generalization,‚Äù in\nProc. Int. Conf. on Learn. Rep., 2017.\n[57] D. Qin, S. Gammeter, L. Bossard, T. Quack, and L. Van Gool,\n‚ÄúHello neighbor: Accurate object retrieval with k-reciprocal near-\nest neighbors,‚Äù in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,\n2011, pp. 777‚Äì784.\n[58] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, ‚ÄúCurriculum\nlearning,‚Äù in Proc. Int. Conf. Mach. Learn., 2009, pp. 41‚Äì48.\n[59] D. R. Hardoon, S. Szedmak, and J. Shawe-Taylor, ‚ÄúCanonical\ncorrelation analysis: An overview with application to learning\nmethods,‚Äù Neural Computation, vol. 16, no. 12, pp. 2639‚Äì2664, 2004.\n[60] F. Schroff, D. Kalenichenko, and J. Philbin, ‚ÄúFacenet: A uniÔ¨Åed\nembedding for face recognition and clustering,‚Äù in Proc. IEEE Conf.\nComput. Vis. Pattern Recognit., 2015, pp. 815‚Äì823.\n[61] H. Lee, C. Ekanadham, and A. Y. Ng, ‚ÄúSparse deep belief net\nmodel for visual area v2,‚Äù in Proc. Neur. Info. Proc. Sys., 2008, pp.\n873‚Äì880.\n[62] T. Xiao, H. Li, W. Ouyang, and X. Wang, ‚ÄúLearning deep fea-\nture representations with domain guided dropout for person re-\nidentiÔ¨Åcation,‚Äù in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,\n2016, pp. 1249‚Äì1258.\n[63] J. Ye, Z. Zhao, and H. Liu, ‚ÄúAdaptive distance metric learning for\nclustering,‚Äù in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2007,\npp. 1‚Äì7.\n[64] C. Qin, S. Song, G. Huang, and L. Zhu, ‚ÄúUnsupervised neigh-\nborhood component analysis for clustering,‚Äù Neurocomputing, vol.\n168, pp. 609‚Äì617, 2015.\n[65] D. Chen, D. Xu, H. Li, N. Sebe, and X. Wang, ‚ÄúGroup consistent\nsimilarity learning via deep crf for person re-identiÔ¨Åcation,‚Äù in\nProc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 8649‚Äì8658.\n[66] Y. Wu, Y. Lin, X. Dong, Y. Yan, W. Ouyang, and Y. Yang, ‚ÄúEx-\nploit the unknown gradually: One-shot video-based person re-\nidentiÔ¨Åcation by stepwise learning,‚Äù in Proc. IEEE Conf. Comput.\nVis. Pattern Recognit., 2018, pp. 5177‚Äì5186.\n[67] M.\nGou,\nS.\nKaranam,\nW.\nLiu,\nO.\nCamps,\nand\nR.\nJ.\nRadke, ‚ÄúDukemtmc4reid: A large-scale multi-camera person re-\nidentiÔ¨Åcation dataset,‚Äù in Workshop of IEEE Conf. Comput. Vis.\nPattern Recognit., 2017, pp. 10‚Äì19.\n[68] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,\nD. Erhan, V. Vanhoucke, and A. Rabinovich, ‚ÄúGoing deeper with\nconvolutions,‚Äù in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,\n2015, pp. 1‚Äì9.\n[69] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning\nfor image recognition,‚Äù in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit., 2016, pp. 770‚Äì778.\n[70] D. P. Kingma and J. Ba, ‚ÄúAdam: A method for stochastic optimiza-\ntion,‚Äù arXiv:1412.6980, 2014.\n[71] M. Ye, X. Lan, and P. C. Yuen, ‚ÄúRobust anchor embedding for\nunsupervised video person re-identiÔ¨Åcation in the wild,‚Äù in Proc.\nEur. Conf. Comput. Vis., 2018, pp. 170‚Äì186.\n[72] Y. Chen, X. Zhu, and S. Gong, ‚ÄúDeep association learning for\nunsupervised video person re-identiÔ¨Åcation,‚Äù Proc. Bri. Mach. Vis.\nConf., 2018.\n[73] D. Chen, H. Li, T. Xiao, S. Yi, and X. Wang, ‚ÄúVideo person re-\nidentiÔ¨Åcation with competitive snippet-similarity aggregation and\nco-attentive snippet embedding,‚Äù in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit., 2018, pp. 1169‚Äì1178.\n[74] W. Li, R. Zhao, and X. Wang, ‚ÄúHuman reidentiÔ¨Åcation with\ntransferred metric learning,‚Äù in Proc. Asi. Conf. Comp. Vis., 2012,\npp. 31‚Äì44.\n[75] D. Gray and H. Tao, ‚ÄúViewpoint invariant pedestrian recognition\nwith an ensemble of localized features,‚Äù in Proc. Eur. Conf. Comput.\nVis., 2008, pp. 262‚Äì275.\n[76] B. J. Prosser, W.-S. Zheng, S. Gong, and T. Xiang, ‚ÄúPerson re-\nidentiÔ¨Åcation by support vector ranking,‚Äù in Proc. Bri. Mach. Vis.\nConf., 2010.\n[77] D. J. Pearce, ‚ÄúAn improved algorithm for Ô¨Ånding the strongly\nconnected components of a directed graph,‚Äù Victoria University,\nWellington, NZ, Tech. Rep, 2005.\n[78] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger,\n‚ÄúDensely connected convolutional networks,‚Äù in Proc. IEEE Conf.\nComput. Vis. Pattern Recognit., 2017, pp. 4700‚Äì4708.\nMinxian Li is a postdoctoral researcher at\nQueen Mary University of London, United Kin-\ndom and also an assistant professor of Nanjing\nUniversity of Science and Technology, China. He\nreceived his Ph.D. in Pattern Recognition and\nIntelligent System from Nanjing University of Sci-\nence and Technology, China. His research inter-\nests include computer vision, pattern recognition\nand deep learning.\nXiatian Zhu is a Computer Vision Researcher\nat Vision Semantics Limited, London, UK. He\nreceived his Ph.D. from Queen Mary University\nof London. He won The Sullivan Doctoral Thesis\nPrize 2016, an annual award representing the\nbest doctoral thesis submitted to a UK Univer-\nsity in computer vision. His research interests\ninclude computer vision and machine learning.\nShaogang Gong is Professor of Visual Compu-\ntation at Queen Mary University of London (since\n2001), a Fellow of the Institution of Electrical En-\ngineers and a Fellow of the British Computer So-\nciety. He received his D.Phil (1989) in computer\nvision from Keble College, Oxford University. His\nresearch interests include computer vision, ma-\nchine learning and video analysis.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2019-03-01",
  "updated": "2019-03-01"
}