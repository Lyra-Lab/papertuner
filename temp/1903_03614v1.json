{
  "id": "http://arxiv.org/abs/1903.03614v1",
  "title": "Gradient Descent based Optimization Algorithms for Deep Learning Models Training",
  "authors": [
    "Jiawei Zhang"
  ],
  "abstract": "In this paper, we aim at providing an introduction to the gradient descent\nbased optimization algorithms for learning deep neural network models. Deep\nlearning models involving multiple nonlinear projection layers are very\nchallenging to train. Nowadays, most of the deep learning model training still\nrelies on the back propagation algorithm actually. In back propagation, the\nmodel variables will be updated iteratively until convergence with gradient\ndescent based optimization algorithms. Besides the conventional vanilla\ngradient descent algorithm, many gradient descent variants have also been\nproposed in recent years to improve the learning performance, including\nMomentum, Adagrad, Adam, Gadam, etc., which will all be introduced in this\npaper respectively.",
  "text": "IFM LAB TUTORIAL SERIES # 1, COPYRIGHT c⃝IFM LAB\nGradient Descent based Optimization Algorithms for Deep\nLearning Models Training\nJiawei Zhang\njiawei@ifmlab.org\nFounder and Director\nInformation Fusion and Mining Laboratory\n(First Version: February 2019; Revision: March 2019.)\nAbstract\nIn this paper, we aim at providing an introduction to the gradient descent based op-\ntimization algorithms for learning deep neural network models. Deep learning models in-\nvolving multiple nonlinear projection layers are very challenging to train. Nowadays, most\nof the deep learning model training still relies on the back propagation algorithm actually.\nIn back propagation, the model variables will be updated iteratively until convergence with\ngradient descent based optimization algorithms. Besides the conventional vanilla gradient\ndescent algorithm, many gradient descent variants have also been proposed in recent years\nto improve the learning performance, including Momentum, Adagrad, Adam, Gadam, etc.,\nwhich will all be introduced in this paper respectively.\nKeywords:\nGradient Descent; Optimization Algorithm; Deep Learning\nContents\n1\nIntroduction\n2\n2\nConventional Gradient Descent based Learning Algorithms\n6\n2.1\nVanilla Gradient Descent\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.2\nStochastic Gradient Descent . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.3\nMini-batch Gradient Descent\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n2.4\nPerformance Analysis of Vanilla GD, SGD and Mini-Batch GD . . . . . . .\n9\n3\nMomentum based Learning Algorithms\n10\n3.1\nMomentum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n3.2\nNesterov Accelerated Gradient\n. . . . . . . . . . . . . . . . . . . . . . . . .\n12\n4\nAdaptive Gradient based Learning Algorithms\n13\n4.1\nAdagrad . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n4.2\nRMSprop . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n4.3\nAdadelta . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n5\nMomentum & Adaptive Gradient based Learning Algorithms\n17\n5.1\nAdam\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n5.2\nNadam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n1\narXiv:1903.03614v1  [cs.LG]  11 Mar 2019\nJIAWEI ZHANG, IFM LAB DIRECTOR\n6\nHybrid Evolutionary Gradient Descent Learning Algorithms\n20\n6.1\nGadam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n6.1.1\nModel Population Initialization . . . . . . . . . . . . . . . . . . . . .\n21\n6.1.2\nModel Learning with Adam . . . . . . . . . . . . . . . . . . . . . . .\n22\n6.1.3\nModel Evolution with Genetic Algorithm\n. . . . . . . . . . . . . . .\n22\n6.1.4\nNew Generation Selection and Evolution Stop Criteria . . . . . . . .\n24\n7\nA Summary\n24\n1. Introduction\nIn the real-world research and application works about deep learning, training the deep\nmodels eﬀectively still remains one of the most challenging work for both researchers and\npractitioners. By this context so far, most of the deep model training is still based on the\nback propagation algorithm, which propagates the errors from the output layer backward\nand updates the variables layer by layer with the gradient descent based optimization al-\ngorithms. Gradient descent plays an important role in training the deep learning models,\nand lots of new variant algorithms have been proposed in recent years to further improve\nits performance. Compared with the high-order derivative based algorithms, e.g., Newton’s\nmethod, etc., gradient descent together with its various variant algorithms based on ﬁrst-\norder derivative are much more eﬃcient and practical for deep models. In this paper, we\nwill provide a comprehensive introduction to the deep learning model training algorithms.\nFormally, given the training data T = {(x1, y1), (x2, y2), · · · , (xn, yn)} involving n pairs\nof feature-label vectors, where the feature vector xi ∈Rdx and label vector yi ∈Rdy are of\ndimensions dx and dy respectively. We can represent the deep learning model used to classify\nthe data as a mapping: F(·; θ) : X →Y, where X and Y denotes the feature and label space\nrespectively and θ denotes the variable vector involved in the mapping. Formally, given an\ninput data instance featured by vector xi ∈X, we can denotes the output by the deep\nlearning model as ˆyi = F(xi; θ). Compared against the true label vector yi of the input\ninstance, we can represent the introduced error by the model with the loss term ℓ(ˆyi, yi).\nIn the remaining part of this paper, we may use error and loss interchangeably without any\ndiﬀerentiations.\nMany diﬀerent loss functions, i.e., ℓ(·, ·), have been proposed to measure the introduced\nmodel errors. Some representative examples include\n• Mean Square Error (MSE):\nℓMSE(ˆyi, yi) = 1\ndy\ndy\nX\nj=1\n(yi(j) −ˆyi(j))2 .\n(1)\n• Mean Absolute Error (MAE):\nℓMAE(ˆyi, yi) = 1\ndy\ndy\nX\nj=1\n|yi(j) −ˆyi(j)| .\n(2)\n2\nIFM LAB TUTORIAL SERIES # 1, COPYRIGHT c⃝IFM LAB\nClass\nInstance 1\nInstance 2\nInstance 3\nDog\n0.49\n0.45\n0.21\nCat\n0.43\n0.53\n0.15\nBird\n0.08\n0.02\n0.64\nFigure 1: An Example to Illustrate the Hinge Loss and Cross Entropy Loss Functions.\n• Hinge Loss:\nℓHinge(ˆyi, yi) =\nX\nj̸=lyi\nmax(0, ˆyi(j) −ˆyi(lyi) + 1),\n(3)\nwhere lyi denotes the true class label index for the instance according to vector yi.\n• Cross Entropy Loss:\nℓCE(ˆyi, yi) = −\ndy\nX\nj=1\nyi(j) log ˆyi(j).\n(4)\nDepending on the number of classes involved as well as the activation functions being\nused for the model, the cross entropy function can be further speciﬁed into the sigmoid\ncross entropy (for the binary classiﬁcation tasks with sigmiod function as the activation\nfunction for the output layer) and softmax cross entropy (for the multi-class classiﬁcation\ntasks with softmax function as the activation function for the output layer).\nThe mean square error and mean absolute error are usually used for the regression tasks,\nwhile the hinge loss and cross entropy are usually used for the classiﬁcation tasks instead.\nBesides these loss functions introduced above, many other loss functions can be used for\nthe speciﬁc learning problems as well, e.g., huber loss, cosine distance, and the readers may\nrefer to the referred articles for more information.\nTo provide more information about the hinge loss and the cross entropy loss functions,\nwe will use an example to illustrate their usages in real-world problems.\n3\nJIAWEI ZHANG, IFM LAB DIRECTOR\nExample 1 As show in Figure 1, based on the three input images (i.e., the data instances),\nwe can achieve their prediction labels by a deep learning model as shown in the bottom table.\nFor the dog input image (i.e., instance 1), we know its true label should be y1 = [1, 0, 0]⊤\nand the prediction label is ˆy1 = [0.49, 0.43, 0.08]⊤, where these three entries in the vector\ncorrespond to the dog, cat and bird class labels respectively. Similarly, we can represent\nthe true labels and prediction labels of the cat image and bird image (i.e., instance 2 and\ninstance 3) as vectors y2 = [0, 1, 0]⊤, ˆy2 = [0.45, 0.53, 0.02]⊤, y3 = [0, 0, 1]⊤, and ˆy3 =\n[0.21, 0.15, 0.64]⊤respectively.\nBased on the true labels and the prediction labels, we can represent the introduced loss\nterms by the deep learning model as follows\n• Instance 1: For instance 1, we know its true label is dog (i.e., entry 1 in the label\nvector), and its true label index is ly1 = 1. Therefore, we can represent the introduced\nhinge loss for instance 1 as follows:\nℓHinge(ˆy1, y1) =\nX\nj̸=1\nmax(0, ˆy1(j) −ˆy1(1) + 1)\n= max(0, ˆy1(2) −ˆy1(1) + 1) + max(0, ˆy1(3) −ˆy1(1) + 1)\n= max(0, 0.43 −0.49 + 1) + max(0, 0.08 −0.49 + 1)\n= 1.53.\n(5)\n• Instance 2: For instance 2, we know its true label is cat (i.e., entry 2 in the label\nvector), and its true label index will be ly2 = 2. Therefore, the introduced hinge loss\nfor instance 2 can be represented as\nℓHinge(ˆy2, y2) =\nX\nj̸=2\nmax(0, ˆy2(j) −ˆy2(2) + 1)\n= max(0, ˆy2(1) −ˆy2(2) + 1) + max(0, ˆy2(3) −ˆy2(2) + 1)\n= max(0, 0.45 −0.53 + 1) + max(0, 0.02 −0.53 + 1)\n= 1.41.\n(6)\n• Instance 3: Similarly, for the input instance 3, we know its true label index is ly3 = 3,\nand the introduced hinge loss for the instance can be denoted as\nℓHinge(ˆy3, y3) =\nX\nj̸=3\nmax(0, ˆy3(j) −ˆy3(3) + 1)\n= max(0, ˆy3(1) −ˆy3(3) + 1) + max(0, ˆy2(2) −ˆy3(3) + 1)\n= max(0, 0.21 −0.64 + 1) + max(0, 0.15 −0.64 + 1)\n= 1.08.\n(7)\nMeanwhile, based on the true label and the prediction label vectors of these 3 input data\ninstances, we can represent their introduced cross entropy loss as follows:\n4\nIFM LAB TUTORIAL SERIES # 1, COPYRIGHT c⃝IFM LAB\n• Instance 1:\nℓCE(ˆy1, y1) = −\ndy\nX\nj=1\ny1(j) log ˆy1(j)\n= −1 × log 0.49 −0 × log 0.43 −0 × log 0.08\n= 0.713.\n(8)\n• Instance 2:\nℓCE(ˆy2, y2) = −\ndy\nX\nj=1\ny2(j) log ˆy2(j)\n= −0 × log 0.45 −1 × log 0.53 −0 × log 0.02\n= 0.635.\n(9)\n• Instance 3:\nℓCE(ˆy3, y3) = −\ndy\nX\nj=1\ny3(j) log ˆy3(j)\n= −0 × log 0.21 −0 × log 0.15 −1 × log 0.64\n= 0.446.\n(10)\nFurthermore, based on the introduced loss for all the data instances in the training set,\nwe can represent the total introduced loss term by the deep learning model as follows\nL(θ; T ) =\nX\n(xi,yi)∈T\nℓ(ˆyi, yi) =\nX\n(xi,yi)∈T\nℓ(F(xi; θ), yi).\n(11)\nBy ﬁtting the training data, the training of the deep learning model aims at minimizing\nthe loss introduced on the training set so as to achieve the optimal variables, i.e., the optimal\nθ. Formally, the objective function of deep learning model training can be represented as\nfollows:\nmin\nθ∈Θ L(θ; T ),\n(12)\nwhere Θ denotes the variable domain and we can have Θ = Rdθ (dθ denotes the dimension of\nvariable vector θ) if there is no other constraints on the variables to be learned. To address\nthe objective function, in the following sections, we will introduce a group of gradient descent\nbased optimization algorithms, which can learn the globally optimal or locally optimal\nvariables for the deep learning models in the case if the loss function is convex or non-\nconvex respectively. A more comprehensive experimental analysis about their performance\nwill be provided at the end of this paper with some hands-on experiments on some real-world\ndatasets.\n5\nJIAWEI ZHANG, IFM LAB DIRECTOR\n2. Conventional Gradient Descent based Learning Algorithms\nIn this section, we will introduce the conventional vanilla gradient descent, the stochastic\ngradient descent and the mini-batch gradient descent algorithms.\nThe main diﬀerences\namong them lie in the amount of training data used to update the model variables in each\niteration.\nThey will also serve as the base algorithms for the variant algorithms to be\nintroduced in the following sections.\n2.1 Vanilla Gradient Descent\nVanilla gradient descent is also well-known as the the batch gradient descent. Given the\ntraining set T as introduced in the previous section, the vanilla gradient descent optimiza-\ntion algorithm optimizes the model variables iteratively with the following equation:\nθ(τ) = θ(τ−1) −η · ∇θL(θ(τ−1); T ),\n(13)\nwhere τ ≥1 denotes the updating iteration.\nIn the above updating equation, the physical meanings of notations θ(τ), ∇θL(θ(τ−1); T )\nand η are illustrated as follows:\n1. Term θ(τ) denotes the model variable vector updated via iteration τ. At τ = 0, vector\nθ(0) denotes the initial model variable vector, which is usually randomly initialized\nsubject to certain distributions, e.g., uniform distribution U(a, b) or normal distri-\nbution N(µ, σ2). To achieve better performance, the hyper-parameters a, b in the\nuniform distribution, or µ, σ in the normal distribution can be ﬁne-tuned.\n2. Term ∇θL(θ(τ−1); T ) denotes the derivative of the loss function L(θ; T ) regarding the\nvariable θ based on the complete training set T and the variable vector value θ(τ−1)\nachieved in iteration τ −1.\n3. Term η denotes the learning rate in the gradient descent algorithm, which is usually\na hyper-parameter with a small value (e.g., 10−3). Fine-tuning of the parameter is\nnecessary to achieve a fast convergence in practical applications of the vanilla gradient\ndescent algorithm.\nSuch an iterative updating process continues until convergence, and the variable vector\nθ achieved at convergence will be outputted as the (globally or locally) optimal variable\nfor the deep learning models. The pseudo-code of the vanilla gradient descent algorithm\nis available in Algorithm 1. In Algorithm 1, the model variables are initialized with the\nnormal distribution. The normal distribution standard deviation σ is the input parameters,\nand the mean µ is set as 0. As to the the convergence condition, it can be (1) the loss term\nchanges in two sequential iterations is less than a pre-speciﬁed threshold, (2) the model\nvariable θ changes is within a pre-speciﬁed range, or (3) the pre-speciﬁed iteration round\nnumber has been reached.\n6\nIFM LAB TUTORIAL SERIES # 1, COPYRIGHT c⃝IFM LAB\nAlgorithm 1 Vanilla Gradient Descent\nRequire: Training Set: T ; Learning Rate η; Normal Distribution Std: σ.\nEnsure: Model Parameter θ\n1: Initialize parameter with Normal distribution θ ∼N(0, σ2)\n2: Initialize convergence tag = False\n3: while tag == False do\n4:\nCompute gradient ∇θL(θ; T ) on the training set T\n5:\nUpdate variable θ = θ −η · ∇θL(θ; T )\n6:\nif convergence condition holds then\n7:\ntag = True\n8:\nend if\n9: end while\n10: Return model variable θ\nAccording to the descriptions, in the vanilla gradient descent algorithm, to update\nthe model variables, we need to compute the gradient of the loss function regarding the\nvariable, i.e., ∇θL(θ(τ−1); T ), in each iteration, which is usually very time consuming for\na large training set. To improve the learning eﬃciency, we will introduce the stochastic\ngradient descent and mini-batch gradient descent learning algorithms in the following two\nsubsections.\n2.2 Stochastic Gradient Descent\nVanilla gradient descent computes loss function gradient with the complete training set,\nwhich will be very ineﬃcient if the training set contains lots of data instances. Instead of\nusing the complete training set, the stochastic gradient descent (SGD) algorithm updates the\nmodel variables by computing the loss function gradient instances by instances. Therefore,\nstochastic gradient descent can be much faster than vanilla gradient descent, but it may\nalso cause heavy ﬂuctuations of the loss function in the updating process.\nFormally, given the training set T and the initialized model variable vector θ(0), for each\ninstance (xi, yi) ∈T , stochastic gradient descent algorithm will update the model variable\nwith the following equation iteratively:\nθ(τ) = θ(τ−1) −η · ∇θL(θ(τ−1); (xi, yi)),\n(14)\nwhere the loss term L(θ; (xi, yi)) = ℓ(F(xi; θ), yi) denotes the loss introduced by the model\non data instance (xi, yi). The pseudo-code of the stochastic gradient descent learning algo-\nrithm is available in Algorithm 2\n7\nJIAWEI ZHANG, IFM LAB DIRECTOR\nAlgorithm 2 Stochastic Gradient Descent\nRequire: Training Set T ; Learning Rate η; Normal Distribution Std: σ.\nEnsure: Model Parameter θ\n1: Initialize parameter with Normal distribution θ ∼N(0, σ2)\n2: Initialize convergence tag = False\n3: while tag == False do\n4:\nShuﬄe the training set T\n5:\nfor each data instance (xi, yi) ∈T do\n6:\nCompute gradient ∇θL(θ; (xi, yi)) on the training instance (xi, yi)\n7:\nUpdate variable θ = θ −η · ∇θL(θ; (xi, yi))\n8:\nend for\n9:\nif convergence condition holds then\n10:\ntag = True\n11:\nend if\n12: end while\n13: Return model variable θ\nAccording to Algorithm 2, the whole training set will be shuﬄed before the updating\nprocess. Even though the learning process of stochastic gradient descent may ﬂuctuate a\nlot, which actually also provides stochastic gradient descent with the ability to jump out\nof local optimum. Meanwhile, by selecting a small learning rate η and decreasing it in the\nlearning process, stochastic gradient descent can almost certainly converge to the local or\nglobal optimum.\n2.3 Mini-batch Gradient Descent\nTo balance between the vanilla gradient descent and stochastic gradient descent learning\nalgorithms, mini-batch gradient descent proposes to update the model variables with a mini-\nbatch of training instances instead. Formally, let B ⊂T denote the mini-batch of training\ninstances sampled from the training set T . We can represent the variable updating equation\nof the deep learning model with the mini-batch as follows:\nθ(τ) = θ(τ−1) −η · ∇θL(θ(τ−1); B),\n(15)\nwhere L(θ; B) denotes the loss term introduced by the model on the mini-batch B. The\npseudo-code of the mini-batch gradient descent algorithm can be illustrated in Algorithm 3.\n8\nIFM LAB TUTORIAL SERIES # 1, COPYRIGHT c⃝IFM LAB\nAlgorithm 3 Mini-batch Gradient Descent\nRequire: Training Set T ; Learning Rate η; Normal Distribution Std σ; Mini-batch Size b.\nEnsure: Model Parameter θ\n1: Initialize parameter with Normal distribution θ ∼N(0, σ2)\n2: Initialize convergence tag = False\n3: while tag == False do\n4:\nShuﬄe the training set T\n5:\nfor each mini-batch B ⊂T do\n6:\nCompute gradient ∇θL(θ; B) on the mini-batch B\n7:\nUpdate variable θ = θ −η · ∇θL(θ; B)\n8:\nend for\n9:\nif convergence condition holds then\n10:\ntag = True\n11:\nend if\n12: end while\n13: Return model variable θ\nIn the mini-batch gradient descent algorithm, the batch size b is provided as a parameter,\nwhich can take values, like 64, 128 or 256. In most of the cases, b should not be very large\nand its speciﬁc value depends on the size of the training set a lot. The mini-batches are\nusually sampled sequentially from the training set T . In other words, the training set T\ncan be divided into multiple batches of size b, and these batches can be picked one by one\nfor updating the model variables sequentially. In some versions of the mini-batch gradient\ndescent, instead of sequential batch selection, they propose to randomly sample the mini-\nbatches from the training set, which is also referred to as the random mini-batch generation\nprocess.\nCompared with vanilla gradient descent, the mini-batch gradient descent algorithm is\nmuch more eﬃcient especially for the training set of an extremely large size. Meanwhile,\ncompared with the stochastic gradient descent, the mini-batch gradient descent algorithm\ngreatly reduces the variance in the model variable updating process and can achieve much\nmore stable convergence.\n2.4 Performance Analysis of Vanilla GD, SGD and Mini-Batch GD\nThese three gradient descent algorithms introduced here work well for many optimization\nproblems, and can all converge to a promising (local or global) optimum. However, these\nalgorithms also suﬀer from several problems listed as follows:\n• Learning Rate Selection: The learning rate η may aﬀect the convergence of the gradi-\nent descent algorithms a lot. A large learning rate may diverge the learning process,\nwhile a small learning rate renders the convergence too slow. Therefore, selecting a\ngood learning rate will be very important for the gradient descent algorithms.\n• Learning Rate Adjustment: In most of the cases, a ﬁxed learning rate in the whole\nupdating process cannot work well for the gradient descent algorithms. In the initial\nstages, the algorithm may need a larger learning rate to reach a good (local or global)\noptimum fast. However, in the later stages, the algorithm may need to adjust the\nlearning rate with a smaller value to ﬁne-tune the performance instead.\n9\nJIAWEI ZHANG, IFM LAB DIRECTOR\n• Variable Individual Learning Rate: For diﬀerent variables, their update may actually\nrequire a diﬀerent learning rates instead in the updating process. Therefore, how to\nuse an individual learning rate for diﬀerent variables is required and necessary.\n• Saddle Point Avoid: Formally, the saddle point denotes the points with zero gradient\nin all the dimensions. However, for some of the dimensions, the saddle point is the\nlocal minimum, while for some other dimensions, the point is the local maximum.\nHow to get out from such saddle points is a very challenging problem.\nIn the following part of this paper, we will introduce several other gradient descent\nbased learning algorithm variants, which are mainly proposed to resolve one or several of\nthe above problems.\n3. Momentum based Learning Algorithms\nIn this section, we will introduce a group of gradient descent variant algorithms, which\npropose to update the model variables with both the current gradient as well as the his-\ntorical gradients simultaneously, which are named as the momentum based gradient descent\nalgorithm. Here, these algorithms will be talked about by using the mini-batch GD as the\nbase learning algorithm.\n3.1 Momentum\nTo smooth the ﬂuctuation encountered in the learning process for the gradient descent\nalgorithms (e.g., mini-batch gradient descent), Momentum [6] is proposed to accelerate the\nupdating convergence of the variables. Formally, Momentum updates the variables with the\nfollowing equation:\nθ(τ) = θ(τ−1) −η · ∆v(τ) where ∆v(τ)= ρ · ∆v(τ−1) + (1 −ρ) · ∇θL(θ(τ−1)),\n(16)\nIn the above equation, v(τ) denotes the momentum term introduced for keeping record of\nthe historical gradients till iteration τ and function L(θ) = L(θ; B) denotes the loss function\non a mini-batch B. Parameter ρ ∈[0, 1] denotes the weight of the momentum term. The\npseudo-code of the Momentum based gradient descent learning algorithm is available in\nAlgorithm 4.\n10\nIFM LAB TUTORIAL SERIES # 1, COPYRIGHT c⃝IFM LAB\nAlgorithm 4 Momentum based Mini-batch Gradient Descent\nRequire: Training Set T ; Learning Rate η; Normal Distribution Std σ; Mini-batch Size b; Momen-\ntum Term Weight: ρ.\nEnsure: Model Parameter θ\n1: Initialize parameter with Normal distribution θ ∼N(0, σ2)\n2: Initialize Momentum term ∆v = 0\n3: Initialize convergence tag = False\n4: while tag == False do\n5:\nShuﬄe the training set T\n6:\nfor each mini-batch B ⊂T do\n7:\nCompute gradient ∇θL(θ; B) on the mini-batch B\n8:\nUpdate term ∆v = ρ · ∆v + (1 −ρ) · ∇θL(θ; B)\n9:\nUpdate variable θ = θ −η · ∆v\n10:\nend for\n11:\nif convergence condition holds then\n12:\ntag = True\n13:\nend if\n14: end while\n15: Return model variable θ\nBased on such a recursive updating equation of vector θ(τ), we can actually achieve\nan equivalent representation of θ(τ) merely with the gradient terms of the loss function\naccording to the following Lemma.\nLemma 1 Vector ∆v(τ) can be formally represented with the following equation:\n∆v(τ) =\nτ−1\nX\nt=0\nρt · (1 −ρ) · ∇θL(θ(τ−1−t)).\n(17)\nProof The Lemma can be proved by induction on τ.\n1. Base Case: In the case that τ = 1, according to the recursive representation of θ, we\nhave\n∆v(1) = ρ · ∆v(0) + (1 −ρ) · ∇θL(θ(0))\n= (1 −ρ) · ∇θL(θ(0)),\n(18)\nwhere ∆v(0) is initialized as a zero vector.\n2. Assumption: We assume the equation holds for τ = k, i.e.,\n∆v(k) =\nk−1\nX\nt=0\nρt · (1 −ρ) · ∇θL(θ(k−1−t)).\n(19)\n11\nJIAWEI ZHANG, IFM LAB DIRECTOR\n3. Induction: For τ = k+1, based on Equation (16), we can represent the vector ∆v(k+1)\nto be\n∆v(k+1) = ρ · ∆v(k) + (1 −ρ) · ∇θL(θ(k))\n= ρ ·\nk−1\nX\nt=0\nρt · (1 −ρ) · ∇θL(θ(k−1−t)) + (1 −ρ) · ∇θL(θ(k))\n=\nk\nX\nt=1\nρt · (1 −ρ) · ∇θL(θ(k−t)) + (1 −ρ) · ∇θL(θ(k))\n=\nk\nX\nt=0\nρt · (1 −ρ) · ∇θL(θ(k−t)),\n(20)\nwhich can conclude this proof.\nConsidering the parameter ρ ∈[0, 1], the gradients of the iterations which are far away\nfrom the current iteration will decay exponentially. Compared with the conventional gradi-\nent descent algorithm, the Momentum can achieve a faster convergence rate, which can be\nexplained based on the gradient updated in the above equation. For the gradient descent\nalgorithm, the gradient updated in iteration τ can be denoted as ∇θL(θ(τ−1)), while the up-\ndated gradient for Momentum can be denoted as ∆v(τ) = (1−ρ)·∇θL(θ(τ−1))+ρ·∆v(τ−1).\nTerm ∆v(τ−1) keeps records of the historical gradients. As the gradient descent algorithms\nupdates the variables from the steep zone to the relatively ﬂat zone, the large historical\ngradient terms stored in ∆v(τ−1) allows will actually accelerate the algorithm to reach the\n(local or global) optimum.\n3.2 Nesterov Accelerated Gradient\nFor the gradient descent algorithms introduced so far, they all update the variables based\non the gradients at both the historical or current points without knowledge about the future\npoints they will reach in the learning process. It makes the learning process to be blind and\nthe learning performance highly unpredictable.\nThe Nesterov Accelerated Gradient (i.e., NAG) [5] method propose to resolve this prob-\nlem by updating the variables with the gradient at an approximated future point instead.\nThe variable θ to be achieved at step τ can be denoted as θ(τ), and it can be estimated\nas ˆθ\n(τ) = θ(τ−1) −η · ρ · ∆v(τ−1), where ∆v(τ−1) denotes the momentum term at iteration\nτ −1. Formally, assisted with the look-ahead gradient and momentum, the variable updating\nequations in NAG can be formally represented as follows:\nθ(τ) = θ(τ−1) −η · ∆v(τ), where\n(ˆθ\n(τ)\n= θ(τ−1) −η · ρ · ∆v(τ−1),\n∆v(τ)\n= ρ · ∆v(τ−1) + (1 −ρ) · ∇θL(ˆθ\n(τ)).\n(21)\nBy computing the gradient one step forward, the NAG algorithm is able to update the\nmodel variables much more eﬀectively. The pseudo-code of the NAG learning algorithm is\navailable in Algorithm 5.\n12\nIFM LAB TUTORIAL SERIES # 1, COPYRIGHT c⃝IFM LAB\nAlgorithm 5 NAG based Mini-batch Gradient Descent\nRequire: Training Set T ; Learning Rate η; Normal Distribution Std σ; Mini-batch Size b; Momen-\ntum Term Weight: ρ.\nEnsure: Model Parameter θ\n1: Initialize parameter with Normal distribution θ ∼N(0, σ2)\n2: Initialize Momentum term ∆v = 0\n3: Initialize convergence tag = False\n4: while tag == False do\n5:\nShuﬄe the training set T\n6:\nfor each mini-batch B ⊂T do\n7:\nCompute ˆθ = θ −η · ρ · ∆v\n8:\nCompute gradient ∇θL(ˆθ; B) on the mini-batch B\n9:\nUpdate term ∆v = ρ · ∆v + (1 −ρ) · ∇θL(ˆθ; B)\n10:\nUpdate variable θ = θ −η · ∆v\n11:\nend for\n12:\nif convergence condition holds then\n13:\ntag = True\n14:\nend if\n15: end while\n16: Return model variable θ\n4. Adaptive Gradient based Learning Algorithms\nIn this section, we will introduce a group of learning algorithms which updates the variables\nwith adaptive learning rates. The algorithms introduced here include Adagrad, RMSprop\nand Adadelta respectively.\n4.1 Adagrad\nFor the learning algorithms introduced before, the learning rate in these methods are mostly\nﬁxed and identical for all the variables in vector θ. However, in the variable updating pro-\ncess, for diﬀerent variables, their required learning rates can be diﬀerent. For the variables\nreaching the optimum, a smaller learning rate is needed; while for the variables far away\nfrom the optimum, we may need to use a relatively larger learning rate so as to reach the\noptimum faster. To resolve these two problems, in this part, we will introduce one adaptive\ngradient method, namely Adagrad [3].\nFormally, the learning equation for Adagrad can be represented as follows:\nθ(τ) = θ(τ−1) −\nη\np\ndiag(G(τ)) + ϵ · I\ng(τ−1) where\n(\ng(τ−1)\n= ∇θL(θ(τ−1)),\nG(τ)\n= Pτ−1\nt=0 g(t)(g(t))⊤.\n(22)\nIn the above updating equation, operator diag(G) deﬁnes a diagonal matrix of the same\ndimensions as G but only with the elements on the diagonal of G, and ϵ is a smoothing\nterm to avoid the division of zero values on the diagonal of the matrix. Matrix G(τ) keeps\nrecords of the computed historical gradients from the beginning until the current iteration.\nConsidering the values on the diagonal of matrix G(τ) will be diﬀerent, the learning rate of\nvariables in vector θ will be diﬀerent, e.g., η(τ)\ni\n=\nη\n√\nG(τ)(i,i)+ϵ for variable θ(i) in iteration\n13\nJIAWEI ZHANG, IFM LAB DIRECTOR\nτ. In addition, since the matrix G(τ) will be updated in each iteration, the learning rate for\nthe same variable in diﬀerent iterations will be diﬀerent as well, which is the reason why\nthe algorithm is called the adaptive learning algorithms. The pseudo-code of Adagrad is\nprovided in Algorithm 6.\nAlgorithm 6 Adagrad based Mini-batch Gradient Descent\nRequire: Training Set T ; Learning Rate η; Normal Distribution Std σ; Mini-batch Size b.\nEnsure: Model Parameter θ\n1: Initialize parameter with Normal distribution θ ∼N(0, σ2)\n2: Initialize Matrix G = 0\n3: Initialize convergence tag = False\n4: while tag == False do\n5:\nShuﬄe the training set T\n6:\nfor each mini-batch B ⊂T do\n7:\nCompute gradient vector g = ∇θL(θ; B) on the mini-batch B\n8:\nUpdate matrix G = G + gg⊤\n9:\nUpdate variable θ = θ −\nη\n√\ndiag(G)+ϵ·Ig\n10:\nend for\n11:\nif convergence condition holds then\n12:\ntag = True\n13:\nend if\n14: end while\n15: Return model variable θ\nSuch a learning mechanism in Adagrad allows both adaptive learning rate in the learning\nprocess without the need of manual tuning, as well as diﬀerent learning rate for diﬀerent\nvariables. Furthermore, as the iteration continues, the values on the diagonal of matrix\nG will be no decreasing, i.e., the learning rates of the variables will keep decreasing in\nthe learning process. It will also create problems for the learning in later iterations, since\nthe variables can no longer be eﬀectively updated with information from the training data.\nIn addition, Adagrad still needs an initial learning rate parameter η to start the learning\nprocess, which can also be treated as one of the shortcomings of Adagrad.\n4.2 RMSprop\nTo resolve the problem with monotonically decreasing learning rate in Adagrad (i.e., entries\nin matrix\nη\n√\ndiag(G(τ))+ϵ·I), in this part, we will introduce another learning algorithm, namely\nRMSprop [9]. RMSprop properly decays the weight of the historical accumulated gradient\nin the matrix G deﬁned in Adagrad, and also allows the adjustment of learning rate in the\nupdating process. Formally, the variable updating equations in RMSprop can be represented\nwith the following equations:\nθ(τ) = θ(τ−1) −\nη\np\ndiag(G(τ)) + ϵ · I\ng(τ−1),\n(23)\nwhere\n(\ng(τ−1)\n= ∇θL(θ(τ−1)),\nG(τ)\n= ρ · G(τ−1) + (1 −ρ) · g(τ−1)(g(τ−1))⊤.\n(24)\n14\nIFM LAB TUTORIAL SERIES # 1, COPYRIGHT c⃝IFM LAB\nIn the above equation, the denominator term is also usually denoted as RMS(g(τ−1)) =\np\ndiag(G(τ)) + ϵ · I (RMS denotes root mean square metric on vector g(τ−1)). Therefore,\nthe updating equation is also usually written as follows:\nθ(τ) = θ(τ−1) −\nη\nRMS(g(τ−1))g(τ−1).\n(25)\nIn the representation of matrix G, parameter ρ denotes the weight of the historically accu-\nmulated gradients, and ρ is usually set as 0.9 according to GeoﬀHinton in his Lecture at\nCoursera. Based on the representation of G, for the gradient computed in t iterations ahead\nof the current iteration, they will be assigned with a very small weight, i.e., ρ(t) · (1 −ρ),\nwhich decays exponentially with t. The pseudo-code of RMSprop is provided in Algorithm 7,\nwhere the learning rate η is still needed and provided as a parameter in the algorithm.\nAlgorithm 7 RMSprop based Mini-batch Gradient Descent\nRequire: Training Set T ; Learning Rate η; Normal Distribution Std σ; Mini-batch Size b; Param-\neter ρ.\nEnsure: Model Parameter θ\n1: Initialize parameter with Normal distribution θ ∼N(0, σ2)\n2: Initialize Matrix G = 0\n3: Initialize convergence tag = False\n4: while tag == False do\n5:\nShuﬄe the training set T\n6:\nfor each mini-batch B ⊂T do\n7:\nCompute gradient vector g = ∇θL(θ; B) on the mini-batch B\n8:\nUpdate matrix G = ρ · G + (1 −ρ) · gg⊤\n9:\nUpdate variable θ = θ −\nη\n√\ndiag(G)+ϵ·Ig\n10:\nend for\n11:\nif convergence condition holds then\n12:\ntag = True\n13:\nend if\n14: end while\n15: Return model variable θ\n4.3 Adadelta\nAdadelta [10] and RMSprop are proposed separately by diﬀerent people almost at the same\ntime, which address the monotonically decreasing learning rate in Adagrad with identical\nmethods. Meanwhile, compared with RMSprop further improves Adagrad by introducing\na mechanism to eliminate the learning rate from the updating equations of the variables.\nBased on the following updating equation we introduce before in RMSprop:\nθ(τ) = θ(τ−1) −\nη\nRMS(g(τ−1))g(τ−1).\n(26)\nHowever, as introduced in [10], the learning algorithms introduced so far fail to consider\nthe units of the learning variables in the updating process.\nHere, the units eﬀectively\nindicate the physical meanings of the variables, which can be “km”, “s” or “kg”. If the\nparameter θ has hypothetical units, then the updates in parameter, i.e., ∆θ = ∇θL(θ),\n15\nJIAWEI ZHANG, IFM LAB DIRECTOR\nshould have the same units as well. However, for the learning algorithms we have introduced\nbefore, e.g., SGD, Momentum, NAD, and Adagrad, such an assumption cannot hold. For\ninstance, for the case of SGD, the units of update term is actually proportional to the\ninverse of the units of θ:\nunits of ∆θ ∝units of g ∝units of ∂L(·)\n∂θ\n∝\n1\nunits of θ.\n(27)\nTo handle such a problem, Adadelta proposes to look at the second-order methods,\ne.g., Newton’s method that uses Hessian approximation. In Newton’s method, the units of\nvariables updated can be denoted as\nunits of ∆θ ∝units of H−1g ∝units of\n∂L(·)\n∂θ\n∂2L(·)\n∂θ2\n∝units of θ,\n(28)\nwhere H = ∂2L(θ)\n∂θ2\ndenotes the Hessian matrix computed with the derivatives of the loss\nfunction regarding the variables.\nTo match the units, Adadelta rearranges the Newton’s method updating term as\n∆θ = −\n∂L(·)\n∂θ\n∂2L(·)\n∂θ2\n,\n(29)\nfrom which we can derive\nH−1 = −\n1\n∂2L(θ)\n∂θ2\n= −∆θ\n∂L(θ)\n∂θ\n.\n(30)\nTherefore, we can rewrite the updating equation for the variables based on Newton’s\nmethod as follows:\nθ(τ) = θ(τ−1) −(H(τ))−1g(τ−1)\n= θ(τ−1) −∆θ(τ)\n∂L(θ(τ))\n∂θ\ng(τ−1).\n(31)\nSimilar to RMSprop, Adadelta approximates the denominator in the above equation\nwith the RMS of the previous gradients. Meanwhile, the ∆θ(τ) term in the current iteration\nis unknown yet. Adadelta proposes to approximate the numerator in the above equation,\nand adopts a similar way to use RMS(∆θ) to replace ∆θ instead. Formally, the variable\nupdating equation in Adadelta can be formally written as follows:\nθ(τ) = θ(τ−1) −RMS(∆θ(τ−1))\nRMS(g(τ−1)) g(τ−1),\n(32)\nwhere RMS(∆θ(τ−1)) keep records of the ∆θ in the prior iterations until the previous\niteration τ −1. The pseudo-code of the Adadelta algorithm is provided in Algorithm 8. Ac-\ncording to the algorithm description, Adadelta doesn’t use any learning rate in the updating\nequation, which eﬀectively resolves the two weakness of Adagrad introduced before.\n16\nIFM LAB TUTORIAL SERIES # 1, COPYRIGHT c⃝IFM LAB\nAlgorithm 8 Adadelta based Mini-batch Gradient Descent\nRequire: Training Set T ; Learning Rate η; Normal Distribution Std σ; Mini-batch Size b; Param-\neter ρ.\nEnsure: Model Parameter θ\n1: Initialize parameter with Normal distribution θ ∼N(0, σ2)\n2: Initialize Matrix G = 0\n3: Initialize Matrix Θ = 0\n4: Initialize convergence tag = False\n5: while tag == False do\n6:\nShuﬄe the training set T\n7:\nfor each mini-batch B ⊂T do\n8:\nCompute gradient vector g = ∇θL(θ; B) on the mini-batch B\n9:\nUpdate matrix G = ρ · G + (1 −ρ) · gg⊤\n10:\nComputer updating vector ∆θ = −\n√\ndiag(Θ)+ϵ·I\n√\ndiag(G)+ϵ·Ig\n11:\nUpdate matrix Θ = ρ · Θ + (1 −ρ) · ∆θ(∆θ)⊤\n12:\nUpdate variable θ = θ + ∆θ\n13:\nend for\n14:\nif convergence condition holds then\n15:\ntag = True\n16:\nend if\n17: end while\n18: Return model variable θ\n5. Momentum & Adaptive Gradient based Learning Algorithms\nIn this section, we will introduce the learning algorithms which combine the advantages of\nboth the Momentum algorithm and the algorithm with adaptive learning rate, including\nAdam and Nadam respectively.\n5.1 Adam\nIn recent years, a new learning algorithm, namely Adaptive Moment Estimation (Adam)\n[4], has been introduced, which only computes the ﬁrst-order gradients with little memory\nrequirement. Similar to RMSprop and Adadelta, Adam keeps records of the past squared\nﬁrst-order gradients; while Adam also keeps records of the past ﬁrst-order gradients as\nwell, both of which will decay exponentially in the learning process. Formally, we can use\nvectors m(τ) and v(τ) to denote the terms storing the ﬁrst-order gradients and the squared\nﬁrst-order gradients respectively, whose concrete representations are provided as follows:\nm(τ) = β1 · m(τ−1) + (1 −β1) · g(τ−1),\nv(τ) = β2 · v(τ−1) + (1 −β2) · g(τ−1) ⊙g(τ−1),\n(33)\nwhere vector g(τ−1) = ∇θL(θ(τ−1)) and g(τ−1) ⊙g(τ−1) denote the element-wise product\nof the vectors. In the above equation, vector v(τ) actually stores the identical information\nas diag(G(τ)) used in Equation (24). However, instead of storing the whole matrix, Adam\ntends to use less space by keeping such a vector record.\nAs introduced in [4], vectors m(τ) and v(τ) are biased toward zero, especially when β1\nand β2 are close to 1, since m(0) and v(0) are initialized to be the zero vectors respectively.\n17\nJIAWEI ZHANG, IFM LAB DIRECTOR\nTo resolve such a problem, Adam introduces to rescale the terms as follows:\nˆm(τ) = m(τ)\n1 −βτ\n1\n,\nˆv(τ) =\nv(τ)\n1 −βτ\n2\n,\n(34)\nwhere the superscripts τ of terms βτ\n1 and βτ\n2 denotes the power instead of the iteration\ncount index (τ) used before.\nBased on the rescaled vector ˆm(τ) and matrix ˆv(τ), Adam will update the model variable\nwith the following equation:\nθ(τ) = θ(τ−1) −\nη\n√\nˆv(τ) + ϵ\n⊙ˆm(τ)\n(35)\nAccording to the above descriptions, Adam can be viewed as an integration of the\nRMSprop algorithm with the Momentum algorithm, which allows both faster convergence\nand adaptive learning rate simultaneously. Formally, the pseudo-code of the Adam learning\nalgorithm is provided in Algorithm 9, where β1 and β2 are inputted as the parameters\nfor the algorithm. According to [4], the parameters in Adam can be initialized as follows:\nϵ = 10−8, β1 = 0.9 and β2 = 0.999.\nAlgorithm 9 Adam\nRequire: Training Set T ; Learning Rate η; Normal Distribution Std σ; Mini-batch Size b; Decay\nParameters β1, β2.\nEnsure: Model Parameter θ\n1: Initialize parameter with Normal distribution θ ∼N(0, σ2)\n2: Initialize vector m = 0\n3: Initialize vector v = 0\n4: Initialize step τ = 0\n5: Initialize convergence tag = False\n6: while tag == False do\n7:\nShuﬄe the training set T\n8:\nfor each mini-batch B ⊂T do\n9:\nUpdate step τ = τ + 1\n10:\nCompute gradient vector g = ∇θL(θ; B) on the mini-batch B\n11:\nUpdate vector m = β1 · m + (1 −β1) · g\n12:\nUpdate vector v = β2 · v + (1 −β2) · g ⊙g\n13:\nRescale vector ˆm = m/(1 −βτ\n1 )\n14:\nRescale vector ˆv = v/(1 −βτ\n2 )\n15:\nUpdate variable θ = θ −\nη\n√\nˆv+ϵ ⊙ˆm\n16:\nend for\n17:\nif convergence condition holds then\n18:\ntag = True\n19:\nend if\n20: end while\n21: Return model variable θ\n18\nIFM LAB TUTORIAL SERIES # 1, COPYRIGHT c⃝IFM LAB\n5.2 Nadam\nAdam introduced in the previous section can be viewed as an integration of Momentum\nbased learning algorithm with the adaptive gradient based learning algorithm, where the\nvanilla momentum is adopted. In [2], a learning algorithm is introduced to replace the\nvanilla momentum with the Nesterov’s accelerated gradient (NAG) instead, and the new\nlearning algorithm is called the Nesterov-accelerated Adam (Nadam). Before providing the\nupdating equation of Nadam, we would like to illustrate the updating equation of NAG as\nfollows again (Equation (21)):\nθ(τ) = θ(τ−1) −η · ∆v(τ), where\n(\n∆v(τ)\n= ρ · ∆v(τ−1) + (1 −ρ) · ∇θL(ˆθ\n(τ)),\nˆθ\n(τ)\n= θ(τ−1) −η · ρ · ∆v(τ−1).\n(36)\nAccording to the above updating equation, the momentum term ∆v(τ−1) is used twice\nin the process: (1) ∆v(τ−1) is used to compute ˆθ\n(τ); and (2) ∆v(τ−1) is used to compute\n∆v(τ). Nadam proposes to change the above method with the following updating equations\ninstead:\nθ(τ) = θ(τ−1) −η ·\n\u0010\nρ · ∆v(τ) + (1 −ρ) · ∇θL(θ(τ−1))\n\u0011\n,\n(37)\nwhere\n∆v(τ) = ρ · ∆v(τ−1) + (1 −ρ) · ∇θL(θ(τ−1)).\n(38)\nCompared with NAG, the above equation doesn’t look-ahead in computing the gradient\nterm; while compared with vanilla momentum, the above equation uses both the momentum\nterm and the gradient term in the current iteration. Term ρ · ∆v(τ) + (1 −ρ) · ∇θL(θ(τ−1))\nused in the updating equation actually is an approximation to ∆v(τ+1) in the next iteration,\nwhich achieve the objective of looking ahead in updating the variables.\nMeanwhile, according to Equation (35), we can rewrite the updating equation of Adam\nas follows:\nθ(τ) = θ(τ−1) −\nη\n√\nˆv(τ) + ϵ\n⊙ˆm(τ),\n(39)\nwhere\nˆm(τ) = m(τ)\n1 −βτ\n1\n, and m(τ) = β1 · m(τ−1) + (1 −β1) · g(τ).\n(40)\nBy replacing the ˆm(τ) term into Equation (39), we can rewrite it as follows:\nθ(τ) = θ(τ−1) −\nη\n√\nˆv(τ) + ϵ\n⊙\n \nβ1 · m(τ−1)\n1 −βτ\n1\n+ (1 −β1) · g(τ)\n1 −βτ\n1\n!\n,\n= θ(τ−1) −\nη\n√\nˆv(τ) + ϵ\n⊙\n \nβ1 · ˆm(τ−1) + (1 −β1) · g(τ)\n1 −βτ\n1\n!\n.\n(41)\nSimilar to the analysis provided in Equation (37), Nadam proposes to look-ahead by re-\nplacing the ˆm(τ−1) term used in the parentheses with ˆm(τ) instead, which can bring about\nthe following updating equation:\nθ(τ) = θ(τ−1) −\nη\n√\nˆv(τ) + ϵ\n⊙\n \nβ1 · ˆm(τ) + (1 −β1) · g(τ)\n1 −βτ\n1\n!\n.\n(42)\n19\nJIAWEI ZHANG, IFM LAB DIRECTOR\nThe pseudo-code of the Nadam algorithm is provided in Algorithm 10, where most of\nthe code are identical to those in Algorithm 9, except the last line in updating the variable\nθ.\nAlgorithm 10 Nadam\nRequire: Training Set T ; Learning Rate η; Normal Distribution Std σ; Mini-batch Size b; Decay\nParameters β1, β2.\nEnsure: Model Parameter θ\n1: Initialize parameter with Normal distribution θ ∼N(0, σ2)\n2: Initialize vector m = 0\n3: Initialize vector v = 0\n4: Initialize step τ = 0\n5: Initialize convergence tag = False\n6: while tag == False do\n7:\nShuﬄe the training set T\n8:\nfor each mini-batch B ⊂T do\n9:\nUpdate step τ = τ + 1\n10:\nCompute gradient vector g = ∇θL(θ; B) on the mini-batch B\n11:\nUpdate vector m = β1 · m + (1 −β1) · g\n12:\nUpdate vector v = β2 · v + (1 −β2) · g ⊙g\n13:\nRescale vector ˆm = m/(1 −βτ\n1 )\n14:\nRescale vector ˆv = v/(1 −βτ\n2 )\n15:\nUpdate variable θ = θ −\nη\n√\nˆv+ϵ ⊙\n\u0010\nβ1 · ˆm + (1−β1)·g\n1−βτ\n1\n\u0011\n16:\nend for\n17:\nif convergence condition holds then\n18:\ntag = True\n19:\nend if\n20: end while\n21: Return model variable θ\n6. Hybrid Evolutionary Gradient Descent Learning Algorithms\nAdam together with its many variants have been shown to be eﬀective for optimizing a large\ngroup of problems. However, for the non-convex objective functions of deep learning models,\nAdam cannot guarantee to identify the globally optimal solutions, whose iterative updating\nprocess may inevitably get stuck in local optima. The performance of Adam is not very\nrobust, which will be greatly degraded for the objective function with non-smooth shape\nor learning scenarios polluted by noisy data. Furthermore, the distributed computation\nprocess of Adam requires heavy synchronization, which may hinder its adoption in large-\ncluster based distributed computational platforms.\nOn the other hand, genetic algorithm (GA), a metaheuristic algorithm inspired by the\nprocess of natural selection in evolutionary algorithms, has also been widely used for learning\nthe solutions of many optimization problems. In GA, a population of candidate solutions will\nbe initialized and evolved towards better ones. Several attempts have also been made to use\nGA for training deep neural network models [12, 7, 1] instead of the gradient descent based\nmethods. GA has demonstrated its outstanding performance in many learning scenarios,\nlike non-convex objective function containing multiple local optima, objective function with\n20\nIFM LAB TUTORIAL SERIES # 1, COPYRIGHT c⃝IFM LAB\nM1\nM2\nM3\n…\nMg\nADAM\nT\nV\nValidation & \nPair Selection\n( M1\nM2 )\nM3 )\nM4 )\n…\nMg )\n( M5\n( M4\n( M2\nCrossover\nMutation\nADAM\nT\nM1\nM2\nM3\n…\nMg\nM1\nM2\nM3\n…\nMg\nM1\nM2\nM3\n…\nMg\nM1\nM2\nM3\n…\nMg\nGeneration 1\nGeneration 2\n… …\nGeneration \nSelection\nGeneration \nSelection\nParent Model Pairs\nChild Models\nV\nV\nV\nM1\nM2\nM3\n…\nMg\nGeneration K\nUnit Model \nSelection\nADAM\nADAM\nGenetic \nAlgorithm\nGenetic \nAlgorithm\nGenetic \nAlgorithm\nFigure 2: Overall Architecture of Gadam Model.\nnon-smooth shape, as well as a large number of parameters and noisy environments. GA also\nﬁts the parallel/distributed computing setting very well, whose learning process can be easily\ndeployed on parallel/distributed computing platforms. Meanwhile, compared with Adam,\nGA may take more rounds to converge in addressing optimization objective functions.\nIn this section, we will introduce a new optimization algorithm, namely Gadam (Genetic\nAdaptive Momentum Estimation) [11], which incorporates Adam and GA into a uniﬁed\nlearning scheme. Instead of learning one single model solution, Gadam works with a group\nof potential unit model solutions. In the learning process, Gadam learns the unit models\nwith Adam and evolves them to the new generations with genetic algorithm. In addition,\nAlgorithm Gadam can work in both standalone and parallel/distributed modes, which will\nalso be studied in this section.\n6.1 Gadam\nThe overall framework of the model learning process in Gadam is illustrated in Figure 2,\nwhich involves multiple learning generations. In each generation, a group of unit model\nvariable will be learned with Adam from the data, which will also get evolved eﬀectively\nvia the genetic algorithm.\nGood candidate variables will be selected to form the next\ngeneration. Such an iterative model learning process continues until convergence, and the\noptimal unit model variable at the ﬁnal generation will be selected as the output model\nsolution. For simplicity, we will refer to unit models and their variables interchangeably\nwithout distinguishing their diﬀerences.\n6.1.1 Model Population Initialization\nGadam learns the optimal model variables based on a set of unit models (i.e., variables of\nthese unit models by default), namely the unit model population, where the initial unit\nmodel generation can be denoted as set G(0) = {M(0)\n1 , M(0)\n2 , · · · , M(0)\ng } (g is the population\nsize and the superscript represents the generation index).\nBased on the initial genera-\ntion, Gadam will evolve the unit models to new generations, which can be represented as\nG(1), G(2), · · · , G(K) respectively. Here, parameter K denotes the total generation number.\n21\nJIAWEI ZHANG, IFM LAB DIRECTOR\nFor each unit model in the initial generation G(0), e.g., M(0)\ni\n, its variables θ(0)\ni\nis initial-\nized in Gadam with random values sampled from certain distributions (e.g., the standard\nnormal distribution).\nThese initial generation serve as the starting search points, from\nwhich Gadam will expand to other regions to identify the optimal solutions. Meanwhile,\nfor the unit models in the following generations, their variable values will be generated via\nGA from their parent models respectively.\n6.1.2 Model Learning with Adam\nIn the learning process, given any model generation G(k) (k ∈{1, 2, · · · , K}), Gadam will\nlearn the (locally) optimal variables for each unit model with Adam. Formally, Gadam\ntrains the unit models with several epochs. In each epoch, for each unit model M(k)\ni\n∈G(k),\na separated training batch will be randomly sampled from the training dataset, which can\nbe denoted as B = {(x1, y1), (x2, y2), · · · , (xb, yb)} ⊂T (here, b denotes the batch size and\nT represents the complete training set). Let the loss function introduced by unit model\nM(k)\ni\nfor training mini-batch B be ℓ(θ(k)\ni\n), and the learned model variable vector by the\ntraining instance can be represented as\n¯θ(k)\ni\n= Adam\n\u0010\nℓ(θ(k)\ni\n)\n\u0011\n.\n(43)\nDepending on the speciﬁc unit models and application settings, the loss function will have\ndiﬀerent representations, e.g., mean square loss, hinge loss or cross entropy loss. Such a\nlearning process continues until convergence, and the updated model generation G(k) with\n(locally) optimal variables can be represented as ¯G(k) = { ¯\nM(k)\n1 , ¯\nM(k)\n2 , · · · , ¯\nM(k)\ng\n}, whose\ncorresponding variable vectors can be denoted as ¯θ(k)\n1 , ¯θ(k)\n2 , · · · , ¯θ(k)\ng } respectively.\n6.1.3 Model Evolution with Genetic Algorithm\nIn this part, based on the learned unit models, i.e., ¯G(k), Gadam will further search for\nbetter solutions for the unit models eﬀectively via the genetic algorithm.\nModel Fitness Evaluation\nUnit models with good performance may ﬁt the learning task better. Instead of evolving\nmodels randomly, Gadam proposes to pick good unit models from the current generation\nto evolve. Based on a sampled validation batch V ⊂T , the ﬁtness score of unit models,\ne.g., ¯\nM(k)\ni\n, can be eﬀectively computed based on its loss terms introduced on V as follows\nL(k)\ni\n= L( ¯\nM(k)\ni\n; V) =\nX\n(xj,yj)∈V\nℓ(xj, yj; ¯θ(k)\ni\n).\n(44)\nBased on the computed loss values, the selection probability of unit model ¯\nM(k)\ni\ncan be\ndeﬁned with the following softmax equation\nP( ¯\nM(k)\ni\n) =\nexp(−ˆL(k)\ni\n)\nPg\nj=1 exp(−ˆL(k)\nj\n) .\n(45)\nNecessary normalization of the loss values is usually required in real-world applications, as\nexp(−L(k)\ni\n) may approach 0 or ∞for extremely large positive or small negative loss values\n22\nIFM LAB TUTORIAL SERIES # 1, COPYRIGHT c⃝IFM LAB\nL(k)\ni\n. As indicated in the probability equation, the normalized loss terms of all unit models\ncan be formally represented as [ ˆL(k)\n1 , ˆL(k)\n2 , · · · , ˆL(k)\ng ]⊤, where ˆL(k)\ni\n∈[0, 1], ∀i ∈{1, 2, · · · , g}.\nAccording to the computed probabilities, from the unit model set ¯G(k), g pairs of unit\nmodels will be selected with replacement as the parent models for evolution, which can be\ndenoted as P = {( ¯\nM(k)\ni1 , ¯\nM(k)\nj1 ), ( ¯\nM(k)\ni2 , ¯\nM(k)\nj2 ), · · · , ( ¯\nM(k)\nig , ¯\nM(k)\njg )}.\nUnit Model Crossover\nGiven a unit model pair, e.g., ( ¯\nM(k)\nip , ¯\nM(k)\njp ) ∈P, Gadam inherits their variables to\nthe child model via the crossover operation. In crossover, the parent models, i.e.,\n¯\nM(k)\nip\nand ¯\nM(k)\njp , will also compete with each other, where the parent model with better perfor-\nmance tend to have more advantages. We can represent the child model generated from\n( ¯\nM(k)\nip , ¯\nM(k)\njp ) as ˜\nM(k)\np . For each entry in the weight variable ˜θ\n(k)\np\nof the child model ˜\nM(k)\np ,\nGadam initializes its values as follows:\n˜θ\n(k)\np (m) = 1(rand ≤p(k)\nip,jp) · ¯θ(k)\nip (m) + 1(rand > p(k)\nip,jp) · ¯θ(k)\njp (m).\n(46)\nIn the equation, binary function 1(·) returns 1 iﬀthe condition holds. Term “rand” denotes\na random number in [0, 1]. The probability threshold p(k)\nip,jp is deﬁned based on the parent\nmodels’ performance:\np(k)\nip,jp =\nexp(−ˆL(k)\nip )\nexp(−ˆL(k)\nip ) + exp(−ˆL(k)\njp ) .\n(47)\nIf ˆL(k)\nip > ˆL(k)\njp , i.e., model ¯\nM(k)\nip\nintroduces a larger loss than ¯\nM(k)\njp , we will have 0 < p(k)\nip,jp <\n1\n2.\nWith such a process, based on the whole parent model pairs in set P, Gadam will be\nable to generate the children model set as set ˜G(k) = { ˜\nM(k)\n1 , ˜\nM(k)\n2 , · · · , ˜\nM(k)\ng\n}.\nUnit Model Mutation\nTo avoid the unit models getting stuck in local optimal points, Gadam adopts an op-\neration called mutation to adjust variable values of the generated children models in set\n{ ˜\nM(k)\n1 , ˜\nM(k)\n2 , · · · , ˜\nM(k)\ng\n}. Formally, for each child model ˜\nM(k)\nq\nparameterized with vector\n˜θ\n(k)\nq , Gadam will mutate the variable vector according to the following equation, where its\nmth entry can be updated as\n˜θ\n(k)\nq (m) = 1(rand ≤pq) · rand(0, 1) + 1(rand > p) · ˜θ\n(k)\nq (m),\n(48)\nIn the equation, term pq denotes the mutation rate, which is strongly correlated with the\nparent models’ performance:\npq = p ·\n\u0010\n1 −P( ¯\nM(k)\niq ) −P( ¯\nM(k)\njq )\n\u0011\n.\n(49)\nFor the child models with good parent models, they will have lower mutation rates. Term p\ndenotes the base mutation rate which is usually a small value, e.g., 0.01, and probabilities\nP( ¯\nM(k)\niq ) and P( ¯\nM(k)\njq ) are deﬁned in Equation (45).\nThese unit models will be further\ntrained with Adam until convergence, which will lead to the kth children model generation\n˜G(k) = { ˜\nM(k)\n1 , ˜\nM(k)\n2 , · · · , ˜\nM(k)\ng\n}.\n23\nJIAWEI ZHANG, IFM LAB DIRECTOR\n6.1.4 New Generation Selection and Evolution Stop Criteria\nAmong these learned unit models in the learned parent model set ¯G(k) = { ¯\nM(k)\n1 , ¯\nM(k)\n2 , · · · , ¯\nM(k)\ng\n}\nand children model set ˜G(k) = { ˜\nM(k)\n1 , ˜\nM(k)\n2 , · · · , ˜\nM(k)\ng\n}, Gadam will re-evaluate their ﬁtness\nscores based on a shared new validation batch. Among all the unit models in ¯G(k)∪˜G(k), the\ntop g unit models will be selected to form the (k + 1)th generation, which can be formally\nrepresented as set G(k+1) = {M(k+1)\n1\n, M(k+1)\n2\n, · · · , M(k+1)\ng\n}. Such an evolutionary learning\nprocess will stop if the maximum generation number has reached or there is no signiﬁcant\nimprovement between consequential generations, e.g., G(k) and G(k+1):\n\f\f\f\f\f\nX\nM(k)\ni\n∈G(k)\nL(k)\ni\n−\nX\nM(k+1)\ni\n∈G(k+1)\nL(k+1)\ni\n\f\f\f\f\f ≤λ,\n(50)\nThe above equation deﬁnes the stop criterion of Gadam, where λ is the evolution stop\nthreshold.\nThe optimization algorithm Gadam actually incorporate the advantages of both Adam\nand genetic algorithm. With Adam, the unit models can eﬀectively achieve the (locally/globally)\noptimal solutions very fast with a few training epochs. Meanwhile, via genetic algorithm\nbased on a number of unit models, it will also provide the opportunity to search for so-\nlutions from multiple starting points and jump out from the local optima. According to\n[4], for a smooth function, Adam will converge as the function gradient vanishes. On the\nother hand, the genetic algorithm can also converge according to [8]. Based on these prior\nknowledge, the convergence of Gadam can be proved as introduced in [11]. The training\nof unit models with Gadam can be eﬀectively deployed on parallel/distributed computing\nplatforms, where each unit model involved in Gadam can be learned with a separate pro-\ncess/server. Among the processes/servers, the communication costs are minor, which exist\nmerely in the crossover step. Literally, among all the g unit models in each generation, the\ncommunication costs among them is O(k · g · dθ), where dθ denotes the dimension of vector\nθ and k denotes the required training epochs to achieve convergence by Adam.\n7. A Summary\nIn this paper, we have introduced the gradient descent algorithm together with its vari-\nous recent variant algorithms for training the deep learning models. Based on the recent\ndevelopments in this direction, this paper will be updated accordingly later in the near\nfuture.\n24\nIFM LAB TUTORIAL SERIES # 1, COPYRIGHT c⃝IFM LAB\nReferences\n[1] Eli David and Iddo Greental. Genetic algorithms for evolving deep neural networks.\nCoRR, abs/1711.07655, 2017.\n[2] Timothy Dozat. Incorporating Nesterov Momentum into Adam.\n[3] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online\nlearning and stochastic optimization. J. Mach. Learn. Res., 12:2121–2159, July 2011.\n[4] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.\nCoRR, abs/1412.6980, 2014.\n[5] Yurii Nesterov. A method of solving a convex programming problem with convergence\nrate O(1/sqr(k)). Soviet Mathematics Doklady, 27:372–376, 1983.\n[6] Ning Qian. On the momentum term in gradient descent learning algorithms. Neural\nNetw., 12(1):145–151, January 1999.\n[7] Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O.\nStanley, and JeﬀClune.\nDeep neuroevolution: Genetic algorithms are a competi-\ntive alternative for training deep neural networks for reinforcement learning. CoRR,\nabs/1712.06567, 2017.\n[8] Dirk Thierens and David E. Goldberg.\nConvergence models of genetic algorithm\nselection schemes.\nIn Proceedings of the International Conference on Evolutionary\nComputation. The Third Conference on Parallel Problem Solving from Nature: Par-\nallel Problem Solving from Nature, PPSN III, pages 119–129, London, UK, UK, 1994.\nSpringer-Verlag.\n[9] T. Tieleman and G. Hinton. Lecture 6.5—RmsProp: Divide the gradient by a running\naverage of its recent magnitude. COURSERA: Neural Networks for Machine Learning,\n2012.\n[10] Matthew D. Zeiler.\nADADELTA: an adaptive learning rate method.\nCoRR,\nabs/1212.5701, 2012.\n[11] Jiawei Zhang and Fisher B. Gouza. GADAM: genetic-evolutionary ADAM for deep\nneural network optimization. CoRR, abs/1805.07500, 2018.\n[12] Jiawei Zhang and Fisher B. Gouza.\nSEGEN: sample-ensemble genetic evolutional\nnetwork model. CoRR, abs/1803.08631, 2018.\n25\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2019-03-11",
  "updated": "2019-03-11"
}