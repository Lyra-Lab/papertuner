{
  "id": "http://arxiv.org/abs/1903.04739v1",
  "title": "Syllable-based Neural Named Entity Recognition for Myanmar Language",
  "authors": [
    "Hsu Myat Mo",
    "Khin Mar Soe"
  ],
  "abstract": "Named Entity Recognition (NER) for Myanmar Language is essential to Myanmar\nnatural language processing research work. In this work, NER for Myanmar\nlanguage is treated as a sequence tagging problem and the effectiveness of deep\nneural networks on NER for Myanmar language has been investigated. Experiments\nare performed by applying deep neural network architectures on syllable level\nMyanmar contexts. Very first manually annotated NER corpus for Myanmar language\nis also constructed and proposed. In developing our in-house NER corpus,\nsentences from online news website and also sentences supported from\nALT-Parallel-Corpus are also used. This ALT corpus is one part of the Asian\nLanguage Treebank (ALT) project under ASEAN IVO. This paper contributes the\nfirst evaluation of neural network models on NER task for Myanmar language. The\nexperimental results show that those neural sequence models can produce\npromising results compared to the baseline CRF model. Among those neural\narchitectures, bidirectional LSTM network added CRF layer above gives the\nhighest F-score value. This work also aims to discover the effectiveness of\nneural network approaches to Myanmar textual processing as well as to promote\nfurther researches on this understudied language.",
  "text": "International Journal on Natural Language Computing (IJNLC) Vol.8, No.1, February 2019 \nDOI: 10.5121/ijnlc.2019.8101                                                                                                                        1 \n \nSYLLABLE-BASED NEURAL NAMED ENTITY \nRECOGNITION FOR MYANMAR LANGUAGE \n \nHsu Myat Mo and Khin Mar Soe \n \nNatural Language Processing Lab., University of Computer Studies,  \nYangon, Myanmar \n \nABSTRACT \n \nNamed Entity Recognition (NER) for Myanmar Language is essential to Myanmar natural language \nprocessing research work. In this work, NER for Myanmar language is treated as a sequence tagging \nproblem and the effectiveness of deep neural networks on NER for Myanmar language has been \ninvestigated. Experiments are performed by applying deep neural network architectures on syllable level \nMyanmar contexts. Very first manually annotated NER corpus for Myanmar language is also constructed \nand proposed. In developing our in-house NER corpus, sentences from online news website and also \nsentences supported from ALT-Parallel-Corpus are also used. This ALT corpus is one part of the Asian \nLanguage Treebank (ALT) project under ASEAN IVO. This paper contributes the first evaluation of neural \nnetwork models on NER task for Myanmar language. The experimental results show that those neural \nsequence models can produce promising results compared to the baseline CRF model. Among those neural \narchitectures, bidirectional LSTM network added CRF layer above gives the highest F-score value.  This \nwork also aims to discover the effectiveness of neural network approaches to Myanmar textual processing \nas well as to promote further researches on this understudied language. \n \nKEYWORDS \n \nBidirectional LSTM_CRF, Myanmar Language, Named Entity Recognition, Neural architectures, Syllable-\nbased  \n \n1. INTRODUCTION \n \nNamed Entity Recognition is the process of automatically tagging, identifying of labeling \ndifferent named entities (NE) in text in accordance with the predefined sets of NE categories \n(e.g., person, organization, location and time and so on). NER has been a challenging problem in \nMyanmar language processing. Currently, Myanmar NLP is at an initial stage and lexical \nresources available are very limited. On the other hand, it has both complex and rich morphology \nas well as ambiguity. These facts can lead to wrong word segmentation. The current Myanmar-\nEnglish Machine Translation system couldn’t recognize names written in Myanmar language \nproperly. Moreover, the most important motivation behind this work is that there is no currently \navailable NER tool that can extract named entities in Myanmar scripts. As far as it is concerned, \nthe previous proposed methods for Myanmar NER adopted dictionary look-up approach, rule-\nbased techniques and a combination of rule-based and statistical N-grams based method together \nwith name database.  \n \nThe previous proposed methods for NER can be roughly classified into dictionary-based or rule-\nbased and traditional statistical sequence labeling approaches and hybrid [1]. Those approaches \nrequire linguistic knowledge and feature engineering. \n \nInternational Journal on Natural Language Computing (IJNLC) Vol.8, No.1, February 2019 \n2 \n \nNeural network models have the advantage of minimizing the effort in feature engineering, since \ndeep layers of neural networks can discover relevant features to tasks. General neural network \narchitecture for sequence labelling tasks was developed and proposed in [2]. Following this work, \nRecurrent Neural Networks (RNN-based networks) have been designed for modeling sequential \ndata and have been proved to be quite efficient in NER as a sequential tagging task. As a special \nkind of RNN, Long Short-Term Memory (LSTM) neural network has been verified to be \npowerful in modeling sequential data. Moreover, bidirectional LSTM neural network which is \nderived from LSTM network, introduces two independent layers to accumulate contextual \ninformation from the past and future histories. Bidirectional LSTM network has advantages in \nmemorizing information for long periods in both directions so that it makes great improvement in \nlinguistic computation. In [3], the authors combined a bidirectional LSTM network and a \nConditional Random Fields (CRF) to form a BiLSTM_CRF network. This kind of model can use \nthe past input features and sentence level tag information and the future input features. With the \nrapid development of deep learning, recent research revealed that those kinds of networks \nsignificantly outperform statistical algorithms. \n \nIn this work, NER is considered as a sequence labeling task and experiments are carried out on \nsyllable level data. In order to explore the effectiveness of deep learning on Myanmar Language, \nexperiments are conducted using bidirectional LSTM network by adding CRF inference layer \nabove. We will refer that model as BiLSTM_CRF in the following sections. . In our experiments, \nwe firstly encode syllable level information by using various neural networks such as \nConvolutional Neural Network (CNN) and bidirectional LSTM network on characters and then \nconcatenate character and syllable level representation. The combined representation was fed into \nbidirectional LSTM network to learn context information of each syllable. On the top of \nbidirectional LSTM, a CRF inference layer was jointly added to decode the optimal label. \nBesides, we also performed the neural experiments with softmax layer on the top. That kind of \nmodel will be referred as BiLSTM in the following sections. Experiments are carried out with \ndifferent parameters and network settings. Moreover, we tried to investigate which network \n(CNN and bidirectional LSTM) is more powerful in learning character representation. \nExperiments with these neural networks, without additional feature engineering, achieved \npromising results compared to the baseline CRF model. This study is the very first work to apply \nneural networks to Myanmar NER. \n \n2. RELATED WORKS \n \nAs far as being aware and up to our knowledge, no work has been published for using deep neural \nnetworks on NER for Myanmar language. Previous effort on Myanmar NER had been done by \nrule-based and statistical research works. A method for Myanmar Named Entity Identification \nusing a hybrid method was proposed in [4]. Their method was a combination of rule-based and \nstatistical N-grams based method and name database was used as well. In [5], the authors \nproposed a Myanmar Named Identification algorithm that defines the names by using some of the \nPOS information, NE identification rules and clues words in the left and/or the right contexts of \nNEs that carry information for NE identification. Their limitation is that input sentence must be \nwith specified POS tags. As a weakness, sematic implication of proper names is defeasible. \nMoreover, those approaches require linguistic knowledge and feature engineering. CRF-based \nNER for Myanmar language can be seen in [6]. In this paper, CRF, one of the statistical \napproaches had been proposed for Myanmar NER task. Moreover, the authors tried to explore on \nvarious combination of features for this approach. \n \nAlthough statistical approaches such as CRFs have been widely applied to NER tasks, those \napproaches heavily rely on feature engineering. The authors of [7] introduced two neural \narchitectures for NER that use no features. One was based on bidirectional LSTMs and CRF, and \nInternational Journal on Natural Language Computing (IJNLC) Vol.8, No.1, February 2019 \n3 \n \nanother one was constructed using a transition-based approach. Likewise, a neural network \narchitecture that automatically detects word- and character-level features using a hybrid \nbidirectional LSTM and CNN architecture to eliminate the need for most feature engineering was \nproposed in [8]. \n \nOver the past few years, various deep neural networks have been applied for NER on different \nlanguages, e.g., Italian neural NER [9], Mongolian neural NER [10], Japanese neural NER [11] \nand Russian NER [12] and so on. Likewise, deep neural architectures also have been applied on \nNER for different domains, e.g., Neural NER for Medical Entities in Twitter [13] and NER for \nTwitter Massages [14], Biomedical Neural NER [15] and disease NER [16]. Their works revealed \nthat neural networks have the great capability for NER tasks and significantly outperforms \nstatistical algorithms. \n \n3. MYANMAR LANGUAGE \n \nMyanmar language is the official language of the Republic of the Union of Myanmar and has \nmore than one thousand years' history. According to the documents, the Myanmar script was \ndescended from the Brahmi script of ancient South India. It belongs to the Sino-Tibetan language \nfamily. Myanmar scripts are written in sequence from left to right in which white space may \nsometimes be inserted between phrases but regular white space is not used between words or \nbetween syllables (see Figure. 1 for the example). Words in Myanmar language are composed of \nsingle or multiple syllables. Moreover, it can be said that a Myanmar syllable is a composition of \nmultiple characters. The Myanmar scripts usually have 75 characters in total and those characters \ncan be classified into 12 groups. For more details, you can check the paper [17]. \n \n \n \n \nFigure 1. Example of Myanmar writing  \n \n \nWords in Myanmar language are composition of one or more syllables and a syllable may also \ncontain one or more characters.  A word ‘န ိုင်ငံ’ is composed of two syllables ‘န ိုင်’ and ‘ငံ’. A \nsyllable in Myanmar language may be made up of one or several characters. For example, the \nsyllable ‘န ိုင်’ incudes five Unicode characters, i.e., ‘န, ိ,  , င’ and ‘်’ (see Figure 2). In this \npaper, the character refers to the Unicode character instead of non-standard font characters. \n \n \nFigure 2. An example of a Myanmar word formation \n \nFor Myanmar language, there are some complex font problems. Most Myanmar people are more \nfamiliar with fonts that are not standard Unicode font. To make Myanmar syllable structure \nrepresented in a definite way, we have Unicode level as the character level in this work. \n \nA Myanmar syllable structure formation is quite definite and simple. Although the constituents \ncan appear in different sequences, a Myanmar syllable usually consists of one initial consonant \nfollowed by zero or more medials, zero or more vowels and optional dependent various signs. \nInternational Journal on Natural Language Computing (IJNLC) Vol.8, No.1, February 2019 \n4 \n \nIndependent vowels, independent various signs and digits can act as stand-alone syllables. \nMoreover, only one consonant can even stand as a syllable. According to the Unicode standard, \nvowels are stored after the consonant. Detailed syllable segmentation structure for Myanmar \nlanguage is explained in [17] and [18].  \n \nFor example, the following Myanmar sentence, “ရန်ကိုန်တွင်မ ိုိုးမရွွာပါ။”, contains seven syllables. \nThe character “။” is sentence ending punctuation mark. White space is used between syllables \n(see Figure 3). The meaning of the sentence is “It doesn’t rain in Yangon”. “Yangon” is the city \nof the Republic of the Union of Myanmar. The proposed syllable-based sequential labeling model \nfor Myanmar NER is trained and tested on the syllable level input data. \n \n \n \nFigure 3. Syllable-based segmented sentence \n \nMyanmar language is very complex compared to English and other European languages. On the \nother hand, language resources for Myanmar NLP researches have not been well prepared until \nnow. As one of agglutinative languages, Myanmar has complex morphological structures which \ncan lead to suffer the models from data sparsity. Besides, for models in which words are treated \nas basic unit to construct distributed representation, there are problems for those rich \nmorphological words. On the other hand, it is needed to deal with out-of-vocabulary words and \nword segmentation problem is one of the problems for Myanmar language. As described above, \nwords in Myanmar language are not always separated by spaces, so word segmentation is \nnecessary and segmentation errors will affect the level of NER performance. In Myanmar \nlanguage, syllable is the smallest linguistic unit that can carry information about word. For those \nreasons, syllable is treated as the basic unit for label tagging in all our NER experiments. \n \n3.1. CHALLENGES OF NER FOR MYANMAR LANGUAGE \n \nThe task of identifying names automatically in Myanmar text is complex compared to other \nlanguages for many reasons. One of the reasons is the lack of resources such as annotated corpus, \nname lists, gazetteers or name dictionary, etc. which means that Myanmar language is resource-\nconstrained language. Besides, Myanmar language has distinct characteristics and having no \ncapitalized latter which is the main indicator of proper names for some other languages like \nEnglish. Moreover, its writing structure is of the free order, makes the NER a complex process. \nSome proper names of foreign person and location are loanwords or transliterated words so that \nthere are wide variations in some Myanmar terms. Myanmar names also take all morphological \ninflections which can lead to ambiguity. This ambiguity of NE may lead to problem in classifying \nnamed entities into predefined types. It can be said that how to perform the task of identifying \nnames in Myanmar text automatically is still challenging. \n \n4. METHODOLOGY \n \n4.1. CONVOLUTIONAL NEURAL NETWORK (CNN) \n \nConvolutional Neural Network (CNN) is popular in dealing with images and it also has excellent \nresults in computer vision. Lately, it has been applied in NLP research tasks and it can be found \nthat it outperformed traditional models such as bag of words and n-grams and so on. \n \n \nInternational Journal on Natural Language Computing (IJNLC) Vol.8, No.1, February 2019 \n5 \n \n4.2. RECURRENT NEURAL NETWORK (RNN) \n \nRecurrent Neural Network (RNN) is powerful for learning sequential information. RNN does the \nsame operation over and over again on stream of data. RNN comes with the assumption that the \noutput is being dependent on the previous computations while a traditional neural network \nassumes all inputs (and outputs) are independent of each other. In theory, RNNs can make use of \ninformation in arbitrarily long sequences, but in practice they fail due to the gradient \nvanishing/exploding problems [19]. \n \n4.3. LONG SHORT-TERM MEMORY (LSTM) \n \nLSTMs, a special kind of RNN, are designed to address the gradient vanishing problems of \nRNNs. Therefore, they are capable of learning long-term dependencies. They were introduced by \n[20]. The key to LSTM is the cell state which is controlled by three multiplicative gates. The \nformulas to update each gate and cell state of input x are defined as follows: \n \n ) \n \n \n(1) \n \n  \n \n \n(2) \n \n   \n \n(3) \n \n  \n \n(4) \n \n   \n \n \n(5) \n \n \n \n          (6) \n \nwhere  denotes the sigmoid function; \n is the element-wise multiplication operator; W terms \ndenote weight metrics; b are bias vectors; and i, f, o denote input, forget and output gate \nrespectively and c are cell activation vectors. The term h represents the hidden layer nodes. \n \n4.4. BIDIRECTIONAL LSTM \n \nBiLSTM network, a modification of the LSTM, is designed to capture information of sequential \ndata and have access to both past and future contexts. It consists of two LSTMs, a forward and \nbackward LSTM propagating in two directions. The forward LSTM reads stream of input data \nand computes the forward hidden states, while the backward LSTM reads the sequence in reverse \norder and creates the backward hidden states. The idea behind is that it creates two separate \nhidden states for each sequence so that the information of sequences from both directions is \nmemorized. By concatenating the two hidden states, the final output is formed. This advantage of \nmemorizing information for long periods in both directions can make great improvement in \nlinguistic computation. \n \n4.5. CONDITIONAL RANDOM FIELDS (CRF) \n \nCRF is a statistical probabilistic model for structured prediction. It provides a probabilistic \nframework for labeling and segmenting sequential data, based on the conditional approach. Let y \nbe a tag label sequence and x be an input sequence of words. Conditional models are used to label \nthe observation input sequence x by selecting the label sequence y that maximizes the conditional \nprobability\n. To do so, a conditional probability is computed: \nInternational Journal on Natural Language Computing (IJNLC) Vol.8, No.1, February 2019 \n6 \n \n        \n               (7) \n \nwhere Score is determined by defining some log potentials \n such that:  \n \n  \n \n(8) \n \nIn here, there are two kinds of potentials: emission and transition. \n \n      (9) \n \nIn training, log probability of a correct tag sequence is maximized. \n \n5. OUR WORK \n \nExperiments are performed by comparing different neural models on syllable level text rather \nthan word level and the performance results are compared with baseline statistical CRF model. In \nour neural training, we try to investigate various neural network architectures that automatically \ndetect syllable and character level features using bidirectional LSTM, CNN and also GRU \narchitecture to eliminate the need for most feature engineering. Both CNN and bidirectional \nLSTM networks have been investigated for modeling character level information. \n \n5.1. TRAINING SETUP \n \nExperiments were carried out as syllable-based sequence labeling, in which the LSTM-based \nneural networks were trained. In order to convert the NER problem into a sequence tagging \nproblem, a label is assigned for each token (syllable) to indicate the named entities in training and \ntest data. We used the Myanmar syllable segmentation algorithm “sylbreak” of [21] on sentences \nfor the syllable data representation and syllable-based labeling. \n \n5.2. DATA PREPARATION \n \nAs mentioned before, annotated corpus for Myanmar language are limited and scattered. For our \nexperiment and further researches on Myanmar NER, we developed a manually annotated \nMyanmar NER corpus. There is no other available Myanmar NE corpus that has as much data as \nour NE corpus. For Corpus building, Myanmar news sentences were collected from online \nofficial websites and manually annotated according to defined NEs tags. Moreover, we also take \nMyanmar sentences from ALT-Parallel-Corpus because a lot of transliterated names appear in \nthese sentences. Sentences from ALT corpus are translated from International news so that a lot \nof transliterated names are detected. The ALT corpus is one part of the Asian Language Treebank \n(ALT) project. Currently we have over 60K sentences in total. We separate those data into three \nsets, 58,043 sentences for training, 1,200 sentences for development and 1,200 sentences for \ntesting. We defined six types of NE tags for manual annotation: PNAME, LOC, ORG, RACE, \nTIME and NUM.  \n \nPNAME tag is used to indicate person names including nickname or alias, while LOC tag is \ndefined for location entities. In this case, location entities include politically or geographically \ndefined places (cities, provinces, countries, international regions, bodies of water, mountain, etc.). \nLocation also includes man-made structures like airports, highways, streets, factories and \nmonuments. Names of organizations (government and non-government organizations, \ncorporations, institutions, agencies, companies and other groups of people defined by an \nestablished organizational structure) are annotated with ORG tag. In our Myanmar language, \nInternational Journal on Natural Language Computing (IJNLC) Vol.8, No.1, February 2019 \n7 \n \nsome location names and names of national races have same spelling in writing scripts. For \nexample, the location name “ကရင်” (Kayin State) and one of the national races “ကရင်” (Kayin \nrace). For this reason, the NE tag RACE is defined to indicate names of national races. TIME is \nused for dates, months and years. NUM tag is used to indicate number format in sentences. \nTable 1 lists the entities distribution in our in-house NE corpus. \n \nTable 1.  Corpus Data Statistics. \n \nNE tags \nNumber of entities \nTrain \nDev \nTest \nPNAME  \n34262 \n622 \n517 \nLOC \n60910 \n1365 \n1211 \nORG \n19084 \n375 \n281 \nRACE  \n5359 \n200 \n161 \nTIME  \n28385 \n556 \n530 \nNUM \n19505 \n363 \n433 \n \n5.2.1. TAGGING SCHEME \n \nIn order to convert the NER problem into a sequence labeling problem, a label is assigned for \neach token (syllable) to indicate the NE in sentence. As tagging scheme, IOBES (Inside, Outside, \nBegin, End and Single) scheme is used for all the experiments. \n \n5.3. EXPERIMENTS WITH CRF \n \nFor syllable-based CRF trainings, an open source toolkit for linear chain CRF [22] was used.  \nExperiments were conducted by tuning various parameters with different features. Firstly, we \nonly used the tokens and their neighbouring contents as features and set the window size as 5 and \nthe best F-score of 89.05% is obtained when the cut-off threshold parameter is 3 and hyper-\nparameter c is 2.5. We also tried to add a small-sized named dictionary and clue words list as \nadditional features into the experiment. The additional features make the F-score have around 2.4 \n% increase (91.47 %) compared to the F-score of 89.05 % (Table 2). It shows that CRF works the \nbest when feature engineering is carefully prepared.  \n \nTable 2.  Results on baseline CRF \n \nCRF models \nPrecision \nRecall \nF-measure \nSyllable-based \n89.75 \n88.36 \n89.05 \nSyllable-based+clue word \nfeature+name list \n90.92 \n92.02 \n91.47 \n \n5.4. NEURAL MODEL TRAINING \n \nGiven a Myanmar sentence, syllable is treated as the basic training unit for label tagging. We first \nlearn the input representation of each input token, and then the learned representation was fed into \nBiLSTM network for sequence leaning. On top of the network, a CRF inference layer determines \nthe tag with the maximum probability of the syllable (see Figure. 4). \n \nFor the implementation of the neural network model training, we utilized the PyTorch framework \n[23] which provides flexible choices of feature inputs and output structure. Experiments were run \non Tesla K80 GPU.  Based on different parameter settings, the training time for each experiment \nis different.  \nInternational Journal on Natural Language Computing (IJNLC) Vol.8, No.1, February 2019 \n8 \n \nAs to input embedding setting, we tried experiments on using pre-trained 100-dim of embedding \non syllable level and character level data, respectively. The data used for training the syllable and \ncharacter embedding includes 200K sentences in total. In input embedding, there are two parts: \ncharacter representation and syllable representation. Character embedding was tried to learn from \ntraining data by CNN and also with bidirectional LSTM network. Given a training sequence, we \ntook syllables as basic training unit and syllables were projected into a d-dimension space and \ninitialized as dense vectors. \n \nAs to optimization, both the stochastic gradient descent algorithm (SGD) and Adam algorithm \nwere tried. For the SGD, it was performed with initial learning rate of 0.015 and momentum 0.1. \nThe learning decay rate was set as 0.05. For the Adam algorithm, the initial learning rate was set \nas 0.0015. Both optimizations had batch sizes set as 30.   \n \n \n \nFigure 4. The architecture of our network \n \nEarly Stopping was used based on the performance on validation sets. A dropout of 0.5 was set \nfor both embedding and output layers to mitigate overfitting in the training process. The hidden \ndimension was set to 200 in the whole experiment. For syllable-based BiLSTM_CRF which used \nCNN in character representation experiments, the best accuracy appears at 82 epochs and for the \nsyllable-based BiLSTM_CRF which applied bidirectional LSTM on character representation \nexperiments, the best accuracy happens at 77 epochs. \n \n5.5. EXPERIMENTAL RESULTS AND ANALYSIS \n \nIn Table 3, we listed the best experimental results from different model structures. For both \nBiLSTM and BiLSTM_CRF on syllable level NER, we relied on a LSTM model or CNN model \nto learn char features first and then concatenated it with the syllables’ embeddings as the input of \nto the BiLSTM or BiLSTM_CRF model. The char features learned from LSTM is not so good as \nCNN in our experiments. When the GRU is applied on syllable, it makes the F-score value drops \ncontinuously as the epoch increases, which is out of our expectation so we did not list the results \nin Table 3. The performance of using SGD optimization algorithm is slightly worse than using \nAdam, which you can see from Table 3. The random embedding also did not perform better than \nthe pre-trained word embedding from word2vec. So the results in the Table 3 are from pre-trained \nembedding. For the syllable-based experiments, we can see that BiLSTM_CRF performs slightly \nbetter than BiLSTM.  \nInternational Journal on Natural Language Computing (IJNLC) Vol.8, No.1, February 2019 \n9 \n \n \nAs to the syllable based system, since syllables as inputs contain more information than \nindividual characters as inputs, it is reasonable that it performs better than the character based \nsystem. On syllable level data, by using CNN to extract character features from the data as \nadditional information inputs, better results are produced compared to not using character features \nor using LSTM to extract character features (the best performance is highlighted in Table 3).    \nIf compared the results in Table 3 with the results in Table 2, we can see that neural models \nperform better than statistical CRF models.  By comparison, syllable-based neural models without \nadditional feature engineering perform better than CRF models. Syllable-based CRF model with \nadditional feature is approaching the results of neural models. Although those experiments give \nthe promising results, the size of data used for neural network model training is not so big \ncompared to other NER corpus of other languages. Normally more data can help neural network \nlearn better. Moreover, due to time and data limit, the hyper-parameters used in the experiment \nmay be not the best. This needs modelling experiences and also large amount of trial and error \nexperiments to decide. \n \nTable 3.  F-score results comparison between different neural models. \n \nModels \nPrecision/Recall/F-measure \nDev  \nTest \nBiLSTM(SGD) \n89.40/88.48/88.94 \n89.99/89.97/89.98 \nBiLSTM (Adam) \n89.32/88.47/88.89 \n90.71/91.64/91.17 \nBiLSTM_CRF(Adam) \nwith CharCNN \n91.06/90.45/90.75 \n94.66/94.99/94.82 \nBiLSTM_CRF(Adam) \nwith CharLSTM \n90.17/89.68/89.92 \n94.71/94.83/94.77 \nBiLSTM_CRF(SGD) \nwith CharCNN \n89.57/90.37/89.97 \n93.01/94.19/93.61 \n \nBiLSTM_CRF(SGD) \nwith CharLSTM \n84.82/85.65/85.65 \n90.24/91.83/91.03 \n \n \n6. CONCLUSION \n \nIn this paper, we have explored the effectiveness of neural network on Myanmar NER and \nconducted a systematic comparison between neural approaches and traditional CRF approaches \non our manually annotated NE corpus. Experiment results revealed that the performance of neural \nnetworks on Myanmar NER is quite promising, because neural models did not use any \nhandcrafted features or additional resources. Although our NE corpus is not so big, neural \nnetwork models  produce better performance than CRF models for Myanmar NER, we still \nbelieve with more data and more experiments, neural networks can learn better so as to produce \nbetter results. From the experiments, we can see that neural network performs much better while \nin the experiments of using CRF models, only by adding additional name list features and clue \nword list, produced the similar accuracy as the syllable-based neural models. \n \nAnyway, this exploration of using neural networks for Myanmar NER is the first work to apply \nneural networks on Myanmar language. It showed us that BiLSTM_CRF network on syllable \nlevel data jointly with CNN to extract character feature can facilitate Myanmar NER. With more \ndata and more experiments, better results will be reported in the future and we will keep exploring \nneural networks on other Myanmar NLP work, e.g., POS tagging and word segmentation, too. \n \n \n \n \nInternational Journal on Natural Language Computing (IJNLC) Vol.8, No.1, February 2019 \n10 \n \nACKNOWLEDGEMENTS \n \nThe ASEAN IVO (http://www.nict.go.jp/en/asean_ivo/index.html) project “Open Collaboration \nfor Developing and Using Asian Language Treebank” was involved in the production of the \ncontents \nof \nthis \npublication \nand \nfinancially \nsupported \nby \nNICT \n(http://www.nict.go.jp/en/index.html). \n \nREFERENCES \n \n[1] \nAlireza Mansouri, Lilly Suriani Affendey, Ali Mamat, (2008), “Named Entity Recognition \nApproaches”, Proceedings of IJCSNS International Journal of Computer Science and Network \nSecurity 8(2), 339–344. \n \n[2] \nRonan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel P. \nKuksa, (2011) “Natural language processing (almost) from scratch”, CoRR, abs/1103.0398. \n \n[3] \nZhiheng Huang, Wei Xu, and Kai Yu, (2015), “Bidirectional LSTM-CRF models for sequence \ntagging”, CoRR, abs/1508.01991. \n \n[4] \nThi Thi Swe, Hla Hla Htay, (2010), “A Hybrid Methods for Myanmar Named Entity Identification \nand \nTransliteration \ninto \nEnglish”, \nhttp://www.lrec-\nconf.org/proceedings/lrec2010/workshops/W16.pdf. \n \n[5] \nThida Myint, Aye Thida, (2014), “Named Entity Recognition and Transliteration in Myanmar Text”, \nPhD Research, University of Computer Studies, Mandalay. \n \n[6] \nMo H.M., Nwet K.T., Soe K.M. (2017) “CRF-Based Named Entity Recognition for Myanmar \nLanguage”. In: Pan JS., Lin JW., Wang CH., Jiang X. (eds) Genetic and Evolutionary Computing. \nICGEC 2016. Advances in Intelligent Systems and Computing, vol 536. Springer, Cham. \n \n[7] \nGuillaume Lample, Miguel Balesteros, Sandeep Subramanian, Kazuya Kawakami and Chris Dyer, \n(2016), “Neural Architecture for Named Entity Recognition”, Proceedings of NAACL-HLT 2016, \npages 260–270, Association for Computational Linguistics. \n \n[8] \nJason P.C. Chiu and Eric Nichols, (2016), “Named Entity Recognition with Bidirectional LSTM-\nCNNs”, Transactions of the Association for Computational Linguistics, vol. 4, pp. 357–370. \n \n[9] \nDaniele Bonadiman, Aliaksei Severyn and Alessandro Moschitti, (2015), “Deep Neural Networks for \nNamed Entity Recognition in Italian”. \n \n[10] Weihua Wang, Feilong Bao and Guanglai Gao, (2016), “Mongolian Named Entity Recognition with \nBidirectional Recurrent Neural Networks”, IEEE 28th International Conference on Tools with \nArtificial Intelligence. \n \n[11] Shotaro Misawa, Motoki Taniguchi, Yasuhide Miura and Tomoko Ohkuma, (2017), “Character-based \nBidirectional LSTM-CRF with words and characters for Japanese Named Entity Recognition”, \nProceedings of the First Workshop on Subword and Character Level Models in NLP, pages 97–102, \nAssociation for Computational Linguistics. \n \n[12] Anh L.T, Arkhipov M.Y., and Burtsev M.S., (2017), “Application of a Hybrid Bi-LSTM-CRF model \nto the task of Russian Named Entity Recognition”. \n \n[13] Antonio Jimeno Yepes and Andrew MacKinlay, (2016), “NER for Medical Entities in Twitter using \nSequence Neural Networks”, Proceedings of Australasian Language Technology Association \nWorkshop, pages 138−142. \nInternational Journal on Natural Language Computing (IJNLC) Vol.8, No.1, February 2019 \n11 \n \n[14] Nut Limsopatham and Nigel Collier, (2016), “Bidirectional LSTM for Named Entity Recognition in \nTwitter Messages”, Proceedings of the 2nd Workshop on Noisy User-generated Text, pages 145–152. \n \n[15] Lishuang Li, Like Jin, Zhenchao Jiang, Dingxin Song and Degen Huang, (2015), “Biomedical Named \nEntity Recognition Based on Extended Recurrent Newual Networks”, IEEE International Conference \non Bioinfonnatics and Biomedicine. \n \n[16] Zhehuan Zhao, Zhihao Yang, Ling Luo, Yin Zhang, Lei Wang, Hongfei Lin and Jian Wang, (2015), \n“ML-CNN: a novel deep learning based disease named entity recognition architecture”, IEEE \nInternational Conference on Bioinfonnatics and Biomedicine. \n \n[17] Zin Maung Maung and Yoshiki Mikami, (2008), “A rulebased syllable segmentation of myanmar \ntext”, In IJCNLP, Workshop on NLP for Less Privileged Languages, pages 51–58. \n \n[18] Tin Htay Hlaing and Yoshiki Mikami, (2014), “Automatic syllable segmentation of myanmar texts \nusing finite state transducer”, ICTer, 6(2). \n \n[19] Yoshua Bengio, Patrice Simard, and Paolo Frasconi, (1994), “Learning long-term dependencies with \ngradient descent is difficult”, IEEE transactions on neural networks, 5(2):157-166. \n \n[20] Sepp Hochreiter and J¨urgen Schmidhuber, (1997), “Long short-term memory”, Neural computation, \n9(8):1735–1780. \n \n[21] Ye Kyaw Thu, (2017), “Syllable segmentation tool for myanmar language (burmese)”, https:// \ngithub.com/ye-kyaw-thu/sylbreak. \n \n[22] Taku Kudo, (2005), “Crf++: Yet another crf toolkit”, Software available at http://crfpp. sourceforge. \nnet. \n \n[23] Jie Yang, (2017),  https://github.com/jiesutd/PyTorchSeqLabel \n \n[24] Nils Remers and Iryna Gurevych, (2017), “Optimal Hyperparameters for Deep LSTM-Networks for \nSequence Labeling Tasks”, EMNLP. \n \n[25] Xuezhe Ma and Eduard Hovy, (2016), “ End-to-end Sequence Labeling via Bi-directional LSTM-\nCNNs-CRF”, ACL 2016, Berlin, Germany. \n \n \nAUTHORS  \n \nHsu Myat Mo got her B.C.Sc (Hons) in 2010, followed by M.C.Sc (Credit:) in 2012, \nrespectively. Currently she is doing her Ph.D research focusing on Myanmar NER in \nNatural Language Processing Lab, at the University of Computer Studies, Yangon. \n \n \nDr. Khin Mar Soe got Ph.D(IT) in 2005. Currently she is working as a Professor and also \nthe Head of Natural Language Processing Lab, at the University of Computer Studies, \nYangon. She has been supervising Master thesis and Ph.D researches on Natural Language \nProcessing. Moreover, she participated in the project of ASEAN MT, the machine \ntranslation project for South East Asian languages. \n \n \n  \n \n \n \n",
  "categories": [
    "cs.CL"
  ],
  "published": "2019-03-12",
  "updated": "2019-03-12"
}