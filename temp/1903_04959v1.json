{
  "id": "http://arxiv.org/abs/1903.04959v1",
  "title": "Deep Multi-Agent Reinforcement Learning with Discrete-Continuous Hybrid Action Spaces",
  "authors": [
    "Haotian Fu",
    "Hongyao Tang",
    "Jianye Hao",
    "Zihan Lei",
    "Yingfeng Chen",
    "Changjie Fan"
  ],
  "abstract": "Deep Reinforcement Learning (DRL) has been applied to address a variety of\ncooperative multi-agent problems with either discrete action spaces or\ncontinuous action spaces. However, to the best of our knowledge, no previous\nwork has ever succeeded in applying DRL to multi-agent problems with\ndiscrete-continuous hybrid (or parameterized) action spaces which is very\ncommon in practice. Our work fills this gap by proposing two novel algorithms:\nDeep Multi-Agent Parameterized Q-Networks (Deep MAPQN) and Deep Multi-Agent\nHierarchical Hybrid Q-Networks (Deep MAHHQN). We follow the centralized\ntraining but decentralized execution paradigm: different levels of\ncommunication between different agents are used to facilitate the training\nprocess, while each agent executes its policy independently based on local\nobservations during execution. Our empirical results on several challenging\ntasks (simulated RoboCup Soccer and game Ghost Story) show that both Deep MAPQN\nand Deep MAHHQN are effective and significantly outperform existing independent\ndeep parameterized Q-learning method.",
  "text": "Deep Multi-Agent Reinforcement Learning with Discrete-Continuous Hybrid\nAction Spaces\nHaotian Fu1 , Hongyao Tang1 , Jianye Hao1 , Zihan Lei2 , Yingfeng Chen2 , Changjie Fan2\n1Tianjin University\n2Fuxi AI Lab in Netease\n{haotianfu, bluecontra, jianye.hao}@tju.edu.cn,\n{leizihan, chenyingfeng1, fanchangjie}@corp.netease.com\nAbstract\nDeep Reinforcement Learning (DRL) has been ap-\nplied to address a variety of cooperative multi-agent\nproblems with either discrete action spaces or con-\ntinuous action spaces.\nHowever, to the best of\nour knowledge, no previous work has ever suc-\nceeded in applying DRL to multi-agent problems\nwith discrete-continuous hybrid (or parameterized)\naction spaces which is very common in practice.\nOur work ﬁlls this gap by proposing two novel\nalgorithms: Deep Multi-Agent Parameterized Q-\nNetworks (Deep MAPQN) and Deep Multi-Agent\nHierarchical Hybrid Q-Networks (Deep MAH-\nHQN). We follow the centralized training but de-\ncentralized execution paradigm:\ndifferent levels\nof communication between different agents are\nused to facilitate the training process, while each\nagent executes its policy independently based on\nlocal observations during execution. Our empiri-\ncal results on several challenging tasks (simulated\nRoboCup Soccer and game Ghost Story) show that\nboth Deep MAPQN and Deep MAHHQN are ef-\nfective and signiﬁcantly outperform existing inde-\npendent deep parameterized Q-learning method.\n1\nIntroduction\nReinforcement learning (RL) has recently shown a great suc-\ncess on a variety of cooperative multi-agent problems, such as\nmultiplayer games [Peng et al., 2017], autonomous cars [Cao\net al., 2013] and network packet delivery [Ye et al., 2015].\nIn many such settings, it is necessary for agents to learn de-\ncentralized policies due to the partial observability and lim-\nited communication. Fortunately, we can learn such policies\nusing the paradigm of centralized training and decentralized\nexecution. Forester et al. [2016] developed a decentralized\nmulti-agent policy gradient algorithm; Lowe et al. [2017] ex-\ntended DDPG [Lillicrap et al., 2016] to multi-agent setting\nwith a centralized Q-function; Rashid et al. [2018] employs\na Qmix network that estimates joint action-values as a com-\nplex non-linear combination of per-agent action values that\ncondition on local observations only.\nThese popular multi-agent reinforcement learning meth-\nods all require the action space to be either discrete or\ncontinuous.\nHowever, very often the action space in real\nworld is discrete-continuous hybrid, such as Robot soc-\ncer [Hausknecht, 2016; Masson et al., 2016] and Real Time\nStrategic (RTS) games [Xiong et al., 2018]. In such settings,\neach agent usually needs to choose a discrete action and the\ncontinuous parameters associated with it at each time step.\nAn obvious approach to handling this is to simply approxi-\nmate hybrid action spaces by a discrete set or relax it into a\ncontinuous set [Hausknecht and Stone, 2016]. However, such\napproaches suffer from a number of limitations: for the con-\ntinuous part of hybrid action, establishing a good approxima-\ntion usually requires a huge number of discrete actions; for\nthe discrete part of hybrid action, relaxing them into a con-\ntinuous set might signiﬁcantly increase the complexity of the\naction space.\nAn alternative and better solution is to learn directly\nover hybrid action spaces. Following this direction, Xiong\net al. [2018] proposed Parameterized Deep Q-Network (P-\nDQN) for single-agent learning in hybrid action spaces with-\nout approximation or relaxation by seamlessly integrating\nDQN [Mnih et al., 2013] and DDPG [Lillicrap et al., 2016].\nHowever P-DQN cannot be directly applied to multi-agent\nsettings due to the non-stationary property in multi-agent en-\nvironments.\nIn multi-agent settings, explicit coordination\nmechanism among agents’ hybrid action spaces needs to be\nintroduced.\nIn this work, we propose two approaches to address co-\noperative multi-agent problems in discrete-continuous hybrid\naction spaces based on centralized training and decentralized\nexecution framework. The ﬁrst approach, Deep Multi-Agent\nParameterized Q-networks (Deep MAPQN), extends the ar-\nchitecture of P-DQN to multi-agent settings by leveraging the\nidea of Qmix [2018]. Our algorithm utilizes a joint action-\nvalue function to update policies of hybrid actions for all\nagents. However, Deep MAPQN requires to compute contin-\nuous parameters for all the discrete actions of all agents and\nthus may suffer from high computational complexity when\nthe discrete part of hybrid action space has large dimensions.\nTo alleviate this problem, we propose the second approach,\nDeep Multi-Agent Hierarchical Hybrid Q-networks (Deep\nMAHHQN). In contrast to Deep MAPQN, Deep MAHHQN\nonly needs to calculate continuous parameters of the optimal\ndiscrete action for each agent. In addition, Deep MAHHQN\nfurther alleviates the non-stationary issue of multi-agent en-\narXiv:1903.04959v1  [cs.LG]  12 Mar 2019\nvironments by realizing centralized training fashion at both\naction levels and augmenting each centralized training frame-\nwork with information about policies of other action levels.\nEmpirical results on standard benchmark game Half Field Of-\nfense (HFO) and a large-scale online video game Ghost Story\nshow the superior performance of our approaches compared\nto independent P-DQN.\n2\nBackground\n2.1\nCooperative Stochastic Game\nIn this work, we consider a Cooperative Stochastic\nGame [Wei et al., 2018a] in partially observable settings\nmodeled as a tuple {S, U, r, P, γ, H, N} . This game for N\nagents is deﬁned by a set S of states describing the possible\nconﬁgurations of all agents, a set of observations O1, ..., ON\nfor each agent and a joint action space of N agents deﬁned as\nU = A1 ×· · ·×AN. At each time step, each agent i chooses\nan action ai ∈Ai using policy πi and constitutes a joint ac-\ntion ⃗a ∈U, which produces the next state s′ following the\nstate transition function P(s′|s,⃗a): S × U × S →[0, 1]. All\nagents share the same reward function r(s,⃗a): S × U →R.\nγ is the discount factor and H is the time horizon.\n2.2\nDeep Multi-agent Reinforcement Learning\nMulti-agent learning has been investigated comprehensively\nin both discrete action domains and continuous action do-\nmains under framework of centralized training and decen-\ntralized execution.\nMADDPG [Lowe et al., 2017] and\nQmix [Rashid et al., 2018] are representative ones for dis-\ncrete and continuous action domains..\nMADDPG mainly focuses on multi-agent problems with\ncontinuous action spaces. The core idea is to learn a central-\nized critic Qµ\nI (x, a1, · · · , aN) for each agent which condi-\ntions on global information. Qµ\nI (x, a1, · · · , aN) takes as in-\nput the actions of all agents, a1, · · · , aN, in addition to global\nstate information x (i.e., x = (o1, · · · , oN)) and outputs the\ncentralized action-value for agent i.\nQmix employs a mixing network that estimates joint\naction-values Qtot as a complex non-linear combination of\nper-agent action value that conditions only on local observa-\ntions. The weights of the mixing network are produced by\nseparate hypernetworks which take the global state informa-\ntion as input. Importantly, Qmix ensures a global argmax\nperformed on Qtot yields the same result as a set of individual\nargmax operations performed on each agent’s action-value\nQi by a monotonicity constraint:\n∂Qtot\n∂Qi\n≥0,\n(1)\nUnfortunately, such multi-agent algorithms can only be ap-\nplied to either discrete action space or continuous action\nspace. For multi-agent problems with hybrid action space,\nexplicit coordination mechanism among agents needs to be\nintroduced.\n2.3\nParameterized Deep Q-Networks\nTo handle reinforcement learning problems with hybrid ac-\ntion space, Xiong et al. [2018] propose a new framework\ncalled Parameterized Deep Q-Networks (P-DQN). The core\nidea is to update the discrete-action and continuous-action\npolicies separately combining the structure of DQN [Mnih\net al., 2013] and DDPG [Lillicrap et al., 2016]. P-DQN ﬁrst\nchooses low-level parameters associated with each high level\ndiscrete action, then ﬁgures out which discrete-continuous\nhybrid action pair maximizes the action-value function.\nMore concretely, we can deﬁne the discrete-continuous hy-\nbrid action space A as:\nA = {(k, xk)|xk ∈Xk for all k ∈[K]} ,\n(2)\nwhere [K] = {1, · · · , K} is the discrete action set, Xk is a\ncontinuous set for all k ∈[K]. Then we can deﬁne a de-\nterministic function which maps the state and each discrete\naction k to its corresponding continuous parameter xk:\nxk = µk(s; θ),\n(3)\nwhere θ are weights of the deterministic policy network. We\nfurther deﬁne an action-value function Q (s, k, xk; ω) which\nmaps the state and hybrid actions to real values. Here ω are\nweights of the value network.\nP-DQN updates the action-value function Q by minimizing\nthe following loss:\nlQ(ω) = 1\n2 [Q(s, k, xk; ω) −y]2 ,\n(4)\nwhere y = r + maxk∈[K] γQ(s′, k, µk(s′; θ); ω), and s′ de-\nnotes the next state after taking hybrid action (k, xk). The\npolicy µk for the continuous part is updated by minimizing\nthe following loss with parameters ω ﬁxed:\nlΘ(θ) = −\nK\nX\nk=1\nQ(s, k, µk(s; θ); ω)\n(5)\nNote that here the action value function Q (s, k, xk; ω)\nmainly plays two roles. First, it outputs the greedy policy for\ndiscrete action (which is consistent with DQN) . Secondly, it\nworks as the critic value for different associated continuous\nparameters like DDPG which provides gradients for updating\npolicies in continuous action space.\n3\nMethods\nTo handle hybrid action spaces in multi-agent settings,\none natural approach is to adopt the independent learning\nparadigm and equip each agent with an independent P-DQN\nalgorithm. However, one major issue is that each agent’s pol-\nicy changes during training, resulting in the non-stationarity\nof environments. As we will show in our experiments in Sec-\ntion 4, independent P-DQN does not perform well in practice.\nIn this paper, we propose two novel deep multi-agent learn-\ning methods for hybrid action spaces, Deep MAPQN and\nDeep MAHHQN. By leveraging the current state-of-the-art\nsingle-agent deep RL for hybrid action spaces and coordina-\ntion techniques for multi-agent learning, both Deep MAPQN\nand Deep MAHHQN can support multiple agents to learn ef-\nfective coordination policies directly over the hybrid action\nspaces.\nFigure 1: The overall Deep MAPQN structure\n3.1\nDeep Multi-Agent Parameterized Q-Networks\n(Deep MAPQN)\nThe ﬁrst algorithm is a natural extention of single-agent P-\nDQN. We leverage Qmix [Rashid et al., 2018] architecture to\ncoordinate the policy update over hybrid action spaces among\nagents. The overall structure of Deep MAPQN is shown in\nFigure 1.\nFor each agent, we adopt the same settings in P-DQN. Con-\ncretely, considering a game with N agents, each agent i uses a\ndeterministic policy network µki(θi) and an action value net-\nwork Qi(ωi) to output hybrid action (k∗\ni , x∗\nki). The determin-\nistic policy network µki(θi) takes each agent’s observation oi\nas input and outputs the optimal continuous parameters xki\nfor all the discrete actions ki ∈{1, · · · , Ki}. Then the action\nvalue network Qi outputs the optimal hybrid action by:\n(k∗\ni , x∗\nki) = argmax(ki,xki)Qi (si, (ki, xki); ωi) ,\n(6)\nwhere ωi are parameters for action value network of agent i.\nTo achieve coordinated update among agents’ action value\nnetworks, we utilize a mixing network and produce a fully\ncentralized state-action value function Qtot which can facil-\nitate the coordinated update of the decentralized policies in\nhybrid action spaces. The mixing network consists of a feed-\nforward neural network and separate hypernetworks. The hy-\npernetworks take the global state s as the input and output the\nweights of the feedforward network. The feedforward net-\nwork takes each agent’s output Qi as input and mixing them\nmonotonically, producing the joint action value denoted by\nQtot.1 Here we deﬁne the mixing network as a non-linear\ncomplex function f and denote this process by:\nQtot = f(s, Q1, · · · , QN; ωmix)\n(7)\nWe update the mixing network weights ωmix along with each\nagent’s action value network weights ωi by minimizing:\nL(ω) = Es,⃗k,⃗xk,r,s′∼D[ytot −Qtot(s,⃗k, ⃗xk)]2,\n(8)\n1The detailed structure for the mixing network can be found at:\nhttps://bit.ly/2Eaci2X.\nwhere ytot\n=\nr + γ max⃗k′,⃗xk′ Qtot(s′,⃗k′, ⃗xk′(θ′)), and\n(⃗k, ⃗xk) is the joint action, θ′ are parameters of target policy\nnetworks. Our framework for computing Qtot ensures off-\npolicy learning updates while each agent can still choose the\ngreedy action with respect to its own Qi in a decentralized\nfashion. This is because a global argmax on Qtot is equiva-\nlent with argmax on each Qi as explained in Section 2.2.\nFinally we need to compute gradients for deterministic pol-\nicy network. We ﬁrst take the sum of Q-values of all the dis-\ncrete actions for each agent i:\nbQi =\nKi\nX\nki=1\nQi (si, ki, xki; ωi) , where ki ∈{1, 2, · · · , Ki} ,\n(9)\nand then feed them into the mixing network, producing the\nvalue of Qs\ntot. This process can be denoted by:\nQs\ntot = f(s, bQ1, · · · , bQN; ωmix)\n(10)\nWe update all agents’ continuous policies µi (i ∈N) by max-\nimizing Qs\ntot with parameters ωi and ωmix ﬁxed, the gradient\ncan be written as:\n∇θil(θi) =\nEs,⃗k∼D[∇θiµki(oi)∇xkiQs\ntot(s,⃗k, ⃗xk; ω) |xki=µki(oi)]\n(11)\nIn this way we can update the policies of continuous param-\neters for all the different agents and different discrete actions\nin one single training step.\nHowever, this algorithm may result in high computational\ncomplexity during both training and execution phases: ev-\nery time we compute the joint action value Qtot, we need to\ncompute continuous parameters for all the K discrete actions\nof all the agents. This is particularly severe when the discrete\npart of hybrid action space has a large dimension. The same\nproblem exists in original P-DQN as well.\n3.2\nDeep Multi-Agent Hierarchical Hybrid\nQ-Networks (Deep MAHHQN)\nTo address the problem we mentioned in previous section,\nwe propose another novel algorithm Deep MAHHQN, as\ninspired by Hierarchical Learning [Kulkarni et al., 2016;\nTang et al., 2018].\nThe overall structure of Deep MAHHQN is illustrated in\nFigure 2. Deep MAHHQN consists of a high-level network\ncoordinating the learning over joint discrete actions and a\nlow-level network for learning the coordinated policy over\nthe continuous parameters. We train the high-level network\nand low-level network separately and both of them follow the\ncentralized training but decentralized execution paradigm.\nDifferent from Deep MAPQN, when choosing hybrid ac-\ntions, a deep MAHHQN agent ﬁrst chooses a discrete ac-\ntion through high-level network and then decides the corre-\nsponding continuous parameters conditioning on the given\ndiscrete action and individual observation through low-level\nnetwork. This is consistent with human’s decision-making\nprocess since in real world humans usually tend to decide\nwhat to do before deciding to what extent to do it.\nFigure 2: The overall Deep MAHHQN structure\nAs for the high-level network, each agent utilizes the ba-\nsic DQN [Mnih et al., 2015] framework to select a discrete\naction ki given its own observation oi. The high-level poli-\ncies among agents over discrete actions are coordinated by\nusing the mixing network structure [Rashid et al., 2018]. Im-\nportantly, unlike Qmix which only takes global state s as\ninputs for the hypernetworks to produce weights of mixing\nnetworks, we further consider low-level network’s current\npolicies for each agent. This information is critical for co-\nordinating agents’ discrete actions since the hybrid action\nspaces are highly correlated in determining the global opti-\nmal policies. Concretely, at each training step, we ﬁrst cal-\nculate the continuous parameters xi related to each agent’s\nhigh level action ki using the current low-level network’s pol-\nicy (i.e., xi = µi(ϕ(oi, ki)) ). Then we combine them (i.e.,\nχ = {x1, · · · , xN} ) with the global state s and feed them\ninto the hypernetworks to generate weights for the mixing\nnetwork. The high-level networks’ parameters are updated\nby minimizing the following loss:\nL(ωh) = Es,⃗k,r,s′∼D(yh\ntot −Qh\ntot(s,⃗k, χ))2,\n(12)\nwhere ⃗k denotes the joint discrete action sampled from the\nreplay buffer, ytot = r + γ max⃗k′ Qh\ntot(s′,⃗k′, χ′), χ′ is a set\nof continuous parameters from low-level target policies.\nFor the low-level part, each agent i chooses continuous pa-\nrameters xi according to its new observation:\nboi = ϕ(oi, ki),\n(13)\nwhere ki is the discrete action obtained from the high-level\npart. In our experiments, we simply concatenate oi and ki as\nthe new observation boi. In order to learn a coordinated pol-\nicy over the corresponding continuous parameters, we apply\nMulti-agent Actor-critic framework [Lowe et al., 2017] and\nfurther modify it with a centralized Q function for each agent.\nConsidering N agents with low-level policies parameterized\nby θ = {θ1, · · · , θN}, and let µ = {µ1, · · · , µN} be the set\nof all agents’ low-level policies. The gradient with expected\nreturn for agent i with low-level policy µi can be written as:\n∇θil(θi) =\nEs,k,x∼D[∇θiµi(boi)∇xiQl\ni(s, k1, x1, · · · , kN, xN)|xi=µi( b\noi)]\n(14)\nHere Ql\ni(s, k1, x1, · · · , kN, xN) is a centralized action-value\nfunction that takes as input the hybrid actions of all\nagents (k1, x1), · · · , (kN, xN), in addition to the global\nstate s,\nand outputs the low-level Q-value for agent\ni.\nThe experience replay buffer D contains the tuples\n(s, k1, x1, · · · , kN, xN, r1, · · · , rN, s′).\nThe centralized Q\nfunction is updated as:\nL(ωl\ni) = Es,k,x,r,s′∼D[yi −Ql\ni(s, k1, x1, · · · , kN, xN)]2,\nyi = ri + γQl′\ni (s′, k′\n1, x′\n1, · · · , k′\nN, x′\nN)|x′\nj=µ′\nj( b\noi′),\n(15)\nwhere Ql′\ni denotes the low-level target Q network for agent i.\nNote that k′\ni are derived from high-level target policies. Com-\nbining (14) and (15) yields our proposed low-level network.\nCompared with Deep MAPQN, Deep MAHHQN only\nneeds to calculate one discrete action with the optimal contin-\nuous parameters for each agent. This would signiﬁcantly re-\nduce the algorithm’s computational complexities, which will\nbe validated in our experimental results. Moreover, both low-\nlevel and high-level training frameworks of Deep MAHHQN\nare augmented with extra information about policies of other\nagents and policies of other action levels. In hybrid action\nenvironments, we expect that such kind of communication\nwould better alleviate the non-stationary issue of the environ-\nments.\nWhen training Deep MAHHQN, we let the low-level net-\nwork train alone for m steps and then start training high-level\nand low-level network together. The main reason is that the\nassociated low-level continuous parameters play an important\nrole when our high-level network computes the value of Qh\ntot.\nOtherwise, the gradients for high-level network can be very\nnoisy and misleading since the low-level policies are still ex-\nploring at a high rate at the very beginning.\n4\nExperimental Results\nIn this section, we evaluate our algorithms in 1) the standard\nbenchmark game HFO, 2) 3v3 mode in a large-scale online\nvideo game Ghost Story. We compare our algorithms, Deep\nMAPQN and Deep MAHHQN, with independent P-DQN in\nall our experiments.\n4.1\nExperiments with Half Field Offense (HFO)\nEnvironment Settings\nHalf ﬁeld Offense (HFO) is an abstraction of full RoboCup\n2D game. The environment features in a hybrid action space.\nPrevious work [Hausknecht and Stone, 2016; Wang et al.,\n2018; Wei et al., 2018b] applied RL to the single-agent ver-\nsion of HFO, letting one agent try to goal without a goal-\nkeeper (1v0). In this paper, we apply our proposed algorithms\non the challenging multi-agent problems of HFO , which in-\ncludes 1v2 (two agents with the shared objective of defending\nthe goal against one opponent) and 2v1 (two agents with the\nshared objective of scoring the goal against one goalie). The\nopponent (or goalie) we play against are all built-in hand-\ncoded agents.\nWe use the high level feature set and each agent’s obser-\nvation is a 21-d vector consisting of its position and orien-\ntation; distance and angle to the ball and goal; an indicator\nif the agent can kick etc. A full list of state information can\nbe found at the ofﬁcial website https://github.com/mhauskn/\nHFO/blob/master/doc/manual.pdf.\nIn our experiments, we use a discrete-continuous hy-\nbrid action space.\nThe full set of hybrid actions is:\nKick To (targetx, targety, speed); Move To (targetx,\ntargety); Dribble To (targetx, targety); Intercept().\nValid values for targetx,y ∈[−1, 1] and speed ∈[0, 3]. In\nthis settings, acting randomly is almost unlikely to score or\nsuccessfully defend the goal and the exploration task proves\ntoo difﬁcult to gain traction on a reward that only consists of\nscoring goals. Thus, we do reward engineering to our two\ntasks respectively to alleviate the sparse reward problem,\nwhich will be described in details in following sections.\n1v2 Defense\nIn the defending scenario, our models control two agents with\nthe shared objective of defending the goal. Note that for de-\nfensive agents, only two actions Move To and Intercept are\napplicable since the defensive players do not control the ball.\nThe reward for each time step is calculated as a weighted sum\nof the following three types of statistics:\n• Move to ball reward:\nA reward proportional to the\nchange in distance between the agent and the ball.\n• Punishment for no agent in goal area. We add a punish-\nment if there’s no defensive agent in the goal area.\n• Bonus points for game result. Agents will get extra pos-\nitive points if they successfully defend the goal and vice\nversa.\nWe can see from Figure 3 that both Deep MAPQN and\nDeep MAHHQN achieve much higher performance than P-\nDQN. This demonstrates the beneﬁts of explicitly coordinat-\ning the joint hybrid policies among agents. In addition, Deep\nFigure 3: Successfully defense rates for Deep MAPQN, Deep MAH-\nHQN and P-DQN in 1v2 defense mode of HFO\nMAHHQN is found to outperform Deep MAPQN after con-\nvergence. We attribute this to the improved communication\nbetween different agents when we do centralized training for\nDeep MAHHQN. We further examine the learned behaviours\nof policies in order to better understand the methods. The\nagents of Deep MAPQN and Deep MAHHQN tend to play\ndifferent roles automatically when defending the ball: one\nagent moves directly to the goal area and act like a goal-\nkeeper while the other one approaches the offensive player\ntrying to capture the ball. In contrast, the agents of P-DQN\ntend to approach the offensive player or move to the goal area\ntogether without cooperating with an appropriate division of\ntheir roles. A video of our learned policies may be viewed at\nhttps://youtu.be/ndJYZFL5BxE.\n2v1 Offense\nWe further evaluate our algorithms on 2v1 offense mode\nwhich have larger action space and the coordination task is\nexpected to be more challenging. In this scenario, our models\ncontrol two agents aiming to score a goal against one goalie.\nWe add a discrete action Shoot() to our action list and there\nare totally ﬁve types of actions for this experiment. The re-\nward for each time step is calculated as a weighted sum of the\nfollowing types of statistics:\n• Move to ball reward: Similar to last section.\n• Kick to goal reward: A reward proportional to change in\ndistance between the ball and the center of the goal.\n• Additional punishment: To avoid long shot, we add an\nadditional punishment if the ball is too far away from\nboth agents when it is still outside the goal area.\n• Bonus points for game result: Agents will get extra\npoints if they successfully score a goal.\nAs shown in Figure 4, in this challenging offense problem,\nindependent P-DQN fails to learn the coordination policy. In\npractice, we observe that independent P-DQN agents ﬁrst try\nto approach the ball, then dribble it to somewhere and stop\nwithout shooting. A primary reason for this may be the lack\nof information of other teammates. In comparison, our pro-\nposed methods Deep MAPQN and Deep MAHHQN perform\nmuch better. Similar to the previous defense setting, the Deep\nMAPQN agents learn policies a little faster than Deep MAH-\nFigure 4: Goal rates for Deep MAPQN, Deep MAHHQN and P-\nDQN in 2v1 offense mode of HFO\nFigure 5: A screen shot of a 3v3 game mode in Ghost Story\nHQN in terms of the number of time steps required. The un-\nderlying reason is that in Deep MAPQN we update policies of\ncontinuous parameters associated with all the discrete actions\nat one single training step. However, such a framework also\nintroduces huge computational complexity since each train-\ning step of Deep MAPQN occupies much more computing\nresources than Deep MAHHQN. This problem can be seen\nmore clearly in our second game (Ghost Story) with more\nagents.\nTo summarize, both Deep MAPQN and Deep MAHHQN\noutperform independent P-DQN, and Deep MAHHQN seems\nto perform slightly better than Deep MAPQN. In our next sce-\nnario, we test our algorithms in a complicated practical envi-\nronment with larger hybrid action spaces and state spaces.\n4.2\nExperiments with Ghost Story\nWe evaluate the proposed algorithms with Ghost Story (or\nQianNvYouHun) – a fantasy massive multiplayer online role-\nplaying game (MMORPG) from NetEase, in which each of\nthe learning agents controls an individual hero. We performed\nour evaluation in the ”3v3” game mode, where 3 hero agents\ncooperate with each other to ﬁght against the other 3 built-in\nAI on the opposite side. At each time step, every hero can\nchoose to move or use one of its own skills. The game ends\nwhen all 3 heroes on the same side are killed.\nThe observation for each agent is a 97-d feature vector\nwhich is manually constructed using output from the game\nengine. Concretely, these features consist of some basic prop-\nerties of the agents: agent’s Health Point (HP), value range of\nattack, value range of defense2, carried skills and Cool Down\n(CD), carried buffs3, relative positions etc.\nWe simplify the actions of each hero into ﬁve hybrid ac-\ntion types: Move(x, y), move to a relative position(x,y); and\nfour skills, Tanlang(x, y), Siguai(x, y), Tianshou() and\nHegu(x, y). When a hero player chooses to use a skill, the\nenemies near the relative position (x, y) will be attacked or\nadded some buffs depending on the speciﬁc types of selected\nskills. Skill Tianshou has no parameters since it functions\n2The agent’s true attack or defense value is uniformly distributed\non the given value range.\n3Buffs can be beneﬁcial, such as increase agent’s own defense\nvalue, or harmful, such as decrease its own attack value, but buffs\ncan only exist for a short time period.\nFigure 6: Win rates for Deep MAPQN, Deep MAHHQN and P-\nDQN in 3v3 mode of Ghost Story\nas adding a buff on the hero agent itself. More details of our\nexperimental settings can be found at https://bit.ly/2Eaci2X.\nAt each time step, each agent receives a joint reward con-\nsists of four parts: (1) the change in HP for all heroes; (2) a\npunishment for the agents which did not do anything (e.g. No\nenemy is near the skill’s target point; the selected skill’s CD\nis not zero ); (3) small bonus points for killing an enemy hero\nand huge bonus points for winning the game.\nFigure 6 shows our experimental results. Clearly we can\nsee independent P-DQN fails to learn a coordinated policy\nthat can consistently defeat the enemies. Both Deep MAPQN\nand Deep MAHHQN outperform independent P-DQN learn-\ning method and get a win rate over 75%. Moreover, we ﬁnd\nDeep MAHHQN method performs slightly better than Deep\nMAPQN, which further validates that the improved commu-\nnication between agents using information about policies of\nother action levels can better stabilize the centralized train-\ning process. It indicates that Deep MAHHQN framework\nprovides a better way to handle problems with large hybrid\naction space and number of agents. A ﬁnal note is that the ac-\ntual training time of Deep MAPQN is about three days while\nDeep MAHHQN takes less than one day to train on the same\nNVidia Geforce GTX 1080Ti GPU. This is quite reasonable\nsince each MAPQN agent need to compute continuous pa-\nrameters for all the discrete actions at each single training\nstep while in Deep MAHHQN we only need to calculate low\nlevel continuous parameters associated with the selected op-\ntimal high-level discrete action.\n5\nConclusion\nThis paper should be seen as the ﬁrst attempt at applying\ndeep reinforcement learning in cooperative multi-agent set-\ntings with discrete-continuous hybrid action spaces.\nTwo\nnovel approaches are proposed under the paradigm of cen-\ntralized training and decentralized execution.\nThe experi-\nmental results show their superiority to independent param-\neterized Q-learning method under both the standard testbed\nHFO and a large-scale MMORPG game. As future work, we\nwish to extend our algorithms to competitive multi-agent set-\ntings, and conduct additional experiments with more agents\nand larger hybrid action space to further investigate the differ-\nence of performance between our two proposed algorithms.\nReferences\n[Cao et al., 2013] Yongcan Cao, Wenwu Yu, Wei Ren, and\nGuanrong Chen. An overview of recent progress in the\nstudy of distributed multi-agent coordination. IEEE Trans-\nactions on Industrial Informatics, 9:427–438, 2013.\n[Foerster et al., 2016] Jakob N. Foerster, Yannis M. Assael,\nNando de Freitas, and Shimon Whiteson.\nLearning to\ncommunicate with deep multi-agent reinforcement learn-\ning. In Proceedings of NeurIPS, pages 2137–2145, 2016.\n[Hausknecht and Stone, 2016] Matthew J. Hausknecht and\nPeter Stone. Deep reinforcement learning in parameter-\nized action space. In Proceedings of ICLR, pages 861–868,\n2016.\n[Hausknecht, 2016] Matthew J. Hausknecht. Half ﬁeld of-\nfense : An environment for multiagent learning and ad\nhoc teamwork. In Proceedings of ALA, pages 1391–1398,\n2016.\n[Kulkarni et al., 2016] Tejas\nD.\nKulkarni,\nKarthik\nNarasimhan,\nArdavan Saeedi,\nand Josh Tenenbaum.\nHierarchical deep reinforcement learning:\nIntegrat-\ning temporal abstraction and intrinsic motivation.\nIn\nProceedings of NeurIPS, pages 3675–3683, 2016.\n[Lillicrap et al., 2016] Timothy P. Lillicrap,\nJonathan J.\nHunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yu-\nval Tassa, David Silver, and Daan Wierstra. Continuous\ncontrol with deep reinforcement learning. Proceedings of\nICLR, pages 1052–1059, 2016.\n[Lowe et al., 2017] Ryan Lowe, Yi Wu, Aviv Tamar, Jean\nHarb, Pieter Abbeel, and Igor Mordatch.\nMulti-agent\nactor-critic for mixed cooperative-competitive environ-\nments.\nIn Proceedings of NeurIPS, pages 6382–6393,\n2017.\n[Masson et al., 2016] Warwick Masson, Pravesh Ranchod,\nand George Konidaris. Reinforcement learning with pa-\nrameterized actions. In Proceedings of AAAI, pages 1934–\n1940, 2016.\n[Mnih et al., 2013] Volodymyr Mnih, Koray Kavukcuoglu,\nDavid Silver, Alex Graves, Ioannis Antonoglou, Daan\nWierstra, and Martin A. Riedmiller.\nPlaying atari with\ndeep reinforcement learning. CoRR, abs/1312.5602, 2013.\n[Mnih et al., 2015] Volodymyr Mnih, Koray Kavukcuoglu,\nDavid Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-\nmare, Alex Graves, Martin A. Riedmiller, Andreas Fid-\njeland, Georg Ostrovski, Stig Petersen, Charles Beattie,\nAmir Sadik, Ioannis Antonoglou, Helen King, Dharshan\nKumaran, Daan Wierstra, Shane Legg, and Demis Has-\nsabis.\nHuman-level control through deep reinforcement\nlearning. Nature, 518:529–533, 2015.\n[Peng et al., 2017] Peng Peng,\nQuan Yuan,\nYing Wen,\nYaodong Yang, Zhenkun Tang, Haitao Long, and Jun\nWang.\nMultiagent bidirectionally-coordinated nets for\nlearning to play starcraft combat games.\nCoRR,\nabs/1703.10069, 2017.\n[Rashid et al., 2018] Tabish Rashid, Mikayel Samvelyan,\nChristian Schr¨oder de Witt, Gregory Farquhar, Jakob N.\nFoerster, and Shimon Whiteson. Qmix: Monotonic value\nfunction factorisation for deep multi-agent reinforcement\nlearning.\nIn Proceedings of ICML, pages 4292–4301,\n2018.\n[Tang et al., 2018] Hongyao Tang, Jianye Hao, Tangjie Lv,\nYingfeng Chen, Zongzhang Zhang, Hangtian Jia, Chunxu\nRen, Yan Zheng, Changjie Fan, and Li Wang.\nHierar-\nchical deep multiagent reinforcement learning.\nCoRR,\nabs/1809.09332, 2018.\n[Wang et al., 2018] Qing Wang, Jiechao Xiong, Lei Han,\nPeng Sun, Han Liu, and Tong Zhang.\nExponentially\nweighted imitation learning for batched historical data. In\nProceedings of NeurIPS, pages 6291–6300, 2018.\n[Wei et al., 2018a] Ermo Wei, Drew Wicke, David Freelan,\nand Sean Luke. Multiagent soft q-learning. In Proceedings\nof AAAI, pages 1729–1736, 2018.\n[Wei et al., 2018b] Ermo Wei, Drew Wicke, and Sean Luke.\nHierarchical approaches for reinforcement learning in pa-\nrameterized action space. In Proceedings of AAAI, pages\n3211–3218, 2018.\n[Xiong et al., 2018] Jiechao Xiong, Qing Wang, Zhuoran\nYang, Peter P Sun, Lei Han, Yang Zheng, Haobo Fu,\nTong Zhang, Ji Liu, and Hao Liu. Parametrized deep q-\nnetworks learning: Reinforcement learning with discrete-\ncontinuous hybrid action space. CoRR, abs/1810.06394,\n2018.\n[Ye et al., 2015] Dayong Ye, Minjie Zhang, and Yun Yang.\nA multi-agent framework for packet routing in wireless\nsensor networks. Sensors, 15(5):10026–10047, 2015.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.MA",
    "stat.ML"
  ],
  "published": "2019-03-12",
  "updated": "2019-03-12"
}