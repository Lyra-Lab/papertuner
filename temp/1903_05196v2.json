{
  "id": "http://arxiv.org/abs/1903.05196v2",
  "title": "A Review of Reinforcement Learning for Autonomous Building Energy Management",
  "authors": [
    "Karl Mason",
    "Santiago Grijalva"
  ],
  "abstract": "The area of building energy management has received a significant amount of\ninterest in recent years. This area is concerned with combining advancements in\nsensor technologies, communications and advanced control algorithms to optimize\nenergy utilization. Reinforcement learning is one of the most prominent machine\nlearning algorithms used for control problems and has had many successful\napplications in the area of building energy management. This research gives a\ncomprehensive review of the literature relating to the application of\nreinforcement learning to developing autonomous building energy management\nsystems. The main direction for future research and challenges in reinforcement\nlearning are also outlined.",
  "text": "A REVIEW OF REINFORCEMENT LEARNING FOR AUTONOMOUS\nBUILDING ENERGY MANAGEMENT\nKarl Mason\nSchool of Electrical and Computer Engineering\nGeorgia Institute of Technology\nAtlanta, GA, USA\nkmason35@gatech.edu\nSantiago Grijalva\nSchool of Electrical and Computer Engineering\nGeorgia Institute of Technology\nAtlanta, GA, USA\nsgrijalva@ece.gatech.edu\nABSTRACT\nThe area of building energy management has received a signiﬁcant amount of interest in recent years.\nThis area is concerned with combining advancements in sensor technologies, communications and\nadvanced control algorithms to optimize energy utilization. Reinforcement learning is one of the\nmost prominent machine learning algorithms used for control problems and has had many successful\napplications in the area of building energy management. This research gives a comprehensive review\nof the literature relating to the application of reinforcement learning to developing autonomous\nbuilding energy management systems. The main direction for future research and challenges in\nreinforcement learning are also outlined.\nKeywords reinforcement learning · building energy management · smart homes · smart grid · deep learning · machine\nlearning\n1\nIntroduction\nMeeting the global energy needs in an efﬁcient and sustainable manner is one of the most pressing issues facing society\ntoday. The energy consumption from buildings accounts for approximately 40% of the total global energy consumption\n[1]. It is therefore imperative to ﬁnd innovative ways to help reduce and optimize the energy consumed by buildings.\nSome of the main sources of energy consumption in buildings include: Heating Ventilation and Air Conditioning\n(HVAC), water heating, and lighting. Reducing the energy consumed by buildings has many beneﬁts. The consumer is\nbeneﬁted with lower energy bills. Electricity providers beneﬁt from reduced peak loads that must be met. There is also\na wider beneﬁt to society as a whole from reducing building energy consumption and reduction of emissions. Much\nof the energy that powers the grid comes from greenhouse gas emitting sources, e.g. coal. Therefore, a reduction in\nbuilding energy consumption would also correspond to a reduction in the emission of greenhouse gasses such as carbon\ndioxide.\nThe prevalence of digital systems in every aspect of modern society is largely due to the steady reduction in the cost and\nsize of micro-processors, along with their increased computing power. The widespread availability of computing has\nmanifested itself in the area of building energy consumption in the form of advanced sensors and actuators, and building\nenergy management systems [2]. These systems consist of monitoring and controlling the activity of each device within\nthe building with the aim of reducing the demand placed onto the grid and the cost to the consumer. The difﬁculty of\nthis management process is increased with the addition of photovoltaic (PV) panels, batteries, electric vehicles, and\nsmart appliances. Building energy management systems are comprised of many components. Sensors are needed to\nmonitor the building, e.g. temperature, humidity and load sensors. Communication is needed between devices in order\nto facilitate the monitoring and scheduling of their operation. This communication between devices is an example of\nthe “Internet of Things” (IoT). Smart meters are needed to record energy consumption and to communicate with energy\nproviders. Many devices are equipped with rudimentary control mechanisms such as rule-based decision systems.\nFor example, the thermostat that controls a HVAC system turns heating and cooling on/off when the temperature of\nthe room crosses a user-deﬁned threshold. This paper will provide an overview of how more sophisticated machine\narXiv:1903.05196v2  [cs.LG]  15 Mar 2019\nlearning control systems, in particular Reinforcement Learning (RL), have improved the operation of building energy\nmanagement systems.\nReinforcement Learning (RL) is a form of machine learning that consists of an agent interacting in an environment,\nlearning what actions to take depending on the state of the environment [3]. The agent learns by trial and error and is\nrewarded for taking desirable actions. The environment is often modelled as a Markov Decision Process (MDP). RL\nalgorithms date back to the 1970s and 1980s. One of the more popular RL algorithms, Q Learning, was ﬁrst proposed\nin 1989 [4]. These algorithms have been applied to a wide range of problems over the years, everything from trafﬁc\nlight control [5] to watershed management [6]. The combination of RL algorithms with deep neural networks has\nsigniﬁcantly increased the effectiveness of RL methods and has enabled them to be applied to tasks involving computer\nvisions, e.g. self-driving cars. Recent notable achievements of RL have resulted in a signiﬁcant amount of research into\nRL and its applications. These achievements include mastering the game of Go and playing Atari games.\nMany researchers have since begun applying reinforcement learning to some of the challenging areas of building energy\nmanagement. RL has been applied to tasks such as HVAC control, water heater control, electric vehicle charging,\nlighting control and appliance scheduling. An advantage of applying RL to address these problems is that the algorithm\nlearns by itself what the best control policy is. When implementing more traditional rule-based approaches, the designer\nmust handcraft the thresholds that the system will adhere to. This naïve approach is not necessarily able to minimize\nenergy consumption to the extent that RL can. There are several factors that increase the complexity of applying RL to\nthese problems, such as identifying what state information is needed, conﬂicting objectives and simulator design. These\nwill be discussed in detail throughout the paper. The main contributions of this research are:\n1. To give a comprehensive review of the literature relating to the application of RL to building energy manage-\nment.\n2. To quantify the impact of RL on building energy management systems in terms of energy savings.\n3. To establish the limitations of RL for building energy management and outline potential areas of future\nresearch.\nThe outline of the paper is as follows. Sections 2 will give outline the research area of building energy management. An\noverview of reinforcement learning will be provided in Section 3. Section 4 will give a comprehensive account of the\napplications of reinforcement learning within the area of building energy management. Section 5 will discuss some of\nthe limitations and potential areas for future work concerning the application of reinforcement learning to building\nenergy management. Finally, Section 6 will outline what can be concluded as a result of this research.\n2\nAutonomous Building Energy Management\nThe development of autonomous building energy management systems has received a lot of interest in recent years for\na number of reasons, one of the primary reasons being the drive to increase the energy efﬁciency in buildings through\noperational methods. As stated in the introduction, energy consumption in buildings accounts for a signiﬁcant portion\nof the total worldwide energy consumption [1]. There are a number of factors that inﬂuence how energy efﬁcient a\nbuilding is, e.g. insulation, construction materials and overall design. The aim of developing autonomous building\nenergy management systems is to further reduce both overall energy consumption and the cost of powering buildings by\nutilizing advancements in technologies such as:\n• Autonomous control systems\n• Internet of Things (IoT)\n• Batteries\n• Smart grid\n2.1\nHVAC\nHeating Ventilation and Air Conditioning (HVAC) is one of the most energy intensive consumers of energy in buildings.\nIt is estimated that HVAC is responsible for 50% of building energy consumption in the US and between 10 - 20% of\noverall consumption in developed countries [7]. This is due to expectation of thermal comfort in developed countries,\nrather than it being regarded as a luxury [8]. It is also well known that there is a strong correlation between outside\ntemperatures and HVAC energy consumption [9]. This energy consumption is expected to increase due to the increased\nnumber of extreme weather events observed globally [10]. The most basic HVAC control system would be a threshold-\nbased approach in which a thermostat is used to regulate the temperature. If the temperature crosses some predeﬁned\n2\nthreshold, the HVAC system activates to heat/cool the environment as required. There have been a wide number of\nstrategies proposed to control the operation of HVAC systems [11]. Section 4 will outline previous applications of RL\nto HVAC control.\n2.2\nLighting and Appliances\nThere are of course many other energy intensive devices within the buildings besides HVAC. Much of the energy\nconsumed in domestic dwellings comes from: lighting, televisions, water heaters, washing machines, dryers, fridge\nfreezers, etc. In order to reduce energy consumption of appliances, the design of these devices has been signiﬁcantly\nimproved in recent years. For example, some dryers are designed to have moisture sensors to prevent over drying. A\nsigniﬁcant amount of effort has focused on changing consumer behaviour, e.g. turning lights off when the room is not\nin use or turning off television sets when not in use. It is known that consumer behaviour plays a signiﬁcant role in\nenergy consumption [12]. Previous studies have shown that providing regular feedback to consumers of their energy\nconsumption can reduce their consumption by 15% on average [13]. Another crucial issue with regards to the energy\nconsumption of appliances is the variation in energy demand throughout the day. This affects the load demand placed on\nthe electrical grid. This will be discussed further in Section 2.5. This provides an opportunity for autonomous building\nenergy management systems to help reduce these peak demands by managing energy consumption, e.g. dimming lights\nat peak times. Autonomous control systems are now being developed to effectively schedule appliances to minimize\nenergy cost [14].\n2.3\nElectric Vehicles and Batteries\nThe automotive industry is also heavily inﬂuencing the behaviour of residential energy consumption due to the current\ntransition from fossil fuel powered vehicles to electric vehicles. The need to draw power from the grid in order to\ncharge electric vehicles signiﬁcantly increases the electrical load placed on the grid. Grahn et al. investigated the affect\nPlug in Hybrid Electric Vehicles (PHEVs) have on the domestic energy consumption in contrast to other appliances in\nthe home [15]. The authors of this study estimate that if all of the 4.3 million vehicles in Sweden were electric, this\nwould correspond to an additional 34 GWh of electricity consumption. This is approximately 10 % of Sweden’s daily\nelectricity consumption [15]. This is a signiﬁcant additional load for the grid to adapt to.\nVehicle to grid (V2G) technology is a promising solution to address this problem [16]. Vehicle to grid technology\nworks by selling electricity back to the grid when the car is not being used. A variation V2G is to consider the electric\nvehicle as a deferrable load. When the grid needs more power, charging of electric vehicle can stop for a few seconds\nor minutes. In this process there is no actual injection of power from the vehicle to the grid, but a pause in charging\nthat can help the grid. V2G also has the potential to alleviate some of the load on the grid during peak demand. This\nfurther motivates the need for effective building energy management systems. For example, a potentially effective\nstrategy would be to charge the vehicle overnight when electricity is cheapest and to sell energy back to the grid at\n7 pm when demand is high. Electric vehicles could also contribute to address the problem of incorporating variable\nrenewable energy into the grid. The variability of renewable sources is a challenge to incorporating technologies such\nas Photovoltaic (PV) into the grid [17]. This will be discussed further in Section 2.5. Battery technology in general\nis expected to play a major role in addressing some of the issues raised above [18]. The Tesla Powerwall is a prime\nexample of this [19]. Energy management systems have already been developed that utilize battery technology to help\nmanage the electrical demand [20].\n2.4\nSmart Meters and the Internet of Things\nSmart meters are a key development that will enable the large-scale deployment of autonomous building energy\nmanagement systems [21]. They enable communication with electricity providers that aids their operation. Smart\nmeters also enable the control of appliances within the home. Smart meters are already in widespread use. Italy and\nSweden were two of the ﬁrst to countries to have completed their deployment of smart meters nation-wide, with many\nother countries making signiﬁcant progress in their deployment [22].\nThe term “Internet of Things” or IoT, refers to a network of devices, appliances, sensors and electronics that can connect\nwith one another [23]. It is this communication between devices that enables building energy management systems to\noperate.\n2.5\nElectrical Grid and Renewables\nOne of the main challenges when developing autonomous building control systems is integrating it with the electrical\ngrid. These systems need to schedule their operation in order to minimize the cost of electricity to the consumer. This\n3\nis further complicated with many individuals installing PV solar panels, as previously discussed. This gives rise to\nthe recent paradigm of “prosumers” (producers and consumers) [24]. Within the prosumer paradigm, individuals both\nconsume energy from the grid and produce energy which is sold to the grid. This creates issues of instability within the\ngrid which must be addressed [25]. This is due to feeders with high levels of PV penetration producing an excess of\nelectricity that the grid struggles to cope with [26].\nAnother factor which increases the complexity of implementing autonomous building control systems is the price\nof energy [27]. Electricity prices vary based on demand and are therefore considered by many control systems [28].\nWeather also has a signiﬁcant effect on energy consumption. As stated previously, energy consumption via HVAC\nincreases signiﬁcantly with both exceedingly high and low outside weather temperatures [9]. In addition to this, sunny\nweather results in a higher output from PV systems, this further exacerbates the problems mentioned relating to grid\ninstabilities [25].\nThe next section will outline Reinforcement learning, a popular machine learning method that has been applied to many\nproblems relating to smart grid, building energy management, and smart homes.\n3\nReinforcement Learning\nReinforcement Learning (RL) is a subgroup of machine learning research that involves an agent learning by itself what\nactions to take in an environment so that it maximizes some reward [3]. This typically involves a signiﬁcant amount of\ntrial and error from the agent as it learns what actions result in the highest reward. Figure 1 illustrates this interaction\nwith its environment. Algorithm 1 presents a generic pseudocode, outlining the broad steps taken in a typical RL\nalgorithm. The agent interacts with its environment in discrete time steps. An agent typically has a policy (π) which\ndetermines what actions it will take. The goal is then to ﬁnd the optimum policy π* ∈Π, where Π is the set of possible\npolicies. The value of a policy π in a given state s is calculated using the value function in Equation 1.\nV π(s) = E[Σ∞\nk=0γkrt+k+1 | s, π]\n(1)\nWhere E is the expected future return and γ is the discount factor. In order for the agent to assess the value of its\ncurrent state, it must also consider the expected future rewards it will obtain if it follows its current policy. Equation 1\nquantiﬁes this mathematically. The discount factor γ determines how much weighting the agent gives to future rewards,\nas discussed in Section 3.1. The value function of state s for the optimum policy π∗is stated in Equation 2, calculated\nusing the Bellman equation.\nV π∗(s) = maxE[rt+1 + γV π∗(st+1) | s, π∗]\n(2)\nFigure 1: Agent Environment Interaction\nOne of the ﬁrst signiﬁcant achievements of RL was its application to learn to play backgammon in 1995 by Gerald\nTesauro [29]. Some of the more recent achievements include learning to play Atari games at a level comparable to\n4\nInitialize Agent\nwhile Episode e <Emax do\nInitialize Environment\nfor Time t = 1 to T do\nObserve state st\nSelect action a ∈A\nObserve reward r and new state s′\nUpdate policy π\nTransition to new state st+1\nend\nend\nAlgorithm 1: Generic Reinforcement Learning Algorithm\nan expert human player [30] in 2015 and mastering the game of GO [31] in 2016 by beating the world champion Lee\nSedol. This is by no means a comprehensive list of the many achievements of RL algorithms. RL has been successfully\napplied to a vast range of problems including building energy management, which will be discussed later.\n3.1\nProblem Types\nThe most common way to model an RL problem is as a Markov Decision Process (MDP) [3]. A MDP is a discrete\ntime framework for modelling decision making. A MDP is deﬁned as a tuple ⟨S, A, T, R, γ⟩, where S is the set of\nstates the environment can be, A is the set of possible actions, T = Pra(s, s′) = Pr(st+1 = s′ | st = s, at = a) is\nthe probability that taking action a in state s will lead to state s′ at time t+1, and ﬁnally R = Ra(s, s′) is the reward\nreceived after transitioning from state s to s′. RL agents account for future rewards using a discount factor γ ∈[0, 1], a\nhigher value makes the agent more forward thinking.\nAn extension to the standard MDP is the Partially Observable Markov Decision Process (POMDP) [32]. In POMDPs it\nis assumed that the underlying problem is a MDP, however the agent does not have complete knowledge of the state of\nthe environment. A POMDP is deﬁned as the tuple ⟨S, A, T, R, Ω, O, γ⟩. Here Ωand O are the set of observations and\nthe set of conditional observation probabilities respectively. When the environment transitions to state s′, the agent\nreceives the observation o ∈Ωwith probability O(o | s′, a).\nFor problems that require multiple agents, RL is often combined with Multi-Agent Systems (MAS) [33]. This is referred\nto as Multi-Agent Reinforcement Learning (MARL) [34]. One of the key challenges of MARL is to ensure that agents\ncoordinate their actions to achieve a global optimal result. This can be difﬁcult to achieve in practice as each agent\nmay only have partial information of the environment, making it a decentralized problem. These sorts of problems\ncan be modeled as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) [35]. Dec-POMDPs\nare deﬁned by the tuple ⟨I, S, A, T, R, Ω, O, h⟩. In this case, I is the set of agents, A = ×Ai is the joint action space,\nΩ= ×Ωi is the joint observation space and ﬁnally h is the horizon.\nAnother type of RL problem domain are problems that consist of multiple tasks, i.e. Multi Task Learning (MTL). In\nthis type of problem, each task Tj has a tuple ⟨D, Tj, Rj, Oj⟩where D is the underlying domain, Tj is the task speciﬁc\ntransition, Rj is the task speciﬁc reward and Oj is the task speciﬁc observation. This is closely related to Transfer\nLearning (TL), in which the goal is to transfer knowledge learned when solving one problem to solve a different\nproblem [36]. The distinction being that in MTL, the tasks are variants of the same underlying problem.\nMany RL problems have multiple objectives. These problems are referred to as Multi-Objective RL (MORL) problems\n[37]. In MORL, the MDP is extended to be a Multi-Objective MDP (MOMDP) where the agent receives a vector\nreward signal R(s, a) = (R1(s, a), R2(s, a), ..., Rm(s, a)) where m is the number of objectives. The goal here is to\nlearn multiple policies simultaneously for each objective [38, 39].\nThe task of learning, itself can be viewed as a learning problem. This is referred to as Meta learning [40]. Meta learning\nhas been applied to RL algorithms. For example the RL2 algorithm consists of a fast and slow RL algorithm [41]. The\nfast RL learner learns how to complete the task while the slow RL learner learns the optimum parameters of the fast\nlearner.\nMulti-Agent RL problems can also be either cooperative or competitive [42]. In cooperative MARL, each agents’\nobjective aligns with one another either in the form of a shared goal or goals that do not conﬂict with one another. In\ncompetitive environments, agents have conﬂicting objectives whereby a gain for one agent results in a loss for another.\n5\n3.2\nModel-Free vs Model-Based\nRL algorithms can be subcategorized into two groups, model-based and model-free algorithms. In model-based\nalgorithms, the agent learns a model of the environment based on its observing how the state of the environment changes\nwhen certain actions are taken. These observations are used to estimate the environments state transition function\nT(s′ | s, a) and reward R. Once the algorithm learns a model of the environment, it can be combined with a planning\nalgorithm to decide what actions to take [43]. Examples of model-based algorithms include: Explicit Explore and\nExploit (E 3) [44], Prioritized sweeping [45], Dyna [46] and Queue-Dyna [47].\nModel-free approaches do not need to develop a model of the environment. Instead model-free approaches learn a\npolicy, via trial and error, with the aim of approximating the optimum policy. Many of the routinely used RL algorithms\nare model-free, e.g. Q Learning [4] and SARSA [48]. Model-free approaches are more popular in the literature as they\nare generally less computationally expensive. Model-based approaches ﬁrst require building an accurate model of the\nproblem, which can be difﬁcult. Once an accurate model of the environment is obtained, the algorithm must ﬁnd the\noptimum policy by planning ahead.\n3.3\nDiscrete and Continuous Search Spaces\nMany RL algorithms, such as Q learning [4], represent the values of each state action pair in a look up table (a Q table\nfor Q learning). This approach works well for problems where the state action space is small. However, a fundamental\nlimitation with the tabular approach for representing Q values is its scalability. It’s clear that when the number of states\nand actions increase, the size of the table quickly becomes exceedingly large. The state action space becomes inﬁnite\nfor problems with continuous variables. The widely adopted solution to this is to replace the Q table with a function\napproximator, most commonly a neural network. When implementing a neural network to represent the Q function, the\nnetwork reads in the state of the environment as input. The output of the network is the Q value for each action. This\nfunction approximation allows RL algorithms to deal with exceedingly large state spaces such as those with images.\nThis is the approach taken for all of the recent work in deep RL and Atari games [30]. The image in Figure 2 illustrates\na deep neural network that can be used as a function approximator for RL algorithms.\nFigure 2: Neural Network Function Approximator [49]\nFunction approximation works well for problems with continuous state spaces. This approach still presents a problem\nfor domains with continuous action spaces. One approach would be to discretize the action space, however this will\nlimit the performance of the RL algorithm. A number of RL algorithms have been proposed to address problems with\ncontinuous action variables, including: Deep Deterministic Policy Gradient (DDPG) [50] and Sequential Monte Carlo\n(SMC) [51].\n3.4\nQ Learning\nOne of the most prominent RL algorithms is Q Learning [4]. The Q Learning algorithm has the properties of being\nan off-policy and model-free reinforcement learning method. Off-policy refers to an agent that learns the value of its\npolicy independent of its actions [52]. When applied to discrete state action spaces, the agents policy is determined by\nits Q table, a matrix where Q = S × A. These Q values are used to select what action to take in a given state. Once the\n6\nagent takes an action (a) for the current environmental state (s), the agent receives a reward (r). The agent then updates\nit’s Q table based on Equation 3.\nQ(st, at) ←Q(st, at) + α[r + γ max\nat Q(st+1, at) −Q(st, at)]\n(3)\nWhere α ∈[0, 1] is the learning rate and γ ∈[0, 1] is the discount factor. It is also common to implement Q Learning\nwith an ϵ-greedy selection policy where a random action is selected with probability ϵ ∈[0, 1]. The purpose of this is to\nencourage exploration. Another option is to use a softmax action selection policy, outlined in Section 3.6.\n3.5\nSARSA\nAnother popular RL algorithm is SARSA (State Action Reward State Action) [48]. As is the case with Q Learning,\nSARSA is a model-free learning algorithm. Unlike Q Learning, SARSA is an on-policy learning approach meaning\nthat it estimates the value of its policy at a given time while using that particular policy. SARSA updates its Q values\nusing Equation 4.\nQ(st, at) ←Q(st, at) + α[r + γQ(st+1, at+1) −Q(st, at)]\n(4)\nWhere each parameter has the same meaning as in Equation 3.\n3.6\nActor Critic\nActor Critic (AC) is a RL algorithm that consists of two agents: an actor and a critic. The actor makes decisions based\non its observations of the environment and current policy. The purpose of the critic is to observe both the state of the\nenvironment and the reward obtained from the environment based on the actor’s decision. The critic then gives feedback\nto the actor [53]. Actor critic is an on-policy and model-free RL algorithm.\nIn actor critic learning, the actor takes an action a. This results in a Temporal Difference (TD) error, calculated using\nEquation 5.\nδt = rt+1 + γV (st+1) −V (st)\n(5)\nWhere V is the current estimate of the state value. The critic then updates its estimate of the value function using\nEquation 6.\nV (st) ←V (st) + αδt\n(6)\nWhere α is the learning rate. The actor also uses the TD error to update its preference (p) to select that action again via\nEquation 7\np(st, at) ←p(st, at) + βδt\n(7)\nWhere β ∈[0, 1]. The actor then uses this preference in combination with a softmax selection method to choose its\naction. Its policy is outlined in Equation 8.\nπ(st, at) =\nexp( Q(s,a)\nτ\n)\nΣB\nb=1exp( Q(s,a)\nτ\n)\n(8)\nWhere τ is the temperature and is used to calibrate the Softmax selection policy. Lower τ values increase the likelihood\nof selecting the best estimated action while higher values make each action selection equally likely. As stated in Section\n3.4, ϵ-greedy is another popular action selection policy. While Figure 1 illustrates how an agent generally interacts with\nits environment, Figure 3 speciﬁcally illustrates how AC agents interact with their environment.\n3.7\nRecent Methods\nThe algorithms outlined in the previous sections are still routinely used in the literature. There have been some\nsigniﬁcant advances made in RL that have led to new and powerful techniques. The most signiﬁcant of these being the\ncombination of RL with deep neural networks as previously mentioned in Section 3.3. One of the more popular recent\nmethods is the Deep Q Network (DQN) algorithm [30]. DQN utilizes both an experience replay of past state action\npairs to evaluate policies and a periodic update for the target network. The parameters (θ) of the policy function (Q) are\n7\nFigure 3: Actor Critic Environment Interaction\nupdated by calculating the error between the policy network (Q) and the target network ( ˆQ) parameterised by θ−, as\nshown in Equation 9.\nLi(θi) = E(s,a,r,s′)∼U(D)(ri + γmaxa′ ˆQ(s′, a′; θ−\ni ) −Q(s, a; θi))2\n(9)\nWhere E(s,a,r,s′)∼U(D) is an episode (s, a, r, s′) sampled uniformly from the dataset D of previous experiences. The\nDQN algorithm was applied to a range of Atari games and found to exceed human level performance.\nAnother recent RL algorithm is the Asynchronous Advantage Actor Critic (A3C) algorithm [54]. As mentioned in\nSection 3.6, actor critic methods consists of a policy π(s) (actor) and a value function V (s) (critic). This algorithm\nutilizes multiple learners that interact with their environment at the same time. The “Advantage” portion of the\nA3C algorithm consists of estimating how much better an action was than expected, calculated as A = R −V (s).\nThe “Asynchronous” portion of A3C comes from the fact that each learner is running individually in parallel. This\ndramatically speeds up learning. Each learner is initialized to the global policy. They then interact with each individual\nenvironment and calculate the value and loss. This is then used to calculate the gradients for the policy (Equation 10)\nand value function (Equation 11).\ndθ ←dθ + ▽θ′logπ(ai|si; θ′)(R −V (si; θ′\nv))\n(10)\ndθv ←dθv + ∂(R −V (si; θ′\nv))2/∂θ′\nv\n(11)\nWhere θ and θv are the parameters of the global policy and value function, and θ′ and θ′\nv are the parameters of the\nthread speciﬁc individual policy and value function. These are used to asynchronously update the global θ and θv.\nDQN and A3C are two state of the art deep RL methods. The aim of this section was to give an overview of some\nrecent developments in RL and is by no means a comprehensive review of RL as this is outside of the scope of this\npaper. For further reading on the state of the art in RL see the work of Arulkumaran et al. [55]. The next section will\ngive a comprehensive account of the applications of RL to building energy management systems.\n4\nAutonomous Building Energy Management via Reinforcement Learning\nAn overview of both the problem of building energy management and RL was provided in Sections 2 and 3. This\nsection will now provide a comprehensive review of the literature that combines these two areas of research.\n4.1\nHVAC\nReinforcement learning has been successfully applied to many areas of building energy management, including HVAC\ncontrol. Typically, the environment states for the RL algorithm include factors such as: time of day, outdoor temperature,\nindoor temperature, weather forecast and occupancy. Typical RL actions include: temperature set points, air ﬂow\ncontrol, heating control and cooling control. The RL algorithm also needs some sort of reward to operate, all previous\nstudies in the literature calculate rewards based on energy cost, thermal comfort or a combination of both.\n8\nThe ﬁrst application in the literature of RL to HVAC control was in 1996. Anderson et al. applied Q learning in\nconjunction with a PI controller to modulate the output of the PI controller for a heating coil [56, 57]. This approach\nperformed better than the PI controller alone. A 2006 study applied a combined RL - Model Predictive Control (MPC)\napproach to control HVAC operation at the Energy Resource Station Laboratory building in Ankeny, Iowa [58]. It\nwas found that the combined approach performed better than either Q learning or MPC individually. An actor critic\n- neural network learning approach was applied to adjust the signal of a local control for HVAC control in 2008 by\nDu and Fei [59]. This study reports signiﬁcant improvements from a combined PID actor critic learning approach\nthan a stand-alone PID controller. Dalamagkidis and Kolokotsa implemented the Recursive Least-Squares Temporal\nDifference (RLS TD) for HVAC control in 2008 and reported better performance with an RL controlled HVAC system\nthan with a fuzzy PD controller [60]. Yu and Dexter implemented a Q(λ) learning approach with fuzzy discretization of\nthe state space variables to select rules that determine the operation of the HVAC system [61]. A 2013 study by Urieli\nand Stone apply Tree Search to control a HVAC heat pump [62]. The Tree Search method resulted in a 7 - 14% saving\nwhen compared to the standard rule-based control. In 2015 Barrett and Linder applied Q learning to the problem of\nHVAC control combined with Bayesian Learning for occupancy prediction [63]. The results outline a 10% energy\nsavings improvement over a programmable control method. This work was then extended to apply parallel Q learning to\nHVAC control [64]. Another 2015 study by Ruelens et al. implemented a combined Auto-Encoder (AE) Q learning to\ncontrol a heat pump for a HVAC system [65]. The authors reported energy savings in the range of 4 - 11%, comparable\nto the previous study.\nYang et al. implemented a Batch Q learning with a neural network to control a PV powered heating system [66]. The\nresults report a 10% improvement on a standard rule-based system. In 2017 Wei et al. utilized deep neural network and\napplied Deep RL (DQN) to the problem of HVAC control and report energy saving improvements over conventional\nQ learning in the range of 20 - 70% [67, 68]. Another 2017 study by Wang et al. applied a Monte Carlo Actor Critic\nwith Long Short Term Memory (LSTM) neural network to the task of HVAC control [69]. Their results indicate a 15%\nthermal comfort improvement and a 2.5% energy efﬁciency improvement when compared to other methods. More\nrecently, Marantos et al. proposed using a Neural Fitted Q-Iteration (NFQ) approach for HVAC control in 2018 [70].\nThis approach also used a neural network to approximate the Q function. The results of this study report signiﬁcant\nsavings in terms of both energy consumption and thermal comfort when compared to rule-based controllers. In 2018,\nZhang et al. applied Asynchronous Advantage Actor Critic (A3C) (a deep RL approach) to control the HVAC system\nfor a simulation of the Intelligent Workplace building in Pittsburgh, USA [71]. The authors report a 15% energy saving\nimprovement upon the base case. Chen et al. applied Q learning to control both the HVAC and window systems [72].\nThe authors report energy savings of 13 and 23% and lowered discomfort ratings by 62 and 80% in the two buildings\ntested. In a 2018 study, Patyn et al. compared the performance difference of Q learning on three different neural network\narchitectures (convolutional neural networks (CNN), LSTM networks and a feed forward multi-layer neural network)\nfor HVAC control [73]. Their results indicate that the LSTM and multi-layer network perform best however the LSTM\ntraining time was twice as long.\nThis section has outlined numerous successful applications of RL to HVAC control with a variety of RL algorithms.\nThe most common RL algorithm implemented was Q learning. The vast majority of studies report energy savings in the\nrange of ≈10% when compared to rule-based approaches. There is also a recent trend to apply deep RL to HVAC\ncontrol, which has outperformed traditional tabular RL. This trend is unsurprising given the recent success and attention\nthat deep learning has received.\n4.2\nWater Heater\nAs discussed in Section 2, water heaters consume a signiﬁcant amount of energy. This section will explore some of the\napplications of RL to control water heaters with the aim of reducing energy costs. Some of the state variables include:\ntime of day, current water temperature and forecast usage. The action that the RL agent makes is generally to turn the\nheater on or off. The reward given to the agent is the electricity consumption.\nA 2014 study by Al-Jabery et al. applied Q learning and a fuzzy Q learning to control a water heater [74]. Their results\nindicate that their proposed fuzzy Q learning algorithm gives smoother convergence than the standard Q learning.\nAl-Jabery et al. then explored applying an Actor Critic - Q learning approach to control a water heater in 2017, where\nthey included the grid load in the state space for the RL agent [75]. This study reports energy cost savings between\n6 and 26% using RL. In 2014 Ruelens et al. applied batch Q learning to control a cluster of 100 water heaters [76].\nThe authors reported that within 45 days, the RL algorithm reduced electricity costs when compared to a hysteresis\ncontroller. In a more recent 2018 study, Ruelens et al. applied Q iteration to a physical water heater in a laboratory\nsetting and found that Q iteration was able to reduce the energy consumption by 15% over 40 days when compared\nto a thermostat controller [77]. De Somer et al. applied Q learning to control the heating cycle of a domestic water\nheater to maximize the consumption from a local PV source. The authors then evaluate the Q learning agent on 6\n9\nresidential houses and report a 20% increase in the amount of local PV consumption [78]. Kazmi et al. did a similar\nstudy in the Netherlands where deep RL was applied to 32 homes [79]. The authors report a 20% reduction in the\nenergy consumption for water heating and no loss in user comfort.\nAlthough there are signiﬁcantly fewer studies in the literature applying RL to water heaters than HVAC. Based on the\npapers reviewed, there appears to be a signiﬁcant reduction in energy consumption when applying RL to water heaters.\nMany of the studies report reductions of ≈20% when compared to the baseline. A similar trend can be observed that\nrecent research is more focused on the application of deep RL as it has demonstrated its effectiveness.\n4.3\nHome Management Systems\nThe studies outlined in Sections 4.1 and 4.2 consider applying RL to HVAC and water heaters respectively in isolation.\nThis section explores applications of RL within the home to manage multiple appliances, lighting, PV, battery, etc.\nApplying RL in this manner is needed as the task of building energy management is a complex problem with multiple\nfactors that must be managed to reduce the overall energy consumption. Similar to the previously mentioned sections,\nthe states for the RL algorithm when exploring building energy management in a more holistic manner, generally\nconsist of the time of day, temperature information and the current usage state of the various appliances. Many of these\nstudies also include other state information such as electricity prices, grid load and information in relation to PV panels\nsuch as solar irradiance. The actions available to the RL agent in the studies outlined in this section are similar to those\noutlined in the previous sections, i.e. turning a device or appliance on or off. In the cases of studies which incorporate\nbatteries, the actions are to charge/discharge the battery or to do nothing.\nIn 2018, a study by Reymond et al. applied Fitted Q Iteration to learn to schedule a number of household appliances\nincluding a heat pump, water heater and dishwasher [80]. Their results indicate that independent learning performs\n9.65% better than a centralized learning approach. Wei et al. implemented a dual iterative Q Learning algorithm\nfor residential battery management [81]. The authors report a 32.16% savings in energy cost when compared to the\nbaseline. A 2015 study by Guan et al. utilize temporal difference learning to control the energy storage of a battery in\nthe presence of a PV panel [82]. It was found that temporal difference learning resulted in improvements of 59.8%\nreduction in energy costs. Wan et al. implemented actor critic learning with two deep neural networks as a residential\nenergy management system [83]. In this study, the RL algorithm learned when to purchase electricity from the utility\nprovider based on previous prices, house hold loads and current battery charge. The authors report an 11.38% reduction\nin energy cost by using actor critic learning when evaluated over 100 days. Remani et al. applied Q learning to schedule\nmultiple devices within the home including lighting, clothes dryer, dish washer, etc. [84]. The authors implemented a\nprice based demand response and included a PV panel in the system model and reported an approximate daily energy\ncost saving of 15%. Wen et al. proposed a demand response energy management systems for small buildings that\nenables automated device scheduling in response to electrical price ﬂuctuations [85]. The authors implement Q learning\nwith the aim of taking advantage of the estimated 65% of potential energy savings for small buildings by efﬁcient device\nscheduling and report improvements upon the baseline. Bazenkov and Goubko utilize inverse reinforcement learning\n(IRL) to predict consumer appliance usage and report a higher accuracy using IRL than other machine learning methods\nsuch as random forest [86]. A 2018 study by Mocanu et al. implement both Deep Q Learning (DQL) and Deep Policy\nGradients (DPG) to optimize the energy management system for 10, 20 and 48 houses [87]. This study explored the use\nof electric vehicles, PV panels and building appliances. The authors report electricity cost savings of 27.4 % for DPG\nand 14.1% for DQL.\nA combined approach was proposed by Wang et al. that implements Q learning to manage a battery in a residential\nsetting with installed PV [88]. Q learning was combined with a model-based control algorithm to minimize energy costs.\nThis hybrid approach demonstrated signiﬁcant cost savings in the range of 23 - 72%. A similar study was conducted by\nMbuwir et al. that implemented ﬁtted Q iteration for battery management in order to maximize the energy provided by\nPV [89]. This was simulated using Belgian consumer data and demonstrated a cost saving of 19%. Rayati et al. utilize\nQ learning for home energy management in a setting with PV installation and energy storage [90]. This study also\nconsiders home comfort and CO2 emissions when learning the optimal control policy. The authors report a maximum\nenergy saving of 40%, peak load reduction of 17% and CO2 social cost reduction of 50%. Sheikhi et al. implement Q\nlearning to control a smart energy hub that consists of a combined heat and power, auxiliary boiler, electricity storage\nand heating storage [91]. The authors report a saving of 30% and 50% for energy cost and peak loads respectively.\nThe results outlined in the literature relating to the application of RL to home energy management varies signiﬁcantly\nmore than in the previous two sections. The highest reported energy cost reduction was 72% when implementing\nRL with a model-based controller [88]. There is a greater capacity to generate savings when applying RL to home\nenergy management systems with batteries and solar than simply applying RL to devices. This is reﬂected in the studies\nmentioned here. In terms of algorithms, Q learning is again the most routinely used method.\n10\n4.4\nSmart Homes and the Electrical Grid\nWhile the studies in the previous sections have demonstrated the effectiveness of RL when applied within the home,\nmany studies in the literature examine how RL can also have a positive impact when considering how these smart\nhomes are incorporated into the grid. This section will outline the applications of RL that relate to smart homes and the\ngrid. Many of these studies evaluate control systems for a community of homes, rather than single homes in isolation,\nand therefore utilize multi-agent RL in many cases. There are many studies that focus on the application of RL to\nthe operation of a microgrid to address problems such as thermal generator scheduling and incorporating PV into a\nmicrogrid. These are outside of the scope of this paper and will therefore not be discussed. This section solely focuses\non studies that explore incorporating smart homes into the electrical grid. For a more general review of smart grid\ntechnologies, a comprehensive review is provided by Tuballa et al. [92].\nJiang et al. implemented a hierarchical multi-agent Q learning approach for dynamic demand response and distributed\nenergy resource management in a microgrid [93]. This microgrid also includes wind power and a battery. This\nstudy reports a 19% reduction in energy costs for the whole community. Kim et al. implemented a multi-agent Q\nlearning approach for residential energy consumption based on electricity prices in a microgrid [94]. Signiﬁcant\nsavings were reported over the baseline myopic purchasing strategy. A multi-agent RL approach was implemented\nby Anvari-Moghaddam et al. to integrate intelligent homes into the grid where batteries and renewables are also\nconsidered [95]. The authors’ proposed structure consisted of: a central coordinator agent, a building management\nagent, a renewables agent, a battery agent and a services agent. The authors report an energy cost saving of 5%. A\nrecent 2018 paper by Prasad and Dusparic implement a multi-agent deep RL approach to enable homes to share energy\nwith one another in order to minimize cost for the community as a whole [96]. The authors implemented a multi-agent\nDQN approach where each agent controlled a house. The agent could consume/store energy, request energy from or\ngrant energy to neighbours, deny energy to neighbours or purchase from the grid. Each home was had an installed\nbattery. The results indicate savings of 97 and 156kWh during the winter and summer respectively for a 10 home\ncommunity.\nLu et al. recently implemented Q learning for electricity pricing with the aim to schedule pricing to maximize the\nproviders proﬁt when purchasing from an electricity wholesaler and minimize customers cost [97]. A 2018 study by\nKoﬁnas et al. implemented fuzzy Q learning to control the operation of a battery and desalination plant in conjunction\nwith a PV panel for a microgrid that also includes residential electricity consumers [98]. Claessens et al. applied Q\nlearning with a convolutional neural network for residential load control [99]. It was found that the proposed approach\nprovided near optimum performance when compared to a theoretically optimal benchmark. In 2018, Kim and Lim\nutilized Q learning to learn when to charge and discharge a battery and when to buy and sell from the grid [100]. The\nauthors report signiﬁcant energy cost savings when compared to other methods.\nThe studies outlined in this section focus on how RL can reduce energy costs for multiple smart homes and how RL can\nhelp integrate these homes into the grid. As was observed in the previous sections, the studies here also demonstrate the\neffectiveness of RL for these problems. Multi-agent RL is a commonly used approach to learn policies for multiple\nhomes as it is well suited to the distributed nature of this control task. Based on these studies, electricity cost savings\ncan range from 5% - 19%. In terms of utilizing RL to balance residential loads with the grid, it was demonstrated that\nRL performs with near optimum performance and signiﬁcantly reduces electricity costs for the grid.\n5\nDiscussion\nA common theme across all of the various applications of RL within building energy management systems is that RL\ncan provide signiﬁcant savings in each problem it is applied to. As new and effective RL algorithms are developed,\nthese new methods gradually make their way into the smart homes literature, e.g. deep RL methods. This section will\noutline some of the limitations of RL for building energy management and also some directions for future research.\n5.1\nLimitations\nThere are however limitations of applying RL to building energy management systems. The overwhelming majority of\nstudies outlined in this paper discuss applications of RL to simulated versions of problems relating to building energy\nmanagement. While this is perfectly acceptable and a natural way to apply RL to these problems, this approach relies\nheavily on accurate simulator design and data that is representative of real-world scenarios. Since RL is an online\nlearning algorithm, it could be directly applied to a building energy management system for a physical building without\never learning within a simulated environment. The problem with this approach is that in order for the RL algorithm\nto learn an effective control policy that minimizes energy cost, it must learn by trial and error. This would result in\nan initial exploration period where the RL agent would evaluate different policies, many of which would have an\n11\nunacceptably high energy cost associated with it. Accurate simulators are needed for this reason, so that the RL agent\ncan learn in simulation which policies are best. These pre-trained agents can then further reﬁne their policies when\nimplemented in physical systems. This is a viable approach as was demonstrated by the application of RL to the HVAC\noperation at the Energy Resource Station Laboratory building in Ankeny, Iowa [58].\n5.2\nFuture Directions\nThe ﬁrst and most obvious trend that will be experienced relating to the application of RL to building energy management\nis the move towards deep RL algorithms. These deep RL methods are capable of learning more complex policies\nthat are more sophisticated than those represented by shallow neural network or look up tables. As the volume of\ndata collected by sensors will only increase, deep RL methods will become necessary to develop effective policies\nwhen interacting in environments with very large state action spaces. It is worth noting here that increasingly powerful\ncomputers will also enable researchers to train RL agents with increasingly complex policies.\nAnother potential route for future research in RL for building energy management is to apply many of the variants of\ntraditional RL, some of which were outlined in Section 3.1. For example, based on our extensive literature review, there\nare no examples within the literature that apply multi task RL to building energy management. Multi-task learning\ncould be potentially useful for learning different HVAC control policies depending on how many occupants are in the\nhouse. Transfer learning could be useful for transferring knowledge learnt when learning to schedule appliances to\nscheduling water heaters.\nSome of the problems relating to building energy management can be framed as multi-objective RL problems. The\nmost obvious example of this is the trade of between thermal comfort and energy cost. There are numerous examples in\nthe literature that apply multi-objective optimization algorithms to address this problem, however none of these studies\nutilize any state of the art multi-objective RL algorithms, e.g. Pareto Q learning [37].\nAn additional avenue for future research would be the application of meta learning to RL problems in building energy\nmanagement, e.g. the RL2 algorithm mentioned in Section 3.1 [41]. There are no applications of meta reinforcement\nlearning to building energy management. Methods such as RL2 could be useful for learning more effective control\npolicies for the various within the studies outlined in this paper.\nIn terms of the tasks that RL is applied to, many studies in the literature are beginning to explore implementing\nmulti-agent RL to control a community of homes rather than a single home in isolation. Developing control policies\nfor groups of homes has the added beneﬁt of allowing homes to pool their resources in a manner that results in\ngreater savings for everyone. A natural route for this research to follow into the future would be to apply multi-agent\nRL to larger communities of homes to explore the scalability of multi-agent RL in this domain. Conducting game\ntheory analysis in this area would also be a promising area of research. Decentralized multi-agent control policies for\nlarge communities of homes would rely heavily on cooperation to achieve cost savings for all, it would therefore be\nworthwhile to explore the issue of cooperation and defection in such systems.\nBased on the extensive literature review conducted, all of the current research relating to applications of RL in building\nenergy management evaluates systems in which the environment does not signiﬁcantly change outside of its normal\nenvironmental conditions. This raises the question of how well do these learned control policies perform when faced\nwith signiﬁcant environmental changes, e.g. extreme weather events, battery degradation or failure, PV panal installation\nor failure, homes where the number of occupants increases or decreases, etc. It is not clear based on the current research,\nhow the learned control policies can adapt to these signiﬁcant environmental changes, i.e. are the learned policies brittle\nand would result in catastrophic failure with sub optimal performance in the event of these changes? If the RL agent is\nrobust and capable of adapting to such environmental changes, how long would it take for the RL agent to relearn new\npolicies in such scenarios? These questions would be worth investigating in depth in future studies.\n6\nConclusion\nThis paper has discussed the key ideas behind developing building energy management systems, given a brief introduc-\ntion into reinforcement learning and provided a comprehensive account of the applications of reinforcement learning to\nbuilding energy management. As a result of this literature review, it is clear that:\n1. Reinforcement learning algorithms signiﬁcantly improve the energy efﬁciency of homes. Energy savings vary\nsigniﬁcantly depending on the speciﬁc application. Broadly speaking RL can typically provide savings of\n≈10% for HVAC applications, ≈20% for water heaters and > 20% for more complete buildings energy\nmanagement systems.\n12\n2. The vast majority of studies that implement RL for problems relating to building energy management are\nin simulation only. With accurate simulator design however, RL is a viable approach for building energy\nmanagement outside of simulation.\n3. Much of the recent research for RL and building energy management is focused on applications on deep RL\nalgorithms due to their increased effectiveness over traditional approaches. This is likely to become more\npronounced into the future.\nReferences\n[1] Payam Nejat, Fatemeh Jomehzadeh, Mohammad Mahdi Taheri, Mohammad Gohari, and Muhd Zaimi Abd\nMajid. A global review of energy consumption, co2 emissions and policy in the residential sector (with an\noverview of the top ten co2 emitting countries). Renewable and sustainable energy reviews, 43:843–862, 2015.\n[2] Bin Zhou, Wentao Li, Ka Wing Chan, Yijia Cao, Yonghong Kuang, Xi Liu, and Xiong Wang. Smart home\nenergy management systems: Concept, conﬁgurations, and scheduling strategies. Renewable and Sustainable\nEnergy Reviews, 61:30–40, 2016.\n[3] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\n[4] Christopher John Cornish Hellaby Watkins. Learning from Delayed Rewards. PhD thesis, King’s College,\nCambridge, UK, 1989.\n[5] Patrick Mannion, Jim Duggan, and Enda Howley. An experimental review of reinforcement learning algorithms\nfor adaptive trafﬁc signal control. In Autonomic Road Transport Support Systems, pages 47–66. Springer, 2016.\n[6] Karl Mason, Patrick Mannion, Jim Duggan, and Enda Howley. Applying multi-agent reinforcement learning to\nwatershed management. In Proceedings of the Adaptive and Learning Agents workshop (at AAMAS 2016), 2016.\n[7] Luis Pérez-Lombard, José Ortiz, and Christine Pout. A review on buildings energy consumption information.\nEnergy and buildings, 40(3):394–398, 2008.\n[8] Liu Yang, Haiyan Yan, and Joseph C Lam. Thermal comfort and building energy consumption implications–a\nreview. Applied energy, 115:164–173, 2014.\n[9] Danny HW Li, Liu Yang, and Joseph C Lam. Impact of climate change on energy use in the built environment in\ndifferent climate zones–a review. Energy, 42(1):103–112, 2012.\n[10] Dim Coumou and Stefan Rahmstorf. A decade of weather extremes. Nature climate change, 2(7):491, 2012.\n[11] Abdul Afram and Farrokh Janabi-Shariﬁ. Theory and applications of hvac control systems–a review of model\npredictive control (mpc). Building and Environment, 72:343–355, 2014.\n[12] Kirsten Gram-Hanssen. Efﬁcient technologies or user behaviour, which is the more important when reducing\nhouseholds’ energy consumption? Energy Efﬁciency, 6(3):447–457, 2013.\n[13] Georgina Wood and Marcus Newborough. Dynamic energy-consumption indicators for domestic appliances:\nenvironment, behaviour and design. Energy and buildings, 35(8):821–841, 2003.\n[14] Christopher O Adika and Lingfeng Wang. Autonomous appliance scheduling for household energy management.\nIEEE transactions on smart grid, 5(2):673–682, 2014.\n[15] Pia Grahn, Joakim Munkhammar, Joakim Widén, Karin Alvehag, and Lennart Söder. Phev home-charging model\nbased on residential activity patterns. IEEE Transactions on power Systems, 28(3):2507–2515, 2013.\n[16] Kang Miao Tan, Vigna K Ramachandaramurthy, and Jia Ying Yong. Integration of electric vehicles in smart\ngrid: A review on vehicle to grid technologies and optimization techniques. Renewable and Sustainable Energy\nReviews, 53:720–732, 2016.\n[17] Willett Kempton and Jasna Tomi´c. Vehicle-to-grid power implementation: From stabilizing the grid to supporting\nlarge-scale renewable energy. Journal of power sources, 144(1):280–294, 2005.\n[18] Bruce Dunn, Haresh Kamath, and Jean-Marie Tarascon. Electrical energy storage for the grid: a battery of\nchoices. Science, 334(6058):928–935, 2011.\n[19] Cong Nam Truong, Maik Naumann, Ralph Ch Karl, Marcus Müller, Andreas Jossen, and Holger C Hesse.\nEconomics of residential photovoltaic battery systems in germany: The case of tesla’s powerwall. Batteries,\n2(2):14, 2016.\n[20] Abdallah Tani, Mamadou Baïlo Camara, and Brayima Dakyo. Energy management in the decentralized generation\nsystems based on renewable energy—ultracapacitors and battery to compensate the wind/load power ﬂuctuations.\nIEEE Transactions on Industry Applications, 51(2):1817–1827, 2015.\n13\n[21] Soma Shekara Sreenadh Reddy Depuru, Lingfeng Wang, Vijay Devabhaktuni, and Nikhil Gudi. Smart meters\nfor power grid—challenges, issues, advantages and status. In 2011 IEEE/PES Power Systems Conference and\nExposition, pages 1–7. IEEE, 2011.\n[22] Noelia Uribe-Pérez, Luis Hernández, David de la Vega, and Itziar Angulo. State of the art and trends review of\nsmart metering in electricity grids. Applied Sciences, 6(3):68, 2016.\n[23] Biljana L Risteska Stojkoska and Kire V Trivodaliev. A review of internet of things for smart home: Challenges\nand solutions. Journal of Cleaner Production, 140:1454–1464, 2017.\n[24] Santiago Grijalva, Mitch Costley, and Nathan Ainsworth. Prosumer-based control architecture for the future\nelectricity grid. In Control Applications (CCA), 2011 IEEE International Conference on, pages 43–48. IEEE,\n2011.\n[25] M Honarvar Nazari, Zak Costello, Mohammad Javad Feizollahi, Santiago Grijalva, and Magnus Egerstedt.\nDistributed frequency control of prosumer-based electric energy systems. IEEE Transactions on Power Systems,\n29(6):2934–2942, 2014.\n[26] Matthew J Reno, Robert J Broderick, and Santiago Grijalva. Smart inverter capabilities for mitigating over-\nvoltage on distribution systems with high penetrations of pv. In Photovoltaic Specialists Conference (PVSC),\n2013 IEEE 39th, pages 3153–3158. IEEE, 2013.\n[27] Qinran Hu and Fangxing Li. Hardware design of smart home energy management system with dynamic price\nresponse. IEEE Transactions on Smart grid, 4(4):1878–1887, 2013.\n[28] Frauke Oldewurtel, Andreas Ulbig, Alessandra Parisio, Göran Andersson, and Manfred Morari. Reducing peak\nelectricity demand in building climate control using real-time pricing and model predictive control. In CDC,\npages 1927–1932, 2010.\n[29] Gerald Tesauro. Temporal difference learning and td-gammon. Communications of the ACM, 38(3):58–68, 1995.\n[30] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex\nGraves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep\nreinforcement learning. Nature, 518(7540):529, 2015.\n[31] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian\nSchrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with\ndeep neural networks and tree search. nature, 529(7587):484, 2016.\n[32] Matthijs TJ Spaan. Partially observable markov decision processes. In Reinforcement Learning, pages 387–414.\nSpringer, 2012.\n[33] Michael Wooldridge. An introduction to multiagent systems. John Wiley & Sons, 2009.\n[34] Lucian Busoniu, Robert Babuška, and Bart De Schutter. Multi-agent reinforcement learning: An overview.\nInnovations in multi-agent systems and applications-1, 310:183–221, 2010.\n[35] Christopher Amato, Girish Chowdhary, Alborz Geramifard, N Kemal Ure, and Mykel J Kochenderfer. Decentral-\nized control of partially observable markov decision processes. In Decision and Control (CDC), 2013 IEEE\n52nd Annual Conference on, pages 2398–2405. IEEE, 2013.\n[36] Matthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey. Journal of\nMachine Learning Research, 10(Jul):1633–1685, 2009.\n[37] Kristof Van Moffaert and Ann Nowé. Multi-objective reinforcement learning using sets of pareto dominating\npolicies. The Journal of Machine Learning Research, 15(1):3483–3512, 2014.\n[38] Patrick Mannion, Sam Devlin, Karl Mason, Jim Duggan, and Enda Howley. Policy invariance under reward\ntransformations for multi-objective reinforcement learning. Neurocomputing, 263:60–73, 2017.\n[39] Patrick Mannion, Karl Mason, Sam Devlin, Jim Duggan, and Enda Howley. Dynamic economic emissions\ndispatch optimisation using multi-agent reinforcement learning. In Proceedings of the Adaptive and Learning\nAgents workshop (at AAMAS 2016), 2016.\n[40] Christiane Lemke, Marcin Budka, and Bogdan Gabrys. Metalearning: a survey of trends and technologies.\nArtiﬁcial intelligence review, 44(1):117–130, 2015.\n[41] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl 2: Fast reinforcement\nlearning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.\n[42] Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Korjus, Juhan Aru, Jaan Aru, and Raul\nVicente. Multiagent cooperation and competition with deep reinforcement learning. PloS one, 12(4):e0172395,\n2017.\n14\n[43] Soumya Ray and Prasad Tadepalli. Model-based reinforcement learning. Encyclopedia of Machine Learning,\npages 690–693, 2010.\n[44] Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Machine learning,\n49(2-3):209–232, 2002.\n[45] Andrew W Moore and Christopher G Atkeson. Prioritized sweeping: Reinforcement learning with less data and\nless time. Machine learning, 13(1):103–130, 1993.\n[46] Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM SIGART Bulletin,\n2(4):160–163, 1991.\n[47] Jing Peng and Ronald J Williams. Efﬁcient learning and planning within the dyna framework. Adaptive Behavior,\n1(4):437–454, 1993.\n[48] Gavin A Rummery and Mahesan Niranjan.\nOn-line Q-learning using connectionist systems, volume 37.\nUniversity of Cambridge, Department of Engineering Cambridge, England, 1994.\n[49] Karl Mason. Advances in evolutionary neural networks with applications in energy systems and the environment.\nPhD thesis, NUI Galway, 2018.\n[50] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver,\nand Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971,\n2015.\n[51] Alessandro Lazaric, Marcello Restelli, and Andrea Bonarini. Reinforcement learning in continuous action spaces\nthrough sequential monte carlo methods. In Advances in neural information processing systems, pages 833–840,\n2008.\n[52] Christopher J.C.H. Watkins and Peter Dayan. Technical note: Q-learning. Machine Learning, 8(3-4):279–292,\n1992.\n[53] GR Gajjar, SA Khaparde, P Nagaraju, and SA Soman. Application of actor-critic learning algorithm for optimal\nbidding problem of a genco. IEEE Transactions on Power Systems, 18(1):11–18, 2003.\n[54] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David\nSilver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International\nconference on machine learning, pages 1928–1937, 2016.\n[55] Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath. A brief survey of deep\nreinforcement learning. arXiv preprint arXiv:1708.05866, 2017.\n[56] Charles W Anderson, Douglas C Hittle, Alon D Katz, and RM Kretchman. Reinforcement learning, neural\nnetworks and pi control applied to a heating coil. In Proc. of the Int. Conf. EANN, volume 96, pages 135–142,\n1996.\n[57] Charles W Anderson, Douglas C Hittle, Alon D Katz, and R Matt Kretchmar. Synthesis of reinforcement\nlearning, neural networks and pi control applied to a simulated heating coil. Artiﬁcial Intelligence in Engineering,\n11(4):421–429, 1997.\n[58] Simeng Liu and Gregor P Henze. Experimental analysis of simulated reinforcement learning control for active and\npassive building thermal storage inventory: Part 2: Results and analysis. Energy and buildings, 38(2):148–161,\n2006.\n[59] Dajun Du and Minrui Fei. A two-layer networked learning control system using actor–critic neural network.\nApplied mathematics and computation, 205(1):26–36, 2008.\n[60] Konstantinos Dalamagkidis and Dionysia Kolokotsa. Reinforcement learning for building environmental control.\nIn Reinforcement Learning. InTech, 2008.\n[61] Zhen Yu and Arthur Dexter. Online tuning of a supervisory fuzzy controller for low-energy building system\nusing reinforcement learning. Control Engineering Practice, 18(5):532–539, 2010.\n[62] Daniel Urieli and Peter Stone. A learning agent for heat-pump thermostat control. In Proceedings of the\n2013 international conference on Autonomous agents and multi-agent systems, pages 1093–1100. International\nFoundation for Autonomous Agents and Multiagent Systems, 2013.\n[63] Enda Barrett and Stephen Linder. Autonomous hvac control, a reinforcement learning approach. In Joint\nEuropean Conference on Machine Learning and Knowledge Discovery in Databases, pages 3–19. Springer,\n2015.\n[64] Enda Barrett. Automated control and parallel learning hvac apparatuses, methods and systems, August 4 2016.\nUS Patent App. 15/011,170.\n15\n[65] Frederik Ruelens, Sandro Iacovella, Bert J Claessens, and Ronnie Belmans. Learning agent for a heat-pump\nthermostat with a set-back strategy using model-free reinforcement learning. Energies, 8(8):8300–8318, 2015.\n[66] Lei Yang, Zoltan Nagy, Philippe Gofﬁn, and Arno Schlueter. Reinforcement learning for optimal control of low\nexergy buildings. Applied Energy, 156:577–586, 2015.\n[67] Tianshu Wei, Yanzhi Wang, and Qi Zhu. Deep reinforcement learning for building hvac control. In Proceedings\nof the 54th Annual Design Automation Conference 2017, page 22. ACM, 2017.\n[68] Tianshu Wei, Xiaoming Chen, Xin Li, and Qi Zhu. Model-based and data-driven approaches for building\nautomation and control. In Proceedings of the International Conference on Computer-Aided Design, page 26.\nACM, 2018.\n[69] Yuan Wang, Kirubakaran Velswamy, and Biao Huang. A long-short term memory recurrent neural network\nbased reinforcement learning controller for ofﬁce heating ventilation and air conditioning systems. Processes,\n5(3):46, 2017.\n[70] Charalampos Marantos, Christos P Lamprakos, Vasileios Tsoutsouras, Kostas Siozios, and Dimitrios Soudris.\nTowards plug&play smart thermostats inspired by reinforcement learning. In Proceedings of the Workshop on\nINTelligent Embedded Systems Architectures and Applications, pages 39–44. ACM, 2018.\n[71] Zhiang Zhang, Adrian Chong, Yuqi Pan, Chenlu Zhang, Siliang Lu, and Khee Poh Lam. A deep reinforcement\nlearning approach to using whole building energy model for hvac optimal control. In 2018 Building Performance\nAnalysis Conference and SimBuild, 2018.\n[72] Yujiao Chen, Leslie K Norford, Holly W Samuelson, and Ali Malkawi. Optimal control of hvac and window\nsystems for natural ventilation through reinforcement learning. Energy and Buildings, 169:195–205, 2018.\n[73] Christophe Patyn, Frederik Ruelens, and Geert Deconinck. Comparing neural architectures for demand response\nthrough model-free reinforcement learning for heat pump control. In 2018 IEEE International Energy Conference\n(ENERGYCON), pages 1–6. IEEE, 2018.\n[74] Khalid Al-Jabery, Don C Wunsch, Jinjun Xiong, and Yiyu Shi. A novel grid load management technique\nusing electric water heaters and q-learning. In Smart Grid Communications (SmartGridComm), 2014 IEEE\nInternational Conference on, pages 776–781. IEEE, 2014.\n[75] Khalid Al-Jabery, Zhezhao Xu, Wenjian Yu, Donald C Wunsch, Jinjun Xiong, and Yiyu Shi. Demand-side\nmanagement of domestic electric water heaters using approximate dynamic programming. IEEE Transactions on\nComputer-Aided Design of Integrated Circuits and Systems, 36(5):775–788, 2017.\n[76] Frederik Ruelens, Bert J Claessens, Stijn Vandael, Sandro Iacovella, Pieter Vingerhoets, and Ronnie Belmans.\nDemand response of a heterogeneous cluster of electric water heaters using batch reinforcement learning. In\nPower Systems Computation Conference (PSCC), 2014, pages 1–7. IEEE, 2014.\n[77] Frederik Ruelens, Bert J Claessens, Salman Quaiyum, Bart De Schutter, R Babuška, and Ronnie Belmans.\nReinforcement learning applied to an electric water heater: from theory to practice. IEEE Transactions on Smart\nGrid, 9(4):3792–3800, 2018.\n[78] Oscar De Somer, Ana Soares, Koen Vanthournout, Fred Spiessens, Tristan Kuijpers, and Koen Vossen. Using\nreinforcement learning for demand response of domestic hot water buffers: A real-life demonstration. In\nInnovative Smart Grid Technologies Conference Europe (ISGT-Europe), 2017 IEEE PES, pages 1–7. IEEE, 2017.\n[79] Hussain Kazmi, Fahad Mehmood, Stefan Lodeweyckx, and Johan Driesen. Gigawatt-hour scale savings on a\nbudget of zero: Deep reinforcement learning based optimal control of hot water systems. Energy, 144:159–168,\n2018.\n[80] Mathieu Reymond, Christophe Patyn, Roxana R˘adulescu, Geert Deconinck, and Ann Nowé. Reinforcement\nlearning for demand response of domestic household appliances. In Proceedings of the Adaptive and Learning\nAgents workshop (at AAMAS 2018), 2018.\n[81] Qinglai Wei, Derong Liu, and Guang Shi.\nA novel dual iterative q-learning method for optimal battery\nmanagement in smart residential environments. IEEE Transactions on Industrial Electronics, 62(4):2509–2518,\n2015.\n[82] Chenxiao Guan, Yanzhi Wang, Xue Lin, Shahin Nazarian, and Massoud Pedram. Reinforcement learning-based\ncontrol of residential energy storage systems for electric bill minimization. In Consumer Communications and\nNetworking Conference (CCNC), 2015 12th Annual IEEE, pages 637–642. IEEE, 2015.\n[83] Zhiqiang Wan, Hepeng Li, and Haibo He. Residential energy management with deep reinforcement learning. In\n2018 International Joint Conference on Neural Networks (IJCNN), pages 1–7. IEEE, 2018.\n16\n[84] T Remani, EA Jasmin, and TP Imthias Ahamed. Residential load scheduling with renewable generation in the\nsmart grid: A reinforcement learning approach. IEEE Systems Journal, (99):1–12, 2018.\n[85] Zheng Wen, Daniel O’Neill, and Hamid Maei. Optimal demand response using device-based reinforcement\nlearning. IEEE Transactions on Smart Grid, 6(5):2312–2324, 2015.\n[86] Nikolay Bazenkov and Mikhail Goubko. Advanced planning of home appliances with consumer’s preference\nlearning. In Russian Conference on Artiﬁcial Intelligence, pages 249–259. Springer, 2018.\n[87] Elena Mocanu, Decebal Constantin Mocanu, Phuong H Nguyen, Antonio Liotta, Michael E Webber, Madeleine\nGibescu, and Johannes G Slootweg. On-line building energy optimization using deep reinforcement learning.\nIEEE Transactions on Smart Grid, 2018.\n[88] Yanzhi Wang, Xue Lin, and Massoud Pedram. A near-optimal model-based control algorithm for households\nequipped with residential photovoltaic power generation and energy storage systems. IEEE Transactions on\nSustainable Energy, 7(1):77–86, 2016.\n[89] Brida V Mbuwir, Frederik Ruelens, Fred Spiessens, and Geert Deconinck. Battery energy management in a\nmicrogrid using batch reinforcement learning. Energies, 10(11):1846, 2017.\n[90] Mohammad Rayati, Aras Sheikhi, and Ali Mohammad Ranjbar. Optimising operational cost of a smart energy\nhub, the reinforcement learning approach. International Journal of Parallel, Emergent and Distributed Systems,\n30(4):325–341, 2015.\n[91] A Sheikhi, M Rayati, and AM Ranjbar. Dynamic load management for a residential customer; reinforcement\nlearning approach. Sustainable Cities and Society, 24:42–51, 2016.\n[92] Maria Lorena Tuballa and Michael Lochinvar Abundo. A review of the development of smart grid technologies.\nRenewable and Sustainable Energy Reviews, 59:710–725, 2016.\n[93] Bingnan Jiang and Yunsi Fei. Smart home in smart microgrid: A cost-effective energy ecosystem with intelligent\nhierarchical agents. IEEE Transactions on Smart Grid, 6(1):3–13, 2015.\n[94] Byung-Gook Kim, Yu Zhang, Mihaela Van Der Schaar, and Jang-Won Lee. Dynamic pricing and energy\nconsumption scheduling with reinforcement learning. IEEE Transactions on Smart Grid, 7(5):2187–2198, 2016.\n[95] Amjad Anvari-Moghaddam, Ashkan Rahimi-Kian, Maryam S Mirian, and Josep M Guerrero. A multi-agent\nbased energy management solution for integrated buildings and microgrid system. Applied Energy, 203:41–56,\n2017.\n[96] Amit Prasad and Ivana Dusparic. Multi-agent deep reinforcement learning for zero energy communities. arXiv\npreprint arXiv:1810.03679, 2018.\n[97] Renzhi Lu, Seung Ho Hong, and Xiongfeng Zhang. A dynamic pricing demand response algorithm for smart\ngrid: Reinforcement learning approach. Applied Energy, 220:220–230, 2018.\n[98] Panagiotis Koﬁnas, George Vouros, and Anastasios I Dounis. Energy management in solar microgrid via\nreinforcement learning using fuzzy reward. Advances in Building Energy Research, 12(1):97–115, 2018.\n[99] Bert J Claessens, Peter Vrancx, and Frederik Ruelens. Convolutional neural networks for automatic state-time\nfeature extraction in reinforcement learning applied to residential load control. IEEE Transactions on Smart\nGrid, 9(4):3259–3269, 2018.\n[100] Sunyong Kim and Hyuk Lim. Reinforcement learning based energy management algorithm for smart energy\nbuildings. Energies, 11(8):2010, 2018.\n17\n",
  "categories": [
    "cs.LG",
    "cs.SY",
    "stat.ML"
  ],
  "published": "2019-03-12",
  "updated": "2019-03-15"
}