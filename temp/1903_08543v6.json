{
  "id": "http://arxiv.org/abs/1903.08543v6",
  "title": "Optimizing thermodynamic trajectories using evolutionary and gradient-based reinforcement learning",
  "authors": [
    "Chris Beeler",
    "Uladzimir Yahorau",
    "Rory Coles",
    "Kyle Mills",
    "Stephen Whitelam",
    "Isaac Tamblyn"
  ],
  "abstract": "Using a model heat engine, we show that neural network-based reinforcement\nlearning can identify thermodynamic trajectories of maximal efficiency. We\nconsider both gradient and gradient-free reinforcement learning. We use an\nevolutionary learning algorithm to evolve a population of neural networks,\nsubject to a directive to maximize the efficiency of a trajectory composed of a\nset of elementary thermodynamic processes; the resulting networks learn to\ncarry out the maximally-efficient Carnot, Stirling, or Otto cycles. When given\nan additional irreversible process, this evolutionary scheme learns a\npreviously unknown thermodynamic cycle. Gradient-based reinforcement learning\nis able to learn the Stirling cycle, whereas an evolutionary approach achieves\nthe optimal Carnot cycle. Our results show how the reinforcement learning\nstrategies developed for game playing can be applied to solve physical problems\nconditioned upon path-extensive order parameters.",
  "text": "Optimizing thermodynamic trajectories using evolutionary and gradient-based\nreinforcement learning\nChris Beeler1,2,∗Uladzimir Yahorau3, Rory Coles4, Kyle Mills3,5, Stephen Whitelam6, and Isaac Tamblyn1,3,5†\n1University of Ottawa, Ottawa, ON, Canada\n2National Research Council of Canada, Ottawa, ON, Canada\n3University of Ontario Institute of Technology,\nOshawa, ON, Canada\n4University of Victoria, Victoria, BC, Canada\n5Vector Institute for Artiﬁcial Intelligence,\nToronto, Ontario, Canada\n6Molecular Foundry,\nLawrence Berkeley National Laboratory, Berkeley, CA, USA\n(Dated: November 22, 2021)\nUsing a model heat engine, we show that neural network-based reinforcement learning can iden-\ntify thermodynamic trajectories of maximal eﬃciency. We consider both gradient and gradient-free\nreinforcement learning. We use an evolutionary learning algorithm to evolve a population of neural\nnetworks, subject to a directive to maximize the eﬃciency of a trajectory composed of a set of ele-\nmentary thermodynamic processes; the resulting networks learn to carry out the maximally-eﬃcient\nCarnot, Stirling, or Otto cycles. When given an additional irreversible process, this evolutionary\nscheme learns a previously unknown thermodynamic cycle. Gradient-based reinforcement learning\nis able to learn the Stirling cycle, whereas an evolutionary approach achieves the optimal Carnot\ncycle. Our results show how the reinforcement learning strategies developed for game playing can\nbe applied to solve physical problems conditioned upon path-extensive order parameters.\nI.\nINTRODUCTION\nGames, whether played on a board, such as chess or\nGo, or played on the computer, are a major component\nof human culture [1].\nIn the language of physics, in-\nstances of a game are trajectories, time-ordered sequences\nof elementary steps. The outcome of a game is a path-\nextensive order parameter determined by the entire his-\ntory of the trajectory. Playing games was once the pre-\nserve of human beings, but machine learning methods,\nmore speciﬁcally reinforcement learning, now outperform\nthe most talented humans in all the aforementioned ex-\namples [2–19]. While there are more scientiﬁc examples\nof reinforcement learning [20–25], we choose to compare\nwith games as they are the easiest applications to under-\nstand. Motivated by the correspondence between games\nand trajectories, it is natural to ask how the machine-\nlearning and optimal control methods that have mastered\ngame-playing might be applied to understand physical\nprocesses whose outcomes are path-extensive quantities.\nThere are many examples of such processes. For in-\nstance, the success or failure of molecular self-assembly\nis determined by a time history of elementary dynamical\nprocesses, including the binding and unbinding of parti-\ncles [26–29]. Dynamical systems, such as chemical net-\nworks and molecular machines [30–33], are characterized\nby time-extensive observables, such as work or entropy\nproduction [34–40]. In none of these cases do we possess\na complete theoretical or practical understanding of how\n∗christopher.beeler@uottawa.ca\n† isaac.tamblyn@uottawa.ca\nto build an arbitrary structure or maximize the eﬃciency\nof an arbitrary machine. Traditional methods of inquiry\nin physics focus on applying physical intuition and the\nmanipulation and simulation of equations; perhaps ma-\nchine learning can provide us with further insight into\nphysical problems of a path-extensive nature.\nMotivated by this speculation, we show here that neu-\nral network-based reinforcement learning can maximize\nthe eﬃciency of the simplest type of physical trajectories,\nthe deterministic, quasi-static ones of classical thermody-\nnamics. We introduce a model heat engine characterized\nby a set of thermodynamic state variables. A neural net-\nwork takes as input the current observation of the engine\nand chooses one of a set of basic thermodynamic pro-\ncesses to produce a new observation; this change com-\nprises one step of a trajectory. We chose this simple and\nwell-known system for pedagogical purposes. Using an\neasy-to-understand system allows us to focus on how re-\ninforcement learning methods can be applied to physics\nproblems. We generate a set of trajectories of ﬁxed length\nusing a set of networks whose parameters are initially\nrandomly chosen and optimized using two diﬀerent meth-\nods. In the ﬁrst method (gradient-free), we retain and\nmutate only those networks whose trajectories show the\ngreatest thermal eﬃciency. Repeating this evolutionary\nprocess many times results in networks whose trajecto-\nries reproduce the maximally eﬃcient Carnot, Stirling,\nor Otto cycles, depending upon which basic thermody-\nnamic processes are allowed.\nThis evolutionary proce-\ndure can also learn previously unknown thermodynamic\ncycles if new processes are allowed. In the second method\n(gradient-based), we update the network parameters us-\ning gradient-based reinforcement learning. In this study,\narXiv:1903.08543v6  [cs.NE]  22 Nov 2021\n2\nFIG. 1. (a) Model heat engine and (b) the neural network that controls it. Note that the diagram of the neural network has\ntwo additional input nodes for the gradient-based reinforcement learning method. (c) A summary of the actions in P-V space\navailable to the network; see Tab. I.\nwe explore how a reinforcement learning problem must\nbe framed and the types of solutions acquired by these\ntwo methods. We also compare the advantages and dis-\nadvantages of each in ﬁnding a solution to this problem.\nII.\nMODEL HEAT ENGINE AND\nTHERMODYNAMIC TRAJECTORIES\nIn Fig. 1(a) we show a model heat engine, a device\nable to transform thermal energy into work [41, 42]. The\nengine consists of a working substance, which we assume\nto be a monatomic ideal gas, housed within a friction-\nless container of variable volume V , whose minimum and\nmaximum values are Vmin and Vmax, respectively. The\nworking substance may be connected to a hot or cold\nreservoir held at temperature, Th = 500 K and Tc = 300\nK, respectively. For the gradient-free method, the instan-\ntaneous observation st of the system at time t is then\nspeciﬁed by the volume-temperature vector st = (V, T),\nwith the pressure of the system ﬁxed by the ideal-gas\nequation PV = NkBT [43]. Further on, we will discuss\nthe reasons this formulation of the problem cannot be\nused for gradient-based reinforcement learning methods.\nTo evolve the heat engine we use the neural network\nshown in Fig. 1(b). The network is a nonlinear function\nthat takes as input the current observation st of the sys-\ntem, and outputs the probabilities πθ(at|st) of moving\nto any new observation st+1 through a thermodynamic\nprocess at ∈{a1, a2, . . . , aM} (in the language of rein-\nforcement learning this mapping is called a policy [44]).\nThe symbol θ denotes the internal parameters of the net-\nwork, discussed shortly.\nHere we consider determinis-\ntic evolution through conﬁguration space, with πθ(a∗\nt |st)\nequal to 1 for a chosen process a∗\nt , and equal to zero\notherwise. Enacting the chosen process corresponds to\none step of a trajectory.\nGiven an initial observation\ns0, K applications of the network produces a trajectory\nω = s0 →s1 →· · · →sK of K steps through conﬁg-\nuration space. We focus on trajectories of ﬁxed length\n(K = 200).\nThe elementary actions available to the network cor-\nrespond to the basic thermodynamic processes shown\nin Tab. I, summarized graphically in Fig. 1(c). These\nprocesses include reversible compression and expansion,\nalong isotherms or adiabats, and reversible temperature\nchanges along isochores. Implicitly this means inﬁnitely\nmany heat baths spaced between Th and Tc are available,\na condition, which we shall prove in Appendix A, does\nnot undermine Carnot’s Theorem of maximum eﬃciency.\nAll compression and expansion processes are performed\nusing a ﬁxed change in volume. If an isothermal process\nat Th (or Tc) is selected when the system is not at the\ncorrect temperature, then isochoric heating (or cooling)\nis performed ﬁrst to reach the necessary temperature for\nthe isothermal process to occur. Note that the isochoric\nprocesses are not used in the Carnot cycle, but are re-\nquired to make approximations of it when using ﬁxed\nchanges in volume because the system must reach spe-\nciﬁc volumes in order to be adiabatically heated from Tc\nto Th and cooled from Th to Tc.\nUpon\nundertaking\nany\naction\nst\n→\nst+1,\nwe\nrecord the resulting changes of work, ∆Wstst+1, and\nheat\ninput\nfrom\nthe\nhot\nreservoir,\n∆Qin\nstst+1\n=\n∆Qstst+1H(∆Qstst+1δTf,Th); these are listed in Tab. I.\nHere H(·) is the Heaviside function, equal to 1 for pos-\nitive values of ∆Qstst+1 and 0 otherwise, and Tf is the\ntemperature of the system following the move. δα,β is\nthe Kronecker delta (1 if α = β and 0 otherwise). We de-\nﬁne the thermodynamic eﬃciency of a K-step trajectory\n3\nas\nηK ≡\nPK−1\nt=0 ∆Wstst+1\nPK−1\nt=0 ∆Qin\nstst+1\n.\n(1)\nThe thermal eﬃciency, a path-extensive quantity, is used\nas a means of ranking trajectories, and the networks that\ngenerate them, during our evolutionary learning proce-\ndure. The neural network selects processes deterministi-\ncally based on observations, therefore once a trajectory\nproduces a single cycle, that cycle will repeat until the\nmaximum number of trajectory steps have occurred. For\nthis reason, the maximum value η = maxK ηK for all\nK points along a long trajectory is suﬃcient to identify\neﬃcient thermodynamic cycles.\nWe could terminate a\ntrajectory after it produces a single cycle, however we\nchose this way for consistency with the gradient-based\nreinforcement learning method used where we do require\nmultiple cycles.\nIII.\nNEURAL NETWORK-BASED POLICY\nThe neural network, which contains two layers of tun-\nable weights, performs computations as follows.\nTwo\ninput neurons receive the current observation st, and\nthe output is comprised of M ≤8 neurons, each cor-\nresponding to one of the actions shown in Tab.\nI (in\nsome simulations we prohibit certain actions). The net-\nwork possesses one hidden layer of 1024 neurons, each\nconnected to every input and output neuron. This ar-\nchitecture was chosen after several tests as it can pro-\nduce optimal results while still keeping a low computa-\ntional cost relative to larger (more parameters) networks\nthat produce similar results. Let the indices i ∈{1, 2},\nj ∈{1, . . . , 1024}, and k ∈{1, . . . , M} label the neu-\nrons of the input, hidden, and output layers, respec-\ntively.\nThe input Ii of the two nodes i = 0, 1 of the\ninput layer are, respectively, scaled versions of the cur-\nrent temperature (T −Tc)/(Th −Tc) ∈[0, 1] and volume\n(V −Vmin)/(Vmax −Vmin) ∈[0, 1] of the system. We set\nthe output signal Si of each input-layer node as Si = Ii.\nThe input Ij to neuron j in the hidden layer is\nIj =\n2\nX\ni=1\nSiwij,\n(2)\nwhere the sum runs over the two neurons in the input\nlayer, and wij is the weight of the connection between\nnodes i and j. We set the output signal Sj of neuron j\nto be\nSj = 1\n2 [tanh (Ij + bj)] ,\n(3)\nwhere bj is a bias associated with neuron j.\nThe input Ik to neuron k in the output layer is\nIk =\n1024\nX\nj=1\nSjwjk,\n(4)\nMaximum\nAverage\nFIG. 2. (a) The evolution, as a function of generation num-\nber, of the probability distribution P(η) of eﬃciencies η of\ntrajectories of the model heat engine.\nThe maximum and\naverage eﬃciency of the population are shown above.\nThe\nCarnot eﬃciency is ηmax = 0.4. (b) Trajectories in P-V space\nproduced by the best-performing networks in generations 20,\n21, 22, 24, 25, and 212, in the boxes labeled I-VI, respectively.\nThe colors (shades), curvature, and direction of the branches\ncorrespond to the processes shown in Fig. 1. Highly-evolved\nnetworks enact the Carnot cycle.\nwhere the sum runs over all 1024 neurons of the hidden\nlayer. Finally, we take the output signal Sk from each\noutput-layer neuron to be equal to Ik.\nTo choose an\naction we pick the output neuron, k⋆, with the largest\nvalue of Sk. Given a current observation st, this action a⋆\nt\ndeﬁnes a new observation s⋆\nt+1 via Tab. I. The probability\nπθ(a⋆\nt |st) is then unity, and all other πθ(at|st) are zero.\nWe denote by θ = {{w}, {b}} the set of all weights and\nbiases of the network. Initially each weight and bias is\nchosen from a Gaussian distribution with zero mean and\nunit variance.\nIV.\nEVOLUTIONARY LEARNING DYNAMICS\n(GRADIENT-FREE)\nWith the thermodynamic system and means of evolv-\ning it deﬁned, we introduce an evolutionary learning dy-\n4\nTABLE I. All possible actions that can be taken on our model heat engine and their corresponding ∆W and ∆Q equations.\nAction\n∆W\n∆Q\nAdiabatic Compression\n−3\n2NkBTi\n\u0012\u0010\nVi\nVf\n\u0011 2\n3 −1\n\u0013\n0\nAdiabatic Expansion\n−3\n2NkBTi\n\u0012\u0010\nVi\nVf\n\u0011 2\n3 −1\n\u0013\n0\nIsothermal Compression at Th (T = Th)\nNkBTh log\n\u0010\nVf\nVi\n\u0011\nNkBTh log\n\u0010\nVf\nVi\n\u0011\nIsothermal Expansion at Th (T = Th)\nNkBTh log\n\u0010\nVf\nVi\n\u0011\nNkBTh log\n\u0010\nVf\nVi\n\u0011\nIsothermal Compression at Th (T̸=Th)\nNkBTh log\n\u0010\nVf\nVi\n\u0011\nNkBTh log\n\u0010\nVf\nVi\n\u0011\n+ 3\n2NkB (Th −Ti)\nIsothermal Expansion at Th (T̸=Th)\nNkBTh log\n\u0010\nVf\nVi\n\u0011\nNkBTh log\n\u0010\nVf\nVi\n\u0011\n+ 3\n2NkB (Th −Ti)\nIsothermal Compression at Tc (T = Tc)\nNkBTc log\n\u0010\nVf\nVi\n\u0011\nNkBTc log\n\u0010\nVf\nVi\n\u0011\nIsothermal Expansion at Tc (T = Tc)\nNkBTc log\n\u0010\nVf\nVi\n\u0011\nNkBTc log\n\u0010\nVf\nVi\n\u0011\nIsothermal Compression at Tc (T̸=Tc)\nNkBTc log\n\u0010\nVf\nVi\n\u0011\nNkBTc log\n\u0010\nVf\nVi\n\u0011\n+ 3\n2NkB (Tc −Ti)\nIsothermal Expansion at Tc (T̸=Tc)\nNkBTc log\n\u0010\nVf\nVi\n\u0011\nNkBTc log\n\u0010\nVf\nVi\n\u0011\n+ 3\n2NkB (Tc −Ti)\nIsochoric Heating\n0\n3\n2NkB (Th −Ti)\nIsochoric Cooling\n0\n3\n2NkB (Tc −Ti)\nnamics designed to produce networks able to propagate\neﬃcient thermodynamic trajectories.\nWe start with a\npopulation of 100 networks, with the internal parame-\nters θ of each initialized in the random fashion described\nabove.\nWe name this population generation 1.\nThis\npopulation produces thermodynamic trajectories ω of K\nsteps with the distribution P(η) of eﬃciencies η shown\nin Fig. 2(a).\nSome of the networks in earlier genera-\ntions do not appear in these distributions due to their\nundeﬁned or very negative eﬃciencies. Even the best-\nperforming members of this population produce eﬃcien-\ncies much lower than the Carnot eﬃciency, which is the\nmost eﬃcient trajectory possible given the set of allowed\nthermodynamic processes [42]. We have ηmax = 0.4 with\nour choice of parameters.\nWe next perform the ﬁrst step of evolutionary learning\ndynamics. We keep the 25 generation-1 networks whose\ntrajectories have the largest η, and we discard the rest.\nWe create 75 new networks by drawing uniformly from\nthe set of 25, each time “mutating” all weights w and bi-\nases b: for each weight or bias we draw a random number\nδ from a Gaussian distribution with zero mean and unit\nvariance, and update the weight or bias as w →w + ϵδ\nor b →b + ϵδ, where ϵ = 0.05 is an evolutionary learning\nrate.\nThe new population of the 25 best generation-1 net-\nworks and their 75 mutant oﬀspring constitute genera-\ntion 2. We simulate those 100 networks for K steps, pro-\nducing the distribution of eﬃciencies shown in Fig. 2(a).\nContinuing this alternation of evolutionary dynamics (re-\ntaining and mutating the best networks of the current\ngeneration) and physical dynamics (using the new gener-\nation of networks to generate a set of trajectories) gives\nrise to networks able to propagate increasingly eﬃcient\ntrajectories [Fig. 2(a)]. After about 100 generations, we\nobtain networks whose eﬃciencies are equal to that of\nthe Carnot cycle (to within four decimal places). Inspec-\ntion of the trajectories corresponding to these values of η\nshow that they indeed form Carnot cycles; see Fig. 2(b).\nSeveral features of this learning process are notable. In\nlearning to maximize the eﬃciency of a thermodynamic\ntrajectory, networks have learned to propagate cycles, as\nopposed to non-closed trajectories in P-V space, because\ncycles lead in general to larger eﬃciencies. As the ther-\nmal eﬃciency of the Carnot cycle, deﬁned as\nηmax = 1 −Tc\nTh\n,\n(5)\nis independent of volume, the Carnot cycle has no abso-\nlute scale associated with it, meaning the Carnot cycle\non two heat engines with two distinct total volumes are\nequivalent.\nHowever, this is not true for the Stirling,\nOtto, or the discretized approximations of the Carnot\ncycles, whose thermal eﬃciencies can be simply derived\nusing the Eq. (1) with the processes in Tab. I [41] and\nare deﬁned as\nηS =\nTh −Tc\nTh + 3(Th−Tc)\n2 log(Vr)\n,\n(6)\nfor the Stirling cycle, where Vr = Vmax/Vmin,\nηO = 1 −V\n−2\n3\nr\n,\n(7)\n5\nfor the Otto cycle, and\nη′\nC =\nTh\n\u0012\nlog\n\u0010\nV2\nVmin\n\u0011\n−3\n2\n\u0012\u0010\nV2\nVmax\n\u0011 2\n3 −1\n\u0013\u0013\n3\n2\n\u0012\nTh −Tc\n\u0010\nV1\nVmin\n\u0011 2\n3 \u0013\n+ Th log\n\u0010\nV2\nVmin\n\u0011\n+\nTc\n\u0012\nlog\n\u0010\nV1\nVmax\n\u0011\n−3\n2\n\u0012\u0010\nV1\nVmin\n\u0011 2\n3 −1\n\u0013\u0013\n3\n2\n\u0012\nTh −Tc\n\u0010\nV1\nVmin\n\u0011 2\n3 \u0013\n+ Th log\n\u0010\nV2\nVmin\n\u0011\n(8)\nfor the discretized Carnot cycle, where V1 is the volume\nwhen adiabatic compression begins, and V2 is the volume\nwhen adiabatic expansion begins. If V1 = (Th/Tc)3/2Vmin\nand V2 = (Tc/Th)3/2Vmax then the volume terms cancel\nand we get η′\nC = ηmax. Otherwise, all of these equations\ncontain volume terms and therefore the Stirling, Otto,\nand discretized Carnot cycles are volume dependent. In-\nterestingly, ηO is independent of the thermal baths, how-\never it can only be performed if Th > TcV 2/3. When this\nis the case we have\nηmax = 1 −Tc\nTh\n> 1 −\nTc\nTcV\n2\n3\nr\n= 1 −V\n−2\n3\nr\n= ηO,\n(9)\nhowever the Carnot cycle is not technically possible in\nthis scenario as it is impossible to heat the system the\nrequired amount with adiabatic processes, so we only\nbrieﬂy consider it. For ﬁxed Th and Tc, ηS and ηO are\nmaximized by maximizing the volume ratio Vr, however\nη′\nC is maximized by minimizing the diﬀerence between V1\nand V2 with the volume values used in the true Carnot cy-\ncle. Given the ﬁxed step sizes permitted for the processes\nin Tab. I, networks have learned to enact the size of a\ncycle that allows the best approximation of the Carnot\ncycle.\nGiven only a set of processes and a path-extensive mea-\nsure of eﬃciency, our neural network-based evolutionary\nlearning framework is able to maximize path eﬃciency\nand so deduce a classic result of physics.\nThis learn-\ning framework is also successful if it is presented with\na diﬀerent set of processes. When denied the adiabatic\nprocesses of Tab. I, our gradient-free evolutionary algo-\nrithm learns the Stirling cycle [45], which is maximally-\neﬃcient in this context (with our systems parameters,\nηS ≈0.291.); when denied the isothermal processes it\nsimilarly learns the maximally-eﬃcient Otto cycle [46].\nIn fact, these are the only types of cycles possible in those\nscenarios. For the Otto cycle, Th, must be increased so\nwe set it to 1000 K (ηO ≈0.658 and ηmax = 0.8).\nExtensions to unknown thermodynamic processes are\nstraightforward, and inspection of the resulting solutions\nprovides physical insight in an unfamiliar setting.\nAs\nan illustration, we replace the standard monatomic ideal\ngas adiabatic process, for which TV 2/3 is constant, with\na ﬁctitious irreversible process for which\nTV 2/3 ∝(1 −k)∆V/(V0−V1);\n(10)\nFIG. 3. We apply the evolutionary process described in Fig. 2\nto a new setting in which the adiabatic processes of Tab. I\nare replaced with the irreversible process (10); panel (a) sum-\nmarizes the new set of accessible moves. (b) Highly-evolved\nnetworks learn to enact a hybrid of the Stirling and Carnot\ncycles, and the resulting equations of state can be identiﬁed\nby curve ﬁtting.\nhere k = 2/5 and ∆V are the fraction of thermal en-\nergy lost to the surrounding environment, and the change\nin volume upon making the move, respectively. We al-\nlow the network access to this process and the others of\nTab. I (excluding adiabatic processes), summarized in\nFig. 3(a). In this setting, we do not know in advance the\nmost eﬃcient trajectory. In Fig. 3(b) we show that the\nsolution identiﬁed by our evolutionary learning scheme\nis a hybrid of the Stirling and Carnot cycles. Upon in-\nvestigation of this solution, it is clear that the detriment\nto the thermal eﬃciency caused by the increased use of\nheat from the isochore (labelled process 4) is more ben-\neﬁcial than trying to approximate just the Carnot cy-\ncle given these irreversible processes.\nIn this modiﬁed\nheat engine, we deﬁne an approximation of the Carnot\ncycle by replacing the adiabatic processes with the irre-\nversible processes such that they heat and cool the sys-\ntem in a qualitatively similar manner. The irreversible\ncompression process is thermally ineﬃcient compared to\nadiabatic compression, therefore requiring more work to\n6\nheat the system without using the thermal reservoirs. If\nk is suﬃciently low, the approximated Carnot cycle is\nmost eﬃcient and as k approaches 1.0, the hybrid cycle\napproaches the Stirling cycle. Note that for suﬃciently\nhigh k the irreversible process becomes physically unrea-\nsonable. By ﬁtting equations to each branch of the cycle,\nwe identify the equations of state that result from the\nirreversible process (10). These results highlight the gen-\neral applicability of the learning scheme and indicate the\nphysical insight that can be obtained by interrogating\nsolutions identiﬁed by machine learning.\nV.\nGRADIENT-BASED REINFORCEMENT\nLEARNING\nAs an alternative to the gradient-free evolutionary\nlearning, we consider a method which includes gradi-\nent information. We note however, that while the ob-\nservation st = (V, T) contains suﬃcient information to\ncorrectly determine the required thermodynamic process\nat any step t, as shown above, it is not suﬃcient for\ndetermining thermal eﬃciency.\nFor example, suppose\nthe Carnot cycle requires K steps with s0 = (Vmax, Tc).\nK steps later we arrive back at the same observation,\nsK = s0, achieving a thermal eﬃciency of ηmax. Now\nif we perform the reverse Carnot cycle, we would be-\ngin and end at the same observations as before, how-\never this time achieving a thermal eﬃciency of −ηmax,\nmeaning that the relation of the current observation rep-\nresentation to thermal eﬃciency is one-to-many. Unlike\ngradient-free algorithms, traditional gradient-based rein-\nforcement learning algorithms operate using the rewards\nassociated with speciﬁc steps of a trajectory, instead of\nassigning a score to an entire trajectory. For this reason,\nthese algorithms cannot be used on this speciﬁc simulated\nheat engine because it is not Markovian. A common ex-\nample of a Markovian problem would be chess, where one\nneeds to only know the current state of the board, not\nhow the individual pieces were moved to their current\npositions in order to make an optimal move.\nA.\nMarkovian model heat engine\nWe now introduce a Markovian simulated heat en-\ngine designed for gradient-based reinforcement learning\n(we will also consider this Markovian system with our\ngradient-free approach). In this new problem, which will\nbe referred to as the Markovian heat engine problem,\nwe provide the agent with non-negative work and heat\nbudgets, deﬁned as W ∗\nK = W ∗\n0 + PK−1\nt=0 ∆Wstst+1 and\nQ∗\nK = Q∗\n0 −PK−1\nt=0 ∆Qin\nstst+1 respectively, where W ∗\n0 and\nQ∗\n0 are the initial budget values. For a thermodynamic\nprocess to be performed at time t, W ∗\nt−1 ≥−∆Wst−1st\nand Q∗\nt−1 ≥Qin\nst−1st are required to ensure W ∗\nt and Q∗\nt\nare non-negative. These budgets represent the amount of\nwork the agent can put into the system, where any work\nproduced during a trajectory is added to the budget, and\nthe amount of heat that can be transferred from the hot\nthermal reservoir to the heat engine. We deﬁne the ob-\nservation for this new heat engine as st = (V, T, W ∗\nt , Q∗\nt ),\nhowever we still consider a cycle as a trajectory that re-\nturns the same values of V and T. With this new repre-\nsentation, the observations at the beginning and end of a\ngiven singular cycle are now unique and Markovian. This\nmeans that a given observation contains all the required\ninformation about the state of the simulated heat engine\nand therefore the previous trajectory is not required. Full\nMarkovity is not necessarily required to apply gradient-\nbased reinforcement learning, however it does ensure the\nmapping from observation–action pairs to rewards can\nbe learned by the agent. As W ∗\n0 and Q∗\n0 are ﬁxed, the\nrelation of observation to thermal eﬃciency is now many-\nto-one, solving the issue of mapping an observation to a\nsingle thermal eﬃciency.\nAs the observations for the start of each repeated cy-\ncle are unique, if a trajectory produces one cycle, it does\nnot guarantee it will produce many cycles. This poses\na greater challenge as each subsequent cycle must now\nbe learned individually.\nThis could be solved by re-\ninitializing the system to the initial observation after a\ncycle is performed, however this would force a solution\nfor single cycles where we would like to ﬁnd a more gen-\neral solution for the system. Thermal eﬃciency could be\nassigned to the ﬁnal observation–action pair of a trajec-\ntory to update a given policy using gradients in order\nto produce the most thermally eﬃcient cycle, similar to\nwhat was done in the previous case, however this does\nnot encourage trajectories containing more than a single\ncycle as thermal eﬃciency does not change when a cycle\nis repeated exactly.\nTo avoid explicitly penalizing a trajectory for stopping\nafter a single cycle, we instead use ∆W = W ∗\nK −W ∗\n0\nat the end of a trajectory.\nAs ∆Q = Q∗\n0 −Q∗\nK has\na ﬁxed ﬁnite maximum, using Eq. 1, maximizing ∆W\nmaximizes thermal eﬃciency while also encouraging tra-\njectories with many cycles, satisfying our desired require-\nments. To help clarify this, consider the following exam-\nple. Suppose some cycle on this heat engine produces 0.2\nunits of work and consumes 0.5 units of heat. This would\ngive a thermal eﬃciency of 0.4. Now if we performed this\ncycle 10 times we would produce 2.0 units of work and\nconsume 5.0 units of heat. However this would still give\na thermal eﬃciency of 0.4. If we use thermal eﬃciency\nas the reward, these two cases are equivalent, however if\nwe use ∆W as the reward, the second case is preferred.\nB.\nProximal policy optimization\nWith our newly deﬁned engine and reward scheme, we\nturn to proximal policy optimization (PPO) [13] as the\ngradient-based reinforcement learning algorithm to ﬁnd\na policy, πθ, that produces the desired trajectories. PPO\nis a commonly used gradient-based reinforcement learn-\n7\nFIG. 4. We apply the PPO reinforcement learning process to our Markovian simulated heat engine; panel (a) shows the most\nthermally eﬃcient trajectory produced in this process overlaid with the ideal solution, the discretized Carnot cycle. (b) The\nlearning dynamics of thermal eﬃciency as a function of training steps for both the PPO and evolutionary agents as they learn\nto maximize ∆W. (c) ∆W as a function of ∆Q for the trajectory produced by the best performing PPO and evolutionary\nagents and (d) the ideal solution (discretized Carnot cycle), where ∆Q = 1 maps to the amount of energy used during a single\nanalytic Carnot cycle.\ning algorithm which has had good performance on many\nproblems [47–51]. The true policy, π∗\nθ, requires certain\nfeatures from the system in order to produce these de-\nsired trajectories.\nWhile the observation may not ex-\nplicitly contain these features, we assume we can extract\nthem using a neural network approximation of π∗\nθ. PPO\naims to maximize the policy gradient objective function,\nLPG\nt\n(θ) = ˆE\nh\nlog πθ (at|st) ˆAt\ni\n,\n(11)\nwhere the expectation is taken with respect to the ac-\ntions at and ˆAt = ˆAθ (st|at) is the estimate of the ad-\nvantage function deﬁned as the diﬀerence between the\nQ-function Qπθ(st, at) and the value function Vπθ(st).\nThe Q-function is the expected discounted future reward\nif the process at is performed starting at st and πθ is\nfollowed for the remainder of the trajectory starting at\nst+1, deﬁned as\nQπθ(st, at) = rt + γ max\na\nQπθ(st+1, a),\n(12)\nwhere 0 ≤γ ≤1 is the discount factor. The value func-\ntion is the expected discounted future reward if πθ is\nfollowed for the remainder of the trajectory starting at\nst, deﬁned as\nVπθ(st) = max\na\nQπθ(st, a).\n(13)\nOur estimate of future reward is rarely perfect, therefore\nit is discounted relative to the time scale of the reward.\nFor example, when deciding on where to eat, we only\ncare about the immediate reward we get, so we heavily\ndiscount our estimate of future reward in this case, how-\never when investing in stocks, we care about the long-\nterm reward, so our estimate of future reward should be\nminimally discounted. Qπθ and Vπθ are not known ana-\nlytically but it is assumed they require the same features\nas πθ and therefore we have our same neural network ap-\nproximation of πθ output the value at st, i.e. the output\nof the neural network is (πθ(st), Vπθ(st)). However, it is\nmore eﬃcient to instead maximize the trust region policy\noptimization (TRPO) [52] objective function,\nLCPI\nt\n(θ) = ˆE\n\u0014 πθ (at|st)\nπθold (at|st)\nˆAt\n\u0015\n= ˆE\nh\nrθ,θold (at|st) ˆAt\ni\n,\n(14)\nwhere πθold is the set of parameters of the previous pol-\nicy. The trivial solution to this is to make extreme mod-\niﬁcations to πθ, therefore PPO clips the probability ratio\nrθ,θold and takes the minimum,\nLCLIP\nt\n(θ) = ˆE\nh\nmin\n\u0010\nrθ (at|st) ˆAt, rCLIP\nθ\n(at|st) ˆAt\n\u0011i\nrCLIP\nθ,θold (at|st) = clip (rθ,θold (at|st) , 1 −ϵ, 1 + ϵ) ,\n(15)\nwhere ϵ is a hyperparameter and the clip function is de-\nﬁned as\nclip(x, a, b) = min(max(x, a), b) = max(min(x, b), a).\n(16)\nWhile this objective function is designed to optimize the\npolicy, it is not designed to optimize the value function,\ntherefore we use a mean squared error cost function,\nLVF\nt\n(θ) =\n\u0000Vπθ (st) −V targ\nt\n\u00012 ,\n(17)\nwhere V targ\nt\nis the target value. Combing this cost func-\ntion with our objective function and adding in a so called\n8\n“entropy” term S to encourage exploration, we have\nLPPO\nt\n(θ) = ˆE\n\u0002\nLCLIP\nt\n(θ) −c1LVF\nt\n(θ) + c2S [πθ] (st)\n\u0003\n,\n(18)\nfor our overall PPO objective function, where c1 and c2\nare hyper-parameters and S [πθ] is deﬁned by\nS [πθ] (st) =\nX\na\nπθ (at|st) log (πθ (at|st)) .\n(19)\nNote that we have the negative of our cost term, so max-\nimizing LPPO\nt\nsimultaneously maximizes our objective\nfunction and minimizes our cost function. To update the\nparameters of our neural network, the gradient of LPPO\nt\nis estimated with respect to θ using backpropagation and\nwe make changes to θ in the ascending direction of this\ngradient.\nC.\nResults and Discussion\nUsing the identical network architecture as with the\ngradient-free method, we randomly initialize a policy and\nallow it to produce trajectories in the newly-deﬁned ther-\nmodynamic system. As the policy interacts with this sys-\ntem, we collect tuples of the form (st, at, rt, st+1) which\nare the experiences used to update the policy with PPO.\nWe require all four of these because the reward rt specif-\nically arises from how the action at maps the observation\nst to the next observation st+1.\nrt and st+1 are also\nrequired to update our approximation of the value func-\ntion and calculate ˆAt. To perform these updates, we used\nthe following hyper-parameters: discount factor of 0.99,\nϵ = 0.2, c1 = 0.5, c2 = 0.01, a batch size of 128, a total\nnumber of training steps of 2×106, and a learning rate of\n2.5×10−4 which determines the step size when updating\nthe parameters θ.\nUnlike the gradient-free approach previously discussed,\nPPO is unable to achieve an optimal result. However the\nagent was able to produce a trajectory that not only fol-\nlows the Stirling cycle as shown in Fig. 4(a), and does so\nmore than once. When looking at the learning dynam-\nics shown in Fig. 4(b), we can see that the policy that\nproduces the most thermally eﬃcient trajectory occurs\nat ∼2.5×105 training steps, and then thermal eﬃciency\nsteadily decreases in all subsequent policies. This is likely\ndue to the Stirling cycle being a local maximum (in terms\nof ∆W). When a policy is at a local maximum for long\nenough, the value function is optimized on the trajec-\ntories found at this local maximum to the point that it\nbecomes useless when evaluated on any trajectories found\noutside of the local maximum. Due to the deterministic\nnature of these problems, only a single trajectory is seen\nat this local maximum.\nEventually, the entropy term\nused for stochastic exploration forces the policy to stray\nfrom this locally optimal trajectory, causing the policy to\nproduce a trajectory unseen by the agent recently. As the\nvalue function is no longer accurate at this observation,\nthe agent is unable to bring the policy back to this local\nmaximum causing performance to diminish. The Stirling\ncycle is a local maximum because it produces more work\nthan the Carnot cycle, i.e. achieves a higher reward per\ncycle, however as seen when comparing Figs. 4(c) and\n(d), the Stirling cycle consumes a greater amount of heat\nthan the Carnot cycle. This makes it more viable in the\nshort term, however, due to the upper bound constraint\nplaced on ∆Q, the Carnot cycle is still the ideal solution\nwhen tasked with maximizing ∆W.\nWith the maximum amount of heat allowed, the only\nway to increase ∆W beyond what is produced by two\nStirling cycles, a trajectory would need to instead pro-\nduce either three Carnot cycles and one Stirling cycle or\nﬁve Carnot cycles.\nThe diﬀerences between these tra-\njectories and the one the agent produces are not trivial.\nMaking any slight modiﬁcations to the found trajectory\ndecreases ∆W. It is only when an entire Stirling cycle\nis replaced with multiple Carnot cycles in a trajectory\nthat there would be any improvement. Additionally, the\nreward signal used to update a policy comes from the\nend of a trajectory, whereas due to the repetitive nature\nof the non-Markovian problem, we were able to select\nthe maximal score along the entire trajectory. Now the\npolicy is required to produce trajectories that not only\nfollow optimal paths but also end in optimal positions,\nincreasing the diﬃculty of the Markovian problem even\nfurther.\nVI.\nEVOLUTIONARY LEARNING ON\nMARKOVIAN HEAT ENGINE\nIf ∆W is used as the scoring metric, the evolutionary\nlearning method from before can also be applied to this\nvariant of the simulated heat engine.\nDoing so yields\ntrajectories with lower thermal eﬃciencies and less work\nproduced compared with the gradient-based method as\nshown in Figs. 4(b) and (c).\nSimilar to the gradient-\nbased method, the gradient-free method starts it’s tra-\njectory by producing a Stirling cycle, however it is un-\nable to complete the second cycle. While the desired tra-\njectory is identical to that of the non-Markovian prob-\nlem, this task is a more challenging one because the\ncycle must be learned over and over again as the heat\nbudget is consumed.\nThis shows that the evolution-\nary learning method is not necessarily superior to the\ngradient-based method in all ways, but rather each ap-\nproach is suited for diﬀerent tasks. The gradient-based\nmethod was able to outperform the evolutionary learning\nmethod on the Markovian problem, however, it was the\nevolutionary learning method that was able to ﬁnd the\nmaximally eﬃcient trajectory when applied to the non-\nMarkovian problem. The evolutionary learning method\nwas able to be applied to both tasks, however, because\nof the nature of gradient-based reinforcement learning,\nwe could only apply it to the Markovian problem. The\nfact the gradient-based method ﬁnds this local maximum\nwhile the gradient-free does not supports the idea that\n9\nFIG. 5. T-S diagram for the (a) Carnot, (b) discretized Carnot, and (c) Stirling cycles. The red (dark gray) area (labelled\nQout) represents the heat removed from the system, the white area (labelled ∆W) represents the work produced by the system,\nand the red (dark gray) and white areas together represent the heat added to the system.\ngradient-based reinforcement learning can propagate the\nreward assigned to speciﬁc observation–action pairs in\norder to produce at least a partially optimal solution.\nEvolutionary learning methods cannot straightforwardly\nmake use of these speciﬁc assignments and this is likely\nthe reason why it is outperformed by the gradient-based\nlearning method on the Markovian problem. Although\nthe evolutionary learning method seems to be the go-\nto method, being able to deﬁne a problem the way we\ndid for the non-Markovian one is not always possible. It\nwould be possible to make the Markovian problem easier\nfor the gradient-based learning method, but not without\nproviding such a high level of feedback that the prob-\nlem becomes pointless to solve. For example, one could\nreward the gradient-based learning agent as it performs\neach of the correct steps in the most thermally eﬃcient\ntrajectory, however, this approach would only work in the\ncase an optimal solution was known beforehand. Novelty\nsearch techniques might allow this gradient-based learn-\ning method to ﬁnd the optimal trajectory, however, this\nwould increase the computational cost of this method\neven further. Given that gradient-based methods are al-\nready more computationally expensive than gradient-free\nmethods, and that the gradient-free method has already\nachieved the optimal result, we have chosen not to ex-\nplore these options.\nVII.\nCONCLUSIONS\nMotivated by the correspondence between games and\nphysical trajectories, we have shown that neural network-\nbased evolutionary learning can optimize the eﬃciency of\ntrajectories of classical thermodynamics. Given a set of\nphysical processes and a path-extensive measure of eﬃ-\nciency, networks evolve to learn the maximally-eﬃcient\nCarnot, Stirling, or Otto cycles, reproducing classic re-\nsults of physics that were originally derived by applica-\ntion of physical insight. Given new processes, the evo-\nlutionary framework identiﬁes solutions that when in-\nterrogated provide physical insight into the problem at\nhand. We have also shown that with slight modiﬁcations,\ngradient-based reinforcement learning can optimize the\neﬃciency of trajectories of classical thermodynamics, al-\nthough not quite as eﬀectively.\nACKNOWLEDGMENTS\nCB, UY, RC, KM, and IT performed work at the Na-\ntional Research Council of Canada under the auspices\nof the AI4D Program.\nIT acknowledges NSERC. SW\nperformed work at the Molecular Foundry, Lawrence\nBerkeley National Laboratory, supported by the Oﬃce\nof Science, Oﬃce of Basic Energy Sciences, of the U.S.\nDepartment of Energy under Contract No. DE-AC02–\n05CH11231.\nThe simulated heat engines (or environments [15] as\ncommonly referred to in the reinforcement learning com-\nmunity) used in this study can be found at https:\n//clean.energyscience.ca/gyms.\nAppendix A: Proof of Carnot’s theorem on inﬁnite\nheat bathes\nUsing T-S diagrams, shown in Fig. 5, the thermal eﬃ-\nciency of a cycle is determined by η = ∆W/(∆W +Qout)\nwhere ∆W is the work produced by the system and Qout\nis the heat removed from the system. Using Fig. 5(a), the\nthermal eﬃciency of the Carnot cycle is given by Eq. (5),\nwhich is independent of the minimum and maximum en-\ntropies reach by the Carnot cycle, denoted by SC\nmin and\nSC\nmax respectively. For cycles that operate between SC\nmin\nand SC\nmax, since Tc is the minimum temperature the sys-\ntem can reach, η is maximized by maximizing ∆W and\nminimizing Qout, which results in the Carnot cycle. Now\nconsider cycles that operate outside of the entropy range\nSC\nmin to SC\nmax, such as the ones shown in Figs. 5(b) and\n(c). It is impossible for the system to simultaneously be\n10\nat Th and an entropy less than SC\nmin, therefore the ratio of\nwork produced and heat removed by the system between\npoints II and III will be less than that of the Carnot\ncycle. Similarly, it is impossible for the system to simul-\ntaneously be at Tc and and entropy greater than SC\nmax,\ntherefore the ratio of work produced and heat removed\nby the system between points IV and I will be less than\nthe Carnot cycle. Putting all these together, any cycle\noperating within SC\nmin and SC\nmax is at most eﬃcient as the\nCarnot cycle, and any cycle operating outside of SC\nmin and\nSC\nmax is less eﬃcient, therefore Carnot’s theorem holds for\ninﬁnitely many heat bathes.\n[1] J. M. Roberts, M. J. Arth, and R. R. Bush, American\nanthropologist 61, 597 (1959).\n[2] C. J. Watkins and P. Dayan, Machine learning 8, 279\n(1992).\n[3] V.\nMnih,\nK.\nKavukcuoglu,\nD.\nSilver,\nA.\nGraves,\nI. Antonoglou, D. Wierstra, and M. Riedmiller, arXiv\npreprint arXiv:1312.5602 (2013).\n[4] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Ve-\nness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K.\nFidjeland, G. Ostrovski, et al., Nature 518, 529 (2015).\n[5] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling,\nJournal of Artiﬁcial Intelligence Research 47, 253 (2013).\n[6] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lilli-\ncrap, T. Harley, D. Silver, and K. Kavukcuoglu, in In-\nternational conference on machine learning (2016), pp.\n1928–1937.\n[7] Y. Tassa,\nY. Doron,\nA. Muldal,\nT. Erez,\nY. Li,\nD. d. L. Casas, D. Budden, A. Abdolmaleki, J. Merel,\nA. Lefrancq, et al., arXiv preprint arXiv:1801.00690\n(2018).\n[8] E. Todorov,\nT. Erez,\nand Y. Tassa,\nin Intelligent\nRobots and Systems (IROS), 2012 IEEE/RSJ Interna-\ntional Conference on (IEEE, 2012), pp. 5026–5033.\n[9] M. L. Puterman, Markov decision processes:\ndiscrete\nstochastic dynamic programming (John Wiley & Sons,\n2014).\n[10] A. Asperti, D. Cortesi, and F. Sovrano, in Machine\nLearning, Optimization, and Data Science, edited by\nG. Nicosia, P. Pardalos, G. Giuﬀrida, R. Umeton, and\nV. Sciacca (Springer International Publishing, Cham,\n2019), pp. 264–275.\n[11] M. Riedmiller, in European Conference on Machine\nLearning (Springer, 2005), pp. 317–328.\n[12] M. Riedmiller, T. Gabel, R. Hafner, and S. Lange, Au-\ntonomous Robots 27, 55 (2009).\n[13] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and\nO. Klimov, arXiv preprint arXiv:1707.06347 (2017).\n[14] F. P. Such, V. Madhavan, E. Conti, J. Lehman, K. O.\nStanley, and J. Clune, arXiv preprint arXiv:1712.06567\n(2017).\n[15] G. Brockman, V. Cheung, L. Pettersson, J. Schneider,\nJ. Schulman, J. Tang, and W. Zaremba, arXiv preprint\narXiv:1606.01540 (2016).\n[16] M. Kempka, M. Wydmuch, G. Runc, J. Toczek, and\nW. Ja´skowski, in Computational Intelligence and Games\n(CIG), 2016 IEEE Conference on (IEEE, 2016), pp. 1–8.\n[17] M. Wydmuch, M. Kempka, and W. Ja´skowski, IEEE\nTransactions on Games 11, 248 (2019).\n[18] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre,\nG. Van Den Driessche, J. Schrittwieser, I. Antonoglou,\nV. Panneershelvam, M. Lanctot, et al., nature 529, 484\n(2016).\n[19] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou,\nA. Huang, A. Guez, T. Hubert, L. Baker, M. Lai,\nA. Bolton, et al., Nature 550, 354 (2017).\n[20] K. Mills, P. Ronagh, and I. Tamblyn, Nature Machine\nIntelligence 2, 509 (2020).\n[21] P.\nAndreasson,\nJ.\nJohansson,\nS.\nLiljestrand,\nand\nM. Granath, Quantum 3, 183 (2019).\n[22] Z. Zhou, S. Kearnes, L. Li, R. N. Zare, and P. Riley,\nScientiﬁc reports 9, 1 (2019).\n[23] M. I. Radaideh, I. Wolverton, J. Joseph, J. J. Tusar,\nU. Otgonbaatar, N. Roy, B. Forget, and K. Shirvan, Nu-\nclear Engineering and Design p. 110966 (2020).\n[24] T. Badloe, I. Kim, and J. Rho, Physical Chemistry\nChemical Physics 22, 2337 (2020).\n[25] M. Popova, O. Isayev, and A. Tropsha, Science advances\n4, eaap7885 (2018).\n[26] J. J. De Yoreo, P. U. Gilbert, N. A. Sommerdijk, R. L.\nPenn, S. Whitelam, D. Joester, H. Zhang, J. D. Rimer,\nA. Navrotsky, J. F. Banﬁeld, et al., Science 349, aaa6760\n(2015).\n[27] M. F. Hagan and D. Chandler, Biophysical Journal 91,\n42 (2006).\n[28] A. W. Wilber, J. P. Doye, A. A. Louis, E. G. Noya, M. A.\nMiller, and P. Wong, The Journal of Chemical Physics\n127, 085106 (2007).\n[29] S. Whitelam and R. L. Jack, Annual review of physical\nchemistry 66, 143 (2015).\n[30] D. T. Gillespie, Annu. Rev. Phys. Chem. 58, 35 (2007).\n[31] T. McGrath, N. S. Jones, P. R. ten Wolde, and T. E.\nOuldridge, Physical Review Letters 118, 028101 (2017).\n[32] U. Seifert, Reports on progress in Physics 75, 126001\n(2012).\n[33] A. I. Brown and D. A. Sivak, Proceedings of the National\nAcademy of Sciences 114, 11057 (2017).\n[34] U. Seifert, Physical Review Letters 95, 040602 (2005).\n[35] V. Lecomte, C. Appert-Rolland, and F. Van Wijland,\nJournal of statistical physics 127, 51 (2007).\n[36] F. Ritort, Advances in Chemical Physics 137, 31 (2008).\n[37] J. P. Garrahan, R. L. Jack, V. Lecomte, E. Pitard, K. van\nDuijvendijk, and F. van Wijland, Journal of Physics A:\nMathematical and Theoretical 42, 075007 (2009).\n[38] T. Speck, A. Engel, and U. Seifert, Journal of Statisti-\ncal Mechanics: Theory and Experiment 2012, P12001\n(2012).\n[39] V. Lecomte, A. Imparato, and F. v. Wijland, Progress of\nTheoretical Physics Supplement 184, 276 (2010).\n[40] R. J. Harris, Journal of Statistical Mechanics: Theory\nand Experiment 2015, P07021 (2015).\n[41] H. B. Callen, Thermodynamics and an introduction to\nthermostatistics (John Wiley & Sons, New York, 1985),\n2nd ed.\n[42] S. Carnot, Reﬂections on the motive power of ﬁre by Sadi\nCarnot and other papers on the Second Law of Thermo-\n11\ndynamics by E. Clapeyron and R (1824).\n[43] M.\nS.\nSilberberg,\nPrinciples\nof\ngeneral\nchemistry\n(McGraw-Hill Higher Education New York, 2007).\n[44] R. S. Sutton, A. G. Barto, F. Bach, et al., Reinforcement\nlearning: An introduction (MIT press, 1998).\n[45] T. Finkelstein, Tech. Rep., AIAA-94-3951-CP (1829).\n[46] M. Mozurkewich and R. S. Berry, Journal of Applied\nPhysics 53, 34 (1982).\n[47] E. Bøhn, E. M. Coates, S. Moe, and T. A. Johansen,\nin 2019 International Conference on Unmanned Aircraft\nSystems (ICUAS) (IEEE, 2019), pp. 523–533.\n[48] L. C. Melo and M. R. O. A. M´aximo, in 2019 Latin amer-\nican robotics symposium (LARS), 2019 Brazilian sympo-\nsium on robotics (SBR) and 2019 workshop on robotics\nin education (WRE) (IEEE, 2019), pp. 37–42.\n[49] H. Wei, X. Liu, L. Mashayekhy, and K. Decker, in 2019\nIEEE Vehicular Networking Conference (VNC) (IEEE,\n2019), pp. 1–8.\n[50] L. Zhang, Y. Zhang, X. Zhao, and Z. Zou, Image and\nVision Computing 108, 104126 (2021).\n[51] L. De Vree and R. Carloni, IEEE Transactions on Neural\nSystems and Rehabilitation Engineering 29, 607 (2021).\n[52] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and\nP. Moritz, in International conference on machine learn-\ning (2015), pp. 1889–1897.\n",
  "categories": [
    "cs.NE",
    "cond-mat.stat-mech",
    "cs.LG",
    "physics.comp-ph"
  ],
  "published": "2019-03-20",
  "updated": "2021-11-22"
}