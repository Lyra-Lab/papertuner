{
  "id": "http://arxiv.org/abs/1903.09366v2",
  "title": "Macro Action Reinforcement Learning with Sequence Disentanglement using Variational Autoencoder",
  "authors": [
    "Heecheol Kim",
    "Masanori Yamada",
    "Kosuke Miyoshi",
    "Hiroshi Yamakawa"
  ],
  "abstract": "One problem in the application of reinforcement learning to real-world\nproblems is the curse of dimensionality on the action space. Macro actions, a\nsequence of primitive actions, have been studied to diminish the dimensionality\nof the action space with regard to the time axis. However, previous studies\nrelied on humans defining macro actions or assumed macro actions as repetitions\nof the same primitive actions. We present Factorized Macro Action Reinforcement\nLearning (FaMARL) which autonomously learns disentangled factor representation\nof a sequence of actions to generate macro actions that can be directly applied\nto general reinforcement learning algorithms. FaMARL exhibits higher scores\nthan other reinforcement learning algorithms on environments that require an\nextensive amount of search.",
  "text": "Macro Action Reinforcement Learning with Sequence Disentanglement using\nVariational Autoencoder\nHeecheol Kim1∗, Masanori Yamada2∗, Kosuke Miyoshi3 and Hiroshi Yamakawa4\n1,3,4Dwango Artiﬁcial Intelligence Laboratory\n2NTT Secure Platform Laboratories\n1h-kim@isi.imi.i.u-tokyo.ac.jp\n2masanori.yamada.cm@hco.ntt.co.jp\n3miyoshi@narr.jp\n4hiroshi yamakawa@dwango.co.jp\nAbstract\nOne problem in the application of reinforcement\nlearning to real-world problems is the curse of di-\nmensionality on the action space. Macro actions,\na sequence of primitive actions, have been studied\nto diminish the dimensionality of the action space\nwith regard to the time axis. However, previous\nstudies relied on humans deﬁning macro actions or\nassumed macro actions as repetitions of the same\nprimitive actions.\nWe present Factorized Macro\nAction Reinforcement Learning (FaMARL) which\nautonomously learns disentangled factor represen-\ntation of a sequence of actions to generate macro\nactions that can be directly applied to general rein-\nforcement learning algorithms. FaMARL exhibits\nhigher scores than other reinforcement learning al-\ngorithms on environments that require an extensive\namount of search.\n1\nIntroduction\nReinforcement learning has gained signiﬁcant attention re-\ncently in both robotics and machine-learning communities\nbecause of its potential of wide application to different do-\nmains.\nRecent studies have achieved above-human level\ngame play in Go [Silver et al., 2016; Silver et al., 2017] and\nvideo games [Mnih et al., 2015; OpenAI, 2018]. Application\nof reinforcement learning to real-world robots has also been\nwidely studied [Levine et al., 2016; Haarnoja et al., 2018].\nReinforcement learning involves learning the relationship\nbetween a state and action on the basis of rewards. Rein-\nforcement learning fails when the dimensionality of a state or\naction increases. This is why reinforcement learning is of-\nten considered data inefﬁcient, i.e., requiring a large number\nof trials. The curse of dimensionality on the state space is\npartially solved using a convolutional neural network (CNN)\n[Krizhevsky et al., 2012; Mnih et al., 2015]; training pol-\nicy from raw image input has become possible by applying a\nCNN against the input states. However, reducing the dimen-\nsionality on the action side is still challenging. The search\n∗Both authors equally contributed to this paper.\nspace can be exponentially wide with a longer sequence and\nhigher action dimension.\nApplication of macro actions to reinforcement learning has\nbeen studied to reduce the dimensionality of actions.\nBy\ncompressing the sequence of primitive actions, macro actions\ndiminish the search space. Previous studies deﬁned macro\nactions as repetitions of the same primitive actions [Sharma\net al., 2017] or requiring humans to manually deﬁne them\n[Hausknecht and Stone, 2015]. However, more sophisticated\nmacro actions should contain different primitive actions in\none sequence without humans having to manually deﬁning\nthese actions.\nWe propose Factorized Macro Action Reinforcement\nLearning (FaMARL), a novel algorithm for abstracting the\nsequence of primitive actions to macro actions by learn-\ning disentangled representation [Bengio, 2013] of a given\nsequence of actions, reducing dimensionality of the action\nsearch space. Our algorithm uses Factorized Action Varia-\ntional Autoencoder (FAVAE) [Yamada et al., 2019], a varia-\ntion of VAE [Kingma and Welling, 2013], to learn macro ac-\ntions from given expert demonstrations. Using the acquired\ndisentangled latent variables as macro actions, FaMARL\nmatches the state with the latent variables of FAVAE instead\nof primitive actions directly. The matched latent variables are\nthen decoded into a sequence of primitive actions and applied\nrepeatedly to the environment. FaMARL is not limited to just\nrepeating the same primitive actions multiple times, because\nthis compresses any kind of representation with FAVAE. We\nexperimentally show that FaMARL can learn environments\nwith high dimensionality of the search space.\n2\nRelated work\nApplying a sequence of actions to reinforcement learning has\nbeen studied [Sharma et al., 2017; Vezhnevets et al., 2016;\nLakshminarayanan et al., 2017; Durugkar et al., 2016].\nFine Grained Action Repetition (FiGAR) successfully adopts\nmacro actions into deep reinforcement learning [Sharma et\nal., 2017], showing that Asynchronous Advantage Actor-\nCritic (A3C)[Mnih et al., 2016], an asynchronous variant of\ndeep reinforcement learning algorithm, with a learning time\nscale of repeating the action as well as the action itself scores\nhigher than that with primitive actions in Atari 2600 Games.\narXiv:1903.09366v2  [cs.LG]  4 Jun 2019\nThere are mainly two differences between FaMARL and\nFiGAR. First, FiGAR can only generate macro actions that\nare the repeat of the same primitive actions. On the other\nhand, macro actions generated with FaMARL can be a com-\nbination of different primitive actions because FaMARL ﬁnds\na disentangled representation of a sequence of continuous ac-\ntions and uses the decoded sequence as macro actions. Sec-\nond, FaMARL learns how to generate macro actions and op-\ntimizes the policy for the target task independently, while\nFiGAR learns both simultaneously. Despite FaMARL can-\nnot learn macro actions end-to-end, this algorithm can easily\nrecycle acquired macro actions to new target tasks, because\nmacro actions are acquired independent to target tasks.\nHausknecht proposed using a parameterized continu-\nous action space in the reinforcement learning framework\n[Hausknecht and Stone, 2015]. This approach, however, is\nlimited in the fact that the action has to be selected at ev-\nery time step, and humans need to parameterize the action.\nFaMARL can be viewed as an expansion of this model to\ntime series.\n3\nSequence-Disentanglement Representation\nLearning by Factorized Action Variational\nAutoEncoder\nVAE [Kingma and Welling, 2013] is a generative model that\nlearns probabilistic latent variables z via the probability dis-\ntribution learning of a dataset. VAE encodes data x to latent\nvariable z and reconstructs x from z.\nThe β-VAE [Higgins et al., 2017] and CCI-VAE [Burgess\net al., 2018], which is an improved β-VAE, are models for\nlearning the disentangled representations. These models dis-\nentangle z by adding the constraint to reduce the total corre-\nlation to VAE. FAVAE [Yamada et al., 2019] is an extended\nβ-VAE model to learn disentangled representations from se-\nquential data.\nFAVAE has a ladder network structure and\ninformation-bottleneck-type loss function. This loss function\nof FAVAE is deﬁned as\n−Eqφ(z|(x1:T )) [log pθ (x1:T |z)]\n+ β\nX\n˜l\n\f\fDKL (qφ (z| (x1:T )) ||p (z))˜l −C˜l\n\f\f ,\n(1)\nwhere p (z) = N (0, 1), ˜l is the index of the ladder, β is a\nconstant greater than zero that encourages disentangled repre-\nsentation learning by weighting Kullback-Leibler divergence\nterm, and C is called information capacity for supporting the\nreconstruction. In the learning phase, C increases linearly\nalong with epochs from 0 to Clast. The Clast is determined\nby ﬁrst training FAVAE with a small amount of β (we used\nβ = 0.1) and Clast = 0. The last value of Dkl(q(z|x)||p(z))\nis used as Clast. Each ladder requires a C . For example, a\n3-ladder network requires 3 Cs.\n4\nProposed algorithm\nOur objective is to ﬁnd factorized macro actions from given\ntime series of expert demonstrations and search for the opti-\nmal policy of a target task based on these macro actions in-\nstead of primitive actions. The target task can differ from\nAlgorithm 1 Unsupervised segmentation of macro actions\nInput:\nExpert demonstration D ←⟨A1, A2, ..., An⟩on\nBase, where Ai ←⟨ai1, ai2, ..., aim⟩(m ←length of ith\nepisode)\nParameter: Encoder qseg\n1: Dslice ←⟨⟨d11, d12, ...⟩, ⟨d21, d22, ...⟩, ..., ⟨dn1, dn2, ...⟩⟩\n// Slice all Ai with WindowSize ←4\n2: Train qseg(dij) with dij ∋Dslice\n3: distanceij ←|qseg(dij) −qseg(dij−1)|\n4: Segment Ai ∋D with distancei\n5: Dseg ←⟨⟨x11, x12, ...⟩, ⟨x21, x22, ...⟩, ..., ⟨xn1, xn2, ...⟩⟩\nAlgorithm 2 Factorized macro action with proximal policy\noptimization (PPO)\nInput: Decoder of FAVAE θ\nParameter: PPO Agent ψ\n1: while converge do\n2:\nzt ∼πψ(zt|st)\n3:\n⟨at, at+1, ..., at+l⟩←pθ(bt|zt)\n4:\nrtot ←0\n5:\nfor k ←t to t + l do\n6:\nsk+1, rk ←p(ak, sk)\n7:\nrtot ←rtot + rk\n8:\nend for\n9:\nMinimize equation 4 using rtot\n10: end while\nthe task that the expert demonstrations are generated. We use\nFAVAE [Yamada et al., 2019] to ﬁnd factorized macro ac-\ntions. The details of FaMARL are given in Sections 4.1 and\n4.2.\nOne might be curious why we do not apply expert demon-\nstrations or their segmentations, directly to the reinforcement\nlearning agent to learn a new task. There are two reasons for\nlearning disentangled factors of (segmented) expert demon-\nstrations. First, if the agent explores these expert demonstra-\ntions only, it can only mimic expert demonstrations to solve\nthe task, which results in serious deﬁciencies in generalizing\nmacro actions. Consider a set of demonstrations containing\nactions of ⟨turn right 70◦, turn right 60◦, ..., turn right 10◦\n⟩. If the environment requires the agent to turn right 80◦,\nthe agent cannot complete the task. On the other hand, latent\nvariables trained with the expert demonstrations acquire gen-\nerated macro actions to ”turn right x◦. Thus, the agent can\neasily adapt to the target task. Second, without latent vari-\nables, the action space is composed by listing only all expert\ndemonstrations, forming a discrete action space. This causes\nthe curse of dimensionality, detering fast convergence on the\ntask.\n4.1\nUnsupervised segmentation of macro actions\nAn episode of an expert demonstration is composed of a se-\nries of macro actions, e.g., when humans show a demonstra-\ntion of moving an object by hand, that demonstration is com-\nposed of 1)extending a hand to the object, 2)grasping the ob-\nFigure 1: Overview of FaMARL\nject, 3)moving the hand to the target position, and 4)releasing\nthe object.\nTherefore, expert demonstrations ﬁrst need to be seg-\nmented into each macro action. One signiﬁcant challenge is\nthat there are usually no ground-truth labels for macro ac-\ntions. One possible solution is to ask experts to label their ac-\ntions. However, this is another burden and incurs additional\ncost.\nLee proposed a simple method using an AE [Hinton and\nSalakhutdinov, 2006; Vincent et al., 2008] to segment sig-\nnal data [Lee et al., 2018]. This method, simply speaking,\ntrains an AE with sliding windows of signal data, acquiring\nthe temporal characteristics of the sliding windows. Then,\nthe distance between the encoded features of two adjacent\nsliding windows is calculated. All the peaks of the distance\ncurve are selected as segmentation points. One advantage of\nthis method is that it is not domain-speciﬁc. This method can\nbe easily applied to expert demonstration data since it is as-\nsumed that there are no speciﬁc data characteristics.\nOn our implementation of this segmentation method, dis-\ntance is deﬁned as |qseg(ai) −qseg(ai−1)|, where qseg(aij)\nrefers to the encoded feature of the jth sliding window on ith\ntrajectory data. We used a sliding window size of 4. Any\ndistance point that is highest among 10 adjacent points with\na margin of 0.05 is selected as a peak.\n4.2\nLearning disentangled latent variables with\nFAVAE\nOnce the expert demonstrations are segmented, FAVAE learns\nfactors that compose macro actions. However, FAVAE can-\nnot directly intake segmented macro actions.\nThis is be-\ncause segmented macro actions may have different lengths,\nwhile FAVAE cannot compute data with different lengths be-\ncause it uses a combination of 1D convolution and multi-\nlayer perceptron which requires an uniﬁed data size across\nall datasets. To address this issue, macro actions are padded\nwith trailing zeros to match the data length of L, the input\nsize of FAVAE. Also, two additional dimensions actionon\nand actionoff are added to macro actions to identify if ac-\ntion ak at timestep k is a real action or zero-padded one. The\nactionon is ⟨10, 11, 12, ...1l, 0l+1, 0l+2, ...0L⟩and actionoff\nis ⟨00, 01, 02, ...0l, 1l+1, 1l+2, ...1L⟩, where subscript l is the\nlength of a macro action and subscript L is the input size\nof FAVAE. The cutting point of a real action against zero-\npadding is computed by the ﬁrst timestep where actionoff\nis selected from the softmax of actionon and actionoff. We\nused the mean squared error for reconstruction loss. Also,\nFAVAE used three ladders and CCI is applied. [Burgess et\nal., 2018].\n4.3\nLearning policy with proximal policy\noptimization (PPO)\nOur key idea of diminishing the search space is to search on\nthe latent space of the macro actions instead of primitive ac-\ntions directly. We used proximal policy optimization (PPO)\n[Schulman et al., 2017] as the reinforcement learning algo-\nrithm, although any kind of reinforcement learning algorithm\ncan be used1.\nPPO is used following the loss function:\nLCLIP (ψ) = Et[min(ρt(ψ) ˆ\nAt, clip(ρt(ψ), 1 −ϵ, 1 + ϵ) ˆ\nAt) (2)\nHere, ρt(ψ) = πψ(at|st)\nπold(at|st), where ρt denotes the probability\nratio.\nIntegrating PPO with macro actions generated with FAVAE\nis simply to replace the primitive action of every time step\nwith the macro action with a step interval l which is the length\nof the macro action. Therefore, the model of the environment\nwith respect to a macro action is:\nst+l,\nt+l\nX\nk=t\nrk ∼p(zt, st)\n(3)\nwhere p(zt, st) is the transition model of the environment.\nThe PPO agent matches a latent variable zt on input state\nst.\nThe decoder φ of FAVAE then decodes zt into series of\nactions: ⟨at, at+1, at+2, ..., at+L⟩, where subscript L is the\noutput length of the decoder. Then actions are trimmed using\nthe value of the softmax of actionon and actionoff, which is\nalso decoded from the decoder.\nThe macro action is cropped to ⟨at, at+1, ..., at+l⟩where\nsubscript l is the ﬁrst timestep at which actionoff is selected.\nThis macro action is applied to the environment without feed-\nback. Rewards between t and t + l are summed and regarded\nas the reward for the result of output zt+l.\n1Our\nimplementation\nof\nPPO\nis\nbased\non\nhttps://github.com/Anjum48/rl-examples\n(a) Base\n(b) Maze\nFigure 2: ContinuousWorld tasks\nThus, the objective function of PPO can be modiﬁed as:\nLCLIP (ψ) = Et′[min(ρt′(ψ) ˆ\nAt′, clip(ρt′(ψ), 1 −ϵ, 1 + ϵ) ˆ\nAt′) (4)\nwhere t′ is the time step from the perspective of the macro\naction. If t and t′ indicate the same time step in the environ-\nment, the relationship of t + l = t′ + 1 is established.\n5\nExperiments\nFaMARL was tested in two environments: ContinuousWorld,\na simple 2D environment with continuous action and state\nspaces, and RobotHand, a 2D environment with simulated\nrobot hand made by Box2D, a 2D physics engine2.\n5.1\nContinuousWorld\nThe objective with this environment is to ﬁnd the optimal tra-\njectory from the starting position (blue dot in Figure 2) to the\ngoal position (red dot in Figure 2). The reward of this envi-\nronment is −|x −g|, where x is the position of the agent and\ng is the position of the goal. The action space is deﬁned by\nthe ⟨acceleration to the x axis and acceleration to the y axis\n⟩.\nThere are two tasks in ContinuousWorld: Base and Maze.\nIn Base, the agent and goal are randomly placed at the cor-\nners, top or bottom. Thus, the number of cases for initial-\nization is 2 ∗2 = 4. To acquire factors of macro actions re-\ngardless of scale, the size of map is selected between [2.5, 5]\nrandomly. In Maze, the agent and goal are always placed at\nthe same position. However, the entrances in the four walls\nare set randomly for each episode so that the agent has to ﬁnd\nan optimal policy on different entrance positions. This makes\nthis environment difﬁcult because walls act like strong local\noptima of reward; the agent has to make a long detour with\nlower rewards to ﬁnally ﬁnd the optimal policy.\nOur purpose was to ﬁnd disentangled macro actions from\nexpert demonstrations in Base and applying the macro ac-\ntions to complete the target tasks. 100 episodes of the expert\ndemonstrations were generated in Base using programmed\nscripts.\nWe compared four different scripts: DownOnly,\nDown&Up, PushedDownOnly, and PushedDown&Up. All\nscripts are illustrated in Figure 3. For DownOnly, the goal\nis only initialized at the bottom of the aisle; therefore, the\nmacro actions do not include upward movements. On the\n2Dataset and other supplementary results are available at\nhttps://github.com/FaMARLSupplement/FaMARLSupplement\n(a)\n(b)\n(c)\n(d)\nFigure 3: Examples of script trajectories.\nDownOnly uses only\ntrajectories in 3a, Down&Up uses those in 3a and 3b, Pushed-\nDownOnly uses those in 3c, and PushedDown&Up uses those in\n3c and 3d\n(a) Comparison among different actions\n(b) Example trajectories of macro\nactions.\nColor change indi-\ncates change in macro action\nFigure 4: Results of Maze\nother hand, Down&Up does not limit the position of the goal;\nthus, upward and downward movements are included in the\nmacro actions. For PushedDownOnly and PushedDown&Up,\nthe agent always accelerates upward or downward, according\nto the goal position.\nWith the expert demonstrations generated in Base, we\nused FaMARL in Maze.\nWe used β\n=\n50.\nAmong\nFaMARL with macro actions acquired from expert demon-\nstrations of PushedDownOnly, PPO with primitive actions,\nand FiGAR, FaMARL performed best for this task and other\ntwo algorithms failed to converge (Figure 4a).\nIt is also\nobvious that the choice of macro action is critical. While\nPushedDownOnly outperformed the primitive action, other\nmacro actions could not complete the task. Because Pushed-\nDownOnly does not contain any demonstrated actions of\nmoving upwards, this can dramatically diminish the action\nspace to search. On the other hand, Down&Up is similar to\njust repeatedly moving one direction, which was not sufﬁ-\n(a) (3,1): Node that learned factor (b) (2,1): Node that did not learn\nany factor\nFigure 5: Examples of latent traversal on (Ladder, Index of z) of\nDown&Up\ncient for completing the task.\nFigure 5 shows visualized example trajectories of latent\ntraversal for Down&Up.\nLatent traversal is a technique\nthat shifts only one latent variable and ﬁxes the other vari-\nables for observing the decoded output from the modiﬁed\nlatent variables. If disentangled factor representation is ac-\nquired, the output shows meaningful changes.\nOtherwise,\nchanges are not distinguishable. Also, if the number of la-\ntent variables exceeds that of factors that form the sequence\nof actions, only some of the latent variables acquire fac-\ntors and the others show no changes when traversed. Fig-\nure 5a shows that the 1st variable of the 3rd ladder changed\nto ⟨−3.0, −1.0, +1.0, +3.0⟩. This changed the direction of\nthe agent’s trajectory, while Figure 5b shows no change. This\nresult indicates that FAVAE learns the disentangled represen-\ntation of a given sequence of actions.\nComparison among different β of equation 1 and numbers\nof expert trajectories are shown in Figure 6 using Pushed-\nDownOnly. Figure 6a illustrates the experiment with differ-\nent β. FAVAE did not learn factors in a disentangled manner\nwhen β was low. The entangled latent variables of macro\nactions severely deters matching the state space with macro\naction space for an optimal policy because the latent space,\nwhich actually matches with the state space, is distorted. On\nContinuousWorld, we found that β ≥1.0 is enough to com-\nplete Maze. Figure 6b illustrates the experiment with differ-\nent numbers of expert trajectories. Even though we used 100\nexpert trajectories across all experiments, the number of tra-\njectories did not impact the performance of FaMARL.\n5.2\nRobotHand\nRobotHand has four degrees of freedom (DOFs), i.e., mov-\ning along the x axis, moving along the y axis, rotation,\nand grasping operation. The entire environment was built\nwith Box2D https://box2d.org/ and rendered with OpenGL\nhttps://www.opengl.org/. Similar to Base task at Continuous-\nWorld, Base at RobotHand, which is a pegging task, provides\n100 expert demonstrations to learn disentangled macro ac-\ntions. And the target tasks Reaching and BallPlacing are com-\npleted with the acquired macro actions. We used β = 0.1 on\nthis environment.\nBase (Figure 7a) is a pegging task.\nIn Base, the robot\nmoves a rod from a blue basket to a red one. We chose this\ntask because the pegging task is complex enough to contain\n(a) Comparison among different β on PushedDownOnly\n(b) Comparison among different numbers of expert trajecto-\nries of PushedDownOnly\nFigure 6: Comparison between different β and numbers of expert\ntrajectories in Maze\n(a) Base\n(b) Reaching\n(c) BallPlacing\nFigure 7: RobotHand tasks\nall macro actions that might be used in target tasks.\nReaching (Figure 7b) is a simple task. The robot hand has\nto reach for a randomly generated goal position (red) as fast\nas possible. To make this task sufﬁciently difﬁcult, we used a\nsparse reward setting in which the robot hand only receives a\npositive reward of +100 for reaching the goal position within\na distance of 0.5 m; otherwise there is a time penalty of -1.\nIn BallPlacing (Figure 7c), the robot hand has to carry the\nball (blue) to the goal position (red). The ball is initialized at\nrandom positions within a certain range, and the goal position\nis ﬁxed. The reward is deﬁned by −|b −g| where b is the po-\nsition of the ball and g is the position of the goal. An episode\nends when the ball hits the edges or reaches the goal position\nwithin a distance of 0.5 m. An additional reward of +200 is\ngiven when the ball reaches the goal.\nFigure 8 is a comparison of FaMARL, PPO with primi-\ntive actions, and FiGAR on both Reaching and BallPlacing.\nPPO with primitive actions and FiGAR respectively failed to\nlearn Reaching and BallPlacing, while FaMARL successfully\nlearned both tasks. Because the reward of Reaching is sparse,\n(a) Reaching\n(b) BallPlacing\nFigure 8: Comparison of FaMARL, PPO with primitive actions, and\nFiGAR in RobotHand tasks\nusing primitive actions fails to ﬁnd rewards. on the other\nhand, even though the reward of BallPlacing is not sparse,\nit requires precisely controlling a ball to the goal., FiGAR,\nwhich repeats the same primitive actions a number of times,\ncould not precisely control the ball. FaMARL is the only al-\ngorithm that completed both tasks.\nIt should be noted that in the RobotHand experiments\nFaMARL optimized its behavior by shortening macro ac-\ntions, while fully using the advantages of exploring with\nmacro actions. In Reaching, the average length of macro ac-\ntions gradually diminished (Figure 9b). However, when time\npenalty (in Reaching, time penalty of -1 was added to the re-\nward at every time step) is eliminated, the length of a macro\naction did not diminish (Figure 9). This is because the agent\ndid not need to optimize its policy in accordance with speed.\nA macro action can be inefﬁcient in optimizing policy com-\npare to a primitive action because the optimal policy for the\ntask may not match macro actions, but a suboptimal policy\nwill. That is why FaMARL gradually uses primitive-like ac-\ntions (macro actions with lengths of 1 3) instead of keeping\nmacro actions with dozens of primitive actions.\n6\nLimitations of FaMARL\nFaMARL exhibits generally better scores than using primitive\nactions. However, there are limitations with FaMARL.\n6.1\nLack of feed-back control\nSearching on macro actions instead of primitive actions fa-\ncilitates searching on the action space in exchange for fast\n(a) Reaching with time penalty\n(b) Reaching without time penalty\nFigure 9: Average macro action length and rewards in Reaching\nwith/without time penalty\nresponse to unexpected changes in state.\nWe failed to\ntrain BipedalWalker-v23 with FaMARL based on the ex-\npert demonstration at BipedalWalker-v2. Because a bipedal-\nlocomotion task requires highly precise control for balancing\ninduced from instability of the environment; thus, diminish-\ning the search space by macro actions in exchange for faster\nresponse was not adequate.\n6.2\nCompatibility of macro actions with task\nFigure 4 shows that the type of macro actions is critical. If the\ntargeted task does not require the macro actions that are ab-\nstracted from expert demonstrations, FaMARL will easily fail\nbecause the actions an optimal policy requires are not present\nin the acquired macro actions. Thus, choosing appropriate\nexpert demonstrations for a targeted task is essential for trans-\nferring macro actions to target tasks.\n7\nDiscussion\nWe proposed FaMARL, an algorithm of using expert demon-\nstrations to learn disentangled latent variables of macro ac-\ntions to search on these latent spaces instead of primitive ac-\ntions directly for efﬁcient search. FaMARL exhibited higher\nscores than other reinforcement learning algorithms in tasks\nthat require extensive iterations of search when proper ex-\npert demonstrations are provided. This is because FaMARL\ndiminishes the searching space based on acquired macro ac-\ntions. We consider this a promising ﬁrst step for practical\n3https://gym.openai.com/envs/BipedalWalker-v2/\napplication of macro actions in reinforcement learning in a\ncontinuous actions space. However, FaMARL could not com-\nplete a task that requries actions outside of macro actions. the\ntasks that need actions outside of restricted searching space\ncannot be solved. Possible solutions include searching opti-\nmal policy with both macro actions and primitive actions.\nReferences\n[Bengio, 2013] Yoshua Bengio. Deep learning of represen-\ntations: Looking forward. In International Conference on\nStatistical Language and Speech Processing, pages 1–37.\nSpringer, 2013.\n[Burgess et al., 2018] Christopher P Burgess, Irina Higgins,\nArka Pal, Loic Matthey, Nick Watters, Guillaume Des-\njardins, and Alexander Lerchner. Understanding disentan-\ngling in β-vae. arXiv preprint arXiv:1804.03599, 2018.\n[Durugkar et al., 2016] Ishan P Durugkar, Clemens Rosen-\nbaum, Stefan Dernbach, and Sridhar Mahadevan. Deep\nreinforcement learning with macro-actions. arXiv preprint\narXiv:1606.04615, 2016.\n[Haarnoja et al., 2018] Tuomas Haarnoja, Aurick Zhou, Se-\nhoon Ha, Jie Tan, George Tucker, and Sergey Levine.\nLearning to walk via deep reinforcement learning. arXiv\npreprint arXiv:1812.11103, 2018.\n[Hausknecht and Stone, 2015] Matthew Hausknecht and Pe-\nter Stone. Deep reinforcement learning in parameterized\naction space. arXiv preprint arXiv:1511.04143, 2015.\n[Higgins et al., 2017] Irina Higgins, Loic Matthey, Arka Pal,\nChristopher Burgess, Xavier Glorot, Matthew Botvinick,\nShakir Mohamed, and Alexander Lerchner.\nbeta-vae:\nLearning basic visual concepts with a constrained varia-\ntional framework. In International Conference on Learn-\ning Representations, 2017.\n[Hinton and Salakhutdinov, 2006] Geoffrey E Hinton and\nRuslan R Salakhutdinov. Reducing the dimensionality of\ndata with neural networks. science, 313(5786):504–507,\n2006.\n[Kingma and Welling, 2013] Diederik P Kingma and Max\nWelling. Auto-encoding variational bayes. arXiv preprint\narXiv:1312.6114, 2013.\n[Krizhevsky et al., 2012] Alex Krizhevsky, Ilya Sutskever,\nand Geoffrey E Hinton. Imagenet classiﬁcation with deep\nconvolutional neural networks. In Advances in neural in-\nformation processing systems, pages 1097–1105, 2012.\n[Lakshminarayanan et al., 2017] Aravind\nS\nLakshmi-\nnarayanan, Sahil Sharma, and Balaraman Ravindran.\nDynamic\naction\nrepetition\nfor\ndeep\nreinforcement\nlearning. In AAAI, pages 2133–2139, 2017.\n[Lee et al., 2018] Wei-Han Lee, Jorge Ortiz, Bongjun Ko,\nand Ruby Lee. Time series segmentation through auto-\nmatic feature learning. arXiv preprint arXiv:1801.05394,\n2018.\n[Levine et al., 2016] Sergey Levine, Chelsea Finn, Trevor\nDarrell, and Pieter Abbeel. End-to-end training of deep\nvisuomotor policies.\nThe Journal of Machine Learning\nResearch, 17(1):1334–1373, 2016.\n[Mnih et al., 2015] Volodymyr Mnih, Koray Kavukcuoglu,\nDavid Silver, Andrei A Rusu, Joel Veness, Marc G Belle-\nmare, Alex Graves, Martin Riedmiller, Andreas K Fidje-\nland, Georg Ostrovski, et al. Human-level control through\ndeep reinforcement learning.\nNature, 518(7540):529,\n2015.\n[Mnih et al., 2016] Volodymyr Mnih, Adria Puigdomenech\nBadia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim\nHarley, David Silver, and Koray Kavukcuoglu.\nAsyn-\nchronous methods for deep reinforcement learning. In In-\nternational conference on machine learning, pages 1928–\n1937, 2016.\n[OpenAI, 2018] OpenAI. Openai ﬁve. https://blog.openai.\ncom/openai-ﬁve/, 2018.\n[Schulman et al., 2017] John Schulman, Filip Wolski, Pra-\nfulla Dhariwal, Alec Radford, and Oleg Klimov.\nProx-\nimal policy optimization algorithms.\narXiv preprint\narXiv:1707.06347, 2017.\n[Sharma et al., 2017] Sahil Sharma, Aravind S Lakshmi-\nnarayanan, and Balaraman Ravindran.\nLearning to re-\npeat: Fine grained action repetition for deep reinforcement\nlearning. arXiv preprint arXiv:1702.06054, 2017.\n[Silver et al., 2016] David Silver, Aja Huang, Chris J Maddi-\nson, Arthur Guez, Laurent Sifre, George Van Den Driess-\nche, Julian Schrittwieser, Ioannis Antonoglou, Veda Pan-\nneershelvam, Marc Lanctot, et al.\nMastering the game\nof go with deep neural networks and tree search. nature,\n529(7587):484, 2016.\n[Silver et al., 2017] David\nSilver,\nJulian\nSchrittwieser,\nKaren Simonyan, Ioannis Antonoglou, Aja Huang, Arthur\nGuez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian\nBolton, et al. Mastering the game of go without human\nknowledge. Nature, 550(7676):354, 2017.\n[Vezhnevets et al., 2016] Alexander Vezhnevets, Volodymyr\nMnih, Simon Osindero, Alex Graves, Oriol Vinyals, John\nAgapiou, et al.\nStrategic attentive writer for learning\nmacro-actions. In Advances in neural information process-\ning systems, pages 3486–3494, 2016.\n[Vincent et al., 2008] Pascal\nVincent,\nHugo\nLarochelle,\nYoshua Bengio, and Pierre-Antoine Manzagol. Extract-\ning and composing robust features with denoising autoen-\ncoders. In Proceedings of the 25th international confer-\nence on Machine learning, pages 1096–1103. ACM, 2008.\n[Yamada et al., 2019] Masanori Yamada,\nKim Heecheol,\nKosuke Miyoshi, and Hiroshi Yamakawa.\nFavae: Se-\nquence disentanglement using information bottleneck\nprinciple. arXiv preprint arXiv:1902.08341, 2019.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.RO",
    "stat.AP",
    "stat.ML"
  ],
  "published": "2019-03-22",
  "updated": "2019-06-04"
}