{
  "id": "http://arxiv.org/abs/1903.11260v2",
  "title": "Small Data Challenges in Big Data Era: A Survey of Recent Progress on Unsupervised and Semi-Supervised Methods",
  "authors": [
    "Guo-Jun Qi",
    "Jiebo Luo"
  ],
  "abstract": "Representation learning with small labeled data have emerged in many\nproblems, since the success of deep neural networks often relies on the\navailability of a huge amount of labeled data that is expensive to collect. To\naddress it, many efforts have been made on training sophisticated models with\nfew labeled data in an unsupervised and semi-supervised fashion. In this paper,\nwe will review the recent progresses on these two major categories of methods.\nA wide spectrum of models will be categorized in a big picture, where we will\nshow how they interplay with each other to motivate explorations of new ideas.\nWe will review the principles of learning the transformation equivariant,\ndisentangled, self-supervised and semi-supervised representations, all of which\nunderpin the foundation of recent progresses. Many implementations of\nunsupervised and semi-supervised generative models have been developed on the\nbasis of these criteria, greatly expanding the territory of existing\nautoencoders, generative adversarial nets (GANs) and other deep networks by\nexploring the distribution of unlabeled data for more powerful representations.\nWe will discuss emerging topics by revealing the intrinsic connections between\nunsupervised and semi-supervised learning, and propose in future directions to\nbridge the algorithmic and theoretical gap between transformation equivariance\nfor unsupervised learning and supervised invariance for supervised learning,\nand unify unsupervised pretraining and supervised finetuning. We will also\nprovide a broader outlook of future directions to unify transformation and\ninstance equivariances for representation learning, connect unsupervised and\nsemi-supervised augmentations, and explore the role of the self-supervised\nregularization for many learning problems.",
  "text": "QI et al.: SMALL DATA CHALLENGES IN BIG DATA ERA: A SURVEY OF RECENT PROGRESS ON UNSUPERVISED AND SEMI-SUPERVISED METHODS\n1\nSmall Data Challenges in Big Data Era: A\nSurvey of Recent Progress on Unsupervised\nand Semi-Supervised Methods\nGuo-Jun Qi, Senior Member, IEEE, and Jiebo Luo, Fellow, IEEE,\nAbstract—Representation learning with small labeled data have emerged in many problems, since the success of deep neural\nnetworks often relies on the availability of a huge amount of labeled data that is expensive to collect. To address it, many efforts have\nbeen made on training sophisticated models with few labeled data in an unsupervised and semi-supervised fashion. In this paper, we\nwill review the recent progresses on these two major categories of methods. A wide spectrum of models will be categorized in a big\npicture, where we will show how they interplay with each other to motivate explorations of new ideas. We will review the principles of\nlearning the transformation equivariant, disentangled, self-supervised and semi-supervised representations, all of which underpin the\nfoundation of recent progresses. Many implementations of unsupervised and semi-supervised generative models have been developed\non the basis of these criteria, greatly expanding the territory of existing autoencoders, generative adversarial nets (GANs) and other\ndeep networks by exploring the distribution of unlabeled data for more powerful representations. We will discuss emerging topics by\nrevealing the intrinsic connections between unsupervised and semi-supervised learning, and propose in future directions to bridge the\nalgorithmic and theoretical gap between transformation equivariance for unsupervised learning and supervised invariance for\nsupervised learning, and unify unsupervised pretraining and supervised ﬁnetuning. We will also provide a broader outlook of future\ndirections to unify transformation and instance equivariances for representation learning, connect unsupervised and semi-supervised\naugmentations, and explore the role of the self-supervised regularization for many learning problems.\nIndex Terms—Unsupervised methods, semi-supervised methods, domain adaptation, transformation equivariance and invariance,\ndisentangled representations, generative models, auto-encoders, generative adversarial networks, auto-regressive models, ﬂow-based\ngenerative models, transformers, self-supervised methods, teach-student models, instance discrimination and equivariance.\n!\n1\nINTRODUCTION\nT\nHIS paper aims at a comprehensive survey of recent pro-\ngresses on unsupervised and semi-supervised methods\naddressing the challenges of training models with a small\nnumber or none of labeled data when a large volume of\nunlabeled data are available. The success of deep learning\noften hinges on the availability of a large number of labeled\ndata, where millions of images are labeled to train the deep\nneural networks [1], [2] to enable these models to be on par\nwith or even surpass the human performances.\nHowever, in many cases, it is challenging to collect a\nsufﬁciently large number of labeled data, and this inspires\nmany research efforts on exploring the unsupervised in-\nformation beyond labeled data to train robust models for\nvarious learning tasks.\n•\nUnlabeled data. While the number of labeled data\nwould be extremely small, unlabeled data could be\nremarkably big. The distribution of those unlabeled\ndata provides important clues on learning robust\nrepresentations that are generalizable to new learn-\ning tasks. The unlabeled data can be leveraged in\nboth an unsupervised and a semi-supervised fashion,\n•\nG.-J. was with the Laboratory for MAchine Perception and LEarning\n(maple-lab.net) and the Futurewei Technologies, Bellevue, WA, 98004.\nE-mail: guojunq@gmail.com\n•\nJ. Luo was with the Department of Computer Science, University of\nRochester, Rochester, NY 14627.\nE-mail: jluo@cs.rochester.edu\ndepending on whether additional labeled examples\nare leveraged to train models. Unlabeled data can\nalso assist models to close the domain gap between\ndifferent tasks, and this leads to a large category of\nunsupervised and semi-supervised domain adapta-\ntion approaches.\n•\nAuxiliary tasks. Auxiliary tasks can also be lever-\naged to mitigate small data problems as an important\nsource of side information. For example, a related\ntask can be a learning problem on a disjoint set of\nconcepts that are related to the target task. This falls\ninto the category of Zero-Shot Learning (ZSL) and\nFew-Shot Learning (FSL) problems. In a generalized\nsense, the ZSL problem can be viewed as an unsuper-\nvised learning problem with no labeled example on\nthe target task, while the FSL is semi-supervised with\nfew available labeled data. Both aim to transfer the\nsemantic knowledge or the knowledge of learning\n(e.g., meta-learning [3], [4], [5]) from the source tasks\nto the target ones.\nThe focus of this survey is on the unsupervised and\nsemi-supervised methods by exploring the unlabeled ex-\namples to address the small data problem. Although we\nwill not review the ZSL and FSL methods that leverage the\ninformation from auxiliary tasks, it would be beneﬁcial for\nus to start by looking at all these methods in a big picture.\nThis will give us a better understanding of where we are in\narXiv:1903.11260v2  [cs.CV]  2 Jan 2021\nQI et al.: SMALL DATA CHALLENGES IN BIG DATA ERA: A SURVEY OF RECENT PROGRESS ON UNSUPERVISED AND SEMI-SUPERVISED METHODS\n2\nFig. 1: An overview of the landscape of unsupervised and semi-supervised methods. This ﬁgure shows the relations\nbetween different methods, and where they intersect with each other. Please refer to Figure C.3 for the categorization of\nthese methods for this survey.\nthe journey towards conquering the small data challenges.\nDifferent ways of leveraging various sources of informa-\ntion lead to a wide spectrum of learning methods to address\nthe challenge of few labeled data from different perspectives\nas illustrated in Figure 1. In the appendix, we also provide\na chart of unsupervised and semi-supervised learning in\nSection C.3.\n1.1\nUnsupervised Methods\nAt the leftmost end of the spectrum are unsupervised\nmethods trained without labeled data. These unsupervised\nmethods seek to learn representations that are sufﬁciently\ngeneralizable to adapt to various learning tasks in future.\nIn this case, the representations learned from unsupervised\nmethods are usually assessed based on the performances of\ndownstream classiﬁcation tasks on top of these representa-\ntions.\nA variety of principles and models have been devoted\nto training unsupervised representations. As shown in Fig-\nure C.3, we will review them from several different per-\nspectives. First, we will review the emerging principle of\nTransformation Equivariant Representations(TER) pioneered in\nHinton’s seminal work [6] as well as the recent formula-\ntion of unsupervised training of such representations [7].\nIt follows by reviewing a number of generative networks\nrepresentative in many recent models, including the vari-\nants of auto-encoders, Generative Adversarial Nets (GANs),\nFlow-based Generative Networks, and Transformers (See\nFigure C.3). The principle of learning disentangling represen-\ntations from these generative models is also central to many\nunsupervised methods, and we will review them on how to\nextract interpretable generative factors from unlabeled data.\nFinally, self-supervised methods constitutes a large category\nof unsupervised models, and we will review autoregressive\nmodels as well as the self-supervised training of image and\nvideo representations.\nZero-Shot Learning (ZSL) also sits on the left end of\nthe spectrum by mining the auxiliary tasks usually on a\ndisjoint set of concepts. Compared with the pure unsuper-\nvised methods, it often explores the semantic correlations\nbetween concepts by word embedding and visual attributes,\nand uses them to transfer the knowledge from the source\nto the target concepts. Given a new sample, the zero-shot\nlearning can assign it to an unseen concept with its semantic\nembedding closest to the representation of the sample. The\nZSL is unsupervised because the training examples are not\nlabeled on the unseen concepts that the ZSL aims to learn.\nWe refer the interested readers to a more detailed review of\nZSL methods [8].\n1.2\nSemi-Supervised Methods\nAlong the spectrum to the right are the semi-supervised\nmethods, which explore both unlabeled and labeled exam-\nples to train the models. The idea lies in that unlabeled ex-\namples provides important clues on how data are generally\ndistributed in the space, and a robust model can be trained\nby exploring this distribution. For example, a robust model\nought to make stable and smooth predictions under ran-\ndom transformations (e.g., translations, rotations, ﬂipping\nor even random perturbations by a GAN [9]) along the\ndirection of data manifold, or avoid from placing its decision\nboundary on high density areas of data distribution.\nAlong this direction, as shown in Figure C.3, we will\nreview semi-supervised generative models extending their\nunsupervised counterparts, such as semi-supervised auto-\nencoders and GANs, as well as their disentangled represen-\ntations. A variety of teach-student models will also be re-\nviewed by encouraging the consistency between the teacher\nand student models on both labeled and unlabeled data to\ntrain semi-supervised models. They can be categorized by\ndifferent ways of the teacher models being obtained – by\neither applying random and adversarial perturbations [10],\n[11] to or averaging over an ensemble of student models\n[10].\nIn the spectrum of semi-supervised methods also resides\nFew-Shot Learning (FSL) when auxiliary tasks on a disjoint\nQI et al.: SMALL DATA CHALLENGES IN BIG DATA ERA: A SURVEY OF RECENT PROGRESS ON UNSUPERVISED AND SEMI-SUPERVISED METHODS\n3\nset of concepts are leveraged to improve the model training.\nOn one hand, it is like zero-shot learning when conceptual\ncorrelations can be used to share information between dif-\nferent concepts through their embedded representations. On\nthe other hand, a group of auxiliary tasks can be sampled\nfrom a collection of base concepts, and a meta-model can be\ntrained to distill the knowledge of how to update models\nwith few examples (e.g., the initial point, and the rule of\nupdating model parameters) [4] along with the unlabeled\nexamples in a semi-supervised fashion [5], [12], [13]. Thus,\nthe FSL can be viewed as a semi-supervised problem that\nhas few labeled examples available on the target concepts,\nalong with many examples labeled on auxiliary concepts\n(which thus should be viewed as unlabeled on the target\nconcepts). For a comprehensive review of FSL, the interested\nreaders can refer to [14].\n1.3\nConnections between Unsupervised and Semi-\nSupervised Learning\nWe\nwill\nshow\nthat\nexisting\nunsupervised\nand\nsemi-\nsupervised methods share many common ideas and prin-\nciples. One of core ideas in both methods lies in exploring\nthe crucial role of unlabeled data and the distributions in\nunsupervised and semi-supervised training of representa-\ntions, no matter if labeled data are involved. For example,\nboth unlabeled and labeled data can be augmented under\nvarious forms of transformations and noises to explore\ntheir invariance and equivariance. Such data augmentations\nunderly many unsupervised and semi-supervised methods\nto regularize the model training [7], [10], [15], [16], [17],\n[18], [19], [20] or ﬁnd the model vulnerability to make it\nmore robust [11], [21], [22]. Many unsupervised models such\nas Auto-Encoders, GANs and disentangled representations\nhave also been tailored into semi-supervised counterparts\nby conditioning on both data and labels.\nIndeed, unsupervised and semi-supervised methods\nshare many common principles that are surprisingly insight-\nful and important, which deserves our attentions in future\ndirections. We will sort out these principles in Section 5 and\noutline its future directions. In particular, we will discuss\nemerging topics to bridge the algorithmic and theoretical\ngap between transformation equivariance for unsupervised\nlearning and transformation invariance for supervised learn-\ning, and combine unsupervised pretraining and supervised ﬁne-\ntuning. We will also provide an outlook of future directions\nto unify transformation and instance equivariances for repre-\nsentation learning, connect unsupervised and semi-supervised\naugmentations, and explore the role of the self-supervised reg-\nularization for various learning problems. We expect a more\ngeneral learning theory and framework can be developed\nto reveal the connections between unsupervised and semi-\nsupervised learning.\nThe remainder of this paper is organized as follows. Un-\nsupervised methods will be reviewed in Section 2, followed\nby a survey of semi-supervised methods in Section 3. We\nwill also review unsupervised and semi-supervised domain\nadaptations in Section 4. After reviewing existing works,\nwe will elaborate on emerging topics and future directions\nin Section 5 that will connect unsupervised and semi-\nsupervised learning in multiple future directions. Finally,\nwe will conclude the survey in Section 6.\n2\nUNSUPERVISED METHODS\nIn this section, we will survey the literature on learning\nunsupervised representations. The goal of training an un-\nsupervised representation from unlabeled examples is to\nensure it can generalize to new tasks in future.\nWe will start the review with the emerging principle of\nlearning Transformation Equivariant Representations, to a\nvariety of representative generative models and their disen-\ntangled representations of interpretable generative factors,\nand to various self-supervised methods for training image\nand video representations.\n2.1\nUnsupervised Representation Learning\nThe methods for training unsupervised representations\nroughly fall into the following three groups of research.\n•\nTransformation-Equivariant Representations. Re-\ncently, learning transformation-equivariant represen-\ntations (TERs) from unlabeled data has attracted\nmany attentions in both unsupervised and super-\nvised methods. In particular, a good TER equivaries\nwith different types of transformations so that the\nscene structure in an image can be compactly en-\ncoded into its representation. Then the successive\nproblems for recognizing unseen visual concepts can\nbe performed on top of the trained TER. The notion\nof TER was originally proposed by Hinton et al. [6] in\nintroducing capsule nets and it has been formalized\nin various ways. We will review it in Section 2.2.\n•\nGenerative Models. Auto-Encoders, Generative Ad-\nversarial Nets and many other generative models\nhave been widely studied in unsupervised learning\nproblems, from which compact representations can\nbe learned to characterize the generative process for\nunlabeled data. We will review the learning and in-\nference problems for these models, as well as discuss\nthe disentanglement of the resultant representations\ninto generative factors that can interpret both intrin-\nsic and extrinsic data variations. More generative\nmodels besides the auto-encoders and GANs will\nalso be reviewed in Section 2.3.\n•\nSelf-Supervised Methods. There also exist a large\nvariety of self-supervisory signals to train models\nwithout access to any labeled data, including auto-\nregressive models that are self-supervised to recon-\nstruct data themselves. We will review different gen-\nres of self-supervisory signals for learning unsuper-\nvised representations in Section 2.4.\nWe also evaluate these unsupervised methods in Section A\nin the appendix.\n2.2\nTransformation-Equivariant Representations\nBefore we start to review the methods of unsupervised\nrepresentation learning, it is beneﬁcial to ponder over what\nproperties ought to be possessed by a good representation,\nparticulary from the great success of the Convolutional\nNeural Networks (CNNs). This should lay the foundation\nfor the practices in learning unsupervised representations.\nAlthough a solid theory is still lacking, it is thought that\nboth equivalence and invariance to image translations play\nQI et al.: SMALL DATA CHALLENGES IN BIG DATA ERA: A SURVEY OF RECENT PROGRESS ON UNSUPERVISED AND SEMI-SUPERVISED METHODS\n4\na critical role in the success of CNNs, particularly for su-\npervised classiﬁcation tasks [1], [6]. A typical Convolutional\nNeural Network (CNN) consists of two parts: the feature\nmaps of input images through multiple convolutional layers,\nand the classiﬁer of fully connected layers mapping the feature\nmaps to the target labels.\nWhile the resultant feature maps are equivariant to the\ntranslation of an input image, the fully connected classiﬁer\nshould be predict labels invariant to any transformations.\nBefore the concept of learning Transformation-Equivariance\nRepresentations (TER) was proposed by Hinton et al. [6],\n[23], [24], [25], most attentions have been paid on the trans-\nformation invariance criterion to train supervised models\nby minimizing the classiﬁcation errors on labeled images\naugmented with various transformations [1]. Unfortunately,\nit is impossible to directly apply transformation invari-\nance to train an unsupervised representation – without the\nguidance of label supervision, this would lead to a trivial\nrepresentation invariant to all examples.\nThus, it is a natural choice to adopt the transforma-\ntion equivariance as the criterion to train an unsupervised\nrepresentation, hoping it could be generalizable to unseen\ntasks without knowledge their labels. This is contrary to the\ncriterion of transformation invariance that tends to tailor\nthe learned representations more specialized to the labels\nof given tasks. Indeed, it is straightforward to see that the\nfeature maps generated through convolutional layers equiv-\nary with the translations – the feature maps of translated\nimages are also shifted in the same way subject to edge\npadding effect [1]. This inspires many works to generalize\nthis idea to consider more types of transformations beyond\ntranslations (e.g., general image warping and projective\ntransformations) [23]. This can learn a good representation\nof images by encoding their intrinsic visual structures that\nequivary with many transformations.\nAlong this line of research, Group-Equivariant Convo-\nlutions (GEC) [23] have been proposed by directly train-\ning feature maps as a function of different transformation\ngroups. The resultant feature maps are proved to equiv-\nary exactly with designated transformations. However, the\nform of group-equivariant convolutions is strictly deﬁned,\nwhich limits the ﬂexibility of its representation in many\napplications. Alternatively, a more ﬂexible way to enforce\ntransformation equivariance is explored by maximize the\ndependency between the resultant representations and the\nchosen transformations, which results in Auto-Encoding\nTransformation (AET) [7]. Compared with GEC, the AET\ndoes not exactly comply with the criterion of transformation\nequivariance, in pursuit for the ﬂexibility in the form of\nunsupervised representations.\n2.2.1\nGroup-Equivariant Convolutions\nConsider a group G, which could consist of compositions of\nvarious transformations such as rotations, translations, and\nmirror reﬂections. The goal of Group-Equivariant Convolu-\ntion (GEC) is to produce feature maps that equivary to all\ntransformations g ∈G from the group.\nTo formally introduce the concept of transformation\nequivariance, we can view an input image and a feature\nmap f as a function over an image grid Z2,\nf : Z2 →R,\nwhere f(p) gives the feature at a pixel location p. For\nsimplicity, we only consider a single channel feature map,\nbut it can be directly extended to multi-channel scenario\nwithout any difﬁculty.\nWhen a transformation g ∈G is applied to f, it results\nin a transformed image or feature map Lgf: [Lgf](x) =\n[f ◦g−1] = f(g−1x). Then we say a convolution with a\nkernel ﬁlter ψ is transformation equivariant to g, if [Lgf] ⋆\nψ = Lg[f ⋆ψ], that is the convolution with a transformed\ninput equals to the transformation of the convolution with\nthe original input.\nTo enable the transformation equivariance, in GEC, a\nfeature map is considered as a function of the group G, that\nis deﬁned as f : G →R.\nThen the group convolution with an input image f on\nZ2 is deﬁned as [f ⋆ψ](g) = P\ny∈Z2 f(y)ψ(g−1y), yielding a\ngroup convolved feature map [f ⋆ψ] deﬁned over G. Thus,\nall the feature maps after the input image are functions of G,\nand the group convolution of such a feature map f with a\nﬁlter ψ is deﬁned as [f ⋆ψ](g) = P\nh∈G f(h)ψ(g−1h), where\nthe ﬁlter ψ is also deﬁned on G. If we restrict the group G to\ntranslation, it is not hard to show that the group convolution\nreduces to a conventional convolution.\nCohen and Welling [23] have proved the transformation\nequivariance of the above group convolutions,\n[[Luf] ⋆ψ] (g) = [Lu[f ⋆ψ]] (g)\nwhere [Luf](h) = f(u−1h) deﬁnes the operator Lu of\napplying a transformation u to the input f. This shows the\nconvolution of a transformed input is equal to the trans-\nformation of the convolved input, i.e., the transformation\nequivariance.\nThe group convolutions are often trained in a supervised\nfashion to represent images together with some classiﬁca-\ntion layers (e.g., fully connected and softmax layers) in a\nneural network [23]. In principle, unsupervised training of\nthem can also be performed by treating them as the encoder\nin an auto-encoder architecture. Moreover, there exists an ef-\nﬁcient implementation by decomposing group convolutions\ninto a ﬁlter transformation and a planar convolution [23].\nThe idea of training group-equivariant representations\nhas been extended to explore the transformation equivari-\nance in more scenarios. For example, the group equivariant\ncapsule nets combine the group-equivariant convolutions\nwith the dynamic routing mechanism to train capsule nets\n[26]; Spherical images are analyzed in the SO(3) group by\nSpherical CNNs [27], while the equivariance properties of\nsteerable representations have be studied in the SO(2) group\nby Steerable CNNs [28]. For more implementation details,\nwe refer the readers to [23], [26], [28].\n2.2.2\nAuto-Encoding Transformations\nAlthough group convolutions guarantee the transformation\nequivariance mathematically, they have a much restricted\nform of feature maps as a function of the considered trans-\nformation group. In many applications, we often prefer\nmore ﬂexible forms of representations that can be trained\nin an unsupervised fashion by exploring the distribution\nof unlabeled data. In this section, we will review the re-\ncently proposed paradigms of Auto-Encoding Transforma-\nQI et al.: SMALL DATA CHALLENGES IN BIG DATA ERA: A SURVEY OF RECENT PROGRESS ON UNSUPERVISED AND SEMI-SUPERVISED METHODS\n5\ntions (AET) [7] as well as the variational approach Auto-\nencoding Variational Transformations (AVT) [16].\nAuto-Encoding Transformations\nUnlike the conventional Auto-Encoding Data (AED)\nparadigm that learns representations by reconstructing data,\nthe AET seeks to train the unsupervised model by decoding\ntransformations from the representations of original and\ntransformed images. It assumes that if a transformation\ncan be reconstructed, the representations should contain all\nnecessary information about the visual structures of images\nbefore and after the transformation such that the represen-\ntations are transformation equivariant. Moreover, there is\nno restriction on the form of the representations, and this\nmakes it ﬂexible to choose a suitable form of representations\nfor future tasks.\nFormally, consider a transformation t sampled from a\ndistribution p(t), along with an image x drawn from a\ndata distribution p(x). By applying t to x, one transforms\nx to t(x). Then the AET aims at learning an encoder\nEθ : x 7→Eθ(x) with the parameters θ, which extracts\nthe representation Eθ(x) of the given sample x. Meanwhile,\na transformation decoder Dφ : [Eθ(x), Eθ(t(x))] 7→ˆt is\nalso learned, which estimates ˆt of the input transformation\nt by decoding it from the representations of original and\ntransformed images.\nThe learning problem of Auto-Encoding Transforma-\ntions (AET) boils down to learn the representation encoder\nEθ and the transformation decoder Dφ jointly. For this pur-\npose, the AET can be trained by minimizing the following\nreconstruction error ℓ(t,ˆt) between a transformation t and\nits estimate ˆt,\nmin\nθ,φ\nE\nt∼p(t),x∼p(x) ℓ(t,ˆt)\nwhere the estimate ˆt of the transformation is a function\nof the encoder Eθ and the decoder Dφ such that ˆt =\nDφ [Eθ(x), Eθ(t(x))], and the expectation E is taken over\nthe sampled transformations and images. Then, the network\nparameters of Eθ and Dφ are jointly updated over mini-\nbatches by back-propagating the gradient of the loss ℓ.\nIn [7], three types of transformations have been consid-\nered in the AET model: parametric transformations, GAN-\ninduced transformations and non-parametric transforma-\ntions. This shows a wide spectrum of transformations can\nbe integrated into the AET model.\nAutoencoding Variational Transformation\nFrom an information-theoretic point of view, Qi et al. [16]\npropose an alternative Auto-encoding Variational Transfor-\nmation (AVT) model that reveals the connection between the\ntransformations and representations by maximizing their\nmutual information. It assumes that a good TER ought\nto maximize its probabilistic dependency on transforma-\ntions, such that the representation contains the intrinsic\ninformation to decode the transformations when the visual\nstructures of images are transformed extrinsically.\nFormally, the representation z of a transformed image\nt(x) is speciﬁed by the mean fθ(t(x)) and the variance\nσθ(t(x)) in the AVT, such that\nz = fθ(t(x)) + σθ(t(x)) ◦ϵ\nwhere ϵ is drawn from a normal distribution N(0, I), ◦\ndenotes the element-wise product and θ is the model pa-\nrameters.\nWith this probabilistic representation, the mutual infor-\nmation can be maximized to learn θ, that is\nmax\nθ\nEx∼p(x)I(t, z|x).\n(1)\nDirectly maximizing the mutual information could be\nintractable and a variational lower bound\nI(t, z|x) ≥Epθ(t,z|x)qφ(t|z, x).\nhas been derived by introducing a surrogate transformation\ndecoder qφ(t|z, x) that is the conditional probability of the\ntransformation t on the representation z and the image x.\nThis enables us to jointly train the representation encoder\npθ and the transformation decoder qφ efﬁciently by maxi-\nmizing the above lower bound of the mutual information.\nWe refer the interested readers to [16] for more details.\n2.3\nGenerative Representations\nGenerative models, such as Generative Adversarial Nets\n[29], Auto-Encoders and their variants have emerged as\npowerful tools to extract expressive representations from\nunlabeled data in an unsupervised fashion. In this sub-\nsection, we will review several directions of representation\nlearning based on the unsupervised models, particularly\nGANs and auto-encoders as well as their representation\ndisentangling counterparts for modeling independent and\ninterpretable generative factors that are useful for many\ndownstream tasks.\nWe will show that these generative models are largely\nrelated. For example, GANs rely on learning an encoder\nto infer the representation from data [30], [31] and reduce\nmode collapse [32], while the auto-encoders can be en-\nhanced with the adversarial training to generate sharper\nreconstruction of data [33] from the whole space of latent\ncodes [34]. Various forms of disentangled representations\nare also learned based on these generative models, opening\nan active research direction towards extracting, disentan-\ngling and interpreting generative factors from representa-\ntions.\n2.3.1\nAuto-Encoders\nAuto-Encoders and many variants [35], [36], [37], [38] are the\ngenerative models seeking to reconstruct the input data by\njointly training a pair of encoder (inference component) and\ndecoder (reconstructor component). Here we will review the\nVariational Auto-Encoders (VAE) [36] as well as the Denois-\ning Auto-Encoders (DAE) [37], [38] and Contractive Auto-\nEncoders (CAE) [35], which are closely related with the\nregularization mechanisms for disentangled representations\nin Section 2.3.3 and semi-supervised methods in Section 3.2.\nVariational Auto-Encoders\nThe Variational Auto-Encoder (VAE) [36] trains an auto-\nencoder model by maximizing the variational lower bound\nof the marginal data likelihood pθ(x) of a parameterized\nmodel pθ. For this, a variational encoder qφ(z|x) is used\nto approximate the intractable posterior pθ(z|x), resulting\nQI et al.: SMALL DATA CHALLENGES IN BIG DATA ERA: A SURVEY OF RECENT PROGRESS ON UNSUPERVISED AND SEMI-SUPERVISED METHODS\n6\nin the following inequality to lower bound the marginal\nlikelihood:\nlog pθ(x) ≥Eqφ(z|x)[log pθ(x|z)] −DKL (qφ(z|x)||p(z))\nwhere p(z) is the prior of representation, and pθ(x|z) is\nthe decoder. Reparameterization trick is also introduced to\nsample from qφ(z|x) as\nz = gφ(x, ϵ) = µφ(x) + σφ(x) ⊙ϵ\nwhere ϵ is randomly drawn from a simple Gaussian dis-\ntribution with zero mean and unit deviation, and ⊙is the\nelement-wise product. In this way, the model parameters φ\nare separated from the random noises, and thus the error\nsignals can be back-propagated through the neural network\nto train the VAE.\nLater on, when reviewing the disentangled representa-\ntions in Section 2.3.3, we will see that the VAE provides a\npowerful tool to study and implement the representation\ndisentanglement to provide interpretable generative factors.\nTowards Robust Auto-Encoders\nBoth Denoising Auto-Encoders (DAE) [37] and Contrac-\ntive Auto-Encoders (CAE) [35] aim to learn robust represen-\ntations insensitive to noises on input data.\nUnlike the typical auto-encoders, the DAE [37] takes\nnoise-corrupted samples as input and attempts to recon-\nstruct original data. This forces the neural networks to learn\nthe robust representations that can be used to recover the\nuncorrupted clean data. There are many ways to corrupt\ndata. For example, some parts of input data can be randomly\nremoved and the DAE attempts to recover the missing parts;\nan image can also be randomly transformed by rotations,\ntranslations and mirror ﬂips, and the DAE aims to learn\nrobust representations from which the original image before\nthe transformation can be recovered.\nThe CAE [35] learns the robust representations in a dif-\nferent way. Rather than relying on a decoder to reconstruct\nthe original data in the DAE, the CAE directly penalizes\nthe changes of representations learned by the encoder E\nin presence of the small perturbations on input data. This\nresults in the following penalty on the Frobenius norm of\nJacobi matrix around an input sample x to train the CAE\n∥JE(x)∥2\nF =\nX\ni,j\n\u0012∂Ei(x)\n∂xj\n\u00132\nwhere Ei denotes the ith element of the encoded represen-\ntation of x.\nThe idea of regularizing the model training by adding\nnoises to the model input or even model itself has led to\nmany regularization methods to train robust supervised and\nunsupervised models. Adversarial noises can be even more\ncapable of training robust classiﬁers than random noises\nby encouraging smooth predictions on both labeled and\nunlabeled examples that are adversarially affected. We will\ntake a closer look at them in the context of semi-supervised\nmethods in Section 3.2.\n2.3.2\nGAN-based Representations\nIn a GAN model, data are generated from the noises fed into\nits generator, and thus these noises can be viewed as the\nnatural representations of data produced by the generator.\nConsidering the proved results [39], [40] that many GAN\nvariants have the generalized ability of generating data with\nindistinguishable distribution from that of real examples,\nthe GAN representations are also complete for all real data.\nHowever, there exists a challenge that given a real sam-\nple, we have to invert the generator to obtain the noise\nrepresentation corresponding to the sample. Thus, an en-\ncoder is required that can directly output the noise from\nwhich the corresponding sample can be generated and thus\nrepresented.\nFor this purpose, the idea of adversarially training a\ngenerator and its corresponding encoder has been indepen-\ndently developed in Bidirectional Generative Adversarial\nNetworks (BiGAN) [30] and Adversarially Learned Infer-\nence (ALI) [31], respectively. The idea is later integrated\ninto a regularized loss-sensitive GAN model with proved\ndistributional consistency and generalizability to generate\nreal data [41].\nBiGAN and ALI: Adversarial Representation Learning\nFormally, these methods aim to learn triple elements\nfrom a GAN model: 1) a generator G : Z →X mapping\nfrom a distribution p(z) of input noises Z to a distribution\npg(x) of generated samples X ; 2) an encoder E : X →Z\nmapping a sample x ∈X back to a noise z ∈Z such that\nideally G(z) equals to x, i.e., E is the inverse of G; 3) a\ndiscriminator D : X ×Z →[0, 1] that assigns a probability to\ndistinguish a real pair (x, E(x)) from a fake pair (G(z), z).\nCompared with the classic GAN, there are two major\ndifferences. First, the encoder is the extra element for rep-\nresentation learning. Second, a discriminator has a joint\nsample-noise pair rather than a single sample as input to\ndistinguish real from fake pairs.\nThe above triple elements can be jointly trained with a\nminimax objective\nmin\nG,E max\nD V (D, E, G)\n(2)\nwhere\nV (D, E, G) ≜Ex∼p(x)[log D(x, E(x))]\n+ Ez∼p(z)[log(1 −D(G(z), z))]\nand p(x) is the real data distribution. This minimax problem\ncan be solved by the alternating gradient based methods like\nin training the classic GAN [29].\nDonahue et al. [30] have proved in an ideal case, the\nresultant encoder E inverts the generator G almost every-\nwhere, i.e., E = G−1 (See Theorem 2 in [30]). Moreover,\nit has also been shown that the joint training of E and\nG in (2) is performed by minimizing the ℓ0 loss of auto-\nencoders (See Theorem 3 of [30]), which makes E a desired\nrepresentation model for its input samples.\nMore Related Works\nBesides BiGAN and ALI, there exist other hybrid meth-\nods jointly training auto-encoders and GANs to perform\nadversarial representation learning and inference in an in-\ntegrated framework [32], [33], [34], [42], [43].\nFor example, Larsen et al. [33] use the intermediate\nrepresentation from the GAN’s discriminator to measure\nQI et al.: SMALL DATA CHALLENGES IN BIG DATA ERA: A SURVEY OF RECENT PROGRESS ON UNSUPERVISED AND SEMI-SUPERVISED METHODS\n7\nthe similarity between reconstructed and input images as\nthe reconstruction error to train the VAE. Alternatively,\nadversarial autoencoders [34] have been proposed to train\nthe VAE by matching the aggregated posterior q(z) =\nR\nx q(z|x)p(x)dx of the noises from the data distribution\np(x) with that of prior distribution p(z). The match between\ndistributions is performed by training a discriminator to\ntell q(z) apart from p(z) and guide the encoder q(z|x) to\nproduce the aggregated posterior indistinguishable from the\nprior. In this way, the training of VAE is regularized to\nensure the decoder yields a generative model that maps the\ngiven prior to the desired data distribution.\nThe marriage between VAE and GAN has also been\nexplored to relieve the mode collapse problem. For example,\nSrivastava et al. [32] train an encoder (called reconstructor\nin that paper) to invert the generator, and reduce the mode\ncollapse of generated samples by having the distribution\nof encoded data match with the input Gaussian noise. The\nassumption is if mode collapses occur, it is unlikely for\nthe reconstructor to map all generated samples back to the\ndistribution of original Gaussian noises, and this results\nin a strong learning signal to train both generator and\nreconstructor.\nHuang et al. [43] have taken a further step by introducing\nan IntroAVE model with the posterior q(z|x) as the discrim-\ninator directly to distinguish between real and fake data.\nSpeciﬁcally, the posterior of z conditioned on real samples\nx is encouraged to match the prior p(z), while that of z\non the generated samples is supposed to deviate from p(z).\nThen, the generator can be trained to generate samples by\nmatching the posterior with the prior. It has been shown\nthat IntroAVE is able to generate the data indistinguishable\nfrom real samples.\n2.3.3\nDisentangled Representations\nDisentangling representations [44] has been proposed to\nfacilitate downstream tasks by providing interpretable and\nsalient attributes to depict data. Bengio et al. [45] propose\nthat a small subset of the latent variables in a disentangled\nrepresentation ought to change as data change in response\nto real-world events and transformations.\nFor example, a set of meaningful attributes, such as facial\nexpressions, poses, eye colors, hairstyles, genders and even\nidentities, can be separately allocated to disentangle facial\nimages, and they can be extremely useful for solving future\nrecognition problems without having to be exposed to some\nsupervised data. This suggests good representations that\nare generalizable to natural supervised tasks ought to be\nas disentangle as possible to provide a rich set of factorized\nattributes to depict data.\nInfoGAN: Disentangling GAN-based Representation\nThe effort on disentangling representations has led to the\nInfoGAN [44] and its variants [46], [47] in literature to train\ngenerative models that can create data from disentangled\nrepresentations. Speciﬁcally, the InfoGAN assumes there are\ntwo types of noise variables fed into its generator: 1) a\nvector of incompressible noises z, which do not factorize\ninto any semantic representations and could be used by the\ngenerator in an entangled fashion as in the conventional\nGAN; 2) a vector of latent codes c, which represent salient\ndisentangled information about the generated sample x and\nwill not be lost during the generative process.\nThus, the assumption of the InfoGAN is to maximize\nthe mutual information between latent codes c and the\ngenerated samples G(z, c) by combining combination these\ntwo types of noises. It should prevent the generator from\nignoring the dependency on the latent codes that contain the\nsalient knowledge about the generated samples. The mutual\ninformation I(c, G(z, c)) is maximized over the generator G\nto train the InfoGAN along with the minimax objective of\nthe conventional GAN. A tractable variational lower bound\nof I(c, G(z, c)) is derived by a surrogate distribution q(c|x)\nto approximate the true posterior p(c|x):\nI(c, G(z, c)) ≥Ec∼p(c),x∼G(z,c)[log q(c|x)] + H(c)\nwhere p(c) is the prior distribution on latent codes and H(c)\nis its entropy. More details on the InfoGAN can be found in\n[44].\nβ-VAE: Disentangling VAE Representation\nThe idea of disentangling representations has also been\nextended to other unsupervised models as well. Among\nthem is β-VAE [46], which aims to disentangle the inferred\nposterior q(z|x) by imposing a constraint on matching it\nto an isotropic Gaussian p(z) = N(0, I). It creates a latent\ninformation bottleneck on the inferred posterior by limiting\nits capacity. Such a regularization not only encourages a\nmore efﬁcient representation of data, but also disentangles\nthe representations into independent factors due to the\nisotropic prior.\nThe following objective is maximized to train the VAE\nmodel\nL(q(z|x), p(x|z)) = Eq(z|x)p(x|z) −βDKL(q(z|x)||p(z))\nwhere the positive Lagrangian multiplier β comes from the\nconstraint DKL(q(z|x)||p(z)) < ϵ.\nIt is not hard to ﬁnd when β = 1, the formulation\nreduces to the conventional VAE model. As β increases, a\nstronger constraint on the latent information bottleneck is\nenforced to control the capacity and conditional indepen-\ndence of the representation q(z|x). A higher β would trade\noff between the reconstruction ﬁdelity of the β-VAE model\nand the disentanglement degree of the learned representa-\ntions.\nDisentanglement Metric\nTo measure the degree of disentanglement of the learned\nrepresentations, a disentanglement metric score [46] is de-\nsigned by the assumption that disentangled representations\ncould enable robust classiﬁcation of data based on their\nrepresentations even using a simple classiﬁer. A number of\nimages are generated by ﬁxing one of generative factors in\nthe representations while randomly sampling all the others.\nThen a low capable linear classiﬁer is used to identify this\nfactor and the resultant accuracy is reported as the disen-\ntanglement metric score. Obviously, if the independence and\ninterpretability property of the disentangled representations\nhold, the ﬁxed factor should have a small variance, and thus\nthe classiﬁer ought to have high accuracy in identifying it\nand gives the high disentanglement score.\nQI et al.: SMALL DATA CHALLENGES IN BIG DATA ERA: A SURVEY OF RECENT PROGRESS ON UNSUPERVISED AND SEMI-SUPERVISED METHODS\n8\nHowever, it is argued that a linear classiﬁer could still\nbe sensitive to hyperparameters and optimizers, and its\ndisentanglement metric would suffer from a failure mode\nif only K −1 out of K factors were disentangled. To address\nit, an alternative metric is proposed [46] to directly use the\nvariance of each dimension in the resultant representation as\nthe indicator of the ﬁxed factor, and apply a majority-vote\nclassiﬁer to predict the chosen factor. This avoids tuning\noptimization hyperparameters, as well as circumvents the\nfailure mode of the other metric.\nMore Disentangled Representations\nDisentangling representations has been sought in many\nother generative models besides InfoGAN and β-VAE. The\nFactorVAE [48] proposes to minimize the Total Correla-\ntion (TC) DKL(q(z)||¯q(z)) between the aggregated poste-\nrior q(z) and its factorized form ¯q(z) = Q\nj q(zj), which\nmeasures the dependence for multiple random factors. Fol-\nlowing the density-ratio trick, a discriminator is trained to\ndistinguish samples between two posteriors and output the\nprobability of a sample z being from the true aggregated\nposterior q(z). Then the factorized VAE is trained by mini-\nmizing the VAE lower bound along with the obtained TC.\nCompared with β-VAE, the FactorVAE avoids unnecessarily\npenalizing the mutual information I(x, z) term, and thus\nyields better reconstruction of data while still sufﬁciently\ndisentangling the representations of generative factors.\nIn addition, disentangling representations has also been\nstudied in the context of semi-supervised methods [49], [50],\nwhich will be reviewed in the next section.\n2.3.4\nMore Generative Models\nFlow-based Generative Models\nThe ﬂow-based generative models [51], [52], [53] map\na random noise z drawn from a simple distribution (e.g.,\nmulti-variate Gaussian) to a data sample x through a series\nof bijective functions\nx\nf1\n←→h1 = f1(x)\nf2\n←→h2 = f2(h1) · · ·\nfK\n←→z = fK(hK−1)\nThis sequence of invertible functions is called a ﬂow. It\nallows us to compute the log-likelihood of x tractably by\nthe change of variables formula as\nlog p(x) = log p(z) +\nK\nX\ni=1\nlog | det( dhi\ndhi−1\n)|.\nThree different types of invertible ﬂow functions – Act-\nnorm, invertible convolution and afﬁne coupling layer –\nhave been adopted [53] to construct an one-step ﬂow in\na deep Generative fLOW (GLOW) model. A squeezing\noperator also deﬁnes a multi-scale structure with different\nlevels of data abstraction in the GLOW [52]. Each step in\nGLOW has a log-determinant that can be easily computed\nas it has a triangular Jacobian matrix, and thus the resultant\ndata loglikelihood can be maximized efﬁciently to train the\nmodel.\nSelf-Attention and Transformer\nThe Transformer [54] has been proposed as an alternative\nto the recurrent neural networks, and it has stacked self-\nattention layers, as well as point-wise fully connected layers\nand positional encoding to capture the dependency between\ninput and output sequences in its encoder and decoder\ncomponents.\nThe self-attention is the key. Each embedding in a se-\nquence is mapped to a tuple of query, key and value. Then\nthe output at each position is a sum of the values weighted\nby the similarity between the current query and the keys\nof the sequence. A multi-head attention is often adopted to\nlinearly project the queries, keys and values multiple times\nwith different projection weights, and the resultant outputs\nfrom these linear projections are concatenated and projected\nto the ﬁnal result.\nBesides the self-attention, each layer in the encoder and\ndecoder contains a fully connected feed-forward network\napplied to each position separately and identically. Posi-\ntional encoding by sine and cosine functions is also added to\neach embedding, which provides information about the po-\nsitions in the sequence. Transformer has become a powerful\nunsupervised representation of word embedding in natural\nlanguage tasks, and more details about the Transformer and\nits application can be found in [54], [55].\n2.4\nSelf-Supervised Methods\nA large variety of self-supervisory signals have been pro-\nposed to train unsupervised representations as well. We\nwill start by reviewing the autoregressive models as a large\ncategory of self-supervised models, and proceed to learn\nself-supervised representations for images and videos. We\nwill focus on basic ideas and principles for self-supervised\nrepresentation learning in this survey. For a more complete\nreview of self-supervised learning on multimodal audio-\nvisual data [56], [57], [58], readers can refer to [59].\n2.4.1\nAutoregressive Models\nOne of categories of self-supervised models are trained by\npredicting the context, missing or future data, and they are\noften referred to as auto-regressive models. Among them\nare PixelRNN [60], PixelCNN [61], [62], and Transformer\n[54]. They can generate useful unsupervised representations\nsince the contexts from which the unseen parts of data are\npredicted often depend on the same shared latent represen-\ntations.\nPixelRNN\nSpeciﬁcally, in the PixelRNN [60], an image is divided\ninto a regular grid of small patches, and a recurrent architec-\nture is built to predict the features of the current patch based\non its context. Three variants of RNNPixels are proposed to\ngenerate the sequence of image patches in different ways:\nRow LSTM, Diagonal BiLSTM and Multi-Scale PixelRNN.\nFor the Row LSTM [60], an image is generated row\nby row from top to bottom, and the context of a patch is\nroughly a triangle above the patch. In contrast, the Diagonal\nLSTM scans an image diagonally from a corner at the top\nand reaches the opposite corner at the bottom, and thus\nit has a diagonal context. The Multi-Scale PixelRNN [60]\nis composed of an unconditional PixelRNN and one or\nmore PixelRNN layers. An unconditional PixelRNN is ﬁrst\napplied to generate a smaller image subsampled from the\noriginal one, and then a conditional PixelRNN layer takes\nQI et al.: SMALL DATA CHALLENGES IN BIG DATA ERA: A SURVEY OF RECENT PROGRESS ON UNSUPERVISED AND SEMI-SUPERVISED METHODS\n9\nthe smaller image as input to generate the original larger\nimage. Multiple layers of conditional PixelRNN layers can\nbe stacked to progressively generate the original image from\nthe low to high resolutions.\nPixelCNN\nA disadvantage of the Row and Diagonal LSTM is the\nhigh computational cost as the feature of each patch must\nbe computed sequentially. This can be avoided by using\na convolutional structure to compute the features of all\npatches at once. Masked convolutions are used to avoid the\nviolation of conditional dependence only on the previous\nrather than future context. Compared with the PixelRNN\nwith a potentially unbounded range of dependency, the\nPixelCNN [61] comes at a cost of limiting the context of\neach patch to a bounded receptive ﬁeld. Thus multiple\nconvolutional layers can be stacked to increase the context\nsize.\nOn the other hand, gated activations have been intro-\nduced to the PixelCNN [61]. This results in a Gated Pixel-\nCNN that is able to model more complex interdependency\nbetween different patches. Moreover, the Gated PixelCNN\nis augmented with a horizontal stack conditioned on the\ncurrent row so far, as well as a vertical stack dependent on\nall previous rows. By combining the outputs of both stacks,\nthe blind spot can be avoided in the receptive ﬁeld.\nContrastive Predictive Coding and Instance Discrimina-\ntion\nAuto-regressive models can be used as a decoder in the\nauto-encoder architecture, where they are forced to output\npowerful representations useful for predicting the future\npatches. This enables us to train representations in an auto-\nregressive fashion without accessing any labeled data.\nContrastive Predictive Coding (CPC) [63] has made a\nnotable effort on training such auto-regressive models. It\naims to maximize the mutual information I(c, x) between\nthe latent representations of the context c and the future\nsample x, and thus more accurate future predictions can\nbe made by maximally sharing information through the\nsequence. More details about CPC and its application in\ntraining auto-regressive models and learning unsupervised\nfeatures can be found in [63].\nThe idea of CPC has also inspired the non-parametric in-\nstance discrimination [15]. By sampling a subset of samples\ninto a memory bank [15], queue [64] or merely minibatch\n[65], it trains an unsupervised representation by distinguish-\ning positive pairs of augmented samples from negative\nones. Formally, it minimizes the following contrast loss\nL = −\nX\nu,u′\nlog\nexp(s(u, u′))\nP\nu,v exp(s(u, v))\n(3)\nwhere u, u′ denote a positive pair of instances augmented\nfrom the same sample, u, v are a pair of instances aug-\nmented from two random samples, and s denotes a sim-\nilarity function. Thus, this loss deﬁnes a non-parametric\nsoftmax loss to discriminate positive pairs against negative\nones through their similarities.\nMore recently, together with stronger [66] and multi-\ncrop [67] augmentations, the unsupervised representation\nlearned by the contrastive learning has achieved almost the\nsame top-1 accuracy on ImageNet as its fully supervised\ncounterpart with ResNet-50 [66]. This demonstrates a note-\nworthy milestone for the great potentials of unsupervised\nlearning in absence of labeled data.\n2.4.2\nImage Representations\nIn addition to autoregressive models, self-supervised meth-\nods explore the other forms of self-supervised signals to\ntrain deep neural networks. These self-supervised signals\ncan be directly derived from data themselves without hav-\ning to manually label them.\nContexts. For example, Doersch et al. [68] use the relative\npositions of two randomly sampled patches from an image\nas self-supervised information to train the model. Pathak\net al. [69] train a context encoder to generate the contents\nof missing parts from their surroundings by minimizing\na combination of pixel-wise reconstruction error and an\nadversarial loss. Mehdi and Favaro [70] propose to train a\nconvolutional neural network by solving Jigsaw puzzles.\nColorization. Image colorization has also been used in a\nself-supervised task to train convolutional networks in lit-\nerature [71], [72]. Zhang et al. [73] present a cross-channel\nauto-encoder by reconstructing a subset of data channels\nfrom another subset with the cross-channel features being\nconcatenated as data representation.\nSurrogate classes, targets and clustering. Dosovitskiy et\nal. [74] train CNNs by classifying a set of surrogate classes,\neach of which is formed by applying various transfor-\nmations to an individual image. In contrast, Bojanowski\net al. [75] use Noise As Target (NAT) by jointly learning\nthe representation and assigning each sample to one of a\nﬁxed set of target values. Instead, Caron [76] et al. train\na DeepCluster model by iteratively clustering features and\nusing the resultant representations to update the network.\nCounting, motion and rotations. Noroozi et al. [77] learn\ncounting features that satisfy equivalence relations between\ndownsampled and tiled images. Egomotion [78] has also\nbeen used as a self-supervisory signal to model the repre-\nsentation of visual elements present in consecutive images\nto ﬁnd their correspondences when an agent moves in an\nenvironment. Gidaris et al. [79] train neural networks by\nclassifying image rotations in a discrete set. It learns a spe-\ncial case of transformation-equivariance representations as\nthe learned representation ought to encode the information\nabout them by equivarying with the applied rotations.\n2.4.3\nVideo Representations\nThe idea of self-supervision has also been employed to\ntrain feature representations for videos by exploring the\nunderlying temporal information. For example, the Arrow\nof Time (AoT) [80] has been used as the supervisory signal\nto learn the representations of videos for both high-level\nsemantics and low-level physics, while avoiding artiﬁcial\ncues from the video production rather than the physical\nworld.\nThe order of a sequence of frames can also supervise the\ntraining of video representations to capture the spatiotem-\nporal information [81]. To this end, the Tuple veriﬁcation\napproach is proposed to train a CNN model by extracting\nthe representation of individual frames and determining\nQI et al.: SMALL DATA CHALLENGES IN BIG DATA ERA: A SURVEY OF RECENT PROGRESS ON UNSUPERVISED AND SEMI-SUPERVISED METHODS\n10\nwhether a randomly sampled tuple of frames is in the\ncorrect order to disambiguate directional confusion in video\nclips.\nA disentangled representation of images has also been\nproposed by leveraging the temporal coherence between\nvideo frames. A DrNet model [82] is trained to factorize\neach frame into a stationary content representation and a\ntime-varying pose representation with an adversarial loss.\nIt assumes that the pose representation should carry no\ninformation about video identity, and the adversarial loss\nprevents the pose features from being discriminative from\none video to another. The DrNet can learn powerful content\nand pose representations that can be combined to generate\nframes further into future than existing approaches [82].\n3\nSEMI-SUPERVISED METHODS\nIn this section, we will review the semi-supervised methods\n[83], [84], [85], [86] from two different perspectives.\n•\nSemi-supervised generative models. In Section 3.1\nThe semi-supervised auto-encoders, GANs and dis-\nentangled representations will be reviewed in echo-\ning their unsupervised counterparts. We will show\nhow these semi-supervised generative models could\nbe derived from the corresponding unsupervised\ngenerative models, shedding us some light on the\nintrinsic connection between the unsupervised and\nsemi-supervised methods.\n•\nTeacher-Student models. This is a large category of\nsemi-supervised models that have achieved the state-\nof-the-art performances in literature, where a single\nor an ensemble of teacher models are trained to pre-\ndict on unlabeled examples and the predicted labels\nare used to supervise the training of a student model.\nWe will review various genres of teacher models\n– noisy teachers, teacher ensemble and adversarial\nteachers – in Section 3.2, and show how they could\nbe trained against various noise and/or adversarial\nmechanisms to build more robust semi-supervised\nmodels.\nWe also provide an evaluation of different semi-supervised\nmethods in Section B in the appendix.\n3.1\nSemi-Supervised Generative Models\nIn this section, we will review a large variety of semi-\nsupervised generative models.\n3.1.1\nSemi-Supervised Auto-Encoders\nKingma et al. [87] extend the unsupervised variational auto-\nencoders to two forms of semi-supervised models.\nThe ﬁrst latent-feature discriminative model (M1) is\nquite straightforward. On top of the latent representation\nz of a sample x by a VAE model, a classiﬁer is trained\nto predict its label. While the VAE is trained on both the\nlabeled and the unlabeled part of a training set, the classiﬁer\nis trained based on labeled examples.\nThe second generative semi-supervised model (M2) is\nmore complex. In addition to the latent representation z, a\nsample x is generated by another class variable y, which\nis latent for a unlabeled x or seen for a labeled one. The\ndata is explained by a generative process considering the\nadditional class variable:\np(y) = Cat(y|π), p(z) = N(z|0, I), pθ(x|y, z) = fθ(x, y, z)\nwhere p(y) is a multinomial distribution for the class prior.\nUnlike the VAE, the M2 introduces a pair of variational\nposteriors to infer z and y:\nqφ(z|y, x) = N(z|µφ(y, x), σ2\nφ(x)), qφ(y|x) = Cat(y|πφ(x)).\nThen the joint posterior over z and y can be inferred by\nqφ(z, y) = qφ(z|y, x)qφ(y|x).\nAmong them, qφ(y|x) can be used as the classiﬁer to pre-\ndict the label of a test sample. To train the M2, two cases are\nconsidered [87] to derive the variational lower bound of the\nmarginal distribution pθ(x, y) for labeled pairs (x, y) and\npθ(x) for unlabeled samples x, respectively. Combining the\ntwo bounds results in a maximum loglikelihood problem.\nHowever, an additional classiﬁcation cost ought to be\nadded to the ﬁnal objective function so that the classiﬁer\nqφ(y|x) is trained with both labeled and unlabeled exam-\nples. Similar to the VAE, reparameterization trick is used to\nperform the back-propagation [36].\nFinally, M1 and M2 can be combined by learning the\nM2 using the embedded representation z1 from a M1\nmodel. The M2 model has its own latent representation\nz2 along with a label variable y for each sample. This\nresults in a two-layer deep generative model to generate z1\nfrom (z2, y) and x from z1 successively: pθ(x, y, z1, z2) =\np(y)p(z2)pθ(z1|y, z2)pθ(x|z1).\nIn addition to the M1 and M2 models and the hybrid,\nthe efforts on introducing supervision information into the\nvariational auto-encoders have been made in literature [88],\n[89], [90] in different ways. Later on, we will review how to\ndisentangle representations from the semi-supervised VAEs\nby partially specifying graphical dependency between a\nsubset of random variables [90] in order to factorize and\ninterpret data variations.\n3.1.2\nSemi-Supervised GANs\nThe GANs have also been adopted to enable the semi-\nsupervised learning from two different perspectives. One of\nthem considers to train a K+1 classiﬁer with K given labels\nto classify and a fake class to represent generated samples. It\nexplores the distribution of unlabeled examples by treating\nthem as belonging to the ﬁrst K real classes, and a feature\nmatching trick is used to unleash competitive performances\n[91].\nOn the contrary, the other paradigm views the generator\nof a learned GAN model as the (local) parameterization\nof the data manifold, so that the label invariance can be\ncharacterized over the manifold along its tangents. This\nis closely related with the Laplace-Beltrami operator that\nis merely approximated by the graph Laplacian in classic\ngraph-based semi-supervised models.\nWe will review these two paradigms of semi-supervised\nGANs below.\nTraining K + 1 Classiﬁers with Feature Matching\nSalimans et al. [91] propose the improved techniques\nto train the semi-supervised GANs. By putting real and\nQI et al.: SMALL DATA CHALLENGES IN BIG DATA ERA: A SURVEY OF RECENT PROGRESS ON UNSUPERVISED AND SEMI-SUPERVISED METHODS\n11\ngenerated samples together, it trains a classiﬁer to label each\nsample to one of K real classes or a fake class. All unlabeled\ndata are classiﬁed to real examples for one of the ﬁrst K\nclasses, while the generated examples are classiﬁed to fake\nexamples. The conventional classiﬁcation cost is deﬁned\nover labeled data, which is combined with the unsupervised\nGAN loss to train the model.\nMoreover, a trick called feature matching has to be used\nto train the generator. Instead of training the generator by\nmaximizing the likelihood of its generated examples being\nclassiﬁed to the K real classes, it is trained by minimizing\nthe discrepancy between the features of the real and the\ngenerated samples extracted from an intermediate layer of\nthe classiﬁer. This trick has played a critical role in deliv-\nering the competitive performances in training the semi-\nsupervised GANs [91].\nPursuit of Label Invariance via Local GANs\nThe graph Laplacian has been widely used to charac-\nterize the change of the labels over the samples connected\nin a graph. Minimizing the graph Laplacian can make\nsmooth predictions over the labels between the connected\nsimilar samples. While the graph is used to approximate\nthe unknown data manifold, the graph Laplacian is indeed\nan approximate to the Laplace-Beltrami operator over the\nunderlying data manifold.\nIn [9], a notable effort has been made to learn localized\nGAN that deﬁnes a local generator G(x, z) around each\nsample x with z. This gives rise to the local coordinates\naround each sample x over the data manifold in which x\nis the origin, i.e., G(x, 0) = x. In this way, the entire data\nmanifold can be covered by a family of local coordinates. It\nallows us to deﬁne the gradient of a classiﬁcation function\nf(x) over the manifold as\n∇G\nx f ≜∇zf(G(x, z))|z=0 = JT\nx ∇xf(x)\nwhere Jx is the Jacobian matrix of G(x, z) at z = 0.\nWith these notations, it can be revealed that the func-\ntional gradient over the manifold is closely connected with\nthe Laplace-Beltrami operator △f ≜div(∇G\nx f) such that\nZ\nM\n∥∇G\nx f∥2dPX =\nZ\nM\nf△fdPX .\nTherefore, one can directly calculate the Laplace-Beltrami\noperator without the approximate graph-based Laplacian\nthat is often used in classic semi-supervised methods [92].\nThen the semi-supervised classiﬁer p(y|x) is trained by\nencouraging the label invariance over the data manifold by\nminimizing\nK\nX\nk=1\nEp(x)∥∇G\nx log p(y = k|x)∥2\nalong with the loss of the semi-supervised GANs [91].\nMoreover, the localized GAN allows us to explain the\nmode collapse of the generator from a geometric point of\nview as the manifold being local collapsed into a lower di-\nmensionality. Then an orthogonal constraint on the Jacobian\nmatrix can be imposed to train the generator and prevent it\nfrom collapsing on the manifold.\n3.1.3\nSemi-Supervised Disentangled Representations\nInverse Graphics Networks\nThe Deep Convolutional Inverse Graphics Network (DC-\nIGN) [49] implements a semi-supervised variational auto-\nencoder model by engineering a vision model as inverse\ngraphics. In other words, it aims to learn a collection of “\ngraphics codes” by which images can be transformed and\nrendered like in a graphics program. These graphics codes\nare viewed as disentangled representations of images.\nDC-IGN is built on top of a VAE model, but is trained in\na semi-supervised fashion. The learned representations are\ndisentangled into few extrinsic variables such as azimuth\nangle, elevation angle and azimuth of light sources, along\nwith a number of intrinsic variables depicting identity,\nshape, expression and surface textures. In a mini-batch,\nonly one of factors is varied with all other others are ﬁxed,\ngenerating the images with only one active transformation\ncorresponding to the chosen factor that are fed forward\nthrough the network. The other variables corresponding to\ninactive transformations are clamped to their mean. The\ngradients of error signals are backpropagated through the\nnetwork, while the gradients corresponding to the inactive\ntransformations are forced to their difference from the mean\nover the mini-batch, and this could train the encoder such\nthat all the information about the active transformation\nwould be concentrated on the chosen variable.\nThe DC-IGN is semi-supervised to engineer inverse\ngraphics as training images with various transformations\nare available from 3D face and chair datasets. We also note\nthat a number of inverse graphics models [93], [94], [95]\nhave been proposed to train disentangled representations.\nAmong them are deep lambertian networks [96] that assume\na Lambertian reﬂectance model and implicitly construct the\n3D representation, and transforming auto-encoders [6], [97]\nthat use a domain-speciﬁc decoder to reconstruct images,\nas well as [98] with an approximate differentiable renderer\nto explicitly capture the relationship between changes in\nmodel parameters and image observations.\nDisentangling Semi-Supervised VAEs\nIn [90], a generalized form of semi-supervised VAEs is\nproposed to disentangle interpretable variables from the\nlatent representations. It compiles the graphical model for\nmodeling a general dependency on observed and unob-\nserved latent variables with neural networks, and a stochas-\ntic computation graph [99] is used to infer with and train\nthe resultant generative model.\nFor this purpose, importance sampling estimates are\nused to maximize the lower bound of both the super-\nvised and semi-supervised likelihoods. By expanding each\nstochastic node into a subgraph, the stochastic computation\ngraph is built to train the resultant model. Speciﬁcally, a\ndistribution type and a neural network of parameter func-\ntion are speciﬁed for each node in both the generative and\ninference models. The reparameterization trick is adopted\nto sample the unsupervised and semi-supervised variables,\nand the weight of the importance sampling is calculated\nfrom the joint probability of all semi-supervised variables.\nThis model enables us to ﬂexibly specify the dependen-\ncies on the disentangled representations to interpret data\nQI et al.: SMALL DATA CHALLENGES IN BIG DATA ERA: A SURVEY OF RECENT PROGRESS ON UNSUPERVISED AND SEMI-SUPERVISED METHODS\n12\nvariations, and leave the rest unspeciﬁed ones to be learned\nin an entangled fashion.\n3.2\nTeacher-Student Models\nThe\nidea\nbehind\nteacher-student\nmodels\nfor\nsemi-\nsupervised learning is to obtain a single or an ensemble\nof teachers, and use the predictions on unlabeled examples\nas targets to supervise the training of a student model.\nConsistency between the teacher and the student is maxi-\nmized to improve the student’s performance and stability\non classifying unlabeled samples.\nVarious ways of training the teacher and maximizing the\nconsistency between the teacher and the student lead to a\nvariety of the semi-supervised models of this category.\nSpeciﬁcally, applying random noises to the input and\nhidden layers of models can be traced back to [100], [101],\n[102], which have been shown to be equivalent to adding\nextra regularization terms to the objective function. In a\nteacher-student method, a noisy teacher is obtained by feed-\ning noisy samples into a corrupted model, and the pre-\ndiction bias is minimized to train the model between the\nteacher and the student (Γ-model [17]), or between two\ncorrupted copies of the model (Π-model [10]).\nThe idea is extended to convene an ensemble of teachers\ntemporally over epochs to guide the training of their stu-\ndent. The exponential moving average of their predictions\nis used to improve the accuracy of predicted labels by the\nteacher ensemble on unlabeled examples (Temporal Ensem-\nbling [10]). Alternatively, the exponential weighted average\ncan be made over model parameters to form the predictions\nmade by the teacher ensemble (Mean Teacher [18]). Both\nmethods rely on random noises added to input samples and\nmodel parameters respectively to improve the robustness of\nexploring unlabeled data when imposing the consistency\nbetween the teacher and student models.\nRather than adding random noises, adversarial examples\nare calculated that would maximally change the predicted\nlabels by a student model. This yields an adversarial teacher,\nand the student is trained and updated by minimizing the\ndeviation from the adversarial examples by the teacher.\nThis yields the virtual Adversarial Training (VAT), which\nhas achieved the state-of-the-art performance on semi-\nsupervised learning.\nIn the following, we will elaborate on different teacher-\nstudent methods.\n3.2.1\nNoisy Teachers: Γ and Π Models\nBoth Γ and Π models are developed on the belief that a\nrobust model ought to have stable predictions under any\nrandom transformation of data and perturbations to the\nmodel [103]. This could push the decision boundary apart\nfrom training examples, and make the model insensitive to\nthe noises on the data and the model parameters. Thus, ran-\ndom noises and perturbations are added into the inputs and\nthe parameters of a student model to form a noisy teacher,\nand the deviation from the predictions by the teacher is\nminimized to train the student model.\nSpeciﬁcally, the Γ-model [17] has a multi-layered latent\nrepresentation z(l) of each layer l, and uses an auto-encoder\nto obtain an estimated ˆz(l) by denoising from the corrupted\n˜z(l). Then the sum of squared errors between the (batch-\nnormalized) estimate and the clean latent representations\nover layers\nL\nX\nl=1\nλl∥ˆz(l) −z(l)∥2\nis minimized to train the clear student model weighted\nby positive hyperparameter coefﬁcients λl across different\nlayers.\nOn the contrary, Π-model [10] is simpliﬁed by minimiz-\ning the difference between noisy outputs. In the context\nof semi-supervised learning problem, given a labeled or\nunlabeled sample x , it is corrupted by some noise and\nfed into the model perturbed by random dropout and pool-\ning schemes [103]. This process is run twice, yielding two\nversions of its outputs y′ and y′′. Then, the squared error\nbetween them is minimized to encourage the consistency\nbetween noisy outputs, combined with the classiﬁcation cost\non labeled examples to train the model. Unlike Γ-model that\nmatches a clean and a corrupted representation, Π-model\nruns the corrupted branch twice to match noisy outputs.\nHowever, both models rely on random noises to explore\ntheir resilience against noisy inputs and perturbed models,\nwhich would be ineffective in ﬁnding a competent teacher\nto train the robust models. Thus, an ensemble of teachers are\ntracked over epochs to form a more capable teacher model,\nresulting in the following temporal ensembling [10] and mean\nteacher [18] methods.\n3.2.2\nTeacher Ensemble: Temporal Ensembling and Mean\nTeacher\nTemporal Ensembling [10] and Mean Teacher [18] are similar\nto each other in tracking an ensemble of models over time to\nhave a better teacher model. However, they differ in main-\ntaining an exponential moving average over the predictions\n(temporal ensembling) by or the parameters (mean teacher)\nof the tracked models.\nFormally, consider a model y = fθ(x, η) parameterized\nby θ that outputs the prediction y for an input x under some\nnoises η added to the model parameters and/or the input.\nFor the temporal ensembling, at each epoch, the target\nprediction on a given sample x is updated in an Exponential\nMoving Average (EMA) fashion online as\ny′ ←αy′ + (1 −α)fθ(x, η)\nwith a positive smoothing coefﬁcient α. The resultant EMA\nprediction is further normalized to construct a target y for\ntraining the model by minimizing\nEx,η∥fθ(x, η) −y∥2\nAgain, this objective is combined with the classiﬁcation\ncost over mini-batches to train the model θ corrupted with\nnoise η. Since it is expensive to update the predictions over\nindividual examples for every iteration, their target values\nare updated only once per epoch, making the information\nfrom earlier models being incorporated into training the\nmodel at a slower pace.\nContrary to temporal ensembling, the mean teacher\nkeeps an EMA over the model parameters rather than\nindividual predictions\nθ′ ←αθ′ + (1 −α)θ\nQI et al.: SMALL DATA CHALLENGES IN BIG DATA ERA: A SURVEY OF RECENT PROGRESS ON UNSUPERVISED AND SEMI-SUPERVISED METHODS\n13\nwith the parameters θ of the current student model. Then\nthe student model is updated by minimizing over θ\nEx,η,η′∥fθ(x, η) −fθ′(x, η′)∥.\nWhile both temporal ensembling and mean teacher track\na collection of previous models to predict the teacher’s\ntargets to supervise the training process, they still rely on\nadding random noises to train stable models with consistent\npredictions. It has been revealed that a locally isotropic\noutput distribution around a sample cannot be achieved by\ntraining the model against randomly drawn noises without\nknowing the model’s vulnerability to adversarial noises [21].\nThis motivates an alternative method by using adversarial\nteachers [11] to supervise the training process.\n3.2.3\nMixtureMatch and FixMatch Teachers\nMixMatch and FixMatch teachers represent another cate-\ngory of student-teacher model for semi-supervised learning.\nIn MixMatch [19], the current model makes a prediction\non each unlabeled sample, which is linearly combined with\nthe groundtruth label of anther example. This results in a\npredicted label on a mixed example on the segment of the\nunlabeled and labeled samples. This mixed example will\nbe added to augment the training set to update the current\nmodel.\nFixMatch [20] further simpliﬁes MixMatch, and achieves\neven better performance. It applies a stronger and a weaker\naugmentation to a unlabeled sample, and predicts the labels\non two augmentations. It then trains by ﬁxing the label\non the weaker augmentation, and using it to teach on the\nstronger augmentation. In other words, it seek to minimize\nthe deviation between the labels on two augmentations, but\nonly backpropagate its errors through the stronger one. It\nis based on the assumption that the predicted label on the\nweaker augmentation is more likely to be true than that on\nthe stronger augmentation, and it represents a hierarchical\naugmentation strategy for semi-supervised learning.\n3.2.4\nAdversarial Teachers: Virtual Adversarial Training\nAdversarial training has been used to regularize a model\nand make it robust against adversarial examples [21], [22].\nSpeciﬁcally, the model is trained to make a smooth predic-\ntion along an adversarial direction of input examples. This\napproach has been extended to Virtual Adversarial Training\n(VAT) [11], where an adversarial direction can be sought\naround unlabeled data, along which the model is the most\ngreatly altered. This allows to train the model in a semi-\nsupervised fashion.\nFormally, consider a labeled or an unlabeled example x,\nand a parameterized model with a conditional distribution\npθ(y|x) of the output label. The VAT ﬁnds the most adver-\nsarial direction radv(x) on x by\nradv(x) = arg max\n∥r∥2≤ϵ D[pθ(y|x), pθ(y|x + r)]\nwith a divergence measure D between two distributions,\nwhere the adversarial direction is sought within a radius ϵ\naround the sample.\nThen an adversarial loss is minimized train the model\nmin\nθ\nExD[pθ(y|x), pθ(y|x + radv(x))]\nover both labeled and unlabeled examples, together with\nthe minimization of the classiﬁcation cost.\nThe adversarial direction radv(x) can be found in a\nclosed form as the ﬁrst dominant eigenvector of the Hes-\nsian matrix of D[pθ(y|x), pθ(y|x + r)] as a function of r\nat r = 0, which in turn allows a fast power iteration\nalgorithm to solve radv(x). This can be easily integrated into\nthe stochastic gradient method to iteratively update θ over\nmini-batches.\n4\nDOMAIN ADAPTATION\nWe will review the domain adaptation problem in both\nunsupervised and semi-supervised fashion.\n4.1\nUnsupervised Domain Adaptation\nOne of interesting applications of the GANs is to adapt the\nlearned representations and models from a source to a target\ndomain. Speciﬁcally, for the unsupervised domain adapta-\ntion, a set of labeled source examples S = {(xi, yi)|i =\n1, · · · , n} are sampled from the distribution pS of a source\ndomain, while there are another set of unlabeled examples\nT\n= {xi|i = 1, · · · , m} from the distribution pT of a\ntarget domain. Then the goal of the unsupervised domain\nadaptation is to learn a classiﬁer f that has a low risk\nRT = Pr(x,y)∼pT (f(x) ̸= y) on the target distribution.\nWe categorize the unsupervised domain adaptation into\nthe unsupervised method, since the target domain contains\nno supervision information although the source domain is\nsupervised.\nThere are several different approaches to the unsu-\npervised domain adaptation problem. Here we focus on\nreviewing the adversarial learning methods that are well\nrelated with the GAN models by leveraging the property\nthat it can generate samples with an indistinguishable dis-\ntribution from the target samples.\nAs outlined in [104], there are three design choices in\ndeveloping an unsupervised domain adaptation algorithm:\n1) tying weights: whether the weights are shared across the\nrepresentation models for the source and target domains; 2)\nbase model: whether a discriminative or generative model\nis adapted from the source to target domain; 3) adversarial\nobjectives that are used to train the models.\nDifferent choices have resulted in various models.\nAdversarial Discriminative Domain Adaptation\nAdversarial Discriminative Domain Adaptation (ADDA)\n[104] unties the weights of the representation models for\nsource and target domain. Instead, it learns two separate\nmodels MS and MT to map source and target samples\nto their respective representations. First, a classiﬁer f is\ntrained on top of the representation MS based on the labeled\nexamples from the source domain:\nmin\nMS,f E(x,y)∼pSℓ(f(MS(x)), y)\nwhere ℓis the classiﬁcation error on a labeled example.\nThen MS is ﬁxed, and the target representation model\nMT is trained so that both models output consistent dis-\ntributions that match each other. A GAN-based objective\nis used to achieve this by learning a domain discriminator\nQI et al.: SMALL DATA CHALLENGES IN BIG DATA ERA: A SURVEY OF RECENT PROGRESS ON UNSUPERVISED AND SEMI-SUPERVISED METHODS\n14\nD that distinguishes a source representation from its target\ncounterpart,\nmax\nD\nEx∼pS log D(MS(x)) + Ex∼pT (1 −log D(MT (x))).\nAn adversarial loss is then minimized to train the target\nrepresentation MT by confusing the domain discriminator\nthat the representations generated by MT comes from the\nsource domain:\nmax\nMT Ex∼pT log D(MT (x)).\nThe discriminator D and the target representation MT\nare optimized iteratively to convergence. Then, a test sample\nx is classiﬁed by f(MT (x)) based on the trained classiﬁer f\nand the target model MT .\nGradient Reversal Layer\nUnlike the ADDA, the Gradient Reversal Layer (GRL)\nmodel [105] chooses to tie the weights for the source and\ntarget representations (i.e., MS = MT = M). The classiﬁer\nf, the shared representation M, and the domain discrimina-\ntor D will be trained jointly.\nIt introduces the following regularizer over the shared\nM and the domain discriminator D\nmax\nD min\nM R(D, M) ≜Ex∼pS log D(M(x))\n+ Ex∼pT (1 −log D(M(x))).\nIn other words, a shared representation M is learned to map\nsamples, no matter from the source or the target domain, to\nthe same distribution such that D cannot distinguish them.\nThis regularizer is combined with the classiﬁcation loss,\nyielding the joint optimization problem\nmax\nD min\nM,f E(x,y)∼pSℓ(f(M(x)), y) + R(D, M).\n(4)\nCompared with ADDA, the classiﬁer is jointly trained\nwith the representation, and it optimizes the true minimax\nobjective that is vulnerable to the vanishing gradient [104].\n4.2\nSemi-Supervised Domain Adaptation\nThe boundary between the unsupervised and the semi-\nsupervised domain adaptations becomes blurred when ad-\nditional labeled examples are available on the target do-\nmain. For example, in the GRL, the classiﬁcation loss (4) can\nbe minimized with not only the labeled source examples but\nalso the labeled target examples.\nAlternatively, Pixel-Level Domain Adaptation (PixelDA)\n[106] chooses to directly adapt source images x ∼pS to their\ntarget counterparts with a GAN generator G(x, z) for a sam-\npled noise z to match with the target distribution pT . Then\na classiﬁer can be trained by combining the labeled adapted\nimages {(G(x, z), y)|(x, y) ∼pS} and the labeled target\nimages {(x, y)|(x, y) ∼pT } in a semi-supervised fashion.\nAdditional content similarity loss can also be minimized to\nutilize the prior knowledge regarding the image adaptation\nprocess.\nMoreover, a two-stream architecture has been proposed\n[107] to train two networks for the source and target do-\nmains simultaneously. It does not attempt to directly en-\nforce domain invariance since domain invariant features\ncould undermine the discriminator power of the learned\nclassiﬁers. Instead, it explicitly models the domain shift by\nmodeling both the similarity and the difference between the\nsource and the target data.\nSpeciﬁcally, it trains two network streams separately on\nthe labeled data from two domains. A weight regularizer\nis introduced by minimizing the difference between the\nweights of two network streams up to a linear transfor-\nmation. This encourages two related streams to model the\ndomain invariance while admitting the presence of the\ndifference between domains. Then, the domain discrepancy\ncan be minimized over the representations of source and\ntarget samples. This could be implemented by minimizing\nthe Maximum Mean Discrepancy (MMD) [108], [109], [110],\n[111] in a kernel space. In the meantime, the idea of GRL\n[105] can also be applied to train a domain classiﬁer that\nought to perform poorly when the representations for two\ndomains become indistinguishable.\n4.3\nMore Related Works\nThere exist other variants of unsupervised domain adapta-\ntion methods based on the adversarial or non-adversarial\ntraining. For example, Domain Confusion [112] proposes\nan objective under which the two untied representations\nare trained to map onto a uniform distribution by viewing\ntwo domains identically. CoGAN [113] trains two GANs\nthat generate the source and target images respectively. The\ndomain-invariance is achieved by tying high-level parame-\nters of the two GANs and a classiﬁer is trained based on the\noutput of the discriminator.\n5\nEMERGING TOPICS AND FUTURE DIRECTIONS\nNow we will discuss emerging topics on unsupervised and\nsemi-supervised learning and their future directions.\n5.1\nTransformation Equivariance vs. Invariance\nA more theoretical topic lies on revealing the intrinsic re-\nlation between transformation equivariance and invariance\nin learning representations. On the one hand, the pursuit of\nTransformation Equivariant Representation (TER) has been\nspotlighted as one of critical criteria [6] that achieves the\nstate-of-the-art performances in unsupervised learning [7],\n[16]. However, it is also important and necessary to apply\nthe transformation invariance to train discriminative net-\nworks with labeled data in supervised tasks for recognizing\nimages and objects.\nAt ﬁrst glance, it looks like a dilemma to enforce two\ncriteria simultaneously, but they actually co-exist well in\nthe celebrated convolutional neural networks underpinning\nthe great success of deep learning – the convolutional fea-\nture maps equivary to the translations while the output\npredictions should be invariant under various transforma-\ntions [1]. The recent efforts [23] on generalizing translation\nequivariance to generic transformations also present great\npotentials of training more powerful representations and\ndiscriminative models atop to address small data challenges\n[7], [16], [23].\nHowever, a deep understanding of the relationship be-\ntween transformation equivariance and transformation invari-\nance is still lacking to bridge the gap between training unsu-\npervised and supervised models. While there is no doubt on\nQI et al.: SMALL DATA CHALLENGES IN BIG DATA ERA: A SURVEY OF RECENT PROGRESS ON UNSUPERVISED AND SEMI-SUPERVISED METHODS\n15\nthe fundamental roles of transformation equivariance and\ninvariance in unsupervised and supervised model, we still\ndo not know the best way to integrate them in a coherent\nmanner.\nIndeed, the unsupervised representation learning con-\ncerns more on the generalizability to new tasks, while\nthe supervised tasks are more interested in discriminative\npower for given tasks. How can the pursuits of transforma-\ntion equivariance and invariance be suitably combined to\nreach better balance between generalization and discrimina-\ntion? Should we still separate the unsupervised learning of\ntransformation equivariant representation from the super-\nvised training for transformation invariant classiﬁers as in\nthe CNNs? We believe insightful answers to these questions\ncould lead to more transformative and efﬁcient way to\nintegrate both principles to address small data challenges\nfor new tasks emerging everyday. This is a fundamental\nquestion we would like to answer in future.\n5.2\nUnsupervised vs. Supervised Network Pretraining\nPretraining of deep networks are often a critical step before\nthey are ﬁnetuned on new tasks. For example, we often\npretrain a deep network on ImageNet and ﬁne-tune it on\nthe other image datasets for various downstream tasks\nsuch as image classiﬁcation, object detection and semantic\nsegmentation. While it has achieved huge successes, it is\noften limited to supervised pretraining on labeled datasets,\nand could result in inevitable gaps between the prelabeled\ndatasets and the downstream problems. For example, the\nprelabeled and the target datasets are often annotated with\ndifferent labels. Even worse, they could focus on various\ntasks from image-level classiﬁcation to localization of ob-\njects. In such a setting, supervised pretraining may not be\na natural solution to pretraining a deep network for unseen\nfuture tasks.\nFortunately, unsupervised training of deep networks can\nbetter generalize to downstream tasks without relying on\npre-labeled datasets. It has great advantages in not only\navoiding potential gaps to downstream tasks but also lever-\naging much larger unlabeled datasets.\n5.2.1\nUnsupervised Pretraining for Future Tasks\nThe results of unsupervised methods on cross-dataset tasks\nhave shown impressive results. As shown in Table A.4,\nthe unsupervised models pretrained on the unlabeled Im-\nageNet dataset have comparable performances to the fully\nsupervised models trained with the Places labels. More-\nover, a unsupervised deep network pretrained on ImageNet\nachieves better performance on object detection task than its\nsupervised pre-trained counterpart. This demonstrates that\nthe unsupervised pre-training is a promising alternative to\nthe supervised pretraining approach.\nThis is not surprising. Indeed, the ability of unsuper-\nvised methods is more than that. With more network capac-\nities and larger datasets, we expect the unsupervised net-\nworks can deliver more impressive results. Unsupervised\nrepresentations endowed with better generalizability to new\ntasks should beneﬁt from a greater number of unlabeled\ndata. To this end, more ambitious goals should be set\nto train more powerful unsupervised representations, and\nTABLE 1: Error rates on CIFAR-10 when different num-\nbers of labeled examples per class are used to train the\nsupervised, the semi-supervised and the downstream classi-\nﬁers for unsupervised representations. For the unsupervised\nmodels, a convolutional block is trained with the labeled\nexamples on top of the ﬁrst twod blocks of NIN and 13-layer\nnetworks pre-traine with all unlabeled data. We compare\nwith both the fully supervised and the semi-supervised\nmodels.\n20\n100\n400\n1000\n5000\nSupervised conv\n66.34\n52.74\n25.81\n16.53\n6.93\nSupervised non-linear\n65.03\n51.13\n27.17\n16.13\n7.92\nImproved GAN [91]\n–\n–\n18.63\n–\n–\nTemporal Ensembling [10]\n–\n–\n12.16\n–\n5.60\nVAT+EntMin [11]\n–\n–\n10.55\n–\n–\nΠ model\n–\n27.36\n13.20\n–\n6.06\nLocalized GAN [9]\n–\n17.44\n14.23\n–\n–\nMean Teacher [18]\n–\n21.55\n12.31\n–\n5.94\nRotNet + conv (NIN) [79]\n35.37\n24.72\n17.16\n13.57\n8.05\nAET [7]\n34.83\n24.35\n16.28\n12.58\n7.82\nAVT [16]\n35.44\n24.26\n15.97\n12.57\n7.75\nAVT (13-layers) [16]\n26.20\n18.44\n13.56\n10.86\n6.3\nmore challenging evaluation protocols on transfer learning\nscenarios should be considered in future.\n5.2.2\nUnsupervised\nPretraining\nfor\nSemi-Supervised\nLearning\nAnother aspect of unsupervised training is its ability of\nexploring both labeled and unlabeled data in a semi-\nsupervised fashion. First, a representation is trained on the\nunlabeled data, followed by training a classiﬁer on top of\nthe representation with a small number of labeled examples.\nThis differs from the classic semi-supervised methods, in\nwhich a deep network is trained jointly on both labeled and\nunlabeled data.\nIn\ncontrast,\nunsupervised\ntraining\nin\nthe\nsemi-\nsupervised setting has its own advantage by decoupling\nthe unsupervised training of a base representation from\nthe supervised training of a light-weighted classiﬁer with\nfew labeled examples. This makes it more efﬁcient to train\nunsupervised representations generalizable to new tasks,\nand could result in surprisingly competitive results.\nTable 1 compares the unsupervised methods with\nboth fully supervised and semi-supervised models. Note\nthat many compared semi-supervised and fully-supervised\nmodels are based on a 13-layer convolutional architecture\n[10], [18] on CIFAR-10, while the protocol adopted to com-\npare the unsupervised models is often based on the NIN\narchitecture. For the sake of a fair comparison, we also\nimplement the same 13-layer architecture for the AVT model\n(the last row of Table 1), where the ﬁrst two blocks of\nconvolutions are unsupervised pretrained and the last block\nis trained with varying numbers of labeled data. The result\nshows the promising potential of using unsupervised train-\ning in a semi-supervised way, since the AVT outperforms\nmany existing semi-supervised models particularly when\nthe number of labeled data is very small. We expect these\nunsupervised models could be further improved in future,\nand provide us with more ﬂexibility and generalizability\nQI et al.: SMALL DATA CHALLENGES IN BIG DATA ERA: A SURVEY OF RECENT PROGRESS ON UNSUPERVISED AND SEMI-SUPERVISED METHODS\n16\nto handle new tasks with few labeled data in such a semi-\nsupervised setting.\n5.3\nFuture Directions\nUnsupervised and semi-supervised training of deep net-\nworks are closely related with many shared aspects of\nmethodologies. As we have reviewed, unsupervised meth-\nods, such as Auto-Encoders, GANs and disentangled repre-\nsentations, all have their semi-supervised counterparts. This\nis not surprising as they provide representation models that\ncan be trained in both unsupervised and semi-supervised\nfashion. This inspires us to explore both unsupervised and\nsemi-supervised methods from an integrated point of view\nin the following directions as illustrated in Figure 2.\n5.3.1\nUnifying Instance and Transformation Equivariances\nInstance discrimination [15] and transformation prediction\n[7], [16] have emerged as two large categories of unsu-\npervised methods with leading performances in literature.\nWhile the instance discrimination was originally inspired\nby the CPC reviewed in Section 2.4, the transformation\nprediction has been largely shaped by auto-encoding trans-\nformations on 2D images [7] and 3D cloud points [114].\nThese two categories of methods approach the unsuper-\nvised learning from two distinctive dimensions of discrim-\nination: instance [15] and transformation [7], [16]. Instance\ndiscrimination attempts to learn feature representations\nwhich ought to equivary to individual instances and thus\nare discriminative to distinguish one instance from another\n[15]. In contrast, transformation prediction seeks to learn\nthe features that equivary to various transformations [16],\nwhich can be a complex composition of spatial and non-\nspatial transformations [115]. The learned representations\nshould be sufﬁciently discriminative from which applied\ntransformations can be predicted.\nThis inspires us to learn feature representations that\njointly equivary to both instances and their transformations.\nExisting works we reviewed in this paper shed a light\non unifying both directions from an information theoretic\npoint of view. For example, the contrastive loss in [63] was\nderived by maximizing the mutual information between the\nrepresentations and their contexts (e.g., if they come from\nthe same instance). On the other hand, the transformation\nequivariant representations [16] are also derived by maxi-\nmizing the mutual information with the transformations as\nin Eq. (1). This leads to a natural choice to unify both equiv-\nariances by jointly maximizing the mutual information with\nboth instances and transformations. While both equivariances\nhave shown promising results in training expressive unsu-\npervised representations, we believe an integrated solution\nhas a greater potential that deserves our special attentions in\nfuture to close the performance gap to the fully supervised\nmodels as well as improve generalizability to unseen tasks.\n5.3.2\nSemi-Supervised and Unsupervised Augmentations\nData augmentation has become a standard preprocessing\nstep in training deep networks since the introduction of\nAlexnet [1]. It aims to augment the training set with more\nvariations through different transformations such as spatial\naugmentations (e.g., random crops, mirror ﬂips, random\ntranslations), color jittering, and random noises.\nAugmentations can not only be applied to labeled exam-\nples to train supervised models (i.e., supervised data augmen-\ntation), but also play critical roles to explore variations in\nunsupervised setting (i.e., semi-supervised data augmentation).\nIn the review of semi-supervised methods, we have shown\nthe roles of these augmentations in training robust semi-\nsupervised models with noise-corrupted samples (cf. Sec-\ntion 3.2). A robust semi-supervised classiﬁers is trained to\nmake consistent predictions over the augmented examples\nin ambient spaces [103] or along the tangent directions of the\ndata manifold [9]. The semi-supervised augmentations can\nalso be added in an adversarial rather than random fashion\nto rectify the vulnerability of a model [11]. A new trend\nhas emerged to mix up the augmentations on labeled and\nunlabeled data in such a semi-supervised setting [19], or\nadopt a hierarchical data augmentation by ﬁxing weakly\naugmented data to train strongly augmented ones [20].\nWe also have unsupervised data augmentations that\nplay a crucial role in training unsupervised models. For\nexample, in instance discrimination [15] and its predecessor\nExamplar-CNN [74], an unlabeled example is augmented\ninto multiple versions under different transformations, and\nthe unsupervised model is trained by distinguishing exam-\nples from one another up to these augmentations. In con-\ntrast, the transformation prediction approaches such as the\nAET [7] train unsupervised representations by directly pre-\ndicting an applied transformation from a pair of examples\naugmented from the same instance. Thus, these methods\nform two types of augmentations in unsupervised learning\n– inter-instance augmentation for instance prediction, and\nintra-instance augmentation for transformation prediction.\nAs mentioned in the last subsection, these two types of aug-\nmentations can be uniﬁed to jointly learn representations\nequivariant to both instances and transformations.\nIn future, we will also explore the potential of applying\nsemi-supervised and unsupervised augmentations to vari-\nous learning problems. For example, as discussed below, we\ncan explore unsupervised augmentation in semi-supervised\nlearning tasks as a self-trained regularizer.\nMoreover, data augmentations can be combined with\nnetwork augmentations to make the learned models more\nrobust. For example, one can also add random or adver-\nsarial noises to networks on their weights and architectures\nto form an ensemble of augmented networks as in many\nteacher-student models we reviewed in Section 3.2. The\naugmented networks can not only expose the potential vul-\nnerability of network architectures (e.g., through adversarial\ndropout [116]), but also train more robust networks by\nexploring the change of network predictions in presence of\nnoises on the weights. We believe a combination of data\nand network augmentations can further improve the model\nrobustness to learn more generalizable representations.\n5.3.3\nSelf-Supervision As a Regularizer\nSelf-supervised learning we reviewed in Section 2.4 not only\nconstitutes a large category of unsupervised methods, but\nalso shows impressive performances in various tasks as an\nunsupervised regularizer. As illustrated in Figure 2(c), the\nidea is to train a shared backbone network jointly with a\nQI et al.: SMALL DATA CHALLENGES IN BIG DATA ERA: A SURVEY OF RECENT PROGRESS ON UNSUPERVISED AND SEMI-SUPERVISED METHODS\n17\n(a)\nUnifying\nTransformation-and-\nInstance Equivariance\n(b) Supervised vs. Unsupervised Aug-\nmentations\n(c) Self-Supervised Learning As a Regu-\nlarizer\nFig. 2: Future directions for (a) unifying transformation and instance equivariant representation learning, (b) supervised\nvs. unsupervised data augmentations, and (c) self-supervised learning as a regularizer.\ncombination of target-dependent loss and a self-supervised\nregularizer. Since the self-supervised loss only takes data as\ninputs, it does not rely on any task-speciﬁc labels and thus\ncan be minimized regardless of the underlying task.\nHere, we mention several learning tasks in which it is\nworth exploring the role of self-supervised regularization.\nSelf-Supervised Semi-Supervised Learning It is natural to\nleverage self-supervised learning for semi-supervised tasks.\nUnsupervised and semi-supervised learning, which are the\ntwo main themes of this survey, converge here. A self-\nsupervised loss can be applied to learn the instance [15]\nand transformation [16] equivariances over unlabeled data,\nwhich can be combined with a supervised loss such as cross-\nentropy on labeled data. This seeks to regularize the training\nof classiﬁers by exploring such inter- and intra-instance vari-\nations in the learned representations under a composite of\nspatial, color, and temporal transformations and augmenta-\ntions. Exciting performances have been shown to train self-\nsupervised semi-supervised models [115], [117]. The current\nmethods only use the self-supervised loss to regularize the\ntraining of the classiﬁers in an indirect fashion through the\nshared representations. In future, more efforts are needed\nto understand the role of self-supervised regularizer on the\npredicted labels of the jointly trained classiﬁers, and we\nexpect a theory can be developed to help us understand\nthe self-supervised regularization in the supervised training,\nwhich can further improve performances.\nSelf-Supervised Domain Adaptation Self-supervised reg-\nularizer has also been applied to bridge the domain gap\n[118], [119] in addition to the methods reviewed in Section 4.\nThis combines the self-supervised loss such as jigsaw loss\n[70] and transformation prediction loss [79] to understand\nintrinsic data variations when adapting classiﬁers across\ndifferent domains. It is based on the assumption that the\nunsupervised nature of a self-supervised task enables the\nlearning of a common representation space across domains\nby aligning source and target samples regardless of their\nlabels. This will be able to generalize a source classiﬁer to the\ntarget domain. However, although it has demonstrated com-\npetitive performances [118], [119], we still need to devote\nmore efforts to understand how the domain gap is closed\nby a common self-supervised task, as well as the relations\nwith the other domain adaptation methods.\nSelf-Supervised Generative Adversarial Networks Self-\nsupervised regularization is also applied to train the GANs.\nThe idea of using rotation prediction task as a regularizer\nwas presented in [120] to self-train the GAN discriminators.\nIt aims to combine the advantages of self-supervision and\nadversarial training to bridge the gap between conditional\nand unconditional GANs. A more sophisticated Transforma-\ntion GAN [121] was proposed recently to explore the joint\ndistribution of samples and their transformations. It extends\nthe idea of the self-supervised AET [7] by forcing the dis-\ncriminator to distinguish between real and fake samples as\nwell as their transformed copies. This makes the self-trained\nGANs better generalizable in producing unseen samples of\nvariations corresponding to different transformations.\nThe task-agnostic nature of self-supervised learning en-\nables its wide applicability for many problems beyond the\naforementioned tasks. In future, we can explore the applica-\ntion of self-supervision in more learning problems, as well\nas seek a uniﬁed theory of self-supervised regularization for\na large variety of learning tasks.\n6\nCONCLUSIONS\nIn this paper, we review two large categories of small\ndata methods – unsupervised and semi-supervised meth-\nods. In particular, a large variety of generative models\nare reviewed, including auto-encoders, GANs, Flow-based\nmodels, and autoregressive models, in both supervised and\nsemi-supervised categories. We also compare several emerg-\ning criteria and principles in training these models, such as\ntransformation equivariance and invariance in training un-\nsupervised and supervised representations, and the disen-\ntanglement of unsupervised and semi-supervised represen-\ntations for factorized and interpretable deep networks. Un-\nsupervised and semi-supervised domain adaptations have\nalso been reviewed to reveal the recent progress on bridg-\ning the gaps between distributions of different domains in\npresence of unlabeled and labeled data, respectively. We\nalso discuss the future directions to reveal the connections\nbetween unsupervised and semi-supervised learning.\nREFERENCES\n[1]\nA. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁ-\ncation with deep convolutional neural networks,” in Advances in\nneural information processing systems, 2012, pp. 1097–1105.\nQI et al.: SMALL DATA CHALLENGES IN BIG DATA ERA: A SURVEY OF RECENT PROGRESS ON UNSUPERVISED AND SEMI-SUPERVISED METHODS\n18\n[2]\nK. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning\nfor image recognition,” in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2016, pp. 770–778.\n[3]\nC. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning\nfor fast adaptation of deep networks,” in Proceedings of the 34th\nInternational Conference on Machine Learning-Volume 70.\nJMLR.\norg, 2017, pp. 1126–1135.\n[4]\nM. A. Jamal, G.-J. Qi, and M. Shah, “Task-agnostic meta-learning\nfor few-shot learning,” arXiv preprint arXiv:1805.07722, 2018.\n[5]\nX. Li, Q. Sun, Y. Liu, Q. Zhou, S. Zheng, T.-S. Chua, and B. Schiele,\n“Learning to self-train for semi-supervised few-shot classiﬁca-\ntion,” in Advances in Neural Information Processing Systems, 2019,\npp. 10 276–10 286.\n[6]\nG. E. Hinton, A. Krizhevsky, and S. D. Wang, “Transforming auto-\nencoders,” in International Conference on Artiﬁcial Neural Networks.\nSpringer, 2011, pp. 44–51.\n[7]\nL. Zhang, G.-J. Qi, L. Wang, and J. Luo, “Aet vs. aed: Unsuper-\nvised representation learning by auto-encoding transformations\nrather than data,” arXiv preprint arXiv:1901.04596, 2019.\n[8]\nY. Fu, T. Xiang, Y.-G. Jiang, X. Xue, L. Sigal, and S. Gong,\n“Recent advances in zero-shot recognition,” arXiv preprint\narXiv:1710.04837, 2017.\n[9]\nG.-J. Qi, L. Zhang, H. Hu, M. Edraki, J. Wang, and X.-S. Hua,\n“Global versus localized generative adversarial nets,” in Proceed-\nings of IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2018.\n[10]\nS. Laine and T. Aila, “Temporal ensembling for semi-supervised\nlearning,” arXiv preprint arXiv:1610.02242, 2016.\n[11]\nT. Miyato, S.-i. Maeda, S. Ishii, and M. Koyama, “Virtual ad-\nversarial training: a regularization method for supervised and\nsemi-supervised learning,” IEEE transactions on pattern analysis\nand machine intelligence, 2018.\n[12]\nM. Ren, E. Triantaﬁllou, S. Ravi, J. Snell, K. Swersky, J. B.\nTenenbaum, H. Larochelle, and R. S. Zemel, “Meta-learning\nfor\nsemi-supervised\nfew-shot\nclassiﬁcation,”\narXiv\npreprint\narXiv:1803.00676, 2018.\n[13]\nT. Ma and A. Zhang, “Afﬁnitynet: semi-supervised few-shot\nlearning for disease type prediction,” in Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, vol. 33, 2019, pp. 1069–1076.\n[14]\nW.-Y. Chen, Y.-C. Liu, Z. Kira, Y.-C. F. Wang, and J.-B.\nHuang, “A closer look at few-shot classiﬁcation,” in International\nConference on Learning Representations, 2019. [Online]. Available:\nhttps://openreview.net/forum?id=HkxLXnAcFQ\n[15]\nZ. Wu, Y. Xiong, S. X. Yu, and D. Lin, “Unsupervised feature\nlearning via non-parametric instance discrimination,” in Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2018, pp. 3733–3742.\n[16]\nG.-J. Qi and et al., “Avt: Unsupervised learning of transformation\nequivariant representations by autoencoding variational transfor-\nmations,” 2019.\n[17]\nA. Rasmus, M. Berglund, M. Honkala, H. Valpola, and T. Raiko,\n“Semi-supervised learning with ladder networks,” in Advances in\nNeural Information Processing Systems, 2015, pp. 3546–3554.\n[18]\nA. Tarvainen and H. Valpola, “Mean teachers are better role\nmodels: Weight-averaged consistency targets improve semi-\nsupervised deep learning results,” in Advances in neural informa-\ntion processing systems, 2017, pp. 1195–1204.\n[19]\nD. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver,\nand C. A. Raffel, “Mixmatch: A holistic approach to semi-\nsupervised learning,” in Advances in Neural Information Processing\nSystems, 2019, pp. 5050–5060.\n[20]\nK. Sohn, D. Berthelot, C.-L. Li, Z. Zhang, N. Carlini, E. D.\nCubuk, A. Kurakin, H. Zhang, and C. Raffel, “Fixmatch: Sim-\nplifying semi-supervised learning with consistency and conﬁ-\ndence,” arXiv preprint arXiv:2001.07685, 2020.\n[21]\nA. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial examples\nin the physical world,” arXiv preprint arXiv:1607.02533, 2016.\n[22]\nC. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Good-\nfellow, and R. Fergus, “Intriguing properties of neural networks,”\narXiv preprint arXiv:1312.6199, 2013.\n[23]\nT. Cohen and M. Welling, “Group equivariant convolutional\nnetworks,” in International conference on machine learning, 2016,\npp. 2990–2999.\n[24]\nT. S. Cohen, M. Geiger, and M. Weiler, “Intertwiners between\ninduced representations (with applications to the theory of equiv-\nariant neural networks),” arXiv preprint arXiv:1803.10743, 2018.\n[25]\nS. Sabour, N. Frosst, and G. E. Hinton, “Dynamic routing between\ncapsules,” in Advances in Neural Information Processing Systems,\n2017, pp. 3856–3866.\n[26]\nJ. E. Lenssen, M. Fey, and P. Libuschewski, “Group equivariant\ncapsule networks,” arXiv preprint arXiv:1806.05086, 2018.\n[27]\nT. S. Cohen, M. Geiger, J. K¨ohler, and M. Welling, “Spherical\ncnns,” arXiv preprint arXiv:1801.10130, 2018.\n[28]\nT. S. Cohen and M. Welling, “Steerable cnns,” arXiv preprint\narXiv:1612.08498, 2016.\n[29]\nI. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-\nFarley, S. Ozair, A. Courville, and Y. Bengio, “Generative adver-\nsarial nets,” in Advances in neural information processing systems,\n2014, pp. 2672–2680.\n[30]\nJ. Donahue, P. Kr¨ahenb¨uhl, and T. Darrell, “Adversarial feature\nlearning,” arXiv preprint arXiv:1605.09782, 2016.\n[31]\nV. Dumoulin, I. Belghazi, B. Poole, O. Mastropietro, A. Lamb,\nM. Arjovsky, and A. Courville, “Adversarially learned inference,”\narXiv preprint arXiv:1606.00704, 2016.\n[32]\nA. Srivastava, L. Valkov, C. Russell, M. U. Gutmann, and C. Sut-\nton, “Veegan: Reducing mode collapse in gans using implicit\nvariational learning,” in Advances in Neural Information Processing\nSystems, 2017, pp. 3308–3318.\n[33]\nA. B. L. Larsen, S. K. Sønderby, H. Larochelle, and O. Winther,\n“Autoencoding beyond pixels using a learned similarity metric,”\narXiv preprint arXiv:1512.09300, 2015.\n[34]\nA. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and B. Frey, “Ad-\nversarial autoencoders,” arXiv preprint arXiv:1511.05644, 2015.\n[35]\nS. Rifai, P. Vincent, X. Muller, X. Glorot, and Y. Bengio, “Contrac-\ntive auto-encoders: Explicit invariance during feature extraction,”\nin Proceedings of the 28th International Conference on International\nConference on Machine Learning.\nOmnipress, 2011, pp. 833–840.\n[36]\nD. P. Kingma and M. Welling, “Auto-encoding variational bayes,”\narXiv preprint arXiv:1312.6114, 2013.\n[37]\nP. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol, “Ex-\ntracting and composing robust features with denoising autoen-\ncoders,” in Proceedings of the 25th international conference on Ma-\nchine learning.\nACM, 2008, pp. 1096–1103.\n[38]\nP. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol,\n“Stacked denoising autoencoders: Learning useful representa-\ntions in a deep network with a local denoising criterion,” Journal\nof machine learning research, vol. 11, no. Dec, pp. 3371–3408, 2010.\n[39]\nS. Arora, R. Ge, Y. Liang, T. Ma, and Y. Zhang, “Generalization\nand equilibrium in generative adversarial nets (gans),” arXiv\npreprint arXiv:1703.00573, 2017.\n[40]\nG.-J. Qi, “Loss-sensitive generative adversarial networks on lips-\nchitz densities,” arXiv preprint arXiv:1701.06264, 2017.\n[41]\nM. Edraki and G.-J. Qi, “Generalized loss-sensitive adversarial\nlearning with manifold margins,” in Proceedings of European Con-\nference on Computer Vision (ECCV 2018), 2018.\n[42]\nD. Ulyanov, A. Vedaldi, and V. Lempitsky, “It takes (only) two:\nAdversarial generator-encoder networks,” in Thirty-Second AAAI\nConference on Artiﬁcial Intelligence, 2018.\n[43]\nH. Huang, R. He, Z. Sun, T. Tan et al., “Introvae: Introspective\nvariational autoencoders for photographic image synthesis,” in\nAdvances in Neural Information Processing Systems, 2018, pp. 52–63.\n[44]\nX. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and\nP. Abbeel, “Infogan: Interpretable representation learning by in-\nformation maximizing generative adversarial nets,” in Advances\nin neural information processing systems, 2016, pp. 2172–2180.\n[45]\nY. Bengio, A. Courville, and P. Vincent, “Representation learning:\nA review and new perspectives,” IEEE transactions on pattern\nanalysis and machine intelligence, vol. 35, no. 8, pp. 1798–1828, 2013.\n[46]\nI. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick,\nS. Mohamed, and A. Lerchner, “beta-vae: Learning basic visual\nconcepts with a constrained variational framework,” 2016.\n[47]\nI. Jeon, W. Lee, and G. Kim, “Ib-gan: Disentangled representation\nlearning with information bottleneck gan,” 2018.\n[48]\nH. Kim and A. Mnih, “Disentangling by factorising,” arXiv\npreprint arXiv:1802.05983, 2018.\n[49]\nT. D. Kulkarni, W. F. Whitney, P. Kohli, and J. Tenenbaum, “Deep\nconvolutional inverse graphics network,” in Advances in neural\ninformation processing systems, 2015, pp. 2539–2547.\n[50]\nT. Karaletsos, S. Belongie, and G. R¨atsch, “Bayesian rep-\nresentation learning with oracle constraints,” arXiv preprint\narXiv:1506.05011, 2015.\nQI et al.: SMALL DATA CHALLENGES IN BIG DATA ERA: A SURVEY OF RECENT PROGRESS ON UNSUPERVISED AND SEMI-SUPERVISED METHODS\n19\n[51]\nL. Dinh, D. Krueger, and Y. Bengio, “Nice: Non-linear inde-\npendent components estimation,” arXiv preprint arXiv:1410.8516,\n2014.\n[52]\nL. Dinh, J. Sohl-Dickstein, and S. Bengio, “Density estimation\nusing real nvp,” arXiv preprint arXiv:1605.08803, 2016.\n[53]\nD. P. Kingma and P. Dhariwal, “Glow: Generative ﬂow with\ninvertible 1x1 convolutions,” in Advances in Neural Information\nProcessing Systems, 2018, pp. 10 236–10 245.\n[54]\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin Advances in Neural Information Processing Systems, 2017, pp.\n5998–6008.\n[55]\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,” arXiv preprint arXiv:1810.04805, 2018.\n[56]\nA. Owens, J. Wu, J. H. McDermott, W. T. Freeman, and A. Tor-\nralba, “Ambient sound provides supervision for visual learning,”\nin European conference on computer vision. Springer, 2016, pp. 801–\n816.\n[57]\nR. Arandjelovic and A. Zisserman, “Objects that sound,” in\nProceedings of the European Conference on Computer Vision (ECCV),\n2018, pp. 435–451.\n[58]\nB. Korbar, D. Tran, and L. Torresani, “Cooperative learning of\naudio and video models from self-supervised synchronization,”\nin Advances in Neural Information Processing Systems, 2018, pp.\n7763–7774.\n[59]\nL. Jing and Y. Tian, “Self-supervised visual feature learning with\ndeep neural networks: A survey,” arXiv preprint arXiv:1902.06162,\n2019.\n[60]\nA. v. d. Oord, N. Kalchbrenner, and K. Kavukcuoglu, “Pixel\nrecurrent neural networks,” arXiv preprint arXiv:1601.06759, 2016.\n[61]\nA. Van den Oord, N. Kalchbrenner, L. Espeholt, O. Vinyals,\nA. Graves et al., “Conditional image generation with pixelcnn\ndecoders,” in Advances in Neural Information Processing Systems,\n2016, pp. 4790–4798.\n[62]\nT. Salimans, A. Karpathy, X. Chen, and D. P. Kingma, “Pix-\nelcnn++:\nImproving\nthe\npixelcnn\nwith\ndiscretized\nlogistic\nmixture likelihood and other modiﬁcations,” arXiv preprint\narXiv:1701.05517, 2017.\n[63]\nA. v. d. Oord, Y. Li, and O. Vinyals, “Representation learning with\ncontrastive predictive coding,” arXiv preprint arXiv:1807.03748,\n2018.\n[64]\nK. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum\ncontrast for unsupervised visual representation learning,” arXiv\npreprint arXiv:1911.05722, 2019.\n[65]\nT. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple\nframework for contrastive learning of visual representations,”\narXiv preprint arXiv:2002.05709, 2020.\n[66]\nX. Wang and G.-J. Q. Qi, “Contrastive learning with stronger aug-\nmentations,” in Submitted to International Conference on Learning\nRepresentations, 2021, under review.\n[67]\nM. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and\nA. Joulin, “Unsupervised learning of visual features by contrast-\ning cluster assignments,” arXiv preprint arXiv:2006.09882, 2020.\n[68]\nC. Doersch, A. Gupta, and A. A. Efros, “Unsupervised visual\nrepresentation learning by context prediction,” in Proceedings of\nthe IEEE International Conference on Computer Vision, 2015, pp.\n1422–1430.\n[69]\nD. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A.\nEfros, “Context encoders: Feature learning by inpainting,” in\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2016, pp. 2536–2544.\n[70]\nM. Noroozi and P. Favaro, “Unsupervised learning of visual rep-\nresentations by solving jigsaw puzzles,” in European Conference on\nComputer Vision.\nSpringer, 2016, pp. 69–84.\n[71]\nR. Zhang, P. Isola, and A. A. Efros, “Colorful image colorization,”\nin European Conference on Computer Vision.\nSpringer, 2016, pp.\n649–666.\n[72]\nG. Larsson, M. Maire, and G. Shakhnarovich, “Learning repre-\nsentations for automatic colorization,” in European Conference on\nComputer Vision.\nSpringer, 2016, pp. 577–593.\n[73]\nR. Zhang, P. Isola, and A. A. Efros, “Split-brain autoencoders:\nUnsupervised learning by cross-channel prediction.”\n[74]\nA. Dosovitskiy, J. T. Springenberg, M. Riedmiller, and T. Brox,\n“Discriminative unsupervised feature learning with convolu-\ntional neural networks,” in Advances in Neural Information Pro-\ncessing Systems, 2014, pp. 766–774.\n[75]\nP. Bojanowski and A. Joulin, “Unsupervised learning by predict-\ning noise,” arXiv preprint arXiv:1704.05310, 2017.\n[76]\nM. Caron, P. Bojanowski, A. Joulin, and M. Douze, “Deep cluster-\ning for unsupervised learning of visual features,” arXiv preprint\narXiv:1807.05520, 2018.\n[77]\nM. Noroozi, H. Pirsiavash, and P. Favaro, “Representation learn-\ning by learning to count,” in The IEEE International Conference on\nComputer Vision (ICCV), 2017.\n[78]\nP. Agrawal, J. Carreira, and J. Malik, “Learning to see by mov-\ning,” in Proceedings of the IEEE International Conference on Computer\nVision, 2015, pp. 37–45.\n[79]\nS. Gidaris, P. Singh, and N. Komodakis, “Unsupervised repre-\nsentation learning by predicting image rotations,” arXiv preprint\narXiv:1803.07728, 2018.\n[80]\nD. Wei, J. J. Lim, A. Zisserman, and W. T. Freeman, “Learning and\nusing the arrow of time,” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2018, pp. 8052–8060.\n[81]\nI. Misra, C. L. Zitnick, and M. Hebert, “Shufﬂe and learn: unsu-\npervised learning using temporal order veriﬁcation,” in European\nConference on Computer Vision.\nSpringer, 2016, pp. 527–544.\n[82]\nE. L. Denton et al., “Unsupervised learning of disentangled\nrepresentations from video,” in Advances in Neural Information\nProcessing Systems, 2017, pp. 4414–4423.\n[83]\nM. Wang, X.-S. Hua, T. Mei, R. Hong, G. Qi, Y. Song, and L.-\nR. Dai, “Semi-supervised kernel density estimation for video\nannotation,” Computer Vision and Image Understanding, vol. 113,\nno. 3, pp. 384–396, 2009.\n[84]\nJ. Tang, X.-S. Hua, G.-J. Qi, and X. Wu, “Typicality ranking via\nsemi-supervised multiple-instance learning,” in Proceedings of the\n15th ACM international conference on Multimedia, 2007, pp. 297–\n300.\n[85]\nJ. Tang, H. Li, G.-J. Qi, and T.-S. Chua, “Integrated graph-based\nsemi-supervised multiple/single instance learning framework\nfor image annotation,” in Proceedings of the 16th ACM international\nconference on Multimedia, 2008, pp. 631–634.\n[86]\nY. Song, G.-J. Qi, X.-S. Hua, L.-R. Dai, and R.-H. Wang, “Video\nannotation by active learning and semi-supervised ensembling,”\nin 2006 IEEE International Conference on Multimedia and Expo.\nIEEE, 2006, pp. 933–936.\n[87]\nD. P. Kingma, S. Mohamed, D. J. Rezende, and M. Welling, “Semi-\nsupervised learning with deep generative models,” in Advances\nin neural information processing systems, 2014, pp. 3581–3589.\n[88]\nL. Maaløe, C. K. Sønderby, S. K. Sønderby, and O. Winther, “Aux-\niliary deep generative models,” arXiv preprint arXiv:1602.05473,\n2016.\n[89]\nC. K. Sønderby, T. Raiko, L. Maaløe, S. K. Sønderby, and\nO. Winther, “Ladder variational autoencoders,” in Advances in\nneural information processing systems, 2016, pp. 3738–3746.\n[90]\nS. Narayanaswamy, T. B. Paige, J.-W. Van de Meent, A. Des-\nmaison, N. Goodman, P. Kohli, F. Wood, and P. Torr, “Learning\ndisentangled representations with semi-supervised deep genera-\ntive models,” in Advances in Neural Information Processing Systems,\n2017, pp. 5925–5935.\n[91]\nT. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford,\nand X. Chen, “Improved techniques for training gans,” in Ad-\nvances in Neural Information Processing Systems, 2016, pp. 2234–\n2242.\n[92]\nX. Zhu, “Semi-supervised learning with graphs,” Ph.D. disserta-\ntion, 2005.\n[93]\nT. D. Kulkarni, V. K. Mansinghka, P. Kohli, and J. B. Tenenbaum,\n“Inverse graphics with probabilistic cad models,” arXiv preprint\narXiv:1407.1339, 2014.\n[94]\nV. Jampani, S. Nowozin, M. Loper, and P. V. Gehler, “The in-\nformed sampler: A discriminative approach to bayesian inference\nin generative computer vision models,” Computer Vision and\nImage Understanding, vol. 136, pp. 32–44, 2015.\n[95]\nV. K. Mansinghka, T. D. Kulkarni, Y. N. Perov, and J. Tenenbaum,\n“Approximate bayesian image interpretation using generative\nprobabilistic graphics programs,” in Advances in Neural Informa-\ntion Processing Systems, 2013, pp. 1520–1528.\n[96]\nY. Tang, R. Salakhutdinov, and G. Hinton, “Deep lambertian\nnetworks,” arXiv preprint arXiv:1206.6445, 2012.\n[97]\nT. Tieleman, Optimizing neural networks that generate images.\nUni-\nversity of Toronto (Canada), 2014.\n[98]\nM. M. Loper and M. J. Black, “Opendr: An approximate dif-\nferentiable renderer,” in European Conference on Computer Vision.\nSpringer, 2014, pp. 154–169.\nQI et al.: SMALL DATA CHALLENGES IN BIG DATA ERA: A SURVEY OF RECENT PROGRESS ON UNSUPERVISED AND SEMI-SUPERVISED METHODS\n20\n[99]\nJ. Schulman, N. Heess, T. Weber, and P. Abbeel, “Gradient estima-\ntion using stochastic computation graphs,” in Advances in Neural\nInformation Processing Systems, 2015, pp. 3528–3536.\n[100] C. M. Bishop, “Training with noise is equivalent to tikhonov\nregularization,” Neural computation, vol. 7, no. 1, pp. 108–116,\n1995.\n[101] R. Reed, S. Oh, and R. Marks, “Regularization using jittered\ntraining data,” in Neural Networks, 1992. IJCNN., International Joint\nConference on, vol. 3.\nIEEE, 1992, pp. 147–152.\n[102] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov, “Dropout: a simple way to prevent neural net-\nworks from overﬁtting,” The Journal of Machine Learning Research,\nvol. 15, no. 1, pp. 1929–1958, 2014.\n[103] M. Sajjadi, M. Javanmardi, and T. Tasdizen, “Regularization\nwith stochastic transformations and perturbations for deep semi-\nsupervised learning,” in Advances in Neural Information Processing\nSystems, 2016, pp. 1163–1171.\n[104] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, “Adversarial dis-\ncriminative domain adaptation,” in Computer Vision and Pattern\nRecognition (CVPR), vol. 1, no. 2, 2017, p. 4.\n[105] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,\nF. Laviolette, M. Marchand, and V. Lempitsky, “Domain-\nadversarial training of neural networks,” The Journal of Machine\nLearning Research, vol. 17, no. 1, pp. 2096–2030, 2016.\n[106] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Krish-\nnan, “Unsupervised pixel-level domain adaptation with genera-\ntive adversarial networks,” in The IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), vol. 1, no. 2, 2017, p. 7.\n[107] A. Rozantsev, M. Salzmann, and P. Fua, “Beyond sharing weights\nfor deep domain adaptation,” IEEE transactions on pattern analysis\nand machine intelligence, 2018.\n[108] E. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell, “Deep\ndomain confusion: Maximizing for domain invariance,” arXiv\npreprint arXiv:1412.3474, 2014.\n[109] M. Long, Y. Cao, J. Wang, and M. I. Jordan, “Learning trans-\nferable features with deep adaptation networks,” arXiv preprint\narXiv:1502.02791, 2015.\n[110] A. Gretton, K. Borgwardt, M. Rasch, B. Sch¨olkopf, and A. J.\nSmola, “A kernel method for the two-sample-problem,” in Ad-\nvances in neural information processing systems, 2007, pp. 513–520.\n[111] M. Long, J. Wang, G. Ding, J. Sun, and P. S. Yu, “Transfer feature\nlearning with joint distribution adaptation,” in Proceedings of the\nIEEE international conference on computer vision, 2013, pp. 2200–\n2207.\n[112] E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko, “Simultaneous\ndeep transfer across domains and tasks,” in Proceedings of the\nIEEE International Conference on Computer Vision, 2015, pp. 4068–\n4076.\n[113] M.-Y. Liu and O. Tuzel, “Coupled generative adversarial net-\nworks,” in Advances in neural information processing systems, 2016,\npp. 469–477.\n[114] X. Gao, W. Hu, and G.-J. Qi, “Graphter: Unsupervised learning\nof graph transformation equivariant representations via auto-\nencoding node-wise transformations,” 2020.\n[115] X. Wang, D. Kihara, J. Luo, and G.-J. Qi, “Enaet: Self-trained en-\nsemble autoencoding transformations for semi-supervised learn-\ning,” arXiv preprint arXiv:1911.09265, 2019.\n[116] L. Zhang and G.-J. Qi, “Wcp: Worst-case perturbations for semi-\nsupervised deep learning,” in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 2020.\n[117] X. Zhai, A. Oliver, A. Kolesnikov, and L. Beyer, “S4l: Self-\nsupervised semi-supervised learning,” in Proceedings of the IEEE\ninternational conference on computer vision, 2019, pp. 1476–1485.\n[118] F. M. Carlucci, A. D’Innocente, S. Bucci, B. Caputo, and T. Tom-\nmasi, “Domain generalization by solving jigsaw puzzles,” in\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2019, pp. 2229–2238.\n[119] Y. Sun, E. Tzeng, T. Darrell, and A. A. Efros, “Unsupervised\ndomain adaptation through self-supervision,” arXiv preprint\narXiv:1909.11825, 2019.\n[120] T. Chen, X. Zhai, M. Ritter, M. Lucic, and N. Houlsby, “Self-\nsupervised gans via auxiliary rotation loss,” in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, 2019,\npp. 12 154–12 163.\n[121] J. Wang, W. Zhou, G.-J. Qi, Z. Fu, Q. Tian, and H. Li, “Transfor-\nmation gan for unsupervised image synthesis and representation\nlearning,” in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2020.\n[122] E. Oyallon and S. Mallat, “Deep roto-translation scattering for\nobject classiﬁcation,” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2015, pp. 2865–2873.\n[123] A. Radford, L. Metz, and S. Chintala, “Unsupervised represen-\ntation learning with deep convolutional generative adversarial\nnetworks,” arXiv preprint arXiv:1511.06434, 2015.\n[124] E. Oyallon, E. Belilovsky, and S. Zagoruyko, “Scaling the scatter-\ning transform: Deep hybrid networks,” in International Conference\non Computer Vision (ICCV), 2017.\n[125] X. Wang and A. Gupta, “Unsupervised learning of visual repre-\nsentations using videos,” in Proceedings of the IEEE International\nConference on Computer Vision, 2015, pp. 2794–2802.\n[126] P. Kr¨ahenb¨uhl, C. Doersch, J. Donahue, and T. Darrell, “Data-\ndependent initializations of convolutional neural networks,”\narXiv preprint arXiv:1511.06856, 2015.\n[127] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva, “Learn-\ning deep features for scene recognition using places database,”\nin Advances in neural information processing systems, 2014, pp. 487–\n495.\n[128] A. Oliver, A. Odena, C. A. Raffel, E. D. Cubuk, and I. Good-\nfellow, “Realistic evaluation of deep semi-supervised learning\nalgorithms,” in Advances in Neural Information Processing Systems,\n2018, pp. 3235–3246.\n[129] A. Krizhevsky, “Learning multiple layers of features from tiny\nimages,” Citeseer, Tech. Rep., 2009.\n[130] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.\nNg, “Reading digits in natural images with unsupervised feature\nlearning,” 2011.\nGuo-Jun Qi Guo-Jun Qi (M14-SM18) is the\nChief Scientist leading and overseeing an in-\nternational R&D team in the domain of mul-\ntiple intelligent cloud services, including smart\ncities, visual computing service, medical intelli-\ngent service, and connected vehicle service at\nFuturewei, since August 2018. He was a faculty\nmember in the Department of Computer Sci-\nence and the director of MAchine Perception\nand LEarning (MAPLE) Lab at the University of\nCentral Florida since August 2014. Prior to that,\nhe was also a Research Staff Member at IBM T.J. Watson Research\nCenter, Yorktown Heights, NY. Dr. Qi has published over 150 papers\nin a broad range of venues. Among them are the best student paper\nof ICDM 2014, “the best ICDE 2013 paper” by IEEE Transactions on\nKnowledge and Data Engineering, as well as the best paper (ﬁnalist) of\nACM Multimedia 2007 (2015).\nJiebo Luo Jiebo Luo (S93-M96-SM99-F09)\njoined the University of Rochester in Fall 2011 af-\nter over ﬁfteen proliﬁc years at Kodak Research\nLaboratories, where he was a Senior Principal\nScientist leading research and advanced de-\nvelopment. He has been involved in numerous\ntechnical conferences, including serving as the\nprogram co-chair of ACM Multimedia 2010,IEEE\nCVPR 2012, ACM ICMR 2016, and IEEE ICIP\n2017. He has served on the editorial boards of\nthe IEEE Transactions on Pattern Analysis and\nMachine Intelligence, IEEE Transactions on Multimedia, IEEE Transac-\ntions on Circuits and Systems for Video Technology, IEEE Transactions\non Big Data, ACM Transactions on Intelligent Systems and Technology,\nPattern Recognition, Machine Vision and Applications, Knowledge and\nInformation Systems, and Journal of Electronic Imaging. Dr. Luo is a\nFellow of the SPIE, IAPR, IEEE, ACM, and AAAI.\nQI et al.: SMALL DATA CHALLENGES IN BIG DATA ERA: A SURVEY OF RECENT PROGRESS ON UNSUPERVISED AND SEMI-SUPERVISED METHODS\n21\nAPPENDIX A\nEVALUATIONS ON UNSUPERVISED LEARNING\nUnsupervised methods are often evaluated based on\ntheir performances on downstream tasks. In particular,\nthe learned unsupervised representations can be used to\nperform classiﬁcation tasks on benchmark datasets such\nCIFAR-10, ImageNet, Places and Pascal VOC.\nFor the sake of fair comparison, some efforts have been\nmade on setting standard evaluation protocols on these\ndatasets. In this subsection, we review such a protocol that\nhas been widely adopted by many unsupervised methods.\nAlthough not all approaches have been compared against\nthis protocol due to the legacy reason, it has emerged in\nliterature to allow a fair and direct comparison among many\nrecent methods [7], [74], [75], [79], [122], [123], [124].\nA.1\nEvaluation Protocol\nThe evaluation of an unsupervised model often consists\nof two stages. The ﬁrst stage is the unsupervised training\nof representations with only unlabeled examples. In the\nsecond stage, a supervised classiﬁer is trained on top of the\nlearned representations to evaluate their performances on\ngeneralizability to a new classiﬁcation task.\nTake the evaluation protocol on the ImageNet dataset\nfor example. The AlexNet is widely used as the backbone\nto learn the unsupervised representations, which consists of\nﬁve convolutional layers and three fully connected layers\n(including a softmax layer with 1, 000-way outputs). There\nare several settings below adopted to test unsupervised\nmodels for classiﬁcation tasks.\nNonlinear Classiﬁers In this setting, the convolutional\nlayers up to Conv4 and Conv5 are frozen after they are\nunsupervised trained in the ﬁrst stage. All the layers above\nthe Conv4 and Conv5 are supervised trained with the\nlabeled data in the evaluation stage. In other words, the\nnonlinear classiﬁers are trained on the unsupervised rep-\nresentations for the evaluation purpose. For example, in the\nConv4 setting, Conv5 and three fully connected layers are\ntrained with the labeled examples, including the last 1000-\nway output layer. Table A.2 shows the comparison between\nseveral unsupervised models. The fully supervised model\n(ImageNet Labels) and the random model are also included\nin the table to show the upper and lower bounds on the\nclassiﬁcation performances, respectively.\nLinear Classiﬁers A single fully connected layer can also\nbe added on top of unsupervised representations to train a\nweak linear classiﬁer. As shown in Table A.3, the linear layer\nis trained upon different convolutional layers of feature\nmaps. The linear classiﬁer can be trained very efﬁciently\nand the results show that a good trade off between training\nefﬁciency and test accuracy can be achieved with a linear\nclassiﬁer on top of a properly trained unsupervised repre-\nsentation.\nCross-Dataset Tasks Cross-dataset tasks are also performed\nto compare the generalizability of the unsupervised repre-\nsentations to the tasks on new datasets.\nA.2\nResults\nAs shown in Table A.4, unsupervised models have also been\nevaluated by pretraining on the ImageNet dataset. Then a\nTABLE A.2: Top-1 accuracy with non-linear layers on Ima-\ngeNet. AlexNet is used as backbone to train the unsuper-\nvised models. After unsupervised features are learned, non-\nlinear classiﬁers are trained on top of Conv4 and Conv5 lay-\ners with labeled examples to compare their performances.\nThe fully supervised models and random models give up-\nper and lower bounded performances.\nMethod\nConv4\nConv5\nImageNet Labels [75](Upper Bound)\n59.7\n59.7\nRandom [77] (Lower Bound)\n27.1\n12.0\nTracking [125]\n38.8\n29.8\nContext [68]\n45.6\n30.4\nColorization [71]\n40.7\n35.2\nJigsaw Puzzles [70]\n45.3\n34.6\nBiGAN [30]\n41.9\n32.2\nNAT [75]\n-\n36.0\nDeepCluster [76]\n-\n44.0\nRotNet [79]\n50.0\n43.8\nAET [7]\n53.2\n47.0\nAVT [16]\n54.2\n48.4\nTABLE A.3: Top-1 accuracy with linear layers on ImageNet.\nAlexNet is used as backbone to train the unsupervised\nmodels under comparison. A 1, 000-way linear classiﬁer is\ntrained upon various convolutional layers of feature maps\nthat are spatially resized to have about 9, 000 elements. Fully\nsupervised and random models are also reported to show\nthe upper and the lower bounds of unsupervised model\nperformances. Only a single crop is used and no dropout or\nlocal response normalization is used during testing, except\nthe models denoted with * where ten crops are applied to\ncompare results.\nMethod\nConv1\nConv2\nConv3\nConv4\nConv5\nImageNet Labels [79]\n19.3\n36.3\n44.2\n48.3\n50.5\nRandom [79]\n11.6\n17.1\n16.9\n16.3\n14.1\nRandom rescaled [126]\n17.5\n23.0\n24.5\n23.2\n20.6\nContext [68]\n16.2\n23.3\n30.2\n31.7\n29.6\nContext Encoders [69]\n14.1\n20.7\n21.0\n19.8\n15.5\nColorization [71]\n12.5\n24.5\n30.4\n31.5\n30.3\nJigsaw Puzzles [70]\n18.2\n28.8\n34.0\n33.9\n27.1\nBiGAN [30]\n17.7\n24.5\n31.0\n29.9\n28.0\nSplit-Brain [73]\n17.7\n29.3\n35.4\n35.2\n32.8\nCounting [77]\n18.0\n30.6\n34.3\n32.5\n25.7\nRotNet [79]\n18.8\n31.7\n38.7\n38.2\n36.5\nIns. Disc. [15]\n16.8\n26.5\n31.8\n34.1\n35.6\nAET [7]\n19.2\n32.8\n40.6\n39.7\n37.7\nAVT [16]\n19.5\n33.6\n41.3\n40.3\n39.1\nDeepCluster* [76]\n13.4\n32.3\n41.0\n39.6\n38.2\nAET [7]*\n19.3\n35.4\n44.0\n43.6\n42.4\nsingle-layer logistic regression classiﬁer is trained on top of\ndifferent convolutional layers of feature maps with Places\nlabels. Table A.5 shows the classiﬁcation, object detection\nand semantic segmentations on PASCAL VOC, where the\nmodels are still based on AlexNet variants and pretrained\non the ImageNet in an unsupervised fashion. Both results\nare compared against the fully supervised models trained\nwith the Places labels and ImageNet labels, as well as the\nrandom networks.\nQI et al.: SMALL DATA CHALLENGES IN BIG DATA ERA: A SURVEY OF RECENT PROGRESS ON UNSUPERVISED AND SEMI-SUPERVISED METHODS\n22\nTABLE A.4: Top-1 accuracy on the Places dataset with linear\nlayers. A 205-way logistic regression classiﬁer is trained\non top of various layers of feature maps that are spatially\nresized to have about 9, 000 elements. All unsupervised\nfeatures are pre-trained on the ImageNet dataset, which\nare frozen when training the logistic regression layer with\nPlaces labels. The unsupervised models are also compared\nwith fully-supervised networks trained with Places Labels\nand ImageNet labels, along with random models.\nMethod\nConv1\nConv2\nConv3\nConv4\nConv5\nPlaces labels [127]\n22.1\n35.1\n40.2\n43.3\n44.6\nImageNet labels\n22.7\n34.8\n38.4\n39.4\n38.7\nRandom\n15.7\n20.3\n19.8\n19.1\n17.5\nRandom rescaled [126]\n21.4\n26.2\n27.1\n26.1\n24.0\nContext [68]\n19.7\n26.7\n31.9\n32.7\n30.9\nContext Encoders [69]\n18.2\n23.2\n23.4\n21.9\n18.4\nColorization [71]\n16.0\n25.7\n29.6\n30.3\n29.7\nJigsaw Puzzles [70]\n23.0\n31.9\n35.0\n34.2\n29.3\nBiGAN [30]\n22.0\n28.7\n31.8\n31.3\n29.7\nSplit-Brain [73]\n21.3\n30.7\n34.0\n34.1\n32.5\nCounting [73]\n23.3\n33.9\n36.3\n34.7\n29.6\nRotNet [79]\n21.5\n31.0\n35.1\n34.6\n33.7\nIns. Dis. [15]\n18.8\n24.3\n31.9\n34.5\n33.6\nAET [7]\n22.1\n32.9\n37.1\n36.2\n34.7\nAVT [16]\n22.3\n33.1\n37.8\n36.7\n35.6\nTABLE A.5: Results on PASCAL VOC 2007 classiﬁcation and\ndetection tasks, and PASCAL VOC 2012 segmentation task.\nFor classiﬁcation, the convolutional features before Conv5\n(fc6-8) or the whole model (all) are ﬁne-tuned on PASCAL\nVOC dataset after they are unsupervised pretrained on\nImageNet. For detection, multi-scale is used for training and\na single scale for testing. The mean Average Precision (mAP)\nis reported for classiﬁcation and detection tasks, while the\nmean Intersection over Union (mIoU) for the segmentation.\nMethod\nClassiﬁcation\nDetection\nSegmentation\nLayers\nfc6-8\nall\nall\nall\nImageNet labels [75]\n78.9\n79.9\n56.8\n48.0\nRandom [79]\n-\n53.3\n43.4\n19.8\nRandom rescaled [126]\n39.2\n56.6\n45.6\n32.6\nEgomotion [78]\n31.0\n54.2\n43.9\n-\nContext Encoders [69]\n34.6\n56.5\n44.5\n29.7\nTracking\n55.6\n63.1\n47.4\n-\nContext [68]\n55.1\n65.3\n51.1\n-\nColorization [71]\n61.5\n65.6\n46.9\n35.6\nBiGAN [30]\n52.3\n60.1\n46.9\n34.9\nJigsaw Puzzles [70]\n-\n67.6\n53.2\n37.6\nNAT [75]\n56.7\n65.3\n49.4\n-\nSplit-Brain [73]\n63.0\n67.1\n46.7\n36.0\nColorProxy [72]\n-\n65.9\n-\n-\nCounting [77]\n-\n67.7\n51.4\n36.6\nRotNet [79]\n70.87\n72.97\n54.4\n39.1\nAET [7]\n-\n-\n57.4\n-\nAPPENDIX B\nEVALUATIONS ON SEMI-SUPERVISED LEARNING\nWe will summarize some results by semi-supervised meth-\nods here. More evaluation protocols for comparing semi-\nsupervised methods can be found in [128].\nB.1\nDatasets\nFirst we will introduce two datasets widely used in the\nevaluation.\nCIFAR-10 Dasetset. The dataset [129] contains 50, 000 train-\ning images and 10, 000 test images on ten image categories.\nWe train the semi-supervised LGAN model in experiments,\nwhere 100 and 400 labeled examples are labeled per class\nand the remaining examples are left unlabeled. The experi-\nment results on this dataset are reported by averaging over\nten runs.\nSVHN Dataset. The dataset [130] contains 32 × 32 street\nview house numbers that are roughly centered in images.\nThe training set and the test set contain 73, 257 and 26, 032\nhouse numbers, respectively. In an experiment, 50 and 1, 00\nlabeled examples per digit are used to train the model, and\nthe remaining unlabeled examples are used as auxiliary data\nto train the model in semi-supervised fashion.\nB.2\nResults\nBoth CIFAR-10 and SVHN are often used to evaluate the\nperformances of semi-supervised models by training them\nwith all unlabeled training images and a various amount\nof labeled examples. Then the error rate is reported on a\nseparate test set.\nFor the sake of fair comparison, a 13-layer convolutional\nneural network is often adopted to train the models (See\nTable 5 of [10]). For most of models, random translations and\nhorizontal ﬂips are applied as data augmentations of input\nimages. There are also two forms of noises used in many\nmodels (e.g., Π model, mean teacher, Temporal ensembling):\nGaussian noises are applied to the input layers while the\nrandom dropout is applied within the networks.\nTable A.7 and Table A.6 compare the results on CIFAR-10\nand SVHN datasets respectively, from which we can see that\nthe class of teach-student models turn out to outperform\nother methods well on both datasets. In particular, VAT has\nachieved the most outstanding performances among these\ncompared models.\nAPPENDIX C\nCHART OF UNSUPERVISED AND SEMI-SUPERVISED\nLEARNING\nFinally, we present a chart in Figure C.3 showing the cate-\ngorization of unsupervised and semi-supervised methods.\nThis is for readers’ convenience to look up the relevant\nmethods reviewed in this survey.\nQI et al.: SMALL DATA CHALLENGES IN BIG DATA ERA: A SURVEY OF RECENT PROGRESS ON UNSUPERVISED AND SEMI-SUPERVISED METHODS\n23\nTABLE A.6: Error rates on SVHN with a various number of labeled examples used to train different models. Note two\nversions of results have been reported on Π model separately.\nMethod\n250\n500\n1000\nAll Labels\nSupervised only [18]\n27.77 ± 3.18\n16.88 ± 1.30\n12.32 ± 0.95\n2.75 ± 0.10\nM1+M2 [87]\n-\n-\n36.02 ± 0.10\n-\nImproved GAN [91]\n-\n18.44±4.8\n8.11±1.3\n-\nALI [31]\n-\n-\n7.41 ± 0.65\n-\nLocalized GAN [9]\n-\n5.48 ± 0.29\n4.79 ± 0.16\n-\nΠ model [10]\n-\n6.65 ± 0.53\n4.82 ± 0.17\n2.54 ± 0.04\nΠ model [18]\n9.69 ± 0.92\n6.83 ± 0.60\n4.95 ± 0.26\n2.50 ± 0.07\nTemporal Ensembling [10]\n-\n5.12 ± 0.13\n4.42 ± 0.16\n2.74 ± 0.06\nVAT + EntMin [11]\n-\n-\n3.86\n-\nMean Teacher [18]\n4.35 ± 0.50\n4.18 ± 0.27\n3.95 ± 0.19\n2.50 ± 0.05\nTABLE A.7: Error rates on CIFAR-10 with a various number of labeled examples used to train different models. Note two\nversions of results have been reported on Π model separately.\nMethod\n1000\n2000\n4000\nAll Labels\nSupervised only [18]\n46.43 ± 1.21\n33.94 ± 0.73\n20.66 ± 0.57\n5.82 ± 0.15\nImproved GAN [91]\n-\n-\n18.63 ± 2.32\n-\nALI [31]\n-\n-\n17.99 ± 1.62\n-\nLocalized GAN [9]\n17.44 ± 0.25\n-\n14.23 ± 0.27\n-\nΠ model [10]\n-\n-\n12.36 ± 0.31\n5.56 ± 0.10\nΠ model [18]\n27.36 ± 1.20\n18.02 ± 0.60\n13.20 ± 0.27\n6.06 ± 0.11\nTemporal Ensembling [10]\n-\n-\n12.16 ± 0.31\n5.60 ± 0.10\nVAT + EntMin [11]\n-\n-\n10.55\n-\nMean Teacher [18]\n21.55 ± 1.48\n15.73 ± 0.31\n12.31 ± 0.28\n5.94 ± 0.15\nQI et al.: SMALL DATA CHALLENGES IN BIG DATA ERA: A SURVEY OF RECENT PROGRESS ON UNSUPERVISED AND SEMI-SUPERVISED METHODS\n24\nRepresentation Learning with Small Labeled Data\nUnsupervised Methods\nTransformation Equivariant Representations\nGroup Equivariant Convolutions (GEC)\nGEC [23], Steerable CNN [28], Spherical CNN [27], Group Equivariant\nCapsule Networks [26]\nAuto-Encoding Transformations (AET): AET [7], AVT [16]\nGenerative Models\nAuto-Encoders\nVariational Auto-Encoders (VAE) [36], Denoising Auto-Encoders (DAE) [37],\nContractive Auto-Encoders (CAE) [35]\nGenerative Adversarial Nets (GAN)\nGAN [29], BiGAN [30], ALI [31], IntroAVE [43], VEEGAN [32]\nDisentangled Representations: InfoGAN [44], β-VAE [46], FactorVAE [48]\nMore Generative Models\nFlow-based Generative Models: GLOW [53],RealNVP [52], NICE [51]\nSelf-Attention: Transformer [54], BERT [55]\nSelf-Supervised Methods\nAutoregressive Models\nPixelRNN [60], PixelCNN [60], [61], Gated PixelCNN [61]\nContractive Predictive Coding (CPC) [63], instance discrimination [15]\nImage Representations\nContext Prediction [68], Context Encoder [69], Jigsaw Puzzle [77],\nSplit-Brain [73], Colorization [71], ColorProxy [72], Counting [70],\nNAT [75], DeepCluster [76], Exampler-CNN [74], Egomotion [78], RotNet\n[79]\nVideo Representations\nArrow of Time (AoT) [80], Tuple Verification [81], DrNet [82]\nSemi-Supervised Methods\nSemi-Supervised Generative Models\nSemi-Supervised Auto-Encoders: M1, M2, M1+M2 [87]\nSemi-Supervised GANs: Improved GAN [91], Localized GAN [9]\nSemi-Supervised Disentangled Representations\nDeep Convolutional Inverse Graphics Network (DC-IGN) [49]\nDisentangling VAE [90]\nTeach-Student Models\nNoisy Teachers: Γ [17] and Π Models [10]\nTeacher Ensemble: Temporal Ensembling [10], Mean Teacher [18]\nAdversarial Teacher: Virtual Adversarial Training (VAT) [11]\nDomain Adaptation (DA)\nUnsupervised Domain Adaptation\nAdversarial Discriminative Domain Adaptation (ADDA) [104], Gradient Reversal\nLayer (GRL) [105], Domain Confusion [112], CoGAN [113]\nSemi-Supervised Domain Adaptation: PixelDA [106], Two-Stream DA [107]\nFig. C.3: The chart presents the categorization of unsupervised and semi-supervised methods. The methods are organized\nby where they are reviewed in this survey.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2019-03-27",
  "updated": "2021-01-02"
}