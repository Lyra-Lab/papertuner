{
  "id": "http://arxiv.org/abs/1903.12355v2",
  "title": "Local Aggregation for Unsupervised Learning of Visual Embeddings",
  "authors": [
    "Chengxu Zhuang",
    "Alex Lin Zhai",
    "Daniel Yamins"
  ],
  "abstract": "Unsupervised approaches to learning in neural networks are of substantial\ninterest for furthering artificial intelligence, both because they would enable\nthe training of networks without the need for large numbers of expensive\nannotations, and because they would be better models of the kind of\ngeneral-purpose learning deployed by humans. However, unsupervised networks\nhave long lagged behind the performance of their supervised counterparts,\nespecially in the domain of large-scale visual recognition. Recent developments\nin training deep convolutional embeddings to maximize non-parametric instance\nseparation and clustering objectives have shown promise in closing this gap.\nHere, we describe a method that trains an embedding function to maximize a\nmetric of local aggregation, causing similar data instances to move together in\nthe embedding space, while allowing dissimilar instances to separate. This\naggregation metric is dynamic, allowing soft clusters of different scales to\nemerge. We evaluate our procedure on several large-scale visual recognition\ndatasets, achieving state-of-the-art unsupervised transfer learning performance\non object recognition in ImageNet, scene recognition in Places 205, and object\ndetection in PASCAL VOC.",
  "text": "Local Aggregation for Unsupervised Learning of Visual Embeddings\nChengxu Zhuang\nStanford University\nAlex Lin Zhai\nStanford University\nDaniel Yamins\nStanford University\nAbstract\nUnsupervised approaches to learning in neural networks\nare of substantial interest for furthering artiﬁcial intelli-\ngence, both because they would enable the training of net-\nworks without the need for large numbers of expensive\nannotations, and because they would be better models of\nthe kind of general-purpose learning deployed by humans.\nHowever, unsupervised networks have long lagged behind\nthe performance of their supervised counterparts, espe-\ncially in the domain of large-scale visual recognition. Re-\ncent developments in training deep convolutional embed-\ndings to maximize non-parametric instance separation and\nclustering objectives have shown promise in closing this\ngap. Here, we describe a method that trains an embedding\nfunction to maximize a metric of local aggregation, caus-\ning similar data instances to move together in the embed-\nding space, while allowing dissimilar instances to separate.\nThis aggregation metric is dynamic, allowing soft clusters\nof different scales to emerge. We evaluate our procedure\non several large-scale visual recognition datasets, achiev-\ning state-of-the-art unsupervised transfer learning perfor-\nmance on object recognition in ImageNet, scene recognition\nin Places 205, and object detection in PASCAL VOC.\n1. Introduction\nDeep convolutional neural networks (DCNNs) have\nachieved great success on many tasks across a variety of\ndomains, such as vision [37, 60, 24, 23, 7], audition [26,\n21, 11, 47], and natural language processing [68, 29, 9, 38].\nHowever, most successful DCNNs are trained in a super-\nvised fashion on labelled datasets [37, 60, 24, 10, 26], re-\nquiring the costly collection of large numbers of annota-\ntions. There is thus substantial interest in ﬁnding methods\nthat can train DCNNs solely using unlabeled data, which\nare often readily available. Over many decades of work,\nsubstantial progress has been achieved using a wide vari-\nety of unsupervised learning approaches [6, 67, 71, 36, 13,\n15, 70, 48, 63, 49]. Nevertheless, unsupervised networks\nare still typically signiﬁcantly lower performing than their\nsupervised counterparts, and are rarely used in real-world\napplications [6, 47, 7].\nIn contrast to the inefﬁciency of unsupervised learning\nin artiﬁcial neural networks, humans and non-human pri-\nmates develop powerful and domain-general visual systems\nwith very few labels [42, 5, 65, 1, 22, 4, 61]. Although the\nmechanisms underlying the efﬁciency of biological learning\nstill remain largely unknown [5], researchers reliably report\nthat infants as young as three months can group perceptually\nsimilar stimuli [46], even for stimulus types that the infants\nhave never seen before. Moreover, this ability arises long\nbefore these infants appear to have an explicit concept of\nobject category [46, 54, 31, 8]. These ﬁndings suggest that\nbiological unsupervised learning may take advantage of in-\nherent visual similarity, without requiring sharp boundaries\nbetween stimulus categories.\nInspired by these results, we propose a novel unsuper-\nvised learning algorithm through local non-parametric ag-\ngregation in a latent feature space. First, we non-linearly\nembed inputs in a lower-dimensional space via a neural\nnetwork. We then iteratively identify close neighbors sur-\nrounding each example in the embedding space, while op-\ntimizing the embedding function to strengthen the degree\nof local aggregation. Our procedure, which we term Local\nAggregation (LA), causes inputs that are naturally dissim-\nilar to each other to move apart in the embedding space,\nwhile allowing inputs that share statistical similarities to ar-\nrange themselves into emergent clusters. By simultaneously\noptimizing this soft clustering structure and the non-linear\nembedding in which it is performed, our procedure exposes\nsubtle statistical regularities in the data. The resulting rep-\nresentation in turn robustly supports downstream tasks.\nHere, we illustrate the LA procedure in the context of\nlarge-scale visual learning. Training a standard convolution\nneural network with LA using images from ImageNet [10]\nsigniﬁcantly outperforms current state-of-art unsupervised\nalgorithms on transfer learning to classiﬁcation tasks on\nboth ImageNet and the Places 205 dataset [72]. In addition,\nLA shows consistent improvements as the depth of the em-\nbedding function increases, allowing it to achieve 60.2%\ntop-1 accuracy on ImageNet classiﬁcation. This is, as far\nas we know, the ﬁrst time an unsupervised model has sur-\npassed the milestone AlexNet network trained directly on\n1\narXiv:1903.12355v2  [cs.CV]  10 Apr 2019\nthe supervised task. We also show that, through further ﬁne-\ntuning, LA trained models obtain state-of-the-art results on\nthe PASCAL object detection task.\nThe remainder of this paper is organized as follows: in\nsection 2, we discuss related work; in section 3, we describe\nthe LA method; in section 4, we show experimental results;\nin section 5, we present analyses illustrating how this algo-\nrithm learns and justifying key parameter choices.\n2. Related Work\nUnsupervised learning methods span a very broad spec-\ntrum of approaches going back to the roots of artiﬁcial neu-\nral networks [53, 39, 2, 58, 27, 41, 28, 30, 25], and are\ntoo numerous to fully review here. However, several recent\nworks have achieved exciting progress in unsupervised rep-\nresentation learning [6, 67, 71]. Although the LA method\ndraws inspiration from these works, it differs from them in\nsome important conceptual ways.\nDeepCluster. DeepCluster [6] (DC) trains a DCNN in a\nseries of iterative rounds. In each round, features from the\npenultimate layer of the DCNN from the previous round\nare clustered, and the cluster assignments are used as self-\ngenerated supervision labels for further training the DCNN\nusing standard error backprogation.\nLike DC, LA also\nuses an iterative training procedure, but the speciﬁc process\nwithin each iteration differs signiﬁcantly. First, unlike the\nclustering step of DC where all examples are divided into\nmutually-exclusive clusters, our method identiﬁes neigh-\nbors separately for each example, allowing for more ﬂex-\nible statistical structures than a partition. Indeed, as shown\nin Section 5.2, the use of individual semantic neighbor iden-\ntiﬁers rather than global clustering is important for perfor-\nmance improvement. Secondly, the optimization step of LA\ndiffers from that of DC by optimizing a different objective\nfunction. Speciﬁcally, DC optimizes the cross-entropy loss\nbetween predicted and ground truth cluster labels, requiring\nan additional and computationally expensive linear readout\nlayer. Moreover, due to arbitrary changes in the cluster label\nindices across iterative rounds, this additional readout layer\nneeds to be frequently recomputed. In contrast, LA em-\nploys an objective function that directly optimizes a local\nsoft-clustering metric, requiring no extra readout layer and\nonly a small amount of additional computation on top of the\nfeature representation training itself. These differences lead\nboth to better ﬁnal performance and substantially improved\ntraining efﬁciency.\nInstance Recognition.\nThe Instance Recognition\ntask [67] (IR) treats each example as its own “category” and\noptimizes the DCNN representation to output an embedding\nin which all examples are well-separated from each other.\nLA uses a similar embedding framework, but achieves sig-\nniﬁcantly better performance by pursuing a distinct opti-\nmization goal. Speciﬁcally, while IR optimizes for equally\nseparating representations of all examples, LA encourages a\nbalance between separation and clustering on a per-example\nbasis, as measured by the local aggregation criterion. For\nthis reason, the LA approach can be thought of as a princi-\npled hybrid between the DC and IR approaches.\nSelf-supervised “missing-data” tasks.\nThese tasks\nbuild representations by hiding some information about\neach example input, and then optimizing the network to\npredict the hidden information from the visible information\nthat remains. Examples include context prediction [13], col-\norization of grayscale images [13], inpainting of missing\nportions of images [52], and the Split-Brain method [71].\nHowever, it is ultimately unclear whether these tasks are\nperfectly aligned with the needs of robust visual represen-\ntation. Indeed, it has been found that deeper networks bet-\nter minimizing the loss functions used in such tasks gain\nlittle transfer learning performance on object recognition\ntasks [14]. Moreover, most missing-data tasks rely on struc-\ntures that are speciﬁc to visual data, making them poten-\ntially less general than the embedding/clustering concepts\nused in DC, IR or our LA method.\nGenerative models. Another broad class of unsuper-\nvised learning algorithm, often termed deep generative\nmodels, focuses on reconstructing input images from a bot-\ntlenecked latent representation. The networks trained by\nthese algorithms use the latent representations for other\ntasks, including object recognition. These learning meth-\nods include classical ones such as Restricted Boltzman Ma-\nchines [27, 41] as well as more recent ones such as Varia-\ntional Auto-Encoders [35] and Generative Adversarial Net-\nworks [15, 20]. Although the features learned by gener-\native models have been put to a wide variety of exciting\nuses [40, 69, 12, 43, 33], their power as latent representa-\ntions for downstream visual tasks such as object recognition\nhas yet to be fully realized.\n3. Methods\nOur overall objective is to learn an embedding func-\ntion fθ (realized via a neural network) that maps images\nI = {x1, x2, ..., xN} to features V = {v1, v2, ..., vN}\nwith vi = fθ(xi) in a compact D-dimension representation\nspace where similar images are clustered while dissimilar\nimages are separated. To achieve this objective, we design\nan iterative procedure to bootstrap the aggregation power of\na deep non-linear embedding function. More speciﬁcally, at\nany given stage during training the embedding function, we\ndynamically identify two sets of neighbors for an xi and its\nembedding vi: close neighbors Ci and background neigh-\nbors Bi. Intuitively, close neighbors are those whose em-\nbeddings should be made similar to vi, while background\nneighbors are used to set the distance scale with respect\nto which the judgement of closeness should be measured.\nTo help better understand these two sets, we provide a\nschematic illustration in Fig. 1, and describe the details of\nhow they are deﬁned mathematically in section 3.1. Using\nBi and Ci, we then deﬁne the level of local aggregation\nL(Ci, Bi|θ, xi) near each input xi, which characterizes the\nrelative level of closeness within Ci, compared to that in\nBi. The parameters θ of the neural network realizing the\nembedding function are then tuned over the course of train-\ning to maximize L(Ci, Bi|θ, xi).\n3.1. Neighbor Identiﬁcation\nWe ﬁrst describe how the neighbor types Bi and Ci are\ndeﬁned. Nearest-neighbor based identiﬁcation for Bi: At\nany given step of optimization, the background neighbors\nfor a given embedded point vi are simply deﬁned as the k\nclosest embedded points Nk(vi) within V, where distance\nis judged using the cosine distance on the embedding space.\nThe number k of background neighbors to be used is a hy-\nperparameter of the algorithm. Robustiﬁed clustering-based\nidentiﬁcation for Ci: To identify close neighbors, we ﬁrst\napply an unsupervised clustering algorithm on all embed-\nded points V to cluster the representations into m groups\nG = {G1, G2, ..., Gm}. Let g(vi) denote the cluster label\nof vi in this clustering result, i.e. i ∈Gg(vi). In the sim-\nplest version of our procedure, we then deﬁne Ci to be the\nset Gg(vi). However, because clustering can be a noisy and\nsomewhat arbitrary process, we compute multiple cluster-\nings under slightly different conditions, and then aggregate\nneighbors across these multiple clusterings to achieve more\nstable results. Speciﬁcally, let {G(j)} be clusters for H\ndistinct clusterings, where G(j) = {G(j)\n1 , G(j)\n2 , ..., G(j)\nm(j)}\nwith j ∈{1, 2, ..., H}, and {g(j)} deﬁned accordingly. We\nthen deﬁne Ci = SH\nj=1 G(j)\ng(j)(vi). The number m of clus-\nters and number H of clusterings are hyperparameters of\nthe algorithm. In this work, we use k-means clustering as\nthe standard unsupervised algorithm.\nIntuitively, background neighbors are an unbiased sam-\nple of nearby points that (dynamically) set the scale at\nwhich “close-ness” should be judged; while close neigh-\nbors are those that are especially nearby, relative to those\nin other clusters. The mathematical deﬁnitions above rep-\nresent just one speciﬁc way to formalize these ideas, and\nmany alternatives are possible. In Section 5.2, we show that\nour choices are not arbitrary by exploring the consequences\nof making alternate decisions.\n3.2. Local Aggregation Metric\nGiven the deﬁnition of Bi and Ci, we describe the for-\nmulation of our local aggregation metric, L(Ci, Bi|θ, xi).\nWe build our formulation upon a non-parametric softmax\noperation proposed by Wu et al. in [67]. In that work, the\nauthors deﬁne the probability that an arbitrary feature v is\nrecognized as the i-th image to be:\nP(i|v) =\nexp(vT\ni v/τ)\nPN\nj=1 exp(vT\nj v/τ)\n(1)\nwhere τ ∈[0, 1] is a ﬁxed scale hyperparameter, and where\nboth {vi} and v are projected onto the L2-unit sphere in the\nD-dimensional embedding space (e.g. normalized such that\n∥v∥2 = 1).\nFollowing equation 1, given an image set A, we then\ndeﬁne the probability of feature v being recognized as an\nimage in A as:\nP(A|v) =\nX\ni∈A\nP(i|v)\n(2)\nFinally, we formulate L(Ci, Bi|θ, xi) as the negative\nlog-likelihood of vi being recognized as a close neighbor\n(e.g. is in Ci), given that vi is recognized as a background\nneighbor (e.g. is in Bi):\nL(Ci, Bi|θ, xi) = −logP(Ci ∩Bi|vi)\nP(Bi|vi)\n(3)\nThe loss to be minimized is then:\nLi = L(Ci, Bi|θ, xi) + λ∥θ∥2\n2\n(4)\nwhere λ is a regularization hyperparameter.\nDiscussion. Because the deﬁnition of L(Ci, Bi|θ, xi)\nis somewhat involved, we describe a simple conceptual\nanalysis that illustrates the intuition for why we chose it\nas a measure of local aggregation. Letting Cc\ni denote the\ncomplement of Ci in I, we have P(Bi|vi) = P(Cc\ni ∩\nBi|vi) + P(Ci ∩Bi|vi). Thus, from equation 3, we see\nthat L(Ci, Bi|θ, xi) is minimized when P(Ci ∩Bi|vi) is\nmaximized and P(Cc\ni ∩Bi|vi) is minimized. It is easy\nto understand the meaning of minimizing P(Cc\ni ∩Bi|vi):\nthis occurs as the distances between vi and its non-close\nbackground neighbors are maximized. The consequences of\nmaximizing P(Ci ∩Bi|vi) are a bit more subtle. As shown\nempirically in [66] (albeit in the supervised context), as long\nas the scaling parameter τ ≪1, maximizing P(A|vi) for\nany set A causes the emergence of natural “sub-categories”\nin (the embeddings of) A, and encourages vi to move closer\nto one of these sub-categories rather than their overall av-\nerage. This empirical result can be intuitively understood\nby recognizing the fact that exp(vT\ni v/τ) increases expo-\nnentially when vT\ni v approaches 1, suggesting that P(A|vi)\nwill approach 1 when A includes a small cluster of features\nthat are all very close to v. Putting these observations to-\ngether, the optimized representation space created by min-\nimizing L(Ci, Bi|θ, xi) is, intuitively, like that shown in\nFig. 1: a set of embedded points that have formed into small\nclusters at a distribution of natural scales.\n...\n...\n...\nInput\nConvNet\nConvNet\nConvNet\nEmbedding Space\nAfter Optimization\n......\n......\nx1\nx3\nx2\njetchev2016texture\njetchev2016texture\nx4\nv1\nClose Neighbors\nBackground \nNeighbors\nFigure 1. Illustration of the Local Aggregation (LA) method. For each input image, we use a deep neural network to embed it into a lower\ndimension space (”Embedding Space” panel). We then identify its close neighbors (blue dots) and background neighbors (black dots). The\noptimization seeks to push the current embedding vector (red dot) closer to its close neighbors and further from its background neighbors.\nThe blue arrow and black arrow are examples of inﬂuences from different neighbors on the current embedding during optimization. The\n”After Optimization” panel illustrates the typical structure of the ﬁnal embedding after training.\n3.3. Memory Bank\nAs deﬁned above, the neighbor identiﬁcation procedures\nand the loss function implicitly describe computations in-\nvolving all the embedded features V, which soon becomes\nintractable for large datasets. To address this issue, we fol-\nlow [67, 66] and maintain a running average for V, which\nis called the memory bank, denoted ¯V = {¯v1, ¯v2, ..., ¯vN}.\nSimilarly to [67, 66], we initialize the memory bank with\nrandom D-dimensional unit vectors and then update its val-\nues by mixing ¯vi and vi during training as follows:\n¯vi ←(1 −t)¯vi + tvi\n(5)\nwhere t ∈[0, 1] is a ﬁxed mixing hyperparameter. With the\nhelp of ¯V, we can then rewrite the neighbor identiﬁcation\nprocedures and equation 1 by replacing the feature sets V\nwith ¯V. In particular for Ci, the cluster label function g is\napplied to ¯vi by index identiﬁcation, ensuring the chosen\ncluster includes the index i itself. After this replacement, it\nis no longer necessary to recompute V before every step to\nidentify (good approximations of) Ci and Bi.\n4. Results\nIn this section, we describe tests of the LA method on\nvisual representation learning and compare its performance\nto that of other methods.\n4.1. Experiment Settings\nWe ﬁrst list key parameters used for network training.\nFollowing [67], we set parameter τ = 0.07, D = 128,\nλ = 0.0001, and t = 0.5. For all network structures, we\nuse SGD with momentum of 0.9 and batch size 128. Initial\nlearning rates are set to 0.03, and dropped by a factor of 10\nwhen validation performances saturate, typically leading to\ntraining for 200 epochs with two learning rate drops. Most\nof these parameters are taken from [67], as our conceptual\nframework is similar, but a further hyper-parameter search\nmight lead to better results, given that our optimization goal\ndiffers substantially.\nAs a warm start for our models, we begin training using\nthe IR loss function for the ﬁrst 10 epochs, before switch-\ning over to using the LA method. Following the methods\nof [6], for AlexNet [37] and VGG16 [60] architectures, we\nadd batch normalization (BN) layers [32] after all convolu-\ntion and fully-connected layers, before ReLu operations, to\nallow a higher learning rate and a faster convergence speed.\nThough adding BN is known to improve convergence speed\nbut not typically to lead to higher ﬁnal ImageNet perfor-\nmance levels using supervised training regimes, it is unclear\nwhether this remains true when using unsupervised train-\ning methods. Importantly, the potentially competitive IR\nmethod [67] did not originally include BN in their AlexNet\nand VGG16, so to ensure that we have fairly compared that\nmethod to LA or DC, we also train AlexNet and VGG16\nwith BN on the IR task. For all structures, we replace the ﬁ-\nnal category readout layer with a linear layer with D output\nunits, followed by a L2-normalization operation to ensure\nthat the output is a unit vector.\nWe set k = 4096 for computing Bi using the near-\nest neighbors procedure.\nIn computing Ci, we use k-\nmeans [44] implemented in Faiss [34] as the standard unsu-\npervised clustering algorithm, generating multiple cluster-\nings for robustness via different random initializations. Us-\ning the notation of Section 3, AlexNet is trained with H =\n3, m = 30000, VGG16 is trained with H = 6, m = 10000,\nall ResNet structures are trained with H = 10, m = 30000.\nWe justify all parameter choices and intuitively explain why\nthey are optimal in Section 5.2. All code for reproducing\nour training is available at: [WEBSITE WITHHOLD].\n4.2. Transfer Learning Results\nAfter fully training networks on ImageNet, we then test\nthe quality of the learned visual representations by evalu-\nating transfer learning to other tasks, including ImageNet\nclassiﬁcation on held-out validation images, scene classi-\nﬁcation on Places205 [72], and object detection on PAS-\nCAL VOC 2007 [17]. For classiﬁcation tasks, we also re-\nport K-nearest neighbor (KNN) classiﬁcation results using\nthe embedding features, acquired via a method similar to\nthat in [67]. Speciﬁcally, we take top K nearest neighbors\nNK for the feature v either (for ImageNet) from the saved\nmemory bank or (for Places) from the computed network\noutputs for center crops of training images. Their labels are\nthen weighted by exp(vT\ni v/τ) and combined to get ﬁnal\npredictions. We report results for K = 200 as in [67].\nObject Recognition. To evaluate transfer learning for\nthe ImageNet classiﬁcation task, we ﬁx network weights\nlearned during the unsupervised procedure, add a linear\nreadout layer on top of each layer we want to evaluate, and\ntrain the readout using cross-entropy loss together with L2\nweight decay. We use SGD with momentum of 0.9, batch\nsize 128, and weight decay 0.0001. Learning rate is ini-\ntialized at 0.01 and dropped by a factor of 10 when per-\nformance saturates, typically leading to 90 training epochs\nwith two learning rate drops. We report 10-crop valida-\ntion performances to ensure comparability with [6]. Per-\nformance results in Table 1 show that LA signiﬁcantly\noutperforms other methods with all architectures, espe-\ncially in deeper architectures. LA-trained AlexNet reaches\n42.4%, which is 1.4% higher than previous state-of-the-\nart. Improvements over previous unsupervised state-of-the-\nart are substantially larger for VGG16 (+4.9%), ResNet-\n18 (+3.7%), and ResNet-50 (+6.2%). In particular, LA-\ntrained ResNet-50 achieves 60.2% top-1 accuracy on Im-\nageNet classiﬁcation, surpassing AlexNet trained directly\non the supervised task. Using KNN classiﬁers, LA out-\nperforms the IR task by a large margin with all architec-\ntures. There is a consistent performance increase for the\nLA method both from overall deeper architectures, and\nfrom earlier layers to deeper layers within an architecture.\nMost alternative training methods (e.g. [51, 45, 13, 70]) do\nnot beneﬁt signiﬁcantly from increasing depth. For exam-\nple, ResNet-101 trained using Color [70] can only achieve\n39.6% and the best performance using ResNet-101 with un-\nsupervised task is only 48.7% with CPC [50].\nScene Categorization. To test the generalization abil-\nity of the learned representations to a data distribution dis-\ntinct from that used in training, we assessed transfer to the\nPlaces [72] dataset, which includes 2.45M images labelled\nwith 205 scene categories. As in the previous section, we\ntrain linear readout layers for the scene categorization task\non top of the pretrained ImageNet model, using training\nprocedures and hyper-parameters identical to those used in\nMethod\nconv1 conv2 conv3 conv4 conv5 KNN\nAlexNet\nRandom\n11.6\n17.1\n16.9\n16.3\n14.1\n3.5\nContext [13]\n16.2\n23.3\n30.2\n31.7\n29.6\n–\nColor [70]\n13.1\n24.8\n31.0\n32.6\n31.8\n–\nJigsaw [48]\n19.2\n30.1\n34.7\n33.9\n28.3\n–\nCount [49]\n18.0\n30.6\n34.3\n32.5\n25.7\n–\nSplitBrain [71]\n17.7\n29.3\n35.4\n35.2\n32.8\n11.8\nIR [67]\n16.8\n26.5\n31.8\n34.1\n35.6\n31.3\nIR(with BN)*\n18.4\n30.1\n34.4\n39.2\n39.9\n34.9\nDC [6]\n13.4\n32.3\n41.0\n39.6\n38.2\n–\nLA (ours)\n18.7\n32.7\n38.1\n42.3\n42.4\n38.1\nVGG16\nIR\n16.5\n21.4\n27.6\n35.1\n39.2\n33.9\nIR(with BN)*\n13.2\n18.7\n27.3\n39.8\n50.4\n42.1\nDC*\n18.2\n27.5\n41.5\n51.3\n52.7\n–\nLA (ours)\n14.3\n23.4\n28.3\n44.5\n57.6\n46.6\nResNet-18\nIR\n16.0\n19.9\n29.8\n39.0\n44.5\n41.0\nDC*\n16.4\n17.2\n28.7\n44.3\n49.1\n–\nLA (ours)\n9.1\n18.7\n34.8\n48.4\n52.8\n45.0\nResNet-50\nIR\n15.3\n18.8\n24.9\n40.6\n54.0\n46.5\nDC*\n18.9\n27.3\n36.7\n52.4\n44.2\n–\nLA (ours)\n10.2\n23.3\n39.3\n49.0\n60.2\n49.4\nTable 1.\nImageNet transfer learning and KNN classiﬁer perfor-\nmance. Numbers within the red box are the best for the given\narchitecture. Performances of most methods using AlexNet are\ntaken from [6, 67]. *: performance number produced by us, please\nrefer to the supplementary material for training details.\nImageNet transfer learning. Results shown in Table 2 il-\nlustrate that the LA method surpasses previous methods in\ntransfer learning performance with all architectures, espe-\ncially with deeper networks. Please refer to the supplemen-\ntary material for K-nearest neighbor classiﬁcation perfor-\nmance. These result indicate strong generalization ability\nof the visual representations learned via the LA method.\nObject Detection.\nThe results presented in Table 1\nand 2 illustrate the utility of LA for learning representations\nfor visual categorization tasks. However, visual challenges\nfaced in real life also include other tasks, such as object\ndetection. Therefore, we also evaluate the transfer learn-\ning ability of our models to the object detection task in the\nPASCAL VOC 2007 [17] dataset. The typical PASCAL\ndetection task evaluation procedure [6, 71, 67, 64] ﬁne-\ntunes unsupervised architectures using the Fast RCNN [19]\nmethod. However, Fast RCNN is substantially less com-\nputationally efﬁcient than more recently proposed pipelines\nsuch as Faster RCNN [55] or Mask RCNN [23], and is\nless well-supported by validated reference implementations\nMethod\nconv1 conv2 conv3 conv4 conv5\nAlexNet\nRandom\n15.7\n20.3\n19.8\n19.1\n17.5\nContext [13]\n19.7\n26.7\n31.9\n32.7\n30.9\nColor [70]\n22.0\n28.7\n31.8\n31.3\n29.7\nJigsaw [48]\n23.0\n32.1\n35.5\n34.8\n31.3\nSplitBrain [71]\n21.3\n30.7\n34.0\n34.1\n32.5\nIR [67]\n18.8\n24.3\n31.9\n34.5\n33.6\nIR(with BN)*\n21.3\n33.0\n36.5\n39.2\n38.7\nDC [6]\n19.6\n33.2\n39.2\n39.8\n34.7\nLA (ours)\n18.7\n32.7\n38.2\n40.3\n39.5\nVGG16\nIR\n17.6\n23.1\n29.5\n33.8\n36.3\nIR(with BN)*\n17.3\n22.9\n27.3\n39.3\n45.8\nDC*\n21.5\n31.6\n40.9\n45.2\n44.2\nLA (ours)\n20.1\n25.9\n31.9\n44.0\n50.0\nResNet-18\nIR\n17.8\n23.0\n30.1\n37.0\n38.1\nDC*\n16.4\n22.5\n30.5\n40.4\n41.8\nLA (ours)\n18.9\n26.7\n36.5\n44.7\n45.6\nResNet-50\nIR\n18.1\n22.3\n29.7\n42.1\n45.5\nDC*\n20.1\n29.1\n35.3\n43.2\n38.9\nLA (ours)\n10.3\n26.4\n39.9\n47.2\n50.1\nTable 2.\nPlaces transfer learning performance. *: performances\nproduced by us, please refer to the supplement for details.\nin common deep learning frameworks. To ensure training\nefﬁciency and correctness, in this work we have used the\nFaster RCNN pipeline from validated implementations in\nboth TensorFlow and Pytorch. However, because the per-\nformance achieved by Faster RCNN can vary somewhat\nfrom that of Fast RCNN, direct comparison of these results\nto numbers generated with Fast RCNN may be mislead-\ning. For this reason, we have additionally evaluated models\ntrained with IR and DC using Faster RCNN where possible.\nFor implementation details, please refer to the supplemen-\ntary material. Results are shown in Table 3, illustrating that\nthe LA method achieves state-of-the-art unsupervised trans-\nfer learning for the PASCAL detection task. Interestingly,\nthe performance gaps between the best unsupervised meth-\nods and the supervised controls are comparatively smaller\nfor the PASCAL task than for the classiﬁcation tasks.\n5. Analysis\n5.1. Visualizations\nIn this subsection, we analyze the embedding space\nthrough visualizations.\nDensity distribution in the embedding space. The LA\noptimization objective seeks to minimize the distances be-\nMethod\nAFast AFaster VFast VFaster RFaster\nSupervised\n56.8\n54.3\n67.3\n70.0\n74.6\nJigsaw [48]\n53.2\n–\n–\n–\n–\nVideo [63]\n47.2\n–\n60.2\n–\n–\nContext [13]\n51.1\n–\n61.5\n–\n–\nTrans [64]\n–\n–\n63.2\n–\n–\nIR [67]\n48.1\n53.1\n60.5\n65.6\n65.4\nDC\n55.4\n–\n65.9\n–\n–\nLA (ours)\n–\n53.5\n–\n68.4\n69.1\nTable 3.\nPASCAL VOC 2007 detection mAP. A=AlexNet,\nV=VGG16, and R=ResNet50. Bold numbers are the best in their\ncolumns. Performances with Faster RCNN are produced by us,\nexcept that of ResNet50 of IR, which is as reported in [67]. Most\nnumbers using Fast RCNN are taken from [6, 67]. For numbers\nproduced by us, we show the averages of three independent runs.\nStandard deviations are close to 0.2% in all cases.\nLA-50\nLA\nIR\nLA-50\nLA\nIR\n10\n70\n50\n30\n×103\n10\n70\n50\n30\n×103\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.36\n0.34\n0.32\n0.30\n0.28\n0.26\nBackground Density\nLocal Density\nDensity\nImage count\nFigure 2.\nDistributions across all ImageNet training images of\nlocal and background densities for feature embeddings. We com-\npare features from ResNet-18 (orange bars) and Resnet-50 (green\nbars) architectures as trained by the LA method, as well as that of\na ResNet-18 architecture trained by the Instance Recognition (IR)\nmethod (blue bars). The local and background densities at each\nembedded vector are estimated by averaging dot products between\nthat vector and, respectively, its top 30 or its 1000th-4096th, near-\nest neighbors in ¯V. See supplementary material for more detail.\ntween vi and Ci while maximizing those between vi and\nBi, intuitively leading to an embedding that is locally dense\nat some positions but generally sparse across the space. In-\ndeed, Figure 2 shows that the local density of the LA em-\nbedding is much higher than that created by the IR method,\nwhile the background density is only slightly higher (note\ndiffering x-axis scales in the ﬁgure). Moreover, insofar as\ndeeper networks achieve lower minimums of the LA ob-\njective, we expect that their embeddings will exhibit higher\nlocal density and lower background density as compared to\nshallower networks. By comparing the density distributions\nof the ResNet-18 embedding to that of ResNet-50, Figure 2\nshows that this expectation is conﬁrmed. These results help\nbetter characterize the LA optimization procedure.\nSuccess and failure examples. To help qualitatively il-\nlustrate the successes and failures of the LA objective, Fig-\nure 3 shows nearest neighbors in the training set for sev-\neral validation images, both correctly and incorrectly clas-\nNearest neighbors\nSuccessful images\nFailure images\nGT: Binoculars \nPred: Mountain bike\nGT: Bolete\nPred: Stinkhorn\nGT: Table Lamp\nPred: Saltshaker\nZebra\nRock_beauty\nGreat grey owl\nFigure 3.\nFor each of several validation images in the left-most\ncolumn, nearest neighbors in LA-trained RestNet-50 embedding,\nwith similarity decreasing from left to right. The three top columns\nare successfully-classiﬁed cases, with high KNN-classiﬁer con-\nﬁdence, while the lower three are failure cases, with low KNN-\nclassiﬁer conﬁdence.\nsiﬁed according to the nearest-neighbor classiﬁer. Unsur-\nprisingly, the successful examples show that the LA-trained\nmodel robustly groups images belonging to the same cat-\negory regardless of backgrounds and view points. Interest-\ningly, however, the network shows substantial ability to rec-\nognize high-level visual context. This is even more obvious\nfor the failure cases, where it can be seen that the network\ncoherently groups images according to salient characteris-\ntics. In fact, most failure cases produced by the LA model\nappear to be due to the inherently ill-posed nature of the\nImageNet category labelling, in which the category label is\nonly one of several potentially valid object types present in\nthe image, and which no unsupervised method could unam-\nbiguously resolve. To further illustrate this point, we use the\nmulti-dimensional scaling (MDS) algorithm [3] to visualize\npart of the embedding space (see Fig. 4). In particular, the\nLA successfully clusters images with trombones regardless\nof background, number of trombones, or viewpoint, while\nit (perhaps inevitably) distinguishes those images from im-\nages of humans playing trombones.\nHigh Accuracy Classes\nLow Accuracy Classes\nFigure 4.\nMulti-dimensional scaling (MDS) embedding results\nfor network outputs of classes with high validation accuracy (left\npanel) and classes with low validation accuracy (right panel). For\neach class, we randomly choose 100 images of that class from\nthe training set and apply the MDS algorithm to the resulting 600\nimages. Dots represent individual images in each color-coded cat-\negory. Gray boxes show examples of images from a single class\n(”trombone”) that have been embedded in two distinct subclusters.\nChoice of Bi\n{1, 2, ..., N} Cluster-based N4096\nNN performance\n30.2\n33.2\n35.7\nTable 4. Nearest neighbor validation performances of ResNet-18\ntrained with different choices of Bi. We use H = 3 and m =\n1000 for cluster-based Bi to make the number of neighbors in Bi\ncomparable to 4096. In all experiments, we use cluster-based Ci\nwith H = 1 and m = 10000.\n5.2. Ablations\nIn this subsection, we empirically justify the design of\nthe LA procedure by ablating or modifying several key fea-\ntures of the procedure. We also provide analyses suggesting\nintuitive reasons underlying the meaning and inﬂuence of\nparameters on ﬁnal performance. Please refer to the supple-\nmentary material for further analyses.\nDynamic Locality for Background Neighbors.\nWe\nchose a nearest-neighbor based procedure for identifying\nBi to embody the idea of dynamically rescaling the local\nbackground against which closeness is judged. We tested\ntwo ablations of our procedure that isolate the relevance of\nthis choice, including (i) simply using all inputs for back-\nground, or (ii) using a ﬁxed clustering-based identiﬁcation\nprocedure. (See supplement for details on how these were\ndeﬁned.) Experiments show that the local dynamic nearest-\nneighbor procedure is substantially more performant than\neither ablation (see Table 4).\nThe desirability of a lo-\ncal rather than global background measurement is consis-\ntent with the observation that the density of features varies\nwidely across the embedding space (see Figure 2). That\nthe dynamic nature of the computation of the background is\nCi\n{i}\nNk′ (1, 10k) (3, 10k) (10, 10k) (10, 30k)\nNN 33.9\n0.1\n35.7\n36.2\n36.1\n37.9\nTable 5. Nearest neighbor validation performances of ResNet-18\ntrained with different choices of Ci. All experiments use N4096\nas Bi. {i} means Ci only includes vi itself. (1, 10k) means\nclustering-based Ci with H = 1 and m = 10000. Other pairs\nhave similar meanings. See the supplementary material for details.\nuseful is illustrated by the comparison of results from com-\nputing neighbors in an online fashion from vi, relative to\nthe cluster-based procedure depending only on ¯V.\nRobust Clustering for Close Neighbors.\nWe also\nsought to understand the importance of the speciﬁc cluster-\ning procedure for deﬁning close neighbors Ci. One alter-\nnative to using cluster-based identiﬁcation would be to in-\nstead identify “especially close” neighbors as those within\na neighborhood Nk′, for some k′ ≪k. Using this in the\ndeﬁnition of Ci is equivalent to optimizing the embedding\nto bring especially close neighbors closer together, while\nsomewhat further away neighbors are moved apart. While\nthis approach would have been a conceptually simpler way\nto deﬁne local aggregation than the cluster-based deﬁnition\nof close neighbors, it turns out to be substantially less effec-\ntive in producing a useful representation (see Table 5).\nGiven the need for cluster-based identiﬁcation, a variety\nof alternative approaches to k-means are theoretically pos-\nsible, including DBSCAN [16], Afﬁnity Propagation [18],\nspectral methods [59, 62], and gaussian mixtures [56, 57].\nHowever, our present context is strongly constrained by the\nrequirement that the clustering algorithm scale well to large\ndatasets, effectively limiting the options to k-means and\nDBSCAN. Unfortunately, DBSCAN is known to perform\npoorly in settings with high ambient dimensions or highly\nvariable density distributions [16], both of which are char-\nacteristics of the embedding space we work with here (see\nFigure 2). Indeed, we ﬁnd that replacing k-means with DB-\nSCAN leads to trivial representations, across a wide variety\nof parameter settings (see supplement for details).\nThe robust clustering procedure described in Section 3.1\nhas several hyperparameters, including number of clusters\nm and number of clusterings H. To intuitively understand\ntheir effect, we performed a set of network characteriza-\ntion experiments (see supplement for details). These exper-\niments indicated that two basic factors were of importance\nin creating clusterings that lead to good representations: the\nskewness of the cluster of close neighbors around its in-\ntended target, as measured by the distance from the cluster\ncenter to the embedded vector vi, and the size of the clus-\nter, as measured by its cardinality as a set. We found that\n(i) clusterings of close neighbors with lower skewness were\nrobustly associated with better performance, indicating that\nskewness should be minimized whenever possible; and (ii)\nthere was an optimal size for the set of close neighbors that\nscaled with the representation capacity (i.e. depth) of the\nunderlying network. Both of these facts are consistent with\na picture in which the ideal embedding is one in which each\ncategory is equally likely to occur and in which each ex-\nample of each category is equally “representative” – e.g. in\nwhich clusters of points corresponding to natural categories\noccupy isotropic spheres of equal size. Networks of smaller\ncapacity that cannot completely achieve the optimal distri-\nbution will (poorly) approximate the optimal embedding by\nfracturing their embeddings of single categories into subsets\nthat maintain isotropy by reducing the relative size of clus-\nters, each containing only part of the true category. These\nconsiderations help explain the optimal settings for parame-\nters H and m: higher H (i.e. more clusterings) will tend to\nproduce more isotropic clusters, as outliers due to random-\nness are averaged out. However, increasing H beyond a\npoint set by the capacity of the network will lead to clusters\nof too large a size for the network to handle (see supple-\nment Figure 1, from A to B, or from B to C). This nega-\ntive inﬂuence can be shown in Table 5 by the slight perfor-\nmance drop from (3, 10k) to (10, 10k). Increasing m (e.g.\nthe number of clusters) can then compensate by decreasing\nthe neighborhood size without increasing cluster anisotropy\n(see supplement Figure 1, from C to D). This conpensation\ncan be shown in Table 5 by the performance increase from\n(10, 10k) to (10, 30k). More experiments detailing these\nconclusions are shown in the supplementary material.\n6. Discussion\nIn this work, we have introduced a local aggregation\n(LA) objective for learning feature embeddings that seeks to\ndiscover a balance between bringing similar inputs together\nand allowing dissimilar inputs to move apart, embodying a\nprincipled combination of several key ideas from recent ad-\nvances in unsupervised learning. We have shown that when\napplied to DCNNs, the LA objective creates representations\nthat are useful for transfer learning to a variety of challeng-\ning visual tasks. We also analyze aspects of our procedure,\ngiving an intuition for how it works.\nIn future work we hope to improve the LA objective\nalong a variety of directions, including incorporating non-\nlocal manifold learning-based priors for detecting similar-\nity, improving identiﬁcation of dissimilarity via measures\nof representational change over multiple steps of learning,\nand extending to the case of non-deterministic embedding\nfunctions. We also seek to apply the LA objective beyond\nthe image processing domain, including to video and audio\nsignals. Finally, we hope to compare the LA procedure to\nbiological vision systems, both in terms of the feature rep-\nresentations learned and the dynamics of learning during\nvisual development.\nReferences\n[1] J. Atkinson. The developing visual brain. 2002. 1\n[2] H. B. Barlow. Unsupervised learning. Neural computation,\n1(3):295–311, 1989. 2\n[3] I. Borg and P. Groenen. Modern multidimensional scaling:\nTheory and applications. Journal of Educational Measure-\nment, 40(3):277–280, 2003. 7\n[4] J. A. Bourne and M. G. Rosa. Hierarchical development of\nthe primate visual cortex, as revealed by neuroﬁlament im-\nmunoreactivity: early maturation of the middle temporal area\n(mt). Cerebral cortex, 16(3):405–414, 2005. 1\n[5] O. Braddick and J. Atkinson. Development of human visual\nfunction. Vision research, 51(13):1588–1609, 2011. 1\n[6] M. Caron, P. Bojanowski, A. Joulin, and M. Douze. Deep\nclustering for unsupervised learning of visual features. In\nProceedings of the European Conference on Computer Vi-\nsion (ECCV), pages 132–149, 2018. 1, 2, 4, 5, 6, 12\n[7] J. Carreira and A. Zisserman. Quo vadis, action recognition?\na new model and the kinetics dataset. In proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 6299–6308, 2017. 1\n[8] L. B. Cohen and M. S. Strauss. Concept acquisition in the\nhuman infant. Child development, pages 419–424, 1979. 1\n[9] A. Conneau, H. Schwenk, L. Barrault, and Y. Lecun. Very\ndeep convolutional networks for natural language process-\ning. arXiv preprint arXiv:1606.01781, 2, 2016. 1\n[10] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.\nImagenet: A large-scale hierarchical image database. 2009.\n1, 13\n[11] L. Deng, G. Hinton, and B. Kingsbury. New types of deep\nneural network learning for speech recognition and related\napplications: An overview. In 2013 IEEE International Con-\nference on Acoustics, Speech and Signal Processing, pages\n8599–8603. IEEE, 2013. 1\n[12] E. L. Denton, S. Chintala, R. Fergus, et al. Deep genera-\ntive image models using a laplacian pyramid of adversarial\nnetworks. In Advances in neural information processing sys-\ntems, pages 1486–1494, 2015. 2\n[13] C. Doersch, A. Gupta, and A. A. Efros. Unsupervised vi-\nsual representation learning by context prediction. In Pro-\nceedings of the IEEE International Conference on Computer\nVision, pages 1422–1430, 2015. 1, 2, 5, 6\n[14] C. Doersch and A. Zisserman.\nMulti-task self-supervised\nvisual learning. In Proceedings of the IEEE International\nConference on Computer Vision, pages 2051–2060, 2017. 2\n[15] J. Donahue, P. Kr¨ahenb¨uhl, and T. Darrell. Adversarial fea-\nture learning. arXiv preprint arXiv:1605.09782, 2016. 1,\n2\n[16] M. Ester, H.-P. Kriegel, J. Sander, X. Xu, et al. A density-\nbased algorithm for discovering clusters in large spatial\ndatabases with noise. In Kdd, volume 96, pages 226–231,\n1996. 8\n[17] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and\nA. Zisserman. The pascal visual object classes (voc) chal-\nlenge. International journal of computer vision, 88(2):303–\n338, 2010. 5\n[18] B. J. Frey and D. Dueck. Clustering by passing messages\nbetween data points. science, 315(5814):972–976, 2007. 8\n[19] R. Girshick. Fast r-cnn. In Proceedings of the IEEE inter-\nnational conference on computer vision, pages 1440–1448,\n2015. 5\n[20] I. Goodfellow,\nJ. Pouget-Abadie,\nM. Mirza,\nB. Xu,\nD. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-\nerative adversarial nets. In Advances in neural information\nprocessing systems, pages 2672–2680, 2014. 2\n[21] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos,\nE. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates,\net al. Deep speech: Scaling up end-to-end speech recogni-\ntion. arXiv preprint arXiv:1412.5567, 2014. 1\n[22] R. S. Harwerth, E. L. Smith, G. C. Duncan, M. Craw-\nford, and G. K. Von Noorden.\nMultiple sensitive periods\nin the development of the primate visual system. Science,\n232(4747):235–238, 1986. 1\n[23] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick. Mask r-cnn.\nIn Proceedings of the IEEE international conference on com-\nputer vision, pages 2961–2969, 2017. 1, 5\n[24] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n770–778, 2016. 1, 12\n[25] D. O. Hebb. The organization of behavior. na, 1961. 2\n[26] G. Hinton, L. Deng, D. Yu, G. Dahl, A.-r. Mohamed,\nN. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, B. Kingsbury,\net al. Deep neural networks for acoustic modeling in speech\nrecognition. IEEE Signal processing magazine, 29, 2012. 1,\n12\n[27] G. E. Hinton, S. Osindero, and Y.-W. Teh.\nA fast learn-\ning algorithm for deep belief nets.\nNeural computation,\n18(7):1527–1554, 2006. 2\n[28] G. E. Hinton and R. R. Salakhutdinov.\nReducing the\ndimensionality of data with neural networks.\nscience,\n313(5786):504–507, 2006. 2\n[29] J. Hirschberg and C. D. Manning. Advances in natural lan-\nguage processing. Science, 349(6245):261–266, 2015. 1\n[30] J. J. Hopﬁeld. Neural networks and physical systems with\nemergent collective computational abilities. Proceedings of\nthe national academy of sciences, 79(8):2554–2558, 1982. 2\n[31] J. S. Husaim and L. B. Cohen. Infant learning of ill-deﬁned\ncategories. Merrill-Palmer Quarterly of Behavior and De-\nvelopment, pages 443–456, 1981. 1\n[32] S. Ioffe and C. Szegedy. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift.\narXiv preprint arXiv:1502.03167, 2015. 4\n[33] N. Jetchev, U. Bergmann, and R. Vollgraf. Texture synthesis\nwith spatial generative adversarial networks. arXiv preprint\narXiv:1611.08207, 2016. 2\n[34] J. Johnson, M. Douze, and H. J´egou. Billion-scale similarity\nsearch with gpus. arXiv preprint arXiv:1702.08734, 2017. 4\n[35] D. P. Kingma and M. Welling. Auto-encoding variational\nbayes. arXiv preprint arXiv:1312.6114, 2013. 2\n[36] P. Kr¨ahenb¨uhl, C. Doersch, J. Donahue, and T. Darrell. Data-\ndependent initializations of convolutional neural networks.\narXiv preprint arXiv:1511.06856, 2015. 1\n[37] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\nImagenet\nclassiﬁcation with deep convolutional neural networks. In\nAdvances in neural information processing systems, pages\n1097–1105, 2012. 1, 4\n[38] A. Kumar, O. Irsoy, P. Ondruska, M. Iyyer, J. Bradbury,\nI. Gulrajani, V. Zhong, R. Paulus, and R. Socher. Ask me\nanything: Dynamic memory networks for natural language\nprocessing. In International Conference on Machine Learn-\ning, pages 1378–1387, 2016. 1\n[39] Q. V. Le, M. Ranzato, R. Monga, M. Devin, K. Chen, G. S.\nCorrado, J. Dean, and A. Y. Ng. Building high-level fea-\ntures using large scale unsupervised learning. arXiv preprint\narXiv:1112.6209, 2011. 2\n[40] C. Ledig, L. Theis, F. Husz´ar, J. Caballero, A. Cunningham,\nA. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, et al.\nPhoto-realistic single image super-resolution using a gener-\native adversarial network. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n4681–4690, 2017. 2\n[41] H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Convolu-\ntional deep belief networks for scalable unsupervised learn-\ning of hierarchical representations.\nIn Proceedings of the\n26th annual international conference on machine learning,\npages 609–616. ACM, 2009. 2\n[42] T. L. Lewis and D. Maurer. Multiple sensitive periods in hu-\nman visual development: evidence from visually deprived\nchildren.\nDevelopmental Psychobiology: The Journal of\nthe International Society for Developmental Psychobiology,\n46(3):163–183, 2005. 1\n[43] C. Li and M. Wand.\nPrecomputed real-time texture syn-\nthesis with markovian generative adversarial networks. In\nEuropean Conference on Computer Vision, pages 702–716.\nSpringer, 2016. 2\n[44] S. Lloyd. Least squares quantization in pcm. IEEE transac-\ntions on information theory, 28(2):129–137, 1982. 4\n[45] T. Malisiewicz, A. Gupta, and A. A. Efros.\nEnsemble of\nexemplar-svms for object detection and beyond. 2011. 5\n[46] D. Mareschal and P. C. Quinn. Categorization in infancy.\nTrends in cognitive sciences, 5(10):443–450, 2001. 1\n[47] K. Noda, Y. Yamaguchi, K. Nakadai, H. G. Okuno, and\nT. Ogata. Audio-visual speech recognition using deep learn-\ning. Applied Intelligence, 42(4):722–737, 2015. 1\n[48] M. Noroozi and P. Favaro. Unsupervised learning of visual\nrepresentations by solving jigsaw puzzles. In European Con-\nference on Computer Vision, pages 69–84. Springer, 2016. 1,\n5, 6\n[49] M. Noroozi, H. Pirsiavash, and P. Favaro. Representation\nlearning by learning to count. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pages 5898–\n5906, 2017. 1, 5\n[50] A. v. d. Oord, Y. Li, and O. Vinyals.\nRepresentation\nlearning with contrastive predictive coding. arXiv preprint\narXiv:1807.03748, 2018. 5\n[51] D. Pathak, R. Girshick, P. Doll´ar, T. Darrell, and B. Hariha-\nran. Learning features by watching objects move. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 2701–2710, 2017. 5\n[52] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A.\nEfros. Context encoders: Feature learning by inpainting. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 2536–2544, 2016. 2\n[53] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-\nsentation learning with deep convolutional generative adver-\nsarial networks. arXiv preprint arXiv:1511.06434, 2015. 2\n[54] D. H. Rakison and L. M. Oakes. Early category and con-\ncept development: Making sense of the blooming, buzzing\nconfusion. Oxford University Press, 2003. 1\n[55] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: To-\nwards real-time object detection with region proposal net-\nworks. In Advances in Neural Information Processing Sys-\ntems (NIPS), 2015. 5, 13\n[56] D. Reynolds. Gaussian mixture models. Encyclopedia of\nbiometrics, pages 827–832, 2015. 8\n[57] D. A. Reynolds, T. F. Quatieri, and R. B. Dunn. Speaker\nveriﬁcation using adapted gaussian mixture models. Digital\nsignal processing, 10(1-3):19–41, 2000. 8\n[58] T. D. Sanger.\nOptimal unsupervised learning in a single-\nlayer linear feedforward neural network. Neural networks,\n2(6):459–473, 1989. 2\n[59] J. Shi and J. Malik. Normalized cuts and image segmenta-\ntion. Departmental Papers (CIS), page 107, 2000. 8\n[60] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. arXiv preprint\narXiv:1409.1556, 2014. 1, 4\n[61] O. Sporns, D. R. Chialvo, M. Kaiser, and C. C. Hilgetag. Or-\nganization, development and function of complex brain net-\nworks. Trends in cognitive sciences, 8(9):418–425, 2004. 1\n[62] X. Y. Stella and J. Shi. Multiclass spectral clustering. In null,\npage 313. IEEE, 2003. 8\n[63] X. Wang and A. Gupta. Unsupervised learning of visual rep-\nresentations using videos. In Proceedings of the IEEE Inter-\nnational Conference on Computer Vision, pages 2794–2802,\n2015. 1, 6\n[64] X. Wang, K. He, and A. Gupta. Transitive invariance for\nself-supervised visual representation learning. In Proceed-\nings of the IEEE International Conference on Computer Vi-\nsion, pages 1329–1338, 2017. 5, 6\n[65] J. Wattam-Bell, D. Birtles, P. Nystr¨om, C. Von Hofsten,\nK. Rosander, S. Anker, J. Atkinson, and O. Braddick. Reor-\nganization of global form and motion processing during hu-\nman visual development. Current Biology, 20(5):411–415,\n2010. 1\n[66] Z. Wu, A. A. Efros, and S. X. Yu. Improving generaliza-\ntion via scalable neighborhood component analysis. In Pro-\nceedings of the European Conference on Computer Vision\n(ECCV), pages 685–701, 2018. 3, 4\n[67] Z. Wu, Y. Xiong, S. X. Yu, and D. Lin. Unsupervised feature\nlearning via non-parametric instance discrimination. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 3733–3742, 2018. 1, 2, 3, 4, 5, 6\n[68] T. Young, D. Hazarika, S. Poria, and E. Cambria.\nRe-\ncent trends in deep learning based natural language process-\ning. ieee Computational intelligenCe magazine, 13(3):55–\n75, 2018. 1\n[69] H. Zhang, T. Xu, H. Li, S. Zhang, X. Wang, X. Huang, and\nD. N. Metaxas. Stackgan: Text to photo-realistic image syn-\nthesis with stacked generative adversarial networks. In Pro-\nceedings of the IEEE International Conference on Computer\nVision, pages 5907–5915, 2017. 2\n[70] R. Zhang, P. Isola, and A. A. Efros. Colorful image col-\norization. In European conference on computer vision, pages\n649–666. Springer, 2016. 1, 5, 6\n[71] R. Zhang, P. Isola, and A. A. Efros. Split-brain autoencoders:\nUnsupervised learning by cross-channel prediction. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 1058–1067, 2017. 1, 2, 5, 6\n[72] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva.\nLearning deep features for scene recognition using places\ndatabase. In Advances in neural information processing sys-\ntems, pages 487–495, 2014. 1, 5, 13\nSupplementary Material\nA. Clustering Combination\nIn this section we provide an illustration ﬁgure for the\neffects of combining multiple clusterings in Figure 1, which\nis also mentioned in Section 5.2. Additionally, we show\nthe nearest neighbor validation performances in Table 1 to\nsupport our hyper-parameter choices for H, m in different\narchitectures.\nA.\nC.\nB.\nD.\nFigure 1.\nIllustration of the effect of combining across multi-\nple clusterings to achieve robustness. The target embedded vector\nvi is represented by the red dot, while blue dots represent close\nneighbors Ci under the speciﬁed hyperparameter settings.\nSetting\nNetwork\nA\nV\nR-18 R-50\n(1, 1k)\n–\n–\n35.2\n–\n(1, 10k)\n30.6 38.9 35.7\n40.2\n(1, 20k)\n–\n–\n35.0\n–\n(3, 10k)\n31.1\n–\n36.2\n–\n(6, 10k)\n30.4 39.7 37.3\n42.4\n(10, 10k)\n–\n–\n36.1\n42.3\n(10, 30k)\n–\n–\n37.9\n43.4\nTable 1.\nNearest neighbor validation performances of different\narchitectures trained with different choices of Ci.\n“A” means\n“AlexNet”. “V” means VGG16. “R” means “ResNet”. Similarly\nto Table 5 in the main text, (1, 10k) means clustering-based Ci\nwith H = 1 and m = 10000.\nB. Results Details\nB.1. Transfer Learning Details\nBesides the settings listed in the main paper, there are\nadditional settings for data augmentation during our trans-\nfer learning training to ImageNet and Places 205 datasets.\nIn general, we use random crop and random horizontal\nﬂip as data augmentation techniques during transfer learn-\ning for all architectures on both ImageNet and Places 205\ndatasets, where the speciﬁc random crop implementation\nvaries across networks and datasets. For AlexNet on Im-\nageNet and all architectures on Places 205, we use the\nAlexNet style random crop [26], which is ﬁrst resizing the\nimage so that its smallest side is 256 and then randomly\ncropping a 224 × 224 patch. For VGG16, ResNet-18, and\nResNet-50 on ImageNet, we use the ResNet style random\ncrop [24], which is ﬁrst randomly choosing a patch whose\naspect ratio and area sufﬁce two conditions and then resiz-\ning that path to 224 × 224. The two sufﬁced conditions are:\nits area is at least 20% of the overall area and at most 100%\nof the overall area; its aspect ratio ranges from 3/4 to 4/3.\nWe use the same data augmentation techniques for the same\narchitecture trained with different methods.\nB.2. DeepCluster Results Details\nThe DeepCluster [6] VGG16, ResNet-18, and ResNet-\n50 results are produced by us, where the DC-VGG16 net-\nwork is provided by the authors and the DC-ResNet-18 and\nDC-ResNet-50 networks are trained by us using the pro-\nvided source codes.\nMore speciﬁcally, for ResNet-18, two implementations\nof DC-ResNet-18 network are trained. Both of them modi-\nﬁes the standard ResNet-18 architecture by removing the ﬁ-\nnal pooling and ﬁnal fully connected layer and then adding\nadditional fully connected layers, where the last layer has\n10000 units. One implementation (DC-ResNet-18-A) only\nhas that 10000-unit fully connected layer and the other im-\nplementation (DC-ResNet-18-B) has two more 4096-unit\nfully connected layers before that. We ﬁnd that DC-ResNet-\n18-B performs slightly better than DC-ResNet-18-A and\nthus report the performances of DC-ResNet-18-B in the\nmain paper.\nSimilarly for ResNet-50, two implementations (DC-\nResNet-50-A and DC-ResNet-50-B) are trained. However,\nwe ﬁnd it impossible to train DC-ResNet-50-B as the k-\nmeans clustering results always become trivial at the third\nepoch. So the results reported in the paper are from DC-\nResNet-50-A, which should only be slightly worse than\nDC-ResNet-50-B.\nOther hyper-parameters for network training are mostly\nthe same as used in the provided source codes. Meanwhile,\nall hyper-parameters for transfer learning to ImageNet and\nPlaces 205 are also the same as provided, except the data\naugmentation techniques which are the same as described\nin Section B.1.\nB.3. Places KNN Results\nWe run models on center crops of training images in\nPlaces 205 [72] dataset to generate the memory bank ¯V. We\nthen run the KNN validation similarly to the ImageNet [10]\nKNN procedure, which is described in the main paper. The\nresults are shown in Table 2.\nNetwork\nKNN\nIR with BN - A\n36.9\nLA - A\n37.5\nIR with BN - V\n40.1\nLA - V\n41.9\nIR - R18\n38.6\nLA - R18\n40.3\nIR - R50\n41.6\nLA - R50\n42.4\nTable 2.\nKNN results for Places 205 dataset.\n“A” means\n“AlexNet”. “V” means VGG16. “R” means “ResNet”.\nB.4. Faster RCNN Details\nOur Faster RCNN [55] implementations are based on tf-\nfaster-rcnn. We use SGD with momentum of 0.9, batch size\n256, and weight decay 0.0001. Learning rate is initialized\nas 0.001 and dropped by a factor of 10 after 50000 steps.\nWe train the models for 70000 steps. In particular, we set\nthe number of total RoIs for training the region classiﬁer\nto be 128 to reproduce the original Faster RCNN results,\nas indicated by [?]. For AlexNet, we ﬁne-tune all layers.\nFor VGG16, we ﬁx “conv1” and “conv2” while ﬁne-tuning\nothers. For ResNet-50, we ﬁx the ﬁrst convolution layer\nand the ﬁrst three blocks while ﬁne-tuning others. Other\nhyper-parameters are the same as the default settings in tf-\nfaster-rcnn.\nC. Other Hyperparameters\nThere are several other adjustable hyper-parameters in\nLA training procedure, such as the updating frequency for\nthe clustering results, the parameter k in Nk for Bi, and\nwhether doing clustering on ¯V or network outputs on center\ncrops of I. In this section, we show results of experiments\nillustrating the inﬂuences of these parameters in Table 3.\nSetting\nNN perf.\nBaseline\n35.7\nk = 2048\n35.4\nk = 8192\n35.8\ncenter crop\n35.8\nmore freq\n35.7\nTable 3.\nNearest neighbor validation performances for ResNet-\n18 trained with different settings. “Baseline” uses H = 1, m =\n10000, and k = 4096. Other settings change one of the hyper-\nparameters while keeping the others the same. “center crop” rep-\nresents the experiment with clustering result acquired on the center\ncrops rather than ¯V. “more freq” represents the experiment with\nclustering result updated every 1000 steps.\n",
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "published": "2019-03-29",
  "updated": "2019-04-10"
}