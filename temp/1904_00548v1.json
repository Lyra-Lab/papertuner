{
  "id": "http://arxiv.org/abs/1904.00548v1",
  "title": "Unsupervised Contextual Anomaly Detection using Joint Deep Variational Generative Models",
  "authors": [
    "Yaniv Shulman"
  ],
  "abstract": "A method for unsupervised contextual anomaly detection is proposed using a\ncross-linked pair of Variational Auto-Encoders for assigning a normality score\nto an observation. The method enables a distinct separation of contextual from\nbehavioral attributes and is robust to the presence of anomalous or novel\ncontextual attributes. The method can be trained with data sets that contain\nanomalies without any special pre-processing.",
  "text": "Unsupervised Contextual Anomaly Detection using Joint Deep\nVariational Generative Models\nYaniv Shulman\nyaniv@aleph-zero.info\nAbstract\nA method for unsupervised contextual anomaly detection is proposed using a cross-linked pair\nof Variational Auto-Encoders (VAE) for assigning a normality score to an observation. The\nmethod enables a distinct separation of contextual from behavioral attributes and is robust to the\npresence of anomalous or novel contextual attributes. The method can be trained with data sets\nthat contain anomalies without any special pre-processing.\n1. Introduction\nAnomaly detection is an important area of research since anomalies represent a substantial\ndeviation from the normal characteristics of a system or process of interest. Often these processes\nresult in highly dimensional data sets, with complex relationships within the data and exhibit\nstochastic behavior. Furthermore the anomalies by deﬁnition contain high self-information mea-\nsure and therefore carry useful information about the underlying data generation process. There\nexist a number of similar deﬁnitions of what an anomaly is however in this paper the following\ndeﬁnition is adopted [11]:\n1. Anomalies are diﬀerent from the norm in respect to their attributes.\n2. They are rare in a data set compared to the normal instances.\n3. In addition a novel observation is deﬁned as an observation that is substantially diﬀerent\nthan any observation in the training data set.\nIn this paper a method for contextual anomaly detection is proposed using a cross-linked\npair of Variational Auto-Encoders (VAE) for assigning a normality score to an observation. The\nmethod enables a distinct separation of contextual from behavioral attributes and is robust to\nthe presence of anomalous or novel contextual attributes. The method can be trained with data\nsets that contain anomalies without any special pre-processing. In addition the method can be\nextended in a straight forward way to further decompose and separately model the joint varia-\ntional approximation by introducing additional independent recognition networks thus allowing\nfor more accurate representation in the latent space.\narXiv:1904.00548v1  [stat.ML]  1 Apr 2019\nIn summary the key contributions of this paper are:\n• A novel architecture for auto-encoding joint latent variational Bayes.\n• A novel method for robust unsupervised anomaly detection in the presence of contextual\nanomalies.\n2. Preliminaries\n2.1. Anomaly Detection\nIn this section a number of criteria for broadly categorizing anomaly detection algorithms is\nbrieﬂy discussed. These concepts are covered in more detail in [1, 8, 11].\nProximity based anomaly detection assumes that anomalous data are isolated from the major-\nity of the data whether in relation to clusters or global/local dense regions. To determine if an\nobservation is anomalous, the distance to the clusters or the density estimate is calculated to\ngenerate a normality score; Statistical based anomaly detection assumes that data is generated\nfrom a known probability distribution which can be described by parametric or non-parametric\nformulation. To determine if a data point is an anomaly the probability of it being generated\nfrom the assumed distribution is determined and a normality score is produced derived from this\nprobability; Deviation based anomaly detection is based on the reconstruction errors following\na spectral or other transformation of the data to a lower dimensional space and then back to the\noriginal space. The magnitude of the reconstruction error is used to generate a normality score.\nSupervised anomaly detection is employed where both the training and test data sets specify\nfor each observation whether it is normal or anomalous; Semi-supervised anomaly detection is\ntypically deﬁned as scenarios where the training data contains only normal observations; Unsu-\npervised anomaly detection is the case where there are no labels provided in either the training\nor the testing data sets and no assumptions are made on the existence or number of anomalous\nobservations in the available data.\nContextual anomaly detection is formulated such that the data contains two types of attributes,\nbehavioral and contextual attributes. Behavioral attributes are attributes that relate directly to the\nprocess of interest whereas contextual attributes relate to exogenous but highly aﬀecting factors\nin relation to the process. Generally the behavioral attributes are conditional on the contextual\nattributes.\n2.2. Variational Auto-Encoder\nIn this section a brief overview of the Variational Auto-Encoder (VAE) [14] is provided for\npresenting the notation used in subsequent sections of the paper.\nA Variational Auto-Encoder (VAE) is a directed probabilistic graphical model that enables an\neﬃcient variational inference for intractable posterior distributions which are approximated by\na neural network. The VAE is comprised of two serially adjoined neural networks which are\nreferred to as encoder/recognizer and decoder/generator respectively. The generator network\ng(z, θ) where z is a latent variable approximates the generative process pθ(x) = pθ(x|z)pθ(z).\nThe recognition network f(x, φ) models qφ(z|x) a variational approximation of the intractable\nposterior pθ(z|x). All parameters are learned jointly and eﬃciently by employing the Stochastic\n2\nGradient Variational Bayes (SGVB) [14] estimator. As the marginal likelihood of the data p(x) is\nintractable, the problem is transformed into an optimization problem where the objective function\nof the VAE is the Evidence Lower Bound (ELBO), a lower bound on log p(x) as formulated in\nequation 2c.\nlog pθ({x(i)}N\ni=1) =\nN\nX\nl=1\nlog pθ(x(i))\n(1a)\n= KL(qφ(z|x) ∥pθ(z)) + L(θ, φ, x)\n(1b)\nlog pθ(x) ≥L(θ, φ, x)\n(2a)\n= Eqφ(z|x)[−log qφ(z|x) + log pθ(x|z)]\n(2b)\n= −KL(qφ(z|x) ∥pθ(z)) + Eqφ(z|x)[log pθ(x|z)]\n(2c)\nWhere the inequality in equation 2a follows from the non-negativity of the KullbackLeibler\ndivergence. A complete derivation can be found in [5].\n2.3. Conditional Variational Auto-Encoder\nIn this section a very brief overview of the Conditional Variational Auto-Encoder (CVAE)\n[17]. The CVAE expands on the learning capacity of the VAE by deﬁning an architecture that\nenables the model to learn explicit joint variational approximation of the latent variable qφ(z|x, y)\nand a directly modulated conditional generative pθ(y|x, z) model. In CVAE the input is denoted\nas x, the output is denoted as y and the latent variable is z. The CVAE utilizes the SGVB\noptimization framework and an objective function closely related to the VAE deﬁned in equation\n3e.\nlog pθ(y|x) =\n(3a)\nKL(qφ(z|x, y) ∥pθ(z|x, y)) + Eqφ(z|x,y)[−log qφ(z|x, y) + log pθ(y, z|x)]\n(3b)\n≥Eqφ(z|x,y)[−log qφ(z|x, y) + log pθ(y, z|x)]\n(3c)\n= Eqφ(z|x,y)[−log qφ(z|x, y) + log pθ(z|x)] + Eqφ(z|x,y)[log pθ(y|x, z)]\n(3d)\n= −KL(qφ(z|x, y) ∥pθ(z|x)) + Eqφ(z|x,y)[log pθ(y|x, z)]\n(3e)\n2.4. Related Work\nAnomaly detection has attracted large interest from the research community over decades\ndue to the varied areas of application and theoretical importance. There are many suggested\nmethods for the general case however a much smaller number of methods that deal explicitly\nwith contextual anomaly detection exist. A review of related work is given in [8] and in [11], the\nlatter being more recent and also endeavors to provide an elaborate comparative evaluation for\na large number of methods. In this section the focus is on more recent methods proposed either\nfor contextual anomaly detection or anomaly detection that make use of variational inference\nand deep learning methods. Note that both supervised, semi-supervised and unsupervised meth-\nods are included. [15] proposes a contextual anomaly detection method (ROCOD) for dealing\n3\nwith situations where there are abnormal or sparse contextual attributes by utilizing local and\nglobal behavioral models conditional on the context. [15] also performs comparative analysis\nof a number of methods and demonstrates that state-of-the-art point methods achieve relatively\npoor results on contextual anomaly detection problems. [19] has proposed a method for general\ncontextual anomaly detection and proposes three diﬀerent expectation-maximization algorithms\nfor learning the model. Additionally [19] comparatively evaluates more than 13 diﬀerent data\nsets against several other non-contextual anomaly detection methods. [12] propose a multivariate\nconditional outlier detection framework for clinical applications by deﬁning a multi-variate func-\ntion to calculate the normality score. [3] propose a method for improved unsupervised learning\nof L2 constrained representations for clustering analysis using deep Auto-Encoders. Normality\nscores are then calculated based on similarity measure to clusters. Note that in [3] the number\nof clusters is assumed to be known. [18] apply a Stochastic Recurrent Network (STORN) [4] for\nsupervised detection of anomalies in robot sensors time series data. [2] suggests an anomaly de-\ntection method using a VAE and proposes the Reconstruction Probability a novel normality score\nbased on the probabilistic measure expressed in the objective function of the VAE. [20] suggest\nDonot, an unsupervised anomaly detection algorithm utilizing a Variational Auto-Encoder for\nanomaly detection in Seasonal KPI arising from web applications utilizing the Reconstruction\nProbability.\n3. Problem Description\n3.1. Unsupervised Contextual Anomaly Detection\nMost anomaly detection methods known to the author at this time do not provide explicit\ntreatment of contextual and behavioral attributes separately but simply merge the two attribute\ntypes into a single observation thus transforming the original task into a standard point anomaly\ndetection [11, 8]. On the other hand some contextual anomaly detection methods either require\na labeled data set for training or are designed for speciﬁc domains therefore it seems not many\nmethods exist to perform general unsupervised contextual anomaly detection. Furthermore by\ndeﬁnition relatively little information is available on the distribution of the behavioral attributes\nin low density areas of the contextual subspace which results in an additional challenge for the\nexisting algorithms especially when there is no information available on the distribution of the\nbehavioral attributes when the context is in itself novel.\nIn this paper the focus is on unsupervised contextual anomaly detection where the training and\ntesting data sets are generated by the same process. It is of interest to develop a robust model\nthat is able to learn eﬃciently the state of the process and correctly predict an observation as an\nanomaly when the behavioral attributes are in fact an anomaly given the context. However it is\ndesirable for such a model to be robust to anomalies present in the contextual attributes and use\nthe best available relevant context to make meaningful predictions.\n4\nFigure 1: Illustration of the generative model as a directed graphical model. x is the behavioral attributes for the process\nof interest, c is the contextual attributes which in this case do not participate directly in the generative process pθ(x) =\nRR\npθ(x|zx, zc)pθ(zx)pθ(zc)dzxdzc. Solid lines denote the generative process whereas dashed line denote the variational\napproximations.\n4. Proposed Method\n4.1. The Data Generation Model\nGiven a data set of observations D = {d(i) = [c(i), x(i)] | c(i) ∈C, x(i) ∈X}N\ni=1 where [◦, ◦]\ndenotes concatenation, the set X = {x(i)}N\ni=1 contains only behavioral attributes and the set C =\n{c(i)}N\ni=1 contains only the corresponding contextual attributes and [x(i), c(i)] are jointly and inde-\npendently drawn. The data generation process where the N samples are taken can be modeled as\nfollows:\n1. A sample z(i)\nx is taken from a latent variable zx with prior distribution pθ(zx).\n2. A sample z(i)\nc is taken from a latent variable zc with prior distribution pθ(zc).\n3. A sample c(i) is taken from a variable c with conditional distribution pθ(c|zc).\n4. A sample x(i) is taken from a variable x with conditional distribution pθ(x|zx, zc).\nThe generative process is deﬁned as pθ(x) =\nRR\npθ(x|zx, zc)pθ(zx)pθ(zc)dzxdzc,\npθ(c) =\nR\npθ(c|zc)pθ(zc)dzc and is chosen so to prevent c from modulating the generative process\nof x directly for reasons brought in subsequent sections. Figure 4.1 provides an overview of the\ngenerative process. pθ(x) and pθ(c) are often intractable.\n4.2. Joint Deep Variational Generative Models\n4.2.1. The Variational Bound\nLet z = [zx, zc] denote the complete set of latent variables. The variational lower bound of\npθ(x) and pθ(c) is deﬁned as follows:\nlog pθ(c) ≥−KL(qφ(zc|x, c) ∥pθ(zc)) + Eqφ(zc|x,c)[log pθ(c|zc)]\n(4)\nlog pθ(x) ≥−KL(qφ(z|x) ∥pθ(z)) + Eqφ(z|x)[log pθ(x|z)]\n(5)\nTo optimize jointly the variational lower bound objective of the two marginal likelihoods\nequations 4 and 5 are combined.\n5\nlog pθ(c) + log pθ(x) ≥\n−KL(qφ(zc|x, c) ∥pθ(zc)) + Eqφ(zc|x,c)[log pθ(c|zc)]\n−KL(qφ(z|x) ∥pθ(z)) + Eqφ(z|x)[log pθ(x|z)]\n(6)\nGiven the KL terms in equation 6 may be integrated analytically under certain conditions for\ncalculating the empirical loss, the objective is optimized using the Stochastic Gradient Variational\nBayes (SGVB) [14] estimator:\nL(θ, φ, c(i), x(i)) =\n−KL(qφ(z(i)\nc |x(i), c(i)) ∥pθ(z(i)\nc )) −KL(qφ(z(i)|x(i)) ∥pθ(z(i)))\n+ 1\nL\nL\nX\nl=1\nlog pθ(c(i)|z(i,l)\nc ) + 1\nL\nL\nX\nl=1\nlog pθ(x(i)|z(i,l))\n(7)\nWhere z(i,l)\nc\n= gφ(x(i), c(i), ε(i,l)\nc ), εc ∼N(0, I) and z(i,l) = hφ(x(i), c(i), ε(i,l)), ε ∼N(0, I), L is the\nnumber of samples. The ﬁrst two KL terms in equation 7 represent the latent error for the two\nvariational distributions qφ(zc|x, c) and qφ(z|x), and the two remaining terms the log probability of\nthe reconstruction errors for the contextual and behavioral attributes C = {c(i)}N\ni=1 and X = {x(i)}N\ni=1\nrespectively.\n4.2.2. Architecture\nTo approximate the posteriors of the joint generative models pθ(c|zc) and pθ(x|zx, zc) two\nrecognition networks and two generator networks are jointly trained. The behavioral attributes x\nare input into one of the recognition networks and both the contextual and behavioral attributes\n[x, c] are input into the other. Both recognition networks output the parameters of the variational\napproximations to the prior followed by L samples that are drawn from the variational approx-\nimations to form a Monte Carlo approximation of the expectations of the reconstruction with\nrespect to variational approximations [14]. This architecture provides a number of beneﬁts:\n1. Explicit treatment of behavioral and contextual attributes.\n2. Enables an indirect modulation of the generative process of pθ(x|z) by c based on the\nlatent representation of the contextual attributes rather than a direct modulation of the pro-\ncess as done in a CVAE architecture which results in increased robustness to the presence\nof outliers and novelties in the contextual space. Intuitively this can be explained by the\nsimilarity of the latent representation of c to spectral dimensionality reduction representa-\ntion which maps the data into a known sub-space, but with the increased model capacity\nof the recognition network and the beneﬁt of a probabilistic interpretation.\n3. Enables assigning diﬀerent priors for the contextual and behavioral spaces, having multiple\nof each as a method to decompose and separately model the joint latent distribution.\n4.2.3. Training and Classiﬁcation\nAll recognition and generator networks are jointly trained using the Stochastic Gradient Vari-\national Bayes (SGVB) [14] estimator. Having learned the model parameters a normality score\ncan be obtained by either calculating a reconstruction error norm for the behavioral attributes\n6\nFigure 2: Illustration of the architecture\n||x(i) −ˆx(i)|| or by calculating the Reconstruction Probability of x(i) deﬁned as Eqxφ(z|x)log pθ(x|z)\n[2]. Note that the reconstructed context ˆc(i) is not strictly required for assigning a normality score\nfor classiﬁcation but can be used to estimate the normality score of the context if desired. Figure\n4.2.3 provides an overview of the architecture.\n5. Experimental Results\n5.1. Kddcup99\n5.1.1. Data\nComparative evaluation of contextual anomaly detection methods is a challenging task due\nto lack of availability of common and suitable data sets that are both labeled and partitioned into\nbehavioral and contextual attributes. To overcome this challenge a publicly available data set the\nKddcup991 was adopted as well as an evaluation method used in [15] to provide a performance\nbaseline. The Kddcup99 is ”the data set used for The Third International Knowledge Discovery\nand Data Mining Tools Competition, which was held in conjunction with KDD-99 The Fifth\nInternational Conference on Knowledge Discovery and Data Mining. The competition task was\n1http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n7\nto build a network intrusion detector, a predictive model capable of distinguishing between “bad”\nconnections, called intrusions or attacks, and “good” normal connections. This database contains\na standard set of data to be audited, which includes a wide variety of intrusions simulated in a\nmilitary network environment”. The Kddcup99 data set is by a large margin the most challenging\ndata set evaluated by [15] and therefore was elected for this experiment. An eﬀort was made to\nadhere to the same method of pre-processing and data inclusion as described in [15] however\nthere are some diﬀerences as described subsequently.\nThe observations from the r2l and u2r attack families were retained as well as attacks of type\nipsweep and nmap, and normal observations. This results in a total of 605,803 observations out\nof which 595,797 are labeled as normal, and the rest 10,006 are considered anomalies (approx.\n1.652%). Similarly to [15] the service, duration, src bytes and dst bytes were used as behavioral\nattributes and all other as contextual attributes. The logarithm of duration, src bytes and dst bytes\nwas taken since these attributes are processed in the same manner in [15]. All categorical features\nwere one-hot-encoded and ﬁnally all attributes are normalized to [0, 1] range. The resulting data\nset contains 65 behavioral attributes and 45 contextual attributes and enables quantitative analysis\nof the proposed algorithm’s eﬀectiveness against the algorithms evaluated in [15] on a similar\ndata set.\n5.1.2. Model\nThe model is comprised of behavioral recognizer and generator networks and contextual\nrecognizer and generator networks as in the basic architecture described in section 4.2.2 and\nillustrated in ﬁgure 4.2.3. The arrangement of units in the behavioral recognizer MLP were: 65\n(input), 58, 32 and 4 units for the latent output, with the generator having a mirror architecture.\nThe arrangement of units in the contextual recognizer MLP were: 110 (input), 40, 22 and 4 units\nfor the latent output, with the generator having a mirror architecture except for the output layer\ncontaining 45 units. All activation functions in the MLPs are Relu where applicable, however\nthe latent parameters layer as well as the outputs of both generators employ linear activation.\nIsotropic normal distribution were assumed to the data and latent distributions which lead to the\ntotal empirical objective is presented in equation 8, note there is an added L1 regularization term\nover the MLPs’ weights with λ = 10−5.\nL(θ, φ, c(i), x(i)) =\n−1\n2\n\u0014 |z|\nX\ni=1\n(1 + log((σ(i)\nz )2) −(µ(i)\nz )2 −(σ(i)\nz )2) +\n|zc|\nX\ni=1\n(1 + log((σ(i)\nzc )2) −(µ(i)\nzc )2 −(σ(i)\nzc )2)\n\u0015\n+ 1\nL\nL\nX\nl=1\n||x(i,l) −ˆx(i,l)||2 + 1\nL\nL\nX\nl=1\n||c(i,l) −ˆc(i,l)||2 + λ\nX\nw\n|w| =\n−1\n2\n|zx|\nX\ni=1\n(1 + log((σ(i)\nzx )2) −(µ(i)\nzx )2 −(σ(i)\nzx )2) −\n|zc|\nX\ni=1\n(1 + log((σ(i)\nzc )2) −(µ(i)\nzc )2 −(σ(i)\nzc )2)]\n+ 1\nL\nL\nX\nl=1\n||x(i,l) −ˆx(i,l)||2 + 1\nL\nL\nX\nl=1\n||c(i,l) −ˆc(i,l)||2 + λ\nX\nw\n|w|\n(8)\nWhere L = 1 since a mini-batch size of 200 was used, |z|, |zc| and |zx| are the dimensions of\nthe latent variables z, zc and zx respectively. For optimization Adam [13] was employed. Note\nthat the aforementioned architecture is likely not optimal and was chosen based on previous\n8\npersonal experience for illustrative purposes with no attempt to ﬁnd an optimal hyper-parameter\nsetting for this experiment. Training was performed with early stop strategy once the loss on the\nvalidation set has started increasing.\n5.1.3. Additional Baselines\nDespite aiming to compare primarily against the results presented in [15] for diligence the\nsame data set was evaluated by three additional algorithms: Isolation Forest [16], One Class\nSVM [9] and Local Outlier Factor [7]. Not much eﬀort was put into ﬁne tuning these algorithms\non the target data set and the results should be taken as indicative only.\n5.1.4. Metrics\nThe following metrics were evaluated against each of the methods:\n1. Area under the Precision-Recall Curve (PRC): The area under the curve when plotting the\nrecall on the x-axis against precision on the y-axis for all relevant possible threshold values\nfor discriminating between normal and anomalous observations. PRC is recommended in\nscenarios where the data set is highly imbalanced [10]. The area under the curve (AUC)\nprovides a summary statistic to the performance of a classiﬁer in the PRC space.\n2. Average Precision Score (APS): Provides a summary statistic for the Precision-Recall\nCurve as a weighted mean of precision obtained at each threshold, with the weight being\nthe increase in recall from the previous threshold, calculated as APS = P\nn(Rn −Rn−1)Pn\nwhere Pn and Rn are the precision and recall at the n −th threshold.\n3. Area under the Receiver Operating Characteristics Curve (ROC): The ROC curve en-\nables the visualization of the relative trade-oﬀbetween true-positive rate (TPR) and false-\npositive rate (FPR) by plotting the FPR on the x-axis against TPR on the y-axis for all\nrelevant threshold values. The area under the curve (AUC) provides a summary statistic to\nthe performance of a classiﬁer in the ROC space.\n4. Top-100 Precision: The fraction of correctly detected anomalies in the top 100 scored\nobservations.\n5.1.5. Performance Metrics for Standard Classiﬁcation\nDue to the challenges related to binary classiﬁcation over a highly imbalanced data sets [6]\na cross-validation with 5-fold stratiﬁed partitioning was performed where the ratio of the two\nclasses in each of the the train/test partitions was kept equal to the distribution in the complete\ndata set. The results are summarized in the following tables:\nMethod\nPRC (AUC)\nAPS\nROC (AUC)\nTop-100 Precis.\nJLVAE\n0.51848\n0.51874\n0.99257\n0.018\nIF [16]\n0.00842\n0.00855\n0.01937\n0\nOCSVM [9]\n0.00846\n0.00853\n0.02459\n0\nLOF [7]\n0.02458\n0.03579\n0.64849\n0.056\nTable 1: Summary of mean results obtained over the 5-folds for all methods.\nThe results obtained demonstrate a substantial improvement compared to the benchmark\nalgorithms tested in the described setting and to the results obtained by [15] for a similar data\n9\nset. The following tables contain detailed information as to the results obtained for each of the\nalgorithms and k-folds.\nK-Fold\nPRC (AUC)\nAPS\nROC (AUC)\nTop-100 Precision\n1\n0.50543\n0.5057\n0.99240\n0.01\n2\n0.53492\n0.53524\n0.99321\n0.03\n3\n0.51293\n0.5131\n0.99227\n0.01\n4\n0.53134\n0.53162\n0.99264\n0.03\n5\n0.50777\n0.50805\n0.99233\n0.01\nmean\n0.51848\n0.51874\n0.99257\n0.018\nTable 2: JLVAE - proposed method.\nK-Fold\nPRC (AUC)\nAPS\nROC (AUC)\nTop-100 Precision\n1\n0.0084\n0.00854\n0.01773\n0\n2\n0.0084\n0.0085\n0.01203\n0\n3\n0.00843\n0.00859\n0.02282\n0\n4\n0.00848\n0.0086\n0.02942\n0\n5\n0.00838\n0.00853\n0.01486\n0\nmean\n0.00842\n0.00855\n0.01937\n0\nTable 3: Isolation Forest.\nK-Fold\nPRC (AUC)\nAPS\nROC (AUC)\nTop-100 Precision\n1\n0.00847\n0.00854\n0.02476\n0\n2\n0.00847\n0.00853\n0.02475\n0\n3\n0.00846\n0.00853\n0.02462\n0\n4\n0.00846\n0.00853\n0.02469\n0\n5\n0.00846\n0.00852\n0.02414\n0\nmean\n0.00846\n0.00853\n0.02459\n0\nTable 4: One Class SVM.\nK-Fold\nPRC (AUC)\nAPS\nROC (AUC)\nTop-100 Precision\n1\n0.02442\n0.03593\n0.64976\n0.01\n2\n0.02464\n0.03627\n0.64744\n0.09\n3\n0.02394\n0.0353\n0.64671\n0.03\n4\n0.02483\n0.0351\n0.64564\n0.08\n5\n0.02506\n0.03633\n0.65290\n0.07\nmean\n0.02458\n0.03579\n0.64849\n0.056\nTable 5: Local Outlier Factor.\n10\n5.2. Waste Water Treatment Plant\n5.2.1. Robustness to Contextual Anomalies\nTo demonstrate the eﬀectiveness of the method in dealing with contextual anomalies it was\nevaluated on a real-world waste water treatment plant located at Western Australia. The plant\ndesign features a splitter chamber that divides the incoming waste water into two wells each\nhaving two pumps. Waste water pumped by the pumps are then merged into a single outlet pipe\nby a series of two joiner pipes, one joining the pumps output in each well, and one joining the\ntwo well’s output. The control logic for the plant under normal conditions will turn pumps on\nand oﬀas required to meet inﬂow conditions and also use variable speed drives to modulate the\nspeed of the operational pumps based on a level reading of the splitter chamber. This design\nresults in a system where the operational characteristics of a pump is not independent from the\nother pumps.\n5.2.2. Data\nThe data set contains roughly 30 months of operational data, close to 150 attributes and about\n690,200 coincident observations with 2 minutes frequency and is comprised of the following\ninformation:\n1. Sensors speciﬁc per pump such as vibration, temperature, speed, operational pressures and\nﬂows, power supply characteristics, and more.\n2. Generated features per pump such as eﬃciency.\n3. Environmental readings from the two wells.\n4. Other useful data such as the splitter chamber level and external weather conditions.\nThe data is assumed to contain anomalies of unknown nature and frequency. The data was\npartitioned such that data generated in a particular pump run-cycle was kept together and not\npartitioned across sets. Partitioning was done into training (65%), validation(15%) and testing\n(%20) sets where the percentages represent the portion of pump run-cycles rather than single\nobservations. Lastly the data was not pre-processed except for aligning observations in time by\nmean interpolation, discarding partial observations with the remaining observations standardized.\nNote that there are no categorical attributes in this data set.\n5.2.3. Model\nA model is developed for each pump individually where the behavioral attribute are the data\nrelating directly to the operational sensor readings of the pump, and where contextual attributes\nare some of the behavioral attributes of the remaining pumps as well as environmental factors\nsuch as the splitter chamber level and weather conditions. For example, a model for pump one\nwill include as context the inﬂow and outﬂow rate and pressure of pumps 2-4, the splitter cham-\nber level and environmental information. The setup was similar to the one described in section\n5.1.2 with arrangement of units in the behavioral recognizer as follows: 28 (input), 20, 10 and 5\nunits for the latent output, with the generator having a mirror architecture. The arrangement of\nunits in the contextual recognizer were: 38 (input), 20, 10 and 2 units for the latent output, with\nthe generator having 4, 7 and 10 units in the output layer. Note that similarly to the previous ex-\nperiment the aforementioned architecture is likely not optimal and was chosen based on personal\nexperience of the author for illustrative purposes.\n11\n5.2.4. Metrics\nIn this case it is intended to evaluate the models robustness to contextual anomalies and\nnovelties. To do so the following method is applied. A threshold was set so that the number\nof anomalies detected by the model in the test data set is roughly 1%. Then 10,000 normal\nobservation are randomly selected and transformed by scaling and oﬀsetting a randomly chosen\nsubset of the attributes element-wise where scale ∼U(−2.5, 2.5) and of f set ∼U(−2.0, 2.0)\nresulting in 15 new test data sets. For the A data sets approximately 10% of each group was\ntransformed, for the B data sets about 30% and for the C data sets about 50%. For the D,E and\nF data sets the same absolute number of attributes was transformed in each of the groups. Given\nthe attributes are standardized to zero mean and unit standard deviation the noise levels applied\nto the attributes are substantial and result in many anomalies detected as per the summary in table\n6:\nData set\n# behavior trans.\n# context trans.\n# anomalies reported\nA1\n3\n0\n1240\nA2\n0\n1\n7\nA3\n3\n1\n576\nB1\n9\n0\n4614\nB2\n0\n3\n2\nB3\n9\n3\n4251\nC1\n14\n0\n6949\nC2\n0\n5\n10\nC3\n14\n5\n5909\nDx\n2\n0\n288\nDc\n0\n2\n0\nEx\n5\n0\n1567\nEc\n0\n5\n13\nFx\n10\n0\n4485\nFc\n0\n10\n17\nTable 6: Summary of number of anomalies detected in the noisy data sets.\nThe results demonstrate the algorithm is robust to anomalies and novelties in the contextual\ndata attributes whilst maintaining sensitivity to anomalies in the behavioral space. It is notable\nthat even when the entire set of contextual attributes is transformed in data set Fc, still less\nanomalies are reported than data set Dx where only two behavioral attributes are corrupted with\nnoise.\n6. Conclusion\nIn this paper a novel algorithm for contextual anomaly detection is presented and a novel\nANN architecture comprised of multiple cross-linked VAEs to model directed graphical distribu-\ntion models for modeling generative processes. The algorithm performs well in the test scenarios\nand is robust to contextual anomalies and novelties.\n12\n7. Acknowledgements\nThis research was supported by the Water Corporation of Western Australia. I gratefully\nacknowledge my colleagues from the Water Corporation for access to infrastructure and for their\ncooperation, which greatly assisted the research.\nReferences\n[1] C. C. Aggarwal. Outlier Analysis. Springer Publishing Company, Incorporated, 2013.\n[2] J. An and S. Cho. Variational autoencoder based anomaly detection using reconstruction probability. 2015.\n[3] C¸ . Aytekin, X. Ni, F. Cricri, and E. Aksu. Clustering and unsupervised anomaly detection with L2 normalized deep\nauto-encoder representations. CoRR, abs/1802.00187, 2018.\n[4] J. Bayer and C. Osendorfer. Learning stochastic recurrent networks. 2015.\n[5] D. M. Blei, A. Kucukelbir, and J. D. McAuliﬀe. Variational inference: A review for statisticians. Journal of the\nAmerican Statistical Association, 112(518):859–877, 2017.\n[6] P. Branco, L. Torgo, and R. P. Ribeiro. A survey of predictive modelling under imbalanced distributions. CoRR,\nabs/1505.01658, 2015.\n[7] M. M. Breunig, H.-P. Kriegel, R. T. Ng, and J. Sander. Lof: Identifying density-based local outliers. SIGMOD\nRec., 29(2):93–104, May 2000.\n[8] V. Chandola, A. Banerjee, and V. Kumar. Anomaly detection : A survey. ACM Computing Surveys, 09:1–72, 2009.\n[9] C. Cortes and V. Vapnik. Support-vector networks. Machine Learning, 20(3):273–297, Sep 1995.\n[10] J. Davis and M. Goadrich. The relationship between precision-recall and roc curves. In Proceedings of the 23rd\nInternational Conference on Machine Learning, ICML ’06, pages 233–240, New York, NY, USA, 2006. ACM.\n[11] M. Goldstein and S. Uchida. A comparative evaluation of unsupervised anomaly detection algorithms for multi-\nvariate data. PLoS ONE, 11(4):1–31, 04 2016.\n[12] C. Hong and M. Hauskrecht. Multivariate conditional outlier detection and its clinical application. In Proceedings\nof the Thirtieth AAAI Conference on Artiﬁcial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA., pages\n4216–4217, 2016.\n[13] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014.\n[14] D. P. Kingma and M. Welling. Auto-encoding variational bayes. 2014.\n[15] J. Liang and S. Parthasarathy.\nRobust contextual outlier detection: Where context meets sparsity.\nCoRR,\nabs/1607.08329, 2016.\n[16] F. T. Liu, K. M. Ting, and Z.-H. Zhou. Isolation-based anomaly detection. ACM Trans. Knowl. Discov. Data,\n6(1):3:1–3:39, Mar. 2012.\n[17] K. Sohn, H. Lee, and X. Yan. Learning structured output representation using deep conditional generative models.\nIn C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information\nProcessing Systems 28, pages 3483–3491. Curran Associates, Inc., 2015.\n[18] M. S¨olch, J. Bayer, M. Ludersdorfer, and P. van der Smagt. Variational inference for on-line anomaly detection in\nhigh-dimensional time series. CoRR, abs/1602.07109, 2016.\n[19] X. Song, M. Wu, C. Jermaine, and S. Ranka. Conditional anomaly detection. IEEE Transactions on Knowledge\nand Data Engineering, 19, 2007.\n[20] H. Xu, W. Chen, N. Zhao, Z. Li, J. Bu, Z. Li, Y. Liu, Y. Zhao, D. Pei, Y. Feng, J. Chen, Z. Wang, and H. Qiao.\nUnsupervised anomaly detection via variational auto-encoder for seasonal kpis in web applications. In Proceedings\nof the 2018 World Wide Web Conference, WWW ’18, pages 187–196, Republic and Canton of Geneva, Switzerland,\n2018. International World Wide Web Conferences Steering Committee.\n13\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2019-04-01",
  "updated": "2019-04-01"
}