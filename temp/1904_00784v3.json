{
  "id": "http://arxiv.org/abs/1904.00784v3",
  "title": "A Survey of Code-switched Speech and Language Processing",
  "authors": [
    "Sunayana Sitaram",
    "Khyathi Raghavi Chandu",
    "Sai Krishna Rallabandi",
    "Alan W Black"
  ],
  "abstract": "Code-switching, the alternation of languages within a conversation or\nutterance, is a common communicative phenomenon that occurs in multilingual\ncommunities across the world. This survey reviews computational approaches for\ncode-switched Speech and Natural Language Processing. We motivate why\nprocessing code-switched text and speech is essential for building intelligent\nagents and systems that interact with users in multilingual communities. As\ncode-switching data and resources are scarce, we list what is available in\nvarious code-switched language pairs with the language processing tasks they\ncan be used for. We review code-switching research in various Speech and NLP\napplications, including language processing tools and end-to-end systems. We\nconclude with future directions and open problems in the field.",
  "text": "arXiv:1904.00784v3  [cs.CL]  22 Jul 2020\nA Survey of Code-switched Speech and Language\nProcessing\nSunayana Sitaram\nMicrosoft Research India\nKhyathi Raghavi Chandu, Sai Krishna Rallabandi, Alan W Black\nCarnegie Mellon University\nAbstract\nCode-switching, the alternation of languages within a conversation or utterance,\nis a common communicative phenomenon that occurs in multilingual communi-\nties across the world. This survey reviews computational approaches for code-\nswitched Speech and Natural Language Processing. We motivate why processing\ncode-switched text and speech is essential for building intelligent agents and sys-\ntems that interact with users in multilingual communities. As code-switching\ndata and resources are scarce, we present a comprehensive list of datasets avail-\nable in various code-switched language pairs with the language processing tasks\nthey can be used for. We discuss shared tasks and benchmarks that have been\nproposed to evaluate language processing systems on code-switched text and\nspeech. We review code-switching research in various Speech and NLP applica-\ntions, including language processing tools and end-to-end systems. We discuss\nthe evaluation of code-switched speech and NLP systems, including recently\nproposed benchmarks. We conclude with future directions and open problems\nin the ﬁeld.\nKeywords:\ncode-switching, multilingualism, speech processing, Natural\nLanguage Processing, survey\nPreprint submitted to Computer Speech and Language (CSL)\nJuly 24, 2020\n1. Introduction\nLinguistic code choice refers to the use of a language for a speciﬁc com-\nmunicative purpose and code-switching denotes a shift from one language to\nanother within a single utterance. Not only is there a plethora of diﬀerent lan-\nguages across the world, but speakers also often mix these languages within the\nsame utterance. In fact, some form of code-switching is expected to occur in\nalmost every scenario that involves multilinguals [1]. This can go beyond mere\ninsertion of borrowed words, ﬁllers and phrases, and include morphological and\ngrammatical mixing. Such shifts not only convey group identity [2], embody\nsocietal patterning [3] and signal cultural discourse strategies [4] but also have\nbeen shown to reduce the social and interpersonal distance [5] in both formal\n[6, 7] and informal settings.\nIn this paper we refer to this phenomenon as code-switching, though the\nterm code-mixing is also used.\nWhile such switching is typically considered\ninformal - and is more likely to be found in speech and in casual text as now\nfound in social media - it is also found in semi formal and formal settings such\nas news paper headlines and teaching. Therefore, we argue that code-switching\nshould not be looked down upon or ignored but be acknowledged as a genuine\nform of communication that deserves analysis and development of tools and\ntechniques to be handled appropriately. As language technologies improve and\npermeate more and more applications that involve interactions with humans\n[8, 9], it is imperative that they take phenomena such as code-switching into\naccount for any consumer facing technology.\nCode-switching is most common among peers who have similar ﬂuency in\neach language. For example ﬂuent bilingual Spanish and English people may of-\nten ﬂoat between their languages, in a form of communication called Spanglish.\nIndian sub-continent residents, who often have a substantial ﬂuency in English\nwill often mix their speech with their regional languages in Hinglish (Hindi),\nTenglish (Telugu), Tamlish (Tamil) and others. But it is not just English that\ncode-switching occurs with. Southern Mainland Chinese residents who, for ex-\n2\nample, speak Cantonese and Shanghaiese, may switch with Putonghua (stan-\ndard Mandarin). Arabic Dialects are often mixed with Modern Standard Arabic.\nThe distinction between languages and dialects is hard to deﬁne, but we see that\ncode-switching appears with dialects too. African American Vernacular English\n(AAVE) speakers will commonly switch between AAVE and Standard American\nEnglish; Scottish people may switch between Scots and Standard English. At\nan extreme, code-switching could also be used to describe register shifting in\nmonolingual speech. Formal speech versus slang or swearing may follow similar\nfunctions and patterns as those in code-switching among two distinct languages.\n1.1. Why should we care about code-switched language processing?\nIt is important to realize that humans are good at constructing language\nregisters and learning new communication methods. Not only are we good at\ndoing this with human-human communication, we also construct and learn to\nuse such registers for human-machine communication eﬃciently, such as Linux\ncommand-line expressions, or the grammar of Alexa interactions. If we want\nmachines to partake in such human-human conversations, we need to also be\nable to understand what is being said in these varied registers.\nFor the large companies, understanding code-switched communication will\nenable better advertisement-targeting. Understanding genuine user sentiment\nabout aspects of products helps improve future versions. [10] found a correla-\ntion between language use and sentiment, showing that ignoring one language\nin favor of the other, or ignoring code-switched languages altogether may lead\nto the wrong conclusions about user sentiment. For healthcare, understanding\nhow people feel, if they are being open, will help to give better care, and enable\nbetter communication with patients, and better distribution and uptake of pre-\nventative care. For educators, communication in the right register for tutoring,\nor understanding if concepts are or are not understood is crucial. For entertain-\nment, non-playing characters should communicate in the appropriate register for\nthe game, and/or be able to understand natural code-switched communication\nwith other players.\n3\nUnlike pidgins or creoles [11, 12, 13], where speakers may not have full\nﬂuency in the language of inﬂuence, we are primarily interested in situations\nwhere participants have ﬂuency in each of the languages but are choosing not to\nstay within one language. Code-switching is not a simple linguistic phenomena\nand depending on the languages involved, and the type of code-switching the\ninteraction between the component languages may be quite diﬀerent. It is easy\nto identify at least linguistic sharing, cross-lingual transfer, lexical borrowing\nas well as speech errors with restarts commonly within the code-switched data.\nLikewise although there may be language technology tasks that can be achieved\nwith straightforward techniques, it is clear that some tasks, such as semantic\nrole labeling will require complex cross-lingual analysis.\nMany have identiﬁed the notion of a matrix language in code-switching [14],\nthat there is an underlying language choice which mostly deﬁnes the grammar\nand morphological aspects of the utterance. From a language technologies point\nof view, especially when considering code-switched data generation using any\nform of language modeling, it is possible to identify ‘bad’ code-switching or even\n‘wrong’ code-switching. Although it is obviously not a binary decision, there\nare extremes that will almost always be wrong. We cannot in general randomly\nchoose which language a word would be realized in, or simply state that we will\nchoose alternate languages for each word. That is, there are constraints, there\nis an underlying grammar and there are multiple linguistic theories that have\nbeen proposed for code-switching. Modeling the grammar is challenging, even if\nthere may be an eventual standardized Hinglish that everyone in Northern India\nmay speak, at present, such code-switched languages are very dynamic, and will\nhave very diverse ideolects across speakers. This is reminiscent of pidgins and\ncreoles which can develop over time, but they too, especially as they are not\nnormally written languages, are also diverse.\nBut we should not give up, there is underlying structure, and there are\nconstraints, and we have good machine learning modeling techniques that can\ndeal with uncertainty. Recently, there has been quite a lot of interest in the\nspeech and NLP community on processing code-switched speech and text, and\n4\nthis paper aims at describing progress made in the ﬁeld, and discussing open\nproblems.\nThis survey is organized as follows. First, we introduce why code-switching\nis a challenging and important problem for speech and NLP. Next, in Section\n2, we brieﬂy describe linguistic studies on code-switching with other theoreti-\ncal aspects. In Section 3 we describe speech and NLP corpora and resources\nthat have been created for code-switched language pairs. Section 4 describes\ntechniques for building models for code-switching in speciﬁc speech and NLP\napplications. Section 5 describes various shared tasks and challenges that have\nbeen conducted to evaluate code-switching, and introduces benchmarks that\nevaluate models across tasks and languages. We conclude in Section 6 with a\ndescription of the challenges that remain to be addressed and future directions.\n2. Background\nResearch on code-switching is not recent, and this phenomenon has been\nstudied by linguists for decades. In this section, we provide a description of\nlinguistic studies on code-switching and how to characterize code-switched lan-\nguages. We do not attempt to be comprehensive, since providing a complete\ndescription of code-switching research in linguistics is out of the scope of this\npaper.\n2.1. Types of code-switching\nCode-switching is deﬁned as the ‘juxtaposition within the same speech ex-\nchange of passages of speech belonging to two diﬀerent grammatical systems or\nsub-systems’[15], while code-mixing is ‘the embedding of linguistic units such\nas phrases, words and morphemes of one language into an utterance of another\nlanguage’[16]. The distinction between code-switching, mixing and lexical bor-\nrowing is often not clear and can be thought of as lying on a continuum [17].\nIn this paper, we use the terms ‘code-switching’ and ‘code-mixing’ interchange-\nably, although the distinction between the two may be important for certain\napplications.\n5\nThe extent and type of code-switching can vary across language pairs. [18]\nused word-level Language Identiﬁcation to estimate which language pairs were\ncode-switched on Twitter. They found that around 3.5% of tweets were code-\nswitched, with the most common pairs being English-Spanish, English-French\nand English-Portuguese. English-German tweets typically had only one switch\npoint, implying that the tweets usually contained translations of the same con-\ntent in English and German, while English-Turkish tweets had the most switch\npoints, implying ﬂuid switching between the two languages. Code-switching can\nalso vary within a language pair. For example, casual conversational Hinglish\nmay be diﬀerent from Hinglish used in Bollywood movies, which may be diﬀer-\nent from Hinglish seen on Twitter.\n2.2. Linguistic Models of Code-switching\nEarly approaches investigate code-switching by laying down a formal frame-\nwork taking into account the two grammatical systems of the languages being\nmixed and a mechanism to switch between these two systems at the intra-\nsentential level [19]. This model mainly explores asymmetric relations between\nthe two grammars, without an explicit formalism of a third grammar and the\nunderstanding of where and how to switch closed class items.\nQuantitative analysis conducted by [20] revealed two constraints (1) Free\nMorpheme Constraint and (2) Equivalence Constraint that function simulta-\nneously. The Free Morpheme constraint speciﬁes that it is possible to switch\nbetween full sentences as well as any constituent within the sentence if a free\nmorpheme is present in a constituent.\nThe Equivalence Constraint speciﬁes\nthat language switches generally occur at points where there is no violation of\nsyntactic rules of the participating languages.\n[21] worked on incorporating both linguistic and extra-linguistic factors into\na single analytical model. This study concludes that there are no visibly ungram-\nmatical combinations of the two languages and code-switching is independent of\nthe bilingual ability of the speaker. [22] showed that there exists a constituent\ntree labeling, implying that around a switch point there is a constraint on an\n6\nequivalence order in constituents. The above described linguistic theories are\nalso used in [23] to identify governing relationships between constituents. [24]\nhave demonstrated evidence that a constrained Universal Grammar needs reﬁne-\nment of f-selection in code-switching as compared to monolingual speech. [25]\nhave proposed four categories for any switch point comprising of harmonization,\nneutralization, compromise, and blocking.\n[26] have a rather interesting approach towards analyzing grammatical vari-\nants in code-switching based on pre-conceptualized assumptions. They claim\nthat grammar in this context is subject to poly-idiolectal repertoires of bilingual\nspeakers and sociolinguistic factors take precedence over grammatical factors.\nHence they propose accounting for variability among the bilingual speakers.\nThis same work was extended later to examine intra-sentential switching focus-\ning on bilingual compound verbs and using grammatical knowledge.\nThe linguistic theories mentioned above were put to use in computational\nframeworks by [27]. They address several issues such as the absence of literal\nlevel translation pairs, sensitivity to minor alignment errors and the under spec-\niﬁcation of the original models. The human evaluation of generated sentences\nreveals that the acceptability of code-switching patterns depend not only on\nsocio-linguistic factors but also cognitive factors. This work was later extended\nin [28] to perform language modeling by leveraging the theories discussed above\nto generate synthetic code-switched text.\nWhile on one hand, there are studies of formally constructing grammatical\nrepresentations to understand the nature of code-switching, there is also work\nthat focuses on understanding the psycho-linguistic aspect of this subject per-\ntaining to how and when this occurs. There are studies pertaining to socially\ndetermined and pragmatic choices in the developmental perspective of switching\nin bilingual infants [29].\nAnother stream of work talks about the factors triggering code-switching\nthat are attributed to ‘cognate’ or trigger words including proper nouns, cog-\nnate content words with good and moderate form overlap, and cognate func-\ntion words. [30] have studied attested contact-induced changes based on prior\n7\nlinguistic theories regarding the types of structural changes in calques, distribu-\ntions, frequencies, inventory and stability. [31] explored three diﬀerent hypoth-\nesis and presented empirical evidence for the same. They are the relationships\nbetween (i) cognate stimuli and code-switching, (ii) syntactic information and\ncode-switching, (iii) entrainment in a code-switched conversation among bilin-\nguals. The empirical evidence demonstrates that there is a strong correlation\nbetween precedence of cognates and code-switching, relationship between POS\ntags and code-switching and the convergence of the rate of entrianment in code-\nswitching.\n[32] present an integrated representation of inter and intra-sentential phe-\nnomena as well as spoken and written modalities of code-switching. This is done\nin order to make better reuse of the minimally available code-switched data by\nanalyzing various global dimensions such as modality, discourse, granularity, so-\ncial familiarity and social hierarchy. These properties pave way to potentially\nfootprint corpora and functional derivations. [33] present a systematic approach\nto analyze code-switching in conversations. Patterns of switching are analyzed\nin multi-party conversations from Hindi movie scripts to establish identity and\nsocial contexts.\n2.3. Measuring the amount of code-switching\nVarious metrics have been proposed to measure the amount of code-switching\nin corpora. The Code-mixing Index (CMI) [34] is an utterance and corpus level\nmetric proposed to measure the amount of code-switching in corpora by using\nword frequencies. [35] propose M-Index which quantiﬁes the ratio of languages\nin the corpora based on the Gini coeﬃcient to measure the inequality of the\ndistribution of languages in the corpus. [36] extend this metric to describe the\nprobability of switching within a corpus by summing up the probabilities that\nthere has been a language switch. This metric is termed Integration Index (\nI-Index) and has values of in the range from 0 (a monolingual text in which no\nswitching occurs) to 1.\n8\n[37] and [38] also propose the following metrics: Language Entropy, Span\nEntropy, Burstiness and Memory. Language Entropy and Span Entropy are the\nnumber of bits needed to represent the distribution of language spans. Bursti-\nness quantiﬁes whether the switching has periodic character or occurs in bursts.\nMemory captures the tendency of consecutive language spans to be positively\nor negatively autocorrelated.\n[39] propose techniques to automatically determine the matrix language of a\ncode-switched utterance. Although the notion of the matrix language is based\non the underlying grammar of the sentence, [39] show that the matrix language\ncan be determined by word-count alone as an approximation. [40] character-\nize languages as being asymmetric or symmetric depending on whether code\nswitching is insertional or alternating, and show that the same grammatical\nconstraints hold in both cases.\n3. Data and resources\nOver the last few years, signiﬁcant progress has been made in the ﬁelds of\nSpeech Processing and Natural Language Processing mainly owing to the use\nof large and powerful Machine Learning models such as Deep Neural Networks\n(DNNs). DNNs typically require large labeled corpora for training, which can be\nfound for a few languages such as US English, Mandarin and Modern Standard\nArabic, which are commonly termed as high-resource languages. In the presence\nof large datasets, models can be trained to achieve high accuracies on tasks such\nas Automatic Speech Recognition, Machine Translation and Parsing.\nHowever, most languages in the world do not have the necessary data and\nresources to create models with high enough accuracies to be used in real-world\nsystems. The situation is even more stark for code-switched languages, since\nconsiderable care is taken to leave out foreign words while building monolin-\ngual resources. So, even if monolingual resources exist for one or more of the\nlanguages being mixed, code-switched speech and language resources are very\nscarce.\n9\nHowever, owing to the recent interest in code-switched speech and language\nprocessing, there are some speech and text data sets available for a few language\npairs, which we describe next.\n3.1. Speech data\nData used for building Automatic Speech Recognition (ASR) and Text to\nSpeech (TTS) systems typically consists of recorded speech and the correspond-\ning transcripts. For ASR systems, the speech may be spontaneous or read, and\ntypically needs to be at least a few thousand hours to build systems that are\nusable. For TTS systems, a few hours of clean, well recorded speech from a\nsingle speaker is typically enough. Below is a list of code-switched data sets\navailable for speech processing.\n• SEAME [41] is a corpus of Mandarin-English code-switching by bilinguals\nin Singapore and Malaysia, with Mandarin being the dominant language\nin the recordings. It consists of 63 hours of interviews and conversational\nspeech from 97 speakers. [42] describes the updated SEAME corpus with\nan additional 129 hours of speech.\n• The HKUST Mandarin-English Corpus [43] also consists of interviews\nand conversational speech with 5 hours of transcribed and 15 hours of\nuntranscribed speech.\n• The CECOS corpus [44] contains 12 hours of prompted Mandarin-English\nspeech from 77 speakers.\n• The OC16-CE80 corpus is a 80 hour Mandarin-English corpus with En-\nglish words embedded in Mandarin utterances [45].\n• The CUMIX Cantonese-English speech corpus [46] contains 17 hours of\ncode-switched speech read by 80 speakers.\n• A Mandarin-Taiwanese corpus is described in [47] containing 4000 utter-\nances recorded by 16 speakers.\n10\n• [48] present an artiﬁcially generated Japanese-English code-switched cor-\npus using a Japanese and English Text-to-speech system from a bilingual\nspeaker. The corpus consists of 280k speech utterances.\n• BANGOR-MIAMI [49] is a Spanish-English code-switched corpus con-\nsisting of 56 audio recordings and their corresponding transcripts. The\nrecordings consist of informal conversations between two or more speak-\ners, involving a total of 84 speakers.\n• A small Spanish-English corpus consisting of 40 minutes of spontaneous\nspeech is described in [50].\n• [51] describe an audio and video corpus of elicited code-switched Spanish-\nEnglish and Hindi-English dialogues. The corpus consists of over 700 calls\nto an automated agent by workers on Amazon Mechanical Turk, although\nonly a small subset of these have been transcribed.\n• The MSR Hindi-English database [52] consists of\n50 hours of conver-\nsational speech between Hindi-English bilinguals. There are around 500\nspeakers in the corpus.\n• [53] crawled blogs to collect Hindi-English code-switched utterances. 71\nspeakers recorded around 7000 utterances which were then transcribed\nand used to build an ASR system.\n• [54] describe the creation of a phonetically balanced Hindi-English corpus\nfor code-switched ASR. This corpus contains read speech from 78 speak-\ners, with each speaker having recorded around a minute of speech. The\nprompts have been collected from news websites and sampled for phonetic\ncoverage.\n• [55] collected a small Hindi-English corpus of student interviews.\nThe\ncorpus contains 3 minutes of transcribed speech from 9 speakers.\n• [56] collected 1000 hours of Malay-English speech from 208 Chinese, Malay\nand Indian speakers.\n11\n• An Egyptian Arabic-English speech corpus is described in [57]. It consists\nof 5.3 hours of speech from interviews with 12 participants, of which 4.5\nhours of speech has been transcribed.\n• The MCSM database (Maghrebian Code-switching in Media) [58] consists\nof broadcasts from Morocco, Algeria and Tunisia with varying amounts\nof code-switching between French and Arabic. The FACST corpus (French\nArabic Code-switching Triggered)[59] consists of 7.3 hours of French-Algerian\nArabic code-switched speech from 20 bilingual speakers.\n• [60] describes a corpus of Turkish-German conversational speech consist-\ning of 5 hours of annotated speech data. The corpus is annotated with\nspeech and orthography information, including inter and intra-word switch\npoints.\n• [61] describe a corpus of radio broadcasts in Frisian covering a 50 year\ntime span containing code-switching with Dutch.\nThe corpus consists\nof 18.5 hours of speech annotated with speaker information, dialect and\ncode-switching details and the presence of background noise/music.\n• A corpus of code-switched isiZulu-English consisting of transcribed speech\nfrom soap operas is described in [62]. It contains around 16 hours of tran-\nscribed speech with code-switching boundary annotations. This corpus is\nextended in [63, 64] to also include 14 hours of English-isiXhosa, English-\nSetswana, and English-Sesotho code-switched speech.\n• For Sepedi-English, two code-switching speech corpora are available [65].\nThe Sepedi Radio corpus consists of broadcast speech which has been used\nfor analyzing code-switching in this language pair. The Sepedi Prompted\nSpeech Corpus consists of 10 hours of speech from 20 speakers.\n• Microsoft and SpeechOcean.com recently released 60 hours of code-switched\nspeech in three language pairs - Tamil-English, Telugu-English and Gujarati-\nEnglish as part of a shared task on Spoken Language Identiﬁcation [66].\n12\n• Although no code-switched speech databases exist for Speech Synthesis,\nbilingual TTS databases are available from the same speakers in a number\nof Indian languages and English [67].\n3.2. Text data\nIn this section, we describe various resources that exist for processing code-\nswitched text. Since the type of data and resources vary greatly with the task\nat hand, we describe them separately for each task.\n3.2.1. Language Identiﬁcation (LID)\nLanguage ID data sets consist of code-switched sentences that are labeled at\nthe word-level with language information. Conventional LID systems operate\nat the sentence or document level, which leads to the requirement of word-level\nLID for code-switched sentences. A couple of shared tasks played an important\nrole in establishing datasets for language identiﬁcation [68] [69].\n• A predetermined set of 11 users of Facebook users were selected to search\nfor publicly available content that resulted in 2335 posts and 9813 com-\nments [70]. Two levels of annotations were performed on this data com-\nprising of diﬀerent levels of code-switching and language tags including\nEnglish, Bengali, Hindi, Mixed, Universal and Undeﬁned. A similar ap-\nproach is also followed by [71] to collect more data from Bengali, Hindi\nand English with ﬁner annotation schema catering to named entities and\nalso explicitly annotating suﬃxes.\n• [72] used a two step process by crawling tweets related to 28 hashtags\ncomprising of contexts ranging from sports, movies, religion, politics etc,\nthat resulted in 811981 tweets. Identifying 3577 users from these tags,\ntweets from these users are crawled in order to gather more mixed language\nthus resulting in 725173 distinct tweets written in Roman script. These\ntweets are annotated with English, Hindi and Other tags.\n13\n• Another very large scale dataset that is is not explicitly targeted at code-\nswitching but contains it is [73] that addresses curating socially represen-\ntative text by taking into account geographic, social, topical and multi-\nlingual diversity. This corpus consists of Tweets from 197 countries in 53\nlanguages.\n• In [74] authors employed diﬀerent approaches to collect data in two diﬀer-\nent code mixed scenarios. For Nepali - English, they selected 42 twitter\nusers and collected 2000 code switched tweets from each of them. For\nSpanish English they performed a geographical based search and used 50\ntweets from 135 users after ﬁltering based on manual inspection.\n3.2.2. Named Entity Recognition (NER)\nNamed Entity Recognition (NER) datasets for code-switching are similar to\nLID datasets, with word-level annotations.\n• A shared task was organized to address NER for code-switched texts using\naround 50k Spanish-English and around 10k Arabic-English annotated\ntweets [75].\n• Twitter is a commonly used source of code-switched data. [76] annotated\n3,638 tweets with three Named Entity tags ‘Person’, ‘Organization’ and\n‘Location’ using the BIO scheme.\n3.2.3. Part of Speech (POS) Tagging\nPOS tagging data sets consist of code-switched sentences tagged at the word\nlevel with POS information.\n• Public pages from Facebook pages of three celebrities and the BBC Hindi\nnews page are used to gather 6,983 posts and comments and annotated\nwith POS tags in addition to matrix language information [77].\n• ICON 2015 conducted a shared task on POS tagging for which they re-\nleased data in Hindi-English, Bengali-English, Tamil-English [78]. The\ndataset contains 1k-3k annotated utterances for each language pair.\n14\n• Code-switched Turkish-German tweets were annotated based on Univer-\nsal Dependencies POS tags and the authors proposed guidelines for the\nTurkish parts to adopt language-general heuristics to gather a corpus of\n1029 tweets [79].\n• 922 sentences of spoken Spanish-English conversational data is transcribed\nand annotated with POS tags in [80].\n• [81] gathered 1106 messages (552 Facebook posts and 554 tweets) in Hindi-\nEnglish and annotated them with a Twitter speciﬁc tagset. [82] describe\nan English-Bengali corpus consisting of Twitter messages and two English-\nHindi corpora consisting of Twitter and Facebook messages tagged with\ncoarse and ﬁne grained POS tags.\n• [83] crowd-sourced POS tags using the Universal POS tagset to annotate\nthe BANGOR-MIAMI corpus which is a conversational speech dataset\nwith Spanish-English code-switching.\n• [84] create a corpus of 886,252 tokens in Modern Standard Arabic-Egyptian\nArabic annotated with POS as well as 16 code-switching tags. The code-\nswitching tags include language ID information as well as special tags to\nlabel mixed words that contain morphemes from diﬀerent languages. This\ndataset consists of tweets and sentences from news, commentaries and dis-\ncussion forms. The annotation is being extended to other Arabic dialects\nsuch as Levantine, Iraqi, Gulf, Moroccan and Tunisian.\n3.2.4. Parsing\nDatasets for parsing contain code-switched sentences with dependency parses\nand chunking tags.\n• [85] have worked on using monolingual resources to parse low resource\nlanguages in the presence of code-switching. While the training data com-\nprises of Russian UD v2.0 corpus with 3,850 sentences and 40 Komi sen-\n15\ntences, the test set comprises of 80 Komi-Russian multilingual sentences\nand 25 Komi spoken sentences.\n• [86] have presented a dataset of 450 Hindi-English CM tweets for evalua-\ntion purposes annotated with dependency parse relations.\n• A shallow parsing dataset comprising of 8450 tweets annotated with lan-\nguage id, normalized script, POS tagset and chunking tags is described in\n[87].\n• Code-switched test utterances for the NLmaps corpus are constructed by\n[88]. They use a parallel corpus of English and German utterances which\nshare the same logical form to construct code-switched utterances. They\nuse 1500 pairs of sentences from each language for training and 880 pairs\nfor testing.\n3.2.5. Question Answering (QA)\nQuestion answering (QA) datasets typically consist of questions and answers\nthat are in the form of articles, images, tuples etc. In case of code-switched QA,\nthe questions are typically in code-switched form.\n• A ﬁrst step towards creating a code-switched QA dataset was attempted\nby collecting 3000 questions from a version of a TV show “Who wants to\nbe a Millionaire?” and general knowledge questions from primary school\ntextbooks for Hindi-English code-switching questions [89]. Out of the 3000\nquestions, 1000 unique questions are used in order to avoid any individual\nbiases of language usage.\n• In lieu of addressing the possibility of lexical bias from entrainment in\n[89], another eﬀort was made on a larger scale to collect 5933 questions\nfor Hindi-English, Tamil-English, Telugu-English grounded on articles and\nimages [90]. The ﬁnal dataset included 1,694 Hindi-English, 2,848 Tamlil-\nEnglish and 1,391 Telugu-English factoid questions and their answers [91].\n16\n• Another section of eﬀorts that move towards using monolingual data from\nEnglish and weakly supervised and imperfect bilingual embeddings pro-\nvided a test set of 250 Hindi-English code-switched questions mapped\nbetween SimpleQuestions dataset and Freebase tuples [92].\n• One of the early eﬀorts also include leveraging around 300 messages from\nsocial media platforms like Twitter and blogs to collect 506 questions from\nthe domains of sports and tourism [93].\n3.3. Natural Language Inference\n[94] present the ﬁrst dataset for code-switched NLI, in which premises are\ntaken from Bollywood (Hindi) movie scripts and annotators create hypotheses\nthat entail or contradict the premises. The dataset contains 400 premises and\naround 2k hypotheses.\n3.4. Social media datasets\nVarious datasets from social media such as Facebook and Twitter have been\ncollected for diﬀerent NLP tasks, which we describe in this section.\n[95] collected 1959 Hindi-English tweets and asked annotators to rank tweets\naccording to relevance for speciﬁc queries.\n[96] collect a Twitter corpus of around 4k tweets and annotate it for Hate\nSpeech. [97] create a corpus of around 3k tweets for automated irony detection.\n4. Code-switched Speech and NLP Techniques\n[98] present a brief survey of code-switching studies in NLP. [99] describe the\nchallenges in computational processing of core NLP tasks as well as downstream\napplications. They highlight issues caused due to combining two languages at\nthe lexical and syntactic level, using examples from several tasks and language\npairs.\nIn this paper, we provide a comprehensive description of work done in code-\nswitched speech and NLP. Various approaches have been taken to build speech\n17\nand NLP systems for code-switched languages depending on the availability of\nmonolingual, bilingual and code-switched data. When there is a complete lack\nof code-switched data and resources, a few attempts have been made to build\nmodels using only monolingual resources from the two languages being mixed.\nDomain adaptation or transfer learning techniques can be used, wherein\nmodels are built on monolingual data and resources in the two languages and a\nsmall amount of ‘in-domain’ code-switched data can be used to tune the models.\nWord embeddings have been used recently for a wide variety of NLP tasks.\nCode-switched embeddings can be created using code-switched corpora [100],\nhowever, in practice such resources are not available and other techniques such\nas synthesizing code-switched data for training such embeddings can be used.\nMassive multilingual models such as multilingual BERT [101] have also been\nexplored in code-switched NLP.\n4.1. Automatic Speech Recognition\nSince code-switching is a spoken language phenomenon, it is important that\nAutomatic Speech Recognizers (ASRs) that are deployed in multilingual com-\nmunities are able to handle code-switching. In addition, ASR systems tend to\nbe the ﬁrst step in a pipeline of diﬀerent systems in applications such as con-\nversational agents, so any errors made by ASR systems can propagate through\nthe system and lead to failures in interactions.\nAttempts have been made to approach the problem of code-switched ASR\nfrom the acoustic, language and pronunciation modeling perspectives.\nInitial attempts at handling code-switched speech recognition identiﬁed the\nlanguage being spoken by using a Language Identiﬁcation (LID) system and\nthen used the appropriate monolingual decoder for recognition. One approach\nis to identify the language boundaries and subsequently use an monolingual\nASR system to recognize monolingual fragments [102]. Another approach runs\nmultiple recognizers in parallel with an LID system and uses scores from all the\nsystems for decoding speech [103]. In [56], no LID system is used - instead, two\nrecognizers in English and Malay are run in parallel and the hypotheses pro-\n18\nduced are re-scored to get the ﬁnal code-switched recognition result. However,\nthe disadvantages with multi-pass approaches are that errors made by the LID\nsystem are not possible to recover from. [47] suggest a single-pass approach with\nsoft decisions on LID and language boundary detection for Mandarin-Taiwanese\nASR.\nThe choice of phone set is important in building ASR systems and for code-\nswitched language pairs, the choice of phoneset is not always obvious, since one\nlanguage can have an inﬂuence on the pronunciation of the other language. [104]\ndevelop a cross-lingual phonetic Acoustic Model for Cantonese-English speech,\nwith the phone set designed based on linguistic knowledge. [105] present three\napproaches for Mandarin-English ASR - combining the two phone inventories,\nusing IPA mappings to construct a bilingual phone set and clustering phones\nby using the Bhattacharyya distance and acoustic likelihood. The clustering\napproach outperforms the IPA-based mapping and is comparable to the com-\nbination of the phone inventories. [52] describe approaches to combine phone\nsets, merge phones manually using knowledge and iterative merging using ASR\nerrors on Hindi-English speech. Although the automatic approach is promis-\ning, manual merging using expert knowledge from a bilingual speaker performs\nbest. [106] use IPA, Bhattacharya distance and discriminative training to com-\nbine phone sets for Mandarin-English. When code-switching occurs between\nclosely related languages, the phone set of one language can be extended to\ncover the other, as is suggested in [107] for Ukranian-Russian ASR. In this\nwork, the Ukranian phone set and lexicon are extended to cover Russian words\nusing phonetic knowledge about both languages.\n[65] describe an ASR system for Sepedi-English in which a single Sepedi\nlexicon is used for decoding.\nEnglish pronunciations in terms of the Sepedi\nphone set are obtained by phone-decoding English words with the Sepedi ASR.\n[54] use a common Wx-based phone set for Hindi-English ASR built using a\nlarge amount of monolingual Hindi data with a small amount of code-switched\nHindi-English data. [108] use cross-lingual data sharing to tackle the problem of\nhighly imbalanced Mandarin-English code-switching, where the speakers speak\n19\nprimarily in Mandarin. In [109], authors attempt to alleviate the problem of L2\nword pronunciation by creating linguistically motivated pairwise mappings\nWhen data from both languages is available but there is no or very lit-\ntle data in code-switched form, bilingual models can be built. [106] train the\nAcoustic Model on bilingual data, while [110] and [111] use existing monolin-\ngual models and with a phone-mapped lexicon and modiﬁed Language Model\nfor Hindi-English ASR. In [112], authors create synthetic code mixed speech\nby concatenating segments from diﬀerent monolingual utterances and employ\nthis to improve Hindi-English code mixed ASR performance. In [113], authors\nﬁrst detect real and untranscribed code mixed segments from online archives.\nThey then employ semi supervised and active learning techniques to obtain tran-\nscriptions and use as augmented data to train code switched models. In [114]\nauthors follow semi supervised training and show that incorporating language\nand speaker information is helpful while building bilingual acoustic models. In\n[115] authors combine monolingual and bilingual graphs together with a uniﬁed\nacoustic model.\n[116] propose a technique known as meta transfer learning to select the best\nmonolingual data for transfer that can improve code-switched models. [117]\ndescribe the importance of data selection between subsets of English, Mandarin\nand code-switched datasets for improving Mandarin-English ASR and show that\nsimply pooling all the data leads to worse results.\n[118] build a bilingual DNN-based ASR system for Frisian-Dutch broadcast\nspeech using both language-dependent and independent phones. The language\ndependent approach, where each phone is tagged with the language and mod-\neled separately performs better. [119] decode untranscribed data with this ASR\nsystem and add the decoded speech to ASR training data after rescoring using\nLanguage Models. In [120], this ASR is signiﬁcantly improved with augmented\ntextual and acoustic data by adding more monolingual data in Dutch, auto-\nmatically transcribing untranscribed data, generating code-switched data using\nRecurrent LMs and machine translation.\n[64, 121] build a uniﬁed ASR system for ﬁve South African languages, by us-\n20\ning interpolated language models from English-isiZulu, English-isiXhosa, English-\nSetswana and English-Sesotho.\nThis system is capable of recognizing code-\nswitched speech in any of the ﬁve language combinations.\n[122] use semi-supervised techniques to improve the lexicon, acoustic model\nand language model of English-Mandarin code-switched ASR. They modify the\nlexicon to deal with accents and treat utterances that the ASR system performs\npoorly on as unsupervised data. In [123] authors utilise ASR and TTS in a semi\nsupervised fashion to learn code switching. They further show that integrating\nlanguage embeddings allows the framework to address even the language pairs\nnot seen during training [124].\nIn [125] authors jointly train two Mandarin-English acoustic models that\ndiﬀer in the choice of acoustic units describing the salient acoustic and phonetic\ninformation. In [126], they observe that sharing parameters between the primary\nand auxiliary tasks helps capture language switching information.\nRecent studies have explored end-to-end ASR for code-switching.\nTradi-\ntional end-to-end ASR models require a large amount of training data, which is\ndiﬃcult to ﬁnd for code-switched speech. [127] propose a CTC-based model for\nMandarin-English speech, in which the model is ﬁrst trained using monolingual\ndata and then ﬁne-tuned on code-switched data. [128] use transfer learning from\nmonolingual models, wordpieces as opposed to graphemes and multitask learn-\ning with language identiﬁcation as an additional task for Mandarin-English end-\nto-end ASR. In [129], authors address the scenario where monolingual speakers\nattempt to comprehend code switched speech in the context of a dialog. To ad-\ndress this, they build a system to recognize code mixed speech and translate it\nto monolingual text. In [130], authors present a hypothesis that the discrepancy\nbetween distributions of token representations for diﬀerent languages restricts\nend to end models. To alleviate this, they constrain the token representations\nusing Shannon divergence and cosine distance. In [131], authors perform a frame\nlevel language detection and adjust the posterior distribution with CTC condi-\ntioned on the language detection. [132] present an RNN-T model with language\nbias that can improve upon an RNN-T model without any LID information,\n21\nwithout needing an explicit LID system.\nIn [133], authors explore using two types of units: characters for both Man-\ndarin and English, characters for Mandarin and sub word units for English. In\n[134] authors employ BPE sub word units. In [135], authors employ a frame\nlevel language recognition system to seed CTC based acoustic model.\nRecent work by [136] showed that speech recognition models ﬁne-tuned on\ncode-switched data regress on monolingual speech. To alleviate this issue and\nbuild robust models that can improve on both monolingual and code-switched\nspeech recognition, the authors propose using Learning Without Forgetting and\nadversarial training. [137] extends this work by proposing a multi-task approach\nto domain adversarial training that shows further improvements on both mono-\nlingual and code-switched ASR.\nAs stated earlier, switching/mixing and borrowing are not always clearly\ndistinguishable. Due to this, the transcription of code-switched and borrowed\nwords is often not standardized, and can lead to the presence of words be-\ning cross-transcribed in both languages. [138] automatically identify and dis-\nambiguate homophones in code-switched data to improve recognition of code-\nswitched Hindi-English speech.\n4.2. Language Modeling\nLanguage models (LMs) are used in a variety of Speech and NLP systems,\nmost notably in ASR and Machine Translation. Although there is signiﬁcantly\nmore code-switched text data compared to speech data in the form of informal\nconversational data such as on Twitter, Facebook and Internet forums, robust\nlanguage models typically require millions of sentences to build. Code-switched\ntext data found on the Internet may not follow exactly the same patterns as\ncode-switched speech. This makes building LMs for code-switched languages\nchallenging.\nMonolingual data in the languages being mixed may be available, and some\napproaches use only monolingual data in the languages being mixed [139] while\n22\nothers use large amounts of monolingual data with a small amount of code-\nswitched data.\nOther approaches have used grammatical constraints imposed by theories of\ncode-switching to constrain search paths in language models built using artiﬁ-\ncially generated data. [140] use inversion constraints to predict CS points and\nintegrate this prediction into the ASR decoding process. [141] integrate Func-\ntional Head constraints (FHC) for code-switching into the Language Model for\nMandarin-English speech recognition. This work uses parsing techniques to re-\nstrict the lattice paths during decoding of speech to those permissible under the\nFHC theory. [142] assign weights to parallel sentences to build a code-switched\ntranslation model that is used with a language model for decoding code-switched\nMandarin-English speech.\n[143] show that a training curriculum where an Recurrent Neural Network\n(RNN) LM is trained rst with interleaved monolingual data in both languages\nfollowed by code-switched data gives the best results for English-Spanish LM.\n[100] extend this work by using grammatical models of code-switching to gener-\nate artiﬁcial code-switched data and using a small amount of real code-switched\ndata to sample from the artiﬁcially generated data to build Language Models.\n[144] uses Factored Language Models for rescoring n-best lists during ASR\ndecoding. The factors used include POS tags, code-switching point probability\nand LID. In [145], [146] and [147], RNNLMs are combined with n-gram based\nmodels, or converted to backoﬀmodels, giving improvements in perplexity and\nmixed error rate.\n[148, 149] investigate the importance of syntactic information such as Part-\nof-Speech(POS) in predicting the switching point. They observe that the switchig\nattitude is speaker dependent[150].\n[151] synthesize isiZulu-English bigrams using word embeddings and use\nthem to augment training data for LMs, which leads to a reduction in per-\nplexity when tested on a corpus of soap opera speech.\nIn [152] authors employ dual RNNs for language model training while [153,\n154, 155] investigate the applicability of artiﬁcially generated code mixed data\n23\nfor data augmentation.\nIn [156], authors show that encoding language information improves lan-\nguage model by learning code switch points. In [157] authors present a dis-\ncriminative training based approach for model code mixed text. Alternatively,\nauthors in [158] propose to manipulate n gram based language model by employ-\ning clustering for the infrequent words. In [159] authors present an approach\nusing multi task learning by jointly learning language modeling as well POS\ntagging.\n[160] use a bilingual attention language model that learns cross-lingual prob-\nabilities by using parallel data simultaneously along with the language modeling\nobjective and achieves high reductions in perplexity over the SEAME corpus.\n4.3. Code-switching detection from speech\nIn [161] authors show that humans exploit prosodic cues to detect code\nmixing. They also show taht humans can anticipate switch poitns even in noisy\nspeech.\nAs mentioned earlier, some ASR systems ﬁrst try to detect the language\nbeing spoken and then use the appropriate model to decode speech. In case\nof intra-sentential switching, it may be useful to be able to detect the code-\nswitching style of a particular utterance, and be able to adapt to that style\nthrough specialized language models or other adaptation techniques.\n[162] look at the problem of language detection from code-switched speech\nand classify code-switched corpora by code-switching style and show that fea-\ntures extracted from acoustics alone can distinguish between diﬀerent kinds of\ncode-switching in a single language.\nIn [163, 164] authors investigate the eﬀectiveness of using retrained multilin-\ngual DNNS and augmenting the data for detecting the language. In [165, 166]\nauthors employ word based lexical information [167] build HMM based acoustic\nmodel followed by an SVM based decision classiﬁer to identify the code mixing\nbetween Northern Sotho and English.\n24\n4.4. Speech Synthesis\nMost Text to Speech (TTS) systems assume that the input is in a single\nlanguage and that it is written in native script. However, due to the rise in\nglobalization, phenomena such as code-switching are now seen in various types\nof text ranging from news articles through comments/posts on social media,\nleading to co-existence of multiple languages in the same sentence. Incidentally,\nthese typically are the scenarios where TTS systems are widely deployed as\nspeech interfaces and therefore these systems should be able to handle such\ninput. Even though independent monolingual synthesizers today are of very\nhigh quality, they are not fully capable of eﬀectively handling such mixed content\nthat they encounter when deployed. These synthesizers in such cases speak out\nthe wrong/accented version at best or completely leave the words from the other\nlanguage out at worst. Considering that the words from other language(s) used\nin such contexts are often the most important content in the message, these\nsystems need to be able to handle this scenario better.\nCurrent approaches handling code-switching fall into three broad categories:\nphone mapping, multilingual or polyglot synthesis.\nIn phone mapping, the\nphones of the foreign language are substituted with the closest sounding phones\nof the primary language, often resulting in strongly accented speech. In a mul-\ntilingual setting, each text portion in a diﬀerent language is synthesised by a\ncorresponding monolingual TTS system. This typically means that the diﬀer-\nent languages will have diﬀerent voices unless each of the voices is trained on\nthe voice of same multilingual speaker.\nEven if we have access to bilingual\ndatabases, care needs to be taken to ensure that the recording conditions of\nthe two databases are very similar. The polyglot solution refers to the case\nwhere a single system is trained using data from a multilingual speaker. Similar\napproaches to dealing with code-switching have been focused on assimilation\nat the linguistic level, and advocate applying a foreign linguistic model to a\nmonolingual TTS system. The linguistic model might include text analysis and\nnormalisation, a G2P module and a mapping between the phone set of the for-\neign language and the primary language of the TTS system [168, 169, 170].\n25\nOther approaches utilise cross-language voice conversion techniques [171] and\nadaptation on a combination of data from multiple languages[172]. Assimilation\nat the linguistic level is fairly successful for phonetically similar languages [170],\nand the resulting foreign synthesized speech was found to be more intelligible\ncompared to an unmodiﬁed non-native monolingual system but still retains a\ndegree of accent of the primary language. This might in part be attributed to\nthe non-exact correspondence between individual phone sets.\n[173] ﬁnd from subjective experiments that listeners have a strong preference\nfor cross-lingual systems with Hindi as the target language. However, in prac-\ntice, this method results in a strong foreign accent while synthesizing the English\nwords. [174, 175] propose a method to use a word to phone mapping instead,\nwhere an English word is statistically mapped to Indian language phones.\n[176] train speech synthesizers for Hindi-English, Tamil-English and Hindi-\nTamil by randomizing the order of bilingual training data which are then used\nto synthesize monolingual and code-switched text. This leads to improvements\nin subjective metrics for the code-switched speech and marginal degradation in\nmonolingual speech.\n[177] present an end-to-end code-switched TTS for Mandarin English, in\nwhich they use bilingual data with a shared encoder that contains language\ninformation and separate decoders. [178] extend this approach to use a bilingual\nphonetic posteriorgram (PPG) to synthesize code-switched speech using only\nmonolingual data. [179] also use a language speciﬁc encoder along with a multi-\nhead attention mechanism in the decoder resuling in large improvements in the\nSEAME corpus.\n4.5. Language Identiﬁcation\nThe task of lexical level language identiﬁcation (LID) is one of the skeletal\ntasks for the lexical level modeling of downstream NLP tasks. Most research\nhas focused on word-level LID, although some work on utterance-level LID\nalso exists.\n[180] build tools for web-scale analysis of code-switching, using\nan utterance-level language identiﬁcation system based on the language ratio of\n26\nthe two languages involved. A large amount of research in this area has been\nconducted due to shared tasks on word-level LID ([68], [69]).\nSocial media data, especially posts from Facebook was used to collect data\nfor the task of LID [70] of Bengali, Hindi and English code-switching. Techniques\ninclude dictionary based lookup, supervised techniques applied at word level\nalong with ablation studies of contextual cues and CRF based sequence labeling\napproaches. Character level n-gram features and contextual information are\nfound to be useful as features.\n[181] is among the ﬁrst computational approaches towards determining intra-\nword switching by segmenting the words into smaller meaningful units through\nmorphological segmentation and then performing language identiﬁcation prob-\nabilistically.\nThis was followed by intra-word approaches [182, 183] and ap-\nproaches that incorporate information beyond word level [184, 185, 186, 187,\n188, 189, 190, 191, 192, 193]. In addition to features, model based variants have\nbeen proposed by [194, 195, 196, 197, 198].\n[199] make use of patterns in language usage of Hinglish along with the con-\nsecutive POS tags for LID. [71] have also experimented with n-gram modeling\nwith pruning and SVM based models with feature ablations Hindi-English and\nBengali-English LID. [72] have worked on re-deﬁning and re-annotating language\ntags from social media cues based on cultural, core and therapeutic borrowings.\n[73] have introduced a socially equitable LID system known as EQUILID by\nexplicitly modeling switching with character level sequence to sequence models\nto encompass dialectal variability in addition to code-switching. [200] present a\nweakly supervised approach with a CRF based on a generalization expectation\ncriteria that outperformed HMM, Maximum Entropy and Naive Bayes methods\nby considering this a sequence labeling task.\nRecently, POS tagging has also been examined as a means to perform lan-\nguage identiﬁcation in code-switched scenarios [201]. To this end, they have col-\nlected a Devanagari corpus and annotated it with POS tags followed by translit-\nerating it into Roman text. The complementary English data is annotated with\nPOS tags as well. Several classical approaches including SVM, Decision Trees,\n27\nLogistic Regression and Random Forests have been experimented with. The\nfeature set that included POS tags along with the word length and the word\nitself with a random forest resulted in the highest performance. Hence mono-\nlingual data with corresponding POS tags seem useful in performing language\nidentiﬁcation of code-switched text.\n4.6. Named Entity Recognition\nAnother sequence labeling task of interest is Named Entity Recognition\n(NER). [75] organized a shared task on NER in code-switching by collecting data\nfrom tweets for Spanish-English and Arabic-English. [202] augmented state-of-\nthe-art character level Convolutional Neural Networks (CNNs) with Bi-LSTMs\nfollowed by a CRF layer, by enriching resources from external sources by stack-\ning layers of pre-trained embeddings, Brown clusters and gazetteer lists. [203]\nattempted to build models from observations from data comprising of less than\n3% of surface level Named Entities and a high Out of Vocabulary (OOV) per-\ncentage. To address these issues they rely on character based BiLSTM models\nand leveraging external resources. Prior to this shared task, [204] posed this\ntask as a multi-task learning problem by using a character level CNNs to model\nnon-standard spelling variations followed by a word level Bi-LSTM to model\nsequences. This work also highlights the importance of gazetteer lists since it is\nsimilar to a low resource setting.\n[205] studied Arabic text on social media by exploring the inﬂuence of word\nembedding based representations on NER. Along similar lines, [206] also in-\nvestigated how word representations are capable of boosting semi-supervised\napproaches to NER. [76] collected tweets from topics like politics, social events,\nsports and annotated them with three Named Entity Tags in the BIO scheme\nand explored CRF, LSTM and Decision Tree methods. Formal and informal lan-\nguage speciﬁc features were leveraged to employ Conditional Random Fields,\nMargin Infused Relaxed Algorithm, Support Vector Machines and Maximum\nEntropy Markov Models to perform NER on informal text in Twitter [207].\n[208] collected a dataset in an attempt to benchmark for the task of Named En-\n28\ntity Recognition in Arabish from three diﬀerent sources: Twitter, transcribed\nconversational speech and translating a standard NER dataset. The dataset\ncomprises of 6k sentences with 130k tokens.\nThe baseline model itself is a\nBiLSTM-CRF which is one of the heavily investigated architectures in the task\nof NER. On top of this, they have adopted the FLAIR framework [209] to in-\nvestigate diﬀerent types of embeddings along with pooled datasets. They have\nalso experimented with word embeddings that are not only traditionally used\nand also more recently used such as contextual embeddings in their architec-\nture and discovered that a combination of both performed better. [210] extend\nthe LSTM architecture to combat high percentage of out of vocabulary words\nin code-switched data using transfer learning with bilingual character repre-\nsentations. Additionally, they also remove the noise with normalization of the\nspellings. Alternative to the fusion approach see above, [211] utilize the self at-\ntention mechanism over the charcacter based embeddings. The ﬁnal embedding\nrepresentation is obtained by feeding these word and character based embed-\ndings through a stacked BiLSTM with residual connections. Inspired by this,\n[212] proposed multilingual meta embeddings that extend the scope to other\nrelated and similar languages. They circumvent the problem of lexical level\nlanguage identiﬁcation using the same self attention mechanism on pre-trained\nword embeddings. [213] propose the use of hierarchical meta-embeddings that\ncombine word and sub-word level embeddings to achieve SOTA performance on\nEnglish-Spanish NER.\n4.7. POS Tagging\nRecently there has been interest in code-switched structured prediction tasks\nlike POS tagging and parsing. [77] used a dual mechanism of utilizing both a\nCRF++ based tagger and a Twitter POS tagger in order to tag sequences of\nmixed language. The same work also proposed a dataset that is obtained from\nFacebook that is annotated at a multi-level for the tasks of LID, text normal-\nization, back transliteration and POS tagging. They claim that joint modeling\nof all these tasks is expected to yield better results. [79] presented POS annota-\n29\ntion for Turkish-German tweets that align with existing language identiﬁcation\nbased on POS tags from Universal Dependencies. [80] explored the exploita-\ntion of monolingual resources such as taggers (for Spanish and English data)\nand heuristic based approaches in conjunction with machine learning techniques\nsuch as SVM, Logit Boost, Naive Bayes and J48. This work shows that many\nerrors occur in the presence of intra-sentential switching thus establishing the\ncomplexity of the task.\n[81] have also gathered data from social media platforms such as Facebook\nand Twitter and have annotated them at coarse and ﬁne grained levels. They fo-\ncus on comparing language speciﬁc taggers with ML based approaches including\nCRFs, Sequential Minimal Optimization, Naive Bayes and Random Forests and\nobserv that Random Forests performed the best, although only marginally bet-\nter than combinations of individual language taggers. [83] use crowd-sourcing\nfor annotating universal POS labels for Spanish-English speech data by splitting\nthe task into three subtasks. These are 1. labeling a subset of tokens automat-\nically 2. disambiguating a subset of high frequency words 3. crowd-sourcing\ntags by decisions based on questions in the form of a decision tree structure.\nThe choice of mode of tagging is based on a curated list of words.\n[214] use a stacked model technique and compare them to joint modeling\nand pipeline based techniques to ﬁnd that that best stacked model that utilizes\nall features outperform the joint and pipeline-based models.\n[215] carry out normalization of code-switched data and assess the impact\non POS tagging as a downstream task. They ﬁnd that automatic normalization\nleads to a performance gain in POS tagging.\n4.8. Parsing\n[216] worked on bilingual syntactic parsing techniques for Hindi-English\ncode-switching using head-driven phrase structure grammar.\nThe parses in\ncases of ambiguities are ordered based on ontological derivations from Word-\nNet through a Word Sense Disambiguator [217]. However, there is an assumed\nexternal constraint in this work, where the head of the phrase determines the\n30\nsyntactic properties of the subcategorized elements irrespective of the languages\nto which these words belong.\n[86] leveraged a non-linear neural approach for the task of predicting the\ntransitions for the parser conﬁgurations of arc-eager transitions by leveraging\nonly monolingual annotated data by including lexical features from pre-trained\nword representations. [87] also worked on a pipeline and annotating data for\nshallow parsing by labeling three individual sequence labeling tasks based on\nlabels, boundaries and combination tasks where a CRF is trained for each of\nthese tasks.\n[88] performed multilingual semantic parsing using a transfer learning ap-\nproach for code-switched text utilizing cross lingual word embeddings in a se-\nquence to sequence framework. [85] compared diﬀerent systems for dependency\nparsing and concluded that the Multilingual BIST parser is able to parse code-\nswitched data relatively well.\n[218] present a Universal Dependencies dataset in Hindi-English and a neural\nstacking model for parsing with a new decoding scheme that outperforms prior\napproaches.\n4.9. Question Answering\nSo far, we have seen individual speech and NLP applications which can be\nused as part of other downstream applications. One very impactful downstream\napplication of casual and free mixing beyond mere borrowing in terms of infor-\nmation need is Question Answering (QA). This is especially important in the\ndomains of health and technology where there is a rapid change in vocabulary\nthereby resulting in rapid variations of usage with mixed languages. One of the\ninitial eﬀorts in eliciting code-mixed data to perform question classiﬁcation was\nundertaken by [89]. This work leveraged monolingual English questions from\nwebsites for school level science and maths, and from Indian version of the show\n‘Who wants to be a Millionaire?’. Crowd-workers are asked to translate these\nquestions into mixed language in terms of how they would frame this question\nto a friend next to them.\n31\nLexical level language identiﬁcation, transliteration, translation and adja-\ncency features are used to build an SVM based Question Classiﬁcation model\nfor data annotated based on coarse grained ontology proposed by [219]. Since\nthis mode of data collection has the advantage of gathering parallel corpus of\nEnglish questions with their corresponding code-switched questions, there is a\npossibility of lexical bias due to entrainment.\nIn order to combat this, [90]\ndiscussed techniques to crowd-source code-mixed questions based on a couple\nof sources comprising of code-mixed blog articles and based on certain fulcrum\nimages. They organized the ﬁrst edition of the code-mixed question answer-\ning challenge where the participants used techniques based on Deep Semantic\nSimilarity model for retrieval and pre-trained DrQA model ﬁne-tuned on the\ntraining dataset. An end-to-end web based QA system WebShodh is built and\nhosted by [220] which also has an additional advantage of collecting more data.\n[92] trained TripletSiamese-Hybrid CNN to re-rank candidate answers that\nare trained on the SimpleQuestions dataset in monolingual English as well as\nwith loosely translated code-mixed questions in English thereby eliminating the\nneed to actually perform full ﬂedged translation to answer queries. [93] gathered\na QA dataset from Facebook messages for Bengali-English CM domain.\nIn\naddition to this line of work, there were eﬀorts for developing a cross-lingual QA\nsystem where questions are asked in one language (English) and the answer is\nprovided in English but the candidate answers are searched in Hindi newspapers\n[221].\n[222] presented a query oriented multi-document summarization system for\nTelugu-English with a dictionary based approach for cross language query ex-\npansion using bilingual lexical resources. Cross language QA systems are ex-\nplored in European languages as well [223], [224].\n4.10. Sentiment Analysis/stance detection\n[225] provide a benchmarking dataset to perform sentiment analysis on 3,062\nEnglish-Spanish tweets. The annotations are based on SentiStrength into the\nlabels of positive, negative and neutral classes. The same group later extended\n32\nthis work to compare code-switching in monolingual and multilingual settings\n[226, 227]. The comparisons made between a multilingual model trained on a\nmultilingual dataset, separate monolingual models, and a monolingual model\nthat is triggered based on the language identiﬁcation demonstrate the eﬀective-\nness of the multilingual model to deal with code-switched scenarios. [228] use\na more classical word probabilities based approach to determine the sentiment\nof a tweet about a movie. In speciﬁc, this is performed for tweets in Telugu-\nEnglish mixed data by transliterating each Roman word to the corresponding\nTelugu script and computing the probability of the word in each class. [229]\nconducted a shared task for sentiment analysis of social media data in two lan-\nguage pairs. The dataset used for the shared task includes around 12k and 2500\ntweets released for training in Hindi-English and Bengali-English respectively.\nThe best performing system of the shared task used word and character level\nn-gram features with an SVM classiﬁer. A similar trend is observed by [230]\nwhile comparing the models of Naive Bayes and SVM to perform sentiment\nclassiﬁcation on movie reviews in Bengali-English.\nContrary to this, [231] use CNNs to model sub-word level representations.\nThese are then given to a dual encoder, which both capture the sentiment at the\nsentence level and at the sub-word level. Similarly, [232] approached this using\nmultitask learning over a CNN based encoder to classify the stance taken on a\npopular issue of ‘Demonetization’. The auxiliary task is posed as a manipulation\nof the primary task by combining the labels.\nExtending this to emotion detection, there have been several attempts to\nmodel this as a graph problem. [233] present a proposition of scheme to anno-\ntate data collected with emotions for Chinese-English corpus speciﬁcally. The\nschema is developed to address the choice of text in which a sentiment is ex-\npressed. This can be either in one of Chinese, or English text, or using both\nthe languages, or using a mixed language text. [234] gathered a dataset from\nWeibo.com which have labelled emotions which are then self aligned among\nthe languages using statistical machine translation paradigm. They use label\npropagation over a bipartite graph constructed on bilingual and sentiment in-\n33\nformation.\nThey have extended this work to joint factor graph model [235]\nbetween the two kinds of information identifying the necessity of correlating\ndiﬀerent emotions as well in addition to sentiment and languages. Along very\nsimilar lines, [236] use belief propagation over the factor graphs which poses this\nas a dynamic programming approach to query a graphical model. The graph\nitself is constructed as joint factor graph model by utilizing both the bilingual\nword information and the emotion related information.\n4.11. Hate Speech Detection\nIn [237], authors employ transfer learning. They ﬁrst train a CNN based\nmodel on a large corpus of hateful tweets as source task followed by ﬁne tuning\non a transliterated set in the same language. In [238], authors use a combination\nof psycho-linguistic feature and basic features and perform model averaging. In\n[239], authors investigate both hierarchical employing phonemic units and sub\nworld level models to detect hate speech from code mixed data.\n4.12. Natural Language Inference\n[94] present the ﬁrst work on code-switched NLI, where the task is to predict\nif a hypotheses entails or contradicts the given premise, which is in the form of a\nconversation taken from Bollywood (Hindi) movies. They ﬁne-tune multilingual\nBERT for the task, however, the accuracy of this model is only slightly better\nthan chance showing that NLI is a very challenging problem for code-switched\nNLP.\n4.13. Machine Translation\n[240] developed a machine translation scheme for translating Hinglish into\npure English and pure Hindi forms by performing cross morphological analysis.\n[241] show that a zero shot Neural Machine Translation system can also deal\nwith code-switched inputs, however, the results are not as good as monolingual\ninputs.\n34\n4.14. Dialogue and discourse\n[242] study lexical and prosodic features of code-switched Hindi-English di-\nalogue and ﬁnd that the embedded language (English) fragments are spoken\nmore slowly and with more vocal eﬀort, and pitch variation is higher in the\ncode-switched portion of the dialogues compared to the monolingual parts.\n[243] treat code-choice as linguistic style and study accommodation across\nturns in dialogues in Spanish-English and Hindi-English. They ﬁnd that ac-\ncommodation is aﬀected by the markedness of the languages in context and is\nsometimes seen after a few turns, leading to delayed accommodation.\nIn [244], authors investigate the eﬀectiveness of linguistically motivated strate-\ngies of code mixing in a goal oriented dialogue setting.\nCross-lingual Question Answering systems were extended to dialog systems\nfor railway inquiries [245]. Recently, there has been an attempt to create code-\nmixed version of goal oriented conversations [246] from the DSTC2 restaurant\nreservation dataset.\nAs we have discussed earlier, code-switching is a phenomenon observed in\ninformal scenarios, which implies that it is a suitable setting in conversational\nspeech. In coherence to this thought, there has been work on incorporating\nand comparing text and speech based features in identifying language at turn\nlevel in a dialog [247]. Hence this work focuses on inter-sentential switching as\nopposed to intra-sentential switching. This work demonstrated the eﬃcacy of\ni-Vector features in comparison to spectral features for speech segments. While\nthe best performing system is based on text features, this work equips the ﬁeld\nto work with speech based features when transcriptions are not available.\n4.15. User Interfaces\nWhile languages that are being mixed may share the same script (such as in\nthe case of English and Spanish), this is not true for many language pairs that\nare frequently code-switched, particularly when the languages are not related\nto each other. In such cases, users may choose to use the same script to write\nboth languages, or use a mixed script. This has implications not only in how\n35\nto process mixed languages, but also on how to display them. [248] presents a\nstudy on the interaction between script-mixing and language mixing for Hindi-\nEnglish and shows that script choice may be used for emphasis, disambiguation\nand marking whether a word is borrowed or not.\n4.16. Optical Character Recognition\n[249] extend a standard OCR model to enable transcription of code-switched\ntext by jointly performing transcription with word-level language identiﬁcation.\nThe model provides signiﬁcant error reductions in historical texts.\n4.17. Improving cross-lingual models\nRecently, code-switched text has been used to improve the performance of\ncross-lingual systems. Code-switching is seen as a bridge to anchor representa-\ntions in diﬀerent languages so that they can come closer in a common space and\nlead to improved performance in cross-lingual NLP tasks [250]. [251] use code-\nswitched text along with an English language identiﬁer to retrieve documents\nwritten in Romanized Hindi. [252] use alternating language modeling by arti-\nﬁcially generating code-switched text using phrase alignments between parallel\nsentences to improve performance on cross-lingual tasks such as XNLI [253].\n[250] use a similar approach in which they synthesize random code-switched\nsentences in multiple languages to improve zero-shot performance on XNLI.\n5. Evaluation of Code-switched Systems\nMuch of the progress in a new ﬁeld can be shaped by shared tasks in which\ncommon datasets are released and participants compete to build systems for a\nspeciﬁc task. There have been several shared tasks conducted for code-switched\ntext processing, and a few shared tasks for code-switched speech processing over\nthe last few years. Shared tasks for code-switched NLP have included Language\nIdentiﬁcation [254, 68, 255], transliterated search [256], code-mixed entity ex-\ntraction [257], mixed script information retrieval [258, 259], POS tagging [260],\n36\nNamed Entity Recognition [75], Sentiment Analysis [229] and Question An-\nswering [90, 91]. There have been fewer shared task for code-switched speech\nprocessing, however, the Blizzard challenge 2014 had a code-switched speech\nsynthesis task [261], and code-switched ASR challenges have been conducted\nfor Mandarin English [45, 262].\nRecently, a spoken Language Identiﬁcation\nchallenge was conducted for inter and intra-utterance LID [66] in three code-\nswitched language pairs.\nEach of these shared task have spurred research in their respective sub-\nareas of code-switched speech and NLP. However, it is not clear how well these\nindividual models can generalize across diﬀerent tasks and language pairs. To\naddress this gap, benchmarks for evaluating code-switching across diﬀerent NLP\nhave been proposed.\nThe GLUECoS benchmark [263] consists of 11 datasets spanning diﬀerent\ntasks for code-switching across two language pairs Spanish-English and Hindi-\nEnglish, including a new task for code-switching, Natural Language Inference\n(NLI). The GLUECoS benchmark aims to add more tasks to evaluate the general\nlanguage understanding capabilities of models, including tasks such as Question\nAnswering, Natural Language Generation and Summarization, Machine Trans-\nlation and NLI. The LINCE benchmark [264] consists of 10 datasets across\n5 language pairs. The tasks include LID, NER, POS tagging and Sentiment\nAnalysis.\nEvaluations conducted on the benchmarks described above indicate that\nmassively multilingual contextual language models such as multilingual BERT\n[101] outperform cross-lingual models and other task-speciﬁc models.\nThese\nmodels can be further improved by adding synthetic code-switched data to pre-\ntraining, as shown in [263]. While models on some word level tasks such as\nLanguage Identiﬁcation and Named Entity Recognition reach high accuracy,\nthe performance on harder tasks like Sentiment Analysis, Question Answering\nand NLI is much worse and there is a large gap between the performance of\nmodels on monolingual tasks compared to code-switched tasks. This indicates\nthat massive multilingual models do not perform as well on code-switching as\n37\nthey do on monolingual or even cross-lingual tasks. However, pre-training or\nﬁne-tuning such models on synthetic code-switched data in the absence of real\ncode-switched data seems to a be promising future direction.\n6. Challenges and Future Directions\nAlthough code-switching is a persistent phenomenon through out the whole\nworld, access to data will always be limited. Monolingual corpora will always be\neasier to ﬁnd as monolingual discourse is more common in formal environments\nand hence more likely to be archived. Code-switching data, by its nature of\nbeing used in more informal contexts, is less likely to be archived and hence\nharder to ﬁnd as training data. As code-switching is more likely to be used\nin less task speciﬁc contexts, with less explicit function it may also be more\ndiﬃcult to label such data.\nMost code-switching studies focus on pairs with one high resource language\n(e.g. English, Spanish, MSA, Putonghua) and a lower resource language, real-\nistically the position is much more complex than that. Although we consider\nHinglish data low resourced, there are many other Northern Indian languages\nthat are code-switched with Hindi and access to that data is even harder. Thus\ncode-switching studies will inherently always be data starved and our models\nmust therefore expect to work with limited data.\nMost current work in code-switching looks at one particular language pair.\nIt is not yet the case that architectures for multiple pairs are emerging, except\nperhaps within the Indian sub-continent where there are similar usage patterns\nwith English and various regional languages. However it is clear that not all\ncode-switching is the same. Relative ﬂuency, social prestige, topical restrictions\nand grammatical constraints have quite diﬀerent eﬀects on code-switching prac-\ntices thus it is hard to consider general code-switching models over multiple\nlanguage pairs.\nWe should also take into the account that as code-switching is more typical\nin less formal occasions, there are some tasks that are more likely to involve\n38\ncode-switching that others. Thus we are unlikely to encounter programming\nlanguages that use code-switching, but we are much more likely to encounter\ncode-switching in sentiment analysis. Likewise analysis of parliamentary tran-\nscripts are more likely to be monolingual, while code-switching is much more\nlikely in social media. Of course its not just the forum that aﬀects the distribu-\ntion, the topic too may be a factor.\nThese factors of use of code-switch should inﬂuence how we consider develop-\nment of code-switched models. Although it may be possible to build end-to-end\nsystems where large amounts of code-switching data is available, in well-deﬁned\ntask environments, such models will not have the generalizations we need to\ncover the whole space. For models that can make use of large amounts of un-\nlabeled data for training, generating synthetic code-switched data may be a\npromising direction. However, most models of code-switched data generation\nrely on syntactic constraints and do not take into account sociolinguistic factors\nthat aﬀect code-switched language. Building models that are capable of incor-\nporating these factors could lead to more realistic data generation, which could\nlead to better models for code-switched speech and NLP.\nIt is not yet clear yet from the NLP point of view if code-switching analysis\nshould be treated primarily as a translation problem, or be treated as a new\nlanguage itself. It is however likely as with many techniques in low-resource\nlanguage processing, exploiting resources from nearby languages will have an\nadvantage. It is common (though not always) that one language involved in\ncode-switching has signiﬁcant resources (e.g. English, Putonghua, Modern Stan-\ndard Arabic). Thus transfer learning approaches are likely to oﬀer short term\nadvantages. Also given the advancement of language technologies, particularly\ndue to the rise of massively multilingual models, developing techniques that can\nwork over multiple pairs of code-switched languages may lead to faster develop-\nment and generalization of the ﬁeld.\nEvaluating code-switched speech and NLP is challenging due to the lack\nof standardized datasets.\nAlthough initial attempts at creating benchmarks\nhave been made, a comprehensive evaluation of code-switched systems across\n39\nspeech and NLP tasks in many typologically diﬀerent language pairs is required.\nSuch evaluation benchmarks are even more important due to the prevalence of\nmultilingual models that perform zero-shot cross-lingual transfer well, and are\nalso expected to perform well on code-switched languages.\nSpeech and language technologies for code-switching is not yet a mature\nﬁeld. It is noted that the references to work in this article are for the most\npart the beginnings of analysis. They are investigating the raw tools that are\nnecessary in order for the development of full systems. While there has been\na lot of work on individual Speech and NLP systems for code-switching, there\nare no end-to-end systems that can interact in code-switched language with\nmultilingual humans. Speciﬁcally we are not yet seeing full end-to-end digital\nassistants for code-switched interaction, or sentiment analysis for code-switched\nreviews, or grammar and spelling for code-switched text. This is partly due to\nlack of data for such end-to-end systems, however, a code-switching intelligent\nagent has to be more than just the sum of parts that can handle code-switching.\nTo build eﬀective systems that can code-switch, we will also have to leverage the\nwork done in sociolinguistics to understand how, when and why to code-switch.\nReferences\nReferences\n[1] R. Hickey, The handbook of language contact, John Wiley & Sons, 2012.\n[2] P. Auer, A postscript: Code-switching and social identity, Journal of prag-\nmatics 37 (3) (2005) 403–410.\n[3] M. Heller, Negotiations of language choice in montreal, Language and\nsocial identity (1982) 108–118.\n[4] R. Jacobson, Codeswitching Worldwide. II, Vol. 126, Walter de Gruyter,\n2011.\n[5] A. Camilleri, Language values and identities: Code switching in secondary\nclassrooms in Malta, Linguistics and education 8 (1) (1996) 85–103.\n40\n[6] X. Qian, G. Tian, Q. Wang, Codeswitching in the primary eﬂclassroom\nin china–two case studies, System 37 (4) (2009) 719–730.\n[7] E. Rezvani, H. J. Street, A. E. Rasekh, Code-switching in iranian ele-\nmentary eﬂclassrooms: An exploratory investigation., English language\nteaching 4 (1) (2011) 18–25.\n[8] M. A. Peabody, Methods for pronunciation assessment in computer aided\nlanguage learning, Ph.D. thesis, Massachusetts Institute of Technology\n(2011).\n[9] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zitnick,\nD. Parikh, Vqa: Visual question answering, in: Proceedings of the IEEE\ninternational conference on computer vision, 2015, pp. 2425–2433.\n[10] P. Agarwal, A. Sharma, J. Grover, M. Sikka, K. Rudra, M. Choudhury, I\nmay talk in english but gaali toh hindi mein hi denge: A study of english-\nhindi code-switching and swearing pattern on social networks, in: 2017\n9th International Conference on Communication Systems and Networks\n(COMSNETS), IEEE, 2017, pp. 554–557.\n[11] L. Todd, L. Todd, Pidgins and creoles, Routledge, 2003.\n[12] J. Arends, P. Muysken, N. Smith, Pidgins and creoles: An introduction,\nVol. 15, John Benjamins Publishing, 1995.\n[13] M. Sebba, Contact languages: Pidgins and creoles, Macmillan Interna-\ntional Higher Education, 1997.\n[14] C. Myers-Scotton, Contact linguistics: Bilingual Encounters and Gram-\nmatical Outcomes, Oxford University Press on Demand, 2002.\n[15] J. J. Gumperz, Discourse strategies, Vol. 1, Cambridge University Press,\n1982.\n[16] C.\nMyers-Scotton,\nDuelling\nlanguages:\nGrammatical\nstructure\nin\ncodeswitching, Oxford University Press, 1997.\n41\n[17] K. Bali, J. Sharma, M. Choudhury, Y. Vyas, ” i am borrowing ya mixing?”\nan analysis of english-hindi code mixing in facebook, in: Proceedings of\nthe First Workshop on Computational Approaches to Code Switching,\n2014, pp. 116–126.\n[18] S. Rijhwani, R. Sequiera, M. Choudhury, K. Bali, C. S. Maddila, Esti-\nmating code-switching on twitter with a novel generalized word-level lan-\nguage detection technique, in: Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers),\nVol. 1, 2017, pp. 1971–1982.\n[19] A. K. Joshi, Processing of sentences with intra-sentential code-switching,\nin:\nProceedings of the 9th conference on Computational linguistics-\nVolume 1, Academia Praha, 1982, pp. 145–150.\n[20] S. Poplack, Syntactic structure and social function of code-switching,\nVol. 2, Centro de Estudios Puertorrique˜nos,[City University of New York],\n1978.\n[21] S. Poplack, Sometimes ill start a sentence in spanish y termino en espanol:\ntoward a typology of code-switching1, Linguistics 18 (7-8) (1980) 581–618.\n[22] D. Sankoﬀ, A formal production-based explanation of the facts of code-\nswitching, Bilingualism: language and cognition 1 (1) (1998) 39–50.\n[23] A.-M. Di Sciullo, P. Muysken, R. Singh, Government and code-mixing,\nJournal of linguistics 22 (1) (1986) 1–24.\n[24] H. M. Belazi, E. J. Rubin, A. J. Toribio, Code switching and x-bar theory:\nThe functional head constraint, Linguistic inquiry (1994) 221–237.\n[25] M. Sebba, A congruence approach to the syntax of codeswitching, Inter-\nnational Journal of Bilingualism 2 (1) (1998) 1–19.\n[26] P. Gardner-Chloros, M. Edwards, Assumptions behind grammatical ap-\nproaches to code-switching: When the blueprint is a red herring, Trans-\nactions of the Philological Society 102 (1) (2004) 103–129.\n42\n[27] G. Bhat, M. Choudhury, K. Bali, Grammatical constraints on intra-\nsentential code-switching:\nFrom theories to working models, arXiv\npreprint arXiv:1612.04538.\n[28] A. Pratapa, G. Bhat, M. Choudhury, S. Sitaram, S. Dandapat, K. Bali,\nLanguage modeling for code-mixing: The role of linguistic theory based\nsynthetic data, in: Proceedings of the 56th Annual Meeting of the Asso-\nciation for Computational Linguistics, 2018.\n[29] U. Lanvers, Language Alternation in infant bilinguals: A developmental\napproach to codeswitching, International Journal of Bilingualism.\n[30] A. Backus, Codeswitching and language change: One thing leads to an-\nother?, International Journal of Bilingualism 9 (3-4) (2005) 307–340.\n[31] V. Soto, N. Cestero, J. Hirschberg, The role of cognate words, pos tags\nand entrainment in code-switching., in: Interspeech, 2018, pp. 1938–1942.\n[32] S. Hartmann, M. Choudhury, K. Bali, An integrated representation of\nlinguistic and social functions of code-switching, in: Proceedings of the\nEleventh International Conference on Language Resources and Evaluation\n(LREC 2018), 2018.\n[33] A. Pratapa, M. Choudhury, Quantitative characterization of code switch-\ning patterns in complex multi-party conversations: A case study on hindi\nmovie scripts, in: Proceedings of the 14th International Conference on\nNatural Language Processing (ICON-2017), 2017, pp. 75–84.\n[34] B. Gamb¨ack, A. Das, Comparing the Level of Code-Switching in Corpora,\nin: LREC, 2016.\n[35] R. Barnett, E. Cod´o, E. Eppler, M. Forcadell, P. Gardner-Chloros,\nR. Van Hout, M. Moyer, M. C. Torras, M. T. Turell, M. Sebba, et al., The\nLIDES Coding Manual: A Document for Preparing and Analyzing Lan-\nguage Interaction Data Version 1.1, International Journal of Bilingualism\n4 (2) (2000) 131–271.\n43\n[36] G. A. Guzman, J. Serigos, B. Bullock, A. J. Toribio, Simple tools for\nexploring variation in code-switching for linguists, in: Proceedings of the\nSecond Workshop on Computational Approaches to Code Switching, 2016,\npp. 12–20.\n[37] G. Guzm´an, J. Ricard, J. Serigos, B. E. Bullock, A. J. Toribio, Metrics for\nmodeling Code-Switching across Corpora, Proc. Interspeech 2017 (2017)\n67–71.\n[38] G. A. Guzm´an, J. Ricard, J. Serigos, B. Bullock, A. J. Toribio, Moving\ncode-switching research toward more empirically grounded methods., in:\nCDH@ TLT, 2017, pp. 1–9.\n[39] B. Bullock, W. Guzm´an, J. Serigos, V. Sharath, A. J. Toribio, Predicting\nthe presence of a matrix language in code-switching, in: Proceedings of the\nthird workshop on computational approaches to linguistic code-switching,\n2018, pp. 68–75.\n[40] B. E. Bullock, G. A. Guzm´an, J. Serigos, A. J. Toribio, Should code-\nswitching models be asymmetric?, in: Interspeech, 2018, pp. 2534–2538.\n[41] D.-C. Lyu, T.-P. Tan, E. S. Chng, H. Li, Seame: A Mandarin-English\nCode-Switching Speech Corpus in South-East Asia, in: Eleventh Annual\nConference of the International Speech Communication Association, 2010.\n[42] G. Lee, T.-N. Ho, E.-S. Chng, H. Li, A review of the mandarin-english\ncode-switching corpus: Seame, in: 2017 International Conference on Asian\nLanguage Processing (IALP), IEEE, 2017, pp. 210–213.\n[43] Y. Li, Y. Yu, P. Fung, A Mandarin-English Code-Switching Corpus., in:\nLREC, 2012, pp. 2515–2519.\n[44] H.-P. Shen, C.-H. Wu, Y.-T. Yang, C.-S. Hsu, Cecos: A chinese-english\ncode-switching speech database, in:\n2011 International Conference on\nSpeech Database and Assessments (Oriental COCOSDA), IEEE, 2011,\npp. 120–123.\n44\n[45] D. Wang, Z. Tang, D. Tang, Q. Chen, Oc16-ce80: A chinese-english\nmixlingual database and a speech recognition baseline, in: 2016 Confer-\nence of The Oriental Chapter of International Committee for Coordination\nand Standardization of Speech Databases and Assessment Techniques (O-\nCOCOSDA), IEEE, 2016, pp. 84–88.\n[46] J. Y. Chan, P. Ching, T. Lee, Development of a Cantonese-English Code-\nMixing Speech Corpus, in: Ninth European Conference on Speech Com-\nmunication and Technology, 2005.\n[47] D.-C. Lyu, R.-Y. Lyu, Y.-c. Chiang, C.-N. Hsu, Speech Recognition on\nCode-Switching among the Chinese Dialects, in: Acoustics, Speech and\nSignal Processing, 2006. ICASSP 2006 Proceedings. 2006 IEEE Interna-\ntional Conference on, Vol. 1, IEEE, 2006, pp. I–I.\n[48] S. Nakayama, T. Kano, Q. T. Do, S. Sakti, S. Nakamura, Japanese-english\ncode-switching speech data construction, in: 2018 Oriental COCOSDA-\nInternational Conference on Speech Database and Assessments, IEEE,\n2018, pp. 67–71.\n[49] M. Deuchar, P. Davies, J. Herring, M. C. P. Couto, D. Carter, Building\nBilingual Corpora, Advances in the Study of Bilingualism (2014) 93–111.\n[50] J. C. Franco, T. Solorio, Baby-steps towards building a Spanglish Lan-\nguage Model, in: International Conference on Intelligent Text Processing\nand Computational Linguistics, Springer, 2007, pp. 75–84.\n[51] V. Ramanarayanan, D. Suendermann-Oeft, Jee haan, I’d like both, por fa-\nvor: Elicitation of a Code-Switched Corpus of Hindi–English and Spanish–\nEnglish Human–Machine Dialog, Proc. Interspeech 2017 (2017) 47–51.\n[52] S. Sivasankaran, B. M. L. Srivastava, S. Sitaram, K. Bali, M. Choudhury,\nPhone Merging for Code-Switched Speech Recognition, in: Proceedings of\nthe Third Workshop on Computational Approaches to Linguistic Code-\nSwitching, 2018, pp. 11–19.\n45\n[53] G. Sreeram, K. Dhawan, R. Sinha, Hindi-English Code-Switching Speech\nCorpus, arXiv preprint arXiv:1810.00662.\n[54] A. Pandey, B. M. L. Srivastava, S. V. Gangashetty, Adapting Monolingual\nResources for Code-Mixed Hindi-English Speech Recognition, in: Asian\nLanguage Processing (IALP), 2017 International Conference on, IEEE,\n2017, pp. 218–221.\n[55] A. Dey, P. Fung, A Hindi-English Code-Switching Corpus., in: LREC,\n2014, pp. 2410–2413.\n[56] B. H. Ahmed, T.-P. Tan, Automatic Speech Recognition of Code Switch-\ning Speech using 1-best Rescoring, in: Asian Language Processing (IALP),\n2012 International Conference on, IEEE, 2012, pp. 137–140.\n[57] I. Hamed, M. Elmahdy, S. Abdennadher, Collection and Analysis of Code-\nSwitch Egyptian Arabic-English Speech Corpus, in: Proceedings of the\nEleventh International Conference on Language Resources and Evaluation\n(LREC-2018), 2018.\n[58] D. MOHDEB-AMAZOUZ, A.-D. Martine, L. LAMEL, Arabic-French\nCode-Switching across Maghreb Arabic dialects: A Quantitative Anal-\nysis.\n[59] D. Amazouz, M. Adda-Decker, L. Lamel, The French-Algerian Code-\nSwitching Triggered Audio Corpus (FACST), in: LREC 2018 11th edition\nof the Language Resources and Evaluation Conference,, 2018.\n[60] ¨O. C¸etino˘glu, A code-switching corpus of turkish-german conversations,\nin: Proceedings of the 11th Linguistic Annotation Workshop, 2017, pp.\n34–40.\n[61] E. Yilmaz, J. Dijkstra, H. Velde, H. Heuvel, D. van Leeuwen, Longitudinal\nSpeaker Clustering and Veriﬁcation Corpus with Code-Switching Frisian-\nDutch Speech.\n46\n[62] E. van der Westhuizen, T. Niesler, Automatic Speech Recognition of\nEnglish-isizulu Code-Switched Speech from South African Soap Operas,\nProcedia Computer Science 81 (2016) 121–127.\n[63] E. van der Westhuizen, T. Niesler, A ﬁrst south african corpus of multi-\nlingual code-switched soap opera speech, in: Proceedings of the Eleventh\nInternational Conference on Language Resources and Evaluation (LREC\n2018), 2018.\n[64] E. Yılmaz, A. Biswas, E. van der Westhuizen, F. de Wet, T. Niesler, Build-\ning a Uniﬁed Code-Switching ASR System for South African Languages,\narXiv preprint arXiv:1807.10949.\n[65] T. I. Modipa, M. H. Davel, F. De Wet, Implications of Sepedi/English\nCode Switching for ASR Systems.\n[66] S. Shah, S. Sitaram, R. Mehta, First workshop on speech processing for\ncode-switching in multilingual communities: Shared task on code-switched\nspoken language identiﬁcation.\n[67] A. Baby, A. L. Thomas, H. Myrthy, Resources for Indian Languages, in:\nProceedings of Text, Speech and Dialogue, 2016.\n[68] T. Solorio, E. Blair, S. Maharjan, S. Bethard, M. Diab, M. Ghoneim,\nA. Hawwari, F. AlGhamdi, J. Hirschberg, A. Chang, et al., Overview for\nthe ﬁrst shared task on language identiﬁcation in code-switched data, in:\nProceedings of the First Workshop on Computational Approaches to Code\nSwitching, 2014, pp. 62–72.\n[69] R. Sequiera, M. Choudhury, P. Gupta, P. Rosso, S. Kumar, S. Banerjee,\nS. K. Naskar, S. Bandyopadhyay, G. Chittaranjan, A. Das, et al., Overview\nof FIRE-2015 Shared Task on Mixed Script Information Retrieval.\n[70] U. Barman, A. Das, J. Wagner, J. Foster, Code mixing: A challenge for\nlanguage identiﬁcation in the language of social media, in: Proceedings of\n47\nthe ﬁrst workshop on computational approaches to code switching, 2014,\npp. 13–23.\n[71] A. Das, B. Gamb¨ack, Identifying Languages at the Word level in Code-\nMixed Indian Social Media Text.\n[72] J. Patro, B. Samanta, S. Singh, A. Basu, P. Mukherjee, M. Choudhury,\nA. Mukherjee, All that is English may be Hindi: Enhancing Language\nIdentiﬁcation through Automatic Ranking of the Likeliness of Word Bor-\nrowing in Social Media, in: Proceedings of the 2017 Conference on Em-\npirical Methods in Natural Language Processing, 2017, pp. 2264–2274.\n[73] D. Jurgens, Y. Tsvetkov, D. Jurafsky, Incorporating Dialectal Variability\nfor Socially Equitable Language Identiﬁcation, in: Proceedings of the 55th\nAnnual Meeting of the Association for Computational Linguistics (Volume\n2: Short Papers), Vol. 2, 2017, pp. 51–57.\n[74] S. Maharjan, E. Blair, S. Bethard, T. Solorio, Developing language-tagged\ncorpora for code-switching tweets, in: Proceedings of The 9th Linguistic\nAnnotation Workshop, 2015, pp. 72–84.\n[75] G. Aguilar, F. AlGhamdi, V. Soto, M. Diab, J. Hirschberg, T. Solorio,\nNamed Entity Recognition on Code-Switched Data:\nOverview of the\nCALCS 2018 Shared Task, in: Proceedings of the Third Workshop on\nComputational Approaches to Linguistic Code-Switching, 2018, pp. 138–\n147.\n[76] V. Singh, D. Vijay, S. S. Akhtar, M. Shrivastava, Named Entity Recogni-\ntion for Hindi-English Code-Mixed Social Media Text, in: Proceedings of\nthe Seventh Named Entities Workshop, 2018, pp. 27–35.\n[77] Y. Vyas, S. Gella, J. Sharma, K. Bali, M. Choudhury, POS Tagging of\nEnglish-Hindi Code-Mixed Social Media Content, in: Proceedings of the\n2014 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), 2014, pp. 974–979.\n48\n[78] S. Ghosh, S. Ghosh, D. Das, Part-of-speech tagging of code-mixed social\nmedia text, in: Proceedings of the Second Workshop on Computational\nApproaches to Code Switching, 2016, pp. 90–97.\n[79] ¨O. C¸etino˘glu, C¸. C¸¨oltekin, Part of Speech Annotation of a Turkish-\nGerman Code-Switching Corpus, in: Proceedings of the 10th Linguistic\nAnnotation Workshop held in conjunction with ACL 2016 (LAW-X 2016),\n2016, pp. 120–130.\n[80] T. Solorio, Y. Liu, Part-Of-Speech Tagging for English-Spanish Code-\nSwitched Text, in: Proceedings of the Conference on Empirical Methods in\nNatural Language Processing, Association for Computational Linguistics,\n2008, pp. 1051–1060.\n[81] A. Jamatia, B. Gamb¨ack, A. Das, Part-Of-Speech Tagging for Code-Mixed\nEnglish-Hindi Twitter and Facebook Chat Messages, in: Proceedings of\nthe International Conference Recent Advances in Natural Language Pro-\ncessing, 2015, pp. 239–248.\n[82] A. Jamatia, B. Gamb¨ack, A. Das, Collecting and annotating indian social\nmedia code-mixed corpora, in: International Conference on Intelligent\nText Processing and Computational Linguistics, Springer, 2016, pp. 406–\n417.\n[83] V. Soto, J. Hirschberg, Crowdsourcing universal part-of-speech tags for\ncode-switching.\n[84] M. Diab, M. Ghoneim, A. Hawwari, F. AlGhamdi, N. AlMarwani, M. Al-\nBadrashiny, Creating a large multi-layered representational repository of\nlinguistic code switched arabic data, LREC 2016.\n[85] N. Partanen, K. Lim, M. Rießler, T. Poibeau, Dependency parsing of\ncode-switching data with cross-lingual feature representations, in: Pro-\nceedings of the Fourth International Workshop on Computatinal Linguis-\ntics of Uralic Languages, 2018, pp. 1–17.\n49\n[86] I. Bhat, R. A. Bhat, M. Shrivastava, D. Sharma, Joining hands: Exploiting\nmonolingual treebanks for parsing of code-mixing data, in: Proceedings\nof the 15th Conference of the European Chapter of the Association for\nComputational Linguistics: Volume 2, Short Papers, Vol. 2, 2017, pp.\n324–330.\n[87] A. Sharma, S. Gupta, R. Motlani, P. Bansal, M. Shrivastava, R. Mamidi,\nD. M. Sharma, Shallow parsing pipeline for hindi-english code-mixed so-\ncial media text, in: Proceedings of NAACL-HLT, 2016, pp. 1340–1345.\n[88] L. Duong, H. Afshar, D. Estival, G. Pink, P. Cohen, M. Johnson, Multi-\nlingual semantic parsing and code-switching, in: Proceedings of the 21st\nConference on Computational Natural Language Learning (CoNLL 2017),\n2017, pp. 379–389.\n[89] K. C. Raghavi, M. Chinnakotla, M. Shrivastava, “Answer ka type kya\nhe?” Learning to Classify Questions in Code-Mixed Language.\n[90] K. Chandu, E. Loginova, V. Gupta, J. van Genabith, G. Neuman,\nM. Chinnakotla, E. Nyberg, A. W. Black, Code-Mixed Question Answer-\ning Challenge: Crowd-sourcing Data and Techniques, in: Proceedings of\nthe Third Workshop on Computational Approaches to Linguistic Code-\nSwitching, 2018, pp. 29–38.\n[91] K. Chandu, E. Loginova, V. Gupta, J. v. Genabith, G. Neumann, M. Chin-\nnakotla, E. Nyberg, A. W. Black, Code-mixed question answering chal-\nlenge:\nCrowd-sourcing data and techniques, in:\nThird Workshop on\nComputational Approaches to Linguistic Code-Switching, Association for\nComputational Linguistics (ACL), 2019, pp. 29–38.\n[92] V. Gupta, M. Chinnakotla, M. Shrivastava, Transliteration Better than\nTranslation? Answering Code-mixed Questions over a Knowledge Base,\nin: Proceedings of the Third Workshop on Computational Approaches to\nLinguistic Code-Switching, 2018, pp. 39–50.\n50\n[93] S. Banerjee, S. K. Naskar, P. Rosso, S. Bandyopadhyay, The First Cross-\nScript Code-Mixed Question Answering Corpus.\n[94] S. Khanuja, S. Dandapat, S. Sitaram, M. Choudhury, A new dataset for\nnatural language inference from code-mixed conversations, in: Proceed-\nings of the The 4th Workshop on Computational Approaches to Code\nSwitching, 2020, pp. 9–16.\n[95] K. Chakma, A. Das, Cmir: A corpus for evaluation of code mixed infor-\nmation retrieval of hindi-english tweets, Computaci´on y Sistemas 20 (3)\n(2016) 425–434.\n[96] A. Bohra, D. Vijay, V. Singh, S. S. Akhtar, M. Shrivastava, A dataset of\nhindi-english code-mixed social media text for hate speech detection, in:\nProceedings of the second workshop on computational modeling of peoples\nopinions, personality, and emotions in social media, 2018, pp. 36–41.\n[97] D. Vijay, A. Bohra, V. Singh, S. S. Akhtar, M. Shrivastava, A dataset for\ndetecting irony in hindi-english code-mixed social media text., in: EM-\nSASW@ ESWC, 2018, pp. 38–46.\n[98] S. Thara, P. Poornachandran, Code-mixing: A brief survey, in: 2018\nInternational Conference on Advances in Computing, Communications\nand Informatics (ICACCI), IEEE, 2018, pp. 2382–2388.\n[99] ¨O. C¸etino˘glu, S. Schulz, N. T. Vu, Challenges of computational processing\nof code-switching, arXiv preprint arXiv:1610.02213.\n[100] A. Pratapa, G. Bhat, M. Choudhury, S. Sitaram, S. Dandapat, K. Bali,\nLanguage modeling for code-mixing: The role of linguistic theory based\nsynthetic data, in: Proceedings of the 56th Annual Meeting of the Asso-\nciation for Computational Linguistics, 2018.\n[101] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Multilingual bert (2018).\n51\n[102] J. Y. Chan, P. Ching, T. Lee, H. M. Meng, Detection of language boundary\nin code-switching utterances by bi-phone probabilities, in: International\nSymposium on Chinese Spoken Language Processing, IEEE, 2004.\n[103] J. Weiner, N. T. Vu, D. Telaar, F. Metze, T. Schultz, D.-C. Lyu, E.-S.\nChng, H. Li, Integration of Language Identiﬁcation into a recognition sys-\ntem for spoken conversations containing code-switches, in: Spoken Lan-\nguage Technologies for Under-Resourced Languages, 2012.\n[104] J. Y. Chan, H. Cao, P. Ching, T. Lee, Automatic recognition of Cantonese-\nEnglish code-mixing speech, International Journal of Computational Lin-\nguistics & Chinese Language Processing, Volume 14, Number 3, Septem-\nber 2009.\n[105] S. Yu, S. Hu, S. Zhang, B. Xu, Chinese-English bilingual speech recog-\nnition, in: Proceedings of International Conference on Natural Language\nProcessing and Knowledge Engineering, IEEE, 2003.\n[106] N. T. Vu, D.-C. Lyu, J. Weiner, D. Telaar, T. Schlippe, F. Blaicher, E.-S.\nChng, T. Schultz, H. Li, A ﬁrst speech recognition system for Mandarin-\nEnglish code-switch conversational speech, in: IEEE International Confer-\nence on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2012.\n[107] T. Lyudovyk, V. Pylypenko, Code-switching Speech Recognition for\nclosely related languages, in: Spoken Language Technologies for Under-\nResourced Languages, 2014.\n[108] C.-F. Yeh, L.-S. Lee, An improved framework for recognizing highly im-\nbalanced bilingual code-switched lectures with Cross-language Acoustic\nmodeling and frame-level language identiﬁcation, IEEE Transactions on\nAudio, Speech, and Language Processing(ICASSP) 2015.\n[109] C. M. White, S. Khudanpur, J. K. Baker, An investigation of acoustic\nmodels for multilingual code-switching, in: Ninth Annual Conference of\nthe International Speech Communication Association, 2008.\n52\n[110] K. Bhuvanagiri, S. Kopparapu, An approach to mixed language Automatic\nSpeech Recognition, Oriental COCOSDA, Kathmandu, Nepal, 2010.\n[111] K. Bhuvanagirir, S. K. Kopparapu, Mixed language speech recognition\nwithout explicit identiﬁcation of language, American Journal of Signal\nProcessing.\n[112] K. Taneja, S. Guha, P. Jyothi, B. Abraham, Exploiting monolingual\nspeech corpora for code-mixed speech recognition, Proc. Interspeech 2019\n(2019) 2150–2154.\n[113] Y. Long, Y. Li, Q. Zhang, S. Wei, H. Ye, J. Yang, Acoustic data augmen-\ntation for mandarin-english code-switching speech recognition, Applied\nAcoustics 161 (2020) 107175.\n[114] E. Yılmaz, M. McLaren, H. van den Heuvel, D. A. van Leeuwen, Semi-\nsupervised acoustic model training for speech with code-switching, Speech\nCommunication 105 (2018) 12–22.\n[115] E. Yılmaz, S. Cohen, X. Yue, D. van Leeuwen, H. Li, Multi-graph decoding\nfor code-switching asr, arXiv preprint arXiv:1906.07523.\n[116] G. I. Winata, S. Cahyawijaya, Z. Lin, Z. Liu, P. Xu, P. Fung, Meta-\ntransfer learning for code-switched speech recognition, arXiv preprint\narXiv:2004.14228.\n[117] H. Zhang, H. Xu, V. T. Pham, H. Huang, E. S. Chng, Monolingual data se-\nlection analysis for english-mandarin hybrid code-switching speech recog-\nnition, arXiv preprint arXiv:2006.07094.\n[118] E. Yılmaz, H. van den Heuvel, D. van Leeuwen, Investigating bilingual\ndeep neural networks for automatic recognition of code-switching frisian\nspeech, Procedia Computer Science 81 (2016) 159–166.\n[119] E. Yilmaz, H. Heuvel, D. A. van Leeuwen, Exploiting untranscribed broad-\ncast data for improved code-switching detection.\n53\n[120] E. Yılmaz, H. v. d. Heuvel, D. A. van Leeuwen, Acoustic and textual data\naugmentation for improved asr of code-switching speech, arXiv preprint\narXiv:1807.10945.\n[121] A. Biswas, E. Yılmaz, F. de Wet, E. van der Westhuizen, T. Niesler, Semi-\nsupervised development of asr systems for multilingual code-switched\nspeech in under-resourced languages, arXiv preprint arXiv:2003.03135.\n[122] P. Guo, H. Xu, L. Xie, E. S. Chng, Study of semi-supervised approaches\nto improving english-mandarin code-switching speech recognition, arXiv\npreprint arXiv:1806.06200.\n[123] S. Nakayama, A. Tjandra, S. Sakti, S. Nakamura, Speech chain for semi-\nsupervised learning of japanese-english code-switching asr and tts, in:\n2018 IEEE Spoken Language Technology Workshop (SLT), IEEE, 2018,\npp. 182–189.\n[124] S. Nakayama, A. Tjandra, S. Sakti, S. Nakamura, Zero-shot code-\nswitching asr and tts with multilingual machine speech chain, in:\n2019 IEEE Automatic Speech Recognition and Understanding Workshop\n(ASRU), IEEE, 2019, pp. 964–971.\n[125] X. Song, Y. Liu, D. Yang, Y. Zou, A multi-task learning approach for\nmandarin-english code-switching conversational speech recognition, in: In-\nternational Symposium on Intelligence Computation and Applications,\nSpringer, 2017, pp. 102–111.\n[126] X. Song, Y. Zou, S. Huang, S. Chen, Y. Liu, Investigating multi-task\nlearning for automatic speech recognition with code-switching between\nmandarin and english, in: 2017 International Conference on Asian Lan-\nguage Processing (IALP), IEEE, 2017, pp. 27–30.\n[127] G. I. Winata, A. Madotto, C.-S. Wu, P. Fung, Towards end-to-end auto-\nmatic code-switching speech recognition, arXiv preprint arXiv:1810.12620.\n54\n[128] C. Shan, C. Weng, G. Wang, D. Su, M. Luo, D. Yu, L. Xie, Investigating\nend-to-end speech recognition for mandarin-english code-switching.\n[129] S. Nakayama, T. Kano, A. Tjandra, S. Sakti, S. Nakamura, Recognition\nand translation of code-switching speech utterances.\n[130] Y. Khassanov, H. Xu, V. T. Pham, Z. Zeng, E. S. Chng, C. Ni, B. Ma,\nConstrained output embeddings for end-to-end code-switching speech\nrecognition with only monolingual data, arXiv preprint arXiv:1904.03802.\n[131] K. Li, J. Li, G. Ye, R. Zhao, Y. Gong, Towards code-switching asr for end-\nto-end ctc models, in: ICASSP 2019-2019 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2019, pp.\n6076–6080.\n[132] S. Zhang, J. Yi, Z. Tian, J. Tao, Y. Bai, Rnn-transducer with language\nbias for end-to-end mandarin-english code-switching speech recognition,\narXiv preprint arXiv:2002.08126.\n[133] N. Luo, D. Jiang, S. Zhao, C. Gong, W. Zou, X. Li, Towards end-to-end\ncode-switching speech recognition, arXiv preprint arXiv:1810.13091.\n[134] Z. Zeng, Y. Khassanov, V. T. Pham, H. Xu, E. S. Chng, H. Li, On the end-\nto-end solution to mandarin-english code-switching speech recognition,\narXiv preprint arXiv:1811.00241.\n[135] K. Li, J. Li, G. Ye, R. Zhao, Y. Gong, Towards code-switching asr for end-\nto-end ctc models, in: ICASSP 2019-2019 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2019, pp.\n6076–6080.\n[136] S. Shah, B. Abraham, S. Sitaram, V. Joshi, et al., Learning to recognize\ncode-switched speech without forgetting monolingual speech recognition,\narXiv preprint arXiv:2006.00782.\n55\n[137] G. Reddy Madhumani, S. Shah, B. Abraham, V. Joshi, S. Sitaram, Learn-\ning not to discriminate: Task agnostic learning for improving monolingual\nand code-switched speech recognition, arXiv (2020) arXiv–2006.\n[138] B. M. L. Srivastava, S. Sitaram, Homophone Identiﬁcation and Merging\nfor Code-switched Speech Recognition, Proeedings of Interspeech 2018.\n[139] F. Weng, H. Bratt, L. Neumeyer, A. Stolcke, A study of multilingual\nspeech recognition, in: Fifth European Conference on Speech Communi-\ncation and Technology, 1997.\n[140] Y. Li, P. Fung, Code-switch language model with inversion constraints for\nmixed language speech recognition, Proceedings of COLING 2012 (2012)\n1671–1680.\n[141] Y. Li, P. Fung, Code switch language modeling with functional head con-\nstraint, in: 2014 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), IEEE, 2014, pp. 4913–4917.\n[142] Y. Li, P. Fung, Language modeling for mixed language speech recognition\nusing weighted phrase extraction., in: Interspeech, 2013, pp. 2599–2603.\n[143] A. Baheti, S. Sitaram, M. Choudhury, K. Bali, Curriculum design for\ncode-switching: Experiments with language identiﬁcation and language\nmodeling with deep neural networks, Proceedings of International Con-\nference on Natural Language Processing (ICON), 2017.\n[144] J. Gebhardt, Speech recognition on english-mandarin code-switching data\nusing factored language models, Ph.D. thesis, MS thesis, Department of\nInformatics, Karlsruhe Institute of Technology (2011).\n[145] H. Adel, N. T. Vu, F. Kraus, T. Schlippe, H. Li, T. Schultz, Recurrent neu-\nral network language modeling for code switching conversational speech,\nin: 2013 IEEE International Conference on Acoustics, Speech and Signal\nProcessing, IEEE, 2013, pp. 8411–8415.\n56\n[146] H. Adel, N. T. Vu, T. Schultz, Combination of recurrent neural networks\nand factored language models for code-switching language modeling, in:\nProceedings of the 51st Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 2: Short Papers), Vol. 2, 2013, pp. 206–211.\n[147] H. Adel, K. Kirchhoﬀ, D. Telaar, N. T. Vu, T. Schlippe, T. Schultz, Fea-\ntures for factored language models for code-switching speech, in: Spoken\nLanguage Technologies for Under-Resourced Languages, 2014.\n[148] N. T. Vu, T. Schultz, Exploration of the impact of maximum entropy\nin recurrent neural network language models for code-switching speech,\nin: Proceedings of The First Workshop on Computational Approaches to\nCode Switching, 2014, pp. 34–41.\n[149] H. Adel, N. T. Vu, K. Kirchhoﬀ, D. Telaar, T. Schultz, Syntactic and se-\nmantic features for code-switching factored language models, IEEE/ACM\ntransactions on audio, speech, and language Processing 23 (3) (2015) 431–\n440.\n[150] N. T. Vu, H. Adel, T. Schultz, An investigation of code-switching attitude\ndependent language modeling, in: International Conference on Statistical\nLanguage and Speech Processing, Springer, 2013, pp. 297–308.\n[151] E. van der Westhuizen, T. Niesler, Synthesising isizulu-english code-switch\nbigrams using word embeddings., in: INTERSPEECH, 2017, pp. 72–76.\n[152] S. Garg, T. Parekh, P. Jyothi, Code-switched language models using dual\nrnns and same-source pretraining, arXiv preprint arXiv:1809.01962.\n[153] F. Blaicher, Smt-based text generation for code-switching language mod-\nels, Ph.D. thesis, Masters thesis, Cognitive Systems Lab (CSL), Karlsruhe\nInsitutite of (2011).\n[154] G. I. Winata, A. Madotto, C.-S. Wu, P. Fung, Code-switched language\nmodels using neural based synthetic data from parallel sentences, arXiv\npreprint arXiv:1909.08582.\n57\n[155] C.-T. Chang, S.-P. Chuang, H.-Y. Lee, Code-switching sentence genera-\ntion by generative adversarial networks and its application to data aug-\nmentation, arXiv preprint arXiv:1811.02356.\n[156] K. Chandu, T. Manzini, S. Singh, A. W. Black, Language informed mod-\neling of code-switched text, in: Proceedings of the Third Workshop on\nComputational Approaches to Linguistic Code-Switching, 2018, pp. 92–\n97.\n[157] H. Gonen, Y. Goldberg, Language modeling for code-switching: Evalua-\ntion, integration of monolingual data, and discriminative training, arXiv\npreprint arXiv:1810.11895.\n[158] Z. Zeng, H. Xu, T. Y. Chong, E.-S. Chng, H. Li, Improving n-gram\nlanguage modeling for code-switching speech recognition, in: 2017 Asia-\nPaciﬁc Signal and Information Processing Association Annual Summit\nand Conference (APSIPA ASC), IEEE, 2017, pp. 1596–1601.\n[159] G. I. Winata, A. Madotto, C.-S. Wu, P. Fung, Code-switching lan-\nguage modeling using syntax-aware multi-task learning, arXiv preprint\narXiv:1805.12070.\n[160] G. Lee, H. Li, Modeling code-switch languages using bilingual parallel\ncorpus, in: Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, 2020, pp. 860–870.\n[161] P. E. Piccinini, M. Garellek, Prosodic cues to monolingual versus code-\nswitching sentences in english and spanish, in: Proceedings of the 7th\nSpeech Prosody Conference, 2014, pp. 885–889.\n[162] S. Rallabandi, A. W. Black, On building mixed lingual speech synthesis\nsystems, Proceedings of Interspeech.\n[163] E. Yılmaz, H. van den Heuvel, D. van Leeuwen, Code-switching detection\nusing multilingual dnns, in: 2016 IEEE Spoken Language Technology\nWorkshop (SLT), IEEE, 2016, pp. 610–616.\n58\n[164] E. Yılmaz, H. v. d. Heuvel, D. A. van Leeuwen, Code-switching detec-\ntion with data-augmented acoustic and language models, arXiv preprint\narXiv:1808.00521.\n[165] D.-C. Lyu, R.-Y. Lyu, C.-L. Zhu, M.-T. Ko, Language identiﬁcation in\ncode-switching speech using word-based lexical model, in: 2010 7th In-\nternational Symposium on Chinese Spoken Language Processing, IEEE,\n2010, pp. 460–464.\n[166] D.-C. Lyu, T.-P. Tan, E.-S. Chng, H. Li, An analysis of a mandarin-english\ncode-switching speech corpus: Seame, Age 21 (2010) 25–8.\n[167] K. R. Mabokela, M. J. Manamela, M. Manaileng, Modeling code-switching\nspeech on under-resourced languages for language identiﬁcation, in: Spo-\nken Language Technologies for Under-Resourced Languages, 2014.\n[168] L. M. Tomokiyo, A. W. Black, K. A. Lenzo, Foreign accents in synthetic\nspeech: Development and Evaluation, in: Ninth European Conference on\nSpeech Communication and Technology, 2005.\n[169] N. Campbell, Talking foreign - Concatenative Speech Synthesis and the\nLanguage Barrier, in: Seventh European Conference on Speech Commu-\nnication and Technology, 2001.\n[170] L. Badino, C. Barolo, S. Quazza, Language independent phoneme map-\nping for foreign TTS, in: Fifth ISCA Workshop on Speech Synthesis, 2004.\n[171] M. Mashimo, T. Toda, K. Shikano, N. Campbell, Evaluation of cross-\nlanguage voice conversion based on GMM and STRAIGHT.\n[172] J. Latorre, K. Iwano, S. Furui, Polyglot synthesis using a mixture of Mono-\nlingual corpora, in: International Conference on Acoustics, Speech, and\nSignal Processing, 2005. Proceedings(ICASSP), IEEE, 2005.\n[173] S. Sitaram, S. K. Rallabandi, S. Black, Experiments with cross-lingual\nsystems for synthesis of Code-Mixed text, in: 9th ISCA Speech Synthesis\nWorkshop, 2017.\n59\n[174] N. K. Elluru, A. Vadapalli, R. Elluru, H. Murthy, K. Prahallad, Is word-\nto-phone mapping better than phone-phone mapping for handling English\nwords?, in: Proceedings of the 51st Annual Meeting of the Association for\nComputational Linguistics, 2013.\n[175] S. K. Rallabandi, A. Vadapalli, S. Achanta, S. Gangashetty, IIIT Hy-\nderabad‘s submission to the Blizzard Challenge 2015, in: Proceedings of\nBlizzard Challenge 2015, ISCA, 2015.\n[176] A. L. Thomas, A. Prakash, A. Baby, H. A. Murthy, Code-switching in\nindic speech synthesisers., in: Interspeech, 2018, pp. 1948–1952.\n[177] Y. Cao, X. Wu, S. Liu, J. Yu, X. Li, Z. Wu, X. Liu, H. Meng, End-to-\nend code-switched tts with mix of monolingual recordings, in: ICASSP\n2019-2019 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), IEEE, 2019, pp. 6935–6939.\n[178] Y. Cao, S. Liu, X. Wu, S. Kang, P. Liu, Z. Wu, X. Liu, D. Su, D. Yu,\nH. Meng, Code-switched speech synthesis using bilingual phonetic poste-\nriorgram with only monolingual corpora, in: ICASSP 2020-2020 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP), IEEE, 2020, pp. 7619–7623.\n[179] X. Zhou, E. Yılmaz, Y. Long, Y. Li, H. Li, Multi-encoder-decoder\ntransformer\nfor\ncode-switching\nspeech\nrecognition,\narXiv\npreprint\narXiv:2006.10414.\n[180] C. Lignos, M. Marcus, Toward web-scale analysis of codeswitching, in:\n87th Annual Meeting of the Linguistic Society of America, 2013.\n[181] G. Chittaranjan, Y. Vyas, K. Bali, M. Choudhury, Word level Language\nIdentiﬁcation using CRF: Code-Switching shared task report of MSR In-\ndia system, in: Proceedings of The First Workshop on Computational\nApproaches to Code Switching, 2014, pp. 73–79.\n60\n[182] M. X. Xia, Codeswitching language identiﬁcation using subword informa-\ntion enriched word vectors, in: Proceedings of The Second Workshop on\nComputational Approaches to Code Switching, 2016, pp. 132–136.\n[183] D. Nguyen, L. Cornips, Automatic detection of intra-word code-switching,\nin: Proceedings of the 14th SIGMORPHON Workshop on Computational\nResearch in Phonetics, Phonology, and Morphology, 2016, pp. 82–86.\n[184] A. Chanda, D. Das, C. Mazumdar, Unraveling the english-bengali code-\nmixing phenomenon, in: Proceedings of the Second Workshop on Com-\nputational Approaches to Code Switching, 2016, pp. 80–89.\n[185] Y.-L. Yeong, T.-P. Tan, Language identiﬁcation of code switching malay-\nenglish words using syllable structure information, in: Spoken Languages\nTechnologies for Under-Resourced Languages, 2010.\n[186] H. Elfardy, M. Diab, Token level identiﬁcation of linguistic code switching,\nin: Proceedings of COLING 2012: Posters, 2012, pp. 287–296.\n[187] Y.-L. Yeong, T.-P. Tan, Applying grapheme, word, and syllable infor-\nmation for language identiﬁcation in code switching sentences, in: 2011\nInternational Conference on Asian Language Processing, IEEE, 2011, pp.\n111–114.\n[188] Y.-L. Yeong, T.-P. Tan, Language identiﬁcation of code switching sen-\ntences and multilingual sentences of under-resourced languages by using\nmulti structural word information, in: Fifteenth Annual Conference of the\nInternational Speech Communication Association, 2014.\n[189] R. Shirvani, M. Piergallini, G. S. Gautam, M. Chouikha, The howard\nuniversity system submission for the shared task in language identiﬁcation\nin spanish-english codeswitching, in: Proceedings of the second workshop\non computational approaches to code switching, 2016, pp. 116–120.\n61\n[190] N. Dongen, Analysis and prediction of dutch-english code-switching in\ndutch social media messages, Master’s thesis, Universiteit van Amsterdam,\nAmsterdam, Netherlands.\n[191] P. Shrestha, Codeswitching detection via lexical features in conditional\nrandom ﬁelds, in: Proceedings of the Second Workshop on Computational\nApproaches to Code Switching, 2016, pp. 121–126.\n[192] Y. Samih, W. Maier, Detecting code-switching in moroccan arabic social\nmedia, SocialNLP@ IJCAI-2016, New York.\n[193] Y. Samih, S. Maharjan, M. Attia, L. Kallmeyer, T. Solorio, Multilingual\ncode-switching identiﬁcation via lstm recurrent neural networks, in: Pro-\nceedings of the Second Workshop on Computational Approaches to Code\nSwitching, 2016, pp. 50–59.\n[194] U. K. Sikdar, B. Gamb¨ack, Language identiﬁcation in code-switched text\nusing conditional random ﬁelds and babelnet, in: Proceedings of the Sec-\nond Workshop on Computational Approaches to Code Switching, 2016,\npp. 127–131.\n[195] A. Jaech, G. Mulcaire, M. Ostendorf, N. A. Smith, A neural model for\nlanguage identiﬁcation in code-switched tweets, in: Proceedings of The\nSecond Workshop on Computational Approaches to Code Switching, 2016,\npp. 60–64.\n[196] J. C. Chang, C.-C. Lin, Recurrent-neural-network for language detection\non twitter code-switching corpus, arXiv preprint arXiv:1412.4314.\n[197] N. Jain, R. A. Bhat, Language identiﬁcation in code-switching scenario,\nin: Proceedings of the First Workshop on Computational Approaches to\nCode Switching, 2014, pp. 87–93.\n[198] A. Chanda, D. Das, C. Mazumdar, Columbia-jadavpur submission for\nemnlp 2016 code-switching workshop shared task: System description, in:\n62\nProceedings of the Second Workshop on Computational Approaches to\nCode Switching, 2016, pp. 112–115.\n[199] H. Jhamtani, S. K. Bhogi, V. Raychoudhury, Word-level Language Iden-\ntiﬁcation in Bi-lingual code-switched texts, in: Proceedings of the 28th\nPaciﬁc Asia Conference on Language, Information and Computing, 2014.\n[200] B. King, S. Abney, Labeling the languages of words in Mixed-Language\ndocuments using weakly supervised methods, in: Proceedings of the Con-\nference of the North American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, 2013.\n[201] M. Z. Ansari, S. Khan, T. Amani, A. Hamid, S. Rizvi, Analysis of part of\nspeech tags in language identiﬁcation of code-mixed text, in: Advances in\nComputing and Intelligent Systems, Springer, 2020, pp. 417–425.\n[202] M. Attia, Y. Samih, W. Maier, Ghht at calcs 2018: Named Entity Recog-\nnition for Dialectal Arabic using Neural Networks, in: Proceedings of\nthe Third Workshop on Computational Approaches to Linguistic Code-\nSwitching, 2018.\n[203] P. Geetha, K. Chandu, A. W. Black, Tackling Code-Switched NER: Par-\nticipation of CMU, in: Proceedings of the Third Workshop on Computa-\ntional Approaches to Linguistic Code-Switching, 2018.\n[204] G. Aguilar, S. Maharjan, A. P. L. Monroy, T. Solorio, A Multi-task ap-\nproach for Named Entity Recognition in Social Media data, in: Proceed-\nings of the 3rd Workshop on Noisy User-generated Text, 2017.\n[205] A. Zirikly, M. Diab, Named Entity Recognition for Arabic Social Media,\nin: Proceedings of the 1st Workshop on Vector Space Modeling for Natural\nLanguage Processing, 2015.\n[206] J. L. C. Zea, J. E. O. Luna, C. Thorne, G. Glavaˇs, Spanish NER with\nWord Representations and Conditional Random Fields, in: Proceedings\nof the Sixth Named Entity Workshop, 2016.\n63\n[207] D. Etter, F. Ferraro, R. Cotterell, O. Buzek, B. Van Durme, Nerit: Named\nEntity Recognition for Informal Text.\n[208] C. Sabty, A. Sherif, M. Elmahdy, S. Abdennadher, Techniques for named\nentity recognition on arabic-english code-mixed data.\n[209] A. Akbik, D. Blythe, R. Vollgraf, Contextual string embeddings for se-\nquence labeling, in: COLING 2018, 27th International Conference on\nComputational Linguistics, 2018, pp. 1638–1649.\n[210] G. I. Winata, C.-S. Wu, A. Madotto, P. Fung, Bilingual character rep-\nresentation for eﬃciently addressing out-of-vocabulary words in code-\nswitching named entity recognition, arXiv preprint arXiv:1805.12061.\n[211] C. Wang, K. Cho, D. Kiela, Code-switched named entity recognition with\nembedding attention, in: Proceedings of the Third Workshop on Compu-\ntational Approaches to Linguistic Code-Switching, 2018, pp. 154–158.\n[212] G. I. Winata, Z. Lin, P. Fung, Learning multilingual meta-embeddings\nfor code-switching named entity recognition, in: Proceedings of the 4th\nWorkshop on Representation Learning for NLP (RepL4NLP-2019), 2019,\npp. 181–186.\n[213] G. I. Winata, Z. Lin, J. Shin, Z. Liu, P. Fung, Hierarchical meta-\nembeddings for code-switching named entity recognition, arXiv preprint\narXiv:1909.08504.\n[214] U. Barman, J. Wagner, J. Foster, Part-of-speech tagging of code-mixed\nsocial media content: Pipeline, stacking and joint modelling, in: Pro-\nceedings of the Second Workshop on Computational Approaches to Code\nSwitching, 2016, pp. 30–39.\n[215] R. van der Goot, ¨O. C¸etino˘glu, Lexical normalization for code-switched\ndata and its eﬀect on pos-tagging, arXiv preprint arXiv:2006.01175.\n64\n[216] P. Goyal, M. R. Mital, A. Mukerjee, A Bilingual Parser for Hindi, En-\nglish and code-switching structures, in: 10th Conference of The European\nChapter, 2003.\n[217] D. Sharma, K. Vikram, M. R. Mital, A. Mukerjee, A. M. Raina,\nSaarthaka- An Integrated Discourse Semantic Model for Bilingual Cor-\npora, in: Proceedings of International Conference on Universal Knowledge\nand Language, 2002.\n[218] I. A. Bhat, R. A. Bhat, M. Shrivastava, D. M. Sharma, Univer-\nsal dependency parsing for hindi-english code-switching, arXiv preprint\narXiv:1804.05868.\n[219] X. Li, D. Roth, Learning Question Classiﬁers, in: Proceedings of the 19th\ninternational conference on Computational linguistics-Volume 1, Associa-\ntion for Computational Linguistics, 2002.\n[220] K. R. Chandu, M. Chinnakotla, A. W. Black, M. Shrivastava, Webshodh:\nA Code Mixed Factoid Question Answering System for Web, in: Interna-\ntional Conference of the Cross-Language Evaluation Forum for European\nLanguages, Springer, 2017.\n[221] S. Sekine, R. Grishman, Hindi-English cross-lingual Question-Answering\nSystem, ACM Transactions on Asian Language Information Processing\n(TALIP) 2003.\n[222] P. Pingali, J. Jagarlamudi, V. Varma, A Dictionary based approach with\nQuery Expansion to Cross Language Query based Multi-Document Sum-\nmarization: Experiments in Telugu-English, Citeseer 2008.\n[223] G. Neumann, B. Sacaleanu, A Cross Language Question/Answering Sys-\ntem for German and English.\n[224] B. Magnini, S. Romagnoli, A. Vallin, J. Herrera, A. Pe˜nas, V. Peinado,\nF. Verdejo, M. de Rijke, The multiple Language Question Answering\nTrack, 2003.\n65\n[225] D. Vilares, M. A. Alonso, C. G´omez-Rodr´ıguez, En-es-cs: An english-\nspanish code-switching twitter corpus for multilingual sentiment analysis,\nin: Proceedings of the Tenth International Conference on Language Re-\nsources and Evaluation (LREC’16), 2016, pp. 4149–4153.\n[226] D. Vilares, M. A. Alonso, C. G´omez-Rodr´ıguez, Sentiment analysis on\nmonolingual, multilingual and code-switching twitter corpora, in: Pro-\nceedings of the 6th Workshop on Computational Approaches to Subjec-\ntivity, Sentiment and Social Media Analysis, 2015, pp. 2–8.\n[227] D. Vilares, M. A. Alonso, C. G´omez-Rodr´ıguez, Supervised sentiment\nanalysis in multilingual environments, Information Processing & Manage-\nment 53 (3) (2017) 595–607.\n[228] S. Padmaja, S. Fatima, S. Bandu, M. Nikitha, K. Prathyusha, Sentiment\nextraction from bilingual code mixed social media text, in: Data Engi-\nneering and Communication Technology, Springer, 2020, pp. 707–714.\n[229] B. G. Patra, D. Das, A. Das, Sentiment analysis of code-mixed indian\nlanguages: An overview of sail code-mixed shared task@ icon-2017, arXiv\npreprint arXiv:1803.06745.\n[230] S. Mandal, D. Das, Analyzing roles of classiﬁers and code-mixed factors\nfor sentiment identiﬁcation, arXiv preprint arXiv:1801.02581.\n[231] Y. K. Lal, V. Kumar, M. Dhar, M. Shrivastava, P. Koehn, De-mixing sen-\ntiment from code-mixed text, in: Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics: Student Research\nWorkshop, 2019, pp. 371–377.\n[232] J. Utsav, D. Kabaria, R. Vajpeyi, M. Mina, V. Srivastava, Stance detec-\ntion in hindi-english code-mixed data, in: Proceedings of the 7th ACM\nIKDD CoDS and 25th COMAD, 2020, pp. 359–360.\n66\n[233] S. Lee, Z. Wang, Emotion in code-switching texts: Corpus construction\nand analysis, in: Proceedings of the eighth SIGHAN workshop on Chinese\nlanguage processing, 2015, pp. 91–99.\n[234] Z. Wang, S. Lee, S. Li, G. Zhou, Emotion detection in code-switching\ntexts via bilingual and sentimental information, in: Proceedings of the\n53rd Annual Meeting of the Association for Computational Linguistics and\nthe 7th International Joint Conference on Natural Language Processing\n(Volume 2: Short Papers), 2015, pp. 763–768.\n[235] Z. Wang, S. Y. M. Lee, S. Li, G. Zhou, Emotion analysis in code-switching\ntext with joint factor graph model, IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing 25 (3) (2016) 469–480.\n[236] C. Supraja, V. M. Rao, Emotion detection in code-switching text, Emo-\ntion.\n[237] K. Rajput, R. Kapoor, P. Mathur, P. Kumaraguru, R. R. Shah, et al.,\nTransfer learning for detecting hateful sentiments in code switched lan-\nguage, in:\nDeep Learning-Based Approaches for Sentiment Analysis,\nSpringer, 2020, pp. 159–192.\n[238] A. Khandelwal, N. Kumar, A uniﬁed system for aggression identiﬁcation\nin english code-mixed and uni-lingual texts, in: Proceedings of the 7th\nACM IKDD CoDS and 25th COMAD, 2020, pp. 55–64.\n[239] T. Santosh, K. Aravind, Hate speech detection in hindi-english code-mixed\nsocial media text, in: Proceedings of the ACM India Joint International\nConference on Data Science and Management of Data, 2019, pp. 310–313.\n[240] R. M. K. Sinha, A. Thakur, Machine Translation of Bilingual Hindi-\nEnglish(Hinglish) Text, 10th Machine Translation summit (MT Summit\nX), Phuket, Thailand.\n[241] M. Johnson, M. Schuster, Q. V. Le, M. Krikun, Y. Wu, Z. Chen, N. Tho-\nrat, F. Vi´egas, M. Wattenberg, G. Corrado, et al., Googles multilingual\n67\nneural machine translation system: Enabling zero-shot translation, Trans-\nactions of the Association for Computational Linguistics 5 (2017) 339–351.\n[242] P. Rao, M. Pandya, K. Sabu, K. Kumar, N. Bondale, A study of lexical and\nprosodic cues to segmentation in a hindi-english code-switched discourse,\nProc. Interspeech 2018 (2018) 1918–1922.\n[243] A. Bawa, M. Choudhury, K. Bali, Accommodation of conversational code-\nchoice, in: Proceedings of the Third Workshop on Computational Ap-\nproaches to Linguistic Code-Switching, 2018, pp. 82–91.\n[244] E. Ahn, C. Jimenez, Y. Tsvetkov, A. Black, What code-switching strate-\ngies are eﬀective in dialogue systems?, Proceedings of the Society for Com-\nputation in Linguistics 3 (1) (2020) 308–318.\n[245] R. Reddy, N. Reddy, S. Bandyopadhyay, Dialogue based Question Answer-\ning System in Telugu, in: Proceedings of the Workshop on Multilingual\nQuestion Answering-MLQA, 2006.\n[246] S. Banerjee, N. Moghe, S. Arora, M. M. Khapra, A dataset for building\ncode-mixed goal oriented conversation systems, COLING.\n[247] V. Ramanarayanan, R. Pugh, Y. Qian, D. Suendermann-Oeft, Automatic\nturn-level language identiﬁcation for code-switched spanish–english dialog,\nin: 9th International Workshop on Spoken Dialogue System Technology,\nSpringer, 2019, pp. 51–61.\n[248] A. Srivastava, K. Bali, M. Choudhury, Understanding script-mixing: A\ncase study of hindi-english bilingual twitter users, in:\nProceedings of\nthe The 4th Workshop on Computational Approaches to Code Switch-\ning, 2020, pp. 36–44.\n[249] D. Garrette, H. Alpert-Abrams, T. Berg-Kirkpatrick, D. Klein, Unsuper-\nvised code-switching for multilingual historical document transcription,\n68\nin: Proceedings of the 2015 Conference of the North American Chap-\nter of the Association for Computational Linguistics: Human Language\nTechnologies, 2015, pp. 1036–1041.\n[250] L. Qin, M. Ni, Y. Zhang, W. Che, Cosda-ml:\nMulti-lingual code-\nswitching data augmentation for zero-shot cross-lingual nlp,\narXiv\npreprint arXiv:2006.06402.\n[251] A. R. KhudaBukhsh,\nS. Palakodety,\nJ. G. Carbonell,\nHarnessing\ncode switching to transcend the linguistic barrier,\narXiv preprint\narXiv:2001.11258.\n[252] J. Yang, S. Ma, D. Zhang, S. Wu, Z. Li, M. Zhou, Alternating language\nmodeling for cross-lingual pre-training., in: AAAI, 2020, pp. 9386–9393.\n[253] A. Conneau, G. Lample, R. Rinott, A. Williams, S. R. Bowman,\nH. Schwenk, V. Stoyanov, Xnli: Evaluating cross-lingual sentence rep-\nresentations, arXiv preprint arXiv:1809.05053.\n[254] M. Diab, J. Hirschberg, P. Fung, T. Solorio, Proceedings of the ﬁrst work-\nshop on computational approaches to code switching, in: Proceedings of\nthe First Workshop on Computational Approaches to Code Switching,\n2014.\n[255] G. Molina, F. AlGhamdi, M. Ghoneim, A. Hawwari, N. Rey-Villamizar,\nM. Diab, T. Solorio, Overview for the second shared task on language\nidentiﬁcation in code-switched data, arXiv preprint arXiv:1909.13016.\n[256] R. S. Roy, M. Choudhury, P. Majumder, K. Agarwal, Overview of the ﬁre\n2013 track on transliterated search, in: Post-Proceedings of the 4th and\n5th Workshops of the Forum for Information Retrieval Evaluation, 2013,\npp. 1–7.\n[257] P. R. Rao, S. L. Devi, Cmee-il: Code mix entity extraction in indian lan-\nguages from social media text@ ﬁre 2016-an overview., in: FIRE (Working\nNotes), 2016, pp. 289–295.\n69\n[258] R. Sequiera, M. Choudhury, P. Gupta, P. Rosso, S. Kumar, S. Banerjee,\nS. K. Naskar, S. Bandyopadhyay, G. Chittaranjan, A. Das, et al., Overview\nof ﬁre-2015 shared task on mixed script information retrieval., in: FIRE\nWorkshops, Vol. 1587, 2015, pp. 19–25.\n[259] S. Banerjee, K. Chakma, S. K. Naskar, A. Das, P. Rosso, S. Bandy-\nopadhyay, M. Choudhury, Overview of the mixed script information re-\ntrieval (msir) at ﬁre-2016, in: Forum for Information Retrieval Evaluation,\nSpringer, 2016, pp. 39–49.\n[260] A. Jamatia, A. Das, Task report: Tool contest on pos tagging for code-\nmixed indian social media (facebook, twitter, and whatsapp) text@ icon\n2016., Proceedings of ICON.\n[261] K. Prahallad, A. Vadapalli, S. Kesiraju, H. Murthy, S. Lata, T. Nagarajan,\nM. Prasanna, H. Patil, A. Sao, S. King, et al., The blizzard challenge 2014,\nin: Proc. Blizzard Challenge workshop, Vol. 2014, 2014.\n[262] X. Shi, Q. Feng, L. Xie, The asru 2019 mandarin-english code-switching\nspeech recognition challenge: Open datasets, tracks, methods and results,\narXiv preprint arXiv:2007.05916.\n[263] S. Khanuja, S. Dandapat, A. Srinivasan, S. Sitaram, M. Choudhury,\nGluecos: An evaluation benchmark for code-switched nlp, arXiv preprint\narXiv:2004.12376.\n[264] G. Aguilar, S. Kar, T. Solorio, Lince: A centralized benchmark for lin-\nguistic code-switching evaluation, in: Proceedings of The 12th Language\nResources and Evaluation Conference, 2020, pp. 1803–1813.\n70\n",
  "categories": [
    "cs.CL",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-03-25",
  "updated": "2020-07-22"
}