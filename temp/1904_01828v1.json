{
  "id": "http://arxiv.org/abs/1904.01828v1",
  "title": "Unsupervised Deep Tracking",
  "authors": [
    "Ning Wang",
    "Yibing Song",
    "Chao Ma",
    "Wengang Zhou",
    "Wei Liu",
    "Houqiang Li"
  ],
  "abstract": "We propose an unsupervised visual tracking method in this paper. Different\nfrom existing approaches using extensive annotated data for supervised\nlearning, our CNN model is trained on large-scale unlabeled videos in an\nunsupervised manner. Our motivation is that a robust tracker should be\neffective in both the forward and backward predictions (i.e., the tracker can\nforward localize the target object in successive frames and backtrace to its\ninitial position in the first frame). We build our framework on a Siamese\ncorrelation filter network, which is trained using unlabeled raw videos.\nMeanwhile, we propose a multiple-frame validation method and a cost-sensitive\nloss to facilitate unsupervised learning. Without bells and whistles, the\nproposed unsupervised tracker achieves the baseline accuracy of fully\nsupervised trackers, which require complete and accurate labels during\ntraining. Furthermore, unsupervised framework exhibits a potential in\nleveraging unlabeled or weakly labeled data to further improve the tracking\naccuracy.",
  "text": "Unsupervised Deep Tracking\nNing Wang1\nYibing Song2∗Chao Ma3\nWengang Zhou1\nWei Liu2∗\nHouqiang Li1\n1 CAS Key Laboratory of GIPAS, University of Science and Technology of China\n2 Tencent AI Lab\n3 MoE Key Lab of Artiﬁcial Intelligence, AI Institute, Shanghai Jiao Tong University\nwn6149@mail.ustc.edu.cn, dynamicstevenson@gmail.com, chaoma@sjtu.edu.cn\nzhwg@ustc.edu.cn, wl2223@columbia.edu, lihq@ustc.edu.cn\nAbstract\nWe propose an unsupervised visual tracking method in\nthis paper.\nDifferent from existing approaches using ex-\ntensive annotated data for supervised learning, our CNN\nmodel is trained on large-scale unlabeled videos in an un-\nsupervised manner. Our motivation is that a robust tracker\nshould be effective in both the forward and backward pre-\ndictions (i.e., the tracker can forward localize the target ob-\nject in successive frames and backtrace to its initial position\nin the ﬁrst frame). We build our framework on a Siamese\ncorrelation ﬁlter network, which is trained using unlabeled\nraw videos. Meanwhile, we propose a multiple-frame val-\nidation method and a cost-sensitive loss to facilitate unsu-\npervised learning. Without bells and whistles, the proposed\nunsupervised tracker achieves the baseline accuracy of fully\nsupervised trackers, which require complete and accurate\nlabels during training. Furthermore, unsupervised frame-\nwork exhibits a potential in leveraging unlabeled or weakly\nlabeled data to further improve the tracking accuracy.\n1. Introduction\nVisual tracking is a fundamental task in computer vision,\nwhich aims to localize the target object in the video given\na bounding box annotation in the ﬁrst frame. The state-\nof-the-art deep tracking methods [1, 46, 15, 55, 27, 60, 58,\n54, 4, 19, 33, 34] typically use pretrained CNN models for\nfeature extraction. These models are trained in a supervised\nmanner, requiring a large quantity of annotated ground-truth\nlabels. Manual annotations are always expensive and time-\nconsuming, whereas extensive unlabeled videos are readily\navailable on the Internet. It deserves to investigate how to\nexploit unlabeled video sequences for visual tracking.\n∗Y. Song and W. Liu are the corresponding authors. This work is done\nwhen N. Wang is an intern in Tencent AI Lab. The source code and results\nare available at https://github.com/594422814/UDT.\nSupervised Training:\nAnnotated sequences      \nForward tracking       \nUnsupervised Training:\nUnlabeled sequences      \nForward and Backward tracking      \nFigure 1. The comparison between supervised and unsupervised\nlearning. Visual tracking methods via supervised learning require\nground-truth labels for every frame of the training videos. By uti-\nlizing the forward tracking and backward veriﬁcation, we train the\nunsupervised tracker without heavyweight annotations.\nIn this paper, we propose to learn a visual tracking model\nfrom scratch via unsupervised learning. Our intuition re-\nsides on the observation that visual tracking can be per-\nformed in both the forward and backward manners. Ini-\ntially, given the target object annotated on the ﬁrst frame, we\ncan track the target object forward in the subsequent frames.\nWhen tracking backward, we use the predicted location in\nthe last frame as the initial target annotation and track it\nbackward towards the ﬁrst frame. The estimated target lo-\ncation in the ﬁrst frame via backward tracking is expected to\nbe identical with the initial annotation. After measuring the\ndifference between the forward and backward target trajec-\ntories, our network is trained in an unsupervised manner1 by\nconsidering the trajectory consistency as shown in Fig. 1.\nThrough exploiting consecutive frames in unlabeled videos,\nour model learns to locate targets by repeatedly performing\nforward tracking and backward veriﬁcation.\nThe proposed unsupervised learning scheme aims to\nacquire a generic feature representation, while not being\n1In this paper, we do not distinguish between the term unsupervised and\nself-supervised, as both refer to learning without ground-truth annotations.\narXiv:1904.01828v1  [cs.CV]  3 Apr 2019\nstrictly required to track a complete object. For a video se-\nquence, we randomly initialize a bounding box in the ﬁrst\nframe, which may not cover an entire object. Then, the\nproposed model learns to track the bounding box region in\nthe following sequences. This tracking strategy shares sim-\nilarity with the part-based [30] or edge-based [28] tracking\nmethods that focus on tracking the subregions of the target\nobjects. As the visual object tracker is not expected to only\nconcentrate on the complete objects, we use the randomly\ncropped bounding boxes for tracking initialization during\ntraining.\nWe integrate the proposed unsupervised learning into the\nSiamese based correlation ﬁlter framework [54]. The pro-\nposed network consists of two steps in the training process:\nforward tracking and backward veriﬁcation. We notice that\nthe backward veriﬁcation is not always effective since the\ntracker may successfully return to the initial target location\nfrom a deﬂected or false position. In addition, challenges\nsuch as heavy occlusion in unlabeled videos will further de-\ngrade the network representation capability. To tackle these\nissues, we propose multiple frames validation and a cost-\nsensitive loss to beneﬁt the unsupervised training. The mul-\ntiple frames validation increases the discrepancy between\nthe forward and backward trajectories to reduce veriﬁcation\nfailures. Meanwhile, the cost-sensitive loss mitigates the\ninterference from noisy samples during training.\nThe proposed unsupervised tracker is shown effective on\nthe benchmark datasets. Extensive experimental results in-\ndicate that without bells and whistles, the proposed unsu-\npervised tracker achieves comparable performance with the\nbaseline fully supervised trackers [1, 49, 54]. When inte-\ngrated with additional improvements such as the adaptive\nonline model update [9, 7], the proposed tracker exhibits\nstate-of-the-art performance. It is worth mentioning that\nthe unsupervised framework shows potential in exploiting\nunlabeled Internet videos to learn good feature representa-\ntions for tracking scenarios. Given limited or noisy labels,\nthe unsupervised method exhibits comparable results with\nthe corresponding supervised framework. In addition, we\nfurther improve the tracking accuracy by using more unla-\nbeled data. Sec. 4.2 shows a complete analysis of different\ntraining conﬁgurations.\nIn summary, the contributions of our work are three-fold:\n• We propose an unsupervised tracking method based\non the Siamese correlation ﬁlter backbone, which is\nlearned via forward and backward tracking.\n• We propose a multiple-frame validation method and a\ncost-sensitive loss to improve the unsupervised learn-\ning performance.\n• The extensive experiments on the standard bench-\nmarks show the favorable performance of the proposed\nmethod and reveal the potential of unsupervised learn-\ning in visual tracking.\n2. Related Work\nIn this section, we perform a literature review on the\ndeep tracking methods, forward-backward trajectory anal-\nysis, and unsupervised representation learning.\nDeep Visual Tracking.\nExisting deep tracking meth-\nods either ofﬂine learn a speciﬁc CNN model for online\ntracking or simply utilize off-the-shelf deep models (e.g.,\nVGG [43, 3]) for feature extraction. The Siamese trackers\n[1, 46, 49, 54, 55, 15, 27, 60, 58] formulate the tracking\ntask as a similarity matching process. They typically ofﬂine\nlearn a tracking network and do not ﬁne-tune the model on-\nline. On the other hand, some trackers adopt off-the-shelf\nCNN models as the feature extraction backbone. They in-\ncrementally train binary classiﬁcation layers [37, 45, 39] or\nregression layers [44, 31] based on the initial frame. These\nmethods typically achieve high accuracy while consuming\na huge computational cost. The Discriminative Correlation\nFilter (DCF) based trackers [2, 16, 8, 30, 5, 52, 18] tackle\nthe tracking task by solving a ridge regression problem us-\ning densely sampled candidates, which also beneﬁt from the\npowerful off-the-shelf deep features (e.g., [35, 40, 53, 7]).\nThe main distinction is that deep DCF trackers merely uti-\nlize off-the-shelf models for feature extraction and do not\nonline train additional layers or ﬁne-tune the CNN models.\nDifferent from the above deep trackers using off-the-shelf\nmodels or supervised learning, the proposed method trains\na network from scratch using unlabeled data in the wild.\nForward-Backward Analysis. The forward-backward tra-\njectory analysis has been widely explored in the liter-\nature.\nThe tracking-learning-detection (TLD) [20] uses\nthe Kanade-Lucas-Tomasi (KLT) tracker [47] to perform\nforward-backward matching to detect tracking failures. Lee\net al. [25] proposed to select the reliable base tracker by\ncomparing the geometric similarity, cyclic weight, and ap-\npearance consistency between a pair of forward-backward\ntrajectories. However, these methods rely on empirical met-\nrics to identify the target trajectories. In addition, repeat-\nedly performing forward and backward tracking brings in a\nheavy computational cost for online tracking. Differently,\nin TrackingNet [36], forward-backward tracking is used for\ndata annotation and tracker evaluation. In this work, we\nrevisit this scheme to train a deep visual tracker in an unsu-\npervised manner.\nUnsupervised Representation Learning. Our framework\nrelates to the unsupervised representation learning. In [26],\nthe feature representation is learned by sorting sequences.\nThe multi-layer auto-encoder on large-scale unlabeled data\nhas been explored in [24]. Vondrick et al. [50] proposed\nto anticipate the visual representation of frames in the fu-\nture. Wang and Gupta [56] used the KCF tracker [16] to pre-\nprocess the raw videos, and then selected a pair of tracked\nimages together with another random patch for learning\nCNNs using a ranking loss. Our method differs from [56] in\n(b)  Unsupervised Learning Pipeline using a Siamese Network\nCNN\nSearch\nSearch\nTemplate\nTemplate\nPseudo Label\nForward  Tracking\nBackward  Tracking\nResponse\nResponse\nInitial Label\nCNN\nCNN\nCNN\nCorrelation \nFilter\nCorrelation \nFilter\n(a)  Unsupervised Learning Motivation\nTemplate Patch Search Patch\nSearch Patch Template Patch\nForward Tracking\nusing \nInitial Label\nBackward Tracking\nusing \nPseudo Label\nConsistency Loss \nComputation\n#1\n#2\n#1\n#2\n#1\n#2\n#1\n#2\n#2\n#1\nConsistency \nLoss\nFeature\nFigure 2. An overview of unsupervised deep tracking. We show our motivation in (a) that we track forward and backward to compute the\nconsistency loss for network training. The detailed training procedure is shown in (b), where unsupervised learning is integrated into a\nSiamese correlation ﬁlter network. Note that during online tracking, we only track forward to predict the target location.\ntwo aspects. First, we integrate the tracking algorithm into\nunsupervised training instead of merely utilizing an off-the-\nshelf tracker as the data pre-processing tool. Second, our\nunsupervised framework is coupled with a tracking objec-\ntive function, so the learned feature representation is effec-\ntive in presenting the generic target objects. In the visual\ntracking community, unsupervised learning has rarely been\ntouched. To the best of our knowledge, the only related\nbut different approach is the auto-encoder based method\n[51]. However, the encoder-decoder is a general unsuper-\nvised framework [38], whereas our unsupervised method is\nspecially designed for tracking tasks.\n3. Proposed Method\nFig. 2(a) shows an example from the Butterﬂy sequence\nto illustrate forward and backward tracking. In practice, we\nrandomly draw bounding boxes in unlabeled videos to per-\nform forward and backward tracking. Given a randomly ini-\ntialized bounding box label, we ﬁrst track forward to predict\nits location in the subsequent frames. Then, we reverse the\nsequence and take the predicted bounding box in the last\nframe as the pseudo label to track backward. The predicted\nbounding box via backward tracking is expected to be iden-\ntical with the original bounding box in the ﬁrst frame. We\nmeasure the difference between the forward and backward\ntrajectories using the consistency loss for network training.\nAn overview of the proposed unsupervised Siamese corre-\nlation ﬁlter network is shown in Fig. 2(b). In the following,\nwe ﬁrst revisit the correlation ﬁlter based tracking frame-\nwork and then illustrate the details of our unsupervised deep\ntracking approach.\n3.1. Revisiting Correlation Tracking\nThe Discriminative Correlation Filters (DCFs) [2, 16]\nregress the input features of a search patch to a Gaussian\nresponse map for target localization. When training a DCF,\nwe select a template patch X with the ground-truth label Y.\nThe ﬁlter W can be learned by solving the ridge regression\nproblem as follows:\nmin\nW ∥W ∗X −Y∥2\n2 + λ∥W∥2\n2,\n(1)\nwhere λ is a regularization parameter and ∗denotes the cir-\ncular convolution. Eq. 1 can be efﬁciently calculated in the\nFourier domain [2, 8, 16] and the DCF can be computed by\nW = F −1\n\u0012\nF(X) ⊙F ⋆(Y)\nF ⋆(X) ⊙F(X) + λ\n\u0013\n,\n(2)\nwhere ⊙is the element-wise product, F(·) is the Discrete\nFourier Transform (DFT), F −1(·) is the inverse DFT, and\n⋆denotes the complex-conjugate operation. In each sub-\nsequent frame, given a search patch Z, the corresponding\nresponse map R can be computed in the Fourier domain:\nR = W ∗Z = F −1 (F ⋆(W) ⊙F(Z)) .\n(3)\nThe above DCF framework starts from learning a target\ntemplate W using the template patch X and then convolves\nW with a search patch Z to generate the response. Re-\ncently, the Siamese correlation ﬁlter network [49, 54] em-\nbeds the DCF in a Siamese framework and constructs two\nshared-weight branches as shown in Fig. 2(b). The ﬁrst one\nis the template branch which takes a template patch X as in-\nput and extracts its features to further generate a target tem-\nplate via DCF. The second one is the search branch which\ntakes a search patch Z as input for feature extraction. The\ntarget template is then convolved with the CNN features of\nthe search patch to generate the response map. The advan-\ntage of the Siamese DCF network is that both the feature\nextraction CNN and correlation ﬁlter are formulated into an\nend-to-end framework, so that the learned features are more\nrelated to the visual tracking scenarios.\n3.2. Unsupervised Learning Prototype\nGiven two consecutive frames P1 and P2, we crop the\ntemplate and search patches from them, respectively. By\nconducting forward tracking and backward veriﬁcation, the\nproposed framework does not require ground-truth labeling\nfor supervised training. The difference between the initial\nbounding box and the predicted bounding box in P1 will\nformulate a consistency loss for network learning.\nForward Tracking. We follow [54] to build a Siamese cor-\nrelation ﬁlter network to track the initial bounding box re-\ngion in frame P1. After cropping the template patch T from\nthe ﬁrst frame P1, the corresponding target template WT\ncan be computed as:\nWT = F −1\n\u0012\nF(ϕθ(T)) ⊙F ⋆(YT)\nF ⋆(ϕθ(T)) ⊙F(ϕθ(T)) + λ\n\u0013\n,\n(4)\nwhere ϕθ(·) denotes the CNN feature extraction operation\nwith trainable network parameters θ, and YT is the label of\nthe template patch T. This label is a Gaussian response cen-\ntered at the initial bounding box center. Once we obtain the\nlearned target template WT, the response map of a search\npatch S from frame P2 can be computed by\nRS = F −1(F ⋆(WT) ⊙F(ϕθ(S))).\n(5)\nIf the ground-truth Gaussian label of patch S is available,\nthe network ϕθ(·) can be trained by computing the L2 dis-\ntance between RS and the ground-truth. In the following,\nwe show how to train the network without labels by exploit-\ning backward trajectory veriﬁcation.\nBackward Tracking. After generating the response map\nRS for frame P2, we create a pseudo Gaussian label cen-\ntered at its maximum value, which is denoted by YS. In\nbackward tracking, we switch the role between the search\npatch and the template patch. By treating S as the template\npatch, we generate a target template WS using the pseudo\nlabel YS. The target template WS can be learned using\nEq. (4) by replacing T with S and replacing YT with YS.\nThen, we generate the response map RT through Eq. (5) by\nreplacing WT with WS and replacing S with T. Note that\nwe only use one Siamese correlation ﬁlter network to track\nforward and backward. The network parameters θ are ﬁxed\nduring the tracking steps.\nConsistency Loss Computation. After forward and back-\nward tracking, we obtain the response map RT. Ideally,\nRT should be a Gaussian label with the peak located at the\ninitial target position. In other words, RT should be as sim-\nilar as the originally given label YT. Therefore, the repre-\nsentation network ϕθ(·) can be trained in an unsupervised\nmanner by minimizing the reconstruction error as follows:\nLun = ∥RT −YT∥2\n2.\n(6)\nWe perform back-propagation of the computed loss to\nupdate the network parameters. During back-propagation,\nwe follow the Siamese correlation ﬁlter methods [54, 59] to\nupdate the network as:\n∂Lun\n∂ϕθ(T) = F −1\n\u0012\n∂Lun\n∂(F (ϕθ(T)))⋆+\n\u0012\n∂Lun\n∂(F (ϕθ(T)))\n\u0013⋆\u0013\n,\n∂Lun\n∂ϕθ(S) = F −1\n\u0012\n∂Lun\n∂(F (ϕθ(S)))⋆\n\u0013\n.\n(7)\n3.3. Unsupervised Learning Improvements\nThe proposed unsupervised learning method constructs\nthe objective function based on the consistency between\nRT and YT. In practice, the tracker may deviate from the\ntarget in the forward tracking but still return to the original\nposition during the backward process. However, the pro-\nposed loss function does not penalize this deviation because\nof the consistent predictions. Meanwhile, the raw videos\nmay contain uninformative or even corrupted training sam-\nples with occlusion that deteriorate the unsupervised learn-\ning process. We propose multiple frames validation and a\ncost-sensitive loss to tackle these limitations.\n3.3.1\nMultiple Frames Validation\nWe propose a multiple frames validation approach to alle-\nviate the inaccurate localization issue that is not penalized\nby Eq. (6). Our intuition is to involve more frames dur-\ning forward and backward tracking to reduce the veriﬁca-\ntion failures. The reconstruction error in Eq. (6) tends to be\nampliﬁed and the computed loss will facilitate the training\nprocess.\nDuring unsupervised learning, we involve another frame\nP3 which is the subsequent frame after P2. We crop a search\npatch S1 from P2 and another search patch S2 from P3. If\nthe generated response map RS1 is different from its corre-\nsponding ground-truth response, this error tends to become\nlarger in the next frame P3. As a result, the consistency is\nmore likely to be broken in the backward tracking, and the\ngenerated response map RT is more likely to deviate from\nYT. By simply involving more search patches during for-\nward and backward tracking, the proposed consistency loss\n#1\n#2\nSearch Patch\nTemplate Patch\n Search\nPatch #1\n#1\n#2\n#3\n Search\nPatch #2\nTemplate Patch \nCoincidental Success\nError Accumulation\nFigure 3. Single frame validation and multiple frames validation.\nThe inaccurate localization in single frame validation may not be\ncaptured as shown on the left. By involving more frames as shown\non the right, we can accumulate the localization error to break the\nprediction consistency during forward and backward tracking.\nwill be more effective to penalize the inaccurate localiza-\ntions as shown in Fig. 3. In practice, we use three frames to\nvalidate and the improved consistency loss is written as:\nLun = ∥eRT −YT∥2\n2,\n(8)\nwhere eRT is the response map generated by an additional\nframe during the backward tracking step.\n3.3.2\nCost-sensitive Loss\nWe randomly initialize a bounding box region in the ﬁrst\nframe P1 for forward tracking. This bounding box region\nmay contain noisy background context (e.g., occluded tar-\ngets). Fig. 5 shows an overview of these regions. To allevi-\nate the background interference, we propose a cost-sensitive\nloss to exclude noisy samples for network training.\nDuring unsupervised learning, we construct multiple\ntraining pairs from the training sequences. Each training\npair consists of one initial template patch T in frame P1 and\ntwo search patches S1 and S2 from the subsequent frames\nP2 and P3, respectively. These training pairs form a training\nbatch to train the Siamese network. In practice, we ﬁnd that\nfew training pairs with extremely high losses prevent the\nnetwork training from convergence. To reduce the contri-\nbutions of noisy pairs, we exclude 10% of the whole train-\ning pairs which contain a high loss value. Their losses can\nbe computed using Eq. (8). To this end, we assign a bi-\nnary weight Ai\ndrop to each training pair and all the weight\nelements form the weight vector Adrop. The 10% of its ele-\nments are 0 and the others are 1.\nIn addition to the noisy training pairs, the raw videos in-\nclude lots of uninformative image patches which only con-\ntain the background or still targets. For these patches, the\nobjects (e.g., sky, grass, or tree) hardly move. Intuitively,\nthe target with a large motion contributes more to the net-\nwork training. Therefore, we assign a motion weight vector\nAmotion to all the training pairs. Each element Ai\nmotion can\nbe computed by\nAi\nmotion =\n\r\rRi\nS1 −Yi\nT\n\r\r2\n2 +\n\r\rRi\nS2 −Yi\nS1\n\r\r2\n2 ,\n(9)\nTemplate or search patches\nUnlabeled sequences in the wild\n...\nFigure 4. An illustration of training samples generation. The pro-\nposed method simply crops and resizes the center regions from\nunlabeled videos as the training patches.\nwhere Ri\nS1 and Ri\nS2 are the response maps in the i-th train-\ning pair, Yi\nT and Yi\nS1 are the corresponding initial and\npseudo labels, respectively.\nEq. (9) calculates the target\nmotion difference from frame P1 to P2 and P2 to P3. The\nlarger value of Ai\nmotion indicates that the target undergoes a\nlarger movement in this continuous trajectory. On the other\nhand, we can interpret that the large value of Ai\nmotion rep-\nresents the hard training pair which the network should pay\nmore attentions to. We normalize the motion weight and the\nbinary weight as follows,\nAi\nnorm =\nAi\ndrop · Ai\nmotion\nPn\ni=1 Ai\ndrop · Ai\nmotion\n,\n(10)\nwhere n is number of the training pairs in a mini-batch. The\nﬁnal unsupervised loss in a mini-batch is computed as:\nLun = 1\nn\nn\nX\ni=1\nAi\nnorm ·\n\r\r\r eRi\nT −Yi\nT\n\r\r\r\n2\n2 .\n(11)\n3.4. Unsupervised Training Details\nNetwork Structure. We follow the DCFNet [54] to use\na shallow Siamese network with only two convolutional\nlayers.\nThe ﬁlter sizes of these convolutional layers are\n3 × 3 × 3 × 32 and 3 × 3 × 32 × 32, respectively. Besides,\na local response normalization (LRN) layer is employed at\nthe end of convolutional layers. This lightweight structure\nenables extremely efﬁcient online tracking.\nTraining Data. We choose the widely used ILSVRC 2015\n[42] as our training data to fairly compare with existing su-\npervised trackers. In the data pre-processing step, existing\nsupervised approaches [1, 49, 54] require ground-truth la-\nbels for every frame. Meanwhile, they usually discard the\nframes where the target is occluded, or the target is partially\nout of view, or the target infrequently appears in tracking\nscenarios (e.g., snake). This requires a time-consuming hu-\nman interaction to preprocess the training data.\nIn contrast, we do not preprocess any data and simply\ncrop the center patch in each frame. The patch size is the\nhalf of the whole image and further resized to 125 × 125 as\nFigure 5. Examples of randomly cropped center patches from\nILSVRC 2015 [42]. Most patches contain valuable contents while\nsome are less meaningful (e.g., the patches on the last row).\nthe network input as shown in Fig. 4. We randomly choose\nthree cropped patches from the continuous 10 frames in a\nvideo. We set one of the three patches as the template and\nthe remaining as search patches. This is based on the as-\nsumption that the center located target objects are unlikely\nto move out of the cropped region in a short period. We\ntrack the objects appearing in the center of the cropped re-\ngions, while not specifying their categories. Some examples\nof the cropped regions are exhibited in Fig. 5.\n3.5. Online Object Tracking\nAfter ofﬂine unsupervised learning, we online track the\ntarget object following forward tracking as illustrated in\nSec. 3.2. To adapt the object appearance variations, we on-\nline update the DCF parameters as follows:\nWt = (1 −αt)Wt−1 + αtW,\n(12)\nwhere αt ∈[0, 1] is the linear interpolation coefﬁcient. The\ntarget scale is estimated through a patch pyramid with scale\nfactors {as|a = 1.015, s = {−1, 0, 1}} following [10]. We\ndenote the proposed Unsupervised Deep Tracker as UDT,\nwhich merely uses standard incremental model update and\nscale estimation. Furthermore, we use an advanced model\nupdate that adaptively changes αt as well as a better DCF\nformulation following [7]. The improved tracker is denoted\nas UDT+.\n4. Experiments\nIn this section, we ﬁrst analyze the effectiveness of our\nunsupervised learning framework. Then, we compare with\nstate-of-the-art trackers on the standard benchmarks includ-\ning OTB-2015 [57], Temple-Color [29] and VOT-2016 [21].\n4.1. Experimental Details\nIn our experiments, we use the stochastic gradient de-\nscent (SGD) with a momentum of 0.9 and a weight decay\n0\n10\n20\n30\n40\n50\nLocation error threshold (pixels)\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nPrecision rate\nPrecision plots of OPE\nUDT-FullySupervised [80.6]\nUDT-Weakly [78.9]\nUDT-MoreData [76.9]\nUDT-Finetune [76.7]\nUDT [76.0]\nUDT-StandardLoss [74.5]\nUDT-SingleTrajectory [73.2]\n0\n0.2\n0.4\n0.6\n0.8\n1\nOverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nSuccess rate\nSuccess plots of OPE\nUDT-FullySupervised [62.6]\nUDT-Weakly [61.4]\nUDT-MoreData [60.1]\nUDT-Finetune [60.0]\nUDT [59.4]\nUDT-StandardLoss [58.6]\nUDT-SingleTrajectory [57.4]\nFigure 6. The precision and success plots of our UDT tracker with\ndifferent conﬁgurations on the OTB-2015 dataset [57]. In the leg-\nend, we show the distance precision at 20 pixels threshold and\narea-under-curve (AUC) score.\nof 0.005 to train our model. Our unsupervised network is\ntrained for 50 epoches with a learning rate exponentially\ndecays from 10−2 to 10−5 and a mini-batch size of 32. All\nthe experiments are executed on a computer with 4.00GHz\nIntel Core I7-4790K and NVIDIA GTX 1080Ti GPU.\nOn the OTB-2015 [57] and TempleColor [29] datasets,\nwe use one-pass evaluation (OPE) with distance precision\n(DP) at 20 pixels and the area-under-curve (AUC) of the\noverlap success plot. On the VOT2016 [21], we measure the\nperformance using the Expected Average Overlap (EAO).\n4.2. Ablation Study and Analysis\nUnsupervised and supervised learning. We use the same\ntraining data [42] to train our network via fully supervised\nlearning. Fig. 6 shows the evaluation results where the fully\nsupervised training conﬁguration improves UDT by 3% un-\nder the AUC scores.\nStable training. We analyze the effectiveness of our sta-\nble training by using different conﬁgurations. Fig. 6 shows\nthe evaluation results of multiple learned trackers.\nThe\nUDT-StandardLoss indicates the results from the tracker\nlearned without using hard sample reweighing (i.e., Amotion\nin Eq. (9)). The UDT-SingleTrajectory denotes the results\nfrom the tracker learned only using the prototype frame-\nwork in Sec. 3.2. The results show that multiple frames\nvalidation and cost-sensitive loss improve the accuracy.\nUsing high-quality training data. We analyze the perfor-\nmance variations by using high-quality training data. In\nILSVRC 2015 [42], instead of randomly cropping patches,\nwe add offsets ranging from [-20, +20] pixels to the ground-\ntruth bounding boxes for training samples collection. These\npatches contain more meaningful objects than the randomly\ncropped ones. The results in Fig. 6 show that our tracker\nlearned using weakly labeled samples (i.e., UDT-Weakly)\nproduce comparable results with the supervised conﬁgu-\nration.\nNote that the predicted target location by exist-\ning object detectors or optical ﬂow estimators is normally\nwithin 20 pixels offset with respect to the ground-truth.\nThese results indicate that UDT achieves comparable per-\nformance with supervised conﬁguration when using less ac-\nTable 1. Comparison results with fully-supervised baseline (left) and state-of-the-art (right) trackers on the OTB-2015 benchmark [57].\nThe evaluation metric is AUC score. Our unsupervised UDT tracker performs favorably against baseline methods shown on the left, while\nour UDT+ tracker achieves comparable results with the recent state-of-the-art supervised trackers shown on the right.\nTrackers\nSiamFC DCFNet CFNet UDT DSiam EAST\nHP\nSA-Siam SiamPRN RASNet SACF Siam-tri RT-MDNet MemTrack StructSiam UDT+\n[1]\n[54]\n[49]\n[14]\n[17]\n[13]\n[15]\n[27]\n[55]\n[59]\n[12]\n[19]\n[58]\n[60]\nAUC score (%)\n58.2\n58.0\n56.8\n59.4\n60.5\n62.9\n60.1\n65.7\n63.7\n64.2\n63.3\n59.2\n65.0\n62.6\n62.1\n63.2\nSpeed (FPS)\n86\n70\n65\n70\n25\n159\n69\n50\n160\n83\n23\n86\n50\n50\n45\n55\ncurate labels produced by existing detection or ﬂow estima-\ntion methods.\nFew-shot domain adaptation. We collect the ﬁrst 5 frames\nfrom the videos in OTB-2015 [57] with only the ground-\ntruth bounding box available in the ﬁrst frame. Using these\nlimited samples, we ﬁne-tune our network by 100 iterations\nusing the forward-backward pipeline. This training process\ntakes around 6 minutes. The results (i.e., UDT-Finetune)\nshow that the performance is further enhanced. Our ofﬂine\nunsupervised training learns general feature representation,\nwhich can be transferred to a speciﬁc domain (e.g., OTB)\nusing few-shot adaptation. This domain adaptation is sim-\nilar to MDNet [37] but our initial parameters are ofﬂine\nlearned in an unsupervised manner.\nAdopting more unlabeled data. Finally, we utilize more\nunlabeled videos for network training. These additional raw\nvideos are from the OxUvA benchmark [48] (337 videos in\ntotal), which is a subset of Youtube-BB [41]. In Fig. 6,\nour UDT-MoreData tracker gains performance improve-\nment (0.9% DP and 0.7% AUC), which illustrates unlabeled\ndata can advance the unsupervised training. Nevertheless,\nin the following we remain using the UDT and UDT+ track-\ners which are only trained on [42] for fair comparisons.\n4.3. State-of-the-art Comparison\nOTB-2015 Dataset. We evaluate the proposed UDT and\nUDT+ trackers with state-of-the-art real-time trackers in-\ncluding ACT [4], ACFN [6], CFNet [49], SiamFC [1], SCT\n[5], CSR-DCF [32], DSST [8], and KCF [16] using preci-\nsion and success plots metrics. Fig. 7 and Table 1 show that\nthe proposed unsupervised tracker UDT is comparable with\nthe baseline supervised methods (i.e., SiamFC and CFNet).\nMeanwhile, the proposed UDT tracker exceeds DSST algo-\nrithm by a large margin. As DSST is a DCF based tracker\nwith accurate scale estimation, the performance improve-\nment indicates that our unsupervised feature representation\nis more effective than empirical features.\nIn Fig. 7 and\nTable 1, we do not compare with some remarkable non-\nrealtime trackers. For example, MDNet [37] and ECO [7]\ncan yield 67.8% and 69.4% AUC on the OTB-2015 dataset,\nbut they are far from real-time.\nIn Table 1, we also compare with more recently proposed\nsupervised trackers.\nThese latest approaches are mainly\nbased on the Siamese network and trained using ILSVRC\n[42]. Some trackers (e.g., SA-Siam [15] and RT-MDNet\n0\n10\n20\n30\n40\n50\nLocation error threshold (pixels)\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nPrecision rate\nPrecision plots of OPE\nACT [84.2]\nUDT+ [83.1]\nACFN [79.4]\nSiamFC [77.1]\nCSR-DCF [77.0]\nUDT [76.0]\nSCT [76.0]\nCFNet [74.8]\nKCF [69.6]\nDSST [68.9]\n0\n0.2\n0.4\n0.6\n0.8\n1\nOverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nSuccess rate\nSuccess plots of OPE\nUDT+ [63.2]\nACT [62.5]\nUDT [59.4]\nSiamFC [58.2]\nCSR-DCF [58.1]\nACFN [57.0]\nCFNet [56.8]\nSCT [53.7]\nDSST [51.8]\nKCF [48.5]\nFigure 7. Precision and success plots on the OTB-2015 dataset [57]\nfor recent real-time trackers.\n0\n10\n20\n30\n40\n50\nLocation error threshold (pixels)\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nPrecision rate\nPrecision plots of OPE\nUDT+ [71.7]\nSiamFC [68.8]\nUDT [65.8]\nCSR-DCF [64.7]\nSCT [62.7]\nCFNet [60.7]\nKCF [54.9]\nDSST [53.4]\n0\n0.2\n0.4\n0.6\n0.8\n1\nOverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nSuccess rate\nSuccess plots of OPE\nUDT+ [54.1]\nUDT [50.7]\nSiamFC [50.3]\nCSR-DCF [47.7]\nSCT [46.6]\nCFNet [45.6]\nDSST [40.5]\nKCF [38.7]\nFigure 8. Precision and success plots on the Temple-Color dataset\n[29] for recent real-time trackers.\n[19]) adopt pre-trained CNN models (e.g., AlexNet [23]\nand VGG-M [3]) for network initialization. The SiamRPN\n[27] additionally uses more labeled training videos from\nYoutube-BB dataset [41]. Compared with existing methods,\nthe proposed UDT+ tracker does not require data labels or\noff-the-shelf deep models while still achieving comparable\nperformance and efﬁciency.\nTemple-Color Dataset. The Temple-Color [29] is a more\nchallenging benchmark with 128 color videos. We com-\npare our method with the state-of-the-art trackers illustrated\nin Sec. 4.3. The propose UDT tracker performs favorably\nagainst SiamFC and CFNet as shown in Fig. 8.\nVOT2016 Dataset. Furthermore, we report the evaluation\nresults on the VOT2016 benchmark [21]. The expected av-\nerage overlap (EAO) is the ﬁnal metric for tracker rank-\ning according to the VOT report [22]. As shown in Ta-\nble 2, the performance of our UDT tracker is comparable\nwith the baseline trackers (e.g., SiamFC). The improved\nUDT+ tracker performs favorably against state-of-the-art\nfully-supervised trackers including SA-Siam [15], Struct-\nSiam [60] and MemTrack [58].\nAttribute Analysis. On the OTB-2015 benchmark, we fur-\n0 \n10 \n20 \n30 \n40 \n50 \n60 \n70 \nUDT-fully \nUDT \nSiamFC \nCFNet \nAUC  Score  (%) \nFigure 9. Attribute-based evaluation on the OTB-2015 dataset\n[57].\nThe 11 attributes are background clutter (BC), deforma-\ntion (DEF), fast motion (FM), in-plane rotation (IPR), illumination\nvarition (IV), low resolution (LR), motion blur (MB), occlusion\n(OCC), out-of-plane rotation (OPR), out-of-view (OV), and scale\nvarition (SV), respectively.\nTable 2. Comparison with state-of-the-art and baseline trackers on\nthe VOT2016 benchmark [21]. The evaluation metrics include Ac-\ncuracy, Failures (over 60 sequences), and Expected Average Over-\nlap (EAO). The up arrows indicate that higher values are better for\nthe corresponding metric and vice versa.\nTrackers\nAccuracy (↑) Failures (↓) EAO (↑)\nFPS (↑)\nECO [7]\n0.54\n-\n0.374\n6\nC-COT [11]\n0.52\n51\n0.331\n0.3\npyMDNet [37]\n-\n-\n0.304\n2\nSA-Siam [15]\n0.53\n-\n0.291\n50\nStructSiam [60]\n-\n-\n0.264\n45\nMemTrack [58]\n0.53\n-\n0.273\n50\nSiamFC [1]\n0.53\n99\n0.235\n86\nSCT [5]\n0.48\n117\n0.188\n40\nDSST [8]\n0.53\n151\n0.181\n25\nKCF [16]\n0.49\n122\n0.192\n170\nUDT (Ours)\n0.54\n102\n0.226\n70\nUDT+ (Ours)\n0.53\n66\n0.301\n55\nther analyze the performance variations over different chal-\nlenges as shown in Fig. 9. On the majority of challeng-\ning scenarios, the proposed UDT tracker outperforms the\nSiamFC and CFNet trackers.\nCompared with the fully-\nsupervised UDT tracker, the unsupervised UDT does not\nachieve similar tracking accuracies under illumination vari-\nation (IV), occlusion (OCC), and fast motion (FM) scenar-\nios. This is because the target appearance variations are sig-\nniﬁcant in these video sequences. Without strong supervi-\nsion, the proposed tracker is not effective to learn a robust\nfeature representation to overcome these variations.\nQualitative Evaluation. We visually compare the proposed\nUDT tracker to some supervised trackers (e.g., ACFN,\nSiamFC, and CFNet) and a baseline DCF tracker (DSST) on\neight challenging video sequences. Although the proposed\nUDT tracker does not employ online improvements, we still\nobserve that UDT effectively tracks the target, especially\nUDT\nSiamFC\nCFNet\nACFN\nDSST\nFigure 10. Qualitative evaluation of our proposed UDT and other\ntrackers including SiamFC [1], CFNet [49], ACFN [6], and DSST\n[8] on 8 challenging videos from OTB-2015. From left to right and\ntop to down are Basketball, Board, Ironman, CarScale, Diving,\nDragonBaby, Bolt, and Tiger1, respectively.\non the challenging Ironman and Diving video sequences as\nshown in Fig. 10. It is worth mentioning that such a robust\ntracker is learned using unlabeled videos without ground-\ntruth supervisions.\nLimitation. (1) As discussed in the Attribute Analysis, our\nunsupervised feature representation may lack the objectness\ninformation to cope with complex scenarios. (2) Since our\napproach involves both forward and backward tracking, the\ncomputational load is another potential drawback.\n5. Conclusion\nIn this paper, we proposed how to train a visual tracker\nusing unlabeled video sequences in the wild, which has\nrarely been investigated in visual tracking. By designing an\nunsupervised Siamese correlation ﬁlter network, we veriﬁed\nthe feasibility and effectiveness of our forward-backward\nbased unsupervised training pipeline. To further facilitate\nthe unsupervised training, we extended our framework to\nconsider multiple frames and employ a cost-sensitive loss.\nExtensive experiments exhibit that the proposed unsuper-\nvised tracker, without bells and whistles, performs as a solid\nbaseline and achieves comparable results with the classic\nfully-supervised trackers. Finally, unsupervised framework\nshows attractive potentials in visual tracking, such as utiliz-\ning more unlabeled data or weakly labeled data to further\nimprove the tracking accuracy.\nAcknowledgements. This work was supported in part to Dr. Houqiang\nLi by the 973 Program under Contract No. 2015CB351803 and NSFC\nunder contract No.\n61836011, and in part to Dr.\nWengang Zhou by\nNSFC under contract No. 61822208 and 61632019, Young Elite Scientists\nSponsorship Program By CAST (2016QNRC001), and the Fundamental\nResearch Funds for the Central Universities. This work was supported\nin part by National Key Research and Development Program of China\n(2016YFB1001003), STCSM(18DZ1112300).\nReferences\n[1] Luca Bertinetto, Jack Valmadre, Jo˜ao F Henriques, Andrea\nVedaldi, and Philip HS Torr. Fully-convolutional siamese\nnetworks for object tracking. In ECCV, 2016.\n[2] David S Bolme, J Ross Beveridge, Bruce A Draper, and\nYui Man Lui. Visual object tracking using adaptive corre-\nlation ﬁlters. In CVPR, 2010.\n[3] Ken Chatﬁeld, Karen Simonyan, Andrea Vedaldi, and An-\ndrew Zisserman. Return of the devil in the details: Delving\ndeep into convolutional nets. In BMVC, 2014.\n[4] Boyu Chen, Dong Wang, Peixia Li, Shuang Wang, and\nHuchuan Lu.\nReal-time’actor-critic’tracking.\nIn ECCV,\n2018.\n[5] Jongwon Choi, Hyung Jin Chang, Jiyeoup Jeong, Yian-\nnis Demiris, and Jin Young Choi.\nVisual tracking using\nattention-modulated disintegration and integration. In CVPR,\n2016.\n[6] Jongwon Choi, Hyung Jin Chang, Sangdoo Yun, Tobias Fis-\ncher, Yiannis Demiris, and Jin Young Choi. Attentional cor-\nrelation ﬁlter network for adaptive visual tracking. In CVPR,\n2017.\n[7] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and\nMichael Felsberg. Eco: Efﬁcient convolution operators for\ntracking. In CVPR, 2017.\n[8] Martin Danelljan, Gustav H¨ager, Fahad Khan, and Michael\nFelsberg. Accurate scale estimation for robust visual track-\ning. In BMVC, 2014.\n[9] Martin Danelljan, Gustav H¨ager, Fahad Shahbaz Khan, and\nMichael Felsberg. Adaptive decontamination of the training\nset: A uniﬁed formulation for discriminative visual tracking.\nIn CVPR, 2016.\n[10] Martin Danelljan, Gustav Hager, Fahad Shahbaz Khan, and\nMichael Felsberg. Learning spatially regularized correlation\nﬁlters for visual tracking. In ICCV, 2015.\n[11] Martin Danelljan, Andreas Robinson, Fahad Shahbaz Khan,\nand Michael Felsberg.\nBeyond correlation ﬁlters: Learn-\ning continuous convolution operators for visual tracking. In\nECCV, 2016.\n[12] Xingping Dong and Jianbing Shen. Triplet loss in siamese\nnetwork for object tracking. In ECCV, 2018.\n[13] Xingping Dong, Jianbing Shen, Wenguan Wang, Yu Liu,\nLing Shao, and Fatih Porikli.\nHyperparameter optimiza-\ntion for tracking with continuous deep q-learning. In CVPR,\n2018.\n[14] Qing Guo, Wei Feng, Ce Zhou, Rui Huang, Liang Wan, and\nSong Wang. Learning dynamic siamese network for visual\nobject tracking. In ICCV, 2017.\n[15] Anfeng He, Chong Luo, Xinmei Tian, and Wenjun Zeng. A\ntwofold siamese network for real-time object tracking. In\nCVPR, 2018.\n[16] Jo˜ao F Henriques, Rui Caseiro, Pedro Martins, and Jorge\nBatista. High-speed tracking with kernelized correlation ﬁl-\nters. TPAMI, 37(3):583–596, 2015.\n[17] Chen Huang, Simon Lucey, and Deva Ramanan. Learning\npolicies for adaptive tracking with deep feature cascades. In\nICCV, 2017.\n[18] Jianglei Huang and Wengang Zhou.\nRe2ema: Regular-\nized and reinitialized exponential moving average for target\nmodel update in object tracking. In AAAI, 2019.\n[19] Ilchae Jung, Jeany Son, Mooyeol Baek, and Bohyung Han.\nReal-time mdnet. In ECCV, 2018.\n[20] Zdenek Kalal,\nKrystian Mikolajczyk,\nand Jiri Matas.\nTracking-learning-detection.\nTPAMI, 34(7):1409–1422,\n2012.\n[21] Matej Kristan, Jiri Matas, Ales Leonardis, Michael Felsberg,\nLuka Cehovin, Gustavo Fern´andez, Tomas Vojir, Hager, and\net al. The visual object tracking vot2016 challenge results.\nIn ECCV Workshop, 2016.\n[22] Matej Kristan, Jiri Matas, Aleˇs Leonardis, Tom´aˇs Voj´ıˇr,\nRoman Pﬂugfelder, Gustavo Fernandez, Georg Nebehay,\nFatih Porikli, and Luka ˇCehovin.\nA novel performance\nevaluation methodology for single-target trackers. TPAMI,\n38(11):2137–2155, 2016.\n[23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classiﬁcation with deep convolutional neural net-\nworks. In NIPS, 2012.\n[24] Quoc V Le. Building high-level features using large scale\nunsupervised learning. In ICASSP, 2013.\n[25] Dae Youn Lee, Jae Young Sim, and Chang Su Kim. Multi-\nhypothesis trajectory analysis for robust visual tracking. In\nCVPR, 2015.\n[26] Hsin-Ying Lee, Jia-Bin Huang, Maneesh Singh, and Ming-\nHsuan Yang. Unsupervised representation learning by sort-\ning sequences. In ICCV, 2017.\n[27] Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, and Xiaolin Hu.\nHigh performance visual tracking with siamese region pro-\nposal network. In CVPR, 2018.\n[28] Feng Li, Yingjie Yao, Peihua Li, David Zhang, Wangmeng\nZuo, and Ming-Hsuan Yang. Integrating boundary and center\ncorrelation ﬁlters for visual tracking with aspect ratio varia-\ntion. In ICCV Workshop, 2017.\n[29] Pengpeng Liang, Erik Blasch, and Haibin Ling. Encoding\ncolor information for visual tracking: algorithms and bench-\nmark. TIP, 24(12):5630–5644, 2015.\n[30] Si Liu, Tianzhu Zhang, Xiaochun Cao, and Changsheng Xu.\nStructural correlation ﬁlter for robust visual tracking.\nIn\nCVPR, 2016.\n[31] Xiankai Lu, Chao Ma, Bingbing Ni, Xiaokang Yang, Ian\nReid, and Ming-Hsuan Yang. Deep regression tracking with\nshrinkage loss. In ECCV, 2018.\n[32] Alan Lukezic, Tomas Vojir, Luka Cehovin Zajc, Jiri Matas,\nand Matej Kristan.\nDiscriminative correlation ﬁlter with\nchannel and spatial reliability. In CVPR, 2017.\n[33] Wenhan Luo, Peng Sun, Fangwei Zhong, Wei Liu, Tong\nZhang, and Yizhou Wang. End-to-end active object track-\ning and its real-world deployment via reinforcement learn-\ning. TPAMI, 2019.\n[34] Wenhan Luo, Junliang Xing, Anton Milan, Xiaoqin Zhang,\nWei Liu, Xiaowei Zhao, and Tae-Kyun Kim.\nMulti-\nple object tracking: A literature review.\narXiv preprint\narXiv:1409.7618, 2014.\n[35] Chao Ma, Jia-Bin Huang, Xiaokang Yang, and Ming-Hsuan\nYang. Hierarchical convolutional features for visual tracking.\nIn ICCV, 2015.\n[36] Matthias M¨uller, Adel Bibi, Silvio Giancola, Salman Al-\nSubaihi, and Bernard Ghanem. Trackingnet: A large-scale\ndataset and benchmark for object tracking in the wild. In\nECCV, 2018.\n[37] Hyeonseob Nam and Bohyung Han. Learning multi-domain\nconvolutional neural networks for visual tracking. In CVPR,\n2016.\n[38] Bruno A Olshausen and David J Field. Sparse coding with an\novercomplete basis set: A strategy employed by v1? Vision\nresearch, 37(23):3311–3325, 1997.\n[39] Shi Pu, Yibing Song, Chao Ma, Honggang Zhang, and Ming-\nHsuan Yang. Deep attentive tracking via reciprocative learn-\ning. In NeurIPS, 2018.\n[40] Yuankai Qi, Shengping Zhang, Lei Qin, Hongxun Yao,\nQingming Huang, and Jongwoo Lim Ming-Hsuan Yang.\nHedged deep tracking. In CVPR, 2016.\n[41] Esteban Real, Jonathon Shlens, Stefano Mazzocchi, Xin Pan,\nand Vincent Vanhoucke. Youtube-boundingboxes: A large\nhigh-precision human-annotated data set for object detection\nin video. In CVPR, 2017.\n[42] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al.\nImagenet large\nscale visual recognition challenge. IJCV, 115(3):211–252,\n2015.\n[43] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014.\n[44] Yibing Song, Chao Ma, Lijun Gong, Jiawei Zhang, Rynson\nLau, and Ming-Hsuan Yang. Crest: Convolutional residual\nlearning for visual tracking. In ICCV, 2017.\n[45] Yibing Song, Chao Ma, Xiaohe Wu, Lijun Gong, Linchao\nBao, Wangmeng Zuo, Chunhua Shen, Rynson W.H. Lau,\nand Ming-Hsuan Yang. Vital: Visual tracking via adversarial\nlearning. In CVPR, 2018.\n[46] Ran Tao, Efstratios Gavves, and Arnold WM Smeulders.\nSiamese instance search for tracking. In CVPR, 2016.\n[47] Carlo Tomasi and Takeo Kanade. Detection and tracking of\npoint features. 1991.\n[48] Jack Valmadre, Luca Bertinetto, Jo˜ao F Henriques, Ran Tao,\nAndrea Vedaldi, Arnold Smeulders, Philip Torr, and Efstra-\ntios Gavves. Long-term tracking in the wild: A benchmark.\nIn ECCV, 2018.\n[49] Jack Valmadre, Luca Bertinetto, Jo˜ao F Henriques, Andrea\nVedaldi, and Philip HS Torr.\nEnd-to-end representation\nlearning for correlation ﬁlter based tracking. In CVPR, 2017.\n[50] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. An-\nticipating visual representations from unlabeled video.\nIn\nCVPR, 2016.\n[51] Naiyan Wang and Dit-Yan Yeung. Learning a deep compact\nimage representation for visual tracking. In NIPS, 2013.\n[52] Ning Wang, Wengang Zhou, and Houqiang Li. Reliable re-\ndetection for long-term tracking. TCSVT, 2019.\n[53] Ning Wang, Wengang Zhou, Qi Tian, Richang Hong, Meng\nWang, and Houqiang Li. Multi-cue correlation ﬁlters for ro-\nbust visual tracking. In CVPR, 2018.\n[54] Qiang Wang, Jin Gao, Junliang Xing, Mengdan Zhang, and\nWeiming Hu. Dcfnet: Discriminant correlation ﬁlters net-\nwork for visual tracking. arXiv preprint arXiv:1704.04057,\n2017.\n[55] Qiang Wang, Zhu Teng, Junliang Xing, Jin Gao, Weiming\nHu, and Stephen Maybank. Learning attentions: Residual\nattentional siamese network for high performance online vi-\nsual tracking. In CVPR, 2018.\n[56] Xiaolong Wang and Abhinav Gupta. Unsupervised learning\nof visual representations using videos. In ICCV, 2015.\n[57] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Object track-\ning benchmark. TPAMI, 37(9):1834–1848, 2015.\n[58] Tianyu Yang and Antoni B Chan. Learning dynamic memory\nnetworks for object tracking. In ECCV, 2018.\n[59] Mengdan Zhang, Qiang Wang, Junliang Xing, Jin Gao, Peixi\nPeng, Weiming Hu, and Steve Maybank. Visual tracking via\nspatially aligned correlation ﬁlters network. In ECCV, 2018.\n[60] Yunhua Zhang, Lijun Wang, Jinqing Qi, Dong Wang,\nMengyang Feng, and Huchuan Lu. Structured siamese net-\nwork for real-time visual tracking. In ECCV, 2018.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2019-04-03",
  "updated": "2019-04-03"
}