{
  "id": "http://arxiv.org/abs/1904.05526v2",
  "title": "A Selective Overview of Deep Learning",
  "authors": [
    "Jianqing Fan",
    "Cong Ma",
    "Yiqiao Zhong"
  ],
  "abstract": "Deep learning has arguably achieved tremendous success in recent years. In\nsimple words, deep learning uses the composition of many nonlinear functions to\nmodel the complex dependency between input features and labels. While neural\nnetworks have a long history, recent advances have greatly improved their\nperformance in computer vision, natural language processing, etc. From the\nstatistical and scientific perspective, it is natural to ask: What is deep\nlearning? What are the new characteristics of deep learning, compared with\nclassical methods? What are the theoretical foundations of deep learning? To\nanswer these questions, we introduce common neural network models (e.g.,\nconvolutional neural nets, recurrent neural nets, generative adversarial nets)\nand training techniques (e.g., stochastic gradient descent, dropout, batch\nnormalization) from a statistical point of view. Along the way, we highlight\nnew characteristics of deep learning (including depth and over-parametrization)\nand explain their practical and theoretical benefits. We also sample recent\nresults on theories of deep learning, many of which are only suggestive. While\na complete understanding of deep learning remains elusive, we hope that our\nperspectives and discussions serve as a stimulus for new statistical research.",
  "text": "A Selective Overview of Deep Learning\nJianqing Fan∗\nCong Ma‡\nYiqiao Zhong∗\nApril 16, 2019\nAbstract\nDeep learning has arguably achieved tremendous success in recent years.\nIn simple words, deep\nlearning uses the composition of many nonlinear functions to model the complex dependency between\ninput features and labels.\nWhile neural networks have a long history, recent advances have greatly\nimproved their performance in computer vision, natural language processing, etc. From the statistical\nand scientiﬁc perspective, it is natural to ask: What is deep learning? What are the new characteristics of\ndeep learning, compared with classical methods? What are the theoretical foundations of deep learning?\nTo answer these questions, we introduce common neural network models (e.g., convolutional neural\nnets, recurrent neural nets, generative adversarial nets) and training techniques (e.g., stochastic gradient\ndescent, dropout, batch normalization) from a statistical point of view. Along the way, we highlight new\ncharacteristics of deep learning (including depth and over-parametrization) and explain their practical\nand theoretical beneﬁts. We also sample recent results on theories of deep learning, many of which are\nonly suggestive. While a complete understanding of deep learning remains elusive, we hope that our\nperspectives and discussions serve as a stimulus for new statistical research.\nKeywords: neural networks, over-parametrization, stochastic gradient descent, approximation theory, gen-\neralization error.\nContents\n1\nIntroduction\n2\n1.1\nIntriguing new characteristics of deep learning . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n1.2\nTowards theory of deep learning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n1.3\nRoadmap of the paper . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2\nFeed-forward neural networks\n5\n2.1\nModel setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.2\nBack-propagation in computational graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n3\nPopular models\n8\n3.1\nConvolutional neural networks\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n3.2\nRecurrent neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n3.3\nModules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n4\nDeep unsupervised learning\n14\n4.1\nAutoencoders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n4.2\nGenerative adversarial networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n5\nRepresentation power: approximation theory\n17\n5.1\nUniversal approximation theory for shallow NNs\n. . . . . . . . . . . . . . . . . . . . . . . . .\n18\n5.2\nApproximation theory for multi-layer NNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nAuthor names are sorted alphabetically.\n∗Department of Operations Research and Financial Engineering, Princeton University, Princeton, NJ 08544, USA; Email:\n{jqfan, congm, yiqiaoz}@princeton.edu.\n1\narXiv:1904.05526v2  [stat.ML]  15 Apr 2019\n6\nTraining deep neural nets\n20\n6.1\nStochastic gradient descent\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n6.2\nEasing numerical instability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n6.3\nRegularization techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n7\nGeneralization power\n25\n7.1\nAlgorithm-independent controls: uniform convergence\n. . . . . . . . . . . . . . . . . . . . . .\n25\n7.2\nAlgorithm-dependent controls . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n8\nDiscussion\n29\n1\nIntroduction\nModern machine learning and statistics deal with the problem of learning from data: given a training dataset\n{(yi, xi)}1≤i≤n where xi ∈Rd is the input and yi ∈R is the output1, one seeks a function f : Rd 7→R from a\ncertain function class F that has good prediction performance on test data. This problem is of fundamental\nsigniﬁcance and ﬁnds applications in numerous scenarios. For instance, in image recognition, the input x\n(reps. the output y) corresponds to the raw image (reps. its category) and the goal is to ﬁnd a mapping f(·)\nthat can classify future images accurately. Decades of research eﬀorts in statistical machine learning have been\ndevoted to developing methods to ﬁnd f(·) eﬃciently with provable guarantees. Prominent examples include\nlinear classiﬁers (e.g., linear / logistic regression, linear discriminant analysis), kernel methods (e.g., support\nvector machines), tree-based methods (e.g., decision trees, random forests), nonparametric regression (e.g.,\nnearest neighbors, local kernel smoothing), etc. Roughly speaking, each aforementioned method corresponds\nto a diﬀerent function class F from which the ﬁnal classiﬁer f(·) is chosen.\nDeep learning [70], in its simplest form, proposes the following compositional function class:\n\b\nf(x; θ) = WLσL(WL−1 · · · σ2(W2σ1(W1x)))\n\f\f θ = {W1, . . . , WL}\n\t\n.\n(1)\nHere, for each 1 ≤l ≤L, σℓ(·) is some nonlinear function, and θ = {W1, . . . , WL} consists of matrices\nwith appropriate sizes.\nThough simple, deep learning has made signiﬁcant progress towards addressing\nthe problem of learning from data over the past decade. Speciﬁcally, it has performed close to or better\nthan humans in various important tasks in artiﬁcial intelligence, including image recognition [50], game\nplaying [114], and machine translation [132]. Owing to its great promise, the impact of deep learning is\nalso growing rapidly in areas beyond artiﬁcial intelligence; examples include statistics [15, 111, 76, 104, 41],\napplied mathematics [130, 22], clinical research [28], etc.\nTable 1: Winning models for ILSVRC image classiﬁcation challenge.\nModel\nYear\n# Layers\n# Params\nTop-5 error\nShallow\n< 2012\n—\n—\n> 25%\nAlexNet\n2012\n8\n61M\n16.4%\nVGG19\n2014\n19\n144M\n7.3%\nGoogleNet\n2014\n22\n7M\n6.7%\nResNet-152\n2015\n152\n60M\n3.6%\nTo get a better idea of the success of deep learning, let us take the ImageNet Challenge [107] (also\nknown as ILSVRC) as an example. In the classiﬁcation task, one is given a training dataset consisting of 1.2\nmillion color images with 1000 categories, and the goal is to classify images based on the input pixels. The\nperformance of a classiﬁer is then evaluated on a test dataset of 100 thousand images, and in the end the\ntop-5 error2 is reported. Table 1 highlights a few popular models and their corresponding performance. As\n1When the label y is given, this problem is often known as supervised learning. We mainly focus on this paradigm throughout\nthis paper and remark sparingly on its counterpart, unsupervised learning, where y is not given.\n2The algorithm makes an error if the true label is not contained in the 5 predictions made by the algorithm.\n2\nFigure 1: Visualization of trained ﬁlters in the ﬁrst layer of AlexNet. The model is pre-trained on ImageNet\nand is downloadable via PyTorch package torchvision.models. Each ﬁlter contains 11×11×3 parameters\nand is shown as an RGB color map of size 11 × 11.\ncan be seen, deep learning models (the second to the last rows) have a clear edge over shallow models (the\nﬁrst row) that ﬁt linear models / tree-based models on handcrafted features. This signiﬁcant improvement\nraises a foundational question:\nWhy is deep learning better than classical methods on tasks like image recognition?\n1.1\nIntriguing new characteristics of deep learning\nIt is widely acknowledged that two indispensable factors contribute to the success of deep learning, namely\n(1) huge datasets that often contain millions of samples and (2) immense computing power resulting from\nclusters of graphics processing units (GPUs). Admittedly, these resources are only recently available: the\nlatter allows to train larger neural networks which reduces biases and the former enables variance reduction.\nHowever, these two alone are not suﬃcient to explain the mystery of deep learning due to some of its “dreadful”\ncharacteristics: (1) over-parametrization: the number of parameters in state-of-the-art deep learning models\nis often much larger than the sample size (see Table 1), which gives them the potential to overﬁt the training\ndata, and (2) nonconvexity: even with the help of GPUs, training deep learning models is still NP-hard [8]\nin the worst case due to the highly nonconvex loss function to minimize. In reality, these characteristics are\nfar from nightmares. This sharp diﬀerence motivates us to take a closer look at the salient features of deep\nlearning, which we single out a few below.\n1.1.1\nDepth\nDeep learning expresses complicated nonlinearity through composing many nonlinear functions; see (1).\nThe rationale for this multilayer structure is that, in many real-world datasets such as images, there are\ndiﬀerent levels of features and lower-level features are building blocks of higher-level ones. See [134] for a\nvisualization of trained features of convolutional neural nets; here in Figure 1, we sample and visualize weights\nfrom a pre-trained AlexNet model. This intuition is also supported by empirical results from physiology and\nneuroscience [56, 2]. The use of function composition marks a sharp diﬀerence from traditional statistical\nmethods such as projection pursuit models [38] and multi-index models [73, 27]. It is often observed that\ndepth helps eﬃciently extract features that are representative of a dataset. In comparison, increasing width\n(e.g., number of basis functions) in a shallow model leads to less improvement. This suggests that deep\nlearning models excel at representing a very diﬀerent function space that is suitable for complex datasets.\n1.1.2\nAlgorithmic regularization\nThe statistical performance of neural networks (e.g., test accuracy) depends heavily on the particular opti-\nmization algorithms used for training [131]. This is very diﬀerent from many classical statistical problems,\nwhere the related optimization problems are less complicated. For instance, when the associated optimization\n3\n(a) MNIST images\n(b) training and test accuracies\nFigure 2: (a) shows the images in the public dataset MNIST; and (b) depicts the training and test accuracies\nalong the training dynamics. Note that the training accuracy is approaching 100% and the test accuracy is\nstill high (no overﬁtting).\nproblem has a relatively simple structure (e.g., convex objective functions, linear constraints), the solution\nto the optimization problem can often be unambiguously computed and analyzed. However, in deep neural\nnetworks, due to over-parametrization, there are usually many local minima with diﬀerent statistical perfor-\nmance [72]. Nevertheless, common practice runs stochastic gradient descent with random initialization and\nﬁnds model parameters with very good prediction accuracy.\n1.1.3\nImplicit prior learning\nIt is well observed that deep neural networks trained with only the raw inputs (e.g., pixels of images) can\nprovide a useful representation of the data. This means that after training, the units of deep neural networks\ncan represent features such as edges, corners, wheels, eyes, etc.; see [134]. Importantly, the training process\nis automatic in the sense that no human knowledge is involved (other than hyper-parameter tuning). This\nis very diﬀerent from traditional methods, where algorithms are designed after structural assumptions are\nposited. It is likely that training an over-parametrized model eﬃciently learns and incorporates the prior\ndistribution p(x) of the input, even though deep learning models are themselves discriminative models. With\nautomatic representation of the prior distribution, deep learning typically performs well on similar datasets\n(but not very diﬀerent ones) via transfer learning.\n1.2\nTowards theory of deep learning\nDespite the empirical success, theoretical support for deep learning is still in its infancy. Setting the stage,\nfor any classiﬁer f, denote by E(f) the expected risk on fresh sample (a.k.a. test error, prediction error\nor generalization error), and by En(f) the empirical risk / training error averaged over a training dataset.\nArguably, the key theoretical question in deep learning is\nwhy is E( ˆfn) small, where ˆfn is the classiﬁer returned by the training algorithm?\nWe follow the conventional approximation-estimation decomposition (sometimes, also bias-variance trade-\noﬀ) to decompose the term E( ˆfn) into two parts. Let F be the function space expressible by a family of\nneural nets. Deﬁne f ∗= argminfE(f) to be the best possible classiﬁer and f ∗\nF = argminf∈FE(f) to be the\nbest classiﬁer in F. Then, we can decompose the excess error E ≜E( ˆfn) −E(f ∗) into two parts:\nE = E(f ∗\nF) −E(f ∗)\n|\n{z\n}\napproximation error\n+ E( ˆfn) −E(f ∗\nF)\n|\n{z\n}\nestimation error\n.\n(2)\nBoth errors can be small for deep learning (cf. Figure 2), which we explain below.\n4\n• The approximation error is determined by the function class F. Intuitively, the larger the class, the smaller\nthe approximation error. Deep learning models use many layers of nonlinear functions (Figure 3)that can\ndrive this error small. Indeed, in Section 5, we provide recent theoretical progress of its representation\npower. For example, deep models allow eﬃcient representation of interactions among variable while shallow\nmodels cannot.\n• The estimation error reﬂects the generalization power, which is inﬂuenced by both the complexity of the\nfunction class F and the properties of the training algorithms. Interestingly, for over-parametrized deep\nneural nets, stochastic gradient descent typically results in a near-zero training error (i.e., En( ˆfn) ≈0;\nsee e.g. left panel of Figure 2). Moreover, its generalization error E( ˆfn) remains small or moderate. This\n“counterintuitive” behavior suggests that for over-parametrized models, gradient-based algorithms enjoy\nbenign statistical properties; we shall see in Section 7 that gradient descent enjoys implicit regularization\nin the over-parametrized regime even without explicit regularization (e.g., ℓ2 regularization).\nThe above two points lead to the following heuristic explanation of the success of deep learning models.\nThe large depth of deep neural nets and heavy over-parametrization lead to small or zero training errors, even\nwhen running simple algorithms with moderate number of iterations. In addition, these simple algorithms\nwith moderate number of steps do not explore the entire function space and thus have limited complexities,\nwhich results in small generalization error with a large sample size. Thus, by combining the two aspects, it\nexplains heuristically that the test error is also small.\n1.3\nRoadmap of the paper\nWe ﬁrst introduce basic deep learning models in Sections 2–4, and then examine their representation power\nvia the lens of approximation theory in Section 5. Section 6 is devoted to training algorithms and their\nability of driving the training error small. Then we sample recent theoretical progress towards demystifying\nthe generalization power of deep learning in Section 7. Along the way, we provide our own perspectives, and\nat the end we identify a few interesting questions for future research in Section 8. The goal of this paper\nis to present suggestive methods and results, rather than giving conclusive arguments (which is currently\nunlikely) or a comprehensive survey. We hope that our discussion serves as a stimulus for new statistics\nresearch.\n2\nFeed-forward neural networks\nBefore introducing the vanilla feed-forward neural nets, let us set up necessary notations for the rest of this\nsection. We focus primarily on classiﬁcation problems, as regression problems can be addressed similarly.\nGiven the training dataset {(yi, xi)}1≤i≤n where yi ∈[K] ≜{1, 2, . . . , K} and xi ∈Rd are independent\nacross i ∈[n], supervised learning aims at ﬁnding a (possibly random) function ˆf(x) that predicts the\noutcome y for a new input x, assuming (y, x) follows the same distribution as (yi, xi). In the terminology\nof machine learning, the input xi is often called the feature, the output yi called the label, and the pair\n(yi, xi) is an example. The function ˆf is called the classiﬁer, and estimation of ˆf is training or learning. The\nperformance of ˆf is evaluated through the prediction error P(y ̸= ˆf(x)), which can be often estimated from\na separate test dataset.\nAs with classical statistical estimation, for each k ∈[K], a classiﬁer approximates the conditional prob-\nability P(y = k|x) using a function fk(x; θk) parametrized by θk.\nThen the category with the highest\nprobability is predicted. Thus, learning is essentially estimating the parameters θk. In statistics, one of the\nmost popular methods is (multinomial) logistic regression, which stipulates a speciﬁc form for the functions\nfk(x; θk): let zk = x⊤βk + αk and fk(x; θk) = Z−1 exp(zk) where Z = PK\nk=1 exp(zk) is a normalization\nfactor to make {fk(x; θk)}1≤k≤K a valid probability distribution. It is clear that logistic regression induces\nlinear decision boundaries in Rd, and hence it is restrictive in modeling nonlinear dependency between y and\nx. The deep neural networks we introduce below provide a ﬂexible framework for modeling nonlinearity in\na fairly general way.\n5\nhidden layer\ninput layer\noutput layer\nhidden layer\nσ\nσ\nσ\nσ\nσ\nσ\nσ\nσ\nσ\nσ\nFigure 3: A feed-forward neural network with an input layer, two hidden layers and an output layer. The\ninput layer represents raw features {xi}1≤i≤n.\nBoth hidden layers compute an aﬃne transform (a.k.s.\nindices) of the input and then apply an element-wise activation function σ(·). Finally, the output returns a\nlinear transform followed by the softmax activation (resp. simply a linear transform) of the hidden layers for\nthe classiﬁcation (resp. regression) problem.\n2.1\nModel setup\nFrom the high level, deep neural networks (DNNs) use composition of a series of simple nonlinear functions\nto model nonlinearity\nh(L) = g(L) ◦g(L−1) ◦. . . ◦g(1)(x),\nwhere ◦denotes composition of two functions and L is the number of hidden layers, and is usually called\ndepth of a NN model. Letting h(0) ≜x, one can recursively deﬁne h(l) = g(l)\u0000h(l−1)\u0001\nfor all ℓ= 1, 2, . . . , L.\nThe feed-forward neural networks, also called the multilayer perceptrons (MLPs), are neural nets with a\nspeciﬁc choice of g(l): for ℓ= 1, . . . , L, deﬁne\nh(ℓ) = g(l)\u0000h(l−1)\u0001\n≜σ\n\u0000W(ℓ)h(ℓ−1) + b(ℓ)\u0001\n,\n(3)\nwhere W(l) and b(l) are the weight matrix and the bias / intercept, respectively, associated with the l-th\nlayer, and σ(·) is usually a simple given (known) nonlinear function called the activation function. In words,\nin each layer ℓ, the input vector h(ℓ−1) goes through an aﬃne transformation ﬁrst and then passes through a\nﬁxed nonlinear function σ(·). See Figure 3 for an illustration of a simple MLP with two hidden layers. The\nactivation function σ(·) is usually applied element-wise, and a popular choice is the ReLU (Rectiﬁed Linear\nUnit) function:\n[σ(z)]j = max{zj, 0}.\n(4)\nOther choices of activation functions include leaky ReLU, tanh function [79] and the classical sigmoid function\n(1 + e−z)−1, which is less used now.\nGiven an output h(L) from the ﬁnal hidden layer and a label y, we can deﬁne a loss function to minimize.\nA common loss function for classiﬁcation problems is the multinomial logistic loss. Using the terminology of\ndeep learning, we say that h(L) goes through an aﬃne transformation and then the soft-max function:\nfk(x; θ) ≜\nexp(zk)\nP\nk exp(zk),\n∀k ∈[K],\nwhere z = W(L+1)h(L) + b(L+1) ∈RK.\nThen the loss is deﬁned to be the cross-entropy between the label y (in the form of an indicator vector) and\nthe score vector (f1(x; θ), . . . , fK(x; θ))⊤, which is exactly the negative log-likelihood of the multinomial\nlogistic regression model:\nL(f(x; θ), y) = −\nK\nX\nk=1\n1{y = k} log pk,\n(5)\n6\nwhere θ ≜{W(ℓ), b(ℓ) : 1 ≤ℓ≤L + 1}. As a ﬁnal remark, the number of parameters scales with both the\ndepth L and the width (i.e., the dimensionality of W(ℓ)), and hence it can be quite large for deep neural\nnets.\n2.2\nBack-propagation in computational graphs\nTraining neural networks follows the empirical risk minimization paradigm that minimizes the loss (e.g.,\n(5)) over all the training data. This minimization is usually done via stochastic gradient descent (SGD). In a\nway similar to gradient descent, SGD starts from a certain initial value θ0 and then iteratively updates the\nparameters θt by moving it in the direction of the negative gradient. The diﬀerence is that, in each update,\na small subsample B ⊂[n] called a mini-batch—which is typically of size 32–512—is randomly drawn and\nthe gradient calculation is only on B instead of the full batch [n]. This saves considerably the computational\ncost in calculation of gradient. By the law of large numbers, this stochastic gradient should be close to the\nfull sample one, albeit with some random ﬂuctuations. A pass of the whole training set is called an epoch.\nUsually, after several or tens of epochs, the error on a validation set levels oﬀand training is complete. See\nSection 6 for more details and variants on training algorithms.\nThe key to the above training procedure, namely SGD, is the calculation of the gradient ∇ℓB(θ), where\nℓB(θ) ≜|B|−1 X\ni∈B\nL(f(xi; θ), yi).\n(6)\nGradient computation, however, is in general nontrivial for complex models, and it is susceptible to numerical\ninstability for a model with large depth. Here, we introduce an eﬃcient approach, namely back-propagation,\nfor computing gradients in neural networks.\nBack-propagation [106] is a direct application of the chain rule in networks.\nAs the name suggests,\nthe calculation is performed in a backward fashion: one ﬁrst computes ∂ℓB/∂h(L), then ∂ℓB/∂h(L−1), . . .,\nand ﬁnally ∂ℓB/∂h(1). For example, in the case of the ReLU activation function3, we have the following\nrecursive / backward relation\n∂ℓB\n∂h(ℓ−1) =\n∂h(ℓ)\n∂h(ℓ−1) · ∂ℓB\n∂h(ℓ) = (W(ℓ))⊤diag\n\u0010\n1{W(ℓ)h(ℓ−1) + b(ℓ) ≥0}\n\u0011 ∂ℓB\n∂h(ℓ)\n(7)\nwhere diag(·) denotes a diagonal matrix with elements given by the argument. Note that the calculation of\n∂ℓB/∂h(ℓ−1) depends on ∂ℓB/∂h(ℓ), which is the partial derivatives from the next layer. In this way, the\nderivatives are “back-propagated” from the last layer to the ﬁrst layer. These derivatives {∂ℓB/∂h(ℓ)} are\nthen used to update the parameters. For instance, the gradient update for W(ℓ) is given by\nW(ℓ) ←W(ℓ) −η ∂ℓB\n∂W(ℓ) ,\nwhere\n∂ℓB\n∂W (ℓ)\njm\n= ∂ℓB\n∂h(ℓ)\nj\n· σ′ · h(ℓ−1)\nm\n,\n(8)\nwhere σ′ = 1 if the j-th element of W(ℓ)h(ℓ−1) + b(ℓ) is nonnegative, and σ′ = 0 otherwise. The step size\nη > 0, also called the learning rate, controls how much parameters are changed in a single update.\nA more general way to think about neural network models and training is to consider computational\ngraphs. Computational graphs are directed acyclic graphs that represent functional relations between vari-\nables. They are very convenient and ﬂexible to represent function composition, and moreover, they also\nallow an eﬃcient way of computing gradients.\nConsider an MLP with a single hidden layer and an ℓ2\nregularization:\nℓλ\nB(θ) = ℓB(θ) + rλ(θ) = ℓB(θ) + λ\n\u0010 X\nj,j′\n\u0000W (1)\nj,j′\n\u00012 +\nX\nj,j′\n\u0000W (2)\nj,j′\n\u00012\u0011\n,\n(9)\nwhere ℓB(θ) is the same as (6), and λ ≥0 is a tuning parameter. A similar example is considered in [45]. The\ncorresponding computational graph is shown in Figure 4. Each node represents a function (inside a circle),\nwhich is associated with an output of that function (outside a circle). For example, we view the term ℓB(θ)\nas a result of 4 compositions: ﬁrst the input data x multiplies the weight matrix W(1) resulting in u(1),\n3The issue of non-diﬀerentiability at the origin is often ignored in implementation.\n7\nmatmul\nrelu\nmatmul\n+\n\" # SoS\n$\n%(')\n)(')\n*\n12\n12\n,\n-(')\n-(.)\ncross \nentropy\n/,\n0\nFigure 4: The computational graph illustrates the loss (9). For simplicity, we omit the bias terms. Symbols\ninside nodes represent functions, and symbols outside nodes represent function outputs (vectors/scalars).\nmatmul is matrix multiplication, relu is the ReLU activation, cross entropy is the cross entropy loss, and\nSoS is the sum of squares.\nthen it goes through the ReLU activation function relu resulting in h(1), then it multiplies another weight\nmatrix W(2) leading to p, and ﬁnally it produces the cross-entropy with label y as in (5). The regularization\nterm is incorporated in the graph similarly.\nA forward pass is complete when all nodes are evaluated starting from the input x. A backward pass\nthen calculates the gradients of ℓλ\nB with respect to all other nodes in the reverse direction. Due to the chain\nrule, the gradient calculation for a variable (say, ∂ℓB/∂u(1)) is simple: it only depends on the gradient value\nof the variables (∂ℓB/∂h) the current node points to, and the function derivative evaluated at the current\nvariable value (σ′(u(1))). Thus, in each iteration, a computation graph only needs to (1) calculate and\nstore the function evaluations at each node in the forward pass, and then (2) calculate all derivatives in the\nbackward pass.\nBack-propagation in computational graphs forms the foundations of popular deep learning programming\nsoftwares, including TensorFlow [1] and PyTorch [92], which allows more eﬃcient building and training of\ncomplex neural net models.\n3\nPopular models\nMoving beyond vanilla feed-forward neural networks, we introduce two other popular deep learning models,\nnamely, the convolutional neural networks (CNNs) and the recurrent neural networks (RNNs). One impor-\ntant characteristic shared by the two models is weight sharing, that is some model parameters are identical\nacross locations in CNNs or across time in RNNs. This is related to the notion of translational invariance in\nCNNs and stationarity in RNNs. At the end of this section, we introduce a modular thinking for constructing\nmore ﬂexible neural nets.\n3.1\nConvolutional neural networks\nThe convolutional neural network (CNN) [71, 40] is a special type of feed-forward neural networks that is\ntailored for image processing. More generally, it is suitable for analyzing data with salient spatial structures.\nIn this subsection, we focus on image classiﬁcation using CNNs, where the raw input (image pixels) and\nfeatures of each hidden layer are represented by a 3D tensor X ∈Rd1×d2×d3. Here, the ﬁrst two dimensions\nd1, d2 of X indicate spatial coordinates of an image while the third d3 indicates the number of channels. For\ninstance, d3 is 3 for the raw inputs due to the red, green and blue channels, and d3 can be much larger (say,\n256) for hidden layers. Each channel is also called a feature map, because each feature map is specialized to\ndetect the same feature at diﬀerent locations of the input, which we will soon explain. We now introduce\ntwo building blocks of CNNs, namely the convolutional layer and the pooling layer.\n1. Convolutional layer (CONV). A convolutional layer has the same functionality as described in (3), where\n8\nX 2 R28⇥28⇥3\nFk 2 R5⇥5⇥3\n28\n28\n3\n5\n5\n3\n24\n24\n1\ninput feature map\nﬁlter\noutput feature map\n˜\nX 2 R24⇥24⇥1\nFigure 5: X ∈R28×28×3 represents the input feature consisting of 28 × 28 spatial coordinates in a total\nnumber of 3 channels / feature maps.\nFk ∈R5×5×3 denotes the k-th ﬁlter with size 5 × 5.\nThe third\ndimension 3 of the ﬁlter automatically matches the number 3 of channels in the previous input. Every 3D\npatch of X gets convolved with the ﬁlter Fk and this as a whole results in a single output feature map ˜X:,:,k\nwith size 24 × 24 × 1. Stacking the outputs of all the ﬁlters {Fk}1≤k≤K will lead to the output feature with\nsize 24 × 24 × K.\nthe input feature X ∈Rd1×d2×d3 goes through an aﬃne transformation ﬁrst and then an element-wise\nnonlinear activation. The diﬀerence lies in the speciﬁc form of the aﬃne transformation. A convolutional\nlayer uses a number of ﬁlters to extract local features from the previous input. More precisely, each ﬁlter\nis represented by a 3D tensor Fk ∈Rw×w×d3 (1 ≤k ≤˜d3), where w is the size of the ﬁlter (typically 3 or\n5) and ˜d3 denotes the total number of ﬁlters. Note that the third dimension d3 of Fk is equal to that of\nthe input feature X. For this reason, one usually says that the ﬁlter has size w × w, while suppressing the\nthird dimension d3. Each ﬁlter Fk then convolves with the input feature X to obtain one single feature\nmap Ok ∈R(d1−w+1)×(d1−w+1), where4\nOk\nij =\n\n[X]ij , Fk\n\u000b\n=\nw\nX\ni′=1\nw\nX\nj′=1\nd3\nX\nl=1\n[X]i+i′−1,j+j′−1,l[Fk]i′,j′,l.\n(10)\nHere [X]ij ∈Rw×w×d3 is a small “patch” of X starting at location (i, j). See Figure 5 for an illustration of\nthe convolution operation. If we view the 3D tensors [X]ij and Fk as vectors, then each ﬁlter essentially\ncomputes their inner product with a part of X indexed by i, j (which can be also viewed as convolution,\nas its name suggests).\nOne then pack the resulted feature maps {Ok} into a 3D tensor O with size\n(d1 −w + 1) × (d1 −w + 1) × ˜d3, where\n[O]ijk = [Ok]ij.\n(11)\nThe outputs of convolutional layers are then followed by nonlinear activation functions. In the ReLU\ncase, we have\n˜Xijk = σ(Oijk),\n∀i ∈[d1 −w + 1], j ∈[d2 −w + 1], k ∈[ ˜d3].\n(12)\nThe convolution operation (10) and the ReLU activation (12) work together to extract features ˜\nX from\nthe input X. Diﬀerent from feed-forward neural nets, the ﬁlters Fk are shared across all locations (i, j).\nA patch [X]ij of an input responds strongly (that is, producing a large value) to a ﬁlter Fk if they are\npositively correlated. Therefore intuitively, each ﬁlter Fk serves to extract features similar to Fk.\nAs a side note, after the convolution (10), the spatial size d1×d2 of the input X shrinks to (d1 −w + 1) × (d2 −w + 1)\nof ˜\nX. However one may want the spatial size unchanged. This can be achieved via padding, where one\n4To simplify notation, we omit the bias/intercept term associated with each ﬁlter.\n9\n6\n7\n15\n13\n3\n14\n1\n9\n16\n8\n2\n10\n11\n5\n4\n12\n14\n15\n15\n16\n14\n10\n16\n8\n12\n2 ⇥2 max pooling\nstride = 1\n14\n15\n16\n12\n2 ⇥2 max pooling\nstride = 2\nFigure 6: A 2 × 2 max pooling layer extracts the maximum of 2 by 2 neighboring pixels / features across the\nspatial dimension.\n32 ⇥32 ⇥1\n28 ⇥28 ⇥6\n14 ⇥14 ⇥6\n10 ⇥10 ⇥16\n5 ⇥5 ⇥16\n120\n84\n10\nCONV 5 ⇥5\nCONV 5 ⇥5\nPOOL 2 ⇥2\nPOOL 2 ⇥2\nFC\nFC\nFC\nFigure 7: LeNet is composed of an input layer, two convolutional layers, two pooling layers and three fully-\nconnected layers. Both convolutions are valid and use ﬁlters with size 5 × 5. In addition, the two pooling\nlayers use 2 × 2 average pooling.\nappends zeros to the margins of the input X to enlarge the spatial size to (d1 + w −1) × (d2 + w −1). In\naddition, a stride in the convolutional layer determines the gap i′ −i and j′ −j between two patches Xij\nand Xi′j′: in (10) the stride is 1, and a larger stride would lead to feature maps with smaller sizes.\n2. Pooling layer (POOL). A pooling layer aggregates the information of nearby features into a single one.\nThis downsampling operation reduces the size of the features for subsequent layers and saves computa-\ntion. One common form of the pooling layer is composed of the 2 × 2 max-pooling ﬁlter. It computes\nmax{Xi,j,k, Xi+1,j,k, Xi,j+1,k, Xi+1,j+1,k}, that is, the maximum of the 2 × 2 neighborhood in the spatial\ncoordinates; see Figure 6 for an illustration. Note that the pooling operation is done separately for each\nfeature map k. As a consequence, a 2 × 2 max-pooling ﬁlter acting on X ∈Rd1×d2×d3 will result in an\noutput of size d1/2×d2/2×d3. In addition, the pooling layer does not involve any parameters to optimize.\nPooling layers serve to reduce redundancy since a small neighborhood around a location (i, j) in a feature\nmap is likely to contain the same information.\nIn addition, we also use fully-connected layers as building blocks, which we have already seen in Section 2.\nEach fully-connected layer treats input tensor X as a vector Vec(X), and computes ˜\nX = σ(WVec(X)).\nA fully-connected layer does not use weight sharing and is often used in the last few layers of a CNN. As\nan example, Figure 7 depicts the well-known LeNet 5 [71], which is composed of two sets of CONV-POOL\nlayers and three fully-connected layers.\n3.2\nRecurrent neural networks\nRecurrent neural nets (RNNs) are another family of powerful models, which are designed to process time\nseries data and other sequence data. RNNs have successful applications in speech recognition [108], machine\ntranslation [132], genome sequencing [21], etc. The structure of an RNN naturally forms a computational\ngraph, and can be easily combined with other structures such as CNNs to build large computational graph\n10\n!\"\n&\"\n&#\n&$\n&%\n)\"\n)#\n)$\n)%\n(&&\n(&&\n(&&\n(!&\n(&'\n(&'\n(&'\n(&'\n!\"\n!#\n!$\n!%\n&\"\n&#\n&$\n&%\n)%\n(&&\n(&&\n(&&\n(!&\n(!&\n(!&\n(!&\n(&'\n!\"\n!#\n!$\n!%\n&\"\n&#\n&$\n&%\n)\"\n)#\n)$\n)%\n(&&\n(&&\n(&&\n(!&\n(!&\n(!&\n(!&\n(&'\n(&'\n(&'\n(&'\n(a) One-to-many\n(b) Many-to-one\n(c) Many-to-many\nFigure 8: Vanilla RNNs with diﬀerent inputs/outputs settings. (a) has one input but multiple outputs; (b)\nhas multiple inputs but one output; (c) has multiple inputs and outputs. Note that the parameters are\nshared across time steps.\nmodels for complex tasks. Here we introduce vanilla RNNs and improved variants such as long short-term\nmemory (LSTM).\n3.2.1\nVanilla RNNs\nSuppose we have general time series inputs x1, x2, . . . , xT . A vanilla RNN models the “hidden state” at time\nt by a vector ht, which is subject to the recursive formula\nht = f θ(ht−1, xt).\n(13)\nHere, fθ is generally a nonlinear function parametrized by θ. Concretely, a vanilla RNN with one hidden\nlayer has the following form5\nht = tanh (Whhht−1 + Wxhxt + bh) ,\nwhere tanh(a) = e2a−1\ne2a+1,\nzt = σ (Whyht + bz) ,\nwhere Whh, Wxh, Why are trainable weight matrices, bh, bz are trainable bias vectors, and zt is the output\nat time t. Like many classical time series models, those parameters are shared across time. Note that in\ndiﬀerent applications, we may have diﬀerent input/output settings (cf. Figure 8). Examples include\n• One-to-many: a single input with multiple outputs; see Figure 8(a). A typical application is image\ncaptioning, where the input is an image and outputs are a series of words.\n• Many-to-one: multiple inputs with a single output; see Figure 8(b). One application is text sentiment\nclassiﬁcation, where the input is a series of words in a sentence and the output is a label (e.g., positive\nvs. negative).\n• Many-to-many: multiple inputs and outputs; see Figure 8(c). This is adopted in machine translation,\nwhere inputs are words of a source language (say Chinese) and outputs are words of a target language\n(say English).\nAs the case with feed-forward neural nets, we minimize a loss function using back-propagation, where\nthe loss is typically\nℓT (θ) =\nX\nt∈T\nL(yt, zt) = −\nX\nt∈T\nK\nX\nk=1\n1{yt = k} log\n\u0012\nexp([zt]k)\nP\nk exp([zt]k)\n\u0013\n,\nwhere K is the number of categories for classiﬁcation (e.g., size of the vocabulary in machine translation),\nand T ⊂[T] is the length of the output sequence. During the training, the gradients ∂ℓT /∂ht are computed\nin the reverse time order (from T to t). For this reason, the training process is often called back-propagation\nthrough time.\n5Similar to the activation function σ(·), the function tanh(·) means element-wise operations.\n11\n!\"\n#\n!\"\n#$%\n!\"$%\n#\ntime\ndepth\nFigure 9: A vanilla RNN with two hidden layers. Higher-level hidden states hℓ\nt are determined by the old\nstates hℓ\nt−1 and lower-level hidden states hℓ−1\nt\n. Multilayer RNNs generalize both feed-forward neural nets\nand one-hidden-layer RNNs.\nOne notable drawback of vanilla RNNs is that, they have diﬃculty in capturing long-range dependencies\nin sequence data when the length of the sequence is large. This is sometimes due to the phenomenon of\nexploding / vanishing gradients. Take Figure 8(c) as an example. Computing ∂ℓT /∂h1 involves the product\nQ3\nt=1(∂ht+1/∂ht) by the chain rule. However, if the sequence is long, the product will be the multiplication\nof many Jacobian matrices, which usually results in exponentially large or small singular values. To alleviate\nthis issue, in practice, the forward pass and backward pass are implemented in a shorter sliding window\n{t1, t1 + 1, . . . , t2}, instead of the full sequence {1, 2, . . . , T}. Though eﬀective in some cases, this technique\nalone does not fully address the issue of long-term dependency.\n3.2.2\nGRUs and LSTM\nThere are two improved variants that alleviate the above issue: gated recurrent units (GRUs) [26] and long\nshort-term memory (LSTM) [54].\n• A GRU reﬁnes the recursive formula (13) by introducing gates, which are vectors of the same length as\nht. The gates, which take values in [0, 1] elementwise, multiply with ht−1 elementwise and determine how\nmuch they keep the old hidden states.\n• An LSTM similarly uses gates in the recursive formula. In addition to ht, an LSTM maintains a cell\nstate, which takes values in R elementwise and are analogous to counters.\nHere we only discuss LSTM in detail. Denote by ⊙the element-wise multiplication. We have a recursive\nformula in replace of (13):\n\n\n\n\nit\nf t\not\ngt\n\n\n\n=\n\n\n\n\nσ\nσ\nσ\ntanh\n\n\n\nW\n\n\nht−1\nxt\n1\n\n,\nct = f t ⊙ct−1 + it ⊙gt,\nht = ot ⊙tanh(ct),\nwhere W is a big weight matrix with appropriate dimensions. The cell state vector ct carries information of\nthe sequence (e.g., singular/plural form in a sentence). The forget gate f t determines how much the values\nof ct−1 are kept for time t, the input gate it controls the amount of update to the cell state, and the output\ngate ot gives how much ct reveals to ht. Ideally, the elements of these gates have nearly binary values.\nFor example, an element of f t being close to 1 may suggest the presence of a feature in the sequence data.\nSimilar to the skip connections in residual nets, the cell state ct has an additive recursive formula, which\nhelps back-propagation and thus captures long-range dependencies.\n12\n3.2.3\nMultilayer RNNs\nMultilayer RNNs are generalization of the one-hidden-layer RNN discussed above. Figure 9 shows a vanilla\nRNN with two hidden layers. In place of (13), the recursive formula for an RNN with L hidden layers now\nreads\nhℓ\nt = tanh\n\nWℓ\n\n\nhℓ−1\nt\nhℓ\nt−1\n1\n\n\n\n,\nfor all ℓ∈[L],\nh0\nt ≜xt.\nNote that a multilayer RNN has two dimensions: the sequence length T and depth L. Two special cases are\nthe feed-forward neural nets (where T = 1) introduced in Section 2, and RNNs with one hidden layer (where\nL = 1). Multilayer RNNs usually do not have very large depth (e.g., 2–5), since T is already very large.\nFinally, we remark that CNNs, RNNs, and other neural nets can be easily combined to tackle tasks\nthat involve diﬀerent sources of input data. For example, in image captioning, the images are ﬁrst processed\nthrough a CNN, and then the high-level features are fed into an RNN as inputs. Theses neural nets combined\ntogether form a large computational graph, so they can be trained using back-propagation. This generic\ntraining method provides much ﬂexibility in various applications.\n3.3\nModules\nDeep neural nets are essentially composition of many nonlinear functions. A component function may be\ndesigned to have speciﬁc properties in a given task, and it can be itself resulted from composing a few\nsimpler functions. In LSTM, we have seen that the building block consists of several intermediate variables,\nincluding cell states and forget gates that can capture long-term dependency and alleviate numerical issues.\nThis leads to the idea of designing modules for building more complex neural net models. Desirable\nmodules usually have low computational costs, alleviate numerical issues in training, and lead to good\nstatistical accuracy. Since modules and the resulting neural net models form computational graphs, training\nfollows the same principle brieﬂy described in Section 2.\nHere, we use the examples of Inception and skip connections to illustrate the ideas behind modules.\nFigure 10(a) is an example of “Inception” modules used in GoogleNet [123]. As before, all the convolutional\nlayers are followed by the ReLU activation function. The concatenation of information from ﬁlters with\ndiﬀerent sizes give the model great ﬂexibility to capture spatial information. Note that 1 × 1 ﬁlters is an\n1 × 1 × d3 tensor (where d3 is the number of feature maps), so its convolutional operation does not interact\nwith other spatial coordinates, only serving to aggregate information from diﬀerent feature maps at the same\ncoordinate. This reduces the number of parameters and speeds up the computation. Similar ideas appear\nin other work [78, 57].\n1×'2\n3456\n1×'2\n3456\n1×'2\n3456\n1×'2\n3456\n3×72\n3456\n5×82\n3456\n3×72\nPOOL\nconcat\n3×94\n5678\n3×94\n5678\n+\n$\n$\n;($)\n(a) “Inception” module\n(b) Skip connections\nFigure 10: (a) The “Inception” module from GoogleNet. Concat means combining all features maps into a\ntensor. (b) Skip connections are added every two layers in ResNets.\nAnother module, usually called skip connections, is widely used to alleviate numerical issues in very deep\nneural nets, with additional beneﬁts in optimization eﬃciency and statistical accuracy. Training very deep\n13\nneural nets are generally more diﬃcult, but the introduction of skip connections in residual networks [50, 51]\nhas greatly eased the task.\nThe high level idea of skip connections is to add an identity map to an existing nonlinear function. Let\nF(x) be an arbitrary nonlinear function represented by a (fragment of) neural net, then the idea of skip\nconnections is simply replacing F(x) with x+F(x). Figure 10(b) shows a well-known structure from residual\nnetworks [50]—for every two layers, an identity map is added:\nx 7−→σ(x + F(x)) = σ(x + W′σ(Wx + b) + b′),\n(14)\nwhere x can be hidden nodes from any layer and W, W′, b, b′ are corresponding parameters. By repeating\n(namely composing) this structure throughout all layers, [50, 51] are able to train neural nets with hundreds\nof layers easily, which overcomes well-observed training diﬃculties in deep neural nets.\nMoreover, deep\nresidual networks also improve statistical accuracy, as the classiﬁcation error on ImageNet challenge was\nreduced by 46% from 2014 to 2015. As a side note, skip connections can be used ﬂexibly. They are not\nrestricted to the form in (14), and can be used between any pair of layers ℓ, ℓ′ [55].\n4\nDeep unsupervised learning\nIn supervised learning, given labelled training set {(yi, xi)}, we focus on discriminative models, which essen-\ntially represents P(y | x) by a deep neural net f(x; θ) with parameters θ. Unsupervised learning, in contrast,\naims at extracting information from unlabeled data {xi}, where the labels {yi} are absent. In regard to this\ninformation, it can be a low-dimensional embedding of the data {xi} or a generative model with latent vari-\nables to approximate the distribution PX(x). To achieve these goals, we introduce two popular unsupervised\ndeep leaning models, namely, autoencoders and generative adversarial networks (GANs). The ﬁrst one can\nbe viewed as a dimension reduction technique, and the second as a density estimation method. DNNs are\nthe key elements for both of these two models.\n4.1\nAutoencoders\nRecall that in dimension reduction, the goal is to reduce the dimensionality of the data and at the same time\npreserve its salient features. In particular, in principal component analysis (PCA), the goal is to embed the\ndata {xi}1≤i≤n into a low-dimensional space via a linear function f such that maximum variance can be\nexplained. Equivalently, we want to ﬁnd linear functions f : Rd →Rk and g : Rk →Rd (k ≤d) such that\nthe diﬀerence between xi and g(f(xi)) is minimized. Formally, we let\nf (x) = Wfx ≜h\nand\ng (h) = Wgh,\nwhere\nWf ∈Rk×d and Wg ∈Rd×k.\nHere, for simplicity, we assume that the intercept/bias terms for f and g are zero. Then, PCA amounts to\nminimizing the quadratic loss function\nminimizeWf ,Wg\n1\nn\nn\nX\ni=1\n∥xi −WfWgxi∥2\n2 .\n(15)\nIt is the same as minimizing ∥X −WX∥2\nF subject to rank(W) ≤k, where X ∈Rp×n is the design matrix.\nThe solution is given by the singular value decomposition of X [44, Thm. 2.4.8], which is exactly what PCA\ndoes. It turns out that PCA is a special case of autoencoders, which is often known as the undercomplete\nlinear autoencoder.\nMore broadly, autoencoders are neural network models for (nonlinear) dimension reduction, which gen-\neralize PCA. An autoencoder has two key components, namely, the encoder function f(·), which maps the\ninput x ∈Rd to a hidden code/representation h ≜f(x) ∈Rk, and the decoder function g(·), which maps\nthe hidden representation h to a point g(h) ∈Rd. Both functions can be multilayer neural networks as\n(3). See Figure 11 for an illustration of autoencoders. Let L(x1, x2) be a loss function that measures the\ndiﬀerence between x1 and x2 in Rd. Similar to PCA, an autoencoder is used to ﬁnd the encoder f and\n14\nhidden layer\noutput layer\ninput layer\nh = f (x)\nx\ng (h)\nx\nencoder\ndecoder\nL (x, g (h))\nFigure 11: First an input x goes through the decoder f(·), and we obtain its hidden representation h = f(x).\nThen, we use the decoder g(·) to get g(h) as a reconstruction of x. Finally, the loss is determined from the\ndiﬀerence between the original input x and its reconstruction g(f(x)).\ndecoder g such that L(x, g(f(x))) is as small as possible. Mathematically, this amounts to solving the\nfollowing minimization problem\nminimizef,g\n1\nn\nn\nX\ni=1\nL (xi, g (hi))\nwith\nhi = f (xi) ,\nfor all i ∈[n].\n(16)\nOne needs to make structural assumptions on the functions f and g in order to ﬁnd useful representations\nof the data, which leads to diﬀerent types of autoencoders. Indeed, if no assumption is made, choosing f and\ng to be identity functions clearly minimizes the above optimization problem. To avoid this trivial solution,\none natural way is to require that the encoder f maps the data onto a space with a smaller dimension,\ni.e., k < d. This is the undercomplete autoencoder that includes PCA as a special case. There are other\nstructured autoencoders which add desired properties to the model such as sparsity or robustness, mainly\nthrough regularization terms. Below we present two other common types of autoencoders.\n• Sparse autoencoders.\nOne may believe that the dimension k of the hidden code hi is larger than the\ninput dimension d, and that hi admits a sparse representation. As with LASSO [126] or SCAD [36], one\nmay add a regularization term to the reconstruction loss L in (16) to encourage sparsity [98]. A sparse\nautoencoder solves\nminf,g\n1\nn\nn\nX\ni=1\nL (xi, g (hi))\n|\n{z\n}\nloss\n+ λ ∥hi∥1\n| {z }\nregularizer\nwith\nhi = f (xi) , for all i ∈[n].\nThis is similar to dictionary learning, where one aims at ﬁnding a sparse representation of input data on\nan overcomplete basis. Due to the imposed sparsity, the model can potentially learn useful features of the\ndata.\n• Denoising autoencoders.\nOne may hope that the model is robust to noise in the data: even if the\ninput data xi are corrupted by small noise ξi or miss some components (the noise level or the missing\nprobability is typically small), an ideal autoencoder should faithfully recover the original data. A denoising\nautoencoder [128] achieves this robustness by explicitly building a noisy data ˜xi = xi+ξi as the new input,\n15\nsource distribution\ntraining samples\nn PZ\n{xi}1in\nsample\nsample\nxi\nz\nG\nD\ng (z)\n1: real\n0: fake\nd (·)\nFigure 12: GANs consist of two components, a generator G which generates fake samples and a discriminator\nD which diﬀerentiate the true ones from the fake ones.\nand then solves an optimization problem similar to (16) where L (xi, g (hi)) is replaced by L (xi, g (f(˜xi))).\nA denoising autoencoder encourages the encoder/decoder to be stable in the neighborhood of an input,\nwhich is generally a good statistical property. An alternative way could be constraining f and g in the\noptimization problem, but that would be very diﬃcult to optimize. Instead, sampling by adding small\nperturbations in the input provides a simple implementation. We shall see similar ideas in Section 6.3.3.\n4.2\nGenerative adversarial networks\nGiven unlabeled data {xi}1≤i≤n, density estimation aims to estimate the underlying probability density\nfunction PX from which the data is generated. Both parametric and nonparametric estimators [115] have\nbeen proposed and studied under various assumptions on the underlying distribution. Diﬀerent from these\nclassical density estimators, where the density function is explicitly deﬁned in relatively low dimension,\ngenerative adversarial networks (GANs) [46] can be categorized as an implicit density estimator in much\nhigher dimension. The reasons are twofold: (1) GANs put more emphasis on sampling from the distribution\nPX than estimation; (2) GANs deﬁne the density estimation implicitly through a source distribution PZ and\na generator function g(·), which is usually a deep neural network. We introduce GANs from the perspective\nof sampling from PX and later we will generalize the vanilla GANs using its relation to density estimators.\n4.2.1\nSampling view of GANs\nSuppose the data {xi}1≤i≤n at hand are all real images, and we want to generate new natural images.\nWith this goal in mind, GAN models a zero-sum game between two players, namely, the generator G and\nthe discriminator D.\nThe generator G tries to generate fake images akin to the true images {xi}1≤i≤n\nwhile the discriminator D aims at diﬀerentiating the fake ones from the true ones. Intuitively, one hopes to\nlearn a generator G to generate images where the best discriminator D cannot distinguish. Therefore the\npayoﬀis higher for the generator G if the probability of the discriminator D getting wrong is higher, and\ncorrespondingly the payoﬀfor the discriminator correlates positively with its ability to tell wrong from truth.\nMathematically, the generator G consists of two components, an source distribution PZ (usually a stan-\ndard multivariate Gaussian distribution with hundreds of dimensions) and a function g(·) which maps a\nsample z from PZ to a point g(z) living in the same space as x. For generating images, g(z) would be a\n3D tensor. Here g(z) is the fake sample generated from G. Similarly the discriminator D is composed of\none function which takes an image x (real or fake) and return a number d(x) ∈[0, 1], the probability of x\nbeing a real sample from PX or not. Oftentimes, both the generating function g(·) and the discriminating\nfunction d(·) are realized by deep neural networks, e.g., CNNs introduced in Section 3.1. See Figure 12 for\nan illustration for GANs. Denote θG and θD the parameters in g(·) and d(·), respectively. Then GAN tries\nto solve the following min-max problem:\n16\nmin\nθG max\nθD\nEx∼PX [log (d (x))] + Ez∼PZ [log (1 −d (g (z)))] .\n(17)\nRecall that d(x) models the belief / probability that the discriminator thinks that x is a true sample. Fix\nthe parameters θG and hence the generator G and consider the inner maximization problem. We can see\nthat the goal of the discriminator is to maximize its ability of diﬀerentiation. Similarly, if we ﬁx θD (and\nhence the discriminator), the generator tries to generate more realistic images g(z) to fool the discriminator.\n4.2.2\nDensity estimation view of GANs\nLet us now take a density-estimation view of GANs. Fixing the source distribution PZ, any generator G\ninduces a distribution PG over the space of images. Removing the restrictions on d(·), one can then rewrite\n(17) as\nmin\nPG max\nd(·)\nEx∼PX [log (d (x))] + Ex∼PG [log (1 −d (x))] .\n(18)\nObserve that the inner maximization problem is solved by the likelihood ratio, i.e.\nd∗(x) =\nPX (x)\nPX (x) + PG (x).\nAs a result, (18) can be simpliﬁed as\nmin\nPG\nJS (PX ∥PG) ,\n(19)\nwhere JS(·∥·) denotes the Jensen–Shannon divergence between two distributions\nJS (PX∥PG) = 1\n2KL\n\u0000PX ∥PX+PG\n2\n\u0001\n+ 1\n2KL\n\u0000PG ∥PX+PG\n2\n\u0001\n.\nIn words, the vanilla GAN (17) seeks a density PG that is closest to PX in terms of the Jensen–Shannon di-\nvergence. This view allows to generalize GANs to other variants, by changing the distance metric. Examples\ninclude f-GAN [90], Wasserstein GAN (W-GAN) [6], MMD GAN [75], etc. We single out the Wasserstein\nGAN (W-GAN) [6] to introduce due to its popularity. As the name suggests, it minimizes the Wasserstein\ndistance between PX and PG:\nmin\nθG\nWS (PX∥PG)\n=\nmin\nθG\nsup\nf:f 1-Lipschitz\nEx∼PX [f (x)] −Ex∼PG [f (x)] ,\n(20)\nwhere f(·) is taken over all Lipschitz functions with coeﬃcient 1. Comparing W-GAN (20) with the original\nformulation of GAN (17), one ﬁnds that the Lipschitz function f in (20) corresponds to the discriminator D\nin (17) in the sense that they share similar objectives to diﬀerentiate the true distribution PX from the fake\none PG. In the end, we would like to mention that GANs are more diﬃcult to train than supervised deep\nlearning models such as CNNs [110]. Apart from the training diﬃculty, how to evaluate GANs objectively\nand eﬀectively is an ongoing research.\n5\nRepresentation power: approximation theory\nHaving seen the building blocks of deep learning models in the previous sections, it is natural to ask: what is\nthe beneﬁts of composing multiple layers of nonlinear functions. In this section, we address this question from\na approximation theoretical point of view. Mathematically, letting H be the space of functions representable\nby neural nets (NNs), how well can a function f (with certain properties) be approximated by functions in\nH. We ﬁrst revisit universal approximation theories, which are mostly developed for shallow neural nets\n(neural nets with a single hidden layer), and then provide recent results that demonstrate the beneﬁts of\ndepth in neural nets. Other notable works include Kolmogorov-Arnold superposition theorem [7, 120], and\ncircuit complexity for neural nets [91].\n17\n5.1\nUniversal approximation theory for shallow NNs\nThe universal approximation theories study the approximation of f in a space F by a function represented\nby a one-hidden-layer neural net\ng(x) =\nN\nX\nj=1\ncjσ∗(w⊤\nj x −bj),\n(21)\nwhere σ∗: R →R is certain activation function and N is the number of hidden units in the neural net. For\ndiﬀerent space F and activation function σ∗, there are upper bounds and lower bounds on the approximation\nerror ∥f −g∥. See [93] for a comprehensive overview. Here we present representative results.\nFirst, as N →∞, any continuous function f can be approximated by some g under mild conditions.\nLoosely speaking, this is because each component σ∗(w⊤\nj x −bj) behaves like a basis function and functions\nin a suitable space F admits a basis expansion. Given the above heuristics, the next natural question is:\nwhat is the rate of approximation for a ﬁnite N?\nLet us restrict the domain of x to a unit ball Bd in Rd. For p ∈[1, ∞) and integer m ≥1, consider the\nLp space and the Sobolev space with standard norms\n∥f∥p =\nh Z\nBn |g(x)|p dx\ni1/p\n,\n∥f∥m,p =\nh\nX\n0≤|k|≤m\n∥Dkf∥p\np\ni1/p\n,\nwhere Dkf denotes partial derivatives indexed by k ∈Zd\n+. Let F ≜Fm\np be the space of functions f in the\nSobolev space with ∥f∥m,p ≤1. Note that functions in F have bounded derivatives up to m-th order, and\nthat smoothness of functions is controlled by m (larger m means smoother). Denote by HN the space of\nfunctions with the form (21). The following general upper bound is due to [85].\nTheorem 1 (Theorem 2.1 in [85]). Assume σ∗: R →R is such that σ∗has arbitrary order derivatives in an\nopen interval I, and that σ∗is not a polynomial on I. Then, for any p ∈[1, ∞), d ≥2, and integer m ≥1,\nsup\nf∈Fm\np\ninf\ng∈HN\n∥f −g∥p ≤Cd,m,p N −m/d,\nwhere Cd,m,p is independent of N, the number of hidden units.\nIn the above theorem, the condition on σ∗(·) is mainly technical. This upper bound is useful when the\ndimension d is not large. It clearly implies that the one-hidden-layer neural net is able to approximate any\nsmooth function with enough hidden units. However, it is unclear how to ﬁnd a good approximator g; nor\ndo we have control over the magnitude of the parameters (huge weights are impractical). While increasing\nthe number of hidden units N leads to better approximation, the exponent −m/d suggests the presence of\nthe curse of dimensionality. The following (nearly) matching lower bound is stated in [80].\nTheorem 2 (Theorem 5 in [80]). Let p ≥1, m ≥1 and N ≥2. If the activation function is the standard\nsigmoid function σ(t) = (1 + e−t)−1, then\nsup\nf∈Fm\np\ninf\ng∈HN\n∥f −g∥p ≥C′\nd,m,p (N log N)−m/d,\n(22)\nwhere C′\nd,m,p is independent of N.\nResults for other activation functions are also obtained by [80]. Moreover, the term log N can be removed\nif we assume an additional continuity condition [85].\nFor the natural space Fm\np of smooth functions, the exponential dependence on d in the upper and lower\nbounds may look unappealing. However, [12] showed that for a diﬀerent function space, there is a good\ndimension-free approximation by the neural nets.\nSuppose that a function f : Rd 7→R has a Fourier\nrepresentation\nf(x) =\nZ\nRd ei⟨ω,x⟩˜f(ω) dω,\n(23)\n18\nwhere ˜f(ω) ∈C. Assume that f(0) = 0 and that the following quantity is ﬁnite\nCf =\nZ\nRd ∥ω∥2| ˜f(ω)| dω.\n(24)\n[12] uncovers the following dimension-free approximation guarantee.\nTheorem 3 (Proposition 1 in [12]). Fix a C > 0 and an arbitrary probability measure µ on the unit ball Bd\nin Rd. For every function f with Cf ≤C and every N ≥1, there exists some g ∈HN such that\n\u0014Z\nBd(f(x) −g(x))2 µ(dx)\n\u00151/2\n≤2C\n√\nN\n.\nMoreover, the coeﬃcients of g may be restricted to satisfy PN\nj=1 |cj| ≤2C.\nThe upper bound is now independent of the dimension d. However, Cf may implicitly depend on d, as\nthe formula in (24) involves an integration over Rd (so for some functions Cf may depend exponentially\non d). Nevertheless, this theorem does characterize an interesting function space with an improved upper\nbound. Details of the function space are discussed by [12]. This theorem can be generalized; see [81] for an\nexample.\nTo help understand why a dimensionality-free approximation holds, let us appeal to a heuristic argument\ngiven by Monte Carlo simulations. It is well-known that Monte Carlo approximation errors are independent\nof dimensionality in evaluation of high-dimensional integrals. Let us generate {ωj}1≤j≤N randomly from a\ngiven density p(·) in Rd. Consider the approximation to (23) by\ngN(x) = 1\nN\nN\nX\nj=1\ncjei⟨ωj,x⟩,\ncj =\n˜f(ωj)\np(ωj) .\n(25)\nThen, gN(x) is a one-hidden-layer neural network with N units and the sinusoid activation function. Note\nthat EgN(x) = f(x), where the expectation is taken with respect to randomness {ωj}. Now, by indepen-\ndence, we have\nE(gN(x) −f(x))2 = 1\nN Var(cjei⟨ωj,x⟩) ≤1\nN Ec2\nj,\nif Ec2\nj < ∞. Therefore, the rate is independent of the dimensionality d, though the constant can be.\n5.2\nApproximation theory for multi-layer NNs\nThe approximation theory for multilayer neural nets is less understood compared with neural nets with one\nhidden layer. Driven by the success of deep learning, there are many recent papers focusing on expressivity\nof deep neural nets. As studied by [125, 35, 84, 94, 15, 111, 77, 103], deep neural nets excel at representing\ncomposition of functions. This is perhaps not surprising, since deep neural nets are themselves deﬁned by\ncomposing layers of functions. Nevertheless, it points to a new territory rarely studied in statistics before.\nBelow we present a result based on [77, 103].\nSuppose that the inputs x have a bounded domain [−1, 1]d for simplicity. As before, let σ∗: R →R be a\ngeneric function, and σ∗= (σ∗, · · · , σ∗)⊤be element-wise application of σ∗. Consider a neural net which is\nsimilar to (3) but with scaler output: g(x) = Wℓσ∗(· · · σ∗(W2σ∗(W1x)) · · · ). A unit or neuron refers to an\nelement of vectors σ∗(Wk · · · σ∗(W2σ∗(W1x)) · · · ) for any k = 1, . . . , ℓ−1. For a multivariate polynomial\np, deﬁne mk(p) to be the smallest integer such that, for any ϵ > 0, there exists a neural net g(x) satisfying\nsupx |p(x) −g(x)| < ϵ, with k hidden layers (i.e., ℓ= k + 1) and no more than mk(p) neurons in total.\nEssentially, mk(p) is the minimum number of neurons required to approximate p arbitrarily well.\nTheorem 4 (Theorem 4.1 in [103]). Let p(x) be a monomial xr1\n1 xr2\n2 · · · xrd\nd with q = Pd\nj=1 rj. Suppose that\nσ∗has derivatives of order 2q at the origin, and that they are nonzero. Then,\n(i) m1(p) = Qd\nj=1(rj + 1);\n(ii) mink mk(p) ≤Pd\nj=1 (7⌈log2(rj)⌉+ 4).\n19\nThis theorem reveals a sharp distinction between shallow networks (one hidden layer) and deep networks.\nTo represent a monomial function, a shallow network requires exponentially many neurons in terms of the\ndimension d, whereas linearly many neurons suﬃce for a deep network (with bounded rj). The exponential\ndependence on d, as shown in Theorem 4(i), is resonant with the curse of dimensionality widely seen in\nmany ﬁelds; see [30]. One may ask: how does depth help? Depth circumvents this issue, at least for certain\nfunctions, by allowing us to represent function composition eﬃciently. Indeed, Theorem 4(ii) oﬀers a nice\nresult with clear intuitions: it is known that the product of two scalar inputs can be represented using 4\nneurons [77], so by composing multiple products, we can express monomials with O(d) neurons.\nRecent advances in nonparametric regressions also support the idea that deep neural nets excel at repre-\nsenting composition of functions [15, 111]. In particular, [15] considered the nonparametric regression setting\nwhere we want to estimate a function ˆfn(x) from i.i.d. data Dn = {(yi, xi)}1≤i≤n. If the true regression\nfunction f(x) has certain hierarchical structure with intrinsic dimensionality6 d∗, then the error\nEDnEx\n\f\f\f ˆfn(x) −f(x)\n\f\f\f\n2\nhas an optimal minimax convergence rate O(n−\n2q\n2q+d∗), rather than the usual rate O(n−\n2q\n2q+d ) that depends on\nthe ambient dimension d. Here q is the smoothness parameter. This provides another justiﬁcation for deep\nneural nets: if data are truly hierarchical, then the quality of approximators by deep neural nets depends on\nthe intrinsic dimensionality, which avoids the curse of dimensionality.\nWe point out that the approximation theory for deep learning is far from complete. For example, in\nTheorem 4, the condition on σ∗excludes the widely used ReLU activation function, there are no constraints\non the magnitude of the weights (so they can be unreasonably large).\n6\nTraining deep neural nets\nThe existence of a good function approximator in the NN function class does not explain why in practice\nwe can easily ﬁnd them. In this section, we introduce standard methods, namely stochastic gradient descent\n(SGD) and its variants, to train deep neural networks (or to ﬁnd such a good approximator). As with many\nstatistical machine learning tasks, training DNNs follows the empirical risk minimization (ERM) paradigm\nwhich solves the following optimization problem\nminimizeθ∈Rp\nℓn (θ) ≜1\nn\nn\nX\ni=1\nL (f (xi; θ) , yi) .\n(26)\nHere L(f(xi; θ), yi) measures the discrepancy between the prediction f(xi; θ) of the neural network and the\ntrue label yi. Correspondingly, denote by ℓ(θ) ≜E(x,y)∼D[L(f(x; θ), y)] the out-of-sample error, where D\nis the joint distribution over (y, x). Solving ERM (26) for deep neural nets faces various challenges that\nroughly fall into the following three categories.\n• Scalability and nonconvexity. Both the sample size n and the number of parameters p can be huge for\nmodern deep learning applications, as we have seen in Table 1. Many optimization algorithms are not\npractical due to the computational costs and memory constraints.\nWhat is worse, the empirical loss\nfunction ℓn(θ) in deep learning is often nonconvex.\nIt is a priori not clear whether an optimization\nalgorithm can drive the empirical loss (26) small.\n• Numerical stability. With a large number of layers in DNNs, the magnitudes of the hidden nodes can be\ndrastically diﬀerent, which may result in the “exploding gradients” or “vanishing gradients” issue during\nthe training process.\nThis is because the recursive relations across layers often lead to exponentially\nincreasing / decreasing values in both forward passes and backward passes.\n• Generalization performance. Our ultimate goal is to ﬁnd a parameter ˆθ such that the out-of-sample error\nℓ(ˆθ) is small. However, in the over-parametrized regime where p is much larger than n, the underlying\n6Roughly speaking, the true regression function can be represented by a tree where each node has at most d∗children.\nSee [15] for the precise deﬁnition.\n20\nneural network has the potential to ﬁt the training data perfectly while performing poorly on the test\ndata. To avoid this overﬁtting issue, proper regularization, whether explicit or implicit, is needed in the\ntraining process for the neural nets to generalize.\nIn the following three subsections, we discuss practical solutions / proposals to address these challenges.\n6.1\nStochastic gradient descent\nStochastic gradient descent (SGD) [101] is by far the most popular optimization algorithm to solve ERM (26)\nfor large-scale problems. It has the following simple update rule:\nθt+1 = θt −ηtG(θt)\nwith\nG\n\u0000θt\u0001\n= ∇L\n\u0000f\n\u0000xit; θt\u0001\n, yit\n\u0001\n(27)\nfor t = 0, 1, 2, . . ., where ηt > 0 is the step size (or learning rate), θ0 ∈Rp is an initial point and it is\nchosen randomly from {1, 2, · · · , n}. It is easy to verify that G(θt) is an unbiased estimate of ∇ℓn(θt). The\nadvantage of SGD is clear: compared with gradient descent, which goes over the entire dataset in every\nupdate, SGD uses a single example in each update and hence is considerably more eﬃcient in terms of both\ncomputation and memory (especially in the ﬁrst few iterations).\nApart from practical beneﬁts of SGD, how well does SGD perform theoretically in terms of minimizing\nℓn(θ)? We begin with the convex case, i.e., the case where the loss function is convex w.r.t. θ. It is well\nunderstood in literature that with proper choices of the step sizes {ηt}, SGD is guaranteed to achieve both\nconsistency and asymptotic normality.\n• Consistency. If ℓ(θ) is a strongly convex function7, then under some mild conditions8, learning rates that\nsatisfy\n∞\nX\nt=0\nηt = +∞\nand\n∞\nX\nt=0\nη2\nt < +∞\n(28)\nguarantee almost sure convergence to the unique minimizer θ∗≜argminθℓ(θ), i.e., θt a.s.\n−−→θ∗as t →\n∞[101, 64, 16, 69]. The requirements in (28) can be viewed from the perspective of bias-variance tradeoﬀ:\nthe ﬁrst condition ensures that the iterates can reach the minimizer (controlled bias), and the second\nensures that stochasticity does not prevent convergence (controlled variance).\n• Asymptotic normality. It is proved by [97] that for robust linear regression with ﬁxed dimension p, under\nthe choice ηt = t−1,\n√\nt (θt −θ∗) is asymptotically normal under some regularity conditions (but θt is not\nasymptotically eﬃcient in general). Moreover, by averaging the iterates of SGD, [96] proved that even\nwith a larger step size ηt ∝t−α, α ∈(1/2, 1), the averaged iterate ¯θ\nt = t−1 Pt\ns=1 θs is asymptotic eﬃcient\nfor robust linear regression. These strong results show that SGD with averaging performs as well as the\nMLE asymptotically, in addition to its computational eﬃciency.\nThese classical results, however, fail to explain the eﬀectiveness of SGD when dealing with nonconvex\nloss functions in deep learning. Admittedly, ﬁnding global minima of nonconvex functions is computationally\ninfeasible in the worst case. Nevertheless, recent work [4, 32] bypasses the worst case scenario by focusing\non losses incurred by over-parametrized deep learning models. In particular, they show that (stochastic)\ngradient descent converges linearly towards the global minimizer of ℓn(θ) as long as the neural network is\nsuﬃciently over-parametrized. This phenomenon is formalized below.\nTheorem 5 (Theorem 2 in [4]). Let {(yi, xi)}1≤i≤n be a training set satisfying mini,j:i̸=j ∥xi−xj∥2 ≥δ > 0.\nConsider ﬁtting the data using a feed-forward neural network (1) with ReLU activations.\nDenote by L\n(resp. W) the depth (resp. width) of the network.\nSuppose that the neural network is suﬃciently over-\nparametrized, i.e.,\nW ≫poly\n\u0012\nn, L, 1\nδ\n\u0013\n,\n(29)\n7For results on consistency and asymptotic normality, we consider the case where in each step of SGD, the stochastic\ngradient is computed using a fresh sample (y, x) from D. This allows to view SGD as an optimization algorithm to minimize\nthe population loss ℓ(θ).\n8One example of such condition can be constraining the second moment of the gradients: E\n\u0002\n∥∇L\n\u0000xi, yi; θt\u0001\n∥2\n2\n\u0003\n≤C1 +\nC2∥θt −θ∗∥2\n2 for some C1, C2 > 0. See [16] for details.\n21\nwhere poly means a polynomial function. Then with high probability, running SGD (27) with certain random\ninitialization and properly chosen step sizes yields ℓn(θt) ≤ε in t ≍log 1\nε iterations.\nTwo notable features are worth mentioning: (1) ﬁrst, the network under consideration is suﬃciently over-\nparametrized (cf. (29)) in which the number of parameters is much larger than the number of samples, and\n(2) one needs to initialize the weight matrices to be in near-isometry such that the magnitudes of the hidden\nnodes do not blow up or vanish. In a nutshell, over-parametrization and random initialization together\nensure that the loss function (26) has a benign landscape9 around the initial point, which in turn implies\nfast convergence of SGD iterates.\nThere are certainly other challenges for vanilla SGD to train deep neural nets: (1) training algorithms\nare often implemented in GPUs, and therefore it is important to tailor the algorithm to the infrastructure,\n(2) the vanilla SGD might converge very slowly for deep neural networks, albeit good theoretical guarantees\nfor well-behaved problems, and (3) the learning rates {ηt} can be diﬃcult to tune in practice. To address\nthe aforementioned challenges, three important variants of SGD, namely mini-batch SGD, momentum-based\nSGD, and SGD with adaptive learning rates are introduced.\n6.1.1\nMini-batch SGD\nModern computational infrastructures (e.g., GPUs) can evaluate the gradient on a number (say 64) of\nexamples as eﬃciently as evaluating that on a single example. To utilize this advantage, mini-batch SGD\nwith batch size K ≥1 forms the stochastic gradient through K random samples:\nθt+1 = θt −ηtG(θt)\nwith\nG(θt) = 1\nK\nK\nX\nk=1\n∇L\n\u0000f\n\u0000xik\nt ; θt\u0001\n, yik\nt\n\u0001\n,\n(30)\nwhere for each 1 ≤k ≤K, ik\nt is sampled uniformly from {1, 2, · · · , n}.\nMini-batch SGD, which is an\n“interpolation” between gradient descent and stochastic gradient descent, achieves the best of both worlds:\n(1) using 1 ≪K ≪n samples to estimate the gradient, one eﬀectively reduces the variance and hence\naccelerates the convergence, and (2) by taking the batch size K appropriately (say 64 or 128), the stochastic\ngradient G(θt) can be eﬃciently computed using the matrix computation toolboxes on GPUs.\n6.1.2\nMomentum-based SGD\nWhile mini-batch SGD forms the foundation of training neural networks, it can sometimes be slow to converge\ndue to its oscillation behavior [122]. Optimization community has long investigated how to accelerate the\nconvergence of gradient descent, which results in a beautiful technique called momentum methods [95, 88].\nSimilar to gradient descent with moment, momentum-based SGD, instead of moving the iterate θt in the\ndirection of the current stochastic gradient G(θt), smooth the past (stochastic) gradients {G(θt)} to stabilize\nthe update directions. Mathematically, let vt ∈Rp be the direction of update in the tth iteration, i.e.,\nθt+1 = θt −ηtvt.\nHere v0 = G(θ0) and for t = 1, 2, · · ·\nvt = ρvt−1 + G(θt)\n(31)\nwith 0 < ρ < 1. A typical choice of ρ is 0.9. Notice that ρ = 0 recovers the mini-batch SGD (30), where\nno past information of gradients is used. A simple unrolling of vt reveals that vt is actually an exponential\naveraging of the past gradients, i.e., vt = Pt\nj=0 ρt−jG(θj). Compared with vanilla mini-batch SGD, the\ninclusion of the momentum “smoothes” the oscillation direction and accumulates the persistent descent\ndirection. We want to emphasize that theoretical justiﬁcations of momentum in the stochastic setting is not\nfully understood [63, 60].\n9In [4], the loss function ℓn(θ) satisﬁes the PL condition.\n22\n6.1.3\nSGD with adaptive learning rates\nIn optimization, preconditioning is often used to accelerate ﬁrst-order optimization algorithms. In principle,\none can apply this to SGD, which yields the following update rule:\nθt+1 = θt −ηtP −1\nt\nG(θt)\n(32)\nwith Pt ∈Rp×p being a preconditioner at the t-th step.\nNewton’s method can be viewed as one type\nof preconditioning where Pt = ∇2ℓ(θt).\nThe advantages of preconditioning are two-fold: ﬁrst, a good\npreconditioner reduces the condition number by changing the local geometry to be more homogeneous, which\nis amenable to fast convergence; second, a good preconditioner frees practitioners from laboring tuning of the\nstep sizes, as is the case with Newton’s method. AdaGrad, an adaptive gradient method proposed by [33],\nbuilds a preconditioner Pt based on information of the past gradients:\nPt =\nn\ndiag\n\u0010\nt\nX\nj=0\nG\n\u0000θt\u0001\nG\n\u0000θt\u0001⊤\u0011o1/2\n.\n(33)\nSince we only require the diagonal part, this preconditioner (and its inverse) can be eﬃciently computed in\npractice. In addition, investigating (32) and (33), one can see that AdaGrad adapts to the importance of each\ncoordinate of the parameters by setting smaller learning rates for frequent features, whereas larger learning\nrates for those infrequent ones. In practice, one adds a small quantity δ > 0 (say 10−8) to the diagonal\nentries to avoid singularity (numerical underﬂow). A notable drawback of AdaGrad is that the eﬀective\nlearning rate vanishes quickly along the learning process. This is because the historical sum of the gradients\ncan only increase with time. RMSProp [52] is a popular remedy for this problem which incorporates the\nidea of exponential averaging:\nPt =\nn\ndiag\n\u0010\nρPt−1 + (1 −ρ)G\n\u0000θt\u0001\nG\n\u0000θt\u0001⊤\u0011o1/2\n.\n(34)\nAgain, the decaying parameter ρ is usually set to be 0.9. Later, Adam [65, 100] combines the momentum\nmethod and adaptive learning rate and becomes the default training algorithms in many deep learning\napplications.\n6.2\nEasing numerical instability\nFor very deep neural networks or RNNs with long dependencies, training diﬃculties often arise when the val-\nues of nodes have diﬀerent magnitudes or when the gradients “vanish” or “explode” during back-propagation.\nHere we discuss three partial solutions to alleviate this problem.\n6.2.1\nReLU activation function\nOne useful characteristic of the ReLU function is that its derivative is either 0 or 1, and the derivative remains\n1 even for a large input. This is in sharp contrast with the standard sigmoid function (1 + e−t)−1 which\nresults in a very small derivative when inputs have large magnitude. The consequence of small derivatives\nacross many layers is that gradients tend to be “killed”, which means that gradients become approximately\nzero in deep nets.\nThe popularity of the ReLU activation function and its variants (e.g., leaky ReLU) is largely attributable\nto the above reason. It has been well observed that the ReLU activation function has superior training\nperformance over the sigmoid function [68, 79].\n6.2.2\nSkip connections\nWe have introduced skip connections in Section 3.3. Why are skip connections helpful for reducing numerical\ninstability? This structure does not introduce a larger function space, since the identity map can be also\nrepresented with ReLU activations: x = σ(x) −σ(−x).\n23\nOne explanation is that skip connections bring ease to the training / optimization process.\nSuppose\nthat we have a general nonlinear function F(xℓ; θℓ).\nWith a skip connection, we represent the map as\nxℓ+1 = xℓ+ F(xℓ; θℓ) instead. Now the gradient ∂xℓ+1/∂xℓbecomes\n∂xℓ+1\n∂xℓ\n= I + ∂F(xℓ; θℓ)\n∂xℓ\ninstead of\n∂F(xℓ; θℓ)\n∂xℓ\n,\n(35)\nwhere I is an identity matrix. By the chain rule, gradient update requires computing products of many\ncomponents, e.g., ∂xL\n∂x1 = QL−1\nℓ=1\n∂xℓ+1\n∂xℓ, so it is desirable to keep the spectra (singular values) of each component\n∂xℓ+1\n∂xℓ\nclose to 1. In neural nets, with skip connections, this is easily achieved if the parameters have small\nvalues; otherwise, this may not be achievable even with careful initialization and tuning. Notably, training\nneural nets with hundreds of layers is possible with the help of skip connections.\n6.2.3\nBatch normalization\nRecall that in regression analysis, one often standardizes the design matrix so that the features have zero\nmean and unit variance. Batch normalization extends this standardization procedure from the input layer\nto all the hidden layers. Mathematically, ﬁx a mini-batch of input data {(xi, yi)}i∈B, where B ⊂[n]. Let\nh(ℓ)\ni\nbe the feature of the i-th example in the ℓ-th layer (ℓ= 0 corresponds to the input xi). The batch\nnormalization layer computes the normalized version of h(ℓ)\ni\nvia the following steps:\nµ ≜1\n|B|\nX\ni∈B\nh(ℓ)\ni ,\nσ2 ≜1\n|B|\nX\ni∈B\n\u0000h(ℓ)\ni\n−µ\n\u00012\nand\nh(l)\ni,norm ≜h(ℓ)\ni\n−µ\nσ\n.\nHere all the operations are element-wise. In words, batch normalization computes the z-score for each feature\nover the mini-batch B and use that as inputs to subsequent layers. To make it more versatile, a typical batch\nnormalization layer has two additional learnable parameters γ(ℓ) and β(ℓ) such that\nh(l)\ni,new = γ(l) ⊙h(l)\ni,norm + β(l).\nAgain ⊙denotes the element-wise multiplication. As can be seen, γ(ℓ) and β(ℓ) set the new feature h(l)\ninew\nto have mean β(ℓ) and standard deviation γ(ℓ). The introduction of batch normalization makes the training\nof neural networks much easier and smoother. More importantly, it allows the neural nets to perform well\nover a large family of hyper-parameters including the number of layers, the number of hidden units, etc. At\ntest time, the batch normalization layer needs more care. For brevity we omit the details and refer to [58].\n6.3\nRegularization techniques\nSo far we have focused on training techniques to drive the empirical loss (26) small eﬃciently. Here we\nproceed to discuss common practice to improve the generalization power of trained neural nets.\n6.3.1\nWeight decay\nOne natural regularization idea is to add an ℓ2 penalty to the loss function. This regularization technique\nis known as the weight decay in deep learning. We have seen one example in (9). For general deep neural\nnets, the loss to optimize is ℓλ\nn(θ) = ℓn(θ) + rλ(θ) where\nrλ(θ) = λ\nL\nX\nℓ=1\nX\nj,j′\n\u0002\nW (ℓ)\nj,j′\n\u00032.\nNote that the bias (intercept) terms are not penalized. If ℓn(θ) is a least square loss, then regularization\nwith weight decay gives precisely ridge regression. The penalty rλ(θ) is a smooth function and thus it can\nbe also implemented eﬃciently with back-propagation.\n24\n6.3.2\nDropout\nDropout, introduced by [53], prevents overﬁtting by randomly dropping out subsets of features during train-\ning. Take the l-th layer of the feed-forward neural network as an example. Instead of propagating all the\nfeatures in h(ℓ) for later computations, dropout randomly omits some of its entries by\nh(ℓ)\ndrop = h(ℓ) ⊙maskℓ,\nwhere ⊙denotes element-wise multiplication as before, and maskℓis a vector of Bernoulli variables with\nsuccess probability p. It is sometimes useful to rescale the features h(ℓ)\ninv drop = h(ℓ)\ndrop/p, which is called\ninverted dropout. During training, maskℓare i.i.d. vectors across mini-batches and layers. However, when\ntesting on fresh samples, dropout is disabled and the original features h(ℓ) are used to compute the output\nlabel y. It has been nicely shown by [129] that for generalized linear models, dropout serves as adaptive\nregularization. In the simplest case of linear regression, it is equivalent to ℓ2 regularization. Another possible\nway to understand the regularization eﬀect of dropout is through the lens of bagging [45]. Since diﬀerent\nmini-batches has diﬀerent masks, dropout can be viewed as training a large ensemble of classiﬁers at the same\ntime, with a further constraint that the parameters are shared. Theoretical justiﬁcation remains elusive.\n6.3.3\nData augmentation\nData augmentation is a technique of enlarging the dataset when we have knowledge about invariance structure\nof data. It implicitly increases the sample size and usually regularizes the model eﬀectively. For example,\nin image classiﬁcation, we have strong prior knowledge about what invariance properties a good classiﬁer\nshould possess. The label of an image should not be aﬀected by translation, rotation, ﬂipping, and even\ncrops of the image. Hence one can augment the dataset by randomly translating, rotating and cropping the\nimages in the original dataset.\nFormally, during training we want to minimize the loss ℓn(θ) = P\ni L(f(xi; θ), yi) w.r.t. parameters θ,\nand we know a priori that certain transformation T ∈T where T : Rd →Rd (e.g., aﬃne transformation)\nshould not change the category / label of a training sample. In principle, if computation costs were not a\nconsideration, we could convert this knowledge to a constraint fθ(Txi) = fθ(xi), ∀T ∈T in the minimization\nformulation. Instead of solving a constrained optimization problem, data augmentation enlarges the training\ndataset by sampling T ∈T and generating new data {(Txi, yi)}. In this sense, data augmentation induces\ninvariance properties through sampling, which results in a much bigger dataset than the original one.\n7\nGeneralization power\nSection 6 has focused on the in-sample / training error obtained via SGD, but this alone does not guarantee\ngood performance with respect to the out-of-sample / test error. The gap between the in-sample error and\nthe out-of-sample error, namely the generalization gap, has been the focus of statistical learning theory since\nits birth; see [112] for an excellent introduction to this topic.\nWhile understanding the generalization power of deep neural nets is diﬃcult [135, 99], we sample re-\ncent endeavors in this section.\nFrom a high level point of view, these approaches can be divided into\ntwo categories, namely algorithm-independent controls and algorithm-dependent controls. More speciﬁcally,\nalgorithm-independent controls focus solely on bounding the complexity of the function class represented\nby certain deep neural networks. In contrast, algorithm-dependent controls take into account the algorithm\n(e.g., SGD) used to train the neural network.\n7.1\nAlgorithm-independent controls: uniform convergence\nThe key to algorithm-independent controls is the notion of complexity of the function class parametrized\nby certain neural networks. Informally, as long as the complexity is not too large, the generalization gap of\nany function in the function class is well-controlled. However, the standard complexity measure (e.g., VC\ndimension [127]) is at least proportional to the number of weights in a neural network [5, 112], which fails to\nexplain the practical success of deep learning. The caveat here is that the function class under consideration\n25\nis all the functions realized by certain neural networks, with no restrictions on the size of the weights at all.\nOn the other hand, for the class of linear functions with bounded norm, i.e., {x 7→w⊤x | ∥w∥2 ≤M}, it is\nwell understood that the complexity of this function class (measured in terms of the empirical Rademacher\ncomplexity) with respect to a random sample {xi}1≤i≤n is upper bounded by maxi ∥xi∥2M/√n, which is\nindependent of the number of parameters in w. This motivates researchers to investigate the complexity\nof norm-controlled deep neural networks10 [89, 14, 43, 74]. Setting the stage, we introduce a few necessary\nnotations and facts. The key object under study is the function class parametrized by the following fully-\nconnected neural network with depth L:\nFL ≜\n\b\nx 7→WLσ (WL−1σ (· · · W2σ (W1x)))\n\f\f (W1, · · · , WL) ∈W\n\t\n.\n(36)\nHere (W1, W2, · · · , WL) ∈W represents a certain constraint on the parameters. For instance, one can\nrestrict the Frobenius norm of each parameter Wl through the constraint ∥Wl∥F ≤MF(l), where MF(l) is\nsome positive quantity. With regard to the complexity measure, it is standard to use Rademacher complexity\nto control the capacity of the function class of interest.\nDeﬁnition 1 (Empirical Rademacher complexity). The empirical Rademacher complexity of a function\nclass F w.r.t. a dataset S ≜{xi}1≤i≤n is deﬁned as\nRS (F) = Eε\nh\nsup\nf∈F\n1\nn\nn\nX\ni=1\nεif (xi)\ni\n,\n(37)\nwhere ε ≜(ε1, ε2, · · · , εn) is composed of i.i.d. Rademacher random variables, i.e., P(εi = 1) = P(εi = −1) =\n1/2.\nIn words, Rademacher complexity measures the ability of the function class to ﬁt the random noise rep-\nresented by ε. Intuitively, a function class with a larger Rademacher complexity is more prone to overﬁtting.\nWe now formalize the connection between the empirical Rademacher complexity and the out-of-sample error;\nsee Chapter 24 in [112].\nTheorem 6. Assume that for all f ∈F and all (y, x) we have |L(f(x), y)| ≤1. In addition, assume that\nfor any ﬁxed y, the univariate function L(·, y) is Lipschitz with constant 1. Then with probability at least\n1 −δ over the sample S ≜{(yi, xi)}1≤i≤n\ni.i.d.\n∼D, one has for all f ∈F\nE(y,x)∼D [L (f(x), y)]\n|\n{z\n}\nout-of-sample error\n≤1\nn\nn\nX\ni=1\nL (f(xi), yi)\n|\n{z\n}\nin-sample error\n+2RS (F) + 4\nr\nlog (4/δ)\nn\n.\nIn English, the generalization gap of any function f that lies in F is well-controlled as long as the\nRademacher complexity of F is not too large. With this connection in place, we single out the following\ncomplexity bound.\nTheorem 7 (Theorem 1 in [43]). Consider the function class FL in (36), where each parameter Wl has\nFrobenius norm at most MF(l). Further suppose that the element-wise activation function σ(·) is 1-Lipschitz\nand positive-homogeneous (i.e., σ(c · x) = cσ(x) for all c ≥0). Then the empirical Rademacher complex-\nity (37) w.r.t. S ≜{xi}1≤i≤n satisﬁes\nRS (FL) ≤max\ni\n∥xi∥2 · 4\n√\nL QL\nl=1 MF(l)\n√n\n.\n(38)\nThe upper bound of the empirical Rademacher complexity (38) is in a similar vein to that of linear\nfunctions with bounded norm, i.e., maxi ∥xi∥2M/√n, where\n√\nL QL\nl=1 MF(l) plays the role of M in the\nlatter case. Moreover, ignoring the term\n√\nL, the upper bound (38) does not depend on the size of the\nnetwork in an explicit way if MF (l) sharply concentrates around 1. This reveals that the capacity of the\n10Such attempts have been made in the seminal work [13].\n26\nneural network is well-controlled, regardless of the number of parameters, as long as the Frobenius norm\nof the parameters is bounded. Extensions to other norm constraints, e.g., spectral norm constraints, path\nnorm constraints have been considered by [89, 14, 74, 67, 34]. This line of work improves upon traditional\ncapacity analysis of neural networks in the over-parametrized setting, because the upper bounds derived\nare often size-independent. Having said this, two important remarks are in order: (1) the upper bounds\n(e.g., QL\nl=1 MF(l)) involve implicit dependence on the size of the weight matrix and the depth of the neural\nnetwork, which is hard to characterize; (2) the upper bound on the Rademacher complexity oﬀers a uniform\nbound over all functions in the function class, which is a pure statistical result. However, it stays silent\nabout how and why standard training algorithms like SGD can obtain a function whose parameters have\nsmall norms.\n7.2\nAlgorithm-dependent controls\nIn this subsection, we bring computational thinking into statistics and investigate the role of algorithms in the\ngeneralization power of deep learning. The consideration of algorithms is quite natural and well motivated:\n(1) local/global minima reached by diﬀerent algorithms can exhibit totally diﬀerent generalization behaviors\ndue to extreme nonconvexity, which marks a huge diﬀerence from traditional models, (2) the eﬀective capacity\nof neural nets is possibly not large, since a particular algorithm does not explore the entire parameter space.\nThese demonstrate the fact that on top of the complexity of the function class, the inherent property of\nthe algorithm we use plays an important role in the generalization ability of deep learning. In what follows,\nwe survey three diﬀerent ways to obtain upper bounds on the generalization errors by exploiting properties\nof the algorithms.\n7.2.1\nMean ﬁeld view of neural nets\nAs we have emphasized, modern deep learning models are highly over-parametrized. A line of work [83, 117,\n105, 25, 82, 61] approximates the ensemble of weights by an asymptotic limit as the number of hidden units\ntends to inﬁnity, so that the dynamics of SGD can be studied via certain partial diﬀerent equations.\nMore speciﬁcally, let ˆf(x; θ) = N −1 PN\ni=1 σ(θ⊤\ni x) be a function given by a one-hidden-layer neural net\nwith N hidden units, where σ(·) is the ReLU activation function and parameters θ ≜[θ1, . . . , θN]⊤∈RN×d\nare suitably randomly initialized. Consider the regression setting where we want to minimize the population\nrisk RN(θ) = E[(y −ˆf(x; θ))2] over parameters θ. A key observation is that this population risk depends\non the parameters θ only through its empirical distribution, i.e., ˆρ(N) = N −1 PN\ni=1 δθi where δθi is a point\nmass at θi. This motivates us to view express RN(θ) equivalently as R(ˆρ(N)), where R(·) is a functional\nthat maps distributions to real numbers. Running SGD for RN(·)—in a suitable scaling limit—results in\na gradient ﬂow on the space of distributions endowed with the Wasserstein metric that minimizes R(·). It\nturns out that the empirical distribution ˆρ(N)\nk\nof the parameters after k steps of SGD is well approximated\nby the gradient follow, as long as the the neural net is over-parametrized (i.e., N ≫d) and the number of\nsteps is not too large. In particular, [83] have shown that under certain regularity conditions,\nsup\nk∈[0,T/ε]∩N\n\f\f\fR(ˆρ(N)) −R (ρkε)\n\f\f\f ≲eT\nr\n1\nN ∨ε ·\nr\nd + log N\nε ,\nwhere ε > 0 is an proxy for the step size of SGD and ρkε is the distribution of the gradient ﬂow at time kε.\nIn words, the out-of-sample error under θk generated by SGD is well-approximated by that of ρkε. Viewing\nthe optimization problem from the distributional aspect greatly simpliﬁes the problem conceptually, as\nthe complicated optimization problem is now passed into its limit version—for this reason, this analytical\napproach is called the mean ﬁeld perspective. In particular, [83] further demonstrated that in some simple\nsettings, the out-of-sample error R(ρkε) of the distributional limit can be fully characterized. Nevertheless,\nhow well does R(ρkε) perform and how fast it converges remain largely open for general problems.\n7.2.2\nStability\nA second way to understand the generalization ability of deep learning is through the stability of SGD. An\nalgorithm is considered stable if a slight change of the input does not alter the output much. It has long been\n27\nobserved that a stable algorithm has a small generalization gap; examples include k nearest neighbors [102,\n29], bagging [18, 19], etc. The precise connection between stability and generalization gap is stated by [17,\n113]. In what follows, we formalize the idea of stability and its connection with the generalization gap. Let\nA denote an algorithm (possibly randomized) which takes a sample S ≜{(yi, xi)}1≤i≤n of size n and returns\nan estimated parameter ˆθ ≜A(S). Following [49], we have the following deﬁnition for stability.\nDeﬁnition 2. An algorithm (possibly randomized) A is ε-uniformly stable with respect to the loss function\nL(·, ·) if for all datasets S, S′ of size n which diﬀer in at most one example, one has\nsup\nx,y EA [L (f(x; A (S)), y) −L (f(x; A (S′)), y)] ≤ε.\nHere the expectation is taken w.r.t. the randomness in the algorithm A and ε might depend on n. The loss\nfunction L(·, ·) takes an example (say (x, y)) and the estimated parameter (say A(S)) as inputs and outputs\na real value.\nSurprisingly, an ε-uniformly stable algorithm incurs small generalization gap in expectation, which is\nstated in the following lemma.\nLemma 1 (Theorem 2.2 in [49]). Let A be ε-uniformly stable. Then the expected generalization gap is no\nlarger than ε, i.e.,\n\f\f\f\f\fEA,S\n\"\n1\nn\nn\nX\ni=1\nL(f(xi; A (S)), yi) −E(x,y)∼D [L (f(x; A (S)), y)]\n#\f\f\f\f\f ≤ε.\n(39)\nWith Lemma 1 in hand, it suﬃces to prove stability bound on speciﬁc algorithms. It turns out that SGD\nintroduced in Section 6 is uniformly stable when solving smooth nonconvex functions.\nTheorem 8 (Theorem 3.12 in [49]). Assume that for any ﬁxed (y, x), the loss function L(f(x; θ), y), viewed\nas a function of θ, is L-Lipschitz and β-smooth. Consider running SGD on the empirical loss function with\ndecaying step size αt ≤c/t, where c is some small absolute constant. Then SGD is uniformly stable with\nε ≲T 1−\n1\nβc+1\nn\n,\nwhere we have ignored the dependency on β, c and L.\nTheorem 8 reveals that SGD operating on nonconvex loss functions is indeed uniformly stable as long\nas the number of steps T is not large compared with n. This together with Lemma 1 demonstrates the\ngeneralization ability of SGD in expectation. Nevertheless, two important limitations are worth mentioning.\nFirst, Lemma 1 provides an upper bound on the out-of-sample error in expectation, but ideally, instead of\nan on-average guarantee under EA,S, we would like to have a high probability guarantee as in the convex\ncase [37]. Second, controlling the generalization gap alone is not enough to achieve a small out-of-sample\nerror, since it is unclear whether SGD can achieve a small training error within T steps.\n7.2.3\nImplicit regularization\nIn the presence of over-parametrization (number of parameters larger than the sample size), conventional\nwisdom informs us that we should apply some regularization techniques (e.g., ℓ1 / ℓ2 regularization) so that\nthe model will not overﬁt the data. However, in practice, neural networks without explicit regularization\ngeneralize well. This phenomenon motivates researchers to look at the regularization eﬀects introduced by\ntraining algorithms (e.g., SGD) in this over-parametrized regime. While there might exits multiple, if not\ninﬁnite global minima of the empirical loss (26), it is possible that practical algorithms tend to converge to\nsolutions with better generalization powers.\nTake the underdetermined linear system Xθ = y as a starting point. Here X ∈Rn×p and θ ∈Rp with p\nmuch larger than n. Running gradient descent on the loss 1\n2∥Xθ −y∥2\n2 from the origin (i.e., θ0 = 0) results\nin the solution with the minimum Euclidean norm, that is GD converges to\nmin\nθ∈Rp\n∥θ∥2\nsubject to\nXθ = y.\n28\nIn words, without any ℓ2 regularization in the loss function, gradient descent automatically ﬁnds the solution\nwith the least ℓ2 norm. This phenomenon, often called as implicit regularization, not only has been empirically\nobserved in training neural networks, but also has been theoretically understood in some simpliﬁed cases,\ne.g., logistic regression with separable data. In logistic regression, given a training set {(yi, xi)}1≤i≤n with\nxi ∈Rp and yi ∈{1, −1}, one aims to ﬁt a logistic regression model by solving the following program:\nmin\nθ∈Rp\n1\nn\nn\nX\ni=1\nℓ\n\u0000yix⊤\ni θt\u0001\n.\n(40)\nHere, ℓ(u) ≜log(1 + e−u) denotes the logistic loss. Further assume that the data is separable, i.e., there\nexists θ∗∈Rp such that yiθ∗⊤xi > 0 for all i. Under this condition, the loss function (40) can be arbitrarily\nclose to zero for certain θ with ∥θ∥2 →∞. What happens when we minimize (40) using gradient descent?\n[119] uncovers a striking phenomenon.\nTheorem 9 (Theorem 3 in [119]). Consider the logistic regression (40) with separable data. If we run GD\nθt+1 = θt −η 1\nn\nn\nX\ni=1\nyixiℓ′\u0000yix⊤\ni θt\u0001\nfrom any initialization θ0 with appropriate step size η > 0, then normalized θt converges to a solution with\nthe maximum ℓ2 margin. That is,\nlim\nt→∞\nθt\n∥θt∥2\n= ˆθ,\n(41)\nwhere ˆθ is the solution to the hard margin support vector machine:\nˆθ ≜arg min\nθ∈Rp ∥θ∥2,\nsubject to\nyix⊤\ni θ ≥1\nfor all 1 ≤i ≤n.\n(42)\nThe above theorem reveals that gradient descent, when solving logistic regression with separable data,\nimplicitly regularizes the iterates towards the ℓ2 max margin vector (cf. (41)), without any explicit regular-\nization as in (42). Similar results have been obtained by [62]. In addition, [47] studied algorithms other than\ngradient descent and showed that coordinate descent produces a solution with the maximum ℓ1 margin.\nMoving beyond logistic regression, which can be viewed as a one-layer neural net, the theoretical under-\nstanding of implicit regularization in deeper neural networks is still limited; see [48] for an illustration in\ndeep linear convolutional neural networks.\n8\nDiscussion\nDue to space limitations, we have omitted several important deep learning models; notable examples include\ndeep reinforcement learning [86], deep probabilistic graphical models [109], variational autoencoders [66],\ntransfer learning [133], etc. Apart from the modeling aspect, interesting theories on generative adversarial\nnetworks [10, 11], recurrent neural networks [3], connections with kernel methods [59, 9] are also emerging.\nWe have also omitted the inverse-problem view of deep learning where the data are assumed to be generated\nfrom a certain neural net and the goal is to recover the weights in the NN with as few examples as possible.\nVarious algorithms (e.g., GD with spectral initialization) have been shown to recover the weights successfully\nin some simpliﬁed settings [136, 118, 42, 87, 23, 39].\nIn the end, we identify a few important directions for future research.\n• New characterization of data distributions. The success of deep learning relies on its power of eﬃciently\nrepresenting complex functions relevant to real data. Comparatively, classical methods often have optimal\nguarantee if a problem has a certain known structure, such as smoothness, sparsity, and low-rankness [121,\n31, 20, 24], but they are insuﬃcient for complex data such as images. How to characterize the high-\ndimensional real data that can free us from known barriers, such as the curse of dimensionality is an\ninteresting open question?\n29\n• Understanding various computational algorithms for deep learning. As we have emphasized throughout this\nsurvey, computational algorithms (e.g., variants of SGD) play a vital role in the success of deep learning.\nThey allow fast training of deep neural nets and probably contribute towards the good generalization\nbehavior of deep learning in practice. Understanding these computational algorithms and devising better\nones are crucial components in understanding deep learning.\n• Robustness. It has been well documented that DNNs are sensitive to small adversarial perturbations that\nare indistinguishable to humans [124]. This raises serious safety issues once if deploy deep learning models\nin applications such as self-driving cars, healthcare, etc. It is therefore crucial to reﬁne current training\npractice to enhance robustness in a principled way [116].\n• Low SNRs. Arguably, for image data and audio data where the signal-to-noise ratio (SNR) is high, deep\nlearning has achieved great success. In many other statistical problems, the SNR may be very low. For\nexample, in ﬁnancial applications, the ﬁrm characteristic and covariates may only explain a small part of\nthe ﬁnancial returns; in healthcare systems, the uncertainty of an illness may not be predicted well from\na patient’s medical history. How to adapt deep learning models to excel at such tasks is an interesting\ndirection to pursue?\nAcknowledgements\nJ. Fan is supported in part by the NSF grants DMS-1712591 and DMS-1662139, the NIH grant R01-\nGM072611 and the ONR grant N00014-19-1-2120. We thank Ruying Bao, Yuxin Chen, Chenxi Liu, Weijie\nSu, Qingcan Wang and Pengkun Yang for helpful comments and discussions.\nReferences\n[1] Martín Abadi and et. al. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015.\nSoftware available from tensorﬂow.org.\n[2] Reza Abbasi-Asl, Yuansi Chen, Adam Bloniarz, Michael Oliver, Ben DB Willmore, Jack L Gallant,\nand Bin Yu. The deeptune framework for modeling and characterizing neurons in visual cortex area\nv4. bioRxiv, page 465534, 2018.\n[3] Zeyuan Allen-Zhu and Yuanzhi Li. Can SGD Learn Recurrent Neural Networks with Provable Gener-\nalization? ArXiv e-prints, abs/1902.01028, 2019.\n[4] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.\nA convergence theory for deep learning via over-\nparameterization. arXiv preprint arXiv:1811.03962, 2018.\n[5] Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. cambridge\nuniversity press, 2009.\n[6] Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks.\n70:214–223, 06–11 Aug 2017.\n[7] Vladimir I Arnold. On functions of three variables. Collected Works: Representations of Functions,\nCelestial Mechanics and KAM Theory, 1957–1965, pages 5–8, 2009.\n[8] Sanjeev Arora and Boaz Barak. Computational complexity: a modern approach. Cambridge University\nPress, 2009.\n[9] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.\nFine-grained analysis of\noptimization and generalization for overparameterized two-layer neural networks.\narXiv preprint\narXiv:1901.08584, 2019.\n30\n[10] Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in\ngenerative adversarial nets (GANs). In Proceedings of the 34th International Conference on Machine\nLearning-Volume 70, pages 224–232. JMLR. org, 2017.\n[11] Yu Bai, Tengyu Ma, and Andrej Risteski. Approximability of discriminators implies diversity in GANs.\narXiv preprint arXiv:1806.10586, 2018.\n[12] Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE\nTransactions on Information theory, 39(3):930–945, 1993.\n[13] Peter L Bartlett. The sample complexity of pattern classiﬁcation with neural networks: the size of the\nweights is more important than the size of the network. IEEE transactions on Information Theory,\n44(2):525–536, 1998.\n[14] Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for\nneural networks. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and\nR. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 6240–6249. Curran\nAssociates, Inc., 2017.\n[15] Benedikt Bauer and Michael Kohler. On deep learning as a remedy for the curse of dimensionality in\nnonparametric regression. Technical report, Technical report, 2017.\n[16] Léon Bottou. Online learning and stochastic approximations. On-line learning in neural networks,\n17(9):142, 1998.\n[17] Olivier Bousquet and André Elisseeﬀ.\nStability and generalization.\nJournal of machine learning\nresearch, 2(Mar):499–526, 2002.\n[18] Leo Breiman. Bagging predictors. Machine learning, 24(2):123–140, 1996.\n[19] Leo Breiman et al. Heuristics of instability and stabilization in model selection. The annals of statistics,\n24(6):2350–2383, 1996.\n[20] Emmanuel J Candès and Terence Tao. The power of convex relaxation: Near-optimal matrix comple-\ntion. arXiv preprint arXiv:0903.1476, 2009.\n[21] Chensi Cao, Feng Liu, Hai Tan, Deshou Song, Wenjie Shu, Weizhong Li, Yiming Zhou, Xiaochen Bo,\nand Zhi Xie. Deep learning and its applications in biomedicine. Genomics, proteomics & bioinformatics,\n16(1):17–32, 2018.\n[22] Tianqi Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary diﬀerential\nequations. arXiv preprint arXiv:1806.07366, 2018.\n[23] Yuxin Chen, Yuejie Chi, Jianqing Fan, and Cong Ma. Gradient descent with random initialization:\nFast global convergence for nonconvex phase retrieval. Mathematical Programming, pages 1–33, 2019.\n[24] Yuxin Chen, Yuejie Chi, Jianqing Fan, Cong Ma, and Yuling Yan. Noisy matrix completion: Un-\nderstanding statistical guarantees for convex relaxation via nonconvex optimization. arXiv preprint\narXiv:1902.07698, 2019.\n[25] Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized\nmodels using optimal transport. In Advances in neural information processing systems, pages 3040–\n3050, 2018.\n[26] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. arXiv preprint arXiv:1406.1078, 2014.\n[27] R Dennis Cook et al. Fisher lecture: Dimension reduction in regression. Statistical Science, 22(1):1–26,\n2007.\n31\n[28] Jeﬀrey De Fauw, Joseph R Ledsam, Bernardino Romera-Paredes, Stanislav Nikolov, Nenad Tomasev,\nSam Blackwell, Harry Askham, Xavier Glorot, Brendan O’Donoghue, Daniel Visentin, et al. Clinically\napplicable deep learning for diagnosis and referral in retinal disease. Nature medicine, 24(9):1342, 2018.\n[29] Luc Devroye and Terry Wagner. Distribution-free performance bounds for potential function rules.\nIEEE Transactions on Information Theory, 25(5):601–604, 1979.\n[30] David L Donoho. High-dimensional data analysis: The curses and blessings of dimensionality. AMS\nmath challenges lecture, 1(2000):32, 2000.\n[31] David L Donoho and Jain M Johnstone. Ideal spatial adaptation by wavelet shrinkage. biometrika,\n81(3):425–455, 1994.\n[32] Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent ﬁnds global\nminima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018.\n[33] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and\nstochastic optimization. Journal of Machine Learning Research, 12(Jul):2121–2159, 2011.\n[34] Weinan E, Chao Ma, and Qingcan Wang. A priori estimates of the population risk for residual networks.\narXiv preprint arXiv:1903.02154, 2019.\n[35] Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In Conference\non Learning Theory, pages 907–940, 2016.\n[36] Jianqing Fan and Runze Li.\nVariable selection via nonconcave penalized likelihood and its oracle\nproperties. Journal of the American statistical Association, 96(456):1348–1360, 2001.\n[37] Vitaly Feldman and Jan Vondrak. High probability generalization bounds for uniformly stable algo-\nrithms with nearly optimal rate. arXiv preprint arXiv:1902.10710, 2019.\n[38] Jerome H Friedman and Werner Stuetzle. Projection pursuit regression. Journal of the American\nstatistical Association, 76(376):817–823, 1981.\n[39] Haoyu Fu, Yuejie Chi, and Yingbin Liang. Local geometry of one-hidden-layer neural networks for\nlogistic regression. arXiv preprint arXiv:1802.06463, 2018.\n[40] Kunihiko Fukushima and Sei Miyake. Neocognitron: A self-organizing neural network model for a\nmechanism of visual pattern recognition. In Competition and cooperation in neural nets, pages 267–\n285. Springer, 1982.\n[41] Chao Gao, Jiyi Liu, Yuan Yao, and Weizhi Zhu. Robust estimation and generative adversarial nets.\narXiv preprint arXiv:1810.02030, 2018.\n[42] Surbhi Goel, Adam Klivans, and Raghu Meka. Learning one convolutional layer with overlapping\npatches. arXiv preprint arXiv:1802.02547, 2018.\n[43] Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural\nnetworks. arXiv preprint arXiv:1712.06541, 2017.\n[44] Gene H Golub and Charles F Van Loan. Matrix computations. JHU Press, 4 edition, 2013.\n[45] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.\n[46] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information pro-\ncessing systems, pages 2672–2680, 2014.\n[47] Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in terms\nof optimization geometry. arXiv preprint arXiv:1802.08246, 2018.\n32\n[48] Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent on\nlinear convolutional networks. In Advances in Neural Information Processing Systems, pages 9482–\n9491, 2018.\n[49] Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of stochas-\ntic gradient descent. arXiv preprint arXiv:1509.01240, 2015.\n[50] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recogni-\ntion. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778,\n2016.\n[51] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual net-\nworks. In European conference on computer vision, pages 630–645. Springer, 2016.\n[52] Geoﬀrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning lecture\n6a overview of mini-batch gradient descent. 2012.\n[53] Geoﬀrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdi-\nnov.\nImproving neural networks by preventing co-adaptation of feature detectors.\narXiv preprint\narXiv:1207.0580, 2012.\n[54] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–\n1780, 1997.\n[55] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected con-\nvolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 4700–4708, 2017.\n[56] David H Hubel and Torsten N Wiesel. Receptive ﬁelds, binocular interaction and functional architecture\nin the cat’s visual cortex. The Journal of physiology, 160(1):106–154, 1962.\n[57] Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt\nKeutzer.\nSqueezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size.\narXiv preprint arXiv:1602.07360, 2016.\n[58] Sergey Ioﬀe and Christian Szegedy.\nBatch normalization: Accelerating deep network training by\nreducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\n[59] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and gener-\nalization in neural networks. In Advances in neural information processing systems, pages 8580–8589,\n2018.\n[60] Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Accelerating\nstochastic gradient descent. arXiv preprint arXiv:1704.08227, 2017.\n[61] Adel Javanmard, Marco Mondelli, and Andrea Montanari. Analysis of a two-layer neural network via\ndisplacement convexity. arXiv preprint arXiv:1901.01375, 2019.\n[62] Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. arXiv preprint\narXiv:1803.07300, 2018.\n[63] Rahul Kidambi, Praneeth Netrapalli, Prateek Jain, and Sham Kakade. On the insuﬃciency of ex-\nisting momentum schemes for stochastic optimization. In 2018 Information Theory and Applications\nWorkshop (ITA), pages 1–9. IEEE, 2018.\n[64] Jack Kiefer, Jacob Wolfowitz, et al. Stochastic estimation of the maximum of a regression function.\nThe Annals of Mathematical Statistics, 23(3):462–466, 1952.\n[65] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n33\n[66] Diederik P Kingma and Max Welling.\nAuto-encoding variational bayes.\narXiv preprint\narXiv:1312.6114, 2013.\n[67] Jason M Klusowski and Andrew R Barron. Risk bounds for high-dimensional ridge function combina-\ntions including neural networks. arXiv preprint arXiv:1607.01434, 2016.\n[68] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁcation with deep convo-\nlutional neural networks.\nIn Advances in neural information processing systems, pages 1097–1105,\n2012.\n[69] Harold Kushner and G George Yin. Stochastic approximation and recursive algorithms and applications,\nvolume 35. Springer Science & Business Media, 2003.\n[70] Yann LeCun, Yoshua Bengio, and Geoﬀrey Hinton. Deep learning. nature, 521(7553):436, 2015.\n[71] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haﬀner. Gradient-based learning applied to\ndocument recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n[72] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape\nof neural nets. In Advances in Neural Information Processing Systems, pages 6391–6401, 2018.\n[73] Ker-Chau Li. Sliced inverse regression for dimension reduction. Journal of the American Statistical\nAssociation, 86(414):316–327, 1991.\n[74] Xingguo Li, Junwei Lu, Zhaoran Wang, Jarvis Haupt, and Tuo Zhao. On tighter generalization bound\nfor deep neural networks: Cnns, resnets, and beyond. arXiv preprint arXiv:1806.05159, 2018.\n[75] Yujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In International\nConference on Machine Learning, pages 1718–1727, 2015.\n[76] Tengyuan Liang. How well can generative adversarial networks (GAN) learn densities: A nonparametric\nview. arXiv preprint arXiv:1712.08244, 2017.\n[77] Henry W Lin, Max Tegmark, and David Rolnick. Why does deep and cheap learning work so well?\nJournal of Statistical Physics, 168(6):1223–1247, 2017.\n[78] Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400, 2013.\n[79] Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectiﬁer nonlinearities improve neural network\nacoustic models. In Proc. icml, volume 30, page 3, 2013.\n[80] VE Maiorov and Ron Meir. On the near optimality of the stochastic approximation of smooth functions\nby neural networks. Advances in Computational Mathematics, 13(1):79–103, 2000.\n[81] Yuly Makovoz.\nRandom approximants and neural networks.\nJournal of Approximation Theory,\n85(1):98–109, 1996.\n[82] Song Mei, Theodor Misiakiewicz, and Andrea Montanari.\nMean-ﬁeld theory of two-layers neural\nnetworks: dimension-free bounds and kernel limit. arXiv preprint arXiv:1902.06015, 2019.\n[83] Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean ﬁeld view of the landscape of two-layer\nneural networks. Proceedings of the National Academy of Sciences, 115(33):E7665–E7671, 2018.\n[84] Hrushikesh Mhaskar, Qianli Liao, and Tomaso Poggio. Learning functions: when is deep better than\nshallow. arXiv preprint arXiv:1603.00988, 2016.\n[85] Hrushikesh N Mhaskar. Neural networks for optimal approximation of smooth and analytic functions.\nNeural computation, 8(1):164–177, 1996.\n[86] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,\nAlex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control\nthrough deep reinforcement learning. Nature, 518(7540):529, 2015.\n34\n[87] Marco Mondelli and Andrea Montanari. On the connection between learning two-layers neural networks\nand tensor decomposition. arXiv preprint arXiv:1802.07301, 2018.\n[88] Yurii E Nesterov. A method for solving the convex programming problem with convergence rate o\n(1/kˆ 2). In Dokl. Akad. Nauk SSSR, volume 269, pages 543–547, 1983.\n[89] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.\nNorm-based capacity control in neural\nnetworks. In Conference on Learning Theory, pages 1376–1401, 2015.\n[90] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers\nusing variational divergence minimization. In Advances in Neural Information Processing Systems,\npages 271–279, 2016.\n[91] Ian Parberry. Circuit complexity and neural networks. MIT press, 1994.\n[92] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming\nLin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic diﬀerentiation in pytorch. 2017.\n[93] Allan Pinkus. Approximation theory of the mlp model in neural networks. Acta numerica, 8:143–195,\n1999.\n[94] Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. Why and\nwhen can deep-but not shallow-networks avoid the curse of dimensionality: a review. International\nJournal of Automation and Computing, 14(5):503–519, 2017.\n[95] Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR Compu-\ntational Mathematics and Mathematical Physics, 4(5):1–17, 1964.\n[96] Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM\nJournal on Control and Optimization, 30(4):838–855, 1992.\n[97] Boris Teodorovich Polyak and Yakov Zalmanovich Tsypkin. Adaptive estimation algorithms: conver-\ngence, optimality, stability. Avtomatika i Telemekhanika, (3):71–84, 1979.\n[98] Christopher Poultney, Sumit Chopra, Yann LeCun, et al. Eﬃcient learning of sparse representations\nwith an energy-based model. In Advances in neural information processing systems, pages 1137–1144,\n2007.\n[99] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.\nDo cifar-10 classiﬁers\ngeneralize to cifar-10? arXiv preprint arXiv:1806.00451, 2018.\n[100] Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. 2018.\n[101] Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical\nStatistics, 22(3):400–407, 1951.\n[102] William H Rogers and Terry J Wagner. A ﬁnite sample distribution-free performance bound for local\ndiscrimination rules. The Annals of Statistics, pages 506–514, 1978.\n[103] David Rolnick and Max Tegmark. The power of deeper networks for expressing natural functions.\narXiv preprint arXiv:1705.05502, 2017.\n[104] Yaniv Romano,\nMatteo Sesia,\nand Emmanuel J Candès.\nDeep knockoﬀs.\narXiv preprint\narXiv:1811.06687, 2018.\n[105] Grant M Rotskoﬀand Eric Vanden-Eijnden. Neural networks as interacting particle systems: Asymp-\ntotic convexity of the loss landscape and universal scaling of the approximation error. arXiv preprint\narXiv:1805.00915, 2018.\n35\n[106] David E Rumelhart, Geoﬀrey E Hinton, and Ronald J Williams. Learning internal representations by\nerror propagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive Science,\n1985.\n[107] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.\nIma-\ngeNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV),\n115(3):211–252, 2015.\n[108] Haşim Sak, Andrew Senior, and Françoise Beaufays. Long short-term memory recurrent neural network\narchitectures for large scale acoustic modeling. In Fifteenth annual conference of the international\nspeech communication association, 2014.\n[109] Ruslan Salakhutdinov and Geoﬀrey Hinton. Deep boltzmann machines. In Artiﬁcial intelligence and\nstatistics, pages 448–455, 2009.\n[110] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Im-\nproved techniques for training GANs. In Advances in Neural Information Processing Systems, pages\n2234–2242, 2016.\n[111] Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with relu activation\nfunction. arXiv preprint arXiv:1708.06633, 2017.\n[112] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms.\nCambridge university press, 2014.\n[113] Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability and\nuniform convergence. Journal of Machine Learning Research, 11(Oct):2635–2670, 2010.\n[114] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,\nThomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without\nhuman knowledge. Nature, 550(7676):354, 2017.\n[115] Bernard W Silverman. Density estimation for statistics and data analysis. Chapman & Hall, CRC,\n1998.\n[116] Chandan Singh, W James Murdoch, and Bin Yu.\nHierarchical interpretations for neural network\npredictions. arXiv preprint arXiv:1806.05337, 2018.\n[117] Justin Sirignano and Konstantinos Spiliopoulos. Mean ﬁeld analysis of neural networks. arXiv preprint\narXiv:1805.01053, 2018.\n[118] Mahdi Soltanolkotabi. Learning relus via gradient descent. In Advances in Neural Information Pro-\ncessing Systems, pages 2007–2017, 2017.\n[119] Daniel Soudry, Elad Hoﬀer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit\nbias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):2822–\n2878, 2018.\n[120] David A Sprecher. On the structure of continuous functions of several variables. Transactions of the\nAmerican Mathematical Society, 115:340–355, 1965.\n[121] Charles J Stone. Optimal global rates of convergence for nonparametric regression. The annals of\nstatistics, pages 1040–1053, 1982.\n[122] Ilya Sutskever, James Martens, George Dahl, and Geoﬀrey Hinton. On the importance of initialization\nand momentum in deep learning. In International conference on machine learning, pages 1139–1147,\n2013.\n36\n[123] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru\nErhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings\nof the IEEE conference on computer vision and pattern recognition, pages 1–9, 2015.\n[124] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,\nand Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.\n[125] Matus Telgarsky. Beneﬁts of depth in neural networks. arXiv preprint arXiv:1602.04485, 2016.\n[126] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical\nSociety: Series B (Methodological), 58(1):267–288, 1996.\n[127] VN Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of events to\ntheir probabilities. Theory of Probability & Its Applications, 16(2):264–280, 1971.\n[128] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and com-\nposing robust features with denoising autoencoders. In Proceedings of the 25th international conference\non Machine learning, pages 1096–1103. ACM, 2008.\n[129] Stefan Wager, Sida Wang, and Percy S Liang. Dropout training as adaptive regularization. In Advances\nin neural information processing systems, pages 351–359, 2013.\n[130] E Weinan, Jiequn Han, and Arnulf Jentzen.\nDeep learning-based numerical methods for high-\ndimensional parabolic partial diﬀerential equations and backward stochastic diﬀerential equations.\nCommunications in Mathematics and Statistics, 5(4):349–380, 2017.\n[131] Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal\nvalue of adaptive gradient methods in machine learning.\nIn I. Guyon, U. V. Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information\nProcessing Systems 30, pages 4148–4158. Curran Associates, Inc., 2017.\n[132] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.\nGoogle’s neural machine translation\nsystem: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144,\n2016.\n[133] Jason Yosinski, JeﬀClune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep\nneural networks? In Advances in neural information processing systems, pages 3320–3328, 2014.\n[134] Jason Yosinski, JeﬀClune, Anh Nguyen, Thomas Fuchs, and Hod Lipson.\nUnderstanding neural\nnetworks through deep visualization. arXiv preprint arXiv:1506.06579, 2015.\n[135] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep\nlearning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.\n[136] Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees\nfor one-hidden-layer neural networks. In Proceedings of the 34th International Conference on Machine\nLearning-Volume 70, pages 4140–4149. JMLR. org, 2017.\n37\n",
  "categories": [
    "stat.ML",
    "cs.LG",
    "math.ST",
    "stat.ME",
    "stat.TH"
  ],
  "published": "2019-04-10",
  "updated": "2019-04-15"
}