{
  "id": "http://arxiv.org/abs/1904.06049v1",
  "title": "Distributed Layer-Partitioned Training for Privacy-Preserved Deep Learning",
  "authors": [
    "Chun-Hsien Yu",
    "Chun-Nan Chou",
    "Emily Chang"
  ],
  "abstract": "Deep Learning techniques have achieved remarkable results in many domains.\nOften, training deep learning models requires large datasets, which may require\nsensitive information to be uploaded to the cloud to accelerate training. To\nadequately protect sensitive information, we propose distributed\nlayer-partitioned training with step-wise activation functions for\nprivacy-preserving deep learning. Experimental results attest our method to be\nsimple and effective.",
  "text": "Distributed Layer-Partitioned Training for Privacy-Preserved Deep Learning\nChun-Hsien Yu\nHTC Research\nJimmy Yu@htc.com\nChun-Nan Chou\nHTC Research\njason.cn chou@htc.com\nEmily Chang\nHTC Research\nemilyjchang30@gmail.com\nAbstract\nDeep Learning techniques have achieved remarkable re-\nsults in many domains. Often, training deep learning mod-\nels requires large datasets, which may require sensitive in-\nformation to be uploaded to the cloud to accelerate training.\nTo adequately protect sensitive information, we propose dis-\ntributed layer-partitioned training with step-wise activation\nfunctions for privacy-preserving deep learning. Experimen-\ntal results attest our method to be simple and effective.\n1\nIntroduction\nSince 2012, neural networks and deep architectures have\nproven very effective in application areas such as computer\nvision [6] and disease diagnosis [3]. Although deep archi-\ntectures such as convolutional neural networks (CNNs) [5]\nemerged as early as the 1980s, they have not risen into the\nspotlight until now. Their recent popularity is due to the\nrising importance of scale, in both data volume and compu-\ntation resources [2].\nWhen the data scale or data volume is small, a local com-\nputer can handle the training workload. However, when\ndata volume is large, even one-hundredth of AlphaGo’s\nscale, most local computing facilities do not have sufﬁcient\ncomputation resources to complete training tasks in a timely\nfashion. Take training AlexNet [9] as an example. The task\nof training one million images took more than one week on\na typical local host. Considering that training parameters\nsuch as learning rate and momentum must be experimented\nupon to ﬁnd the best settings, tens of rounds of training\nare required. Thus, a training task can demand weeks on\na typical local host to complete. This long latency is not\nfeasible for many urgent applications such as national se-\ncurity, investment decisions, and disease outbreak predic-\ntion. The logical solution is to upload data onto the cloud\nto take advantage of the distributed computing resources.\nHowever, many institutions such as the government, hospi-\ntals, and ﬁnancial institutes forbid data from leaving their\nlocal sites for privacy and security reasons. In this work,\nwe propose a distributed layer-partitioned training method-\nology that permits metadata (not the original data) to leave\na local site, while at the same time preserving data privacy.\nThe principal idea is that the original data is processed into\nirreversible secured metadata before leaving a local site.\nOur method permits substantial training to be run distribu-\ntively on the irreversible secured metadata of remote sites,\nwhereas the original data stays at the local secured site.\n2\nRelated Work\nWhile deep learning is ﬂourishing in many domains, its\nprivacy concerns have attracted much recent attention. Sev-\neral pieces of research have shown that training data can be\nrecovered with the access to trained models, which runs the\nrisk of inadvertently revealing sensitive information. For\nexample, [4] exploited a model inversion attack to retrieve\nrecognizable images from a facial-recognition model.\nSome previous works have attempted to address deep\nlearning privacy concerns. [1] adopted differential privacy\nin the deep learning training process.\nSpeciﬁcally, they\nadded noises to gradients when applying stochastic gradi-\nent descent (SGD). [14] devised distributed selective SGD,\nthereby enabling each client to train its model locally and to\nexchange model parameters selectively with the global one\nin the cloud server.\nAlthough these approaches can introduce certain difﬁ-\nculties in precisely recovering training samples, they are\nstill vulnerable to some attacks such as generative adver-\nsarial network-based attacks [7]. In contrast, deep learning\nmodels trained using our proposed training methodology\navoid such attacks since the trained models are separated\ninto two distinct parts. Some works such as [11] applied\ncryptographic techniques to address the privacy issues of\ndeep learning in cloud computing, which is another alterna-\ntive but often forces learning over encrypted domains.\n3\nBrief Introduction of CNN\nConvolutional Neural Network (CNN) is a representative\nmodel and used by our work to study privacy-preserving\narXiv:1904.06049v1  [cs.LG]  12 Apr 2019\nschemes. Throughout this paper, we focus on discussing\nCNNs, since inputs to CNNs are often images that can be\nvisualized and are easier to illustrate. However, our pro-\nposed training methodology can be applied to all neural net-\nworks that have activation functions. CNNs are typically\ncomposed of four computation operations:\n• Matrix multiplication. In fully connected layers, the\ncomputation is the multiplication of two matrices.\n• Convolution.\nConvolutional layers apply some de-\nsigned ﬁlters to extract features from the input matrix.\n• Pooling In such layers, a function such as max or aver-\nage is used to aggregate data.\n• Non-linear activation. In order to model nonlinearity,\nthe neural network introduces the activation function\nduring the evaluation of neuron outputs. The tradi-\ntional way to evaluate a neuron output f as a function\ng of its input z is with f = g(z) where g can be a sig-\nmoid function or a hyperbolic tangent function. Both\nof these functions are saturating non-linear. That is,\nthe range of either function is ﬁxed between a mini-\nmum and maximum. Non-saturating activation func-\ntion ReLU proposed by [12] can also be used.\nOut of the four steps, the ﬁrst two steps are reversible,\nin that, if one gets a hold of the output of a neural network\nlayer and its model parameters, one can recreate the layer’s\ninput. The third step is irreversible since the pooling step\naggregates inputs using an operator such as average, max,\nand min. For example, given an average of n numbers, one\ncannot recreate the original n numbers. For the last step,\nsigmoid and hyperbolic tangent functions are one-to-one\nfunctions so both are reversible. On the other hand, ReLU\nis an irreversible function, but our experiments showed that\ninputs to ReLU can be recovered from its outputs well.\n4\nDistributed Layer-Partitioned Training\nIn this section, we ﬁrst describe the system architecture\nof our distributed layer-partitioned training. We then ex-\nplain how to derive raw training data out of CNNs’ ﬁrst\nconvolutional layer from its outputs. Next, we explain how\nto reverse-engineer the three conventional activation func-\ntions. Finally, we propose a step-wise activation function.\n4.1\nSystem Architecture\nFig. 1 illustrates the architecture of our proposed training\nsystem. Our system keeps the design of a CNN model un-\nchanged but partitions it into two different parts. One is the\nﬁrst convolutional layer that is secured in a local machine\nSecured Local Host\n…\nCloud Servers\nFigure 1.\nThe high-level view of our system\narchitecture includes: 1) a secured local host\nhaving raw data and the ﬁrst convolutional\nlayer of a CNN model and 2) a cloud server\nthat is responsible for the rest computation.\nand produces the metadata; the other is the rest of the layers\nthat are to be trained on the metadata on distributed sites.\nBased on this training architecture, we avoid exposing\nthe raw data out of a local secured site. However, if we\nsimply enforce the ﬁrst convolutional layer of a CNN on\nthe local site, the potential risk of recovering the original\ndata still remains, which we explain in the next subsection.\n4.2\nRisk of Generic Convolutional Layers\nAssume the input data has the dimension (Cin, Hin, Win),\nand the output result and the kernels of the ﬁrst con-\nvolutional layer has the dimension (Cout, Hout, Wout) and\nthe size (S × S), respectively. Thus, the weights of the\nconvolutional kernels in the ﬁrst layer has the dimension\n(Cout, Cin, S, S). Based on our assumptions, the problem\nof recovering the input data of the convolutional layer from\nits output can be formulated as solving a system of linear\nequations: Ax = b, where x is the input data, A stands for\nthe weights of the convolutional kernels in the ﬁrst layer,\nand b is the output of the ﬁrst layer. In the linear equa-\ntions, the number of the unknown parameters we need to\nsolve is Cin ×Hin ×Win, and the number of the equations is\nCout × Hout × Wout. Therefore, if Cout is large enough, this\nsystem of linear equations has a unique solution x∗that is\nthe same as the original input data. Please note that due to\nthe nature of the convolutional operation, these linear equa-\ntions are not dependent.\nHowever, if the dimension of the input data is high, e.g.\nthe images in CIFAR-10, the system of linear equations be-\ncomes too large to be solved due to the limitation of the\ncomputation power. Thus, we divide the linear system into\nmany small sub-systems {Aixi = bi | ∀i}, and the solu-\ntions {x∗\ni | ∀i} is equal to x∗. In some cases, this may lead\nto the following situation: x∗is unique, but {x∗\ni | ∀i} is\nnot. However, if the number of the convolutional kernels is\nless than or equal to Cout, we can show that\nx∗is unique iff {x∗\ni | ∀i} is unique.\nIn the following, we elaborate the method dividing the orig-\ninal system of linear equations into sub-systems. First, we\ndenote the weights of the ith convolutional kernel as ωi\nthat is a tensor of weights. After we ﬁx a pair (hout, wout),\nwhich corresponds to the location of the output results,\nwe can trace back the location of the input tensor, de-\nnoted as (Hin, Win). Moreover, this corresponding space\n[Images](Cin,Hin,Win) is equal to the size of ωi, for i =\n1, . . . , Cout. Therefore, we can state the linear system as\nX\nω1 ⊙[Image](Cin,Hin,Win) = [Output](1,hout,wout)\n...\nX\nωCout ⊙[Image](Cin,Hin,Win) = [Output](Cout,hout,wout),\nwhere ⊙is the element-wised products. It is clear that we\nhave the unique solution [Image]∗\n(Cin,Hin,Win) if and only if\nCout × Hout × Wout is larger than Cin × Hin × Win. Based\non the above-mentioned method, there exists an effective\nway to derive the original input data from the outputs of\ngeneric convolutional layers. Consequently, if the metadata\nis caught and the local convoluational layer is compromised,\nthe original data is vulnerable to be leaked.\n4.3\nRisk of Conventional Activation Functions\nIn this subsection, we explain the approach to derive the\ninputs of three conventional activation functions from their\noutputs, revealing the potential privacy problem. Basically,\nif we can infer the inverse functions of these three activation\nfunctions, the input data can be obtained easily.\nThe sigmoid function is one of the widely used activa-\ntion functions. Because sigmoid is a bijective function, its\nunique inverse function exists, namely\nz = sigmoid−1(y) = −ln (1\ny −1).\nSimilarly, the hyperbolic tangent function is also bijec-\ntive, and its inverse function is deﬁned as follows:\ntanh −1(z) = ln (1 + z) −ln (1 −z)\n2\n.\nUnlike the prior two activation functions, ReLU is the\nonly surjective function, which is ReLU(z) = max(0, z).\nThus, we could reverse the result of ReLU only if the output\nis positive and use the following method to reconstruct the\ninput information. First, we pick those equations with posi-\ntive values. Second, if the number of linear equations is still\ninsufﬁcient to solve the inputs, we meet the requirement of\nthe equation number by including some of the zero-valued\nequations. Lastly, we can solve the system of linear equa-\ntions as if ReLU is an identity.\nFigure 2. A step-wise activation example.\n4.4\nStep-Wise Activation Function\nTo make the information irreversible, we modify the con-\nventional activation functions to be step-wise. We perform\nquantization on the original functions. Fig. 2 shows one\nstep-wise version of sigmoid function. Formally, we divide\nthe input domain into intervals and make the output step-\nwise. Given the number of intervals n, the clipping value v\nand an activation function g, the step-wise version of g is\ngstep(x) = g(sign(x) · ⌊min(|x|, v)\nv/n\n⌋· v\nn),\nwhere sign(·) is a function that returns 1 if its input is non-\nnegative, and −1 otherwise.\n5\nPreliminary Evaluation\nWe performed experiments to verify that 1) models\ntrained using our proposed method can reach equivalent\naccuracy, and 2) raw images cannot be reversed after our\nproposed step-wise functions were applied.\nWe applied\nour method to MobileNetV2 [13] on two standard image\ndatasets: MNIST [10] and CIFAR-10 [8]. We chose these\ntwo public datasets because they have a long record of serv-\ning as benchmarks in machine learning [1]. In all of our\nexperiments, we utilized SGD with momentum to train the\nmodels and ran 90 epochs by using the same initialization\nmethod and hyper-parameter settings.\nTo examine the effect of our method on prediction accu-\nracy, we substituted our step-wise sigmoid functions with\ndifferent interval values n for the ﬁrst activation function\nin MobileNetV2, whereas the other components remain un-\nchanged. Table. 1 shows that the accuracy of the model\nusing our step-wise sigmoid increases as the value of n in-\ncreases. In comparison with the accuracy of the model using\nthe original sigmoid, the accuracy degradation of using our\nstep-wise sigmoid with n = 21 is negligible for MNIST.\nThis result provides us a setting to maintain equivalent ac-\ncuracy. On the more complicated CIFAR-10 [8] dataset,\nthe accuracy degradation using our privacy-preserved is less\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\nCIFAR-10\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\nMNIST\nFigure 3.\n(a) original images, (b) ∼(e) re-\nversed results of convolutional layers w/o ac-\ntivation function, w/ sigmoid, w/ hyperbolic\ntangent, w/ ReLU, (f) ∼(i) reversed results of\nour proposed step-wise function on sigmoid\nwith the clipping value v = 10 and the number\nof intervals n set to 3, 5, 11 and 21.\nTable 1. Accuracy with and without step-wise\nactivation function. M and C stand for MINST\nand CIFAR-10, respectively.\nSig.\nStep.\nn = 3\nStep.\nn = 5\nStep.\nn = 11\nStep.\nn = 21\nM.\n99.68%\n10.28%\n23.27%\n99.57%\n99.65%\nC.\n86.94%\n13.74%\n23.45%\n49.91%\n81.28%\ncompetitive. A further increase in the interval value n is ex-\npected to increase its accuracy.\nTo evaluate the capability of preserving data privacy, we\nvisualized the input images from the outputs of the ﬁrst\nconvolutional layer in our experimental models by using\nthe method described in the previous section. The results\nshown in Fig. 3 demonstrate that most reversed inputs of\nour proposed step-wise sigmoid functions are unrecogniz-\nable. Qualitatively, this result demonstrates that our pro-\nposed scheme to be effective in preserving data privacy.\n6\nConclusion\nOur proposed distributed layer-partition training with\nstep-wise activation functions can protect the original data\nagainst malicious attacks better. The preliminary evalua-\ntion shows our proposed methodology successfully creates\nconsiderable difﬁculties for recovering raw data even if the\nmetadata is captured and the underlying weights are com-\npromised by adversaries. We also observe that tradeoffs ex-\nist between accuracy and data privacy. Based on our obser-\nvation, a hospital can set the interval parameter properly to\nachieve the desired balance between accuracy and privacy.\nAs future work, we will investigate what the ideal interval\nvalues of different settings are and extend our idea to work\nwith other tasks such as object detection and segmentation.\nReferences\n[1] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan,\nI. Mironov, K. Talwar, and L. Zhang. Deep learning with\ndifferential privacy. In Proceedings of CCS, pages 308–318.\nACM, 2016.\n[2] E. Y. Chang. Foundations of large-scale multimedia infor-\nmation management and retrieval: mathematics of percep-\ntion. Springer Science & Business Media, 2011.\n[3] E. Y. Chang, M.-H. Wu, K.-F. T. Tang, H.-C. Kao, and C.-\nN. Chou. Artiﬁcial intelligence in xprize deepq tricorder. In\nProceedings of MMHealth@MM, pages 11–18. ACM, 2017.\n[4] M. Fredrikson, S. Jha, and T. Ristenpart. Model inversion at-\ntacks that exploit conﬁdence information and basic counter-\nmeasures. In Proceedings of CCS, pages 1322–1333. ACM,\n2015.\n[5] K. Fukushima and S. Miyake.\nNeocognitron:\nA self-\norganizing neural network model for a mechanism of visual\npattern recognition. In Competition and cooperation in neu-\nral nets, pages 267–285. Springer, 1982.\n[6] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition. In Proceedings of CVPR, pages 770–\n778, 2016.\n[7] B. Hitaj, G. Ateniese, and F. Perez-Cruz. Deep models under\nthe gan: information leakage from collaborative deep learn-\ning. In Proceedings of CCS, pages 603–618. ACM, 2017.\n[8] A. Krizhevsky and G. Hinton. Learning multiple layers of\nfeatures from tiny images. Technical report, Citeseer, 2009.\n[9] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet\nclassiﬁcation with deep convolutional neural networks. In\nProceedings of NIPS, pages 1097–1105, 2012.\n[10] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-\nbased learning applied to document recognition. Proceed-\nings of the IEEE, 86(11), 1998.\n[11] P. Li, J. Li, Z. Huang, T. Li, C.-Z. Gao, S.-M. Yiu, and\nK. Chen.\nMulti-key privacy-preserving deep learning in\ncloud computing.\nFuture Generation Computer Systems,\n74:76–85, 2017.\n[12] V. Nair and G. E. Hinton. Rectiﬁed linear units improve\nrestricted boltzmann machines.\nIn Proceedings of ICML,\npages 807–814, 2010.\n[13] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C.\nChen.\nMobilenetv2: Inverted residuals and linear bottle-\nnecks. In Proceedings of CVPR, pages 4510–4520, 2018.\n[14] R. Shokri and V. Shmatikov. Privacy-preserving deep learn-\ning. In Proceedings of CCS, pages 1310–1321. ACM, 2015.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-04-12",
  "updated": "2019-04-12"
}