{
  "id": "http://arxiv.org/abs/1904.06618v1",
  "title": "UR-FUNNY: A Multimodal Language Dataset for Understanding Humor",
  "authors": [
    "Md Kamrul Hasan",
    "Wasifur Rahman",
    "Amir Zadeh",
    "Jianyuan Zhong",
    "Md Iftekhar Tanveer",
    "Louis-Philippe Morency",
    "Mohammed",
    "Hoque"
  ],
  "abstract": "Humor is a unique and creative communicative behavior displayed during social\ninteractions. It is produced in a multimodal manner, through the usage of words\n(text), gestures (vision) and prosodic cues (acoustic). Understanding humor\nfrom these three modalities falls within boundaries of multimodal language; a\nrecent research trend in natural language processing that models natural\nlanguage as it happens in face-to-face communication. Although humor detection\nis an established research area in NLP, in a multimodal context it is an\nunderstudied area. This paper presents a diverse multimodal dataset, called\nUR-FUNNY, to open the door to understanding multimodal language used in\nexpressing humor. The dataset and accompanying studies, present a framework in\nmultimodal humor detection for the natural language processing community.\nUR-FUNNY is publicly available for research.",
  "text": "UR-FUNNY: A Multimodal Language Dataset for Understanding Humor\nMd Kamrul Hasan1*, Wasifur Rahman1*, Amir Zadeh2, Jianyuan Zhong1,\nMd Iftekhar Tanveer1, Louis-Philippe Morency2, Mohammed (Ehsan) Hoque1\n1 - Department of Computer Science, University of Rochester, USA\n2 - Language Technologies Institute, SCS, CMU, USA\n{mhasan8,echowdh2}@cs.rochester.edu, abagherz@cs.cmu.edu,\njzhong9@u.rochester.edu, itanveer@cs.rochester.edu,\nmorency@cs.cmu.edu, mehoque@cs.rochester.edu\nI can put a bizarre \nnew idea in your \nmind right now.\nI can say,  imagine a \njellyfish waltzing in a \nlibrary while thinking \nabout quantum \nmechanics.\nNow, if everything has gone \nrelatively well in your life, \nyou probably haven’t had that \nthought before. \nSo because of this \nability, we humans \nare able to transmit \nknowledge across \ntime.\nYour brain takes \nthose vibrations from \nyour eardrums and \ntransforms them into \nthoughts. \nVideo\nAudio\nText\nHumor?\nYes\nNo\nContext\nPunchline\nt=0:41\nt=1:15\nt=1:20\nt=1:09\nt=1:03\nt=0:52\nFigure 1: An example of the UR-FUNNY dataset. UR-FUNNY presents a framework to study the dynamics of\nhumor in multimodal language. Machine learning models are given a sequence of sentences with the accompanying\nmodalities of vision and acoustic. Their goal is to detect whether or not the sequence will trigger immediate\nlaughter by detecting whether or not the last sentence constitutes a punchline.\nAbstract\nHumor is a unique and creative communica-\ntive behavior displayed during social interac-\ntions.\nIt is produced in a multimodal man-\nner, through the usage of words (text), gestures\n(vision) and prosodic cues (acoustic).\nUn-\nderstanding humor from these three modali-\nties falls within boundaries of multimodal lan-\nguage; a recent research trend in natural lan-\nguage processing that models natural language\nas it happens in face-to-face communication.\nAlthough humor detection is an established\nresearch area in NLP, in a multimodal con-\ntext it is an understudied area.\nThis paper\npresents a diverse multimodal dataset, called\nUR-FUNNY, to open the door to understand-\ning multimodal language used in expressing\nhumor. The dataset and accompanying stud-\nies, present a framework in multimodal humor\ndetection for the natural language processing\ncommunity. UR-FUNNY is publicly available\nfor research.\n1\nIntroduction\nHumor is a unique communication skill that re-\nmoves barriers in conversations. Research shows\nthat effective use of humor allows a speaker\nto establish rapport (Stauffer, 1999), grab atten-\ntion (Wanzer et al., 2010), introduce a difﬁcult\nconcept without confusing the audience (Garner,\n2005) and even to build trust (Vartabedian and\nVartabedian, 1993). Humor involves multimodal\ncommunicative channels including effective use of\nwords (text), accompanying gestures (vision) and\nsounds (acoustic). Being able to mix and align\nthose modalities appropriately is often unique to\nindividuals, attributing to many different styles.\nStyles include gradually building up to a punch-\nline using text, audio, video or in combination\nof any of them, a sudden twist to the story with\nan unexpected punchline (Ramachandran, 1998),\ncreating a discrepancy between modalities (e.g.,\nsomething funny being said without any emotion,\narXiv:1904.06618v1  [cs.LG]  14 Apr 2019\nalso known as dry humor), or just laughing with\nthe speech to stimulate the audience to mirror the\nlaughter (Provine, 1992).\nModeling humor using a computational frame-\nwork is inherently challenging due to factors such\nas: 1) Idiosyncrasy: often humorous people are\nalso the most creative ones (Hauck and Thomas,\n1972). This creativity in turn adds to the dynamic\ncomplexity of how humor is expressed in a multi-\nmodal manner. Use of words, gestures, prosodic\ncues and their (mis)alignments are toolkits that a\ncreative user often experiments with. 2) Contex-\ntual Dependencies: humor often develops through\ntime as speakers plan for a punchline in advance.\nThere is a gradual build up in the story with a\nsudden twist using a punchline (Ramachandran,\n1998). Some punchlines when viewed in isola-\ntion (as illustrated in Figure 1) may not appear\nfunny. The humor stems from the prior build up,\ncross-referencing multiple sources, and its deliv-\nery. Therefore, a full understanding of humor re-\nquires analyzing the context of the punchline.\nUnderstanding the unique dependencies across\nmodalities and its impact on humor require knowl-\nedge from multimodal language; a recent research\ntrend in the ﬁeld of natural language processing\n(Zadeh et al., 2018b). Studies in this area aim to\nexplain natural language from three modalities of\ntext, vision and acoustic. In this paper, alongside\ncomputational descriptors for text, gestures such\nas smile or vocal properties such as loudness are\nmeasured and put together in a multimodal frame-\nwork to deﬁne humor recognition as a multimodal\ntask.\nThe main contribution of this paper to the NLP\ncommunity is introducing the ﬁrst multimodal lan-\nguage (including text, vision and acoustic modal-\nities) dataset of humor detection named “UR-\nFUNNY”.\nThis dataset opens the door to un-\nderstanding and modeling humor in a multimodal\nframework. The studies in this paper present per-\nformance baselines for this task and demonstrate\nthe impact of using all three modalities together\nfor humor modeling.\n2\nBackground\nThe dataset and experiments in this paper are con-\nnected to the following areas:\nHumor Analysis:\nHumor analysis has been\namong active areas of research in both natural lan-\nguage processing and affective computing. No-\nDataset\n#Pos\n#Neg\nMod\ntype\n#spk\n16000 One-Liners 16000 16000\n{l}\njoke\n-\nPun of the Day\n2423\n2423\n{l}\npun\n-\nPTT Jokes\n1425\n2551\n{l}\npolitical\n-\nTed Laughter\n4726\n4726\n{l}\nspeech\n1192\nBig Bang Theory\n18691 24981\n{l,a}\ntv show\n<50\nUR-Funny\n8257\n8257\n{l,a,v}\nspeech\n1741\nTable 1: Comparison between UR-FUNNY and no-\ntable humor detection datasets in the NLP community.\nHere, ‘pos’, ’neg’ , ‘mod’ and ‘spk’ denote positive,\nnegative, modalities and speaker respectively.\ntable datasets in this area include “16000 One-\nLiners” (Mihalcea and Strapparava, 2005), “Pun\nof the Day” (Yang et al., 2015), “PTT Jokes”\n(Chen and Soo, 2018), “Ted Laughter” (Chen and\nLee, 2017), and “Big Bang Theory”\n(Bertero\net al., 2016).\nThe above datasets have studied\nhumor from different perspectives. For example,\n“16000 One-Liner” and “Pun of the Day” focus\non joke detection (joke vs. not joke binary task),\nwhile “Ted Laughter” focuses on punchline detec-\ntion (whether or not punchline triggers laughter).\nSimilar to “Ted Laughter”, UR-FUNNY focuses\non punchline detection. Furthermore, punchline\nis accompanied by context sentences to properly\nmodel the build up of humor.\nUnlike previous\ndatasets where negative samples were drawn from\na different domain, UR-FUNNY uses a challeng-\ning negative sampling case where samples are\ndrawn from the same videos. Furthermore, UR-\nFUNNY is the only humor detection dataset which\nincorporates all three modalities of text, vision and\naudio. Table 1 shows a comparison between previ-\nously proposed datasets and UR-FUNNY dataset.\nFrom modeling aspect, humor detection is done\nusing hand-crafted and non-neural models (Yang\net al., 2015), neural based RNN and CNN models\nfor detecting humor in Yelp (de Oliveira et al.,\n2017) and TED talks\n(Chen and Lee, 2017).\nNewer approaches have used\n(Chen and Soo,\n2018) highway networks “16000 One-Liner” and\n“Pun of the Day” datasets. There have been very\nfew attempts at using extra modalities alongside\nlanguage for detecting humor, mostly limited to\nadding simple audio features (Rakov and Rosen-\nberg, 2013; Bertero et al., 2016).\nFurthermore,\nthese attempts have been restricted to certain top-\nics and domains (such as “Big Bang Theory” TV\nshow (Bertero et al., 2016)).\nMultimodal Language Analysis: Studying nat-\nural language from modalities of text, vision and\nacoustic is a recent research trend in natural lan-\nguage processing (Zadeh et al., 2018b). Notable\nworks in this area present novel multimodal neu-\nral architectures (Wang et al., 2019; Pham et al.,\n2019; Hazarika et al., 2018; Poria et al., 2017;\nZadeh et al., 2017), multimodal fusion approaches\n(Liang et al., 2018; Tsai et al., 2018; Liu et al.,\n2018; Zadeh et al., 2018a; Barezi et al., 2018)\nas well as resources (Poria et al., 2018a; Zadeh\net al., 2018c, 2016; Park et al., 2014; Rosas et al.,\n2013; W¨ollmer et al., 2013).\nMultimodal lan-\nguage datasets mostly target multimodal sentiment\nanalysis (Poria et al., 2018b), emotion recognition\n(Zadeh et al., 2018c; Busso et al., 2008), and per-\nsonality traits recognition (Park et al., 2014). UR-\nFUNNY dataset is similar to the above datasets in\ndiversity (speakers and topics) and size, with the\nmain task of humor detection. Beyond the scope\nof multimodal language analysis, the dataset and\nstudies in this paper have similarities to other ap-\nplications in multimodal machine learning such\nlanguage and vision studies, robotics, image cap-\ntioning, and media description (Baltruˇsaitis et al.,\n2019).\n3\nUR-FUNNY Dataset\nIn this section we present the UR-FUNNY dataset.\nWe ﬁrst discuss the data acquisition process, and\nsubsequently present statistics of the dataset as\nwell as multimodal feature extraction and valida-\ntion.\n3.1\nData Acquisition\nA suitable dataset for the task of multimodal hu-\nmor detection should be diverse in a) speakers:\nmodeling the idiosyncratic expressions of humor\nmay require a dataset with large number of speak-\ners, and b) topics: different topics exhibit different\nstyles of humor as the context and punchline can\nbe entirely different from one topic to another.\nTED talks 1 are among the most diverse idea\nsharing channels, in both speakers and topics.\nSpeakers from various backgrounds, ethnic groups\nand cultures present their thoughts through a\nwidely popular channel 2. The topics of these pre-\nsentations are diverse; from scientiﬁc discoveries\nto everyday ordinary events. As a result of diver-\nsity in speakers and topics, TED talks span across\n1Videos on www.ted.com are publicly available for\ndownload.\n2More than 12 million subscribers on YouTube https:\n//www.youtube.com/user/TEDtalksDirector\na broad spectrum of humor. Therefore, this plat-\nform presents a unique resource for studying the\ndynamics of humor in a multimodal setup.\nTED videos include manual transcripts and au-\ndience markers. Transcriptions are highly reliable,\nwhich in turn allow for aligning the text and audio.\nThis property makes TED talks a unique resource\nfor newest continuous fusion trends (Chen et al.,\n2017). Transcriptions also include reliably anno-\ntated markers for audience behavior. Speciﬁcally,\nthe “laughter” marker has been used in NLP stud-\nies as an indicator of humor (Chen and Lee, 2017).\nPrevious studies have identiﬁed the importance of\nboth punchline and context in understanding and\nmodeling the humor. In a humorous scenario, con-\ntext is the gradual build up of a story and punch-\nline is a sudden twist to the story which causes\nlaughter (Ramachandran, 1998). Using the pro-\nvided laughter marker, the sentence immediately\nbefore the marker is considered as the punchline\nand the sentences prior to punchline (but after pre-\nvious laughter marker) are considered context.\nWe collect 1866 videos as well as their tran-\nscripts from TED portal. These 1866 videos are\nchosen from 1741 different speakers and across\n417 topics. The laughter markup is used to ﬁl-\nter out 8257 humorous punchlines from the tran-\nscripts (Chen and Lee, 2017). The context is ex-\ntracted from the prior sentences to the punchline\n(until the previous humor instances or the begin-\nning of video is reached).\nUsing a similar ap-\nproach, 8257 negative samples are chosen at ran-\ndom intervals where the last sentence is not im-\nmediately followed by a laughter marker. The last\nsentence is assumed a punchline and similar to the\npositive instances, the context is chosen. This neg-\native sampling uses sentences from the same dis-\ntribution, as opposed to datasets which use sen-\ntences from other distributions or domains as neg-\native sample\n(Yang et al., 2015; Mihalcea and\nStrapparava, 2005). After this negative sampling,\nthere is a homogeneous 50% split in the dataset\nbetween positive and negative examples.\nUsing forced alignment, we mark the beginning\nand end of each sentence in the video as well as\nwords and phonemes in the sentences (Yuan and\nLiberman, 2008). Therefore, an alignment is es-\ntablished between text, audio and video. Utilizing\nthis alignment, the timing of punchline as well as\ncontext is extracted for all instances in the dataset.\n \n \n  (a) Distribution of Punchline Sentence Length in Number of Words\n(c) Distribution of Context Length\nin Number of Sentences\n               (d) Distribution of Punchline (left) and Context (right) \n             Sentence Duration in Seconds\n(e) Ted Talk Categories \n<4 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39>39\n0\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0\n1\n2\n3\n4\n>=5\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\nsleep\nHIV\nGuns\ntoy\nIran\nGod\npain\nhack\npiano\njazz\nbees\nnovel\nfunny\nevil\nants\niraq\nsmell\nAIDS\nIslam\nebola\nurban\nPTSD\nvocals\ncyborg\nmeme\nBrand\nBrazil\nglacier\nDebate\nSyria\nsuicide\nmarkets\nmining\ntelecom\nresources\nsinger\nstreet art\norigami\nviolin\nTED-Ed\n(b) Distribution of Context Sentence Length in Number of Words\n<4 4\n5\n6\n7\n8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39>39\n0\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n  humor       non-humor\n  humor        non-humor\n0\n2\n4\n6\n8\n10\n12\n14\n16\n0\n2\n4\n6\n8\n10\n12\n14\nnon-humor\nhumor\nnon-humor\nhumor\nnon-humor\nhumor\nFigure 2: Overview of UR-FUNNY dataset statistics. (a) the distribution of punchline sentence length for humor\nand non-humor cases. (b) the distribution of context sentence length for humor and non-humor cases. (c) distri-\nbution of the number of sentences in the context. (d) distribution of the duration (in seconds) of punchline and\ncontext sentences. (e) topics of the videos in UR-FUNNY dataset. Best viewed in zoomed and color.\nGeneral\ntotal #videos\n1866\ntotal duration in hour\n90.23\ntotal #distinct speakers\n1741\ntotal #distinct topics\n417\ntotal #humor instances\n8257\ntotal #non-humor instances\n8257\ntotal #words\n965573\ntotal #unique words\n32995\ntotal #sentences\n63727\navg length of sentences in words\n15.15\navg duration of sentences (s)\n4.64\nPunchline\n#sentences in punchline\n1\navg #words in punchline\n16.14\navg #words in humorous punchline\n15.17\navg #words in non-humorous punchline\n17.10\navg duration of punchline (s)\n4.97\navg duration of humorous punchline (s)\n4.58\navg duration of non-humorous punchline (s)\n5.36\nContext\navg total #words in context\n42.33\navg #words in context sentences\n14.80\navg #sentences in context\n2.86\navg #sentences in humorous context\n2.82\navg #sentences in non-humorous context\n2.90\navg duration of context (s)\n14.7\navg duration of humorous context (s)\n13.79\navg duration of non-humorous context (s)\n15.62\navg duration of context sentences (s)\n4.25\navg duration of humorous context sentences (s)\n4.79\navg duration of non-humorous context sentences (s)\n4.52\nTable 2: Summary of the UR-FUNNY dataset statis-\ntics. Here, ‘#’ denotes number, ‘avg’ denotes average\nand ‘s’ denotes seconds\n3.2\nDataset Statistics\nThe high level statistics of UR-FUNNY dataset\nare presented in Table 2.\nTotal duration of the\nentire dataset is 90.23 hours.\nThere are a total\nof 1741 distinct speakers and a total of 417 dis-\ntinct topics in the UR-FUNNY dataset. Figure 2.e\nshows the word cloud of the topics based on log-\nfrequency of the topic. The top most ﬁve frequent\ntopics are technology, science, culture, global is-\nsues and design 3. There are in total 16514 video\nsegments of humor and not humor instances (equal\nsplits of 8257). The average duration of each data\ninstance is 19.67 seconds, with context an average\nof 14.7 and punchline with an average of 4.97 sec-\nonds. The average number of words in punchline\nis 16.14 and the average number of words in con-\ntext sentences is 14.80.\nFigure 2 shows an overview for some of the\nimportant statistics of UR-FUNNY dataset. Fig-\nure 2.a demonstrates the distribution of punchline\nfor humor and non-humor cases based on number\nof words. There is no clear distinction between\nhumor and non-humor punchlines as both follow\nsimilar distribution. Similarly, Figure 2.b shows\nthe distribution of number of words per context\nsentence.\nBoth humor and non-humor context\nsentences follow the same distribution. Majority\n3Metadata collected from www.ted.com\nTrain\nVal\nTest\n#humor instances\n5306\n1313\n1638\n#not humor instances\n5292\n1313\n1652\n#videos used\n1166\n300\n400\n#speakers\n1059\n294\n388\navg #words in punchline\n15.81\n16.94\n16.55\navg #words in context\n41.69\n42.86\n43.94\navg #sentences in context\n2.84\n2.81\n2.95\npunchline avg duration(second)\n4.85\n5.25\n5.15\ncontext avg duration(second)\n14.39\n14.91\n15.54\nTable 3: Statistics of train, validation & test folds of\nUR-FUNNY dataset. Here, ‘avg’ denotes average and\n‘#’ denotes number.\n(≥90%) of punchlines have length less than 32.\nIn terms of number of seconds, Figure 2.d shows\nthe distribution of punchline and context sentence\nlength in terms of seconds.\nFigure 2.c demon-\nstrates the distribution of number of context sen-\ntences per humor and non-humor data instances.\nNumber of context sentences per humor and non-\nhumor case is also roughly the same. The statistics\nin Figure 2 show that there is no trivial or degen-\nerate distinctions between humor and non-humor\ncases. Therefore, classiﬁcation of humor versus\nnon-humor cases cannot be done based on simple\nmeasures (such as number of words); it requires\nunderstanding the content of sentences.\nTable 3 shows the standard train, validation and\ntest folds of the UR-FUNNY dataset. These folds\nshare no speaker with each other - hence standard\nfolds are speaker independent (Zadeh et al., 2016).\nThis minimizes the chance of overﬁtting to iden-\ntity of the speakers or their communication pat-\nterns.\n3.3\nExtracted Features\nFor each modality, the extracted features are as\nfollows:\nLanguage: Glove word embeddings (Pennington\net al., 2014) are used as pre-trained word vectors\nfor the text features.\nP2FA forced alignment\nmodel (Yuan and Liberman, 2008) is used to align\nthe text and audio on phoneme level. From the\nforce alignment, we extract the timing annotations\nof context and punchline on word level. Then, the\nacoustic and visual cues are aligned on word level\nby interpolation (Chen et al., 2017).\nAcoustic: COVAREP software (Degottex et al.,\n2014) is used to extraction acoustic features at\nthe rate of 30 frame/sec.\nWe extract follow-\ning 81 features:\nfundamental frequency (F0),\nVoiced/Unvoiced\nsegmenting\nfeatures\n(VUV)\n(Drugman and Alwan, 2011), normalized am-\nplitude quotient (NAQ), quasi open quotient\n(QOQ) (Kane and Gobl, 2013), glottal source\nparameters (H1H2, Rd,Rd conf) (Drugman et al.,\n2012; Alku et al., 2002, 1997), parabolic spectral\nparameter (PSP), maxima dispersion quotient\n(MDQ), spectral tilt/slope of wavelet responses\n(peak/slope), Mel cepstral coefﬁcient (MCEP\n0-24), harmonic model and phase distortion mean\n(HMPDM 0-24) and deviations (HMPDD 0-12),\nand the ﬁrst 3 formants. These acoustic features\nare related to emotions and tone of speech.\nVisual:\nOpenFace facial behavioral analysis\ntool (Baltruˇsaitis et al., 2016) is used to extract\nthe facial expression features at the rate of 30\nframe/sec.\nWe extract all facial Action Units\n(AU) features based on the Facial Action Coding\nSystem (FACS) (Ekman, 1997). Rigid and non-\nrigid facial shape parameters are also extracted\n(Baltruˇsaitis et al., 2016). We observed that the\ncamera angle and position changes frequently\nduring TED presentations.\nHowever, for the\nmajority of time, the camera stays focused on\nthe presenter. Due to the volatile camera work,\nthe only consistently available source of visual\ninformation was the speaker’s face.\nUR-FUNNY dataset is publicly available for\ndownload alongside all the extracted features.\n4\nMultimodal Humor Detection\nIn this section, we ﬁrst outline the problem for-\nmulation for performing binary multimodal hu-\nmor detection on UR-FUNNY dataset. We then\nproceed to study the UR-FUNNY dataset through\nthe lens of a contextualized extension of Memory\nFusion Network (MFN) (Zadeh et al., 2018a) - a\nstate-of-the-art model in multimodal language.\n4.1\nProblem Formulation\nUR-FUNNY dataset is a multimodal dataset with\nthree modalities of text, vision and acoustic. We\ndenote the set of these modalities as M = {t,v,a}.\nEach of the modalities come in a sequential form.\nWe assume word-level alignment between modal-\nities (Yuan and Liberman, 2008). Since frequency\nof the text modality is less than vision and acous-\ntic (i.e. vision and acoustic have higher sampling\n!\",$,%\n!\",$,$\n...\n!\",$,&'(\n!),$,%\n!),$,$\n...\n!),$,&'(\n!*,$,%\n!*,$,$\n...\n!*,$,&'(\n+ = 1\n+ = 2\n...\n...\n...\n+ = /0\n+ = 2\nContext\nLSTMt\nLSTMv\nLSTMa\nℎ*,$\nℎ\",$\nℎ),$\n2\nFigure 3: The structure of Unimodal Context Network\nas outlined in Section 4.2.1. For demonstration pur-\npose, we show the case for n = 2 (second context sen-\ntence). After n = NC, the output H (outlined by blue)\nis complete. Best viewed in color.\nrate), we use expected visual and acoustic descrip-\ntors for each word (Chen et al., 2017).\nAfter\nthis process, each modality has the same sequence\nlength (each word has a single vision and acoustic\nvector accompanied with it).\nEach data sample in the UR-FUNNY can be de-\nscribed as a triplet (l,P,C) with l being a binary\nlabel for humor or non-humor. P is the punch-\nline and C is the context.\nBoth punchline and\ncontext have multiple modalities P = {Pm;m ∈\nM}, C = {Cm;m ∈M}. If there are NC con-\ntext sentences accompanying the punchline, then\nCm = [Cm,1,Cm,2,...,Cm,NC] - simply context\nsentences start from ﬁrst sentence to the last (NC)\nsentence. KP is the number of words in the punch-\nline and KCn∣NC\nn=1 is the number of words in each\nof the context sentences respectively. As examples\nof this notation, Pm,k refers to the kth entry in the\nmodality m of the punchline. Cm,n,k refers to the\nkth entry in the modality m of the nth context.\nModels developed on UR-FUNNY dataset are\ntrained on triplets of (l,P,C). During testing only\na tuple (P,C) is given to predict the l. l is the label\nfor laughter, speciﬁcally whether or not the inputs\nP,C are likely to trigger a laughter.\n4.2\nContextual Memory Fusion Baseline\nMemory Fusion Network (MFN) is among the\nstate-of-the-art models for several multimodal\ndatasets (Zadeh et al., 2018a). We devise an exten-\nsion of the MFN model, named Contextual Mem-\nory Fusion Network 4(C-MFN), as a baseline for\nhumor detection on UR-FUNNY dataset. This is\ndone by introducing two components to allow the\ninvolvement of context in the MFN model: 1) Uni-\nmodal Context Network, where information from\neach modality is encoded using M Long-short\nTerm Memories (LSTM), 2) Multimodal Context\nNetwork, where unimodal context information are\nfused (using self-attention) to extract the multi-\nmodal context information. We discuss the com-\nponents of the C-MFN model in the continuation\nof this section.\n4.2.1\nUnimodal Context Network\nTo model the context, we ﬁrst model each modal-\nity within the context.\nUnimodal Context Net-\nwork (Figure 3) consists of M LSTMs, one for\neach modality m ∈M denoted as LSTMm. For\neach context sentence n of each modality m ∈M,\nLSTMm is used to encode the information into a\nsingle vector hm,n. This single vector is the last\noutput of the LSTMm over Cm,n as input. The\nrecurrence step for each LSTM is the utterance\nof each word (due to word-level alignment vision\nand acoustic modalities also follow this time-step).\nThe output of the Unimodal Context Network is\nthe set H = {hm,n;m ∈M,1 ≤n < NC}.\n4.2.2\nMultimodal Context Network\nMultimodal Context Network (Figure 4) learns a\nmultimodal representation of the context based on\nthe output H of the Unimodal Context Network.\nSentences and modalities in the context can form\ncomplex asynchronous spatio-temporal relations.\nFor example, during the gradual buildup of the\ncontext, the speaker’s facial expression may be im-\npacted due to an arbitrary previously uttered sen-\ntence. Transformers (Vaswani et al., 2017) are a\nfamily of neural models that specialize in ﬁnd-\ning various temporal relations between their inputs\nthrough self-attention. By concatenating represen-\ntations hm∈M,n (i.e. for all M modalities of the\nnth context), self-attention model can be applied\nto ﬁnd asynchronous spatio-temporal relations in\n4Code available through hidden-for-blind-review.\n!\n\" = 1\n\" = 2\n...\n...\n...\n\" = &'\nEmbedding\nSelf-Attention\nResidual\nFeed Forward\nResidual\nEncoder\n(!\nFigure 4: The structure of Multimodal Context Net-\nwork as outlined in Section 4.2.2. The output H of the\nUnimodal Context Network is connected to an encoder\nmodule to get the multimodal output ˆH. For the details\nof components outlined in orange please refer to the\nauthors’ original paper. (Vaswani et al., 2017). Best\nviewed in color.\nthe context. We use an encoder with 6 intermedi-\nate layers to derive a multimodal representation ˆH\nconditioned on H. ˆH is also spatio-temporal (as\nproduced output of encoders in a transformer are).\nThe output of Multimodal Context Network is the\noutput ˆH of the encoder.\n4.2.3\nMemory Fusion Network (MFN)\nAfter learning unimodal (H) and multimodal ( ˆH)\nrepresentations of context, we use a Memory Fu-\nsion Network (MFN) to model the punchline (Fig-\nure 5).\nMFN contains 2 types of memories:\na System of LSTMs with M unimodal memo-\nries to model each modality in punchline, and\na Multi-view Gated Memory which stores multi-\nmodal information. We use a simple trick to com-\nbine the Context Networks (Unimodal and Mul-\ntimodal) with the MFN: we initialize the memo-\nries in the MFN using the outputs H (unimodal\nrepresentation) and\nˆH (multimodal representa-\ntion).\nFor System of LSTMs, this is done by\ninitializing the LSTM cell state of modality m\nwith Dm(hm,1≤n<NC). Dm is a fully connected\nneural network that maps the information from\nSystem of LSTMs\nMulti-view Gated\nMemory\nDelta-memory \nAttention\nℎ\",$%& %'( )\"\nℎ*,$%& %'( )*\nℎ+,$%& %'( )+\n,- ∈/,$\n,- ∈/,0\n,- ∈/,12\n. . .\n34\n4\n)\nFigure 5: The initialization and recurrence process of\nMemory Fusion Network (MFN). The outputs of Uni-\nmodal and Multimodal Context Networks (H and ˆH)\nare used initializing the MFN neural components. For\nthe details of components outlined in orange please re-\nfer to the authors’ original paper (Zadeh et al., 2018a).\nBest viewed in color.\nhm,1≥j≥NC (mth modality in context) to the cell\nstate of the mth LSTM in the System of LSTMs.\nThe Multi-view Gated Memory is initialized based\non a non-linear projection D( ˆH) where D is a\nfully connected neural network. Similar to con-\ntext where modalities are aligned at word level,\npunchline is also aligned the same way. There-\nfore a word-level implementation of the MFN is\nused, where a word and accompanying vision and\nacoustic descriptors are used as input to the Sys-\ntem of LSTMs at each time-step. The Multi-view\nGated Memory is updated iteratively at every re-\ncurrence of the System of LSTMs using a Delta-\nmemory Attention Network.\nThe ﬁnal prediction of humor is conditioned on\nthe last state of the System of LSTMs and Multi-\nview Gated Memory using an afﬁne mapping with\nSigmoid activation.\n5\nExperiments\nIn the experiments of this paper, our goal is\nto establish a performance baseline for the\nUR-FUNNY dataset.\nFurthermore, we aim to\nunderstand the role of context and punchline, as\nwell as role of individual modalities in the task\nof humor detection. For all the experiments, we\nuse the proposed contextual extension of Memory\nModality\nT\nA+V\nT+A\nT+V\nT+A+V\nC-MFN (P)\n62.85\n53.3\n63.28\n63.22\n64.47\nC-MFN (C)\n57.96\n50.23\n57.78\n57.99\n58.45\nC-MFN\n64.44\n57.99\n64.47\n64.22\n65.23\nTable 4: Binary accuracy for different variants of C-\nMFN and training scenarios outlined in Section 5. The\nbest performance is achieved using all three modalities\nof text (T), vision (V) and acoustic (A).\nFusion Network (MFN), called C-MFN (Section\n4.2).\nAside the proposed C-MFN model, the\nfollowing variants are also studied:\nC-MFN (P): This variant of the C-MFN uses\nonly punchline with no contextual information.\nEssentially, this is equivalent to a MFN model\nsince initialization trick is not used.\nC-MFN (C): This variant of the C-MFN uses\nonly contextual information without punchline.\nEssentially, this is equivalent to removing the\nMFN and directly conditioning the humor pre-\ndiction on the Unimodal and Multimodal Context\nNetwork outputs (Sigmoid activated neuron after\napplying DM;m ∈M on H and D on ˆH).\nThe above variants of the C-MFN allow for\nstudying the importance of punchline and con-\ntext in modeling humor. Furthermore, we com-\npare the performance of the C-MFN variants in\nthe following scenarios: (T) a only text modal-\nity is used without vision and acoustic, (T+V) text\nand vision modalities are used without acoustic,\n(T+A) text and acoustic modalities are used with-\nout vision, (A+V) only vision and acoustic modal-\nities are used, (T+A+V) all modalities are used to-\ngether.\nWe compare the performance of C-MFN vari-\nants across the above scenarios. This allows for\nunderstanding the role of context and punchline in\nhumor detection, as well as the importance of dif-\nferent modalities. All the models for our experi-\nments are trained using categorical cross-entropy.\nThis measure is calculated between the output of\nthe model and ground-truth labels.\n6\nResults and Discussion\nThe results of our experiments are presented in Ta-\nble 4. Results demonstrate that both context and\npunchline information are important as C-MFN\noutperforms C-MFN (P) and C-MFN (C) models.\nPunchline is the most important component for de-\ntecting humor as the performance of C-MFN (P) is\nsigniﬁcantly higher than C-MFN (C).\nModels that use all modalities (T+A+V) out-\nperform models that use only one or two modal-\nities (T, T+A, T+V, A+V). Between text (T) and\nnonverbal behaviors (A+V), text shows to be the\nmost important modality. Most of the cases, both\nmodalities of vision and acoustic improve the per-\nformance of text alone (T+V, T+A).\nBased on the above observations, each neural\ncomponent of the C-MFN model is useful in im-\nproving the prediction of humor. The results also\nindicate that modeling humor from a multimodal\nperspective yields successful results.\nThe human performance 5 on the UR-FUNNY\ndataset is 82.5%.\nThe results from Table 4 demonstrate that while\na state-of-the-art model can achieve a reason-\nable level of success in modeling humor, there is\nstill a large gap between human-level performance\nwith state of the art.\nTherefore, UR-FUNNY\ndataset presents new challenges to the ﬁeld of\nNLP, speciﬁcally research areas of humor detec-\ntion and multimodal language analysis.\n7\nConclusion\nIn this paper, we presented a new multimodal\ndataset for humor detection called UR-FUNNY.\nThis dataset is the ﬁrst of its kind in the NLP com-\nmunity. Humor detection is done from the per-\nspective of predicting laughter - similar to (Chen\nand Lee, 2017). UR-FUNNY is diverse in both\nspeakers and topics. It contains three modalities\nof text, vision and acoustic. We study this dataset\nthrough the lens of a Contextualized Memory Fu-\nsion Network (C-MFN). Results of our experi-\nments indicate that humor can be better modeled\nif all three modalities are used together. Further-\nmore, both context and punchline are important in\nunderstanding humor. The dataset and the accom-\npanying experiments will be made publicly avail-\nable.\n5This is calculated by averaging the performance of two\nannotators over a shufﬂed set of 100 humor and 100 non-\nhumor cases. The annotators are given the same input as\nthe machine learning models (similar context and punchline).\nThe annotators agree 84% of times.\nReferences\nPaavo\nAlku,\nTom\nB¨ackstr¨om,\nand\nErkki\nVilk-\nman. 2002.\nNormalized amplitude quotient for\nparametrization of the glottal ﬂow. the Journal of\nthe Acoustical Society of America, 112(2):701–710.\nPaavo Alku, Helmer Strik, and Erkki Vilkman. 1997.\nParabolic spectral parametera new method for quan-\ntiﬁcation of the glottal ﬂow. Speech Communica-\ntion, 22(1):67–79.\nTadas Baltruˇsaitis,\nChaitanya Ahuja,\nand Louis-\nPhilippe Morency. 2019.\nMultimodal machine\nlearning: A survey and taxonomy. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence,\n41(2):423–443.\nTadas Baltruˇsaitis, Peter Robinson, and Louis-Philippe\nMorency. 2016.\nOpenface:\nan open source fa-\ncial behavior analysis toolkit. In 2016 IEEE Win-\nter Conference on Applications of Computer Vision\n(WACV), pages 1–10. IEEE.\nElham J Barezi, Peyman Momeni, Pascale Fung, et al.\n2018. Modality-based factorization for multimodal\nfusion. arXiv preprint arXiv:1811.12624.\nDario Bertero, Pascale Fung, X Li, L Wu, Z Liu,\nB Hussain, Wc Chong, Km Lau, Pc Yue, W Zhang,\net al. 2016. Deep learning of audio and language\nfeatures for humor prediction. In LREC.\nCarlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe\nKazemzadeh, Emily Mower, Samuel Kim, Jean-\nnette N Chang, Sungbok Lee, and Shrikanth S\nNarayanan. 2008. Iemocap: Interactive emotional\ndyadic motion capture database.\nLanguage re-\nsources and evaluation, 42(4):335.\nLei Chen and Chong Min Lee. 2017. Predicting audi-\nence’s laughter during presentations using convolu-\ntional neural network. In Proceedings of the 12th\nWorkshop on Innovative Use of NLP for Building\nEducational Applications, pages 86–90.\nMinghai Chen, Sen Wang, Paul Pu Liang, Tadas Bal-\ntruˇsaitis, Amir Zadeh, and Louis-Philippe Morency.\n2017.\nMultimodal sentiment analysis with word-\nlevel fusion and reinforcement learning. In Proceed-\nings of the 19th ACM International Conference on\nMultimodal Interaction, pages 163–171. ACM.\nPeng-Yu Chen and Von-Wun Soo. 2018. Humor recog-\nnition using deep learning. In Proceedings of the\n2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 2 (Short Pa-\npers), volume 2, pages 113–117.\nGilles Degottex, John Kane, Thomas Drugman, Tuomo\nRaitio, and Stefan Scherer. 2014.\nCovarepa col-\nlaborative voice analysis repository for speech tech-\nnologies. In 2014 ieee international conference on\nacoustics, speech and signal processing (icassp),\npages 960–964. IEEE.\nThomas Drugman and Abeer Alwan. 2011. Joint ro-\nbust voicing detection and pitch estimation based on\nresidual harmonics. In Twelfth Annual Conference\nof the International Speech Communication Associ-\nation.\nThomas Drugman, Mark Thomas, Jon Gudnason,\nPatrick Naylor, and Thierry Dutoit. 2012.\nDetec-\ntion of glottal closure instants from speech signals:\nA quantitative review. IEEE Transactions on Audio,\nSpeech, and Language Processing, 20(3):994–1006.\nRosenberg Ekman. 1997. What the face reveals: Basic\nand applied studies of spontaneous expression using\nthe Facial Action Coding System (FACS). Oxford\nUniversity Press, USA.\nRandy Garner. 2005. Humor, analogy, and metaphor:\nHam it up in teaching. Radical Pedagogy, 6(2):1.\nWilliam E Hauck and John W Thomas. 1972. The re-\nlationship of humor to intelligence, creativity, and\nintentional and incidental learning. The journal of\nexperimental education, 40(4):52–55.\nDevamanyu Hazarika, Soujanya Poria, Amir Zadeh,\nErik Cambria, Louis-Philippe Morency, and Roger\nZimmermann. 2018.\nConversational memory net-\nwork for emotion recognition in dyadic dialogue\nvideos. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), volume 1, pages\n2122–2132.\nYu-Hsiang\nHuang.\n2018.\nattention-\nis-all-you-need-pytorch.\nhttps:\n//github.com/jadore801120/\nattention-is-all-you-need-pytorch.\nJohn Kane and Christer Gobl. 2013. Wavelet maxima\ndispersion for breathy to tense voice discrimination.\nIEEE Transactions on Audio, Speech, and Language\nProcessing, 21(6):1170–1179.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nPaul Pu Liang, Ziyin Liu, Amir Zadeh, and Louis-\nPhilippe Morency. 2018.\nMultimodal language\nanalysis with recurrent multistage fusion.\narXiv\npreprint arXiv:1808.03920.\nZhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-\nnarasimhan, Paul Pu Liang, Amir Zadeh, and Louis-\nPhilippe Morency. 2018. Efﬁcient low-rank multi-\nmodal fusion with modality-speciﬁc factors. arXiv\npreprint arXiv:1806.00064.\nRada Mihalcea and Carlo Strapparava. 2005. Making\ncomputers laugh: Investigations in automatic humor\nrecognition. In Proceedings of the Conference on\nHuman Language Technology and Empirical Meth-\nods in Natural Language Processing, pages 531–\n538. Association for Computational Linguistics.\nLuke de Oliveira, ICME Stanford, and Alfredo L´ainez\nRodrigo. 2017. Humor detection in yelp reviews.\nSunghyun Park, Han Suk Shim, Moitreya Chatterjee,\nKenji Sagae, and Louis-Philippe Morency. 2014.\nComputational analysis of persuasiveness in social\nmultimedia: A novel dataset and multimodal predic-\ntion approach. In Proceedings of the 16th Interna-\ntional Conference on Multimodal Interaction, pages\n50–57. ACM.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014.\nGlove: Global vectors for word\nrepresentation. In Proceedings of the 2014 confer-\nence on empirical methods in natural language pro-\ncessing (EMNLP), pages 1532–1543.\nHai Pham, Paul Pu Liang, Thomas Manzini, Louis-\nPhilippe Morency, and Barnabas Poczos. 2019.\nFound in translation: Learning robust joint repre-\nsentations by cyclic translations between modalities.\narXiv preprint arXiv:1812.07809.\nSoujanya Poria,\nErik Cambria,\nDevamanyu Haz-\narika, Navonil Mazumder, Amir Zadeh, and Louis-\nPhilippe Morency. 2017.\nMulti-level multiple at-\ntentions for contextual multimodal sentiment analy-\nsis. In 2017 IEEE International Conference on Data\nMining (ICDM), pages 1033–1038. IEEE.\nSoujanya Poria, Devamanyu Hazarika, Navonil Ma-\njumder, Gautam Naik, Erik Cambria, and Rada Mi-\nhalcea. 2018a.\nMeld: A multimodal multi-party\ndataset for emotion recognition in conversations.\narXiv preprint arXiv:1810.02508.\nSoujanya Poria, Amir Hussain, and Erik Cambria.\n2018b. Multimodal Sentiment Analysis, volume 8.\nSpringer.\nRobert R Provine. 1992. Contagious laughter: Laugh-\nter is a sufﬁcient stimulus for laughs and smiles.\nBulletin of the Psychonomic Society, 30(1):1–4.\nRachel Rakov and Andrew Rosenberg. 2013. ” sure, i\ndid the right thing”: a system for sarcasm detection\nin speech. In Interspeech, pages 842–846.\nVilayanur S Ramachandran. 1998. The neurology and\nevolution of humor, laughter, and smiling: the false\nalarm theory. Medical hypotheses, 51(4):351–354.\nVer´onica P´erez Rosas, Rada Mihalcea, and Louis-\nPhilippe Morency. 2013.\nMultimodal sentiment\nanalysis of spanish online videos. IEEE Intelligent\nSystems, 28(3):38–45.\nDavid Stauffer. 1999. Let the good times roll: Build-\ning a fun culture.\nHarvard Management Update,\n4(10):4–6.\nYao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh,\nLouis-Philippe Morency, and Ruslan Salakhutdinov.\n2018. Learning factorized multimodal representa-\ntions. arXiv preprint arXiv:1806.06176.\nRobert A Vartabedian and Laurel Klinger Vartabedian.\n1993. Humor in the workplace: A communication\nchallenge.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nYansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang,\nAmir Zadeh, and Louis-Philippe Morency. 2019.\nWords can shift: Dynamically adjusting word rep-\nresentations using nonverbal behaviors.\narXiv\npreprint arXiv:1811.09362.\nMelissa B Wanzer, Ann B Frymier, and Jeffrey Irwin.\n2010. An explanation of the relationship between\ninstructor humor and student learning: Instructional\nhumor processing theory. Communication Educa-\ntion, 59(1):1–18.\nMartin W¨ollmer, Felix Weninger, Tobias Knaup, Bj¨orn\nSchuller, Congkai Sun, Kenji Sagae, and Louis-\nPhilippe Morency. 2013. Youtube movie reviews:\nSentiment analysis in an audio-visual context. IEEE\nIntelligent Systems, 28(3):46–53.\nDiyi Yang, Alon Lavie, Chris Dyer, and Eduard Hovy.\n2015. Humor recognition and humor anchor extrac-\ntion. In Proceedings of the 2015 Conference on Em-\npirical Methods in Natural Language Processing,\npages 2367–2376.\nJiahong Yuan and Mark Liberman. 2008.\nSpeaker\nidentiﬁcation on the scotus corpus. Journal of the\nAcoustical Society of America, 123(5):3878.\nAmir Zadeh, Minghai Chen, Soujanya Poria, Erik\nCambria, and Louis-Philippe Morency. 2017. Ten-\nsor fusion network for multimodal sentiment analy-\nsis. arXiv preprint arXiv:1707.07250.\nAmir Zadeh, Paul Pu Liang, Navonil Mazumder,\nSoujanya Poria, Erik Cambria, and Louis-Philippe\nMorency. 2018a. Memory fusion network for multi-\nview sequential learning.\nIn Thirty-Second AAAI\nConference on Artiﬁcial Intelligence.\nAmir Zadeh, Paul Pu Liang, Louis-Philippe Morency,\nSoujanya Poria, Erik Cambria, and Stefan Scherer.\n2018b. Proceedings of grand challenge and work-\nshop on human multimodal language (challenge-\nhml).\nIn\nProceedings\nof\nGrand\nChallenge\nand Workshop on Human Multimodal Language\n(Challenge-HML).\nAmir Zadeh, Rowan Zellers, Eli Pincus, and Louis-\nPhilippe Morency. 2016.\nMosi: multimodal cor-\npus of sentiment intensity and subjectivity anal-\nysis in online opinion videos.\narXiv preprint\narXiv:1606.06259.\nAmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Po-\nria, Erik Cambria, and Louis-Philippe Morency.\n2018c. Multimodal language analysis in the wild:\nCmu-mosei dataset and interpretable dynamic fu-\nsion graph. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), volume 1, pages 2236–\n2246.\nA\nHyperparameter Space Search\nIn this appendix we present the hyperparameter\nspace explored for C-MFN model.\n• Uni-modal Context Network:\n1. This module has three LSTMs.\nHid-\nden size for them was chosen randomly\nfrom:\n– For LSTMl:[32,64,88,128,156,256]\n– For LSMTa:[8,16,32,48,64,80]\n– For LSTMv:[8,16,32,48,64,80]\n• Multimodal Context Network:\n1. We use the optimal conﬁgurations as de-\nscribed in (Vaswani et al., 2017) and im-\nplemented in (Huang, 2018). Some of\nthe main conﬁgurations are:\n– d model(output dimension of En-\ncoder):512,\n– d k(dimension of key):64,\n– d v(dimension of value):64,\n– n head(number of heads used in\nmulti-headed attention):8,\n– n layers(number of layers used in\nEncoder):6,\n– n warmup steps:4000,\n– dropout:0.1\n2. To regularize the output of D( ˜H), we\nrandomly choose a dropout rate from\n[0.0,0.2,0.5,0.1]\n3. To regularize the output Dm(H), we\nuse a dropout probability randomly\nfrom:\n– For m=l:[0.0,0.1,0.2,0.5]\n– For m=a:[0.0,0.2,0.5,0.1]\n– For m=v:[0.0,0.2,0.5,0.1]\n• Memory Fusion Network(MFN):\n1. System of LSTMs:\nHidden size of\nLSTMm, m ∈[l,a,v] was randomly\nchosen from:\n– For LSTMl,[32,64,88,128,156,256]\n– For LSTMa,[8,16,32,48,64,80]\n– For LSTMv,[8,16,32,48,64,80]\n2. Delta Memory Attention:This section\nhas two afﬁne transformation, we call\nthem NN1 and NN2.\n– The\nprojection\nshape\nof\nNN1\nis\nchosen\nrandomly\nfrom\n[32,64,128,256]\nand\nthat\nout-\nput goes through a dropout layer\nwhose\ndropout\nrate\nis\nchosen\nrandomly from [0.0,0.2,0.5,0.7]\n– Similarly, the projection shape of\nNN2\nis\nchosen\nrandomly\nfrom\n[32,64,128,256]\nfollowed\nby\na\ndropout\nlayer\nwhose\ndropout\nrate\nis\nchosen\nrandomly\nfrom\n[0.0,0.2,0.5,0.7].\n3. Multi-view gated memory also has two\nafﬁne transformation denoted here as\nGamma1 and Gamma2.\n– Gamma1 ﬁrst does a projection\nof shape chosen randomly from\n[32,64,128,256]\nfollowed\nby\na\ndropout whose rate is randomly cho-\nsen from [0.0,0.2,0.5,0.7].\n– Gamma2 ﬁrst does a projection\nand then a dropout. The projection\nshape is chosen randomly from\n[32,64,128,256]\nand\ndropout\nrate\nis\nchosen\nfrom\nrandomly\n[0.0,0.2,0.5,0.7].\n– The memory size of this module\nis chosen randomly from the set\n[64,128,256,300,400].\n• Optimizer After some trial and error, we\nfound that the model works best for an\nAdam optimizer(Kingma and Ba, 2014)\ninitialized with β1\n=\n0.9,β2\n=\n0.98\nand\nϵ\n=\n10−9.\nThe\nlearning\nrate\nwas varied by the formula learning rate\n= d0.5\nmodel * min(step num−0.5,step num *\nwarmup steps−1.5).\nThe optimizer and the\nscheduler is identical to the one chosen in\n(Vaswani et al., 2017)\n",
  "categories": [
    "cs.LG",
    "cs.CL",
    "stat.ML"
  ],
  "published": "2019-04-14",
  "updated": "2019-04-14"
}