{
  "id": "http://arxiv.org/abs/1904.07248v1",
  "title": "Machine Learning in Astronomy: a practical overview",
  "authors": [
    "Dalya Baron"
  ],
  "abstract": "Astronomy is experiencing a rapid growth in data size and complexity. This\nchange fosters the development of data-driven science as a useful companion to\nthe common model-driven data analysis paradigm, where astronomers develop\nautomatic tools to mine datasets and extract novel information from them. In\nrecent years, machine learning algorithms have become increasingly popular\namong astronomers, and are now used for a wide variety of tasks. In light of\nthese developments, and the promise and challenges associated with them, the\nIAC Winter School 2018 focused on big data in Astronomy, with a particular\nemphasis on machine learning and deep learning techniques. This document\nsummarizes the topics of supervised and unsupervised learning algorithms\npresented during the school, and provides practical information on the\napplication of such tools to astronomical datasets. In this document I cover\nbasic topics in supervised machine learning, including selection and\npreprocessing of the input dataset, evaluation methods, and three popular\nsupervised learning algorithms, Support Vector Machines, Random Forests, and\nshallow Artificial Neural Networks. My main focus is on unsupervised machine\nlearning algorithms, that are used to perform cluster analysis, dimensionality\nreduction, visualization, and outlier detection. Unsupervised learning\nalgorithms are of particular importance to scientific research, since they can\nbe used to extract new knowledge from existing datasets, and can facilitate new\ndiscoveries.",
  "text": "Draft version April 17, 2019\nTypeset using LATEX preprint2 style in AASTeX61\nMACHINE LEARNING IN ASTRONOMY: A PRACTICAL OVERVIEW\nDalya Baron1\n1School of Physics and Astronomy\nTel-Aviv University\nTel Aviv 69978, Israel\nABSTRACT\nAstronomy is experiencing a rapid growth in data size and complexity. This change fosters the\ndevelopment of data-driven science as a useful companion to the common model-driven data analysis\nparadigm, where astronomers develop automatic tools to mine datasets and extract novel informa-\ntion from them.\nIn recent years, machine learning algorithms have become increasingly popular\namong astronomers, and are now used for a wide variety of tasks. In light of these developments,\nand the promise and challenges associated with them, the IAC Winter School 2018 focused on big\ndata in Astronomy, with a particular emphasis on machine learning and deep learning techniques.\nThis document summarizes the topics of supervised and unsupervised learning algorithms presented\nduring the school, and provides practical information on the application of such tools to astronomical\ndatasets. In this document I cover basic topics in supervised machine learning, including selection\nand preprocessing of the input dataset, evaluation methods, and three popular supervised learning\nalgorithms, Support Vector Machines, Random Forests, and shallow Artiﬁcial Neural Networks. My\nmain focus is on unsupervised machine learning algorithms, that are used to perform cluster analysis,\ndimensionality reduction, visualization, and outlier detection. Unsupervised learning algorithms are\nof particular importance to scientiﬁc research, since they can be used to extract new knowledge from\nexisting datasets, and can facilitate new discoveries.\nKeywords: methods: data analysis, methods: statistical\ndalyabaron@gmail.com\narXiv:1904.07248v1  [astro-ph.IM]  15 Apr 2019\n2\n1. CONTEXT\nAstronomical datasets are undergoing a rapid\ngrowth in size and complexity, thus introduc-\ning Astronomy to the era of big data science\n(e.g., Ball & Brunner 2010; Pesenson et al.\n2010). This growth is a result of past, ongo-\ning, and future surveys, that produce massive\nmulti-temporal and multi-wavelength datasets,\nwith a wealth of information to be extracted and\nanalyzed. Such surveys include the Sloan Digi-\ntal Sky Survey (SDSS; York et al. 2000), which\nprovided the community with multi-color im-\nages of ∼1/3 of sky, and high-resolution spec-\ntra of millions of Galactic and extra-galactic ob-\njects.\nPan-STARRS (Kaiser et al. 2010) and\nthe Zwicky Transient Facility (Bellm 2014) per-\nform a systematic exploration of the variable\nsky, delivering time-series of numerous aster-\noids, variable stars, supernovae, active galac-\ntic nuclei, and more. Gaia (Gaia Collaboration\net al. 2016) is charting the three-dimensional\nmap of the Milky Way, and will provide accurate\npositional and radial velocity measurements for\nover a billion stars in our Galaxy and through-\nout the Local Group. Future surveys, e.g., DESI\n(Levi et al. 2013), SKA (Dewdney et al. 2009),\nand LSST (Ivezic et al. 2008), will increase the\nnumber of available objects and their measured\nproperties by more than an order of magnitude.\nIn light of this accelerated growth, astronomers\nare developing automated tools to detect, char-\nacterize, and classify objects using the rich\nand complex datasets gathered with the dif-\nferent facilities.\nMachine learning algorithms\nhave gained increasing popularity among as-\ntronomers, and are widely used for a variety of\ntasks.\nMachine learning algorithms are generally di-\nvided into two groups.\nSupervised machine\nlearning algorithms are used to learn a map-\nping from a set of features to a target vari-\nable, based on example input-output pairs pro-\nvided by a human expert (see e.g., Connolly\net al. 1995; Collister & Lahav 2004; Re Fiorentin\net al. 2007; Mahabal et al. 2008; Daniel et al.\n2011; Laurino et al. 2011; Morales-Luis et al.\n2011; Bloom et al. 2012; Brescia et al. 2012;\nRichards et al. 2012; Krone-Martins et al. 2014;\nMasci et al. 2014; Miller 2015; Wright et al.\n2015; Djorgovski et al. 2016; D’Isanto et al.\n2016; Lochner et al. 2016; Castro et al. 2018;\nNaul et al. 2018; D’Isanto & Polsterer 2018;\nD’Isanto et al. 2018; Krone-Martins et al. 2018;\nZucker & Giryes 2018; Delli Veneri et al. 2019;\nIshida et al. 2019; Mahabal et al. 2019; Nor-\nris et al. 2019; Reis et al. 2019). Unsupervised\nlearning algorithms are used to learn complex\nrelationships that exist in the dataset, with-\nout labels provided by an expert.\nThese can\nroughly be divided into clustering, dimension-\nality reduction, and anomaly detection (e.g.,\nBoroson & Green 1992; Protopapas et al. 2006;\nD’Abrusco et al. 2009; Vanderplas & Connolly\n2009; S´anchez Almeida et al. 2010; Ascasibar &\nS´anchez Almeida 2011; D’Abrusco et al. 2012;\nMeusinger et al. 2012; Fustes et al. 2013; Krone-\nMartins & Moitinho 2014; Baron et al. 2015;\nHocking et al. 2015; Gianniotis et al. 2016; Nun\net al. 2016; Polsterer et al. 2016; Baron & Poz-\nnanski 2017; Reis et al. 2018a,b). The latter al-\ngorithms are arguably more important for scien-\ntiﬁc research, since they can be used to extract\nnew knowledge from existing datasets, and can\npotentially facilitate new discoveries.\nIn view of the shift in data analysis paradigms\nand associated challenges, the IAC Winter\nSchool 2018 focused on big data in Astron-\nomy.\nIt included both lectures and hands-on\ntutorials, which are publicly available through\ntheir website1. The school covered the following\ntopics: (1) general overview on the use of ma-\nchine learning techniques in Astronomy: past,\npresent and perspectives, (2) data challenges\nand solutions in forthcoming surveys, (3) su-\n1 http://www.iac.es/winterschool/2018/\nMachine Learning in Astronomy\n3\npervised learning: classiﬁcation and regression,\n(4) unsupervised learning and dimensionality\nreduction techniques, and (5) shallow and deep\nneural networks.\nIn this document I summa-\nrize the topics of supervised and unsupervised\nlearning algorithms, with special emphasis on\nunsupervised techniques. This document is not\nintended to provide a rigorous statistical back-\nground, but rather to present practical infor-\nmation on popular machine learning algorithms\nand their application to astronomical datasets.\nSupervised learning algorithms are discussed\nin section 2, with an emphasis on optimiza-\ntion (section 2.1), input datasets (section 2.2),\nand three popular algorithms: Support Vector\nMachine (section 2.3), Decision Trees and Ran-\ndom Forest (section 2.4), and shallow Artiﬁcial\nNeural Networks (section 2.5).\nUnsupervised\nlearning algorithms are discussed in section 3,\nin particular distance assignment (section 3.1),\nclustering algorithms (section 3.2), dimension-\nality reduction algorithms (section 3.3), and\nanomaly detection algorithms (section 3.4).\n2. SUPERVISED LEARNING\nSupervised machine learning algorithms are\nused to learn a relationship between a set of\nmeasurements and a target variable using a\nset of provided examples. Once obtained, the\nrelationship can be used to predict the tar-\nget variable of previously-unseen data.\nThe\nmain diﬀerence between traditional model ﬁt-\nting techniques and supervised learning algo-\nrithms is that in traditional model ﬁtting the\nmodel is predeﬁned, while supervised learning\nalgorithms construct the model according to the\ninput dataset. Supervised learning algorithms\ncan, by construction, describe very complex\nnon-linear relations between the set of measure-\nments and the target variable, and can therefore\nbe superior to traditional algorithms that are\nbased on ﬁtting of predeﬁned models.\nIn machine learning terminology, the dataset\nconsists of objects, and each object has mea-\nsured features and a target variable. In Astron-\nomy, the objects are usually physical entities\nsuch as stars or galaxies, and their features are\nmeasured properties, such as spectra or light-\ncurves, or various higher-level quantities derived\nfrom observations, such as a variability period\nor stellar mass. The type of target variable de-\npends on the particular task.\nIn a classiﬁca-\ntion task, the target variables are discrete (often\ncalled labels), for example, classiﬁcation of spec-\ntra into stars or quasars. In a regression task,\nthe target variable is continuous, for example,\nredshift estimation using photometric measure-\nments.\nSupervised learning algorithms often have\nmodel parameters that are estimated from the\ndata. These parameters are part of the model\nthat is learned from the data, are often saved\nas part of the learned model, and are required\nby the model when making predictions.\nEx-\namples of model parameters include: support\nvectors in Support Vector Machines, splitting\nfeatures and thresholds in Decision Trees and\nRandom Forest, and weights of Artiﬁcial Neu-\nral Networks. In addition, supervised learning\nalgorithms often have model hyper-parameters,\nwhich are external to the model and whose val-\nues and are often set using diﬀerent heuristics.\nExamples of model hyper-parameters include:\nthe kernel shape in Support Vector Machines,\nthe number of trees in Random Forests, and\nthe number of hidden layers in Artiﬁcial Neural\nNetworks.\nThe application of supervised learning algo-\nrithms is usually divided into three stages. In\nthe training stage, the model hyper-parameters\nare set, and the model and the model param-\neters are learned from a subset of the input\ndataset, called the training set. In the valida-\ntion stage, the model hyper-parameters are op-\ntimized according to some predeﬁned cost func-\ntion, often using a diﬀerent subset of the in-\nput dataset, called the validation set. During\n4\nthe validation stage, the model training is car-\nried out iteratively for many diﬀerent choices\nof hyper-parameters, and the hyper-parameters\nthat result in the best performance on the vali-\ndation set are chosen. Finally, in the test stage,\nthe trained model is used to predict the tar-\nget variable of a diﬀerent subset of the input\ndataset, called the test set.\nThe latter stage\nis necessary in order to assess the performance\nof the trained model on a previously-unseen\ndataset, i.e., a subset of the input data that\nwas not used during the training and valida-\ntion stages, and can be used to compare diﬀer-\nent supervised learning algorithms. Once these\nstages are completed, the model can be used to\npredict the target variable of new, previously-\nunseen datasets.\nThis section provides some basic principles\nof supervised machine learning, and presents\nseveral popular algorithms used in Astronomy.\nFor a detailed review on the subject, see Biehl\n(2019).\nThe section starts by describing the\ncost functions that are usually used to opti-\nmize model hyper-parameters, assess the per-\nformance of the ﬁnal model, and compare dif-\nferent supervised learning algorithms (section\n2.1). Then, section 2.2 gives additional details\non the input dataset, in particular its parti-\ntion to training, validation, and test sets, fea-\nture selection and normalization, and imbal-\nanced datasets.\nFinally, three popular algo-\nrithms are presented: Support Vector Machine\n(section 2.3), Decision Trees and Random For-\nest (section 2.4), and shallow Artiﬁcial Neural\nNetworks (section 2.5).\n2.1. Evaluation Metrics\nThere are diﬀerent evaluation metrics one can\nuse to optimize supervised learning algorithms.\nEvaluation metrics are used to optimize the\nmodel hyper-parameters, to assess the perfor-\nmance of the ﬁnal model, to select optimal sub-\nset of features, and to compare between diﬀer-\nent supervised learning algorithms. The evalua-\n1\n2\n4\n5\n6\n8\n13\nPredicted label\n1\n2\n4\n5\n6\n8\n13\nTrue label\n94\n2\n0\n2\n0\n0\n0\n18 81\n0\n0\n0\n0\n0\n32\n0\n53 14\n0\n0\n0\n32\n0\n1\n65\n0\n0\n0\n26\n0\n5\n66\n0\n1\n0\n78\n0\n0\n4\n0\n13\n0\n1\n1\n5\n1\n2\n3\n83\nFigure 1. Example of a confusion matrix taken\nfrom Mahabal et al. (2017), who trained a deep\nlearning model to distinguish between 7 classes of\nvariable stars, marked by 1, 2, 4, 5, 6, 8, and 13\nin the diagram. The confusion matrix shows the\nnumber of objects in each class versus the number\nof objects predicted by the model to belong to a\nparticular class. In the best-case scenario, the con-\nfusion matrix will contain non-zero elements only\nin its diagonal, and zero elements otherwise.\ntion metrics are computed during the validation\nand the test stages, where the trained model is\napplied to a previously-unseen subset of the in-\nput dataset, and the target variable predicted\nby the model is compared to the target variable\nprovided in the input data.\nIn regression tasks, where the target variable\nis continuous, the common metrics for eval-\nuating the predictions of the model are the\nMean Absolute Error (MAE) and the Mean\nSquared Error (MSE). The MAE is equal to\n1\nn\nPn\ni=1 |yi −ˆyi|, where n is the number of ob-\njects in the validation or test set, ˆyi is the tar-\nget variable predicted by the model, and yi is\nthe target variable provided in the input data.\nThe MSE is equal to\n1\nn\nPn\ni=1 (yi −ˆyi)2.\nThe\nMachine Learning in Astronomy\n5\nMSE has the disadvantage of heavily weight-\ning outliers, since it squares each term in the\nsum, giving outliers a larger weight. When out-\nlier weighting is undesirable, it is better to use\nthe MAE instead. Finally, it is worth noting\nadditional metrics used in the literature, for ex-\nample, the Normalized Median Absolute Devi-\nation, the Continuous Rank Probability Score,\nand the Probability Integral Transform (see e.g.,\nD’Isanto & Polsterer 2018; D’Isanto et al. 2018).\nIn classiﬁcation tasks, where the target vari-\nable is discrete, the common evaluation metrics\nare the Classiﬁcation Accuracy, the Confusion\nMatrix, and the Area Under ROC Curve. Clas-\nsiﬁcation accuracy is the ratio of the number\nof correct predictions (i.e., the class predicted\nby the model is similar to the class provided in\nthe input dataset) to the total number of pre-\ndictions made. This value is obviously bound\nbetween 0 and 1. The accuracy should be used\nwhen the number of objects in each class is\nroughly similar, and when all predictions and\nprediction errors are equally important. Con-\nfusion matrices are used in classiﬁcation tasks\nwith more than two classes.\nFigure 1 shows\nan example of a confusion matrix, taken from\nMahabal et al. (2017), who trained a super-\nvised learning model to distinguish between 7\nclasses of variable stars. Each row and column\nin the matrix represents a particular class of ob-\njects, and the matrix shows the number of ob-\njects in each class versus the number of objects\npredicted by the model to belong to a particu-\nlar class. In the best-case scenario, we expect\nthe confusion matrix to be purely diagonal, with\nnon-zero elements on the diagonal, and zero ele-\nments otherwise. Furthermore, similarly to the\naccuracy, one can normalize the confusion ma-\ntrix to show values that range from 0 to 1, thus\nremoving the dependence on the initial number\nof objects in each class.\nFinally, the receiver operating characteristic\ncurve (ROC curve) is a useful visualization tool\n0.0\n0.5\n1.0\nFalse Positive Rate\n0.0\n0.5\n1.0\nTrue Positive Rate\nmodel 1\nmodel 2\nA B C\nFigure 2. An illustration of an ROC curve, where\nthe true positive rate is plotted against the false\npositive rate. In the best-case scenario, we expect\nthe true positive rate to be 1, and the false positive\nrate to be 0. The black line represents the result-\ning ROC curve for random assignment of classes,\nwhich is the worst-case scenario. The diagram is\npopulated by varying the model hyper-parameters\nand plotting the true positive rate versus the false\npositive rate obtained for the validation set. The\npink curve represents the ROC curve of model 1,\nwhere A, B, and C represent three particular choices\nof hyper-parameter value. The purple curve repre-\nsents the ROC curve of model 2. The area under\nthe ROC curve can be used to select the optimal\nmodel, which, in this case, is model 1.\nof a supervised algorithm performance in a bi-\nnary classiﬁcation task. Figure 2 shows an il-\nlustration of an ROC curve, where the True\nPositive Rate is plotted against the False Posi-\ntive Rate. The true positive rate represents the\nnumber of ”true” events that are correctly iden-\ntiﬁed by the algorithm, divided by the total\nnumber of ”true” events in the input dataset.\nThe false positive rate represents the number\nof ”false” events that were wrongly classiﬁed as\n”true” events divided by the total number of\n6\n”false” events.\nFor example, if we are inter-\nested in detecting gravitational lenses in galaxy\nimages, ”true” events are images with gravi-\ntational lenses, and ”false” events are images\nwithout gravitational lenses.\nIn the best-case\nscenario, we expect the true positive rate to\nbe 1, and the false positive rate to be 0. The\nROC curve diagram is generated by varying the\nmodel hyper-parameters, and plotting the true\npositive rate versus the false positive rate ob-\ntained for the validation set (the continuous\nROC curves presented in Figure 2 are produced\nby varying the classiﬁcation/detection thresh-\nold of a particular algorithm.\nThis threshold\ncan be considered as a model hyper-parameter).\nFurthermore, the diagram can be used to com-\npare the performance of diﬀerent supervised\nlearning algorithms, by selecting the algorithm\nwith the maximal area under the curve. For ex-\nample, in Figure 2, model 1 outperforms model\n2 for any choice of hyper-parameters. Finally,\ndepending on the task, one can decide to op-\ntimize diﬀerently. For some tasks one cannot\ntolerate false negatives (e.g., scanning for ex-\nplosives in luggage), while for others the total\nnumber of errors is more important.\n2.2. Input Dataset\nThe input to any supervised learning algo-\nrithm consists of a set of objects with measured\nfeatures, and a target variable which can be ei-\nther continuous or discrete.\nAs noted in the\nintroduction to this section, the input dataset\nis divided into three sets, the training, valida-\ntion, and test sets.\nThe model is initially ﬁt\nto the training set.\nThen, the model is ap-\nplied to the validation set. The validation set\nprovides an unbiased evaluation of the model\nperformance while tuning the model’s hyper-\nparameters.\nValidation sets are also used for\nregularization and to avoid overﬁtting, with the\ncommon practice of stopping the training pro-\ncess when the error on the validation dataset\nincreases. Finally, the test set is used to pro-\nvide an unbiased evaluation of the ﬁnal model,\nand can be used to compare between diﬀerent\nsupervised learning algorithms. To perform a\ntruly unbiased evaluation of the model perfor-\nmance, the training, validation, and test sets\nshould be mutually exclusive.\nThe dataset splitting ratios depend on the\ndataset size and on the algorithm one trains.\nSome algorithms require a substantial amount\nof data to train on, forcing one to enlarge the\ntraining set at the expense of the others. Algo-\nrithms with a few hyper-parameters, which are\neasily validated and tuned, require small vali-\ndation sets, whereas models with many hyper-\nparameters might require larger validation sets.\nIn addition, in Cross Validation the dataset can\nbe repeatedly split into training and validation\nsets, for example, by randomly selecting objects\nfrom a predeﬁned set, and the model is then\niteratively trained and validated on these dif-\nferent sets (see e.g., Miller et al. 2017). There\nare diﬀerent splitting methods that are imple-\nmented in python and are publicly available in\nthe scikit-learn library2.\nThe performance of all supervised learning al-\ngorithms strongly depends on the input dataset,\nand in particular on the features that are se-\nlected to form the dataset.\nMost supervised\nlearning algorithms are not constructed to work\nwith hundreds or thousands of features, making\nfeature selection a key part of the applied ma-\nchine learning process.\nFeature selection can\nbe done manually by an expert in the ﬁeld, by\ndeﬁning features that are most probably rele-\nvant for the task at hand.\nThere are various\nalternative ways to select an optimal set of fea-\ntures without domain knowledge, including ﬁl-\nter methods, wrapper methods, and embedded\nmethods.\nFilter methods assign a statistical\nscore to each feature, and features are selected\n2\nhttps://scikit-learn.org/stable/model_\nselection.html\nMachine Learning in Astronomy\n7\nor removed according to this score. In wrapper\nmethods, diﬀerent combinations of features are\nprepared, and the combination that results in\nthe best accuracy is selected. Embedded meth-\nods learn which features best contribute to the\naccuracy of the model during the model con-\nstruction (see also Donalek et al. 2013; D’Isanto\net al. 2016, 2018). Some to these methods are\nincluded in the scikit-learn library3. Finally,\na pertinent note on deep learning, in particular\nConvolutional Neural Networks. The structure\nof these networks allows them to take raw data\nas an input (e.g., spectra, light-curves, and im-\nages), and perform eﬃcient feature extraction\nduring the training process. Thus, using such\nmodels, there is usually no need to perform fea-\nture selection prior to the training stage.\nFeature scaling is an additional key part of\nthe data preparation process. While some algo-\nrithms do not require any feature scaling prior\nto training (e.g., Decision Trees and Random\nForest), the performance of other algorithms\nstrongly depends on it, and it is advised to apply\nsome feature scaling prior to their training (e.g.,\nfor Support Vector Machine). There are various\nways to scale the features in the initial dataset,\nincluding standardization, mean normalization,\nmin-max scaling, and application of dimension-\nality reduction algorithms to the initial dataset.\nThese will not be further discussed in this doc-\nument, however, many feature scaling methods\nare available in scikit-learn4.\nFinally, it is worth noting the topic of im-\nbalanced datasets.\nImbalanced data typically\nrefers to the problem of a classiﬁcation task\nwhere the classes are not equally represented.\nDuring training, many supervised learning al-\ngorithms assign equal weights to all the objects\nin the sample, resulting in a good performance\n3\nhttps://scikit-learn.org/stable/modules/\nfeature_selection.html\n4\nhttps://scikit-learn.org/stable/modules/\npreprocessing.html\non the larger class, and worse performance on\nthe smaller class. In addition, the regular ac-\ncuracy cannot be used to evaluate the resulting\nmodel. Assume for example that we are inter-\nested in detecting gravitational lenses, and our\ndataset contains 100 000 images of galaxies, out\nof which 100 images show evidence of lensing.\nFor such a dataset, an algorithm that classiﬁes\nall objects as ”not lens”, regardless of the input\nfeatures, will have an accuracy of 0.999. There\nare several methods to train and evaluate su-\npervised learning algorithms in the presence of\nimbalanced datasets.\nOne could apply diﬀer-\nent weights to diﬀerent classes of objects during\nthe training process, or undersample the larger\nclass, or oversample the smaller class of objects.\nThese, and additional methods, are available in\nscikit-learn. Instead of the classiﬁcation ac-\ncuracy, one can use the ROC curve (ﬁgure 2),\nand select the hyper-parameters that result in\nthe desired true positive versus false negative\nrates.\n2.3. Support Vector Machine\nOne of the most popular supervised learning\nalgorithms is Support Vector Machine (SVM),\nwhich has been applied in Astronomy for a va-\nriety of tasks (e.g., Qu et al. 2003; Huertas-\nCompany et al. 2008; Fadely et al. 2012; Ma lek\net al. 2013; Kov´acs & Szapudi 2015; Krakowski\net al. 2016; Hartley et al. 2017; Hui et al. 2018;\nKsoll et al. 2018; Pashchenko et al. 2018). Given\na dataset with N features, SVM ﬁnds a hyper-\nplane in the N-dimensional space that best sep-\narates the given classes. In a two-dimensional\nspace, this hyperplane is a line that divides the\nplane into two parts, where every class lies on a\ndiﬀerent side.\nThe optimal hyperplane is de-\nﬁned to be the plane that has the maximal\nmargin, i.e the maximum distance between the\nplane and the data points. The latter are called\nthe support vectors. Once obtained, the hyper-\nplane serves as a decision boundary, and new\nobjects are classiﬁed according to their location\n8\n4\n6\n8\n10\nfeature 1\n12\n10\n8\n6\n4\n2\nfeature 2\nFigure 3.\nIllustration of the SVM best hyper-\nplane for a two-dimensional dataset with linearly-\nseparable classes. The two classes are plotted with\npink and purple circles, and the support vectors are\nmarked with black circles. The hyperlane is marked\nwith a solid grey line.\nwith respect to the hyperplane. Figure 3 shows\nan illustration of the SVM hyperplane for a two-\ndimensional dataset, in which the classes are\nlinearly-separable.\nMore often than not, the diﬀerent classes in\nthe dataset are not linearly-separable.\nThe\nleft panel of ﬁgure 4 shows an example of a\ntwo-dimensional dataset with two classes, which\nare not linearly-separable (i.e., the classes can-\nnot be separated using a single straight line).\nThe classiﬁcation problem can be approached\nwith the SVM kernel trick.\nInstead of con-\nstructing the decision boundary in the input\ndata space, the dataset is mapped into a trans-\nformed feature space, which is of higher dimen-\nsion, where linear separation might be possible.\nOnce the decision boundary is found, it is back-\nprojected to the original input space, resulting\nin a non-linear boundary.\nThe middle panel\nof ﬁgure 4 shows the three-dimensional feature\nspace that resulted from such a mapping, where,\nin this representation, the classes are linearly-\nseparable.\nThe right panel of ﬁgure 4 shows\nthe result of the back-projection of the decision\nboundary. To apply the kernel trick, one must\ndeﬁne the kernel function that is related to the\nnon-linear feature mapping.\nThere is a wide\nvariety of kernel functions, the most popular\nbeing Gaussian Radial Basis Function (RBF),\nPolynomial, and Sigmoid. The kernel function\nis a hyper-parameter of SVM, and these func-\ntions usually depend on additional parameters,\nwhich are also hyper-parameters of the model.\nSVM is available in scikit-learn5\nSVM is simple and robust, allowing one\nto classify a large variety of datasets and to\nconstruct very non-linear decision boundaries.\nSince SVM is based on measuring Euclidean\ndistances between the objects in the sample\nand the hyperplane, it is very sensitive to fea-\nture scaling.\nTherefore, it is advised to scale\nthe features. SVM can be applied to datasets\nwith many features, but its performance might\nbe strongly aﬀected by the presence of irrel-\nevant features in the dataset.\nIt is therefore\nrecommended to perform feature selection prior\nto the training.\n2.4. Decision Trees and Random Forest\nEnsemble methods are meta-algorithms that\ncombine several supervised learning techniques\ninto a single predictive model, resulting in an\noverall improved performance, compared to the\nperformance of each individual supervised algo-\nrithm. Ensemble methods either combine dif-\nferent supervised learning algorithms, or com-\nbine the information of a single algorithm that\nwas trained on diﬀerent subsets of the training\nset. One of the most popular ensemble methods\nis Random Forest, which is a collection of Deci-\nsion Trees (Breiman et al. 1984; Breiman 2001).\nRandom Forest is mainly used as a supervised\nalgorithm for classiﬁcation and regression (e.g.,\nCarliles et al. 2010; Bloom et al. 2012; Pichara\n5\nhttps://scikit-learn.org/stable/modules/\ngenerated/sklearn.svm.SVC.html\nMachine Learning in Astronomy\n9\n1\n0\n1\nfeature 1\n1\n0\n1\nfeature 2\ninput data space\nfeature 1\n1\n0\n1\nfeature 2\n1\n0\n1\nr\n0.5\n1.0\nfeature space\n1\n0\n1\nfeature 1\n1\n0\n1\nfeature 2\ninput data space\nFigure 4. Application of the SVM kernel trick to a two-dimensional dataset that consists of two classes\nwhich are not linearly-separable. The left panel shows the dataset, where the diﬀerent classes are represented\nby pink and purple circles. The middle panel shows the three-dimensional feature space that resulted from\nthe applied mapping, where the classes can be separated with a two-dimensional hyperplane. The right panel\nshows the result of back-projecting the decision boundary to the input space, where the support vectors are\nmarked with black circles, and the decision boundary with a solid grey line.\net al. 2012; Pichara & Protopapas 2013; M¨oller\net al. 2016; Miller et al. 2017; Plewa 2018; Yong\net al. 2018; Ishida et al. 2019), but can also be\nused in an unsupervised setting, to produce sim-\nilarity measures between the objects in the sam-\nple (Shi & Horvath 2006; Baron & Poznanski\n2017; Reis et al. 2018a,b).\nA decision tree is a non-parametric model\nconstructed during the training stage, which is\ndescribed by a top-to-bottom tree-like graph,\nand is used in both classiﬁcation and regression\ntasks. The decision tree is a set of consecutive\nnodes, where each node represents a condition\non one feature in the dataset. The conditions\nare of the form Xj > Xj,th, where Xj is the\nvalue of the feature at index j and Xj,th is some\nthreshold, both of which are determined during\nthe training stage. The lowest nodes in the tree\nare called terminal nodes or leaves, and they\ndo not represent a condition, but instead carry\nthe assigned label of a particular path within\nthe tree.\nFigure 5 shows a visualization of a\ntrained decision tree, taken from Vasconcellos\net al. (2011), who trained decision tree classi-\nﬁers to distinguish stars from galaxies.\nTo describe the training process of the deci-\nsion tree, I consider the simple case of a clas-\nsiﬁcation task with two classes.\nThe training\nprocess starts with the entire training set in the\nhighest node of the tree – the root. The algo-\nrithm searches for the feature Xj and the fea-\nture threshold Xj,th that result in the best sep-\naration of the two classes, where the deﬁnition\nof best separation is a model hyper-parameter,\nwith the typical choices being the Gini impurity\nor the information gain. Once the best feature\nand best threshold are determined, the training\nset propagates to the left and right nodes below\nthe root, according to the established condition.\nThis process is repeated recursively, such that\ndeeper nodes split generally smaller subsets of\nthe original data. In its simplest version, the\nrecursive process stops when every leaf of the\ntree contains a single class of objects.\nOnce\nthe decision tree is trained, it can be used to\npredict the class of previously-unseen objects.\nThe prediction is done by propagating the ob-\nject through the tree, according to its measured\nfeatures and the conditions in the nodes. The\npredicted class of the object is then the label of\nthe terminal leaf (for additional information see\n10\nBreiman et al. 1984; Vasconcellos et al. 2011;\nReis et al. 2019).\nDecision trees have several advantages. First,\nin their simpler forms, they have very few hyper-\nparameters, and can be applied to large datasets\nwith numerous features. Second, their recursive\nstructures are easily interpretable, in particu-\nlar, they can be used to determine the feature\nimportance. Feature importance represents the\nrelative importance of diﬀerent features to the\nclassiﬁcation task at hand. Since the trees are\nconstructed recursively with the aim of splitting\nthe dataset into the predeﬁned classes, features\nthat are selected earlier in the training process,\ncloser to the root, are more important than fea-\ntures that are selected later, closer to the ter-\nminal nodes. Obviously, features that were not\nselected in any node during the training process,\ncarry little relevant information to the classiﬁ-\ncation task. In more complex versions, decision\ntrees can provide a measure of uncertainty for\nthe predicted classes (see e.g., Breiman et al.\n1984; Vasconcellos et al. 2011). However, in the\nsimplest version, there are no restrictions on the\nnumber of nodes or the depth of the resulting\ntree, making the algorithm extremely sensitive\nto outliers.\nThe resulting classiﬁer will typi-\ncally show a perfect performance on the training\nset, but a poor performance on new previously-\nunseen datasets. A single decision tree is typi-\ncally prone to overﬁtting the training data, and\ncannot generalize to new datasets. Therefore, it\nis rarely used in its single form.\nRandom Forest is a collection of decision trees,\nwhere diﬀerent decision trees are trained on dif-\nferent randomly-selected subsets of the original\ntraining set, and during the training of each in-\ndividual tree, random subsets of the features are\nused to construct the conditions in individual\nnodes. This randomness reduces the correlation\nbetween the diﬀerent trees, resulting in some-\nwhat diﬀerent tree structures with diﬀerent con-\nditions in their nodes. The Random Forest pre-\nFigure 5. An example of a trained decision tree,\ntaken from Vasconcellos et al. (2011). The decision\ntree contains nodes which represent conditions on\nfeatures from the dataset, in this case petroR90 r,\npsfMag r, and nLStar r. The terminal nodes rep-\nresent the assigned label of each particular path\nwithin the tree, which are 1, 1, 2, 1, and 2 from\nleft to right, and represent stars and galaxies.\ndiction is an aggregate of individual predictions\nof the trees in the forest, in the form of a ma-\njority vote. That is, a previously-unseen object\nis propagated through the diﬀerent trees, and\nits assigned label is the label reached in the ma-\njority of the trees. While a single decision tree\ntends to overﬁt the training data, the combina-\ntion of many decision trees in the form of a Ran-\ndom Forest generalizes well to previously un-\nseen datasets, resulting in a better performance\n(for additional information see Breiman 2001).\nAs previously noted, Random Forest is one of\nthe most popular machine learning algorithms\nin Astronomy.\nRandom Forest can be applied to datasets\nwith thousands of features, with a moderate\nincrease in running time.\nThe algorithm has\na handful of hyper-parameters, such as the\nnumber of trees in the forest and the num-\nber of randomly-selected features to consider\nin each node in each tree. In terms of perfor-\nmance, model complexity, and number of hyper-\nparameters, Random Forest is between SVM\nand Deep Neural Networks. The main disad-\nMachine Learning in Astronomy\n11\nvantage of Random Forest, which applies to\nmost supervised learning algorithms, is its in-\nability to take into account feature and label\nuncertainties. This topic is of particular inter-\nest to astronomers, and I discuss it below in\nsection 2.4.1.\nFinally, it is worth noting that ensemble meth-\nods rely on the construction of a diverse col-\nlection of classiﬁers and aggregation of their\npredictions (see e.g., Kuncheva & Whitaker\n2003). Ensemble methods in the form of ”bag-\nging” tend to decrease the classiﬁcation vari-\nance, while methods in the form of ”boosting”\ntend to decrease the classiﬁcation bias.\nRan-\ndom Forest is a ”bagging” ensemble method,\nwhere the ensemble consists of a diverse col-\nlection of individual trees that are trained on\ndiﬀerent subsets of the data.\nIn ”boosting”,\nthe decision trees are built sequentially, such\nthat each tree is presented with training sam-\nples that the previous tree failed to classify. One\nof the most popular ”boosting” methods in As-\ntronomy is Adaboost (Freund & Schapire 1997).\nThe three algorithms described in this section\nare available in the scikit-learn library6.\n2.4.1. Probabilistic Random Forest\nWhile shown to be very useful for various\ntasks in Astronomy, many Machine Learning\nalgorithms were not designed for astronomical\ndatasets, which are noisy and have gaps. In par-\nticular, measured features typically have a wide\nrange of uncertainty values, and these uncer-\ntainties are often not taken into account when\ntraining the model. Indeed, the performance of\n6\nhttps://scikit-learn.org/stable/modules/\ngenerated/sklearn.tree.DecisionTreeClassifier.\nhtml\nhttps://scikit-learn.org/stable/\nmodules/generated/sklearn.ensemble.\nRandomForestClassifier.html\nhttps://scikit-learn.org/stable/modules/\ngenerated/sklearn.ensemble.AdaBoostClassifier.\nhtml\nMachine Learning algorithms depends strongly\non the signal-to-noise ratio of the objects in the\nsample, and a model optimized on a dataset\nwith particular noise characteristics will fail on\na similar dataset with diﬀerent noise proper-\nties.\nFurthermore, while in computer vision\nthe labels provided to the algorithm are consid-\nered to be ”ground truth” (e.g., classiﬁcation of\ncats and dogs in images), in Astronomy the la-\nbels might suﬀer from some level of ambiguity.\nFor example, in a classiﬁcation task of ”real”\nversus ”bogus” in transient detection on diﬀer-\nence images (see e.g., Bloom et al. 2012), the\nlabels in the training set are obtained from a\nmanual classiﬁcation of scientists and citizen-\nscientists.\nWhile some might classify a given\nevent as ”real”, others may classify it as ”bo-\ngus”.\nIn addition, labels in the training set\ncould be the output of a diﬀerent algorithm,\nwhich provides a label with an associated uncer-\ntainty. Such uncertainties are also not treated\nby most Machine Learning algorithms.\nRecently, Reis et al. (2019) modiﬁed the tra-\nditional Random Forest algorithm to take into\naccount uncertainties in the measurements (i.e.,\nfeatures) as well as in the assigned class. The\nProbabilistic Random Forest algorithm treats\nthe features and the labels as random variables\nrather than deterministic quantities, where each\nrandom variable is represented by a probabil-\nity distribution function, whose mean is the\nprovided measurement and its variance is the\nprovided uncertainty. Their tests showed that\nthe Probabilistic Random Forest outperforms\nthe traditional Random Forest when applied to\ndatasets with various noise properties, with an\nimprovement of up to 10% in classiﬁcation ac-\ncuracy with noisy features, and up to 30% with\nnoisy labels. In addition to the dramatic im-\nprovement in classiﬁcation accuracy, the Prob-\nabilistic Random Forest naturally copes with\nmissing values in the data, and outperforms\nRandom Forest when applied to a dataset with\n12\ndiﬀerent noise characteristics in the training and\ntest sets. The Probabilistic Random Forest was\nimplemented in python and is publicly avail-\nable on github7.\n2.5. Artiﬁcial Neural Networks\nArtiﬁcial neural networks are a set of algo-\nrithms with structures that are vaguely inspired\nby the biological neural networks that consti-\ntute the human brain. Their ﬂexible structure\nand non-linearity allows one to perform a wide\nvariety of tasks, including classiﬁcation and re-\ngression, clustering, and dimensionality reduc-\ntion, making them extremely popular in Astron-\nomy (e.g., Storrie-Lombardi et al. 1992; Weaver\n& Torres-Dodgen 1995; Singh et al. 1998; Snider\net al. 2001; Firth et al. 2003; Tagliaferri et al.\n2003; Vanzella et al. 2004; Blake et al. 2007;\nBanerji et al. 2010; Eatough et al. 2010; Brescia\net al. 2013, 2014; Ellison et al. 2016; Teimoorinia\net al. 2016; Mahabal et al. 2017; Bilicki et al.\n2018; Huertas-Company et al. 2018; Naul et al.\n2018; Parks et al. 2018; Das & Sanders 2019). In\nthis section I describe the main building blocks\nof shallow artiﬁcial neural networks, and their\nuse for classiﬁcation and regression tasks. The\nsection does not include details on Deep Learn-\ning, in particular Convolutional Neural Net-\nworks, Recurrent Neural Networks, and Gener-\native Adversarial Networks (see lectures by M.\nHuertas-Company for details on deep learning).\nFigure 6 is an illustration of a shallow neu-\nral network architecture. The network consists\nof an input layer, output layer, and several hid-\nden layers, where each of these contain neurons\nthat transmit information to the neurons in the\nsucceeding layer. The input data is transmit-\nted from the input layer, through the hidden\nlayers, and reaches the output layer, where the\ntarget variable is predicted. The value of every\nneuron in the network (apart from the neurons\n7 https://github.com/ireis/PRF\nFigure 6.\nIllustration of a shallow neural net-\nwork architecture. The network consists of an input\nlayer, two hidden layers, and an output layer. The\ninput dataset is propagated from the input layer,\nthrough the hidden layers, to the output layer,\nwhere a prediction of a target variable is made.\nEach neuron is a linear combination of the neuron\nvalues in the previous layer, followed by an appli-\ncation of a non-linear activation function (see text\nfor more details).\nin the input layer) is a linear combination of\nthe neurons in the previous layer, followed by\nan application of a non-linear activation func-\ntion. That is, the values of the neurons in the\nﬁrst hidden layer are given by ⃗x1 = f1(W1⃗x0),\nwhere ⃗x0 is a vector that describes the values of\nthe neurons in the input layer (the input data),\nW1 is a weight matrix that describes the lin-\near combination of the input values, and f1 is\na non-linear activation function.\nIn a similar\nmanner, the values of the neurons in the sec-\nond hidden layer are given by ⃗x2 = f2(W2⃗x1),\nwith a similar notation. Finally, the values of\nthe neurons in the output layer are given by\n⃗x3 = f3(W3⃗x2) = f3(W3f2(W2f1(W1⃗x0))). The\nweights of the network are model parameters\nwhich are optimized during training via back-\npropagation (for additional details see lectures\nby M. Huertas-Company). The non-linear ac-\nMachine Learning in Astronomy\n13\ntivation functions are model hyper-parameters,\nwith common choices being sigmoid, rectiﬁed\nlinear unit function (RELU), hyperbolic tan\nfunction (TANH), and softmax. The number of\nhidden layers and the number of neurons in each\nof these layers are additional hyper-parameters\nof the model. The number of neurons in the in-\nput and the output layers are deﬁned according\nto the classiﬁcation or regression task at hand.\nShallow neural networks are available in scikit-\nlearn8.\nOwing to their ﬂexible structure and non-\nlinearity, artiﬁcial neural networks are power-\nful algorithms, capable of describing extremely\ncomplex relations between the input data and\nthe target variable. As just noted, these net-\nworks have many hyper-parameters, and they\nusually require a large amount of data to train\non. Furthermore, due to their complexity, they\ntend to overﬁt the dataset, and various tech-\nniques, such as dropout, are applied to over-\ncome this issue. In addition, these networks are\nharder to interpret, compared to SVM or Ran-\ndom Forest. However, studies have shown that\ndeep neural networks can greatly outperform\ntraditional algorithms such as SVM, Random\nForest, and shallow neural networks, given raw\nand complex data, such as images, spectra, and\nlight-curves (see e.g., Huertas-Company et al.\n2018; Naul et al. 2018; Parks et al. 2018 and\nreferences within).\n3. UNSUPERVISED LEARNING\nUnsupervised Learning is a general term that\nincorporates a large set of statistical tools, used\nto perform data exploration, such as clustering\nanalysis, dimensionality reduction, visualiza-\ntion, and outlier detection. Such tools are par-\nticularly important in scientiﬁc research, since\nthey can be used to make new discoveries or\nextract new knowledge from the dataset. For\n8\nhttps://scikit-learn.org/stable/modules/\nneural_networks_supervised.html\nexample, a cluster analysis that reveals two dis-\ntinct clusters of planets might suggest that the\ntwo populations are formed through diﬀerent\nformation channels, or, a successful embedding\nof a complex high-dimensional dataset onto two\ndimensions might suggest that the observed\ncomplexity can be attributed to a small number\nof physical parameters (e.g., the large variety\nof stellar spectra can be attributed to a single\nsequence in temperature, stellar mass, and lu-\nminosity; e.g., Hertzsprung 1909 and Russell\n1914). While visual inspection of the dataset\ncan achieve these goals, it is usually limited to\n3–12 dimensions (see lecture 2 by S. G. Djor-\ngovski). Visual inspection becomes impractical\nwith modern astronomical surveys, which pro-\nvide hundreds to thousands of features per ob-\nject. It is therefore necessary to use statistical\ntools for this task.\nUnsupervised Learning algorithms take as an\ninput only the measured features, without la-\nbels, and as such, they cannot be trained with\nsome ”ground truth”. Their output is typically\na non-linear and non-invertible transformation\nof the input dataset, consisting of an associ-\nation of diﬀerent objects to diﬀerent clusters,\nlow-dimensional representation of the objects in\nthe sample, or a list of peculiar objects. Such\nalgorithms consist of internal choices and a cost\nfunction, which does not necessarily coincide\nwith our scientiﬁc motivation.\nFor example,\nalthough K-means algorithm is often used to\nperform clustering analysis (see section 3.2.1),\nit is not optimized to detect clusters, and it\nwill reach a correct global optimum even if the\ndataset is not composed of clusters.\nIn addi-\ntion, these algorithms often have several exter-\nnal free parameters, which cannot be optimized\nthrough the algorithm’s cost function. Diﬀer-\nent choices of external parameters may result\nin signiﬁcantly diﬀerent outputs, resulting in\ndiﬀerent interpretations of the same dataset.\nThe interpretation of the output of an unsu-\n14\npervised learning algorithm must be carried out\nwith extreme caution, taking into account its\ninternal choices and cost function, and the out-\nput’s sensitivity to a change of external param-\neters. However, since unsupervised learning is\ntypically used for exploration, not necessarily\nto reach a precisely determined goal, the lack\nof objective to optimize is often not critical. It\ndoes make it hard to know when one has really\nexhausted exploring all possibilities.\n3.1. Distance Assignment\nThe ﬁrst step in the large majority of unsuper-\nvised learning algorithms is distance assignment\nbetween the objects in the sample. In most of\nthe cases it is assumed that the measured fea-\ntures occupy a euclidean space, and the distance\nbetween the objects in the sample is measured\nwith euclidean metric. This choice might not be\nappropriate for some astronomical datasets, and\nusing a metric that is more appropriate could\nimprove the algorithm’s performance. For ex-\nample, when the input dataset consists of fea-\ntures that are extracted from astronomical ob-\nservations, these features typically do not have\nthe same physical units (e.g., stellar mass, tem-\nperature, size and morphology of a galaxy, bolo-\nmetric luminosity, etc). In such cases, euclidean\ndistance assignment will be dominated by fea-\ntures that are distributed over the largest dy-\nnamical range (e.g., black hole mass ∼108 M⊙),\nand will not be sensitive to the other features\nin the input dataset (e.g., stellar velocity disper-\nsion ∼200 km/sec). Therefore, when applying\nunsupervised machine learning algorithms to a\nset of derived features, it is advised to rescale\nthe features (see also section 2.2).\nWhen applied to astronomical observations,\nsuch as spectra, light-curves, or images, the eu-\nclidean distance might not trace the distances a\nscientist would assign. This is partly since the\neuclidean metric implicitly assumes that all the\nfeatures are equally important, which is not nec-\nessarily the case. Figure 7 shows two examples\nof such scenarios. The left panel shows spectra\nof three galaxies, marked by A, B, and C, and the\nright panel shows three synthetic light-curves.\nIn the galaxy case, galaxy A appears diﬀerent,\nsince it shows a strong emission line. Thus, we\nwould expect that the distance between galaxy\nA and B will be larger than the distance between\ngalaxy B and C. However, most of the pixels in a\ngalaxy spectrum are continuum pixels, and the\ncontinuum of galaxy B is slightly bluer than the\ncontinuum of galaxy C. Since the spectra are\ndominated by continuum pixels, the euclidean\ndistance between galaxy B and C will be larger\nthan the distance to galaxy A. That is, using\na euclidean metric, galaxy B is diﬀerent. Simi-\nlarly in the light-curve case, since the euclidean\nmetric is sensitive to horizontal shifts, the dis-\ntance between light-curve B and C will be larger\nthan the distance between light-curve A and C.\nIn many scientiﬁc applications, we want to de-\nﬁne a metric in which A stands out. Such met-\nrics can be based on domain knowledge, where\ne.g., the metric is invariant to shifts, rotations,\nand ﬂips, or they can be based on some measure\nof feature importance, e.g., emission line pixels\nbeing more important than continuum pixels.\nThere are several distance measures which can\nbe more appropriate for astronomical datasets\nand may result in improved performance, such\nas cross correlation-based distance (see e.g.,\nProtopapas et al. 2006; Nun et al. 2016; Reis\net al. 2018b), Distance Correlation, Dynamic\nTime Warp, Canberra Distance, and distance\nmeasures that are based on supervised learning\nalgorithms (see Reis et al. 2018b for details).\nSection 3.1.1 describes a general distance mea-\nsure that is based on the Random Forest al-\ngorithm, and was shown to work particularly\nwell on astronomical spectra. Since the Ran-\ndom Forest ranks features according to their im-\nportance, the Random Forest-based distance is\nheavily inﬂuenced by important features, and is\nless aﬀected by irrelevant features.\nMachine Learning in Astronomy\n15\n4000\n5000\n6000\n7000\n8000\nwavelength (Å)\n0\n1\n2\n3\n4\nnormalised flux\nA\nB\nC\n0\n2\n4\n6\n8\n10\n12\ntime (days)\n0\n2\n4\n6\nnormalised flux\nA\nB\nC\nFigure 7. Illustration of the disadvantages in using the Euclidean metric to measure distances between\nastronomical observations. The left panel shows three galaxy spectra, marked by A, B, C, where galaxy\nA appears visually diﬀerent. However, the Euclidean distance between galaxy B and C is larger than the\ndistance between galaxy A and B, since most of the pixels in a galaxy spectrum are continuum pixels, and\ngalaxy B is slightly bluer than galaxy C. The right panel shows three synthetic light-curves.\nSince the\nEuclidean metric is not invariant to horizontal shifts, the distance between light-curve B and C is larger than\nthe distance to light-curve A, although the latter appears visually diﬀerent.\n3.1.1. General Similarity Measure with Random\nForest\nSo far, the discussion on Random Forest (sec-\ntion 2.4) was focused on a supervised setting,\nwhere the model is trained to perform classiﬁ-\ncation and regression. However, Random For-\nest can be used in an unsupervised setting, to\nestimate the distance between every pair of ob-\njects in the sample. Shi & Horvath (2006) pre-\nsented such a method, and Baron & Poznanski\n(2017), followed by Reis et al. (2018a) and Reis\net al. (2018b), applied the method to astronomi-\ncal datasets, with a few modiﬁcations that made\nthe algorithm more suitable for such datasets.\nIn an unsupervised setting, the dataset con-\nsists only of measured features, without labels,\nand is represented by an N × M matrix, where\nevery row represents an object (N objects in the\nsample), and every column represents a feature\n(M measured features per object). To translate\nthe problem into a supervised learning prob-\nlem that can be addressed with Random For-\nest, a synthetic dataset is built. The synthetic\ndataset is represented by a N × M matrix, sim-\nilarly to the original dataset, where each fea-\nture (column) in the synthetic data is built by\nsampling from the marginal distribution of the\nsame feature in the original dataset. One can,\ninstead, shuﬄe the values in every column of\nthe original data matrix, resulting in a similar\nsynthetic data matrix. The process of creating\nthe synthetic data is illustrated in ﬁgure 8 with\na simpliﬁed example taken from Baron & Poz-\nnanski (2017), where the objects in the dataset\nhave only two features. The left panel shows\nthe original dataset, with the marginal distri-\nbutions in each of the features plotted on the\ntop and on the right.\nThe right panel shows\nthe synthetic dataset, where the features show\nthe same marginal distribution as in the origi-\nnal dataset, but stripped of the covariance seen\nin the original dataset.\nOnce the synthetic dataset is constructed, the\noriginal dataset is labeled as class 1 and the syn-\nthetic dataset is labeled as class 2, and Random\nForest is trained to distinguish between the two\nclasses. During this training phase, the Random\nForest is trained to detect covariance, since it is\npresent only in the original dataset and not in\nthe synthetic one. As a result, the most impor-\n16\n50\n100\nFeature 1\n20\n40\n60\n80\n100\n120\nFeature 2\nOriginal Data\n25\n50\n75\n100\nFeature 1\n20\n40\n60\n80\n100\nFeature 2\nSynthetic Data\nFigure 8. Illustration of synthetic data construction, taken from Baron & Poznanski (2017), in a simpliﬁed\nexample of a dataset with only two features. The left panel shows the distribution of the features of the\noriginal dataset, and their marginal distributions. The synthetic dataset (right panel) is constructed by\nsampling from the marginal distribution of each feature in the original dataset. The resulting dataset shows\nthe same marginal distribution in its features, but is stripped of the covariance that was present in the\noriginal dataset.\ntant features in the decision trees will be fea-\ntures that show correlations with others. Hav-\ning the trained forest, the distance between dif-\nferent objects (in the original dataset) is deﬁned\nas follows. Every pair of objects is propagated\nthrough all the decision trees in the forest, and\ntheir similarity is deﬁned as the number of times\nthey were both classiﬁed as class 1, and reached\nthe same terminal leaf. This similarity S can\nrange between 0 to the number of trees in our\nforest. S is a measure of the similarity between\nthese two objects since objects that have a sim-\nilar path inside the decision tree have similar\nfeatures, and as a consequence are represented\nby the same model (for more details see Baron\n& Poznanski 2017).\nBaron & Poznanski (2017), Reis et al. (2018a),\nand Reis et al. (2018b) showed that this deﬁni-\ntion of similarity between objects traces valu-\nable information about their diﬀerent physical\nproperties. Speciﬁcally, they showed that such\nmetric works particularly well for spectra, and\ntraces information coming from diﬀerent emis-\nsion and absorption lines, and their connec-\ntion to the continuum emission.\nReis et al.\n(2018a) applied this method to infrared spec-\ntra of stars, and showed that this metric traces\nphysical properties of the stars, such as metal-\nlicity, temperature, and surface gravity.\nThe\nalgorithm was implemented in python and is\npublicly available on github9.\n3.2. Clustering Algorithms\nClustering analysis, or clustering, is the task\nof grouping objects in the sample, such that ob-\njects in the same group, which is called a cluster,\nare more similar to each other than to objects in\nother groups. The deﬁnition of a cluster changes\n9 https://github.com/dalya/WeirdestGalaxies\nMachine Learning in Astronomy\n17\nfrom one algorithm to the next, and this section\ndescribes centroid-based clustering (K-means;\nsection 3.2.1) and connectivity-based clustering\n(Hierarchical clustering; section 3.2.2).\nAlso\npopular is distribution-based clustering, with\nthe prominent method being Gaussian Mixture\nModels, which I do not discuss in this docu-\nment (see e.g., de Souza et al. 2017 and refer-\nences within). Throughout this section, I give\nexamples with a simpliﬁed dataset that consists\nof two features. Obviously, a two-dimensional\ndataset can be visualized and clusters can be\ndetected manually, however, these algorithms\ncan be used to search for clusters in complex\nhigh-dimensional datasets.\n3.2.1. K-means\nOne of the most widely used clustering meth-\nods is K-means, which is a centroid-based clus-\ntering algorithm (e.g., MacQueen 1967).\nK-\nmeans is simple and robust, even when perform-\ning clustering analysis in a high-dimensional\nspace. It was used in Astronomy in various con-\ntexts, to study stellar and galaxy spectra, X-\nray spectra, solar polarization spectra, spectra\nfrom asteroids, and more (see e.g., Balazs et al.\n1996; Hojnacki et al. 2007; Galluccio et al. 2008;\nS´anchez Almeida et al. 2010; Simpson et al.\n2012; S´anchez Almeida & Allende Prieto 2013;\nGarcia-Dias et al. 2018 and references therein).\nThe ﬁrst step of K-means is distance assign-\nment between the objects in the sample. The\ndefault distance is the euclidean metric, but\nother metrics, which are more appropriate for\nthe particular dataset at hand, can be used.\nThen, the algorithm selects k random objects\nfrom the dataset which serve as the initial cen-\ntroids, where k is an external free parameter.\nEach object in the dataset is then assigned to\nits closest of the k centroids. Then, new cluster\ncentroids are computed by taking the average\nposition of the objects that are associated with\nthe given cluster. These two steps, re-assigning\nobjects to a cluster according to their distance\nfrom the centroid and recomputing the cluster\ncentroids, are repeated iteratively until reach-\ning convergence.\nConvergence can be deﬁned\nin several manners, for example, when the large\nmajority of the objects are no longer reassigned\nto diﬀerent centroids (90% and more), or when\nthe cluster centroids converge to a set location.\nThe output of the algorithm consists of the clus-\nter centroids, and an association of the diﬀer-\nent objects to the diﬀerent clusters. K-means is\navailable in the scikit-learn library10.\nPanel (A) in Figure 9 shows an application of\nK-means to a two-dimensional dataset, setting\nk = 2 and using euclidean distances, where the\ndots represent the objects in the sample, trian-\ngles represent the cluster centroids, and diﬀer-\nent colors represent diﬀerent clusters. As noted\nin the previous paragraph, K-means starts by\nrandomly selecting k objects as the initial clus-\nter centroids. In some cases, diﬀerent random\nassignments of initial centroids might result in\ndiﬀerent outputs, particularly when K-means\nconverges to a local minimum. To avoid it, one\ncan run K-means several times, each time with\ndiﬀerent randomly-selected centroids, and select\nthe output that results in the minimum sum of\nsquared distances between the objects and their\ncentroids. Panel (B) in Figure 9 shows an appli-\ncation of K-means to a similar dataset, but with\nk = 3. k is an example of an external parame-\nter that cannot be optimized with the cost func-\ntion, since the cost function decreases monoton-\nically with k. Obviously, setting k to be equal\nto the number of objects in the sample will re-\nsult in the minimum possible cost of zero, since\neach object will be deﬁned as its own cluster.\nFinding the best k is not trivial in most of the\ncases, and studies either use the elbow method,\nor deﬁne probability-based scores to constrain\nit (see e.g., S´anchez Almeida et al. 2010). In\n10\nhttps://scikit-learn.org/stable/modules/\ngenerated/sklearn.cluster.KMeans.html\n18\n0\n1\n2\nfeature 1\n0.0\n0.5\n1.0\n1.5\nfeature 2\n(A)\n0\n1\n2\nfeature 1\n0.0\n0.5\n1.0\n1.5\nfeature 2\n(B)\n0\n1\n2\nfeature 1\n0.0\n0.5\n1.0\n1.5\n2.0\nfeature 2\n(C)\n0\n1\n2\nfeature 1\n100\n101\nfeature 2\n(D)\nFigure 9. Four examples of K-means application to diﬀerent two-dimensional datasets, where the objects\nin the sample are marked with circles and the clusters centroids are marked with triangles. The diﬀerent\ncolors represent diﬀerent clusters. Panel (A) shows K-means application with k = 2 and a euclidean distance.\nPanel (B) shows K-means application to a similar dataset, but with k = 3. Panel (C) shows K-means output\nfor a dataset without clear clusters, and panel (D) shows the result for a dataset with features which are\ndistributed over diﬀerent dynamical scales.\nsome cases, the distribution of the distances of\nall the objects in the sample contains several\ndistinguishable peaks, and can guide the selec-\ntion of the correct k.\nPanel (C) in Figure 9 shows an application\nof K-means to a dataset with no clear clus-\nters. This is an example in which the output\nconsists of clusters while the dataset does not\nshow clear clusters. To test for such cases, one\ncan compare the distribution of distances be-\ntween objects within the same cluster to the\ntypical distance between cluster centroids. In\nthis particular example, these are roughly sim-\nilar, suggesting that there are no clear clusters\nin the dataset.\nPanel (D) shows an applica-\ntion of K-means to a dataset with features that\nare distributed over diﬀerent dynamical scales,\nand one can see that K-means failed in ﬁnding\nthe correct clusters. The K-means cost function\nis based on the summed distances between the\nobjects and their centroids, and since the dis-\ntances between the objects are much larger in\nfeature 2 (y-axis), the optimal output is com-\npletely dominated by its values. To avoid such\nissues, it is recommended to either normalize\nthe features, or rank-order them before apply-\ning K-means. K-means will also fail when the\ndataset contains outliers, since they can have a\nsigniﬁcant impact on the centroids placements,\nand therefore on the resulting clusters. Thus,\noutliers should be removed before applying K-\nmeans to the dataset.\n3.2.2. Hierarchical Clustering\nHierarchical clustering is another popular al-\ngorithm in cluster analysis, aiming at build-\ning a hierarchy of clusters (e.g., Ward 1963).\nThe two main types of Hierarchical clustering\nare Agglomerative Hierarchical clustering, also\nnamed the ”bottom-up” approach, where each\nobject starts as an individual cluster and clus-\nters are merged iteratively, and Divisive Hierar-\nchical clustering, or ”top-down”, where all the\nobjects start in one cluster, then split recur-\nsively into smaller clusters. Hierarchical clus-\ntering has been applied to various astronomical\ndatasets, such as X-ray spectra, extracted fea-\ntures from galaxy images, and absorption spec-\ntra of interstellar gas (see e.g., Hojnacki et al.\n2007; Baron et al. 2015; Hocking et al. 2015;\nPeth et al. 2016; Ma et al. 2018a). The discus-\nsion in this section is focused on Agglomerative\nHierarchical clustering.\nThe ﬁrst step of Hierarchical clustering is also\nassigning distances between the objects in the\nsample, using the euclidean metric by default.\nAll the objects in the sample start as one-sized\nclusters. Then, the algorithm merges the two\nMachine Learning in Astronomy\n19\nclosest clusters into a single cluster. The pro-\ncess is repeated iteratively, merging the two\nclosest clusters into a single cluster, until the\ndataset consists of a single cluster, which in-\ncludes many smaller clusters.\nTo do so, one\nmust deﬁne a distance between clusters that\ncontain more than one object, also referred to\nas ”linkage method”. There are several diﬀer-\nent linkage methods, which include ”complete\nlinkage”, where the distance between the clus-\nters is deﬁned as the maximal distance between\nthe objects in the two clusters, ”single linkage”,\nwhere the distance between the clusters is de-\nﬁned as the minimal distance between the ob-\njects in the clusters, ”average linkage”, where\nthe distance is deﬁned as the average distance\nbetween the objects in the clusters, and ”ward\nlinkage”, which minimizes the variance of the\nclusters merged. The linkage method is an ex-\nternal free parameter of the algorithm, and it\nhas a signiﬁcant inﬂuence on the output.\nThe result of Hierarchical clustering is usu-\nally visualized with a dendrogram. Figure 10\nshows an application of Hierarchical clustering\nto a dataset with 11 objects, marked by A, B, ..,\nK in the diagram. The dendrogram represents\nthe history of the hierarchal merging process,\nwith the vertical axis showing the distance at\nwhich two clusters were merged. Clusters A and\nB were merged ﬁrst, since their merging distance\nis the shortest (up to this point the clusters con-\ntain a single object).\nThen, clusters C and D\nwere merged. Following that, clusters (A, B)\nand (C, D) were merged, since they were the\nclusters with the shortest distance. The next\ntwo merging clusters are J and K, and the pro-\ncess continues until clusters (A, B, C, D, E,\nF) and (G, H, I, J, K) are merged into a sin-\ngle cluster.\nThe dendrogram can be used to\nstudy the structure of the dataset, in particu-\nlar, it can be used to infer the number of clus-\nters in the dataset. The example in Figure 10\nsuggests that the dataset consists of two clus-\nFigure 10. Visualization of a dendrogram for a\ndataset with 11 objects, marked by A, B, .., K.\nThe dendrogram represents the hierarchical merg-\ning history, where the y-axis represents the distance\nat which two clusters were merged. The clusters\nthat merged with a shorter distance were merged\nearlier in the process, and in this case, the merger\nhistory is A and B, C and D, (A,B) and (C,D), J\nand K, and so on. The dashed horizontal line repre-\nsents the threshold t that is used to deﬁne the ﬁnal\nclusters in the dataset (see text for more details).\nters, (A, B, C, D, E, F) and (G, H, I, J,\nK), since the merging distance of inter-cluster\nobjects (d1 in the ﬁgure) is much shorter than\nthe merging distance of the two ﬁnal clusters (d2\nin the ﬁgure). The dashed horizontal line repre-\nsents the threshold t used in Hierarchical clus-\ntering for the ﬁnal cluster deﬁnition.\nGroups\nthat are formed beneath the threshold t are de-\nﬁned as the ﬁnal clusters in the dataset, and are\nthe output of the algorithm. t is an external free\nparameter of the algorithm, and cannot be op-\ntimized using the cost function. It has a signiﬁ-\ncant eﬀect on the resulting clusters, in particu-\nlar, as t decreases, the number of resulting clus-\nters increases. In some cases, the dendrogram\n20\n0\n2\nfeature 1\n0.5\n0.0\n0.5\n1.0\nfeature 2\nward linkage\n(73)\n(80)\n(92)\n(114)\n(83)\n(117)\n(90)\n(51)\n(67)\n(106)\n(55)\n(72)\n0\n10\n20\n30\ndistance\nward linkage\n0\n2\nfeature 1\n0.5\n0.0\n0.5\n1.0\nfeature 2\nsingle linkage\n(2)\n598\n310\n278\n(495)\n861\n(2)\n92\n304\n(2)\n(3)\n(490)\n0.0\n0.1\n0.2\n0.3\ndistance\nsingle linkage\n0\n2\nfeature 1\n0.5\n0.0\n0.5\n1.0\nfeature 2\ncomplete linkage\n(48)\n(83)\n(69)\n(141)\n(93)\n(122)\n(63)\n(91)\n(52)\n(79)\n(66)\n(93)\n0\n1\n2\n3\ndistance\ncomplete linkage\n0\n2\nfeature 1\n0.5\n0.0\n0.5\n1.0\nfeature 2\naverage linkage\n(99)\n(54)\n(84)\n(90)\n(67)\n(79)\n(97)\n(78)\n(89)\n(106)\n(66)\n(91)\n0.0\n0.5\n1.0\n1.5\ndistance\naverage linkage\nFigure 11.\nApplication of Hierarchical clustering to a two-dimensional dataset, using diﬀerent linkage\nmethods, and setting t to be 0.7 of the maximal merging distance. The top panels show the distribution of\nthe objects in the dataset, colored by the ﬁnal cluster association. The bottom panels show the resulting\ndendrograms of each linkage method, and the dashed horizontal line represents the threshold t used to deﬁne\nthe ﬁnal clusters. The dendrograms are truncated for representational purposes, and the number objects in\neach truncated branch is indicated on the x-axis in parentheses.\ncan be used to estimate a ”natural” threshold\nvalue. Agglomerative Hierarchical clustering is\navailable in scikit-learn11. Visualization of\nthe resulting dendrogram can be done with the\nscipy library, and is presented in the hands-on\ntutorials12.\nFigure 11 shows an example of Hierarchi-\ncal clustering application to a two-dimensional\ndataset, using diﬀerent linkage methods, setting\nt to be 0.7 of the maximal merging distance.\nThe top panels show the distribution of the ob-\njects in the dataset, colored by the ﬁnal clusters\ndetected by the algorithm, and the bottom pan-\n11\nhttps://scikit-learn.org/stable/\nmodules/generated/sklearn.cluster.\nAgglomerativeClustering.html\n12\nhttps://github.com/dalya/IAC_Winter_\nSchool_2018; see also:\nhttps://joernhees.de/blog/2015/08/26/\nscipy-hierarchical-clustering-and-dendrogram-tutorial/\nels show the dendrograms for each of the linkage\nmethods. Looking at the ﬁrst row, one can see\nthat diﬀerent linkage methods result in diﬀer-\nent cluster deﬁnitions, in particular, both the\nnumber of detected clusters and the association\nof objects to clusters change for diﬀerent link-\nage methods. In this particular example, it is\nclear that the single linkage returns the correct\noutput, however, for high-dimensional datasets\nsuch a visualization is not possible. To select\nthe ”correct” linkage method for the particular\ndataset at hand, it is advised to examine the re-\nsulting dendrograms. In this speciﬁc case, the\nsecond row shows that the single linkage-based\ndendrogram reveals the most signiﬁcant clus-\ntering, with two detected clusters.\nA general\nrule of thumb to select of the ”correct” linkage\nmethod is by selecting the linkage that results\nin the largest diﬀerence between the merging\nMachine Learning in Astronomy\n21\nFigure 12. Application of Hierarchical clustering to reorder and visualize a complex correlation matrix.\nThe left panel shows a correlation matrix calculated for a dataset with 400 objects, where the colorbar\nrepresents the Pearson correlation coeﬃcient between every pair of objects.\nThe rows and columns are\nordered according to the object index, and little information can be extracted from such a visualization.\nThe right panel shows the same correlation matrix, reordered according to the appearance order of the\nobjects in the dendrogram. The latter representation reveals interesting structures, and can be used to\ninterpret the dataset.\ndistance of the resulting clusters and the ﬁnal,\nmaximal, merging distance.\nAs shown in ﬁgure 11, Hierarchical clustering\ncan be used to detect clusters which other clus-\ntering algorithms, such as K-means and Gaus-\nsian Mixture Models, cannot detect. Since it is\nbased on connectivity, it can be used to detect\nclusters that are distributed over a non-trivial\nmanifold (this is usually possible only with the\nsingle linkage method).\nFurthermore, Hierar-\nchal clustering is less sensitive to outliers in the\ndataset, since these will be merged last, and\nthus will not eﬀect the structure of the den-\ndrogram and the resulting clusters that merged\nbefore that. Perhaps the most interesting ap-\nplication of Hierarchical clustering is reordering\nand visualizing complex distance or correlation\nmatrices. The left panel of Figure 12 shows as\nexample of a correlation matrix, calculated for a\ncomplex dataset with 400 objects. The rows and\ncolumns of the correlation matrix are ordered\naccording to the object index, and clearly, this\nrepresentation conveys little information about\nthe structure of the dataset. Instead, one can\nperform Hierarchical clustering and extract a\ndendrogram for this particular dataset. Then,\none can rearrange the objects in the correlation\nmatrix according to their order of appearance\nin the dendrogram. The reordered correlation\nmatrix is shown in the right panel of Figure 12,\nwhere one can ﬁnd at least two clusters of ob-\njects, such that objects that belong to a given\ncluster show strong correlations, and objects\nthat belong to diﬀerent clusters show a strong\nnegative correlation. Therefore, the process de-\nscribed above may reveal rich structures in the\ndataset, which may allow one to explore and\nextract information from it, even without per-\nforming cluster analysis (see also de Souza &\nCiardi 2015).\n3.3. Dimensionality Reduction Algorithms\nDimensionality reduction refers to the process\nof reducing the number of features in the orig-\n22\ninal dataset, either by selecting a subset of the\nfeatures that best describe the dataset, or by\nconstructing a new set of features that pro-\nvide a good description of the dataset. Some\nof the dimensionality reduction algorithms pro-\nvide principle components or prototypes, which\nare a small set of objects that have the same di-\nmensions as the objects in the original dataset,\nand are used to represent all the objects in\nthe sample.\nOther algorithms aim at embed-\nding the high-dimensional dataset onto a low-\ndimensional space, without using principle com-\nponents or prototypes. When applying dimen-\nsionality reduction algorithms to data, we al-\nways lose some information.\nOur goal is to\nchoose an algorithm that retains most of the rel-\nevant information, where relevant information\nstrongly depends on our scientiﬁc motivation.\nDimensionality reduction is useful for a vari-\nety of tasks. In a supervised learning setting,\nsince many algorithms are not capable of man-\naging thousands of features, dimensionality re-\nduction is used to decrease the number of fea-\ntures under consideration, by removing redun-\ndancy in the original set of features. Although\nnot very popular in Astronomy, dimensional-\nity reduction is also used for compression, and\nwill become more relevant for surveys such as\nthe SKA (Dewdney et al. 2009), where keep-\ning all the data is no longer possible. Perhaps\nmost importantly, dimensionality reduction can\nbe used to visualize and interpret complex high-\ndimensional datasets, with the goal of uncover-\ning hidden trends and patterns.\n3.3.1. Principle Component Analysis\nPrinciple Component Analysis (PCA) is a lin-\near feature projection, which transforms data in\nhigh-dimensional space to a lower-dimensional\nspace, such that the variance of the dataset\nin the low-dimensional representation is max-\nimized. In practice, PCA constructs a covari-\nance matrix of the dataset and computes its\neigenvectors. The eigenvectors that correspond\n0.0\n0.2\n0.4\n0.6\nfeature 1\n0.6\n0.7\n0.8\n0.9\n1.0\n1.1\n1.2\n1.3\nfeature 2\nprinciple\ncomponent 1\nprinciple\ncomponent 2\nFigure\n13.\nApplication\nof\nPCA\nto\ntwo-\ndimensional dataset.\nThe two principle compo-\nnents are marked with black arrows, where prin-\nciple component 1 accounts for most of the vari-\nance in the data, and principle component 2, which\nis orthogonal to principle component 1, accounts\nfor the rest of the variance in the data. Every ob-\nject in the sample can be accurately represented as\na linear combination of the two principle compo-\nnents, and can be represented approximately using\nthe ﬁrst principle component.\nto the largest eigenvalues are used to recon-\nstruct a large fraction of the variance in the\noriginal dataset. These eigenvectors, also called\nprinciple components, are arranged such that\nthe ﬁrst principle component has the largest\npossible variance, and each succeeding compo-\nnent has the highest possible variance, under\nthe constraint that it is orthogonal to the pre-\nceding components.\nThe number of principal\ncomponents is at most the number of features\nin the dataset. Every object in the sample can\nbe represented by a linear combination of the\nprinciple components, where the representation\nis accurate only when all the principle compo-\nnents are used. When a subset of the princi-\nple components is used, the representation is\napproximate, resulting in a dimensionality re-\nduction. The coeﬃcients of the linear combi-\nMachine Learning in Astronomy\n23\nFigure 14. Application of PCA, ICA, and NMF to a sample of SDSS spectra, taken from Vanderplas et al.\n(2012). Additional details on ICA and NMF can be found in Ivezi´c et al. (2014). The columns represent\ndiﬀerent algorithms, and the rows represent the resulting components using each of the algorithms, sorted\nby their importance. The grey horizontal line represents zero ﬂux. While the ﬁrst two PCA components\nresemble galaxy spectra (with an old stellar population), the next three components do not represent a\nphysical component in galaxy spectra, in particular, they show negative ﬂux values. On the other hand,\nthe NMF components resemble more physical components, with the ﬁrst corresponding to an old stellar\npopulation, the second corresponding to blue continuum emission, which might be due to an AGN or O-\nand B-type stars, the third corresponding to younger stellar population (A-type stars), and the forth and\nﬁfth components corresponding to emission lines.\nnation of principle components can be used to\nembed the high-dimensional dataset onto a low-\ndimensional plane (typically 2 or 3 dimensions).\nFigure 13 shows an application of PCA to a\ntwo-dimensional dataset, where the two princi-\nple components are marked with black arrows.\nOne can see that the ﬁrst principle component is\noriented towards the direction with the maximal\nvariance in the data, and the second principle\ncomponent, which is orthogonal to the ﬁrst one,\ndescribes the remaining variance.\nPCA is among the most popular tools in As-\ntronomy, and it has been used to search for\nmultivariate correlations in high-dimensional\ndatasets, estimate physical parameters of sys-\ntems from their spectra, decompose complex\nspectra into a set of principle components which\nare then used as empirical templates, and more\n(e.g., Boroson & Green 1992; Djorgovski 1995;\nZhang et al. 2006; Vanden Berk et al. 2006;\nRogers et al. 2007; Re Fiorentin et al. 2007;\nBailey 2012). It is simple to use, has no free pa-\nrameters, and is easily interpretable. However,\nPCA performs a linear decomposition of the ob-\njects in the sample, which is not appropriate in\nmany contexts. For example, absorption lines\nand dust extinction are multiplicative eﬀects\nwhich cannot be described by a linear decom-\nposition. Furthermore, PCA tends to ﬁnd linear\ncorrelations between variables, even if those are\nnon-linear, and it fails in cases where the mean\nand the covariance are not enough to deﬁne\nthe dataset.\nSince it constructs its principle\ncomponents to trace the maximal variance in\n24\nthe data, it is extremely sensitive to outliers,\nand these should be removed prior to apply-\ning PCA. Finally, the principle components of\nthe dataset can contain negative values, which\nis also not appropriate in many astronomical\nsetups. For example, applying PCA to galaxy\nspectra results in principle components with\nnegative values, which is of course not physi-\ncal, since the emission of a galaxy is a sum of\npositive contributions of diﬀerent light sources,\nattenuated by absorbing sources such as dust or\ngas.\nFinally, it is worth noting two additional tech-\nniques, Independent Component Analysis (ICA;\nHyv¨arinen & Oja 2000) and Non-Negative Ma-\ntrix Factorization (NMF; Paatero & Tapper\n1994), which are useful for a variety of tasks.\nICA is a method used to separate a multivari-\nate signal into additive components, which are\nassumed to be non-Gaussian and statistically\nindependent from each other.\nICA is a very\npowerful technique, which is often invoked in\nthe context of blind signal separation, such as\nthe ”cocktail party problem”.\nNMF decom-\nposes a matrix into the product of two non-\nnegative ones, and is used in Astronomy to de-\ncompose observations to non-negative compo-\nnents. Figure 14 shows an application of PCA,\nICA, and NMF, taken from Vanderplas et al.\n(2012), on SDSS spectra (see Ivezi´c et al. 2014\nfor additional details about ICA and NMF).\nThe columns represent diﬀerent algorithms, and\nthe rows represent the resulting components\nusing each of the algorithms, sorted by their\nimportance.\nOne can see that while the ﬁrst\ntwo PCA components resemble galaxy spectra\n(with an old stellar population), the next three\ncomponents do not represent a physical compo-\nnent in galaxy spectra, in particular, they show\nnegative ﬂux values.\nOn the other hand, the\nNMF components resemble more physical com-\nponents, with the ﬁrst corresponding to an old\nstellar population, the second corresponding to\nblue continuum emission, which might be due to\nan AGN or O- and B-type stars, the third corre-\nsponding to younger stellar population (A-type\nstars), and the forth and ﬁfth components cor-\nresponding to emission lines. Obviously, the re-\nsemblance is not perfect and one can see residual\nemission line components in the ﬁrst and third\ncomponents (in the ﬁrst component these are\ndescribed by absorption lines with central wave-\nlengths corresponding to the strongest emission\nlines in galaxy spectra). The three algorithms\nare available in scikit-learn13.\n3.3.2. t-Distributed Stochastic Neighbor\nEmbedding\nt-Distributed Stochastic Neighbor Embedding\n(tSNE; van der Maaten & Hinton 2008) is a non-\nlinear dimensionality reduction technique that\nembeds high-dimensional data in a low dimen-\nsional space, typically of two or three dimen-\nsions, and is mostly used to visualize complex\ndatasets.\nThe algorithm models every high-\ndimensional object using a two (or three) di-\nmensional point, such that similar objects are\nrepresented by nearby points, whereas dissim-\nilar objects are represented by distant points,\nwith a high probability. tSNE has been used in\nAstronomy to visualize complex datasets and\ndistance matrices, and study their structure\n(e.g., Lochner et al. 2016; Anders et al. 2018;\nNakoneczny et al. 2018; Reis et al. 2018a; Alib-\nert 2019; Giles & Walkowicz 2019; Moreno et al.\n2019).\nThe ﬁrst step of tSNE is to assign distances\nbetween the objects in the sample, using the\neuclidean metric by default. Then, tSNE con-\nstructs a probability distribution over pairs of\n13\nhttps://scikit-learn.org/stable/modules/\ngenerated/sklearn.decomposition.PCA.html\nhttps://scikit-learn.org/stable/modules/\ngenerated/sklearn.decomposition.FastICA.html\nhttps://scikit-learn.org/stable/modules/\ngenerated/sklearn.decomposition.NMF.html\nMachine Learning in Astronomy\n25\nFigure 15. Application of tSNE to a sample of 53 synthetic single stellar population models, with diﬀerent\nages. The left panel shows 11 out of the 53 spectra, where each spectrum consist of 4 300 ﬂux values. The\nright panels show the resulting two-dimensional embedding using diﬀerent perplexity values, where every\npoint in the 2D plane is colored according to the stellar age, such that purple points represent young stellar\npopulations, and yellow points represent old stellar populations.\nhigh-dimensional objects such that similar ob-\njects have a high probability of being picked,\nwhereas dissimilar objects have an extremely\nlow probability of being picked. This is achieved\nby modeling the probability distribution using\na Gaussian kernel, which depends on the as-\nsigned distance between the objects, and a scale\nparameter, named the perplexity, which is an\nexternal free parameter of the algorithm. The\nperplexity aﬀects the neighborhood of objects\nbeing considered, in terms of their probability of\nbeing selected, where a small perplexity results\nin a very small neighborhood around a given\nobject, and a large perplexity results in a larger\nneighborhood. The algorithm then embeds the\nhigh-dimensional objects into a low dimensional\nspace, such that the probability distribution\nover pairs of points in the low-dimensional plane\nwill be as similar as possible to the probabil-\nity distribution in the high-dimensional space.\nThe axes in the low-dimensional representation\nare meaningless and not interpretable. tSNE is\navailable in the scikit-learn library14.\nFigure 15 shows an application of tSNE to\na sample of 53 single stellar population mod-\nels, taken from the MILES library (Vazdekis\net al. 2010).\nThe left panel shows 11 out of\nthe 53 spectra, where each spectrum has 4 300\nﬂux values, and therefore 4 300 features, ordered\nby age.\nWhile the dataset appears complex,\nit actually represents a one-dimensional mani-\nfold, where all the observed properties can be\nattributed to a change in a single parameter,\n14\nhttps://scikit-learn.org/stable/modules/\ngenerated/sklearn.manifold.TSNE.html\n26\nthe age. Therefore, we expect that in the low\ndimensional representation, all the objects will\noccupy a single line. The right panels show the\ntSNE two-dimensional embedding for diﬀerent\nperplexity values. When the perplexity is set to\np = 1, the neighborhoods considered by tSNE\nare too small, and the data is represented by\nmany small clusters, although the dataset does\nnot consist of clusters. When the perplexity is\nset to p = 2 or p = 5, the dataset is represented\nby an almost perfect one-dimensional manifold,\ntracing the correct structure of the data. When\nthe perplexity is too high, e.g. p = 50, all the\npoints are plotted at the origin, and no structure\ncan be seen. The perplexity has a signiﬁcant ef-\nfect on the resulting embedding, and cannot be\noptimized using the tSNE cost function. It rep-\nresents the scale of the clusters tSNE is sensitive\nto, and setting a small perplexity value allows\ndetection of small clusters, while setting a large\nperplexity value allows study of the global struc-\nture of the dataset.\nHowever, one must take\ninto account that the low-dimensional embed-\ndings by tSNE sometimes show small clusters,\neven if these do not exist in the dataset.\nIn\naddition, one must be careful when trying to\ninterpret long distances in the tSNE map. For\nexample, when p = 2, tSNE embeds the objects\nconsidering only neighborhoods of up to about\nthe 6 nearest neighbors around each point, thus,\ndistances smaller than those spanned by this\nneighborhood are conserved, while longer dis-\ntances are not. While the distances d1 and d2\nappear similar in the tSNE map in Figure 15,\nthe distances between the corresponding objects\nin the original distance matrix are diﬀerent by\na factor of more than ﬁve.\nFinally, it is worth noting an additional tool,\nUniform Manifold Approximation and Projec-\ntion (UMAP; McInnes et al. 2018), which can\nbe used to perform non-linear dimensionality re-\nduction. UMAP is a fast algorithm, that sup-\nports a wide variety of distance metrics, includ-\ning non-metric distance functions such as cosine\ndistance and correlation distance. Furthermore,\nMcInnes et al. (2018) show that UMAP often\noutperforms tSNE at preserving global struc-\ntures in the input dataset.\nUMAP is imple-\nmented in python and is publicly available on\ngithub15.\n3.3.3. Autoencoders\nAn autoencoder is a type of artiﬁcial neu-\nral network used to learn an eﬃcient low-\ndimensional representation of the input dataset,\nand is used for compression, dimensionality re-\nduction, and visualization (Gianniotis et al.\n2015; Yang & Li 2015; Gianniotis et al. 2016;\nMa et al. 2018b; Schawinski et al. 2018). Figure\n16 shows a simpliﬁed example of an autoen-\ncoder architecture. The network consists of two\nparts, the encoder and the decoder. In the en-\ncoding stage, the input data propagates from\nthe input layers to the hidden layers, which\ntypically have a decreasing number of neurons,\nuntil reaching the bottleneck, which is the hid-\nden layer consisting of two neurons. In other\nwords, the encoder performs a compression of\nthe input dataset, by representing it using two\ndimensions.\nThen, the decoder reconstructs\nthe input data, using the information from the\ntwo-dimensional hidden layer. The weights of\nthe network are optimized during the training,\nwhere the loss function is deﬁned as the squared\ndiﬀerence between the input data and the re-\nconstructed data.\nOnce trained, the dataset\ncan be represented in a low-dimensional space,\nalso called the latent space, using the values\ngiven in the bottleneck hidden layer.\nAs for\nall neural network-based algorithms, these net-\nworks are general and ﬂexible, and can be used\nto represent very complex datasets. However,\nthis complexity comes with a price. Such net-\nworks can have many diﬀerent architectures,\n15 https://github.com/lmcinnes/umap\nMachine Learning in Astronomy\n27\nEncoder\nDecoder\nLatent space: \nLow-dimensional \nrepresentation\nInput \nReconstructed \ninput \nFigure 16.\nA simpliﬁed example of an autoen-\ncoder architecture, used to perform compression,\ndimensionality reduction, and visualization.\nThe\nnetwork consists of two parts, the encoder and the\ndecoder. The encoder reduces the dimensions of the\ninput data, while the decoder reconstructs the in-\nput using the low-dimensional representation. The\nweights of the network are optimized during train-\ning to minimize the squared diﬀerences between the\ninput and its reconstruction.\nThe bottleneck of\nthe network, also called the latent vector or latent\nspace, represents the low-dimensional representa-\ntion of the input dataset.\nand thus a large number of free parameters,\nand are diﬃcult to interpret (see lectures by M.\nHuertas-Company for additional details).\n3.3.4. Self Organizing Maps\nA self-organizing map (also named Kohonen\nmap; Kohonen 1982) is a type of artiﬁcial neural\nnetwork that is trained in an unsupervised man-\nner and produces a low-dimensional (typically\ntwo-dimensional) representation of the input\ndataset. During training, the two-dimensional\nmap self-organizes itself to match the input\ndataset, preserving its topology very closely.\nIn Astronomy, self-organizing maps have been\nused to perform semi-supervised classiﬁcation\nand regression, clustering, visualization of com-\nplex datasets, and outlier detection (see e.g.,\nMeusinger et al. 2012; Fustes et al. 2013; Car-\nrasco Kind & Brunner 2014; Armstrong et al.\n2016; Polsterer et al. 2016; Armstrong et al.\n2017; Meusinger et al. 2017; Rahmani et al.\n2018). Figure 17 shows a schematic illustration\nof a self-organizing map, taken from Carrasco\nKind & Brunner (2014). The input dataset con-\nsists of n objects with m features each.\nThe\nnetwork consists of an input layer with m neu-\nrons, and an output layer with k neurons, orga-\nnized as a two-dimensional lattice. The neurons\nfrom the input layer are connected to the out-\nput layer with weight vectors, which have the\nsame dimensions as the input objects (m in this\ncase). Contrary to typical artiﬁcial neural net-\nworks, where the weights are used to multiply\nthe input object values, followed by an appli-\ncation of an activation function, the weights of\nself-organizing maps are characteristics of the\noutput neurons themselves, and they represent\nthe ”coordinates” of each of the k neurons in\nthe input data space. That is, the weight vec-\ntors serve as templates (or prototypes) of the\ninput dataset.\nThe training of a self-organizing map is a com-\npetitive process, where each neuron in the out-\nput layer competes with the other neurons to\nbest represent the input dataset. The ﬁrst step\nof self-organizing maps is a random initializa-\ntion of the weights, where typically, the initial\nweights are set to be equal to randomly-selected\nobjects from the input dataset. Then, the al-\ngorithm iterates over the objects in the input\ndataset. In each iteration, the algorithm com-\nputes the distance between the particular object\nand all the neurons in the output layer, using\nthe euclidean distance between the object’s fea-\ntures and the weight vectors that are associated\nwith the neurons. Then, the algorithm deter-\nmines the closest neuron to the input object,\nand updates its weight vector to be somewhat\ncloser to the input object. The algorithm also\nupdates the weight vectors of the neighboring\nneurons, such that closer neurons are updated\n28\nFigure 17.\nA schematic illustration of a self-\norganizing map, taken from Carrasco Kind & Brun-\nner (2014). The input dataset consists of n objects,\neach with m features, and it is mapped to a two-\ndimensional lattice of k neurons. Each neuron is\nrepresented by a weight vector, which has the same\ndimensions as the input objects. The weights are\ncharacteristics of the neurons themselves, and they\nrepresent the coordinate of each neuron in the input\ndata space. These weights are updated during the\ntraining process, and once the algorithm is trained,\nthey represent a set of templates (or prototypes)\nthat describe the diﬀerent objects in the dataset.\nmore than farther neurons. The update magni-\ntude is determined by a kernel function, usually\na Gaussian, which depends on a learning radius\nparameter.\nThe update magnitude of all the\nneurons depends on a learning rate parameter,\nwhere both the learning radius and the learn-\ning rate decrease with time. The self-organizing\nmap converges after a number of iterations, and\nin its ﬁnal form it separates the input dataset\ninto groups of similar objects, which are repre-\nsented by nearby neurons in the output layer.\nThe ﬁnal weights of the network represent pro-\ntotypes of the diﬀerent groups of objects, and\nthey are usually used to manually inspect the\ndataset. Self-organizing map is implemented in\npython and is publicly available on github16.\n16 https://github.com/sevamoo/SOMPY\nSelf-organizing maps are general and ﬂexible,\nand their capability of sorting the input dataset\nonto a two-dimensional plane allows manual in-\nspection of a relatively small number of proto-\ntypes, and use these to explore the structure of\nthe dataset. However, in their simpler versions,\nself-organizing maps cannot be applied to astro-\nnomical images, since the algorithm is based on\neuclidean similarities, which is not invariant to\nrotations and ﬂips. Polsterer et al. (2016) de-\nveloped PINK, which is a self-organizing map\nthat is invariant to rotations and ﬂips, which is\nparticularly useful for galaxy images, for exam-\nple. Figure 18 shows an application of PINK\nto 200 000 radio images, taken from Polsterer\net al. (2016). The left panel shows the result-\ning two-dimensional map, which contains the\nderived prototypes. These prototypes allow a\nclear separation of the input dataset into diﬀer-\nent morphological types, and by manually in-\nspecting the map, one can explore the range\nof morphological types in the dataset, without\nmanually inspecting thousands of images. The\nright panel shows eight outliers, which are ob-\njects which are not well-represented by any pro-\ntotype.\n3.4. Anomaly Detection\nOutlier detection is the natural step after clas-\nsiﬁcation and the analysis of the dataset struc-\nture.\nIn Astronomy, outlier detection algo-\nrithms were applied to various datasets, includ-\ning galaxy and stellar spectra, galaxy images,\nastronomical light-curves such as variable stars,\nradio images, and more (Meusinger et al. 2012;\nProtopapas et al. 2006; Fustes et al. 2013; Nun\net al. 2016; Agnello 2017; Baron & Poznanski\n2017; Solarz et al. 2017; Hocking et al. 2018;\nReis et al. 2018a; Segal et al. 2018; Giles &\nWalkowicz 2019).\nThere are various types of\noutliers we expect to ﬁnd in our datasets. Out-\nliers can be objects that were not intended to be\nin our datasets, such as a quasar in a sample of\nstars, or various observational or pipeline errors.\nMachine Learning in Astronomy\n29\nFigure 18. Application of PINK to 200 000 radio images from Radio Galaxy Zoo, taken from Polsterer\net al. (2016). The left panel shows the resulting two-dimensional map containing the derived prototypes.\nThe right panel shows eight outliers that were selected based on their dissimilarity with the prototypes, and\nheatmaps that indicate their distance to all the prototypes.\nSuch outliers are not scientiﬁcally interesting,\nbut we wish to detect them and remove them\nfrom our samples. Outliers can be extreme ob-\njects drawn from the tail of well-characterized\ndistributions, such as the most massive super-\nmassive black hole or the most luminous super-\nnova. Such objects are interesting because they\nallow us to test our models, which were built\nto describe the bulk of the population, in ex-\ntreme regimes, and by studying them, we can\nlearn more about the bulk of the population.\nThe most interesting types of the outliers are\nthe ”unknown unknowns” (Baron & Poznan-\nski 2017), objects we did not know we should\nbe looking for, and may be completely new ob-\njects which oﬀer the opportunity to unveil new\nphysics.\nFurthermore, in Astronomy, outliers\ncan actually be very common phenomena, that\noccur on time-scales much shorter than other\ntime scales of the system. For example, if every\ngalaxy in the universe becomes green for 100\nyears, while in the rest of the time it evolves\non a time scale of hundereds of million of years\nwith its ”regular” blue or red color, the proba-\nbility of detecting a galaxy in its ”green phase”\nusing surveys such as the SDSS is close to zero.\nTherefore, although this ”green phase” occurs\nin every galaxy and might be a fundamental\nphase in galaxy evolution, green galaxies will\nappear as outliers in our datasets.\nUnknown\nunknowns\nare\nusually\ndetected\nserendipitously, when experts visually inspect\ntheir datasets, and discover an object that does\nnot follow the accepted paradigm. Manual in-\nspection becomes impractical in the big data\nera, where surveys provide millions of observa-\ntions of a particular class of objects. Indeed,\nthe vast majority of observations are no longer\ninspected by humans.\nTo facilitate new dis-\ncoveries, we must develop and use oﬀ-the-shelf\nalgorithms to perform anomaly detection (see\ndiscussion by Norris 2017a,b).\nOutlier detec-\ntion is in some sense the ultimate unsupervised\nlearning task, since we cannot deﬁne what we\nare looking for. Therefore, outlier detection al-\ngorithms must be as generic as possible, but at\nthe same time they must be optimized to learn\nthe characteristics of the dataset at hand, since\n30\noutliers are deﬁned as being diﬀerent, in some\nsense, from the bulk of the population.\nIn most supervised and unsupervised tasks,\nthe input dataset either consists of the origi-\nnal astronomical observations (spectra, light-\ncurves, images, etc), or features that were\nextracted from the original observations, and\nthere are advantages and disadvantages to both\nof these choices. However, anomaly detection\nshould be carried out on a dataset which is as\nclose as possible to the original dataset, and\nnot on extracted features. First, deﬁning fea-\ntures directly limits the type of outliers one\nmight ﬁnd. For example, in galaxy spectra, ex-\ntracted features include the properties of the\nstellar population, and measurements of diﬀer-\nent emission lines. Such features will not carry\ninformation about new unidentiﬁed emission\nlines in a galaxy, and if such anomalous galaxies\nexist, they will not be marked as outliers. Sec-\nond, extracted features are usually an output\nof some pipeline, which can sometimes fail and\nextract erroneous feature measurements. Such\nerrors typically occur in outlying objects, since\nthe models that are used to extract the fea-\ntures cannot describe them well. In such cases,\nthe resulting features, which were wrongly mea-\nsured, typically show values that are consistent\nwith features measured for the bulk of the pop-\nulation, and such outliers cannot be detected\n(see Baron & Poznanski 2017 for examples).\n3.4.1. Anomaly Detection with Supervised\nLearning\nSupervised learning algorithms can be used\nto detect outliers, in both classiﬁcation and re-\ngression tasks. When applied to new previously\nunseen objects, most supervised learning algo-\nrithms provide some measure of uncertainty or\nclassiﬁcation probability.\nOutliers can be de-\nﬁned as objects that have a large classiﬁcation\nuncertainty or low classiﬁcation probability. For\nexample, a Random Forest algorithm is trained\nto distinguish between the spectra of stars and\nquasars. Then, it predicts the class (and class\nprobabilities) of previously unseen objects. An\nobject that is classiﬁed as a star with a prob-\nability of 0.55 (and probability of 0.45 to be\na quasar) is probably more anomalous than an\nobject that is classiﬁed as a star with a proba-\nbility of 0.95. Therefore, the anomaly score of\nthe diﬀerent objects in the sample can be de-\nﬁned according to the classiﬁcation probability.\nWhile easy to interpret, such a process will only\nreveal the outliers that ”shout the loudest”. Us-\ning again the star-quasar classiﬁcation example,\nwhile a galaxy spectrum will be marked as an\noutlier by such a procedure, more subtle out-\nliers, such as stars with anomalous metallicity,\nwill not be detected. This is because supervised\nlearning algorithms are optimized to perform\nclassiﬁcation (or regression), and as such, they\nwill use only the features that are relevant for\nthe classiﬁcation task at hand. Therefore, ob-\njects which show outlier properties in these fea-\ntures will be detected, while objects that show\noutlier properties in features that are less rele-\nvant for the classiﬁcation task, will not be de-\ntected.\n3.4.2. Anomaly Detection with Unsupervised\nLearning\nThere are several ways to perform outlier de-\ntection in an unsupervised setting. First, one\ncan assign pair-wise distances between the ob-\njects in the sample, and deﬁne outliers as ob-\njects that have a large average distance from the\nrest of the objects (see e.g., Protopapas et al.\n2006; Baron & Poznanski 2017). As discussed\nin section 3.1, in some cases, using a euclidean\nmetric might not result in an optimal perfor-\nmance, and one should consider other metrics.\nFor example, the unsupervised Random Forest-\nbased distance was shown to work particularly\nwell on spectra (Baron & Poznanski 2017; Reis\net al. 2018a,b), and cross correlation-based dis-\ntances work well for time series (Protopapas\net al. 2006; Nun et al. 2016).\nMachine Learning in Astronomy\n31\nAn additional way to perform outlier detec-\ntion is by applying dimensionality reduction\n(which sometimes requires distance assignment\nas well).\nOnce the high dimensional dataset\nis embedded onto a low dimensional plane, it\ncan be visualized. Outliers can be deﬁned as\nobjects that are located far from most of the\nobjects or on the edges of the observed distri-\nbutions.\nThe success of this process strongly\ndepends on the procedure used to perform di-\nmensionality reduction, and one must take into\naccount the internal choices and loss function of\nthe algorithm. For example, when using PCA\nto project the dataset onto a two-dimensional\nplane, one must take into account that while\nsome outliers will be objects with extreme val-\nues in this 2D projection, and thus will be de-\ntected, other outliers can be objects that show\nextreme values in the other eigenvectors, which\nare not used for the projection and visualiza-\ntion, and thus will not show up as outliers. An-\nother example is an auto-encoder, where some\noutliers will show up as extreme objects in the\nlatent space (the two-dimensional representa-\ntion), while other outliers will show typical val-\nues in the latent space, but a large reconstruc-\ntion error on the decoder side. The ﬁnal exam-\nple is tSNE, where one must take into account\nthe fact that the distances of the objects in the\ntwo-dimensional projection are not euclidean.\nIn particular, while short distances in the origi-\nnal distance matrix are roughly conserved in the\ntSNE map, long distances are not.\n3.4.3. One-Class SVM\nOne of the most popular outlier detection\nalgorithms is one-class SVM (Sch¨olkopf et al.\n1999). In one-class SVM, the input data is con-\nsidered to be composed of a single class, rep-\nresented by a single label, and the algorithm\nestimates a distribution that encompasses most\nof the observations. This is done by estimating\na probability distribution function which makes\nmost of the observed data more likely than the\nrest, and a decision rule that separates these ob-\nservations by the largest possible margin. This\nprocess is similar to the supervised version of\nSVM, but applied to a dataset with a single la-\nbel.\nTo optimize over the free parameters of\nSVM, such as the kernel shape and its param-\neters, the input dataset is usually divided into\na training set and a validation set (there is no\nneed for a test set, since this is an unsuper-\nvised setting). The algorithm is trained on the\ntraining set, resulting in some decision function,\nwhile the free parameters are optimized using\nthe validation set. Therefore, the chosen ker-\nnel shape and its free parameters are chosen to\ngive the highest classiﬁcation accuracy on the\nvalidation set, where the classiﬁcation accuracy\nis deﬁned by the number of objects that are\nclassiﬁed as inliers (the opposite of outliers) by\nthe resulting decision function (see e.g., Solarz\net al. 2017). The learned decision function is\nthen used to deﬁne outliers. Outliers are ob-\njects that are outside the decision function, and\ntheir anomaly score can be deﬁned by the dis-\ntance of the outliers from the decision function.\nOne-class SVM is feasible only with datasets\ncomposed of a handful of features. Therefore, it\ncannot be directly applied to astronomical ob-\nservations such as images, light-curves, or spec-\ntra, but can be applied to photometry or de-\nrived features.\nOne-class SVM is available in\nthe scikit-learn library17.\n3.4.4. Isolation Forest\nAnother very popular outlier detection algo-\nrithm is Isolation Forest (Liu et al. 2008). Iso-\nlation Forest consists of a set of random trees.\nThe process of building such a forest is similar to\nthe training process of Random Forest, but here\nboth the feature and the splitting value are ran-\ndomly selected at each node. Within each tree,\noutliers will tend to separate earlier from the\n17\nhttps://scikit-learn.org/stable/modules/\ngenerated/sklearn.svm.OneClassSVM.html\n32\nrest of the sample. Therefore, the anomaly score\ncan be deﬁned as the depth at which a speciﬁc\nobject was split from the rest, averaged over all\nthe trees in the forest. The running time of Iso-\nlation Forest is O(N), where N is the number\nof objects in the sample, and it can be applied\nto datasets with numerous features. Baron &\nPoznanski (2017) compared its performance to\nthat of the unsupervised Random Forest-based\noutlier detection, and found that Isolation For-\nest is capable of ﬁnding the most obvious out-\nliers, those that ”shout the loudest”, but cannot\ndetect subtle outliers, which are typically more\ninteresting in an astronomical context. On the\nother hand, Reis et al. (2018a) and Reis et al.\n(2018b) found that when tuning the range of\npossible feature values that are randomly se-\nlected in each node (i.e., instead of deﬁning the\npossible range to be between the minimum and\nmaximum feature values, one could deﬁne the\nrange to be between the 10th and 90th per-\ncentiles), Isolation Forest results in a compa-\nrable performance to that of the unsupervised\nRandom Forest. Isolation Forest is available in\nthe scikit-learn library18.\n4. SUMMARY\nIn recent years, machine learning algorithms\nhave gained increasing popularity in Astron-\nomy, and have been used for a wide variety of\ntasks. In this document I summarized some of\nthe popular machine learning algorithms and\ntheir application to astronomical datasets. I re-\nviewed basic topics in supervised learning, in\nparticular selection and preprocessing of the in-\nput dataset, evaluation metrics of supervised al-\ngorithms, and a brief description of three popu-\nlar algorithms: SVM, Decision Trees and Ran-\ndom Forest, and shallow Artiﬁcial Neural Net-\nworks. I mainly focused on unsupervised learn-\n18\nhttps://scikit-learn.org/stable/modules/\ngenerated/sklearn.ensemble.IsolationForest.\nhtml\ning techniques, which can be roughly divided\ninto clustering analysis, dimensionality reduc-\ntion, and outlier detection.\nThe most popu-\nlar application of machine learning in Astron-\nomy is its supervised setting, where a machine\nis trained to perform classiﬁcation or regres-\nsion according to previously-acquired scientiﬁc\nknowledge. While less popular in Astronomy,\nunsupervised learning algorithms can be used\nto mine our datasets for novel information, and\npotentially enable new discoveries. In section\n4.1 I list a number of open questions and issues\nrelated to the application of machine learning\nalgorithms in Astronomy. Then, in section 4.2, I\nrefer the reader to textbooks and online courses\nthat give a more extensive overview of the sub-\nject.\n4.1. Open Questions\nThe main issues of applying supervised learn-\ning algorithms to astronomical datasets include\nuncertainty treatment, knowledge transfer, and\ninterpretability of the resulting models.\nAs\nnoted in section 3.1.1, most supervised learning\nalgorithms are not constructed for astronomi-\ncal datasets, and they implicitly assume that all\nmeasured features are of the same quality, and\nthat the provided labels can be considered as\nground truth. However, astronomical datasets\nare noisy and have gaps, and in many cases, the\nlabels provided by human experts suﬀer from\nsome level of ambiguity.\nAs a result, super-\nvised learning algorithms perform well when ap-\nplied to high signal-to-noise ratio datasets, or\nto datasets with uniform noise properties. The\nperformance of supervised learning algorithms\nstrongly depends on the noise characteristics of\nthe objects in the sample, and as such, an algo-\nrithm that was trained on a dataset with partic-\nular noise characteristics will fail to generalize\nto a similar dataset with diﬀerent noise char-\nacteristics. It is therefore necessary to change\nexisting tools and to develop new algorithms,\nwhich take into account uncertainties in the\nMachine Learning in Astronomy\n33\ndataset during the model construction.\nFur-\nthermore, such algorithms should provide pre-\ndiction uncertainties, which are based on the\nintrinsic properties of the objects in the sample\nand on their measurement uncertainties.\nThe second challenge in applying supervised\nlearning algorithms to astronomical datasets is\nrelated to knowledge transfer. That is, an al-\ngorithm that is trained on a particular survey,\nwith a particular instrument, cadence, and ob-\nject targeting selection, will usually fail to gen-\neralize to a diﬀerent survey with diﬀerent char-\nacteristics, even if the intrinsic properties of the\nobjects observed by the two surveys are simi-\nlar. As a result, machine learning algorithms\nare typically applied to concluded surveys, and\nrarely applied to ongoing surveys that have not\nyet collected enough labeled data.\nThe topic\nof knowledge transfer is of particular impor-\ntance when searching for rare phenomena, such\nas gravitational lenses in galaxy images, where\nsupervised learning algorithms that are trained\non simulated data cannot generalize well to real\ndatasets. This challenge can be addressed with\ntransfer learning techniques. While such tech-\nniques are discussed in the computer science lit-\nerature, they are seldom applied in Astronomy.\nThe third challenge in applying supervised\nlearning algorithms to astronomical datasets is\nrelated to the interpretation of the resulting\nmodels. While supervised learning algorithms\noﬀer an extremely ﬂexible and general frame-\nwork to construct complex decision functions,\nand can thus outperform traditional algorithms\nin classiﬁcation and regression tasks, the result-\ning models are often diﬃcult to interpret. That\nis, we do not always understand what the model\nlearned, and why it makes the decisions that it\nmakes.\nAs scientists, we usually wish to un-\nderstand the constructed model and the deci-\nsion process, since this information can teach\nus something new about the underlying physics.\nThis challenge is of particular importance in\nstate-of-the-art deep learning techniques, which\nwere shown to perform exceptionally-well in a\nvariety of tasks. As we continue to develop new\ncomplex tools to perform classiﬁcation and re-\ngression, it is important to devise methods to\ninterpret their results as well.\nWhen applying unsupervised learning algo-\nrithms to astronomical datasets, the main chal-\nlenges include the interpretation of the results\nand comparison of diﬀerent unsupervised learn-\ning algorithms.\nUnsupervised learning algo-\nrithms often optimize some internal cost func-\ntion, which does not necessarily coincide with\nour scientiﬁc motivation, and since these algo-\nrithms are not trained according to some def-\ninition of ”ground truth”, their results might\nlead to erroneous interpretations of trends and\npatterns in our datasets. Many of the state-of-\nthe-art algorithms are modular, thus allowing\nus to deﬁne a cost function that is more appro-\npriate for the task at hand. It is therefore neces-\nsary to formulate cost functions that match our\nscientiﬁc goals better. To interpret the results\nof an unsupervised learning algorithm and to\ncompare between diﬀerent algorithms, we still\nuse domain knowledge, and the process can-\nnot be completely automatized.\nTo improve\nthe process of interpreting the results, we must\nimprove the machine-human interface through\nwhich discoveries are made, e.g., by construct-\ning visualization tools that incorporate post-\nprocessing routines which are typically carried\nout after applying unsupervised learning algo-\nrithms.\nFinally, as we continue to apply un-\nsupervised learning algorithms to astronomical\ndatasets, it is necessary to construct evaluation\nmetrics that can be used to compare the outputs\nof diﬀerent algorithms.\n4.2. Further Reading\nTo learn more about the basics of machine\nlearning algorithms, I recommend the publicly-\navailable machine learning course in cours-\n34\nera19.\nFor an in-depth reading on statistics,\ndata mining, and machine learning in Astron-\nomy, I recommend the book by Ivezi´c et al.\n(2014), which covers in greater depth many\nof the topics presented in this document, and\nmany other related topics. For additional ex-\namples on machine learning in Astronomy, im-\nplemented in python, I recommend astroML20\n(Vanderplas et al. 2012).\nI am grateful to I. Arcavi, N. Lubelchick, D.\nPoznanski, I. Reis, S. Shahaf, and A. Stern-\nberg for valuable discussions regarding the top-\nics presented in this document and for helpful\ncomments on the text.\nSoftware: astroML (Vanderplas et al. 2012),\nscikit-learn (Pedregosa et al. 2011),\nSciPy\n(Jones et al. 2001–), matplotlib (Hunter 2007),\nand IPython (P´erez & Granger 2007).\nREFERENCES\nAgnello, A. 2017, MNRAS, 471, 2013\nAlibert, Y. 2019, arXiv e-prints, arXiv:1901.09719\nAnders, F., Chiappini, C., Santiago, B. X., et al.\n2018, A&A, 619, A125\nArmstrong, D. J., Pollacco, D., & Santerne, A.\n2017, MNRAS, 465, 2634\nArmstrong, D. J., Kirk, J., Lam, K. W. F., et al.\n2016, MNRAS, 456, 2260\nAscasibar, Y., & S´anchez Almeida, J. 2011,\nMNRAS, 415, 2417\nBailey, S. 2012, PASP, 124, 1015\nBalazs, L. G., Garibjanyan, A. T., Mirzoyan,\nL. V., et al. 1996, A&A, 311, 145\nBall, N. M., & Brunner, R. J. 2010, International\nJournal of Modern Physics D, 19, 1049\nBanerji, M., Lahav, O., Lintott, C. J., et al. 2010,\nMNRAS, 406, 342\nBaron, D., & Poznanski, D. 2017, MNRAS, 465,\n4530\nBaron, D., Poznanski, D., Watson, D., et al. 2015,\nMNRAS, 451, 332\nBellm, E. 2014, in The Third Hot-wiring the\nTransient Universe Workshop, ed. P. R.\nWozniak, M. J. Graham, A. A. Mahabal, &\nR. Seaman, 27–33\nBilicki, M., Hoekstra, H., Brown, M. J. I., et al.\n2018, A&A, 616, A69\n19\nhttps://www.coursera.org/learn/\nmachine-learning\n20 http://www.astroml.org/\nBlake, C., Collister, A., Bridle, S., & Lahav, O.\n2007, MNRAS, 374, 1527\nBloom, J. S., Richards, J. W., Nugent, P. E., et al.\n2012, PASP, 124, 1175\nBoroson, T. A., & Green, R. F. 1992, ApJS, 80,\n109\nBreiman, L. 2001, Machine Learning, 45, 5. http:\n//dx.doi.org/10.1023/A:1010933404324\nBreiman, L., Friedman, J. H., Olshen, R. A., &\nStone, C. J. 1984, Monterey, CA: Wadsworth &\nBrooks/Cole Advanced Books & Software\nBrescia, M., Cavuoti, S., D’Abrusco, R., Longo,\nG., & Mercurio, A. 2013, ApJ, 772, 140\nBrescia, M., Cavuoti, S., Longo, G., & De Stefano,\nV. 2014, A&A, 568, A126\nBrescia, M., Cavuoti, S., Paolillo, M., Longo, G.,\n& Puzia, T. 2012, MNRAS, 421, 1155\nCarliles, S., Budav´ari, T., Heinis, S., Priebe, C., &\nSzalay, A. S. 2010, ApJ, 712, 511\nCarrasco Kind, M., & Brunner, R. J. 2014,\nMNRAS, 438, 3409\nCastro, N., Protopapas, P., & Pichara, K. 2018,\nAJ, 155, 16\nCollister, A. A., & Lahav, O. 2004, PASP, 116,\n345\nConnolly, A. J., Szalay, A. S., Bershady, M. A.,\nKinney, A. L., & Calzetti, D. 1995, AJ, 110,\n1071\nD’Abrusco, R., Fabbiano, G., Djorgovski, G.,\net al. 2012, ApJ, 755, 92\nD’Abrusco, R., Longo, G., & Walton, N. A. 2009,\nMNRAS, 396, 223\nMachine Learning in Astronomy\n35\nDaniel, S. F., Connolly, A., Schneider, J.,\nVanderplas, J., & Xiong, L. 2011, AJ, 142, 203\nDas, P., & Sanders, J. L. 2019, MNRAS, 484, 294\nde Souza, R. S., & Ciardi, B. 2015, Astronomy\nand Computing, 12, 100\nde Souza, R. S., Dantas, M. L. L., Costa-Duarte,\nM. V., et al. 2017, MNRAS, 472, 2808\nDelli Veneri, M., Cavuoti, S., Brescia, M., Longo,\nG., & Riccio, G. 2019, arXiv e-prints,\narXiv:1902.02522\nDewdney, P. E., Hall, P. J., Schilizzi, R. T., &\nLazio, T. J. L. W. 2009, IEEE Proceedings, 97,\n1482\nD’Isanto, A., Cavuoti, S., Brescia, M., et al. 2016,\nMNRAS, 457, 3119\nD’Isanto, A., Cavuoti, S., Gieseke, F., & Polsterer,\nK. L. 2018, A&A, 616, A97\nD’Isanto, A., & Polsterer, K. L. 2018, A&A, 609,\nA111\nDjorgovski, S. 1995, ApJL, 438, L29\nDjorgovski, S. G., Graham, M. J., Donalek, C.,\net al. 2016, ArXiv e-prints: 1601.04385,\narXiv:1601.04385\nDonalek, C., Arun Kumar, A., Djorgovski, S. G.,\net al. 2013, arXiv e-prints, arXiv:1310.1976\nEatough, R. P., Molkenthin, N., Kramer, M.,\net al. 2010, MNRAS, 407, 2443\nEllison, S. L., Teimoorinia, H., Rosario, D. J., &\nMendel, J. T. 2016, MNRAS, 458, L34\nFadely, R., Hogg, D. W., & Willman, B. 2012,\nApJ, 760, 15\nFirth, A. E., Lahav, O., & Somerville, R. S. 2003,\nMNRAS, 339, 1195\nFreund, Y., & Schapire, R. E. 1997, J. Comput.\nSyst. Sci., 55, 119.\nhttp://dx.doi.org/10.1006/jcss.1997.1504\nFustes, D., Manteiga, M., Dafonte, C., et al. 2013,\nArXiv e-prints: 1309.2418, arXiv:1309.2418\nGaia Collaboration, Prusti, T., de Bruijne,\nJ. H. J., et al. 2016, A&A, 595, A1\nGalluccio, L., Michel, O., Bendjoya, P., & Slezak,\nE. 2008, in American Institute of Physics\nConference Series, Vol. 1082, American Institute\nof Physics Conference Series, ed. C. A. L.\nBailer-Jones, 165–171\nGarcia-Dias, R., Allende Prieto, C., S´anchez\nAlmeida, J., & Ordov´as-Pascual, I. 2018, A&A,\n612, A98\nGianniotis, N., K¨ugler, D., Tino, P., Polsterer, K.,\n& Misra, R. 2015, arXiv e-prints,\narXiv:1505.00936\nGianniotis, N., K¨ugler, S. D., Tiˇno, P., &\nPolsterer, K. L. 2016, ArXiv e-prints,\narXiv:1601.05654\nGiles, D., & Walkowicz, L. 2019, MNRAS, 484,\n834\nHartley, P., Flamary, R., Jackson, N., Tagore,\nA. S., & Metcalf, R. B. 2017, MNRAS, 471,\n3378\nHertzsprung, E. 1909, Astronomische Nachrichten,\n179, 373\nHocking, A., Geach, J. E., Davey, N., & Sun, Y.\n2015, ArXiv e-prints: 1507.01589,\narXiv:1507.01589\nHocking, A., Geach, J. E., Sun, Y., & Davey, N.\n2018, MNRAS, 473, 1108\nHojnacki, S. M., Kastner, J. H., Micela, G.,\nFeigelson, E. D., & LaLonde, S. M. 2007, ApJ,\n659, 585\nHuertas-Company, M., Rouan, D., Tasca, L.,\nSoucail, G., & Le F`evre, O. 2008, A&A, 478,\n971\nHuertas-Company, M., Primack, J. R., Dekel, A.,\net al. 2018, ApJ, 858, 114\nHui, J., Aragon, M., Cui, X., & Flegal, J. M.\n2018, MNRAS, 475, 4494\nHunter, J. D. 2007, Computing In Science &\nEngineering, 9, 90\nHyv¨arinen, A., & Oja, E. 2000, Neural Netw., 13,\n411. http://dx.doi.org/10.1016/\nS0893-6080(00)00026-5\nIshida, E. E. O., Beck, R., Gonz´alez-Gait´an, S.,\net al. 2019, MNRAS, 483, 2\nIvezi´c, ˇZ., Connolly, A., Vanderplas, J., & Gray,\nA. 2014, Statistics, Data Mining and Machine\nLearning in Astronomy (Princeton University\nPress)\nIvezic, Z., Tyson, J. A., Abel, B., et al. 2008,\nArXiv: 0805.2366, arXiv:0805.2366\nJones, E., Oliphant, T., Peterson, P., et al. 2001–,\nSciPy: Open source scientiﬁc tools for Python, ,\n, [Online; accessed ¡today¿].\nhttp://www.scipy.org/\nKaiser, N., Burgett, W., Chambers, K., et al.\n2010, in Proc. SPIE, Vol. 7733, Ground-based\nand Airborne Telescopes III, 77330E\nKohonen, T. 1982, Biological Cybernetics, 43, 59.\nhttps://doi.org/10.1007/BF00337288\n36\nKov´acs, A., & Szapudi, I. 2015, MNRAS, 448,\n1305\nKrakowski, T., Ma lek, K., Bilicki, M., et al. 2016,\nA&A, 596, A39\nKrone-Martins, A., Ishida, E. E. O., & de Souza,\nR. S. 2014, MNRAS, 443, L34\nKrone-Martins, A., & Moitinho, A. 2014, A&A,\n561, A57\nKrone-Martins, A., Delchambre, L., Wertz, O.,\net al. 2018, A&A, 616, L11\nKsoll, V. F., Gouliermis, D. A., Klessen, R. S.,\net al. 2018, MNRAS, 479, 2389\nKuncheva, L. I., & Whitaker, C. J. 2003, Machine\nLearning, 51, 181.\nhttps://doi.org/10.1023/A:1022859003006\nLaurino, O., D’Abrusco, R., Longo, G., & Riccio,\nG. 2011, MNRAS, 418, 2165\nLevi, M., Bebek, C., Beers, T., et al. 2013, ArXiv :\n1308.0847, arXiv:1308.0847\nLiu, F. T., Ting, K. M., & Zhou, Z. H. 2008,\nIEEE, 413\nLochner, M., McEwen, J. D., Peiris, H. V., Lahav,\nO., & Winter, M. K. 2016, ArXiv e-prints:\n1603.00882, arXiv:1603.00882\nMa, R., Angryk, R. A., Riley, P., & Filali\nBoubrahimi, S. 2018a, ApJS, 236, 14\nMa, Z., Xu, H., Zhu, J., et al. 2018b, arXiv\ne-prints, arXiv:1812.07190\nMacQueen, J. B. 1967, in Proc. of the ﬁfth\nBerkeley Symposium on Mathematical\nStatistics and Probability, ed. L. M. L. Cam &\nJ. Neyman, Vol. 1 (University of California\nPress), 281–297\nMahabal, A., Sheth, K., Gieseke, F., et al. 2017,\nArXiv e-prints, arXiv:1709.06257\nMahabal, A., Djorgovski, S. G., Turmon, M., et al.\n2008, Astronomische Nachrichten, 329, 288\nMahabal, A., Rebbapragada, U., Walters, R.,\net al. 2019, PASP, 131, 038002\nMa lek, K., Solarz, A., Pollo, A., et al. 2013, A&A,\n557, A16\nMasci, F. J., Hoﬀman, D. I., Grillmair, C. J., &\nCutri, R. M. 2014, AJ, 148, 21\nMcInnes, L., Healy, J., & Melville, J. 2018, arXiv\ne-prints, arXiv:1802.03426\nMeusinger, H., Br¨unecke, J., Schalldach, P., & in\nder Au, A. 2017, A&A, 597, A134\nMeusinger, H., Schalldach, P., Scholz, R.-D., et al.\n2012, A&A, 541, A77\nMiller, A. A. 2015, ApJ, 811, 30\nMiller, A. A., Kulkarni, M. K., Cao, Y., et al.\n2017, AJ, 153, 73\nM¨oller, A., Ruhlmann-Kleider, V., Leloup, C.,\net al. 2016, JCAP, 12, 008\nMorales-Luis, A. B., S´anchez Almeida, J., Aguerri,\nJ. A. L., & Mu˜noz-Tu˜n´on, C. 2011, ApJ, 743, 77\nMoreno, J., Vogeley, M. S., & Richards, G. 2019,\nin American Astronomical Society Meeting\nAbstracts, Vol. 233, American Astronomical\nSociety Meeting Abstracts #233, 431.02\nNakoneczny, S., Bilicki, M., Solarz, A., et al. 2018,\narXiv e-prints, arXiv:1812.03084\nNaul, B., Bloom, J. S., P´erez, F., & van der Walt,\nS. 2018, Nature Astronomy, 2, 151\nNorris, R. P. 2017a, PASA, 34, e007\n—. 2017b, Nature Astronomy, 1, 671\nNorris, R. P., Salvato, M., Longo, G., et al. 2019,\narXiv e-prints, arXiv:1902.05188\nNun, I., Protopapas, P., Sim, B., & Chen, W.\n2016, The Astronomical Journal, 152, 71. http:\n//stacks.iop.org/1538-3881/152/i=3/a=71\nPaatero, P., & Tapper, U. 1994, Environmetrics,\n5, 111\nParks, D., Prochaska, J. X., Dong, S., & Cai, Z.\n2018, MNRAS, 476, 1151\nPashchenko, I. N., Sokolovsky, K. V., & Gavras,\nP. 2018, MNRAS, 475, 2326\nPedregosa, F., Varoquaux, G., Gramfort, A., et al.\n2011, Journal of Machine Learning Research,\n12, 2825\nP´erez, F., & Granger, B. E. 2007, Computing in\nScience and Engineering, 9, 21.\nhttp://ipython.org\nPesenson, M. Z., Pesenson, I. Z., & McCollum, B.\n2010, Advances in Astronomy, 2010, 350891\nPeth, M. A., Lotz, J. M., Freeman, P. E., et al.\n2016, MNRAS, 458, 963\nPichara, K., & Protopapas, P. 2013, ApJ, 777, 83\nPichara, K., Protopapas, P., Kim, D.-W.,\nMarquette, J.-B., & Tisserand, P. 2012,\nMNRAS, 427, 1284\nPlewa, P. M. 2018, MNRAS, 476, 3974\nPolsterer, K., Gieseke, F., Igel, C., Doser, B., &\nGianniotis, N. 2016, Parallelized rotation and\nﬂipping INvariant Kohonen maps (PINK) on\nGPUs, ,\nProtopapas, P., Giammarco, J. M., Faccioli, L.,\net al. 2006, MNRAS, 369, 677\nQu, M., Shih, F. Y., Jing, J., & Wang, H. 2003,\nSoPh, 217, 157\nMachine Learning in Astronomy\n37\nRahmani, S., Teimoorinia, H., & Barmby, P. 2018,\nMNRAS, 478, 4416\nRe Fiorentin, P., Bailer-Jones, C. A. L., Lee,\nY. S., et al. 2007, A&A, 467, 1373\nReis, I., Baron, D., & Shahaf, S. 2019, AJ, 157, 16\nReis, I., Poznanski, D., Baron, D., Zasowski, G., &\nShahaf, S. 2018a, MNRAS, 476, 2117\nReis, I., Poznanski, D., & Hall, P. B. 2018b,\nMNRAS, 480, 3889\nRichards, J. W., Homrighausen, D., Freeman,\nP. E., Schafer, C. M., & Poznanski, D. 2012,\nMNRAS, 419, 1121\nRogers, B., Ferreras, I., Lahav, O., et al. 2007, in\nAstronomical Society of the Paciﬁc Conference\nSeries, Vol. 371, Statistical Challenges in\nModern Astronomy IV, ed. G. J. Babu & E. D.\nFeigelson, 431\nRussell, H. N. 1914, Popular Astronomy, 22, 275\nS´anchez Almeida, J., Aguerri, J. A. L.,\nMu˜noz-Tu˜n´on, C., & de Vicente, A. 2010, ApJ,\n714, 487\nS´anchez Almeida, J., & Allende Prieto, C. 2013,\nApJ, 763, 50\nSchawinski, K., Turp, D., & Zhang, C. 2018, in\nAmerican Astronomical Society Meeting\nAbstracts, Vol. 231, American Astronomical\nSociety Meeting Abstracts #231, 309.01\nSch¨olkopf, B., Williamson, R., Smola, A.,\nShawe-Taylor, J., & Platt, J. 1999, in\nProceedings of the 12th International\nConference on Neural Information Processing\nSystems, NIPS’99 (Cambridge, MA, USA: MIT\nPress), 582–588. http://dl.acm.org/\ncitation.cfm?id=3009657.3009740\nSegal, G., Parkinson, D., Norris, R. P., & Swan, J.\n2018, arXiv e-prints, arXiv:1805.10718\nShi, T., & Horvath, S. 2006, Journal of\nComputational and Graphical Statistics, 15, 118\nSimpson, J. D., Cottrell, P. L., & Worley, C. C.\n2012, MNRAS, 427, 1153\nSingh, H. P., Gulati, R. K., & Gupta, R. 1998,\nMNRAS, 295, 312\nSnider, S., Allende Prieto, C., von Hippel, T.,\net al. 2001, ApJ, 562, 528\nSolarz, A., Bilicki, M., Gromadzki, M., et al. 2017,\nA&A, 606, A39\nStorrie-Lombardi, M. C., Lahav, O., Sodre, L., J.,\n& Storrie-Lombardi, L. J. 1992, MNRAS, 259,\n8P\nTagliaferri, R., Longo, G., Andreon, S., et al. 2003,\nLecture Notes in Computer Science, 2859, 226\nTeimoorinia, H., Bluck, A. F. L., & Ellison, S. L.\n2016, MNRAS, 457, 2086\nvan der Maaten, L., & Hinton, G. 2008, Journal of\nMachine Learning Research, 9, 2579.\nhttp://www.jmlr.org/papers/v9/\nvandermaaten08a.html\nVanden Berk, D. E., Shen, J., Yip, C.-W., et al.\n2006, AJ, 131, 84\nVanderplas, J., & Connolly, A. 2009, AJ, 138,\n1365\nVanderplas, J., Connolly, A., Ivezi´c, ˇZ., & Gray,\nA. 2012, in Conference on Intelligent Data\nUnderstanding (CIDU), 47 –54\nVanzella, E., Cristiani, S., Fontana, A., et al.\n2004, A&A, 423, 761\nVasconcellos, E. C., de Carvalho, R. R., Gal,\nR. R., et al. 2011, AJ, 141, 189\nVazdekis, A., S´anchez-Bl´azquez, P.,\nFalc´on-Barroso, J., et al. 2010, MNRAS, 404,\n1639\nWard, J. H. 1963, Journal of the American\nStatistical Association, 58, 236.\nhttps://www.tandfonline.com/doi/abs/10.\n1080/01621459.1963.10500845\nWeaver, W. B., & Torres-Dodgen, A. V. 1995,\nApJ, 446, 300\nWright, D. E., Smartt, S. J., Smith, K. W., et al.\n2015, MNRAS, 449, 451\nYang, T., & Li, X. 2015, MNRAS, 452, 158\nYong, S. Y., King, A. L., Webster, R. L., et al.\n2018, ArXiv e-prints, arXiv:1806.07090\nYork, D. G., Adelman, J., Anderson, Jr., J. E.,\net al. 2000, AJ, 120, 1579\nZhang, J.-n., Wu, F.-c., Luo, A.-l., & Zhao, Y.-h.\n2006, Chinese Astronomy and Astrophysics, 30,\n176\nZucker, S., & Giryes, R. 2018, AJ, 155, 147\n",
  "categories": [
    "astro-ph.IM"
  ],
  "published": "2019-04-15",
  "updated": "2019-04-15"
}