{
  "id": "http://arxiv.org/abs/1904.07320v1",
  "title": "Low-Rank Deep Convolutional Neural Network for Multi-Task Learning",
  "authors": [
    "Fang Su",
    "Hai-Yang Shang",
    "Jing-Yan Wang"
  ],
  "abstract": "In this paper, we propose a novel multi-task learning method based on the\ndeep convolutional network. The proposed deep network has four convolutional\nlayers, three max-pooling layers, and two parallel fully connected layers. To\nadjust the deep network to multi-task learning problem, we propose to learn a\nlow-rank deep network so that the relation among different tasks can be\nexplored. We proposed to minimize the number of independent parameter rows of\none fully connected layer to explore the relations among different tasks, which\nis measured by the nuclear norm of the parameter of one fully connected layer,\nand seek a low-rank parameter matrix. Meanwhile, we also propose to regularize\nanother fully connected layer by sparsity penalty, so that the useful features\nlearned by the lower layers can be selected. The learning problem is solved by\nan iterative algorithm based on gradient descent and back-propagation\nalgorithms. The proposed algorithm is evaluated over benchmark data sets of\nmultiple face attribute prediction, multi-task natural language processing, and\njoint economics index predictions. The evaluation results show the advantage of\nthe low-rank deep CNN model over multi-task problems.",
  "text": "arXiv:1904.07320v1  [cs.LG]  12 Apr 2019\nApril 17, 2019\nJournal of Experimental & Theoretical Artiﬁcial Intelligence\nmain\nTo appear in the COMPUTATIONAL INTELLIGENCE AND NEUROSCIENCE\nVol. 00, No. 00, Month 20XX, 1–16\nLow-Rank Deep Convolutional Neural Network for Multi-Task\nLearning\nFang Sua, Hai-Yang Shangb ∗, Jing-Yan Wangc\na Shaanxi University of Science & Technology, Xi’an, Shaanxi Province, P.R.China, 710021\nb Lanzhou University of Finance and Economics, Lanzhou ,Gansu Province, P.R.China, 730000\nc New York University Abu Dhabi, Abu Dhabi, United Arab Emirates\n(v5.0 released July 2015)\nIn this paper, we propose a novel multi-task learning method based on the deep convolu-\ntional network. The proposed deep network has four convolutional layers, three max-pooling\nlayers, and two parallel fully connected layers. To adjust the deep network to multi-task\nlearning problem, we propose to learn a low-rank deep network so that the relation among\ndiﬀerent tasks can be explored. We proposed to minimize the number of independent param-\neter rows of one fully connected layer to explore the relations among diﬀerent tasks, which\nis measured by the nuclear norm of the parameter of one fully connected layer, and seek a\nlow-rank parameter matrix. Meanwhile, we also propose to regularize another fully connected\nlayer by sparsity penalty, so that the useful features learned by the lower layers can be se-\nlected. The learning problem is solved by an iterative algorithm based on gradient descent\nand back-propagation algorithms. The proposed algorithm is evaluated over benchmark data\nsets of multiple face attribute prediction, multi-task natural language processing, and joint\neconomics index predictions. The evaluation results show the advantage of the low-rank deep\nCNN model over multi-task problems.\nKeywords: Deep Learning; Convolutional Neural Network; Multi-Task Learning;\nEconomics Index Prediction\n1.\nIntroduction\n1.1.\nBackgrounds\nIn\nmachine\nlearning\napplications,\nmulti-task\nlearning\nhas\nbeen\na\npopular\ntopic\n(Argyriou, Evgeniou and Pontil,\n2007;\nBaniata, Park and Park,\n2018;\nCaruana,\n1997;\nDoulamis and Voulodimos,\n2016;\nEvgeniou and Pontil,\n2004;\nJacob, Vert and Bach,\n2009;\nMao, Mu, Zheng and Yan,\n2014;\nRuder,\n2017;\nXue, Liao, Carin and Krishnapuram,\n2007;\nZhou, Wang, Jiang and Li).\nIt\ntries\nto\nsolve multiple related machine learning problems simultaneously. The motive is that\nfor many situations, multiple tasks are closely related, and the prediction results of\ndiﬀerent tasks should be consistent. Accordingly, borrowing the prediction of other\ntasks to help the prediction of a given task is natural. For example, in the face\nattribute prediction problem, given an image, the prediction of female gender and\nwearing long hair is usually related (Cho and Wong, 2008; Owusu, Zhan and Mao, 2014;\nWong and Cho, 2010; Yao, Chen, Jia and Liu, 2018; Zhong, Sullivan and Li, 2016).\n∗Corresponding author.\nApril 17, 2019\nJournal of Experimental & Theoretical Artiﬁcial Intelligence\nmain\nMoreover, in the problem of natural language processing, it is also natural to leverage\nthe problems of part-of-speech (POS) tagging and noun chuck prediction, since a word\nwith a POS of a noun usually appears in a noun chunk (Collobert and Weston, 2008;\nHerath, Ikeda, Ishizaki, Anzai and Aiso,\n1992;\nJabbar, Iqbal, Akhunzada and Abbas,\n2018; Lyon and Frank, 1997; Shams and Mercer, 2016). Multi-task learning aims to\nbuild a joint model for multiple tasks from the same input data.\nIn recent years, deep learning has been proven to be the most powerful data representa-\ntion method (Chao, Zhi, Dong and Liu, 2018; Chu, Huang, Xie, Tan, Kamal and Xiong,\n2018;\nGeng, Zhang, Li, Gu, Liang, Liang, Wang, Wu, Patil and Wang,\n2017;\nGlorot, Bordes and Bengio,\n2011;\nGuo, Liu, Oerlemans, Lao, Wu and Lew,\n2016;\nHu, Wang, Peng, Qiu, Shi and Liu,\n2018;\nL¨angkvist, Karlsson and Loutﬁ,\n2014;\nLeCun, Bengio and Hinton,\n2015;\nNgiam, Khosla, Kim, Nam, Lee and Ng,\n2011;\nSadouk, Gadi and Essouﬁ,\n2018;\nSchmidhuber,\n2015;\nVoulodimos, Doulamis, Bebis and Stathaki, 2018a; Voulodimos, Doulamis, Doulamis and Protopapadakis,\n2018b; Wu, Zhai, Li, Cui, Wang and Patil; Zhang, Liang, Li, Fang, Wang, Geng and Wang,\n2017; Zhang, Liang, Su, Qu and Wang, 2018a). Deep learning methods learn a neural\nnetwork of multiple layers to extract the hierarchical patterns from the original data,\nand provide high-level and abstractive features for the learning problems. For example,\nfor the face recognition problems, a deep learning model learn simple patterns by the\nlow-level layers, such as lines, edges, circles, and squares. In the median-level layers,\nparts of faces are learned, such as eyes, noses, mouths, etc. In the high-level layers,\npatterns of entire faces of diﬀerent users are obtained. With the deep learning model,\nwe can explore the hidden but eﬀective patterns from the original data directly with\nmultiple layers, even without domain knowledge and hand-coded features. This is a\ncritical advantage compared to traditional shallow learning paradigms models.\nRemark: If shallow learning paradigms is applied in this case, the model structure\nwill not be suﬃcient to extract complex hierarchical features. The users of these shallow\nlearning models have to code all these complex hierarchical features manually in the\nfeature extraction process, which is diﬃcult and some times impossible.\nRemark: If other non-neural networks learning models is used, such as spectral cluster-\ning, the hidden pattern of input data features cannot be directly explored. For example,\nspectral clustering treats each data point as a node in a graph, and sperate them by\ncutting the graph. However, it still needs a powerful data representation method to build\nthe graph, and itself cannot work well with a high-quality graph. Meanwhile, neural net-\nwork models, especially deep neural network models, have the ability to represent the\nhidden patterns of input data points and build the high-quality graph accordingly. Thus\nthe non-neural network models and neural network models are complementary.\nMost\nrecently, deep learning has been found very eﬀective for multi-task learning problems.\nFor example, the following works have discussed the usage of deep learning for multi-task\nprediction.\n• Zhang et al. (Zhang, Luo, Loy and Tang, 2014) formulated a deep learning model\nconstrained by multiple tasks, so that the early stopping can be applied to diﬀerent\ntasks to obtain good learning convergence. Furthermore, diﬀerent tasks regarding\nface images, including facial landmark detection, head pose estimation, and facial\nattribute detection are considered together by using a common deep convolutional\nneural network.\n• Liu et al. (Liu, Gao, He, Deng, Duh and Wang, 2015a) proposed a deep neural\nnetwork learning method for multi-task learning problems, especially for learning\nrepresentations across multiple tasks. The proposed method can combine cross-task\n2\nApril 17, 2019\nJournal of Experimental & Theoretical Artiﬁcial Intelligence\nmain\ndata, and also regularize the neural network to make it generalized to new tasks.\nIt can be used for both multiple domain classiﬁcation problems and information\nretrieval problems.\n• Collobert et al. (Collobert and Weston, 2008) proposed a convolutional neural net-\nwork for multi-task learning problem in natural language processing applications.\nThe targeted multiple tasks include POS tagging, noun chunk prediction, named\nentity recognition, etc. The proposed network is not only applied to multi-task\nlearning, but also applied to semi-supervised learning, where only a part of the\ntraining set is labeled.\n• Seltzer et al. (Seltzer and Droppo, 2013) proposed to learn a deep neural network\nfor multiple tasks which shares the same data representations. This model is used to\nthe applications of acoustic models with a primary task and one or more additional\ntasks. The tasks include phone labeling, phone context prediction, and state context\nprediction.\nHowever, the relation among diﬀerent tasks is not explored explicitly. Although the\ndeep neural model can learn eﬀective high-level abstractive features, without explicitly\nexplore the relation of diﬀerent tasks, diﬀerent groups of level features may be used to\ndiﬀerent tasks. Thus the deep features are separated for diﬀerent tasks, and the relation-\nships among diﬀerent tasks are ignored during the learning process of the deep network.\nTo solve this problem, we propose a novel deep learning method by regularizing the\nparameters of the neural network regarding multiple tasks by low-rank.\n1.2.\nOur Contributions\nThe proposed deep neural network is composed of four convolutional layers, three max-\npooling layers, and two parallel fully connected layers. The convolutional layers are used\nto extract useful patterns from the local region of the input data, and the max-pooling\nlayers are used to reduce the size of the intermediate outputs of convolutional layers\nwhile keeping the signiﬁcant responses. The last two fully connected layers are used to\nmap the outputs of convolutional and max-pooling layers to the labels of multiple tasks.\nThe rows of the transformation matrices of the full connection layers are correspond-\ning to the mapping of diﬀerent tasks. We assume that the tasks under consideration\nare closely related, thus the rows of the transformation matrices are not completely in-\ndependent to each other, thus we seek such a transformation matrix with a minimum\nnumber of independent rows. We use the rank of the transformation matrix to measure\nthe number of the independent rows and measure it by the nuclear norm. During the\nlearning process, we propose to minimize the nuclear norm of one fully connected layer’s\ntransformation matrix. Meanwhile, we also assume that for a group of related tasks, only\nall the high-level features generated by the convolutional layers and max-pooling layers\nare useful. Thus it is necessary to select useful features. To this end, we propose to seek\nsparse rows for the second fully connected layer. The sparsity of the second transforma-\ntion matrix is measured by its ℓ1 norm, and we also minimize it in the learning process. Of\ncourse, we hope the predictions of the two fully connected layers could be low-rank and\nsparse simultaneously, and also consistent with each other. Thus we propose to minimize\nthe squared ℓ2 norm distance between the prediction vectors of the two fully connected\nlayers. Meanwhile, we also reduce the prediction error and the complexity of the ﬁlters\nof the convolutional layers measured by the squared ℓ2 norms. The objective function is\nthe linear combination of these terms.\nWe developed an iterative algorithm to minimize the objective function. In each it-\n3\nApril 17, 2019\nJournal of Experimental & Theoretical Artiﬁcial Intelligence\nmain\neration, the transformation matrices and the ﬁlters are updated alternately. The trans-\nformation matrices are optimized by the gradient descent algorithm, and the ﬁlters are\noptimized by the back-propagation algorithms.\n1.3.\nPaper Origination\nThe rest parts of this paper are organized as follows. In section 2, we introduce the\nproposed method by modeling the problem as a minimization problem and develop an\niterative algorithm to solve it. In section 4, we conclude the paper.\n2.\nProposed Method\n2.1.\nProblem Modeling\nSuppose we have a set of n data points for the training process, denoted as {x1, · · · , xn},\nwhere xi is the i-th data point. xi could be an image (presented as a matrix of pixels)\nor text (a sequence of embedding vectors of words). The problem of multi-task learning\nis to predict the label vectors of m tasks. For xi, the label vector is denoted as yi =\n[yi1, · · · , yim]⊤∈{1, −1}m, where yij = 1 if xi is a positive sample for the j-th task, and\nyij = −1 otherwise.\nTo this end, we build a deep convolutional network to map the input data point to an\noutput label vector. The network is composed of 4 convolutional layers, 3 max-pooling\nlayers, and 2 parallel fully-connected layers. The structure of the deep network is given in\nFigure 1. Please note that for diﬀerent types of input data, the convolutional and max-\npooling layers are adjustable. For matrix inputs such as images, the layers perform 2-D\nconvolution and 2-D max-pooling, while for sequences such as text, the layers conduct\n1-D convolution and 1-D max-pooling.\nWe denote the intermediate output vector of the ﬁrst 7 layers as φ(x) ∈Rp, where x is\nthe input, and p is the number of pools of the last max-pooling layer. The set of ﬁlters in\nthe convolutional layers of φ(x) are denoted as Φ. The outputs of the two parallel fully\nconnected layers are denoted as\nf1(x) = W1φ(x) ∈Rm, and f2(x) = W2φ(x) ∈Rm,\n(1)\nwhere W1 ∈Rm×p and W2 ∈Rm× are the transformation matrix of the two layers. The\ntwo fully connection layers map the a p-dimensional vector φ(x) to two vectors of m\nscores for m tasks. Each score measures the degree of the given data point belonging to\nthe positive class. The two fully connected layers are corresponding to the low-rank and\nsparse prediction results of the network. By fusing their results, we can explore both the\nlow-rank structure of the prediction scores of multi-tasks, and also the sparse structure\nof the deep features learned from the network. In our model, the ﬁrst fully connected\nlayer f1(x) is responsible for the low-rank structure, while the second fully connected\nlayer f1(x) is responsible for the sparse structure.\nThe ﬁnal outputs of the network are the summation of the outputs of the two fully\nconnected layers,\ng1(x) = W1φ(x) + W2φ(x) ∈Rm.\n(2)\n4\nApril 17, 2019\nJournal of Experimental & Theoretical Artiﬁcial Intelligence\nmain\nInput\nConvolutional Layer\nMax-Pooling Layer\nConvolutional Layer\nMax-Pooling Layer\nFully-Connected Layer\nConvolutional Layer\nMax-Pooling Layer\nFully-Connected Layer\nX2\n+\nOutput\nFigure 1. Sturcture of the proposed deep convolutional network.\nTo learn the parameters of the deep network of g1(x), we consider the following four\nproblems.\n• Low-Rank Regularization As we discussed earlier, the tasks are not completely\nindependent from each other, but they are closely related to each other. To explore\nthe relationships between diﬀerent tasks, we learn a deep and shared representation\nφ(x) for the input data x. Based on this shared representation, we also request the\ntransformation matrix W1 of one of the last fully connected layer to be of low-\nrank. The motive is that the m columns of W1 actually map the representation\nφ(x) to the m scores of m-tasks. The rank of W1 measures the maximum number\nof linearly independent columns of W1. Thus by minimizing the rank of W1, we can\nimpose the mapping functions of diﬀerent tasks to be dependent on each other and\nminimize the number of independent tasks. To measure the rank the matrix W1,\nrank(W1), we use the nuclear norm of W1, denoted as ||W1||∗. ||W1||∗is calculated\nas the summation of its singular values,\n||W1||∗=\nX\nl\n̺l,\n(3)\nwhere ̺l is its l-th singular value. We propose to learn W1 by regularizing its rank\nas follows,\n5\nApril 17, 2019\nJournal of Experimental & Theoretical Artiﬁcial Intelligence\nmain\nmin\nW1 ||W1||∗.\n(4)\n• Sparse Regularization We further regularize the mapping transformation matrix\nof the second fully connected layer by sparsity. The motive of the sparsity is that\nthe eﬀective deep features for diﬀerent tasks might be diﬀerent, and for each task,\nnot all the feature are needed. Although we learn a group of deep features in φ(x)\nand share it with all the tasks, for a speciﬁc task and its relevant tasks, only\na small number of deep features are necessary, and feature selection is a critical\nstep. For the purpose of features selection, we impose the sparsity penalty to the\ntransformation matrix of the second fully connected layer, W2, since it maps the\ndeep features to the prediction scores of m tasks. To measure the sparsity of W2,\nwe use the ℓ1 norm of W2, which is the summation of the absolute values of all the\nelements of the matrix,\n||W2||1 =\nX\njk\n|[W2]jk| .\n(5)\nWe minimize the ℓ1 norm of W2 to learn a sparse W2,\nmin\nW2\n1\n2||W2||1\n(6)\n• Prediction Consistency The outputs of the two fully connected layers of low-\nrank and sparsity may give diﬀerent results. However, we how they can be consistent\nwith each other so that the prediction results can be low-rank and sparse simulta-\nneously. To this end, we impose to minimize the squared ℓ2 norm distance between\nthe prediction results of the two layers over all the training data points,\nmin\nΦ,W1,W2\n1\n2\nn\nX\ni=1\n∥W1φ(xi) −W2φ(xi)∥2\n2 .\n(7)\n• Prediction Error Minimization We also propose to learn an eﬀective multi-task\npredictor by minimizing the prediction error. To measure the prediction error of\na data point, x, we calculate the squared ℓ2 norm distance between its prediction\nresult g(x) and its true label vector y,\n||y −g(x)||2\n2 = ||y −(W1φ(x) + W2φ(x))||2\n2.\n(8)\nWe learn the parameters of the deep network by minimizing the errors over all the\ntraining data points,\nmin\nΦ,W1,W2\n1\n2\nn\nX\ni=1\n∥yi −(W1φ(xi) + W2φ(xi))∥2\n2 .\n(9)\n6\nApril 17, 2019\nJournal of Experimental & Theoretical Artiﬁcial Intelligence\nmain\n• Complexity Reduction Finally, we regularize the ﬁlters of the convolutional\nlayers, Φ, by the squared ℓ2 norms to prevent the network from being over complex,\nmin\nΦ\n1\n2 ∥Φ∥2\n2 .\n(10)\nThe overall optimization problem is the weighted combination of the problems above,\nmin\nΦ,W1,W2\n(\ng = 1\n2 ∥Φ∥2\n2 + C1\n2\nn\nX\ni=1\n∥yi −(W1φ(xi) + W2φ(xi))∥2\n2\n+C2||W1||∗+ C3\n2 ||W2||1 + C4\n2\nn\nX\ni=1\n∥W1φ(xi) −W2φ(xi)∥2\n2\n)\n(11)\nwhere C1, C2, C3 and C4 are the weights of diﬀerent objective terms, and g is the\noverall objective function. By optimizing this problem, we can obtain a deep convolutional\nnetwork with a low-rank and sparse deep features for the problem of multi-task learning.\n2.2.\nOptimization\nTo solve the problem in (12), we use the alternate optimization method. The parameters\nare updated iteratively in an iterative algorithm. When one parameter is updated, others\nare ﬁxed. In the following sections, we will discuss how to solve them separately.\n2.2.1.\nUpdating W1\nWhen we update W1, we ﬁx W2 and Φ, remove the terms irrelevant to W1 from (12),\nand obtain the following optimization problem,\nmin\nW1\n(\ng1(W1) = C1\n2\nn\nX\ni=1\n∥yi −(W1φ(xi) + W2φ(xi))∥2\n2\n+C2||W1||∗+ C4\n2\nn\nX\ni=1\n∥W1φ(xi) −W2φ(xi)∥2\n2\n)\n,\n(12)\nwhere g1 is the objective function of this problem. To solve this problem, we use the\ngradient descent algorithm. W1 is descended to the direction of the gradient of g1(W1),\nW1 ←W1 −ς∇g1(W1),\n(13)\nwhere ∇g1(W1) is the gradient function of g1(W1), and ς is the descent step size. To\ncalculate the gradient function ∇g1(W1), we ﬁrst split the objective into two terms,\n7\nApril 17, 2019\nJournal of Experimental & Theoretical Artiﬁcial Intelligence\nmain\ng1(W1) = g11(W1) + g12(W1), where\ng11(W1) = C1\n2\nn\nX\ni=1\n∥yi −(W1φ(xi) + W2φ(xi))∥2\n2\n+ C4\n2\nn\nX\ni=1\n∥W1φ(xi) −W2φ(xi)∥2\n2 ,\nand g12(W1) = C2||W1||∗.\n(14)\nThe ﬁrst term g11(W1) is a quadratic term while g12(W1) is a unclear term. Thus the\ngradient function of g1(W1) is the sum of the gradient functions of the two terms,\n∇g1(W1) = ∇g11(W1) + ∇g12(W1).\n(15)\nwhere ∇g11(W1) can be easily obtained as\ng11(W1) = −C1\nn\nX\ni=1\n(yi −(W1φ(xi) + W2φ(xi))) φ(xi)⊤\n+ C4\nn\nX\ni=1\n(W1φ(xi) −W2φ(xi)) φ(xi)⊤.\n(16)\nTo obtain the gradient function of g12(W1) = C2 ∥W1∥∗, we ﬁrst decompose W1 by\nsingular value decomposition (SVD),\nW1 = UΣV,\n(17)\nwhere U and V are the two orthogonal matrices, Σ is a diagonal matrix containing all\nthe singular values. According to the Proposition 1 of (Zhen, Yu, He and Li, 2017), the\ngradient of ∥W1∥∗= UΣ−1|Σ|V , thus\ng12(W1) = C2UΣ−1|Σ|V.\n(18)\n2.2.2.\nUpdating W2\nTo update W2, we also ﬁx other parameters and remove the irrelevant terms,\ng2(W2) = C1\n2\nn\nX\ni=1\n∥yi −(W1φ(xi) + W2φ(xi))∥2\n2\n+ C3\n2 ||W2||1 + C4\n2\nn\nX\ni=1\n∥W1φ(xi) −W2φ(xi)∥2\n2\n= g21(W2) + g22(W2),\n(19)\n8\nApril 17, 2019\nJournal of Experimental & Theoretical Artiﬁcial Intelligence\nmain\nwhere\ng21(W2) = C1\n2\nn\nX\ni=1\n∥yi −(W1φ(xi) + W2φ(xi))∥2\n2 + C4\n2\nn\nX\ni=1\n∥W1φ(xi) −W2φ(xi)∥2\n2 (20)\nis a quadratic term, and\ng22(W2) = C3\n2 ||W2||1\n(21)\nis a ℓ1 norm term. We also use the gradient descent algorithm to update W2,\nW2 ←W2 −ς∇g2(W2), where\n∇g2(W2) = ∇g21(W2) + ∇g21(W2), and\n∇g21(W2) = −C1\nn\nX\ni=1\n(yi −(W1φ(xi) + W2φ(xi))) φ(xi)⊤\n−C4\nn\nX\ni=1\n(W1φ(xi) −W2φ(xi)) φ(xi)⊤.\n(22)\nTo obtain the gradient function of g21(W2), we rewrite W2 and ∥W2∥1 as follows,\nW2 =\n\n\nw21\n...\nw2m\n\n, where ∥W2∥1 =\nm\nX\ni=1\n∥w2i∥1 =\nm\nX\ni=1\n∥w2i∥1, and\n∥w2i∥1 =\nd\nX\nj=1\n\f\fw2ij\n\f\f =\nd\nX\nj=1\nw2i2\nj\n\f\fw2ij\n\f\f = w2idiag (|w2i1| , · · · , |w2id|)−1 w⊤\n2i\n(23)\nand w2i = [w2i1, · · · , w2id] is the i-th row of W2. The gradient function of g22(W2)\nregarding W2, we decompose the problem to the gradients of g22 regarding to diﬀeren\nrows of W2, since in the problem the rows are independent to each other,\n∇g22(W2) =\n\n\n∇g22(w21)\n...\n∇g22(w2m),\n\n\n(24)\nwhere ∇g22(w2i) is the gradient of g22 regarding w2i, and according to (23), we have the\nsub-gradient of g22 as follows,\n∇g22(w2i) = C3w2idiag (|w2i1| , · · · , |w2id|)−1 .\n(25)\n9\nApril 17, 2019\nJournal of Experimental & Theoretical Artiﬁcial Intelligence\nmain\n2.2.3.\nUpdating Φ\nTo optimize the ﬁlters of the deep network, we ﬁx both W1 and W2 and use the back-\npropagation algorithm based on the chain rule. The corresponding problem is given as\nfollows,\nmin\nΦ\n(\ng3(Φ) = 1\n2 ∥Φ∥2\n2 +\nn\nX\ni=1\n\u0012C1\n2 ∥yi −(W1φ(xi) + W2φ(xi))∥2\n2\n+C4\n2 ∥W1φ(xi) −W2φ(xi)∥2\n2\n\u0013\u001b\n= 1\n2 ∥Φ∥2\n2 +\nn\nX\ni=1\ng3i(Φ), where\ng3i(Φ) = C1\n2 ∥yi −(W1φ(xi) + W2φ(xi))∥2\n2\n+ C4\n2 ∥W1φ(xi) −W2φ(xi)∥2\n2\n(26)\nis a data point-wise term. Back propagation is based on gradient descent algorithm,\nΦ ←Φ −ς∇g3(Φ),\n(27)\nand according to the chain rule,\n∇g3(Φ) = Φ +\nn\nX\ni=1\n∇g3i(Φ), where\n∇g3i(Φ) = ∇g3i(φ(xi))∇Φφ(xi), and\n∇g3i(φ(xi)) = −C1(W1 + W2)⊤(yi −(W1φ(xi) + W2φ(xi)))\n+ C4(W1 −W2)⊤(W1φ(xi) −W2φ(xi)) .\n(28)\n3.\nExperiments\nIn this section, we test the proposed method over several multi-task learning problems\nand compare it to the state-of-the-art deep learning methods for the multi-task learning\nproblem.\n3.1.\nExperiment Setting\nWe test the proposed method over the following benchmark data sets.\n• Large-scale CelebFaces Attributes (CelebA) Dataset The ﬁrst dataset we\nused is a face image data set, named CelebA Dataset (Liu, Luo, Wang and Tang,\n2015b). This data set has 202,599 images and each image has 40 binary attributes,\nsuch as wearing eyeglasses, wearing hats, having a pointy nose, smiling, etc. The\nprediction of each attribute is treated as a task, thus this is 40-task multi-task\n10\nApril 17, 2019\nJournal of Experimental & Theoretical Artiﬁcial Intelligence\nmain\nlearning problem. The input data is image pixels. The downloading URL for this\ndata set is at http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html.\n• Annotated Corpus for Named Entity Recognition The second data set\nwe used is a data set for named entity recognition. It contains 47,959 sentences,\nwhich contains 1,048,576 words. Each word is tagged by a named entity type,\nsuch as Geographical Entity, Organization, Person, etc, or a non-named entity.\nMoreover, each work is also tagged by a part-of-speech (POS) type, such as noun,\npronoun, adjective, determiner, verb, adverb, etc. Meanwhile, we also have the\nlabels of noun chunk. We have three tasks for each work, named entity recog-\nnition (NER), POS labeling, and noun clunking. For each word, we use a win-\ndow of size 7 to extract the context, and the embedding vectors of the words\nin the window are used as the input. This dataset can be downloaded from\nhttps://www.kaggle.com/abhinavwalia95/entity-annotated-corpus.\n• Economics The third data set we used is a data set for tasks of property price\ntrend and stock price trend prediction. The input data is the wave of historical\ndata of property prices and stock prices, and each data point is the data of three\nmonths of both prices of properties and stocks, and the label of each data point is\nthe trend of stock price and property price. We collect the data of last 20 years of\nUSA and China, and generate a total number of 480 data points.\nIn the experiments, we split an entire data set to a training set and a test set of the\nequal sizes. The training set is used to learn the parameters of the deep network, and\nthen we use the test set to evaluate the performance of the proposed learning method.\nTo measure the performance, we use the average accuracy for diﬀerent tasks.\n3.2.\nExperiment Results\n3.2.1.\nComparison of prediction accuracy of diﬀerent methods\nWe compare the proposed method against several deep learning-based multi-task meth-\nods, including the methods proposed by Zhang et al. (Zhang et al., 2014), Liu et al.\n(Liu et al., 2015a), Collobert et al. (Collobert and Weston, 2008), and Seltzer et al.\n(Seltzer and Droppo, 2013). The results are reported in Fig. 2. According to the results,\nthe proposed methods always achieves the best prediction performances, over three multi-\ntask learning tasks, especially in the NER and Economics. For the Economics benchmark\ndataset, our method is the only method which obtains an average prediction accuracy\nhigher than 0.80, while the other methods only obtain accuracies lower than 0.75. This\nis not surprising since our method has the ability to explore the inner relation between\ndiﬀerent tasks by the low-rank regularization of the weights of the CNN model for dif-\nferent tasks. In the Economics benchmark data set, the number of training examples\nis small, thus it is even more necessary to borrow the data representation of diﬀer-\nent tasks. For the CelebFaces data set, the improvement of the proposed method over\nthe other methods are slight. Moreover, we also observe that the methods Zhang et al.\n(Zhang et al., 2014) and Liu et al. (Liu et al., 2015a) outperforms the methods of Col-\nlobert et al. (Collobert and Weston, 2008), and Seltzer et al. (Seltzer and Droppo, 2013)\nin most cases.\n11\nApril 17, 2019\nJournal of Experimental & Theoretical Artiﬁcial Intelligence\nmain\nFigure 2. Prediction performance of compared methods over benchmark data sets.\nFigure 3. Running time of compared methods over benchmark data sets.\n3.2.2.\nComparison of running time of diﬀerent methods\nWe also report the running time of the training processes of the compared methods in\nFig. 3. According to the results reported in the ﬁgure, the training process of Seltzer et\nal. (Seltzer and Droppo, 2013)’s method is the longest, and the most eﬃcient method is\nCollobert et al. (Collobert and Weston, 2008)’s algorithm. Our method’s running time\nof the training process is longer than Zhang et al. (Zhang et al., 2014) and Collobert\net al. (Collobert and Weston, 2008)’s methods, but still acceptable for the data sets of\nCelebFaces and NER. While for the training process over the Economics benchmark data\nset, the running time is very short compared to the other two data sets, since its size is\nrelatively small.\n12\nApril 17, 2019\nJournal of Experimental & Theoretical Artiﬁcial Intelligence\nmain\n3.2.3.\nInﬂuence of tradeoﬀparameters\nIn our method, there are four important of tradeoﬀparameters, which controls the\nweights of the terms of classiﬁcation errors, the rank of the weight matrix, and the\nℓ1 norm sparsity of the weight matrix, and the consistency of predictions of the sparse\nmodel and low-rank model. The four tradeoﬀparameters are C1, C2, C3 and C4. We\nstudy the inﬂuences of the changes of their values to the prediction accuracy and report\nthe results of our method with varying values of these parameters in Fig. 4. We have the\nfollowing observations as follows.\n• According to the results in Fig. 4, when the values of C1 increase from 0.01 to 100,\nthe prediction accuracy keeps growing. This is due to the fact that this parameter\nis the weight of the classiﬁcation error term, and when its value is increasing the\nclassiﬁcation error over the training set plays a more and more important role in\nthe learning process, thus it boosts the classiﬁcation performance accordingly. But\nwhen its value is larger than 100, the performance improvement is not signiﬁcant\nanymore.\n• When the values of C2 increases, the performance of the proposed keeps improv-\ning. This is due to the importance of the low-rank regularization of the proposed\nmethod. C2 controls the weight of the low-rank regularization term, and it is the\nkey to explore the relationships among diﬀerent tasks of multi-task problem. This\nis even more obvious for the Economics data set, where the data size is small, and\ncross-task information plays a more important role.\n• The proposed algorithm seems stable to the changes in the values of C3, which is\nthe weight of the sparsity term of the objective. This term plays the role of feature\nselection over the convolutional representation of the input data. The stability over\nthe changes of C3 implies that the convolutional features extracted by our model\nalready give good performances, thus the feature selection does not signiﬁcantly\nimprove the performances.\n• For the parameter C4, the average accuracy improves slightly when its value in-\ncreases until it reaches 100, then the performances seem to decrease slightly. This\nsuggests that the consistency between sparsity and low-rank somehow improves the\nperformance, but it does not always help. For forcing the consistency with a large\nweight for the consistency term, the performance will not be improved.\n4.\nConclusion\nIn this paper, we proposed a novel deep learning method for the multi-task learn-\ning problem.\nThe proposed deep network\nhas\nconvolutional,\nmax-pooling,\nand\nfully connected layers. The parameters of the network are regularized by low-rank\nto explore the relationships among diﬀerent tasks. Meanwhile, it also has the\nfunction of deep feature selection by imposing sparsity regularization. The learn-\ning of the parameters are modeled as a joint minimization problem and solved\nby an iterative algorithm. The experiments over the benchmark data sets show\nits advantage over the state-of-the-art deep learning-based multi-task models. In\nthe future, we will apply the proposed method to other ﬁelds, such as global\nforest\nproduct\nprediction\n(Jiang, Carter, Fu, Jacobson, Zipp, Jin and Yang,\n2019;\nLiu, Gao, Chen, Fu, Jiang, Wang and Kou, 2019; Liu, Wang and Fu, 2018), biomate-\nrials\nscience\n(Zhang, Kramer, Smith, Allen, Leeper, Li, Morton, Gallazzi and Ulery,\n13\nApril 17, 2019\nJournal of Experimental & Theoretical Artiﬁcial Intelligence\nmain\nFigure 4. Inﬂuences of tradeoﬀparameters over benchmark data set.\n2018b; Zhang, Leeper, Wang, White and Ulery, 2018c; Zhang, Morton, Smith, Gallazzi, White and Ulery,\n2018d; Zhang, Smith, Allen, Kramer, Schauﬂinger and Ulery, 2018e; Zhang and Ulery,\n2018), bio-informatics, (Gui, Ma, Wang and Wilkins, 2016; Gui et al., 2016), wireless\nnetworks\n(Liu, Chen and Zhan,\n2013;\nLiu, Zhan and Chen,\n2014;\nYang and Zhao,\n2015;\nYang, Zhao, Hai, Liu, Hoi and Li,\n2016),\ninformation\ncommunication\n(Wang, Millet and Smith, 2014, 2016), etc.\nReferences\nArgyriou, A., Evgeniou, T., Pontil, M., 2007. Multi-task feature learning, in: Advances in neural\ninformation processing systems, pp. 41–48.\nBaniata, L., Park, S., Park, S.B., 2018. A neural machine translation model for arabic dialects\nthat utilises multitask learning (mtl). Computational Intelligence and Neuroscience 2018.\nCaruana, R., 1997. Multitask learning. Machine learning 28, 41–75.\nChao, H., Zhi, H., Dong, L., Liu, Y., 2018.\nRecognition of emotions using multichannel eeg\ndata and dbn-gc-based ensemble deep learning framework. Computational Intelligence and\nNeuroscience 2018.\nCho, S.Y., Wong, J.J., 2008. Human face recognition by adaptive processing of tree structures\nrepresentation. Neural Computing and Applications 17, 201–215.\nChu, Y., Huang, C., Xie, X., Tan, B., Kamal, S., Xiong, X., 2018. Multilayer hybrid deep-learning\nmethod for waste classiﬁcation and recycling. Computational Intelligence and Neuroscience\n2018.\nCollobert, R., Weston, J., 2008. A uniﬁed architecture for natural language processing: Deep\nneural networks with multitask learning, in: Proceedings of the 25th international conference\non Machine learning, ACM. pp. 160–167.\nDoulamis, N., Voulodimos, A., 2016. Fast-mdl: Fast adaptive supervised training of multi-layered\ndeep learning models for consistent object tracking and classiﬁcation, in: 2016 IEEE Interna-\ntional Conference on Imaging Systems and Techniques (IST), IEEE. pp. 318–323.\nEvgeniou, T., Pontil, M., 2004. Regularized multi–task learning, in: Proceedings of the tenth\n14\nApril 17, 2019\nJournal of Experimental & Theoretical Artiﬁcial Intelligence\nmain\nACM SIGKDD international conference on Knowledge discovery and data mining, ACM. pp.\n109–117.\nGeng, Y., Zhang, G., Li, W., Gu, Y., Liang, R.Z., Liang, G., Wang, J., Wu, Y., Patil, N., Wang,\nJ.Y., 2017. A novel image tag completion method based on convolutional neural transforma-\ntion, in: International Conference on Artiﬁcial Neural Networks, Springer. pp. 539–546.\nGlorot, X., Bordes, A., Bengio, Y., 2011. Domain adaptation for large-scale sentiment classi-\nﬁcation: A deep learning approach, in: Proceedings of the 28th international conference on\nmachine learning (ICML-11), pp. 513–520.\nGui, T., Ma, C., Wang, F., Wilkins, D.E., 2016. Survey on swarm intelligence based routing pro-\ntocols for wireless sensor networks: An extensive study, in: 2016 IEEE International Conference\non Industrial Technology (ICIT), IEEE. pp. 1944–1949.\nGuo, Y., Liu, Y., Oerlemans, A., Lao, S., Wu, S., Lew, M.S., 2016. Deep learning for visual\nunderstanding: A review. Neurocomputing 187, 27–48.\nHerath, S., Ikeda, T., Ishizaki, S., Anzai, Y., Aiso, H., 1992. Analysis system for sinhalese unit\nstructure. Journal of Experimental & Theoretical Artiﬁcial Intelligence 4, 29–48.\nHu, G., Wang, K., Peng, Y., Qiu, M., Shi, J., Liu, L., 2018. Deep learning methods for underwater\ntarget feature extraction and recognition. Computational Intelligence and Neuroscience 2018.\nJabbar, A., Iqbal, S., Akhunzada, A., Abbas, Q., 2018. An improved urdu stemming algorithm\nfor text mining based on multi-step hybrid approach. Journal of Experimental & Theoretical\nArtiﬁcial Intelligence , 1–21.\nJacob, L., Vert, J.p., Bach, F.R., 2009. Clustered multi-task learning: A convex formulation, in:\nAdvances in neural information processing systems, pp. 745–752.\nJiang, W., Carter, D.R., Fu, H., Jacobson, M.G., Zipp, K.Y., Jin, J., Yang, L., 2019.\nThe\nimpact of the biomass crop assistance program on the united states forest products market:\nAn application of the global forest products model. Forests 10, 215.\nL¨angkvist, M., Karlsson, L., Loutﬁ, A., 2014. A review of unsupervised feature learning and deep\nlearning for time-series modeling. Pattern Recognition Letters 42, 11–24.\nLeCun, Y., Bengio, Y., Hinton, G., 2015. Deep learning. Nature 521, 436–444.\nLiu, G., Gao, Z., Chen, B., Fu, H., Jiang, S., Wang, L., Kou, Y., 2019. Study on threshold\nselection methods in calculation of ocean environmental design parameters. IEEE Access .\nLiu, X., Gao, J., He, X., Deng, L., Duh, K., Wang, Y.Y., 2015a. Representation learning using\nmulti-task deep neural networks for semantic classiﬁcation and information retrieval., in: HLT-\nNAACL, pp. 912–921.\nLiu, X., Wang, M., Fu, H., 2018. Visualized analysis of knowledge development in green building\nbased on bibliographic data mining. The Journal of Supercomputing , 1–17.\nLiu, Y., Chen, J., Zhan, Y.j., 2013. Local patches alignment embedding based localization for\nwireless sensor networks. Wireless personal communications 70, 373–389.\nLiu, Y., Zhan, Y.J., Chen, J., 2014. An integrated approach to sink and sensor role selection in\nwireless sensor networks: Using dynamic programming. Adhoc & Sensor Wireless Networks\n22.\nLiu, Z., Luo, P., Wang, X., Tang, X., 2015b.\nDeep learning face attributes in the wild, in:\nProceedings of International Conference on Computer Vision (ICCV).\nLyon, C., Frank, R., 1997. Using single layer networks for discrete, sequential data: an example\nfrom natural language processing. Neural Computing & Applications 5, 196–214.\nMao, W., Mu, X., Zheng, Y., Yan, G., 2014. Leave-one-out cross-validation-based model selection\nfor multi-input multi-output support vector machine. Neural Computing and Applications 24,\n441–451.\nNgiam, J., Khosla, A., Kim, M., Nam, J., Lee, H., Ng, A.Y., 2011. Multimodal deep learning, in:\nProceedings of the 28th international conference on machine learning (ICML-11), pp. 689–696.\nOwusu, E., Zhan, Y.Z., Mao, Q.R., 2014. An svm–adaboost-based face detection system. Journal\nof Experimental & Theoretical Artiﬁcial Intelligence 26, 477–491.\nRuder, S., 2017. An overview of multi-task learning in deep neural networks. arXiv preprint\narXiv:1706.05098 .\nSadouk, L., Gadi, T., Essouﬁ, E., 2018. A novel deep learning approach for recognizing stereo-\n15\nApril 17, 2019\nJournal of Experimental & Theoretical Artiﬁcial Intelligence\nmain\ntypical motor movements within and across subjects on the autism spectrum disorder. Com-\nputational Intelligence and Neuroscience 2018.\nSchmidhuber, J., 2015. Deep learning in neural networks: An overview. Neural networks 61,\n85–117.\nSeltzer, M.L., Droppo, J., 2013.\nMulti-task learning in deep neural networks for improved\nphoneme recognition, in: Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE In-\nternational Conference on, IEEE. pp. 6965–6969.\nShams, R., Mercer, R.E., 2016. Supervised classiﬁcation of spam emails with natural language\nstylometry. Neural Computing and Applications 27, 2315–2331.\nVoulodimos, A., Doulamis, N., Bebis, G., Stathaki, T., 2018a.\nRecent developments in deep\nlearning for engineering applications. Computational Intelligence and Neuroscience 2018.\nVoulodimos, A., Doulamis, N., Doulamis, A., Protopapadakis, E., 2018b.\nDeep learning for\ncomputer vision: a brief review. Computational intelligence and neuroscience 2018.\nWang, Y., Millet, B., Smith, J.L., 2014. Informing the use of vibrotactile feedback for information\ncommunication: an analysis of user performance across diﬀerent vibrotactile designs, in: Pro-\nceedings of the Human Factors and Ergonomics Society Annual Meeting, SAGE Publications\nSage CA: Los Angeles, CA. pp. 1859–1863.\nWang, Y., Millet, B., Smith, J.L., 2016. Designing wearable vibrotactile notiﬁcations for infor-\nmation communication. International Journal of Human-Computer Studies 89, 24–34.\nWong, J.J., Cho, S.Y., 2010. A face emotion tree structure representation with probabilistic\nrecursive neural network modeling. Neural Computing and Applications 19, 33–54.\nWu, Y., Zhai, H., Li, M., Cui, F., Wang, L., Patil, N., . Learning image convolutional represen-\ntations and complete tags jointly. Neural Computing and Applications , 1–12.\nXue, Y., Liao, X., Carin, L., Krishnapuram, B., 2007. Multi-task learning for classiﬁcation with\ndirichlet process priors. Journal of Machine Learning Research 8, 35–63.\nYang, P., Zhao, P., 2015. A min-max optimization framework for online graph classiﬁcation,\nin: Proceedings of the 24th ACM International on Conference on Information and Knowledge\nManagement, ACM. pp. 643–652.\nYang, P., Zhao, P., Hai, Z., Liu, W., Hoi, S.C., Li, X.L., 2016. Eﬃcient multi-class selective sam-\npling on graphs, in: Proceedings of the Thirty-Second Conference on Uncertainty in Artiﬁcial\nIntelligence, AUAI Press. pp. 805–814.\nYao, S., Chen, Z., Jia, Y., Liu, C., 2018. Cascade heterogeneous face sketch-photo synthesis via\ndual-scale markov network. Journal of Experimental & Theoretical Artiﬁcial Intelligence 30,\n217–233.\nZhang, G., Liang, G., Li, W., Fang, J., Wang, J., Geng, Y., Wang, J.Y., 2017. Learning convolu-\ntional ranking-score function by query preference regularization, in: International Conference\non Intelligent Data Engineering and Automated Learning, Springer. pp. 1–8.\nZhang, G., Liang, G., Su, F., Qu, F., Wang, J.Y., 2018a. Cross-domain attribute representation\nbased on convolutional neural network, in: International Conference on Intelligent Computing,\nSpringer. pp. 134–142.\nZhang, R., Kramer, J.S., Smith, J.D., Allen, B.N., Leeper, C.N., Li, X., Morton, L.D., Gallazzi,\nF., Ulery, B.D., 2018b. Vaccine adjuvant incorporation strategy dictates peptide amphiphile\nmicelle immunostimulatory capacity. The AAPS journal 20, 73.\nZhang, R., Leeper, C.N., Wang, X., White, T.A., Ulery, B.D., 2018c. Immunomodulatory va-\nsoactive intestinal peptide amphiphile micelles. Biomaterials science 6, 1717–1722.\nZhang, R., Morton, L.D., Smith, J.D., Gallazzi, F., White, T.A., Ulery, B.D., 2018d. Instruc-\ntive design of triblock peptide amphiphiles for structurally complex micelle fabrication. ACS\nBiomaterials Science & Engineering 4, 2330–2339.\nZhang, R., Smith, J.D., Allen, B.N., Kramer, J.S., Schauﬂinger, M., Ulery, B.D., 2018e. Pep-\ntide amphiphile micelle vaccine size and charge inﬂuence the host antibody response. ACS\nBiomaterials Science & Engineering 4, 2463–2472.\nZhang, R., Ulery, B.D., 2018. Synthetic vaccine characterization and design. Journal of Bio-\nnanoscience 12, 1–11.\nZhang, Z., Luo, P., Loy, C.C., Tang, X., 2014. Facial landmark detection by deep multi-task\n16\nApril 17, 2019\nJournal of Experimental & Theoretical Artiﬁcial Intelligence\nmain\nlearning, in: European Conference on Computer Vision, Springer. pp. 94–108.\nZhen, X., Yu, M., He, X., Li, S., 2017. Multi-target regression via robust low-rank learning. IEEE\nTransactions on Pattern Analysis and Machine Intelligence .\nZhong, Y., Sullivan, J., Li, H., 2016. Face attribute prediction using oﬀ-the-shelf cnn features,\nin: Biometrics (ICB), 2016 International Conference on, IEEE. pp. 1–7.\nZhou, D., Wang, J., Jiang, B., Li, Y., . Multiple-relations-constrained image classiﬁcation with\nlimited training samples via pareto optimization. Neural Computing and Applications , 1–22.\n17\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-04-12",
  "updated": "2019-04-12"
}