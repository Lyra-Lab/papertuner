{
  "id": "http://arxiv.org/abs/1904.07404v3",
  "title": "swTVM: Towards Optimized Tensor Code Generation for Deep Learning on Sunway Many-Core Processor",
  "authors": [
    "Mingzhen Li",
    "Changxi Liu",
    "Jianjin Liao",
    "Xuegui Zheng",
    "Hailong Yang",
    "Rujun Sun",
    "Jun Xu",
    "Lin Gan",
    "Guangwen Yang",
    "Zhongzhi Luan",
    "Depei Qian"
  ],
  "abstract": "The flourish of deep learning frameworks and hardware platforms has been\ndemanding an efficient compiler that can shield the diversity in both software\nand hardware in order to provide application portability. Among the existing\ndeep learning compilers, TVM is well known for its efficiency in code\ngeneration and optimization across diverse hardware devices. In the meanwhile,\nthe Sunway many-core processor renders itself as a competitive candidate for\nits attractive computational power in both scientific computing and deep\nlearning workloads. This paper combines the trends in these two directions.\nSpecifically, we propose swTVM that extends the original TVM to support\nahead-of-time compilation for architecture requiring cross-compilation such as\nSunway. In addition, we leverage the architecture features during the\ncompilation such as core group for massive parallelism, DMA for high bandwidth\nmemory transfer and local device memory for data locality, in order to generate\nefficient codes for deep learning workloads on Sunway. The experiment results\nshow that the codes generated by swTVM achieves 1.79x on average compared to\nthe state-of-the-art deep learning framework on Sunway, across six\nrepresentative benchmarks. This work is the first attempt from the compiler\nperspective to bridge the gap of deep learning and Sunway processor\nparticularly with productivity and efficiency in mind. We believe this work\nwill encourage more people to embrace the power of deep learning and Sunway\nmany-core processor.",
  "text": "Towards Optimized Tensor Code Generation for\nDeep Learning on Sunway Many-Core Processor\nMingzhen Li†1,2, Changxi Liu†3, Jianjin Liao1, Xuegui Zheng1, Hailong Yang∗1,2\nRujun Sun4, Jun Xu5, Lin Gan6, Guangwen Yang6, Zhongzhi Luan1 and Depei Qian1\nSchool of Computer Science and Engineering, Beihang University1, Beijing, China\nState Key Laboratory of Software Development Environment2, Beijing, China\nNational University of Singapore3, Singapore\nState Key Laboratory of Mathematical Engineering and Advanced Computing4, Wuxi, China\nScience and Technology on Special System Simulation Laboratory Beijing Simulation Center5, Beijing, China\nDepartment of Computer Science and Technology, Tsinghua University6, Beijing, China\nAbstract—The ﬂourish of deep learning frameworks and\nhardware platforms has been demanding an efﬁcient compiler\nthat can shield the diversity in both software and hardware in\norder to provide application portability. Among the existing deep\nlearning compilers, TVM is well known for its efﬁciency in code\ngeneration and optimization across diverse hardware devices. In\nthe meanwhile, the Sunway many-core processor renders itself\nas a competitive candidate for its attractive computational power\nin both scientiﬁc computing and deep learning workloads. This\npaper combines the trends in these two directions. Speciﬁcally, we\npropose swTVM that extends the original TVM to support ahead-\nof-time compilation for architecture requiring cross-compilation\nsuch as Sunway. In addition, we leverage the architecture\nfeatures during the compilation such as core group for massive\nparallelism, DMA for high bandwidth memory transfer and\nlocal device memory for data locality, in order to generate\nefﬁcient codes for deep learning workloads on Sunway. The\nexperiment results show that the codes generated by swTVM\nachieves 1.79× on average compared to the state-of-the-art\ndeep learning framework on Sunway, across six representative\nbenchmarks. This work is the ﬁrst attempt from the compiler\nperspective to bridge the gap of deep learning and Sunway\nprocessor particularly with productivity and efﬁciency in mind.\nWe believe this work will encourage more people to embrace the\npower of deep learning and Sunway many-core processor.\nIndex Terms—Sunway processor, Deep learning compiler, Code\ngeneration, Performance optimization\nI. INTRODUCTION\nCurrently, deep learning has achieved outstanding perfor-\nmance in many ﬁelds, including self-driving car [1], face\ndetection [2] and machine translation [3]. The deep learning\nframeworks such as TensorFlow [4], PyTorch [5], MxNet [6],\nand Caffe [7], provide an efﬁcient platform to support the\nresearch and development on intelligent applications. In the\nmeanwhile, emerging deep learning algorithms exhibit increas-\ning demands for massive computation power. To satisfy the\ncomputation demand, various accelerating hardwares such as\nGPU, FPGA [8] and ASIC [9] have been applied in the\ndeep learning ﬁeld. Current deep learning frameworks almost\nrely on the high performance libraries such as cuDNN [10]\n†Contributed equally.\n∗Corresponding author.\nand MKL [11], which are provided by the hardware vendors\nto accelerate the deep learning workloads. With new deep\nlearning algorithms and hardwares arising rapidly, the engi-\nneering cost for porting the algorithms to the hardwares has\nincreased dramatically. It is necessary to ﬁnd a way to deploy\nthese emerging deep learning algorithms on the underlying\nhardwares automatically and efﬁciently.\nTo address the above problem, the end-to-end compil-\ners [12]–[16] for deep learning workloads have been proposed.\nFor example, TVM [15], XLA [4], Tiramisu [16] and Tensor\nComprehension [14] are the state-of-the-art deep learning\ncompilers. Taking TVM for example, it digests deep learning\nmodels implemented using different frameworks as input, and\ngenerates efﬁcient model codes targeting various hardware\ndevices as output. Fundamentally, TVM adopts the design of\ntwo level optimization to automatically generate codes for\ndeep learning models. On graph level, it applies multiple\noptimizations to the computation graph derived from the\ndeep learning model, such as operator fusion and data layout\ntransformation. On operator level, it converts the computations\ninto the tensor operations targeting the various hardwares\nand hides the memory latency by optimizing the instruction\npipeline. Moreover, TVM can optimize the code generation\nautomatically according to the shape and data layout of the\ninput to each layer for better performance.\nMeanwhile, for its compelling computation power, Sunway\nmany-core processor serves as the basic building block of\nSunway TaihuLight supercomputer, which is the ﬁrst super-\ncomputer to achieve over 100 petaFlops in the world. The\nSunway SW26010 processor consists of four core groups\n(CG). Each CG, including a Management Processing Element\n(MPE) and 64 Computing Processing Elements (CPEs), can\nachieve 765 GFlops peak performance in double-precision.\nThe memory attached to each CG is 8GB with the bandwidth\nof 34.1GB/s. The MPE is a complete 64-bit RISC core,\ntypically used for task control and management, whereas the\nCPE is also a 64-bit RISC core but with limited functionalities,\ntypically used for computation. In addition, each CPE has a\n64KB local device memory (LDM), that is managed explicitly\nby software. The executables on Sunway are generated through\narXiv:1904.07404v3  [cs.LG]  11 Jul 2022\ncross-compilation with MPE and CPE as different compilation\ntargets. Due to the limitation of Sunway customized operating\nsystem, the dynamic linked libraries are not supported.\nTo embrace the advantage of automatic compilation and\nhigh performance for deep learning workload, it is intuitive\nto adapt TVM to Sunway processor. However, the unique\ncompilation environment and architecture features prevent a\nnaive adoption of TVM to Sunway. Firstly, TVM relies on\ndynamic link libraries to generate executables on different\nhardware devices, which is not supported on Sunway. In\naddition, its code organization fails to recognize the different\ncompilation targets for MPE and CPEs, and thus incapable\nof managing the function calls between MPE and CPEs.\nSecondly, the memory capacity of each CG on Sunway is quite\nlimited. During the deep learning computation, large memory\noccupancy is required to store the intermediate data as well\nas the weight parameters. How to allocate the memory space\nefﬁciently and leverage the unique architecture features such as\nDMA for high bandwidth data transfer is important to generate\ncode with high performance. Thirdly, each CPE within a CG\ncontains a 64KB LDM that can be used to buffer data with\nexplicit software management. How to leverage the limited\nLDM on each CPE with improved data locality is critical\nfor realizing the performance advantage of Sunway processor\nduring code generation.\nTo address the above challenges, we propose swTVM, a\ndeep learning compiler tailored for the unique compilation\nenvironment and architecture features on Sunway processor.\nIn swTVM, we provide ahead-of-time (AOT) code generation\nthat manages the function calls as well as compilation for MPE\nand CPE explicitly. In addition, we apply several optimizations\nto the tensor operations so that the architecture features such as\nDMA and LDM are better utilized during code generation. To\nthe best of our knowledge, this is the ﬁrst work to implement\nan end-to-end deep learning compiler on Sunway processor.\nSpeciﬁcally, this paper makes the following contributions:\n• We implement the ahead-of-time (AOT) code generation,\nthat produces different compilation targets for MPE and\nCPE as well as manages the function calls between\nMPE and CPE efﬁciently. In addition, we manage the\nintermediate memory space for each tensor operation\nglobally, which avoids the overhead of frequent memory\nallocation during computation.\n• We apply several optimizations to the tensor operations\nregarding the unique architecture features on Sunway.\nSpeciﬁcally, we propose a DMA control interface that\nmanipulates the DMA data transfers for each tensor\nduring computation. In addition, we design a LDM man-\nagement mechanism that buffers the tensor data as much\nas possible to reduce the latency for accessing memory.\nMoreover, the DMA instructions are automatically in-\nserted during code generation to improve the accessibility\nof the buffered data.\n• We propose swTVM that implements AOT code gener-\nation and architecture speciﬁc optimizations on top of\nTVM, which offers the high performance of Sunway pro-\ncessor to the deep learning community through automatic\ncompilation. The evaluation results show that swTVM\nachieves 1.79× speedup on average for representative\nmodels compared to the state-of-the-art deep learning\nframework.\nThe rest of this paper is organized as follows. In Section II,\nwe present the background of the deep learning compiler and\nSunway processor. Section III presents the design overview\nof swTVM. Section IV and Section V describe the details of\ncode generation in AOT mode and optimizations for tensor\noperations on Sunway. Section VI presents the evaluation\nresults of swTVM compared to swCaffe. Section VII presents\nthe related work, and section VIII concludes this paper.\nII. BACKGROUND\nA. Sunway Processor\nEach Sunway SW26010 processor has four CGs, where\neach CG contains 1 MPE and 64 CPEs. The executables\non Sunway are generated through cross-compilation on x86\nprocessor using customized compiler. Due to the limitation\nof the customized operating system on Sunway, it does not\nsupport dynamic linked libraries. Instead, the executables\nare generated with libraries statically linked. Moreover, the\ncodes running on MPE and CPEs are compiled as different\ncompilation targets (using compilation ﬂags of -host) and -\nslave, respectively).\nAs for memory hierarchy, each CPE has 16KB L1 instruc-\ntion cache and 64KB local device memory (LDM). The LDM\nis commonly used as a programmable buffer with explicit\nsoftware control. There are two ways to access main memory\non Sunway. The ﬁrst one is to use DMA, which prefers large\nand continuous data access. The other one is to use global\nload/store (Gload/Gstore) instruction, which prefers small and\nrandom data access compared to the DMA.\nTwo parallel programming models are supported on Sunway\nto exploit the massive parallelism of the CPEs, including Ope-\nnACC and Athread. OpenACC is more programmer friendly,\nwith which programmers can utilize CPEs without knowing\nabout the underlying architecture details. While with Athread,\nprogrammers can buffer the data in LDM, which provides the\nopportunity to reduce the accesses to main memory through\nexplicit control. In this paper, we generate Athread codes on\nSunway for better performance.\nAlthough the LDM of CPE sounds similar to the shared\nmemory on GPU, their design philosophies are quite differ-\nent. GPU adopts SIMT parallelism that accesses the shared\nmemory through concurrent threads within a warp. The GPU\nprogram achieves better performance if threads within a warp\naccess a continuous memory region at the same time. However,\non Sunway the CPEs access the memory and buffer the data in\nLDM independently. Therefore, without careful management,\nsevere contention on memory bandwidth would occur and\nthus degrade the performance signiﬁcantly. In addition, when\nbuffering large continuous data block to LDM, the DMA data\ntransfer can be utilized for higher memory bandwidth.\nB. Automated Compilation for Deep Learning\nThere are increasing demands of deploying emerging deep\nlearning models to various hardware devices, so that enormous\nengineering efforts are required to match the algorithms with\nthe hardware efﬁciently. Currently, the performance of the\ndeep learning models mainly depends on the computation\nlibrary, such as cuDNN and MKL provided by hardware\nvendors. However, it is unsustainable to perform labor inten-\nsive performance tuning to match various hardware as new\nalgorithms are arising rapidly. The deep learning compiler\nprovides a way to build an efﬁcient mapping between new\nalgorithms and various hardware targets, and thus improves\nthe portability of the deep learning models.\nDespite different implementation approaches adopted by\ndifferent deep learning compilers, their design philosophies\n(e.g., two level optimization) are somehow converging [17].\nTherefore, we take TVM for illustration. TVM uses the idea\nof two-level optimization, including graph level and operator\nlevel. On graph level, it converts the deep learning models\nto the computation graph, and then applies optimizations\nsuch as operator fusion and data layout transformation. On\noperator level, it optimizes the code generation targeting\nspeciﬁc hardware through loop optimization (e.g., loop tiling\nand loop unrolling). However, adapting existing deep learning\ncompiler to Sunway processors introduces several challenges\nto be addressed, due to the unique compilation environment\nand architecture features of Sunway.\nC. Challenges for DL compilation on Sunway\nThe ﬁrst challenge is that Sunway processor relies on cross-\ncompilation to generate executables and does not support\ndynamic linked libraries. It prohibits naive adaption of existing\ndeep learning compiler such as TVM to Sunway. Therefore,\ncode generation in AOT mode needs to be supported in the\ndeep learning compiler so that it can compile the executables\nwith static linked libraries. In addition, an efﬁcient code\norganization is required with AOT code generation in order to\nsupport different compilation targets as well as function calls\nfor MPE and CPEs. Moreover, the memory capacity of a CG is\nquite limited compared to the large volume of data generated\nduring the tensor operation. To avoid the overhead of frequent\nmemory allocation during computation, the memory needs to\nbe managed globally in AOT code generation.\nThe second challenge is to optimize the generated code\nregarding the unique architecture features of Sunway. Summa-\nrizing from existing researches [18]–[21] and the benchmark-\ning [22], the key to achieving high performance on Sunway\nis to 1) fully utilize the computing resources of CPEs for\nmassive parallelism, and 2) leverage the LDM of each CPE to\nalleviate the bottleneck of memory access. Therefore, when\nthe deep learning compiler optimizes the generated codes,\nthe three rules need to be followed: 1) use the DMA as\nmuch as possible when accessing main memory. The DMA\nrequires accessing large and continuous data block, which\nprovides higher memory bandwidth; 2) leverage the LDM\nto buffer as much data as possible during the computation.\nsw5cc and sw5CC compiler\nFrameworks\nCNTK    CoreML\nComputational Graph\nAOT module on Graph Level\nHigh Level Graph Rewriting\nOptimized Computational Graph\nAdd AOT \nModule to \nTVM\nManage \nMemory under \nAOT Pattern\nDecide \nCompiler \nTarget\nOptimization and Code Generation\nExecutable File\nDMA \nControl\nInterface\nLDM \nManagement \nModel\nDMA \nAuto-Insertion \nAlgorithm\nOptimized Low Level Loop Programs\nDeclarative Tensor \nExpressions\nHardwar-Aware\nOptimization Primitives\nExecute\non a CG\nExecute\non CPEs\nDeploy on\nCPEs\nSection 4\nSection 5\n●●●\nDL Model Definition\ni1\nd3\nc1\np1\n(c)\n(a)\nGraph\nLevel\nOperator\nLevel\nPool\nConv\nRelu\n…\nLayer Module\nLayer\nLibrary\n●●●\ni1\nc1\np1\nd3\nComputation Queue\nMemory Allocation Module\nPara.\nInit.\nMo.\nMem\nPool\nTmp Mem\nPool Mem.\nConv Mem.\nRelu Mem.\n…\nDMA Control\nInterface\nAuto DMA\nInsertion\n●●●\nL1\nI1\nComputation Instructions\nL*\nLoo\np\nI*\nCalculation\n●●●\nL11\nI11\nL12\nLn\nIn\nI12\nAdjust buffer size and reorder loop\nSunway Code \nGeneration\nLayer \nImplementation\nCG 2\nM\nP\nE\n8*8 \nCPE\nmeshes\nMC\nCG 0\nDeploy on\na CG\n(b)\nFig. 1: (a) The design overview of swTVM, (b) the Sunway\narchitecture and (c) the automatic code generation of deep\nlearning models on MPE and CPEs.\nThe LDM reduces the latency to access main memory; 3)\nminimize the frequency of memory access as much as possible.\nThe computation should exhibit better data locality and re-\naccessibility after each memory access.\nIn sum, implementing an end-to-end deep learning compiler\nrequires both adaptions to the compilation environment on\nSunway and optimizations targeting the architecture features\nto improve the performance of generated codes.\nIII. DESIGN OVERVIEW\nTo address the challenges described in Section II-C, we pro-\npose swTVM for the Sunway many-core processor. In swTVM,\nwe implement the AOT code generation as an extension to\nTVM, and manage the code organization for MPE and CPEs\nrespectively. In addition, we manipulate the memory allocation\nof the tensor operation globally. The grey components in\nFigure 1(a) show the contribution of our work. We produce\nC source codes in AOT mode, which are then compiled by\nSunway native compiler in order to generate the executable.\nThe advantage of AOT code generation is that the memory\nallocation for each layer is determined based on the input\nand output of each layer before the actual computation, which\navoids frequent memory allocation during the computation and\nthus eliminates the overhead of operations related to memory\nallocation.\nThe MPE codes generated in AOT mode are primarily\nresponsible for calling each layer according to the topology\nof the deep learning models, whereas the CPE codes are\nresponsible for the speciﬁc computation of operators deﬁned\nby the layers. The codes generated for a Sunway CG consist\nof three parts: layer module, memory allocation module and\nparameter initialization module. To generate the code, the\n1 Func main \n2 Begin\n3\n/*Allocate Memory*/ \n4\nDtype input[ … ]\n5\nDtype conv1P1[…]\n6\n……\n7\nDtype shareMem[…]\n8\n/* Init Net Parameter \n9\nand Input*/\n10\nInitInput(input)\n11\nInitPara(conv1P1) \n12\n…… \n13\n/*Calculate */\n14\nConv1(input1,conv1,shareMem)\n15\nPool1(conv1,pool1,shareMem)\n16\n……\n17\nDense3(dense2,output,shareMem)\n18\n/* Output */\n19\nOutput(output)\n20 End\n1 Func layer( in1,out1,ptr)\n2 Begin\n3\n/*Allocate Memory*/\n4\nDtype * tmp1 = ptr\n5\n….\n6\n/*Init */\n7\nPara para[1]\n8\ninit_struct(para,in1,out1,tmp1)\n9\n……\n10\nspawn(layer_slave,para )\n11\njoin()\n12 End\n1 typedef  struct Para{\n2\nDtype in1\n3\nDtype out1\n4\nDtype tmp1\n5\n……  } Para;\n1 Func  conv1 ( in1, out1,ptr)\nLayer\n(b) layers_mix.h\n(c) layers.c\n(d) layers.h\n(e) layers.s.c\n(a) main.c\n1 Func layer_slave(  para ) \n2 Begin     \n3\n/*Init */\n4\ndma_get(para)\n5\nDtype * in1 = para.in1\n6\nDtype * out1 = para.out1\n7\nDtype * tmp1 = para.tmp1\n8\n…….\n9\n/*Allocate Buffer Memory*/\n10\nDtype input1_buffer[…]\n11\nDtype output1_buffer[…]\n12\nDtype tmp1_buffer[…]\n13\n/*Calculate, Read \n14\nand Write Buffer*/\n15 ……\n16 End \nFig. 2: AOT code generation on Sunway processor.\nmodel deﬁnition in Figure 1(a) is transformed and stored by\nlayer in the computation queue in the upper part of Figure 1(c).\nThe layer module invokes the layer implementations from the\nlayer library, and the memory allocation module allocates\nthe memory space for each layer within the computation\nqueue. The parameter initialization module is responsible for\ninitializing the parameters of the layer implementations within\nthe layer library.\nTo leverage the architecture features on Sunway, we opti-\nmize the implementation of each operator, as shown in the\nbottom part of Figure 1(c). Speciﬁcally, we design a DMA\ncontrol interface, which provides the DMA schedule primitives\nfor the layer library. In addition, since the LDM on each CPE is\nonly 64KB which cannot store the entire tensors, we design a\nLDM management mechanism to control the amount of tensor\ndata to be buffered in LDM automatically. It can also adjust\nthe buffer size and reorder the computation loops according\nto the conﬁguration of each layer. Moreover, to improve\nthe locality of the buffered data, we design an algorithm to\ninsert DMA instructions into the appropriate locations of the\ngenerated code automatically. The code generation module\nthen generates code with Sunway syntax and provides the layer\nimplementation into the layer library, which is utilized by the\nlayer module to fulﬁll the layers in the computation queue.\nNote that, although swTVM is targeting the Sunway many-\ncore processor, the approaches are also valuable when building\nend-to-end deep learning compilers for other emerging proces-\nsors with cache-less design.\nIV. AOT CODE GENERATION\nTo implement AOT code generation, we should consider\nthe implementation of each layer and the approach to convert\nthe deep learning model topology into the function calls of\nlayers with the dependencies satisﬁed. swTVM transforms the\nmodel topology into the actual implementation on Sunway\nprocessor, as shown in Figure 2. After code generation, the\nimplementation contains a series of operations such as memory\nallocation, parameter initialization, and function calls in the\nmain function (e.g., Func main).\nSince the MPE are cores with complete functionality, the\ngenerated codes can run on MPE directly. Whereas for CPEs,\nwe need to generate separate ﬁles for compiling, as shown in\nFigure 2. We use a struct to accept multiple parameters in\nthe CPE function (Figure 2). In order to remove the depen-\ndency on the struct deﬁnition from the interface when calling\nthe layer, we encapsulate CPE functions with another interface\nthat renders the layer function calls as ordinary function calls\n(Figure 2(d)). The encapsulating interface is also useful when\nhandling the memory allocation of the intermediate data for\ncomplex layers. The encapsulated function is organized in a\nseparate ﬁle (Figure 2(d)) with MPE as its compilation target.\nThe parameters stored in the struct ﬁle is only visible to\nthe ﬁles containing the CPE function and encapsulated CPE\nfunction. We achieve the AOT code generation for each layer\nby organizing the code of each layer into the above three ﬁles\nin addition to a header ﬁle (Figure 2(c)) for the encapsulated\nCPE function.\nA. Managing Memory Allocation\nThe memory allocation on both main memory and LDM\nfor input/output data as well as temporal data of each layer\nneeds to be managed explicitly. The memory allocated for\ninput/output data includes intermediate data generated between\nlayers, and weight parameters that cannot be released or\noverwrote during computation. Once completing one layer,\neach operator stores its result into main memory and then\nused by other operators. Since this data is stored in the main\nmemory, the memory space is allocated and freed by MPE, as\nshown in Figure 2(a) (line 3-7).\nComplex operators usually generate temporal data. The data\nis never re-used and thus can be freed once the computation\ncompletes. Because the temporal data is usually larger than\nthe capacity of LDM (i.e., 64KB), it is also stored in the main\nmemory, whose allocation and deallocation are controlled in\nthe main function. When an operator is invoked, it uses a\nportion of the memory space that has already been allocated\nin the main function, which reduces the overhead for allocation\nand deallocation for each operator. The memory space for\ntemporal data is allocated by MPE and used by CPEs. The\nimplementation details are listed in Func main for MPE and\nFunc layer slave (e.g., in layers.s.c ﬁle) for CPEs in Figure 2.\nThe LDM utilization in Func Layer slave is described in\nSection V.\nB. Managing Function Call\nAs shown in Figure 1(a), the implementation of swTVM is\norganized into three levels, which ﬁrst transforms the topology\nof a deep learning model into computation graph, and then\napplies a serial of optimizations at graph level, and eventually\nimplements the computation on speciﬁc hardware at operator\nlevel. In AOT code generation, the Func main in Figure 2(a)\nis responsible for maintaining the dependency of function\ncalls in the computation graph, whereas Fun layer slave in\nFigure 2(e) implements each operator. Note that the Func layer\nin Figure 2(c) is the interface that connects Fun layer slave\nand Func main, and fulﬁlls the function call of each operator\nin the computation graph.\nIn addition, function calls for architecture speciﬁc codes\ncan also be organized into three levels, including function call\non MPE, function call on CPEs and function call from MPE\nto CPEs. These three levels correspond to the operators at\ngraph level, operator level, and from graph level to operator\nlevel in swTVM. The graph level generates Func Main, which\nruns on MPE. And the Func layer is the implementation of\nthe function call from graph level to operator level, which\nis invoked by Func main on MPE and then invokes the\nFunc layer slave on CPEs. Func layer slave implements the\ncomputation performed at operator level.\nWith such design, swTVM can organize the AOT code\ngeneration and Sunway architecture optimizations through\nlayered function calls, rather than relying on sophisticated low-\nlevel implementation details. In addition, through managing\nthe dependencies of function calls, swTVM is able to generate\ncodes for MPE and CPEs as different compilation targets.\nC. Implementation Details\nThe Func main shown in Figure 2(a) consists of four stages,\nincluding memory allocation stage, parameter/input initializa-\ntion stage, computation stage and output stage. During mem-\nory allocation stage, in addition to memory for the parameter\nand input/output of the deep learning model, temporal memory\nis also allocated for each layer, the size of which satisﬁes\nthe maximum memory usage of each layer. The dependency\nacross all layers is analyzed to decide the order of function\ncalls. Each function in the computation stage corresponds to\none or more layers in the model topology.\nThe implementation of each operator consists of Func layer\non MPE, Func layer slave on CPEs and parameter structure\nPara. Func layer is further divided into three parts, such as the\nmemory allocation for temporal space, parameter initialization,\nand computation. The memory allocation for temporal space\nis only required for the layer that combines multiple sub-\noperators such as convolution and pooling. For such layers, the\ninput of one sub-operator depends on the intermediate results\nfrom the previous sub-operator. Considering the overhead of\nfrequent memory allocation, we allocate temporal memory\nspace in the main function and share it across operators.\nThe format for calling the function on CPEs is to use the\nfunction name and parameter struct, as shown in Figure 2(c)\n(line 10). Para is the parameter struct that is only visible to\ncorresponding layer.c and layer.slave.c ﬁles. Func layer slave\nconsists of parameter parsing, LDM allocation, and computa-\ntion. At the beginning of the function, the tensors are loaded\nfrom memory and then buffered in LDM. The LDM space is\nallocated through static arrays to buffer the tensor. The main\nmemory is accessed through DMA instructions, which can be\noverlapped with the computation for efﬁciency, and the details\nare described in Section V.\nD. Invoking Optimized Kernel Libraries\nSince there are several optimized libraries available on\nSunway processor for accelerating matrix multiplication and\nconvolution computation, such as xMath, swGEMM and\nswDNN [23]. swTVM provides optional approach to easily\nintegrate these libraries for better code generation, which is\nimplemented in the following two stages. In schedule mapping\nstage, swTVM uses the intrinsic APIs to generate function calls\nto external libraries, and bypasses them to the code generation\nstage. Considering performance variation of different libraries\nacross different tensor operations, swTVM invokes the libraries\nwith optimal performance. For example, it invokes xMath and\nswGEMM for accelerating standard convolution and depthwise\nconvolution operators respectively. In code generation stage,\nswTVM identiﬁes the invoked libraries and automatically adds\nthe relevant header ﬁles and parameters to generate the canon-\nical C codes. Besides, it adds the corresponding ﬂags to the\ncompilation conﬁgurations (e.g., Makeﬁle).\nV. OPTIMIZING TENSOR OPERATION\nA. DMA Control Interface\nAn efﬁcient DMA control interface plays an important\nrole in swTVM to generate high-performance implementations\nof deep learning models on Sunway. In swTVM, the DMA\ncontrol interface provides schedule primitives to control DMA\nin order to manage the data access efﬁciently. Figure 3\nshows an example to control the tensor data access in matrix\nmultiplication through the DMA control interface. Figure 3(a)\nshows the computation deﬁnition in swTVM, and Figure 3(b)\nshows the plain IR generated by swTVM, which is the same as\noriginal TVM. The split primitive splits the loop iterator into\ntwo parts (line 8-9 of Figure 3(a)). We call the outer part as\nparallel iterator and inner part as buffer iterator. And swTVM\nuses the parallel iterators to assign computation to CPEs for\nparallelization and buffer iterators for DMA data transfer.\nswTVM can also buffer data along multiple dimensions.\nIn Figure 3(c), tensor B is buffered along two dimensions\n(line 1). This allows fast access to the value along these two\ndimensions of tensor B when calculating the sub-region of\ntensor C. Additionally, swTVM can specify which tensor to be\nbuffered and the region of the tensor to be buffered during code\ngeneration. To buffer partial of the tensor along one dimension,\nsplit, buffer read, and buffer write primitives are applied in\nsequence to split the dimension and buffer the corresponding\ndata. After invoking the above primitives, Load Data region\n(line 6-11 in Figure 3(d)) generates the IR code of the read\nbuffer for tensor B and A, whereas Store Data region (line\n15-16 in Figure 3(d)) generates the IR code of write buffer\nfor tensor C. The generated IR is then translated to DMA\ninstructions during code generation.\nBuffering data along multiple dimensions also occurs in\nconvolution operator. The convolution operation is the com-\nputation among high-dimension tensors, where certain dimen-\nsions of the tensor may be quite small If only buffering\ndata along only one dimension, the LDM space is not fully\nutilized. In such a case, buffering the tensor data along multiple\ndimensions improve the LDM utilization. When buffering, we\nsatisfy the data access of the outer loop with high priority,\nwhich improves the locality of buffered data.\n1\nFor x in range(0,1)\n2\nFor y.o in range(8)\n3\nFor y.i in range(128)\n4\nC[x,y.o*128+y.i] = 0;\n5\nFor k.o in range(16)\n6\nFor y.i in range(128)\n7\nFor k.i in range(64)\n8\nBB[k.i,y.i] =\n9\nB[k.o*64+k.i][y.o*128+y.i]\n10\nFor k.i in range(64)\n11\nAA[k.i] = A[x,k.o*64+k.i]\n12\nFor y.i in range(128)\n13\nFor k.i in range(64)\n14\nCC[y.i] += AA[k.i] * BB[k.i,y.i]\n15\nFor y.i in range(128)\n16\nC[x,y.o*128+y.i] = CC[y.i] \n1\nFor x in range(0,1)\n2\nFor y.o in range(8)\n3\nFor y.i in range(128)\n4\nC[x,y.o*64+y.i] = 0\n5\nFor k.o in range(16)\n6\nFor k.i in range(64)\n7\ndma(BB[k.i,y.i],B[k.o*64+k.i][y.o*128+y.i],128)\n8\ndma(AA[k.i], A[x][k.o*64+k.i],64)\n9\nFor y.i in range(128)\n10\nFor k.i in range(64)\n11\nCC[y.i] += AA[k.i] * BB[k.i,y.i]\n12\ndma(C[x,y.o*128+y.i] , CC[y.i] , 128)\n1 For x in range(0,1)\n2 For y.o in range(8)\n3\nFor k.o in range(16)\n4\nFor y.i in range(128)\n5\nFor k.i in range(64)\n6\nC[x,y.o*128+y.i] += \n7\nA[x,k.o*64+k.i] *\n8\nB[k.o*64+k.i,y.o*128+y.i]\nTransform\nBuffered IR\nto DMA\nInstructions\non Sunway\n1\nBB = s.buffer_read(B,  [ki,yi]  )\n2\nAA = s.buffer_read(A,  [ki]  )\n3\nCC = s.buffer_write( C, [ yi ] )\nInitialize\nBuffer\nLoad\nData\nStore\nData\nC = A * B\n1\nM=1, K=N=1024\n2\nA = tvm.placeholder((M,K), name='A’)\n3\nB = tvm.placeholder((K,N), name='B')\n4\nC = tvm.compute((M,N), lambda x,y:\n5\ntvm.sum(  A[x,k] * B[k,y]  , axis = k),\n6\nname = \"C\")\n7\ns = tvm.create_schedule(C.op)\n8\nyo,yi = s[C].split( C.op.axis[1], 128)\n9\nko,ki = s[C].split( k , 64)\n10 s[C].reorder(yo,ko,yi,ki)\nGenerate IR\nPlain\nIR\n(a)\n(b)\n(c)\n(d)\n(e)\nAutomatic\nBuffered IR\nInsertion\nFig. 3: An example of matrix multiplication implementation\ngenerated by swTVM with optimizations targeting Sunway.\nBuffer\nBuffer Direction\nResult Element\nElement Needed\nx\ny\nl1\nl2\nl1\nl2\nA\nC\nB\nk\nk\nFig. 4: Buffer size dependency\nof matrix A, B, and C within\nmatrix multiplication.\nTensor\nIter Iter\nBuf Iter\nA\nx,k\nki\nB\nk,y\nki * yi\nC\nx,y\nyi\n1\nBB = s.buffer_read(B,  [ki,yi]  )\n2\nAA = s.buffer_read(A,  [ki]  )\n3\nCC = s.buffer_write( C, [ yi ] )\nUsedLDM = ki + ki * yi + yi < LDMSize\n(ki < k, yi < y, ki,yi 𝜖𝜖N; LDMSize = 64KB)\nBuf Iter\nPossible Value\nki\n1,2,...,2i,…,256,…,  < k\nyi\n1,2,...,2i,…,256,…,  < y\n（a）\n（b）\n（c）\n（d）\nFig. 5: Procedure of calcu-\nlating the buffer size on the\nmatrix multiplication.\nIn complex layer such as convolution, the subscript to access\nthe tensor data along one dimension is determined by multiple\nloop iterators. To handle such case, the DMA control interface\naccepts multiple loop iterators and allows the user to specify\nthe expression on calculating the subscript based on these loop\niterators, which determines the range of each dimension to\nbe buffered. One such example is shown in the expression\nyy × stride + ry. The DMA control interface also supports\nexpression inferring, which accepts the subscript expression\nand analyzes the correlation between the loop iterators and\ntensor dimensions automatically.\nB. LDM Management Mechanism\nTo better control the data buffering in LDM, we design the\nLDM management mechanism, which determines the buffer\nsize and the dimensions of tensor to be buffered. In addition,\nit reorders the computation loops to improve the locality of\nthe buffered data.\n1) Determining the Buffer Size: Due to the limited LDM\nspace, the difﬁculty to determine the buffer size of each tensor\nis to identify the dependencies, which means the buffer size\nof one tensor can affect the buffer size of another. Figure 4\nshows an example of buffer size dependency within the matrix\nmultiplication (A × B = C). The dimensions of matrix A, B\nand C are (x,k), (k,y), and (x,y), respectively. When the buffer\nsize of matrix C and matrix A is l2 and l1 respectively along\nthe same dimension, the buffer size of matrix B is l1 and l2\nalong k and y dimension.\nFigure 5 shows the procedure of calculating the buffer size\non the matrix multiplication and the possible size of each\nbuffer. When determining the buffer dimension of each tensor\nusing buffer read and buffer write in Figure 5(a), swTVM\nconstructs a table that describes the buffer iterators of each\ntensor as shown in Figure 5(b). The sum of the buffer size\nfrom all buffer iterators of each tensor can be expressed in\nan equation, as shown in Figure 5(c). Then the buffer size of\neach tensor can be determined by choosing a possible value\nthat satisﬁes the above equation. We limit the possible values\nto the power of two for better performance on Sunway, as\nshown in Figure 5(d). We use a greedy algorithm to search\nfor the minimum possible value.\nConstraint 1 - When determining the buffer size of each\ntensor, the following constraints should be satisﬁed.\n• The buffer size of a buffer iterator cannot be larger than\nthe original dimension.\n• The sum of the buffer size of all tensors cannot be larger\nthan the LDM size.\n• When the buffer size in the lowest dimension is one, no\nbuffer is allocated for the tensor, which means that the\ndata is directly accessed from main memory.\nStrategy 1 - swTVM uses an approximate algorithm (Algo-\nrithm 1 in Appendix) to managing data buffer in LDM, which\nensures an acceptable search time for an optimal solution. The\nalgorithm consists of two parts, the initial part to allocate a pre-\ndeﬁned LDM memory space and expanding part to maximize\nthe LDM utilization. In Algorithm 1, the iterators can be\nclassiﬁed into three types:\n• sizeiter: determines the buffer size and is the index of the\nlowest dimension of the tensor that can be expanded;\n• numiter: determines the number of DMA instructions\nand is the index of the dimension (except the lowest\ndimension) of the tensor;\n• compiter: is the iterator that satisﬁes the conditions of\nboth sizeiter and numiter.\nAt the beginning of the algorithm, the sequence of the\niterators is reordered. For compiters, it is reordered by the\nascending order of the affected number of tensors. Whereas\nfor sizeiters, it is reordered by the ascending order of the\nbuffer size (line 9-10). After that, the buffer iterator for each\nloop iterator is initialized to a pre-deﬁned size across each\ntensor (line 12-37). The buffer size is set to the minimum\nbetween the loop range and InitValue. The number InitValue\nis chosen based on empirical study that reading InitValue ﬂoats\nper memory access achieves good bandwidth, and InitValue is\nset to 64 on Sunway (line 17-21). Then, the algorithm checks\nif the buffer size is larger than the size of LDM. If so, the\namount of data to be transferred for current buffer iterators or\neven the previous buffered iterators needs to be reduced to ﬁt\nin the limited size of LDM (line 22-31).\nDuring the initialization, the algorithm invokes the UPDATE\nfunction if the range of the buffer iterator equals to the range\nof the loop iterator. When the dimension of the tensor to\nbe buffered is no longer associated with any iterators, the\nhigher dimension needs be adjusted to change the buffer size.\nAnd the CLASSIFY function is invoked to update numiters,\nsizeiters and compiters (line 23-25, 33-36, 46-52). After the\ninitialization, if the LDM still has free space, the buffer size\nof each iterator is expanded to improve the LDM utilization.\nWe use a greedy algorithm to load as much data into LDM\nas possible. The algorithm terminates when the LDM usage\nreaches the maximum size (line 39-53).\nWe take the matrix multiplication in Figure 3 to illustrate\nthe process of the algorithm, where x, y and k is numiter,\nsizeiter, and compiter respectively. We set the buffer size of\ny to 64 and ensure our buffer size not exceeding the LDM\ncapacity. Then, we set k to 64 that leads to the LDM usage of\n16.5KB. Since there is no numiter satisfying the condition of\nUPDATE, the algorithm enters the expanding part. When y is\nset to 128, the LDM usage increases to 32.75KB. Continuing\nto expand k to 128, the buffer size reaches 65KB, which is\nlarger than the LDM capacity (64KB). Therefore, x = 1, y =\n128, and k = 64 are chosen as the buffer sizes.\n2) Loop Reordering: After initializing the buffer size for\neach tensor, the loop order is adjusted to improve the locality\nof the buffered data.\nConstraint 2 - To ensure the correctness after loop reorder-\ning, the following constraints need to be satisﬁed.\n• The buffer iterator cannot be ahead of the parallel iterator,\nboth of which are split from the same iterator;\n• The parallel iterator of the output tensor must be at the\noutermost loop to prevent write conﬂict;\n• The buffer cannot be ahead of other iterator associated\nwith the same tensor;\n• The child iterator split from the parent iterator inherits\nits parent’s order.\nStrategy 2 - Under the above constraints, we reorder\nthe loop iterators that are not associated with the tensor\nand insert the DMA instruction into the suitable location to\navoid unnecessary DMA transfers. For the conﬂicting DMA\ninstructions, the loops are reordered, and the loop order with\nthe least number of DMA instructions is chosen, as shown\nin Algorithm 2 in Appendix. First, all buffered iterators are\nmoved to the innermost loop. And then, the order of non-\nbuffered iterators are determined. The buffered iterators with\nlocations undecided are inserted to the current loop with the\nnumber of DMA instructions for all tensors evaluated. The\niterators with the least number of DMA instructions is chosen\ni\n●●●\np\n●●●\n●●●\n●●●\nIterator j,v\nremoved\nIterator k,w\nremoved\n●●●\nThe procedure of determining DMA locations from outer loop to inner loop\nDMA location determined\nIterator i removed\nLoop/parallel\nIterator\nBuffer \nIterators\n(a)\n(b)\n(c)\nempty\nAssociated iterator\nset of the tensor\nFig. 6: The illustration of DMA auto-insertion algorithm. (a)\nthe initial states of iterators, (b) iterator i to be removed and\n(c) the DMA locations are determined for the tensor.\nas the loop iterators for current loop (line 2-11). The above\nprocess is repeated until all iterators are evaluated, which\nderives the ﬁnal loop order (line 16-22). The time complexity\nof Algorithm 2 is also within polynomial time.\nWe take the matrix multiplication in Figure 3 to illustrate the\nloop reordering. The iterators for which the order to be decided\nis x, yo and ko. The least number of DMA instructions for x,\nyo and ko is 256 + 128, 256 + 16 and 256 + 8 respectively.\nTherefore, ko is chosen ﬁrst. And then, the least number of\nDMA instructions for x and yo are both 8. Therefore, the\noriginal loop order is unchanged. After that, the ﬁnal loop\norder for x, yo and ko is determined.\nC. DMA Auto-Insertion Algorithm\nWith the DMA control interface and LDM management\nmechanism available, we propose an algorithm to implement\nthe auto-insertion of DMA instructions during the code gen-\neration. The DMA auto-insertion algorithm consists of three\nparts as following.\nDetermining the buffer size and the starting memory\nlocation. First, the buffer dimension is split into two parts,\nwhich makes the range of the inner loop within the buffer\nsize. When the subscript of the dimension is correlated with\nonly one loop iterator, the starting memory location of the\nbuffer is calculated by setting the loop iterator of the inner\nloop to 0, whereas the buffer size is the range of the inner\nloop. All the buffer operations in Figure 3(c) belong to the\nabove case. However, for complex operators such as stride\nconvolution, the subscript of one dimension of the tensor is\nalways correlated with several loop iterators. To obtain the\nstarting memory location of the buffer, all loop iterators are\nset to zero and calculated in the subscript expression. The\nsize of the buffer is the difference between the result of the\nsubscript expression with all iterators set to their maximum\nvalue and the starting memory location.\nDetermining the locations of DMA instruction insertion.\nFigure 6 illustrates the process of determining the DMA\ninsertion location for a tensor. At the beginning, we have\nthe iterators which are associated with the tensor. Figure 6(a)\nshows the initial states of the iterators. The tensor has loop\niterators such as v, j and i, and buffer iterators such as k and\nw. Then we iterate through the outer loops to inner loops. If\nthe iterator of the current loop is not within the associated\niterator set of the tensor, then the algorithm proceeds to the\nnext loop. If the current iterator belongs to the set but does\nnot belong to the buffer iterators, then the iterator is removed\nfrom the associated iterator set, which indicates the iterator is\ndetermined. In Figure 6(b), the iterator i is removed from the\nassociated iterator set since it satisﬁes the above condition.\nWhen all iterators except the buffer iterators are removed\nfrom the associated iterator set, as shown in Figure 6(c),\nthe locations to insert DMA instructions are determined. The\nabove procedure is repeated for all tensors to determine the\nlocations of DMA instruction insertion correspondingly.\nGenerating code with Sunway DMA syntax. Figure 3(e)\nshows the pseudo-code of the inserted DMA instructions.\nWhen generating the code, the DMA instructions whose\nmemory address and LDM buffer address are continuous, are\ncombined to reduce the number of DMA instructions.\nD. Parallelism\nTo achieve better parallel efﬁciency with CPEs, the load\nbalance and write conﬂict need to be considered when gener-\nating codes. The load balance can be achieved by using the\nathread parallel primitive, which splits computation task into\nsub-tasks along the highest dimension of the tensor. Take the\nvector multiplication (v = v1 × v2) with parallel implemen-\ntation as an example. For the vector v with dimension size\nof 1,024, we divide its dimensions into CoreNum chunks. As\nCoreNum on Sunway is 64, the size of each chunk is 16.\nThe begin and end indicates the range of sub-tasks for each\nCPE, which is determined by the id of CPE and the number of\nthe tasks. The less optimal case happens when the size along\nthe high dimension of the tensor is less than the number of\nCPEs. Such a case can be solved by using the fuse primitive\nto combine multiple dimensions until the size is large enough.\nAnd the write conﬂict can be avoided by splitting the tasks\nalong the dimension of the tensor to be written.\nVI. EVALUATION\nA. Experiment Setup\nIn this section, we evaluate the performance of the codes\ngenerated by swTVM on a CG of Sunway processor. We\ncompare swTVM with swCaffe [24], which is the deep learn-\ning framework customized for Sunway. We present the end-\nto-end performance and the operator-level performance to\ndemonstrate the efﬁciency of swTVM, and we provide the\nrooﬂine model analysis to better understand the generated\ncodes. Besides, we show the compilation overhead of swTVM.\nFor benchmarks, we select eight representative deep learn-\ning models that are the widely-used in inference tasks, as\nshown in Table I. Notably, swCaffe fails to execute ShufﬂeNet\nand Bert-base due to unsupported layers (e.g., permute, lay-\nernorm, and embedding). While swTVM supports them and\ngenerates high-performance codes for them, demonstrating its\nportability. Each model under each batch size is executed for\n100 times and the average execution time is reported.\nswTVM performs the compilation on the x86 platform. The\ncompilation environment includes gcc/g++ 4.8.5 and Python\nTABLE I: Deep learning models in experiments.\nModel\nTask\nBatch Size (bs)\nInput Size\nResNet18\nImage Classiﬁcation\n1, 2, 4, 8\n(bs,3,224,224)\nResNet50\nImage Classiﬁcation\n1, 2, 4, 8\n(bs,3,224,224)\nVGG16\nImage Classiﬁcation\n1, 2, 4, 8\n(bs,3,224,224)\nYOLOv3\nObject Detection\n1, 2 ,4 ,8\n(bs,3,416,416)\nDCGAN\nImage Classiﬁcation\n1, 2, 4, 8\n(bs,100,1,1)\nMobileNet\nImage Classiﬁcation\n1, 2, 4, 8\n(bs,3,224,224)\nShufﬂeNet\nImage Classiﬁcation\n1, 2, 4, 8\n(bs,3,224,224)\nBert-base\nQuestion Answering\n1, 2, 4, 8\n(bs,seqlen=16)\n3.6.2. The codes generated by swTVM is a group of C/C++\nﬁles, which are then compiled by Sunway native compilers\n(sw5cc for C and sw5CC for C++) with -O3. swCaffe is\nconﬁgured with the recommended high-performance libraries,\nincluding swDNN and xMath. And all optimization macros\n(e.g., ofﬂoading im2col, relu, pooling, batchnorm operators to\nswDNN) are enabled.\nB. End-to-End Performance\nThe end-to-end performance of swTVM across all bench-\nmarks is shown in Figure 7. Notably, swTVM is conﬁgured\nwith two conﬁgurations of graph-level optimizations: OPT=1\nenables the basic operator fusion, and OPT=4 enables all\nbuilt-in optimization passes of TVM. swTVM under both\nconﬁgurations outperforms swCaffe in nearly all benchmarks.\nSpeciﬁcally, the average speedups of swTVM (OPT=1) com-\npared the swCaffe baseline under the four batch sizes (i.e., 1,\n2, 4, 8) are 1.71×, 1.61×, 1.56×, and 1.55×, respectively.\nAnd the average speedups of swTVM (OPT=4) are 1.79×,\n1.66×, 1.62×, and 1.61×. This is because swTVM exploits\nthe graph-level optimizations standing on the basis of TVM,\nwhile swCaffe ignores them. For example, the operator fusion\ncan reduce the number of kernel launches on CPEs, eliminate\nthe corresponding DMA transfer between LDM and main\nmemory, and allow better sharing of the computation. With\nmore graph-level optimizations enabled, the performance of\nswTVM (OPT=4) is better than that of swTVM (OPT=1).\nThe acceleration from memory-intensive operators (e.g.,\nbatch norm) dominates the performance improvement of\nswTVM. Among the benchmarks, YOLOv3 has the largest\nbatch norm operators (with the largest input size), and thus\nit has higher speedup ration. With the increasing batch sizes,\nthe speedup of swTVM decreases slightly, because of the\ninefﬁcient batch norm implementation of swCaffe baseline.\nswCaffe implements batch norm through matrix multiplication\nwhich shows non-trivial overhead, therefore, the computation\ntime remains nearly constant even doubling the batch size.\nNotably, as for MobileNet, the depthwise conv2d operators\ndominate over 95% of the computation time and are also\nhighly-optimized by swTVM. Consequently, on MobileNet,\nswTVM achieves the maximum speedup of 2.79×, which is\nquite stable even with different batch sizes.\nC. Operator-Level Performance\nIn order to evaluate the effectiveness of swTVM, we\nfurther perform operator-level performance comparison. We\nResNet18\nResNet50\nVGG16\nYOLOv3\nDCGAN\nMobileNet\n0.0\n1.0\n2.0\n3.0\nSpeedup\n1.46\n1.30\n1.20\n2.58\n0.99\n2.76\n1.59\n1.48\n1.26\n2.58\n1.01\n2.79\nswCaffe\nswTVM (OPT=1)\nswTVM (OPT=4)\n(a) batch size = 1\nResNet18\nResNet50\nVGG16\nYOLOv3\nDCGAN\nMobileNet\n0.0\n1.0\n2.0\n3.0\nSpeedup\n1.26\n1.22\n1.26\n2.16\n1.03\n2.72\n1.39\n1.38\n1.32\n2.14\n1.00\n2.76\nswCaffe\nswTVM (OPT=1)\nswTVM (OPT=4)\n(b) batch size = 2\nResNet18\nResNet50\nVGG16\nYOLOv3\nDCGAN\nMobileNet\n0.0\n1.0\n2.0\n3.0\nSpeedup\n1.22\n1.20\n1.28\n1.98\n0.98\n2.72\n1.31\n1.34\n1.36\n1.97\n1.00\n2.75\nswCaffe\nswTVM (OPT=1)\nswTVM (OPT=4)\n(c) batch size = 4\nResNet18\nResNet50\nVGG16\nYOLOv3\nDCGAN\nMobileNet\n0.0\n1.0\n2.0\n3.0\nSpeedup\n1.20\n1.20\n1.30\n1.92\n0.98\n2.73\n1.28\n1.32\n1.38\n1.92\n1.04\n2.74\nswCaffe\nswTVM (OPT=1)\nswTVM (OPT=4)\n(d) batch size = 8\nFig. 7: End-to-end performance of swTVM with two conﬁguration of graph-level optimization, OPT=1 and OPT=4. The y-axis\nrepresents the speedup compared to swCaffe.\nclassify the operator into three categories, including convo-\nlution, dense, and memory-intensive operators. The exper-\niment results is shown in Figure 8. Since YOLOv3 and\nDCGAN has no dense operator, the corresponding bars are\nleft blank. On the three categories, swTVM achieves 1.36×,\n1.29×, and 11.36× speedups on average compared to swCaffe,\nrespectively. Among them, swTVM achieves the maximum\nspeedup on memory-intensive operators, which mainly contain\nbatch norm, relu, pooling, bias add operators. These operators\nbeneﬁt a lot from the operator fusion, which fuses multiple\nsmall operators together to avoid redundant DMA transfers\nbetween LDM and main memory. The batch norm operator\nin swCaffe is implemented as a batchnorm layer (applying the\nmean and the variance) and a scale layer (scaling and then\nshifting, i.e., ax + b). swTVM fuses mean/variance applying\nand scaling to the preceding convolution operator and also fuse\nthe shifting to the next relu operator, leading to superior perfor-\nmance. As for convolution and dense operators, both swTVM\nand swCaffe can leverage the optimized kernel libraries such\nas swDNN, xMath, etc., so their performance could be similar.\nIf these operators are conﬁgured with bias (e.g., all dense\noperators, convolution operators of VGG16), swTVM regards\nthe bias computation as memory-intensive operators while\nswCaffe regards them as part of convolution/dense operators.\nAs a result, the speedup of swTVM on these operators are\nslightly better, and the speedup on memory-intensive operators\nmay decrease slightly.\nBesides, the convolution operators from MobileNet opti-\nmized by swTVM show 2.74× speedup on average. These\noperators are depthwise convolutions, and each is transformed\nResNet18\nResNet50\nVGG16\nYOLOv3\nDCGAN\nMobileNet\n0.0\n1.0\n2.0\n3.0\nSpeedup\n1.02\n1.00\n1.43\n0.99\n0.96\n2.74\n1.56\n1.20\n1.00\n1.40\n13.42\n9.43\n0.82\n20.25\n6.82\n17.45\nConv\nDense\nMemory-intensive\nFig. 8: Performance of convolution, dense, and memory-\nintensive layers of swTVM compared to swCaffe, when batch\nsize is set to 1.\ninto im2col and tall-skinny matrix multiplication with param-\neters M, N, K, where M is 1, N represents the feature map\nsize, K is the kernel size. In this scenario, swTVM invokes\nthe optimal swGEMM library rather than xMath and achieves\nsuperior performance to swCaffe. Although the memory-\nintensive operators from DCGAN has 6.82× speedup, the\noverall speedup of DCGAN is still negligible, as shown in\nFigure 7. It is because the memory-intensive operators is\nnot the performance bottleneck, which contribute to less than\n2% of the end-to-end inference time. Similarly, the memory-\nintensive operators from VGG16 contribute to less than 5%.\nD. Rooﬂine Analysis\nWe further perform the rooﬂine analysis to study the effec-\ntiveness of the codes generated by swTVM. Figure 9 presents\nthe experiment results of the overall model inference across\n10\n1\n10\n0\n10\n1\n10\n2\nOperational Intensity (Flops/Byte, log scale)\n10\n0\n10\n1\n10\n2\n10\n3\nPerformance (GFlops, log scale)\n34.0\nMemory Bound\nCompute Bound\nTheoretical Peak 765.6 GFlops\nPeak Stream Bandwidth 22.5GB/s\nResNet18\nResNet50\nVGG16\nYOLOv3\nDCGAN\nMobileNet\nShuffleNet\nBert-base\nConv\nDense\nFig. 9: Rooﬂine analysis. All benchmarks under the batch sizes\nof 1, 2, 4, and 8 are included.\nResNet18\nResNet50\nVGG16\nYOLOv3\nDCGAN\nMobileNet\nShuffleNet\nBert-base\n0\n20\n40\n60\n80\nTime(s)\n20.61\n23.86\n47.57\n46.36\n14.01\n26.26\n58.98\n61.93\n12.84\n8.46\n47.78\n38.77\n51.38\n50.34\n43.88\n34.48\nTVM-codegen\nswTVM-codegen\nswTVM-make\nFig. 10: Compilation overhead of swTVM on Sunway proces-\nsor, comparing to that of TVM on x86 CPU.\nall benchmarks, as well as the results of the convolution\nand dense operators. Only the lightweight models, MobileNet\nand ShufﬂeNet, lie on the left of the ridge point. They have\nlow operational intensity since they are designed for low-\npower edge devices. Most benchmarks lie on the right of the\nridge point due to the high operational intensity and achieve\nbetter performance, because swTVM generates efﬁcient codes\nfor the convolution, dense, and memory-intensive operators.\nSpeciﬁcally, the convolution operators optimized by swTVM\nreach 419.83 GFlops, more than half of the peak performance\nof a CG.\nE. Compilation Overhead\nThe compilation overhead of swTVM can be attributed to\ntwo parts. The ﬁrst part (codegen) is the AOT generation of\noptimized C/C++ codes and corresponding makeﬁle, whereas\nthe second part (make) is the compilation through the native\nC/C++ compiler of Sunway. Figure 10 presents the breakdown\nof the compilation overhead of swTVM and the compilation\noverhead of TVM on x86 CPU. It is obvious that their total\ncompilation overhead are comparable. The codegen time of\nswTVM is much lower than TVM, whereas the make time is\ndetermined by the native compilers on Sunway.\nVII. RELATED WORK\nA. Deep Learning Compiler\nCurrently, the deep learning community develops rapidly.\nThere are always emerging deep learning models and hardware\ndevices. However, the engineering efforts of porting various\nmodels to numerous hardware devices increase dramatically.\nUnder this background, the end-to-end deep learning compilers\nare proposed. XLA [4] from Google focuses on the high-level\ncomputation graph, and it can fuse those subgraphs together to\ngenerate efﬁcient code. DLVM [25] is similar to TensorFlow\nXLA, which focuses on the high-level, but it promotes using\nlinear algebra instead of the computation graph to express\nthe higher-level of the models. As they pay less attention to\nthe hardware level, signiﬁcant engineering effort is needed for\neach hardware and operation combination. TVM [15] proposes\nthe end-to-end compiler for neural networks and now supports\nvarious hardware. Recent works such as Glow [12], Tensor\nComprehensions [14] and nGraph [13] can all be classiﬁed into\nthis category. Glow lays emphasis on its two-phase strongly-\ntyped intermediate representation and nGraph pays more atten-\ntion to how to simplify the connection between deep learning\nframeworks and hardware. Tensor Comprehensions provides a\nlanguage similar to math to describe the neural network and\nsupports optimizing the computational kernel according to the\nparameter of neural networks in JIT mechanism. There are also\na few emerging tensor compilers optimizing the bottleneck\noperators [26]–[29]. However, they all lack the support of\nSunway many-core processors.\nB. Performance Optimization on Sunway\nAs a supercomputer consisting of massive Sunway many-\ncores processors, Sunway TaihuLight achieved the peak per-\nformance of 125PFlops and ranked the ﬁrst place in Top500\nfrom 2016 to 2018. There are a lot of optimization works tar-\ngeting the architecture features on Sunway, which are valuable\nfor our work to generate high performance code.\nFor applications, molecular dynamics [30], earthquake sim-\nulation [31], and atmospheric dynamics [32] won the Gordon\nBell Prize of ACM. For algorithms, there are plenty of algo-\nrithms optimized on Sunway such as BFS [18], SpMV [19],\nSpTRSV [20], [21], and Cholesky factorization [33]. BFS is\nan essential algorithm in calculating the shortest route and\nthe maximum ﬂow problem, and the optimization on Sun-\nway achieves 23,755 giga-traversed edges per second. Sparse\ncomputation such as SpMV is one of the important computa-\ntional kernels in scientiﬁc applications. The implementation\nof SpMV on Sunway achieves 15.5× speedup on average\nover 18 representative datasets. There are also two related\nworks regarding the deep learning on Sunway. swDNN [23]\nis a neural network library customized for Sunway with\ntremendous engineering efforts. swCaffe [24] proposes a deep\nlearning framework for distributed training on Sunway.\nTo the best of our knowledge, there is no existing work\non the end-to-end deep learning compiler that exploits the\narchitecture advantage of Sunway processor.\nVIII. CONCLUSION\nWe propose a deep learning compiler, swTVM, for Sunway\nprocessor. swTVM adopts AOT code generation to address the\nunique compilation environment on Sunway, and leverages\nseveral architecture features during code generation so that\nthe computing capability of Sunway can be better utilized.\nSpeciﬁcally, a DMA control interface is proposed to manipu-\nlate the data access of the tensor better. A LDM management\nmechanism is designed to buffer data in LDM in order to\nreduce the memory access latency. Moreover, a DMA auto-\ninsertion algorithm is proposed to identify the locations for\ninserting DMA instructions automatically with improved data\nre-use. In brief, swTVM bridges the gap of deep learning and\nSunway processor with improved productivity and efﬁciency.\nREFERENCES\n[1] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp,\nP. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang et al., “End\nto end learning for self-driving cars,” arXiv preprint arXiv:1604.07316,\n2016.\n[2] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao, “Joint face detection and\nalignment using multitask cascaded convolutional networks,” IEEE\nSignal Processing Letters, vol. 23, no. 10, pp. 1499–1503, 2016.\n[3] K. Cho, B. Van Merri¨enboer, C. Gulcehre, D. Bahdanau, F. Bougares,\nH. Schwenk, and Y. Bengio, “Learning phrase representations using\nrnn encoder-decoder for statistical machine translation,” arXiv preprint\narXiv:1406.1078, 2014.\n[4] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\nS. Ghemawat, G. Irving, M. Isard et al., “Tensorﬂow: a system for large-\nscale machine learning.” in OSDI, vol. 16, 2016, pp. 265–283.\n[5] N. Ketkar, “Introduction to pytorch,” in Deep Learning with Python.\nSpringer, 2017, pp. 195–208.\n[6] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu,\nC. Zhang, and Z. Zhang, “Mxnet: A ﬂexible and efﬁcient machine\nlearning library for heterogeneous distributed systems,” arXiv preprint\narXiv:1512.01274, 2015.\n[7] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\nS. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture for\nfast feature embedding,” in Proceedings of the 22nd ACM international\nconference on Multimedia.\nACM, 2014, pp. 675–678.\n[8] C. Wang, L. Gong, Q. Yu, X. Li, Y. Xie, and X. Zhou, “Dlau: A scalable\ndeep learning accelerator unit on fpga,” IEEE Transactions on Computer-\nAided Design of Integrated Circuits and Systems, vol. 36, no. 3, pp.\n513–517, 2016.\n[9] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa,\nS. Bates, S. Bhatia, N. Boden, A. Borchers et al., “In-datacenter\nperformance analysis of a tensor processing unit,” in Proceedings of the\n44th annual international symposium on computer architecture, 2017,\npp. 1–12.\n[10] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran, B. Catanzaro,\nand E. Shelhamer, “cudnn: Efﬁcient primitives for deep learning,” arXiv\npreprint arXiv:1410.0759, 2014.\n[11] E. Wang, Q. Zhang, B. Shen, G. Zhang, X. Lu, Q. Wu, and Y. Wang,\n“Intel math kernel library,” in High-Performance Computing on the\nIntel® Xeon Phi™.\nSpringer, 2014, pp. 167–188.\n[12] N. Rotem, J. Fix, S. Abdulrasool, S. Deng, R. Dzhabarov, J. Hegeman,\nR. Levenstein, B. Maher, S. Nadathur, J. Olesen et al., “Glow: Graph\nlowering compiler techniques for neural networks,” arXiv preprint\narXiv:1805.00907, 2018.\n[13] S. Cyphers, A. K. Bansal, A. Bhiwandiwalla, J. Bobba, M. Brookhart,\nA. Chakraborty, W. Constable, C. Convey, L. Cook, O. Kanawi et al.,\n“Intel ngraph: An intermediate representation, compiler, and executor\nfor deep learning,” arXiv preprint arXiv:1801.08058, 2018.\n[14] N. Vasilache, O. Zinenko, T. Theodoridis, P. Goyal, Z. DeVito, W. S.\nMoses, S. Verdoolaege, A. Adams, and A. Cohen, “Tensor comprehen-\nsions: Framework-agnostic high-performance machine learning abstrac-\ntions,” arXiv preprint arXiv:1802.04730, 2018.\n[15] T. Chen, T. Moreau, Z. Jiang, L. Zheng, E. Yan, H. Shen, M. Cowan,\nL. Wang, Y. Hu, L. Ceze et al., “Tvm: An automated end-to-end\noptimizing compiler for deep learning,” in 13th USENIX Symposium\non Operating Systems Design and Implementation (OSDI 18), 2018, pp.\n578–594.\n[16] R. Baghdadi, J. Ray, M. B. Romdhane, E. Del Sozzo, A. Akkas,\nY. Zhang, P. Suriana, S. Kamil, and S. Amarasinghe, “Tiramisu: A poly-\nhedral compiler for expressing fast and portable code,” in Proceedings\nof the 2019 IEEE/ACM International Symposium on Code Generation\nand Optimization, ser. CGO 2019.\nIEEE Press, 2019, p. 193–205.\n[17] M. Li, Y. Liu, X. Liu, Q. Sun, X. You, H. Yang, Z. Luan, L. Gan,\nG. Yang, and D. Qian, “The deep learning compiler: A comprehensive\nsurvey,” IEEE Transactions on Parallel and Distributed Systems, vol. 32,\nno. 3, pp. 708–727, 2021.\n[18] H. Lin, X. Tang, B. Yu, Y. Zhuo, W. Chen, J. Zhai, W. Yin, and\nW. Zheng, “Scalable graph traversal on sunway taihulight with ten mil-\nlion cores,” in Parallel and Distributed Processing Symposium (IPDPS),\n2017 IEEE International.\nIEEE, 2017, pp. 635–645.\n[19] C. Liu, B. Xie, X. Liu, W. Xue, H. Yang, and X. Liu, “Towards efﬁcient\nspmv on sunway manycore architectures,” in Proceedings of the 2018\nInternational Conference on Supercomputing.\nACM, 2018, pp. 363–\n373.\n[20] M. Li, Y. Liu, H. Yang, Z. Luan, and D. Qian, “Multi-role sptrsv\non sunway many-core architecture,” in 2018 IEEE 20th International\nConference on High Performance Computing and Communications;\nIEEE 16th International Conference on Smart City; IEEE 4th Interna-\ntional Conference on Data Science and Systems (HPCC/SmartCity/DSS).\nIEEE, 2018, pp. 594–601.\n[21] X. Wang, W. Liu, W. Xue, and L. Wu, “Swsptrsv: A fast sparse\ntriangular solve with sparse level tile layout on sunway architectures,”\nin Proceedings of the 23rd ACM SIGPLAN Symposium on Principles\nand Practice of Parallel Programming, ser. PPoPP ’18.\nNew York,\nNY, USA: Association for Computing Machinery, 2018, p. 338–353.\n[Online]. Available: https://doi.org/10.1145/3178487.3178513\n[22] Z. Xu, J. Lin, and S. Matsuoka, “Benchmarking sw26010 many-\ncore processor,” in 2017 IEEE International Parallel and Distributed\nProcessing Symposium Workshops (IPDPSW), 2017, pp. 743–752.\n[23] J. Fang, H. Fu, W. Zhao, B. Chen, W. Zheng, and G. Yang, “swdnn: A\nlibrary for accelerating deep learning applications on sunway taihulight,”\nin Parallel and Distributed Processing Symposium (IPDPS), 2017 IEEE\nInternational.\nIEEE, 2017, pp. 615–624.\n[24] L. Li, J. Fang, H. Fu, J. Jiang, W. Zhao, C. He, X. You, and G. Yang,\n“swcaffe: A parallel framework for accelerating deep learning applica-\ntions on sunway taihulight,” in 2018 IEEE International Conference on\nCluster Computing (CLUSTER).\nIEEE, 2018, pp. 413–422.\n[25] R. Wei, L. Schwartz, and V. Adve, “Dlvm: A modern compiler infras-\ntructure for deep learning systems,” arXiv preprint arXiv:1711.03016,\n2017.\n[26] J. Zhao, B. Li, W. Nie, Z. Geng, R. Zhang, X. Gao, B. Cheng,\nC. Wu, Y. Cheng, Z. Li, P. Di, K. Zhang, and X. Jin, “Akg: Automatic\nkernel\ngeneration\nfor\nneural\nprocessing\nunits\nusing\npolyhedral\ntransformations,”\nin\nProceedings\nof\nthe\n42nd\nACM\nSIGPLAN\nInternational Conference on Programming Language Design and\nImplementation, ser. PLDI 2021.\nNew York, NY, USA: Association\nfor Computing Machinery, 2021, p. 1233–1248. [Online]. Available:\nhttps://doi.org/10.1145/3453483.3454106\n[27] K. Zhu, W. Zhao, Z. Zheng, T. Guo, P. Zhao, J. Bai, J. Yang,\nX. Liu, L. Diao, and W. Lin, “Disc: A dynamic shape compiler for\nmachine learning workloads,” in Proceedings of the 1st Workshop on\nMachine Learning and Systems, ser. EuroMLSys ’21.\nNew York, NY,\nUSA: Association for Computing Machinery, 2021, p. 89–95. [Online].\nAvailable: https://doi.org/10.1145/3437984.3458838\n[28] Z. Jia, O. Padon, J. Thomas, T. Warszawski, M. Zaharia, and A. Aiken,\n“Taso: optimizing deep learning computation with automatic generation\nof graph substitutions,” in Proceedings of the 27th ACM Symposium on\nOperating Systems Principles, 2019, pp. 47–62.\n[29] H. Wang, J. Zhai, M. Gao, Z. Ma, S. Tang, L. Zheng, Y. Li, K. Rong,\nY. Chen, and Z. Jia, “{PET}: Optimizing tensor programs with partially\nequivalent transformations and automated corrections,” in 15th USENIX\nSymposium on Operating Systems Design and Implementation (OSDI\n21), 2021, pp. 37–54.\n[30] J. Zhang, C. Zhou, Y. Wang, L. Ju, Q. Du, X. Chi, D. Xu, D. Chen,\nY. Liu, and Z. Liu, “Extreme-scale phase ﬁeld simulations of coarsening\ndynamics on the sunway taihulight supercomputer,” in Proceedings\nof the International Conference for High Performance Computing,\nNetworking, Storage and Analysis.\nIEEE Press, 2016, p. 4.\n[31] H. Fu, C. He, B. Chen, Z. Yin, Z. Zhang, W. Zhang, T. Zhang, W. Xue,\nW. Liu, W. Yin et al., “18.9-pﬂops nonlinear earthquake simulation on\nsunway taihulight: Enabling depiction of 18-hz and 8-meter scenarios,”\nin Proceedings of the International Conference for High Performance\nComputing, Networking, Storage and Analysis.\nACM, 2017, p. 2.\n[32] C. Yang, W. Xue, H. Fu, H. You, X. Wang, Y. Ao, F. Liu, L. Gan,\nP. Xu, L. Wang, G. Yang, and W. Zheng, “10m-core scalable fully-\nimplicit solver for nonhydrostatic atmospheric dynamics,” in SC ’16:\nProceedings of the International Conference for High Performance\nComputing, Networking, Storage and Analysis, 2016, pp. 57–68.\n[33] M. Li, Y. Liu, H. Yang, Z. Luan, L. Gan, G. Yang, and D. Qian, “Accel-\nerating sparse cholesky factorization on sunway manycore architecture,”\nIEEE Transactions on Parallel and Distributed Systems, vol. 31, no. 7,\npp. 1636–1650, 2020.\nAPPENDIX\nAlgorithm 1 LDM management algorithm.\n1: function LDMMANAGEMENT(itervars, tensorset)\n2:\n/*Classify itervars to sizeiters, numiters and compiters*/\n3:\nInitV alue = 64\n4:\n{sizeiters, numiters, compiters} ←\n5:\nCLASSIFY (itervars, tensorset)\n6:\nfor iter ∈itervar do\n7:\nBuffer(iter) ←1\n8:\nend for\n9:\nSort(compiters)\n10:\nSort(sizeiters)\n11:\nIters = {sizeiters, compiters}\n12:\n/* initial buffer size */\n13:\nwhile Iters ̸= {} do\n14:\nsizeiters ←{}; compiters ←{}\n15:\nfor i ←0, LEN(Iters) do\n16:\niter ←Iters(i)\n17:\nif range(iter) < InitV alue then\n18:\nBuffer(iter) ←range(iter)\n19:\nUPDATE(itervars, tensorSet, iter, UP)\n20:\nelse Buffer(iter) ←InitV alue\n21:\nend if\n22:\nwhile dma use > dma size do\n23:\nif Buffer(iter) == range(iter) then\n24:\nUPDATE(itervars, tensorSet, iter, DOWN)\n25:\nend if\n26:\nBuffer(iter) ←Buffer(iter)/2\n27:\nif Buffer(iter) == 0 then\n28:\nBuffer(iter) ←1; i ←i −2\n29:\nBreak\n30:\nend if\n31:\nend while\n32:\nend for\n33:\nsizeiters, numiters, compiters ←\n34:\nCLASSIFY (itervars, tensorset)\n35:\nSort(compiters); Sort(sizeiters)\n36:\nIters = {sizeiters, compiters}\n37:\nend while\n38:\n/* expand buffer size */\n39:\nwhile True do\n40:\niter = select(Iters)\n41:\nBuffer(iter) ←Buffer(iter) ∗2\n42:\nif dma use > dma size then\n43:\nBuffer(iter) ←Buffer(iter)/2\n44:\nBreak\n45:\nend if\n46:\nif Buffer(iter) == range(iter) then\n47:\nUPDATE(itervars, tensorSet, iter, UP)\n48:\n{sizeiters, numiters, compiters} ←\n49:\nCLASSIFY (iteriters, tensorset)\n50:\nSort(compiters); Sort(sizeiters)\n51:\nIters = {sizeiters, compiters}\n52:\nend if\n53:\nend while\n54: end function\nAlgorithm 2 Loop reordering algorithm.\n1: /*select the iter which requires the least number of DMA transfers*/\n2: function SELECT(Iters)\n3:\ncur iter ←NULL; cur dmatimes ←INTMAX\n4:\nfor iterid ←0, LEN(Iters) do\n5:\niter ←Iters(iterid); dmatimes ←count(iter)\n6:\nif cur dmatimes<dmatimes then\n7:\ncur iter ←iter; cur dmatimes ←dmatimes\n8:\nend if\n9:\nend for\n10:\nreturn cur iter\n11: end function\n12: function REORDERLOOP(bufferiters, iters)\n13:\niterorder ←[ ]\n14:\n/* Classify itervars to bufferiters and iters */\n15:\niterorder.add(bufferiters)\n16:\nwhile true do\n17:\niter ←SELECT(iters)\n18:\nif iter ̸= NULL then\n19:\niterorder.add(iter); iters.rm(iter)\n20:\nelse break\n21:\nend if\n22:\nend while\n23:\nreturn iterorder\n24: end function\n",
  "categories": [
    "cs.LG",
    "cs.PL",
    "stat.ML"
  ],
  "published": "2019-04-16",
  "updated": "2022-07-11"
}