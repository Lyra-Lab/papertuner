{
  "id": "http://arxiv.org/abs/1904.07615v2",
  "title": "Total Denoising: Unsupervised Learning of 3D Point Cloud Cleaning",
  "authors": [
    "Pedro Hermosilla",
    "Tobias Ritschel",
    "Timo Ropinski"
  ],
  "abstract": "We show that denoising of 3D point clouds can be learned unsupervised,\ndirectly from noisy 3D point cloud data only. This is achieved by extending\nrecent ideas from learning of unsupervised image denoisers to unstructured 3D\npoint clouds. Unsupervised image denoisers operate under the assumption that a\nnoisy pixel observation is a random realization of a distribution around a\nclean pixel value, which allows appropriate learning on this distribution to\neventually converge to the correct value. Regrettably, this assumption is not\nvalid for unstructured points: 3D point clouds are subject to total noise, i.\ne., deviations in all coordinates, with no reliable pixel grid. Thus, an\nobservation can be the realization of an entire manifold of clean 3D points,\nwhich makes a na\\\"ive extension of unsupervised image denoisers to 3D point\nclouds impractical. Overcoming this, we introduce a spatial prior term, that\nsteers converges to the unique closest out of the many possible modes on a\nmanifold. Our results demonstrate unsupervised denoising performance similar to\nthat of supervised learning with clean data when given enough training examples\n- whereby we do not need any pairs of noisy and clean training data.",
  "text": "Total Denoising: Unsupervised Learning of 3D Point Cloud Cleaning\nPedro Hermosilla\nUlm University\nTobias Ritschel\nUniversity College London\nTimo Ropinski\nUlm University\nAbstract\nWe show that denoising of 3D point clouds can be\nlearned unsupervised, directly from noisy 3D point cloud\ndata only. This is achieved by extending recent ideas from\nlearning of unsupervised image denoisers to unstructured\n3D point clouds. Unsupervised image denoisers operate un-\nder the assumption that a noisy pixel observation is a ran-\ndom realization of a distribution around a clean pixel value,\nwhich allows appropriate learning on this distribution to\neventually converge to the correct value. Regrettably, this\nassumption is not valid for unstructured points: 3D point\nclouds are subject to total noise, i. e., deviations in all co-\nordinates, with no reliable pixel grid. Thus, an observa-\ntion can be the realization of an entire manifold of clean 3D\npoints, which makes a na¨ıve extension of unsupervised im-\nage denoisers to 3D point clouds impractical. Overcoming\nthis, we introduce a spatial prior term, that steers converges\nto the unique closest out of the many possible modes on\na manifold. Our results demonstrate unsupervised denois-\ning performance similar to that of supervised learning with\nclean data when given enough training examples - whereby\nwe do not need any pairs of noisy and clean training data.\n1. Introduction\nWhile the amount of clean 3D geometry is limited by\nthe manual effort of human 3D CAD modelling, the amount\nof 3D point clouds is growing rapidly everyday: our city’s\nstreets, the interior of everyday buildings, and even the\ngoods we consume are routinely 3D-scanned. Regrettably,\nthese data are corrupted by scanner noise and as such not ac-\ncessible to supervised learning that requires pairs of noisy\nand clean data. Consequently, it is desirable to be able to\ndenoise the acquired noisy 3D point clouds by solely using\nthe noisy data itself.\nTwo necessary recent developments indicate that this\nmight be possible: deep learning on 3D point clouds [20]\nand unsupervised denoising of images [18, 16, 2].\nUnfortunately, these two methods cannot be combined\nna¨ıvely. To learn our unsupervised 3D point cloud denoisers\n(Fig. 1), we need to overcome two main limitations: the\nFigure 1. We learn 3D point cloud cleaning (right), unsupervised,\nfrom noisy examples alone (left).\npractical obstacle to have a pair of two noisy scans of the\nsame object and the theoretical difﬁculty that noise in 3D\npoint clouds is total.\nWe refer to noise as ‘total’ (Fig. 2) when distortions are\nnot conﬁned to the range (pixel values) while the domain is\nclean (as pixel positions are), but to the more challenging\nsetting where both domain and range are affected by noise.\nThe name is chosen in analogy to total least squares [10],\nwhich dealt with simultaneous noise in domain and range,\nbut for a linear, non-deep setting.\nThis paper’s evaluation shows for simulated noise of dif-\nferent kind, as well as for real point clouds, how our un-\nsupervised approach nonetheless outperforms a supervised\napproach given enough, and in some cases even when given\nthe same magnitude of, training data, while it runs efﬁ-\nciently in a single pass on large point clouds.\nClean\nNoisy\nNoisy\nNoisy\nStructured\nCommon noise\n2D image pixels\nUnstructured\nTotal noise\n3D point clouds\nFigure 2. To learn denoising of 3D point clouds, we need to extend\nfrom common noise that is clean in one part of the signal, to a total\nsetting, where all parts of the signal are noisy. This example shows\nthree realizations of common noise (left) and total noise (right)\nfor three samples (colors). Please note, how total noise is “more\nnoisy” as both axis are corrupted.\n1\narXiv:1904.07615v2  [cs.CV]  17 Oct 2019\n2. Related Work\nImage denoising.\nDenoising images is one of the most\nbasic image manipulation operations. The most primitive\nvariants are based on linear ﬁlters such as the Gaussian ﬁl-\nter, eventually with additional sharpening [14]. While non-\nlinear ﬁlters, such as median, bilateral [25] or non-local\nmeans [3] are frequently used in practice, state-of-the-art\nresults are achieved by optimizing for sparsity [6].\nRe-\ncently, it has become popular to learn denoising, when pairs\nof clean and noisy images are available [4].\nLehtinen et al. [18] proposed a method to learn denoising\nwith access to only two noisy images, instead of a clean-\nnoisy pair. Taking it a step further, Noise2Void [16] and\nNoise2Self [2] are two extensions that remove the require-\nment to have two copies of one image corrupted with noise\nand instead work on a single image. In both cases, this is\nachieved by regressing the image from itself. This is done\nby creating a receptive ﬁeld with a “blind spot”, and a net-\nwork regresses the blind spot from its context. We will de-\ntail the theory behind those papers [18, 16, 2] in Sec. 3.\n3D point cloud denoising.\n3D point clouds capture ﬁne\nspatial details but remain substantially more difﬁcult to han-\ndle than images, due to their irregular structure [19].\nAs for images, linear ﬁlters can be applied to remove\nnoise [17], but at the expense of details. As a remedy, image\noperators such as bilateral [8, 5], non-local means [22] or\nsparse coding [1] have been transferred to point clouds.\nWith the advent of PointNet [20], deep learning-based\nprocessing of point clouds has become tractable. Four no-\ntable deep methods to denoise 3D point clouds were sug-\ngested. The ﬁrst is PointProNet, that denoises patches of\npoints by projecting them to a learned local frame and us-\ning Convolutional Neural Networks (CNN) in a supervised\nsetup to move the points back to the surface [23]. However,\nthe accuracy of the method is determined by the accuracy\nof the local frame estimation, which results in artifacts in\nextreme sharp edges. The second approach by Rakotosaona\net al. [21] uses PCPNet [12] (a variant of PointNet [20]) to\nmap noisy point clouds to clean ones. Third, Yu et al. [31]\nlearns to preserve edges, that dominate men-made objects.\nFinally, Yifan et al. [30] deﬁne a clean surface from noisy\npoints by upsampling.\nAll these deep denoising approaches are supervised, as\nthey require pairs of clean and noisy point clouds, which\nin practice are produced by adding noise to synthetic point\nclouds. Our approach does not require such pairs.\nNoise and learning.\nNoise is an augmentation strategy\nused in denoising auto-encoders [26]. These are however\nnot aiming to denoise, but add noise to improve robustness.\nAlso is their target not noisy, but noise is in the input or\nadded to internal states.\n3. Denoising Theory\nBased on denoising in the regular domain, i. e., images,\nwith or without supervision, we will establish a formalism\nthat can later also be applied to derive our unstructured 3D\ncase.\n3.1. Regular Domains\nPixel noise.\nAn observation yi at pixel i in a noise cor-\nrupted image is a sample of a noise distribution yi ∼\np(z|xi) around the true value xi. This is shown in Fig. 3, a).\nThe black curve is the true signal and pixels (dotted vertical\nlines) sample it at ﬁxed positions i (black circles) according\nto a sampling distribution p(z|xi) (yellow curve) around the\ntrue value (pink circle).\nSupervised.\nIn classic supervised denoising, we know\nboth a clean xi and a noisy value y ∼p(z|xi) for pixel\ni and minimize\narg min\nΘ\nEy∼p(z|xi)l(fΘ(y), xi),\nwhere f is a tunable function with parameters Θ, and l is\na loss such as L2. Here and in the following, we omit the\nfact that the input to f comprises of many y that form an\nentire image, or at least a patch. We also do not show an\nouter summation over all images (and later, point cloud) ex-\nemplars.\nUnsupervised, paired.\nLearning a mapping from one\nnoisy realization of an image to another noisy realization\nof the same image is achieved by Noise2Noise [18]. It has\nbeen shown, that learning\narg min\nΘ\nEy1∼p(z|xi)Ey2∼p(z|xi)l(fΘ(y1), y2),\nconverges to the same value as if it had been learned using\nthe mean / median / mode of the distribution p(z|x) when l\nis L2 / L1 / L0. In most cases, i. e., for mean-free noise, the\nmean / median / mode is also the clean value. We refer to\nthis method as ‘paired’, as it needs two realizations of the\nsignal, i. e., one image with two realizations of noise.\nUnsupervised, unpaired.\nLearning a mapping from all\nnoisy observations in one image, except one pixel, to\nthis held-out pixel is achieved by Noise2Void [16] and\nNoise2Self [2]:\narg min\nΘ\nEy∼p(z|xi)l(fΘ(y), y),\nHere, f is a special form of J -incomplete [2] maps that\nhave no access to pixel i when regressing it, i. e., a ‘blind\nObservation\nOther pixel obs.\nUnique mode\nSurface\nDistribution\nNon-unique mode\na)\nb)\nc)\nRange\nDomain\nDomain\nDomain\nDomain\nDomain\nPixel\ni\ni+1\ni-1\ny\nx\nz\np(y|x)\nx\ny\ny\np(z|   )\nq(z|y)\nFigure 3. Substantial differences exist when denoising structured and unstructured data. (a) For structured data, each pixel value follows\na sampling distribution p(z|xi) (yellow curve) around the true value (pink circle). (b) For unstructured data, the distribution p(z|S) has a\nmanifold of modes (pink line). (c) By using the proposed proximity-appearance prior, a unique mode closest to the surface is determined.\nspot’. The same relation between mean / median / mode and\nloss as in Noise2Noise applies. Note that this formulation\ndoes not require two images, and we, therefore, refer to it\nas ‘unpaired’.\nDiscussion.\nAll three methods described above, work un-\nder the assumption that, in a structured pixel grid, the range\n(vertical axis in Fig. 2, left and Fig. 3, a) axis i and the do-\nmain z (horizontal axis) have different semantics. The noise\nis only in the range: it is not uncertain where a pixel is, only\nwhat its true value would be.\n3.2. Unstructured Domains\nPoint noise.\nAs for pixels, we will denote clean points\nas x, noisy points as y and the noise model as p.\nAll\npoints in our derivation can be either positional with XYZ\ncoordinates, or positional with appearance, represented as\nXYZRGB points.\nTo our knowledge, deep denoising of colored point\nclouds has not been proposed. We will not only show how\nour technique can also be applied to such data but more-\nover, how color can help substantially to overcome chal-\nlenges when training unsupervised learning of a point cloud\ndenoiser. Surprisingly, this beneﬁt can be exploited dur-\ning training, even when no color is present at test time. If\navailable, it will help, and we can also denoise position and\nappearance jointly.\nSupervised.\nDenoising a point cloud means to learn\narg min\nΘ\nEy∼p(z|S)l(fΘ(y), S),\nthe sum of the losses l (e. g., Chamfer) between fΘ(y) and\nthe surface S of the 3D object. Such supervised methods\nhave been proposed, but they remain limited by the amount\nof training data available [23, 21], as they require access to\na clean point cloud.\n4. Unsupervised 3D Point Cloud Denoising\nWe will ﬁrst describe why a paired approach is not feasi-\nble for unstructured data before we introduce our unpaired,\nunsupervised approach.\n4.1. Inapplicability of ‘Paired’ Approaches\nLearning a mapping fΘ(Y1) = Y2 from one noisy point\ncloud realization Y1 to another noisy point cloud realiza-\ntion Y2 that both have the same clean point cloud X and\nwhere the i-th point in both point clouds is a realization\nof the i-th ground truth value, would be a denoiser in the\nsense of Noise2Noise [18]. Regrettably, Noise2Noise can-\nnot be applied to unsupervised learning from unstructured\npoint clouds for two reasons.\nFirst, this paired design, same as for images, would re-\nquire supervision in the form of two realizations of the same\npoint cloud corrupted by different noise realizations. While\nthis is already difﬁcult to achieve for 2D image sensors, it is\nnot feasible for 3D scanners.\nSecond, it would require a network architecture to know\nwhich point is which, similar as it is given by the regular\nstructure of an image that explicitly encodes each pixel’s\nidentity i. This is never the case for total noise in points.\nOpposed to this, modern convolutional deep point process-\ning [20, 13] is exactly about becoming invariant under re-\nordering of points.\nIn order to overcome this problem in a supervised set-\nting, Rakotosaona et al. [21] simulated such pairing by se-\nlecting, for each noisy observation, the closest point in the\nclean point cloud as the target for the loss. However, this\nis just an approximation of the real surface whose accuracy\ndepends on the quality of the sampling of the clean data.\nFortunately, we can show that a pairing assumption is not\nrequired, such that our approach operates not only unsuper-\nvised but also unpaired, as we will detail next.\n4.2. Unpaired\nLearning a mapping from a noisy realization to itself\nfΘ(Y) = Y is an unsupervised and unpaired denoiser in\nthe sense of Noise2Void [16] or Noise2Self [2]. Deﬁning J\nincompleteness in a point cloud is no difﬁculty: just prevent\naccess of f to point y itself when learning point y from\nthe neighbors of y. Thus, essentially, we train a network\nto map each point to itself without information about itself.\nUnfortunately, there is the following catch with total noise.\na)\nb)\nFigure 4. Comparing small (left) and large noise (right) we see the\nmodes (pink) deviate from the GT surface (black).\nProblem statement.\nDifferent from observing pixels at\nindex i in an image (dotted line Fig. 3, a), which tell us\nthat y is a realization of a hidden value xi to infer, it is un-\nknown which hidden surface point is realized when observ-\ning a point in an unpaired setting. A noisy point observa-\ntion y, can be a realization of p(z|x1) in the same way as it\ncould be a realization of p(z|x2). Consequently, the distri-\nbution p(z|S) has a manifold of modes (pink line in Fig. 3,\nb). Learning a mapping from a noisy realization to itself\nwill try to converge to this multimodal distribution, since,\nfor the same neighborhood, the network will try to regress\ndifferent points from this distribution at the same time.\nWe, therefore, have to look into two questions. First,\nwhat can be said about the similarity of this manifold of\nmodes and the clean surface? And second, how can we de-\ncide which of the many possible modes is the right one?\nAnswering the second, and deriving bounds for the ﬁrst\nquestion are the key contributions of this paper, enabling\nunsupervised 3D point cloud denoising.\n4.3. Manifold of Modes vs. Clean Surface\nConcerning the ﬁrst question, the manifold of modes is\nclose to the surface, but not identical. Fig. 4, a), shows a\nclean surface as a black line, with a small amount of noise,\nwhere most samples are close to the clean surface. In this\ncondition, the learning converges to a solution identical to a\nsolution it would have converged to, as when trained on the\npink line, which is very similar to the clean surface. With\nmore noise, however, it becomes visible in Fig. 4, b) that\nthis manifold is not identical to the surface.\nWe note, that the mode surface is the convolution of the\ntrue surface and the noise model p. We cannot recover de-\ntails removed by this convolution. This is different from\nsupervised NN-based deconvolution, which has access to\npairs of convolved and clean data. In our case, the convo-\nlution is on the limit case of the learning data and we never\nobserve non-convolved, clean data.\nIt is further worth noting, that not all noise distributions\nlead to a manifold that is a surface in 3D or would be a con-\nnected path in our 2D illustrations. Only uni-modal noise\ndistributions, such a scanner noise, have no branching or\ndisconnected components. Our solution will not depend on\nthe topology of this mode structure.\n4.4. Unique Modes\nAs explained above, the na¨ıve implementation of unsu-\npervised unpaired denoising will not have a unique mode to\nconverge to. Therefore, we regularize the problem by im-\nposing the prior q(z|y) that captures the probability that a\ngiven observation y is a realization of the clean point z.\nWe suggest using a combination of spatial and appear-\nance proximity\nq(z|y) = p(z|S) ∗k(z −y)\n(1)\nk(d) =\n1\nσ\n√\n2π exp\n\u0012\n−||Wd||2\n2\n2σ2\n\u0013\n,\n(2)\nwhere σ is the bandwidth of k and W = diag(w) is a diag-\nonal weight matrix trading spatial and appearance locality.\nWe use a value w = 1/αr, r being 5% of the diameter\nof the model and α a scaling factor. In the case of point\nclouds with appearance, we use w = β in the appearance\nrows/columns, otherwise, we only consider proximity. For\nmore details about the values for such parameters please re-\nfer to the supplementary material.\nThis results in convergence to the nearest (in space and\nappearance) mode when optimizing\narg min\nΘ\nEy∼p(z|S)Eq∼q(z|y)l(fΘ(y), q),\n(3)\nThe effect of this prior is seen in Fig. 3, c). Out of many\nmodes, the unique closest one remains.\nNote, that our choice of a Gaussian prior q is not related\nto a Gaussianity of the noise model p, which we do not as-\nsume. The only assumption made here is that out of many\nexplanations, the closest one is correct. We experimented\nwith other kernels such as Wendland [28] and inverse multi-\nquadratic but did not observe an improvement.\nAppearance to the rescue\nAs mentioned above, 3D point\nclouds that come with RGB color annotation are a surpris-\ning opportunity to further overcome the limitations of un-\nsupervised training. Otherwise, in some cases, the spatial\nprior cannot resolve round edges. This is not because the\nnetwork f is unable to resolve them, but because unsuper-\nvised training does not ‘see’ the sharp details. Fig. 5 de-\ntails how colors resolve this: without RGB, the corners are\nrounded in Fig. 5, a). When adding color, here red and blue\n(Fig. 5. b), the points become separated (Fig. 5, c). The\nsampling of the prior q(z|y) on a red point, will never pick\na blue one and vice-versa. Consequently, the learning be-\nhaves as if it had seen the sharp detail.\nThus, using color in the prior reinforces some of the\nstructure, which was lost when not relying on a regular\npixel grid. We do not know which noisy point belongs to\nwhich measurement, but we have a strong indication, that\nsomething of different color, is not a noisy observation of\na)\nd)\nc)\nb)\nFigure 5. Bilaterality: The manifold of modes of the distribution of\na 2D point cloud without color can be curved for strong noise (a).\nDifferent appearancea, denoted as red and blue points in b, can\nbe used to establish bilateral distances, lifting points to 3D. The\nresulting manifold of modes (d) now preserves sharp appearance\nedges.\nthe same point. Of course, it is possible, that two obser-\nvations y1 and y2 appear to be from a different point, but\nhappen to be measurements of the same clean surface point\nx, whereby range noise affects the color. Fortunately, such\nspurious false negatives are less problematic (they create\nvariance) than the permanent false positives, that lead to\nrounding (bias). Symmetrically, and maybe more severe,\na difference in color is not always a reliable indicator of a\ngeometric discontinuity either. It is, if color is dominated by\nshading, but texture and shadow edges may lead to a high\nfalse negative distance. Note, that the color is only required\nfor training and never used as input to the network.\n4.5. Converging to the mode\nWe train the network to converge to the mode of the prior\ndistribution q(z|y) by using the approximation of the L0\nloss function proposed by Lehtinen et al. [18], (|fΘ(y) −\nq| + ϵ)γ, where their ϵ = 10−8, and their γ is annealed\nfrom 2 to 0 over the training.\nThus, our unsupervised training converges to the same as\ntraining supervised to ﬁnd the closest – in XYZ space and\nRGB appearance, when available – mode on the distribution\nS ∗p resulting from convolving the clean surface S and the\nnoise model p.\n4.6. Implementation\nPrior.\nTo minimize Eq. 3 we need to draw samples ac-\ncording to the prior q which is implemented using rejection\nsampling: we pick a random point ˆq from Y within r from\ny, and train on it only if k(ˆq−y) > ξ for a uniform random\nξ ∈(0, 1). In practice, a single sample is used to estimate\nthis inner expected value over q(z|y).\n0\n1\n2\n1\n0\nLevel\nPoints\n10%\n10%\n5%\nReceptive\nﬁeld\n64\nFeatures\n3/6\n5%\n128\n64\n3\nFigure 6. Architecture overview: We start from a noisy point cloud\nin the top and perform two levels of unstructured encoding, that re-\nduce the receptive ﬁeld, followed by two levels of decoding using\ntransposed unstructured convolutions.\nArchitecture.\nWe implement f using an unstructured\nencoder-decoder based on Monte Carlo convolution [13]\n(Fig. 6). Such an architecture consumes the point cloud,\ntransforms spatial neighborhoods into latent codes deﬁned\non a coarser point set (encoder), and up-sample these to the\noriginal point resolution (decoder). The effective receptive\nﬁeld, so the neighborhood from which the NN regressed\npoints are considered, is 30 % of the diameter of the model.\nIn particular, we perform two levels of encoding, the ﬁrst\nwith a receptive ﬁeld of 5 %, the second at 10 %. The Pois-\nson disk radii for pooling in Level 1 and Level 2 are half the\nsize of the receptive ﬁelds.\nThis architecture is fast to execute, allowing to denoise in\nparallel 800 K points in 13 seconds on a single machine with\na GeForce RTX 2080. Moreover, this architecture is com-\nposed of only 25 K trainable parameters, orders of magni-\ntude smaller than other networks (0.8 million for PointNet\nor 1.4 million for PointNet++).\nTraining.\nBesides these beneﬁts, our method is also easy\nto implement as seen in Alg. 1. Here, Q denotes a set of\nprior samples q for all points in a point cloud. All opera-\ntions are deﬁned on batches that have the size of the point\ncloud. We use an ADAM optimizer [15] with an initial\nlearning rate of .005, which is decreased during training.\nAlgorithm 1 Unsupervised point cloud denoiser training\n1: for all noisy point clouds Y do\n2:\nΞ ←RANDOMUNIFORMBATCH(0, 1)\n3:\nQ ←SAMPLEPRIORBATCH(Y, Ξ)\n4:\nΘ ←MINIMIZEBATCH(||fΘ(Y) −Q||0)\n5: end for\nIteration.\nSimilar as in previous work [21], our results\nimproved if the output of the network is fed as input\nagain. However, this introduces two problems: clustering\nof points, and shrinking of the point cloud after several it-\nerations. We address these problems in a similar way as\nRakotosaona et al. [21]. In order to prevent clustering we\nintroduce the following regularization term that enforces a\npoint cloud with equidistant samples:\nLr = arg min\nΘ\nEy∼p(z|S)\nmax\ny′∈n(Y,y) ∥fΘ(y), fΘ(y′)∥2\nwhere n(Y, y) is the set of points from the noisy point cloud\nwithin a patch centered at y. To prevent shrinking we re-\nmove low-frequency displacements before translating the\nnoisy points. The supplemental materials show the effect\nof these iterations.\n5. Evaluation\nOur experiments explore the application, both to syn-\nthetic (Sec. 5.2) and to real data (Sec. 5.3). For synthetic\ndata, we know the answer and can apply different metrics\nto quantify the performance of our approach, while we do\nnot know the ground truth for real data and results are lim-\nited to a qualitative study.\n5.1. Setup\nData set.\nWe have collected 15 different classes with 7\ndifferent polygonal models each (5 for training and 2 for\ntesting) from ModelNet-40 [29] and sampled the surface\nwith points as explained next. As we optionally use RGB\nappearance, it is computed using Lambertian shading from\n3 random directional lights.\nSampling.\nWe simulate different forms of noise to corrupt\nthe clean data of the synthetic data set.\nIn the SIMPLE noise model, we sample each mesh us-\ning Poisson Disk sampling [27] to obtain clean point clouds\nwithin the range of 13 K and 190 K points each, resulting in\n22 million points for training and 10 million points for test-\ning. Then, we add Gaussian noise with a standard deviation\nof .5%, 1%, and 1.5% of the bounding box diagonal.\nThe ADVANCED sampling emulates true sensor noise\nmaking use of Blendsor [11], a library to simulate sensor\nnoise. In particular, we choose to emulate a Velodyne HDL-\n64E 3D scan. These devices introduce two types of noise in\nthe measurements, a distance bias for each laser unit and\na per-ray Gaussian noise. In our data, we use a standard\ndeviation of .5% of the diagonal of the bounding box for\nthe distance bias and three different levels of per-ray noise,\n.5%, 1%, and 1.5%. This generates point clouds within the\nrange of 3 K and 120 K points each, resulting in 12 million\npoints for training and 5 million points for testing.\nWe investigate levels of distortion where a surface is still\nconceivable.\nMore severe corruptions with uneven sam-\npling or outliers are to be explored in future work.\nMetric.\nWe use the Chamfer distance from Fan et al. [7],\nd(Y, S, X) = 1\nN\nX\ny∈Y\nmin\ns∈S ||y−s||2+ 1\nM\nX\nx∈X\nmin\ny∈Y ||y−x||2\nwhere less is better. The ﬁrst term measures the average\ndistance between the predicted points to their closest point\nin a polygonal surface S. The second term measures how\nthe points are distributed in the ground truth surface.\nSince the clean point clouds X follow a Poisson Disk\ndistribution, by measuring their distance to the closest pre-\ndicted point we are able to determine if the surface is\nequally covered by our predictions.\nThe metric is applied in the test data with three different\nrealizations of noise and averaged over two complete train-\nings to reduce variance in the estimate of the metric.\nMethods.\nWe compare our unsupervised approach with\nclassical methods as well as supervised machine learning\napproaches. To obtain insights regarding the effectivity of\nthe individual subparts, we investigate ablations with and\nwithout the spatial and/or the appearance prior.\nThe classic baselines are MEAN and BILATERAL [5],\nwhich are also unsupervised. Their parameters are chosen\nto be optimal on the training set.\nAs supervised learning-based denoisers, we use the same\narchitecture, as we have employed for the unsupervised set-\nting, whereby we use the training algorithm proposed in\nPointCleanNet [21]. While this means, that we do not use\nthe original PointCleanNet network architecture, which is\nbased on PointNet, we believe that our evaluation is more\ninsightful with a uniﬁed architecture – especially since the\narchitecture employed by us has outperformed PointNet on\na range of other tasks [13].\nFinally, we study three variants of our approach. The\nﬁrst one is with NO PRIOR.\nThe second we denote as\nNO COLOR, which is our prior but only based on prox-\nimity. The last one is FULL which includes all our con-\ntributions. More precisely, we use XYZ point clouds for\nNO PRIOR and NO COLOR and XYZRGB point clouds for\nFULL. Again, color is only used to sample the prior, not as\nan input to the network during training or testing.\n5.2. Quantitative Results\nDenoising performance.\nWe start with SIMPLE noise and\nlook into ADVANCED later. A comparison of the average\nerror across the test set for different methods is shown in\nTbl. 1, whereby each column represents one method. All\nmethods are trained with the same amount of training ex-\nemplars, that is, 22 million points.\nAs can be seen, our full method (orange) performs best.\nWe even outperform the supervised competitor (red), likely\nTable 1. Error (less is better) per method on SIMPLE noise.\nOurs\nMean Bilat. No p. No c. Full Sup.\n•\n•\n•\n•\n•\n•\n.598\n.592\n.582\n.547 .542 .545\nError\nbecause the network has to ﬁnd more valid generalizations\nand is less prone to over-ﬁtting. As can also be seen, the\nother non-learned methods like mean (violet) and bilateral\n(blue) are not competitive, even when tuned to be optimal\non training data.\nWe further see a clear distinction be-\ntween ablations of our method and the full approach. When\ntraining without a spatial prior (cyan), the method is much\nworse than supervised and only slightly better than mean. A\nmethod not using color for training (green) – but including\nthe spatial prior – can achieve almost full performance, but\nonly adding color will outperform supervised.\nThis comparison is on the same amount of data. How-\never, in most real-world scenarios, the number of noisy\npoint clouds can be assumed to be much higher than the\namount of clean ones. We will study this relation next.\nSupervision scalability.\nWe will now study, how super-\nvised method scale with the number of clean point clouds\nand our method with the amount of noisy point clouds.\nThe outcome is seen in Tbl. 2 where different methods\nare columns and different amounts of training data are rows.\nThe plot to the right shows the relation as a graph. We show\nthe logarithmic ﬁt to the individual measurements shown as\npoints The color encoding is identical for all plots. The dif-\nference between the methods is measured wrt. the number\nof total training points ranging from .5 to 22 millions.\nNot unexpected, we see all methods beneﬁt from more\ntraining data. We see that our method performs better than\nsupervised across a range of training data magnitudes. At\naround 22 million points, the highest we could measure,\nthe red and orange lines of our full model and supervised\ncross. That only means, that after this point, our unsuper-\nvised method needs more training data to achieve the same\nperformance as a supervised method.\nWe further see that the ablations of our method without\nTable 2. Error (less is better) for different amount of supervision.\nOurs\nTrain\ndata\nNo p. No c. Full Sup.\n•\n•\n•\n•\n.5 M\n.587\n.557 .558 .574\n1 M\n.584\n.550 .557 .563\n4 M\n.584\n.553 .543 .546\n22 M\n.582\n.547 .542 .545\n.6\n.52\n.5\n22\nError\nTrain pts\nTable 3. Error (less is better) for different levels of SIMPLE noise.\nOurs\nNoise\nLevels\nNo p. No c. Full Sup.\n•\n•\n•\n•\n1.5 %\n.734\n.698 .691 .695\n1.0 %\n.578\n.534 .525 .515\n0.5 %\n.435\n.411 .408 .426\n.4\n.7\n.5\n1.5\nError\nNoise\na prior and without color do not only perform worse, but\nalso scale less favorable, while ours (orange) is similar to\nsupervised (red). Admittedly, supervised scales best.\nAmount of noise.\nWhile we have studied the average over\nthree levels of noise in the previous plots, Tbl. 3 looks into\nthe scalability with noise levels in units of scene diameter\npercentages. We ﬁnd that error is increasing as expected\nwith noise, but all methods do so in a similar fashion. In\ntwo cases, we win over supervised, in one case supervised\nwins, resulting in the improved average reported above.\nDenoising performance.\nWhile we have studied SIMPLE\nGaussian noise, we now relax this assumption and explore\nADVANCED simulated scanner noise generated as explained\nin Sec. 5.1. Contrary to real scanned data, it has the beneﬁt\nthat the ground truth is known.\nTbl. 4 shows the error of different methods for this type\nof noise. We see, that in this case, our full method (orange)\nperforms better than any other unsupervised method, such\nas mean or bilateral (violet and blue). A supervised method\ncan perform better than other methods for this noise at the\nsame amount of training data input, 12 million points. Fi-\nnally, we also see that ablations without the suggested prior\n(cyan and green) have a higher error, indicating the priors\nare equally relevant for this type of noise, too.\nUpgrading\nNotably, we can upgrade any supervised de-\nnoiser in a code-transparent, architecture-agnostic fashion\nto become unsupervised. Consider a supervised denoiser,\nwhich takes clean-noisy pairs instead of noisy ones only, as\nwe do. To apply our method, all we do is to resample the\npoint using our spatial and / or color prior, and “pretend”\nthis to be the clean point cloud.\nTable 4. Error (less is better) per method on ADVANCED noise.\nOurs\nMean Bilat. No p. No c. Full Sup.\n•\n•\n•\n•\n•\n•\n.378\n.362\n.393\n.359 .356 .329\nError\nFigure 7. Multiple real world pairs of noisy input scans (left) and the result of our denoiser (right), accompanied by zoomed insets.\nPCNet [21]\nNoisy Sup. Our\n4.54 1.36 1.34\nWe have done so for Point-\nCleanNet [21] and evaluated on\ntheir dataset. We see even with-\nout modifying their architecture\nor supervision, we still slightly\noutperform theirs.\n5.3. Qualitative Results\nHere, we repeat the above experiments, on real world\nnoisy point clouds from a Mobile Laser Scanning setup\nbased on a Velodyne HDL32 3D scanner. We used the Paris-\nrue-Madame data set [24] which is composed of 20 million\npoints. We subdivide the model into parts of ca. 150 K\npoints each, resulting in 17 million points used during train-\ning and 3 million points for testing. Note that in this setting,\nnoise is part of the data and does not need to be simulated.\nFurthermore, and most important, no clean ground truth is\navailable. Consequentially, the error cannot be quantiﬁed\nand we need to rely on human judgment.\nWe see in Fig. 7, how our method removes the noise and\nproduces a clean point cloud without shrinking, with uni-\nform sampling as well as details. We cannot apply a visu-\nalization of the error, as the ground truth is unknown. We\ninstead provided point cloud renderings by representing the\npoint clouds as a mesh of spheres with shading.\n5.4. Ablation\nNo prior\nWhen not using the prior in space (green), the\ndenoiser learned across different types of noise (Tbl. 1),\nmagnitudes of noise (Tbl. 3) and amounts of training data\n(Tbl. 2) is consistently worse and not much better than\nGaussian or bilateral. This indicates it is essential.\nNo appearance\nMaking use of appearance consistently\nimproves the outcome across the aforementioned three axes\nFigure 8. Including and not including color in the prior.\nof variations in Tbl. 1 (and Tbl. 4), Tbl. 2 and Tbl. 3, either\ntaking the quality beyond supervision or very close to it.\nEffect of color\nFig. 8 shows a sharp edge with two dif-\nferent colors to be denoised. Including the color, slightly\nreduces the error (less high-error yellow, more blue).\n6. Conclusions\nWe have presented an unsupervised learning method to\ndenoise 3D point clouds without needing access to clean ex-\namples, and not even noisy pairs. This allows the method to\nscale with natural data instead of clean CAD models deco-\nrated with synthetic noise. Our achievements were enabled\nby a network that maps the point cloud to itself in combina-\ntion with a spatial locality and a bilateral appearance prior.\nUsing appearance in the prior is optional, but can improve\nthe result, without even being input to the network, neither\nat test nor at training time. Denoising with color as input, as\nwell as joint denoising of color and position, remains future\nwork. Our results indicate we can outperform supervised\nmethods, even with the same number of training examples.\nAcknowledgements\nThis work was partially funded by the\nDeutsche Forschungsgemeinschaft (DFG), grant RO 3408/2-1\n(ProLint), and the Federal Ministry for Economic Affairs and En-\nergy (BMWi), grant ZF4483101ED7 (VRReconstruct). We ac-\nknowledge Gloria Fackelmann for the supplementary video narra-\ntion.\nReferences\n[1] Haim Avron, Andrei Sharf, Chen Greif, and Daniel Cohen-\nOr. l1-sparse reconstruction of sharp point set surfaces. ACM\nTrans. Graph, 29(5):135, 2010. 2\n[2] Joshua Batson and Lo¨ıc Royer. Noise2Self: Blind denoising\nby self-supervision. CoRR, abs/1901.11365, 2019. 1, 2, 3\n[3] Antoni Buades, Bartomeu Coll, and Jean-Michel Morel. A\nnon-local algorithm for image denoising. In CVPR, pages\n60–5, 2005. 2\n[4] Harold C Burger, Christian J Schuler, and Stefan Harmeling.\nImage denoising: Can plain neural networks compete with\nBM3D? In CVPR, pages 2392–2399, 2012. 2\n[5] Julie Digne and Carlo de Franchis. The Bilateral Filter for\nPoint Clouds. Image Processing On Line, 7:278–287, 2017.\n2, 6\n[6] Michael Elad and Michal Aharon.\nImage denoising via\nsparse and redundant representations over learned dictionar-\nies. Trans. Image Processing, 15(12):3736–45, 2006. 2\n[7] Haoqiang Fan, Hao Su, and Leonidas J. Guibas. A point\nset generation network for 3D object reconstruction from a\nsingle image. CoRR, abs/1612.00603, 2016. 6\n[8] Shachar Fleishman, Iddo Drori, and Daniel Cohen-Or. Bi-\nlateral mesh denoising. ACM Trans. Graph., 22(3):950–3,\n2003. 2\n[9] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel\nUrtasun. Vision meets robotics: The kitti dataset. Interna-\ntional Journal of Robotics Research (IJRR), 2013. 10, 11\n[10] Gene H Golub and Charles F Van Loan.\nAn analysis of\nthe total least squares problem. SIAM J Numerical Analy-\nsis, 17(6):883–893, 1980. 1\n[11] Michael Gschwandtner, Roland Kwitt, Andreas Uhl, and\nWolfgang Pree.\nBlensor: blender sensor simulation tool-\nbox. In Int. Symposium on Visual Computing, pages 199–\n208, 2011. 6\n[12] Paul Guerrero, Yanir Kleiman, Maks Ovsjanikov, and\nNiloy J. Mitra.\nPCPNET: learning local shape properties\nfrom raw point clouds. CoRR, abs/1710.04954, 2017. 2\n[13] Pedro Hermosilla, Tobias Ritschel, Pere-Pau Vazquez, Al-\nvar Vinacua, and Timo Ropinski. Monte Carlo convolution\nfor learning on non-uniformly sampled point clouds. ACM\nTrans. Graph., 37(6), 2018. 3, 5, 6\n[14] Anil K Jain. Fundamentals of digital image processing. En-\nglewood Cliffs, NJ: Prentice Hall,, 1989. 2\n[15] Diederik P. Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. CoRR, abs/1412.6980, 2014. 5\n[16] Alexander Krull, Tim-Oliver Buchholz, and Florian Jug.\nNoise2Void - learning denoising from single noisy images.\nCoRR, abs/1811.10980, 2018. 1, 2, 3\n[17] In-Kwon Lee.\nCurve reconstruction from unorganized\npoints. Computer aided geometric design, 17(2):161–177,\n2000. 2\n[18] Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli\nLaine,\nTero Karras,\nMiika Aittala,\nand Timo Aila.\nNoise2Noise: Learning image restoration without clean data.\nICML, 2018. 1, 2, 3, 5\n[19] Marc Levoy and Turner Whitted. The use of points as a dis-\nplay primitive. UNC ChapelHill Technical Report, 1985. 2\n[20] Charles Ruizhongtai Qi,\nHao Su,\nKaichun Mo,\nand\nLeonidas J. Guibas. Pointnet: Deep learning on point sets for\n3D classiﬁcation and segmentation. CoRR, abs/1612.00593,\n2016. 1, 2, 3\n[21] Marie-Julie Rakotosaona, Vittorio La Barbera, Paul Guer-\nrero, Niloy J. Mitra, and Maks Ovsjanikov. POINTCLEAN-\nNET: learning to denoise and remove outliers from dense\npoint clouds. 2019. 2, 3, 5, 6, 8\n[22] Guy Rosman, Anastasia Dubrovina, and Ron Kimmel.\nPatch-collaborative spectral point-cloud denoising. In Com-\nputer Graphics Forum, volume 32, pages 1–12, 2013. 2\n[23] Riccardo Roveri, A. Cengiz ¨Oztireli, Ioana Pandele, and\nMarkus H. Gross.\nPointpronets: Consolidation of point\nclouds with convolutional neural networks. Comput. Graph.\nForum, 37(2):87–99, 2018. 2, 3\n[24] Andr´es Serna, Beatriz Marcotegui, Franois Goulette, and\nJean-Emmanuel Deschaud.\nParis-rue-madame database -\na 3D mobile laser scanner dataset for benchmarking ur-\nban detection, segmentation and classiﬁcation methods. In\nICPRAM, 2014. 8, 10, 13\n[25] Carlo Tomasi and Roberto Manduchi. Bilateral ﬁltering for\ngray and color images. In ICCV, page 839, 1998. 2\n[26] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua\nBengio, and Pierre-Antoine Manzagol. Stacked denoising\nautoencoders: Learning useful representations in a deep net-\nwork with a local denoising criterion. J Machine Learning\nRes., 11:3371–408, 2010. 2\n[27] Li-Yi Wei.\nParallel poisson disk sampling.\nACM Trans.\nGraph., 27(3):20:1–20:9, 2008. 6\n[28] Holger Wendland. Piecewise polynomial, positive deﬁnite\nand compactly supported radial functions of minimal de-\ngree. Advances in computational Mathematics, 4(1):389–96,\n1995. 4\n[29] Zhirong Wu, S. Song, A. Khosla, Fisher Yu, Linguang\nZhang, Xiaoou Tang, and J. Xiao. 3D ShapeNets: A deep\nrepresentation for volumetric shapes. In CVPR, pages 1912–\n1920, 2015. 6\n[30] Wang Yifan, Shihao Wu, Hui Huang, Daniel Cohen-Or, and\nOlga Sorkine-Hornung. Patch-based progressive 3D point\nset upsampling. In CVPR, 2019. 2\n[31] Lequan Yu, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, and\nPheng-Ann Heng. Ec-net: an edge-aware point set consoli-\ndation network. In ECCV, pages 386–402, 2018. 2\nSupplementary material\nA. Error distribution\nWe studied the distribution of points wrt. the distance to the\nunderlying surface in our test dataset. Fig. 9 (left) presents a his-\ntogram with the results of such study. Here we can see how the\ndistribution of points has its maximum at distance 0.0 from the\nreal surface and decreases with distance, following the same dis-\ntribution as in the supervised setting.\nB. Convergence\nFig. 9 (right) illustrates the evaluation error during training for\ntwo networks trained on the smallest dataset (.5 Million points).\nOne network is trained with our algorithm (FULL) and the other\none with supervised training. We can see that in both cases the\nloss decreases during training similarly. However, our algorithm\nconverges to a lower error than supervised learning. With more\ntraining data, this gap is reduced until the curves cross.\nC. Prior Kernel\nWe experimented with different kernels for our prior: Gaussian\n(kg), Wendland (kw), and Inverse Multi-Quadric (ki). All kernels\nwere adjusted to ﬁt in the range [−1, 1]:\nkg(d) =\n1\nσ\n√\n2π\nexp\n\u0012\n−||Wd||2\n2\n2σ2\n\u0013\n,\nkw(d) = (1 −||Wd||2\n2)4(1 + 4||Wd||2\n2),\nki(d) =\n1\np\n1 + (5||Wd||2\n2)2 ,\nwhere W = diag(w) and w = 1/αr. We performed an ablation\nstudy for different values of α and for each kernel independently.\nFor the Gaussian kernel we used α1 = .3, α2 = .5, and α3 = .7,\nand for the Wendland and Inverse Multi-Quadric kernel we used\nα1 = .8, α2 = 1., and α3 = 1.2. Tbl. 5 shows that the best\nperformance was obtained by the Gaussian kernel.\n.0\n.5\nDistance\n# Points\nTrain steps\nError\nError histogram.\nEval. loss during training.\nFigure 9. Ours (FULL) • vs Supervised •\nTable 5. Error obtained for different functions used as our prior.\nPrior\nα1\nα2\nα3\nGauss. (kg) •\n.567\n.548\n.556\nWedland (kw) •\n.595\n.560\n.561\nInv. MQ (ki) •\n.577\n.577\n.582\nD. Noise levels\nTest\nTrain 0.5% 1.0% 1.5%\n0.5%\n.435 .576 .738\n1.0%\n.431 .551 .708\n1.5%\n.467 .594 .741\nAll\n.411 .534 .698\nTbl.\n3 on the paper tests\none neural network trained for\nall noise levels at the same time.\nIn this experiment, we have\ntrained our network using only\none level of noise and report its\ntest error for all levels. We can\nsee from the table, that perfor-\nmance drops only marginally.\nE. Iterative reﬁnement\nSince our approximation of the surface during training is not\nexact, the results improve by applying our network several times\niteratively. Fig. 10 presents a visual encoding of the distance of\neach point to the real surface after applying each denoising step.\nF. Additional Qualitative Results\nWe trained our network with the Kitti dataset [9] in order to\nevaluate the robustness of our method with spare point clouds.\nFig. 11 presents some qualitative results. We can see that our net-\nwork is able to remove the noise successfully.\nMoreover, Fig. 12, and Fig. 13 provide more qualitative results\non all the datasets used in the evaluation. In Fig. 12 we can see\nhow our algorithm obtains a quality similar to the network trained\nwith supervised data. Fig. 13 illustrates the result of applying our\nmethod to models from the Paris-rue-Madame Database [24] com-\nposed of 375 K-800 K points.\nFigure 10. Resulting point cloud of applying our network several times iteratively on a noisy point cloud.\nFigure 11. Qualitative results of our network trained on the Kitti dataset [9]. Note how our network is able to remove the noise in sparse\npoint clouds as the one illustrated in the bottom image.\nFigure 12. Visual encoding of the distance from each point to the underlying ground truth surface. Here blue indicates zero distance to\nthe surface and yellow 2% of the diagonal of the model. We can see that for the two simulated datasets with different levels of noise, our\nalgorithm achieves (Full) almost the same quality as a network trained with supervised data (Sup.). Moreover, we evaluated two variants\nof our training procedure: a prior without color information (No Color), and No Prior.\nFigure 13. Comparison of our denoising algorithm (right) with the noisy scanned data (left) from the Paris-rue-Madame Database [24].\nThe number of points for each model are, from top to bottom, 800 K, 450 K, 450 K, and 375 K points. Our network is able to process each\nmodel in parallel in a workstation with a single Nvidia RTX 2080.\n",
  "categories": [
    "cs.CV",
    "cs.GR"
  ],
  "published": "2019-04-16",
  "updated": "2019-10-17"
}