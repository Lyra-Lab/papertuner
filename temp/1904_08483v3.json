{
  "id": "http://arxiv.org/abs/1904.08483v3",
  "title": "Deep learning for image segmentation: veritable or overhyped?",
  "authors": [
    "Zhenzhou Wang"
  ],
  "abstract": "Deep learning has achieved great success as a powerful classification tool\nand also made great progress in sematic segmentation. As a result, many\nresearchers also believe that deep learning is the most powerful tool for pixel\nlevel image segmentation. Could deep learning achieve the same pixel level\naccuracy as traditional image segmentation techniques by mapping the features\nof the object into a non-linear function? This paper gives a short survey of\nthe accuracies achieved by deep learning so far in image classification and\nimage segmentation. Compared to the high accuracies achieved by deep learning\nin classifying limited categories in international vision challenges, the image\nsegmentation accuracies achieved by deep learning in the same challenges are\nonly about eighty percent. On the contrary, the image segmentation accuracies\nachieved in international biomedical challenges are close to ninty five\npercent. Why the difference is so big? Since the accuracies of the competitors\nmethods are only evaluated based on their submitted results instead of\nreproducing the results by submitting the source codes or the software, are the\nachieved accuracies verifiable or overhyped? We are going to find it out by\nanalyzing the working principle of deep learning. Finally, we compared the\naccuracies of state of the art deep learning methods with a threshold selection\nmethod quantitatively. Experimental results showed that the threshold selection\nmethod could achieve significantly higher accuracy than deep learning methods\nin image segmentation.",
  "text": "Deep learning for image segmentation: \nveritable or overhyped? \nZHENZHOU WANG*  \nCollege of electrical and electronic engineering, Shandong University of technology, Zibo City, \n255000,China \n*wangzz@sdut.edu.cn \nAbstract: Deep learning has achieved great success as a powerful classification tool and also \nmade great progress in sematic segmentation. As a result, many researchers also believe that \ndeep learning is the most powerful tool for pixel-level image segmentation. Could deep \nlearning achieve the same pixel-level accuracy as traditional image segmentation techniques \nby mapping the features of the object into a non-linear function? This paper gives a short \nsurvey of the accuracies achieved by deep learning so far in image classification and image \nsegmentation. Compared to the high accuracies achieved by deep learning in classifying \nlimited categories in international vision challenges, the image segmentation accuracies \nachieved by deep learning in the same challenges are only about 80%. On the contrary, the \nimage segmentation accuracies achieved in international biomedical challenges are close to \n95%. Why the difference is so big? Since the accuracies of the competitors‚Äô methods are only \nevaluated based on their submitted results instead of reproducing the results by submitting the \nsource codes or the software, are the achieved accuracies verifiable or overhyped? We are \ngoing to find it out by analyzing the working principle of deep learning. Finally, we compared \nthe accuracies of state of the art deep learning methods with a threshold selection method \nquantitatively. Experimental results showed that the threshold selection method could achieve \nsignificantly higher accuracy than deep learning methods in image segmentation.  \n1. Introduction \nMost deep learning models are based on artificial neural network that is invented by \nsimulating human brains. Although most deep learning models have various differences from \nthe structural and functional properties of human brains, they are analogical to some extent. \nThe neural dynamics of human brain corresponds to a form of variational EM algorithm, i.e. \nwith approximate rather than exact posteriors [1]. For instance, the human brain could \nrecognize an object without knowing how to calculate its exact size, height, shape or other \ninformation and just judge based on some approximated information. Similar to human brains, \ndeep learning models classify the objects without knowing how to calculate its exact size, \nheight, shape or other information. In nature, deep learning combines many continuous \nregression units discretely and approximates a discrete non-linear mapping function without \nknowing its exact form. By nature, deep learning is similar to, but different from regression.  \nThe linear regression model constructs the global linear mapping between the inputs and \noutputs. Different from the linear regression model, deep learning needs to construct the local \nlinear models to determine the global non-linear mapping between the inputs and the outputs. \nWe show a simple example in Fig.1 to demonstrate the difference between the linear \nregression model and the deep learning model. For simplicity, we only draw one hidden layer \nof deep learning. For the linear regression model in Fig. 1(a), there are three variables for the \ninputs and two variables for the outputs and they are formulated by the following equations.  \nùë¶1 = ùë§11 √ó ùë•1 + ùë§21 √ó ùë•2 + ùë§31 √ó ùë•3 + ùëè1                                (1) \nùë¶2 = ùë§12 √ó ùë•1 + ùë§22 √ó ùë•2 + ùë§32 √ó ùë•3 + ùëè2                                (2) \n \n1 \n \n  \n    \n \n(a)                                                                               (b) \nFig. 1. Demonstration of the difference between a linear regression model and a simple learning model (a) The linear \nregression model; (b) The simplified deep learning model. \n \nAs can be seen, the above model can be easily solved by the least squares method. \nHowever, when the inputs are not linearly separable, there will be no linear mapping between \nthe inputs and the outputs. Deep learning uses many hidden layers to construct the nonlinear \nmapping. As simplified in Fig. 1 (b), one hidden layer with three neurons is added and the \nnonlinear mapping between the input and the output is formulated by the following equations.    \n‚Ñé1 = ùëä11 √ó ùëã1 + ùëä21 √ó ùëã2 + ùëä31 √ó ùëã3 + ùëè1                             (3) \n‚Ñé2 = ùëä12 √ó ùëã1 + ùëä22 √ó ùëã2 + ùëä32 √ó ùëã3 + ùëè2                             (4) \n‚Ñé3 = ùëä13 √ó ùëã1 + ùëä23 √ó ùëã2 + ùëä33 √ó ùëã3 + ùëè3                             (5) \nùëå1 = ùëä‚Ä≤11 √ó ‚Ñé1 + ùëä‚Ä≤21 √ó ‚Ñé2 + ùëä‚Ä≤31 √ó ‚Ñé3 + ùëè‚Ä≤1                         (6) \nùëå2 = ùëä‚Ä≤12 √ó ‚Ñé1 + ùëä‚Ä≤22 √ó ‚Ñé2 + ùëä‚Ä≤32 √ó ‚Ñé3 + ùëè‚Ä≤2                          (7) \nA process called ‚Äòtraining‚Äô is used to solve the above model and it relies on descent \nalgorithms [2]. Different from the linear regression, deep learning could only determine these \nparameters to make the outputs respond to the inputs according to the training data instead of \ngenerate a continuous and meaningful mapping function between the inputs and the outputs. \nWith the continuous and meaningful mapping function, i.e. the exact mathematical equations, \nregression could determine the linear or non-linear mapping between the continuous input \nvariables and the continuous output variables. On the contrary, deep learning could only \ndetermine the non-linear mapping between the discrete input variables and the finite number \nof discrete output variables with a set of discrete training parameters. \n Without loss of generality, regression is formulated as a mapping model by the following \nsimple equation.  \nùë¶ ‚âà ùëì(ùë•, ùë§)                                                             (8) \nùëì is a continuous linear or non-linear mapping function and it has the exact mathematical \nfunction. ùë§ denotes the coefficients to be determined by regression. ùë• denotes the continuous \ninput variable and ùë¶ denotes the continuous output variable.  \nSimilarly, deep learning could be formulated as: \nùëåùëñ ‚âà ùêπ‡µ´ùëãùëó, ùëä‡µØ; ùëñ= 1, ‚Ä¶ , ùêº; ùëó= 1, ‚Ä¶ , ùêΩ                             (9) \n2 \n \nùêπ is a discrete and non-linear mapping function and its exact mathematical function is \nnever known. ùëä denotes the discrete mapping parameters to be determined by training. ùëãùëó \ndenotes the discrete input variable and ùêº denotes its total number. ùëåùëñ denotes the discrete \noutput variable and ùêΩ denotes its total number.  \nFor image segmentation, the inputs of the deep learning are the features extracted from \nthe image and the output is a prediction map. For the predication map to have pixel-level \naccuracy, the number of the outputs ùëÄ is computed as:  \nùêΩ= ùêøùëÄ√óùëÅ                                                                     (10) \nwhere ùëÄ is the width of the image and ùëÅ is the height of the image. ùêø denotes the total \nnumber of labeling. If the image is segmented into two classes, ùêø equals 2. If the image is \nsegmented into three classes, ùêø equals 3, etc.  To segment an image with a resolution of \n100 √ó 100 into two classes, the number of outputs is close to infinity, which is intractable for \npractical implementation. Consequently, the predication map could reach the semantic level \nin practice. Therefore, traditional image segmentation techniques are usually used to refine \nthe prediction result to achieve the pixel-level accuracy.   \n2. Deep learning for object classification and detection  \nThe deep learning revolution took place around 2012 when it won several well-known \ninternational speech and image recognition or classification competitions. The competitions \nthat made deep learning resurge include, but not limited to: large-scale automatic speech \nrecognition competition [3-4], mixed national institute of standards and technology (MNIST) \ndatabase competition [5], traffic sign competition [6], Chinese handwriting recognition \ncompetition [7], ImageNet large scale visual recognition competition [8-9], pattern analysis \nstatistical modeling and computational learning (PASCAL) visual object classes database \ncompetition [10] and so on.  \n   Large-scale automatic speech recognition competition uses the Texas Instrument and \nMassachusetts Institute of Technology (TIMIT) dataset that contains a total of 6300 sentences, \n10 sentences spoken by each of 630 speakers from 8 major dialect regions of the United \nStates. The test data has a core portion containing 24 speakers, 2 male and 1 female from each \ndialect region. According to Eq. (9), the number of inputs is 6300 and the number of the \noutputs is 8, which is sufficient for deep learning to determine the training parameters \nrobustly. In [4], it is reported that the recognition error rate for the training sets is 13.3 and the \nrecognition error rate for the core test sets is 16.5. The MNIST database of handwritten digits \nhas a training set of 60,000 examples and a test set of 10,000 examples. According to Eq. (9), \nthe number of inputs is 60000 and the number of the outputs is 10, which is sufficient for \ndeep learning to determine the training parameters. In [5], it is reported that the error rate is \nonly 0.23%. Traffic sign competition contains 39209 training images in 43 classes. According \nto Eq. (9), the number of inputs is 39209 and the number of the outputs is 43, which is \nsufficient for deep learning to determine the training parameters. In [6], it is reported that a \nrecognition rate of 99.46% had been achieved and it was better than the one of humans on this \ntask (98.84%). Chinese handwriting recognition competition contains a large dataset of \nisolated characters with about 3.9 million samples of 7,356 classes (7,185 Chinese characters \nand 171 symbols). According to Eq. (9), the number of inputs is 3900000 and the number of \noutputs is 7356, which is also sufficient for deep learning to determine the training parameters \nrobustly. In [7], a 97.39% character recognition rate was achieved. ImageNet image \nclassification competition between 2012 and 2014 contains 1281167 images for 1000 classes \nof objects. According to Eq. (9), the number of inputs is 1281167 and the number of outputs \nis 1000, which is sufficient for deep learning to determine the training parameters. In [8], an \nerror rate of 15.3% had been achieved, which ignite the revolution of deep learning. In [9], it \nis reported that the best error rate 6.66% was achieved. However, it only achieved 43.93% \naverage precision for object detection, which already ranked the first among 17 competing \nteams. In the last session of ImageNet Large Scale Visual Recognition Challenge that took \n3 \n \nplace in 2017, the achieved best object detection accuracy was 73.14%. All these measures \nrange from 0 to 1 and the higher the better. For the PASCAL datasets, 8498 images are used \nto train a deep learning method called fully convolutional network (FCN) and only 67.5 % \naverage precision was achieved [10]. From the visual results in [10], it is seen that even the \ninput is the ground truth, the precision of the output is still not high. Since deep learning is a \ndiscrete mapping function and is not capable of generating continuous (infinite) outputs, the \nperformance of deep learning in such cases will be reduced significantly. Consequently, no \ngood results are expected when deep learning is used for image segmentation unless the \nresolution of the image is extremely small.  \nFor better comparison, we list all the competitions mentioned above in Table 1 with their \ncritical data information and best accuracies achieved so far. As can be seen, the performance \nof deep learning is inversely proportional to the number of outputs with the assumption that \nsufficient inputs are available. Similar to Shannon theorem, a theorem about the proportion of \nthe number of the inputs and the number of the outputs should be studied to guide the \nperformance of the deep learning.  \n \nTable 1. Comparisons of the best accuracies achieved by deep learning in international competitions with the critical \ndata information  \nCompetitions \nNumber of inputs \nNumber of outputs \nBest accuracy \nVoice recognition \n6300 \n8 \n83.5% [2] \nDigits recognition \n60000 \n10 \n99.77% [4] \nTraffic sign recognition \n39209 \n43 \n99.46% [5] \nChinese handwriting recognition \n3900000 \n7356 \n97.39% [6] \nImageNet image classification \n1281167 \n1000 \n93.34% [7] \nImageNet object detection \n1281167 \n‚âàinfinity \n73.14% [8] \nPASCAL VOC image segmentation \n8498 \n‚âàinfinity \n67.5 % [9];79.5%[10] \n \n3. Deep learning for biomedical image segmentation \nIn theory, deep learning does not have the ability to achieve an accurate image segmentation \nresult since it simulates human brains to classify the objects based on approximation \ninformation instead of the exact information of the object as analyzed in the above sections. \nHowever, there are many reported research work about the unprecedented segmentation \naccuracy achieved by deep learning. Let us look into the details of how these researchers \nmade it. In [11], the author who had won three international competitions [5-7] claimed that \nhis deep neural network was trained with 3 million pixels with only 2 outputs: membrane \ndenoted as 1 and non-membrane denoted as 0. The output is a prediction map which looks the \nsame as the probability map generated by another competitor [12] and the popular method to \ngenerate the probability map is described in [13]. As can be seen, the prediction map is \ngenerated based on the features of the image instead of the pixels of the image. Therefore, \ndeep learning works on the feature level instead of the pixel level for image segmentation. \nAccordingly, deep learning could not achieve pixel level accuracy without further processing. \nIt is seen from the illustration in [14], there were serious segmentation errors in the final \nsegmentation result by the deep learning method proposed in [11]. In another object detection \ncompetition [15], the same author of [11] used his deep learning method to detect mitosis in \nbreast cancer histology images. At this time, the achieved F-measure accuracy was only \n78.2%. So far, the author did not disclose any related source codes for other researchers to \nreproduce the segmentation accuracy or to verify the proposed deep learning method, which \nindicates that the international competitions based on the submitted results might not be \nreliable [16].  \nIn [17], the authors gave a comprehensive survey of deep learning in cardiology and \nconcluded that a complete theoretical understanding of deep learning is not yet available. \nTherefore, the authors claimed that successful application of artificial intelligence (AI) in the \nmedical field relies on achieving interpretable models and big datasets. Unfortunately, deep \n4 \n \nlearning is a black box instead of an interpretable model. Because of the black box property \nof deep learning, its performance might vary significantly in segmenting the same images \nwith different implementations. We take the segmentation of the left ventricle (LV) by deep \nlearning methods as a typical example and summarize the accuracies achieved by deep \nlearning methods [18-47] based on public magnetic resonance image (MRI) and \ncomputational tomography (CT) datasets in Table 2. For the most popular dataset (2009 \nMICCAI challenge), the best accuracy achieved so far by deep learning based methods was \n0.94 [21], where the deep learning was used to yield an approximation segmentation and then \ndeformable model was used to refine the final segmentation result. The results in [21] verified \nthe point that deep learning does not have the ability to achieve an accurate image \nsegmentation result. However, the reported accuracy in [21] seems to be exaggerated. The \ninferred shape by deep learning is used as initialization contour for the evolvement of the \ndeformable model. However, the accuracy of the deformable model is determined by the \ngenerated edge map instead of its initialization contour. Therefore, the final accuracy of the \ncombined method should be similar to that of the deformable model based methods. On the \ncontrary, the reported accuracy was much better than those reported by the deformable model \nbased methods. In and after 2018, the reported accuracy on the 2009 MICCAI challenge \ndatasets have dropped to 0.91 [27] and 0.9 [28]. \n \nTable 2. Comparison of the accuracies achieved by deep leaning methods on public left ventricle datasets (Please \nnote that the results in Table 2 should not be trusted fully since the accuracy is computed by the authors \nthemselves or by the organizers based on the submitted results without verifying the corresponding source \ncodes.) \nDatasets\\Performance \nDICE similarity coefficients achieved in \nor before 2017 \nDICE similarity coefficients \nachieved in or after 2018 \nDatasets from York \nUniversity(MRI dataset) \n0.75 [18]; 0.91[19] \nNA \nSunnybrook 2009 MICCAI \nchallenge (MRI dataset) \n0.93 \n[19];0.91[20];0.94[21];0.88[22];0.92 \n[23];0.85 [24];0.88[25];0.9 [26] \n0.91[27];0.9[28] \nACDC challenge STACOM \n2017 \n(MRI dataset) \n0.86[29];0.97[30];0.95[31];0.96[32];0.9\n5[33];0.95[34];0.95[35];0.92[36];0.82[3\n7];0.94[38] \n0.92 [39-40] \nMMWHS Challenge \nSTACOM 2017 (CT dataset) \n80.9[41];90.8[42] \n79.3 [43] \nSTACOM 2011 challenge(MRI \ndataset) \n0.92 [44]; NA [45] \nNA \nSTACOM 2013 challenge(MRI \ndataset) \n0.82 [46] \nNA \nSTACOM 2018 challenge(MRI \ndataset) \nNA \n0.85 [47] \n \n \nIn 2017, statistical atlases and computational modeling (STACOM) of the heart workshop \nheld two challenges: automated cardiac diagnosis challenge (ACDC) and multi-modality \nwhole heart segmentation (MMWHS) challenge. The same as other international \nchallenges or competitions, the competition teams were requested to submit their \nsegmentation results to evaluate the accuracy of their methods. For the first time, deep \nlearning based methods achieved the unprecedented accuracy by many teams, e.g. 0.97 \n[30], 0.96 [32] and 0.95 [32, 33-35]. One thing in common about the deep learning \nmethods designed by these teams is that deep learning is the sole technique for \nsegmentation and no other segmentation techniques were used for refinement. The \norganizer thus claimed that the problem of LV segmentation has been solved. Many of \nthe teams had announced that they disclosed their codes or implementation details via the \nGitbub. However, the author‚Äôs team did not find any useful thing from their disclosed \ncodes and they did not respond to the email either. Many other researchers could not \n5 \n \nreproduce similar accuracies either. For instance, the achieved accuracy on the ACDC \ndatasets dropped from 0.95 to 0.92 in 2018 [39]. The achieved accuracy on the MMWHS \ndatasets dropped from 0.91[42] to 0.79 [43] in 2018. In the latest STACOM 2018 \nchallenge, the achieved accuracy for LV segmentation dropped to 0.85 [47]. As can be seen, \nthe international competitions based on the submitted results are not reliable [16]. The reasons \nthat made the accuracies achieved during STACOM 2017 so high include that (1), it is easy \nto improve the accuracy of the submitted results semi-automatically or manually; (2), \nThe winners of the international challenges have a bright career future, which allure them \nto be dishonest. (3), most of them believe in that deep learning could actually reach that \nhigh accuracy even they did not achieve it during the practical implementations. Their \nblind faith in deep learning mainly comes from the great success of deep learning in \nclassifying limited categories. As pointed out in this paper, deep learning could only give \nan approximation or a rough predication map instead of an accurate segmentation result \nfor image segmentation.  \n4. Comparison of deep learning methods with traditional methods \nFirstly, we compare the deep learning based methods with the slope difference distribution \n(SDD) threshold selection method proposed in [48]. The performance of SDD is affected by \nits parameters, the bandwidth B and the fitting number N significantly [48]. The segmentation \nresults of selecting the bandwidth as 5 and the fitting number as 15 were published in [49] \nwhile its source codes had been made publicly accessible in 2017. Without notifying the \nreviewers that the source codes were available, the submitted paper had been rejected by \nmany journals with almost the same reason that deep learning was better. However, the \nachieved accuracy by SDD in [49] is 0.9246 which is higher than most reported accuracies by \ndeep learning methods. When the parameters of SDD are changed to 11 and 11 respectively, \nthe segmentation accuracy could be improved to 0.95, which is greater than the accuracies \nreported by all deep learning methods on the same public datasets [50]. However, the \nselection rules of SDD become much more complex with a higher bandwidth and a lower \nfitting number. It seems that many reviewers could not accept the fact that deep learning is \nnot the best image segmentation technique. It is not known why they believe in deep learning \nso deeply? Maybe they are overwhelmed by the great success of deep learning in other fields. \nSo far, we only found two deep learning methods [51-52] that have disclosed their source \ncodes and their trained models [53-54]. However, the reproduced accuracy is significantly \nlower than their claimed accuracy in their papers as shown in Table 3. On the contrary, SDD \nbased method [55] achieved significantly better accuracy with freely available MATLAB \nsource codes [56] for verification.  \n \nTable 3. Comparison of DICE similarity coefficients achieved by the disclosed codes of two deep learning methods \nand the SDD method on the ACDC dataset. \nValidation\\Methods \nDeep learning [51,53] \nDeep learning [52,54] \nSDD [55,56] \nPublished accuracy \n94% [51] \n94% [52] \n95.44% [55] \nSource codes accuracy \n90.28% [53] \n87.13% [54] \n95.44% [56] \n \nSecondly, we compare the deep learning based methods with other segmentation methods \nin segmenting different medical images. The quantitative comparisons are shown in Table 4. \nThe achieved Jaccard Index accuracy by the deep learning method for ventricle \nsegmentation in [47] was only 0.77 which is lower than that of a tracking method [57]. Deep \nlearning was combined with surface evolution to segment CT livers in [58]. Deep learning \nwas combined with graph cut to segment the CT livers in [59]. Even combined with deep \nlearning for prediction, the reported liver segmentation accuracy was still lower than the \npreviously reported liver segmentation accuracy by the non-deep-learning methods [60-61]. \nFor the liver tumor detection, the reported accuracy by deep learning was only 69% [62] \ncompared to 88.9% computed by a non-deep-learning method [63]. Deep learning was also \nused for cell segmentation [64] and cell tracking [66]. Deep learning was combined with \n6 \n \nwatershed for cell segmentation in [58] and the reported accuracy was only 86%. In addition, \nit was reported in [65] that the accuracy of cell segmentation by support vector machine was \nmore robust than deep learning. In [66], deep learning was used for cell tracking and it was \nreported that only one cell could be tracked, which is much more inefficient than traditional \ncell tracking methods [67-68].  \n The cases described above are only the tip of the iceberg, there are also many cases in \nwhich deep learning performed very poorly or it was outperformed by other methods. \nHowever, it is usually difficult to publish these results since the traditional methods may be \nlack of originality for publishing. In many cases, the claimed accuracy in the published paper \nand the reported accuracy in international competitions might make people think that it is the \ninability of the researcher instead of inability of deep learning for the failure of a \nsegmentation task. Almost no authors that use deep learning for image segmentation would \nlike to disclose enough data, source codes and the required information to reproduce their \nresults. Besides the deliberate exaggeration, there might be mistakes that caused the reported \naccuracy to be unreliable.  \n \nTable 4. Comparisons of the accuracies of deep leaning methods and non-deep-learning methods in segmenting the \nmedical images (Please note that the results in Table 4 should not be trusted fully since the accuracy is \ncomputed by the authors themselves without verifying the corresponding source codes.) \nCompetitions \nDeep learning methods \nnon-deep-learning methods \nMRI Ventricle segmentation \nJaccard Index 0.77 [47] \nJaccard Index 0.84 [57] \nCT liver segmentation on \nMICCAI-Sliver07 test set \nScore 79.3% [58]; \n77.8% [59] \nScore 79.6%[60] \nCT liver segmentation on \n3Dircabd database \nVOE 9.36¬±3.34 [59] \nVOE 9.15¬±1.44 [61] \nLiver tumor segmentation \nDICE 69% [62] \nDICE 88.9% [63] \nCell segmentation \nOR 0.83% [64] \nOR 0.69%[65] \nCell tracking \nCapable of tracking one cell at a \ntime [66] \nCapable of tracking hundreds of \ncells simultaneously [67-68] \nPlease note: Jaccard Index, DICE and Score: the higher the better; VOE and OR: the lower the better. \n \n5. Discussion \nThe fetishistic devotion to deep learning is mainly caused by its overhype. This overhype is \npartially caused by the international challenges. Deep learning monopolized the champions of \nall the international challenges. Ironically, the organizers of the international challenges only \nevaluate the submitted results instead of the source codes or the software to rank the \ncompetitors. As a result, deep learning won almost all the international image segmentation \nchallenges. According to Gary Marcus, the professor at New York University, ‚ÄúDeep learning \nsystems are most often used as classification system in the sense that the mission of a typical \nnetwork is to decide which of a set of categories (defined by the output variables on the \nneural network) a given input belongs to.‚Äù [69]. According to our analysis in this paper, deep \nlearning is a discrete, but not continuous regression model. As a discrete non-linear mapping \nfunction, the performance of deep learning will be affected significantly when the number of \nthe outputs approaches infinity. Thus, deep learning could not achieve pixel-level accuracy \nfor image segmentation as traditional methods can.  \nCompared to the regression model, deep learning has the major advantage that deep \nlearning is more robust in estimating the discrete response variables for the linear indivisible \ndata. Up to now, deep learning has been recognized as the most powerful classification tool. \nHowever, deep learning also has the following limitations. \n (1), deep learning determines a set of parameters instead of the exact mathematical \nfunctions to map the input variables into the output variables. Without the exact mathematical \nfunctions, not only deep learning could not be explained reasonably, but also the output \nvariables could not be determined as continuous response variables. Consequently, deep \nlearning could only map sufficient inputs into a finite number of categories. When the number \n7 \n \nof output is infinite or close to infinity, the performance of deep learning will be affected \nsignificantly.  \n (2), Lack of the exact mathematical mapping, deep learning could not yield the ideal \noutputs even when it is tested with the inputs used during the training process or tested with \nthe ground truth. \n(3), Lack of the exact mathematical mapping, deep learning is only powerful for the \nclassification applications, where sufficient input variables are available while the number of \noutput variables is comparatively limited. In information theory, there is Shannon theorem \nthat tells the maximum rate at which information can be transmitted over the channel with the \nspecified bandwidth in the presence of noise. In deep learning, there is no theorem to \ndetermine the maximum number of the output variables when the input variables are known, \nwhich indicates that deep learning is immature.   \n(4), the training process of deep learning relies on large amounts of human-annotated \ndata to determine the parameters. However, when different annotated training data are used, \nthe performance of deep learning may vary significantly.  \n(5), there is no known efficient training algorithms with provable guarantees for deep \nlearning and its success relies on descent algorithms, such as coordinate, gradient or \nstochastic gradient descent [2]. \n(6), since there are no guaranteed rules for the training process of deep learning, deep \nlearning is easy to be overhyped or utilized by people who conduct false advertising for their \ndeep learning based techniques or products.  \n \n6. Conclusion \nIn conclusion, deep learning could only yield a semantic-level predication map for image \nsegmentation and relies on other image segmentation techniques to obtain the pixel-level \naccuracy. When the number of the outputs approaches infinity, the approximating ability or \nthe classification ability of deep learning will go down significantly. Deep learning became so \nhot not because it is perfect or omnipotent, but because no alternative techniques are available \nyet. Excessive overhype of deep learning is partially caused by some international challenges \nthat have significant scientific impact, but do not evaluate the competitors‚Äô algorithms strictly.  \nReferences \n[1]  Y. Bengio et al., Towards biologically plausible deep learning, arXiv: 1502.04156 (2015). \n[2]  E. Abbe and C. Sandon, Provable limitations of deep learning, arXiv: 1812.06369. pp. 1-52, 2018 \n[3] T. Laszl√≥, Phone Recognition with Hierarchical Convolutional Deep Maxout Networks. EURASIP Journal on \nAudio, Speech, and Music Processing. 25, (2015). \n[4]  O. Abdel-Hamid et al., Convolutional Neural Networks for Speech Recognition. IEEE/ACM Transactions on \nAudio, Speech, and Language Processing. Vol. 22, No. 10, pp. 1533‚Äì1545, (2014). \n[5]  D. Ciresan, U. Meier, J. Schmidhuber, Multi-column deep neural networks for image classification. 2012 IEEE \nConference on Computer Vision and Pattern Recognition: pp. 3642‚Äì3649. (2012).  \n[6] D. Cire≈üan et al., Multi-column deep neural network for traffic sign classification. Neural Networks. 32: 333‚Äì\n338, (2012).  \n[7] D. Cire≈üan, U. Meier, Multi-column deep neural network for offline handwritten chinese character \nclassification. 2015 International Joint conference on Neural networks. Killarney, 2015, pp.1-6. \n[8] A. Krizhevsky, I. Sutskever, G.E. Hinton, ImageNet classification with deep convolutional neural \nnetworks. Communications of the ACM. 60(6), pp. 84‚Äì90, (2017). \n[9] O. Russakovsky et al., \"ImageNet Large Scale Visual Recognition Challenge\". International journal of computer \nvision, Vol. 115, Issue 3, pp. 211-252, (2015). \n[10] E. Shelhamer, J. Long, T. Darrell, Fully Convolutional Networks for Semantic Segmentation. IEEE Transactions \non Pattern Analysis and Machine Intelligence, vol. 39, no. 4, pp. 640-651, (2017). \n[11] D. Ciresan et al., Deep neural networks segment neuronal membranes in electron microscopy images.  Advances \nin Neural Information Processing Systems 25, pp. 2843‚Äì2851, (2012). \n[12] D. Laptev et al., Ansisotropic ssTEM image segmentation using dense correspondence across sections. Med \nImage Comput Comput Assist Interv. , 15(Pt 1):323-30, (2012). \n[13] P. Arbelaez et al., Contour Detection and Hierarchical Image Segmentation, in IEEE Transactions on Pattern \n8 \n \nAnalysis and Machine Intelligence, vol. 33, no. 5, pp. 898-916, (2011). \n[14] A. C. Ignacio et al., Crowdsourcing the creation of image segmentation algorithms for connectomics, Frontiers \nin neuroanatomy, vol. 9 No.142, (2015). \n[15] D. Ciresan, et al., Mitosis Detection in Breast Cancer Histology Images using Deep Neural \nNetworks. Proceedings MICCAI. (2013). \n[16] Maier-Hein, L., Eisenmann, M., Reinke, A. et al. Why rankings of biomedical image analysis competitions \nshould be interpreted with care. Nat Commun 9, 5217 (2018). https://doi.org/10.1038/s41467-018-07619-7 \n[17] P. Bizopoulos and D. Koutsouris, \"Deep Learning in Cardiology\", IEEE Reviews in Biomedical Engineering, \nvol. 12, pp. 168-193, 2019. \n[18] X. L. Yang L. Gobeawan, S.Y. Yeo, W.T. Tang, Z.Z. Wu, Y. Su, Automatic segmentation of left ventricular \nmyocardium by deep convolutional and de-convolutional neural networks, 2016 Computing in Cardiology \nConference, Vancouver, BC, pp. 81-84, 2016. \n[19] X.L. Yang, Z. Zeng, and Y. Su, ‚ÄúDeep convolutional neural networks for automatic segmentation of left ventricle \ncavity from cardiac magnetic resonance images,‚Äù IET Comput. Vis., vol. 11, no. 8, pp. 643‚Äì649, 2017.  \n[20] H.F. Hu et al, Automatic segmentation of the left ventricle in cardiac MRI using local binary fitting model and \ndynamic programming techniques, PloS one, vol. 9, No.12, pp. e114760 , 2014.  \n[21] M. Avendi, A. Kheradvar, and H. Jafarkhani, ‚ÄúA combined deep-learning and deformable-model approach to \nfully automatic segmentation of the left ventricle in cardiac MRI,‚Äù Med. Image Anal., vol. 30, pp. 108‚Äì119, \n2016 \n[22] L. K. Tan, Y. M. Liew, E. Lim, and R. A. McLaughlin, ‚ÄúCardiac left ventricle segmentation using convolutional \nneural network regression,‚Äù in Proc. IEEE EMBS Conf. Biomed. Eng. Sci., 2016, pp. 490‚Äì493.  \n[23] L. V. Romaguera, M. G. F. Costa, F. P. Romero, and C. F. F. Costa Filho, ‚ÄúLeft ventricle segmentation in cardiac \nMRI images using fully convolutional neural networks,‚Äù in Medical Imaging 2017: Computer Aided Diagnosis, \nvol. 10134. Bellingham, WA, USA: Int. Soc. Opt. Photon., 2017, p. 101342Z.  \n[24] C. Rupprecht, E. Huaroc, M. Baust, and N. Navab, ‚ÄúDeep active contours,‚Äù 2016. arXiv:1607.05074. \n[25] T. Anh Ngo and G. Carneiro, ‚ÄúFully automated non-rigid segmentation with distance regularized level set \nevolution initialized and constrained by deep-structured inference,‚Äù in Proc. IEEE Conf. Comput. Vis. Pattern \nRecognit., 2014, pp. 3118‚Äì3125. \n[26] A. H. Curiale, F. D. Colavecchia, P. Kaluza, R. A. Isoardi, and G. Mato, ‚ÄúAutomatic myocardial segmentation \nby using a deep learning network in cardiac MRI,‚Äù in Proc. IEEE Comput. Conf., XLIII Latin Amer, 2017, pp. 1‚Äì\n6.  \n[27] L. Qi, X. Y. Liu, B.Q. Yang and L. S. Xu, ‚ÄúSegmentation of left ventricle endocardium based on transfer \nlearning of fully convolutional networks‚Äù, Journal of northeastern University (Natural science), Vo. 39, Issue, 11, \npp. 1577-1582, 2018. \n[28] H.F. Hu, N. Pan, J.Y. Wang, T.L. Yin, R.Z. Ye, Automatic segmentation of left ventricle from cardiac MRI via \ndeep learning and region constrained dynamic programming, Neurocomputing, Vol. 347, pp. 139-148, 2019. \n[29] Chen A. et al. (2018) Transfer Learning for the Fully Automatic Segmentation of Left Ventricle \nMyocardium in Porcine Cardiac Cine MR Images. In: Pop M. et al. (eds) Statistical Atlases and \nComputational Models of the Heart. ACDC and MMWHS Challenges. STACOM 2017. Lecture Notes in \nComputer Science, vol. 10663. Springer, Cham \n[30] Zotti, C., Luo, Z., Humbert, O., Lalande, A., & Jodoin, P.-M. (2018). GridNet with Automatic Shape Prior \nRegistration for Automatic MRI Cardiac Segmentation. Statistical Atlases and Computational Models of the \nHeart. ACDC and MMWHS Challenges, 73‚Äì81.doi:10.1007/978-3-319-75541-0_8  \n[31] Grinias E., Tziritas G. (2018) Fast Fully-Automatic Cardiac Segmentation in MRI Using MRF Model \nOptimization, Substructures Tracking and B-Spline Smoothing. In: Pop M. et al. (eds) Statistical Atlases \nand Computational Models of the Heart. ACDC and MMWHS Challenges. STACOM 2017. Lecture Notes \nin Computer Science, vol. 10663. Springer, Cham \n[32] Wolterink, J. M., Leiner, T., Viergever, M. A., & I≈°gum, I. (2018). Automatic Segmentation and Disease \nClassification Using Cardiac Cine MR Images. Statistical Atlases and Computational Models of the Heart. \nACDC and MMWHS Challenges, 101‚Äì110.doi:10.1007/978-3-319-75541-0_11  \n[33] 33ÔºåBaumgartner C.F., Koch L.M., Pollefeys M., Konukoglu E. (2018) An Exploration of 2D and 3D \nDeep Learning Techniques for Cardiac MR Image Segmentation. In: Pop M. et al. (eds) Statistical Atlases \nand Computational Models of the Heart. ACDC and MMWHS Challenges. STACOM 2017. Lecture Notes \nin Computer Science, vol. 10663. Springer, Cham \n[34] Isensee F., Jaeger P.F., Full P.M., Wolf I., Engelhardt S., Maier-Hein K.H. (2018) Automatic Cardiac \nDisease Assessment on cine-MRI via Time-Series Segmentation and Domain Specific Features. In: Pop \nM. et al. (eds) Statistical Atlases and Computational Models of the Heart. ACDC and MMWHS \nChallenges. STACOM 2017. Lecture Notes in Computer Science, vol 10663. Springer, Cham \n[35] Isensee F., Jaeger P.F., Full P.M., Wolf I., Engelhardt S., Maier-Hein K.H. (2018) Automatic Cardiac \nDisease Assessment on cine-MRI via Time-Series Segmentation and Domain Specific Features. In: Pop \nM. et al. (eds) Statistical Atlases and Computational Models of the Heart. ACDC and MMWHS \nChallenges. STACOM 2017. Lecture Notes in Computer Science, vol 10663. Springer, Cham \n[36] Khened M., Alex V., Krishnamurthi G. (2018) Densely Connected Fully Convolutional Network for \nShort-Axis Cardiac Cine MR Image Segmentation and Heart Diagnosis Using Random Forest. In: Pop M. \net al. (eds) Statistical Atlases and Computational Models of the Heart. ACDC and MMWHS Challenges. \n9 \n \nSTACOM 2017. Lecture Notes in Computer Science, vol 10663. Springer, Cham \n[37] Yang X., Bian C., Yu L., Ni D., Heng PA. (2018) Class-Balanced Deep Neural Network for Automatic \nVentricular Structure Segmentation. In: Pop M. et al. (eds) Statistical Atlases and Computational Models \nof the Heart. ACDC and MMWHS Challenges. STACOM 2017. Lecture Notes in Computer Science, vol. \n10663. Springer, Cham \n[38] Jang Y., Hong Y., Ha S., Kim S., Chang HJ. (2018) Automatic Segmentation of LV and RV in Cardiac \nMRI. In: Pop M. et al. (eds) Statistical Atlases and Computational Models of the Heart. ACDC and \nMMWHS Challenges. STACOM 2017. Lecture Notes in Computer Science, vol. 10663. Springer, Cham \n[39] B.Y. Song and J. Tian, ‚ÄúAutomatic cardiac bi-ventricle MRI segmentation and disease classification using deep \nlearning‚Äù, thesis of XiDian University.  \n[40] Yakun  Chang,  Baoyu  Song,  Cheolkon  Jung,  and  Liyu  Huang. Automatic Segmentation and Cardiopathy \nClassification in Cardiac Mri Images Based on Deep Neural Networks [C]. IEEE International Conference on \nAcoustics, Speech and Signal Processing. April 15-20, 2018. \n[41] Yang X., Bian C., Yu L., Ni D., Heng PA. (2018) 3D Convolutional Networks for Fully Automatic Fine-\nGrained Whole Heart Partition. In: Pop M. et al. (eds) Statistical Atlases and Computational Models of the \nHeart. ACDC and MMWHS Challenges. STACOM 2017. Lecture Notes in Computer Science, vol 10663. \nSpringer, Cham \n[42] Payer C., ≈†tern D., Bischof H., Urschler M. (2018) Multi-label Whole Heart Segmentation Using CNNs \nand Anatomical Label Configurations. In: Pop M. et al. (eds) Statistical Atlases and Computational \nModels of the Heart. ACDC and MMWHS Challenges. STACOM 2017. Lecture Notes in Computer \nScience, vol 10663. Springer, Cham \n[43] T. Liu, Y. Tian, S. Zhao, X. Huang and Q. Wang, \"Automatic Whole Heart Segmentation Using a Two-Stage U-\nNet Framework and an Adaptive Threshold Window,\" IEEE Access, vol. 7, pp. 83628-83636, 2019.  \n[44] P.V. Tran, A Fully Convolutional Neural Network for Cardiac Segmentation in Short-Axis MRI. \narXiv:1604.00494 [cs], 2016. \n[45]  L. K. Tan, Y. M. Liew, E. Lim, and R. A. McLaughlin, ‚ÄúConvolutional neural network regression for short-axis \nleft ventricle segmentation in cardiac cine MR sequences,‚Äù Med. Image Anal., vol. 39, pp. 78‚Äì86, 2017.  \n[46] H. Yang, J. Sun, H. Li, L. Wang, and Z. Xu, ‚ÄúDeep fusion net for multiatlas segmentation: Application to cardiac \nMR images,‚Äù in International Conference on Medical Image Computing and Computer-Assisted Intervention. \nNew York, NY, USA: Springer, 2016, pp. 521‚Äì528. \n[47] L. K. Tan, Y. M. Liew, E. Lim, and R. A. McLaughlin, ‚ÄúConvolutional neural network regression for short-axis \nleft ventricle segmentation in cardiac cine mr sequences,‚Äù Medical image analysis, vol. 39, pp. 78‚Äì86, 2017. \n[48]  Z.Z. Wang. \"A new approach for segmentation and quantification of cells or nanoparticles.''  IEEE Trans. Ind. \nInform. vol.12, no.3, pp.962-971, 2016. \n[49] Z.Z. Wang, Automatic and Optimal Segmentation of the Left Ventricle in Cardiac Magnetic Resonance Images \nIndependent of the Training Sets, IET image processing, 10.1049/iet-ipr.2018.5878, 2019 \n[50] Z. Wang, \"Automatic Localization and Segmentation of the Ventricles in Magnetic Resonance Images,\" in IEEE \nTransactions on Circuits and Systems for Video Technology, doi: 10.1109/TCSVT.2020.2981530. \n[51] Khened, M., Kollerathu, V. A., & Krishnamurthi, G. (2019). Fully convolutional multi-scale residual DenseNets \nfor cardiac segmentation and automated cardiac diagnosis using ensemble of classifiers. Medical image \nanalysis, 51, 21‚Äì45. https://doi.org/10.1016/j.media.2018.10.004 \n[52] Bai, W., Sinclair, M., Tarroni, G., Oktay, O., Rajchl, M., Vaillant, G., Lee, A. M., Aung, N., Lukaschuk, E., \nSanghvi, M. M., Zemrak, F., Fung, K., Paiva, J. M., Carapella, V., Kim, Y. J., Suzuki, H., Kainz, B., Matthews, \nP. M., Petersen, S. E., Piechnik, S. K., ‚Ä¶ Rueckert, D. (2018). Automated cardiovascular magnetic resonance \nimage analysis with fully convolutional networks. Journal of cardiovascular magnetic resonance : official \njournal of the Society for Cardiovascular Magnetic Resonance, 20(1), 65. https://doi.org/10.1186/s12968-018-\n0471-x \n[53] https://github.com/mahendrakhened/Automated-Cardiac-Segmentation-and-Disease-Diagnosis \n[54] https://github.com/baiwenjia/ukbb_cardiac \n[55] Z.H. Wang and Z.Z. Wang, \"Fully automated segmentation of the left ventricle in in Magnetic Resonance \nImages,\" 2020,arXiv:2007.10665 \n[56] https://www.mathworks.com/matlabcentral/fileexchange/78417-sdd-lv-segmentation-for-comparison-with-dl-\nand-cnn-methods \n[57] Li, B., Liu, Y., Occleshaw, C.J., Cowan, B.R., Young, A.A., 2010. In-line automated tracking for ventricular \nfunction with magnetic resonance imaging. JACC 3, 860‚Äì866. doi:10.1016/j.jcmg.2010.04.013.  \n[58]  P. Hu et al., Automatic 3D Liver Segmentation Based on Deep Learning and Globally Optimized Surface \nEvolution, Phys. Med. Biol., vol. 61, no. 24, pp. 8676‚Äì8698 (2016). \n[59]  F. Lu et al., Automatic 3D liver location and segmentation via convolutional neural network and graph cut, Int J \nComput Assist Radiol Surg, Vol. 12, No. 2, pp.1-12 (2017).  \n[60] S.D. S. Al-Shaikhli, M.Y. Yang and B. Rosenhahn, Automatic 3d liver segmentation using sparse representation \nof global and local image information via level set formulation, arXiv: 1508.01521, pp. (2015). \n[61] G. Li et al., Automatic liver segmentation based on shape constraints and deformable graph cut in CT images. \nIEEE Transactions on Image Processing, Vol. 24, No. 12, , 5315-5329 (2015). \n[62] G. Chlebus et al. Automatic liver tumor segmentation in CT with fully convolutional neural networks and \n10 \n \nobject-based postprocessing. Scientific Reports, Vol. 8, No. 15497, pp. 1-7, 2018. \n[63] L. Massoptier and S. Casciaro, A new fully automatic and robust algorithm for fast segmentation of liver tissue \nand tumors from CT scans, European radiology, Vol. 18, No.8, pp. 1658-1665, (2008). \n[64] Y. AI-Kofahi et al., A deep learning-based algorithm for 2-D cell segmentation in microscopy images. BMC \nbioinformatics, Vol. 19, No. 365, pp. 1-11 (2018).  \n[65] X. Zheng et al., Fast and robust segmentation of white blood cell images by self-supervised learning. Micron, \nVol. 107, No. 1, pp. 55-71(2018).  \n[66] T. He et al., Cell tracking using deep neural networks with multi-task learning. Image and vision computing, Vol. \n60, No. 1, pp. 142-153 (2017). \n[67] V. Ulman et al., An objective comparison of cell-tracking algorithms. Nature Methods, Vol. 14, No. 12, pp.1141-\n1152, 2017. \n[68] Z.Z. Wang, L.J. Yin and Z.H. Wang, \"A New Approach for cell detection and tracking. \" IEEE Access. 7:99889 - \n99899, 2019. \n[69]  G. Marcus, Deep learning: a critical appraisal, arXiv:1801.00631, pp. 1-27, (2018). \n \n \n11 \n \n",
  "categories": [
    "cs.CV",
    "cs.LG"
  ],
  "published": "2019-04-16",
  "updated": "2020-07-23"
}