{
  "id": "http://arxiv.org/abs/1904.10337v1",
  "title": "MinCall - MinION end2end convolutional deep learning basecaller",
  "authors": [
    "Neven Miculinić",
    "Marko Ratković",
    "Mile Šikić"
  ],
  "abstract": "The Oxford Nanopore Technologies's MinION is the first portable DNA\nsequencing device. It is capable of producing long reads, over 100 kBp were\nreported. However, it has significantly higher error rate than other methods.\nIn this study, we present MinCall, an end2end basecaller model for the MinION.\nThe model is based on deep learning and uses convolutional neural networks\n(CNN) in its implementation. For extra performance, it uses cutting edge deep\nlearning techniques and architectures, batch normalization and Connectionist\nTemporal Classification (CTC) loss. The best performing deep learning model\nachieves 91.4% median match rate on E. Coli dataset using R9 pore chemistry and\n1D reads.",
  "text": "MinCall — MinION\nend2end convolutional deep learning basecaller\nNeven Miculini´c, Marko Ratkovi´c, and Mile ˇSiki´c\n{neven.miculinic, marko.ratkovic, mile.sikic}@fer.hr\nUniversity of Zagreb, Faculty of Electrical Engineering and Computing, Zagreb, Croatia\nAbstract. The Oxford Nanopore Technologies’s MinION is the first portable DNA sequencing\ndevice. It is capable of producing long reads, over 100 kBp were reported. However, it has\nsignificantly higher error rate than other methods. In this study, we present MinCall, an end2end\nbasecaller model for the MinION. The model is based on deep learning and uses convolutional\nneural networks (CNN) in its implementation. For extra performance, it uses cutting edge\ndeep learning techniques and architectures, batch normalization and Connectionist Temporal\nClassification (CTC) loss. The best performing deep learning model achieves 91.4% median\nmatch rate on E. Coli dataset using R9 pore chemistry and 1D reads.\nAvailability: MinCall is available at https://github.com/nmiculinic/minion-basecaller/\nunder the MIT license.\nKeywords: Basecaller, MinION, R9, CNN, CTC, Next generation sequencing\n1\nIntroduction\nIn recent years, deep learning methods significantly improved the state-of-the-art in multiple domains\nsuch as computer vision, speech recognition, and natural language processing [14,16]. In this paper,\nwe present application of deep learning for DNA basecalling problem.\nOxford Nanopore Technology’s MinION nanopore sequencing platform [20] is the first portable\nDNA sequencing device. It produces longer reads than competing technologies. In addition, it enables\nreal-time data analysis which makes it suitable for various applications.\nAlthough MinION is able to produce long reads, even up to 882 kb [18,19], they have an error\nrate of 10% or higher. This has been somewhat alleviated with new R9 pore model, which replaced\nprevious R7 one. In this paper, we show that this error rate can be reduced by our approach with\nthe properly trained neural network model.\n1.1\nSequencing overview\nConceptually, the MinION sequencer is a variation on the now standard shotgun sequencing approach.\nFirst, DNA is sheared into smaller fragments and adapters are ligated to either end of the fragments.\nThe resulting DNA fragments pass through a protein embedded in a membrane via a nanometre-sized\nchannel, a nanopore. A single DNA strand passes through the pore. Optionally, hairpin protein adapter\ncan connect two DNA strands, allowing both template and complement read passing through the\nnanopore sequentially for more accurate reads. This technique is referred to as 2D reads. However or\nfocus is on 1D reads containing only single-strand DNA and no hairpin adapter. As DNA strand passes\nthrough the nanopore, they are propelled by the current. However, this current varies depending on\nspecific nucleotide context within the nanopore, changing its resistance. Currency is sampled multiple\ntimes per second, 4000Hz in our dataset, and from this data, passing DNA fragment is deduced. By\ndesign, pores are 6 nucleotides wide, and many models use this information internally. However, we\ncreated a model independent of pore width which requires less feature engineering.\narXiv:1904.10337v1  [q-bio.GN]  22 Apr 2019\nII\nNeven Miculini´c, Marko Ratkovi´c, Mile ˇSiki´c\n1.2\nRelated work\nThe core of the decoding process is the basecalling step, that is translating the current samples to the\nnucleotide sequence. Nowadays there are multiple basecalling options, both official and unofficial ones.\nEarlier models were Hidden Markov model (HMM)-based where hidden state modeled DNA\nsequence of length 6 (6-mer) in the nanopore. Pore models were used in computing emission probabil-\nities. [17,21,24,25] and the recent open source HMM-based basecaller Nanocall [4]. Modern basecallers\nuse RNN base models, and in this paper, we used CNN with beam search instead.\nWe compared our model on R9 chemistry with Metrichor (HMM-based approach), Nanonet1 and\nDeepNano [3] (RNN based approaches). Detailed basecaller overview can be found in [15].\n2\nDataset\nUsed dataset E.Coli K-12 strands from [18] has been previously passed through MinKNOW and has been\nbasecalled by Metrichor. Since the focus of this paper was 1D read analysis, only 1D reads were used.\nTable 1. Used dataset\nNumber of reads Total bases [bp]2 Whole genome size [bp]\nE. Coli\n164471\n1 481 687 490\n4 639 675\n2.1\nData preprocessing\nTo help the training process, raw signal is split into smaller blocks that are used as inputs. For each\nMetrichor basecalled event it is easy to determine the block it falls into using start field. Using this\ninformation output given by Metrichor can be determined for each block.\nTo correct errors produced by Metrichor and possibly increase the quality of data, each read is\naligned to the reference. This is done using aligner GraphMap [23] that returns the best position in\nthe genome, which is hopefully, the part of the genome from which the read originated.\nAlignment part in the genome is used as a target. Using CIGAR string returned by aligner we\ncan correct Metrichor data and get target output for each block. This process is shown in Fig. 1.\nTo eliminate the possibility of overfitting to the known reference, the model is trained and tested\non reads from different sources. Due to the limited amount of public available raw nanopore sequence\ndata, E. Coli was divided into two regions. Reads were split into train and test portions, depending on\nwhich region of E. Coli they align. If read aligns inside first 70% of the E. Coli, it is placed into train\nset, and if it aligns to the second portion, it is placed into test set. Reads whose alignment overlaps\ntrain and test region are not used. Important to note that E. Coli genome is cyclical, so reads with\nalignments that wrap over edges are also discarded. Total train set consist of over 110 thousand reads.\nDue to CTC merged nature during decoding, adjacent duplicates are merged into one, we preprocess\nthe target nucleotide sequence with surrogate nucleotides, such that each second repeated nucleotide\nis replaced with its surrogate. The example is provided in Fig. 2. All raw input data were normalized\nto zero mean and unit variance as it yields superior performance with neural networks.\n1 https://github.com/nanoporetech/nanonet/\n2 Total number of bases called by Metrichor\nMinCall — MinION end2end convolutional deep learning basecaller\nIII\nFig. 1. Dataset preparation\nTarget\n:\nA\nG\nA\nA\nA\nA\nA\nA\nA\nPreprocessed:\nA\nG\nA\nA’ A\nA’ A\nA’ A\nFig. 2. Target nucleotide sequence preprocessing\n3\nMethod\nInstead of opting for the traditional path using HMM or newly adopted RNN we tried using residual\nCNN (Convolutional neural networks) [9]. For loss, we used CTC (Connectionist Temporal Classifica-\ntion) [7] between basecalled and target sequence. Other building blocks used are Batch normalization\n(BN) [11] and pooling layers. Described model is implemented using tensorflow [1] and open source\nwarp-ctc [2] GPU CTC loss implementation.\nThe final model is a residual neural network consisting of 72 residual blocks BN3-ELU4 -CONV5-\nBN-ELU-CONV, to a grand total of 2 million parameters. The used model is a variant of architecture\nproposed in paper [10] with the difference of ELU being used as activation instead of ReLU as it is\nreported [22] to speeds up the learning process and improve accuracy as the depth increases.\nEach convolutional layer in this models uses 64 channels with kernel size 3. Because sequenced\nread is always shorter than the raw signal, pooling with kernel size two is used every 24 layers resulting\nin a reduction of dimensionality by factor 8. This is beneficial in faster learning, better generalization\nand increased basecalling speed.\nTraining the model is the minimization of previously described CTC loss. It was done using\nAdam [12] with default parameters, exponentially decaying learning rate starting from 10−3, decay\nrate of 5·10−2 over 100k steps6 and minibatch size 8. To prevent gradients exploding on bad inputs,\nthey were clipped to range [-2, 2]. We observed no overfitting due to large dataset size.\nDuring development, we tried ReLu and PrELU [8] with no significant result difference. Also, differ-\nent channel numbers, receptive field width, and various other hyperparameters have been tested during\nhyperparameter optimization. SigOpt [5]7 bayesian hyperparameter optimization library was used.\n3 Batch normalization\n4 Exponential Linear Unit\n5 1D convolutional layer\n6 We use tf.train.exponential decay where current learning rate, lr is lr=initial lr·decay rate\nglobal step\ndecay step\n7 https://sigopt.com/\nIV\nNeven Miculini´c, Marko Ratkovi´c, Mile ˇSiki´c\n4\nResults\nDeveloped tool was compared with other available basecallers that support R9 chemistry. This includes\nthird-party basecaller DeepNano and official basecallers by Oxford Nanopore (cloud-based Metrichor\nand Nanonet).\nThe exact error rate metric is unreliable since multiple pipeline tools could be the issue. First the\nsample is prepared, hopefully, uncontaminated and matching reference genome as close as possible then\nsequenced using the MinION device obtaining raw data. Next, our model (or any other) is applied to\nbasecall the sequences. To evaluate error rate metric basecalled read is aligned to the reference genome\nusing GraphMap [23].\nThe fact that ground truth is not known makes evaluation difficult. Different methods for evaluation\nwere used to get clearer information about each basecaller. Different evaluation metrics are described\nand analyzed in the following subsections. For completeness, speed measurements are given in Table 2.\nTable 2. Base calling speeds measured in base pairs per second. Tested on Intel Xeon E5-2640 v2 @ 2Ghz\nwith NVIDIA Titan X Black GPU. DeepNano does not support GPU basecalling.\nMinCall Nanonet DeepNano\nSpeed CPU(bp/s) 1363.34\n897.49\n692.37\nSpeed GPU(bp/s) 6571.76\n3828.39\n-\n4.1\nPer read metrics\nA portion of the read length that aligns correctly is called match rate. Same goes for mismatches and\ninsertions. The sum of all matches, mismatches, and insertions is equal to the read length. Results\non E.Coli test set with Graphmap aligner are shown in Table 3. Furthermore, we plot Kernel Density\nEstimation(KDE) plots for each mentioned statistic on E. Coli dataset in Fig. 3.\nTable 3. Alignment specifications of E. Coli R9 basecalled reads using GraphMap\nMatch %\n(median)\nMismatch %\n(median)\nInsertion %\n(median)\nDeletion %\n(median)\nDeepNano 90.254762\n6.452852\n3.274420\n11.829965\nMetrichor\n90.560455\n5.688105\n3.660381\n8.328271\nNanonet\n90.607674\n5.608912\n3.652791\n8.299046\nMinCall\n91.408591\n5.019141\n3.477739\n7.471608\n4.2\nConsensus metrics\nIn Subsection 4.1 we showed metrics after the read is aligned to reference genome. In a second approach,\nwe reconstruct the original genome from basecalled reads.\nMinCall — MinION end2end convolutional deep learning basecaller\nV\nFig. 3. KDE plot for the distribution of percentage of alimnment operations for E. Coli\nConsensus from pileup Since the reference genome of E. Coli is known, we simply align all the\nreads to the genome, stack them on top of each other forming pileup of read bases. Using majority vote,\ndominant bases are called on each position. The resulting sequence is called consensus. When calling\nconsensus for deletions, there has to be a majority of deletions of the same length. Calling insertions\nhas an additional condition, the majority has to agree on both length and the bases of insertion. Fig. 4\nshows how consensus is called from pileup created from aligned reads. Pileup is stored in mpileup format.\nAll models show a slight bias towards deletions than insertions, but this may be the limitation of\ntechnology as it has been reported that deletion and mismatch rates for nanopore data are ordinarily\nhigher than insertion rates [23]. Results are shown in Table 4.\nFig. 4. Consensus from pileup\nVI\nNeven Miculini´c, Marko Ratkovi´c, Mile ˇSiki´c\nTable 4. Consensus specifications of E. Coli R9 basecalled reads\nTotal called\n[bp]\nCorrectly called\n[bp]\nMatch\n%\nSnp\n%\nInsertion\n%\nDeletion\n%\nDeepNano 1510244.0\n1493242.0\n98.8742 1.0044\n0.1214\n0.9041\nMetrichor\n1515893.0\n1502588.0\n99.1223 0.7464\n0.1313\n0.6300\nNanonet\n1414237.0\n1385515.0\n97.9691 1.5700\n0.4609\n1.5158\nMinCall\n1517828.0\n1506233.0\n99.2361 0.6474 0.1165\n0.5510\nConsensus from de novo assembly In this evaluation method, the consensus sequence is calculated\nusing de novo genome assembly. For this task, fast and accurate de novo genome assembler ra8 [26]\nwas used and obtained consensus sequence is compared to the reference using dnadiff present in the\nMumer9. The length of the reference, consensus sequence, number of contigs and percentages of aligned\nbases from the reference to the query and vice versa are shown in the Table 5. Average identity\nsummarizes how closely does the assembled sequence match the reference. This is run on full E. Coli\nsequence run for 1D template reads (∼160k reads), for our tool, Nanonet and Metrichor. Developed\ntool has shown an increase in quality of the assembled sequence over Metrichor by offering longer\nconsensus, higher identity percentage, and overall smaller edit distance10.\nTable 5. Assembly and consensus results for E. Coli\nMetrichor\nMinCall\nNanonet\nAln. bases ref. (bp) 4639641(100.00%) 4639612(100.00%) 4639031(99.99%)\nAln. bases query (bp) 4604787(100.00%) 4614351(100.00%) 4599745(99.99%)\nAvg. Identity\n98.76\n99.06\n98.47\nEdit distance\n60418\n46686\n74341\n5\nConclusion and further work\nIn this paper, we used CNN instead of already tried RNN or HMM approaches, which resulted in higher\naccuracy compared to other existing basecallers. Unlike HMM and RNN, there’s no explicit dependency\non previous hidden state, therefore this model is massively parallelizable and more sensible given data\nnature — that is we’re dealing with signal processing, not heavily context-depended language modeling.\nAll test are done on data for R9 chemistry, but the developed open source code could easily be\nadjusted and trained on R9.4 and newest R9.5 data when it becomes publicly available.\nCurrently, without support for newer sequencing data, this model has limited application. It can be\nused as a demonstration of a different approach to basecalling which yields promising results. As newer\nversions of basecallers by Oxford Nanopore do not offer any support for data sequenced with previous\n8 https://github.com/rvaser/ra\n9 https://github.com/garviz/MUMmer\n10 Calculated using https://github.com/isovic/racon/blob/master/scripts/edcontigs.py\nMinCall — MinION end2end convolutional deep learning basecaller\nVII\nversion of chemistries, this tool can be used to re-basecall that data and to improve the quality of\nreads retrospectively.\nFuture work includes experiments with recently proposed Scaled exponential linear units (SELU) [13]\nthat eliminate the need for normalization techniques such as used batch normalization. Possible im-\nprovements of the model include the combination of convolutions and attention mechanism proposed\njust recently in the paper [6] showing excellent results for tasks of language translation in both speed\nand accuracy. Another option could be the usage of stacked simple models, such as logistic regression\nand SVM to predict each nucleotide given raw signal context, similar to our deep learning model\npre-CTC layer, and use linear chain CRF for full sequence basecalling.\n6\nAcknowledgments\nWe’re grateful to various other people whose code, tools and advice we’ve used in completing this\nproject: Fran Juriˇsi´c, Ana Marija Selak, Ivan Sovi´c, Robert Vaser and Martin ˇSoˇsi´c.\nDuring this paper creation, we used Sigopt academic license for hyperparameter optimization. We\ngratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU\nused for this research. This work has been supported in part by Croatian Science Foundation under\nthe project UIP-11-2013-7353 ”Algorithms for Genome Sequence Analysis”.\nReferences\n1. Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., et al.: TensorFlow: Large-scale machine learning\non heterogeneous systems (2015), http://tensorflow.org/, software available from tensorflow.org\n2. Amodei, D., Anubhai, R., Battenberg, E., Case, C., et al.: Deep speech 2: End-to-end speech recognition\nin english and mandarin. CoRR abs/1512.02595 (2015), http://arxiv.org/abs/1512.02595\n3. Boˇza, V., Brejov´a, B., Vinaˇr, T.: DeepNano: Deep Recurrent Neural Networks for Base Calling in MinION\nNanopore Reads. ArXiv e-prints (Mar 2016)\n4. David, M., Dursi, L.J., Yao, D., Boutros, P.C., Simpson, J.T.: Nanocall: an open source basecaller for\noxford nanopore sequencing data. Bioinformatics p. btw569 (2016)\n5. Dewancker, I., McCourt, M., Clark, S., Hayes, P., Johnson, A., Ke, G.: A strategy for ranking optimization\nmethods using multiple criteria. In: Workshop on Automatic Machine Learning. pp. 11–20 (2016)\n6. Gehring, J., Auli, M., Grangier, D., Yarats, D., Dauphin, Y.N.: Convolutional sequence to sequence\nlearning (2017)\n7. Graves, A., Fern´andez, S., Gomez, F., Schmidhuber, J.: Connectionist temporal classification: labelling\nunsegmented sequence data with recurrent neural networks. In: Proceedings of the 23rd international\nconference on Machine learning. pp. 369–376. ACM (2006)\n8. He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectifiers: Surpassing human-level performance\non imagenet classification (2015)\n9. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition. pp. 770–778 (2016)\n10. He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual networks (2016)\n11. Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by reducing internal\ncovariate shift (2015)\n12. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization (2014)\n13. Klambauer, G., Unterthiner, T., Mayr, A., Hochreiter, S.: Self-normalizing neural networks (2017)\n14. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks.\nIn: Pereira, F., Burges, C.J.C., Bottou, L., Weinberger, K.Q. (eds.) Advances in Neural Information\nProcessing Systems 25, pp. 1097–1105. Curran Associates, Inc. (2012), https://goo.gl/UpFBv8\n15. de Lannoy, C.V., de Ridder, D., Risse, J.: A sequencer coming of age: De novo genome assembly using\nminion reads. bioRxiv (2017), http://www.biorxiv.org/content/early/2017/05/26/142711\nVIII\nNeven Miculini´c, Marko Ratkovi´c, Mile ˇSiki´c\n16. LeCun, Y., Bengio, Y.: The handbook of brain theory and neural networks. chap. Convolutional\nNetworks for Images, Speech, and Time Series, pp. 255–258. MIT Press, Cambridge, MA, USA (1998),\nhttp://dl.acm.org/citation.cfm?id=303568.303704\n17. Loman, N.J., Quick, J., Simpson, J.T.: A complete bacterial genome assembled de novo using only\nnanopore sequencing data. Nature methods 12(8), 733–735 (2015)\n18. Loman,\nN.:\nNanopore\nR9\nrapid\nrun\ndata\nrelease,\nhttp://lab.loman.net/2016/07/30/\nnanopore-r9-data-release/, [Online; posted 30-July-2016]\n19. Loman,\nN.:\nThar\nshe\nblows!\nUltra\nlong\nread\nmethod\nfor\nnanopore\nsequencing,\nhttp:\n//lab.loman.net/2017/03/09/ultrareads-for-nanopore/, [Online; posted 9-March-2017]\n20. Mikheyev, A.S., Tin, M.M.: A first look at the oxford nanopore minion sequencer. Molecular ecology\nresources 14(6), 1097–1102 (2014)\n21. Schreiber, J., Karplus, K.: Analysis of nanopore data using hidden markov models. Bioinformatics p.\nbtv046 (2015)\n22. Shah, A., Kadam, E., Shah, H., Shinde, S., Shingade, S.: Deep residual networks with exponential linear\nunit (2016)\n23. Sovi´c, I., ˇSiki´c, M., Wilm, A., Fenlon, S.N., Chen, S., Nagarajan, N.: Fast and sensitive mapping of\nnanopore sequencing reads with graphmap. Nature communications 7 (2016)\n24. Szalay, T., Golovchenko, J.A.: De novo sequencing and variant calling with nanopores using poreseq.\nNature biotechnology 33(10), 1087–1091 (2015)\n25. Timp, W., Comer, J., Aksimentiev, A.: Dna base-calling from a nanopore using a viterbi algorithm.\nBiophysical journal 102(10), L37–L39 (2012)\n26. Vaser, R., Sovic, I., Nagarajan, N., Sikic, M.: Fast and accurate de novo genome assembly from long\nuncorrected reads. bioRxiv (2016), http://biorxiv.org/content/early/2016/08/05/068122\n",
  "categories": [
    "q-bio.GN",
    "cs.LG"
  ],
  "published": "2019-04-22",
  "updated": "2019-04-22"
}