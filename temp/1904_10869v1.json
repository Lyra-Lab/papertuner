{
  "id": "http://arxiv.org/abs/1904.10869v1",
  "title": "Variational approach to unsupervised learning",
  "authors": [
    "Swapnil Nitin Shah"
  ],
  "abstract": "Deep belief networks are used extensively for unsupervised stochastic\nlearning on large datasets. Compared to other deep learning approaches their\nlayer-by-layer learning makes them highly scalable. Unfortunately, the\nprinciples by which they achieve efficient learning are not well understood.\nNumerous attempts have been made to explain their efficiency and applicability\nto a wide class of learning problems in terms of principles drawn from\ncognitive psychology, statistics, information theory, and more recently\nphysics, but quite often these imported principles lack strong scientific\nfoundation. Here we demonstrate how one can arrive at convolutional deep belief\nnetworks as potential solution to unsupervised learning problems without making\nassumptions about the underlying framework. To do this, we exploit the notion\nof symmetry that is fundamental in machine learning, physics and other fields,\nutilizing the particular form of the functional renormalization group in\nphysics.",
  "text": " \n \nVariational approach to unsupervised learning \nSwapnil Nitin Shah (swapnil.nitin.shah@gmail.com) \nIBM Systems, 11400 Burnet Road, Austin, Texas, 78727 \n \nDeep belief networks are used extensively for \nunsupervised stochastic learning on large datasets.  \nCompared to other deep learning approaches their \nlayer-by-layer \nlearning \nmakes \nthem \nhighly \nscalable5,6.  Unfortunately, the principles by which \nthey achieve efficient learning are not well \nunderstood.  Numerous attempts have been made \nto explain their efficiency and applicability to a \nwide class of learning problems in terms of \nprinciples drawn from cognitive psychology, \nstatistics, information theory, and more recently \nphysics7, but quite often these imported principles \nlack strong scientific foundation. Here we \ndemonstrate how one can arrive at convolutional \ndeep belief networks as potential solution to \nunsupervised learning problems without making \nassumptions about the underlying framework.  To \ndo this, we exploit the notion of symmetry that is \nfundamental in machine learning, physics and \nother fields, utilizing the particular form of the \nfunctional renormalization group in physics. \nUnsupervised learning is a form of machine \nlearning \nthat \nlearns \nfrom \nunclassified \nand \nuncategorized data by identifying similarities between \nsamples of data which are provided during a training \nphase. It is then required to react based on whether or \nnot a test data sample, not presented to it in the training \nphase, bears any of the identified similarities with the \ntraining samples1. Over the years, numerous \napproaches have been proposed and used for \nunsupervised learning problems to a varying degree of \nsuccess. These primarily include latent variable \nmodels, network models and clustering algorithms2. \nWhile each of these models works well with a subset \nof the problems, general applicability of the solution \nthey provide, and its evaluation thereof lacks strong \ngrounding in scientific literature3,4. \nOne model in particular, the Deep Belief Network \n(DBN), has received widespread attention from the \nMachine Learning community in recent times5,6 owing \nto fast training while ensuring good fidelity of \noutcomes across a large subset of problems. DBNs are \nneural network models comprising of multiple layers \nof latent (hidden) variables with each pair of \nconsecutive layers along with the connections between \nthem constituting a Restricted Boltzmann Machine \n(RBM). When trained on samples of data, a DBN \nlearns the probability distribution of the samples.  \nMehta and Schwab7 provide an analogy between \nDBNs and the mathematical tool of variational \nRenormalization Group (RG) which is widely used to \nstudy physical systems at various energy scales. It \nshows how a step of real space renormalization of spin \nsystems is analogous to training of an RBM. The \nhidden spins of the RBM serve as coarse grained \ndescription of the visible spins. The renormalization \nstep, when done exactly, ensures that the learnt \nvariational joint distribution of hidden and visible \nspins when marginalized over the hidden spins is \nexactly equal to the prior probability of the visible \nspins. Albeit an important first step towards providing \nevidence in favor of DBNs, a more fundamental \nunderstanding of the underlying concepts is required \nto explain (a) how DBNs provide a solution to the \ngeneral stochastic unsupervised learning problem, (b) \nwhy there is an explicit need for stacking RBMs (in a \nDBN) as against use of single RBM, (c) criteria to \nensure that trained RBM produces exact RG \ntransformation and (d) applicability to general \nproblems/systems that cannot be reduced to the Ising \nmodel or which have non-thermal fixed points \n(continuous fourier spectra). In this paper, we attempt \nto address these questions after having introduced the \nrequisite mathematical formalism. \nGiven a set X and a structure 𝑆(𝑥)  𝑥𝜖𝑋, a \ntransformation 𝑓∶𝑋→𝑋 is a symmetry if f is \ninvertible and preserves S. The group of all such \ntransformations is called the Symmetry group of S \nwith function composition as the group operation. \nSymmetry can be a discrete transformation or a \ncontinuous one. The transformation group of latter is \nwhat is known as a Lie group8. In this context, \nunsupervised learning can be thought of as learning \nthe structure S which is invariant under the symmetries \nof the data.  \nLet a data-point be described by a field 𝜙 𝜖 Φ over \nthe d data dimensions and a functional 𝑆[𝜙]: Φ →ℝ \nwhere ℝ is the set of reals. A continuous infinitesimal \ntransformation then has the following form \n𝜙1(𝑥) = 𝜙(𝑥) + 𝜖𝐹(𝑥, 𝜙),      𝜖→0                               (1) \nThe functional S varies as                            \n𝑆[𝜙1] = 𝑆[𝜙] + 𝜖∫𝑑9𝑥\n:;\n:< ∙𝐹(𝑥, 𝜙)                              (2)   \nIf the transformation (1) is a symmetry of S, we have \n𝑆[𝜙1] = 𝑆[𝜙]   ⇒   ∫𝑑9𝑥\n:;\n:< ∙𝐹(𝑥, 𝜙) = 0            (3) \nIf one considers a probability distribution 𝑃[𝜙] over \nall field configurations from which the data is \nsampled, the stochastic unsupervised learning problem \nnow becomes learning a structure Π which remains \ninvariant under symmetries of the average field  \n𝜙A(𝑥), defined as the functional integral \n𝜙A(𝑥) = 〈𝜙(𝑥)〉= ∫[𝐷𝜙] 𝑃[𝜙] ∙𝜙(𝑥)                     (4) \nwhere [𝐷𝜙] is the functional integral measure. Hence, \nthe remaining article will primarily focus on properties \nof the structure Π and its computation. From a \ntransformation of the form (1) which leaves the \nfunctional integral measure [𝐷𝜙] invariant, the \ntransformation of the average field 𝜙A(𝑥) is \n𝜙A′(𝑥) = 𝜙A(𝑥) + 𝜖〈𝐹(𝑥, 𝜙)〉                                   (5) \n[𝐷𝜙] ≡[𝐷𝜙1]  \nFrom (2), \n〈𝑆[𝜙1]〉= 〈𝑆[𝜙]〉+ 𝜖∫𝑑9𝑥〈:;\n:< ∙𝐹(𝑥, 𝜙)〉                 (6) \nAs the average field 𝜙A(𝑥) also belongs to the set of \nall field configurations Φ, all measure preserving \nsymmetries (invariant [𝐷𝜙]) of S should correspond to \na subset of the symmetries of Π (vice-versa is not \ngenerally true). First, let us consider the case of \ncontinuous symmetries of S. Mathematically, Π[𝜙A] is \na functional of the average field 𝜙A such that for all \n𝐹(𝑥, 𝜙), \n∫𝑑9𝑥〈:;\n:< ∙𝐹(𝑥, 𝜙)〉= ∫𝑑9𝑥\n:G\n:<H ∙〈𝐹(𝑥, 𝜙)〉            (7) \nIn other words, for every measure preserving \ncontinuous symmetry of 𝑆, there is a corresponding \ncontinuous symmetry of Π. Let us derive the form of \nthe probability distribution 𝑃[𝜙] for which (7) \nbecomes an identity. Under all measure preserving \nfield transformations of form (1), the functional \nintegral ∫[𝐷𝜙] 𝑃[𝜙] should remain invariant (as it is \nsum of probabilities of all fields). So, to ensure (7) is \nan identity, one can have \n∫𝑑9𝑥\n:I\n:< = ∫𝑑9𝑥\n:G\n:<H 𝑃[𝜙] −∫𝑑9𝑥\n:;\n:< 𝑃[𝜙]            (8)           \nIntegrating (8) with respect to 𝜙, \n𝑃[𝜙] =\nK\nL 𝑒\nN ;[<]O∫9PQ RS\nRTH(Q) ∙ <(Q)                            (9) \n𝑍= ∫[𝐷𝜙]𝑒\nN ;[<]O ∫9PQ RS\nRTH(Q) ∙ <(Q)                      (10) \nwhere Z is the partition function which ensures 𝑃[𝜙] \nis a probability measure. It is then straightforward to \nrealize that Π[𝜙A] is the Legendre transform of ln(𝑍) \n(up to a constant independent of 𝜙A) \nΠ[𝜙A] = ∫𝑑9𝑥 \n:G\n:<H (𝑥) ∙𝜙A(𝑥) − ln (𝑍)                (11) \nIn Quantum Field theory (QFT), the functional Π is the \neffective action with S as the bare (microscopic) action \nand (7) is, then, a manifestation of the Slavnov-Taylor \nidentities9. Although Π is constructed so as to have a \ncontinuous symmetry corresponding to every measure \npreserving continuous symmetry for S, it is not directly \nevident if the correspondence would hold even for \ndiscrete symmetries of the bare functional S. To this \nend, consider a discrete invertible transformation \nwhich leaves the functional integral measure [𝐷𝜙] \ninvariant \n𝜙1(𝑥) = 𝜙(𝑥) + Δ(𝜙, 𝑥)                                          (12) \n𝜙(𝑥) = 𝜙′(𝑥) + G(𝜙′, 𝑥)  \n[𝐷𝜙] ≡[𝐷𝜙1]                                        \nThen, \n𝑆[𝜙1] = 𝑆[𝜙]   ⇒   \n:;\n:<Z\n<1\n∙[1 +\n]^\n]<Z\n<_ =\n:;\n:<Z\n<\n    (13) \nLet \n𝐻(𝑥, 𝜙) = 𝑒∫9PQ RS\nRTH(Q)∙^(<,Q) −[1 +\n]^\n]<Z\n<Oa(<,Q)_       (14) \nFrom (12), (13) and (14),  \n∫𝑑9𝑥〈:;\n:< ∙𝐻(𝑥, 𝜙)〉                                                                    \n=\nK\nL ∫𝑑9𝑥b∫[𝐷𝜙] 𝑒\nN ;[<]O∫9PQ RS\nRTH(Q)∙c<(Q)O^(<,Q)d :;\n:<Z\n<\n−\n∫[𝐷𝜙] 𝑒\nN ;[<]O∫9PQ RS\nRTH(Q)∙<(Q) :;\n:<Z\n<\n∙[1 +\n]^\n]<Z\n<Oa(<,Q)_e  \n=\nK\nL ∫𝑑9𝑥b∫[𝐷𝜙1] 𝑒\nN ;f<ghO∫9PQ RS\nRTH(Q)∙<g(Q) :;\n:<Z\n<gOac<g,Qd −\n∫[𝐷𝜙] 𝑒\nN ;[<]O∫9PQ RS\nRTH(Q)∙<(Q) :;\n:<Z\n<Oa(<,Q)\ne    =     0               (15)     \nThen from (7) and (15), \n ∫𝑑9𝑥〈:;\n:< ∙𝐻(𝑥, 𝜙)〉= ∫𝑑9𝑥\n:G\n:<H ∙〈𝐻(𝑥, 𝜙)〉= 0       (16) \nThis shows that every measure preserving discrete \nsymmetry of S also leads to a corresponding \ncontinuous symmetry of Π.  \nRewriting (11), \nΠ[𝜙A] = ln j\nk\n∫PPl RS\nRTH(l)∙TH(l)\n∫[m<]k\nn o[T]p ∫PPl RS\nRTH(l) ∙ T(l)q             (17) \nIf one associates a) the average field 𝜙A and effective \naction Π[𝜙A] with the layer of visible spins 𝑣 of an \nRBM like structure and b) 𝜙 and bare action 𝑆[𝜙] with \nthe layer of hidden spins ℎ thereof, after suitable \ndiscretization, from (9) and (17) \nln[𝑝(𝑣)] ≡lnf𝑃[𝜙A]h    =     Π[𝜙A] −𝑆[𝜙A]          (18) \nThis model (Fig. 1) suffers from a major issue \ncomputationally- \n \nFigure 1 | Graphical representation of the restricted \nBoltzmann machine. Visible layer (observed variables) is \ncomprised of units 𝑣 and the hidden layer (latent variables) \nis comprised of units ℎ. The layers are fully connected, and \nthe connections W are bidirectional. \nIn general, the effective functional Π is not solvable \nperturbatively in 𝜙A owing to divergent integrals \n(similar to effective action in QFT). In QFT, this \nbrings in the need for exact non-perturbative \nrenormalization group which aids one in studying the \nsame physical system at different energy scales. Of \nparticular interest, in this context, is the functional \nrenormalization group which interpolates between the \nbare \nand \neffective \naction \nfunctionals10. \nMathematically, the variation of the interpolating \neffective average action Πu with infrared energy \n(momentum) cutoff scale is described by the Wetterich \nflow equation (20). Effective average action Πu \ninterpolates between effective action Π (𝑘→0) and \nbare action S (𝑘→∞). \n𝑒NGx[<H] = ∫[𝐷𝜒] exp }−𝑆[𝜙A + 𝜒] +\n ∫\n9P~\n(•€)P •\n:Gx\n:<H(Q)\n‚ (−𝑝) ∙χ„(𝑝) −\nK\n• χ„∗(𝑝) ∙𝑅u(𝑝) ∙χ„(𝑝)‡ˆ (19) \nThen in real space, \n]Gx[<H]\n]u\n=\nK\n• ∫𝑑9𝑥 𝑑9𝑦b\n]Š‹x(ŒNQ)\n]u\n∙}\n:•Gxf𝜙𝑎h\n:<H(Q):<H(Œ) (𝑥, 𝑦) +\n     𝑅‹u(𝑥−𝑦)ˆ\nNK\ne                                                           (20) \nAlso10, \n]Gx[<H]\n]u\n=\nK\n• ∫𝑑9𝑥 𝑑9𝑦•\n]Š‹x(ŒNQ)\n]u\n∙[〈𝜙(𝑦)𝜙(𝑥)〉u −\n    𝜙A(𝑦)𝜙A(𝑥)]‡                                                        (21)  \nwhere 𝑓•(𝑝) denotes the d-dimensional fourier \ntransform of 𝑓(𝑥), 𝑔‘(𝑥) denotes the inverse fourier \ntransform of 𝑔(𝑝) and 𝑅u(𝑝) is an IR regulator that \nprovides a mass-like contribution to the bare action \nthat suppresses the contributions of momenta 𝑝≪𝑘. \nFunctional renormalization group flow equation (20) \nis exact and reproduces the effective action in the \ninfrared limit (𝑘→0) circumventing the divergences \nencountered in perturbative calculations10. Quite \nnaturally, in order to mitigate the aforementioned issue \nwith the RBM model, one might resort to \nincorporating the functional renormalization group \nidea into the proposed model. Rewriting the integral in \n(19) in terms of 𝜙“ ≡𝜙A + 𝜒 and taking the \nlogarithm, \nΠu[𝜙A]  \n= ∫\n9P~\n(•€)P ”\n:•x\n:<H(Q)\n‚ (−𝑝) ∙𝜙A\n–(𝑝) +\nK\n• 𝜙A\n– ∗(𝑝) ∙𝑅u(𝑝) ∙𝜙A\n–(𝑝)— −\n     ln 𝑍u                                                                                       (22) \n𝑍u = ∫f𝐷𝜙“h exp ˜−𝑆f𝜙“h + ∫\n9P~\n(•€)P ”\n:Gx\n:<H(Q)\n‚ (−𝑝) ∙𝜙“\n–(𝑝) −\n    \nK\n• ˜𝜙“\n– ∗(𝑝) ∙𝜙“\n–(𝑝) −𝜙“\n– ∗(𝑝) ∙𝜙A\n–(𝑝) −𝜙“\n–(𝑝) ∙𝜙A\n– ∗(𝑝)™∙\n   𝑅u(𝑝)—™                                                                   (23)   \nOne can identify (22) with (11) in the limit (𝑘→0)10. \nAs 𝑆[𝜙A] is independent of 𝑘, (17) and (18) then allow \nus to associate every Πu[𝜙A] with the logarithm of the \nmarginalized probability of a layer of spins in a \ncascading structure with each layer differing from the \nnext, in the IR cutoff, by an infinitesimal amount dk. \nThis lets us treat two consecutive layers in the cascade \nas an RBM and (20) shows how the effective average \naction for one layer depends on that of the next.  \n \nFigure 2 | Graphical representation of the RG cascade. The \nIR cutoff 𝑘 decreases as one moves down in the cascade \nrecovering the full scale-independent effective functional \n(action) at the bottom. \nIndeed, the action 𝑆[𝜙A] serves as the logarithm of \nmarginalized probability distribution of the uppermost \nlayer (𝑘→∞) and the exact effective action Π[𝜙A] \nserves as the logarithm of marginalized probability \ndistribution of the lowermost layer (𝑘→0) in the \nresulting cascade (Fig. 2). As this cascading structure \nresults \nfrom \nthe \nexact \nnon-perturbative \nrenormalization group (20), computation of the \neffective functional Π using this model is exact and \nfree from integral divergences. This stacked model \ntherefore \ncircumvents \nthe \nuncomputability, \nencountered \nin \nthe \nsingle \nRBM \nmodel \nfor \ncomputation, of the effective functional Π. One feature \nof the RG flow equation (20) viz., the IR regulator \n𝑅u(𝑝) has non-trivial implications for the cascade. As \nagainst the conventional RBM model, where all units \nin the hidden layer are connected to all units in the \nvisible layer and vice-versa, the IR regulator 𝑅u(𝑝) \nintroduces a receptive field (a neighborhood) over the \nhidden spins (upper layer in the cascade) and the \nvisible spins (lower layer in the cascade) for each \nspatial coordinate (Fig. 2). For the purposes of \nillustration, consider the following IR regulator 𝑅u(𝑝) \nin a one-dimensional lattice   \n𝑅u(𝑝) = ˜\nu\n~™\nš›\n    , 𝑛∈ℕ                                                  (24) \nwhere, ℕ is the set of natural numbers. The regulator \nin (24) provides a smooth, yet sharp momentum cutoff \nat 𝑝 ~ 𝑘 for large 𝑛. For 𝑝≪𝑘, the regulator 𝑅u(𝑝) \nhas a large value while for 𝑝≫𝑘, 𝑅u(𝑝) →0. Inverse \nfourier transform of (24) yields \n𝑅‹u(𝑡) =\nu∙€\n(š›NK)! (𝑘∙𝑡)š›NK ∙sign(𝑡)  \n       (25) \nwhere, sign(𝑡) is the signum function. From (25), it \ncan be seen that 𝑅u(𝑡) also has a sharp spatial cutoff \nat |𝑡| ~ 𝑐› ∙𝑘NK for large n, where 𝑐› is a constant \nindependent of 𝑘. For |𝑡| ≪(𝑐› ∙𝑘NK), 𝑅u(𝑡) →0 \nwhile for |𝑡| ≫(𝑐› ∙𝑘NK), 𝑅u(𝑡) has a large value. \nFrom (25) and (20), it is easy to see that for every \nspatial coordinate 𝑥, only those spatial coordinates 𝑦 \nfor which |𝑦−𝑥| ≲(𝑐› ∙𝑘NK) would have a \nsignificant contribution to integral (20). It can be seen \nfrom (21), which is an alternative form of (20), how \nthis generates a receptive field on the spins of \nconsecutive layers (Fig. 2). This analysis is easily \ngeneralizable to higher dimensions as one can always \nselect a regulator 𝑅u(𝑝) which is separable in all \ndimensions and has form (24) in each dimension. The \nproposed cascade to compute the effective functional \nΠ from the bare functional S does present a \nmanifestation in conventional machine learning theory \nunder suitable discretization viz., the convolutional \ndeep belief network (CDBN). A CDBN comprises of \nstacked convolutional RBMs with an intermediate \nnon-linear max-pooling layer for dimensionality \nreduction11. The hidden layer of one RBM in the stack \nserves as the visible layer of another (after max-\npooling). Also, a convolutional RBM differs from a \nregular RBM in that the former introduces a receptive \nfield over the hidden layer, for each spin in the visible \nlayer, on which a convolution kernel (of the size \ndescribed by the receptive field) acts to evaluate the \nprobability of the visible spin. In the proposed cascade \nmodel, we have already established how one can treat \nevery two consecutive layers as an RBM and we have \nalso seen how the regulator 𝑅u(𝑝) generates a \nreceptive field over the consecutive layers. In light of \nthe above, it can be seen how CDBN presents itself as \na solution to the stochastic unsupervised learning \nproblem by computing the effective functional Π from \nthe \nbare \nfunctional \nS \nvia \nthe \nfunctional \nrenormalization group flow (20). As against a \nconventional CDBN, however, no max-pooling layers \nare employed between two consecutive RBMs in the \nproposed cascade. The receptive field increases as one \nmoves downwards in this CDBN (k decreases) with a \nfully connected RBM at the bottom of the stack. \nCDBN aims to maximize ln(𝑝(𝑣)) of configurations \n𝑣 𝜖 𝑉 of units in the lowermost layer, where 𝑉 is a \nsubset of all possible configurations11. In the proposed \ncascade, this is analogously achieved by extremizing \nthe effective functional Π (supremum per definition of \nLegendre transform) as Schwinger functional ln(𝑍) is \nconvex12. However, the bare functional S is not a given \nin the case of a CDBN. Hence, the training procedure \naims to compute S, the logarithm of the marginal \nprobability of spins in the uppermost layer that \nmaximizes the effective functional Π, the logarithm of \nthe marginal probability of spins in the lowermost \n(visible) layer. One also needs to keep in mind that the \nRG step is exact only for an infinitesimal change in the \ncutoff scale k and thus the proposed cascade is not \ndirectly realizable. At best, only a finite approximation \nof the proposed scheme is possible to implement. \nThe notion of symmetry is quite fundamental to \nboth machine learning and physics. Previous attempts \nmade to draw analogies based on this notion are either \npurely empirical or are plagued by assumptions which \nare not based on scientifically established principles7. \nThe first half of the paper attempts to provide a \nsolution to the stochastic unsupervised learning \nproblem as formulating the effective functional Π \nwhich encompasses all symmetries of stochastic data. \nThis formulation is seen to be that of the effective \naction in QFT. The second half of the paper is devoted \nto computing Π exactly, which suffers from integral \ndivergences when a perturbative expansion is \nattempted (reminiscent of the direct uncomputability \nof effective action in QFT). A non-perturbative RG \nmethod, \nthe \nfunctional \nrenormalization \ngroup \nproduces the exact effective action from the bare \naction in QFT and is therefore employed for \ncomputation of the effective functional Π. Form (17) \nand RG flow equation (20) then describe how \nfunctional RG can be performed by employing a \ncascade of RBMs with each layer differing from the \nnext in the IR cutoff by an infinitesimal amount dk. \nEquation (25) illustrates how a suitable IR regulator \n𝑅u(𝑝) describes a receptive field over two consecutive \nlayers in computation of (20). This receptive field \nincreases as one moves downwards in the cascade (k \ndecreases) with a fully connected RBM at the bottom \nof the stack. As (20) is an exact equation, each stacked \nRBM computes the exact RG step. Although the \nconventional CDBN model provides a manifestation \nof the RG flow (20) for discrete lattice systems with \nlayers of spins, the functional renormalization group is \napplicable to a wide class of physical systems (not \nnecessarily discrete), whether in or out of equilibrium \nas against Ising models which only apply to lattice \nsystems in thermal equilibrium. Functional RG has \nbeen used successfully in numerous problems in \ncondensed matter and high energy physics (e.g., \nasymptotic safety). We present a manifestation of the \nproposed cascade model (under discretization) in the \nconventional machine learning theory viz., the CDBN \nand that the training procedure for CDBN computes \nthe bare functional S, the logarithm of the marginal \nprobability of spins in the uppermost layer that \nmaximizes the effective functional Π, the logarithm of \nthe marginal probability of the lowermost (visible) \nlayer. In turn, this describes how a structure as \ncomplex as a convolutional deep belief network \nemerges as a solution to a stochastic unsupervised \nlearning problem without making any assumptions \nabout the underlying framework.  \nCompeting interests - The author declares no \ncompeting interests. \n1. \nUnsupervised \nlearning: \nFoundations \nof \nneural \ncomputation.     Computers     &     Mathematics      with \nApplications 38, 256 (1999). \n2. \nDuda, R. O., Hart, P. E. & Stork, D. G. Pattern \nclassification. (Wiley, 2000). \n3. \nEstivill-Castro, \nV. \nWhy \nso \nmany \nclustering \nalgorithms. ACM SIGKDD Explorations Newsletter \n4,65–75 (2002). \n4. \nFeldman, R. & Sanger, J. The text mining handbook: \nadvanced approaches in analyzing unstructured data. \n(Cambridge University Press, 2013). \n5. \nHinton, G. Deep Belief Nets. Encyclopedia of Machine \nLearning \nand \nData \nMining \n335–338 \n(2017). \ndoi:10.1007/978-1-4899-7687-1_67 \n6. \nBengio, \nY. \nLearning \nDeep \nArchitectures \nfor \nAI. Foundations \nand \nTrends® \nin \nMachine \nLearning2,1–127 (2009). \n7. \nPankaj, Schwab, J., D. & Mehta. An exact mapping \nbetween the Variational Renormalization Group and \nDeep \nLearning. [stat.ML] \n(2014). \nhttps://arxiv.org/abs/1410.3831.  \n8. \nChevalley, C. Theory of Lie groups. (Princeton \nUniversity Press, 1955). \n9. \nBilal, \nA. \nAdvanced \nQuantum \nField \nTheory: \nRenormalization,  Non - Abelian  Gauge  Theories  and \nAnomalies (2014). \n10. Wetterich, C. Exact evolution equation for the effective \npotential. Physics Letters B301,90–94 (1993). \n11. Lee, H., Grosse, R., Ranganath, R. & Ng, A. Y. \nConvolutional deep belief networks for scalable \nunsupervised \nlearning \nof \nhierarchical \nrepresentations. Proceedings of the 26th Annual \nInternational Conference on Machine Learning - ICML \n09(2009). doi:10.1145/1553374.1553453 \n12. Brown, L. S. Quantum field theory. (Cambridge \nUniversity Press, 2012). \n \n",
  "categories": [
    "cond-mat.dis-nn",
    "cs.LG"
  ],
  "published": "2019-04-24",
  "updated": "2019-04-24"
}