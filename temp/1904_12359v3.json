{
  "id": "http://arxiv.org/abs/1904.12359v3",
  "title": "Unsupervised Feature Learning for Point Cloud by Contrasting and Clustering With Graph Convolutional Neural Network",
  "authors": [
    "Ling Zhang",
    "Zhigang Zhu"
  ],
  "abstract": "To alleviate the cost of collecting and annotating large-scale point cloud\ndatasets, we propose an unsupervised learning approach to learn features from\nunlabeled point cloud \"3D object\" dataset by using part contrasting and object\nclustering with deep graph neural networks (GNNs). In the contrast learning\nstep, all the samples in the 3D object dataset are cut into two parts and put\ninto a \"part\" dataset. Then a contrast learning GNN (ContrastNet) is trained to\nverify whether two randomly sampled parts from the part dataset belong to the\nsame object. In the cluster learning step, the trained ContrastNet is applied\nto all the samples in the original 3D object dataset to extract features, which\nare used to group the samples into clusters. Then another GNN for clustering\nlearning (ClusterNet) is trained to predict the cluster ID of all the training\nsamples. The contrasting learning forces the ContrastNet to learn high-level\nsemantic features of objects but probably ignores low-level features, while the\nClusterNet improves the quality of learned features by being trained to\ndiscover objects that probably belong to the same semantic categories by the\nuse of cluster IDs. We have conducted extensive experiments to evaluate the\nproposed framework on point cloud classification tasks. The proposed\nunsupervised learning approach obtained comparable performance to the\nstate-of-the-art unsupervised learning methods that used much more complicated\nnetwork structures. The code of this work is publicly available via:\nhttps://github.com/lingzhang1/ContrastNet.",
  "text": "Unsupervised Feature Learning for Point Cloud Understanding by Contrasting\nand Clustering Using Graph Convolutional Neural Networks\nLing Zhang, Zhigang Zhu\nThe City College of The City University of New York\nlzhang006@citymail.cuny.edu, zzhu@ccny.cuny.edu\nAbstract\nTo alleviate the cost of collecting and annotating large-\nscale ”3D object” point cloud data, we propose an unsuper-\nvised learning approach to learn features from an unlabeled\npoint cloud dataset by using part contrasting and object\nclustering with deep graph convolutional neural networks\n(GCNNs). In the contrast learning step, all the samples in\nthe 3D object dataset are cut into two parts and put into\na ”part” dataset. Then a contrast learning GCNN (Con-\ntrastNet) is trained to verify whether two randomly sampled\nparts from the part dataset belong to the same object. In the\ncluster learning step, the trained ContrastNet is applied to\nall the samples in the original 3D object dataset to extract\nfeatures, which are used to group the samples into clusters.\nThen another GCNN for clustering learning (ClusterNet) is\ntrained from the orignal 3D data to predict the cluster IDs\nof all the training samples. The contrasting learning forces\nthe ContrastNet to learn semantic features of objects, while\nthe ClusterNet improves the quality of learned features by\nbeing trained to discover objects that belong to the same se-\nmantic categories by using cluster IDs. We have conducted\nextensive experiments to evaluate the proposed framework\non point cloud classiﬁcation tasks. The proposed unsuper-\nvised learning approach obtains comparable performance\nto the state-of-the-art with heavier shape auto-encoding un-\nsupervised feature extraction methods. We have also tested\nthe networks on object recognition using partial 3D data, by\nsimulating occlusions and perspective views, and obtained\npractically useful results. The code of this work is publicly\navailable at: https://github.com/lingzhang1/ContrastNet.\n1. Introduction\nWith ever increasing applications, point cloud data un-\nderstanding with deep graph convolutional neural networks\n(GCNNs) has drawn extensive attention [23, 25, 35, 17].\nVarious networks, such as PointNet [23], PointNet++ [25],\nDGCNN [35] and etc., and datasets such as ModelNet [37],\nFigure 1. Each row consists of a 3D point cloud object and its\nfour different segments. Human can easily recognize the object\nand the locations of the segments in the object even for a small\nsegment.Inspired by this observation, we propose to train GCNNs\nto learn features from a unlabeled dataset by recognizing whether\ntwo segments are from the same object.\nShapeNet [6], and SUNCG [31], have been proposed for\npoint cloud understanding tasks.\nWith the help of deep\nmodels and large-scale labeled datasets, signiﬁcant progress\nhas been made on point cloud understanding tasks, includ-\ning classiﬁcation, segmentation and detection.\nGCNNs typically have millions of parameters which\ncould easily lead to over-ﬁtting.\nLarge-scale annotated\ndatasets are needed for the training of such deep net-\nworks.\nHowever, the collection and annotation of point\ncloud datasets are very time-consuming and expensive since\npixel-level annotations are needed.\nWith their powerful\nability to learn useful representations from unlabeled data,\nunsupervised learning methods, sometimes also known as\nself-supervised learning methods, have drawn signiﬁcant at-\ntention.\nThe general pipeline of unsupervised learning with a\ndeep neural network is to design a ”pretext” task for the\nnetwork to solve while the label for this pretext tasks can be\nautomatically generated based on the attributes of the data.\nAfter the network is trained with the pretext task, the net-\nwork will be able to capture useful features. Recently, many\nunsupervised learning methods have been proposed to learn\nimage features by training networks to solve pretext tasks,\nsuch as playing image jigsaw [21], clustering images [5],\npredicting image rotations [12], image inpainting [22], gen-\narXiv:1904.12359v3  [cs.CV]  13 Aug 2019\nerating images with generation adversarial network [26],\netc. The unsupervised learning methods for image feature\nlearning have obtained great success and the performance of\nunsupervised learning methods sometimes come very close\nto supervised methods [5, 13].\nA number of unsupervised learning methods have also\nbeen proposed for point cloud unsupervised learning [15,\n7, 9, 36, 1, 38]. Most of them are based on auto-encoders\n[9, 36, 1, 38]. Various auto-encoders are proposed to obtain\nfeatures by training them to reconstruct the 3D point cloud\ndata. Since the main purpose of such auto-encoders is to re-\nconstruct the data, the networks thus trained may memorize\nthe low-level features of the point cloud.\nIn this paper, we propose an unsupervised feature learn-\ning approach for point cloud by training GCNNs to solve\ntwo pretext tasks consecutively, which are part contrasting\nand object clustering. Speciﬁcally, the network is trained\nto accomplish two pretext tasks: to compare (contrast) two\npoint cloud cuts and to cluster point cloud objects. First,\nall the 3D point objects are cut into two parts and a GCNN\n(called ContrastNet) is trained to verify whether two ran-\ndomly sampled parts from the dataset belong to the same\nobject. Second, the point cloud data is clustered into clus-\nters by using the features learned by the ContrastNet, and\nanother GCNN (called ClusterNet) is trained to predict the\ncluster ID of each point cloud data.\nIn summary, our main contributions in this paper are as\nfollows:\n• A simple and effective unsupervised feature learning\nframework is proposed for point cloud data. By train-\ning deep graph CNNs to solve two pretext tasks, part\ncontrasting and object clustering, the networks are able\nto learn semantic features for point cloud data without\nusing any annotations.\n• Extensive experiments are performed showing that our\nproposed approach outperforms most of the state-of-\nthe-art unsupervised learning methods. With the pro-\nposed unsupervised method, our model obtains 86.8%\nand 93.8% classiﬁcation accuracy on ModelNet40 and\nModelNet10 datasets respectively.\n• As a practical consideration, we have also tested ob-\nject recognition using partial 3D data, by simulating\nocclusions and perspective views. Experiments show\nthat our proposed approach generates results that are\npractically useful.\n2. Related Work\nPoint Cloud Understanding: Various approaches have\nbeen proposed for point cloud understanding tasks, includ-\ning classiﬁcation, segmentation, and recognition and detec-\ntion. These approaches can be classiﬁed into three types:\nhand-crafted methods [2, 33, 3, 28, 27, 18, 14, 7], CNNs\non regular 3D data [20, 37, 34, 16, 24, 32, 29, 8, 10], and\nCNNs on unordered 3D point cloud data [23, 25, 35, 17].\nThe ﬁrst type of methods is hand-crafted based meth-\nods. These traditional methods capture the local geometric\nstructure information of point cloud data such as intrinsic\ndescriptors [2, 33, 3], or extrinsic descriptors [28, 27, 18,\n14, 7]. These methods have very limited performance of\n3D data analysis.\nApplying 3D convolutional neural networks to regular\n3D data usually obtained better performance than tradi-\ntional hand-crafted features. There are several approaches\nto handle the regular 3D data with CNNs: volumetric meth-\nods [20, 37, 34, 16, 24] voxelize unordered data to a static\n3D grid then 3D CNNs are used to process the data. This\nkind of methods has a constraint on efﬁciency and com-\nplexity due to the data sparsity and cost of CNNs. Multi-\nview methods [32, 29] use 2D CNNs after rendering the 3D\ndata into 2D images, which have obtained signiﬁcant per-\nformance improvement on the classiﬁcation task. However,\nthis kind of methods has constraints in doing point level\ntask, such as segmentation. Spectral methods [4, 19] apply\nspectral CNNs on meshes that are constrained by the ex-\npandability to other data formats. Feature-extracting meth-\nods [8, 10] extract features of 3D data and then apply CNNs\nto the features, which deeply depend on the quality of the\nextracted features.\nRecently, a number of methods have been proposed for\nunderstanding unordered point cloud data [23, 25, 35, 17].\nQi et al. made the ﬁrst attempt to design a deep network ar-\nchitecture, named PointNet [23], for using unordered point\ncloud to perform 3D shape classiﬁcation, shape part seg-\nmentation and scene semantic parsing tasks. PointNet pro-\ncess each 3D point in a sample individually, therefore disar-\nrangement of the point cloud will not constrain the function\nof the model. However, because of this, PointNet does not\nutilize the local structure of point cloud, which limits its\nability to recognize ﬁne-grained patterns. Later, they pro-\nposed PointNet++ which applied PointNet recursively on a\nnested partitioning of the input point set [25] to improve\nthe PointNet and address the impact of local information\nlost. PointCNN [17] was proposed as a generalization of\ntypical CNNs to feature learning from point clouds by the\npermutation of the points into a latent and potentially canon-\nical order and the weighting of the input features associated\nwith the points. To capturing local structure, Wang et al.\nproposed DGCNN [35] with an edge-convolution network\n(EdgeConv) to speciﬁcally model local neighborhood infor-\nmation by applying convolutions over the k nearest neigh-\nbors calculated by KNN in metric space, and the k nearest\nneighbors can be dynamically updated in different layers.\nUnsupervised Feature Learning:\nVarious unsuper-\nvised learning methods have been proposed to learn features\nFigure 2. The unsupervised feature learning pipeline includes three main steps: (a) ContrastNet for part contrast learning, by verifying\nwhether two point cloud cuts belong to the same object; (b) Cluster samples of 3D objects and assign cluster IDs, using the features learned\nby ContrastNet; (c) ClusterNet for object clustering learning, by training the network with the 3D point cloud data while the labels are the\ncluster IDs assigned by the clustering step.\nfrom unlabeled data [15, 7, 9, 36, 1, 38, 5]. Girdhar et al.\nproposed the TL-embedding network[9], which consists of\nan autoencoder that ensures the representation is generative\nand a convolutional network that ensures the representation\nis predictable. Sharma et al. proposed a fully convolutional\nvolumetric autoencoder to learn volumetric representation\nfrom noisy data by estimating the voxel occupancy grids\n[36]. Achlioptas et al. proposed LatentGAN by introduc-\ning a new deep auto-encoder network with state-of-the-art\nreconstruction quality and generalization ability for point\ncloud data [1]. Yang et al. proposed FoldingNet which is\nan end-to-end autoencoder that is the state-of-the-art for un-\nsupervised feature learning on point clouds [38]. In their\nwork, a graph-based enhancement is applied to the encoder\nto enforce local structures on top of PointNet, and a folding-\nbased decoder deforms a canonical 2D grid onto the under-\nlying 3D object surface of a point cloud.\nMost of the deep learning based methods use auto-\nencoder variations for learning features on unlabeled point\ncloud data. However, the purpose of the autoencoder is to\nreconstruct the data and the feature may have a good perfor-\nmance on low-level tasks such as completion, reconstruc-\ntion, and denoise, but have an inferior performance on tasks\ndemands more high-level semantic meanings. Therefore,\nwe propose the ContrastNet and ClusterNet to learn features\nby exploring high-level semantic features. Our method out-\nperforms most of the unsupervised methods on two Mold-\nelNet datasets and only 0.6% lower than the supervised\nmethod PointNet on the ModelNet40 dataset.\n3. Method\nTo learn features from unlabeled point cloud data, we\npropose to learn features by training networks to accom-\nplish both of the part contrasting and the object clustering\npretext tasks. The pipeline of our framework is illustrated\nin Fig. 2, which includes three major steps: ContrastNet\nfor part contrast learning, clustering using the learned fea-\ntures, and then ClusterNet for object cluster learning using\nthe cluster IDs. Here is a summary of the three modules be-\nfore we get into details of the ContrastNet and ClusterNet.\na) ContrastNet: Part Contrast Learning: The ﬁrst\nstep is to learn features by training a network called Con-\ntrastNet to accomplish the part contrast task. Speciﬁcally,\nthe part contrast task is to verify whether two point cloud\nsegments (parts) belong to the same sample (object). The\npositive pair is drawn by selecting two different segments\nfrom the same object, while the negative pair is drawn by\nselecting two segments from two different objects.\nb) Clustering to Obtain Pseudo-labels: After the train-\ning with the part contrasting ﬁnished, the trained Con-\ntrastNet can obtain high-level semantic features from point\ncloud data.\nUsing the extracted features, the 3D point\ncloud data samples are clustered into different clusters.\nKmeans++ [11] is used as the clustering algorithm in the\npaper. The point cloud data from the same cluster have high\nsimilarity while the data from different clusters have low\nsimilarity.\nc) ClusterNet:\nClassiﬁcation using Pseudo-labels:\nOnce obtained the clusters for the training data by using\nthe Kmeans++ algorithm, the cluster IDs can be used as the\npseudo-labels to train another network called ClusterNet.\nThe clustering is used to boost the quality of the learned\nfeatures of the ContrastNet. The architecture of the network\nfor this step does not depend on the previous self-supervised\nmodel ContrastNet and therefore it can be ﬂexibly designed\nas the demands.\n3.1. ContrastNet: Part Contrast Learning\nWhen a point cloud data is observed from different\nviews, only part of the 3D object can be seen. The observ-\nable part can be very different based on the view. For ex-\nFigure 3. The architecture of ContrastNet for part contrast learning. The positive pairs are generated by randomly sampling two segments\nfrom the same point cloud sample, while the negative pairs are generated by randomly sampling two segments from two different samples.\nA dynamic graph convolutional neural network (DGCNN)[35] is used as the backbone network. Note that the top and bottom parts of\nthe ContrastNet is the same DGCNN (i.e., sharing the same parameters). The features of two segments are concatenated and fed to fully\nconnected layers to make the prediction of positive or negative. The part contrast learning does not require any data annotations by humans.\nample, as shown in Fig. 1, for the same airplane, when it is\nobserved from different views, the observed segments can\nbe totally different. However, the different segments still\nbelong to the same object.\nInspired by this observation, we proposed the part (seg-\nment) contrast as the ﬁrst pretext task for a GCNN to\nsolve. The task is deﬁned as to train a ContrastNet to ver-\nify whether two point cloud segments belong to the same\nobject. The positive pair is drawn by selecting two differ-\nent segments from the same object, while the negative pair\nis drawn by selecting two segments from two different ob-\njects. The illustration of the part contrast task is shown in\nFig. 3.\nWe randomly split one object into two segments, thus\ngenerating a ”part” dataset. Then a pair for the segments\nare randomly selected. If a pair of the segments are from\nthe same object, this pair is a positive instance that will be\nlabeled as 1. Otherwise, if a pair comes from two objects,\nit is a negative instance and will be labeled as 0. More im-\nplementation details will be introduced in Section 4.1. We\nmodel this task as a binary classiﬁcation problem. As the\ntraining goes on, the segments from the same object should\nhave a smaller distance while the segments from different\nobjects have a larger distance. In this way, the semantic\nfeatures can be learned.\nNote that since a pair of parts from two objects that be-\nlong to the same category will be treated as a ”negative”\ninstance instead of ”positive”, the training of positive and\nnegative has certain percentage of ”error” in the input data.\nFor example, in ModelNet40 dataset, objects belong to 40\ncategories. Without using the labels in training ContrastNet,\napproximately there is an 1/40 (2.5%) error in the input data\nfor verifying positive or negative instances.\nAs for the network architecture, we choose DGCNN [35]\nas the backbone model since this model speciﬁcally cap-\ntures the local structure of the point cloud with dynamically\nconstructed graphs and yields better performance. The de-\ntails of the network architecture are shown in Fig. 3. There\nare two branches, one for each point cloud segment, from\na pair of input segments. Each branch consists of a spatial\ntransformer network to align the point cloud and followed\nby 4 EdgeConv layers (to construct graphs over k nearest\nneighbors calculated by KNN) with 64, 64, 64, 128 kernel\nsizes, respectively. After which, one convolutional layer\nwith 256 channels is used to embed the four embeddings\nobtained by the four EdgeConv layers to high dimensional\nspace.\nThe feature then is pooled into a 256-dimension\nvector by applying the max-pooling layer. The two fea-\nture vectors from the two branches are then concatenated\ninto a vector to be fed to three fully-connected layers (with\n1, 024, 512, 2 vector lengths, respectively). The ReLU acti-\nvation and batch normalization are used for each layer and\n50% dropout is used on each fully-connected layer. The\ncross-entropy loss is optimized by Adam and backpropaga-\ntion to compute the gradient.\n3.2. ClusterNet: Knowledge Transfer with Clusters\nThe underline intuition of clustering is that 3D objects\nfrom the same categories have high similarity than those\nfrom different categories. After obtaining the clusters of\nthe data by using the Kmeans++, based on the features ex-\ntracted by ContrastNet, the cluster IDs of the data are used\nas the ”pseudo” labels to train a ClusterNet, so that more\nmeaningful features may be extracted from it. We hope that\nusing cluster IDs as pseudo labels in ClusterNet can provide\nmore powerful self-supervision and therefore, the network\ncan learn more representative features for object classiﬁca-\ntion.\nGiven any unlabeled point cloud dataset X\n=\n{x1, x2, . . . , xN} of N images, the clustering process can\nbe parameterized as [11]:\nmin\nC∈Rd×k\n1\nN\nN\nX\nn=1\nmin\nyn∈{0,1}k ∥fθ(xn)−Cyn∥2\n2,\ny⊤\nn 1k = 1,\n(1)\nwhere fθ is the feature extractor that can map any point\ncloud data into a vector, θ is the set of corresponding pa-\nrameters that need to be optimized, yn is the cluster ID.\nSolving this clustering problem provides a set of optimal as-\nsignments (y∗\nn)n≤N and a centroid matrix C∗. The cluster\nID assignments (y∗\nn)n≤N are then used as the pseudo-labels\nto train a ClusterNet.\nThe training of the ClusterNet, also based on DGCNN\n[35], with the cluster ID assignments as the pseudo-labels,\nis described as:\nmin\nθ,W\n1\nN\nN\nX\nn=1\nℓ(gW (fθ(xn)) , yn) ,\n(2)\nwhere the purpose of training is to ﬁnd the optimal parame-\nters θ∗such that the mapping fθ∗produces good general-\npurpose features for point cloud data classiﬁcation.\nA\nparameterized classiﬁer gW predicts the correct labels of\nthe data based on the features fθ(xn).\nAll the parame-\nters are learned by optimizing this loss function. In super-\nvised training, parameters θ are optimized with the human-\nannotated labels while each data xn is paired with a human-\nannotated label yn in {0, 1}k. In our unsupervised learning\ntraining, each data xn is paired with a pseudo label yn that\nis generated by the clustering algorithm. The label yn indi-\ncates the data’s membership to one of the k clusters, where\nk can be speciﬁed in the clustering algorithm. In our ex-\nperiments (below), various number of clusters are tested for\ncomparing the impact on feature extraction.\n4. Experimental Results\nWe conduct extensive experiments to evaluate the pro-\nposed approach and the quality of the learned features for\npoint cloud on the point cloud classiﬁcation task.\n4.1. Implementation Details\nContrastNet:\nDuring the part contrast unsupervised\nlearning, each object is cut by randomly generated 15 planes\ninto 30 segments.\nEach selected segment has at least\n512 points. Any two segments from the same object are\ntreated as the positive samples while any two segments from\ntwo different objects are treated as negative samples. The\nDGCNN is used as the backbone of the ContrastNet. Dur-\ning the unsupervised part contrast training phase, the learn-\ning rate is 0.001, momentum is 0.9, the optimizer is Adam,\nthe learning rate decay rate is 0.7, and the decay step is\n200000.\nClusterNet: The Kmeans++ is used as the clustering\nalgorithm to cluster the data based on the embeddings ex-\ntracted by the ContrastNet. We test the performance of dif-\nferent cluster numbers to train the ClusterNet. The same\nDGCNN structure is used as the backbone in the Cluster-\nNet except that the size of the last dense layer is the cluster\nnumber. During the training with pseudo labels, the learn-\ning rate is 0.001, momentum is 0.9, the optimizer is Adam,\nthe learning rate decay rate is 0.7, and the decay step is\n200000.\n4.2. Datasets\nAll the experiments for both the ContrastNet and Clus-\nterNet are done on three point cloud benchmarks: Model-\nNet40, ShapeNet, and ModelNet10. Data augmentation in-\ncluding random rotation, shift, and jittering are used during\nall the training phases. The ModelNet40 dataset contains\n12, 311 meshed CAD models covering 40 classes. There are\n9, 843 and 2, 468 samples in the training and testing splits,\nrespectively. In all our experiments, 1024 points are ran-\ndomly picked for each model during the training and test-\ning phases. This dataset is used to train and test our un-\nsupervised learning method. During training, this dataset\nhas been used for learning features without using the class\nlabels. During the testing phase, this dataset is used to eval-\nuate the quality of the learned features. The ModelNet10\ndataset contains 10 categories including 3991 meshed CAD\nmodels for training and 909 models for testing. We ran-\ndomly sample 2048 points from the mesh faces and use their\n(x, y, z) coordinates as input in all experiments. This model\nis only used for testing the quality of the learned features.\nThe ShapeNet part dataset that contains 16 categories in-\ncluding 12, 137 models for training and 2, 874 for testing\nfrom ShapeNet dataset. In all our experiments, 1024 points\nare randomly picked for each model during the training and\ntesting phases. This dataset is used for unsupervised train-\ning.\n4.3. Can ContrastNet Fulﬁll Part Contrast Task?\nThe hypothesis of our idea is that the ContrastNet is able\nto learn semantic features by accomplishing the ”part con-\ntrasting” pretext task, and then the learned features can be\nused for other downstream tasks, such as point cloud classi-\nﬁcation. Therefore, we test the performance of ContrastNet\nin verifying whether two patches belong to the same ob-\nject. No human-annotated labels are used during the train-\ning phase of the ContrastNet.\nThe performance of the ContrastNet in part contrasting is\nshown in Table 1. The average accuracy of part contrasting\nis more than 90% on the two datasets, with cross-dataset\ntesting.\nFigure 4. Visualization of object embedding of the ModelNet10\ntest data through part contrast training on the ShapeNet dataset.\nThe features are learned by part contrast learning (left) and then\nboosted by object clustering (right).\nTo verify whether the ContrastNet learned useful fea-\ntures, we visualize the testing data by using TSNE, as\nshown in Fig. 4 (left). All of the data covering 10 classes\nof ModelNet10 is visualized.\nThe ﬁgure shows that the\nContrastNet indeed learned semantic features and the data\nfrom the same class are closer than the data from differ-\nent classes. Later Clustering enhances the features more (as\nshown in Fig. 4 (right), which will be discussed more in the\nimpact on classiﬁcation.\n4.4. Transfer Features Learned by ContrastNet to\nClassiﬁcation Task\nTo quantitatively evaluate the quality of the learned fea-\ntures for classiﬁcation by using the part contrasting pretext\ntask, we conduct experiments on three different datasets:\nShapeNet, ModelNet10, and ModelNet40. The features are\nextracted by the ContrastNet only trained for the part con-\ntrasting task on unlabeled data. A linear classiﬁer SVM is\ntrained based on the learned features, and the testing classi-\nﬁcation accuracy is reported in the column ”ContrastNet” in\nTable 2. Following the practice of previous work [38, 36],\nwe conduct cross-dataset training and testing to verify the\ngeneralization ability of features between different datasets.\nAs shown in Table 2, when trained only with a linear\nclassiﬁer SVM on one dataset, the ContrastNet trained on\nShapeNet is able to achieve 84.1% and 91.0% on Model-\nNet40 and ModelNet10 dataset respectively. As a compar-\nison, the model trained on ModelNet40 and tested on the\nTraining\nTesting\nAccuracy(%)\nShapeNet\nShapeNet\n94.0\nShapeNet\nModelNet40\n86.4\nModelNet40\nModelNet40\n95.0\nModelNet40\nShapeNet\n90.9\nTable 1. Performance of ContrastNet in part contrasting on\nShapeNet and ModelNet40 datasets.\nsame dataset achieved 85.7%. These results validate the\neffectiveness of the proposed method and that the learned\nfeatures by the proposed unsupervised learning method can\nbe transferred among different datasets.\n4.5. Can Clustering Boost Performance?\nThe part contrast learning indeed forces the ContrastNet\nto learn semantic features. However, the fact that objects\nbelong to the same classes were treated as different ob-\njects in part contrasting (without knowing their labels) may\nhave a negative impact on the quality of learned features\nby ContrastNet. Therefore, clustering is applied to discover\nthe objects with similar appearances and the ClusterNet is\ntrained to learn features by using the cluster IDs as object\nlabels. We hope that the ClusterNet should be able to help\nthe model learn more discriminative features for classiﬁca-\ntion.\nThe features extracted by the ContrastNet for a dataset\nare used to group the data into a number of clusters using the\nKmeans++ algorithm. A ClusterNet is trained from scratch\nwith 3D point cloud data as the input to predict the cluster\nID of each data sample. After the training is ﬁnished, the\nnetwork is tested on the point cloud classiﬁcation task using\nthe same SVM on the same three benchmarks as above. The\nclassiﬁcation results are shown in column ”ClusterNet” of\nTable 2, where the number of clusters is selected as 300 (see\nbelow for a discussion).\nTraining\nTesting\nContrastNet(%)\nClusterNet(%)\nShapeNet\nModelNet40\n84.1\n86.8 (+2.7)\nShapeNet\nModelNet10\n91.0\n93.8 (+2.8)\nModelNet40\nModelNet40\n85.7\n88.6 (+2.9)\nTable 2. Comparison of 3D object classiﬁcation results using Con-\ntrastNet and ClusterNet.\nAs shown in Table 2, training the ClusterNet to predict\nthe cluster ID of each data, generated by Kmeans++ based\non the features learned by ContrastNet, can signiﬁcantly\nboost the point cloud classiﬁcation accuracy. The clustering\nboosts the classiﬁcation accuracy on all the three datasets by\nat least 2.7%. These improvements validate the effective-\nness of using clustering to boost the quality of the learned\nfeatures, also visualized shown in Fig. 4.\nWe also conduct experiments to evaluate the impact of\nthe numbers of clusters on the quality of features.\nBy\nvarying the numbers of clusters, we have examined the\npoint cloud classiﬁcation accuracy on the three benchmark\ndatasets. As shown in Table 3, the points cloud classiﬁca-\ntion performance ﬁrst improved when larger cluster num-\nbers are used and then saturated when the numbers are\nlarger than certain values. When more clusters are applied,\nthe ﬁne-grained object groups should be discovered which\nprobably leads to more discriminative features.\n4.6. Quality of Clustering: Further Study\nTo further analysis the quality of learned features, we\nevaluate the quality of the clusters by calculating the accu-\nracy of each cluster. Speciﬁcally, for each cluster, we assign\nthe category label of the majority data as the label and eval-\nuate the accuracy of all the data. We cluster the ShapeNet\nand ModelNet40 into 16 and 40 clusters respectively, since\nthese numbers equal to the actual numbers of categories in\nthe two datasets.\nAs shown in Table 4, the cluster accuracy on ShapeNet\nis 83.4% which means that 83.4% of the data are correctly\nclustered into the same labels using Kmeans++. The clus-\ntering accuracy on ModelNet40 is 64.2%, which is much\nlower than that of ShapNet, probably because ModelNet has\nmore categories than ShapeNet. Nevertheless, even with\nthe low clustering accuracy, the testing accuracy obtained\nafter the ClusterNet using SVM on ModelNet40 dataset is\n87.4%, a 23.2% ”improvement” over the training data ”ac-\ncuracy”, which means ClusterNet can signiﬁcantly optimize\nthe quality of the features with a clustering step for the data.\nThe clustering can cluster point cloud objects into groups\nthat the objects from the same groups have smaller distances\nin the feature space while objects from different groups have\nlarger distances in the feature space. The quality of the clus-\nter indicates the discriminative ability of the learned fea-\ntures. Therefore, we randomly select 6 clusters and show\nthe cluster center of each and the top 5 objects that closest\nto the cluster center. As shown in Fig. 5, the object from the\nsame cluster have very high similar appearance and geome-\ntry.\nShapeNet\nShapeNet\nModelNet40\nClusters\nModelNet40\nModelNet10\nModelNet40\n100\n86.2%\n93.5%\n87.7%\n200\n86.4%\n93.6%\n88.2%\n300\n86.8%\n93.8%\n88.6%\n400\n86.5%\n93.2%\n87.8%\nTable 3. The relation of number of clusters and the performance\non point cloud classiﬁcation. The classiﬁcation performance im-\nproved slightly when larger cluster numbers are used.\nPre-training\nClusters\nClustering Acc.\nTesting Acc.\nShapeNet\n16\n83.4%\n86.1%\nModelNet40\n40\n64.2%\n87.4%\nTable 4. The accuracy of the clustering and testing results. The\nclustering accuracy seems to depend on the numbers of the cate-\ngories. The testing accuracy are obtained from ClusterNet using\nSVM, that has 23.2% improvement on ModelNet40.\nFigure 5. Visualizing clustering result after applying Kmeans++\non the unlabeled data. The ﬁrst column is the discovered centrioid\nof each cluster and the other ﬁve columns are the top ﬁve closest\ndata to the centroid.\n4.7. Practical Considerations: Occlusions and Per-\nspective Views\nWe have known that ContrastNet and ClusterNet can\nlearn useful features for recognizing full objects. However,\nin real-life a sensor can only observe part of an object due\nto occlusions and/or perspective views. Fig. 6 and Fig. 1\nshow that different perspective views and various part seg-\nments (respectively) from one object might be very differ-\nent from each other, even when 3D point cloud can still be\nobtained. To verify if our deep models can still classify\npart segments and perspective views, we train two sets of\nour ContrasNet and ClusterNet models and test the classi-\nﬁcation accuracy on ModelNet40, using part segments and\nperspective views, respectively. In each case, after extract-\ning features using the ContrasNet using the part segments\n(or perspective views), the pseudo labels are obtained using\nthe ContrastNet features with Kmeans++. Then we train a\nClusterNet for each case and extract features using part seg-\nments (or perspective views) on ModelNet40. Note that in\neach case, the features in both training and testing and by\nboth the ContrastNet model and ClusterNet model are ex-\ntracted on part segments (or perspective views) instead of\nfull objects. Each part segment (or perspective view) shall\ncontain at least 512 points. A linear SVM is trained based\non the features extracted from the two steps (ContrastNet\nand ClusterNet) to obtain the classiﬁcation performance.\nThe results in Table 5 shows that the accuracy (in col-\numn ”Acc. Part”) of part segments classiﬁcation and the ac-\ncuracy (in column ”Acc. Perspective”) of perspective views\nclassiﬁcation are 82.4% and 75.8%. This indicates that even\nthough we only feed the part segments or perspective views\nto our networks, they still can learning high-quality features\nFigure 6. Several perspective views of 3D point cloud of an object.\nThe views from different perspectives might be totally different.\nfor the classiﬁcation task. The results validate the practi-\ncal usefulness of the proposed approach. Nevertheless, we\nwould like to note that when the full object data points are\nused for the training and testing (in column ”Acc. Full”,\nlisted again here from Table 2 ), the performance is around\n6% higher than using the features obtained from part seg-\nments and around 13% higher than using the features from\nperspective views. This might also indicate that the per-\nspective views which only have 3D points on surfaces from\nsingle perspective views are not as descriptive as parts of\nthe volumetric body of a 3D object.\nModel\nAcc. Full\nAcc. Part\nAcc. Perspective\nContrastNet\n85.7%\n79.4%\n72.0%\nClusterNet\n88.6%\n82.4%\n75.8%\nTable 5. The columns ”Acc. Full”, ”Acc. Part”, and ”Acc. Per-\nspective” are accuracy obtained from the features extracted on full\nobjects, part segments, and perspective views, respectively.\n4.8. Comparison with the State of the Art\nIn this section, we compare our approach with both su-\npervised models [23, 25, 17, 35] and other unsupervised\nlearning models [15, 7, 9, 30, 36, 1, 38] on point cloud clas-\nsiﬁcation benchmarks ModelNet10 and ModelNet40. In all\ncomparison, we use the ClusterNet.\nFollowing the common practice [38, 36], all the unsu-\npervised models are trained on the ShapeNet data with the\nsame procedure (Table 6).\nAll the methods ran a linear\nSVM upon on the high-dimensional features obtained by\nusnupervised training. The methods in [15, 7] are hand-\ncrafted features and methods in [9, 30, 36, 1, 38] are deep\nlearning based methods. On the MoldelNet40 dataset, our\nmethod outperforms all the methods except FoldingNet\n(1.6% lower), which is the latest work for unsupervised fea-\nture learning. On the ModelNet10 dataset, our methods out-\nperforms SPH [15], LFD [7], TLNetwork [9], VConv-DAE\n[30], and 3DGAN [36], and only 0.6% lower than Fold-\ningNet [38]. We would like to note that our ClusterNet has\na much simpler structure and is much easier in training.\nWe also compare the performance with the most recent\nsupervised methods including PointNet [23], PointNet++\nModels\nModelNet40 (%)\nModelNet10 (%)\nSPH [15]\n68.2\n79.8\nLFD [7]\n75.5\n79.9\nT-L Network [9]\n74.4\n-\nVConv-DAE [30]\n75.5\n80.5\n3D-GAN [36]\n83.3\n91.0\nLatent-GAN [1]\n85.7\n95.3\nFoldingNet [38]\n88.4\n94.4\nClusterNet (Ours)\n86.8\n93.8\nTable 6. The comparison on classiﬁcation accuracy between our\nClusterNet and other unsupervised methods on point cloud classi-\nﬁcaton dataset ModelNet40 and ModelNet10.\n[25], PointCNN [17], and DGCNN [35]. All the parameters\nof these methods are trained with human-annotated labels,\nwhile our results are obtained by training linear SVM based\non the features extracted by the ClustertNet. As shown in\nTable 7, the supervised methods have better performance\nbecause all the parameters are tuned by the hand-annotated\nlabels. With the unsupervised learned features and a linear\nSVM, the performance of our model (using unsupervised\nDGCNN as the base model) is only 3.6% lower than the\nsupervised DGCNN. These results demonstrate the effec-\ntiveness of our unsupervised learning method.\nModels\nAcc.(%)\nModels\nAcc.(%)\nPointNet [23]\n89.2\nDGCNN [35]\n92.2\nPointNet++ [25]\n90.7\nPointCNN [17]\n92.2\nClusterNet(Ours)\n88.6\nTable 7. The comparison on classiﬁcation accuracy between our\nunsupervised ClusterNet and the supervised methods on point\ncloud classiﬁcaton on ModelNet40.\n5. Conclusion\nWe have proposed a straightforward and effective\nmethod for learning high-level features for point cloud data\nfrom unlabeled data. The experiment results demonstrate\nthat proposed pretext tasks (part contrasting and object clus-\ntering) are able to provide essential semantic information\nof the point cloud data for the network to learn semantic\nfeatures. Our proposed methods have been evaluated on\nthree public point cloud benchmarks and obtained compara-\nble performance with other state-of-the-art self-supervised\nlearning methods and showed practical applications to oc-\ncluded and perspective data.\n6. Acknowledgments\nThe work is supported by the US National Science Foun-\ndation via awards #CNS-1737533 and #IIP-1827505, and\nBentley Systems, Inc. under a CUNY-Bentley Collabora-\ntive Research Agreement (CRA).\nReferences\n[1] P. Achlioptas, O. Diamanti, I. Mitliagkas, and L. Guibas.\nRepresentation learning and adversarial generation of 3d\npoint clouds.\narXiv preprint arXiv:1707.02392, 2(3):4,\n2017.\n[2] M. Aubry, U. Schlickewei, and D. Cremers. The wave kernel\nsignature: A quantum mechanical approach to shape analy-\nsis. In ICCVW, pages 1626–1633. IEEE, 2011.\n[3] M. M. Bronstein and I. Kokkinos. Scale-invariant heat ker-\nnel signatures for non-rigid shape recognition. In 2010 IEEE\nComputer Society Conference on Computer Vision and Pat-\ntern Recognition, pages 1704–1711. IEEE, 2010.\n[4] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral\nnetworks and locally connected networks on graphs. arXiv\npreprint arXiv:1312.6203, 2013.\n[5] M. Caron, P. Bojanowski, A. Joulin, and M. Douze. Deep\nclustering for unsupervised learning of visual features. In\nECCV, pages 132–149, 2018.\n[6] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan,\nQ. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su,\net al. Shapenet: An information-rich 3d model repository.\narXiv preprint arXiv:1512.03012, 2015.\n[7] D.-Y. Chen, X.-P. Tian, Y.-T. Shen, and M. Ouhyoung. On\nvisual similarity based 3d model retrieval.\nIn Computer\ngraphics forum, volume 22, pages 223–232. Wiley Online\nLibrary, 2003.\n[8] Y. Fang, J. Xie, G. Dai, M. Wang, F. Zhu, T. Xu, and\nE. Wong. 3d deep shape descriptor. In CVPR, pages 2319–\n2328, 2015.\n[9] R. Girdhar, D. F. Fouhey, M. Rodriguez, and A. Gupta.\nLearning a predictable and generative vector representation\nfor objects. In ECCV, pages 484–499. Springer, 2016.\n[10] K. Guo, D. Zou, and X. Chen. 3d mesh labeling via deep\nconvolutional neural networks. TOG, 35(1):3, 2015.\n[11] J. A. Hartigan and M. A. Wong. Algorithm as 136: A k-\nmeans clustering algorithm. Journal of the Royal Statistical\nSociety. Series C (Applied Statistics), 28(1):100–108, 1979.\n[12] L. Jing and Y. Tian. Self-supervised spatiotemporal feature\nlearning by video geometric transformations. arXiv preprint\narXiv:1811.11387, 2018.\n[13] L. Jing and Y. Tian.\nSelf-supervised visual feature learn-\ning with deep neural networks: A survey.\narXiv preprint\narXiv:1902.06162, 2019.\n[14] A. E. Johnson and M. Hebert. Using spin images for efﬁcient\nobject recognition in cluttered 3d scenes. TPAMI, 21(5):433–\n449, 1999.\n[15] M. Kazhdan, T. Funkhouser, and S. Rusinkiewicz. Rotation\ninvariant spherical harmonic representation of 3 d shape de-\nscriptors. In Symposium on geometry processing, volume 6,\npages 156–164, 2003.\n[16] R. Klokov and V. Lempitsky. Escape from cells: Deep kd-\nnetworks for the recognition of 3d point cloud models. In\nICCV, pages 863–872, 2017.\n[17] Y. Li, R. Bu, M. Sun, W. Wu, X. Di, and B. Chen. Pointcnn:\nConvolution on x-transformed points. In Advances in Neural\nInformation Processing Systems, pages 820–830, 2018.\n[18] H. Ling and D. W. Jacobs. Shape classiﬁcation using the\ninner-distance. TPAMI, 29(2):286–299, 2007.\n[19] J. Masci, D. Boscaini, M. Bronstein, and P. Vandergheynst.\nGeodesic convolutional neural networks on riemannian man-\nifolds. In ICCVW, pages 37–45, 2015.\n[20] D. Maturana and S. Scherer. Voxnet: A 3d convolutional\nneural network for real-time object recognition.\nIn IROS,\npages 922–928. IEEE, 2015.\n[21] M. Noroozi and P. Favaro. Unsupervised learning of visual\nrepresentations by solving jigsaw puzzles. In ECCV, pages\n69–84. Springer, 2016.\n[22] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A.\nEfros. Context encoders: Feature learning by inpainting. In\nCVPR, pages 2536–2544, 2016.\n[23] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep\nlearning on point sets for 3d classiﬁcation and segmentation.\nIn CVPR, pages 652–660, 2017.\n[24] C. R. Qi, H. Su, M. Nießner, A. Dai, M. Yan, and L. J.\nGuibas. Volumetric and multi-view cnns for object classi-\nﬁcation on 3d data. In CVPR, pages 5648–5656, 2016.\n[25] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet++: Deep\nhierarchical feature learning on point sets in a metric space.\nIn NIPS, pages 5099–5108, 2017.\n[26] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-\nsentation learning with deep convolutional generative adver-\nsarial networks. arXiv preprint arXiv:1511.06434, 2015.\n[27] R. B. Rusu, N. Blodow, and M. Beetz. Fast point feature\nhistograms (fpfh) for 3d registration. In ICRA, pages 3212–\n3217. IEEE, 2009.\n[28] R. B. Rusu, N. Blodow, Z. C. Marton, and M. Beetz. Align-\ning point cloud views using persistent feature histograms.\nIn 2008 IEEE/RSJ International Conference on Intelligent\nRobots and Systems, pages 3384–3391. IEEE, 2008.\n[29] M. Savva, F. Yu, H. Su, M. Aono, B. Chen, D. Cohen-\nOr, W. Deng, H. Su, S. Bai, X. Bai, et al. Shrec16 track:\nlargescale 3d shape retrieval from shapenet core55. In Pro-\nceedings of the eurographics workshop on 3D object re-\ntrieval, pages 89–98, 2016.\n[30] A. Sharma, O. Grau, and M. Fritz. Vconv-dae: Deep vol-\numetric shape learning without object labels. In European\nConference on Computer Vision, pages 236–250. Springer,\n2016.\n[31] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and\nT. Funkhouser.\nSemantic scene completion from a single\ndepth image. In CVPR, pages 1746–1754, 2017.\n[32] H. Su, S. Maji, E. Kalogerakis, and E. Learned-Miller. Multi-\nview convolutional neural networks for 3d shape recognition.\nIn ICCV, pages 945–953, 2015.\n[33] J. Sun, M. Ovsjanikov, and L. Guibas. A concise and prov-\nably informative multi-scale signature based on heat diffu-\nsion. In Computer graphics forum, volume 28, pages 1383–\n1392. Wiley Online Library, 2009.\n[34] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Octree gen-\nerating networks: Efﬁcient convolutional architectures for\nhigh-resolution 3d outputs.\nIn ICCV, pages 2088–2096,\n2017.\n[35] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and\nJ. M. Solomon. Dynamic graph cnn for learning on point\nclouds. arXiv preprint arXiv:1801.07829, 2018.\n[36] J. Wu, C. Zhang, T. Xue, B. Freeman, and J. Tenenbaum.\nLearning a probabilistic latent space of object shapes via\n3d generative-adversarial modeling. In NIPS, pages 82–90,\n2016.\n[37] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and\nJ. Xiao. 3d shapenets: A deep representation for volumetric\nshapes. In CVPR, pages 1912–1920, 2015.\n[38] Y. Yang, C. Feng, Y. Shen, and D. Tian. Foldingnet: Point\ncloud auto-encoder via deep grid deformation.\nIn CVPR,\npages 206–215, 2018.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2019-04-28",
  "updated": "2019-08-13"
}