{
  "id": "http://arxiv.org/abs/1904.12462v1",
  "title": "Deep Learning-Based Video Coding: A Review and A Case Study",
  "authors": [
    "Dong Liu",
    "Yue Li",
    "Jianping Lin",
    "Houqiang Li",
    "Feng Wu"
  ],
  "abstract": "The past decade has witnessed great success of deep learning technology in\nmany disciplines, especially in computer vision and image processing. However,\ndeep learning-based video coding remains in its infancy. This paper reviews the\nrepresentative works about using deep learning for image/video coding, which\nhas been an actively developing research area since the year of 2015. We divide\nthe related works into two categories: new coding schemes that are built\nprimarily upon deep networks (deep schemes), and deep network-based coding\ntools (deep tools) that shall be used within traditional coding schemes or\ntogether with traditional coding tools. For deep schemes, pixel probability\nmodeling and auto-encoder are the two approaches, that can be viewed as\npredictive coding scheme and transform coding scheme, respectively. For deep\ntools, there have been several proposed techniques using deep learning to\nperform intra-picture prediction, inter-picture prediction, cross-channel\nprediction, probability distribution prediction, transform, post- or in-loop\nfiltering, down- and up-sampling, as well as encoding optimizations. In the\nhope of advocating the research of deep learning-based video coding, we present\na case study of our developed prototype video codec, namely Deep Learning Video\nCoding (DLVC). DLVC features two deep tools that are both based on\nconvolutional neural network (CNN), namely CNN-based in-loop filter (CNN-ILF)\nand CNN-based block adaptive resolution coding (CNN-BARC). Both tools help\nimprove the compression efficiency by a significant margin. With the two deep\ntools as well as other non-deep coding tools, DLVC is able to achieve on\naverage 39.6\\% and 33.0\\% bits saving than HEVC, under random-access and\nlow-delay configurations, respectively. The source code of DLVC has been\nreleased for future researches.",
  "text": "Deep Learning-Based Video Coding:\nA Review and A Case Study\nDong Liu, Yue Li, Jianping Lin, Houqiang Li, Feng Wu\nAbstract\nThe past decade has witnessed great success of deep learning technology in many disciplines, especially in\ncomputer vision and image processing. However, deep learning-based video coding remains in its infancy. This\npaper reviews the representative works about using deep learning for image/video coding, which has been an actively\ndeveloping research area since the year of 2015. We divide the related works into two categories: new coding schemes\nthat are built primarily upon deep networks (deep schemes), and deep network-based coding tools (deep tools) that\nshall be used within traditional coding schemes or together with traditional coding tools. For deep schemes, pixel\nprobability modeling and auto-encoder are the two approaches, that can be viewed as predictive coding scheme\nand transform coding scheme, respectively. For deep tools, there have been several proposed techniques using deep\nlearning to perform intra-picture prediction, inter-picture prediction, cross-channel prediction, probability distribution\nprediction, transform, post- or in-loop Ô¨Åltering, down- and up-sampling, as well as encoding optimizations. According\nto the newest reports, deep schemes have achieved comparable or even higher compression efÔ¨Åciency than the state-\nof-the-art traditional schemes, such as High EfÔ¨Åciency Video Coding (HEVC) based scheme, for image coding; deep\ntools have demonstrated the compression capability beyond HEVC for video coding. However, deep schemes have\nnot yet reached the current height of HEVC for video coding, and deep tools remain largely unexplored at many\naspects including the tradeoff between compression efÔ¨Åciency and encoding/decoding complexity, the optimization\nfor perceptual naturalness or semantic quality, the speciality and universality, the federated design of multiple deep\ntools, and so on. In the hope of advocating the research of deep learning-based video coding, we present a case study\nof our developed prototype video codec, namely Deep Learning Video Coding (DLVC). DLVC features two deep\ntools that are both based on convolutional neural network (CNN), namely CNN-based in-loop Ô¨Ålter (CNN-ILF) and\nCNN-based block adaptive resolution coding (CNN-BARC). Both tools help improve the compression efÔ¨Åciency by\na signiÔ¨Åcant margin. With the two deep tools as well as other non-deep coding tools, DLVC is able to achieve on\naverage 39.6% and 33.0% bits saving than HEVC, under random-access and low-delay conÔ¨Ågurations, respectively.\nThe source code of DLVC has been released for future researches.\nIndex Terms\nDeep learning, image coding, prediction, transform, video coding.\nThe authors are with the CAS Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, University of\nScience and Technology of China, Hefei 230027, China (e-mail: dongeliu@ustc.edu.cn).\narXiv:1904.12462v1  [cs.MM]  29 Apr 2019\n1\nDeep Learning-Based Video Coding:\nA Review and A Case Study\nI. INTRODUCTION\nA. Image/Video Coding\nImage/video coding usually refers to the computing technology that compresses image/video into binary code (i.e.\nbits) so as to facilitate storage and transmission. The compression may or may not ensure perfect reconstruction\nof image/video from the bits, which is termed lossless and lossy coding respectively. For natural image/video,\nthe compression efÔ¨Åciency of lossless coding is usually below requirement, so most of efforts are devoted to\nlossy coding. Lossy image/video coding solutions are evaluated at two aspects: Ô¨Årst is the compression efÔ¨Åciency,\ncommonly measured by the number of bits (coding rate), the less the better; second is the incurred loss, commonly\nmeasured by the quality of the reconstructed image/video compared with the original image/video, the higher the\nbetter.\nImage/video coding is a fundamental and enabling technology for computer image processing, computer vision,\nand visual communication. The research and development of image/video coding can be dated back to as early as\n1960s, much earlier than the appearance of modern imaging, image processing, and visual communication systems.\nAs an example, Picture Coding Symposium, a prestigious international forum devoted speciÔ¨Åcally to advancements\nin image/video coding, started in the year of 1969. Since then, numerous efforts from both academia and industry\nhave been devoted to this Ô¨Åeld.\nDue to the requirement of interoperability, a series of standards regarding image/video coding have been crafted\nin the past three decades. In international standardization organizations, ISO/IEC has two experts group namely Joint\nPhotographic Experts Group (JPEG) and Moving Picture Experts Group (MPEG) for standardization of image/video\ncoding technology, while ITU-T has its own Video Coding Experts Group (VCEG). These organizations have\npublished several famous, widely adopted standards, such as JPEG [121], JPEG 2000 [103], H.262 (MPEG-2 Part\n2) [115], H.264 (MPEG-4 Part 10 or AVC) [128], H.265 (MPEG-H Part 2 or HEVC) [108], and so on. At present,\nH.265/HEVC, which was formally published in 2013, represents the state-of-the-art image/video coding technology.\nAlong with the progress of video technology, especially the popularization of ultra-high deÔ¨Ånition (UHD) video,\nthere is an urgent requirement to further increase compression efÔ¨Åciency so as to accommodate UHD video in\nlimited storage and limited transmission bandwidth. Thus, after HEVC, MPEG and VCEG form the Joint Video\nExperts Team (JVET) to explore advanced video coding technology, and the team developed Joint Exploration\nModel (JEM) for study. Moreover, since the year of 2018, the JVET team has been working on a new video coding\nstandard, informally called Versatile Video Coding (VVC), as the successor of HEVC. It is anticipated that VVC\nmay improve the compression efÔ¨Åciency by saving around 50% bits while maintaining the same quality, especially\n2\nfor UHD video, compared to HEVC. Nonetheless, it is worth noting that the improvement of VVC is probably\nachieved at the cost of multiplicative encoding/decoding complexity.\nB. Deep Learning for Image/Video Coding\nThe past decade has witnessed the emerging and booming of deep learning, a class of techniques that are\nincreasingly adopted in the hope of approaching the ultimate goal of artiÔ¨Åcial intelligence [59]. Deep learning\nbelongs to machine learning technology, and has the distinction of its computational models, known as deep artiÔ¨Åcial\nneural networks or deep networks for short, which are composed of multiple (usually more than three) processing\nlayers, each layer is further composed of multiple simple but non-linear basic computational units. One beneÔ¨Åt\nof such deep networks is believed to be the capacity for processing data with multiple levels of abstraction, and\nconverting data into different kinds of representations. Note that these representations are not manually designed;\ninstead, the deep network including the processing layers is learned from massive data using a general machine\nlearning procedure. Deep learning eliminates the necessity of handcrafted representations, and thus is regarded\nuseful especially for processing natively unstructured data, such as acoustic and visual signal, whilst processing\nsuch data has been a longstanding difÔ¨Åculty in the artiÔ¨Åcial intelligence Ô¨Åeld.\nSpeciÔ¨Åcally for processing image/video, deep learning using convolutional neural network (CNN) has revolution-\nized the paradigm in computer vision and image processing. In 2012, Krizhevsky et al. [57] designed a 8-layer\nCNN, which won the image classiÔ¨Åcation challenge by a surprisingly low error rate compared with previous works.\nIn 2014, Girshick et al. [33] promoted the performance of object detection by a signiÔ¨Åcant margin with the proposed\nregions with CNN features. Also in 2014, Dong et al. [27] proposed a 3-layer CNN for single image super-resolution\n(SR), which outperforms the previous methods in both reconstruction quality and computational speed. In 2017,\nZhang et al. [142] presented a deep CNN for image denoising, and demonstrated that a single CNN model may\ntackle with several different image restoration tasks including denoising, single image SR, and compression artifact\nreduction, while these tasks had been studied separately for a long while.\nWitnessing such successful cases, experts cannot help but ask whether deep learning can beneÔ¨Åt image/video\ncoding as well. In history, artiÔ¨Åcial neural network is not strange to the image/video coding community. From\n1980s to 1990s, a number of researches were conducted on neural network-based image coding [28], [48], but\nthen the networks were shallow and the compression efÔ¨Åciency was not satisfactory. Thanks to the abundance of\ndata, the more and more powerful computing platform, and the development of advanced algorithms, it is now\npossible to train very deep networks with even more than 1000 layers [40]. Thus, the exploration of using deep\nlearning for image/video coding is worthy of reconsideration, and indeed has been an actively developing research\narea since 2015. At present, researches have shown promising results, conÔ¨Årmed the feasibility of deep learning-\nbased image/video coding. Nonetheless, this technology is far from mature and calls for much more research and\ndevelopment efforts.\nIn this paper, we aim to provide a comprehensive review of the newest reports about deep learning-based\nimage/video coding (until the end of 2018), as well as to present a case study of our developed prototype video\n3\ncodec namely Deep Learning Video Coding (DLVC), so as to make interested readers aware of the status quo.\nReaders may also refer to [84] for a recently published review paper about the same theme.\nThe remainder of this paper is organized as follows. Sections II and III provide a review of related works about\nusing deep learning for image/video coding. The related works are divided into two categories, and reviewed in\nthe two sections respectively. The Ô¨Årst category is deep schemes, i.e. new coding schemes that are built primarily\nupon deep networks; the second category is deep tools, i.e. deep network-based coding tools that are embedded\ninto traditional, non-deep coding schemes; a deep tool may either replace its counterpart in the traditional scheme,\nor be newly added into the scheme. Section IV presents the case study of our developed DLVC, with all the design\ndetails and experimental results. Section V summarizes our perspectives on some open problems for future research,\nand then concludes this paper. Table I lists the abbreviations used in this paper.\nC. Preliminaries\nIn this paper, we consider coding methods for natural image/video, i.e. the image/video as-is seen by human\ntaken by daily cameras or mobile phones. Although the methods are usually generally applicable, they have been\nspeciÔ¨Åcally designed for natural image/video, and may not perform that well for other kinds (e.g. biomedical,\nremote-sensing).\nCurrently, almost all the natural image/video is in digital format. A grayscale digital image can be denoted by\nx ‚ààDm√ón, where m and n are the number of rows (height) and number of columns (width) of the image, and D\nis the deÔ¨Ånition domain of a single picture element (pixel). For example, D = {0, 1, . . . , 255} is a common setting,\nwhere |D| = 256 = 28, thus the pixel value can be represented by an 8-bit integer; accordingly, an uncompressed\ngrayscale digital image has 8 bits-per-pixel (bpp), while compressed bits are deÔ¨Ånitely less.\nA color image is usually decomposed into multiple channels to record the color information. For example,\nusing the RGB color space, a color image can be denoted by x ‚ààDm√ón√ó3, where 3 corresponding to three\nchannels‚ÄìRed, Green, Blue. Since human vision is more sensitive to luminance than to chrominance, the YCbCr\n(YUV) color space is much more adopted than RGB, and the U and V channels are typically down-sampled\nto achieve compression. For example, in the so-called YUV420 color format, a color image can be denoted by\nX = {xY ‚ààDm√ón, xU ‚ààD\nm\n2 √ó n\n2 , xV ‚ààD\nm\n2 √ó n\n2 }.\nA color video is composed by multiple color images, called frames, to record the scene at different timestamps.\nFor example, in the YUV420 color format, a color video can be denoted by V = {X0, X1, . . . , XT ‚àí1} where\nT is the number of frames, each Xi = {x(i)\nY\n‚ààDm√ón, x(i)\nU\n‚ààD\nm\n2 √ó n\n2 , x(i)\nV\n‚ààD\nm\n2 √ó n\n2 }. If m = 1080, n =\n1920, |D| = 210, and a video has 50 frames-per-second (fps), then the data rate of the uncompressed video is\n1080 √ó 1920 √ó (10 + 10\n4 + 10\n4 ) √ó 50 = 1, 555, 200, 000 bits-per-second (bps), about 1.555 Gbps. Obviously, the\nvideo should be compressed by a ratio of hundreds to thousands before it can be efÔ¨Åciently transmitted over the\ncurrent wired and wireless networks.\nThe existing lossless coding methods can achieve a compression ratio of about 1.5 to 3 for natural image, which is\nclearly below requirement. Thus, lossy coding is introduced to compress more but at the cost of incurring loss. The\n4\nTABLE I\nLIST OF ABBREVIATIONS\nAbbreviation\nRemark\nAVC\nAdvanced Video Coding, i.e. H.264 [128]\nBARC\nblock adaptive resolution coding\nBD-rate\nBjontegaard‚Äôs delta-rate [13]\nBPG\nBetter Portable Graphics, an image coding format based on HEVC\nCNN\nconvolutional neural network\nCTU\ncoding tree unit\nCU\ncoding unit\nDLVC\nDeep Learning Video Coding, our developed prototype video codec\nGAN\ngenerative adversarial network\nHEVC\nHigh EfÔ¨Åciency Video Coding, i.e. H.265 [108]\nHM\nHEVC reference software\nILF\nin-loop Ô¨Ålter\nJEM\nJoint Exploration Model, a video coding software developed by JVET\nJPEG1\nJoint Photographic Experts Group, a group of ISO/IEC\nJPEG2\na standard published by ISO/IEC [121]\nJVET\nJoint Video Experts Team, a team of MPEG and VCEG\nLSTM\nlong short-term memory\nMAE\nmean-absolute-error\nMC\nmotion compensation\nME\nmotion estimation\nMPEG\nMoving Picture Experts Group, a group of ISO/IEC\nMSE\nmean-squared-error\nMS-SSIM\nmulti-scale SSIM\nPSNR\npeak signal-to-noise ratio\nQP\nquantization parameter\nReLU\nrectiÔ¨Åed linear unit [90]\nRNN\nrecurrent neural network\nSR\nsuper-resolution\nSSIM\nstructural similarity [126]\nVCEG\nVideo Coding Experts Group, a group of ITU-T\nVVC\nVersatile Video Coding, an incoming video coding standard\nloss can be measured by the difference between original and reconstructed images, e.g. using mean-squared-error\n(MSE) for grayscale image:\nMSE = ||x ‚àíxrec||2\nm √ó n\n(1)\nAccordingly, the quality of reconstructed image compared with original image can be measured by peak signal-to-\nnoise ratio (PSNR):\nPSNR = 10 √ó log10\n(max(D))2\nMSE\n(2)\nwhere max(D) is the maximal value in D, e.g. 255 for 8-bit grayscale image. For color image/video, the PSNR\nvalues of Y, U, V are usually separately calculated. For video, the PSNR values of different frames are usually\n5\nùë•ùëö√óùëõ\nùë•ùëõ\nùë•ùëñ\nùë•1\nFig. 1. Illustration of a typical predictive coding scheme, where the pixels are encoded/decoded one by one in the raster scan order. For the\npixel xi (marked gray), all the previous pixels (marked green), i.e. above of xi and left in the same row of xi, can be used as condition to\npredict the pixel value of xi. The green area is also called the context for xi. For simpliÔ¨Åcation, context can be chosen as a subset of the green\narea.\nseparately calculated and then averaged. There are other quality metrics in replacement of PSNR, such as structural\nsimilarity (SSIM) and multi-scale SSIM (MS-SSIM) [126].\nTo compare different lossless coding schemes, it is sufÔ¨Åcient to compare the compression ratio, or the resulting\nrate (bpp, bps, etc.). To compare different lossy coding schemes, it is necessary to take into account both rate\nand quality. For example, to calculate the relative rates at several different quality levels, and then to average the\nrates, is a commonly adopted method; the average relative rate is known as Bjontegaard‚Äôs delta-rate (BD-rate) [13].\nThere are other important aspects to evaluate image/video coding schemes, including encoding/decoding complexity,\nscalability, robustness, and so on.\nII. REVIEW OF DEEP SCHEMES\nIn this section we review some representative coding schemes that are built primarily upon deep networks.\nGenerally speaking, there are two approaches for deep image coding schemes, i.e. pixel probability modeling and\nauto-encoder. These two approaches are combined together in several deep schemes. In addition, we discuss deep\nvideo coding schemes and special-purpose coding schemes, where special-purpose schemes are further categorized\ninto perceptual coding and semantic coding.\nA. Pixel Probability Modeling\nAccording to Shannon‚Äôs information theory [102], the optimal method for lossless coding can reach the minimal\ncoding rate ‚àílog2 p(x) where p(x) is the probability of the symbol x. To reach this target, a number of lossless\ncoding methods have been invented, and arithmetic coding is believed to be among the optimal ones [129]. In\nessence, given a probability p(x), arithmetic coding ensures the coding rate to be as near as possible to ‚àílog2 p(x)\nup to rounding error. Thus, the remaining problem is to Ô¨Ånd out the probability, which is however very difÔ¨Åcult for\nnatural image/video as it is of very high dimension.\n6\nOne way to estimate p(x), where x is an image, is to decompose the image into m√ón pixels and to estimate the\nprobabilities of these pixels one by one (e.g. in the raster scan order). This is a typical predictive coding strategy.\nNote that\np(x) = p(x1)p(x2|x1) ¬∑ ¬∑ ¬∑ p(xi|x1, . . . , xi‚àí1) ¬∑ ¬∑ ¬∑ p(xm√ón|x1, . . . , xm√ón‚àí1)\n(3)\nwhich is illustrated in Fig. 1. Here the condition for xi is also called the context for xi. When the image is large,\nthe conditional probability can be difÔ¨Åcult to estimate. A simpliÔ¨Åcation is to reduce the range of context, e.g.\np(x) = p(x1)p(x2|x1) ¬∑ ¬∑ ¬∑ p(xi|xi‚àík, . . . , xi‚àí1) ¬∑ ¬∑ ¬∑ p(xm√ón|xm√ón‚àík, . . . , xm√ón‚àí1)\n(4)\nwhere k is a prechosen constant.\nAs known, deep learning is good at solving regression and classiÔ¨Åcation problems. Therefore, it has been proposed\nto estimate the probability p(xi|x1, . . . , xi‚àí1) given the context x1, . . . , xi‚àí1, using trained deep networks. This\nstrategy is proposed for other kinds of high-dimensional data in as early as 2000 [12], but is applied to image/video\nuntil recently. For example, in [58], the probability estimation is considered for binary images, i.e. xi ‚àà{‚àí1, +1},\nwhere it sufÔ¨Åces to predict a single probability value p(xi = +1|x1, . . . , xi‚àí1) for each pixel. The paper presents\nthe neural autoregressive distribution estimator (NADE), where a feed-forward network with one hidden layer is\nused for each pixel, and the parameters are shared across these networks. The parameter sharing also help to speed\nup the computations for each pixel. A similar work is presented in [37], where the feed-forward network also has\nconnections skipping the hidden layer, and the parameters are also shared. Both [58] and [37] perform experiments\non the binarized MNIST dataset1. Uria et al. [116] extend the NADE to real-valued NADE (RNADE), where the\nprobability p(xi|x1, . . . , xi‚àí1) is made up by a mixture of Gaussians, and the feed-forward network needs to output\na set of parameters for the Gaussian mixture model, instead of a single value in NADE. Their feed-forward network\nhas one hidden layer and parameter sharing, but the hidden layer is equipped with rescaling to avoid saturation,\nand uses rectiÔ¨Åed linear unit (ReLU) [90] instead of sigmoid. They also consider mixture of Laplacians rather than\nGaussians. Experiments are conducted on 8√ó8 natural image patches, where the pixel value is added with noise\nand converted to real value. In [117], NADE and RNADE are improved by using different orderings of the pixels\nas well as using more hidden layers in the network. In [120], RNADE is improved by enhancing the Gaussian\nmixture model (GMM) with deep GMM.\nDesigning advanced networks has been an important theme for improving pixel probability modeling. In [109],\nmulti-dimensional long short-term memory (LSTM) based network is proposed, together with mixtures of conditional\nGaussian scale mixtures, a generalization of GMM, for probability modeling. LSTM is a kind of recurrent neural\nnetworks (RNNs), and is regarded good at modeling sequential data. The spatial variant of LSTM is used for images.\nLater in [118], several different networks are studied, including RNNs and CNNs that are known as PixelRNN and\nPixelCNN, respectively. For PixelRNN, two variants of LSTM, called row LSTM and diagonal BiLSTM, are\nproposed, where the latter is speciÔ¨Åcally designed for images. PixelRNN incorporates residual connections [40] to\nhelp train deep networks with up to 12 layers. For PixelCNN, in order to suit for the shape of context (see Fig.\n1The raw MNIST dataset can be accessed at http://yann.lecun.com/exdb/mnist/.\n7\n1), masked convolutions are proposed. PixelCNN is also as deep as having 15 layers. Compared with previous\nworks, PixelRNN and PixelCNN are more dedicated to natural images: they consider pixels as discrete values (e.g.\n0, 1, . . . , 255), and predict a multinomial distribution over the discrete values; they deal with color images (in RGB\ncolor space); multi-scale PixelRNN is proposed; and they work well on the CIFAR-10 and ImageNet datasets. Quite\na number of researches follow the approach of PixelRNN and PixelCNN. In [119], Gated PixelCNN is proposed\nto improve the PixelCNN, and achieves comparable performance with PixelRNN but with much less complexity.\nIn [99], PixelCNN++ is proposed with the following improvements upon PixelCNN: a discretized logistic mixture\nlikelihood is used rather than a 256-way multinomial distribution; down-sampling is used to capture structures at\nmultiple resolutions; additional short-cut connections are introduced to speed up training; dropout is adopted for\nregularization; RGB is combined for one pixel. In [18], PixelSNAIL is proposed, in which casual convolutions are\ncombined with self attention.\nMost of the aforementioned works directly model pixel probability. In addition, pixel probability may be modeled\nas a conditional one upon explicit or latent representations. That says, we may estimate\np(x|h) =\nm√ón\nY\ni=1\np(xi|x1, . . . , xi‚àí1, h)\n(5)\nwhere h is the additional condition. Note also that p(x) = p(h)p(x|h), which means the modeling is split into\nan unconditional and a conditional. For example, in [119], the additional condition can be image class or high-\nlevel image representations that are derived by another deep network. In [56], PixelCNN with latent variables are\nconsidered, where the latent variables are derived from the original image: they can be a quantized grayscale version\nof the original color image, or a multi-resolution image pyramid.\nRegarding practical image coding schemes, in [64], a network with trimmed convolutions is adopted to predict\nprobabilities for binary data, while a 8-bit grayscale image with the size of m √ó n is converted to a binary cube\nwith the size of m √ó n √ó 8 to be processed by the network. The network is similar to PixelCNN but is of three\ndimension. The trimmed convolutional network-based arithmetic encoding (TCAE) is reportedly better than the\nprevious non-deep lossless coding schemes, such as TIFF, GIF, PNG, JPEG-LS, and JPEG 2000-LS; on the Kodak\nimage set2, TCAE achieves a compression ratio of 2.00. Differently, in [4], CNN is used in the wavelet transform\ndomain rather than the pixel domain, i.e. CNN is to predict wavelet detail coefÔ¨Åcients from coefÔ¨Åcients within\nneighboring subbands.\nFor video coding, in [52], PixelCNN is generalized to video pixel network (VPN) for the pixel probability\nmodeling of video. VPN is composed of CNN encoders (for previous frames to predict the current frame) and\nPixelCNN decoders (for prediction inside the current frame). CNN encoders preserve at all layers the spatial\nresolution of input frames to maximize representational capacity. Dilated convolutions are adopted to enlarge\nreceptive Ô¨Åelds and better capture global motion. The outputs of the CNN encoders are combined over time with\na convolutional LSTM that also preserves the resolution. The PixelCNN decoders use masked convolutions and\n2The Kodak image set can be accessed at http://www.r0k.us/graphics/kodak/.\n8\nCode\nSpace\nImage   \nSpace\nùíö\nùíô\n‡∑ùùíö\n‡∑ùùíô\nùëîùëé\nùëû\nùëîùë†\nPerception\nSpace\nùíõ\n‡∑úùíõ\nùëîùëù\nùëîùëù\nùê∑\nùëÖ\nmin\nùëîùëé,ùëîùë†,ùëûùê∑ùíõ, ‡∑úùíõ+ ùúÜùëÖ(ùëû(ùíö))\nFig. 2. Illustration of a typical transform coding scheme. The original image x is transformed by an analysis function ga to achieve the code\ny. The code y is quantized (denoted by q) and compressed into bits. The number of bits is used to measure the coding rate (R). The quantized\ncode ÀÜy is then inversely transformed by a synthesis function gs to achieve the reconstructed image ÀÜx. Both of x and ÀÜx are further transformed\nby a same perceptual function gp, resulting in z and ÀÜz, respectively. The difference between z and ÀÜz is used to measure the distortion (D).\nadopt multinomial distributions over discrete pixel values. VPN is experimented on the Moving MNIST and Robotic\nPushing datasets.\nIn addition, Schiopu et al. [101] investigate a lossless image coding scheme, where they use CNN to predict pixel\nvalue rather than its distribution. The predicted value is subtracted from the true pixel value, resulting in residue\nthat is then coded. In addition, they consider the adaptive selection among the CNN predictor and some non-CNN\npredictors.\nB. Auto-Encoder\nAuto-encoder originates from the well-known work of Hinton and Salakhutdinov [42], which trains a network for\ndimensionality reduction and the network consists of encoding part and decoding part. The encoding part converts\nan input high-dimension signal to its low-dimension representation, and the decoding part recovers (not perfectly)\nthe high-dimension signal from the low-dimension representation. Auto-encoder enables automated learning of\nrepresentations and eliminates the need of hand-crafted features, which is also believed to be one of the most\nimportant advantages of deep learning.\nIt seems quite straightforward to adopt the auto-encoder network for lossy image coding: the encoder and decoder\nare trained out, and we just need to encode the learned representation. However, the traditional auto-encoder is not\noptimized for compression, and directly using a trained auto-encoder is not efÔ¨Åcient [127]. When we consider the\ncompression requirement, there are several challenges: First, the low-dimension representation shall be quantized\nthen coded, but the quantization step is not differentiable, making a difÔ¨Åculty to train the network. Second, lossy\ncoding is to achieve a better tradeoff between rate and quality, so the rate shall be taken into account when training\nthe network, but the rate is not easy to calculate or estimate. Third, a practical image coding scheme needs to consider\nvariable rate, scalability, encoding/decoding speed, interoperability, and so on. In response to these challenges, a\nnumber of researches have been conducted especially in recent years.\nA conceptual illustration of auto-encoder-based image coding scheme is shown in Fig. 2, which is a typical\ntransform coding strategy. The original image x is transformed to y = ga(x), and y is quantized then coded. The\n9\ndecoded ÀÜy is inversely transformed to ÀÜx = gs(ÀÜy). Considering the tradeoff between rate and quality, we can train\nthe network to minimize the joint rate-distortion cost D + ŒªR where D is calculated or estimated as the difference\nbetween x and ÀÜx (note that the difference may be calculated or estimated in a perception space), R is calculated\nor estimated from the quantized code, and Œª is the Lagrange multiplier. All of the existing researches follow this\nscheme more or less and differ in their network structure and loss function.\nFor the network structure, RNNs and CNNs are the widely used two categories. The most representative works\ninclude:\n‚Ä¢ Toderici et al. [111] propose a general framework for variable rate image compression. They use binary\nquantization to generate codes, and do not consider the rate during training, i.e. the loss is only end-to-end\ndistortion, measured by MSE. Their framework indeed provides a scalable coding functionality, where RNN\n(speciÔ¨Åcally LSTM) with convolutional and deconvolutional layers is reported to perform well. They provide\nresults on a large-scale dataset of 32√ó32 thumbnails. Later, Toderici et al. [112] propose an improved version,\nwhere they use a neural network like PixelRNN [118] to compress the binary codes; they also introduce a\nnew gated recurrent unit (GRU) inspired by the residual network (ResNet) [40]. They report better results\nthan JPEG on the Kodak image set using MS-SSIM as quality metric. Johnston et al. [51] further improve\nthe RNN-based method by introducing hidden-state priming into RNN, using an SSIM-weighted loss function,\nand enabling spatially adaptive bitrates. They achieve better results than BPG on the Kodak image set using\nMS-SSIM. Covell et al. [22] enable spatially adaptive bitrates by training stop-code tolerant RNNs.\n‚Ä¢ Ball¬¥e et al. [9] propose a general framework for rate-distortion optimized image compression. They use multiary\nquantization to generate integer codes and consider the rate during training, i.e. the loss is the joint rate-\ndistortion cost, where distortion can be MSE or others. To estimate the rate, they use adding a random noise\nto replace the quantization during training, and use the differential entropy of the noisy ‚Äúcode‚Äù as a proxy\nfor the rate. As for the network structure, they use the generalized divisive normalization (GDN) transform,\nwhich consists of a linear mapping (matrix multiplication) followed by a nonlinear parametric normalization;\nthe effectiveness of the proposed GDN for image coding is veriÔ¨Åed in [8]. Later, Ball¬¥e et al. [10] propose an\nimproved version, where they use 3 convolutional layers each followed by down-sampling and a GDN operation\nto implement the transform; accordingly, the use 3 layers of inverse GDN + up-sampling + convolution to\nimplement the inverse transform. In addition, they design an arithmetic coding method to compress the integer\ncodes. They report better results than JPEG and JPEG 2000 on the Kodak image set using MSE as quality\nmetric. Furthermore, Ball¬¥e et al. [11] improve their scheme by incorporating a scale hyper-prior into the auto-\nencoder, which is inspired by the variational auto-encoder [55]. They use another transform ha to convert y\ninto w = ha(y), quantize and encode w (transmitted as side information), and use another inverse transform\nhs to convert the decoded ÀÜw into the estimated standard deviation of the quantized ÀÜy, which is then used\nduring the arithmetic coding of ÀÜy. On the Kodak image set and using PSNR as quality metric, their method\nis only slightly worse than BPG.\nBesides [9], several works also concentrate on dealing with the non-differentiable quantization and/or the es-\n10\ntimation of rate. Theis et al. [110] adopt a very simple work-around for quantization: quantization is performed\nas usual in the forward pass, but the gradients are directly passed through the quantization layer in the backward\npass. Surprisingly this work-around works well. In addition, they replace the rate with an upper bound that is\ndifferentiable. Dumas et al. [29] consider a stochastic winner-take-all mechanism, where the entries in y with\nthe largest absolute values are kept and the other entries are set to 0; then the entries are uniformly quantized\nand compressed. Agustsson et al. [2] propose a soft-to-hard vector quantization scheme, where they use a soft\nquantization (i.e. assigning a representation to multiple codes with different membership values) rather than hard\nquantization (i.e. assigning a representation to only one code) during training, and they adopt an annealing process\nto let the soft quantization approach the hard quantization gradually. Note that their scheme takes advantage of\nvector quantization while other works usually adopt scalar quantization. Li et al. [65] introduce an importance map\nfor rate estimation, where the importance map is quantized to a mask and the mask decides how many bits are kept\nat each location, thus the sum of the importance map can be used as a rough estimate of the coding rate.\nBesides [111], several works also consider the functionality of variable rate with less or no training for different\nrates. In [110], scale parameters are introduced and a pretrained auto-encoder is Ô¨Åne-tuned for different rates. In\n[30], a unique learned transform is proposed, together with variable quantization step for different rates. In [15],\na multi-scale decomposition transform is trained and optimized for all scales; and rate allocation algorithms are\nprovided to determine the optimal scale of each image block for either a target rate or a target quality factor.\nBesides, scalable coding is considered in [146] differently from that in [111]. In [146], an image is decomposed\ninto multiple bit-planes, which are transformed and quantized in parallel; bidirectional assembling gated units are\nproposed to reduce the correlation between different bit-planes.\nSeveral works consider advanced network structures and different loss functions. Theis et al. [110] adopt a sub-\npixel structure for computational efÔ¨Åciency. Rippel and Bourdev [97] present a pyramid decomposition followed\nby inter-scale alignment network, which is lightweight and runs in real-time. They also use a discriminator loss\nin addition to the reconstruction loss. Snell et al. [104] use the MS-SSIM as loss function instead of MSE or\nmean-absolute-error (MAE) to train auto-encoders, and they Ô¨Ånd that MS-SSIM is better calibrated to perceptual\nquality. Zhou et al. [149] use deeper networks for encoder/decoder and a separate network for post-processing at\nthe decoder side. They also replace the Gaussian model in [11] with the Laplacian model.\nAs mentioned before, pixel probability modeling represents predictive coding and auto-encoder represents trans-\nform coding. These two strategies can be combined for higher compression efÔ¨Åciency. Mentzer et al. [87] propose\na practical lossless image coding scheme, where they use auto-encoders at multiple levels to learn the condition\nfor pixel probability modeling. Mentzer et al. [86] integrate pixel probability modeling (a 3D PixelCNN) into auto-\nencoder so as to estimate the coding rate and to train the PixelCNN and the auto-encoder jointly. Baig et al. [6]\nintroduce partial-context image inpainting into the variable rate compression framework [111], which is actually\nto predict a block from the block‚Äôs context, assuming the blocks are encoded/decoded one by one in the raster\nscan order (similar to what is shown in Fig. 1 but at the block level). The prediction signal is added onto the\nnetwork output signal to achieve ÀÜx, i.e. the transform coding network deals with the prediction residues. Minnen\net al. [89] additionally consider rate allocation among the blocks. Similarly but in a different manner, Minnen et\n11\nal. [88] improve upon [11] by augmenting the hyper-prior with the context, i.e. they use not only ÀÜw but also the\ncontext to predict the probability of each entry of ÀÜy. Their method outperforms BPG on the Kodak image set and\nusing PSNR as quality metric, which represents the state of the art by the end of 2018. Lee et al. [60] introduce\nthe context adaptive entropy model into the hyper-prior ÀÜw.\nMoreover, Cheng et al. [21] apply principle component analysis on the learned representation, which is virtually\na second transform.\nC. Video Coding\nStarting from 2017, a few researches have been reported for deep video coding schemes. Compared to image\ncoding, video coding calls for efÔ¨Åcient methods to remove the inter-picture redundancy. Inter-picture prediction\nis then an important issue in these researches. Motion estimation and compensation is widely adopted, but is\nimplemented by trained deep networks until recently.\nChen et al. [17] seems the Ô¨Årst to report a video coding scheme by using trained deep networks as auto-encoders.\nSpeciÔ¨Åcally, they divide video frames into 32√ó32 blocks and for each block they choose one from two modes:\nintra coding or inter coding. If using intra coding, there is an auto-encoder to compress the block. If using inter\ncoding, then they perform motion estimation and compensation using the traditional method, and input the residues\nto another auto-encoder. For both auto-encoders, the encoded representations are directly quantized and coded by\nthe Huffman method. This scheme is quite rough and does not compete H.264.\nWu et al. [131] propose a video coding scheme with image interpolation, where the key frames (I frames) are Ô¨Årst\ncompressed by the deep image coding scheme in [112], and the remaining frames (B frames) are then compressed\nin a hierarchical order. For each B frame, two compressed frames (either I frames or previously compressed B\nframes) before and after are used to ‚Äúinterpolate‚Äù the current frame: the motion information is used to warp the\ntwo compressed frames (i.e. motion compensation), and then the two warped frames are sent as side information\nto a variable rate image coding scheme that processes the current frame. The scheme is reported to perform on par\nwith H.264.\nChen et al. [20] propose another video coding scheme with the so-called PixelMotionCNN. In their scheme,\nframes are compressed in the temporal order, and each frame is divided into blocks that are compressed in the\nraster scan order. Before one frame is compressed, the previous two compressed frames are used to ‚Äúextrapolate‚Äù\nthe current frame. When a block is to be compressed, the extrapolated frame together with the block‚Äôs context are\nsent to the PixelMotionCNN to generate a prediction signal for the current block, then the prediction residues are\ncompressed by the variable rate image coding scheme in [112]. This scheme also performs on par with H.264.\nLu et al. [80] propose a real end-to-end deep video coding scheme, which can be viewed as a ‚Äúdeepened‚Äù\nversion of the traditional video coding schemes. SpeciÔ¨Åcally in their scheme, for each frame to be compressed,\nan optical Ô¨Çow estimation module is used to obtain the motion information between the frame and the previous\ncompressed frames. Motion compensation is also performed by a trained network, to generate a prediction signal for\nthe current frame. For the prediction residues and the motion information, two auto-encoders are used to compress\nthem respectively. The entire network is jointly optimized with a single loss function, i.e. the joint rate-distortion\n12\ncost. This scheme reportedly achieves better compression efÔ¨Åciency than H.264, and even outperforms HEVC (x265\nencoder) when evaluated with MS-SSIM.\nRippel et al. [98] present the to-date most sophisticated deep video coding scheme, which inherits and extends\na deepened version of the traditional video coding schemes. Their scheme has the following new features: (1) only\none auto-encoder to compress motion information and prediction residues simultaneously; (2) a state that is learned\nfrom the previous frames and updated recursively; (3) motion compensation with multiple frames and multiple\noptical Ô¨Çows; (4) a rate control algorithm. This scheme is reported to outperform HEVC reference software (HM)\nwhen evaluated with MS-SSIM.\nBy the end of 2018, we do not observe any report that a deep video coding scheme can outperform HM when\nevaluated with PSNR, which seems a hard mission.\nD. Special-Purpose Coding\nMost of the researches about deep schemes concern image/video coding for signal Ô¨Ådelity, i.e. to minimize the\ndistortion between original and reconstructed image/video subject to a given rate, where the distortion can be deÔ¨Åned\nas MSE or other differences. However, if we do not concern the Ô¨Ådelity, we may instead care about the perceptual\nnaturalness of the reconstructed image/video, or the utility of the reconstructed image/video in semantic analysis\ntasks. The latter two kinds of quality metrics are termed perceptual naturalness and semantic quality. There have\nbeen a few works that tailor image/video coding for these quality metrics.\n1) Perceptual Coding: Since the boom of generative adversarial network (GAN) [34], deep networks are known\nto be capable in generating perceptually natural images. Leveraging this capability at the decoder side can surely\nimprove the perceptual quality of decoded images. Different from the generator in normal GANs, the decoder should\nalso ensure the decoded images to be similar to original images, which raises a problem of controlled generation\nand the encoder actually provides the control signal in the coded bits.\nInspired by the variational auto-encoder (VAE) [55], Gregor et al. [36] propose Deep Recurrent Attentive Writer\n(DRAW) for image generation, which extends the traditional VAE by using RNNs as encoder and decoder. Unfolding\nthe encoder RNN produces a series of latent representations. Then, Gregor et al. [35] introduce convolutional DRAW,\nand observe that it is able to transform an image into a series of increasingly detailed representations, ranging from\nglobal conceptual aspects to low level details. Thus, they suggest a conceptual compression scheme, whose one\nbeneÔ¨Åt is to achieve plausible reconstruction images at very low bit rates.\nIt has been realized that perceptual naturalness can be evaluated by the discriminator in GAN [14]. Several\nresearches are devoted to deep coding schemes for perceptual quality using the discriminator loss solely or jointly\nwith MSE or other losses. For example, Santurkar et al. [100] propose the so-called generative compression schemes\nfor both image and video. For image, they Ô¨Årst train a canonical GAN, then they use the generator as the decoder, Ô¨Åx\nit, and train the encoder to minimize a sum of MSE and feature loss. For video, they reuse the encoder and decoder\ntrained for image, transmit only a few frames, and restore the other frames at the decoder side via interpolation. Their\nschemes are able to achieve very high compression ratio. Kim et al. [54] build a new video compression scheme,\nwhere a few key frames are normally compressed (by H.264) and the other frames are extremely compressed.\n13\nIndeed, edges are extracted from the down-sampled non-key frames and transmitted. At the decoder side, the key\nframes are Ô¨Årstly reconstructed, then edges are similarly extracted from them. A conditional GAN is trained with\nthe reconstructed key frames where edge is the condition. Then the conditional GAN is used to generate the non-key\nframes. Again, their scheme performs well at very low bit rates.\n2) Semantic Coding: A few researches have been conducted on deep coding schemes that preserve the semantic\ninformation or concern the semantic quality.\nAgustsson et al. [3] present a GAN-based image compression scheme for extremely low bit rates. The scheme\ncombines auto-encoder and GAN, collapsing the decoder and the generator into one. In addition, a semantic label\nmap can be used as an additional input to the encoder, and as a condition for the discriminator. It is reported that\nthe proposed scheme reconstructs images with higher semantic quality, in the sense that the semantic segmentation\non these images is more accurate than that on BPG-compressed images at the same rate.\nLuo et al. [81] propose a concept of deep semantic image compression (DeepSIC), which incorporates the\nsemantic information (e.g. classes) into the coded bits. There are two versions of DeepSIC, both based on auto-\nencoder. In the one version, the semantic information is extracted from the representation y, and encoded into\nthe bits. In the other version, the semantic information is not encoded, but extracted at the decoder side from the\nquantized representation ÀÜy. Torfason et al. [113] investigate performing semantic analysis tasks (classiÔ¨Åcation and\nsemantic segmentation) from the quantized representation rather than from the reconstructed image. That says, the\ndecoding process is omitted. They show that the classiÔ¨Åcation and segmentation accuracy values are very close\nbetween the representation and the image, but the computational complexity is reduced signiÔ¨Åcantly. Zhang et al.\n[143] study a deep image coding scheme for simultaneous compression and retrieval. Their motivation is that the\ncoded bits can be used not only for reconstructing image but also for retrieving similar images without decoding.\nThey use an auto-encoder to compress image into bits, and use a revised classiÔ¨Åcation network to extract binary\nfeatures. Then they combine the two parts of bits, and Ô¨Åne-tune the feature extraction network for image retrieval.\nTheir results indicate that at the same rate, the reconstructed images are better than JPEG-compressed ones, and\nthe retrieval accuracy improves due to the Ô¨Åne-tuning.\nAkbari et al. [5] design a scalable image coding scheme where the coded bits consist of three layers. The Ô¨Årst\nlayer is the semantic segmentation map coded losslessly. The second layer is a down-sampled version of the original\nimage also coded losslessly. With the Ô¨Årst two layers, a network is trained to predict the original image and the\nprediction residues are coded by BPG as the third layer. This scheme is reported to outperform BPG on the Kodak\nimage set when evaluated with PSNR and MS-SSIM.\nChen and He [19] consider deep coding for facial images with semantic quality metric instead of PSNR or\nperceptual quality. For this purpose, their loss function has three parts: MAE, discriminator loss, and a semantic\nloss, where the semantic loss is to project the original and reconstructed images into a compact Euclidean space\nthrough a learned transformation, and to calculate the Euclidean distance between them. Accordingly, their scheme\nperforms very well when evaluated with face veriÔ¨Åcation accuracy at the same rate.\n14\nCoder Control\nEntropy \nCoding\nLuma Intra \nEstimation\nChroma Intra \nEstimation\nIntra Prediction\nInter Prediction\nMotion \nEstimation`\nDequant. & \nInv. Transform\nUp-\nSampling\nIn-Loop\nFilters\nPost-Loop \nFilters\nTransform & \nQuantization\nDown-\nSampling\nInput Video\nOutput Video\n1\n2\n3\n5\n9\n7\n6\n8\n4\n8\n5\n1\nDeep Intra-Picture \nPrediction\n2\nDeep Cross-Channel \nPrediction\n3\nDeep Inter-Picture \nPrediction\n4\nDeep Probability\nDistribution Prediction\n5\nDeep Transform\n6\nDeep In-Loop Filtering\n7\nDeep Post-Loop Filtering\n8\nDeep Down- and Up-\nSampling\n9\nDeep Encoding \nOptimization\nSplit into CTUs\nCoded Bits\nIntra/Inter \nSelection \nFig. 3. Illustration of a traditional hybrid video coding scheme as well as the locations of deep tools inside the scheme. Note that the yellow\nlines indicate the Ô¨Çow of prediction, and the blue boxes indicate the tools that are used at the encoder side only.\nIII. REVIEW OF DEEP TOOLS\nIn this section we review some representative works about using trained deep networks as tools within the\ntraditional coding schemes or together with traditional coding tools. Generally speaking, the traditional video coding\nschemes adopt a hybrid coding strategy, i.e. a combination of predictive coding and transform coding. As depicted\nin Fig. 3, an input video sequence is divided into pictures, pictures are divided into blocks (the largest block is called\nCTU, which can be divided into smaller CUs, in HEVC [108]), and blocks are divided into channels (i.e. Y, U, V).\nThe pictures/blocks/channels are compressed in a predeÔ¨Åned order, and the previously compressed ones can be used\nto predict the following ones, which is known as intra-picture prediction (between blocks), cross-channel prediction\n(between channels), and inter-picture prediction (between pictures), respectively. The prediction residues are then\ntransformed and quantized and entropy coded to achieve the Ô¨Ånal bits. Some auxiliary information such as block\npartition and prediction mode is also entropy coded into the bits (not shown in the Ô¨Ågure). Probability distribution\nprediction is used in the entropy coding step. Since the quantization step loses information and may cause artifacts,\nÔ¨Åltering is proposed to enhance the reconstructed video, which may be performed in-loop (before predicting the\nnext picture) or out-of-loop (before output). In addition, to reduce the data volume, the pictures/blocks/channels may\nbe down-sampled before being compressed, and up-sampled afterwards. Finally, the encoder needs to control the\ndifferent modules and combine them to achieve a tradeoff between coding rate, quality, and computational speed.\nEncoding optimization is an important theme in practical coding systems.\n15\n2N\nL\n2N\nL\nN\nN\nIPFCN\nùêæ1\nùêæ2\nùêæùëë=ùëÅ2\nFig. 4. Illustration of a fully connected network for intra prediction (IPFCN).\nTrained deep networks can act as almost all of the modules shown in Fig. 3, where we have indicated different\nlocations for deep tools. In the following, we will review the works about deep tools according to where they are\nused in the entire scheme.\nA. Intra-Picture Prediction\nIntra-picture prediction, or intra prediction for short, is a tool to predict between blocks inside the same picture.\nH.264 introduces intra prediction with several predeÔ¨Åned prediction modes, such as DC prediction and extrapolation\nalong different directions [128]. The encoder can choose a prediction mode for each block and signal the choice to\nthe decoder. To decide mode, it is a common strategy to compare the coding rate and distortion of different modes\nand to select the mode with the minimal rate-distortion cost. In HEVC, more prediction modes are introduced [108].\nLi et al. [63] propose a fully connected network for intra prediction that is depicted in Fig. 4. For the current\nN √ó N block, they use L rows above and L columns to the left, in total 4NL + L2 pixels as context. They use an\nimage set known as the New York City Library to generate training data, in which the raw image is compressed at\ndifferent quantization parameters (QPs). When training the network, they investigate two strategies: the Ô¨Årst is to\ntrain a single model with all training data, and the second is to split the training data into two groups by considering\nthe HEVC prediction modes, and to train two models respectively. The strategy of two models turns out better for\ncompression. They integrate the trained networks as new prediction modes along with the HEVC modes. They\nreport around 3% BD-rate reduction than HM.\nPfaff et al. [94] also adopt fully connected network for intra prediction, but propose to train multiple networks\nas different prediction modes. In addition, they propose to train a separate network whose input is also the block‚Äôs\ncontext but output is the predicted likelihood of different modes. Moreover, they propose to use a different transform\nfor each of the network-based prediction modes. Their reported performance is high: around 6% BD-rate reduction\nthan an improved version of HM (with advanced block partitioning).\nHu et al. [44] devise a progressive spatial RNN for intra prediction. Different from the above works, they propose\nto leverage the sequential modeling capacity of RNN to generate prediction progressively from the context to the\nblock. In addition, they suggest the use of sum-of-absolute-transformed-difference (SATD) as the loss function and\nargue that SATD correlates better to the rate-distortion cost.\n16\nCui et al. [23] consider a CNN for intra prediction, or more speciÔ¨Åcally, intra prediction reÔ¨Ånement. They use\nthe HEVC prediction modes to generate prediction, and then use a trained CNN to reÔ¨Åne the prediction. Note that\nthe CNN has not only the HEVC prediction but also the context as its input. This method seems achieving only\nmarginal gain.\nB. Inter-Picture Prediction\nInter-picture prediction, or inter prediction for short, is a tool to predict between video frames so as to remove\nthe redundancy along the temporal dimension. Inter prediction is the kernel of video coding and it largely decides\nthe compression efÔ¨Åciency of a video coding scheme. In the traditional video coding schemes, inter prediction is\nmostly fulÔ¨Ålled by block-level motion estimation (ME) and motion compensation (MC). Given a reference frame\nand a block to be coded, ME is to Ô¨Ånd the location in the reference frame where the content is the most similar\nto that inside the to-be-coded block, and MC is to retrieve the content at the found location so as to predict the\nblock. Many techniques have been proposed to improve block-level ME and MC, such as using multiple reference\nframes, bi-directional inter prediction (i.e. using two reference frames jointly), fractional-pixel ME and MC, and\nso on.\nInspired by the multiple reference frames, Lin et al. [71] propose a new inter prediction mechanism by extrapo-\nlating the multiple reference frames. SpeciÔ¨Åcally they adopt a Laplacian pyramid of GANs to extrapolate a frame\nfrom the previously compressed four frames. This extrapolated frame serves as another reference frame. They report\naround 2% BD-rate reduction than HM.\nInspired by the bi-directional inter prediction, Zhao et al. [148] propose a method to enhance the prediction\nquality. The previous bi-directional prediction simply computes a linear combination of two prediction blocks.\nThey propose to employ trained CNN to combine the two prediction blocks in a nonlinear and data-driven manner.\nInspired by the fractional-pixel ME and MC, a number of researches are conducted on the fractional-pixel\ninterpolation problem, which aims at generating imaginary pixels at fractional locations on the reference frame\nbecause the motion between two frames is not aligned to integer pixels. Here, a major difÔ¨Åculty is how to\nprepare training data because fractional pixels are imaginary. Yan et al. [137] propose to use a CNN for half-\npixel interpolation, where they suggest a method that blurs a high-resolution image and then samples pixels from\nthe blurred image: odd locations as integer pixels and even locations as half pixels. This method is inherited in [76],\nwhere the authors analyze the effect of different blurring degrees. Zhang et al. [141] propose another method, which\nformulates the fractional interpolation as a resolution enhancement problem. Thus, they down-sample high-resolution\nimages to achieve training data. Yan et al. [136] consider a different formulation, treating the fractional-pixel MC\nas an inter-picture regression problem. They use video sequences to retrieve training data, where they rely on\nthe fractional-pixel ME to align different frames, and use reference frame as integer pixels and current frame\nas fractional pixels. Yan et al. [135] further discover a key characteristic of the fractional interpolation problem,\nnamely its invertibility: if fractional pixels can be interpolated from integer pixels, then integer pixels should also\nbe interpolated from fractional pixels. Based on the invertibility, they propose an unsupervised manner to train CNN\nfor half-pixel interpolation.\n17\nIn addition to the improvements of inter prediction methods, another approach is considered where intra and\ninter predictions are combined. SpeciÔ¨Åcally, the generation of prediction signal is based on not only reference\nframe but also context in current frame. For example, Huo et al. [45] propose to use a trained CNN to reÔ¨Åne the\ninter prediction signal. They Ô¨Ånd that using the context of the to-be-predicted block can improve the prediction\nquality. Similarly, Wang et al. [124] also reÔ¨Åne the inter prediction signal by a CNN, where the CNN inputs include\nthe inter prediction signal, the context of the current block, and the context of the inter prediction block.\nC. Cross-Channel Prediction\nCross-channel prediction is to predict between different channels. In the YUV format, the luma channel (Y) is\nusually coded before the chroma channels (U and V). Thus, it is possible to predict U from Y, and to predict\nV from Y and U. A traditional method, known as Linear Model (LM), is intended for cross-channel prediction.\nThe key idea of LM is that chroma can be predicted from luma using a linear function, but the coefÔ¨Åcients of\nthe function is not transmitted; instead, they are estimated from the context by performing a linear regression. The\nlinear assumption seems over simpliÔ¨Åed.\nBaig and Torresani [7] investigate colorization for image compression. Colorization is to predict chroma from\nluma, which is an ill-posed problem because one luma value can correspond to multiple chroma values. Accordingly,\nthey propose a tree-structured CNN, which is able to generate multiple predictions (called multiple hypotheses)\ngiven one grayscale image as input. When used for compression, the trained CNN is applied at the encoder side, and\nthe branch that produces the best prediction signal is encoded as side information for decoder. They integrate the\nmethod into JPEG, without changing the coding of luma, and experimental results show that the proposed method\noutperforms JPEG for chroma coding.\nLi et al. [67] propose a cross-channel prediction method analogous to LM. In particular, they design a hybrid\nneural network consisting of a fully connected part and a convolutional part. The former is used to process the\ncontext, including three channels, and the latter is to process the luma channel of the current block. Twofold features\nare fused to get the Ô¨Ånal prediction. This method outperforms LM by providing more than 2% BD-rate for chroma\ncoding.\nD. Probability Distribution Prediction\nAs mentioned before, accurate probability estimation is the key problem in entropy coding. Thus, several works\nhave been done to utilize deep learning for probability distribution prediction to improve the entropy coding\nefÔ¨Åciency. These works deal with different parts of the information. For example, the intra prediction mode of each\nblock is required to be sent to decoder, and Song et al. [106] design a CNN to predict the probability distribution\nof the intra prediction mode based on the context. Similarly, Pfaff et al. [94] predict the probability distribution\nof the intra prediction mode based on the context, but using a fully connected network. If an encoding/decoding\nscheme allows multiple transforms and each block can be assigned a transform mode, then Puri et al. [96] propose\nto use a CNN to predict the probability distribution of the transform mode, which is based on the quantized\ntransform coefÔ¨Åcients. In a more recent work, Ma et al. [82] consider the entropy coding of the quantized transform\n18\ncoefÔ¨Åcients, speciÔ¨Åcally the DC coefÔ¨Åcients. They design a CNN to predict the probability distribution of the DC\ncoefÔ¨Åcient of a block, from the context of the block as well as the AC coefÔ¨Åcients of the block.\nE. Transform\nTransform is an important tool in the hybrid video coding framework to convert signal (usually residues) into\ncoefÔ¨Åcients that are then quantized and coded. At the very beginning, video coding schemes adopt discrete cosine\ntransform (DCT), which is then replaced by integer cosine transform (ICT) in H.264. HEVC also adopts ICT but\nadditionally uses integer sine transform for 4√ó4 luma blocks. Adaptive multiple transforms and secondary transform\nare also studied. Nonetheless, all these transforms are still very simple.\nInspired by auto-encoder, Liu et al. [73] propose a CNN-based method to achieve a DCT-like transform for image\ncoding. The proposed transform consists of a CNN and a fully connected layer, where the CNN is to preprocess\nthe input block and the fully connected layer is to fulÔ¨Åll the transform. In their implementation, the fully connected\nlayer is initialized by the transform matrix of DCT, but then is trained together with the CNN. They use a joint\nrate-distortion cost to train the network, where rate is estimated by the l1-norm of the quantized coefÔ¨Åcients. They\nalso investigate asymmetric auto-encoders, i.e. the encoding part and decoding part are not symmetric, different\nfrom the traditional auto-encoders. Their experimental results show that the trained transform is better than the\nÔ¨Åxed DCT, and the asymmetric auto-encoders can be useful to achieve a tradeoff between compression efÔ¨Åciency\nand encoding/decoding time.\nF. Post- or In-Loop Filtering\nMost of the widely used image and video coding schemes are lossy coding ones, i.e. the reconstructed image/video\nis not exactly the original image/video, for the sake of compression. The loss is usually due to the quantization\nprocess shown in Fig. 3. When the quantization step is large, the loss is large too, which may lead to visible\nartifacts in the reconstructed image/video, such as blocking, blurring, ringing, color shift, and Ô¨Çickering. Filtering\nis the tool to reduce these artifacts, to improve the quality of the reconstructed image/video, and thus to improve\nthe compression efÔ¨Åciency indirectly. For image, the Ô¨Åltering is also known as post-processing because it does not\nchange the encoding process. For video, the Ô¨Åltering is divided into in-loop and out-of-loop, depending on whether\nthe Ô¨Åltered frame is used as reference for the following frames. In HEVC, two in-loop Ô¨Ålters are presented, namely\ndeblocking Ô¨Ålter (DF) [91] and sample adaptive offset (SAO) [31].\nPost- or in-loop Ô¨Åltering occupies the majority of the related works about deep learning-based image/video coding:\n‚Ä¢ Earlier works have focused on post-Ô¨Åltering for image coding, especially JPEG. For example, Dong et al. [26]\npropose a 4-layer CNN for compression artifacts reduction, namely ARCNN. ARCNN achieves more than 1dB\nimprovement in PSNR than JPEG on the 5 classical test images when the quality factor (QF) is between 10\nand 40. Cavigelli et al. [16] use a deeper CNN (12-layer) with hierarchical skip connections and test for higher\nQF from 40 to 76. Wang et al. [125] leverage the prior knowledge of JPEG compression, i.e. quantization of\nthe DCT coefÔ¨Åcients of 8√ó8 blocks, and propose a dual-domain (pixel domain and transform domain) based\nmethod. They achieve both higher quality and less computing time than ARCNN. Dual-domain processing is\n19\nalso studied in [38]. Guo and Chao [39] propose a one-to-many network, which is trained by a combination of\nperceptual loss, naturalness loss, and JPEG loss. Another work about loss function is presented in [32], which\nsuggests the usage of discriminator loss like in GAN. Ororbia et al. [92] propose an iterative post-Ô¨Åltering\nmethod by using a trained RNN. Recently, several works treat JPEG post-Ô¨Åltering as an image restoration task,\nlike denoising or super-resolution, and propose different networks for a series of image restoration tasks [78],\n[140], [142], [144].\n‚Ä¢ Later on, researches are more and more conducted for out-of-loop Ô¨Åltering in video coding, especially HEVC.\nDai et al. [24] propose a 4-layer CNN for post-Ô¨Åltering of intra frames, where the CNN has variable Ô¨Ålter size\nand residue connection, and named VRCNN. Wang et al. [122] use a 10-layer CNN for out-of-loop Ô¨Åltering,\nwhere they train a CNN to Ô¨Ålter one image and used the trained CNN on the video frames individually. Yang\net al. [138] propose to train different CNN models for I frames and P frames respectively, and verify the\nbeneÔ¨Åt. Jin et al. [50] suggest the use of a discriminator loss in addition to the MSE loss. Li et al. [62]\npropose to transmit some side information to decoder to select one model for each frame from a previously\ntrained set of models. In addition, Yang et al. [139] propose to utilize the inter-picture correlation during the\npost-Ô¨Åltering process by inputting multiple neighboring frames into the CNN to enhance one frame. Wang et\nal. [123] also consider the inter-picture correlation, but using a multi-scale convolutional LSTM. While the\naforementioned works take only the decoded frames as input to the CNN, He et al. [41] propose to input\nthe block partition information together with decoded frame into the CNN, Kang et al. [53] also input the\nblock partition information into the CNN and design a multi-scale network, Ma et al. [83] input the intra\nprediction signal and the decoded residual signal into the CNN, and Song et al. [107] input the QP plus\nthe decoded frame into the CNN (they also quantize the network parameters to ensure consistency between\ndifferent computing platforms). A different work is presented in [114], which does not enhance the decoded\nframes directly; instead, they propose to calculate the compression residues (i.e. the original video minus the\ndecoded video, to be distinguished from prediction residues) at the encoder side, and train an auto-encoder to\nencode the compression residues and send to the decoder side. Their method is reported to perform well on\ndomain-speciÔ¨Åc video sequences, e.g. in video game streaming services.\n‚Ä¢ It is more challenging to integrate CNN-based Ô¨Ålter into the coding loop because the Ô¨Åltered frame will serve\nas reference and will affect the other coding tools. Park and Kim [93] train a 3-layer CNN as an in-loop Ô¨Ålter\nfor HEVC. They train two models for two QP ranges: 20‚Äì29 and 30‚Äì39, respectively, and use one model for\neach frame according to its QP. The CNN is applied after DF, and SAO is turned off. They also design two\ncases to apply the CNN-based Ô¨Ålter: in the one case, the Ô¨Ålter is applied on speciÔ¨Åed frames based on picture\norder count (POC); in the other, the Ô¨Ålter is tested for each frame and if it improves quality then it is applied,\none binary Ô¨Çag for each frame is signaled to decoder in this case. Meng et al. [85] use an LSTM as an in-loop\nÔ¨Ålter, which is applied after DF and before SAO in HEVC. The network has decoded frame together with\nblock partition information as its input, and is trained with a combination of MS-SSIM loss and MAE loss.\nZhang et al. [145] propose a residual highway CNN (RHCNN) for in-loop Ô¨Åltering in HEVC. RHCNN-based\nÔ¨Ålter is applied after SAO. They train different RHCNN models for I, P, and B frames, respectively. They also\n20\ndivide QPs into several ranges and train a separate model for each range. Dai et al. [25] propose a deep CNN\ncalled VRCNN-ext for in-loop Ô¨Åltering in HEVC. They design different strategies for I frames and P/B frames:\nCNN-based Ô¨Ålter replaces DF and SAO for I frames, but is applied after DF and before SAO for P/B frames\nwith CTU- and CU-level control. At CTU-level, one binary Ô¨Çag for each CTU is signaled to control the on/off\nof CNN-based Ô¨Ålter; if the Ô¨Çag is off, then at CU-level, a binary classiÔ¨Åer is used to decide whether to turn\non CNN-based Ô¨Ålter for each CU. Jia et al. [46] also consider a deep CNN for in-loop Ô¨Åltering in HEVC. The\nÔ¨Ålter is applied after SAO and controlled by frame- and CTU-level Ô¨Çags. If frame-level Ô¨Çag is ‚Äúoff,‚Äù then the\ncorresponding CTU-level Ô¨Çags are omitted. In addition, they train multiple CNN models and train a content\nanalysis network that decides one model for each CTU, which saves the bits of CNN model selection.\nG. Down- and Up-Sampling\nA trend of the video technology is to increase the resolution at different dimensions, such as spatial resolution (i.e.\nnumber of pixels), temporal resolution (i.e. frame rate), and pixel value resolution (i.e. bit-depth). The increasing\nresolution results in multiplied data volume, which raises a great challenge to video transmission systems. When\nthe bandwidth for transmission is limited (e.g. using 2G or 3G mobile network), a common practice is to decrease\nvideo resolution before encoding and to increase video resolution back after decoding. This is known as the down-\nand up-sampling-based coding strategy. The down- and up-sampling can be performed in the spatial domain, the\ntemporal domain, the pixel value domain, or a combination of these domains. Traditionally, the down- and up-\nsampling Ô¨Ålters are often handcrafted. Recently, it is proposed to train deep networks as down- and up-sampling\nÔ¨Ålters for efÔ¨Åcient video coding. There are two categories of related researches.\nThe Ô¨Årst category is focused on training deep networks as up-sampling Ô¨Ålters only, while still using handcrafted\ndown-sampling Ô¨Ålters. This is inspired by the success of super-resolution, e.g. [27]. For example in [1], a joint\nspatial and pixel value down-sampling is proposed, where the spatial down-sampling is achieved by a handcrafted\nlow-pass Ô¨Ålter and the pixel value down-sampling is achieved by bitwise right shift. At the encoder side, a support\nvector machine is used to decide whether to perform down-sampling for each frame. At the decoder side, a CNN\nis trained to up-sample the decoded video to its original resolution. In [69], Li et al. only consider spatial down-\nsampling which is also performed by a handcrafted Ô¨Ålter, and train a CNN for up-sampling. But different from [1],\nthey propose a block adaptive resolution coding (BARC) framework. SpeciÔ¨Åcally, for each block inside a frame,\nthey consider two coding modes: down-sampling then coding and direct coding. The encoder can choose a mode\nfor each block and signal the chosen mode to the decoder. In addition, in the down-sampling coding mode, they\nfurther design two sub-modes: using a handcrafted simple Ô¨Ålter for up-sampling, and using the trained CNN for\nup-sampling. The sub-mode is also signaled to the decoder. Li et al. [69] investigate BARC only for I frames. Later,\nLin et al. [72] extend the BARC framework for P and B frames and build a complete BARC-based video coding\nscheme. While the aforementioned works perform down-sampling in the pixel domain, Liu et al. [77] propose\ndown-sampling in the residue domain, i.e. they down-sample the inter prediction residues, and they up-sample the\nresidues by a trained CNN with considering the prediction signal. They also follow the BARC framework.\n21\nThe second category trains not only up-sampling but also down-sampling Ô¨Ålters to allow for more Ô¨Çexibility.\nFor example in [47], a compression framework with two CNNs is studied. The Ô¨Årst CNN down-samples an image,\nthe down-sampled image is then compressed by an existing image encoder (such as JPEG and BPG), and then\ndecoded, the second CNN up-samples the decoded image. One drawback of this framework is that it cannot be\ntrained end-to-end because the image encoder/decoder is not differentiable. To address this problem, Jiang et al.\n[47] decide to optimize the two CNNs alternatively. Differently, Zhao et al. [147] use a virtual codec that is\nactually a CNN to approximate the functionality of‚Äìand thus replace‚Äìthe encoder/decoder; they also insert a CNN\nto perform post-processing before the up-sampling CNN; their scheme is fully convolutional and can be trained end-\nto-end. Moreover, Li et al. [68] simply remove the encoder/decoder and keep only the two CNNs during training;\nconsidering that the down-sampled image will be compressed, they propose a novel regularization loss for training,\nwhich requires the down-sampled image to be not quite different from the ideal low-pass and decimated (which is\napproximated by a handcrafted Ô¨Ålter) image. The regularization loss is veriÔ¨Åed to be useful when training down-\nand up-sampling CNNs jointly for image coding.\nH. Encoding Optimizations\nThe aforementioned deep tools are intended for increasing the compression efÔ¨Åciency, especially for reducing\nbitrate while keeping the same PSNR. There are some other deep tools that target different aspects. In this subsection,\nwe review several deep tools for three different objectives: fast encoding, rate control, and region-of-interest (ROI)\ncoding. Since these tools are used only at the encoder side, we call them encoding optimization tools in summary.\n1) Fast Encoding: Regarding the state-of-the-art video coding standards, H.264 and HEVC, the decoder is\ncomputationally simple, but the encoder is much more complex. This is because more and more coding modes are\nintroduced into the video coding standards, and each block can be assigned a different mode. The mode of each\nblock is signaled to the decoder, so the decoder only needs to compute the given mode. But to Ô¨Ånd the mode for\neach block, the encoder usually needs to compare the multiple optional modes and select the optimal one, where\noptimality is claimed in the rate-distortion sense. Therefore, if the encoder performs an exhaustive search, then\nthe compression efÔ¨Åciency is the highest, but the computational complexity may be also very high. Any practical\nencoder will adopt heuristic algorithms to search for a better mode, where machine learning especially deep learning\ncan help.\nLiu et al. [79] present a hardware design for HEVC intra encoder, where they adopt a trained CNN to help decide\nCU partition mode. SpeciÔ¨Åcally in HEVC intra coding, a CTU is split into CUs recursively to form a quadtree\nstructure. Their trained CNN will decide whether to split a 32√ó32/16√ó16/8√ó8 CU or not based on the content\ninside the CU and the speciÔ¨Åed QP. Actually, this is a binary decision problem. Xu et al. [134] additionally consider\nHEVC inter encoder, and propose an early-terminated hierarchical CNN and an early-terminated hierarchical LSTM\nto help decide CU partition mode, for I frames and P/B frames, respectively. Jin et al. [49] also consider the CU\npartition mode decision but for the incoming VVC rather than HEVC, because in VVC a quadtree-bintree (QTBT)\nstructure is designed for CU partition, which is more complex than that in HEVC. They train a CNN to perform 5-\nway classiÔ¨Åcation for a 32√ó32 CU, where different classes indicate different tree depths. Xu et al. [133] investigate\n22\nthe CU partition mode decision for H.264 to HEVC transcoding. They design a hierarchical LSTM network to\npredict the CU partition mode from the features extracted from H.264 coded bits.\nSong et al. [105] study a CNN-based method for fast intra prediction mode decision in HEVC intra encoder.\nThey train a CNN to derive a list of most probable modes for each 8√ó8/4√ó4 block based on the content and the\nspeciÔ¨Åed QP, and then choose a mode from the list by the normal rate-distortion optimized process.\n2) Rate Control: Given a limited transmission bandwidth, video encoder tries to produce bits that do not overÔ¨Çow\nthe bandwidth. This is known as the rate control requirement.\nOne traditional rate control method is to allocate bits to different blocks according to the R-Œª model [61]. In\nthat model, each block has two parameters Œ± and Œ≤ that are to be determined. Previously, the parameters are\nestimated by an empirical formula. In [66], Li et al. propose to train a CNN to predict the parameters for each\nCTU. Experimental results show that the proposed method achieves higher compression efÔ¨Åciency as well as lower\nrate control error.\nHu et al. [43] attempt to leverage reinforcement learning methods for intra-frame rate control. They draw an\nanalogy between the rate control problem and the reinforcement learning problem: the texture complexity of the\nblocks and bit balance are regarded as the environment state, the quantization parameter is regarded as an action\nthat an agent needs to take, and the negative distortion of the blocks is regarded as an immediate reward. They\ntrain a neural network as the agent.\n3) ROI Coding: ROI refers to the regions of interest in an image. In image compression, it is often required that\nthe content in ROI shall be of high quality and the content not in ROI can be of low quality. Many image coding\nschemes, such as JPEG and JPEG 2000, support ROI coding. Then, how to identify the ROI is a research problem\nand has been addressed by using deep learning. Prakash et al. [95] propose a CNN-based method to generate\na multi-scale ROI (MS-ROI) map to guide the following JPEG coding. They use a trained image classiÔ¨Åcation\nnetwork on an image, pick the top Ô¨Åve classes predicted by the network, and identify the regions that correspond\nto these classes. Thus, their MS-ROI map indicates salient regions that are related to semantic analysis.\nIV. CASE STUDY OF DLVC\nWe now turn to the case study of our developed DLVC, a prototype video codec. Indeed, DLVC was developed as\na proposal in response to the joint call for proposals on video compression with capability beyond HEVC. Now the\nsource code of DLVC has been released for future researches3. DLVC is crafted upon the JEM software, contains a\nnumber of improvements than JEM, and especially has two deep coding tools: CNN-based in-loop Ô¨Ålter (CNN-ILF)\nand CNN-based block adaptive resolution coding (CNN-BARC), both of which are based on trained CNN models.\nThe scheme of DLVC is illustrated in Fig. 5. In this section, we focus on the two deep tools. More technical details\nabout DLVC can be found in the technical report [132].\n3https://github.com/fvc2018/dlvc, http://dlvc.bitahub.com/.\n23\nCNN-Based\nDown-Sampling\nSimple\nDown-Sampling\nCoder Control\nTransform & \nQuantization\nInter\nPrediction\nIntra\nPrediction\nDequant. &\nInv. Transform\nCNN-\nBased Up-\nSampling\nSimple \nUp-\nSampling\nSecond Stage\nUp-Sampling\nIn-Loop Filtering \n(including CNN-ILF)\nIntra/Inter\nSelection\nOutput Video\nInput Video\nSplit into CTUs\nEntropy\nCoding\nCoded Bits\n_\nFig. 5. Illustration of the developed DLVC scheme. Inside the blue block is the proposed CNN-ILF. The green blocks correspond to CNN-BARC.\nResBlock1\nResBlock2\nResBlock16\nConv1\nConv2\nSum\nComp_Img\n3√ó3\n3√ó3\n64\n1\nConv\nConv\nReLU\nSum\n3√ó3\n3√ó3\n64\n64\nRec_Img\nFig. 6. The network structure of the proposed CNN-ILF. The numbers shown above and beneath each convolutional (Conv) layer indicate the\nkernel size (e.g. 3√ó3) and the number of output channels (e.g. 64), respectively.\nA. CNN-Based In-Loop Filter\nAs mentioned in Section III-F, a great number of researches have been conducted on using trained CNN models\nfor post- or in-loop Ô¨Åltering. CNN-ILF represents our efforts at this aspect.\nThe network structure of our proposed CNN-ILF is illustrated in Fig. 6. Inspired by the SR network in [70],\n24\nwe design a deep CNN having 16 residual blocks (ResBlocks) and 2 convolutional layers, in total 34 layers. Each\nResBlock consists of 2 convolutional layers separated by a ReLU mapping, and a skip connection. The entire\nnetwork has a global skip connection from the input to the output. These skip connections are crucial to train an\nefÔ¨Åcient network and to accelerate the convergence in training.\nTo train the network, we have used a set of natural images and compressed each image by the DLVC intra coding\n(turning off all in-loop Ô¨Ålters) at different QPs. For each QP we train a separate model. We only use the luma\ncomponent for training but the trained models are used for both luma and chroma channels during compression.\nWe divide images into 70√ó70 sub-images and shufÔ¨Çe the sub-images to prepare training data. The loss function is\nMSE, i.e. the error between the network-output image and the original uncompressed image. We use the stochastic\ngradient descent algorithm to train the network until convergence.\nWe apply the trained models in DLVC. The CNN-ILF is applied after deblocking Ô¨Ålter and before sample adaptive\noffset. There are different models corresponding to different QPs, and one model is selected for each frame according\nto the frame‚Äôs QP. For each CTU there are two binary Ô¨Çags that control the on/off of the CNN-ILF for luma and\nchroma respectively. These Ô¨Çags are decided at the encoder side and transmitted to the decoder.\nB. CNN-Based Block Adaptive Resolution Coding\nCNN-BARC is a down- and up-sampling-based coding tool that uses trained CNN models as the down- and\nup-sampling Ô¨Ålters. In DLVC, CNN-BARC is applied only for intra frame coding. The down-sampling coding or\ndirect coding mode is decided for each CTU, and the down-sampling coding mode has two sub-modes: using CNNs\nfor down- and up-sampling, and using simple interpolation Ô¨Ålters for down- and up-sampling. All the modes and\nsub-modes are decided by the encoder and signaled to the decoder.\nThe networks for down- and up-sampling are illustrated in Fig. 7. SpeciÔ¨Åcally, the down-sampling CNN (CNN-\nDS) has 10 convolutional layers where the Ô¨Årst layer has a stride of 2 to achieve 2√ó down-sizing. CNN-DS also\nembraces residue learning but here the original image is bicubic down-sampled to serve as the skip connection. The\nup-sampling CNN (CNN-US) is similar to the SR network in [70], and has 16 ResBlocks, 3 convolutional layers,\n1 deconvolutional layer, and a global skip connection.\nThe CNN-DS and CNN-US are trained in four steps. First, we remove the convolutional layers in the CNN-DS,\nmaking it a simple bicubic down-sampling, and train the CNN-US to minimize the end-to-end MSE (i.e. the error\nbetween original image and down-sampled-up-sampled image). Second, we add back the layers of CNN-DS, Ô¨Åx\nthe parameters of the CNN-US, and train the CNN-DS to minimize the end-to-end MSE. Third, we Ô¨Åne-tune the\nparameters of CNN-DS and CNN-US simultaneously, using a combination of two losses: the one is end-to-end\nMSE, and the other is the down-sampled MSE (i.e. the error between bicubic down-sampled image and network\ndown-sampled image), where the latter serves as a regularization term. Fourth, we Ô¨Åx the parameters of the CNN-\nDS, and compress the down-sampled images by the DLVC intra coding (turning off all in-loop Ô¨Ålters) at different\nQPs. For each QP we train a separate CNN-US model.\nThere are two mode selection steps regarding CNN-BARC in the DLVC encoder. The Ô¨Årst is to decide which\ndown- and up-sampling (sub-)mode, and the second is to decide whether to perform down-sampling. We compare\n25\nConv1\nOrig_Img\n4√ó4\n64\n(Stride=2)\nConv2\n3√ó3\n64\nReLU\nConv9\n3√ó3\n64\nReLU\nConv10\n3√ó3\n1\nSum\nLR_Img\nBicubic Down-\nSampling \n(a)\nResBlock1\nResBlock16\nConv1\nConv2\nSum\nLR_CTU\n3√ó3\n3√ó3\n64\n64\nHR_CTU\nDeconv\n6√ó6\n64\nConv3\n3√ó3\n1\n(b)\nFig. 7. The network structure of the proposed CNN-BARC, including (a) CNN-DS for down-sampling and (b) CNN-US for up-sampling. Note\nthat the Ô¨Årst Conv layer in (a) has a stride of 2 to achieve 2√ó down-sampling.\nthe rate-distortion costs of different modes to make decision. The rate is the number of coded bits, and the distortion\nis the MSE between original and reconstructed CTUs. For fair comparison, we always calculate the distortion at\nthe original resolution. Last but not the least, after an intra frame is compressed, we perform up-sampling again\nfor the down-sampled-coded CTUs. More details about CNN-BARC can be found in [68], [69].\nC. Compression Performance\nWe test the compression performance of DLVC on the 10 video sequences that are recommended by JVET. These\nsequences are divided into Class A and Class B according to spatial resolution: 5 sequences have UHD (3840√ó2160)\nresolution and the other 5 have HD (1920√ó1080) resolution. Different coding conÔ¨Ågurations including all-intra (AI),\nlow-delay (LD), and random-access (RA) are tested. We compare DLVC with the HEVC reference software‚ÄìHM\nversion 16.164 and its variants as well as JEM version 7.05, and use BD-rate [13] to measure the relative compression\nefÔ¨Åciency.\nTable II presents the BD-rate results of DLVC compared to the HEVC anchor. Obviously, DLVC improves the\ncompression efÔ¨Åciency very signiÔ¨Åcantly. Considering the Y channel, DLVC achieves on average 39.6% and 33.0%\nBD-rate reduction than HEVC, under RA and LD conÔ¨Ågurations, respectively. The results indicate that when using\nDLVC to replace HEVC, the bits can be reduced by more than 30% without lowering the reconstruction quality.\n4HM version 16.16, https://hevc.hhi.fraunhofer.de/svn/svn HEVCSoftware/tags/HM-16.16/.\n5JEM version 7.0, https://jvet.hhi.fraunhofer.de/svn/svn HMJEMSoftware/tags/HM-16.6-JEM-7.0/.\n26\nTABLE II\nBD-RATE RESULTS OF DLVC COMPARED TO HM16.16\nRandom-Access\nLow-Delay\nY\nU\nV\nY\nU\nV\nFoodMarket\n‚àí38.42%\n‚àí45.57%\n‚àí50.10%\n-\n-\n-\nCatRobot\n‚àí46.87%\n‚àí59.26%\n‚àí53.80%\n-\n-\n-\nDaylightRoad\n‚àí47.34%\n‚àí60.65%\n‚àí46.19%\n-\n-\n-\nParkRunning\n‚àí38.43%\n‚àí29.65%\n‚àí31.58%\n-\n-\n-\nCampÔ¨ÅreParty\n‚àí41.20%\n‚àí38.07%\n‚àí54.53%\n-\n-\n-\nAvg. Class-A\n‚àí42.45%\n‚àí46.64%\n‚àí47.24%\n-\n-\n-\nBQTerrace\n‚àí37.53%\n‚àí50.06%\n‚àí56.97%\n‚àí35.56%\n‚àí52.79%\n‚àí59.11%\nRitualDance\n‚àí33.30%\n‚àí42.15%\n‚àí45.71%\n‚àí28.23%\n‚àí41.19%\n‚àí43.19%\nMarketPlace\n‚àí35.34%\n‚àí51.78%\n‚àí49.99%\n‚àí26.85%\n‚àí46.65%\n‚àí47.83%\nBasketballDrive\n‚àí37.09%\n‚àí52.86%\n‚àí51.20%\n‚àí32.98%\n‚àí50.91%\n‚àí52.69%\nCactus\n‚àí40.80%\n‚àí50.92%\n‚àí46.82%\n‚àí41.37%\n‚àí56.66%\n‚àí54.68%\nAvg. Class-B\n‚àí36.81%\n‚àí49.55%\n‚àí50.14%\n‚àí33.00%\n‚àí49.64%\n‚àí51.50%\nAvg. All\n‚àí39.63%\n‚àí48.10%\n‚àí48.69%\n‚àí33.00%\n‚àí49.64%\n‚àí51.50%\nTable III presents the BD-rate results of DLVC (document number J0032) compared to the JEM anchor. For\ncomparison, we also include the BD-rate results of the other proposals in response to the joint call for proposals.\nConsidering the Y channel, DLVC achieves on average 10.1% and 11.8% BD-rate reduction than JEM, under RA\nand LD conÔ¨Ågurations, respectively. DLVC is among the best proposals from the BD-rate perspective.\nTable IV veriÔ¨Åes the effectiveness of the proposed CNN-ILF. SpeciÔ¨Åcally, we use a variant of HM that adds the\nQTBT structure, which outperforms the vanilla HM, as the anchor. We integrate the CNN-ILF into the anchor and\nturn on/off the CNN-ILF for comparison. As shown, CNN-ILF achieves signiÔ¨Åcant BD-rate reduction: on average\n5.5%, 5.2%, 6.4% for the Y channel under RA, LD, AI conÔ¨Ågurations, respectively.\nTable V veriÔ¨Åes the effectiveness of the proposed CNN-BARC. We use another variant of HM that adds the\nquadtree-bintree-triple-tree (QTBTTT) structure, which further outperforms the HM plus QTBT, as the anchor. We\nintegrate the CNN-BARC into the anchor and turn on/off the CNN-BARC for comparison. As shown, CNN-BARC\nachieves signiÔ¨Åcant BD-rate reduction under AI conÔ¨Åguration: on average 5.4% for the Y channel. The BD-rate\nunder RA and LD conÔ¨Ågurations is less signiÔ¨Åcant, because CNN-BARC is applied on intra frames only.\nV. PERSPECTIVES AND CONCLUSIONS\nA. Open Problems\n‚Ä¢ Deep Schemes or Deep Tools. Shall we be ambitious to expect deep scheme to be the future of video coding,\nor shall we be satisÔ¨Åed with deep tools within traditional non-deep schemes? In other words, can the non-deep\nschemes be completely replaced by deep schemes? As for now the answer to this question is probably ‚Äúno‚Äù\nbecause deep schemes in general do not outperform non-deep schemes for video coding. But as research goes\non, the answer may become ‚Äúyes‚Äù via two ways: Ô¨Årst, deep schemes may be improved so much that they\n27\nTABLE III\nBD-RATE RESULTS OF ALL THE PROPOSALS IN RESPONSE TO THE JOINT CALL FOR PROPOSALS COMPARED TO JEM7.0\nProposal\nOrganizations\nRandom-Access\nLow-Delay\nY\nU\nV\nY\nU\nV\nJ0011\nDJI, Peking Univ.\n‚àí1.57%\n‚àí0.71%\n‚àí1.72%\n‚àí3.30%\n‚àí0.67%\n‚àí4.26%\nJ0012\nEricsson, Nokia\n‚àí0.90%\n‚àí1.14%\n‚àí1.15%\n‚àí0.17%\n‚àí0.48%\n‚àí1.93%\nJ0013\nETRI, Sejong Univ.\n0.64%\n‚àí0.39%\n‚àí0.89%\n0.85%\n‚àí1.27%\n‚àí2.50%\nJ0014\nFraunhofer HHI\n‚àí7.55%\n‚àí6.94%\n‚àí5.96%\n‚àí7.22%\n‚àí7.62%\n‚àí5.71%\nJ0015\nInterDigital, Dolby\n‚àí3.98%\n‚àí3.28%\n‚àí3.16%\n‚àí3.64%\n‚àí2.55%\n‚àí4.48%\nJ0016\nKDDI\n‚àí0.57%\n‚àí0.52%\n‚àí1.30%\n‚àí0.17%\n0.38%\n‚àí0.40%\nJ0017\nLG Electronics\n‚àí2.52%\n‚àí5.29%\n‚àí6.19%\n‚àí2.09%\n‚àí2.47%\n‚àí3.70%\nJ0018\nMediaTek\n‚àí16.60%\n‚àí6.75%\n‚àí10.43%\n‚àí9.41%\n‚àí1.92%\n‚àí3.35%\n‚àí14.40%\n‚àí5.13%\n‚àí8.82%\n‚àí7.20%\n0.23%\n‚àí0.96%\nJ0020\nPanasonic\n‚àí2.28%\n‚àí3.44%\n‚àí3.88%\n‚àí2.02%\n‚àí1.02%\n‚àí2.36%\n‚àí0.06%\n0.91%\n0.52%\n‚àí0.09%\n2.87%\n1.21%\nJ0021\nQualcomm, Technicolor\n‚àí15.53%\n‚àí3.66%\n‚àí5.97%\n‚àí12.65%\n‚àí17.40%\n‚àí19.95%\n‚àí10.26%\n0.05%\n‚àí1.65%\n‚àí9.87%\n‚àí11.73%\n‚àí14.88%\nJ0022\n‚àí13.60%\n‚àí3.80%\n‚àí5.63%\n‚àí12.69%\n‚àí16.65%\n‚àí18.75%\nJ0023\nRWTH Aachen Univ.\n‚àí0.79%\n‚àí1.52%\n‚àí1.52%\n‚àí0.84%\n‚àí0.58%\n‚àí0.80%\nJ0024\nSamsung, Huawei,\n‚àí6.01%\n10.34%\n8.53%\n‚àí0.38%\n14.73%\n15.84%\nGoPro, HiSilicon\n‚àí4.24%\n10.71%\n9.23%\n‚àí\n‚àí\n‚àí\nJ0026\nSharp, Foxconn\n‚àí6.15%\n‚àí5.68%\n‚àí5.68%\n‚àí5.57%\n9.00%\n8.69%\nJ0027\nNHK, Sharp\n‚àí2.14%\n‚àí5.55%\n‚àí5.61%\n‚àí2.23%\n‚àí1.97%\n‚àí3.83%\nJ0028\nSony\n‚àí2.41%\n‚àí4.85%\n‚àí5.14%\n‚àí2.25%\n‚àí6.74%\n‚àí7.34%\nJ0029\nTencent\n‚àí4.70%\n‚àí8.34%\n‚àí8.91%\n‚àí4.47%\n‚àí9.47%\n‚àí10.82%\nJ0031\nUniv. Bristol\n‚àí4.54%\n20.19%\n18.68%\n‚àí0.52%\n5.74%\n5.81%\nJ0032\nUSTC and others\n‚àí10.11%\n‚àí9.59%\n‚àí9.97%\n‚àí11.83%\n‚àí13.22%\n‚àí16.49%\nMore details can be obtained at\nhttp://phenix.it-sudparis.eu/jvet/doc end user/current meeting.php?id meeting=174&search id group=1&search sub group=1.\nTABLE IV\nBD-RATE RESULTS OF CNN-ILF ON TOP OF HM16.16 + QTBT\nRandom-Access\nLow-Delay\nAll-Intra\nY\nU\nV\nY\nU\nV\nY\nU\nV\nAvg. Class-A\n‚àí5.1%\n‚àí3.8%\n‚àí4.5%\n-\n-\n-\n‚àí5.7%\n‚àí5.1%\n‚àí5.9%\nAvg. Class-B\n‚àí5.9%\n‚àí5.8%\n‚àí5.8%\n‚àí5.2%\n‚àí7.2%\n‚àí8.4%\n‚àí7.1%\n‚àí6.5%\n‚àí7.8%\nAvg. All\n‚àí5.5%\n‚àí4.8%\n‚àí5.1%\n‚àí5.2%\n‚àí7.2%\n‚àí8.4%\n‚àí6.4%\n‚àí5.8%\n‚àí6.8%\nclearly beat non-deep schemes; second, the coding tools in a traditional coding scheme (e.g. HEVC) may be\nall replaced by corresponding deep tools, leading to a ‚Äúdeepened‚Äù coding scheme that is better than before.\nThe second way may be more practical according to our subjective feeling.\n‚Ä¢ Compression EfÔ¨Åciency versus Computational Complexity. Compared the existing deep tools with their coun-\n28\nTABLE V\nBD-RATE RESULTS OF CNN-BARC ON TOP OF HM16.16 + QTBTTT\nRandom-Access\nLow-Delay\nAll-Intra\nY\nU\nV\nY\nU\nV\nY\nU\nV\nAvg. Class-A\n‚àí1.8%\n0.6%\n0.4%\n-\n-\n-\n‚àí7.2%\n2.0%\n2.0%\nAvg. Class-B\n‚àí1.0%\n0.1%\n‚àí0.1%\n‚àí0.3%\n‚àí0.3%\n‚àí0.0%\n‚àí3.6%\n0.5%\n0.4%\nAvg. All\n‚àí1.4%\n0.3%\n0.2%\n‚àí0.3%\n‚àí0.3%\n‚àí0.0%\n‚àí5.4%\n1.3%\n1.2%\nterparts in the traditional non-deep schemes, one may easily notice that the computational complexity of the\nformer is much higher than the latter. High complexity is indeed a general issue of deep learning, and a\ncritical issue that hinders the adoption of deep networks in scenarios of limited computing resource, e.g.\nmobile phones. This general issue is now addressed at two aspects: Ô¨Årst, to develop novel, efÔ¨Åcient, compact\ndeep networks that maintain the high performance (i.e. compression efÔ¨Åciency for video coding) but require\nmuch less computations; second, to advocate the adoption of hardware that is speciÔ¨Åcally designed for deep\nnetworks.\n‚Ä¢ Optimization for Perceptual Naturalness or Semantic Quality. Coding schemes designed for natural video are\nusually serving for human viewing, e.g. television, movie, micro-video. It is natural for these schemes that\nthe quality of reconstructed video shall be evaluated based on human perception. Nonetheless, for traditional\nnon-deep coding schemes, the most widely adopted quality metric is still PSNR, which corresponds to human\nperception poorly. For deep schemes or deep tools, a few works have been done to optimize them for perceptual\nnaturalness, e.g. using discriminator loss. Moreover, there are coding schemes that serve for automatic semantic\nanalysis instead of human viewing, such as surveillance video coding. For these schemes, the quality metric\nshall be semantic quality [74], which remains largely unexplored. As a special note, we Ô¨Ånd that there is\na tradeoff between signal Ô¨Ådelity, perceptual naturalness, and semantic quality [75], which implies that the\noptimization target shall be aligned with the actual requirement.\n‚Ä¢ Speciality and Universality. To one extreme, can one coding scheme be simply the best for any kind of video?\nThe answer is ‚Äúno‚Äù due to the no-free-lunch theorem, which is claimed in the machine learning literature [130]\nand also applies for compression. To another extreme, can we have a special coding scheme for each video?\nNot to mention the practical difÔ¨Åculty, such a coding ‚Äústrategy‚Äù is useless because it is no more than assigning\nan identiÔ¨Åer to each video. In between the two extremes are practical and useful coding schemes. That says,\ncoding schemes shall have both speciality and universality to some extent. For deep schemes and deep tools,\nit implies that the training data shall be carefully selected to reÔ¨Çect the interesting data domain. Researches at\nthis aspect are expected.\n‚Ä¢ Federated Design of Multiple Deep Tools. Currently, most of deep tools have been designed individually,\nbut once they are applied jointly, they may not collaborate well, or may even conÔ¨Çict with each other. The\nunderlying reason is that multiple coding tools are indeed dependent. For example, different prediction tools\ngenerate different predictions and lead to variety of residual signal, so transform tools dealing with residual\n29\nsignal perform differently. Ideally, multiple deep tools shall be designed in a federated manner. However, this\ncan be difÔ¨Åcult because the dependency among tools is complicated.\nB. Future Work\nIn the predictable future, the requirement about video coding technology is still increasing. For entertainment,\nvirtual reality and augmented reality applications are calling for techniques to tackle with new data, such as depth\nmap, point cloud, 3D surface, and so on. For surveillance, the need of intelligent analysis pushes the upgrade\nof video resolution. For scientiÔ¨Åc observation, more and more observing instruments are directly connected to a\nvideo recorder and generate massive video data. All these requirements drive the development of video coding to\nachieve higher compression efÔ¨Åciency, lower computational complexity, and smarter integration into video analytical\nsystems. We believe deep learning-based video coding techniques are promising for these challenging objectives.\nEspecially, we expect a holistic framework based on deep networks and integrating image/video acquisition, coding,\nprocessing, analysis, and understanding, which indeed mimics human vision system.\nACKNOWLEDGMENT\nThis work is supported by the Natural Science Foundation of China under Grant 61772483. Authors would like to\nthank the following colleagues and collaborators: Jizheng Xu, Bin Li, Zhibo Chen, Li Li, Fangdong Chen, Yuanying\nDai, Changsheng Gao, Lei Guo, Shuai Huo, Ye Li, Kang Liu, Changyue Ma, Haichuan Ma, Rui Song, Yefei Wang,\nNing Yan, Kun Yang, Qingyu Zhang, Zhenxin Zhang, and Haitao Yang.\nREFERENCES\n[1] M. Afonso, F. Zhang, and D. R. Bull, ‚ÄúVideo compression based on spatio-temporal resolution adaptation,‚Äù IEEE Transactions on Circuits\nand Systems for Video Technology, vol. 29, no. 1, pp. 275‚Äì280, 2019.\n[2] E. Agustsson, F. Mentzer, M. Tschannen, L. Cavigelli, R. Timofte, L. Benini, and L. V. Gool, ‚ÄúSoft-to-hard vector quantization for\nend-to-end learning compressible representations,‚Äù in NIPS, 2017, pp. 1141‚Äì1151.\n[3] E. Agustsson, M. Tschannen, F. Mentzer, R. Timofte, and L. V. Gool, ‚ÄúExtreme learned image compression with GANs,‚Äù in CVPR\nWorkshops, 2018, pp. 2587‚Äì2590.\n[4] E. Ahanonu, ‚ÄúLossless image compression using reversible integer wavelet transforms and convolutional neural networks,‚Äù Master‚Äôs thesis,\nUniversity of Arizona, 2018.\n[5] M. Akbari, J. Liang, and J. Han, ‚ÄúDSSLIC: Deep semantic segmentation-based layered image compression,‚Äù in ICASSP, 2019, pp.\n2042‚Äì2046.\n[6] M. H. Baig, V. Koltun, and L. Torresani, ‚ÄúLearning to inpaint for image compression,‚Äù in NIPS, 2017, pp. 1246‚Äì1255.\n[7] M. H. Baig and L. Torresani, ‚ÄúMultiple hypothesis colorization and its application to image compression,‚Äù Computer Vision and Image\nUnderstanding, vol. 164, pp. 111‚Äì123, 2017.\n[8] J. Ball¬¥e, ‚ÄúEfÔ¨Åcient nonlinear transforms for lossy image compression,‚Äù in PCS, 2018, pp. 248‚Äì252.\n[9] J. Ball¬¥e, V. Laparra, and E. P. Simoncelli, ‚ÄúEnd-to-end optimization of nonlinear transform codes for perceptual quality,‚Äù in PCS.\nIEEE,\n2016, pp. 1‚Äì5.\n[10] ‚Äî‚Äî, ‚ÄúEnd-to-end optimized image compression,‚Äù arXiv preprint arXiv:1611.01704, 2016.\n[11] J. Ball¬¥e, D. Minnen, S. Singh, S. J. Hwang, and N. Johnston, ‚ÄúVariational image compression with a scale hyperprior,‚Äù arXiv preprint\narXiv:1802.01436, 2018.\n[12] Y. Bengio and S. Bengio, ‚ÄúModeling high-dimensional discrete data with multi-layer neural networks,‚Äù in NIPS, 2000, pp. 400‚Äì406.\n[13] G. Bjontegaard, ‚ÄúCalcuation of average PSNR differences between RD-curves,‚Äù VCEG, Tech. Rep. VCEG-M33, 2001.\n30\n[14] Y. Blau and T. Michaeli, ‚ÄúThe perception-distortion tradeoff,‚Äù in CVPR, 2018, pp. 6228‚Äì6237.\n[15] C. Cai, L. Chen, X. Zhang, and Z. Gao, ‚ÄúEfÔ¨Åcient variable rate image compression with multi-scale decomposition network,‚Äù IEEE\nTransactions on Circuits and Systems for Video Technology, DOI: 10.1109/TCSVT.2018.2880492, 2018.\n[16] L. Cavigelli, P. Hager, and L. Benini, ‚ÄúCAS-CNN: A deep convolutional neural network for image compression artifact suppression,‚Äù in\nIJCNN.\nIEEE, 2017, pp. 752‚Äì759.\n[17] T. Chen, H. Liu, Q. Shen, T. Yue, X. Cao, and Z. Ma, ‚ÄúDeepCoder: A deep neural network based video compression,‚Äù in VCIP.\nIEEE,\n2017, pp. 1‚Äì4.\n[18] X. Chen, N. Mishra, M. Rohaninejad, and P. Abbeel, ‚ÄúPixelSNAIL: An improved autoregressive generative model,‚Äù in ICML, 2018, pp.\n863‚Äì871.\n[19] Z. Chen and T. He, ‚ÄúLearning based facial image compression with semantic Ô¨Ådelity metric,‚Äù Neurocomputing, vol. 338, pp. 16‚Äì25, 2019.\n[20] Z. Chen, T. He, X. Jin, and F. Wu, ‚ÄúLearning for video compression,‚Äù IEEE Transactions on Circuits and Systems for Video Technology,\nDOI: 10.1109/TCSVT.2019.2892608, 2019.\n[21] Z. Cheng, H. Sun, M. Takeuchi, and J. Katto, ‚ÄúDeep convolutional autoencoder-based lossy image compression,‚Äù in PCS.\nIEEE, 2018,\npp. 253‚Äì257.\n[22] M. Covell, N. Johnston, D. Minnen, S. J. Hwang, J. Shor, S. Singh, D. Vincent, and G. Toderici, ‚ÄúTarget-quality image compression with\nrecurrent, convolutional neural networks,‚Äù arXiv preprint arXiv:1705.06687, 2017.\n[23] W. Cui, T. Zhang, S. Zhang, F. Jiang, W. Zuo, Z. Wan, and D. Zhao, ‚ÄúConvolutional neural networks based intra prediction for HEVC,‚Äù\nin DCC.\nIEEE, 2017, p. 436.\n[24] Y. Dai, D. Liu, and F. Wu, ‚ÄúA convolutional neural network approach for post-processing in HEVC intra coding,‚Äù in MMM.\nSpringer,\n2017, pp. 28‚Äì39.\n[25] Y. Dai, D. Liu, Z.-J. Zha, and F. Wu, ‚ÄúA CNN-based in-loop Ô¨Ålter with CU classiÔ¨Åcation for HEVC,‚Äù in VCIP, 2018, pp. 1‚Äì4.\n[26] C. Dong, Y. Deng, C. C. Loy, and X. Tang, ‚ÄúCompression artifacts reduction by a deep convolutional network,‚Äù in ICCV, 2015, pp.\n576‚Äì584.\n[27] C. Dong, C. C. Loy, K. He, and X. Tang, ‚ÄúLearning a deep convolutional network for image super-resolution,‚Äù in ECCV.\nSpringer,\n2014, pp. 184‚Äì199.\n[28] R. D. Dony and S. Haykin, ‚ÄúNeural network approaches to image compression,‚Äù Proceedings of the IEEE, vol. 83, no. 2, pp. 288‚Äì303,\n1995.\n[29] T. Dumas, A. Roumy, and C. Guillemot, ‚ÄúImage compression with stochastic winner-take-all auto-encoder,‚Äù in ICASSP.\nIEEE, 2017,\npp. 1512‚Äì1516.\n[30] ‚Äî‚Äî, ‚ÄúAutoencoder based image compression: can the learning be quantization independent?‚Äù in ICASSP.\nIEEE, 2018, pp. 1188‚Äì1192.\n[31] C.-M. Fu, E. Alshina, A. Alshin, Y.-W. Huang, C.-Y. Chen, C.-Y. Tsai, C.-W. Hsu, S.-M. Lei, J.-H. Park, and W.-J. Han, ‚ÄúSample adaptive\noffset in the HEVC standard,‚Äù IEEE Transactions on Circuits and Systems for Video technology, vol. 22, no. 12, pp. 1755‚Äì1764, 2012.\n[32] L. Galteri, L. Seidenari, M. Bertini, and A. Del Bimbo, ‚ÄúDeep generative adversarial compression artifact removal,‚Äù in ICCV, 2017, pp.\n4826‚Äì4835.\n[33] R. Girshick, J. Donahue, T. Darrell, and J. Malik, ‚ÄúRich feature hierarchies for accurate object detection and semantic segmentation,‚Äù in\nCVPR, 2014, pp. 580‚Äì587.\n[34] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, ‚ÄúGenerative adversarial nets,‚Äù\nin NIPS, 2014, pp. 2672‚Äì2680.\n[35] K. Gregor, F. Besse, D. J. Rezende, I. Danihelka, and D. Wierstra, ‚ÄúTowards conceptual compression,‚Äù in NIPS, 2016, pp. 3549‚Äì3557.\n[36] K. Gregor, I. Danihelka, A. Graves, D. Rezende, and D. Wierstra, ‚ÄúDRAW: A recurrent neural network for image generation,‚Äù in ICML,\n2015, pp. 1462‚Äì1471.\n[37] K. Gregor and Y. LeCun, ‚ÄúLearning representations by maximizing compression,‚Äù arXiv preprint arXiv:1108.1169, 2011.\n[38] J. Guo and H. Chao, ‚ÄúBuilding dual-domain representations for compression artifacts reduction,‚Äù in ECCV. Springer, 2016, pp. 628‚Äì644.\n[39] ‚Äî‚Äî, ‚ÄúOne-to-many network for visually pleasing compression artifacts reduction,‚Äù in CVPR, 2017, pp. 3038‚Äì3047.\n[40] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image recognition,‚Äù in CVPR, 2016, pp. 770‚Äì778.\n[41] X. He, Q. Hu, X. Zhang, C. Zhang, W. Lin, and X. Han, ‚ÄúEnhancing HEVC compressed videos with a partition-masked convolutional\nneural network,‚Äù in ICIP.\nIEEE, 2018, pp. 216‚Äì220.\n[42] G. E. Hinton and R. R. Salakhutdinov, ‚ÄúReducing the dimensionality of data with neural networks,‚Äù Science, vol. 313, no. 5786, pp.\n504‚Äì507, 2006.\n31\n[43] J.-H. Hu, W.-H. Peng, and C.-H. Chung, ‚ÄúReinforcement learning for HEVC/H.265 intra-frame rate control,‚Äù in ISCAS.\nIEEE, 2018,\npp. 1‚Äì5.\n[44] Y. Hu, W. Yang, M. Li, and J. Liu, ‚ÄúProgressive spatial recurrent neural network for intra prediction,‚Äù arXiv preprint arXiv:1807.02232,\n2018.\n[45] S. Huo, D. Liu, F. Wu, and H. Li, ‚ÄúConvolutional neural network-based motion compensation reÔ¨Ånement for video coding,‚Äù in ISCAS,\n2018, pp. 1‚Äì4.\n[46] C. Jia, S. Wang, X. Zhang, S. Wang, J. Liu, S. Pu, and S. Ma, ‚ÄúContent-aware convolutional neural network for in-loop Ô¨Åltering in high\nefÔ¨Åciency video coding,‚Äù IEEE Transactions on Image Processing, DOI: 10.1109/TIP.2019.2896489, 2019.\n[47] F. Jiang, W. Tao, S. Liu, J. Ren, X. Guo, and D. Zhao, ‚ÄúAn end-to-end compression framework based on convolutional neural networks,‚Äù\nIEEE Transactions on Circuits and Systems for Video Technology, vol. 28, no. 10, pp. 3007‚Äì3018, 2018.\n[48] J. Jiang, ‚ÄúImage compression with neural networks‚ÄìA survey,‚Äù Signal Processing: Image Communication, vol. 14, no. 9, pp. 737‚Äì760,\n1999.\n[49] Z. Jin, P. An, L. Shen, and C. Yang, ‚ÄúCNN oriented fast QTBT partition algorithm for JVET intra coding,‚Äù in VCIP.\nIEEE, 2017, pp.\n1‚Äì4.\n[50] Z. Jin, P. An, C. Yang, and L. Shen, ‚ÄúQuality enhancement for intra frame coding via CNNs: An adversarial approach,‚Äù in ICASSP.\nIEEE, 2018, pp. 1368‚Äì1372.\n[51] N. Johnston, D. Vincent, D. Minnen, M. Covell, S. Singh, T. Chinen, S. Jin Hwang, J. Shor, and G. Toderici, ‚ÄúImproved lossy image\ncompression with priming and spatially adaptive bit rates for recurrent networks,‚Äù in CVPR, 2018, pp. 4385‚Äì4393.\n[52] N. Kalchbrenner, A. van den Oord, K. Simonyan, I. Danihelka, O. Vinyals, A. Graves, and K. Kavukcuoglu, ‚ÄúVideo pixel networks,‚Äù in\nICML, 2017, pp. 1771‚Äì1779.\n[53] J. Kang, S. Kim, and K. M. Lee, ‚ÄúMulti-modal/multi-scale convolutional neural network based in-loop Ô¨Ålter design for next generation\nvideo codec,‚Äù in ICIP, 2017, pp. 26‚Äì30.\n[54] S. Kim, J. S. Park, C. G. Bampis, J. Lee, M. K. Markey, A. G. Dimakis, and A. C. Bovik, ‚ÄúAdversarial video compression guided by\nsoft edge detection,‚Äù arXiv preprint arXiv:1811.10673, 2018.\n[55] D. P. Kingma and M. Welling, ‚ÄúAuto-encoding variational bayes,‚Äù arXiv preprint arXiv:1312.6114, 2013.\n[56] A. Kolesnikov and C. H. Lampert, ‚ÄúLatent variable PixelCNNs for natural image modeling,‚Äù arXiv preprint arXiv:1612.08185, 2016.\n[57] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‚ÄúImagenet classiÔ¨Åcation with deep convolutional neural networks,‚Äù in NIPS, 2012, pp.\n1097‚Äì1105.\n[58] H. Larochelle and I. Murray, ‚ÄúThe neural autoregressive distribution estimator,‚Äù in Proceedings of the Fourteenth International Conference\non ArtiÔ¨Åcial Intelligence and Statistics, 2011, pp. 29‚Äì37.\n[59] Y. LeCun, Y. Bengio, and G. Hinton, ‚ÄúDeep learning,‚Äù Nature, vol. 521, no. 7553, pp. 436‚Äì444, May 2015.\n[60] J. Lee, S. Cho, and S.-K. Beack, ‚ÄúContext-adaptive entropy model for end-to-end optimized image compression,‚Äù arXiv preprint\narXiv:1809.10452, 2018.\n[61] B. Li, H. Li, L. Li, and J. Zhang, ‚ÄúŒª domain rate control algorithm for high efÔ¨Åciency video coding,‚Äù IEEE Transactions on Image\nProcessing, vol. 23, no. 9, pp. 3841‚Äì3854, 2014.\n[62] C. Li, L. Song, R. Xie, and W. Zhang, ‚ÄúCNN based post-processing to improve HEVC,‚Äù in ICIP.\nIEEE, 2017, pp. 4577‚Äì4580.\n[63] J. Li, B. Li, J. Xu, R. Xiong, and W. Gao, ‚ÄúFully connected network-based intra prediction for image coding,‚Äù IEEE Transactions on\nImage Processing, vol. 27, no. 7, pp. 3236‚Äì3247, 2018.\n[64] M. Li, S. Gu, D. Zhang, and W. Zuo, ‚ÄúEnlarging context with low cost: EfÔ¨Åcient arithmetic coding with trimmed convolution,‚Äù arXiv\npreprint arXiv:1801.04662, 2018.\n[65] M. Li, W. Zuo, S. Gu, D. Zhao, and D. Zhang, ‚ÄúLearning convolutional networks for content-weighted image compression,‚Äù in CVPR,\n2018, pp. 673‚Äì681.\n[66] Y. Li, B. Li, D. Liu, and Z. Chen, ‚ÄúA convolutional neural network-based approach to rate control in HEVC intra coding,‚Äù in VCIP.\nIEEE, 2017, pp. 1‚Äì4.\n[67] Y. Li, L. Li, Z. Li, J. Yang, N. Xu, D. Liu, and H. Li, ‚ÄúA hybrid neural network for chroma intra prediction,‚Äù in ICIP, 2018, pp.\n1797‚Äì1801.\n[68] Y. Li, D. Liu, H. Li, L. Li, Z. Li, and F. Wu, ‚ÄúLearning a convolutional neural network for image compact-resolution,‚Äù IEEE Transactions\non Image Processing, vol. 28, no. 3, pp. 1092‚Äì1107, 2019.\n32\n[69] Y. Li, D. Liu, H. Li, L. Li, F. Wu, H. Zhang, and H. Yang, ‚ÄúConvolutional neural network-based block up-sampling for intra frame\ncoding,‚Äù IEEE Transactions on Circuits and Systems for Video Technology, vol. 28, no. 9, pp. 2316‚Äì2330, 2018.\n[70] B. Lim, S. Son, H. Kim, S. Nah, and K. M. Lee, ‚ÄúEnhanced deep residual networks for single image super-resolution,‚Äù in CVPR\nWorkshops, 2017, pp. 136‚Äì144.\n[71] J. Lin, D. Liu, H. Li, and F. Wu, ‚ÄúGenerative adversarial network-based frame extrapolation for video coding,‚Äù in VCIP, 2018, pp. 1‚Äì4.\n[72] J. Lin, D. Liu, H. Yang, H. Li, and F. Wu, ‚ÄúConvolutional neural network-based block up-sampling for HEVC,‚Äù IEEE Transactions on\nCircuits and Systems for Video Technology, DOI: 10.1109/TCSVT.2018.2884203, 2018.\n[73] D. Liu, H. Ma, Z. Xiong, and F. Wu, ‚ÄúCNN-based DCT-like transform for image compression,‚Äù in MMM.\nSpringer, 2018, pp. 61‚Äì72.\n[74] D. Liu, D. Wang, and H. Li, ‚ÄúRecognizable or not: Towards image semantic quality assessment for compression,‚Äù Sensing and Imaging,\nvol. 18, no. 1, pp. 1‚Äì20, 2017.\n[75] D. Liu, H. Zhang, and Z. Xiong, ‚ÄúOn the classiÔ¨Åcation-distortion-perception tradeoff,‚Äù arXiv preprint arXiv:1904.08816, 2019.\n[76] J. Liu, S. Xia, W. Yang, M. Li, and D. Liu, ‚ÄúOne-for-all: Grouped variation network based fractional interpolation in video coding,‚Äù\nIEEE Transactions on Image Processing, vol. 28, no. 5, pp. 2140‚Äì2151, 2019.\n[77] K. Liu, D. Liu, H. Li, and F. Wu, ‚ÄúConvolutional neural network-based residue super-resolution for video coding,‚Äù in VCIP, 2018, pp.\n1‚Äì4.\n[78] P. Liu, H. Zhang, K. Zhang, L. Lin, and W. Zuo, ‚ÄúMulti-level wavelet-CNN for image restoration,‚Äù in CVPR Workshops, 2018, pp.\n773‚Äì782.\n[79] Z. Liu, X. Yu, Y. Gao, S. Chen, X. Ji, and D. Wang, ‚ÄúCU partition mode decision for HEVC hardwired intra encoder using convolution\nneural network,‚Äù IEEE Transactions on Image Processing, vol. 25, no. 11, pp. 5088‚Äì5103, 2016.\n[80] G. Lu, W. Ouyang, D. Xu, X. Zhang, C. Cai, and Z. Gao, ‚ÄúDVC: An end-to-end deep video compression framework,‚Äù in CVPR, 2019.\n[81] S. Luo, Y. Yang, Y. Yin, C. Shen, Y. Zhao, and M. Song, ‚ÄúDeepSIC: Deep semantic image compression,‚Äù in International Conference\non Neural Information Processing.\nSpringer, 2018, pp. 96‚Äì106.\n[82] C. Ma, D. Liu, X. Peng, and F. Wu, ‚ÄúConvolutional neural network-based arithmetic coding of DC coefÔ¨Åcients for HEVC intra coding,‚Äù\nin ICIP, 2018, pp. 1772‚Äì1776.\n[83] L. Ma, Y. Tian, and T. Huang, ‚ÄúResidual-based video restoration for HEVC intra coding,‚Äù in BigMM.\nIEEE, 2018, pp. 1‚Äì7.\n[84] S. Ma, X. Zhang, C. Jia, Z. Zhao, S. Wang, and S. Wang, ‚ÄúImage and video compression with neural networks: A review,‚Äù IEEE\nTransactions on Circuits and Systems for Video Technology, DOI: 10.1109/TCSVT.2019.2910119, 2019.\n[85] X. Meng, C. Chen, S. Zhu, and B. Zeng, ‚ÄúA new HEVC in-loop Ô¨Ålter based on multi-channel long-short-term dependency residual\nnetworks,‚Äù in DCC.\nIEEE, 2018, pp. 187‚Äì196.\n[86] F. Mentzer, E. Agustsson, M. Tschannen, R. Timofte, and L. V. Gool, ‚ÄúConditional probability models for deep image compression,‚Äù in\nCVPR, 2018, pp. 4394‚Äì4402.\n[87] ‚Äî‚Äî, ‚ÄúPractical full resolution learned lossless image compression,‚Äù arXiv preprint arXiv:1811.12817, 2018.\n[88] D. Minnen, J. Ball¬¥e, and G. Toderici, ‚ÄúJoint autoregressive and hierarchical priors for learned image compression,‚Äù in NIPS, 2018, pp.\n10 794‚Äì10 803.\n[89] D. Minnen, G. Toderici, M. Covell, T. Chinen, N. Johnston, J. Shor, S. J. Hwang, D. Vincent, and S. Singh, ‚ÄúSpatially adaptive image\ncompression using a tiled deep network,‚Äù in ICIP.\nIEEE, 2017, pp. 2796‚Äì2800.\n[90] V. Nair and G. E. Hinton, ‚ÄúRectiÔ¨Åed linear units improve restricted boltzmann machines,‚Äù in ICML, 2010, pp. 807‚Äì814.\n[91] A. Norkin, G. Bjontegaard, A. Fuldseth, M. Narroschke, M. Ikeda, K. Andersson, M. Zhou, and G. van der Auwera, ‚ÄúHEVC deblocking\nÔ¨Ålter,‚Äù IEEE Transactions on Circuits and Systems for Video Technology, vol. 22, no. 12, pp. 1746‚Äì1754, 2012.\n[92] A. G. Ororbia, A. Mali, J. Wu, S. O‚ÄôConnell, D. Miller, and C. L. Giles, ‚ÄúLearned neural iterative decoding for lossy image compression\nsystems,‚Äù arXiv preprint arXiv:1803.05863, 2018.\n[93] W.-S. Park and M. Kim, ‚ÄúCNN-based in-loop Ô¨Åltering for coding efÔ¨Åciency improvement,‚Äù in IEEE Image, Video, and Multidimensional\nSignal Processing Workshop.\nIEEE, 2016, pp. 1‚Äì5.\n[94] J. Pfaff, P. Helle, D. Maniry, S. Kaltenstadler, W. Samek, H. Schwarz, D. Marpe, and T. Wiegand, ‚ÄúNeural network based intra prediction\nfor video coding,‚Äù in Applications of Digital Image Processing XLI, vol. 10752.\nInternational Society for Optics and Photonics, 2018,\np. 1075213.\n[95] A. Prakash, N. Moran, S. Garber, A. DiLillo, and J. Storer, ‚ÄúSemantic perceptual image compression using deep convolution networks,‚Äù\nin DCC.\nIEEE, 2017, pp. 250‚Äì259.\n33\n[96] S. Puri, S. Lasserre, and P. Le Callet, ‚ÄúCNN-based transform index prediction in multiple transforms framework to assist entropy coding,‚Äù\nin EUSIPCO.\nIEEE, 2017, pp. 798‚Äì802.\n[97] O. Rippel and L. Bourdev, ‚ÄúReal-time adaptive image compression,‚Äù in ICML, 2017, pp. 2922‚Äì2930.\n[98] O. Rippel, S. Nair, C. Lew, S. Branson, A. G. Anderson, and L. Bourdev, ‚ÄúLearned video compression,‚Äù arXiv preprint arXiv:1811.06981,\n2018.\n[99] T. Salimans, A. Karpathy, X. Chen, and D. P. Kingma, ‚ÄúPixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood\nand other modiÔ¨Åcations,‚Äù arXiv preprint arXiv:1701.05517, 2017.\n[100] S. Santurkar, D. Budden, and N. Shavit, ‚ÄúGenerative compression,‚Äù in PCS.\nIEEE, 2018, pp. 258‚Äì262.\n[101] I. Schiopu, Y. Liu, and A. Munteanu, ‚ÄúCNN-based prediction for lossless coding of photographic images,‚Äù in PCS.\nIEEE, 2018, pp.\n16‚Äì20.\n[102] C. E. Shannon, ‚ÄúA mathematical theory of communication,‚Äù Bell System Technical Journal, vol. 27, no. 3, pp. 379‚Äì423, 1948.\n[103] A. Skodras, C. Christopoulos, and T. Ebrahimi, ‚ÄúThe JPEG 2000 still image compression standard,‚Äù IEEE Signal Processing Magazine,\nvol. 18, no. 5, pp. 36‚Äì58, 2001.\n[104] J. Snell, K. Ridgeway, R. Liao, B. D. Roads, M. C. Mozer, and R. S. Zemel, ‚ÄúLearning to generate images with perceptual similarity\nmetrics,‚Äù in ICIP.\nIEEE, 2017, pp. 4277‚Äì4281.\n[105] N. Song, Z. Liu, X. Ji, and D. Wang, ‚ÄúCNN oriented fast PU mode decision for HEVC hardwired intra encoder,‚Äù in GlobalSIP.\nIEEE,\n2017, pp. 239‚Äì243.\n[106] R. Song, D. Liu, H. Li, and F. Wu, ‚ÄúNeural network-based arithmetic coding of intra prediction modes in HEVC,‚Äù in VCIP, 2017, pp.\n1‚Äì4.\n[107] X. Song, J. Yao, L. Zhou, L. Wang, X. Wu, D. Xie, and S. Pu, ‚ÄúA practical convolutional neural network as loop Ô¨Ålter for intra frame,‚Äù\nin ICIP.\nIEEE, 2018, pp. 1133‚Äì1137.\n[108] G. J. Sullivan, J.-R. Ohm, W.-J. Han, and T. Wiegand, ‚ÄúOverview of the high efÔ¨Åciency video coding (HEVC) standard,‚Äù IEEE Transactions\non Circuits and Systems for Video Technology, vol. 22, no. 12, pp. 1649‚Äì1668, 2012.\n[109] L. Theis and M. Bethge, ‚ÄúGenerative image modeling using spatial LSTMs,‚Äù in NIPS, 2015, pp. 1927‚Äì1935.\n[110] L. Theis, W. Shi, A. Cunningham, and F. Husz¬¥ar, ‚ÄúLossy image compression with compressive autoencoders,‚Äù arXiv preprint\narXiv:1703.00395, 2017.\n[111] G. Toderici, S. M. O‚ÄôMalley, S. J. Hwang, D. Vincent, D. Minnen, S. Baluja, M. Covell, and R. Sukthankar, ‚ÄúVariable rate image\ncompression with recurrent neural networks,‚Äù arXiv preprint arXiv:1511.06085, 2015.\n[112] G. Toderici, D. Vincent, N. Johnston, S. J. Hwang, D. Minnen, J. Shor, and M. Covell, ‚ÄúFull resolution image compression with recurrent\nneural networks,‚Äù in CVPR, 2017, pp. 5306‚Äì5314.\n[113] R. Torfason, F. Mentzer, E. Agustsson, M. Tschannen, R. Timofte, and L. V. Gool, ‚ÄúTowards image understanding from deep compression\nwithout decoding,‚Äù arXiv preprint arXiv:1803.06131, 2018.\n[114] Y.-H. Tsai, M.-Y. Liu, D. Sun, M.-H. Yang, and J. Kautz, ‚ÄúLearning binary residual representations for domain-speciÔ¨Åc video streaming,‚Äù\nin AAAI, 2018, pp. 7363‚Äì7370.\n[115] P. Tudor, ‚ÄúMPEG-2 video compression,‚Äù Electronics & Communication Engineering Journal, vol. 7, no. 6, pp. 257‚Äì264, 1995.\n[116] B. Uria, I. Murray, and H. Larochelle, ‚ÄúRNADE: The real-valued neural autoregressive density-estimator,‚Äù in NIPS, 2013, pp. 2175‚Äì2183.\n[117] ‚Äî‚Äî, ‚ÄúA deep and tractable density estimator,‚Äù in ICML, 2014, pp. 467‚Äì475.\n[118] A. van den Oord, N. Kalchbrenner, and K. Kavukcuoglu, ‚ÄúPixel recurrent neural networks,‚Äù in ICML, 2016, pp. 1747‚Äì1756.\n[119] A. van den Oord, N. Kalchbrenner, O. Vinyals, L. Espeholt, A. Graves, and K. Kavukcuoglu, ‚ÄúConditional image generation with\nPixelCNN decoders,‚Äù in NIPS, 2016, pp. 4790‚Äì4798.\n[120] A. van den Oord and B. Schrauwen, ‚ÄúFactoring variations in natural images with deep Gaussian mixture models,‚Äù in NIPS, 2014, pp.\n3518‚Äì3526.\n[121] G. K. Wallace, ‚ÄúThe JPEG still picture compression standard,‚Äù IEEE Transactions on Consumer Electronics, vol. 38, no. 1, pp. xviii‚Äìxxxiv,\n1992.\n[122] T. Wang, M. Chen, and H. Chao, ‚ÄúA novel deep learning-based method of improving coding efÔ¨Åciency from the decoder-end for HEVC,‚Äù\nin DCC.\nIEEE, 2017, pp. 410‚Äì419.\n[123] T. Wang, W. Xiao, M. Chen, and H. Chao, ‚ÄúThe multi-scale deep decoder for the standard HEVC bitstreams,‚Äù in DCC.\nIEEE, 2018,\npp. 197‚Äì206.\n[124] Y. Wang, X. Fan, C. Jia, D. Zhao, and W. Gao, ‚ÄúNeural network based inter prediction for HEVC,‚Äù in ICME.\nIEEE, 2018, pp. 1‚Äì6.\n34\n[125] Z. Wang, D. Liu, S. Chang, Q. Ling, Y. Yang, and T. S. Huang, ‚ÄúD3: Deep dual-domain based fast restoration of JPEG-compressed\nimages,‚Äù in CVPR, 2016, pp. 2764‚Äì2772.\n[126] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, ‚ÄúImage quality assessment: From error visibility to structural similarity,‚Äù IEEE\nTransactions on Image Processing, vol. 13, no. 4, pp. 600‚Äì612, 2004.\n[127] Y. Watkins, O. Iaroshenko, M. Sayeh, and G. Kenyon, ‚ÄúImage compression: Sparse coding vs. bottleneck autoencoders,‚Äù in IEEE Southwest\nSymposium on Image Analysis and Interpretation.\nIEEE, 2018, pp. 17‚Äì20.\n[128] T. Wiegand, G. J. Sullivan, G. Bjontegaard, and A. Luthra, ‚ÄúOverview of the H.264/AVC video coding standard,‚Äù IEEE Transactions on\nCircuits and Systems for Video Technology, vol. 13, no. 7, pp. 560‚Äì576, 2003.\n[129] I. H. Witten, R. M. Neal, and J. G. Cleary, ‚ÄúArithmetic coding for data compression,‚Äù Communications of the ACM, vol. 30, no. 6, pp.\n520‚Äì541, 1987.\n[130] D. H. Wolpert and W. G. Macready, ‚ÄúNo free lunch theorems for optimization,‚Äù IEEE Transactions on Evolutionary Computation, vol. 1,\nno. 1, pp. 67‚Äì82, 1997.\n[131] C.-Y. Wu, N. Singhal, and P. Kr¬®ahenb¬®uhl, ‚ÄúVideo compression through image interpolation,‚Äù in ECCV, 2018, pp. 416‚Äì431.\n[132] F. Wu, D. Liu et al., ‚ÄúDescription of SDR video coding technology proposal by University of Science and Technology of China, Peking\nUniversity, Harbin Institute of Technology, and Wuhan University,‚Äù JVET, Tech. Rep. JVET-J0032, 2018.\n[133] J. Xu, M. Xu, Y. Wei, Z. Wang, and Z. Guan, ‚ÄúFast H.264 to HEVC transcoding: A deep learning method,‚Äù IEEE Transactions on\nMultimedia, DOI: 10.1109/TMM.2018.2885921, 2018.\n[134] M. Xu, T. Li, Z. Wang, X. Deng, R. Yang, and Z. Guan, ‚ÄúReducing complexity of HEVC: A deep learning approach,‚Äù IEEE Transactions\non Image Processing, vol. 27, no. 10, pp. 5044‚Äì5059, 2018.\n[135] N. Yan, D. Liu, B. Li, H. Li, T. Xu, and F. Wu, ‚ÄúConvolutional neural network-based invertible half-pixel interpolation Ô¨Ålter for video\ncoding,‚Äù in ICIP, 2018, pp. 201‚Äì205.\n[136] N. Yan, D. Liu, H. Li, B. Li, L. Li, and F. Wu, ‚ÄúConvolutional neural network-based fractional-pixel motion compensation,‚Äù IEEE\nTransactions on Circuits and Systems for Video Technology, vol. 29, no. 3, pp. 840‚Äì853, 2019.\n[137] N. Yan, D. Liu, H. Li, and F. Wu, ‚ÄúA convolutional neural network approach for half-pel interpolation in video coding,‚Äù in ISCAS. IEEE,\n2017, pp. 1‚Äì4.\n[138] R. Yang, M. Xu, T. Liu, Z. Wang, and Z. Guan, ‚ÄúEnhancing quality for HEVC compressed videos,‚Äù IEEE Transactions on Circuits and\nSystems for Video Technology, DOI: 10.1109/TCSVT.2018.2867568, 2018.\n[139] R. Yang, M. Xu, Z. Wang, and T. Li, ‚ÄúMulti-frame quality enhancement for compressed video,‚Äù in CVPR, 2018, pp. 6664‚Äì6673.\n[140] K. Yu, C. Dong, L. Lin, and C. C. Loy, ‚ÄúCrafting a toolchain for image restoration by deep reinforcement learning,‚Äù in CVPR, 2018, pp.\n2443‚Äì2452.\n[141] H. Zhang, L. Song, Z. Luo, and X. Yang, ‚ÄúLearning a convolutional neural network for fractional interpolation in HEVC inter coding,‚Äù\nin VCIP.\nIEEE, 2017, pp. 1‚Äì4.\n[142] K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang, ‚ÄúBeyond a Gaussian denoiser: Residual learning of deep CNN for image denoising,‚Äù\nIEEE Transactions on Image Processing, vol. 26, no. 7, pp. 3142‚Äì3155, 2017.\n[143] Q. Zhang, D. Liu, and H. Li, ‚ÄúDeep network-based image coding for simultaneous compression and retrieval,‚Äù in ICIP.\nIEEE, 2017,\npp. 405‚Äì409.\n[144] Y. Zhang, L. Sun, C. Yan, X. Ji, and Q. Dai, ‚ÄúAdaptive residual networks for high-quality image restoration,‚Äù IEEE Transactions on\nImage Processing, vol. 27, no. 7, pp. 3150‚Äì3163, 2018.\n[145] Y. Zhang, T. Shen, X. Ji, Y. Zhang, R. Xiong, and Q. Dai, ‚ÄúResidual highway convolutional neural networks for in-loop Ô¨Åltering in\nHEVC,‚Äù IEEE Transactions on Image Processing, vol. 27, no. 8, pp. 3827‚Äì3841, 2018.\n[146] Z. Zhang, Z. Chen, J. Lin, and W. Li, ‚ÄúLearned scalable image compression with bidirectional context disentanglement network,‚Äù arXiv\npreprint arXiv:1812.09443, 2018.\n[147] L. Zhao, H. Bai, A. Wang, and Y. Zhao, ‚ÄúLearning a virtual codec based on deep convolutional neural network to compress image,‚Äù\narXiv preprint arXiv:1712.05969, 2017.\n[148] Z. Zhao, S. Wang, S. Wang, X. Zhang, S. Ma, and J. Yang, ‚ÄúEnhanced bi-prediction with convolutional neural network for high efÔ¨Åciency\nvideo coding,‚Äù IEEE Transactions on Circuits and Systems for Video Technology, DOI: 10.1109/TCSVT.2018.2876399, 2018.\n[149] L. Zhou, C. Cai, Y. Gao, S. Su, and J. Wu, ‚ÄúVariational autoencoder for low bit-rate image compression,‚Äù in CVPR Workshops, 2018,\npp. 2617‚Äì2620.\n",
  "categories": [
    "cs.MM",
    "eess.IV"
  ],
  "published": "2019-04-29",
  "updated": "2019-04-29"
}