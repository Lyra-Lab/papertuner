{
  "id": "http://arxiv.org/abs/1904.13255v2",
  "title": "Generative Adversarial Imagination for Sample Efficient Deep Reinforcement Learning",
  "authors": [
    "Kacper Kielak"
  ],
  "abstract": "Reinforcement learning has seen great advancements in the past five years.\nThe successful introduction of deep learning in place of more traditional\nmethods allowed reinforcement learning to scale to very complex domains\nachieving super-human performance in environments like the game of Go or\nnumerous video games. Despite great successes in multiple domains, these new\nmethods suffer from their own issues that make them often inapplicable to the\nreal world problems. Extreme lack of data efficiency, together with huge\nvariance and difficulty in enforcing safety constraints, is one of the three\nmost prominent issues in the field. Usually, millions of data points sampled\nfrom the environment are necessary for these algorithms to converge to\nacceptable policies.\n  This thesis proposes novel Generative Adversarial Imaginative Reinforcement\nLearning algorithm. It takes advantage of the recent introduction of highly\neffective generative adversarial models, and Markov property that underpins\nreinforcement learning setting, to model dynamics of the real environment\nwithin the internal imagination module. Rollouts from the imagination are then\nused to artificially simulate the real environment in a standard reinforcement\nlearning process to avoid, often expensive and dangerous, trial and error in\nthe real environment. Experimental results show that the proposed algorithm\nmore economically utilises experience from the real environment than the\ncurrent state-of-the-art Rainbow DQN algorithm, and thus makes an important\nstep towards sample efficient deep reinforcement learning.",
  "text": "University of Birmingham\nUndergraduate Thesis\nGenerative Adversarial Imagination for Sample\nEﬃcient Deep Reinforcement Learning\nKacper P. Kielak\nBSc Artiﬁcial Intelligence and Computer Science\nStudent ID: 1698133\nSupervised by\nDr. Per Kristian Lehre\nSchool of Computer Science, University of Birmingham\nApril, 2019\narXiv:1904.13255v2  [cs.LG]  10 Jun 2019\nAbstract\nReinforcement learning has seen great advancements in the past ﬁve years. The\nsuccessful introduction of deep learning in place of more traditional methods al-\nlowed reinforcement learning to scale to very complex domains achieving super-\nhuman performance in environments like the game of Go or numerous video games.\nDespite great successes in multiple domains, these new methods suﬀer from their\nown issues that make them often inapplicable to the real world problems. Extreme\nlack of data eﬃciency, together with huge variance and diﬃculty in enforcing safety\nconstraints, is one of the three most prominent issues in the ﬁeld. Usually, millions\nof data points sampled from the environment are necessary for these algorithms\nto converge to acceptable policies.\nThis thesis proposes novel Generative Adversarial Imaginative Reinforcement Learn-\ning algorithm. It takes advantage of the recent introduction of highly eﬀective\ngenerative adversarial models, and Markov property that underpins reinforcement\nlearning setting, to model dynamics of the real environment within the internal\nimagination module. Rollouts from the imagination are then used to artiﬁcially\nsimulate the real environment in a standard reinforcement learning process to\navoid, often expensive and dangerous, trial and error in the real environment.\nExperimental results show that the proposed algorithm more economically utilises\nexperience from the real environment than the current state-of-the-art Rainbow\nDQN algorithm, and thus makes an important step towards sample eﬃcient deep\nreinforcement learning.\n1\nAcknowledgments\nI would like to express my great gratitude to Dr Per Kristian Lehre for his invalu-\nable guidance over the past year. His out-of-the-box suggestion during the planning\nphase of the project allowed me to put together diﬀerent ideas that ﬁnally led to\nthe work presented in this thesis.\nIn addition, I would like to thank Dr Lukasz Kaiser from Google Brain for ﬁnding\nthe time to answer my questions regarding his recent ﬁndings.\n2\nContents\nContents\n3\n1\nIntroduction\n5\n2\nBackground\n7\n2.1\nReinforcement learning . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.2\nDeep reinforcement learning . . . . . . . . . . . . . . . . . . . . . .\n9\n2.3\nGenerative adversarial networks . . . . . . . . . . . . . . . . . . . .\n11\n3\nRelated work\n14\n3.1\nModel-based reinforcement learning . . . . . . . . . . . . . . . . . .\n14\n3.1.1\nLearning the model - imagination . . . . . . . . . . . . . . .\n14\n3.1.2\nApplications of imagination\n. . . . . . . . . . . . . . . . . .\n15\n3.2\nGANs in reinforcement learning . . . . . . . . . . . . . . . . . . . .\n17\n4\nImaginative framework for data eﬃciency\n17\n4.1\nModel-free module\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n4.2\nImagination module . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n4.3\nModel-free phase\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n4.4\nImagination training phase . . . . . . . . . . . . . . . . . . . . . . .\n20\n4.5\nImagination based phase . . . . . . . . . . . . . . . . . . . . . . . .\n21\n4.6\nSummary\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n5\nImagination module structure for GAIRL\n23\n6\nExperimental setup\n26\n6.1\nEnvironments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n6.1.1\nMountainCar\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n6.1.2\nAcrobot . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n6.2\nImplementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n6.2.1\nModel-free reinforcement learning . . . . . . . . . . . . . . .\n29\n6.2.2\nGenerative models\n. . . . . . . . . . . . . . . . . . . . . . .\n30\n6.2.3\nGAIRL\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n6.3\nEvaluation metrics\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n7\nResults\n35\n7.1\nImagination performance . . . . . . . . . . . . . . . . . . . . . . . .\n36\n7.1.1\nReward generation . . . . . . . . . . . . . . . . . . . . . . .\n36\n3\n7.1.2\nState generation . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n7.2\nData eﬃciency\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n8\nDiscussion\n40\n9\nConclusion\n43\nBibliography\n44\nAppendix A Numerical results for data eﬃciency\n50\nAppendix B Computational eﬃciency analysis\n51\nAppendix C Generative hyperparameters for MNIST\n53\n4\n1\nIntroduction\nOne of the most prominent dilemmas in the ﬁeld of artiﬁcial intelligence is to pro-\nduce fully independent agents that learn optimal behaviour and develop over time\npurely by trial and error interaction with the surrounding environment. A math-\nematical framework that encapsulates the problem of these autonomous systems\nis reinforcement learning (RL) (Sutton & Barto 1998). Although over the past\nfew years exceptional progress has been made in devising artiﬁcial agents that can\nlearn and solve problems in a variety of domains using RL approaches (Arulku-\nmaran et al. 2017), these techniques are still not ideal. They require an immense\namount of non-optimal interaction with the real environment before they begin\nto operate acceptably well and they do not eﬃciently adapt to new tasks, even\nwithin the identical environmental setting (Irpan 2018).\nSo far RL researchers were concentrating on mastering games like backgammon,\nchess, go, or various video games. In these settings dynamics of the environment\nare either entirely known and thus can be simulated (rules of the board games),\nor they can be queried and reset inﬁnitely many times without any additional\ncosts (video games). It allowed producing inﬁnite amounts of data for the agent to\nlearn. However, these kinds of conditions are rare in the real world. Dynamics of\nthe environment are usually unknown and are too involved to approximate using\nrule-based methods. Often, we also cannot let the agent do millions of arbitrary\ntrial-and-error live experiments freely.\nIt is simple to imagine the use of reinforcement learning agent to optimise user\nexperience on the website. Every bad decision in the real environment may result\nin a loss of an unsatisﬁed customer. Millions of such choices, before the agent\nconverges to the optimal policy is too big of a risk for any company. Another\nexample can be an autonomous car accustomed to driving in a speciﬁc country\nthat suddenly ﬁnds itself in another country with a completely diﬀerent driving\nculture. A human could quickly adapt to the new reality, but a current state-of-the-\nart reinforcement learning agent would require enormous amounts of experience\nﬁrst, highly increasing a probability of a severe accident.\nModel-based reinforcement learning algorithms promise to solve this obstacle by\nusing known dynamics of the environment to analyse probable scenarios.\nThe\nagent can imagine various circumstances, and learn or reason based on them,\nwithout actually executing expensive trial-and-error exploration in the real world.\nUse of internal forecasts of the world for decision making and reasoning was deeply\nexamined within the neuroscience community (Tolman 1948, Hassabis et al. 2007,\n5\nSchacter et al. 2012). It has been demonstrated to exist within the learning pro-\ncess of humans and several animals (Pfeiﬀer & Foster 2013, Leinweber et al. 2017).\nAgain, it is manageable from the RL perspective when we fully understand the\nmodel of the environment as exhibited by the AlphaGo (Silver et al. 2017). How-\never, as discussed earlier, we do not always know the exact speciﬁcs of the envi-\nronment and, frequently, we have no prior knowledge regarding its dynamics at\nall.\nOne plausible solution to that diﬃculty could be learning the model of the envi-\nronment instead. Some work has been done already on the subject. Most promi-\nnently, Oh et al. (2015) and Leibfried et al. (2016) showed that the dynamics of\nthe environment can be modelled with very high accuracy. Nonetheless, although\nlearned ’imaginative’ model helped to improve outcomes in environments that re-\nquire long-term planning, it did not signiﬁcantly reduce the size of the system\nexposure required for training a well-performing agent (Racani`ere et al. 2017).\nThis brings about the following questions:\n• Can learning the imaginative model of the environment be more data eﬃcient\nthan learning an optimal policy?\n• If so, can the learned imagination fulﬁl the promise of sample eﬃcient model-\nbased RL in settings where dynamics of the real environment are unknown?\nThis study subsequently answers both of the questions. It combines a few recent\nand a few less recent ideas from the ﬁeld to do so. It hypothesises that recent\nadvancement in the generative adversarial networks architecture (Goodfellow et al.\n2014), and inherent to the RL setting Markov property can provide a positive\nanswer to the ﬁrst question. Furthermore, it considers that the potential use of\nimagination within the structure similar to the Dyna-Q algorithm (Sutton 1990)\nmay indeed profoundly improve sample eﬃciency of RL in unknown environments.\nFollowing from the introduction, this thesis is structured as follows: Section 2\nprovides scientiﬁc background and basic theoretical fundamentals necessary for\nfull understanding of the conducted research. Section 3 gives an overview of al-\nready existing studies that are relevant to this topic. Section 4 introduces novel\nGenerative Adversarial Imaginative Reinforcement Learning (GAIRL) algorithm\ndeveloped to test the above-stated hypothesis. Section 5 then goes into details of\nmethods that were used to eﬃciently create accurate imagination, i.e. learn the\nmodel of the environment. Section 6 describes an experimental setting used to\nevaluate newly proposed GAIRL algorithm and compare it to the current state-\nof-the-art. Section 7 presents the qualitative results that are further discussed in\n6\nsection 8. Finally, section 9 summarises the work carried out.\n2\nBackground\n2.1\nReinforcement learning\nReinforcement learning is the problem of learning an optimal policy (i.e.\nbe-\nhaviour) for a given environment (Sutton & Barto 1998). RL is formalised by\nMarkov decision processes (MDPs).\nAn MDP can be formulated as a tuple\n(S, A, T, R, γ) where S is a (discrete or continuous) set of possible states, A is\na (discrete or continuous) set of allowed actions, T is a transition probabilities\nfunction, R is a (deterministic or stochastic) reward function, and γ ∈(0; 1) is a\ndiscount factor for future rewards (controls agent’s time preference). At any given\ntime t, the reinforcement learning agent observes an environment state st ∈S\nand selects an action at ∈A. Then, the reward rt+1 = Rat\nst is returned from the\nenvironment, and the environment moves to the state st+1 ∈S with transition\nprobability T at\nstst+1 = P(st+1|st, at). T and R fully describe the dynamics of the\nenvironment.\nFigure 1: Reinforcement learning setting\nA critical characteristic of MDPs is that it follows the Markov assumption. Namely,\nat any point in time t, history of encountered states s0, s1, . . . , st−1, st can be\n7\nsimpliﬁed to the last state st without any loss in information, i.e:.\nP(st+1|st) = P(st+1|s1, . . . , st)\n(1)\nIt is an assumption required by RL algorithms. Unfortunately, this property is\nnot always observable in the real world. We can often circumvent its absence by\nclever preprocessing of the state space S or by translating this lack of information\nto additional stochasticity on top of the transition and reward functions. However,\neven with these countermeasures, lack of Markov assumption may limit the range\nof optimal strategies that our agent can learn.\nThe agent interacts with an MDP by selecting actions accordingly to the policy π\nthat maps states to the probability distribution over A. The goal of obtaining an\noptimal policy can be formulated as learning a policy π∗that maximises the state\nvalue function V π : S →R (Bellman equation):\nV π(s) =\nX\na∈A\nπ(a|s)(Ra\ns + γ\nX\ns′∈S\nT a\nss′V π(s′))\n(2)\nFrom which we can also deﬁne state-action value function Qπ : S × A →R:\nQπ(s, a) = Ra\ns + γ\nX\ns′∈S\nT a\nss′\nX\na′∈A\nπ(a′|s′)Qπ(s′, a′)\n(3)\nThere is always at least one optimal policy and it satisﬁes/entails the following:\n∀\nπ ∀\ns∈S V π∗(s) ≥V π(s)\n(4)\nV π∗(s) = max\na∈A (Ra\ns + γ\nX\ns′∈S\nT a\nss′V π∗(s′))\n(5)\nQπ∗(s, a) = Ra\ns + γ\nX\ns′∈S\nT a\nss′ max\na′∈A Qπ∗(s′, a′)\n(6)\nThe setting could potentially be directly solved employing simple linear algebra:\nV π∗= (I −γT)−1R; however, it works exclusively for ﬁnite-state MDPs and its\ntime complexity is O(n2.4) (Coppersmith & Winograd 1990). It is computationally\ninfeasible for large and complex environments that are encountered in a vast ma-\njority of conditions. Therefore, it is rarely used, and most of the focus is given to\nalgorithms that leverage sampling of the environment to approximate the optimal\nsolution.\n8\nThere are three diﬀerent types of RL algorithms:\n• Value-based - explicitly learn state-action value function (3) of the envi-\nronment and use it to infer optimal policy π(s) = argmaxa∈A Q(s, a). E.g.\nQ-learning (Watkins & Dayan 1992).\n• Policy-based - do not learn state value function explicitly but instead directly\nlearn policy mapping π : S →A. E.g. REINFORCE (Williams 1992).\n• Actor-critic - combines learning policy (actor) and value function (critic).\nThe actor makes choices about actions but it is updated by the feedback from\nthe critic who then directly interacts with the environment. E.g. natural\nactor-critic (Peters et al. 2005).\nValue-based methods are usually the simplest and the fastest. The downside is that\nthey do not work in continuous action spaces or when we want to learn stochastic\npolicy. Policy-based methods have better convergence qualities and can handle\nmore complex policies and action spaces but are usually ineﬃcient and converge\nto local minima. Actor-critic methods try to connect the advantages of both.\nThese traditional algorithms allowed for a big advancement in computer science\ncreating artiﬁcial agents that have reached human-level performance in multiple\ngames (e.g. backgammon (Tesauro 1995)) without any prior knowledge. They\nwork ﬂawlessly for smaller state and action spaces where state value function (2)\nand/or state-action value function (3) can be expressed by the look-up table. A\ntabular representation of these functions, however, is often not viable due to too\nbig or continuous domains.\n2.2\nDeep reinforcement learning\nTo allow RL to scale to more involved situations, one can propose using function\napproximators (such as linear regression, neural networks, or Bayesian methods)\nto approximate state-action value function Q or policy π. Unfortunately, recursive\nupdates of the functions (2) (3) and lack of independent and identically distributed\nvariables (future states/rewards highly depends on current state and action) in the\nRL setting break most of the assumptions expected by standard machine learning\nmethods.\nTherefore, trials of plugging non-linear function approximators into\nthe existing algorithms in the place of a tabular representation were resulting in\ncomplete divergence and failure.\nBecause it limited scalability of RL to many of the real-world problems, consider-\n9\nable amount of work has been done to solve this obstacle. Recently, Mnih et al.\n(2015) introduced Deep Q-Network (DQN), an RL algorithm that for the ﬁrst time\nwas capable of leveraging black-box properties of deep learning. They bypassed\nthe dilemma of recursive updates and lack of i.i.d. by proposing two adjustments\nto the standard Q-learning algorithm (Watkins & Dayan 1992).\nReplay buﬀer - rather than instantly learning from sampled data that is highly\ncorrelated, algorithm stores N most recent tuples (st, at, rt+1, st+1) in a replay\nbuﬀer D. When updating the value function Q, it uses a random mini-batch of\nsamples from D to estimate gradients. It reduces the correlation between samples\nby breaking their ordering.\nTarget network - instead of updating Q function directly with itself, the algorithm\nmaintains two distinct networks: the online network Q and the target network bQ.\nbQ is simply a ﬁxed snapshot of Q taken every C updates. The agent determines\nactions in a conventional manner accordingly to the network Q, but Q is updated\nusing a revised Bellman equation:\nQ(s, a) = Ra\ns + γ\nX\ns′∈S\nP a\nss′\nX\na′∈A\nπ(a′|s′) bQ(s′, a′)\n(7)\nIt stabilises the whole learning process and avoids exploding gradients by partially\neliminating recursiveness in network updates.\nDQN achieved super-human level performance on Atari 2600 games from the Ar-\ncade Learning Environment (ALE) (Bellemare, Naddaf, Veness & Bowling 2013)\nwith raw pixels as input alone. This contribution opened an enormous amount of\nopportunities for RL. A lot of new algorithms and DQN improvements followed.\nSchaul et al. (2015) enhanced DQN slightly increasing its data eﬃciency by ap-\nplying prioritised experience replay to more frequently replay more informative\nsamples. Van Hasselt et al. (2016) devised Double DQN that extends DQN to\nthe double Q-learning method (Van Hasselt 2010) that addresses an overestima-\ntion bias of the standard Q-learning. Wang et al. (2015) introduced a new du-\nelling network architecture speciﬁcally for the value-based RL that outperformed\nconventional supervised machine learning architectures. Bellemare et al. (2017)\nproduced DQN-based algorithm that, alternatively to learning scalar state-action\nvalue function Q, learns a categorical distribution of the future returns Z whose\nexpectation is Q proving both theoretically and experimentally that this proce-\ndure improves the original DQN algorithm. Hessel et al. (2017) then consolidated\nall improvements enumerated in this paragraph (and a few smaller ones) into an\n10\nalgorithm called Rainbow DQN. It is the current RL state-of-the-art for discrete\naction spaces.\nHowever, DQN, as a deep variant of Q-learning (value-based method), does not\nwork in case of continuous actions space. Hence, it was soon merged with determin-\nistic policy gradient methods (Silver et al. 2014) to create the Deep Deterministic\nPolicy Gradient (DDPG) actor-critic algorithm (Lillicrap et al. 2015) solving this\nissue. Recently, following the success of the Rainbow DQN, multiple incremental\nreﬁnements suitable to the DDPG were consolidated forming Distributed Distribu-\ntional Deep Deterministic Policy Gradient (D4PG) algorithm (Barth-Maron et al.\n2018). D4PG is the current state-of-the-art algorithm for complicated, continuous\naction space settings.\nAlthough, the introduction of deep neural networks for the ﬁrst time enabled\nRL algorithms to solve extremely complex problems and surpass humans at many\nlevels, all of them suﬀer from tremendous data ineﬃciency. Rainbow DQN requires\naround 15 million frames of interaction with the real environment to match median\nhuman performance. It corresponds to over 8 days of constant play at the regular\nrate of 20 frames per second. It requires a total of full 200M frames to reach its peak\nperformance (over three full months). In contrast, median human performance\nis deﬁned as a score achieved by a person after merely 15 minutes of training\nbeforehand.\nAs explained in the introduction, this is not an enormous problem when training\nagent to master board or video games as computational power is relatively cheap\nnowadays. Three months of 20 frames per second can be shortened to 1 hour\nof ∼45,000 frames per second given powerful enough infrastructure. However,\nit makes deep RL inapplicable to any other problem where obtaining samples of\nexperience comes with potential additional costs like losing unsatisﬁed customers\nor causing an accident.\n2.3\nGenerative adversarial networks\nRecently, Goodfellow et al. (2014) introduced generative adversarial networks\n(GANs).\nThey became a successful and widespread tool for data generation,\nprofoundly over-performing previously used methods. Contrary to standard ap-\nproaches that primarily focused on minimising L1 (8) or L2 (9) loss between a\ngenerated output and a real output on the individual level, GANs make it possible\nto work on the data distribution level minimising a diﬀerence between a generated\n11\ndata distribution and a real data distribution instead.\nL1 =\nn\nX\ni=1\n|xgenerated −xtrue|\n(8)\nL2 =\nn\nX\ni=1\n(xgenerated −xtrue)2\n(9)\nGANs work by deﬁning two separate networks. The generator G : Z →X that\nmaps a noise vector z ∈Z coming from the noise distribution pz onto the data\nspace X, and the discriminator D : X →[0, 1] that maps an input from the data\nspace X to the probability of the input coming from the real data distribution\npdata. Both networks play a two-player minimax game with the objective:\nmin\nG max\nD\nE\nx∼pdata[log(D(x))] + E\nz∼pz[log(1 −D(G(z)))]\n(10)\nAs theoretically proven in the original study, this minimax game is equivalent to\nminimising the Jensen-Shanon (JS) divergence between pdata and pg:\nJS(pdata||pg) ∝\nE\nx∼pdata[log\npdata(x)\npdata(x) + pg(x)] + E\nx∼pg[log\npg(x)\npdata(x) + pg(x)]\n(11)\nand thus, modelling pg to as closely resemble pdata as possible.\nUnfortunately, GANs are also known for their lack of stability in training, often\ncausing situations where one of the networks starts to completely overwhelm the\nother. This results in diminishing gradients for both of the networks. They also\ntend to ignore certain spectrums of the distribution (mode collapse problem).\nTherefore, numerous researchers tried to stabilise the original GAN algorithm. One\nof the most successful improvements was replacing JS divergence with the Earth-\nMover distance (1st Wasserstein metric) creating the Wasserstein GAN (Arjovsky\net al. 2017):\nmin\nG max\nD\nE\nx∼pdata[D(x)] + E\nz∼pz[−D(G(z))]\n(12)\nNow, the discriminator (in the paper called critic) is no longer trying to predict if\nthe data comes from the real or fake distribution. It is rather providing an actual\nreal-valued distance (as measured by the Earth-Mover metric) between the data\ngenerated by the generator and the data coming from the real distribution. Hence,\nthe goal is not longer to balance both networks but to make sure that the critic can\nconverge to the real distance before letting the generator to improve. It has been\nmathematically proven in the study that the Wasserstein GAN always converges\ngiven that the critic is inﬁnitely more powerful than the generator.\n12\nThe issue with the Wasserstein GAN is that it constructs its minmax value func-\ntion using the Kantorovich-Rubinstein duality (Villani 2008). Therefore, to be\ntheoretically and practically sound, the critic needs to represent values coming\nfrom the set of 1-Lipschitz functions:\nf is 1-Lipschitz\n⇐⇒|f(x1) −f(x2)| ≤|x1 −x2|\n(13)\nThe original Wasserstein GAN enforced this limitation by clipping the weights of\nthe critic network to space [−c; c]. Setting the c hyperparameter is, however, a\nnon-trivial task that introduced new instability problems.\nFortunately, Gulrajani et al. (2017) circumvented this concern by appending a\ngradient penalty to the ﬁnal minimax objective instead of clipping weights of the\ncritic:\nmin\nG max\nD\nE\nx∼pdata[D(x)] + E\nz∼pz[−D(G(z))] + λ\nE\nbx∼N(0,1)[(||∇bxD(bx)||2 −1)2]\n(14)\nwhere λ is a gradient penalty factor hyperparameter. λ = 10 in the original paper\nand λ = 0 recovers original Wassersetein GAN objective.\nUnfortunately, the choice between the original GAN and the Wasserstein GAN\nwith gradient penalty (WGANGP) is not that trivial. Lucic et al. (2018) showed\nthat WGANGP is not necessarily outperforming standard GAN given suitable\nhyperparameters conﬁguration. WGANGP also takes much longer to train because\nthe critic, for every step of the generator, has to converge to the appropriate\nvalue of the Wasserstein distance fully. However, WGANGP’s hyperparameters\nare much simpler to optimise. The choice between both usually comes down to the\ntrade-oﬀbetween computer power consumed by training and researcher/developer\ntime spent on tuning the hyperparameters.\nFurthermore, much research has been devoted to conditional GANs where the gen-\nerator, rather than taking only a random noise z as an input, takes another value\ny based on which the ﬁnal output is conditioned (Mirza & Osindero 2014). This\nenables us to not only generate arbitrary samples that follow the data distribu-\ntion but also to have an inﬂuence on what precise spectrum of the distribution to\nobtain. This is particularly valuable when ﬁtting GANs to the RL setting where\nthe reward and next state tuple (rt+1, st+1) is conditioned on the current state and\nchosen action pair (st, at). The state-of-the-art in conditional GANs, especially in\nthe area of computer vision, is PIX2PIX GAN that exhibited extraordinary results\nin multiple image-to-image translation problems (Isola et al. 2017).\n13\nFigure 2: Comparison between GAN and L1-loss-based model on the classical\nMNIST dataset. Upper row presents results of an L1 loss generation, whereas\nlower row presents results of a GAN. The GAN produces very realistic images\nof handwritten data, indistinguishable from the original, due to successful data\ndistribution approximation. L1 model, on the other hand, is only able to ﬁnd\nimages that minimise its mean error on the level of the individual, thus producing\nblurry results.\n3\nRelated work\n3.1\nModel-based reinforcement learning\nAs brieﬂy discussed in section 1, one of the promises of the model-based rein-\nforcement learning is to drastically improve the sample eﬃciency of reinforcement\nlearning. The agent with access to the transition matrix T and reward function\nR (section 2.1) could internally reason about potential scenarios and outcomes\nwithout performing risky exploration in the real environment. Unfortunately, the\nspeciﬁc characteristics of the RL environment are usually unknown, and hence\nthese methods cannot be directly applied.\n3.1.1\nLearning the model - imagination\nOne plausible solution to that diﬃculty could be learning the model of the en-\nvironment instead. The idea of learning the model of the environment from the\nsampled experience when its dynamics are not fully known is one of the most im-\nportant concepts adapted in this thesis. However, this notion is not new. Current\ninvestigation in this area concentrates on application of variational autoencoders\n(VAEs) (Kingma & Welling 2013), recurrent neural networks (RNNs) (Medsker\n& Jain 1999), and/or Bayesian methods. Lenz et al. (2015) introduced a novel\nrobot control algorithm leveraging RNN that predicted the position of robot’s\nparts. Bellemare, Veness & Bowling (2013) factors the state space to decompose\nmodel learning into smaller, more manageable sub-problems and applies Bayesian\ninference to predict future states.\n14\nA substantial breakthrough in learning the model of the environment in the RL\nsetting and the algorithm that is most widely applicable was proposed by Oh\net al. (2015). The paper presented two novel Encoding-Transformation-Decoding\narchitectures to learn the transition probabilities function T. They ﬁrst encode\nhigh-dimensional state st using convolutional network (LeCun et al. 1999), then\nthe transformation conditioned on at is performed to convert a high-level encoding\nof the of the current state to a high-level encoding of the next state, and ﬁnally\ndecoding using deconvolutional network (Zeiler et al. 2010) is executed to decode\nhigh-level next-state features into the full representation of the next state st+1. The\nﬁrst architecture employs a simple feed-forward method and takes a ﬁxed history\nof states as an input. The other takes advantage of LSTM cells (Hochreiter &\nSchmidhuber 1997) to capture the most relevant features from the past.\nThis work was later extended through a straightforward modiﬁcation to support\nmodelling of reward function R, and thus, to able to learn the full model of the\nenvironment (Leibfried et al. 2016). It was also incrementally improved by Chiappa\net al. (2017) by the alteration of the original architecture and exploration of a few\nnovel ideas.\nNevertheless, these model generation techniques have two major areas for improve-\nment:\n• They do not utilise Markov property (1) fully, treating the model learning\nas a highly sequential and non-stationary problem instead of transforming\nit into much easier, stationary scenario. It makes capturing all intrinsics of\nthe environment much harder to learn.\n• They utilise L1 or L2 objective to train the model and predict future states.\nL1/L2 loss penalises each diﬀerence between prediction and ground truth\nuniformly. Thus, it struggles to prioritise small but signiﬁcant features over\nlarge but meaningless (e.g. slight ball location change over reduced satura-\ntion of a black background in Pong Atari game) and often produces blurry\nresults. This is a well-known problem when applying standard classiﬁca-\ntion/regression deep learning models for generative tasks.\n3.1.2\nApplications of imagination\nHistorically, there has been a signiﬁcant amount of work devoted to model-based\nreinforcement learning methods that took advantage of known environment dy-\nnamics. Firstly, as already mentioned in section 2.1, having that knowledge allows\n15\nto directly solve the RL setting using simple linear algebra. If state/action is too in-\ntricate for that, we can apply traditional planning techniques. Additionally, Monte\nCarlo tree search (Browne et al. 2012) can be deployed for more reliable evaluation\nof the future rewards. Several model-based techniques like DQN-cognate methods\ncoupled with prior knowledge about the intrinsic model of the environment have\nalready yielded outstanding eﬀects (Silver et al. 2016, 2017).\nSadly, using these techniques directly on the imagination (approximated real en-\nvironment) has been shown to perform very poorly (Talvitie 2014, 2015). It is\nbecause approximations, by their nature, introduce a certain level of error. This\nerror quickly compounds over time when performing long environment rollouts\ninto the future. Utilising this approximation to estimate policy using planning-\nlike methods is directly translating this compounded error into estimated policy\ncausing it to be, in the best case, sub-optimal.\nOne of the ﬁrst algorithms that managed to do that successfully was Dyna-Q\n(Sutton 1990). It used a look-up table for imagination modelling. The imagination\nwas later used to simulate and replace the real environment. Unfortunately, it did\nnot scale to more complex problems due to the use of a ﬁnite look-up table. The\nhigh-level structure of the algorithm proposed in this thesis is inspired by the\ntraditional Dyna-Q and can be partially viewed as its expansion to bigger and/or\ncontinuous state spaces.\nMost recently, Racani`ere et al. (2017) successfully employed model learning tech-\nniques described in section 3.1.1 by combining them with the standard model-free\nmethods in the decision making process. Their results showed improvements in the\neﬀective exploration and in the environments where long-term planning is crucial,\neven, when the imagination of the agent was far from perfect. Nevertheless, it did\nnot signiﬁcantly reduce the size of the system exposure necessary for training a\nwell-performing agent.\nRecent work that most closely resembles this thesis was done by Venkatraman\net al. (2016) and Gu et al. (2016). They also try to improve sample eﬃciency\nof RL by using Dyna-Q inspired algorithms. However, both of the studies adopt\nsimple, often linear, methods to train the imagination. Hence, they can only be\napplied to speciﬁc domains where such methods are successful. Additionally, their\ndata eﬃciency is not speciﬁcally better than the current model-free state-of-the-art\nRainbow DQN (Hessel et al. 2017).\n16\n3.2\nGANs in reinforcement learning\nGANs and RL are usually treated as separate ﬁelds of machine learning research\nby the community. There was not much attention to combining them or employing\nadvances from one domain to improve the other. Recent work by Pfau & Vinyals\n(2016) presented a deep connection between GANs and actor-critic RL trying to\nencourage both communities to learn from each other.\nA few studies to utilise GAN architecture to devise novel RL algorithms followed.\nDoan et al. (2018) introduced GAN Q-learning, a model-free distributional al-\nternative to the DQN algorithm, by using generative adversarial architecture to\nexpress Bellman update (3) implicitly. Ho & Ermon (2016) obtained signiﬁcant\nperformance gains in imitation learning (Schaal et al. 2003) by applying a novel\nGenerative Adversarial Imitation Learning algorithm.\nHenderson et al. (2018)\nproposed an innovative OptionGAN architecture for inverse reinforcement learn-\ning (Ng et al. 2000) improving results in one-shot transfer.\nThis thesis proposes the use of GANs to learn the dynamics of the environment\nand form the agent’s imagination.\nSimilar approach was presented by Xiao &\nKesineni (2016) and Azizzadenesheli et al. (2018). They both examined learning\nthe model of the environment for Atari games to, similarly to the AlphaGo (Silver\net al. 2017), apply a combination of DQN with the Monte Carlo tree search to\neﬀectively discover the optimal policy.\nBoth studies reported negative results\ndue to the too short MCTS rollouts as proven in the former.\nAzizzadenesheli\net al. (2018), however, was able to remarkably eﬃciently learn a very accurate\nrepresentation of model dynamics using generative adversarial approach.\n4\nImaginative framework for data eﬃciency\nBuilding upon previous research, this thesis introduces Generative Adversarial\nImaginative Reinforcement Learning (GAIRL) algorithm to answer questions posed\nin section 1 and to improve overall data eﬃciency of deep RL. As brieﬂy mentioned\nin section 3, study by Sutton (1990) inspired the main structure of the novel algo-\nrithm. This structure is a focus of this section, without going much into details of\nthe generative adversarial imagination. Transforming the imaginative framework\nspeciﬁcally into the GAIRL algorithm is a topic of the next section.\nThe imaginative core underpinning GAIRL is divided into two separate modules:\nthe model-free module (MFM) and the imagination module (IM). It also makes\n17\nuse of the concept of memory M. The memory is simply an array storing previous\nreal-environment experience, similarly to the replay buﬀer in the DQN-cognate\nalgorithms that was described in section 2.2.\nThese modules are utilised across three distinct training phases. First is the model-\nfree phase (MFP) that only makes use of the MFM, followed by the imagination\ntraining phase (ITP) where solely the IM is operated, ﬁnalised by the imagination-\nbased phase (IBP) that combines both of the modules.\n4.1\nModel-free module\nThe MFM consists of a standard model-free reinforcement learning algorithm; in\nthis study, it is the state-of-the-art Rainbow DQN. It is the core decision-making\nmodule that models policy mapping π : S →A of the agent. The advantage of\nthe imaginative framework is that it can leverage any other existing model-free\nalgorithm if it is more suited for a given task (e.g. D4PG in environments with\ncontinuous action space) or if a model-free algorithm that overperforms Rainbow\nDQN is introduced in the future.\nFigure 3: Model-free module. It is in the essence a standard RL algorithm working\nexactly as described in sections 2.1 and 2.1. It represents policy π(st) = at that is\nincrementally updated based on the consequences of performed actions (rt+1, st+1).\n4.2\nImagination module\nThe IM is a crucial part of the whole framework. It is a trainable system that can\nsimulate the behaviour of the environment dynamics, i.e. accurately approximate\nthe transition probabilities function T and the reward function R, similarly to\n18\nthe approach presented by Leibfried et al. (2016). Similarly to the MFM, any\ngenerative model can be employed in place of the IM. Nonetheless, this thesis\nfocuses on a specially designed IM to optimise its data eﬃciency. In depth design\nof the IM to create the GAIRL algorithm can be found in section 5.\nFigure 4: Imagination module. Its goal is to replace the real environment by mod-\nelling its dynamics (functions T and R). Thus, its input/output exactly resembles\ninput/output of the environment shown in Figure 1.\n4.3\nModel-free phase\nThe MFP mostly follows the standard RL procedure taking advantage of the\nmodel-free RL algorithm employed within the MFM. Similarly to the standard\nRL, it is based entirely on the real environment. The only diﬀerence is that, in\naddition to the use of experience samples returned by the environment to improve\nthe policy π encapsulated in the MFM, it also stores these samples within the\nagent’s memory M.\nMFP is the only one out of three phases that requires the real environment to\nsample experience. Therefore, to limit the amount of the real experience that is\nrequired, it is also recommended to keep it as short as possible.\n19\nFigure 5: Model-free phase. Excluding the memory that stores transition tuples\n(st, at, rt+1, st+1), the whole concept was already visualised in Figure 1.\nMFM\ngiven a state st produces an action at. The pair (st, at) is fed to the real environ-\nment (technically st is not produced by the agent nor fed into the environment,\nenvironment simply is in the state st). Environment moves into the next state st+1\nand produces the reward rt+1. The pair (rt+1, st+1) is then given as a feedback to\nthe agent and together with the pair (st, at) it is saved in the memory. Finally, the\nnext state st+1 becames the new current state (st) and the whole process starts\nfrom the beginning.\n4.4\nImagination training phase\nImagination training phase is focused on using transitions samples (st, at, rt+1, st+1)\nstored in the memory M to train the IM to accurately follow the dynamics of the\n20\nreal environment (approximate functions T and R) in a purely supervised learning\nmanner.\nLength of the ITP does not aﬀect the real environment at all as it exploits past,\nmemorised, experience instead.\nHence, it can, and it should to produce more\noptimal results, run until the IM fully converges to the representation of the data\nthat is stored in the memory.\n4.5\nImagination based phase\nHaving fully trained imagination, we can start leveraging it. The IBP is focused\non improving the agent’s policy by letting MFM train and rollout experimental\nscenarios on the IM instead of making potentially expensive trial and error in the\nreal world. In fact, the MFM is not even aware of the fact that its learning process\nhas been moved onto the ’fake’ environment instead of the real one.\nFigure 6: Imagination based phase. The concept works exactly like the one visu-\nalised in Figure 1 or in Figure 5 (excluding memory). The only diﬀerence is that\nthe real environment was replaced by the imagination module.\nIBP, similarly to the ITP, should last until the agent’s policy fully converges to the\nimaginative environment to extract as much signal, from experience gathered so\n21\nfar, as possible. Nevertheless, in practice, experiments focused on performing only\nthree times more steps in the imagination than in the real environment, even, when\nthe policy did not always fully converged during the imagination based phase.\n4.6\nSummary\nThe whole premise of the framework is to extract meaningful signal from the\nobtained experience as eﬃciently as possible.\nImagination module serves as a\nway to generalise possible distribution of the world to create a safe, artiﬁcial,\nimaginative environment where the agent can learn and experiment without any\nrisks associated with the actions in the real world.\nThe three phases work in a loop following Algorithm 1. The loop part is crucial.\nIn the ﬁrst MFP, the agent usually performs completely random and experimental\nactions and is very likely to not reach more advanced environment states such as\nthe next level in a video game. Going through the ITP and the IBP can allow\nit to master the ﬁrst level of the game. However, it needs to go back to the real\nenvironment to explore newly reachable states to be able to imagine them in the\nsecond iteration accurately. This process should continue until the agent’s policy\nfully converges to the given environment.\nConvenient characteristic of the whole framework is that it can work for any model-\nfree RL algorithm in place of the MFM and any generative module in place of the\nIM. Nevertheless, only the framework that speciﬁcally leverages GANs and Markov\nproperty for the data eﬃcient IM is deﬁned as GAIRL as explained in the next\nsection.\nAlgorithm 1 Imagination for sample eﬃcient reinforcement learning\n1: procedure GAIRL(MFM, IM, env)\n2:\nInitialise MFM\n3:\nInitialise IM\n4:\nCreate and initialise M\n5:\nwhile true do\n6:\nTrain MFM on env while collecting experience samples in M (MFP)\n7:\nif MFM converged on env then\n8:\nreturn MFM agent\n9:\nTrain IM on the data from M (ITP)\n10:\nTrain MFM on IM (IBP)\n22\n5\nImagination module structure for GAIRL\nThe framework described in section 4 is based on the assumption that the IM can\nlearn the dynamics of the real environment accurately, and eﬃciently enough. As\ndiscussed in section 3.1.1, there already exist some studies on this topic. Most\nprominently Oh et al. (2015) used variational autoencoders in combination with\nrecurrent neural networks to create a model that can predict the next state in\nAtari games conditioned on the current state and the chosen action. Its results\nwere extremely accurate; however, it did not focus on sample eﬃciency.\nContrary to the previous studies, the IM of the GAIRL algorithm takes into ac-\ncount Markov property simplifying the whole setting to the straightforward super-\nvised learning problem with an entirely stationary mapping S × A →R × S. It\nis, therefore, theoretically, much more data eﬃcient, especially compared to the\nstandard model-free policy learning that tries to learn a behaviour maximising\nexpected cumulative reward, i.e. the extremely non-stationary sum of all future\nrewards.\nAlthough any supervised learning model that can ﬁt the above-mentioned descrip-\ntion could be used in the framework, certain algorithms are superior to others. As\ndescribed in section 2.3, standard deep learning models based on the L1 or L2 loss\ndo not perform well in generative tasks. In theory, the best performing architecture\nto approximate diﬀerent high-dimensional generative distributions are generative\nadversarial networks (Goodfellow et al. 2014). GANs should add another advan-\ntage over popular variational autoencoder approach. As mentioned in section 3.2,\nuse of the PIX2PIX GAN (Isola et al. 2017) for this setting already yielded remark-\nably good and sample eﬃcient results in the very recent study (Azizzadenesheli\net al. 2018).\nAnother essential characteristic of GANs is that they take random noise as an\ninput (in addition to the conditional input in case of standard generative models).\nThis quality provides an additional advantage over previously used methods in\nhighly stochastic environments. It should allow GAN, if needed, to produce a\nset of diverse stochastic outputs for the same conditional input, instead of just a\ndeterministic mean of possible outputs.\n23\nFigure 7: GAIRL’s imagination training phase. For the state prediction, a tuple\n(st, at) sampled from the memory serves as a conditional input for both generator\nand critic. The generator, given random noise z and a conditional input, tries to\ngenerate a ’fake’ next state s′\nt+1 to as closely resemble the real next state st+1 as\npossible. Then, the critic, knowing conditional inputs for each of the possible fake\nand real states, calculates a Wasserstein distance between the generator conditional\ndistribution pg and the real environment conditional distribution penv that the\ngenerator aims to reduce. For the reward prediction, a tuple (st, at) is the standard\ninput to the MLP. The MLP tries then to predict the following reward.\nThe\npredicted reward r′\nt+1 is then combined with the real reward rt+1 to calculate loss\nthat the MLP tries to minimise.\nAlthough single GAN seems like a perfect match for the IM, in practice, it is easier\nto construct the IM using two separate deep learning models. One predicting the\nnext state (modelling transition probabilities function T), another predicting the\nexpected reward (modelling reward function R). In the proposed framework, the\n24\nFigure 8: The state-predicting generator and the reward-predicting MLP plugged\ninto the IBP. Both generator and MLP work as the imagination module.\nnext state generation is handled by a GAN because state space is often highly\ndimensional structure perfectly suited for that architecture. Reward, however, is\nalways represented by a single, real-valued, scalar. Therefore, it can be simpliﬁed\nto a simple regression problem. Hence, GAIRL employs a traditional L1/L2 loss\nbased regression model for the reward generation task. The full internal structure\nof the IM module during the ITP can be seen in Figure 7. Figure 8 shows how\ntrained imagination is then used in the IBP.\nThe ﬁnal crucial step towards a successful use of the IM in the GAIRL setting was\ndeciding on starting states for the IM rollouts. Real environment simply starts in a\nparticular state. IM, on the other hand, is just a combination of supervised learning\nmodels, it does not have any inherently associated internal state. States are merely\ninputs to the machine learning system. Therefore, in the IBP, whenever there is\n25\na need to start a new episode, a random state is sampled from the memory. Just\nthen, having these ground truth initial state, the IM is used to simulate possible\nscenarios into the future in the standard manner.\n6\nExperimental setup\n6.1\nEnvironments\nTo asses the capabilities of the algorithms, OpenAI Gym (Brockman et al. 2016)\nwas employed. It provides traditional and most popular reinforcement learning\nenvironments in an easy to use manner. For the past four years, the most important\nreinforcement learning benchmarks were Atari games from the Atari Learning\nEnvironment (Bellemare, Naddaf, Veness & Bowling 2013) (also provided by the\nOpenAI gym). Unfortunately, RL algorithms require very extensive hardware to\nconverge to the optimal policies for these games (up to 100GB of RAM per single\nrun of the algorithm).\nTherefore, due to high time constraints and limited resources, this study employs\nslightly simpler benchmarks from the classic control family of the problems. It\nmainly focuses on the MountainCar (Moore 1990), and Acrobot (Sutton 1996)\nenvironments. Although classic control environments are simpler and operate on\nlower dimensions, they were the standard benchmark in the reinforcement learning\ncommunity for decades before Atari games were adopted. Nevertheless, in addition\nto showing the proof of concept of the GAIRL framework on the MountainCar and\nAcrobot, the plan is to continue the work to analyse the framework on currently\nadopted test environments.\n6.1.1\nMountainCar\nMountainCar is based on a simple, low-dimensional setting. The agent controls\na car that must drive up the steep slope. The car is not able to climb the hill\ndirectly. The agent has to learn to leverage the fact that it is situated in a valley\nand can use potential energy of the opposite slopes.\nThe state space is represented by two continuous variables: horizontal position of\nthe car xt, and velocity of the car across the car’s axis vt (st = (xt, vt) ∈R2).\nAction space is deﬁned by a single choice: drive left, or drive right (at ∈{−1, 1}).\n26\nFigure 9: MountainCar environment.\nThe agent gets a reward r = −1 for every time step until the episode is terminated\n(car drives up the right hill). An optimal policy should minimise the time it takes\nto reach the top of the hill, and thus maximise the cumulative reward obtained\nduring the episode.\nTo solve the MountainCar environment, the algorithm has to be capable of han-\ndling continuous state space. Additionally, what makes it harder to learn is a\nvery sparse reward signal. The agent has no information about the goal until it is\nreached. A meaningful signal occurs once per from 200 (close-to-optimal policy)\nto over 5000 (random policy) actions.\n6.1.2\nAcrobot\nAcrobot, although still fairly low-dimensional, is harder and much more diﬃcult\ntask. The agent controls a two-link, under-actuated robot arm. The ﬁrst (upper)\njoint, attached to the background, is out of control of the agent. The only con-\ntrollable part is the torque of the lower joint of the robot. The goal is to balance\nthe whole arm, so the tip of the second link swings above the episode termination\nline.\nSix continuous variables represent the state space: sinus and cosine of the angle\nαt between the ﬁrst link and a horizontal line; sinus and cosine of the angle βt\n27\nFigure 10: Acrobot environment.\nbetween the ﬁrst and the second link; an angular velocity ωt of the ﬁrst joint; and\nan angular velocity θt of the second joint.\nst = (sin αt, cos αt, sin βt, cos βt, ωt, θt) ∈[−1; 1]4 × R2\nAction space is again deﬁned by a single choice, although slightly modiﬁed: apply\npositive torque, no torque, or negative torque to the second joint (at ∈{−1, 0, 1}).\nSimilarly to the MountainCar, the agent gets a reward r = −1 for every time step\nuntil the episode ﬁnishes (tip of the second link swings above the termination line)\nto encourage policies that take the least amount of time to complete the episode.\n6.2\nImplementation\nAll algorithms were implemented using the Tensorﬂow machine learning library\n(Abadi et al. 2015) with Python as a front-end programming language. Results and\nperformance of the algorithms were captured and visualised using TensorBoard, a\nvisualisation tool that is a part of the Tensorﬂow framework.\n28\n6.2.1\nModel-free reinforcement learning\nImplementation started with the standard Deep Q-Network code following the\noriginal paper (Mnih et al. 2015). The initial structure consisted of 2 hidden lay-\ners with 24 nodes each to accustom for simpler environments. Although the most\npopular activation function for hidden layers of deep neural networks is rectiﬁer\nlinear unit (ReLU) (f(x) = max(0, x)) introduced by Nair & Hinton (2010), from\nmy experience it fairly often causes ’dead neurons’ problem, i.e. input to the ReLU\nis negative (thus output is a constant 0) causing lack of gradient ﬂowing through\nthe node during the backpropagation. Therefore, from the beginning, DQN was\nemploying leaky rectiﬁer linear units (f(x) = max(αx, x)) with default parameter\nαlrelu = 0.2 (Maas et al. 2013). Starting conﬁguration was using random mini-\nbatches of 32 experience samples from the replay buﬀer and Adam optimisation\ntechnique (Kingma & Ba 2014) to derive an optimal set of network weights.\nIt was then debugged and hypertuned until it was able to solve both, previously\nmentioned, environments easily. Most interesting changes performed during the\nhypertuning regarded the batch size and the optimisation technique. Initially, the\nagent was not able to even reach close-to-optimal policies. A single change from\nAdam optimiser to the standard stochastic gradient descent without any additional\nacceleration (SGD) was enough to ﬁx the issue. Deep reinforcement learning is\nknown to suﬀer from high instability. Momentum-based optimisation may often\nexacerbate this problem.\nAdditionally, SGD has been recently shown to have\nbetter generalisation properties (Wilson et al. 2017), what may be another reason\nfor its superiority in this setting. Following this change, the agent became able to\nreach optimal-like behaviour; however, it was quickly forgetting what it has learned\nand collapsing back to sub-optimal policies. The solution was to increase the batch\nsize, from 32 to 256 strongly. A batch made out of only 32 samples was often not\ndiverse enough to represent the actual signal coming from the environment, thus\nagain intensifying instabilities of deep RL.\nAfterwards, the state-of-the-art Rainbow DQN was written inheriting the DQN\nstructure and following improvements described by Hessel et al. (2017). It followed\nthe same debugging and hypertuning procedure. The ﬁnal Rainbow DQN hyper-\nparameters are presented in Table 1. The same Rainbow DQN was then used both\nas a baseline and within the model-free module of the GAIRL framework.\n29\nHyperparameter\nValue\nHidden layers\n[24, 24]\nHidden layers activation\nLeaky ReLU\nLeakiness parameter (αlrelu)\n0.2\nDropout probability\n0\nFinal layer activation\nLinear\nOptimiser\nGradient descent\nLearning rate (αlr)\n5 × 10−3\nGradient clipping\n1\nDiscount factor (γ)\n0.99\nExploration strategy\nϵ-greedy\nExploration ϵ decay\n1 →0.05 (linear)\nExploration decay start\n1000 steps\nExploration decay length\n10,000 steps\nReplay buﬀer size\n10,000\nReplay batch size\n256\nPrioritisation ϵ\n1 × 10−5\nPrioritisation α\n0.6\nPrioritisation β decay\n0.4 →1 (linear)\nPrioritisation decay length\n50,000\nNoisy networks δ0\n0.5\nMulti-step returns n\n3\nOnline network update frequency\n4\nTarget network update frequency\n500\nTable 1: Final parameters of the Rainbow DQN agent.\n6.2.2\nGenerative models\nFollowing the successful implementation, debugging, and hypertuning reinforce-\nment learning agents, implementation of the second group of necessary algo-\nrithms – generative models – has started.\nAll of the neural networks for the\ngenerative models were also initially conﬁgured to use Leaky ReLU activation for\nthe hidden layers and Adam optimisation for training.\nIn the ﬁrst step, I implemented standard GAN based on the original work by\nGoodfellow et al. (2014). Similarly to the DQN, it was then debugged and hy-\npertuned; however, this time using standard MNIST dataset (LeCun et al. 1999).\nAt ﬁrst, both generator and discriminator seemed like they entirely lack gradient\n30\nto progress, yet none of the well known GAN issues occurred. Interestingly, the\nproblem did not lay purely in standard hyperparameters of the networks but in the\nweights initialisation. Originally, GAN’s weights were initialised according to the\nsame distribution as weights of RL algorithms. In DQN, weights have to be ini-\ntialised to values with a mean µ = 0 and a very low standard distribution to avoid\ndivergence and exploding gradients that can be caused by recursive updates. For\nGANs, however, these values were too low to produce any meaningful signal. In-\ncreasing the standard deviation of initial weights distribution solved the diﬃculty.\nUnfortunately, networks imbalance problem that was described in detail in section\n2.3 followed. Discriminator started to completely overwhelm generator causing\nthe gradient of both networks to vanish. The solution to that was decreasing the\nnumber of discriminator training steps for each generator training step from k = 5\nto k = 1.\nAfter the original GAN was implemented, both Wasserstein GAN (WGAN) and\nWasserstein GAN with gradient penalty (WGANGP) were built on top. Because\ntuning weight clipping constant c in WGAN can be extremely hard and mundane,\nafter making sure there are no bugs in the code, focus moved to the WGANGP im-\nplementation, without fully hypertuning the standard WGAN. Sadly, the WGANGP\ndid not perfectly work on the dataset with the same hyperparameters as the stan-\ndard GAN. The reason behind it was that, unlike the discriminator in the standard\nGAN, the critic in the WGANGP has to be much more powerful than the gener-\nator so it can fully converge to the real value of the Wasserstein distance between\nthe real and the generated data. Slightly increasing the network size of the critic\nand the number of critic steps per single generator step from k = 1 to k = 10\ncaused the algorithm to perform much better. Nevertheless, it was still suﬀering\nfrom high variance and problematic convergence. As hypothesised and proved by\nthe next runs of the model, this was caused by too high learning rate in both of\nthe networks.\nFurthermore, after implementation of the GAN, WGAN, and the WGANGP based\non the original papers, they were expanded to allow for a conditional generation as\nproposed by Mirza & Osindero (2014). Conditional versions of these algorithms did\nnot need any additional hypertuning to learn probabilistic distribution underlying\nthe MNIST dataset adequately.\nAs described in section 5, an L1/L2 loss based model is also necessary for the\nexpected reward generation. Therefore, in addition to GANs, a standard multilayer\nperceptron (MLP), optimising for the mean absolute error (L1 loss), has been\nimplemented. L1 objective, instead of the most popular mean squared error (L2),\n31\nhas been chosen because it had been shown to work better for generative tasks\n(Zhao et al. 2017). The MLP was also initially debugged and hypertuned to ﬁt\nthe MNIST dataset. The comparison between implemented WGANGP and MLP\non the MNIST dataset can be seen in ﬁgure Figure 2. It is a great example of the\nsuperiority of GANs over traditional models in high-dimensional generative tasks.\nWhat is also crucial, the property that all generative models followed equally,\nboth for the MNIST dataset and as a part of the imaginative framework, was\nnormalisation. Namely, all values from the data, both inputs and outputs, were\nscaled to ﬁt the range [0; 1]. It further optimised their performance but also made\nit easier to interpret results. The fact that the output space is a range [0; 1] is\nleveraged by employed evaluation metrics as described in section 6.3.\nFinal hyperparameters of models that were optimised for the MNIST can be found\nin Appendix C. Nonetheless, they are not important for the ﬁnal results described\nin section 7.\n6.2.3\nGAIRL\nOnce generative algorithms and the model-free RL agent had been implemented,\nputting together the GAIRL agent begun. As explained in sections 4 and 5, the\nmodel-free module consists of the RL agent, and two generative models take the\nplace of the imagination.\nIn place of the MFM, the ﬁnal version of Rainbow DQN described in section\n6.2.1 is used. Precisely the same network structure and hyperparameters were\nemployed for two reasons: Firstly, to entirely make sure that experimental results\nshow actual merit of the GAIRL method, and are not merely caused by varying\nhyperparameters between the algorithms. Secondly, to test the promise of GAIRL\nto work as a universal sample eﬃciency enhancement in a very modular, plug\nand play manner, that does not require any special tuning of the RL algorithms\nemployed within the MFM.\nAlthough one of the central premises of the research presented in this thesis is the\nadvantage of GANs for the next state generation part of the imagination module,\nit was plausible that on chosen, less dimensional benchmarks they may actually\nperform worse than the standard L1/L2 loss approaches due to their higher level\nof complexity. Therefore, two diﬀerent versions of the IM were implemented: an\nMLP-based imagination and a WGANGP-based imagination.\nThe MLP-based\nversion uses an MLP model for both next state and reward generation. Whereas\n32\nWGANGP-based sticks to the standard GAIRL premise of using a GAN for the\nnext state and an MLP for the reward. Also, this decision rendered a good com-\nparison between Generative Adversarial Imaginative Reinforcement Learning and\npotentially simpler version of the framework. For both versions, the output of the\nMLP used for the reward generation is rounded to the closest integer (to accustom\nfor discrete rewards in the experimental environments).\nHyperparameter\nValue\nHidden layers\n[512, 512]\nHidden layers activation\nLeaky ReLU\nLeakiness parameter (αlrelu)\n0.2\nDropout probability\n0\nFinal layer activation [generator]\nTanh\nFinal layer activation [critic]\nLinear\nPenalty coeﬃcient (λ)\n10\nCritic steps per one generator step (k)\n10\nOptimiser\nAdam\nLearning rate (αlr)\n2 × 10−4\nFirst Adam decay rate (β1)\n0.5\nSecond Adam decay rate (β2)\n0.9\nTable 2: Final parameters of the state-generating WGANGP for the WGANGP-\nbased imagination.\nAs described in section 2.3, an original GAN could be more computationally eﬃ-\ncient than the WGANGP. However, a hard deadline on the thesis moved the focus\non minimising the time necessary to tune hyperparameters, instead of minimising\nthe computational eﬃciency.\nWhat is more, both environments presented in this study have deterministic dy-\nnamics, and so generative models do not require any inherent stochasticity. There-\nfore the random noise input to the WGANGP was omitted. All of the hyperpa-\nrameters, both for WGANGP and MLPs, had to also be customised to the new\nsetting. Tables 2 and 3 show ﬁnal hyperparameter choices for all of the generative\nmodels used for the ﬁnal evaluation.\nIn practice, the imaginative framework also maintains two separate memory mod-\nules: one representing a training set, the other a test set. It is vital to analyse\nthe capabilities of the imagination module thoroughly. The memory used for ex-\nperiments consisted of 200,000 most recent samples from the environment, 80% of\n33\nHyperparameter\nValue\nHidden layers\n[512, 512]\nHidden layers activation\nLeaky ReLU\nLeakiness parameter (αlrelu)\n0.2\nDropout probability\n0\nFinal layer activation\nLinear\nOptimiser\nAdam\nLearning rate (αlr)\n2 × 10−4\nFirst Adam decay rate (β1)\n0.9\nSecond Adam decay rate (β2)\n0.999\n(a) MLP used for the state generation in the\nMLP-based imagination.\nHyperparameter\nValue\nHidden layers\n[64, 64]\nHidden layers activation\nLeaky ReLU\nLeakiness parameter (αlrelu)\n0.2\nDropout probability\n0.25\nFinal layer activation\nLinear\nOptimiser\nAdam\nLearning rate (αlr)\n2 × 10−4\nFirst Adam decay rate (β1)\n0.9\nSecond Adam decay rate (β2)\n0.999\n(b) MLP used for the reward generation in both\nMLP-based and WGANGP-based imagination.\nTable 3: Final parameters of multilayer perceptrons used for the imaginative framework.\nwhich was used for training and 20% was separated purely for assessing the IM. A\ncritical characteristic of the memory was that it artiﬁcially increased the number\nof experience samples that result in terminal states by oversampling (Ling & Li\n1998). It was necessary due to a massive imbalance of terminal/non-terminal states\nin almost every RL setting. To do so, GAIRL keeps track of the mean length of\nepisodes µep. Once the episode ﬁnishes (a transition sample with a terminal state\noccurs), the obtained sample is replicated ⌈µep⌋many times.\nAs per algorithm 1, three training phases, MFP, ITP, and IBP continue in a\nloop starting from the model-free phase.\nMFP was initially set to operate for\nρ1 = 20,000 steps in the real environment, ITP for ρ2 = 20,000 stochastic gradient\ndescent imagination training steps, and IBP for ρ3 = 60,000 steps in the imag-\ninative environment. However, initial experiments showed that longer ITP with\nρ2 = 40,000 steps better generalises on both environments and so the hyperpa-\nrameter has been changed for the ﬁnal version of the algorithm.\n6.3\nEvaluation metrics\nDiﬀerent metrics have been used to test diﬀerent aspects of the algorithms. Pre-\ncision (15) and recall (16) is employed to assess the performance of the reward\ngeneration (normalised reward in the chosen environment can be either 0 or 1).\nPrecision describes a ratio of true positives (correctly generated 1s) within all gen-\nerated positives (all generated 1s, no matter if correctly or not). Recall deﬁnes\n34\nhow many 1s were correctly generated out of all true 1s.\nThey provide much\nmore information about the real performance of the machine learning model than\na standard accuracy metric, especially in situations of high-class imbalance that\ntakes place in chosen environments (reward 0 is extremely more common than 1).\nprecision =\ntrue positives\ntrue positives + false positives\n(15)\nrecall =\ntrue positives\ntrue positives + false negatives\n(16)\nState, on the other hand, follows fully continuous dynamics.\nTherefore, mean\nabsolute error (MAE) (17) is used for state-generating models. MAE is simply an\naveraged L1 loss that calculates a mean absolute diﬀerence between a generated\nstate and a ground truth state. It has been chosen over more popular mean squared\nerror for the same reasons L1 loss has been chosen over L2 loss (see section 6.2.2).\nGiven that all outputs are normalised between 0 and 1, the value of 1 −MAE can\nalso be referred to as accuracy.\nMAE = 1\nn\nn\nX\ni=1\n|xgenerated −xtrue|\n(17)\nFinally, to evaluate data eﬃciency of algorithms (the main focus of this thesis),\nsimply a mean reward from the 100 most recent episodes, in regards to the number\nof steps performed in the real environment, is used.\n7\nResults\nFirst, imagination capabilities to accurately approximate real environment dynam-\nics are presented. Then, focus moves onto data eﬃciency of diﬀerent variations\nof the imaginative framework and the state-of-the-art Rainbow DQN. Further-\nmore, computational eﬃciency analysis of the proposed framework can be found\nin Appendix B. All experiments were performed using algorithms and parameters\ndescribed in section 6.2, the same for both environments. Each of the showcased\nresults is based on 15 independent runs of the algorithm. In all of the plots, lines\nrepresent a mean value for the runs. Opaque areas represent a standard deviation\naround the mean.\n35\n7.1\nImagination performance\n7.1.1\nReward generation\nFigure 11 shows the performance of the reward imagination submodule in terms of\nprecision and recall in both environments. Both metrics, besides recall of Acrobot\nreward that requires twice as much experience, converge after 120,000 steps. For\nMountainCar, as it is a simple environment, the MLP can easily reach over 99%\nrecall and precision in a few GAIRL iterations. Acrobot is a bit more challenging\nand even converged imagination’s precision can drop below 99%. Although results\nof this magnitude are suﬃciently accurate, they could potentially be improved if\nmore time was spent on hypertuning the machine learning model.\n(a) Recall on the test memory.\n(b) Precision on the test memory.\nFigure 11: Performance of the reward-generating MLP imagination submodule on\nexperimental environments. The x-axis represents training steps performed solely\nin the ITP (in other phases the performance stays constant). Y-axis shows the\nvalue of an appropriate metric.\nWhat can be intriguing, are big jumps about every 60,000 steps in the perfor-\nmance of the network as the training progresses. They are caused by the the main\nGAIRL algorithm loop. For every GAIRL iteration, agent gathers 20,000 samples\nof experience from the real environment (ρ1 = 20,000) to then train the imagina-\ntion for 60,000 gradient descent steps (ρ2 = 60,000). Therefore, each multiple of\n60,000 marks a point after which more real data enters the process helping the\nmodel to better generalise to unseen samples. I.e. in the ﬁrst 60,000 steps imag-\nination learns only based on 16,000 samples (80% of 20,000 because another 20%\n36\nbelongs to the test memory), in the 60,000 −120,000 period based on 32,000, in\nthe 12,000 −180,000 based on 48,000, and so on.\n7.1.2\nState generation\n(a) Wasserstein loss for both Acrobot and\nMountainCar (WGANGP only).\n(b) State generation MAE for MountainCar.\n(c) State generation MAE for Acrobot.\nFigure 12: Performance of the state-generating imagination submodule on experi-\nmental environments. The x-axis represents training steps performed solely in the\nITP (in other phases the performance stays constant). The y-axis shows the value\nof an appropriate metric.\nThe eﬀectiveness of the state imagination submodule is presented in Figure 12.\nBoth Figure 12b and 12c show MAE for both WGANGP and MLP state genera-\n37\ntion. Figure 12a, on the other hand, shows Wasserstein distance as approximated\nby the critic for WGANGP imagination only.\nWe can see that WGANGP performs worse than the MLP in terms of MAE. This\nis, however, expected behaviour. MLP optimises for the L1 loss, that is in essence\nMAE multiplied by a constant, directly. GANs superiority lays in the fact that\nthey do not optimise towards minimising mean error on the individual level, but\ntowards minimising the diﬀerence between data distributions. These plots only\nshowcase that indeed both of the modules seem to model dynamics of the real\ndistribution accurately. Using them for direct comparison of the models would be\nunfair for the WGANGP.\nMLP reaches over 99.9% accuracy on MountainCar and 99.5% accuracy on Ac-\nrobot, even without much of hypertuning.\nUnlike with the reward, very high\naccuracy of state generation is crucial because the correctness of future states\nhighly depends on the correctness of previous states. When making rollouts into\nthe future using pure imagination, errors may compound. Given accuracy a of the\nstate generation model and rollout of length τ, ﬁnal state’s accuracy may drop, in\nthe worst case, to aτ (nevertheless, this is also unlikely).\nEven WGANGP that does not optimise for MAE reaches good results of over 97%\naccuracy for both environments proving its convergence properties. This is also\nshown by the estimated Wasserstein distance between the generator distribution\nand the environment distribution that reaches values lower than 0.015 for both\nenvironments (even below 0.005 for MountainCar).\nThese results show a high promise of replacing the real environment with the\nimagination, especially considering the fact that not much time and resources have\nbeen spent on optimising the architecture and parameters used in the experiments.\n7.2\nData eﬃciency\nData eﬃciency is the main problem targeted in this study.\nResults in section\n7.1 show that deep learning models can eﬃciently learn the dynamics of the real\nenvironment. The remaining question is whether these models are accurate enough\nto replace the real environment in the RL process.\nFigure 13 presents results of both MLP-based and WGANGP-based imaginative\nframework in comparison to the imagination-free state-of-the-art algorithm. Both\nimaginative algorithms highly outperform imagination-free Rainbow DQN being\n38\nmore than twice as data eﬃcient on both environments.\nGAIRL requires even as much as over three times fewer experience samples than\nthe framework with MLP-based imagination, and over 6 times less than the current\nstate-of-the-art on the more complex Acrobot environment. Surprisingly, despite\nthe simplicity of MountainCar setting, GAIRL also solves it most optimally.\nWhat is also important, one of the concerns was that imagination-aided agent\nwould be much less stable than the standard model-free algorithm. It is indeed\nthe case on the simple MountainCar, especially for WGANGP-based framework.\nRemarkably, once the complexity of the environment increases (Acrobot), it is no\nlonger the case.\n(a) Results on MountainCar\n(b) Results on Acrobot\nFigure 13: Performance of the MLP-based and WGANGP-based framework com-\npared to the state-of-the-art baseline. The x-axis represents a size of experience\nfrom the real environment. The y-axis represents a mean reward from past 100\nepisodes. Black dashed line represents the top performance achieved by the state-\nof-the-art.\nIn addition, two-tailed Wilcoxon signed rank tests were performed to statistically\ncompare obtained results.\nThere were N = 15 non-zero diﬀerence samples in\nevery comparison. The critical value to make sure that the results are statistically\nsigniﬁcant (p ≤0.05) for N = 15 samples is ωc = 25\nFirstly, the comparison between the imagination-free state of the art, and the\nmlp-based imaginative algorithm was made. For the MountainCar environment,\nWilcoxon test produced rank sums T +\nm1 = 114 and T −\nm1 = 6. Clearly, ωm1 = T −\nm1 <\nωc. Similarly, in the Acrobot environment rank sums T +\na1 = 111 and T −\na1 = 9 were\n39\ncollected. Again, ωa1 = T −\na1 < ωc. Therefore, the null hypothesis stating that the\nimaginative framework does not improve sample eﬃciency of the state of the art\ncan be safely rejected.\nThen, the focus moved to the analysis of GAIRL beneﬁts over the more primitive\nMLP-based imaginative framework. Rank sums for the MountainCar environment\nin this scenario are T +\nm2 = 82 and T −\nm2 = 38. This time, ωm2 > ωc. Nevertheless,\nfor the Acrobot, T +\na2 = 102 and T −\na2 = 18, so ωa2 < ωc. Thus, the superiority\nof GANs in the generative framework is statistically signiﬁcant only for the more\ncomplicated environment. This result was somewhat expected; the central promise\nof GANs is to work much better on complex and high dimensional domains. The\nMLP should not be much worse in an elementary environment like MountainCar.\nFull table with the data that was used to calculate test statistics is available in\nAppendix A.\n8\nDiscussion\nTwo research questions have been posed at the beginning of this thesis:\n• Can learning the imaginative model of the environment be more eﬃcient\nthan learning an optimal policy?\n• If so, can the learned imagination fulﬁl the promise of sample eﬃcient model-\nbased RL in settings where dynamics of the real environment are unknown?\nBoth of the answers have been subsequently answered: Results presented in sec-\ntion 7 clearly show that simply exploiting Markov property allows imagination to\nconverge with almost order of magnitude less data than it is required for learning\nan optimal policy. Additionally, the imagination was later successfully used to\nimprove on data eﬃciency of the state-of-the-art Rainbow DQN algorithm.\nInitially, the thesis also hypothesised that the GAIRL framework presented through-\nout the thesis could produce a positive answer to both of the questions. Although\nin the end it did, one part of the hypothesis has been shown to be redundant in\nterms of simply answering these questions. Namely, generative adversarial archi-\ntecture is not a necessity. More traditional models like multilayer perceptron can\nbe successfully deployed within the imaginative framework as well. At least in sim-\nple environments used for the evaluation. Nevertheless, although not necessary,\nGANs tend to produce better results and should scale better to more complex\n40\nsettings.\nTwo main novel contributions of this thesis would be:\n• Eﬃcient learning of the environment dynamics (creating imagination) by\nleveraging Markov property and advantages of generative adversarial net-\nworks.\n• Successful use of imagination to highly improve state-of-the-art data eﬃ-\nciency of deep reinforcement learning through Dyna-Q-inspired algorithm.\nUnfortunately, a month before the ﬁnal submission deadline, Kaiser et al. (2019)\nintroduced SimPLe – a novel deep RL algorithm that follows similar principles to\nthe GAIRL framework. It was also inspired by the Dyna-Q algorithm and pro-\nduced substantially more data eﬃcient results than the Rainbow DQN. Therefore,\nit strips the novelty out of the second contribution of this thesis. However, be-\ncause SimPLe leverages traditional L1/L2 loss model proposed by Oh et al. (2015)\nwith only a few minor modiﬁcations, it often suﬀers from inaccurate generation\n(blurry images, disappearing small but crucial features). GAIRL’s approach of\nusing GANs promises to circumvent this diﬃculty. However, more benchmarks in\nmore complex environments should be performed to prove this hypothesis fully.\nThis brings us to the point in the next paragraph.\nGAIRL seems to overperform most recent state-of-the-art and even introduce cru-\ncial contribution on top of the similar advancement that was produced by the\ntop research institution in parallel. Nevertheless, the set of environments used for\ntesting was limited due to strict time constraints. The framework needs to be\nevaluated on a higher variety of domains. Each of them should reﬂect one or more\nof the following properties: a very high dimensional state space, non-deterministic\ntransition dynamics, or a partially observable Markov decision process (POMDP).\nGANs architecture promises to handle the ﬁrst two smoothly, theoretically am-\nplifying the gap between GAIRL and other approaches. However, GAIRL may\nperform worse in environments following the POMDP properties. Additionally, it\nshould be empirically compared with the recently introduced SimPLe algorithm.\nFurther experiments are the critical next step and are planned to continue after\nthe submission of this thesis.\nWhat is more, more detailed optimisation of GAIRL architecture has been left for\nfuture work. Not much focus has been given to hyperparameters. It would be\ninteresting to see a thorough study of the best parameter choices for the GAIRL\nsetting.\nPotentially, sample eﬃciency could have been even more signiﬁcantly\nimproved by decreasing the length of the model-free phase and compensating it\n41\nwith the agent’s training in the imagination-based phase. Additionally, reward\nand state generation should be combined within a single machine learning model.\nIt is especially essential for the reward generation so it can utilise the beneﬁts of\nGANs in stochastic environments.\nAnother promising direction is to modify GAIRL’s memory to follow a similar\nstructure to the prioritised replay buﬀer (Schaul et al. 2015) that is used in the\nRainbow DQN. It could lead to intriguing results. Moreover, current use of Wasser-\nstein GAN could be utilised to guide the exploration-exploitation trade-oﬀ. As\nAzizzadenesheli et al. (2018) mentioned, high Wasserstein distance produced by\nthe critic for certain states could indicate that the agent is unsure of the possible\noutcomes of such a state. Therefore, the agent should potentially move there to\nexplore the search space better, even if it seems less optimal.\nFinally, the length of the imagination training phase ρ2 could become adaptive.\nCurrently, it takes a ﬁxed constant.\nHowever, imagination does not need the\nsame amount of training time at every iteration. The ﬁrst iteration should take\nthe longest, as the imagination starts without any prior knowledge.\nHowever,\nduring the second iteration, it already has a general overview of the world. It\njust needs to update its model slightly to capture newly gathered data-points.\nConsequently, by the law of large numbers, it could follow that if n indicates the\nnumber of iterations then limn→inf ρ2 = 0, potentially saving a substantial amount\nof computational power.\nOne of the main issues raised during the project demonstration session was that the\npresented comparison of GAIRL with Rainbow DQN is not fair towards the latter.\nThe argument was that the GAIRL framework performs a considerable amount of\nadditional computation in the background. The fact that GAIRL requires more\ncomputation is true. Although it has the same time complexity in terms of the\nbig O notation, it performs slower and is less computationally eﬃcient as shown in\nAppendix B. Naturally, using the real environment directly instead of its imperfect\nimagination allows the agent to ﬁnd the optimal policy quicker, even without\nmentioning the computational power required to learn the imagination.\nNonetheless, data eﬃciency, not computational eﬃciency, is one of the most press-\ning problems in the ﬁeld (Irpan 2018), and the focus of this thesis. Computational\npower is cheap and widely accessible. Performing millions of random trial and\nerror actions in the real environment, however, is often either very expensive or\neven impossible as explained throughout the thesis.\nFurthermore, this type of\ncomparison is not new to the ﬁeld. It has been used repeatedly in the literature\n(Mnih et al. 2015, Silver et al. 2016, 2017, Hessel et al. 2017, Schulman et al. 2017,\n42\nKaiser et al. 2019)\nAnother argument was that the imaginative module would not be able to grasp the\ndynamics of complex environments. Example of the real world in case of robotics\nwas given. It is a reasonable concern. We cannot know for sure before performing\nappropriate experiments in such an environment as already mentioned earlier.\nHowever, the imagination module requires the same assumptions and the same\nsetting as the current state-of-the-art RL algorithms. In theory, if imagination\ncannot encapsulate the real environment, then the state-of-the-art is not able to\nlearn a close-to-optimal policy either. Notwithstanding, this question is open for\nfuture experimental work.\nThe last point referred to employed benchmark environments. Namely, what is\nthe point of learning the imaginative simulation for the environment that already\nis a simulation of a car/robot? Undeniably, it does not make sense in practice.\nHowever, the goal of this research was not to ﬁnd the best solution to the Ac-\nrobot or MountainCar. It was about improving the general-use state-of-the-art\nreinforcement learning. Simulations were used merely as a mean to compare the\ncapabilities of presented RL algorithms. Optimal or close-to-optimal solutions to\nthese environments, much better than any RL algorithm, has been devised decades\nago. It did not stop, however, RL researchers to use them for benchmark purposes\non conferences such as NeurIPS or ICML.\n9\nConclusion\nThe goal of this thesis was to improve data eﬃciency of the state-of-the-art rein-\nforcement learning. This has been successfully achieved by introducing an imagi-\nnative framework that can accurately and eﬃciently approximate dynamics of the\nreal environment by making use of Markov property and generative adversarial\nmodels.\nIt presented experimental evidence that supports the superiority of GANs over\nstandard generative models for conditional state prediction within the reinforce-\nment learning setting. It also introduced a way to utilise imperfect approximation\nof the real world to limit the amount of data needed to train an optimally behaving\nagent.\nSimilar advancement regarding sample-eﬃcient reinforcement learning has been\nreleased in parallel (Kaiser et al. 2019). The study presented here, however, not\n43\nonly conﬁrms the results obtained by Kaiser et al. (2019) but also adds an impor-\ntant contribution to the ﬁeld that promises to improve the state-of-the-art even\nfurther.\nNevertheless, more experiments are needed to ensure the superiority of the intro-\nduced framework, and there is still room open for future work.\nBibliography\nAbadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,\nG. S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A.,\nIrving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg,\nJ., Man´e, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens,\nJ., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan,\nV., Vi´egas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y. &\nZheng, X. (2015), ‘TensorFlow: Large-scale machine learning on heterogeneous\nsystems’. Software available from tensorﬂow.org.\nURL: http://tensorﬂow.org/\nArjovsky, M., Chintala, S. & Bottou, L. (2017), Wasserstein generative adversarial\nnetworks, in ‘International Conference on Machine Learning’, pp. 214–223.\nArulkumaran, K., Deisenroth, M. P., Brundage, M. & Bharath, A. A. (2017), ‘A\nbrief survey of deep reinforcement learning’, arXiv preprint arXiv:1708.05866 .\nAzizzadenesheli, K., Yang, B., Liu, W., Brunskill, E., Lipton, Z. & Anandkumar,\nA. (2018), ‘Surprising negative results for generative adversarial tree search’.\nBarth-Maron, G., Hoﬀman, M. W., Budden, D., Dabney, W., Horgan, D., Muldal,\nA., Heess, N. & Lillicrap, T. (2018), ‘Distributed distributional deterministic\npolicy gradients’, arXiv preprint arXiv:1804.08617 .\nBellemare, M. G., Dabney, W. & Munos, R. (2017), A distributional perspective\non reinforcement learning, in ‘International Conference on Machine Learning’,\npp. 449–458.\nBellemare, M. G., Naddaf, Y., Veness, J. & Bowling, M. (2013), ‘The arcade\nlearning environment: An evaluation platform for general agents’, Journal of\nArtiﬁcial Intelligence Research 47, 253–279.\n44\nBellemare, M., Veness, J. & Bowling, M. (2013), Bayesian learning of recur-\nsively factored environments, in ‘International Conference on Machine Learn-\ning’, pp. 1211–1219.\nBrockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J.\n& Zaremba, W. (2016), ‘Openai gym’.\nBrowne, C. B., Powley, E., Whitehouse, D., Lucas, S. M., Cowling, P. I., Rohlfsha-\ngen, P., Tavener, S., Perez, D., Samothrakis, S. & Colton, S. (2012), ‘A survey\nof monte carlo tree search methods’, IEEE Transactions on Computational In-\ntelligence and AI in games 4(1), 1–43.\nChiappa, S., Racaniere, S., Wierstra, D. & Mohamed, S. (2017), ‘Recurrent envi-\nronment simulators’, arXiv preprint arXiv:1704.02254 .\nCoppersmith, D. & Winograd, S. (1990), ‘Matrix multiplication via arithmetic\nprogressions’, Journal of symbolic computation 9(3), 251–280.\nDoan, T., Mazoure, B. & Lyle, C. (2018), ‘Gan q-learning’, arXiv preprint\narXiv:1805.04874 .\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,\nCourville, A. & Bengio, Y. (2014), Generative adversarial nets, in ‘Advances in\nneural information processing systems’, pp. 2672–2680.\nGu, S., Lillicrap, T., Sutskever, I. & Levine, S. (2016), Continuous deep q-learning\nwith model-based acceleration, in ‘International Conference on Machine Learn-\ning’, pp. 2829–2838.\nGulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V. & Courville, A. C. (2017),\nImproved training of wasserstein gans, in I. Guyon, U. V. Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan & R. Garnett, eds, ‘Advances in Neural\nInformation Processing Systems 30’, Curran Associates, Inc., pp. 5767–5777.\nURL:\nhttp://papers.nips.cc/paper/7159-improved-training-of-wasserstein-\ngans.pdf\nHassabis, D., Kumaran, D., Vann, S. D. & Maguire, E. A. (2007), ‘Patients with\nhippocampal amnesia cannot imagine new experiences’, Proceedings of the Na-\ntional Academy of Sciences 104(5), 1726–1731.\nHenderson, P., Chang, W.-D., Bacon, P.-L., Meger, D., Pineau, J. & Precup,\nD. (2018), Optiongan: Learning joint reward-policy options using generative\nadversarial inverse reinforcement learning.\n45\nHessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W.,\nHorgan, D., Piot, B., Azar, M. & Silver, D. (2017), ‘Rainbow: Combining im-\nprovements in deep reinforcement learning’, arXiv preprint arXiv:1710.02298\n.\nHo, J. & Ermon, S. (2016), Generative adversarial imitation learning, in ‘Advances\nin Neural Information Processing Systems’, pp. 4565–4573.\nHochreiter, S. & Schmidhuber, J. (1997), ‘Long short-term memory’, Neural com-\nputation 9(8), 1735–1780.\nIrpan, A. (2018), ‘Deep reinforcement learning doesn’t work yet’, https://www.\nalexirpan.com/2018/02/14/rl-hard.html.\nIsola, P., Zhu, J.-Y., Zhou, T. & Efros, A. A. (2017), Image-to-image translation\nwith conditional adversarial networks, in ‘2017 IEEE Conference on Computer\nVision and Pattern Recognition (CVPR)’, IEEE, pp. 5967–5976.\nKaiser, L., Babaeizadeh, M., Milos, P., Osinski, B., Campbell, R. H., Czechowski,\nK., Erhan, D., Finn, C., Kozakowski, P., Levine, S. et al. (2019), ‘Model-based\nreinforcement learning for atari’, arXiv preprint arXiv:1903.00374 .\nKingma, D. P. & Ba, J. (2014), ‘Adam: A method for stochastic optimization’,\narXiv preprint arXiv:1412.6980 .\nKingma, D. P. & Welling, M. (2013), ‘Auto-encoding variational bayes’, arXiv\npreprint arXiv:1312.6114 .\nLeCun, Y., Haﬀner, P., Bottou, L. & Bengio, Y. (1999), Object recognition with\ngradient-based learning, in ‘Shape, contour and grouping in computer vision’,\nSpringer, pp. 319–345.\nLeibfried, F., Kushman, N. & Hofmann, K. (2016), ‘A deep learning approach\nfor joint video frame and reward prediction in atari games’, arXiv preprint\narXiv:1611.07078 .\nLeinweber, M., Ward, D. R., Sobczak, J. M., Attinger, A. & Keller, G. B. (2017),\n‘A sensorimotor circuit in mouse cortex for visual ﬂow predictions’, Neuron\n95(6), 1420–1432.\nLenz, I., Knepper, R. A. & Saxena, A. (2015), Deepmpc: Learning deep latent\nfeatures for model predictive control., in ‘Robotics: Science and Systems’.\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D.\n46\n& Wierstra, D. (2015), ‘Continuous control with deep reinforcement learning’,\narXiv preprint arXiv:1509.02971 .\nLing, C. X. & Li, C. (1998), Data mining for direct marketing: Problems and\nsolutions., in ‘Kdd’, Vol. 98, pp. 73–79.\nLucic, M., Kurach, K., Michalski, M., Gelly, S. & Bousquet, O. (2018), Are gans\ncreated equal? a large-scale study, in ‘Advances in neural information processing\nsystems’, pp. 698–707.\nMaas, A. L., Hannun, A. Y. & Ng, A. Y. (2013), Rectiﬁer nonlinearities improve\nneural network acoustic models, in ‘Proc. icml’, Vol. 30, p. 3.\nMedsker, L. & Jain, L. C. (1999), Recurrent neural networks: design and applica-\ntions, CRC press.\nMirza, M. & Osindero, S. (2014), ‘Conditional generative adversarial nets’, arXiv\npreprint arXiv:1411.1784 .\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare,\nM. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G. et al.\n(2015), ‘Human-level control through deep reinforcement learning’, Nature\n518(7540), 529.\nMoore, A. W. (1990), ‘Eﬃcient memory-based learning for robot control’.\nNair, V. & Hinton, G. E. (2010), Rectiﬁed linear units improve restricted boltz-\nmann machines, in ‘Proceedings of the 27th international conference on machine\nlearning (ICML-10)’, pp. 807–814.\nNg, A. Y., Russell, S. J. et al. (2000), Algorithms for inverse reinforcement learn-\ning., in ‘Icml’, pp. 663–670.\nOh, J., Guo, X., Lee, H., Lewis, R. L. & Singh, S. (2015), Action-conditional\nvideo prediction using deep networks in atari games, in ‘Advances in neural\ninformation processing systems’, pp. 2863–2871.\nPeters, J., Vijayakumar, S. & Schaal, S. (2005), Natural actor-critic, in ‘European\nConference on Machine Learning’, Springer, pp. 280–291.\nPfau, D. & Vinyals, O. (2016), ‘Connecting generative adversarial networks and\nactor-critic methods’, arXiv preprint arXiv:1610.01945 .\nPfeiﬀer, B. E. & Foster, D. J. (2013), ‘Hippocampal place-cell sequences depict\nfuture paths to remembered goals’, Nature 497(7447), 74.\n47\nRacani`ere, S., Weber, T., Reichert, D., Buesing, L., Guez, A., Rezende, D. J., Ba-\ndia, A. P., Vinyals, O., Heess, N., Li, Y. et al. (2017), Imagination-augmented\nagents for deep reinforcement learning, in ‘Advances in Neural Information Pro-\ncessing Systems’, pp. 5690–5701.\nSchaal, S., Ijspeert, A. & Billard, A. (2003), ‘Computational approaches to motor\nlearning by imitation’, Philosophical Transactions of the Royal Society of London\nB: Biological Sciences 358(1431), 537–547.\nSchacter, D. L., Addis, D. R., Hassabis, D., Martin, V. C., Spreng, R. N. &\nSzpunar, K. K. (2012), ‘The future of memory: remembering, imagining, and\nthe brain’, Neuron 76(4), 677–694.\nSchaul, T., Quan, J., Antonoglou, I. & Silver, D. (2015), ‘Prioritized experience\nreplay’, arXiv preprint arXiv:1511.05952 .\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A. & Klimov, O. (2017), ‘Proxi-\nmal policy optimization algorithms’, arXiv preprint arXiv:1707.06347 .\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G.,\nSchrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M. et al. (2016),\n‘Mastering the game of go with deep neural networks and tree search’, nature\n529(7587), 484.\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D. & Riedmiller, M. (2014),\nDeterministic policy gradient algorithms, in ‘ICML’.\nSilver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A.,\nHubert, T., Baker, L., Lai, M., Bolton, A. et al. (2017), ‘Mastering the game of\ngo without human knowledge’, Nature 550(7676), 354.\nSutton, R. S. (1990), Integrated architectures for learning, planning, and reacting\nbased on approximating dynamic programming, in ‘Machine Learning Proceed-\nings 1990’, Elsevier, pp. 216–224.\nSutton, R. S. (1996), Generalization in reinforcement learning: Successful exam-\nples using sparse coarse coding, in ‘Advances in neural information processing\nsystems’, pp. 1038–1044.\nSutton, R. S. & Barto, A. G. (1998), Introduction to reinforcement learning, Vol.\n135, MIT press Cambridge.\nTalvitie, E. (2014), Model regularization for stable sample rollouts., in ‘UAI’,\npp. 780–789.\n48\nTalvitie, E. (2015), Agnostic system identiﬁcation for monte carlo planning, in\n‘Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence’.\nTesauro, G. (1995), Td-gammon: A self-teaching backgammon program, in ‘Ap-\nplications of Neural Networks’, Springer, pp. 267–285.\nTolman, E. C. (1948), ‘Cognitive maps in rats and men.’, Psychological review\n55(4), 189.\nVan Hasselt, H. (2010), Double q-learning, in ‘Advances in Neural Information\nProcessing Systems’, pp. 2613–2621.\nVan Hasselt, H., Guez, A. & Silver, D. (2016), Deep reinforcement learning with\ndouble q-learning., in ‘AAAI’, Vol. 2, Phoenix, AZ, p. 5.\nVenkatraman, A., Capobianco, R., Pinto, L., Hebert, M., Nardi, D. & Bagnell,\nJ. A. (2016), Improved learning of dynamics models for control, in ‘International\nSymposium on Experimental Robotics’, Springer, pp. 703–713.\nVillani, C. (2008), Optimal transport: old and new, Vol. 338, Springer Science &\nBusiness Media.\nWang, Z., Schaul, T., Hessel, M., Van Hasselt, H., Lanctot, M. & De Freitas, N.\n(2015), ‘Dueling network architectures for deep reinforcement learning’, arXiv\npreprint arXiv:1511.06581 .\nWatkins, C. J. & Dayan, P. (1992), ‘Q-learning’, Machine learning 8(3-4), 279–292.\nWilliams, R. J. (1992), ‘Simple statistical gradient-following algorithms for con-\nnectionist reinforcement learning’, Machine learning 8(3-4), 229–256.\nWilson, A. C., Roelofs, R., Stern, M., Srebro, N. & Recht, B. (2017), The marginal\nvalue of adaptive gradient methods in machine learning, in ‘Advances in Neural\nInformation Processing Systems’, pp. 4148–4158.\nXiao, T. & Kesineni, G. (2016), ‘Generative adversarial networks for model based\nreinforcement learning with tree search’.\nZeiler, M. D., Krishnan, D., Taylor, G. W. & Fergus, R. (2010), ‘Deconvolutional\nnetworks’.\nZhao, H., Gallo, O., Frosio, I. & Kautz, J. (2017), ‘Loss functions for image\nrestoration with neural networks’, IEEE Transactions on Computational Imag-\ning 3(1), 47–57.\n49\nAppendix A\nNumerical results for data eﬃciency\nEnvironment\nRandom seed\nImagination-free\nMLP imagination\nWGANGP imagination\nMountainCar\n10\n490.1\n856.1\n292.6\nMountainCar\n50\n1314.4\n625.2\n255.7\nMountainCar\n100\n821.5\n397.3\n300.3\nMountainCar\n500\n514.6\n297.2\n325\nMountainCar\n1000\n970.2\n415.3\n395.8\nMountainCar\n5000\n510.8\n245.6\n134\nMountainCar\n10000\n950.5\n222.3\n317.8\nMountainCar\n50000\n516.3\n372.6\n336.1\nMountainCar\n100000\n1156\n390.9\n556.5\nMountainCar\n500000\n881.3\n531.5\n236\nMountainCar\n1000000\n726.1\n298.3\n217.1\nMountainCar\n5000000\n687.9\n207.7\n210.6\nMountainCar\n10000000\n791.7\n456.1\n411\nMountainCar\n50000000\n1095\n369\n335.5\nMountainCar\n100000000\n935.2\n377.2\n533.3\nAcrobot\n10\n641.2\n205.1\n71.2\nAcrobot\n50\n814.5\n74.3\n94.4\nAcrobot\n100\n328.4\n118.1\n71.4\nAcrobot\n500\n153.6\n152.7\n163.3\nAcrobot\n1000\n548.8\n75.8\n150\nAcrobot\n5000\n207.5\n96.3\n119.9\nAcrobot\n10000\n254.7\n87\n65.1\nAcrobot\n50000\n601\n317.6\n132.1\nAcrobot\n100000\n176.2\n359.7\n100\nAcrobot\n500000\n653.2\n230.2\n151.6\nAcrobot\n1000000\n285.9\n291.1\n124.3\nAcrobot\n5000000\n399.7\n117.9\n101.3\nAcrobot\n10000000\n397.2\n199.3\n74.1\nAcrobot\n50000000\n318.1\n172\n86.8\nAcrobot\n100000000\n324.1\n139.7\n68.8\nTable 4: Algorithms comparison. Runs are grouped by the environment and used random seed.\nValues in the columns represent the number of samples from the real environment needed before\nconvergence.\n50\nAppendix B\nComputational eﬃciency analysis\nTable 5 presents a comparison of the algorithms in regards to the time they required\nuntil convergence on a 4CPU instance. For both Acrobot and MountainCar, imag-\ninative framework employing MLP for the imagination module requires approxi-\nmately 7 times more time to converge to the optimal policy than the imagination-\nfree agent. Full GAIRL algorithms run approximately 2.5 times longer than the\nMLP-based agent and 17 times longer than the imagination-free agent.\nIt clearly shows that the state-of-the-art is much more computationally eﬃcient\nthan any of the proposed algorithms. Combining it with the results from section 7,\nwe can observe that there is an inverse proportional relation between data eﬃciency\nand computational eﬃciency.\nHowever, GAIRL performance should scale better with higher computational power.\nThe most demanding part of GAIRL is the imagination training phase. The ITP\nemploys a standard supervised learning process that can easily leverage high paral-\nlelism provided by GPUs and TPUs. Highly sequential reinforcement learning, on\nthe other hand, is much harder to parallelise. Naturally, GAIRL can never reach\nthe same eﬃciency as imagination-free options; however, it may get asymptotically\ncloser when more powerful hardware is provided.\n51\nEnvironment\nRandom seed\nWGANGP imagination\nMLP imagination\nImagination-free\nMountainCar\n10\n464.534\n298.221\n49.078\nMountainCar\n50\n822.961\n304.695\n55.173\nMountainCar\n100\n651.695\n212.879\n29.919\nMountainCar\n500\n1047.146\n414.624\n31.019\nMountainCar\n1000\n334.610\n232.937\n46.608\nMountainCar\n5000\n652.493\n340.321\n33.857\nMountainCar\n10000\n333.088\n246.786\n35.686\nMountainCar\n50000\n285.082\n133.015\n42.096\nMountainCar\n100000\n906.235\n375.812\n32.433\nMountainCar\n500000\n761.129\n111.714\n59.211\nMountainCar\n1000000\n615.542\n286.956\n31.195\nMountainCar\n5000000\n741.330\n381.619\n42.303\nMountainCar\n10000000\n936.039\n152.674\n41.251\nMountainCar\n50000000\n695.203\n166.715\n36.890\nMountainCar\n100000000\n603.739\n225.416\n35.841\nMean\n–\n656.722\n258.959\n40.171\nAcrobot\n10\n119.521\n138.605\n13.962\nAcrobot\n50\n104.571\n84.227\n41.711\nAcrobot\n100\n312.638\n216.284\n20.661\nAcrobot\n500\n536.062\n192.328\n8.235\nAcrobot\n1000\n334.610\n127.010\n26.104\nAcrobot\n5000\n485.868\n96.694\n10.359\nAcrobot\n10000\n333.088\n209.477\n22.162\nAcrobot\n50000\n152.203\n70.388\n10.528\nAcrobot\n100000\n259.356\n185.065\n30.179\nAcrobot\n500000\n605.006\n28.556\n19.241\nAcrobot\n1000000\n262.557\n125.856\n12.956\nAcrobot\n5000000\n116.214\n171.756\n11.067\nAcrobot\n10000000\n335.291\n143.612\n14.851\nAcrobot\n50000000\n224.329\n91.233\n26.151\nAcrobot\n100000000\n313.412\n185.561\n19.451\nMean\n–\n335.155\n137.777\n19.175\nTable 5: Algorithms comparison. Runs are grouped by the environment and used random\nseed. Values in the columns represent the number of minutes that passed before convergence.\nExperiments run on Google Cloud Platform, each on the instance with 4 Intel Skylake CPUs\nand 8GB of RAM.\n52\nAppendix C\nGenerative hyperparameters for MNIST\nHyperparameter\nValue\nHidden layers\n[256, 512, 1024]\nHidden layers activation\nLeaky ReLU\nLeakiness parameter (αlrelu)\n0.2\nDropout probability\n0\nFinal layer activation\nTanh\nOptimiser\nAdam\nLearning rate (αlr)\n2 × 10−4\nFirst Adam decay rate (β1)\n0.9\nSecond Adam decay rate (β2)\n0.999\nTable 6: Final parameters of the multilyer perceptron used for learning the gener-\native distribution of the MNIST dataset.\nHyperparameter\nValue\nGenerator layers\n[256, 512, 1024]\nGenerator ﬁnal layer activation\nTanh\nCritic layers\n[1024, 1024, 1024]\nCritic ﬁnal layer activation\nLinear\nCritic steps for one generator step\n10\nHidden layers activation\nLeaky ReLU\nLeakiness parameter (αlrelu)\n0.2\nDropout probability\n0\nNoise size\n100\nPenalty coeﬃcient\n10\nOptimiser\nAdam\nLearning rate (αlr)\n2 × 10−4\nFirst Adam decay rate (β1)\n0.5\nSecond Adam decay rate (β2)\n0.9\nTable 7: Final parameters of the Wasserstein GAN with Gradient Penalty used\nfor learning the generative distribution of the MNIST dataset.\n53\nHyperparameter\nValue\nGenerator layers\n[256, 512, 1024]\nGenerator dropout probability\n0\nGenerator ﬁnal layer activation\nTanh\nDiscriminator layers\n[1024, 512, 256]\nDiscriminator dropout probability\n0.2\nDiscriminator ﬁnal layer activation\nSigmoid\nDiscriminator steps for one generator step\n1\nHidden layers activation\nLeaky ReLU\nLeakiness parameter (αlrelu)\n0.2\nNoise size\n100\nOptimiser\nAdam\nLearning rate (αlr)\n2 × 10−4\nFirst Adam decay rate (β1)\n0.9\nSecond Adam decay rate (β2)\n0.999\nTable 8: Final parameters of the original GAN used for learning the generative\ndistribution of the MNIST dataset.\n54\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2019-04-30",
  "updated": "2019-06-10"
}