{
  "id": "http://arxiv.org/abs/1905.01684v2",
  "title": "Unsupervised Detection of Distinctive Regions on 3D Shapes",
  "authors": [
    "Xianzhi Li",
    "Lequan Yu",
    "Chi-Wing Fu",
    "Daniel Cohen-Or",
    "Pheng-Ann Heng"
  ],
  "abstract": "This paper presents a novel approach to learn and detect distinctive regions\non 3D shapes. Unlike previous works, which require labeled data, our method is\nunsupervised. We conduct the analysis on point sets sampled from 3D shapes,\nthen formulate and train a deep neural network for an unsupervised shape\nclustering task to learn local and global features for distinguishing shapes\nwith respect to a given shape set. To drive the network to learn in an\nunsupervised manner, we design a clustering-based nonparametric softmax\nclassifier with an iterative re-clustering of shapes, and an adapted\ncontrastive loss for enhancing the feature embedding quality and stabilizing\nthe learning process. By then, we encourage the network to learn the point\ndistinctiveness on the input shapes. We extensively evaluate various aspects of\nour approach and present its applications for distinctiveness-guided shape\nretrieval, sampling, and view selection in 3D scenes.",
  "text": "Unsupervised Detection of Distinctive Regions on 3D Shapes\nXIANZHI LI, LEQUAN YU, and CHI-WING FU, The Chinese University of Hong Kong\nDANIEL COHEN-OR, Tel Aviv University\nPHENG-ANN HENG, The Chinese University of Hong Kong\nThis paper presents a novel approach to learn and detect distinctive regions\non 3D shapes. Unlike previous works, which require labeled data, our method\nis unsupervised. We conduct the analysis on point sets sampled from 3D\nshapes, then formulate and train a deep neural network for an unsupervised\nshape clustering task to learn local and global features for distinguishing\nshapes with respect to a given shape set. To drive the network to learn\nin an unsupervised manner, we design a clustering-based nonparametric\nsoftmax classifier with an iterative re-clustering of shapes, and an adapted\ncontrastive loss for enhancing the feature embedding quality and stabilizing\nthe learning process. By then, we encourage the network to learn the point\ndistinctiveness on the input shapes. We extensively evaluate various aspects\nof our approach and present its applications for distinctiveness-guided shape\nretrieval, sampling, and view selection in 3D scenes.\nCCS Concepts: • Computing methodologies →Neural networks; Shape\nanalysis.\nAdditional Key Words and Phrases: shape analysis, unsupervised, learning,\nneural network, distinctive regions\nACM Reference Format:\nXianzhi Li, Lequan Yu, Chi-Wing Fu, Daniel Cohen-Or, and Pheng-Ann Heng.\n2020. Unsupervised Detection of Distinctive Regions on 3D Shapes. ACM\nTrans. Graph. 1, 1 (April 2020), 14 pages. https://doi.org/10.1145/nnnnnnn.\nnnnnnnn\n1\nINTRODUCTION\nReasoning about distinctive regions on 3D shapes has a wide range\nof applications in computer graphics and geometric processing, e.g.,\nobject retrieval [Gal and Cohen-Or 2006; Shilane and Funkhouser\n2006], shape matching [Castellani et al. 2008; Shilane and Funkhouser\n2007], and view selection [Lee et al. 2005; Leifman et al. 2012]. In\nthis work, we follow the definition of distinctive regions proposed\nby Shilane and Funkhouser [2006; 2007], i.e., the distinction of a\nsurface region in an object is defined as\nhow useful the region is for distinguishing the object from\nothers of different types1.\n1Type refers to the specific class that an object belongs to, e.g., chair, table, and car.\nAuthors’ addresses: Xianzhi Li; Lequan Yu; Chi-Wing Fu, The Chinese University of\nHong Kong, {xzli,lqyu,cwfu}@cse.cuhk.edu.hk; Daniel Cohen-Or, Tel Aviv University,\ndcor@mail.tau.ac.il; Pheng-Ann Heng, The Chinese University of Hong Kong, pheng@\ncse.cuhk.edu.hk.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\n© 2020 Association for Computing Machinery.\n0730-0301/2020/4-ART $15.00\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\nFig. 1. Distinctive regions detected by our method on the same shape shown\nin (a,b), relative to (a) an inter-class dataset (i.e., the whole ModelNet40\ndataset) and (b) an intra-class dataset (i.e., only the tail-engine and four-\nengine airplanes in ModelNet40), with their corresponding distinctiveness-\nguided sampling results (c,d). The colors in (a,b) indicate the region distinc-\ntiveness with red color being the most distinctive.\nFig. 2. Top-3 Shape retrieval results using our intra-class distinctiveness-\nguided retrieval method (top row) and using FoldingNet [Yang et al. 2018],\na general unsupervised feature learning method (bottom row). Conven-\ntional methods can only retrieve shapes with similar overall appearance,\nwhile guided by distinctiveness enables the retrieval of shapes with similar\ndistinctive features, e.g., the two engines at the back.\nHence, distinctive regions of a shape should be common and unique\nin its own type, compared with shapes of other types. So, distinctive\ninvolves, and is quantified relative to, a given set of 3D shapes.\nHowever, existing methods [Shilane and Funkhouser 2006, 2007;\nSong et al. 2018] either rely on hand-crafted local features, or detect\ndistinctive regions in a supervised setting, meaning that they require\nlabels on data. Hand-crafted features generally do not generalize\nwell to other shapes, since their representation capabilities are lim-\nited by the pre-defined fixed operations. Also, in most application\nscenarios, it is difficult to acquire labels or pre-classify 3D shapes\ndue to annotation efforts. In light of these limitations, the challeng-\ning problem is to explore unsupervised methods to detect distinctive\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: April 2020.\narXiv:1905.01684v2  [cs.GR]  20 Apr 2020\n2\n•\nLi X. et al.\nregions directly from the 3D shapes in a data-driven manner. In\nsuch a setting, the set of given shapes has not been pre-analyzed by\nany means, and no local descriptors are pre-defined on them.\nIn this work, we present a method to compute distinctive regions\non 3D shapes in an unsupervised setting. Our method is based on\na neural network that learns and analyzes a given set of shapes\nwithout relying on hand-crafted local features, and assigns to each\npoint on the shapes a degree of distinctiveness. First, we sample\nand represent each given shape as a point cloud, a lightweight\nand flexible representation. We design a deep neural network and\ntrain it on the point clouds for an unsupervised clustering task. By\ntraining, our network can learn both per-point features and per-\nshape features. In particular, to drive the network to learn to cluster\nthe shapes for detecting distinctive regions, we design a joint loss\nfunction composed of a clustering-based nonparametric softmax\nloss and an adapted contrastive loss. For the network to learn to\ncluster the shapes, it has to attend to the discriminative features\namong the shape clusters. Hence, by analyzing the resulting per-\npoint features and per-shape features, we can obtain a degree of\ndistinctiveness per point in the point sets, and further project the\nper-point distinctiveness in each point set back to the original shape.\nFigure 1 shows two visual examples of using our method to find\ndistinctive regions in the same shape (red being the most distinctive)\nrelative to two different datasets: the inter-class dataset (a) is the\nwhole ModelNet40 dataset [Wu et al. 2015], whereas the intra-class\ndataset (b) comprises only the tail-engine and four-engine airplanes\nin ModelNet40. Comparing the distinctive regions in (a) and (b)\nreveals an interesting phenomenon that (a) tends to focus on the\ncontour of the airplane, while (b) tends to pay more attention to the\nlocal regions. This result corresponds to the definition of distinction,\nsince in the inter-class dataset, the contour of the tail-engine airplane\nis common and unique in the airplane class compared with others,\nwhile in the intra-class dataset, the tail and engines are more helpful\nfor distinguishing the tail-engine airplanes from the four-engine\nairplanes. Sampling the point sets away from the distinctive regions\nonly has little effect on the classification (see Figure 1 (c) & (d)). Later,\nwe shall show, in an extensive empirical experiment presented in\nSection 4.2, that the points located on the distinctive regions are\nthe key points for classification performance. Besides, the detected\ndistinctive regions can further facilitate the development of various\napplications, e.g., fine-grained shape retrieval; see an example result\nin Figure 2, and Sections 4 and 5 for more results.\nIt is worth noting that, the notion of distinction is closely related\nto saliency as they both measure regional importance. However,\nwhile distinction considers how common and unique a region is\nrelative to objects of other types, saliency considers how unique and\nvisible a region is relative to other regions within the same object.\nOverall, the contributions of this work are summarized below.\nWe develop a novel unsupervised framework to detect distinctive\nregions on 3D shapes that does not require hand-crafted features\nand labels on data. We design a new clustering-based nonparametric\nsoftmax classifier and adopt an adapted contrastive loss to encourage\nthe network to learn in an unsupervised manner. We performed\nextensive experiments to evaluate the effectiveness of our method:\nquantitatively evaluating on how the detected distinctive regions\nhelp shape classification, a user study to compare our results with\nhuman, etc. Further, we show how distinctiveness contributes to\napplications for shape retrieval, sampling, and view selection.\n2\nRELATED WORK\nDistinctive region detection. The concept of distinction, or distinc-\ntiveness, was first proposed by Shilane and Funkhouser [2006; 2007].\nThe main idea of their methods is to extract local shape descriptors\nfor local regions on each shape, then to obtain the distinctiveness of\neach local region by comparing the difference between all pairs of\nshape descriptors in the training database. To avoid the drawback of\nhand-crafted shape descriptors, Song et al. [2018] employed a classi-\nfication network to consume multi-view images of given 3D shapes\nas input and learn view-based distinction by back-propagating the\nclassification probability. Next, a Markov random field is employed\nto combine the view-based distinctions across multiple views. De-\nspite the success in finding distinctive regions, existing approaches\nare all supervised, meaning that they all need class labels on the\nshapes given in the training dataset. In contrast, our method detects\ndistinctive regions in an unsupervised manner.\nBesides 3D shapes, the concept of distinction was also mentioned\nin several works on images. Given a large collection of geo-localized\nimages, Doersch et al. [2012] developed a discriminative clustering\napproach to find visual elements that occur much more often in\none geographic region than in others, e.g., the kinds of windows,\nbalconies, and street signs that are distinctive in Paris, compared\nwith those in London. Later, several approaches were developed\nto extract discriminative regions from images for image classifica-\ntion [Juneja et al. 2013; Singh et al. 2012; Sun and Ponce 2013]. More\nrecently, Wang et al. [2016] proposed a patch-based framework by\nintroducing triplets of patches with geometric constraints to mine\ndiscriminative regions for fine-grained intra-class classification.\nSaliency region detection. Similar to distinction, saliency also mea-\nsures regional importance of a shape, but it considers how unique\nand visible a region is relative to other regions within the same ob-\nject. Lee et al. [2005] devised a scale-dependent measure to compute\nthe mesh saliency, while Gal and Cohen-Or [2006] developed local\nsurface descriptors to extract salient geometric features for partial\nshape matching and retrieval. These techniques are typically based\non curvature, or other geometric features. To alleviate the limitation\nof hand-crafted geometric features, several works adopt data-driven\nmethods to effectively find the saliency for 3D surfaces [Chen et al.\n2012; Shu et al. 2019]. In other aspects, Shtrom et al. [2013] detected\nsaliency in large point sets, while Ponjou Tasse et al. [2015] detected\nsaliency in point sets with a cluster-based approach. Very recently,\nWang et al. [2018] developed an eye tracking system to obtain mesh\nsaliency from human viewing behavior.\nNetwork explanation. Our work is also related to the visualiza-\ntion of neuron activities in deep neural networks. Zeiler and Fer-\ngus [2014] devised a perturbation-based method to find the contribu-\ntion of each portion of the input by removing or masking them, and\nthen running a forward pass on the new input to contrast with the\noriginal input. Such approach tends to be slow as the number of test\nregions grows. Instead, backpropagation-based methods [Ancona\net al. 2018; Shrikumar et al. 2017; Sundararajan et al. 2017; Zhang\net al. 2018] compute the contribution of all the input regions in\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: April 2020.\nUnsupervised Detection of Distinctive Regions on 3D Shapes\n•\n3\nFig. 3. The overall framework of our unsupervised learning approach to detect distinctive regions on a given set of 3D shapes.\na single forward and backward pass through the network. Unlike\nthe backpropagation-based methods, Zhou et al. [2016] formulated\nthe Class Activation Map (CAM) to localize the discriminative re-\ngions, and Selvaraju et al. [2017] further developed the grad-CAM\nfor producing visual explanations for decisions from the CNN mod-\nels. These methods, however, require the class labels to visualize\nthe neuron activities, while in our work, we analyze the network\nactivities in an unsupervised training.\nDeep neural networks for point set. Following PointNet [Qi et al.\n2017a] and PointNet++ [Qi et al. 2017b] to embed features directly\nfrom point sets, several works successively introduce methods to im-\nprove the capturing of geometric information, e.g., SpiderCNN [Xu\net al. 2018], KCNet [Shen et al. 2018], PointGrid [Le and Duan 2018],\npointwise convolution [Hua et al. 2018], DGCNN [Wang et al. 2019],\nSPLATNet [Su et al. 2018], and PointCNN [Li et al. 2018]. Besides\nembedding point features for recognition tasks, several works pro-\npose to learn point features for registration, e.g., [Aoki et al. 2019;\nWang and Solomon 2019]. In our network, we adopt PointCNN as\na module to extract point features, but other network models can\nalso be used for the purpose.\n3\nMETHOD\n3.1\nOverview\nGiven a set of shapes S = {Sj }Nobj\nj=1 , let Pj = {pi,j }N\ni=1 be a set of\n3D points sampled on the j-th shape Sj, where Nobj is the number\nof shapes in S; N is the number of points in each point set Pj; and\npi,j ∈R3 is the 3D coordinates of the i-th point in Pj. The problem\nof detecting distinctive regions on shape Sj is\nTo predict a per-point distinctiveness valuedi,j ∈[0, 1]\nfor each point pi,j on Sj relative to the shapes in S\nthat are of different types from Sj,\nwhere a large di,j ≈1 indicates that the associated region exists\nmainly in shapes of the same type as Sj but not in shapes of other\ntypes, and a small di,j ≈0 indicates that the associated region exists\nin all types of shapes. Hence, di,j indicates the degree to which the\nassociated region distinguishes shape Sj from others, and there is\nno requirement for the size of the distinctive regions.\nSuch goal requires us to consider not only the object itself, but\nalso the other objects in the given reference set. Intuitively, the\ndesigned network should contain per-point importance, while also\nhaving the ability to classify object types. Hence, we propose a novel\nframework to learn both per-point features and per-shape features,\nas shown in Figure 3. The per-point features are for calculating\nthe distinctiveness di,j, while the per-shape features are for shape\nclassification. To perform the feature embedding in an unsupervised\nmanner, we drive the network to learn by solving an unsupervised\nshape clustering task using our joint loss (see Figure 3 (right)).\nIn the following, we first introduce the network for feature embed-\nding (Section 3.2). Next, we introduce a clustering-based nonpara-\nmetric softmax classifier (Section 3.3) and an adapted contrastive\nloss (Section 3.4) to drive the unsupervised network to learn. Lastly,\nwe give details on the end-to-end network training (Section 3.5),\nand describe how we obtain the per-point distinctiveness values\nfrom the embedded features (Section 3.6).\n3.2\nFeature Embedding\nExtracting the per-point features. In this part, we aim to learn an\nembedding function fθ :\nFj = fθ (Pj) ,\n(1)\nwhere Fj ∈RN ×M is the set of extracted per-point features (see Fj in\nFigure 3), each row fi,j in Fj is a per-point feature of M channels, and\nfθ is a deep neural network with parameters θ. In theory, fi,j should\nrepresent the underlying local geometric structures around each\npoint and further reveal the point’s distinctiveness. We apply a point-\nset-network to extract per-point features, and the choice of the point-\nset-network is flexible. Most recent networks on processing point\nsets can be employed; here, we adopt the segmentation architecture\nof PointCNN [Li et al. 2018] to learn fθ .\nAdaptive per-point features refinement. So far, the per-point fea-\ntures are extracted locally over each given shape by the point-set\nnetwork. As the distinctiveness requires shape-level context, not\nsimply local context around each point, we further propose to refine\nthe per-point features Fj. To this end, we adopt [Woo et al. 2018] to\nformulate the channel-spatial attention unit (see Figure 4) to fuse\nthe M feature channels over the N per-point features together and\nproduce the refined per-point embedding feature Fr\nj .\nExtracting the per-shape feature. Further, we use an average-\npooling operation to obtain the global feature gj from Fr\nj :\ngj =\nÍN\ni=1 fr\ni,j\nN\n,\n(2)\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: April 2020.\n4\n•\nLi X. et al.\nFig. 4. The channel-spatial attention unit.\nwhere fr\ni,j denotes the i-th per-point feature vector in Fr\nj . As shown\nin Figure 3, the process of training the network to learn the local (per-\npoint) and global (per-shape) features is driven by an unsupervised\njoint loss function, which we shall elaborate below.\n3.3\nClustering-based Nonparametric Softmax\nTo drive the network to classify objects in an unsupervised manner,\nwe propose a clustering-based nonparametric softmax classifier.\nIn a typical supervised deep neural network for classification, the\nsoftmax classifier is commonly employed and the probability of the\nj-th object being recognized as the q-th class is\nP(yj = q|gj) =\nexp(wTq gj)\nÍC\nk=1 exp(wT\nk gj)\n,\n(3)\nwhere yj is the class label of the j-th object; C is a hyperparameter\nthat denotes the number of classes; q ∈{1, ...,C} is the class assign-\nment; wk is the weight vector for the k-th class; k ∈{1, ...,C}; and\nwT\nk gj measures how well gj matches the k-th class, so wk serves\nas the class prototype of the k-th class.\nIn supervised learning, we can leverage the class labels provided\nin training data to learn the class prototype wk of each class. This\nis, however, not possible for unsupervised learning. Recently, an\nobservation was reported by Liu et al. [2018] that when the network\nhas successfully converged, the class prototype is usually consistent\nwith the average of all the feature vectors belonging to the same\nclass. Based on this observation, we thus approximate the class\nprototype by using the average of all the feature vectors belonging\nto the class, since no class labels are given in our setting.\nWe adopt the above observation to our problem by formulating\nthe clustering-based nonparametric softmax classifier, where we itera-\ntively re-cluster the per-shape feature vectors gj in the network and\ntake the average feature vector of each cluster to estimate the clus-\nter prototype wk. In this way, we can approximate the probability\nP(yj = q|gj) for unsupervised learning as\nP(yj = q|gj) ≈\nexp(¯gTq gj/τ)\nÍC\nk=1 exp(¯gT\nk gj/τ)\n,\n(4)\nwhere ¯gk=\n1\n|Ck |\nÍ\nt ∈Ck gt is the average feature vector over all per-\nshape global feature vectors gt of cluster Ck; we take ¯gk (per-cluster)\nto approximate wk for unsupervised learning; and C denotes the\nnumber of clusters in our unsupervised setting. Further, we enforce\n∥gj ∥=1 via an L2-normalization layer in the network and make use\nof τ, which is a temperature parameter, to control the concentration\nlevel of the distribution [Hinton et al. 2015; Wu et al. 2018].\nIn our experiments, we set τ as 0.07, following the setting in [Wu\net al. 2018]. Then, our learning objective is to maximize P(yj = q|gj),\nor equivalently, to minimize the negative log-likelihood of the prob-\nability. Therefore, our clustering-based nonparametric softmax loss is\nformulated as\nLcluster = −\nNobj\nÕ\nj=1\nlog P(yj = q|gj).\n(5)\nIn our implementation, we use spectral clustering [Stella and Shi\n2003; Von Luxburg 2007] to cluster the per-shape global features gj\nin each training epoch. Experimentally, we found that our network\ndetects similar distinctive regions when equipped with different\nclustering algorithms; see Supplementary Material Part A for the\nevaluation. Also, please see Supplementary Material Part B for the\neffect of having different C on extracting distinctive regions.\n3.4\nAdapted Contrastive Learning\nInaccurate clustering results are inevitable, so relying only on the\nclustering-based loss may mislead the learning of the network. Moti-\nvated by [Bachman et al. 2019; Hénaff et al. 2019; Hjelm et al. 2019],\nto stabilize and enhance the feature learning in the network, we for-\nmulate an adapted contrastive loss, which is particularly important\nat the beginning of the training process when the clustering results\nare more random. Considering input point set Pj to the network\nas the anchor, for each training epoch, we form a positive point set\nsample P+\nj and a negative point set sample P−\nj for Pj, such that the\nper-shape global feature g+\nj associated with P+\nj is close to gj, while\nthe per-shape global feature g−\nj associated with P−\nj is far from gj.\n• For P−\nj , we randomly pick a point set from the shapes in the\nclusters that Pj does not belong to.\n• For P+\nj , since the intra-class clustering results may not be\nreliable, especially at the beginning of the training process,\nwe thus do not randomly pick from the cluster that Pj belongs\nto. Rather, we resample another point set P+\nj on the given 3D\nshape (Sj) associated with Pj and pass P+\nj to the network to\ngenerate g+\nj . Note that P+\nj and Pj are different point sets due\nto randomness in the point sampling process, but essentially,\nthey describe the same object, i.e., Sj.\nWe take the above triplet {gj, g+\nj , g−\nj } to form an adapted contrastive\nloss following [Hadsell et al. 2006] as\nLcontrastive = D(gj, g+\nj ) + max(0, λ −D(gj, g−\nj )),\n(6)\nwhere D is the Euclidean distance in feature space, and we set\nλ = 2.0 in our experiments. Importantly, we generate such triplet\ninput dynamically for each Pj in each training epoch. Using this\nstrategy, we can increase the diversity of the training samples and\nproduce more reliable samples as the training progresses.\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: April 2020.\nUnsupervised Detection of Distinctive Regions on 3D Shapes\n•\n5\nclustering\ng1\ng2\ng3\ngNobj\n…\nrandom\ninitialization\nupdate per-cluster\nglobal features\ng1\ng2\n…\ngC\nclustering\ng1\ng2\ng3\ngNobj\n…\nupdate per-shape\nglobal features\nupdate per-cluster\nglobal features\ng1\ng2\n…\ngC\n1st epoch\n…\ng1\ng2\ng3\ngNobj\n…\nfinal per-shape\nglobal features\nlast epoch\nFig. 5. Illustration of the network training process.\n3.5\nEnd-to-end Network Training\nOverall, we end-to-end train the network to learn the features for\nclustering the given shapes using the joint loss function\nL(θ) = Lcluster + αLcontrastive + β∥θ ∥2,\n(7)\nwhere α balances the two loss terms and β is the multiplier of weight\ndecay in the regularization term (see Section 4.1 for their values).\nIn summary, the feature embedding is conducted in a self-training\nway by iteratively learning the feature vectors, re-clustering them,\nthen using the clustering results to fine-tune the model. Figure 5\nillustrates the whole training process, where we first randomly\ninitialize the per-shape global feature gj of each training sample\nPj, cluster them into C classes, and generate the per-cluster global\nfeature ¯gk for each cluster. Early in the training, these gj and ¯gk\nare unlikely reliable, but as the training progresses, we iteratively\nupdate these per-shape and per-cluster features in each training\nepoch, these features can then gradually converge and let us further\nobtain the per-point distinctiveness in the given shapes. Here, C\nremains unchanged during the training, and our method does not\nrequire even class size in the training data. The clustering method we\nemployed, i.e., spectral clustering, will divide the training samples\nautomatically into C clusters based on the feature similarity.\nFigure 6 shows t-SNE visualizations that reveal the clustering of\nthe per-shape features (gj) during the unsupervised training. Here,\nwe cluster over 9000 shapes into 40 classes. The dimension of the\nfeatures (i.e., M) is 128 in our implementation.\n3.6\nObtaining and Visualizing the Distinctiveness\nAs introduced earlier in Section 3.1, we design our unsupervised\napproach to learn both per-shape and per-point features in the given\nshapes. After the training to meet the shape clustering task, the\nresponse of the activation neuron associated with the per-point\nfeatures should positively correlate to its confidence of the detec-\ntion [Zhang et al. 2018]. Therefore, we obtain the per-point distinc-\ntiveness di,j from fr\ni,j of each point pi,j by taking the maximum\nvalue in fr\ni,j and normalizing di,j between 0 and 1 for each shape.\nFor a comparison of applying other alternatives to extract di,j from\nfr\ni,j, including the mean, L2 norm, average of the three largest values,\netc., please refer to Supplementary Material Part C.\nFurthermore, to visualize the distinctiveness results, we project\nthe per-point distinctiveness on point set Pj back to the original\nshape Sj and obtain a distinctiveness value for every vertex on\nthe original shape by averaging the distinctiveness values over the\nFig. 6. t-SNE visualizations of the per-shape features clustering during the\nunsupervised training process.\nnearby sampled points in Pj; see Figure 1 and Figure 7 for example\nresults, where regions in red are the most distinctive.\n4\nEXPERIMENTS AND RESULTS\n4.1\nImplementation Details\nOur method was implemented using TensorFlow [Abadi et al. 2016].\nTo train the network, we randomly sampled 2,048 points for each\nshape as input and augmented the input point sets on-the-fly, in-\ncluding random rotation, scaling, shifting, and jittering. Moreover,\nwe empirically set α and β in Eq. (7) as 3.0 and 10−5, respectively, and\ntrained our network for 200 epochs using the Adam optimizer [Kingma\nand Ba 2014] with a learning rate of 0.01. See Supplementary Ma-\nterial Part J for further analysis on α and β. For the ModelNet40\ndataset [Wu et al. 2015] with 9,839 training samples, it took about\n25 hours to train our network on a 12GB TITAN Xp GPU with a\nbatch size of 50. The network has 0.48M parameters. For inference,\nit took about 0.017 seconds for our method to predict the per-point\ndistinctiveness values for a point cloud of 2,048 points.\nBased on PointCNN [Li et al. 2018], we further made the following\nadaptations to the feature embedding component in the network\narchitecture. First, we use a fixed-size query ball [Qi et al. 2017b]\ninstead of KNN or geodesic-like KNN [Yu et al. 2018] to find the\nlocal neighborhood for extracting the point features; note also that\naccording to [Qi et al. 2017b], query ball is preferred for finding the\nlocal neighborhood in tasks that require local pattern recognition,\ne.g., our distinctive detection task. Second, we removed the X-conv\noperation in the deconvolution part of the PointCNN segmentation\nnetwork and directly used feature interpolation [Qi et al. 2017b]\nfor per-point feature restoration. In this way, we can reduce the\nnumber of network parameters and speed up the network training\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: April 2020.\n6\n•\nLi X. et al.\nFig. 7. Distinctive regions detected by our method unsupervisedly on various 3D models in ModelNet40 [Wu et al. 2015]; red indicates high distinctive regions.\nprocess with little degradation in the quality of the results. Lastly,\nwe explored different network backbones (i.e., PointNet and Point-\nNet++) for learning the per-point local features; see Supplementary\nMaterial Part D for the experimental results.\n4.2\nDetecting Distinctive Regions\nDistinctiveness visualization. We employed the ModelNet40 train-\ning split dataset [Wu et al. 2015] and trained our network in an un-\nsupervised manner to sort the models in the dataset into 40 clusters.\nFigure 7 shows the distinctive regions detected by our method on\na variety of models in the dataset, where red color indicates high\ndistinctive regions. When we determine if a region is distinctive,\ninstead of looking just at the shape itself, we consider all shapes\nof different classes (but not shapes of its own class) in the dataset.\nTaking the Person shape in Figure 7 (right) as an example, the head,\nfeet, and hand are found to be more distinctive (red), while the body\npart is less (blue). Since we compare the Person shapes with shapes\nof other types, these human parts are distinctive for the network to\nrecognize the Person shapes relative to others. For the two Lamp\nshapes in Figure 7 (middle & top-right), our method detects the bulb\nas distinctive, since it is common and unique in this class compared\nwith shapes in other classes. For the Chair shapes, not only the legs\nare detected, the back is also detected as distinctive. Similarly, our\nmethod detects as distinctive the handle of the Cup, the leaves of\nthe Plant, the struts of the Guitar, the handle of the Door, etc. For\nmore results, please refer to Supplementary Material Part K.\nIn particular, our network does not simply locate high-curvature\nregions and extremities as distinctive. We show several another\nexamples in Figure 8, where the detected distinctive regions are not\nextreme regions. For example, for the four shapes shown in the top\nrow, the detected distinctive regions are not extreme regions at all.\nConcerning the other four shapes on the bottom, for the Chair shape,\nits back (see the region marked by the black arrow), which is not\nFig. 8. Distinctive regions detected by our method may not simply lie on\nhigh-curvature and extreme regions, as shown in these examples. Also, not\nall extreme regions are detected as distinctive (see purple arrows).\nextreme region, is also detected. For the Table and Monitor shapes,\nsome obvious extreme regions are not detected (see the regions\nmarked by the purple arrows). For more non-extreme examples,\nplease see Supplementary Material Part M.\nQuantitative evaluation. Next, we quantitatively evaluated how\nhelpful the detected distinctive regions are to shape classification. Here,\nwe employed totally 2,468 models in the ModelNet40 testing dataset\nas test shapes, and used three different preferences to downsample\npoints from pre-sampled point sets (N = 2, 048) on the test shapes:\n(i) probability to preserve a point based on the distinctiveness at the\npoint; (ii) probability to preserve a point based on the curvature at\nthe point; and (iii) ignore the point importance and downsample\npoints at random. Therefore, preference (i) leads to the production\nof more points on our detected distinctive regions, while preference\n(ii) leads to the production of more points on high-curvature regions.\nThen, we fed the downsampled points into a classification network\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: April 2020.\nUnsupervised Detection of Distinctive Regions on 3D Shapes\n•\n7\nFig. 9. Our network, when trained with different datasets (left), detects different distinctive regions on the same given objects (right): (i) a training set of\nfour-engine and tail-engine airplanes in the top row, and (ii) a training set of four-engine and two-engine airplanes in the bottom row.\nFig. 10. Overall shape classification accuracy when we downsample points\non test shapes using three different preferences. Having more distinctive\npoints detected by our method (orange plot) leads to higher accuracy.\n(i.e., PointNet [Qi et al. 2017a]), which has been pre-trained on the\nModelNet40 training split dataset with 2,048 points on each shape for\na shape classification task, and computed the overall classification\naccuracy averaged over all the test shapes.\nFigure 10 plots the overall classification accuracy for point sets\ndownsampled with the three different preferences using decreas-\ning number of downsampled points. Comparing the orange plot\nwith the gray and blue plots in Figure 10, we can see that having\nmore points on the distinctive regions can better preserve the dis-\ncrimination of the shapes, leading to higher shape classification\naccuracy, particularly for results with fewer points. Hence, this\nquantitative comparison shows that the distinctive regions detected\nby our method are helpful to the classification of 3D shapes. Addi-\ntionally, since the orange plot is above the gray plot, this indicates\nthat not all high-curvature regions are important for recognition and\nour method does not simply take all regions of specific curvature\nprofiles, e.g., sharp corners and extremities, as distinctive.\n4.3\nEffect of using Different Training sets\nA notable characteristic of detecting distinctive regions is that the\nresults highly relate to the given set of shapes, or the training set.\nTo verify this, we conducted an experiment to explore the effect of\ntraining set as follows. First, we collected two training sets: (i) 500\nfour-engine airplanes and 250 tail-engine airplanes (see Figure 9\n(top-left) for examples), and (ii) we kept the four-engine airplanes\nbut replaced the tail-engine airplanes with around 1,000 two-engine\nairplanes (see Figure 9 (bottom-left) for examples). Please see Supple-\nmentary Material Part E for more examples in the datasets. Further,\nwe trained our network on each training set separately with C set\nto two, and employed the two trained network models to detect\ndistinctive regions on four-engine airplanes.\nFigure 9 (right) shows the distinctive regions detected on four\ndifferent four-engine airplanes. The interesting observation is that\nwhen trained with the four-engine vs. tail-engine airplane dataset\n(top row), our network tends to highlight all the four engines on the\ntest airplanes. On the other hand, when trained with the four-engine\nvs. two-engine airplane dataset (bottom row), our network tends to\nonly highlight the outer two engines on the airplanes as distinctive\nregions. In particular, the four test airplanes have different size,\nengine shape, and wing shape, where the middle two have special\nstructures, i.e., extra fuel tanks and a radar on top. Yet, given these\nshapes as inputs, our method still detects consistent distinctive\nregions. Further, if we look closer to the training set on top left, the\ntwo kinds of airplanes not only have different engine locations but\nalso different tail shapes. Even though these tail regions are not\nas dominant as the engines, our trained network can still weakly\nhighlight the tails on the test airplanes; compare the tails of the\ntest airplanes on top-right vs. bottom-right in Figure 9. Please see\nSupplementary Material Part L for results on other datasets.\n4.4\nComparing with Other Methods\nFigure 11 shows the distinctive regions detected by a traditional\nmethod [Shilane and Funkhouser 2007] (without deep neural net-\nworks), by a weakly-supervised deep learning method [Song et al.\n2018], and by our method. We train our network using the same\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: April 2020.\n8\n•\nLi X. et al.\nFig. 11. Distinctive regions detected by Shilane et al. [2007] (top row), by Song et al. [2018] (middle row), and by our method (bottom row). For all the three\nmethods, distinctive regions on the person shapes are detected by comparing the person shapes with shapes of other different class types.\ntraining set, i.e., the Princeton Shape Benchmark [Shilane et al. 2004]\nas in [Shilane and Funkhouser 2007], and most class types of train-\ning samples in [Song et al. 2018] are the same as ours. For all the\nthree methods, distinctive regions on the person shapes are detected\nby comparing the person shapes with shapes of other different class\ntypes in the data. The results of [Shilane and Funkhouser 2007]\nand [Song et al. 2018] were directly acquired from their papers.\nFrom the results, we can see that [Shilane and Funkhouser 2007]\ntends to highlight elbows as distinctive regions and ignore semantic\nparts such as heads and feet, due to the limited representation capa-\nbility of the hand-crafted features. For [Song et al. 2018], it is able\nto highlight heads and hands as distinctive. Compared with [Song\net al. 2018], even though our method is unsupervised, it can detect\nnot only the heads and hands but also the feet as distinctive. In par-\nticular, our method detects consistent distinctive regions on these\nPerson shapes, even they have different poses and shapes.\n4.5\nUnsupervised vs. Weakly-supervised Learning\nTo explore if our unsupervised network can meaningfully cluster\nthe shapes for detecting distinctive regions, we further compared\nit with a weakly-supervised version of our method. Specifically,\nwe used the class labels provided in ModelNet40, added a fully-\nconnected layer with 40 output neurons after the global feature (see\nFigure 3) to regress the class scores, then used the cross entropy\nloss to replace the unsupervised loss to train this weakly-supervised\nnetwork. From the results presented in Figure 12, we can see that\nmost distinctive regions detected by the weakly-supervised network\n(top) can also be found by the unsupervised network (bottom); the\nunsupervised network only misses a few of them, e.g., some leaves\nin Plant. This comparison result gives evidence that even without\nthe class labels, the performance of our unsupervised method is still\ncomparable to that of the weakly-supervised version of our method.\nFig. 12. Distinctive regions detected by our method (bottom row) and a\nweakly-supervised version of our method (top row). Even without using any\nclass label, our method, which is unsupervised, can still detect regions that\nare similar to those by a weakly-supervised version of our method.\nFurther, we quantitatively compare the two results by follow-\ning [Dutagaci et al. 2012] to compute the False Negative Error\n(FNE) and False Positive Error (FPE). Specifically, given distinc-\ntiveness values di,j and bdi,j at point pi,j detected by the unsuper-\nvised and weakly-supervised networks, respectively, we first located\ntwo sets of more distinctive points per shape Sj by a threshold dt :\nQj = {pi,j |di,j > dt } and c\nQj = {pi,j |bdi,j > dt }. By regarding b\nQj as\nthe ground truth, a pointbq ∈b\nQj is said to be covered byQj, if there ex-\nists point q ∈Qj, such that ||bq−q||2 ≤rDj and q is not closer to any\nother point in b\nQj, where Dj is the bounding sphere diameter of shape\nSj and r is a parameter (ratio) to control the localization tolerance.\nThen, we compute FNEj = (|b\nQj | −Nc)/|b\nQj | = 1 −Nc/|b\nQj |, where\nNc is the number of points in b\nQj that are covered by Qj. On the\nother hand, each covered point in b\nQj corresponds to a unique point\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: April 2020.\nUnsupervised Detection of Distinctive Regions on 3D Shapes\n•\n9\nFig. 13. Average false negative error (FNE) and false positive error (FPE)\nover 400 models plotted against tolerance parameter r.\nFig. 14. Visual results in ablation study.\nin Qj, so the points in Qj that are without any correspondence in\nb\nQj are regarded as false positives. Hence, FPEj = (|Qj | −Nc)/|Qj | =\n1 −Nc/|Qj |. Here, FNE ∈[0, 1], FPE ∈[0, 1], and a small value\nindicates high consistency between Qj and b\nQj.\nFigure 13 plots the FNE and FPE values averaged over 400 randomly-\nselected objects (from the 40 different classes in the ModelNet40\ntesting split) against r. We can see that when r is just around 8% to\n10%, both FNE and FPE are very close to zero, thus demonstrating\nthe high consistency between the distinctive points detected by the\ntwo networks. Particularly, the average FPE is very low even when\nr = 0, meaning that most of the distinctive regions detected by our\nunsupervised method are also the distinctive regions detected by\nthe weakly-supervised version of our method.\n4.6\nAblation Study\nNext, we analyzed the major elements in our network by remov-\ning or replacing each of them when we train our network on the\nModelNet40 models: (i) w/o Atten – removing the channel-spatial\nattention unit (see Figure 4 for details); (ii) w/o Cont – removing\nthe Lcontrastive term from the joint loss in Eq. (7); and (iii) A-\nCenter-Cont – replacing Lcluster in Eq. (4) with an adapted center\nloss [Wen et al. 2016] for unsupervised clustering-based learning:\n1\n2\nÍNobj\nj=1 ∥gj −¯gq ∥2 with a goal of minimizing the intra-cluster vari-\nations, while keeping the features of different clusters separable.\nTable 1. Comparing the overall shape classification accuracy on ModelNet40\nwhen downsampling the test shapes with more points on the distinctive\nregions detected under different settings.\nDifferent\nNumber of downsampled points\nsettings\n1024\n512\n256\n128\n64\n32\nw/o Atten\n0.877\n0.871\n0.806\n0.648\n0.402\n0.244\nw/o Cont\n0.878\n0.871\n0.808\n0.653\n0.413\n0.245\nA-Center-Cont\n0.876\n0.865\n0.818\n0.658\n0.405\n0.235\nOur full pipeline\n0.881\n0.874\n0.829\n0.729\n0.550\n0.408\nFigure 14 shows the distinctive regions detected by our method\nunder the four different settings. Comparing the results produced\nwith the w/o Atten setting (left-most) and with our full pipeline\n(right-most), we can see that the attention unit helps locate distinc-\ntive regions that span over a larger spatial areas, e.g., the Chair’s\nback. Looking at the results produced with w/o Cont and A-Center-\nCont, we can see that they tend to miss some important regions, e.g.,\nthe Table’s desktop, or contain noise, e.g., the Chair’s back.\nBesides visual comparison, we performed the same quantitative\nevaluation on the results here, as in Section 4.2. Table 1 shows the\nshape classification accuracy on the ModelNet40 test dataset for the\nfour different settings in terms of decreasing number of downsam-\npled points. From the table, we can observe that our full pipeline\nleads to the highest classification accuracy. Since we preserve more\npoints on distinctive regions, this means that the three network ele-\nments being explored in this experiment all contribute to improve\nthe detection of the distinctive regions.\n4.7\nUser Studies\nTo obtain a sense of how consistent our results are with humans,\nwe conducted two user studies, which we shall elaborate below. The\nkey idea behind the studies is that we try to simulate the network\nclustering process with humans to obtain the distinctive regions\nof some test shapes. Specifically, we started by introducing the\ndefinition of distinction to each participant to confirm that all par-\nticipants understood the meaning of distinction. Then, we showed\nthe training dataset to the participants. However, to avoid fatigue,\nwe randomly selected a subset of 3D shapes from different classes in\nthe training dataset, and showed these shapes to each participant on\na computer display, on which the participant can rotate each shape\nand explore its details. Please refer to Supplementary Material Part F\nfor some screenshots. After that, each participant was given a set of\ntest shapes, and asked to cluster the shapes and label the distinctive\nregions on the shapes that affect how they cluster. All the shapes\nare presented simply in a colorless manner to the participants in\nboth of the studies.\nIntra-class prediction. The first user study explores how humans\nfind distinctive regions on shapes of the same class. Here, we em-\nployed (i) the dataset of four-engine vs. two-engine airplanes; and\n(ii) the dataset of four-engine vs. tail-engine airplanes, as presented\nin Section 4.3; see again Figure 9 (left). To avoid bias due to the\ndataset similarity, we randomly divided the participants into two\ngroups, one for each set. For the first group, we randomly selected\n10 four-engine airplanes and 22 two-engine airplanes, and presented\nthem in random order on a computer display. Then, the participants\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: April 2020.\n10\n•\nLi X. et al.\nFig. 15. The ten objects on which the participants mark distinctive regions.\nThe above boxes reveal the participant-marked distinctive regions and the\ncorresponding number of participants marked on each region. Note that\neach participant may mark more than one region on the same object.\nTable 2. Quantitative evaluation on the inter-class prediction consistency\nbetween our method and the participants.\nBottle Chair Cone Cup Door Guitar Lamp Plant Table Vase\nAvg\nFNE\n0.02\n0.40\n0.08\n0.10\n0\n0.51\n0.07\n0.07\n0\n0\n0.13\nFPE\n0\n0.53\n0.03\n0.03\n0.40\n0.20\n0.03\n0.07\n0.35\n0.30\n0.19\nWME\n0.03\n0.33\n0.12\n0.15\n0.18\n0.50\n0.09\n0.06\n0\n0\n0.15\nwere asked to divide the 32 airplanes into two clusters and label\nthe distinctive regions on the four-engine airplanes. For the other\ngroup of participants, we randomly selected 10 four-engine and\n22 tail-engine airplanes from the other dataset, and performed the\nsame procedure with the participants.\nAll ten participants (both groups) recruited in this study clustered\nthe airplanes in the same way as our network. For the four-engine\nvs. two-engine dataset, all participants marked the outer two en-\ngines as distinctive in the four-engine airplanes. Their results are\nthe same as our network predictions; see Figure 9 (bottom-right).\nFor the four-engine vs. tail-engine dataset, all participants marked\nthe four engines as distinctive in the four-engine airplanes, and\ntheir results are almost the same as our network (see Figure 9 (top-\nright)), except for the tails of the airplanes; since the tail-engine\nairplanes mostly have T-shaped tails, which are generally absent in\nfour-engine airplanes. Without our reminder, only one participant\nnoticed the T-shaped tails, but when we asked the other participants\nwhether such tails are also distinctive, they all strongly agreed. This\nstudy shows that our network is able to attend to large and small\ndistinctive regions, which may even be overlooked by humans.\nInter-class prediction. The second study explores how humans\nfind distinctive regions between shapes of various kinds. Here, we\nemployed the models from ModelNet40 and recruited 30 participants.\nTo avoid fatigue, we randomly selected 75 shapes evenly from 15\ndifferent classes, and further selected ten objects of different classes\nfrom the set; see Figure 15. Then, we showed the 75 shapes to each\nparticipant and asked him/her to mark distinctive regions on the\nten selected objects. Particularly, we explained the definition of\ndistinctiveness, i.e., the distinctive regions should be common in\neach specific class, while being unique relative to other classes.\nDuring the study, we found that the size of the marked region\nvaries among the participants, even at the same object location. Also,\nthey focus more on the shape features than on the size of the marked\nregions. Figure 15 shows the summary of human marked regions on\nthe ten objects. In each marked region, the corresponding number\nindicates how many participants marked on the same area.\nTo quantitatively compare the distinctive regions marked by the\nparticipants and detected by our method, we adapted the FNE and\nFPE metrics (see Section 4.5) as follows. First, for regions detected\nby our network, we set a threshold to keep only the high-distinctive\nregions as the final detected distinctive regions; see Figure 7 for the\ndistinctiveness distribution on the ten objects. Intuitively, we keep\nonly the yellow and red regions on the objects. Next, we compared\nthese regions with the regions marked by each participant, and\nregard his/her marked regions as the ground truth, where a marked\nregion is said to be covered, if both the participant-marked region\nand the network-detected region cover almost the same structures.\nOn the other hand, the network-detected regions that are covered\nby the participant-marked regions are said to be the true positives.\nHence, for each (k-th) participant, we define N c\nj,k as the number of\nparticipant-marked regions that are covered and T h\nj,k as the total\nnumber of participant-marked regions, on the j-th object. Also, we\ndefine Td\nj as the total number of network-detected regions on the\nj-th object. Then, similar to Section 4.5, we compute the correspond-\ning FNEj,k = 1 −N c\nj,k/T h\nj,k and FPEj,k = 1 −N c\nj,k/Td\nj , and further\ncompute the FNE and FPE values averaged over all the participants\nper object. Additionally, to account for the frequently-marked re-\ngions, we adopted the Weighted Miss Error (WME) metric [Dutagaci\net al. 2012], i.e., WME = 1 −Í\nk N c\nj,k/Í\nk T h\nj,k.\nTable 2 shows the per-object FNE, FPE, and WME values, as well\nas their overall averages over the ten objects. All three metrics range\n[0, 1], where a low value indicates high consistency between the\nparticipant-marked and network-detected regions. From the table,\nwe can see that most values are very low and several are even zeros,\nindicating that most participant-marked distinctive regions can be\ndetected by the network, and vice versa. However, values for some\ncomplex objects are a bit higher, e.g., the Chair and the Guitar, since\ndifferent participants may mark on different structures. Yet, the\noverall values are very close to zeros, meaning that the distinctive\nregions detected by our network are highly consistent with the\ndistinctive regions marked by the participants. Furthermore, we\nalso conducted a statistical test to show that 30 participants are\nsufficient to produce stable human labels in the analysis. Please\nrefer to Supplementary Material Part F for the details.\n5\nAPPLICATIONS\nBeing able to discover distinctive regions on 3D shapes enables\nus to support and enhance various applications, such as shape re-\ntrieval [Gal and Cohen-Or 2006; Shilane and Funkhouser 2006],\nshape simplification [Garland and Heckbert 1997], remeshing [Al-\nliez et al. 2002], best view selection [Shilane and Funkhouser 2007],\nperception-aware 3D printing [Zhang et al. 2015]. In this section, we\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: April 2020.\nUnsupervised Detection of Distinctive Regions on 3D Shapes\n•\n11\nFig. 16. Two sets of shape retrieval results (left & right). In each set, we show the top-five similar shapes retrieved by using the per-shape global feature from\nFoldingNet [Yang et al. 2018] (top) and from our network, i.e., gj (middle), and by using our distinctiveness-guided global feature hj (bottom). Retrieved\nshapes of substructures (armrest (left) & swivel base (right)) similar to the query shape are marked over a yellow background.\npresent three typical applications to demonstrate the applicability\nof our technique in distinctiveness-guided shape retrieval, sampling,\nand view selection in 3D scenes.\n5.1\nDistinctiveness-guided Shape Retrieval\nConventional approaches for shape retrieval first extract representa-\ntive shape descriptors then retrieve similar shapes based on the\ndistance between the extracted descriptors; see [Tangelder and\nVeltkamp 2004] for a survey. Though these approaches have achieved\npromising performance for inter-class retrieval, they tend to have\nlimited ability for fine-grained intra-class retrieval, since the ex-\ntracted descriptors are global. With our detected distinctive regions,\nwe can perform distinctiveness-guided shape retrieval, which enables\nfine-grained intra-class shape retrieval, e.g., retrieving swivel chairs\nfrom a large collection of chairs; see Figure 16 (right).\nThe main idea is to only consider these distinctive point features\nrather than regarding all the point features equally. In detail, we\nrandomly sample N (e.g., 2,048) points on each shape Sj and feed\nthese points into our trained network to obtain local per-point\nfeatures Fr\nj and global per-shape features gj; see Section 3.2. As\ndescribed in Section 3.6, we further obtain per-point distinctiveness\ndi,j from fr\ni,j. To facilitate fine-grained intra-class retrieval, instead\nof directly using gj as the representative descriptor, we select only\nthe more distinctive per-point features and average over them to\nobtain the distinctiveness-guided global feature hj:\nhj =\nÍN\ni=1 I{di,j > ∆d }fr\ni,j\nÍN\ni=1 I{di,j > ∆d }\n,\n(8)\nwhere I is the indicator function and ∆d is the threshold. Lastly, we\nmeasure the similarity between shapes by computing the Euclidean\ndistance between hj of the shapes.\nFigure 16 shows two sets of results, each using a different chair as\nthe query shape. As a comparison, we applied a recent unsupervised\nnetwork FoldingNet [Yang et al. 2018] to extract per-shape global\nfeature for similarity computation, and the results are shown in the\nFig. 17. Top-five shape retrieval results by using a Table shape with four legs\n(top) or using an Airplane shape with a propeller (bottom) as the query to\nsearch over the Chair dataset. Retrieved shapes with similar substructures\nare marked over a yellow background.\ntop row. Besides, we also directly employed our per-shape feature\ngj for retrieval; see the middle row. In each set, we retrieved the\ntop-five similar shapes from a data pool of 100 different chairs,\nand manually marked in yellow (see the figure) the results with\nsubstructures similar to the query shape: for the left example in\nFigure 16, we consider a retrieval result as similar to the query\nshape, if it also has an armrest and four legs as its substructures,\nwhile for the right example in Figure 16, we consider a retrieval\nresult as similar to the query shape, if it also has a swivel base as\nits substructures. From Figure 16, we can see that using the global\ndescriptors, no matter extracted by FoldingNet or our network, the\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: April 2020.\n12\n•\nLi X. et al.\nFig. 18. Using the network-predicted distinctiveness values over a scene, we can find best views with maximized distinctiveness.\nFig. 19. Shape classification accuracy using distinctiveness-guided sampling\n(top row) vs. conventional Poisson disk sampling (bottom row).\nresults have very different substructures from the query shapes.\nGuided by the network-predicted distinctiveness values, hj can help\nretrieve shapes with more similar substructures (marked in yellow).\nPlease see Supplementary Material Part G for more results.\nTo further explore the ability of our distinctiveness-guided shape\nretrieval, we perform shape retrieval on the Chair dataset using a\nTable and an Airplane as the query. Figure 17 shows the results. For\nthe top case, the four legs in the Table shape are found to be dis-\ntinctive, so when using it as the query, four-leg chairs are obtained.\nFor the bottom case, the Airplane shape has a propeller, so chairs\nwith substructures that look like the propeller (distinctive regions)\ncan be retrieved as the results. These results demonstrate that our\ndistinctiveness-guided shape retrieval can pay more attention to the\nlocal distinctive regions, instead of simply the overall structure.\n5.2\nDistinctiveness-guided Sampling\nSampling is a common task in computer graphics, as well as in many\ndomains, for generating point samples to represent a continuous\nshape. Using the distinctiveness detected on 3D shapes, we can\nguide the point sampling process by emphasizing the distinctive\nregions on the shapes. In this way, the sampled points can more\neffectively describe the shapes in terms of discrimination ability.\nIn our implementation, we take the point distinctiveness values\ndetected by our network to control the local sampling density in\nan adaptive Poisson disk sampling process. That is, we set higher\nsampling density (or equivalent, smaller Poisson disks) for more\ndistinctive regions, and vice versa. Figure 19 presents the sampling\nresults on a four-engine airplane object with decreasing number of\npoints, where (i) the top row shows the results produced using an\nadaptive Poisson disk sampling guided by the network-predicted\ndistinctiveness values, and (ii) the bottom row shows the results\nproduced by the conventional Poisson disk sampling, which ran-\ndomly but uniformly samples the given object. From the results, we\ncan see that distinctiveness-guided sampling (top) arranges more\npoints in high distinctive regions, such as the engines, thereby en-\nhancing the preservation of the shape’s characteristics. Please refer\nto Supplementary Material Part H for more sampling results.\nFurthermore, we compare the two-class classification accuracy\nfor the sampled point sets using our unsupervised network, and\npresent the overall shape classification accuracy as plots in Fig-\nure 19. In this quantitative comparison, we can further show that\nthe points produced from distinctiveness-guided sampling lead to\nhigher classification accuracy, even with fewer points.\n5.3\nView Selection in 3D Scenes\nGiven a 3D scene, where are the distinctive regions to attend to?\nUsing our unsupervised framework, we can find the distinctive\nregions in an input 3D scene and locate the best views to look at these\nregions. Here, we define the best views as those with maximized\ndistinctiveness displayed in the views.\nTo find such views, we first sample local patches of 2,048 points\nin the input scene, and feed these patches as inputs to our network\nto predict per-point distinctiveness. Since our network is trained on\nindividual objects, we crop regions of around one-meter diameter\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: April 2020.\nUnsupervised Detection of Distinctive Regions on 3D Shapes\n•\n13\nFig. 20. Asymmetry in the detected distinctiveness.\nin the scene for point sampling. Then, we can combine the results\nfrom the local patches to obtain distinctiveness over the scene. Next,\nwe generate a set of candidate views by uniformly sampling 50\ndifferent views on the upper hemisphere that bounds the scene, and\nevaluate the quality of each view by averaging the distinctiveness\nover the visible points in the view. Lastly, we choose the view with\nthe highest averaged distinctiveness as the best view.\nFigures 18 (a) & (b) show the best views that were automatically\nselected for two different scenes (courtesy of 3D Warehouse [2019]),\nwhere the camera was set to look at the center of the scene when\nsampling the candidate views. As a comparison, we also show the\ncorresponding worst views for the two scenes; see (c) & (d). From\nthese results we can see that, the selected best views can reasonably\npresent most distinctive regions in the views and contain rich in-\nformation of the scene, while the worst views exhibit very limited\ninformation. Besides setting the camera to look at the whole scene,\nwe can set it to look at specific areas or distinctive regions in the\nscene, and find local best views, meaning that we consider only the\ndistinctive regions in the user-specified area when searching for\nthe best view with maximized distinctiveness. The red and blue\nboxes in Figure 18 (a) mark two example areas, while Figures 18\n(e) & (f) present the corresponding local best views found in the\nareas. Please refer to Supplementary Material Part I for more view\nselection results.\n6\nCONCLUSION AND FUTURE WORK\nWe presented a technique to learn and detect distinctive regions on\n3D shapes. The technique analyzes a given set of objects without\nany supervision. The shapes are represented by point clouds, and the\nanalysis is performed by a deep neural network that learns per-point\nand per-shape features from the point clouds. Further, we formulate\nthe unsupervised joint loss for a shape clustering task of the per-\nshape features, thereby implicitly encouraging the network to learn\nthe distinctiveness of the per-point features relative to the shapes in\ndifferent clusters. We demonstrated the effectiveness of our method\nvia extensive experiments, and presented several applications based\non the network-predicted distinctiveness.\nDespite the promising performance that our method has achieved,\none limitation is that insufficient training samples or severely un-\neven class size would certainly affect the network’s classification\ncapability. However, such requirement on training data also appears\nin typical deep-learning-based methods. On the other hand, our\nmethod may detect asymmetrical distinctive regions on shapes; see\nFigure 20. This may be related to our current distinctiveness extrac-\ntion method, which is rather local when extracting the per-point\ndistinctiveness. In the future, we plan to integrate the prior knowl-\nedge of shape symmetry into our current analysis framework when\nextracting the distinctiveness. While distinction and saliency are\ndifferent (as explained in the introduction), we may further explore\ntheir relationship by taking a machine learning approach. Besides,\narmed with the distinctiveness analysis, in our future work, we are\nconsidering generating novel shapes with control on their distinc-\ntive features, making them more inter-class distinct and possibly\nmore intra-class distinct, thereby enriching the variability within\nthe class, while remaining distinct to the other classes. Generally\nspeaking, we believe that a stronger and better set analysis will lead,\nin the future, to better synthesis of 3D shapes.\nACKNOWLEDGMENTS\nWe thank all the anonymous reviewers for their comments and feed-\nback. We also acknowledge help from our volunteers for conducting\nuser studies. This work was supported by grants from the Research\nGrants Council of the Hong Kong Special Administrative Region\n(Project no. CUHK 14201717 and 14201918), the CUHK Research\nCommittee Direct Grant for Research 2018/19, and the Israel Science\nFoundation as part of the ISF-NSFC joint program (grant number\n2217/15, 2472/17). The work was also partially supported by ISF\ngrant 2366/16.\nREFERENCES\nMartin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean,\nMatthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath\nKudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit\nSteiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and\nXiaoqiang Zheng. 2016. TensorFlow: A system for large-scale machine learning. In\n12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16).\n265–283. https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf\nPierre Alliez, Mark Meyer, and Mathieu Desbrun. 2002. Interactive geometry remeshing.\nACM Transactions on Graphics (SIGGRAPH) 21, 3 (2002), 347–354.\nMarco Ancona, Enea Ceolini, Cengiz Oztireli, and Markus Gross. 2018. Towards better\nunderstanding of gradient-based attribution methods for deep neural networks. In\nInternational Conference on Learning Representations (ICLR).\nYasuhiro Aoki, Hunter Goforth, Rangaprasad Arun Srivatsan, and Simon Lucey. 2019.\nPointNetLK: Robust & efficient point cloud registration using PointNet. In IEEE\nConference on Computer Vision and Pattern Recognition (CVPR). 7163–7172.\nPhilip Bachman, R Devon Hjelm, and William Buchwalter. 2019. Learning representa-\ntions by maximizing mutual information across views. In International Conference\non Neural Information Processing Systems (NIPS). 15509–15519.\nUmberto Castellani, Marco Cristani, Simone Fantoni, and Vittorio Murino. 2008. Sparse\npoints matching by combining 3D mesh saliency with statistical descriptors. Com-\nputer Graphics Forum (Eurographics) 27, 2 (2008), 643–652.\nXiaobai Chen, Abulhair Saparov, Bill Pang, and Thomas Funkhouser. 2012. Schelling\npoints on 3D surface meshes. ACM Transactions on Graphics (SIGGRAPH) 31, 4\n(2012), 29:1–29:12.\nCarl Doersch, Saurabh Singh, Abhinav Gupta, Josef Sivic, and Alexei Efros. 2012. What\nmakes Paris look like Paris? ACM Transactions on Graphics (SIGGRAPH) 31, 4 (2012),\n101:1–101:9.\nHelin Dutagaci, Chun Pan Cheung, and Afzal Godil. 2012. Evaluation of 3D interest\npoint detection techniques via human-generated ground truth. The Visual Computer\n28, 9 (2012), 901–917.\nRan Gal and Daniel Cohen-Or. 2006. Salient geometric features for partial shape\nmatching and similarity. ACM Transactions on Graphics 25, 1 (2006), 130–150.\nMichael Garland and Paul S. Heckbert. 1997. Surface simplification using quadric error\nmetrics. In Proceedings of SIGGRAPH. 209–216.\nRaia Hadsell, Sumit Chopra, and Yann LeCun. 2006. Dimensionality reduction by\nlearning an invariant mapping. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR). 1735–1742.\nOlivier J. Hénaff, Ali Razavi, Carl Doersch, S. M. Eslami, and Aaron van den Oord. 2019.\nData-efficient image recognition with contrastive predictive coding. arXiv preprint\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: April 2020.\n14\n•\nLi X. et al.\narXiv:1905.09272 (2019).\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a\nneural network. arXiv preprint arXiv:1503.02531 (2015).\nR Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman,\nAdam Trischler, and Yoshua Bengio. 2019. Learning deep representations by mutual\ninformation estimation and maximization. In International Conference on Learning\nRepresentations (ICLR).\nBinh-Son Hua, Minh-Khoi Tran, and Sai-Kit Yeung. 2018. Pointwise convolutional\nneural networks. In IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR). 984–993.\nMayank Juneja, Andrea Vedaldi, C.V. Jawahar, and Andrew Zisserman. 2013. Blocks\nthat shout: Distinctive parts for scene classification. In IEEE Conference on Computer\nVision and Pattern Recognition (CVPR). 923–930.\nDiederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization.\narXiv preprint arXiv:1412.6980 (2014).\nTruc Le and Ye Duan. 2018. PointGrid: A deep network for 3D shape understanding. In\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR). 9204–9214.\nChang Ha Lee, Amitabh Varshney, and David W. Jacobs. 2005. Mesh saliency. ACM\nTransactions on Graphics (SIGGRAPH) 24, 3 (2005), 659–666.\nGeorge Leifman, Elizabeth Shtrom, and Ayellet Tal. 2012. Surface regions of interest for\nviewpoint selection. In IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR). 414–421.\nYangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. 2018.\nPointCNN: Convolution on X-transformed points. In International Conference on\nNeural Information Processing Systems (NIPS). 828–838.\nYu Liu, Guanglu Song, Jing Shao, Xiao Jin, and Xiaogang Wang. 2018. Transductive cen-\ntroid projection for semi-supervised large-scale recognition. In European Conference\non Computer Vision (ECCV). 70–86.\nFlora Ponjou Tasse, Jiri Kosinka, and Neil Dodgson. 2015. Cluster-based point set\nsaliency. In IEEE International Conference on Computer Vision (ICCV). 163–171.\nCharles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. 2017a. PointNet: Deep\nlearning on point sets for 3D classification and segmentation. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR). 652–660.\nCharles R. Qi, Li Yi, Hao Su, and Leonidas J. Guibas. 2017b. PointNet++: Deep hierar-\nchical feature learning on point sets in a metric space. In International Conference\non Neural Information Processing Systems (NIPS). 5099–5108.\nRamprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam,\nDevi Parikh, and Dhruv Batra. 2017. Grad-CAM: Visual explanations from deep net-\nworks via gradient-based localization. In IEEE International Conference on Computer\nVision (ICCV). 618–626.\nYiru Shen, Chen Feng, Yaoqing Yang, and Dong Tian. 2018. Mining point cloud local\nstructures by kernel correlation and graph pooling. In IEEE Conference on Computer\nVision and Pattern Recognition (CVPR). 4548–4557.\nPhilip Shilane and Thomas Funkhouser. 2006. Selecting distinctive 3D shape descriptors\nfor similarity retrieval. In IEEE Intl. Conf. on Shape Modeling and Applications (SMI).\n18:1–18:10.\nPhilip Shilane and Thomas Funkhouser. 2007. Distinctive regions of 3D surfaces. ACM\nTransactions on Graphics 26, 2 (2007), 7:1–7:15.\nPhilip Shilane, Patrick Min, Michael Kazhdan, and Thomas Funkhouser. 2004. The\nPrinceton shape benchmark. In IEEE Intl. Conf. on Shape Modeling and Applications\n(SMI). 167–178.\nAvanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning Important\nFeatures Through Propagating Activation Differences. In Proceedings of International\nConference on Machine Learning (ICML). 3145–3153.\nElizabeth Shtrom, George Leifman, and Ayellet Tal. 2013. Saliency detection in large\npoint sets. In IEEE International Conference on Computer Vision (ICCV). 3591–3598.\nZhenyu Shu, Shiqing Xin, Xin Xu, Ligang Liu, and Ladislav Kavan. 2019. Detecting 3D\npoints of interest using multiple features and stacked auto-encoder. IEEE Transactions\nVisualization & Computer Graphics 25, 8 (2019), 2583–2596.\nSaurabh Singh, Abhinav Gupta, and Alexei A. Efros. 2012. Unsupervised discovery\nof mid-level discriminative patches. In European Conference on Computer Vision\n(ECCV). 73–86.\nRan Song, Yonghuai Liu, and Paul Rosin. 2018. Distinction of 3D objects and scenes via\nclassification network and Markov random field. IEEE Transactions Visualization &\nComputer Graphics (2018), To appear.\nX. Yu Stella and Jianbo Shi. 2003. Multiclass spectral clustering. In IEEE International\nConference on Computer Vision (ICCV). 313–320.\nHang Su, Varun Jampani, Deqing Sun, Subhransu Maji, Evangelos Kalogerakis, Ming-\nHsuan Yang, and Jan Kautz. 2018. SPLATNet: Sparse lattice networks for point cloud\nprocessing. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n2530–2539.\nJian Sun and Jean Ponce. 2013. Learning discriminative part detectors for image\nclassification and cosegmentation. In IEEE International Conference on Computer\nVision (ICCV). 3400–3407.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic Attribution for Deep\nNetworks. In Proceedings of International Conference on Machine Learning (ICML).\n3319–3328.\nJohan W. H. Tangelder and Remco C. Veltkamp. 2004. A survey of content based 3D\nshape retrieval methods. In IEEE Intl. Conf. on Shape Modeling and Applications (SMI).\n145–156.\nUlrike Von Luxburg. 2007. A tutorial on spectral clustering. Statistics and computing\n17, 4 (2007), 395–416.\nXi Wang, Sebastian Koch, Kenneth Holmqvist, and Marc Alexa. 2018. Tracking the\ngaze on objects in 3D: how do people really look at the Bunny? ACM Transactions\non Graphics (SIGGRAPH Asia) 37, 6 (2018), 188:1–188:18.\nYaming Wang, Jonghyun Choi, Vlad Morariu, and Larry S. Davis. 2016. Mining dis-\ncriminative triplets of patches for fine-grained classification. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR). 1163–1172.\nYue Wang and Justin M. Solomon. 2019. Deep Closest Point: Learning Representations\nfor Point Cloud Registration. In IEEE International Conference on Computer Vision\n(ICCV). 3523–3532.\nYue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M.\nSolomon. 2019. Dynamic graph CNN for learning on point clouds. ACM Transactions\non Graphics 38, 5 (2019), 146:1–146:12.\n3D Warehouse. 2019. https://3dwarehouse.sketchup.com/ [Online; accessed 02-Jan-\n2019].\nYandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. 2016. A discriminative feature\nlearning approach for deep face recognition. In European Conference on Computer\nVision (ECCV). 499–515.\nSanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. 2018. CBAM:\nConvolutional block attention module. In European Conference on Computer Vision\n(ECCV). 3–19.\nZhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and\nJianxiong Xiao. 2015. 3D ShapeNets: A deep representation for volumetric shapes.\nIn IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 1912–1920.\nZhirong Wu, Yuanjun Xiong, X. Yu Stella, and Dahua Lin. 2018. Unsupervised feature\nlearning via non-parametric instance discrimination. In IEEE Conference on Computer\nVision and Pattern Recognition (CVPR). 3733–3742.\nYifan Xu, Tianqi Fan, Mingye Xu, Long Zeng, and Yu Qiao. 2018. SpiderCNN: Deep\nlearning on point sets with parameterized convolutional filters. In European Confer-\nence on Computer Vision (ECCV). 90–105.\nYaoqing Yang, Chen Feng, Yiru Shen, and Dong Tian. 2018. FoldingNet: Point cloud\nauto-encoder via deep grid deformation. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR). 206–215.\nLequan Yu, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, and Pheng-Ann Heng. 2018.\nEC-Net: an Edge-aware Point set Consolidation Network. In European Conference\non Computer Vision (ECCV). 398–414.\nMatthew D. Zeiler and Rob Fergus. 2014. Visualizing and understanding convolutional\nnetworks. In European Conference on Computer Vision (ECCV). 818–833.\nJianming Zhang, Sarah Adel Bargal, Zhe Lin, Jonathan Brandt, Xiaohui Shen, and Stan\nSclaroff. 2018. Top-down neural attention by excitation backprop. International\nJournal Computer Vision 126, 10 (2018), 1084–1102.\nXiaoting Zhang, Xinyi Le, Athina Panotopoulou, Emily Whiting, and Charlie C. L. Wang.\n2015. Perceptual models of preference in 3D printing direction. ACM Transactions\non Graphics (SIGGRAPH Asia) 34, 6 (2015), 215:1–215:12.\nBolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. 2016.\nLearning deep features for discriminative localization. In IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR). 2921–2929.\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: April 2020.\n",
  "categories": [
    "cs.GR",
    "cs.CV"
  ],
  "published": "2019-05-05",
  "updated": "2020-04-20"
}