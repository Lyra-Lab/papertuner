{
  "id": "http://arxiv.org/abs/1905.01974v1",
  "title": "A Novel Task-Oriented Text Corpus in Silent Speech Recognition and its Natural Language Generation Construction Method",
  "authors": [
    "Dong Cao",
    "Dongdong Zhang",
    "HaiBo Chen"
  ],
  "abstract": "Millions of people with severe speech disorders around the world may regain\ntheir communication capabilities through techniques of silent speech\nrecognition (SSR). Using electroencephalography (EEG) as a biomarker for speech\ndecoding has been popular for SSR. However, the lack of SSR text corpus has\nimpeded the development of this technique. Here, we construct a novel\ntask-oriented text corpus, which is utilized in the field of SSR. In the\nprocess of construction, we propose a task-oriented hybrid construction method\nbased on natural language generation algorithm. The algorithm focuses on the\nstrategy of data-to-text generation, and has two advantages including\nlinguistic quality and high diversity. These two advantages use template-based\nmethod and deep neural networks respectively. In an SSR experiment with the\ngenerated text corpus, analysis results show that the performance of our hybrid\nconstruction method outperforms the pure method such as template-based natural\nlanguage generation or neural natural language generation models.",
  "text": "A Novel Task-Oriented Text Corpus in Silent Speech \nRecognition and its Natural Language Generation \nConstruction Method \nDong Cao \nDeepBlue Technology (Shanghai) Co., Ltd. \nAddr: No.369,Weining Road,  \nChangning District, Shanghai, China \nDeepBlue Academy of Sciences \nDeepBlue Institute, Chongqing, China \ncaodong@deepblueai.com \nDongdong Zhang* \nDeepBlue Academy of Sciences \n DeepBlue Technology (Shanghai)Co., Ltd. \nAddr: No.369,Weining Road,  \nChangning District, Shanghai, China \nzhangdongdong@deepblueai.com \n HaiBo Chen* \nDeepBlue Technology (Shanghai)Co., Ltd. \nAddr: No.369,Weining Road,  \nChangning District, Shanghai, China \nchenhaibo@deepblueai.com \n \nABSTRACT \nMillions of people with severe speech disorders around the world \nmay regain their communication capabilities through techniques of \nsilent speech recognition (SSR). Using electroencephalography \n(EEG) as a biomarker for speech decoding has been popular for \nSSR. However, the lack of SSR text corpus has impeded the \ndevelopment of this technique. Here, we construct a novel task-\noriented text corpus, which is utilized in the field of SSR. In the \nprocess of construction, we propose a task-oriented hybrid \nconstruction method based on natural language generation \nalgorithm. The algorithm focuses on the strategy of data-to-text \ngeneration, and has two advantages including linguistic quality and \nhigh diversity. These two advantages use template-based method \nand deep neural networks respectively. In an SSR experiment with \nthe generated text corpus, analysis results show that the \nperformance of our hybrid construction method outperforms the \npure method such as template-based natural language generation or \nneural natural language generation models. \nKeywords \nnatural language generation; silent speech recognition; deep \nneural networks \n1. INTRODUCTION \nAcoustic speech recognition (ASR) has already become one of the \nmost successful techniques in the era of artificial intelligence (AI) \n[1], [2]. However, millions of people around the world who lost \ntheir abilities to speak cannot enjoy the convenience of ASR [3]. \nUnder certain circumstances such as large background noise or high \nconfidential communications, ASR may lose its advantage or even \ndoesnâ€™t work [4].  \nOne possible solution is the silent speech recognition (SSR) which \nis an alternative for these situations [5], [6]. Many biological \nindicators has been used as the input for the SSR among which \nelectroencephalography (EEG) is the most promising one [7]â€“[11]. \nEEG is a measure of electrical potentials between different \nlocations on the skull. Krishna et.al showed that using EEG can \nhelp ASR systems overcome performance loss in the presence of \nnoise [12]. Some studies have also reported that it is possible to \ndecipher imagined speech directly through EEG [13]. Although a \nhuge progress has been made in EEG-based SSR, it is impossible \nto evaluated the performances of different systems due to the lack \nof a consensus text corpus for SSR. \nIn the process of constructing EEG-based SSR text data sets, \nvarious NLP methods play an important role. Many methods for \nattention and neural sequence-to-sequence framework have been \nproposed [14]-[19]. A method has been proposed that of word-\nbased and character-based sequence-to-sequence models[20]. This \nmodel is utilized for data-to-text natural language generation. \nBERT has been successfully applied to a variety of natural \nlanguage processing tasks such as name entity recognition and \nmachine reading comprehensions[21]-[25]. The literature [26] first \nproposed to extend the BERT to the sequence generation task, and \nconstructed a new natural language generation model based on the \npre-training language model. In order to take full advantage of the \nBERT's context modeling capabilities, the proposed method uses a \ntwo-stage decoding process. \n2. TEXT CORPUS FOR SSR \nIn the field of general speech recognition, there are a large number \nof mature text corpus available. These texts are organized and \ncomplete. The content covers all aspects and application fields of \ndaily conversation. Based on these extensive text corpus, it can be \nfurther constructed a rich speech training set for general speech \nrecognition. \nDifferent from the general speech recognition application, the \ndesign and construction of training corpus is a challenging task in \nthe field of SSR, because Using EEG as a biomarker for speech \ndecoding has been popular for SSR. We want to build an SSR \nsystem based on EEG. Then, there is no ready-made â€˜EEG-text pair' \ndata set that can be used for model training. We must build such a \ndata set almost from zero. Quantitatively, if you want to build an \navailable system similar to general speech recognition, then the \namount of data required must be cumulative tens of thousands of \nhours of 'EEG-text pair' data sets. For most academic teams, this is \na daunting task that requires a lot of manpower, material and time.  \n3. HYBRID MODELS FOR TEXT CORPUS \n3.1 Task-Oriented Text Corpus \nIn order to solve the above dilemma, we can choose a compromise \napproach, considering that in application, the essential needs of \nSSR are those patients who lose their throat vocal function due to \nillness. Their outstanding scenario for SSR applications is life \nsupport. Therefore, based on this consideration, we avoid the need \nfor a massive â€˜EEG-text pairâ€™ data set from the generic scenario \nSAMPLE: Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that \ncopies are not made or distributed for profit or commercial advantage and \nthat copies bear this notice and the full citation on the first page. To copy \notherwise, or republish, to post on servers or to redistribute to lists, \nrequires prior specific permission and/or a fee. \nConferenceâ€™10, Month 1â€“2, 2010, City, State, Country. \nCopyright 2010 ACM 1-58113-000-0/00/0010 â€¦$15.00. \nDOI: http://dx.doi.org/10.1145/12345.67890 \n* Corresponding author \nSSR. We start with the help of this task-oriented scene to construct \nthe task-oriented text corpus. In this way, the demand for data set \nsize is significantly reduced. Therefore, we need to construct \nsession texts that serve the life support class in a targeted manner. \nSuch texts will be further applied to the EEG signal data collection \nprocess to construct a task-oriented â€˜EEG-text pairâ€™ data set. \nThe characteristics of the life-assisted task-oriented session text is \nthat the task-oriented session has a clear purpose relative to the \nopen domain session, requiring high language quality and clear \nstructure. Moreover, with the language habits and language styles \nof different users, the language presents richness and diversity in a \ntask-oriented session. The special requirements of this kind of task-\noriented session text construction bring new challenges to the \ndesign and construction of our data set. \nwe adopt a task-oriented corpus hybrid construction method based \non natural language generation algorithm. The algorithm focuses \non the strategy of data-to-text generation[20], and has the following \ntwo advantages, the first one is linguistic quality based on template-\nbased method and the second is high diversity based on deep neural \nnetworks. \n3.2 Task-Oriented Hybrid Models \nThe literature [20] based on the neural network method realizes the \ndata-to-text natural langual generation. Inspired by this method, we \nconstruct a task-oriented hybrid construction method based on \nnatural language generation algorithm. This method is used to \ngenerate task-oriented text corpus in the field of SSR. The \nalgorithm includes two aspects, first constructing the seed text \ncorpus, then generate task-oriented text corpus based on the seed \ntext corpus using the deep neural network method. \nThe task-oriented hybrid models are used to construct the corpus. \nSpecific steps are as follows. \nStep 1. According to the requirements applied to the life aid class, \ndesign the corresponding cycle template, the template needs to be \nconcise and regular, and can basically cover the life assisted scene, \nwith the focus on the relevant high frequency words. \nStep 2. Construct the seed text corpus ğ¶\"##$ based on the cycle \ntemplate of natural language generation method. \nStep 3. The scale of the seed text corpus needs to be properly \ncontrolled. The corpus quality is used as a criterion to control the \nscale, and the scale adjustment can be returned to the step 2 iterative \nupdate. \nStep 4. The step 3 result is judged manually, mainly due to the \nfollowing considerations. The scale of the seed text corpus here is \nstrictly controlled to a small extent, and we have relatively high \nquality requirements for the seed text corpus. \nStep 5. Using the seed textcorpus as a training set, training based \non the deep neural network model, the model ğ‘€& is obtained. \nStep 6. Based on the model ğ‘€&. Further inference, get the final \nmeet-required task-oriented corpus ğ¶'(. \nStep 7. The task-oriented corpus ğ¶'( generated in step6 may not \nnecessarily meet the requirements in some cases. For example, in \nthe high diversity aspect, we need to go back to step5 and further \niterate and optimize until it meets the requirements. \n3.3 Constructing Seeds Text Corpus \n \n \nFigure 1. seed lexicon \n \nIn this section, we introduce the method of constructing the seed \nText Corpus. \nStep 1. Limit the scope of life scenes \nStep 2. According to the word frequency, extract common nouns, \nverbs, adjectives and quantifiers from the established scenes to \nform a seed lexicon, as shown in Figure 1. \nStep 3. According to the rule structure of grammatical subject, \npredicate and object, the corresponding vocabulary is extracted \nfrom the seed lexicon to form a sentence. \nStep 4. The statement is expanded to include appropriate \nquantifiers and adjectives before the corresponding nouns. \nExample: \na1. I want to wear a coat \na2. I want to wear a long coat \na3. I want to wear a long coat with velvet \nb1. He wants to drink water \nb2. He wants to drink a glass of water \nb3. He wants to drink a cup of warm water. \nb4. He wants to drink a cup of warm water with sugar. \n3.4 Constructing Text Corpus Based on Deep \nNeural Networks NLG Method \nIn this section, we designed the NLG model to implement non-fixed \nlength input and generate non-fixed length output text. We need to \nconstruct the sequence-to-sequence structure. First implement \nembeddings for inputting non-fixed length text.  \nWe use BERT[25][26] to implement the encoding function, where \nthe BERT stands for means bidirectional encoder representations \nfrom transformers. In the process of using BERT, Due to the use of \nlanguage models for transformer learning, empirical improvements \nhave shown that unsupervised pre-training is an important integral \npart of many natural language processing systems. The BERT \nmethod suggests that a general language model may exist. Prior to \nthis, when people solved NLP problems, they often customized \nspecific models for specific problems. Howerver, BERT \nsuccessfully solves a wide range of NLP tasks. Based on the above \ndescription,  our structure is shown in Figure 2. \n \nFigure 2. Embedding based on BERT \n \nBased on the idea of these literature [20][27]-[29], We implement \nthe natural language generation transformer model for specific text \nto text.  Furthermore, we have adopted a method based on these \ndocuments[30]-[33]. The detailed process is described below. The \nbasic structure of LSTM is as follows. Where ğœ*, ğœ*, ğœ‰* and ğœ“* \nrepresent input gate, forget gate, output gate and memory cell, \nrespectively.  \n \nğœ* = Ïƒ0Î2â„œ*45 + â„‘2ğœ—9: + ğ›½2< \nğœ* = Ïƒ(Î>â„œ*45 + â„‘>ğœ—9: + ğ›½>) \nğœ‰* = Ïƒ(Î@â„œ*45 + â„‘@ğœ—9: + ğ›½@) \nğœ‘* = tanh(ÎFâ„œ*45 + â„‘Fğœ—9: + ğ›½F) \nÏˆ* = ğœ* â¨€ Ïˆ*45 + ğœ* â¨€ ğœ‘* \nâ„œ* = ğœ‰* â¨€ ğ‘¡ğ‘ğ‘›â„(Ïˆ*) \n \nWhere ğœ—9: stand for the embedding of input. Î and â„‘ stand for \ncorresponding matrix, and  Î² represent bias. They are all trainable \nparameters. â„œ* stand for the hidden state of LSTM. \nUtilizing bi-directional LSTM structure[31], we can extract text \ncontext information from both forward and backward directions. \nThe hidden state can be expressed as follows. \n \nâ„œ* = â„œO: â¨ â„œQ: \n \nwhere â„œO: and â„œQ: represent hidden state of backward direction \nand hidden state of forward direction, respectively. and â¨ stands \nfor the concat operation. \nOutput text sequence â„’5,â„’T,. . . , â„’( ï¼Œhas a defined generation \nrule[20][34]. The elements â„’V, (ğ‘˜= 1,2,â‹¯, ğ‘) are generated as \nfollows. \n \n       p(â„’V|â„’5â„’T â‹¯â„’V45) = ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(W( \n                                             âˆ‘\n((ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘ Vğ’²â„œ#))\nf\n#g5\nâ„œ#))) \n   \nWhere ğ‘ V has the following representation. \n \n            ğ‘ V = ğ‘ğ‘–~ğ¿ğ‘†ğ‘‡ğ‘€((ğ‘Šop*(â„’V45) \n                     â¨âˆ‘\n((ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘ V45ğ’²â„œ#))\nf\n#g5\nâ„œ#)),ğ‘ V45) \n \nWherein, input text sequence ğœ—9 = (ğœ—9q, ğœ—9r, â‹¯, ğœ—9s ). where \nW , ğ’² and ğ‘Šop*  represent output matrix, attention matrix and \noutput embedding matrices, respectively. \n \n4. RESULTS AND ANALYSIS \nBased on the methods presented in Sections 3.3 and 3.4, we \ncombine the above two methods to construct a hybrid algorithm for \ntask-oriented corpus. \nTask-oriented hybrid model can learn to express structured input in \nan appropriate way. These advantages mainly come from the \nexquisite and elegant seeds text corpus design and deep neural \nnetworks natural language generation method. The hybrid model \nhas two advantages including linguistic quality and high diversity. \nIn the field of SSR experiment, analysis results show that the \nperformance of our hybrid model outperforms the pure method \nsuch as template-based natural language generation or neural \nnatural language generation models. \n5. ACKNOWLEDGMENTS \nWe are very grateful to DeepBlue Technology (Shanghai) Co., Ltd. \nand DeepBlue Academy of Sciences for their support.Thanks to the \nsupport of the Equipment pre-research project (No. 31511060502). \n6. REFERENCES \n[1] Bowman, M., Debray, S. K., and Peterson, L. L. 1993.A. B. \nNassif, I. Shahin, I. Attili, M. Azzeh, and K. Shaalan. Speech \nRecognition Using Deep Neural Networks: A Systematic \nReview. IEEE Access, vol. 7, pp. 19143â€“19165, 2019. \n[2] Z. Ling et al.. Deep Learning for Acoustic Modeling in \nParametric Speech Generation: A systematic review of \nexisting techniques and future trends. IEEE Signal \nProcess. Mag., vol. 32, no. 3, pp. 35â€“52, May 2015. \n[3] G. Sharpe, V. Camoes Costa, W. DoubÃ©, J. Sita, C. McCarthy, \nand P. Carding. Communication changes with \nlaryngectomy and impact on quality of life: a review. Qual. \nLife Res., Nov. 2018. \n[4] J. Rajnoha and P. PollÃ¡k. ASR systems in Noisy \nEnvironment: Analysis and Solutions for Increasing Noise \nRobustness. vol. 20, no. 1, p. 11, 2011. \n[5] B. Denby, T. Schultz, K. Honda, T. Hueber, J. M. Gilbert, and \nJ. S. Brumberg. Silent speech interfaces. Speech Commun., \nvol. 52, no. 4, pp. 270â€“287, Apr. 2010. \n[6] M. J. Fagan, S. R. Ell, J. M. Gilbert, E. Sarrazin, and P. M. \nChapman. Development of a (silent) speech recognition \nsystem for patients following laryngectomy. Med. Eng. \nPhys., vol. 30, no. 4, pp. 419â€“425, May 2008. \n[7] T. Hueber, E.-L. Benaroya, G. Chollet, B. Denby, G. Dreyfus, \nand M. Stone. Development of a silent speech interface \ndriven by ultrasound and optical images of the tongue \nand lips. Speech Commun., vol. 52, no. 4, pp. 288â€“300, Apr. \n2010. \n[8] R. Hofe et al.. Small-vocabulary speech recognition using \na silent speech interface based on magnetic sensing. \nSpeech Commun., vol. 55, no. 1, pp. 22â€“32, Jan. 2013. \n[9] G. N. Sonawane, A. N. Shewale, and G. G. Gujarathi. A \nReview on Modalities of EMG Based Speech Recognition \na1. I want to wear a coat\na2. I want to wear a long coat\na3. I want to wear a long coat with velvet\nb1. He wants to drink water\nb2. He wants to drink a glass of water\nb3. He wants to drink a cup of warm water.\nb4. He wants to drink a cup of warm water with sugar.\n\u0001\u0002\u0003\u0004\n\u0004\u0007\r\f \u0002\n\u0005\u0007\u0006\u0006\t\u000b\b\nfor Silent Speech Interface. p. 6, International Journal of \nEngineering Science and Computing, July 2016. \n[10] Richard Li, Jason Wu, Thad Starner. 2019. TongueBoard: An \nOral Interface for Subtle Input. In Augmented Human \nInternational Conference 2019 (AH2019), March 11â€“12, \n2019, Reims, France. ACM, New York, NY, USA, Article 4, \n9 pages. https://doi.org/10.1145/3311823.3311831 \n[11] H. Akbari, B. Khalighinejad, J. L. Herrero, A. D. Mehta, and \nN. Mesgarani. Towards reconstructing intelligible speech \nfrom the human auditory cortex. Sci. Rep., vol. 9, no. 1, \nDec. 2019. \n[12] G. Krishna, C. Tran, J. Yu, and A. H. Tewfik. Speech \nRecognition with no speech or with noisy speech. Mar. \n2019. \n[13] C. H. Nguyen, G. K. Karavas, and P. Artemiadis. Inferring \nimagined speech using EEG signals: a new approach using \nRiemannian manifold features.  J. Neural Eng., vol. 15, no. \n1, p. 016002, Feb. 2018. \n[14] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. \nNeural machine translation by jointly learning to align and \ntranslate. arXiv preprint arXiv:1409.0473, 2014.  \n[15] Abigail See, Peter J. Liu, and Christo- pher D. Manning. Get \nto the point: Summarization with pointer-generator networks. \nIn Proceedings of the 55th Annual Meeting of the \nAssociation for Computational Lin- guistics, ACL, pages \n1073â€“1083, 2017.  \n[16] Sebastian Gehrmann, Yuntian Deng, and Alexander M Rush. \nBottom-up abstractive summarization. arXiv preprint \narXiv:1808.10792, 2018.  \n[17] Wei Li, Xinyan Xiao, Yajuan Lyu, and Yuanzhuo Wang. \nImproving Neural Abstractive Document Summarization \nwith Explicit Information Selection Modeling. In EMNLP, \npages 1787â€“1796, 2018.  \n[18] Romain Paulus, Caiming Xiong, Richard Socher, and Palo \nAlto. A deep reinforced model for abstractive \nsummarization. ICLR, pages 1â€“13, 2018.  \n[19] Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Peihao Su, \nDavid Vandyke, and Steve J. Semantically Conditioned \nLSTM-based Natural Language Generation for Spoken \nDialogue Systems. In Proceedings of the 2015 Conference on \nEmpirical Methods in Natural Language Processing, EMNLP \n2015, pages 1711â€“1721, Lisbon, Por- tugal. \n[20] Glorianna Jagfeld, Sabrina Jenne, Ngoc Thang Vu. \nSequence-to-Sequence Models for Data-to-Text Natural \nLanguage Generation: Word- vs. Character-based Processing \nand Output Diversity. arXiv preprint arXiv: 1810.04864, \n2018. \n[21] Chris Alberti, Kenton Lee, Michael Collins. A BERT \nBaseline for the Natural Questions. arXiv preprint \narXiv:1901.08634. 2019 \n[22] Alex Wang, Kyunghyun Cho. BERT has a Mouth, and It \nMust Speak: BERT as a Markov Random Field Language \nModel. arXiv preprint arXiv:1902.04094 .2019 \n[23] Qian Chen, Zhu Zhuo, Wen Wang. BERT for Joint Intent \nClassification and Slot Filling. arXiv preprint \narXiv:1902.10909. 2019 \n[24] Chanwoo Jeong, Sion Jang, Hyuna Shin, Eunjeong \nPark, Sungchul Choi. A Context-Aware Citation \nRecommendation Model with BERT and Graph \nConvolutional Networks. arXiv preprint arXiv:1903.06464. \n2019 \n[25] Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina \nToutanova. BERT: Pre-training of Deep Bidirectional \nTransformers for Language Understanding. arXiv preprint \narXiv:1810.04805. 2018 \n[26] Haoyu Zhang, Yeyun Gong, Yu Yan, Nan Duan, Jianjun Xu, \nJi Wang , Ming Gong and Ming Zhou. Pretraining-Based \nNatural Language Generation for Text Summarization. arXiv \npreprint arXiv: 1902.09243,2019. \n[27] Kyunghyun Cho, Bart Van Merrie Ìˆnboer, C Ì§ alar Gu Ìˆlc Ì§ehre, \nDzmitry Bahdanau, Fethi Bougares, Hol- ger Schwenk, and \nYoshua Bengio. Learning phrase representations using rnn \nencoderâ€“decoder for statistical machine translation. In \nProceedings of the 2014 Conference on Empirical Methods \nin Nat- ural Language Processing (EMNLP), pages 1724â€“ \n1734, Doha, Qatar.  \n[28] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob \nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, \nIllia Polosukhin. Attention Is All You Need. \narXiv:1706.03762 \n[29] Zhiheng Huang, Wei Xu, Kai Yu. Bidirectional LSTM-CRF \nModels for Sequence Tagging. arXiv:1508.01991 \n[30] Qi Zhang, Xiaoyu Liu, Jinlan Fu. Neural Networks \nIncorporating Dictionaries for Chinese Word Segmentation. \nAssociation for the Advancement of Artificial Intelligence, \n2018. \n[31] Graves, A., and Schmidhuber, J. 2005. Framewise phoneme \nclassification with bidirectional lstm networks. In Neural \nNetworks, 2005. IJCNNâ€™05. Proceedings. 2005 IEEE \nInternational Joint Conference on, volume 4, 2047â€“2052. \n[32] David Ha, Andrew Dai, Quoc V. Le. HyperNetworks. \narXiv:1609.09106. 2016 \n[33] Hochreiter, S., Schmidhuber, J. Long short-term memory. \nNeural computation 9(8):1735â€“1780. 1997. \n[34] Thang Luong, Hieu Pham, and Christopher Manning. \nEffective approaches to attention-based neural machine \ntranslation. In Proceedings of the 2015 Conference on \nEmpirical Methods in Natural Lan- guage Processing, \nEMNLP 2015, pages 1412â€“1421, Lisbon, Portugal. \n \n \n \n \n",
  "categories": [
    "cs.CL"
  ],
  "published": "2019-04-19",
  "updated": "2019-04-19"
}