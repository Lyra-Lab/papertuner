{
  "id": "http://arxiv.org/abs/1905.02005v2",
  "title": "Deep Ordinal Reinforcement Learning",
  "authors": [
    "Alexander Zap",
    "Tobias Joppen",
    "Johannes Fürnkranz"
  ],
  "abstract": "Reinforcement learning usually makes use of numerical rewards, which have\nnice properties but also come with drawbacks and difficulties. Using rewards on\nan ordinal scale (ordinal rewards) is an alternative to numerical rewards that\nhas received more attention in recent years. In this paper, a general approach\nto adapting reinforcement learning problems to the use of ordinal rewards is\npresented and motivated. We show how to convert common reinforcement learning\nalgorithms to an ordinal variation by the example of Q-learning and introduce\nOrdinal Deep Q-Networks, which adapt deep reinforcement learning to ordinal\nrewards. Additionally, we run evaluations on problems provided by the OpenAI\nGym framework, showing that our ordinal variants exhibit a performance that is\ncomparable to the numerical variations for a number of problems. We also give\nfirst evidence that our ordinal variant is able to produce better results for\nproblems with less engineered and simpler-to-design reward signals.",
  "text": "Deep Ordinal Reinforcement Learning\nAlexander Zap, Tobias Joppen, and Johannes F¨urnkranz\nTU Darmstadt, 64289 Darmstadt, Germany\nalexander.zap@stud.tu-darmstadt.de\n{tjoppen, juffi}@ke.tu-darmstadt.de\nAbstract. Reinforcement learning usually makes use of numerical re-\nwards, which have nice properties but also come with drawbacks and\ndiﬃculties. Using rewards on an ordinal scale (ordinal rewards) is an al-\nternative to numerical rewards that has received more attention in recent\nyears. In this paper, a general approach to adapting reinforcement learn-\ning problems to the use of ordinal rewards is presented and motivated.\nWe show how to convert common reinforcement learning algorithms to\nan ordinal variation by the example of Q-learning and introduce Ordinal\nDeep Q-Networks, which adapt deep reinforcement learning to ordinal\nrewards. Additionally, we run evaluations on problems provided by the\nOpenAI Gym framework, showing that our ordinal variants exhibit a\nperformance that is comparable to the numerical variations for a num-\nber of problems. We also give ﬁrst evidence that our ordinal variant\nis able to produce better results for problems with less engineered and\nsimpler-to-design reward signals.\nKeywords: Reinforcement Learning · Ordinal Rewards\n1\nIntroduction\nConventional reinforcement learning (RL) algorithms rely on numerical feedback\nsignals. Their main advantages include the ease of aggregation, eﬃcient gradient\ncomputation, and many use cases where numerical reward signals come naturally,\noften representing a quantitative property. However, in some domains numerical\nrewards are hard to deﬁne and are often subject to certain problems. One issue of\nnumerical feedback signals is the diﬃculty of reward shaping, which is the task of\ncreating a reward function. Since RL algorithms use rewards as direct feedback to\nlearn a behavior which optimizes the aggregation of received rewards, the reward\nfunction has a signiﬁcant impact on the behavior that is learned by the algorithm.\nManual creation of the reward function is often expensive, non-intuitive and\ndiﬃcult in certain domains and can therefore cause a bias in the optimal behavior\nthat is learned by an algorithm. Since the learned behavior is sensitive to these\nreward values, the rewards of an environment should not be introduced or shaped\narbitrarily if they are not explicitly known or naturally deﬁned. This can also lead\nto another problem called reward hacking, where algorithms are able to exploit a\nreward function and miss the intended goal of the environment, caused by being\nable to receive better rewards through undesired behavior. The use of numerical\narXiv:1905.02005v2  [cs.LG]  11 Jul 2019\n2\nA. Zap et al.\nrewards furthermore requires inﬁnite rewards to model undesired decisions in\norder to not allow trade-oﬀs for a given state. This can be illustrated by an\nexample in the medical domain, where it is undesirable to be able to compensate\none occurrence of death of patient with multiple occurrences of cured patient to\nstay at a positive reward in average and therefore artiﬁcial feedback signals\nare used that can not be averaged. These issues have motivated the search for\nalternatives, such as preference-based feedback signals [13].\nIn this paper, we investigate the use of rewards on an ordinal scale, where we\nhave information about the relative order of various rewards, but not about the\nmagnitude of the quality diﬀerences between diﬀerent rewards. Our goal is to\nextend reinforcement learning algorithms so that they can make use of ordinal\nrewards as an alternative feedback signal type in order to avoid and overcome\nthe problems with numerical rewards.\nReinforcement learning with ordinal rewards has multiple advantages and\ndirectly addresses multiple issues of numerical rewards. Firstly, the problem of\nreward shaping is minimized, since the manual creation of the ordinal reward\nfunction speciﬁcally by the reward ordering often is intuitive and can be done\neasily without the need of exact speciﬁcations for reward values. Even though\nthe creation of ordinal reward values, so-called reward tiers, through the ascend-\ning order of feedback signals introduces a naturally deﬁned bias, it omits the\nlargely introduced artiﬁcial bias by the manual shaping of reward values. At\nthe same time, ordinal rewards simplify the problem of reward hacking because\nthe omission of speciﬁc numeric reward values has the eﬀect that any possible\nexploitation of rewards by an algorithm is only dependent on an incorrect re-\nward order, which can be more easily ﬁxed than the search for correct numerical\nvalues. While the use of inﬁnite rewards can not be modelled directly, it is still\npossible to deﬁne inﬁnite rewards as highest or lowest ordinal reward tier, and\nimplement policies which completely avoid and encourage certain tiers.\nSince the creation of the ordinal reward function is cheap and intuitive, it\nis especially suitable for newly deﬁned environments since it enables the easy\ndeﬁnition of ordinal rewards by ordering the possible outcomes naturally by de-\nsirability. Additionally it should be noted that for existing environments with\nnumerical rewards it is possible to extract ordinal rewards from these environ-\nments.\nThe focus of this paper is the technique of using ordinal rewards for rein-\nforcement learning. To this end, we propose an alternative reward aggregation\nfor ordinal rewards, introduce a method for policy determination from ordinal\nrewards and compare the performance of ordinal reward algorithms to algo-\nrithms for numerical rewards. In Section 2, we discuss related work and previous\napproaches. A formal deﬁnition of common reinforcement learning terminology\ncan be found in Section 3. Section 4 introduce reinforcement learning algorithms\nwhich use ordinal reward aggregations instead of numerical rewards, and illus-\ntrates the diﬀerences to conventional approaches. In Section 5 experiments are\nexecuted on the framework of OpenAI Gym and common reinforcement learning\nalgorithms are compared to ordinal reinforcement learning.\nDeep Ordinal Reinforcement Learning\n3\n2\nRelated Work\nThe technique of using rewards on an ordinal scale as an alternative to numerical\nrewards is mainly based on the approach of preference learning (PL) [1]. In\ncontrast to traditional supervised learning, PL follows the core idea of having\npreferences over states or symbols as labels and predicting these preferences\nas the output on unseen data instances instead of labelling data with explicit\nnominal or numerical values.\nRecently, there have been several proposals for combining PL with RL, where\npairwise preferences over trajectories, states or actions are deﬁned and applied as\nfeedback signals in reinforcement learning algorithms instead of the commonly\nused numerical rewards. For a survey of such preference-based reinforcement\nlearning algorithms, we refer the reader to [13].\nWhile preference-based RL provides algorithms for learning an agent’s be-\nhavior from pairwise comparison of trajectories, [12] presents an approach for\ncreating preferences over multiple trajectories in the order of ascending ordinal\nreward tiers, thereby deviating from the concept of pairwise comparisons over\ntrajectories. Using a tutor as an oracle, this approach approximates a latent nu-\nmerical reward score from a sequence of received ordinal feedback signals. This\nalternative reward computation functions as a reward transformation from the\nordinal to the numerical scale and is applicable on top of an existing reinforce-\nment learning algorithm.\nContrary to this approach, we do not use a tutor for the comparison of\ntrajectories but can directly use ordinal rewards as a feedback signal. In order to\nuse environments where numerical feedback already exists without the need for\nacquiring human feedback about the underlying preferences, we automatically\nextract rewards on an ordinal scale from existing environments with numerical\nrewards. To this end, we adapt an approach that has been proposed for Monte-\nCarlo Tree Search [4] to reinforcement learning.\nFurthermore, we handle ordinal rewards in a similar manner as previous\napproaches by directly using aggregated received ordinal rewards for comparing\ndiﬀerent options. The idea of direct comparison of ordinal rewards builds on the\nworks of [10], [11], [2] and [4], which provide criteria for the direct comparison\nof ordinal reward aggregations. We utilize the approach of [4], which transfers\nthe numerical reward maximization problem into a best-choice maximization\nproblem for an alternative computation of the value function for reinforcement\nlearning from ordinal feedback signals. [4] used this idea for adapting Monte\nCarlo Tree Search to the use of ordinal rewards.\nIn summary, we automatically transfer numerical feedback into preference-\nbased feedback and propose a new conceptual idea to utilize ordinal rewards\nfor reinforcement learning, which should not be seen as an alternative for the\nexisting algorithms stated above. Hence, we do not compare the performance of\nour new approach to any of the algorithms that use additional human feedback,\nbut to common RL techniques that use numerical feedback.\n4\nA. Zap et al.\n3\nMarkov Decision Process and Reinforcement Learning\nIn this section, we brieﬂy recapitulate Markov decision processes and reinforce-\nment learning algorithms. Our notation and terminology is based on [8].\n3.1\nValue function and policy for Markov Decision Process\nA Markov Decision Process (MDP) is deﬁned as a tuple of (S, A, P, R) with S\nbeing a ﬁnite set of states, A being a ﬁnite set of actions, T being the transition\nfunction S × A × S →R that models the probability of reaching a state s′ when\naction a is performed in state s, and R being the reward function S ×A×S →R\nwhich maps a reward r from a subset of possible rewards r ∈{r1, ..., rn} ⊂R to\nexecuting action a in state s and reaching s′ in the process. For further work we\nassume that T is deterministic and a transition always has the probability of 0\nor 1. Furthermore it is assumed that each action a ∈A is executable in any state\ns ∈S, hence the transition function is deﬁned for every element in S × A × S. A\npolicy π is the speciﬁcation which decision to take based on the environmental\nstate. In a deterministic setting, it is modeled as a mapping π : S →A which\ndirectly maps an environmental state s to the decision a which should be taken in\nthis state. The value function Vπ(s) represents the expected quality of a policy\nπ in state s with respect to the rewards that will be received in the future.\nValue functions for numerical rewards are computed by the expectation of the\ndiscounted sum of rewards E[R]. The value function Vπ(s) of a policy π in an\nenvironmental state s therefore can be computed by\nVπ(s) = E[R],\nR =\n∞\nX\nt=0\nγtrt\n(1)\nwhere R is the discounted sum of rewards when following policy π, γ a discount\nfactor, and rt the direct reward at time step t. The optimal policy π∗in a state\ns is the policy with the largest Vπ(s), which complies with the goal of an RL\nalgorithm to maximize expected future reward.\n3.2\nReinforcement Learning\nReinforcement learning can be described as the task of learning a policy that\nmaximizes the expected future numerical reward. The agent learns iteratively by\nupdating its current policy π after every action and the corresponding received\nreward from the environment. Furthermore, the agent may perform multiple\ntraining sessions, so-called episodes, in the environment. Using the previously\ndeﬁned formalism, this can be expressed as approximating the optimal policy\niteratively with a function ˆπ, by repeatedly choosing actions that lead to states\ns with the highest estimated value function Vˆπ(s). In the following section two\ncommon reinforcement learning algorithms are introduced.\nDeep Ordinal Reinforcement Learning\n5\nQ-learning. The key idea of the Q-learning algorithm [9] is to estimate Q-\nvalues Q(s, a), which estimate the expected future sum of rewards E[R] when\nchoosing an action a in a state s and following the optimal policy π∗afterwards.\nHence the Q-value can be seen as a measure of goodness for a state-action pair\n(s, a), and therefore, in a given state s, the optimal policy π∗should select the\naction a that maximizes this value in comparison to other available actions in\nthat state. The approximated Q-values are stored and iteratively updated in a\nQ-table. The Q-table is updated after an action a has been performed in a state\ns and the reward r and the newly reached state s′ is observed. The computation\nof the expected Q-value is done by\nˆQ(s, a) = r(s, a) + γ max\na′ Q(s′, a′)\n(2)\nFollowing this so-called Bellman equation, every previously estimated Q-value is\nupdated with the newly computed expected Q-value with the formula\nQ(s, a) = Q(s, a) + α[r(s, a) + γ max\na′ Q(s′, a′) −Q(s, a)]\n(3)\nwhere α represents a learning rate and γ the discount factor.\nDeep Q-Network. The original Q-learning algorithm is limited to very simple\nproblems, because of the explicitly stored Q-table, which essentially memorizes\nthe quality of each possible state-action pair independently. Thus it requires,\ne.g., that each state-action pair has to be visited a certain number of times in\norder to make a reasonable prediction for this pair. A natural extension of this\nmethod is to replace the Q-table with a learned Q-function, which is able to\npredict a quality value for a given, possibly previously unseen state-action pair.\nThe key idea behind the Deep Q-Network (DQN) [6,7] is to learn a continuous\nfunction QDQN(s) in the form of a deep neural network with m input nodes,\nwhich represent the feature vector of s, and n output nodes, each containing the\nQ-value of one action a.\nNeural networks can be iteratively updated to ﬁt the output nodes to the\ndesired Q-values. The expected Q-value for a state-action pair is calculated in\nthe same manner as deﬁned in (2) with the diﬀerence that the Q-values are now\npredicted by the DQN, with one output node QDQN\na\n(s) for each possible action\na. Therefore (2) becomes\nˆQDQN\na\n(s) = r(s, a) + γ max\na′ QDQN\na′\n(s′)\n(4)\nwhere QDQN\na\n(s) represents the Q-value node of action a in state s.\nIn order to optimize the learning procedure, DQN makes use of several opti-\nmizations such as experience replay, the use of a separate target and evaluation\nnetwork, and Double Deep Q-Network. More details on these techniques can be\nfound in the following paragraphs.\n6\nA. Zap et al.\nExperience replay. Using a neural network to ﬁt the Q-value of the previously\nexecuted state-action pair as described in (4) leads to overﬁtting to recent ex-\nperiences because of the high correlation between environmental states across\nmultiple successive time steps, and the property of neural networks to overﬁt\nrecently seen training data. Instead of only using the previous state-action pair\nfor ﬁtting the DQN, experience replay [5] uses a memory M to store previous\nexperience instances (s, a, r, s′) and iteratively reuses a random sample of these\nexperiences to update the network prediction at every time step.\nTarget and evaluation networks. Frequently updating the neural network, which\nis simultaneously used for the prediction of the expected Q-value, leads to un-\nstable ﬁtting of the network. Therefore these two tasks, ﬁrstly the prediction of\nthe target Q-value for network ﬁtting and secondly the prediction of the Q-value\nwhich is used for policy computation, allows for a split into two networks. These\ntwo networks are the evaluation network, which is used for policy computation,\nand the target network, which is used for predicting the target value for contin-\nuously ﬁtting the evaluation network. In order to keep the target network up to\ndate, it is replaced by a copy of the evaluation network every c steps.\nDouble Deep Q-Network. Deep Q-Networks tend to overestimate the prediction\nof Q-values for some actions, which may result in an unjustiﬁed bias towards\ncertain actions. To address this problem, Double Deep Q-Networks [3] addition-\nally use the target and evaluation networks to decouple the action choice and\nQ-value prediction by letting the evaluation network choose the next action to\nbe played, and letting the target network predict the respective Q-value.\n4\nDeep Ordinal Reinforcement Learning\nIn this section, Markov decision processes and reinforcement learning algorithms\nare adapted to settings with ordinal reward signals. More concretely, we present\na method for reward aggregation that ﬁts ordinal rewards and explain how this\nmethod can be used in Q-learning and Deep Q-Networks in order to learn to\nsolve environments that return feedback signals on an ordinal scale.\n4.1\nOrdinal Markov Decision Process\nSimilar to the standard Markov Decision Process, [10] deﬁnes an ordinal version\nof an MDP as a tuple of (S, A, T, Ro) with the only diﬀerence that Ro is the\nreward function S × A × S →N is modiﬁed to return ordinal rewards instead of\nnumerical ones. Thus, it maps executing action a in state s and reaching state s′\nto an ordinal reward ro from a subset of possible ordinal rewards ro ∈{1, ..., n} ⊂\nN, with n representing the number of ordinal rewards. Whereas a real-valued\nreward provides information about the qualitative size of the reward, the ordinal\nscale breaks rewards down to naturally ordered reward tiers. These reward tiers\nsolely represent the rank of desirability of a reward compared to all other possible\nDeep Ordinal Reinforcement Learning\n7\nrewards, which is noted as the ranking position ro of a reward r in the set of all\npossible rewards {r1, ..., rn}. Interpreting the reward signals on an ordinal scale\nstill allows us to order and directly compare individual reward signals, but while\nthe numerical scale allows for comparison of rewards by means of the magnitude\nof their diﬀerence, ordinal rewards do not provide this information.\nIn order to aggregate multiple ordinal rewards, a distribution to store and\nrepresent the expected frequency of received rewards on the ordinal scale is\nconstructed. This distribution is represented by a vector D(s, a), in which di(s, a)\nrepresents the frequency of receiving the ordinal reward ri by executing a in s.\nThe distribution vector is deﬁned by\nD(s, a) =\n\n\nd1(s, a)\n...\ndn(s, a)\n\n\n(5)\nThrough normalization of distribution vector D, a probability distribution P can\nbe constructed, which represents the expected probability of receiving a reward.\nThe probability distribution is represented by a probability vector P(s, a), in\nwhich pi(s, a) represents the estimated probability of receiving the ordinal reward\nri by executing a in s. Hence the probability vector can be deﬁned by\nP(s, a) =\n\n\np1(s, a)\n...\npn(s, a)\n\nwith\nn\nX\ni=1\npi(s, a) = 1 and 0 ≤pi(s, a) ≤1\n(6)\nValue function for ordinal rewards. While numerical rewards enable the\nrepresentation of value function Vπ(s) by the expected sum of rewards, the value\nfunction for environments with ordinal rewards needs to be estimated diﬀerently.\nSince ordinal rewards are aggregated in a distribution of received ordinal rewards,\nthe calculation of value function Vπ(s) in state s can be done based on P(s, a)\nfor action a that is selected by policy π. Hence the computation of the value\nfunction can be modeled by the following formula of\nVπ(s) = F(P(s, a)) with a = π(s)\n(7)\nThe computation of the value function from probability distribution P(s, a)\nthrough function F is performed by the technique of measure of statistical supe-\nriority [4]. This measure computes the probability that action a receives a better\nordinal reward than a random alternative action a′ in the same environmental\nstate s. This probability can be calculated through the sum of all probabilities of\na receiving a better ordinal reward o than a′. Hence the probability of an action\na performing better than another action a′ can be deﬁned as\nP(a ≻a′) =\nn\nX\no=1\npo(s, a) ·\n\u0012\npo<(s, a′) + 1\n2po(s, a′)\n\u0013\nwith po<(s, a) =\no−1\nX\ni=1\npi(s, a)\n8\nA. Zap et al.\nTo deal with ties, additionally half the probability of a receiving the same reward\ntier as a′ is added.\nThe function of the measure of statistical superiority therefore is computed\nthrough the averaged winning probability of a against all other actions a′ by\nF(P(s, a)) = E[P(a ≻a′)] =\nP\na′ P(a ≻a′)\nk −1\n(8)\nfor k available actions in state s.\nBased on (7), the optimal policy π∗can be determined in the same way as\nfor numerical rewards (1) by maximizing the respective value function Vπ(s).\n4.2\nTransformation of existing numerical rewards to ordinal\nrewards\nIf an environment has pre-deﬁned rewards on a numerical scale, transforming\nnumerical rewards r ∈{r1, ..., rn} ⊂R into ordinal rewards ro ∈{1, ..., n} ⊂N\ncan easily be done by translating every numerical reward to its ordinal position\nwithin all possible numerical rewards. This way the lowest possible numerical\nreward is mapped to position 1, and the highest numerical reward is mapped to\nposition n, with n representing the number of possible numerical rewards. This\ntransformation process simply results in removing the metric and semantic of\ndistances of rewards but keeping the order.\n4.3\nOrdinal Reinforcement Learning\nIn Section 4.1, we have shown how to compute a value function Vπ(s) and deﬁned\nthe optimal policy π∗for environments with ordinal rewards. This can now be\nused for adapting common reinforcement learning algorithms to ordinal rewards.\nOrdinal Q-learning. For the adaptation of the Q-learning algorithm to ordinal\nrewards, we do not directly update a Q-value Q(s, a) that represents the quality\nof a state-action pair (s, a) but update the distribution D(s, a) of received ordi-\nnal rewards. The target distribution is computed by adding the received ordinal\nreward i (represented through unit vector ei of length n) to the distribution\nD(s′, π∗(s′)) of taking an action in the new state s′ according to the optimal\npolicy π∗. The previous distribution D(s, a) is updated with the target distribu-\ntion by interpolating both values with learning rate α, which can be seen in the\nformula\nD(s, a) = D(s, a) + α[ei(s, a) + γD(s′, π∗(s′)) −D(s, a)]\n(9)\nIn this adaptation of Q-learning1, the expected quality of state-action pair (s, a)\nis not represented by the Q-value Q(s, a) (3) but by the function F(P(s, a))\n(8) of the probability distribution P(s, a), which is derived from the iteratively\nupdated distribution D(s, a).\n1 This technique of modifying the Q-learning algorithm to deal with rewards on an or-\ndinal scale can analogously be applied to other Q-table based reinforcement learning\nalgorithms like Sarsa and Sarsa-λ [14]\nDeep Ordinal Reinforcement Learning\n9\n...\nS1\nS2\nS3\nS4\nH1\nHn\nd1(s, a1)\nd2(s, a1)\nd3(s, a1)\nd4(s, a1)\nInput\nlayer\nHidden\nlayer\nOutput\nlayer\n. . .\n...\nS1\nS2\nS3\nS4\nH1\nHn\nd1(s, ak)\nd2(s, ak)\nd3(s, ak)\nd4(s, ak)\nInput\nlayer\nHidden\nlayer\nOutput\nlayer\nFigure 1: Example of an array of ordinal deep neural networks for DQN for\nreward distribution prediction\nOrdinal Deep Q-Network. Because ordinal rewards are aggregated by a dis-\ntribution instead of a numerical value, the neural network is adapted to predict\ndistributions D(s, a) instead of Q-values for every possible action. Hence for one\naction the network does not predict a 1-dimensional Q-value, but predicts an\nn-dimensional reward distribution with n being the length of the ordinal scale.\nSince this distribution has to be computed for each of k actions, the adapta-\ntion of the Deep Q-Network algorithm to ordinal rewards requires a diﬀerently\nstructured neural network. Contrary to the original Deep Q-Network where one\nnetwork simultaneously predicts k Q-values for all actions, the structure of the\nordinal DQN consists of an array of k neural networks, from which every network\ncomputes the expected ordinal reward distribution D(s, a) for one separate ac-\ntion a. In a deep neural network for the prediction of distributions every output\nnode of the network computes one distribution value di(s, a). The structure of\nneural networks used for the prediction of distributions can be seen in Figure 1.\nThe prediction of the ordinal reward distributions D(s, a) for all actions can\nafterwards be normalized to a probability distribution and used in order to com-\npute the value function Vπ(s) through the measure of statistical superiority as\nhas been previously deﬁned in (7). Once the value function and policy have been\nevaluated, the ordinal variant of the DQN algorithm follows a similar procedure\nas ordinal Q-learning and updates the prediction of the reward distribution for\n(s, a) by ﬁtting DDQN\na\n(s) to the target reward distribution:\nˆDDQN\na\n(s) = ero(s, a) + γDDQN\nπ∗(s′)(s′)\n(10)\nThe main diﬀerence in the update step between ordinal Q-learning (9) and\nordinal DQN consists of ﬁtting the neural network of action a for input s to the\nexpected reward distribution by backpropagation instead of updating a Q-table\nentry (s, a). Additional modiﬁcations to the ordinal Deep Q-Network in form of\nexperience replay, the split of the target and evaluation network and the usage\nof a Double DQN are done in a similar fashion as described with the standard\nDQN algorithm in Section 3.2. These modiﬁcations can be seen in the following\nparagraphs.\n10\nA. Zap et al.\nExperience replay. A memory M is used to sample multiple saved experience\nelements (s, a, ro, s′) randomly and replay these previously seen experiences by\nﬁtting the ordinal DQN networks to the samples of earlier memory elements.\nTarget and evaluation networks. In order to prevent unstable behavior by using\nthe same networks for the prediction and updating step, we use separate evalu-\nation networks to predict reward distributions for the policy computation, and\nuse target networks to predict the target reward distributions which are used for\nﬁtting the evaluation networks continuously.\nDouble Deep Q-Network. The neural networks of ordinal DQN tend to overes-\ntimate the prediction of the reward distributions for some actions, which may\nresult in an unjustiﬁed bias towards certain actions. Therefore, in order to de-\ntermine the next action to be played by π∗, the measure of statistical superiority\nis computed based on the reward distributions predicted by the evaluation net-\nworks. Afterwards the prediction of the reward distribution for this action is\ncomputed by the respective target network.\n5\nExperiments and Results\nIn the following, the standard reinforcement algorithms described in Section 3.2\nand the ordinal reinforcement learning algorithms described in Section 4.3 are\nevaluated and compared in a number of testing environments.2\n5.1\nExperimental setup\nThe environments which are used for evaluation are provided by OpenAI Gym,3\nwhich can be viewed as a uniﬁed toolbox for our experiments. All environments\nexpect an action input after every time step and return feedback in form of the\nnewly reached environmental state, the direct reward for the executed action, and\nthe information whether the newly reached state is terminal. The environments\nthat the algorithms were tested on were CartPole and Acrobot.4\nPolicies of the reinforcement learning algorithms were modiﬁed to use ϵ-greedy\nexploration [8], which encourages early exploration of the state space and in-\ncreases exploitation of the learned policy over time. In the experiments the max-\nimum exploitation is reached after half of the total episodes. In order to directly\ncompare the standard and the ordinal variants of reinforcement learning algo-\nrithms, the quality of the learned policy and the computational eﬃciency are\ninvestigated across all environments with varying episode numbers. Information\n2 The source code for the implementation of the experiments can be found in https:\n//github.com/az79nefy/OrdinalRL.\n3 For further information about OpenAI visit https://gym.openai.com.\n4 Further technical details about the environments CartPole and Acrobot from Ope-\nnAI can be found in https://gym.openai.com/envs/CartPole-v0/ and https:\n//gym.openai.com/envs/Acrobot-v1/.\nDeep Ordinal Reinforcement Learning\n11\nFigure 2: CartPole scores of standard and ordinal Q-learning for 400 and 10000\nepisodes\nabout the quality of the learned policy is derived from the sum of rewards over a\nwhole episode (score) or the win rate while the eﬃciency is measured by real-time\nprocessing time. Additionally to the standard variant with unchanged rewards,\nthe performance of standard Q-learning algorithms is tested with changed re-\nwards in order to simulate the performance on environments where no optimal\nreward engineering has been performed. It should be noted that the modiﬁca-\ntions of the rewards is performed under the constraints of remaining existing\nreward order, therefore not changing the transformation to the ordinal scale.\nThe change of rewards (CR) from the existing numerical rewards r ∈{r1, ..., rn}\nis performed for all rewards by the calculation of rCR,i = ri−min(r)\n100\n.\nThe parameter conﬁguration of the Q-learning algorithms is learning rate\nα = 0.1 and discount factor γ = 0.9. The parameter conﬁguration of the Deep\nQ-Network algorithm is learning rate α = 0.0005 and discount factor γ = 0.9. As\nfor the network speciﬁc parameters, the Adam optimizer is used for the network\nﬁtting, the target network is getting replaced every 300 ﬁtting updates, the\nexperience memory size is 200000 and the replay batch size is 64.\n5.2\nExperimental results\nThe results of the comparison between numerical and ordinal algorithms for\nthe CartPole- and Acrobot-environment in terms of score, win rate and compu-\ntational time are shown and investigated in the following. This comparison is\nperformed based on the averaged results from 10 and respectively 5 independent\nruns of Q-learning and Deep Q-Network on the environments.\nQ-learning. In Figure 2 the scores for the CartPole-environment over the course\nof 400 and 10000 episodes can be seen which were played by an agent using the\nordinal (orange) as well as the standard Q-learning algorithm, with (red) and\nwithout (blue) modiﬁed rewards. Additionally the individual dots in this ﬁgure\nrepresent the scores achieved by the respective algorithms by using the optimal\npolicy instead of ϵ-greedy exploration. The evaluation of these scores shows that\nthe ordinal variant of Q-learning performs better than the standard variant with\n12\nA. Zap et al.\nFigure 3: Comparison of value function margin for best action of standard and\nordinal Q-learning for 400 and 10000 episodes of CartPole\nengineered rewards for 400 episodes and reaches the optimal score of 200 quicker\nfor 10000 episodes. Additionally the use of ordinal rewards signiﬁcantly outper-\nforms the standard variant with modiﬁed rewards for both episode numbers.\nTherefore it can be seen that ordinal Q-learning is able to learn a good policy\nbetter than the standard variants for the CartPole-environment.\nIn order to explain the diﬀerence of learned behavior between the standard\nand ordinal variant, the average relative diﬀerence of Q-values Q(s, a) and respec-\ntively measure of statistical superiority functions F(P(s, a)) for the two possible\nactions were plotted and compared in Figure 3 for standard (blue) and ordinal\n(orange) Q-learning. It can be seen for both episode numbers that the policy\nwhich is learned by ordinal RL through the measure of statistical superiority\nconverges to a diﬀerence of 0, meaning that the function F(P(s, a)) converges\nto similar values for both actions. This can be interpreted as the policy learning\nto play safely and rarely entering any critical states where this function would\nindicate strong preference towards one action (e.g. in big angles). On the other\nside it can be seen for 400 episodes that common RL does not converge towards\nsimilar Q-values for the actions over time and therefore a policy is learned that\nenters critical states more often. It should be noted that the Q-value diﬀerences\nfor standard Q-learning converges to 0 for evaluations with more episodes and a\nsafe policy is eventually learned as well.\nIn Figure 4 the win rates from the Acrobot-environment were plotted over\nthe course of 400 and 10000 episodes similarly as the scores for the CartPole-\nenvironment and it can be seen for low episode numbers that while the policy\nlearned by the standard variant of Q-learning with unchanged rewards performs\nbetter than the policy learned by the ordinal variant, changing the numerical\nvalues of rewards yields the same performance as the ordinal variant. But for high\nepisode numbers it should be noted that the ordinal variant reaches a similar\nperformance as the standard variant with a win rate of 0.3 after 10000 episodes\nand clearly outperforms the win rate of the standard Q-learning algorithm with\nCR.\nDeep Ordinal Reinforcement Learning\n13\nFigure 4: Acrobot win rates of standard and ordinal Q-learning for 400 and 10000\nepisodes\nFigure 5: Comparison of value function margin for best action of standard and\nordinal Q-learning for 400 and 10000 episodes of Acrobot\nSimilar as for the CartPole-environment, the F- and Q-function margins of\nthe best actions over the course of 400 and 10000 episodes were compared in\nFigure 5 and yield diﬀerent observations for the standard and ordinal variants,\nand it can be therefore be concluded that the learned policies diﬀer. While the\nordinal variant decreases the relative margin of F(P(s, a)) of the best action and\ntherefore learns a policy which plays safely, the standard variant learns a policy\nwhich maximizes the Q-value margin of the best action and therefore follows a\npolicy which enters critical states more often. While the standard variant learns\na good policy quicker, it should be noted that both policies perform comparably\nafter many episodes despite the policy diﬀerences.\nAs can be seen in Table 1, using the ordinal variant results in an additional\ncomputational load by a factor between 0.8 and 1.2 for CartPole and 0.5 for\nAcrobot. The additionally required computational capacity is caused by the\ncomputation of the measure of statistical superiority which is less eﬃcient than\ncomputing the expected sum of rewards. This factor could be reduced by using\nthe iterative update of the function measure of statistical superiority described\nin [4].\n14\nA. Zap et al.\nTable 1: Computation time comparison of standard and ordinal Q-learning for\nvarying episode numbers\nNumber of\nCartPole\nAcrobot\nepisodes\nStandard\nOrdinal\nStandard\nOrdinal\n400\n2.10 s\n4.17 s\n35.74 s\n52.85 s\n2000\n10.07 s\n24.86 s\n174.38 s\n266.40 s\n10000\n67.29 s\n130.09 s\n855.15 s\n1258.30 s\n50000\n354.52 s\n667.87 s\n4149.78 s\n6178.76 s\nDeep Q-Network. In Figure 6 the scores achieved in the CartPole-environment\nby the ordinal as well as the standard Deep Q-Network, with and without CR,\ncan be seen over the course of 160 and 1000 episodes. For 160 episodes it can\nbe seen that ordinal DQN as well as the standard variant without CR converge\nto a good policy reaching an episode score close to 150. Contrary to this perfor-\nmance, modiﬁed rewards negatively impact standard Q-learning and therefore\nits performance is signiﬁcantly worse, not reaching a score above 100. Addition-\nally for low episode numbers it should be noted that the policy learned by the\nordinal variant of Deep Q-Network is able to achieve good scores faster than the\nstandard variant, matching the observation made for the Q-learning algorithms.\nThe evaluation for 1000 episodes shows that the performances of standard, with\nand without CR, and ordinal DQNs are comparable.\nFigure 7 plots the win rate of Deep Q-Network algorithms for the Acrobot-\nenvironment over the course of 160 and 1000 episodes. For 160 episodes standard\nDQN with engineered rewards performs better than the ordinal variant, but loses\nthis quality once the rewards are modiﬁed. For high episode numbers it can be\nseen that the ordinal variant is comparable to the standard algorithm without\nCR and solves the environment with a win rate of close to 1.0, but clearly\noutperforms the standard DQN with modiﬁed rewards which is only able to\nachieve a win rate of 0.6. It should be noted that all variants of DQN are able\nto learn a better policy than their respective Q-learning algorithms, achieving a\nhigher win rate in less than 160 episodes.\nAdditionally, it should be noted that the use of the ordinal variant of DQN\nadds an additional computational factor between 0 and 0.5 for the CartPole-\nenvironment and 1.0 for the Acrobot-environment, as can be seen in Table 2.\nSince the evaluation of the ordinal Deep Q-Network algorithm shows compa-\nrable results to the standard DQN with engineered rewards and furthermore out-\nperforms the standard variant with modiﬁed rewards, it can be concluded that\nthe conversion of the Deep Q-Network algorithm to ordinal rewards is successful.\nTherefore it has been shown that algorithms of deep reinforcement learning can\nas well be adapted to the use of ordinal rewards.\nDeep Ordinal Reinforcement Learning\n15\nFigure 6: CartPole scores of standard and ordinal DQN for 160 and 1000 episodes\nFigure 7: Acrobot win rates of standard and ordinal DQN for 160 and 1000\nepisodes\n6\nConclusion\nIn this paper we have shown that the use of ordinal rewards for reinforcement\nlearning is able to reach and even improve the quality of standard reinforcement\nlearning algorithms with numerical rewards. We compared RL algorithms for\nboth numerical and ordinal rewards on a number of tested environments and\ndemonstrated that the performance of the ordinal variant is mostly comparable\nto the learned common RL algorithms that make use of engineered rewards while\nbeing able to signiﬁcantly improve the performance for modiﬁed rewards.\nFinally, it should be noted that ordinal reinforcement learning enables the\nlearning of a good policy for environments without much eﬀort to manually\nshape rewards. We hereby lose the possibility of reward shaping to the same\ndegree that numerical rewards would allow, but therefore gain a more simple-\nto-design reward structure. Hence, our variant of reinforcement learning with\nordinal rewards is especially suitable for environments that do not have a natural\nsemantic of numerical rewards or where reward shaping is diﬃcult. Additionally\nthis method enables the usage of new and unexplored environments for RL only\nwith the speciﬁcation of an order of desirability instead of the needed eﬀort of\nmanually engineering numerical rewards with sensible semantic meaning.\n16\nA. Zap et al.\nTable 2: Computation time comparison of standard and ordinal DQN\nNumber of\nCartPole\nAcrobot\nepisodes\nStandard\nOrdinal\nStandard\nOrdinal\n160\n1520.01 s\n2232.48 s\n3659.44 s\n7442.49 s\n400\n6699.69 s\n7001.79 s\n9678.80 s\n19840.88 s\n1000\n15428.41 s\n15526.84 s\n23310.36 s\n47755.90 s\nAcknowledgements\nThis work was supported by DFG. Calculations for this research were conducted\non the Lichtenberg high performance computer of the TU Darmstadt.\nReferences\n1. F¨urnkranz, J., H¨ullermeier, E. (eds.): Preference Learning. Springer-Verlag (2011)\n2. Gilbert, H., Weng, P.: Quantile reinforcement learning. CoRR abs/1611.00862\n(2016)\n3. Hasselt, H.v., Guez, A., Silver, D.: Deep Reinforcement Learning with Double\nQ-Learning. In: Proceedings of the Thirtieth AAAI Conference on Artiﬁcial Intel-\nligence. pp. 2094–2100. AAAI’16, AAAI Press (2016)\n4. Joppen,\nT.,\nF¨urnkranz,\nJ.:\nOrdinal\nMonte\nCarlo\nTree\nSearch.\nCoRR\nabs/1901.04274 (2019)\n5. Lin, L.J.: Reinforcement Learning for Robots Using Neural Networks. Ph.D. thesis,\nCarnegie Mellon University, Pittsburgh, PA, USA (1992), uMI Order No. GAX93-\n22750\n6. Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,\nD., Riedmiller, M.A.: Playing Atari with Deep Reinforcement Learning. CoRR\nabs/1312.5602 (2013)\n7. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G.,\nGraves, A., Riedmiller, M.A., Fidjeland, A., Ostrovski, G., Petersen, S., Beattie, C.,\nSadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., Hassabis,\nD.: Human-level control through deep reinforcement learning. Nature 518(7540),\n529–533 (2015)\n8. Sutton, R.S., Barto, A.G.: Reinforcement learning - an introduction. Adaptive\ncomputation and machine learning, MIT Press, second edn. (2018)\n9. Watkins, C.J., Dayan, P.: Q-learning. Machine Learning 8, 279–292 (1992)\n10. Weng, P.: Markov decision processes with ordinal rewards: Reference point-based\npreferences. In: Proceedings of the 21st International Conference on Automated\nPlanning and Scheduling (ICAPS-11). AAAI Press, Freiburg, Germany (2011)\n11. Weng, P.: Ordinal Decision Models for Markov Decision Processes. In: Proceedings\nof the 20th European Conference on Artiﬁcial Intelligence (ECAI-12). pp. 828–833.\nIOS Press, Montpellier, France (2012)\n12. Weng, P., Busa-Fekete, R., H¨ullermeier, E.: Interactive Q-Learning with Ordinal\nRewards and Unreliable Tutor. In: Proceedings of the ECML/PKDD-13 Workshop\non Reinforcement Learning from Generalized Feedback: Beyond Numeric Rewards\n(2013)\nDeep Ordinal Reinforcement Learning\n17\n13. Wirth, C., Akrour, R., Neumann, G., F¨urnkranz, J.: A Survey of Preference-Based\nReinforcement Learning Methods. Journal of Machine Learning Research 18(136),\n1–46 (2017)\n14. Zap, A.: Ordinal Reinforcement Learning. Master’s thesis, Technische Universit¨at\nDarmstadt (2019), To appear\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2019-05-06",
  "updated": "2019-07-11"
}