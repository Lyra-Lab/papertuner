{
  "id": "http://arxiv.org/abs/1905.02825v1",
  "title": "Toybox: A Suite of Environments for Experimental Evaluation of Deep Reinforcement Learning",
  "authors": [
    "Emma Tosch",
    "Kaleigh Clary",
    "John Foley",
    "David Jensen"
  ],
  "abstract": "Evaluation of deep reinforcement learning (RL) is inherently challenging. In\nparticular, learned policies are largely opaque, and hypotheses about the\nbehavior of deep RL agents are difficult to test in black-box environments.\nConsiderable effort has gone into addressing opacity, but almost no effort has\nbeen devoted to producing high quality environments for experimental evaluation\nof agent behavior. We present TOYBOX, a new high-performance, open-source*\nsubset of Atari environments re-designed for the experimental evaluation of\ndeep RL. We show that TOYBOX enables a wide range of experiments and analyses\nthat are impossible in other environments.\n  *https://kdl-umass.github.io/Toybox/",
  "text": "TOYBOX: A Suite of Environments for\nExperimental Evaluation of Deep Reinforcement Learning\nEmma Tosch 1 Kaleigh Clary 1 John Foley 2 David Jensen 1\nAbstract\nEvaluation of deep reinforcement learning (RL)\nis inherently challenging. In particular, learned\npolicies are largely opaque, and hypotheses about\nthe behavior of deep RL agents are difﬁcult to\ntest in black-box environments. Considerable ef-\nfort has gone into addressing opacity, but almost\nno effort has been devoted to producing high-\nquality environments for experimental evaluation\nof agent behavior. We present TOYBOX, a new\nhigh-performance, open-source* subset of Atari\nenvironments re-designed for the experimental\nevaluation of deep RL. We show that TOYBOX\nenables a wide range of experiments and analyses\nthat are impossible in other environments.\n*https://kdl-umass.github.io/Toybox/\n1. Introduction\nSince DeepMind’s 2015 Nature paper, the Arcade Learn-\ning Environment (ALE) has become the de facto deep RL\nbenchmark for new training algorithms (Bellemare et al.,\n2013; Mnih et al., 2015; Machado et al., 2017). ALE has\nseveral appealing qualities: humans learn to play Atari and\nbecome more skilled with experience, it is a “real-world”\nenvironment that was not originally constructed to evalu-\nate RL methods, and it has greater complexity than prior\nenvironments (e.g., GridWorld, mountain car).\nALE has been used in several ways to evaluate the perfor-\nmance of deep RL agents. The vast majority of evaluations\nfollow a version of the approach described by Bellemare\net al. (2012): ﬁrst researchers choose network architectures\nand tune hyperparameters on small set of Atari games; then\nthey train agents using those hyperparameters on new games,\nreporting the learning curves (or a statistic of a collection of\nthose curves) (Mnih et al., 2015; Van Hasselt et al., 2016;\nMnih et al., 2016; Hessel et al., 2018).\n1University of Massachusetts Amherst, Amherst, USA 2Smith\nCollege, Northampton, USA. Correspondence to: Emma Tosch\n<etosch@cs.umass.edu>.\nWhile ALE has enabled demonstration and evaluation\nof much more complex behaviors of deep RL agents, it\npresents challenges as a suite of evaluation environments\nfor topics on the frontier of deep RL.\nChallenge: Limited variation within games. Very little about\nindividual games can be systematically altered, so ALE is\npoorly suited to testing how changes in the environment\naffect training and performance. New benchmarks such as\nOpenAI’s Sonic the Hedgehog emulator and CoinRun inject\nenvironmental variation into the training schedule, while\nintroducing train/test splits (Nichol et al., 2018; Cobbe et al.,\n2018). Similarly, Zhang et al. (2018) suggest benchmarks\nthat incorporate the kind of non-random noise found in\nnature. Kansky et al. (2017) implemented Breakout variants\nin order to achieve variation for generalization.\nChallenge: No counterfactual evaluation. Meanwhile, as-\nsertions about intelligent agent behavior remain untestable\nin the face of black-box evaluation environments. For exam-\nple, ALE does not enable testing the conjecture that agents\ntrained on Breakout learn to build tunnels (Mnih et al., 2015)\nor that they enter a tunneling mode (Greydanus et al., 2018).\nNo system currently permits experiments to answer counter-\nfactual questions about agent behavior.\nContribution.\nWe propose TOYBOX, a suite of high-\nperformance and highly parameterized Atari-like environ-\nments designed for the purpose of experimentation. We\ndemonstrate that the TOYBOX implementations of three\nAtari 2600 games achieve similar performance to their ALE\ncounterparts across three deep RL algorithms. We demon-\nstrate that TOYBOX enables a range of post-training analy-\nses not previously possible, and we show that TOYBOX is\northogonal to concurrent efforts in deep RL to address issues\nof robustness, generalization, and reproducible evaluation.\nOrganization. The rest of the paper is organized as follows:\nSection 2 introduces the TOYBOX design and its functional\ncapabilities. Section 3 describes our evaluation, including\nperformance and ﬁdelity testing against the ALE. Section 4\ndescribes four behavioral tests we present as case studies.\nRelated work not otherwise addressed can be found in Sec-\ntion 5. Section 6 discusses TOYBOX applications beyond\nthe scope of this paper. We conclude in Section 7.\narXiv:1905.02825v1  [cs.LG]  7 May 2019\nTOYBOX\n2. TOYBOX: System Design\nTOYBOX is a high-performance, highly parameterized suite\nof Atari-like games implemented in Rust, with bindings to\nPython. The suite currently contains three games: Break-\nout, Amidar, and Space Invaders. We chose these games\nfor diversity of genre (paddle-based, maze, and shooter,\nrespectively) and likely familiarity to readers.1\nSoftware requirements. Atari 2600 games were designed\nfor human players. For TOYBOX, the primary user is a\nreinforcement learning algorithm, and we expect machine\nlearning researchers to be able to customize gameplay. To\nthat end, we developed TOYBOX to meet the following set\nof software requirements\nR1 TOYBOX should only leverage the CPU, even for graph-\nical tasks. Although modern games leverage the GPU\nfor faster rendering, we expect the machine learning\nlibraries to be using the GPU, and so we wish to create\nour screen images using CPU-only.\nR2 TOYBOX should be at least as efﬁcient as the Stella-\nemulated version of the game. Since reinforcement\nlearning algorithms require millions of frames of train-\ning data, we must be able to simulate and render mil-\nlions of frames in order to enable efﬁcient use of com-\nputation resources for learning.\nR3 TOYBOX should provide for data-driven user cus-\ntomization. Changing the bricks in Breakout, the board\nin Amidar, or the alien conﬁguration in Space Invaders\nshould not require re-compilation of the core game code,\nnor should it require the ability to write Rust.\nR4 TOYBOX should be accessible through OpenAI Gym,\nwhich is a Python API. Furthermore, TOYBOX should\nbe usable as a drop-in replacement for the analogous\nALE environment.\ntoybox.rs\nSpace Invaders\nConfig\nState\ntoybox.py\nOpenAI Gym \nEnvironments\nToybox\nState\nSimulator\ntoybox.BaseEnv\ngym.Env\nAmidar\nConfig\nState\nBreakout\nConfig\nState\nSpaceInvadersEnv\nAmidarEnv\nBreakoutEnv\nactions = [NOOP, \nFIRE, LEFT, RIGHT]\nFigure 1: The TOYBOX architecture.\n1Although Amidar may not be familiar, it is very similar to\nPacMan, but with simpler rules.\nAgent\nEnv Model\nEnvironment\nPhysical System\nResearcher\nAgent Model\nEnv. Model\nAction\nAt\nRt+1\nSt+1\nReward\nRt\nState\nSt\nFigure 2: A traditional RL diagram, augmented with fea-\ntures required for counterfactual reasoning (shaded above).\nThere is a physical system that governs behavior within\nthe environment that may only be partially known. The\nresearcher has a model of the environment’s physics and,\nwhen attempting to explain agent behavior, must implic-\nitly form a model of the agent’s model. Arrows emanating\nfrom the researcher point to elements of the system where\nthe researcher may intervene. The dotted line from the re-\nsearcher to the environment is possible in TOYBOX, but is\nnot typically considered.\nArchitecture. Figure 1 depicts the TOYBOX architecture.\nThe game logic is written in Rust. Every game implements\ntwo core structs: Config and State. The Config struct\ncontains data that we would generally only expect to be\ninitialized at the start of an episode (i.e., a game, which\nmay include multiple lives). The State struct contains\ndata that may change between frames. At any point during\nexecution, a TOYBOX game can be paused, state exported\nand modiﬁed, and resumed with the new state.\nAside: Why Atari? Given the range of open problems in\ndeep RL, creating a new ALE-like system may not seem like\nan effective way to facilitate cutting-edge deep RL research.\nHowever, there are still many poorly understood properties\nof Atari games and agent behavior on them.\nThere are many axes of complexity in the reinforcement\nlearning environment: required planning horizon, reward\nfunction assignment, number of actions available, environ-\nment stochasticity, and state representation are all different\ncomponents affecting the total complexity of the environ-\nment, each presenting unique challenges for policy learning:\nTOYBOX\n• Planning Horizon Atari environments requiring tempo-\nrally extended planning strategies for success, as in Frost-\nbite, still present challenges to deep reinforcement learn-\ning algorithms (Jiang et al., 2015; Lake et al., 2017;\nFarquhar et al., 2018).\n• Reward Function Environments with sparse reward\nfunctions are another sticking point for RL, having only\nrecently seen signiﬁcant progress on games in this cate-\ngory (Ecoffet et al., 2018; Salimans & Chen, 2018; Burda\net al., 2019). Atari examples include Pitfall and Mon-\ntezuma’s Revenge.\n• State Representation Operating over pixel representa-\ntions like Atari games means the agent experiences a huge\nstate space, but the underlying objects deﬁning those sen-\nsory representations can often be represented more com-\npactly (Guestrin et al., 2003; Diuk et al., 2008; Kansky\net al., 2017). Models trained on a compact representation\nof state can train more quickly (Keramati et al., 2018;\nMelnik et al., 2018), but the underlying environment dy-\nnamics have not changed.\n• Action Space Complexity Increasing the size of the ac-\ntion space can quickly lead to intractable computation\nfor Q-value or policy function approximation, especially\nwhen that approximation is computationally expensive\nas in deep RL (Dulac-Arnold et al., 2015). Atari games\navailable in ALE allow 4-18 actions (Mnih et al., 2013).\nWhile this may seem small, even environments with as\nfew as 10 actions can present challenges to efﬁcient learn-\ning (Dulac-Arnold et al., 2012).\n• Environment Stochasticity Previous work has shown\nthat environments with higher stochasticity can be more\ndifﬁcult for some types of RL algorithms to learn (Hen-\nderson et al., 2017). Atari games are deterministic, but\nTOYBOX enables Atari-like environments with parame-\nterized stochasticity.\nDespite these open challenges, it has been argued that\nAtari’s environments are not sufﬁciently complex to evalu-\nate reinforcement learning agents because the source code\nis small (Zhang et al., 2018). However, source code size\nas minimum description length is a poor proxy for envi-\nronment complexity. As Raghu et al. (2017) have shown,\nErd˝os-Selfridge-Spencer games can be represented quite\ncompactly, simply requiring the assignment of two parame-\nters, but represent a large combinatorial space of potential\ngames for evaluating RL algorithms.\nUltimately, each of these axes of complexity is observed\nthrough the agent’s interaction with the environment. Fig-\nure 2 depicts the traditional RL diagram, overlaid with the\ninterventions a researcher may make, as well as the models\nTable 1:\nTOYBOX vs ALE performance.\nThousand\nframes per second (kFPS) on a MacBook Air (OSX Version\n10.13.6) with a 1.6 GHz Intel Core i5 processor having 4 log-\nical cores. Rates are averaged over 30 trials of 10 000 steps\nand reported to two signiﬁcant digits, with standard error.\nWe consistently observed an approximately 95% slowdown\nwhen interacting with both ALE (C++) and TOYBOX (Rust)\nvia OpenAI Gym. All benchmarks are run from CPython\n3.5 and include FFI overhead (via atari-py for ALE).\nRaw kFPS\nGym kFPS\nBreakout\nALE\n52 (1.3)\n3.4 (0.065)\nTOYBOX\n230 (5.4)\n7.2 (0.23)\nAmidar\nALE\n61 (2.9)\n3.0 (0.083)\nTOYBOX\n250 (2.3)\n6.0 (0.112)\nSpace\nInvaders\nALE\n55 (1.3)\n3.9 (0.072)\nTOYBOX\n120 (3.4)\n5.2 (0.082)\nthat inform both the researcher’s and the agent’s decision-\nmaking. The solid arrows represent current avenues for\nexperimentation used by deep RL researchers to evaluate\nmodels: sensory perception of state, action selection manip-\nulations, and reward function deﬁnition.\nWithout intervention or introspection of the environment,\nresearchers must use observational data of agent behavior\nto reason about experimental results. TOYBOX enables new\nmethodology for experimentation in deep RL. Ultimately,\nwe mimic Atari games because there is a font of untapped\nresearch questions related to testing and explaining the be-\nhavior of deep RL agents. We chose our initial set of games\nto establish that there are surprising results on even seem-\ningly “solved” games such as Breakout and Space Invaders.\n3. Evaluation\nPerformance. We achieved R1 by designing a simple CPU\ngraphics library; we demonstrate TOYBOX efﬁciency (R2)\nin Table 1. Note that TOYBOX permits researchers to pro-\ncess games entirely in grayscale and achieve substantial\nadditional performance gains. However, since this is not\na feature offered in ALE, we only compared against the\nTOYBOX RGB(A) rendering.\nFidelity. Figure 3 depicts frames at roughly equivalent\npoints in the execution of a ﬁxed action trace. If TOYBOX\nperfectly reproduced the games in ALE, the frames would\nbe exactly the same. Three factors prevent exact replication\nof games: (1) Atari 2600 game source code is not available,\n(2) there are no formal speciﬁcations and few informal spec-\niﬁcations of games,2 and (3) inferring arbitrarily complex\nprograms from data is extremely challenging (Raychev et al.,\n2Some informal speciﬁcations contain errors (e.g., the Atari\nmanual for Breakout refers to a row of bricks that does not exist).\nTOYBOX\n(a) Upper: ALE; lower: TOYBOX.\n(b) Left: ALE; right: TOYBOX.\n(c) Upper: ALE; lower: TOYBOX.\nFigure 3: Side-by-side comparisons of screen shots from ALE and TOYBOX Atari games. Each game represents a different\ndeterministic action trace, but traces are the same between ALE and TOYBOX. TOYBOX implementations of Breakout and\nSpace Invaders have nondeterministic elements. Amidar is deterministic in both ALE and TOYBOX. Due to idiosyncrasies\nat the start of ALE Amidar gameplay, the frames are not from identical points in the action trace; frames for Breakout and\nSpace Invaders are. Note that for Amidar ALE appears to be missing an enemy, which is due to Atari only rendering a\nsubset of sprites each frame due to computational constraints.\n2016). Therefore, comparing frames of program traces is\nnot a sufﬁcient measure of how closely we have approxi-\nmated ALE games.\nA human study could help assess whether the two imple-\nmentations are are sufﬁciently alike. While we did solicit\nfeedback from Atari aﬁcionados during development (via a\nplayable interface), we did not view human perception of\nequivalence as a sufﬁcient measure of ﬁdelity. Human play-\ners rely on unique problem-solving capabilities that deep\nRL agents have not yet achieved, while deep networks are\nundeterred by the kind of noise that can confuse humans\n(Szegedy et al., 2014; Dubey et al., 2018). Instead, we fo-\ncused on tuning our environments to produce comparable\npost-training agent performance.\nMethodology. We used three off-the-shelf implementations\nof training algorithms with default parameter settings for\n5e7 steps from OpenAI Baselines (Dhariwal et al., 2017):\na2c (Mnih et al., 2016), acktr (Wu et al., 2017), and\nppo2 (Schulman et al., 2017). Due to issues with vari-\nability across agents and environments (Henderson et al.,\n2017; Clary et al., 2018; Jordan et al., 2018), we trained ten\nreplicates for each of these training algorithms, differenti-\nated by their random seed. Since there are various other\nuncontrolled sources of randomness, we evaluated each of\nthese thirty agents per game using thirty unique random\nseeds over thirty games. Figure 4 depicts our results: we\nﬁnd that agents achieve sufﬁciently similar performance in\neach analogous environment, and have roughly equivalent\nrankings (idiosyncrasies discussed in the in Fig. 4 caption).\nFindings. TOYBOX is much faster than ALE and our rank-\ning results in Fig. 4 show that our current implementations\nare comparable to their ALE counterparts. We will continue\nto strive for ﬁdelity over the course of game development.\n4. Case Studies\nWe demonstrate (R3: Customization) with four case studies,\nin which we test the post-training performance3 of agents\naccording to some hypothesis about behavior. This sort of\ntesting is useful for evaluating a single agent prior to de-\nployment (i.e., acceptance testing) or as existential proof\nfor behavior under counterfactual conditions. None of these\ntests are currently possible in ALE because they all rely on\nresuming gameplay from an arbitrary modiﬁed state. Fur-\nthermore, all experiments in this section can be expressed in\nfewer than 200 lines of code, in addition to the code required\nfor loading up the OpenAI baselines models. We include a\ncode snippet from one of our experiments in Figure 7.\nBreakout: Polar Angles.\nAn agent that has learned to\nplay Breakout must have at least learned to hit the ball. Our\nﬁrst test manipulates the starting angle of the ball.\nWe modiﬁed the start state to change the initial launch angle\nof the ball in 5◦increments (72 conﬁgurations). Figure 5a\ndepicts the results. Note that the agent fails to achieve\nany score with horizontal ball angles: since Breakout has\n3The analyses in this paper focus on evaluations of post-training\nperformance, but TOYBOX interventions can be applied at any\ntime–including during training.\nTOYBOX\nFigure 4: Rankings across 300 model replicates per model,\nper game, with standard error. Each horizontal bar is the\naverage performance for a trained model, evaluated over 30\ngames. Stridations should be similar in both environments.\nBreakout: One training seed led an agent trained using\nacktr to have abysmally poor performance in every trial\n(i.e., all average scores below 2). Amidar: The lack of\nwithin-game variation is reﬂected in the short error bars\nand similar rankings between backends. Space Invaders:\nTOYBOX only implements the ﬁrst level of Space Invaders.\nSince there is currently no way to detect levels within ALE,\nwe let both agents play indeﬁnitely. However, because\nSpace Invaders levels vary considerably, performance is not\ncomparable. For this reason we use different scales for the\nscore (x-axis).\n(a) Polar starts\n(b) Tunneling\nFigure 5: Breakout case study. Results from counterfactual\nqueries for starting angles and tunnels. Both tests were\nrun on an agent trained with the default OpenAI Baselines\nparameters for ppo for 5e7 steps, one life, and a 4 min.\ntimeout. (a): The black lines indicate the starting angles\nseen during training, the light gray area the maximum score\nachieved from this starting angle, and the dark gray area\nrepresents the mean score achieved across trials. (b): Brick\nhue scaled according to the inverse of the median number of\nsteps required to clear the particular brick in that test; bright-\nyellow represents fewer steps and red represents more steps\n(17-400).\nno gravity, balls simply bounce horizontally forever, never\nhitting any bricks or threatening the paddle. The agent\nalso sometimes struggled with vertical angles. When we\nobserved this behavior, the agent would keep the ball aligned\nperfectly in the center of the board, hitting it precisely in\nthe center of the paddle, failing to make progress. This is an\nunexpected behavior that is entirely unlike human gameplay.\nIn all, we found the agent to be resilient to starting angles,\nalbeit with high variance. This suggests that an agent can be\nsuccessful even with balls traveling at angles it may never\nhave observed in training, a powerful recommendation for\nthe training algorithms that produced such robust RL agents.\nBreakout: Tunneling.\nOne of the most promising behav-\niors observed in deep RL has been the apparent ability to\nlearn higher-level strategies. Perhaps no high-level strategy\nhas been written about more than “tunneling” in the game of\nBreakout, which happens when the player clears a column\nof bricks, causing the ball to bounce through the hole and\nonto the ceiling, clearing many bricks rapidly (Mnih et al.,\n2015; Greydanus et al., 2018). One way to test whether an\nagent intentionally exploits tunneling is to give it a board\nwith a nearly complete tunnel, save for a single brick, and\ntest whether the agent can prioritize aiming at that single\nbrick.\nFor every brick, we removed all other bricks in the column,\ncreating a nearly-completed tunnel. Figure 5b depicts the\nresults: the value for each brick is the reciprocal of the\nmedian number of time steps before that brick was removed.\nTOYBOX\n(a) Accumulated game score of a single a2c-trained agent.\n(b) Ranking of 30 model replicates.\nFigure 6: Amidar case study. for four enemy movement protocols (“Control” is a lookup table, “Amidar” is the “Amidar\nmovement,” ”Random” enemies move in a random direction at every junction, and “Target” causes enemies to pursue the\nplayer when it is in line of sight). (a): Upper left: baseline performance of the agent on each of the four protocols. Lower\nleft: baseline performance of the agent on each protocol without the ability to jump over enemies. Upper right: “Ganging up”\ntest, where all agents start close to the player. Lower right: “Ganging up” test with no jump. (b): Score ranking of 30 model\nreplicates for the baseline condition with jumps (i.e., the upper left corner of the left graph) for each movement protocol.\nIf an agent were able to build tunnels, we would expect, for\nexample, symmetry along one or both axes. Instead, we see\nthat the agent clears one column in the center very quickly,\nthe left adjacent column and some bricks in the upper left\nregion a bit more slowly, and the remaining bricks take all\nabout the same time. Observing agent gameplay, we saw\nthe agents hit the ball to predictable locations, regardless of\nthe board conﬁguration.\nAmidar: Enemy Protocol.\nSuppose we would like to test\nwhether an agent has learned to avoid adversarial elements\nof a game: e.g., the enemies in Amidar. To test this, we\nmight drop the agent around the corner from an enemy, or\nposition the enemies to “gang up” on the player, forcing the\nagent to move in a particular direction.\nThis kind of intervention is only meaningful if enemy po-\nsition is a function of current location. Observation led us\nto conclude that enemies move in ﬁxed loops, likely im-\nplemented as lookup tables. This contrasts with “Amidar\nmovement” is believed to dictate enemy behavior.4 The\nprotocol matters for intervention because, for a lookup table,\nmoving enemies will have no effect: enemies will simply\n“teleport” to the next location in the lookup table.\nThe upper left plot in Fig. 6a shows a baseline test for how\n4An enemy moves with a diagonal velocity, ﬂipping the ver-\ntical direction when encountering the top or bottom of the board\nand horizontal direction when encountering the left or right edge\n(https://en.wikipedia.org/wiki/Amidar)\n# Fourth test: Target Player\n# Delete Existing Enemies\nconfig = toybox.get_config_json()\nconfig[’enemies’] = []\n# Add Enemies that chase the player:\nfor i in range(5):\nconfig[’enemies’].append({\n’EnemyTargetPlayer’ : {\n’start’ : starts[i],\n’start_dir’: ’Right’,\n’dir’: ’Right’,\n’player_seen’: None\n}})\n# Update game configuration\ntoybox.write_config_json(config)\n# Run 30 trials with OpenAI Gym API\nobs = env.reset()\nfor trial in range(n_trials):\nn_steps = 0\ndone = False\n# until death or max_steps\nwhile n_steps < max_steps and not done:\naction = agent.step(obs)\nobs, _, death, info = env.step(\naction)\ndone = death and not toybox.\ngame_over()\nlog_step(toybox)\nn_steps += 1\nlog_after_episode(toybox)\nFigure 7: Code snippet from our Amidar Protocol test. We\nbuild JSON conﬁg in Python and run it in TOYBOX.\nTOYBOX\nFigure 8: Space Invaders case study. The top three bands\ndepict the tests of the solitary ﬁrst, second, and third shields\nrespectively. The bars depict the total number of steps spent\nin the corresponding horizontal location, while the boxplots\ndepict the scores. The fourth band shows the behavior for\nall shields present (green; score boxplots on the left) and no\nshields present (purple; score boxplots on the right). The\nbottom band shows the Space Invaders terrain and default\npositions of the shields.\nan individual trained agent performs under each of four\ndifferent enemy movement protocols: (1) a lookup table, on\nwhich the model was trained, (2) the “Amidar movement”\nprotocol, (3) a random protocol, where at each junction the\nenemy chooses a random direction, and (4) an adversarial\nprotocol, where enemies explore via random turns until\nthe player is within line of sight, at which time they move\ntoward the player’s location. Note that, since enemies start\nfar away from the player, the agent can (and does) easily\nmake progress at the start of the game, regardless of enemy\nposition. However, as the game progresses, the enemies\nclose in and there are fewer opportunities for rewards.\nThe upper right plot in Fig. 6a shows a test in which ene-\nmies “gang up” on the player: the enemies start position is\nmodiﬁed to be close to the player. We were at ﬁrst surprised\nto see how well the agent did; however, upon examination,\nwe found we found that the agent was using up the jump\nbutton, which allows the player to bypass enemies, at be-\nginning of the game. The lower half of Fig. 6a depicts the\nresults of running the baseline and test for no jumps: while\nthe baseline performs similar, the player dies quickly for all\nnon-lookup table enemy protocols.\nSpace Invaders: Shield Usage.\nIn Space Invaders, the\nplayer can seek refuge under three shields from the frontier\nof alien ships shooting down. We ran a test to see whether\nremoving two of the three shields would cause an agent to\nuse the remaining one more often. We also ran two baseline\ncomparisons for a ﬁxed amount of time: one where all\nsheilds are present (the default setting) and one where no\nshields were present.\nFigure 8 shows the results under test. Since score provides\nan incomplete picture of agent behavior, we also tracked\nthe agent’s location (a simple query in TOYBOX). We ob-\nserve that the player does not appear to change its preferred\nlocations under any of the tests.\nA Note on Negative Results: Space Invaders, as we have\nimplemented it, has turned out to be a fairly uninteresting\ngame. Randomly selecting from the trimmed action set\nthat OpenAI Gym allows can lead to fairly good perfor-\nmance. Furthermore, our implementation, which included\nboth random and adversarial enemy behavior, led the agent’s\nbehavior to be invariant to randomness in enemy behavior.\nFindings. We have shown a range of interventions and\nqueries possible with TOYBOX, all of which would be\nimpossible to conduct using ALE. The interventions we\ndemonstrated were designed to demonstrate the power of\nTOYBOX’s design and implementation, rather than to satisfy\nany particular RL research agenda. We were able to rapidly\niterate on all of our experiments due to TOYBOX’s fast per-\nformance and its simple API for editing state. In addition\nto highlighting TOYBOX’s capacity for evaluating a single\nagent, we have shown how TOYBOX may be used to eval-\nuate models, by comparing the post-training performance\nranking under test.\n5. Related Work\nWe are hardly the ﬁrst to suggest new or different bench-\nmarks for deep RL (Kansky et al., 2017; Zhang et al., 2018;\nWang et al., 2019). Four major qualities differentiate TOY-\nBOX from prior work: (1) it is based on a widely used and\naccepted community standard (ALE); (2) results on ALE\ncan be replicated and compared in TOYBOX, providing con-\ntinuity to individual research trajectories; (3) a wide array\nof features of TOYBOX environments are intervenable; fur-\nthermore, a particular conﬁguration is easily exported and\ncan be shared as part to further replication efforts; and (4)\nan individual game may be modiﬁed to produce a family of\ngames, leading to a potentially inﬁnite number of environ-\nments per-game; for example, the injection of real-world\nimages into the background of Breakout described in (Zhang\net al., 2018) would be trivial to implement in TOYBOX.\nRecall the available interventions in the traditional RL re-\nsearch environment shown in Figure 2. Most existing work\nTOYBOX\nmanipulates the state input to the agent (i.e., the agent’s per-\nception of state), the reward function, or the agent’s actions,\ne.g.:\n• State input: Injecting real-world data into the background\nof Atari 2600 games simulates non-random noise (Zhang\net al., 2018). Skipping frames periodically is a critically\nimportant hyperparameter for tuning algorithms to play\nAtari (Mnih et al., 2013; Braylan et al., 2015; Seita, 2016).\n• Reward function:\nHybrid reward structures decom-\npose the reward function, making it easier for some\nagents to learn particularly difﬁcult games in the Atari\nsuite (Van Seijen et al., 2017).\n• Agent actions: Sticky actions, human starts, and ran-\ndom starts are all methods for intervening on the agent’s\nactions outside the normal parameters of ϵ-greedy explo-\nration (Sutton et al., 1998; Mnih et al., 2013; Bellemare\net al., 2013; Nair et al., 2015).\nThese efforts can help combat overﬁtting, learning spurious\ncorrelations, or generally failing to make progress on a task.\nTOYBOX is orthogonal to such efforts.\nWe have introduced relevant citations throughout the pa-\nper. Here we highlight critical work that was not otherwise\nmentioned.\nEvaluation and Replication. Recent investigations into in\nhow the community handles the evaluation and replication\nof agent performance has exposed some serious challenges\nthat the community needs to address (Henderson et al., 2017;\nBalduzzi et al., 2018; Clary et al., 2018; Jordan et al., 2018).\nEnvironments such as TOYBOX and evaluations of the style\npresented in Section 4 are one possible way to ameliorate\nissues surrounding replication, robustness, and variability.\nAdversarial RL. Much work on adversarial RL focuses\non exploiting decision boundaries (Mandlekar et al., 2017),\nadding nonrandom noise to state input for the purpose of\naltering or misdirecting policies (Huang et al., 2017), or\nintroducing additional agents to apply adversarial force dur-\ning training to produce agents with more robust policies in\nphysics simulations (Pinto et al., 2017).\nSaliency maps. Saliency maps were developed as an in-\nsight into model behavior (Simonyan et al., 2013), but more\nrecently have been put forth as tool for explainability (Grey-\ndanus et al., 2018). We show that in at least one case,\nsaliency maps can be misleading, due in part to bias ex-\nempliﬁed by the researcher’s models of the agent and the\nenvironment as shown in Fig. 2. Experiments enabled by\nTOYBOX provide much more speciﬁc information and can\ndisambiguate competing hypotheses about agent behavior.\n6. Discussion\nThis paper is a proof-of-concept for experimentation about\nthe behavior of deep RL agents. However, there are many\npossible applications beyond this type of post-training test-\ning:\nRejection sampling/dynamic analysis. One of the biggest\nstrengths offered in TOYBOX is the ability to answer arbi-\ntrary questions about the environment structures and code at\nany time. Agents may encounter local minima during train-\ning that are not representative of the target deployment dis-\ntribution, due to factors such as random seeds (Irpan, 2018).\nTraining replicates with many random seeds is a costly so-\nlution to this problem. Instead, researchers could use TOY-\nBOX to monitor environmental features to test whether an\nagent is spending too much time in an undesirable state.\nSimilar types of model monitoring could be used to identify\n“detachment,” a condition of the agent-environment interac-\ntion that induces catastrophic forgetting (Kirkpatrick et al.,\n2017; Ecoffet et al., 2018).\nDatasets from game families. The ability to generate a\nfamily of games with similar but different mechanics pro-\nvides a convenient dataset which can be used in a variety\nof ways. For example, with TOYBOX, a researcher can\ndeﬁne a family of Breakout-style games with slightly differ-\nent movement physics (e.g., ball velocity and acceleration,\npaddle-bounce mechanics) sampled from some real-valued\nparameter domain. This can be used to create a train/test\nsplit over environments (Cobbe et al., 2018), for supporting\ntransfer learning experiments from one game physics to an-\nother (Taylor & Stone, 2009), or for testing generalization\nacross multiple environments (Guestrin et al., 2003).\nAdversarial testing. With total control over environment\ndynamics, trained agents can be stress-tested by running\nthe agent on progressively more difﬁcult versions of the\ngame. Tests of this form can serve to disambiguate agent\nbehavior that can be explained in multiple ways—much as\nTOYBOX’s more advanced Amidar movement protocols re-\nvealed agents had not necessarily learned to avoid enemies\nso much as memorize their observed paths. Difﬁculty can\nbe increased by increasing stochasticity in the environment,\nor increase the speed or accuracy of adversarial game ele-\nments. Similar methods could be used to create a suite of\ncurriculum learning environments.\n7. Conclusions\nWe have shown that TOYBOX unlocks novel and impor-\ntant capabilities for evaluating deep reinforcement learning\nagents. We introduce a new paradigm for thinking about\nevaluating agents, in the style of acceptance testing. We\ndemonstrate TOYBOX capabilities with four case studies\nand outline a variety of other applications.\nTOYBOX\nReferences\nBalduzzi, D., Tuyls, K., Perolat, J., and Graepel, T. Re-\nevaluating evaluation. In Advances in Neural Information\nProcessing Systems, 2018.\nBellemare, M. G., Veness, J., and Bowling, M. Investi-\ngating contingency awareness using atari 2600 games.\nIn Proceedings of the Twenty-Sixth AAAI Conference on\nArtiﬁcial Intelligence, 2012.\nBellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M.\nThe Arcade Learning Environment: An Evaluation Plat-\nform for General Agents. Journal of Artiﬁcial Intelligence\nResearch, 47:253–279, jun 2013.\nBraylan, A., Hollenbeck, M., Meyerson, E., and Miikku-\nlainen, R. Frame skip is a powerful parameter for learning\nto play atari. In Workshops at the Twenty-Ninth AAAI\nConference on Artiﬁcial Intelligence, 2015.\nBurda, Y., Edwards, H., Storkey, A., and Klimov, O. Explo-\nration by random network distillation. In International\nConference on Learning Representations, 2019.\nClary, K., Tosch, E., Foley, J., and Jensen, D.\nLet’s\nPlay Again: Variability of Deep Reinforcement Learning\nAgents in Atari Environments. In NeurIPS 2018 Work-\nshop on Critiquing and Correcting Trends in Machine\nLearning, 2018.\nCobbe, K., Klimov, O., Hesse, C., Kim, T., and Schulman,\nJ. Quantifying generalization in reinforcement learning.\narXiv preprint arXiv:1812.02341, 2018.\nDhariwal, P., Hesse, C., Klimov, O., Nichol, A., Plappert,\nM., Radford, A., Schulman, J., Sidor, S., and Wu, Y. Ope-\nnAI Baselines.\nhttps://github.com/openai/\nbaselines, 2017.\nDiuk, C., Cohen, A., and Littman, M. L. An object-oriented\nrepresentation for efﬁcient reinforcement learning. In\nProceedings of the 25th International Conference on\nMachine Learning, ICML ’08, pp. 240–247, New York,\nNY, USA, 2008. ACM. ISBN 978-1-60558-205-4. doi:\n10.1145/1390156.1390187.\nDubey, R., Agrawal, P., Pathak, D., Grifﬁths, T. L., and\nEfros, A. A. Investigating human priors for playing video\ngames. arXiv preprint arXiv:1802.10217, 2018.\nDulac-Arnold, G., Denoyer, L., Preux, P., and Gallinari, P.\nFast reinforcement learning with large action sets using\nerror-correcting output codes for mdp factorization. In\nFlach, P. A., De Bie, T., and Cristianini, N. (eds.), Ma-\nchine Learning and Knowledge Discovery in Databases,\npp. 180–194, Berlin, Heidelberg, 2012. Springer Berlin\nHeidelberg.\nDulac-Arnold, G., Evans, R., van Hasselt, H., Sunehag, P.,\nLillicrap, T., Hunt, J., Mann, T., Weber, T., Degris, T., and\nCoppin, B. Deep reinforcement learning in large discrete\naction spaces. arXiv preprint arXiv:1512.07679, 2015.\nEcoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., and\nClune, J. Montezuma’s Revenge Solved by Go-Explore,\na New Algorithm for Hard-Exploration Problems (Sets\nRecords on Pitfall, Too). https://eng.uber.com/\ngo-explore/, Nov. 2018.\nFarquhar, G., Rocktaschel, T., Igl, M., and Whiteson, S.\nTreeqn and atreec: Differentiable tree-structured models\nfor deep reinforcement learning. In ICLR 2018: Proceed-\nings of the Sixth International Conference on Learning\nRepresentations, April 2018.\nGreydanus, S., Koul, A., Dodge, J., and Fern, A. Visu-\nalizing and understanding atari agents. arXiv preprint\narXiv:1711.00138, 2018.\nGuestrin, C., Koller, D., Gearhart, C., and Kanodia, N. Gen-\neralizing plans to new environments in relational mdps. In\nProceedings of the 18th international joint conference on\nArtiﬁcial intelligence, pp. 1003–1010. Morgan Kaufmann\nPublishers Inc., 2003.\nHenderson, P., Islam, R., Bachman, P., Pineau, J., Precup,\nD., and Meger, D. Deep Reinforcement Learning that\nMatters. In AAAI Conference on Artiﬁcial Intelligence\n(AAAI). arXiv preprint 1709.06560, 2017.\nHessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostro-\nvski, G., Dabney, W., Horgan, D., Piot, B., Azar, M., and\nSilver, D. Rainbow: Combining improvements in deep re-\ninforcement learning. In Thirty-Second AAAI Conference\non Artiﬁcial Intelligence, 2018.\nHuang, S., Papernot, N., Goodfellow, I., Duan, Y., and\nAbbeel, P. Adversarial attacks on neural network policies.\nIn ICLR Workshop, 2017.\nIrpan, A.\nDeep reinforcement learning doesn’t work\nyet. https://www.alexirpan.com/2018/02/\n14/rl-hard.html, 2018.\nJiang, N., Kulesza, A., Singh, S., and Lewis, R. The depen-\ndence of effective planning horizon on model accuracy.\nIn Proceedings of the 2015 International Conference on\nAutonomous Agents and Multiagent Systems, pp. 1181–\n1189. International Foundation for Autonomous Agents\nand Multiagent Systems, 2015.\nJordan, S. M., Cohen, D., and Thomas, P. S. Using Cumu-\nlative Distribution Based Performance Analysis to Bech-\nmark Models. In NeurIPS 2018 Workshop on Critiquing\nand Correcting Trends in Machine Learning, 2018.\nTOYBOX\nKansky, K., Silver, T., M´ely, D. A., Eldawy, M., L´azaro-\nGredilla, M., Lou, X., Dorfman, N., Sidor, S., Phoenix,\nS., and George, D. Schema networks: Zero-shot trans-\nfer with a generative causal model of intuitive physics.\nIn Proceedings of the 34th International Conference on\nMachine Learning, 2017.\nKeramati, R., Whang, J., Cho, P., and Brunskill, E. Strate-\ngic object oriented reinforcement learning.\nCoRR,\nabs/1806.00175, 2018.\nKirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J.,\nDesjardins, G., Rusu, A. A., Milan, K., Quan, J., Ra-\nmalho, T., Grabska-Barwinska, A., et al. Overcoming\ncatastrophic forgetting in neural networks. Proceedings\nof the National Academy of Sciences, pp. 201611835,\n2017.\nLake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gersh-\nman, S. J. Building machines that learn and think like\npeople. Behavioral and Brain Sciences, 40, 2017.\nMachado, M. C., Bellemare, M. G., Talvitie, E., Veness,\nJ., Hausknecht, M. J., and Bowling, M.\nRevisiting\nthe Arcade Learning Environment: Evaluation Proto-\ncols and Open Problems for General Agents.\nCoRR,\nabs/1709.06009, 2017.\nMandlekar, A., Zhu, Y., Garg, A., Fei-Fei, L., and Savarese,\nS. Adversarially robust policy learning: Active construc-\ntion of physically-plausible perturbations. In Intelligent\nRobots and Systems (IROS), 2017 IEEE/RSJ International\nConference on, pp. 3932–3939. IEEE, 2017.\nMelnik, A., Fleer, S., Schilling, M., and Ritter, H. Modu-\nlarization of end-to-end learning: Case study in arcade\ngames. In NeurIPS 2018 Workshop on Causal Learning,\n2018.\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,\nAntonoglou, I., Wierstra, D., and Riedmiller, M. Playing\natari with deep reinforcement learning. arXiv preprint\narXiv:1312.5602, 2013.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,\nJ., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-\nland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik,\nA., Antonoglou, I., King, H., Kumaran, D., Wierstra, D.,\nLegg, S., and Hassabis, D. Human-level Control through\nDeep Reinforcement Learning. Nature, 518:529 EP –, 02\n2015.\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,\nT., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn-\nchronous methods for deep reinforcement learning. In\nInternational conference on Machine Learning, pp. 1928–\n1937, 2016.\nNair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon,\nR., De Maria, A., Panneershelvam, V., Suleyman, M.,\nBeattie, C., Petersen, S., et al. Massively parallel meth-\nods for deep reinforcement learning.\narXiv preprint\narXiv:1507.04296, 2015.\nNichol, A., Pfau, V., Hesse, C., Klimov, O., and Schulman,\nJ. Gotta learn fast: A new benchmark for generalization\nin rl. arXiv preprint arXiv:1804.03720, 2018.\nPinto, L., Davidson, J., Sukthankar, R., and Gupta, A. Ro-\nbust adversarial reinforcement learning. In Precup, D.\nand Teh, Y. W. (eds.), Proceedings of the 34th Interna-\ntional Conference on Machine Learning, volume 70 of\nProceedings of Machine Learning Research, pp. 2817–\n2826, International Convention Centre, Sydney, Australia,\n06–11 Aug 2017. PMLR.\nRaghu, M., Irpan, A., Andreas, J., Kleinberg, R., Le, Q. V.,\nand Kleinberg, J. Can Deep Reinforcement Learning\nsolve Erdos-Selfridge-Spencer Games? arXiv preprint\narXiv:1711.02301, 2017.\nRaychev, V., Bielik, P., Vechev, M., and Krause, A. Learning\nprograms from noisy data. In Proceedings of the 43rd An-\nnual ACM SIGPLAN-SIGACT Symposium on Principles\nof Programming Languages (POPL 2016), pp. 761–774.\nACM, 2016.\nSalimans, T. and Chen, R. Learning Montezuma’s Revenge\nfrom a Single Demonstration. In NeurIPS 2018 Deep\nReinforcement Learning Workshop, 2018.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nKlimov, O. Proximal Policy Optimization Algorithms.\narXiv preprint arXiv:1707.06347, 2017.\nSeita, D. Frame Skipping and Pre-Processing for Deep Q-\nNetworks on Atari 2600 Games. http://tiny.cc/\nd7uh2y, 2016.\nSimonyan, K., Vedaldi, A., and Zisserman, A. Deep inside\nconvolutional networks: Visualising image classiﬁcation\nmodels and saliency maps, 2013.\nSutton, R. S., Barto, A. G., et al. Reinforcement learning:\nAn introduction. MIT press, 1998.\nSzegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan,\nD., Goodfellow, I., and Fergus, R. Intriguing proper-\nties of neural networks. In International Conference on\nLearning Representations, 2014.\nTaylor, M. E. and Stone, P. Transfer learning for reinforce-\nment learning domains: A survey. J. Mach. Learn. Res.,\n10:1633–1685, December 2009. ISSN 1532-4435.\nTOYBOX\nVan Hasselt, H., Guez, A., and Silver, D. Deep reinforce-\nment learning with double q-learning. In AAAI, volume 2,\npp. 5, 2016.\nVan Seijen, H., Fatemi, M., Romoff, J., Laroche, R., Barnes,\nT., and Tsang, J. Hybrid reward architecture for rein-\nforcement learning. In Advances in Neural Information\nProcessing Systems, pp. 5392–5402, 2017.\nWang, R., Lehman, J., Clune, J., and Stanley, K. O. Paired\nopen-ended trailblazer (poet): Endlessly generating in-\ncreasingly complex and diverse learning environments\nand their solutions. arXiv preprint arXiv:1901.01753,\n2019.\nWu, Y., Mansimov, E., Grosse, R. B., Liao, S., and Ba,\nJ. Scalable trust-region method for deep reinforcement\nlearning using kronecker-factored approximation.\nIn\nAdvances in neural information processing systems, pp.\n5279–5288, 2017.\nZhang, A., Wu, Y., and Pineau, J. Natural environment\nbenchmarks for reinforcement learning. arXiv Preprint\narXiv:1811.06032, 2018.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-05-07",
  "updated": "2019-05-07"
}