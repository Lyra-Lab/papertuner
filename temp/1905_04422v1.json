{
  "id": "http://arxiv.org/abs/1905.04422v1",
  "title": "Controlled Natural Languages and Default Reasoning",
  "authors": [
    "Tiantian Gao"
  ],
  "abstract": "Controlled natural languages (CNLs) are effective languages for knowledge\nrepresentation and reasoning. They are designed based on certain natural\nlanguages with restricted lexicon and grammar. CNLs are unambiguous and simple\nas opposed to their base languages. They preserve the expressiveness and\ncoherence of natural languages. In this report, we focus on a class of CNLs,\ncalled machine-oriented CNLs, which have well-defined semantics that can be\ndeterministically translated into formal languages, such as Prolog, to do\nlogical reasoning. Over the past 20 years, a number of machine-oriented CNLs\nemerged and have been used in many application domains for problem solving and\nquestion answering. However, few of them support non-monotonic inference. In\nour work, we propose non-monotonic extensions of CNL to support defeasible\nreasoning.\n  In the first part of this report, we survey CNLs and compare three\ninfluential systems: Attempto Controlled English (ACE), Processable English\n(PENG), and Computer-processable English (CPL). We compare their language\ndesign, semantic interpretations, and reasoning services. In the second part of\nthis report, we first identify typical non-monotonicity in natural languages,\nsuch as defaults, exceptions and conversational implicatures. Then, we propose\ntheir representation in CNL and the corresponding formalizations in a form of\ndefeasible reasoning known as Logic Programming with Defaults and Argumentation\nTheory (LPDA).",
  "text": "Controlled Natural Languages and Default Reasoning\nby\nTiantian Gao\nDepartment of Computer Science\nStony Brook University\nAbstract\nControlled natural languages (CNLs) are effective languages for knowledge represen-\ntation and reasoning. They are designed based on certain natural languages with restricted\nlexicon and grammar. CNLs are unambiguous and simple as opposed to their base lan-\nguages. They preserve the expressiveness and coherence of natural languages. In this\nreport, we focus on a class of CNLs, called machine-oriented CNLs, which have well-\ndeﬁned semantics that can be deterministically translated into formal languages, such as\nProlog, to do logical reasoning. Over the past 20 years, a number of machine-oriented\nCNLs emerged and have been used in many application domains for problem solving and\nquestion answering. However, few of them support nonmonotonic inference. In our work,\nwe propose nonmonotonic extensions of CNL to support defeasible reasoning.\nIn the ﬁrst part of this report, we survey CNLs and compare three inﬂuential sys-\ntems: Attempto Controlled English (ACE), Processable English (PENG), and Computer-\nprocessable English (CPL). We compare their language design, semantic interpretations,\nand reasoning services. In the second part of this report, we ﬁrst identify typical nonmono-\ntonicity in natural languages, such as defaults, exceptions and conversational implicatures.\nThen, we propose their representation in CNL and the corresponding formalizations in a\nform of defeasible reasoning known as Logic Programming with Defaults and Argumen-\ntation Theory (LPDA).\narXiv:1905.04422v1  [cs.AI]  11 May 2019\nContents\n1\nIntroduction\n1\n2\nAttempto Controlled English\n3\n2.1\nVocabulary\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.2\nConstruction Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.3\nInterpretation Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.4\nEvaluation of ACE\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.5\nSemantic Interpretations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.6\nAttempto Parsing Engine . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.7\nRACE Reasoner . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n3\nProcessable English\n10\n3.1\nBasics of PENG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n3.2\nChart Parsing in PENG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n3.3\nReasoning in PENG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n3.4\nNonmonotonic Extensions of PENG . . . . . . . . . . . . . . . . . . . . . . .\n14\n3.4.1\nDefaults and Exceptions Use Case . . . . . . . . . . . . . . . . . . . .\n15\n3.4.2\nThe Marathon Puzzle Use Case\n. . . . . . . . . . . . . . . . . . . . .\n17\n3.4.3\nThe Jobs Puzzle Use Case . . . . . . . . . . . . . . . . . . . . . . . .\n19\n4\nComputer-Processable Language\n20\n4.1\nLanguage Properties\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n4.2\nSemantic Interpretations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n4.3\nReasoning Services . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n5\nNonmonotonicity in Controlled Natural Languages\n25\n5.1\nDefaults and Exceptions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n5.2\nConversational Implicatures\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n6\nConclusion\n32\nii\nReferences\n34\nA Appendix\n37\nA.1\nCircumscription . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\nA.2\nAnswer Set Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\nA.3\nLogic Programming with Defaults and Argumentation Theories\n. . . . . . . .\n39\nB\nAppendix\n40\nB.1\nThe Marathon Puzzle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\nB.2\nThe Jobs Puzzle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\niii\n1\nIntroduction\nControlled natural languages (CNLs) are effective languages for knowledge representation and\nreasoning. According to Kuhn, “A controlled natural language is a constructed language that is\nbased on a certain natural language, being more restrictive concerning lexicon, syntax, and/or\nsemantics while preserving most of its natural properties” [21]. Unlike the languages that\ndevelop naturally, constructed languages are the languages whose lexicon and syntax are de-\nsigned with intent. A CNL is constructed on the basis of an existing natural language, such\nas English, French, or German. Words in the lexicon of a CNL mainly come from its base\nlanguage. Some CNLs may include special symbols in their lexicon. For example, in Common\nLogic Controlled English (CLCE) the parentheses are used for deeply nested sentences and for\nlists of more than two elements [48]. Words may or may not be used in the same manner as\nin the base language. Some words are used with fewer senses or reserved as key-words for\nspeciﬁc purposes. In CLCE, the word “every“ is treated as a universal quantiﬁer. It does not\nallow any universally quantiﬁed noun phrases to be used as the object of a preposition. CNLs\nhave a well-deﬁned syntax to form phrases, sentences and texts. The syntax of a CNL is gener-\nally simpler than that of the source language. Sentences are interpreted in a deterministic way.\nCNLs are more accurate than natural languages, because the language is more restrictive, but\nnot all CNLs have formal semantics. Those that have formal semantics can be processed by\ncomputers for knowledge representation, machine translation, and logical reasoning. Although\na CNL may deviate from its base language in the lexicon, syntax, and/or semantics, it still\npreserves most of the natural properties of the base language, so the reader would correctly\ncomprehend the CNL with little effort.\nCNLs generally fall into two categories: human-oriented CNLs and machine-oriented\nCNLs [42]. Human-oriented CNLs are designed to make the texts easier for readers to under-\nstand. They are applied in technical writings and inter-person communications. Basic English\nwas the ﬁrst English-based controlled natural language created by Charles Ogden in 1930 [34].\nIt has a tremendous impact on the development of controlled natural languages. Basic English\nhas 850 core root words which are composed of 600 nouns, 150 adjectives and 100 functional\nwords that put the nouns and adjectives into operation. These 850 words can do all the work\nthat 20,000 English words do in daily life. Root words can be extended to form plurals, neg-\native adjectives, etc.. For example, plurals are formed by appending an “S“ to the end of a\nroot word. An adjective is given a negative meaning with the preﬁx, “UN-“. Basic English\nsubstitutes verbs in English with operators. The operator is one of the 18 verbs: put, take, give,\nget, come, go, make, keep, let, do, be, seem, have, may, will, say, see, and send. A verb can be\ndescribed by an operator combined with a preposition or a noun. For instance, Basic English\nuses put in to represent the verb, inject, in English. There are 10 rules of grammar that deﬁne\nthe order of words in a sentence, how root words can be extended to form plurals, adjectives,\nadverbs, negative adjectives, and compound words, the composition of questions, and so on.\nIn addition to the 850 core root words, Basic English also deﬁnes a list of international words.\nThe international words are intended for scientiﬁc and technical writing. Basic English is used\nin business, science, economics, and politics. The beneﬁts of Basic English are two-fold. First,\nit can serve as an international auxiliary language and work as an aid for teaching English as\na second language. Second, it provides a simple introduction to English for foreign language\n1\nspeakers.\nOther inﬂuential human-oriented CNLs include Special English and Simpliﬁed Technical\nEnglish (STE). Special English was developed by Voice of America in 1959 [33]. The vocab-\nulary is limited to 1580 words. Special English has been used in a multitude of daily news\nprograms in the United States. It also serves as a resource for English learners and has been\nadopted by other countries for news broadcasting. Simpliﬁed Technical English (STE) was\ndeveloped for aerospace industry maintenance manuals [17]. The dictionary consists of ap-\nproved and unapproved words. Unapproved words are not allowed to be used. Instead, the\ndictionary provides alternative words that refer to the same meaning. STE also allows users\nto add approved words to the dictionary. STE, as a subset of English, reduces ambiguities and\nimproves clarity and comprehensibility through restrictions on grammar and style. It also helps\nin computer-assisted and machine translation.\nMachine-oriented CNLs, as opposed to human-oriented ones, have formal semantics which\ncan be understood and processed by computers for the purpose of knowledge representation\nand logical reasoning. Attempto Controlled English (ACE) was the ﬁrst CNL that can be\ntranslated to ﬁrst-order logic [10]. ACE is a subset of English deﬁned by a restricted grammar\nalong with interpretation rules that control the semantic analysis of grammatically correct ACE\nsentences. ACE uses discourse resolution structure (DRS) as the logical structure to represent\nthe semantics of a set of ACE sentences [20]. ACE is supported by a language processor,\nAttempto Parsing Engine (APE), and a reasoner, RACE. APE is an online language processor\nthat allows users to compose ACE sentences as input and generates their semantics in DRS and\nﬁrst-order logic clauses as output. RACE is a CNL reasoner that supports theorem proving,\nconsistency checking, and question answering. ACE has been applied to various areas such as\nsemantic web, bioinformatics and so on.\nOther examples include PENG, CELT and CLP [40, 36, 5]. Both PENG and CELT are in-\nspired by ACE. They are a subset of English with restricted grammar and use DRS as semantic\nrepresentation. Unlike ACE, PENG does not require users to learn the grammar of the lan-\nguage. Instead, it designs a predictive editor that informs users of the look-ahead information\nthat guides users to proceed based on the structure of the current sentence. CELT translates the\ncontrolled language to formal logical statements that use terms in an existing large ontology,\nthe Suggested Upper Merged Ontology (SUMO) [31]. This gives each term multiple deﬁni-\ntions. CPL is a CNL developed by Boeing [5]. It uses Knowledge Machine (KM), an advanced\nframe-based language, to represent the semantics of its language [6]. The translation is through\nthree main steps: parsing, generation of an intermediate logical form (LF), and conversion of\nthe LF to statements in the KM knowledge representation language. The KM statements can\nbe used for reasoning and question-answering.\nThe above four machine-oriented CNLs are designed for general purposes. They can be\napplied in various application domains. Some machine-oriented CNLs are designed for spe-\nciﬁc application domains. For example, SBVR Structured English is intended for describing\nbusiness vocabulary and for representing business rules [3]. Rabbit is a CNL that can be trans-\nlated into OWL and achieves both comprehension by domain experts and computational ex-\nperts [19]. LegalRuleML is used for representing legislative documents in XML-based rules in\norder to conduct legal reasoning [35].\n2\nAlthough a number of machine-oriented CNLs exist, few of them support nonmonotonic\nreasoning. In our work, we propose nonmonotonic extensions of CNL to support defeasible\nreasoning. A logic is nonmonotonic if some conclusions can be defeated by the addition of new\nknowledge. This contrasts with monotonic formalisms, where the addition of new knowledge\nwill not invalidate any previously derived conclusion. Nonmonotonic reasoning represents the\nnatural way of human reasoning in real life. People draw conclusions based on a number\nof unstated default assumptions which are supposed to be true. Some conclusions may be\nretracted when the addition of new knowledge violates one of the default assumptions. For\ninstance, given the following knowledge base,\n1. Typically, birds ﬂy.\n2. Penguins are birds.\n3. Penguins do not ﬂy.\n4. Tweety is a bird.\nthe ﬁrst sentence is in fact interpreted as “in the absence of any information to the contrary,\nwe assume that birds ﬂy.” Hence, it is reasonable to make the conclusion that “Tweety ﬂies.”\nHowever, if later we add the fact that “Tweety is a penguin,” we will withdraw the previous\nconclusion and conclude that “Tweety does not ﬂy.” Classical approaches that deal with de-\nfaults include default logic [38], circumscription [25], and autoepistemic logic [24]. Modern\napproaches include answer set programming (ASP) [24] and logic programming with defaults\nand argumentation theories (LPDA) [51]. In this report, we focus on three logical frame-\nworks: circumscription, ASP and LPDA, where circumscription is used in Wainer’s approach\nto model conversational implicatures, ASP is used in PENG to handle defaults and exceptions,\nand LDPA is used in our approach to model nonmonotonicity in CNL. They will be discussed\nin Section 5.2, 3.4, and 5.1 respectively. Technical details of these three frameworks can be\nfound in the appendix.\nThe following is organized as follows: in Section 2-4, we study three inﬂuential CNL\nsystems, Attempto Controlled English (ACE), Processable English (PENG), and Computer-\nprocessable English (CPL). We compare their language design, semantic interpretations, and\nreasoning services. In Section 5, we ﬁrst identify typical nonmonotonicity in natural languages,\nsuch as defaults, exceptions and conversational implicatures. Then, we propose their represen-\ntation in CNL and the corresponding formalizations in a form of defeasible reasoning known\nas Logic Programming with Defaults and Argumentation Theory (LPDA).\n2\nAttempto Controlled English\nAttempto Controlled English (ACE) will be discussed in this section. Subsection 1-4 introduce\nthe language properties of ACE, including vocabulary, construction rules and interpretation\n3\nrules. Subsection 5 describes Discourse Representation Structure (DRS), the semantic interpre-\ntation of ACE sentences. Subsection 6 shows the Attempto Parsing Engine (APE). Subsection\n7 presents the Attempto reasoning engine, RACE reasoner.\n2.1\nVocabulary\nThe vocabulary consists of function words, such as determiners, articles, pronouns, and quan-\ntiﬁers, some ﬁxed phrases (e.g. “there is a” and “it is not the case that”), and 100,000 content\nwords, including adverbs, adjectives, nouns, verbs, and prepositions. ACE also allows users\nto add new content words to the lexicon. Content words are written as Prolog atoms. The\npredicates describe the part of speech (POS) of the words. Each predicate has at least two\narguments. The ﬁrst argument is a word in ACE lexicon. The second argument speciﬁes the\ncorresponding logical symbol of the ﬁrst argument used in representing the semantics of ACE\nsentences. A predicate may represent some additional information by adding a few more ar-\nguments. Below is an example that describes the Prolog representations of the words “fast,”\n“faster,” and “fastest” respectively,\n1. adv(fast, fast).\n2. adv comp(faster, fast).\n3. adv sup(fastest, fast).\nwhere predicate adv says that “fast” is an adverb, predicate adv comp says that “faster” is a\ncomparative adverb, predicate adv sup says that “fastest” is a superlative adverb. All three\nwords use the same logical symbol, “fast,” in logical representations.\n2.2\nConstruction Rules\nThe construction rules deﬁne the format of words, phrases, sentences, and ACE texts.\nWords Function words and some ﬁxed phrases are not allowed to be modiﬁed by users. Users\ncan create compound words by concatenating two or more content words with hyphens.\nPhrases Phrases include noun phrases, modifying nouns, modifying noun phrases, verb\nphrases, and modifying verb phrases. Noun phrases in ACE are a subset of noun phrases\nin English plus arithmetic expressions, sets, and lists. Modifying nouns and noun phrases\nare those that are proceeded or followed by adjectives, relative clauses, or possessives.\nVerb phrases in ACE form a subset of verb phrases in English with speciﬁc deﬁnitions\nof negations and modalities. Modifying verb phrases are those that are accompanied by\nadverbs or preposition phrases.\n4\nSentences ACE sentences include declarative, interrogative, and imperative sentences.\nDeclarative sentences include simple sentences, there is/are-sentences, boolean for-\nmulae, and composite sentences. Interrogative sentences include yes/no queries, wh-\nqueries, and how much/many-queries which end with a question mark. Imperative sen-\ntences are commands and end with an exclamation mark.\nACE Texts ACE texts are sequences of declarative, interrogative, and imperative sentences.\n2.3\nInterpretation Rules\nACE restricts sentences to be interpreted in only one deterministic way. The interpretation rules\nspecify how a grammatically correct ACE sentence is translated. Below are three examples of\nthe interpretation rules:\n1. Prepositional phrases modify the verb not the noun\n• A customer {enters a card with a code} .\n2. Relative clauses modify the immediately preceding noun\n• A customer enters {a card that has a code} .\n3. Anaphora is resolved using the nearest antecedent noun that agrees in gender and number\n• Brad was born in Seattle. It’s a beautiful place.\nThe ﬁrst sentence can be paraphrased in two ways in English, “A customer uses a code to\nenter a card” and “A customer enters a card. The card has a code.” In ACE, it is only translated\nin the ﬁrst way. In the second sentence, given a relative clause, an ACE sentence only refers\nto the closest noun that precedes it. Hence, the relative clause, “that has a code,” modiﬁes the\nnoun, “card.” In the third sentence, ACE resolves the pronoun, “it,” in the second sentence by\nlooking for the nearest antecedent noun that agrees with gender and number, so ACE associates\n“it” with the word “Seattle.”\n2.4\nEvaluation of ACE\nThere are advantages and disadvantages in the design of ACE. The language has a large vo-\ncabulary and a multitude of construction rules. The number of words in the ACE lexicon is\nmore than half of the words (171,476) that are used in current English according to the second\nedition of the 20-volume Oxford English Dictionary [47]. Its construction rules cover various\nsentence structures plus mathematical expressions and boolean formulae. On one hand, these\nallow users to describe more things and thereby make the language very expressive. On the\nother, these add complexity to the language. It is difﬁcult for casual users to learn the language\n5\nand takes them many trials to get to a grammatically correct sentence. Interpretation rules en-\nsure a deterministic way to interpret an ACE sentence and thus avoid many ambiguities that\nexist in English, however this does not always lead to natural interpretations and often results\nin stilted English. For example, “Brad is an actor. He is handsome.” The pronoun, “He,” in\nthe second sentence refers to the proper name, “Brad,” in the ﬁrst sentence. However, ACE re-\nvolves anaphora by referring “He” back to the noun, “actor.” Users can only avoid this problem\nby rephrasing the sentences in a way that follows the ACE interpretation rules.\n2.5\nSemantic Interpretations\nThe semantics of ACE are represented by Discourse Resolution Structures (DRSs) [20]. A\nDRS is a diagram structure consisting of two parts:\n• a set of discourse referents (discourse variables), called the “universe” of the DRS, which\nwill always be displayed at the top of the diagram [20].\n• a set of DRS-conditions, typically displayed below the universe [20].\nA referent can stand for an object introduced by a noun, a predicate introduced by a verb,\netc.. A condition can be either simple or complex. A simple condition is a logical atom\nfollowed by an index. A complex condition is constructed from other DRSs connected by\noperators such as negation, disjunction, and implication. For each simple condition, the logical\natom is restricted to one of the 8 predicates: object, property, relation, predicate, modiﬁer adv,\nmodiﬁer pp, has part, and query. Their deﬁnitions are given in [11]. The index shows the\nlocation of the input from which the condition is introduced.\nThe two ACE sentences, “A bird eats a little worm. An eagle kills a large snake,” are\nrepresented in one logical unit, as shown in Figure 1. In the ﬁrst condition, the object-predicate\nrepresents the bird-object. It has 6 arguments: A, bird, countable, na, eq, and 1. The ﬁrst\nargument, “A,” is the label for the bird-object. It can be referenced by other conditions if the\nconditions are within the scope of the DRS to which A belongs. The second argument indicates\nthat the word, “bird,” introduces the object-predicate. The third argument identiﬁes the bird-\nobject as a countable object. The fourth argument shows that the bird-object does not come\ntogether with any measurement unit (e.g. kg, cm) in the input. The ﬁfth and the sixth arguments\nspecify the quantity of the bird-object to be one. The index, “1/2,” signiﬁes that the bird-object\nis introduced by the second word of the ﬁrst ACE sentence in the input. The same applies to\nthe worm-, eagle-, and snake-objects. In the third condition, the property-predicate describes\nthe little-property. It has 3 arguments: B, little, and pos. The ﬁrst argument, “B,” refers to the\nworm-object. The second argument shows that the word “little” introduces the little-property.\nThe third argument denotes the degree of the little-property as positive. The fourth condition\nrepresents the eat-predicate. It has 4 arguments: C, eat, A, and B. The ﬁrst argument, “C,” is\nthe label for the eat-predicate. Similar to the bird-object, C can be used by other conditions as\nwell. The second argument indicates that this predicate introduces the word “eat.” The third\nand fourth arguments refer to the bird- and worm-objects.\n6\nFigure 1: DRS for “A bird eats a little worm. An eagle kills a large snake.”\nDRSs can be nested. DRS-conditions are allowed to use the referents from their ancestor\nbut not descendant DRSs. A multi-sentential paragraph is always represented by one DRS\ninstead of a conjunction of individual DRSs. This is because sentences may be cross-referenced\namong each other. Representing a paragraph by a conjunction of individual DRSs cannot\nindicate their connections. A DRS is constructed incrementally by processing each sentence\nin order. Once a sentence gets processed, new referents and conditions are incorporated into\nthe current DRS, which is constructed from all preceding sentences. Anaphora is resolved by\nreferring to a variable from the current DRS.\n2.6\nAttempto Parsing Engine\nAttempto parsing engine (APE) is a language processor that accepts ACE texts as input and\ngenerates paraphrases, DRSs and ﬁrst-order logic clauses. Paraphrases indicate to users how\nAPE comprehends the ACE sentences. Users can rephrase their input to get another para-\nphrases if they do not accept APE’s interpretations. APE generates ﬁrst-order logic clauses\nfrom DRSs using the methods introduced in [20]. The core part of APE is a top-down parser\nthat implements the construction and interpretation rules. The parser uses the Deﬁnite Clause\nGrammar (DCG) enhanced by feature structures [13]. It is written in GULP (Graph Uniﬁcation\nLogic Programming) which extends Prolog by adding the operators “:” and “..,” and a number\nof built-in predicates [8]. The operator “:” binds a feature name to its value. The operator “..”\njoins one feature-value pair to the next. In XSB Prolog, list can do the same thing as “..” [39].\nThere are several problems with APE. First, logical clauses are not represented in the usual\nsense of ﬁrst-order logic. An example is shown in Figure 2. In Attempto, a noun is represented\nby a pre-deﬁned predicate object where the ﬁfth and the sixth ﬁelds denote the quantity in-\nformation. Here, the predicate object(A, bird, countable, na, eq, 1) indicates that there is one\nbird. If the sentence is changed to “Two birds eat a little worm,” then the object predicate for\nbird becomes object(A, bird, countable, na, eq, 2). The way Attempto does simply treats the\nquantiﬁcation information as an adjective modiﬁer to the noun. If there is a query that asks\nfor the quantity information of bird, it will check the ﬁfth and the sixth ﬁelds of the object\npredicate. This is not represented correctly in ﬁrst-order logic. In ﬁrst-order logic, the quantity\ninformation should be represented by existential quantiﬁers. For instance, the sentence, “There\n7\nare two birds,” is translated into ﬁrst-order logic as shown below:\n∃x1∃x2(bird(x1) ∧bird(x2))\nwhere x1 and x2 denote two unique bird entities.\n(a) DRS\n(b) First-order Logic\nFigure 2: The ACE sentence, “A bird eats a little worm.”\nSecond, APE does not recognize certain sentence structures such as “he seeks...” which\ncannot be represented in ﬁrst-order logic. For example, Figure 3 shows the DRS and ﬁrst-order\nlogic clause for the sentence, “John seeks a unicorn.” In fact, “John seeks a unicorn” indicates\nthat there may or may not exist a unicorn, so the unicorn cannot be simply existentially quan-\ntiﬁed as in ﬁrst-order logic. This type of sentences is represented by intentional logic, which is\nan extension of ﬁrst-order logic [9].\n(a) DRS\n(b) First-order Logic\nFigure 3: The ACE sentence, “John seeks a unicorn.”\n2.7\nRACE Reasoner\nRACE is an ACE reasoner that accepts ACE texts as input and supports consistency checking,\ntheorem proving, and question answering [12]. RACE is implemented in Prolog. It is an\nextension of Satchmo, which is a theorem prover based on the model generation paradigm\n8\n[23]. Satchmo works with ﬁrst-order logic clauses of the form Body →Head where Body\nis true or a conjunction of logical atoms, and Head is fail or a disjunction of logical atoms.\nNegation is expressed as an implication to false. Satchmo executes the clauses by forward-\nreasoning and generates a minimal ﬁnite model of clauses. RACE extends Satchmo by giving\na justiﬁcation for every proof, ﬁnding all minimal unsatisﬁable subsets of clauses if the axioms\nare not consistent, etc..\nRACE has certain limitations. First, RACE does not work properly in some cases. An\nexample is shown in Figure 4. There are ﬁve consistent axioms. RACE can prove that there\nis a human based on either the ﬁrst and fourth sentences or the second and third sentences.\nHowever, RACE cannot prove that there are two humans in total. The ﬁfth axiom says that\n“John is not Mary,” but RACE cannot answer the query, “Is John Mary?” from the axioms.\nFigure 4: An example of RACE reasoning\nSecond, RACE does not support defeasible reasoning [32]. For example, given the sen-\ntences, “Birds ﬂy. Penguins are birds. Penguins don’t ﬂy,” RACE shows that these axioms are\ninconsistent. This is because the second and the third axioms, which state that penguins are\nthe exceptions of birds that ﬂy, challenge the ﬁrst axiom. Such exceptions are very common\nin natural languages, especially in law, legislation, policies, but they cause contradiction in\nRACE.\n9\n3\nProcessable English\nIn this section, we introduce Processable English (PENG), a system developed by Rolf Schwit-\nter at Macquarie University who was also one of the Attempto group members [49]. The ﬁrst\nsubsection gives an overview of PENG’s language properties and compares it with Attempto.\nThe second subsection presents the basics of chart parsing and its application to PENG’s lan-\nguage processor. The third subsection discusses the reasoning service in PENG. The fourth\nsubsection describes the way PENG handles defaults and exceptions in CNL and its support\nfor non-monotonic reasoning based on the answer set programming (ASP) paradigm.\n3.1\nBasics of PENG\nThe dictionary of PENG consists of predeﬁned function words (including determiners, cardi-\nnals, connectives, prepositions), 3,000 content words (e.g. nouns, proper nouns, verbs, adjec-\ntives, and adverbs), and illegal words. Illegal words are those that are banned by PENG, such\nas wish, can, could, should, might, and all personal pronouns. Users can expand the lexicon as\nin ACE.\nA simple PENG sentence is constructed by the following grammar, where curly brackets\nindicate the enclosed elements, are optional.\nSentence →Subject + Predicate\nSubject →Determiner {+Pre-nominal Modiﬁer}\n+ Nominal Head {+Post-nominal Modiﬁer} | Nominal Head\nPredicate →{Negation} + Verbal Head + Complement {+Adjunct}\nComplex sentences are built from simple sentences using coordinators (and, or), subordi-\nnators (if, before, after, while), and constructors (for every, there). PENG resolves anaphora\nby referring back to the most recent noun phrase that agrees in gender and number. In this\nway, PENG functions similarly to ACE. In addition, PENG identiﬁes synonyms within sen-\ntences. For example, given the sentence, “The fog hangs over Dreadsbury Mansion. The mist\nis creepy,” PENG recognizes that “mist” is a synonym for “fog,” and the noun phrase “The\nmist” is an anaphora for “The fog” [49].\nLike ACE, PENG interprets sentences in a deterministic way. Once sentences are evaluated,\nthe PENG language processor generates paraphrases for users. If PENG misinterprets a user’s\nintentions, it allows that user to rephrase the submission. As discussed in section 2.5, ACE\nuses DRSs to represent the semantics of the language; PENG also uses DRSs as its semantic\nrepresentations and translates DRSs to ﬁrst-order logic clauses.\nPENG has several key advantages over ACE. For one, PENG has a much smaller lexi-\ncon and simpler grammar. This makes the language very accessible and easy to learn. Most\n10\nimportantly, users do not need to consult the grammar manual when composing sentences. In-\nstead, PENG can inform users of all possible words which can follow their current inputs [46].\nDetails of this feature are discussed next.\n3.2\nChart Parsing in PENG\nThe PENG language processor is based on a top-down incremental chart parser that operates\non DCG grammar enhanced with feature structures. A chart parser is a parser that uses a chart\nto save the information of the substrings that have already been successfully analyzed. It reuses\nthis information later if needed [49]. This eliminates backtracking and avoids re-discovering\nthe same substring over and over again.\nFor example, given the following context-free grammar and the sentence, “Tom walks\nslowly,” a backtracking DCG parser will start from the ﬁrst rule and evaluate the non-terminal\nsymbols, N and V b. After it ﬁnds the ﬁrst rule fails, it will explore the second rule and re-\nevaluate N and V b. A chart parser, on the other hand, avoids this duplication of work by\nstoring the computation results of the ﬁrst rule in a chart and reusing them when exploring the\nsecond rule.\nS →N V b\nS →N V b Adv\nN →Tom\nV b →walks\nAdv →slowly\nThe chart for the above example is shown in Figure 5. It is represented by a directed graph\nconsisting of 4 vertices (v0, v1, v2, v3) and a number of edges labelled by dotted rules. A dotted\nrule is the same as a production rule for the context-free grammar except that it has a dot\ninserted somewhere in its right-hand side. The dot indicates the extent to which the hypothesis\nthat the rule is applicable has been veriﬁed. Edges can be active or inactive. Active edges\nrefer to partially veriﬁed hypotheses; inactive edges signify fully conﬁrmed hypotheses. For\nthe purposes of this explanation, edge 1 will be referred to as e1, edge 2 to e2, etc.. In Figure\n5, e1 is an active edge that denotes a hypothesis that S can be transformed to a substring that\nrepresents a N V b Adv sequence. e8 is also an active edge that represents a similar hypothesis,\nbut in this case the hypothesis has been partially veriﬁed. The parser has conﬁrmed that N\nV b can derive the substring “Tom walks,” but hasn’t analyzed Adv yet. e3 is an inactive edge\nwhich indicates a fully conﬁrmed hypothesis that N can generate “Tom.”\nChart parsing can be conducted either top-down or bottom-up. PENG implements the top-\ndown approach. The parsing process involves combining active edges with inactive edges\nbased on the fundamental rule described in [49]. It uses a data structure, called agenda, to\nmaintain a list of active edges and organize the order in which these edges are executed. In\nPENG’s approach, the agenda behaves like a stack that manages the edges in the last-in-ﬁrst-\nout manner. In this example, the agenda is initialized with e1 and e2; the chart is set up with\n11\ne3, e4, and e5. Each time, the parser pops the ﬁrst edge from the agenda and adds it to the\nchart. This active edge is then combined with applicable inactive edges in the chart to form\nnew edges, which may be active or inactive. The parser keeps the inactive edges in the chart\nand pushes all active edges to the agenda. This process repeats until the parser ﬁnds an inactive\nedge that spans the whole sentence.\nIncremental chart parsing is an extension of the chart parsing framework. It can handle\nmodiﬁcations to previously parsed strings without the need for from scratch re-processing.\nBasically, the parser uses the edge dependency information to determine which edges to re-\ncompute. An edge is dependent on another if it is triggered by the latter. Once an edge is\nchanged, only the edges that are dependent on it are updated. For the previous example which\nis shown in Figure 5, if the user deletes the last word, “slowly,” from the sentence, the parser\nwill ﬁrst remove e5 and v3. Next, the parser will eliminate e9 because this edge is dependent\non e5. All other edges are unaffected. The incremental chart parser supports three edit opera-\ntions: insertion, deletion, and replacement. Users can insert, delete, or replace a word from the\nsentence. Details of the corresponding algorithms are given in [41].\nFigure 5: The chart for the sentence, “Tom walks slowly.”\n3.3\nReasoning in PENG\nThe PENG reasoner accepts grammatically correct sentences as input and generates the output\nin controlled languages as well. It supports consistency checking, informativity checking, and\nquestion/answering. Consistency checking ensures that a set of PENG sentences is semanti-\ncally consistent. As compared to RACE, which is the Attempto reasoner discussed in section\n2.7, PENG can also handle synonyms in sentences. For example, given the input, “Mary lives\nin Seattle. Mary does not reside in Seattle,” RACE will not ﬁnd the inconsistency because\nit treats “live” and “reside” as words having different meanings. PENG, on the other hand,\ncan identify the synonyms information and therefore evaluates the input as inconsistent. Infor-\nmativity checking guarantees that sentences are concise with no redundant information. For\n12\ninstance, given the sentence, “Mary lives in Seattle. Mary resides in Seattle,” the second sen-\ntence violates the informativity constraint because it expresses the same meaning as the ﬁrst\none. For question/answering, the PENG reasoner operates in the same way as RACE.\nThe implementation of the reasoner is based on a theorem-prover Otter and a model builder\nMACE [27, 26]. Otter (Organized Techniques for Theorem-proving and Effective Research)\nis a resolution-based theorem-prover that works on ﬁrst-order logic with equality. It proves a\nﬁrst-order formula Φ valid by assuming ¬Φ and generating an empty clause (a contradiction)\nbased on the inference rules (e.g. binary resolution, hyper-resolution, UR-resolution, binary-\nparamodulation). The PENG reasoner does not require users to specify which inference rules\nto use in the process of theorem proving. Instead, it relies on Otter’s autonomous mode that\ndecides on the inference strategies.\nMACE (Models and Computer Examples) is a model builder that searches for ﬁnite models\nof ﬁrst-order statements. It is based on a SAT solver, which implements the Davis, Putnam,\nLogemann and Loveland (DPLL) algorithm. The DPLL algorithm decides the satisﬁability of\na ﬁrst-order propositional formula in conjunctive normal form (CNF) and builds a model for\nit if there exists a satisfying assignment [30]. Basic operations of DPLL include unit prop-\nagation and pure literal elimination. A unit clause is a clause that contains only one single\nunassigned literal, say l. Once a unit clause is found, unit propagation replaces all occurrences\nof ls to true and ¬ls to false and then simpliﬁes the resulting formula. A positive literal l is\npure if it only appears in one clause of the formula. Pure literal elimination assigns l as true\nand removes the clause that contains it. Initially, the value for each literal in the given input\nis unassigned. DPLL ﬁrst runs unit propagation and pure literal elimination while updating\nthe current truth assignment. After simplifying the formula that results from the ﬁrst step, it\nchooses an undeﬁned literal and assigns a truth value to it. Next, DPLL calls itself recursively\nwith the current propositional formula and truth assignment. The entire procedure repeats un-\ntil all clauses are true under the current truth assignment, which is the satisfying assignment.\nOtherwise, it backtracks to a previous branching point and maps the literal to the opposite truth\nvalue. A ﬁrst-order propositional formula is unsatisﬁable if the DPLL procedure terminates\nwithout ﬁnding a satisfying assignment.\nMACE conducts model building in a ﬁve-step pipeline. In the ﬁrst step MACE translates\nthe given input, a ﬁrst-order formula, to a set of ﬁrst-order clauses using a subroutine of Otter.\nIn the second step, MACE transforms the ﬁrst-order clauses to a set of ﬂattened, relational\nclauses. A ﬂattened relational clause is similar to a ﬁrst-order clause except that it does not\ncontain any constant or function symbols. Essentially, each n-ary function symbol in the ﬁrst-\norder clauses is replaced by an n+1-ary predicate symbol. For example, the function literal\ng(x, y) = z is substituted with a three-place predicate G(x, y, z) where x, y, and z are vari-\nables. A constant symbol, say e, is rewritten to the predicate E(z) where z is a variable. In\nanother example, a positive equality α = β where α and β are nonvariable is replaced by two\nclauses: α ̸= x ∨β = x and α = x ∨β ̸= x where x is a variable. In the third step,\nMACE generates a set of propositional clauses based on the ﬂattened relational clauses and a\ngiven domain size n. Basically, MACE constructs all ground instances of the ﬂattened rela-\ntional clauses over the given domain, and then encodes each atom into a unique integer that\nbecomes a propositional variable. In addition, MACE adds two constraints to each predicate\n13\nintroduced from the previous step. For instance, given the function literal f(x, y) = z and\nits corresponding ﬂattened relational clause F(x, y, z), the ﬁrst condition is that the clause\n¬F(x, y, z1) ∨¬F(x, y, z2) holds for any z1 and z2 where z1 < z2. This guarantees that the\nlast argument is a function of the ﬁrst and second arguments. The second condition is that the\nclause F(x, y, 0) ∨F(x, y, 1) ∨· · · ∨F(x, y, n −1) holds. This ensures that the image of each\nfunction should be within the given domain. In the fourth step, MACE calls the DPLL proce-\ndure to search the model for the propositional clauses. Once a propositional model is found,\nthe ﬁfth step translates it back to the corresponding ﬁrst-order model based on the encodings\nin the second and the third steps.\nThe PENG reasoner runs Otter and MACE in parallel in the process of consistency and\ninformativity checking. Given a theory Ψ, the PENG reasoner uses Otter to detect its inconsis-\ntency while concurrently running MACE to detect its consistency. If Otter succeeds in ﬁnding\na proof for ¬Ψ, then Ψ is inconsistent. If MACE succeeds in constructing a model for Ψ, then\nΨ is consistent. Similarly, given a sentence ψ and its previous context Φ, if Otter succeeds in\nﬁnding a proof for Φ →ψ, then ψ is not informative. If MACE succeeds in constructing a\nmodel for Φ & ¬ψ, then ψ is informative.\n3.4\nNonmonotonic Extensions of PENG\nThe latest version of PENG incorporates typical nonmonotonicity in natural language and uses\nthe answer set programming paradigm [14] to conduct nonmonotonic reasoning. The syntax\nand semantics of answer set programming are described in detail in Appendix A. In this sub-\nsection, we will show some distinguishing features of ASP that can model the nonmonotonicity\nthat occurs in natural language.\nNegation. An ASP program can have two types of negations: negation as failure and strong\nnegation. Negation as failure is used to represent a sentence of the form “there is no evidence\nthat . . .” while a strong negation is used to represent a negation in natural language. For\nexample, given the sentence, “If Mary has an assignment to do and there is no evidence that\nthe library is not open, then Mary will study in the library,” it is translated into the following\nASP rule\nstudy(Mary, library) ←not ¬open(library), has(Mary, assignment)\nConstraints. A constraint is deﬁned as an ASP rule with an empty head. It is used to represent\nthe sentences that denote restrictions in natural language. For instance, given the sentence, “a\nperson cannot be both a male and a female,” it is translated into the following constraint:\n←person(X), male(X), female(X).\nThe rule enforces the answer set to eliminate the atoms person(s1), male(s2), female(s3)\nwhere s1 = s2 and s2 = s3.\nChoice Rules. A choice rule is used to specify the number of head literals that may be included\nin an answer set given that the body is true. For example, given the following sentences,\n14\n1. There are two vertices: v1, v2 and v3.\n2. There are three colors: red, green, and yellow.\n3. Each vertex can be assigned with exactly one color.\nthey are translated into the following ASP rules,\nvertex(v1). vertex(v2). vertex(v3).\ncolor(red). color(green). color(blue).\n1{assign(X, Y ) : color(Y )}1 ←vertex(X)\nwhere the third rule denotes a choice rule. It says that if vertex(X) is in the answer set, then\nfor all Y ’s such that color(Y ) is true the answer set will choose to include one and only one\nhead literal assign(X, Y ).\nDefaults and Exceptions. The general form of a default is represented as\np(X) ←c(X), not ab(d(X)), not ¬p(X).\nwhere d stands for default, ab represents abnormality, not refers to negation as failure, and ¬\ndenotes strong negation. The formula says that X has the property P if 1) X is a class of C, 2)\nX is not abnormal with respect to d, and 3) it cannot be shown that X does not have property\nP. Given the default, if e(X) is a strong exception, then it is represented as\n¬p(X) ←e(X).\nIf e(X) is a weak exception, it is represented as\nab(d(X)) ←not ¬e(X)\nThe formula says X is abnormal with respect to d if there is no evidence that X is not an\nexception.\nNext, we will give three examples from [45, 43, 44] that show how ASP is used to represent\nknowledge with defaults, exceptions, choice rules, and constraints.\n3.4.1\nDefaults and Exceptions Use Case\nPENG extension in [45] deﬁnes a default as a statement that contains words such as generally,\nnormally, or typically. As compared to a strict rule, such as “All birds ﬂy,” a default expresses\na fact that is true in most cases but not always. For example, the statement, “Generally, birds\nﬂy,” indicates that most birds ﬂy with a few exceptions. PENG allows two types of exceptions:\nstrong exception and weak exception. A strong exception refutes the default conclusion and\nderives the opposite one, e.g., “Penguins are birds. They do not ﬂy.” A weak exception makes\nthe default inapplicable without defeating the default conclusion. For instance, given the weak\nexception, “An eagle is a bird. It is injured,” it becomes unknown whether the eagle ﬂies or\n15\nFigure 6: The DRS for the statement, “Generally, birds ﬂy.”\nnot. To represent a default in DRS, PENG introduces a new operator ∼∼>. An example is\ngiven in Figure 6, which shows the DRS for the sentence “Generally, birds ﬂy.”\nNext, we will give an example from [45] that shows how ASP is used to represent the\nsemantics of PENG with defaults and exceptions. The knowledge base contains the following\nCNL sentences:\n1. Sam is a child.\n2. John is the father of Sam and Alice is the mother of Sam.\n3. Every father of a child is a parent of the child.\n4. Every mother of a child is a parent of the child.\n5. Parents of a child normally care about the child.\n6. John does not care about Sam.\n7. Alice is absent.\n8. If there is no evidence that a parent of a child is not absent then the parent abnormally\ncares about the child.\nSentence (1)-(4) denote strict rules, which can be represented in ASP as follows:\nchild(sam)\nfather(john, sam)\nmother(alice, sam)\nparent(X, Y ) ←father(X, Y ), child(Y )\nparent(X, Y ) ←mother(X, Y ), child(Y )\nSentence (5) indicates a default. It is translated into the following rule:\ncare(X, Y ) ←parent(X, Y ), child(Y ), not ab(dcare(X, Y )), not ¬care(X, Y )\nThe formula says that X cares about Y if i) X is a parent of Y , ii) Y is a child, iii) there\nis no evidence that the pair < X, Y > is abnormal with respect to dcare, and iv) there is no\nevidence that X does not care about Y . Sentence (6) is a strong exception to the default. It is\nrepresented as a negative atom, ¬care(john, sam). Sentence (8) denotes that sentence (7) is a\nweak exception to the default. They are represented as the following rules:\n16\nabsent(alice)\nab(dcare(X, Y )) ←parent(X, Y ), child(Y ), not ¬absent(X)\nThe second rule says that the pair < X, Y > is abnormal with respect to dcare if i) X is a parent\nof Y , ii) Y is a child, iii) there is no evidence that X is not absent.\nThe answer set for the above rules is\n{ child(sam), father(john, sam), mother(alice, sam), absent(alice),\n¬care(john, sam), parent(john, sam), parent(alice, sam),\nab(d(care(alice, sam))), ab(d(care(john, sam)))}\nGiven that ¬care(john, sam) is in the answer set, we can conclude that John does not care\nabout Sam. Since neither care(alice, sam) nor ¬care(alice, sam) is in the answer set, it is\nnot known whether Alice cares about Sam or not.\n3.4.2\nThe Marathon Puzzle Use Case\nIn [43], Schwitter rewrites the marathon puzzle in PENG sentences which can be automatically\nand unambiguously translated into an ASP program to solve the problem. The marathon puzzle\nis described in natural language as\nDominique, Ignace, Naren, Olivier, Philippe, and Pascal have arrived as the ﬁrst six at\nthe Paris marathon.\nReconstruct their arrival order from the following information:\n(a) Olivier has not arrived last.\n(b) Dominique, Pascal and Ignace have arrived before Naren and Olivier.\n(c) Dominique who was third last year has improved this year.\n(d) Philippe is among the ﬁrst four.\n(e) Ignace has arrived neither in second nor third position.\n(f) Pascal has beaten Naren by three positions.\n(g) Neither Ignace nor Dominique are on the fourth position.\nThe puzzle is represented in PENG as follows:\n1. Dominique, Ignace, Naren, Olivier, Philippe, and Pascal are runners.\n2. There exist exactly six positions.\n3. Every runner is allocated to exactly one position.\n4. Reject that a runner R1 is allocated to a position and that another runner R2 is allocated\nto the same position and that R1 is not equal to R2.\n5. Reject that Olivier is allocated to the sixth position.\n17\n6. If a runner R1 is allocated to a position P1 and another runner R2 is allocated to a position\nP2 and P1 is smaller than P2 then R1 is before R2.\n7. Reject that Naren is before Dominique, Pascal, and Ignace.\n8. Reject that Olivier is before Dominique, Pascal, and Ignace.\n9. Reject that Dominique is allocated to a position that is greater than or equal to 3.\n10. Reject that Philippe is allocated to a position that is greater than 4.\n11. Reject that Ignace is allocated to the second position.\n12. Reject that Ignace is allocated to the third position.\n13. Reject that Pascal is allocated to a position P1 and that Naren is allocated to a position\nP2 and that P1 is not equal to P2 minus 3.\n14. Reject that Ignace is allocated to the fourth position.\n15. Reject that Dominique is allocated to the fourth position.\nSentence (1) is translated into six ASP facts:\nrunner(dominique), runner(ignace),\nrunner(naren), runner(olivier), runner(philippe), runner(pascal). Sentence (2) is repre-\nsented as an ASP fact with the range notation: position(1 . . . 6). Sentence (3) denotes a choice\nrule with the cardinality constraint:\n1{allocated to(X, Y ) : position(Y )}1 ←runner(X)\nThe rule says that if runner(X) is in the answer set, then for al Y ’s such that position(Y )\nholds the answer set will include exactly one head literal allocated to(X, Y ). Sentence (5)\ndenotes a constraint. It is represented as\n←allocated to(olivier, 6).\nThe constraint forces the atom allocated to(olivier, 6) to be excluded from the answer set.\nOther constraints include sentence (7)-(15). They are represented in the similar way as sentence\n(5) does. Sentence (6) is a conditional sentence. It is represented as the following ASP rule:\nbefore(X1, X2) ←runner(X1), position(Y 1), allocatedto(X1, Y 1),\nrunner(X2), position(Y 2), allocatedto(X2, Y 2), Y 1 < Y 2.\nThe complete ASP program is shown in Appendix B. There is one answer set for the above ASP\nprogram where the position information is {position(ignace, 1), position(dominique, 2),\nposition(pascal, 3), position(philippe, 4), position(olivier, 5), position(naren, 6)}.\n18\n3.4.3\nThe Jobs Puzzle Use Case\nJust as the marathon puzzle, Schwitter also solves the jobs puzzle using ASP [44]. The jobs\npuzzle is described as\n1. There are four people: Roberta, Thelma, Steve, and Pete.\n2. Among them, they hold eight different jobs.\n3. Each holds exactly two jobs.\n4. The jobs are: chef, guard, nurse, telephone operator, police ofﬁcer (gender not implied),\nteacher, actor, and boxer.\n5. The job of nurse is held by a male.\n6. The husband of the chef is the telephone operator.\n7. Roberta is not a boxer.\n8. Pete has no education past the ninth grade.\n9. Roberta, the chef, and the police ofﬁcer went golﬁng together.\nThe corresponding CNL sentences of the jobs puzzle are shown below:\n1. Roberta is a person. Thelma is a person. Steve is a person. Pete is a person.\n2. Roberta is female. Thelma is female.\n3. Steve is male. Pete is male.\n4. Exclude that a person is male and that the person is female.\n5. If there is a job then exactly one person holds the job.\n6. If there is a person then the person holds exactly two jobs.\n7. Chef is a job. Guard is a job. Nurse is a job. Telephone operator is a job. Police ofﬁcer\nis a job. Teacher is a job. Actor is a job. Boxer is a job.\n8. If a person holds a job as nurse then the person is male.\n9. If a person holds a job as actor then the person is male.\n10. If a ﬁrst person holds a job as chef and a second person holds a job as telephone operator\nthen the second person is a husband of the ﬁrst person.\n11. If a ﬁrst person is a husband of a second person then the ﬁrst person is male.\n19\n12. If a ﬁrst person is a husband of a second person then the second person is female.\n13. Exclude that Roberta holds a job as boxer.\n14. Exclude that Pete is educated.\n15. If a person holds a job as nurse then the person is educated.\n16. If a person holds a job as police ofﬁcer then the person is educated.\n17. If a person holds a job as teacher then the person is educated.\n18. Exclude that Roberta holds a job as chef.\n19. Exclude that Roberta holds a job as police ofﬁcer.\n20. Exclude that a person holds a job as chef and that the person holds a job as police ofﬁcer.\nFor the above CNL sentences, sentence (1)-(3) and (7) denote ASP facts. Sentence (5)-(6)\nindicate choice rules. Sentence (4), (13)-(14), (18)-(20) signify constraints. Sentence (8)-(12),\n(15)-(17) represent conditional sentences. The complete ASP program is shown in Appendix\nB. There is one answer set for the above problem. The job information for the answer set is\n{holds(thelma, chef), holds(roberta, guard), holds(steve, nurse), holds(pete, operator),\nholds(steve, police), holds(roberta, teacher), holds(pete, actor), holds(thelma, boxer)}\nwhere predicate holds(X, Y ) indicates that X holds the job Y .\n4\nComputer-Processable Language\nIn this section, we discuss Computer-Processable Language (CPL), which was developed by\nPeter Clark at University of Texas [5]. The ﬁrst subsection introduces CPL’s language prop-\nerties, the second subsection discusses its semantic interpretations, and the third gives an\noverview of CPL’s inference system.\n4.1\nLanguage Properties\nThe vocabulary of CPL is based on a pre-deﬁned Component Library (CLib) ontology, which\nwas developed at UT Austin [2]. Unlike ACE or PENG, CPL does not allow users to extend the\nvocabulary. Instead, it uses WordNet to map the words that are outside of the target ontology\nto the closest concepts within the CLib ontology. In particular, modal words, such as probably\nand mostly, are not in CPL because they cannot be represented in ﬁrst-order logic.\nCPL accepts three types of sentences: ground facts, rules, and questions. Ground facts are\nbasic CPL sentences that have the following forms:\n20\nThere is | are NP\nNP verb [NP] [PP]⋆\nNP is | are passive-verb [by NP] [PP]⋆\nwhere NP denotes a noun phrase, PP refers to a prepositional phrase, [NP] signiﬁes that the\nnoun phrase is optional, and [PP]⋆indicates that there can be zero or more prepositional\nphrases. Rules are of the form:\nIF Sentence [AND Sentence]⋆THEN Sentence [AND Sentence]⋆\nwhere Sentence denotes a basic CPL sentence, and [AND Sentence]⋆denotes zero or multiple\nbasic CPL sentences. CPL accepts ﬁve forms of questions that begin with “What is,” “What\nare,” “How many,” “How much,” or “Is it true.”\nFor ground facts, objects are considered existentially quantiﬁed. To express universally\nquantiﬁed objects, users must use IF . . . THEN . . . rules. Words that indicate universal\nquantiﬁers (e.g. all, every, most) are banned in CPL. Compared to ACE and PENG, which\nuse the words such as “all” and “every” to describe universally quantiﬁed objects, CPL makes\nthe language more redundant and stilted. For example, given the ACE/PENG sentence, “every\nstudent that gets an A is smart,” it is written in CPL as “IF a student gets an A THEN the\nstudent is smart.”\nUnlike ACE or PENG, CPL does not allow pronouns. Users must use either deﬁnite ref-\nerence or ordinal reference to indicate previously mentioned objects. A deﬁnite reference is\nresolved by searching for the most recent, previously mentioned noun. For example, given the\nsentences, ”There is a dog in the park. The dog is cute,” “the dog” in the second sentence refers\nto the dog mentioned in the ﬁrst sentence. An ordinal reference is resolved in a similar way,\nexcept that it counts the number of occurrences of the mentioned noun from the beginning of\na paragraph and chooses the appropriate occurrence as deﬁned by the ordinal. For example,\ngiven the sentences, “Tom has a dog. Mary has a dog. The ﬁrst dog is cute. The second dog is\nsmart,” to resolve “the ﬁrst dog” in the third sentence, the CPL interpreter starts from the ﬁrst\nsentence and ﬁnds the ﬁrst occurrence of dog, Tom’s dog. Similarly, CPL resolves “the second\ndog” in the fourth sentence by searching the second occurrence of dog from the beginning of\nthe paragraph.\n4.2\nSemantic Interpretations\nThe semantics of CPL are represented by KM (Knowledge Machine) sentences. KM is a\npowerful frame-based knowledge representation language [7]. It represents ﬁrst-order logic\nclauses in LISP-like syntax.\nThe CPL interpreter translates a CPL sentence into KM sentences in three steps. First,\nthe interpreter uses a bottom-up, broad coverage chart parser, called SAPIR, to parse a CPL\nsentence and then generates a logical form (LF) [18]. An LF is a simpliﬁed and normalized tree\nstructure with logic-type elements [4]. It uses variables, which are preﬁxed by underscores, to\nrepresent noun phrases. An LF captures the syntactic properties and relations of the sentence,\n21\nincluding tense, aspect, polarity, and a tag set consisting of S (sentence), PP (prepositional\nphrase), NN (noun compound), PN (proper name), PLUR (plural), and PLUR-N (numbered\nplural). For example, the sentence “the cat sat on the reed mat” is shown below in the LF form:\n((VAR X1 “the” “cat”)\n(VAR X2 “the” “mat” (NN “reed” “mat”))\n(S (PAST) X1 “sit” (PP “on” X2)))\nThe ﬁrst line says X1 is a variable that represents a noun phrase, “the cat.” The second line\nsays i) X2 is a variable that denotes the noun phrase, “the reed mat,” ii) “reed mat” is a noun\ncompound where “reed” is the noun modiﬁer of “mat.” The third line says i) the tense of the\nsentence is simple past, ii) X1, which denotes “the cat,” is the subject of the sitting event, iii)\nthe prepositional phrase, “on the reed mat,” is the prepositional modiﬁer of “sit.”\nSecond, an initial logic generator is used to transform the LF into ground logical assertions\n(KM sentences) by applying a set of simple, syntactic rewrite rules. Logical assertions are\nbinary predicates that represent the syntactic relations and the prepositions of the LF generated\nin Step 1, including subject (syntactic subject), sobject (syntactic object), mod (modiﬁer), all\nprepositions, etc.. Other information, such as part of speech, tense, polarity, and aspect, is\nomitted. For the above example, “the cat sat on the reed mat,” its logical assertions are shown\nbelow:\nsubject( Sit1, Cat1)\n“on”( Sit1, Mat1)\nmod( Mat1, Reed1)\nHere, Sit1, Cat1, Mat1, Reed1 refer to the words “sit,” “cat,” “mat,” and “reed” re-\nspectively in the LF. They are Skolem constants that denote the instances of some concepts\n(classes) in the target ontology. These concepts are determined in the third step. Predicates\nsubject and mod denote the syntactic relations of the sentence. Predicate “on” signiﬁes the\npreposition “on” in the LF.\nThird, subsequent post-processing is performed based on the logical assertions generated\nin Step 2, including word sense disambiguation, semantic role labelling, and structural re-\norganization. For word sense disambiguation, each Skolem instance is assigned with a concept\nin the target ontology, based on the word each Skolem instance corresponds to and its part\nof speech information. Essentially, the interpreter selects the most common synset (a group\nof synonymous words) for a word in WordNet and then maps the WordNet synset to the cor-\nresponding concept in CLib ontology. For the above example, four additional sentences are\nadded to the knowledge base:\nisa( Sit1, Sit n1)\nisa( Cat1, Cat n1)\nisa( Mat1, Mat n1)\nisa( Reed1, Reed n1)\n22\nwhere Sit n1, Cat n1, Mat n1, Reed n1 denote the CLib ontology concepts. The predi-\ncate isa is a binary relation. It indicates that an entity is an instance of a class.\nBy using word sense disambiguation, CPL is capable of identifying synonyms. Different\nwords that denote the same concept will be mapped to the same concept in CLib ontology, and\nthus will be co-referenced. This is more advantageous than ACE and PENG. For instance, ACE\ncannot recognize synonyms. Hence, words that represent the same concept will be regarded\nas different if they are distinct. PENG has a list of pre-deﬁned synonyms in its dictionary, but\nits synonym information is much less than that of WordNet, which contains 155,287 words\norganized in 117,659 synsets for a total of 206,941 word-sense pairs [29].\nNext, semantic role labelling (SRL) is performed to replace some syntactic relations with\nsemantically meaningful relations. For the above example, the binary predicates generated in\nStep 2 are replaced by the following:\nagent( Sit1, Cat1)\nlocation( Sit1, Mat1)\nmaterial( Mat1, Reed1)\nwhere agent indicates that an entity initiates, performs or causes an event, location signiﬁes\nthat an event ends at a place, and material shows that an entity is made of another entity.\nFinally, structural re-organization is deployed. For example, given two binary predicates\nsubject( Be1, Rose2) and object( Be1, Red3), structural re-organization will merge them\ninto a single predicate, “be”( Rose2, Red3). Another example is that any equal predicate,\nsay equal( Color1, Red2), will be removed from the logical assertions and the occurrences\nof Color1 will be replaced by Red2 as well.\nSimilar to ACE and PENG, CPL will display the ﬁnal interpretations and the paraphrases of\nthe input to users for judgement. If the users do not accept the interpretation, they can rephrase\nthe original sentence and/or modify the word senses and semantic relations.\n4.3\nReasoning Services\nFor CPL’s reasoning service, users can compose questions in CPL using one of the ﬁve forms\ndescribed in section 4.1. Compared to ACE and PENG, CPL does not support consistency\nchecking, informativity checking, nor does it support theorem proving for a given set of sen-\ntences.\nCPL deploys KM as the underlying inference system. As is mentioned in section 4.2,\nKM is a frame-based knowledge representation language implemented in Common Lisp. The\nbasic representation unit is a frame, which consists of a set of slots and values. Slots denote\nbinary relations between instances. They can represent the properties of a class or an instance.\nKM deﬁnes three types of properties for a class: its own properties, assertional properties, and\ndeﬁnitional properties. A class’s own properties describe the meta-data of the class itself. They\ndo not apply to any member of the class. Assertional properties denote the properties that are\nimplied by the membership of the class. They are formulated as uni-directional implications:\n23\n∀x (C(x) →s1(x, v11), . . . , s1(x, v1k1), . . . , sn(x, vn1), . . . , sn(x, vnkn))\nwhere C denotes a class, si (i = 1, 2, . . . , n) represents a slot indicating one of the class’s\nassertional properties, vij(i = 1, 2, . . . , n, j = 1, 2, . . . , ki) is a value in slot si, and si(x, vij)\nis a binary relation that holds between instances of C and the value of slot si. Deﬁnitional\nproperties are the properties that are both implied by the membership of the class and are suf-\nﬁcient to conclude the class membership of an instance. They are formulated as bi-directional\nimplications:\n∀x (C(x) ↔s1(x, v11), . . . , s1(x, v1k1), . . . , sn(x, vn1), . . . , sn(x, vnkn))\nwhere si (i = 1, 2, . . . , n) represents a slot indicating one of the class’s deﬁnitional properties.\nAn instance can be classiﬁed into class C if it satisﬁes all of C’s deﬁnitional properties. Both\nassertional and deﬁnitional properties apply to every member of the class. Classes can inherit\nthe properties from their superclasses. Once an instance is created, it will inherit the properties\nfrom all of its superclasses.\nKM has a mechanism, called automatic classiﬁcation, which will automatically attempt to\nre-classify an instance into the most speciﬁc class once it is created or modiﬁed. For example,\ngiven the classes Square and Rectangle where Square is deﬁned to be the subclass of Rectangle\nwith the deﬁnitional property of equal length and width, if the user creates an instance of\nRectangle with its length equal to its width, KM will search the inheritance hierarchy and\nre-classify the instance into the Square class. Later, if the user modiﬁes the instance, say\nchanging its length to be smaller than its width, KM will again search the inheritance hierarchy\nand re-classify the instance into the Rectangle class.\nKM performs inference when a query is issued to the knowledge base. A query is of the\nform:\n(the slotn of ( . . . (the slot2 of (the slot1 of expr))))\nwhere sloti (i = 1, 2, . . . , n) represents a slot and expr denotes a KM expression that evaluates\nto one or multiple instances. The semantics of a query are formulated as an access path. It is a\njoin of binary predicates of the form:\nP1(X0, x1), P2(x1, x2), . . . , Pn(xn−1, xn)\nwhere X0 is a constant, and xi (i = 1, 2, . . . , n) is a free variable. It denotes a set S of values\nfor xn such that\n∀xn (xn ∈S ↔∃x1, . . . , xn−1 P1(X0, x1), P2(x1, x2), . . . , Pn(xn−1, xn))\nKM evaluates a query from right to left in n iterations. In iteration 1, KM processes\n(the slot1 of expr) by computing the value of expr and then ﬁnding the value of slot1 for\neach instance returned by expr. The automatic classiﬁcation mechanism is always implicitly\napplied to each instance generated in the whole process. The results are fed as input to iteration\n2. In iteration 2, KM ﬁnds the value of slot2 for each instance in the input. The results are\nagain passed to iteration 3. This process repeats until it ﬁnishes processing slotn.\n24\n5\nNonmonotonicity in Controlled Natural Languages\nAccording to [1], nonmonotonic logics are a family of logics that are designed to repre-\nsent the kind of defeasible inference in everyday life, where reasoners draw a set of conclu-\nsions that are justiﬁed by the given knowledge base. These conclusions may be invalidated\nwith the addition of more knowledge. Examples of nonmonotonic logic frameworks include\ncircumscription[25], default logic [38], autoepistemic logic [28], etc.. In this section, we ﬁrst\nstudy two types of nonmonotonic phenomena that occur in natural languages: defaults and ex-\nceptions and conversational implicatures. Then propose their adapted representations in CNLs\nand the corresponding formalizations in nonmonotonic reasoning frameworks.\n5.1\nDefaults and Exceptions\nA default is a statement that is true by default about a collection of instances but may be\ndefeated by information about some speciﬁc instances. The latter are called exceptions. This\ncontrasts with a deﬁnite statement in the real world where the occurrences of such instances\nwill falsify the validity of the statement. In natural languages, a default can be empirically\ncategorized into two types: one that is directly declared and one that is indirectly speciﬁed. A\ndefault is directly declared if it is a statement that generalizes what an object does and contains\nparticular words (e.g. generally, typically, normally) [45]. For instance, “Normally, birds ﬂy,”\n“If she has an essay to write, typically she will study in the library,” etc. A default is stated\nindirectly if it does not contain any keywords but can be identiﬁed by the context or based\non some commonsense knowledge, e.g., “There is a lot of rain in Seattle.” A default hides a\nnumber of unstated assumptions; in the previous case, e.g., “There is no drought in Seattle,”\n“The climate in Seattle does not change,” etc.. The default is defeated if one of the assumptions\ndoes not hold. Similarly, exceptions can be noted directly with the keyword, such as except, or\nexpressed indirectly. Examples are shown below,\n1. If she has an essay to write, she will study late in the library except on weekends.\n2.a Typically, birds ﬂy.\n2.b Penguins do not ﬂy.\n3.a If Mary has a writing assignment, typically she will study in the library.\n3.b If Mary has a coding assignment, normally she will study in CS lab.\nSentence (1) contains a default statement and its exception information is directly speciﬁed\nby the keyword “except.” The sentence says that normally she will study in the library. But\nif it is a weekend, she will not study in the library. Sentence (2.a) denotes a default, which\ngeneralizes what birds do. For sentence (2.b), although it does not contain any keywords to\nindicate an exception to (2.a), we can still identify the exception based on the context and on\ncommensense knowledge. Sentence (3.a) and (3.b) are ambiguous. The default conclusions\ndrawn from (3.a) and (3.b) are incompatible with each other because Mary cannot study in two\ndifferent locations at the same time. However, there is no information of what is an exception\n25\nto what. Hence, it is impossible to decide where Mary will study when she has both a writing\nand a coding assignment to do.\nWe adapt defaults and exceptions that occur in natural languages to the design of CNLs.\nUnlike natural languages, CNLs have to be precise and unambiguous without losing the natu-\nralness of the language. To the best of our knowledge, PENG is the only CNL that incorporates\ndefaults and supports nonmonotonic reasoning. As described in Section 3.4, PENG deﬁnes a\ndefault as a general statement that contains words, such as normally, generally, and typically.\nThere are two types of exceptions in PENG: strong exception and weak exception. A strong\nexception contradicts the conclusion generated by the default. A weak exception makes the\ndefault inapplicable. Defaults and exceptions are formalized in ASP programs. One problem\nwith PENG is its stilted way to indicate that a statement is a weak exception to the default. An\nexample is shown below:\n4.a Parents of a child normally care about the child.\n4.b Tom is a parent of a child.\n4.c Tom does not care about his child.\n4.d Alice is a parent of a child.\n4.e Alice is absent.\nSentence 4.a is a default, which is formalized as\ncare(X,Y) :- parent(X,Y), child(Y), not ab(d(care(X,Y))), not ¬care(X,Y).\nwhere d stands for default, ab represents abnormal, not signiﬁes negation as failure, and ¬\nrefers to strong negation. Sentence (4.c) denotes a strong exception to (4.a). In order to ensure\nthat sentence 4.e is a weak exception to 4.a, users have to write the cancellation rule in CNL as\n“If there is no evidence that a parent of a child is not absent then the parent abnormally cares\nabout the child.” This makes the language processor translate it into the following ASP rule:\nab(d(care(X,Y))) :- parent(X,Y), child(Y), not ¬absent(X).\nThe way the cancellation rule is expressed in PENG is just a direct translation of the intended\nASP rule, which is rather unintuitive.\nWe propose a different approach to the representations and formalizations of defaults and\nexceptions in CNLs. Each CNL sentence is associated with a unique identiﬁer, which denotes\nthe location of the sentence in the text. The identiﬁer can be an English phrase, e.g. the second\nsentence of the third paragraph, or a user-deﬁned label, such as para3sent2, where para stands\nfor paragraph and sent stands for sentence. We assume that CNL sentences are defeasible\nby default and therefore can be defeated. To denote a deﬁnite statement, users must add the\nkeyword, strict, at the beginning of the sentence, e.g.,\n5. (strict) Obama won the presidential election in 2012.\nException information has to be mentioned directly. A default can have three types of\nexceptions: refutation, rebuttal, and cancellation. The refutation of a default offers a conclusion\nthat is incompatible with the conclusion drawn from the default and will override the default.\n26\nHere, we assume that the “refutation” relation is transitive. That is, if statement A refutes B\nand B refutes C, statement A refutes C as well. The structure of a refutation is a CNL statement\nproceeded by its reference information, which speciﬁes the corresponding sentences the current\nstatement causes an exception to. The reference information is represented by either template\n6.a or 6.b,\n6.a except\n6.b exception to id1, id2, . . . , idn\nwhere idi’s (i = 1, 2, . . . , n) denote sentence identiﬁers. Template 6.a indicates that the current\nsentence is a refutation to its previous one. Template 6.b says that the sentence is a refutation\nto sentences with ids id1, id2, . . . , idn. The rebuttal of a default is also a conclusion that\nis incompatible with the default. However, there is no information on which conclusion has\nmore weight. Hence, no conclusion can be drawn from the default and its rebuttal. To specify\nthe situation where two conclusions are incompatible with each other, we write it as a CNL\nstatement preceded by the keyword, conﬂict constraint, e.g.,\n7.a Tom votes for Obama.\n7.b Tom votes for Romney.\n7.c Obama is a candidate.\n7.d Romney is a candidate.\n7.e (conﬂict constraint) A person can vote for at most one candidate.\nAccording to 7.e, sentence 7.a and 7.b are incompatible and therefore rebut each other. Thus,\nboth 7.a and 7.b are false. The cancellation of a default simply renders the default inapplicable\nand therefore cancels the default conclusion. A cancellation is represented as a conditional of\nthe form:\nIf P, then cancel id1, id2, . . . , idn.\nwhere P denotes the premises of the conditional. Note, all exceptions are defeasible. They can\nbe defeated by other statements as well. In this case, an exception can be used to defeat other\nstatements only when the exception itself is not defeated, e.g.,\n8.a If John gets an offer from BOA, John will join BOA.\n8.b (except) If John gets an offer from Amazon, John will join Amazon.\n8.c John gets an offer from BOA.\n8.d John gets an offer from Amazon.\n8.e Amazon goes bankcruptcy.\n8.f If Amazon goes bankcruptcy, then cancel 8.b.\nSentence 8.b causes an exception to 8.a. However, based on 8.e and 8.f, sentence 8.b is de-\nfeated. In this case, sentence 8.b cannot be used to refute 8.a. Hence, we can conclude that\n“John will join BOA.”\nWe formalize CNL sentences using Logic Programming with Defaults and Argumentation\nTheories (LPDA) [51], which is a powerful framework for defeasible reasoning based on well-\nfounded semantics [37]. LPDA has well-deﬁned semantics for defaults and exceptions. There\nare two types of rules: strict rules and defeasible rules. Each LPDA program is accompanied\n27\nwith an argumentation theory, which decides when a defeasible rule is defeated. There are\nthree special predicates: opposes, overrides, and cancel where opposes indicates the literals\nthat are incompatible with each other, overrides denotes a binary relation between defeasible\nrules indicating priority, and cancel cancels a defeasible rule. We use predicate overrides\nand cancel to formalize the “refutation” and “cancellation” relations between a default and its\nexceptions respectively. We use predicate opposes to formalize the incompatibilities between\na defaults and rebuttals.\nNext, we give a complicated example that includes three types of exceptions and show how\nthey are formalized in LPDA to perform defeasible reasoning.\n9.a John is a store member.\n9.b John is an SBU employee.\n9.c John buys a can of coke.\n9.d John buys a lobster.\n9.e Mary is a store member.\n9.f Mary is an SBU employee.\n9.g Mary buys salmon.\n9.h Mary is in the blacklist.\n9.i A Coke is a beverage.\n9.j Lobster is seafood.\n9.k Salmon is seafood.\n9.l If a customer buys a beverage, the customer gets a discount of $1.50.\n9.m (except) If a customer is a store member and buys a beverage, the customer gets a dis-\ncount of $2.50.\n9.n If a customer is a store member and buys seafood, the customer gets a discount of $7.50.\n9.o If a customer is an SBU employee and buys seafood, the customer gets a discount of\n$5.00.\n9.p If a store member is in the blacklist, then cancel 9.m, 9.n.\n9.q (conﬂict constraint) A customer gets at most one discount for any product.\nThe CNL sentences are formalized in LPDA as follows:\nmember(John).\nsbuemployee(John).\nbuy(John,coke).\nbuy(John,lobster).\nmember(Mary).\nsbuemployee(Mary).\nbuy(Mary,salmon).\nblacklist(Mary).\nbeverage(coke).\nseafood(lobster).\nseafood(salmon).\n@{r1} discount(?Customer,?Product,?Amount):-\nbuy(?Customer,?Product),beverage(?Product),?Amount is 1.50.\n@{r2} discount(?Customer,?Product,?Amount):-\n28\nbuy(?Customer,?Product),beverage(?Product),\nmember(?Customer),?Amount is 2.50.\n@{r3} discount(?Customer,?Product,?Amount):-\nbuy(?Customer,?Product),seafood(?Product),\nmember(?Customer),?Amount is 7.50.\n@{r4} discount(?Customer,?Product,?Amount):-\nbuy(?Customer,?Product),seafood(?Product),\nsubemployee(?Customer),?Amount is 5.00.\ncancel(r2):-member(?Customer),blacklist(?Customer).\ncancel(r3):-member(?Customer),blacklist(?Customer).\nopposes(discount(?Customer,?Product,?Amount1),\ndiscount(?Customer,?Product,?Amount2)):-\nbuy(?Customer,?Product),?Amount1 != ?Amount2.\noverrides(r2,r1).\nFor the above example, if the user asks “How much discount does John get for buying a coke,”\nit will answer $2.50. Sentence 9.l and 9.m give rise to the conclusions that John gets a discount\nof $2.50 and $1.50 respectively for buying a coke. According to 9.q, they are incompatible\nbecause John cannot get two discounts for buying one product. However, since sentence 9.m\nrefutes 9.l, the conclusion drawn from 9.m will override the one made from 9.l. Thus, we have\nthe conclusion that John gets a discount of $2.50 for buying a coke. Next, if the user asks\n“How much discount does John get for buying a lobster,” it will return no answer. Just like the\nprevious case, sentence 9.n and 9.o generate incompatible conclusions: John gets a discount of\n$7.50 and $5.00 for buying a lobster. Without any refutation information between 9.n and 9.o,\nthe conclusion that John gets a discount for buying a lobster cannot be justiﬁed. Finally, if the\nuser asks “How much discount does Mary get for buying salmon,” it will answer $5.00. Even\nthough 9.n and 9.o generate incompatible conclusions for Mary’s purchase, since Mary is in\nthe blacklist, sentence 9.n will be defeated by 9.p. In this case, sentence 9.n cannot be used to\nrebut 9.o. Thus, we have the conclusion that Mary gets a discount of $5.00 for buying salmon.\n5.2\nConversational Implicatures\nThe concept of conversational implicatures was proposed and systematically studied by H. P.\nGrice [16]. Grice stated that conversational implicatures are a component of the meaning of the\nutterances that are drawn not only from the literal meaning of the given linguistic expressions,\nbut also based on some general principles of conversational rationality. These principles are\nalso known as Grice’s maxims:\na Quantity\n(a) Make your contribution as informative as is required.\n(b) Do not make your contribution more informative than is required.\nb Quality\n29\n(a) Do not say what you believe to be false.\n(b) Do not say that for which you lack adequate evidence.\nc Relation\n(a) Be relevant.\nd Manner\n(a) Avoid obscurity of expression.\n(b) Avoid ambiguity.\n(c) Be brief.\n(d) Be orderly.\nSpeakers are presumed to observe and obey the above maxims in their communications. Based\non this assumption, when a speaker appears to break one of the maxims, it will cause the\ninterpreter to make some inferences regarding what the speaker really means. For instance,\n10. The ﬁnal exam will take place in the CS building or the main library.\n11. Speaker A: Students said the exam covered all chapters in the book and was very difﬁcult.\nSpeaker B: It covered all chapters in the book.\nSentence 10 says that the speaker thinks both places are possible. Hence, according to the\nmaxim of quantity, we can conclude that the speaker does not know exactly where the exam\ntakes place. In sentence 11, speaker A said that the exam not only covered all chapters but\nalso was very difﬁcult. However, speaker B only mentioned that the exam covered all chapters.\nAgain, according to the maxim of quantity, we can conclude that the exam covered all chapters\nbut was not necessarily difﬁcult. Conversational implicatures are defeasible in nature. This\ncontrasts with the semantics of entailment in classical logic. For instance, if speaker B adds\nthat “in fact, the exam was also very difﬁcult,” it will cancel the implicature that “the exam was\nnot difﬁcult.”\nGrice divides conversational implicatures into two classes: generalized implicatures and\nparticularized implicatures. Generalized implicatures do not strongly depend on the context.\nThey arise only when particular lexical items are used in the sentence. Particularized impli-\ncatures, on the other hand, are based on speciﬁc context and arise only when such particular\ncontext occurs. In the previous example, sentence 10 denotes a generalized implicature, which\nis triggered by the connective or. Sentence 11 denotes a particularized implicature where the\nimplicature generated from B’s utterance arises from the context A provides. Generalized im-\nplicatures can be further divided into many sub-classes, e.g., scalar implicatures and clausal\nimplicatures. For the rest of this subsection, we will focus on modelling scalar implicatures\nusing nonmonotonic logics.\n30\nScalar implicatures are a class of generalized implicatures derived from the maxim of quan-\ntity. They are based on implicature scales, each denoting a set of lexical items ordered by their\ninformativeness, e.g., <a few, some, many, all>. Given a sentence that uses one of the lexical\nitems in the implicature scale, its corresponding weaker (and stronger) sentences are deﬁned as\nthose where such lexical items are substituted for the weaker (and stronger) ones in the impli-\ncature scale. The theory of scalar implicatures says that i) all weaker sentences are entailed by\nthe given sentence, and ii) all stronger sentence are false by default. For instance, based on the\nimplicature scale, <a few, some, many, all>, the sentence, “Many students passed the exam,”\nentails that i) “Some students passed the exam,” and ii) “A few students passed the exam.”\nBesides, it also implies that “Not all students passed the exam.”\nOne of the previous works that use nonmonotonic logics to model scalar implicatures is\nWainer’s compositional approach, which is based on circumscription [50]. Wainer deﬁnes the\nextended meaning of a sentence as a semantic content of the sentence itself combined with (one\nof) its generalized implicatures. The general form of the extended meaning is represented as\nα ∧(¬abn(c1) →ˆβ)\nHere, α and ˆβ denote the semantic content of the sentence and (one of) its generalized im-\nplicatures respectively. The predicate abn is the abnormality predicate, which is an ad hoc\nsymbol that does not appear in α or ˆβ. Similarly, the constant c1 is also an ad hoc symbol\nthat is only used in one speciﬁc formula. The extended meaning of the sentence is obtained by\ncircumscribing predicate abn, that is, by minimizing the extension of predicate abn. Details of\ncircumscription can be found in Appendix A.\nAs before, we model scalar implicatures using LPDA. For the implicature scales related to\nquantiﬁers, e.g., <a few, some, many, all>, our current approach only supports one instance,\n<some, all>, where some and all correspond to the existential and universal quantiﬁers respec-\ntively in ﬁrst-order logic. For example, given the sentence, “Some students pass exam1,” its\nextended meaning is “Some students pass exam1. There exists at least one student who does\nnot pass exam1.” The extended meaning is formalized in LPDA as shown below:\nstudent(#1).\npass(#1,e1).\nstudent(#2).\n@r1 neg pass(#2,e1).\nwhere rule r1 indicates a defeasible rule and #1 and #2 are Skolem constants. The ﬁrst two rules\nrepresent the semantic content of the original sentence. The third and fourth rules represent the\nsemantic content of the scalar implicature. If later, the speaker adds that “in fact, all students\npass the exam,” the following rule will be added,\npass(?x,e1):-student(?x).\nThis will refute the default conclusion that “there is a student who did not pass exam1.”\nWe can also model the class of implicature scales related to predicates, e.g., <cute, beauti-\nful, stupendous>. For example, given the sentence, “Mary is beautiful,” its extended meaning\nis “Mary is beautiful. Mary is not stupendous.” The semantics of the implicature scale together\n31\nwith the extended meaning of the sentence is formalized as\ncute(?x):-beautiful(?x).\nbeautiful(?x):-stupendous(?x).\nbeautiful(Mary).\n@r2 neg stupendous(Mary).\nThe above rules entail that Mary is cute. If later, the speaker says that “in fact, Mary is also\nstupendous,” the strong fact, stupendous(Mary), will be added to the knowledge base and refute\nrule r2.\nIn addition to the implicature scales related to quantiﬁers and predicates, Wainer also men-\ntions the implicature scale, <or, all>. For example, the sentence, “Mary or Tom passed the\nexam,” indicates that one of Mary or Tom passes the exam, but the speaker does not know\nexactly whom. The sentence also implies that “Mary and Tom pass the exam” is false by de-\nfault. For our current approach, we do not support this kind of implicature scale. Instead, we\npropose to simulate the logical operations inclusive or and exclusive or in CNL. We use the\nconnective or, to denote the inclusive or. For example, that sentence, “John votes for Obama\nor Romney.” indicates that John votes at least for one of them. The sentence is formalized in\nLPDA as shown below:\nvote(John,Romney):-neg vote(John, Obama).\nvote(John,Obama):-neg vote(John, Romney).\nThe above rules guarantee that if there is a fact that John does not vote Obama (or Romney),\nthen the conclusion that John votes Romney (or Obama) can be derived. Besides, these rules are\nconsistent also when there are the facts that Tom votes for both Obama and Romney. However,\nif Tom votes neither for Obama nor Romney, it will cause an inconsistency because it violates\nthe constraint that John votes for at least of the candidates. We can use the connective either\n. . . or to denote the exclusive or. For example, the sentence, “John votes either for Obama or\nfor Romney,” is formalized as\nneg vote(John,Romney):-vote(John, Obama).\nneg vote(John,Obama):-vote(John, Romney).\nIf there is a fact that John votes Obama (or Romney), the above rules ensure that the conclusion\nthat John does not vote for Romney (or Obama) can be derived. If John votes for both Obama\nand Romney, it will cause an inconsistency because it violates the constraint that John can vote\nfor only one of the candidates.\n6\nConclusion\nIn this report, we study controlled natural languages and propose their extensions to support\nnonmonotonic reasoning. First, we give an overview of CNLs in terms of language design,\nclassiﬁcations and their development.\nThen, we introduce three CNL systems: Attempto\nControlled English (ACE), Processable English (PENG), and Computer-processable English\n32\n(CPL). We identify their shared traits and distinguishing characteristics in language design,\nsemantic interpretations, and reasoning services. Finally, we propose our extensions of CNL\nto support defeasible reasoning. We present the representation of defaults, exceptions and con-\nversational implicatures in CNL and their formalizations in Logic Programming with Defaults\nand Argumentation Theory.\n33\nReferences\n[1] G. Aldo Antonelli.\nNon-monotonic logic.\nIn Edward N. Zalta, editor, The Stanford\nEncyclopedia of Philosophy. Winter 2012 edition, 2012.\n[2] Ken Barker, Bruce Porter, and Peter Clark. A library of generic concepts for compos-\ning knowledge bases. In Proceedings of the 1st international conference on Knowledge\ncapture, pages 14–21. ACM, 2001.\n[3] Donald Chapin, DE Baisley, and H Hall. Semantics of business vocabulary & business\nrules (SBVR). In Rule Languages for Interoperability, 2005.\n[4] Peter Clark and Phil Harrison. Boeing’s nlp system and the challenges of semantic repre-\nsentation. In Proceedings of the 2008 Conference on Semantics in Text Processing, pages\n263–276. Association for Computational Linguistics, 2008.\n[5] Peter Clark, Philip Harrison, Thomas Jenkins, John A Thompson, Richard H Wojcik,\net al. Acquiring and using world knowledge using a restricted subset of english. In\nFLAIRS Conference, pages 506–511, 2005.\n[6] Peter Clark and Bruce Porter. KM - the knowledge machine 1.4: Users manual (revision\n1), 1999.\n[7] Peter Clark, Bruce Porter, and Boeing Phantom Works. KM - the knowledge machine 2.0:\nUsers manual. Department of Computer Science, University of Texas at Austin, 2004.\n[8] Michael A Covington. GULP 3.1: An extension of Prolog for uniﬁcation-based grammar.\nCiteseer, 1994.\n[9] Flip G Droste and John E Joseph. Linguistic Theory and Grammatical Description: Nine\nCurrent Approaches, volume 75. John Benjamins Publishing, 1991.\n[10] Norbert E Fuchs, Kaarel Kaljurand, and Tobias Kuhn. Attempto Controlled English for\nknowledge representation. In Reasoning Web, pages 104–124. Springer, 2008.\n[11] Norbert E. Fuchs, Kaarel Kaljurand, and Tobias Kuhn. Discourse Representation Struc-\ntures for ACE 6.5. Technical Report iﬁ-2009.04, Department of Informatics, University\nof Zurich, Zurich, Switzerland, 2009.\n[12] Norbert E Fuchs and Uta Schwertel. Reasoning in Attempto Controlled English. Springer,\n2003.\n[13] Norbert E Fuchs and Rolf Schwitter. Specifying logic programs in controlled natural\nlanguage. arXiv preprint cmp-lg/9507009, 1995.\n[14] Michael Gelfond and Yulia Kahl. Knowledge representation, reasoning, and the design\nof intelligent agents. Michael Gelfond, 2012.\n34\n[15] Michael Gelfond and Vladimir Lifschitz. The stable model semantics for logic program-\nming. In ICLP/SLP, volume 88, pages 1070–1080, 1988.\n[16] H Paul Grice. 4. logic and conversation. The Semantics-Pragmatics Boundary in Philos-\nophy, page 47, 2013.\n[17] ASD Simpliﬁed Technical English Maintenance Group. ASD-STE 100: Simpliﬁed Tech-\nnical English : International Speciﬁcation for the Preparation of Maintenance Docu-\nmentation in a Controlled Language. Aerospace and Defence Industries Association of\nEurope, 2007.\n[18] Philip Harrison and Michael Maxwell. A new implementation of GPSG. In Proc. 6th\nCanadian Conf on AI, pages 78–83, 1986.\n[19] Glen Hart, Martina Johnson, and Catherine Dolbear. Rabbit: Developing a controlled nat-\nural language for authoring ontologies. In The Semantic Web: research and applications,\npages 348–360. Springer, 2008.\n[20] Hans Kamp and Uwe Reyle. From discourse to logic: Introduction to modeltheoretic\nsemantics of natural language, formal logic and discourse representation theory. Num-\nber 42. Springer, 1993.\n[21] Tobias Kuhn. A survey and classiﬁcation of controlled natural languages. 2013.\n[22] Vladimir Lifschitz. Circumscription. 1996.\n[23] Rainer Manthey and Franc¸ois Bry. Satchmo: a theorem prover implemented in Prolog. In\n9th International Conference on Automated Deduction, pages 415–434. Springer, 1988.\n[24] Wiktor Marek and Mirosław Truszczy´nski. Autoepistemic logic. Journal of the ACM\n(JACM), 38(3):587–618, 1991.\n[25] John McCarthy. Circumscriptiona form of non-monotonic reasoning. Artiﬁcial intelli-\ngence, 13(1):27–39, 1980.\n[26] William McCune. Mace 2.0 reference manual and guide. arXiv preprint cs/0106042,\n2001.\n[27] William W McCune. Otter 3.0 reference manual and guide, volume 9700. Argonne\nNational Laboratory Argonne, IL, 1994.\n[28] Drew McDermott. Nonmonotonic logic ii: Nonmonotonic modal theories. Journal of the\nACM (JACM), 29(1):33–57, 1982.\n[29] George A Miller. Wordnet: a lexical database for English. Communications of the ACM,\n38(11):39–41, 1995.\n[30] Robert Nieuwenhuis, Albert Oliveras, and Cesare Tinelli. Abstract DPLL and abstract\nDPLL modulo theories. In Logic for Programming, Artiﬁcial Intelligence, and Reasoning,\npages 36–50. Springer, 2005.\n35\n[31] Ian Niles and Adam Pease. Origins of the IEEE standard upper ontology. In Work-\ning Notes of the IJCAI-2001 Workshop on the IEEE Standard Upper Ontology. Citeseer,\n2001.\n[32] Donald Nute.\nDefeasible logic, handbook of logic in artiﬁcial intelligence and logic\nprogramming (vol. 3): nonmonotonic reasoning and uncertain reasoning, 1994.\n[33] Voice of America (Organization). VOA Special English Word Book: A List of Words Used\nin Special English Programs on Radio, Television, and the Internet. Voice of America,\n2007.\n[34] Charles Kay Ogden. Basic English: A general introduction with rules and grammar,\nvolume 29. Kegan Paul, Trench, Trubner, 1944.\n[35] Monica Palmirani, Guido Governatori, Antonino Rotolo, Said Tabet, Harold Boley, and\nAdrian Paschke. Legalruleml: Xml-based rules and norms. In Rule-Based Modeling and\nComputing on the Semantic Web, pages 298–312. Springer, 2011.\n[36] Adam Pease and John Li. Controlled English to logic translation. In Theory and Appli-\ncations of Ontology: Computer Applications, pages 245–258. Springer, 2010.\n[37] Teodor C Przymusinski. Well-founded and stationary models of logic programs. Annals\nof Mathematics and Artiﬁcial Intelligence, 12(3-4):141–187, 1994.\n[38] Raymond Reiter. A logic for default reasoning. Artiﬁcial intelligence, 13(1):81–132,\n1980.\n[39] Konstantinos Sagonas, Terrance Swift, and David S. Warren. XSB as an efﬁcient deduc-\ntive database engine. In In Proceedings of the ACM SIGMOD International Conference\non the Management of Data, pages 442–453. ACM Press, 1994.\n[40] Rolf Schwitter. English as a formal speciﬁcation language. In Database and Expert Sys-\ntems Applications, 2002. Proceedings. 13th International Workshop on Natural Language\nand Information Systems, pages 228–232. IEEE, 2002.\n[41] Rolf Schwitter. Incremental chart parsing with predictive hints. In Proceedings of the\nAustralasian Language Technology Workshop, pages 1–8, 2003.\n[42] Rolf Schwitter. Controlled natural languages for knowledge representation. In Proceed-\nings of the 23rd International Conference on Computational Linguistics: Posters, pages\n1113–1121. Association for Computational Linguistics, 2010.\n[43] Rolf Schwitter. Answer set programming via controlled natural language processing. In\nControlled Natural Language, pages 26–43. Springer, 2012.\n[44] Rolf Schwitter. The jobs puzzle: Taking on the challenge via controlled natural language\nprocessing. Theory and Practice of Logic Programming, 13(4-5):487–501, 2013.\n[45] Rolf Schwitter. Working with defaults in a controlled natural language. In Australasian\nLanguage Technology Association Workshop 2013, page 106, 2013.\n36\n[46] Rolf Schwitter, Anna Ljungberg, and David Hood. Ecole–a look-ahead editor for a con-\ntrolled language. EAMT-CLAW03, pages 141–150, 2003.\n[47] John Andrew Simpson, Edmund SC Weiner, et al. The Oxford English dictionary, vol-\nume 2. Clarendon Press Oxford, 1989.\n[48] John F Sowa. Common logic controlled English (draft), 2004.\n[49] Kerry Trentelman. Processable English: The theory behind the PENG system. 2009.\n[50] Jacques Wainer. Modeling generalized implicatures using non-monotonic logics. Journal\nof logic, language and information, 16(2):195–216, 2007.\n[51] Hui Wan, Benjamin Grosof, Michael Kifer, Paul Fodor, and Senlin Liang. Logic program-\nming with defaults and argumentation theories. In Logic Programming, pages 432–448.\nSpringer, 2009.\nA\nAppendix\nIn this section, we introduce three nonmonotonic reasoning frameworks: circumscription, an-\nswer set programming and logic programming with defaults and argumentation theories.\nA.1\nCircumscription\nCircumscription was ﬁrst proposed by McCarthy [25] and later enhanced by Lifschitz [22] with\nsome extensions to the original approach. Circumscription has the same syntax as classical\nlogic but with different semantics. Unlike classical logic, circumscription only considers the\npreferred models that have the minimal extensions for certain predicates. For instance, the\nsentence, “Eagles are birds. Normally, birds ﬂy,” is represented as\nbird(eagle) ∧(bird(X) ∧¬ab(X) →fly(X))\nwhere ab denotes abnormality. Intuitively, circumscription attempts to minimize the set of\nobjects that are considered to be abnormal and therefore deems an object to be abnormal only if\nis necessary. For the above formula, since there is no information on the abnormality of eagles,\nwe can assume that eagles are not abnormal and therefore get a conclusion that eagles ﬂy.\nFormally, let A(P) be a ﬁrst-order logic sentence containing predicate P, the circumscription\nof P in A(P), denoted as CIRC[A(P); P], is represented as the second-order formula A∗(P):\nA(P) ∧¬∃p[A(p) ∧p < P]\nwhere p is a predicate variable that has the same arity as P and p < P indicates that the\nextension of p is a strict subset of P. The formula says that there does not exist any predicate\np such that i) p satisﬁes all of the conditions of P in A(P) and ii) p has a smaller extension\nthan P. For the semantics of circumscription, model M1 is a submodel of M2 in P, denoted\n37\nas M1 ≤P M2, if i) M1 and M2 have the same domain, ii) the extension of P in M1 is a\nsubset of that in M2, iii) all other predicate and function symbols that occur in A have the\nsame extensions in M1 and M2. Thus, model M is the minimal model of A(P) with respect\nto P if and only if for any model M ′ of A(P) such that M ′ ≤P M, it implies that M ′ = M.\nCircumscription leads to a nonmonotonic inference relation: A(P)|∼ϕ iff A∗(P) |= ϕ. That\nis, formula ϕ can be inferred from A(P) if and only if ϕ is entailed by A∗(P). Extensions of\ncircumscription include parallel circumscription, prioritized circumscription, etc. Details can\nbe found in [22].\nA.2\nAnswer Set Programming\nAnswer set programming is based on stable model semantics [15]. In normal programs, a rule\nis of the form\nc ←a1, a2, . . . , am, not b1, not b2, . . . , not bn\nwhere ai (1 ≤i ≤m), bj (1 ≤j ≤n), and c are atoms. Given an interpretation M of a\nnormal program Π, the reduct of the program with respect to M, denoted as ΠM, is obtained\nby i) removing the rules that contain negative literals, not l, where l ∈M, and ii) removing\nall negative literals that occur in the rest of the rules. The interpretation, M, is a stable model\nof Π if M = LM(ΠM) where LM(ΠM) stands for the least model of ΠM. Stable models are\nnot necessarily unique. There can be zero, one, or multiple stable models for a normal logic\nprogram. For example, given the program, {q ←not p, p ←not q}, there are two stable\nmodels: {p} and {q}. However, for the program, {q ←not p}, there is no stable model.\nThe formal language used in answer set programming paradigm is called AnsProlog, which\nextends normal programs with i) contraints, ii) strong negation, and iii) disjunction. An ASP\nrule is formally represented as\nc1 ∨c2 ∨· · · ∨ck ←a1, a2, . . . , am, not b1, not b2, . . . , not bn\nwhere ai (1 ≤i ≤m), bj (1 ≤j ≤n), cl (1 ≤l ≤k) are literals, not denotes negation as\nfailure, and the rule head is a disjunction of literals. An ASP rule is called a constraint if the\nrule head is empty. An example is shown below:\n←l1, l2, . . . , ln\nThe constraint forces li (1 ≤i ≤n) not to be included in the models (answer sets) of the\nprogram.\nFor the semantics of ASP, let Π1 be an ASP program that does not contain any negative\nliteral in the body of the rules and M1 be a set of ground literals, M1 is an answer set for Π1\nif i) M1 satisﬁes all the rules in Π1 and ii) M1 is minimal. Next, let Π2 be an ASP program\nthat contains negative literals in the body of the rules and M2 be a set of ground literals, M2\nis an answer set for Π2 if M2 is the answer set of ΠM2\n2 , the reduct of Π2 with respect to M2.\nGiven the deﬁnition of answer sets, an ASP program Π′ entails a ground literal l if l is in every\nanswer set of Π′. For any query of the form l1 ∧l2 . . . ∧ln where li (1 ≤i ≤n) is a literal, it\nwill give one of the following answers:\n38\n• yes, if Π |= {l1, l2, . . . , ln}\n• no, if there exists li such that Π |= ¬li\n• unknown, otherwise\nAnswer set programming can represent defaults and exceptions. The general form of a\ndefault is\np(X) ←c(X), not ab(d(X)), not ¬p(X).\nwhere d stands for default, ab represents abnormality, not refers to negation as failure, and ¬\ndenotes strong negation. The formula says that “Normally elements of class C has the property\nP” or “X has the property P if 1) X is a class of C, 2) X is not abnormal with respect to d,\nand 3) it cannot be shown that X does not have property P.” A default can have two types\nof exceptions: strong exception and weak exception. A strong exception refutes the default\nconclusion and derives the opposite one. A weak exception stops the application of the default\nwithout defeating the default conclusion. Let e(X) be a strong exception to the default. It is\nrepresented as\n¬p(X) ←e(X).\nwhere P(X) denotes the default conclusion. A weak exception is denoted by a cancellation\naxiom. Let e′(X) be a weak exception, it is represented as\nab(d(X)) ←not ¬e′(X)\nThe formula says that X is abnormal with respect to d if there is no evidence that X is not an\nexception.\nA.3\nLogic Programming with Defaults and Argumentation Theories\nLogic programming with defaults and argumentation theories (LPDA) is a unifying defeasible\nreasoning framework that works on defaults and exceptions with prioritized rules and argumen-\ntation theories. It is based on well-founded semantics [37], which are three-valued semantics\ncontaining true, false, and undeﬁned. A literal has one of the following forms:\n• An atomic formula.\n• neg A, where A is an atomic formula.\n• not A, where A is an atom.\n• not neg A, where A is an atom.\n• not not L and neg neg L, where L is a literal.\n39\nLet A be an atom. A not-free literal refers to a literal that can be reduced to A or ¬A. A\nnot-literal refers to a literal that can be reduced to not A or not ¬A. LPDA has two types of\nrules: deﬁnite rules and defeasible rules. A deﬁnite rule is of the form:\nL ←Body\nwhere L is a not-free literal and Body is a conjunction of literals. A defeasible rule is of the\nform:\n@r L ←Body\nwhere r is a term that denotes the label of the rule.\nEach LPAD program is accompanied with an argumentation theory that decides when a\ndefeasible rule is defeated. An argumentation theory is a set of deﬁnite rules with four special\npredicates: defeated, opposes, overrides, and cancel where defeated denotes the defeated-\nness of a defeasible rule, opposes indicates the literals that are incompatible with each other,\noverrides denotes a binary relation between defeasible rules indicating priority, and cancel\ncancels a defeasible rule. There are multiple sets of argumentation theories. Users can specify\none of them for use or manipulate with the existing ones. A rule is defeated if it is refuted,\nrebutted, or disqualiﬁed. The meaning of refuted, rebutted, and disqualiﬁed depends on the\nargumentation theories. Generally, a rule is refuted if there is another rule that draws an in-\ncompatible conclusion with a higher priority. A rule is rebutted if there is another rule that\ndraws an incompatible conclusion without any priority information. A rule is disqualiﬁed if it\nis cancelled, self-defeated, etc..\nGiven an LPDA program P and the argumentation theory AT, the well-founded model of\nP∪AT, denoted as WFM(P, AT), is deﬁned as the limit of the following transﬁnite induction:\n• Ik = LPM(P∪AT\nIk−1 ), where k is a non-limit ordinal\n• Ik = ∪i≤kIi, where k is a limit ordinal\nP ∪AT can be reduced to a normal logic program P ′ ∪AT such that P ′ ∪AT has the same\nwell-founded model as P ∪AT. The reduction is done by changing every defeasible rule\n@r L ←Body to a deﬁnite rule of the form:\n@r L ←Body, not $defeated(handle(r, L))\nwhere the term handle(r, L) is called the handle of the rule.\nB\nAppendix\nIn this section, we show the ASP programs for the marathon puzzle and the jobs puzzle.\n40\nB.1\nThe Marathon Puzzle\nrunner(dominique).\nrunner(ignace).\nrunner(naren).\nrunner(olivier).\nrunner(pascal).\nrunner(philippe).\nposition(1..6).\n1 { allocated to(A,B) : position(B) } 1 :- runner(A).\n:- runner(C), position(D), allocated to(C,D), runner(E), allocated to(E,D), C != E.\n:- allocated to(olivier,6).\nbefore(F,G) :- runner(F), position(H), allocated to(F,H), runner(G), position(I), allocated\nto(G,I), H ¡ I.\n:- before(naren,dominique).\n:- before(naren,pascal).\n:- before(naren,ignace).\n:- before(olivier,dominique).\n:- before(olivier,pascal).\n:- before(olivier,ignace).\n:- position(J), J ¿= 3, allocated to(dominique,J).\n:- position(K), K ¿ 4, allocated to(philippe,K).\n:- allocated to(ignace,2).\n:- allocated to(ignace,3).\n:- position(L), allocated to(pascal,L), position(M), allocated to(naren,M), L != M - 3.\n:- allocated to(ignace,4).\n:- allocated to(dominique,4).\nanswer(N,O) :- runner(N), position(O), allocated to(N,O).\n#hide. #show answer/2.\nB.2\nThe Jobs Puzzle\nperson(roberta). person(thelma). person(steve). person(pete).\nfemale(roberta). female(thelma).\nmale(steve). male(pete).\n:- person(X), male(X), female(X).\n1 hold(X,Y) : person(X) 1 :- job(Y).\n2 hold(X,Y) : job(Y) 2 :- person(X).\njob(chef). job(guard). job(nurse). job(operator). job(police). job(teacher). job(actor).\njob(boxer).\nmale(X) :- person(X), job(nurse), hold(X,nurse).\nmale(X) :- person(X), job(actor), hold(X,actor).\nhusband(Y,X)\n:-\nperson(X),\njob(chef),\nhold(X,chef),\nperson(Y),\njob(operator),\nhold(Y,operator).\nmale(X) :- person(X), person(Y), husband(X,Y).\n41\nfemale(Y) :- person(X), person(Y), husband(X,Y).\n:- job(boxer), hold(roberta,boxer).\n:- educated(pete).\neducated(X) :- person(X), job(nurse), hold(X,nurse).\neducated(X) :- person(X), job(police), hold(X,police).\neducated(X) :- person(X), job(teacher), hold(X,teacher).\n:- job(chef), hold(roberta,chef).\n:- job(police), hold(roberta,police).\n:- person(X), job(chef), hold(X,chef), job(police),hold(X,police).\nanswer(hold(X,Y)) :- job(Y), hold(X,Y).\n42\n",
  "categories": [
    "cs.AI",
    "cs.CL"
  ],
  "published": "2019-05-11",
  "updated": "2019-05-11"
}