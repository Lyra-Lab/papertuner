{
  "id": "http://arxiv.org/abs/1905.05699v1",
  "title": "Development of Deep Learning Based Natural Language Processing Model for Turkish",
  "authors": [
    "Baris Baburoglu",
    "Adem Tekerek",
    "Mehmet Tekerek"
  ],
  "abstract": "Natural language is one of the most fundamental features that distinguish\npeople from other living things and enable people to communicate each other.\nLanguage is a tool that enables people to express their feelings and thoughts\nand to transfers cultures through generations. Texts and audio are examples of\nnatural language in daily life. In the natural language, many words disappear\nin time, on the other hand new words are derived. Therefore, while the process\nof natural language processing (NLP) is complex even for human, it is difficult\nto process in computer system. The area of linguistics examines how people use\nlanguage. NLP, which requires the collaboration of linguists and computer\nscientists, plays an important role in human computer interaction. Studies in\nNLP have increased with the use of artificial intelligence technologies in the\nfield of linguistics. With the deep learning methods which are one of the\nartificial intelligence study areas, platforms close to natural language are\nbeing developed. Developed platforms for language comprehension, machine\ntranslation and part of speech (POS) tagging benefit from deep learning\nmethods. Recurrent Neural Network (RNN), one of the deep learning\narchitectures, is preferred for processing sequential data such as text or\naudio data. In this study, Turkish POS tagging model has been proposed by using\nBidirectional Long-Short Term Memory (BLSTM) which is an RNN type. The proposed\nPOS tagging model is provided to natural language researchers with a platform\nthat allows them to perform and use their own analysis. In the development\nphase of the platform developed by using BLSTM, the error rate of the POS\ntagger has been reduced by taking feedback with expert opinion.",
  "text": "TÜRKÇE İÇİN DERİN ÖĞRENME TABANLI DOĞAL DİL İŞLEME \nMODELİ GELİŞTİRİLMESİ \nBarış BABÜROĞLUa, Adem TEKEREKb, Mehmet TEKEREKc \n \naKahramanmaraş Sütçüimam Üniversitesi, Enformatik Anabilim Dalı, 46040, Kahramanmaraş, Türkiye, \nbarisbaburoglu@gmail.com \nbGazi Üniversitesi, Bilgi İşlem Dairesi Başkanlığı, 06560, Ankara, Türkiye, atekerek@gazi.edu.tr \ncKahramanmaraş Sütçüimam Üniversitesi, Eğitim Fakültesi, Bilgisayar ve Öğretim Teknolojileri Eğitimi Bölümü, \n46040, Kahramanmaraş, Türkiye, tekerek@ksu.edu.tr \nÖzet— Doğal dil, insanları diğer canlılardan ayıran ve insanların iletişim kurmasını sağlayan en \ntemel özelliklerden biridir. Dil, insanın duygu ve düşüncelerini ifade etmede kullandığı ve \nkültürlerin nesiller boyunca aktarılmasını sağlayan bir araçtır. Günlük hayatta karşılaşılan yazılar \nve sesler birer doğal dil örneğidir. Doğal dilde birçok kelime zamanla yok olurken diğer taraftan \nyeni kelimeler de türetilmektedir. Bu yüzden doğal dil işleme (DDİ) süreci insan için bile karmaşık \nyapıya sahipken, bilgisayar ortamında işlenmesi de zor olmaktadır. İnsanların dili nasıl kullandığını \ndil bilim alanı incelemektedir. Dil bilimciler ve bilgisayar bilimcilerinin ortak çalışmasını gerektiren \ndoğal dil işleme çalışmaları, insan bilgisayar etkileşiminde önemli rol oynamaktadır. Doğal dil \nişleme çalışmaları, yapay zekâ teknolojilerinin, dil bilimi alanında kullanılması ile artmıştır. Yapay \nzekâ çalışma alanlarından olan derin öğrenme yöntemleri ile doğal dile yakın seviyede platformlar \ngeliştirilmektedir. Dili anlama, makine çevirisi ve sözcük etiketleme için geliştirilen platformlar \nderin öğrenme yöntemlerinden faydalanmaktadır. Derin öğrenme mimarilerinden olan \nözyinelemeli sinir ağları (Recurrent Neural Network - RNN), metin veya ses verileri gibi sıralı \nverileri işlemede tercih edilmektedir. Bu çalışmada bir RNN türü olan iki yönlü uzun-kısa vadeli \nbellek (Bidirectional Long Short - Term Memory - BLSTM) kullanılarak Türkçe sözcük etiketleme \nmodeli önerilmiştir. Önerilen sözcük etiketleme modeli, doğal dil araştırmacılarına, kendi \nanalizlerini gerçekleştirme ve kullanabilme imkânı verecek bir platform ile sunulmaktadır. İki yönlü \nLSTM kullanılarak geliştirilen platformun geliştirilme aşamasında uzman görüşü ile geri bildirimler \nalınarak, sözcük etiketleyicinin hata oranı azaltılmıştır. \nAnahtar Kelimeler— Doğal Dil İşleme, Derin Öğrenme, İki Yönlü Uzun-Kısa Vadeli Bellek, \nÖzyinelemeli Sinir Ağları, Sözcük Etiketleme \nDEVELOPMENT OF DEEP LEARNING BASED NATURAL LANGUAGE \nPROCESSING MODEL FOR TURKISH \nAbstract— Natural language is one of the most fundamental features that distinguish people from \nother living things and enable people to communicate each other. Language is a tool that enables \npeople to express their feelings and thoughts and to transfers cultures through generations. Texts \nand audio are examples of natural language in daily life. In the natural language, many words \ndisappear in time, on the other hand new words are derived. Therefore, while the process of natural \nlanguage processing (NLP) is complex even for human, it is difficult to process in computer system. \nThe area of linguistics examines how people use language. NLP, which requires the collaboration of \nlinguists and computer scientists, plays an important role in human computer interaction. Studies \nin NLP have increased with the use of artificial intelligence technologies in the field of linguistics. \nWith the deep learning methods which are one of the artificial intelligence study areas, platforms \nclose to natural language are being developed. Developed platforms for language comprehension, \nmachine translation and part of speech (POS) tagging benefit from deep learning methods. \nRecurrent Neural Network (RNN), one of the deep learning architectures, is preferred for processing \nsequential data such as text or audio data. In this study, Turkish POS tagging model has been \nproposed by using Bidirectional Long-Short Term Memory (BLSTM) which is an RNN type. The \nproposed POS tagging model is provided to natural language researchers with a platform that \nallows them to perform and use their own analysis. In the development phase of the platform \ndeveloped by using BLSTM, the error rate of the POS tagger has been reduced by taking feedback \nwith expert opinion. \nKeywords— Natural Language Processing, Deep Learning, Bidirectional Long-Short Term \nMemory, Recursive Neural Networks, POS Tagging \n \n1. GİRİŞ \nDoğal dil, insanların kendilerini ifade etmeleri ve \niletişim kurabilmeleri için kullanılan bir araçtır. \nChomsky, dilin çocukluk yıllarında duyulandan, \ndoğal bir dile dönüşümünün, insanın genetik \nyapısıyla ilişkili olduğunu ifade etmektedir \n(Chomsky, 1986). İnsanların dili nasıl edindiği, \nürettiği ve anladığı dil biliminin araştırma alanıdır. \nDil bilimciler dilsel ifadeleri işlemek için kural \ntabanlı yaklaşımlar öne sürmüşlerdir. Ancak dil \nkullanımının üreticiliği kurallara her zaman \nuymamaktadır. Bu doğrultuda dil ifadelerine \ndilbilgisi kuralları uygulamak yerine istatistiksel \nyaklaşımlar uygulanarak dil kullanımında ortak \nkalıplar \nelde \nedilmeye \nçalışılmıştır. \nDilin \nistatistiksel modelleri, dil bilimi ve bilgisayar \nbiliminin alt bilim dalı olan doğal dil işleme (DDİ) \nçalışmalarında başarı ile uygulanmıştır (Schütze & \nManning, 1999). DDİ’nin amacı, doğal dilleri \notomatik \nolarak \noluşturma \nve \nanlamadaki \nproblemleri incelemektir (Young, Hazarika, Poria, \n& Cambria, 2018). DDİ insanlar tarafından üretilen \nsesleri ve metinleri işleyerek insan bilgisayar \netkileşiminin sağlanmasına yardımcı olmaktadır.  \nDDİ insan dilinin otomatik analizi ve gösterimi için \nteorik \nolarak \nmotive \nedilmiş \nhesaplama \nteknikleridir (Cambria & White, 2014). Her yaşta \ninsanın sosyal medyaya ulaşabildiği bir ortamda, \nüretilen veri miktarı, her geçen gün artarak devam \netmektedir. İnsanlar tarafından doğal olarak \noluşturulan veriler, doğrudan işlenecek durumda \ndeğildir. Bu yüzden insan makine iletişimini \nsağlamak için verileri anlamlandırma ve verimli \nkullanabilme \nçabası \nbirçok \nalanın \nbirlikte \nçalışmasını gerektirmiştir. İlk zamanlarda yapılan \nçoğu DDİ çalışmaları, tek tek sözcüklere \nodaklanmışken 19. yüzyılın sonlarına doğru \nsözcüklerin birbirleriyle olan ilişkisine ve bütün \nüzerinde anlam bilim çalışmalarına yönelmiştir \n(Cambria & White, 2014).  \nDDİ çalışmaları ilk olarak metni anlamak için ses \nveya metinden özellik çıkarımı yapan bir ön \nişlemeden \ngeçmektedir. \nArdından \nşekilbilim \n(morphological), sözdizim (syntactic), anlambilim \n(semantic) ve söylev (discourse) işleme çalışmaları \ngerçekleştirilebilmektedir. Bu çalışma alanları \nsözcük kökleri, sözcük bağlamları ve anlambilim \naçısından bazı zorluklara sahiptir. Zorlukları aşmak \niçin geliştirilen dilbilgisine dayalı kural tabanlı \nDDİ çalışmaları (Brill 1992), (J. Gimenez and L. \nMarquez \n2004), \nel \nyapımı \nözelliklere \ndayanmaktadır. El yapımı özellikler zaman \nalmakta ve yetersiz kalmaktadır (Young, Hazarika, \nPoria, & Cambria, 2018). Eksikliklerin giderilmesi \niçin geliştirilen yapay zekâ yöntemlerinin önemli \nbir uygulaması olan derin öğrenme, yapay sinir ağı \nyapısı, güçlü donanımı ve büyük veri girdisi ile \ndaha iyi sonuçlar elde edilmesini sağlamıştır (Song \n& Lee, 2013). \n1965 yılında derin öğrenmenin temeli kabul edilen \nçok katmanlı bir perceptron türü algoritma \n(Ivakhnenko & Lapa, 1965) önerilmiştir. Fakat o \nyıllardaki basit bir ağın eğitiminin uzun sürmesi ve \nyüksek hesaplama maliyetlerinden dolayı, destek \nvektör makinaları gibi el ile hazırlanmış özelliklere \nsahip modeller (Cortes & Vapnik, 1995)) kabul \ngörmüştür (Şeker, Dirib, & Balık, 2017). Yakın \nzamanda grafik işleme birimi (GPU) ve diğer \ndonanımsal \ngelişmeler \nsayesinde \nhesaplama \nmaliyetleri \ndüştüğünden, \nçok \nsayıda \ngizli \nkatmandan oluşan yapay sinir ağları tekrar \nkullanılmaya başlanmıştır (Schmidhuber, 2015). \nBu doğrultuda DDİ çalışmalarında, makine \nçevirisi, bilgi alma, metin özetleme, soru \ncevaplama, bilgi çıkarma, konu modelleme ve \nsözcük etiketleme gibi görevlere, derin öğrenme \nuygulanmasına odaklanılmaktadır (Young, et al., \n2018) (Şeker, Dirib, & Balık, 2017). \nBasit bir derin öğrenme çerçevesinin, adlandırılmış \nvarlık tanıma, anlamsal rol etiketleme ve sözcük \netiketleme gibi birçok DDİ görevinde, en modern \nyaklaşımlardan daha iyi performans gösterdiği \nortaya konulmuştur (Collobert, ve diğerleri, 2011). \nDDİ alanında istatistiksel yöntemler, kural tabanlı \nyöntemlerden daha başarılı olmaktadır. Bu alanda \nsözcük etiketlemesi, sözcük türlerinin sözcüklere \natanması ile gerçekleştirilmektedir (sözcük/isim, \nsıfat, fiil, vb.). Etiketler bilgisayarların cümlede \nifade edileni anlamasında kolaylık sağlamaktadır. \nAncak sözcükler farklı bağlamlarda kullanıldığı \nzaman farklı anlamlar ifade edebilmektedir. \nÖrneğin ‘yüz’ sözcüğü kullanıldığı bağlama göre \nisim veya fiil etiketini alabilmektedir. Verilen \nörneği incelediğimizde, her sözcüğü etiketlemenin \nsöz konusu olmadığı görülmektedir. Sözcük \netiketlemede \nbelirsizliği \ngidermek \niçin \netiketlenecek \nsözcüğün \nöncesinde \nkullanılan \nsözcüklere (geri yönde) ve sonrasında kullanılan \nsözcüklere(ileri yönde) bakılarak sözcük, doğru \netiket \nsınıfına \neklenebilmektedir. \nEtiketleme \nişlemlerinde, iki yönde ki bilgileri kullanarak \nolasılık üreten, bir tekrarlayan sinir ağı türü olan, \niki yönlü uzun-kısa vadeli bellek (BLSTM) \nağlarının, sıralı verileri etiketlemek için doğal dil \nişleme \nçalışmalarında \nçok \netkili \nolduğu \ngösterilmektedir (Wang, Qian, K. Soong, He, & \nZhao, 2015). BLSTM mimarisi, dil modelleme \n(Sundermeyer, \nSchlüter, \n& \nNey, \n2012), \n(Sundermeyer, Ney, & Schluter, 2015), dili anlama \n(Yao, Zweig, Hwang, Shi, & Yu, 2013), makine \nçevirisi (Sundermeyer, Alkhouli, Wuebker, & Ney, \n2014) ve sözcük etiketlemesi gibi doğal dil işleme \nalanındaki uygulamalar için üstün performans elde \nedilmesine yardımcı olmaktadır (Wang, Qian, K. \nSoong, He, & Zhao, 2015). \nBu çalışma 4 bölümden oluşmaktadır. 2. bölümde \nçalışmada kullanılan derin öğrenme yöntemleri \nanlatılmıştır. \n3. \nbölümde \nönerilen \nmodelin \ngeliştirilme \nsüreçlerinden \nbahsedilmiştir. \n4. \nbölümde ise sonuçlara değinilmiştir. \n2. DERİN ÖĞRENME \nDerin öğrenme, özellik çıkarma için ardışık işlem \nbirimi katmanları kullanan ve her katman çıktısı bir \nsonraki katmanın girişini besleyen bir yapay sinir \nağı türüdür (Deng & Yu, 2014). Yüksek hesaplama \nve veri işlemeye ihtiyaç duyan alanlarda başarılı \nsonuçlar sunan derin öğrenme yöntemleri, DDİ \naraştırmalarında da başarı göstermektedir. DDİ \nproblemlerini \nhedef \nalan \nmakine \nöğrenme \nyaklaşımları, çok yüksek boyutlu ve seyrek \nözellikler üzerine eğitilmiş sığ modellere (örneğin, \ndestek vektör makinaları ve lojistik regresyon) \ndayanmaktadır. Ancak vektör temsillerine dayanan \nderin sinir ağları, çeşitli DDİ problemlerinde \nbaşarılı sonuçlar göstermektedir. Bu sonuçlar, \nsözcük ekleme ve derin öğrenme yöntemlerinin \nbaşarısı ile ortaya çıkmıştır (Mikolov, Karafiat, \nBurget, Cernocky, & Khudanpur, 2010) (Mikolov, \nSutskever, Chen, Corrado, & Dean, 2013). \nDerin öğrenme yöntemlerinin, DDİ problemlerinde \niyi performans göstermesi ile zor DDİ çalışmalarını \nçözmek için Konvolüsyonel sinir ağları (CNNs), \ntekrarlayan sinir ağları ve özyinelemeli sinir \nağları(RNNs)  gibi temel derin öğrenme ile ilgili \nmodeller önerilmektedir (Young, Hazarika, Poria, \n& Cambria, 2018). Peilu Wang ve arkadaşlarının \nyaptığı bir çalışmada (2015), sözcük etiketleme \ngörevi için sözcük ekleme ile Çift Yönlü Uzun Kısa \nVadeli Bellek Tekrarlayan Sinir Ağı (BLSTM-\nRNN) kullanımı önerilmiştir. BLSTM-RNN’in \nkonuşma ifadeleri, el yazıları veya metin verileri \ngibi sıralı verileri etiketlemek için çok etkili olduğu \ngösterilmiştir. \n2.1. Uzun Kısa Vadeli Bellek Ağları (LSTMs) \nLSTM’ler uzun vadeli bağımlılık problemini \nçözmek için tasarlanan özel bir RNN türüdür \n(Hochreiter & Schmidhuber, 1997). Bilgiyi uzun \nsüre hatırlama davranışı sergilerler. LSTM'ler tüm \ntekrarlayan sinir ağları gibi bir sinir ağının tekrar \neden hücrelerine sahiptir, fakat tekrar eden hücre \ntek bir sinir ağı kapısına sahip olmak yerine, \netkileşime giren 3 adet kapıya (Şekil 1) sahiptir \n(Graves & Schmidhuberab, 2005). \n \nŞekil 1. Bir Uzun-Kısa Vadeli Hafıza Hücresi \nBir \nLSTM \ndavranışını \ntanımlayan \nkapılar \ngüncelleme (update), unutma (forget) ve çıkış \n(output) kapıları olarak adlandırılmaktadır. Burada \ni<t>, f <t> ve o<t> sırasıyla t anındaki güncelleme, \nunutma ve çıkış kapılarını, c<t>, hücre durumunu \nve a<t>, tüm işlemler sonunda karar verilen bilgiyi \nbelirtir. \nBir sigmoid(σ) işlevine sahip unutma kapısı (eşitlik \n1) giriş (x<t>) ve önceki hücre durumu (a<t−1>) \nbilgisine \nbakarak \nbilginin \nunutulup \nunutulmayacağına karar verir. \nf <t> = σ(Wf[a<t−1>, x<t>]) + bf \n(1) \nBir sigmoid(σ) işlevine sahip güncelleme kapısı \n(eşitlik 2) hangi bilgilerin güncelleneceğine karar \nverir. \ni<t> = σ(Wu[a<t−1>, x<t>]) + bu \n(2) \nBir \ntan h \nişlevi, \nhücre \ndurumuna \n(c<t>) \neklenebilecek yeni hücre durumu aday bilgileri \n(c̃<t>) (eşitlik 3) vektörünü oluşturur. \nc̃<t> = tan h(Wc[a<t−1>, x<t>]) + bc \n(3) \nDolayısıyla, mevcut hücre durumu(c<t>) (eşitlik \n4), hem önceki hücre durumu (c̃<t>) hem de hücre \ntarafından üretilen mevcut bilgileri kullanılarak \nelde edilmiş olur. \nc<t> = i<t> ∗c̃<t> + f <t> ∗c<t> \n(4) \nBir sigmoid(σ) işlevine sahip çıkış kapısı (eşitlik 5) \nhücre durumunun hangi kısımlarını üreteceğimize \nkarar verir. Daha sonra hücre durumu (c<t>) tan h \nişlevi ile filtrelenir ve çıkış kapısının çıktısı ile \nçarpılır. Bu durumda sadece karar verilen bilgiler \n(a<t>) (eşitlik 6) elde edilir. \no<t> = σ(Wo[a<t−1>, x<t>]) + bo \n(5) \na<t> = o<t> tan h(c<t>) \n(6) \nStandart LSTM ağları dizileri geçici sırayla işler, \ngelecekteki bağlamı görmezden gelirler. (Schuster \n& K. Paliwal, 1997)’da önerilen iki yönlü RNN \ngizli bağlantıların ileriye doğru sırayla aktığı, \nikinci bir katman uygulayarak tek yönlü LSTM \nağlarını genişletmektedir.  \n2.2. İki Yönlü Uzun Kısa Vadeli Bellek Ağları \n(BLSTMs) \nİki Yönlü Özyinelemeli Sinir Ağları (BRNNs) bir \ndizinin hem önceki zamanlardaki hem de ileriki \nzamanlardaki bilgilerini kullanabilmeyi sağlar. Bu \nyöntem, normal bir RNN'nin durum nöronlarını \npozitif zaman yönünden (ileri durumlar- a⃗ ) ve \nnegatif zaman yönünden (geri durumlar- a⃖⃗) \nbirbiriyle bağlantısı olmayan iki duruma (şekil 2) \nbölmektedir (Schuster & K. Paliwal, 1997). \n \nŞekil 2. İki Yönlü Özyinelemeli Sinir Ağının \n(BRNN) 3 Zaman Adımı \nBöylece normal bir RNN’den farklı olarak iki \nkatmandan gelen değerler ile hesaplama yapılır. \nGeri durumlar ve ileri durumlardan elde edilen \ndeğerler, ağırlıklar ve koşullu olasılık bayes \ndeğerlerinin hesaplanmasıyla t anındaki ŷ<t> \ndeğeri (eşitlik 7) elde edilmiş olur. \nŷ<t> = g(Wy[a⃗ <t>, a⃖⃗<t>]) + by \n(7) \nİki yönlü özyinelemeli sinir ağlarında tekrar eden \nhücrelerde LSTM hücresi kullanıldığında İki yönlü \nuzun-kısa vadeli bellek ağları (BLSTMs) mimarisi \n(Şekil \n3) \nelde \nedilmektedir \n(Graves \n& \nSchmidhuberab, 2005). \n \nŞekil 3. İki Yönlü Uzun-Kısa Vadeli Bellek \nAğının ileri ve geri yönde zaman dizisi \nBu mimaride ileri ve geri yöndeki hesaplamalar \naynı anda çalıştırılır ve iki yönde yapılan \nhesaplamalar sonucu ulaşılan bilgiler birleştirilerek \nçıktı sonucuna ulaşılır. Bu şekilde iki yöndeki \nbilgilerin kullanılması sıralı verilerin işlenmesinde \navantaj \nsağlar. \n(Graves \n& \nSchmidhuberab, \n2005)’deki araştırmada iki yönlü ağların tek yönlü \nağlardan daha etkili olduğunu gösterilmiştir. İki \nyönlü uzun-kısa vadeli bellek(BLSTM) ağları, \ndoğal dil işleme çalışmalarında da başarılı şekilde \nkullanılmaktadır (Wang, Qian, K. Soong, He, & \nZhao, 2015).  \nGünümüzde DDİ alanında çalışan araştırmacılar, \nderin \nöğrenme \nyöntemlerinin \nkullanımına \nyönelmektedir. \nDerin \nöğrenme \nyöntemleri \nkullanılarak \ngeliştirilen \ndoğal \ndil \nişleme \nçalışmalarının \nİngilizce \nüzerine \nyoğunlaştığı \ngörülmektedir (Şeker, Dirib, & Balık, 2017). \nTürkçe için derin öğrenme yöntemleri ile doğal dil \nişleme çalışmaları sınırlıdır. Türkçe sözcük \netiketleme için derin öğrenme ile eğitilmiş bir \nmodel henüz bulunmamaktadır. Bu çalışmada, \nRNN mimarisinde iki yönlü uzun-kısa vadeli \nbellek kullanılarak Türkçe sözcük etiketleme için \nbir DDİ modelinin geliştirilmesi amaçlanmıştır. \nPlatform ile dil bilim bilimi ve bilgisayar bilimi \naltında \nçalışan \naraştırmacılar \niçin \nkaynak \neksikliğinin giderilmesi hedeflenmektedir. Ayrıca \nplatformun, insana özgü doğal dile yakın seviyede \nişleme \ngerçekleştirmeyi \ndestekleyeceği \ndüşünülmektedir. \n3. ÖNERİLEN DDİ MODELİ \nBu çalışmada Keras ve TensorFlow ve iki yönlü \nLSTM \nRNN(BLSTM-RNN) \nderin \nöğrenme \nmodeli kullanılmıştır. Yazılım geliştirilmesi için \nPython programlama dili kullanılmıştır.  \nBLSTM-RNN ile eğitimi gerçekleştirilen derlemin \niçeriğinde 35 konu ile işaretlenmiş, konu başına \n200 belgeden oluşan, toplam 7000 adet belge \nbulunmaktadır. Her belgenin başlığı, özeti, (varsa) \nanahtar kelimeleri, kaynak ismi bulunmaktadır. \n(Ozturk, Sankur, Gungor, & Yilmaz, 2014)’deki \nçalışma ile hazırlanan bu belgelerin özet kısımları \nkullanılmıştır. \nKeras kütüphanesinin kelimelerle veya etiketlerle \ndeğil sayılarla çalışması gerekmektedir. Her bir \nkelimeye ve etikete benzersiz bir tamsayı atanarak \nbir sözlükte dizine alınmıştır. Bu sözlükler kelime \ndağarcığı ve etiket dağarcığı olarak adlandırılabilir. \nAyrıca bilinmeyen kelimeler için bir değer (OOV -\nOut Of Vocabulary) eklenmiştir. Keras yalnızca \nsabit boyutlu dizilerle ilgilendiği için veri setindeki \nen uzun cümle hesaplanmıştır. Buna göre diğer \ncümlelerde aynı boyutu sağlamak için bir değer \n(indeks olarak “0” ve karşılık gelen etiket olarak “-\nPAD-”) eklenmiştir. Son olarak eğitim aşamasına \ngeçilmeden önce Etiket dizileri One-Hot Encoded \netiketlerinin dizilerine dönüştürülmüştür. \nToplanan özet kısımlar, standart bir trigram Saklı \nMarkof Model konuşma parçacığı etiketleyici \n(HMM POS Tagger) ile eğitim verisinin sözcük \netiketlemesi gerçekleştirilmiştir (Çöltekin, 2014). \nElde \nedilen \nveriler \nNLTK \nKütüphanesi \nkullanılarak büyük harf, sayılar ve noktalama \nişaretlerinden temizlenerek eğitime hazır forma \ndönüştürülmüştür. \nWeb tabanlı olarak geliştirilen Türkçe için iki \nyönlü \nLSTM-RNN \nile \ndoğal \ndil \nişleme \nplatformunun \nmimarisi \nve \nçalışma \nşekli \nsunulmaktadır (Şekil 4). \n \nŞekil 4. İki Yönlü LSTM-RNN Türkçe Doğal Dil \nİşleme Platformu \nPlatform, araştırmacılara kaynak sağlaması ve \nkendi kendini geliştirmesi için kullanıcılara metin \ngirdisi ve belge girdisi PDF olmak üzere 2 ayrı \nekrandan veri girişi sağlamaktadır. Metin analizi \nekranı, kullanıcılara el ile metin girme ve analiz \ngerçekleştirme imkânı sağlamaktadır. Belge analizi \nekranı ile kullanıcılar PDF formatında belgeler \nyükleyerek \nverilerin \nanalizlerini \ngerçekleştirebilmektedir. Bu iki ekrandan alınan \nveriler \nilk \nolarak \nön \nişleme \nsürecinden \ngeçmektedir. \nÖn işleme aşamasında yazılı belgelerden elde \nedilen verilerin işlenebilir hale getirilmesi için DDİ \nnormalleştirme süreci uygulanır. Normalleştirme \nsüreci \nküçük \nharfe \ndönüştürme, \nnoktalama \nişaretlerinden arındırma ve sözcük parçalama \n(tokenize) işlemleridir. Veri ilk olarak küçük harfe \ndönüştürülür. İkinci aşamada, sözcük parçalama ile \nveriden cümleler elde edilir. Son olarak da veri, \ncümle \niçerisinde \nnoktalama \nişaretlerinden \narındırılarak sözcük etiketleme için hazır hale \ngetirilir.  \nÖn işlemeden geçen veri, sözcük etiketleme \naşamasında, iki yönlü LSTM ile analiz edilir. \nAnaliz sonucu her sözcüğe cümle içerisindeki \nbağlamına göre en olası etiket atanır.  \nEtiketli veri kısmında, etiketi atanan sözcükler ve \nfrekansları \nkullanıcıya \nsunulmaktadır. \nGerçekleştirilen \nanaliz \nsonuçlarının \ndökümü \nalınarak Türkçe doğal dil işleme çalışmalarında \nkullanılabilir. Ayrıca kullanıcı düzenleme önerme \nseçeneği ile yanlış olduğunu düşündüğü etiketleri \ndüzelterek platformun geliştirilmesine katkıda \nbulunabilir. \nDüzenleme \nyapılan \netiketlenmiş \nveriler daha sonra eğitim aşamasına dâhil edilip \netiketleme \ndoğruluğunun \narttırılması \niçin \nkullanılır.  \n \n \nŞekil 5. Metin Analizi Ekranı \n4. SONUÇ \nBu \nçalışmada, \nderin \nöğrenme \nyöntemleri \nkullanılarak dinamik ve kullanımı kolay bir web \ntabanlı DDİ modeli sunulmuştur. Herhangi bir el \nyapımı özellik veya dış araç kullanmadan \nsözcüklerin \nbağlamsal \nözelliklerinden \nfaydalanarak etiket tahminleri gerçekleştirilmiştir. \nBağlama bakılarak eğitim verisinde bulunmayan \nsözcükler tahmin edilmiştir. Tahmin için iki yönlü \nuzun-kısa vadeli bellek derin öğrenme mimarisi \nkullanılmıştır. \nKullanıcılara \ntahmin \nedilen \netiketleri düzeltme imkânı verilerek sistemin \ngeliştirilmesine dâhil olmaları sağlanmıştır. Derin \nöğrenme \nkullanarak \ndoğal \ndil \nişlemeyi \ngerçekleştiren ve kullanıcıları da sürece dâhil \nederek doğal kavramının sürekliliğinin sağlayan \nTürk dilinde ilk platform olduğu düşünülmektedir. \nGeliştirilen modelde kullanılan eğitim verisinin \nmiktarı arttıkça daha başarılı sonuçlar elde \nedileceğine inanılmaktadır. \nKaynakça \n \nAksan, M., & Aksan, Y. (2018). Linguistic \ncorpora : A view from Turkish. Studies in \nTurkish Language Processing (s. 301-\n327). içinde Springer Verlag. \nAksan, Y., & Yaldır, Y. (2012). A corpus-based \nword frequency list of Turkish: Evidence \nfrom the Turkish National Corpus. 15 th \nInternational Conference on Turkish \nLinguistics.  \nBill, E. (1992). A Simple Rule-Based Part of \nSpeech Tagger. ANLC '92 Proceedings of \nthe third conference on Applied natural \nlanguage processing. Trento. \nCambria, E., & White, B. (2014). Jumping NLP \ncurves: A review of natural language \nprocessing research. IEEE Computational \nIntelligence Magazine, 9(2), 48-57. \nCambria, E., & White, B. (2014). Jumping NLP \nCurves: A Review of Natural Language \nProcessing Research. IEEE \nCOMPUTATIONAL INTELLIGENCE \nMAGAZINE, 9(2), 48-57. \nChomsky, N. (1986). Knowledge of Language: Its \nNature, Origin, and Use. New York. \nCollobert, R., Weston, J., Bottou, L., Karlen, M., \nKavukcuoglu, K., & Kuksa, P. (2011). \nNatural Language Processing (Almost) \nfrom Scratch. Journal of Machine \nLearning Research, 12, 2493-2537. \nCortes, C., & Vapnik, V. (1995). Support-Vector \nNetworks. Machine Learning, 20(3), 273-\n297. \nÇöltekin, Ç. (2014). A Set of Open Source Tools \nfor Turkish Natural Language Processing. \nProceedings of the Ninth International \nConference on Language Resources and \nEvaluation (LREC’14), (s. 1079–1086). \nDeng, L., & Yu, D. (2014). Deep Learning: \nMethods and Applications. Foundations \nand Trends in Signal Processing (Cilt 7, \ns. 197-387). içinde \nDobbin, K. K., & Simon, R. M. (2011). Optimally \nsplitting cases for training and testing \nhigh dimensional classifiers. Dobbin and \nSimon BMC Medical Genomics, 4(31). \nDunne, R. A., & Campbell, N. A. (1997). On The \nPairing Of The Softmax Activation And \nCross-Entropy Penalty Functions And \nThe Derivation Of The Softmax \nActivation Function. Conf. on Neural \nNetworks, (s. 181-185 ). Melb. \nGraves, A., & Schmidhuberab, J. (2005). \nFramewise Phoneme Classification with \nBidirectional LSTM and Other Neural \nNetwork Architectures. Neural Networks, \n602-610. \nHochreiter, S., & Schmidhuber, J. (1997). Long \nShort-Term Memory. Neural \nComputation, 9(8), 1735-1780. \nIvakhnenko, A., & Lapa, V. (1965). Cybernetic \npredicting devices. \nKingma, D. P., & Ba, J. L. (2015). Adam: A \nMethod for Stochastic Optimization. 3rd \nInternational Conference for Learning \nRepresentations.  \nKöksal, A. (1976). A first approach to a \ncomputerized model for the automatic \nmorphological analysis of Turkish. \nLeCun, Y., Bengio, Y., & Hinton, G. (2015). \nDeep learning. 521(7553), 436-444. \nMikolov, T., Karafiat, M., Burget, L., Cernocky, \nJ., & Khudanpur, S. (2010). Recurrent \nNeural Network Based Language Model. \nİnterspeech, 2.  \nMikolov, T., Sutskever, I., Chen, K., Corrado, G., \n& Dean, J. (2013). Distributed \nRepresentations of Words and Phrases \nand their Compositionality. Proceedings \nof the 26th International Conference on \nNeural Information Processing Systems, \n(s. 3111-3119). Lake Tahoe, Nevada. \nOzturk, S., Sankur, B., Gungor, T., & Yilmaz, M. \nB. (2014). Türkçe Etiketli Metin Derlemi \n(Turkish Labeled Text Corpus). 2014 \n22nd Signal Processing and \nCommunications Applications Conference \n(SIU).  \nSchmidhuber, J. (2015). Deep Learning in Neural \nNetworks: An Overview. 61, 85-117. \nSchuster, M., & K. Paliwal, K. (1997). \nBidirectional Recurrent Neural Networks. \nIEEE Transactions on Signal Processing, \n45(11), 2673-2681. \nSchuster, M., & K. Paliwal, K. (1997). \nBidirectional Recurrent Neural Networks. \nIEEE TRANSACTIONS ON SIGNAL \nPROCESSING, 45(11), 2673 - 2681. \nSchütze, H., & Manning, C. D. (1999). \nFoundation of Statistical Natural \nLanguage Processing.  \nSong, H., & Lee, S.-Y. (2013). Hierarchical \nRepresentation Using NMF. International \nConference on Neural Information \nProcessing 2013: Neural Information \nProcessing, (s. 466-473). \nSundermeyer, M., Alkhouli, T., Wuebker, J., & \nNey, H. (2014). Translation Modeling \nwith Bidirectional Recurrent Neural \nNetworks. EMNLP Conference, (s. 14-\n25). Doha, Qatar. \nSundermeyer, M., Ney, H., & Schluter, R. (2015). \nFrom Feedforward to Recurrent LSTM \nNeural Networks for Language Modeling. \nIEEE/ACM Transactions on Audio, \nSpeech and Language Processing, 23(3), \n517-529. \nSundermeyer, M., Schlüter, R., & Ney, H. (2012). \nLSTM Neural Networks for Language \nModeling. Interspeech. USA. \nŞeker, A., Dirib, B., & Balık, H. H. (2017). Derin \nÖğrenme Yöntemleri ve Uygulamaları \nHakkında Bir İnceleme. Gazi Journal of \nEngineering Sciences (GJES), 3(3), 47-\n64. \nVoskoglou, C. (2017, 5 4). What is the best \nprogramming language for Machine \nLearning. 2019 tarihinde \ndevelopereconomics: \nhttps://www.developereconomics.com/bes\nt-machine-learning-language adresinden \nalındı \nWang, P., Qian, Y., K. Soong, F., He, L., & Zhao, \nH. (2015, 10 21). Part-of-Speech Tagging \nwith Bidirectional Long Short-Term \nMemory Recurrent Neural Network. \narXiv:1510.06168v1 [cs.CL]. adresinden \nalındı \nYao, K., Zweig, G., Hwang, M.-Y., Shi, Y., & \nYu, D. (2013). Recurrent Neural \nNetworks for Language Understanding. \nInterspeech, (s. 2524–2528). \nYoung, T., Hazarika, D., Poria, S., & Cambria, E. \n(2018). Recent Trends in Deep Learning \nBased Natural Language Processing. \nIEEE Computational Intelligence \nMagazine, 55-75. \n \n \n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2019-05-07",
  "updated": "2019-05-07"
}