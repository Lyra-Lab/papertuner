{
  "id": "http://arxiv.org/abs/1905.05857v3",
  "title": "Variational Regret Bounds for Reinforcement Learning",
  "authors": [
    "Pratik Gajane",
    "Ronald Ortner",
    "Peter Auer"
  ],
  "abstract": "We consider undiscounted reinforcement learning in Markov decision processes\n(MDPs) where both the reward functions and the state-transition probabilities\nmay vary (gradually or abruptly) over time. For this problem setting, we\npropose an algorithm and provide performance guarantees for the regret\nevaluated against the optimal non-stationary policy. The upper bound on the\nregret is given in terms of the total variation in the MDP. This is the first\nvariational regret bound for the general reinforcement learning setting.",
  "text": "arXiv:1905.05857v3  [cs.LG]  10 Sep 2019\nVariational Regret Bounds for Reinforcement Learning\nRonald Ortner\nrortner@unileoben.ac.at\nPratik Gajane\npratik.gajane@unileoben.ac.at\nLehrstuhl für Informationstechnologie, Montanuniversität Leoben, Austria\nPeter Auer\nauer@unileoben.ac.at\nAbstract\nWe\nconsider\nundiscounted\nreinforcement\nlearning\nin\nMarkov\ndecision\nprocesses\n(MDPs) where both the reward functions and\nthe state-transition probabilities may vary\n(gradually or abruptly) over time.\nFor this\nproblem setting, we propose an algorithm and\nprovide performance guarantees for the regret\nevaluated against the optimal non-stationary\npolicy. The upper bound on the regret is given\nin terms of the total variation in the MDP.\nThis is the ﬁrst variational regret bound for the\ngeneral reinforcement learning setting.\n1\nINTRODUCTION\nA Markov decision process (MDP) is a discrete-time\nstate-transition system in which the transition dynam-\nics follow the Markov property (Puterman [1994],\nBertsekas and Tsitsiklis [1996]). MDPs are a standard\nmodel to express uncertainty in reinforcement learning\nproblems. In the classical MDP model, the transition\ndynamics and the reward functions are time-invariant.\nHowever, such ﬁxed transition dynamics and reward\nfunctions are insufﬁcient to model real world problems\nin which parameters of the world change over time. To\ndeal with such problems, we consider a setting in which\nboth the transition dynamics and the reward functions\nmay vary over time. These changes can be either abrupt\nor gradual. As a motivation, consider the problem of de-\nciding which ads to place on a webpage. The instanta-\nneous reward is the payoff when viewers are redirected\nto an advertiser, and the state captures the details of the\ncurrent ad. With a heterogeneous group of viewers, an\ninvariant state-transition function cannot accurately cap-\nture the transition dynamics. The instantaneous reward,\ndependent on external factors, is also better represented\nby changing reward functions. For additional motiva-\ntion and further applications, see [Yuan Yu and Mannor,\n2009a,b, Abbasi et al., 2013].\n1.1\nMAIN CONTRIBUTION\nFor reinforcement learning in MDPs with changes in\nreward functions and transition probabilities, we pro-\nvide an algorithm, Variation-aware UCRL, a variant of\nUCRL with restarts [Jaksch et al., 2010], which restarts\naccording to a schedule dependent on the variation in\nthe MDP (deﬁned in Section 2 below). For reinforce-\nment learning in an MDP with S states, A actions, di-\nameter D, and changes with a variation of V we derive\nfor our algorithm a high-probability upper bound on the\ncumulative regret after T steps of ˜O(V 1/3T 2/3DS\n√\nA).\nThis bound is optimal with respect to time T and\nvariation V and improves the known regret bound of\n˜O(L1/3T 2/3DS\n√\nA) for UCRL with restarts in MDPs\nwith L abrupt changes, when using a restart schedule de-\npendent on L [Jaksch et al., 2010]. In case when reward\nfunctions and transition probabilities change gradually,\nthe latter bound becomes trivial when L is of order T 1/3,\nwhile our bound is still sublinear as long as the varia-\ntion is sufﬁciently small. To the best of our knowledge,\nour bounds are the ﬁrst variational bounds for the gen-\neral reinforcement learning setting. So far, variational\nregret bounds have been derived only for simpler bandit\nsettings [Besbes et al., 2014, Chen et al., 2019].\n1.2\nRELATED WORK\nNilim and El Ghaoui [2005] consider MDPs with arbi-\ntrarily changing state-transition probabilities but ﬁxed re-\nward functions where it is assumed that the uncertainty\nin the transition probabilities is state-wise independent.\nThey provide a robust dynamic programming algorithm\nand prove that it is optimal with respect to the worst-case\nperformance in terms of the expected total cost.\nEven-Dar et al. [2005] and Dick et al. [2014] consider\nthe problem of MDPs with ﬁxed state-transition proba-\nbilities and changing reward functions and measure the\nperformance of the learner against the best stationary\npolicy in hindsight. Even-Dar et al. [2005] assume that\nthe learner has complete knowledge of all the previous\nreward functions (i.e., also for states not visited) and pro-\nvide regret bounds which depend on the mixing time.\nDick et al. [2014] model learning in MDPs as an online\nlinear optimization problem and propose solutions based\non variants of mirror-descent.\nYuan Yu and Mannor [2009a] and Yuan Yu and Mannor\n[2009b] consider arbitrary changes in the reward func-\ntions and arbitrary, but bounded, changes in the state-\ntransition probabilities.\nThey also give regret bounds\nthat scale with the proportion of changes in the state-\ntransition kernel and which in the worst case grow lin-\nearly with time.\nAbbasi et al. [2013] consider MDP problems with\n(oblivious) adversarial changes in state-transition prob-\nabilities and reward functions and provide an algorithm\nwhich guarantees O(\n√\nT) regret with respect to a com-\nparison set of stationary (expert) policies.\n2\nSETTING\nWe start with collecting some basic facts about Markov\ndecision processes (MDPs). In a (time-homogeneous)\nMDP M = (S, A, ¯r, p, s1) with a set S of S states,\na set A of A actions the learner starts in some initial\nstate s1. At each time step t = 1, 2, . . . she chooses\nan action at = a in the current state st = s, receives a\nrandom reward rt with mean ¯r(s, a) and observes a tran-\nsition to the next state st+1 = s′ according to transition\nprobabilities p(s′|s, a). Note that in a time-homogeneous\nMDP mean rewards and transition probabilities only de-\npend on the current state and the chosen action.\nAn MDP is called communicating, if for any two\nstates s, s′, when starting in s it is possible to reach s′\nwith positive probability choosing appropriate actions.\nIn communicating MDPs we deﬁne the diameter to be\nthe minimal expected time it takes to get from any state\nto any other state in the MDP, cf. [Jaksch et al., 2010].\nFor acting in an MDP one usually considers stationary\npolicies π : S →A that ﬁx for each state s the ac-\ntion π(s) to choose. The average reward ρ(M, π) of a\nstationary policy π is the limit of the expected average\naccumulated reward when following π, i.e.,\nρ(M, π) := lim\nT →∞\n1\nT E\n\" T\nX\nt=1\nrt\n#\n.\nThe optimal average reward ρ∗(M) = maxπ ρ(M, π)\nin communicating MDPs is independent of the initial\nstate s1 and cannot be increased when using nonstation-\nary policies [Puterman, 1994].\nIn the problem setting we consider the underlying MDP\nis not time-homogeneous. Rather the mean rewards and\ntransition probabilities depend on the current step t. Ac-\ncordingly, we write them as ¯rt(s, a) and pt(s′|s, a), re-\nspectively, and denote the (time-homogeneous) MDP at\nstep t by Mt = (S, A, ¯rt, pt, s1). We assume that all\nMDPs Mt are communicating with diameter Dt and de-\nnote by D a common upper bound on all Dt.\nObviously, in a nonstationary MDP the optimal policy\nin general will not be stationary anymore. We are in-\nterested in online regret bounds after any T steps taken\nby the learner. Accordingly, we consider the optimal ex-\npected T -step reward v∗\nT (s1) that can be achieved by any\n(time dependent) policy when starting in s1, and deﬁne\nthe regret after T steps as\nRT := v∗\nT (s1) −\nT\nX\nt=1\nrt.\nIf there are no changes, this basically corresponds to the\nstandard notion of regret as used e.g. by Jaksch et al.\n[2010] (apart from an additive constant of order D, cf.\nfootnote 1 ibid.). In the following, we assume that the\nrandom rewards rt are always bounded in [0, 1].\n2.1\nDEFINITION OF VARIATION\nWe consider individual terms for the variation in mean\nrewards and transition probabilities, that is,\nV r\nT\n:=\nT −1\nX\nt=1\nmax\ns,a\n\f\f¯rt+1(s, a) −¯rt(s, a)\n\f\f, and\nV p\nT\n:=\nT −1\nX\nt=1\nmax\ns,a\n\r\rpt+1(·|s, a) −pt(·|s, a)\n\r\r\n1.\nThese “local” variation measures can also be used to\nbound a more “global” notion of variation in average re-\nward deﬁned as\nVT :=\nT −1\nX\nt=1\n\f\fρ∗(Mt+1) −ρ∗(Mt)\n\f\f.\nTheorem 1. VT ≤V r\nT + DV p\nT .\nThe proof of Theorem 1 is given in Section 5.1 below.\nAs an example of Ortner et al. [2014a] shows, the bound\nof Theorem 1 is best possible.\nWhile VT is a more straightforward adaptation of the no-\ntion of variation of Besbes et al. [2014] from the bandit\nto the MDP setting, in the latter it seems more natural to\nwork with the local variation measures for rewards and\ntransition probabilities, as the learner does not have di-\nrect access to the average rewards of policies.\n3\nALGORITHM\nFor reinforcement learning in the changing MDP set-\nting, we propose Variation-aware UCRL (shown as Al-\ngorithm 1), which is based on the UCRL algorithm of\nJaksch et al. [2010].\nUCRL is an algorithm that is based on the idea of being\noptimistic in the face of uncertainty. It maintains esti-\nmates of rewards and transition probabilities (line 6) and\nemploys conﬁdence intervals to deﬁne a set M of MDPs\nthat are plausible with respect to the observations so far\n(line 7). When computing a new policy the algorithm\nchooses the policy ˜π and the MDP ˜\nM in M that give the\nhighest average reward (line 8). UCRL employs this op-\ntimistic policy ˜π until the state-action visits in some state\nhave doubled (lines 9–10), when a new policy is com-\nputed. The time intervals in which the policy is ﬁxed are\ncalled episodes.\nFor the changing MDP setting, we use adapted conﬁ-\ndence intervals (1) and (2) to account for the variation\nin rewards and transition probabilities. The arising al-\ngorithm basically corresponds to the colored UCRL2 al-\ngorithm suggested by Ortner et al. [2014b] for reinforce-\nment learning in MDPs with given similarities.\nWhile the regret of Variation-aware UCRL contains a\nterm that is linear in the number of steps (cf. Theorem 11\nbelow), we can obtain sublinear regret bounds by restart-\ning the algorithm according to a suitable scheme shown\nas Algorithm 2. Our restart schedule is optimized with\nrespect to the variation, as the regret bounds presented in\nthe next section will show. Note that the algorithm needs\n(upper bounds on) the local variations V r\nT and V p\nT as an\ninput. Alternatively, an upper bound on the global vari-\nation VT (replacing the term V r\nT + V p\nT in the algorithm)\ncould be used as well. As the bound of Theorem 1 is\nbest possible, this gives regret bounds that are worse by\na factor up to D1/3 however.\nThe idea of restarting UCRL in the changing MDP set-\nting has already been considered by Jaksch et al. [2010].\nWhen a bound L on the total number of changes is\nknown, then using a restart schedule adapted to L gave\nthe following regret bound.1\n1Jaksch et al. [2010] consider a slightly different notion of\nregret deﬁned as P\nt(ρ∗\nt −rt), where ρ∗\nt := ρ∗(Mt) is the\noptimal average reward at step t. However, when there are at\nmost L changes, the difference to our notion of regret is only\nof order LD.\nAlgorithm 1 Variation-aware UCRL\n1: Input: States S, actions A, conﬁdence parameter δ,\nvariation parameters ˜V r, ˜V p for rewards and transi-\ntion probabilities.\n2: Initialization: Set current time step t := 1.\n3: for episode k = 1, . . . do\n4:\nSet episode start tk := t.\n5:\nLet vk(s, a) denote the state-action counts for vis-\nits in current episode k, and Nk(s, a) be the counts\nfor visits before episode k.\n6:\nFor s, s′ ∈S and a ∈A, compute estimates\nˆrk (s, a) :=\nP\nτ rτ · 1sτ =s,aτ =a\nmax(1, Nk(s, a)) ,\nˆpk (s′|s, a) := #\n\b\nτ : sτ = s, aτ = a, sτ+1 = s′\t\nmax(1, Nk(s, a))\n.\nCompute policy ˜πk :\n7:\nLet Mk be the set of plausible MDPs ˜\nM with re-\nwards ˜r(s, a) and transition probabilities ˜p(·|s, a)\nsatisfying\n|˜r(s, a) −ˆrk(s, a)|\n≤˜V r +\nr\n8 log (8SAt3\nk/δ)\nmax (1,Nk(s,a)),\n(1)\n\r\r˜p(·|s, a) −ˆpk(·|s, a)\n\r\r\n1\n≤˜V p +\nr\n8S log (8SAt3\nk/δ)\nmax (1,Nk(s,a)) .\n(2)\n8:\nUse extended value iteration (see Section 3.1.2 of\nJaksch et al. [2010]) to ﬁnd a policy ˜πk and an op-\ntimistic MDP ˜\nMk ∈Mk such that\n˜ρk := ρ( ˜\nMk, ˜πk) =\nmax\nM′∈Mk ρ∗(M ′).\nExecute policy ˜πk:\n9:\nwhile vk(st, ˜πk(st)) < max(1, Nk(st, ˜πk(st)))\ndo\n10:\nChoose action at = ˜πk(st), obtain reward rt, and\nobserve st+1. Set t = t + 1.\n11: end for\nAlgorithm 2 Variation-aware UCRL with restarts\n1: Input: States S, actions A, conﬁdence parameter δ,\nvariation terms V r\nT and V p\nT .\n2: Initialization: Set current time step τ := 1.\n3: for phase i = 1, . . . do\n4:\nPerform UCRL with conﬁdence parameter δ/2τ 2\nfor θi :=\nl\ni2\n(V r\nT +V p\nT )2\nm\nsteps.\n5:\nSet τ = τ + θi.\n6: end for\nTheorem 2 (Jaksch et al. [2010]). In an MDP with\nat most L changes,\nafter T\nsteps the regret of\nUCRL restarted with conﬁdence parameter\nδ\nL2 at steps\n\u0006\ni3\n(L+1)2\n\u0007\nfor i = 1, 2, 3, . . . is upper bounded as\nRT ≤65 · (L + 1)1/3 T 2/3DS\nq\nA log\n\u0000 T\nδ\n\u0001\nwith probability of at least 1 −δ.\nNote that the restart schedule of Theorem 2 basically\ncorresponds to performing UCRL for ∼\ni2\nL2 steps for\ni = 1, 2, . . ., which is similar to our algorithm replac-\ning L by the variation term V r\nT + V p\nT .\n4\nMAIN RESULT\nThe following regret bound for Variation-aware UCRL\nwith restarts is our main result. The proof is given in the\nnext section.\nTheorem 3. After any T steps, the regret of the restart\nscheme for Variation-aware UCRL of Algorithm 2 is\nbounded as\nRT ≤74 · (V r\nT + V p\nT )1/3 T 2/3DS\nq\nA log\n\u0000 16S2AT 5\nδ\n)\nwith probability 1 −δ, provided that in each phase i the\nvariation parameters ˜V r\ni , ˜V p\ni\nare set to the respective\ntrue variation values for phase i.\nIf there are L changes, the bounds of Theorems 2 and 3\nare of the same order. On the other hand, the bound of\nTheorem 3 is better than the bound of Theorem 2, when\nL is large but the variation small as e.g. when having\nsmall gradual changes at any time step. Thus, Theorem 3\ncan be considered as an improvement over Theorem 2.\nWith respect to the variation and the horizon, our bound\nis optimal, as bounds of ˜O(V 1/3T 2/3) are already best\npossible in the bandit setting [Besbes et al., 2014].\n5\nANALYSIS\nWe start with some preliminaries. First, we introduce\nthe Poisson equation for the optimal policy in a com-\nmunicating MDP. That is, the mean rewards ¯r(s, π∗(s))\nunder an optimal policy π∗and the respective optimal\naverage reward ρ∗are related via the Poisson equation\nρ∗(M ′) −¯r(s, π∗(s)) = P\ns′ p′(s′|s, π∗(s)) · λ(s′) −\nλ(s), where λ is the so-called bias function for π∗, cf.\n[Puterman, 1994]. It holds that each λ(s) as well as the\nspan of the bias function Λ := maxs λ(s)−mins′ λ(s) is\nupper bounded by the diameter, cf. [Jaksch et al., 2010,\nBartlett and Tewari, 2009].\nWe will frequently make use of Azuma-Hoeffding in-\nequality, which we state here for convenience.\nLemma 4 (Azuma-Hoeffding inequality [Hoeffding,\n1963]). Let X1, X2, . . . be a martingale difference se-\nquence with |Xi| ≤c for all i. Then for all ǫ > 0 and\nn ∈N,\nP\nn\nn\nX\ni=1\nXi ≥ǫ\no\n≤exp\n\u0010\n−\nǫ2\n2nc2\n\u0011\n.\n5.1\nA PERTURBATION BOUND\nWe continue with establishing a perturbation bound on\nthe optimal average reward, which is a generalization of\nLemma 8 of Ortner et al. [2014a].\nLemma 5.\nAssume we have two MDPs M\n=\n(S, A, ¯rt, pt, s1), M ′ = (S, A, ¯r′, p′, s1) on the same\nstate and action space. The MDP M may be non-time-\nhomogeneous so that its mean rewards ¯rt and transition\nprobabilities pt are allowed to depend on time t. We\nassume that M ′ is time-homogeneous and communicat-\ning with optimal policy π′∗, such that for all steps t =\n1, . . . , T ,\nmax\ns\n\f\f¯rt(s, π′∗(s)) −¯r′(s, π′∗(s))\n\f\f ≤∆r\nt(s),\nmax\ns\n\r\rpt(·|s, π′∗(s)) −p′(·|s, π′∗(s))\n\r\r\n1 ≤∆p\nt (s).\nIf π′∗is performed on M for T steps, then denoting by st\nthe state visited at step t it holds that\nT ρ∗(M ′) −\nT\nX\nt=1\n¯rt(st, π′∗(st))\n≤\nT\nX\nt=1\n\u0000Λ′∆p\nt (st) + ∆r\nt(st)\n\u0001\n+\nT\nX\nt=1\n\u0010 X\ns′\npt(s′|st) · λ′(s′) −λ′(st)\n\u0011\n,\nwhere λ′ is the bias function and Λ′ the respective bias\nspan of π′∗on M ′.\nProof. The proof is a modiﬁcation of the proof of\nLemma 8 in Appendix A of [Ortner et al., 2014a].\nAbbreviating\n¯rt(s)\n:=\n¯rt(s, π′∗(s)),\n¯r′(s)\n:=\n¯r′(s, π′∗(s)) and pt(s′|s) := pt(s′|s, π′∗(s)), p′(s′|s) :=\np′(s′|s, π′∗(s)) in the following, we can write\nT ρ∗(M ′) −\nT\nX\nt=1\n¯rt(st) =\nT\nX\nt=1\n\u0000ρ∗(M ′) −¯rt(st)\n\u0001\n≤\nT\nX\nt=1\n\u0000ρ∗(M ′) −¯r′(st)\n\u0001\n+\nT\nX\nt=1\n\u0000¯r′(st) −¯rt(st)\n\u0001\n≤\nT\nX\nt=1\n\u0000ρ∗(M ′) −¯r′(st)\n\u0001\n+\nT\nX\nt=1\n∆r\nt(st).\n(3)\nFor bounding the ﬁrst term in (3) we use that by the\nPoisson equation for policy π′∗on M ′ we have that\nρ∗(M ′) −¯r′(s) = P\ns′ p′(s′|s) · λ′(s′) −λ′(s). Ac-\ncordingly, it holds that\nT\nX\nt=1\n\u0000ρ∗(M ′) −¯r′(st)\n\u0001\n=\nT\nX\nt=1\n\u0010 X\ns′\np′(s′|st) · λ′(s′) −λ′(st)\n\u0011\n≤\nT\nX\nt=1\n\u0010 X\ns′\npt(s′|st) · λ′(s′) −λ′(s′)\n\u0011\n+\nT\nX\nt=1\nX\ns′\n\u0000p′(s′|st) −pt(s′|st)\n\u0001\nλ′(st)\n≤\nT\nX\nt=1\n\u0010 X\ns′\npt(s′|st)·λ′(s′) −λ′(st)\n\u0011\n+\nT\nX\nt=1\nΛ′∆p\nt (st),\nwhence the lemma follows together with (3).\nFor the analysis of the last term in the bound of Lemma 5\nwe can use the following result, which is a simple gener-\nalization of a technique used by Jaksch et al. [2010].\nLemma 6. Consider some MDP M = (S, A, ¯r, p, s1)\nand let f : S →R be some function on the state\nspace of M. Then for any (possibly nonstationary) policy\nchoosing at each step τ action aτ in the current state sτ,\nit holds with probability at least 1 −δ,\nT\nX\nτ=1\n\u0010 X\ns′\np(s′|sτ, aτ) · f(s′) −f(sτ)\n\u0011\n≤F\nq\n2T log\n\u0000 1\nδ\n\u0001\n+ F,\nwhere F := maxs f(s) −mins f(s) is the span of f.\nProof. Following an argument due to Jaksch et al.\n[2010] we write\nT\nX\nτ=1\n\u0010 X\ns′\np(s′|sτ) · f(s′) −f(sτ)\n\u0011\n=\nT\nX\nτ=1\n\u0010 X\ns′\np(s′|sτ) · f(s′) −f(sτ+1)\n\u0011\n+ f(sT +1) −f(s1).\n(4)\nNow f(sT +1) −f(s1) ≤F, while the sum is a martin-\ngale difference sequence P\nτ Xτ with |Xτ| ≤F. The\nlemma follows by an application of Azuma-Hoeffding\n(Lemma 4).\nThe following corollary is a variant of Lemma 9 con-\ntained in the (unpublished) appendix of [Ortner et al.,\n2014a].2\nCorollary 7. For two communicating MDPs M, M ′ that\nsatisfy the assumptions of Lemma 5 for time and state\nindependent values ∆r, ∆p (i.e., ∆r\nt(s) ≤∆r and\n∆p\nt (s) ≤∆p for all s and all t) it holds that\nρ∗(M ′) −ρ∗(M) ≤Λ′∆p + ∆r.\nProof. From Lemmata 5 and 6 we have that with proba-\nbility 1 −δ\nρ∗(M ′) −1\nT\nT\nX\nt=1\n¯r(st, π′∗(st))\n≤Λ′∆p + ∆r + 1\nT\n\u0000Λ′p\n2T log (1/δ) + Λ′\u0001\n.\nChoosing δ = 1/T this yields for T →∞and taking\nexpectations that\nρ∗(M ′) −ρ∗(M) ≤ρ∗(M ′) −ρ(M, π′∗)\n≤Λ′∆p + ∆r.\nCorollary 7 allows to give the following quick proof of\nTheorem 1.\nProof of Theorem 1. Let ∆r\nt\n:= maxs,a |¯rt+1(s, a) −\n¯rt(s, a)| and ∆p\nt := ∥pt+1(·|s, a) −pt(·|s, a)∥1. Then\nby Corollary 7 and the assumption that the diameters of\nall Mt are bounded by D we have for t = 1, . . . , T −1\n|ρ∗(Mt+1) −ρ∗(Mt)| ≤D∆p\nt + ∆r\nt\nand Theorem 1 follows by summing over all t.\n5.2\nOPTIMISM\nWe show that the set of plausible MDPs with high proba-\nbility contains each MDP Mt the learner acts on in step t.\nThis is the theoretical justiﬁcation for optimism, as it will\nallow us to show in the next section that the true reward\ncan be upper bounded by the optimistic value ˜ρ.\nLemma 8. With probability 1 −5δ\n6 , the set M(t) of\nplausible MDPs computed at any time step t contains all\nMDPs Mτ for τ = 1, . . . , T .\nProof. The proof is similar to the handling of failing con-\nﬁdence intervals for the colored UCRL algorithm given\nin Appendix A.2 of Ortner et al. [2014b].\n2We note that a bound on the absolute value of the differ-\nence in average reward as stated in [Ortner et al., 2014a] will\nin general depend on the bias spans resp. the diameters of both\nMDPs, that is, the maximum of both values.\nFix a state-action pair (s, a), and let τ1, τ2, . . . be the\nNt(s, a) time steps at which action a has been chosen in\nstate s, i.e., (sτi, aτi) = (s, a) for all i. For the analy-\nsis of the transition probability estimates ˆpt computed at\nstep t we consider all vectors x indexed over the states\nwith entries ±1. Then writing x(s) for the entry in x\nwith index s we have\n\r\rˆpt(·|s, a) −E\n\u0002\nˆpt(·|s, a)\n\u0003\r\r\n1\n=\nX\ns′\n\f\f\fˆpt(s′|s, a) −E\n\u0002\nˆpt(s′|s, a)\n\u0003\f\f\f\n≤\nmax\nx∈{−1,1}S\nX\ns′\n\u0010\nˆpt(s′|s, a) −E\n\u0002\nˆpt(s′|s, a)\n\u0003\u0011\nx(s′)\n≤\nmax\nx∈{−1,1}S\n1\nNt(s, a)\nNt(s,a)\nX\ni=1\nXi(x),\nwhere we set\nXi(x) := x(sτi+1) −\nX\ns′\npτi(s′|sτi, aτi) x(s′).\nNow PNt(s,a)\ni\nXi(x) is a martingale difference se-\nquence with |Xi(x)| ≤2 for any ﬁxed x and ﬁxed\nNt(s, a) = n so that by Azuma-Hoeffding (Lemma 4)\nP\n( n\nX\ni=1\nXi(x) ≥\nq\n8Sn log\n\u0000 8SAt3\nδ\n\u0001\n)\n≤\nδ\n8SSAt3 .\nA union bound over all 2S vectors x and all possible val-\nues of Nt(s, a) shows that with probability 1 −\nδ\n4SAt2\n\r\rˆpt(·|s, a) −E\n\u0002\nˆpt(·|s, a)\n\u0003\r\r\n1 ≤\ns\n8S log(8SAt3/δ)\nmax(1, Nt(s, a)) .\n(5)\nFinally, note that for any ﬁxed Nt(s, a) we have\nE\n\u0002\nˆpt(·|s, a)\n\u0003\n=\n1\nNt(s, a)\nNt(s,a)\nX\ni=1\npτi(·|s, a),\nso that for all τ\n\r\rE\n\u0002\nˆpt(·|s, a)\n\u0003\n−pτ(·|s, a)\n\r\r\n1 ≤V p\nT ,\nwhich together with (5) shows that with probability 1 −\nδ\n4SAt2 the transition probabilities pτ(·|s, a) at each step τ\nare contained in the conﬁdence intervals (2) at step t.\nFor the rewards, ˆrt(s, a) −E\n\u0002\nˆrt(s, a)\n\u0003\nas well as\nE\n\u0002\nˆrt(s, a)\n\u0003\n−ˆrt(s, a) can be written as martingale dif-\nference sequences and Azuma-Hoeffding can be used\nto show that with probability 1 −\nδ\n4SAt2 , the rewards\n¯rτ(s, a) at each step τ are contained in the conﬁdence\nintervals (1) for step t.\nA union bound over all t and all state-action pairs con-\ncludes the proof, noting that P\nt\nδ\n2t2 ≤5δ\n6 .\nIn the following, we assume that the statement of\nLemma 8 holds, so that we need to consider the respec-\ntive error probability only once.\n5.3\nT -STEP VS. AVERAGE REWARD\nIn this section we consider the difference between the\noptimal T -step reward and the optimal average reward.\nFirst we recall the well-known fact that the optimal T -\nstep policy does not deviate by much from the opti-\nmal policy in respect to average reward, see e.g. Exer-\ncise 38.17 of Lattimore and Szepesvári [2019].\nLemma 9. Let M be a time-homogeneous and commu-\nnicating MDP with diameter D and rewards in [0, 1].\nFurther let v∗\nT (M, s) be the optimal T -step reward when\nstarting in state s. Then for any state s,\nv∗\nT (M, s) ≤T ρ∗(M) + D.\nAccordingly, the T -step reward in the changing MDP\nsetting can also bounded by the optimistic average re-\nward ˜ρ.\nLemma 10. Under the assumption of Lemma 8, for all k\nand all s,\nv∗\nT (s) ≤T ˜ρk + D.\nProof. Fix any k. As in Section 3.1 of [Jaksch et al.,\n2010] we consider the following extended MDP ˜\nM +\nk that\ncorresponds to the set of plausible MDPs Mk. That is,\nfor any state s in S the extended action set contains for\neach a in the original action set A, each value ˜r(s, a) in\n(1), and each transition probability distribution ˜p(·|s, a)\nin (2) a respective action with reward ˜r(s, a) and tran-\nsition probability distribution ˜p(·|s, a). By the assump-\ntion that Lemma 8 holds, the true values for rewards\nand transition probabilities at any step τ are contained in\nthese conﬁdence intervals, so that there is a nonstation-\nary T -step policy on ˜\nM +\nk whose expected T -step reward\nis v∗\nT (s). Therefore, for the optimal nonstationary T -step\nreward on ˜\nM +\nk , denoted by ˜vT , we have\n˜vT ≥v∗\nT (s).\n(6)\nFurther, as ˜\nM +\nk contains the original transition probabili-\nties of each Mτ, its diameter is bounded by D. Hence, by\nLemma 9, ˜vT ≤T ρ∗( ˜\nM +\nk ) + D. As ρ∗( ˜\nM +\nk ) = ˜ρk (cf.\nSection 3.1 of Jaksch et al. [2010]) this shows together\nwith (6) the lemma.\n5.4\nREGRET WITHOUT RESTARTS\nAs a next step, we derive the following regret bound\nfor Variation-aware UCRL without restarts (i.e. Algo-\nrithm 1).\nTheorem 11. If the variation parameters are set to their\ntrue values, that is, ˜V r := V r\nT and ˜V p := V p\nT , then after\nany T steps the regret of Variation-aware UCRL without\nrestarts (i.e., Algorithm 1) is upper bounded by\n32DS\nq\nAT log\n\u0000 8SAT 3\nδ\n\u0001\n+ 2T (V r\nT + DV p\nT )\nwith probability 1 −δ. This bound also holds when the\nalgorithm starts in an initial state s1 that is different from\nthe initial state s∗\n1 of the optimal T -step policy we com-\npare to.\nFor the proof we will use the following two basic\nfacts about UCRL (Proposition 18 and Lemma 19 of\nJaksch et al. [2010]), which can be derived from its\nepisode termination criterion. As we use the same crite-\nrion these results also hold for our variation-aware adap-\ntation.\nLemma 12. The number of episodes K of Variation-\naware UCRL after any T > SA steps is bounded by\nSA log2\n\u0000 T\nSA\n\u0001\n.\nLemma 13.\nX\ns,a\nK\nX\nk=1\nvk(s, a)\np\nmax(1, Nk(s, a))\n≤\n\u0000√\n2 + 1\n\u0001√\nSAT.\nProof of Theorem 11. First, let ˜ρmin := mink ˜ρk. Then\ndenoting the length of episode k by Tk := tk+1 −tk\n(setting tK+1 := T ), by Lemma 10 we have\nv∗\nT (s∗\n1) ≤T ˜ρmin + D ≤\nK\nX\nk=1\nTk ˜ρk + D.\n(7)\nFurther,\nanother\napplication\nof\nAzuma-Hoeffding\n(Lemma 4) shows that for the rewards rt collected by\nthe algorithm with probability 1 −δ\n12\nT\nX\nt=1\n\u0000¯r(st, at) −rt\n\u0001\n≤\nq\n2T log\n\u0000 12\nδ\n\u0001\n.\n(8)\nCombining (7) and (8) the regret is bounded as\nRT = v∗\nT (s∗\n1) −\nT\nX\nt=1\nrt\n= v∗\nT (s∗\n1) −\nT\nX\nt=1\n¯r(st, at) +\nT\nX\nt=1\n\u0000¯r(st, at) −rt\n\u0001\n≤\nK\nX\nk=1\n\u0010\nTk ˜ρk −\ntk+1−1\nX\nt=tk\n¯r(st, ˜πk(st))\n\u0011\n+\nq\n2T log\n\u0000 12\nδ\n\u0001\n+ D.\n(9)\nNow we are going to bound the term in the sum for each\nepisode k using Lemma 5. Indeed, we perform the opti-\nmal policy ˜πk of the optimistic MDP ˜\nMk with average\nreward ˜ρk on the underlying (non-time-homogeneous)\ntrue MDP, and the rewards ˜rk and the transition proba-\nbilities ˜pk of ˜\nMk satisfy the conﬁdence intervals (1) and\n(2) according to Lemma 8 so that\n\f\f¯rt(s, a) −˜rk(s, a)\n\f\f\n≤\n\f\f¯rt(s, a) −ˆrk(s, a)\n\f\f +\n\f\fˆrk(s, a) −˜rk(s, a)\n\f\f\n≤V r\nT + ˜V r + 2\nr\n8 log (8SAt3\nk/δ)\nmax (1,Nk(s,a))\nas well as\n\r\rpt(·|s, a) −˜pk(·|s, a)\n\r\r\n1\n≤\n\r\rpt(·|s, a) −ˆpk(·|s, a)\n\r\r\n1+\n\r\rˆpk(·|s, a) −˜pk(·|s, a)\n\r\r\n1\n≤V p\nT + ˜V p + 2\nr\n8S log (8SAt3\nk/δ)\nmax (1,Nk(s,a)) .\nBy assumption ˜V r = V r\nT and ˜V p = V p\nT , and Lemma 5\ngives\nTk ˜ρk −\ntk+1−1\nX\nt=tk\n¯r(st, ˜πk(st))\n≤2˜Λk\ntk+1−1\nX\nt=tk\n\u0010\nV p\nT +\nr\n8S log (8SAt3\nk/δ)\nmax (1,Nk(st,˜πk(st)))\n\u0011\n(10)\n+ 2\ntk+1−1\nX\nt=tk\n\u0010\nV r\nT +\nr\n8 log (8SAt3\nk/δ)\nmax (1,Nk(st,˜πk(st)))\n\u0011\n(11)\n+\ntk+1−1\nX\nt=tk\n\u0010 X\ns′\npt(s′|st, ˜πk(st)) · ˜λk(s′) −˜λk(st)\n\u0011\n,\n(12)\nwhere ˜λk is the bias function of ˜πk on ˜\nMk and ˜Λk is\nthe respective bias span. Since by Lemma 8 the set of\nplausible MDPs contains each MDP Mt and the diameter\nof each Mt is bounded by D, the bias span ˜Λk ≤D, cf.\n[Jaksch et al., 2010, Bartlett and Tewari, 2009].3\nAccordingly, the sum of (10) and (11) over all episodes\nis bounded by Lemma 13 as\n≤2T (DV p\nT + V r\nT )\n+ 2(D + 1)\nq\n8S log\n\u0000 8SAT 3\nδ\n\u0001 X\ns,a\nK\nX\nk=1\nvk(s,a)\n√\nmax (1,Nk(s,a))\n≤2T (DV p\nT + V r\nT )\n3Note that for the argument it would be sufﬁcient if just one\nof the MDPs Mt is plausible. However, for the restart scheme\nof Algorithm 2 we would need a plausible Mt in each phase.\n+ 2\n\u0000√\n2 + 1\n\u0001\n(D + 1)S\nq\n8AT log\n\u0000 8SAT 3\nδ\n\u0001\n.\n(13)\nThe sum in (12) can be written as in the proof of\nLemma 6 as\ntk+1−1\nX\nt=tk\n\u0010 X\ns′\npt(s′|st, ˜πk(st)) · ˜λk(s′) −˜λk(st)\n\u0011\n=\ntk+1−1\nX\nt=tk\n\u0010 X\ns′\npt(s′|st, ˜πk(st)) · ˜λk(s′) −˜λk(st+1)\n\u0011\n+ ˜λk(stk+1) −˜λk(stk).\nNow ˜λk(stk+1) −˜λk(stk) ≤D, so that summing over\nall episodes gives by Azuma Hoeffding (Lemma 4) and\nLemma 12 that with probability 1 −δ\n12\nK\nX\nk=1\ntk+1−1\nX\nt=tk\n\u0010 X\ns′\npt(s′|st, ˜πk(st)) · ˜λk(s′) −˜λk(st)\n\u0011\n≤\nT\nX\nt=1\n\u0010 X\ns′\npt(s′|st, ˜πk(st)) · ˜λk(t)(s′) −˜λk(t)(st+1)\n\u0011\n+ KD\n≤D\nq\n2T log\n\u0000 δ\n12\n\u0001\n+ DSA log2\n\u0000 T\nSA\n\u0001\n,\n(14)\nwhere k(t) denotes the episode in which time step t oc-\ncurs.\nThus, combining (9)–(14) taking into account the error\nprobabilities for (8), (14), and Lemma 8, we yield that\nwith probability 1 −δ the regret is bounded by\nRT ≤\nq\n2T log\n\u0000 12\nδ\n\u0001\n+ D + 2T (DV p\nT + V r\nT )\n+ 2\n\u0000√\n2 + 1\n\u0001\n(D + 1)S\nq\n8AT log\n\u0000 8SAT 3\nδ\n\u0001\n.\n+ D\nq\n2T log\n\u0000 12\nδ\n\u0001\n+ DSA log2\n\u0000 T\nSA\n\u0001\nand some simpliﬁcations analogous to Appendix C.4 of\nJaksch et al. [2010] give the claimed regret bound.\n5.5\nPROOF OF THEOREM 3\nFinally, we are ready to give the proof of the regret\nbound for the restart scheme of Algorithm 2. Abusing\nnotation we write V r\ni\nand V p\ni\nfor the variation of re-\nwards and transition probabilities in phase i and abbrevi-\nate Vi := V r\ni + V p\ni , V := V r\nT + V p\nT and θi :=\n\u0006 i2\nV 2\n\u0007\n.\nFirst, let us bound the number of phases N. Obviously,\nstep T is reached in phase N when\nN−1\nX\ni=1\nl i2\nV 2\nm\n< T ≤\nN\nX\ni=1\nl i2\nV 2\nm\n.\nRecalling that PN\ni=1 i2 = 1\n6N(N + 1)(2N + 1) > 1\n3N 3\nwe obtain\nT >\nN−1\nX\ni=1\nl i2\nV 2\nm\n>\nN−1\nX\ni=1\ni2\nV 2 > (N −1)3\n3V 2\n,\nso that the number of phases is bounded as\nN < 1 +\n3√\n3V 2T.\n(15)\nWriting τi for the initial step of phase i and s∗\nτi for the\n(random) state visited by the optimal T -step policy at\nstep τi, we can decompose the regret as\nv∗\nT (s1) −\nT\nX\nt=1\nrt =\nN\nX\ni=1\n\u0010\nE\n\u0002\nv∗\nθi(s∗\nτi)\n\u0003\n−\nτi−1\nX\nt=τi\nrt\n\u0011\n. (16)\nBy Theorem 11 and a union bound over all possible val-\nues for state s∗\nτi, the i-th summand (i = 1, . . . , N) in\n(16) with probability 1 −\nδ\n2τ 2\ni is bounded by\n32DS\nq\nA log\n\u0000 16S2AT 5\nδ\n\u0001\n·\np\nθi + 2DVi · θi.\nIf\n3√\n3V 2T < 1, then we also have 3V 2T < 1 and hence\n3V 2T 2 < T , so that\nV T <\n√\n3 · V T <\n√\nT .\nFurther, in this case by (15) we have N = 1 with θ1 = T\nand V1 = V , so that the regret is bounded by\n32DS\nq\nA log\n\u0000 16S2AT 5\nδ\n\u0001\n·\n√\nT + 2DV T\n<\n\u0010\n32DS\nq\nA log\n\u0000 16S2AT 5\nδ\n\u0001\n+ 2D\n\u0011\n·\n√\nT,\nwhich is upper bounded by the claimed regret bound.\nOn the other hand, if\n3√\n3V 2T ≥1, then N < 2\n3√\n3V 2T\nfrom (15) and summing over all N phases yields from\n(16) that with error probability P\ni\nδ\n2τ 2\ni < P\nt\nδ\n2t2 < δ\nthe regret is bounded by\n32DS\nq\nA log\n\u000016S2AT 5\nδ\n\u0001\n·\nN\nX\ni=1\np\nθi + 2D\nN\nX\ni=1\nVi\n\u0010 i2\nV 2 +1\n\u0011\n.\nNoting that using Jensen’s inequality\nN\nX\ni=1\np\nθi ≤\n√\nNT ≤1.7 · V 1/3T 2/3\nand that also\nN\nX\ni=1\nVi\n\u0010 i2\nV 2 + 1\n\u0011\n≤\nN\nX\ni=1\nVi\n\u0010N 2\nV 2 + 1\n\u0011\n≤N 2\nV\n+ V\n< 8.33 · V 1/3T 2/3 + V,\nconcludes the proof, noting that the claimed bound holds\ntrivially if V ≥T , so that we may assume that V < T\nand hence V < V 1/3T 2/3.\n6\nDISCUSSION AND EXTENSIONS\nThe regret bound of Theorem 11 relies on the assumption\nthat the variation for rewards and transition probabilities\nare known in advance. Accordingly it is necessary for the\nrestart scheme to know the respective variation terms for\neach single phase. It is easy to check that if upper bounds\non these values are used to set ˜V r and ˜V p instead, the\nregret bounds of Theorems 2 and 11 simply depend on\nthese upper bounds instead of the true values.\nIn principle, it is also possible to set the variation parame-\nters ˜V r and ˜V p in Algorithm 1 to 0. Then Lemma 8 need\nnot hold anymore, that is, it is not guaranteed that the set\nof plausible MDPs contains any of the MDPs Mt. Ac-\ncordingly, we cannot rely on Lemma 10 anymore, which\nis based on Lemma 8 and guarantees that the optimistic\naverage reward is an upper bound on the true reward.\nHowever, taking into account the true variation one can\nstill establish an upper bound on the true reward.\nLemma 14. Let ˜ρ be the optimistic average reward com-\nputed when using the true variations V r\nT and V p\nT and\n˜ρ0 be the optimistic average reward computed with vari-\nation parameters ˜V r = 0 and ˜V p = 0. Then\n˜ρ ≤˜ρ0 + V r\nT + DV p\nT .\nProof. The rewards and transition probabilities of the re-\nspective optimistic MDPs ˜\nM and ˜\nM 0 with ˜ρ = ρ∗( ˜\nM)\nand ˜ρ0 = ρ∗( ˜\nM 0) differ by at most V r\nT and V p\nT , respec-\ntively. Hence the claim of the lemma follows by Corol-\nlary 7, recalling that the bias span of the optimal policy\nin ˜\nM is bounded by the diameter D, cf. the proof of The-\norem 11.\nThus, in principle one could use Lemma 14 to replace\nLemma 10 in the proof of Theorem 11. It is easy to check\nthat this works ﬁne except for the second application of\nLemma 8 used to show that the bias span ˜Λ is bounded\nby D. Indeed, the set of plausible MDPs M0 computed\nwith ˜V r = 0 and ˜V p = 0 need not contain any of the\nMDPs Mt and hence there is no guarantee that it contains\nan MDP with diameter bounded by D.\nStill it is possible to obtain an alternative bound on the\nbias span by observing that M0 contains with high prob-\nability an MDP where for each state-action pair (s, a)\nthere is a subset T of {1, 2, . . ., T } such that the transi-\ntion probabilities under (s, a) are of the form\np(·|s, a) =\n1\n|T |\nX\nt∈T\npt(·|s, a).\n(17)\nThe intution here is that the set T corresponds to the time\nsteps when a sample for the state-action pair (s, a) has\nbeen taken. Let\nˆ\nM be the set of MDPs with transition\nprobabilities of the form speciﬁed in (17). Then deﬁning\nˆD := max\nM∈ˆ\nM\nD(M)\nto be the maximal diameter over all MDPs in\nˆ\nM, one\nhas ˜Λ ≤ˆD and the following regret bound holds when\nsetting the variation parameters to 0.\nTheorem 15. With probability 1 −δ, the regret of\nVariation-aware UCRL with variation parameters set\nto 0 is upper bounded by\nRT ≤32 ˆDS\nq\nAT log\n\u0000 8SAT 3\nδ\n) + 2T (V r\nT + DV p\nT ).\nUsing Theorem 15, one can derive the following regret\nbound for the restart scheme of Algorithm 2. Note that\nthe restart scheme still needs V r\nT + V p\nT as input.\nTheorem 16. With probability 1 −δ, the regret of the\nrestart scheme of Algorithm 2 with variation parameters\nset to 0 in each phase is bounded by\nRT ≤74 · (V r\nT + V p\nT )1/3 T 2/3 ˆDS\nq\nA log\n\u0000 16S2AT 5\nδ\n).\nSimilar bounds obviously also hold when the variation\nparameters are set to any value smaller than the true vari-\nation. Note that D ≤ˆD, as the set ˆ\nM also contains the\nMDPs Mt for each t. However, the following example\nshows that ˆD in general cannot be bounded by D and in\nsome even simple cases can be unbounded.\nExample. Consider two MDPs M1, M2 over the same\nstate space {s, s′} and the same action space {a, a′}.\nIn M1 the nonzero transition probabilities are given by\np1(s|s′, a) = 1, p1(s′|s, a) = 1/D, p1(s|s, a) =\n1 −1/D, and p1(s|s, a′) = p1(s′|s′, a′) = 1.\nIn\nM2 the roles of a and a′ are swapped, that is, we\nhave p2(s|s, a) = p2(s′|s′, a) = 1, p2(s|s′, a′) =\n1, p2(s′|s, a′)\n=\n1/D, and p2(s|s, a′)\n=\n1 −\n1/D. Obviously, both MDPs have diameter D. How-\never, the MDP M with nonzero transition probabilities\np(s|s, a) := p2(s|s, a) = 1, p(s′|s′, a) := p2(s′|s′, a) =\n1, p(s|s, a′) := p1(s|s, a′) = 1, and p(s′|s′, a′) :=\np1(s′|s′, a′) = 1 is contained in\nˆ\nM, but does not have\nﬁnite diameter, as the states s, s′ are not connected.\nTo conclude, we note that recently variational bounds\nfor the (contextual) bandit setting have been derived also\nfor the case when the variation is unknown [Chen et al.,\n2019]. Achieving such bounds in our setting seems not\neasy, as sampling a particular state-action pair usually\ncauses some transition costs.\nAcknowledgements.\nThis work has been supported\nby the Austrian Science Fund (FWF): I 3437-N33 in\nthe framework of the CHIST-ERA ERA-NET (DELTA\nproject).\nReferences\nYasin Abbasi, Peter L Bartlett, Varun Kanade, Yevgeny\nSeldin, and Csaba Szepesvári.\nOnline learning in\nMarkov decision processes with adversarially cho-\nsen transition probability distributions. In Advances\nin Neural Information Processing Systems 26, pages\n2508–2516, 2013.\nPeter L. Bartlett and Ambuj Tewari. REGAL: A regu-\nlarization based algorithm for reinforcement learning\nin weakly communicating MDPs. In Proceedings of\nthe 25th Conference on Uncertainty in Artiﬁcial Intel-\nligence, UAI 2009, pages 25–42, 2009.\nDimitri P. Bertsekas and John N. Tsitsiklis.\nNeuro-\nDynamic Programming. Athena Scientiﬁc, 1st edition,\n1996.\nOmar Besbes, Yonatan Gur, and Assaf Zeevi. Stochastic\nmulti-armed-bandit problem with non-stationary re-\nwards. In Advances in Neural Information Processing\nSystems 27, pages 199–207, 2014.\nYifang Chen, Chung-Wei Lee, Haipeng Luo, and Chen-\nYu Wei. A new algorithm for non-stationary contex-\ntual bandits: Efﬁcient, optimal, and parameter-free. In\nProceedings of the 32nd Conference On Learning The-\nory, COLT 2019, 2019.\nTravis Dick, András György, and Csaba Szepesvári.\nOnline learning in Markov decision processes with\nchanging cost sequences. In Proceedings of the In-\nternational Conference on Machine Learning, ICML\n2014, pages 512–520, 2014.\nEyal Even-Dar, Sham M Kakade, and Yishay Mansour.\nExperts in a Markov decision process. In Advances\nin Neural Information Processing Systems 17, pages\n401–408, 2005.\nWassily Hoeffding. Probability inequalities for sums of\nbounded random variables. Journal of the American\nStatistical Association, 58(301):13–30, 1963.\nThomas Jaksch, Ronald Ortner, and Peter Auer. Near-\noptimal regret bounds for reinforcement learning.\nJournal of Machine Learning Research, 11:1563–\n1600, August 2010.\nTor Lattimore and Csaba Szepesvári. Bandit Algorithms.\nCambridge University Press, 2019. to appear.\nArnab Nilim and Laurent El Ghaoui.\nRobust control\nof Markov decision processes with uncertain transi-\ntion matrices. Operations Research, 53(5):780–798,\nSeptember 2005.\nRonald Ortner, Odalric-Ambrym Maillard, and Daniil\nRyabko.\nSelecting near-optimal approximate state\nrepresentations in reinforcement learning. In Algorith-\nmic Learning Theory – 25th International Conference,\nALT 2014, pages 140–154, 2014a.\nRonald Ortner, Daniil Ryabko, Peter Auer, and Rémi\nMunos. Regret bounds for restless Markov bandits.\nTheoretical Computer Science, 558:62–76, 2014b.\nMartin L. Puterman. Markov Decision Processes: Dis-\ncrete Stochastic Dynamic Programming. Wiley, New\nYork, 1994.\nJia Yuan Yu and Shie Mannor.\nArbitrarily modulated\nMarkov decision processes.\nIn Proceedings of the\nIEEE Conference on Decision and Control, pages\n2946–2953, 2009a.\nJia Yuan Yu and Shie Mannor. Online learning in Markov\ndecision processes with arbitrarily changing rewards\nand transitions. In 2009 International Conference on\nGame Theory for Networks, pages 314–322, 2009b.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-05-14",
  "updated": "2019-09-10"
}