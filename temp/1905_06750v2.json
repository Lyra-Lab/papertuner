{
  "id": "http://arxiv.org/abs/1905.06750v2",
  "title": "Random Expert Distillation: Imitation Learning via Expert Policy Support Estimation",
  "authors": [
    "Ruohan Wang",
    "Carlo Ciliberto",
    "Pierluigi Amadori",
    "Yiannis Demiris"
  ],
  "abstract": "We consider the problem of imitation learning from a finite set of expert\ntrajectories, without access to reinforcement signals. The classical approach\nof extracting the expert's reward function via inverse reinforcement learning,\nfollowed by reinforcement learning is indirect and may be computationally\nexpensive. Recent generative adversarial methods based on matching the policy\ndistribution between the expert and the agent could be unstable during\ntraining. We propose a new framework for imitation learning by estimating the\nsupport of the expert policy to compute a fixed reward function, which allows\nus to re-frame imitation learning within the standard reinforcement learning\nsetting. We demonstrate the efficacy of our reward function on both discrete\nand continuous domains, achieving comparable or better performance than the\nstate of the art under different reinforcement learning algorithms.",
  "text": "Random Expert Distillation: Imitation Learning via Expert Policy\nSupport Estimation\nRuohan Wang1\nCarlo Ciliberto1\nPierluigi Amadori1\nYiannis Demiris1\n{r.wang16,c.ciliberto,p.amadori,y.demiris}@imperial.ac.uk\nJune 10, 2019\nAbstract\nWe consider the problem of imitation learning from a ﬁnite set of expert trajectories, without\naccess to reinforcement signals. The classical approach of extracting the expert’s reward function\nvia inverse reinforcement learning, followed by reinforcement learning is indirect and may be\ncomputationally expensive. Recent generative adversarial methods based on matching the policy\ndistribution between the expert and the agent could be unstable during training. We propose a\nnew framework for imitation learning by estimating the support of the expert policy to compute\na ﬁxed reward function, which allows us to re-frame imitation learning within the standard\nreinforcement learning setting. We demonstrate the eﬃcacy of our reward function on both discrete\nand continuous domains, achieving comparable or better performance than the state of the art\nunder diﬀerent reinforcement learning algorithms.\n1\nIntroduction\nWe consider a speciﬁc setting of imitation learning - the task of policy learning from expert demonstra-\ntions - in which the learner only has a ﬁnite number of expert trajectories without any further access to\nthe expert. Two broad categories of approaches to this settings are behavioral cloning (BC) Pomerleau\n(1991), which directly learns a policy mapping from states to actions with supervised learning from\nexpert trajectories; and inverse reinforcement learning (IRL) Ng & Russell (2000); Abbeel & Ng\n(2004), which learns a policy via reinforcement learning, using a cost function extracted from expert\ntrajectories.\nMost notably, BC has been successfully applied to the task of autonomous driving Bojarski et al.\n(2016); Bansal et al. (2018). Despite its simplicity, BC typically requires a large amount of training\ndata to learn good policies, as it may suﬀer from compounding errors caused by covariate shift Ross\n& Bagnell (2010); Ross et al. (2011). BC is often used as a policy initialization step for further\nreinforcement learning Nagabandi et al. (2018); Rajeswaran et al. (2017).\nIRL estimates a cost function from expert trajectories and uses reinforcement learning to derive\npolicies. As the cost function evaluates the quality of trajectories rather than that of individual actions,\nIRL avoids the problem of compounding errors. IRL is eﬀective with a wide range of problems, from\ncontinuous control benchmarks in the Mujoco environment Ho & Ermon (2016), to robot footsteps\nplanning Ziebart et al. (2008).\nGenerative Adversarial Imitation Learning (GAIL) Ho & Ermon (2016); Baram et al. (2017)\nconnects IRL to the general framework of Generative Adversarial Networks (GANs) Goodfellow et al.\n1Department of Electrical and Electronic Engineering, Imperial College London, SW7 2BT, United Kingdom\n1\narXiv:1905.06750v2  [cs.LG]  7 Jun 2019\n(2014), and re-frames imitation learning as distribution matching between the expert policy and the\nlearned policy via Jensen-Shannon Divergence. However, GAIL naturally inherits the challenges\nof GAN training, including possible training instability, and overﬁtting to training data Arjovsky\n& Bottou (2017); Brock et al. (2018). Similarly, generative moment matching imitation learning\n(GMMIL) considers distribution matching via maximum mean discrepancy. Both GAIL and GMMIL\niteratively update the reward function and the learned policy during training.\nIn this paper, we propose imitation learning via expert policy support estimation, a new general\nframework for recovering a reward function from expert trajectories. We propose Random Expert\nDistillation (RED) by providing a connection between Random Network Distillation (RND) Burda\net al. (2018) – a method to design intrinsic rewards for RL exploration based on the \"novelty\" of\nstates visited – and support estimation ideas. Our method computes a ﬁxed reward function from\nexpert trajectories, and obviates the need for dynamic update of reward functions found in classical\nIRL, GAIL and GMMIL. Evaluating RED using diﬀerent reinforcement learning algorithms on both\ndiscrete and continuous domains, we show that the proposed method achieves comparable or better\nperformance than the state-of-arts methods on a variety of tasks, including a driving scenario (see\nFigure 4b). To the best of our knowledge, our method is the ﬁrst to explore expert policy support\nestimation for imitation learning.\n2\nBackground\nWe review the formulation of Markov Decision Process (MDP) in the context of which we consider\nimitation learning. We also review previous approaches to imitation learning and support estimation\nrelated to our proposed method.\nSetting. We consider an inﬁnite-horizon discounted MDP, deﬁned by the tuple (S, A, P, r, p0, γ),\nwhere S is the set of states, A the set of actions, P : S × A × S →[0, 1] the transition probability,\nr : S × A →R the reward function, p0 : S →[0, 1] the distribution over initial states, and γ ∈(0, 1)\nthe discount factor. Let π be a stochastic policy π : S × A →[0, 1] with expected discounted reward\nEπ(r(s, a)) ≜E(P∞\nt=0 γtr(st, at)) where s0 ∼p0, at ∼π(·|st), and st+1 ∼P(·|st, at) for t ≥0. We\ndenote πE the expert policy.\n2.1\nImitation Learning\nWe brieﬂy review the main methods for imitation learning:\nBehavioral Cloning (BC) learns a control policy π : S →A directly from expert trajectories via\nsupervised learning. Despite its simplicity, BC is prone to compounding errors: small mistakes in\nthe policy cause the agent to deviate from the state distribution seen during training, making future\nmistakes more likely. Mistakes accumulate, leading to eventual catastrophic errors Ross et al. (2011).\nWhile several strategies have been proposed to address this Ross & Bagnell (2010); Sun et al. (2017),\nthey often require access to the expert policy during the entire training process, rather than a ﬁnite\nset of expert trajectories. BC is commonly used to initialize control policies for reinforcement learning\nRajeswaran et al. (2017); Nagabandi et al. (2018).\nInverse Reinforcement Learning (IRL) models the expert trajectories with a Boltzmann distribution\n2\nZiebart et al. (2008), where the likelihood of a trajectory is deﬁned as:\npθ(τ) = 1\nZ exp(−cθ(τ))\n(1)\nwhere τ = {s1, a1, s2, a2...} is a trajectory, cθ(τ) = P\nt cθ(st, at) a learned cost function parametrized\nby θ, and the partition function Z ≜\nR\nexp(−cθ(τ))d(τ) the integral over all trajectories consistent with\ntransition function of the MDP. The main computational challenge of IRL is the eﬃcient estimation\nthe partition function Z. IRL algorithms typically optimize the cost function by alternating between\nupdating the cost function and learning an optimal policy with respect to the current cost function\nwith reinforcement learning Abbeel & Ng (2004); Ng & Russell (2000).\nImitation Learning via Distribution Matching frames imitation learning as distribution matching\nbetween the distribution of state-action pairs of the expert policy and that of the learned policy. In Ho\n& Ermon (2016); Finn et al. (2016), the authors connect IRL to distribution matching via Generative\nAdversarial Networks (GANs) Goodfellow et al. (2014). Known as Generative Adversarial Imitation\nLearning (GAIL), the method imitates an expert policy by formulating the following minimax game:\nmin\nπ\nmax\nD∈(0,1) Eπ(log D(s, a)) + EπE(log(1 −D(s, a))) −λH(π)\n(2)\nwhere the expectations Eπ and EπE denote the joint distributions over state-action pairs of the learned\npolicy and the expert policy, respectively, and H(π) ≜Eπ(−log π(a|s)) is the entropy of the learned\npolicy. GAIL has been successfully applied to various control tasks in the Mujoco environment Ho &\nErmon (2016); Baram et al. (2017). However, GAIL inherits the challenges of GANs, including possible\ntraining instability such as vanishing or exploding gradients, as well as overﬁtting to training data\nArjovsky & Bottou (2017); Brock et al. (2018). While numerous theoretical and practical techniques\nhave been proposed to improve GANs (e.g Arjovsky et al. (2017); Salimans et al. (2016)), a large-scale\nstudy of GANs show that many GAN algorithms and architectures achieve similar performance with\nsuﬃcient hyperparameter tuning and random restarts, and no algorithm or network architecture\nstands out as the clear winner on all tasks Lucic et al. (2018).\nSimilar to GAIL, Kim & Park (2018) proposed generative moment matching imitation learning\n(GMMIL) by minimizing the maximum mean discrepancy between the expert policy and the learned\npolicy. Though GMMIL avoids the diﬃcult minimax game, the cost of each reward function evaluation\ngrows linearly with the amount of training data, which makes scaling the method to large dataset\npotentially diﬃcult. In addition, we demonstrate in our experiments that GMMIL may fail to estimate\nthe appropriate reward functions.\n2.2\nSupport Estimation with Kernel Methods\nAs we will motivate in detail in Sec. 3, we argue that estimating the support of the expert policy\ncan lead to good reward functions for imitation learning. In this section we review one of the most\nwell-established approaches to support estimation of a distribution from a ﬁnite number of i.i.d.\nsamples, which relies on a kernelized version of principal component analysis Schölkopf et al. (1998).\nThe idea is to leverage the Separating Property De Vito et al. (2014) of suitable reproducing kernel\nHilbert spaces, which guarantees the covariance operator of the embedded data to precisely capture\nthe geometrical properties of the support of the underlying distribution. This allows us to derive a\ntest function that is zero exclusively on points belonging to the support of the distribution.\n3\nFormally, let X ⊆Rd be a set (in our setting X = S × A is the joint state-action space) and let\nk : X × X →R be a positive deﬁnite kernel with associated reproducing kernel Hilbert space (RKHS)\nH Aronszajn (1950) and feature map φ : X →H, such that k(x, x′) = ⟨φ(x), φ(x′)⟩H for any x, x′ ∈X.\nFor any set U ⊆X, denote by Φ(U) = {φ(x) | x ∈U} and Φ(U) the closure of its span in H. The\nseparating property guarantees that for any closed subset U of X, Φ(U) = Φ(X) ∩Φ(U). In other\nwords, the separating property ensures that\nx ∈U\n⇐⇒\nφ(x) ∈Φ(U).\n(3)\nAs shown in De Vito et al. (2014), several kernels allow the separating property to hold, including the\npopular Gaussian kernel k(x, x′) = exp(−∥x −x′∥/σ) for any σ > 0.\nThe separating property suggests a natural strategy to test whether a point x ∈X belongs to a\nclosed set U. Let PU : H →H be the orthogonal projection operator onto Φ(U) (which is a linear\noperator since Φ(U) is a linear space), we have\nx ∈U\n⇐⇒\n∥(I −PU)φ(x)∥H = 0,\nsince Eq. (3) corresponds to φ(x) = PUφ(x), or equivalently (I −PU)φ(x) = 0.\nWe can leverage the machinery introduced above to estimate the support supp(π) ⊆X of a\nprobability distribution π on X. We observe that the covariance operator Cπ = Eπ[φ(x)φ(x)⊤] encodes\nsuﬃcient information to recover the orthogonal projector Psupp(π) (denoted Pπ in the following for\nsimplicity). More precisely, let C†\nπ denote the pseudoinverse of Cπ. A direct consequence of the\nseparating property is that Pπ = C†\nπCπ De Vito et al. (2014).\nWhen π is unknown and observed only through a ﬁnite number of N i.i.d examples {xi}N\ni=1, it\nis impossible to obtain Cπ (and thus Pπ) exactly. A natural strategy is to consider the empirical\ncovariance operator ˆC =\n1\nN\nPN\ni=1 φ(xi)φ(xi)⊤as a proxy of the ideal one. Then, the projector is\nestimated as ˆP = ˆC†\nm ˆCm, where Cm is the operator comprising the m ≤n principal component\ndirections of C, where m is a hyperparameter to control the stability of the pseudoinverse when\ncomputing ˆP. We can then test whether a point x ∈X belongs to supp(π), by determining if\n\r\r\r(I −ˆP)φ(x)\n\r\r\r\n2\nH =\nD\nφ(x), (I −ˆP)φ(x)\nE\nH = k(x, x) −\nD\nφ(x), ˆPφ(x)\nE\nH ,\n(4)\nis greater than zero (in practice a threshold τ > 0 is introduced). Note that we have used the fact\nthat ˆP ⊤ˆP = ˆP, since ˆP is an orthogonal projector. Although ˆC and ˆP are operators between possibly\ninﬁnite dimensional spaces, we have for any x ∈X (see Schölkopf et al., 1998)\nD\nφ(x), ˆPφ(x)\nE\nH = K⊤\nx K†\nmKx,\nwhere K ∈Rn×n is the empirical kernel matrix of the training examples, with entries Kij = k(xi, xj).\nHere, Km denotes the matrix obtained by performing PCA on K and taking the ﬁrst m ≤n principal\ndirections and Kx ∈Rn is the vector with entries (Kx)i = k(xi, x). Therefore ⟨φ(x), ˆPφ(x)⟩H can be\ncomputed eﬃciently in practice.\nThe theoretical properties of the strategy described above have been thoroughly investigated in\nthe previous literature De Vito et al. (2014); Rudi et al. (2017). In particular, it has been shown\nthat the Hausdorﬀdistance between supp(π) and the set induced by ˆP, tends to zero when n →+∞,\nprovided that the number of principal components m grows with the number of training examples.\nMoreover, it has been shown that the support estimator enjoys fast learning rate under standard\nregularity assumptions.\n4\n3\nImitation Learning via Expert Policy Support Estimation\nFormally, we are interested in extracting a reward function ˆr(s, a) from a ﬁnite set of trajectories\nD = {τi}N\ni=1 produced by an expert policy πE within a MDP environment. Here each τi is a trajectory\nof state-action pairs of the form τi = {s1, a1, s2, a2, ..., sT , aT }. Assuming that the expert trajectories\nare consistent with some unknown reward function r∗(s, a), our goal is for a policy learned by applying\nRL to ˆr(s, a), to achieve good performance when evaluated against r∗(s, a) (the standard evaluation\nstrategy for imitation learning). In contrast with traditional imitation learning, we do not explicitly\naim to match πE exactly.\nWe motivate our approach with a thought experiment. Given a discrete action space and a\ndeterministic expert such that a∗\ns = πE(s), the resulting policy is a Dirac’s delta at each state s ∈S,\nfor all (s, a). Consider the reward function\nrE(s, a) =\n(\n1 if (s, a) ∈supp(πE)\n0 if (s, a) /∈supp(πE)\n(5)\nwhich corresponds to the indicator function of supp(πE). It follows that a RL agent trained with this\nreward function would be able to imitate the expert exactly, since the discrete action space allows the\nrandom exploration of the agent to discover optimal actions at each state. In practice, the expert\npolicy is unknown and only a ﬁnite number of trajectories sampled according to πE are available. In\nthese context we can use support estimation techniques to construct a reward function ˆr.\nFor continuous action domains, sparse reward such as rE is unlikely to work since it is improbable\nfor the agent, via random exploration, to discover the optimal actions, while all other actions are\nconsidered equally bad. Instead, since support estimation produces a score with Eq. (4) for testing\neach input, we may directly use that score to construct the reward function ˆr. The rationale is that\nactions similar to that of the expert in any given state would still receive high scores from support\nestimation, allowing the RL agent to discover those actions during exploration.\nBased on the motivation above, we hypothesize that support estimation of the expert policy’s\nstate-action distribution provides a viable reward function for imitation learning. Intuitively, the\nreward function encourages the RL agent to behave similarly as the expert at each state and remain\non the estimated support of the expert policy. Further, this allows us to compute a ﬁxed reward\nfunction based on the expert trajectories and re-frame imitation learning in the standard context of\nreinforcement learning.\nWe note that for stochastic experts, the support of πE might coincide with the whole space S × A.\nSupport estimation would hence produce an uninformative, ﬂat reward function rE when given an\ninﬁnite amount of training data. However, we argue that an inﬁnite amount of training data should\nallow BC to successfully imitate the expert, bypassing the intermediate step of extracting a reward\nfunction from the expert trajectories.\n3.1\nPractical Support Estimation\nFollowing the strategy introduced in Sec. 2.2, we consider a novel approach to support estimation. Our\nmethod takes inspiration from kernel methods but applies to other models such as neural networks.\nLet H be a RKHS and f ∈H a function parametrized by θ ∈Θ. Consider the regression problem\nthat admits a minimizer on H\ninf\nf∈H\nZ\n(fθ(x) −f(x))2 dπ(x),\n5\nThe minimal ∥·∥H solution corresponds to fπ,θ = C†\nπCπfθ = Pπfθ, the orthogonal projection of fθ\nonto the range of Cπ (see e.g Engl et al., 1996). For any x ∈X we have\nfθ(x) −fπ,θ(x) = ⟨fθ, (I −Pπ)φ(x)⟩H .\nIn particular, if x ∈supp(π), we have (I −Pπ)φ(x) = 0 and hence fθ(x) −fπ,θ(x) = 0. The converse\nis unfortunately not necessarily true: for a point x ̸∈supp(π), (I −Pπ)φ(x) may still be orthogonal to\nfθ and thus fθ(x) −fπ,θ(x) = 0.\nA strategy to circumvent this issue is to consider multiple fθ, and then average their squared errors\n(fθ −fπ,θ)2. The rationale is that if x ̸∈Sπ, by spanning diﬀerent directions in H, at least one of the\nfθ will not be orthogonal to (I −Pπ)φ(x), which leads to a non-zero value when compared against\nfπ,θ(x). In particular, if we sample θ according to a probability distribution over Θ, we have\nEθ(fθ −fπ,θ)2 = Eθ\nD\n(I −Pπ)φ(x), fθf⊤\nθ (I −Pπ)φ(x)\nE\nH\n= ⟨(I −Pπ)φ(x), Σ(I −Pπ)φ(x)⟩H ,\nwhere Σ = Eθ[fθf⊤\nθ ] is the covariance of the random functions fθ generated from sampling the\nparameters θ. Ideally, we would like to sample θ such that Σ = I, which allows us to recover the\nsupport estimator ∥(I −Pπ)φ(x)∥H introduced in Sec. 2.2. However, it is not always clear how to\nsample fθ so that its covariance corresponds to the identity in practice. In this sense, this approach\ndoes not oﬀer the same theoretical guarantees as the kernelized method reviewed in Sec. 2.2.\nThe discussion above provides us with a general strategy for support estimation. Consider a\nhypotheses space of functions fθ parametrized by θ ∈Θ (e.g. neural networks). We can sample\nθ1, . . . , θK functions according to a distribution over Θ. Given a training dataset {xi}N\ni=1 sampled\nfrom π, we solve the K regression problems\nˆθk = min\nθ∈Θ\n1\nN\nN\nX\ni=1\n(fθ(xi) −fθk(xi))2,\n(6)\nfor every k = 1, . . . , K. Then for any x ∈X, we can test whether it belongs to the support of π by\nchecking whether\n1\nK\nK\nX\nk=1\n(fˆθk(x) −fθk(x))2,\n(7)\nis larger than a prescribed threshold. As K and N grow larger, the estimate above will approximate\nincreasingly well the desired quantity Eθ(fπ,θ(x) −fθ(x))2.\n3.2\nReward Function with Random Network Distillation\nWe may interpret the recent method of Random Network Distillation (RND) Burda et al. (2018) as\napproximate support estimation. Formally, RND sets up a randomly generated prediction problem\ninvolving two neural networks: a ﬁxed and randomly initialized target network fθ which sets the\nprediction problem, and a predictor network fˆθ trained on the state-action pairs of the expert\ntrajectories.\nfθ takes an observation to an embedding fθ : S × A →RK and similarly so for\nfˆθ : S × A →RK, which is analogous to the K prediction problems deﬁned in Eq.\n(6).\nThe\npredictor network is trained to minimize the expected mean square error by gradient descent L(s, a) =\n||fˆθ(s, a) −fθ(s, a)||2\n2 with respect to its parameters ˆθ. After training, the score of a state-action pair\n6\nAlgorithm 1 Random Expert Distillation\nInput: data D = {(si, ai)}N\ni=1, Θ function models, initial policy π0.\nRandomly sample θ ∈Θ\nˆθ =Minimize(fˆθ, fθ, D)\nr(·) = exp(−σ1∥fˆθ(·) −fθ(·)∥2\n2)\nCompute rterm from Eq. (9)\nπ =ReinforcementLearning(r, rterm, π0).\nReturn: π.\nbelonging to the support of πE is predicted by L(s, a), analogous to Eq. (7). One concern to RND\nis that a suﬃciently powerful optimization algorithm may recover fθ perfectly, causing L(s, a) to be\nzero everywhere. Our experiments conﬁrm the observations in Burda et al. (2018) that standard\ngradient-based methods do not behave in this undesirable way.\nWith the prediction error L(s, a) as the estimated score of (s, a) belonging to the support of πE ,\nwe propose a reward function that has worked well in practice,\nr(s, a) = exp(−σ1L(s, a))\n(8)\nwhere σ1 is a hyperparameter. As L(s, a) is positive, r(s, a) ∈[0, 1]. We choose σ1 such that r(s, a)\nfrom expert demonstrations are mostly close to 1.\n3.3\nTerminal Reward Heuristic\nFor certain tasks, there are highly undesirable terminal states (e.g. car crashes in autonomous driving).\nIn such contexts, We further introduce a terminal reward heuristic to penalize the RL agent from ending\nan episode far from the estimated support of the expert policy. Speciﬁcally, let ¯r = −1\nN\nPN\ni=1 r(si, ai)\nbe the average reward computed using expert trajectories, and r(sT , aT ) the reward of the ﬁnal state\nof an episode, we deﬁne\nrterm =\n\u001a −σ2¯r\nif σ3¯r > r(sT , aT )\n0\notherwise\n(9)\nwhere σ2, σ3 are hyperparameters. We apply the heuristic to an autonomous driving task in the exper-\niment, and demonstrate that it corresponds nicely with dangerous driving situations and encourages\nRL agents to avoid them. We note that the heuristic is not needed for all other tasks considered in\nthe experiments.\n3.4\nRandom Expert Distillation\nWe present our algorithm Random Expert Distillation (RED) in Algorithm 1. The algorithm computes\nthe estimated support of the expert policy and generates a ﬁxed reward function according to Eq.\n(8). The reward function is used in RL for imitation learning. As we report in the experiments, a\nrandom initial policy π0 is suﬃcient for the majority of tasks considered in the experiments, while the\nremaining tasks requires initialization via BC. We will further discuss this limitation of our method in\nthe results.\n7\n(a) n = 5\n(b) n = 10\n(c) n = 50\n(d) n = 100\nFigure 1: True mean episodic reward during training on the simple domain, with diﬀerent expert dataset sizes.\n3.5\nAlternative to RND\nAutoEncoder (AE) networks Vincent et al. (2008) set up a prediction problem of minθ ||fθ(x) −x||2\n2.\nAE behaves similarly to RND in the sense that it also yields low prediction errors for data similar to\nthe training set. AE could be also seen as approximate support estimation in the context of Eq. (7), by\nreplacing randomly generated fθk with identity functions on each dimension of the input. Speciﬁcally,\nAE sets up K prediction problems\nmin\nθ\n1\nN\nN\nX\ni=1\n(fθ(xi) −Ik(xi))2\nfor k = 1, . . . , K where Ik(x) = xk and K is the input size. However, as discussed in Burda et al.\n(2018), AE with a bottleneck layer may suﬀer from input stochasticity or model misspeciﬁcation,\nwhich are two sources of prediction errors undesirable for estimating the similarity of input to the\ntraining data. Instead, we have found empirically that overparametrized AEs with a ℓ2 regularization\nterm prevent the trivial solution of an identity function, allow easier ﬁtting of training data, and may\nbe used in place of RND. We include AE in our experiments to demonstrate that support estimation\nin general appears to be a viable strategy for imitation learning.\n4\nExperiments\nWe evaluate the proposed method on multiple domains. We present a toy problem to motivate the use\nof expert policy support estimation for imitation learning; and to highlight the behaviors of diﬀerent\nimitation learning algorithms. We then evaluate the proposed reward function on ﬁve continuous\ncontrol tasks from the Mujoco environment. Lastly, we test on an autonomous driving task with a\nsingle trajectory provided by a human driver. The code for reproducing the experiments is available\nonline.1\n1https://github.com/RuohanW/red.git\n8\n(a) GMMIL\n(b) GAIL\n(c) AE\n(d) GMMIL fails intermittently\n(e) GAIL overﬁts with state near 1\n(f) RED\nFigure 2: Estimated reward functions of the expert using diﬀerent imitation learning algorithms on the simple\ndomain. For better visualization of the reward, we use r(s, a) = 1 −α1L(s, a) as the reward function for AE\nand RED.\n4.1\nSimple Domain\nWe consider a stateless task where s ∼unif(−1, 1), a discrete action space a ∈{−1, 1} and the reward\nfunction r(s, a) = as. It is clear that the expert policy to this problem is πE(s) = sign(s). Using Deep\nQ Learning Mnih et al. (2015) as the reinforcement learning algorithm, we compare the proposed\nmethod against AE, GAIL and GMMIL with an expert dataset of size n = 5, 10, 50, 100 respectively.\nIn Figure 1, we show the true mean episodic reward of diﬀerent algorithms during training. Except\nfor GMMIL, all other algorithms converge to the expert policy for all sizes of the dataset. In addition,\nRED and AE converge to the expert policy faster as the extracted reward functions recover the correct\nreward function nearly perfectly (Figures 2f, 2c). In contrast, GAIL requires adversarial training to\nrecover the correct reward function, a noisy process during which all actions from the learned policy,\nboth optimal or not, are considered “wrong” by the discriminator. In fact, when the discriminator is\noverparametrized, it would overﬁt to the training data gradually and generate arbitrary reward for\nsome region of the state-action space (Figure 2e). The results are consistent with observations from\nBrock et al. (2018) that the discriminator is overﬁtting, and that early stopping may be necessary to\nprevent training collapse. For GMMIL, we observe that the method intermittently generates wrong\nreward functions (Figure 2d), causing the performance to oscillate and converges to lower mean reward,\nespecially when the expert data is sparse. This is likely due to the noise in estimating distribution\nmoments from limited samples.\n9\nTable 1: Episodic reward (as provided by the environment) on Mujoco tasks by diﬀerent methods evaluated\nover 50 runs. HalfCheetah and Ant uses BC initialization in RED and AE. We are unsuccessful with GMMIL\non Ant.\nHopper\nHalfCheetah\nReacher\nWalker2d\nAnt\nGAIL\n3614.22 ± 7.17\n4515.70 ± 549.49\n-32.37 ± 39.81\n4877.98 ± 2848.37\n3186.80 ± 903.57\nGMMIL\n3309.30 ± 26.28\n3464.18 ± 476.50\n-11.89 ± 5.27\n2967.10 ± 702.03\n-\nAE\n3478.31 ± 3.09\n3380.74 ± 101.94\n-10.91 ± 5.62\n4097.61 ± 118.06\n3778.61 ± 422.63\nRED\n3625.96 ± 4.32\n3072.04 ± 84.71\n-10.43 ± 5.20\n4481.37 ± 20.94\n3552.77 ± 348.67\nFigure 3: True mean episodic reward of GMMIL, GAIL, AE and RED on Hopper during training. Agents\ntrained with RED improve faster compared to other methods.\n4.2\nMujoco Tasks\nWe further evaluate RED on ﬁve continuous control tasks from the Mujoco environment: Hopper,\nReacher and HalfCheetah, Walker2d and Ant. Similarly, we compare RED against AE, GAIL and\nGMMIL, using Trust Region Policy Optimization (TRPO) Schulman et al. (2015) in Table 1. Similar to\nthe experiments in Ho & Ermon (2016), we consider learning with 4 trajectories of expert demonstration\ngenerated by an expert policy trained with RL. All RL algorithms terminate within 5M environment\nsteps. We note that we were unsuccessful in the Ant task with GMMIL after an extensive search for\nthe appropriate hyperparameters.\nThe results suggest that support estimation of expert policies is a viable strategy for imitation\nlearning, as the AE and RED are both able to achieve good results with the ﬁxed reward functions\nconstructed from support estimation of the expert policy. While RED and AE underperform on the\nHalfCheetah, both methods are comparable or better than GAIL and GMMIL in all other tasks.\nIn particular, RED and AE achieve much smaller variance in performance, likely due to the ﬁxed\nreward function. Consistent with the observations from the simple domain in Sec. 4.1, RED appears\nto achieve faster performance improvements during early training (Figure 3). We note that though\nthe best performance AE can achieve is similar to RED, AE is much more sensitive to the random\ninitialization of parameters, seen from the large standard deviation in Figure 3.\nA limitation of our method is that HalfCheetah and Ant require a policy initialized with BC to\n10\n(a) Episodic reward (distance travelled without col-\nlision) on the driving task with diﬀerent methods.\nThe expert performance of 7485 corresponds with\ntrack completion without collision.\nBoth AE and\nRED uses the terminal reward heuristic.\nAverage\nStd\nBest\nGAIL\n795\n395\n1576\nGMMIL\n2024\n981\n3624\nBC\n1033\n474\n1956\nAE\n4378\n1726\n7485\nRED\n4825\n1552\n7485\nExpert\n7485\n0\n7485\n(b) The expert driver providing demonstration on\nthe driving scenario. The scenario consists of driving\nthrough a straight track while avoiding obstacles.\nachieve good results while GAIL and GMMIL could start with a random policy in the two tasks. We\nhypothesize that the evolving reward functions of GAIL and GMMIL may provide better exploration\nincentives to the RL agent. As GAIL is orthogonal to our method and may be used together, we leave\nit to future work on how to combine the beneﬁts of both RED and GAIL to achieve more robust\nalgorithms.\n4.3\nAutonomous Driving Task\nLastly, we evaluate RED, AE, GAIL, GMMIL and BC on an autonomous driving task, using a single\ndemonstration provided by a human driver. The environment consists of a straight track with obstacles\nplaced randomly at one of the three lanes of the track (Figure 4b). A human demonstrator was\ninstructed to drive through the track and avoid any obstacles, while keeping the speed around 100\nkm/h. We sampled the expert driving actions at 20 Hz. For the environment, we use a vector of size\n24 to represent state (20 dimensions for the LIDAR reading, 3 dimensions for the relative position,\nand orientation of the track with respect to the car, and 1 dimension for the vehicle speed) to output\nthe steering and the accelerator command for the vehicle. We include the terminal reward heuristic\ndeﬁned in Eq. 9. For evaluation, we measure the distance a learned policy is able to control the vehicle\nthrough the track without any collision with the obstacles. The task is challenging as the learner must\ngeneralize from the limited amount of training data, without explicit knowledge about the track width\nor the presence of obstacles. The driving task also allows us to qualitatively observe the behaviors of\nlearned policies.\nIn this task, we initialize all policies with BC and use stochastic value gradient method with\nexperience replay Heess et al. (2015) as the reinforcement learning algorithm. The algorithm is referred\nto as SVG(1)-ER in the original paper.\nTable 4a shows the average and best performance of each method evaluated over 5 runs through\nthe same track. We note that the expert performance of 7485 corresponds with track completion\nwithout collision. The results suggest that RED achieves the best performance. For both RED and\nAE, the reward functions correctly identify dangerous situations, such as collision or near-collision,\nby assigning those states with zero rewards. Figure 5 shows a few representative states where the\nreward function outputs zero reward. In contrast, we observe that GAIL tends to overﬁt to the\nexpert trajectory. During training, GAIL often \"forgets\" how to avoid the same obstacle after the\ndiscriminator update the reward function. Such behaviors prevent the learned policy from avoiding\nobstacles consistently. For GMMIL, we observe behaviors similar to that found in Sec. 4.1: the reward\n11\n(a) Near Collision\n(b) Collision\n(c) Oﬀ-road Collision\n(d) Oﬀ-road\nFigure 5: Representative scenarios where the reward functions of AE and RED assign near zero rewards.\nThe scenarios correspond well with various dangerous states as they are dissimilar to those from expert\ndemonstrations.\nfunction intermittently produces problematic incentives, such as assigning positive rewards for states\nimmediately preceding collisions.\n5\nConclusion\nWe propose a new general framework of imitation learning via expert policy support estimation. We\nconnect techniques such as Random Network Distillation and AutoEncoders to approximate support\nestimation; and introduce a method for eﬃciently learning a reward function suitable for imitation\nlearning from the expert demonstrations. We have shown empirically in multiple tasks that support\nestimation of expert policies is a viable strategy for imitation learning, and achieves comparable or\nbetter performance compared to the state-of-art. For future works, combining diﬀerent approaches of\nrecovering the expert’s reward function appears to be a promising direction.\nReferences\nAbbeel, P. and Ng, A. Y. Apprenticeship learning via inverse reinforcement learning. In Proceedings\nof the twenty-ﬁrst international conference on Machine learning, pp. 1. ACM, 2004.\nArjovsky, M. and Bottou, L. Towards principled methods for training generative adversarial networks.\nstat, 1050:17, 2017.\nArjovsky, M., Chintala, S., and Bottou, L. Wasserstein generative adversarial networks. In International\nConference on Machine Learning, pp. 214–223, 2017.\nAronszajn, N. Theory of reproducing kernels. Transactions of the American mathematical society, 68\n(3):337–404, 1950.\nBansal, M., Krizhevsky, A., and Ogale, A. Chauﬀeurnet: Learning to drive by imitating the best and\nsynthesizing the worst. arXiv preprint arXiv:1812.03079, 2018.\nBaram, N., Anschel, O., Caspi, I., and Mannor, S. End-to-end diﬀerentiable adversarial imitation\nlearning. In International Conference on Machine Learning, pp. 390–399, 2017.\nBojarski, M., Del Testa, D., Dworakowski, D., Firner, B., Flepp, B., Goyal, P., Jackel, L. D.,\nMonfort, M., Muller, U., Zhang, J., et al. End to end learning for self-driving cars. arXiv preprint\narXiv:1604.07316, 2016.\n12\nBrock, A., Donahue, J., and Simonyan, K. Large scale gan training for high ﬁdelity natural image\nsynthesis. arXiv preprint arXiv:1809.11096, 2018.\nBurda, Y., Edwards, H., Storkey, A., and Klimov, O. Exploration by random network distillation.\narXiv preprint arXiv:1810.12894, 2018.\nDe Vito, E., Rosasco, L., and Toigo, A. A universally consistent spectral estimator for the support of\na distribution. Appl Comput Harmonic Anal, 37:185–217, 2014.\nEngl, H. W., Hanke, M., and Neubauer, A. Regularization of inverse problems, volume 375. Springer\nScience & Business Media, 1996.\nFinn, C., Christiano, P., Abbeel, P., and Levine, S. A connection between generative adversarial\nnetworks, inverse reinforcement learning, and energy-based models. arXiv preprint arXiv:1611.03852,\n2016.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and\nBengio, Y. Generative adversarial nets. In Advances in neural information processing systems, pp.\n2672–2680, 2014.\nHeess, N., Wayne, G., Silver, D., Lillicrap, T., Erez, T., and Tassa, Y. Learning continuous control\npolicies by stochastic value gradients. In Advances in Neural Information Processing Systems, pp.\n2944–2952, 2015.\nHo, J. and Ermon, S. Generative adversarial imitation learning. In Advances in Neural Information\nProcessing Systems, pp. 4565–4573, 2016.\nKim, K.-E. and Park, H. S. Imitation learning via kernel mean embedding. AAAI, 2018.\nLucic, M., Kurach, K., Michalski, M., Gelly, S., and Bousquet, O. Are gans created equal? a large-scale\nstudy. In Advances in neural information processing systems, pp. 698–707, 2018.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller,\nM., Fidjeland, A. K., Ostrovski, G., et al. Human-level control through deep reinforcement learning.\nNature, 518(7540):529, 2015.\nNagabandi, A., Kahn, G., Fearing, R. S., and Levine, S. Neural network dynamics for model-based\ndeep reinforcement learning with model-free ﬁne-tuning. In 2018 IEEE International Conference on\nRobotics and Automation (ICRA), pp. 7559–7566. IEEE, 2018.\nNg, A. Y. and Russell, S. J.\nAlgorithms for inverse reinforcement learning.\nIn Proceedings of\nthe Seventeenth International Conference on Machine Learning, pp. 663–670. Morgan Kaufmann\nPublishers Inc., 2000.\nPomerleau, D. A. Eﬃcient training of artiﬁcial neural networks for autonomous navigation. Neural\nComputation, 3(1):88–97, 1991.\nRajeswaran, A., Kumar, V., Gupta, A., Vezzani, G., Schulman, J., Todorov, E., and Levine, S.\nLearning complex dexterous manipulation with deep reinforcement learning and demonstrations.\narXiv preprint arXiv:1709.10087, 2017.\n13\nRoss, S. and Bagnell, D. Eﬃcient reductions for imitation learning. In Proceedings of the thirteenth\ninternational conference on artiﬁcial intelligence and statistics, pp. 661–668, 2010.\nRoss, S., Gordon, G., and Bagnell, D. A reduction of imitation learning and structured prediction\nto no-regret online learning. In Proceedings of the fourteenth international conference on artiﬁcial\nintelligence and statistics, pp. 627–635, 2011.\nRudi, A., De Vito, E., Verri, A., and Odone, F. Regularized kernel algorithms for support estimation.\nFrontiers in Applied Mathematics and Statistics, 3:23, 2017.\nSalimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and Chen, X. Improved\ntechniques for training gans. In Advances in Neural Information Processing Systems, pp. 2234–2242,\n2016.\nSchölkopf, B., Smola, A., and Müller, K.-R. Nonlinear component analysis as a kernel eigenvalue\nproblem. Neural computation, 10(5):1299–1319, 1998.\nSchulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. Trust region policy optimization. In\nInternational Conference on Machine Learning, pp. 1889–1897, 2015.\nSun, W., Venkatraman, A., Gordon, G. J., Boots, B., and Bagnell, J. A.\nDeeply aggrevated:\nDiﬀerentiable imitation learning for sequential prediction. arXiv preprint arXiv:1703.01030, 2017.\nVincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. Extracting and composing robust features\nwith denoising autoencoders. In Proceedings of the 25th international conference on Machine learning,\npp. 1096–1103. ACM, 2008.\nZiebart, B. D., Maas, A. L., Bagnell, J. A., and Dey, A. K. Maximum entropy inverse reinforcement\nlearning. In AAAI, volume 8, pp. 1433–1438. Chicago, IL, USA, 2008.\n14\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-05-16",
  "updated": "2019-06-07"
}