{
  "id": "http://arxiv.org/abs/1905.07187v1",
  "title": "An Essay on Optimization Mystery of Deep Learning",
  "authors": [
    "Eugene Golikov"
  ],
  "abstract": "Despite the huge empirical success of deep learning, theoretical\nunderstanding of neural networks learning process is still lacking. This is the\nreason, why some of its features seem \"mysterious\". We emphasize two mysteries\nof deep learning: generalization mystery, and optimization mystery. In this\nessay we review and draw connections between several selected works concerning\nthe latter.",
  "text": "An essay on optimization mystery of\ndeep learning\nEugene A. Golikov\nNeural Networks and Deep Learning Lab., MIPT\ngolikov.ea@mipt.ru\ngolikov.e.a000@gmail.com\nMay 20, 2019\n1\nIntroduction\nDespite its huge empirical success, deep learning still preserves many features of\nalchemy [Rahimi, 2017]: progress in this ﬁeld is obtained mainly by trial and error,\nand our intuition about how do neural networks actually work often misleads us.\nAlchemy, in order to become usual chemistry, needs a theoretical ground. For\nnow, a solid theoretical ground for deep learning is lacking, however, fortunately,\nmany pieces of theory appeared from diﬀerent directions during several past years.\nThe purpose of this essay is not to provide a comprehensive review, but to draw\nconnections between some works on this topic. The list of works mentioned here is\nby no means representative, or, all the more so, complete.\nSince the theory of deep learning is lacking, some features of neural networks\nlearning seem ”mysterious”. We emphasize two mysteries of deep learning:\n1. Generalization mystery. It is very common for contemporary neural net-\nworks to have many more parameters than the number of training examples\nat hand. Having huge abundance of parameters results in existence of ”bad”\nlocal minima in terms of test error [Zhang et al., 2016]. Classic generalization\nbounds based on VC-dimension are pessimistic, and consequently vacuous in\nthis case. However, stochastic gradient descent seems to avoid these bad local\nminima. Hence new, neural-net-speciﬁc, bounds have to be developed.\n2. Optimization mystery. It is indeed surprising, why (stochastic) gradient\ndescent does not get trapped into ”bad” local minima in terms of test error.\nHowever, it is also quite surprising, why gradient descent (a local optimization\nmethod!) does not get trapped into bad local minima in terms of train error.\nIt seems true that, since the number of parameters in modern neural networks\nis usually much greater than the number of training samples, there exists such\nparameter conﬁguration for which our neural net perfectly ﬁts the data, how-\never, it is far from obvious why the gradient descent ﬁnds this conﬁguration.\nIn the current essay we will talk about the optimization mystery.\narXiv:1905.07187v1  [cs.LG]  17 May 2019\nFigure 1: Results of training Conv-small of [Miyato et al., 2017] on subset of 4000\nsamples of CIFAR10. Initial numbers of ﬁlters in convolutional layers were multiplied\nby width factor.\nIn order to illustrate the above-mentioned phenomena, we trained a modern\nconvolutional architecture, Conv-small from [Miyato et al., 2017], on a subset of\n4000 examples of CIFAR-10 dataset. We also multiplied the number of ﬁlters in\neach convolutional layer by some width factor, in order to illustrate how the number\nof parameters aﬀects optimization and generalization.\nThe results are shown in Figure 1. One can see that the network indeed gen-\neralizes well, despite of severe overparameterization. Moreover, the test accuracy\ndoes not decrease as network grows larger. From the optimization side, we see that\nSGD indeed ﬁnds a global minimum in terms of train error (note that there are no\nglobal minima in terms of cross-entropy in any ﬁnite ball in weight-space). More-\nover, as network grows larger, optimization becomes faster (in terms of number of\niterations).\n2\nOptimization mystery\n2.1\nLoss landscape\nLet us ﬁrst deal with one of these observations: gradient descent ﬁnds a global\nminimum for random initialization.\nIt is known for randomly initialized gradi-\nent descent that it is able to ﬁnd a local minimum almost surely (Theorem 4.8\nof [Lee et al., 2016]). Hence it is tempting to propose the following hypothesis:\nAll local minima of loss landscape of neural nets are global.\n(1)\nWe begin with the simplest optimization problem — l2-regression:\nLshallow(W) = 1\n2Ex,y∼D∥y −Wx∥2\n2 →min\nW .\n(2)\nOne can easily show that all local minima of this problem are indeed global, and in\nthe case of non-degenerate data covariance matrix Σxx = ExxxT, this minimum is\nunique.\nThere are two possible ways to depart from this trivial setting: acquiring depth,\nand adding a non-linearity.\n2\n2.1.1\nDeep linear nets\nLet us consider a deep linear net ﬁrst. The corresponding l2-regression problem is\ngiven as follows:\nLdeep(W1:H) = 1\n2Ex,y∼D∥y −WH . . . W1x∥2\n2 →min\nW1:H .\n(3)\nThis setting looks odd, since in terms of a class of realizable functions, a deep linear\nnet is equivalent to a shallow one with rank restriction. Indeed, let Wk ∈Rdk×dk−1,\nand consider the following shallow problem:\nLshallow(R) = 1\n2Ex,y∼D∥y −Rx∥2\n2 →min\nR ,\ns. t. rk R ≤dp,\n(4)\nwhere p ∈Arg mink dk — index of the ”bottleneck” layer. Then, we have a many-\nto-one correspondence:\nW1:H →R = WH . . . W1,\nand all minima of a deep problem are of the same value as ones of a shallow problem:\nmin\nR Lshallow = min\nW1:H Ldeep.\nNote, however, that both problems are (generally) non-convex: shallow problem\nis non-convex, because of non-convexity of its optimization domain (for non-trivial\nrank-constraint), and deep problem is non-convex due to weight-space symmetries.\nIndeed, if we multiply one of the weight matrices by a non-zero factor, and divide\nanother weight matrix by the same factor simultaneously, the corresponding function\nwill not change.\nThe above-proposed hypothesis 1 is false for a general non-convex problem, how-\never, it was proven in [Lu and Kawaguchi, 2017] (see Theorem 2.2 there) that it is\ntrue for a rank-constrained shallow problem1. Moreover, [Lu and Kawaguchi, 2017]\nprove that the same is true for a deep problem too.\nConsider a ﬁnite dataset X ∈Rd0×m of size m. Assume X to be of full rank\n(this corresponds to non-degeneracy of the corresponding covariance matrix). At\nthe high level, the proof is based on three theorems:\n1. Theorem 2.1: If W1:H is a local minimum of Ldeep(W1:H), then R = WH . . . W1\nis a local minimum of Lshallow(R).\n2. Theorem 2.2: Every local minimum of Lshallow(R) is global.\n3. Theorem 2.3: Every local minimum of Ldeep(W1:H) is global.\nNote that Theorem 2.3 is a simple corollary of two previous theorems. Indeed,\nlet W1:H be a local minimum of Ldeep, then due to Theorem 2.1, R = WH . . . W1 is\na local minimum of Lshallow. Since all local minima of Lshallow are of the same value\n(Theorem 2.2), all local minima of Ldeep are of the same value too. Hence they are\nall global.\nProof of Theorem 2.1 starts with the following observation. Let ¯W1:H be a local\nminimum of Ldeep. Let W1:H be a small perturbation of ¯W1:H. In order to prove\n1More precisely, Theorem 2.2 of [Lu and Kawaguchi, 2017] assumes a ﬁnite dataset. It is not\nobvious how to generalize it to a general data distribution.\n3\nthat ¯R = ¯WH . . . ¯W1 is a local minimum of Lshallow, we have to show that for any\nperturbation R of ¯R holds Lshallow(R) ≥Lshallow( ¯R). Our aim then is to prove that\nfor any perturbation R there exists a perturbation W1:H such that R = WH . . . W1.\nOnce this is proven, we can conclude that:\nLshallow(R) = Ldeep(W1:H) ≥Ldeep( ¯W1:H) = Lshallow( ¯R).\nPerturbation R that corresponds to perturbation W1:H is explicitly constructed\nfor the case rk ¯R = dp, and H = 2. Generalization to H > 2 is given by induction\n(Theorem 3.1). If rk ¯R < dp, it is proven in Theorem 3.2 and Theorem 3.3 that one\ncan perturb ¯W1:H in such a way that perturbation would have rk(WH . . . W1) = dp,\nwill be a local minimum of Ldeep, and loss will not change. In other words, we can\nalways perturb a local minimum to make it full rank and retain local minimality.\n2.1.2\nNon-linear nets\nSo, our hypothesis is true for linear nets. However, a linear net is not a practical case,\nand we have to move to non-linear nets. A general loss of the simplest non-linear\nnet is given as follows:\nL(W) = 1\n2Ex,y∼D∥y −σ(Wx)∥2\n2,\n(5)\nwhere σ(·) is an element-wise non-linearity. Unfortunately, it happens that even a\nsimple one-layer non-linear regression has a quite hard-to-analyze loss landscape,\neven for gaussian data distribution and labels generated from a teacher net of the\nsame architecture (see, e.g. [Tian, 2017]). Hence one can hardly hope to obtain a\nsimple, analyzable loss landscape for a non-linear net with one hidden layer:\nL(W1:2) = 1\n2Ex,y∼D∥y −W2σ(W1x)∥2\n2.\n(6)\nWide shallow sigmoid nets.\nSurprisingly, our hypothesis becomes true for a\nnon-linear net as its hidden layer becomes suﬃciently wide. More precisely, consider\nthe following variant of the previous problem:\nL(W1:2) = 1\n2∥Y −W2σ(W1X)∥2\nF →min\nW1:2,\n(7)\nwhere index F denotes Frobenius norm, X ∈Rd0×m is a ﬁnite dataset of size m, Wk ∈\nRdk×dk−1. Note that here d1 denotes the width of a hidden layer. [Yu and Chen, 1995]\nproved that as long as d1 ≥m and columns of X (data points) are distinct, for\nσ(z) = (1 + exp(−z))−1 all local minima are global2.\nThe proof is based on the following observation. Let W1:2 be a local minimum\nof 7. Let us ﬁx W1, and consider the following surrogate loss:\nLW1(W2) = L(W1:2).\nNote that LW1 corresponds to the loss of a linear regression 2 with modiﬁed dataset\nF1 = σ(W1X) ∈Rd1×m.\nSince the problem LW1(W2) →minW2 is convex, all\n2Actually, [Yu and Chen, 1995] proved their theorem for the case d1 = m, but it is not hard to\ngeneralize it to d1 ≥m.\n4\nof its critical points are minima of the same value. Obviously, if F1 has rank m\n(this necessitates the width d1 of a hidden layer to be greater than the number of\ndatapoints m), we can perfectly ﬁt the modiﬁed dataset, and all local minima of\nLW1 have zero value. In this case, L(W1:2) = LW1(W2) = 0, and W1:2 is indeed a\nglobal minimum of L.\nUnfortunately, the condition d1 ≥m is not suﬃcient for F1 to always be of\nrank m. However, it is proven in the paper that as long as all columns of X are\ndistinct, the set of such W1 for which rk F1 < m, has Lebesgue measure of zero3. The\nproof suﬃciently utilizes the fact that sigmoid is an analytic function; the proven\nstatement is false for ReLU.\nThe proof of the main theorem proceeds as follows. We have already proven\nthe theorem for rk F1 = m. However, if rk F1 < m, since the value of the measure\nfor the corresponding W1 is zero, we can ﬁnd a small enough perturbation ˆW1, so\nthat rk ˆF1 = m. Suppose L(W1:2) > 0. Then, for a small enough perturbation\nˆW1, L( ˆW1, W2) > 0. Launch a gradient ﬂow on L ˆ\nW1. It will ﬁnd a point ˜W2 with\nzero loss, since rk ˆF1 = m. Hence a gradient ﬂow leaves some ﬁxed vicinity of W1:2\n(vicinity of positive loss), no matter how small the perturbation ∥ˆW1 −W1∥is. This\nmeans that the point W1:2 is unstable in Lyapunov sense, hence it cannot be a local\nminimum. Hence all local minima of 7 have value zero, regardless of the rank of\nF1 = σ(W1X); hence all of them are global.\nDeep and wide sigmoid nets.\nWe see that our hypothesis 1 is true for wide-\nenough shallow nets. What about deep nets? Consider a problem, similar to 7:\nL(W1:H) = ∥Y −WHσ(WH−1 . . . σ(W1X) . . .)∥2\nF →min\nW1:H,\n(8)\nwhere as before, X ∈Rd0×m denotes a ﬁnite dataset of size m, Wk ∈Rdk×dk−1, and\nσ(·) is an element-wise sigmoid non-linearity. In the previous theorem the main\nassumption was that the hidden layer was wide enough. Here we assume a similar\nrestriction:\n∃k : dk ≥m, and all columns of X are distinct.\nThese two assumptions are enough to prove that the set of W1:k, such that Fk =\nσ(Wk . . . σ(W1X) . . .) ∈Rdk×m has rank < m, has Lebesgue measure zero (Lemma\n4.4 of [Nguyen and Hein, 2017]).\nLet us try directly applying the logic of the previous theorem. For a shallow\nnet rk F1 = m implied L(W1:2) = 0 at a local minimum.\nThis was due to the\nfact that the problem LW1(W2) = L(W1:2) →minW2 was convex (since it was a\nlinear regression). Unfortunately, given rk Fk = m, the problem LW1:k(Wk+1:H) =\nL(W1:H) →minWk+1:H is not generally convex (since for H > k + 1 it is not a linear\nregression any more). Hence it is not true that all local minima have zero loss, even\ngiven rk Fk = m.\nHowever, if we additionally assume that our local minimum is non-degenerate in\nthe following sense:\n∀l ∈{k + 2, . . . , H}\nrk Wl = dl,\n3[Yu and Chen, 1995] gave an incorrect proof of this statement. One can ﬁnd a correct proof\nof a more general statement in Lemma 4.4 of [Nguyen and Hein, 2017]\n5\nthen rk Fk = m at a local minimum will imply that the loss in this local minimum\nis zero (Lemma 3.5 of [Nguyen and Hein, 2017]). Note that for a shallow net k = 1,\nH = 2, and this assumption is trivially true. Note also that this assumption implies\ndl ≤dl−1 ∀l ≥k + 2. In other words, the width of a net should not increase after\nthe k-th layer.\nThe condition rk Fk = m does not generally hold even given dk ≥m. Simi-\nlar to the theorem concerning shallow nets, we have to deal with the case when\nrk Fk < m at our local minimum. Previously, we have used the convexity of the\nproblem LW1(W2) →minW2, which implied that continuous-time gradient descent\nﬁnds a global minimum of this problem. Now we have the problem LW1:k(Wk+1:H) =\nL(W1:H) →minWk+1:H, which is not generally convex, and we do not have a guar-\nantee to ﬁnd a global minimum of it with continuous-time gradient descent any\nmore.\nIn their Theorem 4.6, [Nguyen and Hein, 2017] use the following technique. They\nassume the Hessian ∇2\nWk+1:HL(W1:H) to be non-degenerate at a local minimum W1:H\nwe consider. This allows them to use the implicit function theorem to deduce that if\nW1:H is a critical point, then for any perturbation ˆW1:k one can ﬁnd a perturbation\nˆWk+1:H, such that ˆW1:H is also a critical point. Take a perturbation ˆW1:k such that\nˆFk has rank m (such perturbation exists due to Lemma 4.4, mentioned previously).\nThe corresponding perturbed matrices ˆWl for l ≥k + 2 will have ranks dl as long\nas perturbations are small enough.\nHence at the perturbed point ˆW1:H we ﬁnd\nourselves at a simple, previously-discussed case, and conclude that L( ˆW1:H) = 0.\nHowever, since the loss function is continuous wrt weights, we further conclude that\nL(W1:H) = 0, and the considered minimum is global.\nTo sum up, the main theorem (Corollary 3.9 of [Nguyen and Hein, 2017]) states\nthe following. Let W1:H be a local minimum of a problem 8. Assume the following\nconditions hold:\n1. Columns of X (data points) are distinct;\n2. One of the layers is wide enough: ∃k : dk ≥m;\n3. Our minimum is non-degenerate: ∀l ∈{k + 2, . . . , H}\nrk Wl = dl;\n4. Loss function has non-degenerate Hessian at our minimum: det ∇2\nWk+1:HL(W1:H) ̸=\n0.\nThen, L(W1:H) = 0, and W1:H is a global minimum of 8.\nUnfortunately, as we see, this theorem does not state that all local minima of 8\nare global.\nIt states only that some minima, which are non-degenerate in some\nsense, are global.\nWe hypothesize that the third condition could be relaxed to\nsimply dl ≤dl−1 ∀l ∈{k + 2, . . . , H}, but we also suspect that degenerate minima\nin terms of the Hessian can exist, and our initial hypothesis 1 is generally false.\nRemark on non-smooth non-linearities.\nThe above theorem could be general-\nized to all commonly-used smooth non-linearities, e.g. tanh, softplus, however, it is\nunlikely for it to be generalized to non-smooth ones: ReLU, or LeakyReLU. Indeed,\nwhen some units of a ReLU network are deactivated, we eﬀectively obtain a smaller\nnetwork. We do not have guarantees for not-wide-enough nets. Hence, in the worst\ncase, a smaller network could have local minima. These local minima could become\n6\n(degenerate) saddles in the initial network, or again (degenerate) local minima. It is\nnot obvious, whether the second alternative is possible or not. The same reasoning\nholds for the theorem of [Yu and Chen, 1995].\nRemark on criticality with respect to W1:k.\nOne can notice that in the proof\nof the theorem of [Nguyen and Hein, 2017] we have never used the fact that the local\nminimum W1:H is a critical point of loss function L wrt the ﬁrst k weight matrices,\ni.e. that ∇W1:kL(W1:H) = 0. It means that given the four above-stated conditions,\nwe only need to assume Wk+1:H to be a local minimum of LW1:k(Wk+1:H) = L(W1:H).\nHence, according to the theorem, W1:k becomes a minimum of LWk+1:H(W1:k) auto-\nmatically. It is worth thinking of how the condition ∇W1:kL(W1:H) = 0 could be\nused to relax some of the four above-stated assumptions. The same reasoning holds\nfor theorem of [Yu and Chen, 1995].\n2.2\nGradient descent dynamics\nAs we have seen in the previous section, despite the fact, that gradient descent is\nguaranteed to converge to a local minimum, it is not obvious, whether all local min-\nima are global or not, especially for ReLU networks, where theorem of [Nguyen and Hein, 2017]\ndoes not hold. However, as we can see in Figure 1, (stochastic) gradient descent\nindeed converges to a global minimum on the train set.\nA prominent result in this direction was obtained by [Du et al., 2019]. Consider\na ReLU net with one hidden layer and a single output:\nf(W, a, x) =\n1\n√m\nm\nX\nr=1\nar[wT\nr x]+,\n(9)\nwhere x ∈Rd is an input, wr ∈Rd are weights of the hidden layer, ar ∈R are weights\nof the output layer, and [z]+ = max(0, z) denotes an element-wise ReLU. Note that\nthe width of the hidden layer is denoted by m here. We consider l2 regression on a\nﬁnite dataset of size n. This leads to the following loss function:\nL(W) = 1\n2\nn\nX\ni=1\n(f(W, a, xi) −yi)2.\n(10)\nNote that the corresponding optimization problem L(W) →minW is very similar\nto 7. Our new problem diﬀers signiﬁcantly from 7 in the following aspects:\n1. ReLU instead of sigmoid,\n2. optimization is performed wrt to W only.\nNote also the change of notation (in order to be compatible with the paper).\nThe result of [Du et al., 2019] easily transfers to sigmoid, but it is unlikely that\nthe result of [Yu and Chen, 1995] could be generalized to ReLU. Hence the ﬁrst\npoint is by no means a drawback, it is an advantage.\nThe last point is even more crucial. The proof of theorem of [Yu and Chen, 1995]\ncritically relied on optimization wrt W2 (which is denoted as a here). The proof of\n[Du et al., 2019] does not rely on it. The main result (which we have not stated\nyet), could be obtained with or without optimization wrt a.\n7\nBefore stating the main result of [Du et al., 2019], we need several assumptions\nand deﬁnitions. Assume that the weights are initialized as follows:\nwr(0) ∼N(0, I), ar ∼U({−1, 1})\n∀r = 1, . . . , m.\nAssume also that datapoints lie on a sphere and y’s are bounded:\n∥xi∥2 = 1, |yi| < C\n∀i = 1, . . . , n.\nWe consider dynamics of a continuous-time gradient descent:\ndwr(t)\ndt\n= −∂L(W(t))\n∂wr\n.\nIn the proof we are going to reason about the dynamics of network individual pre-\ndictions:\nui(t) = f(W(t), a, xi).\nWe now proceed to the main result. Theorem 3.2 of [Du et al., 2019] states the\nfollowing. Let δ ∈(0, 1) and m = Ω\n\u0010\nn6\nλ4\n0δ3\n\u0011\n; then w.p. ≥1 −δ over initialization we\nhave:\n∥u(t) −y∥2\n2 ≤e−λ0t∥u(0) −y∥2\n2\n∀t ≥0.\n(11)\nHere λ0 is a data-dependent constant, which we will deﬁne soon.\nTheorem 3.1\nof [Du et al., 2019] states that as long as there are no data points which are parallel\nto each other, this constant is strictly positive.\nLet us take a closer look at this result. It states basically that as long as the\nhidden layer is wide enough, neural network predictions on data-points converge to\nground-truth answers exponentially fast with high probability. A trivial consequence\nis that continuous-time gradient descent indeed converges to global minimum with\nhigh probability.\nNote that the continuous-time gradient descent is assumed only for convenience.\nA similar result (with proper learning rate) could be obtained for discrete-time\ngradient descent as well (Theorem 4.1 of [Du et al., 2019]).\nProof sketch.\nThe proof starts with the following reasoning. We want to know\nhow network predictions change with time:\ndui(t)\ndt\n=\nm\nX\nr=1\n∂ui(t)\n∂wr\ndwr\ndt = −\nm\nX\nr=1\n∂ui(t)\n∂wr\n∂L(W(t))\n∂wr\n=\n=\nn\nX\nj=1\n(yj −uj(t))\nm\nX\nr=1\n\u001c∂ui(t)\n∂wr\n, ∂uj(t)\n∂wr\n\u001d\n=\n=\nn\nX\nj=1\n(yj −uj(t)) 1\nmxT\ni xj\nm\nX\nr=1\na2\nr[wT\nr xi ≥0, wT\nr xj ≥0],\nwhere square brackets denote indicators. Note that a2\nr = 1 at initialization, and\ndoes not change during training process. Hence we shall omit it from now on. If we\ndeﬁne\nHij(t) = 1\nmxT\ni xj\nm\nX\nr=1\n[wT\nr xi ≥0, wT\nr xj ≥0],\n(12)\n8\nthen we can rewrite the previous equation as:\ndu(t)\ndt\n= H(t)(y −u(t)).\nThis diﬀerential equation governs the evolution of network predictions on train set.\nOne can easily show that H(t) is a Gram matrix, hence it is positive semi-deﬁnite.\nOur main goal is to show that its minimal eigenvalue remains bounded below with\nλ0/2 throughout the whole process of optimization:\nOur goal is to prove λmin(H(t)) ≥λ0\n2\n∀t ≥0.\n(13)\nIf this holds, we easily obtain the desired result:\nd\ndt∥y −u(t)∥2\n2 = −2(y −u(t))T du(t)\ndt\n=\n= −2(y −u(t))TH(t)(y −u(t)) ≤−λ0∥y −u(t)∥2\n2.\nAfter solving the diﬀerential inequality we get:\n∥y −u(t)∥2\n2 ≤e−λ0t∥y −u(0)∥2\n2,\nwhich is 11. Before giving the sketch of the proof, we have to deﬁne λ0. Let H∞be\nthe expectation of Gram matrix at initialization H(0):\nH∞\nij = Ew1∼N(0,I) . . . Ewr∼N(0,I)Hij(0) = Ew∼N(0,I)(xT\ni xj[wTxi ≥0, wTxj ≥0]).\nWe deﬁne λ0 as its minimal eigenvalue, which is non-negative, since H∞is a Gram\nmatrix. Theorem 3.1 of [Du et al., 2019] states that it is positive as long as data-\npoints are not parallel to each other.\nThe proof of 13 is divided into two parts. In the ﬁrst part we prove that as long\nas m is large enough, H(0) is close to its expectation, H∞, in terms of l2 norm.\nAs a consequence, the spectrum of H(0) does not diﬀer much from the spectrum\nof H∞, and λmin(H(0)) ≥\n3\n4λ0. In the second part we prove that as long as m\nis large enough, H(t) does not change much in terms of l2 norm throughout the\noptimization process. As a consequence, its spectrum also does not change much,\nand λmin(H(t)) ≥1\n2λ0 ∀t ≥0.\nThe ﬁrst part is covered by Lemma 3.1 of [Du et al., 2019]. Essentially, it uses\na concentration bound (Hoeﬀding inequality) to bound |Hij(0) −H∞\nij |, and then to\nbound ∥H(0) −H∞∥2.\nThe second part is a bit more involved. Lemma 3.2 of [Du et al., 2019] states\nthat as long as weights are close enough to initialization, Gram matrix does not\nchange much. This is due to the fact that if we perturb the initial weights, few\nof the indicators in the deﬁnition 12 change.\nLemma 3.3 states that as long as\nλmin(H(s)) ≥λ0/2 on [0, t], W(t) is suﬃciently close to W(0). Lemmas 3.2 and 3.3\ntogether imply that λmin(H(s)) ≥λ0/2 on [0, t] is equivalent to the fact that W(s)\nis close to W(0) on [0, t]. Since for t = 0 both statements should necessarily be true,\nLemma 3.4 states that they are true for all t ≥0 using continuity argument.\n9\nRemark on convergence rate.\nThe theorem 11 states that neural network pre-\ndictions on train dataset converge to train labels exponentially fast, which is a very\nstrong result. However, in optimization theory convergence rate is usually measured\nin terms of how fast the algorithm converges to a stationary point. A (ﬁrst-order)\nstationary point is a critical point of loss function:\n∇WL(W) = 0.\nA second-order stationary point is a stationary point with positive semi-deﬁnite\nhessian of loss function:\n∇WL(W) = 0;\n∇2\nWL(W) ≥0.\nIt is well known in optimization theory that given a general gradient-Lipschitz\nloss function L : Rd →R gradient descent converges to a stationary point in time\nindependent from the number of parameters d. Let us look at how fast the gradient\nof loss function decays to zero in our case:\n\r\r\r\r\n∂L(W(t))\n∂wr\n\r\r\r\r\n2\n=\n1\n√m\n\r\r\r\r\r\nn\nX\nj=1\n(uj(t) −yj)arxj[wT\nr (t)xj ≥0]\n\r\r\r\r\r\n2\n≤\n≤\n1\n√m\nn\nX\nj=1\n|uj(t) −yj| =\n1\n√m∥u(t) −y∥1 ≤\n≤\nr n\nm∥u(t) −y∥2 ≤\nr n\nme−λ0t/2∥u(0) −y∥2.\nWe see that as m increases (which is related to the number of parameters), gradient\nnorm decreases, hence the time to reduce the gradient norm to a value below some\nepsilon also decreases, which was not the case in general setting.\nWhat about second-order stationary points? Theorem 4.8 of [Lee et al., 2016]\nstates that randomly initialized gradient descent converges to a second-order station-\nary point almost surely. However, this theorem tells nothing about the convergence\nrate. [Du et al., 2017] constructs an example of loss function L : Rd →R for which\nrandomly initialized gradient descent requires time exponential in d in order to con-\nverge to a second-order stationary point. It is not hard to check that in our case the\nHessian of the loss function is positive semi-deﬁnite almost everywhere (except the\npoints where ReLU is not diﬀerentiable). Indeed, let us take a look at our Hessian:\n∂2L(W(t))\n∂wr∂ws\n= 1\nm\nn\nX\nj=1\narasx2\nj[wT\nr (t)xj ≥0, wT\ns (t)xj ≥0] =\n= 1\nm\nn\nX\nj=1\n\nar[wT\nr (t)xj ≥0]xj, as[wT\ns (t)xj ≥0]xj\n\u000b\n.\nWe see that the Hessian is a sum of Gram matrices, which are positive semi-deﬁnite,\nhence the whole Hessian is positive semi-deﬁnite too. Given that, our method con-\nverges to a second-order stationary point in time that decreases with the number of\nparameters.\n10\n3\nConclusion\nAs we have seen, the optimization mystery could be revealed, but either for linear\nnetworks (theorem of [Lu and Kawaguchi, 2017]), which are not commonly used, or\nfor unrealistically wide non-linear nets (theorems of [Yu and Chen, 1995], [Nguyen and Hein, 2017],\nand [Du et al., 2019]). We suspect that assuming a cluster structure of the dataset\n(i.e. classes of MNIST are separated from each other in pixel space) will allow to re-\nduce the required amount of overparameterization suﬃciently (this idea was actually\nelaborated in [Nguyen and Hein, 2017]). However, it seems that even for unstruc-\ntured data, the amount of overparameterization required in theorem of [Du et al., 2019]\nis still too large [Zhang et al., 2016]. We suspect that these bounds could be reduced\neven in general case using more involved techniques.\n4\nReferences\n[Du et al., 2017] Du, S. S., Jin, C., Lee, J. D., Jordan, M. I., Poczos, B., and Singh,\nA. (2017). Gradient Descent Can Take Exponential Time to Escape Saddle Points.\narXiv e-prints.\n[Du et al., 2019] Du, S. S., Zhai, X., Poczos, B., and Singh, A. (2019). Gradient\ndescent provably optimizes over-parameterized neural networks. In International\nConference on Learning Representations.\n[Lee et al., 2016] Lee, J. D., Simchowitz, M., Jordan, M. I., and Recht, B. (2016).\nGradient Descent Converges to Minimizers. arXiv e-prints.\n[Lu and Kawaguchi, 2017] Lu, H. and Kawaguchi, K. (2017). Depth Creates No\nBad Local Minima. arXiv e-prints.\n[Miyato et al., 2017] Miyato, T., Maeda, S.-i., Koyama, M., and Ishii, S. (2017).\nVirtual Adversarial Training: A Regularization Method for Supervised and Semi-\nSupervised Learning. arXiv e-prints.\n[Nguyen and Hein, 2017] Nguyen, Q. and Hein, M. (2017). The loss surface of deep\nand wide neural networks. arXiv e-prints.\n[Rahimi, 2017] Rahimi, A. (2017). Machine Learning has become Alchemy. Pre-\nsented at NIPS2017, Test of Time Award.\n[Tian, 2017] Tian, Y. (2017). An Analytical Formula of Population Gradient for\ntwo-layered ReLU network and its Applications in Convergence and Critical Point\nAnalysis. arXiv e-prints.\n[Yu and Chen, 1995] Yu, X.-H. and Chen, G.-A. (1995). On the Local Minima Free\nCondition of Backpropagation Learning. IEEE Transactions on Neural Networks,\npages 1300 – 1303.\n[Zhang et al., 2016] Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.\n(2016). Understanding deep learning requires rethinking generalization. arXiv\ne-prints.\n11\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-05-17",
  "updated": "2019-05-17"
}