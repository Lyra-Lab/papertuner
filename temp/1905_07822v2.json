{
  "id": "http://arxiv.org/abs/1905.07822v2",
  "title": "Minimal Achievable Sufficient Statistic Learning",
  "authors": [
    "Milan Cvitkovic",
    "Günther Koliander"
  ],
  "abstract": "We introduce Minimal Achievable Sufficient Statistic (MASS) Learning, a\ntraining method for machine learning models that attempts to produce minimal\nsufficient statistics with respect to a class of functions (e.g. deep networks)\nbeing optimized over. In deriving MASS Learning, we also introduce Conserved\nDifferential Information (CDI), an information-theoretic quantity that - unlike\nstandard mutual information - can be usefully applied to\ndeterministically-dependent continuous random variables like the input and\noutput of a deep network. In a series of experiments, we show that deep\nnetworks trained with MASS Learning achieve competitive performance on\nsupervised learning and uncertainty quantification benchmarks.",
  "text": "Minimal Achievable Sufﬁcient Statistic Learning\nMilan Cvitkovic 1 G¨unther Koliander 2\nAbstract\nWe introduce Minimal Achievable Sufﬁcient\nStatistic (MASS) Learning, a training method for\nmachine learning models that attempts to produce\nminimal sufﬁcient statistics with respect to a class\nof functions (e.g. deep networks) being optimized\nover. In deriving MASS Learning, we also intro-\nduce Conserved Differential Information (CDI),\nan information-theoretic quantity that — unlike\nstandard mutual information — can be usefully\napplied to deterministically-dependent continu-\nous random variables like the input and output\nof a deep network. In a series of experiments,\nwe show that deep networks trained with MASS\nLearning achieve competitive performance on su-\npervised learning and uncertainty quantiﬁcation\nbenchmarks.\n1. Introduction\nThe representation learning approach to machine learning\nfocuses on ﬁnding a representation Z of an input random\nvariable X that is useful for predicting a random variable Y\n(Goodfellow et al., 2016).\nWhat makes a representation Z “useful” is much debated,\nbut a common assertion is that Z should be a minimal sufﬁ-\ncient statistic of X for Y (Adragni, KoﬁP. & Cook, R. Den-\nnis, 2009; Shamir et al., 2010; James et al., 2017; Achille &\nSoatto, 2018b). That is:\n1. Z should be a statistic of X. This means Z = f(X)\nfor some function f.\n2. Z should be sufﬁcient for Y . This means p(X|Z, Y ) =\np(X|Z).\n3. Given that Z is a sufﬁcient statistic, it should be mini-\nmal with respect to X. This means for any measurable,\n1Department of Computing and Mathematical Sciences, Califor-\nnia Institute of Technology, Pasadena, California, USA 2Acoustics\nResearch Institute, Austrian Academy of Sciences, Vienna, Austria.\nCorrespondence to: Milan Cvitkovic <mcvitkov@caltech.edu>.\nProceedings of the 36 th International Conference on Machine\nLearning, Long Beach, California, PMLR 97, 2019. Copyright\n2019 by the author(s).\nnon-invertible function g, g(Z) is no longer sufﬁcient\nfor Y .1\nIn other words: a minimal sufﬁcient statistic is a random\nvariable Z that tells you everything about Y you could ever\ncare about, but if you do any irreversible processing to Z,\nyou are guaranteed to lose some information about Y .\nMinimal sufﬁcient statistics have a long history in the ﬁeld\nof statistics (Lehmann & Scheffe, 1950; Dynkin, 1951). But\nthe minimality condition (3, above) is perhaps too strong to\nbe useful in machine learning, since it is a statement about\nany function g, rather than about functions in a practical\nhypothesis class like the class of deep neural networks.\nInstead, in this work we consider minimal achievable sufﬁ-\ncient statistics: sufﬁcient statistics that are minimal among\nsome particular set of functions.\nDeﬁnition 1 (Minimal Achievable Sufﬁcient Statistic). Let\nZ = f(X) be a sufﬁcient statistic of X for Y . Z is minimal\nachievable with respect to a set of functions F if f ∈F\nand for any Lipschitz continuous, non-invertible function g\nwhere g ◦f ∈F, g(Z) is no longer sufﬁcient for Y .\nContributions:\n• We\nintroduce\nConserved\nDifferential\nInforma-\ntion (CDI), an information-theoretic quantity that,\nunlike\nmutual\ninformation,\nis\nmeaningful\nfor\ndeterministically-dependent\ncontinuous\nrandom\nvariables, such as the input and output of a deep\nnetwork.\n• We introduce Minimal Achievable Sufﬁcient Statis-\ntic Learning (MASS Learning), a training objective\nbased on CDI for ﬁnding minimal achievable sufﬁcient\nstatistics.\n• We provide empirical evidence that models trained\nby MASS Learning achieve competitive performance\non supervised learning and uncertainty quantiﬁcation\nbenchmarks.\n1This is not the most common phrasing of statistical minimality,\nbut we feel it is more understandable. For the equivalence of this\nphrasing and the standard deﬁnition see Supplementary Material\n7.1.\narXiv:1905.07822v2  [cs.LG]  11 Jun 2019\nMinimal Achievable Sufﬁcient Statistic Learning\n2. Conserved Differential Information\nBefore we present MASS Learning, we need to introduce\nConserved Differential Information (CDI), on which MASS\nLearning is based.\nCDI is an information-theoretic quantity that addresses an\noft-cited issue in machine learning (Bell & Sejnowski, 1995;\nAmjad & Geiger, 2018; Saxe et al., 2018; Nash et al., 2018;\nGoldfeld et al., 2018), which is that for a continuous random\nvariable X and a continuous, non-constant function f, the\nmutual information I(X, f(X)) is inﬁnite. (See Supple-\nmentary Material 7.2 for details.) This makes I(X, f(X))\nunsuitable for use in a learning objective when f is, for\nexample, a standard deep network.\nThe inﬁnitude of I(X, f(X)) has been circumvented in\nprior works by two strategies. One is discretize X and f(X)\n(Tishby & Zaslavsky, 2015; Shwartz-Ziv & Tishby, 2017),\nthough this is controversial (Saxe et al., 2018). Another\nis to use a random variable Z with distribution p(Z|X) as\nthe representation of X rather than using f(X) itself as the\nrepresentation (Alemi et al., 2017; Kolchinsky et al., 2017;\nAchille & Soatto, 2018b). In this latter approach, p(Z|X)\nis usually implemented by adding noise to a deep network\nthat takes X as input.\nThese are both reasonable strategies for avoiding the in-\nﬁnitude of I(X, f(X)). But another approach would be to\nderive a new information-theoretic quantity that is better\nsuited to this situation. To that end we present Conserved\nDifferential Information:\nDeﬁnition 2. For a continuous random variable X taking\nvalues in Rd and a Lipschitz continuous function f : Rd →\nRr, the Conserved Differential Information (CDI) is\nC(X, f(X)) := H(f(X)) −EX [log (Jf(X))]\n(1)\nwhere H denotes the differential entropy\nH(Z) = −\nZ\np(z) log p(z) dz\nand Jf is the Jacobian determinant of f\nJf(x) =\nv\nu\nu\ntdet\n \n∂f(x)\n∂xT\n\u0012∂f(x)\n∂xT\n\u0013T!\nwith ∂f(x)\n∂xT ∈Rr×d the Jacobian matrix of f at x.\nReaders familiar with normalizing ﬂows (Rezende & Mo-\nhamed, 2015) or Real NVP (Dinh et al., 2017) will note that\nthe Jacobian determinant used in those methods is a special\ncase of the Jacobian determinant in the deﬁnition of CDI.\nThis is because normalizing ﬂows and Real NVP are based\non the change of variables formula for invertible mappings,\nwhile CDI is based in part on the more general change of\nvariables formula for non-invertible mappings. More details\non this connection are given in Supplementary Material 7.3.\nThe mathematical motivation for CDI based on the recent\nwork of Koliander et al. (2016) is provided in Supplemen-\ntary Material 7.4. Figure 1 gives a visual example of what\nCDI measures about a function.\n0\n1\n1\nX\np(X)\n0\n½ \n2\nf1(X)\np(f1(X))\nf1(X) = ½ X\nf2(X) = ┃X - ½┃\n0\n½ \n2\nf2(X)\np(f2(X))\nC(X, f1(X)) = 0\nC(X, f2(X)) = - log 2\nFigure 1. CDI of two functions f1 and f2 of the random variable\nX. Even though the random variables f1(X) and f2(X) have the\nsame distribution, C(X, f1(X)) is different from C(X, f2(X)).\nThis is because f1 is an invertible function, while f2 is not. CDI\nquantiﬁes, roughly speaking, “how non-invertible” f2 is.\nThe conserved differential information C(X, f(X)) be-\ntween continuous, deterministically-dependent random vari-\nables behaves much like mutual information does between\ndiscrete random variables. For example, when f is invert-\nible, C(X, f(X)) = H(X), just like with the mutual in-\nformation between discrete random variables. Most im-\nportantly for our purposes, though, C(X, f(X)) obeys the\nfollowing data processing inequality:\nTheorem 1 (CDI Data Processing Inequality). For Lipschitz\ncontinuous functions f and g with the same output space,\nC (X, f(X)) ≥C (X, g(f(X)))\nwith equality if and only if g is invertible almost everywhere.\nThe proof is in Supplementary Material 7.5.\n3. MASS Learning\nWith CDI and its data processing inequality in hand, we can\ngive the following optimization-based characterization of\nminimal achievable sufﬁcient statistics:\nTheorem 2. Let X be a continuous random variable, Y be\na discrete random variable, and F be any set of Lipschitz\nMinimal Achievable Sufﬁcient Statistic Learning\ncontinuous functions with a common output space (e.g.,\ndifferent parameter settings of a deep network). If\nf ∈arg min\nS∈F C(X, S(X))\ns.t. I(S(X), Y ) = max\nS′ I(S′(X), Y )\nthen f(X) is a minimal achievable sufﬁcient statistic of X\nfor Y with respect to F.\nProof. First note the following lemma (Cover & Thomas,\n2006).\nLemma 1. Z\n= f(X) is a sufﬁcient statistic for a\ndiscrete random variable Y if and only if I(Z, Y ) =\nmaxS′ I(S′(X), Y ).\nLemma 1 guarantees that any f satisfying the conditions\nin Theorem 2 is sufﬁcient. Suppose such an f was not\nminimal achievable. Then by Deﬁnition 1 there would exist\na non-invertible, Lipschitz continuous g such that g(f(X))\nwas sufﬁcient. But by Theorem 1, it would then also be\nthe case that C(X, g(f(X))) < C(X, f(X)), which would\ncontradict f minimizing C(X, S(X)).\nWe can turn Theorem 2 into a learning objective over func-\ntions f by relaxing the strict constraint into a Lagrangian\nformulation with Lagrange multiplier 1/β for β > 0:\nC(X, f(X)) −1\nβ I(f(X), Y )\nThe larger the value of β, the more our objective will en-\ncourage minimality over sufﬁciency. We can then sim-\nplify this formulation using the identity I(f(X), Y ) =\nH(Y ) −H(Y |f(X)), which gives us the following op-\ntimization objective:\nLMASS(f) := H(Y |f(X)) + βH(f(X))\n−βEX[log Jf(X)].\n(2)\nWe refer to minimizing this objective as MASS Learning.\n3.1. Practical implementation\nIn practice, we are interested in using MASS Learning to\ntrain a deep network fθ with parameters θ using a ﬁnite\ndataset {(xi, yi)}N\ni=1 of N datapoints sampled from the joint\ndistribution p(x, y) of X and Y . To do this, we introduce\na parameterized variational approximation qφ(fθ(x)|y) ≈\np(fθ(x)|y). Using qφ, we minimize the following empirical\nupper bound to LMASS:\nLMASS ≤bLMASS(θ, φ) := 1\nN\nN\nX\ni=1\n−log qφ(yi|fθ(xi))\n−β log qφ(fθ(xi))\n−β log Jfθ(xi),\nwhere\nthe\nquantity\nqφ(fθ(xi))\nis\ncomputed\nas\nP\ny qφ(fθ(xi)|y)p(y)\nand\nthe\nquantity\nqφ(yi|fθ(xi))\nis computed with Bayes rule as\nqφ(fθ(xi)|yi)p(yi)\nP\ny qφ(fθ(xi)|y)p(y). When\nY is discrete and takes on ﬁnitely many values, as in\nclassiﬁcation problems, and when we choose a variational\ndistribution qφ that is differentiable with respect to φ (e.g. a\nmultivariate Gaussian), then we can minimize bLMASS(θ, φ)\nusing stochastic gradient descent (SGD).\nTo perform classiﬁcation using our trained network, we use\nthe learned variational distribution qφ and Bayes rule:\np(yi|xi) ≈p(yi|fθ(xi)) ≈\nqφ(fθ(xi)|yi)p(yi)\nP\ny qφ(fθ(xi)|y)p(y).\nComputing the Jfθ term in bLMASS for every sample in\nan SGD minibatch is too expensive to be practical. For\nfθ : Rd →Rr, doing so would require on the order of r\ntimes more operations than in standard training of deep net-\nworks by, since computing the Jfθ term involves computing\nthe full Jacobian matrix of the network, which, in our imple-\nmentation, involves performing r backpropagations. Thus\nto make training tractable, we use a subsampling strategy:\nwe estimate the Jfθ term using only a 1/r fraction of the\ndatapoints in a minibatch. In practice, we have found this\nsubsampling strategy to not noticeably alter the numerical\nvalue of the Jfθ term during training.\nSubsampling for the Jfθ term results in a signiﬁcant training\nspeedup, but it must nevertheless be emphasized that, even\nwith subsampling, our implementation of MASS Learning\nis roughly eight times as slow as standard deep network\ntraining. (Unless β = 0, in which case the speed is the\nsame.) This is by far the most signiﬁcant drawback of (our\nimplementation of) MASS Learning. There are many easier-\nto-compute upper bounds or estimates of Jfθ that one could\nuse to make MASS Learning faster, and one could also\npotentially ﬁnd non-invertible network architectures which\nadmit more efﬁciently computable Jacobians, but we do not\nexplore these options in this work.\n4. Related Work\n4.1. Connection to the Information Bottleneck\nThe well-studied Information Bottleneck learning method\n(Tishby et al., 2000; Tishby & Zaslavsky, 2015; Strouse &\nSchwab, 2015; Alemi et al., 2017; Saxe et al., 2018; Amjad\n& Geiger, 2018; Goldfeld et al., 2018; Kolchinsky et al.,\n2019; Achille & Soatto, 2018b;a) is based on minimizing\nthe Information Bottleneck Lagrangian\nLIB(Z) := βI(X, Z) −I(Y, Z)\nfor β > 0, where Z is the representation whose conditional\ndistribution p(Z|X) one is trying to learn.\nMinimal Achievable Sufﬁcient Statistic Learning\nThe LIB learning objective can be motivated based on\npure information-theoretic elegance. But some works like\n(Shamir et al., 2010) also point out the connection between\nthe LIB objective and minimal sufﬁcient statistics, which is\nbased on the following theorem:\nTheorem 3. Let X be a discrete random variable drawn\naccording to a distribution p(X|Y ) determined by the dis-\ncrete random variable Y . Let F be the set of deterministic\nfunctions of X to any target space. Then f(X) is a minimal\nsufﬁcient statistic of X for Y if and only if\nf ∈arg min\nS∈F I(X, S(X))\ns.t. I(S(X), Y ) = max\nS′∈F I(S′(X), Y ).\nThe LIB objective can then be thought of as a Lagrangian\nrelaxation of the optimization problem in this theorem.\nTheorem 3 only holds for discrete random variables. For\ncontinuous X it holds only in the reverse direction, so mini-\nmizing LIB for continuous X has no formal connection to\nﬁnding minimal sufﬁcient statistics, not to mention minimal\nachievable sufﬁcient statistics. See Supplementary Material\n7.6 for details.\nNevertheless, the optimization problems in Theorem 2 and\nTheorem 3 are extremely similar, relying as they both do\non Lemma 1 for their proofs. And the idea of relaxing the\noptimization problem in Theorem 2 into a Lagrangian for-\nmulation to get LMASS is directly inspired by the Informa-\ntion Bottleneck. So while MASS Learning and Information\nBottleneck learning entail different network architectures\nand loss functions, there is an Information Bottleneck ﬂavor\nto MASS Learning.\n4.2. Jacobian Regularization\nThe presence of the Jfθ term in bLMASS is reminiscent of\nthe contrastive autoencoder (Rifai et al., 2011) and Jaco-\nbian Regularization literature (Sokolic et al., 2017; Ross &\nDoshi-Velez, 2018; Varga et al., 2017; Novak et al., 2018;\nJakubovitz & Giryes, 2018). Both these literatures suggest\nthat minimizing EX[∥Df(X)∥F ], where Df(x) = ∂f(x)\n∂xT ∈\nRr×d is the Jacobian matrix, seems to improve generaliza-\ntion and adversarial robustness.\nThis may seem paradoxical at ﬁrst, since by applying the\nAM-GM inequality to the eigenvalues of Df(x)Df(x)T we\nhave\nEX[∥Df(X)∥2r\nF ] = EX[Tr(Df(X)Df(X)T)r]\n≥EX[rr det(Df(X)Df(X)T)]\n= EX[rrJf(X)2]\n≥log EX[rrJf(X)2]\n≥2EX[log Jf(X)] + r log r\nand EX[log Jf(X)] is being maximized by bLMASS. So\nbLMASS might seem to be optimizing for worse general-\nization according to the Jacobian regularization literature.\nHowever, the entropy term in bLMASS strongly encourages\nminimizing EX[∥Df(X)∥F ]. So overall bLMASS seems to\nbe seeking the right balance of sensitivity (dependent on the\nvalue of β) in the network to its inputs, which is precisely in\nalignment with what the Jacobian regularization literature\nsuggests.\n5. Experiments\nIn this section we compare MASS Learning to other ap-\nproaches for training deep networks. Code to reproduce all\nexperiments is available online.2 Full details on all experi-\nments is in Supplementary Material 7.7.\nWe use the abbreviation “SoftmaxCE” to refer to the stan-\ndard approach of training deep networks for classiﬁcation\nproblems by minimizing the softmax cross entropy loss\nbLSoftmaxCE(θ) := −1\nN\nN\nX\ni=1\n\u0010\nlog softmax(fθ(xi))yi\n\u0011\nwhere softmax(fθ(xi))yi is the yith element of the soft-\nmax function applied to the outputs fθ(xi) of the network’s\nlast linear layer. As usual, softmax(fθ(xi))yi is taken to\nbe the network’s estimate of p(yi|xi).\nWe also compare against the Variational Information Bottle-\nneck method (Alemi et al., 2017) for representation learning,\nwhich we abbreviate as “VIB”.\nWe use two networks in our experiments. “SmallMLP”\nis a feedforward network with two fully-connected layers\nof 400 and 200 hidden units, respectively, both with elu\nnonlinearities (Clevert et al., 2015). “ResNet20” is the 20-\nlayer residual network of He et al. (2016).\nWe performed all experiments on the CIFAR-10 dataset\n(Krizhevsky, 2009) and implemented all experiments using\nPyTorch (Paszke et al., 2017).\n5.1. Classiﬁcation Accuracy and Regularization\nWe ﬁrst conﬁrm that networks trained by MASS Learning\ncan make accurate predictions in supervised learning tasks.\nWe compare the classiﬁcation accuracy of networks trained\non varying amounts of data to see the extent to which MASS\nLearning regularizes networks.\nClassiﬁcation accuracies for the SmallMLP network are\nshown in Table 1, and for the ResNet20 network in Table\n2. For the SmallMLP network, MASS Learning performs\nslightly worse than SoftmaxCE and VIB training. For the\n2https://github.com/mwcvitkovic/\nMASS-Learning\nMinimal Achievable Sufﬁcient Statistic Learning\nTable 1. Test-set classiﬁcation accuracy (percent) on CIFAR-10\ndataset using the SmallMLP network trained by various methods.\nFull experiment details are in Supplementary Material 7.7. Val-\nues are the mean classiﬁcation accuracy over 4 training runs with\ndifferent random seeds, plus or minus the standard deviation. Em-\nboldened accuracies are those for which the maximum observed\nmean accuracy in the column was within one standard deviation.\nWD is weight decay; D is dropout.\nMETHOD\nTRAINING SET SIZE\n2500\n10,000\n40,000\nSoftmaxCE\n34.2 ± 0.8\n44.6 ± 0.6\n52.7 ± 0.4\nSoftmaxCE, WD\n23.9 ± 0.9\n36.4 ± 0.9\n48.1 ± 0.1\nSoftmaxCE, D\n33.7 ± 1.1\n44.1 ± 0.6\n53.7 ± 0.3\nVIB, β=1e−1\n32.2 ± 0.6\n40.6 ± 0.4\n46.1 ± 0.5\nVIB, β=1e−2\n34.6 ± 0.4\n43.8 ± 0.8\n51.9 ± 0.8\nVIB, β=1e−3\n35.6 ± 0.5\n44.6 ± 0.6\n51.8 ± 0.8\nVIB, β=1e−1, D\n29.0 ± 0.6\n40.1 ± 0.5\n49.5 ± 0.5\nVIB, β=1e−2, D\n32.5 ± 0.9\n43.9 ± 0.3\n53.6 ± 0.3\nVIB, β=1e−3, D\n34.5 ± 1.0\n44.4 ± 0.4\n54.3 ± 0.2\nMASS, β=1e−2\n29.6 ± 0.4\n39.9 ± 1.2\n46.3 ± 1.2\nMASS, β=1e−3\n32.7 ± 0.8\n41.5 ± 0.7\n47.8 ± 0.8\nMASS, β=1e−4\n34.0 ± 0.3\n41.5 ± 1.1\n47.9 ± 0.8\nMASS, β=0\n34.1 ± 0.6\n42.0 ± 0.6\n48.2 ± 0.9\nMASS, β=1e−2, D\n29.3 ± 1.2\n41.7 ± 0.4\n52.0 ± 0.6\nMASS, β=1e−3, D\n31.5 ± 0.6\n43.7 ± 0.2\n53.1 ± 0.4\nMASS, β=1e−4, D\n32.7 ± 0.8\n43.4 ± 0.5\n53.2 ± 0.1\nMASS, β=0, D\n32.2 ± 1.1\n43.9 ± 0.4\n52.7 ± 0.0\nlarger ResNet20 network, MASS Learning performs equiv-\nalently to the other methods. It is notable that with the\nResNet20 network VIB and MASS Learning both perform\nwell when β = 0, and neither perform signiﬁcantly better\nthan SoftmaxCE. This may be because the hyperparameters\nused in training the ResNet20 network, which were taken\ndirectly from the original paper (He et al., 2016), are specif-\nically tuned for SoftmaxCE training and are more sensitive\nto the speciﬁcs of the network architecture than to the loss\nfunction.\n5.2. Uncertainty Quantiﬁcation\nWe also evaluate the ability of networks trained by MASS\nLearning to properly quantify their uncertainty about their\npredictions. We assess uncertainty quantiﬁcation in two\nways: using proper scoring rules (Lakshminarayanan et al.,\n2017), which are scalar measures of how well a network’s\npredictive distribution p(y|fθ(x)) is calibrated, and by as-\nsessing performance on an out-of-distribution (OOD) detec-\ntion task.\nTables 3 through 8 show the uncertainty quantiﬁcation per-\nformance of networks according to two proper scoring rules:\nthe Negative Log Likelihood (NLL) and the Brier Score.\nThe entropy and test accuracy of the predictive distributions\nare also given, for reference.\nTable 2. Test-set classiﬁcation accuracy (percent) on CIFAR-10\ndataset using the ResNet20 network trained by various methods.\nNo data augmentation was used — full details in Supplementary\nMaterial 7.7. Values are the mean classiﬁcation accuracy over\n4 training runs with different random seeds, plus or minus the\nstandard deviation. Emboldened accuracies are those for which\nthe maximum observed mean accuracy in the column was within\none standard deviation.\nMETHOD\nTRAINING SET SIZE\n2500\n10,000\n40,000\nSoftmaxCE\n50.0 ± 0.7\n67.5 ± 0.8\n81.7 ± 0.3\nVIB, β=1e−3\n49.5 ± 1.1\n66.9 ± 1.0\n81.0 ± 0.3\nVIB, β=1e−4\n49.4 ± 1.0\n66.4 ± 0.5\n81.2 ± 0.4\nVIB, β=1e−5\n50.0 ± 1.1\n67.9 ± 0.8\n80.9 ± 0.5\nVIB, β=0\n50.6 ± 0.8\n67.1 ± 1.0\n81.5 ± 0.2\nMASS, β=1e−3\n38.2 ± 0.7\n59.6 ± 0.8\n75.8 ± 0.5\nMASS, β=1e−4\n49.9 ± 1.0\n66.6 ± 0.4\n80.6 ± 0.5\nMASS, β=1e−5\n50.1 ± 0.5\n67.4 ± 1.0\n81.6 ± 0.4\nMASS, β=0\n50.2 ± 1.0\n67.4 ± 0.3\n81.5 ± 0.2\nFor the SmallMLP network in Tables 3, 4, and 5, VIB\nprovides the best combination of high accuracy and low\nNLL and Brier score across all sizes of training set, despite\nSoftmaxCE with weight decay achieving the best scoring\nrule values. For the larger ResNet20 network in Tables 6\nand 7, MASS Learning provides the best combination of\naccuracy and proper scoring rule performance, though its\nperformance falters when trained on only 2,500 datapoints\nin Table and 8. These ResNet20 UQ results also show\nthe trend that MASS Learning with larger β leads to better\ncalibrated network predictions. Thus, as measured by proper\nscoring rules, MASS Learning can signiﬁcantly improve the\ncalibration of a network’s predictions while maintaining the\nsame accuracy.\nTables 9 through 14 show metrics for performance on an\nOOD detection task where the network predicts not just the\nclass of the input image, but whether the image is from its\ntraining distribution (CIFAR-10 images) or from another\ndistribution (SVHN images (Netzer et al., 2011)). Following\nHendrycks & Gimpel (2017) and Alemi et al. (2018), the\nmetrics we report for this task are the Area under the ROC\ncurve (AUROC) and Average Precision score (APR). APR\ndepends on whether the network is tasked with identify-\ning in-distribution or out-of-distribution images; we report\nvalues for both cases as APR In and APR Out, respectively.\nThere are different detection methods that networks can use\nto identify OOD inputs. One way, applicable to all training\nmethods, is to use the entropy of the predictive distribution\np(y|fθ(x)): larger entropy suggests the input is OOD. For\nnetworks trained by MASS Learning, the variational dis-\ntribution qφ(fθ(x)|y) is a natural OOD detector: a small\nvalue of maxi qφ(fθ(x)|yi) suggests the input is OOD. For\nnetworks trained by SoftmaxCE, a distribution qφ(fθ(x)|y)\nMinimal Achievable Sufﬁcient Statistic Learning\ncan be learned by MLE on the training set and used to detect\nOOD inputs in the same way.\nFor both the SmallMLP network in Tables 9, 10, and 11\nand the ResNet20 network in Tables 12, 13, and 14, MASS\nLearning performs comparably or better than SoftmaxCE\nand VIB. However, one should note that MASS Learning\nwith β = 0 gives performance not signiﬁcantly different to\nMASS Learning with β ̸= 0 on these OOD tasks, which\nsuggests that the good performance of MASS Learning may\nbe due to its use of a variational distribution to produce pre-\ndictions, rather than to the overall MASS Learning training\nscheme.\n5.3. Does MASS Learning ﬁnally solve the mystery of\nwhy stochastic gradient descent with the cross\nentropy loss works so well in deep learning?\nWe do not believe so. Figure 2 shows how the values of the\nthree terms in bLMASS change as the SmallMLP network\ntrains on the CIFAR-10 dataset using either the SoftmaxCE\ntraining or MASS Learning. Despite achieving similar ac-\ncuracies, the SoftmaxCE training method does not seem\nto be implicitly performing MASS Learning, based on the\ndiffering values of the entropy (orange) and Jacobian (green)\nterms between the two methods as training progresses.\n6. Discussion\nMASS Learning is a new approach to representation learn-\ning that performs well on classiﬁcation accuracy, regulariza-\ntion, and uncertainty quantiﬁcation benchmarks, despite not\nbeing directly formulated for any of these tasks. It shows\nparticularly strong performance in improving uncertainty\nquantiﬁcation.\nThere are several potential ways to improve MASS Learn-\ning. Starting at the lowest level: it is likely that we did not\nmanage to minimize bLMASS anywhere close to the extent\npossible in our experiments, given the minimal hyperparam-\neter tuning we performed. In particular, we noticed that the\ninitialization of the variational distribution played a large\nrole in performance, but we were not able to fully explore\nit.\nMoving a level higher, it may be that we are effectively min-\nimized bLMASS, but that bLMASS is not a useful empirical\napproximation or upper bound to LMASS. This could be\ndue to an insufﬁciently expressive variational distribution,\nor simply that the quantities in bLMASS require more data\nto approximate well than our datasets contained.\nAt higher levels still, it may be the case that the Lagrangian\nformulation of Theorem 2 as LMASS is impractical for\nﬁnding minimal achievable sufﬁcient statistics. Or it may be\nthat the difference between minimal and minimal achievable\nsufﬁcient statistics is relevant for performance on machine\nlearning tasks. Or it may simply be that framing machine\nlearning as a problem of ﬁnding minimal sufﬁcient statistics\nis not productive.\nFinally, while we again note that more work is needed to\nreduce the computational cost of our implementation of\nMASS Learning, we believe the concept of MASS learning,\nand the concepts of minimal achievability and Conserved\nDifferential Information we introduce along with it, are\nbeneﬁcial to the theoretical understanding of representation\nlearning.\nAcknowledgements\nWe would like to thank Georg Pichler, Thomas Vidick, Alex\nAlemi, Alessandro Achille, and Joseph Marino for useful\ndiscussions.\nReferences\nAchille, A. and Soatto, S. Emergence of invariance and\ndisentanglement in deep representations. In 2018 Infor-\nmation Theory and Applications Workshop (ITA), pp. 1–9,\n2018a.\nAchille, A. and Soatto, S. Information dropout: Learn-\ning optimal representations through noisy computation.\nIn IEEE Transactions on Pattern Analysis and Machine\nIntelligence, pp. 2897–2905, 2018b.\nAdragni, KoﬁP. and Cook, R. Dennis. Sufﬁcient dimension\nreduction and prediction in regression. Philosophical\nTransactions of the Royal Society A: Mathematical, Phys-\nical and Engineering Sciences, 367(1906):4385–4405,\nNovember 2009. doi: 10.1098/rsta.2009.0110. URL\nhttps://royalsocietypublishing.org/\ndoi/full/10.1098/rsta.2009.0110.\nAlemi, A. A., Fischer, I., Dillon, J. V., and Murphy, K. Deep\nvariational information bottleneck. International Confer-\nence on Learning Representations, abs/1612.00410, 2017.\nURL https://arxiv.org/abs/1612.00410.\nAlemi, A. A., Fischer, I., and Dillon, J. V. Uncertainty in the\nVariational Information Bottleneck. arXiv:1807.00906\n[cs, stat], July 2018.\nURL http://arxiv.org/\nabs/1807.00906. arXiv: 1807.00906.\nAmjad, R. A. and Geiger, B. C. Learning representations\nfor neural network-based classiﬁcation using the informa-\ntion bottleneck principle. IEEE transactions on pattern\nanalysis and machine intelligence, 2018.\nBell, A. J. and Sejnowski, T. J.\nAn information-\nmaximization approach to blind separation and blind\nMinimal Achievable Sufﬁcient Statistic Learning\nTable 3. Uncertainty quantiﬁcation metrics (proper scoring rules) on CIFAR-10 using the SmallMLP network trained on 40,000 datapoints.\nTest Accuracy and Entropy of the network’s predictive distribution are given for reference. Full experiment details are in Supplementary\nMaterial 7.7. Values are the mean over 4 training runs with different random seeds, plus or minus the standard deviation. Emboldened\nvalues are those for which the minimum observed mean value in the column was within one standard deviation. WD is weight decay; D is\ndropout. Lower values are better.\nMethod\nTest Accuracy\nEntropy\nNLL\nBrier Score\nSoftmaxCE\n52.7 ± 0.4\n0.211 ± 0.003\n4.56 ± 0.07\n0.0840 ± 0.0005\nSoftmaxCE, WD\n48.1 ± 0.1\n1.500 ± 0.009\n1.47 ± 0.01\n0.0660 ± 0.0003\nSoftmaxCE, D\n53.7 ± 0.3\n0.606 ± 0.005\n1.79 ± 0.02\n0.0681 ± 0.0005\nVIB, β=1e−1\n46.1 ± 0.5\n0.258 ± 0.005\n5.35 ± 0.15\n0.0944 ± 0.0009\nVIB, β=1e−2\n51.9 ± 0.8\n0.193 ± 0.004\n5.03 ± 0.19\n0.0861 ± 0.0015\nVIB, β=1e−3\n51.8 ± 0.8\n0.174 ± 0.003\n5.49 ± 0.20\n0.0866 ± 0.0015\nVIB, β=1e−1, D\n49.5 ± 0.5\n0.957 ± 0.005\n1.62 ± 0.01\n0.0660 ± 0.0003\nVIB, β=1e−2, D\n53.6 ± 0.3\n0.672 ± 0.014\n1.69 ± 0.01\n0.0668 ± 0.0006\nVIB, β=1e−3, D\n54.3 ± 0.2\n0.617 ± 0.007\n1.75 ± 0.02\n0.0677 ± 0.0005\nMASS, β=1e−2\n46.3 ± 1.2\n0.203 ± 0.005\n6.89 ± 0.16\n0.0968 ± 0.0024\nMASS, β=1e−3\n47.8 ± 0.8\n0.207 ± 0.004\n5.89 ± 0.21\n0.0935 ± 0.0017\nMASS, β=1e−4\n47.9 ± 0.8\n0.212 ± 0.003\n5.71 ± 0.16\n0.0934 ± 0.0017\nMASS, β=0\n48.2 ± 0.9\n0.208 ± 0.004\n5.74 ± 0.20\n0.0927 ± 0.0017\nMASS, β=1e−2, D\n52.0 ± 0.6\n0.690 ± 0.013\n1.85 ± 0.03\n0.0694 ± 0.0005\nMASS, β=1e−3, D\n53.1 ± 0.4\n0.649 ± 0.010\n1.82 ± 0.04\n0.0684 ± 0.0007\nMASS, β=1e−4, D\n53.2 ± 0.1\n0.664 ± 0.020\n1.79 ± 0.02\n0.0680 ± 0.0002\nMASS, β=0, D\n52.7 ± 0.0\n0.662 ± 0.003\n1.82 ± 0.02\n0.0690 ± 0.0003\nTable 4. Uncertainty quantiﬁcation metrics (proper scoring rules) on CIFAR-10 using the SmallMLP network trained on 10,000 datapoints.\nTest Accuracy and Entropy of the network’s predictive distribution are given for reference. Full experiment details are in Supplementary\nMaterial 7.7. Values are the mean over 4 training runs with different random seeds, plus or minus the standard deviation. Emboldened\nvalues are those for which the minimum observed mean value in the column was within one standard deviation. WD is weight decay; D is\ndropout. Lower values are better.\nMethod\nTest Accuracy\nEntropy\nNLL\nBrier Score\nSoftmaxCE\n44.6 ± 0.6\n0.250 ± 0.004\n5.33 ± 0.06\n0.0974 ± 0.0011\nSoftmaxCE, WD\n36.4 ± 0.9\n0.897 ± 0.033\n2.44 ± 0.11\n0.0905 ± 0.0019\nSoftmaxCE, D\n44.1 ± 0.6\n0.379 ± 0.007\n3.76 ± 0.04\n0.0935 ± 0.0012\nVIB, β=1e−1\n40.6 ± 0.4\n0.339 ± 0.011\n4.86 ± 0.23\n0.1017 ± 0.0016\nVIB, β=1e−2\n43.8 ± 0.8\n0.274 ± 0.004\n4.83 ± 0.16\n0.0983 ± 0.0017\nVIB, β=1e−3\n44.6 ± 0.6\n0.241 ± 0.004\n5.50 ± 0.11\n0.0983 ± 0.0005\nVIB, β=1e−1, D\n40.1 ± 0.5\n0.541 ± 0.015\n3.22 ± 0.09\n0.0945 ± 0.0012\nVIB, β=1e−2, D\n43.9 ± 0.3\n0.413 ± 0.009\n3.43 ± 0.09\n0.0927 ± 0.0011\nVIB, β=1e−3, D\n44.4 ± 0.4\n0.389 ± 0.004\n3.61 ± 0.06\n0.0927 ± 0.0004\nMASS, β=1e−2\n39.9 ± 1.2\n0.172 ± 0.008\n10.06 ± 0.37\n0.1109 ± 0.0020\nMASS, β=1e−3\n41.5 ± 0.7\n0.197 ± 0.005\n8.03 ± 0.28\n0.1069 ± 0.0016\nMASS, β=1e−4\n41.5 ± 1.1\n0.208 ± 0.008\n7.55 ± 0.44\n0.1054 ± 0.0023\nMASS, β=0\n42.0 ± 0.6\n0.215 ± 0.009\n7.21 ± 0.28\n0.1043 ± 0.0015\nMASS, β=1e−2, D\n41.7 ± 0.4\n0.399 ± 0.017\n4.21 ± 0.17\n0.0974 ± 0.0013\nMASS, β=1e−3, D\n43.7 ± 0.2\n0.412 ± 0.010\n3.71 ± 0.07\n0.0930 ± 0.0006\nMASS, β=1e−4, D\n43.4 ± 0.5\n0.435 ± 0.011\n3.50 ± 0.05\n0.0923 ± 0.0005\nMASS, β=0, D\n43.9 ± 0.4\n0.447 ± 0.009\n3.40 ± 0.03\n0.0913 ± 0.0008\nMinimal Achievable Sufﬁcient Statistic Learning\nTable 5. Uncertainty quantiﬁcation metrics (proper scoring rules) on CIFAR-10 using the SmallMLP network trained on 2,500 datapoints.\nTest Accuracy and Entropy of the network’s predictive distribution are given for reference. Full experiment details are in Supplementary\nMaterial 7.7. Values are the mean over 4 training runs with different random seeds, plus or minus the standard deviation. Emboldened\nvalues are those for which the minimum observed mean value in the column was within one standard deviation. WD is weight decay; D is\ndropout. Lower values are better.\nMethod\nTest Accuracy\nEntropy\nNLL\nBrier Score\nSoftmaxCE\n34.2 ± 0.8\n0.236 ± 0.025\n8.14 ± 0.84\n0.1199 ± 0.0024\nSoftmaxCE, WD\n23.9 ± 0.9\n0.954 ± 0.017\n3.41 ± 0.07\n0.1114 ± 0.0013\nSoftmaxCE, D\n33.7 ± 1.1\n0.203 ± 0.006\n9.68 ± 0.06\n0.1219 ± 0.0013\nVIB, β=1e−1\n32.2 ± 0.6\n0.247 ± 0.007\n8.33 ± 0.50\n0.1219 ± 0.0013\nVIB, β=1e−2\n34.6 ± 0.4\n0.249 ± 0.004\n7.36 ± 0.18\n0.1175 ± 0.0005\nVIB, β=1e−3\n35.6 ± 0.5\n0.217 ± 0.008\n8.03 ± 0.37\n0.1175 ± 0.0012\nVIB, β=1e−1, D\n29.0 ± 0.6\n0.383 ± 0.011\n6.32 ± 0.16\n0.1219 ± 0.0010\nVIB, β=1e−2, D\n32.5 ± 0.9\n0.260 ± 0.006\n7.41 ± 0.25\n0.1211 ± 0.0019\nVIB, β=1e−3, D\n34.5 ± 1.0\n0.200 ± 0.002\n9.44 ± 0.16\n0.1203 ± 0.0020\nMASS, β=1e−2\n29.6 ± 0.4\n0.047 ± 0.002\n57.13 ± 1.60\n0.1381 ± 0.0007\nMASS, β=1e−3\n32.7 ± 0.8\n0.048 ± 0.004\n46.40 ± 3.81\n0.1322 ± 0.0018\nMASS, β=1e−4\n34.0 ± 0.3\n0.052 ± 0.002\n39.10 ± 1.96\n0.1293 ± 0.0009\nMASS, β=0\n34.1 ± 0.6\n0.061 ± 0.003\n33.60 ± 1.34\n0.1285 ± 0.0012\nMASS, β=1e−2, D\n29.3 ± 1.2\n0.118 ± 0.008\n20.51 ± 0.83\n0.1349 ± 0.0018\nMASS, β=1e−3, D\n31.5 ± 0.6\n0.145 ± 0.004\n15.65 ± 0.71\n0.1289 ± 0.0010\nMASS, β=1e−4, D\n32.7 ± 0.8\n0.185 ± 0.010\n11.21 ± 0.66\n0.1245 ± 0.0011\nMASS, β=0, D\n32.2 ± 1.1\n0.217 ± 0.008\n9.70 ± 0.29\n0.1236 ± 0.0021\nTable 6. Uncertainty quantiﬁcation metrics (proper scoring rules) on CIFAR-10 using the ResNet20 network trained on 40,000 datapoints.\nTest Accuracy and Entropy of the network’s predictive distribution are given for reference. Full experiment details are in Supplementary\nMaterial 7.7. Values are the mean over 4 training runs with different random seeds, plus or minus the standard deviation. Emboldened\nvalues are those for which the minimum observed mean value in the column was within one standard deviation. Lower values are better.\nMethod\nTest Accuracy\nEntropy\nNLL\nBrier Score\nSoftmaxCE\n81.7 ± 0.3\n0.087 ± 0.002\n1.45 ± 0.04\n0.0324 ± 0.0005\nVIB, β=1e−3\n81.0 ± 0.3\n0.089 ± 0.003\n1.51 ± 0.04\n0.0334 ± 0.0005\nVIB, β=1e−4\n81.2 ± 0.4\n0.092 ± 0.002\n1.46 ± 0.05\n0.0331 ± 0.0007\nVIB, β=1e−5\n80.9 ± 0.5\n0.087 ± 0.005\n1.58 ± 0.08\n0.0339 ± 0.0008\nVIB, β=0\n81.5 ± 0.2\n0.079 ± 0.001\n1.70 ± 0.06\n0.0331 ± 0.0007\nMASS, β=1e−3\n75.8 ± 0.5\n0.139 ± 0.003\n1.66 ± 0.07\n0.0417 ± 0.0011\nMASS, β=1e−4\n80.6 ± 0.5\n0.109 ± 0.002\n1.33 ± 0.02\n0.0337 ± 0.0008\nMASS, β=1e−5\n81.6 ± 0.4\n0.095 ± 0.003\n1.36 ± 0.03\n0.0320 ± 0.0005\nMASS, β=0\n81.5 ± 0.2\n0.092 ± 0.000\n1.43 ± 0.04\n0.0325 ± 0.0004\nTable 7. Uncertainty quantiﬁcation metrics (proper scoring rules) on CIFAR-10 using the ResNet20 network trained on 10,000 datapoints.\nTest Accuracy and Entropy of the network’s predictive distribution are given for reference. Full experiment details are in Supplementary\nMaterial 7.7. Values are the mean over 4 training runs with different random seeds, plus or minus the standard deviation. Emboldened\nvalues are those for which the minimum observed mean value in the column was within one standard deviation. Lower values are better.\nMethod\nTest Accuracy\nEntropy\nNLL\nBrier Score\nSoftmaxCE\n67.5 ± 0.8\n0.195 ± 0.011\n2.19 ± 0.06\n0.0557 ± 0.0012\nVIB, β=1e−3\n66.9 ± 1.0\n0.193 ± 0.008\n2.26 ± 0.13\n0.0570 ± 0.0017\nVIB, β=1e−4\n66.4 ± 0.5\n0.197 ± 0.009\n2.30 ± 0.02\n0.0577 ± 0.0007\nVIB, β=1e−5\n67.9 ± 0.8\n0.166 ± 0.010\n2.49 ± 0.13\n0.0561 ± 0.0011\nVIB, β=0\n67.1 ± 1.0\n0.162 ± 0.009\n2.64 ± 0.11\n0.0578 ± 0.0016\nMASS, β=1e−3\n59.6 ± 0.8\n0.252 ± 0.007\n2.61 ± 0.11\n0.0688 ± 0.0014\nMASS, β=1e−4\n66.6 ± 0.4\n0.209 ± 0.009\n2.18 ± 0.05\n0.0570 ± 0.0005\nMASS, β=1e−5\n67.4 ± 1.0\n0.192 ± 0.007\n2.22 ± 0.07\n0.0561 ± 0.0017\nMASS, β=0\n67.4 ± 0.3\n0.189 ± 0.004\n2.30 ± 0.08\n0.0562 ± 0.0007\nMinimal Achievable Sufﬁcient Statistic Learning\nTable 8. Uncertainty quantiﬁcation metrics (proper scoring rules) on CIFAR-10 using the ResNet20 network trained on 2,500 datapoints.\nTest Accuracy and Entropy of the network’s predictive distribution are given for reference. Full experiment details are in Supplementary\nMaterial 7.7. Values are the mean over 4 training runs with different random seeds, plus or minus the standard deviation. Emboldened\nvalues are those for which the minimum observed mean value in the column was within one standard deviation. Lower values are better.\nMethod\nTest Accuracy\nEntropy\nNLL\nBrier Score\nSoftmaxCE\n50.0 ± 0.7\n0.349 ± 0.005\n2.98 ± 0.06\n0.0833 ± 0.0012\nVIB, β=1e−3\n49.5 ± 1.1\n0.363 ± 0.005\n3.10 ± 0.11\n0.0836 ± 0.0020\nVIB, β=1e−4\n49.4 ± 1.0\n0.372 ± 0.016\n3.02 ± 0.10\n0.0833 ± 0.0016\nVIB, β=1e−5\n50.0 ± 1.1\n0.306 ± 0.021\n3.48 ± 0.15\n0.0849 ± 0.0013\nVIB, β=0\n50.6 ± 0.8\n0.271 ± 0.019\n3.80 ± 0.15\n0.0850 ± 0.0007\nMASS, β=1e−3\n38.2 ± 0.7\n0.469 ± 0.012\n3.75 ± 0.08\n0.1010 ± 0.0017\nMASS, β=1e−4\n49.9 ± 1.0\n0.344 ± 0.001\n3.24 ± 0.08\n0.0837 ± 0.0017\nMASS, β=1e−5\n50.1 ± 0.5\n0.277 ± 0.008\n3.81 ± 0.11\n0.0859 ± 0.0005\nMASS, β=0\n50.2 ± 1.0\n0.265 ± 0.009\n3.96 ± 0.15\n0.0861 ± 0.0020\nFigure 2. Estimated value of each term in the MASS Learning loss function, LMASS(f) = H(Y |f(X))+βH(f(X))−βEX[log Jf(X)],\nduring training of the SmallMLP network on the CIFAR-10 dataset. The MASS training was performed with β = 0.001, though the\nplotted values are for the terms without being multiplied by the β coefﬁcients. The values of these terms for SoftmaxCE training are\nestimated using a distribution qφ(fθ(x)|y), with the distribution parameters φ being estimated at each training step by MLE over the\ntraining data.\nMinimal Achievable Sufﬁcient Statistic Learning\ndeconvolution. Neural Computation, 7(6):1129–1159,\nNovember 1995. ISSN 0899-7667.\nClevert, D.-A., Unterthiner, T., and Hochreiter, S. Fast\nand Accurate Deep Network Learning by Exponential\nLinear Units (ELUs). arXiv:1511.07289 [cs], Novem-\nber 2015.\nURL http://arxiv.org/abs/1511.\n07289. arXiv: 1511.07289.\nCover, T. M. and Thomas, J. A. Elements of Information\nTheory 2nd Edition. Wiley-Interscience, Hoboken, NJ, 2\nedition edition, July 2006. ISBN 978-0-471-24195-9.\nDinh, L., Krueger, D., and Bengio, Y. NICE: Non-linear\nIndependent Components Estimation. arXiv:1410.8516\n[cs], October 2014. URL http://arxiv.org/abs/\n1410.8516. arXiv: 1410.8516.\nDinh, L., Sohl-Dickstein, J., and Bengio, S. Density es-\ntimation using Real NVP.\nInternational Conference\non Learning Representations, 2017. URL https://\narxiv.org/abs/1605.08803.\nDynkin, E. B. Necessary and sufﬁcient statistics for afamily\nof probability distributions. Uspekhi Mat. Nauk, 6(1):\n68–90, 1951.\nURL http://www.mathnet.ru/\nphp/archive.phtml?wshow=paper&jrnid=\nrm&paperid=6820&option_lang=eng.\nFederer, H. Geometric Measure Theory. Springer, New\nYork, NY, 1969.\nGoldfeld, Z., Berg, E. v. d., Greenewald, K., Melnyk, I.,\nNguyen, N., Kingsbury, B., and Polyanskiy, Y. Estimating\nInformation Flow in Neural Networks. arXiv:1810.05728\n[cs, stat], October 2018. URL http://arxiv.org/\nabs/1810.05728. arXiv: 1810.05728.\nGoodfellow, I., Bengio, Y., and Courville, A. Deep Learning.\nMIT Press, 2016.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\nlearning for image recognition. 2016 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pp.\n770–778, 2016.\nHendrycks, D. and Gimpel, K. A Baseline for Detecting\nMisclassiﬁed and Out-of-Distribution Examples in Neu-\nral Networks.\nInternational Conference on Learning\nRepresentations, 2017. URL https://arxiv.org/\nabs/1610.02136v3.\nJakubovitz, D. and Giryes, R.\nImproving dnn robust-\nness to adversarial attacks using jacobian regulariza-\ntion.\nECCV, 2018.\nURL https://arxiv.org/\nabs/1803.08680.\nJames, R. G., Mahoney, J. R., and Crutchﬁeld, J. P.\nTrimming the Independent Fat: Sufﬁcient Statistics,\nMutual Information, and Predictability from Effective\nChannel States. Physical Review E, 95(6), June 2017.\nISSN 2470-0045, 2470-0053. doi: 10.1103/PhysRevE.\n95.060102. URL http://arxiv.org/abs/1702.\n01831. arXiv: 1702.01831.\nKingma, D. and Ba, J.\nAdam: A Method for Stochas-\ntic Optimization. International Conference on Learn-\ning Representations, December 2015.\nURL http:\n//arxiv.org/abs/1412.6980.\nKolchinsky, A., Tracey, B. D., and Wolpert, D. H. Nonlinear\nInformation Bottleneck. arXiv:1705.02436 [cs, math,\nstat], May 2017. URL http://arxiv.org/abs/\n1705.02436. arXiv: 1705.02436.\nKolchinsky, A., Tracey, B. D., and Van Kuyk, S. Caveats\nfor information bottleneck in deterministic scenar-\nios. International Conference on Learning Representa-\ntions, 2019. URL http://arxiv.org/abs/1808.\n07593. arXiv: 1808.07593.\nKoliander, G., Pichler, G., Riegler, E., and Hlawatsch, F. En-\ntropy and Source Coding for Integer-Dimensional Singu-\nlar Random Variables. IEEE Transactions on Information\nTheory, 62(11):6124–6154, November 2016. ISSN 0018-\n9448, 1557-9654. doi: 10.1109/TIT.2016.2604248. URL\nhttp://arxiv.org/abs/1505.03337.\narXiv:\n1505.03337.\nKrantz, S. G. and Parks, H. R. Geometric Integration Theory.\nBirkhuser, Basel, Switzerland, 2009.\nKrizhevsky, A. Learning multiple layers of features from\ntiny images. 2009.\nLakshminarayanan, B., Pritzel, A., and Blundell, C. Simple\nand scalable predictive uncertainty estimation using deep\nensembles. NIPS, 2017.\nLehmann, E. L. and Scheffe, H. Completeness, Similar\nRegions, and Unbiased Estimation: Part I. Sankhy: The\nIndian Journal of Statistics (1933-1960), 10(4):305–340,\n1950. ISSN 0036-4452. URL https://www.jstor.\norg/stable/25048038.\nNash, C., Kushman, N., and Williams, C. K. I. Inverting\nSupervised Representations with Autoregressive Neural\nDensity Models. June 2018. URL https://arxiv.\norg/abs/1806.00400.\nNetzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B.,\nand Ng, A. Y. Reading digits in natural images with\nunsupervised feature learning. 2011.\nMinimal Achievable Sufﬁcient Statistic Learning\nNovak, R., Bahri, Y., Abolaﬁa, D. A., Pennington, J., and\nSohl-Dickstein, J.\nSensitivity and Generalization in\nNeural Networks: an Empirical Study. International\nConference on Learning Representations, 2018. URL\nhttp://arxiv.org/abs/1802.08760.\narXiv:\n1802.08760.\nPaszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E.,\nDeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer,\nA. Automatic differentiation in PyTorch. In NIPS-W,\n2017.\nRezende, D. J. and Mohamed, S. Variational inference with\nnormalizing ﬂows. International Conference on Machine\nLearning, 2015. URL https://arxiv.org/abs/\n1505.05770.\nRifai, S., Mesnil, G., Vincent, P., Muller, X., Bengio, Y.,\nDauphin, Y., and Glorot, X. Higher Order Contractive\nAuto-Encoder. In Machine Learning and Knowledge Dis-\ncovery in Databases, Lecture Notes in Computer Science,\npp. 645–660. Springer Berlin Heidelberg, 2011. ISBN\n978-3-642-23783-6.\nRoss, A. S. and Doshi-Velez, F. Improving the adversarial\nrobustness and interpretability of deep neural networks\nby regularizing their input gradients. AAAI, 2018. URL\nhttps://arxiv.org/abs/1711.09404.\nSaxe, A. M., Bansal, Y., Dapello, J., Advani, M., Kolchin-\nsky, A., Tracey, B. D., and Cox, D. D. On the Infor-\nmation Bottleneck Theory of Deep Learning. Interna-\ntional Conference on Learning Representations, February\n2018. URL https://openreview.net/forum?\nid=ry_WPG-A-.\nShamir, O., Sabato, S., and Tishby, N.\nLearning and\ngeneralization with the information bottleneck.\nThe-\noretical Computer Science, 411(29):2696–2711, June\n2010.\nISSN 0304-3975.\ndoi: 10.1016/j.tcs.2010.04.\n006.\nURL http://www.sciencedirect.com/\nscience/article/pii/S030439751000201X.\nShwartz-Ziv,\nR.\nand\nTishby,\nN.\nOpening\nthe\nBlack Box of Deep Neural Networks via Informa-\ntion.\narXiv:1703.00810 [cs], March 2017.\nURL\nhttp://arxiv.org/abs/1703.00810.\narXiv:\n1703.00810.\nSokolic, J., Giryes, R., Sapiro, G., and Rodrigues, M.\nR. D.\nRobust Large Margin Deep Neural Networks.\nIEEE Transactions on Signal Processing, 65(16):4265–\n4280, August 2017. ISSN 1053-587X, 1941-0476. doi:\n10.1109/TSP.2017.2708039.\nURL http://arxiv.\norg/abs/1605.08254. arXiv: 1605.08254.\nStrouse, D. and Schwab, D. J. The deterministic information\nbottleneck. Neural Computation, 29:1611–1630, 2015.\nURL https://arxiv.org/abs/1604.00268.\nTishby, N. and Zaslavsky, N. Deep Learning and the In-\nformation Bottleneck Principle. 2015 IEEE Information\nTheory Workshop (ITW), pp. 1–5, March 2015. URL\nhttps://arxiv.org/abs/1503.02406.\nTishby, N., Pereira, F. C., and Bialek, W. The informa-\ntion bottleneck method. arXiv:physics/0004057, April\n2000. URL http://arxiv.org/abs/physics/\n0004057. arXiv: physics/0004057.\nVarga, D., Csiszrik, A., and Zombori, Z. Gradient Regu-\nlarization Improves Accuracy of Discriminative Mod-\nels.\narXiv:1712.09936 [cs], December 2017.\nURL\nhttp://arxiv.org/abs/1712.09936.\narXiv:\n1712.09936.\nMinimal Achievable Sufﬁcient Statistic Learning\nTable 9. Out-of-distribution detection metrics for SmallMLP network trained on 40,000 CIFAR-10 images, with SVHN as the out-of-\ndistribution examples. Full experiment details are in Supplementary Material 7.7. Values are the mean over 4 training runs with different\nrandom seeds, plus or minus the standard deviation. Emboldened values are those for which the maximum observed mean value in the\ncolumn was within one standard deviation. WD is weight decay; D is dropout. Higher values are better.\nTraining Method\nTest Accuracy\nDetection Method\nAUROC\nAPR In\nAPR Out\nSoftmaxCE\n52.7 ± 0.4\nEntropy\n0.65 ± 0.01\n0.68 ± 0.01\n0.61 ± 0.01\nmaxi qφ(fθ(x)|yi)\n0.38 ± 0.01\n0.42 ± 0.01\n0.43 ± 0.01\nSoftmaxCE, WD\n48.1 ± 0.1\nEntropy\n0.65 ± 0.01\n0.69 ± 0.01\n0.59 ± 0.01\nmaxi qφ(fθ(x)|yi)\n0.43 ± 0.01\n0.43 ± 0.01\n0.48 ± 0.02\nSoftmaxCE, D\n53.7 ± 0.3\nEntropy\n0.71 ± 0.01\n0.75 ± 0.01\n0.65 ± 0.01\nmaxi qφ(fθ(x)|yi)\n0.33 ± 0.00\n0.39 ± 0.00\n0.40 ± 0.00\nVIB, β=1e−1\n46.1 ± 0.5\nEntropy\n0.62 ± 0.01\n0.66 ± 0.01\n0.57 ± 0.01\nRate\n0.47 ± 0.02\n0.49 ± 0.01\n0.46 ± 0.01\nVIB, β=1e−2\n51.9 ± 0.8\nEntropy\n0.64 ± 0.01\n0.67 ± 0.01\n0.59 ± 0.01\nRate\n0.58 ± 0.03\n0.59 ± 0.02\n0.55 ± 0.02\nVIB, β=1e−3\n51.8 ± 0.8\nEntropy\n0.65 ± 0.00\n0.67 ± 0.01\n0.61 ± 0.00\nRate\n0.52 ± 0.03\n0.54 ± 0.03\n0.50 ± 0.03\nVIB, β=1e−1, D\n49.5 ± 0.5\nEntropy\n0.68 ± 0.01\n0.74 ± 0.01\n0.60 ± 0.01\nRate\n0.34 ± 0.01\n0.40 ± 0.01\n0.39 ± 0.00\nVIB, β=1e−2, D\n53.6 ± 0.3\nEntropy\n0.69 ± 0.02\n0.73 ± 0.01\n0.62 ± 0.02\nRate\n0.50 ± 0.03\n0.51 ± 0.02\n0.51 ± 0.03\nVIB, β=1e−3, D\n54.3 ± 0.2\nEntropy\n0.69 ± 0.01\n0.73 ± 0.01\n0.62 ± 0.01\nRate\n0.45 ± 0.01\n0.45 ± 0.01\n0.49 ± 0.01\nMASS, β=1e−2\n46.3 ± 1.2\nEntropy\n0.64 ± 0.01\n0.67 ± 0.01\n0.61 ± 0.01\nmaxi qφ(fθ(x)|yi)\n0.51 ± 0.03\n0.56 ± 0.05\n0.49 ± 0.01\nMASS, β=1e−3\n47.8 ± 0.8\nEntropy\n0.63 ± 0.02\n0.65 ± 0.02\n0.60 ± 0.02\nmaxi qφ(fθ(x)|yi)\n0.63 ± 0.07\n0.64 ± 0.08\n0.60 ± 0.05\nMASS, β=1e−4\n47.9 ± 0.8\nEntropy\n0.63 ± 0.02\n0.65 ± 0.02\n0.60 ± 0.02\nmaxi qφ(fθ(x)|yi)\n0.57 ± 0.06\n0.58 ± 0.05\n0.56 ± 0.05\nMASS, β=0\n48.2 ± 0.9\nEntropy\n0.63 ± 0.02\n0.65 ± 0.02\n0.59 ± 0.02\nmaxi qφ(fθ(x)|yi)\n0.58 ± 0.06\n0.58 ± 0.05\n0.56 ± 0.05\nMASS, β=1e−2, D\n52.0 ± 0.6\nEntropy\n0.73 ± 0.01\n0.75 ± 0.01\n0.67 ± 0.01\nmaxi qφ(fθ(x)|yi)\n0.65 ± 0.06\n0.70 ± 0.06\n0.58 ± 0.05\nMASS, β=1e−3, D\n53.1 ± 0.4\nEntropy\n0.71 ± 0.02\n0.73 ± 0.01\n0.64 ± 0.02\nmaxi qφ(fθ(x)|yi)\n0.64 ± 0.10\n0.66 ± 0.10\n0.60 ± 0.09\nMASS, β=1e−4, D\n53.2 ± 0.1\nEntropy\n0.73 ± 0.01\n0.75 ± 0.01\n0.67 ± 0.01\nmaxi qφ(fθ(x)|yi)\n0.65 ± 0.09\n0.65 ± 0.08\n0.61 ± 0.08\nMASS, β=0, D\n52.7 ± 0.0\nEntropy\n0.71 ± 0.02\n0.74 ± 0.01\n0.65 ± 0.02\nmaxi qφ(fθ(x)|yi)\n0.63 ± 0.09\n0.65 ± 0.08\n0.59 ± 0.09\nMinimal Achievable Sufﬁcient Statistic Learning\nTable 10. Out-of-distribution detection metrics for SmallMLP network trained on 10,000 CIFAR-10 images, with SVHN as the out-of-\ndistribution examples. Full experiment details are in Supplementary Material 7.7. Values are the mean over 4 training runs with different\nrandom seeds, plus or minus the standard deviation. Emboldened values are those for which the maximum observed mean value in the\ncolumn was within one standard deviation. WD is weight decay; D is dropout. Higher values are better.\nTraining Method\nTest Accuracy\nDetection Method\nAUROC\nAPR In\nAPR Out\nSoftmaxCE\n44.6 ± 0.6\nEntropy\n0.62 ± 0.00\n0.64 ± 0.01\n0.59 ± 0.00\nmaxi qφ(fθ(x)|yi)\n0.36 ± 0.01\n0.40 ± 0.01\n0.42 ± 0.00\nSoftmaxCE, WD\n36.4 ± 0.9\nEntropy\n0.62 ± 0.02\n0.62 ± 0.02\n0.60 ± 0.02\nmaxi qφ(fθ(x)|yi)\n0.30 ± 0.01\n0.37 ± 0.00\n0.39 ± 0.01\nSoftmaxCE, D\n44.1 ± 0.6\nEntropy\n0.66 ± 0.01\n0.69 ± 0.01\n0.62 ± 0.01\nmaxi qφ(fθ(x)|yi)\n0.29 ± 0.01\n0.37 ± 0.00\n0.38 ± 0.00\nVIB, β=1e−1\n40.6 ± 0.4\nEntropy\n0.60 ± 0.01\n0.64 ± 0.01\n0.56 ± 0.01\nRate\n0.50 ± 0.02\n0.52 ± 0.02\n0.48 ± 0.01\nVIB, β=1e−2\n43.8 ± 0.8\nEntropy\n0.62 ± 0.00\n0.64 ± 0.01\n0.59 ± 0.01\nRate\n0.55 ± 0.03\n0.57 ± 0.02\n0.53 ± 0.02\nVIB, β=1e−3\n44.6 ± 0.6\nEntropy\n0.62 ± 0.01\n0.64 ± 0.01\n0.59 ± 0.01\nRate\n0.49 ± 0.04\n0.52 ± 0.04\n0.48 ± 0.03\nVIB, β=1e−1, D\n40.1 ± 0.5\nEntropy\n0.62 ± 0.00\n0.65 ± 0.01\n0.57 ± 0.00\nRate\n0.49 ± 0.02\n0.51 ± 0.02\n0.48 ± 0.01\nVIB, β=1e−2, D\n43.9 ± 0.3\nEntropy\n0.67 ± 0.01\n0.69 ± 0.01\n0.62 ± 0.00\nRate\n0.60 ± 0.02\n0.61 ± 0.02\n0.56 ± 0.01\nVIB, β=1e−3, D\n44.4 ± 0.4\nEntropy\n0.67 ± 0.01\n0.69 ± 0.01\n0.63 ± 0.01\nRate\n0.50 ± 0.03\n0.53 ± 0.03\n0.49 ± 0.02\nMASS, β=1e−2\n39.9 ± 1.2\nEntropy\n0.63 ± 0.02\n0.64 ± 0.02\n0.60 ± 0.01\nmaxi qφ(fθ(x)|yi)\n0.54 ± 0.03\n0.58 ± 0.04\n0.50 ± 0.02\nMASS, β=1e−3\n41.5 ± 0.7\nEntropy\n0.61 ± 0.02\n0.62 ± 0.02\n0.59 ± 0.01\nmaxi qφ(fθ(x)|yi)\n0.59 ± 0.07\n0.60 ± 0.06\n0.56 ± 0.06\nMASS, β=1e−4\n41.5 ± 1.1\nEntropy\n0.60 ± 0.00\n0.61 ± 0.01\n0.58 ± 0.00\nmaxi qφ(fθ(x)|yi)\n0.55 ± 0.05\n0.56 ± 0.04\n0.53 ± 0.04\nMASS, β=0\n42.0 ± 0.6\nEntropy\n0.60 ± 0.02\n0.61 ± 0.02\n0.57 ± 0.01\nmaxi qφ(fθ(x)|yi)\n0.55 ± 0.06\n0.57 ± 0.04\n0.54 ± 0.05\nMASS, β=1e−2, D\n41.7 ± 0.4\nEntropy\n0.67 ± 0.01\n0.68 ± 0.01\n0.63 ± 0.01\nmaxi qφ(fθ(x)|yi)\n0.63 ± 0.04\n0.65 ± 0.04\n0.57 ± 0.04\nMASS, β=1e−3, D\n43.7 ± 0.2\nEntropy\n0.67 ± 0.01\n0.68 ± 0.01\n0.63 ± 0.01\nmaxi qφ(fθ(x)|yi)\n0.66 ± 0.05\n0.66 ± 0.04\n0.61 ± 0.06\nMASS, β=1e−4, D\n43.4 ± 0.5\nEntropy\n0.68 ± 0.01\n0.69 ± 0.01\n0.64 ± 0.02\nmaxi qφ(fθ(x)|yi)\n0.64 ± 0.07\n0.65 ± 0.05\n0.59 ± 0.08\nMASS, β=0, D\n43.9 ± 0.4\nEntropy\n0.68 ± 0.00\n0.69 ± 0.01\n0.64 ± 0.00\nmaxi qφ(fθ(x)|yi)\n0.65 ± 0.04\n0.66 ± 0.03\n0.60 ± 0.06\nMinimal Achievable Sufﬁcient Statistic Learning\nTable 11. Out-of-distribution detection metrics for SmallMLP network trained on 2,500 CIFAR-10 images, with SVHN as the out-of-\ndistribution examples. Full experiment details are in Supplementary Material 7.7. Values are the mean over 4 training runs with different\nrandom seeds, plus or minus the standard deviation. Emboldened values are those for which the maximum observed mean value in the\ncolumn was within one standard deviation. WD is weight decay; D is dropout. Higher values are better.\nTraining Method\nTest Accuracy\nDetection Method\nAUROC\nAPR In\nAPR Out\nSoftmaxCE\n34.2 ± 0.8\nEntropy\n0.61 ± 0.01\n0.62 ± 0.01\n0.59 ± 0.01\nmaxi qφ(fθ(x)|yi)\n0.30 ± 0.02\n0.38 ± 0.01\n0.39 ± 0.01\nSoftmaxCE, WD\n23.9 ± 0.9\nEntropy\n0.70 ± 0.03\n0.67 ± 0.03\n0.71 ± 0.04\nmaxi qφ(fθ(x)|yi)\n0.23 ± 0.02\n0.36 ± 0.01\n0.36 ± 0.01\nSoftmaxCE, D\n33.7 ± 1.1\nEntropy\n0.60 ± 0.01\n0.62 ± 0.01\n0.58 ± 0.01\nmaxi qφ(fθ(x)|yi)\n0.27 ± 0.01\n0.37 ± 0.00\n0.37 ± 0.00\nVIB, β=1e−1\n32.2 ± 0.6\nEntropy\n0.58 ± 0.01\n0.60 ± 0.02\n0.56 ± 0.01\nRate\n0.52 ± 0.02\n0.54 ± 0.02\n0.49 ± 0.02\nVIB, β=1e−2\n34.6 ± 0.4\nEntropy\n0.60 ± 0.01\n0.62 ± 0.01\n0.57 ± 0.01\nRate\n0.52 ± 0.04\n0.55 ± 0.04\n0.48 ± 0.03\nVIB, β=1e−3\n35.6 ± 0.5\nEntropy\n0.59 ± 0.01\n0.60 ± 0.01\n0.56 ± 0.01\nRate\n0.50 ± 0.04\n0.53 ± 0.03\n0.48 ± 0.03\nVIB, β=1e−1, D\n29.0 ± 0.6\nEntropy\n0.57 ± 0.01\n0.60 ± 0.01\n0.53 ± 0.01\nRate\n0.45 ± 0.02\n0.48 ± 0.02\n0.46 ± 0.01\nVIB, β=1e−2, D\n32.5 ± 0.9\nEntropy\n0.62 ± 0.01\n0.63 ± 0.02\n0.59 ± 0.01\nRate\n0.53 ± 0.05\n0.56 ± 0.04\n0.52 ± 0.04\nVIB, β=1e−3, D\n34.5 ± 1.0\nEntropy\n0.63 ± 0.01\n0.64 ± 0.02\n0.60 ± 0.01\nRate\n0.56 ± 0.05\n0.57 ± 0.03\n0.54 ± 0.05\nMASS, β=1e−2\n29.6 ± 0.4\nEntropy\n0.59 ± 0.01\n0.61 ± 0.01\n0.56 ± 0.01\nmaxi qφ(fθ(x)|yi)\n0.43 ± 0.03\n0.48 ± 0.03\n0.43 ± 0.01\nMASS, β=1e−3\n32.7 ± 0.8\nEntropy\n0.57 ± 0.01\n0.59 ± 0.02\n0.55 ± 0.01\nmaxi qφ(fθ(x)|yi)\n0.57 ± 0.04\n0.59 ± 0.04\n0.54 ± 0.03\nMASS, β=1e−4\n34.0 ± 0.3\nEntropy\n0.57 ± 0.01\n0.57 ± 0.01\n0.55 ± 0.01\nmaxi qφ(fθ(x)|yi)\n0.59 ± 0.03\n0.58 ± 0.03\n0.57 ± 0.03\nMASS, β=0\n34.1 ± 0.6\nEntropy\n0.57 ± 0.01\n0.58 ± 0.01\n0.55 ± 0.00\nmaxi qφ(fθ(x)|yi)\n0.61 ± 0.03\n0.59 ± 0.04\n0.59 ± 0.04\nMASS, β=1e−2, D\n29.3 ± 1.2\nEntropy\n0.62 ± 0.02\n0.64 ± 0.03\n0.59 ± 0.02\nmaxi qφ(fθ(x)|yi)\n0.50 ± 0.05\n0.54 ± 0.05\n0.47 ± 0.03\nMASS, β=1e−3, D\n31.5 ± 0.6\nEntropy\n0.61 ± 0.02\n0.62 ± 0.03\n0.58 ± 0.01\nmaxi qφ(fθ(x)|yi)\n0.62 ± 0.04\n0.63 ± 0.04\n0.58 ± 0.04\nMASS, β=1e−4, D\n32.7 ± 0.8\nEntropy\n0.61 ± 0.02\n0.61 ± 0.03\n0.59 ± 0.01\nmaxi qφ(fθ(x)|yi)\n0.65 ± 0.04\n0.63 ± 0.04\n0.62 ± 0.05\nMASS, β=0, D\n32.2 ± 1.1\nEntropy\n0.63 ± 0.01\n0.64 ± 0.02\n0.61 ± 0.01\nmaxi qφ(fθ(x)|yi)\n0.65 ± 0.05\n0.64 ± 0.05\n0.62 ± 0.06\nMinimal Achievable Sufﬁcient Statistic Learning\nTable 12. Out-of-distribution detection metrics for ResNet20 network trained on 40,000 CIFAR-10 images, with SVHN as the out-of-\ndistribution examples. Full experiment details are in Supplementary Material 7.7. Values are the mean over 4 training runs with different\nrandom seeds, plus or minus the standard deviation. Emboldened values are those for which the maximum observed mean value in the\ncolumn was within one standard deviation. Higher values are better.\nTraining Method\nTest Accuracy\nDetection Method\nAUROC\nAPR In\nAPR Out\nSoftmaxCE\n81.7 ± 0.3\nEntropy\n0.77 ± 0.02\n0.81 ± 0.02\n0.70 ± 0.02\nmaxi qφ(fθ(x)|yi)\n0.59 ± 0.03\n0.62 ± 0.03\n0.55 ± 0.02\nVIB, β=1e−3\n81.0 ± 0.3\nEntropy\n0.74 ± 0.02\n0.79 ± 0.02\n0.67 ± 0.02\nRate\n0.55 ± 0.04\n0.57 ± 0.05\n0.51 ± 0.03\nVIB, β=1e−4\n81.2 ± 0.4\nEntropy\n0.73 ± 0.02\n0.76 ± 0.03\n0.66 ± 0.02\nRate\n0.50 ± 0.02\n0.54 ± 0.02\n0.48 ± 0.01\nVIB, β=1e−5\n80.9 ± 0.5\nEntropy\n0.75 ± 0.02\n0.80 ± 0.02\n0.67 ± 0.02\nRate\n0.18 ± 0.05\n0.34 ± 0.01\n0.34 ± 0.01\nVIB, β=0\n81.5 ± 0.2\nEntropy\n0.79 ± 0.02\n0.84 ± 0.02\n0.73 ± 0.04\nRate\n0.11 ± 0.03\n0.32 ± 0.01\n0.32 ± 0.01\nMASS, β=1e−3\n75.8 ± 0.5\nEntropy\n0.74 ± 0.03\n0.77 ± 0.03\n0.69 ± 0.03\nmaxi qφ(fθ(x)|yi)\n0.37 ± 0.04\n0.43 ± 0.02\n0.42 ± 0.02\nMASS, β=1e−4\n80.6 ± 0.5\nEntropy\n0.76 ± 0.04\n0.80 ± 0.04\n0.70 ± 0.05\nmaxi qφ(fθ(x)|yi)\n0.48 ± 0.06\n0.53 ± 0.05\n0.47 ± 0.04\nMASS, β=1e−5\n81.6 ± 0.4\nEntropy\n0.77 ± 0.01\n0.82 ± 0.01\n0.71 ± 0.02\nmaxi qφ(fθ(x)|yi)\n0.54 ± 0.03\n0.58 ± 0.03\n0.51 ± 0.02\nMASS, β=0\n81.5 ± 0.2\nEntropy\n0.79 ± 0.03\n0.83 ± 0.02\n0.73 ± 0.03\nmaxi qφ(fθ(x)|yi)\n0.49 ± 0.04\n0.54 ± 0.04\n0.47 ± 0.02\nTable 13. Out-of-distribution detection metrics for ResNet20 network trained on 10,000 CIFAR-10 images, with SVHN as the out-of-\ndistribution examples. Full experiment details are in Supplementary Material 7.7. Values are the mean over 4 training runs with different\nrandom seeds, plus or minus the standard deviation. Emboldened values are those for which the maximum observed mean value in the\ncolumn was within one standard deviation. Higher values are better.\nTraining Method\nTest Accuracy\nDetection Method\nAUROC\nAPR In\nAPR Out\nSoftmaxCE\n67.5 ± 0.8\nEntropy\n0.64 ± 0.02\n0.68 ± 0.02\n0.58 ± 0.02\nmaxi qφ(fθ(x)|yi)\n0.59 ± 0.03\n0.61 ± 0.03\n0.57 ± 0.04\nVIB, β=1e−3\n66.9 ± 1.0\nEntropy\n0.59 ± 0.02\n0.63 ± 0.04\n0.54 ± 0.02\nRate\n0.72 ± 0.05\n0.73 ± 0.05\n0.67 ± 0.05\nVIB, β=1e−4\n66.4 ± 0.5\nEntropy\n0.59 ± 0.01\n0.63 ± 0.02\n0.54 ± 0.01\nRate\n0.59 ± 0.07\n0.60 ± 0.07\n0.56 ± 0.06\nVIB, β=1e−5\n67.9 ± 0.8\nEntropy\n0.61 ± 0.03\n0.65 ± 0.04\n0.56 ± 0.03\nRate\n0.39 ± 0.07\n0.42 ± 0.03\n0.43 ± 0.04\nVIB, β=0\n67.1 ± 1.0\nEntropy\n0.64 ± 0.01\n0.68 ± 0.01\n0.58 ± 0.01\nRate\n0.32 ± 0.03\n0.39 ± 0.01\n0.39 ± 0.01\nMASS, β=1e−3\n59.6 ± 0.8\nEntropy\n0.59 ± 0.02\n0.62 ± 0.03\n0.56 ± 0.02\nmaxi qφ(fθ(x)|yi)\n0.49 ± 0.07\n0.46 ± 0.06\n0.48 ± 0.08\nMASS, β=1e−4\n66.6 ± 0.4\nEntropy\n0.62 ± 0.02\n0.67 ± 0.02\n0.56 ± 0.03\nmaxi qφ(fθ(x)|yi)\n0.61 ± 0.05\n0.61 ± 0.05\n0.60 ± 0.05\nMASS, β=1e−5\n67.4 ± 1.0\nEntropy\n0.64 ± 0.02\n0.69 ± 0.03\n0.58 ± 0.01\nmaxi qφ(fθ(x)|yi)\n0.61 ± 0.08\n0.61 ± 0.06\n0.61 ± 0.09\nMASS, β=0\n67.4 ± 0.3\nEntropy\n0.64 ± 0.01\n0.68 ± 0.02\n0.58 ± 0.01\nmaxi qφ(fθ(x)|yi)\n0.55 ± 0.05\n0.56 ± 0.04\n0.54 ± 0.05\nMinimal Achievable Sufﬁcient Statistic Learning\nTable 14. Out-of-distribution detection metrics for ResNet20 network trained on 2,500 CIFAR-10 images, with SVHN as the out-of-\ndistribution examples. Full experiment details are in Supplementary Material 7.7. Values are the mean over 4 training runs with different\nrandom seeds, plus or minus the standard deviation. Emboldened values are those for which the maximum observed mean value in the\ncolumn was within one standard deviation. Higher values are better.\nTraining Method\nTest Accuracy\nDetection Method\nAUROC\nAPR In\nAPR Out\nSoftmaxCE\n50.0 ± 0.7\nEntropy\n0.51 ± 0.01\n0.52 ± 0.02\n0.49 ± 0.01\nmaxi qφ(fθ(x)|yi)\n0.63 ± 0.04\n0.62 ± 0.03\n0.63 ± 0.04\nVIB, β=1e−3\n49.5 ± 1.1\nEntropy\n0.48 ± 0.05\n0.50 ± 0.05\n0.47 ± 0.03\nRate\n0.68 ± 0.07\n0.68 ± 0.05\n0.66 ± 0.08\nVIB, β=1e−4\n49.4 ± 1.0\nEntropy\n0.47 ± 0.05\n0.50 ± 0.05\n0.47 ± 0.03\nRate\n0.66 ± 0.09\n0.65 ± 0.08\n0.66 ± 0.09\nVIB, β=1e−5\n50.0 ± 1.1\nEntropy\n0.48 ± 0.05\n0.49 ± 0.05\n0.48 ± 0.03\nRate\n0.59 ± 0.10\n0.55 ± 0.08\n0.61 ± 0.09\nVIB, β=0\n50.6 ± 0.8\nEntropy\n0.51 ± 0.07\n0.54 ± 0.08\n0.50 ± 0.06\nRate\n0.52 ± 0.20\n0.53 ± 0.15\n0.56 ± 0.17\nMASS, β=1e−3\n38.2 ± 0.7\nEntropy\n0.48 ± 0.04\n0.50 ± 0.04\n0.47 ± 0.03\nmaxi qφ(fθ(x)|yi)\n0.54 ± 0.11\n0.48 ± 0.06\n0.51 ± 0.08\nMASS, β=1e−4\n49.9 ± 1.0\nEntropy\n0.49 ± 0.04\n0.51 ± 0.05\n0.48 ± 0.03\nmaxi qφ(fθ(x)|yi)\n0.72 ± 0.08\n0.71 ± 0.08\n0.73 ± 0.08\nMASS, β=1e−5\n50.1 ± 0.5\nEntropy\n0.50 ± 0.06\n0.51 ± 0.06\n0.49 ± 0.04\nmaxi qφ(fθ(x)|yi)\n0.69 ± 0.10\n0.68 ± 0.10\n0.70 ± 0.10\nMASS, β=0\n50.2 ± 1.0\nEntropy\n0.51 ± 0.06\n0.53 ± 0.06\n0.50 ± 0.04\nmaxi qφ(fθ(x)|yi)\n0.69 ± 0.07\n0.68 ± 0.07\n0.68 ± 0.07\nMinimal Achievable Sufﬁcient Statistic Learning\n7. Supplementary Material\n7.1. Standard Deﬁnition of Minimal Sufﬁcient Statistics\nThe most common phrasing of the deﬁnition of minimal sufﬁcient statistic is:\nDeﬁnition 3 (Minimal Sufﬁcient Statistic). A sufﬁcient statistic f(X) for Y is minimal if for any other sufﬁcient statistic\nh(X) there exists a measurable function g such that f = g ◦h almost everywhere.\nSome references do not explicitly mention the “measurability” and “almost everywhere” conditions on g, but since we are in\nthe probabilistic setting it is this deﬁnition of f = g ◦h that is meaningful.\nOur preferred phrasing of the deﬁnition of minimal sufﬁcient statistic, which we use in our Introduction, is:\nDeﬁnition 4 (Minimal Sufﬁcient Statistic). A sufﬁcient statistic f(X) for Y is minimal if for any measurable function g,\ng(f(X)) is no longer sufﬁcient for Y unless g is invertible almost everywhere (i.e. there exist a measurable function g−1\nand a set A such that g−1(g(x)) = x for all x ∈A and the event {X ∈Ac} has probability zero).\nThe equivalence of Deﬁnition 3 and Deﬁnition 4 is given by the following lemma:\nLemma 2. Assume that there exists a minimal sufﬁcient statistic h(X) for Y by Deﬁnition 3. Then a sufﬁcient statistic\nf(X) is minimal in the sense of Deﬁnition 3 if and only if it is minimal in the sense of Deﬁnition 4.\nProof. We ﬁrst assume that f(X) is minimal in the sense of Deﬁnition 3. Let g be any measurable function such that g(f(X))\nis sufﬁcient for Y . By the minimality (Def. 3) of f there must exist a measurable function ˜g such that ˜g(g(f(x))) = f(x)\nalmost everywhere. This proves that f is minimal in the sense of Deﬁnition 4.\nNow assume that f(X) is minimal in the sense of Deﬁnition 4 and let ˜f(X) be another sufﬁcient statistic. Because\nh is minimal (Def. 3), there exist g1 such that h = g1 ◦˜f almost everywhere and g2 such that h = g2 ◦f almost\neverywhere. Because f is minimal (Def. 4), g2 must be one-to-one almost everywhere, i.e. there exists a ˜g2 such that\n˜g2 ◦h = ˜g2 ◦g2 ◦f = f almost everywhere. In turn, we obtain that ˜g2 ◦g1 ◦˜f = f almost everywhere, and since ˜f was\narbitrary this proves the minimality of f in the sense of Deﬁnition 3.\n7.2. The Mutual Information Between the Input and Output of a Deep Network is Inﬁnite\nTypically the mutual information between continuous random variables X and Y is given by\nI(X, Y ) =\nZ\np(x, y) log p(x, y)\np(x)p(y)dxdy,\nbut this quantity is only deﬁned when the joint density p(x, y) is integrable, which it is not in the case that Y = f(X). (The\ntechnical term for p(x, y) in this case is a “singular distribution”.) Instead, to compute I(X, f(X)) we must refer to the\n“master deﬁnition” of mutual information (Cover & Thomas, 2006), which is\nI(X, Y ) = sup\nP,Q\nI([X]P, [Y ]Q),\n(3)\nwhere P and Q are ﬁnite partitions of the range of X and Y , respectively, and [X]P is the random variable obtained by\nquantizing X using partition P, and analogously for [Y ]Q.\nFrom this deﬁnition, we can prove the following Lemma:\nLemma 3. If X and Y are continuous random variables, and there are open sets OX and OY in the support of X and Y ,\nrespectively, such that y = f(x) for x ∈OX and y ∈OY , then I(X, Y ) = ∞.\nThis includes all X and Y where Y = f(X) for an f that is continuous somewhere on its domain, e.g., any deep network\n(considered as a function from an input vector to an output vector).\nProof. Suppose X and Y satisfy the conditions of the lemma. Let OX and OY be open sets with f(OX) = OY and\nP[X ∈OX] =: δ > 0, which exist by the lemma’s assumptions. Then let Pn\nOY be a partition of OY into n disjoint sets.\nBecause Y is continuous and hence does not have any atoms, we may assume that the probability of Y belonging to each\nelement of Pn\nOY is equal to the same nonzero value δ/n. Denote by Pn\nOX the partition of OX into n disjoint sets, where\nMinimal Achievable Sufﬁcient Statistic Learning\neach set in Pn\nOX is the preimage of one of the sets in Pn\nOY . We can construct partitions of the whole domains of X and Y as\nPn\nOX ∪Oc\nX and Pn\nOY ∪Oc\nY , respectively. Using these partitions in (3), we obtain\nI(X, Y ) ≥(1 −δ) log(1 −δ) +\nX\nA∈[X]Pn\nOX\nP[X ∈A, Y ∈f(A)] log P[X ∈A, Y ∈f(A)]\nP[X ∈A]P[Y ∈f(A)]\n= (1 −δ) log(1 −δ) + n δ\nn log\nδ\nn\nδ\nn\nδ\nn\n= (1 −δ) log(1 −δ) + δ log n\nδ .\nBy letting n go to inﬁnity, we can see that the supremum in Eq. 3 is inﬁnity.\n7.3. The Change of Variables Formula for Non-invertible Mappings\nThe change of variables formula is widely used in machine learning and is key to recent results in density estimation and\ngenerative modeling like normalizing ﬂows (Rezende & Mohamed, 2015), NICE (Dinh et al., 2014), and Real NVP (Dinh\net al., 2017). But all uses of the change of variables formula in the machine learning literature that we are aware of use it\nwith respect to bijective mappings between random variables, despite the formula also being applicable to non-invertible\nmappings between random variables. To address this gap, we offer the following brief tutorial.\nThe familiar form of the change of variables formula for a random variable X with density p(x) and a bijective, differentiable\nfunction f : Rd →Rd is\nZ\nRd p(x)Jf(x) dx =\nZ\nRd p(f −1(y)) dy.\n(4)\nwhere Jf(x) =\n\f\f det ∂f(x)\n∂xT\n\f\f.\nA slightly more general phrasing of Equation 4 is\nZ\nf −1(B)\ng(x)Jf(x) dx =\nZ\nB\ng(f −1(y)) dy.\n(5)\nwhere g: Rd →R is any non-negative measurable function, and B ⊆Rd is any measurable subset of Rd.\nWe can extend Equation 5 to work in the case that f is not invertible. To do this, we must address two issues. First, if f is\nnot invertible, then f −1(y) is not a single point but rather a set. Second, if f is not invertible, then the Jacobian matrix ∂f(x)\n∂xT\nmay not be square, and thus has no well deﬁned determinant. Both issues can be resolved and lead to the following change\nof variables theorem (Krantz & Parks, 2009), which is based on the so-called coarea formula (Federer, 1969).\nTheorem 4. Let f : Rd →Rr with r ≤d be a differentiable function, g: Rd →R a non-negative measurable function,\nB ⊆Rd a measurable set, and Jf(x) =\ns\ndet\n\u0012\n∂f(x)\n∂xT\n\u0010\n∂f(x)\n∂xT\n\u0011T\u0013\n. Then\nZ\nf −1(B)\ng(x)Jf(x) dx =\nZ\nB\nZ\nf −1(y)\ng(x) dH d−r(x) dy.\n(6)\nwhere H d−r is the (d −r)-dimensional Hausdorff measure (one can think of this as a measure for lower-dimensional\nstructures in high-dimensional space, e.g. the area of 2-dimensional surfaces in 3-dimensional space).3\nWe see in Theorem 4 that Equation 6 looks a lot like Equations 4 and 5, but with f −1(y) replaced by an integral over the set\nf −1(y), which for almost every y is a (d −r)-dimensional set. And if f in Equation 6 happens to be bijective, Equation 6\nreduces to Equation 5.\n3In what follows, we will sometimes replace g by g/Jf such that the Jacobian appears on the right-hand side. Furthermore, we will\nnot only use non-negative g. This can be justiﬁed by splitting g into positive and negative parts provided that either part results in a ﬁnite\nintegral.\nMinimal Achievable Sufﬁcient Statistic Learning\nWe also see that the Jacobian determinant in Equation 5 was replaced by the so-called r-dimensional Jacobian\nv\nu\nu\ntdet\n \n∂f(x)\n∂xT\n\u0012∂f(x)\n∂xT\n\u0013T!\nin Equation 6. A word of caution is in order, as the r-dimensional Jacobian does not have the same nice properties\nfor concatenated functions as does the Jacobian in the bijective case. In particular, we cannot calculate Jf2◦f1 based\non the values of Jf1 and Jf2 because the product ∂f2(x)\n∂xT\n∂f1(x)\n∂xT\n\u0010\n∂f2(x)\n∂xT\n∂f1(x)\n∂xT\n\u0011T\ndoes not decompose into a product of\n∂f2(x)\n∂xT\n\u0010\n∂f2(x)\n∂xT\n\u0011T\nand ∂f1(x)\n∂xT\n\u0010\n∂f1(x)\n∂xT\n\u0011T\n. In other words, the trick used in techniques like normalizing ﬂows and NICE to\ncompute determinants of deep networks for use in the change of variables formula by decomposing the network’s Jacobian\ninto the product of layerwise Jacobians does not work straightforwardly in the case of non-invertible mappings.\n7.4. Motivation for Conserved Differential Information\nFirst, we present an alternative deﬁnition of conditional entropy that is meaningful for singular distributions (e.g., the joint\ndistribution p(X, f(X)) for a function f). More information on this deﬁnition can be found in Koliander et al. (2016).\n7.4.1. SINGULAR CONDITIONAL ENTROPY\nAssume that the random variable X has a probability density function pX(x) on Rd. For a given differentiable function\nf : Rd →Rr (r ≤d), we want to analyze the conditional differential entropy H(X|f(X)). Following Koliander et al.\n(2016), we deﬁne this quantity as:\nH(X|f(X)) = −\nZ\nRr pf(X)(y)\nZ\nf −1(y)\nθd−r\nPr{X∈·|f(X)=y}(x) log\n\u0000θd−r\nPr{X∈·|f(X)=y}(x)\n\u0001\ndH d−r(x) dy\n(7)\nwhere H d−r denotes (d −r)-dimensional Hausdorff measure. The function pf(X) is the probability density function of the\nrandom variable f(X). Although θd−r\nPr{X∈·|f(X)=y} can also be interpreted as a probability density, it is not the commonly\nused density with respect to Lebesgue measure (which does not exist for X|f(X) = y) but a density with respect to a\nlower-dimensional Hausdorff measure. We will analyze the two functions pf(X) and θd−r\nPr{X∈·|f(X)=y} in more detail. The\ndensity pf(X) is deﬁned by the relation\nZ\nf −1(B)\npX(x) dx =\nZ\nB\npf(X)(y) dy ,\n(8)\nwhich has to hold for every measurable set B ⊆Rr. Using the coarea formula (or the related change-of-variables theorem),\nwe see that\nZ\nf −1(B)\npX(x) dx =\nZ\nB\nZ\nf −1(y)\npX(x)\nJf(x) dH d−r(x) dy ,\n(9)\nwhere Jf(x) =\ns\ndet\n\u0012\n∂f(x)\n∂xT\n\u0010\n∂f(x)\n∂xT\n\u0011T\u0013\nis the r-dimensional Jacobian determinant. Thus, we identiﬁed\npf(X)(y) =\nZ\nf −1(y)\npX(x)\nJf(x) dH d−r(x) .\n(10)\nThe second function, namely θd−r\nPr{X∈·|f(X)=y}, is the Radon-Nikodym derivative of the conditional probability Pr{X ∈\n·|f(X) = y} with respect to H d−r restricted to the set where X|f(X) = y has positive probability (in the end, this will be\nthe set f −1(y)). To understand this function, we have to know something about the conditional distribution of X given\nf(X). Formally, a (regular) conditional probability Pr{X ∈·|f(X) = y} has to satisfy three conditions:\n• Pr{X ∈·|f(X) = y} is a probability measure for each ﬁxed y ∈Rr.\nMinimal Achievable Sufﬁcient Statistic Learning\n• Pr{X ∈A|f(X) = ·} is measurable for each ﬁxed measurable set A ⊆Rd.\n• For measurable sets A ⊆Rd and B ⊆Rr, we have\nPr{(X, f(X)) ∈A × B} =\nZ\nB\nPr{X ∈A|f(X) = y}pf(X)(y) dy .\n(11)\nIn our setting, (11) becomes\nZ\nA∩f −1(B)\npX(x) dx =\nZ\nB\nPr{X ∈A|f(X) = y}pf(X)(y) dy .\n(12)\nChoosing\nPr{X ∈A|f(X) = y} =\n1\npf(X)(y)\nZ\nA∩f −1(y)\npX(x)\nJf(x) dH d−r(x) ,\n(13)\nthe right-hand side in (12) becomes\nZ\nB\nPr{X ∈A|f(X) = y}pf(X)(y) dy =\nZ\nB\nZ\nA∩f −1(y)\npX(x)\nJf(x) dH d−r(x) dy\n=\nZ\nA∩f −1(B)\npX(x) dx ,\n(14)\nwhere the ﬁnal equality is again an application of the coarea formula. Thus, we identiﬁed\nθd−r\nPr{X∈·|f(X)=y}(x) =\npX(x)\nJf(x) pf(X)(y) .\n(15)\nAlthough things might seem complicated up to this point, they simplify signiﬁcantly once we put everything together. In\nparticular, inserting (15) into (7), we obtain\nH(X|f(X)) = −\nZ\nRr pf(X)(y)\nZ\nf −1(y)\npX(x)\nJf(x) pf(X)(y) log\n\u0012\npX(x)\nJf(x) pf(X)(y)\n\u0013\ndH d−r(x) dy\n= −\nZ\nRr\nZ\nf −1(y)\npX(x)\nJf(x) log\n\u0012\npX(x)\nJf(x) pf(X)(y)\n\u0013\ndH d−r(x) dy\n= −\nZ\nRd pX(x) log\n\u0012\npX(x)\nJf(x) pf(X)(f(x))\n\u0013\ndx\n(16)\n= H(X) +\nZ\nRd pX(x) log\n\u0000Jf(x)pf(X)(f(x))\n\u0001\ndx\n= H(X) +\nZ\nRd pX(x) log\n\u0000pf(X)(f(x))\n\u0001\ndx +\nZ\nRd pX(x) log\n\u0000Jf(x)\n\u0001\ndx\n= H(X) +\nZ\nRr\nZ\nf −1(y)\npX(x)\nJf(x) log\n\u0000pf(X)(f(x))\n\u0001\ndH d−r(x) dy + E\n\u0002\nlog\n\u0000Jf(X)\n\u0001\u0003\n(17)\n= H(X) +\nZ\nRr\nZ\nf −1(y)\npX(x)\nJf(x) dH d−r(x) log\n\u0000pf(X)(y)\n\u0001\ndy + E\n\u0002\nlog\n\u0000Jf(X)\n\u0001\u0003\n= H(X) +\nZ\nRr pf(X)(y) log\n\u0000pf(X)(y)\n\u0001\ndy + E\n\u0002\nlog\n\u0000Jf(X)\n\u0001\u0003\n= H(X) −H(f(X)) + E\n\u0002\nlog\n\u0000Jf(X)\n\u0001\u0003\n(18)\nwhere (16) and (17) hold by the coarea formula.\nSo, altogether we have that for a random variable X and a function f, the singular conditional entropy between X and f(X)\nis\nH(X|f(X)) = H(X) −H(f(X)) + E\n\u0002\nlog\n\u0000Jf(X)\n\u0001\u0003\n.\n(19)\nThis quantity can loosely be interpreted as being the difference in differential entropies between X and f(X) but with an\nadditional term that corrects for any “uninformative” scaling that f does.\nMinimal Achievable Sufﬁcient Statistic Learning\n7.4.2. CONSERVED DIFFERENTIAL INFORMATION\nFor random variables that are not related by a deterministic function, mutual information can be expanded as\nI(X, Y ) = H(X) −H(X|Y )\n(20)\nwhere H(X) and H(X|Y ) are differential entropy and conditional differential entropy, respectively. As we would like to\nmeasure information between random variables that are deterministically dependent, we can mimic this behavior by deﬁning\nfor a Lipschitz continuous mapping f:\nC(X, f(X)) := H(X) −H(X|f(X)) .\n(21)\nBy (18), this can be simpliﬁed to\nC(X, f(X)) = H(f(X)) −E\n\u0002\nlog\n\u0000Jf(X)\n\u0001\u0003\n(22)\nyielding our deﬁnition of CDI.\n7.5. Proof of CDI Data Processing Inequality\nCDI Data Processing Inequality (Theorem 1)\nFor Lipschitz continuous functions f and g with the same output space,\nC(X, f(X)) ≥C(X, g(f(X))\nwith equality if and only if g is one-to-one almost everywhere.\nProof. We calculate the difference between C(X, f(X)) and C(X, g(f(X))).\nC(X, f(X)) −C(X, g(f(X)))\n(23)\n= H(f(X)) −EX\n\u0002\nlog Jf(X)\n\u0003\n−H(g(f(X))) + EX\n\u0002\nlog Jg◦f(X)\n\u0003\n= H(f(X)) −H(g(f(X))) + EX\n\u0014\nlog Jg(f(X))Jf(X)\nJf(X)\n\u0015\n(24)\n= −EX[log pf(X)(f(X))] + EX\n\u0014\nlog\n\u0012\nX\nz∈g−1(g(f(X)))\npf(X)(f(z))\nJg(f(z))\n\u0013\u0015\n+ EX[log Jg(f(X))]\n(25)\n= EX\n\nlog\n\n\nP\nz∈g−1(g(f(X)))\npf(X)(f(z))\nJg(f(z))\npf(X)(f(X))\nJg(f(X))\n\n\n\n\n(26)\nwhere (24) holds because the Jacobian determinant Jg◦f can be decomposed as g has the same domain and codomain and (25)\nholds because the probability density function of g(f(X)) can be calculated as pg(f(X))(z) = P\nz∈g−1(g(f(X)))\npf(X)(f(z))\nJg(f(z))\nusing a change of variables argument. The resulting term in (26) is clearly always nonnegative which proves the inequality.\nTo prove the equality statement, we ﬁrst assume that (26) is zero. In this case, P\nz∈g−1(g(f(x)))\npf(X)(f(z))\nJg(f(z))\n=\npf(X)(f(x))\nJg(f(x))\nalmost everywhere. Of course, we also have that pf(X)(f(x)) > 0 almost everywhere. Thus, there exists a set A of\nprobability one such that P\nz∈g−1(g(f(x)))\npf(X)(f(z))\nJg(f(z))\n=\npf(X)(f(x))\nJg(f(x))\nand pf(X)(f(x)) > 0 for all x ∈A. In particular, the\nset g−1(g(f(x))) ∩A = {f(x)} and hence g is one-to-one almost everywhere.\nFor the other direction, assume that there exists ˜g such that ˜g(g(f(x))) = f(x) almost everywhere. We can assume without\nloss of generality that pf(X)(f(x)) = 0 for all x that do not satisfy this equation. Restricting the expectation in (26) to the\nvalues that satisfy ˜g(g(f(x))) = f(x) does not change the expectation and gives the value zero.\n7.6. Theorem 3 Only Holds in the Reverse Direction for Continuous X\nThe speciﬁc claim we are making is as follows:\nMinimal Achievable Sufﬁcient Statistic Learning\nTheorem 5. Let X be a continuous random variable drawn according to a distribution p(X|Y ) determined by the discrete\nrandom variable Y . Let F be the set of measurable functions of X to any target space. If f(X) is a minimal sufﬁcient\nstatistic of X for Y then\nf ∈arg min\nS∈F I(X, S(X))\ns.t. I(S(X), Y ) = max\nS′∈F I(S′(X), Y ).\n(27)\nHowever, there may exist a function f satisfying (27) such that f(X) is not a minimal sufﬁcient statistic.\nProof. First, we prove the forward direction. According to Lemma 1, Z = f(X) is a sufﬁcient statistic for Y if and only\nif I(Z, Y ) = I(X, Y ) = maxS′ I(S′(X), Y ). To show the minimality condition in (27) for a minimal sufﬁcient statistic,\nassume that there exists S(X) such that I(S(X), Y ) = maxS′∈F I(S′(X), Y ) and I(X, S(X)) < I(X, f(X)). Because\nf is assumed to be a minimal sufﬁcient statistic, there exists g such that f(X) = g(S(X)) and by the data-processing\ninequality I(X, S(X)) ≥I(X, f(X)), a contradiction.\nNext, we give an example of a function satisfying (27) such that f(X) is not a minimal sufﬁcient statistic. The example is\nthe case when I(X, f(X)) is not ﬁnite, as is the case when f is a deterministic function and X is continuous. (See Lemma\n3.) In this case, I(X, S(X)) is inﬁnite for all deterministic, sufﬁcient statistics S. Thus the set arg minS I(X, S(X))\ncontains not only the minimal sufﬁcient statistics, but all deterministic sufﬁcient statistics. As a concrete example,\nconsider two i.i.d. normally-distributed random variables with mean µ: X = (X1, X2) ∼N(µ, 1). T(X) = X1+X2\n2\nis a\nminimal sufﬁcient statistic for µ. T ′(X) = ( X1+X2\n2\n, X1 · X2) is a non-minimal sufﬁcient statistic for µ. However, both\nstatistics satisfy T, T ′ ∈arg minS∈F I(X, S(X)) since minS∈F I(X, S(X)) = ∞under the constraint I(S(X), Y ) =\nmaxS′∈F I(S′(X), Y ).\n7.7. Experiment Details\nCode to reproduce all experiments is available online at https://github.com/mwcvitkovic/MASS-Learning.\n7.7.1. DATA\nIn all experiments above, the models were trained on the CIFAR-10 dataset (Krizhevsky, 2009). In the out-of-distribution\ndetection experiments, the SVHN dataset (Netzer et al., 2011) was used as the out-of-distribution dataset. All channels in all\ndatapoints were normalized to have zero mean and unit variance across their dataset. No data augmentation was used in any\nexperiments.\n7.7.2. NETWORKS\nThe SmallMLP network is a 2-hidden-layer, fully-connected network with elu nonlinearities (Clevert et al., 2015). The\nﬁrst hidden layer contains 400 hidden units; the second contains 200 hidden units. Batch norm was applied after the linear\nmapping and before the nonlinearity of each hidden layer. Dropout, when used, was applied after the nonlinearity of\neach hidden layer. When used in VIB and MASS, the representation fθ(x) was in R15, with the VIB encoder outputting\nparameters for a fully-covariant Gaussian distribution in R15. The marginal distribution in VIB and each component of the\nvariational distribution qφ (one component for each possible output class) in MASS were both mixtures of 10 full-covariance,\n15-dimensional multivariate Gaussians.\nThe ResNet20 network is the 20-layer residual net of He et al. (2016). We adapted our implementation from https:\n//github.com/akamaster/pytorch_resnet_cifar10, to whose authors we are very grateful. When used in\nVIB and MASS, the representation fθ(x) was in R20, with the VIB encoder outputting parameters for a diagonally-covariant\nGaussian distribution in R20. The marginal distribution in VIB and each component of the the variational distribution\nqφ (one component for each possible output class) in MASS were both mixtures of 10 full-covariance, 20-dimensional\nmultivariate Gaussians.\nIn experiments where a distribution qφ(fθ(x)|y) is used in conjunction with a function fθ trained by SoftmaxCE, each\ncomponent of qφ(fθ(x)|y) was a mixture of 10 full-covariance, 10-dimensional multivariate Gaussians, the parameters φ of\nwhich were estimated by MLE on the training set.\nMinimal Achievable Sufﬁcient Statistic Learning\n7.7.3. TRAINING\nThe SmallMLP network in all experiments and with all training methods was trained using the Adam optimizer (Kingma &\nBa, 2015) with a learning rate of 0.0005 for 100,000 steps of stochastic gradient descent, using minibatches of size 256. All\nquantities we report in this paper were fully-converged to stable values by 100,000 steps. When training VIB, 5 encoder\nsamples per datapoint were used during training, and 10 during testing. When training MASS, the learning rate of the\nparameters of the variational distribution qφ was set at 2.5e−5 to aid numerical stability.\nThe ResNet20 network in all experiments and with all training methods was trained using SGD with an initial learning rate\nof 0.1, decayed by a multiplicative factor of 0.1 at epochs 100 and 150, a momentum factor of 0.9, and minibatches of size\n128. These values were taken directly from the original paper (He et al., 2016). However, unlike the original paper, we did\nnot use data augmentation in order to keep the comparison between different numbers of training points more rigorous. This,\ncombined with the smaller number of training points used, accounts for the around 82% accuracy we observe on CIFAR-10\ncompared to the around 91% accuracy in the original paper. We trained the network for 70,000 steps of stochastic gradient\ndescent. All quantities we report in this paper were fully-converged to stable values by 70,000 steps. When training VIB, 10\nencoder samples per datapoint were used during training, and 20 during testing. When training MASS, the learning rate of\nthe parameters of the variational distribution was the same as those of the network.\nThe values of β we chose for VIB and MASS were selected so that the largest β value used in each experiment was much\nlarger in magnitude than the remaining terms in the VIB or MASS training loss, and the smallest β value used was much\nsmaller than the remaining terms. We made this choice in the hope of clearly observing the effect of the β parameter and\nmore fairly comparing SoftmaxCE, VIB, and MASS. But we note that a ﬁner-tuning of the β parameter would likely result in\nbetter performance for both VIB and MASS. We also note that the reason we omit a β = 0 run for VIB with the SmallMLP\nnetwork was that we could not prevent training from failing due to numerical instability with β = 0 with this network.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-05-19",
  "updated": "2019-06-11"
}