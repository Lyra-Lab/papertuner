{
  "id": "http://arxiv.org/abs/1905.09195v2",
  "title": "On the minimax optimality and superiority of deep neural network learning over sparse parameter spaces",
  "authors": [
    "Satoshi Hayakawa",
    "Taiji Suzuki"
  ],
  "abstract": "Deep learning has been applied to various tasks in the field of machine\nlearning and has shown superiority to other common procedures such as kernel\nmethods. To provide a better theoretical understanding of the reasons for its\nsuccess, we discuss the performance of deep learning and other methods on a\nnonparametric regression problem with a Gaussian noise. Whereas existing\ntheoretical studies of deep learning have been based mainly on mathematical\ntheories of well-known function classes such as H\\\"{o}lder and Besov classes,\nwe focus on function classes with discontinuity and sparsity, which are those\nnaturally assumed in practice. To highlight the effectiveness of deep learning,\nwe compare deep learning with a class of linear estimators representative of a\nclass of shallow estimators. It is shown that the minimax risk of a linear\nestimator on the convex hull of a target function class does not differ from\nthat of the original target function class. This results in the suboptimality\nof linear methods over a simple but non-convex function class, on which deep\nlearning can attain nearly the minimax-optimal rate. In addition to this\nextreme case, we consider function classes with sparse wavelet coefficients. On\nthese function classes, deep learning also attains the minimax rate up to log\nfactors of the sample size, and linear methods are still suboptimal if the\nassumed sparsity is strong. We also point out that the parameter sharing of\ndeep neural networks can remarkably reduce the complexity of the model in our\nsetting.",
  "text": "On the minimax optimality and superiority of deep neural network\nlearning over sparse parameter spaces\nSatoshi Hayakawa∗1 and Taiji Suzuki†1,2\n1Graduate School of Information Science and Technology, The University of Tokyo\n2Center for Advanced Intelligence Project, RIKEN\nAbstract\nDeep learning has been applied to various tasks in the ﬁeld of machine learning and has shown superiority\nto other common procedures such as kernel methods. To provide a better theoretical understanding of the\nreasons for its success, we discuss the performance of deep learning and other methods on a nonparametric\nregression problem with a Gaussian noise. Whereas existing theoretical studies of deep learning have been\nbased mainly on mathematical theories of well-known function classes such as H¨older and Besov classes,\nwe focus on function classes with discontinuity and sparsity, which are those naturally assumed in practice.\nTo highlight the eﬀectiveness of deep learning, we compare deep learning with a class of linear estimators\nrepresentative of a class of shallow estimators. It is shown that the minimax risk of a linear estimator on\nthe convex hull of a target function class does not diﬀer from that of the original target function class. This\nresults in the suboptimality of linear methods over a simple but non-convex function class, on which deep\nlearning can attain nearly the minimax-optimal rate. In addition to this extreme case, we consider function\nclasses with sparse wavelet coeﬃcients. On these function classes, deep learning also attains the minimax\nrate up to log factors of the sample size, and linear methods are still suboptimal if the assumed sparsity is\nstrong. We also point out that the parameter sharing of deep neural networks can remarkably reduce the\ncomplexity of the model in our setting.\nKeywords— neural network, deep learning, linear estimator, nonparametric regression, minimax optimality\n1\nIntroduction\nDeep learning has been successfully applied to a number of machine learning problems, including image anal-\nysis and speech recognition (Schmidhuber, 2015; Goodfellow et al., 2016). However, the rapid expansion of its\napplications has preceded a thorough theoretical understanding, and thus the theoretical properties of neural\nnetworks and their learning have not yet been fully understood. This paper aims to summarize recent develop-\nments in theoretical analyses of deep learning and to provide new approximation and estimation error bounds\nthat theoretically conﬁrm the superiority of deep learning to other representative methods.\nIn this section, we present an overview of the paper; here, we prioritize understandability over strict mathe-\nmatical rigor. Some formal deﬁnitions and restrictions, such as for measurability and integrability, are presented\nin later sections.\n1.1\nNonparametric regression\nThroughout this paper, our intent is to demonstrate the superiority of the deep learning approach to other\nmethods. To do so, we consider a simple nonparametric regression problem and compare the performance of\nvarious approaches in that setting. The nonparametric regression problem we analyze is formulated as follows:\nWe observe n i.i.d. input–output pairs (Xi, Yi) ∈[0, 1]d × R generated by the model\nYi = f ◦(Xi) + ξi,\ni = 1, . . . , n,\nwhere ξi is an i.i.d. noise independent of inputs. The object is to estimate f ◦from the observed\ndata.\n∗satoshi hayakawa@mist.i.u-tokyo.ac.jp\n†taiji@mist.i.u-tokyo.ac.jp\n1\narXiv:1905.09195v2  [stat.ML]  20 Sep 2019\nThis problem setting has been commonly used in statistical learning theory and is not limited to deep learning\n(Yang and Barron, 1999; Zhang et al., 2002; Tsybakov, 2008). In this paper, we assume the noise follows a\nGaussian distribution.\nIn this scenario, a neural network (architecture) is treated as a set of functions F ⊂{f : [0, 1]d →R}.\nOther estimation methods such as kernel ridge regression and wavelet threshold estimators are also regarded\nas such mappings (Bishop, 2006; Donoho and Johnstone, 1994). In this paper, we evaluate the performance\nof estimators by the expected mean squared error E\nh\n∥bf −f ◦∥2\nL2\ni\n(we call this quantity the “estimation error”\nfor simplicity) dependent on n following convention (Wang et al., 2014; Schmidt-Hieber, 2017; Suzuki, 2019),\nwhere the expectation is taken with respect to the training data. Usually, the L2(PX) norm (where PX is the\ndistribution of Xi) has been used in existing studies instead of the Lebesgue L2 norm, but in evaluation the\nupper- (and lower-) boundedness of the density is typically assumed, so for simplicity, we treat Xi as uniformly\ndistributed.\nIf we ﬁx the set of true functions F◦(called a hypothesis space),\nsup\nf ◦∈F◦E\nh\n∥bf −f ◦∥2\nL2\ni\nis the worst-case performance of the estimator (Xi, Yi)n\ni=1 7→bf. We are interested in its asymptotic convergence\nrate with respect to n, the sample size. The minimax rate is determined by the convergence rate of\ninf\n(Xi,Yi)7→b\nf\nsup\nf ◦∈F◦E\nh\n∥bf −f ◦∥2\nL2\ni\n,\nwhere inf is taken over all possible estimators. We compare this to the convergence rate of ﬁxed (with respect\nto n) sequences of estimators determined by some learning procedure such as deep learning to evaluate how\neﬃcient the estimation method is.\nAs a competitor of deep learning, a class of “linear estimators” is considered. Here, we say an estimator is\nlinear if it depends linearly on the outputs Yi; it is expressed as\nbf(x) =\nn\nX\ni=1\nYiϕi(x; X1, . . . , Xn).\nThis estimator class includes several practical estimators such as kernel ridge regression and the Nadaraya–\nWatson estimator. The minimax rate in the class of linear estimators can be slower under some settings; e.g.,\ninf\n(Xi,Yi)7→b\nf\nsup\nf ◦∈F◦E\nh\n∥bf −f ◦∥2\nL2\ni\n≤n−γ\ninf\nb\nf:linear\nsup\nf ◦∈F◦E\nh\n∥bf −f ◦∥2\nL2\ni\nholds for some γ > 0 (for most cases, we consider only the polynomial order). Such situations were reported\nearlier by several authors (Korostelev and Tsybakov, 1993; Donoho and Johnstone, 1998; Zhang et al., 2002).\nIn terms of deep learning analysis, a comparison of deep learning with linear methods has been performed by\nImaizumi and Fukumizu (2019). The present paper also shows the suboptimality of linear methods (Table 1)\nfor sparse function classes, which we deﬁne later.\nOur main contribution here is that we ﬁnd a quite simple and natural function class I0\nΦ for which deep learning\nattains nearly the optimal rate, whereas linear methods are not able to converge faster than the suboptimal rate\nO(n−1/2). In the next subsection, we explain how to treat and analyze deep learning in the context of statistical\nlearning theory.\n1.2\nRelated work on estimation of deep neural networks\nDeep neural networks have a structure of alternating linear (or aﬃne) transformations and nonlinear transfor-\nmations; i.e., in one layer x is transformed to ρ(Wx −v), where W is a matrix and v is a vector and ρ is a\nnonlinear function called an activation function. It is known that the repeated operation of this transformation\ngives a nice approximation of a wide class of nonlinear functions.\nTraditionally, sigmoidal functions have been commonly used as activation functions:\nσ : R →R,\nwith\nlim\nt→∞σ(t) = 1,\nlim\nt→−∞σ(t) = 0.\nIt is known that the set of functions realized by shallow networks with continuous sigmoidal activation is dense\nin any Lp space unless the number of parameters is not limited (Cybenko, 1989). However, a similar result\n2\nTable 1: Estimated error bound: deep learning vs. linear methods\nTarget class\nBs\np,q([0, 1]) (p < 2)\nI0\nΦ\nKp\nΨ (p < 1)\nDeep learning\neO(n−\n2s\n2s+1 )\neO(n−1)\n(Sec. 5.3)\neO(n−\n2α\n2α+1 ) (α = 1\np −1\n2)\n(Sec. 5.4)\nReference\nSuzuki (2019)\nThis work\nLinear methods\nΩ(n−\n2γ\n2γ+1 ) (γ = s + 1\n2 −1\np)\nΩ(n−1\n2 )\n(Sec. 3.2, 5.3, 5.4)\nReference\nDonoho and Johnstone (1998)\nZhang et al. (2002)\nThis work\nNote. More precisely, the actual target function classes are the unit balls of the classes as written. eO means O up to\npoly-log factors, and for each class shown in this table, deep learning attains the minimax-optimal rate in the sense of\neO. Bs\np,q denotes the Besov space with parameters (s, p, q), and the parameters are additionally required to satisfy\ns > 1/p and p, q ≥1 or else s = p = q = 1.\nhas also been shown for non-sigmoidal activation cases (Sonoda and Murata, 2017). In particular, the Rectiﬁed\nLinear Unit (ReLU) activation function ρ(x) = max{x, 0} has shown practical performance (Glorot et al., 2011)\nand is now widely used.\nBasically, deep learning trains a network by minimizing the empirical risk with some regularization:\nminimize 1\nn\nn\nX\ni=1\n(f(Xi) −Yi)2 + λ(f)\nsubject to f ∈F,\nwhere λ(f) is the regularization term and F is the set of functions that are realizations of a speciﬁc neural\nnetwork architecture.\nThis optimization is usually carried out by Stochastic Gradient Descent (SGD) or a\nvariant of it, and the output is not necessarily the global minimum (Goodfellow et al., 2016). In the present\npaper, however, we do not treat this optimization aspect, and we assume an ideal optimization.\nThe number of parameters in deep learning tends to be much larger than the sample size, and hence without\nany regularization, deep models can overﬁt the training sample. To overcome this issue, existing studies have\nutilized sparse regularization to obtain networks with a small number of nonzero parameters. This enables us\nto obtain a tight estimate of the error bounds using the result of approximation error analysis (see Section 2.2).\nYarotsky (2017) reported the eﬀectiveness of ReLU activation in terms of approximation ability, and the\nresult has been exploited in estimation theory for deep learning (Schmidt-Hieber, 2017; Suzuki, 2019). Their\ntarget function classes are H¨older space Cs and Besov space Bs\np,q, which are compatible with functional analysis\nor the theory of diﬀerential equations. Schmidt-Hieber (2017) also pointed out that deep learning is superior to\nlinear estimators when the target function is of the form f ◦(x) = g(w⊤x), with g having some H¨older smoothness.\nIn addition, Imaizumi and Fukumizu (2019) treated estimation theory for piecewise smooth functions using the\napproximation theory described in Petersen and Voigtlaender (2018). This paper investigates new target classes\nto demonstrate the eﬀectiveness of deep learning with ReLU activation.\n1.3\nContribution of this paper\nThe situations where deep learning shows speriority to linear estimators have been studied with particular\ntheories:\n• f ◦(x) = g(w⊤x) with g being smooth (Schmidt-Hieber, 2017),\n• piece-wise smooth functions (Imaizumi and Fukumizu, 2019),\n• Besov spaces in a certain rage of parameters (Suzuki, 2019).\nThese situations can be understood mathematically through the non-convexity of models. To highlight this\nproperty, we introduce a sparse function class (and therefore non-convex) and investigate the generalization\nability of deep learning and linear estimators over the space to see how sparsity (non-convexity) aﬀects the\nestimation error. We indeed show that linear estimators have a critical disadvantage when the target class is\n3\nnon-convex (Section 3). In contrast, deep learning is shown to have the optimality over the sparse function\nclasses we deﬁne (Section 5). These classes are natural in the sense we describe in the next paragraph.\nTable 2: Correspondence between mathematical features and the real-world things\nFeature\nReal-world counterpart\nExamples\nnonparametric regression\nreal-world estimation problems\nimage denoising,\nclassiﬁcation problem, style transfer\nbasis function\nlocalized pattern\npronunciation of vowels,\npainting style, fractal structure in nature\nsparse combination\nof basis functions\nreal data\nspeech of one person,\na painting, a noisy picture\nsparsity\nlow dimensionality\nof actual information\nnatural language,\nhandwriting alphabets\nThe major diﬀerence between existing studies and this work is that we assume an explicit sparsity of target\nclasses, which are deﬁned parametrically. This kind of scenario seems to occur in practice; for example, speech\ndata for a speciﬁc person are supposed to be a sparse linear combination of the person’s pronunciation of\neach letter, and paintings by a speciﬁc painter may be regarded as combinations of patterns (see Table 2).\nIndeed, when we carry out the wavelet expansion to a natural image, its relatively few large coeﬃcients can well\nreconstruct the original image, as is exploited in the ﬁeld of compressive sensing (Cand`es and Wakin, 2008).\nbasis function ψ\nψ1\nψ2\nJ p\nψ1\ninﬁnite\nsparse combination\nJ p\nψ2\ninﬁnite\nsparse combination\nKp\nΨ\nﬁnite combination\nI0\nΨ\nﬁnite combination\nFigure 1: Interpretation of function classes\nWe reﬂect this property to deﬁne new function classes (Section 4). In particular, we introduce a function\nclass I0\nΦ and Kp\nΨ (and J p\nψ as well) with a parameter p > 0 controlling the “sparsity” of the function class. Figure\n1 gives an interpretation of the motivations and deﬁnitions of these function classes. More precisely, p controls\nthe sparsity of coeﬃcients of “inﬁnite sparse combination” appearing in Figure 1 through a criterion called\n“weak ℓp norm” (see Deﬁnition 4.4 for details). Here, “combination” roughly means linear combination, but\nprecisely includes aﬃne transform in the input; i.e., the combination of f1, . . . , fm can be generally expressed\nas f = Pm\ni=1 cifi(Ai · −bi).\n4\nOn the basis of our new function classes, we show that deep learning is superior to linear methods and actually\nattains nearly the minimax-optimal rate over each target class (Table 1). As an extreme case (corresponding\nto the sparsity level p = 0), we treat the class of piecewise constant functions, for which the convergence rate\nof the linear estimators is Ω(n−1/2), whereas deep learning attains the near-minimax rate eO(n−1). This quite\nsimply demonstrates the scenario described in Imaizumi and Fukumizu (2019). For 0 < p < 1, we also show\nthat deep learning attains the nearly minimax-optimal rate eO(n−\n2α\n2α+1 ) in estimating a function in Kp\nΨ, where\nα = 1/p−1/2. Here, we have\n2α\n2α+1 > 1\n2, and the diﬀerence between deep and linear becomes larger as p becomes\nsmaller (i.e., as the sparsity becomes stronger). Kp\nΨ has another parameter, β, which controls the rate of decay\nof coeﬃcients of the function class. Surprisingly, we even ﬁnd that the minimax rate of linear estimators can\nbecome arbitrarily slow under the same sparsity p (and the same order of covering entropy) if the value of β\nis varied. Although we do not yet have the upper bound for the convergence rate of deep learning over the\nrange of parameter values producing this situation (see Theorem 4.15 and Remark 4.16), this indicates that the\ndiﬀerence between deep learning and linear estimators could be arbitrary large. These diﬀerences essentially\narise from the non-convexity of the model. That is, as the non-convexity of the model becomes stronger, the\ndiﬀerence becomes larger.\nIn addition, we see that deep learning takes advantage of wavelet expansions with sparsity because a neural\nnetwork can eﬃciently approximate functions of the form P\ni cif(Ai · −bi) if its subnetwork can approximate\nthe “basis” function f precisely as is also mentioned in B¨olcskei et al. (2017). From this perspective, we see\nthat parameter sharing, mentioned in Section 5.5, is also eﬀective. It can also be said that this paper expands\nthe approximation theory argued in B¨olcskei et al. (2017) to estimation theory over sparse parameter spaces.\nThus, the contribution of this paper is summarized as follows:\n• To deal with sparsity in machine learning, we deﬁne function classes I0\nΦ and Kp\nΨ with a parameter p\ncontrolling the sparsity. We also consider the nonparametric regression problem on these target classes\nand derive the minimax lower bounds of estimation error.\n• We consider linear estimators, which are a competitor of deep learning, investigating them by evaluating\ntheir estimation error over sparse target classes. We show that linear estimators can only attain suboptimal\nrates on sparse and non-convex models and even become arbitrarily slow under the same sparsity with other\nparameters varying. This also gives a uniﬁed understanding of existing studies which describe situations\nwhere deep learning is superior to linear methods.\n• To demonstrate the learning ability of the deep ReLU network on sparse spaces, we construct sparse\nneural networks that nearly attain minimax-optimal rates. It is also shown that parameter sharing in the\nconstruction of neural networks is eﬀective on sparse target classes.\nWe give a brief overview of each section in the following.\nIn Section 2, we introduce general methods used in statistical learning theory, presenting our own proofs or\narguments to the maximum extent possible. Section 2.2 presents an information-theoretic way to obtain a lower\nbound for the minimax rate. Also, the method for evaluating an estimation error by using an approximation\nerror is given. Evaluations of linear minimax rates are given in Section 3. We prove that linear estimators\ncannot distinguish between a function class and its convex hull, and as a consequence linear minimax rates\ncan be rather slower than ordinal minimax rates. Section 4 provides the deﬁnitions of our own target function\nclasses. The ℓ0 norm and the wℓp quasi-norm of coeﬃcients in linear combinations are introduced as indicators\nof sparsity. The minimax lower bounds for the deﬁned classes are also given (which are revealed to be optimal up\nto log factors in the section that follows). In Section 5, we show that deep learning attains the nearly minimax\nrate for deﬁned function classes. In addition, we propose that parameter sharing can be a means of reducing\ncomplexities in regularized networks. Finally, Section 6 provides a summary and presents future directions for\nthis work.\n1.4\nNotation\nWe use the following notation throughout the paper.\n• ∥· ∥∞and ∥· ∥0 are deﬁned as\n∥v∥∞:= max\n1≤i≤m |vi|,\n∥v∥0 := |{1 ≤i ≤m | vi ̸= 0}|\nfor a vector v = (v1, . . . , vm)⊤∈Rm. They are deﬁned similarly for real matrices.\n5\n• As a natural extension of ∥· ∥0, ∥a∥ℓ0 denotes the number of nonzero terms in the sequence a = (ai)∞\ni=1.\n• For p > 0 and a real sequence a = (ai)∞\ni=1, the ℓp norm of a is deﬁned as\n∥a∥ℓp :=\n ∞\nX\ni=1\n|ai|p\n!1/p\n,\nand ℓp denotes the set of all real sequences with a ﬁnite ℓp norm.\n2\nGeneral theories in statistical estimation\n2.1\nGeneral settings and notation\nLet us consider the following regression model. We observe i.i.d. random variables (Xi, Yi) generated by\nYi = f ◦(Xi) + ξi,\ni = 1, 2, . . . , n.\n(1)\nHere, each ξi is an observation noise independent of other variables. In this paper, we use settings such that each\nXi is d-dimensional and uniformly distributed on [0, 1]d, each Yi is one-dimensional, and ξi’s are i.i.d. centered\nGaussian variables with variance σ2 (σ > 0). For simplicity, we sometimes use the notation Xn := (X1, . . . , Xn),\nY n := (Y1, . . . , Yn), and Zn := (Xi, Yi)n\ni=1.\nRemark 2.1. In the following, we often write only bf to indicate an estimator where we should write (Xi, Yi)n 7→\nbf (this mapping is supposed to be measurable). For example, inf(Xi,Yi)n\ni=17→b\nf∈F is simply denoted by inf b\nf∈F.\nIn addition, for the case F = L2([0, 1]d), we omit F and simply write inf b\nf.\nTo evaluate the quality of estimators, we need to adopt some evaluation criteria. For a ﬁxed f ◦and a\nfunction f ∈L2([0, 1]d), we have\nE[(f(X) −Y )2] = E[(f(X) −f ◦(X))2] −2E[ξ(f(X) −f ◦(X))] + E[ξ2]\n= E[(f(X) −f ◦(X))2] + σ2\n= ∥f −f ◦∥2\nL2 + σ2.\nThis implies that the magnitude of the expected error E[(f(Xi) −Yi)2] depends only on that of the L2 distance\n∥f −f ◦∥2\nL2. This leads to the following deﬁnition for a performance criterion.\nDeﬁnition 2.2. The L2 risk for an estimator bf is deﬁned as\nR( bf, f ◦) := E\nh\n∥bf −f ◦∥2\nL2\ni\n.\nFor a model F◦⊂L2([0, 1]d), the minimax L2 risk over F◦is deﬁned as\ninf\nb\nf\nsup\nf ◦∈F◦R( bf, f ◦) = inf\nb\nf\nsup\nf ◦∈F◦E\nh\n∥bf −f ◦∥2\nL2\ni\n,\nwhere bf runs over all estimators (measurable functions).\nWe evaluate the quality of an estimator bf by this L2 risk and compare it with the minimax-optimal risk.\nRemark 2.3. We omit n from the notation because it is treated as a constant when we consider a single\nregression problem.\nHowever, as n goes to ∞, the minimax risk converges to 0, and in this paper we are\ninterested in the convergence rate of the minimax risk.\n2.2\nRelationships between complexity and minimax risk\nIn this section, we introduce a key procedure to evaluate the minimax risk. To do so, we deﬁne complexity\nmeasures called ε-entropy (ε may be taken place by any positive real number), which formally represent com-\nplexities of (totally bounded) metric spaces. This kind of complexity of F◦profoundly aﬀects the convergence\nrate of the minimax risk (Yang and Barron, 1999).\n6\nDeﬁnition 2.4. (van der Vaart and Wellner, 1996; Yang and Barron, 1999) For a metric space (S, d) and ε > 0,\n• a ﬁnite subset T is called ε-packing if d(x, y) > ε holds for any x, y ∈T with x ̸= y, and the logarithm of the\nmaximum cardinality of an ε-packing subset is called the packing ε-entropy and is denoted by M(S,d)(ε);\n• a ﬁnite set U ⊂S is called ε-covering if for any x ∈S there exists y ∈U such that d(x, y) ≤ε, and the\nlogarithm of the minimum cardinality of an ε-covering set is called the covering ε-entropy and is denoted\nby V(S,d)(ε).\nHere, S is the completion of S with respect to the metric d.\nThe concept of ε-entropy is useful to obtain a lower bound of the minimax risk of some function class\nF◦.\nLet F◦⊂L2([0, 1]d) be the class of true functions, equipped with the L2 metric.\nFor simplicity, let\nV (ε) = V(F◦,∥·∥L2)(ε) and M(ε) = M(F◦,∥·∥L2)(ε). Then, the following theorem holds.\nTheorem 2.5. (Yang and Barron, 1999, Theorem 1) In the Gaussian regression model, suppose there exist\nδ, ε > 0 such that\nV (ε) ≤nε2\n2σ2 ,\nM(δ) ≥2nε2\nσ2\n+ 2 log 2.\nThen we have\ninf\nb\nf\nsup\nf∈F◦Pf\n\u0012\n∥bf −f∥L2 ≥δ\n2\n\u0013\n≥1\n2,\ninf\nb\nf\nsup\nf∈F◦Ef\nh\n∥bf −f∥2\nL2\ni\n≥δ2\n8 ,\nwhere Pf is the probability law with f ◦= f, and Ef is the expectation determined by Pf.\nThe above theorem states the relationship between the complexity of the function class and the lower bound\nof the minimax risk. On the other hand, the following theorem is relates the complexity of the estimator and\nthe upper bound of the generalization error (and the minimax risk, at the same time). The following is also\nuseful for evaluating the convergence rate of the empirical risk minimizer of some explicit model, such as neural\nnetworks.\nTheorem 2.6. (Schmidt-Hieber, 2017, Lemma 4) In the Gaussian regression model (1), let bf be the empirical\nrisk minimizer, taking values in F ⊂L2([0, 1]d). Suppose every element f ∈F satisﬁes ∥f∥L∞≤F for some\nﬁxed F > 0. Then, for an arbitrary δ > 0, if V(F,∥·∥L∞)(δ) ≥1, then\nR( bf, f ◦) ≤4 inf\nf∈F ∥f −f ◦∥2\nL2 + C\n\u0012(F 2 + σ2)V(F,∥·∥L∞)(δ)\nn\n+ (F + σ)δ\n\u0013\nholds, where C > 0 is an absolute constant.\nThis sort of evaluation has been obtained earlier in (Gy¨orﬁet al., 2006; Koltchinskii, 2006; Gin´e and Koltchin-\nskii, 2006) and the proof for this is essentially the same as earlier ones. For completeness, however, we give a\nproof for this assertion in the appendix (Section B.1).1\n3\nSuboptimality of linear estimators\nWe consider linear estimators as a competitor to deep learning, and in this section, we characterize their\nsuboptimality by the convexity of the target model. Linear estimators, represented by kernel methods, are\nclassically applied to regression problems. Indeed, some linear estimators have minimax optimality over smooth\nfunction classes such as H¨older classes and Besov classes with some constraint on their parameters (with ﬁxed\ndesign: Donoho and Johnstone, 1998; Tsybakov, 2008). However, as has been pointed out in the literature\n(Korostelev and Tsybakov, 1993; Imaizumi and Fukumizu, 2019), linear estimators can attain only suboptimal\nrates with function classes having discontinuity. We here show that the suboptimality of linear estimators arises\neven with a quite simple target class. Our ﬁrst contribution is to point out that the concept of the convex hull\ngives the same explanation to such suboptimality for several target classes, and based on that argument, we\nthen show that linear estimators perform suboptimally even on a quite simple target class.\n1We noticed some technical ﬂaws in an earlier version of the proof of Schmidt-Hieber (2017), so we include the proof in the\nappendix for completeness.\n7\n3.1\nLinear estimators and its minimax risk\nDeﬁnition 3.1. The estimation scheme (Xi, Yi)n\ni=1 7→bf is called linear if bf has the form\nbf(x) =\nn\nX\ni=1\nYiϕi(x; Xn),\nwhere we suppose E\n\u0002\n∥ϕi(·; Xn)∥2\nL2\n\u0003\n< ∞. Also, we call an estimator bf aﬃne if bf has the form\nbf(x) = bfL(x) + ϕ(x; Xn),\nwhere ϕ has the same condition as ϕi, and bfL is a linear estimator.\nRemark 3.2. The condition E\n\u0002\n∥ϕi(·; Xn)∥2\nL2\n\u0003\n< ∞may be replaced by a weaker version. This actually assures\nthat\n• ϕi(·; Xn) ∈L2([0, 1]d) holds almost surely;\n• E\n\u0002\nϕi(x; Xn)2\u0003\n< ∞holds almost everywhere.\nThe latter condition is only needed in the justiﬁcation of (29).\nA linear estimator is of course an aﬃne estimator as well. Linear or aﬃne estimators are classically used often;\nthey include linear (ridge) regression, the Nadaraya–Watson estimator, and kernel ridge regression (Tsybakov,\n2008; Bishop, 2006; Friedman et al., 2001). For example, the estimator given by kernel ridge regression can be\nexplicitly written as\nbf(x) := (k(x, X1), . . . , k(x, Xn))(K + λIn)−1(Y1, . . . , Yn)⊤,\nwhere λ is a positive constant, k : [0, 1]d×[0, 1]d →R is a positive semi-deﬁnite kernel, and the matrix K ∈Rn×n\nis deﬁned as K := (k(Xi, Xj))i,j. We can see that the diﬀerence in performance between deep learning and\nlinear estimators becomes large in a non-convex model, which can be explained by the following theorem. (This\ntheorem can also be seen as a generalization of Cai and Low (2004, Theorem 5).)\nLet conv(F◦) denote the convex hull of F◦; i.e.,\nconv(F◦) :=\n( k\nX\ni=1\ntifi\n\f\f\f\f\f t1, . . . , tk ≥0,\nk\nX\ni=1\nti = 1, f1, . . . , fk ∈F◦, k ≥1\n)\n.\nNotice that the conv(F◦) is larger than the original set F◦. Let conv(F◦) be the closure of conv(F◦) with\nrespect to the L2 metric (caller closed convex hull). The following assertion holds.\nTheorem 3.3. For aﬃne methods, the minimax risk over F◦coincides with the minimax risk over conv(F◦);\ni.e., the following equality holds:\ninf\nb\nf:aﬃne\nsup\nf ◦∈F◦R( bf, f ◦) =\ninf\nb\nf:aﬃne\nsup\nf ◦∈conv(F◦)\nR( bf, f ◦).\nProof. We only prove the assertion\ninf\nb\nf:aﬃne\nsup\nf ◦∈F◦R( bf, f ◦) =\ninf\nb\nf:aﬃne\nsup\nf ◦∈conv(F◦)\nR( bf, f ◦).\n(2)\nFor the case of conv(F◦), see the appendix (Section B.2).\nFix an estimator bf and let\nbf(x) = ϕ(x; Xn) +\nn\nX\ni=1\nYiϕi(x; Xn).\n(3)\nFor f ◦, g◦∈F◦and t ∈(0, 1), let h◦:= tf ◦+ (1 −t)g◦. Then\nR( bf, h◦) = E\n\"Z\n[0,1]d\n\u0010\nbf(x) −h◦(x)\n\u00112\ndx\n#\n=\nZ\n[0,1]d E\n\u0014\u0010\nbf(x) −h◦(x)\n\u00112\u0015\ndx\n(4)\n8\nholds by Fubini’s theorem (the integrated value is nonnegative). By the convexity of the square, we have\n\u0010\nbf(x) −h◦(x)\n\u00112\n=\n \nϕ(x; Xn) +\nn\nX\ni=1\nYiϕi(x; Xn) −h◦(x)\n!2\n=\n \nϕ(x; Xn) +\nn\nX\ni=1\nξiϕi(x; Xn) +\nn\nX\ni=1\nh◦(Xi)ϕi(x; Xn) −h◦(x)\n!2\n(5)\n≤t\n \nϕ(x; Xn) +\nn\nX\ni=1\nξiϕi(x; Xn) +\nn\nX\ni=1\nf ◦(Xi)ϕi(x; Xn) −f ◦(x)\n!2\n+ (1 −t)\n \nϕ(x; Xn) +\nn\nX\ni=1\nξiϕi(x; Xn) +\nn\nX\ni=1\ng◦(Xi)ϕi(x; Xn) −g◦(x)\n!2\n= t\n \nbf(x)\n\f\f\f\f\nYi=f ◦(Xi)+ξi\n−f ◦(x)\n!2\n+ (1 −t)\n \nbf(x)\n\f\f\f\f\nYi=g◦(Xi)+ξi\n−g◦(x)\n!2\n.\nHere, notice that bf is dependent on whether we choose f ◦, g◦, or h◦. Therefore, we integrate this inequality to\nobtain\nR( bf, h◦) ≤tR( bf, f ◦) + (1 −t)R( bf, g◦).\nThis means that R( bf, ·) is a convex functional, and so LHS ≥RHS holds in (2). Since it is clear that LHS ≤RHS,\nthe equality of (2) holds.\nRemark 3.4. Indeed, Donoho et al. (1990) and Donoho and Johnstone (1998) pointed out that the convex\nhull in the above assertion can be replaced by the quadratic hull, which is generally larger than a convex hull\nin a similar setting (ﬁxed design).\nHowever, their propositions require the assumption of ﬁxed design and\northosymmetricity with some wavelet expansion. Hence, we have explicitly noted Theorem 3.3 under milder\nconditions.\nBy this theorem, we see that linear estimators hardly achieve the minimax rate in a non-convex model.\nThis also explains the diﬀerence between deep learning and linear methods argued in Schmidt-Hieber (2017),\nImaizumi and Fukumizu (2019) and Suzuki (2019) in a uniﬁed manner. In the following section, we demonstrate\na simple example where linear estimator are suboptimal.\n3.2\nFunctions of bounded total variation\nLet us consider a speciﬁc function class as a simple but instructive example, a class whose convex hull becomes\nlarger in terms of the covering entropy. In addition, the convex hull is dense in BV(C) (deﬁned below), over\nwhich linear estimators can only attain a suboptimal rate.\nDeﬁnition 3.5. For k ≥1 and C > 0, deﬁne\nJk(C) :=\n(\na0 +\nk\nX\ni=1\nai1[ti,1]\n\f\f\f\f\f ti ∈(0, 1], |a0| ≤C,\nk\nX\ni=1\n|ai| ≤C\n)\nas functions from [0, 1] to R with jumps occurring at most k times.\nWe can also understand Jk(C) as the set of piece-wise constant functions.\nWe next introduce a function class well-known in the ﬁeld of real analysis, which is indeed related to Jk’s\n(Lemma 3.8).\nDeﬁnition 3.6. For any real numbers a < b and a function f : [a, b] →R, deﬁne the total variation of f on\n[a, b] as\nTVf([a, b]) :=\nsup\nM≥1, a=t0<···<tM=b\nM−1\nX\ni=0\n|f(ti+1) −f(ti)|.\nAlso, for C > 0, deﬁne the set of functions with bounded total variation as\nBV(C) :=\nn\nf : [0, 1] →R\n\f\f\f |f(0)| ≤C, TVf([0, 1]) ≤C\no\n.\n9\nFigure 2: An example of piece-wise constant functions\nRemark 3.7. The condition |f(0)| ≤C is needed to bound the size of the set, and it may be replaced by\nother similar bounding conditions such as supt∈[0,1] |f(t)| ≤C or\nR 1\n0 |f(t)| dt ≤C (e.g., Donoho, 1993). These\nconditions are equivalent up to constant multiplications of C (i.e., BV(C) ⊂BV′(αC) ⊂BV(βC) holds for some\nα, β > 0, where BV′ is a set deﬁned with another constraint). Hence, we adopt |f(0)| ≤C for simplicity of\narguments.\nThen, we have the following assertion. The proof is given in the appendix (Section B.3)\nLemma 3.8. conv(Jk(C)) ⊇BV(C) holds for each k ≥1 and C > 0.\nFrom the above, it follows that linear estimators cannot distinguish Jk(C) and BV(C) in terms of minimax\nconvergence rates (Theorem 3.3).\nSince it is known that the unit ball of B1\n1,1([0, 1]) is included in BV(C) for some C > 0 (Peetre, 1976), the\nfollowing theorem can be seen as a special case of Theorem 1 in Zhang et al. (2002) (see also Table 1).\nTheorem 3.9. There exists a constant c > 0 dependent only on C such that\ninf\nb\nf:linear\nsup\nf ◦∈BV(C)\nR( bf, f ◦) ≥cn−1/2\nholds.\nThe following corollary is one of the main results in this paper.\nCorollary 3.10. For k = 1, 2, . . ., there exists a constant c > 0 dependent only on C such that\ninf\nb\nf:linear\nsup\nf ◦∈Jk(C)\nR( bf, f ◦) ≥cn−1/2\nholds.\nProof. The result is clear from Theorem 3.3, Lemma 3.8, and Theorem 3.9. We can of course take the same c\nas in Theorem 3.9.\nRemark 3.11. On the one hand, the minimax-optimal rate of the unit ball of B1\n1,1([0, 1]) is eΘ(n−2/3) (Table 1),\nwhereas the counterpart of Jk(C) is eΘ(n−1) as is attained by deep learning (proved later; see Corollary 5.8). On\nthe other hand, the fact that the unit ball of B1\n1,1([0, 1]) is included in BV(C) implies that the linear minimax\nrate of BV(C) is not faster than that of B1\n1,1([0, 1])’s unit ball. Since BV(C) and Jk(C) have the same linear\nminimax rate, Jk(C) is a quite extreme example, even in comparison with B1\n1,1([0, 1]).\n10\n4\nSparse target function classes\nAs Jk(C)’s given in Section 3.2 are too simple, we consider sparse target function classes which are generalizations\nof Jk(C)’s and are also related to wavelets. We investigate the performance of deep learning and other methods\nover these sparse classes. The minimax lower bound for each class is also given by applying the arguments in\nSection 2.2. Sparsity well characterizes the spaces whose convex hulls are much larger than the original spaces,\na property that is essential for the proofs that were given in Section 3.\n4.1\nThe ℓ0-bounded aﬃne class\nThe deﬁnition of the following class is inspired by the concept of “aﬃne class” treated in B¨olcskei et al. (2017).\nDeﬁnition 4.1. Given a set Φ ⊂L2([0, 1]d) with ∥ϕ∥L2 = 1 for each ϕ ∈Φ along with constants ns ∈Z>0 and\nC > 0, we deﬁne an ℓ0-bounded aﬃne class I0\nΦ as\nI0\nΦ(ns, C) :=\n( ns\nX\ni=1\nciϕi(Ai · −bi)\n\f\f\f\f\f | det Ai|−1, ∥Ai∥∞, ∥bi∥∞, |ci| ≤C, ϕi ∈Φ, i = 1, . . . , ns\n)\n.\nIf we adopt a jump-type function ϕ ∈Φ such as ϕ = 1[0,1/2), we can see I0\nΦ(ns, C) as a generalization of\nJk(C)’s deﬁned in the previous section. The condition for ci is also regarded as ∥c∥ℓ0 ≤ns, where the ℓ0 norm\nis used as the most extreme measurement of sparsity (Raskutti et al., 2011; Wang et al., 2014).\nLet us derive a minimax lower bound for this class. Although the proof for this assertion can easily be given\nby applying the argument appearing in Tsybakov (2008), we provide it in the appendix (Section B.4).\nTheorem 4.2. There exists a constant C0 > 0 depending only on σ2 such that\ninf\nb\nf\nsup\nf ◦∈I0\nϕ\nR( bf, f ◦) ≥C0\nn\nholds for each n ≥1.\nRemark 4.3. If the set Φ is simple enough; e.g., a ﬁnite set of piece-wise constants or piece-wise polynomials,\nthen this rate can be almost attained by nonlinear estimators. Indeed, in such cases deep learning attains the\nrate O(n−1(log n)3) (Theorem 5.6). However, with a single noncontinuous ϕ ∈Φ, linear estimators become\nsuboptimal; the linear minimax rate is lower-bounded by Ω(n−1/2) (Corollary 3.10).\n4.2\nThe wℓp-bounded function classes\nThe function class we treat in the previous section seems too simple to approximate the real-world data. There-\nfore, we are going to consider larger function classes with “sparsity”. We introduce concepts for measuring the\nsparsity of function classes in order to present a simple treatment of several sparse spaces. These concepts were\nintroduced and discussed previously in Donoho (1993), Donoho (1996), and Yang and Barron (1999).\nDeﬁnition 4.4. For a sequence a = (ai)∞\ni=1 ∈ℓ2, let each |a|(i) denote the i-th largest absolute value of terms\nin a. For 0 < p < 2, the weak ℓp norm of a is deﬁned as\n∥a∥wℓp := sup\ni≥1\ni1/p|a|(i).\n(6)\nHere, notice that ∥· ∥wℓp is not a norm, as (|a|(i))∞\ni=1 is a permutation of (|ai|)∞\ni=1. However, we call it a\n“weak ℓp norm” following the notation used in Donoho (1993) and Donoho (1996).\nDeﬁnition 4.5. Given an orthonormal set ϕ = (ϕi)∞\ni=1 ⊂L2([0, 1]d) and constants C1, C2, β > 0 and 0 < p < 2,\nwe deﬁne a sparse ℓp-approximated set Ip\nϕ as\nIp\nϕ(C1, C2, β) :=\n( ∞\nX\ni=1\naiϕi\n\f\f\f\f\f ∥a∥wℓp ≤C1,\n∞\nX\ni=m+1\na2\ni ≤C2m−β, m = 1, 2, . . .\n)\n.\nThe constraint P∞\ni=m+1 a2\ni ≤Cm−β is called β-minimally tail compactness of (ai)∞\ni=1, which is required to\nmake the set compact in the L2 metric.\n11\nRemark 4.6. To represent sparsity, the ℓp norm of coeﬃcients is also used (see, e.g., Raskutti et al., 2011;\nWang et al., 2014). Note here that ∥a∥ℓp ≤C implies ∥a∥wℓp ≤C. Indeed, ∥a∥ℓp ≤C means that for each i,\ni1/p|a|(i) ≤\n\n\ni\nX\nj=1\n|a|p\n(j)\n\n\n1/p\n≤\n\n\n∞\nX\nj=1\n|aj|p\n\n\n1/p\n≤C.\nThus, a weak ℓp ball contains an ordinary ℓp ball. In addition, consider the case in which d = 1 and ϕ is an\northonormal basis generated by a wavelet in Cr([0, 1]) with r ∈Z>0 satisfying r > α := 1/p −1/2. Then,\nthe Besov norm ∥· ∥Bα\np,p of a function is equivalent to the ℓp-norm ∥· ∥ℓp of wavelet coeﬃcients (Donoho and\nJohnstone, 1998, Theorem 2). In this case, Ip\nϕ may be just a slight expansion of existing space, but our main\ninterest is the case in which ϕ has a discontinuity (e.g., when ϕ is deﬁned by the Haar wavelet), which makes\nthings diﬀerent. Furthermore, notice that Besov spaces with such parameters are omitted in Table 1 (see the\nnote; the upper bounds are given in Suzuki (2019) for a wider range of parameters, but the range for the given\nlower bounds for linear estimators is limited).\nHereinafter, we ﬁx p, C1, C2, and β and often write Ip\nϕ(C1, C2, β) as Ip\nϕ if there is no confusion; therefore,\nconstants appearing in the following may depend on these values. In the following arguments, we ﬁrst derive\na minimax lower bound for Ip\nϕ, and then we introduce a broader function class that is well approximated by\nneural networks.\nTo use Theorem 2.5, we exploit the following lemma. As stated in Section 2.2, the covering entropy of the\nfunction class is important. The proof of the lemma is given in the appendix (Section B.5).\nLemma 4.7. Let α := 1/p −1/2, and suppose β satisﬁes β ≤2α. Then there exists a constant Clow, Cup > 0\nsuch that\nClowε−1/α ≤V(Ip\nϕ,∥·∥L2)(ε) ≤Cupε−1/α(1 + log(1/ε))\nholds for each ε > 0.\nNext, we derive a nearly tight minimax lower bound for Ip\nϕ. In this case, “nearly” means “up to log factors.”\nTheorem 4.8. There exists a constant C = C(p, C1, C2) > 0 such that\ninf\nb\nf\nsup\nf ◦∈Ip\nϕ\nR( bf, f ◦) ≥Cn−\n2α\n2α+1 (log n)−4α2\n2α+1\nholds for each n ≥2.\nProof. In this proof, we write the ε-entropies of Ip\nϕ simply as V (ε) and M(ε).\nFirst, let εn := c\n\u0012log n\nn\n\u0013\nα\n2α+1\nfor some constant c > 0. Then by Lemma 4.7, we have\nV (εn) ≤Cupc−1/α\n\u0012log n\nn\n\u0013−\n1\n2α+1 \u0012\n1 +\nα\n2α + 1(log n −log log n)\n\u0013\n≤cupc−1/αn\n1\n2α+1 (log n)\n2α\n2α+1 ,\nwhere cup > 0 is some constant independent of εn, and we have used n ≥2. Thus we have\nV (εn)\nnε2n\n≤cupc−2−1/α ≤\n1\n2σ2\n(7)\nfor a suﬃciently large c.\nSecond, notice that M(ε) ≥V (ε) holds. Indeed, given a maximal ε-packing of Ip\nϕ, the maximality implies\nthat the set also satisﬁes the condition for being an ε-covering. Now, let δn := C′n−\nα\n2α+1 (log n)−2α2\n2α+1 for some\nconstant C′ > 0. Then we have, by Lemma 4.7,\nM(δn) ≥V (δn) ≥ClowC′−1/αn\n1\n2α+1 (log n)\n2α\n2α+1 ≥C′−1/αclow\n\u00122nε2\nn\nσ2\n+ 2 log 2\n\u0013\n(8)\nfor some constant clow > 0 independent of C′, where we have used n ≥2.\nBy (7), (8), and Theorem 2.5, for a suﬃciently small C′, we have\ninf\nb\nf\nsup\nf∈Ip\nϕ\nE\nh\n∥f −bf∥2\nL2\ni\n≥1\n8C′2n−\n2α\n2α+1 (log n)−4α2\n2α+1 ,\nand the proof is complete.\n12\nRemark 4.9. This minimax lower bound is nearly tight, especially for the wavelet case treated in the following\nsection. Indeed, deep learning (if necessary, with parameter sharing) achieves the rate O(n\n2α\n2α+1 (log n)3), for a\nbroad range of wavelets (Theorem 5.10, 5.14).\n4.3\nSparsity conditions for wavelet coeﬃcients\nIn this subsection, we apply the argument in the previous subsection to orthogonal wavelets. They already have\na broad application area from engineering and physics to pure and applied mathematics, especially in signal\nprocessing, numerical analysis, and so on (Daubechies, 1992). Because wavelets are the mathematical model of\nlocalized patterns, they are suitable for our motivation in setting function classses (see Section 1.3 and Table 2).\nFor simplicity, we only consider the 1-dimensional case in this section, and the treatment of multi-dimensional\ncases is deferred to the appendix (Section A.2).\nDeﬁnition 4.10. Let ψ : [0, 1] →R be a function with ∥ψ∥L2 = 1. For such a function, we deﬁne, for integers\nk, ℓ,\nψk,ℓ(x) := 2k/2ψ(2kx −ℓ),\nk ≥0, 0 ≤ℓ< 2k,\nwhere ψ is treated as 0 outside [0, 1]. Also, ψ is called an orthogonal wavelet if ψ satisﬁes\nZ 1\n0\nψk,ℓ(x)ψk′,ℓ′(x) dx = 0\nfor all (k, ℓ) ̸= (k′, ℓ′).\nDeﬁnition 4.11. Given a 1-dimensional orthonormal wavelet ψ(x) and constants C1, C2, β > 0 and 0 < p < 2,\ndeﬁne\nJ p\nψ(C1, C2, β) :=\n\n\n\nX\nk≥0, 0≤ℓ<2k\nak,ℓψk,ℓ\n\f\f\f\f\f\f\n∥a∥wℓp ≤C1,\nX\nk≥m\na2\nk,ℓ≤C22−βm, m = 0, 1, . . .\n\n\n.\nFor the diﬁnition of multi-dimensional cases, see Deﬁnition A.4.\nRemark 4.12. If we consider the lexical order on (k, ℓ)’s, then J p\nψ(C1, C2, β) is revealed to be β-minimally tail\ncompact (see Deﬁnition 4.5). Thus, Ip\nϕ ⊂J p\nψ holds with some modiﬁcation of constants and they have the same\ndegree of sparsity.\nIn the following, we introduce the class Kp\nΨ as an expansion of J p\nψ.\nThough the following deﬁnition is\ntreating the d-dimensional case, considering only the case d = 1 is suﬃcient for readers to understand the\nessential properties. For multi-dimensional cases, see also Section A.2.\nDeﬁnition 4.13. Let Ψ ⊂L2([0, 1]d) consist of orthonormal wavelets. Then, for an integer ns > 0 and constants\nC1, C2, C3, β > 0 and 0 < p < 2, deﬁne\nKp\nΨ(ns, C1, C2, C3, β) :=\n\n\n\nns\nX\nj=1\nfj(Aj · −bj)\n\f\f\f\f\f\f\nAj ∈Rd×d, bj ∈Rd, | det Aj|−1, ∥Aj∥∞, ∥bj∥∞≤C3,\nfj ∈J p\nψj(C1, C2, β), ψj ∈Ψ, j = 1, . . . , ns\n\n\n.\nRemark 4.14. By Remark 4.12 (and Remark A.5) the bound given in Theorem 4.8 is also the minimax lower\nbound for Kp\nΨ. Moreover, Jk(C), introduced in Section 3, is included in Kp\nΨ, with k ≤ns and a speciﬁc Ψ such\nas one containing the Haar wavelet. Thus, the bounds given in the previous sections are still applicable to this\nfunction class. Indeed, as described in Figure 1, we deﬁne Kp\nΨ as an expansion of I0\nΨ as well.\n4.4\nSuboptimlaity of linear estimators on J p\nψ and Kp\nψ\nFor a wavelet ψ with compact support, linear estimator shows suboptimality on J p\nψ and Kp\nψ, as is shown in\nTheorem 4.15. This can be regarded as a stronger version of Corollary 3.10 and shows the non-eﬃciency of\nlinear estimators in sparse classes. The proof of this theorem is similar to that of Theorem 1 in Zhang et al.\n(2002) and is given in the appendix (Section B.6).\n13\nTheorem 4.15. Let d = 1 and ψ be a bounded and compactly supported wavelet. For any constants C1, C2, β > 0\nand 0 < p < 2, there exists a constant C dependent only on C1, C2, and β such that\ninf\nb\nf:linear\nsup\nf∈J p\nψ(C1,C2,β)\nR( bf, f ◦) ≥Cn−\nβ\n1+β\nholds for each n ≥1. This bound also holds on Kp\nΨ.\nRemark 4.16. From this result, we see that the minimax-optimal rate for linear estimators can be arbitrarily\nslow even with the same sparsity p, i.e., with a bounded covering entropy (by Lemma 4.7). The nearly optimal\nrates attained by deep learning given in Section 5 are unfortunately limited to the case β > 1 (because of\nthe assumption of boundedness), but this still serves as evidence for the non-eﬀectiveness of linear methods in\nestimating sparse classes.\n5\nLearning ability of deep ReLU neural networks\n5.1\nMathematical formulation of deep ReLU neural networks\nFor mathematical treatments of neural networks, we have referenced some recent papers on approximation\ntheory and estimation theory (Suzuki, 2019; Schmidt-Hieber, 2017; Yarotsky, 2017; B¨olcskei et al., 2017; Keiper\net al., 2017). In the following, we deﬁne neural networks mathematically and evaluate their covering entropies.\nDeﬁnition 5.1. Let ρ : R →R. For L, S, D ∈Z>0 and B ≥1 (with D ≥d), deﬁne N(L, S, D, B) as the set of\nall functions f : Rd →R of the form\nf = WL+1 ◦ρ(WL · −vL) ◦· · · ◦ρ(W1 · −v1),\nsatisfying\nW1 ∈RD×d, W2, . . . , WL ∈RD×D, WL+1 ∈R1×D, v1 ∈Rd, v2, . . . , vL ∈RD\nand\n∥vi∥∞, ∥Wi∥∞≤B,\nL+1\nX\ni=1\n∥Wi∥0 +\nL\nX\ni=1\n∥vi∥0 ≤S,\nwhere ρ is operated elementwise, and L, S, and D denote the number of hidden layers, the sparsity, and the\ndimensionality of the layers, respectively. Also, for F > 0, we consider a function class\nNF = NF (L, S, D, B) := {sgn(f) min{|f|, F} | f ∈N(L, S, D, B)}.\n5.2\nComplexity of neural network models\nHereinafter, we use the ReLU activation function ρ(x) = max{x, 0}. Notice that NF can be realized easily using\nReLU activation after an element of N is computed. To evaluate the generalization error of deep learning, we\nneed bounds for the complexity of neural network models. The following lemma gives the required bound as a\nfunction of parameters L, S, D, B.\nLemma 5.2. (Schmidt-Hieber, 2017; Suzuki, 2019) For any 0 < δ < 1, the δ-covering entropy with respect to\n∥· ∥L∞of N(L, S, D, B) (limiting the domain to [0, 1]d) can be bounded as\nV(N(L,S,D,B),∥·∥L∞)(δ) ≤2S(L + 1) log\n\u0012B(L + 1)(D + 1)\nδ\n\u0013\n.\nThe proof for this lemma is given in the appendix (Section B.7). We next introduce a lemma which evaluates\nthe approximation ability of a neural network given the approximation ability of subnetworks. This lemma is also\nstated in B¨olcskei et al. (2017) in another form. Thanks to this lemma, we can only consider the approximation\nof the wavelet basis, where we can exploit many existing studies.\n14\n...\n˜ϕ1\n˜ϕns\n...\nc1\ncns\nA1\nAns\nb1\nbns\nx\n˜f(x)\nFigure 3: Construction of a larger neural network\n0\n1/2\n1\n√\n2\nt\n0\nFigure 4: An approximation of\n√\n2 · 1[1/2,1]\nLemma 5.3. Let Φ ⊂L2([0, 1]d) satisfy ∥ϕ∥L2 = 1 for each ϕ ∈Φ. Suppose for any ϕ ∈Φ there exists a\nfunction g ∈N(L, S, D, B) such that ∥g −ϕ∥L2 ≤ε. Then for any f ◦∈I0\nΦ(ns, C), there exists a function\nf ∈N(L + 2, ns(S + 2Dd + d2 + d + 1), nsD, max{B, C})\nsuch that ∥f −f ◦∥L2 ≤C3/2nsε holds.\nProof. The approximation of f ◦(x) = Pns\ni=1 ciϕi(Aix −bi) ∈I0\nΦ(ns, C) can be constructed as shown in Fig.\n3 (we use the ReLU activation function, and so we compute max{eϕi, 0} and max{−eϕi, 0} and combine them\nafterward), where each eϕi approximates ϕi with an L2-error of at least ε. In this construction,\n∥ef −f ◦∥L2 ≤\nns\nX\ni=1\n|ci|∥ϕi(Ai · −bi) −eϕi(Ai · −bi)∥L2 ≤\nns\nX\ni=1\nC| det Ai|−1/2∥eϕi −ϕi∥L2 ≤C3/2nsε\nholds.\nRemark 5.4. These lemmas evaluate the covering entropy and approximation ability of neural networks. Given\nthese two informations, we can exploit Theorem 2.6 to estimate the generalization error of deep learning by a\nﬁxed size of neural network. However, we should know how large the parameters L, S, D, B get as ε goes to zero\n(or the sample size n goes to inﬁnity). As one can easily see, a shallow ReLU network can approximate piece-wise\nconstant functions easily (more precisely, see e.g., Fig. 4 and the proof of Corollary 5.8). Moreover, deep ReLU\nnetworks can approximate polynomials very eﬃciently (Yarotsky, 2017). Combining these, for example, we can\nstate that\nthere exists an architecture N(L, S, D, B) with L, S, D = O(log(1/ε)) and B = O(1/ε) that can\napproximate piece-wise polynomials with O(ε) L2-error.\nRigorously speaking, we of course have to restrict the function class (the number of non-smooth points, the\ndegree of polynomials, the magnitude of coeﬃcients, etc).\nHowever, as the main concern of this paper is\nstatistical viewpoints, we do not get deeper. The important thing is that the above size of network architecture\nis reasonable to approximate “cheap” functions.\nMotivated by the above remark, we deﬁne the function class AP, which can be approximated by “light”\nnetworks, as follows.\nDeﬁnition 5.5. For C1, C2 > 0, deﬁne AP(C1, C2) as the set of all functions ϕ ∈L2([0, 1]d) satisfying that, for\neach 0 < ε < 1/2, there exist parameters Lε, Sε, Dε, Bε > 0 such that\n• Lε, Sε, Dε ≤C1 log(1/ε) and Bε ≤C2/ε hold;\n• there exists a eϕ ∈N(Lε, Sε, Dε, Bε) such that ∥eϕ −ϕ∥L2 ≤ε.\n15\n5.3\nGeneralization ability for an extreme case (I0\nΦ)\nThe learning ability of neural networks over I0\nΦ is shown in the following. This is the most extreme case in\nterms of the diﬀerence between the performance of deep learning and linear methods.\nTheorem 5.6. Let Φ ⊂L2([0, 1]d) satisfy ∥ϕ∥L2 = 1 for each ϕ ∈L2([0, 1]d) and supϕ∈Φ ∥ϕ∥L∞< ∞. Suppose\nalso Φ ⊂AP(C1, C2) holds for some constants C1, C2 > 0. Then, for each ns, C > 0, there exist constants\nF, C3 > 0 dependent only on ns, C (independent of n) such that the empirical risk minimizer bf over N (n)\nF\nsatisﬁes\nsup\nf ◦∈I0\nΦ(ns,C)\nR( bf, f ◦) ≤C3\n(log n)3\nn\nfor n ≥2, where N (n)\nF\ndenotes\nNF\n\u0000L1/n + 2, ns(S1/n + 2D1/nd + d2 + d + 1), nsD1/n, max\n\b\nB1/n, C\n\t\u0001\n.\nRemark 5.7. This theorem implies that the empirical risk minimizer over N(L, S, D, B) with size\nL, S, D = O(log n),\nB = O(n),\nalmost attains the minimax-optimal rate. Therefore, from the viewpoint of real-world application, a reasonable\nsize of network can behave optimally.\nProof of Theorem 5.6. Let M := supϕ∈Φ ∥ϕ∥L∞.\nIf we deﬁne F := nsCM, each f ◦∈I0\nΦ(ns, C) satisﬁes\n∥f∥L∞≤F. Indeed, f ◦has some expression f ◦= Pns\ni=1 ciϕi(Ai · −bi), and so we have\n∥f ◦∥L∞≤\nns\nX\ni=1\n|ci|∥ϕi∥L∞≤nsCM = F.\nHence, for f ∈N(L, S, D, B) and f ◦∈I0\nΦ(ns, C), ef := sgn(f) min{|f|, F} satisﬁes\n∥ef −f ◦∥2\nL2 =\nZ\n[0,1]d( ef(x) −f ◦(x))2 dx\n≤\nZ\n[0,1]d(f(x) −f ◦(x))2 dx\n(∵f ◦(x) ∈[−F, F])\n= ∥f −f ◦∥2\nL2.\nAlso, notice that NF ’s covering entropy is not greater than that of N, and so we have, by Lemma 5.2 and the\nassumption of the assertion,\nV(N (n)\nF\n,∥·∥L∞)\n\u0012 1\nn\n\u0013\n≤C0(log n)3\nfor some constant C0 > 0. Then, by Theorem 2.6 and Lemma 5.3,\nsup\nf ◦∈I0\nΦ(ns,C)\nR( bf, f ◦) ≤4C3/2ns\nn\n+ C′\n\u0012\nC0(F 2 + σ2)(log n)3\nn\n+ F + σ\nn\n\u0013\n≤C3\n(log n)3\nn\nholds for some C3 > 0.\nCorollary 5.8. Let d = 1. For Jk(C) in Deﬁnition 3.5, there exist a constant F > 0 and a sequence of neural\nnetworks (N (n))∞\nn=2 such that the empirical risk minimizer bf satisﬁes\nsup\nf ◦∈Jk(C)\nR( bf, f ◦) ≤C3\n(log n)3\nn\nfor some constant C3 > 0 independent of n and each n ≥2.\nProof. By Theorem 5.6, it suﬃces to show that ϕ =\n√\n2 · 1[1/2,1] can be approximated within ε-error in L2 by a\nneural network satisfying the condition of Theorem 5.6. This can be actually realized by a shallow network, as\n4\n√\n2\nε\nρ\n\u0012\nt −1 −ε/2\n2\n\u0013\n−4\n√\n2\nε\nρ\n\u0012\nt −1\n2\n\u0013\n−4\n√\n2\nε\nρ\n\u0010\nt −\n\u0010\n1 −ε\n2\n\u0011\u0011\n+ 4\n√\n2\nε\nρ(t −1)\n(Fig. 4) satisﬁes the desired condition.\n16\nRemark 5.9. By Corollary 3.10, Theorem 4.2, and Corollary 5.8, Jk(C) demonstrates an extreme situation,\nwherein neural network learning attains the optimal rate up to log factors whereas linear methods are suboptimal.\nThis result can easily be expanded to the case of d = 2 (if we properly deﬁne Jk for higher dimensions). In\naddition, we can treat a set broader than Jk(C) as I0\nΦ because smooth functions such as polynomials can be\nwell approximated by O(log(1/ε)) weights as has been mentioned in Remark 5.4.\n5.4\nGeneralization ability for the wavelet case (J p\nψ, Kp\nΨ)\nLet us consider the case in which the target function class is Kp\nΨ in Deﬁnition 4.13. Note that, by Lemma 5.3,\nwe only have to consider approximating functions in J p\nψ in Deﬁnition 4.11 for ﬁxed ns. We defer the proof of\nthe following main result to the appendix (Section B.8).\nTheorem 5.10. Let ψ ∈L2([0, 1]d) be an orthonormal wavelet. Suppose there exist constants C′\n1, C′\n2 > 0 such\nthat ψ ∈AP(C′\n1, C′\n2) holds. Then, for each C1, C2, β > 0 and 0 < p < 2, there exists a constant C > 0 dependent\nonly on constants C′\n1, C′\n2, p, C1, C2, β (independent of n) such that the empirical risk minimizer bf over N (n)\nF\n(with some network architecture) satisﬁes\nsup\nf◦∈J p\nψ(C1, C2, β)\n∥f◦∥L∞≤F\nR( bf, f ◦) ≤CF 2n−\n2α\n2α+1 (log n)3\nfor each F ≥max{1, σ} and n ≥2, where α := 1/p −1/2. Moreover, for a set of wavelets Ψ ⊂AP(C′\n1, C′\n2), the\nsame evaluation is valid for Kp\nΨ;\nsup\nf◦∈Kp\nΨ(ns, C1, C2, C3, β)\n∥f◦∥L∞≤F\nR( bf, f ◦) ≤C′F 2n−\n2α\n2α+1 (log n)3\nholds for some C′ > 0, where bf is the empirical risk minimizer of some larger network.\nRemark 5.11. The actual network size of N(L, S, D, B) here is\nL = O(log n),\nS, D = O(n\n1\n2α+1 log n),\nB = O\n\u0010\nnmax{1,\n4α\nβ(2α+1)}\u0011\n(see (37) in the appendix). Though this network is larger than that of Remark 5.7, this is still acceptable size,\nespecially with large α; i.e., in the case the parameter space is very sparse.\nConcretely, ψ constructed by using the Haar wavelet satisﬁes the desired condition. Also, in the case of d = 1\nand β > 1, we can remove the restriction by the constant F, because sup{∥f∥∞| f ∈Jp\nψ} < ∞holds. Note also\nthat this result is nearly minimax optimal; i.e., bf attains the minimax lower bound derived in Theorem 4.8 up\nto log factors and the constraint of boundedness. As p gets smaller, Kp\nΨ gets sparser and more reasonable size of\nneural network can achieve the minimax optimality. Moreover, as is stated in the next remark, linear estimators\ncannot exploit the sparsity of Kp\nΨ in typical settings (e.g., when Ψ contains non-continuous functions). From\nthe viewpoint of approximation ability, this situation can be interpreted as follows; neural networks are allowed\nto approximate just the sparse set Kp\nΨ, whereas linear estimators must be able to approximate all the functions\ncontained in conv(Kp\nΨ). This diﬀerence yields the diﬀerence of complexity; linear estimators require far more\nparameters than neural networks. Finally, this situation results in the great diﬀerence of generalization ability\nbetween deep and linear.\nRemark 5.12. If Ψ contains the Haar wavelet and C3 is suﬃciently large, Kp\nΨ includes Jk in Deﬁnition 3.5\n(also, notice that Jk is bounded in the L∞-norm sense). To be more precise, {f ∈[0, 1]d →R | f(x1, . . . , xd) =\ng(x1), g ∈Jk(C)} is included in Kp\nΨ for some C > 0. The proof of Theorem 3.9 can easily be modiﬁed for this\ncase, and we have\ninf\nb\nf:linear\nsup\nf ◦∈Kp\nΨ, ∥f ◦∥∞≤F\nR( bf, f ◦) ≥cn−1/2\nfor some c > 0. If α > 1/2 (equivalent to p < 1) holds, then the neural network learning is superior to linear\nmethods.\n17\n5.5\nParameter sharing technique to restrict the covering entropy\nThe assumption of the ability for ϕ to be approximated by eϕ imposed in Theorems 5.6 and 5.10 is quite strong,\nand thus we cannot treat a broad range of wavelets. In the proof of Theorem 5.10, however, we do not exploit\nthe full degree of freedom depicted in Fig. 3 because subnetworks share the same approximator eψ. In this\nsubsection, we consider neural networks with parameter sharing.\nDeﬁnition 5.13. Let N be a positive integer. For a given neural network architecture N(L, S, D, B), denote\nthe N-sharing of N(L, S, D, B) by N N(L, S, D, B), deﬁned as\n( N\nX\ni=1\ncif(Ai · −bi)\n\f\f\f\f\f Ai ∈Rd×d, bi ∈Rd, ci ∈R, ∥Ai∥∞, ∥bi∥∞, |ci| ≤B, i = 1, . . . , d, f ∈N(L, S, D, B)\n)\n.\nTheorem 5.14. Given a positive integer N and N(L, S, D, B) with L ≥2, the δ-covering entropy with respect\nto ∥· ∥L∞of N N(L, S, D, B) (limiting the domain to [0, 1]d) can be bounded as\nV(N N(L,S,D,B),∥·∥L∞)(δ) ≤\n\u0000N(d + 1)2 + 2S(L + 1)\n\u0001\n(L + 3) log\n\u0012NB(L + 1)(D + 1)\nδ\n\u0013\nfor any 0 < δ < 1.\nThe proof is straightforward but a bit technical; thus, we defer it to the appendix (Section B.9).\nRemark 5.15. If we use neural networks with parameter sharing, we can use non-trivial wavelets with some\nsmoothness; i.e., in Theorem 5.10, the assumption for ψ can be weakened to the following:\nFor each 0 < ε < 1/2, there exist Lε, Sε, Dε, Bε > 0 satisfying\n• Lε ≤C′\n1 log(1/ε), Dε, Bε ≤C′\n2ε−γ and Sε ≤C′\n3ε−\n1\nα+1 hold for some constants C′\n1, C′\n2, C′\n3, γ > 0;\n• there exists eψ ∈N(Lε, Sε, Dε, Bε) such that ∥eψ −ψ∥L2 ≤ε.\nThis class of ψ is actually broadened as there exist compactly supported wavelets with high regularity (a large\nH¨older exponent) (Daubechies, 1992), and such functions can be approximated well by networks with a small\nnumber of parameters. Indeed, Yarotsky (2017) proved that\nA unit ball of Sobolev space W n,∞([0, 1]d) can be approximated, with L∞-error at most ε, by a\nneural network with O(log(1/ε)) depth, and O(ε−d/n) complexity (corresponds to S, D).\nThough we cannot use this in the original form because L∞should be replaced by L2 and the upper bound of\nB should be given, the reﬁnement is not so diﬃcult (e.g., Schmidt-Hieber, 2017).\n6\nSummary and discussion\n6.1\nSummary\nIn this paper, we have shown that deep learning outperforms other commonly used methods such as linear\nestimators even in a simple case.\nTo evaluate the learning ability of estimators, we employed a Gaussian\nregression problem with a sparse target function space. In such a problem setting, neural network learning attains\nnearly the minimax-optimal rate of convergence with respect to the sample size, whereas a linear estimator can\nonly achieve a suboptimal rate.\nThe main novelty is that the target function spaces were selected to have\nnatural sparsity, instead of following the well-known settings developed by the existing mathematical analyses.\nWe have also shown that parameter sharing is quite eﬀective for widening function classes where (near) minimax\noptimality holds.\n6.2\nDiscussion and future work\nThere are two main limitations in this work that remain to be addressed in future investigations.\nFirst, I0\nΦ(ns, C) is the most extreme case in the sense that deep learning outperforms linear estimators.\nThis class is very simple, and we have additionally deﬁned wℓp-bounded classes Kp\nΨ for 0 < p < 2, with the\n18\nassumption of orthonormal wavelets. However, we should remove the orthogonality if we follow the philosophy\nof deﬁning I0\nΦ(ns, C). For example, using the deﬁnition\nLp\nΦ(C1, C2, C3, β) :=\n( ∞\nX\ni=1\nciϕ(Ai · −bi)\n\f\f\f\f\f\n|ci| ≤C1i−1/p, ∥Ai∥∞, ∥bi∥∞≤C2iβ,\n| det Ai|−1 ≤C3, i = 1, 2, . . . , ϕ ∈Φ\n)\nwould be one possible way.\nOf course, for some range of (p, β), we can show that deep learning attains a\nrate faster than do linear estimators. However, we could not have shown that the convergence rate satisﬁes\nminimax optimality, even up to log factors. This diﬃculty arises from the fact that we have fully exploited the\northogonality in the proof of deep learning’s minimax optimality over Kp\nΨ. It is possible that we can ﬁnd both\na better minimax lower bound for Lp\nΦ and a better approximation bound by neural networks.\nSecond, parameter sharing, mentioned in Subsection 5.5, is used mainly in the context of convolutional neural\nnetworks (CNNs), and this implies the superiority of CNNs in solving a regression problem. However, some of\nthe arguments in this paper are not directly applicable to the analysis of CNNs. Although CNNs have achieved\nnotable success in pattern recognition, theories of CNNs with respect to regression problems have not yet been\nwell argued in the literature.\nIn addition to these issues, a theoretical analysis of stochastic optimization as used in deep learning is needed,\nwhich is not treated in this paper.\nAcknowledgments\nThe authors would like to thank Atsushi Nitanda for his constructive feedback. TS was partially supported by\nMEXT Kakenhi (15H05707, 18K19793 and 18H03201), Japan Digital Design, and JST-CREST.\nReferences\nBishop, C.M., 2006. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer.\nB¨olcskei, H., Grohs, P., Kutyniok, G., Petersen, P., 2017. Optimal approximation with sparsely connected deep\nneural networks. arXiv preprint arXiv:1705.01714 .\nCai, T.T., Low, M.G., 2004. Minimax estimation of linear functionals over nonconvex parameter spaces. The\nAnnals of Statistics 32, 552–576.\nCand`es, E.J., Wakin, M.B., 2008. An introduction to compressive sampling [a sensing/sampling paradigm that\ngoes against the common knowledge in data acquisition]. IEEE signal processing magazine 25, 21–30.\nCybenko, G., 1989. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals\nand Systems 2, 303–314.\nDaubechies, I., 1992. Ten lectures on wavelets. Society for Industrial and Applied Mathematics.\nDonoho, D.L., 1993. Unconditional bases are optimal bases for data compression and for statistical estimation.\nApplied and Computational Harmonic Analysis 1, 100–115.\nDonoho, D.L., 1996. Unconditional bases and bit-level compression. Applied and Computational Harmonic\nAnalysis 3, 388–392.\nDonoho, D.L., Johnstone, I.M., 1998. Minimax estimation via wavelet shrinkage. The Annals of Statistics 26,\n879–921.\nDonoho, D.L., Johnstone, J.M., 1994. Ideal spatial adaptation by wavelet shrinkage. biometrika 81, 425–455.\nDonoho, D.L., Liu, R.C., MacGibbon, B., 1990. Minimax risk over hyperrectangles, and implications. The\nAnnals of Statistics , 1416–1437.\nFriedman, J., Hastie, T., Tibshirani, R., 2001. The elements of statistical learning. Springer.\nGin´e, E., Koltchinskii, V., 2006. Concentration inequalities and asymptotic results for ratio type empirical\nprocesses. The Annals of Probability 34, 1143–1216.\n19\nGlorot, X., Bordes, A., Bengio, Y., 2011. Deep sparse rectiﬁer neural networks, in: Proceedings of the 14th\nInternational Conference on Artiﬁcial Intelligence and Statistics, pp. 315–323.\nGoodfellow, I., Bengio, Y., Courville, A., Bengio, Y., 2016. Deep learning. MIT Press Cambridge.\nGy¨orﬁ, L., Kohler, M., Krzyzak, A., Walk, H., 2006. A distribution-free theory of nonparametric regression.\nSpringer.\nImaizumi, M., Fukumizu, K., 2019. Deep neural networks learn non-smooth functions eﬀectively, in: Proceedings\nof the 22nd International Conference on Artiﬁcial Intelligence and Statistics.\nKeiper, S., Kutyniok, G., Petersen, P., 2017. DGD approximation theory workshop .\nKoltchinskii, V., 2006. Local rademacher complexities and oracle inequalities in risk minimization. The Annals\nof Statistics 34, 2593–2656.\nKorostelev, A.P., Tsybakov, A.B., 1993. Minimax theory of image reconstruction. Springer.\nLaﬀerty, J., Liu, H., Wasserman, L., 2008. Concentration on measure. http://www.stat.cmu.edu/~larry/\n=sml/Concentration.pdf.\nPeetre, J., 1976. New thoughts on Besov spaces. Mathematics Department, Duke University.\nPetersen, P., Voigtlaender, F., 2018. Optimal approximation of piecewise smooth functions using deep ReLU\nneural networks. Neural Networks 108, 296–330.\nRaskutti, G., Wainwright, M.J., Yu, B., 2011. Minimax rates of estimation for high-dimensional linear regression\nover ℓq-balls. IEEE Transactions on Information Theory 57, 6976–6994.\nSchmidhuber, J., 2015. Deep learning in neural networks: An overview. Neural Networks 61, 85–117.\nSchmidt-Hieber, J., 2017. Nonparametric regression using deep neural networks with ReLU activation function.\nThe Annals of Statistics, to appear (arXiv:1708.06633) .\nSonoda, S., Murata, N., 2017. Neural network with unbounded activation functions is universal approximator.\nApplied and Computational Harmonic Analysis 43, 233–268.\nStein, E.M., Shakarchi, R., 2005. Real Analysis: Measure Theory, Integration, and Hilbert Spaces (Princeton\nLectures in Analysis, Book 3). Princeton University Press.\nSuzuki, T., 2019. Adaptivity of deep ReLU network for learning in Besov and mixed smooth Besov spaces:\noptimal rate and curse of dimensionality, in: ICLR 2019.\nTsybakov, A.B., 2008. Introduction to Nonparametric Estimation. Springer.\nvan der Vaart, A.W., Wellner, J.A., 1996. Weak convergence and empirical processes. Springer.\nWang, Z., Paterlini, S., Gao, F., Yang, Y., 2014. Adaptive minimax regression estimation over sparse ℓq-hulls.\nThe Journal of Machine Learning Research 15, 1675–1711.\nYang, Y., Barron, A., 1999. Information-theoretic determination of minimax rates of convergence. The Annals\nof Statistics 27, 1564–1599.\nYarotsky, D., 2017. Error bounds for approximations with deep ReLU networks. Neural Networks 94, 103–114.\nZhang, S., Wong, M.Y., Zheng, Z., 2002. Wavelet threshold estimation of a regression function with random\ndesign. Journal of Multivariate Analysis 80, 256–284.\n20\nA\nMathematical supplements\nA.1\nGeneralities related to the minimax risk\nIn the Gaussian regression model, for two true functions f, g ∈F◦, it is well known that the Kullback–Leibler\n(KL) divergence between two distributions of (X, Y ) generated by f and g is easy to calculate. Let dKL(f, g)\nbe the square root of the KL divergence. The following lemma is essential when one treats a regression problem\nas a parameter estimation problem (Yang and Barron, 1999; Schmidt-Hieber, 2017; Suzuki, 2019); its proof is\ngiven in Section B.10.\nLemma A.1. It holds that dKL(f, g)2 =\n1\n2σ2 ∥f −g∥2\nL2.\nTherefore, the square root of the KL divergence is a metric equivalent to the L2 metric in regression problems\nwith a Gaussian noise.\nWe also introduce a lemma that is useful for deriving a lower bound of the metric entropy.\nLemma A.2. (Donoho, 1993, Lemma 4) Let Ck ⊂ℓ2 be a k-dimensional hypercube of side 2δ > 0 deﬁned as\nCk := {a ∈ℓ2 | |a1|, . . . , |ak| ≤δ, |ak+1| = |ak+2| = · · · = 0}.\nThen there exists a constant A > 0 such that\nV(Ck,∥·∥ℓ2)\n \nδ\n√\nk\n2\n!\n≥Ak,\nk = 1, 2, . . . .\nA.2\nTreatment of multi-dimensional wavelets\nIn this section, we give a multi-dimensional extension of arguments in Section 4.3. Here, we adopt the multipli-\ncation of elementwise wavelets as a multi-dimensional wavelet.\nLemma A.3. For orthogonal wavelets ψ(1), . . . , ψ(d),\nψ(x) :=\nd\nY\ni=1\nψ(i)(xi),\nx = (x1, . . . , xd) ∈Rd,\nis a d-dimensional orthogonal wavelet; i.e.,\n( d\nY\ni=1\nψ(i)\nki,ℓi\n\f\f\f\f\f ki ≥0, 0 ≤ℓi < 2ki, i = 1, . . . , d\n)\nis an orthonormal subset of L2([0, 1]d).\nProof. The normality is clear by Fubini’s theorem. Also, for distinct wavelets ψ, ψ′ in the set, there exists i such\nthat (ki, ℓi) ̸= (k′\ni, ℓ′\ni). Since we have ψψ′ ∈L1([0, 1]d) by the AM-GM inequality, Fubini’s theorem leads to the\nconclusion.\nDeﬁnition A.4. Given an orthonormal wavelet\nψ(x) = ψ(1)(x1) · · · ψ(d)(xd)\nand constants C1, C2, β > 0 and 0 < p < 2, deﬁne\nJ p\nψ(C1, C2, β) :=\n\n\n\nX\n(k,ℓ)∈T0\nak,ℓψk,ℓ\n\f\f\f\f\f\f\n∥a∥wℓp ≤C1,\nX\n(k,ℓ)∈Tm\na2\nk,ℓ≤C22−βm, m = 0, 1, . . .\n\n\n,\nwhere the sets Tm (m = 0, 1, . . .) are deﬁned as\nTm :=\n\n\n(k, ℓ) ∈Zd × Zd\n\f\f\f\f\f\f\nk = (k1, . . . , kd), ℓ= (ℓ1, . . . , ℓd),\nki ≥0, 0 ≤ℓi < 2ki, i = 1, . . . , d,\nmax1≤i≤d ki ≥m\n\n\n,\n21\nand ψk,ℓdenotes\nd\nY\ni=1\nψ(i)\nki,ℓi for each\n(k, ℓ) =\n\u0000(k1, . . . , kd), (ℓ1, . . . , ℓd)\n\u0001\n∈T0.\nRemark A.5. If we deﬁne a partial order on S0 by (k, ℓ) ⪯(k′, ℓ′) ⇔maxi ki ≤maxi k′\ni and then sort it,\nJ p\nψ(C1, C2, β) is revealed to be β/d-minimally tail compact. Thus, Ip\nϕ ⊂J p\nψ holds with some modiﬁcation of\nconstants.\nB\nProofs\nB.1\nProof of Theorem 2.6\n(mainly following the original proof) First, we evaluate the value of\nD :=\n\f\f\f\f\fE\n\"\n1\nn\nn\nX\ni=1\n( bf(Xi) −f ◦(Xi))2\n#\n−R( bf, f ◦)\n\f\f\f\f\f .\nLet X′\n1, . . . , X′\nn be i.i.d. random variables generated to be independent of (Xi, Yi)n\ni=1. Then we have\nR( bf, f ◦) = 1\nn\nn\nX\ni=1\nE\nh\n( bf(X′\ni) −f ◦(X′\ni))2i\n,\nand so we obtain\nD =\n\f\f\f\f\fE\n\"\n1\nn\nn\nX\ni=1\n\u0010\n( bf(Xi) −f ◦(Xi))2 −( bf(X′\ni) −f ◦(X′\ni))2\u0011#\f\f\f\f\f\n≤1\nnE\n\"\f\f\f\f\f\nn\nX\ni=1\n\u0010\n( bf(Xi) −f ◦(Xi))2 −( bf(X′\ni) −f ◦(X′\ni))2\u0011\f\f\f\f\f\n#\n.\nHere, let Gδ = {f1, . . . , fN} be a δ-covering of F with the minimum cardinality in the L∞metric. Notice that\nlog N ≥1. If we deﬁne gj(x, x′) := (fj(x) −f ◦(x))2 −(fj(x′) −f ◦(x′))2 and a random variable J taking values\nin {1, . . . , N} such that ∥bf −fJ∥L∞≤δ, we have\nD ≤1\nnE\n\"\f\f\f\f\f\nn\nX\ni=1\ngJ(Xi, X′\ni)\n\f\f\f\f\f\n#\n+ 8Fδ.\n(9)\nIn the above evaluation, we have used the inequality\n\f\f\f( bf(x) −f ◦(x))2 −(fJ(x) −f ◦(x))2\f\f\f =\n\f\f\f bf(x) −fJ(x)\n\f\f\f\n\f\f\f bf(x) + fJ(x) −2f ◦(x)\n\f\f\f ≤4Fδ.\nDeﬁne constants rj := max{A, ∥fj −f ◦∥L2} (j = 1, . . . , N) and a random variable\nT := max\n1≤j≤N\n\f\f\f\f\f\nn\nX\ni=1\ngj(Xi, X′\ni)\nrj\n\f\f\f\f\f ,\nwhere A > 0 is a deterministic quantity ﬁxed afterward. Then, because of (9), we have\nD ≤1\nnE[rJT] + 8Fδ ≤1\nn\nq\nE[r2\nJ]E[T 2] + 8Fδ ≤1\n2E[r2\nJ] +\n1\n2n2 E[T 2] + 8Fδ\n(10)\nby the Cauchy–Schwarz inequality and the AM-GM inequality.\nHere, by the deﬁnition of J, E[r2\nJ] can be\nevaluated as follows:\nE[r2\nJ] ≤A2 + E\n\u0002\n∥fJ −f ◦∥2\nL2\n\u0003\n≤A2 + E\nh\n∥bf −f ◦∥2\nL2\ni\n+ 4Fδ = R( bf, f ◦) + A2 + 4Fδ.\n(11)\n22\nBecause of the independence of the deﬁned random variables,\nE\n\n\n n\nX\ni=1\ngj(Xi, X′\ni)\nrj\n!2\n=\nn\nX\ni=1\nE\n\"\u0012gj(Xi, X′\ni)\nrj\n\u00132#\n=\nn\nX\ni=1\n\u0012\nE\n\"\n(fj(Xi) −f ◦(Xi))4\nr2\nj\n#\n+ E\n\"\n(fj(X′\ni) −f ◦(X′\ni))4\nr2\nj\n#\u0013\n≤2F 2n\nholds, where we have used the fact that each gj(Xi, X′\ni) is centered. Then, using Bernstein’s inequality, we have,\nin terms of r := min1≤j≤N rj,\nP(T 2 ≥t) = P(T ≥\n√\nt) ≤2N exp\n\n−\nt\n2F 2\n\u0010\n2n +\n√\nt\n3r\n\u0011\n\n,\nt ≥0.\nLet us evaluate E[T 2]. For arbitrary t0 > 0, it holds that\nE[T 2] =\nZ ∞\n0\nP(T 2 ≥t) dt\n≤t0 +\nZ ∞\nt0\nP(T 2 ≥t) dt\n≤t0 + 2N\nZ ∞\nt0\nexp\n\u0012\n−\nt\n8F 2n\n\u0013\ndt + 2N\nZ ∞\nt0\nexp\n\u0012\n−3r\n√\nt\n4F 2\n\u0013\ndt.\nWe compute the values of these two integrals in terms of t0:\nZ ∞\nt0\nexp\n\u0012\n−\nt\n8F 2n\n\u0013\ndt =\n\u0014\n−8F 2n exp\n\u0012\n−\nt\n8F 2n\n\u0013\u0015∞\nt0\n= 8F 2n exp\n\u0012\n−\nt0\n8F 2n\n\u0013\n,\nZ ∞\nt0\nexp\n\u0012\n−3r\n√\nt\n4F 2\n\u0013\ndt =\nZ ∞\nt0\nexp(−a\n√\nt) dt\n(a := 3r/4F 2)\n=\n\u0014\n−2(a\n√\nt + 1)\na2\nexp(−a\n√\nt)\n\u0015∞\nt0\n= 8F 2√t0\n3r\nexp\n\u0012\n−3r√t0\n4F 2\n\u0013\n+ 32F 2\n9r2 exp\n\u0012\n−3r√t0\n4F 2\n\u0013\n.\nNow we determine A = √t0/6n. Since we have r ≥A = √t0/6n,\nE[T 2] ≤t0 + 2N\n\u0012\n8F 2n + 16F 2n + 128F 2n2\nt0\n\u0013\nexp\n\u0012\n−\nt0\n8F 2n\n\u0013\n≤t0 + 16NF 2n\n\u0012\n3 + 16n\nt0\n\u0013\nexp\n\u0012\n−\nt0\n8F 2n\n\u0013\nholds. Letting t0 = 8F 2n log N, the above evaluation can be rewritten as\nE[T 2] ≤8F 2n\n\u0012\nlog N + 6 +\n2\nF 2 log N\n\u0013\n.\n(12)\nFinally, we combine (10), (11), (12), and A2 = 2F 2 log N\n9n\nto obtain\nD ≤\n\u00121\n2R( bf, f ◦) + 1\n2A2 + 2Fδ\n\u0013\n+ 4F 2\nn\n\u0012\nlog N + 6 +\n2\nF 2 log N\n\u0013\n+ 8Fδ\n≤1\n2R( bf, f ◦) + F 2\nn\n\u001237\n9 log N + 32\n\u0013\n+ 10Fδ,\n23\nwhere we have used the fact that log N ≥1. Thus, we obtain the evaluation\nR( bf, f ◦) ≤2E\n\"\n1\nn\nn\nX\ni=1\n( bf(Xi) −f ◦(Xi))2\n#\n+ 2F 2\nn\n\u001237\n9 log N + 32\n\u0013\n+ 20Fδ.\n(13)\nNext, we evaluate the quantity\nbR := E\n\"\n1\nn\nn\nX\ni=1\n( bf(Xi) −f ◦(Xi))2\n#\n.\n(14)\nSince bf is an empirical risk minimizer, for arbitrary f ∈F,\nE\n\"\n1\nn\nn\nX\ni=1\n( bf(Xi) −Yi)2\n#\n≤E\n\"\n1\nn\nn\nX\ni=1\n(f(Xi) −Yi)2\n#\nholds. As Yi = f ◦(Xi) + ξi, we have\nE\n\u0002\n(f(Xi) −Yi)2\u0003\n−E\nh\n( bf(Xi) −Yi)2i\n= E\n\u0002\n(f(Xi) −f ◦(Xi))2\u0003\n−2E [ξif(Xi)] −E\nh\n( bf(Xi) −f ◦(Xi))2i\n+ 2E\nh\nξi bf(Xi)\ni\n=\n\u0010\n∥f −f ◦∥2\nL2 + 2E\nh\nξi bf(Xi)\ni\u0011\n−E\nh\n( bf(Xi) −f ◦(Xi))2i\n.\nHere we have used the fact that\nE[ξif(Xi)] = E[ξi]E[f(Xi)] = 0\nholds because of the independence between ξi and Xi and the fact that both ξi and f(Xi) have a ﬁnite L1 norm.\nThus we have\nbR ≤∥f −f ◦∥2\nL2 + E\n\"\n2\nn\nn\nX\ni=1\nξi bf(Xi)\n#\n.\n(15)\nLet us evaluate the second term on the right-hand side.\n\f\f\f\f\fE\n\"\n2\nn\nn\nX\ni=1\nξi bf(Xi)\n#\f\f\f\f\f =\n\f\f\f\f\fE\n\"\n2\nn\nn\nX\ni=1\nξi( bf(Xi) −f ◦(Xi))\n#\f\f\f\f\f\n≤2δ\nn E\n\" n\nX\ni=1\n|ξi|\n#\n+\n\f\f\f\f\fE\n\"\n2\nn\nn\nX\ni=1\nξi(fJ(Xi) −f ◦(Xi))\n#\f\f\f\f\f .\n(16)\nHere, the ﬁrst term is upper-bounded by applying the Cauchy–Schwarz inequality:\n2δ\nn E\n\" n\nX\ni=1\n|ξi|\n#\n≤2δ\nn E\n\nn1/2\n n\nX\ni=1\nξ2\ni\n!1/2\n≤2δ\n√nE\n\" n\nX\ni=1\nξ2\ni\n#1/2\n= 2σδ.\n(17)\nLet εj (j = 1, . . . , N) be random variables deﬁned as\nεj :=\nPn\ni=1 ξi(fj(Xi) −f ◦(Xi))\n\u0000Pn\ni=1(fj(Xi) −f ◦(Xi))2\u00011/2 ,\nwhere εj := 0 if the denominator equals 0. Notice that each εj follows a centered Gaussian distribution with\nvariance σ2 (conditional on X1, . . . , Xn). Now we have, using the Cauchy–Schwarz inequality and the AM-GM\n24\ninequality,\n\f\f\f\f\fE\n\"\n2\nn\nn\nX\ni=1\nξi(fJ(Xi) −f ◦(Xi))\n#\f\f\f\f\f = 2\nn\n\f\f\f\f\f\f\nE\n\n\n n\nX\ni=1\n(fJ(Xi) −f ◦(Xi))2\n!1/2\nεJ\n\n\n\f\f\f\f\f\f\n≤\n2\n√nE\n\"\n1\nn\nn\nX\ni=1\n(fJ(Xi) −f ◦(Xi))2\n#1/2\nE\n\u0014\nmax\n1≤j≤N ε2\nj\n\u00151/2\n≤\n2\n√n\nq\nbR + 4Fδ E\n\u0014\nmax\n1≤j≤N ε2\nj\n\u00151/2\n≤1\n2( bR + 4Fδ) + 2\nnE\n\u0014\nmax\n1≤j≤N ε2\nj\n\u0015\n.\n(18)\nBy a similar argument as given in the proof of Laﬀerty et al. (2008, Theorem 7.47), for any 0 < t < 1/2σ2,\nexp\n\u0012\ntE\n\u0014\nmax\n1≤j≤N ε2\nj\n\u0015\u0013\n≤E\n\u0014\nmax\n1≤j≤N exp\n\u0000tε2\nj\n\u0001\u0015\n(by Jensen’s inequality)\n≤NE\n\u0002\nexp\n\u0000tε2\n1\n\u0001\u0003\n=\nN\n√\n2πσ2\nZ ∞\n−∞\netx2e−x2\n2σ2 dx =\nN\n√\n1 −2σ2t\nholds. Therefore we have, by determining t = 1/4σ2,\nE\n\u0014\nmax\n1≤j≤N ε2\nj\n\u0015\n≤4σ2 log(\n√\n2N) ≤4σ2(log N + 1).\n(19)\nNow we combine (15)–(19) to obtain\nbR ≤∥f −f ◦∥2\nL2 + 2σδ + 1\n2( bR + 4Fδ) + 8σ2\nn (log N + 1),\nand so\nbR ≤2∥f −f ◦∥2\nL2 + 4(σ + F)δ + 16σ2\nn\n(log N + 1)\n(20)\nholds.\nFinally, since f is an arbitrary element of F, we combine (13), (14), and (20) to have\nR( bf, f ◦) ≤4 inf\nf∈F ∥f −f ◦∥2\nL2 + 1\nn\n\u0012\u001237\n9 F 2 + 32σ2\n\u0013\nlog N + 32(F 2 + σ2)\n\u0013\n+ (18F + 8σ)δ,\nand this leads to the conclusion.\nB.2\nProof of Theorem 3.3\nWe ﬁrst prove the following lemma.\nLemma B.1. For any aﬃne estimator bf and a sequence f ◦\n1 , f ◦\n2 , . . . ∈L2([0, 1]d) convergent to f ◦\n∞∈L2([0, 1]d)\nalmost everywhere,\nR( bf, f ◦\n∞) ≤sup\nm≥1\nR( bf, f ◦\nm)\nholds.\nProof. The conclusion follows immediately from Fatou’s lemma and Eqs. (4) and (5).\nRemark B.2. In the proof of Lemma B.1, we have not used the linearity of bf. Indeed, if bf(X) −f ◦\nm(X) is\nconvergent to bf(X) −f ◦\n∞(X) with probability 1, then we have the same conclusion (where X is a uniformly\ndistributed random variable independent of other observed random variables). Hence, Lemma B.1 is applicable\nto a broader class of estimators, such as estimators continuous with respect to observed data in some metric.\nThe assertion of Theorem 3.3 is now clear from Eq. (2), Lemma B.1, and the fact that a sequence convergent\nto a function in L2 has a subsequence that is convergent to the same function almost everywhere.\n25\nB.3\nProof of Lemma 3.8\nThe following lemma is a well-known property of functions of bounded total variation.\nLemma B.3. (Stein and Shakarchi, 2005) For each function f : [0, 1] →R with TV f([0, 1]) < ∞, there exist\nincreasing f +, f −: [0, 1] →R such that\nf = f + −f −+ f(0),\nf +(0) = f −(0) = 0,\nf +(t) + f −(t) = TV f([0, t]),\n0 ≤t ≤1,\nholds.\nBy using this lemma, Lemma 3.8 can be proven as follows.\nSince J1(C) ⊂Jk(C) holds for each k, it suﬃces to show the assertion for k = 1. By the deﬁnition of convex\nhull, we have\nconv(J1(C)) =\n(\na0 +\nk\nX\ni=1\nai1[ti,1]\n\f\f\f\f\f ti ∈(0, 1], |a0| ≤C,\nk\nX\ni=1\n|ai| ≤C, k ≥1\n)\n=\n∞\n[\nk=1\nJk(C).\nIt is obvious that Jk(C) ⊂BV (C) for each k. Thus, we have only to show that for each f ∈BV (C) and ε > 0,\nthere exist some k ≥1 and fk ∈Jk(C) such that ∥fk −f∥L2 ≤ε holds.\nLet f ∈BV (C), and take f + and f −satisfying the condition of Lemma B.3. Then f can be written as\nf = a0 + f + −f −, where a0 := f(0) is a constant. Let g : [0, 1] →R be an increasing function satisfying\ng(0) = 0, and deﬁne\ngk :=\nk\nX\ni=1\n\u0012\ng\n\u0012 i\nk\n\u0013\n−g\n\u0012i −1\nk\n\u0013\u0013\n1[i/k,1].\nThen we have gk ∈Jk(g(1)) and gk(t) = g(i/k) for t ∈[i/k, (i + 1)/k), and so\nZ 1\n0\n(gk(t) −g(t))2 dt ≤\nk−1\nX\ni=0\n1\nk\n\u0012\ng\n\u0012i + 1\nk\n\u0013\n−g\n\u0012 i\nk\n\u0013\u00132\n≤g(1)\nk\nk−1\nX\ni=0\n\u0012\ng\n\u0012i + 1\nk\n\u0013\n−g\n\u0012 i\nk\n\u0013\u0013\n= g(1)2\nk\nholds because g is increasing. Take f +\nk , f −\nk similarly, and fk := a0 + f +\nk −f −\nk satisﬁes\nfk ∈J2k(C),\n∥fk −f∥2\nL2 ≤2C2\nk ,\nand the proof is complete. We can of course take fk directly only from f, but we have chosen an easier argument.\nB.4\nProof of Theorem 4.2\nSince a smaller set makes the minimax risk smaller, it suﬃces to consider\nI := {cϕ | |c| ≤C} ⊂I0\nΦ.\nFor simplicity, assume C ≥1 (otherwise, take the functions and constants appearing below to be smaller at that\nrate). Deﬁne for each n = 1, 2, . . .\nf +\nn :=\n1\n2√nϕ,\nf −\nn := −\n1\n2√nϕ.\nThen, by applying the argument appearing in Yang and Barron (1999), we have\ninf\nb\nf\nsup\nf∈I\nPf\n\u0012\n∥f −bf∥L2 ≥\n1\n2√n\n\u0013\n≥inf\nb\nf\nsup\nf∈{f +\nn ,f −\nn }\nPf\n\u0012\n∥f −bf∥L2 ≥\n1\n2√n\n\u0013\n≥inf\nb\nf\nsup\nf∈{f +\nn ,f −\nn }\nPf( ef ̸= f),\n(21)\n26\nwhere ef is the closer of the two (f +\nn , f −\nn ) to bf. Following the argument in Tsybakov (2008, Proposition 2.1), we\nhave for any t > 0\nPf +\nn\n\u0010\nef ̸= f +\nn\n\u0011\n= Ef −\nn\n\"\n1{ e\nf=f −\nn }(Zn)\ndPf +\nn\ndPf −\nn\n(Zn)\n#\n≥tPf −\nn\n \nef = f −\nn ,\ndPf +\nn\ndPf −\nn\n(Zn) ≥t\n!\n≥t\n \nPf −\nn ( ef = f −\nn ) −Pf −\nn\n \ndPf +\nn\ndPf −\nn\n(Zn) < t\n!!\n.\n(22)\nHere, Zn denotes the i.i.d. sequence (Xi, Yi)n\ni=1, and dPf +\nn / dPf −\nn represents the Radon–Nikodym derivative.\nThen we have, by (21) and (22),\ninf\nb\nf\nsup\nf∈I\nPf\n\u0012\n∥f −bf∥L2 ≥\n1\n2√n\n\u0013\n≥\n1\n1 + t\n\u0010\ntPf −\nn\n\u0010\nef ̸= f −\nn\n\u0011\n+ Pf +\nn\n\u0010\nef ̸= f +\nn\n\u0011\u0011\n≥\nt\n1 + t\n \n1 −Pf −\nn\n \ndPf +\nn\ndPf −\nn\n(Zn) < t\n!!\n=\nt\n1 + tPf −\nn\n \ndPf +\nn\ndPf −\nn\n(Zn) ≥t\n!\n.\n(23)\nWhen f ◦= f −\nn holds, the Radon–Nikodym derivative appearing in (23) can be explicitly written as\ndPf +\nn\ndPf −\nn\n(Zn) = exp\n \n1\n2σ2\nn\nX\ni=1\n(Yi −f −\nn (Xi))2 −\n1\n2σ2\nn\nX\ni=1\n(Yi −f +\nn (Xi))2\n!\n= exp\n \n1\n2σ2\nn\nX\ni=1\n\u0000ξ2\ni −(ξi + f −\nn (Xi) −f +\nn (Xi))2\u0001\n!\n= exp\n \n2\n2σ2√n\nn\nX\ni=1\nξiϕ(Xi) −\n1\n2σ2n\nn\nX\ni=1\nϕ(Xi)2\n!\n.\n(24)\nLet us consider the right-hand side of (24). First, Pn\ni=1 ξiϕ1(Xi) is a sum of independent symmetric random\nvariables, and so the sum itself is also symmetric (X is symmetric if −X has the same distribution as X), and\nthus we have P (Pn\ni=1 ξiϕ1(Xi) ≥0) ≥1/2. Second, for any s > 0, by Markov’s inequality,\nP\n \n1\n2σ2n\nn\nX\ni=1\nϕ(Xi)2 ≥s\n!\n≤1\nsE\n\"\n1\n2σ2n\nn\nX\ni=1\nϕ(Xi)2\n#\n=\n1\n2σ2s\nholds. We determine s = 2/σ2 to obtain the evaluation\nP\n \n−\n1\n2σ2n\nn\nX\ni=1\nϕ1(Xi)2 ≥−2\nσ2\n!\n≥3\n4.\nFinally, we have\nP\n \n2\n2σ2√n\nn\nX\ni=1\nξiϕ1(Xi) −\n1\n2σ2n\nn\nX\ni=1\nϕ(Xi)2 ≥−2\nσ2\n!\n≥1\n4.\n(25)\nBy (23)–(25) and letting t = e−2/σ2, we have\ninf\nb\nf\nsup\nf∈I\nPf\n\u0012\n∥f −bf∥L2 ≥\n1\n2√n\n\u0013\n≥\ne−2/σ2\n1 + e−2/σ2 · 1\n4 ≥1\n8e−2/σ2,\nand so we ﬁnally obtain the evaluation\ninf\nb\nf\nsup\nf∈I\nEf\nh\n∥f −bf∥2\nL2\ni\n≥e−2/σ2\n32n ,\nand this is the desired result.\n27\nB.5\nProof of Lemma 4.7\nFirst, we prove the lower-bound part, partially using the proofs in Donoho (1996).\nWhen we consider the\ncovering entropy, we have only to consider the coeﬃcients, as ϕ = (ϕi)∞\ni=1 is an orthonormal set. Thus, deﬁne\nA ⊂ℓ2 as\nA :=\n(\na ∈ℓ2\n\f\f\f\f\f ∥a∥wℓp ≤C1,\n∞\nX\ni=m+1\na2\ni ≤C2m−β, m = 1, 2, . . .\n)\n.\n(26)\nThen it suﬃces to evaluate V (ε) := V(A,∥·∥ℓ2)(ε).\nFor each k = 1, 2, . . ., let a(k) ∈ℓ2 be deﬁned as\na(k) := (C1k−1/p, . . . , C1k−1/p\n|\n{z\n}\nk times\n, 0, 0, . . .).\nThen ∥a(k)∥wℓp = C1 holds. Let us consider the second condition. For 1 ≤m ≤k,\nmβ\n∞\nX\ni=m+1\n(a(k)\ni\n)2 = mβ(k −m)(C1k−1/p)2\n(27)\nholds. Since xβ(k−x) is maximized over x ∈[0, k] at x =\nβ\n1+β k, the left-hand side of (27) is bounded independent\nof m as\nsup\nm≥1\nmβ\n∞\nX\ni=m+1\n(a(k)\ni\n)2 ≤C2\n1\nββ\n(1 + β)1+β k1+β−2/p ≤C2\n1\nββ\n(1 + β)1+β ,\nwhere we have used the assumption β ≤2α for the latter inequality. If we deﬁne a constant\nC := min\n(\n1, C1/2\n2\n(1 + β)(1+β)/2\nC1ββ/2\n)\n,\nthen each Ca(k) is an element of A. For each k, let us consider a hyperrectangle deﬁned as\nAk := {a ∈ℓ2 | |ai| ≤a(k)\ni\n, i = 1, 2, . . .}.\nObviously, each Ak is a subset of A (this actually is based on the fact that ϕ is an unconditional basis of Ip\nϕ),\nand so we have V (ε) ≥V(Ak,∥·∥ℓ2)(ε). For each pair of distinct vertices of Ak (which has 2k vertices), the ℓ2\ndistance between the two is at least CC1k−1/p, and so, by setting δ = k−1/p in Lemma A.2, we have, for A\nappearing in the lemma,\nV\n\u0012CC1\n2\nk1/2−1/p\n\u0013\n≥V(Ak,∥·∥ℓ2)\n\u0012CC1\n2\nk1/2−1/p\n\u0013\n≥Ak\nfor each k. If we write C′ = CC1/2, then we have V (C′k−α) ≥Ak. Therefore, for ε ∈[C′2−(j+1)α, C′2−jα],\nV (ε) ≥V (C′2−jα) ≥2jA = 2j+1 A\n2 ≥A\n2\n\u0010 ε\nC′\n\u0011−1/α\n= cε−1/α\nholds, where c := AC′1/α/2. This evaluation holds only for 0 < ε ≤C′; however, we have V (ε) ≥1 for ε > C′,\nand hence V (ε) ≥C′1/αε−1/α holds. Thus, we have reached the desired result.\nThen, we deal with the upper-bound part. By the same logic as in the proof of lower-bound, it suﬃces to\nevaluate the metric entropy of A in (26). Let V (ε) := V(A,∥·∥ℓ2)(ε) similarly. For an arbitrary element a ∈A,\nlet ij be the index of the term of a having the j-th largest absolute value; i.e.,\n|ai1| ≥|ai2| ≥· · · ,\nwhich is a permutation of (|ai|)∞\ni=1. By (6) and the deﬁnition of A,\n∞\nX\nj=k+1\na2\nij ≤\n∞\nX\nj=k+1\nC1j−2/p ≤\nZ ∞\nk\nC1x−2/p dx = C1\n2αk−2α\n28\nholds for each k. Also, by the second condition of A,\nX\ni≥k2α/β+1\na2\ni ≤C2k−β· 2α\nβ = C2k−2α\nholds. Thus, if we deﬁne ea := (a1, . . . , a⌈k2α/β⌉, 0, 0, . . .) and ei1,ei2, . . . similarly,\nb := (bi)∞\ni=1,\nbi :=\n(\neai (= ai)\nif i ∈{ei1, . . . ,eik}\n0\notherwise\n,\nsatisﬁes ∥a −b∥ℓ2 ≤\np\nC1/2α + C2 · k−α. Then its quantization\neb :=\n\u0012\nsgn(bi)⌊k1/2+α|bi|⌋\nk1/2+α\n\u0013∞\ni=1\nsatisﬁes ∥b −eb∥ℓ2 ≤k−α as b has at most k nonzero terms. Since |bi| ≤C1, the number of values possibly taken\nby ebi is at most 2C1k1/2+α + 1. Hence, the logarithm of the number of such eb values can be upper-bounded by\nlog\n\u0012\u0012⌈k2α/β⌉\nk\n\u0013\n(2C1k1/2+α + 1)k\n\u0013\n≤2α\nβ k log k +\n\u00121\n2 + α\n\u0013\nk log k + k log(2C1 + 1)\n≤C0k(log k + 1),\n(28)\nwhere C0 > 0 is a constant. Since ∥a −eb∥ℓ2 ≤(1 +\np\nC1/2α + C2)k−α holds, if we take k ∼ε−1/α in the same\nway as used in the proof of lower-bound, then we reach the conclusion.\nRemark B.4. In the case in which β ≥2α holds, we can obtain a more accurate bound by using Stirling’s\napproximation. However, we can see that such a case no longer requires the concept of weak ℓp norms, or else\nits conditions are too strong. Therefore, we have not treated this case.\nB.6\nProof of Theorem 4.15\nThis proof is a reﬁnement of the proof of Theorem 1 in Zhang et al. (2002). First, we prove the following lemma.\nLemma B.5. Let γ ∈(0, 1), and let m be a positive integer such that m ≤nγ ≤2m. For k = 0, . . . , m −1, let\nAk be the number of X1, . . . , Xn contained in [k/m, (k + 1)/m). Then there exists a constant c = c(γ) > 0 such\nthat\nP\n\u0012\nmax\n0≤k≤m−1 Ak ≥cn\nm\n\u0013\n≤2−n1−γ\nholds for any m, n satisfying the condition.\nProof. For a ﬁxed k, Ak can be written as Ak = Pn\nj=1 ηj, where (ηj)n\nj=1 is an i.i.d. sequence with P(ηj = 0) =\n1/m and P(ηj = 1) = 1 −1/m. Then, by Chernoﬀ’s inequality, we have for t > 0\nP\n\u0010\nAk ≥cn\nm\n\u0011\n≤e−cnt/mE\n\u0002\neAkt\u0003\n= e−cnt/mE\n\u0002\neη1t\u0003n = e−cnt/m\n\u0012\n1 −1\nm + 1\nmet\n\u0013n\n.\nSetting t = log 2 and assuming c > log 2, we obtain\nP\n\u0010\nAk ≥cn\nm\n\u0011\n≤2−cn/m\n\u0012\n1 + 1\nm\n\u0013n\n≤(2−ce)n/m ≤(2−ce)2n1−γ,\nwhere we have used the fact that (1 + 1/x)x is increasing on x > 0. Then we ﬁnally have\nP\n\u0012\nmax\n0≤k≤m−1 Ak ≥cn\nm\n\u0013\n≤m(2−ce)2n1−γ ≤nγ(2−ce)2n1−γ = 2−n1−γ · nγ \u0010\n2−(2c−1)e2\u0011n1−γ\n.\nConsidering the logarithm of the last term, it is suﬃcient to take c as large enough to satisfy\n(2c −1) log 2 ≥2 + γ max\nn≥1\nlog n\nn1−γ ,\nand we obtain the conclusion.\n29\nWe now prove Theorem 4.15. Let γ =\n1\n1+β , and let\nR∗:=\ninf\nb\nf:linear\nsup\nf ◦∈J p\nψ(C1,C2,β)\nE\nh\n∥bf −f ◦∥2\nL2\ni\n.\nFix a linear estimator bf(x) = Pn\ni=1 Yiϕi(x; Xn). For any f ◦∈BV (C), we have by Fubini’s theorem\nR∗≥E\nh\n∥bf −f ◦∥2\nL2\ni\n= E\n\n\n\r\r\r\r\r\nn\nX\ni=1\nf ◦(Xi)ϕi(·; Xn) −f ◦\n\r\r\r\r\r\n2\nL2\n\n+ σ2\nn\nX\ni=1\nE\n\u0002\n∥ϕi(·; Xn)∥2\nL2\n\u0003\n.\n(29)\nTake a suﬃciently large n, and let m be a power of 2 in [ 1\n2nγ, nγ]. Notice that it holds that m ≤nγ ≤2m.\nThen there exists an integer 0 ≤k < m such that\nZ (k+1)/m\nk/m\nE\n\" n\nX\ni=1\nϕi(x; Xn)2\n#\ndx ≤R∗\nσ2m.\n(30)\nLet f ◦= Fm−β/2 ·m1/2ψ(m ·−k), where F := min{C1, C1/2\n2\n}. Then we have f ◦∈J p\nψ(C1, C2, β). Let A denote\nan event assured to have a probability of at least 1 −2−n1−γ in Lemma B.5, and we obtain\nZ (k+1)/m\nk/m\nE\n\n\n n\nX\ni=1\nf ◦(Xi)ϕi(x; Xn)\n!2\n, A\n\ndx\n≤\nZ (k+1)/m\nk/m\nE\n\" n\nX\ni=1\nf ◦(Xi)2\n!  n\nX\ni=1\nϕi(x; Xn)2\n!\n, A\n#\ndx\n≤M · cn\nm (Fm(1−β)/2∥ψ∥∞)2\nZ (k+1)/m\nk/m\nE\n\" n\nX\ni=1\nϕi(x; Xn)2, A\n#\ndx ≤McF 2∥ψ∥2\n∞\nσ2\n· R∗n\nm1+β ,\n(31)\nwhere M is the number of sections [ℓ, ℓ+ 1) such that ℓis an integer and [ℓ, ℓ+ 1) ∩supp(ψ) is not empty. The\nlast inequality has been derived by (30). By (29), (31), and the triangle inequality,\n√\nR∗≥\n\n\nZ (k+1)/m\nk/m\nE\n\n\n n\nX\ni=1\nf ◦(Xi)ϕi(x; Xn) −f ◦(x)\n!2\n, A\n\ndx\n\n\n1/2\n≥\n\u0000P(A)∥f ◦∥2\nL2\n\u00011/2 −\n\nE\n\n\n n\nX\ni=1\nf ◦(Xi)ϕi(x; Xn)\n!2\n, A\n\ndx\n\n\n1/2\n≥(1 −2−n1−γ)1/2Fm−β/2 −\n\u0012McF 2∥ψ∥2\n∞\nσ2\n\u00131/2\nm−(1+β)/2n1/2√\nR∗.\nSince n is suﬃciently large, we can assume 1 −2−n1−γ ≥1/2. Deﬁne a constant G by\nG :=\n\u0012McF 2∥ψ∥2\n∞\nσ2\n\u00131/2\n.\nWe have m1+β = m1/γ ≥( 1\n2nγ)1/γ = 2−1/γ by assumption, and so we obtain\nR∗≥\n(F 2/2)m−β\n(1 + Gm−(1+β)/2n1/2)2 ≥\nF 2\n2(1 + 21/γG)2 m−β ≥\nF 2\n2(1 + 21/γG)2 n−\nβ\n1+β\nas desired.\n30\nB.7\nProof of Lemma 5.2\n(mainly following Suzuki (2019)) For f ∈N(L, S, D, B) expressed as\nf = WL+1 ◦ρ(WL · −vL) ◦· · · ◦ρ(W1 · −v1),\nlet us deﬁne\nAk(f) := ρ(Wk−1 · −vk−1) ◦· · · ◦ρ(W1 · −v1),\nBk(f) := WL+1 ◦ρ(WL · −vL) ◦· · · ◦ρ(Wk · −vk)\nfor k = 1, . . . , L + 1, where A1(f) denotes the identity map, and BL+1(f) = WL+1. Then f = Bk+1(f) ◦ρ(Wk ·\n−vk) ◦Ak(f) holds for k = 1, . . . , L. Here, notice that for each x ∈[0, 1]d and 1 ≤k ≤L + 1,\n∥Ak(f)(x)∥∞≤D∥Wk−1∥∞∥Ak−1(f)(x)∥∞+ ∥vk−1∥∞\n≤DB∥Ak−1(x)∥∞+ B\n≤B + DB2 + D2B3 + · · · + Dk−2Bk−1 + Dk−1Bk−1\n≤Bk−1(D + 1)k−1\n(32)\nholds, where we have used the assumption B ≥1 at the last inequality. Also, the Lipschitz continuity of Bk(f)\ncan be derived as\n∥Bk(f)(x) −Bk(f)(x′)∥∞≤(BD)L−k+2∥x −x′∥∞.\n(33)\nLet ε > 0. Suppose f, g ∈N(L, S, D, B) satisfy\nf = WL+1 ◦ρ(WL · −vL) ◦· · · ◦ρ(W1 · −v1),\ng = W ′\nL+1 ◦ρ(W ′\nL · −v′\nL) ◦· · · ◦ρ(W ′\n1 · −v′\n1)\nand ∥Wi −W ′\ni∥∞≤ε, ∥vi −v′\ni∥∞≤ε for each i. Then we have by (32) and (33)\n|f(x) −g(x)| ≤\nL\nX\nk=1\n|Bk+1(f) ◦ρ(Wk · −vk) ◦Ak(g)(x) −Bk+1(f) ◦ρ(W ′\nk · −v′\nk) ◦Ak(g)(x)|\n+ |(WL+1 −W ′\nL+1)AL+1(g)(x)|\n≤\nL\nX\nk=1\n(BD)L−k+1 \u0000εD(B(D + 1))k−1 + ε\n\u0001\n+ εDBL(D + 1)L\n≤ε(L + 1)BL(D + 1)L+1.\nTherefore, for a ﬁxed sparsity pattern, and letting ε =\n\u0000(L + 1)BL(D + 1)L+1\u0001−1 δ, the δ-covering number is\nbounded by\n\u00122B\nε\n\u0013S\n= δ−S \u00002(L + 1)BL+1(D + 1)L+1\u0001S .\nThe number of such patterns is bounded by\n\u0000(D+1)L\nS\n\u0001\n≤(D + 1)LS, and so the δ-covering entropy is bounded by\nlog\n\u0010\n(d + 1)LSδ−S \u00002(L + 1)BL+1(D + 1)L+1\u0001S\u0011\n(34)\n= S log\n\u00002δ−1(L + 1)(D + 1)2L+1BL+1\u0001\n≤2S(L + 1) log\n\u0012B(L + 1)(D + 1)\nδ\n\u0013\n,\n(35)\nas desired.\nB.8\nProof of Theorem 5.10\nLet N be an integer in [n\n1\n2α+1 , 2n\n1\n2α+1 ]. Also, suppose that we have an integer m in [\n2α\nβ(2α+1) log2 n,\n4α\nβ(2α+1) log2 n]\n(for suﬃciently large n). Fix the target function\nf ◦=\nX\n(k,ℓ)∈T0\nak,ℓψk,ℓ.\n31\nThen, let (k1, ℓ1), . . . , (kN, ℓN) ∈T0 \\ Tm be the N (absolutely) largest coeﬃcients ak1,ℓ1, . . . , akN,ℓN . Let\nT := {(k1, ℓ1), . . . , (kN, ℓN)}.\nThen we have\n\r\r\r\r\r\r\nX\n(k,ℓ)∈T\nak,ℓψk,ℓ−f ◦\n\r\r\r\r\r\r\n2\nL2\n=\nX\n(k,ℓ)∈T0\\T\na2\nk,ℓ≤\nX\n(k,ℓ)∈Tm\na2\nk,ℓ+\n∞\nX\ni=N+1\nC1(i−1/p)2\n≤C2n−\n2α\n2α+1 + C1\nZ ∞\nN\nx−2/p dx\n= C2n−\n2α\n2α+1 + C1\n1 −p\np\nN −2α ≤\n\u0012\nC2 + C1\n1 −p\np\n\u0013\nn−\n2α\n2α+1 .\n(36)\nNext, we approximate P\n(k,ℓ)∈T ak,ℓψk,ℓby some neural network. Now, (k, ℓ) ∈T implies that\n(k, ℓ) =\n\u0000(k1, . . . , kd), (ℓ1, . . . , ℓd)\n\u0001\nsatisﬁes max1≤i≤d ki < m. Let eψ ∈N(Lε, Sε, Dε, Bε) satisfy ∥eψ −ψ∥L2 ≤ε. Since we have\nψk,ℓ(x1, . . . , xd) = 2\nk1+···+kd\n2\nψ\n\u00002k1t1 −ℓ1, . . . , 2kdtd −ℓd\n\u0001\n,\nwe can construct the approximator\nef ∈N(Lε + 2, N(Sε + 2Dεd + d2 + d + 1), NDε, max{Bε, 2m})\n(37)\nin a manner similar to that shown in Fig. 3 such that\n\r\r\r\r\r\r\nef −\nX\n(k,ℓ)\nak,ℓψk,ℓ\n\r\r\r\r\r\r\nL2\n≤\nX\n(k,ℓ)∈T\n|ak,ℓ|∥eψk,ℓ−ψk,ℓ∥L2 ≤C1Nε ≤2C1εn\n1\n2α+1\n(38)\nholds. If we determine ε = 1/n and deﬁne NF by using the set deﬁned in (37), we have, by Lemma 5.2 and the\nassumption,\nV(NF ,∥·∥L∞)\n\u0012 1\nn\n\u0013\n≤2N\n\u0000S1/n + 2D1/nd + d2 + d + 1\n\u0001\n(L1/n + 3) log\n\u0000max{B1/n, 2m}(L1/n + 3)(ND1/n + 1)n\n\u0001\n≤4n\n1\n2α+1 \u0000C′\n1(1 + 2d) log n + d2 + d + 1\n\u0001\n(C′\n1 log n + 3)\n·\n\u0012\nlog log(C′\n2n) +\n2α\nβ(2α + 1) log n + log(C′\n1 log n + 3) + log(C′\n1N log n + 1) + log n\n\u0013\n≤C′n\n1\n2α+1 (log n)3\n(39)\nfor some constant C′ > 0.\nCombining (36), (37), (39), and Theorem 2.6, we obtain an evaluation\nR( bf, f ◦) ≤4\n \u0012\nC2 + C1\n1 −p\np\n\u00131/2\nn−\nα\n2α+1 + 2C1n−\n2α\n2α+1\n!2\n+ C′′\n\u0012\nC′(F 2 + σ2)n−\n2α\n2α+1 (log n)3 + F + σ\nn\n\u0013\n≤CF 2n−\n2α\n2α+1 (log n)3\nfor some C > 0, where bf denotes the empirical risk minimizer over NF .\nB.9\nProof of Theorem 5.14\nWe consider functions expressed as f N = PN\ni=1 cif(Ai · −bi) ∈N N(L, S, D, B), where f ∈N(L, S, D, B). Then\nit suﬃces to consider f deﬁned over [−B(d + 1), B(d + 1)]d ⊂[−B(D + 1), B(D + 1)]d. This changes evaluation\n(32) to ∥Ak(f)(x)∥∞≤Bk(D + 1)k, and so BL+1(D + 1)L+1 in (34) is replaced by BL+2(D + 1)L+2 in the\nevaluation of the covering entropy with the domain limited to [−B(D + 1), B(D + 1)]d. However, the upper\nbound (35) is still valid if L ≥2, and so we have a set Nε for each 0 < ε < 1 satisfying\n32\n• Nε is an ε-covering of N(L, S, D, B) with respect to ∥· ∥L∞, where the domain is limited to [−B(D +\n1), B(D + 1)]d;\n• log |Nε| ≤2S(L + 1) log\n\u0012B(L + 1)(D + 1)\nε\n\u0013\n.\nFor x ∈[0, 1]d, we have |f(Aix −bi)| ≤BL+2(D + 1)L+2 by an evaluation similar to the one in (32). Also, we\nhave the Lipschitz continuity |f(x) −f(x′)| ≤BL+1DL+1∥x −x′∥∞, similar to (33).\nLet us consider two functions in N N(L, S, D, B) expressed as\nf N =\nN\nX\ni=1\ncif(Ai · −bi),\ngN =\nN\nX\ni=1\nc′\nig(A′\ni · −b′\ni),\nf, g ∈N(L, S, D, B),\nsuch that ∥f −g∥L∞([−B(D+1),B(D+1)]d) ≤ε holds and ∥Ai −A′\ni∥∞, ∥bi −b′\ni∥∞, |ci −c′\ni| ≤ε holds for each i.\nThen we have for each x ∈[0, 1]d\n|f N(x) −gN(x)|\n≤\nN\nX\ni=1\n|cif(Aix −bi) −c′\nig(A′\nix −b′\ni)|\n≤\nN\nX\ni=1\n|ci −c′\ni||f(Aix −bi)| +\nN\nX\ni=1\n|c′\ni||f(Aix −bi) −g(Aix −bi)| +\nN\nX\ni=1\n|c′\ni||g(Aix −bi) −g(A′\nix −b′\ni)|\n≤NεBL+2(D + 1)L+2 + NBε + NB · BL+1DL+1(εd + ε)\n≤3NεBL+2(D + 1)L+2.\nIf we determine ε = (3NBL+2(D + 1)L+2)−1δ for 0 < δ < 1, the δ-covering entropy of N N(L, S, D, B) is now\nbounded by\nlog\n \u00122B\nε\n\u0013N(d2+d+1)\n|Nε|\n!\n= N(d2 + d + 1) log\n\u00122B\nε\n\u0013\n+ log |Nε|\n≤N(d + 1)2 log\n\u00126NBL+3(D + 1)L+2\nδ\n\u0013\n+ 2S(L + 1) log\n\u00123NBL+3(L + 1)(D + 1)L+3\nδ\n\u0013\n≤\n\u0000N(d + 1)2 + 2S(L + 1)\n\u0001\n(L + 3) log\n\u0012NB(L + 1)(D + 1)\nδ\n\u0013\n,\nas desired.\nB.10\nProof of Lemma A.1\nThe probability law Pf generated by f is regarded as being on Rd × R. Hence, its density at z = (x, y) is\npf(z) = pX(x)pY |X(y | x) =\n1\n√\n2πσ2 exp\n\u0012\n−(y −f(x))2\n2σ2\n\u0013\n.\nAs pg can be calculated in the same way, we have\ndKL(f, g)2 =\nZ\nRd×R\npf(z) log pf(z)\npg(z) dz =\nZ\nRd×R\npf(z) 1\n2σ2\n\u0000(y −g(x))2 −(y −f(x))2\u0001\ndz.\nThis coincides with the expectation of\n1\n2σ2\n\u0000(Y −g(X))2 −(Y −f(X))2\u0001\nwith Y = f(X) + ξ. The term in\nparentheses is calculated as\nE\n\u0002\n(Y −g(X))2 −(Y −f(X))2\u0003\n= E\n\u0002\n(f(X) −g(X) + ξ)2 −ξ2\u0003\n= E\n\u0002\n(f(X) −g(X))2 −2ξ(f(X) −g(X))\n\u0003\n= E\n\u0002\n(f(X) −g(X))2\u0003\n−2E[ξ] · E[f(X) −g(X)] = ∥f −g∥2\nL2,\nwhere we have used the facts that each X follows the uniform distribution over [0, 1]d and that each ξ is\nindependent of X. Thus, we obtain the desired result.\n33\n",
  "categories": [
    "stat.ML",
    "cs.LG",
    "math.ST",
    "stat.TH",
    "62G08"
  ],
  "published": "2019-05-22",
  "updated": "2019-09-20"
}