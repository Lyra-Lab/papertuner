{
  "id": "http://arxiv.org/abs/1905.09680v1",
  "title": "DEEP-BO for Hyperparameter Optimization of Deep Networks",
  "authors": [
    "Hyunghun Cho",
    "Yongjin Kim",
    "Eunjung Lee",
    "Daeyoung Choi",
    "Yongjae Lee",
    "Wonjong Rhee"
  ],
  "abstract": "The performance of deep neural networks (DNN) is very sensitive to the\nparticular choice of hyper-parameters. To make it worse, the shape of the\nlearning curve can be significantly affected when a technique like batchnorm is\nused. As a result, hyperparameter optimization of deep networks can be much\nmore challenging than traditional machine learning models. In this work, we\nstart from well known Bayesian Optimization solutions and provide enhancement\nstrategies specifically designed for hyperparameter optimization of deep\nnetworks. The resulting algorithm is named as DEEP-BO (Diversified,\nEarly-termination-Enabled, and Parallel Bayesian Optimization). When evaluated\nover six DNN benchmarks, DEEP-BO easily outperforms or shows comparable\nperformance with some of the well-known solutions including GP-Hedge,\nHyperband, BOHB, Median Stopping Rule, and Learning Curve Extrapolation. The\ncode used is made publicly available at https://github.com/snu-adsl/DEEP-BO.",
  "text": "DEEP-BO for Hyperparameter Optimization of Deep\nNetworks\nHyunghun Cho\nDepartment of Transdisciplinary Studies,\nSeoul National University\nwebofthink@snu.ac.kr\nYongjin Kim\nCollege of Liberal Studies,\nSeoul National University\nyongjinkim@snu.ac.kr\nEunjung Lee, Daeyoung Choi\nDepartment of Transdisciplinary Studies,\nSeoul National University\n{ej-lee,choid}@snu.ac.kr\nYongjae Lee\nSchool of Management Engineering,\nUNIST\nyongjaelee@unist.ac.kr\nWonjong Rhee\nDepartment of Transdisciplinary Studies,\nSeoul National University\nwrhee@snu.ac.kr\nAbstract\nThe performance of deep neural networks (DNN) is very sensitive to the partic-\nular choice of hyper-parameters. To make it worse, the shape of the learning\ncurve can be signiﬁcantly affected when a technique like batchnorm is used. As\na result, hyperparameter optimization of deep networks can be much more chal-\nlenging than traditional machine learning models. In this work, we start from\nwell known Bayesian Optimization solutions and provide enhancement strategies\nspeciﬁcally designed for hyperparameter optimization of deep networks. The re-\nsulting algorithm is named as DEEP-BO (Diversiﬁed, Early-termination-Enabled,\nand Parallel Bayesian Optimization). When evaluated over six DNN benchmarks,\nDEEP-BO easily outperforms or shows comparable performance with some of the\nwell-known solutions including GP-Hedge, Hyperband, BOHB, Median Stopping\nRule, and Learning Curve Extrapolation. The code used is made publicly available\nat https://github.com/snu-adsl/DEEP-BO.\n1\nIntroduction\nHyperparameter optimization(HPO) aims to ﬁnd the global optima x∗of an unknown black-box\nfunction f. That is, x∗= arg maxx∈X f(x), where X is a hyperparameter space that can be a\nsubset of Rd, contain categorical, or discrete variables. f(x) can be evaluated at an arbitrary x ∈X.\nBayesian Optimization(BO) algorithms have been deeply studied and used for HPO in many machine\nlearning(ML) and deep neural network(DNN) problems. Recently, DNN HPO has gained greater\nimportance with increasingly complex tasks that ML techniques cannot fully support.\nWe have observed that it is difﬁcult to choose an appropriate HPO algorithm because the search\nspace depends on the data characteristics and the DNN structure. As shown in Figure 1, we ﬁrst\nevaluated 3 different DNN tasks using various HPO algorithms: random search [Bergstra and Bengio,\n2012], Gaussian Process(GP-EI) [Snoek et al., 2012], an adaptive GP algorithm GP-Hedge [Hoffman\net al., 2011], two speed-up BOs using Median Stopping Rule(GP-EI-MSR) and Learning Curve\nPreprint. Under review.\narXiv:1905.09680v1  [cs.LG]  23 May 2019\nExtrapolation(RF-EI-LCE) [Golovin et al., 2017, Domhan et al., 2015], bandit based approach\nHyperband [Li et al., 2017] and BOHB [Falkner et al., 2018]. No free lunch theorem proves that no\nalgorithm can show superior performance in all cases [Wolpert et al., 1997]. Nevertheless, DEEP-BO\nthat this work introduces shows stable performance in most cases.\nDEEP-BO (Diversiﬁed, Early-termination Enabled, Parallel BO) is an algorithm made more robust\nby strategically integrating 3 methods - BO diversiﬁcation, safe early termination, and cost function\ntransformation. We ﬁrst detail how each method comprising DEEP-BO is developed to enhance\nits respective precedents and then proceed to observe how their combination outperforms existing\nstrategies for DNN HPO. We discuss the individual strengths of the comprising methods and observe\nthat their combined use in the form of DEEP-BO shows both greater performance and robustness\nthan when used alone in enhancing traditional BO for DNN HPO tasks.\n(a) Regression\n(b) Language modeling\n(c) Classiﬁcation\nFigure 1: HPO performance comparison on 3 different DNN tasks (lower is better): (a) MLP model\non kin8nm. (b) LSTM model on PTB, (c) ResNet model on CIFAR-10. Each plot compares min\nfunction value of different HPO algorithms as the optimization time t increases. DEEP-BO is our\nmethod. Note that the shaded areas are 0.25 σ error.\n2\nRelated Works\n2.1\nBayesian Optimization\nBayesian optimization is an iterative process that sequentially chooses the next conﬁguration to\nevaluate based on previous observations. Three Sequential Model-Based Global Optimization\n(SMBO) algorithms, Spearmint [Snoek et al., 2012], SMAC [Hutter et al., 2011], and TPE [Bergstra\net al., 2011], were introduced and are compared in practical problems [Eggensperger et al., 2013].\nAcquisition functions such as Probability of Improvement (PI), Expected Improvement (EI), and\nUpper Conﬁdence Bound (UCB) are widely used with these three BO algorithms.\n2.2\nDiversiﬁcation strategies\nDiversiﬁcation strategy is introduced to minimize the risk of loss by splitting different categories\nof methods in various problems. Diversiﬁcation strategies have been successfully used in many\nﬁelds such as communication [Zheng and Tse, 2003] and ﬁnance [Markowitz, 1952] because of their\ntheoretical and practical advantages. Diversiﬁcation strategies have also been applied to HPO of ML\nalgorithms in various ways. GP-Hedge [Hoffman et al., 2011] uses multiple acquisition functions\ntogether in online multi-armed bandit settings to improve HPO performance. For situations where\nthere is no clear reason to choose a particular acquisition function, using a portfolio of them can be\nbeneﬁcial.\n2.3\nEarly stopping strategies\nSpeed up practices today attempt to reap the beneﬁts of HPO in the form of resource allocation\nstrategies by considering learning dynamics of DNN. One common prior is that learning curves\ntypically increase and saturate. Li et al. [2017] further articulates this using envelope functions1\nbefore applying it to the strategy. Even though asymptotic accuracy predictions vary from a heuristic\n1It is deﬁned as the maximum deviation of the intermediate losses from the terminal validation loss toward\nwhich the deviations are suggested to monotonically decrease with resources allocated.\n2\nrule [Golovin et al., 2017], probabilistic modeling [Domhan et al., 2015], to regression [Baker et al.,\n2017, Istrate et al., 2018], the termination criterion is set upon such an assumption. Some works\ndevelop on these precedents. Klein et al. [2016] uses Bayesian methods to Domhan et al. [2015].\nFalkner et al. [2018] applies BO to Li et al. [2017] to capture the density of good conﬁgurations\nevaluated. Most of these works term their strategies as early stopping strategies.\n2.4\nParallel BO\nBO is a naturally sequential process, but parallelization can also be applied. Some studies address the\nfollowing issues about parallel BO: scalability [Springenberg et al., 2016], batch conﬁguration [Wu\nand Frazier, 2016, Nguyen et al., 2016], and exploration-exploitation tradeoffs [Desautels et al., 2014].\nSnoek et al. [2012] calculates Monte Carlo estimates of the acquisition function, which can be used\nin multiprocessor settings where the number of MCMC sampling can control the diversity of a GP\nmodel. This dynamic is brieﬂy examined in this work. Many other strategies have been introduced\nto increase the diversity of parallel BO algorithms: combining UCB and pure exploration [Contal\net al., 2013], using a penalized acquisition function [González et al., 2016], introducing a randomized\nstrategy to divide and conquer [Wang et al., 2017] and modeling the diversity of a batch [Kathuria\net al., 2016].\n3\nDEEP-BO: An Enhanced Bayesian Algorithm for DNN HPO\nWe introduce three methods intended to enhance BO performance and robustness. Appendix A details\nthe full algorithm with more explanation.\n3.1\nDiversiﬁcation of BO\nWhile a typical BO algorithm uses a single modeling algorithm ˆf for all iterations to approximate\nthe true black-box function f, sequential diversiﬁcation uses N modeling algorithms ( ˆf1, . . . , ˆfN) in\nsequence. This approach rotates over the ordered N algorithms in sequence. When M parallel proces-\nsors (workers) are available, the parallel diversiﬁcation strategy can be used by utilizing N modeling\nalgorithms over M processors. Assuming that the time required for evaluating f(x∗) is much longer\nthan that required for modeling ˆfn using history H, which is often true for heavy DNN problems,\nwe can adopt a simple strategy where we assign the modeling result x∗= arg maxx∈X ˆfn(x) to\nwhichever worker that becomes available. Here, n would be cycled from 1 to N in sequence such\nthat diversiﬁcation can be enforced.\n3.2\nEarly termination rule\nWe introduce an early termination rule, which we name as the Compound Rule (CR), developed\non the Median Stopping Rule (MSR) [Golovin et al., 2017] suggested to be robust. We deﬁne\nEarly Termination Rule (ETR) as a rule that economizes on resources by prematurely terminating\nunpromising conﬁgurations 2. MSR records the mean performance up to some epoch j for each\nconﬁguration tried and uses its median value to evaluate the pending conﬁguration’s best performance\nuntil epoch j. Our ETR has 2 design parameters (max epoch E and β ∈(0.0, 0.5]). β = 0.1 is\nused here with less expected aggressiveness. CR considers curves to be one of three types - highly\nuntrainable, highly desirable, and the rest - and aims to ﬁnd the ﬁrst two with checkpoint-1 and\ncheckpoint-2 respectively. Section 5 and Appendix A and B contain further explanation this ETR.\nBelow is a summary of the two checkpoints.\nCheckpoint-1 discards highly untrainable conﬁgurations. It terminates the training of the current kth\nconﬁguration xk at epoch j1 if its until-then best performance is strictly worse than the β percentile\nvalue of the running averages ¯f1:j1(x) for all x ∈H. The running average performance of xk up\nto epoch l is calculated as ¯f1:l(x) = 1\nl\nPl\ni=1 fi(x) where fi(x) is performance of x at epoch i.\nPoor conﬁgurations consistently produce near random-guess performances and more likely arise\nwhen necessary priors for hyperparameters are unavailable which greatly limits the optimization\n2Note that we avoided the term early stopping because historically it has been used to refer to a speciﬁc\nprocess that prevents overﬁtting from excessive training [Prechelt, 1998].\n3\npotential [Gülçehre and Bengio, 2016]. Checkpoint-1 exclusively aims to terminate them with a low\n(β percentile; MSR uses 50th percentile) threshold.\nCheckpoint-2 spares only highly desirable conﬁgurations that are likely to outperform the current\nbest. xk that survives checkpoint-1 is stopped at epoch j2 if its until-then best performance is below\nthe (1 −β) percentile value of the running averages ¯fj1:j2(x) for x ∈S where S ⊂H for x that\nsurvived checkpoint-1. Envelope functions suggest more resources be needed to discern conﬁguration\nquality since envelope overlaps decrease with resources more clearly. We observe this more evidently\nfor highly desirable conﬁgurations. We thereby apply the intuition exclusively to discern these good\nconﬁgurations with j2 proportional to threshold height. We also note that the observed range for\ncheckpoint-2 starts from j1, not the ﬁrst epoch.\n3.3\nParallel BO\nAs Snoek et al. [2012] points out, the main problem with parallel workers is deciding what x to\nevaluate next while a set of points are still being evaluated. The simplest strategy is to ignore others\nand include only completely evaluated results into H. This can result in duplicate selection where\nthe chosen x∗is already being evaluated by another worker. Snoek et al. [2012] adds randomness\nthrough MCMC sampling to reduce duplicate selection. In the case of parallelization, using N\ndifferent algorithms naturally lowers the chance of duplicates because the algorithms tend to make\ndifferent decisions even for the same H. When M is much larger than N, however, duplicate selection\nbecomes more likely even for diversiﬁcation. When it occurs, the following can be considered.\n• naïve: evaluate the duplicate anyway\n• random: randomly select a new candidate\n• next candidate: choose the next best candidate that has not been selected before\n• in-progress: temporarily add the premature evaluation results to H and ﬁnd x∗\nFor DNN, in-progress is attractive because even when the premature result is entirely wrong, it is self-\ncorrected and replaced in H when evaluation completes. Experiments conducted with N = M = 6\nshows that in-progress provides meaningful improvement while the others perform comparably.\nAnother important topic to consider is cost function transformation. For both improving modeling\nperformance and robustness, we also introduce a hybrid transformation which uses a linear scale\nwhen the error rate is above α and log transformation when the error rate is below α. DNN is often\nused for classiﬁcation tasks with the performance as an error rate. When the desired error rate is\nclose to 0, it becomes difﬁcult to accurately model ˆf(x) using the original linear scale. For such\ncase, applying log transformation, known to be effective for modeling nonlinear and multimodal\nfunctions [Jones et al., 1998], can be advantageous. However, when the error rate is high, this log\ntransformation can be harmful.\n4\nEmpirical Results\nIn this study, we thoroughly evaluated DEEP-BO over 6 DNN HPO problems. As summarized in\nTable 1, we chose the most popular benchmark datasets in the DNN community: MNIST, PTB(Penn\nTreebank), CIFAR-10, and CIFAR-100. Three HPO tasks of CNN architectures (LeNet, VGG,\nTable 1: Benchmark problem settings.\nHPO Benchmark\nNumber of hyperparameters\nLearning\nrate scale\nMax epoch\nNumber of\nconﬁgurations\ndiscrete\ncontinuous\ncategorical\nMNIST-LeNet1\n6\n3\n-\nlog\n15\n20,000\nMNIST-LeNet2\n3\n3\n3\nlog\n15\n20,000\nPTB-LSTM\n3\n5\n1\nlinear\n15\n20,000\nCIFAR10-VGG\n6\n2\n2\nlog\n50\n20,000\nCIFAR10-ResNet\n2\n4\n1\nlog\n100\n7,000\nCIFAR100-VGG\n6\n2\n2\nlog\n50\n20000\n4\nResNet) and one of RNN architecture (LSTM) are included. Since practical model implementation\nalmost always limits the number of hyperparameters human experts can adjust, seven to ten hyperpa-\nrameters are selected for optimization for each benchmark task. Learning rate, often the most critical,\nis included in all six problems. To repeat the experiments effectively over multiple algorithms, we\nﬁrst created lookup tables for each task by pre-evaluating surrogate conﬁgurations sampled by Sobol\nsequences Sobol’ [1967]. The appendix C speciﬁes more details of each benchmark.\nWe use two different metrics to evaluate HPO algorithms. The ﬁrst is the success rate, the probability\nan HPO algorithm achieves a target goal within the given time budget t. Assuming the algorithm\ntakes time τ, a random variable, to achieve a target accuracy c, the algorithm’s success rate at time\nt can be deﬁned as successfully ﬁnding ˆx such that f(ˆx) > c before time t, or more simply as\nP(τ ≤t). The second is expected time to achieve a target accuracy expressed as E[τ].\nFigure 2 shows the success rate plots of DEEP-BO and other algorithms in 3 of 6 benchmark cases.\nUnlike other benchmark algorithms, DEEP-BO consistently shows top-level performance for all 3\nbenchmark cases shown. Appendix D of supplementary materials details plots for all benchmarks.\n(a) MNIST-LeNet1 (M = 1)\n(b) PTB-LSTM (M = 1)\n(c) CIFAR10-ResNet (M = 1)\n(d) MNIST-LeNet1 (M = 6)\n(e) PTB-LSTM (M = 6)\n(f) CIFAR10-ResNet (M = 6)\nFigure 2: Success rate to achieve a target error performance using M processor(s). All experiments\nrepeated the HPO run 100 times for each algorithm.\nEven under the conditions of using a single processor, DEEP-BO is not only robust in all benchmarks,\nbut also has excellent performance. In Figure 2a∼2c, we evaluated our method using M = 1, N = 6,\nα = 0.3, β = 0.1 and compared it with an adaptive algorithm GP-Hedge, two speed up algorithms\n(MSR, LCE) and BOHB, a state-of-the-art algorithm by Falkner et al. [2018].\nThe performance of DEEP-BO under multiprocessing conditions is impressive. Figure 2d∼2f plot\nsuccess rates evaluated in 6 parallel processors(N = 6, M = 6, α = 0.3, β = 0.1). DEEP-BO\nis compared with four different settings of GP-EI algorithms regarding synchronization, MCMC\nsampling, and early termination. In GP-EI algorithms, MCMC sampling explained in Snoek et al.\n[2012] was adopted with either 10 or 1 samples used. The GP-EI-MCMC(10) was the baseline\nalgorithm, and GP-EI-MCMC(1) used only one sample to increase randomness. ‘Synch,GP-EI-\nMCMC(10)’ was a special setting where all the six processors were synchronized for modeling\nsuch that there is no difﬁculty with handling incomplete evaluations. All of six processors started\nsimultaneously and had to wait until the evaluations of all six completed such that the next top 6\ncandidates can be launched together. It performs poorly as expected, but is provided as a benchmark.\nBoth GP-EI-MCMC(1) and GP-EI-MCMC(10) observably performed comparably and the increased\nrandomness did not a prompt distinct improvement. MSR enabled GP-EI tends to increase the\nperformance but, it is not signiﬁcantly improved.\nDEEP-BO notably enhances performance with a single processor. DEEP-BO with 6 processors\ncan enhance more through parallelization. As shown in Figure 2, DEEP-BO outperforms in all\nbenchmarks and seems robust than competitors. In terms of expected time, DEEP-BO performance\n5\nTable 2: Summary of performance achieving top-10 accuracy. For success rate, the numbers in\nparenthesis of each benchmark indicate the checkpoints when the number of processors is 1 and 6,\nrespectively. We only show the competitors’ performance due to the page limit. See Appendix D for\nfull benchmark results.\nMeasure\nBenchmark\nSingle processor (M = 1)\nSix processors (M = 6)\nGP-Hedge\nGP-EI-MSR\nBOHB\nDEEP-BO\nGP-EI\nGP-EI-MSR\nDEEP-BO\nSuccess\nrate\nMNIST-LeNet1\n(12h / 2h)\n32%\n32%\n12%\n81%\n15%\n15%\n80%\nMNIST-LeNet2\n(12h / 2h)\n39%\n48%\n1%\n82%\n39%\n41%\n95%\nPTB-LSTM\n(12h / 2h)\n85%\n89%\n29%\n100%\n45%\n60%\n96%\nCIFAR10-VGG\n(6h / 1h)\n53%\n48%\n0%\n80%\n32%\n50%\n75%\nCIFAR10-ResNet (90h / 15h)\n91%\n94%\n4%\n100%\n61%\n81%\n99%\nCIFAR100-VGG\n(6h / 1h)\n92%\n85%\n4%\n100%\n30%\n43%\n99%\nMean\n66%\n66%\n8%\n91%\n37%\n48%\n91%\nExpected\ntime\n(hour)\nMNIST-LeNet1\n17.0 ± 8.2\n16.3 ± 8.6\n20.7 ± 6.4\n8.2 ± 8.4\n10.4 ± 8.5\n4.0 ± 1.5\n1.4 ± 1.0\nMNIST-LeNet2\n29.3 ± 31.2\n24.2 ± 30.4\n35.8 ± 12.1\n7.6 ± 10.4\n4.6 ± 5.4\n2.4 ± 1.4\n0.8 ± 1.0\nPTB-LSTM\n8.3 ± 3.6\n7.9 ± 2.8\n28.3 ± 22.7\n4.7 ± 2.2\n2.1 ± 0.7\n1.8 ± 0.8\n1.0 ± 0.4\nCIFAR10-VGG\n6.6 ± 4.0\n6.8 ± 4.3\n29.4 ± 1.8\n3.4 ± 2.5\n1.5 ± 0.9\n1.3 ± 0.8\n0.8 ± 0.4\nCIFAR10-ResNet\n50.6 ± 29.6\n43.8 ± 25.4\n129.8 ± 17.8\n38.4 ± 17.3\n13.0 ± 5.5\n11.4 ± 4.1\n7.7 ± 2.5\nCIFAR100-VGG\n4.1 ± 2.1\n3.5 ± 1.5\n24.8 ± 9.9\n1.9 ± 0.9\n1.2 ± 0.5\n1.1 ± 0.4\n0.5 ± 0.2\nMean\n25.7\n17.2\n44.8\n10.7\n5.5\n3.7%\n2.0\nFigure 3: Diversity gain\n(a) Expected time\n(b) Performance w.r.t. linear gain\nFigure 4: Parallelization loss.\nusing 1 processor is 10.7 hours, but that with 6 processors is only 2 hours (5.35 times faster). The\nstate of the art algorithm, BOHB, did not perform well in these benchmarks even though we had tried\nto set its hyperparameters properly. We discuss why in the section 5.\nTable 3: Ablation test results with DEEP-BO using 6 processors. Bold refers to the best performance\nand Italic refers to comparable results.\nHybrid\nTransformation\nEarly\nTermination\nIn-\nprogress\nExpected time (hour)\nMNIST-\nLeNet1\nMNIST-\nLeNet2\nPTB-\nLSTM\nCIFAR10-\nVGG\nCIFAR10-\nResNet\nCIFAR100-\nVGG\non\non\non\n1.4 ± 1.0\n0.8 ± 1.0\n1.0 ± 0.4\n0.8 ± 0.4\n7.7 ± 2.5\n0.5 ± 0.2\non\non\noff\n1.6 ± 1.0\n0.7 ± 0.5\n1.1 ± 0.5\n0.9 ± 0.6\n10.5 ± 3.8\n0.7 ± 0.3\non\noff\non\n1.3 ± 0.7\n1.0 ± 1.5\n1.2 ± 0.5\n0.9 ± 0.6\n8.6 ± 2.9\n0.8 ± 0.3\non\noff\noff\n1.8 ± 1.0\n1.4 ± 1.5\n1.4 ± 0.6\n1.4 ± 0.8\n11.6 ± 4.1\n1.1 ± 0.3\noff\non\non\n2.2 ± 1.7\n1.0 ± 0.8\n1.5 ± 0.7\n0.7 ± 0.5\n8.9 ± 3.3\n0.5 ± 0.2\noff\non\noff\n2.7 ± 2.0\n1.2 ± 1.0\n1.7 ± 0.8\n1.0 ± 0.7\n10.6 ± 4.4\n0.8 ± 0.3\noff\noff\non\n2.4 ± 1.8\n1.5 ± 1.1\n1.8 ± 0.7\n1.1 ± 0.7\n10.5 ± 4.4\n0.8 ± 0.3\noff\noff\noff\n2.4 ± 1.6\n1.8 ± 1.3\n2.1 ± 0.9\n1.2 ± 0.8\n12.7 ± 4.9\n1.1 ± 0.4\nThe ablation study in Table 3 shows that, under all conditions, components applied to DEEP-BO\ndo not always perform well but are generally stable. Table 3 shows (on, on, on) produces the most\nsuccessful performances for 3 of 6 tasks and second best performances at worst. Using this with\nsome components turned off can be a little harmful, but it is shown that all the best cases were where\nat least 2 of the 3 switches were on. Overall, (on, on, on) is a beneﬁcial strategy for all benchmarks.\n5\nDiscussion\nA few of the core design principles of DEEP-BO are explained and discussed in this section.\n6\n5.1\nDiversity gain and cooperation effect\nFor an HPO algorithm, we can deﬁne its failure rate as pf = 1 −P(τ ≤t) where τ is a random\nvariable representing the algorithm’s time to achieve the target accuracy c. If the algorithm is run\nover M parallel workers without sharing their histories {Hm} among the M workers, the chance\nthat all M attempts fail is simply pM\nf\nbecause τ forms an i.i.d. process. Therefore, we can see\nthat M th order diversity gain (increase in the exponent of the failure rate) is achieved at the cost\nof M times increase in resources. In Figure 3, success rate curves are shown for a benchmark\nproblem. RF-EI(M=1) is the baseline with success rate equal to 1 −pf, and Theoretical(M=6) is a\nsynthetic curve calculated from 1 −p6\nf. Clearly, success rate of Theoretical(M=6) ramps up much\nfaster than DEEP-BO(M=1) and demonstrates the power of order six diversity. The exciting part\nis the performance of DEEP-BO(M=6) that is shown together. DEEP-BO(M=6) overwhelmingly\noutperforms Theoretical(M=6), indicating that it can achieve much more than order six diversity\n(1 −p6\nf).\nA large portion of the substantial extra improvement is due to the mix of N = 6 individual models\nwith different modeling characteristics, and the effect comes in two-fold. The ﬁrst is the different\n(ideally independent) characteristics of the N models where at least one might work well. This\nis another type of diversity that is different from the pM\nf effect, and basically says at least one of\npf,1, . . . , pf,N will likely be small over the N individual models for any given task. The second is the\ncooperative nature of the diversiﬁed algorithm thanks to the sharing of {Hn}. When an individual\nmodel A is not capable of selecting good target points for exploration and is stuck in a bad region,\nsome of the other models might make better selections that serve well as exploratory points to the\nmodel A. Then the poorly performing model A can escape from the bad cycle and start to perform well.\nIn our empirical experiments, we extensively tried multi-armed-bandit-like approaches to provide\nmore turns to the better performing individual models, but the strategy actually turned out to hurt the\noverall performance. It turns out that it is better to give a fair chance to even the worst performing\nmodel such that its different modeling characteristic can be utilized. Of course, this is assuming that\neven the worst performing model is generally a plausible and useful model for DNN HPO.\n5.2\nParallelization loss\nWhen the expected time is considered, it would be ideal if the time is reduced by 1/M by using M\nprocessors. We examined if such a linear gain in performance can be retained as M becomes large.\nIn Figure 4a, distributions of the expected time are shown for 3, 6, 12, and 18 processors. Expected\ntime and variance can be seen to decrease as the number of processor increases. As for GP-EI, we\nhave found that MCMC(1) performs better probably because the increased randomness is helpful\nwhen multiple processors are used. In Figure 4b, the expected time performance with respect to ×M\nis shown after normalization. The dotted gray line shows the ×M performance. All the algorithms\nshow the decrease in the normalized performance as M is increased. But the values do not drop very\nfast after M reaches 4∼5 processors. For parallel diversiﬁcation, interestingly the performance even\nwent above linear gain when M is between 2 and 4. This is due to the diversiﬁcation and cooperation.\n5.3\nRobust termination criterion\nEvaluating conﬁgurations very early can much save resources, but is possibly hazardous for BO. CR,\nour ETR, intentionally does not locate j1 in early epochs considering cases illustrated in Figure 5\nfor conﬁgurations with \"batchnorm\" whose termination can prompt false alarms that harm BO.\nCheckpoint-1(j1) here was set as rounded E/2 to lessen this possibility. Similar reasoning goes for\ncheckpoint-2 whose observation start point for the scope of epochs observed is set as epoch j1, not\nthe ﬁrst epoch. With all other conditions ﬁxed for former-half epochs, the inter-dataset µ and σ of\nsurvivor rank regret is strictly counter-proportional to the observation start point for checkpoint-2 of\nour ETR (see Appendix B for details). These correlations strongly indicate that a delayed observation\nstart point may encompass more reliable information (smaller µ) and allow greater robustness (smaller\nσ).\nNotable observations with our ETR are DEEP-BO’s results compared to MSR and BOHB in Figure 2\nand the comparison of ablation results about \"Early Termination\" in Table 3. That DEEP-BO\noutperforms MSR in our benchmarks is not too surprising as our rule was developed on MSR. The\n7\n(a) Initial performance\n(b) Later performance\n(c) Top-10 performing curves\nFigure 5: These are based on the pre-evaluated CIFAR10-VGG surrogate conﬁgurations which\nincluded one of the three: batchnorm, dropout, or none. 5a and 5b show the performance distribution\nthese 3 groups for top-200 curves. Conﬁgurations with batchnorm initially (epoch [0, 0.25E])\nperforms worst but later (epoch [0.75E, E]) performs much better, signifying that early epoch\ninformation may be misleading. This relates to conjectures from Choi et al. [2018]. In fact, all top-10\ncurves (5c) were from conﬁgurations that had batchnorm whose group initially performs worst.\nmore interesting part is that from the perspective of success rates, DEEP-BO outperforms BOHB\neven though both methods conceptually refer to similar parts of the envelope function [Li et al., 2017].\nDEEP-BO abstains from evaluating too early and speciﬁes envelope functions to be more evidently\napplicable to well-trainable conﬁgurations. BOHB evaluates from early epochs which can be\ndangerous for cases like Figure 5 particularly when all observed best conﬁgurations have poor initial\nperformances. Terminating very early does help BO’s explorability and allows the algorithm to\nreach easy goal performances very rapidly. However, this could increase false alarms, hindering the\nalgorithm from performing beyond such easy targets. Success rate is the average of Boolean results\nwhere the iteration is matched with 0 if it does not exceed goal performance. The goal for our study\nis set high and thereby BOHB could have failed drastically.\n(a) Envelope Function Concept\n(b) CIFAR10-VGG’s Envelopes\n(c) CIFAR100-VGG’s Envelopes\nFigure 6:\nEnvelope function expectations [Li et al., 2017] and their actual observations in\nCIFAR10/100-VGG for top-100 curves in total. Asymptotes (terminal performances) are approxi-\nmated by the mean of the latter half performances and aligned at 0.5 for comparison.\nEnvelope function overlappings should be minimal to be applicable. Though this is more evidently\nobserved with high performing conﬁgurations, sporadic ﬂuctuations that can bother evaluation still\nexist as Figure 6 illustrates. Thereby, it would be crucial for the ETR to be less ﬂuctuation-prone.\nThis can be aided by referring to a range of epochs rather than just a single epoch. Our rule and MSR\nare designed in such a way and therefore are in advantage in regards of robustness.\nTable 3 shows both the strength of DEEP-BO’s ETR and the general limit of ETRs. The fact that\nDEEP-BO with CR was best for 5 out of 6 cases heuristically supports its robustness. However, one\ncase revealed that there might be no always good ETR. CR was designed to be more robust than its\nprecedents but still has its limits when used alone. Observations recommend ETRs be used along\nwith other methods that help BO’s robustness as this work attempts.\n6\nConclusion\nIn this work, we proposed three different strategies to enhance Bayesian Optimization for DNN\nHPO with a pursuit of improved average performance and enhanced robustness. We ﬁrst adopted the\n8\nconcept of diversiﬁcation. Individual BO models tend to focus on different parts of search space due\nto the different modeling characteristics. With diversiﬁcation, each algorithm ends up serving as an\nexcellent exploration guide to the other individual algorithms while being a greedy exploiter for its\nown selection. Then, we proposed an early termination strategy that works well by being conservative.\nLearning curves of DNN can show distinct patterns depending on what options (e.g. batchnorm)\nare turned on, and it is very important to avoid being too aggressive. For parallel processing, we\nsuggested the in-progress method that uses premature evaluation results of pending DNN evaluations.\nEven when a premature evaluation is inaccurate and harmful, the problem is self-corrected after\ncompleting the evaluation. Additionally, we introduced a cost function transformation technique\nthat can be helpful. Overall, the resulting DEEP-BO works very well and outperformed the existing\nstate-of-the-art algorithms for the six benchmark DNN HPO tasks that we have investigated.\nReferences\nB. Baker, O. Gupta, R. Raskar, and N. Naik. Accelerating neural architecture search using perfor-\nmance prediction. arXiv preprint arXiv:1705.10823, 2017.\nJ. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. Journal of Machine\nLearning Research, 13(Feb):281–305, 2012.\nJ. S. Bergstra, R. Bardenet, Y. Bengio, and B. Kégl. Algorithms for hyper-parameter optimization. In\nAdvances in neural information processing systems, pages 2546–2554, 2011.\nD. Choi, H. Cho, and W. Rhee. On the difﬁculty of dnn hyperparameter optimization using learning\ncurve prediction. In TENCON 2018-2018 IEEE Region 10 Conference, pages 0651–0656. IEEE,\n2018.\nE. Contal, D. Buffoni, A. Robicquet, and N. Vayatis. Parallel gaussian process optimization with\nupper conﬁdence bound and pure exploration. In Joint European Conference on Machine Learning\nand Knowledge Discovery in Databases, pages 225–240. Springer, 2013.\nT. Desautels, A. Krause, and J. W. Burdick. Parallelizing exploration-exploitation tradeoffs in gaussian\nprocess bandit optimization. The Journal of Machine Learning Research, 15(1):3873–3923, 2014.\nT. Domhan, J. T. Springenberg, and F. Hutter. Speeding up automatic hyperparameter optimization of\ndeep neural networks by extrapolation of learning curves. In Proceedings of the 24th International\nJoint Conference on Artiﬁcial Intelligence (IJCAI), 2015.\nK. Eggensperger, M. Feurer, F. Hutter, J. Bergstra, J. Snoek, H. Hoos, and K. Leyton-Brown. Towards\nan empirical foundation for assessing bayesian optimization of hyperparameters. In NIPS workshop\non Bayesian Optimization in Theory and Practice, pages 1–5, 2013.\nK. Eggensperger, F. Hutter, H. H. Hoos, and K. Leyton-Brown. Efﬁcient benchmarking of hyperpa-\nrameter optimizers via surrogates. In AAAI, pages 1114–1120, 2015.\nS. Falkner, A. Klein, and F. Hutter. Bohb: Robust and efﬁcient hyperparameter optimization at scale.\narXiv preprint arXiv:1807.01774, 2018.\nD. Golovin, B. Solnik, S. Moitra, G. Kochanski, J. Karro, and D. Sculley. Google vizier: A service\nfor black-box optimization. In Proceedings of the 23rd ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining, pages 1487–1495. ACM, 2017.\nJ. González, Z. Dai, P. Hennig, and N. Lawrence. Batch bayesian optimization via local penalization.\nIn Artiﬁcial Intelligence and Statistics, pages 648–657, 2016.\nÇ. Gülçehre and Y. Bengio. Knowledge matters: Importance of prior information for optimization.\nThe Journal of Machine Learning Research, 17(1):226–257, 2016.\nM. D. Hoffman, E. Brochu, and N. de Freitas. Portfolio allocation for bayesian optimization. In UAI,\npages 327–336, 2011.\nF. Hutter, H. H. Hoos, and K. Leyton-Brown. Sequential model-based optimization for general\nalgorithm conﬁguration. In International Conference on Learning and Intelligent Optimization,\npages 507–523. Springer, 2011.\n9\nR. Istrate, F. Scheidegger, G. Mariani, D. Nikolopoulos, C. Bekas, and A. C. I. Malossi. Tapas:\nTrain-less accuracy predictor for architecture search. arXiv preprint arXiv:1806.00250, 2018.\nD. R. Jones, M. Schonlau, and W. J. Welch. Efﬁcient global optimization of expensive black-box\nfunctions. Journal of Global optimization, 13(4):455–492, 1998.\nT. Kathuria, A. Deshpande, and P. Kohli. Batched gaussian process bandit optimization via determi-\nnantal point processes. In Advances in Neural Information Processing Systems, pages 4206–4214,\n2016.\nA. Klein, S. Falkner, J. T. Springenberg, and F. Hutter. Learning curve prediction with bayesian\nneural networks. 2016.\nH. J. Kushner. A new method of locating the maximum point of an arbitrary multipeak curve in the\npresence of noise. Journal of Basic Engineering, 86(1):97–106, 1964.\nL. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar. Hyperband: A novel bandit-\nbased approach to hyperparameter optimization. The Journal of Machine Learning Research, 18\n(1):6765–6816, 2017.\nH. Markowitz. Portfolio selection. The journal of ﬁnance, 7(1):77–91, 1952.\nJ. Mockus, V. Tiesis, and A. Zilinskas. chapter bayesian methods for seeking the extremum. Toward\nglobal optimization, volume 2, 1978.\nV. Nguyen, S. Rana, S. K. Gupta, C. Li, and S. Venkatesh. Budgeted batch bayesian optimization.\nIn Data Mining (ICDM), 2016 IEEE 16th International Conference on, pages 1107–1112. IEEE,\n2016.\nL. Prechelt. Automatic early stopping using cross validation: quantifying the criteria. Neural\nNetworks, 11(4):761–767, 1998.\nA. Shah and Z. Ghahramani. Parallel predictive entropy search for batch global optimization of\nexpensive objective functions. In Advances in Neural Information Processing Systems, pages\n3330–3338, 2015.\nJ. Snoek, H. Larochelle, and R. P. Adams. Practical bayesian optimization of machine learning\nalgorithms. In Advances in neural information processing systems, pages 2951–2959, 2012.\nI. M. Sobol’. On the distribution of points in a cube and the approximate evaluation of integrals.\nZhurnal Vychislitel’noi Matematiki i Matematicheskoi Fiziki, 7(4):784–802, 1967.\nJ. T. Springenberg, A. Klein, S. Falkner, and F. Hutter. Bayesian optimization with robust bayesian\nneural networks. In Advances in Neural Information Processing Systems, pages 4134–4142, 2016.\nN. Srinivas, A. Krause, S. M. Kakade, and M. Seeger. Gaussian process optimization in the bandit\nsetting: No regret and experimental design. arXiv preprint arXiv:0912.3995, 2009.\nZ. Wang, C. Gehring, P. Kohli, and S. Jegelka. Batched large-scale bayesian optimization in high-\ndimensional spaces. arXiv preprint arXiv:1706.01445, 2017.\nD. H. Wolpert, W. G. Macready, et al. No free lunch theorems for optimization. IEEE transactions\non evolutionary computation, 1(1):67–82, 1997.\nJ. Wu and P. Frazier. The parallel knowledge gradient method for batch bayesian optimization. In\nAdvances in Neural Information Processing Systems, pages 3126–3134, 2016.\nL. Zheng and D. N. C. Tse. Diversity and multiplexing: A fundamental tradeoff in multiple-antenna\nchannels. IEEE Transactions on information theory, 49(5):1073–1096, 2003.\n10\nAppendix A.\nAlgorithm Description\nAlgorithm A1. DEEP-BO\nInputs: DNN f, modeling algorithm ˆf1, . . . , ˆfN, processor p1, . . . , pM, hyperparameter space\nX, target accuracy c, cost function transformation g, early termination functions h and l, hybrid\ntransformation parameter α, early termination parameter β, max epoch E.\nH ←∅\n# global shared history\nfor i = 1, 2, . . . do\nn ←\nmod (i, N) + 1\nm ←\nmod (i, M) + 1\nUpdate ˆfn with H on pm\n# preset ˆfn to pm when N == M\nSelect x∗∈arg maxx∈X ˆfn(x) on pm\nA ←∅, y∗←0\nfor j = 1, . . . , E do\n# execute asynchronously\nyj ←fj(x∗) on pm where fj(x) is accuracy for jth epoch\nA ←A ∪(j, yj)\ny∗←max(y∗, yj)\nH ←H∪(x∗, g(y∗, α), A)\n# in-progress history update after loss transformation\nif j ∈l(M, β) then\nif y∗< h(j, A, H, β) then\nbreak\n# early termination\nend if\nend if\nend for\nif y∗> c then\nbreak\n# ﬁnish when desired accuracy achieved\nend if\nend for\nHybrid log transformation\ng(y, α) =\n\u001ay\nif y < 1 −α\n1 −log(1 −y) + (log(α) −α),\notherwise\n(1)\nNote that (log(α) - α) is added to make g(y) a continuous function. In our work, we use α = 0.3.\nWhen f(x) is not an accuracy, its range can be scaled to come within [0,1].\nCompound termination rule\nWe designed an early termination rule and named as the Compound Rule (CR). We ﬁrst introduce\ntwo formulaic tools required for computing function l(E, β) and h(j, A, H, β) which constitute our\nCR. Function FX(x) refers to the cumulative distribution function (CDF) for the distribution of a\ngiven random variable X. The rule’s design constrains the range of β to β ∈(0.0, 0.5].\nFX(x) = P(X ≤x)\n(2)\n¯fi:j(x) = 1\nj\nj\nX\nk=i\nfk(x)\nif i ≤j\n(3)\nNext, we show l which returns the two checkpoints’ epoch locations (epoch j1 and j2) and deﬁne S,\nthat holds the history for conﬁgurations that survived the checkpoint j1 computed by function l.\nl(E, β) = {⌊0.5E⌋, ⌊(1 −β)E⌋} = {j1, j2}\n(4)\nS = {s ∈H | for A ∈s, n(A) > j1}\n(5)\n11\nFigure A1. An illustration of our Compound Rule with 3 sample curves. Epoch j1 and j2 are\ncheckpoints that determine whether a given x should be terminated, and c is target accuracy. Here,\nthey are j1 = ⌊0.5E⌋, j2 = ⌊(1−β)E⌋. x is terminated at epoch j1 if its performance at epoch j1 is\nbelow b1 which is computed referring to all x ∈H. The second checkpoint, epoch j2, applies to those\nthat survived the ﬁrst checkpoint for which the same logic applies but with b2 whose computation\nrefers only the history of those that survived the checkpoint-1.\nUsing the above, we show h which returns one of two thresholds. If j ≤j1, h returns the threshold\nfor epoch j1 (checkpoint-1), and otherwise returns the threshold for epoch j2 (checkpoint-2).\nh(j, A, H, β) =\n\u001aF −1\nX (β)\nif j ≤j1\nF −1\nX (1 −β)\notherwise\nfor,\n(6)\nX =\n\u001a{x | x = ¯f1:j1(x) for x ∈H}\nif j ≤j1\n{x | x = ¯fj1:j2(x) for x ∈S}\notherwise\n(7)\nUnder a hypothetical environment where mapping early terminated conﬁgurations with intermediate\nperformances does not penalize BO’s modeling and per-epoch training time is ﬁxed at\n1\nM , larger β is\nobserved to increase the number of conﬁgurations a BO process expectedly encounters per unit time.\nThis is expressed as q(β) below:\nq(β) = (0.5β + (1 −β)3 + (1 −β)β)−1\n(8)\nq(β) initially increases almost linearly with β but starts to wane away from this linear trajectory\nfrom about β = 0.2. Though this conditioned observation suggests choosing β ∈[0.05, 0.20], such\ncondition would not hold for practical BO processes, making deterministic assumptions difﬁcult. β\nmay be tuned based on practical user constraints.\n12\nAppendix B.\nEarly Termination Rule (ETR)\nStructural components for ETR\nETRs rely on parameters that control several smaller knobs composing such structures. Traditionally,\nworks in ETRs explore the theoretical and empirical robustness of the ETR as a single compound\nbut not these knobs that comprise that compound. Comprehension of these knobs and their inner-\nworkings is likely to further our understanding of early termination and potentially provide intuitions\nfor ETR designs. In this part of our study, we attempt to observe and objectify some common knobs\nthat many ETRs may share.\nWe can annotate that an ETR evaluates works on a learning curve model LΘ where Θ deﬁnes L with\npreset features {θ1, θ2...θk} for k ∈N. Each θk refers to the L’s features which include, but are not\nexcluded to, the maximum budget for training a conﬁguration, whether L is a regression model, refers\nto conﬁguration history for adaptation, or uses extrapolated fantasy values for modeling. LΘ takes\n2 vector inputs: the selected hyperparameter conﬁguration x ∈X and a design parameter vector\nv ∈V to decide whether to terminate x. If T = {0, 1} where 0 means termination and 1 means no\ntermination,\nLΘ : X × V →T\n(9)\nwhose domain is a Cartesian product. X would be forced to be an ordered vector space if some\nθk ∈Θ dictates LΘ to refer to the conﬁguration history.\nNumber of elements in v vary according to the proposed ETRs but are common in that they control\nseveral inner knobs at once. We observe and objectify 4 inner knobs and with them build on this\nequation.\n• Observation Start Point s: This refers to a point from which the learning curve is mean-\ningfully observed to compute the threshold. s is usually deﬁned as a position relative to\nthe maximum trainable epochs or resources allocated to x. Its value is observed to be\nproportional to the maximum epochs and is usually expressed as its percentile value.\n• Observation End Point e: This refers to a point until which the learning curve is mean-\ningfully observed to compute the threshold. e is also observed to be deﬁned proportional\nto the maximum epochs and together with point s determines the scope of learning curve\ninformation that the ETR employs for evaluating x. s by deﬁnition cannot exceed e. Cases\nwhere s = e is when the ETR uses the performance at that epoch alone to evaluate x.\n• Observation Checkpoint j: This refers to a point where the learning curve is ultimately\nevaluated for termination. j is also observed to be deﬁned proportional to the maximum\nepochs. e by deﬁnition cannot exceed j. Many cases set j = e so that the evaluation of x is\nmade right after the end of observation. For our study, we ﬁx j to e.\n• Termination Threshold h: This refers to a knob that controls the threshold aggressiveness\nat the point e with which the conﬁguration is terminated or not. h ∈[0, 1] where h = 0\nessentially equals to no ETR applied and h = 1 indiscriminately terminates all conﬁgurations\nat point e. Given v, h with a vector input of performances deﬁned by s and e determines\nhow the LΘ should evaluate x at point j.\nAn ETR that has a single checkpoint to evaluate x requires one value for each knob. An ETR that has\nn checkpoints would require ordered vectors for each knob at length n so that checkpoint-i(i ≤n)\ncorresponds to si, ei, ji, and hi.\nSurvivor Rank Regret\nSurvivor rank regret is calculated as the mean value of rank regrets for conﬁgurations that are not\nterminated by the ETR. We ﬁrst explain rank regrets and then survivor rank regret.\nRank regret is an adaption of the ‘immediate regret’ [Shah and Ghahramani, 2015] whose original\ndeﬁnition is rt = |f(˜xt)−f(x∗)|, where x∗is the known optimizer of f and ˜xt is the recommendation\nof an algorithm after t batch evaluations. Rank regret uses ranks as the basic unit of measurement\n13\ninstead of f(x) where the ranks are derived by using the pre-evaluated conﬁguration tables generated\nwith the Sobol sequences. Rank regret is used for our experiment instead because our work attempts\nto compare between datasets to check for the algorithm’s robustness. ‘Immediate regret’ is not\nsuitable for such inter-dataset analysis because each dataset has different characteristics and different\ndistributions of achievable performance and therefore the scale of rt values about BO vary greatly\nabout datasets. Amount of change observed in rt for before and after applying some ETR to BO is\nthereby useless for inter-dataset comparison and analysis.\nRank regret, and thereby survivor rank regret, on the other hand is able to deliver information on\nrelative performance improvement. Rank regret ﬁrst lines up the conﬁgurations according to their\nbest performances and ranks the best among those conﬁgurations as the ﬁrst. If we deﬁne the rank of\nx in the table as rank(x), we normalize its difference with rank(x∗) which by deﬁnition is 1 so we\nget Rank regret = rank(x)−rank(x∗)\nn(data table)\nwhere n(data table) refer to the number of conﬁgurations in\nthe table. Our pre-evaluated conﬁgurations for each observed dataset contained 20,000 conﬁgurations,\nso n(data table) for this work would simply equal 20,000.\nSurvivor rank regret here is simply the mean value of rank regret values for a subset of observed\nconﬁgurations that were not terminated at any checkpoint of the ETR and was fully trained to the\nmaximum allocated budget for the conﬁguration. This metric is intended to evaluate whether the\nconﬁgurations that the ETR decided to be worth fully training were, on average, better than those\nchosen without the ETR. Smaller survivor rank regret values would indicate that the surviving pool\nof conﬁgurations are those with high performances and therefore more likely reach some given goal\naccuracy. If they are not as good as the baseline average rank regret of an BO with no ETR, then\nthat ETR would be of no help, or even harming, the BO. As this metric is intended for inter-dataset\ncomparison, small inter-dataset variance values for respective survivor rank regret values would\nsignify that the ETR of concern is more likely robust in regards of ETR’s contribution to BO.\nA Delayed Observation Start Point for CR’s Checkpoint-2\nThis part of the work incorporates the structural framework introduced in this appendix to observe\nexperiment results summarized in Table B1 and suggest the motivations for choosing a delayed\nobservation start point (s2 = j1 = ⌊E/2⌋) for checkpoint-2. We note that the results in this\nexperiment is a full version of CR but closer to MSR particularly intended for CR’s checkpoint-2.\nThis experiment limits the focus to observing the effects of the change in s2 on the survivor rank\nregret which acted as one motivation for the design choice of CR’s checkpoint-2.\nAs shown in Table B1, s2 ∈{0.0, 0.1, 0.2, 0.3, 0.4} and e2 ∈{0.5, 0.7, 0.9} (those corresponding to\ncheckpoint-1 is s1 and e1) together form the scope of epochs observed in producing the 90th percentile\nthreshold at e2 = j2 for checkpoint-2 (subscript 2 indicates that e and j are those corresponding\nto checkpoint-2). Among these 2 knobs, we wish to focus on s2 in particular where for the given\ncolumn, the inter-dataset µ and σ values for survivor rank regret strictly decrease as s2 increases.\nThis work considers this dynamic to be an important marker for using a delayed s2 for checkpoint-2\nin regards of enhancing both performance and robustness. Observation of decreasing µ indicates that\nthis algorithm (with a delayed s2) improves BO’s potential performance for datasets overall and the\nobservation with decreasing σ indicates greater robustness among datasets. Though the dynamic\nTable B1. Each cell shows µ ± σ of the respective median survivor rank regret values for 3 pre-\nevaluated surrogate conﬁgurations(MNIST-LeNet2, PTB-LSTM, and CIFAR10-VGG benchmark)\nfor 50 trials with our ETR on the cell’s setting(row:s2, column:e2). As DEEP-BO uses β = 0.1, this\ntable sets the threshold at the 90th percentile value for comparison.\nBO\nGP-EI\nRF-EI\n50%\n70%\n90%\n50%\n70%\n90%\n0%\n13.2 ± 5.1%\n13.2 ± 5.2%\n12.7 ± 4.2%\n4.8 ± 4.8%\n4.2 ± 4.0%\n3.7 ± 2.9%\n10%\n11.3 ± 3.2%\n11.2 ± 3.4%\n10.6 ± 2.5%\n2.9 ± 2.6%\n3.0 ± 2.3%\n2.9 ± 2.2%\n20%\n9.4 ± 1.5%\n9.6 ± 1.2%\n9.2 ± 1.1%\n1.5 ± 0.9%\n1.9 ± 1.2%\n1.7 ± 1.0%\n30%\n8.0 ± 1.1%\n8.8 ± 0.6%\n8.6 ± 0.8%\n1.1 ± 0.6%\n1.2 ± 0.7%\n1.3 ± 0.7%\n40%\n7.1 ± 0.3%\n7.4 ± 0.5%\n7.9 ± 0.5%\n0.5 ± 0.4%\n0.8 ± 0.3%\n0.9 ± 0.4%\n14\nbetween s2 and µ does contribute to the reasoning for our choice of s2 for checkpoint-2, our work\nputs more emphasis on the dynamic between s2 and σ.\nSmaller σ among datasets is an indicator that the effects of an algorithm on BO, whether good or bad\n(here is the former considering the observations with µ values), is stable and therefore more likely\nto act in such steady manner even for unseen datasets. This observation with decreasing σ can also\nbe intuitively inferred, as exempliﬁed in Figure 4, considering the fact that the overall maximum\ndeviations in performance are smaller (though that amount varies according to the type of curves) for\nlatter epochs. As stated in the paper, occasional disasterous failures are intolerable and thereby the\nquality of some algorithm to perform well and steadily is crucial for practical DNN HPO. Therefore,\nthe early termination rule for our DEEP-BO algorithm chooses to have a more delayed observation\nstart point. As checkpoint-2 considers only the conﬁgurations that survived checkpoint-1 at epoch j1,\nthe observation start point for checkpoint-2 here is set at j1 for convenience.\nETR performance comparison\nEvaluating the ETR performance only, we compare our ETR, which here we name as Compound\nrule(CR), to existing ETRs such as Median Stopping Rule(MSR) [Golovin et al., 2017], Learning\nCurve Extrapolation(LCE) [Domhan et al., 2015]. We select GP-EI as a default BO algorithm which\nwill be accelerated. We evaluated BOHB instead of HB. BOHB is an extended version of HB, which\nworks like an ETR by dynamic resource allocation manner, and also this algorithm already utilizes\nthe other BO. Baker et al. [2017] is excluded in this study because this requires a long warm-up time\nto train the regression model, which is impractical to our benchmark problem.\n(a) MNIST-LeNet1\n(b) CIFAR10-ResNet\nFigure B1. Success rate of ETRs achieving Top-10 accuracy. ETRs are used on GP-EI. MSR stands\nfor Median Stopping Rule by Golovin et al. [2017]. CR stands for our compound termination rule.\nLCE stands for Learning Curve Extrapolation by Domhan et al. [2015].\nAs shown in Figure B1, our CR performs better in MNIST-LeNet1 or similar in CIFAR10-ResNet\nthan MSR. However, LCE and BOHB which performs similarly with CR in MNIST-LeNet1 perform\nworst in CIFAR10-ResNet. Especially, the ETRs which performs worse than random means that they\nfailed severely. We tried best to optimize their termination criteria but, we could not make them robust\nin this benchmark. These model-based algorithms assume that the asymptotic performance of a given\nconﬁguration observing with just partial points in a learning curve can be highly predictable. This\nmay cause their prediction failures and also modeling failure of BO. However, model-free algorithms\nsuch as MSR seem to perform robustly in any benchmark.\n15\nAppendix C.\nExperiment Design\nBenchmark algorithms\nFor the modeling algorithms, we adopted two of the most popular modeling algorithms of BO - GP\nand RF. Speciﬁcally, we have closely followed the implementations of Spearmint for GP and SMAC\nfor RF. For GP, we used the automatic relevance determination(ARD) Matérn 5/2 kernel and a Monte\nCarlo estimation of the integrated acquisition as suggested in Snoek et al. [2012]. For RF, we used\nthe hyperparameter setting where the number of trees and the number of minimum items in a split are\nset to 50, 2, respectively. We also one-hot encoded for categorical hyperparameter values to avoid\nunexpected distortion of modeling.\nFor the acquisition function, we utilized the three basic approaches - EI, PI, and UCB. By considering\nall possible combinations between the modeling algorithms and the acquisition functions, we ended\nup with six different algorithms. For DEEP-BO, we have adopted the six combinations as individual\nalgorithms and implemented the diversiﬁcation strategies. The algorithms that are evaluated are\nsummarized below.\n• The random algorithm [Bergstra and Bengio, 2012]\n• Six individual BO algorithms. Cartesian product by two modeling algorithms [Snoek\net al., 2012, Hutter et al., 2011] and three acquisition functions(Probability of Improvement,\nExpected Improvement, and Upper Conﬁdence Bound) [Kushner, 1964, Mockus et al., 1978,\nSrinivas et al., 2009]: GP-EI, GP-PI, GP-UCB, RF-EI, RF-PI, and RF-UCB\n• GP-Hedge introduced by Hoffman et al. [2011], where GP-EI, GP-PI, GP-UCB are used as\nthe three arms of multi-armed bandit\n• RF-EI using Learning Curve Extrapolation (LCE) [Domhan et al., 2015].\n• GP-EI using Median Stopping Rule (MSR) [Golovin et al., 2017]\n• Hyperband (HB) Li et al. [2017]\n• BOHB which allocates resources as an inﬁnite bandit-based approach but, it samples better\nthan random using TPE [Falkner et al., 2018].\n• Our method DEEP-BO.\nRandom algorithm was included because it is frequently used to replace grid-search, but typically its\nperformance is much worse than the others for DNN tasks. This will become obvious in the result\nplots.\n16\nHyperparameter conﬁguration space\nTable C1: The hyperparameter spaces of LeNet on MNIST dataset.\nArchitecture\nName\nType\nRange\nLeNet1\nLeNet2\nLeNet-5\nnumber of ﬁrst convolution\nkernels\ndiscrete\n[1, 350]\n[1, 350]\nﬁrst pooling size\ndiscrete\n[2, 3]\n-\nnumber of second convolution\nkernels\ndiscrete\n[1, 350]\n[1, 350]\nsecond pooling size\ndiscrete\n[2, 3]\n-\nnumber of neurons in fully\nconnected layer\ndiscrete\n[1, 1024]\n[1, 1024]\nsquare length of the convolution\nkernel\ndiscrete\n[2, 10]\n-\nlearning rate\ncontinuous\n[0.0001, 0.4]\n(log scale)\n[0.0001, 0.4]\n(log scale)\nL2 regularization factor\ncontinuous\n[0.0, 1.0]\n[0.0, 1.0]\ndrop out rate\ncontinuous\n[0.0, 0.9]\n[0.0, 1.0]\nactivation function type\ncategorical\n-\nReLU, tanh,sigmoid,\neLU, leaky ReLU\noptimizer type\ncategorical\n-\nAdaDelta, AdaGrad, Adam,\nGD, Momentum, RMSProp\nbatch normalization\ncategorical\n-\nEnable, Disable\nTable C2: The hyperparameter spaces of VGGNet and ResNet on CIFAR10.\nArchitecture\nName\nType\nRange\nVGG\nnumber of ﬁrst convolution kernels\ndiscrete\n[8, 32]\nnumber of second convolution kernels\ndiscrete\n[32, 64]\nnumber of third convolution kernels\ndiscrete\n[64, 128]\nnumber of forth convolution kernels\ndiscrete\n[64, 128]\nnumber of neurons in fully connected layer\ndiscrete\n[10, 1000]\nsquare length of convolution kernel\ndiscrete\n[2, 3]\nlearning rate\ncontinuous\n[0.0001, 0.4]\n(log scale)\nL2 regularization factor\ncontinuous\n[0.0, 1.0]\nactivation function type\ncategorical\nReLU, tanh, eLU\nregularization method\ncategorical\nNone, Dropout, BatchNorm\nResNet\nnumber of layers\ndiscrete\n38, 44, 50, 56, 62, 70, 78\ntraining data batch size\ndiscrete\n45, 90, 180, 360, 450\ndata augmentation\ncategorical\nTrue, False\nlearning rate\ncontinuous\n[0.0001, 0.1]\n(log scale)\nmomentum for MomentumOptimizer\ncontinuous\n[0.1, 0.9]\nweight decay\ncontinuous\n[0.00001, 0.001]\n(log scale)\nbatch normalization decay\ncontinuous\n[0.9, 0.999]\n17\nTable C3: The hyperparameter space of LSTM on PTB dataset.\nArchitecture\nName\nType\nRange\nLSTM\nnumber of neurons in hidden layer\ndiscrete\n[10, 200]\nnumber of hidden layers\ndiscrete\n[1, 2]\nnumber of training steps\ndiscrete\n[10, 20]\ninitial value of uniform random scale\ncontinuous\n[0.01, 0.1]\ndrop out rate\ncontinuous\n[0.0, 0.9]\nlearning rate\ncontinuous\n[0.1, 1.0]\nlearning rate decay\ncontinuous\n[0.5, 1.0]\ngradient clipping by global normalization\ncontinuous\n[5.0, 10.0]\nRNN training module\ncategorical\nBasic, Block, cuDNN\nTable C4: The hyperparameter space of VGGNet on CIFAR100 dataset.\nArchitecture\nName\nType\nRange\nVGG\nnumber of ﬁrst convolution kernels\ndiscrete\n[8, 32]\nnumber of second convolution kernels\ndiscrete\n[32, 64]\nnumber of third convolution kernels\ndiscrete\n[64, 128]\nnumber of forth convolution kernels\ndiscrete\n[64, 128]\nnumber of neurons in fully connected layer\ndiscrete\n[10, 1000]\nsquare length of convolution kernel\ndiscrete\n[2, 3]\nlearning rate\ncontinuous\n[0.0001, 0.4]\n(log scale)\nL2 regularization factor\ncontinuous\n[0.0, 1.0]\nactivation function type\ncategorical\nReLU, tanh,sigmoid, eLU\nregularization method\ncategorical\nNone, Dropout, BatchNorm\nSurrogate conﬁgurations\nThe performance of an HPO algorithm for a speciﬁc task is not deterministic. For example, GP-EI\nwould output a different result for each trial of hyperparameter optimization. Therefore, repeated\nexperiments are necessary to estimate the ‘success rate’ and ‘expected time’ to success in a credible\nway. However, each trial of a single HPO run requires a considerable number of DNN trainings\n(typically a few hundreds of DNN trainings), and thus it would be extremely time-consuming to\nrepeat the experiments over multiple algorithms. In this regard, Eggensperger, Hutter, Hoos, and\nLeyton-Brown [2015] suggest that surrogate benchmarks are cheaper to evaluate and closely follow\nreal-world benchmarks.\nTable C5: Statistics of pre-evaluated conﬁgurations.\nDataset\nMNIST\nPTB\nCIFAR10\nCIFAR100\nArchitecture\nLeNet1\nLeNet2\nLSTM\nVGG\nResNet\nVGG\nTotal evaluation time (day)\n88.1\n55.5\n87.3\n75.2\n385\n82.1\nMean evaluation time (min)\n6.3\n4\n6.3\n5.4\n79\n5.9\nGlobal optimum (top accuracy)\n0.9939\n0.9941\n0.9008\n0.8052\n0.9368\n0.5067\nTarget performance (top-10 accuracy)\n0.9933\n0.9922\n0.8993\n0.7836\n0.9333\n0.4596\nTo utilize surrogate benchmarks, we pre-selected a representative set of model conﬁgurations, pre-\nevaluated all the selected conﬁgurations, and saved their DNN training results (including the evaluated\ntest performance and the consumed time) into a database. This process takes some time to complete\nbecause we need to complete DNN trainings of all the conﬁgurations. In our case, we had 20,000\nconﬁgurations for ﬁve benchmarks and 7,000 for the other, and thus a total of 107,000 conﬁgurations\nworth of DNN training had to be completed for the six benchmarks. However, once completed, we\n18\nwere able to repeatedly perform HPO experiments using the database without requiring to resort to\nactual DNN evaluations every time they were necessary. With the pre-generated database, proper\nbook-keeping can be performed to expedite the experiments. Using this methodology, we were able\nto evaluate each HPO algorithm 100 times for each benchmark task.\nIn order to select ‘a representative set of conﬁgurations’, we used Sobol sequencing method to\nchoose the conﬁgurations. Sobol is a quasi-random low-discrepancy sequence in a unit hypercube,\nand its samples are known to be more evenly distributed in a given space than those from uniform\nrandom sampling. We set an appropriate range for each hyperparameter. Here, each range can be\na real interval or a bounded set of discrete/categorical values. Then, if we were to say that we are\nconcern with d of hyperparameters, we cast this truncated hyperparameter space into a d-dimensional\nunit hypercube and generate a Sobol sequence within the hypercube. Finally, we train DNN for all\nconﬁgurations sampled by Sobol grids and save the necessary information such as respective training\ntime and prediction accuracy. We collected these training results for six benchmarks on machines\nwith NVIDIA 1080 TI GPU.\nHyperparameter conﬁguration of BOHB\nHyperparameter conﬁgurations for the MNIST-LeNet1 benchmark were applied with a minimum\nbudget 1 and maximum budget 81. Other hyperparameters used for BOHB in our experiments are as\nfollows; the scaling parameter η is 3, the number of samples used to optimize the acquisition function\nis 64, the fraction of purely random conﬁguration ρ is 0.33, the minimum budget is 1, the maximum\nbudget is 15.\nHyperparameters, except the minimum and maximum budgets, followed the parameters used for\nthe MNIST dataset in Falkner et al. [2018]. We were able to observe comparable results with those\nproduced in the parameter settings provided by Falkner et al. [2018] when we experimented with the\nalgorithm in our environmental settings by changing its hyperparameters for the given minimum and\nmaximum budgets.\nFor the PTB-LSTM benchmark, we also used the hyperparameter conﬁguration with minimum budget\n1, maximum budget 15 and the scaling parameter η as 3. For the CIFAR10-ResNet benchmark, the\nhyperparameters were applied with a maximum budget of 100 and the scaling parameter η as 3. Other\nhyperpaprameters are the same as above.\n19\nAppendix D.\nBenchmark Performance\nUsing single processor\nTable B3 summarizes the performances of six individual algorithms and the diversiﬁcation algorithms\nfor the six benchmarks.\nTwo types of performance metrics, success rate and expected time, were evaluated and those that\nperformed best were shown in bold. For both metrics, the target performance c was set as the lower\nbound of top-10 best performances found among the pre-evaluated results. All algorithms run until\nthe best performance currently outperform the top-10.\nAs for the success rate using a single processor, we list the success rate after 12 hours for MNIST-\nLeNet, and PTB-LSTM, because most algorithms’ average time for success is less than 12 hours. For\na similar reason, the success rate after 6 hours is considered for CIFAR10-VGG and CIFAR100-VGG.\nIn the case of CIFAR10-ResNet, we list the success rate after 90 hours. Note that the success rate\nlisted in Table D1 are cross-sectional snapshots of Figure 2 that show the time series of success\nrates. On the mean performance of all benchmarks, the success rate of DEEP-BO was 12% superior\nto RF-PI, which was the best among the other algorithms. DEEP-BO achieved 31% of average\nimprovements over the early-termination-disabled individual algorithms. DEEP-BO is also 25%\nbetter than GP-EI-MSR or GP-Hedge when compared to the other advanced algorithms. Compared\nto random, the baseline algorithm, it’s improved by 85%.\nRegarding expected time, DEEP-BO achieved 60% of time reductions over the early-termination-\ndisabled individual algorithms. DEEP-BO decreased 6.5 hours compared to the second fastest\nGP-EI-MSR.\nTable D1. Summary of single processor performance achieving top-10 accuracy.\nMeasure\nBenckmark\nGP-EI\nGP-PI\nGP-UCB\nRF-EI\nRF-PI\nRF-UCB\nSuccess\nrate\nMNIST-LeNet1\n(12h)\n34%\n34%\n29%\n48%\n61%\n41%\nMNIST-LeNet2\n(12h)\n49%\n29%\n48%\n63%\n57%\n65%\nPTB-LSTM\n(12h)\n89%\n83%\n86%\n68%\n92%\n89%\nCIFAR10-VGG\n(6h)\n50%\n43%\n46%\n77%\n77%\n75%\nCIFAR10-ResNet (90h)\n91%\n86%\n92%\n69%\n89%\n86%\nCIFAR100-VGG\n(6h)\n87%\n81%\n79%\n96%\n98%\n97%\nMean\n67%\n59%\n63%\n70%\n79%\n76%\nExpected\ntime\n(hour)\nMNIST-LeNet1\n53.1 ± 74.4\n56.9 ± 81.5\n67.5 ± 84.4\n43.2 ± 59.7\n27.8 ± 65.2\n36 ± 51.7\nMNIST-LeNet2\n29.6 ± 46.7\n77.9 ± 76.6\n25.3 ± 27.5\n16.5 ± 23.8\n45.4 ± 78.5\n20.4 ± 34.3\nPTB-LSTM\n8.2 ± 3.4\n7.9 ± 4.6\n8.6 ± 3.4\n25.8 ± 61.4\n6.4 ± 3.4\n7.4 ± 3.5\nCIFAR10-VGG\n7.4 ± 4.3\n7.2 ± 3.5\n8.1 ± 4.7\n4.7 ± 3.2\n4.4 ± 3.8\n4.4 ± 3.5\nCIFAR10-ResNet\n44.1 ± 28.9\n57.2 ± 32.4\n47.5 ± 24.6\n78.5 ± 59.6\n53.5 ± 35.2\n55.7 ± 30.4\nCIFAR100-VGG\n4.1 ± 1.6\n4.1 ± 2.1\n6.7 ± 11.7\n3.5 ± 6.3\n3.2 ± 7.2\n3.1 ± 5.5\nMean\n24.4\n35.2\n27.3\n28.7\n23.4\n21.2\nMeasure\nBenckmark\nRandom\nGP-Hedge\nGP-EI-MSR\nRF-EI-LCE\nBOHB\nDEEP-BO\nSuccess\nrate\nMNIST-LeNet1\n(12h)\n7%\n32%\n32%\n36%\n12%\n81%\nMNIST-LeNet2\n(12h)\n12%\n39%\n48%\n54%\n1%\n82%\nPTB-LSTM\n(12h)\n3%\n85%\n89%\n41%\n29%\n100%\nCIFAR10-VGG\n(6h)\n5%\n53%\n48%\n78%\n0%\n80%\nCIFAR10-ResNet (90h)\n7%\n91%\n94%\n0%\n4%\n100%\nCIFAR100-VGG\n(6h)\n2%\n92%\n85%\n-%\n4%\n100%\nMean\n6%\n66%\n66%\n42%\n8%\n91%\nExpected\ntime\n(hour)\nMNIST-LeNet1\n169.1 ± 142.6\n17.0 ± 8.2\n16.3 ± 8.6\n16.2 ± 8.3\n20.7 ± 6.4\n8.2 ± 8.4\nMNIST-LeNet2\n99.9 ± 86.1\n29.3 ± 31.2\n24.2 ± 30.4\n41.4 ± 66.6\n35.8 ± 12.1\n7.6 ± 10.4\nPTB-LSTM\n188.3 ± 169.2\n8.3 ± 3.6\n7.9 ± 2.8\n69.2 ± 104.2\n28.3 ± 22.7\n4.8 ± 2.2\nCIFAR10-VGG\n138.2 ± 137.6\n6.6 ± 4.0\n6.8 ± 4.3\n4.1 ± 2.9\n29.4 ± 1.8\n3.4 ± 2.5\nCIFAR10-ResNet\n756.1 ± 724.8\n50.6 ± 29.6\n43.8 ± 25.4\n120.5 ± 0.3\n129.8 ± 17.8\n38.4 ± 17.3\nCIFAR100-VGG\n195.8 ± 155.0\n4.1 ± 2.1\n4.2 ± 3.0\n−± −\n24.8 ± 9.9\n1.9 ± 0.9\nMean\n259.9\n25.7\n17.2\n50.3\n44.8\n10.7\n20\n(a) MNIST-LeNet1\n(b) MNIST-LeNet2\n(c) PTB\n(d) CIFAR10-VGG\n(e) CIFAR10-ResNet\n(f) CIFAR100-VGG\nFigure D1. Success rate to achieve over the top-10 error performance using a single processor(higher\nis better).\n21\n(a) ~1h\n(b) 1~12h\nFigure D2-1. HPO performance comparison of MNIST-LeNet1 benchmark(lower is better). Note\nthat the shaded areas are 0.25 σ error bars.\n(a) ~1h\n(b) 1~9h\nFigure D2-2. HPO performance of MNIST-LeNet2 benchmark.\n(a) ~1h\n(b) 1~12h\nFigure D2-3. HPO performance of PTB-LSTM benchmark.\n22\n(a) ~1h\n(b) 1~12h\nFigure D2-4. HPO performance of CIFAR10-VGG benchmark.\n(a) ~12h\n(b) 12~78h\nFigure D2-5. HPO performance of CIFAR10-ResNet benchmark.\n(a) ~1h\n(b) 1~12h\nFigure D2-6. HPO performance of CIFAR100-VGG benchmark.\nUsing 6 parallel processors\nAs shown in Table D3, our algorithm can complete the tasks in merely 2 hours on average using 6\nprocessors. On the mean performance of all benchmarks, the success rate of DEEP-BO was 34%\nsuperior compared to RF-PI and the expected time decreased by 2.1 hours. Regarding success rate,\nDEEP-BO achieved 117%, 88% of average improvements over the individual BO algorithms and\nETR-applied algorithm, respectively. Regarding expected time, DEEP-BO achieved 60%, 44% of\ntime reductions over the individual algorithms or the ETR-applied algorithm, respectively.\n23\nTable D3. Summary of the 6 processors performance achieving top-10 accuracy.\nMeasure\nBenckmark\nGP-EI\nGP-PI\nGP-UCB\nRF-EI\nRF-PI\nRF-UCB\nGP-EI-MSR\nDEEP-BO\nSuccess\nrate\nMNIST-LeNet1\n(2h)\n15%\n9%\n13%\n32%\n58%\n30%\n15%\n80%\nMNIST-LeNet2\n(2h)\n39%\n17%\n38%\n44%\n56%\n46%\n41%\n95%\nPTB-LSTM\n(2h)\n45%\n68%\n52%\n24%\n63%\n35%\n60%\n95%\nCIFAR10-VGG\n(1h)\n32%\n43%\n39%\n40%\n48%\n32%\n50%\n75%\nCIFAR10-ResNet (15h)\n61%\n59%\n70%\n37%\n58%\n55%\n81%\n99%\nCIFAR100-VGG\n(1h)\n30%\n25%\n37%\n51%\n59%\n47%\n43%\n99%\nMean\n37%\n37%\n42%\n38%\n57%\n41%\n48%\n91%\nExpected\ntime\n(hour)\nMNIST-LeNet1\n10.4 ± 8.5\n11.9 ± 8.9\n8.9 ± 7.8\n4.7 ± 5.2\n2.4 ± 2.0\n4.6 ± 4.3\n4.0 ± 1.5\n1.4 ± 1.0\nMNIST-LeNet2\n4.6 ± 5.4\n8.4 ± 7.3\n3.8 ± 3.8\n3.0 ± 2.8\n3.3 ± 4.5\n2.4 ± 1.9\n2.4 ± 1.4\n0.8 ± 1.0\nPTB-LSTM\n2.1 ± 0.7\n1.7 ± 0.8\n2.0 ± 0.7\n2.5 ± 0.7\n1.8 ± 0.9\n2.3 ± 0.8\n1.8 ± 0.8\n1.0 ± 0.4\nCIFAR10-VGG\n1.5 ± 0.9\n1.5 ± 1.1\n1.5 ± 1.0\n1.5 ± 1.0\n1.4 ± 1.0\n1.8 ± 1.1\n1.3 ± 0.8\n0.8 ± 0.4\nCIFAR10-ResNet\n13.0 ± 5.5\n14.1 ± 6.6\n12.3 ± 5.7\n18.1 ± 7.3\n14.4 ± 6.6\n14.6 ± 6.6\n11.4 ± 4.1\n7.7 ± 2.5\nCIFAR100-VGG\n1.2 ± 0.5\n1.4 ± 0.5\n1.2 ± 0.5\n1.1 ± 0.4\n1.1 ± 1.0\n1.0 ± 0.4\n1.1 ± 0.4\n0.5 ± 0.2\nMean\n5.5\n6.5\n5.0\n5.2\n4.1\n4.4\n3.7\n2.0\n(a) MNIST-LeNet1\n(b) MNIST-LeNet2\n(c) PTB-LSTM\n(d) CIFAR10-VGG\n(e) CIFAR10-ResNet\n(f) CIFAR100-VGG\nFigure D3. Success rate to achieve over the top-10 error performance when using six processors.\n24\nExtra comparisons\nFor fair comparison with HB, we have run the regression sample provided by a HB implemen-\ntation. See https://github.com/zygmuntz/hyperband/blob/master/defs_regression/\nkeras_mlp.py\nFigure D4. HPO of Regression model on kin8nm dataset.\nFor fair comparison with BOHB, we have run the sample code provided by BOHB’s au-\nthor.\nThis example aims to demonstrate how to use the BOHB to tune the hyperparam-\neter of the classiﬁcation model learned with MNIST data.\nIts hyperparameter space has\n9-dimensional space which consists of 5 discrete, 3 continuous, and 1 categorical variable\nrespectively.\nSee https://github.com/automl/HpBandSter/blob/master/hpbandster/\nexamples/example_5_keras_worker.py for details.\n(a) Easy target (5% error)\n(b) Hard target (1% error)\n(c) Best error performance\nFigure D5. Performance comparison on a benchmark problem provided by BOHB.\nAs shown in Figure D5, our DEEP-BO outperforms BOHB and TPE. However, BOHB could not\nachieve any better result compared with TPE. This experiment was repeated 30 times to get a\nstatistically signiﬁcant result(F-statistics is 13.18, p-value is 1e-05). We will discuss later why BOHB\nfailed catastrophically in DNN problem.\nPerformance on the number of hyperparameters\nEggensperger et al. [2013] summarize the performance characteristics of 3 HPO algorithms(Spearmint,\nSMAC, TPE) which are used widely. Spearmint which models using GP tends to be better in the low-\ndimensional continuous problem. Instead, SMAC and TPE tend to be better in the high-dimensional\nproblem. Afterall, Figure C6 shows that our diversiﬁcation strategy always performs well at any\ndimensional problem.\n25\n(a) 4 continuous parameters\n(b) 17 discrete, continuous, categorical parameters\nFigure D6. Performance comparisons when optimizing the different dimensional problem. We\ncreated this problem to evaluate BO algorithms only for the dimension size issue by decreasing or\nincreasing the number of hyperparameters of CIFAR10-VGG benchmark problem.\nFigure D7. Performance comparisons when the number of max epochs is increased. We evaluated our\nalgorithm on the sample code provided by BOHB’s author. Our DEEP-BO always perform better than\nBOHB. In the case of this problem, the asymptotic performance tends to achieved when 6 hours later. Too\nsmall or too much max epoch tends to be ineffective to achieve the target goal in a given budget.\nPerformance by max epochs setting\nIn DNN HPO problems, setting proper max epochs of training is very important to ﬁnd their optimal\nperformance. Because of no way of predicting the optimal number of epochs before evaluated, this\nremains an art of human experts. If we have a lack of prior knowledge about the problem to optimize,\ntoo few or too much of max epochs tends to bring unexpected results even after spending much\nexpense. Therefore, escalating the target performance is required by considering the budget.\n26\n",
  "categories": [
    "cs.LG",
    "cs.DC",
    "stat.ML"
  ],
  "published": "2019-05-23",
  "updated": "2019-05-23"
}