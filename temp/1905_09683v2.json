{
  "id": "http://arxiv.org/abs/1905.09683v2",
  "title": "From semantics to execution: Integrating action planning with reinforcement learning for robotic causal problem-solving",
  "authors": [
    "Manfred Eppe",
    "Phuong D. H. Nguyen",
    "Stefan Wermter"
  ],
  "abstract": "Reinforcement learning is an appropriate and successful method to robustly\nperform low-level robot control under noisy conditions. Symbolic action\nplanning is useful to resolve causal dependencies and to break a causally\ncomplex problem down into a sequence of simpler high-level actions. A problem\nwith the integration of both approaches is that action planning is based on\ndiscrete high-level action- and state spaces, whereas reinforcement learning is\nusually driven by a continuous reward function. However, recent advances in\nreinforcement learning, specifically, universal value function approximators\nand hindsight experience replay, have focused on goal-independent methods based\non sparse rewards. In this article, we build on these novel methods to\nfacilitate the integration of action planning with reinforcement learning by\nexploiting the reward-sparsity as a bridge between the high-level and low-level\nstate- and control spaces. As a result, we demonstrate that the integrated\nneuro-symbolic method is able to solve object manipulation problems that\ninvolve tool use and non-trivial causal dependencies under noisy conditions,\nexploiting both data and knowledge.",
  "text": "Preprint submitted to Frontiers in\nRobotics and AI\nFrom semantics to execution: Integrating\naction planning with reinforcement learning\nfor robotic causal problem-solving\nManfred Eppe 1,∗, Phuong D.H. Nguyen 1, and Stefan Wermter 1\n1Knowledge Technology Institute, Department of Informatics, Universit¨at Hamburg,\nGermany\nCorrespondence*:\nManfred Eppe\neppe@informatik.uni-hamburg.de\nABSTRACT\nReinforcement learning is generally accepted to be an appropriate and successful method to\nlearn robot control. Symbolic action planning is useful to resolve causal dependencies and to\nbreak a causally complex problem down into a sequence of simpler high-level actions. A problem\nwith the integration of both approaches is that action planning is based on discrete high-level\naction- and state spaces, whereas reinforcement learning is usually driven by a continuous\nreward function. Recent advances in model-free reinforcement learning, speciﬁcally, universal\nvalue function approximators and hindsight experience replay, have focused on goal-independent\nmethods based on sparse rewards that are only given at the end of a rollout, and only if the goal\nhas been fully achieved. In this article, we build on these novel methods to facilitate the integration\nof action planning with model-free reinforcement learning. Speciﬁcally, the paper demonstrates\nhow the reward-sparsity can serve as a bridge between the high-level and low-level state- and\naction spaces. As a result, we demonstrate that the integrated method is able to solve robotic\ntasks that involve non-trivial causal dependencies under noisy conditions, exploiting both data\nand knowledge.\nKeywords: reinforcement learning, hierarchical architecture, planning, robotics, neural networks, causal puzzles\n1\nINTRODUCTION\nHow can one realize robots that reason about complex physical object manipulation problems, and how can\nwe integrate this reasoning with the noisy sensorimotor machinery that executes the required actions in a\ncontinuous low-level action space? To address these research questions, we consider reinforcement learning\n(RL) as it is a successful method to facilitate low-level robot control (Deisenroth and Rasmussen, 2011).\nIt is well known that non-hierarchical reinforcement-learning architectures fail in situations involving\nnon-trivial causal dependencies that require the reasoning over an extended time horizon (Mnih et al.,\n2015). For example, the robot in Figure 1 (right) needs to ﬁrst grasp the rake before it can be used to drag\nthe block to a target location. Such a problem is hard to solve by RL-based low-level motion planning\nwithout any high-level method that subdivides the problem into smaller sub-tasks.\nTo this end, recent research has developed hierarchical and model-based reinforcement learning methods\nto tackle problems that require reasoning over a long time horizon, as demanded in domains like robotic tool\nuse, block-stacking (Deisenroth and Rasmussen, 2011), and computer games (Aytar et al., 2018; Pohlen\n1\narXiv:1905.09683v2  [cs.LG]  8 Dec 2019\nEppe et al.\nFrom semantics to execution\net al., 2018). Yet, the problem of realizing an agent that learns to solve open-domain continuous space\ncausal puzzles from scratch, without learning from demonstration or other data sources, remains unsolved.\nThe existing learning-based approaches are either very constrained (e.g., (Deisenroth and Rasmussen,\n2011)), or they have been applied only to low-dimensional non-noisy control problems that do not involve\ncomplex causal dependencies (e.g., (Bacon et al., 2017; Levy et al., 2019)), or they build on learning from\ndemonstration (Aytar et al., 2018).\nA complementary method to address complex causal dependencies is to use pre-speciﬁed semantic\ndomain knowledge, e.g., in the form of an action planning domain description (McDermott et al., 1998).\nWith an appropriate problem description, a planner can provide a sequence of solvable sub-tasks in a\ndiscrete high-level action space. However, the problem with this semantic and symbolic task planning\napproach is that the high-level actions generated by the planner require the grounding in a low-level motion\nexecution layer to consider the context of the current low-level state. For example, executing a high-level\nrobotic action move object to target requires precise information (e.g., the location and the shape)\nof the object to move it to the target location along a context-speciﬁc path. This grounding problem consists\nof two sub-problems P.1 and P.2 that we tackle in this article.\nP.1\nThe ﬁrst grounding sub-problem is to map the discrete symbolic action space to context-speciﬁc\nsubgoals in the continuous state space. For instance, move object to target needs to be\nassociated with a continuous sub-goal that speciﬁes the desired metric target coordinates of the\nobject.\nP.2\nThe second grounding sub-problem is to map the subgoals to low-level context-speciﬁc action\ntrajectories. For instance, the low-level trajectory for move object to target is speciﬁc to the\ncontinuous start- and target location of the object, and to potential obstacles between start and target.\nBoth problems are currently being recognized by the state of the art in combined task and motion planning\n(e.g., Toussaint et al. (2018)), and, from a broader perspective, also in the ﬁeld of state representation\nlearning (e.g., Lesort et al. (2018); Doncieux et al. (2018)). However, to the best of our knowledge,\nthere exist currently no satisfying and scalable solutions to these problems that have been demonstrated\nin the robotic application domain with continuous state- and action representations (see Section 2.4).In\nthis research, we address P.1 by providing a simple, yet principled, formalism to map the propositional\nhigh-level state space to continuous subgoals (Section 3.1). We address P.2 by integrating this formalism\nwith goal-independent reinforcement learning based on sparse rewards (Section 3.2).\nExisting approaches that integrate action planning with reinforcement learning have not been able to\nmap subgoals to low-level motion trajectories for realistic continuous-space robotic applications (Grounds\nand Kudenko, 2005; Ma and Cameron, 2009) because they rely on a continuous dense reward signal that\nis proportional to manually deﬁned metrics that estimate how well a problem has been solved (Ng et al.,\n1999). The manual deﬁnition of such metrics, also known as reward shaping, is a non-trivial problem itself\nbecause the semantic distance to a continuous goal is often not proportional to the metric distance.\nRecently, so-called universal value function approximators (UVFAs) (Schaul et al., 2015) in combination\nwith hindsight experience replay (HER) (Andrychowicz et al., 2017) and neural actor-critic reinforcement\nlearning methods (Lillicrap et al., 2016) have been proposed to alleviate this issue. HER realizes an efﬁcient\noff-policy algorithm that allows for non-continuous sparse rewards without relying on reward shaping.\nSpeciﬁcally, HER treats action trajectories as successful that do not achieve the desired speciﬁed goal,\nby pretending in hindsight that the achieved state was the desired goal state. Our research builds on this\nThis is a provisional ﬁle, not the ﬁnal typeset article\n2\nEppe et al.\nFrom semantics to execution\nFigure 1. A robot performing two object manipulation tasks. 1. Block-stacking (left): The gripper must\nstack three blocks at a random location within the robot’s range on the table (indicated by the transparent\ngoal markers behind the gripper). Herein, the robot needs to subdivide the course of actions into separate\nactions for grasping and placing the individual blocks. 2. Tool use (right): The red block is out of the\ngripper’s range (indicated by the dark brown ellipsoid), so that solving the task of moving the block to\na target location requires the robot to break the problem down into a sequence of high-level actions that\ninvolve grasping the rake, moving the rake towards the block and pulling the rake.\nmethod because the sparse subgoal-speciﬁc rewards allow us to decouple the reward mechanism from the\nhigh-level action planning.\nThis approach enables us to address the following central hypotheses:\nH.1\nWe hypothesize that model-free reinforcement learning with universal value function approximators\n(UVFAs) and hindsight experience replay (HER) is appropriate to learn the grounding of a\ndiscrete symbolic action space in continuous action trajectories. We measure the appropriateness\nby comparing the resulting hybrid discrete/continuous architecture with continuous hierarchical\nreinforcement learning (HRL). We consider our approach to be appropriate if it is better capable\nof learning to solve causal object-manipulation puzzles that involve tool use and causal chains of\nnon-trivial length that HRL.\nH.2\nWe hypothesize that the approach is robust enough to handle a realistic amount of perceptual noise.\nWe consider the approach to be robust to noise if there is no signiﬁcant performance drop when\nmoderate noise, e.g., 1-2% of the observational range, is added to the agent’s state representation.\nWe address these hypotheses by applying our method to three simulated robotic environments that are based\non the OpenAI Gym framework. For these environments, we provide manually deﬁned action planning\ndomain descriptions and combine a planner with a model-free reinforcement learner to learn the grounding\nof high-level action descriptions in low-level trajectories.\nOur research contribution is a principled method and proof-of-concept to ground high-level semantic\nactions in low-level sensorimotor motion trajectories and to integrate model-free reinforcement learning\nwith symbolic action planning. The novelty of this research is to use UVFAs and HER to decouple the\nreward mechanism from the high-level propositional subgoal representations provided by the action planner:\nThis is a provisional ﬁle, not the ﬁnal typeset article\n3\nEppe et al.\nFrom semantics to execution\nInstead of deﬁning an individual reward function for each predicate, our approach allows for a single\nsimple threshold-based sparse reward function that is the same for all predicates.\nOur research goal is to provide a proof-of-concept and a baseline for the integration of action planning\nwith reinforcement learning in continuous domains that involve complex causal dependencies.\nThe remainder of the article is organized as follows. In Section 2 we investigate the state of the art in task\nand motion planning, hierarchical learning and the integration of planning with learning. We identify the\nproblem of grounding high-level actions in low-level trajectories as a critical issue for robots to solve causal\npuzzles. We present our method and the underlying background in Section 3. We describe the realization\nof our experiments in Section 4 and show the experimental results in Section 5 before we discuss and align\nour ﬁndings with the hypotheses in Section 6. We conclude in Section 7.\n2\nSTATE OF THE ART\nOur work is related to robotic task and motion planning, but it also addresses plan execution. Therefore, it\nis also related to hierarchical learning algorithms and the integration of learning with planning.\n2.1\nCombined task and motion planning\nThe ﬁeld of combined task and motion planning (TAMP) investigates methods to integrate low-level\nmotion planning with high-level task planning. The ﬁeld aims at solving physical puzzles and problems that\nare too complex to solve with motion planning alone, often inspired by smart animal behavior (Toussaint\net al., 2018). For example, crows are able to perform a sequence of high-level actions, using tools like\nsticks, hooks or strings, to solve a puzzle that eventually leads to a reward (Taylor et al., 2009). A set\nof related benchmark problems has recently been proposed by Lagriffoul et al. (2018). However, since\nTAMP focuses primarily on the planning aspects and not necessarily on the online action execution, the\nbenchmark environments do not consider a physical action execution layer.\nToussaint et al. (2018) formulate the TAMP problem as an inverted differentiable physics simulator.\nThe authors consider the local optima of the possible physical interactions by extending mixed-integer\nprograms (MIP) (Deits and Tedrake, 2014) to ﬁrst-order logic. The authors deﬁne physical interactions as\naction primitives that are grounded in contact switches. The space of possible interactions is restricted to\nconsider only those interactions that are useful for the speciﬁc problem to solve. These interactions are\nformulated based on a ﬁxed set of predicates and action primitives in the domain of robotic tool use and\nobject manipulation. However, the authors provide only a theoretical framework for planning, and they do\nnot consider the physical execution of actions. Therefore, an empirical evaluation to measure the actual\nperformance of their framework, considering also real-world issues like sensorimotor noise, is not possible.\nOther TAMP approaches include the work by Alili et al. (2010) and de Silva et al. (2013) who both\ncombine a hierarchical symbolic reasoner with a geometrical reasoner to plan human-robot handovers of\nobjects. Both approaches consider only the planning, not the actual execution of the actions. The authors do\nnot provide an empirical evaluation in a simulated or real environment. Srivastava et al. (2014) also consider\naction execution and address the problem of grounding high-level tasks in low-level motion trajectories by\nproposing a planner-independent interface layer for TAMP that builds on symbolic references to continuous\nvalues. Speciﬁcally, they propose to deﬁne symbolic actions and predicates such that they refer to certain\nobjects and their poses. They leave it to the low-level motion planner to resolve the references. Their\napproach scales well on the planning level in very cluttered scenes, i.e., the authors demonstrate that the\nplanning approach can solve problems with 40 objects. The authors also present a physical demonstrator\nThis is a provisional ﬁle, not the ﬁnal typeset article\n4\nEppe et al.\nFrom semantics to execution\nusing a PR2 robot, but they do not provide a principled empirical evaluation to measure the success of\nthe action execution under realistic or simulated physical conditions. Wang et al. (2018) also consider\naction execution, though only in a simple 2D environment without realistic physics. Their focus is on\nsolving long-horizon task planning problems that involve sequences of 10 or more action primitives. To\nthis end, the authors present a method that learns the conditions and effects of high-level action operators\nin a kitchen environment.\nA realistic model that also considers physical execution has been proposed by Leidner et al. (2018).\nThe authors build on geometric models and a particle distribution model to plan goal-oriented wiping\nmotions. Their architecture involves low-level and high-level inference and a physical robotic demonstrator.\nHowever, the authors build on geometric modeling and knowledge only, without providing a learning\ncomponent. Noisy sensing is also not addressed in their work.\n2.2\nHierarchical learning-based approaches\nMost TAMP approaches consider the planning as an ofﬂine process given geometrical, algebraic or\nlogical domain models. This, however, does not necessarily imply the consideration of action execution.\nThe consideration of only the planning problem under idealized conditions is not appropriate in practical\nrobotic applications that often suffer from sensorimotor noise. To this end, researchers have investigated\nhierarchical learning-based approaches that differ conceptually from our work because they build on data\ninstead of domain-knowledge to realize the high-level control framework.\nFor example, Levy et al. (2019) and Nachum et al. (2018) both consider ant-maze problems in a continuous\nstate- and action space. The challenge of these problems lies in coordinating the low-level walking behavior\nof the ant-like agent with high-level navigation. However, these approaches do not appreciate that different\nlevels of the problem-solving process require different representational abstractions of states and actions.\nFor example, in our approach, the planner operates on propositional state descriptions like “object 1 on\ntop of object 2” and generates high-level conceptual actions like “move gripper to object”. In those HRL\napproaches, the high-level state- and action representations are within the same state-and action space as\nthe low-level representations. This leads to larger continuous problem spaces.\nOther existing hierarchical learning-based approaches are limited to discrete action- or state spaces on\nall hierarchical layers. For example, Kulkarni et al. (2016) present the h-DQN framework to integrate\nhierarchical action-value functions with goal-driven intrinsically motivated deep RL. Here, the bottom-level\nreward needs to be hand-crafted using prior knowledge of applications. Vezhnevets et al. (2017) introduce\nthe FeUdal Networks (FuNs), a two-layer hierarchical agent. The authors train subgoal embeddings to\nachieve a signiﬁcant performance in the context of Atari games with a discrete action space. Another\nexample of an approach that builds on discrete actions is the option-critic architecture by Bacon et al.\n(2017). Their method extends gradient computations of intra-option policies and termination functions\nto enable learning options that maximize the expected return within the options framework, proposed by\nSutton et al. (1999). The authors apply and evaluate their framework in the Atari gaming domain.\n2.3\nIntegrating learning and planning\nThere exist several robot deliberation approaches that exploit domain knowledge to deliberate robotic\nbehavior and to perform reasoning (e.g., Eppe et al. (2013); Rockel et al. (2013)). The following examples\nfrom contemporary research extend the knowledge-based robotic control approach and combine it with\nreinforcement learning:\nThis is a provisional ﬁle, not the ﬁnal typeset article\n5\nEppe et al.\nFrom semantics to execution\nThe Dyna Architecture (Sutton, 1991) and its derived methods, e.g., Dyna-Q (Sutton, 1990), queue-\nDyna (Peng and Williams, 1993), RTP-Q (Zhao et al., 1999), aim to speed up the learning procedure of\nthe agent by unifying reinforcement learning and incremental planning within a single framework. While\nthe RL component aims to construct the action model as well as to improve the value function and policy\ndirectly through real experiences from environment interaction, the planning component updates the value\nfunction with simulated experiences collected from the action model. The authors show that instead of\nselecting uniformly experienced state-action pairs during the planning, it is much more efﬁcient to focus on\npairs leading to the goal state (or nearby states) because these cause larger changes in value function. This\nis the main idea of the prioritized sweeping method (Moore and Atkeson, 1993) and derivatives (Andre\net al., 1998). The methods based on Dyna and prioritized sweeping have neither been demonstrated to\naddress sparse rewards nor do they consider mappings between discrete high-level actions and states and\ntheir low-level counter parts.\nMa and Cameron (2009) present the policy search planning method, in which they extend the policy\nsearch GPOMDP (Baxter and Bartlett, 2001) towards the multi-agent domain of robotic soccer. Herein,\nthey map symbolic plans to policies using an expert knowledge database. The approach does not consider\ntool use or similar causally complex problems. A similar restriction pertains to PLANQ-learning framework\n(Grounds and Kudenko, 2005): The authors combine a Q-learner with a high-level STRIPS planner (Fikes\nand Nilsson, 1972), where the symbolic planner shapes the reward function to guide the learners to the\ndesired policy. First, the planner generates a sequence of operators to solve the problem from the problem\ndescription. Then, each of these operators is learned successively by the corresponding Q-learner. This\ndiscrete learning approach, however, has not been demonstrated to be applicable beyond toy problems,\nsuch as the grid world domain that the authors utilize for demonstrations in their paper. Yamamoto et al.\n(2018) propose a hierarchical architecture that uses a high-level abduction-based planner to generate\nsubgoals for the low-level on-policy reinforcement learning component, which employed the proximal\npolicy optimization (PPO) algorithm (Schulman et al., 2017). This approach requires the introduction of an\nadditional task-speciﬁc evaluation function, alongside the basic evaluation function of the abduction model\nto allow the planner to provide the learner with the intrinsic rewards, similar to (Kulkarni et al., 2016). The\nevaluation is only conducted in a grid-based virtual world where an agent has to pick up materials, craft\nobjects and reach a goal position.\nA very interesting approach has been presented by Ugur and Piater (2015). The authors integrate\nlearning with planning, but instead of manually deﬁning the planning domain description they learn it\nfrom observations. To this end, they perform clustering mechanisms to categorize object affordances\nand high-level effects of actions, which are immediately employed in a planning domain description.\nThe authors demonstrate their work on a physical robot. In contrast to our work, however, the authors\nfocus mostly on the high-level inference and not on the robustness that low-level reinforcement-based\narchitectures provide.\n2.4\nSummary of the state of the art\nThe main weaknesses of the related state of the art that we address in this research are the following:\nTAMP approaches (Section 2.1) mainly focus on the planning aspect. Whereas they do not consider the\nphysical execution of the planned actions or only evaluate the plan execution utilizing manually deﬁned\nmapping between high-level (symbolic) and low-level (continuous value). These approaches require domain\nknowledge and model of robots and the environment to specify and execute the task and motion plans,\nwhich may suffer from noisy sensing conditions. On the contrary, hierarchical learning-based approaches\nThis is a provisional ﬁle, not the ﬁnal typeset article\n6\nEppe et al.\nFrom semantics to execution\n(Section 2.2) propose to learn both high-level and low-level from data, but mostly focus on solving\nproblems with discrete action space, and they require internal hand-crafted reward functions. Methods\nwith continuous action space like (Levy et al., 2019; Nachum et al., 2019) only consider setups without\nrepresentational abstractions between the different hierarchical layers. Mixed approaches (Section 2.3)\nthat integrate learning and planning have similar disadvantages as the two other groups. In particular the\nlack of principled approaches to realize the mapping between discrete and continual spaces, the manual\nshaping of reward functions, and the lack of approaches that have been demonstrated and applied in a\ncontinuous-space realistic environment.\n3\nINTEGRATING REINFORCEMENT LEARNING WITH ACTION PLANNING\nFigure 2. Our proposed integration model. Low-level motion planning elements are indicated in green\nand high-level elements in orange color. The abstraction functions fS, fG map the low-level state and\ngoal representations s, g to high-level state and goal representations S, G. These are given as input to the\nplanner to compute a high-level subgoal state Gsub. The subgoal grounding function fsubg maps Gsub to\na low-level subgoal gsub under consideration of the context provided by the current low-level state s and\nthe low-level goal g. The reinforcement learner learns to produce a low-level motion plan that consists of\nactions u based on the low-level subgoal gsub and the low-level state s.\nTo tackle the problems P.1 and P.2, and to address the research goal of grounding high-level actions\nin low-level control trajectories, we propose the architecture depicted in Figure 2. The novelty of the\narchitecture with respect to the state of the art is its ability to learn to achieve subgoals that are provided\nin an abstract symbolic high-level representation by using a single universal sparse reward function that\nis appropriate for all discrete high-level goal deﬁnitions. This involves i) the grounding of the high-level\nrepresentations to low-level subgoals (Section 3.1, Algorithm 1), and ii) the formalization of the abstraction\nof the low-level space to the high-level space (Section 3.1, Equation 4), iii) a UVFA- and HER-based\nreinforcement learner to achieve the low-level subgoals (Section 3.2), and, iv) the integration of an action\nplanner with the reinforcement learning using the abstraction- and grounding mechanisms (Section 3.3).\nThe resulting architecture is able to acquire a large repertoire of skills, similar to a multiple-goal\nexploration process (Forestier et al., 2017). However, instead of sampling and evaluating (sub-)goals\nthrough intrinsic rewards, the subgoals within our architecture are generated by the planner.\n3.1\nAbstraction and grounding of states and goals\nOur abstraction and grounding mechanism tackles the research problem P.1, i.e., the mapping from\nhigh-level actions to low-level subgoals. STRIPS-based action descriptions are deﬁned in terms of state\nchanges based on predicates for pre- and postconditions. To appreciate that the state change is determined\nThis is a provisional ﬁle, not the ﬁnal typeset article\n7\nEppe et al.\nFrom semantics to execution\nby the postcondition predicates, and not by the actions themselves, it is more succinct to deﬁne subgoals in\nterms of postcondition predicates because multiple actions may involve the same postconditions. Therefore,\nwe deﬁne a grounding function for subgoals fsubg. The function is based on predicates instead of actions to\navoid redundancy and to minimize the hand-engineering of domain models and background knowledge.\nTo abstract from low-level perception to high-level world states, we deﬁne abstraction functions fS, fG.\nThese functions do not require any additional background knowledge because they fully exploit the\ndeﬁnition of fsubg. In our formalization of the abstraction and grounding, we consider the following\nconventions and assumptions C.1-C.7:\nC.1\nThe low-level environment state space is fully observable, but observations may be noisy. We\nrepresent low-level environment states with ﬁnite-dimensional vectors s. To abstract away from visual\npreprocessing issues and to focus on the main research questions, we adapt the state representations\ncommonly used in deep reinforcement learning literature (Levy et al., 2019; Andrychowicz et al.,\n2017), i.e., states are constituted by the locations, velocities, and rotations of objects (including the\nrobot itself) in the environment.\nC.2\nThe low-level action space is determined by continuous ﬁnite-dimensional vectors u. For example,\nthe robotic manipulation experiments described in this paper consider a four-dimensional action\nspace that consists of the normalized relative three-dimensional movement of the robot’s gripper in\nCartesian coordinates plus a scalar value to represent the opening angle of the gripper’s ﬁngers.\nC.3\nEach predicate of the planning domain description determines a Boolean property of one object in\nthe environment. The set of all predicates is denoted as P = {p1, · · · , pn}. The high-level world\nstate S is deﬁned as the conjunction of all positive or negated predicates.\nC.4\nThe environment conﬁguration is fully determined by a set of objects whose properties can be\ndescribed by the set of high-level predicates P. Each predicate p ∈P can be mapped to a continuous\nﬁnite-dimensional low-level substate vector sp. For example, the location property and the velocity\nproperty of an object in Cartesian space are both fully determined by a three-dimensional continuous\nvector.\nC.5\nFor each predicate p, there exists a sequence of indices pidx that determines the indices of the\nlow-level environment state vector s that determines the property described by p. For example,\ngiven that p refers to the object being at a speciﬁc location, and given that the ﬁrst three values of s\ndetermine the Cartesian location of an object, we have that pidx = [0, 1, 2]. A requirement is that the\nindices of the individual predicates must not overlap, i.e., abusing set notation: pidx\n1\n∩pidx\n2\n= ∅(see\nAlgorithm 1 for details).\nC.6\nThe high-level action space consists of a set of grounded STRIPS operators (Fikes and Nilsson,\n1972) a that are determined by a conjunction of precondition literals and a conjunction of effect\nliterals.\nC.7\nA low-level goal g is the subset of the low-level state s, indicated by the indices gidx, i.e., g = s[gidx].\nFor example, consider that the low-level state s is a six-dimensional vector where the ﬁrst three\nelements represent the location and the last three elements represent the velocity of an object in\nCartesian space. Then, given that gidx = [0, 1, 2], we have that g = s[gidx] refers to the location of\nthe object.\n3.1.1\nMapping predicates to low-level subgoals\nAbstracting from observations to high-level predicate speciﬁcations is achieved by mapping the low-level\nstate space to a high-level conceptual space. This is realized with a set of functions fp\nsubg that we deﬁne\nThis is a provisional ﬁle, not the ﬁnal typeset article\n8\nEppe et al.\nFrom semantics to execution\nmanually for each predicate p. For a given predicate p, the function fp\nsubg generates the low-level substate\nsp that determines p, based on the current state s and the goal g:\nfp\nsubg(s, g) = sp\n(1)\nTo illustrate how fp\nsubg can be implemented, consider the following two examples from a block-stacking\ntask:\n1.\nConsider a predicate (at target o1) which indicates whether an object is at a given goal\nlocation on the surface of a table. Then the respective function f(at target o1)\nsubg\n(s, g) can be\nimplemented as follows:\nf(at target o1)\nsubg\n(s, g) = [g[0], g[1], g[2]]\n(2)\nIn this case, the function extracts the respective target coordinates for the object o1 directly from g\nand does not require any information from s.\n2.\nConsider further a predicate (on o2 o1), which is true if an object o2 is placed on top of another\nobject o1. Given that the Cartesian location of o1 is deﬁned by the ﬁrst three elements of the state\nvector s, one can deﬁne the following subgoal function:\nf(on o2 o1)\nsubg\n(s, g) = [s[0], s[1], s[2] + hobj], where hobj denotes the height of an object\n(3)\nHere, the target coordinates of the object o2 are computed by considering the current coordinates of\no1, i.e., the ﬁrst three values of s, and by adding a constant for the object height to the third (vertical\naxis) value.\n3.1.2\nGrounding high-level representations in low-level subgoals\nThe function fsubg that maps the high-level subgoal state Gsub in the context of s, g to a low-level subgoal\ngsub builds on Eq. (1), as described with the following Algorithm 1:\nAlgorithm 1 Mapping propositional state representations to continuous state representations\n1: function fsubg(Gsubg, s, g)\n2:\nssubg ←s\n3:\nsubgoal changed ←True\n4:\nwhile subgoal changed do\n5:\nslast\nsubg ←ssubg\n6:\nfor p ∈P do ▷For each predicate in high-level goal state, set low-level subgoal indices pidx\n7:\nssubg[pidx] ←sp, where sp = fp\nsubg(ssubg, g)\n8:\nsubgoal changed ←slast\nsubg ̸= ssubg\nThe while loop is necessary to prevent the situation where the application of fp\nsubg (in line 7, Algorithm\n1) changes ssubg in a manner that affects a previous predicate subgoal function. For example, consider the\ntwo predicates (at target o1) and (on o2 o1). The predicate (at target o1) determines the\nCartesian location of o1, and (on o2 o1) depends on these coordinates. Therefore, it may happen that\nf(at target o1)\nsubg\n= [x, y, z] causes the ﬁrst three elements of ssubg to be x, y, z. However, f(on o2 o1)\nsubg\n=\nThis is a provisional ﬁle, not the ﬁnal typeset article\n9\nEppe et al.\nFrom semantics to execution\n[x′, y′, z′] depends on these x, y, z to determine the x′, y′, z′ that encode the location of o2 in ssubg.\nThe while loop assures that f(on o2 o1)\nsubg\nis applied at least once after f(at target o1)\nsubg\nto consider this\ndependency. This assures that all dependencies between the elements of ssubg are resolved.\nTo guarantee the termination of the Algorithm 1, i.e., to avoid that the alternating changes of ssubg cause\nan inﬁnite loop, the indices pidx must be constrained in such a way that they do not overlap (see assumption\nC.5).\n3.1.3\nAbstracting from low-level state- and goal representations to propositional statements\nTo realize the abstraction from low-level to high-level representations, we deﬁne a set of functions in the\nform of Equation 4. Speciﬁcally, for each predicate p, we deﬁne the following function fp maps the current\nlow-level state and the low-level goal to the predicates’ truth values.\nfp(p, s) = diff(s[pidx], sp) < ϵ, with sp = fp\nsubg(s, g)\n(4)\nEquation 4 examines whether the subgoal that corresponds to a speciﬁc predicate is true, given the\ncurrent observed state s and a threshold value for the coordinates ϵ. In this article, but without any loss\nof generality, we assume that each predicate is determined by three coordinates. Equation 4 computes\nthe difference between these coordinates given the current state, and the coordinates determined by fp\nsubg\nas their Euclidean distance. For example, f(at target o1)\nsubg\nmay deﬁne target coordinates for o1, and\nEquation 4 considers the distance between the target coordinates and the current coordinates.\n3.2\nGeneration of adaptive low-level action trajectories\nTo address the research problem P.2, i.e, the grounding of actions and subgoals in low-level action\ntrajectories, we consider continuous goal-independent reinforcement learning approaches (Lillicrap et al.,\n2016; Schaul et al., 2015). Most reinforcement learning approaches build on manually deﬁned reward\nfunctions based on a metric that is speciﬁc to a single global goal, such as the body height, posture and\nforward speed of a robot that learns to walk (Schulman et al., 2015). Goal-independent reinforcement\nlearning settings do not require such reward shaping (c.f. (Ng et al., 1999)), as they allow one to parameterize\nlearned policies and value functions with goals. We employ the actor-critic deep deterministic policy\ngradient (DDPG) (Lillicrap et al., 2016) approach in combination with hindsight experience replay (HER)\n(Andrychowicz et al., 2017) to realize the goal-independent RL for the continuous control part in our\nframework. Using the HER technique with off-policy reinforcement learning algorithms (like DDPG)\nincreases the efﬁciency of sampling for our approach since HER stores not only the experienced episode\nwith the original goal (episode ←(st, ut, st+1, g)) in the replay buffer. It also stores modiﬁed versions of\nan episode where the goal is retrospectively set to a state that has been achieved during the episode, i.e.,\nepisode′ ←(st, ut, st+1, g′) with g′ = st′[gidx] for some t′ > t.\nTo realize this actor-critic architecture, we provide one fully connected neural network for the actor π,\ndetermined by the parameters θπ and one fully connected neural network for the critic Q, determined by the\nparameters θQ. The input to both networks is the concatenation of the low-level state s and the low-level\nsubgoal gsub. The optimization criterion for the actor π is to minimize the q value provided by the critic.\nThe optimization criterion for the critic is to minimize the mean squared error between the critic’s output\nq and the discounted expected reward according to the Bellmann equation for deterministic policies, as\nThis is a provisional ﬁle, not the ﬁnal typeset article\n10\nEppe et al.\nFrom semantics to execution\ndescribed by Equation 5.\narg min\nθQ (q −Q(st, ut, gsub|θQ))2,\nwhere\nQ(st, ut, gsub|θQ) = rt + γ · Q(st+1, π(st+1, gsub|θπ), gsub|θQ)\n(5)\nGiven that the action space is continuous n-dimensional, the observation space is continuous m-\ndimensional, and the goal space is continuous k-dimensional with k ≤m, the following holds for\nour theoretical framework: At each step t, the agent executes an action ut ∈Rn given a state st ∈Rm and\na goal g ⊆Rk, according to a behavioral policy, a noisy version of the target policy π that deterministically\nmaps the observation and goal to the action1. The action generates a reward rt = 0 if the goal is achieved\nat time t. Otherwise, the reward is rt = −1. To decide whether a goal has been achieved, a function f(st)\nis deﬁned that maps the observation space to the goal space, and the goal is considered to be achieved if\n|f(st) −g| < ϵ for a small threshold ϵ. This sharp distinction of whether or not a goal has been achieved\nbased on a distance threshold causes the reward to be sparse and renders shaping the reward with a\nhand-coded reward function unnecessary.\n3.3\nIntegration of high-level planning with reinforcement learning\nOur architecture integrates the high-level planner and the low-level reinforcement learner as depicted\nin Figure 2. The input to our framework is a low-level goal g. The sensor data that represents the environment\nstate s is abstracted together with g to a propositional high-level description of the world state S and goal\nG. An action planner based on the planning domain deﬁnition language (PDDL) (McDermott et al., 1998)\ntakes these high-level representations as input and computes a high-level plan based on manually deﬁned\naction deﬁnitions (c.f. the Appendix in Section 7 for examples). We have implemented the caching of\nplans to accelerate the runtime performance. The high-level subgoal state Gsub is the expected successor\nstate of the current state given that the ﬁrst action of the plan is executed. This successor state is used as a\nbasis to compute the next subgoal. To this end, Gsub is processed by the subgoal grounding function fsubg\n(Algorithm 1) that generates a subgoal gsub as input to the low-level reinforcement learner.\n4\nEXPERIMENTS\nThis section describes three experiments, designed for the evaluation of the proposed approach. We\nrefer to the experiments as block-stacking, tool use, and ant navigation. The ﬁrst two experiments are\nconducted with a Fetch robot arm, and the latter is adapted from research on continuous reinforcement\nlearning for legged locomotion (Levy et al., 2019). All experiments are executed in the Mujoco simulation\nenvironment (Todorov et al., 2012). For all experiments, we use a three-layer fully connected neural\nnetwork with the rectiﬁed linear unit (ReLU) activation function to represent the actor-critic network of the\nreinforcement learner in both experiments. We choose a learning rate of 0.01, and the networks’ weights\nare updated using a parallelized version of the Adam optimizer (Kingma and Ba, 2015). We use a reward\ndiscount of γ = 1 −1/T, where T is the number of low-level actions per rollout. For the block-stacking,\nwe use 50, 100 and 150 low-level actions for the case of one, two and three blocks respectively. For the tool\nuse experiment, we use 100 low-level actions, and for the ant navigation, we used 900 low-level actions.\n1 Applying action noise for exploration purposes is a common practice for off-policy reinforcement learning (e.g., ϵ-greedy). The kind of noise that we\ninvestigate in this article (e.g., in the experiments section Section 4) is supposed to simulate perceptual errors and not to be confused with this action noise for\nexploration.\nThis is a provisional ﬁle, not the ﬁnal typeset article\n11\nEppe et al.\nFrom semantics to execution\nPreliminary hyperoptimization experiments showed that the optimal number of units for each layer of\nthe neural networks for actor and critic of the reinforcement learning depends on the observation space.\nTherefore, we implement the network architecture such that the number of units in each layer scales with\nthe size of the observation vector. Speciﬁcally, the layers in the actor and critic consist of 12 units per\nelement in the observation vector. For example, for the case of the block-stacking experiment with two\nblocks, this results in 336 neural units per layer (see Section 4.1). We apply the same training strategy of\nHER (Andrychowicz et al., 2017), evaluate periodically learned policies during training without action\nnoise. We use a ﬁxed maximum number of epochs and early stopping at between 80% and 95% success\nrate, depending on the task.\nIn all experiments, we evaluate the robustness of our approach to perceptual noise. That is, in the\nfollowing we refer to perceptual noise, and not to the action noise applied during the exploration phase\nof the RL agent, if not explicitly stated otherwise. To evaluate the robustness to perceptual noise, we\nconsider the amount of noise relative to the value range of the state vector. To this end, we approximate\nthe continuous-valued state range, denoted rng, as the difference between the upper and lower quartile of\nthe elements in the history of the last 5000 continuous-valued state vectors that were generated during the\nrollouts2. For each action step in the rollout we randomly sample noise, denoted sγ, according to a normal\ndistribution with rng being the standard deviation. We add this noise to the state vector. To parameterize\nthe amount of noise added to the state, we deﬁne a noise-to-signal-ratio κ such that the noise added to the\nstate vector is computed as snoisy = s + κ · sγ. We refer to the noise level as the percentage corresponding\nto κ. That is, e.g., κ = 0.01 is equivalent to a noise level of 1%.\nFor all experiments, i.e., block-stacking, tool use and ant navigation, we trained the agent on multiple\nCPUs in parallel and averaged the neural network weights of all CPU instances after each epoch, as\ndescribed by Andrychowicz et al. (2017). Speciﬁcally, we used 15 CPUs for the tool use and the block-\nstacking experiments with one and two blocks; we used 25 CPUs for the block-stacking with 3 blocks. For\nthe ant navigation, we used 15 CPUs when investigating the robustness to noise (Figure 7) and 1 CPU\nwhen comparing our approach to the framework by Levy et al. (2019) (Figure 8). The latter was necessary\nto enable a fair comparison between the approaches because the implementation of the architecture of Levy\net al. (2019) supports only a single CPU. For all experiments, an epoch consists of 100 training rollouts per\nCPU, followed by training the neural networks for actor and critic with 15 batches after each epoch, using\na batch size of 256. The results in Figure 4 and Figure 6 illustrate the median and the upper and lower\nquartile over multiple (n ≥5) repetitions of each experiment.\n4.1\nBlock-stacking\nFigure 1 (left) presents the simulated environment for this experiment, where a number of blocks (i.e., up\nto three) are placed randomly on the table. The task of the robot is to learn how to reach, grasp and stack\nthose blocks one-by-one to their corresponding random target location. The task is considered completed\nwhen the robot successfully places the last block on top of the others in the right order, and moves its\ngripper to another random target location. The difﬁculty of this task increases with the number of blocks\nto stack. The order in which the blocks need to be stacked is randomized for each rollout. The causal\ndependencies involved here are, that a block can only be grasped if the gripper is empty, a block (e.g., A)\ncan only be placed on top of another block (e.g., B) if there is no other block (e.g., C) already on top of\neither A or B, etc.\n2 We use the upper and lower quartile and not the minimum and maximum of all elements to eliminate outliers.\nThis is a provisional ﬁle, not the ﬁnal typeset article\n12\nEppe et al.\nFrom semantics to execution\nFigure 3. An ant agent performing a navigation and locomotion task in a four-room environment. Herein,\nthe agent needs to learn how to walk and ﬁnd a way to reach the desired position. In this case, the agent\nneeds to walk from the upper right room to the target location in the lower left room.\nThe size of the goal space depends on the number of blocks. For this experiment, the goal space is a\nsubset of the state-space that is constituted by the three Cartesian coordinates of the robot’s gripper and\nthree coordinates for each block. That is, the dimension of the goal- and subgoal space is k = (1 + no) · 3,\nwhere no ∈{1, 2, 3} is the number of objects.\nThe state-space of the reinforcement learning agent consists of the Cartesian location and velocity of the\nrobot’s gripper, the gripper’s opening angle, and the Cartesian location, rotation, velocity, and rotational\nvelocity of each object. That is, the size of the state vector is |s| = 4 + no · 12, where no is the number of\nblocks.\nThe planning domain descriptions for all environments are implemented with the PDDL actions and\npredicates provided in the Appendix (in Section A).\n4.2\nTool use\nThe environment utilized for this experiment is shown in Figure 1 (right). A single block is placed\nrandomly on the table, such that it is outside the reachable region of the Fetch robot. The robot has to move\nthe block to a target position (which is randomized for every rollout) within the reachable (dark brown)\nregion on the table surface. In order to do so, the robot has to learn how to use the provided rake. The\nrobot can drag the block either with the left or the right edge of the rake. The observation space consists of\nthe Cartesian velocities, rotations, and locations of the robot’s gripper, the tip of the rake, and the object.\nAn additional approximation of the end of the rake is added in this task. The goal space only contains\nthe Cartesian coordinates of the robot’s gripper, the tip of the rake, and the object. The planning domain\ndescription for this tool use environment can be found in the Appendix (in Section B).\nThis is a provisional ﬁle, not the ﬁnal typeset article\n13\nEppe et al.\nFrom semantics to execution\n4.3\nAnt navigation\nThe environment of navigation and locomotion in a four-connected-room scenario is shown in Figure 3,\nwhere the ant has to ﬁnd a way to the randomly allocated goal location inside one of the four-rooms. The\nstate-space consists of the Cartesian location and transitional velocity of the ant’s torso, along with the\njoint position and velocity of the eight joints of the ant (i.e., each leg has one ankle joint and one hip joint).\nThe goal space contains the Cartesian coordinate of the ant’s torso. There are no other objects involved\nin the task. The planning domain description and the high-level action speciﬁcations for this navigation\nenvironment can be found in the Appendix (in Section C).\n5\nRESULTS\nTo evaluate our approach, we investigate the success rate of the testing phase over time for all experiments,\ngiven varying noise levels κ (see Section 4). The success rate is computed per epoch, by averaging over the\nnumber of successful rollouts per total rollouts over ten problem instances per CPU.\n5.1\nBlock-stacking\nFor the experiment with one block, the approach converges after around ten epochs, even with κ = 0.06,\ni.e., if up to 6% noise is added to the observations. The results are signiﬁcantly worse for 8% and more\nnoise. For the experiment with two blocks, the performance drops already for 6% noise. Interestingly, for\nboth one and two blocks, the convergence is slightly faster if a small amount (1-2%) of noise is added,\ncompared to no noise at all. The same seems to hold for three blocks, although no clear statement can be\nmade because the variance is signiﬁcantly higher for this task.\nFor the case of learning to stack three blocks consider also Figure 5,\nwhich shows how\nmany subgoals have been achieved on average during each epoch. For our particular PDDL\nimplementation,\nsix high-level actions,\nand hence six sugboals,\nare at least required to\nsolve the task: [move gripper to(o1), move to target(o1), move gripper to(o2),\nmove o on o(o2,o1), move gripper to(o3), move o on o(o3,o2)]. First, the gripper\nneeds to move to the randomly located object o1, then, since the target location of the stacked tower is also\nrandomly selected, the gripper needs to transport o1 to the target location. Then the gripper moves to o2\nto place it on top of o1, and repeats these steps for o3.The result shows that the agent can consistently\nlearn to achieve the ﬁrst ﬁve subgoals on average, but is not able to proceed further. This demonstrates that\nthe agent robustly learns to stack the ﬁrst two objects, but fails to stack also the third one.\n5.2\nTool use\nThe results in Figure 6 reveal that our proposed approach allows the agent to learn and complete the\ntask in under 100 training epochs (corresponding to approximately 8 hours with 15 CPUs) even with\na noise level increased up to 4% of the state range. We observe that it becomes harder for the agent to\nlearn when the noise level exceeds 4%. In the case of 8% noise, the learning fails to achieve a reasonable\nperformance in the considered time–ﬁrst 100 training epochs (i.e., it only obtains less than 1% success\nrate). Interestingly, in cases with very low noise levels (1%-2%), the learning performance is better or at\nleast as good as the case with no noise added at all.\nThis is a provisional ﬁle, not the ﬁnal typeset article\n14\nEppe et al.\nFrom semantics to execution\nOne block\nTwo blocks\nThree blocks\nFigure 4. Results of the block-stacking experiments for one (top), two (middle) and three (bottom) blocks\nunder different sensory noise levels.\nThis is a provisional ﬁle, not the ﬁnal typeset article\n15\nEppe et al.\nFrom semantics to execution\nAchieved subgoals,\nthree blocks\nFigure 5. Number of subgoals reached for the case of stacking three blocks.\nTool use\nFigure 6. Results of the tool use experiment under different noise levels. The case for a noise level of 0.01\nis subject to the early stopping mechanism.\n5.3\nAnt navigation\nFigure 7 presents the performance of trained agents following our proposed approach in the ant navigation\nscenario. The results show that the agent can learn to achieve the task in less than 30 training epochs under\nthe low noise level conditions (up to 1%). The performance decreases slightly in the case of 1.5% but the\nagent still can learn the task after around 70 training epochs. With a higher noise level (i.e., 2%), the agent\nrequires longer training time to cope with the environment.\n5.4\nComparison with hierarchical reinforcement learning\nResults in Figure 8 depict the benchmark experiment of our proposed approach with the HRL approach\nby Levy et al. (2019). Though the HRL approach quickly learns the task at the beginning, it does not exceed\na success rate of 70 %. In comparison, our approach learns to solve the task more reliably, eventually\nreaching 100%, but the success rate grows signiﬁcantly later, at around 50 epochs.\nThis is a provisional ﬁle, not the ﬁnal typeset article\n16\nEppe et al.\nFrom semantics to execution\nAnt navigation\nFigure 7. Results of the ant navigation experiment under different noise levels. The curves are subject to\nearly stopping.\nComparison with HRL,\nant navigation\nFigure 8. Comparison of two approaches for the ant navigation experiment between two approaches: our\n(PDDL+HER) approach and hierarchical reinforcement learning (HRL) (Levy et al., 2019)\n6\nDISCUSSION\nThe results indicate that our proof-of-concept addresses the hypotheses H.1 and H.2 as follows:\n6.1\nHypothesis H.1: Ability to ground high-level actions in low-level trajectories\nOur experiments indicate that the grounding of high-level actions in low-level RL-based robot control\nusing the HER approach performs well for small to medium-sized subgoal spaces. However, learning is not\ncompletely automated, as the approach requires the manual deﬁnition of the planning domain and of the\nfunctions fp\nsubg that maps planning domain predicates to subgoals.\nFor the tasks of stacking two blocks and the tool use, the subgoal space involved nine values, and both\ntasks could be learned successfully. The qualitative evaluation and visual inspection of the agent in the\nThis is a provisional ﬁle, not the ﬁnal typeset article\n17\nEppe et al.\nFrom semantics to execution\nrendered simulation revealed that the grasping of the ﬁrst and second block failed more often for the\nexperiment of stacking three blocks than for the experiment of stacking two blocks. Therefore, we conclude\nthat the subgoal space for stacking three blocks, which involves twelve values, is too large.\nHowever, the performance on the control-level was very robust. For example, it happened frequently\nduring the training and exploration phase that the random noise in the actions caused a block to slip out of\nthe robot’s grip. In these cases, the agent was able to catch the blocks immediately while they were falling\ndown. During the tool use experiment, the agent was also able to consider the rotation of the rake, to grasp\nthe rake at different positions, and to adapt its grip it when it was slipping.\nThe results indicate that the approach is able to solve causal puzzles if the subgoal space is not too large.\nThe architecture depends strongly on the planning domain representation that needs to be implemented\nmanually. In practice, the manual domain-engineering that is required for the planning is appropriate for\ntasks that are executed frequently, such as adaptive robotic co-working at a production line or in a drone\ndelivery domain. Due to the caching of plans (see Section 3.3), we have not encountered issues with the\ncomputational complexity problem and run-time issues of the planning approach.\nOur measure of appropriateness that we state in H.1 is to evaluate whether our method outperforms\na state-of-the-art HRL approach. Figure 8 depicts that this is the case in terms of the ﬁnal success rate.\nSpeciﬁcally, the ﬁgure shows that the HRL approach learns faster initially, but never reaches a success rate\nof more than 70%, while our approach is slower but reaches 100%. A possible explanation for this behavior\nis that HRL implements a “curriculum effect” (cf. Eppe et al. (2019)), in the sense that it ﬁrst learns to solve\nsimple subgoals due to its built-in penalization of difﬁcult subgoals. However, as the success rate increases,\nthere are fewer unsuccessful rollouts to be penalized which potentially leads to more difﬁcult subgoals and,\nconsequently, a lower overall success rate. This curriculum effect is not present in our approach because\nthe planning mechanism does not select subgoals according to their difﬁculty. Investigating and exploiting\nthis issue in detail is potentially subject to further research.\n6.2\nHypothesis H.2: Robustness to noise\nFor the block-stacking with up to two blocks and the tool-use experiments, the approach converged with\na reasonable noise-to-signal ratio of four to six percent. For the block-stacking with three blocks and for\nthe ant environment, a smaller amount of noise was required. An interesting observation is that a very low\namount of random noise, i.e., κ = 0.01, improves the learning performance for some cases. Adding random\nnoise, e.g., in the form of dropout, is a common technique to improve neural network-based machine\nlearning because it helps neural networks to generalize better from datasets. One possible explanation for\nthe phenomenon is, therefore, that the noise has the effect of generalizing the input data for the neural\nnetwork training, such that the parameters become more robust.\nNoise is an important issue for real physical robots. Our results indicate that the algorithm is potentially\nappropriate for physical robots, at least for the case of grasping and moving a single block to a target\nlocation. For this case, with a realistic level of noise, the algorithm converged after approximately ten\nepochs (see Figure 4). Per epoch and CPU, the agent conducts 100 training rollouts. A single rollout\nwould take around 20 seconds on a physical robot. Considering that we used 15 CPUs, the equivalent\nrobot training time required is approximately 83 hours. For the real application, the physical training time\ncan potentially further be lowered by applying more neural network training batches per rollout, and by\nperforming pre-training using the simulation along with continual learning deployment techniques, such as\nthe method proposed by Traor´e et al. (2019).\nThis is a provisional ﬁle, not the ﬁnal typeset article\n18\nEppe et al.\nFrom semantics to execution\n7\nCONCLUSION\nWe have developed a hierarchical architecture for robotic applications in which agents must perform\nreasoning over a non-trivial causal chain of actions. We have employed a PDDL planner for the high-level\nplanning and we have integrated it with an off-policy reinforcement learner to enable robust low-level\ncontrol.\nThe innovative novelty of our approach is the combination of action planning with goal-independent\nreinforcement learning and sparse rewards (Lillicrap et al., 2016; Andrychowicz et al., 2017). This\nintegration allowed us to address two research problems that involve the grounding of the discrete high-\nlevel state and action space in sparse rewards for low-level reinforcement learning. We addressed the\nproblem of grounding of symbolic state spaces in continuous-state subgoals (P.1), by proposing a principled\npredicate-subgoal mapping, which involves the manual deﬁnitions of functions fp\nsubg for each predicate p.\nWe assume that the manual deﬁnition of functions fp\nsubg generally involves less engineering effort than\ndesigning a separate reward function for each predicate. Although this assumption heavily depends on\nthe problem domain and may be subject to further discussion, the manual deﬁnition of functions fp\nsubg is\nat least a useful scaffold for further research that investigates the automated learning of functions fp\nsubg,\npossibly building on the research by Ugur and Piater (2015).\nThe predicate-subgoal mapping is also required to address the problem of mapping subgoals to low-level\naction trajectories (P.2) by means of reinforcement learning with sparse rewards using hindsight experience\nreplay (Andrychowicz et al., 2017). Our resulting approach has two advantages over other methods that\ncombine action planning and reinforcement learning, e.g., (Grounds and Kudenko, 2005; Yamamoto\net al., 2018): The low-level action space for our robotic application is continuous and it supports a higher\ndimensionality.\nWe have realized and evaluated our architecture in simulation, and we addressed two hypotheses (H.1\nand H.2): First, we demonstrate that the approach can successfully integrate high-level planning with\nreinforcement learning and this makes it possible to solve simple causal puzzles (H.1); second, we\ndemonstrate robustness to a realistic level of sensory noise (H.2). The latter demonstrates that our approach\nis potentially applicable to real-world robotic applications. The synthetic noise used in our experiments\ndoes not yet fully guarantee that our approach is capable of bridging the reality gap, but we consider it\na ﬁrst step towards real robotic applications (see also Andrychowicz et al. (2018); Nguyen et al. (2018);\nTraor´e et al. (2019)).\nThe causal puzzles that we investigate in this paper are also relevant for hierarchical reinforcement\nlearning (HRL) (e.g., (Levy et al., 2019)), but we have not been able to identify an article that presents\ngood results in problem-solving tasks that have a causal complexity comparable to our experiments. An\nempirical comparison was, therefore, not directly possible. Our approach has the advantage over HRL that\nit exploits domain knowledge in the form of planning domain representations. The disadvantage compared\nto HRL is that the domain knowledge must be hand-engineered. In future work, we plan to complement\nboth approaches, e.g., by building on vector-embeddings to learn symbolic planning domain descriptions\nfrom scratch by means of the reward signal of the reinforcement learning. A similar approach, based on\nthe clustering of affordances, has been presented by Ugur and Piater (2015), and complementing their\nmethod with reinforcement learning suggests signiﬁcant potential. An overview of this topic and potential\napproaches is provided by Lesort et al. (2018). We also plan to apply the approach to a physical robot and\nThis is a provisional ﬁle, not the ﬁnal typeset article\n19\nEppe et al.\nFrom semantics to execution\nto reduce the amount of physical training time by pre-training the agent in our simulation and by applying\ndomain-randomization techniques (Andrychowicz et al., 2018).\nCONFLICT OF INTEREST STATEMENT\nThe authors declare that the research was conducted in the absence of any commercial or ﬁnancial\nrelationships that could be construed as a potential conﬂict of interest.\nFUNDING\nManfred Eppe and Stefan Wermter acknowledge funding by the Experiment! Programme of the Volkswagen\nStiftung. Manfred Eppe, Stefan Wermter and Phuong D.H. Nguyen acknowledge support via the German\nResearch Foundation (DFG) within the scope of the IDEAS project of the DFG priority programme “The\nActive Self”. Furthermore, we acknowledge supports from the European Union’s Horizon 2020 research\nand innovation programme under the Marie Sklodowska-Curie grant agreements No 642667 (SECURE)\nand No 721385 (SOCRATES).\nACKNOWLEDGMENTS\nWe thank Erik Strahl for the technical supports with the computing servers and Fares Abawi for the initial\nversion of the Mujoco simulation for the experiments. We also thank Andrew Levy for providing the code\nof his hierarchical actor-critic reinforcement learning approach (Levy et al., 2019).\nREFERENCES\nAlili, S., Pandey, A. K., Sisbot, A., and Alami, R. (2010). Interleaving Symbolic and Geometric Reasoning\nfor a Robotic Assistant. In ICAPS Workshop on Combining Action and Motion Planning\nAndre, D., Friedman, N., and Parr, R. (1998). Generalized Prioritized Sweeping. In Advances in Neural\nInformation Processing Systems 10, eds. M. I. J. Solla, M. J. Kearns, and S. A. (MIT Press). 1001–1007\nAndrychowicz, M., Baker, B., Chociej, M., Jozefowicz, R., McGrew, B., Pachocki, J., et al. (2018).\nLearning Dexterous In-Hand Manipulation. arXiv preprint arXiv:1808.00177\nAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P., et al. (2017). Hindsight\nExperience Replay. In Conference on Neural Information Processing Systems (NIPS). 5048–5058\nAytar, Y., Pfaff, T., Budden, D., Paine, T. L., Wang, Z., and de Freitas, N. (2018). Playing hard exploration\ngames by watching YouTube. In Conference on Neural Information Processing Systems (NeurIPS).\n2930–2941\nBacon, P.-L., Harb, J., and Precup, D. (2017). The Option-Critic Architecture. In AAAI Conference on\nArtiﬁcial Intelligence. 1726–1734\nBaxter, J. and Bartlett, P. L. (2001). Inﬁnite-horizon policy-gradient estimation. In Journal of Artiﬁcial\nIntelligence Research, vol. 15. 319–350\nde Silva, L., Pandey, A. K., Gharbi, M., and Alami, R. (2013). Towards Combining HTN Planning and\nGeometric Task Planning. In RSS Workshop on Combined Robot Motion Planning and AI Planning for\nPractical Applications\nDeisenroth, M. P. and Rasmussen, C. E. (2011). PILCO: A Model-Based and Data-Efﬁcient Approach to\nPolicy Search. In International Conference on Machine Learning (ICML). 465–472\nDeits, R. and Tedrake, R. (2014).\nFootstep planning on uneven terrain with mixed-integer convex\noptimization. In 2014 IEEE-RAS International Conference on Humanoid Robots (IEEE), 279–286\nThis is a provisional ﬁle, not the ﬁnal typeset article\n20\nEppe et al.\nFrom semantics to execution\nDoncieux, S., Filliat, D., D´ıaz-Rodr´ıguez, N., Hospedales, T., Duro, R., Coninx, A., et al. (2018).\nOpen-ended learning: A conceptual framework based on representational redescription. In Frontiers in\nNeurorobotics, vol. 12. 1–6\nEppe, M., Bhatt, M., and Dylla, F. (2013). Approximate Epistemic Planning with Postdiction as Answer-\nSet Programming. In International Conference on Logic Programming and Nonmonotonic Reasoning\n(LPNMR). 290–303\nEppe, M., Magg, S., and Wermter, S. (2019). Curriculum goal masking for continuous deep reinforcement\nlearning. In International Conference on Development and Learning and Epigenetic Robotics (ICDL-\nEpiRob). 183–188\nFikes, R. and Nilsson, N. (1972). STRIPS: A new approach to the application of theorem proving to\nproblem solving. In Artiﬁcial intelligence, vol. 2. 189 – 208\nForestier, S., Mollard, Y., and Oudeyer, P.-Y. (2017). Intrinsically Motivated Goal Exploration Processes\nwith Automatic Curriculum Learning. arXiv preprint arXiv:1708.02190\nGrounds, M. and Kudenko, D. (2005). Combining Reinforcement Learning with Symbolic Planning. In\nAdaptive Agents and Multi-Agent Systems III. Adaptation and Multi-Agent Learning (Berlin, Heidelberg:\nSpringer Berlin Heidelberg). 75–86\nKingma, D. P. and Ba, J. L. (2015). Adam: a Method for Stochastic Optimization. In International\nConference on Learning Representations (ICLR)\nKulkarni, T. D., Narasimhan, K. K. R., Saeedi, A., and Tenenbaum, J. B. (2016). Hierarchical Deep\nReinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation. In Conference on\nNeural Information Processing Systems (NIPS). 3675–3683\nLagriffoul, F., Dantam, N. T., Garrett, C., Akbari, A., Srivastava, S., and Kavraki, L. E. (2018). Platform-\nIndependent Benchmarks for Task and Motion Planning. In IEEE Robotics and Automation Letters,\nvol. 3. 3765–3772\nLeidner, D., Bartels, G., Bejjani, W., Albu-Sch¨affer, A., and Beetz, M. (2018). Cognition-enabled robotic\nwiping: Representation, planning, execution, and interpretation. In Robotics and Autonomous Systems\n(North-Holland), vol. 114. 199–216\nLesort, T., D´ıaz-Rodr´ıguez, N., Goudou, J. F., and Filliat, D. (2018). State representation learning for\ncontrol: An overview. In Neural Networks (Elsevier Ltd), vol. 108. 379–392\nLevy, A., Konidaris, G., Platt, R., and Saenko, K. (2019). Learning Multi-Level Hierarchies with Hindsight.\nIn International Conference on Learning Representations (ICLR)\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., et al. (2016). Continuous Control with\nDeep Reinforcement Learning. In International Conference on Learning Representations (ICLR)\nMa, J. and Cameron, S. (2009). Combining policy search with planning in multi-agent cooperation. In\nRoboCup 2008: Robot Soccer World Cup XII. Lecture Notes in Computer Science (Springer, Berlin,\nHeidelberg), vol. 5399 LNAI. 532–543\nMcDermott, D., Ghallab, M., Howe, A., Knoblock, C., Ram, A., Veloso, M., et al. (1998). PDDL - The\nPlanning Domain Deﬁnition Language. Tech. rep., Yale Center for Computational Vision and Control\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., et al. (2015). Human-\nlevel control through deep reinforcement learning. In Nature (Nature Publishing Group), vol. 518.\n529–533\nMoore, A. W. and Atkeson, C. G. (1993). Prioritized Sweeping: Reinforcement Learning with Less Data\nand Less Time. In Machine Learning. 103–130\nNachum, O., Brain, G., Gu, S., Lee, H., and Levine, S. (2018). Data-Efﬁcient Hierarchical Reinforcement\nLearning. In Conference on Neural Information Processing Systems (NeurIPS). 3303–3313\nThis is a provisional ﬁle, not the ﬁnal typeset article\n21\nEppe et al.\nFrom semantics to execution\nNachum, O., Gu, S., Lee, H., and Levine, S. (2019). Near-Optimal Representation Learning for Hierarchical\nReinforcement Learning. In International Conference on Learning Representations (ICLR)\nNg, A. Y., Harada, D., and Russell, S. J. (1999). Policy Invariance Under Reward Transformations: Theory\nand Application to Reward Shaping. In International Conference on Machine Learning (ICML) (Morgan\nKaufmann), 278–287\nNguyen, P. D., Fischer, T., Chang, H. J., Pattacini, U., Metta, G., and Demiris, Y. (2018). Transferring\nvisuomotor learning from simulation to the real world for robotics manipulation tasks. In 2018 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS) (IEEE), 6667–6674\nPeng, J. and Williams, R. J. (1993). Efﬁcient learning and planning within the Dyna framework. In IEEE\nInternational Conference on Neural Networks. 168–174 vol.1\nPohlen, T., Piot, B., Hester, T., Azar, M. G., Horgan, D., Budden, D., et al. (2018). Observe and Look\nFurther: Achieving Consistent Performance on Atari. arXiv preprint arXiv:1805.11593\nRockel, S., Neumann, B., Zhang, J., Konecny, S., Mansouri, M., Pecora, F., et al. (2013). An Ontology-\nbased Multi-level Robot Architecture for Learning from Experiences. In AAAI Spring Symposium 2013\n(AAAI Press), 52–57\nSchaul, T., Horgan, D., Gregor, K., and Silver, D. (2015). Universal Value Function Approximators. In\nInternational Conference on Machine Learning (ICML). 1312–1320\nSchulman, J., Levine, S., Jordan, M., and Abbeel, P. (2015). Trust Region Policy Optimization. In\nInternational Conference on Machine Learning (ICML). 1889–1897\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal Policy Optimization\nAlgorithms. arXiv preprint arXiv:1707.06347\nSrivastava, S., Fang, E., Riano, L., Chitnis, R., Russell, S., and Abbeel, P. (2014). Combined Task\nand Motion Planning Through an Extensible Planner-Independent Interface Layer. In International\nConference on Robotics and Automation (ICRA). 639–646\nSutton, R. S. (1990). Integrated architectures for learning, planning, and reacting based on approximating\ndynamic programming. In Machine Learning Proceedings 1990 (Elsevier). 216–224\nSutton, R. S. (1991). Dyna, an integrated architecture for learning, planning, and reacting. In ACM SIGART\nBulletin, vol. 2. 160–163\nSutton, R. S., Precup, D., and Singh, S. (1999). Between MDPs and semi-MDPs: A framework for temporal\nabstraction in reinforcement learning. In Artiﬁcial Intelligence, vol. 112. 181–211\nTaylor, A., Hunt, G., Medina, F., and Gray, R. (2009). Do New Caledonian crows solve physical problems\nthrough causal reasoning? In Proceedings of the Royal Society B: Biological Sciences, vol. 276. 247–254\nTodorov, E., Erez, T., and Tassa, Y. (2012). MuJoCo: A physics engine for model-based control. In IEEE\nInternational Conference on Intelligent Robots and Systems. 5026–5033\nToussaint, M., Allen, K. R., Smith, K. A., and Tenenbaum, J. B. (2018). Differentiable Physics and Stable\nModes for Tool-Use and Manipulation Planning. In Robotics: Science and Systems (RSS)\nTraor´e, R., Caselles-Dupr´e, H., Lesort, T., Sun, T., Rodr´ıguez, N. D., and Filliat, D. (2019). Continual\nreinforcement learning deployed in real-life using policy distillation and sim2real transfer. CoRR\nabs/1906.04452\nUgur, E. and Piater, J. (2015). Bottom-Up Learning of Object Categories, Action Effects and Logical\nRules: From Continuous Manipulative Exploration to Symbolic Planning. In International Conference\non Robots and Automation (ICRA). 2627–2633\nVezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver, D., et al. (2017). FeUdal\nNetworks for Hierarchical Reinforcement Learning. In International Conference on Machine Learning\n(ICML). 3540–3549\nThis is a provisional ﬁle, not the ﬁnal typeset article\n22\nEppe et al.\nFrom semantics to execution\nWang, Z., Garrett, C. R., Kaelbling, L. P., and Lozano-Perez, T. (2018). Active Model Learning and\nDiverse Action Sampling for Task and Motion Planning. In 2018 IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS) (IEEE), 4107–4114\nYamamoto, K., Onishi, T., and Tsuruoka, Y. (2018). Hierarchical Reinforcement Learning with Abductive\nPlanning. In ICML / IJCAI / AAMAS Workshop on Planning and Learning\nZhao, G., Tatsumi, S., and Sun, R. (1999).\nRTP-Q: A Reinforcement Learning System with Time\nConstraints Exploration Planning for Accelerating the Learning Rate.\nIn IEICE Transactions on\nFundamentals of Electronics, Communications and Computer Sciences. vol. 82, 2266–2273\nAPPENDIX\nPDDL domain descriptions\nFor all planning domain deﬁnitions, we used only the STRIPS semantic requirement of the Planning\nDomain Deﬁnition Language (PDDL) (McDermott et al., 1998), i.e., pre- and postconditions, and we\nrealized the quantiﬁcation operators by grounding the variables manually.\nA\nBlock-stacking\nThe domain description of the block-stacking task is described in the following Listing 1.\nListing 1. Block-stacking domain\n(define (domain block)\n(:objects o1 ... on)\n(:predicates\n(gripper_at ?o)\n(gripper_at_target)\n(at_target ?o)\n(on ?o1 ?o2)\n)\n(:action move_gripper_to_o\n:parameters (?o)\n:precondition ()\n:effect (and (gripper_at ?o) (forall ?o1 != ?o: (not (\ngripper_at ?o1))\n(not (on ?o1 ?o)) (not (gripper_at_target\n))))\n)\n(:action move_o_to_target\n:parameters (?o)\n:precondition (gripper_at ?o)\n:effect (at_target ?o)\n)\n(:action move_o_on_o\n:parameters (?o1 ?o2)\n:precondition (and (gripper_at ?o1) )\n:effect (and (on ?o1 ?o2)\n(not (on ?o2 ?o1)))\n)\nThis is a provisional ﬁle, not the ﬁnal typeset article\n23\nEppe et al.\nFrom semantics to execution\n(:action move_gripper_to_target\n:parameters ()\n:precondition ()\n:effect\n(and\n(gripper_at_target)\n(forall ?o: (not (gripper_at ?o))\n)\n)\n)\n)\nB\nTool use\nThe domain description of the tool use task is described in the following listing 2.\nListing 2. Tool use\n(define (domain tool)\n(:requirements :strips)\n(:objects obj rake)\n(:predicates\n(gripper_at ?o)\n(gripper_at_target)\n(at_target ?o)\n(at ?o1 ?o2)\n)\n(:action move_gripper_to_o\n:parameters (?o)\n:precondition ()\n:effect\n(and (gripper_at ?o)\n(forall ?o1 != ?o:\n(not (gripper_at ?o1))\n(not (at ?o0 ?o1))\n(not (gripper_at_target))\n)\n)\n)\n(:action move_o_at_o\n:parameters (?o0 ?o1)\n:precondition (gripper_at ?o0)\n:effect (at ?o0 ?o1)\n)\n(:action move_o_to_target_by_o\n:parameters (?o1 ?o0)\n:precondition (and (at ?o0 ?o1) (gripper_at ?o0) )\nThis is a provisional ﬁle, not the ﬁnal typeset article\n24\nEppe et al.\nFrom semantics to execution\n:effect (and (at_target ?o1) (at ?o0 ?o1) )\n)\n(:action move_o_to_target\n:parameters (?o)\n:precondition (gripper_at ?o)\n:effect (and (at_target ?o) (gripper_at ?o) )\n)\n(:action move_gripper_to_target\n:parameters (?o)\n:precondition ()\n:effect\n(and\n(gripper_at_target)\n(forall ?o:\n(not (gripper_at ?o))\n)\n)\n)\n)\nC\nAnt navigation\nThe domain description of the ant navigation task is described in the following listing 3.\nFor this listing we did not use the built-in PDDL objects and variables (indicated with ?<objname>\nsyntax) to instantiate the predicates and actions. Instead, we implemented a script to generate the predicate\nand action deﬁnitions according to Listing 3 such that the following criteria are met:\n1.\nRooms (denoted <R>) are labeled 00, 01, 10, and 10, such that the 0 and 1 denote the column and\nrow of the 2x2 grid in the ant navigation environment. E.g., room 00 is the lower left room and room\n11 is the upper right room.\n2.\nDoors (denoted <D>, the passages between the rooms) are labeled 0001 0010 0111 and 1011. The\nlabels indicate the passages that connect the rooms. For example door 0001 connects room 00 with\nroom 01.\n3.\nFor each door <D> and room <R> we generate the respective predicate names as listed in the\n:predicates section of the domain deﬁnition.\n4.\nFor each door and room combination we generate the action deﬁnitions indicated in the listing\nbelow, such that the connections of doors and rooms are appropriate. For example we generate\nan action deﬁnition move to room center 00 from door 0001 because it is possible to\nmove from door 0001 to the center of room 00. However, we do not generate the action\nmove to room center 00 from door 0111, because door 0111 is not connected to room\n00.\nListing 3. Ant navigation\n(define (domain ant)\n(:requirements :strips)\n(:predicates\nThis is a provisional ﬁle, not the ﬁnal typeset article\n25\nEppe et al.\nFrom semantics to execution\nat_door_<D>\n; whether the agent is at a door\nat_room_center_<R>\n; whether the agent is at a room-center\nat_target\n; whether the agent is at the target\nin_room_<R>\n; whether the agent is inside a room\ntarget_in_room_<R>\n; whether the target is inside the room\n)\n; Move from room center of <R> to door <D>\n; <D1> != <D> is the other door that is adjacent to <R>\n(:action move_to_room_center_<R>_from_door_<D>\n:precondition (at_door <D>)\n:effect (and\n(at_room_center <R>)\n(not (at_door <D1>)) )\n)\n; Move to a door when inside a room that connects to the door\n(:action move_to_door_<D>_from_<R>\n:precondition (at_room_center_<R>)\n:effect (and (at_door_<D>) (not (at_room_center_<R>)) )\n)\n; Move to the room center of <R> if not at a door <D1> or <D2> of\nthat room\n(:action move_to_room_center_<R>\n:precondition (and (in_room_<R>)\n(not (at_door_<D1>)) (not (\nat_door_<D2>)))\n:effect (at_room_center_<R>)\n)\n; Move to the target within the room <R>\n(:action move_to_target_in_room_<R>\n:precondition (and (at_room_center_<R>)\n(target_in_room_<R>))\n:effect (and (at_target) (in_room_<R>)\n(not (at_room_center_<R>))\n)\n)\nThis is a provisional ﬁle, not the ﬁnal typeset article\n26\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.RO"
  ],
  "published": "2019-05-23",
  "updated": "2019-12-08"
}