{
  "id": "http://arxiv.org/abs/1905.10350v1",
  "title": "Unsupervised Community Detection with Modularity-Based Attention Model",
  "authors": [
    "Ivan Lobov",
    "Sergey Ivanov"
  ],
  "abstract": "In this paper we take a problem of unsupervised nodes clustering on graphs\nand show how recent advances in attention models can be applied successfully in\na \"hard\" regime of the problem. We propose an unsupervised algorithm that\nencodes Bethe Hessian embeddings by optimizing soft modularity loss and argue\nthat our model is competitive to both classical and Graph Neural Network (GNN)\nmodels while it can be trained on a single graph.",
  "text": "arXiv:1905.10350v1  [cs.SI]  20 May 2019\nPublished as a workshop paper at ICLR 2019\nUNSUPERVISED COMMUNITY DETECTION WITH\nMODULARITY-BASED ATTENTION MODEL\nIvan Lobov, Sergey Ivanov\nCriteo\nParis, France\n{i.lobov,s.ivanov}@criteo.com\nABSTRACT\nIn this paper we take a problem of unsupervised nodes clustering on graphs and\nshow how recent advances in attention models can be applied successfully in a\n”hard” regime of the problem. We propose an unsupervised algorithm that en-\ncodes Bethe Hessian embeddings by optimizing soft modularity loss and argue\nthat our model is competitive to both classical and Graph Neural Network (GNN)\nmodels while it can be trained on a single graph.\n1\nINTRODUCTION\nCommunity detection on graphs is a task of learning similar classes of vertices from the network’s\ntopology. It is one of the central problems in data mining and has found numerous applications in\nsociological studies (Goldenberg et al. (2010)), DNA 3D folding (Cabreros et al. (2016)), product\nrecommendations (Clauset et al. (2004)), natural language processing (Ball et al. (2011)) and more.\nIt is not surprising that many fundamental results have been obtained in recent years that shed the\nlight on our understanding of the solvability of this problem in general. Speciﬁcally, precise phase\ntransition and fundamental limits exist for one of the central graph generation models to test commu-\nnity detection algorithms, the so-called Stochastic Block Model SBM(n, p, W) and its symmetric\nvariant Symmetric Stochastic Block Model, SSBM(n, k, A, B) (Abbe (2018)). In this paper, we’ll\nbe dealing with a simpler case, SSBM.\nThe optimization of log-likelihood and modularity are equivalent in SSBM model (Newman, 2013).\nHence, we propose to use a generalization of modularity that outputs the probability of each cluster\nfor a node and is therefore differentiable for a neural network model. Our model is based on the\nrecent advances in NLP where Transformer model (Vaswani et al. (2017)) with attention mechanism\nshows superior results. We adopt the encoder part of Transformer to transform initially obtained\nBethe Hessian embeddings and then produce the probability of each cluster for each node to optimize\nwith our loss function.\nOur contributions include:\n• A new model with attention mechanism that optimizes a soft modularity loss function (and\nrelease the code to reproduce our results1).\n• A comparative study of classical algorithms and recent supervised graph neural network\nand outline several advantages of the unsupervised model.\n2\nRELATED WORK\nSBM models have seen a resurged interest recently due to the conjecture of (Decelle et al. (2011))\nthat hypotheses phase transition for weak recovery case (the one we are interested in this pa-\nper) and the existence of the information-computation gap for 4 communities in the symmetric\ncase. It was proved in (Massouli´e (2014); Bordenave et al. (2015); Mossel et al. (2018)) that efﬁ-\ncient algorithms exists when there are 2 communities and signal-to-noise ratio is greater than one\n1https://github.com/Ivanopolo/modnet\n1\nPublished as a workshop paper at ICLR 2019\n(KS threshold), while (Mossel et al. (2015)) shows that it is impossible to detect communities be-\nlow this threshold for 2-community case. For more than 3 communities it was also proved that\nthere are non-efﬁcient algorithms that can weakly recover communities strictly below KS threshold\n(Abbe & Sandon (2015)). An extensive overview of the area can be found in Abbe (2018).\nGraph neural networks have been recently applied to solve community detection problem with the\ncurrent state-of-the-art LGNN model(Chen et al. (2019)) designed to extract high-order node inter-\nactions via Non-backtracking operator. In parallel line of work, there are graph neural models based\non attention mechanisms (Veliˇckovi´c et al. (2018); Kool et al. (2019b)) that have showed prominent\nresults in other domains such as speech recognition and NLP.\n3\nSSBM REGIMES\nSignal-to-Noise Ratio (SNR) for SSBM(n, k, a/n, b/n) model is deﬁned as:\nSNR =\n(a −b)2\nk(a + (k −1)b)\n(1)\nIt is known that for k ≥2 and SNR > 1 it is possible to detect communities in polynomial-time\n(Abbe & Sandon (2016)). Moreover, when SNR ≤1 and k = 2 the weak recovery of SSBM model\nis not possible theoretically. However, it is also known that for k ≥4 it is possible to weakly recover\nin the setup of SNR > α, where α < 1. This means that there exists a computational threshold after\nwhich only information-theoretical algorithms can recover communities. This gap between what\nis possible to compute efﬁciently and what can be computed in general is known as information-\ncomputation gap. In this work we compare two different regimes: one regime that lies within a\ncomputation threshold and is achievable by classical algorithms (associative case) and one regime\nfor the information-theoretical gap (disassociative case).\n4\nAPPROACH\n4.1\nLOSS\nIf we know the generative model from which a graph is produced, it is possible to make statistical\ninference and evaluate the ﬁtness of the estimated parameters using model’s log likelihood. This\napproach assumes a hard labelling of the nodes and therefore requires some form of relaxation to\nobtain a differentiable loss function. It has been shown in (Newman, 2013) that optimizing log-\nlikelihood for the SSBM model is equivalent to the modularity optimization, a popular heuristic\napproach to graph clustering. And there exists (Havens et al., 2013) a generalization of modularity\nto soft labelling which is well differentiable and can be directly used for learning.\nWe take a soft modularity M as our loss function:\nM = −tr(UBU T )/||A||1\nWhere U ∈RN×C is a matrix of probabilities of nodes attribution to clusters (the number of which\nis a model hyperparameter), B = A −ddT /||A||1, d is a vector of node degrees, A is the adjacency\nmatrix and ||A||1 = P |Aij|.\nAs the modularity might have multiple local optima, we found that it is beneﬁcial if we can include\nadditional information about the graph as a prior to steer the model in the right direction. We use a\nregularizer deﬁned by:\nR =\n\n\n\nC\nX\ni\n\n\nN\nX\nj\nUij −1\nC\n\n\n2\n\n\nWhere C is the number of communities in the graph. The regularizer forces the model to ﬁnd\ncommunities of similar sizes as is expected in SSBM.\nOur ﬁnal loss is:\n\u001aLa = M + λR,\nfor associative communities\nLd = −M + λR,\nfor disassociative communities.\n(2)\n2\nPublished as a workshop paper at ICLR 2019\nWe use negation of modularity loss M as modularity is negative for disassociative case. The beneﬁts\nof using this loss function are the following: we do not require explicit labels for learning, the loss\nis invariant under labels permutation and it is a consistent loss under SSBM with soft community\npartitioning.\n4.2\nMODEL\nOur network takes as input initialized node embeddings and consists of three main blocks: projection\ninto higher dimensions (done via a fully connected network with skip connections (He et al., 2016)\nand batch norm (Ioffe & Szegedy, 2015)), graph attention block and output prediction layer with a\nfully connected layer and a softmax.\nNode Embeddings\nWe initialize node embeddings with eigenvectors recovered as part of the\nBathe Hessian decomposition, a baseline algorithm described in Section 5.3.\nGraph Attention\nAt every layer the attention is done over all neighbors of the node. The Encoder\nmodule is described in (Kool et al., 2019a) which is a form of multi-headed attention on neighbors\nof a node. It is particularly well-suited for our initialization of the node embeddings; the key-value\ndot-products give the model access to the community structure estimated by the Bethe Hessian\neigenvectors.\n4.3\nTRAINING\nOur model is an unsupervised model in a sense that it does not require any labels from the original\ngraph. We train the model separately on each graph repeatedly trying to improve the soft modularity\nloss. Since the method does not require a larger training set, it signiﬁcantly reduces the training\ntime.\nFor all the experiments reported, we trained a model with 2 layers, 3 heads of attention, the size of\nall hidden layers equal to 48 and regularization parameter λ = 0.5.\n5\nEXPERIMENTS\n5.1\nDATASETS\nWe generate two datasets SBM(n, k, a/n, b/n) that correspond to associative and disassociative\ncases. For the disassociative dataset, we set a = 0 and b = 18, which means that there are no\nlinks between vertices of the same cluster. For the associative dataset, we set a = 21 and b = 2.\nAll datasets have n = 400 vertices and k = 5 communities. The parameters for the disassociative\ncase correspond to the information-theoretical gap, while for associative case they correspond to the\nregime when belief propagation can weakly recover the clustering efﬁciently. We generate 6000\ngraphs for training dataset and 1000 for validation.\n5.2\nEVALUATION METRICS\nOverlap measures the intersection of the original assignment y and the predicted assignment ˆy\nacross all possible permutations from the permutation group S (Decelle et al. (2011)):\nO(y, ˆy) = max\nπ∈Sˆ\ny\n(1/n) P\nu\nδy(u),π(ˆy(u)) −1/c\n1 −1/c\nWhere n is the number of nodes in a graph, c is the number of communities and δu,v equals one\nwhen u = v and zero, otherwise.\nMutual information is a similarity measure between two labels assignments y and ˆy:\nI(y, ˆy) =\nn\nX\ni=1\nn\nX\nj=1\ny(i) ∩ˆy(j)\nn\nlog ny(i) ∩ˆy(j)\n|y(i)||ˆy(j)|\n(3)\n3\nPublished as a workshop paper at ICLR 2019\nWe use a normalized version (Normalized Mutual Information, hence NMI) of (3) by the arithmetic\nmean of the entropy for each assignment.\nModularity is a measure of ﬁtness of our inference assuming the SSBM generative model (it differs\nby a constant factor from the log likelihood). It is positive for associative communities and negative\nfor disassociative ones.\n5.3\nBASELINE METHODS\nWe compare our model with a state-of-the-art GNN model, LGNN, and several classical baselines.\nBethe Hessian, introduced in Saade et al. (2014), is a spectral approximation of Belief Propagation\n(BP), a classical algorithm for community detection, by employing the Bethe Hessian operator:\nH = (r2 −1)1 −rA + D,\nwhere r is the largest eigenvalue of non-backtracking operator B, A is the adjacency matrix and\nD is a diagonal matrix of degrees. We ﬁnd the k smallest algebraically eigenvectors of H and\nﬁnd clusters using k-means algorithm. We found that its results are more stable and often better in\npractice than of BP.\nLouvain Modularity (Blondel et al., 2008) is a popular baseline that greedily updates the clusters\nif the modularity is improved. Since it does not control the number of clusters and we often ended\nup with more clusters that in the original graph, we did not compute the overlap metric for it (which\nrequires the same number of communities). We do not report results for the disassociative case as\nthe algorithm cannot ﬁnd communities in this setting by design, it can only merge nodes that are\nneighbors.\nLGNN Chen et al. (2019) is a supervised community recovery algorithm based on GNNs and that\ncan learn operators of inter-graph node and edge connectivity similar to laplacian for nodes or non-\nbacktracking operator for edges.\nGNN\nKipf & Welling (2017) In addition to the attention model, we also experiment with GNN\narchitecture that produces network embeddings that are trained using our loss equation (2).\nFor our GNN and Attention models, we also have a choice of initialization of embeddings. We add\nresults for Random and Bethe-Hessian (BH) initialization.\n5.4\nEMPIRICAL RESULTS\nResults are presented in Tables 1 and 2. Note that in disassociative case, the smaller value for mod-\nularity, the better; for all other metrics, the higher, the better. True labels denote the optimal values\nfor the datasets. Among our 4 proposed algorithms (at the bottom), Attention model with Bethe-\nHessian initialization is superior, showing that both the structure of the model and the initialization\nof embeddings improve the quality on all metrics. Overall, we can see that our modularity-based\napproach achieves competitive results for the associative and disassociative dataset even compared\nwith supervised LGNN. Our model Attention-BH also achieves the top performance on modularity\nmetric among all algorithms as soft modularity is explicitly present in the loss function. We can\nsee that the loss function that we propose allows us to learn as good or even better ﬁt for the gen-\nerative model (SSBM), which shows that it can be efﬁciently used to ﬁnd communities in a fully\nunsupervised way, learning only on the current graph.\n6\nCONCLUSION\nIn this work we propose a novel approach on how to recover communities in unsupervised way\nand conduct experiments comparing to state-of-the-art supervised neural models and unsupervised\nclassical algorithms. Supervised models achieves the state-of-the-art results but with the price of\nhaving more parameters and longer training time. We believe it is interesting to combine LGNN\narchitecture with an unsupervised modularity loss as potential future work.\n4\nPublished as a workshop paper at ICLR 2019\nTable 1: Associative Dataset.\nAlgorithm\nModularity\nOverlap\nNMI\nTrue Labels\n0.52\n1.0\n1.0\nLGNN (supervised)\n0.50\n0.80\n0.67\nBethe Hessian\n0.52\n0.85\n0.69\nLouvain Modularity\n0.48\nN/A\n0.48\nGNN-Random\n0.47\n0.53\n0.48\nGNN-BH\n0.49\n0.66\n0.61\nAttention-Random\n0.42\n0.36\n0.27\nAttention-BH\n0.51\n0.78\n0.67\nTable 2: Disassociative Dataset.\nModularity\nOverlap\nNMI\n-0.20\n1.0\n1.0\n-0.16\n0.24\n0.13\n-0.15\n0.21\n0.11\nN/A\nN/A\nN/A\n0\n0.02\n0.01\n-0.16\n0.2\n0.1\n-0.17\n0.12\n0.04\n-0.18\n0.22\n0.11\nREFERENCES\nE. Abbe and C. Sandon. Crossing the ks threshold in the stochastic block model with information\ntheory. In 2016 IEEE International Symposium on Information Theory (ISIT), pp. 840–844, 2016.\nEmmanuel Abbe. Community detection and stochastic block models: Recent developments. Journal\nof Machine Learning Research, 18(177):1–86, 2018.\nEmmanuel Abbe and Colin Sandon. Detection in the stochastic block model with multiple clusters:\nproof of the achievability conjectures, acyclic bp, and the information-computation gap. CoRR,\nabs/1512.09080, 2015.\nBrian Ball, Brian Karrer, and M. E. J. Newman. An efﬁcient and principled method for detecting\ncommunities in networks. CoRR, abs/1104.3590, 2011.\nVincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. Fast unfolding\nof communities in large networks. Journal of statistical mechanics: theory and experiment, 2008\n(10):P10008, 2008.\nCharles Bordenave, Marc Lelarge, and Laurent Massoulie. Non-backtracking spectrum of random\ngraphs: Community detection and non-regular ramanujan graphs. In Proceedings of the 2015\nIEEE 56th Annual Symposium on Foundations of Computer Science (FOCS), FOCS ’15, 2015.\nI. Cabreros, E. Abbe, and A. Tsirigos. Detecting community structures in hi-c genomic data. In\n2016 Annual Conference on Information Science and Systems (CISS), pp. 584–589, 2016.\nZhengdao Chen, Lisha Li, and Joan Bruna.\nSupervised community detection with line graph\nneural networks.\nIn International Conference on Learning Representations, 2019.\nURL\nhttps://openreview.net/forum?id=H1g0Z3A9Fm.\nAaron Clauset, M. E. J. Newman, , and Cristopher Moore. Finding community structure in very\nlarge networks. Physical Review E, 2004.\nAurelien Decelle, Florent Krzakala, Cristopher Moore, and Lenka Zdeborova. Phase transition in\nthe detection of modules in sparse networks. 2011.\nAnna Goldenberg, Alice X. Zheng, Stephen E. Fienberg, and Edoardo M. Airoldi. A survey of\nstatistical network models. Found. Trends Mach. Learn., 2(2), 2010.\nTimothy C Havens, James C Bezdek, Christopher Leckie, Kotagiri Ramamohanarao, and Marimuthu\nPalaniswami. A soft modularity function for detecting fuzzy communities in social networks.\nIEEE Transactions on Fuzzy Systems, 21(6):1170–1175, 2013.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n770–778, 2016.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\n5\nPublished as a workshop paper at ICLR 2019\nThomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional net-\nworks. In International Conference on Learning Representations (ICLR), 2017.\nWouter Kool,\nHerke van Hoof,\nand Max Welling.\nAttention,\nlearn to solve routing\nproblems!\nIn International Conference on Learning Representations, 2019a.\nURL\nhttps://openreview.net/forum?id=ByxBFsRqYm.\nWouter Kool,\nHerke van Hoof,\nand Max Welling.\nAttention,\nlearn to solve routing\nproblems!\nIn International Conference on Learning Representations, 2019b.\nURL\nhttps://openreview.net/forum?id=ByxBFsRqYm.\nLaurent Massouli´e. Community detection thresholds and the weak ramanujan property. In Proceed-\nings of the Forty-sixth Annual ACM Symposium on Theory of Computing, STOC ’14, 2014.\nElchanan Mossel, Joe Neeman, and Allan Sly. Reconstruction and estimation in the planted partition\nmodel. Probability Theory and Related Fields, 2015.\nElchanan Mossel, Joe Neeman, and Allan Sly. A proof of the block model threshold conjecture.\nCombinatorica, 2018.\nMark EJ Newman. Spectral methods for community detection and graph partitioning. Physical\nReview E, 88(4):042822, 2013.\nAlaa Saade, Florent Krzakala, and Lenka Zdeborov´a. Spectral clustering of graphs with the bethe\nhessian. In Proceedings of the 27th International Conference on Neural Information Processing\nSystems - Volume 1, NIPS’14, 2014.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-\nmation Processing Systems, pp. 5998–6008, 2017.\nPetar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua\nBengio.\nGraph Attention Networks.\nInternational Conference on Learning Representations,\n2018.\n6\n",
  "categories": [
    "cs.SI",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-05-20",
  "updated": "2019-05-20"
}