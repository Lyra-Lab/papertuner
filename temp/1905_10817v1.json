{
  "id": "http://arxiv.org/abs/1905.10817v1",
  "title": "Deep Online Learning with Stochastic Constraints",
  "authors": [
    "Guy Uziel"
  ],
  "abstract": "Deep learning models are considered to be state-of-the-art in many offline\nmachine learning tasks. However, many of the techniques developed are not\nsuitable for online learning tasks. The problem of using deep learning models\nwith sequential data becomes even harder when several loss functions need to be\nconsidered simultaneously, as in many real-world applications. In this paper,\nwe, therefore, propose a novel online deep learning training procedure which\ncan be used regardless of the neural network's architecture, aiming to deal\nwith the multiple objectives case. We demonstrate and show the effectiveness of\nour algorithm on the Neyman-Pearson classification problem on several benchmark\ndatasets.",
  "text": "Deep Online Learning with Stochastic\nConstraints\nGuy Uziel\nDepartment of Computer Science\nTechnion - Israel Institute of Technology\nMay 28, 2019\nAbstract\nDeep learning models are considered to be state-of-the-art in many ofﬂine\nmachine learning tasks. However, many of the techniques developed are not\nsuitable for online learning tasks. The problem of using deep learning models\nwith sequential data becomes even harder when several loss functions need to be\nconsidered simultaneously, as in many real-world applications. In this paper, we,\ntherefore, propose a novel online deep learning training procedure which can be\nused regardless of the neural network’s architecture, aiming to deal with the multiple\nobjectives case. We demonstrate and show the effectiveness of our algorithm on the\nNeyman-Pearson classiﬁcation problem on several benchmark datasets.\n1\nIntroduction\nIn many real-world applications, one has to consider the minimization of several loss\nfunctions simultaneously, which is, of course, an impossible mission. Therefore, one\nobjective is chosen as the primary function to minimize, leaving the others to be bound\nby pre-deﬁned thresholds. For example, in online portfolio selection [5], the ultimate\ngoal is to maximize the wealth of the investor while keeping the risk bounded by a\nuser-deﬁned constant. In the Neyman-Pearson (NP) classiﬁcation (see, e.g., [22]), an\nextension of the classical binary classiﬁcation, the goal is to learn a classiﬁer achieving\nlow type-II error whose type-I error is kept below a given threshold. Another example\nis the online job scheduling in distributed data centers (see, e.g., [14]), in which a job\nrouter receives job tasks and schedules them to different servers to fulﬁll the service.\nEach server purchases power (within its capacity) from its zone market, used for serving\nthe assigned jobs. Electricity market prices can vary signiﬁcantly across time and zones,\nand the goal is to minimize the electricity cost subject to the constraint that incoming\njobs must be served in time.\nIt is indeed possible to adjust any training algorithms capable of dealing with one\nobjective loss to deal with multiple objectives by assigning a positive weight to each\nloss function. However, this modiﬁcation turns out to be a difﬁcult problem, especially\nin the case where one has to maintain the constraints below a given threshold online.\n1\narXiv:1905.10817v1  [cs.LG]  26 May 2019\nRecently, several papers have put their focus on dealing with the multiple objective\ncases in the online setting. In the adversarial setting, it is known that multiple-objective\nis generally impossible when the constraints are unknown a-priory [21]. In the stochastic\nsetting, [20] proposed a framework for dealing with multiple objectives in the i.i.d. case\nand [26] has extended the above to the case where the underlying process is stationary\nand ergodic.\nThe previous approaches, however, focused mainly on the online training of shallow\nmodels in this context. Therefore, the main goal of this paper is to propose an online\ntraining procedure capable of controlling several objectives simultaneously utilizing\ndeep learning models, which have witnessed tremendous success in a wide range of\nofﬂine (batch) machine learning tasks applications [17, 4, 12, 1].\nThe training of deep neural networks is considered to be hard due to many challenges\narising during the process. For example, vanishing gradient, diminishing feature reuse\n[12], saddle points, local minima [8, 9], difﬁculties in choosing a good regularizer\nchoosing hyperparameters.\nThose challenges are even harder to tackle in the online regime, where the data is\ngiven sequentially, and because the data might not be stationary and exhibit different\ndistributions at different periods (concept drift)[10]. Despite the challenges, having\na way to learn a deep neural network in an online fashion can lead to more scalable,\nmemory efﬁcient, and better performance of deep learning models in online learning\ntasks.\nThere are several approaches on how to train deep learning models in the online\nsetting; ﬁrst, we have the naïve approach: directly applying a standard Back-propagation\ntraining on a single instance at each online round. The main critical problem in this\napproach is the lack of ability to tune the model (e.g., choosing the right architecture\nand the hyper-parameters) in online-manner and in the case of composed loss to tune\nthe weights on each of the losses. Since, in the online case, we remind, validation sets\ndo not exist.\nA different approach, which is used mainly in non-stationary data streams such\nas online portfolio selection, is to set two sliding windows, one for training and the\nother one for testing (see, e.g., [15]). After training the model and testing it, the\nsliding window is moved to the next period and trained all over again. It relays on the\nassumption that close data points should exhibit the same distribution. This method\nsuffers from a signiﬁcant reduction of the training data, thus, pruned to overﬁtting and\nis less suitable for cases where real-time prediction.\nAnother approach exploits the principle of \"shallow to deep.\" This is based upon the\nobservation that shallow models converge faster than deeper ones. This approach has\nbeen exploited in the ofﬂine batch setting, for example, using the function preservation\nprinciple [11, 16], and modifying the network architecture and objective functions, e.g.,\nHighway Nets[25]. Those methods which exhibit improve convergence for the ofﬂine\nsetting turned, as demonstrated by [24], not to be suitable for the online setting as the\ninference is made by the deepest layer which requires substantial time for convergence.\nDeeply Supervised Nets [18] are another way to implement the above principle. This\nmethod incorporates companion objectives at every layer, thus addressing the vanishing\ngradient and enabling the learning of more discriminative features at shallow layers.\n[24] recently adjusted this approach to the online setting by setting weights on those\n2\nlayers and optimizing the weights in an online manner using the hedge algorithm.\nIn this paper, we follow the latter approach, as well. By modifying a given network’s\narchitecture, we modify the multi-objective problem into a convex one. Thus, allowing\nthe use of the strong duality theorem. By doing so, we transform the problem into a\nminimax optimization problem by introducing the corresponding Lagrangian. Afterward,\nby a three-step optimization process which is responsible both for optimizing the primal\nand the dual variables and the weight’s of the networks themselves, we can perform as\nwell as the best layer in hindsight while keeping the constraints bounded below a given\nthreshold. While we discuss here the case of only two objective functions, our approach\ncan be extended easily to any arbitrary number of objective functions.\nThe paper is organized as follows: In Section 2, we deﬁne the multi-objective online\noptimization framework. In Section 3, we present our deep training algorithm and prove\nthat under mild conditions, it produces predictions for which the constraints hold. In\nSection 4, we demonstrate our algorithm on 3 public benchmark datasets aiming to\ncontrol the type-I error.\n2\nProblem Formulation\nWe consider the following prediction game. At the beginning of each round, t = 1, 2, . . .,\nthe player receives an observation xt ∈X ⊂Rd generated from an i.i.d. process. The\nplayer is required to make a prediction bt ∈B, where B ⊂Rm is a compact and convex\nset, based on past observations. After making the prediction bt, the label yt ∈Y ⊂Rd\nis revealed and the player suffers two losses, u(bt, yt) and c(bt, yt), where u and c are\nreal-valued continuous functions and convex w.r.t. their ﬁrst argument.\nThe player is using a deep neural network, Sw : X →B, parameterized with\nw ∈RW . We view the player’s prediction strategy as a sequence S ≜{Swt}∞\nt=1 of\nforecasting functions Swt : X →Y; that is, the player’s prediction at round t is given\nby Swt(xt).\nThe player is required therefore to play the game with a strategy that minimizes the\naverage u-loss, 1\nT\nPT\nt=1 u(Swt(xt), yt), while keeping the average c-loss 1\nT\nPT\nt=1 c(Swt(xt), yt)\nbounded below a prescribed threshold γ.\nAs in typical online learning problems [13, 6], the goal of the learner is to perform\nas good as w∗satisfying the following optimization problem:\nminimize\nw∈RW\nE [u(Sw(x), y)]\nsubject to\nE [c(Sw(x), y)] ≤γ,\n(1)\nSince the player is using a neural network, it is probable to assume that Problem 1 is\nnot convex. Therefore, as in [24, 19] we modify the network’s architecture as follows:\nwe denote the network’s hidden layers by l1, . . . , lL (lL is the output layer of Sw), and\nattach an output layer to each one of them, resulting in L predictions at each round,\nSt(xt) ≜\n\u0000S1\nwt(xt), . . . , SL\nwt(xt)\n\u0001\n.\nIn other words, every output layer can be regarded as an expert and can be seen as a\nstrategy on its own. Therefore, by assigning a probability weight to the experts p ∈∆L,\nwhere ∆L is the L-dimensional probability simplex we can modify the problem into the\n3\nfollowing one:1\nminimize\np∈∆L\nE [u(⟨p, Si(xt)⟩, yt)]\nsubject to\nE [c(⟨p, St(xt)⟩, yt)] ≤γ.\n(2)\nNote that Problem 2 is now a convex minimization problem over ∆L. However,\nthere is no guarantee that a feasible solution even exists. In many real-world problems,\none can come up with simple experts satisfying the constraints. For example, in the\nNeyman-Pearson classiﬁcation, one can use the strategy of always predicting the 0-label\nclass, resulting in 0 type-I error, and thus satisfying any (reasonable) constraint on\nthe type-I error. Another example is in the online portfolio selection, where adding a\nstrategy of investing only in cash results in zero risk trading. Thus, we assume that there\nexists such a simple expert, denoted by S0, satisfying the constraints and we artiﬁcially\nadd him to the problem (resulting in that now the problem is minimized over ∆L+1).\nBy the addition of this expert, we require that the player minimizes the main loss,\nwhile the average violation of the constraint is bounded as follows:\n1\nT\nT\nX\nt=1\nc(⟨pt, St(xt)⟩, yt) ≤γ + O( 1\nT ),\nMoreover, now Slater’s condition holds, and the problem is equivalent to ﬁnding the\nsaddle point of the Lagrangian function [3], namely,\nmin\np∈∆L+1 max\nλ∈R+ L(p, λ),\nwhere the Lagrangian is\nL(p, λ) ≜E [u(⟨p, St(xt)⟩, yt)] + λ (E [c(⟨p, St(xt)⟩, yt) −γ)] .\nWe denote the optimal dual by λ∗. Moreover, we set a constant2 λmax ≥1 such that\nλmax > λ∗, and set Λ ≜[0, λmax]. We also deﬁne the instantaneous Lagrangian\nfunction as\nl(p, λ, yt) ≜u(⟨p, St(xt)⟩, yt) + λ (c(⟨p, St(xt)⟩, yt) −γ) .\n(3)\nSummarizing the above, regardless of the neural network’s architecture and on how\nit is trained, we were able to turn the optimization problem into a convex one, and by\nusing the strong duality theorem, we turned the multi-objective problem into a minimax\nproblem.\nIn the next section, we present our algorithm, designed to jointly ﬁnd the minimax\npoint between the experts and optimizing the network’s layers.\n3\nDeep Minimax Exponentiated Gradient\n1The construction above do not depend on the optimization on the weights\n2This can be done, for example, by imposing some regularity conditions on the objectives (see, e.g., [20]).\n4\nAlgorithm 1 Deep Minimax Exponentiated Gradient (DMEG)\n1: Parameters: η, ηλ > 0:\n2: Initialize: λ0 = 0, p0 = (\n1\nL+1, ...,\n1\nL+1), ˆp0 = (\n1\nL+1, ...,\n1\nL+1).\n3: for each period t do\n4:\nReceive context xt\n5:\nCompute predictions St(xt) of the experts\n6:\nPlay (pt, λt)\n7:\nSuffer loss l(⟨pt, St(xt)⟩, λt, yt)\n8:\nUpdate experts’ weights:\nˆpt+1,k ←ˆpt,k exp\n \n−η\nt\nX\ni=1\n∇kl(⟨pt, St(xt)⟩, λi, yi)\n!\nk = 1, . . . , L + 1\npt+1 ←\nˆpt+1\n∥ˆpt+1∥1\n9:\nUpdate λ:\nλt+1 ←λmax\nexp\n\u0010\nηλ Pt\ni=1 ∇λl(⟨pt, St(xt)⟩, λi, yi)\n\u0011\n1 + exp\n\u0010\nηλ Pt\ni=1 ∇λl(⟨pt, St(xt)⟩, λi, yt)\n\u0011\n10:\nUpdate wt+1:\nwt+1 ←Backprop(l(⟨pt, St(xt)⟩, λt, yt), wt)\n11: end for\nWe now turn to present our algorithm Deep Minimax Exponentiated Gradient\n(DMEG), designed for jointly optimizing the network’s parameters, w, and tuning the\nminimax weights between the different experts, p and λ. The algorithm is outlined\nat Algorithm 1; at the beginning of each round the algorithm receives an observation,\nand the corresponding predictions of the different experts and predicts pt, λt (lines\n4-5). Afterwards, the label yt is revealed and the algorithm suffers the instanious\nLagrangian loss (lines 6-7). Then after receiving the loss, the algorithm preforms a\nthree-step optimization where the ﬁrst and second step (lines 8 and 9 respectively) are\nin place to ensure converges to the minimax solution of Problem 2. The last step is the\nbackpropagation step whose role is to update the weights of the neural network, and\nthus improving the performance of the experts (line 10). The optimization steps are\ndescribed in detail below:\nUpdating the experts’ weights\nUsing the \"shallow to deeper\" observation discussed\nbefore, the shallow experts may converge faster and exhibit better results than the\ndeeper layers at early stages, and at later stages, deeper experts might exhibit the better\n5\nresults. Thus, we aggregate the experts’ prediction using the well-known expert learning\nalgorithm Exponentiated Gradients (EG) [13], this algorithm punishes the weights of\nthe experts in a proportional way to the loss they incur during the training period.\nUpdating λ\nSince we are looking for a minimax solution, we need to update the\nlambda parameter alternately, and after the update of p, we update lambda as well. The\nupdates are in place to ensure the maximization of the Lagrangian. This is done using\nagain the EG algorithm applied over two static experts, one predicting 0 and the other\none predicting λmax.\nUpdating the neural network\nIndependently of the two above stages, we optimize\nthe prediction of the neural network based on previous weights and lambda, and thus,\nwe perform a back propagation step using the weighted Lagrangian. The optimization\ncan be done using any traditional deep learning optimizer.\nThe prediction of the network is a weighted average of all the predictions of the\ndifferent experts ⟨pt, St(xt)⟩. Therefore, the total loss of the network which is used for\nthe backpropagation step is given by\nl(⟨pt, St(xt)⟩, λt, yt).\nThe advantages of such integrated optimization are threefold: ﬁrst, as we prove in\nthe next subsection, it ensures that the predictions of the network satisfy the constraints.\nSecond, the EG algorithm between the experts tunes the model towards the best perform-\ning expert and third by the unique structure of the network the layers share information\nbetween them.\n3.1\nTheoretical guarantee\nIn this subsection, we state and prove that our procedure which aggregates between the\ndifferent predictors ensures that the predictions generated by DMEG the violations of\nthe constraints will be bounded. We now state and prove our results; we note that we do\nnot assume anything regarding how (xt, yt) are generated. Since we always can scale a\nbounded loss function, we will assume that l takes values in [0, 1]\nTheorem 1. Let b1, . . . , bT , where bt = ⟨pt, St(xt)⟩be the predictions generated by\nDMEG when applied over an arbitrary neural network Sw. Denote G1,G2 > 0 to be\nbounds on the derivatives w.r.t. the ﬁrst and the second argument of l respectively, then\nif we set η =\n1\nG1\nq\nlog L+1\nT\nand ηλ =\n1\nG2\nq\nlog 2\nT\nthen the following holds:\n1\nT\nT\nX\nt=1\nc(bt, yt) ≤γ + 4 max(G1, G2)\nr\nlog (L + 1)\nT\n.\n(4)\nProof. Using the guarantees of the EG algorithm [13] we get that for every p ∈∆L+1:\n1\nT\nT\nX\nt=1\nl(bt, λt, yt) −1\nT\nT\nX\nt=1\nl(⟨p, St(xt)⟩, λt, yt) ≤2G1\nr\nlog (L + 1)\nT\n.\n(5)\n6\nUsing the guarantees of EG applied over the lambda updates we get that for every\nλ ∈Λ\n1\nT\nT\nX\nt=1\nl(bt, λ, yt) −1\nT\nT\nX\nt=1\nl(bt, λt, yt) ≤2G2\nr\nlog (2)\nT\n.\n(6)\nSumming Equations (5)+(6) we get that for every choice of (p, λ) ∈∆L+1XΛ.\n1\nT\nT\nX\nt=1\nl(bt, λ, yt) −1\nT\nT\nX\nt=1\nl(⟨p, St(xt)⟩, λt, yt) ≤4 max(G1, G2)\nr\nlog (L + 1)\nT\n.\n(7)\nNow, If we set λ = 1, and choose p such that all the probability mass is on the\nartiﬁcial expert then by using again Equation (7) we get the following\n1\nT\nT\nX\ni=1\nc(bi, xi) −1\nT\nT\nX\ni=1\nc(⟨p, St(xt)⟩, xi) ≤4 max(G1, G2)\nr\nlog (L + 1)\nT\n,\n(8)\nusing the guarantees of the artiﬁcial expert we get that Equation (4) holds.\n4\nEmpirical Results\nIn this section, we demonstrate the ability of DMEG to online learn deep neural\nnetworks while controlling a given constraint; we experiment with our algorithm on the\nNeyman-Pearson classiﬁcation deﬁned below.\n4.1\nNeyman-Pearson classiﬁcation\nIn the classical binary classiﬁcation, the learner’s goal is to minimize the classiﬁcation\nerror. This measure does not distinguish between type-I and type-II errors. The Neyman-\nPearson (NP) paradigm, is an extension of the classical binary classiﬁcation in the sense\nthat now the learner minimizes the type-II error while upper bounding the type-I error, in\nother words, the NP classiﬁcation is suitable for the cases when one error is more pricey\nthan the other. For instance, failing to detect a tumor has far more severe consequences\nthan wrongly classifying a healthy area; other examples include spam ﬁltering, machine\nmonitoring. Therefore, it is needed to put the priority on controlling the false negative\nrate, and for a given bound α on the type-I error. For a classiﬁer h ∈H, whose (discrete)\npredictions are denoted by ˆh(x) we deﬁne the NP problem as follows:\nminimize\nh∈H\nE\nh\nˆh(x) ̸= y | y = 0\ni\nsubject to\nE\nh\nˆh(x) ̸= y | y = 1\ni\n≤γ,\n(9)\nAs in classical classiﬁcation, one approach to solve the Neyman-Pearson classiﬁ-\ncation is by using convex surrogate losses (see, e.g., [23]). Therefore, for a convex\nsurrogate loss function R(·, ·) we get the following convex problem:\nminimize\nh∈H\nE [R(h(x), y) | y = 0]\nsubject to\nE [R(h(x), y) | y = 1] ≤γ,\n(10)\n7\nTable 1: Properties of the datasets\nDATASET\nLENGTH\nFEATURES\nTYPE\nSUSY\n5M\n28\nSTATIONARY\nHIGGS\n5M\n18\nSTATIONARY\nCD\n4.5M\n50\nCONCEPT DRIFT\nDuring our experiments we used the binary cross entropy to serve as the surrogate\nloss function.\nWe used the following binary classiﬁcation datasets, all are available in the public\ndomain.\nSusy\nSusy problem actualizes a big streaming data problem consisting of 5 million\nrecords. This dataset was produced by Monte-Carlo simulations and is the classiﬁcation\nof signal processes generating super-symmetric particles [2]. Each instance is repre-\nsented by 18 input attributes where the ﬁrst eight input attributes present the kinematic\nproperties while the remainders are simply the function of the ﬁrst eight input features.\nHiggs\nThis classiﬁcation problem aims to distinguish between a signal process which\nproduces Higgs bosons and a background process which does not. The features contain\nkinematic properties measured by the particle detectors in the accelerator and high-level\nfeatures derived by physicists to help discriminate between the two classes [2].\nCD\nThe concept drift dataset [24] contains three different segments, each comprising a\nthird of the data. All the three segments were generated from an 8-hidden layer network;\nthis challenging concept drift dataset is a part of a broader machine learning task aiming\nto deal with concept drift.\nThe properties of these public datasets are summarized in Table 1.\nFigure 1: Trade-off between the type-II (Y-axis) error and type-I (X-axis)\nCD\nSusy\nHiggs\n8\nFigure 2: The average constraint for DMEG with γ = 0.21\nCD\nSusy\nHiggs\nTable 2: Type-I and the Type-II (in parentheses) of DMEG with γ = 0.2\nBL\nHBP\nMOL\nBE\nDMEG\nSUSY\n.121 (.09)\n.11 (.093)\n.081 (.163) .071 (.136) .073 (.14)\nHIGGS .172 (.155) .161 (.152) .097 (.326) .090 (.215) .091 (.218)\nCD\n.217 (.208) .194 (.219) .096 (.370) .051 (.352) .052 (.353)\n4.2\nImplementation and results\nTo apply DMEG strategy, we used a 20 layer DNN, as in [24], with 100 units in each\nhidden layer and with ReLU Activation. For each one of the 19 hidden layers layer a\nclasiﬁier (a fully-connected layer followed by softmax activation) was attached, resulting\nin 19 classiﬁers (experts), each with depth ranging from 2, . . . , 20. The main objective\nof our experiments is to examine how well DMEG maintains the type-I error constrains.\nThe second objective is to examine the ability of DMEG to track the best expert.\nThe inclusion of the artiﬁcial expert is rather for theoretical purposes, in order to\nensure that Slater’s condition holds for Problem 2. In our experiments, however, we\ndid not add this expert at all, and as we present later on we were still able to fulﬁll the\nconstraints.\nFor the implementation of DMEG, we set η = 0.01 and ηλ = 0.01 without prior\ntuning. The network was trained using a ﬁxed learning rate during all the training\nrounds and across all the experiments and set to 0.001, the optimization was done using\nNesterov accelerated gradient.\nWe used the following baselines:\n• BL - the best result obtained by running instances of a fully connected DNNs\nwith layers ranging from (2,3,4,8,16) and with the same parameters as ours, all\nwere trained using a learning rate of 0.01 Nesterov accelerated gradient.\n• HBP - an algorithm designed for the deep online learning setting, with the same\narchitecture and hyper-parameters as described in [24].\n9\n• MOL - an implementation of a shallow online multi-objective model with param-\neters η = 0.01, ηλ = 0.01 [20].\n• BE - the best performing expert of our model. The expert with the lowest Type-II\nerror among the experts who satisﬁed the constraints.\nAll the experiments were implemented using Keras [7].\nIn Table 2, we present the type-I error and the type-II error (in parentheses) of\nthe different algorithms where for DMEG we set γ = 0.2. It is the place to note\nthat since we have convexiﬁed the NP classiﬁcation problem, we expect to get type-I\nerror lower than γ. First, as we can observe, all the existing methods of training deep\nneural networks online, training algorithms cannot take into consideration the constraint.\nSecond, we can observe that our algorithm, as expected, can maintain the constraint\nacross all the datasets. Moreover, by comparing the results of BE, the best expert of\nour model, we can see that we performed nearly as the best expert both in terms of\ntype-I error and type-II error, emphasizing the ability of our algorithm can track the\nbest performing layer. Together with the superior performance over BL, proves the\nusefulness of DMEG in tuning the model for the problem. The inferior performance\nof MOL comparing to DMEG, manifesting the need in training deep online models\nwith constraints.\nIn another set of experiments, we checked how well our procedure could control\nthe Type-I error, when γ is changed. Therefore for each dataset we run 6 instances of\nDMEG with different values of γ ∈{0.15, 0.18, 0.21, 0.24, 0.27, 0.3}. The results\nof this are presented at Figure 1. First, we can observe that DMEG successfully\nmaintained the constraints. Second, we observe the inevitable tradeoff between the\ntype-I and the Type-II error, which forms the shape of a concave Pareto-frontier. Table 3\nshows the average type-I error of all those instances across all the datasets and over\ndifferent duration in the training period.\nFigure 2 presents the average constraint function across the 3 datasets, for γ = 0.21.\nAs we can see, our algorithm well maintains this constraint across all the datasets during\nthe training period.\n5\nconclusions\nTraining and utilizing deep neural networks in an online learning setting is a challenging\ntask, especially with the need to consider multiple objectives simultaneously. Therefore\nin this paper, we presented DMEG, a novel approach to training a neural network\non the ﬂy while considering several objectives. Due to the non-convex nature of deep\nneural networks, we modiﬁed the problem in order to utilize the strong duality theorem\nand the Lagrangian relaxation. We also proved and demonstrated that our algorithm is\ncapable of controlling given constraints on several datasets.\nFor future research, we wish to investigate further ways to train a neural network in\nthe online setting. By bridging the gap between the approach, we took in this paper and\ntraining on the existing dataset (as done in the sliding window approach). On the one\nhand, the on-the-ﬂy approach presented here gives us the ability the train models in an\nefﬁcient way but in the cost of not fully optimizing the network and utilizing the data\n10\nTable 3: Average type-I error of DMEG with different values of γ and at different\nstages\nSTAGE\nDATASET\nDMEG.15\nDMEG.18\nDMEG.21\nDMEG.24\nDMEG.27\nDMEG.3\nSUSY\n0.044\n0.057\n0.069\n0.087\n0.101\n0.122\n0% −25%\nHIGGS\n0.072\n0.094\n0.097\n0.126\n0.151\n0.178\nCD\n0.051\n0.059\n0.071\n0.105\n0.138\n0.182\nSUSY\n0.043\n0.058\n0.073\n0.092\n0.105\n0.127\n25% −50%\nHIGGS\n0.066\n0.080\n0.087\n0.119\n0.144\n0.162\nCD\n0.055\n0.058\n0.072\n0.103\n0.127\n0.153\nSUSY\n0.042\n0.056\n0.072\n0.090\n0.102\n0.125\n50% −75%\nHIGGS\n0.062\n0.78\n0.085\n0.104\n0.141\n0.177\nCD\n0.048\n0.053\n0.068\n0.086\n0.106\n0.129\nSUSY\n0.044\n0.058\n0.071\n0.088\n0.101\n0.123\n75% −100%\nHIGGS\n0.064\n0.079\n0.096\n0.124\n0.150\n0.172\nCD\n0.030\n0.052\n0.067\n0.079\n0.103\n0.118\ncompare to the ﬁrst approach. Therefore we wish to investigate whether we can devise\nways to better trade-off between the two.\nReferences\n[1] Alex A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation with\ndeep convolutional neural networks. In Advances in neural information processing\nsystems, pages 1097–1105, 2012.\n[2] P. Baldi, P. Sadowski, and D. Whiteson. Searching for exotic particles in high-\nenergy physics with deep learning. Nature communications, 5:4308, 2014.\n[3] A. Ben-Tal and A. Nemirovsky. Optimization iii. Lecture Notes, 2012.\n[4] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and\nnew perspectives. IEEE transactions on pattern analysis and machine intelligence,\n35(8):1798–1828, 2013.\n[5] A. Borodin and R. El-Yaniv. Online Computation and Competitive Analysis.\nCambridge University Press, 2005.\n[6] N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge\nUniversity Press, 2006.\n[7] F. Chollet et al. Keras, 2015.\n11\n[8] A. Choromanska, M. Henaff, M. Mathieu, G. Arous, and Y. LeCun. The loss\nsurfaces of multilayer networks. In Artiﬁcial Intelligence and Statistics, pages\n192–204, 2015.\n[9] Y. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio. Iden-\ntifying and attacking the saddle point problem in high-dimensional non-convex\noptimization. In Advances in neural information processing systems, pages 2933–\n2941, 2014.\n[10] J. Gama, I. Žliobait˙e, A. Bifet, M. Pechenizkiy, and A. Bouchachia. A survey on\nconcept drift adaptation. ACM computing surveys (CSUR), 46(4):44, 2014.\n[11] I. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial\nexamples. arXiv preprint arXiv:1412.6572, 2014.\n[12] K . He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 770–778, 2016.\n[13] D.P. Helmbold, R.E. Schapire, Y. Singer, and M.K. Warmuth. On-line portfolio\nselection using multiplicative updates. Mathematical Finance, 8(4):325–347,\n1998.\n[14] C. Hung, L. Golubchik, and M. Yu.\nScheduling jobs across geo-distributed\ndatacenters. In Proceedings of the Sixth ACM Symposium on Cloud Computing,\npages 111–124. ACM, 2015.\n[15] Z. Jiang, D. Xu, and J. Liang. A deep reinforcement learning framework for the\nﬁnancial portfolio management problem. arXiv preprint arXiv:1706.10059, 2017.\n[16] P. Koh and P. Liang. Understanding black-box predictions via inﬂuence functions.\nIn Proceedings of the 34th International Conference on Machine Learning-Volume\n70, pages 1885–1894. JMLR. org, 2017.\n[17] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. nature, 521(7553):436, 2015.\n[18] C. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-supervised nets. arXiv\npreprint arXiv:1409.5185, 2014.\n[19] C. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-supervised nets. In\nArtiﬁcial Intelligence and Statistics, pages 562–570, 2015.\n[20] M. Mahdavi, T. Yang, and R. Jin. Stochastic convex optimization with multiple\nobjectives. In Advances in Neural Information Processing Systems, pages 1115–\n1123, 2013.\n[21] S. Mannor, J. Tsitsiklis, and J. Yu. Online learning with sample path constraints.\nJournal of Machine Learning Research, 10(Mar):569–590, 2009.\n[22] P. Rigollet and X. Tong. Neyman-pearson classiﬁcation, convexity and stochastic\nconstraints. Journal of Machine Learning Research, 12(Oct):2831–2855, 2011.\n12\n[23] P. Rigollet and X. Tong. Neyman-pearson classiﬁcation, convexity and stochastic\nconstraints. Journal of Machine Learning Research, 12(Oct):2831–2855, 2011.\n[24] D. Sahoo, Q. Pham, J. Lu, and S. Hoi. Online deep learning: learning deep neural\nnetworks on the ﬂy. In Proceedings of the 27th International Joint Conference on\nArtiﬁcial Intelligence, pages 2660–2666. AAAI Press, 2018.\n[25] R. Srivastava, K. Greff, and J. Schmidhuber. Highway networks. arXiv preprint\narXiv:1505.00387, 2015.\n[26] G. Uziel and R. El-Yaniv. Multi-objective non-parametric sequential prediction.\nIn Advances in Neural Information Processing Systems, pages 3372–3380, 2017.\n13\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-05-26",
  "updated": "2019-05-26"
}