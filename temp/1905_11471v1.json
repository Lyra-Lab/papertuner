{
  "id": "http://arxiv.org/abs/1905.11471v1",
  "title": "XLDA: Cross-Lingual Data Augmentation for Natural Language Inference and Question Answering",
  "authors": [
    "Jasdeep Singh",
    "Bryan McCann",
    "Nitish Shirish Keskar",
    "Caiming Xiong",
    "Richard Socher"
  ],
  "abstract": "While natural language processing systems often focus on a single language,\nmultilingual transfer learning has the potential to improve performance,\nespecially for low-resource languages. We introduce XLDA, cross-lingual data\naugmentation, a method that replaces a segment of the input text with its\ntranslation in another language. XLDA enhances performance of all 14 tested\nlanguages of the cross-lingual natural language inference (XNLI) benchmark.\nWith improvements of up to $4.8\\%$, training with XLDA achieves\nstate-of-the-art performance for Greek, Turkish, and Urdu. XLDA is in contrast\nto, and performs markedly better than, a more naive approach that aggregates\nexamples in various languages in a way that each example is solely in one\nlanguage. On the SQuAD question answering task, we see that XLDA provides a\n$1.0\\%$ performance increase on the English evaluation set. Comprehensive\nexperiments suggest that most languages are effective as cross-lingual\naugmentors, that XLDA is robust to a wide range of translation quality, and\nthat XLDA is even more effective for randomly initialized models than for\npretrained models.",
  "text": "XLDA: Cross-Lingual Data Augmentation for Natural Language\nInference and Question Answering\nJasdeep Singh1, Bryan McCann2, Nitish Shirish Keskar2, Caiming Xiong2, Richard Socher2\nStanford University1, Salesforce Research2\njasdeep@cs.stanford.edu {bmccann,nkeskar,cxiong,rsocher}@salesforce.com\nAbstract\nWhile natural language processing systems of-\nten focus on a single language, multilingual\ntransfer learning has the potential to improve\nperformance, especially for low-resource lan-\nguages.\nWe introduce XLDA, cross-lingual\ndata augmentation, a method that replaces a\nsegment of the input text with its transla-\ntion in another language.\nXLDA enhances\nperformance of all 14 tested languages of\nthe cross-lingual natural language inference\n(XNLI) benchmark. With improvements of up\nto 4.8%, training with XLDA achieves state-\nof-the-art performance for Greek, Turkish, and\nUrdu. XLDA is in contrast to, and performs\nmarkedly better than, a more naive approach\nthat aggregates examples in various languages\nin a way that each example is solely in one\nlanguage. On the SQuAD question answering\ntask, we see that XLDA provides a 1.0% per-\nformance increase on the English evaluation\nset. Comprehensive experiments suggest that\nmost languages are effective as cross-lingual\naugmentors, that XLDA is robust to a wide\nrange of translation quality, and that XLDA\nis even more effective for randomly initialized\nmodels than for pretrained models.\n1\nIntroduction\nRecent work on pretraining natural language pro-\ncessing systems (Devlin et al., 2018; Radford\net al., 2018; Howard and Ruder, 2018; Peters et al.,\n2018; McCann et al., 2017) has led to improve-\nments across a wide variety of natural language\ntasks (Wang et al., 2018; Rajpurkar et al., 2016;\nSocher et al., 2013; Conneau et al., 2018). For sev-\neral of these tasks, data can be plentiful for high-\nresource languages like English, Chinese, Ger-\nman, Spanish and French, but both the collec-\ntion and proliferation of data is limited for low-\nresource languages like Urdu. Even when a large\nlanguage model is pretrained on large amounts of\nTraining \ndata\nEn\nEn\nEn\nEn\nEs\nEs\nEn\nEs\nHi\nEn\nPrediction\ntest\nWorse test \naccuracy\nBetter test \naccuracy\nEn   En\nEn   En\nEn   En\nEn   En\nEn   En\nEn   En\n(a)\nकई भाषाओं में यह अस्पष्टता \nहै। \nThis ambiguity is true \nabout most languages .\nMrs. Cavendish is in her \nmother-in-law 's room .\nLa Sra. Cavendish ha \nabandonado el ediﬁcio .\n(b)\nFigure 1:\nComparing cross-lingual data augmenta-\ntion. a) compares the standard monolingual approach,\na naive multilingual approach that aggregates exam-\nples in various languages in a way that each example is\nsolely in one language, and cross-lingual data augmen-\ntation (XLDA). For each, prediction is done in a single\nlanguage. b) two examples of XLDA inputs using the\nXNLI dataset.\nmultilingual data (Devlin et al., 2018; Lample and\nConneau, 2019), languages like English can con-\ntain orders of magnitude more data in common\nsources for pretraining like Wikipedia.\nOne of the most common ways to leverage mul-\ntilingual data is to use transfer learning.\nWord\nembeddings such as Word2Vec (Mikolov et al.,\n2013b) or GloVe (Pennington et al., 2014) use\nlarge amounts of unsupervised data to create\ntask-agnostic word embeddings which have been\nshown to greatly improve downstream task per-\nformance. Multilingual variants of such embed-\ndings (Bojanowski et al., 2017) have also shown\nto be useful at improving performance on com-\nmon tasks across several languages.\nMore re-\narXiv:1905.11471v1  [cs.CL]  27 May 2019\ncently, contextualized embeddings such as CoVe,\nElMo, ULMFit and GPT (McCann et al., 2017;\nPeters et al., 2018; Howard and Ruder, 2018; Rad-\nford et al., 2018) have been shown to signiﬁcantly\nimprove upon aforementioned static embeddings.\nBERT (Devlin et al., 2018) employs a similar\nstrategy by using a masked version of the language\nmodeling objective. Unlike the other approaches,\nBERT also provides a multilingual contextual rep-\nresentation which is enabled by its shared sub-\nword vocabulary and multilingual training data.\nOften, for languages for which large amounts of\ndata is not available, aforementioned techniques\nfor creating embeddings (static or contextualized)\nis not possible and additional strategies need to be\nemployed.\nIn this work, we demonstrate the effectiveness\nof cross-lingual data augmentation (XLDA) as\na simple technique that improves generalization\nacross multiple languages and tasks. XLDA can\nbe used with both pretrained and randomly initial-\nized models without needing to explicitly further\nalign the embeddings. To apply XLDA to any nat-\nural language input, we simply take a portion of\nthat input and replace it with its translation in an-\nother language. This makes XLDA perfectly com-\npatible with recent methods for pretraining (Lam-\nple and Conneau, 2019; Devlin et al., 2018). Addi-\ntionally, the approach seamlessly scales for many\nlanguages and improves performance on all high-\nand low-resource languages tested including En-\nglish, French, Spanish, German, Greek, Bulgar-\nian, Russian, Turkish, Arabic, Vietnamese, Chi-\nnese, Hindi, Swahili and Urdu.\nIn short, this paper makes the following contri-\nbutions:\n• We propose cross-lingual data augmenation\n(XLDA), a new technique for improving the\nperformance of NLP systems that simply re-\nplaces part of the natural language input with\nits translation in another language.\n• We present experiments that show how\nXLDA can be used to improve performance\nfor every language in XNLI, and in three\ncases XLDA leads to state-of-the-art perfor-\nmance.\n• We demonstrate the ability of our method to\nimprove exact-match and F1 on the SQuAD\nquestion-answering dataset as well.\n2\nBackground and Related Work\nMultilingual Methods.\nMuch prior work that\nseeks to leverage multilingual data attempts to\nﬁrst train word embeddings from monolingual\ncorpora (Klementiev et al., 2012; Zou et al.,\n2013; Hermann and Blunsom, 2014) and then\nalign those embeddings using dictionaries be-\ntween languages (Mikolov et al., 2013a; Faruqui\nand Dyer, 2014).\nSome instead train multilin-\ngual word embeddings jointly from parallel cor-\npora (Gouws et al., 2015; Luong et al., 2015).\nJohnson et al. (2017) demonstrate how training\nmultilingual translation in a single model can be\nused for zero-shot translation (i.e., for transla-\ntion pairs with no parallel training data).\nThis\napproach also attained state-of-the-art results for\nmany languages.\nMore recently, similar tech-\nniques have been adapted for extremely low re-\nsource languages (Gu et al., 2018). Neubig and\nHu (2018) showed how to further ﬁne-tune a mul-\ntilingual model by explicitly using a high-resource\nlanguage with a linguistically related low-resource\nlanguage to improve translation quality. More re-\ncently, Conneau et al. (2017); Artetxe et al. (2018)\nshow how to obtain cross-lingual word embed-\ndings through entirely unsupervised methods that\ndo not use any dictionaries or parallel data.\nNatural Language Inference.\nThe Multi-Genre\nNatural Language Inference (MultiNLI) corpus\n(Williams et al., 2017) uses data from ten distinct\ngenres of English language for the the task of nat-\nural language inference (prediction of whether the\nrelationship between two sentences represents en-\ntailment, contradiction, or neither). XNLI (Con-\nneau et al., 2018) is an evaluation set grounded in\nMultiNLI for cross-lingual understanding (XLU)\nin 15 different languages that include low-resource\nlanguages such as Swahili and Urdu. XNLI serves\nas the primary testbed for our proposed method,\nXLDA, which improves over the baseline model\nin all languages and achieves state-of-the-art per-\nformance on Greek, Turkish, and Urdu even with-\nout the prior state-of-the-art pretraining Lample\nand Conneau (2019) introduced concurrently to\nour work.\nQuestion\nAnswering.\nWe\nalso\ninclude\nex-\nperiments on the Stanford Question Answer-\ning Dataset (SQuAD) (Rajpurkar et al., 2016).\nThis dataset consists of context-question-answer\ntriplets such that the answer is completely con-\ntained, as a span, in the context provided. For this\ntask we translate only the training set into 4 lan-\nguages using a neural machine translation system.\nDue to the fact that (machine or human) transla-\ntion may not necessarily retain span positions, the\ntranslation of this dataset is more nuanced than the\nclassiﬁcation datasets discussed previously. For\nthis reason, we do not tamper with the original\nSQuAD validation data; the test set is not publicly\navailable either, so we constrain ourselves to a set-\nting in which XLDA is used at training time but\nthe target language remains English during valida-\ntion and testing.\nUnsupervised\nLanguage\nModels.\nRecently,\nlarge, pretrained, unsupervised language models\nhave been used for XNLI, SQuAD, and the GLUE\nbenchmark (Wang et al., 2018) to improve per-\nformance across the board. BERT (Devlin et al.,\n2018) pretrains deep bidirectional representations\nby jointly conditioning on both left and right\ncontext in all layers.\nA BERT model can then\nbe ﬁne-tuned with an additional output layer\nfor a speciﬁc task. In this way BERT achieved\nsigniﬁcant improvements on both XNLI and\nSQuAD. It is this model that serves as the primary\nbase for the application of XLDA across these\ntasks.\nBack-translation.\nAkin\nto\nXLDA,\nback-\ntranslation is often used to provide additional data\nthrough the use of neural machine translation\nsystems. As a recent example, QANet (Yu et al.,\n2018) employed this technique to achieve then\nstate-of-the-art results on question answering,\nand\nmachine\ntranslation\nsystems\n(Sennrich\net al., 2016) regularly use such techniques to\niteratively improve through a back-and-forth\nprocess. XLDA also translates training data from\na source language into a target language, but\nXLDA does not translate back into the source\nlanguage. In fact, experimental results show that\nXLDA provides improved performance over using\neven the original source data, let alone a noisier\nversion provided through back-translation. This\nindicates that signal in multiple languages can be\nbeneﬁcial to training per se rather than only as an\nintermediary for back-translation.\n3\nXLDA: Cross-Lingual Data\nAugmentation\nXLDA is entirely agnostic to the speciﬁcs of a\nmodel, so in this section we denote a generic NLP\nmodel M. Because M is an NLP model, at least\nsome of its inputs are a form of text. To closely\nalign with the experiments, we describe XLDA in\nthe speciﬁc setting of two linguistic inputs, x and y\nboth sequences of tokens, but the same technique\ncan be applied to an arbitrary number of inputs.\nLet T represent a system that can translate x\nand y into some number of language, L. T may\nbe a collection of human or automatic translators.\nFor a given dataset D = {(xi, yi)}, we create\na new dataset D′ = {(˜xij, ˜yij)}, where ˜xij =\nT (xi), the translation of xi into language j < L\nby T . Similarly for ˜yij = T (yi).\nWe call the scenario in which only D is used\nto train M the monolingual setting. When both a\nsubset of D and a subset of D′ are used for train-\ning, we refer to this as the disjoint, multilingual\nsetting (DMT). In other words DMT is the naive\napproach that aggregates examples in various lan-\nguages in a way that each example is solely in\none language. The XLDA setting is when we al-\nlow cross-over between examples from D and D′\nas well. To be precise, DXLDA may in the most\ngeneral case contain examples from D, D′, and\nadditionally examples of the forms (xi, ˜yij) and\n(˜xij, yi).\n4\nExperiments and Results\nWe experiment with using various subsets of\nDXLDA and demonstrate empirically that some\nprovide better learning environments than the\nmonolingual and disjoint, multilingual settings.\nFirst, we provide details on how different transla-\ntion systems T were used to generate cross-lingual\ndatasets for both tasks under consideration.\n4.1\nCross-lingual Data\nThe ﬁrst task we consider in these experiments\nis MultiNLI (Williams et al., 2017). The XNLI\n(Conneau et al., 2018) dataset provides the dis-\njoint, multilingual version of MultiNLI necessary\nfor XLDA. To create the XNLI dataset, the authors\nused 15 different neural machine translation sys-\ntems (each for a different language pair) to create\n15 separate single language training sets. The val-\nidation and test sets were translated by humans.\nThe fact that XNLI is aligned across languages for\neach sentence pair allows our method to be trained\nbetween the examples in the 15 different training\nsets. Since XLDA only pertains to training, the\nvalidation and test settings remain the same for\neach of the 15 languages, but future work will ex-\nplore how this method can be used at test time as\nwell. Because we have human translated valida-\ntion and test sets for XNLI, it is the primary task\nunder examination in our experiments.\nWe follow a similar process for the Stanford\nQuestion Answering Dataset, SQuAD (Rajpurkar\net al., 2016). Given SQuAD is a span-extraction\nproblem, translation of the training set required\nspecial care. Questions, context paragraphs, and\ngold answers were translated separately. We then\nused exact string matching between the translated\nanswers and the translated context paragraphs to\ndetermine if the translated answer could still be\nfound in the translated context. If the answer ex-\nists, we take its ﬁrst instance as the ground truth\ntranslated span. 65% of German, 69% of French,\n70% of Spanish, and 45% of Russian answer spans\nwere recoverable in this way. To translate the rest\nof the questions we placed a special symbol on\nboth sides of the ground truth span in the English\ncontext paragraph before translation. Occasion-\nally, even with this special symbol, the translated\nanswers could not be recovered from the marked,\ntranslated context. Additionally, the presence of\nthe symbol did inﬂuence gender and tense in the\ntranslation as well. Using this approach on ex-\namples that failed span-recovery in the ﬁrst phase,\nwe were able to recover 81% of German, 96% of\nFrench, 97% of Spanish, and 96% of Russian an-\nswer spans. However, adding this additional data\nfrom the marked, translations led to worse perfor-\nmance across all of the languages. Hence, we only\nuse the translated examples from the ﬁrst phase\nfor all training settings below, effectively reducing\ntraining set size. The validation and test splits for\nSQuAD remain in English alone1. This still allows\nus to explore how XLDA can be used to leverage\nmultilingual supervision at train time for a single\ntarget language at validation time.\n4.2\nModels\nFor XNLI, we demonstrate that XLDA improves\nover the multilingual BERT model (BERTML)\nﬁne-tuned for different languages using a trained\n1The test set is not publicly available, so it could not be\ntranslated as XNLI was.\nclassiﬁcation layer (Devlin et al., 2018). In XNLI,\nthere are two inputs, a premise and a hypothesis.\nThe BERT model takes as input both sequences\nconcatenated together. It then makes a prediction\noff of a special CLS token appended to the start\nof the combined input sequence. We experiment\nwith a variety of settings, but the primary evalua-\ntion of XLDA shows that replacing either one of\nthe XNLI inputs with a non-target language can\nalways improves performance over using only the\ntarget language throughout.\nThese experiments\nare discussed in detail in sections 4.3-4.5. Simi-\nlar experiments with an LSTM baseline that is not\npretrained are outlined in 4.6, which demonstrates\nthat XLDA is also effective for models that are not\npretrained or as complex as BERT.\nFor SQuAD, we demonstrate that XLDA also\nimproves over BERTML ﬁne-tuned by using only\ntwo additional parameter vectors: One for identi-\nfying the start token and for the end token again\nfollowing the recommendations in Devlin et al.\n(2018).\nExperiments with these BERT models\ndemonstrate that XLDA can improve even the\nstrongest multilingual systems pretrained on large\namounts of data.\nFor all of our experiments we use the hy-\nperparameters and optimization strategies recom-\nmended by Devlin et al. (2018). For both datasets\nthe learning rate is warmed up for 10% of the to-\ntal number of updates (which are a function of the\nuser speciﬁed batch size and number of epochs)\nand then linearly decayed to zero over training.\nFor XNLI the batch size is 32, learning rate is 2e-\n5 and number of epochs is 3.0. For SQuAD the\nbatch size is 12, learning rate is 3e-5 and number\nof epochs is 3.0.\n4.3\nPairwise Evaluation\nOur ﬁrst set of experiments comprise a com-\nprehensive pairwise evaluation of XLDA on the\nXNLI dataset. The results are presented in Fig-\nure 2. The language along a row of the table cor-\nresponds to the language evaluated upon. The lan-\nguage along a column of the table corresponds to\nthe auxiliary language used as an augmentor for\nXLDA. Diagonal entries are therefore the valida-\ntion scores for the standard, monolingual training\napproach. Numbers on the diagonal are absolute\nperformance. Numbers on the off-diagonal indi-\ncate change over the diagonal entry in the same\nrow. Through color normalization, deep green rep-\nar bg de el en es fr\nhi ru sw tr\nur vi zh\naugmentor\nar\nbg\nde\nel\nen\nes\nfr\nhi\nru\nsw\ntr\nur\nvi\nzh\ntarget\n72.0 +2.1 +0.7 +0.8 +1.9 +1.3 +1.9 +0.7 +1.3\n-0.6\n+0.9\n-2.2\n+1.5 +2.0\n+1.4 75.3 +2.0 +1.6 +2.3 +2.9 +1.9 +1.1 +2.5 +0.8 +2.0\n-1.4\n+1.5 +1.7\n+0.2 +1.1 77.9 +0.3 +1.2 +1.4 +1.5\n-0.5\n+1.0 +0.1 +0.1\n-2.9\n+0.4 +1.3\n+1.7 +2.4 +2.1 74.7 +1.8 +2.3 +1.7 +1.2 +1.8 +1.0 +0.7\n-2.4\n+1.9 +2.2\n+0.7 +0.7 +1.2 +0.1 82.3 +1.2 +1.4\n-0.2\n+0.9\n-0.9\n+0.2\n-3.5\n+0.4\n0.0\n+0.4 +1.2 +0.4\n-0.4\n+0.7 78.6 +1.6 +0.1 +1.0\n-0.3\n+0.6\n-3.2\n+0.6 +1.0\n+1.0 +1.7 +1.8 +0.8 +0.8 +1.6 78.0\n-0.3\n+1.6\n-0.1\n+0.5\n-3.5\n+0.8 +1.0\n+2.0 +2.4 +3.3 +2.3 +2.2 +1.8 +2.5 67.3 +3.0 +0.6 +2.6 +0.4 +2.1 +3.1\n+1.4 +1.3 +1.1 +0.9 +1.6 +1.8 +1.7 +1.6 73.9 +0.4 +0.7\n-2.0\n+0.7 +1.8\n+1.8 +1.8 +2.2 +1.1 +1.8 +1.9 +2.1 +0.2 +2.0 66.9\n0.0\n-0.7\n+1.9 +1.3\n+0.3 +0.7 +1.1 +0.1 +0.9 +0.9 +0.3 +0.2 +1.1 +0.2 73.6\n-2.1\n+0.3 +1.4\n+1.9 +2.0 +1.8 +1.5 +2.1 +2.5 +2.2 +2.1 +1.9 +1.1 +1.5 62.4\n0.0\n+2.4\n0.0\n+0.4 +0.2\n0.0\n+0.3 +0.6 +0.4\n-0.6\n+0.2\n-0.9\n0.0\n0.0\n74.5 +0.4\n+0.2 +0.4 +1.1 +0.4 +0.4 +1.3 +0.7 +0.1 +0.6\n-0.6\n+0.9\n-1.6\n+0.6 74.9\nFigure 2:\nA comprehensive pairwise evaluation\ndemonstrates that all languages have a cross-lingual\naugmentor that improves over monolingual training.\nRows correspond to the evaluated language. Columns\nare cross-lingual augmentors. Numbers on the diago-\nnal are performance of monolingual training. Numbers\non the off-diagonal indicate change over the diagonal\nentry in the same row. Red to green indicates stronger\nimprovement over the baseline with XLDA.\nresents a large relative improvement of the num-\nber depicted over the standard approach (the di-\nagonal entry in the same row). Deep red repre-\nsents a larger relative decrease compared to the\nstandard approach. Mild yellow reﬂects little to\nno improvement over the standard approach.\nThere exists a cross-lingual augmentor that im-\nproves over the monolingual approach.\nFirst\nnote that the highest performance for each row is\noff-diagonal. This demonstrates the surprising re-\nsult that for every target language, it is actually\nbetter to train with cross-lingual examples than it\nis to train with monolingual examples. For ex-\nample, if Hindi is the target language, the stan-\ndard monolingual approach would train with both\npremise and hypothesis in Hindi, which gives a\nvalidation performance of 67.3%.\nXLDA with\nGerman in this case includes only examples that\nhave either the premise or hypothesis in German\nand the other in Hindi. Therefore, there are no\nexamples in which both premise and hypothesis\nare in the same language. This improves perfor-\nmance by 3.3% to 70.6%.\nSimilarly, for every\nlanguage a XLDA approach exists that improves\nover the standard approach.\nHindi augmented\nEN - XX BLEU\n Δ Accuracy\n-4%\n-2%\n0%\n2%\n4%\n20\n25\n30\n35\n40\n45\nar\nbg\nde\nel\nen\nes\nfr\nhi\nru\nsw\ntr\nur\nvi \nzh \nFigure 3:\nThe BLEU score of the translation system\nseems to have little effect on the language’s perfor-\nmance as a cross-lingual augmentor. The y-axis shows\nthe change XNLI validation accuracy, and the x-axis\nshows the BLEU scores of the NMT systems corre-\nsponding to the language used as a cross-lingual aug-\nmentor. The translation qualities, in BLEU, for the En-\nglish to language X systems were as follows ar:15.8,\nbg:34.2, de:38.8, el:42.4, es:48.5, fr:49.3, hi:37.5,\nru:24.9, sw:24.6, tr:21.9, ur:24.1, vi:39.9, zh:23.2.\nar bg de el en es\nfr\nhi ru sw tr\nur\nvi zh\naugmentor\nar\nbg\nde\nel\nen\nhi\ntarget\n51.8\n53.1\n53.2\n52.3\n52.1\n53.5\n52.0\n53.1\n53.1\n52.7\n52.5\n51.8\n52.3\n53.0\n54.9\n54.4\n54.9\n53.5\n55.9\n54.5\n54.4\n53.8\n56.1\n54.3\n54.2\n53.7\n55.0\n54.8\n57.3\n56.7\n55.8\n56.4\n57.8\n57.6\n57.0\n56.9\n56.6\n56.3\n56.8\n55.3\n57.2\n56.9\n55.0\n54.5\n55.1\n51.8\n54.0\n54.7\n54.2\n55.0\n54.3\n54.5\n53.2\n53.8\n53.1\n53.7\n59.9\n60.4\n61.1\n58.0\n59.1\n59.2\n59.7\n58.7\n60.0\n58.9\n59.2\n58.0\n58.2\n59.1\n54.2\n53.8\n52.8\n53.3\n53.7\n53.1\n52.2\n51.8\n53.3\n52.3\n53.3\n53.1\n52.7\n52.9\nFigure 4: XLDA is on average even more effective for\na given language when used on randomly initialized\nmodels compared to large, pretrained ones.\nby German represents the strongest improvement,\nwhereas Vietnamese augmented by Spanish repre-\nsents the weakest at a 0.6% improvement over the\nstandard approach.\nMost\nlanguages\nare\neffective\naugmentors.\nWith the exception of Urdu, Swahili, Hindi, and\nGreek, all the remaining 10 languages provided a\nnonnegative performance improvement as a aug-\nmentor for each of the 14 target languages. This\ndemonstrates that as long as one avoids the lowest\nresource languages as augmentors, it is far more\nlikely that XLDA will improve over the standard\napproach. It also demonstrates that with limited\ntranslation resources, one should translate into the\nlanguages that are relatively higher resource for\ntraining machine translation systems (e.g. Span-\nish, German, French, English).\nLower resource languages are less effective\naugmentors, but beneﬁt greatly from XLDA.\nExamining this further, it is clear that languages\nthat are often considered relatively lower resource\nin the machine translation community tend to be\nless effective cross-lingual augmentors. For ex-\nample, the abundance of red in the Urdu column\nreveals that Urdu is often the worst augmentor for\nany given language under consideration. On av-\nerage, augmenting with Urdu actually hurts per-\nformance by 1.8%. On the other hand, looking\nat the row for Urdu reveals that it often beneﬁts\nstrongly from XLDA with other languages. Simi-\nlarly, Hindi beneﬁts the most from XLDA, and is\nonly a mildly successful augmentor.\nXLDA is robust to translation quality.\nIn cre-\nating the XNLI dataset, the authors used 15 differ-\nent neural machine translation systems with vary-\ning translation qualities according to BLEU score\nevaluation. The translation qualities are present\nin the caption of Figure 3, which shows that even\nwhen controlling for BLEU score, most languages\ncan be used as effective augmentors.\n4.4\nA Greedy Algorithm for XLDA\nGiven that the pairwise evaluation of Section 4.3\nreveals that most languages are effective cross-\nlingual augmentors, we turn to the case in which\nwe would like to maximize the beneﬁts of XLDA\nby using multiple augmenting languages. In these\nexperiments, we use a simple, greedy approach to\nbuild off of the pairwise results. For any given\ntarget language, we sort the languages in order of\ndecreasing effectiveness as an augmentor (deter-\nmined by relative improvement over the standard\napproach in the pairwise setting). We start with\nthe augmentor that is most effective for that target\nlanguage and add augmentors one at a time in de-\ncreasing order until we reach a augmentor that hurt\nin the pairwise case. The results are presented in\nFigure 5 where each subplot is corresponds to per-\nformance on a target language as number of cross-\nlingual augmentors increases\nGreedy XLDA always improves over using the\nsingle best cross-lingual augmentor.\nFor ev-\nery target language, the greedy approach improves\nover the best pairwise XLDA by a minimum of\n0.9% and provides a minimum of 2.1% improve-\nment over the original standard approach that does\nnot use any form of XLDA. Somewhat surpris-\ningly, it is not the case that more data always helps\nLang.\nBERTML\nGreedy XLDA\n+∆\nar\n71.4\n75.0\n3.6\nbg\n75.9\n79.1\n3.2\nde\n76.1\n78.7\n2.6\nel\n74.8\n78.4\n3.6\nen\n81.5\n83.4\n1.9\nes\n79.0\n80.4\n1.4\nfr\n78.3\n78.8\n0.5\nhi\n67.2\n71.9\n4.7\nru\n74.6\n76.7\n2.1\nsw\n65.2\n70.0\n4.8\ntr\n72.2\n75.1\n2.9\nur\n63.0\n65.9\n2.9\nvi\n72.6\n76.1\n3.5\nzh\n74.6\n76.0\n1.4\nTable 1: Test results on XNLI comparing multilin-\ngual BERT without and with Greedy XLDA. Bold in-\ndicates state-of-the-art performance. At the time of our\nwork, multilingual BERT was the current state-of-the-\nart, but work concurrent to ours now holds state-of-the-\nart on all but the bolded languages(Lample and Con-\nneau, 2019).\nfor XLDA. Most languages have peak validation\nperformance with fewer than the total number of\naugmentors that beneﬁted in pairwise evaluation.\nIn the best cases (Russian, Greek, and Arabic),\ngreedy XLDA improves over the best pairwise\nXLDA by more than 2%. When compared to the\nstandard approach, greedy XLDA improves by as\nmuch as 4.9% (Hindi).\n4.5\nTargeted XLDA\nSince most languages are effective augmentors\nand few are actually harmful to performance (Sec-\ntion 4.3), we now consider the setting in which\na comprehensive pairwise evaluation and greedy\nsearch cannot be done. We use this setting to eval-\nuate whether XLDA improves over the disjoint,\nmultilingual setting described in Section 3. Recall\nthat in the XLDA setting each input to the model\nis in a different language for each example. In\nthe disjoint, multilingual setting, each input is in\nthe same language, but examples themselves may\ncome from different languages.\nThe ‘cross’ in cross-lingual is crucial.\nFigure 7\nshows three selected target languages (English,\nGerman, and Russian) as well as six augmen-\ntors (English, German, Russian, French, Bulgar-\nian, and Arabic).\nWe compare, for each target\nlanguage, how incrementally adding augmentors\n0\n5\n10\n0\n2\n4\n∆accuracy\nar\n0\n5\n10\n0\n2\n4\nbg\n0\n5\n10\n0\n2\n4\nde\n0\n5\n10\n0\n2\n4\nel\n0\n5\n10\n0\n2\n4\nen\n0\n5\n10\n0\n2\n4\nes\n0\n5\n10\n0\n2\n4\nfr\n0\n5\n10\n#augmentors\n0\n2\n4\n∆accuracy\nhi\n0\n5\n10\n#augmentors\n0\n2\n4\nru\n0\n5\n10\n#augmentors\n0\n2\n4\nsw\n0\n5\n10\n#augmentors\n0\n2\n4\ntr\n0\n5\n10\n#augmentors\n0\n2\n4\nur\n0\n5\n#augmentors\n0\n2\n4\nvi\n0\n5\n10\n#augmentors\n0\n2\n4\nzh\nFigure 5: Greedily adding cross-lingual augmentors to BERTML based on the pairwise evaluation.\n0\n1\n2\n3\n0\n2\n4\n6\n∆accuracy\nar\n0\n1\n2\n3\n0\n2\n4\n6\nbg\n0\n1\n2\n3\n0\n2\n4\n6\nde\n0\n1\n2\n3\n#augmentors\n0\n2\n4\n6\n∆accuracy\nel\n0\n1\n2\n3\n#augmentors\n0\n2\n4\n6\nen\n0\n1\n2\n3\n#augmentors\n0\n2\n4\n6\nhi\nFigure 6: Greedily adding cross-lingual augmentors to\nan LSTM baseline.\ninﬂuences performance in both the cross-lingual\nand the disjoint, multilingual settings. It is clear\nthat while both improve over the monolingual set-\nting, XLDA provides a much greater improvement\nas additional augmentors are used. This demon-\nstrates that it is indeed the cross-over of languages\nin XLDA that makes it so effective. It is not as\neffective to train with translated versions of the\ntraining datasets without cross-linguality in each\nexample.\n4.6\nXLDA without BERT\nIn order to test whether the beneﬁts of XLDA\ncome only from the multilinguality of the BERT\nmodel, partially replicate the pairwise evaluation\nof Section 4.3 for an LSTM baseline.\nFor this\nbaseline, we use the same tokenization that we\nused for BERTML. We also use the embeddings\nfrom BERTML, but we do not propagate gra-\ndients through them, so they function just like\nother forms of pretrained embeddings (GloVe,\nWord2Vec, etc.). This NLI model reads the in-\nput with a two-layer BiLSTM, projects the outputs\nfrom the ﬁnal layer to a lower dimension, max-\npools, and passes that through a ﬁnal three-class\nclassiﬁcation layer.\n+de\n+ru\n+es\n+fr\n+bg\n+ar\n0\n2\n4\n∆accuracy\nen\nDMT\nXLDA\n+en\n+ru\n+es\n+fr\n+bg\n+ar\n0\n2\n4\n∆accuracy\nde\n+en\n+de\n+es\n+fr\n+bg\n+ar\nlanguages used\n0\n2\n4\n∆accuracy\nru\nFigure 7:\nBoth cross-lingual data augmentation\n(XLDA) and disjoint, multilingual training (DMT, see\nSection 3) improve over monolingual training, but\nXLDA provides greater improvements as more aug-\nmentors are used.\nXLDA is equally effective for randomly initial-\nized and pretrained models.\nAs can be seen\nin Figure 4, the LSTM baseline sees gains from\nXLDA as substantial as BERTML did. In the best\ncase (Greek augmented by German), performance\nwas improved by 3.3%, just as high as the highest\ngain for BERTML.\nGreedy XLDA is more effective for randomly\ninitialized models than pretrained models.\nAs\ncan be seen in Figure 6, the LSTM baseline sees\ngains from greedy XLDA that are even greater\nthan they were for BERTML. German’s XLDA\nperformance was improved by 3.3% over using\npairwise XLDA alone. This represents an absolute\nimprovement of 5.3% over the standard, monolin-\ngual approach. In the best case (Greek), the abso-\nlute gain was 5.5% and in the worst case it was\n4.0%.\nThis demonstrates that greedy XLDA is\na powerful technique that can be used with pre-\ntrained and randomly initialized models alike.\n4.7\nXLDA for SQuAD\nWe continue with experiments on the Stanford\nQuestion Answering Dataset (SQuAD). In this set-\nting, evaluation is always performed with English\ncontexts and questions. In Table 2, validation re-\nsults are depicted that vary over which languages\nwere used during training for either the question\nor the context. The en-en row represents the case\nin which only English inputs are used.\nIn the ﬁrst group of four rows of Table 2, XLDA\nis only applied to translate the contexts. In the\nsecond group of four rows of Table 2, we also\ntranslate the question. When French is used as\nthe cross-lingual augmentor over the context in-\nput, we see an improvement of 0.5 EM and 1 F1.\nWe also ran these experiments with Russian, Span-\nish, and German, each of which proved to be effec-\ntive cross-lingual augmentors over the context.\nWhen cross-lingual augmentors are used over\nthe question input as well. we still often see im-\nprovement over the baseline, but the improvement\nis always less than when using XLDA only over\nthe context channel. This is in keeping with the\nﬁndings of Asai et al. (2018), which show that the\nability to correctly translate questions is crucial\nfor question answering. In other words, SQuAD\nis extremely sensitive to the translation quality of\nthe question, and it is not surprising that machine\ntranslations of the questions are less effective than\ntranslating the context, which is less sensitive.\nError analysis on the SQuAD dataset provides\ninsight into how XLDA improves performance.\nBy inspection of 300 examples, we found that the\nbaseline BERTML model often makes mistakes\nthat suggest a faulty heuristic based on fuzzy, pat-\ntern matching. To be precise, the model seems\nto regularly depend on the presence of key words\nfrom the question in the context as well. When\nthese key words are closer to plausible, incorrect\nspans than they are to the correct span, the model\ntends to choose these closer, incorrect spans. In\none example, the context contains the text “...The\nSuper Bowl 50 halftime show was headlined by\nQues.\nContext\nEM\nnF1\nen\nen\n81.7\n88.5\nen\nen,fr\n+0.5\n+1.0\nen\nen,es\n+0.4\n+0.8\nen\nen,ru\n+0.2\n+0.7\nen\nen,de\n0.0\n+0.8\nen,fr\nen,fr\n+0.1\n+0.6\nen,es\nen,es\n-0.3\n+0.3\nen,ru\nen,ru\n-0.5\n+0.1\nen,de\nen,de\n-0.4\n0.0\nTable 2: SQuAD validation results when questions and\ncontexts are both in English. en-en represents the case\nin which English questions and contexts are used in\ntraining. The row for en-en,fr demonstrates that by in-\ncluding XLDA with French contexts, performance ac-\ntually improves over using only English.\nthe British rock group Coldplay with special guest\nperformers Beyonce and Bruno Mars, who head-\nlined the Super Bowl XLVII and Super Bowl\nXLVIII halftime shows, respectively...”, the ques-\ntion is “What halftime performer previously head-\nlined Super Bowl XLVII?”, and the true answer is\n“Beyonce”. The baseline model outputs “Bruno\nMars” seemingly because it is closer to key words\n“Super Bowl XLVII”, but the model with XLDA\ncorrectly outputs “Beyonce”. Because these mod-\nels often heavily rely on attention (Bahdanau et al.,\n2014; Vaswani et al., 2017) between the ques-\ntion and context sequences, word-similarity-based\nmatching seems to localize keywords that may\nbe candidate solutions.\nSuch examples are of-\nten correctly answered after XLDA is used dur-\ning training. This suggests that translation of the\ncontext into a different language during training\n(via XLDA) breaks the strong dependence on the\nword-similarity-based heuristic.\nXLDA instead\nforces the model to consider the semantics of the\ncontext instead.\n5\nConclusion\nWe introduce XLDA, cross-lingual data augmen-\ntation, a method that improves the training of NLP\nsystems by replacing a segment of the input text\nwith its translation in another language. We show\nhow reasoning across languages is crucial to the\nsuccess of XLDA compared to naive approaches\nof simply adding more data in different languages\nsuch that each example is solely in a single lan-\nguage. We show the effectiveness of the approach\nwith both massive pretrained models and smaller\nrandomly initialized models.\nWe boost perfor-\nmance on all languages in the XNLI dataset, by\nup to 4.8%, and achieve state of the art results\non 3 languages including the low resource lan-\nguage Urdu. While the empirical results of XLDA\nare promising, further investigation is needed to\nunderstand the causal and linguistic relationship\nbetween XLDA and performance on downstream\ntasks. Additionally, it would be interesting to in-\nvestigate the use a similar heuristic during test\ntime; inference can be performed in multiple lan-\nguages (even if one target language is of interest)\nand they can be aggregated together.\nHowever,\none concern with the use of XLDA is the unex-\nplored possibility for it to allow negative biases\nthat may exist in representations learned for one\nlanguage to affect other languages that are used in\nconjunction with it through XLDA. We can hope\nthat XLDA mitigates biases that are demonstrated\nin representations learned for some languages and\nnot in others, but future work will have to care-\nfully examine how XLDA interacts with multilin-\ngual datasets and the bias therein.\nAcknowledgments\nWe would like to thank Melvin Gruesbeck for the\nillustration of XLDA in ﬁgure 1; Srinath Meadu-\nsani, Lavanya Karanam, Ning Dong and Navin\nRamineni for assistance with compute infrastruc-\nture.\nReferences\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018.\nA robust self-learning method for fully unsupervised\ncross-lingual mappings of word embeddings. arXiv\npreprint arXiv:1805.06297.\nAkari Asai, Akiko Eriguchi, Kazuma Hashimoto, and\nYoshimasa Tsuruoka. 2018. Multilingual extractive\nreading comprehension by runtime machine transla-\ntion. arXiv preprint arXiv:1809.03275.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014.\nNeural machine translation by jointly\nlearning to align and translate.\narXiv preprint\narXiv:1409.0473.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nAlexis Conneau, Guillaume Lample, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Herv´e J´egou. 2017.\nWord translation without parallel data.\narXiv\npreprint arXiv:1710.04087.\nAlexis Conneau, Guillaume Lample, Ruty Rinott, Ad-\nina Williams, Samuel R Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. Xnli: Evaluating cross-\nlingual sentence representations.\narXiv preprint\narXiv:1809.05053.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nManaal Faruqui and Chris Dyer. 2014. Improving vec-\ntor space word representations using multilingual\ncorrelation. In Proceedings of the 14th Conference\nof the European Chapter of the Association for Com-\nputational Linguistics, pages 462–471.\nStephan Gouws, Yoshua Bengio, and Greg Corrado.\n2015. Bilbowa: Fast bilingual distributed represen-\ntations without word alignments.\nJiatao Gu, Hany Hassan, Jacob Devlin, and Victor OK\nLi. 2018. Universal neural machine translation for\nextremely low resource languages. arXiv preprint\narXiv:1802.05368.\nKarl Moritz Hermann and Phil Blunsom. 2014. Multi-\nlingual models for compositional distributed seman-\ntics. arXiv preprint arXiv:1404.4641.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation.\narXiv preprint arXiv:1801.06146.\nMelvin Johnson, Mike Schuster, Quoc V Le, Maxim\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\nFernanda Vi´egas, Martin Wattenberg, Greg Corrado,\net al. 2017.\nGoogles multilingual neural machine\ntranslation system: Enabling zero-shot translation.\nTransactions of the Association for Computational\nLinguistics, 5:339–351.\nAlexandre Klementiev, Ivan Titov, and Binod Bhat-\ntarai. 2012. Inducing crosslingual distributed repre-\nsentations of words. Proceedings of COLING 2012,\npages 1459–1474.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. arXiv preprint\narXiv:1901.07291.\nThang Luong, Hieu Pham, and Christopher D Man-\nning. 2015.\nBilingual word representations with\nmonolingual quality in mind. In Proceedings of the\n1st Workshop on Vector Space Modeling for Natural\nLanguage Processing, pages 151–159.\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In Advances in Neural In-\nformation Processing Systems, pages 6294–6305.\nTomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013a.\nExploiting similarities among languages for ma-\nchine translation. arXiv preprint arXiv:1309.4168.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013b. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in neural information processing\nsystems, pages 3111–3119.\nGraham Neubig and Junjie Hu. 2018. Rapid adapta-\ntion of neural machine translation to new languages.\narXiv preprint arXiv:1808.04189.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014.\nGlove: Global vectors for word\nrepresentation. In Proceedings of the 2014 confer-\nence on empirical methods in natural language pro-\ncessing (EMNLP), pages 1532–1543.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. arXiv preprint arXiv:1802.05365.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018.\nImproving language under-\nstanding by generative pre-training. URL https://s3-\nus-west-2. amazonaws. com/openai-assets/research-\ncovers/languageunsupervised/language\nunder-\nstanding paper. pdf.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016.\nSquad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016.\nEdinburgh neural machine translation sys-\ntems for wmt 16. arXiv preprint arXiv:1606.02891.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013.\nRecursive deep models\nfor semantic compositionality over a sentiment tree-\nbank.\nIn Proceedings of the 2013 conference on\nempirical methods in natural language processing,\npages 1631–1642.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nAlex Wang, Amapreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nAdina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2017. A broad-coverage challenge corpus for\nsentence understanding through inference.\narXiv\npreprint arXiv:1704.05426.\nAdams Wei Yu, David Dohan, Minh-Thang Luong, Rui\nZhao, Kai Chen, Mohammad Norouzi, and Quoc V\nLe. 2018.\nQanet:\nCombining local convolution\nwith global self-attention for reading comprehen-\nsion. arXiv preprint arXiv:1804.09541.\nWill Y Zou, Richard Socher, Daniel Cer, and Christo-\npher D Manning. 2013. Bilingual word embeddings\nfor phrase-based machine translation. In Proceed-\nings of the 2013 Conference on Empirical Methods\nin Natural Language Processing, pages 1393–1398.\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2019-05-27",
  "updated": "2019-05-27"
}