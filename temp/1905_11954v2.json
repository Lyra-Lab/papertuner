{
  "id": "http://arxiv.org/abs/1905.11954v2",
  "title": "Unsupervised Learning from Video with Deep Neural Embeddings",
  "authors": [
    "Chengxu Zhuang",
    "Tianwei She",
    "Alex Andonian",
    "Max Sobol Mark",
    "Daniel Yamins"
  ],
  "abstract": "Because of the rich dynamical structure of videos and their ubiquity in\neveryday life, it is a natural idea that video data could serve as a powerful\nunsupervised learning signal for training visual representations in deep neural\nnetworks. However, instantiating this idea, especially at large scale, has\nremained a significant artificial intelligence challenge. Here we present the\nVideo Instance Embedding (VIE) framework, which extends powerful recent\nunsupervised loss functions for learning deep nonlinear embeddings to\nmulti-stream temporal processing architectures on large-scale video datasets.\nWe show that VIE-trained networks substantially advance the state of the art in\nunsupervised learning from video datastreams, both for action recognition in\nthe Kinetics dataset, and object recognition in the ImageNet dataset. We show\nthat a hybrid model with both static and dynamic processing pathways is optimal\nfor both transfer tasks, and provide analyses indicating how the pathways\ndiffer. Taken in context, our results suggest that deep neural embeddings are a\npromising approach to unsupervised visual learning across a wide variety of\ndomains.",
  "text": "Unsupervised Learning from Video with Deep Neural Embeddings\nChengxu Zhuang1\nTianwei She1\nAlex Andonian2\nMax Sobol Mark1\nDaniel Yamins1\n1Stanford University\n2 MIT\n{chengxuz, shetw, joelmax, yamins}@stanford.edu\nandonian@mit.edu\nAbstract\nBecause of the rich dynamical structure of videos and\ntheir ubiquity in everyday life, it is a natural idea that\nvideo data could serve as a powerful unsupervised learn-\ning signal for visual representations. However, instanti-\nating this idea, especially at large scale, has remained\na signiﬁcant artiﬁcial intelligence challenge. Here we\npresent the Video Instance Embedding (VIE) frame-\nwork, which trains deep nonlinear embeddings on video\nsequence inputs. By learning embedding dimensions that\nidentify and group similar videos together, while pushing\ninherently diﬀerent videos apart in the embedding space,\nVIE captures the strong statistical structure inherent in\nvideos, without the need for external annotation labels.\nWe ﬁnd that, when trained on a large-scale video dataset,\nVIE yields powerful representations both for action recog-\nnition and single-frame object categorization, showing\nsubstantially improving on the state of the art wherever\ndirect comparisons are possible. We show that a two-\npathway model with both static and dynamic processing\npathways is optimal, provide analyses indicating how the\nmodel works, and perform ablation studies showing the\nimportance of key architecture and loss function choices.\nOur results suggest that deep neural embeddings are a\npromising approach to unsupervised video learning for\na wide variety of task domains.\n1. Introduction\nA video’s temporal sequence often contains informa-\ntion about dynamics and events in the world that is\nricher than that in its unordered set of frames. For\nexample, as objects and agents move and interact with\neach other, they give rise to characteristic patterns of\nvisual change that strongly correlate with their visual\nand physical identities, including object category, ge-\nometric shape, texture, mass, deformability, motion\ntendencies, and many other properties. It is thus an at-\ntractive hypothesis that ubiquitiously available natural\nvideos could serve as a powerful signal for unsupervised\nlearning of visual representations for both static and\ndynamic visual tasks.\nHowever, it has been challenging to embody this hy-\npothesis in a concrete neural network that can consume\nunlabelled video data to learn useful feature represen-\ntations, especially in the context of at-scale real-world\napplications. Perhaps the biggest source of diﬃculty in\nmaking progress on unsupervised video learning, though,\nis that unsupervised learning has presented a formidable\nchallenge even for the case of single static images. Even\nfor the case of single static images, the gap in represen-\ntational power between the features learned by unsu-\npervised and supervised neural networks has been very\nsubstantial, to the point where the former were unsuit-\nable for use in any at-scale visual task. However, recent\nadvances in learning with deep visual embeddings have\nbegun to produce unsupervised representations that\nrival the visual task transfer power of representations\nlearned by their supervised counterparts [37, 36, 40, 3].\nThese methods leverage simple but apparently strong\nheuristics about data separation and clustering to iter-\natively bootstrap feature representations that increas-\ningly better capture subtle natural image statistics. As\na result, it is now possible to obtain unsupervised deep\nconvolutional neural networks that outperform “early\nmodern” deep networks (such as AlexNet in [13]) on\nchallenging recognition tasks such as ImageNet, even\nwhen the latter are trained in a supervised fashion.\nMoreover, works in supervised video classiﬁcation, ac-\ntion recognition and video captioning have proposed\nnovel combinations of two- and three-dimensional con-\nvolutional structures that are increasingly well-suited\nto video feature extraction [5].\nIn this work, we show how the idea of deep un-\nsupervised embeddings can be used to learn features\nfrom videos, introducing the Video Instance Embedding\n(VIE) framework. In VIE, videos are projected into a\ncompact latent space via a deep neural network, whose\nparameters are then tuned to optimally distribute em-\nbedded video instances so that similar videos aggregate\nwhile dissimilar videos separate.\nWe ﬁnd that VIE\narXiv:1905.11954v2  [cs.CV]  10 Mar 2020\nlearns powerful representations for transfer learning to\naction recognition in the large-scale Kinetics dataset, as\nwell as for single-frame object classiﬁcation in the Ima-\ngeNet dataset. Moreover, where direct comparison to\nprevious methods is possible, we ﬁnd that VIE substan-\ntially improves on the state-of-the-art. We evaluate sev-\neral possibilities for the unsupervised VIE loss function,\nﬁnding that those that have been shown to be most ef-\nfective in the single-frame unsupervised learning [40] are\nalso most eﬀective in the video context. We also explore\nseveral neural network embedding and frame sampling\narchitectures, ﬁnding that diﬀerent temporal sampling\nstatistics are better priors for diﬀerent transfer tasks,\nand that a two-pathway static-dynamic architecture is\noptimal. Finally, we present analyses of the learned rep-\nresentations giving some intuition as to how the models\nwork, and a series of ablation studies illustrating the\nimportance of key architectural choices. Codes can be\nfound at https://github.com/neuroailab/VIE.\n2. Related Work\nUnsupervised Learning of Deep Visual Em-\nbeddings. In this work, we employ a framework de-\nrived from ideas ﬁrst introduced in the recent literature\non unsupervised learning of embeddings for images [37].\nIn the Instance Recognition (IR) task, a deep nonlinear\nimage embedding is trained to maximize the distances\nbetween diﬀerent images while minimizing distances\nbetween augmentations (e.g. crops) of a given image,\nthus maximizing the network’s ability to recognize in-\ndividual image instances. In the Local Aggregation\n(LA) task [40], the embedding loss also allows selected\ngroups of images to aggregate, dynamically determin-\ning the groupings based on a local clustering measure.\nConceptually, the LA approach resembles a blending\nof IR and the also-recent DeepCluster method [3], and\nis more powerful than either IR or DeepCluster alone,\nachieving state-of-the-art results on unsupervised learn-\ning with images. Another task Contrastive Multiview\nCoding (CMC) [28] achieves similar performance to\nLA within this embedding framework while aggregat-\ning diﬀerent views of a given image. CMC have also\nbeen directly applied to videos, where single frame is\nclustered with its future frame and its corresponding op-\ntical ﬂow image. The VIE framework allows the use of\nany of these embedding objectives, and we test several\nof them here. Other recently proposed methods such\nas [12] and [2] have achieved comparable results to LA\nand CMC through optimizing the mutual information\nof diﬀerent views of the images, though they use much\ndeeper and more complex architectures.\nSupervised Training of Video Networks. Neu-\nral networks have been used for a variety of supervised\nvideo tasks, including captioning [19] and 3D shape ex-\ntraction [23, 1], but the architectures deployed in those\nworks are quite diﬀerent from those used here. The\nstructures we employ are more directly inspired by work\non supervised action recognition. A core architectural\nchoice explored in this literature is how and where to\nuse 2D single-frame vs 3D multi-frame convolution. A\npurely 2D approach is the Temporal Relational Net-\nwork (TRN) [39], which processes aggregates of 2D\nconvolutional features using MLP readouts. Methods\nsuch as I3D [4] have shown that combinations of both\n2D and 3D can be useful, deploying 2D processing on\nRGB videos and 3D convolution on an optical ﬂow\ncomponent. A current high-performing architecture for\naction recognition is the SlowFast network [5], which\ncomputes mixed 2D-3D convolutional features from se-\nquences of images at multiple time scales, including a\nslow branch for low frequency events and a fast branch\nfor higher-frequency events. The dynamic branch of our\ntwo-pathway architecture is chosen to mimic the most\nsuccessful SlowFast network. However, we ﬁnd it useful\nto include in our architecture a static pathway that is\nnot equivalent to either of the SlowFast branches.\nUnsupervised Learning on Videos. The litera-\nture on unsupervised video learning is too extensive to\nreview comprehensively here, so we focus our discussion\non several of the most relevant approaches. Temporal\nautoencoders such as PredNet [22], PredRNN [35], and\nPredRNN++ [34] are intriguing but have not yet ev-\nidenced substantial transfer learning performance at\nscale. Transfer learning results have been generated\nfrom a variety of approaches including the Geometry-\nGuided CNN [6], motion masks [26], VideoGAN [31], a\npairwise-frame siamese triplet network [33], the Shuf-\nﬂe and Learn approach [24], and the Order Prediction\nNetwork (OPN) [21].\nMore recent works, including\nthe Video Rotations Prediction task (3DRotNet) [16],\nthe Video Motion and Appearance task (MoAp) [32],\nthe Space-Time Puzzle task (ST-puzzle) [18], and the\nDense Predictive Coding (DPC) task [9], have reported\nimproved performance, with the help of pretraining on\nlarge-scale datasets and using spatiotemporal network\narchitectures. All these works only operate on relation-\nships deﬁned within a single video, diﬀerentiating them\nfrom VIE, which exploits the relationships both within\nand between diﬀerent videos through a loss function\ndeﬁned on the distribution of video embeddings.\n3. Methods\nVIE Embedding framework. The general prob-\nlem of unsupervised learning from videos can be formu-\nlated as learning a parameterized function φθ(·) from\ninput videos V = {vi|i = 1, 2, .., N}, where each vi con-\n......\n......\n......\n......\n......\n......\nStatic \nPathway\nV1\nV2\nV3\ne. Memory Bank\nMemory Bank\nf. Video Embedding\nd. Frame Sample Embedding\n. . . . . \n. . . . . \ne.g. 2D-CNN\nDynamic\nPathway\ne.g. 2D-3D-CNN\nc. deep NN\na. video input\nb. sampling\nFigure 1: Schematic of the Video Instance Embedding (VIE) Framework. a. Frames from individual videos (v1,\nv2, v3) are b. sampled into sequences of varying lengths and temporal densities, and input into c. deep neural network\npathways that are either static (single image) or dynamic (multi-image). d. Outputs of frame samples from either pathway\nare vectors in the D-dimensional unit sphere SD ⊂RD+1. The running mean value of embedding vectors are calculated\nover online samples for each video, e. stored in a memory bank, and f. at each time step compared via unsupervised loss\nfunctions in the video embedding space. The loss functions require the computation of distribution properties of embedding\nvectors. For example, the Local Aggregation (LA) loss function involves the identiﬁcation of Close Neighbors Ci (light brown\npoints) and Background Neighbors Bi (dark brown points), which are used to determine how to move target point (green)\nrelative to other points (red/blue).\nsists of a sequence of frames {fi,1, fi,2, ..., fi,mi}. Our\noverall approach seeks to embed the videos {vi} as fea-\nture vectors E = {ei} in the D-dimension unit sphere\nSD = {x ∈RD+1 with ||x||2\n2 = 1}. This embedding\nis realized by a neural network φθ : vi 7→SD with\nweight parameters θ, that receives a sequence of frames\nf = {f1, f2, ..., fL} and outputs e = φθ(f) ∈SD. Al-\nthough the number of frames in one video can be ar-\nbitrary and potentially large, L must usually be ﬁxed\nfor most deep neural networks. Therefore, the input f\non any single inference pass is restricted to a subset of\nframes in v chosen according to a frame sampling strat-\negy ρ — that is, a random variable function such that\nv′ ⊂v for all samples v′ drawn from ρ(v). Given ρ, we\nthen deﬁne the associated Video Instance Embed-\nding (VIE) e for video v as the normed (vector-valued)\nexpectation of e under ρ, i.e.\ne =\nEρ[φθ(f)]\n||Eρ[φθ(f)]||2\n∈SD.\n(1)\nIn addition to choosing φθ and ρ, we must choose a loss\nfunction L : E 7→R such optimizing L with respect to\nθ will cause statistically related videos to be grouped\ntogether and unrelated videos to be separated. Note\nthat in theory this function depends on all of embedded\nvectors, although in practice it is only ever evaluated\non a stochastically-chosen batch at any one time, with\ndataset-wide eﬀects captured via a memory bank.\nIn the following subsections, we describe natural\noptions for these main components (architecture φθ,\nsampling strategy ρ, and loss function L). As shown\nin Section 4, such choices can not only inﬂuence the\nquality of learned representations, but also change the\nfocus of the representations along the spectrum between\nstatic and dynamic information extraction.\nArchitecture φ and sampling strategy ρ. Re-\ncent exploration in supervised action recognition has\nprovided a variety of choices for φθ. Although very com-\nplex network options are possible [4], since this work\nis an initial exploration of the interplay between video\nprocessing architecture and unsupervised loss functions,\nwe have chosen to concentrate on ﬁve simple but distinct\nmodel families. They are diﬀerentiated mainly by how\ntheir frame sampling assumptions represent diﬀerent\ntypes of temporal information about the inputs:\n1. Single-frame 2D-CNNs. Although deep 2D con-\nvolutional neural networks (CNNs) taking one frame\nas input ignore temporal information in the videos,\nthey can still leverage context information and have\nachieved nontrivial performance on action recognition\ndatasets [4]. They are also useful baselines for measur-\ning the eﬀect of including temporal information.\n2. 3D-CNNs with dense equal sampling. 3D-CNNs,\nwith spatiotemporal ﬁlters, can be applied to dense\nevenly-sampled frames to capture ﬁne-grained temporal\ninformation. This architecture has proven useful in the\nR3D networks of [29] and the 3DResNets of [10].\n3. Shared 2D-CNNs with sparse unequal sampling.\nThe Temporal Relation Network bins videos into half-\nsecond clips, chooses L consecutive bins, and randomly\nsamples one frame from each bin. A shared 2D-CNN is\nthen applied to each frame and its outputs are concate-\nnated in order and fed into an MLP that creates the\nembedding. Unlike the dense sampling approach, this\nmethod can capture long-range temporal information\nthrough sparse sampling, but as the intervals between\nframes are uneven, the temporal signal can be noisy.\n4. 2D-3D-CNNs with sparse equal sampling. Ad-\ndressing the issue of noisy temporal information in the\nthird family, the Slow branch of the SlowFast [5] archi-\ntecture samples frames equally but sparsely from the\ninput video. These frames are then passed through 2D-\nCNNs with spatial pooling, applying 3D convolutions\ndownstream once spatial redundancy is reduced.\n5. Multi-pathway architectures. Combination archi-\ntectures allow the exploitation of the multiple temporal\nscales present in natural videos, including the SlowFast\napproach [5] (combining 2 and 4), and true two-pathway\nnetworks with both static and dynamic pathways (com-\nbining 1, 2, and 4).\nIn our experiments (§4), we implement these models\nwith CNN backbones that, while accommodating small\nunavoidable diﬀerences due to the input structure, are\notherwise as similar as possible, so that the qualities of\nlearned representations can be fairly compared.\nLoss function L.\nRecent work in unsupervised\nlearning with single images has found useful generic\nmetrics for measuring the quality of deep visual em-\nbeddings [37, 40], including the Instance Recognition\n(IR) and Local Aggegation (LA) loss functions. These\nmethods seek to group similar inputs together in the\nembedding space, while separating dissimilar inputs.\nThey are based on a simple probabilistic perspective for\ninterpreting compact embedding spaces [37, 40]. Specif-\nically, the probability that an arbitrary feature e is\nrecognized as a sample of vi is deﬁned to be:\nP(i|e, E) =\nexp(eT\ni e/τ)\nPN\nj=1 exp(eT\nj e/τ)\n(2)\nwhere temperature τ ∈(0, 1] is a ﬁxed scale hyperpa-\nrameter. Both {ei} and e are projected onto the unit\nsphere SD. With this deﬁnition in mind, we can deﬁne\nthe IR and LA loss functions, adapted to the video\ncontext via eq. 1.\nIR algorithm. The VIE-version of the IR loss is:\nLIR(vi, E) = −log P(i|e, E) + λ∥θ∥2\n2\n(3)\nwhere λ is a regularization hyperparameter, and\nwhere for computational eﬃciency the denominator\nin P(i|e, E) is estimated through randomly choosing\na subset of Q out of all N terms (see [37] for further\ndetails). Intuitively, optimizing this loss will group em-\nbeddings of frame groups sampled from the same video,\nwhich then implicitly gathers other similar videos.\nLA algorithm. Local Aggregation augments IR by\nallowing for a more ﬂexible dynamic detection of which\ndatapoints should be grouped together.\nDeﬁne the\nprobability that a feature e is recognized as being in a set\nof videos A as P(A|e, E) = P\ni∈A P(i|e, E). For a video\nvi and its embedding ei, the LA algorithm identiﬁes two\nsets of neighbors, the close neighbors Ci and background\nneighbors Bi. Ci is computed via dynamic online k-\nmeans clustering, and identiﬁes datapoints expected\nto be “especially similar” to vi; Bi is computed via k-\nnearest neighbors method, and sets the scale of distance\nin terms of which closeness judgements are measured.\nGiven these two neighbor sets, the local aggregation\nloss function measures the negative log-likelihood of a\npoint being recognized as a close neighbor given that is\nalready a background neighbor:\nLLA(xi, E) = −log P(Ci ∩Bi|vi, E)\nP(Bi|vi, E)\n+ λ∥θ∥2\n2.\n(4)\nIntuitively, the LA loss function encourages the emer-\ngence of soft clusters of datapoints at multiple scales.\nSee [40] for more details on the LA procedure.\nMemory Bank. Both the IR and LA loss functions\nimplicitly require access to all the embedded vectors E\nto calculate their denominators. However, recomputing\nE is intractable for big dataset. This issue is addressed,\nas in [37, 36, 40], by approximating E with a memory\nbank ¯E keeping a running average of the embeddings.\n4. Experiments and Results\nExperimental settings. To train our models, we\nuse the Kinetics-400 dataset [17], which contains ap-\nproximately 240K training and 20K validation videos,\neach around 10 seconds in length and labeled in one of\n400 action categories. After downloading, we standard-\nize videos to a framerate of 25fps and reshape all frames\nso that the shortest edge is 320px. After sampling ac-\ncording to the frame sampling strategy for each model\narchitecture, we then apply the spatial random cropping\nand resizing method used in [5]. Following [40] we also\napply color noise and random horizontal ﬂip, using the\nsame spatial window and color noise parameters for all\nthe frames within one video. At test time, we sample\nﬁve equally-spaced sequences of frames, resize them\nso that their shortest side is 256px, and take center\n224 × 224 crop. Softmax logits for the ﬁve samples are\nthen averaged to generate the ﬁnal output prediction.\nWe use ResNet-18v2 [11] as the convolutional back-\nbone for all our model families, to achieve a balance\nbetween model performance and computation eﬃciency.\nImplementations for the diﬀerent model families are de-\nnoted VIE-Single (“Family 1”), VIE-3DResNet1 (“Fam-\nily 2”), VIE-TRN (“Family 3”), VIE-Slow (“Family 4”),\nand VIE-SlowFast (“Family 5”). Two-pathway models\nare created by concatenating single- and multi-frame\nnetwork outputs, yielding VIE-TwoPathway-S (combin-\ning VIE-Single and VIE-Slow) and VIE-TwoPathway-\nSF (combining VIE-Single and VIE-Slowfast). We fol-\nlow [40] for general network training hyperparameters\nincluding initial learning rate, optimizer setting, learn-\ning rate decay schedule, batch size, and weight decay\ncoeﬃcient. Further details of model architecture and\ntraining can be found in the supplementary material.\nTransfer to action recognition on Kinetics. Af-\nter training all models on the unlabelled videos from\nKinetics, we evaluate the learned representations by\nassessing transfer learning performance to the Kinetics\naction recognition task. To compare our method to pre-\nvious work on this task, we reimplement three strong\nunsupervised learning algorithms: OPN [21], 3DRot-\nNet [16], and RotNet [7]. As OPN and RotNet require\nsingle-frame models, we use ResNet18 as their visual\nbackbones. For 3DRotNet, we use 3DResNet18 with\nthe same input resolution as VIE-3DResNet. Reimple-\nmentation details are in the supplementary material.\nTransfer learning is assessed through the standard trans-\nfer procedure of ﬁxing the learned weights and then\ntraining linear-softmax readouts from diﬀerent layers of\nthe ﬁxed model. We implemented this procedure follow-\ning the choices in [40], but using the Kinetics-speciﬁc\ndata augmentation procedure described above. Since\nsome of the models generate outputs with a temporal\ndimension, directly adding a fully-connected readout\nlayer would lead to more trainable readout parameters\nas compared to single frame models. To ensure fair com-\nparisons, we thus average the features of such models\nalong the temporal dimension before readout.\nResults are shown in Table 1. All VIE variants show\nsigniﬁcantly better performance than OPN, RotNet,\nand 3DRotNet. Multi-frame models substantially out-\nperform single-frame versions, an improvement that\ncannot be explained by the mere presence of addi-\ntional frames (see the supplementary material Table\nS1). The two-pathway models achieve the highest per-\nformance, with a maximum accuracy of approximately\n48.5%. The rank order of unsupervised performances\nacross VIE variants are aligned with that of supervised\ncounterparts, indicating that the unsupervised training\n1To be comparable to previous work, our 3DResNet uses a\nlower input resolution, 112×112. See the supplementary material.\nprocedure takes advantage of increased architectural\npower when available. The LA-based VIE-Single model\nperforms better than the IR-based model, consistent\nwith the gap on static object recognition [40]. Finally,\nthough previous work on unsupervised video learning\nhas started to leverage large-scale datasets such as Ki-\nnetics [9, 16, 18], the transfer learning performance of\nthe trained models to Kinetics action recognition has\nnever been reported. We therefore hope these results\nare useful both for understanding the eﬀect of architec-\nture on representation quality and providing a strong\nunsupervised benchmark for future works.\nTransfer to action recognition on UCF101\nand HMDB51. To compare VIE to previous meth-\nods, we evaluate results on the more commonly-used\nUCF101 [27] and HMDB51 [20] action recognition\nbenchmarks.\nWe initialize networks by pretrained\nweights on Kinetics and then ﬁnetune them on these\ndatasets, following the procedures used for the most\nrecent work [16, 32, 18]. We notice that details of the\ndata augmentation pipelines used during ﬁnetuning can\ninﬂuence ﬁnal results. Most importantly, we ﬁnd having\ncolor-noise augmentation can improve the ﬁnetuing per-\nformance. However, the augmentation techniques have\nnot been carefully controlled in previous works. For ex-\nample, ST-puzzle [18], 3DRotNet [16], and MoAp [32]\nonly use the usual random crop and horizontal ﬂip\naugmentations, while DPC [9] also uses color-noise. To\nensure that our comparisons to these algorithms are fair,\nwe therefore test our models with both augmentation\npipelines. Details can be found in the supplement.\nTable 2 and 3 show that VIE substantially outper-\nforms other methods. Making these comparisons re-\nquires some care, because results reported in previous\nworks are often confounded by the variation of multiple\nfactors at once, making it hard to determine if im-\nprovements are really due to a better algorithm, rather\nthan a larger training dataset, a more powerful net-\nwork architecture, or a diﬀerence in input data types.\nFirst, holding network architecture and training dataset\nﬁxed, VIE-3DResNet surpasses the previous state-of-\nthe-art (ST-Puzzle) by 6.5% on UCF101 and 11.1%\non HMDB51, approaching the supervised upper-bound.\nWith the better augmentation pipeline, the improve-\nment on UCF101 is 9.7%.\n3DRotNets are trained\non Kinetics-600, which contains more than twice as\nmuch training data and 50% more categories than that\nused for ST-Puzzle and VIE models, and are trained\nwith larger inputs (64-frame RGB vs VIE’s 16-frame\ninputs). Nonetheless, VIE still shows improvement of\nmore than 6.2% when compared with comparable in-\nput type. 3DRotNet also reports results for a larger\nfused model trained with both 64-frame RGB and frame-\nDatasets\nKinetics\nImageNet\nMetric\nSuper.\nConv3\nConv4\nConv5\nConv3\nConv4\nConv5\nRandom-Single\n–\n9.40\n8.43\n6.84\n7.98\n7.78\n6.23\nOPN-Single* [21]\n–\n16.84\n20.82\n20.86\n13.01\n17.63\n18.29\nRotNet-Single* [7]\n–\n26.25\n30.27\n23.33\n25.77\n27.59\n16.13\n3DRotNet-3DResNet* [16]\n–\n28.30\n29.33\n19.33\n23.34\n22.05\n12.45\nVIE-Single (IR)\n57.59\n23.50\n38.72\n43.85\n22.85\n40.49\n40.43\nVIE-Single\n57.59\n23.84\n38.25\n44.41\n25.02\n40.49\n42.33\nVIE-TRN\n59.43\n25.72\n39.38\n44.91\n27.24\n40.28\n37.44\nVIE-3DResNet\n53.22\n33.01\n41.34\n43.40\n30.18\n35.37\n32.62\nVIE-Slow\n60.84\n24.80\n40.48\n46.36\n20.10\n37.02\n37.45\nVIE-Slowfast\n62.36\n28.68\n42.07\n47.37\n22.61\n36.84\n36.60\nVIE-TwoPathway-S\n–\n26.38\n41.80\n47.13\n23.98\n40.52\n44.02\nVIE-TwoPathway-SF\n–\n29.89\n43.50\n48.53\n23.23\n40.73\n43.69\nSupervised-Single\n–\n22.32\n37.82\n38.26\nSupervised-TRN\n22.82\n41.13\n39.15\nSupervised-3DResNet\n28.09\n34.40\n30.56\nSupervised-Slow\n21.86\n40.77\n32.87\nSupervised-SlowFast\n20.25\n37.41\n30.75\nTable 1: Top-1 transfer learning accuracy (%) on the Kinetics and ImageNet validation sets. “Random” means a randomly\ninitialized ResNet-18 without any training. “Supervised-*” means trained on Kinetics for action recognition. Our supervised\nperformance is not directly comparable to [5] due to diﬀerent visual backbones used. *: These numbers are generated by us.\ndiﬀerence inputs. VIE-Slowfast nonetheless substan-\ntially outperforms this model, using fewer trainable\nparameters (21M vs 32M) and much less training data\n(Kinetics-400 vs Kinetics-600). When compared to the\nvery recent DPC algorithm [9] in Table 3, VIE shows\nimprovement of more than 7.3%, even though DPC\nuses more frames (40 v.s. 16) as inputs and adds an ad-\nditional recurrent cell between the action readout layer\nand its 3DResNet. Again, VIE makes good use of more\ncomplex architectures, as can be seen in the SlowFast\nvs ResNet18 comparison. Moreover, TwoPathway-SF\nachieves even better performance than SlowFast.\nTransfer to static object categorization.\nTo\ndetermine the extent to which the VIE procedure learns\ngeneral visual representations, we also evaluate the\nlearned representations for transfer to image categoriza-\ntion in ImageNet. For models requiring multi-frame\ninputs, we generate a “static video” by tiling still images\nacross multiple frames. Results are shown in Table 1.\nAs they for the action recognition transfer, the two-\npathway models are highest performing for this task\nas well. Interestingly, however, unlike for the case of\naction recognition, the multi-frame dynamic models are\nsubstantially worse than the single frame models on the\nImageNet transfer task, and show a performance drop\nat the highest convolutional layers. In fact, the transfer\nperformance of the single-frame unsupervised model\ntrained on Kinetics is actually better than that of any\nmodel supervised on Kinetics. Taken together, these re-\nNetworks\nAlgorithms\nUCF\nHMDB\nAlexNet†\nCMC [28]\n59.1\n26.7\nOPN [21]\n56.3\n22.1\nC3D\nMoAp [32]\n61.2\n33.4\nST-puzzle [18]\n60.6\n28.3\n3DResNet\nScratch\n47.4\n21.5\nST-puzzle\n65.8\n33.7\n3DRotNet [16]\n62.9\n33.7\n3DRotNet(64f) [16]\n66.0\n37.1\nVIE (ours)\n72.3\n44.8\nSupervised\n84.4\n58.7\nSlowFast\nScratch\n55.8\n21.4\nVIE (ours)\n77.0\n46.5\nSupervised\n88.4\n68.4\nResNet18\nScratch\n46.7\n17.3\nVIE (ours)\n71.2\n38.4\nSupervised\n81.0\n49.9\nVIE-TwoPathway-SF\n78.2\n50.5\nTable 2: Top-1 ﬁnetuning results on UCF101 and HMDB51\ndatasets using models pretrained on Kinetics without color-\nnoise augmentation. We also provide performance of training\nfrom scratch (“Scratch”) and from supervisedly trained mod-\nels on Kinetics (“Supervised”). For 3DRotNet, we compare\nto its model trained with RGB inputs, where 64f means 64\nframes. †: AlexNet results are all pretrained on UCF101.\nsults strongly motivate the two-pathway architecture, as\nNetworks\nAlgorithms\nUCF\nHMDB\n3DResNet\nScratch\n60.0\n27.0\nDPC [9]\n68.2\n34.5\nVIE (ours)\n75.5\n44.6\nSupervised\n84.8\n60.2\nSlowFast\nScratch\n70.0\n37.0\nVIE (ours)\n78.9\n50.1\nSupervised\n89.7\n70.4\nResNet18\nScratch\n57.3\n23.9\nVIE (ours)\n73.1\n41.2\nSupervised\n83.5\n52.9\nVIE-TwoPathway-SF\n80.4\n52.5\nTable 3: Top-1 ﬁnetuning results on UCF101 and HMDB51\ndatasets using models pretrained on Kinetics with color-\nnoise augmentation.\nfeatures that contribute to high performance on action\nrecognition — e.g. processing of dynamical patterns\n— are not optimal for static-image performance. How-\never, the relatively high performance of the static and\ntwo-pathway models shows that VIE can achieve useful\ngeneralization, even when train and test datasets are\nas widely divergent as Kinetics and ImageNet.\n5. Analysis\nBeneﬁt from long-range temporal structure.\nA key idea in VIE is to embed entire videos into the\nlatent space, which is intended to leverage contextual\ninformation contained in the video. This may even\nhave utility for videos with multiple scenes containing\nwidely-diverging content (common for Kinetics), as the\nhigh-dimensional embedding might learn to situate such\nvideos in the latent space so as to retain this structure.\nAs a preliminary test of the validity of this approach, we\ngenerated new training datasets by dividing each video\ninto equal-length bins and then use these temporally-\nclipped datasets to train VIE. Table 4 show that the\nfull-video model outperforms both 2- and 5-bin models,\nespecially on ImageNet transfer learning performance,\nsupporting the choice of embedding entire videos and\nalso indicating that even better performance may be ob-\ntained using longer, more contextually complex videos.\nBeneﬁt from more data. Although VIE achieves\nstate-of-the-art unsupervised transfer performance on\naction recognition, and creates representations more\nuseful for static object categorization than supervision\non action recognition, its learned representation is (un-\nsurprisingly) worse on ImageNet categorization than\nits counterpart directly trained on the (much larger)\nImageNet training set [40]. To test whether VIE would\nbeneﬁt from more videos, we retrain VIE-Single with\nsubsampled Kinetics (see Table 4). Performance on\nImageNet increases consistently and substantially with-\nout obvious saturation, indicating VIE’s representation\ngeneralizability would beneﬁt substantially if trained\non a video dataset of the scale of ImageNet.\nVideo retrieval. We conduct a video retrieval ex-\nperiment using distance in the embedding space. Repre-\nsentative examples are shown in Figure 2. Oftentimes,\nqualitatively similar videos were retrieved, although\nsome failure cases were also observed. Moreover, VIE-\nSlowfast appears to extract context-free dynamic infor-\nmation, while VIE-Single is more biased by per-frame\ncontext, further validating the idea that multi-frame\nmodels develop representations focusing on the dynamic\nfeatures, while single-frame models better extract static\ninformation. For example, in the “cleaning shoes” query,\nthe two nearest VIE-Slowfast neighbors share a common\ndynamic (hand motions) with the query video, while\nhand and shoe position and the backgrounds all vary.\nMeanwhile, VIE-Single only captures object semantics\n(the presence of the hand), lacking information about\nthe movement that hand will make. Retrieval failures\nlikewise exemplify this result: in the bandaging and bak-\ning cookies examples, VIE-Slowfast captures high-level\nmotion patterns inaccessible to the static pathway.\n6. Conclusion\nWe have described the VIE method, an approach\nthat combines multi-streamed video processing archi-\ntectures with unsupervised deep embedding learning,\nand shown initial evidence that deep embeddings are\npromising for large-scale unsupervised video learning.\nIn this work, our goal is not so much to illustrate the\ndominance of one particular novel loss function and ar-\nchitecture, but rather to provide a clear demonstration\nof the fact that a previously very challenging goal —\nunsupervised learning on at-scale video tasks — has be-\ncome substantially more eﬀective that it had previously\nbeen, due to a combination of very recent architectural\nand loss-function ideas. Together with recent results\nin static image categorization [40, 37, 28, 3], our re-\nsults suggest that the deep embedding approach is an\nincreasingly viable framework for general unsupervised\nlearning across many visual tasks.\nWe’ve also found that performance across diﬀerent\ntypes of visual tasks depends in an understandable\nfashion on how frame sampling is accomplished by the\narchitecture, and that a two-pathway model with both\nstatic and dynamic pathways is relatively better than\neither alone. This result is, interestingly, consistent with\nobservations from neuroscience, where it has been shown\nthat both ventral stream [38] and dorsal stream [8] brain\npathways contribute to visual performance, with the\nformer being more sensitive to static objects and the\nDataset\nKinetics\nImageNet\nLayer\nConv3\nConv4\nConv5\nConv3\nConv4\nConv5\nVIE-Single\n23.84\n38.25\n44.41\n25.02\n40.49\n42.33\n70%-VIE-Single\n26.18\n38.87\n43.59\n23.05\n39.63\n39.85\n30%-VIE-Single\n25.54\n37.49\n40.72\n23.33\n38.49\n36.23\n2bin-VIE-Single\n24.54\n39.16\n44.24\n25.55\n41.43\n39.36\n5bin-VIE-Single\n25.17\n38.73\n43.33\n23.90\n40.46\n37.83\nTable 4: Top-1 accuracy (in %) of transfer learning to Kinetics and ImageNet from VIE-Single models trained using diﬀerent\namount of videos or with videos cut into diﬀerent number of bins.\nVIE-Slowfast\nPred:\nfront raises\nPred:\nside kick\nGT:\nbandaging\nPred:\ncleaning shoes\nPred:\nwaxing legs\nGT:\ncleaning shoes\nPred:\nmaking pizza\nPred:\nbaking cookies\nGT:\nbaking cookies\nGT:\nLong jump\nQuery\nVIE-Single\nPred:\nLong jump\nPred:\nLong jump\nFigure 2: Video retrieval results for VIE-Single and VIE-Slowfast models from Kinetics validation set. GT=ground truth\nlabel, Pred=model prediction. For each query video, three nearest training neighbors are shown. Red font indicates an error.\nlatter more sensitive to dynamic stimuli.\nHowever, there are a number of critical limitations\nin the current method that will need to be overcome\nin future work. Our choice of Local Aggregation for\nevaluation in this work should not taken as a statement\nthat it is the only way to achieve useful unsupervised\ndeep neural embeddings. In particular, exploring the\nuse of other very recent unsupervised learning losses,\nsuch as CMC [28] and DIM [14], will also be of great\ninterest. We expect many of these methods would be\ncompatible with the general VIE framework, and would\nlikely be usefully complementary to LA.\nAnother natural direction for improvement of the\narchitecture is to investigate the use of recurrent neural\nnetwork motifs [15, 25] and attentional mechanisms [30].\nIt is likely that improved architectures of these or\nsome other type could better take advantage of the\nrich medium-range temporal structure (e.g. ∼1s-1m in\nlength) of natural video sequences.\nFurther, our current results are likely impacted by\nlimitations in the Kinetics dataset, especially for har-\nnessing the importance of dynamic processing, since\neven in the supervised case, single-frame performance\nis comparatively high. Seeking out and evaluating VIE\non additional datasets will be critical — perhaps most\nimportantly, for applications involving large and previ-\nously uncurated video data where the potential impact\nof unsupervised learning is especially high. It will also\nbe critical to test VIE in video task domains other than\nclassiﬁcation, including object tracking, dynamic 3D\nshape reconstruction and many others.\nAcknowledgments. This work was supported by the\nMcDonnell Foundation (Understanding Human Cogni-\ntion Award Grant No. 220020469), the Simons Foun-\ndation (Collaboration on the Global Brain Grant No.\n543061), the Sloan Foundation (Fellowship FG-2018-\n10963), the National Science Foundation (RI 1703161\nand CAREER Award 1844724), and the National Insti-\ntutes of Health (R01 MH069456), and hardware dona-\ntion from the NVIDIA Corporation.\nReferences\n[1] Amir Akbarzadeh, J-M Frahm, Philippos Mordohai,\nBrian Clipp, Chris Engels, David Gallup, Paul Merrell,\nM Phelps, S Sinha, B Talton, et al. Towards urban\n3d reconstruction from video. In Third International\nSymposium on 3D Data Processing, Visualization, and\nTransmission (3DPVT’06), pages 1–8. IEEE, 2006.\n[2] Philip Bachman, R Devon Hjelm, and William Buch-\nwalter.\nLearning representations by maximizing\nmutual information across views.\narXiv preprint\narXiv:1906.00910, 2019.\n[3] Mathilde Caron, Piotr Bojanowski, Armand Joulin,\nand Matthijs Douze. Deep clustering for unsupervised\nlearning of visual features. In Proceedings of the Euro-\npean Conference on Computer Vision (ECCV), pages\n132–149, 2018.\n[4] Joao Carreira and Andrew Zisserman. Quo vadis, action\nrecognition? a new model and the kinetics dataset.\nIn proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 6299–6308, 2017.\n[5] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik,\nand Kaiming He. Slowfast networks for video recogni-\ntion. arXiv preprint arXiv:1812.03982, 2018.\n[6] Chuang Gan, Boqing Gong, Kun Liu, Hao Su, and\nLeonidas J Guibas. Geometry guided convolutional\nneural networks for self-supervised video representation\nlearning. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages 5589–\n5597, 2018.\n[7] Spyros Gidaris, Praveer Singh, and Nikos Komodakis.\nUnsupervised representation learning by predicting im-\nage rotations. arXiv preprint arXiv:1803.07728, 2018.\n[8] Melvyn A Goodale and A David Milner.\nSeparate\nvisual pathways for perception and action. Trends in\nneurosciences, 15(1):20–25, 1992.\n[9] Tengda Han, Weidi Xie, and Andrew Zisserman. Video\nrepresentation learning by dense predictive coding. In\nProceedings of the IEEE International Conference on\nComputer Vision Workshops, pages 0–0, 2019.\n[10] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh.\nCan Spatiotemporal 3D CNNs Retrace the History of\n2D CNNs and ImageNet?\nProceedings of the IEEE\nComputer Society Conference on Computer Vision and\nPattern Recognition, pages 6546–6555, 2018.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. Identity mappings in deep residual networks. In\nEuropean conference on computer vision, pages 630–645.\nSpringer, 2016.\n[12] Olivier J Hénaﬀ, Ali Razavi, Carl Doersch, SM Eslami,\nand Aaron van den Oord. Data-eﬃcient image recogni-\ntion with contrastive predictive coding. arXiv preprint\narXiv:1905.09272, 2019.\n[13] Geoﬀrey Hinton, Li Deng, Dong Yu, George Dahl,\nAbdel-rahman Mohamed, Navdeep Jaitly, Andrew Se-\nnior, Vincent Vanhoucke, Patrick Nguyen, Brian Kings-\nbury, et al. Deep neural networks for acoustic modeling\nin speech recognition. IEEE Signal processing magazine,\n29, 2012.\n[14] R Devon Hjelm,\nAlex Fedorov,\nSamuel Lavoie-\nMarchildon, Karan Grewal, Phil Bachman, Adam\nTrischler, and Yoshua Bengio. Learning deep repre-\nsentations by mutual information estimation and maxi-\nmization. arXiv preprint arXiv:1808.06670, 2018.\n[15] Sepp Hochreiter and Jürgen Schmidhuber. Long short-\nterm memory. Neural computation, 9(8):1735–1780,\n1997.\n[16] Longlong Jing, Xiaodong Yang, Jingen Liu, and Yingli\nTian. Self-Supervised Spatiotemporal Feature Learning\nvia Video Rotation Prediction. 2018.\n[17] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio\nViola, Tim Green, Trevor Back, Paul Natsev, et al. The\nkinetics human action video dataset. arXiv preprint\narXiv:1705.06950, 2017.\n[18] Dahun Kim, Donghyeon Cho, and In So Kweon. Self-\nSupervised Video Representation Learning with Space-\nTime Cubic Puzzles. 2018.\n[19] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei,\nand Juan Carlos Niebles. Dense-captioning events in\nvideos. In Proceedings of the IEEE International Con-\nference on Computer Vision, pages 706–715, 2017.\n[20] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and\nT. Serre. HMDB: a large video database for human\nmotion recognition. In Proceedings of the International\nConference on Computer Vision (ICCV), 2011.\n[21] Hsin-Ying Lee, Jia-Bin Huang, Maneesh Singh, and\nMing-Hsuan Yang. Unsupervised representation learn-\ning by sorting sequences. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pages\n667–676, 2017.\n[22] William Lotter, Gabriel Kreiman, and David Cox. Deep\npredictive coding networks for video prediction and un-\nsupervised learning. arXiv preprint arXiv:1605.08104,\n2016.\n[23] Takashi Matsuyama, Xiaojun Wu, Takeshi Takai, and\nShohei Nobuhara. Real-time 3d shape reconstruction,\ndynamic 3d mesh deformation, and high ﬁdelity visu-\nalization for 3d video. Computer Vision and Image\nUnderstanding, 96(3):393–434, 2004.\n[24] Ishan Misra, C Lawrence Zitnick, and Martial Hebert.\nShuﬄe and learn: unsupervised learning using tem-\nporal order veriﬁcation. In European Conference on\nComputer Vision, pages 527–544. Springer, 2016.\n[25] Aran Nayebi, Daniel Bear, Jonas Kubilius, Kohitij Kar,\nSurya Ganguli, David Sussillo, James J DiCarlo, and\nDaniel L Yamins. Task-driven convolutional recurrent\nmodels of the visual system. In Advances in Neural In-\nformation Processing Systems, pages 5290–5301, 2018.\n[26] Deepak Pathak, Ross Girshick, Piotr Dollár, Trevor\nDarrell, and Bharath Hariharan. Learning features by\nwatching objects move. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recogni-\ntion, pages 2701–2710, 2017.\n[27] Khurram Soomro, Amir Roshan Zamir, and Mubarak\nShah. UCF101: A dataset of 101 human actions classes\nfrom videos in the wild. CoRR, abs/1212.0402, 2012.\n[28] Yonglong Tian, Dilip Krishnan, and Phillip Isola.\nContrastive\nmultiview\ncoding.\narXiv\npreprint\narXiv:1906.05849, 2019.\n[29] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray,\nYann LeCun, and Manohar Paluri. A closer look at\nspatiotemporal convolutions for action recognition. In\nProceedings of the IEEE conference on Computer Vision\nand Pattern Recognition, pages 6450–6459, 2018.\n[30] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In Ad-\nvances in neural information processing systems, pages\n5998–6008, 2017.\n[31] Carl Vondrick, Hamed Pirsiavash, and Antonio Tor-\nralba.\nGenerating videos with scene dynamics.\nIn\nAdvances In Neural Information Processing Systems,\npages 613–621, 2016.\n[32] Jiangliu Wang, Jianbo Jiao, Linchao Bao, Shengfeng\nHe, Yunhui Liu, and Wei Liu. Self-supervised Spatio-\ntemporal Representation Learning for Videos by Pre-\ndicting Motion and Appearance Statistics. 2019.\n[33] Xiaolong Wang and Abhinav Gupta. Unsupervised\nlearning of visual representations using videos. In Pro-\nceedings of the IEEE International Conference on Com-\nputer Vision, pages 2794–2802, 2015.\n[34] Yunbo Wang, Zhifeng Gao, Mingsheng Long, Jianmin\nWang, and Philip S Yu. Predrnn++: Towards a reso-\nlution of the deep-in-time dilemma in spatiotemporal\npredictive learning. arXiv preprint arXiv:1804.06300,\n2018.\n[35] Yunbo Wang, Mingsheng Long, Jianmin Wang, Zhifeng\nGao, and S Yu Philip.\nPredrnn: Recurrent neural\nnetworks for predictive learning using spatiotemporal\nlstms. In Advances in Neural Information Processing\nSystems, pages 879–888, 2017.\n[36] Zhirong Wu, Alexei A Efros, and Stella X Yu. Improv-\ning generalization via scalable neighborhood component\nanalysis. In Proceedings of the European Conference on\nComputer Vision (ECCV), pages 685–701, 2018.\n[37] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua\nLin. Unsupervised feature learning via non-parametric\ninstance discrimination. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recogni-\ntion, pages 3733–3742, 2018.\n[38] Daniel LK Yamins, Ha Hong, Charles F Cadieu,\nEthan A Solomon, Darren Seibert, and James J DiCarlo.\nPerformance-optimized hierarchical models predict neu-\nral responses in higher visual cortex. Proceedings of\nthe National Academy of Sciences, 111(23):8619–8624,\n2014.\n[39] Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio\nTorralba. Temporal relational reasoning in videos. In\nProceedings of the European Conference on Computer\nVision (ECCV), pages 803–818, 2018.\n[40] Chengxu Zhuang, Alex Lin Zhai, and Daniel Yamins.\nLocal aggregation for unsupervised learning of visual\nembeddings. arXiv preprint arXiv:1903.12355, 2019.\nSupplementary Material\nLA Speciﬁc Parameters\nFor the LA-speciﬁc parameters, we use cluster size\nm = 8000 for constructing close neighbors Ci and near-\nest neighbor size k = 512 for constructing background\nneighbors Bi. These parameters depart somewhat from\nthe optimal parameters found in [40], due to the sub-\nstantial diﬀerence in size, and thus density in the em-\nbedding space, between the Kinetics training set (240K\npoints) and the ImageNet dataset used in [40] (1.2M\npoints).\nNetwork Implementation Details\nFor VIE-Single, we directly apply the ResNet-18 ar-\nchitecture and follow exactly the same preprocessing\npipeline as described in the main text.\nFor VIE-3DResNet, in order to be comparable to\nother works [18, 16] which use a smaller input resolution\nfor their networks, we correspondingly scale down our\ninput image size. More speciﬁcally, during training,\nwe ﬁrst resize the chosen frames so that their shortest\nedges are between 128 and 160px and then get 112×112\nimages through random crops. We then apply the same\ncolor noise and random horizontal ﬂip to get the ﬁnal\ninputs to the networks. During testing, the frames are\nresized so that their shortest side is 128px, and then\nthe center 112 × 112 crops are chosen as inputs. Same\nas in [18, 16], the input clip contains 16 consecutive\nframes.\nFor VIE-TRN, we sample four consecutive half-\nsecond bins, and then one frame from each bin, us-\ning ResNet-18 as the shared 2D-CNN across multiple\nframes, with the outputs of the Conv5 concatenated\nchannel-wise and input into a fully-connected layer to\ngenerate the ﬁnal embedding. This is a simpliﬁed ver-\nsion of the TRN, which runs faster and achieves only\nslightly lower supervised action-recognition performance\nthan the full 8-frame TRN introduced in [39].\nFor VIE-Slow and VIE-SlowFast, we follow [5] but\nmodify it to use ResNet-18 rather than ResNet-50. The\nSlow model/pathway evenly samples one frame from\nevery 16 to assemble a 4-frame input sequence, while\nthe Fast pathway samples one frame from every 4 to\nassemble a 16-frame input sequence.\nSingle-frame Models with Multi-frame Inputs\nTo control for the fact that multi-frame models received\nmore total inputs than single-frame models, we also\nbuilt models which, for any given multi-frame model,\ntakes VIE-Single model, applies it to multiple frames\nusing the same sampling strategy as for the multi-frame\nmodel, and then averages across the per-frame outputs\nbefore training the softmax classiﬁer. These models are\ndenoted with Input-Single. And their performance is\nshown in Table S1.\nModels\nConv3\nConv4\nConv5\nTRN-Input-Single\n25.52\n39.25\n44.27\nSlow-Input-Single\n26.17\n39.24\n44.62\nSf-Input-Single\n25.72\n39.38\n44.29\nTable S1: Top-1 transfer learning accuracy (%) on Kinetics\nfor Input-Single models.\nReimplementation Details\nWe reimplemented OPN [21], RotNet [7], and 3DRot-\nNet [16] methods and train them on Kinetics videos,\nas controls for VIE. The implementation of OPN fol-\nlows the procedure described in the paper as closely\nas possible, including input size, motion-related frame\nsampling, the use of frame-wise spatial jittering and\nchannel dropping, and the learning rate schedule. How-\never, for a fair comparison, we use ResNet-18 as the\nOPN backbone. Our OPN implementation achieves\napproximately 40% in the order prediction training task\non Kinetics, similar to that reported in the original\nOPN paper, suggesting it is functioning as intended.\nAs for RotNet and 3DRotNet, we use ResNet-18 and\n3DResNet-18 as the backbones respectively. The input\nresolution for 3DResNet-18 is set as 112×112, matching\nthe input resolution of VIE-3DResNet. Other details\nfollow the procedure described in the original papers.\nFine-tuning Implementation Details\nIn testing for both preprocessing pipelines, each video is\nsplit into consecutive 16-frame clips and the outputs of\nall clips are averaged to get the ﬁnal prediction. As for\nother parameters, the initial learning rate is 0.01 and\nthe weight decay is 1e-4 for the training from scratch.\nFor ﬁnetuning, the initial learning rate is 0.0005 and the\nweight decay is 1e-5. The learning rate is dropped by 10\nafter validation performance saturates. We report the\nresults on the ﬁrst split for both UCF101 and HMDB51,\nwhich should be close to the 3-split average result.\n",
  "categories": [
    "cs.CV",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2019-05-28",
  "updated": "2020-03-10"
}