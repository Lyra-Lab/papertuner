{
  "id": "http://arxiv.org/abs/1905.12330v3",
  "title": "Word-order biases in deep-agent emergent communication",
  "authors": [
    "Rahma Chaabouni",
    "Eugene Kharitonov",
    "Alessandro Lazaric",
    "Emmanuel Dupoux",
    "Marco Baroni"
  ],
  "abstract": "Sequence-processing neural networks led to remarkable progress on many NLP\ntasks. As a consequence, there has been increasing interest in understanding to\nwhat extent they process language as humans do. We aim here to uncover which\nbiases such models display with respect to \"natural\" word-order constraints. We\ntrain models to communicate about paths in a simple gridworld, using miniature\nlanguages that reflect or violate various natural language trends, such as the\ntendency to avoid redundancy or to minimize long-distance dependencies. We\nstudy how the controlled characteristics of our miniature languages affect\nindividual learning and their stability across multiple network generations.\nThe results draw a mixed picture. On the one hand, neural networks show a\nstrong tendency to avoid long-distance dependencies. On the other hand, there\nis no clear preference for the efficient, non-redundant encoding of information\nthat is widely attested in natural language. We thus suggest inoculating a\nnotion of \"effort\" into neural networks, as a possible way to make their\nlinguistic behavior more human-like.",
  "text": "Word-order biases in deep-agent emergent communication\nRahma Chaabouni1,2, Eugene Kharitonov1, Alessandro Lazaric1,\nEmmanuel Dupoux1,2 and Marco Baroni1,3\n1Facebook A.I. Research\n2Cognitive Machine Learning (ENS - EHESS - PSL Research University - CNRS - INRIA)\n3ICREA\n{rchaabouni,kharitonov,lazaric,dpx,mbaroni}@fb.com\nAbstract\nSequence-processing neural networks led to\nremarkable progress on many NLP tasks. As\na consequence, there has been increasing in-\nterest in understanding to what extent they\nprocess language as humans do.\nWe aim\nhere to uncover which biases such models\ndisplay with respect to “natural” word-order\nconstraints.\nWe train models to communi-\ncate about paths in a simple gridworld, us-\ning miniature languages that reﬂect or violate\nvarious natural language trends, such as the\ntendency to avoid redundancy or to minimize\nlong-distance dependencies.\nWe study how\nthe controlled characteristics of our miniature\nlanguages affect individual learning and their\nstability across multiple network generations.\nThe results draw a mixed picture. On the one\nhand, neural networks show a strong tendency\nto avoid long-distance dependencies. On the\nother hand, there is no clear preference for the\nefﬁcient, non-redundant encoding of informa-\ntion that is widely attested in natural language.\nWe thus suggest inoculating a notion of “ef-\nfort” into neural networks, as a possible way to\nmake their linguistic behaviour more human-\nlike.\n1\nIntroduction\nDeep\nneural\nnetworks,\nand\nin\nparticular\n“sequence-to-sequence”\n(Seq2Seq,\nSutskever\net al., 2014) LSTM recurrent networks, attained\nastounding successes in many linguistic domains\n(Goldberg, 2017), but we still have a poor under-\nstanding of their language processing mechanisms\n(Lake and Baroni, 2018). We study here whether\nword-order constraints commonly observed in\nnatural language are also found as “inductive”\nbiases in recurrent networks. We consider three\nsuch constraints. The ﬁrst is temporal iconicity,\ndeﬁned as the tendency of clauses denoting events\nto reﬂect the chronological order of the denoted\nevents (as in Caesar’s veni, vidi, vici; Greenberg,\n1963; Haiman, 1980; Newmeyer, 1992; Radden\nand Dirven, 2007; Diessel, 2008; Marcus and\nCalude, 2010; de Ruiter et al., 2018). The second\nis the need to disambiguate the role of sentence\nconstituents, that can be achieved either by means\nof ﬁxed-word order (e.g., in an SVO language\nthe ﬁrst noun phrase denotes the subject), or by\noverting morphological markers (e.g., the subject\nis marked with nominative case).\nAs the two\nmechanisms are redundant, a trade-off is generally\nobserved, where languages preferentially adopt\none or the other (Comrie, 1981; Blake, 2001).\nFinally, we consider the general tendency of\nlanguages to avoid or minimize long-distance\ndependencies (Hawkins, 1994; Gibson, 1998;\nFutrell et al., 2015).\nAs Futrell et al. (2015)\nobserve, “I checked [it] out”, with one word\nintervening between the verb and the particle\nit composes with, ‘is easier or more efﬁcient\nto produce and comprehend’ than “I checked\n[the place you recommended] out”, with four\nintervening words.\nWe test whether such constraints affect LSTM-\nbased Seq2Seq models. To this end, we train them\nas agents in a simple 2D gridworld environment,\nin which they give and receive navigation instruc-\ntions in hand-designed artiﬁcial languages satis-\nfying or violating the constraints. We ﬁrst study\nwhich languages are harder to learn for individ-\nual agents. Then, we look at the cultural transmis-\nsion of language characteristics through multiple\nagent generations by means of the iterated learn-\ning paradigm (Kirby et al., 2014).1\nOur results suggest a mixed picture.\nLSTM\nagents are partially affected by natural constraints,\nboth in terms of learning difﬁculty and stability\nof patterns through evolution. For example, they\n1Code\nlink:\nhttps://github.com/\nfacebookresearch/brica.\narXiv:1905.12330v3  [cs.CL]  14 Jun 2019\nshow a strong tendency to avoid long-distance de-\npendencies. Still, some patterns are considerably\ndifferent from those encountered in human lan-\nguage. In particular, LSTMs generally have a pref-\nerence for the reverse version of an iconic lan-\nguage, and only show a weak tendency towards\navoidance of redundant coding.\n2\nRelated work\nThere is increasing interest in applying methods\nfrom linguistics and psychology to gain insights\non the functioning of language processing net-\nworks, as witnessed by the recent BlackBoxNLP\nworkshop at EMNLP 2018 (Linzen et al., 2018).\nIn this context, researchers have looked at how\ntrained models solve different NLP tasks charac-\nterizing their outputs and internal representation.\nWe instead focus directly on uncovering their “in-\nnate” biases while learning a task.\nWe study whether LSTM-based Seq2Seq mod-\nels deployed as communicating agents are sub-\nject to some of the natural pressures that charac-\nterize the typology and evolution of human lan-\nguages. In this respect, we connect to the recent\nresearch line on language emergence in deep net-\nwork agents that communicate to accomplish a\ntask (e.g., Jorge et al., 2016; Havrylov and Titov,\n2017; Kottur et al., 2017; Lazaridou et al., 2017;\nChoi et al., 2018; Evtimova et al., 2018; Lazari-\ndou et al., 2018; Mordatch and Abbeel, 2018).\nMost of this work provides the agents with a basic\ncommunication channel, and evaluates task suc-\ncess and the emerging communication protocol\nin an entirely bottom-up fashion.\nWe train in-\nstead our agents to communicate with simple lan-\nguages possessing the properties we want to study,\nand look at whether such properties make the lan-\nguages easier or harder to learn.\nOther studies\n(Lee et al., 2017b,a) had also seeded their agents\nwith (real) languages, but for different purposes\n(letting them develop translation skills).\nWe introduce miniature artiﬁcial languages that\nrespect or violate speciﬁc constraints. Other stud-\nies have used such languages with human sub-\njects to test hypotheses about the origin of cross-\nlinguistically frequent patterns (see Fedzechkina\net al., 2016b, for a survey). We follow this ap-\nproach to detect biases in Seq2Seq models. We\nspeciﬁcally rely on two different measures. First,\nwe evaluate the speed of learning a particular lan-\nguage, assuming that the faster it is, the easier its\nproperties are for the agent (e.g., Tily et al., 2011;\nHupp et al., 2009). Second, we look at the cul-\ntural evolution of a language by means of the iter-\nated language learning paradigm (see Kirby et al.,\n2014, for a survey). That is, we investigate the\nchanges that modern Seq2Seq networks exposed\nto a language through multiple generations intro-\nduce, checking which biases they expose.\n3\nExperimental setup\n3.1\nLanguages\nOur environment is characterized by trajectories of\n4 oriented actions (LEFT, RIGHT, UP, DOWN).\nA trajectory contains from 1 to 5 segments, each\ncomposed of maximally 3 steps in the same direc-\ntion. A possible 3-segment trajectory is: LEFT\nLEFT RIGHT UP UP UP, with (LEFT LEFT),\n(RIGHT), and (UP UP UP) being its segments.\nFixed- and free-order languages\nIn a ﬁxed-\norder language, a segment is denoted by a phrase\nmade of a command (C) and a quantiﬁer (Q). An\nutterance speciﬁes an order for the phrases. For\nexample, in the forward-iconic language, 3-phrase\nutterances are generated by the following rules:\n(1)\nU →P1 P2 P3\nP(1|2|3) →C Q\nC →(left|right|up|down)\nQ →(1|2|3)\nShorter and longer utterances are generated analo-\ngously (a N-phrase utterance always has form P1\nP2 . . . PN). Importantly, the interpretation func-\ntion associates PN to the N-th segment in a tra-\njectory, hence the temporal iconicity of the gram-\nmar. For example, the utterance “left 2 right 1 up\n3” denotes the 3-segment trajectory: LEFT LEFT\nRIGHT UP UP UP.\nThe backward-iconic language is analogous,\nbut phrases are interpreted right-to-left.\nNon-\niconic languages use the same interpretation func-\ntion associating PN to the N-th segment, but now\nthe grammar licenses phrases in a ﬁxed order dif-\nferent from that of the trajectory. For example, 3-\nphrase utterances might be generated by U →P2\nP3 P1 (the trajectory above would be expressed\nby: “right 1 up 3 left 2”). Relative phrase ordering\nis ﬁxed across utterances irrespective of length.\nFor example, 2-phrase utterances in the language\nwe just illustrated must be generated by U→P2 P1,\nto respect the ﬁxed-relative-ordering constraint for\nP2 and P1 with respect to the 3-phrase rule.\nFixed-order languages with (temporal ordering)\nmarkers use the same utterance rules, but now\neach phrase PN is also associated with an unam-\nbiguous marker. For example, the iconic+markers\nlanguage obeys the ﬁrst rule in (1), but the phrases\nare expanded by:\n(2)\nP1 →ﬁrst C Q\nP2 →second C Q\nP3 →third C Q\nIn the iconic+markers language, the trajectory\nabove is expressed by “ﬁrst left 2 second right 1\nthird up 3”.\nA free-order language licenses the same phrase\nstructures as a ﬁxed-order language and it uses\nthe same interpretation function, but now there\nare rules expanding utterances with all possible\nphrase permutations (e.g., 3-phrase utterances are\nlicensed by 6 rules: U →P1 P2 P3, U →P1\nP3 P2, . . .).2\nBoth “second right 1 third up 3\nﬁrst left 2” and “third up 3 second right 1 ﬁrst\nleft 2” are acceptable utterances in the free-order\nlanguage with markers. Examples of trajectory-\nto-utterance mappings of these artiﬁcial languages\nare provided in Supplementary\nLong-distance language\nWe consider a long-\ndistance language where any phrase can be split\nand wrapped around a single other phrase so that a\nlong-distance dependency is created between the\ncomponents of the outermost phrase.3\nWe treat\nlong-distance dependencies as optional, as in lan-\nguages in which they are optionally triggered, e.g.,\nby information structure factors. We compare the\nlong-distance language to a local free-order lan-\nguage lacking the long-distance split construction.\nSince the long-distance option causes a combina-\ntorial explosion of possible orders, we limit trajec-\ntories to 3 segments. At the same time, to have two\nlanguages partially comparable in terms of variety\nof allowed constructions, we extend the grammars\nof both to license free order within a phrase. Fi-\nnally, markers are preﬁxed to both the command\nand the quantiﬁer, to avoid ambiguities in the long-\ndistance case. Summarizing, the local language is\nsimilar to the free-order+markers one above, but\nmarkers are repeated before each phrase element,\n2Equivalently, a free-order language is generated in two\nstages from a ﬁxed-order one through a scrambling process.\n3 Note also that this language is projective, excluding\ncross-dependencies.\nand extra rules allow the quantiﬁer to precede or\ngo after the command, e.g., both of the follow-\ning structures are permitted: P1 →ﬁrst Q ﬁrst\nC; P1 →ﬁrst C ﬁrst Q (“ﬁrst left ﬁrst 2”; “ﬁrst 2\nﬁrst left”). The long-distance grammar further in-\ncludes rules where P1 has been split in two parts,\nsuch as:\n(3)\nU →ﬁrst C1 P2 ﬁrst Q1 P3\nU →ﬁrst Q1 P2 ﬁrst C1 P3\nwith C1 and Q1 expandable into the usual termi-\nnals (LEFT, RIGHT...and 1, 2, 3, respectively).4\nThe interpretation function associates a discontin-\nuous {CN, QN} phrase with the N-th segment in\nthe trajectory. The ﬁrst rule in (3) licenses the ut-\nterance “ﬁrst left second right second 1 ﬁrst 2 third\nup third 3”, denoting the example trajectory at the\nbeginning of this section. Similar rules are intro-\nduced for all possible splits of a phrase around an-\nother phrase (e.g., the elements of P2 around P1,\nthose of P1 around P3, etc.). Only one split is\nallowed per-utterance. Examples of trajectory-to-\nutterance mappings in the long and local-distance\nlanguages are provided in Supplementary.\nDatasets\nWe generate sentences associated to all\npossible trajectories in the environment (88572 in\nthe ﬁxed- and free-order language environment,\n972 in the local- and long-distance environment\nexperiments). We randomly split all possible dis-\ntinct trajectory-utterance pairs into training (80%)\nand test/validation sections (10% each).\n3.2\nModels\nArchitecture\nThe agents are Encoder-Decoder\nSeq2Seq architectures (Cho et al., 2014; Sutskever\net al., 2014) with single-layer LSTM recurrent\nunits (Hochreiter and Schmidhuber, 1997).\nIn\nlight of the interactive nature of language, an agent\nis always trained to be both a Speaker, taking a\ntrajectory as input and producing an utterance de-\nscribing it, and as a Listener, executing the tra-\njectory corresponding to an input utterance. Input\nand output vocabularies are identical, and contain\nall possible actions and words.5 When an agent\nplays the Speaker role, it uses input action rep-\nresentations and output word representations, and\nconversely in the Listener role. We tie the embed-\n4Equivalently, long-distance constructions are derived by\nmovement rules from canonical underlying structures.\n5Word and action symbols are disjoint, e.g., the action\nsymbol ‘LEFT’ is different from the word symbol ’left’.\ndings of the encoder input and of the decoder out-\nput (Press and Wolf, 2016) making input and out-\nput representations of words and actions coincide.\nAs a result, Speaker training affects the represen-\ntations used in Listener mode and vice versa. Ex-\nperiments without tying (not reported) show simi-\nlar results with slower convergence. We addition-\nally explore a standard attention mechanism (Bah-\ndanau et al., 2014).\nTraining\nWe consider two scenarios. In indi-\nvidual learning, an agent is taught a language\nby interacting with a hard-coded ground-truth\n“teacher”, represented by the training corpus. In\nthe iterated learning setup, a lineage of agents is\ntrained to speak and listen by interacting with a\n“parent” agent.\nAfter convergence, an agent is\nﬁxed and used as a parent to train the next child.\nIndividual learning\nWe synchronously train the\nagent to speak (from trajectory t to utterance u)\nand listen (from utterance u to trajectory t). Train-\ning the Listener is similar to standard Seq2Seq\ntraining with teacher forcing (Goodfellow et al.,\n2016, p. 376). We change the training procedure\nfor the Speaker direction, as we must handle one-\nto-many trajectory-to-utterance mappings in free-\norder languages. We describe it below.\nFor each trajectory, we consider all correspond-\ning utterances equally probable. Given a trajec-\ntory input, an agent must be able to produce,\nwith equal probability, all utterances that corre-\nspond to the input.\nTo achieve this, taking in-\nspiration from the multi-label learning literature,\nwe ﬁt the agent’s output distribution to minimize\nKL-divergence from the uniform over target utter-\nances. We adopt the “Na¨ıve” method proposed by\nJin and Ghahramani (2003) (see Supplementary\nfor how we derive the loss function in Eq. (4)).\nFormally, our languages map trajectories tj to\none (ﬁxed-order) or multiple (free-order) utter-\nances {u}j = {u1\nj, u2\nj, . . .}. The trajectory t is\nfed into the encoder, which produces a represen-\ntation of the action sequence. Next, the latter is\nfed into the decoder along with the start-of-the-\nsequence element u0 = sos. At each step, the\ndecoder’s output layer deﬁnes a categorical distri-\nbution pθ(uk|uk−1, hk) over the next output word\nuk. This distribution is conditioned by the previ-\nous word uk−1 and the hidden state hk. As with\nthe Listener, we use teacher forcing, so that the\ndistribution of each word is conditioned by the\nground-truth terms coming before it.\nOverall, the model parameters θ are optimized\nto minimize the loss L over (tj, {u}j):\nL = −\nX\nj\n1\nnj\nX\nu∈{u}j\n|u|\nX\nk=1\nlog pθ(uk|uk−1, hj,k)\n(4)\nIn Eq. (4), nj denotes the number of target utter-\nances for the jth example, nj = |{u}j|; u iter-\nates over the utterances {u}j; and uk enumerates\nwords in the utterance u as k varies. As the num-\nber of ground-truth utterances {u}j can be high,\nwe sub-sample n = 6 when training free- and\nﬁxed-order languages.6 This considerably speeds\nup training without signiﬁcantly harming perfor-\nmance. We use all the possible utterances when\ntraining on long-distance languages (n equals the\nthe number of all possible utterances).\nFor all studied languages, we perform a grid\nsearch over hidden layer [16,20] and batch sizes\n[16,32], and report test set results of the best\nvalidation conﬁguration for each language re-\ninitialized with 5 different seeds. We stop train-\ning if development set accuracy does not increase\nfor 5 epochs or when 500 epochs are reached. In\nall scenarios, the optimization is performed with\nthe Amsgrad (Reddi et al., 2018) which is an im-\nproved version of the standard Adam (Kingma and\nBa, 2014); we did not experiment with other opti-\nmizers. We use the algorithm with its default pa-\nrameters, as implemented in Pytorch (Paszke et al.,\n2017).\nIterated learning\nAt “generation 0” agent Aθ0\nis trained individually as described above. Once\nAθ0 is trained, we ﬁx its parameters and use it to\ntrain the next-generation agent, Aθ1. Aθ1, after\ntraining, is in its turn ﬁxed and used to train the\nnext agent Aθ2, etc. At each iteration, the child\nagent Aθi+1 is trained to imitate its parent Aθi as\nfollows. Suppose that, given t, the parent agent\nproduces n7 utterances {ˆu} = {ˆu1, ˆu2, ...ˆun}\n(these utterances are obtained by sampling from\nthe parent’s decoder and can be identical). Then,\nwe train the child agent to: (a) listen: map each ut-\nterance ˆuj to the trajectory t, and (b) speak: given\n6Sampling is trivial in the latter case, since {u}j contains\na single utterance. Note that in this case the loss L reduces\nto the negative log-likelihood. This allows us to use the same\nloss function for free- and ﬁxed-order languages.\n7We use the same number n deﬁned in individual learning\nsection.\nDecoder\nEncoder\nu\nt\nu\nt\nt\nˆui\nt\nDecoder\nEncoder\nu\nt\nu\nt\nˆt\nˆui+1\nDecoder\nEncoder\nu\nt\nu\nt\nˆui+1\nt\nDecoder\nEncoder\nu\nt\nˆt\nu\nt\nˆui+2\nA✓\nA✓i+1\nA✓i+2\nA✓i+1\nt\nGeneration i\nGeneration i + 1\nFigure 1: Iterated learning. Language is transmitted\nto a child agent Aθi+1 by teaching it to speak imitat-\ning the utterances of parent Aθi given the same input\ntrajectories (dashed lines) and to listen to the parent\nutterances, converting them to trajectories (continuous\nlines). After training, former child Aθi+1 becomes the\nparent of a new agent Aθi+2.\nthe trajectory t, produce the utterance ˆu that is\nwithin {ˆu} (Fig. 1). Importantly, even if the par-\nent’s parameters are ﬁxed at each generation, the\nchild agent is allowed, while achieving perfect ac-\ncuracy, to introduce changes into its’ parent lan-\nguage, making the latter more closely aligned with\nits “innate” biases. 8\nImportantly, the language is not forced to re-\nmain stationary across generations.\nEvaluation\nWe evaluate agents both as Listen-\ners and as Speakers. The former is standard, as\neach input u maps to a single output t.\nSince\nthe Speaker can be one-to-many, in order to\nobtain a single prediction u given trajectory t,\nwe predict at each time step k a word u∗\nk\n=\narg maxuk(pθ(uk|u∗\nk−1, hk)).\nThis word is fed\nto the next unit of the decoder, and so on until\nu∗\nK = eos. The ﬁnal prediction ˆu∗is then de-\nﬁned as the sequence [u∗\n1, u∗\n2...u∗\nK], and compared\nto M samples from the true distribution P(u|t).\nIf ˆu∗matches one of the true samples, the agent\nsucceeds, otherwise it fails (in iterated learning,\nP(u|t) corresponds to the parent’s distribution).\nIn other words, we are not evaluating the model on\na perfect ﬁt of the ground-truth (parent’s, in case\nof iterated learning) distribution, but we score a hit\nfor it as long as it outputs a combination in P(u|t).\nThis mismatch between the training and evalua-\ntion criteria allows the emergence of interesting\n8as exempliﬁed in the experiments below, the child can\nreach perfect accuracy while having a different distribution\nover the utterances than its parent.\npatterns (as we allow the agent to drift from the\nground-truth distribution) while constituting a rea-\nsonable measure of actual communication success\n(as the agent produces an utterance that is associ-\nated to the input trajectory in the ground-truth).\n4\nExperiments\n4.1\nIconicity, word order, and markers\nWe compare languages with ﬁxed and free or-\nder, with and without markers. Experiments with\nhumans have shown that, as listeners, children\nperform better with iconic sentences than non-\niconic ones (de Ruiter et al., 2018).\nWe check\nwhether Seq2Seq networks show similar prefer-\nences in terms of learning speed and diachronic\npersistence. We compare in particular the forward-\niconic order with the backward-iconic language,\nand three randomly selected non-iconic languages\nwhere the relation between segment and phrase\norder is ﬁxed but arbitrary. Concerning the rela-\ntion between ﬁxed order and markers, typologi-\ncal studies show a trade-off between these cues.\nFor example, languages with ﬂexible word or-\nder (e.g., Japanese, and Russian) often use case\nto mark grammatical function, whereas languages\nwith ﬁxed word order (such as English and Man-\ndarin) often lack case marking (Blake, 2001; Com-\nrie, 1981). This might be explained by a universal\npreference for efﬁcient and non-redundant gram-\nmatical coding (Fedzechkina et al., 2016a; Qian\nand Jaeger, 2012; Zipf, 1949).\nSeq2Seq agents\nmight show similar preferences when tested as\nSpeakers.\nThat is, they might show a learning\nand preservation preference for either ﬁxed no-\nmarking languages or free marking languages.\nIndividual learning.\nFig. 2 shows test accuracy\nduring learning for each language type. The no-\nattention agent has a preference for backward-\niconic both in speaking and listening. This is in\nline with the observation that Seq2Seq machine\ntranslation models work better when the source\nis presented in reverse order as it makes the op-\ntimization problem easier by introducing shorter-\nterm dependencies (Sutskever et al., 2014). The\n(forward) iconic order is better than the non-iconic\nones in the speaking direction only. The attention-\nenhanced model shows much faster convergence\nto near-perfect communication, with less room for\nclear biases to emerge. Still, we observe some in-\nteresting initial preferences. In speaking mode, the\n(a) Speaker: no attention\n(b) Listener: no attention\n(c) Speaker: attention\n(d) Listener: attention\nFigure 2: Iconicity / Fixed vs. free order: Mean test set accuracy in function of training epoch. Error bars\nrepresent standard deviation over ﬁve random seeds. The NonIconic-average curve pools measurements for 3 non-\niconic languages, each with ﬁve runs. Chance accuracy is represented by the horizontal dotted line. The continuous\nlines represent languages without markers, while the dashed lines represent languages with markers.\nagent learns fastest with the forward iconic lan-\nguage, followed by the backward one. The non-\niconic language without markers is the most difﬁ-\ncult to learn, as expected. On the other hand, in lis-\ntening mode we encounter again a preference for\nbackward iconicity.\nOnly the attention agent in speaking mode\nshows a trade-off between order and markers cod-\ning, with a preference for markers-free ﬁxed-\norder iconic languages over their counterparts\nwith markers, and for the free-order language\nwith markers over the marker-less one. Only the\nnon-iconic languages violate the trend: arguably,\nthough, non-iconic order coding is so sub-optimal\nthat redundant markers are justiﬁed in this case.\nIn listening mode, this agent shows the expected\npreference for markers in the free-order case (as\nthe free-order language without markers is mas-\nsively ambiguous, with most utterances mapping\nto multiple trajectories).\nHowever, among the\nﬁxed-order languages, both backward and non-\niconic prefer redundant coding. The agent with-\nout attention also displays a preference for free-\norder+markers in listening mode (while it has se-\nrious difﬁculties to learn to speak this language),\nbut no clear avoidance for redundant coding in ei-\nther modes. In sum, we conﬁrm a preference for\niconic orders. Only the attention-enhanced agent\nin speaking mode displays avoidance of redundant\ncoding.\nIterated learning.\nIn iterated learning,\nwe\nmight expect the lineage of agents that starts with\nless natural non-iconic languages to either con-\nverge to speak more iconic ones, or possibly to\ndrift into low communication accuracy. We more-\nover expect redundant coding to fade, with ﬁxed-\norder+markers languages to either evolve free or-\nder or lose markers. Regarding the free-word or-\nder marked language, we expect it to either con-\nverge to a ﬁxed order (possibly iconic) while los-\ning its markers, as in the historical development\nfrom Old English (a language with ﬂexible con-\nstituent order and rich case marking) to Modern\nEnglish (a language with ﬁxed constituent order\nand a rudimentary case system) (Traugott, 1972),\nor to remain stable maintaining good communica-\ntion accuracy. We focus on the attention agent,\nas the no-attention one converges too slowly for\nmultiple-generation experiments. We simulate 10\ngenerations, repeating each experiment with 5 dif-\nferent initialization seeds. For non-iconic orders,\nwe sample the same 3 languages sampled for indi-\nvidual learning.\nFor ﬁxed-order languages, we do not observe\nany change in accuracy or behavior in the listener\ndirection (the last-generation child is perfectly\nparsing the initial language).\nHowever, we ob-\nserve in speaker mode a (relatively small) decrease\nin accuracy across generations, which, impor-\ntantly, affects the most natural language (forward\niconic without markers) the least, and the most dif-\nﬁcult language (non-iconic without markers) the\nmost (results are in Supplementary). Again, we\nobserve a (weak) tendency for the attention agent\nto yield to the expected natural pressures.\nWe counted the overall number of markers pro-\nduced by children in speaker mode after conver-\ngence, for all test trajectories in all languages with\nredundant coding. It was always constant, show-\ning no trend towards losing markers to avoid re-\ndundant coding. Similarly, there was no tendency,\nacross generations, to start producing multiple ut-\nterances in response to the same test trajectory.\nIn the evolution of the free-order language with\nmarkers, accuracy was relatively stable in both\nspeaking and listening (99.82% and 100%, re-\nspectively, for the last-generation agent, averag-\ning across 25 runs).9 However, we noticed that\nacross generations, the language becomes more\nﬁxed with some preferred orders emerging. Fig. 3\nquantiﬁes this in terms of the entropy of the ob-\nserved phrase order probabilities across all test\nset trajectories (the lower the entropy, the more\nskewed the distribution). There is already a clear\ndecrease for the ﬁrst agent with respect to the\nground-truth distribution, and the trend continues\nacross generations. We analyzed the distribution\nof Speaker utterances for the longest (5-segment)\ntest trajectories in the last generation. We found\nthat, out of 120 possible phrase orders, no last-\ngeneration agent used more than 10. This is in line\nwith the typological observation that even non-\nconﬁgurational languages favor (at least statisti-\n9We run more simulations in this case as we noticed that\nthe ﬁnal language depends on the initial seed, and hence there\nis high variance with only 5 runs. Speciﬁcally, we start with 5\ndifferent parents and simulate 10 generations, repeating each\nexperiment with 5 different seeds\nFigure 3: Phrase-order entropy in attention Speaker\nutterances given test set trajectories, in function of\ntraining generation (-1 represents the initial ground-\ntruth distribution). Curve represents mean across 25\nruns, with error bars for standard deviations.\ncally) certain orders (Hale, 1992; Mithun, 1992)\nand thus an equiprobable distribution of orders, as\nit is the case in our free word-order+markers lan-\nguage, is unlikely. The “survivor” orders of the\nlast generation were not necessarily iconic but de-\npended notably on the seed. The absence of clear\npreference for a speciﬁc order could be explained\nby the fact that attention-enhanced agents, as we\nsaw, can learn any ﬁxed-order language very fast.\nIn this case, the seed of one generation, by ran-\ndomly skewing the statistics in favor of one or-\nder or the other, can signiﬁcantly impact the pref-\nerence toward the favored order, that will then\nspread diachronically throughout the whole iter-\nation.\n4.2\nLocal vs. long-distance\nWe ﬁnally contrast the long-distance and local lan-\nguages described in Section 3.1. In accordance\nwith the linguistic literature (see Introduction), we\npredict that the long-distance language will be\nharder to learn, and it will tend to reduce long-\ndistance constructions in diachrony. Although ev-\nidence for distance minimization is typically from\nproduction experiments (e.g., Futrell et al., 2015),\nwe expect long-distance constructions to also be\nharder in perception, as they cannot be fully incre-\nmentally processed and require keeping material\nin memory for longer spans.\nIndividual learning.\nAs the long-distance lan-\nguage includes all utterances from the local lan-\nguage, it might be trivially harder to learn. To ac-\ncount for this, we construct a set of control lan-\nguages by randomly sampling, for each trajectory,\nthe same number of possible utterances for the lo-\ncal and long-distance controls. We report averaged\n(a) Speaker: attention\n(b) Listener: attention\nFigure 4: Long vs. local distance: Mean test set accuracy as a function of training epoch. The error bars corre-\nspond to the standard deviation, calculated over ﬁve random seeds.\nFigure 5: Frequency of the local and long-distance ut-\nterances produced by the attention Speaker in function\nof training epoch. The input trajectories are taken from\nthe test set. Test set accuracies for the four generations\nshown: 99.99%, 87.62%, 84.54%, 79.38%. At Gen-\neration 0, less epochs were run due to early stopping.\nresults for 3 such languages of both kinds. Details\non their construction are in Supplementary.\nFig. 4 shows test set accuracy across 300 train-\ning epochs for the attention model. The results,\nfor speaking and listening, conﬁrm the preference\nfor the local language.\nThe control languages\nare harder to learn, as they impose an arbitrary\nconstraint on free word order, but they display\nthe preference for the local language even more\nclearly. Overall, we see a tendency for listening\nto be easier than speaking, but this cuts across the\nlocal/long-distance division, and it seems to be a\nmore general consequence of free-order languages\nwith markers being easier in parsing than produc-\ntion (cf. the no-attention agent results in Fig. 2).\nResults without attention (not shown) are com-\nparable in general, although the listener/speaker\nasymmetry is sharper, with no difference in dif-\nﬁculty among the 4 languages when listening.\nIterated\nlearning.\nWe\nstudy\nmultiple-\ngeneration\ntransmission\nof\nthe\nlong-distance\nlanguage with the attention agent. To deal with\nthe problem of skewed relative frequency of\nlong-distance and entirely local utterances, the\nSpeaker direction is trained by ensuring that the\noutput utterance set {u} for each input trajectory\nt contains the same number of long-distance\nand local constructions.\nThis is achieved by\nsub-sampling n = 48 long-distance utterances\nto match the number of possible local construc-\ntions. Fig. 5 shows the relative frequency across\ngenerations of local and long-distance utterances\nproduced by the agent as a Speaker in function of\ntraining (one representative seed of 5). As pre-\ndicted, a clear preference for local constructions\nemerges, conﬁrming the presence of a distance\nminimization bias in Seq2Seq models.\n5\nDiscussion\nWe studied whether word-order constraints widely\nattested in natural languages affect learning and\ndiachronic transmission in Seq2Seq agents. We\nfound that some trends follow natural patterns,\nsuch as the tendency to limit word order to\nfew conﬁgurations, and long-distance dependency\nminimization. In other ways, our agents depart\nfrom typical human language patterns. For exam-\nple, they exhibit a preference for a backward order,\nand there are only weak signs of a trade-off be-\ntween different ways to encode constituent roles,\nwith redundant solutions often being preferred.\nThe research direction we introduced might\nlead to a better understanding of the biases that af-\nfect the linguistic behaviour of LSTMs and simi-\nlar models. This could help current efforts towards\nthe development of artiﬁcial agents that communi-\ncate to solve a task, with the ultimate goal of devel-\noping AIs that can talk with humans. It has been\nobserved that the communication protocol emerg-\ning in such simulations is very different from hu-\nman language (e.g., Kottur et al., 2017; Lewis\net al., 2017; Bouchacourt and Baroni, 2018). A\nbetter understanding of what are the “innate” bi-\nases of standard models in highly controlled se-\ntups, such as the one studied here, should comple-\nment large-scale simulations, as part of the effort\nto develop new methods to encourage the emer-\ngence of more human-like language. For example,\nour results suggest that current neural networks, as\nthey are not subject to human-like least-effort con-\nstraints, might not display the same trend towards\nefﬁcient communication that we encounter in nat-\nural languages. How to incorporate “effort”-based\npressures in neural networks is an exciting direc-\ntion for future work.\n6\nAcknowledgments\nWe would like to thank Roger Levy, Diane\nBouchacourt, Alex Cristea, Kristina Gulordava\nand Armand Joulin for their very helpful feedback.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014.\nNeural machine translation by jointly\nlearning to align and translate.\narXiv preprint\narXiv:1409.0473.\nBarry Blake. 2001. Case. MIT Press, Cambridge, MA.\nDiane Bouchacourt and Marco Baroni. 2018.\nHow\nagents see things:\nOn visual representations in\nan emergent language game.\nIn Proceedings of\nEMNLP, pages 981–985, Brussels, Belgium.\nKyunghyun Cho, Bart Van Merri¨enboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014.\nLearning\nphrase representations using rnn encoder-decoder\nfor statistical machine translation.\narXiv preprint\narXiv:1406.1078.\nEdward Choi, Angeliki Lazaridou, and Nando de Fre-\nitas. 2018. Compositional obverter communication\nlearning from raw visual input. In Proceedings of\nICLR Conference Track, Vancouver, Canada.\nBenrard Comrie. 1981. Language Universals and Lin-\nguistic Typology. Blackwell, Malden, MA.\nLaura de Ruiter, Anna Theakston, Silke Brandt, and\nElena Lieven. 2018.\nIconicity affects children’s\ncomprehension of complex sentences: The role of\nsemantics, clause order, input and individual differ-\nences. Cognition, 171:202–224.\nHolger Diessel. 2008. Iconicity of sequence: A corpus-\nbased analysis of the positioning of temporal ad-\nverbial clauses in English.\nCognitive Linguistics,\n19(3):465–490.\nKatrina Evtimova, Andrew Drozdov, Douwe Kiela, and\nKyunghyun Cho. 2018. Emergent communication\nin a multi-modal, multi-step referential game.\nIn\nProceedings of ICLR Conference Track, Vancouver,\nCanada.\nMaryia Fedzechkina, Elissa Newport, and T. Florian\nJaeger. 2016a.\nBalancing effort and information\ntransmission during language acquisition: Evidence\nfrom word order and case marking. Cognitive Sci-\nence, 41:n/a–n/a.\nMaryia Fedzechkina, Elissa Newport, and T. Florian\nJaeger. 2016b. Miniature artiﬁcial language learn-\ning as a complement to typological data, pages 211–\n232.\nRichard Futrell, Kyle Mahowald, and Edward Gib-\nson. 2015.\nLarge-scale evidence of dependency\nlength minimization in 37 language. Proceedings of\nthe National Academy of Sciences, 112(33):10336–\n10341.\nEdward Gibson. 1998. Linguistic complexity: Locality\nof syntactic dependencies. Cognition, 68(1):1–76.\nYoav Goldberg. 2017.\nNeural Network Methods for\nNatural Language Processing. Morgan & Claypool,\nSan Francisco, CA.\nIan Goodfellow, Yoshua Bengio, and Aaron Courville.\n2016. Deep Learning. MIT Press.\nJoseph Greenberg. 1963. Some universals of grammar\nwith particular reference to the order of meaningful\nelements. In Joseph Greenberg, editor, Universals of\nHuman Language, pages 73–113. MIT Press, Cam-\nbridge, MA.\nJohn Haiman. 1980. The iconicity of grammar: Iso-\nmorphism and motivation.\nLanguage, 56(3):515–\n540.\nKenneth Hale. 1992.\nBasic word order in two ‘free\nword order’ languages.\nIn Doris Payne, editor,\nPragmatics of word order ﬂexibility, pages 63–82.\nJohn Benjamins, Amsterdam, the Netherlands.\nSerhii Havrylov and Ivan Titov. 2017. Emergence of\nlanguage with multi-agent games: Learning to com-\nmunicate with sequences of symbols. In Proceed-\nings of NIPS, pages 2149–2159, Long Beach, CA,\nUSA.\nJohn Hawkins. 1994.\nA Performance Theory of Or-\nder and Constituency. Cambridge University Press,\nCambridge, UK.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997.\nLong short-term memory.\nNeural Computation,\n9(8):1735–1780.\nJulie M. Hupp, Vladimir M. Sloutsky, and Peter W.\nCulicover. 2009.\nEvidence for a domain-general\nmechanism underlying the sufﬁxation preference\nin language.\nLanguage and Cognitive Processes,\n24(6):876–909.\nRong Jin and Zoubin Ghahramani. 2003.\nLearning\nwith multiple labels. In Advances in neural infor-\nmation processing systems, pages 921–928.\nEmilio Jorge, Mikael K˚ageb¨ack, and Emil Gustavsson.\n2016.\nLearning to play Guess Who? and invent-\ning a grounded language as a consequence. In Pro-\nceedings of the NIPS Deep Reinforcement Learning\nWorkshop, Barcelona, Spain.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nSimon Kirby, Tom Grifﬁths, and Kenny Smith. 2014.\nIterated learning and the evolution of language. Cur-\nrent Opinion in Neurobiology, 28:108–114.\nSatwik Kottur, Jos´e Moura, Stefan Lee, and Dhruv Ba-\ntra. 2017. Natural language does not emerge ‘nat-\nurally’ in multi-agent dialog.\nIn Proceedings of\nEMNLP, pages 2962–2967, Copenhagen, Denmark.\nSolomon Kullback. 1997.\nInformation theory and\nstatistics. Courier Corporation.\nBrenden Lake and Marco Baroni. 2018. Generaliza-\ntion without systematicity: On the compositional\nskills of sequence-to-sequence recurrent networks.\nIn Proceedings of ICML, pages 2879–2888, Stock-\nholm, Sweden.\nAngeliki Lazaridou, Karl Moritz Hermann, Karl Tuyls,\nand Stephen Clark. 2018. Emergence of linguistic\ncommunication from referential games with sym-\nbolic and pixel input. In Proceedings of ICLR Con-\nference Track, Vancouver, Canada.\nAngeliki Lazaridou, Alexander Peysakhovich, and\nMarco Baroni. 2017. Multi-agent cooperation and\nthe emergence of (natural) language. In Proceed-\nings of ICLR Conference Track, Toulon, France.\nJason Lee, Kyunghyun Cho, Jason Weston, and Douwe\nKiela. 2017a. Emergent translation in multi-agent\ncommunication. arXiv preprint arXiv:1710.06922.\nSang-Woo Lee, Yu-Jung Heo, and Byoung-Tak Zhang.\n2017b.\nAnswerer in questioner’s mind for goal-\noriented visual dialogue.\nMike Lewis, Denis Yarats, Yann Dauphin, Devi Parikh,\nand Dhruv Batra. 2017. Deal or no deal? End-to-end\nlearning of negotiation dialogues. In Proceedings of\nEMNLP, pages 2443–2453, Copenhagen, Denmark.\nTal Linzen, Grzegorz Chrupała, and Afra Alishahi,\neditors. 2018.\nProceedings of the EMNLP Black-\nboxNLP Workshop. ACL, Brussels, Belgium.\nSolomon Marcus and Andreea Calude. 2010. Syntactic\niconicity, within and beyond its accepted principles.\nRevue Roumaine de Linguistique, 55(1):19–44.\nMarianne Mithun. 1992. Is basic word order univer-\nsal?\nIn Doris Payne, editor, Pragmatics of word\norder ﬂexibility, pages 15–61. John Benjamins, Am-\nsterdam, the Netherlands.\nIgor Mordatch and Pieter Abbeel. 2018. Emergence\nof grounded compositional language in multi-agent\npopulations. In Thirty-Second AAAI Conference on\nArtiﬁcial Intelligence.\nFrederick Newmeyer. 1992. Iconicity and generative\ngrammar. Language, 68(4):756–796.\nAdam Paszke, Sam Gross, Soumith Chintala, Gre-\ngory Chanan, Edward Yang, Zachary DeVito, Zem-\ning Lin, Alban Desmaison, Luca Antiga, and Adam\nLerer. 2017. Automatic differentiation in pytorch.\nIn NIPS-W.\nOﬁr Press and Lior Wolf. 2016.\nUsing the output\nembedding to improve language models.\narXiv\npreprint arXiv:1608.05859.\nTing Qian and T Florian Jaeger. 2012. Cue effective-\nness in communicatively efﬁcient discourse produc-\ntion. Cognitive science, 36(7):1312–1336.\nG¨unter Radden and Ren´e Dirven. 2007. Cognitive En-\nglish Grammar. John Benjamins, Amsterdam, the\nNetherlands.\nSashank J. Reddi, Satyen Kale, and Sanjiv Kumar.\n2018.\nOn the convergence of Adam and beyond.\nIn International Conference on Learning Represen-\ntations.\nLaura E. de Ruiter, Anna L. Theakston, Silke Brandt,\nand Elena V.M. Lieven. 2018. Iconicity affects chil-\ndrens comprehension of complex sentences: The\nrole of semantics, clause order, input and individual\ndifferences. Cognition, 171:202 – 224.\nIlya Sutskever, Oriol Vinyals, and Quoc Le. 2014. Se-\nquence to sequence learning with neural networks.\nIn Proceedings of NIPS, pages 3104–3112, Mon-\ntreal, Canada.\nHarry Tily, Michael C Frank, and T. Florian Jaeger.\n2011. The learnability of constructed languages re-\nﬂects typological patterns. pages 1364–1369.\nE. C. Traugott. 1972.\nIA history of English syntax.\nNew York: Holt, Rinehart and Winston.\nGeorge Zipf. 1949. Human Behavior and the Principle\nof Least Effort. Addison-Wesley, Boston, MA.\n7\nSupplementary Material\n7.1\nDeriving the training loss for the Speaker\nrole\nIn the Speaker role, each input trajectory tj maps\nonto a set of utterances {u}j. We want to train\nan agent such that, given tj, it generates all the\ncorresponding utterances {u}j uniformly. To do\nthat, we follow the “Na¨ıve” approach from Jin and\nGhahramani (2003).\nGiven an input t, the Seq2Seq model deﬁnes a\ndistribution over the output sequences, pθ(u|tj).\nThe KL-divergence (Kullback, 1997) D(P||pθ)\nbetween the uniform distribution P(u|tj) over the\ntarget utterances {u}j and the output distribution\nof the agent, pθ(u|tj), is:\nD(P||pθ) =Eu∼P(u|tj)\n\u0014\nlog P(u|tj)\npθ(u|tj)\n\u0015\n=E −Eu∼P(u|tj) log pθ(u|tj)\n(5)\nwith E independent from θ. Hence, ﬁnding θ that\nminimizes D(P||pθ) is equivalent to minimization\nof L′:\nL′(tj) = −Eu∼P(u|tj) log pθ(u|tj)\n(6)\nNext, assuming that the target set of utterances\n{uj} has nj elements,\nEu∼P(u|tj) log pθ(u|tj) = 1\nnj\nX\nu∈{u}j\nlog pθ(u|tj)\n(7)\nWe expand pθ(u|tj) by iterating words uk in u, as\nin Section 3.2 of the main text:\nlog pθ(u|tj) =\n|u|\nX\nk=1\nlog pθ(uk|uk−1, hj,k)\n(8)\nBy combining Eq. (7) and Eq. (8), we obtain:\nL′(tj) = −1\nnj\nX\nu∈{u}j\n|u|\nX\nk=1\nlog pθ(uk|uk−1, hj,k)\n(9)\nAfter aggregation over all trajectories in the\ndataset, we obtain the full loss that coincides with\nEq. (4) in the main text:\nX\nj\nL′(tj) =\n−\nX\nj\n1\nnj\nX\nu∈{u}j\n|u|\nX\nk=1\nlog pθ(uk|uk−1, hj,k)\n(10)\nThis concludes our derivation of the loss used for\nthe Speaker role. Finally, we note that this deriva-\ntion provides grounding for the sub-sampling we\nuse during the training, as it corresponds to get-\nting a Monte-Carlo estimate of the expectation in\nEq. (6) over n samples, instead of the full support\nof the distribution.\n7.2\nExamples of trajectories and utterances\nIn Table 1, we exemplify how trajectories with\ntwo, three, and ﬁve segments are represented by\nutterances in free- and ﬁxed-order languages with\nand without markers. Note how the free-order lan-\nguage without markers is extremely ambiguous, as\nthe utterances do not encode the execution order of\nthe corresponding trajectories.\nTables 2 and 3 give examples of how trajec-\ntories are represented by utterances in the local\nand long-distance languages, as well as in example\ncontrols. The control languages are constructed\nto enable a fairer comparison between the local\nand long-distance setups. The full long-distance\nlanguage has more possible utterances per trajec-\ntory than the local one (the latter is a subset of\nthe former).\nTheir controls, however, have the\nsame number of utterances. Practically, to con-\nstruct one local control language, we sample 24\ndistinct utterance templates (that is, phrase or-\nders) out of 48 from the full language. We use\n3 different local control languages by sampling\na different subset each time.\nTable 2 exempli-\nﬁes one of these control languages. To construct\none long-distance control language, we also sam-\nple 24 distinct utterance templates from the full\nlong-distance language (out of 144 possible utter-\nances). The latter sampling maintains the propor-\ntion of local and long-distance constructions of the\nfull long-distance language (1/3 vs. 2/3). Again,\nwe sample 3 different long-distance controls. One\nof them is exempliﬁed in Table 3.\n7.3\nIterated Learning of ﬁxed word order\nlanguages\nIn this section, we use the iterated learning\nparadigm to analyze Seq2Seq networks biases to-\nward iconic languages. We expect agents with less\nnatural non-iconic languages to either converge to\nmore iconic ones or diverge with low communi-\ncation accuracy. We simulate 10 generations re-\npeating the process with 5 different initialization\nseed and report the average of communication ac-\ncuracy of each generation in Fig. 6. We observe\nTable 1: Example utterances associated to trajectories of different lengths in the ﬁxed- and free-order languages\nwe consider.\nTrajectory (two segments):\nLEFT DOWN DOWN\nWith markers\nForward-iconic\nReverse-iconic\nNon-iconic 1\nﬁrst left 1 second down 2\nsecond down 2 ﬁrst left 1\nﬁrst left 1 second down 2\nNon-iconic 2\nNon-iconic 3\nFree-order\nsecond down 2 ﬁrst left 1\nﬁrst left 1 second down 2\nﬁrst left 1 second down 2\nsecond down 2 ﬁrst left 1\nWithout markers\nForward-iconic\nReverse-iconic\nNon-iconic 1\nleft 1 down 2\ndown 2 left 1\nleft 1 down 2\nFree-order\nNon-iconic 2\nNon-iconic 2\nleft 1 down 2\ndown 2 left 1\nleft 1 down 2\ndown 2 left 1\nTrajectory (three segments):\nLEFT LEFT LEFT DOWN DOWN UP UP UP\nWith markers\nNon-iconic 1\nForward-iconic\nReverse-iconic\nﬁrst left 3 third up 3 second down 2\nﬁrst left 3 second down 2 third up 3\nthird up 3 second down 2 ﬁrst left 3\nFree-order\nﬁrst left 3 second down 2 third up 3\nﬁrst left 3 third up 3 second down 2\nNon-iconic 2\nNon-iconic 3\nsecond down 2 third up 3 ﬁrst left 3\nsecond down 2 third up 3 ﬁrst left 3\nﬁrst left 3 second down 2 third up 3\nsecond down 2 ﬁrst left 3 third up 3\nthird up 3 ﬁrst left 3 second down 2\nthird up 3 second down 2 ﬁrst left 3\nWithout markers\nNon-iconic 1\nForward-iconic\nReverse-iconic\nleft 3 up 3 down 2\nleft 3 down 2 up 3\nup 3 down 2 left 3\nFree-order\nleft 3 down 2 up 3\nleft 3 up 3 down 2\nNon-iconic 2\nNon-iconic 3\ndown 2 up 3 left 3\ndown 2 up 3 left 3\nleft 3 down 2 up 3\ndown 2 left 3 up 3\nup 3 left 3 down 2\nup 3 down 2 left 3\nTrajectory (ﬁve segments):\nDOWN RIGHT RIGHT UP UP UP RIGHT LEFT LEFT\nWith markers\nForward-iconic\nReverse-iconic\nNon-iconic 1\nﬁrst down 1 second right 2 third up 3 fourth right 1 ﬁfth left 2\nﬁfth left 2 fourth right 1 third up 3 second right 2 ﬁrst down 1\nﬁrst down 1 fourth right 1 third up 3 second right 2 ﬁfth left 2\nFree-order\nﬁrst down 1 second right 2 third up 3 fourth right 1 ﬁfth left 2\nNon-iconic 2\nNon-iconic 3\nﬁrst down 1 second right 2 third up 3 ﬁfth left 2 fourth right 1\nsecond right 2 third up 3 ﬁfth left 2 fourth right 1 ﬁrst down 1\nfourth right 1 ﬁrst down 1 second right 2 ﬁfth left 2 third up 3\n...\nﬁfth left 2 fourth right 1 third up 3 second right 2 ﬁrst down 1\nWithout markers\nNon-iconic 1\nForward-iconic\nReverse-iconic\ndown 1 right 1 up 3 right 2 left 2\ndown 1 right 2 up 3 right 1 left 2\nleft 2 right 1 up 3 right 2 down 1\nFree-order\ndown 1 right 2 up 3 right 1 left 2\nNon-iconic 2\nNon-iconic 3\ndown 1 right 2 up 3 left 2 right 1\nright 2 up 3 left 2 right 1 down 1\nright 1 down 1 right 2 left 2 up 3\n...\nleft 2 right 1 up 3 right 2 down 1\nFigure 6: Iterated learning with ﬁxed-order lan-\nguages. Mean test set attention Speaker accuracy at\nthe end of training over 10 generations. Error bars rep-\nresent standard deviation over 5 random seeds. The\nNonIconic-average curve pools measurements for 3\nnon-iconic languages, each with 5 runs.\nin speaker mode a (relatively small) decrease in\naccuracy across generations, which, importantly,\naffects the most natural language (forward iconic\nwithout markers) the least, and the most difﬁcult\nlanguage (non-iconic without markers) the most.\nThus, we observe a (weak) tendency for the atten-\ntion agent to yield to the expected natural pres-\nsures in terms of iconic order.\nTable 2: Example utterances associated to one trajectory by the local language and one of its controls.\nTrajectory (three segments): DOWN DOWN DOWN LEFT LEFT LEFT UP\nLocal\nﬁrst down ﬁrst 3 second left second 3 third up third 1\nﬁrst down ﬁrst 3 second left second 3 third 1 third up\nﬁrst down ﬁrst 3 second 3 second left third up third 1\nﬁrst down ﬁrst 3 second 3 second left third 1 third up\nﬁrst down ﬁrst 3 third up third 1 second left second 3\nﬁrst down ﬁrst 3 third up third 1 second 3 second left\n...\n. ..\n.. .\nthird 1 third up ﬁrst 3 ﬁrst down second left second 3\nthird 1 third up ﬁrst 3 ﬁrst down second 3 second left\nthird 1 third up second left second 3 ﬁrst down ﬁrst 3\nthird 1 third up second left second 3 ﬁrst 3 ﬁrst down\nthird 1 third up second 3 second left ﬁrst down ﬁrst 3\nthird 1 third up second 3 second left ﬁrst 3 ﬁrst down\nLocal control (one of three)\nﬁrst down ﬁrst 3 second left second 3 third up third 1\nﬁrst down ﬁrst 3 third up third 1 second left second 3\nsecond 3 second left third up third 1 ﬁrst down ﬁrst 3\nsecond 3 second left third 1 third up ﬁrst down ﬁrst 3\nthird up third 1 second left second 3 ﬁrst down ﬁrst 3\nsecond left second 3 ﬁrst 3 ﬁrst down third 1 third up\nsecond left second 3 ﬁrst down ﬁrst 3 third up third 1\nﬁrst 3 ﬁrst down third up third 1 second 3 second left\nthird up third 1 ﬁrst 3 ﬁrst down second 3 second left\nthird up third 1 ﬁrst down ﬁrst 3 second left second 3\nthird 1 third up second left second 3 ﬁrst 3 ﬁrst down\nthird 1 third up second 3 second left ﬁrst 3 ﬁrst down\nsecond 3 second left ﬁrst 3 ﬁrst down third up third 1\nsecond 3 second left third up third 1 ﬁrst 3 ﬁrst down\nﬁrst 3 ﬁrst down third up third 1 second left second 3\nsecond 3 second left ﬁrst down ﬁrst 3 third 1 third up\nthird 1 third up ﬁrst 3 ﬁrst down second 3 second left\nﬁrst down ﬁrst 3 third 1 third up second left second 3\nthird up third 1 second 3 second left ﬁrst 3 ﬁrst down\nthird up third 1 ﬁrst down ﬁrst 3 second 3 second left\nthird up third 1 second left second 3 ﬁrst 3 ﬁrst down\nﬁrst 3 ﬁrst down second 3 second left third up third 1\nﬁrst 3 ﬁrst down second left second 3 third up third 1\nsecond left second 3 ﬁrst down ﬁrst 3 third 1 third up\nTable 3: Example utterances associated to one trajectory by the long-distance language and one of its controls.\nTrajectory (three segments): DOWN DOWN DOWN LEFT LEFT LEFT UP\nLong-distance\nlocal utterances\nﬁrst down ﬁrst 3 second left second 3 third up third 1\nﬁrst down ﬁrst 3 second left second 3 third 1 third up\nﬁrst down ﬁrst 3 second 3 second left third up third 1\nﬁrst down ﬁrst 3 second 3 second left third 1 third up\nﬁrst down ﬁrst 3 third up third 1 second left second 3\nﬁrst down ﬁrst 3 third up third 1 second 3 second left\n. . .\n...\n.. .\nthird 1 third up ﬁrst 3 ﬁrst down second left second 3\nthird 1 third up ﬁrst 3 ﬁrst down second 3 second left\nthird 1 third up second left second 3 ﬁrst down ﬁrst 3\nthird 1 third up second left second 3 ﬁrst 3 ﬁrst down\nthird 1 third up second 3 second left ﬁrst down ﬁrst 3\nthird 1 third up second 3 second left ﬁrst 3 ﬁrst down\nlong-distance utterances\nﬁrst down ﬁrst 3 second left third up third 1 second 3\nﬁrst down ﬁrst 3 second left third 1 third up second 3\nﬁrst down ﬁrst 3 second 3 third up third 1 second left\nﬁrst down ﬁrst 3 second 3 third 1 third up second left\nﬁrst down ﬁrst 3 third up second left second 3 third 1\nﬁrst down ﬁrst 3 third up second 3 second left third 1\n...\n. ..\n.. .\nthird 1 third up ﬁrst 3 second left second 3 ﬁrst down\nthird 1 third up ﬁrst 3 second 3 second left ﬁrst down\nthird 1 third up second left ﬁrst down ﬁrst 3 second 3\nthird 1 third up second left ﬁrst 3 ﬁrst down second 3\nthird 1 third up second 3 ﬁrst down ﬁrst 3 second left\nthird 1 third up second 3 ﬁrst 3 ﬁrst down second left\nLong-distance control (one of three)\nlocal utterances\nﬁrst down ﬁrst 2 second left second 3 third up third 1\nsecond 3 second left ﬁrst 2 ﬁrst down third up third 1\nthird up third 1 ﬁrst down ﬁrst 2 second left second 3\nthird 1 third up ﬁrst down ﬁrst 2 second 3 second left\nsecond left second 3 ﬁrst down ﬁrst 2 third 1 third up\nsecond 3 second left third 1 third up ﬁrst down ﬁrst 2\nsecond 3 second left third up third 1 ﬁrst 2 ﬁrst down\nsecond left second 3 ﬁrst down ﬁrst 2 third up third 1\nlong-distance utterances\nﬁrst 2 ﬁrst down third up second 3 second left third 1\nthird 1 second 3 second left third up ﬁrst down ﬁrst 2\nﬁrst 2 third up third 1 ﬁrst down second left second 3\nﬁrst 2 second 3 second left ﬁrst down third 1 third up\nsecond 3 third up third 1 second left ﬁrst 2 ﬁrst down\nsecond 3 ﬁrst down ﬁrst 2 second left third 1 third up\nsecond 3 second left ﬁrst down third 1 third up ﬁrst 2\nthird 1 second left second 3 third up ﬁrst 2 ﬁrst down\nsecond left second 3 third up ﬁrst 2 ﬁrst down third 1\nthird 1 third up ﬁrst 2 second 3 second left ﬁrst down\nﬁrst down second left second 3 ﬁrst 2 third 1 third up\nthird 1 ﬁrst down ﬁrst 2 third up second left second 3\nthird up third 1 second left ﬁrst 2 ﬁrst down second 3\nthird up third 1 second 3 ﬁrst down ﬁrst 2 second left\nthird 1 third up second left ﬁrst 2 ﬁrst down second 3\nﬁrst 2 ﬁrst down third 1 second 3 second left third up\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2019-05-29",
  "updated": "2019-06-14"
}