{
  "id": "http://arxiv.org/abs/1905.13105v1",
  "title": "ImJoy: an open-source computational platform for the deep learning era",
  "authors": [
    "Wei Ouyang",
    "Florian Mueller",
    "Martin Hjelmare",
    "Emma Lundberg",
    "Christophe Zimmer"
  ],
  "abstract": "Deep learning methods have shown extraordinary potential for analyzing very\ndiverse biomedical data, but their dissemination beyond developers is hindered\nby important computational hurdles. We introduce ImJoy (https://imjoy.io/), a\nflexible and open-source browser-based platform designed to facilitate\nwidespread reuse of deep learning solutions in biomedical research. We\nhighlight ImJoy's main features and illustrate its functionalities with deep\nlearning plugins for mobile and interactive image analysis and genomics.",
  "text": "ImJoy - Ouyang et al. - May 29, 2019  \nImJoy: an open-source computational platform for the deep learning era \n \nWei Ouyang​1,2​*, Florian Mueller​1​*, Martin Hjelmare​2​, Emma Lundberg​2,3,4​, Christophe Zimmer​1 \n \n1 ​Imaging and Modeling Unit, Institut Pasteur, UMR 3691 CNRS, C3BI USR 3756 IP CNRS, Paris, France \n2 ​Science for Life Laboratory, School of Engineering Sciences in Chemistry, Biotechnology and Health,\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nKTH - Royal Institute of Technology, Stockholm, Sweden \n3 ​Department of Genetics, Stanford University, Stanford, CA 94305, USA (visiting appointment) \n4 ​Chan Zuckerberg Biohub, San Francisco, CA 94158, USA (visiting appointment) \n \n* To whom correspondence should be addressed. \nWei Ouyang: weio@kth.se \nFlorian Mueller: muellerf.research@gmail.com \n \n \nDeep learning methods have shown extraordinary potential for analyzing very diverse\n \n \n \n \n \n \n \n \n \n \n \nbiomedical data, but their dissemination beyond developers is hindered by important\n \n \n \n \n \n \n \n \n \n \n \ncomputational hurdles. We introduce ImJoy (​https://imjoy.io/​), a flexible and open-source\n \n \n \n \n \n  \n \n \n \nbrowser-based platform designed to facilitate widespread reuse of deep learning\n \n \n \n \n \n \n \n \n \n \nsolutions in biomedical research. We highlight ImJoy’s main features and illustrate its\n \n \n \n \n \n \n \n \n \n \n \n \nfunctionalities with deep learning plugins for mobile and interactive image analysis and\n \n \n \n \n \n \n \n \n \n \n \n \ngenomics.  \n \nDeep learning methods, which use artificial neural networks to learn complex mappings\n \n \n \n \n \n \n \n \n \n \n \n \nbetween numerical data, have enabled recent breakthroughs in a wide range of biomedical data\n \n \n \n \n \n \n   \n \n \n \n \n \nanalysis tasks. Examples for imaging data include image segmentation​1,2 and medical\n \n \n \n \n \n \n \n \n \n \n \ndiagnosis, where deep learning vastly outperforms more traditional methods and often exceeds\n \n \n \n \n \n \n \n \n \n \n \n \nhuman expert performance​3,4​, or methods to enhance microscopy images, e.g. for denoising or\n \n \n \n \n \n \n \n \n \n \n \n \n \n1 \nImJoy - Ouyang et al. - May 29, 2019  \nsuper-resolution, or to predict fluorescence-like channels from images of unstained samples​5–8​.\n \n \n \n \n \n \n \n \n \n \n \nIn genomics, several studies also\nsuccessfully use deep learning to analyze DNA sequence\n \n \n \n \n \n \n \n \n \n \n \n \n \ndata, for instance to predict the specificity of DNA-binding proteins or the effects of non-coding\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nvariants​9,10​. Many other promising applications abound, for example in protein structure\n \n \n \n \n \n \n \n \n \n \n \ndetermination, chemical synthesis and drug design​11,12​. \n \nThese and other early advances generate considerable interest and a strong demand by\n \n \n \n \n \n \n \n \n \n \n \n \n \nbiomedical researchers to apply and adapt deep learning methods to new data sets and\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nquestions. The potential of deep learning is further increased by the growing number of public\n \n \n \n \n \n  \n \n \n \n \n \n \n \n \ndata repositories in genomics, imaging and other fields​13,14​. However, making full use of recent\n \n \n \n \n \n \n \n \n \n \n \n \n \n \ndeep learning approaches faces considerable bottlenecks. A distinctive challenge of machine\n \n \n \n \n \n \n \n \n \n \n \nlearning methods arises from their strong reliance on training data. Published studies\n \n \n \n \n \n \n \n \n \n \n \n \noccasionally provide already trained neural networks (models), sometimes in the form of\n \n \n \n \n \n \n \n \n \n \n \n \neasy-to-use tools, such as web applications or ImageJ plugins​2,5,6,10​. While these tools are\n \n \n \n \n \n \n \n \n \n \n \n \n \nuseful, they do not allow researchers to retrain models on their own data or on public data sets.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nThis restriction severely limits the reuse of deep learning methods, because models trained on\n \n \n \n \n \n \n \n \n \n \n \n \n \n \none data set do not necessarily perform well on different types of data (model mismatch),\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \npotentially resulting in artifacts​15​, and as a general rule much better results can be achieved by\n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \nfull or partial retraining on new or larger data sets. Therefore, enabling users to retrain existing\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nmodels on other data is essential to realizing the full promise of deep learning in biomedical\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nresearch.  \n \nMany deep learning approaches provide open-source code, typically written in Python and using\n \n \n \n \n \n \n \n \n  \n \n \n \nlibraries such as Tensorflow or Pytorch, which in theory enables retraining. In practice, however,\n \n \n \n \n \n \n  \n \n \n \n \n \n \n2 \nImJoy - Ouyang et al. - May 29, 2019  \nsetting up the required computational environments, which involve many hardware and\n \n \n \n \n \n \n \n \n \n \n \noperating\nsystem-dependent\nlibraries\nwith\ncomplex\ndependencies,\ncan\nbe\ndaunting.\n \n \n \n \n \n \n \n \n \nFurthermore, although these implementations allow prototyping by developers, they use\n \n \n \n \n \n \n \n \n \n \ncommand line instructions and are thus difficult to employ for researchers lacking adequate\n \n \n \n \n \n \n \n \n \n \n \n \n \nprogramming skills. Cloud computing services can alleviate some of these difficulties​1​, but\n \n \n \n \n \n \n \n \n \n \n \n \nrequire users to upload their data, which can cause privacy and confidentiality issues, or conflict\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nwith regulations such as GDPR, especially for biomedical data​16​. Thus, empowering biomedical\n \n \n \n \n \n \n \n \n \n \n \n \nresearchers to take full advantage of recent advances in deep learning currently remains an\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nimportant challenge. \n \nTo address this challenge, we developed ImJoy (​https://imjoy.io​), an open-source platform\n \n \n \n \n \n \n \n \n \n \n \ndesigned to deliver advanced, yet easy-to-use data analysis tools, especially based on deep\n \n \n \n \n \n \n \n \n \n \n \n \n \nlearning. The core of ImJoy is a browser based application (i.e. a software running in a web\n \n \n \n \n \n  \n \n \n \n  \n \n \n  \n \nbrowser) that offers a rich and interactive user interface and leverages modern web\n \n \n \n \n \n \n \n \n \n \n \n \n \ntechnologies (​Fig 1a, Supplementary Figure 1​). ImJoy’s data analysis functionalities are\n \n \n \n \n \n \n \n \n \n \n \nprovided by a flexible system of plugins that operate independently of each other but can be\n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \norganized into workspaces and workflows (​Supplementary Figure 2)​. Depending on the need,\n \n \n \n \n \n \n \n \n \n \n \n \nImJoy can access various computational resources by performing computations either in the\n \n \n \n \n \n \n \n \n \n \n \n \nbrowser itself or using ‘plugin engines’ that can run either locally or remotely (​Fig 1a,\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nSupplementary Figure 1​). Below, we highlight ImJoy’s main features, along with illustrative\n \n \n \n \n \n \n \n \n \n \n \n \nplugins with installation links and descriptions being provided in the Methods. A ​detailed\n \n \n \n \n \n \n \n \n \n \n \n \n \ndocumentation on how to use ImJoy and its plugins and how to develop new ones is provided\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \nonline (​https://imjoy.io/docs​).  \n \n3 \nImJoy - Ouyang et al. - May 29, 2019  \nImJoy provides self-contained plugin development support with a built-in code editor for\n \n \n \n \n \n \n \n \n \n \n \n \ndeveloping and testing plugins. It permits the use of plugins written in different programming\n \n \n \n  \n \n \n \n \n \n \n \n \n \nlanguages (JavaScript, HTML, and Python) within the same analysis workflow. ImJoy’s API\n \n \n \n \n \n \n \n \n \n \n \n \nenables\nbidirectional data exchange and allows plugins to use functions from other plugins\n \n \n \n \n \n \n \n \n \n \n \n \n \n(​Supplementary Figure 2)​. Plugins are stored as separate files, can be hosted on GitHub, and\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ndistributed with a single link, e.g. by email or social media (​Fig 1b​). This link directly opens\n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nImJoy and, importantly, allows developers to control and automate complex installations, e.g. to\n \n \n \n \n \n \n \n \n \n \n \n \n \nset up platform-dependent deep learning libraries (see Methods for more details). This makes\n \n \n \n \n \n \n \n \n \n \n \n \n \ninstallation transparent to the user and removes a major obstacle for tool deployment and reuse\n \n \n \n \n \n \n  \n \n \n \n \n \n \n \nby researchers. ImJoy also makes it easy to develop plugins that leverage a large variety of\n \n \n \n \n  \n \n \n \n \n \n  \n \n \n \navailable advanced user-interface libraries. As a result, ImJoy plugins can be optimized for\n \n \n \n \n  \n \n \n \n \n \n \n \nresponsiveness and flexibility, making them suitable for different screen sizes including\n \n \n \n \n \n \n \n \n \n \n \nsmartphones.\nFurthermore,\nthese\nlibraries\nprovide\nvast\npossibilities\nfor\ninteractive\n \n \n \n \n \n \n \n \n \ndata-visualization. As an illustration, we implemented a plugin for manual annotation of\n \n \n \n \n \n \n \n \n \n \n \n \nstructures of interest in images, a key requirement for defining training targets in image\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nsegmentation (​Methods​: Image Annotator plugin). Another example plugin uses uses high\n \n \n \n \n \n \n \n \n \n \n \ndimensional features previously extracted from deep learning models to analyze the localization\n \n \n \n \n \n \n \n \n \n \n \n \nof proteins based on data from the Human Protein Atlas​17 (HPA, ​https://proteinatlas.org​,\n \n \n \n \n \n \n \n \n \n \n \n \nMethods​: HPA-UMAP plugin, ​Fig 2a​). Using this plugin, researchers can visualize this entire\n \n \n \n \n \n \n \n \n \n \n \n \n \ndata set as scatter plots organized by feature-similarity (using dimensionality reduction\n \n \n \n \n \n \n \n \n \n \n \ntechniques) and interactively inspect the raw images associated to individual data points. Such\n \n \n \n \n \n \n \n \n \n \n \n \n \ninteractive visualization capabilities will be instrumental to explore large image databases.  \n  \n4 \nImJoy - Ouyang et al. - May 29, 2019  \nImJoy offers flexible access to multiple resources for computational analysis, which can be\n \n \n \n \n \n \n \n \n \n \n \n \n \nadapted to the computational requirements of each task. For many applications, computations\n \n \n \n \n \n \n \n \n \n \n \n \ncan be performed efficiently within the browser itself. Indeed, beyond their original purpose of\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nnavigating the Internet, web browsers are now increasingly used as computational platforms.\n \n \n \n \n \n \n \n \n \n \n \n \nThanks to sandboxing inherent to browser computing, ImJoy also offers higher levels of security\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nand stability than non-browser based software. ImJoy supports several browser-based libraries\n \n \n \n \n \n \n \n \n \n \n \nthat offer advanced data processing capabilities. For example, TensorFlow.js allows to run deep\n \n \n \n \n \n \n \n \n \n \n \n \n \nlearning models on graphical processing units (GPUs), while WebAssembly allows to run\n \n \n \n \n \n \n \n \n \n \n \n \nPython code in the browser. Note that browser-based computing does not require an Internet\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nconnection. As an example, we implemented a plugin to classify images of potentially\n \n \n \n \n \n \n \n \n \n \n \n \n \ncancerous skin lesions​18 using deep convolutional networks (​Methods​: Skin-Lesion-Analyzer\n \n \n \n \n \n \n \n \n \nplugin). Thanks to the interface versatility mentioned above, the plugin can run on a smartphone\n \n \n \n \n \n \n \n \n \n \n \n \n  \n \nand directly analyze skin images taken with the camera (​Fig 1b​). Importantly, the actual\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nclassification task is performed in the phone’s browser without sending any data over the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nInternet, thus mitigating privacy concerns. Another example is a plugin to classify protein\n \n \n \n \n \n \n \n \n \n \n \n \n \nlocalization patterns by deep learning (​Methods​: HPA-classification plugin). In addition to\n \n \n \n \n \n \n \n \n \n \n \npredicting localization classes from images, this plugin also shows a class activation map​19​,\n \n \n \n \n \n \n \n \n  \n \n \n \nwhich highlights the image regions most important for the classification. Such information is\n \n \n \n \n \n \n \n \n \n \n \n \n \nvaluable not only for developers aiming to fine-tune the algorithm and improve classification\n \n \n \n \n \n \n \n \n \n \n \n \n \nperformance, but also for users seeking to interpret the output, e.g. for medical diagnosis​20​.  \n \nTo perform more demanding or computationally intensive tasks, such as training or retraining a\n \n \n \n \n \n \n \n \n \n \n \n \n  \ndeep net on large data sets, browser-based computation may not always suffice. Therefore,\n \n \n \n \n \n \n \n \n \n \n \n \n \nImJoy can connect to a plugin engine (​Fig 1a, Supplementary Figure 1​), which can be\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n5 \nImJoy - Ouyang et al. - May 29, 2019  \nlaunched on different hardware, ranging from a local computer to private servers or public cloud\n \n \n \n \n \n  \n \n \n \n \n \n \n \n \nservices. The plugin engine has access to the entire Python ecosystem, and uses Conda to\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nautomatically solve plugin dependencies and set up virtual environments to avoid dependency\n \n \n \n \n \n \n \n \n \n \n \n \nconflicts. This relieves users from setting up complex software dependencies and provides\n \n \n \n \n \n \n \n \n \n \n \n \nreproducible analysis workflows, which is crucial for data science​21​. Importantly, the unified\n \n \n \n \n \n \n \n \n \n \n \n \nplugin interface of ImJoy ensures that remote procedure calls to the plugin engine are\n \n \n \n \n \n \n \n \n \n \n \n \n \n \ntransparent, i.e. the user will be exposed to the same interface independently of where\n \n \n \n \n \n \n \n \n \n \n \n \n \n \ncomputations actually occur. As an illustration, we implemented two ImJoy plugins for deep\n \n \n \n \n \n \n \n \n \n \n \n \n \nlearning based image transformation methods: a plugin implements ANNA-PALM, a recent\n \n \n \n \n \n \n \n \n  \n \nmethod to create dense super-resolution images from sparse localization data using neural\n \n \n \n \n \n \n \n \n \n \n \n \nnetworks​6 (​Methods​: ANNA-PALM plugin), and another implements CARE, which uses deep\n \n \n \n \n \n \n \n \n \n \n \nlearning for 3D image restoration​5 (​Methods​: CARE plugin, Fig 2a​). Other machine learning\n \n \n \n \n \n \n \n \n \n \n \n \n \napplications often require annotations performed by an expert before training, a task that is\n \n \n \n \n \n \n \n \n \n  \n \n \n \ngreatly\nfacilitated\nby\nImJoy’s\nafore-mentioned\nuser-interactivity.\nAs\nan\nexample,\nwe\n \n \n \n \n \n \n \n \n \n \nimplemented a deep learning nuclei segmentation framework. This framework is the winning\n  \n \n \n \n \n \n \n \n \n \n \nentry on the 2018 Kaggle Data Science Bowl for nuclear semententation and based on the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \npopular U-net architecture​2 (​Methods​: DPNUnet plugin). From a central plugin, the user can\n \n \n \n \n \n \n  \n \n \n \n \n \nchoose between plugins for annotation, mask generation, training, or prediction (​Fig 2b​). The\n \n \n \n \n \n \n \n \n \n \n \n \n \nannotation plugin\nallows researchers to manually outline nuclei, using a computer mouse or\n \n \n \n \n \n \n \n \n  \n \n \n \ntouch-screen device, these annotations can then be used for mask generation. The training\n \n \n \n \n \n \n \n \n \n \n \n \n \nplugin trains the U-net on the local or remote plugin engine and the prediction plugin uses the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ntrained U-net to segment new images. For all these examples, training can be monitored in real\n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \ntime, e.g. by inspecting the loss function or outputs for individual samples (​Fig 2a​), which allows\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nto fine-tune the algorithm - for example by adding regularization or perform early stopping to\n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n6 \nImJoy - Ouyang et al. - May 29, 2019  \nremedy overfitting. Together with appropriate computational resources (typically GPUs), ImJoy\n \n \n \n \n \n \n \n \n \n \nthus enables users to train these powerful methods on their own data or on other public data\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nsets, which was not easily possible before. \n \nFinally, we note that ImJoy is ideally suited to integrate with online databases and model\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nrepositories, providing a versatile front-end to efficiently leverage large numbers of data\n \n \n \n \n \n \n \n \n \n \n \n \nsamples and methods. In addition to the HPA-UMAP plugin mentioned above, which already\n \n \n \n \n \n \n \n \n \n \n \n \n \nillustrated interactive exploration of a large image database, we also provide a plugin that\n \n \n \n \n \n \n \n \n \n \n  \n \n \nimplements DeepBind​10​, a deep learning method to predict the sequence-specific binding\n \n \n \n \n \n \n \n \n \n \n \naffinities of proteins (​Methods​: DeepBindScan plugin, Fig 2b​). This plugin is based on Kipoi.org\n \n \n \n \n \n \n \n \n \n  \n \n \n \n- a collaborative initiative that provides ready-to-use trained models for genomics​22​. Unlike the\n  \n \n \n \n \n \n \n \n \n \n \n \noriginal website and command line tool​10​, this plugin allows to analyze any user-defined DNA or\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nRNA sequence from a graphical user interface. Thousands of other models available on\n \n \n \n \n \n \n \n \n \n \n \n \n \nKipoi.org can now easily be implemented through ImJoy.  \n \nWe presented ImJoy, a new computational platform that permits the development of powerful\n \n \n  \n \n \n \n \n \n \n \n \n \ndata processing tools with rich and interactive interfaces, and allows computations to be\n \n \n \n \n \n \n \n \n \n \n \n \n \nperformed in the browser, on local machines or devices, and remote servers. We highlighted\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nplugins that illustrate interactive visualization capabilities, the use of pre-trained deep learning\n \n \n \n \n \n \n \n \n \n \n \n \nmodels, operation on smartphones, retraining of deep models on user-provided data, real-time\n \n \n \n \n \n \n \n \n \n \n \n \nmonitoring of learning, applications to imaging and genomics data, and analysis of large public\n \n \n \n \n \n \n \n \n \n \n \n \n \n \ndata repositories. ImJoy enables flexible hybrid computation modes, where analysis tasks can\n \n \n \n \n \n \n \n \n \n \n \n \nbe distributed to a combination of local and remote computational environments. An interesting\n \n \n  \n \n \n \n \n \n \n \n \n \napplication is the processing of confidential medical data: raw images could be pre-processed\n \n \n \n \n \n \n \n \n \n \n \n \n \n7 \nImJoy - Ouyang et al. - May 29, 2019  \nand anonymised locally, for example to calculate feature vectors which could then be sent to a\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \nremote server for further analysis, e.g for comparison against a database. Although ImJoy was\n \n \n \n \n \n \n \n \n  \n \n \n \n \ndeveloped with a focus on deep learning, it can be used to implement and deploy a much\n \n  \n \n \n \n  \n \n \n \n \n \n \n  \n \nbroader range of computational methods. We foresee several extensions of ImJoy beyond its\n \n \n \n \n \n \n \n \n \n \n \n \n \ncurrent implementation and plugins. Among these are the possibility to incorporate scripts\n \n \n \n \n \n \n \n \n \n \n \n \nalready developed for other widely used data processing platforms, for example ImageJ/Fiji ​22​,\n \n \n \n \n \n \n \n \n \n \n \n \n \nallowing ImJoy to benefit from their functionalities and, conversely, confer new capabilities to\n \n \n \n \n \n \n \n \n \n \n \n \n \nthese tools, such as easy deployment in the cloud. We also envision plugins that leverage\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nhybrid computing to provide real-time feedback of freshly trained models to the user in response\n \n \n \n \n \n \n \n \n \n \n \n \n  \n \nto annotations or other interactions. Such interactivity (human-in-the-loop) can result in better\n \n \n \n \n \n \n \n \n \n \n \n \nfine-tuning of the algorithm and improved learning​23,24​.  \n \nIn sum, we believe that ImJoy will greatly accelerate the adoption of deep learning technique in\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \nbiomedical research by reducing the gap between developers and researchers and thereby\n \n \n \n \n \n \n \n \n \n \n \n \nopen the door to new scientific discoveries, while also promoting reproducible research and\n \n \n \n \n \n \n \n \n \n \n \n \n \nopen science. \n \nAcknowledgements \nThis work was funded by Institut Pasteur. W.O. was a scholar in the Pasteur–Paris University\n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n(PPU) International PhD program and was partly funded by a Fondation de la Recherche\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nMédicale grant to C.Z. (DEQ 20150331762). W.O. is a postdoctoral researcher supported by the\n \n \n \n \n \n \n   \n \n \n \n \n \nKnut\nand\nAlice\nWallenberg\nFoundation\n(2016.0204)\nand\nSwedish\nResearch\nCouncil\n \n \n \n \n \n \n \n \n \n \n(2017-05327). We also acknowledge Investissement d’Avenir grant ANR-16-CONV-0005 for\n \n \n \n \n \n \n \n \n \n8 \nImJoy - Ouyang et al. - May 29, 2019  \nfunding a GPU farm used for testing ImJoy. We thank the IT department of Institut Pasteur, in\n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nparticular Stéphane Fournier and Thomas Menard, for providing access to the kubernetes\n \n \n \n \n \n \n \n \n \n \n \n \ncluster and DGX-1 server for running and testing the ImJoy plugin engine and for technical\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nsupport. We thank Quang tru Huynh for maintaining the GPU farm and for advice and\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nassistance during the development of ImJoy. We also thank Anna Martinez Casals, Peter Thul,\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nHao Xu, Andrey Aristov, Anthony Cesnik, Christian Gnann, Alex Hu, Jyotsana Parmar, Kyle M.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nDouglass, Nico Stuurman, Xian Hao, Shubin Dai, David Guo, Kyrie Zhou for testing and helping\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nwith the ImJoy plugin development. We thank Elena Rensen for proofreading the manuscript.\n \n \n \n \n \n \n \n \n \n \n \n \n \nWe thank Juan Nunez-Iglesias, Shalin Mehta, Bryant Chhun, Joshua Batson, Loic Royer,\n \n \n \n \n \n \n \n \n \n \n \n \nNicholas Sofroniew, and Maxime Woringer for useful advice and discussion. \n \nCode availability \nImJoy (​https://ImJoy.io​) is open-source software released under MIT license. Code is hosted on\n \n  \n \n \n \n \n \n \n  \n \n \nGitHub: ​https://github.com/oeway/ImJoy​.  \n \n \nReferences \n1. Haberl, M. G. ​et al.​ CDeep3M-Plug-and-Play cloud-based deep learning for image \nsegmentation. ​Nat. Methods ​15​, 677–680 (2018). \n2. Falk, T. ​et al.​ U-Net: deep learning for cell counting, detection, and morphometry. ​Nat. \nMethods ​16​, 67 (2019). \n3. Esteva, A. ​et al.​ Dermatologist-level classification of skin cancer with deep neural networks. \nNature ​542​, 115–118 (2017). \n4. Gulshan, V. ​et al.​ Development and Validation of a Deep Learning Algorithm for Detection of \n9 \nImJoy - Ouyang et al. - May 29, 2019  \nDiabetic Retinopathy in Retinal Fundus Photographs. ​JAMA ​316​, 2402–2410 (2016). \n5. Weigert, M. ​et al.​ Content-aware image restoration: pushing the limits of fluorescence \nmicroscopy. ​Nat. Methods ​15​, 1090–1097 (2018). \n6. Ouyang, W., Aristov, A., Lelek, M., Hao, X. & Zimmer, C. Deep learning massively \naccelerates super-resolution localization microscopy. ​Nat. Biotechnol. ​36​, 460–468 (2018). \n7. Ounkomol, C., Seshamani, S., Maleckar, M. M., Collman, F. & Johnson, G. R. Label-free \nprediction of three-dimensional fluorescence images from transmitted-light microscopy. ​Nat. \nMethods ​15​, 917–920 (2018). \n8. Christiansen, E. M. ​et al.​ In Silico Labeling: Predicting Fluorescent Labels in Unlabeled \nImages. ​Cell ​173​, 792-803.e19 (2018). \n9. Eraslan, G., Avsec, Ž., Gagneur, J. & Theis, F. J. Deep learning: new computational \nmodelling techniques for genomics. ​Nat. Rev. Genet.​ 1 (2019). \ndoi:10.1038/s41576-019-0122-6 \n10.\nAlipanahi, B., Delong, A., Weirauch, M. T. & Frey, B. J. Predicting the sequence \nspecificities of DNA- and RNA-binding proteins by deep learning. ​Nat. Biotechnol. ​33​, \n831–838 (2015). \n11.\nSegler, M. H. S., Preuss, M. & Waller, M. P. Planning chemical syntheses with deep \nneural networks and symbolic AI. ​Nature ​555​, 604–610 (2018). \n12.\nWang, S., Sun, S., Li, Z., Zhang, R. & Xu, J. Accurate De Novo Prediction of Protein \nContact Map by Ultra-Deep Learning Model. ​PLOS Comput. Biol. ​13​, e1005324 (2017). \n13.\nOuyang, W. & Zimmer, C. The imaging tsunami: Computational opportunities and \nchallenges. ​Curr. Opin. Syst. Biol. ​4​, 105–113 (2017). \n14.\nBig Data: Astronomical or Genomical? Available at: \nhttps://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002195. (Accessed: \n10 \nImJoy - Ouyang et al. - May 29, 2019  \n17th April 2019) \n15.\nBelthangady, C. & Royer, L. A. Applications, Promises, and Pitfalls of Deep Learning for \nFluorescence Image Reconstruction. (2018). doi:10.20944/preprints201812.0137.v1 \n16.\nTang, H. ​et al.​ Protecting genomic data analytics in the cloud: state of the art and \nopportunities. ​BMC Med. Genomics ​9​, 63 (2016). \n17.\nThul, P. J. ​et al.​ A subcellular map of the human proteome. ​Science ​356​, eaal3321 \n(2017). \n18.\nTschandl, P., Rosendahl, C. & Kittler, H. The HAM10000 dataset, a large collection of \nmulti-source dermatoscopic images of common pigmented skin lesions. ​Sci. Data ​5​, 180161 \n(2018). \n19.\nSelvaraju, R. R. ​et al.​ Grad-CAM: Visual Explanations From Deep Networks via \nGradient-Based Localization. in 618–626 (2017). \n20.\nCastelvecchi, D. Can we open the black box of AI? ​Nat. News ​538​, 20 (2016). \n21.\nGrüning, B. ​et al.​ Bioconda: sustainable and comprehensive software distribution for the \nlife sciences. ​Nat. Methods ​15​, 475 (2018). \n22.\nSchindelin, J. ​et al.​ Fiji: an open-source platform for biological-image analysis. ​Nat. \nMethods ​9​, 676–682 (2012). \n23.\nSommer, C., Straehle, C., Köthe, U. & Hamprecht, F. A. Ilastik: Interactive learning and \nsegmentation toolkit. in ​2011 IEEE International Symposium on Biomedical Imaging: From \nNano to Macro​ 230–233 (2011). doi:10.1109/ISBI.2011.5872394 \n24.\nHolzinger, A. ​et al.​ Interactive machine learning: experimental evidence for the human in \nthe algorithmic loop. ​Appl. Intell.​ (2018). doi:10.1007/s10489-018-1361-5 \n25.\nPontén, F., Jirström, K. & Uhlen, M. The Human Protein Atlas—a tool for pathology. ​J. \nPathol. ​216​, 387–393 (2008). \n11 \nImJoy - Ouyang et al. - May 29, 2019  \n26.\nChen, Y. ​et al.​ Dual Path Networks. ​ArXiv170701629 Cs​ (2017). \n27.\nDeng, J. ​et al.​ Imagenet: A large-scale hierarchical image database. in ​In CVPR​ (2009). \n28.\nAvsec, Ž. ​et al.​ Kipoi: accelerating the community exchange and reuse of predictive \nmodels for genomics. ​bioRxiv​ 375345 (2018). doi:10.1101/375345 \n29.\nMa, N., Zhang, X., Zheng, H.-T. & Sun, J. ShuffleNet V2: Practical Guidelines for \nEfficient CNN Architecture Design. ​ArXiv180711164 Cs​ (2018). \n \n \n12 \nImJoy - Ouyang et al. - May 29, 2019  \nFigure Legends \n \nFigure 1​. ​Overview of ImJoy.  \na) ​The ImJoy core is a browser-app. Plugins, specified libraries and other required resources\n \n \n \n \n  \n \n \n \n \n \n \n \n \n(such as software libraries and pre-trained machine learning models) are retrieved from the\n \n \n \n \n \n \n \n \n \n \n \n \n \nInternet. Once these are downloaded and installed, ImJoy can run offline with a dedicated\n \n \n \n \n \n \n \n \n \n \n \n  \n \ndesktop app. ImJoy permits computing either in the browser, or can connect to multiple plugin\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nengines, which can perform computations locally on a workstation, computing cluster, or remote\n \n \n \n \n \n \n  \n \n \n \n \n \nservers and cloud services. ​b) Developers can write and test plugins directly in ImJoy, then\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ndeploy them as single plugin files to a GitHub repository, which can then serve as a plugin store\n \n \n \n \n \n \n  \n \n \n \n \n \n \n  \n \n \nfor ImJoy. Plugins can be shared with a hyperlink that permits their installation and usage on\n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \ndifferent devices including smartphones.  \n \nFigure 2​. ​ImJoy example plugins​.  \na) The “HPA-UMAP” plugin allows interactive exploration of images from the HPA cell atlas. The\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nscatter plot shows a UMAP representation of features computed by a deep model from images\n \n \n  \n \n \n \n \n \n  \n \n \n \n \nof fluorescently tagged proteins​26​. Each dot corresponds to an image. The deep model has been\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ntrained to classify localization patterns, therefore dots that are close together in the UMAP tend\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \nto have similar localization patterns. When the user clicks on an individual dot, the plugin\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ndisplays the corresponding image on the right. ​b) ​The ImJoy Dashboard allows users to monitor\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ntraining of a deep model in real-time. Top: the learning curve plots the loss, which measures the\n \n  \n \n  \n \n \n \n \n \n \n \n \n \n \n \nmodel’s performance on validation data, as function of iterations. Bottom: the user can inspect\n \n \n \n \n \n \n \n \n \n \n \n \n \n \npredictions on individual samples during training. Here, the three images show the noisy input\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n(left), the output of the currently trained CARE model​5 (middle), and the training target (i.e.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n13 \nImJoy - Ouyang et al. - May 29, 2019  \nground-truth) image (right). ​c) The DPNUnet plugin provides access to a complete framework\n \n \n \n \n \n \n \n \n \n  \n \n \nfor segmentation of nuclei based on the winning entry of the 2018 Kaggle data challenge.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nImages can be annotated with a dedicated plugin, allowing to generate masks for training. The\n \n \n \n \n  \n \n \n \n \n \n \n \n \n \npre-trained model can be retrained on these data and applied for segmentation of new images\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nusing two other plugins. ​d) ​The “DeepBindScan” plugin uses all available DeepBind​10 models to\n \n \n \n \n \n \n \n \n \n \n \n \n \n \npredict protein binding affinities against a user defined DNA- or RNA sequence.  \n \n14 \nLocal \nworkstation\nLocal cluster\nCloud \ncomputing\na\nWeb resources\nb\nImJoy browser app\nSkin-Lesion-Analyzer\nHPA-UMAP\nANNA-PALM\nPlugin Engine\nhttps://imjoy.io/#/app?p=imjoy-team/example-plugins:Skin-Lesion-Analyzer \n➊ develop\n➋ deploy\n➍ use\n➌ share\nImJoy plugins \nHTML\nJavaScript\nPython\nPlugin Repository\nFigure 1\nc\nb\na\nCreate training data: annotations and masks\nGGAGGCG\nGGAAGAT\nGGAGGCG\nGTAGCTG\nTCACTAG\nGTTGGGG\nTTCTCCC\nGCGCGGG\nAGGCCTA\nTGGGACT\nGTGTCGT\nGTCCTGA\nACAGTCG\nAGAGGAG\nCTT\nd\n(Re) training and prediction\nDPNUnet launchpad\nFigure 2\n \n \n \nSupplementary Figure 1:​ ​Overview of ImJoy architecture​.  \nImJoy is a browser application developed with modern web technologies (top). Its functionalities\n   \n \n \n \n \n \n \n \n \n \n \nare implemented with plugins that can run either directly in the browser or on a dedicated plugin\n \n \n \n \n \n \n \n \n  \n \n \n \n  \n \n \nengine (bottom). Browser plugins allow interacting with the user, visualizing data with a large\n \n \n \n \n \n \n \n \n \n \n \n  \n \nvariety\nof\nweb\nlibraries,\nperforming computational tasks in webworkers, which enable\n \n \n \n \n \n \n \n \n \n \n \ncomputational tasks to run in a dedicated thread, or using Python through WebAssembly. Native\n \n \n \n   \n \n \n \n \n \n \n \n \nPython plugins are supported through plugin engines implemented in Python and have access\n \n \n \n \n \n \n \n \n \n \n \n \n \nto the entire Python ecosystem, allowing them to perform demanding computational tasks with\n \n \n \n \n \n \n \n \n \n \n \n \n \npowerful scientific computing libraries such as Numpy, Scipy, or Scikit-image and to leverage\n \n \n \n \n \n \n \n \n \n \n \n \n \ndeep learning tools such as Tensorflow and Pytorch. Virtual environments and dependent\n \n \n \n \n \n \n \n \n \n \n \n \npackages are managed with Conda.  \n \n \n \nSupplementary Figure 2: Plugin interactions in ImJoy​.  \na) Workspaces. ImJoy allows to define different workspaces, each of which can contain different\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nplugins. While keeping these plugins inside their own sandboxes or processes, the ImJoy API\n \n \n \n \n \n \n \n \n \n \n \n \n \n \npermits these plugins to call each other’s API functions and exchange data within the same\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nworkspace. The same API can be used across programming languages or processes running\n \n \n \n \n \n \n \n \n \n \n \n \n \non another computer. For example, a JavaScript plugin rendering a user interface can call\n \n \n \n \n  \n \n \n  \n \n \n \n \nfunctions of a Python plugin for data-processing and then display the results. In the example\n \n  \n \n \n \n \n \n \n \n \n \n \n \n \ncode snippets shown, a JavaScript plugin (​getPlugin-demo​) uses the ImJoy API function\n \n \n \n \n \n \n \n \n \n \n \n \napi.getPlugin to call the function ​calc_exp of a Python plugin (​calculator​) and calculate the\n \n \n \n \n \n \n  \n \n \n \n \n \n \nexponential of the provided input. The result is then reported to the user. All API function calls\n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \nare asynchronous to achieve concurrent execution of plugins. This is implemented by using\n \n \n \n \n \n \n \n \n \n \n \n \n \nasync/await syntax with the function call as in the example plugin 2. ​b) Workflows. Plugins\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nwithin a workspace can also be composed into a workflow, where multiple plugins are chained\n  \n \n \n \n \n \n  \n \n \n \n \n \n \nto perform more complex data analysis and visualization tasks. Note that these workflows can\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nalso be stored and shared with other users via a single URL. In the example shown, a\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nJavaScript plugin asking the user to provide a number is executed first, then this number is used\n \n \n \n \n \n \n  \n  \n \n \n \n \n  \n \nby a second plugin to perform calculations.  \nImJoy - Ouyang et al. - May 29, 2019  \nMethods \nHere, we provide a brief overview of the general ImJoy software design and its plugin system.\n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \nFor a more detailed and regularly updated description, we refer to the ImJoy documentation\n  \n \n \n \n \n \n \n \n \n \n \n \n \n(​https://imjoy.io/docs​). We also list all example plugins with a brief description and installation\n \n \n \n \n \n \n \n  \n \n \n \n \nlinks.  \nImJoy software design \nImJoy is developed as a minimal and flexible plugin-powered browser application built with\n \n \n \n  \n \n \n \n \n \n \n \n \nmodern web technologies (HTML5, JavaScript, WebAssembly). ImJoy provides a server-less\n \n \n \n \n \n \n \n  \n \nsolution with offline support, and runs on mobile devices. By design, ImJoy itself provides very\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nminimal task-specific functionality. It offers a flexible plugin system with plugins allowing to\n \n \n \n \n  \n \n \n \n \n \n \n \nprovide user interfaces and perform computational tasks. ImJoy consists of three parts, and\n \n \n \n \n \n \n \n \n \n \n \n \n \neach can be extended with plugins (detailed below): (1) User Interface, where any existing web\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ndesign library can be used to interact with the user; (2) Web Computational Backend to execute\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ncomputational tasks directly inside web browsers; (3) Native Computational Backend (Plugin\n \n \n \n \n \n \n \n \n \n \n \nEngine) to perform more demanding computational tasks either locally or remotely. \n  \nWe provide a detailed online documentation (​https://imjoy.io/docs​), which includes a user\n \n \n \n \n \n \n \n \n \n \n \nmanual and description of all API functions with examples, dedicated demos to illustrate how to\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nimplement different plugin types, and a section with frequently asked questions (FAQ). Further,\n \n \n \n \n  \n \n \n \n \n \n \n \nImJoy is a community partner on image.sc, where questions for how to use ImJoy and process\n   \n \n \n \n \n \n \n \n \n \n \n \n \n \ndata can be asked in a dedicated forum (​https://forum.image.sc/tags/imjoy​). \n \n15 \nImJoy - Ouyang et al. - May 29, 2019  \nImJoy is open-source (MIT license) and entirely hosted on GitHub in several repositories for the\n  \n \n \n \n \n \n \n \n  \n \n \n \n \nbrowser app, the plugin engine, the desktop app, and different plugin repositories (more details\n \n \n \n \n \n \n \n \n \n \n \n \n \n \non  ​https://imjoy.io​). \nImJoy web application \nThe core of ImJoy is a set of static HTML/Javascript/CSS code which runs completely in the\n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \nbrowser\nand\ncan\nbe\nserved\nwith\nany\nstatic\nweb server. The official ImJoy website\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n(​https://imjoy.io​) is hosted on GitHub and served through GitHub pages. When the user opens\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nthe ImJoy app with the browser, these static web pages will be loaded to the browser. No local\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ndata will be sent to the imJoy.io server. All data will remain local or exchanged only with the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nconnected plugin engine. This so-called serverless design makes ImJoy suitable to work with\n \n \n \n \n \n \n \n \n \n \n \n \n \nsensitive data and offers maximum scalability. Lastly, this design guarantees that the user\n \n \n \n \n \n \n \n \n \n \n \n \n \nalways obtains the latest ImJoy version upon relaunching the app.  \n \nThe Desktop App (​https://github.com/oeway/ImJoy-App​) allows to use ImJoy locally without\n \n \n \n \n \n \n \n \n \n \nInternet connection, which is achieved by running a web server locally. The user can then\n \n \n \n \n \n \n  \n \n \n \n \n \n \n \naccess this locally served version of ImJoy (instead of ​https://imjoy.io ). While the ImJoy core\n \n \n \n \n \n \n \n \n \n  \n \n \n \n \ncan run offline entirely, some plugins may still require the Internet to fetch remote resources,\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ne.g. javascript libraries or trained models for inference. \n \nTo keep the ImJoy application and plugins up to date, users are encouraged to use the online\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nversion. Note that this does not imply that the data will be sent to the server, since the ImJoy.io\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nstatic web server does not accept any user data, and plugins process data either in the browser\n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \nitself or through the local plugin engine. \n16 \nImJoy - Ouyang et al. - May 29, 2019  \nImJoy plugin system \nDifferent plugin types are available to implement user interface, data display, or data processing\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nin the browser or on the plugin engine. Developers can use a consistent API (application\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \nprogram\ninterface)\nacross\ndifferent\nprogramming\nlanguages\n(JavaScript,\nPython),\nand\n \n \n \n \n \n \n \n \n \n`Asynchronous programming` to coordinate plugin processes that run in parallel. Plugins are\n \n \n \n \n \n \n \n \n \n \n \n \nindividual html files, and can be directly developed in ImJoy with the built-in code editor. ImJoy\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nprovides isolated workspaces, where plugins for a given data processing task can be grouped. \n \nDependencies can be defined for each plugin, and will be automatically resolved by ImJoy. A\n \n \n \n \n \n \n \n \n \n \n \n \n \n  \nplugin can run in an isolated virtual environment to avoid conflicts with other plugins. Further,\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nplugin tags provide configurable modes for their dependency installation and execution, e.g. to\n \n \n \n \n \n \n \n \n \n \n \n \n \nspecify operating system dependent versions of Tensorflow, or whether the plugin will use GPU\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nor CPU for computation. This is especially important for modern deep learning libraries, which\n \n \n \n \n  \n \n \n \n \n \n \n \n \nfrequently require operating system and hardware specific dependencies.  \n \nPlugin files can be stored in any GitHub repository, which can then serve as a plugin repository.\n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \nPlugin developers are thus clearly identified and can use ImJoy as a tool to facilitate usage of\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \ntheir code. Once deployed in the Github plugin repository, plugins can then be installed via a\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \nsingle web link. This will open ImJoy in the browser and allow the user to install the plugin.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nSeveral url parameters further allow to control how the plugin is installed and started. For\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nexample, a GitHub commit tag can be provided to control which version of the plugin is installed,\n  \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \nthereby guaranteeing high reproducibility.  \n17 \nImJoy - Ouyang et al. - May 29, 2019  \nExample plugins \nWe designed these example plugins to demonstrate the versatility of ImJoy. For these\n \n \n \n \n \n \n \n \n \n \n \n \n \ndemonstration plugins we deliberately chose to simplify the user experience, for example by\n \n \n \n \n \n \n \n \n \n \n \n \n \nhiding many hyperparameters for neural network training from the interface. However, these\n \n \n \n \n \n \n \n \n \n \n \n \nplugins can be easily adapted, extended or tailored for different use cases and for increased\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nfunctionalities.  \n \nBelow we list all example plugins mentioned in the paper. To install a plugin simply open the\n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \nprovided links starting with http://imjoy.io/#/app in a web browser. ​Please note that ImJoy runs\n \n \n \n \n   \n \n \n \n \n \n \n \nbest on the latest version of Google Chrome (version 74.0+) or Firefox (version 66.0+). \n \nThis link will open ImJoy and display the plugin installation dialog. After confirmation, the plugin\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nwill be installed in a dedicated workspace ‘imjoy-examples’. The plugin will show up in the plugin\n \n \n   \n \n \n \n \n \n \n \n  \n \n \nlist on the left side of the ImJoy interface. Installation progress is shown in the plugin-specific\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \nprogress bar. More details can be found in the plugin log (accessible by clicking on the symbol\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nleft of the plugin name). Once a plugin is correctly installed, its name will turn blue and the\n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \nplugin can be executed by pressing on it. Note that some plugins may install “helper” plugins\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nwhich provide additional functionalities, e.g. a dashboard to monitor training progress. Such\n \n \n \n \n  \n \n \n \n \n \n \nhelper plugins will be shown in the plugin list with a grayed-out name. \n \nPlugins running in the browser (indicated below with “Browser” in the title) can be used directly.\n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \nThe installation link of these plugins is configured such that they start automatically upon\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nsuccessful installation.  \n18 \nImJoy - Ouyang et al. - May 29, 2019  \n \nPlugins requiring the plugin engine are indicated below with “Engine” in the title. To run these\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nplugins, the plugin engine has to be installed as described in the ImJoy user manual\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n(​https://imjoy.io/docs/#/user_manual?id=imjoy-app-and-plugin-engine​).\nOnce\nImJoy\nis\n \n \n \n \nconnected to this engine, the plugins can be assigned to it using the menu to the left of the\n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \nplugin name. The specified workspace folder will then be created on the engine and the virtual\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nenvironments and software dependencies of the plugin will be installed. More details are\n \n \n \n \n \n \n \n \n \n \n \n \n \nprovided in the plugin log, which is accessible by pressing on the inverted exclamation mark\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nnext to the plugin name.  \n \nThe\nprovided\ndeep\nlearning\nplugins\nare\nbuilt\non\nopen-source\nPython\nlibraries\nwith\n \n \n \n \n \n \n \n \n \n \n \n \noperating-system specific dependencies. For these plugins we therefore provide different “tags”\n \n \n \n \n \n \n \n \n \n \n \nfor different operating system, that allow to control which libraries are installed. More details on\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nthe requirements to run the plugin and tested software environments are provided here:\n \n \n \n \n \n \n \n \n \n \n \n \n \nhttps://github.com/imjoy-team/example-plugins/blob/master/README.md \n \nImage Annotator plugin (Browser) \nPlugin installation link: \nhttps://imjoy.io/#/app?w=imjoy-examples&plugin=oeway/ImJoy-Plugins:ImageAnnotator&start=I\nmageAnnotator&fullscreen=1 \n \nThis browser plugin allows to manually outline and annotate different structures of interest in\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nmulti-channel images (TIFF or PNG). The plugin will open with a default image from the Human\n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n19 \nImJoy - Ouyang et al. - May 29, 2019  \nProtein Atlas (HPA)​25​, but other images can be loaded through the file menu if organized in the\n \n \n \n \n \n \n \n \n \n \n \n \n  \n  \n \nrequired folder structure. Different markers can be specified to annotate different structures,\n \n \n \n \n \n \n \n \n \n \n \n \nsuch as nuclei or microtubules. Multi-channel images can be shown as a composite image, or\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \nas individual channels. Annotations are stored in the geojson format and can be either exported\n \n \n \n \n \n  \n \n \n \n \n \n \n \n \nas zip file or sent to a plugin engine for further processing, e.g. to create masks for training\n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \nsegmentation algorithms, as demonstrated in the DPNUnet segmentation plugin (see below).  \n \nLarge data sets can be loaded into the plugin for annotation as folders. For more details on the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nrequired file organization we refer to the plugin documentation. Opening data with existing\n \n \n \n \n \n \n \n \n \n \n \n \n \nannotations, will result in an automatic display of these annotations.  \n \nHPA-UMAP   (Browser) \nPlugin installation link: \nhttps://imjoy.io/#/app?w=imjoy-examples&plugin=imjoy-team/example-plugins:HPA-UMAP&start\n=HPA-UMAP&fullscreen=1 \n \nThis plugin allows to visualize results of a classification of protein localization patterns from\n \n \n \n \n \n \n  \n \n \n \n \n \n \nmicroscope images of the entire HPA cell atlas​25 (v18, 67239 images).\nClassification is\n \n \n \n \n \n \n \n \n \n \n \n \n \nperformed with the deep-learning based winning method of the Human Protein Atlas Image\n \n \n \n \n \n \n \n \n \n \n \n \n \nClassification\nChallenge\n2018\n \n \n \n(​https://www.kaggle.com/c/human-protein-atlas-image-classification​).\nPlots\nshows\na\nUMAP\n \n \n \n \n \nprojection (Uniform Manifold Approximation and Projection) calculated from the last layer of the\n \n \n \n \n \n \n \n \n \n \n \n \n \nneural network (densenet121, dimensions: 1024). Each dot corresponds to one image, each\n \n \n \n \n \n \n \n \n \n \n \n \n20 \nImJoy - Ouyang et al. - May 29, 2019  \ncolor reflects a human annotated organelle localization pattern (e.g. red for nucleoplasmic\n \n \n \n \n \n \n \n \n \n \n \n \nlocalization, yellow for mitochondrial localization, and gray for mixed localization patterns).\n \n \n \n \n \n \n \n \n \n \n \nWhen hovering over UMAP plots, a thumbnail image for the currently selected data point is\n \n \n \n \n  \n \n \n \n \n \n \n \n \n \nshown in the upper left corner. Clicking on a dot will show a larger view of this image together\n \n \n \n \n \n \n \n  \n \n \n  \n \n \n \n \n \n \nwith information about the cell line, the gene corresponding to the protein, the annotated protein\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nlocations, and a link to the HPA website. The plugin also allows to search for a gene, cell line, or\n \n  \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \na localization pattern through a search box in the upper right corner, and display corresponding\n \n \n \n  \n \n \n \n \n \n \n \n \n \n \npoints in the UMAP.\nThe image slideshow allows to inspect the current image as an RGB\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ncomposition, and also each of the four color channels (green, blue, yellow and red) individually.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nThe green channel shows the protein of interest, while the other channels are reference\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nmarkers targeting different cell organelles: blue - nucleus, yellow - endoplasmic reticulum, and\n \n \n \n \n \n  \n \n  \n \n \n \nred - microtubules. \n \nSkin-Lesion-Analyzer   (Browser) \nPlugin installation link: \nhttps://imjoy.io/#/app?w=imjoy-examples&plugin=imjoy-team/example-plugins:Skin-Lesion-Anal\nyzer&start=Skin-Lesion-Analyzer \n \nDisclaimer​: this plugin is intended for academic research only and not tested for medical\n \n \n \n \n \n \n \n \n \n \n \n \n \n \ndiagnostics. \n \nThis plugin implements a deep learning methods that can classify an image of skin into 7 types\n \n \n  \n \n \n \n \n \n \n \n \n \n \n  \n \nof (potentially cancerous) skin lesions​3​. The model is trained with the deep learning library Keras\n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n21 \nImJoy - Ouyang et al. - May 29, 2019  \n(keras.io) and exported for use with Tensorflow.js (a Javascript implementation of the machine\n \n \n \n \n \n \n \n \n \n \n \n \n \nlearning platform Tensorflow), and thus runs entirely in the browser. This allows the plugin to\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nrun on mobile devices such as smartphones. If the plugin is executed on a computer, the input\n \n \n \n \n \n \n  \n \n  \n \n  \n \n \n \nimage can be located on the hard drive; on a smartphone the image can be taken directly with\n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \nthe camera.  \n \nCode was ported from ​https://github.com/vbookshelf/Skin-Lesion-Analyzer​. \n \nHPA-Classification   (Browser) \nPlugin installation link: \nhttps://imjoy.io/#/app?w=imjoy-examples&plugin=imjoy-team/example-plugins:HPA-Classificatio\nn&start=HPA-Classification&fullscreen=1 \n \nThis plugin uses a deep model to predict the subcellular protein localization pattern from a\n \n \n  \n \n \n \n \n \n \n \n \n \n  \nprovided image, based on annotated localization classes from the HPA cell atlas​25​. The model is\n \n \n \n \n \n \n \n \n \n \n \n \n \n  \na lightweight pre-trained neural network (ShuffleNet v2​29​) that enables fast prediction directly in\n \n \n \n \n \n \n \n \n \n \n \n  \nthe browser and won the special price of the Human Protein Atlas Image Classification\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nChallenge 2018 (​https://github.com/CellProfiling/HPA-Special-Prize​). The plugin further displays\n \n \n \n \n \n \n \na Class Activation Map​19​, which indicates which parts of the image were used for predicting\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \neach class. The user can also select a new image from the HPA online database and predict\n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \nthe localization pattern. \n \n22 \nImJoy - Ouyang et al. - May 29, 2019  \nANNA-PALM    (Engine) \nPlugin installation link: \nhttps://imjoy.io/#/app?w=imjoy-examples&plugin=oeway/ImJoy-Plugins:ANNA-PALM \n \nThis plugin implements the ANNA-PALM method​6​, which uses deep learning to reconstruct\n \n \n \n \n \n \n \n \n \n \n \n \nsuper-resolution views from sparse, rapidly acquired localization images and/or widefield\n \n \n \n \n \n \n \n \n \n \nimages. The user can choose between training or prediction from a central launch pad.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nTraining/prediction can be performed either on provided images of microtubules, or on\n \n \n \n \n \n \n \n \n \n \n \n \nuser-supplied data in the csv format of ThunderSTORM. The launch pad also provides access\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nto a detailed user manual, which is the recommended starting point for a new user.  \n \nMore information on ANNA-PALM can be found at ​https://annapalm.pasteur.fr/#/​. Code is ported\n \n \n \n \n \n \n \n \n \n  \n \nfrom ​https://github.com/imodpasteur/ANNA-PALM \n \nCARE    (Engine) \nPlugin installation link: \nhttps://imjoy.io/#/app?w=imjoy-examples&plugin=oeway/ImJoy-Plugins:CARE \n \nThis plugin implements an example of the deep learning-based CARE method (Content-aware\n \n \n \n \n \n \n \n \n \n \n \n \nimage restoration) for denoising of 3D fluorescence microscopy images​5​.  \n \nAs for the ANNA-PALM plugin, the user can choose between training or prediction from a\n \n \n \n \n \n \n \n \n \n \n \n \n \n  \ncentral launch pad and training can be performed either on provided example data (tribolium\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n23 \nImJoy - Ouyang et al. - May 29, 2019  \nimages) or new user-provided data. The launch pad also provides access to a detailed user\n \n \n \n \n \n \n \n \n \n \n \n  \n \n \nmanual, and is the recommended starting point for a new user.  \n \nCode is ported from  ​https://github.com/CSBDeep/CSBDeep/tree/master/examples/denoising3D \n \nDPNUnet     (Engine) \nPlugin installation link: \nhttps://imjoy.io/#/app?w=imjoy-examples&plugin=https://raw.githubusercontent.com/oeway/DPN\nUnet-Segmentation/master/DPNUnet.imjoy.html \n \nThis plugin implements a deep learning model for segmenting nuclei in microscopy images and\n \n \n  \n \n \n \n \n \n  \n \n \n \nis based on the winning entry of the 2018 Kaggle Data Science Bowl on nuclear segmentation\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n(​https://www.kaggle.com/c/data-science-bowl-2018/discussion/5474​). This method uses a Dual\n \n \n \n  \n \nPath Network​26 (DPN) pretrained on ImageNet​27 with a U-net architecture​2​. Together with the\n \n \n \n \n \n \n  \n \n \n \n \n \nImage Annotator plugin (above), the DPNUnet plugin offers a complete framework from image\n \n \n \n \n \n \n \n  \n \n \n \n \nannotation to prediction. In practice, multiple plugins can be selected from a central launch pad:\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \nthe annotation plugin (“annotate images”) permits to browse and annotate training images by\n \n \n \n \n \n \n \n \n \n \n \n \n \nmanually outlining nuclei; the mask generation plugin (“generate masks”) can then be used to\n \n \n \n \n \n \n \n \n \n \n \n \n \n \ncreate masks based on these annotations; the training plugin (“train with data from the engine”)\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nthen uses these masks as targets to re-train the DPNUnet model; finally, the trained model can\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nbe used for segmenting user-supplied images with the prediction plugin (“predict”).  \n \nCode for segmentation is ported from ​https://github.com/selimsef/dsb2018_topcoders  \n24 \nImJoy - Ouyang et al. - May 29, 2019  \nData for training the network is downloaded from \nhttps://www.kaggle.com/c/data-science-bowl-2018 \n \nDeepBindScan   (Engine) \nPlugin installation link: \nhttps://imjoy.io/#/app?w=imjoy-examples&plugin=imjoy-team/example-plugins:DeepBindScan \n \nThis plugin uses a deep learning approach DeepBind to predict the binding affinities of\n \n \n \n \n \n \n \n \n \n \n \n \n \n \ntranscription factors (TFs) and RNA binding proteins (RBPs) in different species to a\n \n \n \n \n \n \n \n \n \n \n \n \n \nuser-provided DNA- or RNA- sequence​10​. It is implemented with Kipoi​28 (​https://kipoi.org/​), which\n \n \n \n \n   \n \n \n \n \n \nprovides ready-to-use trained models for genomics and a corresponding API. The user can\n \n \n \n \n \n \n  \n \n \n \n \n \nspecify a sequence, which will then be used as input to all available pre-trained DeepBind\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nmodels (​https://kipoi.org/groups/DeepBind/​), each of which predicts the binding affinity of a\n \n \n \n \n \n \n \n \n \n  \nsingle TF or RBP from a specific species. The models used require input sequences to be 101\n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \nbase pairs long. Predicted binding affinities are shown in a vertical bar plot and updated\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ndynamically. Final prediction results can also be exported as csv files.  \n \nPlease note that for some organisms (e.g. homo sapiens), models for many proteins are\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nprovided, and the prediction will then take several minutes.  \n \n \n \n25 \n",
  "categories": [
    "cs.LG",
    "q-bio.QM",
    "stat.ML"
  ],
  "published": "2019-05-30",
  "updated": "2019-05-30"
}