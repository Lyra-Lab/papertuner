{
  "id": "http://arxiv.org/abs/1906.01695v1",
  "title": "Reinforcement Learning with Low-Complexity Liquid State Machines",
  "authors": [
    "Wachirawit Ponghiran",
    "Gopalakrishnan Srinivasan",
    "Kaushik Roy"
  ],
  "abstract": "We propose reinforcement learning on simple networks consisting of random\nconnections of spiking neurons (both recurrent and feed-forward) that can learn\ncomplex tasks with very little trainable parameters. Such sparse and randomly\ninterconnected recurrent spiking networks exhibit highly non-linear dynamics\nthat transform the inputs into rich high-dimensional representations based on\npast context. The random input representations can be efficiently interpreted\nby an output (or readout) layer with trainable parameters. Systematic\ninitialization of the random connections and training of the readout layer\nusing Q-learning algorithm enable such small random spiking networks to learn\noptimally and achieve the same learning efficiency as humans on complex\nreinforcement learning tasks like Atari games. The spike-based approach using\nsmall random recurrent networks provides a computationally efficient\nalternative to state-of-the-art deep reinforcement learning networks with\nseveral layers of trainable parameters. The low-complexity spiking networks can\nlead to improved energy efficiency in event-driven neuromorphic hardware for\ncomplex reinforcement learning tasks.",
  "text": "Reinforcement Learning\nwith Low-Complexity Liquid State Machines\nWachirawit Ponghiran∗\nGopalakrishnan Srinivasan∗\nKaushik Roy\nDepartment of ECE\nPurdue University\nWest Lafayette, IN 47906\nwponghir@purdue.edu\nJune 6, 2019\nAbstract\nWe propose reinforcement learning on simple networks consisting of random connections of spiking\nneurons (both recurrent and feed-forward) that can learn complex tasks with very little trainable\nparameters. Such sparse and randomly interconnected recurrent spiking networks exhibit highly non-linear\ndynamics that transform the inputs into rich high-dimensional representations based on past context. The\nrandom input representations can be eﬃciently interpreted by an output (or readout) layer with trainable\nparameters. Systematic initialization of the random connections and training of the readout layer using\nQ-learning algorithm enable such small random spiking networks to learn optimally and achieve the same\nlearning eﬃciency as humans on complex reinforcement learning tasks like Atari games. The spike-based\napproach using small random recurrent networks provides a computationally eﬃcient alternative to\nstate-of-the-art deep reinforcement learning networks with several layers of trainable parameters. The\nlow-complexity spiking networks can lead to improved energy eﬃciency in event-driven neuromorphic\nhardware for complex reinforcement learning tasks.\n1\nIntroduction\nHigh degree of recurrent connectivity among neuronal populations is a key attribute of neural microcircuits in\nthe cerebral cortex and many diﬀerent brain regions [1, 2, 3]. Such common structure suggests the existence\nof a general principle for information processing. However, the principle underlying information processing\nin such recurrent population of spiking neurons is still largely elusive due to the complexity of training\nlarge recurrent Spiking Neural Networks (SNNs). In this regard, reservoir computing architectures [4, 5, 6]\nwere proposed to minimize the training complexity of large recurrent neuronal populations. Liquid State\nMachine (LSM) [4, 5] is a recurrent SNN consisting of an input layer sparsely connected to a randomly\ninterlinked reservoir (or liquid) of excitatory and inhibitory spiking neurons whose activations are passed on\nto a readout (or output) layer, trained using supervised algorithms, for inference. The key attribute of an\nLSM is that the input-to-liquid and the recurrent excitatory↔inhibitory synaptic connectivity matrices and\nweights are ﬁxed a priori. LSM eﬀectively utilizes the rich nonlinear dynamics of Leaky-Integrate-and-Fire\nspiking neurons [7] and the sparse random input-to-liquid and recurrent-liquid synaptic connectivity for\nprocessing spatio-temporal inputs. At any time instant, the spatio-temporal inputs are transformed into a\nhigh-dimensional representation, referred to as the liquid states (or spike patterns), which evolves dynamically\nbased on decaying memory of the past inputs. The memory capacity of the liquid is dictated by its size and\ndegree of recurrent connectivity. Although the LSM, by construction, does not have stable instantaneous\ninternal states like Turing machines [8] or attractor neural networks [9], prior studies have successfully trained\nthe readout layer using liquid activations, estimated by integrating the liquid states (spikes) over time, for\n∗Equal contributors to this work\n1\narXiv:1906.01695v1  [cs.LG]  4 Jun 2019\nspeech recognition [4, 10, 11, 12], image recognition [13], gesture recognition [14, 15], and sequence generation\ntasks [16, 17, 18].\nIn this work, we propose such sparse randomly-interlinked low-complexity LSMs for solving complex\nReinforcement Learning (RL) tasks, which involve an autonomous agent (modeled using the LSM) trained to\nselect actions in a manner that maximizes the expected future rewards received from the environment. For\ninstance, a robot (agent) learning to navigate a maze (environment) based on the reward and punishment\nreceived from the environment is an example RL task. At any given time, the environment state (converted\nto spike trains) is fed to the liquid, which produces a high-dimensional liquid state (spike pattern) based\non decaying memory of the past environment states. We present an optimal initialization strategy for the\nﬁxed input-to-liquid and recurrent-liquid synaptic connectivity matrices and weights to enable the liquid\nto produce high-dimensional representations that lead to eﬃcient training of the liquid-to-readout weights.\nArtiﬁcial rate-based neurons for the readout layer takes the liquid activations and produces action-values to\nguide action selection for a given environment state. The liquid-to-readout weights are trained using the\nQ-learning RL algorithm proposed for deep learning networks [19]. In RL theory [20], the Q-value, also\nknown as the action-value, estimates the expected future rewards for a state-action pair that speciﬁes how\ngood is the action for the current environment state. The readout layer of the LSM contains as many neurons\nas the number of possible actions for a particular RL task. At any given time, the readout neurons predict\nthe Q-value for all possible actions based on the high-dimensional state representation provided by the liquid.\nThe liquid-to-readout weights are then trained using backpropagation [21] to minimize the error between\nthe Q-values predicted by the LSM and the target Q-values estimated from RL theory [22] as described\nin subsection 3.2. We adopt ϵ-greedy policy (explained in subsection 3.2) to select the appropriate action\nbased on the predicted Q-values during training and evaluation. Based on ϵ-greedy policy, a lot of random\nactions are picked in the beginning of the training phase to better explore the environment. Towards the\nend of training and during inference, the action corresponding to the maximum Q-value is selected with\nhigher probability to exploit the learnt experiences. We ﬁrst demonstrate results for training the readout\nweights based on the high-dimensional representations provided by the liquid, as a result of the sparse\nrecurrent-liquid connectivity, on simple Cartpole-balancing RL task [20]. We then comprehensively validate\nthe capability of the LSM and the presented training methodology on complex RL tasks like Pacman [23] and\nAtari games [24]. We note that LSM has been previously trained using Q-learning for RL tasks pertaining to\nrobotic motion control [25, 26, 27]. We demonstrate and benchmark the eﬃcacy of appropriately initialized\nLSM for solving RL tasks commonly used to evaluate deep reinforcement learning networks. In essence,\nthis work provides a promising step towards incorporating bio-plausible low-complexity recurrent SNNs like\nLSMs for complex RL tasks, which can potentially lead to much improved energy eﬃciency in event-driven\nasynchronous neuromorphic hardware implementations [28, 29].\n2\nIntroduction\nHigh degree of recurrent connectivity among neuronal populations is a key attribute of neural microcircuits in\nthe cerebral cortex and many diﬀerent brain regions [1, 2, 3]. Such common structure suggests the existence\nof a general principle for information processing. However, the principle underlying information processing\nin such recurrent population of spiking neurons is still largely elusive due to the complexity of training\nlarge recurrent Spiking Neural Networks (SNNs). In this regard, reservoir computing architectures [4, 5, 6]\nwere proposed to minimize the training complexity of large recurrent neuronal populations. Liquid State\nMachine (LSM) [4, 5] is a recurrent SNN consisting of an input layer sparsely connected to a randomly\ninterlinked reservoir (or liquid) of excitatory and inhibitory spiking neurons whose activations are passed on\nto a readout (or output) layer, trained using supervised algorithms, for inference. The key attribute of an\nLSM is that the input-to-liquid and the recurrent excitatory↔inhibitory synaptic connectivity matrices and\nweights are ﬁxed a priori. LSM eﬀectively utilizes the rich nonlinear dynamics of Leaky-Integrate-and-Fire\nspiking neurons [7] and the sparse random input-to-liquid and recurrent-liquid synaptic connectivity for\nprocessing spatio-temporal inputs. At any time instant, the spatio-temporal inputs are transformed into a\nhigh-dimensional representation, referred to as the liquid states (or spike patterns), which evolves dynamically\nbased on decaying memory of the past inputs. The memory capacity of the liquid is dictated by its size and\ndegree of recurrent connectivity. Although the LSM, by construction, does not have stable instantaneous\n2\ninternal states like Turing machines [8] or attractor neural networks [9], prior studies have successfully trained\nthe readout layer using liquid activations, estimated by integrating the liquid states (spikes) over time, for\nspeech recognition [4, 10, 11, 12], image recognition [13], gesture recognition [14, 15], and sequence generation\ntasks [16, 17, 18].\nIn this work, we propose such sparse randomly-interlinked low-complexity LSMs for solving complex\nReinforcement Learning (RL) tasks, which involve an autonomous agent (modeled using the LSM) trained to\nselect actions in a manner that maximizes the expected future rewards received from the environment. For\ninstance, a robot (agent) learning to navigate a maze (environment) based on the reward and punishment\nreceived from the environment is an example RL task. At any given time, the environment state (converted\nto spike trains) is fed to the liquid, which produces a high-dimensional liquid state (spike pattern) based\non decaying memory of the past environment states. We present an optimal initialization strategy for the\nﬁxed input-to-liquid and recurrent-liquid synaptic connectivity matrices and weights to enable the liquid\nto produce high-dimensional representations that lead to eﬃcient training of the liquid-to-readout weights.\nArtiﬁcial rate-based neurons for the readout layer takes the liquid activations and produces action-values to\nguide action selection for a given environment state. The liquid-to-readout weights are trained using the\nQ-learning RL algorithm proposed for deep learning networks [19]. In RL theory [20], the Q-value, also\nknown as the action-value, estimates the expected future rewards for a state-action pair that speciﬁes how\ngood is the action for the current environment state. The readout layer of the LSM contains as many neurons\nas the number of possible actions for a particular RL task. At any given time, the readout neurons predict\nthe Q-value for all possible actions based on the high-dimensional state representation provided by the liquid.\nThe liquid-to-readout weights are then trained using backpropagation [21] to minimize the error between\nthe Q-values predicted by the LSM and the target Q-values estimated from RL theory [22] as described\nin subsection 3.2. We adopt ϵ-greedy policy (explained in subsection 3.2) to select the appropriate action\nbased on the predicted Q-values during training and evaluation. Based on ϵ-greedy policy, a lot of random\nactions are picked in the beginning of the training phase to better explore the environment. Towards the\nend of training and during inference, the action corresponding to the maximum Q-value is selected with\nhigher probability to exploit the learnt experiences. We ﬁrst demonstrate results for training the readout\nweights based on the high-dimensional representations provided by the liquid, as a result of the sparse\nrecurrent-liquid connectivity, on simple Cartpole-balancing RL task [20]. We then comprehensively validate\nthe capability of the LSM and the presented training methodology on complex RL tasks like Pacman [23] and\nAtari games [24]. We note that LSM has been previously trained using Q-learning for RL tasks pertaining to\nrobotic motion control [25, 26, 27]. We demonstrate and benchmark the eﬃcacy of appropriately initialized\nLSM for solving RL tasks commonly used to evaluate deep reinforcement learning networks. In essence,\nthis work provides a promising step towards incorporating bio-plausible low-complexity recurrent SNNs like\nLSMs for complex RL tasks, which can potentially lead to much improved energy eﬃciency in event-driven\nasynchronous neuromorphic hardware implementations [28, 29].\n3\nMaterials and Methods\n3.1\nLiquid State Machine: Architecture and Initialization\nLiquid State Machine (LSM) consists of an input layer sparsely connected via ﬁxed synaptic weights to\na randomly interlinked liquid of excitatory and inhibitory spiking neurons followed by a readout layer as\ndepicted in Figure 1. The input layer (denoted by P) is modeled as a group of excitatory neurons that spike\nbased on the input environment state following a Poisson process. The sparse input-to-liquid connections are\ninitialized such that each excitatory neuron in the liquid receives synaptic connections from approximately K\nrandom input neurons. This guarantees uniform excitation of the liquid-excitatory neurons by the external\ninput spikes. The ﬁxed input-to-liquid synaptic weights are chosen from a uniform distribution between 0\nand α as shown in Table 3, where α is the maximum bound imposed on the weights. The liquid consists\nof excitatory neurons (denoted by E) and inhibitory neurons (denoted by I) recurrently connected in a\nsparse random manner as illustrated in Figure 1. The number of excitatory neurons is chosen to be 4× the\nnumber of inhibitory neurons as observed in the cortical circuits [30]. We use the Leaky-Integrate-and-Fire\n(LIF) model [7] to mimic the dynamics of both excitatory and inhibitory spiking neurons as described by the\nfollowing diﬀerential equations:\n3\n...\nInput neuron that spikes\nLiquid-inhibitory spiking neuron (I)\nRate-based readout neuron\nLiquid-excitatory spiking neuron (E)\nPopulation of neurons (liquid)\nSimulation\ntime-step\nInput layer\nWIn    E\nWE    Out\n Input neuron that does not spike\n...\n...\n...\nReadout \nGame snapshot\nFigure 1: Illustration of the LSM architecture consisting of an input layer sparsely connected via ﬁxed\nsynaptic weights to randomly recurrently connected reservoir (or liquid) of excitatory and inhibitory spiking\nneurons followed by a readout layer composed of artiﬁcial rate-based neurons.\ndVi\ndt = Vrest −Vi\nτ\n+ Ii(t)\n(1)\nIi(t) =\nX\nl∈NP\nWli · δ(t −tl) +\nX\nj∈NE\nWji · δ(t −tj) −\nX\nk∈NI\nWki · δ(t −tk)\n(2)\nwhere Vi is the membrane potential of the i-th neuron in the liquid, Vrest is the resting potential to which\nVi decays to, with time constant τ, in the absence of input current, and Ii(t) is the instantaneous current\nprojecting into the i-th neuron, and NP , NE, and NI are the number of input, excitatory, and inhibitory\nneurons, respectively. The instantaneous current is a sum of three terms: current from input neurons,\ncurrent from excitatory neurons, and current from inhibitory neurons. The ﬁrst term integrates the sum of\npre-synaptic spikes, denoted by δ(t −tl) where tl is the time instant of pre-spikes, with the corresponding\nsynaptic weights (Wli in Equation 2). Likewise, the second (third) term integrates the sum of pre-synaptic\nspikes from the excitatory (inhibitory) neurons, denoted by δ(t −tj) (δ(t −tk)), with the respective weights\nWji (Wki) in Equation 2. The neuronal membrane potential is updated with the sum of the input, excitatory,\nand negative inhibitory currents as shown in Equation 1. When the membrane potential reaches a certain\nthreshold Vthres, the neuron ﬁres an output spike. The membrane potential is thereafter reset to Vreset and\nthe neuron is restrained from spiking for an ensuing refractory period by holding its membrane potential\nconstant. The LIF model parameters for the excitatory and inhibitory neurons are listed in Table 4.\nThere are four types of recurrent synaptic connections in the liquid, namely, E→E, E→I, I→E, and\nI→I. We express each connection in the form of a matrix that is initialized to be sparse and random, which\ncauses the spiking dynamics of a particular neuron to be independent of most other neurons and maintains\nseparability in the neuronal spiking activity. However, the degree of sparsity needs to be tuned to achieve\nrich network dynamics. We ﬁnd that excessive sparsity (reduced connectivity) leads to weakened interaction\nbetween the liquid neurons and renders the liquid memoryless. On the contrary, lower sparsity (increased\nconnectivity) results in chaotic spiking activity, which eliminates the separability in neuronal spiking activity.\nWe initialize the connectivity matrices such that each excitatory neuron receives approximately C synaptic\nconnections from inhibitory neurons, and vice versa. The hyperparameter C is tuned empirically as discussed\nin subsection 4.1 to avoid common chaotic spiking activity problems that occur when (1) excitatory neurons\nconnect to each other and form a loop that always leads to positive drift in membrane potential, and when\n(2) an excitatory neuron connects to itself and repeatedly gets excited from its activity. Speciﬁcally, for\nthe ﬁrst situation, we have non-zero elements in the connectivity matrix E→E (denoted by WEE) only at\nlocations where elements in the product of connectivity matrices E→I and I→E (denoted by WEI and\n4\nWIE, respectively) are non-zero. This ensures that excitatory synaptic connections are created only for\nthose neurons that also receive inhibitory synaptic connections, which mitigates the possibility of continuous\npositive drift in the respective membrane potentials. To circumvent the second situation, we force the diagonal\nelements of WEE to be zero and eliminate the possibility of repeated self-excitation. Throughout this work,\nwe create a recurrent connectivity matrix for liquid with m excitatory neurons and n inhibitory neurons by\nforming an m × n matrix whose values are randomly drawn from a uniform distribution between 0 and 1.\nConnection is formed between those pairs of neurons where the corresponding matrix entries are lesser than\nthe target connection probability (= C/m). For illustration, consider a liquid with m=1000 excitatory and\nn=250 inhibitory neurons. In order to create the E→I connectivity matrix such that each inhibitory neuron\nreceives synaptic connection from a single excitatory neuron (C=1), we ﬁrst form a 1000 × 250 random\nmatrix whose values are drawn from a uniform distribution between 0 and 1. We then create a connection\nbetween those pairs of neurons where the matrix entries are lesser than 0.1% (1/1000). Similar process is\nrepeated for connection I→E. We then initialize connection E→E based on the product of WEI and WIE.\nSimilarly, the connectivity matrix for I→I (denoted by WII) is initialized based on the product of WIE\nand WEI. The connection weights are initialized from a uniform distribution between 0 and β as shown in\nTable 3 for diﬀerent recurrent connectivity matrices. Note that the weights of the synaptic connections from\ninhibitory neurons are greater than that for synaptic connections from excitatory neurons to account for the\nlower number of inhibitory neurons relative to excitatory neurons. Stronger inhibitory connection weights\nhelp ensure that every neuron receives similar amount of excitatory and inhibitory input currents, which\nimproves the stability of the liquid as experimentally validated in subsection 4.1.\nThe liquid-excitatory neurons are fully-connected to artiﬁcial rate-based neurons in the readout layer for\ninference. The readout layer, which consists of as many output neurons as the number of actions for a given\nRL task, uses the average ﬁring rate/activation of the excitatory neurons to predict the Q-value for every\nstate-action pair. We translate the liquid spiking activity to average rate by accumulating the excitatory\nneuronal spikes over the time period for which the input (current environment state) is presented. We then\nnormalize the spike counts with the maximum possible spike count over the LSM-simulation period, which is\ncomputed as the LSM-simulation period divided by the simulation time-step, to obtain the average ﬁring rate\nof the excitatory neurons that are fed to the readout layer. Since the number of excitatory neurons is larger\nthan the number of output neurons in the readout layer, we gradually reduce the dimension by introducing an\nadditional fully-connected hidden layer between the liquid and the output layer. We use ReLU non-linearity\n[31] after the ﬁrst hidden layer but none after the ﬁnal output layer since the Q-values are unbounded and can\nassume positive or negative values. We train the synaptic weights constituting the fully-connected readout\nlayer using the Q-learning based training methodology that is described in the following subsection 3.2.\n3.2\nQ-Learning Based LSM Training Methodology\nReinforcement Learning (RL) tasks fundamentally involve an agent (for instance, a robot) that is trained to\nnavigate a certain environment (for instance, a maze) in a manner that maximizes the total rewards in the\nfuture. Formally, at any time instant t, the agent receives the environment state st and picks action at from\nthe set of all possible actions. After the environment receives the action at, it transitions to the next state\nbased on the chosen action and feeds back an immediate reward rt+1 and the new environment state st+1.\nAs mentioned in the beginning, the goal of the agent is to maximize the accumulated reward in the future,\nwhich is mathematically expressed as\nRt =\n∞\nX\nt=1\nγt rt\n(3)\nwhere γ ∈[0, 1] is the discount factor that determines the relative signiﬁcance attributed to immediate\nand future reward. If γ is chosen to be 0, the agent maximizes only the immediate reward. However, as γ\napproaches unity, the agent learns to maximize the accumulated reward in the future. Q-learning [22] is a\nwidely used RL algorithm that enables the agent to achieve this objective by computing the state-action\nvalue function (or commonly known as the Q-function), which is the expected future reward for a state-action\npair that is speciﬁed by\nQπ(s, a) = E[Rt|st = s, at = a, π]\n(4)\n5\nwhere Qπ(s, a) measures the value of choosing an action a when in state s following a policy π. If the\nagent follows the optimal policy (denoted by π∗) such that Qπ∗(s, a) = max\nπ\nQπ(s, a), the Q-function can be\nestimated recursively using the Bellman optimality equation that is described by\nQπ∗(s, a) = E[rt+1 + γ max\nat+1 Qπ∗(st+1, at+1)|s, a]\n(5)\nwhere Qπ∗(s, a) is the Q-value for choosing action a from state s following the optimal policy π∗, rt+1 is the\nimmediate reward received from the environment, Qπ∗(st+1, at+1) is the Q-value for selecting action at+1\nfrom the next environment state st+1. Learning the Q-values for all possible state-action pairs is intractable\nfor practical RL applications. Popular approaches approximate Q-function using deep convolutional neural\nnetworks [19, 32, 33, 34].\nIn this work, we model the agent using an LSM, wherein the liquid-to-readout weights are trained to\napproximate the Q-function as described below. At any time instant t, we map the current environment\nstate vector st to input neurons ﬁring at a rate constrained between 0 and φ Hz over certain time period\n(denoted by TLSM) following a Poisson process. The maximum Poisson ﬁring rate φ is tuned to ensure\nsuﬃcient input spiking activity for a given RL task. We follow the method outlined in [35] to generate\nthe Poisson spike trains as explained below. For a particular input neuron in the state vector, we ﬁrst\ncompute the probability of generating a spike at every LSM-simulation time-step based on the corresponding\nPoisson ﬁring rate. Note that the time-steps in the RL task are orthogonal to the time-steps used for the\nnumerical simulation of the liquid. Speciﬁcally, in-between successive time-steps t and t + 1 in the RL task,\nthe liquid is simulated for a time period of TLSM with 1ms separation between consecutive LSM-simulation\ntime-steps. The probability of producing a spike at any LSM-simulation time-step is obtained by scaling the\ncorresponding ﬁring rate by 1,000. We generate a random number drawn from a uniform distribution between\n0 and 1, and produce a spike if the random number is lesser than the neuronal spiking probability. At every\nLSM-simulation time-step, we feed the spike map of the current environment state and record the spiking\noutputs of the liquid-excitatory neurons. We accumulate the excitatory neuronal spikes and normalize the\nindividual neuronal spike counts with the maximum possible spike count over the LSM-simulation period to\nobtain the high-dimensional representation (activation) of the environment state as discussed in the previous\nsubsection 3.1. It is important to note that appropriate initialization of the LSM (detailed in subsection 3.1)\nis necessary to obtain useful high-dimensional representation for eﬃcient training of the liquid-to-readout\nweights as experimentally validated in section 4.\nThe high-dimensional liquid activations are fed to the readout layer that is trained using backpropagation\nto approximate the Q-function by minimizing the mean square error between the Q-values predicted by the\nreadout layer and the target Q-values following [19] as described by the following equations:\nθt+1 = θt + η (Yt −Q(st, at|θt)) ∇θtQ(st, at|θt)\n(6)\nYt = rt+1 + γ max\nat+1 Q(st+1, at+1|θt)\n(7)\nwhere θt+1 and θt are the updated and previous synaptic weights in the readout layer, respectively, η is\nlearning rate, Q(st, at|θt) is vector representing the Q-values predicted by the readout layer for all possible\nactions given the current environment state st using the previous readout weights, ∇θtQ(st, at|θt) is the\ngradient of the Q-values with respect to the readout weights, and Yt is the vector containing the target\nQ-values that is obtained by feeding the next environment state st+1 to the LSM while using the previous\nreadout weights. To encourage exploration during training, we follow ϵ-greedy policy [36] for selecting the\nactions based on the Q-values predicted by the LSM. Based on ϵ-greedy policy, we select a random action\nwith probability ϵ and the optimal action, i.e., the action pertaining to the highest Q-value with probability\n(1−ϵ) during training. Initially, ϵ is set to a large value (closer to unity), thereby permitting the agent to\npick a lot of random actions and eﬀectively explore the environment. As training progresses, ϵ gradually\ndecays to a small value, thereby allowing the agent to exploit its past experiences. During evaluation, we\nsimilarly follow ϵ-greedy policy albeit with much smaller ϵ so that there is a strong bias towards exploitation.\nEmploying ϵ-greedy policy during evaluation also serves to mitigate the negative impact of over-ﬁtting or\nunder-ﬁtting. In an eﬀort to further improve stability during training and achieve better generalization\nperformance, we use the experience replay technique proposed by [19]. Based on experience replay, we store\nthe experience discovered at each time-step (i.e. st, at, rt, and st+1) in a large table and later train the\n6\nLSM by sampling mini-batches of experiences in a random manner over multiple training epochs, leading to\nimproved generalization performance. For all the experiments reported in this work, we use the RMSProp\nalgorithm [37] as the optimizer for error backpropagation with mini-batch size of 32. We adopt ϵ-greedy\npolicy, wherein ϵ gradually decays from 1 to 0.001−0.1 over the ﬁrst 10% of the training steps. Replay\nmemory stores one million recently played frames, which are then used for mini-batch weight updates that\nare carried out after the initial 100 training steps. The simulation parameters for Q-learning are summarized\nin Table 5.\n4\nExperimental Results\nWe ﬁrst present results motivating the importance of careful LSM initialization for obtaining rich high-\ndimensional state representation, which is necessary for eﬃcient training of the liquid-to-readout weights. We\nthen demonstrate the utility of the recurrent-liquid synaptic connections of careful LSM initialization using\nclassic cartpole-balancing RL task [20]. We then validate the capability of appropriately initialized LSM,\ntrained using the presented methodology, for solving complex RL tasks like Pacman [23] and Atari games [24].\n4.1\nLSM Hyperparameter Tuning\nInitializing LSM with appropriate parameters is an important step to construct a model that produces useful\nhigh-dimensional representations. Since the input-to-liquid and recurrent-liquid connectivity matrices of the\nLSM are ﬁxed a priori during training, how these connections are initialized dictates the liquid dynamics. We\nchoose the parameters K (governing the input-to-liquid connectivity matrix) and C (governing the recurrent-\nliquid connectivity matrices) empirically based on three observations: (1) stable spiking activity of the liquid,\n(2) eigenvalue analysis of the recurrent connectivity matrices, and (3) development of liquid-excitatory neuron\nmembrane potential.\nSpiking activity of the liquid is said to be stable if every ﬁnite stream of inputs results in a ﬁnite period of\nresponse. Sustained activity indicates that small input noise can perturb the liquid state and lead to chaotic\nactivity that is no longer dependent on the input stimuli. It is impractical to analyze the stability of the\nliquid for all possible input streams within a ﬁnite time. We investigate the liquid stability by feeding in\nrandom input stimuli and sampling the excitatory neuronal spike counts at regular time intervals over the\nLSM-simulation period for diﬀerent values of K and C. We separately adjust these parameters for each\nlearning task using random representations of the environment from the games. Values of K and C are\nexperimentally determined to be 3 and 4 for cartpole and Pacman experiment, respectively, which ensures\nstable liquid spiking activity while enabling the liquid to exhibit fading memory of the past inputs. Fading\nmemory indicates that the liquid retains input information for a short period of time after the input stimuli\nare cut-oﬀ.\nAnalyzing the eigenvalue spectrum of the recurrent connectivity matrix is another tool to assess the\nstability of the liquid. Each eigenvalue in the spectrum represents an individual mode of the liquid. Real\npart indicates decay rate of the mode while the imaginary part corresponds to the frequency of the mode\n[38]. Liquid spiking activity remains stable as long as all eigenvalues remain within the unit circle. However,\nthis condition is not easily met for realistic recurrent-liquid connections with random synaptic weight\ninitialization [39]. We constrain the recurrent weights (hyperparameter β) such that each neuron receives\nbalanced excitatory and inhibitory synaptic currents as previously discussed in subsection 3.1. This results in\neigenvalues that lie within the unit circle as illustrated in Figure 2(A). In order to emphasize the importance\nof LSM initialization, we also show the eigenvalue spectrum of the recurrent-liquid connectivity matrix when\nthe weights are not properly initialized as shown in Figure 2(B) where many eigenvalues are outside the\nunit circle. Finally, we also use the development of the excitatory neuronal membrane potential to guide\nhyperparameter tuning. The hyperparameters C and β are chosen to ensure that membrane potential exhibits\nbalanced ﬂuctuation as illustrated in Figure 2(C) that plots the membrane potential of 10 randomly picked\nneurons in the liquid.\n7\nMembrane potential (V)\n-0.4\n-0.2\n0.2\n0\n0.4\n0.02\n0\n0.04\n0.06\n0.08\n0.1\nNumerical simulation time (s) \nImaginary part of eigenvalues\nReal part of eigenvalues\n-1\n0\n1\n0\n-1\n1\nImaginary part of eigenvalues\nReal part of eigenvalues\n-1\n0\n1\n0\n-1\n1\n(A)\n(B)\n(C)\nFigure 2: Metrics for guiding hyperparameter tuning: (A) Eigenvalue spectrum of the recurrent-liquid\nconnectivity matrix for an LSM containing 500 liquid neurons. The LSM is initialized with synaptic weights\nlisted in Table 3 based on hyperparameter C=4. All eigenvalues in the spectrum lie within a unit circle. (B)\nEigenvalue spectrum of the recurrent-liquid connectivity matrix initialized with synaptic weights βE→E = 0.4,\nβE→I = 0.1, and βI→E = 0.1. Many eigenvalues in the spectrum are outside the unit circle. (C) Development\nof membrane potentials from 10 randomly picked excitatory neurons in the liquid initialized with synaptic\nweights listed in Table 3 based on hyperparameter C=4. Random representation from the cartpole-balancing\nproblem is used as the input.\n8\n4.2\nLearning to Balance a Cartpole\nCartpole-balancing is a classic control problem wherein the agent has to balance a pole attached to a wheeled\ncart that can move freely on a rail of certain length as shown in Figure 3(A). The agent can exert a unit\nforce on the cart either to the left or right side for balancing the pole and keeping the cart within the rail.\nThe environment state is characterized by cart position, cart velocity, angle of the pole, and angular velocity\nof the pole, which are designated by the tuple (χ, ˙χ, ϕ, ˙ϕ). The environment returns a unit reward every\ntime-step and concludes after 200 time-steps if the pole does not fall or the cart does not goes out of the rail.\nBecause the game is played for a ﬁnite time period, we constrain (χ, ˙χ, ϕ, ˙ϕ) to be within the range speciﬁed\nby (±2.5, ±0.5, ±0.28, ±0.88) for eﬃciently mapping the real-valued state inputs to spike trains feeding into\nthe LSM. Each real-valued state input is mapped to 10 input neurons which have ﬁring rates proportional to\none-hot encoding of the input value representing 10 distinct levels within the corresponding range.\nTraining epochs\nAction-value (Q)\n0.8\no\n-2.4\no\n5.2\no\nForce\nPole\nCart\nRail\n(A)\n(B)\n(D)\n0\n40\n80\n120\n160\n200\n 0\n 20\n 40\n 60\n 80\n 100\n0\n40\n80\n120\n160\n200\nMedian accumulated reward\nTraining epochs\n(C)\n 0\n 20\n 40\n 60\n 80\n 100\nMedian accumulated reward\n1\n2\n3\nAction-value (Q)\nLeft (8.33)\nRight (8.32)\nLeft(7.83)\nRight (8.28)\n0\n0\n0\nAction-value (Q)\nLeft(8.07)\nRight (8.00)\nFigure 3: (A) Illustration of the cartpole-balancing task wherein the agent has to balance a pole attached\nto a wheeled cart that moves freely on a rail of certain length. (B) The median accumulated reward per\nepoch provided by the LSM trained across 10 diﬀerent random seeds for the cartpole-balancing task. Shaded\nregion in the plot represents the 25-th to 75-th percentile of the accumulated reward over multiple random\nseeds. (C) The median accumulated reward per epoch from cartpole training across 10 diﬀerent random\nseeds in which the LSM is initialized to have sparser connectivity between the liquid neurons compared\nto that used for the experiment in (B). (D) Visualization of the learnt Q (action-value) function for the\ncartpole-balancing task at three diﬀerent game-steps designated as 1, 2, and 3. Angle of the pole is written\non the left side of each ﬁgure. Negative angle represents an unbalanced pole to the left and positive angle\nrepresents an unbalanced pole to the right. Black arrow corresponds to a unit force on the left or right side\nof the cart depending on which Q value is larger.\nWe model the agent using an LSM containing 150 liquid neurons, 32 hidden neurons in the fully-connected\nlayer between the liquid and output layer, and 2 output neurons. The maximum ﬁring rate for the input\nneurons representing the environment state is set to 100 Hz. The LSM is trained for 105 time-steps, which are\nequally divided into 100 training epochs containing 1,000 time-steps per epoch. After each epoch, the LSM is\nevaluated for 1,000 time-steps with the probability of choosing a random action ϵ set to 0.05. Note that the\nLSM is evaluated for 1,000 time-steps (multiple gameplays) even though single gameplay lasts a maximum of\n9\nonly 200 time-steps as mentioned in the previous paragraph. We use the accumulated reward averaged over\nmultiple gameplays as the true indicator of the LSM (agent) performance to account for the randomness in\naction-selection as a result of the ϵ-greedy policy. We train the LSM initialized with 10 diﬀerent random\nseeds and obtain median accumulated reward as shown in Figure 3(B). Note that the maximum possible\naccumulated reward per gameplay is 200 since each gameplay lasts at most 200 time-steps. Increase in\nmedian accumulated reward over epochs indicates that the LSM learnt to balance the cartpole using the\ndynamically evolving high-dimensional liquid states. The ability of the liquid to provide rich high-dimensional\ninput representations can be attributed to the careful initialization of the connectivity matrices and weights\n(explained in subsection 3.1), which ensures balance between the excitatory and inhibitory currents to the\nliquid neurons and preserves fading memory of past liquid activity. However, the median accumulated\nreward after 100 training epochs saturates around 125 and does not reach the maximum value of 200. We\nhypothesize that the game score saturation comes from the quantized representation of the environment\nstate, and demonstrate in the following experiment with Pacman that the LSM can learn optimally given a\nbetter state representation. Finally, in order to emphasize the importance of LSM initialization, we also show\nthe median accumulated reward per training epoch for training in which the LSM is initialized to have few\nsynaptic connections. Figure 3(C) indicates that the median accumulated reward is around 90 when the\nLSM initialization is suboptimal.\nTo visualize the learnt action-value function guiding action selection, we compare Q-values produced\nby the LSM during evaluation in three diﬀerent scenarios depicted in Figure 3(D). Note that each Q-\nvalue represents how good is the corresponding action for a given environment state. In scenario 1 (see\nFigure 3(D)-1) that corresponds to the beginning of the gameplay wherein the pole is almost balanced,\nthe value of both the actions are identical. This implies that either action (moving the cart left or right)\nwill lead to a similar outcome. In scenario 2 (see Figure 3(D)-2) wherein the pole is unbalanced to the left\nside, the diﬀerence between the predicted Q values increases. Speciﬁcally, the Q value for applying a unit\nforce on the right side of the cart is higher, which causes the cart to move to the left. Pushing the cart to\nthe left in turn causes the pole to swing back right towards the balanced position. Similarly, in scenario 3\n(see Figure 3(D)-3) wherein the pole is unbalanced to the right side, the Q value is higher for applying a\nunit force on the left side of the cart, which causes the cart to move right and enables the pole to swing left\ntowards the balanced position. This visually demonstrates the ability of the LSM (agent) to successfully\nbalance the pole by pushing the cart appropriately to the left or right based on the learnt Q values.\n4.3\nLearning to Play Pacman\n...\n...\n...\n...\nBinary representation\nEnvironment representation\nGame snapshot\nFigure 4: Illustration of a snapshot from the Pacman game that is translated into 5 two-dimensional binary\nrepresentations corresponding to the location of Pacman, foods, cherries, ghosts, and scared ghosts. The\nbinary intermediate representations are then ﬂattened and concatenated to obtain the environment state\nrepresentation.\nIn order to comprehensively validate the eﬃcacy of the high-dimensional environment representations\n10\nprovided by the liquid, we train the LSM to play a game of Pacman [23]. The objective of the game is to\ncontrol Pacman (yellow in color) to capture all the foods (represented by small white dots) in a grid without\nbeing eaten by the ghosts as illustrated in Figure 4. The ghosts always hunt the Pacman; however, cherry\n(represented by large white dots) make the ghosts temporarily scared of the Pacman and run away. The\ngame environment returns unit reward whenever Pacman consumes food, cherry, or the scared ghost (white\nin color). The game environment also returns a unit reward and restarts when all foods are captured. We use\nthe location of Pacman, food, cherry, ghost and scared ghost as the environment state representation. The\nlocation of each object is encoded as a two-dimensional binary array whose dimension matches with that of\nthe Pacman grid as shown in Figure 4. The binary intermediate representations of all the objects are then\nconcatenated and ﬂattened into a single vector to be fed to the input layer of the LSM.\nTraining epochs\nGrid size 7x17\n0\n2\n4\n6\n8\n10\n12\n 0\n 20\n 40\n 60\n 80\n 100\nMedian accumulated reward\nMedian accumulated reward\nTraining epochs\nGrid size 17x19\n1\n-1\n3\n7\n5\n 0\n 20\n 40\n 60\n 80\n 100\nMedian accumulated reward\n1.5\n2.5\n2.0\n3.0\n3.5\n4.0\n 0\n20\n 40\n 60\n 80\nQ\n_\nGame steps\n1\n2\n3\n4\n1\n3\n4\n2\nTraining epochs\nGrid size 7x7\n-1\n1\n0\n2\n3\n4\n 0\n 20\n 40\n 60\n 80\n 100\n(A)\n(B)\n(C)\n(D)\nFigure 5: Median accumulated reward per epoch obtained by training and evaluating the LSM\nin 3 diﬀerent game settings: (A) grid size 7 × 7, (B) grid size 7 × 17, and (C) grid size 17 × 19. LSM\nis initialized and trained with 7 diﬀerent initial random seeds. Shaded region represents the 25-th to 75-th\npercentile of the accumulated reward over multiple seeds. (D) The plot on the left shows the predicted\nstate-value function for 80 continuous Pacman game steps. The four snapshots from the Pacman game shown\non the right correspond to game steps designated as 1, 2, 3, and 4, respectively, in the state-value plot.\nThe LSM conﬁgurations and game settings used for Pacman experiments are summarized in Table 1, where\neach game setting has diﬀerent degree of complexity with regards to the Pacman grid size and the number of\nfoods, ghosts, and cherries. In the ﬁrst experiment, we use a 7 × 7 grid with 3 foods for Pacman to capture\nand a single ghost to prevent it from achieving its objective. Thus, the maximum possible accumulated reward\nat the end of a successful game is 4. Figure 5(A) shows that the median accumulated reward gradually\nincreases with the number of training epochs and converges closer to the maximum possible reward, thereby\nvalidating the capability of the liquid to provide useful high-dimensional representation of the environment\nstate necessary for eﬃcient training of the readout weights using the presented methodology. Interestingly, in\nthe second experiment using a larger 7 × 17 grid, we ﬁnd that the median reward converges to 12, which is\ngreater than the number of foods available in the grid. This indicates that the LSM does not only learn to\ncapture all the foods; in addition, it also learns to capture the cherry and the scared ghosts, leading to further\nincrease the accumulated reward since consuming the scared ghost results in a unit immediate reward. In the\n11\nﬁnal experiment, we train the LSM to control Pacman in 17 × 19 grid with sparsely dispersed foods. We\nﬁnd that larger grid requires more exploration and training steps for the agent to perform well and achieve\nthe maximum possible reward, resulting in a learning curve that is less steep compared to that obtained for\nsmaller grid sizes in the earlier experiments as shown in Figure 5(C).\nFinally, we plot the average of Q-values produced by the LSM as the Pacman navigates the grid to visualize\nthe correspondence between the learnt Q-values and the enviroment state. As discussed in subsection 3.2, each\nQ-value produced by the LSM provides a measure of how good is a particular action for a give environment\nstate. The Q-value averaged over the set of all possible actions (known as the state-value function) thus\nindicates the value of being in a certain state. Figure 5(D) illustrates the state-value function while playing\nthe Pacman game in a 7×17 grid. The predicted state-value starts at a relatively high level because the\nfoods are abundant in the grid and the ghosts are far away from the Pacman (see Figure 5(D)-1). The\nstate-value gradually decreases as the Pacman navigates through the grid and gets closer to the ghosts. The\npredicted state-value then shoots up after the Pacman consumes cherry and makes the ghosts temporarily\nconsumable (see Figure 5(D)-2), leading to potential additional reward. The predicted state-value drops\nafter the ghosts are reborn (see Figure 5(D)-3). Finally, we observe a slight increase in the state-value\ntowards the end of the game when the Pacman is closer to the last food after it consumes a cherry (see\nFigure 5(D)-4). It is interesting to note that although the scenario in Figure 5(D)-4 is similar to that in\nFigure 5(D)-2, the state-value is smaller since the expected accumulated reward at this step is at most\n3 assuming that the Pacman can capture both the scared ghost and the last food. On the other hand, in\nthe environment state shown in Figure 5(D)-2, the expected accumulated reward is greater than 3 since 4\nfoods and 2 scared ghosts are available for the Pacman to capture.\n4.4\nLearning to Play Atari Games\nFinally, we train the LSM using the presented methodology to play Atari games [24], which are widely\nused to benchmark deep reinforcement learning networks. We arbitrarily select 4 games for evaluation,\nnamely, Boxing, Gopher, Freeway, and Krull. We use the RAM of the Atari machine, which stores 128\nbytes of information about an Atari game, as a representation of the environment [24]. During training, we\nmodiﬁed the reward structure of the game by clipping all positive immediate rewards to 1 and all negative\nimmediate rewards to −1. However, we do not clip the immediate reward during testing and measure the\nactual accumulated reward following [19]. For all selected Atari games, we model the agent using an LSM\ncontaining 500 liquid neurons and 128 hidden neurons. Number of output neurons varies for each game as\nthe number of possible actions is diﬀerent. The maximum Poisson ﬁring rate for the input neurons is set\nto 100 Hz. The LSM is trained for 5 × 103 steps. Figure 6 illustrates that the LSM learnt the optimal\nstrategies to play Boxing and Krull without any prior knowledge of the rules, leading to high accumulated\nreward towards the end of the training. However, learning in Gopher and Freeway progresses relatively slow.\nFor detailed evaluation, we compare the median accumulated reward obtained from playing with the trained\nLSM to the average accumulated reward obtained from playing with random actions for 1 × 105 steps. We\nalso compare the accumulated reward with that reported for human players in [19]. Table 2 shows that the\nLSM achieves better score than human players on Boxing and Krull while comparable albeit lower score on\nFreeway and Gopher.\n5\nDiscussion\nLSM, an important class of biologically plausible recurrent SNNs, has thus far been primarily demonstrated\nfor pattern (speech/image) recognition [12, 13], gesture recognition [14, 15], and sequence generation tasks\n[16, 17, 18] using standard datasets. To the best of our knowledge, our work is the ﬁrst demonstration of\nLSMs, trained using Q-learning based methodology, for complex RL tasks like Pacman and Atari games\ncommonly used to evaluate deep reinforcement learning networks. The beneﬁts of the proposed LSM-based\nRL framework over the state-of-the-art deep learning models are two-fold. First, LSM entails fewer trainable\nparameters as a result of using ﬁxed input-to-liquid and recurrent-liquid synaptic connections. However, this\nrequires careful initialization of the respective matrices for eﬃcient training of the liquid-to-readout weights\nas experimentally validated in section 4. We note that the stability of LSMs could be further enhanced\nby training the recurrent weights using localized Spike Timing Dependent Plasticity based learning rules\n12\n 0\n 20\n 40\n 60\n 80\n 100\nFreeway\nTraining epochs\n0\n5\n10\n15\n20\nMedian accumulated reward\n0\n400\n800\n 0\n 20\n 40\n 60\n 80\n 100\nTraining epochs\nGopher\nMedian accumulated reward\n-30\n-10\n10\n30\n 0\n 20\n 40\n 60\n 80\n 100\nTraining epochs\nBoxing\n 0\n 20\n 40\n 60\n 80\n 100\nKrull\nTraining epochs\n500\n1500\n2500\n3500\nMedian accumulated reward\nMedian accumulated reward\n(A)\n(C)\n(D)\n(B)\nFigure 6: Median accumulated reward per epoch obtained by training and evaluating the LSM\nfor 4 selected Atari games: (A) Boxing, (B) Freeway, (C) Gopher, and (D) Krull. For each game, LSM\nis initialized and trained with 5 diﬀerent initial random seeds. Shaded region represents the 25-th to 75-th\npercentile of the accumulated reward over multiple seeds.\n13\n[40, 41, 42], which incur lower computational complexity compared to the backpropagation-through-time\nalgorithm [43, 12] used for training recurrent SNNs. Second, LSMs can be eﬃciently implemented on\nevent-driven neuromorphic hardware like IBM TrueNorth [28] or Intel Loihi [29], leading to potentially much\nimproved energy eﬃciency while achieving comparable performance to deep learning models on the chosen\nbenchmark tasks. Note that the readout layer in the presented LSM needs to be implemented outside the\nneuromorphic fabric since they are composed of artiﬁcial rate-based neurons that are typically not supported\nin neuromorphic hardware realizations. Alternatively, readout layer composed of spiking neurons could be\nused that can be trained using spike-based error backpropagation algorithms [44, 45, 46, 47, 48, 18]. Future\nworks could also explore STDP-based reinforcement learning rules [49, 50, 51, 52] to render the training\nalgorithm amenable for neuromorphic hardware implementations.\n6\nConclusion\nLiquid State Machine (LSM) is a bio-inspired recurrent spiking neural network composed of an input layer\nsparsely connected to a randomly interlinked liquid of spiking neurons for the real-time processing of spatio-\ntemporal inputs. In this work, we proposed LSMs, trained using the presented Q-learning based methodology,\nfor solving complex Reinforcement Learning (RL) tasks like playing Pacman and Atari that have been\nhitherto benchmarked for deep reinforcement learning networks. We presented initialization strategies for the\nﬁxed input-to-liquid and recurrent-liquid synaptic connectivity matrices and weights to enable the liquid to\nproduce useful high-dimensional representation of the environment state necessary for eﬃcient training of\nthe liquid-to-readout weights. We demonstrated the signiﬁcance of the inherent capability of the liquid to\nproduce rich representation by training the LSM to successfully balance a cartpole. Our experiments on the\nPacman game showed that the LSM learns the optimal strategies for diﬀerent game settings and grid sizes.\nOur analyses on a subset of Atari games indicated that the LSM achieves comparable score to that reported\nfor human players in existing works.\nAuthor Contributions\nGopalakrishnan Srinivasan and Wachirawit Ponghiran wrote the paper. Wachirawit Ponghiran performed the\nsimulations. All authors helped with developing the concepts, conceiving the experiments, and writing the\npaper.\nAcknowledgments\nThis work was supported in part by the Center for Brain Inspired Computing (C-BRIC), one of the six\ncenters in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA, by the\nSemiconductor Research Corporation, the National Science Foundation, Intel Corporation, the DoD Vannevar\nBush Fellowship, and by the U.S. Army Research Laboratory and the U.K. Ministry of Defence under\nAgreement Number W911NF-16-3-0001.\nReferences\n[1] Rodney J Douglas, Christof Koch, Misha Mahowald, KA Martin, and Humbert H Suarez. Recurrent\nexcitation in neocortical circuits. Science, 269(5226):981–985, 1995.\n[2] Kenneth D Harris and Thomas D Mrsic-Flogel. Cortical connectivity and sensory coding. Nature,\n503(7474):51, 2013.\n[3] Xiaolong Jiang, Shan Shen, Cathryn R Cadwell, Philipp Berens, Fabian Sinz, Alexander S Ecker, Saumil\nPatel, and Andreas S Tolias. Principles of connectivity among morphologically deﬁned cell types in adult\nneocortex. Science, 350(6264):aac9462, 2015.\n14\n[4] Wolfgang Maass, Thomas Natschl¨ager, and Henry Markram. Real-time computing without stable states:\nA new framework for neural computation based on perturbations. Neural computation, 14(11):2531–2560,\n2002.\n[5] Wolfgang Maass, Thomas Natschl¨ager, and Henry Markram. A model for real-time computation in\ngeneric neural microcircuits. In Advances in neural information processing systems, pages 229–236,\nVancouver, British Columbia, Canada, 2003.\n[6] Mantas Lukoˇseviˇcius and Herbert Jaeger. Reservoir computing approaches to recurrent neural network\ntraining. Computer Science Review, 3(3):127–149, 2009.\n[7] Peter Dayan, LF Abbott, et al. Theoretical neuroscience: computational and mathematical modeling of\nneural systems. Journal of Cognitive Neuroscience, 15(1):154–155, 2003.\n[8] John E. Savage. Models of computation, volume 136. Addison-Wesley, Reading, MA, USA, 1998.\n[9] Daniel J. Amit. Modeling brain function: The world of attractor neural networks. Cambridge university\npress, New York, NY, USA, 1992.\n[10] Peter Auer, Harald Burgsteiner, and Wolfgang Maass. Reducing communication for distributed learning\nin neural networks. In International Conference on Artiﬁcial Neural Networks, pages 123–128. Springer,\n2002.\n[11] David Verstraeten, Benjamin Schrauwen, Dirk Stroobandt, and Jan Van Campenhout. Isolated word\nrecognition with the liquid state machine: a case study. Information Processing Letters, 95(6):521–528,\n2005.\n[12] Guillaume Bellec, Darjan Salaj, Anand Subramoney, Robert Legenstein, and Wolfgang Maass. Long\nshort-term memory and learning-to-learn in networks of spiking neurons. arXiv preprint arXiv:1803.09574,\n2018.\n[13] Gopalakrishnan Srinivasan, Priyadarshini Panda, and Kaushik Roy. Spilinc: Spiking liquid-ensemble\ncomputing for unsupervised speech and image recognition. Frontiers in neuroscience, 12, 2018.\n[14] Joseph Chrol-Cannon and Yaochu Jin. Learning structure of sensory inputs with synaptic plasticity\nleads to interference. Frontiers in Computational Neuroscience, 9:103, 2015.\n[15] Priyadarshini Panda and Narayan Srinivasa. Learning to recognize actions from limited training examples\nusing a recurrent spiking neural model. Frontiers in neuroscience, 12:126, 2018.\n[16] Priyadarshini Panda and Kaushik Roy. Learning to generate sequences with combination of hebbian and\nnon-hebbian plasticity in recurrent spiking neural networks. Frontiers in neuroscience, 11:693, 2017.\n[17] Wilten Nicola and Claudia Clopath. Supervised learning in spiking neural networks with force training.\nNature communications, 8(1):2208, 2017.\n[18] Guillaume Bellec, Franz Scherr, Elias Hajek, Darjan Salaj, Robert Legenstein, and Wolfgang Maass.\nBiologically inspired alternatives to backpropagation through time for learning in recurrent neural nets.\narXiv preprint arXiv:1901.09049, 2019.\n[19] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,\nAlex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control\nthrough deep reinforcement learning. Nature, 518(7540):529, 2015.\n[20] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press,\nCambridge, MA, USA, 1998.\n[21] David E Rumelhart, Geoﬀrey E Hinton, and Ronald J Williams. Learning representations by back-\npropagating errors. nature, 323(6088):533, 1986.\n[22] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279–292, 1992.\n15\n[23] John DeNero, Dan Klein, and Pieter Abbeel. The Pacman Projects, 2016.\n[24] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\nWojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\n[25] Prashant Joshi and Wolfgang Maass. Movement generation with circuits of spiking neurons. Neural\nComputation, 17(8):1715–1738, 2005.\n[26] Nicolas Berberich. Implementation of a real -time liquid state machine on spinnaker for biomimetic\nrobot controll. Masterarbeit, TUM, 2017.\n[27] Juan Camilo Vasquez Tieck, Marin Vlastelica Poganˇci´c, Jacques Kaiser, Arne Roennau, Marc-Oliver\nGewaltig, and R¨udiger Dillmann. Learning continuous muscle control for a multi-joint arm by extending\nproximal policy optimization with a liquid state machine. In International Conference on Artiﬁcial\nNeural Networks, pages 211–221. Springer, 2018.\n[28] Paul A Merolla, John V Arthur, Rodrigo Alvarez-Icaza, Andrew S Cassidy, Jun Sawada, Filipp Akopyan,\nBryan L Jackson, Nabil Imam, Chen Guo, Yutaka Nakamura, et al. A million spiking-neuron integrated\ncircuit with a scalable communication network and interface. Science, 345(6197):668–673, 2014.\n[29] Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha Choday,\nGeorgios Dimou, Prasad Joshi, Nabil Imam, Shweta Jain, et al. Loihi: A neuromorphic manycore\nprocessor with on-chip learning. IEEE Micro, 38(1):82–99, 2018.\n[30] Michael Wehr and Anthony M Zador. Balanced inhibition underlies tuning and sharpens spike timing in\nauditory cortex. Nature, 426(6965):442, 2003.\n[31] Vinod Nair and Geoﬀrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In\nProceedings of the 27th international conference on machine learning (ICML-10), pages 807–814, Haifa,\nIsrael, 2010.\n[32] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley,\nDavid Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In\nInternational conference on machine learning, pages 1928–1937, 2016.\n[33] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,\nDavid Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint\narXiv:1509.02971, 2015.\n[34] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,\nJulian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the\ngame of Go with deep neural networks and tree search. nature, 529(7587):484, 2016.\n[35] David Heeger. Poisson model of spike generation. Handout, University of Standford, 5:1–13, 2000.\n[36] Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. PhD thesis, King’s College,\nCambridge, 1989.\n[37] Tijmen Tieleman and Geoﬀrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average\nof its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26–31, 2012.\n[38] Kanaka Rajan, LF Abbott, and Haim Sompolinsky. Stimulus-dependent suppression of chaos in recurrent\nneural networks. Physical Review E, 82(1):011903, 2010.\n[39] Kanaka Rajan and LF Abbott. Eigenvalue spectra of random matrices for neural networks. Physical\nreview letters, 97(18):188104, 2006.\n[40] Guo-qiang Bi and Mu-ming Poo. Synaptic modiﬁcations in cultured hippocampal neurons: dependence on\nspike timing, synaptic strength, and postsynaptic cell type. Journal of neuroscience, 18(24):10464–10472,\n1998.\n16\n[41] Sen Song, Kenneth D Miller, and Larry F Abbott. Competitive hebbian learning through spike-timing-\ndependent synaptic plasticity. Nature neuroscience, 3(9):919, 2000.\n[42] Peter U Diehl and Matthew Cook. Unsupervised learning of digit recognition using spike-timing-dependent\nplasticity. Frontiers in computational neuroscience, 9:99, 2015.\n[43] Paul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the IEEE,\n78(10):1550–1560, 1990.\n[44] Priyadarshini Panda and Kaushik Roy. Unsupervised regenerative learning of hierarchical features in\nspiking deep networks for object recognition. In 2016 International Joint Conference on Neural Networks\n(IJCNN), pages 299–306, Vancouver, British Columbia, Canada, 2016. IEEE.\n[45] Jun Haeng Lee, Tobi Delbruck, and Michael Pfeiﬀer. Training deep spiking neural networks using\nbackpropagation. Frontiers in neuroscience, 10:508, 2016.\n[46] Chankyu Lee, Priyadarshini Panda, Gopalakrishnan Srinivasan, and Kaushik Roy.\nTraining deep\nspiking convolutional neural networks with stdp-based unsupervised pre-training followed by supervised\nﬁne-tuning. Frontiers in neuroscience, 12:435, 2018.\n[47] Yujie Wu, Lei Deng, Guoqi Li, Jun Zhu, and Luping Shi. Spatio-temporal backpropagation for training\nhigh-performance spiking neural networks. Frontiers in neuroscience, 12:331, 2018.\n[48] Yingyezhe Jin, Wenrui Zhang, and Peng Li. Hybrid macro/micro level backpropagation for training\ndeep spiking neural networks. In Advances in Neural Information Processing Systems, pages 7005–7015,\nMontral, Quebec, Canada, 2018.\n[49] Jean-Pascal Pﬁster, Taro Toyoizumi, David Barber, and Wulfram Gerstner. Optimal spike-timing-\ndependent plasticity for precise action potential ﬁring in supervised learning. Neural computation,\n18(6):1318–1348, 2006.\n[50] R˘azvan V Florian. Reinforcement learning through modulation of spike-timing-dependent synaptic\nplasticity. Neural Computation, 19(6):1468–1502, 2007.\n[51] Michael A Farries and Adrienne L Fairhall. Reinforcement learning with modulated spike timing–\ndependent synaptic plasticity. Journal of neurophysiology, 98(6):3648–3665, 2007.\n[52] Robert Legenstein, Dejan Pecevski, and Wolfgang Maass. A learning theory for reward-modulated spike-\ntiming-dependent plasticity with application to biofeedback. PLoS computational biology, 4(10):e1000180,\n2008.\n17\nTable 1: LSM conﬁguration and game settings for diﬀerent Pacman experiments reported in this work.\nGrid size\nGhost\nFood\nCherry\nTraining steps\nLiquid neurons\nHidden neurons\n7×7\n1\n3\n0\n5 × 105\n500\n128\n7×17\n2\n6\n2\n5 × 105\n2,000\n512\n17×19\n1\n6\n0\n3 × 106\n3,000\n512\nTable 2: Comparison between median accumulated rewarded over multiple random seeds, average accumulated\nreward from playing with random actions, and accumulated reward from human game tester reported in [19].\nBest median accumulated reward over the last 10 training epochs is reported for each game.\nGame\nHuman player\nRandom actions\nThis work\nBoxing\n4.3\n0.75\n20.2\nFreeway\n29.6\n0.0\n19.75\nGopher\n2,321\n279.3\n611.1\nKrull\n2,395\n1,590\n3,686\nTable 3: Synaptic weight initialization parameters for the ﬁxed LSM connections.\nInput-to-liquid connections\nConnection type\nWeight\nP→E\n[0, 0.6]\nRecurrent-liquid connections\nConnection type\nWeight\nE→E\n[0, 0.05]\nE→I\n[0, 0.25]\nI→E\n[0, 0.3]\nI→I\n[0, 0.01]\n18\nTable 4: Leaky-Integrate-and-Fire (LIF) model parameters for the liquid neurons.\nExcitatory and inhibitory neurons\nParameter\nValue\nVrest\n0\nVreset\n0\nVthres\n0.5\nτ\n20 ms\nτrefrac\n1 ms\n∆t (simulation time-step)\n1 ms\nTable 5: Q-learning simulation parameters.\nParameter\nValue\nReadout weights update frequency\nOnce every game-step\nWarm up steps before training begins\n100\nBatch size for experience replay\n32\nExperience replay buﬀer size\n1 × 106\nDiscount factor\n0.95\nInitial exploration probability during training\n1\nFinal exploration probability during training (Cartpole)\n1 × 10−3\nFinal exploration probability during training (Pacman & Atari)\n1 × 10−1\nExploration probability during evaluation (Cartpole & Atari)\n5 × 10−2\nExploration probability during evaluation (Pacman)\n0\nLearning rate for RMSProp algorithm\n2 × 10−4\nTerm added to denominator for RMSProp algorithm\n1 × 10−6\nWeight decay for RMSProp algorithm\n0\nSmoothing constant for RMSProp algorithm\n0.99\n19\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-06-04",
  "updated": "2019-06-04"
}