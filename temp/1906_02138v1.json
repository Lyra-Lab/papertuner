{
  "id": "http://arxiv.org/abs/1906.02138v1",
  "title": "Exploration with Unreliable Intrinsic Reward in Multi-Agent Reinforcement Learning",
  "authors": [
    "Wendelin Böhmer",
    "Tabish Rashid",
    "Shimon Whiteson"
  ],
  "abstract": "This paper investigates the use of intrinsic reward to guide exploration in\nmulti-agent reinforcement learning. We discuss the challenges in applying\nintrinsic reward to multiple collaborative agents and demonstrate how\nunreliable reward can prevent decentralized agents from learning the optimal\npolicy. We address this problem with a novel framework, Independent\nCentrally-assisted Q-learning (ICQL), in which decentralized agents share\ncontrol and an experience replay buffer with a centralized agent. Only the\ncentralized agent is intrinsically rewarded, but the decentralized agents still\nbenefit from improved exploration, without the distraction of unreliable\nincentives.",
  "text": "Exploration with Unreliable Intrinsic Reward\nin Multi-Agent Reinforcement Learning\nWendelin B¨ohmer 1 Tabish Rashid 1 Shimon Whiteson 1\nAbstract\nThis paper investigates the use of intrinsic reward\nto guide exploration in multi-agent reinforcement\nlearning. We discuss the challenges in applying\nintrinsic reward to multiple collaborative agents\nand demonstrate how unreliable reward can pre-\nvent decentralized agents from learning the op-\ntimal policy. We address this problem with a\nnovel framework, Independent Centrally-assisted\nQ-learning (ICQL), in which decentralized agents\nshare control and an experience replay buffer with\na centralized agent. Only the centralized agent\nis intrinsically rewarded, but the decentralized\nagents still beneﬁt from improved exploration,\nwithout the distraction of unreliable incentives.\n1. Introduction\nRecent successes in challenging computer games like Star-\nCraft 2 (Vinyals et al., 2019) have raised interest in Multi-\nagent Reinforcement Learning (MARL). Here single units\nare modeled as individual agents, for example, in the re-\ncent open source StarCraft Multi-agent Challenge (SMAC,\nSamvelyan et al., 2019). In comparison to single-agent deep\nRL, MARL faces some unique challenges, in particular de-\ncentralization and coordination. In this paper we investigate\nthe equally challenging problem of directed exploration.\nDirected exploration in single-agent deep RL still poses\nmany open questions, like how to generalize visitation\ncounts in large input spaces and how to change the ex-\nploration policy quickly towards newly discovered states.\nHowever, so far, there has been little work on exploration\nfor deep MARL. Exploration in MARL differs from the\nsingle-agent setting in some challenging ways: (i) count-\ning visitations of state-action pairs is infeasible for many\nagents, due to the large joint-action space; (ii) as unexpected\noutcomes can be caused by multiple agents, it is not clear\n1Department of Computer Science, University of Oxford,\nUnited Kingdom. Correspondence to: Wendelin B¨ohmer <wen-\ndelin.boehmer@cs.ox.ac.uk>.\nAccepted to the 2 nd Exploration in Reinforcement Learning Work-\nshop at the International Conference on Machine Learning 2019.\nwhich agent’s action should be reinforced; and (iii) partial\nobservability decreases the reliability of count estimates.\nDecentralization is required in MARL when the agents can-\nnot communicate directly. Moreover, a centralized control\npolicy is often infeasible, as the joint-action space grows\nexponentially in the number of agents. In line with SMAC,\nwe consider the case where the state of the system is only\npartially observable by each agent, although during training\nthe global state may be available. This is called central-\nized training with decentralized execution (Foerster et al.,\n2016). This paper focuses on the simplest value-based algo-\nrithm in this class, a variant of Independent Q-learning (IQL,\nTan, 1993), where each agent acts on partial observations\nand assumes the other agents’ decisions are an unobserved,\nstationary part of the environment.\nHowever, these simple decentralized agents lack coordina-\ntion. Take the example of two predators, who trapped their\nprey in a corner. To catch it, both must attack simultane-\nously, as it will escape if only one predator attacks. From\nthe perspective of each predator, the reward for attacking\ndepends on the actions of the other. When the punishment\nfor letting the prey escape is larger than the reward for\ncatching it, neither agent will learn the optimal strategy\nindependently. There are multiple methods using central-\nized training to mitigate this effect for decentralized poli-\ncies, e.g., multi-agent credit assignment (COMA, Foerster\net al., 2018) and bootstrapping with an approximation of the\njoint Q-value function. For example, Value Decomposition\nNetworks (VDN, Sunehag et al., 2018) optimize the joint\nQ-value, but restrict the Q-value function to a sum of indi-\nvidual agents’ utilities. QMIX (Rashid et al., 2018) goes one\nstep further and mixes the agents’ utilities with a non-linear\nhyper-network, that conditions on the global state. Both\napproaches can execute the decentralized learned policy by\nmaximizing each agent’s utility. However, all the above\ntechniques use relatively simple ϵ-greedy exploration.\nValue-based algorithms that explore provably efﬁciently in a\ntabular setting (e.g. Jin et al., 2018) rely on optimism in the\nface of uncertainty. There are two major lines of research\nin the literature: to use intrinsic reward to over-estimate\nuncertain state-action values or to use Thompson sampling\nfrom a Bayesian posterior of the value function. Various\narXiv:1906.02138v1  [cs.AI]  5 Jun 2019\nExploration with Unreliable Intrinsic Reward in MARL\ntechniques have been proposed to estimate the uncertainty\nof state-action values (summarized in Appendix B), but\nwhether it is used as an intrinsic reward or as the standard\ndeviation of a Gaussian posterior, most works converge at\nan estimate that is supposed to scale with 1/\np\nNt(st, ut),\nwhere Nt(st, ut) counts how often state st and action ut\nhave been observed at time t. For large input spaces, how-\never, these estimates are rough approximations of visitation\ncounts and the resulting uncertainties are highly unreliable.\nThis paper investigates estimated intrinsic reward for decen-\ntralized IQL agents. We evaluate the variance of linear func-\ntions (O’Donoghue et al., 2018) as an uncertainty estimate\nin a novel predator-prey task that emphasizes exploration.\nWe observe empirically that the intrinsic reward accelerates\nlearning, but remains inherently unreliable, which prevents\nﬁnding the optimal solution. To learn reliable decentral-\nized policies in the face of unreliable reward, we propose\nto share control with a second agent that is discarded after\ntraining and can thus be centralized. Only the central agent\nreceives intrinsic rewards, which prevents the decentralized\nagents from being distracted, while still improving their\nexploratory behavior. We show that this new approach to\nMARL exploration drastically speeds up learning of the\ndecentralized agents, while converging to the optimal solu-\ntion. This novel framework is general and can be applied\nto different estimators of intrinsic reward and/or off-policy\nMARL algorithms like VDN and QMIX.\n2. Background\nWe restrict ourselves to cooperative tasks, modeled as a\nDec-POMDP (Oliehoek & Amato, 2016), that is, a tuple\n⟨S, {Ua}, P, r, {Za}, {Oa}, n, γ⟩. The global state of the\nsystem is denoted as s ∈S, and each of the n agents\nchooses actions ua ∈Ua, which together form the joint\naction u ∈U. After executing joint action ut in state st at\ndiscrete time step t, the next state st+1 is drawn from tran-\nsition kernel P(st+1|st, ut), and a reward rt := r(st, ut)\nis determined by the reward function r : S × U →IR.\nWhile a centralized joint policy πc(u|st) can choose joint\nactions u conditioned on the current state st, a decentral-\nized agent policy πa(ua|τ a\nt ) draws only agent a’s action\nua ∈Ua, based on the agent’s current trajectory τ a\nt of\npast actions ua\ni and observations za\ni\n∈Za, which are\ndrawn from the agent’s observation kernel Oa(za\ni |si), that\nis, τ a\nt := [za\n0, ua\n0, za\n1, ua\n1, . . . , za\nt ]. Execution of a joint pol-\nicy π yields an episode with return Rt := P∞\ni=t γi−tri\nat time t. Our goal is to ﬁnd a decentralized joint pol-\nicy π(u|{τ a\nt }) := Qn\na=1 πa(ua|τ a\nt ), which maximizes the\nexpected return for each observed trajectory. Partial observ-\nability of the policy can signiﬁcantly slow down learning\nand we allow access to the global state during training, that\nis, centralized training for decentralized execution.\n2.1. Independent Q-learning (IQL)\nIndependent Q-learning (Tan, 1993) approaches this goal by\ndeﬁning the state-action value function of agent a as the ex-\npectation of the return, following policy π from an observed\ntrajectory τ a\nt , that is, Qa(ua|τ a\nt ) := IEπ[Rt|τ a\nt , ua]. As in\nQ-learning (Watkins & Dayan, 1992), IQL assumes that the\ngreedy policy, which chooses the action with the largest\ncorresponding value, maximizes the expected return in each\nstate. Note that this is not true, as the expected return also\ndepends on the other agents’ behavior, which can introduce\nnon-stationarity. That being said, IQL appears to be stable\nin practice and works quite well in most tasks.\nWe use a neural network with one head for each discrete\naction to approximate the value function (as in DQN, Mnih\net al., 2015). For IQL, we learn a function1 qa\nθ(ua|τ a\nt ),\nparameterized by θ, with gradient-descend on the expected\nsquared Bellman error\nmin\nθ\nIE\nh ∞\nP\nt=0\n\u0000rt+γ max\nu′ qa\nθ′(u′|τ a\nt+1)−qa\nθ(ua\nt |τ a\nt )\n\u00012i\n, (1)\nwhere θ′ are the parameters of a target network, which are\nreplaced with a copy of the current parameters θ from time to\ntime to improve stability. The expectation is approximated\nby drawing mini-batches of trajectories from an experience\nreplay buffer (Lin, 1992). We also use double Q-learning\n(Hasselt et al., 2016) to further improve stability and share\nthe parameters θ of all agents’ value functions for better\ngeneralization (similar to QMIX, Rashid et al., 2018).\n2.2. Intrinsic Reward\nWe employ a local uncertainty measure introduced by\nO’Donoghue et al. (2018). The variance of a linear re-\ngression, i.e., ﬁtting a linear function f(u|s) = w⊤\nu φ(s)\nto a ﬁxed set of state-action pairs {si, ui}t\ni=1 and random\nlabels yi with a Gaussian distribution N(yi|y(si), σ2), is\nVt[f](u|s) = σ2 φ(s)⊤\u0010\ntP\ni=1\nδuiu φ(si)φ(si)\n\u0011−1\nφ(s) . (2)\nAs each head of the IQL value function qa(ua|τ a\nt ) is a lin-\near function of the last network layer, that is, the hidden\nstate φa(τ a\nt ) of a GRU, O’Donoghue et al. (2018) sug-\ngest to use r+\nt :=\np\nVt[qa](ua\nt |τ a\nt ) as a measure of local\nuncertainty. This choice of intrinsic reward is somewhat\njustiﬁed, as for one-hot coded φ(st), the intrinsic reward\nis r+\nt = σ/\np\nNt(st, ut), which corresponds to the tabular\ncase (e.g., in Jin et al., 2018) with scaling factor σ.\n1To condition on trajectories τ a\nt , we follow Hausknecht &\nStone (2015) and use a recurrent network of GRU units (Chung\net al., 2014), which condition on their hidden state, the last action\nua\nt−1, the current observation za\nt and the agent id a.\nExploration with Unreliable Intrinsic Reward in MARL\n3. Method\nIntrinsic reward based on estimated uncertainties rarely re-\nﬂects the precise visitation counts. We investigate how such\nunreliable intrinsic reward can still reliably improve explo-\nration of decentralized agents. The main idea is to introduce\na second controller during training that can be discarded af-\nterwards. This joint agent is intrinsically rewarded and can\nthus explore the environment in a directed fashion. In princi-\nple, the agent’s policy could be learned by many algorithms.\nAs it is only active during training, though, we propose a\ncentralized agent, which conditions on the more informa-\ntive global state. Most importantly, we train simultaneously\nthe decentralized agents, which will be later deployed for\nexecution, on the same replay buffer. These can utilize any\ndecentralized off-policy learning algorithm, but we focus in\nthe following on IQL for simplicity.\n3.1. A Central MARL Agent\nThe large action spaces in MARL make individual heads\nfor each joint action u on value functions infeasible in the\nface of many agents. Maximizing a value function that con-\nditions on all agents’ actions, on the other hand, has to be\nevaluated for all u as well, which can be prohibitively ex-\npensive. Instead, we use the architecture of a COMA critic,\nwhich Foerster et al. (2018) introduced in the context of a\npolicy gradient method. They deﬁne an agent-speciﬁc joint-\nvalue function qa\nψ, parameterized by ψ, which has a head for\neach of a’s actions ua\nt and conditions on the global state, all\nother agents’ actions u−a\nt\n:= [u1\nt, . . . , ua−1\nt\n, ua+1\nt\n, . . . , un\nt ]\nand agent a’s the previous action ua\nt−1:\nqa\nψ(ua\nt |st, u−a\nt\n, ua\nt−1)\n!≈\nIE[Rt|st, ut] .\n(3)\nInstead of maximizing this function w.r.t. the joint action\nut, we propose here to approximate a local maximum by\niteratively choosing the ua\nt that maximizes each individual\nagent’s qa\nψ, and using it for u−a in the next iteration of the\nmaximization. In practice, we initialize ut with the greedy\nactions of the decentralized IQL agents and then perform\nthis iterative local maximization (denoted lmax) for a small\nnumber of iterations. As in IQL, agents share parameters ψ.\nDuring exploration it is important that the sampling policy\nchanges in response to newly discovered states. This change\nis imposed by intrinsic reward, which must therefore be\ntransported quickly to earlier states in the episode. We use\na Q(λ) implementation (Watkins, 1989) to accelerate this\ntransport, but do not cut the traces after exploratory steps.\nThis improves transport, but also introduces non-stationary\ntargets. Our training procedure performs a parameter update\nafter each episode and we compute the targets backwards:\nGλ\nt := rt+(1−λ)γ lmax\n¯u\nqa\nψ′(¯ua|st+1, ¯u−a, ua\nt )+λγGλ\nt+1 ,\nwhere ψ′ denotes the target network and Gλ\nT := 0. The loss\nmin\nψ\nIE\nhT −1\nP\nt=0\nnP\na=1\n\u0000Gλ\nt −qa\nψ(ua\nt |st, u−a\nt\n, ua\nt−1)\n\u00012i\n,\n(4)\nis minimized by gradient descent, where the expectation is\napproximated with the same mini-batches as in IQL and the\nsame stabilization techniques are used as well.\n3.2. Intrinsic Reward Revisited\nIntrinsic rewards, as deﬁned in Section 2.2, induce three\nchallenges for collaborative MARL: dependence on joint\nactions, collaborative rewards, and evolving parameters.\nFirst, estimating (2) is infeasible for MARL, as we would\nhave to estimate one correlation matrix for each joint action\nu. Instead of estimating a measure that depends on counting\nNt(st, ut), we propose here to estimate one based on the\ncount Nt(st+1). Although only a heuristic, this approach\nworks in arbitrary large action spaces.\nSecond, in collaborative tasks all agents should receive the\nsame reward to avoid diverging incentives. However, in\nMARL each agent a estimates a different uncertainty, based\non a’s observations and/or other inputs. As the interaction\nof all agents could have contributed to each agent’s uncer-\ntainty, it is unclear how to reconcile diverging estimates. We\npropose to use the largest uncertainty as intrinsic reward for\nall agents, to consider this potential interaction.\nThird, the agents’ value function parameters continually\nchange, particularly in the beginning of training when explo-\nration is most important. The same inputs x yield different\nrepresentations φa(x) at different times t, and estimates\nwith (2) therefore become outdated after a while. To re-\nﬂect this change, we propose to use an exponentially decay-\ning average of the inverted matrix Ct := (1 −α) Ct−1 +\nPn\na=1 φa\n(xt) φa⊤\n(xt), where xt denotes the value function’s\ninputs at time t and α is a small decay constant. As the\nresulting uncertainty never decays to 0, we also introduce\na bias bt and discard negative intrinsic rewards. The bias\ncan be constant or an average over past uncertainties to only\nreward states with above average novelty.\nThe resulting collaborative intrinsic reward r+\nt is:\nr+\nt := σ max\nn\n0, max\na\n\u0010q\nφa⊤\n(xt+1)C−1\nt φa\n(xt+1)−bt\n\u0011o\n. (5)\n3.3. Independent Centrally-assisted Q-learning (ICQL)\nThe intrinsically rewarded central agent samples in our train-\ning framework episodes, while the decentralized (here IQL)\nagents are simultaneously trained on the shared replay buffer.\nThis improves exploration in two ways: (i) although the de-\ncentralized agents still beneﬁt from the exploration that is\ninduced by (potentially unreliable) intrinsic reward, their\nﬁnal policies are exclusively based on environmental re-\nwards, and (ii) the central agent conditions on the true state\nExploration with Unreliable Intrinsic Reward in MARL\n0\n1M\n2M\n3M\n4M\nEnvironmental Steps\n0\n2\n4\n6\n8\n10\n5M\nepisode reward during training (SEM)\n0\nEnvironmental Steps\n0\n2\n4\n6\n8\n10\n1M\n2M\n3M\n4M\n5M\nICQL (       )\nIQL (       )\nIQL (       )\nmountain prey\nreward for\nvalley prey\nreward for\nepisode reward during testing (SEM)\nFigure 1. 4 agents hunt a mountain and a valley prey. We plot mean and standard error (over 8 seeds) of the training (left) and test\nperformance (right) of IQL, with and without intrinsic reward (i.e. magnitude σ), and of our centrally-assisted exploration framework\nICQL. Note that for ICQL training performance is 50% centralized, but test performance is 100% decentralized.\nof the system, which includes information that may not be\nobservable by the agents.\nHowever, sampling only with the central agent yields an\nout-of-distribution problem for the IQL agents: a single\ndeviating decision can induce a trajectory that has never\nbeen seen during training. We therefore share the sampling\nprocess by deciding randomly at the start of an episode\nwhether the IQL agents or the central agent takes control.\nThe resulting behavior appears quite stable for different\nprobabilities of that choice and we chose 50% for simplicity.\n4. Experiments\nWe extend a common partially observable predator-prey task\nin a grid-world to make it challenging from an exploration\nperspective, as preliminary experiments have shown that the\noriginal task does not require directed exploration of the\nstate space.2 We train 4 agents, with 5 × 5 agent-centric\nobservations, to hunt a mountain and a valley prey. Prey\nmoves randomly in a bounded grid-world of height 41 and\nwidth 10. To simulate a mountain, both agents and valley\nprey do not execute 50% of all ‘up’ actions, and mountain\nprey does not execute 50% of all ‘down’ actions. Valley\nprey spawn randomly on the lowest row, mountain prey on\nthe highest row, and agents on middle row. An episode\nends either when one of the prey is caught, that is, when\nagents/boundaries surround it on all sides, or after 100 steps.\nOnly capturing yields reward, 5 for the valley and 10 for the\nmountain prey. Exploring the state space helps to ﬁnd the\nmountain prey without getting distracted by the valley prey.\n2In the original task, the prey moves randomly and the states in\nwhich the agents meet it are almost uniformly distributed. This pro-\nvides sufﬁcient exploration to ﬁnd an optimal policy and directed\nexploration is unnecessary.\nFigure 1 shows training and test performance3 for three\nalgorithms: the original IQL (red, Section 2.1), IQL with\nintrinsic reward based on the agents’ last layers (green, Sec-\ntion 3.2) and our novel centrally-assisted exploration frame-\nwork ICQL (blue, Section 3.3), where the intrinsic reward is\nbased on the last layer of the central agents’ value functions.\nThe destabilizing effect of unreliable intrinsic reward on\nIQL can be seen in the green IQL (σ=1) curve: it speeds\nup learning, but also prevents the agents from ﬁnding the\noptimal policy (visible both in training and test plots). The\nbonus provides incentives for exploration, but also appears\nto distract the agents when their policy should converge.\nOur ICQL framework (blue) learns even faster, but demon-\nstrates different behavior during training and testing. On the\none hand, one can see the same sub-optimal behavior dur-\ning training (left plot), which executes 50% of the episodes\nwith the intrinsically-rewarded central agent and 50% with\ndecentralized agents trained simultaneously. On the other\nhand, the test performance (right plot) of the decentralized\nagents shows the same improved learning, but none of the\ninstabilities once the mountain prey has been found.\nWe conclude that intrinsic reward is both a blessing and\na curse for MARL settings. We have shown that even un-\nreliable reward can improve directed exploration, but also\nintroduces detracting incentives. Our novel ICQL frame-\nwork for centrally-assisted exploration appears to stabilize\nlearning and further speeds up training, most likely by ex-\nploiting access to the true state.\nIn future work, we will further evaluate how the framework\nperforms with different decentralized learning algorithms,\nlike VDN and QMIX, and employ other uncertainty esti-\n3We implemented all algorithms in the PyMarl framework\n(Samvelyan et al., 2019). Details can be found in Appendix A.\nExploration with Unreliable Intrinsic Reward in MARL\nmates for intrinsic rewards. We also want to investigate the\neffect of adaptive biases and apply our method to StarCraft\nmicromanagement tasks (SMAC, Samvelyan et al., 2019).\nAcknowledgements\nThe authors would like to thank Jakob F¨orster, Gregory\nFarquhar and Christian Schroeder de Witt for fruitful discus-\nsions about decentralization and exploration in MARL. This\nproject has received funding from the European Research\nCouncil (ERC), under the European Union’s Horizon 2020\nresearch and innovation programme (grant agreement num-\nber 637713), and a grant of the EPSRC (EP/M508111/1,\nEP/N509711/1). The experiments were made possible by a\ngenerous equipment grant from NVIDIA.\nReferences\nBellemare, M. G., Srinivasan, S., Ostrovski, G., Schaul, T.,\nSaxton, D., and Munos, R. Unifying count-based explo-\nration and intrinsic motivation. In Advances in Neural\nInformation Processing Systems (NIPS) 29, pp. 1471–\n1479, 2016.\nBurda, Y., Edwards, H., Pathak, D., Storkey, A., Darrell, T.,\nand Efros, A. A. Large-scale study of curiosity-driven\nlearning. In International Conference on Learning Rep-\nresentations (ICLR), 2019.\nChung, J., Gulcehre, C., Cho, K., and Bengio, Y. Empirical\nevaluation of gated recurrent neural networks on sequence\nmodeling. In NIPS Workshop on Deep Learning, 2014.\nURL http://arxiv.org/abs/1412.3555.\nFoerster, J., Assael, I. A., de Freitas, N., and Whiteson,\nS. Learning to communicate with deep multi-agent rein-\nforcement learning. In Advances in Neural Information\nProcessing Systems (NIPS) 29, pp. 2137–2145. 2016.\nFoerster, J. N., Farquhar, G., Afouras, T., Nardelli, N., and\nWhiteson, S. Counterfactual multi-agent policy gradients.\nIn Proceedings of the 15th AAAI Conference on Artiﬁcial\nIntelligence, pp. 2974–2982, 2018.\nFortunato, M., Azar, M. G., Piot, B., Menick, J., Hessel, M.,\nOsband, I., Graves, A., Mnih, V., Munos, R., Hassabis, D.,\nPietquin, O., Blundell, C., and Legg, S. Noisy networks\nfor exploration. In International Conference on Learning\nRepresentations (ICLR), 2018.\nGal, Y., Hron, J., and Kendall, A. Concrete dropout. In Ad-\nvances in Neural Information Processing Systems (NIPS),\npp. 3584–3593, 2017.\nHasselt, H. v., Guez, A., and Silver, D. Deep reinforcement\nlearning with double q-learning. In Proceedings of the\n13th AAAI Conference on Artiﬁcial Intelligence, pp. 2094–\n2100, 2016.\nHausknecht, M. J. and Stone, P.\nDeep recurrent q-\nlearning for partially observable mdps.\nIn 2015\nAAAI Fall Symposia,\npp. 29–37,\n2015.\nURL\nhttp://www.aaai.org/ocs/index.php/\nFSS/FSS15/paper/view/11673.\nJaques, N., Lazaridou, A., Hughes, E., G¨ulc¸ehre, C¸ ., Ortega,\nP. A., Strouse, D., Leibo, J. Z., and de Freitas, N. Intrinsic\nsocial motivation via causal inﬂuence in multi-agent RL.\nCoRR, abs/1810.08647, 2018. URL https://arxiv.\norg/abs/1810.08647.\nJin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M. I. Is\nQ-learning provably efﬁcient? In Advances in Neural\nInformation Processing Systems (NeurIPS) 31, pp. 4863–\n4873. 2018.\nLeibo, J. Z., Hughes, E., Lanctot, M., and Graepel, T.\nAutocurricula and the emergence of innovation from\nsocial interaction: A manifesto for multi-agent intelli-\ngence research. CoRR, abs/1903.00742, 2019. URL\nhttp://arxiv.org/abs/1903.00742.\nLin, L.-J. Self-improving reactive agents based on reinforce-\nment learning, planning and teaching. Machine Learning,\n8(3):293–321, 1992.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Ve-\nness, J., Bellemare, M. G., Graves, A., Riedmiller, M.,\nFidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C.,\nSadik, A., Antonoglou, I., King, H., Kumaran, D., Wier-\nstra, D., Legg, S., and Hassabis, D. Human-level control\nthrough deep reinforcement learning. Nature, 518(7540):\n529–533, February 2015.\nO’Donoghue, B., Osband, I., Munos, R., and Mnih, V. The\nuncertainty Bellman equation and exploration. In Pro-\nceedings of the 35th International Conference on Ma-\nchine Learning (ICML), pp. 3836–3845, 2018.\nOliehoek, F. A. and Amato, C.\nA concise introduction\nto decentralized POMDPs. Springer Publishing Com-\npany, Incorporated, 1st edition, 2016. ISBN 3319289276,\n9783319289274.\nOsband, I., Van Roy, B., and Wen, Z. Generalization and\nexploration via randomized value functions.\nIn Pro-\nceedings of the 33rd International Conference on Inter-\nnational Conference on Machine Learning (ICML), pp.\n2377–2386, 2016.\nOsband, I., Aslanides, J., and Cassirer, A. Randomized prior\nfunctions for deep reinforcement learning. In Advances\nin Neural Information Processing Systems (NeurIPS) 31,\npp. 8617–8629. 2018.\nExploration with Unreliable Intrinsic Reward in MARL\nOstrovski, G., Bellemare, M. G., van den Oord, A., and\nMunos, R. Count-based exploration with neural density\nmodels. In Proceedings of the 34th International Con-\nference on Machine Learning (ICML), pp. 2721–2730,\n2017.\nPathak, D., Agrawal, P., Efros, A. A., and Darrell, T.\nCuriosity-driven exploration by self-supervised predic-\ntion. In Proceedings of the 34th International Conference\non Machine Learning (ICML), 2017.\nPlappert, M., Houthooft, R., Dhariwal, P., Sidor, S., Chen,\nR. Y., Chen, X., Asfour, T., Abbeel, P., and Andrychow-\nicz, M. Parameter space noise for exploration. In Interna-\ntional Conference on Learning Representations (ICLR),\n2018.\nRashid, T., Samvelyan, M., de Witt, C. S., Farquhar, G.,\nFoerster, J. N., and Whiteson, S. QMIX: monotonic value\nfunction factorisation for deep multi-agent reinforcement\nlearning. In International Conference on Machine Learn-\ning (ICML), pp. 4292–4301, 2018.\nRoderick, M., Grimm, C., and Tellex, S. Deep abstract\nQ-networks. In Proceedings of the 17th International\nConference on Autonomous Agents and MultiAgent Sys-\ntems (AAMAS), pp. 131–138, 2018.\nSamvelyan, M., Rashid, T., de Witt, C. S., Farquhar, G.,\nNardelli, N., Rudner, T. G. J., Hung, C., Torr, P. H. S.,\nFoerster, J. N., and Whiteson, S. The StarCraft multi-\nagent challenge. CoRR, abs/1902.04043, 2019. URL\nhttps://arxiv.org/abs/1902.04043.\nSunehag, P., Lever, G., Gruslys, A., Czarnecki, W. M., Zam-\nbaldi, V., Jaderberg, M., Lanctot, M., Sonnerat, N., Leibo,\nJ. Z., Tuyls, K., and Graepel, T. Value-decomposition net-\nworks for cooperative multi-agent learning based on team\nreward. In Proceedings of the 17th International Con-\nference on Autonomous Agents and MultiAgent Systems\n(AAMAS), pp. 2085–2087, 2018.\nTan, M. Multi-agent reinforcement learning: Independent\nvs. cooperative agents. In In Proceedings of the Tenth\nInternational Conference on Machine Learning (ICML),\npp. 330–337, 1993.\nTang, H., Houthooft, R., Foote, D., Stooke, A., Xi Chen,\nO., Duan, Y., Schulman, J., DeTurck, F., and Abbeel,\nP. #Exploration: A study of count-based exploration\nfor deep reinforcement learning. In Advances in Neural\nInformation Processing Systems (NIPS) 30, pp. 2753–\n2762. 2017.\nvan den Oord, A., Kalchbrenner, N., Espeholt, L.,\nkavukcuoglu, k., Vinyals, O., and Graves, A. Conditional\nimage generation with PixelCNN decoders. In Advances\nin Neural Information Processing Systems (NIPS) 29, pp.\n4790–4798. 2016.\nVinyals, O., Babuschkin, I., Chung, J., Mathieu, M.,\nJaderberg, M., Czarnecki, W., Dudzik, A., Huang, A.,\nGeorgiev, P., ichard Powell, Ewalds, T., Horgan, D.,\nKroiss, M., Danihelka, I., Agapiou, J., Oh, J., Dalibard,\nV., Choi, D., Sifre, L., Sulsky, Y., Vezhnevets, S., Molloy,\nJ., Cai, T., Budden, D., Paine, T., Gulcehre, C., Wang,\nZ., Pfaff, T., Pohlen, T., Wu, Y., Yogatama, D., Cohen,\nJ., McKinney, K., Smith, O., Schaul, T., Lillicrap, T.,\nApps, C., Kavukcuoglu, K., Hassabis, D., and Silver,\nD. AlphaStar: Mastering the real-time strategy game\nStarCraft II.\nDeepmind blog, accessed 04/16/2019,\nhttps://deepmind.com/blog/alphastar-ma\nstering-real-time-strategy-game-starcr\naft-ii, 2019.\nWatkins, C. and Dayan, P. Q-learning. Machine Learning,\n8:279–292, 1992.\nWatkins, C. J. C. H. Learning from delayed rewards. PhD\nthesis, Cambridge University, 1989.\nZheng, S. and Yue, Y. Structured exploration via hierar-\nchical variational policy networks, 2018. URL https:\n//openreview.net/forum?id=HyunpgbR-.\nExploration with Unreliable Intrinsic Reward in MARL\nAppendix\nA. Training details\nWe implemented all algorithms in the PyMARL framework\n(Samvelyan et al., 2019), where we used RMSprop with\nlearning rate 0.0005, γ = 0.99, batch size 32 and a replay\nbuffer holding the last 200 episodes. Decentralized agents\nhad a hidden layer of 64 GRU cells, sandwiched between 2\nfeed-forward layers, and central agents had 3 feed-forward\nlayers with 128 hidden neurons each. The target network\nwas updated every 200 episodes and we used ϵ-greedy ex-\nploration, which decayed 1 ≥ϵ ≥0.05 within 20,000 steps.\nIntrinsic reward had magnitude σ = 1, a decay constant\nα = 0.0002 and constant bias bt := 0.01. ICQL approxi-\nmated the local maximum with one lmax iteration.\nB. Related Work\nIn this paper we focus on intrinsic reward for exploration\n(in difference to pure curiosity, Burda et al., 2019). Here the\nuncertainty is often derived from to the prediction quality\nafter training on past trajectories. For example, pseudo-\ncounts are based on the reconstruction probability of visual\nobservations (Bellemare et al., 2016; Ostrovski et al., 2017),\nusing a PixelCNN (van den Oord et al., 2016). Alternatively,\nTang et al. (2017) count visitations using a hash function\non a random linear projection. Furthermore, Pathak et al.\n(2017) use the predictability of the observed transition as\nintrinsic reward signal, and Roderick et al. (2018) reduce\nuncertainty with prior knowledge over state abstractions.\nIn the context of Bayesian posteriors, the uncertainty has\nbeen estimated from an ensemble of value functions, with\noptional bootstrapping techniques (Osband et al., 2016;\n2018). Alternatively, Noisy Nets (Fortunato et al., 2018;\nPlappert et al., 2018) sample a value function for each\nepisode from a diagonal Gaussian posterior over the pa-\nrameters of the neural network. Similarly, Gal et al. (2017)\nsuggested to use Concrete Dropout to estimate the posterior\nfor model-based RL. To include the uncertainty of future\nreward, O’Donoghue et al. (2018) proposed the Uncertainty\nBellman Equation (UBE), which propagates the ‘local un-\ncertainty’ of future decisions with a Bellman operator.\nFor MARL, Zheng & Yue (2018) proposed to coordinate ex-\nploration by sharing latent variables, drawn from a learned\ndistribution. Jaques et al. (2018) focuses on social motiva-\ntions of competitive agents and Leibo et al. (2019) describes\nexploration as an auto-curriculum generated by competing\nspecies of agents.\n",
  "categories": [
    "cs.AI"
  ],
  "published": "2019-06-05",
  "updated": "2019-06-05"
}