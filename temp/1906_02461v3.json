{
  "id": "http://arxiv.org/abs/1906.02461v3",
  "title": "Unsupervised Pivot Translation for Distant Languages",
  "authors": [
    "Yichong Leng",
    "Xu Tan",
    "Tao Qin",
    "Xiang-Yang Li",
    "Tie-Yan Liu"
  ],
  "abstract": "Unsupervised neural machine translation (NMT) has attracted a lot of\nattention recently. While state-of-the-art methods for unsupervised translation\nusually perform well between similar languages (e.g., English-German\ntranslation), they perform poorly between distant languages, because\nunsupervised alignment does not work well for distant languages. In this work,\nwe introduce unsupervised pivot translation for distant languages, which\ntranslates a language to a distant language through multiple hops, and the\nunsupervised translation on each hop is relatively easier than the original\ndirect translation. We propose a learning to route (LTR) method to choose the\ntranslation path between the source and target languages. LTR is trained on\nlanguage pairs whose best translation path is available and is applied on the\nunseen language pairs for path selection. Experiments on 20 languages and 294\ndistant language pairs demonstrate the advantages of the unsupervised pivot\ntranslation for distant languages, as well as the effectiveness of the proposed\nLTR for path selection. Specifically, in the best case, LTR achieves an\nimprovement of 5.58 BLEU points over the conventional direct unsupervised\nmethod.",
  "text": "Unsupervised Pivot Translation for Distant Languages\nYichong Leng∗\nUniversity of Science and Technology of China\nlyc123go@mail.ustc.edu.cn\nXu Tan\nMicrosoft Research\nxuta@microsoft.com\nTao Qin†\nMicrosoft Research\ntaoqin@microsoft.com\nXiang-Yang Li†\nUniversity of Science and Technology of China\nxiangyangli@ustc.edu.cn\nTie-Yan Liu\nMicrosoft Research\ntyliu@microsoft.com\nAbstract\nUnsupervised\nneural\nmachine\ntranslation\n(NMT) has attracted a lot of attention re-\ncently.\nWhile state-of-the-art methods for\nunsupervised translation usually perform well\nbetween similar languages (e.g.,\nEnglish-\nGerman translation),\nthey perform poorly\nbetween distant languages, because unsu-\npervised alignment does not work well for\ndistant languages. In this work, we introduce\nunsupervised pivot translation for distant\nlanguages, which translates a language to a\ndistant language through multiple hops, and\nthe unsupervised translation on each hop\nis relatively easier than the original direct\ntranslation.\nWe propose a learning to route\n(LTR) method to choose the translation path\nbetween the source and target languages.\nLTR is trained on language pairs whose best\ntranslation path is available and is applied to\nthe unseen language pairs for path selection.\nExperiments on 20 languages and 294 distant\nlanguage pairs demonstrate the advantages of\nthe unsupervised pivot translation for distant\nlanguages, as well as the effectiveness of the\nproposed LTR for path selection. Speciﬁcally,\nin the best case, LTR achieves an improvement\nof 5.58 BLEU points over the conventional\ndirect unsupervised method.\n1\nIntroduction\nUnsupervised\nneural\nmachine\ntranslation\n(NMT) (Artetxe et al., 2017b; Lample et al.,\n2017,\n2018),\nwhich\nuses\nonly\nmonolingual\nsentences for translation, is of great importance\n∗The work was done when the ﬁrst author was an intern\nat Microsoft Research Asia.\n† Corresponding author\nfor zero-resource language pairs.\nUnsupervised\ntranslation relies on unsupervised cross-lingual\nword alignment or sentence alignment (Conneau\net al., 2017; Artetxe et al., 2017a), where word\nembedding mapping (Artetxe et al., 2017b; Lam-\nple et al., 2017) and vocabulary sharing (Lample\net al., 2018) are used for word alignment, and\nencoder/decoder weight sharing (Artetxe et al.,\n2017b; Lample et al., 2018) and adversarial train-\ning (Lample et al., 2017) are used for sentence\nalignment.\nUnsupervised cross-lingual alignment works\nreasonably well for a pair of similar languages,\nsuch as English-German or Portuguese-Galician,\nsince they have similar lexica and syntax and\nshare the same alphabets and language branch.\nHowever, the alignment between a pair of dis-\ntant languages, which are not in the same lan-\nguage branch1, such as Danish-Galician is chal-\nlenging. As a consequence, unsupervised transla-\ntion between distant languages is usually of lower\nquality.\nFor example, the unsupervised NMT\nmodel achieves 23.43 BLEU score on Portuguese-\nGalician translation, while just 6.56 on Danish-\nGalician translation according to our experiments.\nIn this work, we focus on unsupervised translation\nof distant languages.\nWe observe that two distant languages can be\nlinked through multiple intermediate hops where\nunsupervised translation of two languages on each\n1In this work, we use language branch to determine\ndistant languages.\nWe choose the taxonomy of lan-\nguage family provided by Ethnologue (Paul et al., 2009)\n(https://www.ethnologue.com/browse/families), which is one\nof the most authoritative and commonly accepted tax-\nonomies. Distant languages can also be deﬁned using other\nprinciples, which we leave to future work.\narXiv:1906.02461v3  [cs.CL]  25 Jun 2019\nhop is easier than direct translation of the two\ndistance languages, considering that the two lan-\nguages on each intermediate hop are more sim-\nilar, or the size of monolingual training data\nis larger.\nTherefore, we propose unsupervised\npivot translation through multiple hops for dis-\ntant languages, where each hop consists of un-\nsupervised translation of a relatively easier lan-\nguage pair.\nFor example, the distant language\npair Danish→Galician can be translated by three\neasier hops: Danish→English, English→Spanish\nand Spanish→Galician. In this way, unsupervised\npivot translation results in better accuracy (12.14\nBLEU score) than direct unsupervised translation\n(6.56 BLEU score) from Danish to Galician in our\nexperiments.\nThe challenge of unsupervised pivot translation\nis how to choose a good translation path. Given\na distant language pair X→Y, there exists a large\namount of paths that can translate from X to Y2,\nand different paths may yield very different trans-\nlation accuracies. Therefore, unsupervised pivot\ntranslation may result in lower accuracy than di-\nrect unsupervised translation if a poor path is cho-\nsen.\nHow to choose a path with good transla-\ntion accuracy is important to guarantee the perfor-\nmance of unsupervised pivot translation.\nA straightforward method is to calculate the\ntranslation accuracies of all possible paths on a\nvalidation set and choose the path with the best\naccuracy.\nHowever, it is computationally unaf-\nfordable due to the large amount of possible paths.\nFor example, suppose we consider at most 3 hops\n(N = 3) and 100 languages (M = 100), and as-\nsume each path takes an average of 20 minutes\nto translate all the sentences in the validation set\nusing one NVIDIA P100 GPU to get the BLEU\nscore. Then it will take nearly 1400000 GPU days\nto evaluate all candidate paths. Even if we just\nconsider 20 languages (M = 20), it will still take\n2200 GPU days. Therefore, an efﬁcient method\nfor path selection is needed. We propose a learning\nto route (LTR) method that adopts a path accuracy\npredictor (a multi-layer LSTM) to select a good\npath for a distant language pair. Given a transla-\ntion path and the translation accuracy of each hop\non the path, the path accuracy predictor can pre-\ndict the overall translation accuracy following this\n2Suppose we only consider translation paths with at most\nN hops. Given M candidate intermediate languages, there\nare\nM!\n(M−N+1)! possible paths.\npath. Such a predictor is ﬁrst trained on a train-\ning set of paths with known overall accuracy, and\nthen used to predict the accuracy of a path unseen\nbefore.\nWe conduct experiments on a large dataset with\n20 languages and a total of 294 distant language\npairs to verify the effectiveness of our method.\nOur proposed LTR achieves an improvement of\nmore than 5 BLEU points on some language pairs.\nThe contributions of this paper are as follows:\n(1) We introduce pivot translation into unsuper-\nvised NMT to improve the accuracy of distant\nlanguages. (2) We propose the learning to route\n(LTR) method to automatically select the good\ntranslation path. (3) Large scale experiments on\nmore than 20 languages and 294 distant language\npairs demonstrate the effectiveness of our method.\n2\nRelated Work\nIn this section, we review the related work from\nthree aspects: unsupervised neural machine trans-\nlation, pivot translation, and path routing.\nUnsupervised NMT\nAs the foundation of unsu-\npervised sentence translation, unsupervised word\nalignment has been investigated by (Conneau\net al., 2017; Artetxe et al., 2017a), where lin-\near embedding mapping and adversarial training\nare used to ensure the distribution-level match-\ning, achieving considerable good accuracy or even\nsurpasses the supervised counterpart for similar\nlanguages. Artetxe et al. (2017b); Lample et al.\n(2017) propose unsupervised NMT that leverages\nword translation for the initialization of the bilin-\ngual word embeddings.\nYang et al. (2018) par-\ntially share the parameter of the encoder and de-\ncoder to enhance the semantic alignment between\nsource and target language. Lample et al. (2018)\nfurther share the vocabulary of source and target\nlanguages and jointly learned the word embed-\ndings to improve the quality of word alignment,\nand achieve large improvements on similar lan-\nguage pairs.\nRecently, inspired by the success\nof BERT (Devlin et al., 2018) and MASS (Song\net al., 2019),\nSong et al. (2019) leverage the\nMASS pre-training in the unsupervised NMT\nmodel and achieve state-of-the-art performance on\nsome popular language pairs.\nPrevious works on unsupervised NMT can in-\ndeed achieve good accuracy on similar language\npairs, especially on the closely related languages\nsuch as English and German that are in the same\nlanguage branch. In this circumstance, they can\nsimply share the vocabulary and learn joint BPE\nfor source and target languages, and share the en-\ncoder and decoder, which is extremely helpful for\nword embedding and latent representation align-\nment. However, they usually achieve poor accu-\nracy on distant languages that are not in the same\nlanguage branch or do not share same alphabets.\nIn this paper, we propose pivot translation for dis-\ntant languages, and leverage the basic unsuper-\nvised NMT model in (Lample et al., 2018) on sim-\nilar languages as the building blocks for the unsu-\npervised pivot translation.\nPivot Translation\nPivot translation has long\nbeen studied in statistical machine translation to\nimprove the accuracy of low/zero-resource trans-\nlation (Wu and Wang, 2007; Utiyama and Isahara,\n2007). Cheng et al. (2017); Chen et al. (2017) also\nadapt the pivot based method into neural machine\ntranslation.\nHowever, conventional pivot trans-\nlation usually leverages a resource-rich language\n(mainly English) as the pivot to help the low/zero-\nresource translation, while our method only relies\non the unsupervised method in each hop of the\ntranslation path. Due to the large amount of possi-\nble path choices, the accuracy drops quickly along\nthe multiple translation hops in the unsupervised\nsetting, unsupervised pivot translation may result\nin lower accuracy if the path is not carefully cho-\nsen. In this situation, path selection (path rout-\ning) will be critical to guarantee the performance\nof pivot translation.\nPath Routing\nRouting is the process of se-\nlecting a path for trafﬁc in a network, or be-\ntween or across multiple networks.\nGenerally\nspeaking, routing can be performed in many\ntypes of networks, including circuit switching net-\nwork (Girard, 1990), computer networks (e.g.,\nInternet) (Huitema, 2000), transportation net-\nworks (Raff, 1983) and social networks (Liben-\nNowell et al., 2005). In this paper, we consider\nthe routing of the translation among multiple lan-\nguages, where the translation accuracy is the crite-\nrion for the path selection. Usually, the translation\naccuracy of the multi-hop path is not simply the\nlinear combination of the accuracy on each one-\nhop path, which makes it difﬁcult to route for a\ngood path.\n3\nUnsupervised Pivot Translation\nObserving that unsupervised translation is usu-\nally hard for distant languages, we split the direct\ntranslation into multiple hops, where the unsuper-\nvised translations on each hop is relatively easier\nthan the original direct unsupervised translation.\nFormally, for the translation from language X to\nY , we denote the pivot translation as\nX →Z1 →... →Zn →Y,\n(1)\nwhere Z1, ..., Zn are the pivot languages and n is\nthe number of pivot languages in the path. We set\nn ∈{0, 1, 2} and consider 3-hop path at most in\nthis paper, considering the computation cost and\naccuracy drop in longer translation path. Note that\nwhen n = 0, it is the one-hop (direct) translation.\nThere exists a large amount of translation paths\nbetween X and Y and each path can result in dif-\nferent translation accuracies, or even lower than\nthe direct unsupervised translation, due to the in-\nformation loss along the multiple translation hops\nespecially when unsupervised translation quality\non one hop is low. Therefore, how to choose a\ngood translation path is critical to ensure the ac-\ncuracy of unsupervised pivot translation. In this\nsection, we introduce the learning to route (LTR)\nmethod for the translation path selection.\n3.1\nLearning to Route\nIn this section, we ﬁrst give the description of the\nproblem formulation, and then introduce the train-\ning data, features and model used for LTR.\nProblem Formulation\nWe formulate the path\nselection as a translation accuracy prediction prob-\nlem. The LTR model learns to predict the trans-\nlation accuracy of each path from language X to\nY given the translation accuracy of each hops in\nthe path, and the path with the highest predicted\ntranslation accuracy among all the possible paths\nis chosen as the output.\nTraining Data\nWe construct the training data\nfor the LTR model in the following steps: (1)\nFrom M languages, we choose the distant lan-\nguage pairs whose source and target languages\nare not in the same language branch.\nWe then\nchoose a small part of the distant language pairs\nas the development/test set respectively for LTR,\nand regard the remaining part as the training set\nfor LTR. (2) In order to get the accuracy of differ-\nent translation paths for the distant language pairs,\nas well as to obtain the input features for LTR,\nwe train the unsupervised model for the transla-\ntion between any languages and obtain the BLEU\nscore of each pair. For M languages, there are to-\ntal M(M −1) language pairs and BLEU scores,\nwhich requires M(M −1)/2 unsupervised mod-\nels since one model can handle both translation di-\nrections following the common practice in unsu-\npervised NMT (Lample et al., 2018). (3) We then\ntest the BLEU scores of the each possible transla-\ntion path for the language pairs in the development\nand test sets, based on the models trained in previ-\nous steps. These BLEU scores are regarded as the\nground-truth data to evaluate the performance of\nunsupervised pivot translation. (4) We just get the\nBLEU scores of a small part of the possible paths\nin the training set, which are used as the training\ndata for LTR model3. We describe the features of\nthe training data in the next paragraph.\nFeatures\nWe extract several features from the\npaths in training data for better model training.\nWithout loss of generality, we take a 3-hop path\nX →Z1 →Z2 →Y as an example, and re-\ngard it as a token sequence consisting of languages\nand one-hops: X, X →Z1, Z1, Z1 →Z2, Z2,\nZ2 →Y and Y . We consider two kinds of fea-\ntures for the token sequence: (1) The token ID.\nThere are a total of 7 tokens in the path shown\nabove. Each token ID is converted into trainable\nembeddings. For a one-hop token like Z1 →Z2,\nits embedding is simply the average of the two\nembeddings of Z1 and Z2. (2) The BLEU score\nof each language and one-hop, where we get the\nBLEU score of each language by averaging the\naccuracy of the one-hop path from or to this lan-\nguage. For example, the BLEU score of the target\nlanguage Z1 in X →Z1 is calculated by averag-\ning all the BLEU scores of the one-hop translation\nfrom other languages to Z1, while the BLEU score\nof the source language Z1 in Z1 →Z2 is cal-\nculated by averaging the BLEU scores of all the\none-hop translation from Z1 to other languages.\nWe concatenate the above two features together in\none vector for each language and one-hop token,\nand get a sequence of features for each path. The\nBLEU score of the path will be used as the label\nfor the LTR model.\n3As described in Footnote 2, we cannot afford to test the\nBLEU scores of all the possible paths, so we just test a small\npart of them for training.\nModel\nWe use a multi-layer LSTM model to\npredict the BLEU score of the translation path.\nThe input of the LSTM model is the feature se-\nquence described in the above paragraph.\nThe\nlast hidden of LSTM is multiplied with an one-\ndimensional vector to predict the BLEU score of\nthe path.\n3.2\nDiscussions\nWe make brief discussions on some possible base-\nline routing methods and compare them with our\nproposed LTR.\nRandom Routing: We randomly choose a path\nas the routing result.\nPrior Pivoting: We set the pivot language for\neach language according to prior knowledge4. De-\nnote PX and PY as the pivot language for X and Y\nrespectively. The path X →PX →PY →Y will\nbe chosen as the routing result by prior pivoting.\nHop Average: The average of the BLEU scores\nof each one-hop in the path is taken as the pre-\ndicted BLEU score for this path. We select the\npath with the highest predicted BLEU score, as\nused in the LTR method.\nCompared with these simple rule based routing\nmethods described above, LTR chooses the path\npurely by learning on a part of the ground-truth\npaths. The feature we designed in LTR can capture\nthe relationship between languages to determine\nthe BLEU score and relative ranking of the paths.\nThis data-driven learning based method (LTR) will\nbe more accurate than the rule based methods. In\nthe next section, we conduct experiments to verify\neffectiveness of our proposed LTR and compare\nwith the baseline methods.\n4\nExperiments Design\nOur experiments consist of two stages in general.\nIn the ﬁrst stage, we need to train the unsupervised\nNMT model between any two languages to get\nthe BLEU scores of each one-hop path. We also\nget the BLEU scores for a part of multi-hop paths\nthrough pivoting, which are used as the training\nand evaluation data for the second stage. In the\nsecond stage, we train the LTR model based on the\ntraining data generated in the ﬁrst stage. In this\nsection, we give brief descriptions of the exper-\niment settings for the unsupervised NMT model\n4For the languages in each language branch, we choose\nthe language with the largest amount of monolingual data in\nthis branch as the pivot language. All languages in the same\nlanguage branch share the same pivot language.\ntraining (the ﬁrst stage) and the LTR model train-\ning and path routing (the second stage).\n4.1\nExperiment Setting for Direct\nUnsupervised NMT\nDatasets\nWe conduct the experiments on 20 lan-\nguages and a total of 20×19=380 language pairs,\nwhich have no bilingual sentence pairs but just\nmonolingual sentences for each language.\nThe\nlanguages involved in the experiments can be di-\nvided into 4 language branches by the taxonomy\nof language family:\nBalto-Slavic branch, Ger-\nmanic branch, Italic branch and Uralic branch5.\nThe language name and its ISO 639-1 code con-\ntained in each branch can be found in the supple-\nmentary materials (Section 1 and 2).\nWe\ncollect\nthe\nmonolingual\ncorpus\nfrom\nWikipedia for each language. We download the\nlanguage speciﬁc Wikipedia contents in XML for-\nmat6, and use WikiExtractor7 to extract and clean\nthe texts.\nWe then use the sentence tokenizer\nfrom the NLTK toolkit8 to generate segmented\nsentences from Wikipedia documents.\nTo ensure we have the development and test set\nfor the large amount of language pairs to evaluate\nthe unsupervised translation accuracy in our ex-\nperiments, we choose the languages that are cov-\nered by the common corpus of TED talks, which\ncontains translations between more than 50 lan-\nguages (Ye et al., 2018)9. In this circumstance,\nwe can leverage the development and test set from\nTED talks for evaluation.\nNote that in the un-\nsupervised setting, we just leverage monolingual\nsentences for unsupervised training and only use\nthe bilingual data for developing and testing. In\norder to alleviate the domain mismatch problem\nthat we train on monolingual data from Wikipedia\nbut test on the evaluation data from TED talks, we\nalso ﬁne-tune the unsupervised models with the\nsmall size of monolingual data from TED talks10.\nThe monolingual data from TED talks is merged\nwith the monolingual data from Wikipedia in the\n5The ﬁrst three branches belong to Indo-European family\nwhile the last branch is actually a language family. We do not\nfurther split the 3 languages in Uralic family into different\nbranches.\n6For example, we download English Wikipedia contents\nfrom https://dumps.wikimedia.org/enwiki.\n7https://github.com/attardi/wikiextractor\n8https://www.nltk.org/\n9https://github.com/neulab/word-embeddings-for-nmt\n10https://github.com/ajinkyakulkarni14/TED-\nMultilingual-Parallel-Corpus/tree/master/Monolingual data\nﬁne-tuning process, which results in better perfor-\nmance for the unsupervised translation. The size\nof Wikipidia and TED talks monolingual data can\nbe found in the supplementary materials (Section\n3).\nAll the sentences in the bilingual and mono-\nlingual data are ﬁrst tokenized with moses tok-\nenizer11 and then segmented into subword sym-\nbols using Byte Pair Encoding (BPE) (Sennrich\net al., 2016).\nWhen training the unsupervised\nmodel, we learn the BPE tokens with 60000 merge\noperations across the source and target languages\nfor each language pair and jointly training the\nembedding using fastext12, following the practice\nin Lample et al. (2018).\nModel\nConﬁgurations\nWe\nuse\ntransformer\nmodel as the basic NMT model structure, consid-\nering it achieves state-of-the-art accuracy and be-\ncomes a popular choice for recent NMT research.\nWe use 4-layer encoder and 4-layer decoder with\nmodel hidden size dmodel and feed-forward hid-\nden size dff being 512, 2048 following the default\nconﬁgurations in Lample et al. (2018). We use\nthe same model conﬁgurations for all the language\npairs.\nModel Training and Inference\nWe train the un-\nsupervised model with 1 NVIDIA Tesla V100\nGPU. One mini-batch contains roughly 4096\nsource tokens and 4096 target tokens, as used in\nLample et al. (2018). We follow the default param-\neters of Adam optimizer (Kingma and Ba, 2014)\nand learning rate schedule in Vaswani et al. (2017).\nDuring inference, we decode with greedy search\nfor all the languages. We evaluate the translation\nquality by tokenized case sensitive BLEU (Pap-\nineni et al., 2002) with multi-bleu.pl13.\n4.2\nExperiment Setting for Routing\nConﬁgurations for Routing\nWe choose the dis-\ntant language pairs from the 20 languages based\non the taxonomy of language family: if two lan-\nguages are not in the same language branch, then\nthey are regarded as distant languages.\nWe get\n294 distant language pairs. As described in Sec-\ntion 3.1, we choose nearly 5% and 10% of the dis-\ntant language pairs as the development and test set\n11https://github.com/moses-smt/mosesdecoder/blob/mast\ner/scripts/tokenizer/tokenizer.perl\n12https://github.com/facebookresearch/fastText\n13https://github.com/moses-smt/mosesdecoder/blob/\nmaster/scripts/generic/multi-bleu.perl\nSource\nTarget\nDT\nGT\nGT(∆)\nPivot-1\nPivot-2\nLTR\nLTR(∆)\nPivot-1\nPivot-2\nDa\nGl\n6.56\n12.14\n5.58\nEn\nEs\n12.14\n5.58\nEn\nEs\nBg\nSv\n4.72\n9.92\n5.20\nEn\nEn\n9.92\n5.20\nEn\nEn\nGl\nSv\n3.79\n8.62\n4.83\nEs\nEn\n8.62\n4.83\nEs\nEn\nSv\nGl\n3.70\n8.13\n4.43\nEn\nEs\n8.13\n4.43\nEn\nEs\nBe\nIt\n2.11\n6.40\n4.29\nUk\nEn\n5.24\n3.13\nEn\nEn\nPt\nBe\n4.76\n8.86\n4.10\nRu\nRu\n8.86\n4.10\nRu\nRu\nGl\nDa\n7.45\n11.33\n3.88\nEs\nEs\n11.33\n3.88\nEs\nEs\nBe\nPt\n6.39\n9.77\n3.38\nRu\nRu\n6.39\n0.00\n-\n-\nIt\nBe\n2.24\n5.19\n2.95\nPt\nRu\n4.64\n2.40\nRu\nRu\nNl\nUk\n4.69\n7.23\n2.54\nDe\nDe\n7.12\n2.53\nRu\nRu\nTable 1: The BLEU scores of a part of the distant language pairs in the test set (Please refer to Section 1 and 4 in\nthe supplementary materials for the corresponding full language name and full results). DT: direct unsupervised\ntranslation. GT: the ground-truth best path. LTR: the routing results of LTR. (∆) is the BLEU gap between GT or\nLTR and DT. Pivot-1 and Pivot-2 are two pivot languages in the path, which will be the same language if the path\nis 2-hop and will both be empty if the path is 1-hop (direct translation).\nLength\n1-hop\n2-hop\n3-hop\nRatio (%)\n7.1\n53.6\n39.3\nTable 2: The length distribution of the best translation\npaths. The ratio is calculated based on all language\npairs in the test set.\nfor routing. Note that if the language pair X →Y\nis in development (test) set, then the language pair\nY →X will be also in development (test) set.\nWe then enumerate all possible paths between any\ntwo language pairs in development and test set,\nand test the BLEU scores of the each possible\npath, which are regarded as the ground-truth data\nto evaluate the performance of the routing method.\nFor the remaining 85% distant language pairs, we\njust test the BLEU score for 10% of all possible\npaths, and use these BLEU scores as the label for\nLTR model training.\nWe use 2-layer LSTM as described in Sec-\ntion 3.1. The dimension of input feature vector\nis 6, which includes the embedding of the token\nID with size of 5, the BLEU score with size 1 (we\nnormalize the BLEU score into 0-1). We change\nthe depth and width of LSTM, but there is no sig-\nniﬁcant gain in performance.\nWe use the mean square error as the training\nloss for the LTR model, and use Adam as the op-\ntimizer. The initial learning rate is 0.01. When\napplying the LTR model on unseen pairs, we pre-\ndict the BLEU scores of all the possible paths (in-\ncluding 1-hop (direct translation), 2-hop and 3-\nhop translation path) between the source and target\nlanguages, and choose the path with the highest\npredicted BLEU score as the routing result. Note\nthat when predicting the path with LTR in infer-\nence time, we do not include the pivot language\nwhich occurs less than 10 times in training set,\nwhich can improve that stability of the LTR pre-\ndiction.\nMethods for Comparison\nWe conduct exper-\nimental comparisons on different methods de-\nscribed in Section 3 for path selection (routing),\nincluding Random Routing (RR), Prior Pivoting\n(PP), Hop Average (HA) and Learning to Route\n(LTR). We also compare these routing methods\nwith the direct unsupervised translation, denoted\nas Direct Translation (DT). We list the BLEU\nscore of the best multi-hop path (the ground truth)\nas a reference, which is denoted as Ground Truth\n(GT).\n5\nResults\nIn this section, we introduce the performance\nof unsupervised pivot translation for distant lan-\nguages. We ﬁrst demonstrate the advantages of\nunsupervised pivot translation by comparing the\nbest translation path (GT) with direction transla-\ntion (DT), and then show the results of our pro-\nposed LTR. We also compare LTR with other rout-\ning methods (RR, PP and HA) to demonstrate its\neffectiveness.\n5.1\nThe Advantage of Unsupervised Pivot\nTranslation\nIn order to demonstrate the advantage of unsuper-\nvised pivot translation for distant languages, we\nﬁrst analyze the distribution of the length of the\nbest translation paths (GT), as shown in Table 2.\nThe direction translation (1-hop) only takes a ra-\ntio of 7.1%, which means that a majority (92.9%)\nFigure 1: The CDF of the BLEU scores for the distant\nlanguage pairs in the test set. The green curve repre-\nsents the direct unsupervised translation (DT), and the\nblack curve represents the best translation path (GT).\nThe other three curves represent the three routing meth-\nods for comparison: blue for hop average (HA), cyan\nfor prior pivoting (PP) and red for our proposed learn-\ning to route (LTR).\nof the distant language pairs need multiple hops to\nimprove the translation accuracy.\nWe further compare the BLEU score of the best\npath (GT, which is also the upper-bound of differ-\nent routing methods) with the direct unsupervised\ntranslation, and show the results for a part of dis-\ntant languages pairs in Table 114. It can be seen\nthat GT can largely outperform the direct transla-\ntion DT with up to 5.58 BLEU points. We further\nplot the CDF of the BLEU scores on all the distant\nlanguage pairs in the test set in Figure 1. It can\nbe seen that the CDF curve of GT is always in the\nright part of DT, which means better accuracy and\ndemonstrates the advantage of unsupervised pivot\ntranslation for distant languages.\n5.2\nResults of LTR\nAccuracy of LTR Model\nAs our LTR selects the\ngood path by ranking according to the predicted\nBLEU score, we ﬁrst report the accuracy of select-\ning the best path. LTR can achieve 57% in terms\nof top-1 accuracy and 86% in terms of top-5 accu-\nracy. Although the top-1 accuracy is not so high, it\nis acceptable because there exists some other route\npath just a little bit lower than the best path. We\nshow the routing results of our LTR for some lan-\nguage pairs in Table 1. Take the Nl-Uk language\npair in Table 1 as an example. The routing result of\nLTR for this pair does not match with GT, which\n14Due to space limitation, we leave the full results of the\ndistant language pairs in the test set in the supplementary ma-\nterials (Section 4).\nMethods\nDT\nRR\nHA\nPP\nLTR\nGT\nBLEU\n6.06\n3.40\n6.92\n7.12\n8.33\n8.70\nTable 3: The performance of different routing meth-\nods. The BLEU score is averaged on all the distant\nlanguage pairs in the test set. The compared methods\ninclude: DT: direct unsupervised translation, RR: ran-\ndom routing, PP: prior pivoting, HA: hop average, LTR:\nour proposed learning to route, and GT: the best trans-\nlation path (the ground truth).\naffects the top-1 accuracy. However, the BLEU\ngap between our selected path and the best path is\nas small as 0.09, which has little inﬂuence on the\nBLEU score of the selected path. Our further anal-\nysis in the next paragraph shows that the averaged\nBLEU score that LTR achieved in test set is close\nto that of GT.\nBLEU Score of Selected Path\nWe further re-\nport the BLEU score of the translation path se-\nlected by LTR as well as other routing methods in\nTable 3, where the BLEU score is averaged over\nall the distant language pairs in the test set.\nIt\ncan be seen that compared with direct unsuper-\nvised translation (DT) which achieves 6.06 aver-\naged BLEU score15, our LTR can achieve 2.27\nBLEU points improvement on average, and is just\n0.37 points lower than the ground truth best path\n(GT). The small gap between the ground truth and\nLTR demonstrates that although LTR fails to se-\nlect the best path in 43% of the distant pairs (just\n57% in terms of top-1 accuracy), it indeed chooses\nthe path which has a BLEU score slightly lower\nthan the best path. Random routing (RR) even per-\nforms worse than DT, demonstrating the routing\nproblem is non-trivial. LTR outperforms PP and\nHA by more than 1 BLEU point on average. We\nalso show the CDF of the BLEU scores of differ-\nent methods in Figure 1, which clearly shows that\nLTR can outperform the PP and HA routing meth-\nods, demonstrating the effectiveness of the pro-\nposed LTR.\n5.3\nExtension to Supervised Pivoting\nIn the previous experiments, we rely purely on un-\nsupervised NMT for pivot translation, assuming\nthat the translation on each hop cannot leverage\nany bilingual sentence pairs. However, there in-\n15The averaged BLEU score seems not high, because the\nunsupervised translations between some hard languages in\nthe test set obtain really low BLEU scores, which affects the\naverage score.\nSource\nTarget\nDT\nGT-unsup\nGT-sup\n∆\nSource\nTarget\nDT\nGT-unsup\nGT-sup\n∆\nDa\nGl\n6.56\n12.14\n15.20\n8.64\nPt\nBe\n4.76\n8.86\n13.03\n8.27\nBg\nSv\n4.72\n9.92\n9.92\n5.20\nGl\nDa\n7.45\n11.33\n15.52\n8.07\nGl\nSv\n3.79\n8.62\n9.58\n5.79\nBe\nPt\n6.39\n9.77\n14.50\n8.11\nSv\nGl\n3.70\n8.13\n9.38\n5.68\nIt\nBe\n2.24\n5.19\n8.60\n6.36\nBe\nIt\n2.11\n6.40\n9.26\n7.15\nNl\nUk\n4.69\n7.23\n8.07\n3.38\nTable 4: The BLEU scores of the same language pairs as shown in Table 1 (Please refer to Section 5 in the\nsupplementary materials for the full results of the test set). GT-sup and GT-unsup represent the ground-truth best\npath with and without supervised pivoting. ∆is the BLEU gap between GT-sup and DT.\nMethods\nDT\nRR\nHA\nPP\nLTR\nGT\nBLEU\n6.06\n3.46\n7.07\n8.84\n9.45\n9.79\nTable 5: The performance of different routing methods\nwhen enhanced with supervised pivoting. The BLEU\nscore is averaged on all the distant language pairs in\nthe test set. The compared methods include: DT: di-\nrect unsupervised translation, RR: random routing, HA:\nhop average, PP: prior pivoting, LTR: our proposed\nlearning to route, and GT: the best translation path (the\nground truth).\ndeed exist plenty of bilingual sentence pairs be-\ntween some languages, especially among the pop-\nular languages of the world, such as the ofﬁcial\nlanguages of the United Nations and the European\nUnion. If we can rely on some supervised hop in\nthe translation path, the accuracy of the translation\nfor distant languages would be greatly improved.\nTake the translation from Danish to Galician\nas an example.\nThe BLEU score of the di-\nrect unsupervised translation is 6.56, while the\nground-truth best unsupervised path (Danish→\nEnglish→Spanish→Galician)\ncan\nachieve\na\nBLEU score of 12.14, 5.58 points higher than\ndirect unsupervised translation. For the translation\non the intermediate hop, i.e, English→Spanish,\nwe have a lot of bilingual data to train a strong\nsupervised translation model. If we replace the\nunsupervised English→Spanish translation with\nthe supervised counterpart, the BLEU score of\nthe path (Danish→English→Spanish→Galician)\ncan improve from 12.14 to 15.2, with 8.64 points\ngain over the direct unsupervised translation.\nNote that the gain is achieved without leveraging\nany bilingual sentence pairs between Danish and\nGalician.\nWithout loss of generality, we choose 6 popu-\nlar languages (we select English, German, Span-\nish, French, Finish and Russian to cover each lan-\nguage branch we considered in this work) as the\nsupervised pivot languages and replace the transla-\ntions between these languages with the supervised\ncounterparts. Note that we do not leverage any\nbilingual data related to the source language and\ntarget languages, and the supervised models are\nonly used in the intermediate hop of a 3-hop path.\nFor the bilingual sentence pairs between pivot lan-\nguages, we choose the common corpus of TED\ntalk which contains translations between multiple\nlanguages (Ye et al., 2018)16.\nTable 4 shows the performance improvements\non the language pairs (the same pairs as shown\nin Table 1). When enhanced with supervised piv-\noting, we can achieve more than 8 BLEU points\ngain over DT on 4 language pairs, without using\nany bilingual data between the source language or\ntarget language. We also compare our proposed\nlearning to route method LTR with RR, HA and\nPP, as showed in Table 5. We conduct the experi-\nments on the original development and test set, but\nremoving the language pairs whose source and tar-\nget languages belong to the supervised pivot lan-\nguages we choose. It can be seen that LTR can\nstill outperform RR, HA and PP and be close to\nGT, demonstrating the effectiveness of LTR in the\nsupervised pivoting setting.\n6\nConclusions and Future Work\nIn this paper, we have introduced unsupervised\npivot translation for distant language pairs, and\nproposed the learning to route (LTR) method to\nautomatically select a good translation path for\na distant language pair. Experiments on 20 lan-\nguages and totally 294 distant language pairs\ndemonstrate that (1) unsupervised pivot transla-\ntion achieves large improvements over direct un-\nsupervised translation for distant languages; (2)\nour proposed LTR can select the translation path\nwhose translation accuracy is close to the ground-\n16This is the same dataset where we choose the develop-\nment and test sets in Section 4.1. The data can be downloaded\nfrom https://github.com/neulab/word-embeddings-for-nmt.\ntruth best path; (3) if we leverage supervised trans-\nlation instead of the unsupervised translation for\nsome popular language pairs in the intermediate\nhop, we can further boost the performance of un-\nsupervised pivot translation.\nFor further works, we will leverage more super-\nvised translation hops to improve the performance\nof unsupervised translation for distant languages.\nWe will extend our method to more distant lan-\nguages.\nReferences\nMikel Artetxe, Gorka Labaka, and Eneko Agirre.\n2017a. Learning bilingual word embeddings with\n(almost) no bilingual data.\nIn Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), vol-\nume 1, pages 451–462.\nMikel Artetxe, Gorka Labaka, Eneko Agirre, and\nKyunghyun Cho. 2017b. Unsupervised neural ma-\nchine translation. arXiv preprint arXiv:1710.11041.\nYun Chen, Yang Liu, Yong Cheng, and Victor OK\nLi. 2017.\nA teacher-student framework for zero-\nresource neural machine translation. arXiv preprint\narXiv:1705.00753.\nYong Cheng, Qian Yang, Yang Liu, Maosong Sun, and\nWei Xu. 2017. Joint training for pivot-based neural\nmachine translation. In Proceedings of IJCAI.\nAlexis Conneau, Guillaume Lample, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Herv´e J´egou. 2017.\nWord translation without parallel data.\narXiv\npreprint arXiv:1710.04087.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nAndre Girard. 1990.\nRouting and dimensioning in\ncircuit-switched networks. Addison-Wesley Long-\nman Publishing Co., Inc.\nChristian Huitema. 2000.\nRouting in the Internet.\nPrentice-Hall,.\nDiederik Kingma and Jimmy Ba. 2014.\nAdam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nGuillaume Lample, Alexis Conneau, Ludovic Denoyer,\nand Marc’Aurelio Ranzato. 2017.\nUnsupervised\nmachine translation using monolingual corpora only.\narXiv preprint arXiv:1711.00043.\nGuillaume Lample, Myle Ott, Alexis Conneau, Lu-\ndovic Denoyer, and Marc’Aurelio Ranzato. 2018.\nPhrase-based & neural unsupervised machine trans-\nlation. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Process-\ning, Brussels, Belgium, October 31 - November 4,\n2018, pages 5039–5049.\nDavid Liben-Nowell, Jasmine Novak, Ravi Kumar,\nPrabhakar Raghavan, and Andrew Tomkins. 2005.\nGeographic routing in social networks.\nPro-\nceedings of the National Academy of Sciences,\n102(33):11623–11628.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, July 6-12, 2002, Philadelphia,\nPA, USA., pages 311–318.\nLewis M Paul, Gary F Simons, Charles D Fennig, et al.\n2009. Ethnologue: Languages of the world. Dal-\nlas, TX: SIL International. Available online at www.\nethnologue. com/. Retrieved June, 19:2011.\nSamuel Raff. 1983. Routing and scheduling of vehi-\ncles and crews: The state of the art. Computers &\nOperations Research, 10(2):63–211.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics, ACL 2016, August 7-12, 2016, Berlin, Ger-\nmany, Volume 1: Long Papers.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2019.\nMass: Masked sequence to se-\nquence pre-training for language generation. In In-\nternational Conference on Machine Learning, pages\n5926–5936.\nMasao Utiyama and Hitoshi Isahara. 2007.\nA com-\nparison of pivot methods for phrase-based statistical\nmachine translation. In Human Language Technolo-\ngies 2007: The Conference of the North American\nChapter of the Association for Computational Lin-\nguistics; Proceedings of the Main Conference, pages\n484–491.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS 2017, 4-9 December 2017, Long\nBeach, CA, USA, pages 6000–6010.\nHua Wu and Haifeng Wang. 2007. Pivot language ap-\nproach for phrase-based statistical machine transla-\ntion. Machine Translation, 21(3):165–181.\nZhen Yang, Wei Chen, Feng Wang, and Bo Xu.\n2018. Unsupervised neural machine translation with\nweight sharing. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics, ACL 2018, Melbourne, Australia, July 15-\n20, 2018, Volume 1: Long Papers, pages 46–55.\nQi Ye, Sachan Devendra, Felix Matthieu, Padmanab-\nhan Sarguna, and Neubig Graham. 2018.\nWhen\nand why are pre-trained word embeddings useful for\nneural machine translation. In HLT-NAACL.\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2019-06-06",
  "updated": "2019-06-25"
}