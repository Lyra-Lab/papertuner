{
  "id": "http://arxiv.org/abs/1906.03697v2",
  "title": "Deep Unsupervised Drum Transcription",
  "authors": [
    "Keunwoo Choi",
    "Kyunghyun Cho"
  ],
  "abstract": "We introduce DrummerNet, a drum transcription system that is trained in an\nunsupervised manner. DrummerNet does not require any ground-truth transcription\nand, with the data-scalability of deep neural networks, learns from a large\nunlabeled dataset. In DrummerNet, the target drum signal is first passed to a\n(trainable) transcriber, then reconstructed in a (fixed) synthesizer according\nto the transcription estimate. By training the system to minimize the distance\nbetween the input and the output audio signals, the transcriber learns to\ntranscribe without ground truth transcription. Our experiment shows that\nDrummerNet performs favorably compared to many other recent drum transcription\nsystems, both supervised and unsupervised.",
  "text": "DEEP UNSUPERVISED DRUM TRANSCRIPTION\nKeunwoo Choi\nSpotify\nkeunwooc@spotify.com\nKyunghyun Cho\nNew York University, Facebook AI Research\nkyunghyun.cho@nyu.edu\nABSTRACT\nWe introduce DrummerNet, a drum transcription system\nthat is trained in an unsupervised manner. DrummerNet\ndoes not require any ground-truth transcription and, with\nthe data-scalability of deep neural networks, learns from a\nlarge unlabeled dataset. In DrummerNet, the target drum\nsignal is ﬁrst passed to a (trainable) transcriber, then recon-\nstructed in a (ﬁxed) synthesizer according to the transcrip-\ntion estimate. By training the system to minimize the dis-\ntance between the input and the output audio signals, the\ntranscriber learns to transcribe without ground truth tran-\nscription. Our experiment shows that DrummerNet per-\nforms favorably compared to many other recent drum tran-\nscription systems, both supervised and unsupervised.\n1\nIntroduction\nTranscription is a music information retrieval task with the\ngoal of estimating the score y when input audio x is given.\nThe majority of the recent transcription systems is based\non supervised learning, where the transcriber is an analy-\nsis system ˆy = Fa(x) that is trained with annotated pairs\n{(xm, ym)}M\nm=1 to minimize the distance between y and\nˆy [6,7,27,31,33,34,37,38].\nThe trend is similar in drum transcription on which we\nfocus in this paper. Supervised learning approaches may\nincorporate models based on frame-based feature extrac-\ntion and classiﬁcation [15], non-negative matrix factoriza-\ntion (NMF) for pattern matching [10], or hidden-Markov\nmodel [25]. More attention has been given recently to\ndeep learning based models such as convolutional neural\nnetworks (CNNs, [13, 34]) and recurrent neural networks\n(RNNs, [33, 37, 38]), all of which have greatly improved\ntranscription systems.\nHowever, the lack of a large-scale annotated dataset is\none of the most frequently mentioned obstacles that hinder\nfurther improvement. In practice, this limits the general-\nizeability of supervised learning systems, as will be dis-\ncussed in Section 4, and using synthetic data is one way to\naddress this issue [7, 39]. Although there have been pro-\nposals to use unlabeled data [42, 43], the issue remains\nas they still rely on supervised learning combined with\nteacher-student learning [16]. Parallel to those approaches,\nc⃝Keunwoo Choi, Kyunghyun Cho. Licensed under a Cre-\native Commons Attribution 4.0 International License (CC BY 4.0). At-\ntribution:\nKeunwoo Choi, Kyunghyun Cho.\n“Deep Unsupervised\nDrum Transcription”, 20th International Society for Music Information\nRetrieval Conference, Delft, The Netherlands, 2019.\nan annotation-free and, therefore, a more scalable and gen-\neralizable alternative would be unsupervised learning.\nUnsurprisingly, one of the humans’ music learning pro-\ncedures, self-taught by trial-and-error, is very similar to un-\nsupervised learning. For example, musicians learn to tran-\nscribe by (a) listening, (b) playing an instrument, (c) iden-\ntifying differences, and (d) making adjustments. Can this\nbe done without any supervision? Yes, if the person can\nspot the pitch difference (e.g., the pitch should be higher\nor lower). Consistent with this logic, developing a tran-\nscription system based on unsupervised learning would be\nfeasible if the system can test the estimation, measure the\nerror, and correct itself accordingly.\nTo implement such an unsupervised transcription sys-\ntem, we need a synthesis system, ˆx = Fs(ˆy), making\nthe overall system ˆx = Fs(Fa(x)). During its training,\nthe system is given {x}M\nm=1 and trained to minimize the\ndistance between x and ˆx. There have been few systems\nrelying on unsupervised learning as explained above. In\nMIR, the system in [1] utilized sparse coding to learn a\ndictionary of time-frequency templates of piano and harp-\nsicord, assuming a (matrix-)multiplication model with ad-\nditive noise, Fs(y) = Ay+e. Yoshii et al. proposed to use\nsparse coding in a jointly-learned chord recognition and\ntranscription system [44]. Berg et al. designed a proba-\nbilistic graphical model that parameterizes the spectral and\ntemporal envelopes, note events, and note activations, in\norder to transcribe piano by inferring their parameters [2].\nIn drum transcription, many systems have used NMF to de-\ncompose a drum track spectrum into spectral templates and\ntheir temporal activations (or transcription) [26, 41]. Sev-\neral variants of NMF were proposed to address the limits of\nthe ﬁxed spectrum template of NMF [19,20,29]. Lastly, a\nsimilar system can be found in computer vision, where the\nparameters of input images are estimated by reconstruction\nusing an image renderer [18].\nIn this paper, we introduce DrummerNet, a deep neu-\nral network based drum transcription system that is trained\nby unsupervised learning.\nWith a more end-to-end ap-\nproach, DrummerNet is distinguished from previous re-\nsearch [1, 2, 44], which has strong priors on the target\nsounds. In §2, we present the system design principle be-\nhind DrummerNet, followed by its details in §3. In §4,\nthe evaluation results are discussed along with the ablation\nstudy. We present our conclusion, the problems of our sys-\ntem, and the future direction towards fully unsupervised\nlearning in transcription/MIR in §5.\narXiv:1906.03697v2  [cs.SD]  28 Jun 2019\nName\nDescription\nNote\nn, N\nThe temporal index/length of audio input\nk, K\nThe index/total number of drum components\nK=11\nx, y\nMixture and transcription\n∈RN\nˆx, ˆy\nEstimations of mixture/transcription\n∈RN\nTable 1: Symbols used in this paper\n6 × Conv1D\n3 × Recurrent layers\nConv1D\nConv1D\n(Kick)\nConv1D\n(Snare)\nConv1D\n(Tambourine)\nConv1D\n(Hi-hats)\nx: audio (input)\nUpsampler (zero-insertion)\n10 × Conv1D\nr: representation\n®\nx: audio (output)\nˆ\ny: estimated transcription \nˆ\ny0ˆ\ny1ˆ\nyk-1\nˆ\nFa\nFs\n2 x Sparsemax\nU-net\nz\nFigure 1:\nBlock diagrams of DrummerNet structure.\nTrainable modules are illustrated as black boxes and ﬁxed\nmodules as rounded grey boxes.\n2\nSystem Design Principles\nTraining the proposed DrummerNet is similar to the previ-\nous unsupervised learning approaches in music [1, 2, 44],\nas they all train a system to output ˆx that reconstructs the\noriginal signal x. The difference between ˆx and x works\nas a proxy of the difference between ˆy to y.\nThere are three conditions under which unsupervised\nlearning of a transcriber can be achieved successfully.\nFirst, the output of the analysis module Fa must be in the\nform of transcription – a set of discrete events representing\nthe timing and intensity of the notes. Second, the synthe-\nsis module Fs must synthesize the audio signal given the\ntranscription input ˆy. Third, all the components in the net-\nwork must be differentiable as we rely on backpropagation\nto train it.\n3\nDrummerNet\nIn this section, we introduce the proposed system structure.\nWe specify the number of channels, kernel size, and stride\nas (channel, kernel, stride). All the convolu-\ntional and recurrent layers use an exponential linear unit as\nan activation function [9]. 1\n1 The implementation of DrummerNet is available on https://\ngithub.com/keunwoochoi/DrummerNet\n3.1\nAnalysis module Fa\nThe analysis module Fa, as illustrated in the top half of\nFigure 1, takes the audio signal x as an input and processes\nit through a series of U-net variant [30], recurrent layers,\nand gated Sparsemax activation [21]. After training, this\nmodule is used as a transcriber (with peak-picking).\nU-net\nThe U-net consists of 1D convolutional layers,\nmax-pooling layers, and concatenations between the en-\ncoder and the decoder. The encoder consists of a convolu-\ntional layer (128, 3, 1) followed by 10 convolutional\nlayers (50, 3, 1) interleaved with max-pooling of size\n2. As a result, it outputs z ∈RN/1024 which has a recep-\ntive ﬁeld size of 3,072 time steps.\nThe decoder has only 6 convolutional layers (50, 3,\n1) interleaved with a concatenation with the feature map\nat the same depth as in the encoder and a ×2 bi-linear in-\nterpolation. We call the output of decoder r ∈RN/16,\nthe representation based on which the transcription is es-\ntimated.\nThe asymmetry between the encoder and the\ndecoder makes the length r to be shorter by a factor of\n42 = 16 compared to that of input x. Assuming the input\naudio is sampled at 16 kHz, 2 r would have a sampling rate\nof 1,000 Hz.\nRecurrent layers\nWe use three recurrent layers: (GRUs\n[8]) {along time-axis, bi-directional, 100-channel}, {along\ntime-axis,\nuni-directional,\n50-channel},\nand\n{along\nchannel-axis, uni-directional, K-channel}. These three re-\ncurrent layers have properties of i) being bi-directional so\nthat the onset at n can be determined by the vicinity of n\n(both the past and the future), ii) enforcing temporal de-\npendency, and iii) enforcing component-wise dependency,\nrespectively. The width (or the hidden vector size) of the\nthird recurrent layer is equal to K, the number of drum\ncomponents in the synthesizer, to map each channel to each\ndrum component.\nSparsemax\nIn an ideal case of transcription, there would\nbe local sparsity along both the time and channel-axes be-\ncause the drum events would not repeat with a rate of\n1,000 Hz (which is faster than 16-beat on 240 BPM), nor\nwould all the K drum components be activated simultane-\nously. Although sparsity is one of the properties that can be\nachieved by the autoregressive nature of the recurrent lay-\ners, we add Sparsemax [21] activation to encourage it ex-\nplicitly. The output of Sparsemax has two important prop-\nerties: i) it always sums to 1 (same as Softmax) and ii) it\nis highly likely to be sparse with actual zeros (unlike Soft-\nmax). In DrummerNet, two Sparsemax layers are applied\nin parallel, one along channel-axis (=instrument-axis) and\nthe other time-axis within a non-overlapping window size\nof 64. This design choice is based on the assumption that\nthere are only a few onsets among notes (channel-axis spar-\nsity) and within 64 samples at ˆy, or 64 ms (temporal spar-\nsity). The outputs from these two Sparsemax layers are\nthen multiplied element-wise.\n2 This is the sampling rate of input audio in our experiment.\nClass\nSubclass\nDescription\nKD\nKD\nKick drum\nSD\nSD\nSnare drum\nHH\nCHH, PHH\nClosed/pedalled hi-hat\nOHH\nOpen hi-hat\nTT\nHIT, MHT,\nHigh/high-mid/\nHFT, LFT*\nhigh-ﬂoor/low-ﬂoor tom\nCY\nRDC, RDB*\nRide cymbal, ride cymbal bell\nCRC, CHC*,\nSPC*\nCrash/china cymbal\nsplash cymbal\nOT\nSST*, TMB\nside stick, tambourine\nTable 2: A drum component hierarchy [36]. The synthe-\nsizer Fs consists of 11 classes, following Subclass of the\ntable with omitting ones marked with asterisks *.\nMedian-Filtering \nOnset Enhancement \nMedian-Filtering \nOnset Enhancement \nCQTs \nCQTs \nx\nxˆ\nMean \nAbsolute \nError \nFigure 2: The block diagrams of loss calculation\nUpsampler\nFinally, the low temporal resolution of the\nSparsemax output is addressed by zero-insertion upsam-\npling by the factor of 16. According to this, we modify the\ntemporal quantization rate of events, unlike the upsampling\nof a digital signal.\n3.2\nSynthesis module Fs\nThe synthesis module Fs consists of K parallel 1D convo-\nlutional layers and a channel-wise summing operator. The\nkernel of each layer is not trained but ﬁxed to the known\nwaveform of each drum component to convert a transcrip-\ntion of a component ˆyk into a track ˆxk. The tracks are\nsummed to generate the ﬁnal output ˆx (= PK\nk=1 ˆxk), the\nsynthesized audio signal. This module is only used during\ntraining.\nIn the implementation, we use K = 11, using Subclass\nin Table 2, following [36]. Ones marked with asterisks\nwere excluded due to their scarcities in our source of iso-\nlated drum recordings, which consisted of 12 virtual drum\ninstruments provided by Logic Pro X. Multiple drum kits,\nincluding rock, pop, funk, and soul 3 , were used to prevent\nthe network from overﬁtting to a speciﬁc drum kit. Dur-\ning training, a drum kit was randomly assigned for every\nbatch.\n3.3\nLearning\nUnable to directly compute the transcription loss during\nunsupervised learning, we carefully designed a loss func-\ntion at the audio level, Lx(x, ˆx), as minimizing it would\nalso minimize the transcription loss, Ly(y, ˆy). To do so,\n3 Brooklyn, Heavy, Liverpool, Neo Soul, Detroit Garage, Motown\nRevisited, Portland, Sunset, Speakeasy, SoCal, Smash, and Slow Jam.\nAll with velocity=98.\nModule\nInput (size)\nOutput (size)\nU-net encoder\nConv1D\nx:(1, N)\n(C∗, N)\n10× Conv1D\n(C∗, N)\n(C∗, N/1024)\nU-net decoder\n6× Conv1D\n(C∗, N/1024)\nr:(C∗, N/16)\nRecurrent layers\n(C∗, N/16)\n(K, N/16)\nSparsemax\n(K, N/16)\n(K, N/16)\nUpsampler\n(K, N/16)\nˆy:(K, N)\nSynthesis module\nChannel splitter\nˆy:(K, N)\nK × ˆyk:(1, N)\nEach Conv1D\nˆyk:(1, N)\nˆxk:(1, N)\nSum (mixer)\nK × ˆxk:(1, N)\nˆx:(K, N)\nTable 3: The shapes of inputs/outputs of the module in\nDrummerNet. C∗indicates the number of channels but\nunspeciﬁed.\nFigure 3: The effect of drum extraction for kick, snare,\nclose hi-hat, and open hi-hat, from top to bottom. Columns\nare from left to right: original waveform, original spec-\ntrum, and onset-enhanced spectrum\nLx should be able to differentiate the drum components\n– kick drum (KD), snare drum (SD), and hi-hat (HH) –\nwhile being invariant to the varying drum kits. Perceptu-\nally, there are clear differences between KD, SD, and HH.\nAlthough both impulsive, KD is in the low-frequency band\nwhile SD is in the mid-frequency band. SD is also rela-\ntively tonal and has a longer envelope. HH is more compli-\ncated to describe due to its variation from its playing tech-\nnique. For example, closed and pedalled-HH’s are in the\nhigh-frequency band, impulsive, and with relatively low\nenergy, while open-HH’s are similar except louder with a\nlonger noisy envelope.\nWe thus deﬁne and use onset spectrum similarity, which\nis designed to represent the similarity based on the onset\npart of sounds in the spectrum domain. As illustrated in 2,\nit is measured by i) applying median-ﬁltering based drum\nextraction [12] which enhances onsets (with a FFT size of\n1024 and median ﬁlter length of 31 on both axes), ii) con-\nverting to multi-resolution CQTs (constant-Q transform)\nfor both x and ˆx, and then iii) calculating the mean abso-\nlute difference between them.\nAmong many spectral magnitude representations, we\nuse (log-magnitude) CQT since the logarithmic frequency\nscale is known to match well to human auditory percep-\ntion [23].\nWe followed the implementation of Pseudo-\nCQT 4 which multiplies linear-to-octave ﬁlterbanks to an\nSTFT. As a result, the CQT covered nearly 8-octave bands\n4 http://librosa.github.io/librosa/\nfrom 32.07 Hz (C1) to 8 kHz (the Nyquist frequency of our\nexperiment) with a 12-band/octave resolution. This imple-\nmentation is differentiable.\nFigure 3 shows the effect of onset enhancement. It pre-\nserves the characteristics of the drum components in the\ntransient part while removing the after-onset components.\nThis process makes Lx and Ly more similar, as the non-\ntransient parts vary more among drum kits due to their ran-\ndom and noisy nature. In a preliminary experiment, for ex-\nample, the network tried to reconstruct all the non-transient\ncomponents of SD using tom-toms and HHs, resulting in\nnon-sparse and severe false-positive detection of onsets.\n4\nExperiments and Analysis\n4.1\nSetup\nFor the training of DrummerNet, we used an in-house\ndataset of drum stems that are crawled from many web-\nsites. The dataset consisted of 3,940 unique tracks averag-\ning 225 seconds each for a total of 249 hours. Since the\ndataset was crawled from various websites, some details,\nsuch as the distribution of drum components, are hard to\nidentify. The tracks were mostly popular western rock/pop\nmusic. Alternatives to this in-house dataset can be found\nin [7] (3,758 drum sample recordings (×8 second = over\n8 hours) or 60,000 synthesized drum loops (×8 second =\nover 133 hours)) and [39] (4,197 drum tracks (259 hours)).\nWe opted for the in-house dataset because it provided more\ndiversity as it was not synthesized.\nEach audio ﬁle was resampled to 16 kHz and down-\nmixed to mono. The training batch size was 16, and for\neach audio ﬁle, we randomly selected a 2-second segment.\nOn average, there were 112.5 segments in a track, and\ntherefore training with 443,250 (=3,940 × 112.5) items\nwould be approximately one epoch. With a Nvidia Tesla\nP100 and a batch size of 32, it took about 9 hours to train\na single epoch. We implemented DrummerNet using Py-\ntorch 1.0 [24] and used Librosa 0.6.3 [22] and Madmom\n0.16 [4] for audio processing and peak-picking.\nWe used a heuristic peak-picking method introduced in\n[5]. This method selects a peak ˆy[n] at n if it satisﬁes the\nthree conditions in Eq. (1),\nˆy[n] =max(x[n −wm], ..., x[n + wm])\nˆy[n] ≥average(x[n −wa], ..., x[n + wa]) + δ\nn >nlp + ww,\n(1)\nwhere the max window wm=50 ms, average window\nwa=100 ms, threshold δ=0.2, waiting window ww=50 ms,\nand nlp is the last detected peak. We mainly use F1 score\nalong with Precision and Recall using mir eval [28]. The\ntolerance window is 50 ms.\nAfter training, we test the system on three public\ndatasets: IDMT-SMT-Drums (SMT, 104 drum tracks, total\n130 minutes [10]), Medley-DB Drums (MDB, 23 tracks,\ntotal 20 minutes [36]), and ENST-drums (ENST, 61 min-\nutes [14], drum-only tracks known as ‘wet-mix’ of ‘minus-\none’ subset). According to [40], a task is DTD 5 if tracks\n5 DTD: drum transcription of drum-only recordings\n1\n10\n100\nnumber of training items [x100,000]\n0.5\n0.6\n0.7\n0.8\n0.9\nAverage F1 scores\nAvg\nSMT\nMDB\nENST\nFigure 4: The F1 scores of DrummerNet over training\nitems on each dataset (SMT, MDB, ENST), averaged over\nKD, SD, and HH. AVG indicates the overall average F1\nscores of three datasets.\nare drum-only, more precisely KD/SD/HH-only, and the\nsystem annotates KD/SD/HH events.\nThis is the case\nfor the SMT dataset.\nA task with the system annotat-\ning KD/SD/HH but with drum tracks consisting of more\nthan those three components, e.g., tom-toms and cym-\nbals, is named DTP 6 in [40].\nFollowing this conven-\ntion, we evaluate DTD with SMT (Section 4.3), and DTP\nwith MDB/ENST. We did not ﬁne-tune for any dataset in\nany experiment and used the whole datasets for evaluation\nonly.\n4.2\nTrend of Performance over Training\nWe did not employ a stopping strategy but trained the net-\nwork for 6 × 106 items (about 13 epochs). As illustrated\nin Figure 4, the overall performance gradually increases as\nthe training proceeds and approaches converging towards\nthe end of training. This indicates that the proposed loss\nfunction is a good proxy of transcription loss. After the ini-\ntial phase of training, the performance differences among\ndatasets remain consistent, probably due to the different\ncharacteristics of drum tracks in each dataset, as will be\ndiscussed in Section 4.4.\n4.3\nRelative Performance against Baselines\nIn this experiment, we trained our system on the in-house\ntraining set without any annotation and evaluated it on\na separate test set (also known as ‘eval-cross (trained\non DTP)’, [40]), which is a stronger condition than a\nusual train/test split scenario in supervised learning (‘eval-\nsubset’, [40]). This setup allows us to measure the gener-\nalization capabilities across the datasets. Speciﬁcally, our\nexperiment is equivalent to DTD, ‘eval-cross (trained on\nDTP)’ experiment in [40]. 7 , which is only available on\nSMT. Therefore, only the performances on SMT are com-\npared in this Section. Overall, the performance of Drum-\nmerNet is favorable to that of recent drum transcription\nsystems.\nWith an average F1 score of 0.869 on SMT,\nthe proposed unsupervised DrummerNet outperformed 9\nout of 10 systems. The nine systems include ones with\ndeep neural networks and supervised approach (ReLUts,\nRNN, lstmpB, tanhB, and GRUts [33,34,37,38]), as well as\nones with NMF and unsupervised approach (AM1, AM2,\n6 DTP: drum transcription in the presence of percussion\n7\nNumbers are omitted in the paper but are available on-\nline:\nhttps://www.audiolabs-erlangen.de/resources/\nMIR/2017-DrumTranscription-Survey.\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n.794\nAM2 (2015)\n.796\nAM1 (2015)\n.805\nReLUts (2016)\n.819\nRNN (2016)\n.830\nPFNMF (2015)\n.831\nlstmpB (2017)\n.847\ntanhB (2016)\n.851\nGRUts (2017)\n.865\nSANMF (2014)\n.869\nDrummerNet \n.903\nNMFD (2012)\nAvg\nKD\nSD\nHH\nFigure 5: The F1 scores of DrummerNet and other sys-\ntems on SMT with ‘eval-cross’ setup, sorted by the as-\ncending order of the overall average. The system names\nfollow [40].\nPFNMF, and SANMF [10, 41]).\nIt did not outperform\nNMFD [20], a system based on the convolutive NMF.\nThe\ncomparison\nbetween\nDrummerNet\nand\nthe\nNMF/unsupervised\nlearning-based\nsystems\n[10, 41]\nimplies that the proposed deep neural network structure\neffectively learns relevant representations. Furthermore,\nDrummerNet allows constant-time inference, unlike NMF\nand other factorization-based approaches which require\niterative optimization in the test time.\nWhat is more interesting is its generalizability. All the\ndeep learning based systems 8 present deteriorating perfor-\nmance in the transfer learning scenario (eval-cross) com-\npared to the dataset split scenario (eval-subset). 9\nHow-\never, less data-driven approaches 10 present similar or even\nincreased performances in eval-cross. This implies that\nthe distributions within datasets are fairly different and bi-\nased to certain types of drum tracks and therefore, a tran-\nscription system trained with those datasets will be also\nbiased accordingly. This limitation may be attributed to\nthe small sizes of those datasets. Theoretically, supervised\ndeep learning systems may generalize better if trained on a\nvery large dataset, which lacks practicality due to the high\nannotation cost. In contrast, it is relatively easy to unbias\nDrummerNet. One only needs to control the distribution of\ndrum tracks by their style/genre/sounds without annotating\nevery note.\n4.4\nQualitative Analysis\nIn this section, we will analyze the performance and the be-\nhavior of DrummerNet by components, datasets, and met-\nrics, as illustrated in Figure 6. Here, we notice two clear\ntrends. First, across all of the three datasets and the met-\nrics, detecting KD was the easiest, followed by SD and HH\n(except the precision on SMT). Second, SMT seems to be\nthe easiest, followed by MDB and ENST. What could be\nthe reasons?\nThe ﬁrst trend is strongly related the proposed loss func-\ntion. KD has the least within-class variability while being\nthe most distinguishable component (the largest mutual-\nclass variability) due to its solitary frequency range. SD\nand HH share both the mid and high-frequency ranges\n8 RNN, tanhB, ReLUts, lstmpB, GRUts - RNN-based systems\n9 See Figure 10 (b) of [40]. Note that most of the reported scores in\npapers also follow eval-subset setup.\n10 SANMF, NMF, PFNMF, AM1, AM2 - NMF-based systems\nKD\nSD\nHH\n0.00\n0.25\n0.50\n0.75\n1.00\n0.978\n0.826\n0.804\nF1=0.869\nKD\nSD\nHH\n0.984\n0.776\n0.882\nPrecision=0.881\nKD\nSD\nHH\n0.977\n0.939\n0.774\nRecall=0.897\nKD\nSD\nHH\n0.00\n0.25\n0.50\n0.75\n1.00\n0.963\n0.697\n0.333\nF1=0.664\nKD\nSD\nHH\n0.962\n0.799\n0.387\nPrecision=0.716\nKD\nSD\nHH\n0.969\n0.702\n0.360\nRecall=0.677\nKD\nSD\nHH\n0.00\n0.25\n0.50\n0.75\n1.00\n0.872\n0.599\n0.393\nF1=0.621\nKD\nSD\nHH\n0.966\n0.857\n0.647\nPrecision=0.823\nKD\nSD\nHH\n0.815\n0.503\n0.320\nRecall=0.546\nFigure 6: Evaluation of DrummerNet on SMT (top), MDB\n(middle), and ENST (bottom) datasets.\nFigure 7: A transcription example of DrummerNet, ‘Real\nDrum 01-12’ in SMT - the output of analysis module (top),\nafter peak-picking (middle), and ground truth (bottom);\nKD, SD, HH (left to right).\nand their sounds can vary signiﬁcantly across drum kits\n– i.e., larger within-class variability and smaller mutual-\nclass variability. A common pattern, consequently, is the\nfalse positive of HH due to SD and vice versa. This is pre-\nsented in Figure 7, where SD has many false positives due\nto HH.\nThe second trend is caused by the mixed use of the\nprobability and the onset velocity in the DrummerNet. Al-\nthough transcription ˆy is the estimated amplitude of drum\ncomponents, the peak-picking method treats ˆy as if it was a\nprobability. This discrepancy becomes problematic when\nthe velocities of drum events in a track vary drastically as\nin the case of MDB and ENST. A failure case is demon-\nstrated in Figure 7, where the HH with strong accents on\nseveral occasion caused DrummerNet to miss many of the\nother HH peaks.\n4.5\nAblation Study\nWe conducted an ablation study where the performance of\nDrummerNet is compared with that of its variants. Figure 8\nshows the reported F1 scores averaged over datasets and\ncomponents. Please refer to the caption in Figure 8 for the\ndeﬁnitions of the system names.\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nF1 score\n0.718\n0.576\n0.659 0.684 0.701 0.707\nDFL\nSOFT\nMEL\nSTFT\nNOE CONV\nAVG\nKD\nSD\nHH\nFigure 8: The ablation study results, F1 scores averaged\nover three datasets per component (KD, SD, HH) and\ntheir overall average (AVG). The label indicates as follow:\nDFL (default DrummerNet as introduced), SOFT (two\nSoftmax layers instead of Sparsemax), MEL (use 128-band\nmelspectrogram instead of CQTs), STFT (use 1024-point\nSTFT instead of CQTs), NOE (not onset enhancement in\nloss), CONV (3-layer convolutional layers instead of re-\ncurrent layers).\nFigure 9: A transcription example of SOFT (DrummerNet\nwith Softmax) , ‘Real Drum 01-12’ in SMT - the output\nof analysis module (top), after peak-picking (middle), and\nground truth (bottom); KD, SD, HH (left to right).\nSparsemax (DFL vs. SOFT)\nAmong all the variants in\nthis experiment, we observe the most dramatic change in\nthe performance when we replaced Sparsemax with Soft-\nmax (SOFT), mostly in a negative way.\nIn SOFT, the\ntwo Softmax layers were applied in sequence instead of\nin-parallel and multiplied, which we tested, but the train-\ning was unstable. The transcription ˆy of SOFT tends to\nbe much noisier with many false positives, as presented in\nFigure 9. We conclude that the sparsity induced by Sparse-\nmax is a crucial factor behind the success of the proposed\nunsupervised transcription.\nFigure 9 provides a good example of the performance\ndegradation pattern for each component. As in Figure 8,\nalthough the scores of all the three components decrease\nin SOFT, the degradation is not as critical for HH as in\nthe case of KD/SD. This observation reﬂects the underly-\ning properties of the different components. KD and SD\nare sparser than HH, and thus may beneﬁt more from the\nintroduction of Sparsemax.\nCQT (DFL vs. MEL vs. STFT)\nReplacing CQTs with\neither melspectrograms (MEL) or short-time Fourier trans-\nform magnitudes (STFT) results in decreased performance.\nUnlike CQTs, where different numbers of FFT are used for\neach octave range, melspectrograms are computed based\non single-resolution STFT. This implies that DrummerNet\nbeneﬁts from CQTs which consider multiple temporal and\nspectral resolutions.\nComparing MEL and STFT, the melfrequency compres-\nsion helps with the better detection of KD but not SD\nnor HH. This is explained by the different frequency band\nweighting of STFT and melspectrogram. Since melfre-\nquency is linear below 1 kHz and logarithmic above 1 kHz\n[32], melspectrogram allocates relatively more bins below\n1 kHz. This means that the loss function in MEL is biased\ntowards the low-frequency range, resulting in training that\nfavors KD over the others.\nOnset Enhancement (DFL vs.\nNOE)\nThe onset en-\nhancement is shown to be boosting the performance, but\nnot signiﬁcantly (0.017). In the learning curve, we ob-\nserve that removing the onset enhancement from the loss\nfunction results in a large performance degradation during\nthe initial phase of training. This is mainly due to false-\npositives in the non-transient part.\nRecurrent layers (DFL vs. CONV)\nOverall, replacing\nthree recurrent layers with three convolutional layers does\nnot make signiﬁcant differences (0.011). This may means\ni) a long-term relationship may not provide additional in-\nformation, probably because the transcription largely de-\npends on local information, and ii) the mutual conditioning\nin the last recurrent layer is not effective in our experiment.\nIn an informal analysis, we observed that with recurrent\nlayers, ˆy still has some local temporal correlation, e.g., the\nactivations are smeared over time, probably because that is\nbetter to reconstruct the input audio.\n5\nConclusion\nWe introduced DrummerNet, a deep neural network that is\ntrained to transcribe drum tracks without a labeled dataset.\nIn the experiment, DrummerNet achieved strong perfor-\nmance compared to existing systems trained with super-\nvised learning, showing its generalizability towards a real-\nworld drum transcription scenario.\nOur ablation study\nshowed that Sparsemax and CQT played a crucial role in\nthe successful training of DrummerNet.\nThe experiment also revealed room for further im-\nprovements. Considering the discreteness of the musical\nnotes, a reinforcement learning approach may be more\nsuitable [35], making the prediction more sparse and re-\nplacing the peak-picking with trainable action. The onset-\nenhancement on audio similarity is a function carefully-\nchosen in order to approximate Ly when x and ˆx are\ngiven.\nUnfortunately, the approximation is limited be-\ncause the exact drum sounds in x are not given, and there-\nfore a perfect reconstruct of (onsets of) the input audio\n(Lx(x, ˆx) = 0) does not lead to a perfect transcription\n(Ly(y, ˆy) = 0). An alternative way would be measuring a\nsimilarity on a (perceptual) representation domain instead\nof the audio, for example, by learning a loss using forward-\nbackward consistency (also known as a cyclic loss [17]) or\nknown audio features. Lastly, the current synthesizer mod-\nule is limited to drums as it does not handle the duration\nof notes. A trainable synthesizer can be used to expand\nDrummerNet to other instruments [3,11], eventually lead-\ning to an unsupervised universal transcription system com-\nbined with instrument recognition.\n6\nAcknowledgement\nWe thank Tristan Jehan and Sebastian Ewert for their valu-\nable comments and discussions. We would also like to ex-\npress our sincere gratitude to Chih-Wei Wu for sharing his\ninsight with us.\n7\nReferences\n[1] Samer A Abdallah and Mark D Plumbley. Unsuper-\nvised analysis of polyphonic music by sparse coding.\nIEEE Transactions on neural Networks, 17(1):179–\n196, 2006.\n[2] Taylor Berg-Kirkpatrick, Jacob Andreas, and Dan\nKlein. Unsupervised transcription of piano music. In\nAdvances in neural information processing systems,\npages 1538–1546, 2014.\n[3] Merlijn Blaauw and Jordi Bonada. A neural parametric\nsinging synthesizer. arXiv preprint arXiv:1704.03809,\n2017.\n[4] Sebastian B¨ock, Filip Korzeniowski, Jan Schl¨uter, Flo-\nrian Krebs, and Gerhard Widmer. madmom: a new\nPython Audio and Music Signal Processing Library.\nIn Proceedings of the 24th ACM International Con-\nference on Multimedia, pages 1174–1178, Amsterdam,\nThe Netherlands, 10 2016.\n[5] Sebastian B¨ock, Florian Krebs, and Markus Schedl.\nEvaluating the online capabilities of onset detection\nmethods. In Proceedings of the International Society\nfor Music Information Retrieval Conference (ISMIR),\npages 49–54, 2012.\n[6] Sebastian B¨ock and Markus Schedl. Polyphonic pi-\nano note transcription with recurrent neural networks.\nIn IEEE international conference on acoustics, speech\nand signal processing (ICASSP), pages 121–124.\nIEEE, 2012.\n[7] Mark Cartwright and Juan Pablo Bello. Increasing\ndrum transcription vocabulary using data synthesis.\nProc. of the 21st Int. Conference on Digital Audio Ef-\nfects (DAFx-18). Aveiro, Portugal, 2018.\n[8] Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho,\nand Yoshua Bengio. Empirical evaluation of gated\nrecurrent neural networks on sequence modeling. In\nNeurIPS - Workshop on Deep Learning, December\n2014, 2014.\n[9] Djork-Arn´e Clevert, Thomas Unterthiner, and Sepp\nHochreiter. Fast and accurate deep network learn-\ning by exponential linear units (elus). arXiv preprint\narXiv:1511.07289, 2015.\n[10] Christian Dittmar and Daniel G¨artner. Real-time tran-\nscription and separation of drum recordings based on\nnmf decomposition. In Proceedings of the conference\non Digital Audio Effects (DAFx), pages 187–194, 2014.\n[11] Jesse Engel, Cinjon Resnick, Adam Roberts, Sander\nDieleman, Mohammad Norouzi, Douglas Eck, and\nKaren Simonyan. Neural audio synthesis of musical\nnotes with wavenet autoencoders. In Proceedings of the\n34th International Conference on Machine Learning-\nVolume 70, pages 1068–1077. JMLR. org, 2017.\n[12] Derry Fitzgerald. Harmonic percussive separation us-\ning median ﬁltering. Proceedings of the conference on\nDigital Audio Effects (DAFx), Graz, Austria, 2010,\n2010.\n[13] Nicolai Gajhede, Oliver Beck, and Hendrik Purwins.\nConvolutional neural networks with batch normaliza-\ntion for classifying hi-hat, snare, and bass percussion\nsound samples. In Proceedings of the Audio Mostly\n2016, pages 111–115. ACM, 2016.\n[14] Olivier Gillet and Ga¨el Richard. Enst-drums: an exten-\nsive audio-visual database for drum signals processing.\nIn Proceedings of the International Society for Music\nInformation Retrieval Conference (ISMIR), pages 156–\n159, 2006.\n[15] Fabien Gouyon, Franc¸ois Pachet, Olivier Delerue, et al.\nOn the use of zero-crossing rate for an application of\nclassiﬁcation of percussive sounds. In Proceedings of\nthe conference on Digital Audio Effects (DAFx-00),\nVerona, Italy, 2000.\n[16] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-\ning the knowledge in a neural network. arXiv preprint\narXiv:1503.02531, 2015.\n[17] Zdenek Kalal, Krystian Mikolajczyk, and Jiri Matas.\nForward-backward error:\nAutomatic detection of\ntracking failures. In 20th International Conference on\nPattern Recognition, pages 2756–2759. IEEE, 2010.\n[18] Angjoo Kanazawa, Shubham Tulsiani, Alexei A Efros,\nand Jitendra Malik. Learning category-speciﬁc mesh\nreconstruction from image collections. In Proceed-\nings of the European Conference on Computer Vision\n(ECCV), pages 371–386, 2018.\n[19] Cl´ement Laroche, H´el`ene Papadopoulos, Matthieu\nKowalski, and Ga¨el Richard. Drum extraction in sin-\ngle channel audio signals using multi-layer non nega-\ntive matrix factor deconvolution. In 2017 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 46–50. IEEE, 2017.\n[20] Henry Lindsay-Smith, Skot McDonald, and Mark\nSandler. Drumkit transcription via convolutive nmf.\nIn International Conference on Digital Audio Effects\n(DAFx), York, UK, 2012.\n[21] Andre Martins and Ramon Astudillo. From softmax to\nsparsemax: A sparse model of attention and multi-label\nclassiﬁcation. In International Conference on Machine\nLearning, pages 1614–1623, 2016.\n[22] Brian McFee, Matt McVicar, Stefan Balke, Vin-\ncent Lostanlen,\nCarl Thom,\nColin Raffel,\nDana\nLee, Kyungyun Lee, Oriol Nieto, Frank Zalkow,\nDan Ellis, Eric Battenberg, Ryuichi Yamamoto, Josh\nMoore, Ziyao Wei, Rachel Bittner, Keunwoo Choi,\nnullmightybofo, Pius Friesch, Fabian-Robert Stter,\nThassilo, Matt Vollrath, Siddhartha Kumar Golu, nehz,\nSimon Waloschek, Seth, Rimvydas Naktinis, Douglas\nRepetto, Curtis ”Fjord” Hawthorne, and CJ Carr. li-\nbrosa/librosa: 0.6.3, February 2019.\n[23] Brian CJ Moore. An introduction to the psychology of\nhearing. Brill, 2012.\n[24] Adam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming Lin,\nAlban Desmaison, Luca Antiga, and Adam Lerer. Au-\ntomatic differentiation in pytorch. In Advances in neu-\nral information processing systems 2017 Workshop,\n2017.\n[25] Jouni Paulus and Anssi Klapuri. Drum sound detec-\ntion in polyphonic music with hidden markov models.\nEURASIP Journal on Audio, Speech, and Music Pro-\ncessing, 2009:14, 2009.\n[26] Jouni Paulus and Tuomas Virtanen. Drum transcription\nwith non-negative spectrogram factorisation. In Signal\nProcessing Conference, 2005 13th European, pages 1–\n4. IEEE, 2005.\n[27] Graham E Poliner and Daniel PW Ellis. A dis-\ncriminative model for polyphonic piano transcription.\nEURASIP Journal on Advances in Signal Processing,\npage 048317, 2006.\n[28] Colin Raffel, Brian Mcfee, Eric J. Humphrey, Justin\nSalamon, Oriol Nieto, Dawen Liang, Daniel P. W. El-\nlis, C Colin Raffel, and Eric J. Humphrey. mir eval:\na transparent implementation of common mir metrics.\nIn In Proceedings of the 15th International Society for\nMusic Information Retrieval Conference, ISMIR, 2014.\n[29] Axel Roebel, Jordi Pons, Marco Liuni, and Math-\nieu Lagrangey. On automatic drum transcription using\nnon-negative matrix deconvolution and itakura saito di-\nvergence. In IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), pages\n414–418. IEEE, 2015.\n[30] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.\nU-net: Convolutional networks for biomedical image\nsegmentation. In International Conference on Medical\nimage computing and computer-assisted intervention,\npages 234–241. Springer, 2015.\n[31] Siddharth Sigtia, Emmanouil Benetos, and Simon\nDixon. An end-to-end neural network for polyphonic\npiano music transcription. IEEE/ACM Transactions on\nAudio, Speech, and Language Processing, 24(5):927–\n939, 2016.\n[32] Malcolm Slaney. Auditory toolbox. Interval Research\nCorporation, Tech. Rep, 10(1998), 1998.\n[33] Carl Southall, Ryan Stables, and Jason Hockman. Au-\ntomatic drum transcription using bi-directional recur-\nrent neural networks. In Proceedings of the Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR), pages 591–597, 2016.\n[34] Carl Southall, Ryan Stables, and Jason Hockman.\nAutomatic drum transcription for polyphonic record-\nings using soft attention mechanisms and convolutional\nneural networks. In Proceedings of the International\nSociety for Music Information Retrieval Conference\n(ISMIR), pages 606–612, 2017.\n[35] Carl Southall, Ryan Stables, and Jason Hockman.\nPlayer vs transcriber: A game approach to data manip-\nulation for automatic drum transcription. In Proceed-\nings of the 19th International Society for Music Infor-\nmation Retrieval Conference (ISMIR), Paris, France,\n2018.\n[36] Carl Southall, Chih-Wei Wu, Alexander Lerch, and\nJason Hockman. MDB drums–an annotated subset of\nmedleydb for automatic drum transcription. In Inter-\nnational Society for Music Information Retrieval Con-\nference (ISMIR) Late-breaking/demo session, 2017.\n[37] Richard Vogl, Matthias Dorfer, and Peter Knees. Re-\ncurrent neural networks for drum transcription. In Pro-\nceedings of the International Society for Music Infor-\nmation Retrieval Conference (ISMIR), pages 730–736,\n2016.\n[38] Richard Vogl, Matthias Dorfer, and Peter Knees. Drum\ntranscription from polyphonic music with recurrent\nneural networks. In IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP),\npages 201–205. IEEE, 2017.\n[39] Richard Vogl, Gerhard Widmer, and Peter Knees. To-\nwards multi-instrument drum transcription. Proc. of the\n21st Int. Conference on Digital Audio Effects (DAFx-\n18). Aveiro, Portugal, 2018.\n[40] Chih-Wei Wu,\nChristian Dittmar,\nCarl Southall,\nRichard Vogl, Gerhard Widmer, Jason Hockman,\nMeinard Muller, and Alexander Lerch. A review of\nautomatic drum transcription. IEEE/ACM Transactions\non Audio, Speech and Language Processing (TASLP),\n26(9):1457–1483, 2018.\n[41] Chih-Wei Wu and Alexander Lerch. Drum transcrip-\ntion using partially ﬁxed non-negative matrix factor-\nization with template adaptation. Proceedings of the\nInternational Society for Music Information Retrieval\nConference (ISMIR), pages 257–263, 2015.\n[42] Chih-Wei\nWu\nand\nAlexander\nLerch.\nAutomatic\ndrum transcription using the student-teacher learning\nparadigm with unlabeled music data. In Proc. Int. Soc.\nMusic Inf. Retrieval Conf., pages 613–620, 2017.\n[43] Chih-Wei Wu and Alexander Lerch. From labeled\nto unlabeled data–on the data challenge in automatic\ndrum transcription. Proceedings of the International\nSociety for Music Information Retrieval Conference\n(ISMIR), 2018.\n[44] Kazuyoshi Yoshii and Masataka Goto. Unsupervised\nmusic understanding based on nonparametric bayesian\nmodels. In IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), pages\n5353–5356, 2012.\n",
  "categories": [
    "cs.SD",
    "cs.AI",
    "eess.AS"
  ],
  "published": "2019-06-09",
  "updated": "2019-06-28"
}