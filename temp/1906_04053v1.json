{
  "id": "http://arxiv.org/abs/1906.04053v1",
  "title": "Joint Semantic Domain Alignment and Target Classifier Learning for Unsupervised Domain Adaptation",
  "authors": [
    "Dong-Dong Chen",
    "Yisen Wang",
    "Jinfeng Yi",
    "Zaiyi Chen",
    "Zhi-Hua Zhou"
  ],
  "abstract": "Unsupervised domain adaptation aims to transfer the classifier learned from\nthe source domain to the target domain in an unsupervised manner. With the help\nof target pseudo-labels, aligning class-level distributions and learning the\nclassifier in the target domain are two widely used objectives. Existing\nmethods often separately optimize these two individual objectives, which makes\nthem suffer from the neglect of the other. However, optimizing these two\naspects together is not trivial. To alleviate the above issues, we propose a\nnovel method that jointly optimizes semantic domain alignment and target\nclassifier learning in a holistic way. The joint optimization mechanism can not\nonly eliminate their weaknesses but also complement their strengths. The\ntheoretical analysis also verifies the favor of the joint optimization\nmechanism. Extensive experiments on benchmark datasets show that the proposed\nmethod yields the best performance in comparison with the state-of-the-art\nunsupervised domain adaptation methods.",
  "text": "Joint Semantic Domain Alignment and\nTarget Classiï¬er Learning for\nUnsupervised Domain Adaptation\nDong-Dong Chen1,2,âˆ—, Yisen Wang2, Jinfeng Yi2, Zaiyi Chen3, Zhi-Hua Zhou1\n1National Key Laboratory for Novel Software Technology, Nanjing University\n2JD AI Research\n3School of Computer Science, University of Science and Technology of China\n{chendd, zhouzh}@lamda.nju.edu.cn, eewangyisen@gmail.com, yijinfeng@jd.com, chenzaiyi@mail.ustc.edu.cn\nAbstract\nUnsupervised domain adaptation aims to transfer the classiï¬er learned from the source domain to\nthe target domain in an unsupervised manner. With the help of target pseudo-labels, aligning class-level\ndistributions and learning the classiï¬er in the target domain are two widely used objectives. Existing\nmethods often separately optimize these two individual objectives, which makes them suffer from the\nneglect of the other. However, optimizing these two aspects together is not trivial. To alleviate the above\nissues, we propose a novel method that jointly optimizes semantic domain alignment and target classiï¬er\nlearning in a holistic way. The joint optimization mechanism can not only eliminate their weaknesses but\nalso complement their strengths. The theoretical analysis also veriï¬es the favor of the joint optimization\nmechanism. Extensive experiments on benchmark datasets show that the proposed method yields the best\nperformance in comparison with the state-of-the-art unsupervised domain adaptation methods.\n1\nIntroduction\nDeep Neural Networks (DNNs) have achieved a great success on many tasks such as image classiï¬cation\nwhen a large set of labeled examples are available [1â€“4]. However, in many real-world applications, there\nare plentiful unlabeled data but very limited labeled data; and the acquisition of labels is costly, or even\ninfeasible. Unsupervised domain adaptation is a popular way to address this issue. It aims at transferring\na well-performing model learned from a source domain to a different but related target domain when the\nlabeled data from the target domain is not available [5].\nMost efforts on unsupervised domain adaptation devote to reducing the domain discrepancy, such that a\nwell-trained classiï¬er in the source domain can be applied to the target domain [6â€“10]. However, these\nmethods only align the distributions in the domain-level, and fail to consider the class-level relations among\nâˆ—Work done while author was an intern at JD AI Research.\narXiv:1906.04053v1  [cs.LG]  10 Jun 2019\nSource\nTarget \n(a) Semantic Domain Alignment (b) Target Classifier Learning\nClass A\nClass B\nPush\nPull\nPull\n(c) Our Method\nFeature Space\nLabel Space\nPush\nLabel Space\nPush\nPush\nPull\nFeature Space\nPush\nPull\nFigure 1: Comparisons of semantic domain alignment methods, target classiï¬er learning methods and our\nproposed method. Note that we jointly optimize semantic domain alignment and target classiï¬er learning in\nthe feature space.\nthe source and target samples. For example, a car in the target domain may be mistakenly aligned to a bike in\nthe source domain. To alleviate the class-level misalignment, semantic domain alignment methods [11â€“15]\nthat enforce the samples from the same class to be close across domains are proposed. However, these domain\nalignment methods neglect the structures in target domain itself. Target classiï¬er learning methods [16, 17]\nlearn target discriminative features by distinguishing the samples in the target domain directly. Nonetheless,\nthey may miss some important supervised information in the source domain. Intuitively, a straightforward\nmethod is to optimize semantic domain alignment and target classiï¬er learning jointly. The joint optimization\nmechanism can not only eliminate their weaknesses, but also complement their strengths. The semantic\ndomain alignment methods enforce the intra-class compactness, distinguishing different samples from the\ntarget domain. The target classiï¬er learning methods enforce the inter-class discrepancy in the target domain,\nwhich in turn help to align the same class samples between two domains. However, as shown in Figure 1 (a)\nand (b), semantic domain alignment works in the feature space while target classiï¬er learning works in the\nlabel space. Thus, optimizing them together is not a trivial task.\nIn this paper, we propose a novel unsupervised domain adaption method that jointly optimizes semantic\ndomain alignment and target classiï¬er learning in a holistic way. The proposed method is called SDA-TCL,\nwhich is short for Semantic Domain Alignment and Target Classiï¬er Learning. Figure 1 (c) illustrats its basic\nidea. We utilize class centers in the feature space as the bridge to jointly optimize semantic domain-invariant\nfeatures and target discriminative features both in the feature space. For target classiï¬er learning, we design\nthe discriminative center loss to learn discriminative features directly by pulling the samples toward their\ncorresponding centers according to their pseudo-labels and pushing them away from the other centers. For\nsemantic domain alignment, we share the class centers between the same classes across domains to pull the\nsamples from the same class together. The main contributions of this paper are as follows:\nâ€¢ To the best of our knowledge, this is the ï¬rst work trying to understand the relationship between\nsemantic domain alignment and target classiï¬er learning.\nâ€¢ We propose a novel method called Semantic Domain Alignment and Target Classiï¬er Learning (SDA-\nTCL), which can jointly optimize semantic domain alignment and target classiï¬er learning in a holistic\nway.\nâ€¢ We show both theoretically and empirically that the proposed joint optimization mechanism is highly\neffective.\n2\n2\nRelated Work\nIn this paper, we focus on the problem of deep unsupervised domain adaptation for image classiï¬cation, and\nmany works along this line of research have been proposed [10, 16, 18â€“20].\nThese works can be roughly divided into the following two categories: The ï¬rst one is to align distributions\nbetween the source and the target domain. Its main idea is to reduce the discrepancy between two domains\nsuch that a classiï¬er learned from the source domain may be directly applied to the target domain. Under this\nmotivation, multiple methods have been used to align the distributions of two domains, such as maximum\nmean discrepancy (MMD) [6, 7, 18], CORrelation ALignment (CORAL) [8, 21], attention [22], and optimal\ntransport [23]. Besides, adversarial learning is also used to learn domain-invariant features [9, 10, 20, 24]. On\npar with these methods aligning distributions in the feature space, some methods align distributions in raw\npixel space by translating source data to the target domain with Image to Image translation techniques [25â€“30].\nIn addition to domain-level distribution alignment, the class-level information in target data is also frequently\nused to align class-level distributions [11â€“15, 31]. Compared with these methods, our method not only aligns\nclass-level distributions, but also learns target discriminative features.\nThe second one is to capture target-speciï¬c structures by constructing a reconstruction network [32, 33],\nadjusting the distances between target samples and decision boundaries [34â€“36], seeking for density-based\nseparations or clusters [37â€“42] and learning target classiï¬ers directly [16, 17]. Compared with these methods,\nour method not only learns target classiï¬ers but also aligns class-level distributions, thus is more desirable.\n3\nMethodology\nIn unsupervised domain adaptation, we have a labeled source data set Ds = {(xs\ni, ys\ni )|i = 1, 2, . . . , N s} and\na unlabeled target data set Dt = {(xt\ni)|i = 1, 2, . . . , N t}. Suppose the source data have C classes, which is\nshared with the target data. Our goal is to learn a model from the data set Ds âˆªDt to classify the samples in\nDt. Assume that each class in source (target) data has its corresponding source (target) class center cs\nj (ct\nj)\n(j âˆˆC = {1, 2, . . . , C}) to represent it in the feature space. In our method, the target sample xt\ni is classiï¬ed\naccording to its closest target center in the feature space. A generator network G (parametrized by Î¸G) is\nutilized to generate the features, denoted by G(xs\ni) for the source sample xs\ni and G(xt\ni) for the target sample\nxt\ni.\nWe aim to jointly optimize semantic domain-invariant features and target discriminative features in the feature\nspace. As illustrated in Figure 2, our loss function consists of three parts: 1) Ls(Î¸G): It learns discriminative\nfeatures for source domain by pulling the source sample toward its corresponding source center according to\nits label and pushing it away from the other source centers. 2) Lt(Î¸G): It learns discriminative features for\ntarget domain by pulling the target sample toward its corresponding target center according to its pseudo-label\nand pushing it away from the other target centers. 3) Lc(Î¸G) : It aligns class-level distributions by pulling the\nsource center and the target center from the same class. We jointly optimize them:\nLG(Î¸G) = Ls(Î¸G) + Î»tLt(Î¸G) + Î»cLc(Î¸G) + Î»dLd(Î¸G),\n(1)\nwhere Î»d, Î»t and Î»c are the balance parameters and Ld(Î¸G) is used to align domain-level distributions for\nproviding a initial classiï¬er to label the pseudo-labels following the previous methods [11, 14].\n3\nğ¿ğ‘ (ğœƒğº)\nğ¿ğ‘(ğœƒğº)\nğ¿ğ‘¡(ğœƒğº)\nSource\nTarget \nClass A\nClass B\nThe center of class A \nThe center of class B \nPush\nPull\nFigure 2: Illustration our proposed method SDA-TCL. We jointly optimize semantic domain alignment and\ntarget classiï¬er learning in the feature space by optimizing Ls(Î¸G), Lt(Î¸G) and Lc(Î¸G).\n3.1\nLearning Source Discriminative Features\nWe aim to pull the source sample toward its corresponding source center and push it away from the other\nsource centers. Here, we design discriminative center loss, which requires that the distances between samples\nand centers from the same class are smaller than a margin Î± and the distances between samples and centers\nfrom different classes are larger than a margin Î². The discriminative center loss be formulated as:\nLs(Î¸G) =\nN s\nX\ni=1\n\u0010\n[d(G(xs\ni), cs\nys\ni ) âˆ’Î±]+ + [Î² âˆ’d(G(xs\ni), cs\neys\ni )]+\n\u0011\n,\n(2)\nwhere d(G(xs\ni), cs\nj) denotes the squared Euclidean distance between sample xs\ni and center cs\nj, and\neys\ni = arg min\njâˆˆC,jÌ¸=ys\ni\nd(G(xs\ni), cs\nj)\n(3)\ndenotes the closest negative center for sample xs\ni in source centers, and [a]+ denotes the rectiï¬er function\nwhich is equal to max(0, a).\nNote that we do not utilize softmax loss for classiï¬cation but design the discriminative center loss. The\ndiscriminative center loss has two advantages compared with softmax loss: 1) The discriminative center\nloss enforces the intra-class compactness, which is helpful to pull ambiguous features away from the class\nboundaries [34, 35]; and 2) The discriminative center loss distinguishes the samples in the feature space\ndirectly, which makes it work in the same space with the class-level alignment.\n3.2\nLearning Target Discriminative Features\nFor the target domain, we aim to learn discriminative features directly in the feature space like source domain.\nHere, we optimize Lt(Î¸G) by utilizing the designed discriminative center loss to pull the target sample toward\nits corresponding target center according to its pseudo-label and push it away from the other target centers,\nwhich can be formulated as:\nLt(Î¸G) =\nNt\nX\ni=1\nwi\n\u0010\n[d(G(xt\ni), ct\nË†yt\ni) âˆ’Î±]+ + [Î² âˆ’d(G(xt\ni), ct\neyt\ni)]+\n\u0011\n,\n(4)\n4\nwhere Ë†yt\ni denotes the pseudo-label for sample xt\ni, eyt\ni denotes the closest negative target center for sample xt\ni\nand wi is the sample weight\nwi =\nd(G(xt\ni), ceyt\ni)\nd(G(xt\ni), cË†yt\ni) âˆ’1.\n(5)\nThen we scale wi to [0, 1] within the same class. A target sample closer to its center than other centers will\nget a big wi, which means the center is more conï¬dent on this sample. Target pseudo-labels are widely used\nin the unsupervised domain adaptation methods [11, 16, 17, 40], while the time to involve pseudo-labels\nhas never been analyzed by these previous methods. Involving pseudo-labels from scratch may bring some\nmistakes by the random pseudo-labels and involving pseudo-labels by a well-learned classiï¬er in the source\ndomain may bring some conï¬dent mistakes, which are hard to be corrected. We utilize pseudo-labels after a\nrelative small iteration parameter Is and increase the importance of pseudo-labels by a ramp-up curve (details\nin Section 5.2).\n3.3\nLearning Semantic Domain-Invariant Features\nTo align class-level distributions, the distances in the feature space between the target samples and the source\nsamples from the same class should be small. Constraining the distances between samples directly may bring\nsome noise because of the inaccurate pseudo-labels [11], we alter to optimize the distances between the\nsource center and target center from the same class. A straightforward method for optimizing Lc(Î¸G) can be\nformulated as:\nLc(Î¸G) =\nC\nX\nj=1\n\r\rcs\nj âˆ’ct\nj\n\r\r\n2 ,\n(6)\nConsidering the parameter Î»c in Eq. 1 needs to be tuned, we here utilize another method, which makes the\nclass centers are shared between the source domain and target domain, to optimize Lc(Î¸G). This means that\nwe set\ncs\nj = ct\nj\n(7)\nfor j âˆˆC = {1, 2, . . . , C} and we do not need to calculate Lc(Î¸G) in Eq. 1. We utilize Cs = {cs\nj} to denote\nthe shared class center set.\nTo align domain-level distributions, we adopt the Reverse Gradient (RevGrad) algorithm [9] to construct\na discriminator network D. The discriminator D classiï¬es whether the feature comes from the source\nor the target domain, and the generator G devotes to fooling D, enforcing the generator G to generate\ndomain-invariant features. The discriminator D is optimized by the standard classiï¬cation loss:\nLd(Î¸D) = âˆ’\nNs\nX\ni=0\nlog(D(G(xs\ni))) âˆ’\nN t\nX\ni=0\nlog(1 âˆ’D(G(xt\ni))),\n(8)\nwhile the generator G is optimized to minimize the domain-invariant loss:\nLd(Î¸G) = âˆ’Ld(Î¸D).\n(9)\n3.4\nThe Complete SDA-TCL Algorithm\nWe present the complete procedure of SDA-TCL in Algorithm 1. We optimize the generator G and class\ncenters {cs\nj} by Eq. 1 and the discriminator D by Eq. 8 on each mini-batch. As we can see, our objective\n5\nAlgorithm 1 SDA-TCL\nInput: Labeled source set Ds, unlabeled target set Dt, total iteration M, and the frequency to update target pseudo-labels\nk\nOutput: The prediction of target data Ë†yt\ni\n1: Initialization:\n2: Randomly initializing the shared center set Cs, generator G and discriminator D.\n3: Randomly initializing target label set {Ë†yt\ni}, target sample weight set {wi}.\n4: Training:\n5: for m = 1 â†’M do\n6:\nGenerate training mini-batch Bs\nm and Bt\nm.\n7:\nif (t mod k) == 0 then\n8:\nUpdate Ë†yt\ni and wi for xt\ni âˆˆDt by Cs and G\n9:\nTrain discriminator D with mini-batch Bs\nm and Bt\nm by minimizing Eq. 8\n10:\nTrain generator G and the shared center set Cs with mini-batch Bs\nm and Bt\nm by minimizing Eq. 1.\n11: Inference:\n12: Predicting Ë†yt\ni by generator G and center set Cs\nloss can be computed in linear time. We update the pseudo-labels and weights for every k iterations for\ncomputational efï¬ciency and we ï¬x k = 15 for all experiments.\n4\nTheoretical Analysis\nFollowing [43], we theoretically analyze SDA-TCL. The following Lemma shows that the upper bound of the\nexpected error on the target samples ÏµT (h) is decided by three terms:\nLemma 1. Let H be the hypothesis space. Given the source domain S and target domain T , we have\nâˆ€h âˆˆH, ÏµT (h) â‰¤ÏµS(h) + 1\n2dHâˆ†H(S, T ) + C,\n(10)\nwhere the ï¬rst term ÏµS(h) denotes the expected error on the source samples, the second term 1\n2dHâˆ†H(S, T )\nis the Hâˆ†H-distance which denotes the divergence between source and target domain, and the third term C\nis the excepted error of the ideal joint hypothesis.\nIn our method, the ï¬rst term can be minimized easily with the source labels. Furthermore, the second term is\nalso expected to be small by optimizing the domain-invariant features between S and T . The third term is\ntreated as a negligibly small term and is usually disregarded by previous methods [7, 9, 20]. However, a large\nC may hurt the performance on the target domain [43]. We will show that our method optimizes the upper\nbound for C.\nTheorem 1. Let fS and fT are the labeling functions for domain S and domain T respectively. f Ë†T denotes\nthe pseudo target labeling function in our method, we have\nC â‰¤min\nhâˆˆH ÏµS(h, f Ë†T ) + ÏµT (h, f Ë†T ) + ÏµS(fS, f Ë†T ) + ÏµT (f Ë†T , fT ).\n(11)\nProof. The excepted error of the ideal joint hypothesis C is deï¬ned as:\nC = min\nhâˆˆH ÏµS(h, fS) + ÏµT (h, fT ).\n(12)\n6\nFollowing the triangle inequality for classiï¬cation error [44, 45], that is, for any labeling functions f1, f2 and\nf3, we have Ïµ(f1, f2) â‰¤Ïµ(f1, f3) + Ïµ(f2, f3), we could have\nC = min\nhâˆˆH ÏµS(h, fS) + ÏµT (h, fT )\n(13)\nâ‰¤min\nhâˆˆH ÏµS(h, fS) + ÏµT (h, f Ë†T ) + ÏµT (f Ë†T , fT )\nâ‰¤min\nhâˆˆH ÏµS(h, f Ë†T ) + ÏµT (h, f Ë†T ) + ÏµS(fS, f Ë†T ) + ÏµT (f Ë†T , fT ).\nÏµS(h, f Ë†T ) + ÏµT (h, f Ë†T ) denotes the disagreement between h and the pseudo target labeling function f Ë†T\nand is minimized by target classiï¬er learning loss Lt(Î¸G) in Eq. 1. ÏµS(fS, f Ë†T ) denotes the disagreement\nbetween the source labeling function fS and the pseudo target labeling function f Ë†T on source samples and is\nminimized by semantic domain alignment loss Lc(Î¸G) in Eq. 1. Speciï¬cally, we align class-level distributions\nby sharing class centers between two domains, so a source sample with class k should be predicted as class k\nby the pseudo target labeling function f Ë†T . Consequently, ÏµS(fS, f Ë†T ) is expected to be small. ÏµT (f Ë†T , fT )\ndenotes the false pseudo-labels ratio, which is assumed to be decreased as the training moves on [11, 16].\nThus, our method SDA-TCL aims to minimize all the four terms in Theorem 1, while the existing methods\nneglected the target classiï¬er learning term or the semantic domain alignment term [11, 14, 16, 17].\n5\nExperiments\nWe evaluate the performance of our method on three benchmark unsupervised domain adaptation datasets\nacross different domain shifts by classiï¬cation accuracy metric.\n5.1\nDatasets and Baselines\nOfï¬ce-31 Dataset [46]. Ofï¬ce-31 dataset contains 4110 images of 31 different categories, which are everyday\nofï¬ce objects. The images belong to three imbalanced distinct domains: (i) Amazon website (A domain, 2817\nimages), (ii) Web camera (W domain, 795 images), (iii) Digital SLR camera (D domain, 498 images). We\nconduct experiments on the above six transfer tasks.\nDigits Datasets [47, 48]. The Digits datasets include USPS [47] (U domain) and MNIST [48] (M domain).\nFor the tasks Uâ†’M and Mâ†’U, we conduct the experiments on two experimental settings: 1) using all the\ntraining data in MNIST and USPS during training [20, 26]; 2) sampling 2,000 training samples from MNIST\nand 1,800 training samples from USPS for training [10].\nVisDA Dataset [49]. VisDA dataset evaluates adaptation from synthetic-object to real-object images\n(Syntheticâ†’Real). To date, this dataset represents the largest dataset for cross-domain object classiï¬-\ncation, with 12 categories for three domains. In the experiments, we regard the training domain as the source\ndomain and the validation domain as the target domain following the setting in [20, 24].\nBaseline Methods. We compared our proposed SDA-TCL with state-of-the-art methods: (I) Domain-\nlevel distribution alignment methods: Gradient Reversal (RevGrad) [9], Similarity Learning (SimNet) [20];\n(II) Class-level distribution alignment methods: Transferable Prototypical Networks (TPN) [12], Domain-\nInvariant Adversarial Learning (DIAL) [14], Similarity Constrained Alignment (SCA) [15]; (III) Aligning\ndistributions on pixel-level methods: Coupled Generative Adversarial Network (CoGAN) [25], Cycle-\nConsistent Adversarial Domain Adaptation (CyCADA) [29], Generate To Adapt (GTA) [30]; (IV) Utilizing\npseudo-labels implicitly methods: Maximum Classiï¬er Discrepancy (MCD) [35]; (V)Learning target classiï¬er\n7\nmethods: Incremental Collaborative and Adversarial Network (iCAN) [17]; (V)State-of-the-art Methods:\nJoint Adaptation Network (JAN) [18], Deep adversarial Attention Alignment (DAAA) [22], Self-Ensembling\n(S-En) [38], Conditional Domain Adversarial Networks (CDAN+E) [24]. With the same protocol, we cite the\nresults from the papers respectively following the previous methods [11, 16]. For a better comparison, we\nreport our implementation of the RevGrad [9] method, which is denoted as RevGrad-ours. We also compare\nour methods with the Source-only setting, where we train the model by only utilizing the source data.\n5.2\nImplementation Detail\nNetwork Architectures. For Digits datasets, we construct the generator network G by utilizing three\nconvolution layers and a fully-connected layer as the embedding layer following [35]. For the Ofï¬ce-31 and\nVisDA dataset, we utilize the ResNet-50 [4] network pre-trained on ImageNet [50] with an embedding layer\nto represent the generator G. The discriminator D is a fully-connected network with two hidden layers of\n1024 units followed by the domain classiï¬er.\nParameters. We use Adam [51] to optimize class centers, the generator G and discriminator D. The learning\nrate are set as 1.0 Ã— 10âˆ’4 for the networks and 1.0 Ã— 10âˆ’2 for the class centers respectively. We divide the\nlearning rate by 10 when optimizing the pre-trained layers. We set the batch size to 32 for each domain and\nthe embedding size to 512. For the margin parameters, following [52, 53], we use the recommended value by\nsetting Î± = 0.2 and Î² = 1.2. For the balance parameters, we set Î»d =\n2\n1+exp(âˆ’10Â·p) âˆ’1 following [19] to\nsuppress noisy signal from the discriminator at the early iterations of training, where p is training progress\nchanging from 0 to 1. We set Î»t = K Ã— Î»d to focus more on the target pseudo-labels as the training process\ngoes on. We choose the parameter K = 5 and the time Is = 200 for involving pseudo-labels via reverse\ncross-validation [19] on the task D â†’A and ï¬x them for the experiments. We run all experiments with\nPyTorch on a Tesla V100 GPU. We repeat each experiment 5 times and report mean accuracy and standard\ndeviation.\n5.3\nResults\nThe results of SDA-TCL on the Ofï¬ce-31, Digits and VisDA datasets are shown in Table 1 and Table 2.\nCompared with the Source-only setting, SDA-TCL improves the performance by utilizing the unlabeled target\ndata in all transfer tasks. It improves the average absolute accuracy by 23.0% in digits experiments, 12.2% in\nOfï¬ce-31 experiments and 22.3% in VisDA experiments.\nCompared with the semantic domain alignment methods (TPN [12], DIAL [14], SCA [15]) and the target\nclassiï¬er learning methods (iCAN [17]), our method outperforms them in most transfer tasks by jointly\noptimizing semantic domain alignment and target classiï¬er learning in the feature space. Compared with\nstate-of-the-art methods, SDA-TCL achieves better or comparable performance in all transfer tasks. Please\nnote that S+En[38] averaged predictions of 16 differently augmentations versions of each image to achieve\nthe accuracy 82.8% on VisDA dataset while SDA-TCL achieves the accuracy 81.9% by making only one\nprediction for each image following the most methods. It is desirable that SDA-TCL outperforms other\nmethods by a large margin in hard tasks, e.g., Wâ†’A and Dâ†’A. Note that we do not tune parameters for\nevery dataset and our results can be improved further by choosing parameters carefully, which is shown in\nSection 5.5.\n8\nTable 1: Accuracy (%) for the Ofï¬ce-31 dataset.\nMethod\nAâ†’W\nDâ†’W\nWâ†’D\nAâ†’D\nDâ†’A\nWâ†’A\nAverage\nRevGrad [9]\n82.0Â±0.4\n96.9Â±0.2\n99.1Â±0.1\n79.7Â±0.4\n68.2Â±0.4\n67.4Â±0.5\n82.2\nJAN [18]\n85.4Â±0.3\n97.4Â±0.2\n99.8Â±0.2\n84.7Â±0.3\n68.6Â±0.3\n70.0Â±0.4\n84.3\nGTA [30]\n89.5Â±0.5\n97.9Â±0.3\n99.8Â±0.4\n87.7Â±0.5\n72.8Â±0.3\n71.4Â±0.4\n86.5\nDAAA [22]\n86.8Â±0.2\n99.3Â±0.1\n100.0Â±0.0\n88.8Â±0.4\n74.3Â±0.2\n73.9Â±0.2\n87.2\nDIAL [14]\n91.7Â±0.4\n97.1Â±0.3\n99.8Â±0.0\n89.3Â±0.4\n71.7Â±0.7\n71.4Â±0.2\n86.8\niCAN [17]\n92.5\n98.8\n100.0\n90.1\n72.1\n69.9\n87.2\nSCA [15]\n93.5\n97.5\n100.0\n89.5\n72.4\n72.7\n87.6\nCDAN+E [24]\n94.1Â±0.1\n98.6Â±0.1\n100.0Â±0.0\n92.9Â±0.2\n71.0Â±0.3\n69.3Â±0.3\n87.7\nSource-only\n72.3Â±0.8\n96.5Â±0.7\n99.1Â±0.5\n80.7Â±0.5\n59.7Â±1.2\n59.7Â±1.5\n78.0\nRevGrad-ours\n83.5Â±0.5\n96.8Â±0.2\n99.2Â±0.5\n83.2Â±0.4\n67.6Â±0.4\n65.8Â±0.6\n82.7\nSDA-TCL\n92.4Â±0.7\n99.1Â±0.1\n100.0Â±0.0\n93.2Â±1.2\n79.0Â±0.3\n77.6Â±1.0\n90.2\n5.4\nAblation Study\nOur method is not a straightforward combination of semantic domain alignment methods and target classiï¬er\nlearning methods. Existing methods [16, 17] utilize two different losses to learn the target discriminative\nfeatures (softmax loss) and semantic domain-invariant features (center alignment loss [11, 13, 14]). Instead,\nWe design the discriminative center loss and share the class centers to carry out the joint optimization\nmechanism in the same space. Here, We implement the origin SDA and TCL with softmax loss, denoted\nas SDA-origin and TCL-origin respectively. We also implement a linear combination of these two origin\nmethods, denoted as Linear-Combination. For a better comparison, We further conduct experiments on our\nmethod without semantic domain alignment (TCL-ours) and without target classiï¬er learning (SDA-ours),\nrespectively. The results are shown in Table 3.\nThere are several interesting observations: (1) SDA-ours and TCL-ours often show different superiority\non different tasks, which means they beneï¬t from the target pseudo-labels from different aspects. As a\nresult, the joint optimization SDA-TCL shows better results than only optimizing one of them. (2) When\ncomparing TCL-ours and TCL-origin, TCL-ours outperforms TCL-origin in the transfer tasks, which may\nbeneï¬t from the features optimized by discriminative center loss having intra-class compactness. When\ncomparing SDA-ours and SDA-origin, SDA-ours shows better results than SDA-origin, which may be owed\nthat the features in SDA-ours are optimized in the same space. These observations, which are consistent with\nthe analysis in Section 3.1, show the effectiveness of discriminative center loss. (3) The Linear-Combination\ndoes not show any advantages while our method SDA-TCL can highlight it. Because Linear-Combination\noptimizes the features in separate space and it is more sensitive to the weight balance parameters compared\nwith our holistic method SDA-TCL in the experiments.\n5.5\nEmpirical Understanding\nThe time to involve pseudo-labels. We utilize the parameter Is to control the time to involve the target\npseudo-labels in Section 3.2 and we conduct experiments by choosing Is from {0, 200, 500, 1000, 1500}\non task Aâ†’W and Wâ†’A. The results shown in Figure 3(a) indicate that there is a trade-off for the time to\ninvolve target pseudo-labels and a relative small iteration could be a good choice, which is consistent with the\n9\nTable 2: Accuracy (%) for the Digit datasets and VisDA dataset. âˆ—means the setting that utilizes all the\ntraining data. â€  indicates that this method uses multiple data augmentations.\nMethod\nUâ†’M\nMâ†’U\nUâˆ—â†’Mâˆ—\nMâˆ—â†’Uâˆ—\nMethod\nSyntheticâ†’Real\nCoGAN [25]\n89.1Â±0.8\n91.2Â±0.8\n93.2\n95.7\nJAN [18]\n61.6\nCyCADA [29]\n-\n-\n96.5Â±0.1\n95.6Â±0.2\nGTA [30]\n69.5\nDIAL [14]\n97.3Â±0.3\n95.0Â±0.2\n99.1Â±0.1\n97.1Â±0.2\nSimNet [20]\n69.6\nMCD [35]\n94.1Â±0.3\n94.2Â±0.7\n-\n96.5Â±0.3\nMCD [35]\n71.9\nCDAN+E [24]\n-\n-\n98.0\n95.6\nCDAN+E [24]\n70.0\nTPN [12]\n94.1\n92.1\n-\n-\nTPN [12]\n80.4\nSource-only\n71.9Â±2.3\n78.1Â±3.5\n70.5Â±1.9\n80.3Â±1.7\nSource-only\n59.6Â±0.2\nSDA-TCL\n97.6Â±0.2\n97.6Â±0.4\n99.0Â±0.1\n98.9Â±0.1\nSDA-TCL\n81.9Â±0.3\nS-En [38]\n-\n-\n98.1Â±2.8\n98.3Â±0.1\nS-En [38]\n74.2\nS-Enâ€  [38]\n-\n-\n99.5Â±0.0\n98.2Â±0.1\nS-Enâ€  [38]\n82.8\nTable 3: Accuracy (%) for the Ofï¬ce-31 dataset and VisDA dataset under different settings.\nMethod\nAâ†’W\nAâ†’D\nDâ†’A\nWâ†’A\nAverage\nSyntheticâ†’Real\nTCL-origin\n89.6Â±0.8\n86.9Â±0.7\n72.3Â±0.4\n68.7Â±0.6\n79.4\n70.8Â±0.5\nSDA-origin\n89.2Â±0.7\n88.3Â±1.0\n72.9Â±0.5\n70.5Â±0.6\n80.2\n68.4Â±0.5\nLinear-Combination\n89.4Â±0.9\n87.2Â±0.6\n73.3Â±0.5\n71.5Â±0.5\n80.4\n70.4Â±0.6\nTCL-ours\n90.0Â±1.7\n92.2Â±2.3\n77.7Â±0.6\n77.1Â±1.8\n84.3\n81.5Â±0.8\nSDA-ours\n92.8Â±0.8\n92.7Â±1.2\n77.6Â±0.5\n76.9Â±0.9\n85.0\n79.8Â±0.8\nSDA-TCL\n92.4Â±0.7\n93.2Â±1.2\n79.0Â±0.3\n77.6Â±1.0\n85.6\n81.9Â±0.3\nanalysis in Section 3.2.\nParameter Sensitivity. In our method SDA-TCL, we use the parameter K to decide Î»t that controls the\nimportance of utilizing the target pseudo-labels. We conduct experiments to evaluate SDA-TCL by choosing\nK in the range of {1,3,5,7,9} on task Aâ†’W and Wâ†’A. From the results shown in Figure 3(b), we can ï¬nd\nthat SDA-TCL can achieve good performance with a wide range of K.\nDistribution Discrepancy. The A-distance is deï¬ned as distA = 2(1 âˆ’2Ïµ) to measure the distribution\ndiscrepancy [43, 54], where Ïµ denotes the test error of a classiï¬er trained to discriminate the source from\ntarget. A smaller distA means a smaller domain gap. Figure 3(c) shows distA on task Aâ†’W with features\nof ResNet, RevGrad, SDA-ours, TCL-ours and SDA-TCL. The results indicate that SDA-TCL can reduce\nthe domain gap more effectively. With class-level distribution alignment, SDA-ours and SDA-TCL have a\nsmaller distA than RevGrad. TCL-ours also has a smaller distA than RevGrad, which indicates that TCL-ours\nis helpful for the domain alignment.\nConvergence. We demonstrate the convergence of ResNet, RevGrad, and SDA-TCL, with the error rates in\nthe target domain on task Aâ†’W shown in Figure 3(d). SDA-TCL has faster convergence than RevGrad and\nthe convergence process is more stable than RevGrad.\n10\n(a) Parameter Is\n(b) Parameter K\n(c) Discrepancy\n(d) Convergence\nFigure 3: Analysis of parameter Is, parameter K, distribution discrepancy, and convergence.\n6\nConclusion\nIn this paper, we proposed a novel method for unsupervised domain adaptation by jointly optimizing semantic\ndomain alignment and target classiï¬er learning in the feature space. The joint optimization mechanism can\nnot only eliminate their weaknesses but also complement their strengths. Experiments on several benchmarks\ndemonstrate that our method surpasses state-of-the-art unsupervised domain adaptation methods. Recently,\nlearnware is deï¬ned to be facilitated with model reusability [55]. The use of a learned model to another task,\nhowever, is not trivial. There have been some efforts towards this direction [56â€“59], whereas the approach\npresented in this paper offers another possibility.\nReferences\n[1]\nA. Krizhevsky, I. Sutskever, and G. E. Hinton, â€œImagenet classiï¬cation with deep convolutional neural\nnetworks,â€ in NIPS, 2012.\n[2]\nK. Simonyan and A. Zisserman, â€œVery deep convolutional networks for large-scale image recognition,â€\nCoRR, vol. abs/1409.1556, 2014.\n[3]\nC. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A.\nRabinovich, â€œGoing deeper with convolutions,â€ in CVPR, 2015.\n[4]\nK. He, X. Zhang, S. Ren, and J. Sun, â€œDeep residual learning for image recognition,â€ in CVPR, 2016.\n[5]\nG. Csurka, â€œDomain adaptation for visual applications: A comprehensive survey,â€ CoRR, vol. abs/1702.05374,\n2017.\n[6]\nE. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell, â€œDeep domain confusion: Maximizing for\ndomain invariance,â€ CoRR, vol. abs/1412.3474, 2014.\n[7]\nM. Long, Y. Cao, J. Wang, and M. I. Jordan, â€œLearning transferable features with deep adaptation\nnetworks,â€ in ICML, 2015.\n[8]\nB. Sun and K. Saenko, â€œDeep CORAL: correlation alignment for deep domain adaptation,â€ in ECCV,\n2016.\n[9]\nY. Ganin and V. S. Lempitsky, â€œUnsupervised domain adaptation by backpropagation,â€ in ICML, 2015.\n[10]\nE. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, â€œAdversarial discriminative domain adaptation,â€ in\nCVPR, 2017.\n11\n[11]\nS. Xie, Z. Zheng, L. Chen, and C. Chen, â€œLearning semantic representations for unsupervised domain\nadaptation,â€ in ICML, 2018.\n[12]\nY. Pan, T. Yao, Y. Li, Y. Wang, C.-W. Ngo, and T. Mei, â€œTransferrable prototypical networks for\nunsupervised domain adaptation,â€ CoRR, vol. abs/1904.11227, 2019.\n[13]\nC. Chen, W. Xie, T. Xu, W. Huang, Y. Rong, X. Ding, Y. Huang, and J. Huang, â€œProgressive feature\nalignment for unsupervised domain adaptation,â€ CoRR, vol. abs/1811.08585, 2018.\n[14]\nY. Zhang, Y. Zhang, Y. Wang, and Q. Tian, â€œDomain-invariant adversarial learning for unsupervised\ndomain adaption,â€ CoRR, vol. abs/1811.12751, 2018.\n[15]\nW. Deng, L. Zheng, and J. Jiao, â€œDomain alignment with triplets,â€ CoRR, vol. abs/1812.00893, 2018.\n[16]\nK. Saito, Y. Ushiku, and T. Harada, â€œAsymmetric tri-training for unsupervised domain adaptation,â€ in\nICML, 2017.\n[17]\nW. Zhang, W. Ouyang, W. Li, and D. Xu, â€œCollaborative and adversarial network for unsupervised\ndomain adaptation,â€ in CVPR, 2018.\n[18]\nM. Long, H. Zhu, J. Wang, and M. I. Jordan, â€œDeep transfer learning with joint adaptation networks,â€\nin ICML, 2017.\n[19]\nY. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and V. S.\nLempitsky, â€œDomain-adversarial training of neural networks,â€ Journal of Machine Learning Research,\nvol. 17, 59:1â€“59:35, 2016.\n[20]\nP. O. Pinheiro, â€œUnsupervised domain adaptation with similarity learning,â€ in CVPR, 2018.\n[21]\nC. Chen, Z. Chen, B. Jiang, and X. Jin, â€œJoint domain alignment and discriminative feature learning\nfor unsupervised deep domain adaptation,â€ CoRR, vol. abs/1808.09347, 2018.\n[22]\nG. Kang, L. Zheng, Y. Yan, and Y. Yang, â€œDeep adversarial attention alignment for unsupervised\ndomain adaptation: The beneï¬t of target expectation maximization,â€ in ECCV, 2018.\n[23]\nB. B. Damodaran, B. Kellenberger, R. Flamary, D. Tuia, and N. Courty, â€œDeepjdot: Deep joint\ndistribution optimal transport for unsupervised domain adaptation,â€ in ECCV, 2018.\n[24]\nM. Long, Z. Cao, J. Wang, and M. I. Jordan, â€œConditional adversarial domain adaptation,â€ in NeurIPS,\n2018.\n[25]\nM.-Y. Liu and O. Tuzel, â€œCoupled generative adversarial networks,â€ in NIPS, 2016.\n[26]\nK. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Krishnan, â€œUnsupervised pixel-level domain\nadaptation with generative adversarial networks,â€ in CVPR, 2017.\n[27]\nJ. Zhu, T. Park, P. Isola, and A. A. Efros, â€œUnpaired image-to-image translation using cycle-consistent\nadversarial networks,â€ in ICCV, 2017.\n[28]\nM.-Y. Liu, T. Breuel, and J. Kautz, â€œUnsupervised image-to-image translation networks,â€ in NIPS,\n2017.\n[29]\nJ. Hoffman, E. Tzeng, T. Park, J. Zhu, P. Isola, K. Saenko, A. A. Efros, and T. Darrell, â€œCycada:\nCycle-consistent adversarial domain adaptation,â€ in ICML, 2018.\n[30]\nS. Sankaranarayanan, Y. Balaji, C. D. Castillo, and R. Chellappa, â€œGenerate to adapt: Aligning domains\nusing generative adversarial networks,â€ in CVPR, 2018.\n[31]\nP. Haeusser, T. Frerix, A. Mordvintsev, and D. Cremers, â€œAssociative domain adaptation,â€ in ICCV,\n2017.\n[32]\nM. Ghifary, W. B. Kleijn, M. Zhang, D. Balduzzi, and W. Li, â€œDeep reconstruction-classiï¬cation\nnetworks for unsupervised domain adaptation,â€ in ECCV, 2016.\n12\n[33]\nK. Bousmalis, G. Trigeorgis, N. Silberman, D. Krishnan, and D. Erhan, â€œDomain separation networks,â€\nin NIPS, 2016.\n[34]\nK. Saito, Y. Ushiku, T. Harada, and K. Saenko, â€œAdversarial dropout regularization,â€ CoRR, vol.\nabs/1711.01575, 2017.\n[35]\nK. Saito, K. Watanabe, Y. Ushiku, and T. Harada, â€œMaximum classiï¬er discrepancy for unsupervised\ndomain adaptation,â€ in CVPR, 2018.\n[36]\nA. Kumar, P. Sattigeri, K. Wadhawan, L. Karlinsky, R. S. Feris, B. Freeman, and G. W. Wornell,\nâ€œCo-regularized alignment for unsupervised domain adaptation,â€ in NeurIPS, 2018.\n[37]\nM. Long, H. Zhu, J. Wang, and M. I. Jordan, â€œUnsupervised domain adaptation with residual transfer\nnetworks,â€ in NIPS, 2016.\n[38]\nG. French, M. Mackiewicz, and M. Fisher, â€œSelf-ensembling for visual domain adaptation,â€ in ICLR,\n2018.\n[39]\nR. Shu, H. H. Bui, H. Narui, and S. Ermon, â€œA dirt-t approach to unsupervised domain adaptation,â€ in\nICLR, 2018.\n[40]\nO. Sener, H. O. Song, A. Saxena, and S. Savarese, â€œLearning transferrable representations for unsuper-\nvised domain adaptation,â€ in NIPS, 2016.\n[41]\nI. H. Laradji and R. Babanezhad, â€œM-ADDA: unsupervised domain adaptation with deep metric\nlearning,â€ CoRR, vol. abs/1807.02552, 2018.\n[42]\nJ. Liang, R. He, Z. Sun, and T. Tan, â€œAggregating randomized clustering-promoting invariant projec-\ntions for domain adaptation,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, vol.\n41, no. 5, pp. 1027â€“1042, 2019.\n[43]\nS. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. W. Vaughan, â€œA theory of learning\nfrom different domains,â€ Machine Learning, vol. 79, no. 1-2, pp. 151â€“175, 2010.\n[44]\nS. Ben-David, J. Blitzer, K. Crammer, and F. Pereira, â€œAnalysis of representations for domain adapta-\ntion,â€ in NIPS, 2006.\n[45]\nK. Crammer, M. J. Kearns, and J. Wortman, â€œLearning from multiple sources,â€ Journal of Machine\nLearning Research, vol. 9, pp. 1757â€“1774, 2008.\n[46]\nK. Saenko, B. Kulis, M. Fritz, and T. Darrell, â€œAdapting visual category models to new domains,â€ in\nECCV, 2010.\n[47]\nJ. J. Hull, â€œA database for handwritten text recognition research,â€ IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. 16, no. 5, pp. 550â€“554, 1994.\n[48]\nY. LeCun, L. Bottou, Y. Bengio, and P. Haffner, â€œGradient-based learning applied to document\nrecognition,â€ Proceedings of the IEEE, vol. 86, no. 11, pp. 2278â€“2324, 1998.\n[49]\nX. Peng, B. Usman, N. Kaushik, J. Hoffman, D. Wang, and K. Saenko, â€œVisda: The visual domain\nadaptation challenge,â€ CoRR, vol. abs/1710.06924, 2017.\n[50]\nO. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. S.\nBernstein, A. C. Berg, and F.-F. Li, â€œImagenet large scale visual recognition challenge,â€ International\nJournal of Computer Vision, vol. 115, no. 3, pp. 211â€“252, 2015.\n[51]\nD. P. Kingma and J. Ba, â€œAdam: A method for stochastic optimization,â€ CoRR, vol. abs/1412.6980,\n2014.\n[52]\nF. Schroff, D. Kalenichenko, and J. Philbin, â€œFacenet: A uniï¬ed embedding for face recognition and\nclustering,â€ in CVPR, 2015.\n13\n[53]\nR. Manmatha, C. Wu, A. J. Smola, and P. KrÃ¤henbÃ¼hl, â€œSampling matters in deep embedding learning,â€\nin ICCV, 2017.\n[54]\nY. Mansour, M. Mohri, and A. Rostamizadeh, â€œDomain adaptation: Learning bounds and algorithms,â€\nin COLT, 2009.\n[55]\nZ.-H. Zhou, â€œLearnware: On the future of machine learning,â€ Frontiers of Computer Science, vol. 10,\nno. 4, pp. 589â€“590, 2016.\n[56]\nH.-J. Ye, D.-C. Zhan, Y. Jiang, and Z.-H. Zhou, â€œRectify heterogeneous models with semantic mapping,â€\nin ICML, 2018.\n[57]\nZ. Shen and M. Li, â€œT2S: domain adaptation via model-independent inverse mapping and model reuse,â€\nin ICDM, 2018.\n[58]\nY. Yang, D.-C. Zhan, Y. Fan, Y. Jiang, and Z.-H. Zhou, â€œDeep learning for ï¬xed model reuse,â€ in AAAI,\n2017.\n[59]\nY.-Q. Hu, Y. Yu, and Z.-H. Zhou, â€œExperienced optimization with reusable directional model for\nhyper-parameter search,â€ in IJCAI, 2018.\n14\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-06-10",
  "updated": "2019-06-10"
}