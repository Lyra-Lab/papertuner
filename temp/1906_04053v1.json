{
  "id": "http://arxiv.org/abs/1906.04053v1",
  "title": "Joint Semantic Domain Alignment and Target Classifier Learning for Unsupervised Domain Adaptation",
  "authors": [
    "Dong-Dong Chen",
    "Yisen Wang",
    "Jinfeng Yi",
    "Zaiyi Chen",
    "Zhi-Hua Zhou"
  ],
  "abstract": "Unsupervised domain adaptation aims to transfer the classifier learned from\nthe source domain to the target domain in an unsupervised manner. With the help\nof target pseudo-labels, aligning class-level distributions and learning the\nclassifier in the target domain are two widely used objectives. Existing\nmethods often separately optimize these two individual objectives, which makes\nthem suffer from the neglect of the other. However, optimizing these two\naspects together is not trivial. To alleviate the above issues, we propose a\nnovel method that jointly optimizes semantic domain alignment and target\nclassifier learning in a holistic way. The joint optimization mechanism can not\nonly eliminate their weaknesses but also complement their strengths. The\ntheoretical analysis also verifies the favor of the joint optimization\nmechanism. Extensive experiments on benchmark datasets show that the proposed\nmethod yields the best performance in comparison with the state-of-the-art\nunsupervised domain adaptation methods.",
  "text": "Joint Semantic Domain Alignment and\nTarget Classiﬁer Learning for\nUnsupervised Domain Adaptation\nDong-Dong Chen1,2,∗, Yisen Wang2, Jinfeng Yi2, Zaiyi Chen3, Zhi-Hua Zhou1\n1National Key Laboratory for Novel Software Technology, Nanjing University\n2JD AI Research\n3School of Computer Science, University of Science and Technology of China\n{chendd, zhouzh}@lamda.nju.edu.cn, eewangyisen@gmail.com, yijinfeng@jd.com, chenzaiyi@mail.ustc.edu.cn\nAbstract\nUnsupervised domain adaptation aims to transfer the classiﬁer learned from the source domain to\nthe target domain in an unsupervised manner. With the help of target pseudo-labels, aligning class-level\ndistributions and learning the classiﬁer in the target domain are two widely used objectives. Existing\nmethods often separately optimize these two individual objectives, which makes them suffer from the\nneglect of the other. However, optimizing these two aspects together is not trivial. To alleviate the above\nissues, we propose a novel method that jointly optimizes semantic domain alignment and target classiﬁer\nlearning in a holistic way. The joint optimization mechanism can not only eliminate their weaknesses but\nalso complement their strengths. The theoretical analysis also veriﬁes the favor of the joint optimization\nmechanism. Extensive experiments on benchmark datasets show that the proposed method yields the best\nperformance in comparison with the state-of-the-art unsupervised domain adaptation methods.\n1\nIntroduction\nDeep Neural Networks (DNNs) have achieved a great success on many tasks such as image classiﬁcation\nwhen a large set of labeled examples are available [1–4]. However, in many real-world applications, there\nare plentiful unlabeled data but very limited labeled data; and the acquisition of labels is costly, or even\ninfeasible. Unsupervised domain adaptation is a popular way to address this issue. It aims at transferring\na well-performing model learned from a source domain to a different but related target domain when the\nlabeled data from the target domain is not available [5].\nMost efforts on unsupervised domain adaptation devote to reducing the domain discrepancy, such that a\nwell-trained classiﬁer in the source domain can be applied to the target domain [6–10]. However, these\nmethods only align the distributions in the domain-level, and fail to consider the class-level relations among\n∗Work done while author was an intern at JD AI Research.\narXiv:1906.04053v1  [cs.LG]  10 Jun 2019\nSource\nTarget \n(a) Semantic Domain Alignment (b) Target Classifier Learning\nClass A\nClass B\nPush\nPull\nPull\n(c) Our Method\nFeature Space\nLabel Space\nPush\nLabel Space\nPush\nPush\nPull\nFeature Space\nPush\nPull\nFigure 1: Comparisons of semantic domain alignment methods, target classiﬁer learning methods and our\nproposed method. Note that we jointly optimize semantic domain alignment and target classiﬁer learning in\nthe feature space.\nthe source and target samples. For example, a car in the target domain may be mistakenly aligned to a bike in\nthe source domain. To alleviate the class-level misalignment, semantic domain alignment methods [11–15]\nthat enforce the samples from the same class to be close across domains are proposed. However, these domain\nalignment methods neglect the structures in target domain itself. Target classiﬁer learning methods [16, 17]\nlearn target discriminative features by distinguishing the samples in the target domain directly. Nonetheless,\nthey may miss some important supervised information in the source domain. Intuitively, a straightforward\nmethod is to optimize semantic domain alignment and target classiﬁer learning jointly. The joint optimization\nmechanism can not only eliminate their weaknesses, but also complement their strengths. The semantic\ndomain alignment methods enforce the intra-class compactness, distinguishing different samples from the\ntarget domain. The target classiﬁer learning methods enforce the inter-class discrepancy in the target domain,\nwhich in turn help to align the same class samples between two domains. However, as shown in Figure 1 (a)\nand (b), semantic domain alignment works in the feature space while target classiﬁer learning works in the\nlabel space. Thus, optimizing them together is not a trivial task.\nIn this paper, we propose a novel unsupervised domain adaption method that jointly optimizes semantic\ndomain alignment and target classiﬁer learning in a holistic way. The proposed method is called SDA-TCL,\nwhich is short for Semantic Domain Alignment and Target Classiﬁer Learning. Figure 1 (c) illustrats its basic\nidea. We utilize class centers in the feature space as the bridge to jointly optimize semantic domain-invariant\nfeatures and target discriminative features both in the feature space. For target classiﬁer learning, we design\nthe discriminative center loss to learn discriminative features directly by pulling the samples toward their\ncorresponding centers according to their pseudo-labels and pushing them away from the other centers. For\nsemantic domain alignment, we share the class centers between the same classes across domains to pull the\nsamples from the same class together. The main contributions of this paper are as follows:\n• To the best of our knowledge, this is the ﬁrst work trying to understand the relationship between\nsemantic domain alignment and target classiﬁer learning.\n• We propose a novel method called Semantic Domain Alignment and Target Classiﬁer Learning (SDA-\nTCL), which can jointly optimize semantic domain alignment and target classiﬁer learning in a holistic\nway.\n• We show both theoretically and empirically that the proposed joint optimization mechanism is highly\neffective.\n2\n2\nRelated Work\nIn this paper, we focus on the problem of deep unsupervised domain adaptation for image classiﬁcation, and\nmany works along this line of research have been proposed [10, 16, 18–20].\nThese works can be roughly divided into the following two categories: The ﬁrst one is to align distributions\nbetween the source and the target domain. Its main idea is to reduce the discrepancy between two domains\nsuch that a classiﬁer learned from the source domain may be directly applied to the target domain. Under this\nmotivation, multiple methods have been used to align the distributions of two domains, such as maximum\nmean discrepancy (MMD) [6, 7, 18], CORrelation ALignment (CORAL) [8, 21], attention [22], and optimal\ntransport [23]. Besides, adversarial learning is also used to learn domain-invariant features [9, 10, 20, 24]. On\npar with these methods aligning distributions in the feature space, some methods align distributions in raw\npixel space by translating source data to the target domain with Image to Image translation techniques [25–30].\nIn addition to domain-level distribution alignment, the class-level information in target data is also frequently\nused to align class-level distributions [11–15, 31]. Compared with these methods, our method not only aligns\nclass-level distributions, but also learns target discriminative features.\nThe second one is to capture target-speciﬁc structures by constructing a reconstruction network [32, 33],\nadjusting the distances between target samples and decision boundaries [34–36], seeking for density-based\nseparations or clusters [37–42] and learning target classiﬁers directly [16, 17]. Compared with these methods,\nour method not only learns target classiﬁers but also aligns class-level distributions, thus is more desirable.\n3\nMethodology\nIn unsupervised domain adaptation, we have a labeled source data set Ds = {(xs\ni, ys\ni )|i = 1, 2, . . . , N s} and\na unlabeled target data set Dt = {(xt\ni)|i = 1, 2, . . . , N t}. Suppose the source data have C classes, which is\nshared with the target data. Our goal is to learn a model from the data set Ds ∪Dt to classify the samples in\nDt. Assume that each class in source (target) data has its corresponding source (target) class center cs\nj (ct\nj)\n(j ∈C = {1, 2, . . . , C}) to represent it in the feature space. In our method, the target sample xt\ni is classiﬁed\naccording to its closest target center in the feature space. A generator network G (parametrized by θG) is\nutilized to generate the features, denoted by G(xs\ni) for the source sample xs\ni and G(xt\ni) for the target sample\nxt\ni.\nWe aim to jointly optimize semantic domain-invariant features and target discriminative features in the feature\nspace. As illustrated in Figure 2, our loss function consists of three parts: 1) Ls(θG): It learns discriminative\nfeatures for source domain by pulling the source sample toward its corresponding source center according to\nits label and pushing it away from the other source centers. 2) Lt(θG): It learns discriminative features for\ntarget domain by pulling the target sample toward its corresponding target center according to its pseudo-label\nand pushing it away from the other target centers. 3) Lc(θG) : It aligns class-level distributions by pulling the\nsource center and the target center from the same class. We jointly optimize them:\nLG(θG) = Ls(θG) + λtLt(θG) + λcLc(θG) + λdLd(θG),\n(1)\nwhere λd, λt and λc are the balance parameters and Ld(θG) is used to align domain-level distributions for\nproviding a initial classiﬁer to label the pseudo-labels following the previous methods [11, 14].\n3\n𝐿𝑠(𝜃𝐺)\n𝐿𝑐(𝜃𝐺)\n𝐿𝑡(𝜃𝐺)\nSource\nTarget \nClass A\nClass B\nThe center of class A \nThe center of class B \nPush\nPull\nFigure 2: Illustration our proposed method SDA-TCL. We jointly optimize semantic domain alignment and\ntarget classiﬁer learning in the feature space by optimizing Ls(θG), Lt(θG) and Lc(θG).\n3.1\nLearning Source Discriminative Features\nWe aim to pull the source sample toward its corresponding source center and push it away from the other\nsource centers. Here, we design discriminative center loss, which requires that the distances between samples\nand centers from the same class are smaller than a margin α and the distances between samples and centers\nfrom different classes are larger than a margin β. The discriminative center loss be formulated as:\nLs(θG) =\nN s\nX\ni=1\n\u0010\n[d(G(xs\ni), cs\nys\ni ) −α]+ + [β −d(G(xs\ni), cs\neys\ni )]+\n\u0011\n,\n(2)\nwhere d(G(xs\ni), cs\nj) denotes the squared Euclidean distance between sample xs\ni and center cs\nj, and\neys\ni = arg min\nj∈C,j̸=ys\ni\nd(G(xs\ni), cs\nj)\n(3)\ndenotes the closest negative center for sample xs\ni in source centers, and [a]+ denotes the rectiﬁer function\nwhich is equal to max(0, a).\nNote that we do not utilize softmax loss for classiﬁcation but design the discriminative center loss. The\ndiscriminative center loss has two advantages compared with softmax loss: 1) The discriminative center\nloss enforces the intra-class compactness, which is helpful to pull ambiguous features away from the class\nboundaries [34, 35]; and 2) The discriminative center loss distinguishes the samples in the feature space\ndirectly, which makes it work in the same space with the class-level alignment.\n3.2\nLearning Target Discriminative Features\nFor the target domain, we aim to learn discriminative features directly in the feature space like source domain.\nHere, we optimize Lt(θG) by utilizing the designed discriminative center loss to pull the target sample toward\nits corresponding target center according to its pseudo-label and push it away from the other target centers,\nwhich can be formulated as:\nLt(θG) =\nNt\nX\ni=1\nwi\n\u0010\n[d(G(xt\ni), ct\nˆyt\ni) −α]+ + [β −d(G(xt\ni), ct\neyt\ni)]+\n\u0011\n,\n(4)\n4\nwhere ˆyt\ni denotes the pseudo-label for sample xt\ni, eyt\ni denotes the closest negative target center for sample xt\ni\nand wi is the sample weight\nwi =\nd(G(xt\ni), ceyt\ni)\nd(G(xt\ni), cˆyt\ni) −1.\n(5)\nThen we scale wi to [0, 1] within the same class. A target sample closer to its center than other centers will\nget a big wi, which means the center is more conﬁdent on this sample. Target pseudo-labels are widely used\nin the unsupervised domain adaptation methods [11, 16, 17, 40], while the time to involve pseudo-labels\nhas never been analyzed by these previous methods. Involving pseudo-labels from scratch may bring some\nmistakes by the random pseudo-labels and involving pseudo-labels by a well-learned classiﬁer in the source\ndomain may bring some conﬁdent mistakes, which are hard to be corrected. We utilize pseudo-labels after a\nrelative small iteration parameter Is and increase the importance of pseudo-labels by a ramp-up curve (details\nin Section 5.2).\n3.3\nLearning Semantic Domain-Invariant Features\nTo align class-level distributions, the distances in the feature space between the target samples and the source\nsamples from the same class should be small. Constraining the distances between samples directly may bring\nsome noise because of the inaccurate pseudo-labels [11], we alter to optimize the distances between the\nsource center and target center from the same class. A straightforward method for optimizing Lc(θG) can be\nformulated as:\nLc(θG) =\nC\nX\nj=1\n\r\rcs\nj −ct\nj\n\r\r\n2 ,\n(6)\nConsidering the parameter λc in Eq. 1 needs to be tuned, we here utilize another method, which makes the\nclass centers are shared between the source domain and target domain, to optimize Lc(θG). This means that\nwe set\ncs\nj = ct\nj\n(7)\nfor j ∈C = {1, 2, . . . , C} and we do not need to calculate Lc(θG) in Eq. 1. We utilize Cs = {cs\nj} to denote\nthe shared class center set.\nTo align domain-level distributions, we adopt the Reverse Gradient (RevGrad) algorithm [9] to construct\na discriminator network D. The discriminator D classiﬁes whether the feature comes from the source\nor the target domain, and the generator G devotes to fooling D, enforcing the generator G to generate\ndomain-invariant features. The discriminator D is optimized by the standard classiﬁcation loss:\nLd(θD) = −\nNs\nX\ni=0\nlog(D(G(xs\ni))) −\nN t\nX\ni=0\nlog(1 −D(G(xt\ni))),\n(8)\nwhile the generator G is optimized to minimize the domain-invariant loss:\nLd(θG) = −Ld(θD).\n(9)\n3.4\nThe Complete SDA-TCL Algorithm\nWe present the complete procedure of SDA-TCL in Algorithm 1. We optimize the generator G and class\ncenters {cs\nj} by Eq. 1 and the discriminator D by Eq. 8 on each mini-batch. As we can see, our objective\n5\nAlgorithm 1 SDA-TCL\nInput: Labeled source set Ds, unlabeled target set Dt, total iteration M, and the frequency to update target pseudo-labels\nk\nOutput: The prediction of target data ˆyt\ni\n1: Initialization:\n2: Randomly initializing the shared center set Cs, generator G and discriminator D.\n3: Randomly initializing target label set {ˆyt\ni}, target sample weight set {wi}.\n4: Training:\n5: for m = 1 →M do\n6:\nGenerate training mini-batch Bs\nm and Bt\nm.\n7:\nif (t mod k) == 0 then\n8:\nUpdate ˆyt\ni and wi for xt\ni ∈Dt by Cs and G\n9:\nTrain discriminator D with mini-batch Bs\nm and Bt\nm by minimizing Eq. 8\n10:\nTrain generator G and the shared center set Cs with mini-batch Bs\nm and Bt\nm by minimizing Eq. 1.\n11: Inference:\n12: Predicting ˆyt\ni by generator G and center set Cs\nloss can be computed in linear time. We update the pseudo-labels and weights for every k iterations for\ncomputational efﬁciency and we ﬁx k = 15 for all experiments.\n4\nTheoretical Analysis\nFollowing [43], we theoretically analyze SDA-TCL. The following Lemma shows that the upper bound of the\nexpected error on the target samples ϵT (h) is decided by three terms:\nLemma 1. Let H be the hypothesis space. Given the source domain S and target domain T , we have\n∀h ∈H, ϵT (h) ≤ϵS(h) + 1\n2dH∆H(S, T ) + C,\n(10)\nwhere the ﬁrst term ϵS(h) denotes the expected error on the source samples, the second term 1\n2dH∆H(S, T )\nis the H∆H-distance which denotes the divergence between source and target domain, and the third term C\nis the excepted error of the ideal joint hypothesis.\nIn our method, the ﬁrst term can be minimized easily with the source labels. Furthermore, the second term is\nalso expected to be small by optimizing the domain-invariant features between S and T . The third term is\ntreated as a negligibly small term and is usually disregarded by previous methods [7, 9, 20]. However, a large\nC may hurt the performance on the target domain [43]. We will show that our method optimizes the upper\nbound for C.\nTheorem 1. Let fS and fT are the labeling functions for domain S and domain T respectively. f ˆT denotes\nthe pseudo target labeling function in our method, we have\nC ≤min\nh∈H ϵS(h, f ˆT ) + ϵT (h, f ˆT ) + ϵS(fS, f ˆT ) + ϵT (f ˆT , fT ).\n(11)\nProof. The excepted error of the ideal joint hypothesis C is deﬁned as:\nC = min\nh∈H ϵS(h, fS) + ϵT (h, fT ).\n(12)\n6\nFollowing the triangle inequality for classiﬁcation error [44, 45], that is, for any labeling functions f1, f2 and\nf3, we have ϵ(f1, f2) ≤ϵ(f1, f3) + ϵ(f2, f3), we could have\nC = min\nh∈H ϵS(h, fS) + ϵT (h, fT )\n(13)\n≤min\nh∈H ϵS(h, fS) + ϵT (h, f ˆT ) + ϵT (f ˆT , fT )\n≤min\nh∈H ϵS(h, f ˆT ) + ϵT (h, f ˆT ) + ϵS(fS, f ˆT ) + ϵT (f ˆT , fT ).\nϵS(h, f ˆT ) + ϵT (h, f ˆT ) denotes the disagreement between h and the pseudo target labeling function f ˆT\nand is minimized by target classiﬁer learning loss Lt(θG) in Eq. 1. ϵS(fS, f ˆT ) denotes the disagreement\nbetween the source labeling function fS and the pseudo target labeling function f ˆT on source samples and is\nminimized by semantic domain alignment loss Lc(θG) in Eq. 1. Speciﬁcally, we align class-level distributions\nby sharing class centers between two domains, so a source sample with class k should be predicted as class k\nby the pseudo target labeling function f ˆT . Consequently, ϵS(fS, f ˆT ) is expected to be small. ϵT (f ˆT , fT )\ndenotes the false pseudo-labels ratio, which is assumed to be decreased as the training moves on [11, 16].\nThus, our method SDA-TCL aims to minimize all the four terms in Theorem 1, while the existing methods\nneglected the target classiﬁer learning term or the semantic domain alignment term [11, 14, 16, 17].\n5\nExperiments\nWe evaluate the performance of our method on three benchmark unsupervised domain adaptation datasets\nacross different domain shifts by classiﬁcation accuracy metric.\n5.1\nDatasets and Baselines\nOfﬁce-31 Dataset [46]. Ofﬁce-31 dataset contains 4110 images of 31 different categories, which are everyday\nofﬁce objects. The images belong to three imbalanced distinct domains: (i) Amazon website (A domain, 2817\nimages), (ii) Web camera (W domain, 795 images), (iii) Digital SLR camera (D domain, 498 images). We\nconduct experiments on the above six transfer tasks.\nDigits Datasets [47, 48]. The Digits datasets include USPS [47] (U domain) and MNIST [48] (M domain).\nFor the tasks U→M and M→U, we conduct the experiments on two experimental settings: 1) using all the\ntraining data in MNIST and USPS during training [20, 26]; 2) sampling 2,000 training samples from MNIST\nand 1,800 training samples from USPS for training [10].\nVisDA Dataset [49]. VisDA dataset evaluates adaptation from synthetic-object to real-object images\n(Synthetic→Real). To date, this dataset represents the largest dataset for cross-domain object classiﬁ-\ncation, with 12 categories for three domains. In the experiments, we regard the training domain as the source\ndomain and the validation domain as the target domain following the setting in [20, 24].\nBaseline Methods. We compared our proposed SDA-TCL with state-of-the-art methods: (I) Domain-\nlevel distribution alignment methods: Gradient Reversal (RevGrad) [9], Similarity Learning (SimNet) [20];\n(II) Class-level distribution alignment methods: Transferable Prototypical Networks (TPN) [12], Domain-\nInvariant Adversarial Learning (DIAL) [14], Similarity Constrained Alignment (SCA) [15]; (III) Aligning\ndistributions on pixel-level methods: Coupled Generative Adversarial Network (CoGAN) [25], Cycle-\nConsistent Adversarial Domain Adaptation (CyCADA) [29], Generate To Adapt (GTA) [30]; (IV) Utilizing\npseudo-labels implicitly methods: Maximum Classiﬁer Discrepancy (MCD) [35]; (V)Learning target classiﬁer\n7\nmethods: Incremental Collaborative and Adversarial Network (iCAN) [17]; (V)State-of-the-art Methods:\nJoint Adaptation Network (JAN) [18], Deep adversarial Attention Alignment (DAAA) [22], Self-Ensembling\n(S-En) [38], Conditional Domain Adversarial Networks (CDAN+E) [24]. With the same protocol, we cite the\nresults from the papers respectively following the previous methods [11, 16]. For a better comparison, we\nreport our implementation of the RevGrad [9] method, which is denoted as RevGrad-ours. We also compare\nour methods with the Source-only setting, where we train the model by only utilizing the source data.\n5.2\nImplementation Detail\nNetwork Architectures. For Digits datasets, we construct the generator network G by utilizing three\nconvolution layers and a fully-connected layer as the embedding layer following [35]. For the Ofﬁce-31 and\nVisDA dataset, we utilize the ResNet-50 [4] network pre-trained on ImageNet [50] with an embedding layer\nto represent the generator G. The discriminator D is a fully-connected network with two hidden layers of\n1024 units followed by the domain classiﬁer.\nParameters. We use Adam [51] to optimize class centers, the generator G and discriminator D. The learning\nrate are set as 1.0 × 10−4 for the networks and 1.0 × 10−2 for the class centers respectively. We divide the\nlearning rate by 10 when optimizing the pre-trained layers. We set the batch size to 32 for each domain and\nthe embedding size to 512. For the margin parameters, following [52, 53], we use the recommended value by\nsetting α = 0.2 and β = 1.2. For the balance parameters, we set λd =\n2\n1+exp(−10·p) −1 following [19] to\nsuppress noisy signal from the discriminator at the early iterations of training, where p is training progress\nchanging from 0 to 1. We set λt = K × λd to focus more on the target pseudo-labels as the training process\ngoes on. We choose the parameter K = 5 and the time Is = 200 for involving pseudo-labels via reverse\ncross-validation [19] on the task D →A and ﬁx them for the experiments. We run all experiments with\nPyTorch on a Tesla V100 GPU. We repeat each experiment 5 times and report mean accuracy and standard\ndeviation.\n5.3\nResults\nThe results of SDA-TCL on the Ofﬁce-31, Digits and VisDA datasets are shown in Table 1 and Table 2.\nCompared with the Source-only setting, SDA-TCL improves the performance by utilizing the unlabeled target\ndata in all transfer tasks. It improves the average absolute accuracy by 23.0% in digits experiments, 12.2% in\nOfﬁce-31 experiments and 22.3% in VisDA experiments.\nCompared with the semantic domain alignment methods (TPN [12], DIAL [14], SCA [15]) and the target\nclassiﬁer learning methods (iCAN [17]), our method outperforms them in most transfer tasks by jointly\noptimizing semantic domain alignment and target classiﬁer learning in the feature space. Compared with\nstate-of-the-art methods, SDA-TCL achieves better or comparable performance in all transfer tasks. Please\nnote that S+En[38] averaged predictions of 16 differently augmentations versions of each image to achieve\nthe accuracy 82.8% on VisDA dataset while SDA-TCL achieves the accuracy 81.9% by making only one\nprediction for each image following the most methods. It is desirable that SDA-TCL outperforms other\nmethods by a large margin in hard tasks, e.g., W→A and D→A. Note that we do not tune parameters for\nevery dataset and our results can be improved further by choosing parameters carefully, which is shown in\nSection 5.5.\n8\nTable 1: Accuracy (%) for the Ofﬁce-31 dataset.\nMethod\nA→W\nD→W\nW→D\nA→D\nD→A\nW→A\nAverage\nRevGrad [9]\n82.0±0.4\n96.9±0.2\n99.1±0.1\n79.7±0.4\n68.2±0.4\n67.4±0.5\n82.2\nJAN [18]\n85.4±0.3\n97.4±0.2\n99.8±0.2\n84.7±0.3\n68.6±0.3\n70.0±0.4\n84.3\nGTA [30]\n89.5±0.5\n97.9±0.3\n99.8±0.4\n87.7±0.5\n72.8±0.3\n71.4±0.4\n86.5\nDAAA [22]\n86.8±0.2\n99.3±0.1\n100.0±0.0\n88.8±0.4\n74.3±0.2\n73.9±0.2\n87.2\nDIAL [14]\n91.7±0.4\n97.1±0.3\n99.8±0.0\n89.3±0.4\n71.7±0.7\n71.4±0.2\n86.8\niCAN [17]\n92.5\n98.8\n100.0\n90.1\n72.1\n69.9\n87.2\nSCA [15]\n93.5\n97.5\n100.0\n89.5\n72.4\n72.7\n87.6\nCDAN+E [24]\n94.1±0.1\n98.6±0.1\n100.0±0.0\n92.9±0.2\n71.0±0.3\n69.3±0.3\n87.7\nSource-only\n72.3±0.8\n96.5±0.7\n99.1±0.5\n80.7±0.5\n59.7±1.2\n59.7±1.5\n78.0\nRevGrad-ours\n83.5±0.5\n96.8±0.2\n99.2±0.5\n83.2±0.4\n67.6±0.4\n65.8±0.6\n82.7\nSDA-TCL\n92.4±0.7\n99.1±0.1\n100.0±0.0\n93.2±1.2\n79.0±0.3\n77.6±1.0\n90.2\n5.4\nAblation Study\nOur method is not a straightforward combination of semantic domain alignment methods and target classiﬁer\nlearning methods. Existing methods [16, 17] utilize two different losses to learn the target discriminative\nfeatures (softmax loss) and semantic domain-invariant features (center alignment loss [11, 13, 14]). Instead,\nWe design the discriminative center loss and share the class centers to carry out the joint optimization\nmechanism in the same space. Here, We implement the origin SDA and TCL with softmax loss, denoted\nas SDA-origin and TCL-origin respectively. We also implement a linear combination of these two origin\nmethods, denoted as Linear-Combination. For a better comparison, We further conduct experiments on our\nmethod without semantic domain alignment (TCL-ours) and without target classiﬁer learning (SDA-ours),\nrespectively. The results are shown in Table 3.\nThere are several interesting observations: (1) SDA-ours and TCL-ours often show different superiority\non different tasks, which means they beneﬁt from the target pseudo-labels from different aspects. As a\nresult, the joint optimization SDA-TCL shows better results than only optimizing one of them. (2) When\ncomparing TCL-ours and TCL-origin, TCL-ours outperforms TCL-origin in the transfer tasks, which may\nbeneﬁt from the features optimized by discriminative center loss having intra-class compactness. When\ncomparing SDA-ours and SDA-origin, SDA-ours shows better results than SDA-origin, which may be owed\nthat the features in SDA-ours are optimized in the same space. These observations, which are consistent with\nthe analysis in Section 3.1, show the effectiveness of discriminative center loss. (3) The Linear-Combination\ndoes not show any advantages while our method SDA-TCL can highlight it. Because Linear-Combination\noptimizes the features in separate space and it is more sensitive to the weight balance parameters compared\nwith our holistic method SDA-TCL in the experiments.\n5.5\nEmpirical Understanding\nThe time to involve pseudo-labels. We utilize the parameter Is to control the time to involve the target\npseudo-labels in Section 3.2 and we conduct experiments by choosing Is from {0, 200, 500, 1000, 1500}\non task A→W and W→A. The results shown in Figure 3(a) indicate that there is a trade-off for the time to\ninvolve target pseudo-labels and a relative small iteration could be a good choice, which is consistent with the\n9\nTable 2: Accuracy (%) for the Digit datasets and VisDA dataset. ∗means the setting that utilizes all the\ntraining data. † indicates that this method uses multiple data augmentations.\nMethod\nU→M\nM→U\nU∗→M∗\nM∗→U∗\nMethod\nSynthetic→Real\nCoGAN [25]\n89.1±0.8\n91.2±0.8\n93.2\n95.7\nJAN [18]\n61.6\nCyCADA [29]\n-\n-\n96.5±0.1\n95.6±0.2\nGTA [30]\n69.5\nDIAL [14]\n97.3±0.3\n95.0±0.2\n99.1±0.1\n97.1±0.2\nSimNet [20]\n69.6\nMCD [35]\n94.1±0.3\n94.2±0.7\n-\n96.5±0.3\nMCD [35]\n71.9\nCDAN+E [24]\n-\n-\n98.0\n95.6\nCDAN+E [24]\n70.0\nTPN [12]\n94.1\n92.1\n-\n-\nTPN [12]\n80.4\nSource-only\n71.9±2.3\n78.1±3.5\n70.5±1.9\n80.3±1.7\nSource-only\n59.6±0.2\nSDA-TCL\n97.6±0.2\n97.6±0.4\n99.0±0.1\n98.9±0.1\nSDA-TCL\n81.9±0.3\nS-En [38]\n-\n-\n98.1±2.8\n98.3±0.1\nS-En [38]\n74.2\nS-En† [38]\n-\n-\n99.5±0.0\n98.2±0.1\nS-En† [38]\n82.8\nTable 3: Accuracy (%) for the Ofﬁce-31 dataset and VisDA dataset under different settings.\nMethod\nA→W\nA→D\nD→A\nW→A\nAverage\nSynthetic→Real\nTCL-origin\n89.6±0.8\n86.9±0.7\n72.3±0.4\n68.7±0.6\n79.4\n70.8±0.5\nSDA-origin\n89.2±0.7\n88.3±1.0\n72.9±0.5\n70.5±0.6\n80.2\n68.4±0.5\nLinear-Combination\n89.4±0.9\n87.2±0.6\n73.3±0.5\n71.5±0.5\n80.4\n70.4±0.6\nTCL-ours\n90.0±1.7\n92.2±2.3\n77.7±0.6\n77.1±1.8\n84.3\n81.5±0.8\nSDA-ours\n92.8±0.8\n92.7±1.2\n77.6±0.5\n76.9±0.9\n85.0\n79.8±0.8\nSDA-TCL\n92.4±0.7\n93.2±1.2\n79.0±0.3\n77.6±1.0\n85.6\n81.9±0.3\nanalysis in Section 3.2.\nParameter Sensitivity. In our method SDA-TCL, we use the parameter K to decide λt that controls the\nimportance of utilizing the target pseudo-labels. We conduct experiments to evaluate SDA-TCL by choosing\nK in the range of {1,3,5,7,9} on task A→W and W→A. From the results shown in Figure 3(b), we can ﬁnd\nthat SDA-TCL can achieve good performance with a wide range of K.\nDistribution Discrepancy. The A-distance is deﬁned as distA = 2(1 −2ϵ) to measure the distribution\ndiscrepancy [43, 54], where ϵ denotes the test error of a classiﬁer trained to discriminate the source from\ntarget. A smaller distA means a smaller domain gap. Figure 3(c) shows distA on task A→W with features\nof ResNet, RevGrad, SDA-ours, TCL-ours and SDA-TCL. The results indicate that SDA-TCL can reduce\nthe domain gap more effectively. With class-level distribution alignment, SDA-ours and SDA-TCL have a\nsmaller distA than RevGrad. TCL-ours also has a smaller distA than RevGrad, which indicates that TCL-ours\nis helpful for the domain alignment.\nConvergence. We demonstrate the convergence of ResNet, RevGrad, and SDA-TCL, with the error rates in\nthe target domain on task A→W shown in Figure 3(d). SDA-TCL has faster convergence than RevGrad and\nthe convergence process is more stable than RevGrad.\n10\n(a) Parameter Is\n(b) Parameter K\n(c) Discrepancy\n(d) Convergence\nFigure 3: Analysis of parameter Is, parameter K, distribution discrepancy, and convergence.\n6\nConclusion\nIn this paper, we proposed a novel method for unsupervised domain adaptation by jointly optimizing semantic\ndomain alignment and target classiﬁer learning in the feature space. The joint optimization mechanism can\nnot only eliminate their weaknesses but also complement their strengths. Experiments on several benchmarks\ndemonstrate that our method surpasses state-of-the-art unsupervised domain adaptation methods. Recently,\nlearnware is deﬁned to be facilitated with model reusability [55]. The use of a learned model to another task,\nhowever, is not trivial. There have been some efforts towards this direction [56–59], whereas the approach\npresented in this paper offers another possibility.\nReferences\n[1]\nA. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation with deep convolutional neural\nnetworks,” in NIPS, 2012.\n[2]\nK. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,”\nCoRR, vol. abs/1409.1556, 2014.\n[3]\nC. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A.\nRabinovich, “Going deeper with convolutions,” in CVPR, 2015.\n[4]\nK. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in CVPR, 2016.\n[5]\nG. Csurka, “Domain adaptation for visual applications: A comprehensive survey,” CoRR, vol. abs/1702.05374,\n2017.\n[6]\nE. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell, “Deep domain confusion: Maximizing for\ndomain invariance,” CoRR, vol. abs/1412.3474, 2014.\n[7]\nM. Long, Y. Cao, J. Wang, and M. I. Jordan, “Learning transferable features with deep adaptation\nnetworks,” in ICML, 2015.\n[8]\nB. Sun and K. Saenko, “Deep CORAL: correlation alignment for deep domain adaptation,” in ECCV,\n2016.\n[9]\nY. Ganin and V. S. Lempitsky, “Unsupervised domain adaptation by backpropagation,” in ICML, 2015.\n[10]\nE. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, “Adversarial discriminative domain adaptation,” in\nCVPR, 2017.\n11\n[11]\nS. Xie, Z. Zheng, L. Chen, and C. Chen, “Learning semantic representations for unsupervised domain\nadaptation,” in ICML, 2018.\n[12]\nY. Pan, T. Yao, Y. Li, Y. Wang, C.-W. Ngo, and T. Mei, “Transferrable prototypical networks for\nunsupervised domain adaptation,” CoRR, vol. abs/1904.11227, 2019.\n[13]\nC. Chen, W. Xie, T. Xu, W. Huang, Y. Rong, X. Ding, Y. Huang, and J. Huang, “Progressive feature\nalignment for unsupervised domain adaptation,” CoRR, vol. abs/1811.08585, 2018.\n[14]\nY. Zhang, Y. Zhang, Y. Wang, and Q. Tian, “Domain-invariant adversarial learning for unsupervised\ndomain adaption,” CoRR, vol. abs/1811.12751, 2018.\n[15]\nW. Deng, L. Zheng, and J. Jiao, “Domain alignment with triplets,” CoRR, vol. abs/1812.00893, 2018.\n[16]\nK. Saito, Y. Ushiku, and T. Harada, “Asymmetric tri-training for unsupervised domain adaptation,” in\nICML, 2017.\n[17]\nW. Zhang, W. Ouyang, W. Li, and D. Xu, “Collaborative and adversarial network for unsupervised\ndomain adaptation,” in CVPR, 2018.\n[18]\nM. Long, H. Zhu, J. Wang, and M. I. Jordan, “Deep transfer learning with joint adaptation networks,”\nin ICML, 2017.\n[19]\nY. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and V. S.\nLempitsky, “Domain-adversarial training of neural networks,” Journal of Machine Learning Research,\nvol. 17, 59:1–59:35, 2016.\n[20]\nP. O. Pinheiro, “Unsupervised domain adaptation with similarity learning,” in CVPR, 2018.\n[21]\nC. Chen, Z. Chen, B. Jiang, and X. Jin, “Joint domain alignment and discriminative feature learning\nfor unsupervised deep domain adaptation,” CoRR, vol. abs/1808.09347, 2018.\n[22]\nG. Kang, L. Zheng, Y. Yan, and Y. Yang, “Deep adversarial attention alignment for unsupervised\ndomain adaptation: The beneﬁt of target expectation maximization,” in ECCV, 2018.\n[23]\nB. B. Damodaran, B. Kellenberger, R. Flamary, D. Tuia, and N. Courty, “Deepjdot: Deep joint\ndistribution optimal transport for unsupervised domain adaptation,” in ECCV, 2018.\n[24]\nM. Long, Z. Cao, J. Wang, and M. I. Jordan, “Conditional adversarial domain adaptation,” in NeurIPS,\n2018.\n[25]\nM.-Y. Liu and O. Tuzel, “Coupled generative adversarial networks,” in NIPS, 2016.\n[26]\nK. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Krishnan, “Unsupervised pixel-level domain\nadaptation with generative adversarial networks,” in CVPR, 2017.\n[27]\nJ. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image translation using cycle-consistent\nadversarial networks,” in ICCV, 2017.\n[28]\nM.-Y. Liu, T. Breuel, and J. Kautz, “Unsupervised image-to-image translation networks,” in NIPS,\n2017.\n[29]\nJ. Hoffman, E. Tzeng, T. Park, J. Zhu, P. Isola, K. Saenko, A. A. Efros, and T. Darrell, “Cycada:\nCycle-consistent adversarial domain adaptation,” in ICML, 2018.\n[30]\nS. Sankaranarayanan, Y. Balaji, C. D. Castillo, and R. Chellappa, “Generate to adapt: Aligning domains\nusing generative adversarial networks,” in CVPR, 2018.\n[31]\nP. Haeusser, T. Frerix, A. Mordvintsev, and D. Cremers, “Associative domain adaptation,” in ICCV,\n2017.\n[32]\nM. Ghifary, W. B. Kleijn, M. Zhang, D. Balduzzi, and W. Li, “Deep reconstruction-classiﬁcation\nnetworks for unsupervised domain adaptation,” in ECCV, 2016.\n12\n[33]\nK. Bousmalis, G. Trigeorgis, N. Silberman, D. Krishnan, and D. Erhan, “Domain separation networks,”\nin NIPS, 2016.\n[34]\nK. Saito, Y. Ushiku, T. Harada, and K. Saenko, “Adversarial dropout regularization,” CoRR, vol.\nabs/1711.01575, 2017.\n[35]\nK. Saito, K. Watanabe, Y. Ushiku, and T. Harada, “Maximum classiﬁer discrepancy for unsupervised\ndomain adaptation,” in CVPR, 2018.\n[36]\nA. Kumar, P. Sattigeri, K. Wadhawan, L. Karlinsky, R. S. Feris, B. Freeman, and G. W. Wornell,\n“Co-regularized alignment for unsupervised domain adaptation,” in NeurIPS, 2018.\n[37]\nM. Long, H. Zhu, J. Wang, and M. I. Jordan, “Unsupervised domain adaptation with residual transfer\nnetworks,” in NIPS, 2016.\n[38]\nG. French, M. Mackiewicz, and M. Fisher, “Self-ensembling for visual domain adaptation,” in ICLR,\n2018.\n[39]\nR. Shu, H. H. Bui, H. Narui, and S. Ermon, “A dirt-t approach to unsupervised domain adaptation,” in\nICLR, 2018.\n[40]\nO. Sener, H. O. Song, A. Saxena, and S. Savarese, “Learning transferrable representations for unsuper-\nvised domain adaptation,” in NIPS, 2016.\n[41]\nI. H. Laradji and R. Babanezhad, “M-ADDA: unsupervised domain adaptation with deep metric\nlearning,” CoRR, vol. abs/1807.02552, 2018.\n[42]\nJ. Liang, R. He, Z. Sun, and T. Tan, “Aggregating randomized clustering-promoting invariant projec-\ntions for domain adaptation,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol.\n41, no. 5, pp. 1027–1042, 2019.\n[43]\nS. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. W. Vaughan, “A theory of learning\nfrom different domains,” Machine Learning, vol. 79, no. 1-2, pp. 151–175, 2010.\n[44]\nS. Ben-David, J. Blitzer, K. Crammer, and F. Pereira, “Analysis of representations for domain adapta-\ntion,” in NIPS, 2006.\n[45]\nK. Crammer, M. J. Kearns, and J. Wortman, “Learning from multiple sources,” Journal of Machine\nLearning Research, vol. 9, pp. 1757–1774, 2008.\n[46]\nK. Saenko, B. Kulis, M. Fritz, and T. Darrell, “Adapting visual category models to new domains,” in\nECCV, 2010.\n[47]\nJ. J. Hull, “A database for handwritten text recognition research,” IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. 16, no. 5, pp. 550–554, 1994.\n[48]\nY. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied to document\nrecognition,” Proceedings of the IEEE, vol. 86, no. 11, pp. 2278–2324, 1998.\n[49]\nX. Peng, B. Usman, N. Kaushik, J. Hoffman, D. Wang, and K. Saenko, “Visda: The visual domain\nadaptation challenge,” CoRR, vol. abs/1710.06924, 2017.\n[50]\nO. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. S.\nBernstein, A. C. Berg, and F.-F. Li, “Imagenet large scale visual recognition challenge,” International\nJournal of Computer Vision, vol. 115, no. 3, pp. 211–252, 2015.\n[51]\nD. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” CoRR, vol. abs/1412.6980,\n2014.\n[52]\nF. Schroff, D. Kalenichenko, and J. Philbin, “Facenet: A uniﬁed embedding for face recognition and\nclustering,” in CVPR, 2015.\n13\n[53]\nR. Manmatha, C. Wu, A. J. Smola, and P. Krähenbühl, “Sampling matters in deep embedding learning,”\nin ICCV, 2017.\n[54]\nY. Mansour, M. Mohri, and A. Rostamizadeh, “Domain adaptation: Learning bounds and algorithms,”\nin COLT, 2009.\n[55]\nZ.-H. Zhou, “Learnware: On the future of machine learning,” Frontiers of Computer Science, vol. 10,\nno. 4, pp. 589–590, 2016.\n[56]\nH.-J. Ye, D.-C. Zhan, Y. Jiang, and Z.-H. Zhou, “Rectify heterogeneous models with semantic mapping,”\nin ICML, 2018.\n[57]\nZ. Shen and M. Li, “T2S: domain adaptation via model-independent inverse mapping and model reuse,”\nin ICDM, 2018.\n[58]\nY. Yang, D.-C. Zhan, Y. Fan, Y. Jiang, and Z.-H. Zhou, “Deep learning for ﬁxed model reuse,” in AAAI,\n2017.\n[59]\nY.-Q. Hu, Y. Yu, and Z.-H. Zhou, “Experienced optimization with reusable directional model for\nhyper-parameter search,” in IJCAI, 2018.\n14\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-06-10",
  "updated": "2019-06-10"
}