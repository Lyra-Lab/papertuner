{
  "id": "http://arxiv.org/abs/1906.04068v1",
  "title": "Hierarchical Representation in Neural Language Models: Suppression and Recovery of Expectations",
  "authors": [
    "Ethan Wilcox",
    "Roger Levy",
    "Richard Futrell"
  ],
  "abstract": "Deep learning sequence models have led to a marked increase in performance\nfor a range of Natural Language Processing tasks, but it remains an open\nquestion whether they are able to induce proper hierarchical generalizations\nfor representing natural language from linear input alone. Work using\nartificial languages as training input has shown that LSTMs are capable of\ninducing the stack-like data structures required to represent context-free and\ncertain mildly context-sensitive languages---formal language classes which\ncorrespond in theory to the hierarchical structures of natural language. Here\nwe present a suite of experiments probing whether neural language models\ntrained on linguistic data induce these stack-like data structures and deploy\nthem while incrementally predicting words. We study two natural language\nphenomena: center embedding sentences and syntactic island constraints on the\nfiller--gap dependency. In order to properly predict words in these structures,\na model must be able to temporarily suppress certain expectations and then\nrecover those expectations later, essentially pushing and popping these\nexpectations on a stack. Our results provide evidence that models can\nsuccessfully suppress and recover expectations in many cases, but do not fully\nrecover their previous grammatical state.",
  "text": "Hierarchical Representation in Neural Language Models: Suppression\nand Recovery of Expectations\nEthan Wilcox1, Roger Levy2, and Richard Futrell3\n1Department of Linguistics, Harvard University, wilcoxeg@g.harvard.edu\n2Department of Brain and Cognitive Sciences, MIT, rplevy@mit.edu\n3Department of Language Science, UC Irvine, rfutrell@uci.edu\nAbstract\nDeep learning sequence models have led to a\nmarked increase in performance for a range of\nNatural Language Processing tasks, but it re-\nmains an open question whether they are able\nto induce proper hierarchical generalizations\nfor representing natural language from linear\ninput alone. Work using artiﬁcial languages as\ntraining input has shown that LSTMs are ca-\npable of inducing the stack-like data structures\nrequired to represent context-free and cer-\ntain mildly context-sensitive languages (Weiss\net al., 2018)—formal language classes which\ncorrespond in theory to the hierarchical struc-\ntures of natural language.\nHere we present\na suite of experiments probing whether neu-\nral language models trained on linguistic data\ninduce these stack-like data structures and\ndeploy them while incrementally predicting\nwords. We study two natural language phe-\nnomena: center embedding sentences and syn-\ntactic island constraints on the ﬁller–gap de-\npendency. In order to properly predict words\nin these structures, a model must be able to\ntemporarily suppress certain expectations and\nthen recover those expectations later, essen-\ntially pushing and popping these expectations\non a stack. Our results provide evidence that\nmodels can successfully suppress and recover\nexpectations in many cases, but do not fully\nrecover their previous grammatical state.\n1\nIntroduction\nDeep learning sequence models such as RNNs (El-\nman, 1990; Hochreiter and Schmidhuber, 1997)\nhave led to a marked increase in performance for a\nrange of Natural Language Processing tasks (Joze-\nfowicz et al., 2016; Dai et al., 2019), but it re-\nmains an open question whether they are able\nto induce hierarchical generalizations from lin-\near input alone. Answering this question is im-\nportant both for technical outcomes—models with\nexplicit hierarchical structure show performance\ngains, at least when training on relatively small\ndatasets (Choe and Charniak, 2016; Dyer et al.,\n2016; Kuncoro et al., 2016)—and for the sci-\nentiﬁc aim of understanding what biases, learn-\ning objectives and training regimes led to human-\nlike linguistic knowledge. Previous work has ap-\nproached this question by either examining mod-\nels’ internal state (Weiss et al., 2018; Mareˇcek and\nRosa, 2018) or by studying model behavior (El-\nman, 1991; Linzen et al., 2016; Futrell et al., 2019;\nMcCoy et al., 2018).\nFor this latter approach, much work has as-\nsessed sensitivity to hierarchy by examining\nwhether the expectations associated with long-\ndistance dependencies can be maintained even in\nthe presence of intervening distractor words (Gu-\nlordava et al., 2018; Marvin and Linzen, 2018).\nFor example, Linzen et al. (2016) fed RNNs with\nthe preﬁx The keys to the cabinet.... If models as-\nsigned higher probability to the grammatical con-\ntinuation are over the ungrammatical continuation\nis, they can be said to have learned the correct\nstructural relationship between the subject and the\nverb, ignoring the syntactically-irrelevant singular\ndistractor, the cabinet. Work in this paradigm has\nuncovered a complex pattern in terms of what spe-\nciﬁc hierarchical structures are and are not repre-\nsented by neural language models.\nAt the same time, work using artiﬁcial lan-\nguages as input has demonstrated that LSTMs are\ncapable of inducing the data structures required to\nproduce hierarchically-structured sequences. For\nexample, Weiss et al. (2018) showed that LSTMs\ncan learn to produce strings of the form anbn,\ncorresponding to context-free languages (Chom-\nsky, 1956), and anbncn, corresponding to mildly\ncontext-sensitive languages.\nProducing these\nstrings requires a stack-like data structure where\nsome number of as are pushed onto the stack so\nthat the same number of bs can be popped from\narXiv:1906.04068v1  [cs.CL]  10 Jun 2019\nS\nVP1\nglittered\nNP1\nCP\nS\nVP2\nstole\nNP2\nCP\n...\nthe thief\nthat\nThe diamond\nThe diamond\nthat thief\n...\nstole\nglittered.\n↑PUSH\n↑PUSH\n↑POP\n↑POP\nFigure 1: Anatomy of a center embedding sentence. At\neach point marked PUSH, comprehenders need to push\nthe expectations generated by the subject noun onto a\nstack-like data structure, and suppress those expecta-\ntions going forward. At the points marked POP, they\nmust recover those expectations.\nit. The hierarchical structures of natural language\nare widely believed to be mildly context-sensitive\n(Shieber, 1985; Weir, 1988; Seki et al., 1991; Joshi\nand Schabes, 1997; Kuhlmann, 2013), so this re-\nsult shows that LSTMs are practically capable of\ninducing the proper data structures to handle the\nhierarchical structure of natural language.\nWhat remains to be seen in a general way is\nthat LSTMs induce and use these structures when\ntrained on natural language input, rather than ar-\ntiﬁcial language input. In this work, we present\ntwo suites of experiments that probe for evidence\nof hierarchical generalizations using two linguis-\ntic structures: center embedding sentences and\nsyntactic island constraints on the ﬁller–gap de-\npendency. These structures exemplify context-free\nhierarchical structure in natural language. In or-\nder to correctly predict words in these structures,\na model must use something like a stack data\nstructure: certain expectations must be temporar-\nily suppressed (pushed onto a stack), then recov-\nered later at the right time and in the right order\n(popped from the stack in last-in-ﬁrst-out order),\nas shown in Figure 1.\nFor both of these contexts we assess how well\nRNNs can suppress local expectations within in-\ntervening blocking-structures and recover expec-\ntations on the far side.\nSuccess at these tasks\nwould provide evidence that models not only ig-\nnore intervening material, but modulate and re-\ncover local expectations based on relative location\nwithin a syntactic structure.\nCenter embeddings are sentences in which a\nclause is embedded within the center of another\nclause, such that the expectations based on the ex-\nternal clause must be temporarily suppressed dur-\ning the internal clause, and then recovered once\nthe internal clause is complete. Such sentences\nwere used as the original argument that natural\nlanguage is not a regular language, but rather at\nleast context-free (Chomsky, 1956). We ﬁnd that\nneural language models can successfully suppress\nand recover expectations in sentences with two-\nlayer embedding depth, but their accuracy depends\non the particular lexical items used.\nSyntactic Islands are structural conﬁgurations\nthat block the ﬁller–gap dependency, which is the\ndependency between a wh-word, such as who or\nwhat, and a gap, which is an empty syntactic posi-\ntion. Using controlled experimental material, we\nﬁnd that models are able to suppress expectations\nfor gaps inside two island constructions and par-\ntially recover them on the far side. However, the\nrecovered expectation is far weaker than in non-\nisland sentences and only robust in one of the\nmodels tested. Together, both experiments provide\nnew evidence that RNN language models can ap-\nproximate a soft notion of hierarchy to drive pre-\ndictions, suppressing local expectations in some\ncontexts and reactivating them based on relative\nsyntactic position.\nOverall our results show that the LSTMs tested\nhave learned an approximate stack-like data struc-\nture to predict natural language, but the deploy-\nment of this structure depends on the particular\nlexical items used, and the recovery of expecta-\ntions is often imperfect, especially for structures\nrequiring deep stacks.\n2\nExperimental Methodology\nIn this work, we adapt psycholinguistic experi-\nmental techniques for neural model assessment.\nIn this paradigm, neural models are fed hand-\ncrafted sentences designed to belie underlying net-\nwork knowledge.\nFollowing standard practice\nin psycholinguistics, statistical signiﬁcance is de-\nrived from linear mixed-effects models (Baayen\net al., 2008), with sum-coded ﬁxed-effect predic-\ntors and maximal random slope structure (Barr\net al., 2013).\nThis method permits us to factor\nout by-item variation and focus on differences in\nmodel behavior on materials differing only in the\nlinguistic features of critical interest. 1\n2.1\nNeural Models Tested\nWe study the behavior of two LSTM Language\nModels, one Transformer model and one base-\nline N-gram model, all trained on English text.\nThe ﬁrst LSTM is the “BIG LSTM+CNN Inputs”\nfrom (Jozefowicz et al., 2016), which we will re-\nfer to as the Google Model.\nIt was trained on\nthe One Billion Word benchmark (Chelba et al.,\n2013), with two hidden layers of 8196 per layer\nand uses Convolutional Neural Net (CNN) charac-\nter embeddings as input. The second LSTM model\nis the best-performing LSTM presented in the sup-\nplementary materials of Gulordava et al. (2018),\nwhich we will refer to as the Gulordava Model. It\nis much smaller, with 650 hidden units per layer,\nand was trained on 90-million words of Wikipedia.\nThe Google model is current state-of-the art for an\nLSTM model unenriched with structural supervi-\nsion, and the Gulordava model has been assessed\nextensively (e.g.\nGulordava et al. 2018; Futrell\net al. 2018; Wilcox et al. 2018; Giulianelli et al.\n2018). The transformer model used here is the\none presented in Dai et al. (2019). It was trained\non the Billion Word Benchmark and has 0.8 Bil-\nlion parameters.\nThe baseline is a 5-gram lan-\nguage model with Kneser-Ney smoothing, trained\non the British National Corpus (Leech, 1992) us-\ning SLIRM V1.5.7 (Stolcke, 2002).\n2.2\nDependent Measure: Surprisal\nWe assess model behavior by measuring the sur-\nprisal values RNN language models assign to\neach word in a given sentence. Surprisal is the\ninverse log probability of a word given its context:\nS(xi) = −log2 p(xi|hi−1),\nIn this case, xi is the current word and hi−1 is\nthe RNN’s hidden state before processing xi. The\nprobability is calculated from the RNN’s softmax\nlayer, and the logarithm is taken in base 2 so that\nthe surprisal is measured in bits. The surprisal at a\ncertain word tells us the extent to which that word\nis expected under the language model’s probabil-\nity distribution. There is a strong tradition linking\nsurprisal values derived from language models to\npsycholinguistic metrics, such as reading times in\n1Our studies were preregistered on aspredicted.org:\nTo see the preregistrations go to http://aspredicted.\norg/blind.php?x=X where X ∈{uw873w,95gj46}.\nhumans (Hale, 2001; Levy, 2008; Smith and Levy,\n2013; Goodkind and Bicknell, 2018).\n3\nCenter Embeddings\nIn a center embedding sentence, the subject of a\nmatrix (or main) clause is modiﬁed by an object-\nextracted relative clause.\nBecause any Noun\nPhrase can serve as the host of a relative clause,\nthe subject of the embedded relative clause can\nrecursively serve as the start of a second center-\nembedding sentence, and so on ad inﬁnitum, pro-\nvided that there are an equal number of subjects\nand verbs, as in Example (1).\n(1) The water [that the customer [that the waiterx\ndisliked]y drank]z was cold.\nCenter embedding sentences exemplify the pattern\nanbn, characteristic of context-free grammars, for\nnatural language. However, the structure requires\nmore than just counting: it is not sufﬁcient that\nthe number of verbs match the number of subjects,\nrather the verbs must semantically and syntacti-\ncally match their appropriate subjects and objects.\nThe verb drank is to be expected at the position\nmarked y in Example (1), but not at x or z, be-\ncause it corresponds to the subject customer and\nthe object water. An incremental predictor must\nsuppress an expectation for the word drink during\nthe region containing x, and then recover this ex-\npectation at y.\nTo assess whether the RNN LMs tested could\nsuppress expectations for verbs set up by subjects\nand activate them in the correct order, we created\n40 test items following the template in (2).\n(2) a. The\ndiamond\nthat\nthe\nthief\nstoleVP1\nglitteredVP2. [match, embedding]\nb. The diamond that the thief glitteredVP1\nstoleVP2. [mismatch, embedding]\nc. The diamond that the thief in the black\nmask\nstoleVP1\nglitteredVP2.\n[match,\nembedding-long]\nd. The diamond that the thief in the black\nmask glitteredVP1 stoleVP2.\n[mismatch,\nembedding-long]\ne. The\nthief\nstoleVP1\n/\nThe\ndiamond\nglitteredVP2 [match, sentence]\nf. The thief glitteredVP1 / The diamond\nstoleVP2 [mismatch, sentence]\nWe use plausibility match of ordering effect to as-\nsess whether the model was linking the right sub-\nject with the right verb. For example, it is plau-\nsible that a diamond glitters and a thief steals, as\nFigure 2: Model results for center embedding sentences. Higher values indicate stronger divergence between\nthe ordering effect match and mismatch conditions, indicating that models have learned the proper subject-verb\npairings for the center embedding construction.\nin (2-a), but implausible that a thief glitters and a\ndiamond steals as in (2-b). In our test sentences\nthe matrix clause subject tended to be an inani-\nmate entity that took an intransitive verb, and the\nrelative clause subject tended to be an animate en-\ntity that took a transitive verb. For each item, we\nmeasure the strength of the models’ expectation\nin terms of what we call the ordering effect at\neach verb: the surprisal in the [mismatch] condi-\ntion minus the surprisal in the [match] condition.\nOur prediction is that if a model has learned the\nordering restrictions imposed by the grammatical\nrules that govern English center embedding and\nuses these restrictions to appropriately guide pre-\ndictions about upcoming words, the ordering ef-\nfect should be at least as great in the two [embed-\nding] conditions as in the [sentence] conditions.\nWe report the summed ordering effect across the\ntwo VPs, which indicates the difference in sur-\nprisal between the two conditions due to speciﬁc\norder of the two verbs. As control sentences, we\nconverted each item into a pair of simple subject-\nverb sentences with no embedding, as in (2-e)–\n(2-f). If the ordering effect for the control sentence\nconditions is not positive, it would call into ques-\ntion our selection of subject–verb pairs.\nThe results from this experiment can be seen\nin Figure 2, with the N-Gram model at left, the\nTransformer model center left and the two LSTM\nmodels at right. Error bars indicate 95% conﬁ-\ndence intervals of across item means, with within-\nitem means subtracted, as advocated in Masson\nand Loftus (2003). The baseline N-Gram model\nshows a positive ordering effect in the control Sen-\ntence conditions, however the ordering effect is\nnot signiﬁcantly different from zero in the two Em-\nbedding conditions. For the Transformer and two\nLSTM models, the ordering effect is positive in\nthe control Sentence conditions, as well as in the\ntwo critical Embedding conditions. Examining the\ncontributions of the individual items themselves,\nwe ﬁnd that the surprisal difference at the second\n(matrix) verb is responsible for the majority of the\neffect. That is, given the context The diamond that\nthe thief ... the continuations stole and glittered\nare equally likely. However, given the partially-\nsaturated contexts in (3), the continuation glittered\nis much more likely in (3-a) than the continuation\nstole is in (3-b).\n(3) a. The diamond that the thief stole...\nb. The diamond that the thief glittered...\nIt is this difference that drives the majority of the\nOrdering Effect for the LSTM and Transformer\nmodels.\nCrucially, this behavior is inconsistent\nwith a linear approach to subject/verb plausibility\nmatch. If the models had learned only that a se-\nmantically plausible verb needed to follow a sub-\nject, then the order of the verbs should have no\neffect on surprisal. The positive ordering effect\nwe see in the two Embedding conditions indicates\nthe neural models have learned that the outer verb\nneeds to be associated with the ﬁrst subject: all\nthree models exhibit a ﬁrst-in-last-out approach to\nlicensing consistent with stack-like representation.\nTurning to differences between the three neural\nmodels: For the Gulordava and Transformer mod-\nels the ordering effect is higher in the control Sen-\ntence and Embedding Short conditions than in the\nEmbedding Long conditions, although neither of\nthe differences are signiﬁcant. But for the Google\nmodel, the ordering effect is larger in the embed-\nding conditions than in the control sentence con-\ndition.\nAlthough this increased effect size may\nat ﬁrst glance be surprising, recall that in the em-\nbedding conditions, there is more preceding con-\ntext than in the control-sentence condition that is\navailable to predict both verbs—including both ar-\nguments of the transitive verb. This larger overall\nordering effect in the embedding conditions sug-\ngests that the Google model, which is trained on\nan order of magnitude more data, may be more ef-\nﬁciently leveraging this additional preceding con-\ntext. It remains an open question why the Trans-\nformer Model, which is trained on the same large\ndataset, is unable to leverage similar contextual\ncues and maintain equally strong verbal expecta-\ntions across the relative clause modiﬁer.\n4\nFiller–Gap Dependency Licensing\n4.1\nMeasuring the Filler–Gap Dependency\nIn English, a range of linguistic structures—such\nas questions and relative clauses—are formed by\ninserting a wh-word and eliding (or gapping) sub-\nsequent material. For example, to turn the tran-\nsitive sentence in (4-a) into a question, a ﬁller (\nwho) is inserted at the beginning of the clause, and\nthe material being questioned (the direct object) is\ngapped, which we represent using the underscores\n(these are for presentational purposes only and are\nnot included in test items).\n(4) a. The count insulted the hostess yesterday.\nb. Who did the count insult\nyesterday?\nCrucially, the ﬁller and the gap depend on each\nother, insofar as a ﬁller word is illicit without a\nsubsequent gap, and a gap is unlicensed without\nan upstream ﬁller. Wilcox et al. (2018) established\nthat the two LSTM language models tested here\nlearn the ﬁller–gap dependency insofar as they\nlearn the 2 × 2 contingency between ﬁllers and\ngaps. To assess this, for each of their test sentences\nthey create four items following the four possible\ncombinations of ﬁllers and gaps, as in (5) (note\nthat in these and subsequent examples the * indi-\ncates ungrammatically).\n(5) a. I know that the count insulted the hostess\nyesterday. [–FILLER, -GAP]\nb.*I know who the count insulted the hostess\nyesterday. [+FILLER, -GAP]\nc.*I know that the count insulted\nyesterday.\n[–FILLER, +GAP]\nd. I know who the count insulted\nyesterday.\n[+FILLER, +GAP]\nTheir logic is as follows: If the models are learn-\ning that gaps require ﬁllers to be licensed, then the\ntransition from an object-taking verb to a preposi-\ntional phrase that indicates a syntactic gap should\nbe less surprising in the presence of an upstream,\nlicensing ﬁller.\nThat is S([–FILLER, +GAP])\nshould be greater than S([+FILLER, +GAP]) in\nthe post gap material “yesterday”.\nWe refer to\nthis difference as the +GAP wh-effect, a large ef-\nfect here indicates that the model has learned that\ngaps require ﬁllers to be licensed. We measure\nthe +GAP wh-effect in temporal adjuncts follow-\ning the gap site, as in yesterday in (5).\nAdditionally, if the models are learning that\nﬁllers set up expectations for gaps, then a ﬁlled\nargument structure position such as a direct object\nshould be less surprising in the absence of an up-\nstream ﬁller, a phenomena which is known in the\npsyhcolinguistics literature as the ﬁlled gap effect.\nThat is, S([+FILLER, –GAP]) should be greater\nthan S([–FILLER, –GAP]). We refer to this differ-\nence as a -GAP wh-effect, a large effect here in-\ndicates that models have learned that ﬁllers set up\nexpectations for gaps. We measure the -GAP wh-\neffect in the embedded verb direct object, e.g. at\n“the hostess” in (5).\nWilcox et al. (2018) sum differences into a sin-\ngle metric, the wh-licensing interaction, which\nthey measure in a post-gap temporal adjunct. In\nthis work, we eschew the wh-licensing and look\ninstead at the two wh-effects in the +GAP and\n-GAP conditions.\nWe do this for two reasons:\nFirst, collapsing all four surprisal values obfus-\ncates which part of the contingency the models\nlearn. It may be the case that the vast majority\nof the licensing interaction comes from surprisal\ndifferences in just one of the two conditions, a\nfact which would be hard to observe by studying\nthe full interaction. Second, if upstream ﬁllers set\nup expectations for empty argument structure po-\nsitions, then the ﬁlled gap effect should be most\nnoticeable on the object itself, not in a subsequent\nadjunct. Measuring the wh-effect separately for\neach condition allows us to take our measurement\nat the precise location where we would expect the\neffect to be the largest.\nθ\nν\nX\nδ\nγ\nβ\nﬁller\nα\n×\n✓\nFigure 3: Island constraints and ﬁlling gaps across is-\nlands. If node X is an island, then a ﬁller outside X\ncannot associate with a gap inside X, but it can asso-\nciate with a ﬁller on the far side of X. For our analyses,\nsuccessful learning of an island constraint implies that\nwe should not see wh-effects at the ﬁrst part of the ma-\nterial δ immediately following the potential gap site,\nbut we should see wh-effects in ν, following a licit gap\nsite.\n4.2\nLicensing Over Syntactic Islands\nIn addition to basic ﬁller–gap dependency licens-\ning, Wilcox et al. (2018) and Wilcox et al. (2019a)\nargue that the RNNs tested show sensitivity to nu-\nmerous island effects (although see Chowdhury\nand Zamparelli (2018) for a contrasting view). Is-\nlands are syntactic positions that locally block the\nﬁller–gap dependency (Ross, 1967). For example,\nﬁllers can associate with gaps located in object po-\nsition of a matrix clause, as in (6-a), but not when\nthe gap occurs within a relative clause, as in (6-b).\n(6) a. Who did the hostess insult\nyesterday?\nb.*Who did the hostess insult [RC the count\nthat knows\n] yesterday?\nCrucially, although islands block the ﬁllers from\nassociating with gaps within the island, they do\nnot prohibit association between ﬁllers and gaps\nthat occur structurally to the right of the island, as\nshown in Figure 3.\nWilcox et al. (2019b) found that while large\nscale models are able to thread the 2 × 2 contin-\ngency between ﬁllers and gaps into syntactically\ncomplex material–such as through numerous sen-\ntential embeddings—they do not thread the depen-\ndency into some island conﬁgurations. Inside of\nrelative clauses and temporal adjuncts, for exam-\nple, the presence or absence of an upstream ﬁller\nhas no effect on the relative surprisal of a gap, and\nthe wh-licensing interaction drops to near zero.\nHowever, model inability to thread the ﬁller–\ngap dependency into island conﬁgurations pro-\nvides only half of the evidence necessary to estab-\nlish that neural models are “learning” islands in a\nway meaningfully similar to humans. Island con-\nﬁgurations act as blockers, but only for the dura-\ntion of the island—the length of the relative clause\nor the temporal adjunct, for the two islands tested\nhere. If RNNs learn islands as local contexts into\nwhich an outside ﬁller cannot license a gap, they\nshould recover their expectations for gaps follow-\ning the island.\nTo assess whether models recover expectations\nfor licit gaps following island conﬁgurations, we\ngenerated test sentences following the template in\n(7), featuring two well-studied islands: adjunct\nislands (7-b) and complex noun phrase islands\n(7-d). In these examples, the island portions of the\nsentences, in which gaps are not allowed, appear\nin boldface.\n(7) a. I know who the count from the southern\nprovince talked very loudly with\non the\nbalcony. [object]\nb.*I know who , after the count insulted\non the balcony , the hostess talked with the\ncountess. [adjunct]\nc. I know who , after insulting the hostess\n, the count talked with\non the balcony.\n[over-adjunct]\nd.*I know who the count that insulted\non\nthe balcony talked with the hostess. [cnp]\ne. I know who the count that insulted the\nhostess talked loudly with\non the bal-\ncony. [over-cnp]\nFor each condition, we created a sentence template\nand seeded each region in the template with be-\ntween three and seven examples. Permuting the\nexamples, we generated thousands of candidate\nsentences, from which we sampled 100 at random\nand measured the wh-effect for the +GAP and –\nGAP conditions.\nIf the models are sensitive to\nthe island constraints, then we expect strong wh-\neffects in the grammatical [object] condition, but\nnot in the ungrammatical [adjunct] and [complex\nnoun phrase] ([cnp]) conditions. Furthermore, if\nmodels are able to recover expectations from gaps\nfollowing the end of an island, we would ex-\npect strong wh-effects in the grammatical [over-\nadjunct] and [over-cnp] conditions.\nThe results from this experiment can be seen in\nFigure 4, with the wh-effect in the +GAP condi-\ntion at left and the –GAP condition at right. The\nbaseline N-Gram model showed wh-effects that\nwere not signiﬁcantly different from zero for all\nFigure 4: Model results for maintaining the ﬁller–gap dependency over island constructions. Strong wh-effects are\nexpected in the grammatical conditions (orange and blue), with reduced wh-effects in the island conditions (green).\nconditions, and is not included in the graphs. Fo-\ncusing on the +GAP condition at left, we see a\nstrong wh-effect in the control object condition but\na signiﬁcant reduction of wh-effect in the adjunct\nand cnp conditions for all models (p < 0.001).\nIn the grammatical over-adjunct and over-cnp we\nstill see a signiﬁcant reduction in wh-effect com-\npared to the object condition (p < 0.001), but a\nsigniﬁcant increase in wh-effect relative to the cor-\nresponding island conditions in many cases. This\nrecovery of expectations is signiﬁcant for CNP\nIslands for all models (p < 0.001) and for the\nAdjunct Islands in the case of the Google model\n(p < 0.001). The results are especially striking for\nthe Google Model: While the absence of an up-\nstream ﬁller induces only one more bit of surprisal\nat the gap site within an island, it induces between\n2-5 more bits of surprisal when a gap occurs licitly\ndownstream of an island.\nTurning to the -GAP conditions at right, the re-\nsults are more mixed. All three models show sig-\nniﬁcantly more licensing interaction in the con-\ntrol object condition compared to the island con-\nditions, except for the Transformer model in the\ncase of CNP Islands. However, only the Google\nModel shows a signiﬁcant recuperation of empty\nargument structure expectation in the cnp vs. over-\ncnp condition (p < 0.001).\nThese results indi-\ncate that the three language models tested are able\nto bracket their expectations for gaps and regain\nthem on the other side in the case of relative\nclauses. However, neither model does a good job\nof recovering the ﬁlled gap effect following an is-\nland, modulo complex noun phrase islands for the\nGoogle model.\n4.3\nWh-Discharge Effects\nThe ﬁller–gap dependency is constrained, insofar\nas ﬁllers can license only one gap. Wilcox et al.\n(2018) found that RNN models were sensitive to\nthis constraint, displaying a reduction in licensing\ninteraction following a gap, if another gap existed\nupstream in the sentence as in (8-a). The presence\nof a ﬁller sets up an expectation for a gap, which is\ndischarged at the ﬁrst gap site, and cannot partic-\nipate in downstream licensing effects. However,\nif models are sensitive to the fact that gaps can-\nnot licitly occur within islands (unless they are li-\ncensed within the island itself), the presence of a\ngap inside a relative clause or a temporal adjunct\nshould not result in the discharge of gap expecta-\ntion.\nTo assess whether gap discharge effects are mit-\nigated when the ﬁrst gap occurs inside of an island,\nwe generated 100 examples following the process\ndescribed in Section 4.2 and the template in (8).\nFollowing the results in Wilcox et al. (2018), sec-\ntion 3.3, we expect a slightly negative wh-effects\nin the subject condition. However, if gaps inside of\nislands do not discharge the wh-effect set up by a\nﬁller, we expect positive wh-effects in the adjunct-\ndischarge and cnp-discharge conditions.\n(8) a. I know who\ntalked very loudly with\non\nthe balcony. [subject]\nb. I know who , after insulting\n, the\ncount talked loudly with\non the balcony.\n[adjunct-discharge]\nc. I know who the old man that insulted\ntalked loudly with\non the balcony. [cnp-\ndischarge]\nThe results from this experiment can be seen in 5.\nFigure 5: Discharge effects for gaps in Subject and Is-\nland positions. Strong Wh-Effects are expected in the\nAdjunct and ComplexNP conditions, with negative wh-\neffects in the Subject condition.\nFor the RNN models, In the –GAP cases, for both\nmodels there is no signiﬁcant difference between\nthe conditions. However, in the +GAP cases, there\nis a signiﬁcant increase in wh-effect between the\nsubject and adjunct-discharge and cnp-discharge\nconditions (p < 0.001 for both models). For the\nTransformer model, the Adjunct and Subject con-\nditions pattern together, and there is a signiﬁcant\nincrease in Wh-Effect for the Complex NP condi-\ntion, in both the +Gap and -Gap cases (p < 0.001).\nThese results conform to those found in 4.2: all\nmodels have a difﬁcult time threading expectations\nfor ﬁlled argument structure positions through\nsyntactically-complex material. However, expec-\ntations surrounding gaps are clear, at least for the\ntwo LSTM models: When gaps occur inside of is-\nlands, they do not trigger the the same discharge\neffects as gaps in subject positions. Interestingly,\nthis generalization seems to be less robust for the\nTransformer model, which demonstrates the cor-\nrect behavior only for Complex NP islands. Over\nall, these results provide further evidence that the\nmodels are able to process the edge of a syntactic\nisland, and recover expectations for gaps on the far\nside.\n5\nGeneral Discussion and Conclusion\nIn this paper, we have provided new evidence that\nneural models can learn hierarchical generaliza-\ntions from linear input alone.\nBy adopting the\npsycholinguistic paradigm for RNN assessment,\nwe have shown that two large-scale LSTM models\nand one Transformer modal can suppress and re-\ncover expectations set up by subject Noun Phrases\nand ﬁllers within intervening blocking structures\nand recover those expectations on the far side of\nthose syntactic blockers.\nThis behavior corre-\nsponds to the idea of pushing and popping ex-\npectations in a stack-like data structure, which\nis required for proper incremental prediction of\ncontext-free languages.\nHowever, the suppression and recovery of ex-\npectations is imperfect. For example, in the ﬁller–\ngap dependency, we found that models only par-\ntially recover expectations for gaps on the far side\nof island structures, especially in the -GAP condi-\ntions, where no model was able to robustly recover\nﬁlled gap expectations. Interestingly, the LSTM\nmodels tended to perform better than Transformer\nmodel, even when trained on orders of magnitude\nless data.\nThese results indicate that the large\nnumber of parameters in the Transformer architec-\nture may result in lower test-time perplexity, but\nmay not necessarily result in more grammatical\nbehavior, at least for the tightly-controlled syntac-\ntic test suites presented here. It may be that the\nsmaller number of parameters in the LSTMs force\nthe models to make more robust, and ultimately\nhumanlike generalizations.\nThis work only assesses two model architec-\ntures. It is likely that neural models with a stronger\nstructural bias, such as RNNGs (Dyer et al., 2016)\nor LSTMs enhanced with a structural bias as in\nShen et al. (2018) would perform better on the\ntests presented here; testing these, and other mod-\nels, will be the basis for future work.\nReferences\nR Harald Baayen, Douglas J Davidson, and Douglas M\nBates. 2008. Mixed-effects modeling with crossed\nrandom effects for subjects and items. Journal of\nMemory and Language, 59(4):390–412.\nDale J Barr, Roger Levy, Christoph Scheepers, and\nHarry J Tily. 2013.\nRandom effects structure for\nconﬁrmatory hypothesis testing: Keep it maximal.\nJournal of Memory and Language, 68(3):255–278.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\nDoo Kok Choe and Eugene Charniak. 2016. Parsing\nas language modeling. In Proceedings of the 2016\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2331–2336.\nNoam Chomsky. 1956. Three models for the descrip-\ntion of language. IRE Transactions on Information\nTheory, 2(3):113–124.\nShammur Absar Chowdhury and Roberto Zamparelli.\n2018. Rnn simulations of grammaticality judgments\non long-distance dependencies. In Proceedings of\nthe 27th International Conference on Computational\nLinguistics, pages 133–144.\nZihang Dai, Zhilin Yang, Yiming Yang, William W\nCohen, Jaime Carbonell, Quoc V Le, and Ruslan\nSalakhutdinov. 2019. Transformer-xl: Attentive lan-\nguage models beyond a ﬁxed-length context. arXiv\npreprint arXiv:1901.02860.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,\nand Noah A. Smith. 2016. Recurrent neural network\ngrammars. In Proc. of NAACL.\nJeffrey L Elman. 1990. Finding structure in time. Cog-\nnitive Science, 14(2):179–211.\nJeffrey L Elman. 1991.\nDistributed representations,\nsimple recurrent networks, and grammatical struc-\nture. Machine learning, 7(2-3):195–225.\nRichard Futrell, Ethan Wilcox, Takashi Morita, and\nRoger Levy. 2018. Rnns as psycholinguistic sub-\njects: Syntactic state and grammatical dependency.\narXiv preprint arXiv:1809.01329.\nRichard Futrell, Ethan Wilcox, Takashi Morita, Peng\nQian, Miguel Ballesteros, and Roger Levy. 2019.\nNeural language models as psycholinguistic sub-\njects:\nRepresentations of syntactic state.\narXiv\npreprint arXiv:1903.03260.\nMario Giulianelli, Jack Harding, Florian Mohnert,\nDieuwke Hupkes, and Willem Zuidema. 2018.\nUnder the hood:\nUsing diagnostic classiﬁers\nto investigate and improve how language mod-\nels track agreement information.\narXiv preprint\narXiv:1808.08079.\nAdam Goodkind and Klinton Bicknell. 2018. Predic-\ntive power of word surprisal for reading times is a\nlinear function of language model quality. In Pro-\nceedings of the 8th Workshop on Cognitive Modeling\nand Computational Linguistics (CMCL 2018), pages\n10–18.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018.\nColor-\nless green recurrent networks dream hierarchically.\narXiv preprint arXiv:1803.11138.\nJohn Hale. 2001. A probabilistic earley parser as a psy-\ncholinguistic model. In Proceedings of the second\nmeeting of the North American Chapter of the Asso-\nciation for Computational Linguistics on Language\ntechnologies, pages 1–8. Association for Computa-\ntional Linguistics.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997.\nLong short-term memory.\nNeural Computation,\n9(8):1735–1780.\nAravind K Joshi and Yves Schabes. 1997.\nTree-\nadjoining grammars. In Handbook of formal lan-\nguages, pages 69–123. Springer.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring the lim-\nits of language modeling. arXiv, 1602.02410.\nMarco Kuhlmann. 2013.\nMildly non-projective de-\npendency grammar.\nComputational Linguistics,\n39(2):355–387.\nAdhiguna Kuncoro, Miguel Ballesteros, Lingpeng\nKong, Chris Dyer, Graham Neubig, and Noah A\nSmith. 2016.\nWhat do recurrent neural network\ngrammars learn about syntax?\narXiv preprint\narXiv:1611.05774.\nGeoffrey Neil Leech. 1992. 100 million words of en-\nglish: the british national corpus (bnc).\nRoger Levy. 2008. Expectation-based syntactic com-\nprehension. Cognition, 106(3):1126–1177.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of lstms to learn syntax-\nsensitive dependencies. Transactions of the Associ-\nation for Computational Linguistics, 4:521–535.\nDavid Mareˇcek and Rudolf Rosa. 2018.\nExtract-\ning syntactic trees from transformer encoder self-\nattentions.\nIn Proceedings of the 2018 EMNLP\nWorkshop BlackboxNLP: Analyzing and Interpret-\ning Neural Networks for NLP, pages 347–349.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. arXiv preprint\narXiv:1808.09031.\nMichael EJ Masson and Geoffrey R Loftus. 2003. Us-\ning conﬁdence intervals for graphically based data\ninterpretation.\nCanadian Journal of Experimen-\ntal Psychology/Revue canadienne de psychologie\nexp´erimentale, 57(3):203.\nR Thomas McCoy, Robert Frank, and Tal Linzen.\n2018.\nRevisiting the poverty of the stimulus:\nhierarchical generalization without a hierarchical\nbias in recurrent neural networks.\narXiv preprint\narXiv:1802.09091.\nJohn Robert Ross. 1967. Constraints on variables in\nsyntax.\nHiroyuki Seki, Takashi Matsumura, Mamoru Fujii,\nand Tadao Kasami. 1991.\nOn multiple context-\nfree grammars.\nTheoretical Computer Science,\n88(2):191–229.\nYikang Shen, Shawn Tan, Alessandro Sordoni, and\nAaron Courville. 2018. Ordered neurons: Integrat-\ning tree structures into recurrent neural networks.\narXiv preprint arXiv:1810.09536.\nStuart M Shieber. 1985. Evidence against the context-\nfreeness of natural language. In Philosophy, Lan-\nguage, and Artiﬁcial Intelligence, pages 79–89.\nSpringer.\nNathaniel J Smith and Roger Levy. 2013. The effect of\nword predictability on reading time is logarithmic.\nCognition, 128(3):302–319.\nAndreas Stolcke. 2002. Srilm-an extensible language\nmodeling toolkit. In Seventh international confer-\nence on spoken language processing.\nDavid Jeremy Weir. 1988.\nCharacterizing mildly\ncontext-sensitive grammar formalisms.\nGail Weiss, Yoav Goldberg, and Eran Yahav. 2018. On\nthe practical computational power of ﬁnite preci-\nsion rnns for language recognition. arXiv preprint\narXiv:1805.04908.\nEthan Wilcox, Roger Levy, and Richard Futrell.\n2019a. What syntactic structures block dependen-\ncies in rnn language models?\narXiv preprint\narXiv:1905.10431.\nEthan Wilcox, Roger Levy, Takashi Morita, and\nRichard Futrell. 2018. What do rnn language mod-\nels learn about ﬁller-gap dependencies?\narXiv\npreprint arXiv:1809.00042.\nEthan Wilcox, Peng Qian, Richard Futrell, Miguel\nBallesteros, and Roger Levy. 2019b. Structural su-\npervision improves learning of non-local grammati-\ncal dependencies. arXiv preprint arXiv:1903.00943.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2019-06-10",
  "updated": "2019-06-10"
}