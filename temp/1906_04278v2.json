{
  "id": "http://arxiv.org/abs/1906.04278v2",
  "title": "Performance Analysis and Characterization of Training Deep Learning Models on Mobile Devices",
  "authors": [
    "Jie Liu",
    "Jiawen Liu",
    "Wan Du",
    "Dong Li"
  ],
  "abstract": "Training deep learning models on mobile devices recently becomes possible,\nbecause of increasing computation power on mobile hardware and the advantages\nof enabling high user experiences. Most of the existing work on machine\nlearning at mobile devices is focused on the inference of deep learning models\n(particularly convolutional neural network and recurrent neural network), but\nnot training. The performance characterization of training deep learning models\non mobile devices is largely unexplored, although understanding the performance\ncharacterization is critical for designing and implementing deep learning\nmodels on mobile devices.\n  In this paper, we perform a variety of experiments on a representative mobile\ndevice (the NVIDIA TX2) to study the performance of training deep learning\nmodels. We introduce a benchmark suite and tools to study performance of\ntraining deep learning models on mobile devices, from the perspectives of\nmemory consumption, hardware utilization, and power consumption. The tools can\ncorrelate performance results with fine-grained operations in deep learning\nmodels, providing capabilities to capture performance variance and problems at\na fine granularity. We reveal interesting performance problems and\nopportunities, including under-utilization of heterogeneous hardware, large\nenergy consumption of the memory, and high predictability of workload\ncharacterization. Based on the performance analysis, we suggest interesting\nresearch directions.",
  "text": "Performance Analysis and Characterization of\nTraining Deep Learning Models on Mobile Devices\nJie Liu, Jiawen Liu, Wan Du and Dong Li\nUniversity of California, Merced\n{jliu279, jliu265, wdu3, dli35}@ucmerced.edu\nAbstract—Training deep learning models on mobile devices\nrecently becomes possible, because of increasing computation\npower on mobile hardware and the advantages of enabling high\nuser experiences. Most of the existing work on machine learning\nat mobile devices is focused on the inference of deep learning\nmodels, but not training. The performance characterization of\ntraining deep learning models on mobile devices is largely unex-\nplored, although understanding the performance characterization\nis critical for designing and implementing deep learning models\non mobile devices.\nIn this paper, we perform a variety of experiments on a\nrepresentative mobile device (the NVIDIA TX2) to study the\nperformance of training deep learning models. We introduce a\nbenchmark suite and a tool to study performance of training\ndeep learning models on mobile devices, from the perspectives of\nmemory consumption, hardware utilization, and power consump-\ntion. The tool can correlate performance results with ﬁne-grained\noperations in deep learning models, providing capabilities to\ncapture performance variance and problems at a ﬁne granularity.\nWe reveal interesting performance problems and opportunities,\nincluding under-utilization of heterogeneous hardware, large\nenergy consumption of the memory, and high predictability of\nworkload characterization. Based on the performance analysis,\nwe suggest interesting research directions.\nI. INTRODUCTION\nDeep learning models have been widely deployed on mobile\ndevices (e.g., mobile phones and smart home hub) to process\non-board sensing data and enable a variety of mobile applica-\ntions (e.g., machine translation, speech recognition, cognitive\nassistance and street navigation) [1]–[3]. Those models are\ndeployed for model inference (not for model training). The\nexisting work has been conducted to analyze the performance\nand resource utilization of deep learning workloads on mobile\ndevices when those models are deployed for model infer-\nence [4]–[10]. Those studies are important for optimizing the\nperformance of deep learning models on mobile devices.\nBesides model inference, training deep learning models\non mobile devices recently becomes possible, because of\nincreasing computation power on mobile hardware and the\nadvantages of enabling high user experiences. In particular,\ntraining deep learning models opens up a new approach\nto utilize the computational resources. As the hardware of\nmobile devices is increasingly powerful and domain-speciﬁc,\nespecially with the emergence of artiﬁcial intelligence (AI)\nchipsets and powerful mobile GPU [11]–[14], training deep\nlearning models is moving from the cloud to mobile devices\nto leverage these decentralized computational resources. Fur-\nthermore, training deep learning models on mobile devices can\navoid transmitting user data to the cloud as in the traditional\nmethod. The traditional method can cause the breach of user\nprivacy (even using an anonymous dataset and mixing it with\nother data). For applications where the training objective is\nspeciﬁed on the basis of data available on each mobile device,\ntraining on mobile devices can signiﬁcantly reduce privacy and\nsecurity risks by limiting the attack surface to only the device.\nMost of the existing work on machine learning at mobile\ndevices is focused on the inference of deep learning models\n(particularly convolutional neural network (CNN) and recur-\nrent neural network (RNN)), but not training. The perfor-\nmance characterization of training deep learning models on\nmobile devices is largely unexplored, although understanding\nthe performance characterization is critical for designing and\nimplementing deep learning models on mobile devices.\nThe recent work studies the performance of training deep\nlearning models on servers [15]. However, training on mobile\ndevices and on servers have different requirements, and have\nto be studied separately. First, training deep learning networks\nshould not interfere with the user’s regular operations on\nmobile devices; The interference can manifest as unexpected\nshorter battery life because of large energy consumption\ncaused by training deep learning networks, or the extended\nlatency of the user’s operations. Second, the training data\nis likely to be collected and used for training on a daily\nbasis. For example, to train a deep learning network for image\nclassiﬁcation, the system can use images collected per day as\ntraining samples and train the model every day. Third, a mobile\ndevice usually has a small memory capacity (compared with\nservers), hence some large deep learning models with tens of\nGB memory footprint (e.g., ResNet201 and VGG19) are not\nsuitable to be trained on mobile devices.\nIn this paper, we perform a variety of experiments on\na representative mobile device (the NVIDIA TX2) to study\nthe performance of training deep learning models. Our study\nprovides insightful observations and reveals potential research\nopportunities. In particular, this paper aims to answer the\nfollowing research questions.\nFirst, is training a deep learning network on a mobile\ndevice even possible? The existing work on federated learning\nhas shown preliminary success of training some machine\nlearning models on mobile devices [16]–[26]. Different from\na typical deep learning model, those machine learning models\nare small in terms of memory consumption and model size.\nTraining deep learning models is known to be compute-\narXiv:1906.04278v2  [cs.LG]  7 Sep 2019\nintensive and memory-intensive. Traditionally deep learning\nmodels are trained on servers with GPU with thousands of\ncores and high memory bandwidth. However, mobile devices\nare under recourse constraint, e.g., limited computation power\nand relatively small memory capacity. It is unknown whether\nand which deep learning models are trainable.\nSecond, how does training various deep learning models in\nmobile devices differ? Deep learning models have shown suc-\ncess in a broad range of application domains. Many deep learn-\ning models, such as DenseNet, Inception, ResNet, SqueezeNet\nand XceptionNet are related to image classiﬁcation, which\nis one of the most common application domains for deep\nlearning models. Other kinds of deep learning models, such\nas reinforcement learning, Generative Adversarial Network\n(GAN) that are used in other application domains such as robot\ncontrols, image generation and natural language processing,\nshould also be taken into consideration. We aim to explore a\ndiverse set of deep learning models in our study.\nThird, what are the major performance problems when we\ntrain deep learning models on mobile devices? Are those\nproblems on mobile devices different from those on servers?\nAnswering the two questions is useful to identify research\nproblems and train deep learning models more efﬁciently on\nmobile devices.\nBy conducting extensive experiments with various deep\nlearning models on a speciﬁc mobile device, the NVIDIA\nTX2, we ﬁnd many insightful observations. In summary, we\nmake the following contributions.\n• We introduce a benchmark suite for studying the work-\nload of training deep learning models on mobile devices.\nThe benchmark suite includes four application domains\nand includes ten common deep learning models. Those\nbenchmarks are chosen with the consideration of possible\nresource constrained on mobile devices. We make our\nbenchmark suite open-source and intend to continually\nexpand it to support various mobile devices.\n• We introduce a tool to study performance of training deep\nlearning models on mobile devices, from the perspectives\nof the memory consumption, hardware utilization, and\npower consumption. More importantly, the tool can cor-\nrelate performance results with ﬁne-grained operations in\ndeep learning models, providing capabilities to capture\nperformance variance and problems at a ﬁne granularity.\n• We reveal interesting performance problems and op-\nportunities, including under-utilization of heterogeneous\nhardware, large energy consumption of memory, and high\npredictability of workload characterization. Based on the\nperformance analysis, we suggest interesting research\ndirections.\nII. TRAINING DEEP LEARNING MODELS ON MOBILE\nDEVICES\nDeep learning is a general-purpose method that can be\nused to learn and model complicated linear and non-linear\nrelationships between input datasets and output. Many deep\nlearning models can be represented as a directed acyclic graph\nwhere nodes of the graph are connected neurons. Embedded\nin the graph, there are a number of parameters (e.g., “weights”\nand “bias”). Those neurons and parameters are organized\nas layers. The process of obtaining the optimal values of\nthose parameters to reach high prediction accuracy is called\n“training”. Training involves many iterations of computation\n(sometimes millions of iterations), in order to acquire the\noptimal parameters. Each iteration is a training step, and\nconsists of forward and backward passes. During the back-\nward pass, a backpropagation algorithm is used to optimize\nthe parameters. The backpropagation algorithm calculates the\ngradient of each parameter. Also, the number of parameters\nand the corresponding gradients are equal. The intermediate\nresults, which are generated by each layer, are dominated by\nfeature maps. The forward pass generates feature maps, and\nthe backward pass uses them to update the parameters. Hence,\nfeature maps need to be temporarily stored in the memory\nduring the forward pass and before the backward pass can\nconsume them.\nThe modern machine learning frameworks, e.g., Tensor-\nFlow [27] and PyTorch [28], employ a dataﬂow graph where\nthe deep learning models training is modeled as a directed\ngraph composed of a set of nodes (operations). By decompos-\ning a deep learning model graph (discussed above) into a set\nof nodes or ﬁne-grained operations (e.g., SoftMax, Sigmoid\nand MatMul), these frameworks greatly improve hardware\nutilization and system throughput. Training a deep learning\nmodel easily involves a large number of operations. In a single\ntraining step of training deep learning models, there can be\ntens of different operations. Each operation can be invoked\nhundreds of times, each of which is an operation instance.\nIII. METHODS\nIn this section, we introduce the metrics and tools we use\nto evaluate the performance of training deep learning models.\nA. Evaluation Metrics\nCPU utilization. This metric quantiﬁes how frequently\nCPU is utilized during deep learning models training. This\nmetric is deﬁned in Equations 1 and 2.\nCPU Core Utilization = T C\nactive × 100\nTtotal\n%\n(1)\nCPU Avg Utilization =\nPn\ni (CPU Core Utilizationi)\nn\n(2)\nEquation 1 is used to calculate the utilization of an in-\ndividual CPU core. In Equation 1, Ttotal denotes the total\ntraining time; T C\nactive indicates the active time of the CPU core.\nEquation 2 is used to calculate the average CPU utilization of\nall CPU cores. In Equation 2, n is the total number of CPU\ncores for training deep learning models, and i is the index of\nthe CPU core. A larger value of CPU Avg Utilization in-\ndicates higher CPU utilization. We want high CPU utilization\nto achieve high throughout processing of training operations.\n2\nData \nPreprocessing\nModel \nTraining\nWarm Up\nPeriod\nNvprof\nCPU and GPU\nVisualization\nNvidia\nTegrastats\nTraining Logs\nMemory \nUsage\nNvidia\nVisual Profiler\nMemory\nProfiler\nCPU\nProfiler\nGPU\nProfiler\nEnergy\nProfiler\nCPU\nUtilization\nGPU\nUtilization\nEnergy\nConsumption\nOperations \nProfiling\nCuda Kernels \nProfiling\nOffline Stage\nOnline Stage\nTensorFlow\nProfiler\nOffline Stage\nFig. 1: Proﬁling tools and proﬁling workﬂow.\nGPU utilization. This metric quantiﬁes how frequently\nGPU is utilized during deep learning model training. This\nmetric is deﬁned in Equation 3.\nGPU Utilization = T G\nactive × 100\nTtotal\n%\n(3)\nAs shown in Equation 3, the GPU utilization is deﬁned\nsimilar to the CPU utilization in Equation 1. We also want\nhigh GPU utilization to achieve high throughout processing\nof training operations.\nPeak memory consumption. Training deep learning mod-\nels can be memory-consuming, as a large amount of param-\neters, derivatives and temporary variables use the memory\nspace. Some popular deep learning models involve a large\nnumber of parameters. For example, VGG-16 and Resnet-\n50 have 138 million and 25 million parameters respectively,\nconsuming 6.3 GB and 5.8 GB memory (the batch size is\n64); SqueezeNet, a small deep learning model designed for\nmobile devices has 5 million parameters, consuming 5.7 GB\nmemory (the batch size is 64). The memory consumption of a\ndeep learning model sets up a constraint on whether training\nthe model on a mobile device is feasible. The peak memory\nconsumption is deﬁned as the maximum memory usage during\nthe training process.\nEnergy consumption. Since a mobile device has limited\nbattery life, reporting energy consumption of training deep\nlearning models is critical to determine if the training is\nfeasible within the battery life. Energy consumption is cal-\nculated based on Equation 4. During the model training, we\ncollect power consumption of the mobile device periodically.\nIn Equation 4, time interval deﬁnes how frequently we\ncollect power data, and Power Consumptioni is the whole\nsystem power of the mobile device collected in a power sample\ndata i.\nEnergy =\nX\ni\ntime interval × Power Consumptioni\n(4)\nThroughput. This metric is used to evaluate the efﬁciency\nof the training process. Throughput in this paper is deﬁned\nas how many training samples can be processed and used\nfor training in one second. For example, when we train\nDenseNet40 using the batch size of 4, we can ﬁnish ﬁve\ntraining steps in one second, and each training step processes\nfour images (samples). Hence, the throughput for training\nDenseNet40 is 20 samples per second.\nIdle state ratio for a core. During the training, some CPU\ncores can be idle (i.e., the utilization is 0). Idle state ratio for\na core is the percentage of the total training time that the core\nis idle.\nB. Proﬁling Tools\nWe use the existing tools, Nvprof [29], Tegrastats [30] and\nTensorFlow Proﬁler [31], for performance analysis and char-\nacterization of training deep learning models on the NVIDIA\nTX2. Nvprof is a proﬁling tool that collects execution time of\ncomputation on CPU and GPU. Nvprof can be used to identify\nidle or busy states of CPU and GPU. When used for GPU,\nNvprof can also be used to identify GPU kernel names (hence\nthe names of operations running on GPU).\nTegrastats is a tool that collects hardware utilization of CPU\nand GPU, power consumption of hardware components (CPU,\nGPU, and memory) and memory consumption.\nTensorFlow Proﬁler\n[31] is a tool integrated into Ten-\nsorFlow runtime to perform operations statistics, including\noperations execution time and dependency between operations.\nNvprof and Tegrastats do not provide APIs that allow\nthe programmer to trigger or stop proﬁling within the ap-\nplication. Nvprof and Tegrastats can run continuously as a\nsystem daemon and collect system-wide information at any\nmoment. Simply using Nvprof and Tegrastats cannot meet the\nuser’s needs, because sometimes the user wants to correlate\nthe proﬁling results (energy and memory consumption) with\noperations during the training process. The training process\nfor deep learning models easily involves a large number of\noperations (thousands or even millions of operations in a single\n3\ntime step). Collecting the proﬁling results for operations is\nchallenging.\nWe develop a tool to address the above challenge. In\nparticular, during the training process, we record the start and\nend times of all operations at each layer; We also periodically\nexamine the execution information (including CPU and GPU\nutilization, power consumption of hardware components) ev-\nery 10ms (we choose 10ms to control the proﬁling overhead).\nThe execution information is dumped into a ﬁle with the\nimplicit timestamp information, due to our periodical proﬁling\nmethod. After the training process, we associate the execution\ninformation with operations based on timing information (i.e.,\nthe start and end times of all operations). Figure 1 shows our\ntools and proﬁling workﬂow.\nIn Section IV (the section of evaluation results), the results\nare presented for all operations as a whole, because that allows\nus to easily present the results.\nC. Training Deep Learning Models on NVIDIA TX2\nWe use the NVIDIA TX2 as our evaluation platform. This\nplatform is an embedded system-on-module (SoM) with a\ndual-core NVIDIA Denver2 plus a quad-core ARM Cortex-\nA57 (six CPU cores in total), eight GB LPDDR4 and inte-\ngrated 256-core Pascal GPU (mobile GPU). The GPU has two\nstreaming multiprocessors (SM), and each SM has 128 cores.\nThe eight GB memory is shared between CPU and GPU. The\npeak power consumption of TX2 is just 15 Watt. TX2 is a\nrepresentative mobile platform. It has been commonly used\nin self-driving cars, robotics and drones. Many other common\nmobile devices, such as Snapdragon 855, Movidius Myriad2\nand Nvidia Xavier, have comparable computation capability\nand memory capacity. Table II summarizes the major hardware\nfeatures of the NVIDIA TX2.\nIV. EVALUATION RESULTS\nIn this section, we present the evaluation results and high-\nlight major observations.\nA. Experiment Setup\nTable II summarizes the deep learning models we use for\nevaluation. The table also lists those deep learning models that\ncannot be successfully trained on TX2 because of the memory\nconstraint. Among those models, DenseNet100 and NMT can\ntrain for a few time steps, but have segmentation faults later on;\nVGG19, ResNet101, ResNet152 and BERT cannot get started\non training at all.\nWe use TensorFlow v1.13 to train the deep learning models.\nUnless indicated otherwise, we use the default conﬁgruations\nfor TensorFlow. Note that we use TensorFlow instead of\nTensorFlow Lite, although TensorFlow Lite targets on mobile\ndevices, because of the following reasons. (1) TensorFlow\nLite only supports model inference, not training. Currently,\nthere is no training framework especially targeting on training\ndeep learning models on mobile devices. (2) TensorFlow and\nTensorFlow Lite have common implementations for many\nTABLE I: The Speciﬁcations of NVIDIA TX2\nHardware\nSpeciﬁcations\nSystems\nTegra TX2 SoC\nCPU1\nQuad-core ARM A57 MPCore\nCache CPU1\nL1 I: 128KB, L1 D: 64KB, L2: 2MB\nCPU2\nDual-core NVIDIA Denver 2 64-Bit\nCache CPU2\nL1 I: 48KB, L1 D: 32KB, L2: 2MB\nGPU\nNVIDIA Pascal GPU (256 CUDA Cores)\nMemory\n8GB 128-bit LPDDR4 Memory\nStorage\n32GB eMMC 5.1\nPower\n7.5W / 15 W\noperations (e.g., convolution, matrix multiplication and max-\npooling), especially those operations in the forward pass of\nsome deep learning models.\nWhen reporting the performance, we skip the ﬁrst three\ntraining steps, because they are often used by the TensorFlow\nruntime system to explore hardware architectures (e.g., cache\ncapacities and memory access latency) for performance opti-\nmization. The performance of the ﬁrst three training steps is\nnot representative of other training steps.\nB. Performance Analysis\nWe study the training performance from the following\nperspectives: hardware (CPU and GPU) utilization, power\nconsumption, and peak memory consumption.\nHardware Utilization\nFigure 3 shows the CPU and GPU utilization when we train\nthe deep learning model Inception V1. The ﬁgure shows the\nhardware utilization for three training steps. Since the NVIDIA\nTX2 includes 6 CPU cores, we use six subgraphs to show the\nutilization of each of the six cores: the ﬁrst two subgraphs\nshow the utilization of two Denver2 cores, and the rest of\nthem shows the utilization of four A57 cores. We have the\nfollowing observations.\nObservation 1: The GPU utilization is generally much\nhigher than the CPU utilization. In most of the times, the GPU\nutilization is close to 100%, while each CPU core utilization\nranges from 0% to 60%. Also, when the GPU utilization is\nhigh, the CPU utilization tends to be low, and vice versa.\nThis indicates that the workload is not balanced well between\nCPU and GPU. There seems a lack of effective coordination\nbetween CPU and GPU. This observation is general and exists\nin many deep learning models (e.g., Inception V1, DCGAN,\nResnet50, Xception).\nOur further investigation reveals that when the GPU utiliza-\ntion is low, CPU is either busy with data fetching from storage\n(SSD) to main memory, or working on small operations that\nare not worth to be ofﬂoaded to GPU due to the large data\ncopy overhead; When the GPU utilization is high, CPU is\nworking on a few small operations, and most of CPU cycles\nare wasted.\nSuch an observation also exists in servers, but the difference\nis that the utilization difference between CPU and GPU on\nservers tends to be larger [49], because GPU on servers are\nmuch more powerful than CPU on servers and hence more\noperations (after kernel fusing) tend to be scheduled on GPU.\n4\nFig. 2: Utilization of GPU and six CPU cores for training Inception V1.\nFig. 3: CPU and GPU utilization of different models.\nObservation 2: The utilization of GPU and each core in\nCPU is predictable. The utilization shows a periodical pattern\nwhere busy cycles alternate with less busy cycles. A period\nof the pattern corresponds to one time step. Across time\nsteps, such a pattern repeatedly appears. This indicates that\nthe computation across time steps remains stable and hence\nis highly predictable. This observation is consistent with the\nexisting work that leverages predictability of deep learning\nworkloads for performance optimization [50], [51].\nSuch an observation also exists in servers. Since this obser-\nvation is determined by the process of training deep learning\nmodels that repeatedly goes through a computation graph (and\nnot hardware architecture-related), this observation is general\nand independent of hardware architectures.\nObservation 3: The GPU utilization is sensitive to the batch\nsize, while the CPU utilization is not. Figure 4 shows the\nCPU and GPU utilization when the batch size changes. For\nDenseNet, the GPU utilization increases from 81.6% to 96.4%\nas the batch size changes from 4 to 64. For SqueezeNet and\nResNet50, the GPU utilization increase from 71.4% to 84.5%\nand from 85.6% to 95.6% respectively, as we increase the\nbatch size. However, for the CPU utilization, there is only\n2.1% difference on average across models.\nAs we increase the batch size, the memory footprint in-\ncreases and computation for operations also increases. Since\nGPU works on most computation-intensive operations during\nthe training, its utilization also increases as more computation\nrequires more thread-level parallelism. The CPU utilization\n5\nTABLE II: Descriptions for deep learning models in our evaluation\nModel\n#Layers\nDominant layer\n#Flops\n#Parameters\nTraining dataset\nBatch size\nSuccessful training?\nApplication domain\nDenseNet40 12 [32]\n40\nCONV\n30M\n1M\nCifar-10 [33]\n1-64\n✓\nComputer Vision\nResNet50 [34]\n50\nCONV\n4G\n98M\nCifar-10\n1-64\n✓\nComputer Vision\nSqueezeNet [35]\n40\nCONV\n837M\n5M\nCifar-10\n1-64\n✓\nComputer Vision\nVGG16 [36]\n16\nCONV\n4G\n134M\nCifar-10\n1-64\n✓\nComputer Vision\nXceptionNet [37]\n39\nCONV\nN/A\n23M\nCifar-10\n1-64\n✓\nComputer Vision\nInceptionV1 [38]\n22\nCONV\n30M\n5M\nCifar-10\n1-64\n✓\nComputer Vision\nChar-CNN [39]\n2\nLSTM+CONV\n23M\n1M\nShakespeare [40]\n1-64\n✓\nNatural Language Processing\nDCGAN [41]\n40\nCONV\n30M\n1M\nCifar-10\n1-64\n✓\nImage Generation\nDeep RL [42]\n4\nCONV\nN/A\nN/A\nAtari 2600 games [43]\n1-64\n✓\nRobotics Control\nAlexNet [44]\n8\nCONV\n727M\n60M\nCifar-10\n1-64\n✓\nComputer Vision\nVGG19 [36]\n19\nCONV\n20G\n548M\nCifar-10\n1-64\nx\nComputer Vision\nBERT [45]\n12\nEmbedding\nN/A\n110M\nSQuAD [46]\n1-64\nx\nNatural Language Processing\nResNet101 [34]\n101\nCONV\n8G\n155M\nCifar-10\n1-64\nx\nComputer Vision\nResNet152 [34]\n152\nCONV\n11G\n220M\nCifar-10\n1-64\nx\nComputer Vision\nDenseNet100 [32]\n100\nCONV\n31M\n7M\nCifar-10\n1-64\nx\nComputer Vision\nseq2seq [47]\n2\nLSTM\n28G\n348M\nIWSLT15 [48]\n1-64\nx\nNatural Language Processing\nFig. 4: CPU and GPU utilization of different models.\nFig. 5: Idle state ratio of six CPU cores for different models.\ndoes not increase very much, because CPU works on small\noperations and most of data objects in those operations can\nbe in caches. Slight increase of memory footprint due to the\nincrease of the batch size does not cause extra cache misses\nand dos not signiﬁcantly impact execution time.\nSuch an observation also exists in servers. But the variance\nof GPU utilization on servers does not change as much as\nthat on mobile devices, because GPU on servers have more\ncores and hence offers more thread-level parallelism to work\non increased computation as we increase the batch size. [51]\nObservation 4: Different cores have different utilization\nduring the training. TX2 has six heterogeneous cores: Two\nof them are Denver2 and four of them are A57. As Figure 5\nshows, the utilization of each core changes differently, as the\nbatch size increases. There is no obvious correlation between\nthe changes of the utilization across cores. We also notice that\nthe two Denver2 cores have the highest idle state ratio (as high\nas 65%) among all CPU cores, which indicates a large room\nfor performance improvement.\nWe do not have the above observation on servers, because\nservers (especially x86 servers) usually do not have heteroge-\nneous CPU cores [52].\nFigure4 shows the GPU utilization of different deep learn-\ning models from different application domains. The models\n6\nFig. 6: Energy consumption of different models.\nFig. 7: Power usage of different models in three iterations.\nfrom the domain of computer vision have the similar GPU\nutilization, hence we show Inception V1 as a representative\nof this domain. We choose other models including LSTM and\nDCGAN to represent different application domains.\nObservation 5: The GPU utilization varies on different\napplication domains. In Figure 4, we ﬁnd the LSTM model\n(the domain of natural language processing) has a low GPU\nutilization (only about 25%), while the models from the\ndomain of computer vision (e.g., ResNet) have higher GPU\nutilization (95%). Those models from computer vision has\nhigh GPU utilization, because they often employ convolution\nwhich is easy to leverage SIMT (Single Instruction Multiple\nThread) on GPU and reach high GPU utilization. In LSTM,\noperations often have dependency and there is lack of available\nthread-level parallelism. The observation is consistent with the\nexisting work [53]–[55] that LSTM has lower utilization than\ncomputer vision models.\nSuch an observation also exists in servers. Since the GPU\nutilization is heavily impacted by the application domain,\nObservation 5 is general and independent of hardware archi-\ntectures [15].\nPower and Energy Consumption\nFigures 7 and 6 show power and energy consumption\nof GPU, CPU, and memory. Figure 7 shows how power\nconsumption changes for three training steps. Figure 6 shows\nenergy consumption for one training step, when the batch\nsize changes. Energy consumption is calculated based on\nEquation 4 with the time interval of 5ms.\nObservation 6: GPU is a power-consuming hardware com-\nponent, but for some deep learning model, the memory con-\nsumes more power than GPU. Figure 4 shows that for the\ndomain of computer vision, GPU is the most time-consuming\nhardware component when we train deep learning models\n(especially CNN models) such as ResNet50 and VGG16. In\nthose models, GPU consumes 4× and 2× of power con-\nsumption of CPU and memory respectively. GPU consumption\ncan take up to 57.4% of the whole system power. Different\nfrom the above examples, the memory is the most power-\n7\nconsuming hardware component (not GPU), when we train\nLSTM. Compared with the CNN models, LSTM has relatively\nbad data locality and causes more intensive memory accesses.\nAs a result, the memory, shared between GPU and CPU, draws\nlarge power consumption.\nSuch an observation does not exist in servers. In servers,\nGPU (including its global memory) is the most power-\nconsuming (e.g., NVIDIA V100 takes up to 250 Watt (more\nthan half of the system power) when training LSTM, while\nthe memory (main memory) takes only 20%-30% of the total\nsystem power.)\nObservation 7: The power consumption across training\nsteps is predictable. Similar to the hardware utilization, the\npower consumption of hardware components shows a period-\nical pattern. This pattern is highly predictable across training\nsteps. Figure 7 shows such results. On servers, we have the\nsimilar observations.\nObservation 8: As we increase the batch size, the energy\nconsumption increases as well, but not in a proportional way.\nFigure 6 shows the results to support this observation. As we\nincrease the batch size from 4 to 64 (16x increase), the energy\nconsumption of the whole system increases as well. However,\nthe increase of the energy consumption is at least 2.2x and at\nmost 10.5x, less than 16x when we change the batch size.\nAlso, different models show quite different energy con-\nsumption. Among the ﬁve deep learning models for computer\nvision, DenseNet40 is the most energy-consuming one, while\nthe Squeezenet is the most energy efﬁcient one. The above\nconclusion is true as we run the training to completion\n(including all time steps). The above observation also exists\nin servers.\nConsistent with the results of power consumption, we notice\nthat for some models (e.g., DenseNet40), GPU is the most\nenergy-consuming one, while for LSMT, the memory is the\nmost energy-consuming one.\nPeak Memory Consumption\nThe memory is one of the key limiters for deep learning\nmodels training on mobile devices. Some large models, e.g.,\nResNet101 and VGG19, consume more than 10 GB memory\nfor training, while TX2 only has 8 GB memory. Those models\ncannot be trained on TX2. In our study, we aim to study\nthe impact of the batch size on the memory consumption of\ndeep learning models. Different from on servers, on mobile\ndevices we must carefully choose the batch size, not only for\ngood training accuracy as on servers, but also for acceptable\nmemory consumption.\nFor training (especially CNN and RNN), the memory is con-\nsumed by the following critical variables: parameters (includ-\ning weights and bias), gradients, input data, and intermediate\ndata. Among them, the intermediate data is the most memory\nconsuming. The intermediate data includes the work space and\nfeature map. The work space is the memory consumed by the\nmachine learning framework (e.g., TensorFlow or PyTorch).\nThe memory consumption of the work space varies for differ-\nent frameworks. The feature map, sitting in the middle of two\nneighbor layers of a CNN or RNN model, is generated by one\nlayer, and used as input of the next layer.\nObservation 9: Choosing a good batch size is critical to\nbe able to train deep learning models on mobile devices.\nFigure 9 shows memory usage as we change the batch size.\nAs expected, parameters, input data and gradients remain\nconstant, as we increase the batch size. But the memory\nconsumption of intermediate data increases signiﬁcantly, as we\nincrease the batch size. For example, for DenseNet40, when\nthe batch size increases from 4 to 64, the memory consumption\nof intermediate data increases from 2.2 GB to 5.9 GB. When\nwe use larger batch sizes (12nd 256), we run out of memory\nfor all models.\nThroughput\nTo quantify throughput, we employ a common metric, train-\ning samples per second, instead of using images per iteration\n(training step) as in some deep learning models, because of the\nfollowing two reasons. First, our collection of deep learning\nmodels includes CNN, RNN and Deep reinforcement learning\nmodels, which means that the training samples for some\nmodels are not images. For example, the training samples\nare sentences for some RNNs (e.g., seq2seq). Second, as we\nchange the batch size for evaluation, the number of training\nsamples for a training step changes as well, which indicates\nthat the execution time per training step (iteration) changes.\nHence, using “second” (the time metric) instead of “iteration”\nmakes more sense.\nObservation 10: Throughput increases as the batch size in-\ncreases. Figure 8 shows the throughput as we change the batch\nsize. For all models, the throughput increases as the batch size\nincreases. For example, for ResNet50, the throughput increases\nfrom 9 to 55 samples per second as the batch size increases\nfrom 4 to 64.\nThe above observation can also be seen in servers [15].\nObservation 11: Across models, throughput changes differ-\nently, as we increase the batch size. In Figure 8, the throughput\nof the deep reinforcement learning model increases from 889\nto 13,618 samples per second as the batch size increases\nfrom 4 to 64 (15.3x speedup). However, for DenseNet and\nResNet50, such throughput speedup is 1.7x and 6.1x, re-\nspectively. The deep reinforcement learning model has big\nthroughput speedup as we increase the batch size. This is\nbecause the training time of the deep reinforcement learning\ndoes not change too much, as we increase the batch size. As\na result, the throughput increases signiﬁcantly, as we increase\nthe batch size.\nThe above observation also exists on servers [15].\nStudy on modeling accuracy\nTraining a deep learning model on a mobile device is\ndifferent from that on a server, because training samples can\nbe dynamically generated when the mobile device is used.\nFor example, training DenseNet40 can be based on training\nsamples (images) collected at the user’s day time. In this\nevaluation, we evaluate a scenario where the user uses a mobile\ndevice to generate 64 images per day, and those images are\nused to train a deep learning model (DenseNet40). We also\n8\nFig. 8: Throughput of deep learning models.\nFig. 9: Memory usage of deep learning models.\nassume that the model, before started to be trained, is already\ntrained on a server, but needs to be trained further, using the\nuser’s new training samples. Figure 10 shows the variance\nof the accuracy, as we use the above training method for 17\ndays. As the day 0, the training accuracy is 60.65%, because\nthe model is already trained on a server.\nObservation 12: Training a deep learning model on a\nmobile device can slowly increase training accuracy. Fig-\nure 10 reveals that the accuracy of DenseNet40 increases\nfrom 60.65% to 61.32% (using three different batch sizes).\nThe above observation reveals that using the above method\nFig. 10: The accuracy variance as we increase training samples\nat a daily base to train DenseNet40.\ndoes slowly increases the accuracy. In this special scenario,\ndepending on whether the user has high requirement on the\nmodel accuracy, the training on the mobile device can continue\nas more training samples are collected or stop.\nV. DISCUSSION AND FUTURE RESEARCH DIRECTIONS\nFeasibility of training deep learning models on mobile\ndevices. Our work demonstrates the feasibility of training\nsome deep learning models on a mobile device. Most of the\nmodels we study are traditionally trained on a server, and\nseldom trained on any mobile device. Those deep learning\nmodels come from various application domains, and have\npotential to provide new services for mobile users.\nOur observation 9 reveals that choosing an appropriate batch\nsize has a big impact on whether training a deep learning\nmodel is feasible. Furthermore, it is well known that the\nbatch size has an impact on the model accuracy. Hence, there\nis a non-trivial tradeoff between the training feasibility and\n9\naccuracy on mobile devices. Such a tradeoff deserves further\nstudy.\nHardware utilization. Mobile devices often offer rich\nhardware heterogeneity (e.g., there are two types of CPU cores\nin the NIVIDA TX2), richer than x86 servers. However, such\nhardware heterogeneity is not leveraged well in the current\nmachine learning frameworks. This fact is pronounced by two\nobservations: (1) The utilization of all CPU cores is relatively\nlow (comparing with GPU); (2) The heterogeneity of CPU\ncores is completely ignored. As a result of such fact, the CPU\ncycles are wasted and the computation power of specialized\nCPU cores (e.g., ARM Cortex-A57) is not fully utilized.\nThe recent work from Facebook [49] reveals that the perfor-\nmance difference between CPU and GPU on mobile devices\nis smaller than that on servers. Based on this work and our\nobservations, we see great opportunities to improve the current\nscheduling mechanism in the machine learning frameworks.\nOnly using GPU for computation-intensive operations may not\nbe a good scheduling strategy. Instead, balancing workloads on\nCPU and GPU to maximize system throughput (for ﬁnishing\noperations) is a better one.\nEnergy consumption. Mobile devices are more sensitive\nto energy consumption than servers. Training deep learning\nmodels on mobile devices raises concerns on whether the\nbattery life is good enough to support training. Although the\nrecent work suggests to train deep learning models when\nthe mobile device is charged\n[16], [56], [57], the charging\ntime can be longer than the training time. The batter may\nstill be needed to ﬁnish training. Hence, minimizing energy\nconsumption during training is critical. Our observation reveals\nthat the memory can be more energy-consuming than CPU and\nGPU, when we train some deep learning networks. Reducing\nenergy consumption of the memory is necessary for mobile\ndevices. How to reduce energy consumption of the memory\nwithout impacting performance (execution time) is an open\ntopic.\nPredictability of workload characterization. The work-\nload of training deep learning networks is predictable, which\nmeans execution time, hardware utilization and power con-\nsumption show a repetitive pattern across training steps. Such\npredictability allows us to apply dynamic proﬁling on a few\ntraining steps to collect workload characterization, based on\nwhich we guide operation scheduling and power manage-\nment in the future training steps. Predictability of execution\ntime during the training has been leveraged in the existing\nwork [50], [51]. We expect to leverage the predictability of\nother characterization in the future work.\nVI. RELATED WORK\nPerformance optimization of deep learning model train-\ning. Some recent works [16]–[26], [58] have demonstrated the\npromise of training neural networks (NN) on mobile devices.\nThey are focused on exploring performance optimization in the\nperspectives of algorithm and system. For example, Mao et al.\n[58] implement a distributed mobile learning system that trains\na neural network by multiple devices of the same local network\nin parallel. They design a scheduler to adapt the training\nconﬁguration for heterogeneous mobile resources and net-\nwork circumstances. Bonawitz et al. [16] develop a federated\nlearning system to achieve NNs training on mobile platforms\nusing TensorFlow. Some practical issues have been addressed,\ne.g., local data distribution, unreliable device connectivity and\nlimited on-board resources. Koneˇcn`y et al. [19] use parameter\ncompression techniques to reduce the uplink communication\ncosts in federated learning. This paper is orthogonal to the\nabove works. Our comprehensive model proﬁling and analysis\ncan be used to develop more efﬁcient NN training schemes on\nmobile devices.\nZhu et al. [15] study the training performance and resource\nutilization of eight deep learning model models implemented\non three machine learning frameworks running on servers\n(not mobile devices) across different hardware conﬁgurations.\nHowever, they do not consider power and energy efﬁciency. In\ncontrast, our work is focused on deep learning models training\non mobile devices.\nProﬁling of deep neural network inference. Many works\nhave been conducted to analyze the performance and resource\nutilization of machine learning workloads (inference, not\ntraining) on mobile devices [4]–[10]. Lu et al. [4] measure\nthe performance and resource usage of each layer in CNNs\nrunning on mobile CPUs and GPUs. Based on the results\nof proﬁling and modeling, they implement a modeling tool\nto estimate the compute time and resource usage of CNNs.\nHowever, they only consider CNNs, but not RNNs or rein-\nforcement learning models which are also important for mobile\napplications. Hanhirova et al. [5] proﬁle the performance\nof multiple CNN-based models for object recognition and\ndetection on both embedded mobile processors and high-\nperformance server processors. They ﬁnd that there exists sig-\nniﬁcant latencythroughput trade-offs. Unfortunately, the above\nworks only study the inference of CNNs. On the contrary, we\nproﬁle and analyze the performance and resource requirements\nof CNNs, RNNs and deep reinforcement learning models\ntraining on mobile devices.\nVII. CONCLUSIONS\nTraining deep learning networks on mobile devices is\nemerging because of increasing computation power on mobile\nhardware and the advantages of enabling high user expe-\nriences. The performance characterization of training deep\nlearning models on mobile devices is largely unexplored,\nalthough understanding the performance characterization is\ncritical for designing and implementing deep learning mod-\nels on mobile devices. This paper is the ﬁrst work that\ncomprehensively studies the performance of training deep\nlearning network on a mobile device. Our study is based on\na set of proﬁling tools on mobile devices, and uses a set of\nrepresentative deep learning models from multiple application\ndomains. We reveal many research opportunities as a result of\nour study. We hope that our study can motivate future study\non optimizing performance of training deep learning networks\non mobile devices.\n10\nREFERENCES\n[1] N. D. Lane, P. Georgiev, and L. Qendro, “Deepear: robust smartphone\naudio sensing in unconstrained acoustic environments using deep learn-\ning,” in Proceedings of the 2015 ACM International Joint Conference\non Pervasive and Ubiquitous Computing, pp. 283–294, ACM, 2015.\n[2] A. Mathur, N. D. Lane, S. Bhattacharya, A. Boran, C. Forlivesi, and\nF. Kawsar, “Deepeye: Resource efﬁcient local execution of multiple deep\nvision models using wearable commodity hardware,” in Proceedings of\nthe 15th Annual International Conference on Mobile Systems, Applica-\ntions, and Services, pp. 68–81, ACM, 2017.\n[3] M. Xu, M. Zhu, Y. Liu, F. X. Lin, and X. Liu, “Deepcache: Principled\ncache for mobile deep vision,” in Proceedings of the 24th Annual Inter-\nnational Conference on Mobile Computing and Networking, pp. 129–\n144, ACM, 2018.\n[4] Z. Lu, S. Rallapalli, K. Chan, and T. La Porta, “Modeling the resource\nrequirements of convolutional neural networks on mobile devices,” in\nProceedings of the 25th ACM international conference on Multimedia,\npp. 1663–1671, ACM, 2017.\n[5] J. Hanhirova, T. K¨am¨ar¨ainen, S. Sepp¨al¨a, M. Siekkinen, V. Hirvisalo, and\nA. Yl¨a-J¨a¨aski, “Latency and throughput characterization of convolutional\nneural networks for mobile computer vision,” in Proceedings of the 9th\nACM Multimedia Systems Conference, pp. 204–215, ACM, 2018.\n[6] R. Adolf, S. Rama, B. Reagen, G.-Y. Wei, and D. Brooks, “Fathom:\nReference workloads for modern deep learning methods,” in 2016 IEEE\nInternational Symposium on Workload Characterization (IISWC), pp. 1–\n10, IEEE, 2016.\n[7] S. Shi, Q. Wang, P. Xu, and X. Chu, “Benchmarking state-of-the-art\ndeep learning software tools,” in 2016 7th International Conference on\nCloud Computing and Big Data (CCBD), pp. 99–104, IEEE, 2016.\n[8] “cnn-benchmarks.” https://github.com/jcjohnson/cnn-benchmarks.\n[9] “convnet-benchmark.” https://github.com/soumith/convnet-benchmarks.\n[10] “DeepBench.” https://github.com/baidu-research/DeepBench.\n[11] “Apple A12.” https://en.wikipedia.org/wiki/Apple A12.\n[12] “Snapdragon 855.” https://www.qualcomm.com/products/snapdragon-\n855-mobile-platform.\n[13] “Kirin 980.” https://en.wikichip.org/wiki/hisilicon/kirin/980.\n[14] “Nvidia Jetson Xavier.” https://developer.nvidia.com/embedded/buy/jetson-\nagx-xavier-devkit.\n[15] H. Zhu, M. Akrout, B. Zheng, A. Pelegris, A. Jayarajan, A. Phanishayee,\nB. Schroeder, and G. Pekhimenko, “Benchmarking and analyzing deep\nneural network training,” in 2018 IEEE International Symposium on\nWorkload Characterization (IISWC), pp. 88–100, IEEE, 2018.\n[16] K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman,\nV. Ivanov, C. Kiddon, J. Konecny, S. Mazzocchi, H. B. McMahan, et al.,\n“Towards federated learning at scale: System design,” arXiv preprint\narXiv:1902.01046, 2019.\n[17] V. Smith, C.-K. Chiang, M. Sanjabi, and A. S. Talwalkar, “Federated\nmulti-task learning,” in Advances in Neural Information Processing\nSystems, pp. 4424–4434, 2017.\n[18] T. Yang, G. Andrew, H. Eichner, H. Sun, W. Li, N. Kong, D. Ram-\nage, and F. Beaufays, “Applied federated learning: Improving google\nkeyboard query suggestions,” arXiv preprint arXiv:1812.02903, 2018.\n[19] J. Koneˇcn`y, H. B. McMahan, F. X. Yu, P. Richt´arik, A. T. Suresh, and\nD. Bacon, “Federated learning: Strategies for improving communication\nefﬁciency,” arXiv preprint arXiv:1610.05492, 2016.\n[20] H. Zhu and Y. Jin, “Multi-objective evolutionary federated learning,”\narXiv preprint arXiv:1812.07478, 2018.\n[21] Y. Zhao, M. Li, L. Lai, N. Suda, D. Civin, and V. Chandra, “Federated\nlearning with non-iid data,” arXiv preprint arXiv:1806.00582, 2018.\n[22] E. Jeong, S. Oh, H. Kim, J. Park, M. Bennis, and S.-L. Kim,\n“Communication-efﬁcient on-device machine learning: Federated dis-\ntillation and augmentation under non-iid private data,” arXiv preprint\narXiv:1811.11479, 2018.\n[23] X. Qi and C. Liu, “Enabling deep learning on iot edge: Approaches and\nevaluation,” in 2018 IEEE/ACM Symposium on Edge Computing (SEC),\npp. 367–372, IEEE, 2018.\n[24] W. Du, X. Zeng, M. Yan, and M. Zhang, “Efﬁcient federated learning\nvia variational dropout,” 2018.\n[25] A. K. Sahu, T. Li, M. Sanjabi, M. Zaheer, A. Talwalkar, and V. Smith,\n“On the convergence of federated optimization in heterogeneous net-\nworks,” arXiv preprint arXiv:1812.06127, 2018.\n[26] S. Wang, T. Tuor, T. Salonidis, K. K. Leung, C. Makaya, T. He, and\nK. Chan, “Adaptive federated learning in resource constrained edge\ncomputing systems,” learning, vol. 8, p. 9, 2018.\n[27] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\nS. Ghemawat, G. Irving, M. Isard, et al., “Tensorﬂow: a System for\nLarge-scale Machine Learning,” in USENIX Symposium on Operating\nSystems Design and Implementation, 2016.\n[28] Adam Paszke and Sam Gross and Soumith Chintala and Gregory\nChanan, “PyTorch.” https://pytorch.org/.\n[29] “Nvidia\nNvprof.”\nhttps://docs.nvidia.com/cuda/proﬁler-users-\nguide/index.html.\n[30] “Nvidia Tegrastats.” https://docs.nvidia.com/jetson/l4t/index.html.\n[31] “TensorFlow Proﬁler.” https://www.tensorﬂow.org/api docs.\n[32] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely\nconnected convolutional networks,” in Proceedings of the IEEE confer-\nence on computer vision and pattern recognition, pp. 4700–4708, 2017.\n[33] A. Krizhevsky and G. Hinton, “Learning multiple layers of features from\ntiny images,” tech. rep., Citeseer, 2009.\n[34] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 770–778, 2016.\n[35] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally,\nand K. Keutzer, “Squeezenet: Alexnet-level accuracy with 50x fewer\nparameters and¡ 0.5 mb model size,” arXiv preprint arXiv:1602.07360,\n2016.\n[36] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.\n[37] F. Chollet, “Xception: Deep learning with depthwise separable convolu-\ntions,” in Proceedings of the IEEE conference on computer vision and\npattern recognition, pp. 1251–1258, 2017.\n[38] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,\nV. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,”\nin Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pp. 1–9, 2015.\n[39] X. Zhang, J. Zhao, and Y. LeCun, “Character-level convolutional\nnetworks for text classiﬁcation,” in Advances in neural information\nprocessing systems, pp. 649–657, 2015.\n[40] A. Singh and X. J. Zhu, eds., Proceedings of the 20th International\nConference on Artiﬁcial Intelligence and Statistics, AISTATS 2017, 20-\n22 April 2017, Fort Lauderdale, FL, USA, vol. 54 of Proceedings of\nMachine Learning Research, PMLR, 2017.\n[41] A. Radford, L. Metz, and S. Chintala, “Unsupervised representation\nlearning with deep convolutional generative adversarial networks,” arXiv\npreprint arXiv:1511.06434, 2015.\n[42] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,\net al., “Human-level control through deep reinforcement learning,”\nNature, vol. 518, no. 7540, p. 529, 2015.\n[43] M. G. Bellemare, J. Veness, and M. Bowling, “Investigating contingency\nawareness using atari 2600 games,” in Twenty-Sixth AAAI Conference\non Artiﬁcial Intelligence, 2012.\n[44] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\nwith deep convolutional neural networks,” in Advances in neural infor-\nmation processing systems, pp. 1097–1105, 2012.\n[45] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” arXiv\npreprint arXiv:1810.04805, 2018.\n[46] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “Squad: 100,000+\nquestions\nfor\nmachine\ncomprehension\nof\ntext,”\narXiv\npreprint\narXiv:1606.05250, 2016.\n[47] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning\nwith neural networks,” in Advances in neural information processing\nsystems, pp. 3104–3112, 2014.\n[48] “IWSLT Evaluation 2015,” https://sites.google.com/site/iwsltevaluation2015/.\n[49] C.-J. Wu, D. Brooks, K. Chen, D. Chen, S. Choudhury, M. Dukhan,\nK. M. Hazelwood, E. Isaac, Y. Jia, B. Jia, et al., “Machine learning at\nfacebook: Understanding inference at the edge.,” in HPCA, pp. 331–344,\n2019.\n[50] M. Sivathanu, T. Chugh, S. S. Singapuram, and L. Zhou, “Astra:\nExploiting Predictability to Optimize Deep Learning,” in International\nConference on Architectural Support for Programming Languages and\nOperating Systems, 2019.\n11\n[51] J. Liu, D. Li, G. Kestor, and J. Vetter, “Runtime Concurrency Control and\nOperation Scheduling for High Performance Neural Network Training,”\nin International Symposium on Parallel and Distributed Systems, 2019.\n[52] S. Mittal and J. S. Vetter, “A survey of cpu-gpu heterogeneous computing\ntechniques,” ACM Computing Surveys (CSUR), vol. 47, no. 4, p. 69,\n2015.\n[53] M. Zhang, S. Rajbhandari, W. Wang, and Y. He, “Deepcpu: Serving\nrnn-based deep learning models 10x faster,” in 2018 {USENIX} Annual\nTechnical Conference ({USENIX}{ATC} 18), pp. 951–965, 2018.\n[54] J. Liu, H. Zhao, M. A. Ogleari, D. Li, and J. Zhao, “Processing-in-\nmemory for energy-efﬁcient neural network training: A heterogeneous\napproach,” in 2018 51st Annual IEEE/ACM International Symposium on\nMicroarchitecture (MICRO), pp. 655–668, IEEE, 2018.\n[55] Z. Jia, M. Zaharia, and A. Aiken, “Beyond data and model parallelism\nfor deep neural networks,” arXiv preprint arXiv:1807.05358, 2018.\n[56] M. Mohri, G. Sivek, and A. T. Suresh, “Agnostic federated learning,”\nCoRR, vol. abs/1902.00146, 2019.\n[57] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, et al.,\n“Communication-efﬁcient learning of deep networks from decentralized\ndata,” arXiv preprint arXiv:1602.05629, 2016.\n[58] J. Mao, Z. Qin, Z. Xu, K. W. Nixon, X. Chen, H. Li, and Y. Chen,\n“Adalearner: An adaptive distributed mobile learning system for neural\nnetworks,” in 2017 IEEE/ACM International Conference on Computer-\nAided Design (ICCAD), pp. 291–296, IEEE, 2017.\n12\n",
  "categories": [
    "cs.LG",
    "cs.PF",
    "stat.ML"
  ],
  "published": "2019-06-10",
  "updated": "2019-09-07"
}