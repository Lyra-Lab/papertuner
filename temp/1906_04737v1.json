{
  "id": "http://arxiv.org/abs/1906.04737v1",
  "title": "Dealing with Non-Stationarity in Multi-Agent Deep Reinforcement Learning",
  "authors": [
    "Georgios Papoudakis",
    "Filippos Christianos",
    "Arrasy Rahman",
    "Stefano V. Albrecht"
  ],
  "abstract": "Recent developments in deep reinforcement learning are concerned with\ncreating decision-making agents which can perform well in various complex\ndomains. A particular approach which has received increasing attention is\nmulti-agent reinforcement learning, in which multiple agents learn concurrently\nto coordinate their actions. In such multi-agent environments, additional\nlearning problems arise due to the continually changing decision-making\npolicies of agents. This paper surveys recent works that address the\nnon-stationarity problem in multi-agent deep reinforcement learning. The\nsurveyed methods range from modifications in the training procedure, such as\ncentralized training, to learning representations of the opponent's policy,\nmeta-learning, communication, and decentralized learning. The survey concludes\nwith a list of open problems and possible lines of future research.",
  "text": "Dealing with Non-Stationarity in Multi-Agent Deep Reinforcement Learning\nGeorgios Papoudakis , Filippos Christianos , Arrasy Rahman , Stefano V. Albrecht\nSchool of Informatics, The University of Edinburgh\n{g.papoudakis, f.christianos, arrasy.rahman, s.albrecht}@ed.ac.uk\nAbstract\nRecent developments in deep reinforcement learning\nare concerned with creating decision-making agents\nwhich can perform well in various complex domains.\nA particular approach which has received increasing\nattention is multi-agent reinforcement learning, in\nwhich multiple agents learn concurrently to coordi-\nnate their actions. In such multi-agent environments,\nadditional learning problems arise due to the contin-\nually changing decision-making policies of agents.\nThis paper surveys recent works that address the\nnon-stationarity problem in multi-agent deep rein-\nforcement learning. The surveyed methods range\nfrom modiﬁcations in the training procedure, such\nas centralized training, to learning representations\nof the opponent’s policy, meta-learning, commu-\nnication, and decentralized learning. The survey\nconcludes with a list of open problems and possible\nlines of future research.\n1\nIntroduction\nDeep learning has revolutionized the development of agents\nwhich can act autonomously in complex environments. Tra-\nditional reinforcement learning (RL) methods, using tabular\nrepresentations or linear function approximators, are difﬁcult\nto scale to high-dimensional environments, and are mostly\napplied to small grid worlds or environments that provide a\nhigh-level state representation. The combination of deep learn-\ning with existing RL algorithms enabled the development of\nagents that act in environments with larger state spaces, such\nas images. Recently, some promising deep RL algorithms that\ncan handle large state environments both with continuous and\ndiscrete action spaces have been proposed (Mnih et al., 2015;\nSchulman et al., 2017; Fujimoto et al., 2018).\nMulti-agent systems consist of multiple agents acting and\nlearning in a shared environment. Many real-world decision\nmaking problems can be modeled as multi-agent systems,\nsuch as autonomous vehicles, resource allocation problems,\nrobot swarms, and human-robot interaction. Despite the re-\ncent success of deep RL in single-agent environments, there\nare additional challenges in multi-agent RL. One major chal-\nlenge, and the focus of this survey, is the non-stationarity of\nmulti-agent environments created by agents that change their\npolicies during the training procedure. This non-stationarity\nstems from breaking the Markov assumption that governs most\nsingle-agent RL algorithms. Since the transitions and rewards\ndepend on actions of all agents, whose decision policies keep\nchanging in the learning process, each agent can enter an end-\nless cycle of adapting to other agents.\nAdditional problems in multi-agent systems include multi-\nagent credit assignment, partial observability, and heterogene-\nity. In the ﬁrst problem, only a subset of agents contribute\nto a reward, and we need to identify and reward them while\navoiding punishing agents that acted optimally. Partial ob-\nservability consists of agents having access only to their local\nobservations and not the actual state of the environment, which\ncan signiﬁcantly hinder training performance. Heterogeneity\nrefers to the fact that agents may have different sensing and act-\ning capabilities, and that their learning algorithms may differ\n(Albrecht and Ramamoorthy, 2012).\nMulti-agent RL is a widely studied topic with surveys rang-\ning from modelling other agents (Albrecht and Stone, 2018)\nto transfer learning (Da Silva and Costa, 2019) and non-\nstationarity (Hernandez-Leal et al., 2017). However, there\nis a recent focus on multi-agent deep RL which is not dis-\ncussed in the above surveys. In this work, we aim to consoli-\ndate progress in the area, and discuss non-stationarity in deep\nmulti-agent RL settings. The survey concludes by discussing\nopen problems and possible future research directions.\n2\nBackground\nIn this section, we will brieﬂy refer to deﬁnitions and previ-\nous works on RL, and we will formulate the non-stationarity\nproblem of multi-agent RL.\n2.1\nMarkov Decision Processes\nWe can mathematically model a decision making problem\nusing Markov Decision Processes (MDP). An MDP is deﬁned\nas (S, A, r, T), where S is the set of possible states, A is the set\nof available actions, r : S ×A×S →R is the reward function\nand T : S ×A×S →[0, 1] is the probability distribution over\nnext states given the current state and action. Additionally, in\nevery MDP a stochastic policy function π : S × A →[0, 1]\nor a deterministic policy function µ : S →R|A| is deﬁned to\nselect an action for the current state.\nGiven a policy π, an MDP has two value functions. The\nstate value function V : S →R is the expected sum of\narXiv:1906.04737v1  [cs.LG]  11 Jun 2019\ndiscounted rewards from a given state under the policy π;\nV π(s) = Eπ[PN\nt=0 γtrt|s], and the state-action value func-\ntion Q : S × A →R is the expected sum of discounted\nrewards from a given state and action under the policy π;\nQπ(s, a) = Eπ[PN\nt=0 γtrt|s, a]. In the equations γ is the dis-\ncount factor, taking values in [0, 1], and rt is the reward after t\ntime steps.\nSolving an MDP requires computing the optimal policy.\nThat is the policy that maximizes the expected discounted sum\nof rewards V ∗(s) = maxπ V π(s). If access to the reward and\ntransition function is available, this problem can be solved by\niterating the Bellman equations and using dynamic program-\nming. However, these two functions are unknown in most\nscenarios. In this case, RL approaches are used to compute\nthe optimal policy.\n2.2\nReinforcement Learning Methods\nRL consists of multiple methods for solving MDPs. Tem-\nporal difference (TD) methods work by estimating the value\nfunctions (or the state-action value function) by interacting\nwith the environment. The greedy policy is computed using\nthe following expression: π(s) = arg maxa Q(s, a). A com-\nmonly used method is Q-learning (Watkins and Dayan, 1992).\nThe state-action value function (Q-values) can be expressed\nin tabular form (storing a separate value for every state and\naction) or can be approximated using a parameterized function.\nIt is then updated by minimizing the following loss function:\nL = 1\n2(r + γ maxa′ Q(s′, a′) −Q(s, a))2. This loss can be\nminimized using a gradient optimizer, either with respect to\nQ-values (in the tabular case) or the parameters.\nAn alternative model-free approach are policy gradient (PG)\nmethods. Given a stochastic or a deterministic policy parame-\nterized by parameters θ, the goal is to compute the policy that\nmaximizes the expected discounted sum of rewards from the\nﬁrst state. Therefore, the objective function is J = V π(s0).\nUsing the policy gradient theorem, the gradient of the objective\nwith respect to the parameters of the policy can be computed\nby maximizing the objective using a gradient-based optimizer.\nIn the case of stochastic policy (Sutton et al., 2000) ∇θJ =\nE[Gt∇θ log πθ(a|s)]. In the case of deterministic policy (Sil-\nver et al., 2014) ∇θJ = E[∇θµ(s)∇aQ(ts, a)|a=µ(s)].\nIn the tabular case or when combined with linear approxi-\nmators, both TD and PG methods are difﬁcult to scale to large\naction and state spaces. Thus, deep networks have been used\n(Mnih et al., 2015) to address this issue. Mnih et al. (2015)\nproposed experience replay and the use of target networks\nin order to deal with instability issues that come with deep\nnetworks.\n2.3\nMarkov Games\nMarkov games (Littman, 1994) are a generalization of MDP\nto multi-agent settings.\nA Markov game is deﬁned as\n(I, S, A, r, T) where, I is the set of N agents, S is the set\nof states, A = A1 × A2... × AN is the set of actions of all the\nagents, r = (r1, r2, ..., rN), where ri : S × A × S →R is\nthe reward function of agent i. Usually, in cooperative setting\nr1 = r2... = rN. T : S × A × S →[0, 1] is the probability\ndistribution over next states, given the current state and the\nactions of all agents.\nIn this survey, we assume that each agent has ac-\ncess only to a local observation o and not to the full\nstate of the environment.\nAs a result, the policy of\nthe agents is conditioned on their history of observations\nhi = [oi,t, oi,t−1, oi,t−2, ...]. Given that the history of all\nagents H = [h1, h2, ..., hN], the joint policy is described\nas π(a|H) = (π1(a1|h1), π2(a2|h2), ..., πN(aN|hN)).\nA\ncommon simpliﬁcation is to condition the policy of each\nagent’s only on the most recent local observation, π(a|ot) =\n(π1(a1|o1,t), π2(a2|o2,t), ..., πN(aN|oN,t)). However, condi-\ntioning on the history of observations experimentally leads to\nbetter results in partially observable environment. Similarly\nto MDP, the goal of each agent is to maximize its expected\ndiscounted sum of rewards V πi\ni (hi) = E[P\ni=0 γiri|hi].\n2.4\nCentralized and Decentralized Architectures\nTwo main architectures can be used for learning in multi-agent\nsystems. The ﬁrst architecture is centralized learning. In cen-\ntralized learning, agents are jointly modelled and a common\ncentralized policy for all the agents is trained. The input to\nthe network is the concatenation of the observation of all the\nagents, and the output is the combination of all the actions.\nThe most important issue of centralized architectures is the\nlarge input and output space. The input increases linearly and\nthe output exponentially with respect to the number of agents.\nOn the other hand, in decentralized learning, the agents\nare trained independently from the others. Each agent has its\npolicy network, takes a local observation as input and outputs\nan action. Although this method does not suffer from scala-\nbility issues, many different problems arise as well. Some of\nthese issues are non-stationarity of the environment, the credit\nassignment problem and lack of explicit coordination. In this\nsurvey, we focus on works that attempt to handle issues related\nto non-stationarity.\n2.5\nThe Non-Stationarity Problem\nIn Markov games, the state transition function T and the re-\nward function of each agent ri depend on the actions of all\nagents. During the training of multiple agents, the policy of\neach agent changes through time. As a result, each agents’ per-\nceived transition and reward functions change as well. Single-\nagent RL procedures which commonly assume stationarity of\nthese functions might not quickly adapt to such changes.\nAs an example, consider a simple game like repeated Rock-\nPaper-Scissors. In it, two agents simultaneously perform ac-\ntions ai ∈{apaper, arock, ascissors} and receive their respec-\ntive rewards. The agents, seeking to maximize their reward,\nlearn a policy πi using the interaction history. Ideally, the\npolicy would learn to play actions that beat the opponent’s\nmost used actions. For instance, if agent 1 showed a pref-\nerence to apaper then π2 would learn to prioritize ascissor.\nSubsequently, agent 2 accumulates a history with that action,\nto which agent 1 reacts by changing its policy accordingly.\nThis process may continue in this fashion, leading to what we\nrefer to as non-stationarity due to changing behaviors.\n3\nDealing with Non-Stationarity\nThe following subsections survey various approaches that have\nbeen proposed to tackle non-stationarity in multi-agent deep\nRL. These approaches range from using modiﬁcations of stan-\ndard RL training methods to computing and sharing additional\nopponent information. Details are summarized in Table 1.\n3.1\nCentralized Critic Techniques\nA step toward dealing with non-stationarity is the centralized\ncritic architecture. For this architecture, an actor-critic algo-\nrithm is used, which consists of two components. The critics’\ntraining is centralized and has access to the observations and\nactions of all agents, while the actors’ training is decentralized.\nSince the actor computes the policy, the critic component can\nbe removed during testing, and therefore the approach has fully\ndecentralized execution. By having access to the observations\nand the actions of the opponent during training, the agents\ndo not experience unexpected changes in the dynamics of the\nenvironment, which results in stabilization of the procedure.\nFoerster et al. (2018b) used the actor-critic algorithm with\nstochastic policies to train agents and evaluated their method in\nStarcraft. The authors proposed a single centralized critic for\nall the agents and a different actor for each agent. Additionally,\nthey proposed a modiﬁcation in the advantage estimation of the\nactor-critic A(s, a) = Q(s, a)−P\na′\ni πi(a′\ni|oi)Q(s, (a−i, a′\ni)).\nThis modiﬁcation serves two purposes. First of all, the ad-\nvantage estimation, which is used in the policy gradient, is\nconditioned on the observations and actions of all the agents.\nSince each agent has access to the observations and actions\nof all the other agents, the policy gradient estimation is con-\nditioned on the policy of the other agents and therefore the\nnon-stationarity is addressed. Additionally, this counterfactual\nadvantage can be interpreted as the value of action a compared\nto all the other action values. As a result, this advantage might\nbe utilized to address the credit assignment problem, and this\nis the core contribution of the paper.\nLowe et al. (2017) proposed a multi-agent architecture using\nthe deterministic policy gradient (Silver et al., 2014) algorithm\n(MADDPG). In this method, each agent uses a centralized\ncritic and a decentralized actor. Since the training of each\nagent is conditioned on the observation and action of all the\nother agents, each agent perceives the environment as station-\nary. Another extension of MADDPG is MiniMax MADDPG\n(M3DDPG) (Li et al., 2019), which uses Minimax Q-learning\n(Littman, 1994) in the critic to exhibit robustness against dif-\nferent opponents with altered policies.\n3.2\nDecentralized Learning Techniques\nHandling non-stationarity in multi-agent systems does not nec-\nessarily require centralized training techniques. An alternative\ndecentralized approach that has been explored to handle non-\nstationarity in multi-agent deep RL problems is self-play. This\napproach trains a neural network, using each agents’ own ob-\nservation as input, by playing it against its current or previous\nversions to learn policies that can generalize to any opponents.\nThis approach can be traced back to the early days of TD-\nGammon (Tesauro, 1995) which managed to win against the\nhuman champion in Backgammon. More recently, self-play\nwas extended to more complex domains such as Go (Silver et\nal., 2017) and even complex locomotion environments with\ncontinuous state and action space (Bansal et al., 2018).\nIn TD-Gammon, the neural networks are trained using tem-\nporal difference methods to predict the outcome of a game.\nUnlike recent approaches, self-play in TD-Gammon only plays\nagainst the current parameter setting of the neural network. It\nis noted in their work (Tesauro, 1995) that using self-play in\nthis way might result in a lack of exploration during training\ndue to the neural networks always choosing the same sequence\nof actions in different episodes in self-play. In their case, this\nproblem did not occur due to the dynamics in Backgammon\nbeing stochastic.\nIn more recent applications of self-play (Silver et al., 2017;\nBansal et al., 2018), an additional modiﬁcation to the train-\ning process was made to ensure that the training is effective\nin environments with deterministic dynamics. In this case,\nrecent self-play approaches stored the neural network param-\neters at different points during learning. Subsequently, the\nopponent during the self-play process is chosen by randomly\nchoosing between the current and previous versions of neural\nnetwork parameters. Apart from extending self-play to de-\nterministic environments, this allowed the neural network to\ngeneralize against a broader range of opponents. As a result,\nself-play managed to train policies that can generalize well in\nenvironments like Go (Silver et al., 2017) and even complex\nlocomotion tasks (Bansal et al., 2018).\nAnother technique which has been used to allow decentral-\nized training is by stabilizing experience replay. Despite play-\ning an essential part in single-agent deep RL approaches (Mnih\net al., 2015), due to environments’ non-stationarity, experience\nreplay might store experiences that are no longer relevant for\ndecentralized learning which results in worse performance.\nFoerster et al. (2017) proposed importance sampling correc-\ntions to adjust the weight of previous experience to current\nenvironment dynamics. When combined with independent\nQ-learning (Tan, 1993), this resulted in signiﬁcantly better\nperformance in a speciﬁc task in the Starcraft game.\n3.3\nOpponent Modelling\nAnother feasible direction that deals with non-stationarity is\nopponent modelling. By modelling the intentions and poli-\ncies of other agents, the training process of the agents might\nbe stabilized. Modelling other agents in multi-agent systems\nhas been widely studied and offers many research opportu-\nnities (Albrecht and Stone, 2018). In this survey, we mostly\nfocus on recent methods that learn models of opponents or use\nthem to condition the agents’ policy on them.\nRaileanu et al. (2018) suggested an approach where agents\nuse their policy to predict the behaviour of other agents. This\nmethod employs an actor-critic architecture and reuses the\nsame network for estimating the goals of the other agents. In\nmore details, a network f(ss/o, zs, ¯zo), with the inputs being\nthe state, the goal of the agent and the goal of the other agent\nrespectively, is being used in a forward pass to decide on an\naction. However, the same network is also used by switching\nthe order of zs and ¯zo to infer the goal of the other agent. Ob-\nserving the actual actions of the opponent allows the agent to\nback-propagate and optimize the vector of trainable param-\neters ¯zo. In contrast, He et al. (2016) developed a second,\nseparate network to encode the opponent’s behaviour. The\ncombination of the two networks is done either by concatenat-\ning their hidden states or by the use of a mixture of experts.\nThis separate network enables faster learning and even allows\nmodelling of changing opponent behavior.\nZhang and Lesser (2010) and Foerster et al. (2018a) pro-\nposed a modiﬁcation of the optimization function in policy\ngradient methods to incorporate the learning procedure of the\nopponents in the training of their agent. Given two agents with\nparameters θ1 and θ2 respectively, the authors proposed the op-\ntimization of V1(θ1, θ2 + ∆θ2), where ∆θ2 = ∇θ2V2(θ1, θ2)\ninstead of the standard V1(θ1, θ2). In this way, the training\nagent has access to the learning trajectory of the opponents,\nand therefore, the training procedure does not suffer from non-\nstationarity. Zhang and Lesser (2010) assumed that the term\n∆θ2 is not differentiable with respect to θ1 and they proved\nconvergence to Nash equilibrium in 2-player 2-action games.\nOn the other hand, Foerster et al. (2018a) proposed Learn-\ning with Opponent Learning Awareness (LOLA) where the\nterm ∆θ2 is differentiable with respect to θ1 to exploit the\nopponent learning dynamics. LOLA experimentally led to\ntit-for-tat behaviour in a different number of games and suc-\ncessfully managed to cooperate in the Independent Prisoners\nDilemma (IPD). Most works, such as Bowling and Veloso\n(2001) deviate in the IPD settings.\nIn order to be able to keep the stability of Zhang and\nLesser (2010) and the opponent dynamics exploitation of\nFoerster et al. (2018a), Letcher et al. (2019) proposed\nStable Opponent Shaping (SOS). Letcher et al. (2019) pro-\nvided examples of differentiable games, where Stable Fixed\nPoints exhibit better behaviour than Nash equilibrium and ex-\namples where LOLA fails to converge in an Stable Fixed Point.\nFor this reason, the authors proposed a partial stop-gradient\noperator, which controls the trade-off between the two meth-\nods. SOS has guarantees of convergence, while at the same\ntime, it results in the same or better performance than LOLA.\nAnother facet of opponent modelling that was enabled by\nthe recent advances in training neural networks is related to\nlearning representations for multi-agent learning. Approaches\nthat fall under this category learn the representations by impos-\ning a certain model structure to compute the representations.\nArchitectures such as graph neural networks (Tacchetti et al.,\n2019), feed-forward neural networks (Grover et al., 2018),\nand recurrent neural networks (Rabinowitz et al., 2018) can\nbe used to produce the representations.\nSpeciﬁc loss functions are used along with gradient-based\noptimization to train these models to output representations\nwhich can predict speciﬁc information of the opponents such\nas their actions (Rabinowitz et al., 2018; Grover et al., 2018) or\nreturns (Tacchetti et al., 2019) received by the modelled agent.\nThe representation models can be trained using loss functions\ncommonly used in supervised learning. Additionally, Grover\net al. (2018) provided an example loss function that combines\naction prediction loss while also maximizing the difference\nbetween representations of different agents’ policies.\nThe representation networks are provided with opponents’\nobservations as input during learning. The policy networks are\nthen trained by receiving agent observations which have been\naugmented with output representations from the representation\nnetworks as input. Under the assumption that these trained\nrepresentation models could generalize to opponents that have\nyet been encountered, these models should be able to provide\nadditional information which might characterize opponents’\npolicy. Empirically, this produced an increased performance\nagainst either learning (Rabinowitz et al., 2018) or stationary\nopponents in various learning environments.\n3.4\nMeta-Learning\nBefore advances in deep RL, approaches like tracking (Sutton\net al., 2007) and context detection (Da Silva et al., 2006)\nwere proposed to allow quicker adaptation in non-stationary\nenvironments. Both approaches adopt a more reactive view on\nhandling non-stationarity by using learning approaches that\nattempt to quickly change the policy or environment models\nonce changes in environment dynamics occur. However, the\nresults of locomotion tasks proposed by Finn et al. (2017)\nhighlighted how reactive approaches such as tracking still\ncannot produce a quick adaptation of deep RL algorithms to\nchanging dynamics using only a few learning updates.\nInstead of formulating a learning algorithm which can train\ndeep neural networks to react to changing environment dy-\nnamics, another approach is to anticipate the changing dy-\nnamics. An optimization process can then be formulated to\nﬁnd initial neural network parameters that, given the chang-\ning dynamics, can learn using small amounts of learning up-\ndates. Meta-learning approaches like Model Agnostic Meta\nLearning (MAML) (Finn et al., 2017) speciﬁcally addresses\noptimization for this particular problem. Al-Shedivat et al.\n(2018) further extended MAML to handle non-stationarity in\nmulti-agent problems.\nThe proposed method by Al-Shedivat et al. (2018) pro-\nvided an optimization process to search for initial neural\nnetwork parameters, θ, which can quickly adapt to non-\nstationarity. The optimization process ﬁrst generates roll-\nout data, τTi, from task Ti.\nIt then updates θ according\nto a policy gradient update rule to produce updated param-\neters φ. The method subsequently runs the policy param-\neterized by φ in the following task, Ti+1, which results\nin a performance denoted by Ri+1(φ). The proposed ap-\nproach ﬁnally utilizes a policy gradient algorithm to search\nfor θ which maximizes the expected performance after up-\ndate, EτTi∼PT (τ|θ)\nh\nEτTi+1∼PT +1(τ|φ) [Ri+1(φ)|τTi, θ]\ni\n. By\nexplicitly optimizing the initial model parameters based on\ntheir expected performance after learning, the proposed meta-\nlearning approach was able to signiﬁcantly outperform adapta-\ntion methods such as tracking and other meta-learning adap-\ntation strategies which have performed well in single-agent\nenvironments. This was tested in iterated adaptation games\nwhere an agent repeatedly play against the same opponent\nwhile only allowed to learn in between each game.\n3.5\nCommunication\nFinally, the last category of methods that we discuss for han-\ndling non-stationarity is communication. Through communi-\ncation, the different training agents can exchange information\nabout their observations, actions and intentions to stabilize\ntheir training. While communication in multi-agent systems\nis a well-studied topic, we will focus on recent methods that\nlearn to communicate using multi-agent deep RL.\nA step in this direction is the work of Foerster et al. (2016b),\nwho proposed the Deep Distributed Recurrent Q-Networks, an\narchitecture where all the agents share the same hidden layers\nand learn to communicate to solve riddles. In the same direc-\ntion, Sukhbaatar et al. (2016) proposed CommNet, an architec-\nture, where the input to each hidden layer hi\nj = f(hi\nj−1, ci\nj−1)\nfor each agent i, is the previous layer hi\nj−1 and a communi-\ncation message ci\nj−1 =\n1\nI−1\nP\ni′ hi\nj−1, which is the average\nof the previous hidden layers of all the other agents. There-\nfore, the agents learn to communicate by sharing the extracted\nfeatures of their observations in some cooperative scenarios.\nSingh et al. (2019) proposed the Individualized Controlled\nContinuous Communication Model (IC3NET), which is an\nextension of CommNet in competitive setting. In IC3NET,\nthere is an additional communication gate, which either al-\nlows or blocks communication between agents, for example\nin competitive settings.\nAll of the previous approaches assume that all agents have\naccess to the hidden layers of the other agents. An elimination\nof this constraint was proposed by Foerster et al. (2016a). The\nauthors ﬁrst suggested the Reinforced Inter-Agent Learning,\nwhere each agent has two Q-networks. The ﬁrst network out-\nputs an action and the second a communication message which\nis fed in the Q-networks of the other agents. Both networks\nare trained using DQN. In the same work, they also developed\nthe differentiable inter-agent learning, where they train only\nthe action network with DQN, while they push the gradients\nof the other agents, through the communication channel to the\ncommunication network. This approach is similar to the work\nof Mordatch and Abbeel (2018), where the authors proposed\na model that takes as input the messages of other agents and\nlearns to output an action and a new communication message.\n4\nOpen Problems\nBased on the methods outlined in this survey, we identify\nseveral open problems with regards to non-stationarity and\npossible lines of future research.\n4.1\nTransfer Learning for Non-Stationarity\nSeveral transfer learning approaches for multi-agent systems\nhave been covered in this survey. In this case, the learned rep-\nresentations and initialization values from meta-learning (Al-\nShedivat et al., 2018) and approaches which learn opponent\nrepresentations (Grover et al., 2018; Tacchetti et al., 2019; Ra-\nbinowitz et al., 2018) can be viewed as transferred knowledge\nwhich might result in quicker adaptation to non-stationarity.\nDespite recent advances, there are still open questions regard-\ning the form of the knowledge being transferred and how to\nleverage them for quicker adaptation.\n4.2\nOpen Multi-Agent Systems\nIn real-world problems, the number of agents in an environ-\nment can be large and diverse. Furthermore, the number of\nagents might change due to agents leaving or entering the\nenvironment. This problem setting is often termed an open\nmulti-agent system (Chandrasekaran et al., 2016). The change\nin the number of agents can cause an action to have different\nconsequences at different points during learning. For example,\na certain action might lead to situations with high returns when\nanother cooperative agent is in the environment, while also be-\ning inconsequential when the other agent left the environment.\nNone of the techniques presented in this survey were tested\nin environments with changing number of agents. In general,\ntransfer learning approaches that can reuse knowledge between\nproblems with a varying number of agents might be a potential\navenue of research in this topic. Furthermore, research on how\nagents can effectively deal with heterogeneity in capabilities\nand learning algorithms (Albrecht and Ramamoorthy, 2012)\nwill play an important part.\n4.3\nLimited Access to Opponent Information\nA large number of the works that we presented in opponent\nmodelling, such as (He et al., 2016; Grover et al., 2018),\nrequires access to the opponent’s observations and chosen ac-\ntions. While this is not a strong assumption during centralized\ntraining, it is very limiting during testing, especially when\nthere is not established communication between the agents.\nMore precisely, assuming that we have access in the observa-\ntions and actions of the opponents during testing is too strong.\nTherefore, it is an open problem to create models that do not\nrely on this assumption.\n4.4\nConvergence Properties\nAn open problem with current multi-agent deep RL methods\nis the lack of theoretical understanding of their convergence\nproperties and what types of outcomes they tend to achieve.\nA theoretical concept that could be used to encourage conver-\ngence are game-theoretic equilibria, such as Correlated and\nNash equilibrium. One drawback of these approaches is the\nrequirement of computing equilibrium solutions (Greenwald\net al., 2003), as well as non-uniqueness of equilibria which\nrequires some form of coordinated equilibrium selection. A\nrecent example in this direction is the work by Li et al. (2019)\nwho used an approximate solution of the Minimax equilibrium.\nTherefore, an interesting research direction is to investigate\napproximate solutions in order to incorporate the notion of\nthese equilibria in multi-agent deep RL.\n4.5\nMulti-Agent Credit Assignment\nAnother problem which arises from approaches with decen-\ntralized execution is related to assigning credit to agents in the\nenvironment. Despite improving centralized training (Foer-\nster et al., 2018b) or learning opponent representations (Tac-\nchetti et al., 2019), methods to handle this problem can still\nbe improved. In general, ﬁnding alternative neural network\narchitectures and learning approaches that can decompose re-\nwards (Rashid et al., 2018; Sunehag et al., 2018) for smaller\ngroup of agents can be a possible future research direction.\n5\nConclusion\nTo summarize our contribution, we identiﬁed ﬁve different\napproaches for handling non-stationarity in multi-agent deep\nRL. In order to understand their characteristics and their dif-\nferences, we provided a detailed categorization in Table 1.\nFinally, we outlined several open problems with respect to\nnon-stationarity and possible directions of future research.\nSettings\nTraining\nExecution\nModelling\nOpp. Info\nAlgorithm\nNum agents\nTacchetti et al. (2019)\nMixed\nCentr.\nDecentr.\nExplicit\nObs / actions\nA2C\n≥2\nSingh et al. (2019)\nMixed\nDecentr.\nDecentr.\nNo\nNone\nPG\n≥2\nLetcher et al. (2019)\nMixed\nDecentr.\nDecentr.\nExplicit\nParameters\nPG\n2\nLi et al. (2019)\nMixed\nCentr.\nDecentr.\nNo\nObs / actions\nDDPG\n≥2\nAl-Shedivat et al. (2018)\nComp.\nDecentr.\nDecentr.\nNo\nNone\nPPO\n2\nBansal et al. (2018)\nComp.\nDecentr..\nDecentr.\nNo\nNone\nPPO\n2\nRaileanu et al. (2018)\nMixed\nCentr.\nCentr.\nExplicit\nObs / actions\nA3C\n2\nMordatch and Abbeel (2018)\nCoop.\nDecentr.\nDecentr.\nNo\nNone\nPG\n≥2\nFoerster et al. (2018a)\nMixed\nDecentr.\nDecentr.\nExplicit\nParameters\nPG\n2\nGrover et al. (2018)\nMixed\nCentr.\nCentr.\nExplicit\nObs / actions\nPPO / DDPG\n2\nRabinowitz et al. (2018)\nMixed\nCentr.\nCentr.\nExplicit\nObs / actions\nImit.\n≥2\nFoerster et al. (2018b)\nCoop.\nCentr.\nDecentr.\nNo\nObs / actions\nActor-critic\n≥2\nLowe et al. (2017)\nMixed\nCentr.\nDecentr.\nNo\nObs / actions\nDDPG\n≥2\nFoerster et al. (2017)\nMixed\nDecentr.\nDecentr.\nNo\nNone\nQ-learning\n≥2\nSukhbaatar et al. (2016)\nCoop.\nDecentr.\nDecentr.\nNo\nNone\nPG\n≥2\nFoerster et al. (2016a)\nCoop.\nCentr.\nDecentr.\nNo\nNone\nQ-learning\n≥2\nHe et al. (2016)\nMixed\nCentr.\nCentr.\nImplicit\nObs\nQ-learning\n2\nZhang and Lesser (2010)\nMixed\nDecentr.\nDecentr.\nExplicit\nParameters\nPG\n2\nTable 1: Categorization of the surveyed algorithms that deal with non-stationarity. The algorithms are categorized based on the environment\nsetting (cooperative, competitive, mixed), their training and execution method (centralized, decentralized), their type of modelling, the opponent\ninformation that they require, the learning algorithm that they use, and the number of agents that can be handled.\nReferences\nMaruan Al-Shedivat, Trapit Bansal, Yuri Burda, Ilya Sutskever,\nIgor Mordatch, and Pieter Abbeel. Continuous adaptation\nvia meta-learning in nonstationary and competitive environ-\nments. International Conference on Learning Representa-\ntions, 2018.\nStefano V Albrecht and Subramanian Ramamoorthy. Com-\nparative evaluation of MAL algorithms in a diverse set of\nad hoc team problems. In International Conference on Au-\ntonomous Agents and Multiagent Systems, pages 349–356,\n2012.\nStefano V Albrecht and Peter Stone.\nAutonomous agents\nmodelling other agents: A comprehensive survey and open\nproblems. Artiﬁcial Intelligence, pages 66–95, 2018.\nTrapit Bansal, Jakub Pachocki, Szymon Sidor, Ilya Sutskever,\nand Igor Mordatch. Emergent complexity via multi-agent\ncompetition.\nIn International Conference on Learning\nRepresentations, 2018.\nMichael Bowling and Manuela Veloso. Rational and conver-\ngent learning in stochastic games. In International Joint\nConference on Artiﬁcial Intelligence, pages 1021–1026,\n2001.\nMuthukumaran Chandrasekaran, Adam Eck, Prashant Doshi,\nand Leenkiat Soh. Individual planning in open and typed\nagent systems. In Conference on Uncertainty in Artiﬁcial\nIntelligence, pages 82–91, 2016.\nFelipe Leno Da Silva and Anna Helena Reali Costa. A survey\non transfer learning for multiagent reinforcement learning\nsystems. Journal of Artiﬁcial Intelligence Research, pages\n645–703, 2019.\nBruno C Da Silva, Eduardo W Basso, Ana LC Bazzan, and\nPaulo M Engel. Dealing with non-stationary environments\nusing context detection. In International Conference on\nMachine Learning, pages 217–224, 2006.\nChelsea Finn, Pieter Abbeel, and Sergey Levine.\nModel-\nagnostic meta-learning for fast adaptation of deep networks.\nIn International Conference on Machine Learning, pages\n1126–1135, 2017.\nJakob Foerster, Ioannis Alexandros Assael, Nando de Freitas,\nand Shimon Whiteson. Learning to communicate with deep\nmulti-agent reinforcement learning. In Advances in Neural\nInformation Processing Systems, pages 2137–2145, 2016.\nJakob Foerster, Yannis M Assael, Nando de Freitas, and Shi-\nmon Whiteson. Learning to communicate to solve riddles\nwith deep distributed recurrent q-networks. arXiv preprint\narXiv:1602.02672, 2016.\nJakob Foerster, Nantas Nardelli, Gregory Farquhar, Triantafyl-\nlos Afouras, Philip HS Torr, Pushmeet Kohli, and Shimon\nWhiteson.\nStabilising experience replay for deep multi-\nagent reinforcement learning. In International Conference\non Machine Learning, pages 1146–1155. JMLR.org, 2017.\nJakob Foerster, Richard Y Chen, Maruan Al-Shedivat, Shimon\nWhiteson, Pieter Abbeel, and Igor Mordatch. Learning with\nopponent-learning awareness. In International Conference\non Autonomous Agents and Multiagent Systems, pages 122–\n130, 2018.\nJakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nan-\ntas Nardelli, and Shimon Whiteson. Counterfactual multi-\nagent policy gradients. In AAAI Conference on Artiﬁcial\nIntelligence, pages 2974–2982, 2018.\nScott Fujimoto, Herke van Hoof, and David Meger. Address-\ning function approximation error in actor-critic methods.\nIn International Conference on Machine Learning, pages\n1587–1596, 2018.\nAmy Greenwald, Keith Hall, and Roberto Serrano. Corre-\nlated Q-learning. In International Conference on Machine\nLearning, pages 242–249, 2003.\nAditya Grover, Maruan Al-Shedivat, Jayesh Gupta, Yuri\nBurda, and Harrison Edwards. Learning policy representa-\ntions in multiagent systems. In International Conference\non Machine Learning, pages 1797–1806, 2018.\nHe He, Jordan Boyd-Graber, Kevin Kwok, and Hal Daum´e\nIII. Opponent modeling in deep reinforcement learning.\nIn International Conference on Machine Learning, pages\n1804–1813, 2016.\nPablo Hernandez-Leal, Michael Kaisers, Tim Baarslag, and\nEnrique Munoz de Cote. A survey of learning in multiagent\nenvironments: Dealing with non-stationarity. arXiv preprint\narXiv:1707.09183, 2017.\nAlistair Letcher, Jakob Foerster, David Balduzzi, Tim\nRockt¨aschel, and Shimon Whiteson. Stable opponent shap-\ning in differentiable games. International Conference on\nLearning Representations, 2019.\nShihui Li, Yi Wu, Xinyue Cui, Honghua Dong, Fei Fang, and\nStuart Russell. Robust multi-agent reinforcement learning\nvia minimax deep deterministic policy gradient. In AAAI\nConference on Artiﬁcial Intelligence, 2019.\nMichael L Littman. Markov games as a framework for multi-\nagent reinforcement learning. In International Conference\non Machine Learning, pages 157–163. 1994.\nRyan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel,\nand Igor Mordatch.\nMulti-agent actor-critic for mixed\ncooperative-competitive environments.\nIn Advances in\nNeural Information Processing Systems, pages 6379–6390,\n2017.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A\nRusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin\nRiedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.\nHuman-level control through deep reinforcement learning.\nNature, 518:529–533, 2015.\nIgor Mordatch and Pieter Abbeel. Emergence of grounded\ncompositional language in multi-agent populations.\nIn\nAAAI Conference on Artiﬁcial Intelligence, pages 1945–\n1952, 2018.\nNeil Rabinowitz, Frank Perbet, Francis Song, Chiyuan Zhang,\nSM Ali Eslami, and Matthew Botvinick. Machine theory of\nmind. In International Conference on Machine Learning,\npages 4215–4224, 2018.\nRoberta Raileanu, Emily Denton, Arthur Szlam, and Rob Fer-\ngus. Modeling others using oneself in multi-agent reinforce-\nment learning. In International Conference on Machine\nLearning, 2018.\nTabish Rashid, Mikayel Samvelyan, Christian Schroeder Witt,\nGregory Farquhar, Jakob Foerster, and Shimon Whiteson.\nQmix: Monotonic value function factorisation for deep\nmulti-agent reinforcement learning. In International Con-\nference on Machine Learning, pages 4292–4301, 2018.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford,\nand Oleg Klimov. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347, 2017.\nDavid Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan\nWierstra, and Martin Riedmiller. Deterministic policy gra-\ndient algorithms. In International Conference on Machine\nLearning, pages 387–395, 2014.\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis\nAntonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Lau-\nrent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mas-\ntering chess and shogi by self-play with a general reinforce-\nment learning algorithm. arXiv preprint arXiv:1712.01815,\n2017.\nAmanpreet Singh, Tushar Jain, and Sainbayar Sukhbaatar.\nLearning when to communicate at scale in multiagent coop-\nerative and competitive tasks. International Conference on\nLearning Representations, 2019.\nSainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. Learn-\ning multiagent communication with backpropagation. In\nAdvances in Neural Information Processing Systems, pages\n2244–2252, 2016.\nPeter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Mar-\nian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc\nLanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al.\nValue-decomposition networks for cooperative multi-agent\nlearning based on team reward. In International Confer-\nence on Autonomous Agents and Multiagent Systems, pages\n2085–2087, 2018.\nRichard S Sutton, David A McAllester, Satinder P Singh, and\nYishay Mansour. Policy gradient methods for reinforcement\nlearning with function approximation.\nIn Advances in\nNeural Information Processing Systems, pages 1057–1063,\n2000.\nRichard S Sutton, Anna Koop, and David Silver. On the role\nof tracking in stationary environments. In International\nConference on Machine Learning, pages 871–878, 2007.\nAndrea Tacchetti, H. Francis Song, Pedro A. M. Mediano,\nVinicius Zambaldi, J´anos Kram´ar, Neil C. Rabinowitz,\nThore Graepel, Matthew Botvinick, and Peter W. Battaglia.\nRelational forward models for multi-agent learning. In In-\nternational Conference on Learning Representations, 2019.\nMing Tan. Multi-agent reinforcement learning: Independent\nvs. cooperative agents.\nIn International Conference on\nMachine Learning, pages 330–337, 1993.\nGerald Tesauro.\nTemporal difference learning and TD-\ngammon. Communications of the ACM, (3):58–68, 1995.\nChristopher JCH Watkins and Peter Dayan. Q-learning. Ma-\nchine Learning, (3-4):279–292, 1992.\nChongjie Zhang and Victor Lesser.\nMulti-agent learning\nwith policy prediction. In AAAI Conference on Artiﬁcial\nIntelligence, pages 927–934, 2010.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.MA",
    "stat.ML"
  ],
  "published": "2019-06-11",
  "updated": "2019-06-11"
}