{
  "id": "http://arxiv.org/abs/1906.06706v7",
  "title": "Interpretations of Deep Learning by Forests and Haar Wavelets",
  "authors": [
    "Changcun Huang"
  ],
  "abstract": "This paper presents a basic property of region dividing of ReLU (rectified\nlinear unit) deep learning when new layers are successively added, by which two\nnew perspectives of interpreting deep learning are given. The first is related\nto decision trees and forests; we construct a deep learning structure\nequivalent to a forest in classification abilities, which means that certain\nkinds of ReLU deep learning can be considered as forests. The second\nperspective is that Haar wavelet represented functions can be approximated by\nReLU deep learning with arbitrary precision; and then a general conclusion of\nfunction approximation abilities of ReLU deep learning is given. Finally,\ngeneralize some of the conclusions of ReLU deep learning to the case of\nsigmoid-unit deep learning.",
  "text": "Interpretations of Deep Learning by Forests and\nHaar Wavelets\nChangcun Huang\nAbstract This paper presents a basic property of region dividing of ReLU\n(rectiﬁed linear unit) deep learning when new layers are successively added, by\nwhich two new perspectives of interpreting deep learning are given. The ﬁrst\nis related to decision trees and forests; we construct a deep learning structure\nequivalent to a forest in classiﬁcation abilities, which means that certain kinds\nof ReLU deep learning can be considered as forests. The second perspective is\nthat Haar wavelet represented functions can be approximated by ReLU deep\nlearning with arbitrary precision; and then a general conclusion of function\napproximation abilities of ReLU deep learning is given. Finally, generalize\nsome of the conclusions of ReLU deep learning to the case of sigmoid-unit\ndeep learning.\nKeywords Deep learning · Interpretation · Region dividing · Forests · Haar\nwavelets · Function approximation.\n1 Introduction\nDeep leaning is nearly the most popular highlight of artiﬁcial intelligence nowa-\ndays and has made great successes in speech recognition [1], computer vision\n[2], playing game go [3], and so on. Despite its successful applications and\nhistory of nearly 40 years since Fukushima’s paper in 1982 [4], the underlying\nprinciple still remains unclear, so that deep learning is often referred to as\n“black box”, which greatly hinders its development.\nOne of the main concerns is why deep neural networks are more powerful\nthan those with shallow layers. The answer to this question is the key of un-\nderstanding deep learning, for which there are mainly three kinds of existing\nresults: The ﬁrst kind is speciﬁc, explaining particular class of functions re-\nalized by deep learning, such as [5,6,7,8]. The second kind [9,10,11,12,13] is\nChangcun Huang\narXiv:1906.06706v7  [cs.LG]  6 Dec 2019\n2\nChangcun Huang\nmore general by studying the expressive ability of deep layers compared with\nshallow ones. The last one is about the function approximation ability of deep\nlearning [14,15].\nPart of this paper belongs to the second kind. We’ll present a basic property\nof region dividing of ReLU [16,17] deep learning when successively adding new\nlayers (Section 3); and then realize the multi-category classiﬁcation via a deep\nlearning structure equivalent to a forest, which is a new interpretation of ReLU\ndeep learning (Section 4).\nThe function approximation problem is also discussed (Section 5). We’ll\nprove that Haar wavelet represented functions can be approximated by ReLU\ndeep learning as precisely as possible. It follows a general result that ReLU\ndeep learning can approximate an arbitrary continuous function on a closed\nset of n-dimensional space; the proof is totally diﬀerent from [14,15], giving a\nnew perspective of interpreting ReLU deep learning.\nFinally, one distinction of this paper is that some conclusions of ReLU deep\nlearning can be generalized to the case of sigmoid-unit deep learning (Section\n6).\nSince ReLU has nearly become the dominant choice of neural units used by\ndeep learning in recent years [9,18], the main topics of this paper are general\nand useful both in theory and engineering.\n2 Preliminaries\nBefore describing the region dividing property of deep learning, some prelim-\ninary results should be introduced ﬁrst.\n2.1 Mechanisms of 3-layer networks\nThe discussion of 3-layer networks is the basis of comparisons between shallow\nand deep networks. And also, there exists 3-layer subnetworks in deep learning,\nin which the mechanism is the same as that of ordinary 3-layer networks.\nWe begin the discussion from a concrete example of two-category classiﬁca-\ntion realized by a 3-layer network. It is well known that each ReLU corresponds\nto a hyperplane dividing the input space into two regions. In the case of two-\ndimensional input space, a hyperplane is reduced to a line. The following notes\nare applicable to the rest of this paper:\nNote 1. Hereafter, when referred to the classiﬁcation by a hyperplane, the\ndata points just being on hyperplanes are not taken into consideration.\nNote 2. We’ll not distinguish between the term of “region dividing” and that\nof “data classiﬁcation”; they usually do the same thing.\nNote 3. For simplicity, all the ﬁgures of neural networks ignore the biases,\nwhich actually exist, however.\nInterpretations of Deep Learning by Forests and Haar Wavelets\n3\n(a) A 3-layer network.\n(b) Region dividing with mutual\ninterference.\nFig. 1: The mechanism of 3-layer networks.\nNote 4. Unless otherwise stated, the term of “deep learning” is the abbrevia-\ntion of “ReLU deep learning”.\nFig.1 (a) is a 3-layer network with three ReLUs in the hidden layer denoted\nby 1, 2 and 3, corresponding to lines of 1, 2, and 3 of Fig.1 (b), respectively.\nNotation. We denote the two diﬀerent sides of a hyperplane by “l-s”, where\nl is the index of the hyperplane and s expresses the output of the ReLU with\nrespect to this hyperplane. l-+ represents one side of hyperplane l, where the\nReLU output is greater than 0; and the other side is denoted by l-0, where the\nReLU output is zero.\nFor instance, in Fig.1(b), 1-+ is the side above line 1 because the data in\nthat half plane gives positive ReLU output, while 1-0 represents the below side\nproducing zero outputs. The objective of the 3-layer network of Fig.1 (a) is to\nclassify the data points of Fig.1 (b) into two categories: the output of the third\nlayer should be 0 or 1 when the input sample belongs to “o” or “∗” category,\nrespectively. Output 1 can be obtained by normalizing the nonzero output of\nthe ReLU.\nIn Fig.1 (b), we can add lines of 1, 2, and 3 one by one for classiﬁcation.\nFirst, Line 1 is added, when the “o” samples below line 1 are correctly clas-\nsiﬁed. The samples above line 1 should be further classiﬁed by more lines,\nsuch as line 2 and line 3. However, for example, when line 3 is added, the “o”\nsamples below line 1 is simultaneously in the side of 3-+, producing nonzero\noutputs; that is to say, the subdividing of the half plane above line 1 by line\n3 makes the ever correct classiﬁcation result below line 1 change to be wrong,\nfor which we may need to add other lines to eliminate the inﬂuence of line 3.\nIn fact, the ﬁnal output expression of 3-layer networks (with a single out-\nput) is\ny = f(\nN\nX\ni=1\nwisi),\n(1)\nwhere si is the ith ReLU output of the hidden layer. If si ̸= 0 and wi ̸= 0,\nthe ith ReLU can inﬂuence the whole sum P wisi by its nonzero output; in\n4\nChangcun Huang\ngeometry language, it means that the ith hyperplane for region dividing will\ninﬂuence half of the input space where this ReLU output is nonzero. The\ninﬂuenced region may include ever correctly divided regions and the right\nresults may be reversed. If the inﬂuence cannot be eliminated by adjusting\npresent hyperplanes, new hyperplanes should be added. This procedure may\noccur recursively; hence the number of hyperplanes needed in 3-layer networks\nmay be extremely larger than that it really needs, when we just want to divide\nthe input space into separated regions without considering mutual inﬂuences.\nThis is the general explanation of Fig.1, from which a conclusion follows:\nTheorem 1. In 3-layer networks, any new added ReLU of the hidden layer\nwill inﬂuence half of the input space where the output of this ReLU is nonzero.\nWe shall show that the interference of hyperplanes to each other can be\navoided in deep learning.\n2.2 Transmitting of input-space regions through layers\nIn deep learning, the input space is only directly connected to the ﬁrst hidden\nlayer; how a region of the input space passes to subsequent layers is a key\nfoundation of subregion dividing via a sequence of layers.\nPascanu et al. [9] used “intermediate layer” to transmit an input-space\nregion, which is actually by means of aﬃne transforms; however, no general\nrigorous conclusions with proofs were presented. Although trivial in mathe-\nmatics, due to great importance, we’ll give detailed descriptions rigorously\nboth in the conclusions and proofs about this problem, as well as add some\nnecessary prerequisites for the establishing of the results.\nLemma 1. Suppose that the input space I is n-dimensional. The n nonzero\noutputs of n ReLUs in the ﬁrst hidden layer form a new space H. If the weight\nmatrix W of size n × n between the input layer and the ﬁrst hidden layer is\nnonsingular, then H is n-dimensional and is an aﬃne transform of a region of\nI. The intersection of the nonzero-output areas of n ReLUs in I is the region\nto be transformed.\nProof. We know that the nonzero output of a ReLU is f(x) = x for x > 0. So\nan n-nonzero output vector y of H can be written as\ny = Wx + b,\n(2)\nwhere x is a vector of a certain region of I and b is the bias vector of the\nn ReLUs. (2) only combines the outputs of n ReLUs to the matrix form.\nObviously, (2) is an aﬃne transform and if W is nonsingular, the dimension\nof H would be n.\nRemark. The geometric meaning of Lemma 1: Nonsingular W of (2) implies\nnon-parallel hyperplanes. Lemma 1 is equivalent to say that if the n hyperplanes\nwith respect to n ReLUs are not parallel to each other, the space H would be\nn-dimensional as well as an aﬃne transform of a region of the input space.\nInterpretations of Deep Learning by Forests and Haar Wavelets\n5\n(a) A deep learning network.\n(b) Region dividing without mutual\ninterference.\nFig. 2: The mechanism of deep learning.\nTheorem 2. In deep learning with n-dimensional input, if each succeeding\nlayer has n ReLUs with nonsingular input weight matrix, a certain region of\nthe input space can be transmitted to subsequent layers one by one in the sense\nof aﬃne transforms.\nProof. The ﬁrst hidden layer of Lemma 1 again can be considered as a new\ninput space. By doing this recursively, a certain region of the initial input\nspace can be transmitted to succeeding layers one by one in the sense of aﬃne\ntransforms, as long as this region is always in the nonzero parts of all the n\nReLUs in each layer.\n3 Basic properties of region dividing via deep learning\nSection 2 has mentioned the mechanism of 3-layer networks that adding a new\nReLU in hidden layer would inﬂuence half of the input space. However, this\ndisadvantage can be avoided in deep learning; we’ll describe a basic property\nof region dividing of deep learning in Theorem 3, which is the basis of the\nwhole paper. The proof of Lemma 2 will be referred to for several times in the\nfollowing sections to construct diﬀerent types of deep learning structures.\n3.1 The two-dimensional case\nAlso begin with an example. Fig.2 is corresponding to Fig.1 of Section 2. In\nFig.1, to subdivide the region above 1-+, line 3 is added in the hidden layer;\nhowever, this operation inﬂuences the ever correctly classiﬁed results. In deep\nlearning, the mutual interference among lines can be avoided by adding a new\nlayer to restrict the inﬂuencing area of a line.\nAs shown in Fig.2 (b), ﬁrst, line 1 is selected to divide the data points into\ntwo separate parts in diﬀerent regions; then, we can always ﬁnd line 2 having\nthe same classiﬁcation eﬀect as line 1. In fact, when line 2 in Fig.2 (b) rotates\n6\nChangcun Huang\ncounterclockwise towards line 1, the region between 1-+ and 2-+ (or between\n1-0 and 2-0) can be as large as possible, such that all the data points above\nline 1 (or below line 1) are encompassed by 1-+ and 2-+ (or by 1-0 and 1-0);\nthis is the way of ﬁnding line 2. Thus, all the data points are either in the\nregion between 1-+ and 2-+ (denoted by region-+) or in the region between\n1-0 and 2-0 (denoted by region-0).\nSince line 1 and line 2 are not parallel to each other, by the remark of\nLemma 1, the space of the two nonzero outputs of ReLU 1 and ReLU 2 of the\nﬁrst hidden layer is two-dimensional, as well as an aﬃne transform of region-\n+; while region-0 is excluded from this layer in terms of zero outputs. Aﬃne\ntransforms do not aﬀect the linear classiﬁcation property of the data; so the\nlinear classiﬁcation in region-+ can be done in the space of the ﬁrst hidden\nlayer, without inﬂuencing region-0 because it has been excluded.\nNow, instead of adding ReLU 3 in the same layer as ReLU 1 and ReLU 2 in\nFig.1 (a), we add it in a new layer called the second hidden layer as shown in\nFig.2 (a) to perform the classiﬁcation of the ﬁrst hidden layer. Correspondingly,\nin Fig.2 (b), line 3 should be added in a region of the ﬁrst hidden layer, which\nis an aﬃne transform of region-+ of the input space; however, this illustration\nis reasonable because the eﬀect of linear classiﬁcation is equivalent.\nObviously, the principle and operation underlying this example are general\nin two-dimensional space. In what follows, we shall directly generalize it to the\nn-dimensional case.\n3.2 The n-dimensional case\nLemma 2. For a 3-layer network with n-dimensional input, the hidden layer\ncan be designed to realize an arbitrary linearly separable classiﬁcation of two\ncategories. One of the category will be excluded by the hidden layer, while the\nother one changes into its aﬃne transform. Adding a new hidden layer can\ndivide a selected region of the input space in the sense of aﬃne transforms\nwithout inﬂuencing an excluded region.\nProof. When the input space is n-dimensional, we need n hyperplanes (ReLUs)\nto construct an n-dimensional space of the hidden layer, each of which realizes\na same two-category classiﬁcation. The function of those n hyperplanes to be\nconstructed is similar to that of line 1 and line 2 in Fig.2 (b).\nFirst choose hyperplane 1 to divide the input space into two regions, con-\ntaining the data points of category-0 and category-+, respectively; category-0\nshould be excluded, while category-+ may need to be subdivided. Then hy-\nperplane 2 with the same classiﬁcation eﬀect as hyperplane 1 can be found\nby the similar method of the two-dimensional case. When hyperplane 2 ro-\ntates towards hyperplane 1 (counterclockwise or clockwise according to their\nrelative positions), there exists inﬁnite number of hyperplanes between them,\nall of which can classify the data in the same eﬀect; choose n −2 of them\nas the remaining hyperplanes to construct an n-dimensional coordinate sys-\ntem. Since the n selected hyperplanes are not parallel to each other, by the\nInterpretations of Deep Learning by Forests and Haar Wavelets\n7\nremark of Lemma 1, the n nonzero outputs of n ReLUs with respect to those\nhyperplanes form an n-dimensional linear space, which is an aﬃne transform\nof a region of the input space; while the region giving n zero outputs of the n\nReLUs will be excluded.\nThe constructed hidden layer has successfully excluded a region containing\ncategory-0 (region-0), as well as transmitted a region containing category-+\n(region-+). If adding a new hidden layer, we can subdivide region-+ of the\ninput space in the sense of aﬃne transforms without inﬂuencing region-0.\nRemark. The purpose of selecting n non-parallel hyperplanes (ReLUs) is to\nconstruct an n-dimensional space to maintain the complete data structure of\nthe n-dimensional input space in the sense of aﬃne transforms. If the number\nof non-parallel hyperplanes is less than n, the outputs will be the subspace of\nthe input space, which may lose information.\nNotation. Denote an arbitrary 2-layer subnetwork of a deep learning struc-\nture by P-C with n-dimensional input, representing the previous layer and\ncurrent layer, respectively; W is the weight matrix between layer P and layer\nC as in (2).\nTheorem 3. In deep learning, if current layer C has n ReLUs with nonsingu-\nlar input weight matrix W, adding ReLUs in a new layer N after layer C can\ndivide a certain region of previous layer P in the sense of aﬃne transforms\nwithout inﬂuencing an excluded region. Similarly, adding new layers one by\none can realize subregion dividing recursively; in each layer, data points that\ndo not need to be subdivided can be put into the excluded region, so that the\nregion dividing of succeeding layers will have no impact on them.\nProof. The ﬁrst part of the theorem is similar to Lemma 2. As long as W is\nnonsingular, even if the n ReLUs of layer C are not specially designed, the\nregion-transmitting property still holds. The rest of the proof is the recursive\napplication of the ﬁrst part.\nRemark 1. Theorem 3 is a basic property of region dividing of certain kinds\nof ReLU deep learning, which is the key of this paper; all the following results\nare the consequences of this theorem.\nRemark 2. Theorem 3 indicates an advantage of deep layers for a type of deep\nlearning structure. To classify complex data points, the deeper the network,\nthe ﬁner the subdividing will be. Once the step of adding new layers stops, the\nlast three layers will perform the classiﬁcation via the mechanism of 3-layer\nnetworks in a ultimate subregion.\n4 Multi-category classiﬁcation of deep learning\nIn this section, the multi-category classiﬁcation ability of deep learning will be\ngiven in Theorem 4. Lemma 3 deals with the two-category case, while Theorem\n4 is the repeated applications of Lemma 3.\n8\nChangcun Huang\n(a) A decision tree for\ntwo-category\nclassiﬁcation.\n(b) The deep learning structure\ncorresponding to the left decision tree.\nFig. 3: Two-category classiﬁcation abilities of deep learning.\nLemma 3. For a ﬁnite number of data points composed of two categories in\nn-dimensional space, deep learning can classify them as a decision tree.\nProof. The proof is constructive by Lemma 2, Theorem 3 and the theory of\ndecision trees. First, we can always construct a decision tree to realize this two-\ncategory classiﬁcation, whose decision functions are linear classiﬁers. Second,\nthere exists a deep learning structure equivalent to that decision tree, which\nis given by the following method.\nAs shown in Fig.3 (a), it’s a four-level decision tree classifying two-dimensional\ndata points and Fig.3 (b) is its corresponding deep learning structure. The\nlayer of deep learning should correspond to the level of the decision tree ex-\ncept that the deep learning adds an output layer with one ReLU. Also in Fig.3,\nthe ﬁrst layer, i.e., the input layer, has two ReLUs with respect to root node\n1 of the decision tree because the input space is two-dimensional; the general\ncase of n-dimensional space is similar.\nIn each layer, for the node having two child nodes, construct 2n ReLUs\nin the next layer: The n of them (left child) separate the data points into\nregion-+ and region-0, which are designed according to the decision function\nof this node by the method of Lemma 2; data points in region-+ can be\nsubclassiﬁed by succeeding layers of child nodes without inﬂuencing region-0\nexcluded. The other n ReLUs (right child) are diﬀerent from the ﬁrst group\nof n ReLUs only in the parameter signs, respectively; they reverse the ReLU\noutputs of data points in region-+ and region-0, which instead makes region-0\nto be subdivided. For example, in Fig.3, node 1 has two child nodes, so that\nfour ReLUs are needed in the next layer; two of them are for left child 2, while\nthe other two are for right child 3. In the second layer, the weights and biases\nof ReLU 3’s are opposite in the signs to those of ReLU 2’s as well as with same\nabsolute values, respectively.\nInterpretations of Deep Learning by Forests and Haar Wavelets\n9\nFig. 4: An example of three-category classiﬁcation.\nFor the leaf node, if the next layer is the last one, just connect its related\nReLUs to the output ReLU, as node 6 and node 7 of Fig.3. Otherwise, we\nshould add one ReLU in each succeeding layer (except for the last one) to\ntransmit the classiﬁcation result to the last layer, such as node 2 and node 5\nin Fig.3; make sure that the weights and bias of the single ReLU of a leaf node\nin each layer maintain the nonzero output.\nThe weights and bias of the output-layer ReLU should be designed to\ndistinguish between a left leaf node and a right leaf node. For instance, let the\nleft leaf node and right leaf node of Fig.3 (a) correspond to zero output and\nnonzero output of deep learning of Fig.3 (b), respectively. The design is easy\nbecause in the layer previous the last one, when the output of a leaf node is\nnonzero, those of other leaf nodes will be mutually exclusive to be zero, due to\nthe properties of decision trees. For example, in Fig.3 (b), when the output of\nReLU 2 in the fourth layer (previous the last one) is nonzero, all the outputs\nof other ReLUs of this layer will be zero. So just consider that only ReLU 2\nexists in that layer, by which the weight between ReLU 2 and the output-layer\nReLU can be designed without inﬂuencing other leaf nodes. The bias of the\noutput-layer ReLU can be set to zero because the weight itself is enough to\nproduce the desired output. Since ReLU 2 is corresponding to a left leaf node,\nobviously, when the bias is zero, if the weight is set to a value less than or\nequal to zero, the design will meet the need. The general case is similar. This\ncompletes the constructing process.\nA forest is a sum of decision trees [19]. We have proved that deep learning\ncan realize the two-category classiﬁcation as a decision tree. Next we’ll show\nthat deep learning can be equivalent to a forest in classiﬁcation abilities.\nTheorem 4. Deep learning can classify arbitrary multi-category ﬁnite number\nof data points as a forest.\nProof. The proof is to reduce the multi-category classiﬁcation to the two-\ncategory case of Lemma 3 [20]. Fig.4 is an example of three-category classi-\n10\nChangcun Huang\nﬁcation, in which each dotted rectangle classiﬁes one of the three categories\nusing the two-category method of Lemma 3. No matter how many categories\nshould be classiﬁed, employ the two-category method to deal with each cate-\ngory separately and combine them into a whole deep learning structure.\nRemark 1. From the viewpoint of forests, the layer depth of deep learning\ncorresponds to the level of decision trees comprising a forest; while the number\nof neural units in each layer is related to that of the nodes in the corresponding\nlevel of the decision tree.\nRemark 2. Bengio et al. [19] stated that decision trees are not easily general-\nized to variations of the training data, while forests do not have this limitation.\nBy Theorem 4, deep learning can realize the function of forests and its gener-\nalization ability can be assured.\n5 Function approximations of deep learning\nThere exists general results about the function approximation ability of 3-layer\nsigmoid-unit networks, such as Hecht-Nielsen [21], Cybenko [22], and Hornik\net al.[23]. Among them, Hecht-Nielsen’s proof is constructive.\nIn the area of ReLU deep learning, Yarotsky [14] and Liang et al. [15] have\nalso discussed such issues, both using similar methods. They ﬁrst constructed\ndeep learning structures to approximate polynomial functions; and then by\nTaylor series of smooth functions, the function approximation ability of deep\nlearning was proved. Although the conclusions are assured, the deep learning\nstructures they mentioned are constrained to certain types.\nThe proofs given blow are totally diﬀerent from [14] and [15], aiming at\nother types of deep learning structures, which will provide a new perspective\nof interpreting deep learning.\nLemma 4. Any piecewise-constant function of Haar wavelets with ﬁnite num-\nber of building-block domains can be approximated by deep learning with ar-\nbitrary precision. If the input space is n-dimensional, 2n hidden layers are\nenough.\nProof. The proof is based on Lemma 2 and Theorem 3. First prove the two-\ndimensional case. For a Haar wavelet represented function f(x1, x2) deﬁned\non a closed set S with ﬁnite building-block domains, we can always divide\nits domains into rectangles (or squares, similar hereafter) with diﬀerent sizes\nand locations, each having a constant value (maybe the same with some other\nrectangles) of the function. The basic idea is to approximate the function by\ndeep learning in each rectangle as precisely as possible. Because the number of\nrectangles is ﬁnite, if the approximation error for each rectangle is arbitrarily\nsmall, then the deep learning approximation to the whole function will be\narbitrarily precise. So we just need to prove the case of one rectangle.\nFirst, for an isolated rectangle, such as Ri in Fig.5 (a), it can be separated\nvia deep learning. For each side of Ri, such as the bottom one, we can always\nInterpretations of Deep Learning by Forests and Haar Wavelets\n11\n(a) An isolated-rectangle domain\nseparated by deep learning.\n(b) The case of adjacent\nrectangles.\n(c) Corresponding\ndeep learning\nstructure.\nFig. 5: Function approximations of deep learning in one-rectangle domain.\nﬁnd two lines (ReLUs) to divide some rectangle domains of f(x1, x2) into two\nparts in two diﬀerent regions, with one of the two lines parallel to the bottom\nside (such as line 1). Ri is in the region where the outputs of the two ReLUs\nare both nonzero; all the rectangles below line 1 should be excluded by line 1\nand line 2, and are in the other region (zero-output region). We see that the\nregion between 1-+ and 2-0 or between 1-0 and 2-+ also gives nonzero output,\nwhich needs to be specially processed later diﬀerent from the classiﬁcation of\ndiscrete data points in Theorem 4.\nAfter doing similar operations to the other sides, except for Ri, the rectan-\ngle domains of f(x1, x2) are all excluded. However, the intersection of nonzero-\noutput regions of the four separations is not Ri, but the region of the plane\nexcluding four zero-output regions, which is a concave polygon (denoted by\nP) formed by eight lines such as in Fig.5 (a).\nNote that only the separation of the bottom side of Ri handled ﬁrst is\ndone in the input space of deep learning; the separations of three other sides\nshould be done in the spaces of three succeeding layers, respectively, as shown\nin Fig.5 (c). However, the above operations are reasonable because of the\nproperties of aﬃne transforms. For example, if the second hidden layer of\nFig.5 (c) corresponds to the separation of the left side of Ri, as long as we\ncan ﬁnd two lines for this side in the input space such as in Fig.5 (a), the\ncorresponding two lines in the space of the ﬁrst hidden layer can also be\nfound, with the parallel and collinear properties invariant. By the architecture\nof Fig.5 (c), the eﬀects of four separations can be combined and ﬁnally only\nthe data points in polygon P can give nonzero outputs. The rest of the proof\nwill not remind this related issue again.\nIn polygon P, let the output of deep learning be the value of the approx-\nimated function f(x1, x2) in rectangle Ri. We now show that the limit of a\n12\nChangcun Huang\nsequence of P can be Ri by adjusting the parameters of eight lines. Denote an\nouter rectangle formed by four of the eight lines parallel to the four respective\nsides of Ri (such as line 1) by Ro. For the separation of the bottom side of\nRi, when line 2 rotates clockwise towards line 1 parallel to the bottom side,\nthe limit of line 2 is line 1; during the rotating process, the classiﬁcation eﬀect\nof separating Ri remains unchanged, while the region between 1-+ and 2-0\nor between 1-0 and 2-+ becomes smaller and smaller. If we do the similar\nrotating operations to the cases of three other sides, the concave polygon P\ncan approximate Ro by any desired precision.\nWhen the outer rectangle Ro shrinks to Ri, the polygon P constructed\nby deep learning can also approximate Ri with arbitrary precision; therefore,\ndeep learning can approximate f(x1, x2) in Ri as precisely as possible.\nNow discuss the case of adjacent rectangles. We call two rectangles adjacent\nwhen their two respective sides are on a same line. In Fig.5 (b), suppose\nthat f(x1, x2) has diﬀerent constant values in the big rectangle Rb and small\nrectangle Rs. As can be seen, the bottom side of Rb shares a same line with\nthe top side of Rs, so that they are adjacent. Rb is to be separated and may\nhave more than one adjacent rectangles; however, we only illustrate one of\nthem, which is enough for the description of the proof.\nAs the case of an isolated rectangle, a concave polygon Pb encompassing\nRb can be constructed by deep learning. In polygon Pb, the output of deep\nlearning is normalized to the value of function f(x1, x2) in Rb. As shown in\nFig.5 (b), part of Rs is separated into Pb, where the output of deep learning\nis not equal to the actual function value in Rs. This type of approximation\nerror occurs in all the adjacent rectangles separated into polygon Pb, where\nthe function value is diﬀerent from that of Rb. So the region of Pb outside Rb\n(denoted by B) is the source of approximation error of deep learning. Deﬁne\nthe approximation error in B as\nE =\nZZ\nB′( ˆf(x1, x2) −f(x1, x2)2)1/2dx1dx2,\n(3)\nwhere ˆf(x1, x2) is the approximating function of deep learning and B′ is a\nsubset of region B on which f(x1, x2) is deﬁned.\nLet\nω = max\nS\n|f(x′\n1, x′\n2) −f(x1, x2)| ,\n(4)\nwhere S is the domain of f(x1, x2) and ω is the maximum variation of f(x1, x2),\nwhich always exists because f(x1, x2) only has ﬁnite number of function values.\nSince the value of ˆf(x1, x2) is also derived from f(x1, x2), it’s obvious that\nE ≤ωSB,\n(5)\nwhere SB is the area of region B. Because the area of Pb can be arbitrarily\nclose to that of Rb, SB tends to be zero as Pb →Rb; thus, E can be as small\nas possible.\nFig.5 (c) is the structure of deep learning constructed for Fig.5 (a) or Fig.5\n(b). The ﬁrst hidden layer is corresponding to the region dividing by line 1\nInterpretations of Deep Learning by Forests and Haar Wavelets\n13\nand line 2 with respect to the bottom side of a rectangle; and the succeeding\nthree layers are the cases of three other sides. The four times of region dividing\nmust be done in diﬀerent layers successively to ensure that their eﬀects can be\ncombined. The ﬁnal output should be normalized to the function value.\nIt is noted that four hidden layers for the 2-dimensional case are enough,\nsince a rectangle has only four sides, each of which needs one hidden layer.\nThe whole structure of deep learning approximating f(x1, x2) can be ob-\ntained by combining the subnetworks of all rectangle domains just like Fig.4,\neach module of a dotted rectangle representing a certain rectangle domain of\nf(x1, x2). This completes the proof of the two-dimensional case.\nSimilarly, the n-dimensional case can be proved. To approximate a single\nhyperrectangle, use 2n hidden layers (each for one of the 2n hyperrectangle\nsides) instead of four as in Fig.5 (c), with each layer having n ReLUs. The ro-\ntating operations changing the parameters of hyperplanes can refer to the proof\nof Lemma 2. For each side of a hyperrectangle, n hyperplanes are constructed\nto separate the hyperrectangle by the method of Lemma 2, with hyperplane 1\nparallel to the side. Hyperplane 2 is second added and other n−2 hyperplanes\nare chosen between hyperplane 1 and hyperplane 2. So we just need to rotate\nhyperplane 2 as in the two-dimensional case, and then to insert other new n−2\nhyperplanes between hyperplane 1 and the rotated hyperplane 2. The rest of\nthe proof is trivial according to the two-dimensional case.\nTheorem 5. Deep learning with 2n hidden layers can approximate any contin-\nuous function on a closed set of n-dimensional space with arbitrary precision.\nProof. We know that Haar wavelets are capable of approximating continuous\nfunctions, while deep learning can approximate Haar wavelets as demonstrated\nin Lemma 4. This completes the proof.\nRemark 1. From the perspective of Haar wavelets, the 2n hidden layers of\ndeep learning are used to approximate hyperrectangle domains of Haar wavelet\nfunctions by the principle of Theorem 3; and the number of neural units in\neach layer corresponds to that of the hyperrectangle domains. In more detail,\nif there exists m hyperrectangle domains, each hidden layer should use m × n\nneural units.\nRemark 2. The approximating accuracy of deep learning by Theorem 5 is\ndetermined by that of Haar wavelets. If we want the approximation error to be\nsmaller, just make the Haar wavelet approximation to a function more precise,\nand then use deep learning to approximate the ﬁner wavelets. The precise the\nHaar wavelets, the more ReLUs we need; however, the number of hidden layers\nis always 2n.\nRemark 3. Lippmann [24] ever gave a little similar proof about the classi-\nﬁcation ability of 3-layer networks composed of threshold logic units (TLUs).\nAlthough he didn’t mention the function approximation problem, his region di-\nviding by neural networks can accurately represent a Haar wavelet function.\nHowever, he only discussed the case of 3-layer networks with TLUs.\n14\nChangcun Huang\n6 Generalizations to sigmoid-unit deep learning\nDeep learning with sigmoid neural units has been successfully used in speech\nanalysis [1] and computer vision [2]. It will be shown later that some related\nconclusions of ReLU deep learning of this paper can be generalized to the\nsigmoid-unit case.\nCorollary 1. All the conclusions of this paper about ReLU deep learning still\nhold in the case of a modiﬁed ReLU, which is\nf(x) = max(0, kx + b),\n(6)\nwhere k and b are real with k > 0.\nProof. (6) only changes the slope of the linear part and the position in x axis\nof a ReLU; however, as long as a neural unit has zero and linear outputs\nseparated by a threshold, all the proofs related to the ReLU are applicable to\nthe modiﬁed case of (6).\nCorollary 2. In sigmoid-unit deep learning, a certain region of input space\ncan be approximately transmitted to hidden layers by any desired precision in\nthe sense of aﬃne transforms.\nProof. The derivative of sigmoid function S(x) is S′(x) = S(x)(1 −S(x)),\ntending to 1/4 when x →0 ; that is to say, S(x) is approximately a line of\ny = x/4 + 1/2 as precisely as possible when x is close enough to zero. Thus, a\ncertain segment of the sigmoid function can be approximately considered as a\nline. According to Corollary 1 and Theorem 2, this corollary holds.\nRemark. In the classic paper [25] of artiﬁcial neural networks, Hopﬁeld also\nreferred to the “linear central region” of S(x) at x = 0 and used this approx-\nimately linear property to transmit information between nonlinear neurons.\nThe thought is similar; however, the details are diﬀerent from the background\nof applications.\nCorollary 3. Sigmoid-unit deep learning can exclude a certain region of the\ninput space or a hidden layer space with arbitrary precision, so that region\ndividing in some other regions can not inﬂuence it.\nProof. The sigmoid function S(x) tends to zero as x →−∞, approximately\ncorresponding to the zero-output part of a ReLU. Selecting probable param-\neters of sigmoid units can exclude a certain region as the case of ReLUs with\narbitrary precision.\nRemark. The above three corollaries suggest that sigmoid-unit deep learning\ncan realize the function of ReLU deep leaning to some extent.\nInterpretations of Deep Learning by Forests and Haar Wavelets\n15\n7 Summary\nThe region-dividing property of deep learning in Theorem 3 is general. On the\nbasis of this property, we established the relationships between deep learning\nand forests, as well as between deep learning and Haar wavelets, by which\nthe multi-category classiﬁcation and function approximation abilities of deep\nlearning were discussed.\nAll topics mentioned above are related to the “black-box” problem of deep\nlearning, which is important both in theory and engineering. We hope that\nthis paper will be helpful to this theme.\nReferences\n1. G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. R. Mohamed, N. Jaitly, A. Senior, V.\nVanhoucke, P. Nguyen, T. N. Sainath, and B. Kingsbury, Deep neural networks for\nacoustic modeling in speech recognition. IEEE Signal processing magazine 29 (2012).\n2. Y. LeCun, L. Bottou, Y. Bengio, P. Haﬀner, Gradient-based learning applied to docu-\nment recognition. Proceedings of the IEEE 86.11, 2278-2324 (1998).\n3. D.Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert,\nL. Baker, M. Lai, A. Bolton, Y. Chen, T. Lillicrap, F. Hui, L. Sifre, G. Driessche, T.\nGraepel, D. Hassabis, Mastering the game of go without human knowledge. Nature\n550.7676, 354 (2017).\n4. K. Fukushima, S. Miyake, Neocognitron: A new algorithm for pattern recognition tol-\nerant of deformations and shifts in position. Pattern recognition 15.6, 455-469 (1982).\n5. O. Delalleau, Y. Bengio, Shallow vs. deep sum-product networks. Advances in Neural\nInformation Processing Systems. 666-674 (2011).\n6. R. Eldan, O. Shamir, The power of depth for feedforward neural networks. Conference\non learning theory, 907-940 (2016).\n7. B. McCane, L. Szymanskic, Deep networks are eﬃcient for circular manifolds. 23rd\nInternational Conference on Pattern Recognition (ICPR). IEEE, 3464-3469 (2016).\n8. D. Rolnick, M. Tegmark, The power of deeper networks for expressing natural functions.\narXiv preprint arXiv:1705.05502v2 (2018).\n9. R. Pascanu, G. Mont´ufar, Y. Bengio, On the number of response regions of deep feed\nforward networks with piece-wise linear activations. arXiv preprint arXiv:1312.6098v5\n(2014).\n10. M. Raghu, B. Poole, J. Kleinberg, S. Ganguli, J. S. Dickstein, On the expressive power\nof deep neural networks. Proceedings of the 34th International Conference on Machine\nLearning-Volume 70. JMLR. org, 2847-2854 (2017).\n11. G. Mont´ufar, R. Pascanu, K. Cho, Y. Bengio, On the number of linear regions of deep\nneural networks. Advances in neural information processing systems, 2924-2932 (2014).\n12. M. Raghu, B. Poole, J. Kleinberg, S. Ganguli, J. Sohl-Dickstein, Survey of expressivity\nin deep neural networks. arXiv preprint arXiv:1611.08083v1 (2016).\n13. H. W. Lin, M. Tegmark, D. Rolnick, Why does deep and cheap learning work so well?.\nJournal of Statistical Physics, 168(6): 1223-1247 (2017).\n14. D. Yarotsky, Error bounds for approximations with deep ReLU networks. Neural Net-\nworks 94: 103-114 (2017).\n15. S. Liang, S. Rayadurgam, Why deep neural networks for function approximation?. arXiv\npreprint arXiv:1610.04161v2 (2017).\n16. V. Nair, G. Hinton, Rectiﬁed linear units improve restricted boltzmann machines. Pro-\nceedings of the 27th international conference on machine learning (ICML-10), 807-814\n(2010).\n17. X. Glorot, A. Bordes, Y. Bengio, Deep sparse rectiﬁer neural networks. Proceedings of\nthe fourteenth international conference on artiﬁcial intelligence and statistics, 315-323\n(2011).\n16\nChangcun Huang\n18. Y. LeCun, Y. Bengio, G. Hinton, Deep learning. Nature 521, 436-444 (2015).\n19. Y. Bengio, O. Delalleau, C. Simard, Decision trees do not generalize to new variations.\nComputational Intelligence, 26(4): 449-467 (2010).\n20. R. O. Duda, P. E. Hart, D. G. Stork, Pattern classiﬁcation, 2nd ed, John Wiley & Sons,\n216-219 (2001).\n21. R. Hecht-Nielsen, Theory of the backpropagation neural network. Neural networks for\nperception, Academic Press, 65-93 (1992).\n22. G. Cybenko, Approximation by superpositions of a sigmoidal function. Mathematics of\ncontrol, signals and systems, 2(4): 303-314 (1989).\n23. K. Hornik, M. Stinchcombe, H. White, Multilayer feedforward networks are universal\napproximators. Neural networks, 2(5): 359-366 (1989).\n24. R. P. Lippmann, An introduction to computing with neural nets. IEEE Assp magazine,\n4(2): 4-22 (1987).\n25. J. J. Hopﬁeld, Neural networks and physical systems with emergent collective com-\nputational abilities. Proceedings of the national academy of sciences, 79(8): 2554-2558\n(1982).\n",
  "categories": [
    "cs.LG",
    "cs.NA",
    "math.NA",
    "stat.ML",
    "I.2.0"
  ],
  "published": "2019-06-16",
  "updated": "2019-12-06"
}