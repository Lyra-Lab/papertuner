{
  "id": "http://arxiv.org/abs/1906.07372v4",
  "title": "RIDM: Reinforced Inverse Dynamics Modeling for Learning from a Single Observed Demonstration",
  "authors": [
    "Brahma S. Pavse",
    "Faraz Torabi",
    "Josiah P. Hanna",
    "Garrett Warnell",
    "Peter Stone"
  ],
  "abstract": "Augmenting reinforcement learning with imitation learning is often hailed as\na method by which to improve upon learning from scratch. However, most existing\nmethods for integrating these two techniques are subject to several strong\nassumptions---chief among them that information about demonstrator actions is\navailable. In this paper, we investigate the extent to which this assumption is\nnecessary by introducing and evaluating reinforced inverse dynamics modeling\n(RIDM), a novel paradigm for combining imitation from observation (IfO) and\nreinforcement learning with no dependence on demonstrator action information.\nMoreover, RIDM requires only a single demonstration trajectory and is able to\noperate directly on raw (unaugmented) state features. We find experimentally\nthat RIDM performs favorably compared to a baseline approach for several tasks\nin simulation as well as for tasks on a real UR5 robot arm. Experiment videos\ncan be found at https://sites.google.com/view/ridm-reinforced-inverse-dynami.",
  "text": "In IEEE Robotics and Automation Letter, Presented at IROS 2020\nJuly 2020\nRIDM: Reinforced Inverse Dynamics Modeling for Learning from a\nSingle Observed Demonstration*\nBrahma S. Pavse,1, Faraz Torabi,1, Josiah Hanna2, Garrett Warnell3, and Peter Stone4\nAbstract— Augmenting reinforcement learning with imitation\nlearning is often hailed as a method by which to improve\nupon learning from scratch. However, most existing methods\nfor integrating these two techniques are subject to several\nstrong assumptions—chief among them that information about\ndemonstrator actions is available. In this paper, we investigate\nthe extent to which this assumption is necessary by introducing\nand evaluating reinforced inverse dynamics modeling (RIDM), a\nnovel paradigm for combining imitation from observation (IfO)\nand reinforcement learning with no dependence on demonstra-\ntor action information. Moreover, RIDM requires only a single\ndemonstration trajectory and is able to operate directly on\nraw (unaugmented) state features. We ﬁnd experimentally that\nRIDM performs favorably compared to a baseline approach\nfor several tasks in simulation as well as for tasks on a\nreal UR5 robot arm. Experiment videos can be found at\nhttps://sites.google.com/view/ridm-reinforced-inverse-dynami.\nI. INTRODUCTION\nTwo of the most prevalent paradigms for behavior learning\nin artiﬁcial agents are imitation learning (IL) [1], [2] and\nreinforcement learning (RL) [3]. Agents that use IL receive a\nstrong training signal in the form of an expert demonstration,\nbut because their goal is to imitate, their task performance is\ntypically bounded above by that of the expert. Agents using\nRL, on the other hand, can theoretically learn behaviors that\nare optimal with respect to a predeﬁned task reward, but\noften have difﬁculty doing so due to practical challenges such\nas large state spaces and sparse reward functions. Because\nof the relative advantages and disadvantages of each of\nthese paradigms, it is natural to investigate whether one can\nintegrate them in order to get the best of both methods.\nWhile combining IL and RL has been explored to a certain\nextent in the literature [4], [5], [6], several important issues\nremain. Most importantly, these techniques require access to\n*This work has taken place in the Learning Agents Research Group\n(LARG) at UT Austin. LARG research is supported in part by NSF\n(CPS-1739964, IIS-1724157, NRI-1925082), ONR (N00014-18-2243), FLI\n(RFP2-000), ARO (W911NF-19-2-0333), DARPA, Lockheed Martin, GM,\nand Bosch. Peter Stone serves as the Executive Director of Sony AI\nAmerica and receives ﬁnancial compensation for this work. The terms of\nthis arrangement have been reviewed and approved by the University of\nTexas at Austin in accordance with its policy on objectivity in research.\nFirst Author and Second Author had equal contribution\n1First Author and Second Author are with Computer Science Depart-\nment, University of Texas at Austin, USA brahmasp@utexas.edu,\nfaraztrb@cs.utexas.edu\n2Third Author is with School of Informatics, University of Edinburgh,\nScotland. To be joining the Computer Sciences Department, University of\nWisconsin-Madison, USA. josiah.hanna@ed.ac.uk\n3Fourth\nAuthor\nis\nwith\nArmy\nResearch\nLaboratory,\nUSA\ngarrett.a.warnell.civ@mail.mil\n4Fifth Author is with Computer Science Department, University of Texas\nat Austin and Sony AI, USA. pstone@cs.utexas.edu\nFig. 1.\nRIDM applies a task-speciﬁc inverse dynamics model, Mθ, on\nthe learner’s current state, st, to the expert’s next state, se\nt+1, such that the\nsequence of executed actions, { eat}, maximizes the cumulative reward from\nthe environment. At each time step, the agent uses the expert’s next state,\nse\nt+1 (black dot), as the set point for Mθ. However, it actually reaches\nst+1 (red dot) instead—which is typically not the set point—since RIDM\noptimizes Mθ to maximize environment task reward instead of minimizing\ntrajectory-tracking error.\nthe internal control signals used by a demonstrator in order\nto be able to leverage the demonstration information [7],\n[6], [8]. This requirement makes it difﬁcult to obtain useful\ndemonstrations since it necessitates a high level of internal\naccess to the demonstration platform, preventing, e.g., the\nuse of numerous, easily-accessible video demonstrations\navailable on websites like YouTube. A second limitation\nof many existing techniques is the requirement for many\nexpert demonstrations [9], which makes obtaining sufﬁcient\ndemonstration data difﬁcult in that it requires a high level\nof access to expert demonstrators. Finally, existing methods\ntypically assume that they have access to task-speciﬁc state\nfeatures during the learning process that can be used to make\nlearning easier [7], [10], [11]. Task-speciﬁc state features\nare ones that somehow augment the agent’s natural (or raw\nstate information using additional domain knowledge—like\nthe distance to certain important subgoals—designed to make\nreward function representation easier (see, e.g., Figure 2).\nWhile providing this domain knowledge may be fairly easy\nfor a speciﬁc task, it will, in general, need to be speciﬁed\nanew for each new task encountered and therefore represents\na practical impediment to using existing methods.\nIn this paper, we propose a new technique for integrating\nIL and RL called reinforced inverse dynamics modeling\n(RIDM) that bypasses the issues identiﬁed above. RIDM\nleverages recent ideas from model-based imitation from\nobservation (IfO) to enable integrated imitation and rein-\nforcement learning from a single, action-free demonstration\nconsisting of only raw states. Moreover, RIDM represents a\nnew paradigm for combining IL and RL in that the agent’s\nbehavior is based on following a ﬁxed demonstration tra-\narXiv:1906.07372v4  [cs.LG]  21 Jul 2020\njectory using a parameterized task-speciﬁc inverse dynamics\nmodel (IDM) (see, Figure 1). A task-speciﬁc IDM is one\nthat maps state-transitions to actions for a speciﬁc task only\nand and may not generalize to other tasks that have different\nreward functions. While RIDM requires the demonstration\ntrajectory during execution, its overall objective is not to\nimitate, but rather to maximize the external environment\nreward. RIDM accomplishes this by using RL to tune the\nIDM that attempts to follow the ﬁxed demonstration such\nthat the resulting behavior leads to the highest environmental\nreward. Formulating the overall RL problem in this way\nallows RIDM to diverge from the demonstration if doing\nso will lead to higher task reward, which is helpful when the\ndemonstration is sub-optimal. To the best of our knowledge,\nwe are the ﬁrst to introduce an algorithm that combines IfO\nand RL.\nTo evaluate our algorithm, we establish a baseline al-\ngorithm by modifying a state-of-the-art IfO algorithm to\nincorporate an external reward signal. We hypothesize that\nRIDM will be able to outperform this baseline in the problem\nof interest where a few, raw-state demonstration is provided.\nWe perform several quantitative experiments focused on both\nsimulated and real robot control tasks, and ﬁnd that RIDM’s\nunique, model-driven approach results in high-quality be-\nhavior trajectories that lead to better performance than the\nbaseline.\nII. RELATED WORK\nThis section provides a broad outline of research related to\nour work. The section is organized as follows. Section II-A\ndetails the most related works on imitation from observation\nand reinforcement learning. Section II-B discusses efforts in\nintegrating reinforcement learning and imitation learning.\nA. Imitation from Observation and Reinforcement Learning\nThe focus of imitation from observation (IfO) [12], [13]\nis to learn a policy that results in similar behavior as the ex-\npert demonstration with state-only demonstrations. There are\nbroadly two approaches: (1) model-based and (2) model-free.\nIn our work, we are focused on model-based approaches.\nMany model-based IfO algorithms use an inverse dynam-\nics model, i.e., a mapping from state-transitions to actions.\nThe most related work to ours may be the work of Nair\net al.[14], who show the learner a single demonstration\nof an expert performing some task with the intention of\nthe learner replicating the task. Their algorithm allows the\nlearner to undergo self-supervision by collecting states and\nactions, which are then used to train a neural network inverse\ndynamics model. The learned model is then applied on the\nexpert demonstration to infer the expert actions. The actions\nare then executed to replicate the demonstrated behavior. An-\nother method is behavioral cloning from observation (BCO)\n[10], which consists of two phases. The ﬁrst phase trains\nan inverse dynamics model in a self-supervised fashion, and\napplies the learned model on the expert demonstration(s) to\ninfer the expert actions. The second phase involves training\na policy by behavioral cloning (BC) [15], which maps the\nexpert states to the inferred actions. BCO, however, does\nnot factor in the environment reward to train the inverse\ndynamics model or policy in either of the phases.\nIterative learning control (ILC) [16] is an older trajectory\ntracking approach which operates in a repetitive manner to\nimprove its tracking precision. The methods developed in\nILC, often use PID controllers and attempt to optimize the\nPID gains so that the agent follows the reference trajectory\nmore accurately. Our work differs from ILC in that our\nobjective is not accurate trajectory tracking, but rather to\nmaximize the available environment reward, and we use RL\nto achieve that objective The beneﬁt of our work is that if\nthe demonstration is sub-optimal, the ﬁnal learned behavior\ncould potentially outperform the demonstrator’s.\nThe focus of reinforcement learning is to train agent to\nlearn a task in an environment by maximizing some notion\nof cumulative reward. In our work, we are focused on\nusing black-box optimization methods. Some of the most\nrelated works are as follows. Hwangbo et al. [17] propose\na method, ROCK*, for tuning a PD controller that performs\nfavorably to CMA-ES on their experiments. Calandra et al.\n[18] use Bayesian Optimization to tune a state machine for\nrobotic locomotion. They also test their method on a linear\ncontroller (which corresponds to a PD controller if the states\ncontain positions and velocities). Neuman-Brosig et al. [19]\napply Bayesian optimization for learning the parameters for\nactive disturbance rejection control. Leonetti et al. [20] use\ncontrolled random search to tune a linear controller. Black\nbox optimization for controller tuning has also been applied\nin several undergrad theses and reports [21]. Our work differs\nfrom the mentioned past work in this area in that we integrate\nIfO and RL which potentially helps with constraining the\namount of exploration required for learning the behavior.\nB. Integrating Reinforcement Learning and Imitation Learn-\ning\nAnother area of research related to our work is dealing\nwith the case in which an expert demonstration may be\nsub-optimal. One way to address this issue is by combining\nreinforcement learning and imitation learning.\nThere has been signiﬁcant effort to combine reinforcement\nlearning and imitation learning. For example, Taylor et al.\n[4] introduced Human-Agent Transfer, an algorithm that\nuses a human demonstration to build a base policy, which\nis further reﬁned using reinforcement learning on a robot\nsoccer domain. Lakshminarayanan et al.[5] uses a hybrid\nformulation of reward and expert state-action information in\nthe replay buffer when training deep Q-network (DQN) to\nspeed-up the training procedure. Hosu et al.[22] use deep RL\nto learn an Atari game but they use human checkpoint replays\nas starting points during the learning process instead of re-\nstarting the game at the end of the episode. Subramanian\net al.[23] and Nair et al.[24] use IL information to alleviate\nthe exploration process in RL. Hester et al.[8] pre-train a\ndeep neural network by optimizing a loss that includes a\ntemporal difference (TD) loss as well as supervised learning\nloss with the expert actions. Zhu et al. [6] optimize a\nlinear combination of the imitation reward outputted by\ngenerative adversarial imitation learning (GAIL) [7] and the\ntask reward. These works assume that the learner has access\nto the expert’s actions.\nOur work is distinct from all these works in that we focus\non the integration of reinforcement learning and imitation\nfrom observation where we only have access to expert state\ntrajectories – not the expert actions.\nIII. PRELIMINARIES\nWe begin by reviewing and establishing notation for re-\ninforcement learning, imitation learning, and imitation from\nobservation.\nA. Reinforcement Learning (RL)\nWe model agents interacting in an environment as a\nMarkov decision process (MDP). A MDP is denoted by the\ntuple M = ⟨S, A, T, R⟩, where S is the state space of the\nagent, A is the action space of the agent, T deﬁnes the\nenvironment transition function that gives the probability of\nthe agent moving from one state to another given that the\nagent took a particular action (i.e., T : S × A × S →[0, 1]),\nand R is the scalar-valued reward function that dictates the\nreward received by the agent when moving from one state\nto another via a particular action. In the context of the MDP\nframework, the reinforcement learning problem is that of\noptimizing the agent’s behavior so as to ﬁnd a control policy,\nπ⋆: S →A, that the agent can use to maximize the total\ncumulative reward it receives.\nB. Imitation Learning (IL)\nIn contrast to reinforcement learning, imitation learning\ninvolves a learner seeking to mimic the behavior of an expert\ndemonstrator rather than maximizing an external reward\nsignal. We denote demonstrations as De = {(se\nt, ae\nt)}, where\nse\nt denotes the state of the expert at a given time index t,\nand ae\nt denotes the action taken by the expert at that time.\nGiven one or many such demonstrations, the goal of IL is\nto learn a control policy π that the learning (imitating) agent\ncan use to produce behavior similar to that of the expert.\nIL in the absence of expert action information, i.e., when\nDe = {se\nt}, is called imitation from observation (IfO). The\nIfO problem is that of learning the same imitation policy\nπ as in IL, but without access to this action information.\nThe learner is shown only the states of the expert. In this\ncase, most IL methods no longer apply, and we must ﬁnd\nnew strategies. We might try, for example, to infer the expert\nactions {ae\nt} to get { eae\nt} for each state {se\nt}, and therefore\napproximate f\nDe = {(se\nt, eae\nt)} and then apply conventional\nIL methods as done by Torabi et al. [10]. In this work, we\nstudy the problem of integrating IfO with RL.\nIV. REINFORCED INVERSE DYNAMICS MODELING\nWe now introduce reinforced inverse dynamics modeling\n(RIDM) – a new method for integrating IfO and RL. RIDM\nlearns a strategy by which an agent can select actions { eat}\nthat allow it to achieve a high level of task performance when\nit has available a single, state-only expert demonstration,\nDe = {se\nt}.\nRIDM does so by learning and using a task-sepciﬁc inverse\ndynamics model (IDM), Mθ, that infers which action to take\nat any given time instant based on both the agent’s current\nstate and a desired next state, the set point. Under RIDM, the\nagent’s actions are computed as eat = Mθ(st, se\nt+1), where\nst is the learner’s current state and se\nt+1 is the state of the\nexpert at the next time instant. The goal of RIDM is to ﬁnd an\noptimal θ such that the generated action sequence maximizes\nthe cumulative reward from the environment, Renv(Mθ).\nThat is, while RIDM selects which actions to take by using\nthe expert’s state sequence as a sequence of set points, it\nevaluates its policy in relation to the environmental reward\nas opposed to, e.g., the trajectory-tracking error. Note that\nit may actually be desirable for the induced state sequence\nto differ from that of the expert’s if doing so allows for\nhigher environment reward. Figure 1 depicts this process. To\nthe best of our knowledge, using such a scheme to perform\nintegrated IfO and RL is unique in the literature.\nRIDM consists of two phases. The goal of the ﬁrst phase\nis to initialize the inverse dynamics model. This phase can\neither be done by selecting θ at random, or – if a known\npolicy is available to the learner – by having the agent\ngenerate its own set of state-action-next-state triples and\nusing supervised learning to ﬁt θ to those triples. In the\nsecond phase, RIDM alternates between generating agent\nbehavior according to θ and the expert demonstration, and\noptimizing θ in response to the amount of environment\nreward obtained by the generated behavior. During this\nphase, the learner uses the demonstration to guide the agent’s\nbehavior (i.e., imitation), but uses the observed environment\nreward to adjust θ such that actions leading to high rewards\nare generated (i.e., reinforcement). The goal of this two-\nphase procedure is to ﬁnd the optimal policy in terms of total\ntask reward (which may outperform the expert) by using the\nexpert demonstration as a guide. The pseudo-code for RIDM\nis given in Algorithm 1, and each phase is described in more\ndetail below. 1\nA. Inverse Dynamics Model Pre-training\nDuring RIDM’s optional ﬁrst phase, an initial value for θ\nis sought. This initialization is accomplished either through\nthe use of data collected by the learner using a pre-deﬁned\nexploration policy or, if such a policy is not available, by\nselecting the parameter value at random. We allow for RIDM\nto take advantage of an available exploration policy so that\nit can achieve a reasonable level of task performance, which\nis likely to get us into a good basin of attraction within the\noptimization landscape.\nIn the case where an exploration policy πpre is available\n(e.g. if a slow-walk policy is available and we want the agent\n1Even though RIDM is described as a method that requires both envi-\nronment rewards and state-only demonstrations, the algorithm can be used\neven if the reward is not available for instance by deﬁning the reward as the\nnegative of the distance between the demonstrated state and the imitator’s\nstate at each time step.\nAlgorithm 1 RIDM\nRequire: Single, state-only demonstration De := {se\nt}\n1: if πpre available then\n2:\nGenerate Dpre := {(spre\nt\n, apre\nt\n)} using πpre\n3:\nInitialize θ as the solution to (1)\n4: else\n5:\nInitialize θ uniformly at random\n6: end if\n7: while θ not converged do\n8:\nfor t = 0 : |De| −1 do\n9:\neat := Mθ(st, se\nt+1)\n10:\nExecute eat and record st+1 and reward rt\n11:\nend for\n12:\nCompute cumulative episode reward Renv = P\nt rt\n13:\nUpdate θ by solving (2)\n14: end while\n15: return θ∗\nto learn a fast walk), RIDM computes an initial value for θ as\nfollows. First, the learner executes πpre in the environment\nand records the resulting experience as a trajectory of length\nT that we denote as Dpre = {(spre\nt\n, apre\nt\n, spre\nt+1)}. The\ninitial value for θ is then computed by solving the following\nsupervised learning problem:\nθ∗= arg max\n \n−1\nT\nT\nX\nt=1\nN\nX\nn=1\n|Mθ(spre\nt\n, spre\nt+1)n −apre\ntn |\nmax (apre\nn ) −min (apre\nn )\n!\n,\n(1)\nwhere N is the dimensionality of the action space, apre\ntn\ndenotes the scalar value of the nth component of the action\nvector apre\nt\n, and max(apre\nn ) denotes the maximum value of\napre\ntn\nacross all t. Above, notice that the goal of the opti-\nmization problem is to select θ such that Mθ(spre\nt\n, spre\nt+1)n\nis a good approximation of the true action value apre\ntn . We\nadopt the particular loss given above because we found that\nit worked well in practice. It is able to effectively trade\noff short-term errors in order to optimize the differences\nacross a full trajectory, and the normalization term ensures\ngreater accuracy for actions which vary over a smaller range.\nRIDM solves (1) using a blackbox optimization technique\n(e.g., CMA-ES[25]). Note, however, that this pre-training\nphase is optional, and only possible when RIDM has access\nto an exploration policy that generates a behavior that is\nqualitatively similar to the desired end behavior.\nB. Inverse Dynamics Model Reinforcement\nRIDM’s required second phase seeks to iteratively update\nthe inverse dynamics model parameters in response to the\nenvironment return. The process executed here is illustrated\nin Figure 1, where one can see that RIDM uses the expert’s\ndemonstration as a behavior template in the sense that the\nexpert’s state trajectory is used as a sequence of set points\nto guide behavior.\nThe iterative updates to θ are computed as follows. First,\nthe learner uses Mθ and the expert demonstration to generate\na trajectory of experience. It does so by, when in state\nst at time step t, executing action eat = Mθ(st, se\nt+1),\nwhich results in a transition to state st+1 and the obser-\nvation of reward rt. After this trajectory has been generated,\nthe learner computes the cumulative environment reward\nRenv(De ; θ) = P\nt rt, which is dependent on both the\n(ﬁxed) expert demonstration data De and the (tunable) model\nparameters θ. In a given iteration, i, an update to θ is\ncomputed as the solution to:\nθi = arg max Renv(De ; θi−1) .\n(2)\nIt is important to note that, here, expert’s actions are\nunknown. While Renv(De ; θ) is used to reinforce the\nlearning of the inverse dynamics model parameters, the\nlearner is always guided by the same, ﬁxed, state-only expert\ndemonstration trajectory.\nFor each iteration of the above procedure, RIDM solves (2)\nagain using a blackbox optimization technique (eg., CMA-\nES[25] or Bayesian optimization[26]).\nV. EMPIRICAL RESULTS\nWe now empirically validate our hypothesis, i.e., that\nbehaviors learned using RIDM will outperform those learned\nby the established baseline. We focus on the case in which\nonly a single, state-only demonstration is available to the\nagent and no task-speciﬁc state augmentation can be per-\nformed. Our experiments are executed in multiple robot\ncontrol domains: simulated tasks are carried out in the\nMuJoCo and SimSpark simulators, and several manipulation\ntasks are carried out on a UR5 robot arm. In the ﬁrst set\nof experiments with the MuJoCo simulator, neural networks\nare used to model IDMs since such models have proven\neffective in these domains in the literature. A neural network\nIDM gets one (state,next-state) pair as input and outputs the\nGaussian distribution parameters from which an action is\nto be sampled and executed by the agent. In the rest of\nthe experiments, i.e., the SimSpark simulator and physical\ntasks, PID controllers are used to model IDMs since, again,\nthese have proven effective in these domains in the past. PID\ncontrollers are not exactly inverse dynamics models due to\ntheir differential and integral terms. However, they retain the\nessential characteristic that RIDM requires, i.e., that, given\na current state and desired next state (or set point), they will\ngenerate an action that attempts to reach the set point. In the\ntraining process, the next state (or set point) at each time\nstep is ﬁxed according to the demonstration, and RIDM uses\nreinforcement learning to tune the IDM parameters (either\nNN parameters or PID gains) such that the overall agent\nbehavior can best maximize an external environment reward.\nRIDM uses CMA-ES[25] in all the simulated experiments,\nbut uses Bayesian optimization[26] in the physical robot\nexperiments to learn the inverse dynamics model due to its\nsuperior sample complexity.\nThis section is organized as follows. First, we provide\nexperimental motivation for studying RIDM in the single,\nstate-only demonstration and raw state space case. Next, we\nestablish a reasonable IfO+RL baseline that can also operate\nin this regime, and we validate our hypothesis by comparing\nRIDM to this baseline. In each of our evaluations, we scale\nthe reported performance metrics such that a score of 0\ncorresponds to the behavior of a random policy, and a score\nof 1 corresponds to the behavior of the expert. Note that,\nwhen the expert performance is sub-optimal, because we are\ncombining imitation learning with reinforcement learning, it\nis reasonable to expect that the algorithm will outperform\nthe expert on some tasks. Finally, we conclude this section\nby reporting additional empirical results for applying RIDM\nto both simulated robot soccer skill learning and to learning\nto perform a behavior on a physical UR5 arm robot.\nA. Experimental Setup\nPrior research has established that the availability of\ndemonstrator action information and many demonstration\ntrajectories are critical for the success of existing imitation\nlearning algorithms [10], [11]. While RIDM is advantageous\nin that it does not depend on the availability of the above\ninformation, we have also claimed that it can operate directly\non raw state information that has not been augmented using\nextra knowledge of the task which, of course, is typically un-\nknown or difﬁcult to obtain. We now seek to experimentally\nmotivate the need for overcoming this issue by showing the\nlevel of reliance on these augmented state spaces in many\nexisting imitation and reinforcement learning algorithms.\nIn Figure 2, we compare the scaled performance of an\nimitation from observation algorithm (GAIfO[11]) and a\nreinforcement learning algorithm (TRPO[27]) when they are\ngiven access to an augmented state space vs. when they are\nexposed to only the raw state space on six tasks from the\nMuJoCo domain [28] 2. Here, the raw state space refers\nto the list of joint angles, and the augmented state space\nalso includes task-speciﬁc information. For instance, in the\nHopper task, the augmented state space includes the agent’s\nglobal position which is advantageous in that it is more\nhighly correlated with the reward signal (see, e.g., [11], [29],\n[7], [27], [30]). In this speciﬁc task, the agent’s goal is that of\ncontrolling the limbs of a 2D, one-legged robot such that it\nmoves forward as fast as possible. The task reward given per\ntime step corresponds to the change in global position of the\nagent, and since this information appears in the augmented\nstate information, both learning algorithms perform much\nbetter (except for GAIfO in the Ant domain, perhaps due\nto the very large (111 dimensional) augmented state space).\nThis advantage can be seen in Figure 2, where the high\nreliance of GAIfO[11] and TRPO[27] on the augmented state\nspace is readily apparent. Such augmentations are, in general,\nrestrictive in that they need to be redeﬁned for each new task.\nBecause we seek to remove the above restriction, we\nuse only the joint angles as the raw state information\nin our experiments. Many robots are comprised of joints,\nand therefore a joint-only representation is reasonably task-\nindependent. The core results of our algorithm are exclu-\n2The MuJoCo experiments use all the standard settings, e.g., the reward\nfunctions, the goals, and the augmented state spaces, as deﬁned in the\nMuJoCo code base.\nReacher HalfCheetah Swimmer\nHopper\nAnt\nWalker2d\nDomain\n0.0\n0.5\n1.0\nScaled Performance\nTRPO (Raw)\nTRPO (Augmented)\nGAIfO (Raw)\nGAIfO (Augmented)\nFig. 2.\nQuantitative exhibition of the importance of an augmented state\nspace for high performance on six MuJoCo domains for GAIfO and TRPO.\nMean and standard deviations are over 100 policy runs. Both methods use\na neural network parameterized policy.\nsively concerned with dealing with the above case, i.e., single\nstate-only demonstration consisting of joint angles.\nB. RIDM Applied to MuJoCo Simulation\nWe now present our core results. This section is organized\nas follows. Section V-B.1 proposes a reasonable baseline\nto compare against our method. Section V-B.2 presents the\nperformance of our algorithm against this baseline.\n1) Baseline: GAIfO+RL: To the best of our knowledge,\nthere is no method in the existing literature which can operate\nin the experimental setting of interest. Therefore, in order to\nunderstand the effectiveness of RIDM, we ﬁrst propose the\nnatural combination of the best existing algorithms for the\ncomponents of RIDM, namely IfO and RL as the baseline.\nGAIfO+RL is based on the current state-of-the-art imita-\ntion from observation algorithm, GAIfO[11]. Starting from\nGAIfO, GAIfO+RL integrates RL by modifying the reward\nfunction used during the agent update step. Instead of the\nreward function being determined solely by the discriminator\nas in GAIfO, GAIfO+RL integrates imitation and reinforce-\nment learning by deﬁning a new reward function that is a\nlinear combination of the discriminator’s output and the task\nreward [6].\nIn Figure 3, we establish that GAIfO+RL is a strong\nbaseline by evaluating the performance of GAIfO alone,\nRL alone (TRPO/PPO), and GAIfO+RL. All three methods\noperate in the raw (un-augmented) state space, and GAIfO\nand GAIfO+RL are also given access to a single, state-only\ndemonstration. While the performance of either pure IfO or\npure RL alone is relatively poor, we can see that GAIfO+RL\nachieves signiﬁcantly higher performance than its parts.\nMoreover, GAIfO+RL operates in the same established\nregime and belongs to the same class of IfO+RL algorithms\nas RIDM, and therefore seems to be a reasonable imitation\nfrom observation + reinforcement learning algorithm.\n2) Hypothesis Validation: We conducted an experiment\ncomparing the scaled performance of RIDM against that\nof the GAIfO+RL baseline on six tasks from the MuJoCo\ndomain. The experts are generated using TRPO/PPO with\naugmented states. However, the demonstrated trajectories\nonly include the raw state information. Here, we ﬁrst model\nReacher\nHalfCheetah Swimmer\nHopper\nAnt\nWalker2d\nDomain\n0.00\n0.25\n0.50\n0.75\n1.00\nScaled Performance\nGAIfO\nTRPO/PPO\nGAIfO + RL\nFig. 3.\nEstablishment of GAIfO+RL as a reasonable IfO+RL baseline\nto compare against RIDM. All methods use the same single state-only\ndemonstration consisting of only raw states (exclusively of joint angles).\nMean and standard deviations are over 100 policy runs. All methods use a\nneural network parameterized policy.\nReacher\nHalfCheetah\nSwimmer\nHopper\nAnt\nWalker2d\nDomain\n0.00\n0.25\n0.50\n0.75\n1.00\nScaled Performance\nGAIfO + RL\nRIDM with NN\nFig. 4. Comparison of RIDM ﬁnal performace against established baseline,\nGAIfO+RL, on the MuJoCo domain on the same single state-only demon-\nstration consisting of only raw states (exclusively of joint angles). Mean and\nstandard deviations for GAIfO+RL and RIDM are over 100 policy runs.\nGAIfO+RL uses a neural network parameterized policy. For each domain,\nin order of x-axis, the numbers of iterations required for RIDM are 700,\n800, 400, 100, 900, and 1300 and for GAIfO+RL are 400, 800, 1000,\n1000, 1200, and 1500\nthe inverse dynamics model using a neural network and\ntrain the network to maximize the received reward while\nattempting to follow the expert trajectory. The results are\npresented in Figure 4. It can be seen that RIDM outperforms\nGAIfO+RL in ﬁve of the domains. The only domain that the\nperformance is worse than the baseline is the Ant domain.\nWe speculate that the neural network IDM is not able to\nlearn a meaningful model due to the complexity of the\ndomain resulting from the large number of joints compared\nto each of the other domains. In Section VI, we show that\nif RIDM uses a lower-dimensional parameterized IDM (e.g.\na PID controller), the performance of the learning agents is\nimproved.\nC. RIDM Applied to SimSpark RoboCup 3D Simulation\nWe now report the results of using RIDM to learn\nagent behaviors in the RoboCup 3D simulation environment,\nSimSpark[31], [32]. Speciﬁcally, our goal was to determine\nwhether or not RIDM could imitate agent skills exhibited by\nthe agents of other teams that participate in the RoboCup 3D\nsimulation competition [33]. Since the opponent’s policies\nare unknown, we obtain the demonstration by executing the\nteams’ computer-readable but non-human-readable code in\nthe environment.\nIn our experiments, we are interested in two tasks: (1)\nspeed walking, and (2) long distance kick-offs. We collect\n0\n200\n400\n600\n800\n1000\nNumber of Iterations\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\nScaled Performance\nGAIfO + RL\nRIDM with NN\nFig. 5.\nComparison of RIDM learning process against established base-\nline, GAIfO+RL, on the Swimmer domain on the same single state-only\ndemonstration consisting of only raw states (exclusively of joint angles).\nSolid lines represent the mean return and shaded areas represent standard\ndeviations over 10 trials. While the shown graph is for Swimmer-v2, we\nobserved the same qualitative trend on other domains as well.\nTABLE I\nRIDM VS. EXPERT FOR SPEED WALKING.\nExpert\nAgent\nSpeed (m/s)\nReward\nFCP\nRIDM (ours)\n0.81\n9.82\nExpert\n0.69\n8.35\nFUT-K\nRIDM (ours)\n0.89\n10.70\nExpert\n0.70\n8.47\ndemonstration data of two teams, FC Portugal (FCP) [34] and\nFUT-K [35]. RIDM pre-trained the model (see Section IV-\nA) using walk and kick exploration policies from our own\nteam, UT Austin Villa[33]. Here, we report results using\nonly RIDM since it proved infeasible to evaluate GAIfO+RL\nin this domain due to the computational time complexity.\nWe found that RIDM performed best with global PD gains\ncommon to all joints as the inverse dynamics model.\nBelow are the reward function details of each task:\n• Speed walking: Summation of distances (meters) trav-\nelled per time-step with a −5 penalty for falling down.\n• Long-distance kick-off:\nRkick = (1 + xtotal) · exp\n\u0010−θ2\n180\n\u0011\n+ xair · 100\nwith a penalty −5 for bumping the ball, −10 for falling\ndown, where xtotal is the x-axis distance traveled by the\nball, θ is the angle between the ball’s trajectory and the\nline between the agent and center of the goal, and xair\nis the x-axis distance for which the ball was traveling\nin the air. Distances are in meters, and θ is in degrees.\nSince we deﬁned these reward functions independently from\nthe demonstrations, the demonstrations do not optimize\nthe reward signals. The demonstrators are trained for the\nRoboCup task; their performances are sub-optimal with\nregards to our designed reward functions.\nTables I and II and summarize our results. We report both\nthe performance of the expert and our agent. We can see that,\nsince RIDM takes advantage of both the reward functions\nand the demonstrations, it allows our agents to outperform\nthe sub-optimal experts.\nTABLE II\nRIDM VS. EXPERT FOR LONG-DISTANCE KICK OFFS.\nExpert\nAgent\nxair (m)\nxtotal (m)\nReward\nFCP\nRIDM (ours)\n13.78\n24.05\n1386.00\nExpert\n8.00\n17.00\n808.00\nFUT-K\nRIDM (ours)\n10.62\n16.23\n1064.00\nExpert\n0.00\n10.00\n1.00\nD. RIDM Applied to a Physical UR5 Robot Arm\nWe also used RIDM for behavior learning on a physical\nrobot. Speciﬁcally, we used a UR5, a 6-degree-of-freedom\nrobotic arm. We considered a reaching task in which the arm\nbegins in a consistent, retracted position, then must move\nits end effector (i.e., the gripper at the end of the arm) to a\ntarget point in Cartesian space, and ﬁnally must stop moving\nonce the end effector has reached the target point. We trained\nthe expert by iterating between iLQR [36] and dynamics\nlearning with a speciﬁed reward function. We then executed\nthis expert policy and recorded the resulting trajectories to\ncreate the demonstration data [37].\nFor the physical arm experiments, we skip the pre-training\nphase for two reasons: 1) we did not have access to a sub-\noptimal policy for each task, and 2) for safety concerns, we\ndid not want to use a random exploration policy. For RIDM’s\nsecond phase, we used a reward function deﬁned as the\nnegative of the Euclidean distance of the end effector of the\narm to the target point at each timestep. We used Bayesian\noptimization[26] as the blackbox optimization algorithm to\nupdate the model parameters in response to the environ-\nment reward. Bayesian optimization works by constructing\na posterior distribution over the space of functions being\noptimized over. Here, this distribution was represented using\na Gaussian process over functions that map PID values to\nthe episode returns. As the training proceeds and more data\nis observed, Bayesian optimization techniques sharpen the\nposterior, resulting in more certainty as to which regions of\nthe parameter space are worth exploring further with more\ntrials and which are not. For simpler optimization problems,\nBayesian optimization is more sample efﬁcient compared to\nCMA-ES and converges within a few iterations.\nTable III represents the results of our experiments on\nthe UR5, where we compare RIDM to a baseline behavior\ngenerated by using the demonstration state sequence as\nset points for the platform’s pre-deﬁned, hard-coded PID\ncontroller parameterization. The reported numbers are the\naverages and standard deviations of episode returns over ﬁve\nseparate experiments all of which are reaching tasks with\ndifferent target points. Table III shows that while RIDM\noutperforms the original PID, it is worse than the expert.\nThe reason is that the expert is optimal with regards to the\ndesigned reward function.\nVI. ADDITIONAL RESULTS\nDue to the high performance of RIDM with PID con-\ntrollers, we performed another set of experiments on the\nMuJoCo domains that used a PID controller as the IDM\nTABLE III\nRIDM VS. ORIGINAL PID CONTROLLER VS. EXPERT.\nAgent\nReaching\nPushing\nPouring\nRIDM (ours)\n-11.94 (1.55)\n-19.01 (1.03)\n-5.87 (0.08)\nOriginal PID controller\n-36.57 (0.97)\n-58.98 (0.15)\n-15.67 (0.68)\nExpert\n-5.64 (0.76)\n-8.43 (0.11)\n-2.31 (0.04)\nReacher\nHalfCheetah Swimmer\nHopper\nAnt\nWalker2d⋆\nDomain\n0.00\n0.25\n0.50\n0.75\n1.00\nScaled Performance\nMax PID in Random Search\nRIDM with NN\nRIDM with PID\nFig. 6.\nComparison of RIDM with PID as the IDM versus RIDM with\nNN as the IDM and the maximum performance between the randomly\ngenerated PID values on the MuJoCo domain on the same single state-only\ndemonstration consisting of only raw states (exclusively of joint angles).\nSince RIDM with PID uses a deterministic inverse dynamics model, we\ndo not report mean or standard deviations of our algorithm. ⋆PID version\nof RIDM used global PID gains for Walker2d-v2, unlike on other domains\nwhere it used local PD gains.\ninstead of a neural network. We compare RIDM with PID\ncontroller to 1) RIDM with NN (from Figure 4) and 2) best-\nperforming randomly generated PID gains. We determined\nthe best-performing random PID gain by sampling 100\nsets of PID gains from a Gaussian distribution with mean\n[0.45, 0.75, 0.15] and standard deviation [.5, .5, .5] (each in-\ndex in the list corresponds to P, I, and D), and selecting the\nbest-performing set. Figure 6 provides experimental results\nfor all six of the domains. One can see that RIDM with a\nPID controller performs similarly to RIDM with an NN, and\nin more complex domains such as Ant and Walker2d, sig-\nniﬁcantly outperforms it. The reasonably good performance\nof random PID gains shows us that even an un-trained PID\ncontroller is an effective IDM. RIDM with a PID controller\nis able to focus on optimizing just the (very few) parameters\nof the PID controller (i.e., the gains) as opposed to a neural\nnetwork policy, where the policy space is much larger.\nVII. CONCLUSION\nIn this paper, we investigated whether or not several\nrestrictive assumptions common to many techniques that\nintegrate imitation and reinforcement learning – access to\ndemonstrator action information, access to several demon-\nstrations, and knowledge of task-speciﬁc state augmentations\n– are necessary. We hypothesized that they are not, and\nwe proposed a new algorithm called RIDM in order to\nvalidate that hypothesis. RIDM is a fundamentally new\nmethod for integrated imitation and reinforcement learning\nthat operates in scenarios for which only a single, raw-state-\nonly demonstration is provided. We experimentally demon-\nstrated that RIDM can ﬁnd behaviors that achieve good task\nperformance in these scenarios. Moreover, our results show\nthat it outperforms a reasonable baseline technique while\ndoing so. We posit that the success of RIDM is due to the\nway in which it generates behavior trajectories and performs\nlearning – RIDM generates behavior by directly using the\ndemonstration data as the set points for a parameterized but\nrobust inverse dynamics model, and iteratively optimizes the\nmodel parameters in response to the environment reward. The\nabove procedure not only generates reasonable trajectories\nover which to learn, but also reduces the learning problem\nto one over a relatively low-dimensional set of parameters\nwhen compared to other approaches.\nThis paper opens up many possible directions for future\nwork. For one, it may be possible to extend RIDM to learn\na generalized controller from numerous demonstrations of\na speciﬁc task. For example, in an arm-reaching task, we\nmay have two different demonstrations with two different\ntarget reaching points. Another open question is how RIDM\nwill perform when using optimization algorithms other than\nCMA-ES, such as TRPO or PPO. Furthermore, in all of\nour experiments, the state spaces are low-level features\n(only joint angles). Another possible future direction is to\ninvestigate how RIDM performs with video demonstrations.\nREFERENCES\n[1] S. Schaal, “Learning from demonstration,” in Advances in neural\ninformation processing systems, 1997, pp. 1040–1046.\n[2] B. D. Argall, S. Chernova, M. Veloso, and B. Browning, “A survey\nof robot learning from demonstration,” Robotics and autonomous\nsystems, vol. 57, no. 5, pp. 469–483, 2009.\n[3] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT press, 2018.\n[4] M. E. Taylor, H. B. Suay, and S. Chernova, “Integrating reinforce-\nment learning with human demonstrations of varying ability,” in The\n10th International Conference on Autonomous Agents and Multiagent\nSystems-Volume 2, 2011, pp. 617–624.\n[5] A. S. Lakshminarayanan, S. Ozair, and Y. Bengio, “Reinforcement\nlearning with few expert demonstrations,” in NIPS Workshop on Deep\nLearning for Action and Interaction, vol. 2016, 2016.\n[6] Y. Zhu, Z. Wang, J. Merel, A. A. Rusu, T. Erez, S. Cabi, S. Tun-\nyasuvunakool, J. Kram´ar, R. Hadsell, N. de Freitas, and N. Heess,\n“Reinforcement and imitation learning for diverse visuomotor skills,”\nCoRR, vol. abs/1802.09564, 2018.\n[7] J. Ho and S. Ermon, “Generative adversarial imitation learning,” in\nNIPS, 2016, pp. 4565–4573.\n[8] T. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul, B. Piot,\nA. Sendonaris, G. Dulac-Arnold, I. Osband, J. Agapiou et al., “Learn-\ning from demonstrations for real world reinforcement learning,” arXiv\npreprint arXiv:1704.03732, 2017.\n[9] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp,\nP. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang et al., “End to\nend learning for self-driving cars,” arXiv preprint arXiv:1604.07316,\n2016.\n[10] F. Torabi, G. Warnell, and P. Stone, “Behavioral cloning from obser-\nvation,” in Proceedings of the 27th International Joint Conference on\nArtiﬁcial Intelligence (IJCAI), July 2018.\n[11] F. Torabi, G. Warnell, and P. . Stone, “Generative adversarial imitation\nfrom observation,” arXiv preprint arXiv:1807.06158, 2018.\n[12] Y. Liu, A. Gupta, P. Abbeel, and S. Levine, “Imitation from ob-\nservation: Learning to imitate behaviors from raw video via context\ntranslation,” CoRR, vol. abs/1707.03374, 2017.\n[13] F. Torabi, G. Warnell, and P. Stone, “Recent advances in imitation\nlearning from observation,” in International Joint Conference on\nArtiﬁcial Intelligence (IJCAI).\nAAAI Press, 2019.\n[14] A. Nair, D. Chen, P. Agrawal, P. Isola, P. Abbeel, J. Malik, and\nS. Levine, “Combining self-supervised learning and imitation for\nvision-based rope manipulation,” CoRR, vol. abs/1703.02018, 2017.\n[15] D. Pomerleau, “Efﬁcient Training of Artiﬁcial Neural Networks for\nAutonomous Navigation,” Neural Computation, 1991.\n[16] D. A. Bristow, M. Tharayil, and A. G. Alleyne, “A survey of iterative\nlearning control,” IEEE control systems magazine, vol. 26, no. 3, pp.\n96–114, 2006.\n[17] J. Hwangbo, C. Gehring, H. Sommer, R. Siegwart, and J. Buchli,\n“Rockefﬁcient black-box optimization for policy learning,” in 2014\nIEEE-RAS International Conference on Humanoid Robots.\nIEEE,\n2014, pp. 535–540.\n[18] R. Calandra, A. Seyfarth, J. Peters, and M. P. Deisenroth, “Bayesian\noptimization for learning gaits under uncertainty,” Annals of Mathe-\nmatics and Artiﬁcial Intelligence, vol. 76, no. 1-2, pp. 5–23, 2016.\n[19] M. Neumann-Brosig, A. Marco, D. Schwarzmann, and S. Trimpe,\n“Data-efﬁcient autotuning with bayesian optimization: An industrial\ncontrol study,” IEEE Transactions on Control Systems Technology,\n2019.\n[20] M. Leonetti, P. Kormushev, and S. Sagratella, “Combining local and\nglobal direct derivative-free optimization for reinforcement learning,”\nCybernetics and Information Technologies, vol. 12, no. 3, pp. 53–65,\n2012.\n[21] A. Marco Valle, “Gaussian process optimization for self-tuning con-\ntrol,” Master’s thesis, Universitat Polit`ecnica de Catalunya, 2015.\n[22] I. Hosu and T. Rebedea, “Playing atari games with deep reinforcement\nlearning and human checkpoint replay,” CoRR, vol. abs/1607.05077,\n2016.\n[23] K. Subramanian, C. L. Isbell, Jr., and A. L. Thomaz, “Exploration from\ndemonstration for interactive reinforcement learning,” in Proceedings\nof the 2016 International Conference on Autonomous Agents &#38;\nMultiagent Systems, ser. AAMAS ’16, 2016, pp. 447–456.\n[24] A. Nair, B. McGrew, M. Andrychowicz, W. Zaremba, and P. Abbeel,\n“Overcoming exploration in reinforcement learning with demonstra-\ntions,” CoRR, vol. abs/1709.10089, 2017.\n[25] N. Hansen, S. D. M¨uller, and P. Koumoutsakos, “Reducing the time\ncomplexity of the derandomized evolution strategy with covariance\nmatrix adaptation (cma-es),” Evol. Comput., vol. 11, no. 1, pp. 1–18,\nMar. 2003.\n[26] M. Pelikan, D. E. Goldberg, and E. Cant´u-Paz, “Boa: The bayesian\noptimization algorithm,” in Proceedings of the 1st Annual Conference\non Genetic and Evolutionary Computation-Volume 1.\nMorgan Kauf-\nmann Publishers Inc., 1999, pp. 525–532.\n[27] J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel, “Trust\nregion policy optimization,” CoRR, vol. abs/1502.05477, 2015.\n[28] E. Todorov, T. Erez, and Y. Tassa, “Mujoco: A physics engine for\nmodel-based control,” in 2012 IEEE/RSJ International Conference\non Intelligent Robots and Systems, IROS 2012, Vilamoura, Algarve,\nPortugal, October 7-12, 2012.\nIEEE, 2012, pp. 5026–5033.\n[29] F. Torabi, G. Warnell, and P. Stone, “Imitation learning from video\nby leveraging proprioception,” in International Joint Conference on\nArtiﬁcial Intelligence (IJCAI), 2019.\n[30] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,\n“Proximal\npolicy\noptimization\nalgorithms,”\narXiv\npreprint\narXiv:1707.06347, 2017.\n[31] J. B¨odecker and M. Asada, “Simspark\nconcepts and application in\nthe robocup 3 d soccer simulation league,” 2008.\n[32] Y. Xu and H. Vatankhah, “Simspark: An open source robot simulator\ndeveloped by the robocup community,” in RoboCup 2013: Robot\nWorld Cup XVII, S. Behnke, M. Veloso, A. Visser, and R. Xiong, Eds.\nBerlin, Heidelberg: Springer Berlin Heidelberg, 2014, pp. 632–639.\n[33] P. MacAlpine, F. Torabi, B. Pavse, J. Sigmon, and P. Stone, “UT Austin\nVilla: RoboCup 2018 3D simulation league champions,” in RoboCup\n2018: Robot Soccer World Cup XXII, ser. Lecture Notes in Artiﬁcial\nIntelligence, D. Holz, K. Genter, M. Saad, and O. von Stryk, Eds.\nSpringer, 2019.\n[34] L. P. Reis, N. Lau, A. Abdolmaleki, N. Shaﬁi, R. Ferreira, A. Pereira,\nand D. Sim˜oes, “Fc portugal 3d simulation team: Team description\npaper 2017,” in RoboCup Symposium, 2017.\n[35] T. Iwanaga, K. Onda, and T. Yamanishi, “Fut-k team description paper\n2017.”\n[36] Y. Tassa, T. Erez, and E. Todorov, “Synthesis and stabilization of\ncomplex behaviors through online trajectory optimization,” in 2012\nIEEE/RSJ International Conference on Intelligent Robots and Systems.\nIEEE, 2012, pp. 4906–4913.\n[37] F. Torabi, S. Geiger, G. Warnell, and P. Stone, “Sample-efﬁcient\nadversarial imitation learning from observation,” arXiv preprint\narXiv:1906.07374, 2019.\n[38] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and\nD. Meger, “Deep reinforcement learning that matters,” CoRR, vol.\nabs/1709.06560, 2017.\n[39] J. Boedecker and M. Asada, “Simspark–concepts and application in\nthe robocup 3d soccer simulation league,” in SIMPAR-2008 Workshop\non the Universe of RoboCup Simulators, 2008, pp. 174–181.\n[40] Y. Xu and H. Vatankhah, “Simspark: An open source robot simulator\ndeveloped by the robocup community,” in RoboCup 2013: Robot\nWorld Cup XVII.\nSpringer, 2014, pp. 632–639.\nVIII. SUPPLEMENTARY MATERIALS\nHere we include details of our inverse dynamics model and experiment details.\nA. Proportional–Integral–Derivative (PID) Controller\nThe PID controller is a popular control loop feedback mechanism used in control systems. Given that we are trying to\nadjust some variable, the PID controller will help in accurately applying the necessary correction to reach a desired setpoint.\nFor example, if we want a robot to move its arm from 10◦to 30◦(desired setpoint), the PID controller will appropriately\ncalculate the necessary torque/force to accomplish this transition. Moreover, the PID controller is also responsive; in other\nwords, if the force applied to move from 10◦to 30◦is less or more than required, it will accordingly respond and adapt.\nMathematically, the PID controller is modeled as follows:\nu(t) = Kpe(t) + Ki\nZ t\n0\ne(t′)dt′ + Kd\nde(t)\ndt\n(3)\nwhere e(t) is the error between the desired setpoint and current point value, Kp, Ki, and Kd are the proportionality\nconstants for the proportional, integral, and derivative terms respectively. Intuitively, each term means the following: the\nproportional term signiﬁes that if the desired setpoint is far from our current point, we should apply a larger correction to\nreach there, the integral term keeps track of the cumulative error of the point from the desired setpoint at each time step,\nthis helps in applying a large correction if we have been far from the desired set point for a long time, and ﬁnally, the\nderivative term represents a damping factor that controls the excessive correction that may result from the proportional and\nintegral components.\nSince the PID controller accounts for the error to get from one state, st, to a desired setpoint, st+1, we view the PID\ncontroller as an inverse dynamics model, a mapping from state-transitions to actions i.e. {(st, st+1) →at}, which tells us\nwhich action at the agent took to go from state st to state st+1. We consider input and output of Equation 3 to be the raw\nstates and low-level actions respectively.\nB. Experiment Details\n1) MuJoCo Experiments: We train the experts for each of these domains using trust region policy optimization (TRPO)\n[27] and proximal policy optimization (PPO) [30], and select those with the best performance. We use the hyperparameters\nspeciﬁed in [27] and [38]. In our case, TRPO worked best for Reacher, HalfCheetah, Swimmer, and Hopper and PPO worked\nbest for Ant and Walker2d. Details of the considered domains are as following:\n• Reacher. The goal is to move a 2D robot arm to a ﬁxed location. We use a 2 dimensional state and action space. The\noriginal state space is 11 dimensions. Since we simplify the state space to only joint angles, we ﬁx the target location.\nThe reward per time-step is given by the distance of the arm from the target per time-step and regularization factor of\nthe actions.\n• HalfCheetah. The goal is to make a cheetah walk as fast as possible. We use a 6 dimensional state and action space.\nThe original state space is 17 dimensions. The reward per time-step is given by the cheetah’s forward velocity and\nregularization of its actions.\n• Swimmer. The goal is to make a snake-like creature swim as fast as possible in a viscous liquid. We use a 2 dimensional\nstate and action space. The original state space is 8 dimensions. The reward per time-step is given by the swimmer’s\nforward velocity and regularization of its actions.\n• Hopper. The goal is to make a 2D one-legged robot hop as fast as possible. We use a 3 dimensional state and action\nspace. The original state space is 11 dimensions. The reward per time-step is given by the change in the global position\nof the hopper, its jump height, its forward velocity, regularization of its actions, and its survival.\n• Ant. The goal is to make a 4-legged ant walk as fast as possible. We use an 8 dimensional state and action space. The\noriginal state space is 111 dimensions. The reward per time-step is given by the change in the global position of the\nant, its forward velocity, regularization of its actions, its contact with the surface, and its survival.\n• Walker2d. The goal is to make a 2D bipedal robot walk as fast as possible. We use a 6 dimensional state and action\nspace. The original state space is 17 dimensions. The reward per time-step is given by the change in the global position\nof the walker, its walk height, its forward velocity, regularization of its actions, and its survival.\n2) 3D Simulation: The RoboCup 3D simulation domain is supported by two components - SimSpark [39], [40] and Open\nDynamics Engine (ODE). SimSpark provides support for simulated physical multiagent system research. The ODE library\nenables realistic simulation of rigid body dynamics.\nIn our experiments, we are interested in imitating two tasks: (1) speed walking and (2) long distance kick-offs. Since\nSimSpark does not have built-in reward functions, we design our own reward function. Refer to Appendix VIII for details\nabout the tasks and the designed reward functions.\nSince SimSpark does not have built-in reward functions, we design our own reward function. We note that the demonstrators\nmay have not used our reward function.\n(a) Reacher\n(b) HalfCheetah\n(c) Swimmer\n(d) Hopper\n(e) Ant\n(f) Walker2d\nFig. 7.\nRepresentative screenshots of the MuJoCo domains considered in this paper.\nFig. 8.\nSimulated Nao robot in SimSpark\n• Speed walking. The goal of this task is to have the agent walk as fast as possible while maintaining stability throughout\nthe episode. To do so, we deﬁne the total reward at the end of the episode to be the cumulative distance travelled per\ntime-step with a −5 penalty for falling down. The distance is measured in meters.\n• Long-distance kick-off. The goal of the task is to kick the ball as far as possibletowards the center of the goal. To do\nso, we deﬁne the reward function to be\nRkick = (1 + xtotal) · exp\n\u0010−θ2\n180\n\u0011\n+ xair · 100\nwith a −5 penalty for slightly bumping the ball, −10 penalty for falling down, where xtotal is the distance travelled\nby the ball along the x-axis, θ is the angle of deviation of the ball’s trajectory from the straight line between the agent\nand center of the goal, and xair is the distance along the x-axis for which the ball was travelling in the air. xtotal and\nxair are in meters, and θ is in degrees. The reward function values kicks that travel in the air for a long distance and\nexponentially decays the reward for off-target kicks.\n3) UR5 Robot Arm: The original PID controller gains are hard-coded in the UR5 drivers when position-control mode is\nactivated. Moreover, robots already include such a controller which is why the proposed method is so attractive, i.e., it can\nleverage common, pre-existing, and well-understood robotics control mechanisms.\nFig. 9.\nUR5 Robot Arm\n",
  "categories": [
    "cs.LG",
    "cs.RO",
    "stat.ML"
  ],
  "published": "2019-06-18",
  "updated": "2020-07-21"
}