{
  "id": "http://arxiv.org/abs/1906.08312v1",
  "title": "Calibrated Model-Based Deep Reinforcement Learning",
  "authors": [
    "Ali Malik",
    "Volodymyr Kuleshov",
    "Jiaming Song",
    "Danny Nemer",
    "Harlan Seymour",
    "Stefano Ermon"
  ],
  "abstract": "Estimates of predictive uncertainty are important for accurate model-based\nplanning and reinforcement learning. However, predictive\nuncertainties---especially ones derived from modern deep learning systems---can\nbe inaccurate and impose a bottleneck on performance. This paper explores which\nuncertainties are needed for model-based reinforcement learning and argues that\ngood uncertainties must be calibrated, i.e. their probabilities should match\nempirical frequencies of predicted events. We describe a simple way to augment\nany model-based reinforcement learning agent with a calibrated model and show\nthat doing so consistently improves planning, sample complexity, and\nexploration. On the \\textsc{HalfCheetah} MuJoCo task, our system achieves\nstate-of-the-art performance using 50\\% fewer samples than the current leading\napproach. Our findings suggest that calibration can improve the performance of\nmodel-based reinforcement learning with minimal computational and\nimplementation overhead.",
  "text": "Calibrated Model-Based Deep Reinforcement Learning\nAli Malik * 1 Volodymyr Kuleshov * 1 2 Jiaming Song 1 Danny Nemer 2 Harlan Seymour 2 Stefano Ermon 1\nAbstract\nEstimates of predictive uncertainty are important\nfor accurate model-based planning and reinforce-\nment learning. However, predictive uncertainties\n— especially ones derived from modern deep learn-\ning systems — can be inaccurate and impose a\nbottleneck on performance. This paper explores\nwhich uncertainties are needed for model-based\nreinforcement learning and argues that good un-\ncertainties must be calibrated, i.e. their prob-\nabilities should match empirical frequencies of\npredicted events. We describe a simple way to\naugment any model-based reinforcement learning\nagent with a calibrated model and show that doing\nso consistently improves planning, sample com-\nplexity, and exploration. On the HALFCHEETAH\nMuJoCo task, our system achieves state-of-the-art\nperformance using 50% fewer samples than the\ncurrent leading approach. Our ﬁndings suggest\nthat calibration can improve the performance of\nmodel-based reinforcement learning with mini-\nmal computational and implementation overhead.\n1. Introduction\nMethods for accurately assessing predictive uncertainty\nare important components of modern decision-making sys-\ntems. Probabilistic methods have been used to improve the\nsafety, interpretability, and performance of decision-making\nagents in various domains, including medicine (Saria, 2018),\nrobotics (Chua et al., 2018; Buckman et al., 2018), and op-\nerations research (Van Roy et al., 1997).\nIn model-based reinforcement learning — a setting in which\nan agent learns a model of the world from past experience\nand uses it to plan future decisions — capturing uncer-\ntainty in the agent’s model is particularly important (Deisen-\n*Equal contribution 1Department of Computer Science, Stan-\nford University, USA\n2Afresh Technologies, San Francisco,\nUSA. Correspondence to: Ali Malik <malikali@stanford.edu>,\nVolodymyr Kuleshov <kuleshov@cs.stanford.edu>.\nProceedings of the 36 th International Conference on Machine\nLearning, Long Beach, California, PMLR 97, 2019. Copyright\n2019 by the author(s).\nFigure 1. Modern model-based planning algorithms with proba-\nbilistic models can over-estimate their conﬁdence (purple distri-\nbution), and overlook dangerous outcomes (e.g., a collision). We\nshow how to endow agents with a calibrated world model that\naccurately captures true uncertainty (green distribution) and im-\nproves planning in high-stakes scenarios like autonomous driving\nor industrial optimisation.\nroth & Rasmussen, 2011). Planning with a probabilistic\nmodel improves performance and sample complexity, es-\npecially when representing the model using a deep neural\nnetwork (Rajeswaran et al., 2016; Chua et al., 2018).\nDespite their importance in decision-making, predictive\nuncertainties can be unreliable, especially when derived\nfrom deep neural networks (Guo et al., 2017a). Although\nseveral modern approaches such as deep ensembles (Laksh-\nminarayanan et al., 2017b) and approximations of Bayesian\ninference (Gal & Ghahramani, 2016a;b; Gal et al., 2017) pro-\nvide uncertainties from deep neural networks, these methods\nsuffer from shortcomings that reduce their effectiveness for\nplanning (Kuleshov et al., 2018).\nIn this paper, we study which uncertainties are needed in\nmodel-based reinforcement learning and argue that good\npredictive uncertainties must be calibrated, i.e. their prob-\nabilities should match empirical frequencies of predicted\nevents. We propose a simple way to augment any model-\nbased reinforcement learning algorithm with a calibrated\nmodel by adapting recent advances in uncertainty estimation\nfor deep neural networks (Kuleshov et al., 2018). We com-\nplement our approach with diagnostic tools, best-practices,\narXiv:1906.08312v1  [cs.LG]  19 Jun 2019\nCalibrated Model-Based Deep Reinforcement Learning\nand intuition on how to apply calibration in reinforcement\nlearning.\nWe validate our approach on benchmarks for contextual\nbandits and continuous control (Li et al., 2010; Todorov\net al., 2012), as well as on a planning problem in inventory\nmanagement (Van Roy et al., 1997). Our results show that\ncalibration consistently improves the cumulative reward and\nthe sample complexity of model-based agents, and also en-\nhances their ability to balance exploration and exploitation\nin contextual bandit settings. Most interestingly, on the\nHALFCHEETAH task, our system achieves state-of-the-art\nperformance, using 50% fewer samples than the previous\nleading approach (Chua et al., 2018). Our results suggest\nthat calibrated uncertainties have the potential to improve\nmodel-based reinforcement learning algorithms with mini-\nmal computational and implementation overhead.\nContributions.\nIn summary, this paper adapts recent ad-\nvances in uncertainty estimation for deep neural networks\nto reinforcement learning and proposes a simple way to im-\nprove any model-based algorithm with calibrated uncertain-\nties. We explain how this technique improves the accuracy\nof planning and the ability of agents to balance exploration\nand exploitation. Our method consistently improves perfor-\nmance on several reinforcement learning tasks, including\ncontextual bandits, inventory management and continuous\ncontrol1.\n2. Background\n2.1. Model-Based Reinforcement Learning\nLet S and A denote (possibly continuous) state and action\nspaces in a Markov Decision Process (S, A, T, r) and let\nΠ denote the set of all stationary stochastic policies π :\nS →P(A) that choose actions in A given states in S. The\nsuccessor state s′ for a given action a from current state s\nare drawn from the dynamics function T(s′|s, a). We work\nin the γ-discounted inﬁnite horizon setting and we will use\nan expectation with respect to a policy π ∈Π to denote\nan expectation with respect to the trajectory it generates:\nEπ[r(s, a)] ≜E [P∞\nt=0 γtr(st, at)], where s0 ∼p0, at ∼\nπ(·|st), and st+1 ∼T(·|st, at) for t ≥0. p0 is the initial\ndistribution over states and r(st, at) is the reward at time t.\nTypically, S, A, γ are known, while the dynamics model\nT(s′|s, a) and the reward function r(s, a) are not known\nexplicitly. This work focuses on model-based reinforcement\nlearning, in which the agent learns an approximate model\nˆT(s′|s, a) of the world from samples obtained by interacting\nwith the environment and uses this model to plan its future\ndecisions.\n1Our\ncode\nis\navailable\nat\nhttps://github.com/\nermongroup/CalibratedModelBasedRL\nProbabilistic Models\nThis paper focuses on probabilistic\ndynamics models bT(s′|s, a) that take a current state s ∈S\nand action a ∈A, and output a probability distribution over\nfuture states s′. We represent the output distribution over the\nnext states, bT(·|s, a), as a cumulative distribution function\nFs,a : S →[0, 1], which is deﬁned for both discrete and\ncontinuous S.\n2.2. Calibration, Sharpness, and Proper Scoring Rules\nA key desirable property of probabilistic forecasts is calibra-\ntion. Intuitively, a transition model bT(s′|s, a) is calibrated if\nwhenever it assigns a probability of 0.8 to an event — such\nas a state transition (s, a, s′) — that transition should occur\nabout 80% of the time.\nFormally, for a discrete state space S and when s, a, s′ are\ni.i.d. realizations of random variables S, A, S′ ∼P, we say\nthat a transition model bT is calibrated if\nP(S′ = s′ | bT(S′ = s′|S, A) = p) = p\nfor all s′ ∈S and p ∈[0, 1].\nWhen S is a continuous state space, calibration is deﬁned\nusing quantiles as P(S′ ≤F −1\nS,A(p)) = p for all p ∈[0, 1],\nwhere F −1\ns,a (p) = inf{y : p ≤Fs,a(y)} is the quantile\nfunction associated with the CDF Fs,a over future states\ns′ (Gneiting et al., 2007). A multivariate extension can be\nfound in Kuleshov et al. (2018).\nNote that calibration alone is not enough for a model to be\ngood. For example, assigning the same average probability\nto each transition may sufﬁce as a calibrated model, but\nthis model will not be useful. Good models need to also be\nsharp: intuitively, their probabilities should be maximally\ncertain, i.e. close to 0 or 1.\nProper Scoring Rules.\nIn the statistics literature, proba-\nbilistic forecasts are typically assessed using proper scoring\nrules (Murphy, 1973; Dawid, 1984). An example is the Brier\nscore L(p, q) = (p −q)2 deﬁned over two Bernoulli distri-\nbutions with natural parameters p, q ∈[0, 1]. Crucially, any\nproper scoring rule decomposes precisely into a calibration\nand a sharpness term (Murphy, 1973):\nProper Scoring Rule = Calibration + Sharpness + const.\nMost loss functions for probabilistic forecasts over both\ndiscrete and continuous variables are proper scoring rules\n(Gneiting & Raftery, 2007). Hence, calibration and sharp-\nness are precisely the two sufﬁcient properties of a good\nforecast.\n2.3. Recalibration\nMost predictive models are not calibrated out-of-the-box\n(Niculescu-Mizil & Caruana, 2005). However, given an\nCalibrated Model-Based Deep Reinforcement Learning\narbitrary pre-trained forecaster H : X →(Y →[0, 1])\nthat outputs CDFs F, we may train an auxiliary model R :\n[0, 1] →[0, 1] such that the forecasts R ◦F are calibrated\nin the limit of enough data. This recalibration procedure\napplies to any probabilistic regression model and does not\nworsen the original forecasts from H when measured using\na proper scoring rule (Kuleshov & Ermon, 2017).\nWhen S is discrete, a popular choice of R is Platt scal-\ning (Platt et al., 1999); Kuleshov et al. (2018) extends Platt\nscaling to continuous variables. Either of these methods can\nbe used within our framework.\n3. What Uncertainties Do We Need In\nModel-Based Reinforcement Learning?\nIn model-based reinforcement learning, probabilistic models\nimprove the performance and sample complexity of plan-\nning algorithms (Rajeswaran et al., 2016; Chua et al., 2018);\nthis naturally raises the question of what constitutes a good\nprobabilistic model.\n3.1. Calibration vs. Sharpness Trade-Off\nA natural way of assessing the quality of a probabilistic\nmodel is via a proper scoring rule (Murphy, 1973; Gneiting\net al., 2007). As discussed in Section 2, any proper scoring\nrule decomposes into a calibration and a sharpness term.\nHence, these are precisely the two qualities we should seek.\nCrucially, not all probabilistic predictions with the same\nproper score are equal: some are better calibrated, and others\nare sharper. There is a natural trade-off between these terms.\nIn this paper, we argue that this trade-off plays an important\nrole when specifying probabilistic models in reinforcement\nlearning. Speciﬁcally, it is much better to be calibrated\nthan sharp, and calibration signiﬁcantly impacts the perfor-\nmance of model-based algorithms. Recalibration methods\n(Platt et al., 1999; Kuleshov et al., 2018) allow us to ensure\nthat a model is calibrated, and thus improve reinforcement\nlearning agents.\n3.2. Importance of Calibration for Decision-Making\nIn order to explain the importance of calibration, we provide\nsome intuitive examples, and then prove a formal statement.\nIntuition.\nConsider a simple MDP with two states sgood\nand sbad. The former has a high reward r(sgood) = 1 and\nthe latter has a low reward r(sbad) = −1.\nFirst, calibration helps us better estimate expected rewards.\nConsider the expected reward ˆr from taking action a in sgood\nunder the model. It is given by ˆr = −1 · ˆT(sbad|sgood, a) +\n1 · ˆT(sgood|sgood, a). If the true transition probability is\nT(sgood|sgood, a) = 80%, but our model ˆT predicts 60%,\nthen in the long run the average reward from a in sgood will\nnot equal to ˆr; incorrectly estimating the reward will in turn\ncause us to choose sub-optimal actions.\nSimilarly, suppose that the model is over-conﬁdent and\nˆT(sgood|sgood, a) = 0; intuitively, we may decide that it\nis not useful to try a in sgood, as it leads to sbad with 100%\nprobability. This is an instance of the classical exploration-\nexploitation problem; many approaches to this problem\n(such as the UCB family of algorithms) rely on accurate\nconﬁdence bounds and are likely to beneﬁt from calibrated\nuncertaintites that more accurately reﬂect the true probabil-\nity of transitioning to a particular state.\nExpectations Under Calibrated Models.\nMore con-\ncretely, we can formalise our intuition about the accuracy\nof expectations via the following statement for discrete vari-\nables; see the Appendix for more details.\nLemma 1. Let Q(Y |X) be a calibrated model over two\ndiscrete variables X, Y ∼P such that P(Y = y | Q(Y =\ny | X) = p) = p. Then any expectation of a function G(Y )\nis the same under P and Q:\nE\ny∼P(Y )\n[G(y)]\n=\nE\nx∼P(X)\ny∼Q(Y |X=x)\n[G(y)] .\n(1)\nIn model-based reinforcement learning, we take expecta-\ntions in order to compute the expected reward of a sequence\nof decisions. A calibrated model will allow us to estimate\nthese more accurately.\n4. Calibrated Model-Based Reinforcement\nLearning\nIn Algorithm 1, we present a simple procedure that aug-\nments a model-based reinforcement learning algorithm with\nan extra step that ensures the calibration of its transition\nmodel. Algorithm 1 effectively corresponds to standard\nmodel-based reinforcement learning with the addition of\nStep 4, in which we train a recalibrator R such that R ◦T is\ncalibrated. The subroutine CALIBRATE can be an instance\nof Platt scaling, for discrete S, or the method of Kuleshov\net al. (2018), when S is continuous (see Algorithm 2 in the\nappendix).\nIn the rest of this section, we describe best practices for\napplying this method.\nDiagnostic Tools.\nAn essential tool for visualising cali-\nbration of predicted CDFs F1, . . . FN is the reliability curve\n(Gneiting et al., 2007). This plot displays the empirical\nfrequency of points in a given interval relative to the pre-\ndicted fraction of points in that interval. Formally, we\nCalibrated Model-Based Deep Reinforcement Learning\nAlgorithm 1 Calibrated Model-Based Reinforcement Learning\nInput: Initial transition model bT : S × A →P(S) and\ninitial dataset of state transitions D = {(st, at), st+1}N\nt=1\nRepeat until sufﬁcient level of performance is reached:\n1. Run the agent and collect a dataset of state transitions\nDnew ←EXECUTEPLANNING( bT). Gather all experi-\nence data D ←D ∪Dnew.\n2. Let Dtrain, Dcal ←PARTITIONDATA(D) be the train-\ning and calibration sets, respectively.\n3. Train a transition model bT ←TRAINMODEL(Dtrain).\n4. Train the recalibrator R ←CALIBRATE( bT, Dcal).\n5. Let bT ←R ◦bT be the new, recalibrated transition\nmodel.\nchoose m thresholds 0 ≤p1 ≤· · · ≤pm ≤1 and,\nfor each threshold pj, compute the empirical frequency\nˆpj = |yt : Ft(yt) ≤pj, t = 1, . . . , N|/N\nPlotting {(pj, ˆpj)} gives us a sense of the calibration of the\nmodel (see Figure 2), with a straight line corresponding to\nperfect calibration. An equivalent, alternative visualisation\nis to plot a histogram of of the probability integral transform\n{Ft(yt)}N\nt=1 and see if it looks like a uniform distribution\n(Gneiting et al., 2007).\nThese visualisations can be quantiﬁed by deﬁning the cali-\nbration loss2 of a model:\ncal(F1, y1, . . . Ft, yY ) =\nm\nX\nj=1\n(ˆpj −pj)2,\n(2)\nas the sum of the squares of the residuals (ˆpj −pj)2. These\ndiagnostic tools should be evaluated on unseen data distinct\nfrom the training and calibration sets as it may reveal signs\nof overﬁtting.\n4.1. Applications to Deep Reinforcement Learning\nAlthough deep neural networks can signiﬁcantly improve\nmodel-based planning algorithms (Higuera et al., 2018;\nChua et al., 2018), their estimates of predictive uncertainty\nare often inaccurate (Guo et al., 2017a; Kuleshov et al.,\n2018).\nVariational Dropout.\nOne popular approach to deriving\nuncertainty estimates from deep neural networks involves\nusing dropout. Taking the mean and the variance of dropout\nsamples leads to a principled Gaussian approximation of\nthe posterior predictive distribution from a Bayesian neural\nnetwork (in regression) (Gal & Ghahramani, 2016a). To\n2This is the calibration term in the two-component decomposi-\ntion of the Brier score.\nuse Algorithm 1 we may instantiate CALIBRATE with the\nmethod of Kuleshov et al. (2018) and pass it the predictive\nGaussian derived from the dropout samples.\nMore generally, our method can be naturally applied on top\nof any probabilistic model without any need to modify or\nretrain this model.\n5. The Beneﬁts of Calibration in Model-Based\nReinforcement Learning\nNext, we examine speciﬁc ways in which Algorithm 1 can\nimprove model-based reinforcement learning agents.\n5.1. Model-Based Planning\nThe ﬁrst beneﬁt of a calibrated model is enabling more\naccurate planning using standard algorithms such as value\niteration or model predictive control (Sutton & Barto, 2018).\nEach of these methods involves estimates of future reward.\nFor example, value iteration performs the update\nV ′(s) ←Ea∼π(·|s)\n\"X\ns′∈S\nˆT(s′|s, a)(r(s′) + V (s′))\n#\n.\nCrucially, this algorithm requires accurate estimates of the\nexpected reward P\ns′∈S ˆT(s′|s, a)r(s′). Similarly, online\nplanning algorithms involve computing the expected reward\nof a ﬁnite sequence of actions, which has a similar form. If\nthe model is miscalibrated, then the predicted distribution\nˆT(s′|s, a) will not accurately reﬂect the true distribution of\nstates that the agent will encounter in the real world. As a\nresult, planning performed in the model will be inaccurate.\nMore formally, let us deﬁne the value of a policy π as\nV (π) = Es∼σπ[V (s)], where σπ is the stationary distribu-\ntion of the Markov chain induced by π. Let V ′(π) be an\nestimate of the value of π in a second MDP in which we\nreplaced the transition dynamics by a calibrated model bT\nlearned from data. Then, the following holds.\nTheorem 1. Let (S, A, T, r) be a discrete MDP and let π\nbe a stochastic policy over this MDP. The value V (π) of\npolicy π under the true dynamics T is equal to the value\nV ′(π) of the policy under any set of calibrated dynamics bT.\nEffectively, having a calibrated model makes it possible to\ncompute accurate expectations of rewards, which in turn\nprovides accurate estimates of the values of states and poli-\ncies. Accurately estimating the value of a policy makes it\neasier to choose the best one by planning.\n5.2. Balancing Exploration and Exploitation\nBalancing exploration and exploitation successfully is a\nfundamental challenge for many reinforcement learning\n(RL) algorithms. A large family of algorithms tackle this\nCalibrated Model-Based Deep Reinforcement Learning\nLinUCB\nCalLinUCB\nOptimal\nLinear\n1209.8 ± 12.1\n1210.3 ± 12.1\n1231.8\nBeta\n1176.3 ± 11.9\n1174.6 ± 12.0\n1202.3\nMushroom\n1429.4 ± 154.0\n1676.1 ± 164.1\n3122.0\nCovertype\n558.14 ± 3.5\n677.8 ± 5.0\n1200.0\nAdult\n131.3 ± 1.2\n198.9 ± 4.7\n1200.0\nCensus\n207.6 ± 1.7\n603.7 ± 3.8\n1200.0\nTable 1. Performance of calibrated/uncalibrated LinUCB on a vari-\nety of datasets, averaged over 10 trials. The calibrated algorithm\n(CalLinUCB) does better on all non-synthetic datasets (bottom\nfour rows) and has similar performance on the synthetic datasets\n(top two rows).\nproblem using notions of uncertainty/conﬁdence to guide\ntheir exploration process. For example, upper conﬁdence\nbound (UCB, Auer et al. (2002)) algorithms pick the action\nwhich has the highest upper bound on its reward conﬁdence\ninterval.\nIn situations where the outputs of the algorithms are uncal-\nibrated, the conﬁdence intervals might provide unreliable\nupper conﬁdence bounds, resulting in suboptimal perfor-\nmance. For example, in a two-arm bandit problem, if a\nmodel is under-estimating the reward of the best arm and\nhas high conﬁdence, it’s upper conﬁdence bound will be\nlow, and it will not be selected. More generally, UCB-style\nmethods need uncertainty estimates to be on the same “order\nof magnitude” so that arms can be compared against each\nother; calibration helps ensure that.\n6. Experiments\nWe evaluate our calibrated model-based reinforcement learn-\ning method on several different environments and algo-\nrithms, including contextual bandits, inventory management,\nand continuous control for robotics.\n6.1. Balancing Exploration and Exploitation\nTo test the effect of calibration on exploration/exploitation,\nwe look at the contextual multi-armed bandit problem (Li\net al., 2010). At each timestep, an agent is shown a context\nvector x and must pick an arm a ∈A from a ﬁnite set\nA. After picking an arm, the agent receives a reward ra,x\nwhich depends both on the arm picked and also on the\ncontext vector shown to the agent. The agent’s goal over\ntime is to learn the relationship between the context vector\nand reward gained from each arm so that it can pick the arm\nwith the highest expected reward at each timestep.\nSetup.\nFor our experiments, we focus on the LinUCB\nalgorithm (Li et al., 2010) — a well-known instantiation of\nthe UCB approach to contextual bandits. LinUCB assumes\na linear relationship between the context vector and the\nexpected reward of an arm: for each arm a ∈A, there is an\nunknown coefﬁcient vector θ∗\na such that E[ra,x] = x⊤θ∗\na.\nLinUCB learns a predictive distribution over this reward\nusing Bayesian ridge regression, in which θ∗\na has a Gaussian\nposterior N(ˆθa, ˆΣa). The posterior predictive distribution is\nalso Gaussian, with mean x⊤ˆθa and with standard deviation\np\nx⊤ˆΣ−1\na x. Thus, the algorithm picks the arm with the\nhighest α-quantile, given by\narg max\na∈A\n\u0012\nx⊤ˆθa + α ·\nq\nx⊤ˆΣ−1\na x\n\u0013\n.\n(3)\nWe apply the recalibration scheme in Algorithm 1 of\nKuleshov et al. (2018) to these predicted Gaussian distribu-\ntions.\nData.\nWe evaluate the calibrated version (CalLinUCB)\nand uncalibrated version (LinUCB) of the LinUCB algo-\nrithm on both synthetic data that satisﬁes the linearity as-\nsumption of the algorithm, as well as on real UCI datasets\nfrom Li et al. (2010). We run the tests on 2000 examples\nover 10 trials and compute the average cumulative reward.\nFigure 2. Top: Performnce of CalibLinUCB and LinUCB on the\nUCI covertype dataset. Bottom: Calibration curves of the LinUCB\nalgorithms on the covertype dataset\nResults.\nWe expect the LinUCB algorithm to already be\ncalibrated on the synthetic linear data since the model is\nwell-speciﬁed, implying no difference in performance be-\ntween CalLinUCB and LinUCB. On the real UCI datasets\nCalibrated Model-Based Deep Reinforcement Learning\nhowever, the linear assumption might not hold, resulting in\nmiscalibrated estimates of the expected reward.\nIn Table 1, we can see that indeed there is no signiﬁ-\ncant difference in performance between the CalLinUCB\nand LinUCB algorithms on the synthetic linear dataset—\nthey both preform optimally. On the UCI datasets how-\never, we see a noticeable improvement with CalLinUCB\non almost all tasks, suggesting that recalibration aids ex-\nploration/exploitation in a setting where the model is mis-\nspeciﬁed. Note that both CalLinUCB and LinUCB perform\nbelow the optimum on these datasets, implying linear mod-\nels are not expressive enough in general for these tasks.\nAnalysis.\nTo get a sense of the effect of calibration on\nthe model’s conﬁdence estimates, we can plot the predicted\nreward with 90% conﬁdence intervals that the algorithm\nexpected for a chosen arm a at timestep t. We can then com-\npare how good this prediction was with respect to the true\nobserved reward. Speciﬁcally, we can look at the timesteps\nwhere the CalLinUCB algorithm picked the optimal action\nbut the LinUCB algorithm did not, and look at both the\nalgorithms’ belief about the predicted reward from both of\nthese actions. An example of this plot can be seen in the\nappendix on Figure 4.\nA key takeaway from this plot is that the uncalibrated al-\ngorithm systematically underestimates the expected reward\nfrom the optimal action and overestimates the expected re-\nward of the action it chose instead, resulting in suboptimal\nactions. The calibrated model does not suffer from this\ndefect, and thus performs better on the task.\n6.2. Model-Based Planning\n6.2.1. INVENTORY MANAGEMENT\nOur ﬁrst model-based planning task is inventory manage-\nment (Van Roy et al., 1997). A decision-making agent\ncontrols the inventory of a perishable good in a store. Each\nday, the agent orders items into the store; if the agent under-\norders, the store runs out of stock; if the agent over-orders,\nperishable items are lost due to spoilage. Perishable inven-\ntory management systems have the potential to positively\nimpact the environment by minimizing food waste and en-\nabling a more effective use of resources (Vermeulen et al.,\n2012).\nModel.\nWe formalize perishable inventory management\nfor one item using a Markov decision process (S, A, P, r).\nStates s ∈S are tuples (ds, qs) consisting of a calendar\nday ds and an inventory state qs ∈ZL, where L ≥1 is\nthe item shelf-life. Each component (qs)l indicates the\nnumber of units in the store that expire in l days; the total\ninventory level is ts = PL\nl=1(qs)l. Transition probabilities\nP are deﬁned as follows: each day sees a random demand\nCalibrated\nUncalibrated\nHeuristic\nShipped\n332,150\n319,692\n338,011\nWasted\n7,466\n3,148\n13,699\nStockouts\n9,327\n17,358\n11,817\n% Waste\n2.2%\n1.0%\n4.1%\n% Stockouts\n2.8%\n5.4%\n3.5%\nReward\n-16,793\n-20,506\n-25,516\nTable 2. Performance of calibrated model planning on an inventory\nmanagement task. Calibration signiﬁcantly improves cumulative\nreward. Numbers are in units, averaged over ten trials.\nof D(s) ∈Z units and sales of max(t(s) −D(s), 0) units,\nsampled at random from all the units in the inventory; at\nthe end of the state transition, the shelf-life of the remaining\nitems is decreased by one (spoiled items are recorded and\nthrown away). Actions a ∈A ⊆Z correspond to orders:\nthe store receives items with a shelf life of L before entering\nthe next state s′. In our experiments we choose the reward r\nto be the sum of waste and unmet demand due to stock-outs.\nData.\nWe use the Corporacion Favorita Kaggle dataset,\nwhich consists of historical sales from a supermarket chain\nin Ecuador. We experiment on the 100 highest-selling items\nand use data from 2014-01-01 to 2016-05-31 for training\nand data from 2016-06-01 to 2016-08-31 for testing.\nAlgorithms.\nWe learn a probabilistic model ˆ\nM : S →\n(R →[0, 1]) of the demand D(s′) in a future state s′ based\non information available in the present state s. Speciﬁcally,\nwe train a Bayesian DenseNet (Huang et al., 2017) to predict\nsales on each of the next ﬁve days based on features from\nthe current day (sales serve as a proxy for demand). We use\nautoregressive features from the past four days, 7-, 14-, and\n28-day rolling means of historical sales, binary indicators\nfor the day of the week and the week of the year, and sine\nand cosine features over the number of days elapsed in the\nyear. The Bayesian DenseNet has ﬁve layers of 128 hidden\nunits with a dropout rate of 0.5 and parametric ReLU non-\nlinearities. We use variational dropout (Gal & Ghahramani,\n2016b) to compute probabilistic forecasts from the model.\nWe use our learned distribution over D(s′) to perform on-\nline planning on the test set using model predictive control\n(MPC) learned on the training set. Speciﬁcally, we sample\n5,000 random trajectories over a 5-step horizon, and choose\nthe ﬁrst action of the trajectory with the highest expected\nreward under the model. We estimate the expected reward\nof each trajectory using 300 Monte Carlo samples from the\nmodel.\nWe also compare the planning approach to a simple heuristic\nrule that always sets the inventory to 1.5 · E[D(s′)], which\nis the expected demand multiplied by a small safety factor.\nCalibrated Model-Based Deep Reinforcement Learning\nResults.\nWe evaluate the agent within the inventory man-\nagement MDP; the demand D(s) is instantiated with the\nhistorical sales on test day d(s) (which the agent did not\nobserve). We measure total cumulative waste and stockouts\nover the 100 items in the dataset, and we report them as a\nfraction of the total number of units shipped to the store.\nTable 2 shows that calibration improves the total cumulative\nreward by 14%. The calibrated model incurs waste and out-\nof-stocks ratios of 2.2% and 2.8%, respectively, compared\nto 1.0% and 5.4% for the uncalibrated one. These values\nare skewed towards a smaller waste, while the objective\nfunction penalizes both equally. The heuristic has ratios of\n4.1% and 3.5%.\n6.2.2. MUJOCO ENVIRONMENTS\nOur second model-based planning task is continuous control\nfrom OpenAI Gym (Brockman et al., 2016) and the Mu-\njoco robotics simulation environment (Todorov et al., 2012).\nHere the agent makes decisions about its torque controls\ngiven observation states (e.g. location / velocity of joints)\nthat maximizes the expected return reward. These environ-\nments are standard benchmark tasks for deep reinforcement\nlearning.\nSetup.\nWe consider calibrating the probablistic ensemble\ndynamics model proposed in (Chua et al., 2018). In this\napproach, the agent learns an ensemble of probabilistic neu-\nral networks (PE) that captures the environment dynamics\nst+1 ∼fθ(st, at), which is used for model-based planning\nwith model predictive control. The policy and ensemble\nmodel are then updated in an iterative fashion. Chua et al.\n(2018) introduce several strategies for particle-based state\npropagation, including trajectory sampling with bootstraped\nmodels (PE-TS); and distribution sampling (PE-DS), which\nsamples from a multimodal distribution as follows:\nst+1 ∼N(E[sp\nt+1], Var[sp\nt+1]),\nsp\nt+1 ∼fθ(st, at) (4)\nPE-TS and PE-DS achieve the highest sample efﬁciency\namong the methods proposed in (Chua et al., 2018).\nTo calibrate the model, we add a ﬁnal sigmoid recalibration\nlayer to the sampling procedure in PE-DS at each step. This\nlogistic layer is applied separately per output state dimen-\nsion and serves as the recalibrator R. It is trained on the\nprocedure described in Algorithm 2, after every trial, on a\nseparate calibration set using cross entropy loss.\nWe consider three continuous control environments from\nChua et al. (2018) 3.\nFor model learning and model-\nbased planning, we follow the training procedure and\nhyperparameters in Chua et al. (2018), as described in\n3We omitted the reacher environment because the reference\npapers did not have SAC results for it.\nhttps://github.com/kchua/handful-of-trials. We also com-\npare our method against Soft Actor-Critic (Haarnoja et al.,\n2018) which is one of the state-of-the-art model-free rein-\nforcement learning algorithms. We use the ﬁnal conver-\ngence reward of SAC as a criterion for the highest possible\nreward achieved in the task (although it may require orders\nof magnitude more samples from the environment).\nResults.\nOne of the most important criteria for evaluating\nreinforcement learning algorithms is sample complexity, i.e,\nthe amount of interactions with the environment in order\nto reach a certain high expected return. We compare the\nsample complexities of SAC, PE-DS and calibrated PE-DS\nin Figure 3. Compared to the model-free SAC method, both\nthe model-based methods use much fewer samples from\nthe environment to reach the convergence performance of\nSAC. However, our recalibrated PE-DS method compares\nfavorably to PE-DS on all three environments.\nNotably, the calibrated PE-DS method outperforms both PE-\nDS by a signiﬁcant margin on the HalfCheetah environment,\nreaching near optimal performance at only around 180k\ntimesteps. To our knowledge, the calibrated PE-DS is the\nmost efﬁcient method on these environments in terms of\nsample complexity.\nAnalysis.\nIn Figure 5 in the appendix, we visualise the\n1-step prediction accuracy for action dimension zero in the\nCartpole environment for both PE-DS and calibrated PE-\nDS. This ﬁgure shows that the calibrated PE-DS model is\nmore accurate, has tighter uncertainty bounds, and is better\ncalibrated, especially in earlier trials. Interestingly, we also\nobserve a superior expected return for calibrated PE-DS for\nearlier trials in Figure 3, suggesting that being calibrated\nis correlated with improvements in model-based prediction\nand planning.\n7. Discussion\nLimitations.\nA potential failure mode for our method\narises when not all forecasts are from the same family of\ndistributions. This can lead to calibrated, but diffuse con-\nﬁdence intervals. Another limitation of the method is its\nscalability to high-dimensional spaces. In our work, the\nuncalibrated forecasts were fully factored, and could be re-\ncalibrated component-wise. For non-factored distributions,\nrecalibration is computationally intractable and requires\napproximations such as ones developed for multi-class clas-\nsiﬁcation (Zadrozny & Elkan, 2002).\nFinally, it is possible that uncalibrated forecasts are still\neffective if they induce a model that correctly ranks the\nagent’s actions in terms of their expected reward (even when\nthe estimates of the reward themselves are incorrect).\nCalibrated Model-Based Deep Reinforcement Learning\nFigure 3. Performance on different control tasks. The calibrated algorithm does at least as good, and often much better than the uncalibrated\nmodels. Plots show maximum reward obtained so far, averaged over 10 trials. Standard error is displayed as the shaded areas.\nExtensions to Safety.\nCalibration also plays an important\nrole in the domain of RL safety (Berkenkamp et al., 2017).\nIn situations where the agent is planning its next action, if it\ndetermines the 90% conﬁdence interval of the predicted next\nstate to be in a safe area but this conﬁdence is miscalibrated,\nthen the agent has a higher chance of entering a failure state.\n8. Related Work\nModel-based Reinforcement Learning.\nModel-based\nRL is effective in low-data and/or high-stakes regimes such\nas robotics (Chua et al., 2018), dialogue systems (Singh\net al., 2000), education (Rollinson & Brunskill, 2015), sci-\nentiﬁc discovery (McIntire et al., 2016), or conservation\nplanning (Ermon et al., 2012). A big challenge of model-\nbased RL is the model bias, which is being addressed by\nsolutions such as model ensembles (Clavera et al., 2018;\nKurutach et al., 2018; Depeweg et al., 2016; Chua et al.,\n2018) or combining with model-free approaches (Buckman\net al., 2018).\nCalibration.\nTwo of the most widely used calibration pro-\ncedures are Platt scaling (Platt et al., 1999) and isotonic\nregression (Niculescu-Mizil & Caruana, 2005). They can be\nextended from binary to multi-class classiﬁcation (Zadrozny\n& Elkan, 2002), to structured prediction (Kuleshov & Liang,\n2015), and to regression (Kuleshov et al., 2018). Calibra-\ntion has recently been studied in the context of deep neural\nnetworks (Guo et al., 2017b; Gal et al., 2017; Lakshmi-\nnarayanan et al., 2017a), identifying important shortcomings\nin their uncertainties.\nProbabilistic forecasting.\nCalibration has been studied\nextensively in statistics (Murphy, 1973; Dawid, 1984) as\na criterion for evaluating forecasts (Gneiting & Raftery,\n2007), including from a Bayesian perspective Dawid (1984).\nRecent studies on calibration have focused on applications\nin weather forecasting (Gneiting & Raftery, 2005), and have\nled to implementations in forecasting systems (Raftery et al.,\n2005). Gneiting et al. (2007) introduced a number of deﬁni-\ntions of calibration for continuous variables, complementing\nearly work on classiﬁcation (Murphy, 1973).\n9. Conclusion\nProbabilistic models of the environment can signiﬁcantly\nimprove the performance of reinforcement learning agents.\nHowever, proper uncertainty quantiﬁcation is crucial for\nplanning and managing exploration/exploitation tradeoffs.\nWe demonstrated a general recalibration technique that can\nbe combined with most model-based reinforcement learning\nalgorithms to improve performance. Our approach leads to\nminimal computational overhead, and empirically improves\nperformance across a range of tasks.\nCalibrated Model-Based Deep Reinforcement Learning\nAcknowledgments\nThis research was supported by NSF (#1651565, #1522054,\n#1733686), ONR (N00014-19-1-2145), AFOSR (FA9550-\n19-1-0024), Amazon AWS, and Lam Research.\nReferences\nAuer, P., Cesa-Bianchi, N., and Fischer, P.\nFinite-time\nanalysis of the multiarmed bandit problem. Mach. Learn.,\n47(2-3):235–256, May 2002. ISSN 0885-6125. doi: 10.\n1023/A:1013689704352. URL https://doi.org/\n10.1023/A:1013689704352.\nBerkenkamp, F., Turchetta, M., Schoellig, A., and Krause, A.\nSafe model-based reinforcement learning with stability\nguarantees. In Guyon, I., Luxburg, U. V., Bengio, S.,\nWallach, H., Fergus, R., Vishwanathan, S., and Garnett,\nR. (eds.), Advances in Neural Information Processing\nSystems 30, pp. 908–918. Curran Associates, Inc., 2017.\nBrockman, G., Cheung, V., Pettersson, L., Schneider, J.,\nSchulman, J., Tang, J., and Zaremba, W. Openai gym.\narXiv preprint arXiv:1606.01540, 2016.\nBuckman, J., Hafner, D., Tucker, G., Brevdo, E., and Lee,\nH. Sample-efﬁcient reinforcement learning with stochas-\ntic ensemble value expansion. In Advances in Neural\nInformation Processing Systems, pp. 8234–8244, 2018.\nChua, K., Calandra, R., McAllister, R., and Levine, S. Deep\nreinforcement learning in a handful of trials using prob-\nabilistic dynamics models. In Bengio, S., Wallach, H.,\nLarochelle, H., Grauman, K., Cesa-Bianchi, N., and Gar-\nnett, R. (eds.), Advances in Neural Information Process-\ning Systems 31, pp. 4759–4770. Curran Associates, Inc.,\n2018.\nClavera, I., Rothfuss, J., Schulman, J., Fujita, Y., As-\nfour, T., and Abbeel, P.\nModel-based reinforcement\nlearning via meta-policy optimization. arXiv preprint\narXiv:1809.05214, 2018.\nDawid, A. P. Present position and potential developments:\nSome personal views: Statistical theory: The prequential\napproach. Journal of the Royal Statistical Society. Series\nA (General), 147:278–292, 1984.\nDeisenroth, M. and Rasmussen, C. E. Pilco: A model-based\nand data-efﬁcient approach to policy search. In Proceed-\nings of the 28th International Conference on machine\nlearning (ICML-11), pp. 465–472, 2011.\nDepeweg, S., Hern´andez-Lobato, J. M., Doshi-Velez, F.,\nand Udluft, S. Learning and policy search in stochastic\ndynamical systems with bayesian neural networks. arXiv\npreprint arXiv:1605.07127, 2016.\nErmon, S., Conrad, J., Gomes, C. P., and Selman, B. Playing\ngames against nature: optimal policies for renewable\nresource allocation. 2012.\nGal, Y. and Ghahramani, Z. Dropout as a Bayesian approxi-\nmation: Representing model uncertainty in deep learning.\nIn Proceedings of the 33rd International Conference on\nMachine Learning (ICML-16), 2016a.\nGal, Y. and Ghahramani, Z. A theoretically grounded ap-\nplication of dropout in recurrent neural networks. In\nAdvances in neural information processing systems, pp.\n1019–1027, 2016b.\nGal, Y., Hron, J., and Kendall, A. Concrete dropout. In\nAdvances in Neural Information Processing Systems, pp.\n3581–3590, 2017.\nGneiting, T. and Raftery, A. E. Weather forecasting with\nensemble methods. Science, 310(5746):248–249, 2005.\nGneiting, T. and Raftery, A. E. Strictly proper scoring\nrules, prediction, and estimation. Journal of the American\nStatistical Association, 102(477):359–378, 2007.\nGneiting, T., Balabdaoui, F., and Raftery, A. E. Probabilistic\nforecasts, calibration and sharpness. Journal of the Royal\nStatistical Society: Series B (Statistical Methodology), 69\n(2):243–268, 2007.\nGuo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q.\nOn calibration of modern neural networks.\nCoRR,\nabs/1706.04599, 2017a. URL http://arxiv.org/\nabs/1706.04599.\nGuo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. On\ncalibration of modern neural networks. arXiv preprint\narXiv:1706.04599, 2017b.\nHaarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft\nactor-critic: Off-policy maximum entropy deep reinforce-\nment learning with a stochastic actor. arXiv preprint\narXiv:1801.01290, 2018.\nHiguera, J. C. G., Meger, D., and Dudek, G. Synthesizing\nneural network controllers with probabilistic model based\nreinforcement learning. arXiv preprint arXiv:1803.02291,\n2018.\nHuang, G., Liu, Z., Van Der Maaten, L., and Weinberger,\nK. Q. Densely connected convolutional networks. In\n2017 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 2261–2269. IEEE, 2017.\nKuleshov, V. and Ermon, S. Estimating uncertainty online\nagainst an adversary. In AAAI, pp. 2110–2116, 2017.\nCalibrated Model-Based Deep Reinforcement Learning\nKuleshov, V. and Liang, P. Calibrated structured prediction.\nIn Advances in Neural Information Processing Systems\n(NIPS), 2015.\nKuleshov, V., Fenner, N., and Ermon, S. Accurate uncer-\ntainties for deep learning using calibrated regression. In\nDy, J. and Krause, A. (eds.), Proceedings of the 35th In-\nternational Conference on Machine Learning, volume 80\nof Proceedings of Machine Learning Research, pp. 2796–\n2804, Stockholmsmssan, Stockholm Sweden, 10–15 Jul\n2018. PMLR.\nURL http://proceedings.mlr.\npress/v80/kuleshov18a.html.\nKull, M. and Flach, P. Novel decompositions of proper scor-\ning rules for classiﬁcation: Score adjustment as precursor\nto calibration. In Joint European Conference on Machine\nLearning and Knowledge Discovery in Databases, pp.\n68–85. Springer, 2015.\nKurutach, T., Clavera, I., Duan, Y., Tamar, A., and Abbeel, P.\nModel-ensemble trust-region policy optimization. arXiv\npreprint arXiv:1802.10592, 2018.\nLakshminarayanan, B., Pritzel, A., and Blundell, C. Simple\nand scalable predictive uncertainty estimation using deep\nensembles. arXiv preprint arXiv:1612.01474, 2017a.\nLakshminarayanan, B., Pritzel, A., and Blundell, C. Simple\nand scalable predictive uncertainty estimation using deep\nensembles. In Advances in Neural Information Process-\ning Systems, pp. 6402–6413, 2017b.\nLi, L., Chu, W., Langford, J., and Schapire, R. E.\nA\ncontextual-bandit approach to personalized news arti-\ncle recommendation.\nIn Proceedings of the 19th In-\nternational Conference on World Wide Web, WWW\n’10, pp. 661–670, New York, NY, USA, 2010. ACM.\nISBN 978-1-60558-799-8.\ndoi:\n10.1145/1772690.\n1772758. URL http://doi.acm.org/10.1145/\n1772690.1772758.\nMcIntire, M., Ratner, D., and Ermon, S. Sparse gaussian\nprocesses for bayesian optimization. In UAI, 2016.\nMurphy, A. H. A new vector partition of the probability\nscore. Journal of Applied Meteorology, 12(4):595–600,\n1973.\nNiculescu-Mizil, A. and Caruana, R. Predicting good prob-\nabilities with supervised learning. In Proceedings of the\n22nd international conference on Machine learning, pp.\n625–632, 2005.\nPlatt, J. et al. Probabilistic outputs for support vector ma-\nchines and comparisons to regularized likelihood meth-\nods. Advances in large margin classiﬁers, 10(3):61–74,\n1999.\nRaftery, A. E., Gneiting, T., Balabdaoui, F., and Polakowski,\nM. Using bayesian model averaging to calibrate forecast\nensembles. Monthly weather review, 133(5):1155–1174,\n2005.\nRajeswaran, A., Ghotra, S., Ravindran, B., and Levine,\nS. Epopt: Learning robust neural network policies us-\ning model ensembles. arXiv preprint arXiv:1610.01283,\n2016.\nRollinson, J. and Brunskill, E. From predictive models to\ninstructional policies. International Educational Data\nMining Society, 2015.\nSaria, S.\nIndividualized sepsis treatment using rein-\nforcement learning.\nNature Medicine, 24(11):1641–\n1642, 11 2018.\nISSN 1078-8956.\ndoi:\n10.1038/\ns41591-018-0253-x.\nSingh, S. P., Kearns, M. J., Litman, D. J., and Walker, M. A.\nReinforcement learning for spoken dialogue systems. In\nAdvances in Neural Information Processing Systems, pp.\n956–962, 2000.\nSutton, R. S. and Barto, A. G. Reinforcement learning: An\nintroduction. MIT press, 2018.\nTodorov, E., Erez, T., and Tassa, Y. Mujoco: A physics\nengine for model-based control. In Intelligent Robots\nand Systems (IROS), 2012 IEEE/RSJ International Con-\nference on, pp. 5026–5033. IEEE, 2012.\nVan Roy, B., Bertsekas, D. P., Lee, Y., and Tsitsiklis, J. N.\nA neuro-dynamic programming approach to retailer in-\nventory management. In Decision and Control, 1997.,\nProceedings of the 36th IEEE Conference on, volume 4,\npp. 4052–4057. IEEE, 1997.\nVermeulen, S. J., Campbell, B. M., and Ingram, J. S. Climate\nchange and food systems. Annual Review of Environment\nand Resources, 37, 2012.\nZadrozny, B. and Elkan, C. Transforming classiﬁer scores\ninto accurate multiclass probability estimates. In Inter-\nnational Conference on Knowledge Discovery and Data\nMining (KDD), pp. 694–699, 2002.\nCalibrated Model-Based Deep Reinforcement Learning\nAppendices\nA. Recalibration\nMost predictive models are not calibrated out-of-the-box due to modeling bias and computational approximations. However,\ngiven an arbitrary pre-trained forecaster H : X →(Y →[0, 1]) that outputs CDFs F, we may train an auxiliary model\nR : [0, 1] →[0, 1] such that the forecasts R◦F are calibrated in the limit of enough data. This procedure, called recalibration,\nis simple to implement, computationally inexpensive, and can be applied to any probabilistic regression model in a black-box\nmanner. Furthermore, it does not increase the loss function of the original model if it belongs to a large family of objectives\ncalled proper losses (Kull & Flach, 2015; Kuleshov & Ermon, 2017).\nAlgorithm 2 CALIBRATE: Recalibration of Transition Dynamics\nInput: Uncalibrated transition model bT : S × A →P(S) that outputs CDFs Fs,a : S →[0, 1], and calibration set\nDcal = {(st, at), st+1}}N\nt=1\nOutput: Auxiliary recalibration model R : [0, 1] →[0, 1].\n1. Construct a recalibration dataset\nD =\nn\u0010\nFst,at(st+1), ˆP(Fst,at(st+1))\n\u0011oN\nt=1\nwhere\nˆP(p) = 1\nN\nN\nX\nt=1\nI[Fst,at(st+1) ≤p].\n2. Train a model R : [0, 1] →[0, 1] (e.g. sigmoid or isotonic regression) on D.\nWhen S is discrete, a popular choice of R is Platt scaling (Platt et al., 1999); Kuleshov et al. (2018) extends Platt scaling to\ncontinuous variables. Either of these methods can be used within our framework. Since this paper focuses on continuous\nstate spaces, we use the method of Kuleshov et al. (2018) described in Algorithm 2, unless otherwise indicated.\nB. Calibrated Discrete MDP\nWe provide a proof in the discrete case that calibrated uncertainties result in correct expectations with respect to the true\nprobability distribution, and thus using calibrated dynamics allow accurate evaluation of policies.\nConsider an inﬁnite-horizon discrete state MDP (S, A, T, R) and a policy π over this MDP. We are interested in evaluating\nthe goodness of this policy at any state s using the usual value iteration:\nVπ(s) = R(s) + γ\nE\na∼π(·|s)\nE\ns′∼T (·|s,a)\n[V (s′)].\n(5)\nFor the given policy π, there exists a stationary distribution σπ that would be obtained from running this policy for a long\ntime. We deﬁne the value of the entire policy V (π) as an expectation with respect to this stationary distribution i.e.\nV (π) = E\ns∼σπ\n[V (s)].\n(6)\nWe want to show that replacing the true dynamics T with calibrated dynamics bT does not affect our evaluation of the\npolicy π. To have a well-deﬁned notion of calibration, we need to deﬁne a joint distribution over the inputs and outputs\nof a predictive model. The inputs are the current state-action pair (s, a) the outputs are distributions over the next state s′.\nTo deﬁne a joint distribution P over (S, A) and S′, we use the stationary distribution σπ, the policy π, and the transition\ndynamics T to deﬁne the sub-components using the chain rule:\nP((s, a), s′) = P(s′|s, a) P(a|s) P(s)\n(7)\n= T(s′|s, a) π(a|s) σπ(s).\n(8)\nCalibrated Model-Based Deep Reinforcement Learning\nNote that deﬁning the joint distribution P this way lets us rewrite V (π) more simply as\nV (π) = E\ns∼σπ\n[V (s)]\n(9)\n= E\ns∼σπ\n[R(s)] + γ E\ns∼σπ\nE\na∼π(·|s)\nE\ns′∼T (·|s,a)\n[V (s′)]\n(10)\n= E\ns∼σπ\n[R(s)] + γ\nE\n((s,a),s′)∼P\n[V (s′)]\n(11)\n= E\ns∼σπ\n[R(s)] + γ\nE\ns′∼P(S′)\n[V (s′)]\n(12)\nThese deﬁnitions allow us to state our theorem.\nTheorem 2. Let (S, A, T, R) be a discrete MDP and let π be a stochastic policy over this MDP. Deﬁne P to be a joint\ndistribution over state-action and future state pairs ((s, a), s′) as outlined in Equation 8. Then the value of policy π under\nthe true dynamics T is equal to the value of the policy under some other dynamics bT that are calibrated with respect to P.\nProof. Since bT is calibrated with respect to P, we have P(s′ = j | bT(s′ = j|s, a) = p)) = p. Let bV (π) be the value of\npolicy π under bT. Then we have\nbV (π) = E\ns∼σπ\n[R(s)] + γ\nE\n(s,a)∼P((s,a))\nE\ns′∼b\nT (s′|s,a)\n[V (y)]\n(13)\n= E\ns∼σπ\n[R(s)] + γ\nE\ns′∼P(S′)\n[V (y)]\n(14)\n= V (π),\n(15)\nwhere the second line follows immediately from Lemma 2 when we take x = (s, a) and y = s′.\n■\nLemma 2. Consider a pair of jointly distributed variables (X, Y ) ∼P over X and Y where X = {x1, . . . , xk} and\nY = {y1, . . . , ym} are discrete spaces, and let Q(Y |X) be a distribution that is calibrated with respect to P. In other\nwords, P(Y = y | Q(Y = y | X) = p) = p. Then, for any arbitrary function g : Y →S with which we want to take an\nexpectation, the following equality holds:\nE\ny∼P(Y )\n[g(y)]\n=\nE\nx∼P(X)\ny∼Q(Y |X=x)\n[g(y)] .\n(16)\nProof. We can rewrite the expectation on the LHS of Equation 16 using the law of total probability and the chain rule to get:\nE\ny∼P(Y )\n[g(y)] =\nX\ny∈Y\ng(y)P(Y = y)\n=\nX\ny∈Y\ng(y)\nZ 1\n0\nP(Y = y, Q(Y = y | X) = p) dp\n=\nX\ny∈Y\ng(y)\nZ 1\n0\nP(Y = y | Q(Y = y | X) = p) P(Q(Y = y | X) = p) dp.\nNote that in the above derivation we perform the following slight abuse of notation:\n{Q(Y = y|X) = p} = {X | Q(Y = y|X) = p}.\nCalibrated Model-Based Deep Reinforcement Learning\nWe can apply the calibration assumption to replace the conditional term with p and rewrite P(Q(Y = y | X) = p) as a sum\nover elements of X. This gives:\nE\ny∼P(Y )\n[g(y)] =\nX\ny∈Y\ng(y)\nZ 1\n0\np · P(Q(Y = y | X) = p) dp\n=\nX\ny∈Y\ng(y)\nZ 1\n0\np ·\nX\nx∈X\nI[Q(Y = y | X = x) = p] · P(X = x) dp\n=\nX\ny∈Y\ng(y)\nX\nx∈X\nQ(Y = y | X = x) · P(X = x)\n=\nX\nx∈X\nP(X = x)\nX\ny∈Y\ng(y) · Q(Y = y | X = x)\n=\nE\nx∼P(X)\nE\ny∼Q(Y |X=x)\n[g(y)] .\n■\nC. Additional Figures\nFigure 4. Predicted expected reward for both LinUCB and CalLinUCB algorithms on the covertype dataset. Figures show predictions at\nrandom timesteps where CalLinUCB chose the optimal action but LinUCB did not. Top: Predicted reward of both algorithms for the\noptimal action. Bottom: Predicted reward of both algorithms for the action which the algorithm chose to pick instead of the optimal action\nat that timestep. We can see LinUCB consistently underestimates reward from optimal action and overestimates reward from other actions.\nOn the other hand, CalLinUCB is more accurate in its uncertainty predictions.\nCalibrated Model-Based Deep Reinforcement Learning\nFigure 5. Cartpole future state predictions. The calibrated algorithm has much tighter uncertainties around the true next state in early\ntraining iterations. Later into training, their uncertainties are almost equivalent.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-06-19",
  "updated": "2019-06-19"
}