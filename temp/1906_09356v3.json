{
  "id": "http://arxiv.org/abs/1906.09356v3",
  "title": "Unsupervised Ensemble Classification with Sequential and Networked Data",
  "authors": [
    "Panagiotis A. Traganitis",
    "Georgios B. Giannakis"
  ],
  "abstract": "Ensemble learning, the machine learning paradigm where multiple algorithms\nare combined, has exhibited promising perfomance in a variety of tasks. The\npresent work focuses on unsupervised ensemble classification. The term\nunsupervised refers to the ensemble combiner who has no knowledge of the\nground-truth labels that each classifier has been trained on. While most prior\nworks on unsupervised ensemble classification are designed for independent and\nidentically distributed (i.i.d.) data, the present work introduces an\nunsupervised scheme for learning from ensembles of classifiers in the presence\nof data dependencies. Two types of data dependencies are considered: sequential\ndata and networked data whose dependencies are captured by a graph. Moment\nmatching and Expectation Maximization algorithms are developed for the\naforementioned cases, and their performance is evaluated on synthetic and real\ndatasets.",
  "text": "IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, COMPILED DECEMBER 22, 2020\n1\nUnsupervised Ensemble Classiﬁcation\nwith Sequential and Networked Data\nPanagiotis A. Traganitis, Member, IEEE, and Georgios B. Giannakis, Fellow, IEEE\nAbstract—Ensemble learning, the machine learning paradigm where multiple models are combined, has exhibited promising perfo-\nmance in a variety of tasks. The present work focuses on unsupervised ensemble classiﬁcation. The term unsupervised refers to\nthe ensemble combiner who has no knowledge of the ground-truth labels that each classiﬁer has been trained on. While most prior\nworks on unsupervised ensemble classiﬁcation are designed for independent and identically distributed (i.i.d.) data, the present work\nintroduces an unsupervised scheme for learning from ensembles of classiﬁers in the presence of data dependencies. Two types of\ndata dependencies are considered: sequential data and networked data whose dependencies are captured by a graph. For both, novel\nmoment matching and Expectation-Maximization algorithms are developed. Performance of these algorithms is evaluated on synthetic\nand real datasets, which indicate that knowledge of data dependencies in the meta-learner is beneﬁcial for the unsupervised ensemble\nclassiﬁcation task.\nIndex Terms—Ensemble learning, unsupervised, sequential classiﬁcation, crowdsourcing, dependent data.\n!\n1\nINTRODUCTION\nA\nS social networks, connected “smart” devices and\nhighly accurate scientiﬁc instruments have permeated\nsociety, multiple machine learning, signal processing and\ndata mining algorithms have been developed to process the\ngenerated data and draw inferences from them. With most\nof these algorithms typically designed to operate under\ndifferent assumptions, combining them can be beneﬁcial\nbecause different algorithms can complement each others\nstrengths.\nEnsemble learning refers to the task of combining mul-\ntiple, possibly heterogeneous, machine learning models\nor learners [1], [2].1 In particular, ensemble classiﬁcation\nrefers to combining different classiﬁers [3], [4]. Most pop-\nular ensemble approaches, such as boosting, bagging and\nstacking, have been developed for the supervised setup.\nBoosting methods improve an ensemble of weak learners\nby iteratively retraining them and properly weighting their\noutputs [5]. Bagging algorithms train an ensemble of base\nclassiﬁers on bootstrap samples of a dataset, and combine\ntheir results using majority voting. Stacking approaches\ntrain a meta-learner using the outputs of base learners as\nfeatures [6], [7].\nIn many cases however, labeled data may not be avail-\nable, and/or one may only have access to pre-trained clas-\nsiﬁers or human annotators, justifying the need for unsuper-\nvised ensemble learning methods. Such a setup emerges in\ndiverse disciplines including medicine [8], biology [9], team\ndecision making, distributed detection, and economics [10],\nand has recently gained attention with the advent of crowd-\nsourcing [11], as well as services such as Amazon’s Mechan-\n•\nPanagiotis A. Traganitis and Georgios B. Giannakis are with the Dept. of\nElectrical and Computer Engineering and the Digital Technology Center,\nUniversity of Minnesota, Minneapolis, MN 55455, USA.\nWork\nsupported\nby\nNSF\ngrants\n1500713\nand\n1514056\nEmails:\n{traga003@umn.edu, georgios@umn.edu}\n1. The terms learner, model, and classiﬁer will be used interchange-\nably throughout this manuscript.\nical Turk [12], and Figure8 [13], to name a few. Particularly\nin crowdsourcing, human annotators play the role of base\nclassiﬁers of an ensemble learning model. Unsupervised\nensemble classiﬁcation is similar to stacking: a meta-learner\nhas to learn how to combine the outputs of multiple base\nlearners, in the absence of labeled data.\nMultiple algorithms attempt to address the unsuper-\nvised ensemble classiﬁcation problem, and a common as-\nsumption is that the data are independent and identically\ndistributed (i.i.d.) from an unknown distribution [14], [15],\n[16], [17], [18], [19], [20]. In several cases however, additional\ndomain knowledge may be available to the meta-learner.\nThis domain knowledge provides information regarding\nthe data distribution, as well as data dependencies. In\nthis paper, two types of data dependence are considered:\nsequential and networked data. Classiﬁcation of sequential\ndata arises in many natural language processing tasks such\nas part-of-speech tagging, named-entity recognition, and\ninformation extraction, to name a few [21]. Examples of\nnetworked data, where data correlations or dependencies\nare captured in a known graph, include citation, social,\ncommunication and brain networks among others. If data\ndo not exhibit network structure, the proposed networked\ndata models can accommodate side information in the form\nof a graph to enhance the label fusion process.\nA novel uniﬁed framework for unsupervised ensemble\nclassiﬁcation with data dependencies is proposed. The pre-\nsented methods and algorithms are built upon simple con-\ncepts from probability, as well as recent advances in tensor\ndecompositions [22] and optimization theory, that enable\nassessing the reliability of multiple learners, and combining\ntheir answers. Similar to prior works for i.i.d. data, in our\nproposed model each learner has a ﬁxed probability of\ndeciding that a datum belongs to class k, given that the\ntrue class of the datum is k′. These probabilities parametrize\nthe learners. Data dependencies are then encoded in the\nmarginal probability mass function (pmf) of the data labels.\narXiv:1906.09356v3  [cs.LG]  20 Dec 2020\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, COMPILED DECEMBER 22, 2020\n2\nFor sequential data, the pmf of data labels is assumed to\nbe a Markov chain, while for networked data the pmf\nof data labels is assumed to be a Markov Random Field\n(MRF.) As we are operating in an unsupervised regime, it\nis required to assume that learners make decisions inde-\npendent of each other. This assumption provides analytical\nand computational tractability. Under this assumption, the\nproposed methods are able to extract the probabilities that\nparametrize learner performance from their responses. As\nan initial step of the proposed framework, the moment-\nmatching method we introduced in [18] for ensemble clas-\nsiﬁcation of i.i.d. data, is adopted to provide rough esti-\nmates for learner parameters. At the second step, our newly\ndeveloped expectation maximization (EM) algorithms are\nemployed to reﬁne the parameters obtained from the initial\nmoment matching step. These EM algorithms are tailored\nfor the dependencies present in the data and produce the\nﬁnal label estimates.\nThe rest of the paper is organized as follows. Section 2\nstates the problem, and provides preliminaries along with a\nbrief description of the prior art in unsupervised ensemble\nclassiﬁcation for i.i.d. data. Section 3 provides an outline\nof moment matching and EM methods for i.i.d. data; Sec-\ntion 4 introduces the proposed approach to unsupervised\nensemble classiﬁcation for sequential data; while Section 5\ndeals with its counterpart for networked data. Section 6\npresents numerical tests to evaluate our methods. Finally,\nconcluding remarks and future research directions are given\nin Section 7.\nNotation: Unless otherwise noted, lowercase bold letters,\nx, denote vectors, uppercase bold letters, X, represent ma-\ntrices, and calligraphic uppercase letters, X , stand for sets.\nThe (i, j)th entry of matrix X is denoted by [X]ij; X⊤\ndenotes the tranpose of matrix X; RD stands for the D-\ndimensional real Euclidean space, R+ for the set of positive\nreal numbers, E[·] for expectation, and ∥· ∥for the ℓ2-\nnorm. Underlined capital letters X denote tensors; while\n[[A, B, C]]K is used to denote compactly a K parallel\nfactor (PARAFAC) analysis tensor [22] with factor matrices\nA := [a1, . . . , aK], B := [b1, . . . , bK], C := [c1, . . . , cK],\nthat is [[A, B, C]]K = PK\nk=1 ak ◦bk ◦ck. Finally, I(A)\ndenotes the indicator function of event A, i.e. I(A) = 1\nif A occurs and is 0 otherwise.\n2\nPROBLEM STATEMENT AND PRELIMINARIES\nConsider a dataset consisting of N data (possibly vectors)\n{xn}N\nn=1 each belonging to one of K possible classes with\ncorresponding labels {yn}N\nn=1, e.g. yn = k if xn belongs\nto class k. The pairs {(xn, yn)}N\nn=1 are drawn from an\nunknown joint distribution D, and X and Y denote ran-\ndom variables such that (X, Y ) ∼D. Consider now M\npre-trained learners that observe {xn}N\nn=1, and provide\nestimates of labels. Let fm(xn) ∈{1, . . . , K} denote the\nlabel assigned to datum xn by the m-th learner. All learner\nresponses are then collected at a meta-learner. Collect all\nlearner responses in the M × N matrix F, that has en-\ntries [F]mn = fm(xn), and all ground-truth labels in the\nN × 1 vector y = [y1, . . . , yN]⊤. The task of unsupervised\nensemble classiﬁcation is: Given only the learner responses\n{fm(xn), m = 1, . . . , M}N\nn=1, we wish to estimate the\nData x\nLearner 1\nLearner 2\n. . .\nLearner M\nMeta-learner\nˆy\nf1(x)\nf2(x)\nfM(x)\nFig. 1. Unsupervised ensemble classiﬁcation setup, where the outputs\nof learners are combined in parallel.\nground-truth labels of the data {yn}; see Fig. 1. In contrast to\nsupervised ensemble classiﬁcation, here the combined mod-\nels (learners) have been trained beforehand. In addition, the\nmeta-learner does not have access to any ground-truth data\nto learn a combining function. Thus, the focus of this work\nis on judiciously combining learner responses. This setup\nis similar to crowdsourced classiﬁcation, which also has the\nadditional challenge that human annotators (corresponding\nto the learners in the unsupervised ensemble model) may\nnot provide responses for all N data.\nConsider that each learner has a ﬁxed probability of de-\nciding that a datum belongs to class k′, when presented with\na datum of class k; and all classiﬁers behavior is presumed\ninvariant throughout the dataset. Clearly, the performance\nof each learner m is then characterized by the so-called\nconfusion matrix Γm, whose (k′, k)-th entry is\n[Γm]k′k := Γm(k′, k) = Pr (fm(X) = k′|Y = k) .\n(1)\nThe K×K matrix Γm has non-negative entries that obey the\nsimplex constraint, since PK\nk′=1 Pr (fm(X) = k′|Y = k) =\n1, for k = 1, . . . , K; hence, entries of each Γm column sum\nup to 1, that is, Γ⊤\nm1 = 1 and Γm ≥0. The confusion\nmatrix showcases the statistical behavior of a learner, as each\ncolumn provides the learners’s probability of deciding the\ncorrect class, when presented with a datum from each class.\nBefore proceeding, we adopt the following assumptions.\nAs1. Responses of different learners per datum are condi-\ntionally independent, given the ground-truth label Y of\nthe same datum X; that is,\nPr (f1(X) = k1, . . . , fM(X) = kM|Y = k)\n=\nM\nY\nm=1\nPr (fm(X) = km|Y = k) =\nM\nY\nm=1\nΓm(km, k)\nAs2. Most learners are better than random.\nAs1 is satisﬁed by learners making decisions indepen-\ndently, which is a rather standard assumption [14], [16],\n[17]. As1 can be thought of as the manifestation of diversity,\nwhich is sought after in ensemble learning systems [2].\nGraphically, this model is depicted in Fig. 2. Under As2,\nthe largest entry per Γm column is the one on the diagonal\n[Γm]kk ≥[Γm]k′k, for k′, k = 1, . . . , K.\nAs2 is another standard assumption, used to alleviate the\ninherent permutation ambiguity in estimating Γm.\nBased on the aforementioned model, knowledge of the\nstructure of the joint pdf of learner responses and labels is\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, COMPILED DECEMBER 22, 2020\n3\nY\nf2(X)\nf1(X)\nfM(X)\n. . .\nFig. 2. Graphical depiction of the Dawid-Skene model for i.i.d. data.\nShaded ellipses are observed variables (here classiﬁer responses).\ncritical. When data {(xn, yn)}N\nn=1 are i.i.d. the joint pdf can\nbe expressed as a product, that is\nPr(F, y) =\nN\nY\nn=1\nPr(fn, yn)\n(2)\nwhere fn := [f1(xn), . . . , fM(xn)]⊤.\nConsider now that data {(xn, yn)}N\nn=1 belong to a se-\nquence, i.e. the nth datum depends on the (n −1)st one.\nPertinent settings emerge with speech and natural language\nprocessing tasks such as word identiﬁcation, part-of-speech\ntagging, named-entity recognition, and information extrac-\ntion [21].\nIn order to take advantage of this dependence, we will\nencode it in the marginal pmf Pr(y) of the labels. Specif-\nically, we postulate that the sequence of labels {yn}N\nn=1\nforms a one-step time-homogeneous Markov chain; that\nis, variable yn depends only on its immediate predecessor\nyn−1. This Markov chain is characterized by a K × K\ntransition matrix T, whose (k, k′)-th entry is given by\n[T]kk′ = T(k, k′) = Pr(yn = k|yn−1 = k′).\nMatrix T has non-negative entries that satisfy the simplex\nconstraint. The marginal probability of {yn}N\nn=1 can be\nexpressed using successive conditioning, as\nPr (y = k) = Pr(y1 = k1)\nN\nY\nn=2\nT(kn, kn−1)\n(3)\nwhere k := [k1, . . . , kN]⊤. Accordingly, the data {xn}N\nn=1\ndepend only on their corresponding yn, and can be drawn\nfrom an unknown conditional pdf as xn ∼Pr(xn|yn = kn).\nThe data pairs {(xn, yn)}N\nn=1 form a hidden Markov model\n(HMM), where the labels {yn}N\nn=1 correspond to the hidden\nvariables of the HMM, while {xn}N\nn=1 correspond to the\nobserved variables of the HMM. As with the i.i.d. case,\nM learners observe {xn}N\nn=1, and provide estimates of\ntheir labels fm(xn). Under As1, the responses of different\nlearners per datum are conditionally independent, given the\nground-truth label yn of the same datum xn. A graphical\nrepresentation of this HMM is provided in Fig. 3.\nAnother type of data dependency considered in this\nwork, is the dependency between networked data. In many\ncases, additional information pertaining to the data is avail-\nable in the form of an undirected graph G(V, E), where\nV and E denote the vertex (or node) and edge sets of\nG, respectively. Examples of such networked data include\nsocial and citation networks [23], [24]. Each node of this\ngraph corresponds to a data point, thus |V| = N, while the\nedges capture pairwise relationships between the data.\nFor networked data, we will take a similar approach\nto sequential data and encode data dependence, meaning\nthe pairwise relationships provided by the graph G, in\nthe marginal pmf Pr(y). Speciﬁcally, we model the la-\nbels {yn}N\nn=1 as being drawn from an MRF. Due to the\nMRF structure of Pr(y), the conditional pmf of yn, for all\nn = 1, . . . , N, satisﬁes the local Markov property\nPr(yn|y−n) = Pr(yn|yNn)\n(4)\nwhere y−n is a vector containing all labels except yn and\nyNn is a vector containing the labels of node n neighbors.\nThen, the joint pmf of all labels is\nPr(y) = 1\nZ exp(−U(y))\n(5)\nwhere Z := P\ny exp(−U(y)) is the normalization constant,\nand U(y) is the so-called energy function. Computing the\nnormalization constant Z involves all possible conﬁgura-\ntions of y; hence, it is intractable for datasets with moderate\nto large size N. By the Hammersley-Clifford theorem the\nenergy function can be written as [25]\nU(y) = 1\n2\nX\n(n,n′)∈E\nV (yn, yn′)\n(6)\nwhere V (yn, yn′) denotes the clique potential of the (n, n′)-\nth edge, which will be deﬁned in the following sections.\nSimilar to the i.i.d. and sequential cases, data vectors\n{xn}N\nn=1 are generated from an unknown conditional pdf\nxn ∼Pr(xn|yn = k) that depends only on their corre-\nsponding label yn. M learners observe {xn}N\nn=1 and provide\nestimates of their labels fm(xn). With As1 still in effect,\nlearner responses are conditionally independent given Y .\n2.1\nPrior work\nMost prior works on unsupervised ensemble classiﬁcation\nfocus on the i.i.d. data case. Possibly the simplest scheme\nis majority voting, where the estimated label of a datum\nis the one that most learners agree upon. Majority voting\npresumes that all learners are equally “reliable,” which\nis rather unrealistic, both in crowdsourcing as well as in\nensemble learning setups. Other unsupervised ensemble\nmethods aim to estimate the parameters that character-\nize the learners’ performance, namely the learner confu-\nsion matrices. A popular approach is joint maximum like-\nlihood (ML) estimation of the unknown labels and the\naforementioned confusion matrices using the expectation-\nmaximization (EM) algorithm [14]. As EM iterations do not\nguarantee convergence to the ML solution, recent works\npursue alternative estimation methods. For binary classiﬁca-\ntion, [15], [26] describe each learner with one parameter, its\nprobability of providing a correct response, and attempts to\nlearn this parameter. Recently, for binary classiﬁcation, [17]\nused the eigendecomposition of the learner crosscovariance\nmatrix, to estimate the entries of the 2×2 confusion matrices\nof learners, while [27] introduced a minimax optimal algo-\nrithm that can infer learner reliabilities.. In the multiclass\nsetting, spectral methods such as [16], [18] utilize third-\norder moments and tensor decompositions to estimate the\nunknown reliability parameters. These estimates can then\ninitialize the EM algorithm of [14].\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, COMPILED DECEMBER 22, 2020\n4\n. . .\nyn−1\nf2(xn−1)\nf1(xn−1)\nfM(xn−1)\n. . .\nf1(xn)\nf2(xn)\nfM(xn)\n. . .\nyn\n. . .\nFig. 3. Graphical representation of the proposed model for sequential data. Shaded ellipses indicate observed variables (here learner responses).\nRecent works advocate unsupervised ensemble ap-\nproaches for sequential data. A method to aggregate learner\nlabels for sequential data relies on conditional random ﬁelds\n(CRFs) [28]. However, this method operates under strong\nand possibly less realistic assumptions requiring e.g., that\nonly one learner provides the correct label for each datum.\nTo relax the assumptions of [28], extensions of the standard\nHidden Markov Model (HMM) to incorporate learner re-\nsponses have been reported along with a variational EM [29]\nalgorithm to aggregate them [30]. As both aforementioned\nmethods require tuning of hyperparameters, a training step\nis necessary, which can be unrealistic in unsupervised set-\ntings.\nWhen features {xn}N\nn=1 are available at the meta-learner,\napproaches based on Gaussian Processes can be used to clas-\nsify the data based on learner responses [31], [32], [33]. These\napproaches can take advantage of some data dependencies,\nas they rely on linear or nonlinear similarities between data;\nhowever, in addition to requiring the data features at the\nmeta-learner, these methods have only been developed for\nbinary classiﬁcation.\nThe present work puts forth a novel scheme for unsuper-\nvised ensemble classiﬁcation in the presence of data dependencies.\nOur approach builds upon our previous work on unsuper-\nvised ensemble classiﬁcation of i.i.d. data [18], [19], and\nmarkedly extends its scope to handle sequential as well as\nnetworked data, without requiring training or access to data\nfeatures. For sequentially dependent data case we present a\nmoment matching algorithm that is able to estimate learner\nconfusion matrices as well as the parameters characterizing\nthe Markov chain of the labels. These confusion matrices\nand parameters are then reﬁned using EM iterations tailored\nfor the sequential data classiﬁcation task. For the network\ndependencies, the moment matching method for i.i.d. data\nis used to initialize an EM algorithm designed for net-\nworked data. To our knowledge, no existing work tackles\nthe ensemble classiﬁcation task under the networked data\nregime. Compared to our conference precursor in [34], here\nwe have included extensive numerical tests as well as a new\nEM algorithm for sequential data along with algorithms that\ntackle networked data.\n3\nUNSUPERVISED ENSEMBLE CLASSIFICATION OF\nI.I.D. DATA\nBefore introducing algorithms for sequential and networked\ndata, we ﬁrst outline moment matching and EM approaches\nfor i.i.d. data. If all learner confusion matrices were ideally\nknown, the label of xn could be estimated using a maximum\na posteriori (MAP) classiﬁer. The latter ﬁnds the label k that\nmaximizes the joint probability of yn and observed learner\nresponses {fm(xn) = km}M\nm=1\nˆyn = arg max\nk∈{1,...,K}\nL(xn|k) Pr(yn = k)\n(7)\n= arg max\nk∈{1,...,K}\nlog πk +\nM\nX\nm=1\nlog(Γm(km, k))\nwhere πk := Pr(yn = k) = Pr(Y = k) and L(xn|k) :=\nPr (f1(xn) = k1, . . . , fM(xn) = kM|Y = k), and the second\nequality of (7) follows from As1 and properties of the loga-\nrithm. In addition, if all classes are assumed equiprobable,\n(7) reduces to the maximum likelihood (ML) classiﬁer.\nBut even for non-equiprobable classes, unsupervised\nensemble classiﬁcation requires estimates of the class priors\nπ := [π1, . . . , πK]⊤as well as all the learner performance\nparameters {Γm}M\nm=1.\n3.1\nEM algorithm for i.i.d. data\nHere we outline how the EM algorithm can be employed\nto estimate the wanted learner performance parameters by\niteratively maximizing the log-likelihood of the observed\nlearner responses; that is, log Pr(F; θ), where θ collects\nall the learner confusion matrices (prior probabilities are\nassumed equal and are dropped for simplicity). Each EM\niteration includes the expectation (E-)step and the maxi-\nmization (M-)step.\nAt the E-step of the (i + 1)st iteration, the estimate θ(i)\nis given, and the so-termed Q-function is obtained as\nQ(θ; θ(i)) = Ey|F;θ(i)[log Pr(y, F; θ)]\n(8)\n= Ey|F;θ(i)[log Pr(F|y; θ)] + Ey|F;θ(i)[log Pr(y; θ)].\nSince the data are i.i.d., we have under As1 that\nEy|F;θ(i)[log Pr(F|y; θ)] =\nN\nX\nn=1\nK\nX\nk=1\nM\nX\nm=1\nlog Γm(fm(xn), k)qnk\nwhere qnk := Pr(yn = k|{fm(xn)}M\nm=1; θ(i)) is the posterior\nof label yn given the observed data and current parameters,\nand\nEy|F;θ(i)[log Pr(y; θ)] =\nN\nX\nn=1\nK\nX\nk=1\nlog Pr(yn = k; θ)qnk.\nUnder this model, it can be shown that [14], [16]\nq(i+1)\nnk\n= 1\nZ exp\n M\nX\nm=1\nK\nX\nk′=1\nI(fm(xn) = k′) log(Γ(i)\nm (k′, k))\n!\n(9)\nwhere Z is the normalization constant.\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, COMPILED DECEMBER 22, 2020\n5\nAt the M-step, learner confusion matrices are updated\nby maximizing the Q-function to obtain\nθ(i+1) = arg max\nθ\nQ(θ; θ(i)).\n(10)\nIt can be shown per learner m that (10) boils down to\n[Γ(i+1)\nm\n]k′k =\nPN\nn=1 q(i+1)\nnk\nI(fm(xn) = k′)\nPK\nk′′=1\nPN\nn=1 q(i+1)\nnk\nI(fm(xn) = k\n′′)\n.\n(11)\nThe E- and M-steps are repeated until convergence, and ML\nestimates of the labels are subsequently found as\nˆy(xn) = arg max\nk∈{1,...,K}\nPr({fm(xn)}M\nm=1, yn = k)\n= arg max\nk∈{1,...,K}\nqnk.\n3.2\nMoment-matching for i.i.d. data\nAs an alternative to EM, the annonator performance param-\neters can be estimated using the moment-matching method\nwe introduced for i.i.d. data in [18], which we outline\nhere before extending it to dependent data in the ensuing\nsections.\nConsider representing label k using the canonical K × 1\nvector ek, namely the k-th column of the K × K identity\nmatrix I. Let fm(X) denote the m-th learner’s response\nin vector format. Since fm(X) is just a vector represen-\ntation of fm(X), it holds that Pr (fm(X) = k′|Y = k) ≡\nPr (fm(X) = ek′|Y = k). With γm,k denoting the k-th col-\numn of Γm, it thus holds that\nγm,k := E[fm(X)|Y = k] =\nK\nX\nk′=1\nek′ Pr (fm(X) = k′|Y = k)\n(12)\nwhere we used the deﬁnition of conditional expectation.\nUsing (12) and the law of total probability, the mean vector\nof responses from learner m, is hence\nE[fm(X)] =\nK\nX\nk=1\nE[fm(X)|Y = k] Pr (Y = k) = Γmπ. (13)\nUpon deﬁning the diagonal matrix Π := diag(π), the K×K\ncross-correlation matrix between the responses of learners m\nand m′ ̸= m, can be expressed as\nRmm′ := E[fm(X)f ⊤\nm′(X)]\n=\nK\nX\nk=1\nE[fm(X)|Y = k] E[f ⊤\nm′(X)|Y = k] Pr (Y = k)\n= Γmdiag(π)Γ⊤\nm′ = ΓmΠΓ⊤\nm′\n(14)\nwhere we successively relied on the law of total probability,\nAs1, and (12). Consider now the K×K×K cross-correlation\ntensor between the responses of learners m, m′ ̸= m and\nm′′ ̸= m′, m, namely\nΨmm′m′′ = E[fm(X) ◦fm′(X) ◦fm′′(X)].\n(15)\nIt can be shown that Ψmm′m′′ obeys a PARAFAC model\nwith factor matrices Γm, Γm′ and Γm′′ [18]; that is,\nΨmm′m′′ =\nK\nX\nk=1\nπkγm,k ◦γm′,k ◦γm′′,k\n(16)\n= [[ΓmΠ, Γm′, Γm′′]]K.\nNote here that the diagonal matrix Π can multiply any of\nthe factor matrices Γm, Γm′, or, Γm′′.\nHaving\navailable\nthe\nsample\naverage\ncounterparts\nof\n(13),\n(14)\nand\n(15),\ncorrespondingly\n{µm}M\nm=1,\n{Smm′}M\nm,m′=1, and {T mm′m′′ }M\nm,m′,m′′=1, estimates of\n{Γm}M\nm=1 and π can be readily obtained. This approach\nis an instantiation of the method of moments estimation\nmethod, see e.g. [35], and can be cast as the following\nconstrained optimization task\nmin\nπ∈Cπ,{Γm∈C}M\nm=1\ng({Γm}M\nm=1, π)\n(17)\nwhere\ng({Γm}, π) :=\nM\nX\nm=1\n∥µm −Γmπ∥2\n2\n+\nM\nX\nm=1\nm′>m\n∥Smm′ −ΓmΠΓ⊤\nm′∥2\nF\n+\nM\nX\nm=1\nm′>m,m′′>m′\n∥T mm′m′′ −[[ΓmΠ, Γm′, Γm′′]]K∥2\nF ,\nC := {Γ ∈RK×K : Γ ≥0, Γ⊤1 = 1}, is the convex set\nof matrices whose columns lie on a probability simplex, and\nCp := {u ∈RK : u ≥0, u⊤1 = 1} denotes the simplex\nconstraint for a K × 1 vector. The non-convex optimization\nin (17) can be solved using the alternating optimization\nmethod described in [18], which is guaranteed to converge\nto a stationary point of g [36]. As2 is used here to address\nthe permutation ambiguity that is induced by the tensor\ndecomposition of (17). Interested readers are referred to [18]\nfor implementation details, and theoretical guarantees.\nUpon obtaining {ˆΓm}M\nm=1 and ˆπ, a MAP classiﬁer can\nbe subsequently employed to estimate the label for each\ndatum; that is, for n = 1, . . . , N, we obtain\nˆy(xn) = arg max\nk∈{1,...,K}\nlog ˆπk +\nM\nX\nm=1\nlog ˆΓm(fm(xn), k)\n(18)\nwhere ˆΓm(k′, k) = [ˆΓm]k′k, and ˆπk = [ˆπ]k. The estimates\n{ˆΓm}M\nm=1, ˆπ, {ˆyn}, can be improved using the EM iterations\nin Sec. 3.1. Such a reﬁnement is desirable when N is rela-\ntively small, and thus moment estimates are not as reliable.\nNext, we will introduce our novel approaches for ensem-\nble classiﬁcation with sequential and networked data.\n4\nSEQUENTIALLY DEPENDENT DATA\nHaving recapped moment matching and EM approaches for\ni.i.d. data, we now turn our attention to the sequential data\ncase. Recall from Sec. 2 that we postulate labels y forming\na one-step time-homogeneous Markov chain, characterized\nby the transition matrix T ∈C, and that learner responses\nobey As1. The labels y along with learner responses F,\nform an HMM. As with the i.i.d. case, here we develop\nboth moment-matching and EM algorithms tailored for\nsequential data.\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, COMPILED DECEMBER 22, 2020\n6\n4.1\nLabel estimation for sequential data\nGiven only learner responses for all data in a sequence, an\napproach to estimating the labels of each datum, meaning\nthe hidden variables of the HMM, is to ﬁnd the sequence k\nthat maximizes the joint probability of the labels y and the\nlearner responses F, namely\nPr (y = k, F)\n= Pr(y1 = k1)\nN\nY\nn=2\nT(kn, kn−1)\nM\nY\nm=1\nΓm(fm(xn), kn) (19)\nwhere the equality is due to (3) and As1. This can be done\nefﬁciently using the Viterbi algorithm [37], [38]. In order\nto obtain estimates of the labels, {Γm}M\nm=1 and T must be\navailable. The next subsections will show that {Γm}M\nm=1 and\nT can be recovered by the learner responses, using moment\nmatching and/or EM approaches.\n4.2\nMoment matching for sequential data\nUnder the sequential data model outlined earlier, we require\nan additional assumption before presenting our moment\nmatching algorithm.\nAs3. The Markov chain formed by the labels {yn} has\na unique stationary distribution π := [π1, . . . , πK]⊤=\n[Pr(Y = 1), . . . , Pr(Y = K)]⊤, and is also irreducible.\nSimilar to [39], this assumption enables decoupling the\nproblem of learning the parameters of interest in two steps.\nFirst, estimates of the confusion matrices {ˆΓm}M\nm=1 and\nstationary priors ˆπ are obtained; and subsequently, the tran-\nsition matrix is estimated as ˆT before obtaining an estimate\nof the labels {ˆyn}N\nn=1.\nUnder As3, the HMM is mixing and assuming that y0 is\ndrawn from the stationary distribution π, the responses of a\nlearner m can be considered to be generated from a mixture\nmodel, see e.g. [39]\nfm(X) ∼\nK\nX\nk=1\nπk Pr(fm(X)|Y = k) .\n(20)\nBased on the latter, the remainder of this subsection will\ntreat labels {yn}N\nn=1, as if they had been drawn i.i.d.\nfrom the stationary distribution π, that is yn ∼π for\nn = 1, . . . , N. Then, the procedure outlined in Sec. 3.2\ncan be readily adopted to obtain estimates of the stationary\ndistribution ˆπ and the confusion matrices {ˆΓm}M\nm=1.\nWith estimates of learner confusion matrices {ˆΓm} and\nstationary probabilities ˆπ at hand, we turn our attention\nto the estimation of the transition matrix T. To this end,\nconsider the cross-correlation matrix of consecutive vec-\ntorized observations between learners m and m′, namely\n˜Rmm′ = E[fm(xn)f ⊤\nm′(xn−1)]. Under the HMM of Sec. 4,\nthe latter is given by\n˜Rmm′ = ΓmTdiag(π)Γ⊤\nm′ = ΓmAΓ⊤\nm′\n(21)\nwhere A := Tdiag(π). Letting ˜Smm′ denote the sample\ncounterpart of (21), and with {ˆΓm}M\nm=1 available, we can\nrecover T as follows. First, we solve the convex moment-\nmatching optimization problem\nmin\nA∈CS\nM\nX\nm=1\nm′>m\n∥˜Smm′ −ˆΓmAˆΓ⊤\nm′∥2\nF\n(22)\nAlgorithm 1 EM algorithm for Sequential Data\nInput: Learner responses {fm(xn)}M,N\nm=1,n=1, initial esti-\nmates T(0), {Γ(0)\nm }M\nm=1.\nOutput: Estimates ˆT, {ˆΓm}M\nm=1.\n1: while not converged do\n2:\nEstimate\nˆq(i+1)\nnk\nand\nˆξ(i+1)\nn\n(k, k′)\nusing\nthe\nforward-backward algorithm (App. A).\n3:\nEstimate {ˆΓ(i+1)\nm\n}M\nm=1 via (27).\n4:\nEstimate ˆT(i+1) via (26).\n5:\ni ←i + 1\n6: end while\nAlgorithm 2 Unsupervised Ensemble Classiﬁer for Sequen-\ntial Data\nInput: Learner responses {fm(xn)}M,N\nm=1,n=1.\nOutput: Estimates of data labels {ˆyn}N\nn=1.\n1: Estimate π, {Γm}M\nm=1 via (17).\n2: Estimate ˆT via (22) and (23).\n3: Estimate\nˆyn\nusing\nthe\nViterbi\nalgorithm\n[cf. Sec. 4.1].\n4: If needed reﬁne estimates of ˆT, {ˆΓm} and {ˆyn}\nusing Alg. 1.\nwhere CS is the set of matrices whose entries are positive\nand sum to 1, namely CS := {X ∈RK×K : X ≥0, 1⊤X1 =\n1}. The constraint is due to the fact that 1⊤T = 1⊤,\ndiag(π)1 = π, and π⊤1 = 1. Note that (22) is a standard\nconstrained convex optimization problem that can be solved\nwith off-the-shelf tools, such as CVX [40]. Having obtained\nˆA from (22), we can then estimate the transition matrix as\nˆT = ˆA(diag(ˆπ))−1.\n(23)\nNote here that explicit knowledge of π is not required, as its\nestimate can be recovered from ˆA as\nˆπ⊤= 1⊤ˆA = 1⊤ˆTdiag(ˆπ) = 1⊤diag(ˆπ).\nThe following proposition argues the consistency of the\ntransition matrix estimates ˆT.\nProposition 1. Given accurate estimates of {Γm} and π, the\nestimate ˆT given by (22) and (23) approaches T as N →∞.\nProof. By the law of large numbers, ˜Smm′ →˜Rmm′ as N →\n∞for all m, m′. Since the objective in (22) is convex, from\n[41], we have that ˆA will converge to A = Tdiag(π) as\nN →∞. Finally, as ˆT can be recovered from ˆA in closed\nform [cf. (23)], the proof is complete.\nWith estimates of {ˆΓm}, ˆπ and ˆT at hand, estimates of\nthe labels {yn}N\nn=1 can be obtained using the method de-\nscribed in Sec. 4.1. Futhermore, the estimates of {ˆΓm}, ˆπ and\nˆT can be used to initialize an EM algorithm (a.k.a. Baum-\nWelch), whose details are provided in the next subsection.\nRemark 1. While here we employed the algorithm of [18]\nto estimate {Γm}, any other unsupervised ensemble classi-\nﬁcation algorithm, such as [14], [16], can be utilized too. In\naddition, methods that jointly estimate confusion matrices\nand Markov chain parameters such as [42], can also be\nappropriately modiﬁed for the ensemble classiﬁcation task.\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, COMPILED DECEMBER 22, 2020\n7\n4.3\nEM algorithm for sequential data\nAs with the i.i.d. case of Sec. 3, the EM algorithm devel-\noped here iteratively maximizes the log-likelihood of the\nobserved learner responses. In order to update the parame-\nters of interest θ := vec([T, Γ1, . . . , ΓM]) per iteration, the\nfollowing two quantities have to be found\nqnk = Pr(yn = k|F, θ)\n(24)\nand\nξn(k, k′) = Pr(yn = k, yn+1 = k′|F, θ) .\n(25)\nLuckily, due to the causal structure of Pr(y), the afore-\nmentioned quantities can be estimated efﬁciently using the\nforward-backward algorithm [37], whose details can be\nfound in Appendix A of the supplementary material.\nAt iteration i, after obtaining q(i+1)\nnk\n, ξ(i+1)\nn\n(k, k′) for\nk, k′ = 1, . . . , K and n = 1, . . . , N, via the forward-\nbackward algorithm, the transition and confusion matrix\nestimates can be updated as\n[ˆT(i+1)]k′k =\nPN−1\nn=1 ξ(i+1)\nn\n(k′, k)\nPN−1\nn=1 q(i+1)\nnk′\n(26)\n[ˆΓ(i+1)\nm\n]k′k =\nPN\nn=1 q(i+1)\nnk\nI(fm(xn) = k′)\nPK\nk′′=1\nPN\nn=1 q(i+1)\nnk\nI(fm(xn) = k\n′′)\n.\n(27)\nThe EM iterations for sequential data with dependent labels\nis summarized in Alg. 1, while the overall ensemble classi-\nﬁer for sequential data is tabulated in Alg. 2. Note that the\nEM algorithm of this subsection does not rely on As3.\n5\nNETWORK DEPENDENT DATA\nTo tackle the networked data case, this section will introduce\nour novel approach to unsupervised ensemble classiﬁcation\nof networked data. Given a graph G encoding data depen-\ndencies, recall from Sec. 2 that the joint pmf of all labels\nfollows an MRF; thus, Pr(y) = 1\nZ exp(−U(y)) with\nU(y) = 1\n2\nX\n(n,n′)∈E\nV (yn, yn′)\nwhere V (yn, yn′) is the clique potential of the (n, n′)-th\nedge. Here, we select the clique potential as\nV (yn, yn′) :=\n(\n0\nif yn = yn′\nδnn′\nif yn ̸= yn′ ,\n(28)\nwhere δnn′ > 0 is some predeﬁned scalar, which controls\nhow much we trust the given graph G. The local energy at\nnode (datum) n of the graph is then deﬁned as\nUn(yn) = 1\n2\nX\nn′∈Nn\nV (yn, yn′).\n(29)\nThis particular choice of clique potentials forces neighboring\nnodes (data) of the graph to have similar labels, and has\nbeen successfully used in image segmentation [43], [44].\nUnder As1 and the aforementioned model, the joint pmf of\nlabel yn and corresponding learner responses {fm(xn)}M\nm=1\ngiven the neighborhood yNn of node n, can be expressed as\nPr\n\u0010\n{fm(xn)}M\nm=1, yn = k|yNn = kNn\n\u0011\n(30)\n=\nM\nY\nm=1\nΓm(fm(xn), k) Pr(yn = k|yNn = kNn)\nand accordingly the posterior probability of label yn as\nPr\n\u0010\nyn = k|{fm(xn)}M\nm=1, yNn = kNn\n\u0011\n(31)\n∝\nM\nY\nm=1\nΓm(fm(xn), k) Pr(yn = k|yNn = kNn)\n= exp\n \n−Un(k) +\nM\nX\nm=1\nlog Γm(fm(xn), k)\n!\n.\n5.1\nLabel estimation for networked data\nFinding ML estimates of the labels ˆy, under the aforemen-\ntioned model, involves the following optimization problem\nˆy = arg max\ny\nPr(F, y) = arg max\ny\nPr(F|y) Pr(y)\n(32)\n= arg max\ny\n1\nZ exp(−U(y)) Pr(F|y).\nUnfortunately, (32) is intractable even for relatively small N,\ndue to the structure of (5). This motivates well approxima-\ntion techniques to obtain estimates of the labels.\nPopular approximation methods include Gibbs sam-\npling [45] and mean-ﬁeld approximations [43]. Here, we\nopted for an iterative method called iterated conditional\nmodes (ICM), which has been used successfully in image\nsegmentation [44]. Per ICM iteration, we are given estimates\n{ˆΓm}M\nm=1, and update the label of datum n by ﬁnding the\nk maximizing its local posterior probability; that is,\n˜y(t)\nn = arg max\nk∈{1,...,K}\nPr\n\u0010\nyn = k|{fm(xn)}M\nm=1, ˜y(t−1)\nNn\n\u0011\n= arg min\nk∈{1,...,K}\nUn(k) −\nM\nX\nm=1\nlog\n\u0010\nˆΓm(fm(xn), k)\n\u0011\n(33)\nwhere the superscript denotes the iteration index, ˜yNn\ndenotes the label estimates provided by the previous ICM\niteration, and the second equality is due to (31). The opti-\nmization in (33) is carried out for n = 1, . . . , N until the\nvalues of ˜y have converged or until a maximum number of\niterations Tmax has been reached.\nThe next subsection puts forth an EM algorithm for\nestimating {ˆyn}N\nn=1 and {ˆΓm}M\nm=1.\n5.2\nEM algorithm for networked data\nAs with the i.i.d. case in Sec. 3 and the sequential case in\nSec. 4, the EM algorithm of this section seeks to iteratively\nmaximize the marginal log-likelihood of observed learner\nresponses. However, the Q-function [cf. Sec. 3.1] is now\ncumbersome to compute under the MRF constraint on y.\nFor this reason, we resort to the approximation technique\nof the previous subsection to compute estimates of qnk =\nPr(yn = k|{fm(xn)}M\nm=1; θ). Speciﬁcally, per EM iteration\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, COMPILED DECEMBER 22, 2020\n8\nAlgorithm 3 EM algorithm for networked data\nInput: Learner\nresponses\n{fm(xn)}M,N\nm=1,n=1,\ninitial\ny(0), {Γ(0)\nm }M\nm=1, Data graph G(V, E).\nOutput: Estimates of data labels {ˆyn}N\nn=1.\n1: while not converged do\n2:\nwhile not converged AND t < Tmax do\n3:\nfor n = 1, . . . , N do\n4:\nUpdate ˜y(t)\nn\nusing (33).\n5:\nend for\n6:\nt ←t + 1\n7:\nend while\n8:\nCompute ˆq(i+1)\nnk\nusing (34).\n9:\nCompute {ˆΓ(i+1)\nm\n}M\nm=1 using (36).\n10:\ni ←i + 1\n11: end while\ni, we let ˆy(i) := [ˆy(i)\n1 , . . . , ˆy(i)\nN ] denote the estimates obtained\nby the iterative procedure of Sec. 5.1. Then, estimates ˆq(i+1)\nnk\nare obtained as [cf. 31]\nˆq(i+1)\nnk\n(34)\n= 1\nZ′ exp\n \n−U (i+1)\nn\n(k) +\nM\nX\nm=1\nlog\n\u0010\nˆΓ(i)\nm (fm(xn), k)\n\u0011!\nwhere\nZ′ =\nX\nk\nexp\n \n−U (i+1)\nn\n(k) +\nM\nX\nm=1\nlog\n\u0010\nˆΓ(i)\nm (fm(xn), k)\n\u0011!\nis the normalization constant, and U (i+1)\nn\n(k) is given by\nU (i+1)\nn\n(k) = 1\n2\nX\nn′∈Nn\nV (k, ˆy(i+1)\nn′\n).\n(35)\nFinally, the M-step that involves ﬁnding estimates of\n{Γm}M\nm=1 is identical to the M-step of the EM algorithm\nof Sec. 3.1 for i.i.d. data; that is,\n[ˆΓ(i+1)\nm\n]k′k =\nPN\nn=1 ˆq(i+1)\nnk\nI(fm(xn) = k′)\nPK\nk′′=1\nPN\nn=1 ˆq(i+1)\nnk\nI(fm(xn) = k\n′′)\n.\n(36)\nSimilar to the i.i.d. case, the aforementioned EM solver\ndeals with a non-convex problem. In addition, the ICM\nmethod outlined in Sec. 5.1 is a deterministic approach that\nperforms greedy optimization. Therefore, proper initializa-\ntion is crucial for obtaining accurate estimates of the labels\nand learner confusion matrices.\nAs with the decoupling approach of Sec. 4, here we ﬁrst\nobtain estimates of learner confusion matrices {ˆΓm}M\nm=1\nand labels ˆy, using the moment-matching algorithm of\nSec. 3.2. These values are then provided as initialization to\nAlg. 3. In cases where N is small to have accurate moment\nestimates, majority voting can be used instead to initialize\nAlg. 3. The entire procedure for unsupervised ensemble\nclassiﬁcation with networked data is tabulated in Alg. 4.\nThe next section will evaluate the performance of our\nproposed schemes.\nRemark 2. Contemporary Bayesian inference tools, such as\nvariational inference [29], can also be appropriately modi-\nﬁed to estimate labels of networked data that are expected\nto increase classiﬁcation performance.\nAlgorithm 4 Unsupervised Ensemble Classiﬁer for Net-\nworked data\nInput: Learner responses {fm(xn)}M,N\nm=1,n=1, Data graph\nG(V, E)\nOutput: Estimates of data labels {ˆyn}N\nn=1\n1: Estimate initial values of {Γm}M\nm=1 via (17).\n2: Estimate initial values of {ˆyn}N\nn=1 using (18).\n3: Reﬁne estimates of {ˆyn}N\nn=1 and {ˆΓm}M\nm=1 using\nAlg. 3.\n6\nNUMERICAL TESTS\nThe performance of the novel algorithms for both sequen-\ntial and networked data is evaluated in this section using\nsynthetic and real datasets. To showcase the importance\nof accounting for data dependencies in the unsupervised\nensemble task, the proposed algorithms are compared to\ntheir counterparts designed for i.i.d. data. Since most of the\nnumerical tests involve data with multiple, and potentially\nimbalanced classes, unless otherwise noted, the metrics\nevaluated are the per-class precision, per-class recall and\nthe overall F-score [46]. F-score is the harmonic mean of\nprecision and recall, that is\nF-score = 2\nK\nK\nX\nk=1\nPrecisionk ∗Recallk\nPrecisionk + Recallk\n(37)\nwhere Precisionk, Recallk denote the per-class precision and\nrecall, respectively. Precisionk for a class k measures the\nproportion of the data predicted to be in class k that are\nactually from this class. Recallk for a class k on the other\nhand measures the proportion of data that were actually in\nclass k and were predicted to be in class k. To assess how ac-\ncurately the algorithms can recover learner parameters, the\naverage confusion matrix estimation error is also evaluated\non synthetic data, as\n¯εCM := 1\nM\nM\nX\nm=1\n∥Γm −ˆΓm∥1\n∥Γm∥1\n= 1\nM\nM\nX\nm=1\n∥Γm −ˆΓm∥1\n(38)\nAll results represent averages over 10 independent Monte\nCarlo runs, using MATLAB [47]. Vertical lines in some\nﬁgures indicate standard deviation.\n6.1\nSequential data\nFor sequential data, Alg. 2 with and without EM reﬁnement\n(denoted as Alg. 2 + Alg. 1 and Alg. 2, respectively) is\ncompared to the single best classiﬁer, with respect to F-\nscore, of the ensemble (denoted as Single best); majority\nvoting (denoted as MV); the moment-matching method of\n[18] described in Sec. 3.2 (denoted as MM); Alg. 1 initialized\nwith majority voting (denoted as MV + Alg. 1); and, ”ora-\ncle” classiﬁers. “Oracle” classiﬁers solve (19) using Viterbi’s\nalgorithm [38], and have access to ground-truth learner\nconfusion matrices {Γm}M\nm=1 and the ground-truth Markov\nchain transition matrix T. These “oracle” classiﬁers are used\nas an ideal benchmark for all other methods. The transition\nmatrix estimation error ∥T −ˆT∥1 is also evaluated using\nsynthetic data. For real data tests, instead of MM the EM\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, COMPILED DECEMBER 22, 2020\n9\nalgorithm of [14] initialized with MM is evaluated (denoted\nas DS.)\nAll datasets in this subsection are split into sequences.\nHere, we assume that per dataset these sequences are drawn\nfrom the same ensemble HMM [cf. Sec. 4]. The reported F-\nscore represents the averaged F-score from all sequences.\n6.1.1\nSynthetic data\nFor synthetic tests, S sequences of Ns, s = 1, . . . S, ground-\ntruth labels each, were generated from a Markov chain,\nwhose transition matrix was drawn at random such that\nT ∈C. Each of the N = P\ns Ns ground-truth labels {yn}N\nn=1\ncorresponds to one out of K possible classes. Afterwards,\n{Γm}M\nm=1 were generated at random, such that Γm ∈C,\nfor all m = 1, . . . , M, and ⌊M/2⌋+ 1 learners are better\nthan random, as per As2. Then learners’ responses were\ngenerated as follows: if yn = k, then the response of\nlearner m will be generated randomly according to the k-\nth column of its confusion matrix, γm,k [cf. Sec. 2], that is\nfm(xn) ∼γm,k.\nFig. 4 shows the average F-score for a synthetic dataset\nwith K = 4, M = 10 learners and a variable number of\ndata N and Ns = 40 for all s = 1, . . . , S. Fig. 5 shows\nthe average confusion and transition matrix estimation er-\nrors for varying N. As the number of data N increases\nthe performance of the proposed methods approaches the\nperformance of the “oracle” one. Accordingly, the confusion\nand transition matrix estimates are approaching the true\nones as N increases. This is to be expected, as noted in\n[18], since the estimated moments are more accurate for\nlarge N. Interestingly, Alg. 1 performs well when initialized\nwith majority voting, even though it reaches a performance\nplateau as N increases. For small N however, it outperforms\nthe other proposed methods. This suggests that initializing\nAlg. 1 with majority voting is preferable when N is not large\nenough to obtain accurate moment estimation.\nThe next experiment evaluates the inﬂuence of the num-\nber of learners M for the sequential classiﬁcation task.\nFigs. 6 and 7 showcase results for an experiment with\nK = 4, ﬁxed number of data N = 103, Ns = 40 and\na varying number of learners M. Clearly, the presence of\nmultiple learners is beneﬁcial, as the F-score increases for all\nalgorithms, while the confusion and transition matrix errors\ndecrease. As with the previous experiment, the performance\nof Alg. 1 + Alg. 2 improves in terms of F-score, as M\nincreases.\n6.1.2\nReal data\nFurther tests were conducted on three real datasets, the\npart-of-speech (POS) tagging dataset, the named entity\nrecognition (NER) dataset, and the biomedical information\nextraction (IE) [30] dataset.\nFor the POS dataset M = 10 classiﬁers were trained\nusing NLTK [48] on subsets of the Brown coprus [49] to\nprovide part-of-speech (POS) tags of text. The number of\ntags is K = 12. Then the classiﬁers provided POS tags for\nall words in the Penn Treebank corpus [50], which contains\nN = 100, 676 words.\nThe NER dataset consists of 5, 916 crowd annotated\nsentences from the CoNLL database [28], [51]. The dataset\ncontains N = 78, 107 words belonging to K = 9 distinct\n100   \n1000  \n10000 \n100000\nN\n20\n30\n40\n50\n60\n70\n80\n90\nF-score\nMM\nMV\nAlg. 1 + Alg. 2\nOracle\nAlg. 2\nMV + Alg. 1\nSingle best\nFig. 4. Average F-score for a synthetic sequential dataset with K = 4\nand M = 10 learners\n100   \n1000  \n10000 \n100000\nN\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\nMM\nAlg. 1 + Alg. 2\nMV + Alg. 1\n(a) Confusion matrix estimation error\n100   \n1000  \n10000 \n100000\nN\n0\n0.5\n1\n1.5\nAlg. 2\nAlg. 1 + Alg. 2\nMV + Alg. 1\n(b) Transition matrix estimation error\nFig. 5. Average estimation errors of confusion matrices and prior prob-\nabilities for a synthetic sequential dataset with K = 4 and M = 10\nlearners\nclasses, each describing a different named-entity such as\nperson, location, and organization. Corresponding to learn-\ners in our models, M = 47 human annotators provided\nlabels for the words in the dataset.\nThe Biomedical IE dataset consists of 5, 000 medical\npaper Abstracts, on which M = 312 human annotators,\nwere tasked with marking all text spans in a given Abstract\nthat identify the population of a randomized controlled trial.\nThe dataset consists of N = 7, 880, 254 words belonging\ninto K = 2 classes: in a span identifying the population\nor outside. For this particular dataset we evaluate Precision\nand Recall per sequence in the following way, which was\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, COMPILED DECEMBER 22, 2020\n10\n5 \n10\n15\n20\nM\n30\n40\n50\n60\n70\n80\n90\n100\nF - score\nMM\nMV\nAlg. 1 + Alg. 2\nOracle\nAlg. 2\nMV + Alg. 1\nSingle best\nFig. 6. Average F-score for a synthetic sequential dataset with K = 4\nand N = 103 data.\n5 \n10\n15\n20\nM\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nMM\nAlg. 1 + Alg. 2\nMV + Alg. 1\n(a) Confusion matrix estimation error\n5 \n10\n15\n20\nM\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\nAlg. 2\nAlg. 1 + Alg. 2\nMV + Alg. 1\n(b) Transition matrix estimation error\nFig. 7. Average estimation errors of confusion matrices and prior prob-\nabilities for a synthetic sequential dataset with K = 2 and M = 10\nlearners\nsuggested in [30]\nPrecision =\n# true positive words\n# words in a predicted span\nRecall =\n# words in a predicted span\n# words in ground-truth span.\nThese new deﬁnitions of precision and recall, allow us to\ncredit partial matches. The justiﬁcation for using these alter-\nnative deﬁnitions is that the previous ones are too strict for\nthis task, where annotated sequences are especially long. We\nused the M = 120 annotators that had provided the largest\nnumber of responses to mantain reasonable computational\ncomplexity. Results for all datasets are listed in Tab. 1.\nFor the POS dataset, it can be seen that Alg. 1 + Alg. 2\nperforms best in all metrics. Similar results are showcased\nfor the NER dataset, with majority voting achieving the best\nprecision, and Alg. 1 + Alg. 2 exhibiting the best recall and\noverall F-score. For the Biomedical IE dataset, while major-\nity voting achieves the best precision of all algorithms, due\nto its low recall, the overall F-score is low. However, Alg. 1\n+ Alg. 2 outperforms competing alternatives with regards\nto F-score, while MV + Alg. 1 exhibits the best recall. Note\nthat the single best learners for the NER and Biomedical\nIE datasets are evaluated only on the subsets of data for\nwhich they have provided responses; the best learner for\nthe NER dataset annotated approximately 12, 500 words,\nwhereas the best learner for the Biomedical IE dataset an-\nnotated approximately 14, 300 words. The performance of\nthe proposed label aggregation methods relies on multiple\nparameters such as the number and ability of learners, and\nhow well the proposed model approximates the learner\nbehavior and data. The modest performance gains of Algs.\n2, 1 and MV + 1 compared to the single best learner may be\nattributed to such issues.\n6.2\nNetworked data\nFor networked data, Alg. 4 (denoted as Alg. 4) is compared\nto the single best classiﬁer, with respect to F-score, of the\nensemble (denoted as Single best), majority voting (denoted\nas MV), the moment-matching method of [18] described in\nSec. 3.2 (denoted as MM) and Alg. 3 initialized with majority\nvoting (denoted as MV + Alg. 3). For real data tests, instead\nof MM the EM algorithm of [14] initialized with MM is\nevaluated (denoted as DS.) The average degree ¯d of the\nnetwork is used to quantify the degree of data dependency\n( ¯d is the number of connections averaged across nodes).\n6.2.1\nSynthetic data\nFor the synthetic data tests, an N-node, K community\ngraph is generated using a stochastic block model [52].\nEach community corresponds to a class, and the labels\n{yn}N\nn=1 indicate the community each node belongs to, i.e.\nyn = k if node n belongs to the k-th community. Afterwards,\n{Γm}M\nm=1 were generated at random, such that Γm ∈C,\nfor all m = 1, . . . , M, and learners are better than random,\nas per As2. Then learners’ responses were generated as\nfollows: if yn = k, then the response of learner m will\nbe generated randomly according to the k-th column of its\nconfusion matrix, γm,k [cf. Sec. 2], that is fm(xn) ∼γm,k.\nFor the synthetic data tests, we set δnn′ = M. Fig. 8 shows\nthe average F-score for a synthetic dataset with K = 4 and\nM = 10 learners for varying number of data N. Here the av-\nerage degree is ¯d = 0.5. Fig. 9 shows the average confusion\nestimation error as N increases. As with sequential data,\nthe F-score of the proposed algorithms increases with N\ngrowing, and confusion matrix estimation error decreases.\nMV+ Alg. 3 quickly reaches a plateau of performance as\nMV also does not improve with increasing N. At the same\ntime Alg. 4 capitalizes on the initialization provided by\nMM. Fig. 10 shows the F-score for a similar experiment,\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, COMPILED DECEMBER 22, 2020\n11\nDataset\nK\nM\nN\nMetric\nSingle best\nMV\nDS\nAlg. 2\nAlg. 1 + Alg. 2\nMV + Alg. 1\nPOS\n12\n10\n100, 676\nPrecision\n0.2316\n0.22916\n0.23406\n0.24785\n0.25856\n0.22972\nRecall\n0.2573\n0.23518\n0.25629\n0.24396\n0.26638\n0.24497\nF-score\n0.2356\n0.22598\n0.22264\n0.23352\n0.24735\n0.23007\nNER\n9\n47\n78, 107\nPrecision\n0.9∗\n0.79\n0.77\n0.74\n0.77\n0.75\nRecall\n0.24∗\n0.59\n0.66\n0.89\n0.69\n0.66\nF-score\n0.89∗\n0.68\n0.71\n0.62\n0.72\n0.70\nBiomedical IE\n2\n120\n7, 880, 254\nPrecision\n0.94∗\n0.89\n0.81\n0.75\n0.69\n0.62\nRecall\n0.76∗\n0.45\n0.57\n0.60\n0.68\n0.74\nF-score\n0.84∗\n0.6\n0.66\n0.67\n0.68\n0.67\nTABLE 1\nResults for real data experiments with sequential data. The asterisk ∗indicates that results are from a subset of available data.\n100  \n1000 \n5000 \n10000\n50000\nN\n40\n50\n60\n70\n80\n90\nF - score\nMM\nMV\nAlg. 4\nMV + Alg. 3\nSingle best\nFig. 8. Average F-score for a synthetic networked dataset with K = 4,\nM = 10 learners and average degree ¯d = 0.5.\n100  \n1000 \n5000 \n10000\n50000\nN\n0\n0.2\n0.4\n0.6\n0.8\n1\nMM\nAlg. 4\nMV + Alg. 3\nFig. 9. Average estimation errors of confusion matrices for a synthetic\nnetworked dataset with K = 4 and M = 10 learners\nbut with network average degree ¯d = 5, i.e. higher graph\nconnectivity. Here, we observe algorithmic behavior similar\nto that of the previous experiment; however, due to the\nhigher connectivity of the graph Alg. 4 has a greater F-score\ngap to MM. This indicates that networked data with higher\nconnectivity beneﬁt more from Alg. 4 and MV+ Alg. 3.\n6.2.2\nReal data\nFurther tests were conducted on six real datasets. For the\nCora, Citeseer [23] and Pubmed [24] datasets the graph G\nand data features {xn} are provided with the dataset. In\nthese cases, M = 10 classiﬁcation algorithms from MAT-\nLAB’s machine learning toolbox were trained on different\nrandomly selected subsets of the datasets. Afterwards, these\nalgorithms provided labels for all data in the dataset. Boston\n100  \n1000 \n5000 \n10000\n50000\nN\n30\n40\n50\n60\n70\n80\n90\n100\nF-score\nMM\nMV\nAlg. 4\nMV + Alg. 3\nSingle best\nFig. 10. Average F-score for a synthetic networked dataset with K = 4,\nM = 10 learners and average degree ¯d = 5.\nUniversity’s biomedical image library (BU-BIL) [53] mag-\nnetic resonance imaging (MRI) dataset contains 35 images of\nrabbit aortas with size 25 × 24. Learner responses per pixel\nare gathered through Amazon’s mechanical turk. Here, the\nper-image graph G is the grid graph deﬁned by the pixels\nof each image, that is each pixel is connected to its adjacent\n8 pixels. For these datasets, we set δnn′ = M. The Music\ngenre and Sentence Polarity datasets [33] are crowdsourcing\ndatasets, where the features {xn} are provided and learner\nresponses F are gathered through crowdsourcing platforms.\nIn these cases, the graphs were generated from the data\nfeatures using k-nearest neighbors. Since the graphs are\ngenerated from the data features, here we set δnn′ = Mn/2,\nwhere Mn is the number of learners that have provided a\nresponse for the n-th datum.\nThe Cora, CiteSeer and Pubmed datasets are citation\nnetworks and the versions used here are preprocessed\nby [54]. The Cora dataset consists of N = 2, 708 scientiﬁc\npublications classiﬁed into K = 7 classes. The features {xn}\nof this dataset are sparse 1, 433-dimensional vectors and\nfor this dataset each classiﬁcation algorithm was trained\non a random subset of 150 instances. The CiteSeer dataset\nconsists of N = 3, 312 scientiﬁc publications classiﬁed into\none of K = 6 classes. The features {xn} of this dataset are\nsparse 3, 703-dimensional vectors, and each classiﬁcation\nalgorithm was trained on a subset of 100 instances. The\nPubmed dataset is a citation network that consists of N =\n19, 717 scientiﬁc publications from the Pubmed database\npertaining to diabetes, classiﬁed into one of K = 3 classes.\nThe features {xn} of this dataset are 500-dimensional vec-\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, COMPILED DECEMBER 22, 2020\n12\ntors, and each classiﬁcation algorithm was trained on a\nsubset of 300 instances. Targeting segmentation, pixels of\neach image in the BU-BIL are classiﬁed in K = 2 classes,\nindicating whether they belong to a biological structure or\nnot, by M = 7 human annotators. The total number of\npixels from the 35 images is N = 28, 181. The Music genre\ndataset contains N = 700 song samples (each of duration\n30secs), belonging into K = 10 music categories, annotated\nby M = 44 human annotators. The sentence polarity dataset\ncontains N = 5, 000 sentences from movie reviews, classi-\nﬁed into K = 2 categories (positive or negative), annotated\nby M = 203 human annotators.\nIn most datasets Alg. 4 exhibits the best performance in\nterms of F-score followed closely by MV+Alg. 3. For the\nMusic Genre and BU-BIL MRI datasets however, MV+Alg. 3\noutperforms Alg. 4. This is to be expected for the Music\nGenre dataset, as N is relatively small for this dataset and as\nsuch the estimated learner moments are not very accurate.\nThis can also be seen from the fact that MV outperforms\nDS. Similarly, for the BU-BIL MRI dataset MV outperforms\nDS, explaining the better performance of MV+Alg. 3. For all\ndatasets, Alg. 4 and MV+Alg. 3 consistently outperform the\nsingle best classiﬁer. Also, note that the single best learners\nfor the Music genre and sentence polarity datasets are\nevaluated only on the subsets of data for which they have\nprovided responses. In particular, the best learner for the\nMusic-genre has annotated 10 data, while the best learner\nfor the Sentence-polarity has annotated only 6 data. Another\ninteresting observation is that for most datasets having a\nrelatively large average degree ¯d, Alg. 4 and MV+Alg. 3\nhave a greater performance gap to their counterparts that\ndo not account for the structure of networked data. Similar\nto synthetic data, this suggests that well connected datasets\ncan beneﬁt more from these types of approaches. Similarly\nto the sequential case, the modest performance gains of\nAlgs. 3, and MV + 3 compared to the single best learner\nin the BU-BIL MRI dataset may be attributed to modeling\ndiscrepancies or signiﬁcantly different ability levels between\nthe learners. All in all, these results show that inclusion of\ngraph information can be beneﬁcial for the unsupervised\nensemble or crowdsourced classiﬁcation task, even when\nthe graph is noisy (as with Sentence Polarity or the Music\ngenre datasets.)\n7\nCONCLUSIONS AND FUTURE DIRECTIONS\nThis paper introduced two novel approaches to unsuper-\nvised ensemble and crowdsourced classiﬁcation in the pres-\nence of data dependencies. Two types of data dependencies\nwere investigated: i) Sequential data; and ii) Networked\ndata, where the dependencies are captured by a known\ngraph. The performance of our novel schemes was evalu-\nated on real and synthetic data.\nSeveral interesting research avenues open up: i) Dis-\ntributed and online implementations of the proposed al-\ngorithms; ii) use of contemporary tools such as variational\ninference to boost performance of the novel approaches;\niii) ensemble classiﬁcation with dependent classiﬁers and\ndependent data; iv) development of more realistic learner\nmodels for dependent data; v) extension of the proposed\nmethods to semi-supervised ensemble learning; vi) rigorous\nperformance analysis of the proposed models.\nREFERENCES\n[1]\nT. G. Dietterich, “Ensemble methods in machine learning,” in Intl.\nWorkshop on Multiple Classiﬁer Systems.\nSpringer, 2000, pp. 1–15.\n[2]\nZ.-H. Zhou, Ensemble Methods: Foundations and Algorithms, 1st ed.\nChapman & Hall/CRC, 2012.\n[3]\nL. I. Kuncheva, Combining Pattern Classiﬁers: Methods and Algo-\nrithms.\nUSA: Wiley-Interscience, 2004.\n[4]\nL. Rokach, “Ensemble-based classiﬁers,” Artif. Intell. Rev., vol. 33,\npp. 1–39, 02 2010.\n[5]\nY. Freund, R. E. Schapire et al., “Experiments with a new boosting\nalgorithm,” in Proc. of the Intl. Conf. on Machine Learning, vol. 96,\nBari, Italy, 1996, pp. 148–156.\n[6]\nD. H. Wolpert, “Stacked generalization,” Neural Networks, vol. 5,\nno. 2, pp. 241 – 259, 1992. [Online]. Available: http://www.\nsciencedirect.com/science/article/pii/S0893608005800231\n[7]\nP. Smyth and D. Wolpert, “Linearly combining density estimators\nvia stacking,” Machine Learning, vol. 36, no. 1, pp. 59–83, Jul 1999.\n[Online]. Available: https://doi.org/10.1023/A:1007511322260\n[8]\nF. Wright, C. De Vito, B. Langer, A. Hunter et al., “Multidisciplinary\ncancer conferences: A systematic review and development of\npractice standards,” European Journal of Cancer, vol. 43, pp. 1002–\n1010, 2007.\n[9]\nM. Micsinai, F. Parisi, F. Strino, P. Asp, B. D. Dynlacht, and\nY. Kluger, “Picking chip-seq peak detectors for analyzing chro-\nmatin modiﬁcation experiments,” Nucleic Acids Research, vol. 40,\nno. 9, May 2012.\n[10] A. Timmermann, “Forecast combinations,” Handbook of Economic\nForecasting, vol. 1, pp. 135–196, 2006.\n[11] D. C. Brabham, “Crowdsourcing as a model for problem solving:\nAn introduction and cases,” Convergence, vol. 14, no. 1, pp. 75–90,\n2008.\n[12] A. Kittur, E. H. Chi, and B. Suh, “Crowdsourcing user studies\nwith mechanical turk,” in Proc. of SIGCHI Conf. on Human Factors\nin Computing Systems.\nFlorence, Italy: ACM, 2008, pp. 453–456.\n[13] “Figure8,” https://www.ﬁgure-eight.com/.\n[14] A. P. Dawid and A. M. Skene, “Maximum likelihood estimation\nof observer error-rates using the EM algorithm,” Applied Statistics,\npp. 20–28, 1979.\n[15] D. R. Karger, S. Oh, and D. Shah, “Efﬁcient crowdsourcing for\nmulti-class labeling,” ACM SIGMETRICS Performance Evaluation\nReview, vol. 41, no. 1, pp. 81–92, 2013.\n[16] Y. Zhang, X. Chen, D. Zhou, and M. I. Jordan, “Spectral methods\nmeet EM: A provably optimal algorithm for crowdsourcing,” in\nAdvances in Neural Information Processing Systems, 2014, pp. 1260–\n1268.\n[17] A. Jaffe, B. Nadler, and Y. Kluger, “Estimating the accuracies of\nmultiple classiﬁers without labeled data.” in AISTATS, vol. 2, San\nDiego, CA, 2015, p. 4.\n[18] P. A. Traganitis, A. Pag`es-Zamora, and G. B. Giannakis, “Blind\nmulticlass ensemble classiﬁcation,” IEEE Transactions on Signal\nProcessing, vol. 66, no. 18, pp. 4737–4752, Sept 2018.\n[19] P. A. Traganitis, A. Pag`es-Zamora, and G. B. Giannakis, “Learning\nfrom unequally reliable blind ensembles of classiﬁers,” in Proc. of\nthe 5th IEEE Global Conference on Signal and Information Processing.\nMontreal, CA: IEEE, 2017.\n[20] P. A. Traganitis and G. B. Giannakis, “Blind multi-class ensemble\nlearning with dependent classiﬁers,” in Proc. of the 26th European\nSignal Processing Conference, Rome, Italy, Sep 2018.\n[21] N. Nguyen and Y. Guo, “Comparisons of sequence labeling\nalgorithms and extensions,” in Proceedings of the 24th International\nConference on Machine Learning,\nser.\nICML\n’07.\nNew\nYork,\nNY,\nUSA:\nACM,\n2007,\npp.\n681–688.\n[Online].\nAvailable:\nhttp://doi.acm.org/10.1145/1273496.1273582\n[22] N. D. Sidiropoulos, L. De Lathauwer, X. Fu, K. Huang, E. E.\nPapalexakis, and C. Faloutsos, “Tensor decomposition for signal\nprocessing and machine learning,” IEEE Transactions on Signal\nProcessing, vol. 65, no. 13, pp. 3551–3582, 2017.\n[23] Q. Lu and L. Getoor, “Link-based classiﬁcation,” in Proceedings of\nthe Twentieth International Conference on International Conference on\nMachine Learning, ser. ICML’03.\nAAAI Press, 2003, pp. 496–503.\n[Online]. Available: http://dl.acm.org/citation.cfm?id=3041838.\n3041901\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, COMPILED DECEMBER 22, 2020\n13\nDataset\nK\nM\nN\n¯d\nSingle best\nMV\nDS\nAlg. 4\nMV + Alg. 3\nCora\n10\n10\n2, 708\n3.9\n0.51\n0.2785\n0.4228\n0.6412\n0.336\nCiteSeer\n7\n10\n3, 312\n2.77\n0.45\n0.4257\n0.4449\n0.5244\n0.5224\nPubmed\n3\n10\n19, 717\n4.49\n0.717\n0.6968\n0.7437\n0.7667\n0.7595\nMusic Genre\n10\n44\n700\n4.8\n1∗\n0.7046\n0.4746\n0.7649\n0.8029\nSen. Polarity\n2\n203\n5, 000\n1.8\n1∗\n0.8895\n0.9129\n0.9153\n0.9139\nBU-BIL MRI\n2\n7\n28, 181\n7.1\n0.851\n0.861\n0.859\n0.86\n0.863\nTABLE 2\nF-score for Real data experiments with Networked data. The asterisk ∗indicates that results are from a subset of available data.\n[24] G. Namata, B. London, L. Getoor, and B. Huang, “Query-driven\nactive surveying for collective classiﬁcation,” in Proc. of Workshop\non Mining and Learning with Graphs, 2012.\n[25] J. M. Hammersley and P. E. Clifford, “Markov random ﬁelds on\nﬁnite graphs and lattices,” Unpublished manuscript, 1971.\n[26] N. Dalvi, A. Dasgupta, R. Kumar, and V. Rastogi, “Aggregating\ncrowdsourced binary ratings,” in Proceedings of the Intl. Conf. on\nWorld Wide Web.\nRio de Janeiro, Brazil: ACM, 2013, pp. 285–294.\n[27] T.\nBonald\nand\nR.\nCombes,\n“A\nminimax\noptimal\nalgorithm\nfor\ncrowdsourcing,”\nin\nAdvances in Neural Infor-\nmation Processing Systems 30,\nI.\nGuyon,\nU.\nV.\nLuxburg,\nS.\nBengio,\nH.\nWallach,\nR.\nFergus,\nS.\nVishwanathan,\nand\nR.\nGarnett,\nEds.\nCurran\nAssociates,\nInc.,\n2017,\npp.\n4352–4360.\n[Online].\nAvailable:\nhttp://papers.nips.cc/paper/\n7022-a-minimax-optimal-algorithm-for-crowdsourcing.pdf\n[28] F. Rodrigues, F. Pereira, and B. Ribeiro, “Sequence labeling\nwith multiple annotators,” Machine Learning, vol. 95, no. 2, pp.\n165–181, May 2014. [Online]. Available: https://doi.org/10.1007/\ns10994-013-5411-2\n[29] M. J. Beal, “Variational algorithms for approximate Bayesian\ninference,” Ph.D. dissertation, University of London, University\nCollege London (United Kingdom, 2003.\n[30] A. T. Nguyen, B. C. Wallace, J. J. Li, A. Nenkova, and M. Lease,\n“Aggregating and Predicting Sequence Labels from Crowd An-\nnotations,” Proc Conf Assoc Comput Linguist Meet, vol. 2017, pp.\n299–309, 2017.\n[31] Y. Yan, R. Rosales, G. Fung, R. Subramanian, and J. Dy,\n“Learning from multiple annotators with varying expertise,”\nMachine Learning, vol. 95, no. 3, pp. 291–327, Jun 2014. [Online].\nAvailable: https://doi.org/10.1007/s10994-013-5412-1\n[32] P. Ruiz, P. Morales- ´Alvarez, R. Molina, and A. Katsaggelos,\n“Learning from crowds with variational Gaussian processes,”\nPattern Recognition,\nvol.\n88,\npp.\n298–311,\nApril\n2019.\n[Online]. Available: http://decsai.ugr.es/vip/ﬁles/journals/1-s2.\n0-S0031320318304060-main.pdf\n[33] F. Rodrigues, F. Pereira, and B. Ribeiro, “Gaussian process\nclassiﬁcation and active learning with multiple annotators,” in\nProceedings of the 31st International Conference on Machine Learning,\nser. Proceedings of Machine Learning Research, E. P. Xing and\nT. Jebara, Eds., vol. 32, no. 2.\nBejing, China: PMLR, 22–24 Jun\n2014, pp. 433–441. [Online]. Available: http://proceedings.mlr.\npress/v32/rodrigues14.html\n[34] P. A. Traganitis, “Blind ensemble classiﬁcation of sequential data,”\nin 2019 IEEE Data Science Workshop (DSW), Minneapolis, MN, June\n2019.\n[35] S. M. Kay, Fundamentals of Statistical Signal Processing, volume I:\nEstimation Theory.\nPrentice Hall, 1993.\n[36] K. Huang, N. D. Sidiropoulos, and A. P. Liavas, “A ﬂexible\nand efﬁcient algorithmic framework for constrained matrix and\ntensor factorization,” IEEE Transactions on Signal Processing, vol. 64,\nno. 19, pp. 5052–5065, 2016.\n[37] L. R. Rabiner, “A tutorial on hidden Markov models and selected\napplications in speech recognition,” Proceedings of the IEEE, vol. 77,\nno. 2, pp. 257–286, Feb 1989.\n[38] G. D. Forney, “The Viterbi algorithm,” Proceedings of the IEEE,\nvol. 61, no. 3, pp. 268–278, March 1973.\n[39] A. Kontorovich, B. Nadler, and R. Weiss, “On learning parametric-\noutput hmms,” in Proceedings of the 30th International Conference on\nMachine Learning, Atlanta, Georgia, USA, 17–19 Jun 2013. [Online].\nAvailable: http://proceedings.mlr.press/v28/kontorovich13.html\n[40] M. Grant and S. Boyd, “CVX: Matlab software for disciplined\nconvex programming, version 2.1,” http://cvxr.com/cvx, Mar.\n2014.\n[41] V. Vapnik, The Nature of Statistical Learning Theory.\nSpringer\nScience and Business Media, 2013.\n[42] K. Huang, X. Fu, and N. D. Sidiropoulos, “Learning hidden\nMarkov models from pairwise co-occurrences with applications\nto topic modeling,” in ICML, 2018.\n[43] J. Zhang, “The mean ﬁeld theory in EM procedures for Markov\nrandom ﬁelds,” IEEE Transactions on Signal Processing, vol. 40,\nno. 10, pp. 2570–2583, Oct 1992.\n[44] J. Besag, “On the statistical analysis of dirty pictures,” Journal of\nthe Royal Statistical Society B, vol. 48, no. 3, pp. 48–259, 1986.\n[45] G. Casella and E. I. George, “Explaining the Gibbs sampler,” The\nAmerican Statistician, vol. 46, no. 3, pp. 167–174, 1992.\n[46] D. M. W. Powers, “Evaluation: from precision, recall and f-\nmeasure to roc, informedness, markedness and correlation,” Intl.\nJournal of Machine Learning Technology, vol. 2, pp. 37–63, 2011.\n[47] MATLAB, version 8.6.0 (R2015b).\nNatick, Massachusetts: The\nMathWorks Inc., 2015.\n[48] E. Loper and S. Bird, “Nltk: The natural language toolkit,”\nin Proceedings of the ACL-02 Workshop on Effective Tools and Method-\nologies for Teaching Natural Language Processing and Computational\nLinguistics - Volume 1, ser. ETMTNLP ’02.\nStroudsburg, PA,\nUSA: Association for Computational Linguistics, 2002, pp. 63–70.\n[Online]. Available: https://doi.org/10.3115/1118108.1118117\n[49] W.\nN.\nFrancis\nand\nH.\nKucera,\n“Brown\ncorpus\nmanual,”\nDepartment\nof\nLinguistics,\nBrown\nUniversity,\nProvidence,\nRhode\nIsland,\nUS,\nTech.\nRep.,\n1979.\n[Online].\nAvailable:\nhttp://icame.uib.no/brown/bcm.html\n[50] M.\nMarcus,\nG.\nKim,\nM.\nA.\nMarcinkiewicz,\nR.\nMacIntyre,\nA. Bies, M. Ferguson, K. Katz, and B. Schasberger, “The\npenn\ntreebank:\nAnnotating\npredicate\nargument\nstructure,”\nin Proceedings of the Workshop on Human Language Technology, ser.\nHLT ’94.\nStroudsburg, PA, USA: Association for Computational\nLinguistics,\n1994,\npp.\n114–119.\n[Online].\nAvailable:\nhttps:\n//doi.org/10.3115/1075812.1075835\n[51] E. F. Tjong Kim Sang and F. De Meulder, “Introduction to\nthe CoNLL-2003 shared task: Language-independent named\nentity\nrecognition,”\nin\nProceedings of the Seventh Conference on\nNatural Language Learning at HLT-NAACL 2003, 2003, pp. 142–\n147. [Online]. Available: https://www.aclweb.org/anthology/\nW03-0419\n[52] P. W. Holland, K. B. Laskey, and S. Leinhardt, “Stochastic\nblockmodels: First steps,” Social Networks, vol. 5, no. 2, pp. 109\n– 137, 1983. [Online]. Available: http://www.sciencedirect.com/\nscience/article/pii/0378873383900217\n[53] D. Gurari, D. Theriault, M. Sameki, B. Isenberg, T. A. Pham,\nA. Purwada, P. Solski, M. Walker, C. Zhang, J. Y. Wong, and\nM. Betke, “How to collect segmentations for biomedical images? a\nbenchmark evaluating the performance of experts, crowdsourced\nnon-experts, and algorithms,” in 2015 IEEE Winter Conference on\nApplications of Computer Vision, 2015, pp. 1169–1176.\n[54] D. Berberidis, A. N. Nikolakopoulos, and G. B. Giannakis, “Adap-\ntive diffusions for scalable learning over graphs,” IEEE Trans. Sig.\nProc., pp. 1–15, 2019.\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, COMPILED DECEMBER 22, 2020\n14\nAPPENDIX A\nTHE FORWARD-BACKWARD ALGORITHM\nLet bn,k denote the probability of observing {fm(xn)}M\nm=1\ngiven that yn = k, that is\nbn,k =\nM\nY\nm=1\nPr(fm(xn)|yn = k) =\nM\nY\nm=1\nΓm(fm(xn), k).\n(39)\nThe forward-backward algorithm [37] seeks to efﬁciently\nobtain the probability of the observed variable sequence\n{fm(xn)}N,M\nn=1,m=1, given current HMM parameter estimates\nθ. It takes advantage of the fact that the past and future\nstates of a Markov chain are independent given the current\nstate. Tailored for our ensemble HMM, we have\nPr(F|θ) =\nK\nX\nk=1\nPr(F1:n, yn = k; θ) Pr(Fn+1:N|yn = k; θ),\n(40)\nwhere F1:n is a matrix collecting all learner responses for\nn′ = 1, . . . , n, and Fn+1:N is a matrix collecting learner\nresponses for n′ = n + 1, . . . , N.\nThe forward backward algorithm computes the proba-\nbility of the observed sequence iteratively using so-called\nforward and backward variables. Deﬁne the forward vari-\nable as\nαn,k = Pr(F1:n, yn = k; θ)\n(41)\nThen let\nα1,k = Pr(y1 = k)b1,k\nfor\nk = 1, . . . , K.\n(42)\nand for n = 1, . . . , N\nαn+1,k = bn+1,k\nK\nX\nk′=1\nαn,k′T(k, k′) .\n(43)\nUpon deﬁning the backward variables as\nβn,k = Pr(Fn+1:N|yn = k; θ)\n(44)\nβN,k = 1\nfor k = 1, . . . , K\n(45)\nit holds for n = N −1, . . . , 1 that\nβn,k =\nK\nX\nk′=1\nT(k, k′)βn+1,k′bn+1,k′.\n(46)\nAll forward and backward variables can be computed itera-\ntively using (43) and (46). Having computed all forward and\nbackward variables, the probability of the observed variable\nsequence is given by\nPr(F|θ) =\nK\nX\nk=1\nαn,kβn,k.\n(47)\nwhich holds for any n ∈{1, . . . , N}. Then the variables of\ninterest, qnk and ξn(k, k′), can be obtained as\nqnk = Pr(yn = k|F, θ) =\nαn,kβn,k\nPK\nk′=1 αn,k′βn,k′ ,\n(48)\nξn(k, k′) = Pr(yn = k, yn+1 = k′|F, θ)\n(49)\n=\nαn,kT(k, k′)bn+1,k′βn+1,k′\nPK\nk′,k′′=1 αn,k′T(k′, k\n′′)bn+1,k′′ βn+1,k′′ .\n",
  "categories": [
    "cs.LG",
    "eess.SP",
    "stat.ML"
  ],
  "published": "2019-06-22",
  "updated": "2020-12-20"
}