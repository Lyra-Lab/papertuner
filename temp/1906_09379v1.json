{
  "id": "http://arxiv.org/abs/1906.09379v1",
  "title": "Evaluating Computational Language Models with Scaling Properties of Natural Language",
  "authors": [
    "Shuntaro Takahashi",
    "Kumiko Tanaka-Ishii"
  ],
  "abstract": "In this article, we evaluate computational models of natural language with\nrespect to the universal statistical behaviors of natural language. Statistical\nmechanical analyses have revealed that natural language text is characterized\nby scaling properties, which quantify the global structure in the vocabulary\npopulation and the long memory of a text. We study whether five scaling\nproperties (given by Zipf's law, Heaps' law, Ebeling's method, Taylor's law,\nand long-range correlation analysis) can serve for evaluation of computational\nmodels. Specifically, we test $n$-gram language models, a probabilistic\ncontext-free grammar (PCFG), language models based on Simon/Pitman-Yor\nprocesses, neural language models, and generative adversarial networks (GANs)\nfor text generation. Our analysis reveals that language models based on\nrecurrent neural networks (RNNs) with a gating mechanism (i.e., long short-term\nmemory, LSTM; a gated recurrent unit, GRU; and quasi-recurrent neural networks,\nQRNNs) are the only computational models that can reproduce the long memory\nbehavior of natural language. Furthermore, through comparison with recently\nproposed model-based evaluation methods, we find that the exponent of Taylor's\nlaw is a good indicator of model quality.",
  "text": "Evaluating Computational Language Models\nwith Scaling Properties of Natural Language\nShuntaro Takahashi ∗E-mail:\ntakahashi@cl.rcast.u-tokyo.ac.jp\nDepartment of Advanced\nInterdisciplinary Studies,\nGraduate School of Engineering,\nThe University of Tokyo\nKumiko Tanaka-Ishii ∗E-mail:\nkumiko@cl.rcast.u-tokyo.ac.jp\nResearch Center for Advanced Science\nand Technology,\nThe University of Tokyo\nIn this article, we evaluate computational models of natural language with respect to the univer-\nsal statistical behaviors of natural language. Statistical mechanical analyses have revealed that\nnatural language text is characterized by scaling properties, which quantify the global structure\nin the vocabulary population and the long memory of a text. We study whether ﬁve scaling\nproperties (given by Zipf’s law, Heaps’ law, Ebeling’s method, Taylor’s law, and long-range\ncorrelation analysis) can serve for evaluation of computational models. Speciﬁcally, we test n-\ngram language models, a probabilistic context-free grammar (PCFG), language models based\non Simon/Pitman-Yor processes, neural language models, and generative adversarial networks\n(GANs) for text generation. Our analysis reveals that language models based on recurrent\nneural networks (RNNs) with a gating mechanism (i.e., long short-term memory, LSTM; a gated\nrecurrent unit, GRU; and quasi-recurrent neural networks, QRNNs) are the only computational\nmodels that can reproduce the long memory behavior of natural language. Furthermore, through\ncomparison with recently proposed model-based evaluation methods, we ﬁnd that the exponent\nof Taylor’s law is a good indicator of model quality.\n1. Introduction\nThe question of evaluation methods for computational models of natural language is\nfundamental in language engineering. Aside from human rating, current evaluation\nmethods rely on the probability distribution produced by the model, or on the n-gram\nsimilarity between the generated text and a corresponding reference written by human\nexperts. The representative metric of the former type is perplexity. Perplexity quantiﬁes\nthe prediction accuracy of a language model and thus requires its probability distribu-\ntion. The latter category includes the metrics BLEU (Papineni et al. 2002) and ROUGE\n(Lin 2004). These evaluation methods compute the n-gram co-occurrence between the\ngenerated text and a reference. Hence, the above methods are reasonable for cases in\nwhich either the probability distribution of the computational model is explicit and\ncomparable or a corresponding reference is given.\nThe emergence of intractable models such as generative adversarial networks\n(GANs) for text generation has revealed the limitation of these conventional evaluation\nmethods. Tentative studies (Yu et al. 2017; Lin et al. 2017; Guo et al. 2018; Rajeswar et al.\n∗4-6-1 Komaba, Meguro-ku, Tokyo 153-8904, Japan.\n© 2005 Association for Computational Linguistics\narXiv:1906.09379v1  [cs.CL]  22 Jun 2019\nComputational Linguistics\nVolume xx, Number xx\n2017; Lu et al. 2018) have sought to generate natural language text in the adversarial\nlearning framework. Because these models do not explicitly output the probability\ndistribution for prediction, they are evaluated by feeding the generated text to other\nmodels, such as a neural language model (Fedus, Goodfellow, and Dai 2018) or a prob-\nabilistic context-free grammar (PCFG) (Rajeswar et al. 2017). Although those proposals\nare promising and worth considering, the effectiveness of the methods for evaluation\nhas not been thoroughly investigated. As an alternative to those approaches, in this\narticle, we test evaluation with the scaling properties of natural language text.\nThe scaling properties of natural language are the universal statistical behaviors\nobserved in natural language text. For example, Zipf’s law characterizes the vocabu-\nlary population with a power-law function for the rank-frequency distribution. Recent\nstatistical mechanical studies (Ebeling and Neiman 1995; Altmann, Pierrehumbert, and\nMotter 2009; Tanaka-Ishii and Bunde 2016; Kobayashi and Tanaka-Ishii 2018; Tanaka-\nIshii and Kobayashi 2018) revealed another statistical aspect of natural language, long\nmemory. This refers to the way that sequences of characters or words in natural lan-\nguage universally exhibit clustering, bursty behavior. In particular, results using Tay-\nlor’s law (Kobayashi and Tanaka-Ishii 2018; Tanaka-Ishii and Kobayashi 2018) show that\na natural language text has a consistent range for the Taylor exponent, which quantiﬁes\nthe degree of burstiness in the text.\nAs the results obtained with scaling properties have clear interpretations, they\nsuggest qualitative implications for language models. For example, evaluation with\nZipf’s law examines whether a model can properly produce infrequent words. Similarly,\nevaluation with Taylor’s law quantiﬁes whether a model can learn the long memory in\na natural language text. In this article, we show that, among the computational models,\nonly neural language models based on recurrent neural networks (RNNs) with a gating\nmechanism can learn and reproduce the long memory of natural language text. None of\nthe other models can reproduce this behavior. In addition, our study demonstrates the\ncapabilities of the scaling properties for evaluating language models.\nThe rest of the article is organized as follows. In §2, we review the evaluation metrics\nthat have been widely used for tasks in natural language processing. In §3, we introduce\nthe scaling properties of natural language: those given by Zipf’s law, Heaps’ law,\nEbeling’s method, Taylor’s law, and long-range correlation analysis. We also explain\nthe methods of applying these scaling properties to evaluate computational models.\nIn §4, we provide a summary of the models of natural language considered in this\narticle. Speciﬁcally, our work covers n-gram language models, mathematical language\nmodels based on the Simon and Pitman-Yor processes, grammatical models, and neural\nlanguage models. The experimental procedure and settings are explained in §5. In §6,\nwe assess the scaling properties as evaluation metrics and compare them with other\nmetrics using a PCFG and neural language models. In §7, we use the scaling properties\nto evaluate the models of natural language and discuss the implications of the results.\n§8 discusses evaluation of GAN models for text generation. Finally, we conclude our\nwork with a summary in §9.\nNote that we describe all computational models of natural language considered\nin this article, as introduced in §4, by the term language model. For some readers this\nmight sound inadequate, because some of these models do not actually form a model to\npredict subsequent words, e.g., a PCFG and the models based on the Simon and Pitman-\nYor processes. Because the term computational models of natural language is long, however,\nfor the sake of brevity we simply use the term language models.\n2\nTakahashi and Tanaka-Ishii\nComputational Linguistics\n2. Previous Evaluation Metrics\nThere are two major approaches to evaluate a language model:\n•\ndirectly inspecting some subpart of the model, or\n•\nverifying the output generated by the model.\nThis section summarizes previous methods of evaluating models from these two view-\npoints, with §2.1 and §2.2 corresponding to the ﬁrst and second approaches, respec-\ntively, and §2.3 considering both. As clariﬁed in §3, our proposal belongs to the second\ncategory.\n2.1 Evaluation using probability distribution: perplexity\nA standard evaluation metric for language models such as n-gram and neural language\nmodels is the perplexity (Manning and Schutze 1999), which is a measure of the pre-\ndiction accuracy. Given a test sample x1, . . . , xN of length N and a language model\nthat predicts the probability of words, denoted as q(xi), the perplexity is deﬁned as the\nnumber e to the power of the average log probability of the correct prediction for every\nword:\nperplexity = e−1\nN\nPN\ni=1 log q(xi).\n(1)\nPerplexity is usually applied to predict the successive token xi given a context of\nlength k, namely, xi−k, . . . , xi−1. The probability distribution q(xi) for prediction must\nbe explicit for evaluation with the perplexity. Moreover, to compare models by using\nthe perplexity, the probability distribution must be deﬁned in a comparable manner.\nFor example, n-gram language models and neural language models are comparable, as\nthey predict the next word from the context.\nBecause perplexity is the current standard metric for automatic evaluation of model\nquality, the other metrics appearing in this article are compared with the perplexity.\n2.2 Evaluation using reference: BLEU/ROUGE\nAnother popular evaluation metric is the n-gram co-occurrence-based approach, includ-\ning BLEU (Papineni et al. 2002) and ROUGE (Lin 2004). These metrics are widely used\nin paired-corpus-oriented tasks such as machine translation and automatic summariza-\ntion. They evaluate by using statistics of the counts of the same n-grams appearing in\nthe machine-generated text and a corresponding reference, which is a correct answer\nwritten by an expert.\nThese approaches only use the output of a model and thus do not require access\nto any of its internal elements. Because they require the corresponding reference for\ncomputing the n-gram co-occurrence, however, their utility is limited to paired-corpus\ntasks.\nAs intractable models such as GANs for text generation cannot have an explicit\nreference, the application of BLEU or ROUGE to those models is not trivial. A series of\nGAN studies (Yu et al. 2017; Lin et al. 2017; Guo et al. 2018; Lu et al. 2018) quantitatively\nmeasured the quality of the generated text with BLEU by regarding the whole training\ndataset as a reference. The validity of this evaluation method remains questionable, as\n3\nComputational Linguistics\nVolume xx, Number xx\nBLEU was designed for comparison between a pair of a machine-generated text and\nits correct reference. Zhu et al. (2018) reported that the application of BLEU with this\napproach does not provide consistent results with different n-grams chosen.\n2.3 Evaluation using other language models\nOne approach for evaluation without using either a model distribution or a reference is\nthe use of language models, i.e., evaluation of language models by using other language\nmodels. Fedus, Goodfellow, and Dai (2018) proposed to evaluate GAN-generated text\nwith a neural language model trained with the same natural language dataset. This di-\nrection is promising, if the language model is a reliable model of natural language. Even\nwith state-of-the-art neural language models, however, the model quality is limited.\nThe use of a clear, transparent model for evaluation, such as an n-gram language\nmodel, would also be a possible method. That approach, however, could only measure\nmodels of the n-gram structures of natural language and would thus be similar to BLEU\nevaluation. The use of a PCFG is another possible method of evaluation without a ref-\nerence. A PCFG is constructed using a parsed corpus such as the Penn Treebank (PTB),\nand the generated text is parsed with the Viterbi algorithm (Forney 1973). The algorithm\ncomputes the log-likelihood of the text. The PCFG is expected to output a small negative\nlog-likelihood for a grammatically correct sentence. As we demonstrate later, however,\nit is doubtful that a PCFG could meaningfully evaluate the grammaticality of a sentence.\n3. Scaling Properties of Natural Language for Evaluation\nIn this section, we explain scaling properties, the statistical properties of natural lan-\nguage text that have a power-law form. One study on the statistics of natural language\nreported nine scaling laws (Altmann and Gerlach 2017). Four of them concern word for-\nmation and a network structure, which do not directly relate to language modeling. This\nleaves ﬁve scaling properties, which can be categorized into those for the vocabulary\npopulation and those for long memory. These properties are characterized by power-\nlaw functions, which involve a power exponent. The exponents of the scaling properties\nhave the capability to characterize the degree of each property. They therefore serve to\nevaluate whether a language model has the same behavior as natural language text.\nSpeciﬁcally, given a text generated by a language model, we set two levels of assessment\nfor evaluation:\nQ1 Does the scaling property hold qualitatively?\nQ2 How does the exponent differ from that of the training data?\nAs revealed in the following sections, many models fail to satisfy even the ﬁrst criterion,\nespecially for long memory. For those models that do satisfy Q1, their exponents can be\ncompared with those of the original text.\nHence, we propose the exponents of scaling properties as metrics to evaluate\nmachine-generated text. Consider a power-law relation y ∝zκ for points (y1,z1),. . .,\n(yN,zN). These points (yi,zi) are calculated for any given text. Let c be the coefﬁcient\nof the power law, and then the exponent κ is estimated by the least-squares method:\nˆκ, ˆc = arg min\nκ,c ε(κ, c),\n(2)\n4\nTakahashi and Tanaka-Ishii\nComputational Linguistics\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Ebeling’s Method\n(d) Taylor’s Law\n(e) Long-Range Correlation\nFigure 1\nScaling properties of the WT2 dataset. (a) Zipf’s law: the rank-frequency distributions of words\n(red) and word pairs (blue). (b) Heaps’ law: the growth of vocabulary size with text length. The\nsolid line is a power-law ﬁtting, and the dashed line represents a power law with exponent\nα = 1.0, meaning that all words in a sequence are unique. (c) Ebeling’s method: ﬂuctuation\nanalysis of character occurrence. (d) Taylor’s law: mean-variance relation of word occurrence. (e)\nLong-range correlation: temporal correlation of the sequence of the return intervals of rare\nwords. All data points of these ﬁve scaling properties are plotted in a log-log scale.\nε(κ, c) ≡\nv\nu\nu\nt\nN\nX\ni=1\n(log yi −log czκ\ni )2/N.\n(3)\nThe data points are regressed on a log-log scale. The regression method could\nbe a problem if the errors between the data points and the ﬁtting function are not\nGaussian distributed. There are other proposed regression methods such as maximum\nlikelihood estimation for Zipf’s law (Clauset, Shalizi, and Newman 2009; Gerlach and\nAltmann 2013). In this article, however, because exponents obtained with the least-\nsquares method are effective in distinguishing machine-generated text from natural\nlanguage text, and because this method has been a conventional standard, we adopt\nit for estimation.\nThe following subsections introduce the ﬁve scaling properties: Zipf’s law, Heaps’\nlaw, Ebeling’s method, Taylor’s law, and a long-range correlation method. As an ex-\nample, Figure 1 shows a visual presentation of these methods for the wikitext-2 (WT2)\ndataset (Merity et al. 2016). WT2 is a collected corpus of well-written Wikipedia articles,\npreprocessed by replacing rare words having frequencies under a certain threshold with\na meta symbol, <unk>. The details of the dataset appear in the ﬁrst row of Table 1, further\nahead in §3.3.\n5\nComputational Linguistics\nVolume xx, Number xx\n3.1 Vocabulary population\n3.1.1 Zipf’s law. Let r be the rank of a particular word type and f(r) be its frequency.\nThe well-known Zipf’s law formulates a power-law relation between the frequency and\nthe rank:\nf(r) ∝r−α,\n(4)\nwith α ≈1.0. This scaling behavior generally holds not only for unigrams but also for\nlarger n-grams, with smaller exponent values. Figure 1(a) shows Zipf distributions for\nWT2, with unigrams in red and bigrams in blue. Because WT2 replaces rare words, as\nmentioned before, the tail of the unigram distribution disappears. The Zipf distributions\nfor unigrams and bigrams typically intersect in the middle of the plots. In practice,\nthe plot is not often aligned linearly in a log-log scale, which makes estimation of the\nexponent α difﬁcult. While previous works have dealt with this problem, it is a sensitive\ntopic and is beyond the scope of this article. We therefore do not estimate α but instead\nobserve the distribution qualitatively.\n3.1.2 Heaps’ law. Heaps’ law describes how the vocabulary size grows with the text size\nfollowing a power-law function. Let n be the length of a text and v(n) be its vocabulary\nsize. Then Heaps’ law is formulated as the following relation:\nv(n) ∝nβ, 0 < β < 1.\n(5)\nFigure 1(b) shows the text sizes and corresponding vocabulary sizes for WT2. The\nexponent β is 0.75 with error ε = 0.13, which is smaller than β = 1.0 (represented\nby the dashed black line). There have been multiple debates on how Heaps’ law is\nmathematically related to Zipf’s law (Baeza-Yates and Navarro 2000; van Leijenhorst\nand van der Weide 2005; Lü, Zhang, and Zhou 2010).\n3.2 Long memory\nThe statistical mechanics domain has introduced two approaches for quantifying long\nmemory in a time series: ﬂuctuation analysis and the long-range correlation method.\nWe introduce two ﬂuctuation analysis methods, one for characters and one for words,\nand one long-range correlation method, applied to words. Although these methods are\nrelated analytically for a well-formed time series (Eisler, Bartos, and Kertész 2007), the\nrelation is nontrivial for real phenomena.\n3.2.1 Ebeling’s method. Ebeling’s method (Ebeling and Neiman 1995) analyzes the\npower-law relation between the lengths of subsequences of a text and the variance of\nthe number of characters in the subsequences. Given a set of elements (characters in\nthis method), W, let y(c, l) be the counts of character c within subsequences of the text\nof length l. Then, the ﬂuctuation function m(l) is deﬁned as\nm(l) =\nX\nc∈W\nm2(c, l) ∝lη,\n(6)\nwhere m2(c, l) is the variance of the counts y(c, l):\nm2(c, l) =< y2(c, l) > −(< y(c, l) >)2.\n(7)\n6\nTakahashi and Tanaka-Ishii\nComputational Linguistics\nTheoretically, if a time series is independent and identically distributed (i.i.d.), then η =\n1.0, in general, and η > 1.0 if a time series has long memory. (Ebeling and Neiman 1995)\nreported that the character sequence of the Bible has exponent η = 1.67, indicating the\npresence of clustering behavior at the character level. Following the original work, we\napply this method at the character level. Figure 1(c) shows the ﬂuctuation analysis m(l)\nfor WT2. The exponent is η = 1.32 with error ε = 0.10.\n3.2.2 Taylor’s law. Taylor’s law was originally reported in two pioneering works (Smith\n1938; Taylor 1961) and has been applied in various domains (Eisler, Bartos, and Kertész\n2007). It describes the power-law relation between the mean and the variance in spa-\ntiotemporal observations. In this article, we apply Taylor’s law for natural language text\nas proposed by Kobayashi and Tanaka-Ishii (2018); Tanaka-Ishii and Kobayashi (2018).\nGiven a text with a set of words, W, for a given segment size l the number of\noccurrences of a particular word w ∈W is counted, and the mean µw and standard\ndeviation σw are calculated. We thus obtain a scatter plot of µ and σ for all elements\nof W. Taylor’s law states the following power-law relation between σ and µ with the\nTaylor exponent ζ:\nσ ∝µζ.\n(8)\nFigure 1(d) shows the Taylor’s law plot for WT2 with l = 5620 (l can be any value\nlarger than one). The scatter plot generally follows a power-law function with exponent\nζ = 0.62 and has some deviation from the regression line, with error ε = 0.15.\nThe Taylor exponent takes the range of values 0.50 ≤ζ ≤1.00, and the two limit\nvalues ζ = 0.50, 1.0 have clear interpretations. For an i.i.d. process, it is proved that ζ =\n0.50. On the other hand, one case with ζ = 1.0 occurs when all segments of length l\ncontain the elements of W with the same proportions. For example, given W = {a, b},\nsuppose that b always occurs twice as often as a in all segments (e.g., one segment with\nthree a and six b, another segment with one a and two b, etc.). Then, both the mean and\nstandard deviation for b are twice those for a, and thus ζ = 1.0. Therefore, the Taylor\nexponent quantiﬁes how consistently words co-occur in a text. The Taylor exponent of\na natural language text typically has a range of 0.55 ≤ζ ≤0.65 and never takes ζ =\n0.50 (which would indicate no long memory). It takes different ranges of values for\ndifferent types of sequences (e.g., child-directed speech and programming source code).\nIt is therefore expected to have the capability to evaluate machine-generated text.\nEbeling’s method and Taylor’s law analysis have the following two differences.\nFirst, Ebeling’s method analyzes the growth of the variance m(l) with respect to the\nlength of the subsequences, l, whereas Taylor’s law analyzes the variance with respect to\nthe mean frequency within a ﬁxed subsequence length. Second, to acquire an exponent\nfor a text, Ebeling’s method takes the sum of the variances over all symbols, whereas\nTaylor’s law obtains the exponent from the individual points for all words.\nFor the latter reason, Ebeling’s method is inﬂuenced by a small number of fre-\nquently appearing symbols. Because it involves the sum of the variances of all words\nthat follow the power law, the behavior of the exponent η often tends to be less sensible\nthan that of the Taylor exponent.\n3.2.3 Long-range correlation. Long-range correlation analysis quantiﬁes the burstiness\nof word occurrence in a natural language text. The analysis measures the degree of\nself-similarity within a sequence. Among such analyses, early works proposed mutual-\ninformation-based methods (Li 1989; Ebeling and Pöschel 1994; Lin and Tegmark 2017).\n7\nComputational Linguistics\nVolume xx, Number xx\nSuch methods compute the mutual information between characters separated by s\ncharacters. These works reported that the mutual information decays according to a\npower law with the distance s. Takahashi and Tanaka-Ishii (2017) showed, however, that\nthe mutual information method cannot quantify the long-range dependence in word\nsequences. Moreover, the mutual information between characters decays quickly and\nreaches a plateau at a distance s ≈101 for natural language texts such as the collected\nworks of Shakespeare and the PTB dataset.\nAnother approach to long-range correlation analysis is the use of the autocorrelation\nfunction (ACF). The ACF c(s) is deﬁned as the Pearson correlation for two elements of\na sequence separated by a distance s:\nc(s) = E[(xt −µ)(xt+s −µ)]\nσ2\n,\n(9)\nwhere µ and σ are the respective mean and standard deviation of the time series xt. The\nvalue of c(s) ranges between -1 and 1. A time series is said to be long-range correlated\nif the ACF c(s) for two elements separated by distance s follows a power law:\nc(s) ∝s−ξ, s > 0, 0 < ξ < 1.\n(10)\nIn the case of application to real-world data, a sequence is said to be long-range corre-\nlated if c(s) takes positive values for s until about 1/100 of the length (Lennartz and\nBunde 2009). For sequences without correlation, c(s) ﬂuctuates around zero.\nBecause the ACF is applicable only for numerical time series, the application of this\nmethod for natural language text requires transformation of the sequence of symbols\ninto a numerical time series. Recent methods do so by considering the intervals of\nword occurrences (Tanaka-Ishii and Bunde 2016). In this article, we apply a method that\nmeasures the ACF of a sequence of the return intervals of rare words, which amounts\nto 1\nQ of the text length. With this method, Tanaka-Ishii and Bunde (2016) reported that\npower-law decay of the ACF is observed for natural language texts.\nFigure 1(e) shows the long-range correlation analysis of word sequences in WT2.\nThe hyperparameter was set to Q = 16 for all results in this article. As seen in the\nﬁgure, the ACF c(s) always takes positive values up to 1/100 of the sequence length\nand follows a power-law function (i.e., a straight line in a log-log plot) with exponent\nξ = 0.33 and error ε = 0.04. Throughout this article, the error ε of this metric is only\nmeasured for s ≤100.\n3.3 Examples of scaling properties for other natural language texts\nExcept for Zipf’s and Heaps’ laws, the scaling properties have hardly appeared in the\ncontext of computational linguistics or language engineering. This may be due to the\nfact that these properties do not directly incorporate semantics or syntax, which are of\ncentral concern in those domains. Instead, the properties quantify the universal struc-\ntures behind natural language in a statistical sense. Those introduced so far are robust\nand apply to texts across different genres and languages as long as the text is sufﬁciently\nlong. Figure 2 shows the scaling properties of another language modeling dataset, the\nPTB. This text also satisﬁes all ﬁve scaling properties. They are indeed universal with\nrespect to the genre or even language. More results are shown in Appendix A. Figure A1\nshows the scaling properties of the collected works of Shakespeare, and their exponents\n8\nTakahashi and Tanaka-Ishii\nComputational Linguistics\nTable 1\nSummary of the datasets used in this article and their statistics.\nTokens\nVocab\nVocabulary Population\nLong Memory\n-ulary\nZipf’s\nHeaps’\nEbeling’s\nTaylor’s\nLong Range\nLaw\nLaw\nMethod\nLaw\nCorrelation\nf(r) ∝r−α\nv(n) ∝nβ\nm(l) ∝lη\nσ ∝µζ\nc(s) ∝s−ξ\nWikitext-2 (English, Wikipedia article)\npreprocessed dataset\n2,088,628\n33,278\nYes\n0.75 (0.13)\n1.33 (0.10)\n0.62 (0.15)\n0.33 (0.04)\noriginal dataset\n2,088,628\n76,617\nYes\n0.78 (0.09)\n1.33 (0.10)\n0.65 (0.11)\n0.32 (0.03)\nPenn Treebank (English, The Wall Street Journal news article)\npreprocessed dataset\n887,521\n10,000\nYes\n0.70 (0.16)\n1.23 (0.06)\n0.56 (0.14)\n0.81 (0.24)\noriginal dataset\n892,008\n89,317\nYes\n0.83 (0.07)\n1.20 (0.05)\n0.57 (0.06)\n0.60 (0.16)\nShakespeare (old English collection of literature works)\noriginal text\n740,706\n83,105\nYes\n0.79 (0.07)\n1.24 (0.09)\n0.59 (0.05)\n0.13 (0.02)\nHong Lou Meng (Chinese, literature work)\noriginal text\n703,034\n18,312\nYes\n0.74 (0.14)\n1.31 (0.07)\n0.58 (0.07)\n0.39 (0.04)\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Ebeling’s Method\n(d) Taylor’s Law\n(e) Long-Range Correlation\nFigure 2\nScaling properties of the Penn Treebank (preprocessed)\nare listed in the third block of Table 1. Likewise, the scaling properties and exponents\nfor Hong Lou Meng, a Chinese literary work, are shown in Figure A2 and listed in\nthe last block of Table 1, respectively. Among the exponents, that of the long-range\ncorrelation, ξ, differs largely among the four datasets considered thus far. In contrast,\nthe other exponents generally take similar values for the datasets.\n4. Computational Models of Natural Language\nThis section introduces the computational models of natural language tested in this ar-\nticle. We test four categories of language models: n-gram models, grammatical models,\n9\nComputational Linguistics\nVolume xx, Number xx\nlanguage models based on the Simon or Pitman-Yor process, and neural language mod-\nels. These categories cover the different genres of language models that have appeared\nin the history of computational linguistics. For every category, some sophisticated,\nadvanced models have been proposed. The experiments reported in §6 and §7, however,\nwere conducted only with the most recent models whose code was available, except for\nthe n-gram models. This served to avoid errors in reimplementation.\n4.1 N-gram models\nAn n-gram language model is the most basic model, as it is an n −1-ordered Markov\nmodel. Let c(Xt\n1) be the count of Xt\n1 = x1, x2, . . . , xt, and then the probability of element\nxt is calculated as\nP(xt+1|Xt\n1) ≈P(xt+1|Xt\nt−n+1) =\nc(Xt+1\nt−n+1)\nc(Xt\nt−n+1) .\n(11)\nThis article examines 3-gram and 5-gram models. Other than the original n-gram\nmodel, we also test models with a variety of smoothing techniques to improve the\nperplexity. In particular, linear interpolation (Stolcke 2002), Katz backoff (Katz 1987),\nand Kneser-Ney smoothing (Kneser and Ney 1995) have been known to enhance the\nperformance of n-gram models. We also set n = 3 and n = 5 for these models to compare\nwith the original n-gram models. It has been empirically veriﬁed that longer context\ndoes not necessarily contribute to improving the perplexity and can even degrade\nperformance (Chen and Goodman 1999). Simple n-gram models, in fact, have been\nmathematically shown to be incapable of reproducing long memory (Kingman 1963;\nLin and Tegmark 2017).\n4.2 Grammatical models\nThe PCFG is a basic grammatical model. We constructed this grammar model with\nthe annotated PTB dataset and used the Natural Language Toolkit (NLTK) (Loper and\nBird 2002) to generate sentences according to the probabilities assigned to productions.\nUnlike an n-gram model, the PCFG generates a text by using a tree.\n4.3 Language models based on Simon/Pitman-Yor processes\nThe Simon and Pitman-Yor processes are abstract models of natural language that\nreproduce Zipf’s law and Heaps’ law. These are generative models, and a sequence\nis formulated over time, either through (1) introduction of new words or (2) sampling\nfrom the past sequence. Let K(Xt\n1) be the number of word types existing in Xt\n1, and let\nnk(Xt\n1) be the frequency of the kth word type in Xt\n1. The sequence starts with K(X0) = 1\nand X0 = x0 at t = 0. For t ≥1, given a constant a with 0 < a < 1, the Simon process\n(Simon 1955) introduces a new word with probability a, or a word is sampled from Xt\n1\nwith probability 1 −a:\nP(xt+1 = wk) =\n(\n(1 −a) nk(Xt\n1)\nt\n1 ≤k ≤K(Xt\n1)\na\nk = K(Xt\n1) + 1\n.\nThe Simon process strictly follows Zipf’s law with exponent α = 1.0 and conse-\nquently Heaps’ law, as well. In contrast, the Pitman-Yor process copes with this problem\n10\nTakahashi and Tanaka-Ishii\nComputational Linguistics\nby decreasing the introduction rate of new words in proportion to K(Xt\n1) via another\nparameter b, with 0 ≤a < 1 and 0 ≤b:\nP(xt+1 = wk) =\n\n\n\nnk(Xt\n1) −a\nt + b\n1 ≤k ≤K(Xt\n1)\naK(Xt\n1) + b\nt + b\nk = K(Xt\n1) + 1\n.\nThese two parameters serve to produce Zipf’s law with slightly convex behavior\n(Goldwater, Grifﬁths, and Johnson 2011). The basic models introduced to this point\ndeﬁne nothing about how to introduce words: we could simply generate random se-\nquences and examine their scaling properties, because the basic formulations thus far\ngovern the nature of the language models elaborated from these basic models.\nBy mapping words to the elements produced, we would generate a language model,\nlike the two-stage model proposed in Goldwater, Grifﬁths, and Johnson (2011). Here,\nwe consider a more advanced model proposed as the hierarchical Pitman-Yor language\nmodel (HPYLM) (Teh 2006), which integrates the Pitman-Yor process into an n-gram\nmodel. 1\n4.4 Neural language models\nState-of-the-art neural language models are known to outperform n-gram language\nmodels by the measure of perplexity. The majority of promising neural language models\n(Mikolov and Zweig 2012; Melis, Dyer, and Blunsom 2018; Merity, Keskar, and Socher\n2018b; Yang et al. 2018) adopts RNNs. An RNN computes a hidden state ht recursively\nfrom the input xt and the previous hidden state ht−1 to create an effective ﬂow of past\ninformation:\nht = Φ(xt, ht−1).\n(12)\nThe function Φ depends on the recurrent architecture of the network. A simple RNN\nmodel computes the hidden vector ht as a matrix transformation of xt and ht−1 by the\nparameters Wxh and Whh and a nonlinear transformation by the sigmoid function:\nht = sigmoid(Wxhxt + Whhht−1 + bh).\n(13)\nIn modern applications, RNNs with a gating mechanism, such as long short-term\nmemory (LSTM) (Hochreiter and Schmidhuber 1997), a gated recurrent unit (GRU)\n(Cho et al. 2014), and quasi-recurrent neural networks (QRNNs) (Bradbury et al. 2017),\nare often adopted. The recurrent architectures of these models are deﬁned as follows.\n1 The implementation used in the experiment is available at https://github.com/musyoku/hpylm.\nAlthough HPYLM is an n-gram language model and it is possible to calculate the perplexity, the\nresulting value is not comparable with those of other n-gram language models and neural language\nmodels. Speciﬁcally, the training data requires using <BOS> and <EOS> to signify the beginning and end\nof a sentence, respectively. This decreases the perplexity because of the regularities introduced by these\ninsertions, such as <EOS> being almost always followed by <BOS>.\n11\nComputational Linguistics\nVolume xx, Number xx\nLSTM\nit = sigmoid(Uixt + Wiht−1 + bi)\n(14)\nft = sigmoid(Ufxt + Wfht−1 + bf)\n(15)\not = sigmoid(Uoxt + Woht−1 + bo)\n(16)\n˜ct = tanh(U˜cxt + W˜cht−1 + b˜c)\n(17)\nct = sigmoid(ft ◦ct−1 + it ◦˜ct)\n(18)\nht = tanh(ct) ◦ot\n(19)\nGRU\nrt = sigmoid(Urxt + Wrht−1 + br)\n(20)\nut = sigmoid(Uuxt + Wuht−1 + bf)\n(21)\n˜ht = tanh(Wxrt ◦xt + Whht + b)\n(22)\nht = (1 −ut) ◦ht−1 + ut ◦˜ht\n(23)\nQRNNs\nzt = sigmoid(W1\nzxt−1 + W2\nzxt)\n(24)\nft = sigmoid(W1\nfxt−1 + W2\nfxt)\n(25)\nht = ft ◦ht−1 + (1 −ft) ◦zt\n(26)\nHere, the operator ◦denotes element-wise multiplication, the capital symbols W and\nU with subscripts are matrices, and the lowercase symbols b with subscripts are bias\nvectors. All these architectures have a gating mechanism (Formula 18, Formula 23, and\nFormula 26), which balances the use of the states at the previous and current time steps.\nIn this article, we consider a total of nine neural language models. Three of them\nare based on a simple RNN, a GRU (Cho et al. 2014), and QRNNs (Bradbury et al.\n2017; Merity, Keskar, and Socher 2018a). The rest are LSTM-based language models.\nThe ﬁrst LSTM model is trained without regularizations such as dropout. The second\nmodel is AWD-LSTM (Merity, Keskar, and Socher 2018b), which applies regulariza-\ntion effectively to achieve competitive prediction performance. The other four models\nintegrate extended architectures of RNN language models, namely, continuous cache\n(Grave, Joulin, and Usunier 2017) and mixture of softmaxes (MoS) (Yang et al. 2018).\nContinuous cache is a memory augmentation architecture that computes a cache prob-\nability pcache from the l most recent context. It computes the similarity between ht and\nhi to estimate the reappearance of the word at time step i. The output probability of\nthe model with continuous cache, denoted as the AWD-LSTM-Cache model, is a linear\ninterpolation of the AWD-LSTM output and the cache probability. We also test a model\nincorporating the Simon process, denoted as the AWD-LSTM-Simon model. It behaves\nas a uniform sampling from the past generated sequence and is a special case of AWD-\nLSTM-Cache. In addition, the MoS architecture reformulates the language modeling\ntask as matrix factorization and is a state-of-the-art language model integrated with\n12\nTakahashi and Tanaka-Ishii\nComputational Linguistics\nAWD-LSTM as the AWD-LSTM-MoS model. Finally, we also consider a combination of\nall these architectures, denoted as the AWD-LSTM-MoS-Cache model.\nThe hyperparameters used in our experiments followed the instructions in (Merity,\nKeskar, and Socher 2018b; Yang et al. 2018). The context length (or the length of back-\npropagation through time) was 70, as given in the references, for both character- and\nword-based models. The cache window size of the AWD-LSTM-Simon model was set to\n10, 000, to balance a large window size with computational efﬁciency. All the language\nmodels were trained to minimize the negative log-likelihood of the training data by\nstochastic gradient algorithms. Note that the perplexity scores for character- and word-\nbased models are not directly comparable, as they indicate bits per character and per\nword, respectively.\n5. Experiments\nFor every language model, a sample text of one million words was generated and eval-\nuated using the metrics explained thus far. We expected models that learned a natural\nlanguage text to be able to generate a sample text with scaling properties resembling\nthose of the original text. In particular, we expected that the exponent values would be\nclose to those of the original dataset.\nThe subsequent two sections, §6 and §7, proceed by examining the scaling prop-\nerties as applied to models that learned WT2 or the PTB. As introduced in §3.3, these\nare two standard datasets used as language model benchmarks. For both WT2 and the\nPTB, the dataset was preprocessed to reduce the vocabulary size. Infrequent words were\nreplaced with <unk>, and numbers were replaced with N in the PTB (Mikolov et al.\n2010). Language models were then constructed by training with either WT2 or the PTB,\nexcept for the Simon and Pitman-Yor processes (but not HPYLM, which does learn) and\nthe PCFG. The PCFG could be constructed only with the PTB dataset, because it requires\na parsed corpus, which does not exist for WT2.\nTable 2 and Table 3 list the perplexity and the scaling exponents of the models for\nthe WT2 and PTB datasets, respectively. Each row presents the results for a single text,\neither real or machine generated. The perplexity is not reported for the Simon model,\nthe Pitman-Yor process, or the PCFG. For the two mathematical models, it was not\nmeasured because they do not have references for computing the prediction accuracy.\nThe perplexity of the PCFG is not reported because its computation does not trivially\nmatch that of the n-gram and neural language models.\nThe ﬁrst blocks in each table indicate the properties of the original datasets with and\nwithout preprocessing. The second blocks list the results for shufﬂed datasets, which\npreserve parts of the n-gram structure. They were tested to check the behavior of the\nevaluation metrics on randomized texts. The shufﬂed datasets were expected to lose\nlong memory and were largely different from the original natural language texts. The\nshufﬂing was conducted as follows. As an example, the text ABCDEFGHI was ﬁrst split\ninto 3-gram chunks, giving ABC/DEF/GHI. Then, the chunks were shufﬂed randomly\nto obtain a 3-gram shufﬂed dataset (i.e., DEF/GHI/ABC). Note that this shufﬂing does\nnot preserve some n-gram structures, such as BCD and FGH, in the original text. The\nremaining blocks correspond to the results for the language models introduced. The\ngrammatical model category is absent in Table 2 because of the lack of a parsed corpus\nfor WT2. Appendix B includes all ﬁgures showing the scaling properties.\n13\nComputational Linguistics\nVolume xx, Number xx\nTable 2\nSummary of the scaling properties of the language models with WT2. † The perplexity measure\nfor HPYLM is not equivalent to that for the N-gram and neural language models because of the\npreprocessing difference. ‡ The values for these models are in bits per character.\nPerplexity\nVocabulary Population\nLong Memory\nZipf’s\nHeaps’\nEbeling’s\nTaylor’s\nLong Range\nLaw\nLaw\nMethod\nLaw\nCorrelation\nf(r) ∝r−α\nv(n) ∝nβ\nm(l) ∝lη\nσ ∝µζ\nc(s) ∝s−ξ\nOriginal Dataset\nWikitext-2 (Preprocessed)\n-\nYes\n0.75 (0.13)\n1.32 (0.10)\n0.62 (0.15)\n0.33 (0.04)\nWikitext-2 (Original)\n-\nYes\n0.78 (0.09)\n1.33 (0.10)\n0.65 (0.11)\n0.32 (0.03)\nShufﬂed Dataset\nWikitext-2(1-gram)\n-\nYes\n0.75 (0.16)\n1.00 (0.01)\n0.50 (0.02)\nNo\nWikitext-2(2-gram)\n-\nYes\n0.76 (0.16)\n1.00 (0.00)\n0.50 (0.01)\nNo\nWikitext-2(5-gram)\n-\nYes\n0.76 (0.16)\n1.00 (0.00)\n0.50 (0.02)\nNo\nWikitext-2(10-gram)\n-\nYes\n0.76 (0.16)\n1.00 (0.00)\n0.50 (0.02)\nNo\nN-gram Language Model\n3-gram\n837.58\nYes\n0.79 (0.13)\n1.00 (0.00)\n0.50 (0.02)\nNo\n5-gram\n534.98\nYes\n0.78 (0.13)\n1.00 (0.00)\n0.50 (0.02)\nNo\nlinear interpolation\n294.72\nYes\n0.78 (0.13)\n1.00 (0.00)\n0.50 (0.02)\nNo\nKatz backoff 3-gram\n285.14\nYes\n0.78 (0.13)\n1.00 (0.00)\n0.50 (0.02)\nNo\nKatz backoff 5-gram\n357.94\nYes\n0.78 (0.13)\n1.00 (0.00)\n0.50 (0.02)\nNo\nKneser-Ney 3-gram\n204.15\nYes\n0.78 (0.13)\n1.00 (0.00)\n0.50 (0.02)\nNo\nKneser-Ney 5-gram\n215.44\nYes\n0.78 (0.13)\n1.00 (0.00)\n0.50 (0.02)\nNo\nSimon/Pitman-Yor Process and Related Language Model\nSimon\n-\nYes\n0.95 (0.15)\n-\n0.50 (0.01)\n0.09 (0.03)\nPitman-Yor\n-\nYes\n0.78(0.09)\n-\n0.50 (0.01)\nNo\nHPYLM\n(184.34†)\nYes\n0.78 (0.13)\n1.00 (0.00)\n0.50 (0.02)\nNo\nNeural Language Model (character based)\nLSTM (no regularization)\n(1.44‡)\nYes\n0.74 (0.17)\n1.06 (0.05)\n0.50 (0.01)\nNo\nAWD-LSTM\n(1.22‡)\nYes\n0.73 (0.15)\n1.27 (0.10)\n0.54 (0.04)\n0.30 (0.05)\nNeural Language Model (word based)\nSimple RNN\n164.51\nYes\n0.79 (0.12)\n1.01 (0.00)\n0.50 (0.02)\nNo\nGRU\n96.22\nYes\n0.79 (0.11)\n1.12 (0.06)\n0.52 (0.03)\n0.52 (Weak)\nQRNN\n74.74\nYes\n0.79 (0.11)\n1.08 (0.03)\n0.52 (0.03)\n0.57 (0.08)\nLSTM (no regularization)\n113.18\nYes\n0.78 (0.12)\n1.10 (0.03)\n0.52 (0.03)\n0.43 (0.15)\nAWD-LSTM\n64.27\nYes\n0.76 (0.13)\n1.30 (0.15)\n0.58 (0.06)\n0.05 (0.01)\nAWD-LSTM-Simon\n61.59\nYes\n0.77 (0.10)\n1.25 (0.15)\n0.55 (0.05)\n0.03 (0.01)\nAWD-LSTM-MoS\n62.44\nYes\n0.78 (0.12)\n1.16 (0.07)\n0.54 (0.04)\n0.33 (0.07)\nAWD-LSTM-MoS-Cache\n59.21\nYes\n0.78 (0.11)\n1.20 (0.07)\n0.57 (0.07)\n0.29 (0.05)\nAWD-LSTM-Cache\n50.39\nYes\n0.78 (0.11)\n1.25 (0.10)\n0.59 (0.07)\n0.14 (0.04)\n6. Evaluation of Metrics\nThe ﬁrst columns of Table 2 and Table 3 list the perplexities of the language models. The\nblank symbol “-” appears in rows for which the perplexity is not available: the original\nand shufﬂed datasets are not language models, while the Simon/Pitman-Yor processes\nand the grammatical model have different deﬁnitions of probability and cannot be\nmeasured comparably with the n-gram and neural language models. The perplexity\nscores in parentheses were measured comparably but are not comparable with the other\nvalues because of their different implementations of preprocessing, as explained at the\nends of §4.3 and §4.4.\nIn terms of perplexity, the neural language models consistently outperformed the\nn-gram models. Among the n-gram models, Kneser-Ney smoothing consistently out-\nperformed the other smoothing techniques. The 3-gram models sometimes had better\nperplexity than the 5-gram models did, as the training datasets in this experiment were\nnot especially large (see Table 1). Among the neural language models, the simple RNN\nmodel had the worst perplexity. The RNNs with a gating mechanism improved the\nperplexity over that of the simple RNN model. In particular, the AWD-LSTM model\n14\nTakahashi and Tanaka-Ishii\nComputational Linguistics\nTable 3\nSummary of the scaling properties of the language models with the PTB. † The perplexity\nmeasure for HPYLM is not equivalent to that for the N-gram and neural language models\nbecause of the preprocessing difference. ‡ The values for these models are in bits per character.\nPerplexity\nVocabulary Population\nLong Memory\nZipf’s\nHeaps’s\nEbeling’s\nTaylor’s\nLong Range\nLaw\nLaw\nMethod\nLaw\nCorrelation\nf(r) ∝r−α\nv(n) ∝nβ\nm(l) ∝lη\nσ ∝µζ\nc(s) ∝s−ξ\nOriginal Dataset\nPenn Treebank (Preprocessed)\n-\nYes\n0.70 (0.16)\n1.23 (0.06)\n0.56 (0.14)\n0.81 (0.24)\nPenn Treebank (Original)\n-\nYes\n0.83 (0.07)\n1.20 (0.05)\n0.57 (0.06)\n0.60 (0.16)\nShufﬂed Dataset\nPenn Treebank (1gram)\n-\nYes\n0.72 (0.18)\n1.00 (0.00)\n0.50 (0.02)\nNo\nPenn Treebank (2gram)\n-\nYes\n0.72 (0.18)\n1.00 (0.00)\n0.50 (0.02)\nNo\nPenn Treebank (5gram)\n-\nYes\n0.72 (0.18)\n1.00 (0.00)\n0.50 (0.02)\nNo\nPenn Treebank (10gram)\n-\nYes\n0.72 (0.18)\n1.00 (0.01)\n0.50 (0.02)\nNo\nN-gram Language Model\n3-gram\n367.79\nYes\n0.71 (0.19)\n0.99 (0.01)\n0.50 (0.02)\nNo\n5-gram\n561.65\nYes\n0.72 (0.21)\n1.00 (0.00)\n0.50 (0.02)\nNo\nlinear interpolation\n238.59\nYes\n0.71 (0.20)\n1.00 (0.00)\n0.50 (0.02)\nNo\nKatz backoff 3-gram\n195.65\nYes\n0.71 (0.19)\n1.00 (0.00)\n0.50 (0.02)\nNo\nKatz backoff 5-gram\n250.18\nYes\n0.71 (0.19)\n1.00 (0.00)\n0.50 (0.02)\nNo\nKneser-Ney 3-gram\n150.64\nYes\n0.72 (0.21)\n1.00 (0.00)\n0.50 (0.02)\nNo\nKneser-Ney 5-gram\n156.70\nYes\n0.71 (0.20)\n1.00 (0.00)\n0.50 (0.02)\nNo\nSimon/Pitman-Yor Process and Related Language Model\nHPYLM\n(140.49†)\nYes\n0.73 (0.21)\n1.00 (0.00)\n0.50 (0.02)\nNo\nGrammatical Model\nPCFG\n-\nYes\n0.73 (0.19)\n1.00 (0.00)\n0.50 (0.02)\nNo\nNeural Language Model (character based)\nLSTM (no regularization)\n(1.38‡)\nYes\n0.79 (0.08)\n1.03 (0.01)\n0.50 (0.01)\nNo\nAWD-LSTM\n(1.18‡)\nYes\n0.76 (0.12)\n1.10 (0.03)\n0.51 (0.02)\n0.40 (0.10)\nNeural Language Model (word based)\nSimple RNN\n123.96\nYes\n0.71 (0.19)\n1.00 (0.01)\n0.50 (0.02)\n0.74 (Weak)\nGRU\n85.05\nYes\n0.71 (0.18)\n1.05 (0.02)\n0.50 (0.02)\n0.40 (Weak)\nQRNN\n62.65\nYes\n0.71 (0.18)\n1.10 (0.03)\n0.51 (0.02)\n0.54 (Weak)\nLSTM (no regularization)\n111.79\nYes\n0.71 (0.19)\n1.04 (0.01)\n0.51 (0.02)\n0.84 (Weak)\nAWD-LSTM\n56.40\nYes\n0.71 (0.18)\n1.06 (0.02)\n0.51 (0.03)\n0.69 (Weak)\nAWD-LSTM-Simon\n57.85\nYes\n0.72 (0.16)\n1.04 (0.01)\n0.51 (0.03)\nNo\nAWD-LSTM-MoS\n54.77\nYes\n0.71 (0.18)\n1.10 (0.03)\n0.52 (0.04)\n0.77 (Weak)\nAWD-LSTM-MoS-Cache\n54.03\nYes\n0.71 (0.18)\n1.13 (0.04)\n0.55 (0.06)\n0.61 (Weak)\nAWD-LSTM-Cache\n52.51\nYes\n0.72 (0.17)\n1.07 (0.02)\n0.53 (0.05)\n0.57 (Weak)\nperformed the best among the RNN language models. The additional architectures of\nthe cache mechanism and MoS contributed to improving the perplexity.\n6.1 Metrics of scaling properties\nThe proposed evaluation metrics should be compared with another evaluation metric\nthat is assumed plausible. In this article, the perplexity is adopted as such a metric.\nAs perplexity has been the standard evaluation metric in language modeling and the\nprediction accuracy is of primary importance for that application, we compare the\nmetrics derived from the scaling properties by comparing them with the perplexity and\nconsider how they correlate with it.\nThe third to seventh columns of Table 2 and Table 3 list the respective results for\nthe scaling properties: Zipf’s law, Heaps’ law, Ebeling’s method, Taylor’s law, and the\nlong-range correlation. Even when the perplexity was not computable, the properties\ncould all still be examined regardless of the kind of language model, except for Ebeling’s\nmethod, because it applies to characters. Overall, except for the long-range correlation,\n15\nComputational Linguistics\nVolume xx, Number xx\nthe results were consistent across the datasets: when a scaling law was followed by one\ndataset, then it was also followed by the other dataset.\nAll the language models qualitatively satisﬁed Zipf’s law. We indicate this by Yes in\nthe tables for the reason stated in §3.1. Relatedly, all the language models also satisﬁed\nHeap’s law. These two properties, however, are present even with a unigram language\nmodel. Despite their fame, Zipf’s law and Heaps’ law have no capacity to distinguish\nrandomized and real text. It is therefore not a challenge for language models to satisfy\nZipf’s and Heaps’ laws.\nIn contrast, the metrics of long memory were capable of quantifying the quality\nof machine-generated texts. For Ebeling’s method (ﬁrst column of the Long Memory\nvertical block), the exponent of the original dataset was η = 1.32 for WT2 and η = 1.23\nfor the PTB, whereas that of both shufﬂed datasets was η = 1.00, thus indicating no long\nmemory in the latter. The neural language models had exponents between η = 1.10 and\nη = 1.30 for WT2, and between η = 1.04 and η = 1.13 for the PTB, whereas the other\nlanguage models were the same as i.i.d. behavior. Ebeling’s method therefore could\nverify the text quality to a certain extent.\nThe last column in each table lists the results for the long-range correlation. If the\ntext was not long-range correlated, this is denoted by No or Weak: No if more than one\nvalue was negative for s ≤10, or Weak if there was one negative value for s ≤100. Such\narbitrariness of judgement is one disadvantage of this metric. In addition, even though\nit has good correspondence with the other two metrics of long memory, it has two\nfurther disadvantages. First, the exponent has poor correlation with the perplexity. The\nsecond disadvantage was exhibited in the degree of long-range correlation listed for the\nSimon model. The degree was high at the beginning and did not decay (see Figure A15\nin Appendix B). As the Simon model had more new words later in a sequence, the\ncorrelation stayed large even for two sequences with a large distance between them.\nTherefore, this non-decaying phenomenon was due not to burstiness but to a different\ncharacteristic speciﬁc to the Simon process. The Taylor exponent for the Simon process\nwas ζ = 0.50, indicating that the long-range correlation observed was not due to long\nmemory behavior.\nFinally, the Taylor exponent ζ seemed the most reliable metric among those derived\nfrom the scaling properties. The left panel of Figure 3 shows the correlation between the\nperplexity of the models and the Taylor exponent ζ. As the perplexity decreased, the\nTaylor exponent ζ showed a steep increase. Because the exponent quantiﬁes the degree\nof burstiness of word occurrence, this result indicates that the better models in terms of\nperplexity can also reproduce that statistical property.\nOverall, the scaling properties of long memory serve for evaluation of generated\ntexts. The Taylor exponent ζ especially has the capability for evaluation.\n6.2 Comparison with PCFG- and language-model-based evaluation\nNext, we test the effectiveness of using the negative log-likelihood from a PCFG (Ra-\njeswar et al. 2017) and the perplexity obtained from a neural language model (Fedus,\nGoodfellow, and Dai 2018). The results show how PCFG-based evaluation is not effec-\ntive, in contrast to evaluation based on the scaling properties.\nIn principle, the negative log-likelihood of a PCFG evaluates the grammaticality\nof text. Rajeswar et al. (2017) used the negative log-likelihood of a PCFG to evaluate\nGAN-generated texts. The scatter plots in Figure 4 show the average negative log-\nlikelihood from a PCFG for the PTB dataset (magenta), the PTB dataset shufﬂed with\n5-grams (green), and the AWD-LSTM-Cache model (blue). Because the PTB dataset is\n16\nTakahashi and Tanaka-Ishii\nComputational Linguistics\nFigure 3\nScatter plots of the perplexity of various models with respect to the Taylor exponent ζ (left) and\nthe perplexity of the eval-AWD-LSTM model (right) for the WT2 dataset. (Left) The Taylor\nexponents of the n-gram language models were consistently ζ = 0.50, which indicates the\nabsence of long memory. In contrast, the neural language models had Taylor exponents of\nζ > 0.50, which indicates the presence of long memory in the generated texts. (Right) The\nperplexity of eval-AWD-LSTM had clear, positive correlation with the perplexities of the\nlanguage models.\nannotated, the negative log-likelihood was calculated for every sentence, and the values\nwere plotted for different sentence lengths. As for the other two cases, because the\noutputs had no sentence boundaries indicated in the training data, consecutive parts\nof a given length n were randomly extracted from the text and fed to the PCFG parser,\nand the negative log-likelihood was then calculated. The NLTK (Loper and Bird 2002)\nparser implementation was used in this work. The shaded area in red represents the\nupper and lower bounds of the original PTB dataset.\nThe average negative log-likelihood of a sentence has a strong linear correlation\nwith its length, and the values for the PTB dataset were consistently lower than those\nfor the generated text of the AWD-LSTM-Cache model and the 5-gram shufﬂed text.\nThe differences from the original PTB dataset, however, were not signiﬁcant, even\nthough the 5-gram and AWD-LSTM-Cache results were calculated merely for n-word\nrandom chunks. Moreover, the average values for the 5-gram shufﬂed text and the\nmachine-generated text were within the range of the PTB’s upper and lower bounds.\nThis indicates that the negative log-likelihood from a PCFG is probably not usable for\nevaluating machine-generated texts.\nApart from the PCFG, Fedus, Goodfellow, and Dai (2018) proposed to evaluate the\nquality of GAN-generated texts with the perplexity computed from a neural language\nmodel. We next test whether that method provides a good measure of the language\nmodels considered here. Accordingly, we used the AWD-LSTM model to evaluate the\ntexts generated by the n-gram and neural language models. To avoid confusion, we\n17\nComputational Linguistics\nVolume xx, Number xx\nFigure 4\nAverage negative log-likelihood of a PCFG for different sentence lengths from the PTB dataset\n(magenta), n-word chunks from the AWD-LSTM-Cache model (blue), and 5-grams from the\nshufﬂed PTB dataset (green). The area shaded red represents the upper and lower bounds of the\nnegative log-likelihood of the PCFG for the PTB dataset.\ncall this the eval-AWD-LSTM model. It was trained with the WT2 and PTB datasets to\nevaluate the texts generated by the various other models (including AWD-LSTM itself).\nThe perplexity of eval-AWD-LSTM was calculated for each machine-generated text\nby formula (1). The rightmost columns of Table 4 and Table 5 list the results, and the\nright panel of Figure 3 shows a scatter plot of the perplexity of the models with respect\nto the perplexity of eval-AWD-LSTM. This method seemed to work well, especially in\nglobally distinguishing the n-gram and neural language model categories: the former\ncategory had perplexities above 600, whereas the latter category had almost all values\nbelow 200 for WT2. The eval-AWD-LSTM perplexity could not, however, detect the\ndifferences among the n-gram language models nor among the neural language models\n(e.g., between Katz backoff and Kneser-Ney, or AWD-LSTM and AWD-LSTM-Cache).\nThe bias caused by the evaluation model is also a problem with this method. In the\nexperiment, AWD-LSTM was the best model by eval-AWD-LSTM evaluation for both\nthe WT2 and PTB datasets. It is likely that worse-performing models whose behavior is\nsimilar to that of the evaluation model are evaluated more highly than are other models\nthat have higher ﬂuency but behave differently from the evaluation model.\nOverall, the evaluation methods using other language models were not consistent.\nThe PCFG-based evaluation could not even clearly distinguish between the shufﬂed\nand original datasets. Evaluation based on a neural language model could detect the\ndifference between the n-gram and neural language models, but it could not distinguish\nquality within those categories of language models. Compared with those methods,\nthe Taylor exponent ζ had a clearer correlation with the perplexity of the models.\nSpeciﬁcally, the exponent satisﬁed ζ = 0.50 for all n-gram language models. It was\nlarger than 0.50 only for the neural language models whose perplexity was better than\nthat of the n-gram language models. Among the neural language models, the Taylor\n18\nTakahashi and Tanaka-Ishii\nComputational Linguistics\nTable 4\nEvaluation of language models by using the AWD-LSTM model (trained with WT2), in\ncomparison with using the perplexity and the Taylor exponent.\nPerplexity\nTaylor exponent\nPerplexity\nfrom\neval-\nAWD-LSTM\nOriginal Dataset\nWikitext-2 (Preprocessed)\n-\n0.62 (0.15)\n33.81\nShufﬂed Dataset\nWikitext-2 (1-gram)\n-\n0.50 (0.02)\n7389.15\nWikitext-2 (2-gram)\n-\n0.50 (0.02)\n2405.15\nWikitext-2 (5-gram)\n-\n0.50 (0.02)\n559.92\nWikitext-2 (10-gram)\n-\n0.50 (0.02)\n236.49\nN-gram Language Model\n3-gram\n837.58\n0.50 (0.02)\n3730.74\n5-gram\n534.98\n0.50 (0.02)\n7532.91\nlinear interpolation\n294.72\n0.50 (0.02)\n1371.75\nKatz backoff 3-gram\n285.14\n0.50 (0.02)\n663.74\nKatz backoff 5-gram\n357.94\n0.50 (0.02)\n664.25\nKneser-Ney 3-gram\n204.15\n0.50 (0.02)\n2562.24\nKneser-Ney 5-gram\n215.44\n0.50 (0.02)\n2743.65\nHPYLM\n184.34\n0.50 (0.02)\n884.76\nNeural Language Model\nSimple RNN\n164.51\n0.50 (0.02)\n645.64\nGRU\n96.22\n0.52 (0.03)\n266.33\nQRNN\n74.74\n0.52 (0.03)\n135.68\nLSTM (no regularization)\n113.18\n0.52 (0.03)\n177.12\nAWD-LSTM\n64.27\n0.58 (0.06)\n88.73\nAWD-LSTM-Simon\n61.59\n0.55 (0.05)\n130.52\nAWD-LSTM-MoS\n62.44\n0.54 (0.04)\n97.89\nAWD-LSTM-MoS-Cache\n59.21\n0.57 (0.07)\n164.39\nAWD-LSTM-Cache\n50.39\n0.59 (0.07)\n109.02\nexponent took high values for the AWD-LSTM family, which had better perplexity than\nthe GRU and QRNN models and the LSTM model without regularization.\n7. Evaluation of Models\nIn this section, we apply the evaluation of metrics in §6.1 to discuss the scaling proper-\nties of the language models. All language models tested in the experiments satisﬁed\nthe scaling properties of vocabulary population, Zipf’s law and Heaps’ law. These\nproperties are relatively easy for models to reproduce, because they concern the static\nprobability distribution of words.\nIn contrast, many of the language models failed to reproduce long memory be-\nhavior. The sole exception was the Simon process, which presented strong long-range\ncorrelation, but this was not caused by burstiness, as explained in §6.1. The lack of\nlong memory in n-gram language models is supported by an analytical argument about\nMarkov models, as mentioned in §4.1. The failure of the PCFG model in our experiment\nsetting can be explained by its lack of inter-sentence structure.\nEven among the neural language models, the simple RNN model failed to re-\nproduce long memory. The Taylor exponent was ζ = 0.50, and the other metrics also\n19\nComputational Linguistics\nVolume xx, Number xx\nTable 5\nEvaluation of language models by using the AWD-LSTM model (trained with the PTB), in\ncomparison with using the perplexity and the Taylor exponent.\nPerplexity\nTaylor exponent\nPerplexity\nfrom\neval-\nAWD-LSTM\nOriginal Dataset\nPenn Tree Bank (Preprocessed)\n-\n0.56 (0.14)\n40.70\nShufﬂed Dataset\nPenn Tree Bank (1-gram)\n-\n0.50 (0.02)\n3698.52\nPenn Tree Bank (2-gram)\n-\n0.50 (0.02)\n1328.39\nPenn Tree Bank (5-gram)\n-\n0.50 (0.02)\n351.22\nPenn Tree Bank (10-gram)\n-\n0.50 (0.02)\n166.93\nN-gram Language Model\n3-gram\n367.79\n0.50 (0.02)\n1697.99\n5-gram\n561.65\n0.50 (0.02)\n3463.88\nlinear interpolation\n238.59\n0.50 (0.02)\n965.58\nKatz backoff 3-gram\n195.65\n0.50 (0.02)\n420.48\nKatz backoff 5-gram\n250.18\n0.50 (0.02)\n471.03\nKneser-Ney 3-gram\n150.64\n0.50 (0.02)\n1324.67\nKneser-Ney 5-gram\n156.70\n0.50 (0.02)\n1411.14\nHPYLM\n140.49\n0.50 (0.02)\n412.13\nNeural Language Model\nSimple RNN\n123.96\n0.50 (0.02)\n321.31\nGRU\n85.05\n0.50 (0.02)\n258.12\nQRNN\n62.65\n0.51 (0.02)\n113.22\nLSTM (no regularization)\n113.18\n0.51 (0.02)\n234.05\nAWD-LSTM\n64.27\n0.51 (0.03)\n90.01\nAWD-LSTM-Simon\n61.59\n0.51 (0.03)\n144.45\nAWD-LSTM-MoS\n62.44\n0.52 (0.04)\n97.73\nAWD-LSTM-MoS-Cache\n59.21\n0.55 (0.06)\n100.56\nAWD-LSTM-Cache\n50.39\n0.53 (0.05)\n123.32\nindicated that the generated text did not have long-range dependence. In contrast,\nthe RNNs with a gating mechanism (LSTM, GRU, and QRNNs) could reproduce long\nmemory behavior. The Taylor exponents of the GRU and QRNN language models were\nboth ζ = 0.52 for WT2, which indicates the presence of long memory to a certain extent.\nThe LSTM language models were consistently the best at reproducing long memory\nbehavior of natural language text for WT2 and the PTB at both the character level and\nthe word level.\nFigure 5 shows (a) Zipf’s law and (b) Taylor’s law results for the AWD-LSTM-\nCache model trained with WT2, which was the best performing model in terms of\nperplexity. Figure 5(a) demonstrates that the Zipf’s law behavior of the dataset shown\nin Figure 1(a) was well recovered. Likewise, Figure 5(b) demonstrates how well the\nAWD-LSTM-Cache model captured and reproduced the Taylor’s law behavior shown\nin Figure 1(d). While the Taylor exponent for the original dataset was ζ = 0.62, the\nAWD-LSTM-Cache model had a Taylor exponent of ζ = 0.59 for WT2. The data points\nin Figure 1(d) were more widely scattered around the regression line than those in\nFigure 5(b) were. Even with the well-performing neural language models, however, the\nscaling properties of long memory were not fully recovered. These differences represent\n20\nTakahashi and Tanaka-Ishii\nComputational Linguistics\n(a) Zipf’s Law\n(b) Taylor’s Law\nFigure 5\nScaling properties of the AWD-LSTM-Cache model trained with WT2.\nTable 6\nSummary of statistics for the COCO image dataset.\nTokens\nVocab\nVocabulary Population\nLong Memory\n-ulary\nZipf’s\nHeaps’\nEbeling’s\nTaylor’s\nLong Range\nLaw\nLaw\nMethod\nLaw\nCorrelation\nf(r) ∝r−α\nv(n) ∝nβ\nm(l) ∝lη\nσ ∝µζ\nc(s) ∝s−ξ\nImage COCO (English, collection of image caption)\noriginal dataset\n105,933\n6,095\nYes\n0.76 (0.09)\n0.99 (0.03)\n0.50 (0.04)\nNo\ngaps between the natural language text and the language model, which may indicate\nroom for improvement.\nTable 7\nBLEU scores and perplexity for eval-AWD-LSTM-based evaluation on texts generated from the\nCOCO image dataset by different GAN models: SeqGAN (Yu et al. 2017), MaliGAN (Che et al.\n2017), RankGAN (Lin et al. 2017), LeakGAN (Guo et al. 2018), and TextGAN (Zhang et al. 2017).\nSeqGAN\nMaliGAN\nRankGAN\nLeakGAN\nTextGAN\nMLE\nImageCoco\nBLEU-2\n0.92\n0.89\n0.94\n0.93\n0.65\n0.92\n1.00\nBLEU-3\n0.75\n0.70\n0.80\n0.82\n0.65\n0.68\n1.00\nBLEU-4\n0.53\n0.48\n0.60\n0.66\n0.60\n0.57\n1.00\nBLEU-5\n0.35\n0.31\n0.41\n0.47\n0.52\n0.39\n1.00\neval-AWD-LSTM\n179.29\n272.53\n132.90\n146.26\n129.93\n176.34\n44.17\n8. Evaluation of GAN Models\nFinally, we discuss the possibility of evaluating GAN-generated text with the scaling\nproperties. Table 6 lists the scaling properties for the COCO image dataset (Lin et al.\n2014). Because current GAN models for text generation cannot produce long texts,\nimage captions constitute the standard dataset for these GAN models. Because of the\ndataset used, the GAN models are limited to generating a certain text type (i.e., image\ncaptions). In particular, as the length of the text is short, the results are readily expected\nnot to reproduce long memory behavior. Yet, it is worthwhile to test the vocabulary\npopulation of the GAN models to understand their capacity.\n21\nComputational Linguistics\nVolume xx, Number xx\n(a) Zipf’s Law\n(b) Taylor’s Law\nFigure 6\nScaling properties of captions in the COCO image dataset.\n(a) Zipf’s Law\n(b) Taylor’s Law\nFigure 7\nScaling properties of captions generated from the COCO image dataset by SeqGAN.\nFigure 6 and Figure 7 show Zipf’s and Taylor’s law graphs for the original dataset\nand the text generated by SeqGAN (Yu et al. 2017), respectively. Unlike the other\nlanguage models, GAN models for text generation had problems reproducing Zipf’s\nlaw. The tail decay for the generated text was faster than that for the dataset. The\nvocabulary size of the generated text was only v(n) = 1, 822 words for n = 118, 264\ngenerated words, whereas the original text had a vocabulary size v(n) = 6, 095 for\nn = 105, 933 words. This result indicates that the GAN model could not produce the\ninfrequent words in the training dataset.\nOn the other hand, long memory was already absent at the level of the training\ndataset. The Taylor exponent was ζ = 0.50 (Figure 6(b)), indicating no memory, which\nwas obviously expected, as the captions were shufﬂed and two consecutive captions\nhad no relation. Through learning of such training data and production caption by\ncaption, the generated text also had no long memory (Figure 7(b)). Indeed, long memory\nanalysis literally requires a model to generate a sufﬁciently long text to allow further\nquality evaluation of natural language.\nNevertheless, other metrics would not provide a better evaluation in this case.\nTable 7 lists the evaluation metrics of BLEU and perplexity by eval-AWD-LSTM for\ntexts generated using different GAN techniques. The BLEU scores for the GAN models\nin Table 7 were extracted from (Zhu et al. 2018). The perplexity scores were computed\nby using the eval-AWD-LSTM model trained with the COCO image dataset and the\n22\nTakahashi and Tanaka-Ishii\nComputational Linguistics\nhyperparameters for the PTB dataset. The perplexity of AWD-LSTM when trained with\nthat dataset was 65.41.\nFor both BLEU and perplexity, the results were inconsistent. In terms of BLEU, the\nbest-performing GAN model varied among RankGAN with BLEU-2, LeakGAN with\nBLEU-3 and BLEU4, and TextGAN with BLEU-5. In contrast, TextGAN was the best\nmodel in terms of eval-AWD-LSTM. In addition to these metrics, the negative log-\nlikelihood of the PCFG was also not effective in evaluating the GAN models in (Zhu\net al. 2018).\nAlthough rigid quantitative evaluation is necessary for comparing GAN models,\nthe existing evaluation metrics are not sufﬁciently reliable. Therefore, further study of\nevaluation metrics is necessary. The Taylor exponent may play a role in such studies\nwhen GAN-based models become able to produce longer texts.\n9. Conclusion\nIn this article, we have investigated the scaling properties of computational models\nof natural language and analyzed whether these metrics could serve for assessing the\nmodels. The scaling properties quantify the vocabulary population and long memory\nbehavior, which are universal qualities of natural language text. These metrics are\napplicable to any model, even those for which the perplexity is not measurable or a\nreference is not available. We tested n-gram language models, a grammatical model,\nmathematical models, neural language models, and GAN models for text generation.\nAmong the ﬁve scaling properties introduced, the exponent of Taylor’s law showed\nthe most reasonable behavior. It had the clearest correlation with the perplexity of the\nn-gram and neural language models.\nOur analysis demonstrated that RNNs with a gating mechanism (LSTM, GRU, and\nQRNNs) are the ﬁrst computational models of natural language that have the capacity\nto reproduce the long memory in natural language text. No other models tested in\nour experiment reproduced the scaling properties of long memory. The LSTM models\nwere the best among the neural language models, as their long memory behavior was\ncloser to that of the original text as compared to the GRU and QRNN models. Yet,\neven the LSTM language models could not entirely recover long memory, including the\nexponents of the scaling properties. This observation conﬁrms the gap between natural\nlanguage text and language models and suggests corresponding room for improve-\nment. Our future work will include investigating other scaling properties that could\nserve for evaluating language models.\n23\nComputational Linguistics\nVolume xx, Number xx\nReferences\nAltmann, Eduardo G. and Martin Gerlach.\n2017. Statistical laws in linguistics.\nCreativity and Universality in Language,\npages 7–26.\nAltmann, Eduardo G., Janet B.\nPierrehumbert, and Adilson E. Motter.\n2009. Beyond word frequency: Bursts,\nlulls, and scaling in the temporal\ndistributions of words. PLoS One,\n4(11):e7678.\nBaeza-Yates, Ricardo and Gonzalo Navarro.\n2000. Block addressing indices for\napproximate text retrieval. Journal of the\nAmerican Society for Information Science,\n51(1):69–82.\nBradbury, James, Stephen Merity, Caiming\nXiong, and Richard Socher. 2017.\nQuasi-recurrent neural networks. In\nProceedings of International Conference on\nLearning Representations, Toulon.\nChe, Tong, Yanran Li, Ruixiang Zhang,\nDevon R Hjelm, Wenjie Li, Yangqiu Song,\nand Yoshua Bengio. 2017.\nMaximum-likelihood augmented discrete\ngenerative adversarial networks. arXiv\npreprint arXiv:1702.07983.\nChen, Stanley F. and Joshua Goodman. 1999.\nAn empirical study of smoothing\ntechniques for language modeling.\nComputer Speech & Language, 13(4):359–394.\nCho, Kyunghyun, Bart van Merriënboer,\nÇaglar Gülçehre, Dzmitry Bahdanau, Fethi\nBougares, Holger Schwenk, and Yoshua\nBengio. 2014. Learning Phrase\nRepresentations using RNN\nEncoder–Decoder for Statistical Machine\nTranslation. In Proceedings of the 2014\nConference on Empirical Methods in Natural\nLanguage Processing, pages 1724–1734,\nDoha.\nClauset, Aaron, Cosma Rohilla Shalizi, and\nM.E.J. Newman. 2009. Power-law\ndistributions in empirical data. SIAM\nreview, 51(4):661–703.\nEbeling, Werner and Alexander Neiman.\n1995. Long-range correlations between\nletters and sentences in texts. Physica A,\n215(3):233–241.\nEbeling, Werner and Thorsten Pöschel. 1994.\nEntropy and long-range correlations in\nliterary english. Europhysics Letters,\n26(4):241–246.\nEisler, Zoltán, Imre Bartos, and Janos\nKertész. 2007. Fluctuation scaling in\ncomplex systems: Taylor’s law and\nbeyond. Advances in Physics, 57(1):89–142.\nFedus, William, Ian Goodfellow, and\nAndrew M. Dai. 2018. MaskGAN: Better\ntext generation via ﬁlling in the _______.\nIn Proceedings of International Conference on\nLearning Representations, Vancouver.\nForney, G. David. 1973. The viterbi\nalgorithm. Proceedings of the IEEE,\n61(3):268–278.\nGerlach, Martin and Eduardo G. Altmann.\n2013. Stochastic model for the vocabulary\ngrowth in natural languages. Physical\nReview X, 3(2):021006.\nGoldwater, Sharon, Thomas L. Grifﬁths, and\nMark Johnson. 2011. Producing\npower-law distributions and damping\nword frequencies with two-stage language\nmodels. Journal of Machine Learning\nResearch, 12:2335–2382.\nGrave, Edouard, Armand Joulin, and Nicolas\nUsunier. 2017. Improving neural language\nmodels with a continuous cache. In\nProceedings of International Conference on\nLearning Representations, Toulon.\nGuo, Jiaxian, Sidi Lu, Han Cai, Weinan\nZhang, Yong Yu, and Jun Wang. 2018.\nLong text generation via adversarial\ntraining with leaked information. In The\nThirty-Second AAAI Conference, pages\n5141–5148, Louisiana.\nHochreiter, Sepp and Jürgen Schmidhuber.\n1997. Long short-term memory. Neural\nComputation, 9(8):1735–1780.\nKatz, Slava M. 1987. Estimation of\nprobabilities from sparse data for the\nlanguage model component of a speech\nrecognizer. IEEE Transactions on Acoustics,\nSpeech, and Signal Processing, 35(3):400–401.\nKingman, J.F.C. 1963. The exponential decay\nof markov transition probabilities.\nProceedings of the London Mathematical\nSociety, s3-13(1):337–358.\nKneser, Reinhard and Hermann Ney. 1995.\nImproved backing-off for m-gram\nlanguage modeling. In Proceedings of the\nIEEE Conference on Acoustics, Speech and\nSignal Processing, volume 1, pages 181–184,\nMichigan.\nKobayashi, Tatsuru and Kumiko\nTanaka-Ishii. 2018. Taylor’s law for human\nlinguistic sequences. In Proceedings of the\n56th Annual Meeting of the Association for\nComputational Linguistics, pages 1138–1148,\nMelbourne.\nvan Leijenhorst, Dick and Theo van der\nWeide. 2005. A formal derivation of\nHeaps’ law. Information Sciences,\n170(2-4):263–272.\n24\nTakahashi and Tanaka-Ishii\nComputational Linguistics\nLennartz, Sabine and Armin Bunde. 2009.\nEliminating ﬁnite-size effects and\ndetecting the amount of whitenoise in\nshort records with long-term memory.\nPhysical Review E, 79(6):066101.\nLi, Wentian. 1989. Mutual information\nfunctions of natural language texts. Santa\nFe Institute Working Paper.\nLin, Chin-Yew. 2004. Rouge: a package for\nautomatic evaluation of summaries. In\nProceedings of the 42nd Annual Meeting of\nthe Association for Computational Linguistics\nWorkshop, Barcelona.\nLin, Henry W. and Max Tegmark. 2017.\nCritical behavior in physics and\nprobabilistic formal languages. entropy,\n19(7):299.\nLin, Kevin, Dianqi Li, Xiaodong He,\nZhengyou Zhang, and Ming-Ting Sun.\n2017. Adversarial ranking for language\ngeneration. In Advances in Neural\nInformation Processing Systems, pages\n3155–3165, California.\nLin, Tsung-Yi, Michael Maire, Serge Belongie,\nPietro Perona, Deva Ramanan, Piotr\nDollár, and C. Lawrence Zitnick. 2014.\nMicrosoft coco: Common objects in\ncontext. In European Conference on\nComputer Vision, pages 740–755, Zurich.\nLoper, Edward and Steven Bird. 2002. Nltk:\nThe natural language toolkit. In\nProceedings of the 40th Annual Meeting of the\nAssociation for Computational Linguistics\nWorkshop, pages 63–70, Pennsylvania.\nLü, Linyuan, Zi-Ke Zhang, and Tao Zhou.\n2010. Zipf’s law leads to Heaps’ law:\nAnalyzing their relation in ﬁnite-size\nsystems. PLoS One, 5(12):e14139.\nLu, Sidi, Lantao Yu, Weinan Zhang, and\nYong Yu. 2018. Cot: Cooperative training\nfor generative modeling. arXiv preprint\narXiv:1804.03782.\nManning, Chris and Hinrich Schutze. 1999.\nFoundations of Statistical Natural Language\nProcessing. MIT Press.\nMelis, Gábor, Chris Dyer, and Phil Blunsom.\n2018. On the state of the art of evaluation\nin neural language models. In Proceedings\nof International Conference on Learning\nRepresentations, Vancouver.\nMerity, Stephen, Nitish Keskar, and Richard\nSocher. 2018a. An analysis of neural\nlanguage modeling at multiple scales.\narXiv preprint arXiv:1803.08240.\nMerity, Stephen, Nitish Shirish Keskar, and\nRichard Socher. 2018b. Regularizing and\noptimizing LSTM language models. In\nProceedings of International Conference on\nLearning Representations, Vancouver.\nMerity, Stephen, Caiming Xiong, James\nBradbury, and Richard Socher. 2016.\nPointer sentinel mixture models. In\nProceedings of International Conference on\nLearning Representations, San Juan.\nMikolov, Tomáš, Martin Karaﬁát, Lukáš\nBurget, Jan Honza ˇCernocký, and Sanjeev\nKhudanpur. 2010. Recurrent neural\nnetwork based language model. In\nProceedings of the 11th Annual Conference of\nthe International Speech Communication\nAssociation, pages 1045–1048, Chiba.\nMikolov, Tomáš and Geoffrey Zweig. 2012.\nContext dependent recurrent neural\nnetwork language model. In The IEEE\nWorkshop on Spoken Language Technology,\npages 234–239, Florida.\nPapineni, Kishore, Salim Roukos, Todd\nWard, and Wei-Jing Zhu. 2002. Bleu: a\nmethod for automatic evaluation of\nmachine translation. In Proceedings of the\n40th Annual Meeting of the Association for\nComputational Linguistics, pages 311–318,\nPennsylvania.\nRajeswar, Sai, Sandeep Subramanian, Francis\nDutil, Christopher Pal, and Aaron\nCourville. 2017. Adversarial generation of\nnatural language. In Proceedings of the 55th\nAnnual Meeting of the Association for\nComputational Linguistics Workshop, pages\n241–251, Vancouver.\nSimon, Herbert A. 1955. On a class of skew\ndistribution functions. Biometrika,\n42(3/4):425–440.\nSmith, H. Fairﬁeld. 1938. An empirical law\ndescribing hetero-geneity in the yields of\nagricultural crops. Journal of Agriculture\nScience, 28(1):1–23.\nStolcke, Andreas. 2002. Srilm - an extensible\nlanguage modeling toolkit. In International\nConference on Spoken Language Processing,\npages 901–904, Colorado.\nTakahashi, Shuntaro and Kumiko\nTanaka-Ishii. 2017. Do neural nets learn\nstatistical laws behind natural language?\nPLoS One, 12(12):e0189326.\nTanaka-Ishii, Kumiko and Armin Bunde.\n2016. Long-range memory in literary texts:\nOn the universal clustering of the rare\nwords. PLoS One, 11(11):e0164658.\nTanaka-Ishii, Kumiko and Tatsuru\nKobayashi. 2018. Taylor’s law for\nlinguistic sequences and random walk\nmodels. Journal of Physics Communications,\n2(11):115024.\nTaylor, Lionel Roy. 1961. Aggregation,\nvariance and the mean. Nature,\n189(4766):732–735.\n25\nComputational Linguistics\nVolume xx, Number xx\nTeh, Yee Whye. 2006. A hierarchical bayesian\nlanguage model based on pitman-yor\nprocesses. In Proceedings of the 44th Annual\nMeeting of the Association for Computational\nLinguistics, pages 985–992, Sydney.\nYang, Zhilin, Zihang Dai, Ruslan\nSalakhutdinov, and William W. Cohen.\n2018. Breaking the softmax bottleneck: A\nhigh-rank RNN language model. In\nProceedings of International Conference on\nLearning Representations, Vancouver.\nYu, Lantao, Weinan Zhang, Jun Wang, and\nYong Yu. 2017. Seqgan: sequence\ngenerative adversarial nets with policy\ngradient. In The Thirty-First AAAI\nConference, pages 2852–2858, California.\nZhang, Yizhe, Zhe Gan, Kai Fan, Zhi Chen,\nRicardo Henao, Dinghan Shen, and\nLawrence Carin. 2017. Adversarial feature\nmatching for text generation. arXiv\npreprint arXiv:1706.03850.\nZhu, Yaoming, Sidi Lu, Lei Zheng, Jiaxian\nGuo, Weinan Zhang, Jun Wang, and Yong\nYu. 2018. Texygen: A benchmarking\nplatform for text generation models. arXiv\npreprint arXiv:1802.01886.\n26\nTakahashi and Tanaka-Ishii\nComputational Linguistics\nAppendix A: Scaling properties of natural language\nThis section presents the ﬁgures for the scaling properties of dataset appeared in this\npaper. The presence of the scaling properties is robust to the genre and the language of\nthe text.\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Ebeling’s Method (d) Taylor’s Law (e) Long-Range Cor-\nrelation\nFigure A1\nScaling properties of the collected works of Shakespeare\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Ebeling’s Method (d) Taylor’s Law (e) Long-Range Cor-\nrelation\nFigure A2\nScaling properties of Hong Lou Meng.\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Ebeling’s Method\n(d) Taylor’s Law\n(e) Long-Range Cor-\nrelation\nFigure A3\nScaling properties of the Penn-Treebank (original)\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Ebeling’s Method\n(d) Taylor’s Law\n(e) Long-Range Cor-\nrelation\nFigure A4\nScaling properties of Wikitext-2 (original)\n27\nComputational Linguistics\nVolume xx, Number xx\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Ebeling’s Method (d) Taylor’s Law (e) Long-Range Cor-\nrelation\nFigure A5\nScaling properties of captions in COCO image dataset\nAppendix B: Scaling properties of language model\nThis section presents the ﬁgures for the scaling properties of language models of WT2\nin this paper.\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Ebeling’s Method (d) Taylor’s Law (e) Long-Range Cor-\nrelation\nFigure A6\nScaling properties of 3-gram language model\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Ebeling’s Method (d) Taylor’s Law (e) Long-Range Cor-\nrelation\nFigure A7\nScaling properties of 5-gram language model\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Ebeling’s Method (d) Taylor’s Law (e) Long-Range Cor-\nrelation\nFigure A8\nScaling properties of linear interpolation n-gram language model\n28\nTakahashi and Tanaka-Ishii\nComputational Linguistics\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Ebeling’s Method (d) Taylor’s Law (e) Long-Range Cor-\nrelation\nFigure A9\nScaling properties of the Katz backoff 3-gram language model\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Ebeling’s Method (d) Taylor’s Law (e) Long-Range Cor-\nrelation\nFigure A10\nScaling properties of Katz backoff 5-gram language model\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Ebeling’s Method (d) Taylor’s Law (e) Long-Range Cor-\nrelation\nFigure A11\nScaling properties of Kneser-Ney 3-gram language model\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Ebeling’s Method (d) Taylor’s Law (e) Long-Range Cor-\nrelation\nFigure A12\nScaling properties of Kneser-Ney 5-gram language model\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Ebeling’s Method\n(d) Taylor’s Law\n(e) Long-Range Cor-\nrelation\nFigure A13\nScaling properties of hierarchical Pitman-Yor language model\n29\nComputational Linguistics\nVolume xx, Number xx\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Ebeling’s Method\n(d) Taylor’s Law\n(e) Long-Range Cor-\nrelation\nFigure A14\nScaling properties of the PCFG constructed from PTB dataset\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Taylor’s Law (d) Long-Range Cor-\nrelation\nFigure A15\nScaling properties of the Simon process. The ﬁgure of Ebeling method does not appear because\nof the inappropriateness of the application.\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Taylor’s Law (d) Long-Range Cor-\nrelation\nFigure A16\nScaling properties of Pitman-Yor process. The ﬁgure of Ebeling method does not appear because\nof the inappropriateness of the application.\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Ebeling’s Method (d) Taylor’s Law (e) Long-Range Cor-\nrelation\nFigure A17\nScaling properties of Simple RNN language model\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Ebeling’s Method (d) Taylor’s Law (e) Long-Range Cor-\nrelation\nFigure A18\nScaling properties of GRU language model\n30\nTakahashi and Tanaka-Ishii\nComputational Linguistics\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Ebeling’s Method (d) Taylor’s Law (e) Long-Range Cor-\nrelation\nFigure A19\nScaling properties of QRNN language model\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Ebeling’s Method\n(d) Taylor’s Law\n(e) Long-Range Cor-\nrelation\nFigure A20\nScaling properties of LSTM without regularization language model\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Ebeling’s Method\n(d) Taylor’s Law\n(e) Long-Range Cor-\nrelation\nFigure A21\nScaling properties of AWD-LSTM\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Ebeling’s Method\n(d) Taylor’s Law\n(e) Long-Range Cor-\nrelation\nFigure A22\nScaling properties of AWD-LSTM-Simon\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Ebeling’s Method\n(d) Taylor’s Law\n(e) Long-Range Cor-\nrelation\nFigure A23\nScaling properties of AWD-LSTM-MoS\n31\nComputational Linguistics\nVolume xx, Number xx\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Ebeling’s Method\n(d) Taylor’s Law\n(e) Long-Range Cor-\nrelation\nFigure A24\nScaling properties of AWD-LSTM-MoS-Cache\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Ebeling’s Method\n(d) Taylor’s Law\n(e) Long-Range Cor-\nrelation\nFigure A25\nScaling properties of AWD-LSTM-Cache\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Ebeling’s Method (d) Taylor’s Law (e) Long-Range Cor-\nrelation\nFigure A26\nScaling properties of LSTM without regularization for character-level modeling\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Ebeling’s Method (d) Taylor’s Law (e) Long-Range Cor-\nrelation\nFigure A27\nScaling properties of AWD-LSTM for character-level modeling\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Ebeling’s Method (d) Taylor’s Law (e) Long-Range Cor-\nrelation\nFigure A28\nScaling properties of the Seq-GAN (the model learns COCO image dataset)\n32\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2019-06-22",
  "updated": "2019-06-22"
}