{
  "id": "http://arxiv.org/abs/1906.10025v2",
  "title": "Modern Deep Reinforcement Learning Algorithms",
  "authors": [
    "Sergey Ivanov",
    "Alexander D'yakonov"
  ],
  "abstract": "Recent advances in Reinforcement Learning, grounded on combining classical\ntheoretical results with Deep Learning paradigm, led to breakthroughs in many\nartificial intelligence tasks and gave birth to Deep Reinforcement Learning\n(DRL) as a field of research. In this work latest DRL algorithms are reviewed\nwith a focus on their theoretical justification, practical limitations and\nobserved empirical properties.",
  "text": "Moscow State University\nFaculty of Computational Mathematics and Cybernetics\nDepartment of Mathematical Methods of Forecasting\nModern Deep Reinforcement Learning Algorithms\nWritten by:\nSergey Ivanov\nqbrick@mail.ru\nScientiﬁc advisor:\nAlexander D’yakonov\ndjakonov@mail.ru\nMoscow, 2019\narXiv:1906.10025v2  [cs.LG]  6 Jul 2019\nContents\n1\nIntroduction\n4\n2\nReinforcement Learning problem setup\n5\n2.1\nAssumptions of RL setting\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.2\nEnvironment model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.3\nObjective\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.4\nValue functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n2.5\nClasses of algorithms\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2.6\nMeasurements of performance\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n3\nValue-based algorithms\n10\n3.1\nTemporal Diﬀerence learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n3.2\nDeep Q-learning (DQN) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n3.3\nDouble DQN\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n3.4\nDueling DQN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n3.5\nNoisy DQN\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n3.6\nPrioritized experience replay . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n3.7\nMulti-step DQN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n4\nDistributional approach for value-based methods\n20\n4.1\nTheoretical foundations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n4.2\nCategorical DQN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n4.3\nQuantile Regression DQN (QR-DQN) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n4.4\nRainbow DQN\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n5\nPolicy Gradient algorithms\n29\n5.1\nPolicy Gradient theorem\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n5.2\nREINFORCE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n5.3\nAdvantage Actor-Critic (A2C)\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n5.4\nGeneralized Advantage Estimation (GAE) . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n5.5\nNatural Policy Gradient (NPG)\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n5.6\nTrust-Region Policy Optimization (TRPO)\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\n5.7\nProximal Policy Optimization (PPO)\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n6\nExperiments\n41\n6.1\nSetup\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n6.2\nCartpole . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n6.3\nPong . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\n6.4\nInteraction-training trade-oﬀin value-based algorithms\n. . . . . . . . . . . . . . . . . .\n43\n6.5\nResults\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\n7\nDiscussion\n47\nA\nImplementation details\n50\nB\nHyperparameters\n51\nC\nTraining statistics on Pong\n52\nD Playing Pong behaviour\n54\n2\nAbstract\nRecent advances in Reinforcement Learning, grounded on combining classical theoretical re-\nsults with Deep Learning paradigm, led to breakthroughs in many artiﬁcial intelligence tasks and\ngave birth to Deep Reinforcement Learning (DRL) as a ﬁeld of research. In this work latest DRL algo-\nrithms are reviewed with a focus on their theoretical justiﬁcation, practical limitations and observed\nempirical properties.\n3\n1.\nIntroduction\nDuring the last several years Deep Reinforcement Learning proved to be a fruitful approach to\nmany artiﬁcial intelligence tasks of diverse domains. Breakthrough achievements include reaching\nhuman-level performance in such complex games as Go [22], multiplayer Dota [16] and real-time\nstrategy StarCraft II [26]. The generality of DRL framework allows its application in both discrete and\ncontinuous domains to solve tasks in robotics and simulated environments [12].\nReinforcement Learning (RL) is usually viewed as general formalization of decision-making task\nand is deeply connected to dynamic programming, optimal control and game theory. [23] Yet its\nproblem setting makes almost no assumptions about world model or its structure and usually sup-\nposes that environment is given to agent in a form of black-box. This allows to apply RL practically\nin all settings and forces designed algorithms to be adaptive to many kinds of challenges. Latest RL\nalgorithms are usually reported to be transferable from one task to another with no task-speciﬁc\nchanges and little to no hyperparameters tuning.\nAs an object of desire is a strategy, i. e.\na function mapping agent’s observations to possible\nactions, reinforcement learning is considered to be a subﬁled of machine learning. But instead of\nlearning from data, as it is established in classical supervised and unsupervised learning problems,\nthe agent learns from experience of interacting with environment. Being more \"natural\" model of\nlearning, this setting causes new challenges, peculiar only to reinforcement learning, such as neces-\nsity of exploration integration and the problem of delayed and sparse rewards. The full setup and\nessential notation are introduced in section 2.\nClassical Reinforcement Learning research in the last third of previous century developed an ex-\ntensive theoretical core for modern algorithms to ground on. Several algorithms are known ever\nsince and are able to solve small-scale problems when either environment states can be enumer-\nated (and stored in the memory) or optimal policy can be searched in the space of linear or quadratic\nfunctions of state representation features. Although these restrictions are extremely limiting, foun-\ndations of classical RL theory underlie modern approaches. These theoretical fundamentals are\ndiscussed in sections 3.1 and 5.1–5.2.\nCombining this framework with Deep Learning [5] was popularized by Deep Q-Learning algo-\nrithm, introduced in [14], which was able to play any of 57 Atari console games without tweaking net-\nwork architecture or algorithm hyperparameters. This novel approach was extensively researched\nand signiﬁcantly improved in the following years. The principles of value-based direction in deep\nreinforcement learning are presented in section 3.\nOne of the key ideas in the recent value-based DRL research is distributional approach, proposed\nin [1]. Further extending classical theoretical foundations and coming with practical DRL algorithms,\nit gave birth to distributional reinforcement learning paradigm, which potential is now being actively\ninvestigated. Its ideas are described in section 4.\nSecond main direction of DRL research is policy gradient methods, which attempt to directly op-\ntimize the objective function, explicitly present in the problem setup. Their application to neural\nnetworks involve a series of particular obstacles, which requested specialized optimization tech-\nniques. Today they represent a competitive and scalable approach in deep reinforcement learning\ndue to their enormous parallelization potential and continuous domain applicability. Policy gradient\nmethods are discussed in section 5.\nDespite the wide range of successes, current state-of-art DRL methods still face a number of\nsigniﬁcant drawbacks. As training of neural networks requires huge amounts of data, DRL demon-\nstrates unsatisfying results in settings where data generation is expensive. Even in cases where\ninteraction is nearly free (e. g. in simulated environments), DRL algorithms tend to require excessive\namounts of iterations, which raise their computational and wall-clock time cost. Furthermore, DRL\nsuﬀers from random initialization and hyperparameters sensitivity, and its optimization process is\nknown to be uncomfortably unstable [9]. Especially embarrassing consequence of these DRL fea-\ntures turned out to be low reproducibility of empirical observations from diﬀerent research groups\n[6]. In section 6, we attempt to launch state-of-art DRL algorithms on several standard testbed envi-\nronments and discuss practical nuances of their application.\n4\n2.\nReinforcement Learning problem setup\n2.1.\nAssumptions of RL setting\nInformally, the process of sequential decision-making proceeds as follows. The agent is pro-\nvided with some initial observation of environment and is required to choose some action from the\ngiven set of possibilities. The environment responds by transitioning to another state and generat-\ning a reward signal (scalar number), which is considered to be a ground-truth estimation of agent’s\nperformance. The process continues repeatedly with agent making choices of actions from observa-\ntions and environment responding with next states and reward signals. The only goal of agent is to\nmaximize the cumulative reward.\nThis description of learning process model already introduces several key assumptions. Firstly,\nthe time space is considered to be discrete, as agent interacts with environment sequentially. Sec-\nondly, it is assumed that provided environment incorporates some reward function as supervised\nindicator of success. This is an embodiment of the reward hypothesis, also referred to as Reinforce-\nment Learning hypothesis:\nProposition 1. (Reward Hypothesis) [23]\n«All of what we mean by goals and purposes can be well thought of as maximization of the expected\nvalue of the cumulative sum of a received scalar signal (reward).»\nExploitation of this hypothesis draws a line between reinforcement learning and classical ma-\nchine learning settings, supervised and unsupervised learning. Unlike unsupervised learning, RL\nassumes supervision, which, similar to labels in data for supervised learning, has a stochastic nature\nand represents a key source of knowledge. At the same time, no data or «right answer» is provided\nto training procedure, which distinguishes RL from standard supervised learning. Moreover, RL is the\nonly machine learning task providing explicit objective function (cumulative reward signal) to max-\nimize, while in supervised and unsupervised setting optimized loss function is usually constructed\nby engineer and is not «included» in data. The fact that reward signal is incorporated in the envi-\nronment is considered to be one of the weakest points of RL paradigm, as for many real-life human\ngoals introduction of this scalar reward signal is at the very least unobvious.\nFor practical applications it is also natural to assume that agent’s observations can be repre-\nsented by some feature vectors, i. e. elements of Rd. The set of possible actions in most practical\napplications is usually uncomplicated and is either discrete (number of possible actions is ﬁnite) or\ncan be represented as subset of Rm (almost always [−1, 1]m or can be reduced to this case)1. RL\nalgorithms are usually restricted to these two cases, but the mix of two (agent is required to choose\nboth discrete and continuous quantities) can also be considered.\nThe ﬁnal assumption of RL paradigm is a Markovian property:\nProposition 2. (Markovian property)\nTransitions depend solely on previous state and the last chosen action and are independent of all\nprevious interaction history.\nAlthough this assumption may seem overly strong, it actually formalizes the fact that the world\nmodeled by considered environment obeys some general laws. Giving that the agent knows the\ncurrent state of the world and the laws, it is assumed that it is able to predict the consequences of\nhis actions up to the internal stochasticity of these laws. In practice, both laws and complete state\nrepresentation is unavailable to agent, which limits its forecasting capability.\nIn the sequel we will work within the setting with one more assumption of full observability. This\nsimpliﬁcation supposes that agent can observe complete world state, while in many real-life tasks\nonly a part of observations is actually available. This restriction of RL theory can be removed by\nconsidering Partially observable Markov Decision Processes (PoMDP), which basically forces learn-\ning algorithms to have some kind of memory mechanism to store previously received observations.\nFurther on we will stick to fully observable case.\n1this set is considered to be permanent for all states of environment without any loss of generality as if agent chooses\ninvalid action the world may remain in the same state with zero or negative reward signal or stochastically select some valid\naction for him.\n5\n2.2.\nEnvironment model\nThough the deﬁnition of Markov Decision Process (MDP) varies from source to source, its essen-\ntial meaning remains the same. The deﬁnition below utilizes several simpliﬁcations without loss of\ngenerality.2\nDeﬁnition 1. Markov Decision Process (MDP) is a tuple (S, A, T, r, s0), where:\n• S ⊆Rd — arbitrary set, called the state space.\n• A — a set, called the action space, either\n– discrete: |A| < +∞, or\n– continuous domain: A = [−1, 1]m.\n• T — transition probability p(s′ | s, a), where s, s′ ∈S, a ∈A.\n• r : S →R — reward function.\n• s0 ∈S — starting state.\nIt is important to notice that in the most general case the only things available for RL algorithm\nbeforehand are d (dimension of state space) and action space A. The only possible way of collecting\nmore information for agent is to interact with provided environment and observe s0. It is obvious\nthat the ﬁrst choice of action a0 will be probably random. While the environment responds by\nsampling s1 ∼p(s1 | s0, a0), this distribution, deﬁned in T and considered to be a part of MDP,\nmay be unavailable to agent’s learning procedure. What agent does observe is s1 and reward signal\nr1 := r(s1) and it is the key information gathered by agent from interaction experience.\nDeﬁnition 2. The tuple (st, at, rt+1, st+1) is called transition. Several sequential transitions\nare usually referred to as roll-out. Full track of observed quantities\ns0, a0, r1, s1, a1, r2, s2, a2, r3, s3, a3 . . .\nis called a trajectory.\nIn general case, the trajectory is inﬁnite which means that the interaction process is neverend-\ning. However, in most practical cases the episodic property holds, which basically means that the\ninteraction will eventually come to some sort of an end3. Formally, it can be simulated by the envi-\nronment stucking in the last state with zero probability of transitioning to any other state and zero\nreward signal. Then it is convenient to reset the environment back to s0 to initiate new interaction.\nOne such interaction cycle from s0 till reset, spawning one trajectory of some ﬁnite length T , is\ncalled an episode. Without loss of generality, it can be considered that there exists a set of termi-\nnal states S+, which mark the ends of interactions. By convention, transitions (st, at, rt+1, st+1)\nare accompanied with binary ﬂag donet+1 ∈{0, 1}, whether st+1 belongs to S+. As timestep t\nat which the transition was gathered is usually of no importance, transitions are often denoted as\n(s, a, r′, s′, done) with primes marking the «next timestep».\nNote that the length of episode T may vary between diﬀerent interactions, but the episodic\nproperty holds if interaction is guaranteed to end after some ﬁnite time T max. If this is not the case,\nthe task is called continuing.\n2.3.\nObjective\nIn reinforcement learning, the agent’s goal is to maximize a cumulative reward. In episodic case,\nthis reward can be expressed as a summation of all received reward signals during one episode and\n2the reward function is often introduced as stochastic and dependent on action a, i. e. R(r | s, a): S × A →P(R),\nwhile instead of ﬁxed s0 a distribution over S is given. Both extensions can be taken into account in terms of presented\ndeﬁnition by extending the state space and incorporating all the uncertainty into transition probability T.\n3natural examples include the end of the game or agent’s failure/success in completing some task.\n6\nis called the return:\nR :=\nT\nX\nt=1\nrt\n(1)\nNote that this quantity is formally a random variable, which depends on agent’s choices and the\noutcomes of environment transitions. As this stochasticity is an inevitable part of interaction process,\nthe underlying distribution from which rt is sampled must be properly introduced to set rigorously\nthe task of return maximization.\nDeﬁnition 3. Agent’s algorithm for choosing a by given current state s, which in general can be\nviewed as distribution π(a | s) on domain A, is called a policy (strategy).\nDeterministic policy, when the policy is represented by deterministic function π : S →A, can\nbe viewed as a particular case of stochastic policy with degenerated policy π(a | s), when agent’s\noutput is still a distribution with zero probability to choose an action other than π(s). In both cases\nit is considered that agent sends to environment a sample a ∼π(a | s).\nNote that given some policy π(a | s) and transition probabilities T, the complete interaction\nprocess becomes deﬁned from probabilistic point of view:\nDeﬁnition 4. For given MDP and policy π, the probability of observing\ns0, a0, s1, a1, s2, a2 . . .\nis called trajectory distribution and is denoted as Tπ:\nTπ :=\nY\nt=0\np(st+1 | st, at)π(at | st)\nIt is always substantial to keep track of what policy was used to collect certain transitions (roll-outs\nand episodes) during the learning procedure, as they are essentially samples from corresponding\ntrajectory distribution. If the policy is modiﬁed in any way, the trajectory distribution changes either.\nNow when a policy induces a trajectory distribution, it is possible to formulate a task of expected\nreward maximization:\nETπ\nT\nX\nt=1\nrt →max\nπ\nTo ensure the ﬁniteness of this expectation and avoid the case when agent is allowed to gather\ninﬁnite reward, limit on absolute value of rt can be assumed:\n|rt| ≤Rmax\nTogether with the limit on episode length T max this restriction guarantees ﬁniteness of optimal\n(maximal) expected reward.\nTo extend this intuition to continuing tasks, the reward for each next interaction step is multiplied\non some discount coeﬃcient γ ∈[0, 1), which is often introduced as part of MDP. This corresponds\nto the logic that with probability 1 −γ agent «dies» and does not gain any additional reward, which\nmodels the paradigm «better now than later». In practice, this discount factor is set very close to 1.\nDeﬁnition 5. For given MDP and policy π the discounted expected reward is deﬁned as\nJ(π) := ETπ\nX\nt=0\nγtrt+1\nReinforcement learning task is to ﬁnd an optimal policy π∗, which maximizes the discounted\nexpected reward:\nJ(π) →max\nπ\n(2)\n7\n2.4.\nValue functions\nSolving reinforcement learning task (2) usually leads to a policy, that maximizes the expected\nreward not only for starting state s0, but for any state s ∈S. This follows from the Markov property:\nthe reward which is yet to be collected from some step t does not depend on previous history and\nfor agent staying at state s the task of behaving optimal is equivalent to maximization of expected\nreward with current state s as a starting state. This is the particular reason why many reinforcement\nlearning algorithms do not seek only optimal policy, but additional information about usefulness of\neach state.\nDeﬁnition 6. For given MDP and policy π the value function under policy π is deﬁned as\nV π(s) := ETπ|s0=s\nX\nt=0\nγtrt+1\nThis value function estimates how good it is for agent utilizing strategy π to visit state s and\ngeneralizes the notion of discounted expected reward J(π) that corresponds to V π(s0).\nAs value function can be induced by any policy, value function V π∗(s) under optimal policy π∗\ncan also be considered. By convention4, it is denoted as V ∗(s) and is called an optimal value func-\ntion.\nObtaining optimal value function V ∗(s) doesn’t provide enough information to reconstruct some\noptimal policy π∗due to unknown world dynamics, i. e. transition probabilities. In other words, be-\ning blind to what state s may be the environment’s response on certain action in a given state makes\nknowing optimal value function unhelpful. This intuition suggests to introduce a similar notion com-\nprising more information:\nDeﬁnition 7. For given MDP and policy π the quality function (Q-function) under policy π is\ndeﬁned as\nQπ(s, a) := ETπ|s0=s,a0=a\nX\nt=0\nγtrt+1\nIt directly follows from the deﬁnitions that these two functions are deeply interconnected:\nQπ(s, a) = Es′∼p(s′|s,a) [r(s′) + γV π(s′)]\n(3)\nV π(s) = Ea∼π(a|s)Qπ(s, a)\n(4)\nThe notion of optimal Q-function Q∗(s, a) can be introduced analogically. But, unlike value\nfunction, obtaining Q∗(s, a) actually means solving a reinforcement learning task: indeed,\nProposition 3. If Q∗(s, a) is a quality function under some optimal policy, then\nπ∗(s) = argmax\na\nQ∗(s, a)\nis an optimal policy.\nThis result implies that instead of searching for optimal policy π∗, an agent can search for optimal\nQ-function and derive the policy from it.\nProposition 4. For any MDP existence of optimal policy leads to existence of deterministic optimal\npolicy.\n4though optimal policy may not be unique, the value functions under any optimal policy that behaves optimally from any\ngiven state (not only s0) coincide. Yet, optimal policy may not know optimal behaviour for some states if it knows how to\navoid them with probability 1.\n8\n2.5.\nClasses of algorithms\nReinforcement learning algorithms are presented in a form of computational procedures specify-\ning a strategy of collecting interaction experience and obtaining a policy with as higher J(π) as pos-\nsible. They rarely include a stopping criterion like in classic optimization methods as the stochasticity\nof given setting prevents any reasonable veriﬁcation of optimality; usually the number of iterations\nto perform is determined by the amount of computational resources. All reinforcement learning\nalgorithms can be roughly divided into four5 classes:\n• meta-heuristics: this class of algorithms treats the task as black-box optimization with zeroth-\norder oracle. They usually generate a set of policies π1 . . . πP and launch several episodes\nof interaction for each to determine best and worst policies according to average return. After\nthat they try to construct more optimal policies using evolutionary or advanced random search\ntechniques [17].\n• policy gradient: these algorithms directly optimize (2), trying to obtain π∗and no additional\ninformation about MDP, using approximate estimations of gradient with respect to policy pa-\nrameters. They consider RL task as an optimization with stochastic ﬁrst-order oracle and make\nuse of interaction structure to lower the variance of gradient estimations. They will be dis-\ncussed in sec. 5.\n• value-based algorithms construct optimal policy implicitly by gaining an approximation of op-\ntimal Q-function Q∗(s, a) using dynamic programming. In DRL, Q-function is represented with\nneural network and an approximate dynamic programming is performed using reduction to\nsupervised learning. This framework will be discussed in sec. 3 and 4.\n• model-based algorithms exploit learned or given world dynamics, i. e.\ndistributions p(s′ |\ns, a) from T. The class of algorithms to work with when the model is explicitly provided is\nrepresented by such algorithms as Monte-Carlo Tree Search; if not, it is possible to imitate the\nworld dynamics by learning the outputs of black box from interaction experience [10].\n2.6.\nMeasurements of performance\nAchieved performance (score) from the point of average cumulative reward is not the only one\nmeasure of RL algorithm quality. When speaking of real-life robots, the required number of simu-\nlated episodes is always the biggest concern. It is usually measured in terms of interaction steps\n(where step is one transition performed by environment) and is referred to as sample eﬃciency.\nWhen the simulation is more or less cheap, RL algorithms can be viewed as a special kind of\noptimization procedures. In this case, the ﬁnal performance of the found policy is opposed to re-\nquired computational resources, measured by wall-clock time. In most cases RL algorithms can be\nexpected to ﬁnd better policy after more iterations, but the amount of these iterations tend to be\nunjustiﬁed.\nThe ratio between amount of interactions and required wall-clock time for one update of policy\nvaries signiﬁcantly for diﬀerent algorithms. It is well-known that model-based algorithms tend to\nhave the greatest sample-eﬃciency at the cost of expensive update iterations, while evolutionary\nalgorithms require excessive amounts of interactions while providing massive resources for paral-\nlelization and reduction of wall-clock time. Value-based and policy gradient algorithms, which will be\nthe focus of our further discussion, are known to lie somewhere in between.\n5in many sources evolutionary algorithms are bypassed in discussion as they do not utilize the structure of RL task in any\nway.\n9\n3.\nValue-based algorithms\n3.1.\nTemporal Diﬀerence learning\nIn this section we consider temporal diﬀerence learning algorithm [23, Chapter 6], which is a\nclassical Reinforcement Learning method in the base of modern value-based approach in DRL.\nThe ﬁrst idea behind this algorithm is to search for optimal Q-function Q∗(s, a) by solving a\nsystem of recursive equations which can be derived by recalling interconnection between Q-function\nand value function (3):\nQπ(s, a) = Es′∼p(s′|s,a) [r(s′) + γV π(s′)] =\n= {using (4)} = Es′∼p(s′|s,a)\n\u0002\nr(s′) + γEa′∼π(a′|s′)Qπ(s′, a′)\n\u0003\nThis equation, named Bellman equation, remains true for value functions under any policies\nincluding optimal policy π∗:\nQ∗(s, a) = Es′∼p(s′|s,a)\n\u0002\nr(s′) + γEa′∼π(a′|s′)Q∗(s′, a′)\n\u0003\n(5)\nRecalling proposition 3, optimal (deterministic) policy can be represented as π∗(s) = argmax\na\nQ∗(s, a). Substituting this for π∗(s) in (5), we obtain fundamental Bellman optimality equation:\nProposition 5. (Bellman optimality equation)\nQ∗(s, a) = Es′∼p(s′|s,a)\nh\nr(s′) + γ max\na′\nQ∗(s′, a′)\ni\n(6)\nThe straightforward utilization of this result is as follows. Consider the tabular case, when both\nstate space S and action space A are ﬁnite (and small enough to be listed in computer memory).\nLet us also assume for now that transition probabilities are available to training procedure. Then\nQ∗(s, a) : S × A →R can be represented as a ﬁnite table with |S||A| numbers. In this case (6)\njust gives a set of |S||A| equations for this table to satisfy.\nAddressing the values of the table as unknown variables, this system of equations can be solved\nusing basic point iteration method: let Q∗\n0(s, a) be initial arbitrary values of table (with the only\nexception that for terminal states s ∈S+, if any, Q∗\n0(s, a) = 0 for all actions a). On each iteration t\nthe table is updated by substituting current values of the table to the right side of equation until the\nprocess converges:\nQ∗\nt+1(s, a) = Es′∼p(s′|s,a)\nh\nr(s′) + γ max\na′\nQ∗\nt (s′, a′)\ni\n(7)\nThis straightforward approach of learning the optimal Q-function, named Q-learning, has been\nextensively studied in classical Reinforcement Learning. One of the central results is presented in\nthe following convergence theorem:\nProposition 6. Let by B denote an operator (S × A →R) →(S × A →R), updating Q∗\nt as in\n(7):\nQ∗\nt+1 = BQ∗\nt\nfor all state-action pairs s, a.\nThen B is a contraction mapping, i. .e. for any two tables Q1, Q2 ∈(S × A →R)\n∥BQ1 −BQ2∥∞≤γ∥Q1 −Q2∥∞\nTherefore, there is a unique ﬁxed point of the system of equations (7) and the point iteration method\nconverges to it.\nThe contraction mapping property is actually of high importance. It demonstrates that the point\niteration algorithm converges with exponential speed and requires small amount of iterations. As\nthe true Q∗is a ﬁxed point of (6), the algorithm is guaranteed to yield a correct answer. The trick is\n10\nthat each iteration demands full pass across all state-action pairs and exact computation of expec-\ntations over transition probabilities.\nIn general case, these expectations can’t be explicitly computed. Instead, agent is restricted to\nsamples from transition probabilities gained during some interaction experience. Temporal Diﬀer-\nence (TD)6 algorithm proposes to collect this data using πt = argmax\na\nQ∗\nt (s, a) ≈π∗and after\neach gathered transition (st, at, rt+1, st+1) update only one cell of the table:\nQ∗\nt+1(s, a) =\n\n\n\n(1 −αt)Q∗\nt (s, a) + αt\nh\nrt+1 + γ max\na′\nQ∗\nt (st+1, a′)\ni\nif s = st, a = at\nQ∗\nt (s, a)\nelse\n(8)\nwhere αt ∈(0, 1) plays the role of exponential smoothing parameter for estimating expectation\nEs′∼p(s′|st,at)(·) from samples.\nTwo key ideas are introduced in the update formula (8): exponential smoothing instead of exact\nexpectation computation and cell by cell updates instead of updating full table at once. Both are\nrequired to settle Q-learning algorithm for online application.\nAs the set S+ of terminal states in online setting is usually unknown beforehand, a slight modiﬁ-\ncation of update (8) is used. If observed next state s′ turns out to be terminal (recall the convention\nto denote this by ﬂag done), its value function is known to be equal to zero:\nV ∗(s′) = max\na′\nQ∗(s′, a′) = 0\nThis knowledge is embedded in the update rule (8) by multiplying max\na′\nQ∗\nt (st+1, a′) on (1 −\ndonet+1). For the sake of shortness, this factor is often omitted but should be always present in\nimplementations.\nSecond important note about formula (8) is that it can be rewritten in the following equivalent\nway:\nQ∗\nt+1(s, a) =\n\n\n\nQ∗\nt (s, a) + αt\nh\nrt+1 + γ max\na′\nQ∗\nt (st+1, a′) −Q∗\nt (s, a)\ni\nif s = st, a = at\nQ∗\nt (s, a)\nelse\n(9)\nThe expression in the brackets, referred to as temporal diﬀerence, represents a diﬀerence be-\ntween Q-value Q∗\nt (s, a) and its one-step approximation rt+1 + γ max\na′\nQ∗\nt (st+1, a′), which must be\nzero in expectation for true optimal Q-function.\nThe idea of exponential smoothing allows us to formulate ﬁrst practical algorithm which can work\nin the tabular case with unknown world dynamics:\nAlgorithm 1: Temporal Diﬀerence algorithm\nHyperparameters: αt ∈(0, 1)\nInitialize Q∗(s, a) arbitrary\nOn each interaction step:\n1. select a = argmax\na\nQ∗(s, a)\n2. observe transition (s, a, r′, s′, done)\n3. update table:\nQ∗(s, a) ←Q∗(s, a) + αt\nh\nr′ + (1 −done)γ max\na′\nQ∗(s′, a′) −Q∗(s, a)\ni\nIt turns out that under several assumptions on state visitation during interaction process this\nprocedure holds similar properties in terms of convergence guarantees, which are stated by the\nfollowing theorem:\n6also known as TD(0) due to theoretical generalizations\n11\nProposition 7. [28] Let’s deﬁne\net(s, a) =\n(\nαt\n(s, a) is updated on step t\n0\notherwise\nThen if for every state-action pair (s, a)\n+∞\nX\nt\net(s, a) = ∞\n+∞\nX\nt\net(s, a)2 < ∞\nthe algorithm 1 converges to optimal Q∗with probability 1.\nThis theorem states that basic policy iteration method can be actually applied online in the way\nproposed by TD algorithm, but demands «enough exploration» from the strategy of interacting with\nMDP during training. Satisfying this demand remains a unique and common problem of reinforce-\nment learning.\nThe widespread kludge is ε-greedy strategy which basically suggests to choose random action\ninstead of a = argmax\na\nQ∗(s, a) with probability εt. The probability εt is usually set close to 1\nduring ﬁrst interaction iterations and scheduled to decrease to a constant close to 0. This heuristic\nmakes agent visit all states with non-zero probabilities independent of what current approximation\nQ∗(s, a) suggests.\nThe main practical issue with Temporal Diﬀerence algorithm is that it requires table Q∗(s, a) to\nbe explicitly stored in memory, which is impossible for MDP with high state space complexity. This\nlimitation substantially restricted its applicability until its combination with deep neural network was\nproposed.\n3.2.\nDeep Q-learning (DQN)\nUtilization of neural nets to model either a policy or a Q-function frees from constructing task-\nspeciﬁc features and opens possibilities of applying RL algorithms to complex tasks, e. g. tasks with\nimages as input. Video games are classical example of such tasks where raw pixels of screen are\nprovided as state representation and, correspondingly, as input to either policy or Q-function.\nMain idea of Deep Q-learning [14] is to adapt Temporal Diﬀerence algorithm so that update for-\nmula (9) would be equivalent to gradient descent step for training a neural network to solve a certain\nregression task. Indeed, it can be noticed that the exponential smoothing parameter αt resembles\nlearning rate of ﬁrst-order gradient optimization procedures, while the exploration conditions from\ntheorem 7 look identical to restrictions on learning rate of stochastic gradient descent.\nThe key hint is that (9) is actually a gradient descent step in the parameter space of the table\nfunctions family:\nQ∗(s, a, θ) = θs,a\nwhere all θs,a form a vector of parameters θ ∈R|S||A|.\nTo unravel this fact, it is convenient to introduce some notation from regression tasks. First, let’s\ndenote by y the target of our regression task, i. e. the quantity that our model is trying to predict:\ny(s, a) := r(s′) + γ max\na′\nQ∗(s′, a′, θ)\n(10)\nwhere s′ is a sample from p(s′ | s, a) and s, a is input data. In this notation (9) is equivalent to:\nθt+1 = θt + αt [y(s, a) −Q∗(s, a, θt)] es,a\nwhere we multiplied scalar value αt [y(s, a) −Q∗(s, a, θt)] on the following vector es,a\nes,a\ni,j :=\n(\n1\n(i, j) = (s, a)\n0\n(i, j) ̸= (s, a)\nto formulate an update of only one component of θ in a vector form. By this we transitioned to\nupdate in parameter space using Q∗(s, a, θ) = θs,a. Remark that for table functions family the\n12\nderivative of Q∗(s, a, θ) by θ for given input s, a is its one-hot encoding, i. e. exactly es,a:\n∂Q∗(s, a, θ)\n∂θ\n= es,a\n(11)\nThe statement now is that this formula is a gradient descent update for regression with input\ns, a, target y(s, a) and MSE loss function:\nLoss(y(s, a), Q∗(s, a, θt)) = (Q∗(s, a, θt) −y(s, a))2\n(12)\nIndeed:\nθt+1 = θt + αt [y(s, a) −Q∗(s, a, θt)] es,a =\n{(12)} = θt −αt\n∂Loss(y, Q∗(s, a, θt))\n∂Q∗\nes,a\n{(11)} = θt −αt\n∂Loss(y, Q∗(s, a, θt))\n∂Q∗\n∂Q∗(s, a, θt)\n∂θ\n=\n{chain rule} = θt −αt\n∂Loss(y, Q∗(s, a, θt))\n∂θ\nThe obtained result is evidently a gradient descent step formula to minimize MSE loss function\nwith target (10):\nθt+1 = θt −αt\n∂Loss(y, Q∗(s, a, θt))\n∂θ\n(13)\nIt is important that dependence of y from θ is ignored during gradient computation (otherwise\nthe chain rule application with y being dependent on θ is incorrect). On each step of temporal dif-\nference algorithm new target y is constructed using current Q-function approximation, and a new\nregression task with this target is set. For this ﬁxed target one MSE optimization step is done ac-\ncording to (13), and on the next step a new regression task is deﬁned. Though during each step the\ntarget is considered to represent some ground truth like it is in supervised learning, here it provides\na direction of optimization and because of this reason is sometimes called a guess.\nNotice that representation (13) is equivalent to standard TD update (9) with all theoretical results\nremaining while the parametric family Q(s, a, θ) is a table functions family. At the same time, (13)\ncan be formally applied to any parametric function family including neural networks. It must be\ntaken into account that this transition is not rigorous and all theoretical guarantees provided by\ntheorem 7 are lost at this moment.\nFurther on we assume that optimal Q-function is approximated with neural network Q∗\nθ(s, a)\nwith parameters θ. Note that for discrete action space case this network may take only s as input\nand output |A| numbers representing Q∗\nθ(s, a1) . . . Q∗\nθ(s, a|A|), which allows to ﬁnd an optimal\naction in a given state s with a single forward pass through the net. Therefore target y for given\ntransition (s, a, r′, s′, done) can be computed with one forward pass and optimization step can be\nperformed in one more forward7 and one backward pass.\nSmall issue with this straightforward approach is that, of course, it is impractical to train neural\nnetworks with batches of size 1. In [14] it is proposed to use experience replay to store all collected\ntransitions (s, a, r′, s′, done) as data samples and on each iteration sample a batch of standard for\nneural networks training size. As usual, the loss function is assumed to be an average of losses for\neach transition from the batch. This utilization of previously experienced transitions is legit because\nTD algorithm is known to be an oﬀ-policy algorithm, which means it can work with arbitrary transi-\ntions gathered by any agent’s interaction experience. One more important beneﬁt from experience\nreplay is sample decorrelation as consecutive transitions from interaction are often similar to each\nother since agent usually locates at the particular part of MDP.\nThough empirical results of described algorithm turned out to be promising, the behaviour of\nQ∗\nθ values indicated the instability of learning process. Reconstruction of target after each optimiza-\ntion step led to so-called compound error when approximation error propagated from the close-\nto-terminal states to the starting in avalanche manner and could lead to guess being 106 and more\ntimes bigger than the true Q∗value. To address this problem, [14] introduced a kludge known as tar-\nget network, which basic idea is to solve ﬁxed regression problem for K > 1 steps, i. .e. recompute\ntarget every K-th step instead of each.\n7in implementations it is possible to combine s and s′ in one batch and perform these two forward passes «at once».\n13\nTo avoid target recomputation for the whole experience replay, the copy of neural network Q∗\nθ\nis stored, called the target network. Its architecture is the same while weights θ−are a copy of Q∗\nθ\nfrom the moment of last target recomputation8 and its main purpose is to generate targets y for\ngiven current batch.\nCombining all things together and adding ε-greedy strategy to facilitate exploration, we obtain\nclassic DQN algorithm:\nAlgorithm 2: Deep Q-learning (DQN)\nHyperparameters: B — batch size, K — target network update frequency, ε(t) ∈(0, 1] —\ngreedy exploration parameter, Q∗\nθ — neural network, SGD optimizer.\nInitialize weights of θ arbitrary\nInitialize θ−←θ\nOn each interaction step:\n1. select a randomly with probability ε(t), else a = argmax\na\nQ∗\nθ(s, a)\n2. observe transition (s, a, r′, s′, done)\n3. add observed transition to experience replay\n4. sample batch of size B from experience replay\n5. for each transition T from the batch compute target:\ny(T ) = r(s′) + γ max\na′\nQ∗(s′, a′, θ−)\n6. compute loss:\nLoss = 1\nB\nX\nT\n(Q∗(s, a, θ) −y(T ))2\n7. make a step of gradient descent using ∂Loss\n∂θ\n8. if t mod K = 0: θ−←θ\n3.3.\nDouble DQN\nAlthough target network successfully prevented Q∗\nθ from unbounded growth and empirically sta-\nbilized learning process, the values of Q∗\nθ on many domains were evident to tend to overestimation.\nThe problem is presumed to reside in max operation in target construction formula (10):\ny = r(s′) + γ max\na′\nQ∗(s′, a′, θ−)\nDuring this estimation max shifts Q-value estimation towards either to those actions that led to high\nreward due to luck or to the actions with overestimating approximation error.\nThe solution proposed in [25] is based on idea of separating action selection and action evalua-\ntion to carry out each of these operations using its own approximation of Q∗:\nmax\na′\nQ∗(s′, a′, θ−) = Q∗(s′, argmax\na′\nQ∗(s′, a′, θ−), θ−) ≈\n≈Q∗(s′, argmax\na′\nQ∗(s′, a′, θ−\n1 ), θ−\n2 )\nThe simplest, but expensive, implementation of this idea is to run two independent DQN («Twin\nDQN») algorithms and use the twin network to evaluate actions:\ny1 = r(s′) + γQ∗\n1(s′, argmax\na′\nQ∗\n2(s′, a′, θ−\n2 ), θ−\n1 )\n8alternative, but more computationally expensive option, is to update target network weights on each step using exponen-\ntial smoothing\n14\ny2 = r(s′) + γQ∗\n2(s′, argmax\na′\nQ∗\n1(s′, a′, θ−\n1 ), θ−\n2 )\nIntuitively, each Q-function here may prefer lucky or overestimated actions, but the other Q-function\njudges them according to its own luck and approximation error, which may be as underestimating\nas overestimating. Ideally these two DQNs should not share interaction experience to achieve that,\nwhich makes such algorithm twice as expensive both in terms of computational cost and sample\neﬃciency.\nDouble DQN [25] is more compromised option which suggests to use current weights of network\nθ for action selection and target network weights θ−for action evaluation, assuming that when the\ntarget network update frequency K is big enough these two networks are suﬃciently diﬀerent:\ny = r(s′) + γQ∗(s′, argmax\na′\nQ∗(s′, a′, θ), θ−)\n3.4.\nDueling DQN\nAnother issue with DQN algorithm 2 emerges when a huge part of considered MDP consists of\nstates of low optimal value V ∗(s), which is an often case. The problem is that when the agent visits\nunpromising state instead of lowering its value V ∗(s) it remembers only low pay-oﬀfor performing\nsome action a in it by updating Q∗(s, a). This leads to regular returns to this state during future\ninteractions until all actions prove to be unpromising and all Q∗(s, a) are updated. The problem\ngets worse when the cardinality of action space is high or there are many similar actions in action\nspace.\nOne beneﬁt of deep reinforcement learning is that we are able to facilitate generalization across\nactions by specifying the architecture of neural network. To do so, we need to encourage the learn-\ning of V ∗(s) from updates of Q∗(s, a). The idea of dueling architecture [27] is to incorporate\napproximation of V ∗(s) explicitly in computational graph. For that purpose we need the deﬁnition\nof advantage function:\nDeﬁnition 8. For given MDP and policy π the advantage function under policy π is deﬁned as\nAπ(s, a) := Qπ(s, a) −V π(s)\n(14)\nAdvantage function is evidently interconnected with Q-function and value function and actually\nshows the relative advantage of selecting action a comparing to average performance of the policy.\nIf for some state Aπ(s, a) > 0, then modifying π to select a more often in this particular state will\nlead to better policy as its average return will become bigger than initial V π(s). This follows from\nthe following property of arbitrary advantage function:\nEa∼π(a|s)Aπ(s, a) = Ea∼π(a|s) [Qπ(s, a) −V π(s)] =\n= Ea∼π(a|s)Qπ(s, a) −V π(s) =\n{using (4)} = V π(s) −V π(s) = 0\n(15)\nDeﬁnition of optimal advantage function A∗(s, a) is analogous and allows us to reformulate\nQ∗(s, a) in terms of V ∗(s) and A∗(s, a):\nQ∗(s, a) = V ∗(s) + A∗(s, a)\n(16)\nStraightforward utilization of this decomposition is following: after several feature extracting lay-\ners the network is joined with two heads, one outputting single scalar V ∗(s) and one outputting\n|A| numbers A∗(s, a) like it was done in DQN for Q-function. After that this scalar value estimation\nis added to all components of A∗(s, a) in order to obtain Q∗(s, a) according to (16). The problem\nwith this naive approach is that due to (15) advantage function can not be arbitrary and must hold\nthe property (15) for Q∗(s, a) to be identiﬁable.\nThis restriction (15) on advantage function can be simpliﬁed for the case when optimal policy is\n15\ninduced by optimal Q-function:\n0 = Ea∼π∗(a|s)Q∗(s, a) −V ∗(s) =\n= Q∗(s, argmax\na\nQ∗(s, a)) −V ∗(s) =\n= max\na\nQ∗(s, a) −V ∗(s) =\n= max\na\n[Q∗(s, a) −V ∗(s)] =\n= max\na\nA∗(s, a)\nThis condition can be easily satisﬁed in computational graph by subtracting max\na\nA∗(s, a) from\nadvantage head. This will be equivalent to the following formula of dueling DQN:\nQ∗(s, a) = V ∗(s) + A∗(s, a) −max\na\nA∗(s, a)\n(17)\nThe interesting nuance of this improvement is that after evaluation on Atari-57 authors discov-\nered that substituting max operation in (17) with averaging across actions led to better results (while\nusage of unidentiﬁable formula (16) led to poor performance). Although gradients can be backprop-\nagated through both operation and formula (17) seems theoretically justiﬁed, in practical implemen-\ntations averaging instead of maximum is widespread.\n3.5.\nNoisy DQN\nBy default, DQN algorithm does not concern the exploration problem and is always augmented\nwith ε-greedy strategy to force agent to discover new states. This baseline exploration strategy\nsuﬀers from being extremely hyperparameter-sensitive as early decrease of ε(t) to close to zero\nvalues may lead to stucking in local optima, when agent is unable to explore new options due to\nimperfect Q∗, while high values of ε(t) force agent to behave randomly for excessive amount of\nepisodes, which slows down learning. In other words, ε-greedy strategy transfers responsibility to\nsolve exploration-exploitation trade-oﬀon engineer.\nThe key reason why ε-greedy exploration strategy is relatively primitive is that exploration priority\ndoes not depend on current state. Intuitively, the choice whether to exploit knowledge by selecting\napproximately optimal action or to explore MDP by selecting some other depends on how explored\nthe current state s is. Discovering a new part of state space after any amount of interaction probably\nindicates that random actions are good to try there, while close-to-initial states will probably be\nsuﬃciently explored after several ﬁrst episodes.\nIn ε-greedy strategy agent selects action using deterministic Q∗(s, a, θ) and only afterwards in-\njects state-independent noise in a form of ε(t) probability of choosing random action. Noisy net-\nworks [4] were proposed as a simple extension of DQN to provide state-dependent and parameter-\nfree exploration by injecting noise of trainable volume to all (or most9) nodes in computational graph.\nLet a linear layer with m inputs and n outputs in q-network perform the following computation:\ny(x) = W x + b\nwhere x ∈Rm is input, W\n∈Rn×m — weights matrix, b ∈Rm — bias.\nIn noisy layers it\nis proposed to substitute deterministic parameters with samples from N (µ, σ) where µ, σ are\ntrained with gradient descent10. On the forward pass through the noisy layer we sample εW ∼\nN (0, Inm×nm), εb ∼N (0, In×n) and then compute\nW = (µW + σW ⊙εW )\nb = (µb + σb ⊙εb)\ny(x) = W x + b\nwhere ⊙denotes element-wise multiplication, µW , σW ∈Rn×m, µb, σb ∈Rn — trainable param-\neters of the layer. Note that the number of parameters for such layers is doubled comparing to\nordinary layers.\n9usually it is not injected in very ﬁrst layers responsible for feature extraction like convolutional layers in networks for\nimages as input.\n10using standard reparametrization trick\n16\nAs the output of q-network now becomes a random variable, loss value becomes a random vari-\nable too. Like in similar models for supervised learning, on each step an expectation of loss function\nover noise is minimized:\nEε Loss(θ, ε) →min\nθ\nThe gradient in this setting can be estimated using Monte-Carlo:\n∇θEε Loss(θ, ε) = Eε∇θ Loss(θ, ε) ≈∇θ Loss(θ, ε)\nε ∼N (0, I)\nIt can be seen that amount of noise actually inﬂicting output of network may vary for diﬀerent\ninputs, i. e. for diﬀerent states. There are no guarantees that this amount will reduce as the inter-\naction proceeds; the behaviour of average magnitude of noise injected in the network with time is\nreported to be extremely sensitive to initialization of σW , σb and vary from MDP to MDP.\nOne technical issue with noisy layers is that on each pass an excessive amount (by the number\nof network parameters) of noise samples is required. This may substantially reduce computational\neﬃciency of forward pass through the network. For optimization purposes it is proposed to ob-\ntain noise for weights matrices in the following way: sample just n + m noise samples ε1\nW\n∼\nN (0, Im×m), ε2\nW ∼N (0, In×n) and acquire matrix noise in a factorized form:\nεW = f(ε1\nW )f(ε2\nW )T\nwhere f is a scaling function, e. g. f(x) = sign(x)\np\n|x|. The beneﬁt of this procedure is that it\nrequires m + n samples instead of mn, but sacriﬁces the interlayer independence of noise.\n3.6.\nPrioritized experience replay\nIn DQN each batch of transitions is sampled from experience replay using uniform distribution,\ntreating collected data as equally prioritized. In such scheme states for each update come from the\nsame distribution as they come from interaction experience (except that they become decorellated),\nwhich agrees with TD algorithm as the basement of DQN.\nIntuitively observed transitions vary in their importance. At the beginning of training most guesses\ntend to be more or less random as they rely on arbitrarily initialized Q∗\nθ and the only source of\ntrusted information are transitions with non-zero received reward, especially near terminal states\nwhere V ∗\nθ (s′) is known to be equal to 0. In the midway of training, most of experience replay is ﬁlled\nwith the memory of interaction within well-learned part of MDP while the most crucial information is\ncontained in transitions where agent explored new promising areas and gained novel reward yet to\nbe propagated through Bellman equation. All these signiﬁcant transitions are drowned in collected\ndata and rarely appear in sampled batches.\nThe central idea of prioritized experience replay [18] is that priority of some transition T =\n(s, a, r′, s′, done) is proportional to temporal diﬀerence:\nρ(T ) := y(T ) −Q∗(s, a, θ) =\np\nLoss(y(T ), Q∗(s, a, θ))\n(18)\nUsing these priorities as proxy of transition importances, sampling from experience replay proceeds\nusing following probabilities:\nP(T ) ∝ρ(T )α\nwhere hyperparameter α ∈R+ controls the degree to which the sampling weights are sparsiﬁed:\nthe case α = 0 corresponds to uniform sampling distribution while α = +∞is equivalent to\ngreedy sampling of transitions with highest priority.\nThe problem with (18) claim is that each transition’s priority changes after each network update.\nAs it is impractical to recalculate loss for the whole data after each step, some simpliﬁcations must\nbe put up with. The straightforward option is to update priority only for sampled transitions in\nthe current batch. New transitions can be added to experience replay with highest priority, i. e.\nmax\nT\nρ(T )11.\nSecond debatable issue of prioritized replay is that it actually substitutes loss function of DQN\nupdates, which assumed uniform sampling of visited states to ensure they come from state visitation\ndistribution:\nET ∼Uniform Loss(T ) →min\nθ\n11which can be computed online with O(1) complexity\n17\nWhile it is not clear what distribution is better to sample from to ensure exploration restrictions of\ntheorem 7, prioritized experienced replay changes this distribution in uncontrollable way. Despite\nits fruitfulness at the beginning and midway of training process, this distribution shift may destabi-\nlize learning close to the end and make algorithm stuck with locally optimal policy. Since formally\nthis issue is about estimating an expectation over one probability with preference to sample from\nanother one, the standard technique called importance sampling can be used as countermeasure:\nET ∼Uniform Loss(T ) =\nM\nX\ni=0\n1\nM Loss(Ti) =\n=\nM\nX\ni=0\nP(Ti)\n1\nMP(Ti) Loss(Ti) =\n= ET ∼P(T )\n1\nMP(T ) Loss(T )\nwhere M is a number of transitions stored in experience replay memory. Importance sampling\nimplies that we can avoid distribution shift that introduces undesired bias by making smaller gradient\nupdates for signiﬁcant transitions which now appear in the batches with higher frequency. The price\nfor bias elimination is that importance sampling weights lower prioritization eﬀect by slowing down\nlearning of highlighted new information.\nThis duality resembles trade-oﬀbetween bias and variance, but important moment here is that\ndistribution shift does not cause any seeming issues at the beginning of training when agent behaves\nclose to random and do not produce valid state visitation distribution anyway. The idea proposed\nin [18] based on this intuition is to anneal the importance sampling weights so they correct bias\nproperly only towards the end of training procedure.\nLossprioritizedER = ET ∼P(T )\n\u0012\n1\nBP(T )\n\u0013β(t)\nLoss(T )\nwhere β(t) ∈[0, 1] and approaches 112 as more interaction steps are executed. If β(t) is set to 0,\nno bias correction is held, while β(t) = 1 corresponds to unbiased loss function, i. e. equivalent to\nsampling from uniform distribution.\nThe most signiﬁcant and obvious drawback of prioritized experience replay approach is that it\nintroduces additional hyperparameters. Although α represents one number, algorithm’s behaviour\nmay turn out to be sensitive to its choosing, and β(t) must be designed by engineer as some sched-\nuled motion from something near 0 to 1, and its well-turned selection may require inaccessible\nknowledge about how many steps it will take for algorithm to «warm up».\n3.7.\nMulti-step DQN\nOne more widespread modiﬁcation of Q-learning in RL community is substituting one-step ap-\nproximation present in Bellman optimality equation (6) with N-step:\nProposition 8. (N-step Bellman optimality equation)\nQ∗(s0, a0) = ETπ∗|s0,a0\n\" N\nX\nt=1\nγt−1r(st) + γN max\naN Q∗(sN, aN)\n#\n(19)\nIndeed, deﬁnition of Q∗(s, a) consists of average return and can be viewed as making T max\nsteps from state s0 after selecting action a0, while vanilla Bellman optimality equation represents\nQ∗(s, a) as reward from one next step in the environment and estimation of the rest of trajectory\nreward recursively. N-step Bellman equation (19) generalizes these two opposites.\nAll the same reasoning as for DQN can be applied to N-step Bellman equation to obtain N-step\nDQN algorithm, which only modiﬁcation appears in target computation:\ny(s0, a0) =\nN\nX\nt=1\nγt−1r(st) + γN max\naN Q∗(sN, aN, θ)\n(20)\n12often it is initialized by a constant close to 0 and is linearly increased until it reaches 1\n18\nTo perform this computation, we are required to obtain for given state s and a not only one next\nstep, but N steps. To do so, instead of transitions N-step roll-outs are stored, which can be done by\nprecomputing following tuples:\nT =\n \ns, a,\nN\nX\nn=1\nγn−1r(n), s(N), done\n!\nwhere r(n) is the reward received in n steps after visitation of considered state s, s(N) is state visited\nin N steps, and done is a ﬂag whether the episode ended during N-step roll-out13. All other aspects\nof algorithm remain the same in practical implementations, and the case N = 1 corresponds to\nstandard DQN.\nThe goal of using N > 1 is to accelerate propagation of reward from terminal states backwards\nthrough visited states to s0 as less update steps will be required to take into account freshly ob-\nserved reward and optimize behaviour at the beginning of episodes. The price is that formula (20)\nincludes an important trick: to calculate such target, for second (and following) step action a′ must\nbe sampled from π∗for Bellman equation (19) to remain true. In other words, application of N-step\nQ-learning is theoretically improper when behaviour policy diﬀers from π∗. Note that we do not face\nthis problem in the case N = 1 in which we are required to sample only from transition probability\np(s′ | s, a) for given state-action pair s, a.\nEven considering π∗≈argmax\na\nQ∗(s, a, θ), where Q∗is our current approximation of π∗,\nmakes N-step DQN an on-policy algorithm when for every state-action pair s, a it is preferable to\nsample target using the closest approximation of π∗available. This questions usage of experience\nreplay or at the very least encourages to limit its capacity to store only M max newest transitions\nwith M max being relatively not very big.\nTo see the negative eﬀect of N-step DQN, consider the following toy example. Suppose agent\nmakes a mistake on the second step after s and ends episode with huge negative reward. Then\nin the case N > 2 each time the roll-out starting with this s is sampled in the batch, the value of\nQ∗(s, a, θ) will be updated with this received negative reward even if Q∗(s′, ·, θ) already learned\nnot to repeat this mistake again.\nYet empirical results in many domains demonstrate that raising N from 1 to 2-3 may result in\nsubstantial acceleration of training and positively aﬀect the ﬁnal performance. On the contrary, the\ntheoretical groundlessness of this approach explains its negative eﬀects when N is set too big.\n13all N-step roll-outs must be considered including those terminated at k-th step for k < N.\n19\n4.\nDistributional approach for value-based methods\n4.1.\nTheoretical foundations\nThe setting of RL task inherently carries internal stochasticity of which agent has no substantial\ncontrol. Sometimes intelligent behaviour implies taking risks with severe chance of low episode\nreturn. All this information resides in the distribution of return R (1) as random variable.\nWhile value-based methods aim at learning expectation of this random variable as it is the quan-\ntity we actually care about, in distributional approach [1] it is proposed to learn the whole distri-\nbution of returns. It further extends the information gathered by algorithm about MDP towards\nmodel-based case in which the whole MDP is imitated by learning both reward function r(s) and\ntransitions T, but still restricts itself only to reward and doesn’t intend to learn world model.\nIn this section we discuss some theoretical extensions of temporal diﬀerence ideas in the case\nwhen expectations on both sides of Bellman equation (5) and Bellman optimality equation (6) are\ntaken away.\nThe central object of study in Q-learning was Q-function, which for given state and action returns\nthe expectation of reward. To rewrite Bellman equation not in terms of expectations, but in terms of\nthe whole distributions, we require a corresponding notation.\nDeﬁnition 9. For given MDP and policy π the value distribution of policy π is a random variable\ndeﬁned as\nZπ(s, a) :=\nX\nt=0\nγtrt+1\n\f\f\f s0 = s, a0 = a\nNote that Zπ just represents a random variable which is taken expectation of in deﬁnition of\nQ-function:\nQπ(s, a) = ETπZπ(s, a)\nUsing this deﬁnition of value distribution, Bellman equation can be rewritten to extend the recur-\nsive connection between adjacent states from expectations of returns to the whole distributions of\nreturns:\nProposition 9. (Distributional Bellman Equation) [1]\nZπ(s, a)\nc.d.f.\n=\nr(s′) + γZπ(s′, a′)\n\f\f s′ ∼p(s′ | s, a), a′ ∼π(a′ | s′)\n(21)\nHere we used some auxiliary notation: by\nc.d.f.\n=\nwe mean that cumulative distribution functions of\ntwo random variables to the right and left are equal almost everywhere. Such equations are called\nrecursive distributional equations and are well-known in theoretical probability theory14. By using\n| we describe a sampling procedure for the random variable to the right side of equation: for given\ns, a next state s′ is sampled from transition probability, then a′ is sampled from given policy, then\nrandom variable Zπ(s′, a′) is sampled to calculate a resulting sample r(s′) + γZπ(s′, a′).\nWhile the space of Q-functions Qπ(s, a) ∈S × A →R is ﬁnite, the space of value distributions\nis a space of mappings from state-action pair to continuous distributions:\nZπ(s, a) ∈S × A →P(R)\nand it is important to notice that even in the table-case when state and action spaces are ﬁnite, the\nspace of value distributions is essentially inﬁnite. Crucial moment for us will be that convergence\nproperties now depend on chosen metric15.\nThe choice of metric in S × A →P(R) represents the same issue as in the space of continuous\nrandom variables P(R): if we choose a metric in the latter, we can construct one in the former:\n14to get familiar with this notion, consider this basic example:\nX1\nc.d.f.\n=\nX2\n√\n2\n+ X3\n√\n2\nwhere X1, X2, X3 are random variables coming from N (0, σ2).\n15in ﬁnite spaces it is true that convergence in one metric guarantees convergence to the same point for any other metric.\n20\nProposition 10. If d(X, Y ) is a metric in the space P(R), then\nd(Z1, Z2) :=\nsup\ns∈S,a∈A\nd(Z1(s, a), Z2(s, a))\nis a metric in the space S × A →P(R).\nThe particularly interesting for us example of metric in P(R) will be Wasserstein metric, which\nconcerns only random variables with bounded moments, so we will additionally assume that for all\nstate-action pairs s, a\nEZπ(s, a)p ≤+∞\nare ﬁnite for p ≥1.\nProposition 11. For 1 ≤p ≤+∞for two random variables X, Y on continuous domain with p-\nth bounded moments and cumulative distribution functions FX and FY correspondingly a Wasser-\nstein distance\nWp(X, Y ) :=\n\n\n1\nZ\n0\n\f\f\fF −1\nX (ω) −F −1\nY\n(ω)\n\f\f\f\np\ndω\n\n\n1\np\nW∞(X, Y ) :=\nsup\nω∈[0,1]\n\f\f\fF −1\nX (ω) −F −1\nY\n(ω)\n\f\f\f\nis a metric in the space of random variables with p-th bounded moments.\nThus we can conclude from proposition 10 that maximal form of Wasserstein metric\nW p(Z1, Z2) =\nsup\ns∈S,a∈A\nWp(Z1(s, a), Z2(s, a))\n(22)\nis a metric in the space of value distributions.\nWe now concern convergence properties of point iteration method to solve (21) in order to obtain\nZπ for given policy π, i. e. solve the task of policy evaluation. For that purpose we initialize Zπ\n0 (s, a)\narbitrarily16 and perform the following updates for all state-action pairs s, a:\nZπ\nt+1(s, a)\nc.d.f.\n:= r(s′) + γZπ\nt (s′, a′)\n(23)\nHere we assume that we are able to compute the distribution of random variable on the right side\nknowing π, all transition probabilities T, the distribution of Zπ\nt and reward function. The question\nwhether the sequence {Zπ\nt } converges to Zπ can be given a detailed answer:\nProposition 12. [1] Denote by B the following operator (S × A →P(R)) →(S × A →P(R)),\nupdating Zπ\nt as in (23):\nZπ\nt+1 = BZπ\nt\nfor all state-action pairs s, a.\nThen B is a contraction mapping in W p (22) for 1 ≤p ≤+∞, i.e. for any two value distribu-\ntions Z1, Z2\nW p(BZ1, BZ2) ≤γW p(Z1, Z2)\nHence there is a unique ﬁxed point of system of equations (21) and the point iteration method con-\nverges to it.\nOne more curious theoretical result is that B is in general not a contraction mapping for such dis-\ntances as Kullback-Leibler divergence, Total Variation distance and Kolmogorov distance17. It shows\n16here we consider value distributions from theoretical point of view, assuming that we are able to explicitly store a table of\n|S||A| continuous distributions without any approximations.\n17one more metric for which the contraction property was shown is Cramer metric:\nl2(X, Y ) =\n\n\nZ\nR\n(FX(ω) −FY (ω))2 dω\n\n\n1\n2\nwhere FX, FY are c.d.f. of random variables X, Y correspondingly.\n21\nthat metric selection indeed inﬂuences convergence rate.\nSimilar to traditional value functions, we can deﬁne optimal value distribution Z∗(s, a). Sub-\nstituting18 π∗(s) = argmax\na\nETπ∗Z∗(s, a) into (21), we obtain distributional Bellman optimality\nequation:\nProposition 13. (Distributional Bellman optimality equation)\nZ∗(s, a)\nc.d.f.\n=\nr(s′) + γZ∗(s′, argmax\na′\nETπ∗Z∗(s′, a′))\n\f\f s′ ∼p(s′ | s, a)\n(24)\nNow we concern the same question whether the point iteration method of solving (24) leads to\nsolution Z∗and whether it is a contraction mapping for some metric. The answer turns out to be\nnegative.\nProposition 14. [1] Point iteration for solving (24) may diverge.\nLevel of impact of this result is not completely clear. Point iteration for (24) preserves means\nof distributions, i. e. it will eventually converge to Q∗(s, a) with all theoretical guarantees from\nclassical Q-learning. The reason behind divergence theorems hides in the rest of distributions like\nother moments and situations when equivalent (in terms of average return) actions may lead to\ndiﬀerent higher moments.\n4.2.\nCategorical DQN\nThere are obvious obstacles for practical application of distributional Q-learning following from\ncomplication of working with arbitrary continuous distributions. Usually we are restricted to approx-\nimations inside some family of parametric distributions, so we have to perform a projection step on\neach iteration.\nSecond matter in combining distributional Q-learning with deep neural networks is to take into\naccount that only samples from p(s′ | s, a) are available for each update. To provide a distributional\nanalog of temporal diﬀerence algorithm 9, some analog of exponential smoothing for distributional\nsetting must be proposed.\nCategorical DQN [1] (also referred as c51) provides straightforward design of practical distribu-\ntional algorithm. While DQN was a resemblance of temporal diﬀerence algorithm, Categorical DQN\nattempts to follow the logic of DQN.\nThe concept is as following. The neural network with parameters θ in this setting takes as in-\nput s ∈S and for each action a outputs parameters ζθ(s, a) of distributions of random variable\nZ∗\nθ(s, a). As in DQN, experience replay can be used to collect observed transitions and sample a\nbatch for each update step. For each transition T = (s, a, r′, s′, done) in the batch a guess is\ncomputed:\ny(T )\nc.d.f.\n:= r′ + (1 −done)γZ∗\nθ\n\u0012\ns′, argmax\na′\nEZ∗\nθ(s′, a′)\n\u0013\n(25)\nNote that expectation of Z∗\nθ(s′, a′) is computed explicitly using the form of chosen parametric family\nof distributions and outputted parameters ζθ(s′, a′), as is the distribution of random variable r′ +\n(1 −done)γZ∗\nθ(s′, a′). In other words, in this setting guess y(T ) is also a continuous random\nvariable, distribution of which can be constructed only approximately. As both target and model\noutput are distributions, it is reasonable to design loss function in a form of some divergence D\nbetween y(T ) and Z∗\nθ(s, a):\nLoss(θ) = ET D\n\u0000y(T ) ∥Z∗\nθ(s, a)\n\u0001\n(26)\nθt+1 = θt −α∂Loss(θt)\n∂θ\n18to perform this step validly, a clariﬁcation concerning argmax operator deﬁnition must be given. The choice of action a\nreturned by this operator in the cases when several actions lead to the same maximal average returns must not depend on\nZ, as this choice aﬀects higher moments of resulted distribution. To overcome this issue, for example, in the case of ﬁnite\naction space all actions can be enumerated and the optimal action with the lowest index is returned by operator.\n22\nThe particular choice of this divergence must be made with concern that y(T ) is a «sample» from\na full one-step approximation of Z∗\nθ which includes transition probabilities:\nyfull(s, a)\nc.d.f.\n:=\nX\ns′∈S\np(s′ | s, a)y(s, a, r(s′), s′, done(s′))\n(27)\nThis form is precisely the right side of distributional Bellman optimality equation as we just incor-\nporated intermediate sampling of s′ into the value of random variable. In other words, if transition\nprobabilities T were known, the update could be made using distribution of yfull as a target.\nLossfull(θ) = Es,aD(yfull(s, a) ∥Z∗\nθ(s, a))\nThis motivates to choose KL(y(T ) ∥Z∗\nθ(s, a)) (speciﬁcally with this order of arguments) as D\nto exploit the following property (we denote by pX a p.d.f. pf random variable X):\n∇θET KL(yfull(s, a) ∥Z∗\nθ(s, a)) = ∇θ\n\u0014\nET\nZ\nR\n−pyfull(s,a)(ω) log pZ∗\nθ (s,a))(ω)dω + const(θ)\n\u0015\n=\n{using (27)} = ∇θET\nZ\nR\nEs′∼p(s′|s,a) −py(T )(ω) log pZ∗\nθ (s,a))(ω)dω =\n{taking expectation out} = ∇θET Es′∼p(s′|s,a)\nZ\nR\n−py(T )(ω) log pZ∗\nθ (s,a))(ω)dω =\n= ∇θET Es′∼p(s′|s,a) KL\n\u0000y(T ) ∥Z∗\nθ(s, a)\n\u0001\nThis property basically states that gradient of loss function (26) with KL as D is an unbiased\n(Monte-Carlo) estimation of gradient of KL-divergence for «full» distribution (27), which resembles\nthe employment of exponential smoothing in temporal diﬀerence learning. For many other diver-\ngences, including Wasserstein metric, same statement is not true, so their utilization in described\nonline setting will lead to biased gradients and all theory-grounded intuition that algorithm moves\nin the right direction becomes distinctively lost. Moreover, KL-divergence is known to be one of the\neasiest divergences to work with due to its nice smoothness properties and wide prevalence in many\ndeep learning pipelines.\nDescribed above motivation to choose KL-divergence as an actual objective for minimization is\ncontradictory. Theoretical analysis of distributional Q-learning, speciﬁcally theorem 12, though con-\ncerning policy evaluation other than optimal Z∗search, explicitly hints that the process converges\nexponentially fast for Wasserstein metric, while even for precisely made updates in terms of KL-\ndivergence we are not guaranteed to get any closer to true solution.\nMore «practical» defect of KL-divergence is that it demands two comparable distributions to\nshare the same domain. This means that by choosing KL-divergence we pledge to guarantee that\ny(T ) and Z∗\nθ(s, a) in (26) have coinciding support. This emerging restriction seems limiting even\nbeforehand as for episodic MDP value distribution in terminal states is obviously degenerated (their\nsupport consists of one point r(s) which is given all probability mass) which means that our value\ndistribution approximation is basically ensured to never be precise.\nIn Categorical DQN, as follows from the name, the family of distributions is chosen to be cate-\ngorical on the ﬁxed support {z0, z1 . . . zA−1} where A is number of atoms. As no prior informa-\ntion about MDP is given, the basic choice of this support is uniform grid from some Vmin ∈R to\nV max ∈R:\nzi = Vmin +\ni\nA −1(Vmax −Vmin),\ni ∈0, 1, . . . A −1\nThese bounds, though, must be chosen carefully as they implicitly assume\nVmin ≤Z∗(s, a) ≤Vmax\nand if these inequalities are not tight, the approximation will obviously become poor.\nTherefore the neural network outputs A numbers, summing into 1, to represent arbitrary distri-\nbution on this support:\nζi(s, a, θ) := P(Z∗\nθ(s, a) = zi)\nWithin this family of distributions, computation of expectation, greedy action selection and KL-\ndivergence is trivial. One problem hides in target formula (25): while we can compute distribution\ny(T ), its support may in general diﬀer from {z0 . . . zA−1}. To avoid the issue of disjoint supports,\n23\na projection step must be done to ﬁnd the closest to target distribution within the chosen family19.\nTherefore the resulting target used in the loss function is\ny(T )\nc.d.f.\n:= ΠC\n\u0014\nr′ + (1 −done)γZ∗\nθ\n\u0012\ns′, argmax\na′\nEZ∗\nθ(s′, a′)\n\u0013\u0015\nwhere ΠC is projection operator.\nThe resulting practical algorithm, named c51 after categorical distributions with A = 51 atoms,\ninherits ideas of experience replay, ε-greedy exploration and target network from DQN. Empirically,\nthough, usage of target network remains an open question as the chosen family of distributions\nrestricts value approximation from unbounded growth by «clipping» predictions at zA−1 and z0, yet\nit is still considered slightly improving performance.\nAlgorithm 3: Categorical DQN (c51)\nHyperparameters: B — batch size, Vmax, Vmin, A — parameters of support, K — target\nnetwork update frequency, ε(t) ∈(0, 1] — greedy exploration parameter, ζ∗— neural net-\nwork, SGD optimizer.\nInitialize weights θ of neural net ζ∗arbitrary\nInitialize θ−←θ\nPrecompute support grid zi = Vmin +\ni\nA−1(Vmax −Vmin)\nOn each interaction step:\n1. select a randomly with probability ε(t), else a = argmax\na\nP\ni ziζ∗\ni (s, a, θ)\n2. observe transition (s, a, r′, s′, done)\n3. add observed transition to experience replay\n4. sample batch of size B from experience replay\n5. for each transition T from the batch compute target:\nP(y(T ) = r′ + γzi) = ζ∗\ni\n \ns′, argmax\na′\nX\ni\nziζ∗\ni (s′, a′, θ−), θ−\n!\n6. project y(T ) on support {z0, z1 . . . zA−1}\n7. compute loss:\nLoss = 1\nB\nX\nT\nKL(y(T ) ∥Z∗(s, a, θ))\n8. make a step of gradient descent using ∂Loss\n∂θ\n9. if t mod K = 0: θ−←θ\n4.3.\nQuantile Regression DQN (QR-DQN)\nCategorical DQN discovered a gap between theory and practice as KL-divergence, used in prac-\ntical algorithm, is theoretically unjustiﬁed. Theorem 12 hints that the true divergence we should care\nabout is actually Wasserstein metric, but it remained unclear how it could be optimized using only\nsamples from transition probabilities T.\nIn [3] it was discovered that selecting another family of distributions to approximate Z∗\nθ(s, a)\nwill reduce Wasserstein minimization task to the search for quantiles of speciﬁc distributions. The\n19to project a categorical distribution with support {v0, v1 . . . vA−1} on categorical distributions with support\n{z0, z1 . . . zA−1} one can just ﬁnd for each vi the closest two atoms zj ≤vi ≤zj+1 and split all probability mass\nfor vi between zj and zj+1 proportional to closeness. If vi < z0, then all its probability mass is given to z0, same with\nupper bound.\n24\nlatter can be done in online setting using quantile regression technique. This led to alternative\ndistributional Q-learning algorithm named Quantile Regression DQN (QR-DQN).\nThe basic idea is to «swap» ﬁxed support and learned probabilities of Categorical DQN. We will\nnow consider the family with ﬁxed probabilities for A-atomed categorical distribution with arbitrary\nsupport {ζ∗\n0(s, a, θ), ζ∗\n1(s, a, θ), . . . , ζ∗\nA−1(s, a, θ)}. Again, we will assume all probabilities to be\nequal given the absence of any prior knowledge; namely, our distribution family is now\nZ∗\nθ(s, a) ∼Uniform\n\u0010\nζ∗\n0(s, a, θ), . . . , ζ∗\nA−1(s, a, θ)\n\u0011\nIn this setting neural network outputs A arbitrary real numbers that represent the support of uni-\nform categorical distribution20, where A is the number of atoms and the only hyperparameter to\nselect.\nFor table-case setting, on each step of point iteration we desire to update the cell for given state-\naction pair s, a with full distribution of random variable to the right side of (24). If we are limited\nto store only A atoms of the support, the true distribution must be projected on the space of A-\natomed categorical distributions. Consider now this task of projecting some given random variable\nwith c.d.f. F (ω) in terms of Wasserstein distance. Speciﬁcally, we will be interested in minimizing\nW1-distance for p = 1 as the theorem 12 states the contraction property for all 1 ≤p ≤+∞and\nwe are free to choose any:\nZ 1\n0\n\f\f\fF −1(ω) −U −1\nz0,z1...zA−1(ω)\n\f\f\f dω →\nmin\nz0,z1...zA−1\n(28)\nwhere Uz0,z1...zA−1 is c.d.f. for uniform categorical distribution on given support. Its inverse, also\nknown as quantile function, has a following simple form:\nU −1\nz0,z1...zA−1(ω) =\n\n\n\n\n\n\n\n\n\n\n\nz0\n0 ≤ω < 1\nA\nz1\n1\nA ≤ω < 2\nA\n...\nzA−1\nA−1\nA\n≤ω < 1\nSubstituting this into (28)\nA−1\nX\ni=0\nZ\ni+1\nA\ni\nA\n\f\fF −1(ω) −zi\n\f\f dω →\nmin\nz0,z1...zA−1\nsplits the optimization of Wasserstein into A independent tasks that can be solved separately:\nZ\ni+1\nA\ni\nA\n\f\fF −1(ω) −zi\n\f\f dω →min\nzi\n(29)\nProposition 15. [3] Let’s denote\nτi :=\ni\nA + i+1\nA\n2\nThen every solution for (29) satisﬁes F (zi) = τi, i. e. it is τi-th quantile of c. d. f. F .\nThe result 15 states that we require only A speciﬁc quantiles of random variable to the right side\nof Bellman equation21. Hence the last thing to do to design a practical algorithm is to develop a pro-\ncedure of unbiased estimation of quantiles for the random variable on the right side of distribution\nBellman optimality equation (24).\n20Note that target distribution is now guaranteed to remain within this distribution family as multiplying on γ just shrinks\nthe support and adding r′ just shifts it.\nWe assume that if some atoms of the support coincide, the distribution is\nstill A-atomed categorical; for example, for degenerated distribution (like in the case of terminal states) ζ∗\n0(s, a, θ) =\nζ∗\n1(s, a, θ) = · · · = ζ∗\nA−1(s, a, θ). This shows that projection step heuristic is not needed for this particular choice of\ndistribution family.\n21It can be proved that for table-case policy evaluation algorithm which stores in each cell not expectations of reward (as\nin Q-learning) but A quantiles updated according to distributional Bellman equation (21) using theorem 15 converges to\nquantiles of Z∗(s, a) in Wasserstein metric for 1 ≤p ≤+∞and its update operator is a contraction mapping in W∞.\n25\nQuantile regression is the standard technique to estimate the quantiles of empirical distribution\n(i. .e. distribution that is represented by ﬁnite amount of i. i. d. samples from it). Recall from machine\nlearning that the constant solution optimizing l1-loss is median, i. .e. 1\n2-th quantile. This fact can be\ngeneralized to arbitrary quantiles:\nProposition 16. (Quantile Regression) [11] Let’s deﬁne loss as\nLoss(c, X) =\n(\nτ(c −X)\nc ≥X\n(1 −τ)(X −c)\nc < X\nThen solution for\nEX Loss(c, X) →min\nc∈R\n(30)\nis τ-th quantile of distribution of X.\nAs usual in the case of neural networks, it is impractical to optimize (30) until convergence on\neach iteration for each of A desired quantiles τi. Instead just one step of gradient optimization\nis made and the outputs of neural network ζ∗\ni (s, a, θ), which play the role of c in formula (30), are\nmoved towards the quantile estimation via backpropagation. In other words, (30) sets a loss function\nfor network outputs; the losses for diﬀerent quantiles are summed up. The resulting loss is\nLossQR(s, a, θ) =\nA−1\nX\ni=0\nEs′∼p(s′|s,a)Ey∼y(T )\n\u0000τ −I[ζ∗\ni (s, a, θ) < y]\n\u0001 \u0000ζ∗\ni (s, a, θ) −y\n\u0001\n(31)\nwhere I denotes an indicator function. The expectation over y ∼y(T ) for given transition can be\ncomputed in closed form: indeed, y(T ) is also an A-atomed categorical distribution with support\n{r′ + γζ∗\n0(s′, a′), . . . , r′ + γζ∗\nA−1(s′, a′)}, where\na′ = argmax\na′\nEZ∗(s′, a′, θ) = argmax\na′\n1\nA\nX\ni\nζ∗\ni (s′, a′, θ)\nand expectation over transition probabilities, as always, is estimated using Monte-Carlo by sampling\ntransitions from experience replay.\nAlgorithm 4: Quantile Regression DQN (QR-DQN)\nHyperparameters: B — batch size, A — number of atoms, K — target network update fre-\nquency, ε(t) ∈(0, 1] — greedy exploration parameter, ζ∗— neural network, SGD optimizer.\nInitialize weights θ of neural net ζ∗arbitrary\nInitialize θ−←θ\nPrecompute mid-quantiles τi =\ni\nA + i+1\nA\n2\nOn each interaction step:\n1. select a randomly with probability ε(t), else a = argmax\na\n1\nA\nP\ni ζ∗\ni (s, a, θ)\n2. observe transition (s, a, r′, s′, done)\n3. add observed transition to experience replay\n4. sample batch of size B from experience replay\n5. for each transition T from the batch compute the support of target distribution:\ny(T )j = r′ + γζ∗\nj\n \ns′, argmax\na′\n1\nA\nX\ni\nζ∗\ni (s′, a′, θ−), θ−\n!\n26\n6. compute loss:\nLoss =\n1\nBA\nX\nT\nX\ni\nX\nj\n\u0000τi −I[ζ∗\ni (s, a, θ) < y(T )j]\n\u0001 \u0000ζ∗\ni (s, a, θ) −y(T )j\n\u0001\n7. make a step of gradient descent using ∂Loss\n∂θ\n8. if t mod K = 0: θ−←θ\n4.4.\nRainbow DQN\nSuccess of Deep Q-learning encouraged a full-scale research of value-based deep reinforcement\nlearning by studying various drawbacks of DQN and developing auxiliary extensions. In many arti-\ncles some extensions from previous research were already considered and embedded in compared\nalgorithms during empirical studies.\nIn Rainbow DQN [7], seven Q-learning-based ideas are united in one procedure with ablation\nstudies held whether all these incorporated extensions are essentially necessary for resulted RL\nalgorithm:\n• DQN (sec. 3.2)\n• Double DQN (sec. 3.3)\n• Dueling DQN (sec. 3.4)\n• Noisy DQN (sec. 3.5)\n• Prioritized Experience Replay (sec. 3.6)\n• Multi-step DQN (sec. 3.7)\n• Categorical22 DQN (sec. 4.2)\nThere is little ambiguity on how these ideas can be combined; we will discuss several non-\nstraightforward circumstances and provide the full algorithm description after.\nTo apply prioritized experience replay in distributional setting, the measure of transition impor-\ntance must be provided. The main idea is inherited from ordinary DQN where priority is just loss for\nthis transition:\nρ(T ) := Loss(y(T ), Z∗(s, a, θ)) = KL(y(T ) ∥Z∗(s, a, θ))\nTo combine noisy networks with double DQN heuristic, it is proposed to resample noise on each\nforward pass through the network and through its copy for target computation. This decision implies\nthat action selection, action evaluation and network utilization are independent and stochastic (for\nexploration cultivation) steps.\nThe one snagging combination here is categorical DQN and dueling DQN. To merge these ideas,\nwe need to model advantage A∗(s, a, θ) in distributional setting. In Rainbow this is done straight-\nforwardly: the network has two heads, value stream v(s, θ) outputting A real values and advantage\nstream a(s, a, θ) outputting A × |A| real values. Then these streams are integrated using the same\nformula (17) with the only exception being softmax applied across atoms dimension to guarantee\nthat output is categorical distribution:\nζ∗\ni (s, a, θ) ∝exp\n \nv(s, θ)i + a(s, a, θ)i −\n1\n|A|\nX\na\na(s, a, θ)i\n!\n(32)\nCombining lack of intuition behind this integration formula with usage of mean instead of theo-\nretically justiﬁed max makes this element of Rainbow the most questionable. During the ablation\nstudies it was discovered that dueling architecture is the only component that can be removed with-\nout noticeable loss of performance. All other ingredients are believed to be crucial for resulting\nalgorithm as they address diﬀerent problems.\n22Quantile Regression can be considered instead\n27\nAlgorithm 5: Rainbow DQN\nHyperparameters: B — batch size, Vmax, Vmin, A — parameters of support, K — target\nnetwork update frequency, N — multi-step size, α — degree of prioritized experience replay,\nβ(t) — importance sampling bias correction for prioritized experience replay, ζ∗— neural\nnetwork, SGD optimizer.\nInitialize weights θ of neural net ζ∗arbitrary\nInitialize θ−←θ\nPrecompute support grid zi = Vmin +\ni\nA−1(Vmax −Vmin)\nOn each interaction step:\n1. select a = argmax\na\nP\ni ziζ∗\ni (s, a, θ, ε), ε ∼N (0, I)\n2. observe transition (s, a, r′, s′, done)\n3. construct N-step transition T =\n\u0010\ns, a, PN\nn=0 γnr(n+1), s(N), done\n\u0011\nand add it to\nexperience replay with priority maxT ρ(T )\n4. sample batch of size B from experience replay using probabilities P(T ) ∝ρ(T )α\n5. compute weights for the batch (where M is the size of experience replay memory)\nw(T ) =\n\u0012\n1\nMP(T )\n\u0013β(t)\n6. for each transition T = (s, a, ¯r, ¯s, done) from the batch compute target (detached\nfrom computational graph to prevent backpropagation):\nε1, ε2 ∼N (0, I)\nP(y(T ) = ¯r + γNzi) = ζ∗\ni\n \n¯s, argmax\n¯a\nX\ni\nziζ∗\ni (¯s, ¯a, θ, ε1), θ−, ε2\n!\n7. project y(T ) on support {z0, z1 . . . zA−1}\n8. update transition priorities\nρ(T ) ←KL(y(T ) ∥Z∗(s, a, θ, ε)), ε ∼N (0, I)\n9. compute loss:\nLoss = 1\nB\nX\nT\nw(T )ρ(T )\n10. make a step of gradient descent using ∂Loss\n∂θ\n11. if t mod K = 0: θ−←θ\n28\n5.\nPolicy Gradient algorithms\n5.1.\nPolicy Gradient theorem\nAlternative approach to solving RL task is direct optimization of objective\nJ(θ) = ET ∼πθ\nX\nt=1\nγt−1rt →max\nθ\n(33)\nas a function of θ. Policy gradient methods provide a framework how to construct an eﬃcient opti-\nmization procedure based on stochastic ﬁrst-order optimization within RL setting.\nWe will assume that πθ(a | s) is a stochastic policy parameterized with θ ∈Θ. It turns out,\nthat if π is diﬀerentiable by θ, then so is our goal (33). We now proceed to discuss the technique of\nderivative calculation which is based on employment of log-derivative trick:\nProposition 17. For arbitrary distribution π(a) parameterized by θ:\n∇θπ(a) = π(a)∇θ log π(a)\n(34)\nIn most general form, this trick allows us to derive the gradient of expectation of an arbitrary\nfunction f(a, θ) : A × Θ →R, diﬀerentiable by θ, with respect to some distribution πθ(a), also\nparameterized by θ:\n∇θEa∼πθ(a)f(a, θ) = ∇θ\nZ\nA\nπθ(a)f(a, θ)da =\n=\nZ\nA\n∇θ [πθ(a)f(a, θ)] da =\n{product rule} =\nZ\nA\n[∇θπθ(a)f(a, θ) + πθ(a)∇θf(a, θ)] da =\n=\nZ\nA\n∇θπθ(a)f(a, θ)da + Eπθ(a)∇θf(a, θ) =\n{log-derivative trick (34)} =\nZ\nA\nπθ(a)∇θ log πθ(a)f(a, θ)da + Eπθ(a)∇θf(a, θ) =\n= Eπθ(a)∇θ log πθ(a)f(a, θ) + Eπθ(a)∇θf(a, θ)\nThis technique can be applied sequentially (to expectations over πθ(a0 | s0), πθ(a1 | s1) and\nso on) to obtain the gradient ∇θJ(θ).\nProposition 18. (Policy Gradient Theorem) [24] For any MDP and diﬀerentiable policy πθ the\ngradient of objective (33) is\n∇θJ(θ) = ET ∼πθ\nX\nt=0\nγt∇θ log πθ(at | st)Qπ(st, at)\n(35)\nFor future references, we require another form of formula (35), which provides another point of\nview. For this purpose, let us deﬁne a discounted state visitation frequency:\nDeﬁnition 10. For given MDP and given policy π its discounted state visitation frequency is\ndeﬁned by\ndπ(s) := (1 −γ)\nX\nt=0\nγtP(st = s)\nwhere st are taken from trajectories T sampled using given policy π.\nDiscounted state visitation frequencies, if normalized, represent a marginalized probability for\nagent to land in a given state s23. It is rarely attempted to be learned, but it assists theoretical\n23the γt weighting in this deﬁnition is often introduced to incorporate the same reduction of contribution of later states in\nthe whole gradient according to (35). Similar notation is sometimes used for state visitation frequency without discount.\n29\nstudy by allowing us to rewrite expectations over trajectories with separated intrinsic and extrinsic\nrandomness of the decision making process:\n∇θJ(θ) = Es∼dπ(s)Ea∼π(a|s)∇θ log πθ(a | s)Qπ(s, a)\n(36)\nThis form is equivalent to (35) as sampling a trajectory and going through all visited states with\nweights γt induces the same distribution as deﬁned in dπ(s).\nNow, although we acquired an explicit form of objective’s gradient, we are able to compute it only\napproximately, using Monte-Carlo estimation for expectations via sampling one or several trajecto-\nries. Second form of gradient (36) reveals that it is possible to use roll-outs of trajectories without\nwaiting for episode ending, as the states for the roll-outs come from the same distribution as they\nwould for complete episode trajectories24. The essential thing is that exactly the policy π(θ) must be\nused for sampling to obtain unbiased Monte-Carlo estimation (otherwise state visitation frequency\ndπ(s) is diﬀerent). These features are commonly underlined by notation Eπ, which is a shorter form\nof Es∼dπ(s)Ea∼π(a|s). When convenient, we will use it to reduce the gradient to a shorter form:\n∇θJ(θ) = Eπ(θ)∇θ log πθ(a | s)Qπ(s, a)\n(37)\nSecond important thing worth mentioning is that Qπ(s, a) is essentially present in the gradient.\nRemark that it is never available to the algorithm and must also be somehow estimated.\n5.2.\nREINFORCE\nREINFORCE [29] provides a straightforward approach to approximately calculate the gradient (35)\nin episodic case using Monte-Carlo estimation: N games are played and Q-function under policy π\nis approximated with corresponding return:\nQπ(s, a) = ET ∼πθ|s,aR(T ) ≈R(T ),\nT ∼πθ | s, a\nThe resulting formula is therefore the following:\n∇θJ(θ) ≈1\nN\nN\nX\nT\nX\nt=0\n\"\nγt∇θ log πθ(at | st)\n X\nt′=t\nγt′−trt′+1\n!#\n(38)\nThis estimation is unbiased as both approximation of Qπ and approximation of expectation over\ntrajectories are done using Monte-Carlo. Given that estimation of gradient is unbiased, stochastic\ngradient ascent or more advanced stochastic optimization techniques are known to converge to local\noptimum.\nFrom theoretical point of view REINFORCE can be applied straightforwardly for any parametric\nfamily πθ(a | s) including neural networks. Yet the enormous time required for convergence and\nthe problem of stucking in local optimums make this naive approach completely impractical.\nThe main source of problems is believed to be the high variance of gradient estimation (38), as\nthe convergence rate of stochastic gradient descent directly depends on the variance of gradient\nestimation.\nThe standard technique of variance reduction is an introduction of baseline. The idea is to add\nsome term that will not aﬀect the expectation, but may aﬀect the variance. One such baseline can\nbe derived using following reasoning: for any distribution it is true that\nR\nA\nπθ(a | s)da = 1. Taking\nthe gradient ∇θ from both sides, we obtain:\n0 =\nZ\nA\n∇θπθ(a | s)da =\n{log-derivative trick (34)} =\nZ\nA\nπθ(a | s)∇θ log πθ(a | s)da =\n= Eπθ(a|s)∇θ log πθ(a | s)\n24in practice and in most policy gradients algorithms, sampling roll-outs never include γt weights, which formally corre-\nsponds to estimating gradient using incorrect equation («approximation»):\n∇θJ(θ) ≈ET ∼πθ\nX\nt=0\n∇θ log πθ(at | st)Qπ(st, at)\nwhich diﬀers from the correct version (35) in ignoring γt multiplier. On the one hand, it equalizes the contribution of dif-\nferent terms and agrees with intuition, but on the other hand such gradient estimation does not imply optimization of any\nreasonable objective and breaks the idea of straightforward gradient ascent [15].\n30\nMultiplying this expression on some constant, we can scale this baseline:\nEπθ(a|s) const(a)∇θ log πθ(a | s) = 0\nNotice that the constant here must be independent of a, but may depend on s. Application of this\ntechnique to our case provides the following result25:\nProposition 19. For any arbitrary function b(s): S →R, called baseline:\n∇θJ(θ) = ET ∼πθ\nX\nt=0\nγt∇θ log πθ(at | st) (Qπ(st, at) −b(st))\nSelection of the baseline is up to us as long as it does not depend on actions at. The intent is to\nchoose it in a way that minimizes the variance.\nIt is believed that high variance of (38) originates from multiplication of Qπ(s, a), which may have\narbitrary scale (e. .g. in a range [100, 200]) while ∇θ log πθ(at | st) naturally has varying signs26.\nTo reduce the variance, the baseline must be chosen so that absolute values of expression inside\nthe expectation are shifted towards zero. Wherein the optimal baseline is provided by the following\ntheorem:\nProposition 20. The solution for\nVT ∼πθ\nX\nt=0\nγt∇θ log πθ(at | st) (Qπ(st, at) −b(st)) →min\nb(s)\nis given by\nb(s) = Ea∼πθ(a|s)γt∥∇θ log πθ(a | s)∥2\n2Qπ(s, a)\nEa∼πθ(a|s)∥∇θ log πθ(a | s)∥2\n2\n(39)\nAs can be seen, optimal baseline calculation involves expectations which again can only be com-\nputed (in most cases) using Monte-Carlo (both for numerator and denominator). For that purpose,\nfor every visited state s estimations of Qπ(s, a) are needed for all (or some) actions a, as otherwise\nestimation of baseline will coincide with estimation of Qπ(s, a) and collapse gradient to zero. Prac-\ntical utilization of result (39) is to consider a constant baseline independent of s with similar optimal\nform:\nb = ET ∼πθ\nP\nt=0 γt∥∇θ log πθ(at | st)∥2\n2Qπ(st, at)\nET ∼πθ\nP\nt=0 ∥∇θ log πθ(at | st)∥2\n2\nUtilization of some kind of baseline, not necessarily optimal, is known to signiﬁcantly reduce the\nvariance of gradient estimation and is an essential part of any policy gradient method. The ﬁnal step\nto make this family of algorithms applicable when using deep neural networks is to reduce variance\nof Qπ estimation by employing RL task structure like it was done in value-based methods.\n5.3.\nAdvantage Actor-Critic (A2C)\nSuppose that in optimal baseline formula (39) it happens that ∥∇θ log πθ(a | s)∥2\n2 = const(a).\nThough in reality this is actually not true, under this circumstance the optimal baseline formula\nsigniﬁcantly reduces and unravels a close-to-optimal but simple form of baseline:\nb(s) = γtEa∼πθ(a|s)Qπ(s, a) = γtV π(s)\nSubstituting this baseline into gradient formula (37) and recalling the deﬁnition of advantage\nfunction (14), the gradient can now be rewritten as follows:\n∇θJ(θ) = Eπ(θ)∇θ log πθ(a | s)Aπ(s, a)\n(40)\nThis representation of gradient is used as the basement for most policy gradient algorithms as it\noﬀers lower variance while selecting the baseline expressed in terms of value functions which can be\n25this result can be generalized by introducing diﬀerent baselines for estimation of diﬀerent components of ∇θJ(θ).\n26this follows, for example, from baseline derivation.\n31\neﬃciently learned similar to how it was done in value-based methods. Such algorithms are usually\nnamed Actor-Critic as they consist of two neural networks: πθ(a | s), representing a policy, called an\nactor, and V π\nφ (s) with parameters φ, approximately estimating actor’s performance, called a critic.\nNote that the choice of value function to learn can be arbitrary; it is possible to learn Qπ or Aπ\ninstead, as all of them are deeply interconnected. Value function V π is chosen as the simplest one\nsince it depends only on state and thus is hoped to be easier to learn.\nHaving a critic V π\nφ (s), Q-function can be approximated in a following way:\nQπ(s, a) ≈r′ + γV π(s′) ≈r′ + γV π\nφ (s′)\nFirst approximation is done using Monte-Carlo, while second approximation inevitably introduces\nbias. Important thing to notice is that at this moment our gradient estimation stops being unbiased\nand all theoretical guarantees of converging are once again lost.\nAdvantage function therefore can be obtained according to the deﬁnition:\nAπ(s, a) = Qπ(s, a) −V π(s) ≈r′ + γV π\nφ (s′) −V π\nφ (s)\n(41)\nNote that biased estimation of baseline doesn’t make gradient estimation biased by itself, as baseline\ncan be an arbitrary function of state. All bias introduction happens inside the approximation of Qπ.\nIt is possible to use critic only for baseline, which allows complete avoidance of bias, but then the\nonly way to estimate Qπ is via playing several games and using corresponding returns, which suﬀers\nfrom higher variance and low sample eﬃciency.\nThe logic behind training procedure for the critic is taken from value-based methods: for given\npolicy π its value function can be obtained using point iteration for solving\nV π(s) = Ea∼π(a|s)Es′∼p(s′|s,a) [r′ + γV π(s′)]\nSimilar to DQN, on each update a target is computed using current approximation\ny = r′ + γV π\nφ (s′)\nand then MSE is minimized to move values of V π\nφ (s) towards the guess.\nNotice that to compute the target for critic we require samples from the policy π which is being\nevaluated. Although actor evolves throughout optimization process, we assume that one update of\npolicy π does not lead to signiﬁcant change of true V π and thus our critic, which approximates value\nfunction for older version of policy, is close enough to construct the target. But if samples from, for\nexample, old policy are used to compute the guess, the step of critic update will correspond to learn-\ning the value function for old policy other than current. Essentially, this means that both actor and\ncritic training procedures require samples from current policy π, making Actor-Critic algorithm on-\npolicy by design. Consequently, samples that were collected on previous update iterations become\nuseless and can be forgotten. This is the key reason why policy gradient algorithms are usually less\nsample-eﬃcient than value-based.\nNow as we have an approximation of value function, advantage estimation can be done using\none-step transitions (41). As the procedure of training an actor, i. .e. gradient estimation (40), also\ndoes not demand sampling the whole trajectory, each update now requires only a small roll-out to\nbe sampled. The amount of transitions in the roll-out corresponds to the size of mini-batch.\nThe problem with roll-outs is that the data is obviously not i. i. d., which is crucial for training\nnetworks. In value-based methods, this problem was solved with experience replay, but in policy\ngradient algorithms it is essential to collect samples from scratch after each update of the networks\nparameters. The practical solution for simulated environments is to launch several instances of\nenvironment (for example, on diﬀerent cores of multiprocessor) in parallel threads and have several\nparallel interactions. After several steps in each environment, the batch for update is collected by\nuniting transitions from all instances and one synchronous27 update of networks parameters θ and\nφ is performed.\nOne more optimization that can be done is to partially share weights of networks θ and φ. It is\njustiﬁed as ﬁrst layers of both networks correspond to basic features extraction and these features\nare likely to be the same for optimal policy and value function. While it reduces the number of train-\ning parameters almost twice, it might destabilize learning process as the scales of gradient (40) and\n27there is also an asynchronous modiﬁcation of advantage actor critic algorithm (A3C) which accelerates the training process\nby storing a copy of network for each thread and performing weights synchronization from time to time.\n32\ngradient of critic’s MSE loss may be signiﬁcantly diﬀerent, so they should be balanced with additional\nhyperparameter.\nAlgorithm 6: Advantage Actor-Critic (A2C)\nHyperparameters: B — batch size, V ∗\nφ — critic neural network, πθ — actor neural network,\nα — critic loss scaling, SGD optimizer.\nInitialize weights θ, φ arbitrary\nOn each step:\n1. obtain a roll-out of size B using policy π(θ)\n2. for each transition T from the roll-out compute advantage estimation:\nAπ(T ) = r′ + γV π\nφ (s′) −V π\nφ\n3. compute target (detached from computational graph to prevent backpropagation):\ny(T ) = r′ + γV π\nφ (s′)\n4. compute critic loss:\nLoss = 1\nB\nX\nT\n\u0010\ny(T ) −V π\nφ\n\u00112\n5. compute critic gradients:\n∇critic = ∂Loss\n∂φ\n6. compute actor gradient:\n∇actor = 1\nB\nX\nT\n∇θ log πθ(a | s)Aπ(T )\n7. make a step of gradient descent using ∇actor + α∇critic\n5.4.\nGeneralized Advantage Estimation (GAE)\nThere is a design dilemma in Advantage Actor Critic algorithm concerning the choice whether to\nuse the critic to estimate Qπ(s, a) and introduce bias into gradient estimation or to restrict critic em-\nployment only for baseline and cause higher variance with necessity of playing the whole episodes\nfor each update step.\nActually, the range of possibilities is wider. Since Actor-Critic is an on-policy algorithm by design,\nwe are free to use N-step approximations instead of one-step: using\nQπ(s, a) ≈\nN−1\nX\nn=0\nγnr(n+1) + γNV π \u0010\ns(N)\u0011\nwe can deﬁne N-step advantage estimator as\nAπ\n(N)(s, a) :=\nN−1\nX\nn=0\nγnr(n+1) + γNV π\nφ\n\u0010\ns(N)\u0011\n−V π\nφ (s)\nFor N = 1 this estimation corresponds to Actor-Critic one-step estimation with high bias and low\nvariance. For N = ∞it yields the estimator with critic used only for baseline with no bias and\nhigh variance. Intermediate values correspond to something in between. Note that to use N-step\nadvantage estimation we have to perform N steps of interaction after given state-action pair.\n33\nUsually ﬁnding a good value for N as hyperparameter is diﬃcult as its «optimal» value may ﬂoat\nthroughout the learning process. In Generalized Advantage Estimation (GAE) [20] it is proposed to\nconstruct an ensemble out of diﬀerent N-step advantage estimators using exponential smoothing\nwith some hyperparameter λ:\nAπ\nGAE(s, a) := (1 −λ)\n\u0010\nAπ\n(1)(s, a) + λAπ\n(2)(s, a) + λ2Aπ\n(3)(s, a) + . . .\n\u0011\n(42)\nHere the parameter λ ∈[0, 1] allows smooth control over bias-variance trade-oﬀ: λ = 0 corre-\nsponds to Actor-Critic with higher bias and lower variance while λ →1 corresponds to REINFORCE\nwith no bias and high variance. But unlike N as hyperparameter, it uses mix of diﬀerent estimators\nin intermediate case.\nGAE proved to be a convenient way how more information can be obtained from collected roll-\nout in practice. Instead of waiting for episode termination to compute (42) we may use «truncated»\nGAE which ensembles only those N-step advantage estimators that are available:\nAπ\ntrunc.GAE(s, a) :=\nAπ\n(1)(s, a) + λAπ\n(2)(s, a) + λ2Aπ\n(3)(s, a) + · · · + λN−1Aπ\n(N)(s, a)\n1 + λ + λ2 + · · · + λN−1\nNote that the amount N of available estimators may be diﬀerent for diﬀerent transitions from roll-\nout: if we performed K steps of interaction in some instance of environment starting from some\nstate-action pair s, a, we can use N = K step estimators; for next state-action pair s′, a′ we have\nonly N = K−1 transitions and so on, while the last state-action pair sN−1, aN−1 can be estimated\nonly using Aπ\n(1) as only N = 1 following transition is available. Although diﬀerent transitions are\nestimated with diﬀerent precision (leading to diﬀerent bias and variance), this approach allows to use\nall available information for each transition and utilize multi-step approximations without dropping\nlast transitions of roll-outs used only for target computation.\n5.5.\nNatural Policy Gradient (NPG)\nIn this section we discuss the motivation and basic principles behind the idea of natural gradient\ndescent, which we will require for future references.\nThe standard gradient descent optimization method is known to be extremely sensitive to the\nchoice of parametrization. Suppose we attempt to solve the following optimization task:\nf(q) →min\nq\nwhere q is a distribution and F is arbitrary diﬀerentiable function. We often restrict q to some\nparametric family and optimize similar objective, but with respect to some vector of parameters θ\nas unknown variable:\nf(qθ) →min\nθ\nClassic example of such problem is maximum likelihood task when we try to ﬁt the parameters of\nour model to some observed data. The problem is that when using standard gradient descent both\nthe convergence rate and overall performance of optimization method substantially depend on the\nchoice of parametrization qθ. The problem holds even if we ﬁx speciﬁc distribution family as many\ndistribution families allow diﬀerent parametrizations.\nTo see why gradient descent is parametrization-sensitive, consider the model which is used at\nsome current point θk to determine the direction of next optimization step:\n(\nf(qθk) + ⟨∇θf(qθk), δθ⟩→min\nδθ\n∥δθ∥2\n2 < αk\nwhere αk is learning rate at step k. Being ﬁrst-order method, gradient descent constructs a «model»\nwhich approximates F locally around θk using ﬁrst-order Taylor expansion and employs standard\nEuclidean metric to determine a region of trust for this model. Then this surrogate task is solved\nanalytically to obtain well-known update formula:\nδθ ∝−∇θf(qθk)\n34\nThe issue arises from reliance on Eucliden metric in the space of parameters. In most parametriza-\ntions, small changes in parameters space do not guarantee small change in distribution space and\nvice versa: some small changes in distribution may demand big steps in parameters space28.\nNatural gradient proposes to use another metric, which achieves invariance to parametrization\nof distribution q using the properties of Fisher matrix:\nDeﬁnition 11. For distribution qθ Fisher matrix Fq(θ) is deﬁned as\nFq(θ) := Ex∼q∇θ log qθ(x)(∇θ log qθ(x))T\nNote that Fisher matrix depends on parametrization. Yet for any parametrization it is guaranteed\nto be positive semi-deﬁnite by deﬁnition. Moreover, it induces a so-called Riemannian metric29 in\nthe space of parameters:\nd(θ1, θ2)2 := (θ2 −θ1)T Fq(θ1)(θ2 −θ1)\nIn natural gradient descent it is proposed to use this metric instead of Euclidean:\n(\nf(qθk) + ⟨∇θf(qθk), δθ⟩→min\nδθ\nδθT Fq(θk)δθ < αk\nThis surrogate task can be solved analytically to obtain the following optimization direction:\nδθ ∝−Fq(θk)−1∇θf(qθk)\n(43)\nThe direction of gradient descent is corrected by Fisher matrix which concerns the scale across dif-\nferent axes. This direction, speciﬁed by Fq(θk)−1∇θf(qθk), is called natural gradient.\nLet’s discuss why this new metric really provides us invariance to distribution parametrization.\nWe already obtained natural gradient for q being parameterized by θ (43). Assume that we have\nanother parametrization qν. These new parameters ν are somehow related to θ; we suppose there\nis some functional dependency θ(ν), which we assume to be diﬀerentiable with jacobian J. In this\nnotation:\nδθ = Jδν,\nJij := ∂θi\n∂νj\n(44)\nThe central property of Fisher matrix, which provides the desired invariance, is the following:\nProposition 21. If θ = θ(ν) with jacobian J, then reparametrization formula for Fisher matrix is\nFq(ν) = JT Fq(θ)J\n(45)\nNow it can be derived that natural gradient for parametrization with ν is the same as for θ. If we\nwant to calculate natural gradient in terms of ν, then our step is, according to (44):\nδθ = Jδν =\n{natural gradient in terms of ν} ∝JFq(νk)−1∇νf(qνk) =\n{Fisher matrix reparametrization (45)} = J\n\u0000JT Fq(θk)J\n\u0001−1 ∇νf(qνk)\n{chain rule} = J\n\u0000JT Fq(θk)J\n\u0001−1 ∇νθ(νk)T ∇θf(qθk) =\n= JJ−1Fq(θk)−1J−T JT ∇θf(qθk) =\n= Fq(θk)−1∇θf(qθk)\n28classic example is that N (0, 100) is similar to N (1, 100) while N (0, 0.1) is completely diﬀerent from N (1, 0.1),\nalthough Euclidean distance in parameter space is the same for both pairs.\n29in Euclidean space the general form of scalar product is ⟨x, y⟩:= xT Gy, where G is ﬁxed positive semi-deﬁnite matrix.\nThe metric induced by this scalar product is correspondingly d(x, y)2 := (y −x)T G(y −x). The diﬀerence in Riemannian\nspace is that G, called metric tensor, depends on x, so the relative distance may vary for diﬀerent points. It is used to\ndescribe the distances between points on manifolds and holds important properties which Fisher matrix inherits as metric\ntensor for distribution space.\n35\nwhich can be seen to be the same as in (43).\nApplication of natural gradient descent in DRL setting is complicated in practice. Theoretically,\nthe only change that must be done is scaling of gradient using inverse Fisher matrix (43). Yet, Fisher\nmatrix requires n2 memory and O(n3) computational costs for inversion where n is the number of\nparameters. For neural networks this causes the same complications as the application of second-\norder optimization methods.\nK-FAC optimization method [13] provides a speciﬁc approximation form of Fisher matrix for neu-\nral networks with linear layers which can be eﬃciently computed, stored and inverted. Usage of\nK-FAC approximation allows to compute natural gradient directly using (43).\n5.6.\nTrust-Region Policy Optimization (TRPO)\nThe main drawback of Actor-Critic algorithm is believed to be the abandonment of experience\nthat was used for previous updates. As the number of updates required is usually huge, this is\nconsidered to be a substantial loss of information. Yet, it is not clear how this information can be\neﬀectively used for newer updates.\nSuppose we want to make an update of π(θ), but using samples collected by some πold. The\nstraightforward approach is importance sampling technique, which naive application to gradient\nformula (40) yields the following result:\n∇θJ(θ) = ET ∼πold P(T | π(θ))\nP(T | πold)\nX\nt=0\n∇θ log πθ(at | st)Aπ(st, at)\nThe emerged importance sampling weight is actually computable as transition probabilities cross\nout:\nP(T | π(θ))\nP(T | πold) =\nQ\nt=1 πθ(at | st)\nQ\nt=1 πold(at | st)\nThe problem with this coeﬃcient is that it tends either to be exponentially small or to explode. Even\nwith some heuristic normalization of coeﬃcients the batch gradient would become dominated by\none or several transitions and destabilize the training procedure by introducing even more variance.\nNotice that application of importance sampling to another representation of gradient (37) yields\nseemingly diﬀerent result:\n∇θJ(θ) = Eπold dπ(θ)(s)\ndπold(s)\nπθ(a | s)\nπold(a | s)∇θ log πθ(a | s)Aπ(s, a)\n(46)\nHere we avoided common for the whole trajectories importance sampling weights by using the def-\ninition of state visitation frequencies. But this result is even less practical as these frequencies are\nunknown to us.\nThe ﬁrst key idea behind the theory concerning this problem is that may be these importance\nsampling coeﬃcients behave more stable if the policies πold and π(θ) are in some terms «close».\nIntuitively, in this case\ndπ(θ)(s)\ndπold(s) of formula (46) is close to 1 as state visitation frequencies are similar,\nand the remained importance sampling coeﬃcient becomes acceptable in practice. And if some two\npolicies are similar, their values of our objective (2) are probably close too.\nFor any two policies, π and πold:\nJ(π) −J(πold) = ET ∼π\nX\nt=0\nγtr(st) −J(πold) =\n= ET ∼π\nX\nt=0\nγtr(st) −V πold(s0) =\n= ET ∼π\n\"X\nt=0\nγtr(st) −V πold(s0)\n#\n=\n{trick P∞\nt=0 (at+1 −at) = −a0\n30} = ET ∼π\n\"X\nt=0\nγtr(st) +\nX\nt=0\nh\nγt+1V πold(st+1) −γtV πold(st)\ni#\n=\n{regroup} = ET ∼π\nX\nt=0\nγt \u0010\nr(st) + γV πold(st+1) −V πold(st)\n\u0011\n=\n36\n{by deﬁnition (3)} = ET ∼π\nX\nt=0\nγt \u0010\nQπold(st, at) −V πold(st)\n\u0011\n{by deﬁnition (14)} = ET ∼π\nX\nt=0\nγtAπold(st, at)\nThe result obtained above is often referred to as relative policy performance identity and is\nactually very interesting: it states that we can substitute reward with advantage function of arbitrary\npolicy and that will shift the objective by the constant.\nUsing the discounted state visitation frequencies deﬁnition 10, relative policy performance iden-\ntity can be rewritten as\nJ(π) −J(πold) =\n1\n1 −γ Es∼dπ(s)Ea∼π(a|s)Aπold(s, a)\nNow assume we want to optimize parameters θ of policy π while using data collected by πold:\napplying importance sampling in the same manner:\nJ(πθ) −J(πold) =\n1\n1 −γ Es∼dπold(s)\ndπθ(s)\ndπold(s)Ea∼πold(a|s)\nπθ(a | s)\nπold(a | s)Aπold(s, a)\nAs we have in mind the idea of πold being close to πθ, the question is how well this identity can\nbe approximated if we assume dπθ(s) = dπold(s). Under this assumption:\nJ(πθ) −J(πold) ≈Lπold(θ) :=\n1\n1 −γ Es∼dπold(s)Ea∼πold(a|s)\nπθ(a | s)\nπold(a | s)Aπold(s, a)\nThe point is that interaction using πold corresponds to sampling from the expectations presented\nin Lπold(θ):\nLπold(θ) = Eπold πθ(a | s)\nπold(a | s)Aπold(s, a)\nThe approximation quality of Lπold(θ) can be described by the following theorem:\nProposition 22. [19]\n\f\fJ(πθ) −J(πold) −Lπold(θ)\n\f\f ≤C max\ns\nKL(πold ∥πθ)[s]\nwhere C is some constant and KL(πold ∥πθ)[s] is a shorten notation for KL(πold(a | s) ∥\nπθ(a | s)).\nThere is an important corollary of proposition 22:\nJ(πθ) −J(πold) ≥Lπold(θ) −C max\ns\nKL(πold ∥πθ)[s]\nwhich not only states that expression on the right side represents a lower bound, but also that the\noptimization procedure\nθk+1 = argmax\nθ\nh\nLπθk (θ) −C max\ns\nKL(πθk ∥πθ)[s]\ni\n(47)\nwill yield a policy with guaranteed monotonic improvement31.\nIn practice there are several obstacles which preserve us from obtaining such procedure. First of\nall, our advantage function estimation is never precise. Secondly, it is hard to estimate precise value\nof constant C. One last obstacle is that it is not clear how to calculate KL-divergence in its maximal\nform (with max taken across all states).\n30and if MDP is episodic, for terminal states V πold(sT ) = 0 by deﬁnition.\n31the maximum of lower bound is non-negative as its value for θ = θk equals zero, which causes J(πk+1)−J(πk) ≥0.\n37\nIn Trust-Region policy optimization [19] the idea of practical algorithm, approximating procedure\n(47), is analyzed. To address the last issue, the naive approximation is proposed to substitute max\nwith averaging across states32:\nmax\ns\nKL(πold ∥πθ)[s] ≈Es∼dπold(s) KL(πold ∥πθ)[s]\nThe second step of TRPO is to rewrite the task of unconstrained minimization (47) in equivalent\nconstrained («trust-region») form33 to incorporate the unknown constant C into learning rate:\n(\nLπold(θ) →max\nθ\nEs∼d(s|πold) KL(πold ∥πθ)[s] < C\n(48)\nNote that this rewrites an update iteration in terms of optimization methods: while Lπold(θ) is an\napproximation of true objective J(πθ) −J(πold), the constraint sets the region of trust to the\nsurrogate. Remark that constraint is actually a divergence in policy space, i. e. it is very similar to a\nmetric in the space of distributions while the surrogate is a function of the policy and depends on\nparameters θ only through πθ.\nTo solve the constrained problem (48), the technique from convex optimization is used. Assume\nthat πold is a current policy and we want to update its parameters θk. Then the objective of (48)\nis modeled using ﬁrst-order Taylor expansion around θk while constraint is modeled using second-\norder 34 Taylor approximation:\n(\nLπold(θk + δθ) ≈⟨∇θ Lπold(θ)|θk , δθ⟩→max\nδθ\nEs∼d(s|πold) KL(πold ∥πθk+δθ) ≈1\n2Es∼d(s|πold)δθT ∇2\nθ KL(πold ∥πθ)\n\f\f\nθk δθ < C\nIt turns out, that this model is equivalent to natural policy gradient, discussed in sec. 5.5:\nProposition 23.\n∇2\nθ KL(πθ ∥πold)[s]\n\f\f\nθk = Fπ(a|s)(θ)\nso KL-divergence constraint can be approximated with metric induced by Fisher matrix. Moreover,\nthe gradient of surrogate function is\n∇θLπold(θ)|θk = Eπold ∇θπθ(a | s)|θk\nπold(a | s)\nAπold(s, a) =\n{πold = πθk} = Eπold∇θ log πθk(a | s)Aπold(s, a)\nwhich is exactly an Actor-Critic gradient. Therefore the formula of update step is given by\nδθ ∝−Fπ(θ)−1∇θLπold(θ)\nwhere ∇θLπold(θ) coincides with standard policy gradient, and Fπ(θ) is hessian of KL-divergence:\nFπ(θ) := Es∼dπold(s) ∇2\nθ KL(πold ∥πθ)\n\f\f\nθk\nIn practical implementations KL-divergence can be Monte-Carlo estimated using collected roll-\nout. The size of roll-out must be signiﬁcantly bigger than in Actor-Critic to achieve suﬃcient precision\nof hessian estimation. Then to obtain a direction of optimization step the following system of linear\nequations\nFπ(θ)δθ = −∇θLπold(θ)\nis solved using a conjugate gradients method which is able to work with Hessian-vector multiplica-\ntion procedure instead of requiring to calculate Fπ(θ) explicitly.\n32the distribution from which the states come is set to be dπold(s) for convenience as this is the distribution from which\nthey come in Lπold(θ).\n33the unconstrained objective is Lagrange function for constrained form.\n34as ﬁrst-order term is zero.\n38\nTRPO also accompanies the update step with a line-search procedure which dynamically adjusts\nstep length using standard backtracking heuristic. As TRPO intuitively seeks for policy improvement\non each step, the idea is to check whether the lower bound (47) is positive after the biggest step\nallowed according to KL-constraint and reduce the step size until it becomes positive.\nUnlike Actor-Critic, TRPO performs extremely expensive complicated update steps but requires\nrelatively small number of iterations in return. Of course, due to many approximations done, the\noverall procedure is only a resemblance of theoretically-justiﬁed iterations (47) providing improve-\nment guarantees.\n5.7.\nProximal Policy Optimization (PPO)\nProximal Policy Optimization [21] proposes alternative heuristic way of performing lower bound\n(47) optimization which demonstrated encouraging empirical results.\nPPO still substitutes max\ns\nKL on average, but leaves the surrogate in unconstrained form, sug-\ngesting to treat unknown constant C as a hyperparameter:\nEπold\n\u0014 πθ(a | s)\nπold(a | s)Aπold(s, a) −C KL(πold ∥πθ)[s]\n\u0015\n→max\nθ\n(49)\nThe naive idea would be to straightforwardly optimize (49) as it is equivalent to solving the con-\nstraint trust-region task (48). To avoid Hessian-involved computations, one possible option is just to\nperform one step of ﬁrst-order gradient optimization of (49). Such algorithm was empirically discov-\nered to perform poorly as importance sampling coeﬃcients\nπθ(a|s)\nπold(a|s) tend to unbounded growth.\nIn PPO it is proposed to cope with this problem in a simple old-fashioned way: by clipping. Let’s\ndenote by\nr(θ) :=\nπθ(a | s)\nπold(a | s)\nan importance sampling weight and by\nrclip(θ) := clip(r(θ), 1 −ϵ, 1 + ϵ)\nits clipped version where ϵ ∈(0, 1) is a hyperparameter. Then the clipped version of lower bound\nis:\nEπold\nh\nmin\n\u0010\nr(θ)Aπold(s, a), rclip(θ)Aπold(s, a)\n\u0011\n−C KL(πold ∥πθ)[s]\ni\n→max\nθ\n(50)\nHere the minimum operation is introduced to guarantee that the surrogate objective remains a\nlower bound. Thus the clipping at 1 + ϵ may occur only in the case if advantage is positive while\nclipping at 1 −ϵ may occur if advantage is negative. In both cases, clipping represents a penalty for\nimportance sampling weight r(θ) being too far from 1.\nThe overall procedure suggested by PPO to optimize the «stabilized» version of lower bound (50)\nis the following. A roll-out is collected using current policy πold with some parameters θ. Then the\nbatches of typical size (as for Actor-Critic methods) are sampled from collected roll-out and several\nsteps of SGD optimization of (50) proceed with respect to policy parameters θ. During this process\nthe policy πold is considered to be ﬁxed and new interaction steps are not performed, while in im-\nplementations there is no need to store old weights θk since everything required from πold is to\ncollect transitions and remember the probabilities πold(a | s). The idea is that during these several\nsteps we may use transitions from the collected roll-out several times. Similar alternative is to per-\nform several epochs of training by passing through roll-out several times, as it is often done in deep\nlearning.\nInteresting fact discovered by the authors of PPO during ablation studies is that removing KL-\npenalty term doesn’t aﬀect the overall empirical performance. That is why in many implementations\nPPO does not include KL-term at all, making the ﬁnal surrogate objective have a following form:\nEπold min\n\u0010\nr(θ)Aπold(s, a), rclip(θ)Aπold(s, a)\n\u0011\n→max\nθ\n(51)\nNote that in this form the surrogate is not generally a lower bound and «improvement guarantees»\nintuition is lost.\n39\nAlgorithm 7: Proximal Policy Optimization (PPO)\nHyperparameters: B — batch size, R — rollout size, n_epochs — number of epochs, ε —\nclipping parameter, V ∗\nφ — critic neural network, πθ — actor neural network, α — critic loss\nscaling, SGD optimizer.\nInitialize weights θ, φ arbitrary\nOn each step:\n1. obtain a roll-out of size R using policy π(θ), storing action probabilities as πold(a | s).\n2. for each transition T from the roll-out compute advantage estimation (detached from\ncomputational graph to prevent backpropagation):\nAπ(T ) = r′ + γV π\nφ (s′) −V π\nφ\n3. perform n_epochs passes through roll-out using batches of size B; for each batch:\n• compute critic target (detached from computational graph to prevent backpropa-\ngation):\ny(T ) = r′ + γV π\nφ (s′)\n• compute critic loss:\nLoss = 1\nB\nX\nT\n\u0010\ny(T ) −V π\nφ\n\u00112\n• compute critic gradients:\n∇critic = ∂Loss\n∂φ\n• compute importance sampling weights:\nrθ(T ) =\nπθ(a | s)\nπold(a | s)\n• compute clipped importance sampling weights:\nrclip\nθ\n(T ) = clip(rθ(T ), 1 −ϵ, 1 + ϵ)\n• compute actor gradient:\n∇actor = 1\nB\nX\nT\n∇θ min\n\u0010\nrθ(T )Aπ(T ), rclip\nθ\n(T )Aπ(T )\n\u0011\n• make a step of gradient descent using ∇actor + α∇critic\n40\n6.\nExperiments\n6.1.\nSetup\nWe performed our experiments using custom implementation of discussed algorithms attempt-\ning to incorporate best features from diﬀerent oﬃcial and unoﬃcial sources and unifying all algo-\nrithms in a single library interface. The full code is available at our github.\nWhile custom implementation might not be the most eﬃcient, it hinted us several ambiguities in\nalgorithms which are resolved diﬀerently in diﬀerent sources. We describe these nuances and the\nchoices made for our experiments in appendix A.\nFor each environment we launch several algorithms to train the network with the same architec-\nture with the only exception being the head which is speciﬁed by the algorithm (see table 1).\nDQN\nLinear transformation to |A| arbitrary real values\nDueling\nFirst head: linear transformation to |A| arbitrary real values\nSecond head: linear transformations to an arbitrary scalar\nAggregated using dueling architecture formula (17)\nCategorical\n|A| linear transformations with softmax to A values\nDueling Categorical\nFirst head: linear transformation to |A| arbitrary real values\nSecond head: |A| linear transformations to A arbitrary real values\nAggregated using dueling architecture formula (32)\nQuantile\n|A| linear transformations to A arbitrary real values\nDueling Quantile\nFirst head: linear transformation to |A| arbitrary real values\nSecond head: |A| linear transformations to A arbitrary real values\nAggregated using dueling architecture formula (32) without softmax\nA2C / PPO\nActor head: linear transformation with softmax to |A| values\nCritic head: linear transformation to scalar value\nTable 1: Heads used for diﬀerent algorithms. Here |A| is the number of actions and A is the chosen number of\natoms.\nFor noisy networks all fully-connected layers in the feature extractor and in the head are substi-\ntuted with noisy layers, doubling the number of their trained parameters. Both usage of noisy layers\nand the choice of the head inﬂuences the total number of parameters trained by the algorithm.\nAs practical tuning of hyperparameters is computationally consuming activity, we set all hyperpa-\nrameters to their recommended values while trying to share the values of common hyperparameters\namong algorithms without aﬀecting overall performance.\nWe choose to give each algorithm same amount of interaction steps to provide the fair compari-\nson of their sample eﬃciency. Thus the wall-clock time, number of episodes played and the number\nof network parameters updates varies for diﬀerent algorithms.\n6.2.\nCartpole\nCartpole from OpenAI Gym [2] is considered to be one of the simplest environments for DRL\nalgorithms testing. The state is described with 4 real numbers while action space is two-dimensional\ndiscrete.\nThe environment rewards agent with +1 each tick until the episode ends. Poor action choices\nlead to early termination. The game is considered solved if agent holds for 200 ticks, therefore 200\nis maximum reward in this environment.\nIn our ﬁrst experiment we launch algorithms for 10 000 interaction steps to train a neural network\non the Cartpole environment. The network consists of two fully-connected hidden layers with 128\nneurons and an algorithm-speciﬁc head. We used ReLU for activations. The results of a single launch\nare provided35 in table 2.\n35we didn’t tune hyperparameters for each of the algorithms, so the conﬁgurations used might not be optimal.\n41\nReached 200\nAverage reward\nAverage FPS\nDouble DQN\n23.0\n126.17\n95.78\nDueling Double DQN\n27.0\n121.78\n62.65\nDQN\n33.0\n116.27\n101.53\nCategorical DQN\n28.0\n110.87\n74.95\nPrioritized Double DQN\n37.0\n110.52\n85.58\nCategorical Prioritized Double DQN\n46.0\n104.86\n66.00\nQuantile Prioritized Double DQN\n42.0\n100.76\n68.62\nCategorical DQN with target network\n44.0\n96.08\n73.92\nQuantile Double DQN\n54.0\n93.14\n75.40\nQuantile DQN\n70.0\n88.12\n77.93\nCategorical Double DQN\n42.0\n81.25\n70.90\nNoisy Quantile Prioritized Dueling DQN\n86.0\n74.13\n21.41\nTwin DQN\n57.0\n71.14\n52.51\nNoisy Double DQN\n67.0\n71.06\n31.81\nNoisy Prioritized Double DQN\n94.0\n67.34\n30.72\nQuantile Regression Rainbow\n106.0\n67.11\n21.54\nRainbow\n91.0\n64.01\n20.35\nNoisy Quantile Prioritized Double DQN\n127.0\n63.01\n28.27\nNoisy Categorical Prioritized Double DQN\n63.0\n62.04\n27.81\nPPO with GAE\n144.0\n53.06\n390.53\nNoisy Prioritized Dueling Double DQN\n180.0\n47.52\n22.56\nPPO\n184.0\n45.19\n412.88\nNoisy Categorical Prioritized Dueling Double DQN\n428.0\n22.09\n20.63\nA2C\n-\n12.30\n1048.64\nA2C with GAE\n-\n11.50\n978.00\nTable 2: Results on Cartpole for diﬀerent algorithms: number of episode when the highest score of 200 was\nreached, average reward across all played episodes and average number of frames processed in a second (FPS).\n6.3.\nPong\nWe used Atari Pong environment from OpenAI Gym [2] as our main testbed to study the be-\nhaviour of the following algorithms:\n• DQN — Deep Q-learning (sec. 3.2)\n• c51 — Categorical DQN (sec. 4.2)\n• QR-DQN — Quantile Regression DQN (sec. 4.3)\n• Rainbow (sec. 4.4)\n• A2C — Advantage Actor Critic (sec. 5.3) extended with GAE (sec. 5.4)\n• PPO — Proximal Policy Optimization (sec. 5.7) extended with GAE (sec. 5.4)\nIn Pong, each episode is split into rounds. Each round ends with player either winning or loosing.\nThe episode ends when the player wins or looses 21 rounds. The reward is given after each round\nand is +1 for winning and -1 for loosing. Therefore the maximum total reward is 21 and the minimum\nis -21. Note that the ﬂag done indicating episode ending is not provided to the agent after each\nround but only at the end of full game (consisting of 21-41 rounds).\nThe standard preprocessing for Atari games proposed in DQN [14] was applied to the environ-\nment (see table 3). Thus, state space is represented by (84, 84) grayscale pixels input (1 channel\nwith domain [0, 255]). Action space is discrete with |A| = 6 actions.\nAll algorithms were given 1 000 000 interaction steps to train the network with the same feature\nextractor presented on ﬁg. 1. The number of trained parameters is presented in table 4. All used\nhyperparameters are listed in table 7 in appendix B.\n42\nNoopResetEnv\nDo nothing ﬁrst 30 frames of games to imitate the\npause between game start and real player reaction.\nMaxAndSkipEnv\nEach interaction steps takes 4 frames of the game\nto allow less frequent switch of action. Max is taken\nover 4 passed frames to obtain an observation.\nFireResetEnv\nPresses «Fire» button at ﬁrst frame to launch\nthe game, otherwise screen remains frozen.\nWarpFrame\nTurns observation to grayscale image of size 84x84.\nTable 3: Atari Pong preprocessing\nAlgorithm\nNumber of trained parameters\nDQN\n1 681 062\nc51\n1 834 962\nQR-DQN\n1 834 962\nRainbow\n3 650 410\nA2C\n1 681 575\nPPO\n1 681 575\nTable 4: Number of trained parameters in Pong experiment.\n6.4.\nInteraction-training trade-oﬀin value-based algorithms\nThere is a common belief that policy gradient algorithms are much faster in terms of computa-\ntional costs while value-based algorithms are preferable when simulation is expensive because of\ntheir sample eﬃciency. This follows from the nature of algorithms, as the fraction «observations per\nnetwork updates» is extremely diﬀerent for these two families: indeed, in DQN it is often assumed\nto perform one network update after each new transitions, while A2C collects about 32-40 observa-\ntions for only one update. That makes the number of network updates performed during 1M steps\ninteraction process substantially diﬀerent and is the main reason of policy gradients speed rate.\nAlso policy gradient algorithms use several threads for parallel simulations (8 in our experiments)\nwhile value-based algorithms are formally single-threaded. Yet they can also enjoy multi-threaded\ninteraction, in the simplest form by playing 1 step in all instances of environment and then perform-\ning L steps of network optimization [8]. For consistency with single-threaded case it is reasonable to\nset the value of L to be equal to the number of threads to maintain the same fraction «observations\nper network updates».\nHowever it has been reported that lowering value of L in two or four times can positively aﬀect\nwall-clock time with some loss of sample eﬃciency, while raising batch size may mitigate this down-\ngrade. The overall impact of such acceleration of value-based algorithms on performance properties\nis not well studied and may alter their behaviour.\nIn our experiments on Pong it became evident that value-based algorithms perform extensive\namount of redundant network optimization steps, absorbing knowledge faster than novel informa-\ntion from new transitions comes in. This reasoning in particular follows from the success of PPO on\nPong task which performs more than 10 times less network updates.\nVanilla algorithm\nAccelerated version\nThreads\n1\n8\nBatch size\n32\n128\nL\n1\n2\nInteractions per update\n1\n4\nTable 5: Setup for value-based acceleration experiment\nWe compared two versions of value-based algorithms: vanilla version, which is single-threaded\nwith standard batch size (32) and L = 1 meaning that each observed transition is followed with\none network optimization step, and accelerated version, where 1 interaction step is performed in\n8 parallel instances of environment and L is set to be 2 instead of 8 which raises the fraction «ob-\n43\nConvolution 8x8 \nwith stride = 4\nConvolution 4x4 \nwith stride = 2\nConvolution 3x3 \nwith stride = 1\nFully­connected layer \nAlgorithm­specific head \n(1, 84, 84)\n(32, 20, 20)\n(64, 9, 9)\n(64, 7, 7)\n512 FEATURES\nFigure 1: Network used for Atari Pong. All activation functions are ReLU. For Rainbow the fully-connected layer\nand all dense layers in the algorithm-speciﬁc head are substituted with noisy layers.\nservations per training step» in four times. To compensate this change we raised batch size in four\ntimes.\nAs expected, average speed of algorithms increases in approximately 3.5 times (see table 6). We\nprovide training curves with respect to 1M performed interaction steps on ﬁg. 2 and with respect\nto wall-clock time on ﬁg. 3. The only vanilla algorithm that achieved better ﬁnal score comparing to\nits accelerated rival is QR-DQN, while other three algorithms demonstrated both acceleration and\nperformance improvement. The latter is probably caused by randomness as relaunch of algorithms\nwithin the same setting and hyperparameters can be strongly inﬂuenced by random seed.\nIt can be assumed that fraction «observations per updates» is an important hyperparameter of\nvalue-based algorithms which can control the trade-oﬀbetween wall-clock time and sample eﬃ-\nciency. From our results it follows that low fraction leads to excessive network updates and may\nslow down learning in several times. Yet this hyperparameter can barely be tuned universally for\nall kinds of tasks opposed to many other hyperparameters that usually have their recommended\ndefault values.\nWe stick further to the accelerated version and use its results in ﬁnal comparisons.\n6.5.\nResults\nWe compare the results of launch of six algorithms on Pong from two perspectives: sample eﬃ-\nciency (ﬁg. 4) and wall-clock time (ﬁg. 5). We do not compare ﬁnal performance of these algorithms\nas all six algorithms are capable to reach near-maximum ﬁnal score on Pong given more iterations,\nwhile results after 1M iterations on a single launch signiﬁcantly depend on chance.\nAll algorithms start with a warm-up session during which they try to explore the environment and\n44\nInteractions per update\nAverage transitions per second\nAlgorithm\nvanilla\naccelerated\nvanilla\naccelerated\nDQN\n1\n4\n55.74\n168.43\nc51\n1\n4\n44.08\n148.76\nQR-DQN\n1\n4\n47.46\n155.97\nRainbow\n1\n4\n19.30\n70.22\nA2C\n40\n656.25\nPPO\n10.33\n327.13\nTable 6: Computational eﬃciency of vanilla and accelerated versions.\n0\n200000\n400000\n600000\n800000\n1000000\ninteraction step\n20\n15\n10\n5\n0\n5\n10\n15\n20\naverage score for the last 20 episodes\nAcceleration's influence on sample efficiency\nDQN accelerated\nDQN vanilla\nc51 accelerated\nc51 vanilla\nQR-DQN accelerated\nQR-DQN vanilla\nRainbow accelerated\nRainbow vanilla\nFigure 2: Training curves of vanilla and accelerated version of value-based algorithms on 1M steps of Pong.\nAlthough accelerated versions perform network updates four times less frequent, the performance degradation\nis not observed.\nlearn ﬁrst dependencies how the result of random behaviour can be surpassed. Epsilon-greedy with\ntuned parameters provides suﬃcient amount of exploration for DQN, c51 and QR-DQN whithout\nslowing down further learning while hyperparameter-free noisy networks are the main reason why\nRainbow has substantially longer warm-up.\nPolicy gradient algorithms incorporate exploration strategy in stochasticity of learned policy but\nunderutilization of observed samples leads to almost 1M-frames warm-up for A2C. It can be ob-\nserved that PPO successfully mitigates this problem by reusing samples thrice. Nevertheless, both\nPPO and A2C solve Pong relatively quickly after the warm-up stage is over.\nValue-based algorithm proved to be more computationally costly. QR-DQN and categorical DQN\nintroduce more complicated loss computation, yet their slowdown compared to standard DQN is\nmoderate. On the contrary, Rainbow is substantially slower mainly because of noise generation\ninvolvement. Furthermore, combination of noisy networks and prioritized replay results in even less\nstable training process.\nWe provide loss curves for all six algorithms and statistics for noise magnitude and prioritized re-\nplay for Rainbow in appendix C; some additional visualizations of trained algorithms playing episodes\nof Pong are presented in appendix D.\n45\n0\n200\n400\n600\n800\nminutes\n20\n15\n10\n5\n0\n5\n10\n15\n20\naverage score for the last 20 episodes\nAcceleration effect on value-based algorithms\nDQN accelerated\nDQN vanilla\nc51 accelerated\nc51 vanilla\nQR-DQN accelerated\nQR-DQN vanilla\nRainbow accelerated\nRainbow vanilla\nFigure 3: Training curves of vanilla and accelerated version of value-based algorithms on 1M steps of Pong from\nwall-clock time.\n0\n200000\n400000\n600000\n800000\n1000000\ninteraction step\n20\n15\n10\n5\n0\n5\n10\n15\n20\naverage score for the last 20 episodes\nComparing different algorithms on Pong\nDQN\nc51\nQR-DQN\nRainbow\nA2C\nPPO\nFigure 4: Training curves of all algorithms on 1M steps of Pong.\n0\n50\n100\n150\n200\nminutes\n20\n15\n10\n5\n0\n5\n10\n15\n20\naverage score for the last 20 episodes\nDQN\n(1h 38m)\nc51\n(1h 52m)\nQR-DQN\n(1h 46m)\nRainbow\n(3h 57m)\nA2C\n(0h 25m)\nPPO\n(0h 50m)\nComparing wall-clock time of different algorithms on Pong\nDQN\nc51\nQR-DQN\nRainbow\nA2C\nPPO\nFigure 5: Training curves of all algorithms on 1M steps of Pong from wall-clock time.\n46\n7.\nDiscussion\nWe have concerned two main directions of universal model-free RL algorithm design and at-\ntempted to recreate several state-of-art pipelines.\nWhile the extensions of DQN are reasonable solutions of evident DQN problems, their eﬀect\nis not clearly seen on simple tasks like Pong36. Current state-of-art in single-threaded value-based\napproach, Rainbow DQN, is full of «glue and tape» decisions that might be not the most eﬀective\nway of training process stabilization.\nDistributional value-based approach is one of the cheapest in terms of resources extensions of\nvanilla DQN algorithm. Although it is reported to provide substantial performance improvement in\nempirical experiments, the reason behind this result remains unclear as expectation of return is the\nkey quantity for agent’s decision making while the rest of learned distribution does not aﬀect his\nchoices. One hypothesis to explain this phenomenon is that attempting to capture wider range of\ndependencies inside given MDP may provide auxiliary helping tasks to the algorithm, leading to bet-\nter learning of expectation. Intuitively it seems that more reasonable switch of DQN to distributional\nsetting would be learning the Bayesian uncertainty of expectation of return given observed data, but\nscalable practical algorithms within this orthogonal paradigm are yet to be created.\nPolicy gradient algorithms are aimed at direct optimization of objective and currently beat value-\nbased approach in terms of computational costs. They tend to have less hyperparameters but are\nextremely sensitive to the choice of optimizer parameters and especially learning rate. We have\naﬃrmed the eﬀectiveness of state-of-art algorithm PPO, which succeeded to solve Pong within an\nhour without hyperparameter tuning. Though on the one hand this algorithm was derived from\nTRPO theory, it essentially deviates from it and substitutes trust region updates with heuristic clip-\nping.\nIt can be observed in our results that PPO provides better gradients to the same network than\nDQN-based algorithms despite the absence of experience replay. While it is fair to assume that\nforgetting experienced transitions leads to information loss, it is also true that most observations\nstored in replay memory are already learned or contain no useful information. The latter makes\nmost transitions in the sampled mini-batches insigniﬁcant, and, while prioritized replay attacks this\nissue, it might still be the case that current experience replay management techniques are imperfect.\nThere are still a lot of deviations of empirical results from theoretical perspectives. It is yet unclear\nwhich techniques are of the highest potential and what explanation lies behind many heuristic ele-\nments composing current state-of-art results. Possibly essential elements of modeling human-like\nreinforcement learning are yet to be unraveled as active research in this area promises substantial\nacceleration, generalization and stabilization of DRL algorithms.\n36although it takes several hours to train, Pong is considered to be the easiest of 57 Atari games and one of the most basic\ntestbeds for RL algorithms.\n47\nReferences\n[1] M. G. Bellemare, W. Dabney, and R. Munos. A distributional perspective on reinforcement learn-\ning.\nIn Proceedings of the 34th International Conference on Machine Learning-Volume 70,\npages 449–458. JMLR. org, 2017.\n[2] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Ope-\nnai gym. arXiv preprint arXiv:1606.01540, 2016.\n[3] W. Dabney, M. Rowland, M. G. Bellemare, and R. Munos. Distributional reinforcement learning\nwith quantile regression. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.\n[4] M. Fortunato, M. G. Azar, B. Piot, J. Menick, I. Osband, A. Graves, V. Mnih, R. Munos, D. Hassabis,\nO. Pietquin, et al. Noisy networks for exploration. arXiv preprint arXiv:1706.10295, 2017.\n[5] I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio. Deep learning, volume 1. MIT press Cam-\nbridge, 2016.\n[6] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger. Deep reinforcement\nlearning that matters. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.\n[7] M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan, B. Piot,\nM. Azar, and D. Silver. Rainbow: Combining improvements in deep reinforcement learning. In\nThirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.\n[8] D. Horgan, J. Quan, D. Budden, G. Barth-Maron, M. Hessel, H. Van Hasselt, and D. Silver. Dis-\ntributed prioritized experience replay. arXiv preprint arXiv:1803.00933, 2018.\n[9] A. Irpan. Deep reinforcement learning doesnвҐЄt work yet. Online (Feb. 14): https://www.\nalexirpan. com/2018/02/14/rl-hard. html, 2018.\n[10] L. Kaiser, M. Babaeizadeh, P. Milos, B. Osinski, R. H. Campbell, K. Czechowski, D. Erhan, C. Finn,\nP. Kozakowski, S. Levine, et al. Model-based reinforcement learning for atari. arXiv preprint\narXiv:1903.00374, 2019.\n[11] R. Koenker and G. Bassett Jr. Regression quantiles. Econometrica: journal of the Econometric\nSociety, pages 33–50, 1978.\n[12] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous\ncontrol with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.\n[13] J. Martens and R. Grosse. Optimizing neural networks with kronecker-factored approximate\ncurvature. In International conference on machine learning, pages 2408–2417, 2015.\n[14] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller. Play-\ning atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.\n[15] C. Nota and P. S. Thomas. Is the policy gradient a gradient? arXiv preprint arXiv:1906.07073,\n2019.\n[16] OpenAI. Openai ﬁve. https://blog.openai.com/openai-five/, 2018.\n[17] T. Salimans, J. Ho, X. Chen, S. Sidor, and I. Sutskever. Evolution strategies as a scalable alternative\nto reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.\n[18] T. Schaul, J. Quan, I. Antonoglou, and D. Silver. Prioritized experience replay. arXiv preprint\narXiv:1511.05952, 2015.\n[19] J. Schulman, S. Levine, P. Abbeel, M. I. Jordan, and P. Moritz. Trust region policy optimization. In\nIcml, volume 37, pages 1889–1897, 2015.\n[20] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control\nusing generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.\n[21] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization\nalgorithms. arXiv preprint arXiv:1707.06347, 2017.\n48\n[22] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Ku-\nmaran, T. Graepel, et al. Mastering chess and shogi by self-play with a general reinforcement\nlearning algorithm. arXiv preprint arXiv:1712.01815, 2017.\n[23] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018.\n[24] R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour. Policy gradient methods for rein-\nforcement learning with function approximation. In Advances in neural information processing\nsystems, pages 1057–1063, 2000.\n[25] H. Van Hasselt, A. Guez, and D. Silver. Deep reinforcement learning with double q-learning. In\nThirtieth AAAI Conference on Artiﬁcial Intelligence, 2016.\n[26] O. Vinyals, I. Babuschkin, J. Chung, M. Mathieu, M. Jaderberg, W. M. Czarnecki, A. Dudzik,\nA. Huang, P. Georgiev, R. Powell, T. Ewalds, D. Horgan, M. Kroiss, I. Danihelka, J. Agapiou,\nJ. Oh, V. Dalibard, D. Choi, L. Sifre, Y. Sulsky, S. Vezhnevets, J. Molloy, T. Cai, D. Budden,\nT. Paine, C. Gulcehre, Z. Wang, T. Pfaﬀ, T. Pohlen, Y. Wu, D. Yogatama, J. Cohen, K. McKinney,\nO. Smith, T. Schaul, T. Lillicrap, C. Apps, K. Kavukcuoglu, D. Hassabis, and D. Silver. AlphaS-\ntar: Mastering the Real-Time Strategy Game StarCraft II. https://deepmind.com/blog/\nalphastar-mastering-real-time-strategy-game-starcraft-ii/, 2019.\n[27] Z. Wang, T. Schaul, M. Hessel, H. Van Hasselt, M. Lanctot, and N. De Freitas. Dueling network\narchitectures for deep reinforcement learning. arXiv preprint arXiv:1511.06581, 2015.\n[28] C. J. Watkins and P. Dayan. Q-learning. Machine learning, 8(3-4):279–292, 1992.\n[29] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. Machine learning, 8(3-4):229–256, 1992.\n49\nAppendix A. Implementation details\nHere we describe several technical details of our implementation which may potentially inﬂuence\nthe obtained results.\nIn most papers on value-based algorithms hyperparameters recommended for Atari games as-\nsume raw input in the range [0, 255], while in various implementations of policy gradient algorithms\nnormalized input in the range [0, 1] is considered. Stepping aside from these agreements may dam-\nage the convergence speed both for value-based and policy gradient algorithms as the change of\ninput domain requires hyperparameters retuning.\nWe use MSE loss emerged in theoretical intuition for DQN while in many sources it is recom-\nmended to use Huber loss37 instead to stabilize learning.\nIn all value-based algorithms except c51 we update target network each K-th frame instead of\nexponential smoothing of its parameters as it is computationally cheaper. For c51 we remove target\nnetwork heuristic as apriori limited domain prevents unbounded growth of predictions.\nWe do not architecturally force quantiles outputted by the network in Quantile Regression DQN\nto satisfy ζ0 ≤ζ1 ≤· · · ≤ζA−1. As in the original paper, we assume that all A outputs of network\nare arbitrary real values and use a standard linear transformation as our last layer.\nIn dueling architectures we subtract mean of A(s, a) across actions instead of theoretically as-\nsumed maximum as proposed by original paper authors.\nWe implement sampling from prioritized replay using SumTree data structure and in informal\nexperiments aﬃrmed the acceleration it provides. The importance sampling weight annealing β(t)\nis represented by initial value β(0) = β which is then linearly annealed to 1 during ﬁrst Tβ frames;\nboth β and Tβ are hyperparameters.\nWe do not allow priorities P(T ) to be greater than 1 by clipping as suggested in the original\npaper. This may mitigate the eﬀect of prioritization replay but stabilizes the process.\nAs importance sampling weights w(T ) =\n1\nBP(T ) are potentially very close to zero, in original\narticle it was proposed to normalize them on max w(T ). In some implementations the maximum is\ntaken over the whole experience replay while in others maximum is taken over current batch, which\nis not theoretically justiﬁed but computationally much faster. We stick to the latter option.\nFor noisy layers we use factorized noise sampling: for layer with m inputs and n outputs we sam-\nple ε1 ∈Rn, ε2 ∈Rm from standard normal distributions and scale both using f(ε) = sign(ε)√ε.\nThus we use f(ε1)f(ε2)T as our noise sample for weights matrix and f(ε2) as noise sample for\nbias. All noise is shared across mini-batch. Noise is resampled on each forward pass through the\nnetwork and thus is independent between evaluation, selection and interaction. Despite all these\nsimpliﬁcations, we found noisy layers to be the most computationally expensive modiﬁcation of\nDQN leading to substantial degradation of wall-clock time.\nFor policy gradient algorithms we add additional policy entropy term to the loss to force ex-\nploration. We also deﬁne actor loss as a scalar function that yields the same gradients as in the\ncorresponding gradient estimation (40) for A2C to compute it using PyTorch mechanics. For PPO\nobjective (51) provides analogous «actor loss»; thus, in both policy gradient algorithms the full loss\nis deﬁned as summation of actor, critic and entropy losses, with the two latter being scaled using\nscalar hyperparameters.\nWe use shared network architecture for policy gradient algorithms with one feature extractor and\ntwo heads, one for policy and one for critic.\nKL-penalty is not used in our PPO implementation. Also we do not normalize advantage esti-\nmations across the roll-out to zero mean and unit standard deviation as additionally done in some\nimplementations.\nWe use PyTorch default initialization for linear and convolutional layers although orthogonal ini-\ntialization of all layers is reported to be beneﬁcial for policy gradient algorithms. Initial values of\nsigmas for noisy layers is set to be constant and equal to σinit\nm\nwhere σinit is a hyperparameter and\nm is the number of inputs in accordance with original paper.\nWe use Adam as our optimizer with default β1 = 0.9, β2 = 0.999, ε = 1e−8. No gradient\nclipping is performed.\n37Huber loss is deﬁned as\nLoss(y, ˆy) =\n(\n(y −ˆy)2\nif |y −ˆy| < 1\n|y −ˆy|\nelse\n50\nAppendix B. Hyperparameters\nDQN\nQR-DQN\nc51\nRainbow\nA2C\nPPO\nReward discount factor γ\n0.99\nε(t)-greedy strategy\n0.01 + 0.99e−\nt\n30 000\n-\n-\nInteractions per training step\n4\n-\nBatch size B\n128\n-\n32\nRollout capacity\n-\n40\n1024\nPPO number of epochs\n-\n3\nReplay buﬀer initialization size38\n10 000 transitions\n-\nReplay buﬀer capacity M\n1 000 000 transitions\n-\nTarget network updates K\neach 1000-th step\n-\nNumber of atoms A\n-\n51\n-\nVmin, Vmax\n-\n-\n[−10, 10]\n-\nNoisy layers std initialization\n-\n-\n-\n0.5\n-\nMultistep N\n-\n-\n-\n3\n-\nPrioritization degree α\n-\n-\n-\n0.5\n-\nPrioritization bias correction β\n-\n-\n-\n0.4\n-\nUnbiased prioritization after\n-\n-\n-\n100 000 steps\n-\nGAE coeﬀ. λ\n-\n0.95\nCritic loss weight\n-\n0.5\nEntropy loss weight\n-\n0.01\nPPO clip ϵ\n-\n0.1\nOptimizer\nAdam\nLearning rate\n0.0001\nTable 7: Selected hyperparameters for Atari Pong\n38number of transitions to collect in replay memory before starting network optimization using mini-batch sampling.\n51\nAppendix C. Training statistics on Pong\n0\n50000\n100000\n150000\n200000\n250000\nnetwork update step\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nloss\nDQN loss behaviour\n0\n50000\n100000\n150000\n200000\n250000\nnetwork update step\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\nloss\nDQN loss (averaged across 1000 steps)\naverage loss\nstd\nFigure 6: DQN loss behaviour during training on Pong.\n0\n50000 100000 150000 200000\nnetwork update step\n0\n1\n2\n3\n4\nloss\nc51 loss behaviour\n0\n50000 100000150000200000\nnetwork update step\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\nQR-DQN loss behaviour\n0\n50000 100000150000200000\nnetwork update step\n1\n2\n3\n4\n5\n6\n7\n8\nRainbow loss behaviour\nFigure 7: Loss behaviours of c51, QR-DQN and Rainbow during training on Pong.\n52\n0\n50000\n100000\n150000\n200000\n250000\nnetwork update step\n0.965\n0.970\n0.975\n0.980\n0.985\n0.990\n0.995\n1.000\nmedian weight in mini-batch \n (smoothed with window=1000)\nImportance sampling correction weights\n0\n50000\n100000\n150000\n200000\n250000\nnetwork update step\n0.01000\n0.01005\n0.01010\n0.01015\n0.01020\n0.01025\nAverage noise magnitude\nFigure 8: Rainbow statistics during training. Left: smoothed with window 1000 median of importance sampling\nweights from sampled mini-batches. Right: average noise magnitude logged at each 20-th step of training.\n0\n5000\n10000\n15000\n20000\n25000\nnetwork update step\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\nloss\nAdvantage Actor-Critic loss behaviour\nActor loss\nCritic loss\nEntropy loss\nFigure 9: A2C loss behaviour during training.\n0\n20000\n40000\n60000\n80000\nnetwork update step\n2.0\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\nloss\nProximal Policy Optimization loss behaviour\nActor loss\nCritic loss\nEntropy loss\nFigure 10: PPO loss behaviour during training.\n53\nAppendix D. Playing Pong behaviour\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\nepisode step\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nstate value\nDQN playing Pong\nPredicted V(s)\nReward-to-go\nlosses\nwins\nFigure 11: DQN playing one episode of Pong.\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\nepisode step\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nstate value\nc51 playing Pong\nPredicted V(s)\nReward-to-go\nlosses\nwins\nFigure 12: c51 playing one episode of Pong.\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\nepisode step\n-10.0\n-8.0\n-6.0\n-4.0\n-2.0\n0.0\n2.0\n4.0\n6.0\n8.0\n10.0\nstate value\nc51 value distribution during one played episode\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nFigure 13: c51 value distribution prediction during one episode of Pong.\n54\n0\n500\n1000\n1500\n2000\n2500\nepisode step\n1\n0\n1\n2\n3\nstate value\nQuantile Regression DQN playing Pong\nPredicted V(s)\nReward-to-go\nlosses\nwins\nFigure 14: Quantile Regression DQN playing one episode of Pong.\n0\n500\n1000\n1500\n2000\n2500\nepisode step\n1\n0\n1\n2\n3\n4\nstate value\nQuantile Regression DQN value distribution approximation during one played episode\nFigure 15: Quantile Regression DQN value distribution prediction during one episode of Pong.\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nepisode step\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\nstate value\nRainbow playing Pong\nPredicted V(s)\nReward-to-go\nlosses\nwins\nFigure 16: Rainbow playing one episode of Pong (exploration turned oﬀ, i.e. all noise samples are zero).\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nepisode step\n-10.0\n-8.0\n-6.0\n-4.0\n-2.0\n0.0\n2.0\n4.0\n6.0\n8.0\n10.0\nstate value\nRainbow value distribution during one played episode\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nFigure 17: Rainbow value distribution prediction during one episode of Pong (exploration turned oﬀ, i.e. all\nnoise samples are zero).\n55\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nepisode step\n2.0\n1.5\n1.0\n0.5\n0.0\n0.5\nstate value\nA2C playing Pong\nPredicted V(s)\nReward-to-go\nlosses\nwins\nFigure 18: A2C playing one episode of Pong.\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nepisode step\nNOOP\nFIRE\nRIGHT\nLEFT\nRIGHTFIRE\nLEFTFIRE\nactions\nA2C policy during one played episode\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nFigure 19: A2C policy distribution during one episode of Pong.\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nepisode step\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\nstate value\nPPO playing Pong\nPredicted V(s)\nReward-to-go\nlosses\nwins\nFigure 20: PPO playing one episode of Pong.\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nepisode step\nNOOP\nFIRE\nRIGHT\nLEFT\nRIGHTFIRE\nLEFTFIRE\nactions\nPPO policy during one played episode\n0.2\n0.4\n0.6\n0.8\nFigure 21: PPO policy distribution during one episode of Pong.\n56\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2019-06-24",
  "updated": "2019-07-06"
}