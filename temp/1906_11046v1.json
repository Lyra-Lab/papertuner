{
  "id": "http://arxiv.org/abs/1906.11046v1",
  "title": "Multi-Agent Deep Reinforcement Learning for Liquidation Strategy Analysis",
  "authors": [
    "Wenhang Bao",
    "Xiao-yang Liu"
  ],
  "abstract": "Liquidation is the process of selling a large number of shares of one stock\nsequentially within a given time frame, taking into consideration the costs\narising from market impact and a trader's risk aversion. The main challenge in\noptimizing liquidation is to find an appropriate modeling system that can\nincorporate the complexities of the stock market and generate practical trading\nstrategies. In this paper, we propose to use multi-agent deep reinforcement\nlearning model, which better captures high-level complexities comparing to\nvarious machine learning methods, such that agents can learn how to make the\nbest selling decisions. First, we theoretically analyze the Almgren and Chriss\nmodel and extend its fundamental mechanism so it can be used as the multi-agent\ntrading environment. Our work builds the foundation for future multi-agent\nenvironment trading analysis. Secondly, we analyze the cooperative and\ncompetitive behaviours between agents by adjusting the reward functions for\neach agent, which overcomes the limitation of single-agent reinforcement\nlearning algorithms. Finally, we simulate trading and develop an optimal\ntrading strategy with practical constraints by using a reinforcement learning\nmethod, which shows the capabilities of reinforcement learning methods in\nsolving realistic liquidation problems.",
  "text": "Multi-Agent Deep Reinforcement Learning for Liquidation Strategy Analysis\nWenhang Bao 1 Xiao-Yang Liu 2\n,\nAbstract\nLiquidation is the process of selling a large num-\nber of shares of one stock sequentially within a\ngiven time frame, taking into consideration the\ncosts arising from market impact and a trader’s\nrisk aversion. The main challenge in optimiz-\ning liquidation is to ﬁnd an appropriate modeling\nsystem that can incorporate the complexities of\nthe stock market and generate practical trading\nstrategies. In this paper, we propose to use multi-\nagent deep reinforcement learning model, which\nbetter captures high-level complexities compar-\ning to various machine learning methods, such\nthat agents can learn how to make best selling\ndecisions. First, we theoretically analyze the Alm-\ngren and Chriss model and extend its fundamental\nmechanism so it can be used as the multi-agent\ntrading environment. Our work builds the foun-\ndation for future multi-agent environment trading\nanalysis. Secondly, we analyze the cooperative\nand competitive behaviors between agents by ad-\njusting the reward functions for each agent, which\novercomes the limitation of single-agent reinforce-\nment learning algorithms. Finally, we simulate\ntrading and develop optimal trading strategy with\npractical constraints by using reinforcement learn-\ning method, which shows the capabilities of rein-\nforcement learning methods in solving realistic\nliquidation problems.\n1. Introduction\nLiquidation, as one kind of stock trading, is one of the main\nfunctions of ﬁnancial institutes, and the ability to minimize\nselling cost and manage risk level would be a key indicator\nof their ﬁnancial performance. Therefore, effective trading\nstrategy is of great importance. Financial institutes are up-\ndating their strategies recently, by making use of advanced\n1Department of Statistics, Columbia University , New York,\nUS 2Electrical Engineering, Columbia University, New York, US.\nCorrespondence to: Wenhang Bao <wb2304@columbia.edu>,\nXiao-Yang Liu <xl2427@columbia.edu>.\nProceedings of the 36 th International Conference on Machine\nLearning, Long Beach, California, PMLR 97, 2019. Copyright\n2019 by the author(s).\nFigure 1. Liquidation: multiple agents sell stocks in the market,\nand their selling decisions would affect each others’ selling cost\nresearch results or cutting-edge technologies. However,\nthere are several challenges. First, liquidation of a large\nnumber of stock shares would have huge impact on the mar-\nket, making the environment difﬁcult to predict. Secondly,\ncurrent methods for static environment ignore the dynamic\nand interactive nature of the stock market. Thirdly, the trad-\ning cost of liquidation depends on the stock market, and\nresearchers are usually not able to collect enough historical\nevents data to obtain practical trading insights.\nFinancial modeling and machine learning are two popular\napproaches in developing trading strategies, but both of\nthem have limitations. For the past years, ﬁnancial insti-\ntutes rely on experienced traders to minimize trading cost\nand manage liquidation risk. Also, researchers build math-\nematical and ﬁnancial models to help develop liquidation\nstrategies (Gomber et al., 2011; Brogaard et al., 2010). How-\never, mathematical and ﬁnancial modelling methods rely on\ntheir assumptions, which usually over-simplify the problem.\nMost recently, researchers started to adopt machine learning\nmethods as well.\nReinforcement learning (RL), one type of machine learning\nmethods, consists of agents interacting with the environment\nto learn an optimal policy by trail and error for sequential\ndecision-making problems (Sutton & Barto, 2018; Van Has-\nselt et al., 2016). While most of the successes of RL have\nbeen in the single agent domain, where modelling or pre-\ndicting the behavior of other actors in the environment is not\nconsidered, the obtained trading strategy (Xiong et al., 2018)\narXiv:1906.11046v1  [q-fin.TR]  24 Jun 2019\nMulti-Agent Deep Reinforcement Learning for Liquidation Strategy Analysis\nignores the stochastic and interactive nature of the trading\nmarket. A more general scenario would be that multiple\norganizations or customers want to liquidate their assets\nunder certain market conditions at the same time. Therefore,\nthe trading market would have multiple players or institutes\nwith similar objectives (Bansal et al., 2017; Tampuu et al.,\n2017), and the behavior of one agent would affect other\nagents’ behaviors (Yang et al., 2018b), as shown in Fig 1.\nAnother scenario would be even if there is only one com-\npany working on the liquidation of one stock, but still there\ncould be multiple traders and each of them be responsible\nfor a certain percentage of shares to sell.\nThis calls for the demand of applying multi-agent reinforce-\nment learning methods to the ﬁnancial industry, which has\nnot been well-studied as much as single-agent reinforce-\nment learning. There are attractive successes of multi-agent\ndeep reinforcement learning in the ﬁelds of gaming playing\n(Silver et al., 2016; Mnih et al., 2015), robotics and ﬁnan-\ncial trading system (Yu et al., 2019; Buehler et al., 2019).\nThe main beneﬁt of using reinforcement learning for liqui-\ndation is that mathematical models or hard-coded trading\nstrategies can be avoided. Reinforcement learning agent\nwould learn the trading strategy on its own. In addition,\na simulated environment would allow agents to adapt to\ndifferent market conditions and trade stocks, and obtain far\nmore experience than human traders could obtain in real\nﬁnancial market (Schaul et al., 2015; Foerster et al., 2017).\nLast but not the least, multi-agent reinforcement learning\nalgorithms can take into account high-level environment\ncomplexities (Hendricks & Wilcox, 2014) and derive more\npractical liquidation strategies accordingly.\nThe main contribution of this paper is the analysis of the\nmulti-agent trading environment, the impact analysis of co-\nordinated relationship between agents, and the derivation\nof liquidation strategies. Ideally, if the multi-agent environ-\nment is complex enough to incorporate all potential players’\nbehaviors, there would be no noise in the stock market, as all\norders are generated by players, and all players’ behaviors\nare modelled systematically by the multi-agent system. We\nbuild a simpliﬁed version of the multi-agent environment,\nwhich is the foundation of more complicated environments.\nFirst, we extend the model proposed by Almgren and Chriss\n(Almgren & Chriss, 2001) to the multi-agent environment\nand provide mathematical proofs. We make use of reinforce-\nment learning to verify our theorems and conclude with the\nnecessity to use multi-agent reinforcement learning instead\nof conventional single-agent reinforcement learning algo-\nrithms to analyze the liquidation problem. Secondly, we\ndemonstrate how agents learn to cooperate or compete with\neach other by deﬁning proper reward functions, analyze how\nthese agents inﬂuence each other as well as the environment\nas a whole, which cannot be analyzed by a single-agent\nenvironment, but of great importance to ﬁnancial institutes.\nThirdly, we derive trading strategies for each agent in a\nsimulated multi-agent environment. This demonstrates the\ncapabilities of reinforcement learning algorithms in learning\nand developing practical liquidation strategies.\nThe remainder of this paper is organized as follows. Section\n2 describes the liquidation problem and reviews the Alm-\ngren and Chriss model that is used to simulate the market\nenvironment. Section 3 introduces the detailed settings of\nmulti-agent reinforcement learning. Section 4 is about the\nextension of the multi-agent market environment. Section\n5 presents the experimental results where we demonstrate\nhow agents would behave in cooperative or competitive re-\nlationships and how to derive liquidation strategy. Section 6\nconcludes this paper and points out some future direction.\nCode is available at: https://github.com/WenhangBao/Multi-\nAgent-RL-for-Liquidation\n2. Problem Description\nIn this section, we ﬁrst describe the liquidation problem\nand explain why it is feasible to use reinforcement learning\nalgorithms to address it. Then we describe the Almgren and\nChriss model or the trading environment.\n2.1. Optimal Liquidation Problem\nWe consider a liquidation trader who aims to sell X shares\nof one stock within a time frame T. Liquidator’s personal\ncharacteristics, such as risk aversion level λ, would remain\nunchanged throughout the process. The trader can either sell\nor not sell stocks, but cannot buy any stock during the time\nframe T. On the last day of the time frame, the liquidation\nprocess ends and the number of shares should be 0. Since\nthe trading volume is tremendous, the market price P will\ndrop during selling, temporarily or permanently, potentially\nresulting in enormous trading costs.\nThe trader or the representative ﬁnancial institute seeks to\nﬁnd an optimal selling strategy, minimizing the expected\ntrading cost E(X), or called implementation shortfall, sub-\nject to certain optimization criterion. The trader would know\nall the environment information includes price, historical\nprice and number of trading days remaining. If there are\nJ traders, they would not know other traders’ information.\nFor instance, they would not know other traders’ remaining\nshares or risk aversion levels.\nBased on the assumption that the trading would have market\nimpacts as well as that agents and environment are inter-\nactive, it is feasible to train agents in the environment and\nderive liquidation strategies with reinforcement learning\nalgorithms (Yang et al., 2018a).\nMulti-Agent Deep Reinforcement Learning for Liquidation Strategy Analysis\n2.2. Environment Model for the Simulation\nThe problem of an optimal liquidation strategy is investi-\ngated by using the Almgren-Chriss market impact model\n(Almgren & Chriss, 2001) on the background that the agents\nliquidate assets completely in a given time frame. The im-\npact of the stock market is divided into three components:\nunaffected price process, permanent impact, and temporary\nimpact. The stochastic component of the price process ex-\nists, but is eliminated from the mean-variance. The price\nprocess permits linear functions of permanent and temporary\nprice. Therefore, the model serves as the trading environ-\nment such that when agents make selling decisions, the\nenvironment would return price information.\nThe price process of the Almgren and Chriss model (Alm-\ngren & Chriss, 2001) is as follows:\n• Price under temporary and permanent impact\nPk = Pk−1 + στ 1/2ξk −τg(nk\nτ ), k = 1, . . . , N\nwhere σ represents the volatility of the stock, ξk are\nrandom variables with zero mean and unit variance,\ng(v) is a function of the average rate of the trading,\nv = nk/τ during time interval tk−1 to tk, nk is the\nnumber of shares to sell during time interval tk−1 to\ntk, N is the total number of trades and τ = T/N.\n• Inventory process: xtk = X −Pk\nj=1 nj, where xtk\nis the number of shares remaining at time tk, with\nxT = 0.\n• Linear permanent impact function g(v) = γv, where\nv = nk\nτ .\n• Temporary impact function h( nk\nτ ) = ϵ sgn(nk)+ η\nτ nk,\nwhere a reasonable estimate of ϵ is the ﬁxed costs of\nselling, and η depends on internal and transient aspects\nof the market micro-structure.\n• Parameters σ, γ, η, ϵ, time frame T, number of trades\nN are set at t = 0.\n3. Deep Reinforcement Learning Approach\nWe model the liquidation process as a Markov decision\nprocess (MDP), and then formulate the multi-agent setting\nwe used to resolve the problem. The training diagram is also\ncovered, which explains how multiple agents interact and\nlearn from environment in details. We use implementation\nshortfall as the metric of selling cost, and the properties of\nMDP process allows us to deﬁne the goal as minimizing the\nexpected implementation shortfall.\n3.1. Liquidation as a MDP Problem\nConsider the stochastic and interactive nature of the trading\nmarket, we model the stock trading process as a Markov\ndecision process, which is speciﬁed as follows:\n• State s = [r, m, l]: a set that includes the information\nof the log-return r ∈RD\n+, where D is the number\nof days of log-return, and the remaining number of\ntrades m normalized by the total number of trades, the\nremaining number of shares l, normalized by the total\nnumber of shares. The log-returns capture information\nabout stock prices before time tk, where k is the current\nstep. It is important to note that in real world trading\nscenarios, this state vector may hold more variables.\n• Action a: we interpret the action ak as a selling frac-\ntion. In this case, the actions will take continuous\nvalues in between 0 and 1.\n• Reward R(s, a): to deﬁne the reward function, we use\nthe difference between two consecutive utility func-\ntions. The utility function is given by:\nU(x) = E(x) + λV (x),\n(1)\nE(x) =\nN\nX\nk=1\nτxkg(nk\nτ ) +\nN\nX\nk=1\nnkh(nk\nτ ),\n(2)\nV (x) = σ2\nN\nX\nk=1\nτx2\nk,\n(3)\nwhere λ is the risk aversion level, and x is the trading\ntrajectory or the vector of shares remaining at each time\nstep k, 0 ≤tk ≤T. After each time step, we compute\nthe utility using the equations for E(x) and V (x) from\nthe Almgren and Chriss model for the remaining time\nand inventory while holding parameter λ constant. De-\nnotes the optimal trading trajectory computed at time t\nby x∗\nt , we deﬁne the reward as:\nRt = Ut(x∗\nt ) −Ut+1(x∗\nt+1).\n(4)\n• Policy π(s): The liquidation strategy of stocks at state\ns. It is essentially the distribution of selling percentage\na at state s.\n• Action-value function Qπ(s, a): the expected reward\nachieved by action a at state s, following policy π.\n3.2. Multi-agent Reinforcement Learning Setting\nThe advantages of multi-agent over single-agent reinforce-\nment learning is the ability to incorporate high-level com-\nplexities in the system. The single-agent environment is a\nspecial case where the number of agents J = 1. It simpliﬁes\nMulti-Agent Deep Reinforcement Learning for Liquidation Strategy Analysis\nthe problem and would automatically inherit all properties\nfrom the multi-agent environment. Following the MDP con-\nﬁguration in the last section, we specify our multi-agent\nreinforcement learning setting as follows:\n• States s = [r, m, l] : in a multi-agent environment,\nthe state vector should have information about the re-\nmaining stocks of each agent. Therefore, in a J agents\nenvironment, the state vector at time tk would be:\n[rk−D, . . . , rk−1, rk, mk, l1,k, . . . , lJ,k],\nwhere\n– rk = log(\nPk\nPk−1 ) is the log-return at time tk.\n– mk = Nk\nN is the number of trades remaining at\ntime tk normalized by the total number of trades.\n– lj,k = xj,k\nXj is the remaining number of shares for\nagent j at time tk normalized by the total number\nof shares.\n• Action a: using the interpretation in Section 3.1, we\ncan determine the number of shares to sell for each at\neach time step using:\nnj,k = aj,k × xj,k,\nwhere xj,k is the number of remaining shares at time\ntk for agent j.\n• Reward R(s, a): denotes the optimal trading trajectory\ncomputed at time t for agent j by x∗\nj,t, we deﬁne the\nreward as:\nRj,t = Uj,t(x∗\nj,t) −Uj,t+1(x∗\nj,t+1).\n(5)\n• Observation O: Each agent only observes limited state\ninformation (Omidshaﬁei et al., 2017). In other words,\nin addition to the environment information, each agent\nonly knows its own remaining shares, but not other\nagents’ remaining shares. The observation vector at\ntime tk for agent j is:\nOj,k = [rk−D, . . . , rk−1, rk, mk, lj,k].\n3.3. Deep Reinforcement Learning Algorithm\nWe adopt the Actor-Critic (Mnih et al., 2016; Lowe et al.,\n2017) method that uses neural networks to approximate both\nthe Q-value and the action. The critic learns the Q-value\nfunction and uses it to update actor’s policy parameters. The\ncritic network estimates the expected return of a state-action\npair. The actor brings the advantage of computing continu-\nous actions without the need of a Q-value function, while\nthe critic supplies the actor with knowledge of the perfor-\nmance. The actor network has state s as input and returns\naction a directly. Actor-critic methods usually have good\nconvergence properties, in contrast to critic-only methods.\nAlgorithm 1 DDPG-Based Multi-agent Training\nInput: number of episodes M, time frame T, minibatch\nsize N, learning rate λ, and number of agents J\n1: for j = 1, J % initialize each agent separately do\n2:\nRandomly initialize critic network Qj(Oj, a|θQ\nj ) and\nactor network µj(Oj|θµ\nj ) with random weight θQ\nj and\nθµ\nj for agent j;\n3:\nInitialize target network Q′\nj and µ′\nj with weights\nθQ′\nj\n←θQ\nj , θµ′\nj ←θµ\nj for each agent j;\n4:\nInitialize replay buffer Bj for each agent j;\n5: end for\n6: for episode = 1, M do\n7:\nInitialize a random process N for action exploration;\n8:\nReceive initial observation state s0;\n9:\nfor t = 1, T do\n10:\nfor j = 1, J %train each agent separately do\n11:\nSelect action aj,t = µj(Oj,t|θµ\nj )+Nt according\nto the current policy and exploration noise;\n12:\nend for\n13:\nEach agent executes action aj,t;\n14:\nMarket state changes to st+1;\n15:\nEach agent observes reward rj,t and observation\nOj,t+1;\n16:\nfor j = 1, J do\n17:\nStore transition (Oj,t, aj,t, rj,t, Oj,t+1) in Bj;\n18:\nSample a random minibatch of N transitions\n(Oj,i , aj,i , rj,i , Oj,i+1) from Bj;\n19:\nSet\nyj,i = rj,i + γQ′\nj(st+1, µ′\nj(Oj,i+1|θµ′\nj |θQ′\nj ))\nfor i = 1, . . . , N;\n20:\nUpdate the critic by minimizing the loss: L =\n1\nN\nP\ni(yj,i −Qj(Oj,i, aj,i|θQ\nj ))2;\n21:\nUpdate the actor policy by using the sampled\npolicy gradient:\n∇θµπ ≈1\nN\nX\ni\n∇aQj(O, a|θQ\nj )|O=Oj,i,a=µj(Oj,i)\n× ∇θµµj(Oj|θµ)|si;\n22:\nUpdate the target networks:\nθQ′\nj\n←τθQ\nj + (1 −τ)θQ′\nj ,\nθµ′\nj ←τθµ\nj + (1 −τ)θµ′\nj .\n23:\nend for\n24:\nend for\n25: end for\nThe Deep Deterministic Policy Gradients (DDPG) algo-\nrithm (Lillicrap et al., 2016) is one example of an actor-\ncritic method. We will use DDPG to generate the optimal\nMulti-Agent Deep Reinforcement Learning for Liquidation Strategy Analysis\nexecution strategy of liquidation. DDPG uses three skills\nto make sure it gets converged experimental results: ex-\nperience replay buffer, learning rate and exploration noise.\nExperienced replay method (Wang et al., 2016) enables the\nstochastic gradient decent method and removes correlations\nbetween consecutive transitions. Learning rate controls the\nupdating speed of the neural network. Exploration noise\naddresses the exploration and exploitation trade-off. With\nthese training skills, the agent would learn from trail and\nerror and ﬁnd the optimal trading trajectory that minimizes\nthe trading cost. In other words, we will use the DDPG al-\ngorithm or Alg. 1 to solve the optimal liquidation problem.\n4. Performance Analysis\nHere we extend the classical environment model to multi-\nagent scenario for liquidation problem analysis.\n4.1. Optimal Multi-agent Liquidation Shortfall\nTheorem 4.1. In a multi-agent environment with J agents\nwhere each agent has Xj shares to sell within a given time\nframe T, the total expected shortfall is larger than or equal\nto the sum of expected shortfall that these agents would\nobtain if they are in single-agent environment, such that:\nJ\nX\nj=1\nE(Xj) ≤E(\nJ\nX\nj=1\nXj),\n(6)\nwhere E(X) is the expected implementation shortfall of\nliquidating X shares of a stock.\nProof. According to the Almgren and Chriss model,\n(namely, equation (20) in (Almgren & Chriss, 2001)), the\noptimal expected shortfall is:\nE(X) = 1\n2γX2 + ϵX + ˜ηφX2,\n(7)\nwhere X is the initial stock size and φ is a parameter related\nwith environment setting but unrelated with the stock size\nX.\nTherefore,\nE(\nJ\nX\nj=1\nXj) = 1\n2γ(\nJ\nX\nj=1\nXj)2 + ϵ\nJ\nX\nj=1\nXj + ˜η(\nJ\nX\nj=1\nXj)2φ\n≥1\n2γ\nJ\nX\nj=1\nX2\nj + ϵ\nJ\nX\nj=1\nXj + ˜η\nJ\nX\nj=1\nX2\nj φ\n=\nJ\nX\nj=1\nE(Xj).\n4.2. Multi-agent Interaction\nTheorem 4.2. In a two-agent environment where agent 1\nhas risk aversion level λ1 and agent 2 has risk aversion\nlevel λ2, where λ1 ̸= λ2, and each of them has the same\nnumber of stocks to liquidate, the biased trajectories x(λ1)\nand x(λ2) would satisfy that\nx∗(λ1) ̸= x(λ1), x∗(λ2) ̸= x(λ2),\nwhere x∗(λ1) and x∗(λ2) are the optimal trading trajecto-\nries when they are the only player in the market.\nRemark. In a multi-agent environment where each agent\nhas risk aversion level λj, the actual trading trajectory\nx(λj) would be biased against the optimal trading trajec-\ntory.\nProof. According to (4) of the Almgren and Chriss model\n(Almgren & Chriss, 2001),\nV (x) = σ2\nN\nX\nk=1\nτx2\nk\nis irrelevant to either temporary or permanent price changes,\nwhere xk is the remaining shares remaining at time tk. The\noptimal trading trajectory is of the form:\nxk = sinh(κ(λ)(T −tj)\nsinh(κ(λ)T)\nX,\nwhere κ(λ) =\nλσ2\nη(1−γτ\n2η ).\nLet X be the total stock size and agent 1 and 2 each has 1\n2X\nshares, the utility function\nU(x) = E(x) + λ∗V (x)\nis a quadratic function of the parameters x1, . . . , xN−1,\nwhere λ∗is the synthesized risk aversion level, x is the\ntrading trajectory, and it could also be written as:\nU(x) = E(x) + λ1V (x1) + λ2V (x2),\nwhere x1, x2 is the trading trajectory for agent 1, 2, respec-\ntively. Then:\n∂U\n∂xk\n= 2τ\n\u001a\n(λ1 + λ2\n2\n)σ2xk −˜η xk−1 −2xk + xk+1\nτ 2\n\u001b\n,\nand ∂U\n∂xk = 0 is equivalent to\n1\nτ 2 (xk−1 −2xk + xk+1) = (˜κ∗)2xk\n(8)\nwith\n˜κ∗=\n(λ1+λ2)\n2\nσ2\nη(1 −γτ\n2η ) ,\nMulti-Agent Deep Reinforcement Learning for Liquidation Strategy Analysis\nwhere the tilde denotes an O(τ) correction; as τ →0, we\nhave ˜κ →κ. Then, we know that the solution to (8) is:\nxk = sinh(κ∗(T −tk))\nsinh(κT)\nX\n̸= sinh(κ(λ1)(T −tk))\nsinh(κ(λ1)T)\n1\n2X + sinh(κ(λ2)(T −tk))\nsinh(κ(λ2)T)\n1\n2X,\nwhere the right-hand side is the total number of remaining\nshares at time t if both agents follow their original trading\ntrajectories, and that is not equal to the total remaining\nshares at time t under optimal trading trajectory, which is\nthe left-hand side of the function. In other words, their new\ntrading trajectories would be biased.\n5. Performance Evaluation\nWe ﬁrst describe the simulation environment in details, and\nthen verify the Theorem 4.1 and Theorem 4.2 by exper-\niments. We then use reinforcement learning methods to\ndemonstrate how agents learn to cooperate or compete with\neach other by deﬁning proper reward functions, and analyze\nhow the relationship would inﬂuence each individual player\nas well as the environment. Finally, we derive practical\ntrading strategies in a multi-agent environment.\nWe implement a typical reinforcement learning workﬂow to\ntrain the actor and critic. We change the single-agent Alm-\ngren and Chriss model (Almgren & Chriss, 2001) settings to\nbuild the multi-agent environment. We adjust reward func-\ntions to manipulate agents’ relationships. We use Alg. 1 to\nﬁnd a policy that can generate the optimal trading trajectory\nwith minimum implementation shortfall. We feed the states\nobserved from our simulator to each agent. These agents\nﬁrst predict actions using the actor model and perform these\nactions in the environment. Then, environment returns their\nrewards and new states. This process continues for a given\nnumber of episodes.\n5.1. Simulation Environment\nThis environment simulates stock prices that follow a dis-\ncrete arithmetic random walk, and that the permanent and\ntemporary market impact functions are linear functions of\nthe rate of trading, as in the Almgren and Chriss model\n(Almgren & Chriss, 2001).\nWe set the total number of shares to 1 million and the initial\nstock price to be P0 = 50, which gives an initial portfolio\nvalue of $50 million dollars. The stock price has 12% annual\nvolatility, a bid-ask spread of 1/8, the difference between\nask price and bid price, and an average daily trading volume\nof 5 million shares. Assuming that there are 250 trading\ndays in a year, this gives a daily volatility in stock price of\n0.12/\n√\n250 ≈0.8% . We use a liquidation time frame of\nT = 60 days and we set the number of trades N = 60. This\nleads to τ = T\nN = 1 , which means that we will be making\none trade per day. These settings are changeable and can be\nadjusted to same day liquidation as well.\nFor the temporary cost function, we set the ﬁxed cost of\nselling to be 1/2 of the bid-ask spread, so ϵ = 1/16. We set\nη such that for each one percent of the daily volume we trade,\nthe price impact equals to the bid-ask spread. For example,\ntrading at a rate of 5% of the daily trading volume incurs a\none-time cost on each trade of 5/8. Under this assumption\nwe have η = (1/8)/(0.01 × 5 × 106) = 2.5 × 106.\nFor the permanent costs, a common rule of thumb is that\nprice effects become signiﬁcant when we sell 10% of the\ndaily volume. Here, by ”signiﬁcant” we mean that the\nprice depression is one bid-ask spread, and that the effect\nis linear for both smaller and larger trading rates, then we\nhave γ = (1/8)/(0.1 × 5 × 106) = 2.5 × 107.\nIn all our experiments, we run the program for 10000\nepisodes, unless it is speciﬁed. Also, we use the follow-\ning reward deﬁnition:\n˜Rj,t = Uj,t(x∗\nj,t) −Uj,t+1(x∗\nj,t+1)\nUj,t(x∗\nj,t)\n,\n(9)\nwhich normalizes the reward.\n5.2. Theorem Veriﬁcation\n5.2.1. OPTIMAL LIQUIDATION SHORTFALL\nWe ﬁrst train one agent A who would need to liquidate 1\nmillion shares of a stock. Then we train two agents B1\nand B2 who have the same targets and X1, X2 = 0.3, 0.7\nmillion shares, respectively. Agents A, B1 and B2 have the\nsame risk aversion level λA = λB1 = λB2 = 1e−6. As we\ncan see from Fig. 2, the expected implementation shortfall\nE(A) is larger than the sum of E(B1) and E(B2).\nThis result justiﬁes Theorem 4.1. The intuition behind The-\norem 4.1 and Equation 7 is that the total expected shortfall\nincreases faster than the total number of stock shares.\n5.2.2. MULTI-AGENT INTERACTION\nHere we would like to analyze the trading trajectory of two\nagents, or Theorem 4.2 as an illustration. We ﬁrst train\nagent A1 with risk aversion level λA1 = 1e −4 and agent\nA2 with risk aversion level λA2 = 1e −9. Both A1 and\nA2 are trained separately in a single-agent environment.\nThen we train agent B1 and B2 with risk aversion level\nλB1 = 1e −4, λB2 = 1e −9, respectively, in a two-agent\nenvironment. All these agents have the same goal as deﬁned\nin Section 3. The trading trajectories of A1, A2, B1, B2 are\nshown in Fig. 3.\nComparing to the single-agent environment, we can see that\nthe trading trajectory of B1 and B2 are biased. Unlike the\nMulti-Agent Deep Reinforcement Learning for Liquidation Strategy Analysis\nFigure 2. Comparison of expected implementation shortfalls: there\nare three agents A, B1 and B2. The expected shortfall of agent A\nis higher than the sum of two expected shortfalls B1 and B2.\nFigure 3. Trading trajectory: comparing to their original trading\ntrajectories, their current trading trajectories are closer to each\nother when they are trained in a multi-agent environment.\nsingle-agent scenario, where they can sell their shares inde-\npendently, now they have to take into consideration of other\nplayers in the market. The selling patterns of other agents\nwould affect their liquidation strategy. The results not only\njustify Remark 4.2 and Theorem 4.2, it also demonstrates\nthe necessity of using multi-agents reinforcement learning\nalgorithm to derive trading strategy. All traders are inﬂuenc-\ning each other when they are executing their own strategy.\nTherefore, training one agent in a single-agent environment\nover-simpliﬁes the stochastic and dynamic nature of the\nstock market, as we have explained in Section 1.\n5.3. Multi-agent Coordinated Relationship\nTo analyze the emergence of a variety of coordinated be-\nhaviors, we adjust the rewarding schemes to change the\nrelationship between agents. There are only two agents in\nthis environment for illustration purpose. Each agent would\nbe responsible for selling 0.5 million shares of a stock. They\nshare the same risk aversion level λ = 1e −6. The only\ndifference between the next two experiments is the deﬁni-\ntion of the reward functions. Then we compare the sum\nof expected shortfalls with the expected shortfall trained\nindependently, to evaluate how the relationship would affect\nthe total as well as individual implementation shortfalls.\nFigure 4. Cooperative and competitive relationships: if two agents\nare in cooperative relationship, the total expected shortfall is not\nbetter than training with independent reward functions. If two\nagents are in a competitive relationship, they would ﬁrst learn to\nminimize expected shortfall, and then malignant competition leads\nto signiﬁcant implementation shortfall increment.\n5.3.1. MULTI-AGENTS COOPERATION\nIn this setting we want to analyze how agents would behave\nwhen they are in a cooperative relationship. Therefore, we\nadjust the reward function as follows:\n˜R∗\n1,t = ˜R∗\n2,t =\n˜R1,t + ˜R2,t\n2\n,\n(10)\nwhere ˜R∗\nj,t are the new reward functions.\nBoth agents would be rewarded by the sum of their individ-\nual rewards. So the two agents would be fully cooperative\nto minimize implementation shortfall. The result is shown\nin Fig. 4. First, we notice that the sum of expected shortfall\ndoes not change much comparing to training two agents with\nreward function ˜Rj,t. Secondly, new individual implemen-\ntation shortfall E∗(x∗\nj) does not change much comparing\nto the original implementation shortfall E(x∗\nj), where x∗\nj is\nthe optimal trading trajectory.\n5.3.2. MULTI-AGENTS COMPETITION\nIn this setting we want to analyze how agents would behave\nwhen they are in a competitive relationship. Therefore, we\nadjust the reward function as follows:\nif ˜R1,t > ˜R2,t then\n˜R∗\n1,t = ˜R1,t,\n˜R∗\n2,t = ˜R2,t −˜R1,t,\nelse\n˜R∗\n2,t = ˜R2,t,\n˜R∗\n1,t = ˜R1,t −˜R2,t,\nend if\nwhere ˜R∗\nj,t are the new reward functions.\nIn this case, the agent that gets higher reward would keep\nit, but the agent with lower reward would be penalized. It\nwould receive reward value equal to its original reward mi-\nMulti-Agent Deep Reinforcement Learning for Liquidation Strategy Analysis\nFigure 5. Trading trajectory: comparing to independent training,\nintroducing a competitor makes the host agent learn to adapt to\nnew environment and sell all shares of stock in the ﬁrst two days.\nnus the higher reward, which is a negative value. As we can\nsee from Fig. 4 that the sum of expected shortfalls ends up\nwith about twice as they are independent or in cooperative re-\nlationship. By looking at the snapshot of trading trajectory at\n1500 episode and 10000 episode, we notice that at the 1500\nepisode, these two agents learn to maximize utility func-\ntion deﬁned in Section 3.1. Both agents perform well and\nroughly have the same expected shortfall. However, as they\nare in a competitive relationship, after another 500 episodes\nof training, one agent learns to ourperform the other, which\nleads to the signiﬁcant increment of the sum of expected\nshortfall, or P2\nj=1 E∗(x∗\nj) > P2\nj=1 E(x∗\nj). The trading\ntrajectory of the last episode shows that one agent learns\nto sell all its shares on Day 1. In addition, the expected\nshortfall for both agents increase, or E∗(x∗\nj) > E(x∗\nj).\nWe conclude that not only their overall performance is di-\nminished, their individual performance is worse as well.\nNone of them is winning from their mutual competition.\n5.4. Liquidation Strategy Development\nWe use reinforcement learning algorithm to develop trading\nstrategies, given the trading trajectories of the competitors.\nHere we introduce an agent who has 0.5 million shares of\nstocks to sell and has risk aversion level λ = 1e −9. We\nhave already seen in Fig. 3 that the optimal trading trajectory\nfor such agent would be a straight line, which means it sells\na ﬁxed amount of stocks everyday.\nWe train an agent who has another 0.5 million shares of\nstocks to sell with risk aversion level λ = 1e −6. For com-\nparison purpose, we also draw the optimal trading trajectory\nwhen the agent is trained independently in a single-agent\nenvironment. As we can see in Fig. 5, if there is no com-\npetitor, the optimal trajectory shows that the agent would\ncomplete the liquidation process in about 20 days. After we\nintroduced the competitor, the trading trajectory completely\nchanged. Now the agent sells all its shares within the ﬁrst 2\ndays. The agent learns to avoid taking unnecessary risk by\nselling all shares in a quite short time, and let the competitor\nagent to bear the execution cost of price drop.\n6. Conclusion and Future Work\n6.1. Contributions\nWe have shown the single-agent environment over-simpliﬁes\nthe dynamic as well as the interactive nature of the stock\nmarket. All orders are generated by individual traders, and\nthese traders act as game players, especially for a systematic\ntrading problem like liquidation.\nIn the present work, we extended the scope of Almgren\nand Chriss model (Almgren & Chriss, 2001) and used re-\ninforcement learning method to verify it, which setups the\nfoundation of the multi-agent trading environment. We illus-\ntrate the demand to use multi-agent environments to develop\ntrading strategies. We analyze how fully cooperative and\ncompetitive relationship would affect the total and individ-\nual implementation shortfalls, respectively. We conclude\nthat cooperative relationship is not better than independent\none, and competitive relationship would hurt the overall and\nindividual performance. Finally, we demonstrate the capa-\nbility of reinforcement learning agent and derived optimal\nliquidation strategy for the host agent against its competitor.\n6.2. Limitations\nAs the goal of this paper is to analyze the environment and\nagents interaction, we keep simple setting as long as it is\nreasonable. Therefore, we did not build more complex neu-\nral network architectures, and our best expected shortfall\nafter 10000 episodes of training is roughly 20% higher than\nthe optimal expected shortfall derived by the Almgren and\nChriss model (Almgren & Chriss, 2001). We can add more\ndynamic factors in the state vector. Also, advanced back-\nground models other than Amlgren and Chriss model could\nalso be considered. While all these methods could poten-\ntially improve this work, we believe that at this moment they\nare not necessary for describing the nature of multi-agent\ntrading environments and analyzing agents’ behaviors, for\nthis preliminary analysis of liquidation problem.\n6.3. Future Work\nDevelopment of more realistic trading environment, includ-\ning more dynamic factors such as news, general strategy\nand legal complaints, would make great contributions to\nﬁnancial analysis. A potential extension is the study of\nstock liquidation by considering the optimistic bull or pes-\nsimistic bear (Li et al., 2019b) or the anomaly events (Li\net al., 2019a). A potential application would be using pre-\ndicted agents’ behaviors to predict stock price movements,\nsay LSTM (Li et al., 2019a).\nMulti-Agent Deep Reinforcement Learning for Liquidation Strategy Analysis\nReferences\nAlmgren, R. and Chriss, N. Optimal execution of portfolio\ntransactions. Journal of Risk, 3:5–40, 2001.\nBansal, T., Pachocki, J., Sidor, S., Sutskever, I., and Mor-\ndatch, I. Emergent complexity via multi-agent competi-\ntion. arXiv preprint arXiv:1710.03748, 2017.\nBrogaard, J. A., Brennan, T., Korajczyk, R., Mcdonald, R.,\nand Vissing-jorgensen, A. High frequency trading and its\nimpact on market quality, 2010.\nBuehler, H., Gonon, L., Teichmann, J., and Wood, B. Deep\nhedging. Quantitative Finance, pp. 1–21, 2019.\nFoerster, J., Nardelli, N., Farquhar, G., Afouras, T., Torr,\nP. H., Kohli, P., and Whiteson, S. Stabilising experience\nreplay for deep multi-agent reinforcement learning. In\nProceedings of the 34th International Conference on Ma-\nchine Learning-Volume 70, pp. 1146–1155. JMLR. org,\n2017.\nGomber, P., Arndt, B., Lutat, M., and Elko Uhle, T. High-\nfrequency trading. SSRN Electronic Journal, 01 2011.\ndoi: 10.2139/ssrn.1858626.\nHendricks, D. and Wilcox, D. A reinforcement learning ex-\ntension to the almgren-chriss framework for optimal trade\nexecution. In IEEE Conference on Computational Intelli-\ngence for Financial Engineering & Economics (CIFEr),\npp. 457–464. IEEE, 2014.\nLi, X., Li, Y., , Liu, X.-Y., and Wang, C. Risk management\nvia anomaly circumvent: Mnemonic deep learning for\nmidterm stock prediction. In KDD Workshop on Anomaly\nDetection in Finance, 2019a.\nLi, X., Li, Y., Zhan, Y., and Liu, X.-Y. Optimistic bull or\npessimistic bear: adaptive deep reinforcement learning\nfor stock portfolio allocation. In ICML Workshop on\nApplications and Infrastructure for Multi-Agent Learning,\n2019b.\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T.,\nTassa, Y., Silver, D., and Wierstra, D. Continuous control\nwith deep reinforcement learning. ICLR, 2016.\nLowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O. P.,\nand Mordatch, I.\nMulti-agent actor-critic for mixed\ncooperative-competitive environments. In Advances in\nNeural Information Processing Systems, pp. 6379–6390,\n2017.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, et al. Human-\nlevel control through deep reinforcement learning. Nature,\n518(7540):529, 2015.\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,\nT., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn-\nchronous methods for deep reinforcement learning. In\nInternational Conference on Machine Learning, pp. 1928–\n1937, 2016.\nOmidshaﬁei, S., Pazis, J., Amato, C., How, J. P., and Vian, J.\nDeep decentralized multi-task multi-agent reinforcement\nlearning under partial observability. In Proceedings of\nthe 34th International Conference on Machine Learning-\nVolume 70, pp. 2681–2690. JMLR. org, 2017.\nSchaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-\ntized experience replay. arXiv preprint arXiv:1511.05952,\n2015.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., et al.\nMastering the game of go with deep neural networks and\ntree search. Nature, 529(7587):484, 2016.\nSutton, R. S. and Barto, A. G. Reinforcement learning: An\nintroduction. MIT press, 2018.\nTampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus,\nK., Aru, J., Aru, J., and Vicente, R. Multiagent cooper-\nation and competition with deep reinforcement learning.\nPloS One, 12(4):e0172395, 2017.\nVan Hasselt, H., Guez, A., and Silver, D. Deep reinforce-\nment learning with double q-learning. In Thirtieth AAAI\nConference on Artiﬁcial Intelligence, 2016.\nWang, Z., Bapst, V., Heess, N., Mnih, V., Munos, R.,\nKavukcuoglu, K., and de Freitas, N.\nSample efﬁ-\ncient actor-critic with experience replay. arXiv preprint\narXiv:1611.01224, 2016.\nXiong, Z., Liu, X.-Y., Zhong, S., Walid, A., et al. Practical\ndeep reinforcement learning approach for stock trading.\nNeurlIPS Workshop on Challenges and Opportunities for\nAI in Financial Services, 2018.\nYang, H., Liu, X.-Y., and Wu, Q. A practical machine\nlearning approach for dynamic stock recommendation. In\nIEEE International Conference On Trust, Security And\nPrivacy (TrustCom), pp. 1693–1697. IEEE, 2018a.\nYang, Y., Luo, R., Li, M., Zhou, M., Zhang, W., and Wang,\nJ. Mean ﬁeld multi-agent reinforcement learning. In 35th\nInternational Conference on Machine Learning, ICML\n2018, volume 80, pp. 5571–5580. PMLR, 2018b.\nYu, P., Lee, J. S., Kulyatin, I., Shi, Z., and Dasgupta, S.\nModel-based deep reinforcement learning for dynamic\nportfolio optimization. arXiv preprint arXiv:1901.08740,\n2019.\n",
  "categories": [
    "q-fin.TR",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-06-24",
  "updated": "2019-06-24"
}