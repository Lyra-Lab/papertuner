{
  "id": "http://arxiv.org/abs/1907.01180v1",
  "title": "Conservative Q-Improvement: Reinforcement Learning for an Interpretable Decision-Tree Policy",
  "authors": [
    "Aaron M. Roth",
    "Nicholay Topin",
    "Pooyan Jamshidi",
    "Manuela Veloso"
  ],
  "abstract": "There is a growing desire in the field of reinforcement learning (and machine\nlearning in general) to move from black-box models toward more \"interpretable\nAI.\" We improve interpretability of reinforcement learning by increasing the\nutility of decision tree policies learned via reinforcement learning. These\npolicies consist of a decision tree over the state space, which requires fewer\nparameters to express than traditional policy representations. Existing methods\nfor creating decision tree policies via reinforcement learning focus on\naccurately representing an action-value function during training, but this\nleads to much larger trees than would otherwise be required. To address this\nshortcoming, we propose a novel algorithm which only increases tree size when\nthe estimated discounted future reward of the overall policy would increase by\na sufficient amount. Through evaluation in a simulated environment, we show\nthat its performance is comparable or superior to traditional tree-based\napproaches and that it yields a more succinct policy. Additionally, we discuss\ntuning parameters to control the tradeoff between optimizing for smaller tree\nsize or for overall reward.",
  "text": "Conservative Q-Improvement: Reinforcement Learning for an\nInterpretable Decision-Tree Policy\nAaron M. Roth1 , Nicholay Topin2 , Pooyan Jamshidi3 and Manuela Veloso2\n1Robotics Institute, Carnegie Mellon University\n2Machine Learning Department, Carnegie Mellon University\n3University of South Carolina\n{aaronr1, ntopin}@andrew.cmu.com, pjamshid@cse.sc.edu, mmv@cs.cmu.edu\nAbstract\nThere is a growing desire in the ﬁeld of reinforce-\nment learning (and machine learning in general) to\nmove from black-box models toward more “inter-\npretable AI.” We improve interpretability of rein-\nforcement learning by increasing the utility of de-\ncision tree policies learned via reinforcement learn-\ning. These policies consist of a decision tree over\nthe state space, which requires fewer parameters\nto express than traditional policy representations.\nExisting methods for creating decision tree poli-\ncies via reinforcement learning focus on accurately\nrepresenting an action-value function during train-\ning, but this leads to much larger trees than would\notherwise be required. To address this shortcom-\ning, we propose a novel algorithm which only in-\ncreases tree size when the estimated discounted fu-\nture reward of the overall policy would increase by\na sufﬁcient amount. Through evaluation in a sim-\nulated environment, we show that its performance\nis comparable or superior to traditional tree-based\napproaches and that it yields a more succinct pol-\nicy. Additionally, we discuss tuning parameters to\ncontrol the tradeoff between optimizing for smaller\ntree size or for overall reward.\n1\nIntroduction\nMany powerful machine learning algorithms are “black\nboxes” [Knight, 2017]. They produce models which can be\nused to make predictions or policies that can be used to per-\nform tasks, but these models are often opaque or require sig-\nniﬁcant analysis to penetrate. “Black blox” algorithms are\nless likely to be accepted for use by an organization, and\nadoption occurs more slowly than for processes which are\ndirectly interpretable [Hihn and Menzies, 2015]. It is difﬁ-\ncult for humans to trust decisions that cannot be veriﬁed, and\nthus much research has gone into creating interpretable mod-\nels [Wu et al., 2017] as part of general growing interest in\ninterpretable AI [Bloomberg, 2018].\nWe look speciﬁcally at the ﬁeld of Reinforcement Learn-\ning (RL), which in recent years has demonstrated impressive\nresults on tasks such as the games of Go and Dota2, walking\nas a simulated human-like agent, and accomplishing a vari-\nety of real-world robotic tasks [Gitau, 2019; Arulkumaran et\nal., 2017]. However, many advanced RL methods today re-\nsult in policies which perform well but which are not directly\ninterpretable. We are motivated to build an RL algorithm that\ncan learn a policy that is in a human understandable form.\nHowever, in contrast to after-the-fact explanations or approx-\nimations, we use an interpretable representation that is simul-\ntaneously the policy used to perform the task.\nA human-understandable RL model is useful in a scenario\nwhere an end-user wishes to continually tweak a policy, or\nchange it after it has been learned.\nConsider a situation\nwhere a household assistant robot learns the preferences of\nit’s owner and how to perform certain kinds of tasks in their\nhome. If the owner goes on a diet, or otherwise changes some\nspeciﬁc aspect of their behavior, it would be useful to com-\nmunicate this to the robot by simply accessing the robot’s\nsettings, selecting the task model, and changing the relevant\nspeciﬁc aspect of the policy. This would be faster than the\nagent having to learn the simple change over time, and less\ncomplicated than requiring the robot to understand a human’s\nrequest and make the change itself.\nTo improve the interpretability of RL policies, we focus on\nlearning a decision tree policy. Decision trees are frequently\nused to explain processes to humans, such as when a deep\nnetwork is trained to explain itself [Alaniz and Akata, 2019;\nHuk Park et al., 2018]. Decision trees can be represented by\ntext or graphically in a way that a human can readily compre-\nhend. The tree, once constructed, also has useful information\nabout which states are treated identically and which features\nyield the highest information gain [Buntine and Niblett, 1992;\nLeroux et al., 2018]. A reasonably-sized tree with labeled\nattributes enables a human to understand the behavior of a\nlearned policy without having to observe its execution.\nOur main contribution is the Conservative Q-Improvement\n(CQI) reinforcement learning algorithm. CQI learns a policy\nin the form of a decision tree, is applicable to any domain in\nwhich the state space can be represented as a vector of fea-\ntures, and generates a human-interpretable decision tree pol-\nicy. Additionally, we evaluate CQI on a modiﬁed RobotNav\nenvironment [Mitchell and Thrun, 1993].\nThe rest of our paper is organized as follows: Section 2\ndiscusses relevant background. Section 3, discusses previous\nrelated work. Section 4 describes our approach. Section 5 de-\narXiv:1907.01180v1  [cs.LG]  2 Jul 2019\nscribes the environment we use to evaluate our method. We\ncompare our algorithm to a reference baseline method in Sec-\ntion 6 and discuss conclusions in Section 7.\n2\nBackground\nReinforcement Learning\nReinforcement learning is a ﬁeld\nof algorithms for obtaining policies of (state →action) map-\npings through experience in an environment [Sutton and\nBarto, 2018]. We focus on Q-learning: Given a set of states\ns ∈S, set of actions a ∈A, and an immediate reward for ex-\necuting an action a in a state s, the expected value (Q-value)\nof performing an action in a state can be learned over time by\nexperimenting with actions in states, observing the reward,\nand updating the Q-value estimate via the Bellman equation:\nQt+1(s, a) ←(1−α)Qt(s, a)+α(r+γ max\na′ Qt(s′, a′)) (1)\nwhere Qt is the current Q-value estimate for a state-action\npair, s and a are the current state and chosen action, α is the\nlearning rate, r is the immediate reward for choosing action\na when in state s (as experienced immediately by interacting\nwith the environment), s′ is the next state the agent is in after\nexecuting a, and a′ is the best possible next action, such that\nthe maximum expected future reward from being in state s′ is\nadded to r after being discounted by the factor γ.\nDecision Trees\nDecision trees have often been used to cre-\nate easy-to-understand solutions to classiﬁcation problems,\namong others. They are trees that start at a root node and\nbranch based on conditions [Quinlan, 1986]. An additional\nbeneﬁt to decision trees is that they can be represented graph-\nically, which aids in human understanding [Mulrow, 2002].\nSome work has explored combining neural nets and decision\ntree forms [Tanno et al., 2018; Zhao, 2001], including using\ndecision trees to explain neural nets [Zhang et al., 2018]. We\nseek to create a reinforcement learning policy that is in deci-\nsion tree form.\nIn an AI context speciﬁcally, it has been suggested that\nhuge decision trees are insufﬁciently comprehensible to hu-\nmans, and that simplifying trees is important for humans to\ninteract with the representation [Quinlan, 1987]. Chess ex-\nperts had difﬁculty understanding an early chess-playing al-\ngorithm that fully “explained” itself with a complex decision\ntree—too complex a tree becomes opaque [Michie, 1983,\n1987].\nIt has been shown that, all else being equal, hu-\nmans prefer simpler explanations compared to more complex\nones [Lombrozo, 2007]. Based on the foregoing, we assume\nfor the remainder of the paper that smaller decision trees are\nmore interpretable by humans.\n3\nRelated Work\nWe are not the ﬁrst to attempt to learn a decision tree policy.\nThe “G algorithm” [Chapman and Kaelbling, 1991] learns a\ndecision tree incrementally as new examples are added. The\nLumberjack algorithm [Uther and Veloso, 2000] constructs a\nLinked Decision Forest—like a decision tree but without the\nneed for repeated internal structure which would otherwise\noccur. TTree [Uther and Veloso, 2003; Uther, 2002] solves\nMDPs or SMDPs and involves both state abstraction (as in\nour and other methods) and temporal abstraction. [Pyeatt et\nal., 2001] is a work which uses a tree as an internal structure\nand incrementally builds the tree, splitting on Q-values. Un-\nlike [Hester et al., 2010], an approach that uses decision trees\nas part of the policy representation, we use a single decision\ntree to represent the entire policy. [Wu et al., 2012] create\ndecision trees using a post-learning transformation, but we\ninstead maintain our policy as a tree at all stages of training\nand merely update our tree.\nThe UTree algorithm [McCallum and Ballard, 1996; Uther\nand Veloso, 1998] learns a tree, starting with a single abstract\nstate and then continually splitting as appropriate (based on\na splitting criterion), ending when a stopping criterion is\nreached. This is similar to our method, except that in UTree\nsplits occur based on how accurately the Q-function is rep-\nresented rather than based on what improves the policy. An\neffort to produce a decision tree via RL is found in [Gupta et\nal., 2015], but the ﬁnal product is not directly interpretable:\nit consists of a tree with linear Gibbs softmax sub-policies as\nleaves. In contrast, our approach creates a tree with action\nchoices as leaves.\nThe best decision-tree-via-RL approach of which we are\naware is the Pyeatt Method [Pyeatt, 2003] (PM), which we\nimplement and use as a comparison. PM differs from our\nmethod in terms of its splitting criteria and the manner in\nwhich splits are performed. Like our method, PM starts with\na single root node and adds branches and nodes over time.\nPM maintains a history of changes in Q-values for each leaf,\nand splits when it believes that this history represents two dis-\ntributions. We instead use a lookahead approach that predicts\nwhich split will produce the largest increase in reward. The\ntrees resulting from the Pyeatt Method tend to be larger than\nthose that result from our approach, which is designed to keep\nthe size of the tree small and manageable.\nIndeed, in none of the above cases are there constraints\non the size of the decision trees or merit placed on having\na smaller tree (beyond gains in performance). Our work is\naimed at enabling interpretability; this requires more than\nsimply using a tree structure as an underlying mechanism.\nCertainly we want to learn a good policy, but no less impor-\ntant for us is that the resulting form of the policy be easy to\nunderstand.\n4\nMethod\n4.1\nMethod Summary\nWe introduce Conservative Q-Improvement (CQI), an algo-\nrithm for learning an interpretable decision-tree policy. CQI\nlearns a policy in the form of a decision tree while only adding\nto the tree when doing so would improve the policy.\nEach node in the tree is a branch node or leaf node. Branch\nnodes have two children and a condition based on a feature of\nthe state space. Leaf nodes correspond to abstract states, and\nindicate actions that are to be taken.\nThe tree is initialized with a single leaf node, representing\na single abstract state. Our method is strictly additive. Over\ntime, it creates branches and nodes by replacing existing leaf\nnodes with a branch node and two child leaf nodes.\nAlgorithm 1: Conservative Q-Improvement\n1 Tree ←initial tree is a single leaf node;\n2 HS ←the starting threshold for what potential ∆Q is\nrequired to trigger a split;\n3 hS ←HS;\n4 D ←the decay for HS and hS values;\n5 for number of episodes do\n6\nwhile episode not done do\n7\nst ←current state at timestep t;\n8\nL ←leaf of Tree corresponding to st;\n9\nat, rt, st+1 ←TakeAction(L);\n10\nUpdateLeafQValues(L, at, rt, st+1);\n11\nUpdateVisitFrequency(Tree, L);\n12\nUpdatePossibleSplits(L, st, at, st+1);\n13\nbestSplit, bestV alue ←BestSplit(Tree, L, at);\n14\nif bestV alue > hS then\n15\nSplitNode(L, bestSplit);\n16\nelse\n17\nhS ←hS · D\n18\nend\n19\nend\n20 end\nThe tree is split only when the expected discounted future\nreward of the new policy would increase above a dynamic\nthreshold. This minimum threshold decreases slowly over\nthe course of training so the split which induces the largest\nincrease in reward is chosen. Additionally, the threshold re-\nsets to its initial value after every split. This way, when the\ncurrent tree does not represent the Q-values as accurately fol-\nlowing a change, a greater increase in reward is required to\njustify a split. In this way, the method is “conservative” in\nperforming splits.\nThe high level algorithm is shown in Algorithm 1. An\noverview is given here, with more in-depth explanations fol-\nlowing in Section 4.2.\nAt a given timestep, the environment will be in state st.\nThis will correspond to some leaf node L. When there is a\nsingle leaf node, all states correspond to this node. When\nthere are multiple leaf nodes, the tree must be traversed to\ndetermine the corresponding L (each branch node having a\nboolean condition that operates on st, indicating which of two\nchildren to traverse).\nOnce L is identiﬁed, an action at is chosen. If exploring,\na random action is taken. If exploiting, an action is chosen\nbased on the highest Q-values on that leaf. Executing the\naction yields a reward rt and next-state st+1. A series of\nupdates are then performed.\nThe Q-values on leaf L are updated using the standard Bell-\nman equation update. Each node keeps track of how often it\nhas been visited. Each leaf node maintains a history of possi-\nble splits. These are potential means of converting the leaf\nnode into a branch node with two child nodes. These es-\ntimated child nodes have hypothetical visit frequencies and\nQ-values, which are updated at this time.\nThen, the possible splits are checked to determine which\nsplit would yield the most split value, where split value is\nincrease-in-expected-reward moderated by visit frequency. If\nthis value is beyond a threshold hS, the split occurs. Other-\nwise, this threshold is decreased slightly, by a decay value D.\nIn this manner, the split with the best expected reward gain\nis chosen, and the time between splits is longer when the in-\ncrease is small, thereby allowing the leaves to better represent\nthe Q-function before a split occurs. Splits can occur at any\ntimestep. hS loosely affects the rate at which splits will occur.\n4.2\nMethod Details\nA detailed explanation of the algorithm follows in this sec-\ntion. The tree itself consists of two node types, “branching\nnodes” and “leaf nodes”.\nA branching node has the following attributes: visit fre-\nquency v, dimension to split on m (our method assumes a\nmultidimensional state space), and a value to split on ub.\nEach state falls within a single node. To determine the node\nfor state s, the tree is traversed, starting at the root. If the\nvalue of the state us in the dimension m is less than the ub\nvalue on the node, the left child branch of the node is tra-\nversed. (If us ≥ub, the right branch is traversed.) When a\nleaf node is reached, traversal ends—the leaf node L here cor-\nresponds to the abstract state s. This is the process referenced\nin Algorithm 1 line 8.\nA leaf node has the following attributes: visit frequency v,\na mapping of actions to Q-values (Q : (a →q)), and a list of\nSplit objects Splits.\nEach Split object contains information about potential\nsplits (one per potential split), and has dimension number m,\nvalue to split on up, and Q-value for each action and visit fre-\nquency for both potential children (left.q, left.v, right.q,\nand right.v).\nThe TakeAction method in line 9 of Algorithm 1 is detailed\nin Algorithm 2. This is the standard RL behavior, where an\nϵ value or function controls whether to explore (randomly\nchoose a valid action) or exploit (use action-Q-value map on\nL to determine action with highest expected value). An ac-\ntion, reward, and next state are returned.\nAlgorithm 2: Take Action\n1 TakeAction(L):\n2 if X ∼U([0,1]) < ϵ then\n3\nat ←random valid action;\n4 else\n5\nat ←the action that has the largest Q-value\n(Q(st, at)) on L;\n6 end\n7 rt ←reward after executing action at;\n8 st+1 ←state after executing action at;\n9 Return at, rt, st+1;\nThe Q-values on the leaves (for each leaf, one Q-value per\nvalid action) are updated using the standard Bellman update,\nas shown in Algorithm 3.\nWe keep track of how often a state node is visited. (A node\nis visited if it is a leaf node corresponding to a state the agent\nis in, or if it is a parent to such a leaf node.) In Algorithm\nAlgorithm 3: UpdateLeafQValues\n1 UpdateLeafQValues(L, at, rt, s′):\n2 L[Q][at] ←(1−α)L[Q][at]+α(r +γ maxa′ Q(s′, a′));\n4, these metrics are updated. Note that the update rule corre-\nsponds to an exponential average, so it is a natural choice to\naccompany Q-learning. N[v] indicates the visit frequency of\nnode N, and {a =⇒b} : T indicates the set of nodes along\nthe direct path from node a to node b in tree T. When de-\ntermining splits later, it is important to weight potential gain\nby relative frequency visited. It serves to convert the value\nincrease within a leaf to an estimated policy-wide increase.\nThe reference to a “sibling of N” on line 4 refers to the child\nof a parent of N that is not N. In our method, a node in the\ntree can have only two children or no children.\nAlgorithm 4: UpdateVisitFrequency\n1 UpdateVisitFrequency(Tree, L):\n2 for node N ∈{Tree[root] =⇒L} : Tree do\n3\nN[v] ←N[v] · d + (1 −d);\n4\nB ←sibling of N;\n5\nB[v] ←B[v] · d;\n6 end\nHyperparameters:\nd ←visit decay factor\nEach leaf keeps track of potential Splits that could be per-\nformed (turning a leaf node into branch node and children\nleaves). As shown in Algorithm 5, the Splits maintain and\nupdate their own metrics for the potential visit frequency\nand potential Q-values for the case that a speciﬁc split is\nperformed. The ̸∼notation is used to express “opposite side”\nEach split has a left and right side. L[Q] refers to the {action\n→Q-values} mapping on L, and L[Q][a] refers to the Q-\nvalue of action a on node L.\nAfter the updates occur, it is necessary to retrieve infor-\nmation as to which of the potential splits are currently the\nbest possible split, and what the expected gain (∆Q) would\nbe if that split occurs. This process is shown in Algorithm\n6. In line 2, VP gets the product of the visit frequencies on\nall of the nodes along the path from the root node to L. SQ\ntracks splits and split values, and split values are the sum of\nthe difference between Q-values (on both the left and right\nAlgorithm 5: UpdatePossibleSplits\n1 UpdatePossibleSplits(L, st, at, s′):\n2 for Split ∈L[Splits] do\n3\nside ←‘left’ or ‘right’ depending on which side of\nthe split corresponds to st;\n4\nSplit[side][Q][at] ←\n(1 −α)L[Q][at] + α(r + γ maxa′ Q(s′, a′));\n5\nSplit[side][v] ←Split[side][v] · d + (1 −d);\n6\nSplit[̸∼side][v] ←Split[side][v] · d;\n7 end\nAlgorithm 6: BestSplit\n1 BestSplit(Tree, L, at):\n2 VP ←Π{T ree[root]=⇒L}:T ree\nN\n(N[v]);\n3 % SQ is a mapping of splits to split value (indicating\nexpected increase in Q if a split is used);\n4 SQ : (split →∆Q) ←∅;\n5 for Split ∈L[Splits] do\n6\ncl ←(maxa′ (Split[left][Q][a′]) −L[Q][at]);\n7\ncr ←(maxa′ (Split[right][Q][a′]) −L[Q][at]);\n8\nSQ[Split] ←\nVP · (cl · Split[left][v] + cr · Split[right][v])\n9 end\n10 bestSplit ←argmaxsplit SQ[split];\n11 bestV alue ←SQ[bestSplit];\nside, each weighted by visits). Each split is investigated to\ndetermine which Split ∈L[Splits] has the largest ∆Q. This\nvalue along with the split itself are returned to the parent al-\ngorithm (where, if ∆Q > a threshold hS, a split is performed,\nand if not, hS is decayed).\n“Performing a split” means to take a leaf node and turn it\ninto a branch node with two children leaf nodes. This process\nis described in Algorithm 7.\nAlgorithm 7: SplitNode\nSplits leaf node N into nodes B, L, R\n1 SplitNode(N, bestSplit):\n2 Let B be an internal branching node.;\n3 Let L, R be left and right children of B, respectively.;\n4 B[v] ←N[v];\n5 B[m] ←bestSplit[m];\n6 B[u] ←bestSplit[u];\n7 for N ∈{L, R} do\n8\nN[Splits] ←{ Add a set of left-right split pairs, one\nfor each dimension and split.}\n9 end\n10 L[v] ←bestSplit[left][v];\n11 L[Q] ←bestSplit[left][Q];\n12 R[v] ←bestSplit[right][v];\n13 R[Q] ←bestSplit[right][Q];\n14 Return B, L, R\n———————\nHyperparameters:\nQinit ←default Q-value;\nnumSplits ←the number of splits to check for in each\ndimension;\n5\nEnvironment\nWe evaluate our approach on the RobotNav environment from\n[Mitchell and Thrun, 1993]. We have also written an open-\nsource OpenAI Gym version of the environment for anyone\nelse to use.1\n1[Link to code available after review.]\nFigure 1: An image of RobotNav, the domain in which we evaluate\nCQI. As noted in [Mitchell and Thrun, 1993]: “a) The simulated\nrobot world, b) Actions”.\nIn this 2D environment, there is a robot, a goal location,\nand one or more obstacles. The robot must navigate to the\ngoal, avoiding the obstacles, as shown in Figure 1.\nIn addition to the action space in the original work, we also\nadd the option for using the following action set (corresponds\nto automatically turning to face the goal after every action):\n1. move directly towards goal\n2. move directly away from goal\n3. move perpendicular to direct-line-to-goal (right)\n4. move perpendicular to direct-line-to-goal (left)\n6\nResults\n6.1\nDirect Comparison\nWe evaluate the Pyeatt Method and Conservative Q-\nImprovement Trees on the RobotNav environment. Overall,\nwe ﬁnd that CQI creates trees with an order of magnitude\nfewer nodes and greater average reward per episode than the\nPyeatt Method. Preliminary experiments on diverse alterna-\ntive domains exhibit the same behavior.\nTo fairly compare both methods, we perform a grid search\nfor each method to determine the best hyperparameters. We\nsearch over alpha values of 0.005, 0.01, and 0.1 through 0.8\nby increments 0.1. We test hist-mins for the Pyeatt Method of\n1000, 3000, 5000, 8000, and 10,000. For CQI, we try split-\nthresh-max values from 10−2 to 109 with a step of ×10. We\ntry num-splits values of 2 through 9. The visit decay and split\nthreshold decay parameters for CQI are held constant at 0.999\nand 0.9999, respectively. All of the best hyperparameter con-\nﬁgurations fall within the ranges we search.\nThe hyperparameters that yield the highest average reward-\nper-episode for PM are history-list-min-size of 5000 and al-\npha of 0.3. CQI is optimized twice, once for highest reward\nand once for smallest tree size. We note that optimizing CQI\nfor smallest tree size still yields a policy with higher reward\nthan the Pyeatt Method. The hyperparameters that yield the\nbest reward for CQI are alpha of 0.01, split-thresh-max of\n10, and num-splits of 3. The hyperparameters that result in\nthe smallest tree size are alpha of 0.2, split-thresh-max of\n107, and num-splits of 7. The results of ten trials for each of\nMethod\nTree Size\nAvg. Reward Per Episode\nAvg.\nStd. Dev.\nAvg.\nStd. Dev\nPyeatt Method\n194.2\n2.35\n-75.73\n59.05\nCQI (reward opt)\n20.2\n1.40\n-18.59\n0.45\nCQI (size opt)\n7\n0\n-27.20\n26.50\nTable 1: Comparison of CQI (optimized for reward or size) to Pyeatt\non RobotNav.\nthese parameter conﬁgurations are shown in Table 1. Gamma\nis set to 0.8 as in [Mitchell and Thrun, 1993]. Policies are\ntrained for 500,000 steps and evaluated across 50,000 steps,\nrecording average reward per episode and ﬁnal tree size. We\nuse epsilon-greedy exploration with parameter ϵ = 1 at the\nstart of training and decay epsilon linearly (to a minimum of\nϵ = 0.05) over the course of 100, 000 steps.\nCQI outperforms PM while resulting in substantially\nsmaller trees. Additionally, the variance for CQI is smaller.\nEven with the best hyperparameters we found, the Pyeatt\nmethod’s maximum reward at the end of training varied be-\ntween -182.57 and -19.58. In contrast, we ﬁnd that CQI (op-\ntimized for reward) consistently learns good policies. CQI\n(optimized for policy size) failed to learn a good policy in\none trial, but otherwise performs better than the reward for\nthe best Pyeatt method trial; the average reward across the\nother nine trials is -18.825 and the variance is 0.47.\nWe believe that CQI manages to achieve smaller tree size\nand greater reward than the Pyeatt Method because it creates\na split only when the policy would improve, not just when\nthe Q-values can be more faithfully represented if a split were\nadded. As a result, CQI avoids extraneous splits, so the agent\ncan beneﬁt from greater abstraction over the state space.\nWe plot the reward obtained by each policy as a function\nof tree size (calculating reward by evaluating the policy as it\nexisted before the subsequent split) in Figure 2. The “only\nsuccessful runs” PM line refers to trials where PM achieved\nmore than −50 reward by the end of training. No trials were\nexcluded when plotting the CQI performance.\nIt is possible to force the Pyeatt method to achieve smaller\nFigure 2: Comparison of Reward vs Tree Size for CQI and Pyeatt\non RobotNav.\nFigure 3: Performance of CQI on RobotNav as a function of split\nthreshold maximum.\ntrees as an end-result of training by increasing history-list-\nmin-size, but it impairs performance. Increasing this parame-\nter to 50,000 yields trees of size 19 and 21 on RoboNav. The\naverage reward over 10 trials was -102.62 and -102.59, with\nvariances of 0.80 and 0.01, respectively. Essentially, when\nthe Pyeatt Method is forced to make trees as small as CQI\nnormally produces, the Pyeatt Method fails to learn.\n6.2\nParameter Sensitivity\nWe also investigated the sensitivity of CQI to changes in hy-\nperparameter values. There are a number of hyperparameters\nthat can be adjusted to inﬂuence desired behavior. Optimizing\nfor either size or reward is one such case. We considered the\neffect of num-split and split-thresh-max on reward and size,\nvarying each while keeping other parameters constant.\nThe num-split parameter has little effect on reward. Across\n10 trials, reward varies only slightly, from -18.84 to -17.88\n(excluding one outlier). With other parameters set to the val-\nues which minimize tree size, there is no discernible effect\non tree size: it ranges between 7 and 9, and with parameters\nset to values which maximized reward, size ranges from 15\nto 21. (At the same time, during the grid search, we observed\na trend of higher num-splits yielding smaller trees for hyper-\nparameter conﬁgurations far from those minimizing size or\nmaximizing reward).\nIncreasing split-threshold-maximum up to 107 has little\neffect on reward (at parameters otherwise optimized for re-\nward), with ﬁnal episode reward ranging from -19.49 to -\n17.75. Note that these are all greater than the best recorded\nPyeatt trial which yielded a reward of -19.58.\nFor values\ngreater that 108, the reward drops precipitously, passing be-\nlow -100, as shown in Figure 3. This follows intuition, since\nsplit-threshold-maximum controls how long to wait before\nmaking a split. With a ﬁxed training period, too long an inter-\nval between splits means that it is impossible to split enough\ntimes to create a successful policy. Another intuitive result\nof increasing split-threshold-maximum is the very clear trend\nof decreasing tree size shown in Figure 4. Waiting longer\nto perform a split means that the earlier splits are better in-\nFigure 4: Policy size of CQI on RobotNav as a function of split\nthreshold maximum.\nformed and therefore higher quality. When this happens, it is\nmore difﬁcult to improve the policy further, so it takes more\nsteps for potential splits to reach the split threshold. This re-\nsult highlights the importance of this parameter since larger\nvalues can yield smaller trees. However, increasing training\ntime allows for smaller trees without sacriﬁcing ﬁnal reward.\nAs long as the training time is sufﬁcient for the chosen split-\nthreshold-maximum, the ﬁnal policy will have sufﬁcient time\nto train and avoid the “cliff” which results in poor policies.\n7\nConclusion and Future Work\nWe have introduced Conservative Q-Improvement. CQI is\na reinforcement learning method applicable to any environ-\nment with discrete actions and multidimensional state space.\nIt produces a policy in the form of a decision tree, and we\nhave designed it to produce smaller trees than existing meth-\nods while not sacriﬁcing policy performance.\nWe investigated the nature of the CQI method, discussing\nways that one might optimize for tradeoffs between smaller\ntrees and strictly better policies.\nWe found that CQI out-\nperformed the Pyeatt method in terms of both tree size and\nreward achieved. This is due to CQI’s conservative nature,\nwhereby it only creates a new node when it will result in an\nimprovement in the policy.\nThe tradeoffs of CQI can be tuned depending on the use\ncase, and depending on domain. For example, if there were\na domain where even CQI produces large trees, a practitioner\ncould settle for lower reward in order to have a smaller tree\nthat can be understood and inspected. On the other hand,\nif all policies for an environment were relatively small, or\nthe policy only had to be manually inspected occasionally or\nwithout hurry, then one could choose parameter values which\nresult in a larger, better-performing policy.\nFuture work could entail tree-building methods that are not\nstrictly additive but rebalance along the way so as to reduce\nﬁnal policy size. Additionally, a regularization term which\nallows the tree size to directly affect the policy’s perceived\nreward would allow CQI to directly optimize for a trade-off\nbetween size and performance on original tasks.\nReferences\nStephan Alaniz and Zeynep Akata.\nXoc:\nExplainable\nobserver-classiﬁer for explainable binary decisions. arXiv\npreprint arXiv:1902.01780, 2019.\nKai Arulkumaran, Marc Peter Deisenroth, Miles Brundage,\nand Anil Anthony Bharath. A brief survey of deep rein-\nforcement learning. CoRR, 2017.\nJason Bloomberg. Don’t trust artiﬁcial intelligence? time to\nopen the ai ‘black box’, 2018.\nWray Buntine and Tim Niblett. A further comparison of split-\nting rules for decision-tree induction. Machine Learning,\n8(1):75–85, 1992.\nDavid Chapman and Leslie Pack Kaelbling. Input general-\nization in delayed reinforcement learning: An algorithm\nand performance comparisons. In IJCAI, volume 91, pages\n726–731, 1991.\nCatherine Gitau. Success stories of reinforcement learning,\n2019.\nUjjwal Das Gupta, Erik Talvitie, and Michael Bowling. Pol-\nicy tree: Adaptive representation for policy gradient. In\nAAAI, pages 2547–2553, 2015.\nTodd Hester, Michael Quinlan, and Peter Stone. Generalized\nmodel learning for reinforcement learning on a humanoid\nrobot. In Robotics and Automation (ICRA), 2010 IEEE In-\nternational Conference on, pages 2369–2374. IEEE, 2010.\nJaitus Hihn and Tim Menzies.\nData mining methods and\ncost estimation models: Why is it so hard to infuse new\nideas?\nIn Automated Software Engineering Workshop\n(ASEW), 2015 30th IEEE/ACM International Conference\non, pages 5–9. IEEE, 2015.\nDong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Anna\nRohrbach, Bernt Schiele, Trevor Darrell, and Marcus\nRohrbach. Multimodal explanations: Justifying decisions\nand pointing to the evidence. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition,\npages 8779–8788, 2018.\nWill Knight. The dark secret at the heart of ai: no one really\nknows how the most advanced algorithms do what they do-\nthat could be a problem, 2017.\nAntonin Leroux, Matthieu Boussard, and Remi D`es.\nIn-\nformation gain ratio correction:\nImproving prediction\nwith more balanced decision tree splits.\narXiv preprint\narXiv:1801.08310, 2018.\nTania Lombrozo. Simplicity and probability in causal expla-\nnation. Cognitive psychology, 55(3):232–257, 2007.\nAndrew Kachites McCallum and Dana Ballard. Reinforce-\nment learning with selective perception and hidden state.\nPhD thesis, University of Rochester. Dept. of Computer\nScience, 1996.\nDonald Michie. Inductive rule generation in the context of the\nﬁfth generation. In Machine Learning Workshop, page 65,\n1983.\nDonald Michie.\nCurrent developments in expert systems.\nIn Proceedings of the Second Australian Conference on\nApplications of expert systems, pages 137–156. Addison-\nWesley Longman Publishing Co., Inc., 1987.\nTom M Mitchell and Sebastian B Thrun. Explanation-based\nneural network learning for robot control.\nIn Advances\nin neural information processing systems, pages 287–294,\n1993.\nEdward J Mulrow. The visual display of quantitative infor-\nmation, 2002.\nLarry D Pyeatt, Adele E Howe, et al. Decision tree func-\ntion approximation in reinforcement learning. In Proceed-\nings of the third international symposium on adaptive sys-\ntems: evolutionary computation and probabilistic graphi-\ncal models, volume 2, pages 70–77. Cuba, 2001.\nLarry D Pyeatt. Reinforcement learning with decision trees.\nIn Applied Informatics, pages 26–31, 2003.\nJ. Ross Quinlan. Induction of decision trees. Machine learn-\ning, 1(1):81–106, 1986.\nJ. Ross Quinlan. Simplifying decision trees. International\njournal of man-machine studies, 27(3):221–234, 1987.\nRichard S Sutton and Andrew G Barto. Reinforcement learn-\ning: An introduction. MIT press, 2018.\nRyutaro Tanno, Kai Arulkumaran, Daniel C Alexander, An-\ntonio Criminisi, and Aditya Nori. Adaptive neural trees.\nCoRR, 2018.\nWilliam TB Uther and Manuela M Veloso. Tree based dis-\ncretization for continuous state space reinforcement learn-\ning. In AAAI/IAAI, pages 769–774, 1998.\nWilliam TB Uther and Manuela M Veloso. The lumberjack\nalgorithm for learning linked decision forests. In Interna-\ntional Symposium on Abstraction, Reformulation, and Ap-\nproximation, pages 219–232. Springer, 2000.\nWilliam TB Uther and Manuela M Veloso. Ttree: Tree-based\nstate generalization with temporally abstract actions.\nIn\nAdaptive agents and multi-agent systems, pages 260–290.\nSpringer, 2003.\nWilliam T Uther.\nTree-based hierarchical reinforcement\nlearning. Technical report, Carnegie Mellon, Univ Pitts-\nburgh, PA, Dept of Computer Science, 2002.\nMin Wu, Atsushi Yamashita, and Hajime Asama. Rule ab-\nstraction and transfer in reinforcement learning by decision\ntree. In System Integration (SII), 2012 IEEE/SICE Interna-\ntional Symposium on, pages 529–534. IEEE, 2012.\nHuijun Wu, Chen Wang, Jie Yin, Kai Lu, and Liming Zhu.\nInterpreting shared deep learning models via explicable\nboundary trees. arXiv preprint arXiv:1709.03730, 2017.\nQuanshi Zhang, Yu Yang, Ying Nian Wu, and Song-Chun\nZhu. Interpreting cnns via decision trees. CoRR, 2018.\nQiangfu Zhao. Evolutionary design of neural network tree-\nintegration of decision tree, neural network and ga.\nIn\nEvolutionary Computation, 2001. Proceedings of the 2001\nCongress on, volume 1, pages 240–244. IEEE, 2001.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.RO"
  ],
  "published": "2019-07-02",
  "updated": "2019-07-02"
}