{
  "id": "http://arxiv.org/abs/1907.01657v2",
  "title": "Dynamics-Aware Unsupervised Discovery of Skills",
  "authors": [
    "Archit Sharma",
    "Shixiang Gu",
    "Sergey Levine",
    "Vikash Kumar",
    "Karol Hausman"
  ],
  "abstract": "Conventionally, model-based reinforcement learning (MBRL) aims to learn a\nglobal model for the dynamics of the environment. A good model can potentially\nenable planning algorithms to generate a large variety of behaviors and solve\ndiverse tasks. However, learning an accurate model for complex dynamical\nsystems is difficult, and even then, the model might not generalize well\noutside the distribution of states on which it was trained. In this work, we\ncombine model-based learning with model-free learning of primitives that make\nmodel-based planning easy. To that end, we aim to answer the question: how can\nwe discover skills whose outcomes are easy to predict? We propose an\nunsupervised learning algorithm, Dynamics-Aware Discovery of Skills (DADS),\nwhich simultaneously discovers predictable behaviors and learns their dynamics.\nOur method can leverage continuous skill spaces, theoretically, allowing us to\nlearn infinitely many behaviors even for high-dimensional state-spaces. We\ndemonstrate that zero-shot planning in the learned latent space significantly\noutperforms standard MBRL and model-free goal-conditioned RL, can handle\nsparse-reward tasks, and substantially improves over prior hierarchical RL\nmethods for unsupervised skill discovery.",
  "text": "Published as a conference paper at ICLR 2020\nDYNAMICS-AWARE UNSUPERVISED DISCOVERY\nOF\nSKILLS\nArchit Sharma∗, Shixiang Gu, Sergey Levine, Vikash Kumar, Karol Hausman\nGoogle Brain\n{architsh,shanegu,slevine,vikashplus,karolhausman}@google.com\nABSTRACT\nConventionally, model-based reinforcement learning (MBRL) aims to learn a\nglobal model for the dynamics of the environment. A good model can poten-\ntially enable planning algorithms to generate a large variety of behaviors and\nsolve diverse tasks. However, learning an accurate model for complex dynami-\ncal systems is difﬁcult, and even then, the model might not generalize well out-\nside the distribution of states on which it was trained. In this work, we combine\nmodel-based learning with model-free learning of primitives that make model-\nbased planning easy. To that end, we aim to answer the question: how can we\ndiscover skills whose outcomes are easy to predict? We propose an unsuper-\nvised learning algorithm, Dynamics-Aware Discovery of Skills (DADS), which\nsimultaneously discovers predictable behaviors and learns their dynamics. Our\nmethod can leverage continuous skill spaces, theoretically, allowing us to learn\ninﬁnitely many behaviors even for high-dimensional state-spaces. We demon-\nstrate that zero-shot planning in the learned latent space signiﬁcantly outper-\nforms standard MBRL and model-free goal-conditioned RL, can handle sparse-\nreward tasks, and substantially improves over prior hierarchical RL methods\nfor unsupervised skill discovery. We have open-sourced our implementation at:\nhttps://github.com/google-research/dads\nFigure 1: A humanoid agent discovers diverse locomotion primitives without any reward using DADS. We\nshow zero-shot generalization to downstream tasks by composing the learned primitives using model predic-\ntive control, enabling the agent to follow an online sequence of goals (green markers) without any additional\ntraining.\n1\nINTRODUCTION\nDeep reinforcement learning (RL) enables autonomous learning of diverse and complex tasks with\nrich sensory inputs, temporally extended goals, and challenging dynamics, such as discrete game-\nplaying domains (Mnih et al., 2013; Silver et al., 2016), and continuous control domains including\nlocomotion (Schulman et al., 2015; Heess et al., 2017) and manipulation (Rajeswaran et al., 2017;\nKalashnikov et al., 2018; Gu et al., 2017). Most of the deep RL approaches learn a Q-function\nor a policy that are directly optimized for the training task, which limits their generalization to\nnew scenarios. In contrast, MBRL methods (Li & Todorov, 2004; Deisenroth & Rasmussen, 2011;\nWatter et al., 2015) can acquire dynamics models that may be utilized to perform unseen tasks\nat test time. While this capability has been demonstrated in some of the recent works (Levine\net al., 2016; Nagabandi et al., 2018; Chua et al., 2018b; Kurutach et al., 2018; Ha & Schmidhuber,\n∗Work done a part of the Google AI Residency program.\n1\narXiv:1907.01657v2  [cs.LG]  14 Feb 2020\nPublished as a conference paper at ICLR 2020\n2018), learning an accurate global model that works for all state-action pairs can be exceedingly\nchallenging, especially for high-dimensional system with complex and discontinuous dynamics.\nThe problem is further exacerbated as the learned global model has limited generalization outside\nof the state distribution it was trained on and exploring the whole state space is generally infeasible.\nCan we retain the ﬂexibility of model-based RL, while using model-free RL to acquire proﬁcient\nlow-level behaviors under complex dynamics?\nWhile learning a global dynamics model that captures all the different behaviors for the entire state-\nspace can be extremely challenging, learning a model for a speciﬁc behavior that acts only in a small\npart of the state-space can be much easier. For example, consider learning a model for dynamics of\nall gaits of a quadruped versus a model which only works for a speciﬁc gait. If we can learn many\nsuch behaviors and their corresponding dynamics, we can leverage model-predictive control to plan\nin the behavior space, as opposed to planning in the action space. The question then becomes: how\ndo we acquire such behaviors, considering that behaviors could be random and unpredictable? To\nthis end, we propose Dynamics-Aware Discovery of Skills (DADS), an unsupervised RL framework\nfor learning low-level skills using model-free RL with the explicit aim of making model-based con-\ntrol easy. Skills obtained using DADS are directly optimized for predictability, providing a better\nrepresentation on top of which predictive models can be learned. Crucially, the skills do not require\nany supervision to learn, and are acquired entirely through autonomous exploration. This means that\nthe repertoire of skills and their predictive model are learned before the agent has been tasked with\nany goal or reward function. When a task is provided at test-time, the agent utilizes the previously\nlearned skills and model to immediately perform the task without any further training.\nThe key contribution of our work is an unsupervised reinforcement learning algorithm, DADS,\ngrounded in mutual-information-based exploration. We demonstrate that our objective can em-\nbed learned primitives in continuous spaces, which allows us to learn a large, diverse set of skills.\nCrucially, our algorithm also learns to model the dynamics of the skills, which enables the use of\nmodel-based planning algorithms for downstream tasks. We adapt the conventional model predic-\ntive control algorithms to plan in the space of primitives, and demonstrate that we can compose the\nlearned primitives to solve downstream tasks without any additional training.\n2\nPRELIMINARIES\nMutual information can been used as an objective to encourage exploration in reinforcement learning\n(Houthooft et al., 2016; Mohamed & Rezende, 2015). According to its deﬁnition, I(X; Y ) =\nH(X) −H(X | Y ), maximizing mutual information I with respect to Y amounts to maximizing\nthe entropy H of X while minimizing the conditional entropy H(X | Y ). In the context of RL, X is\nusually a function of the state and Y a function of actions. Maximizing this objective encourages the\nstate entropy to be high, making the underlying policy to be exploratory. Recently, multiple works\n(Eysenbach et al., 2018; Gregor et al., 2016; Achiam et al., 2018) apply this idea to learn diverse\nskills which maximally cover the state space.\nTo leverage planning-based control, MBRL estimates the true dynamics of the environment by learn-\ning a model ˆp(s′ | s, a). This allows it to predict a trajectory of states ˆτH = (st, ˆst+1, . . . ˆst+H)\nresulting from a sequence of actions without any additional interaction with the environment. While\nmodel-based RL methods have been demonstrated to be sample efﬁcient compared to their model-\nfree counterparts, learning an effective model for the whole state-space is challenging. An open-\nproblem in model-based RL is to incorporate temporal abstraction in model-based control, to enable\nhigh-level planning and move-away from planning at the granular level of actions.\nThese seemingly unrelated ideas can be combined into a single optimization scheme, where we ﬁrst\ndiscover skills (and their models) without any extrinsic reward and then compose these skills to\noptimize for the task deﬁned at test time using model-based planning. At train time, we assume\na Markov Decision Process (MDP) M1 ≡(S, A, p). The state space S and action space A are\nassumed to be continuous, and the A bounded. We assume the transition dynamics p to be stochastic,\nsuch that p : S × A × S 7→[0, ∞). We learn a skill-conditioned policy π(a | s, z), where the skills\nz belongs to the space Z, detailed in Section 3. We assume that the skills are sampled from a prior\np(z) over Z. We simultaneously learn a skill-conditioned transition function q(s′ | s, z), coined as\nskill-dynamics, which predicts the transition to the next state s′ from the current state s for the skill\nz under the given dynamics p. At test time, we assume an MDP M2 ≡(S, A, p, r), where S, A, p\n2\nPublished as a conference paper at ICLR 2020\nmatch those deﬁned in M1, and the reward function r : S × A 7→(−∞, ∞). We plan in Z using\nq(s′ | s, z) to compose the learned skills z for optimizing r in M2, which we detail in Section 4.\n3\nDYNAMICS-AWARE DISCOVERY OF SKILLS (DADS)\nAlgorithm 1: Dynamics-Aware Discovery\nof Skills (DADS)\nInitialize π, qφ;\nwhile not converged do\nSample a skill z ∼p(z) every episode;\nCollect new M on-policy samples;\nUpdate qφ using K1 steps of gradient\ndescent on M transitions;\nCompute rz(s, a, s′) for M transitions;\nUpdate π using any RL algorithm;\nend\nFigure 2: The agent π interacts with the environment to produce a transition s →s′. Intrinsic reward is\ncomputed by computing the transition probability under q for the current skill z, compared to random samples\nfrom the prior p(z). The agent maximizes the intrinsic reward computed for a batch of episodes, while q\nmaximizes the log-probability of the actual transitions of (s, z) →s′.\nWe use the information theoretic paradigm of mutual information to obtain our unsupervised skill\ndiscovery algorithm. In particular, we propose to maximize the mutual information between the next\nstate s′ and current skill z conditioned on the current state s.\nI(s′; z | s) = H(z | s) −H(z | s′, s)\n(1)\n= H(s′ | s) −H(s′ | s, z)\n(2)\nMutual information in Equation 1 quantiﬁes how much can be known about s′ given z and s, or\nsymmetrically, z given the transition from s →s′. From Equation 2, maximizing this objective\ncorresponds to maximizing the diversity of transitions produced in the environment, that is denoted\nby the entropy H(s′ | s), while making z informative about the next state s′ by minimizing the\nentropy H(s′ | s, z). Intuitively, skills z can be interpreted as abstracted action sequences which\nare identiﬁable by the transitions generated in the environment (and not just by the current state).\nThus, optimizing this mutual information can be understood as encoding a diverse set of skills in\nthe latent space Z, while making the transitions for a given z ∈Z predictable. We use the entropy-\ndecomposition in Equation 2 to connect this objective with model-based control.\nWe want to optimize the our skill-conditioned controller π(a | s, z) such that the latent space\nz ∼p(z) is maximally informative about the transitions s →s′. Using the deﬁnition of conditional\nmutual information, we can rewrite Equation 2 as:\nI(s′; z | s) =\nZ\np(z, s, s′) log p(s′ | s, z)\np(s′ | s) ds′dsdz\n(3)\nWe assume the following generative model: p(z, s, s′) = p(z)p(s | z)p(s′ | s, z), where p(z) is\nuser speciﬁed prior over Z, p(s|z) denotes the stationary state-distribution induced by π(a | s, z)\nfor a skill z and p(s′ | s, z) denotes the transition distribution under skill z. Note, p(s′ | s, z) =\nR\np(s′ | s, a)π(a | s, z)da is intractable to compute because the underlying dynamics are unknown.\nHowever, we can variationally lower bound the objective as follows:\nI(s′; z | s) = Ez,s,s′∼p\nh\nlog p(s′ | s, z)\np(s′ | s)\ni\n= Ez,s,s′∼p\nh\nlog qφ(s′ | s, z)\np(s′ | s)\ni\n+ Es,z∼p\nh\nDKL(p(s′ | s, z) || qφ(s′ | s, z))\ni\n≥Ez,s,s′∼p\nh\nlog qφ(s′ | s, z)\np(s′ | s)\ni\n(4)\n3\nPublished as a conference paper at ICLR 2020\nwhere we have used the non-negativity of KL-divergence, that is DKL ≥0. Note, skill-dynamics\nqφ represents the variational approximation for the transition function p(s′ | s, z), which enables\nmodel-based control as described in Section 4. Equation 4 suggests an alternating optimization\nbetween qφ and π, summarized in Algorithm 1. In every iteration:\n(Tighten variational lower bound) We minimize DKL(p(s′ | s, z) || qφ(s′ | s, z)) with respect to\nthe parameters φ on z, s ∼p to tighten the lower bound. For general function approximators like\nneural networks, we can write the gradient for φ as follows:\n∇φEs,z[DKL(p(s′ | s, z) || qφ(s′ | s, z))] = ∇φEz,s,s′\nh\nlog p(s′ | s, z)\nqφ(s′ | s, z)\ni\n= −Ez,s,s′\nh\n∇φ log qφ(s′ | s, z)\ni\n(5)\nwhich corresponds to maximizing the likelihood of the samples from p under qφ.\n(Maximize approximate lower bound) After ﬁtting qφ, we can optimize π to maximize\nEz,s,s′[log qφ(s′ | s, z) −log p(s′ | s)]. Note, this is a reinforcement-learning style optimiza-\ntion with a reward function log qφ(s′ | s, z) −log p(s′ | s). However, log p(s′ | s) is intractable to\ncompute, so we approximate the reward function for π:\nrz(s, a, s′) = log\nqφ(s′ | s, z)\nPL\ni=1 qφ(s′ | s, zi)\n+ log L,\nzi ∼p(z).\n(6)\nThe approximation is motivated as follows: p(s′ | s) =\nR\np(s′ | s, z)p(z|s)dz ≈\nR\nqφ(s′ |\ns, z)p(z)dz ≈1\nL\nPL\ni=1 qφ(s′ | s, zi) for zi ∼p(z), where L denotes the number of samples from\nthe prior. We are using the marginal of variational approximation qφ over the prior p(z) to approx-\nimate the marginal distribution of transitions. We discuss this approximation in Appendix C. Note,\nthe ﬁnal reward function rz encourages the policy π to produce transitions that are (a) predictable\nunder qφ (predictability) and (b) different from the transitions produced under zi ∼p(z) (diversity).\nTo generate samples from p(z, s, s′), we use the rollouts from the current policy π for multiple\nsamples z ∼p(z) in an episodic setting for a ﬁxed horizon T. We also introduce entropy regu-\nlarization for π(a | s, z), which encourages the policy to discover action-sequences with similar\nstate-transitions and to be clustered under the same skill z, making the policy robust besides en-\ncouraging exploration (Haarnoja et al., 2018a). The use of entropy regularization can be justiﬁed\nfrom an information bottleneck perspective as discussed for Information Maximization algorithm in\n(Mohamed & Rezende, 2015). This is even more extensively discussed from the graphical model\nperspective in Appendix B, which connects unsupervised skill discovery and information bottleneck\nliterature, while also revealing the temporal nature of skills z. Details corresponding to implemen-\ntation and hyperparameters are discussed in Appendix A.\n4\nPLANNING USING SKILL DYNAMICS\nGiven the learned skills π(a | s, z) and their respective skill-transition dynamics qφ(s′ | s, z), we\ncan perform model-based planning in the latent space Z to optimize for a reward r that is given to\nthe agent at test time. Note, that this essentially allows us to perform zero-shot planning given the\nunsupervised pre-training procedure described in Section 3.\nIn order to perform planning, we employ the model-predictive-control (MPC) paradigm Garcia et al.\n(1989), which in a standard setting generates a set of action plans Pk = (ak,1, . . . ak,H) ∼P for\na planning horizon H. The MPC plans can be generated due to the fact that the planner is able\nto simulate the trajectory ˆτk = (sk,1, ak,1 . . . sk,H+1) assuming access to the transition dynamics\nˆp(s′ | s, a). In addition, each plan computes the reward r(ˆτk) for its trajectory according to the\nreward function r that is provided for the test-time task. Following the MPC principle, the planner\nselects the best plan according to the reward function r and executes its ﬁrst action a1. The planning\nalgorithm repeats this procedure for the next state iteratively until it achieves its goal.\nWe use a similar strategy to design an MPC planner to exploit previously-learned skill-transition\ndynamics qφ(s′ | s, z). Note that unlike conventional model-based RL, we generate a plan Pk =\n(zk,1, . . . zk,HP ) in the latent space Z as opposed to the action space A that would be used by a\nstandard planner. Since the primitives are temporally meaningful, it is beneﬁcial to hold a primitive\n4\nPublished as a conference paper at ICLR 2020\nAlgorithm 2: Latent Space Planner\ns ←s0;\nInitialize parameters µ1, . . . µHP ;\nfor i ←1 to HE/HZ do\nfor j ←1 to R do\n{zi, . . . zi+HP −1}K\nk=1 ∼\nNi, . . . Ni+HP −1 ;\nCompute renv for\n{zi, . . . zi+HP −1}K\nk=1;\nUpdate µi, . . . , µi+HP −1;\nend\nSample zi from Ni;\nExecute π(a|s, zi) for HZ steps;\nInitialize µi+HP ;\nend\nFigure 3: At test time, the planner executes simulates the transitions in environment using skill-dynamics q,\nand updates the distribution of plans according to the computed reward on the simulated trajectories. After a\nfew updates to the plan, the ﬁrst primitive is executed in the environment using the learned agent π.\nfor a horizon HZ > 1, unlike actions which are usually held for a single step. Thus, effectively, the\nplanning horizon for our latent space planner is H = HP × HZ, enabling longer-horizon planning\nusing fewer primitives. Similar to the standard MPC setting, the latent space planner simulates the\ntrajectory ˆτk = (sk,1, zk,1, ak,1, sk,2, zk,2, ak,2, . . . sk,H+1) and computes the reward r(ˆτk). After\na small number of trajectory samples, the planner selects the ﬁrst latent action z1 of the best plan,\nexecutes it for HZ steps in the environment, and the repeats the process until goal completion.\nThe latent planner P maintains a distribution of latent plans, each of length HP . Each element\nin the sequence represents the distribution of the primitive to be executed at that time step. For\ncontinuous spaces, each element of the sequence can be modelled using a normal distribution,\nN(µ1, Σ), . . . N(µHP , Σ). We reﬁne the planning distributions for R steps, using K samples of\nlatent plans Pk, and compute the rk for the simulated trajectory ˆτk. The update for the parameters\nfollows that in Model Predictive Path Integral (MPPI) controller Williams et al. (2016):\nµi =\nK\nX\nk=1\nexp(γrk)\nPK\np=1 exp(γrp)\nzk,i\n∀i = 1, . . . HP\n(7)\nWhile we keep the covariance matrix of the distributions ﬁxed, it is possible to update that as well\nas shown in Williams et al. (2016). We show an overview of the planning algorithm in Figure 3, and\nprovide more implementation details in Appendix A.\n5\nRELATED WORK\nCentral to our method is the concept of skill discovery via mutual information maximization. This\nprinciple, proposed in prior work that utilized purely model-free unsupervised RL methods (Daniel\net al., 2012; Florensa et al., 2017; Eysenbach et al., 2018; Gregor et al., 2016; Warde-Farley et al.,\n2018; Thomas et al., 2018), aims to learn diverse skills via a discriminability objective: a good\nset of skills is one where it is easy to distinguish the skills from each other, which means they\nperform distinct tasks and cover the space of possible behaviors. Building on this prior work, we\ndistinguish our skills based on how they modify the original uncontrolled dynamics of the system.\nThis simultaneously encourages the skills to be both diverse and predictable. We also demonstrate\nthat constraining the skills to be predictable makes them more amenable for hierarchical composition\nand thus, more useful on downstream tasks.\nAnother line of work that is conceptually close to our method copes with intrinsic motiva-\ntion (Oudeyer & Kaplan, 2009; Oudeyer et al., 2007; Schmidhuber, 2010) which is used to drive\nthe agent’s exploration. Examples of such works include empowerment Klyubin et al. (2005); Mo-\nhamed & Rezende (2015), count-based exploration Bellemare et al. (2016); Oh et al. (2015); Tang\net al. (2017); Fu et al. (2017), information gain about agent’s dynamics Stadie et al. (2015) and\n5\nPublished as a conference paper at ICLR 2020\nforward-inverse dynamics models Pathak et al. (2017). While our method uses an information-\ntheoretic objective that is similar to these approaches, it is used to learn a variety of skills that can be\ndirectly used for model-based planning, which is in contrast to learning a better exploration policy\nfor a single skill.\nThe skills discovered using our approach can also provide extended actions and temporal abstrac-\ntion, which enable more efﬁcient exploration for the agent to solve various tasks, reminiscent of\nhierarchical RL (HRL) approaches. This ranges from the classic option-critic architecture (Sutton\net al., 1999; Stolle & Precup, 2002; Perkins et al., 1999) to some of the more recent work (Bacon\net al., 2017; Vezhnevets et al., 2017; Nachum et al., 2018; Hausman et al., 2018). However, in\ncontrast to end-to-end HRL approaches (Heess et al., 2016; Peng et al., 2017), we can leverage a\nstable, two-phase learning setup. The primitives learned through our method provide action and\ntemporal abstraction, while planning with skill-dynamics enables hierarchical composition of these\nprimitives, bypassing many problems of end-to-end HRL.\nIn the second phase of our approach, we use the learned skill-transition dynamics models to perform\nmodel-based planning - an idea that has been explored numerous times in the literature. Model-based\nreinforcement learning has been traditionally approached with methods that are well-suited for low-\ndata regimes such as Gaussian Processes (Rasmussen, 2003) showing signiﬁcant data-efﬁciency\ngains over model-free approaches (Deisenroth et al., 2013; Kamthe & Deisenroth, 2017; Kocijan\net al., 2004; Ko et al., 2007). More recently, due to the challenges of applying these methods to high-\ndimensional state spaces, MBRL approaches employs Bayesian deep neural networks (Nagabandi\net al., 2018; Chua et al., 2018b; Gal et al., 2016; Fu et al., 2016; Lenz et al., 2015) to learn dynamics\nmodels. In our approach, we take advantage of the deep dynamics models that are conditioned\non the skill being executed, simplifying the modelling problem. In addition, the skills themselves\nare being learned with the objective of being predictable, further assists with the learning of the\ndynamics model. There also have been multiple approaches addressing the planning component\nof MBRL including linear controllers for local models (Levine et al., 2016; Kumar et al., 2016;\nChebotar et al., 2017), uncertainty-aware (Chua et al., 2018b; Gal et al., 2016) or deterministic\nplanners (Nagabandi et al., 2018) and stochastic optimization methods (Williams et al., 2016). The\nmain contribution of our work lies in discovering model-based skill primitives that can be further\ncombined by a standard model-based planner, therefore we take advantage of an existing planning\napproach - Model Predictive Path Integral (Williams et al., 2016) that can leverage our pre-trained\nsetting.\n6\nEXPERIMENTS\nThrough our experiments, we aim to demonstrate that: (a) DADS as a general purpose skill dis-\ncovery algorithm can scale to high-dimensional problems; (b) discovered skills are amenable to\nhierarchical composition and; (c) not only is planning in the learned latent space feasible, but it is\ncompetitive to strong baselines. In Section 6.1, we provide visualizations and qualitative analysis of\nthe skills learned using DADS. We demonstrate in Section 6.2 and Section 6.4 that optimizing the\nprimitives for predictability renders skills more amenable to temporal composition that can be used\nfor Hierarchical RL.We benchmark against state-of-the-art model-based RL baseline in Section 6.3,\nand against goal-conditioned RL in Section 6.5.\n6.1\nQUALITATIVE ANALYSIS\nFigure 4: Skills learned on different MuJoCo environments in the OpenAI gym. DADS can discover diverse\nskills without any extrinsic rewards, even for problems with high-dimensional state and action spaces.\n6\nPublished as a conference paper at ICLR 2020\nIn this section, we provide a qualitative discussion of the unsupervised skills learned using DADS.\nWe use the MuJoCo environments (Todorov et al., 2012) from the OpenAI gym as our test-bed\n(Brockman et al., 2016). We ﬁnd that our proposed algorithm can learn diverse skills without any\nreward, even in problems with high-dimensional state and actuation, as illustrated in Figure 4. DADS\ncan discover primitives for Half-Cheetah to run forward and backward with multiple different gaits,\nfor Ant to navigate the environment using diverse locomotion primitives and for Humanoid to walk\nusing stable locomotion primitives with diverse gaits and direction. The videos of the discovered\nprimitives are available at: https://sites.google.com/view/dads-skill\nQualitatively, we ﬁnd the skills discovered by DADS to be predictable and stable, in line with im-\nplicit constraints of the proposed objective. While the Half-Cheetah will learn to run in both back-\nward and forward directions, DADS will disincentivize skills which make Half-Cheetah ﬂip owing to\nthe reduced predictability on landing. Similarly, skills discovered for Ant rarely ﬂip over, and tend\nto provide stable navigation primitives in the environment. This also incentivizes the Humanoid,\nwhich is characteristically prone to collapsing and extremely unstable by design, to discover gaits\nwhich are stable for sustainable locomotion.\nOne of the signiﬁcant advantages of the proposed objective is that it is compatible with continuous\nskill spaces, which has not been shown in prior work on skill discovery (Eysenbach et al., 2018). Not\nonly does this allow us to embed a large and diverse set of skills into a compact latent space, but also\nthe smoothness of the learned space allows us to interpolate between behaviors generated in the envi-\nronment. We demonstrate this on the Ant environment (Figure 5), where we learn two-dimensional\ncontinuous skill space with a uniform prior over (−1, 1) in each dimension, and compare it to a dis-\ncrete skill space with a uniform prior over 20 skills. Similar to Eysenbach et al. (2018), we restrict\nthe observation space of the skill-dynamics q to the cartesian coordinates (x, y). We hereby call this\nthe x-y prior, and discuss its role in Section 6.2.\nTrajectories in Discrete Skill Space\nTrajectories in Continuous Skill Space\nOrientation of Ant Trajectory\nFigure 5: (Left, Centre) X-Y traces of Ant skills and (Right) Heatmap to visualize the learned continuous skill\nspace. Traces demonstrate that the continuous space offers far greater diversity of skills, while the heatmap\ndemonstrates that the learned space is smooth, as the orientation of the X-Y trace varies smoothly as a function\nof the skill.\nIn Figure 5, we project the trajectories of the learned Ant skills from both discrete and continuous\nspaces onto the Cartesian plane. From the traces of the skills, it is clear that the continuous latent\nspace can generate more diverse trajectories. We demonstrate in Section 6.3, that continuous prim-\nitives are more amenable to hierarchical composition and generally perform better on downstream\ntasks. More importantly, we observe that the learned skill space is semantically meaningful. The\nheatmap in Figure 5 shows the orientation of the trajectory (with respect to the x-axis) as a func-\ntion of the skill z ∈Z, which varies smoothly as z is varied, with explicit interpolations shown in\nAppendix D.\n6.2\nSKILL VARIANCE ANALYSIS\nIn an unsupervised skill learning setup, it is important to optimize the primitives to be diverse. How-\never, we argue that diversity is not sufﬁcient for the learned primitives to be useful for downstream\ntasks. Primitives must exhibit low-variance behavior, which enables long-horizon composition of\nthe learned skills in a hierarchical setup. We analyze the variance of the x-y trajectories in the en-\nvironment, where we also benchmark the variance of the primitives learned by DIAYN (Eysenbach\net al., 2018). For DIAYN, we use the x-y prior for the skill-discriminator, which biases the dis-\ncovered skills to diversify in the x-y space. This step was necessary for that baseline to obtain a\n7\nPublished as a conference paper at ICLR 2020\nStandard Deviation of Trajectories\nDADS without x-y prior\nDIAYN with x-y prior\nDADS with x-y prior\nFigure 6: (Top-Left) Standard deviation of Ant’s position as a function of steps in the environment, averaged\nover multiple skills and normalized by the norm of the position. (Top-Right to Bottom-Left Clockwise) X-Y\ntraces of skills learned using DIAYN with x-y prior, DADS with x-y prior and DADS without x-y prior, where\nthe same color represents trajectories resulting from the execution of the same skill z in the environment. High\nvariance skills from DIAYN offer limited utility for hierarchical control.\ncompetitive set of navigation skills. Figure 6 (Top-Left) demonstrates that DADS, which optimizes\nthe primitives for predictability and diversity, yields signiﬁcantly lower-variance primitives when\ncompared to DIAYN, which only optimizes for diversity. This is starkly demonstrated in the plots\nof X-Y traces of skills learned in different setups. Skills learned by DADS show signiﬁcant control\nover the trajectories generated in the environment, while skills from DIAYN exhibit high variance\nin the environment, which limits their utility for hierarchical control. This is further demonstrated\nquantitatively in Section 6.4.\nWhile optimizing for predictability already signiﬁcantly reduces the variance of the trajectories gen-\nerated by a primitive, we ﬁnd that using the x-y prior with DADS brings down the skill variance\neven further. For quantitative benchmarks in the next sections, we assume that the Ant skills are\nlearned using an x-y prior on the observation space, for both DADS and DIAYN.\n6.3\nMODEL-BASED REINFORCEMENT LEARNING\nThe key utility of learning a parametric model qφ(s′|s, z) is to take advantage of planning algorithms\nfor downstream tasks, which can be extremely sample-efﬁcient. In our setup, we can solve test-\ntime tasks in zero-shot, that is without any learning on the downstream task. We compare with\nthe state-of-the-art model-based RL method (Chua et al., 2018a), which learns a dynamics model\nparameterized as p(s′|s, a), on the task of the Ant navigating to a speciﬁed goal with a dense reward.\nGiven a goal g, reward at any position u is given by r(u) = −∥g −u∥2. We benchmark our method\nagainst the following variants:\n• Random-MBRL (rMBRL): We train the model p(s′|s, a) on randomly collected trajecto-\nries, and test the zero-shot generalization of the model on a distribution of goals.\n8\nPublished as a conference paper at ICLR 2020\n• Weak-oracle MBRL (WO-MBRL): We train the model p(s′|s, a) on trajectories generated\nby the planner to navigate to a goal, randomly sampled in every episode. The distribution\nof goals during training matches the distribution at test time.\n• Strong-oracle MBRL (SO-MBRL): We train the model p(s′|s, a) on a trajectories generated\nby the planner to navigate to a speciﬁc goal, which is ﬁxed for both training and test time.\nAmongst the variants, only the rMBRL matches our assumptions of having an unsupervised task-\nagnostic training. Both WO-MBRL and SO-MBRL beneﬁt from goal-directed exploration dur-\ning training, a signiﬁcant advantage over DADS, which only uses mutual-information-based explo-\nration.\nWe use ∆= PH\nt=1\n−r(u)\nH∥g∥2 as the metric, which represents the distance to the goal g averaged over the\nepisode (with the same ﬁxed horizon H for all models and experiments), normalized by the initial\ndistance to the goal g. Therefore, lower ∆indicates better performance and 0 < ∆≤1 (assuming\nthe agent goes closer to the goal). The test set of goals is ﬁxed for all the methods, sampled from\n[−15, 15]2.\nFigure 7 demonstrates that the zero-shot planning signiﬁcantly outperforms all model-based RL\nbaselines, despite the advantage of the baselines being trained on the test goal(s). For the experi-\nment depicted in Figure 7 (Right), DADS has an unsupervised pre-training phase, unlike SO-MBRL\nwhich is training directly for the task. A comparison with Random-MBRL shows the signiﬁcance of\nmutual-information-based exploration, especially with the right parameterization and priors. This\nexperiment also demonstrates the advantage of learning a continuous space of primitives, which\noutperforms planning on discrete primitives.\nFigure 7: (Left) The results of the MPPI controller on skills learned using DADS-c (continuous primitives)\nand DADS-d (discrete primitives) signiﬁcantly outperforms state-of-the-art model-based RL. (Right) Planning\nfor a new task does not require any additional training and outperforms model-based RL being trained for the\nspeciﬁc task.\n6.4\nHIERARCHICAL CONTROL WITH UNSUPERVISED PRIMITIVES\nWe benchmark hierarchical control for primitives learned without supervision, against our proposed\nscheme using an MPPI based planner on top of DADS-learned skills. We persist with the task of\nAnt-navigation as described in 6.3. We benchmark against Hierarchical DIAYN (Eysenbach et al.,\n2018), which learns the skills using the DIAYN objective, freezes the low-level policy and learns\na meta-controller that outputs the skill to be executed for the next HZ steps. We provide the x-y\nprior to the DIAYN’s disciminator while learning the skills for the Ant agent. The performance\nof the meta-controller is constrained by the low-level policy, however, this hierarchical scheme is\nagnostic to the algorithm used to learn the low-level policy. To contrast the quality of primitives\nlearned by the DADS and DIAYN, we also benchmark against Hierarchical DADS, which learns a\nmeta-controller the same way as Hierarchical DIAYN, but learns the skills using DADS.\nFrom Figure 8 (Left) We ﬁnd that the meta-controller is unable to compose the skills learned by\nDIAYN, while the same meta-controller can learn to compose skills by DADS to navigate the Ant\nto different goals. This result seems to conﬁrm our intuition described in Section 6.2, that the high\nvariance of the DIAYN skills limits their temporal compositionality. Interestingly, learning a RL\n9\nPublished as a conference paper at ICLR 2020\nFigure 8: (Left) A RL-trained meta-controller is unable to compose primitive learned by DIAYN to navigate\nAnt to a goal, while it succeeds to do so using the primitives learned by DADS. (Right) Goal-Conditioned RL\n(GCRL-dense/sparse) does not generalize outside its training distribution, while MPPI controller on learned\nskills (DADS-dense/sparse) experiences signiﬁcantly smaller degrade in performance.\nmeta-controller reaches similar performance to the MPPI controller, taking an additional 200, 000\nsamples per goal.\n6.5\nGOAL-CONDITIONED RL\nTo demonstrate the beneﬁts of our approach over model-free RL, we benchmark against goal-\nconditioned RL on two versions of Ant-navigation: (a) with a dense reward r(u) and (b) with a\nsparse reward r(u) = 1 if ∥u −g∥2 ≤ϵ, else 0. We train the goal-conditioned RL agent using soft\nactor-critic, where the state variable of the agent is augmented with u −g, the position delta to the\ngoal. The agent gets a randomly sampled goal from [−10, 10]2 at the beginning of the episode.\nIn Figure 8 (Right), we measure the average performance of the all the methods as a function of\nthe initial distance of the goal, ranging from 5 to 30 metres. For dense reward navigation, we ob-\nserve that while model-based planning on DADS-learned skills degrades smoothly as the initial\ndistance to goal to increases, goal-conditioned RL experiences a sudden deterioration outside the\ngoal distribution it was trained on. Even within the goal distribution observed during training of\ngoal-conditioned RL model, skill-space planning performs competitively to it. With sparse reward\nnavigation, goal-conditioned RL is unable to navigate, while MPPI demonstrates comparable perfor-\nmance to the dense reward up to about 20 metres. This highlights the utility of learning task-agnostic\nskills, which makes them more general while showing that latent space planning can be leveraged\nfor tasks requiring long-horizon planning.\n7\nCONCLUSION\nWe have proposed a novel unsupervised skill learning algorithm that is amenable to model-based\nplanning for hierarchical control on downstream tasks. We show that our skill learning method can\nscale to high-dimensional state-spaces, while discovering a diverse set of low-variance skills. In ad-\ndition, we demonstrated that, without any training on the speciﬁed task, we can compose the learned\nskills to outperform competitive model-based baselines that were trained with the knowledge of the\ntest tasks. We plan to extend the algorithm to work with off-policy data, potentially using relabelling\ntricks (Andrychowicz et al., 2017; Nachum et al., 2018) and explore more nuanced planning algo-\nrithms. We plan to apply the hereby-introduced method to different domains, such as manipulation\nand enable skill/model discovery directly from images.\n8\nACKNOWLEDGEMENTS\nWe would like to thank Evan Liu, Ben Eysenbach, Anusha Nagabandi for their help in reproducing\nthe baselines for this work. We are thankful to Ben Eysenbach for their comments and discussion\non the initial drafts. We would also like to acknowledge Oﬁr Nachum, Alex Alemi, Daniel Free-\nman, Yiding Jiang, Allan Zhou and other colleagues at Google Brain for their helpful feedback and\ndiscussions at various stages of this work. We are also thankful to Michael Ahn and others in Adept\nteam for their support, especially with the infrastructure setup and scaling up the experiments.\n10\nPublished as a conference paper at ICLR 2020\nREFERENCES\nMart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.\nCorrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew\nHarp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath\nKudlur, Josh Levenberg, Dan Man´e, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah,\nMike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vin-\ncent Vanhoucke, Vijay Vasudevan, Fernanda Vi´egas, Oriol Vinyals, Pete Warden, Martin Watten-\nberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning\non heterogeneous systems, 2015. URL http://tensorflow.org/. Software available from\ntensorﬂow.org.\nJoshua Achiam, Harrison Edwards, Dario Amodei, and Pieter Abbeel. Variational option discovery\nalgorithms. arXiv preprint arXiv:1807.10299, 2018.\nDavid Barber Felix Agakov. The im algorithm: a variational approach to information maximization.\nAdvances in Neural Information Processing Systems, 16:201, 2004.\nAlexander A Alemi and Ian Fischer. Therml: Thermodynamics of machine learning. arXiv preprint\narXiv:1807.04162, 2018.\nMarcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob\nMcGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. CoRR,\nabs/1707.01495, 2017. URL http://arxiv.org/abs/1707.01495.\nPierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Thirty-First AAAI\nConference on Artiﬁcial Intelligence, 2017.\nMarc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.\nUnifying count-based exploration and intrinsic motivation. In Advances in Neural Information\nProcessing Systems, pp. 1471–1479, 2016.\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\nWojciech Zaremba. Openai gym. CoRR, abs/1606.01540, 2016. URL http://arxiv.org/\nabs/1606.01540.\nYevgen Chebotar, Karol Hausman, Marvin Zhang, Gaurav Sukhatme, Stefan Schaal, and Sergey\nLevine.\nCombining model-based and model-free updates for trajectory-centric reinforcement\nlearning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,\npp. 703–711. JMLR. org, 2017.\nKurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learn-\ning in a handful of trials using probabilistic dynamics models. CoRR, abs/1805.12114, 2018a.\nURL http://arxiv.org/abs/1805.12114.\nKurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learn-\ning in a handful of trials using probabilistic dynamics models. In Advances in Neural Information\nProcessing Systems, pp. 4759–4770, 2018b.\nImre Csisz´ar and Frantisek Matus. Information projections revisited. IEEE Transactions on Infor-\nmation Theory, 49(6):1474–1490, 2003.\nChristian Daniel, Gerhard Neumann, and Jan Peters. Hierarchical relative entropy policy search. In\nArtiﬁcial Intelligence and Statistics, pp. 273–281, 2012.\nMarc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efﬁcient approach to policy\nsearch. In Proceedings of the 28th International Conference on machine learning (ICML-11), pp.\n465–472, 2011.\nMarc Peter Deisenroth, Dieter Fox, and Carl Edward Rasmussen. Gaussian processes for data-\nefﬁcient learning in robotics and control. IEEE transactions on pattern analysis and machine\nintelligence, 37(2):408–423, 2013.\n11\nPublished as a conference paper at ICLR 2020\nBenjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need:\nLearning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018.\nCarlos Florensa, Yan Duan, and Pieter Abbeel. Stochastic neural networks for hierarchical rein-\nforcement learning. arXiv preprint arXiv:1704.03012, 2017.\nNir Friedman, Ori Mosenzon, Noam Slonim, and Naftali Tishby. Multivariate information bottle-\nneck. In Proceedings of the Seventeenth conference on Uncertainty in artiﬁcial intelligence, pp.\n152–161. Morgan Kaufmann Publishers Inc., 2001.\nJustin Fu, Sergey Levine, and Pieter Abbeel. One-shot learning of manipulation skills with online\ndynamics adaptation and neural network priors. In 2016 IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS), pp. 4019–4026. IEEE, 2016.\nJustin Fu, John Co-Reyes, and Sergey Levine.\nEx2: Exploration with exemplar models for\ndeep reinforcement learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30,\npp. 2577–2587. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/\n6851-ex2-exploration-with-exemplar-models-for-deep-reinforcement-learning.\npdf.\nYarin Gal, Rowan McAllister, and Carl Edward Rasmussen. Improving pilco with bayesian neural\nnetwork dynamics models.\nIn Data-Efﬁcient Machine Learning workshop, ICML, volume 4,\n2016.\nCarlos E Garcia, David M Prett, and Manfred Morari. Model predictive control: theory and practicea\nsurvey. Automatica, 25(3):335–348, 1989.\nKarol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. arXiv\npreprint arXiv:1611.07507, 2016.\nShixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning for\nrobotic manipulation with asynchronous off-policy updates. In 2017 IEEE International Confer-\nence on Robotics and Automation (ICRA), pp. 3389–3396. IEEE, 2017.\nDavid Ha and J¨urgen Schmidhuber. Recurrent world models facilitate policy evolution. In Advances\nin Neural Information Processing Systems, pp. 2455–2467, 2018.\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.\nSoft actor-critic:\nOff-\npolicy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint\narXiv:1801.01290, 2018a.\nTuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash\nKumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft actor-critic algo-\nrithms and applications. CoRR, abs/1812.05905, 2018b. URL http://arxiv.org/abs/\n1812.05905.\nKarol Hausman, Jost Tobias Springenberg, Ziyu Wang, Nicolas Heess, and Martin Riedmiller.\nLearning an embedding space for transferable robot skills. In International Conference on Learn-\ning Representations, 2018. URL https://openreview.net/forum?id=rk07ZXZRb.\nNicolas Heess, Greg Wayne, Yuval Tassa, Timothy Lillicrap, Martin Riedmiller, and David Silver.\nLearning and transfer of modulated locomotor controllers.\narXiv preprint arXiv:1610.05182,\n2016.\nNicolas Heess, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa, Tom Erez,\nZiyu Wang, SM Eslami, Martin Riedmiller, et al. Emergence of locomotion behaviours in rich\nenvironments. arXiv preprint arXiv:1707.02286, 2017.\nRein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel.\nCuriosity-driven exploration in deep reinforcement learning via bayesian neural networks. CoRR,\nabs/1605.09674, 2016. URL http://arxiv.org/abs/1605.09674.\n12\nPublished as a conference paper at ICLR 2020\nRobert A Jacobs, Michael I Jordan, Steven J Nowlan, Geoffrey E Hinton, et al. Adaptive mixtures\nof local experts. Neural computation, 3(1):79–87, 1991.\nDmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre\nQuillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Qt-opt: Scalable deep\nreinforcement learning for vision-based robotic manipulation. arXiv preprint arXiv:1806.10293,\n2018.\nSanket Kamthe and Marc Peter Deisenroth. Data-efﬁcient reinforcement learning with probabilistic\nmodel predictive control. arXiv preprint arXiv:1706.06491, 2017.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\nAlexander S Klyubin, Daniel Polani, and Chrystopher L Nehaniv. Empowerment: A universal agent-\ncentric measure of control. In 2005 IEEE Congress on Evolutionary Computation, volume 1, pp.\n128–135. IEEE, 2005.\nJonathan Ko, Daniel J Klein, Dieter Fox, and Dirk Haehnel. Gaussian processes and reinforce-\nment learning for identiﬁcation and control of an autonomous blimp. In Proceedings 2007 ieee\ninternational conference on robotics and automation, pp. 742–747. IEEE, 2007.\nJuˇs Kocijan, Roderick Murray-Smith, Carl Edward Rasmussen, and Agathe Girard. Gaussian pro-\ncess model based predictive control. In Proceedings of the 2004 American Control Conference,\nvolume 3, pp. 2214–2219. IEEE, 2004.\nVikash Kumar, Emanuel Todorov, and Sergey Levine. Optimal control with learned local models:\nApplication to dexterous manipulation. In 2016 IEEE International Conference on Robotics and\nAutomation (ICRA), pp. 378–383. IEEE, 2016.\nThanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-ensemble\ntrust-region policy optimization. arXiv preprint arXiv:1802.10592, 2018.\nIan Lenz, Ross A Knepper, and Ashutosh Saxena. Deepmpc: Learning deep latent features for\nmodel predictive control. In Robotics: Science and Systems. Rome, Italy, 2015.\nSergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuo-\nmotor policies. The Journal of Machine Learning Research, 17(1):1334–1373, 2016.\nWeiwei Li and Emanuel Todorov. Iterative linear quadratic regulator design for nonlinear biological\nmovement systems. In ICINCO (1), pp. 222–229, 2004.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-\nstra, and Martin Riedmiller.\nPlaying atari with deep reinforcement learning.\narXiv preprint\narXiv:1312.5602, 2013.\nShakir Mohamed and Danilo Jimenez Rezende. Variational information maximisation for intrinsi-\ncally motivated reinforcement learning. In Advances in neural information processing systems,\npp. 2125–2133, 2015.\nOﬁr Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey Levine. Data-efﬁcient hierarchical\nreinforcement learning. In Advances in Neural Information Processing Systems, pp. 3307–3317,\n2018.\nAnusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network dynamics\nfor model-based deep reinforcement learning with model-free ﬁne-tuning. In 2018 IEEE Interna-\ntional Conference on Robotics and Automation (ICRA), pp. 7559–7566. IEEE, 2018.\nJunhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and Satinder Singh. Action-conditional\nvideo prediction using deep networks in atari games. In Advances in neural information process-\ning systems, pp. 2863–2871, 2015.\nPierre-Yves Oudeyer and Frederic Kaplan. What is intrinsic motivation? a typology of computa-\ntional approaches. Frontiers in neurorobotics, 1:6, 2009.\n13\nPublished as a conference paper at ICLR 2020\nPierre-Yves Oudeyer, Frdric Kaplan, and Verena V Hafner. Intrinsic motivation systems for au-\ntonomous mental development. IEEE transactions on evolutionary computation, 11(2):265–286,\n2007.\nDeepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration\nby self-supervised prediction. In ICML, 2017.\nXue Bin Peng, Glen Berseth, KangKang Yin, and Michiel Van De Panne. Deeploco: Dynamic\nlocomotion skills using hierarchical deep reinforcement learning. ACM Transactions on Graphics\n(TOG), 36(4):41, 2017.\nTheodore J Perkins, Doina Precup, et al. Using options for knowledge transfer in reinforcement\nlearning. University of Massachusetts, Amherst, MA, USA, Tech. Rep, 1999.\nAravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel\nTodorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement\nlearning and demonstrations. arXiv preprint arXiv:1709.10087, 2017.\nCarl Edward Rasmussen. Gaussian processes in machine learning. In Summer School on Machine\nLearning, pp. 63–71. Springer, 2003.\nJ¨urgen Schmidhuber. Formal theory of creativity, fun, and intrinsic motivation (1990–2010). IEEE\nTransactions on Autonomous Mental Development, 2(3):230–247, 2010.\nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael I Jordan, and Philipp Moritz. Trust region\npolicy optimization. In Icml, volume 37, pp. 1889–1897, 2015.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nSergio Guadarrama, Anoop Korattikara, Oscar Ramirez, Pablo Castro, Ethan Holly, Sam Fishman,\nKe Wang, Ekaterina Gonina, Chris Harris, Vincent Vanhoucke, Eugene Brevdo.\nTF-Agents:\nA library for reinforcement learning in tensorﬂow. https://github.com/tensorflow/\nagents, 2018. URL https://github.com/tensorflow/agents. [Online; accessed\n30-November-2018].\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,\nJulian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering\nthe game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.\nNoam Slonim, Gurinder S Atwal, Gasper Tkacik, and William Bialek. Estimating mutual informa-\ntion and multi–information in large networks. arXiv preprint cs/0502017, 2005.\nBradly C. Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement\nlearning with deep predictive models. CoRR, abs/1507.00814, 2015. URL http://arxiv.\norg/abs/1507.00814.\nMartin Stolle and Doina Precup.\nLearning options in reinforcement learning.\nIn International\nSymposium on abstraction, reformulation, and approximation, pp. 212–223. Springer, 2002.\nRichard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A frame-\nwork for temporal abstraction in reinforcement learning. Artiﬁcial intelligence, 112(1-2):181–\n211, 1999.\nHaoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John Schul-\nman, Filip DeTurck, and Pieter Abbeel. # exploration: A study of count-based exploration for\ndeep reinforcement learning. In Advances in neural information processing systems, pp. 2753–\n2762, 2017.\nValentin Thomas, Emmanuel Bengio, William Fedus, Jules Pondard, Philippe Beaudoin, Hugo\nLarochelle, Joelle Pineau, Doina Precup, and Yoshua Bengio. Disentangling the independently\ncontrollable factors of variation by interacting with the world, 2018.\n14\nPublished as a conference paper at ICLR 2020\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.\nIn 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026–5033.\nIEEE, 2012.\nAlexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David\nSilver, and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning. In\nProceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 3540–\n3549. JMLR. org, 2017.\nDavid Warde-Farley, Tom Van de Wiele, Tejas Kulkarni, Catalin Ionescu, Steven Hansen, and\nVolodymyr Mnih. Unsupervised control through non-parametric discriminative rewards. arXiv\npreprint arXiv:1811.11359, 2018.\nManuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control:\nA locally linear latent dynamics model for control from raw images.\nIn Advances in neural\ninformation processing systems, pp. 2746–2754, 2015.\nGrady Williams, Paul Drews, Brian Goldfain, James M Rehg, and Evangelos A Theodorou. Aggres-\nsive driving with model predictive path integral control. In 2016 IEEE International Conference\non Robotics and Automation (ICRA), pp. 1433–1440. IEEE, 2016.\nA\nIMPLEMENTATION DETAILS\nAll of our models are written in the open source Tensorﬂow-Agents (Sergio Guadarrama, Anoop\nKorattikara, Oscar Ramirez, Pablo Castro, Ethan Holly, Sam Fishman, Ke Wang, Ekaterina Gonina,\nChris Harris, Vincent Vanhoucke, Eugene Brevdo, 2018), based on Tensorﬂow (Abadi et al., 2015).\nA.1\nSKILL SPACES\nWhen using discrete spaces, we parameterize Z as one-hot vectors. These one-hot vectors are ran-\ndomly sampled from the uniform prior p(z) = 1\nD, where D is the number of skills. We experiment\nwith D ≤128. For discrete skills learnt for MuJoCo Ant in Section 6.3, we use D = 20. For\ncontinuous spaces, we sample z ∼Uniform(−1, 1)D. We experiment with D = 2 for Ant learnt\nwith x-y prior, D = 3 for Ant learnt without x-y prior (that is full observation space), to D = 5 for\nHumanoid on full observation spaces. The skills are sampled once in the beginning of the episode\nand ﬁxed for the rest of the episode. However, it is possible to resample the skill from the prior\nwithin the episode, which allows for every skill to experience a different distribution than the ini-\ntialization distribution. This also encourages discovery of skills which can be composed temporally.\nHowever, this increases the hardness of problem, especially if the skills are re-sampled from the\nprior frequently.\nA.2\nAGENT\nWe use SAC as the optimizer for our agent π(a | s, z), in particular, EC-SAC (Haarnoja et al.,\n2018b). The s input to the policy generally excludes global co-ordinates (x, y) of the centre-of-\nmass, available for a lot of enviroments in OpenAI gym, which helps produce skills agnostic to the\nlocation of the agent. We restrict to two hidden layers for our policy and critic networks. However,\nto improve the expressivity of skills, it is beneﬁcial to increase the capacity of the networks. The\nhidden layer sizes can vary from (128, 128) for Half-Cheetah to (512, 512) for Ant and (1024, 1024)\nfor Humanoid. The critic Q(s, a, z) is similarly parameterized. The target function for critic Q is\nupdated every iteration using a soft updates with co-efﬁcient of 0.005. We use Adam (Kingma &\nBa, 2014) optimizer with a ﬁxed learning rate of 3e −4 , and a ﬁxed initial entropy co-efﬁcient\nβ = 0.1. While the policy is parameterized as a normal distribution N(µ(s, z), Σ(s, z)) where Σ is\na diagonal covariance matrix, it undergoes through tanh transformation, to transform the output to\nthe range (−1, 1) and constrain to the action bounds.\n15\nPublished as a conference paper at ICLR 2020\nA.3\nSKILL-DYNAMICS\nSkill-dynamics, denoted by q(s′ | s, z), is parameterized by a deep neural network. A common trick\nin model-based RL is to predict the ∆s = s′ −s, rather than the full state s′. Hence, the prediction\nnetwork is q(∆s | s, z). Note, both parameterizations can represent the same set of functions.\nHowever, the latter will be easy to learn as ∆s will be centred around 0. We exclude the global co-\nordinates from from the state input to q. However, we can (and we still do) predict ∆x, ∆y, because\nreward functions for goal-based navigation generally rely on the position prediction from the model.\nThis represents another beneﬁt of predicting state-deltas, as we can still predict changes in position\nwithout explicitly knowing the global position.\nThe output distribution is modelled as a Mixture-of-Experts (Jacobs et al., 1991). We ﬁx the number\nof experts to be 4. We model each expert as a Gaussian distribution. The input (s, z) goes through\ntwo hidden layers (the same capacity as that of policy and critic networks, for example (512, 512)\nfor Ant). The output of the two hidden layers is used as an input to the mixture-of-experts, which\nis linearly transformed to output the parameters of the Gaussian distribution, and a discrete distribu-\ntion over the experts using a softmax distribution. In practice, we ﬁx the covariance matrix of the\nGaussian experts to be an identity matrix, so we only need to output the means for the experts. We\nuse batch-normalization for both input and the hidden layers. We normalize the output targets using\ntheir batch-average and batch-standard deviation, similar to batch-normalization.\nA.4\nOTHER HYPERPARAMETERS\nThe episode horizon is generally kept shorter for stable agents like Ant (200), while longer for\nunstable agents like Humanoid (1000). For Ant, longer episodes do not add value, but Humanoid\ncan beneﬁt from longer episodes as it helps it ﬁlter skills which are unstable. The optimization\nscheme is on-policy, and we collect 2000 steps for Ant and 4000 steps for Humanoid in one iteration.\nThe intuition is to experience trajectories generated by multiple skills (approximately 10) in a batch.\nRe-sampling skills can enable experiencing larger number of skills. Once a batch of episodes is\ncollected, the skill-dynamics is updated using Adam optimizer with a ﬁxed learning rate of 3e −4.\nThe batch size is 128, and we carry out 32 steps of gradient descent. To compute the intrinsic\nreward, we need to resample the prior for computing the denominator. For continuous spaces, we\nset L = 500. For discrete spaces, we can marginalize over all skills. After the intrinsic reward is\ncomputed, the policy and critic networks are updated for 128 steps with a batch size of 128. The\nintuition is to ensure that every sample in the batch is seen for policy and critic updates about 3 −4\ntimes in expectation.\nA.5\nPLANNING AND EVALUATION SETUPS\nFor evaluation, we ﬁx the episode horizon to 200 for all models in all evaluation setups. Depending\nupon the size of the latent space and planning horizon, the number of samples from the planning\ndistribution P is varied between 10 −200. For HP = 1, HZ = 10 and a 2D latent space, we use\n50 samples from the planning distribution P. The co-efﬁcient γ for MPPI is ﬁxed to 10. We use\na setting of HP = 1 and HZ = 10 for dense-reward navigation, in which case we set the number\nof reﬁne steps R = 10. However, for sparse reward navigation it is important to have a longer\nhorizon planning, in which case we set HP = 4, HZ = 25 with a higher number of samples from\nthe planning distribution (200 from P). Also, when using longer planning horizons, we found that\nsmoothing the sampled plans help. Thus, if the sampled plan is z1, z2, z3, z4 . . ., we smooth the plan\nto make z2 = βz1 + (1 −β)z2 and so on, with β = 0.9.\nFor hierarchical controllers being learnt on top of low-level unsupervised primitives, we use PPO\n(Schulman et al., 2017) for discrete action skills, while we use SAC for continuous skills. We keep\nthe number of steps after which the meta-action is decided as 10 (that is HZ = 10). The hidden\nlayer sizes of the meta-controller are (128, 128). We use a learning rate of 1e−4 for PPO and 3e−4\nfor SAC.\nFor our model-based RL baseline PETS, we use an ensemble size of 3, with a ﬁxed planning hori-\nzon of 20. For the model, we use a neural network with two hidden layers of size 400. In our\nexperiments, we found that MPPI outperforms CEM, so we report the results using the MPPI as our\ncontroller.\n16\nPublished as a conference paper at ICLR 2020\nB\nGRAPHICAL MODELS, INFORMATION BOTTLENECK AND UNSUPERVISED\nSKILL LEARNING\nWe now present a novel perspective on unsupervised skill learning, motivated from the literature on\ninformation bottleneck. This section takes inspiration from (Alemi & Fischer, 2018), which helps\nus provide a rigorous justiﬁcation for our objective proposed earlier. To obtain our unsupervised RL\nobjective, we setup a graphical model P as shown in Figure 9, which represents the distribution of\ntrajectories generated by a given policy π. The joint distribution is given by:\np(s1, a1 . . . aT −1, sT , z) = p(z)p(s1)\nT −1\nY\nt=1\nπ(at|st, z)p(st+1|st, at).\n(8)\nz\na1\ns1\na2\ns2\naT\nsT\n...\n...\nFigure 9: Graphical model for the world P in which\nthe trajectories are generated while interacting with\nthe environment. Shaded nodes represent the distri-\nbutions we optimize.\nz\na1\na2\naT\ns1\ns2\nsT\n...\n...\nFigure 10: Graphical model for the world N which is\nthe desired representation of the world.\nWe setup another graphical model N, which represents the desired model of the world. In particular,\nwe are interested in approximating p(s′|s, z), which represents the transition function for a particular\nprimitive. This abstraction helps us get away from knowing the exact actions, enabling model-based\nplanning in behavior space (as discussed in the main paper). The joint distribution for N shown in\nFigure 10 is given by:\nη(s1, a1, . . . sT , aT , z) = η(z)η(s1)\nT −1\nY\nt=1\nη(at)η(st+1|st, z).\n(9)\nThe goal of our approach is to optimize the distribution π(a|s, z) in the graphical model P to min-\nimize the distance between the two distributions, when transforming to the representation of the\ngraphical model Z. In particular, we are interested in minimizing the KL divergence between p and\nη, that is DKL(p||η). Note, if N had the same structure as P, the information lost in projection\nwould be 0 for any valid P. Interestingly, we can exploit the following result from in Friedman et al.\n(2001) to setup the objective for π, without explicitly knowing η:\nmin\nη\nDKL(p||η) = IP −IN,\n(10)\nwhere IP and IN represents the multi-information for distribution P on the respective graphical\nmodels. Note, minη∈N DKL(p||η), which is the reverse information projection (Csisz´ar & Matus,\n2003). The multi-information (Slonim et al., 2005) for a graphical model G with nodes gi is deﬁned\nas:\nIG =\nX\ni\nI(gi; Pa(gi)),\n(11)\nwhere Pa(gi) denotes the nodes upon which gi has direct conditional dependence in G. Using this\ndeﬁnition, we can compute the multi-information terms:\nIP =\nT\nX\nt=1\nI(at; {st, z}) +\nT\nX\nt=2\nI(st; {st−1, at−1})\nand\nIN =\nT\nX\nt=2\nI(st; {st−1, z}).\n(12)\n17\nPublished as a conference paper at ICLR 2020\nFollowing the Optimal Frontier argument in (Alemi & Fischer, 2018), we introduce Lagrange mul-\ntipliers βt ≥0, δt ≥0 for the information terms in IP to setup an objective R(π) to be maximized\nwith respect to π:\nR(π) =\nT −1\nX\nt=1\nI(st+1; {st, z}) −βtI(at; {st, z}) −δtI(st+1; {st, at})\n(13)\n(14)\nAs the underlying dynamics are ﬁxed and unknown, we simplify the optimization by setting δt =\n0 which intuitively corresponds to us neglecting the unchangeable information of the underlying\ndynamics. This gives us\nR(π) =\nT −1\nX\nt=1\nI(st+1; {st, z}) −βtI(at; {st, z})\n(15)\n≥\nT −1\nX\nt=1\nI(st+1; z | st) −βtI(at; {st, z})\n(16)\nHere, we have used the chain rule of mutual information: I(st+1; {st, z}) = I(st+1; st) +\nI(st+1; z | st) ≥I(st+1; z | st), resulting from the non-negativity of mutual information. This\nyield us an information bottleneck style objective where we maximize the mutual information moti-\nvated in Section 3, while minimizing I(at; {st, z}). We can show that the minimization of the latter\nmutual information corresponds to entropy regularization of π(at | st, z), as follows:\nI(at; {st, z}) = Eat∼π(at|st,z),st,z∼p\nh\nlog π(at | st, z)\nπ(at)\ni\n(17)\n= Eat∼π(at|st,z),st,z∼p\nh\nlog π(at | st, z)\np(at)\ni\n−DKL(π(at) || p(at))\n(18)\n≤Eat∼π(at|st,z),st,z∼p\nh\nlog π(at | st, z)\np(at)\ni\n(19)\nfor some arbitrary distribution log p(at) (for example uniform). Again, we have used the non-\nnegativity of DKL to get the inequality. We use Equation 19 in Equation 16 to get:\nR(π) ≥\nT −1\nX\nt=1\nI(st+1; z | st) −βtEat∼π(at|st,z),st,z∼p\nh\nlog π(at | st, z)\ni\n(20)\nwhere we have ignored p(at) as it is a constant with respect to optimization for π. This motivates\nthe use of entropy regularization. We can follow the arguments in Section 3 to obtain an approxi-\nmate lower bound for I(st+1; z | st). The above discussion shows how DADS can be motivated\nfrom a graphical modelling perspective, while justifying the use of entropy regularization from an\ninformation bottleneck perspective. This objective also explicates the temporally extended nature of\nz, and how it corresponds to a sequence of actions producing a predictable sequence of transitions\nin the environment.\nz\na\ns\nFigure 11: Graphical model for the world P rep-\nresenting the stationary state, action distribution.\nShaded nodes represent the distributions we opti-\nmize.\nz\ns\nFigure 12: Graphical model for the world N using\nwhich we is the representation we are interested in.\n18\nPublished as a conference paper at ICLR 2020\nWe can carry out the exercise for the reward function in Eysenbach et al. (2018) (DIAYN) to provide\na graphical model interpretation of the objective used in the paper. To conform with objective\nin the paper, we assume to be sampling to be state-action pairs from skill-conditioned stationary\ndistributions in the world P, rather than trajectories. The objective to be maximized is given by:\nR(π) = −IP + IQ\n(21)\n= −I(a; {s, z}) + I(z; s)\n(22)\n= Eπ[log p(z|s)\np(z) −log π(a|s, z)\nπ(a)\n]\n(23)\n≥Eπ[log qφ(z|s) −log p(z) −log π(a|s, z)] = R(π, qφ)\n(24)\nwhere we have used the variational inequalities to replace p(z|s) with qφ(z|s) and π(a) with a\nuniform prior over bounded actions p(a) (which is ignored as a constant).\nC\nAPPROXIMATING THE REWARD FUNCTION\nWe revisit Equation 4 and the resulting approximate reward function constructed in Equation 6. The\nmaximization objective for policy was:\nR(π | qφ) = Ez,s,s′\u0002\nlog qφ(s′ | s, z) −log p(s′ | s)\n\u0003\n(25)\nThe computational problem arises from the intractability of p(s′ | s) =\nR\np(s′ | s, z)p(z | s)dz,\nwhere both p(s′ | s, z) and p(z | s) ∝p(s | z)p(z) are intractable. Unfortunately, any variational\napproximation results in an improper lower bound for the objective. To see that:\nR(π | qφ) = Ez,s,s′\u0002\nlog qφ(s′ | s, z) −log q(s′ | s)\n\u0003\n−DKL(p(s′ | s) || q(s′ | s))\n(26)\n≤Ez,s,s′\u0002\nlog qφ(s′ | s, z) −log q(s′ | s)\n\u0003\n(27)\nwhere the inequality goes the wrong way for any variational approximation q(s′ | s). Our ap-\nproximation can be seen as a special instantiation of q(s′ | s) =\nR\nqφ(s′ | s, z)p(z)dz. This\napproximation is simple to compute as generating samples from the prior p(z) is inexpensive and\neffectively requires only a forward pass through qφ. Reusing qφ to approximate p(s′ | s) makes\nintuitive sense because we want qφ to reasonably approximate p(s′ | s, z) (which is why we collect\nlarge batches of data and take multiple steps of gradient descent for ﬁtting qφ). While sampling from\nthe prior p(z) is crude, sampling p(z | s) can be computationally prohibitive. For a certain class\nof problems, especially locomotion, sampling from p(z) is a reasonable approximation as well. We\nwant our primitives/skills to be usable from any state, which is especially the case with locomotion.\nEmpirically, we have found our current approximation provides satisfactory results. We also discuss\nsome other potential solutions (and their limitations):\n(a) One could potentially use another network qβ(z | s) to approximate p(z | s) by minimizing\nEs,z∼p\n\u0002\nDKL(p(z | s) || qβ(z | s))\n\u0003\n. Note, the resulting approximation would still be an improper\nlower bound for R(π | qφ). However, sampling from this qβ might result in a better approximation\nthan sampling from the prior p(z) for some problems.\n(b) We can bypass the computational intractability of p(s′ | s) by exploiting the variational lower\nbounds from Agakov (2004). We use the following inequality, used in Hausman et al. (2018):\nH(x) ≥\nZ\np(x, z) log q(z|x)\np(x, z)dxdz\n(28)\nwhere q is a variational approximation to the posterior p(z|x).\nI(s′; z|s) = −H(s′|s, z) + H(s′|s)\n(29)\n≥Ez,s,s′∼p\n\u0002\nlog qφ(s′|s, z)] + Ez,s,s′∼p\n\u0002\nlog qα(z|s′, s)\n\u0003\n+ H(s′, z|s)\n(30)\n= Ez,s,s′∼p\n\u0002\nlog qφ(s′|s, z) + log qα(z|s′, s)] + H(s′, z|s)\n(31)\nwhere we have used the inequality for H(s′|s) to introduce the variational posterior for skill infer-\nence qα(z | s′, s) besides the conventional variational lower bound to introduce q(s′ | s, z). Further\ndecomposing the leftover entropy:\nH(s′, z|s) = H(z|s) + H(s′|s, z)\n19\nPublished as a conference paper at ICLR 2020\nReusing the variational lower bound for marginal entropy from Agakov (2004), we get:\nH(s′|s, z) ≥Es,z\nh Z\np(s′, a|s, z) log q(a|s′, s, z)\np(s′, a|s, z)ds′da\ni\n(32)\n= −log c + H(s′, a|s, z)\n(33)\n= −log c + H(s′|s, a, z) + H(a|s, z)\n(34)\nSince, the choice of posterior is upon us, we can choose q(a|s′, s, z) = 1/c to induce a uniform\ndistribution for the bounded action space. For H(s′|s, a, z), notice that the underlying dynamics\np(s′|s, a) are independent of z, but the actions do depend upon z. Therefore, this corresponds to\nentropy-regularized RL when the dynamics of the system are deterministic. Even for stochastic\ndynamics, the analogy might be a good approximation , assuming the underlying dynamics are not\nvery entropic. The ﬁnal objective (making this low-entropy dynamics assumption) can be written\nas:\nI(s′; z|s) ≥EsEp(s′,z|s)[log qφ(s′|s, z) + log qα(z|s′, s) −log p(z|s)] + H(a|s, z)\n(35)\nWhile this does bypass the intractability of p(s′ | s), it runs into the intractable p(z | s), despite\ndeploying signiﬁcant mathematical machinery and additional assumptions. Any variational approx-\nimation for p(z | s) would again result in an improper lower bound for I(s′; z | s).\n(c) One way to a make our approximation q(s′ | s) to more closely resemble p(s′ | s) is to change\nour generative model p(z, s, s′). In particular, if we resample z ∼p(z) for every timestep of the\nrollout from π, we can indeed write p(z | s) = p(z). Note, p(s′ | s) is still intractable to compute,\nbut marginalizing qφ(s′ | s, z) over p(z) becomes a better approximation of p(s′ | s). However,\nthis severely dampens the interpretation of our latent space Z as temporally extended actions (or\nskills). It becomes better to interpret the latent space Z as dimensional reduction of action space.\nEmpirically, we found that this signiﬁcantly throttles the learning, not yielding useful or interpretable\nskills.\nD\nINTERPOLATION IN CONTINUOUS LATENT SPACE\nFigure 13: Interpolation in the continuous primitive space learned using DADS on the Ant environment cor-\nresponds to interpolation in the trajectory space. (Left) Interpolation from z = [1.0, 1.0] (solid blue) to\nz = [−1.0, 1.0] (dotted cyan); (Middle) Interpolation from z = [1.0, 1.0] (solid blue) to z = [−1.0, −1.0]\n(dotted cyan); (Right) Interpolation from z = [1.0, 1.0] (solid blue) to z = [1.0, −1.0] (dotted cyan).\nE\nMODEL PREDICTION\nFrom Figure 14, we observe that skill-dynamics can provide robust state-predictions over long plan-\nning horizons. When learning skill-dynamics with x−y prior, we observe that the error in prediction\nrises slower with horizon as compared to the norm of the actual position. This provides strong ev-\nidence of cooperation between the primitives and skill-dynamics learned using DADS with x −y\nprior. As the error-growth for skill-dynamics learned on full-observation space is sub-exponential,\nsimilar argument can be made for DADS without x −y prior as well (albeit to a weaker extent).\n20\nPublished as a conference paper at ICLR 2020\nFigure 14: (Left) Prediction error in the Ant’s co-ordinates (normalized by the norm of the actual position)\nfor skill-dynamics. (Right) X-Y traces of actual trajectories (colored) compared to trajectories predicted by\nskill-dynamics (dotted-black) for different skills.\n21\n",
  "categories": [
    "cs.LG",
    "cs.RO",
    "stat.ML"
  ],
  "published": "2019-07-02",
  "updated": "2020-02-14"
}