{
  "id": "http://arxiv.org/abs/1907.02140v2",
  "title": "Integration of Imitation Learning using GAIL and Reinforcement Learning using Task-achievement Rewards via Probabilistic Graphical Model",
  "authors": [
    "Akira Kinose",
    "Tadahiro Taniguchi"
  ],
  "abstract": "Integration of reinforcement learning and imitation learning is an important\nproblem that has been studied for a long time in the field of intelligent\nrobotics. Reinforcement learning optimizes policies to maximize the cumulative\nreward, whereas imitation learning attempts to extract general knowledge about\nthe trajectories demonstrated by experts, i.e., demonstrators. Because each of\nthem has their own drawbacks, methods combining them and compensating for each\nset of drawbacks have been explored thus far. However, many of the methods are\nheuristic and do not have a solid theoretical basis. In this paper, we present\na new theory for integrating reinforcement and imitation learning by extending\nthe probabilistic generative model framework for reinforcement learning, {\\it\nplan by inference}. We develop a new probabilistic graphical model for\nreinforcement learning with multiple types of rewards and a probabilistic\ngraphical model for Markov decision processes with multiple optimality\nemissions (pMDP-MO). Furthermore, we demonstrate that the integrated learning\nmethod of reinforcement learning and imitation learning can be formulated as a\nprobabilistic inference of policies on pMDP-MO by considering the output of the\ndiscriminator in generative adversarial imitation learning as an additional\noptimal emission observation. We adapt the generative adversarial imitation\nlearning and task-achievement reward to our proposed framework, achieving\nsignificantly better performance than agents trained with reinforcement\nlearning or imitation learning alone. Experiments demonstrate that our\nframework successfully integrates imitation and reinforcement learning even\nwhen the number of demonstrators is only a few.",
  "text": "October 17, 2019\nAdvanced Robotics\n0.main\nTo appear in Advanced Robotics\nVol. 00, No. 00, January 2013, 1–15\nFULL PAPER\nIntegration of Imitation Learning using GAIL and Reinforcement Learning\nusing Task-achievement Rewards via Probabilistic Graphical Model\nAkira Kinose and Tadahiro Taniguchi\n(Received 00 Month 201X; accepted 00 Month 201X)\nThe integration of reinforcement learning (RL) and imitation learning (IL) is an important problem\nthat has been studied for a long time in the ﬁeld of intelligent robotics. RL optimizes policies to\nmaximize the cumulative reward, whereas IL attempts to extract general knowledge about the tra-\njectories demonstrated by experts, i.e., demonstrators. Because each of them has its own drawbacks,\nmethods combining them and compensating for each set of drawbacks have been explored thus far.\nHowever, many of the methods are heuristic and do not have a solid theoretical basis. In this paper,\nwe present a new theory for integrating RL and IL by extending the probabilistic graphical model\n(PGM) framework for RL, control as inference. We develop a new PGM for RL with multiple types of\nrewards called probabilistic graphical model for Markov decision processes with multiple optimality\nemissions (pMDP-MO). Furthermore, we demonstrate that the integrated learning method of RL and\nIL can be formulated as a probabilistic inference of policies on pMDP-MO by considering the dis-\ncriminator in generative adversarial imitation learning (GAIL) as an additional optimality emission.\nWe adapt the GAIL and task-achievement reward to our proposed framework, achieving signiﬁcantly\nbetter performance than policies trained with baseline methods.\nKeywords: Imitation learning; Reinforcement learning; Probabilistic inference; Control as inference;\nGenerative adversarial imitation learning\n1.\nIntroduction\nThe integration of reinforcement learning (RL) and imitation learning (IL) is an important\nproblem that has been studied for a long time in the ﬁeld of intelligent robotics and machine\nlearning [1–14]. RL and IL are both well-studied machine learning problems. The goal is to pro-\nvide robots with capabilities to optimize policies automatically. RL is conventionally formalized\nas a reward maximization problem. An RL agent explores the optimal policy by maximizing the\ncumulative rewards. However, it requires many trials in most of the robotic control problems.\nBecause they have high dimensional states-and-actions spaces, exploration and optimization be-\ncomes a diﬃcult problem. In contrast, IL can acquire behavior by mimicking the behaviors of\nexperts. However, its performance depends on the expert, and there is no guarantee of acquiring\nan optimal policy. As a result, RL and IL have their own drawbacks.\nThe research for integrating RL and IL has been conducted to overcome problems and make use\nof the learner’s trial-and-error experience and expert’s demonstrations. However, most variations\nare designed heuristically, i.e., few have a sophisticated theoretical basis [8]. This tends to lead\nto the heuristic parameter tuning and endless exploration of the variants of frameworks for\nintegration.\nThe main goal of this paper is to present the integration of reinforcement and imitation\nlearning methods into a single theoretical framework, i.e., probabilistic graphical model and its\ninference. Such integration allows the learning agents to use expert demonstrations and agent\ntrial and error in a synergistic manner, enabling them to perform complicated tasks, such as\nrobotic control, more swiftly. For this purpose, we developed the probabilistic graphical model\n1\narXiv:1907.02140v2  [cs.LG]  16 Oct 2019\nOctober 17, 2019\nAdvanced Robotics\n0.main\n(PGM) framework with multiple optimality emissions as simultaneous observations. This theory\nis based on a framework that considers RL as probabilistic inference (a.k.a. control as inference)\non the PGM, which has been gradually attracting attention [15–20]. Thus, it is recognized that\nRL, which optimizes a policy by maximizing the accumulating reward in a Markov decision\nprocess (MDP), can be re-formulated as a probabilistic inference in the PGM. By interpreting\nlearning control problems as probability theory, we can connect common probabilistic inferences\nto a wide range of problems, such as robotic control. This connection allows as to use a variety of\napproximate inference methods and probabilistic programming tools for reinforcement learning\nﬂexibly as well.\nWe propose a framework integrating RL and IL by formulating them on the PGM framework\nby introducing multiple optimality emissions into the PGM for RL. We call the new model the\nnew PGM probabilistic graphical model for Markov decision process with multiple optimality\nemission (pMDP-MO). We demonstrate that the integrated learning method of RL and IL\ncan be formulated as a probabilistic inference maximizing multiple optimality by adapting the\noptimality of RL and IL to PGM with multiple optimality emission. Furthermore, we propose\nto use a generative adversarial imitation learning (GAIL) discriminator [21] to calculate the\noptimality distributions directly from the demonstrations. GAIL is a popular state-of-the-art\nIL method, which demonstrates good results in complex high-dimensional control tasks. The\nproposed method learns the optimal policy using maximum entropy RL with task-achievement\nand imitation rewards, which are calculated by the GAIL discriminator. This model can be\nregarded as an extension of GAIL as well as an extension of PGM for RL. The method is\nreferred to as GAIL using task-achievement reward (TRGAIL).\nThe main contributions of this study are as follows:\n• We developed PGM with multiple optimality emissions called pMDP-MO and formulated\nRL with multiple-types of rewards as probabilistic inference in pMDP-MO.\n• We proposed TRGAIL, which integrates RL and IL by using the task-achievement reward\nand imitation reward calculated by the GAIL discriminator as multiple emission distribu-\ntion on PGM, and demonstrated the eﬀectiveness through experiments.\nWe experimented the proposed method on robot manipulation tasks in the physics simulator\nand found that our proposed method is better at sampling eﬃciency and learning performance\nthan the conventional learning method.\n2.\nBackground\n2.1\nReinforcement Learning as Probabilistic Inference\nOur proposal is based on the idea of the extension of PGM for RL as probabilistic inference\n(a.k.a. control as inference). Therefore, in this section, we brieﬂy introduce RL as probabilistic\ninference [22].\nA Markov decision process (MDP) is deﬁned as M = (S, A, P, r, T). S represents the state\nspace. A represents the action space. P : S × A × S →R denotes the transition probability\ndistribution when taking action a in state s and going to s′. r : S × A →R is the reward\nfunction after taking action a in state s. T is the time horizon of the task. We deﬁne π as a\nstochastic policy π : S × A →[0, 1], and πE as an expert policy. The expert demonstrations τE\nis a set of trajectories sampled by policy πE. Trajectory τ consists of a sequence of state and\naction pairs.\nRL aims to optimize the parameters of the policy that maximizes the expected total reward\n2\nOctober 17, 2019\nAdvanced Robotics\n0.main\na1\na2\na3\na4\ns1\ns2\ns3\ns4\na1\na2\na3\na4\ns1\ns2\ns3\ns4\nO1\nO2\nO3\nO4\nFigure 1.\nLeft: Probabilistic graphical model of MDP with states and actions as stochastic variables. Right: Probabilistic\ngraphical model for reinforcement learning as probabilistic inference. The agent observes a single optimality Ot each timestep\nand infers the most probable action assuming that the optimality variables are taking true, i.e., Ot = 1. In this paper, we\nrefer to the graphical model as a probabilistic graphical model from Markov decision process (pMDP).\nbased on the objective function\nθ⋆= arg max\nθ\nT\nX\nt=1\nE(st,at)∼p(st,at|θ)[r(st, at)].\n(1)\nRecently, signiﬁcant progress has been made with regard to RL in continuous action decision\nproblems, such as video games or robot control tasks, by integrating deep learning [3, 23, 24].\nAs summarized by Levine in [22], the maximization of a reward in MDP can be re-formulated\nas the probabilistic inference problem in a PGM. The study of RL as probabilistic inference\n(a.k.a. control as inference) is not new but rather has a decade of history [15–20]. Similar ideas\nhave been proposed independently. Attias proposed an approach to the problem of planning\nunder uncertainty using hidden Markov-style models [15]. Todorov formulated a linear solvable\nMarkov decision process that enables eﬃcient approximation and determines the optimal policy\neﬃciently [16]. Kappen et al. reformulated Todorov’s non-linear stochastic optimal control prob-\nlem as a Kullback-Leibler (KL) minimization problem and eﬃciently calculated it by applying\nan approximate inference to the calculation of optimal control [17].\nIn RL as probabilistic inference, the graphical model is constructed with optimality variables\nindicating whether or not a state-action pair is optimal. Figure 1 illustrates the graphical model\nof MDP with an optimality variable. The optimality variable in RL is denoted as\np(Ot = 1|st, at) = exp(r(st, at)).\n(2)\nBy introducing the optimality variable Ot as in Equation (2), RL can be formulated as proba-\nbilistic inference. In RL as probabilistic inference, the policy is optimized by solving the proba-\nbilistic inference, such that the policy distribution is close to the optimal trajectory distribution\nconditioned to Ot = 1.\nOne way to optimize actions is as a particular type of structured variational inference. We\naim to approximate the posterior distribution over actions when we condition Ot = 1 for all\nt ∈{1, . . . , T}:\np (τ|O1:T ) ∝p (τ, O1:T ) = p (s1)\nY\nt=1\np (Ot = 1|st, at) p (st+1|st, at)\n= p (s1)\nT\nY\nt=1\nexp (r (st, at)) p (st+1|st, at)\n=\n\"\np (s1)\nT\nY\nt=1\np (st+1|st, at)\n#\nexp\n T\nX\nt=1\nr (st, at)\n!\n(3)\n3\nOctober 17, 2019\nAdvanced Robotics\n0.main\nwith another distribution:\nq(τ) = q (s1)\nT\nY\nt=1\nq (st+1|st, at) q (at|st) .\n(4)\nBy optimizing the variational lower bound, we can approximate the inference of an optimal\ntrajectory distribution. Using Equation 3, 4, and Jensen’s inequality, the variational lower bound\nof the log-likelihood is given by\nlog p (O1:T ) = log\nZZ\np (O1:T , s1:T , a1:T ) ds1:T da1:T\n= log\nZZ\np (O1:T , s1:T , a1:T ) q (s1:T , a1:T )\nq (s1:T , a1:T )ds1:T da1:T\n= log E(s1:T ,a1:T )∼q(s1:T ,a1:T )\n\u0014p (O1:T , s1:T , a1:T )\nq (s1:T , a1:T )\n\u0015\n≥E(s1:T ,a1:T )∼q(s1:T ,a1:T ) [log p (O1:T , s1:T , a1:T ) −log q (s1:T , a1:T )]\n= E(s1:T ,a1:T )∼q(s1:T ,a1:T )\n\" T\nX\nt=1\nr (st, at) −log q (at|st)\n#\n.\n(5)\nFrom the above, in the context of RL as inference, reward maximization by RL in MDP is\nequivalent to the objective function of maximum entropy RL [22].\nIn recent studies, maximum a posteriori policy optimization, which is one of the RL methods\nserving as a probabilistic inference, shows good results by applying the EM algorithm to the\nproblem of RL [25]. Fu et al. formulated a probabilistic inference that maximizes the probability\nof the occurrence of events in the framework of control as inference and proposed a generalization\nof inverse reinforcement learning (IRL) methods to cases in which full demonstrations are not\nneeded, such as when only samples of desired goal states are available [26].\n2.2\nGenerative Adversarial Imitation Learning\nWe consider integrating GAIL into RL via the framework of PGM for RL. Therefore, in this\nsection, we brieﬂy summarize GAIL and IL. The goal of IL is to acquire the behavior that\nmimics expert behavior. IL can be learned without reward signal r when compared to RL,\nbut it is necessary to prepare expert demonstrations in advance. IL can be classiﬁed into two\napproaches:\n(1) Behavior cloning (BC) [27, 28], which learns policy over state-action pairs in supervised\nlearning on expert demonstration τE. BC has been successfully applied to autonomous\ndriving [29] and locomotions [30, 31]. The BC approach is diﬃcult to use in the real world\nbecause of the compounding error caused by a covariate shift.\n(2) Inverse reinforcement learning (IRL) [21, 32–35] recovers the reward function under the\nassumption that the expert policy is optimal and learns the policy on the recovered reward\nfunction. However, inverse reinforcement learning is too expensive to perform because it\nrequires solving an RL in its learning process loop.\nRecently, Ho and Ermon developed GAIL [21], which is the IL method inspired by generative\nadversarial networks (GAN) [36]. GAIL is able to imitate the policy for complex high-dimensional\ncontrol tasks. In the GAIL framework, the agent imitates the behavior of the expert policy by\nmatching the generated state-action distribution with the distribution of experts.\nGAIL’s generator tries to make the discriminator recognize that the state-action pairs gener-\n4\nOctober 17, 2019\nAdvanced Robotics\n0.main\nated by the policy are generated from an expert. GAIL’s discriminator distinguishes state-action\npairs from those generated by the generator and expert. As learning progresses, the discrimina-\ntor guides the policy to match the state-action pairs generated by the generator with the expert\nstate-action pairs. When the Jensen-Shannon divergence between agent policy distributions and\nexpert policy distribution is minimized, the agent policy is optimal under the condition that the\nexpert policy is the optimal policy.\nGAIL’s objective function is denoted as\nmax\nθ\nmin\nω Eπθ [log(Dω(s, a))] + EπE [log(1 −Dω(s, a))]\n(6)\nwhere πθ is the agent policy that is the role of the generator, πE is the expert policy, and Dω is\na discriminator that tries to distinguish state-action pairs generated from πθ and πE. In other\nwords, Dω outputs the probability that the state-action pair is optimal under the assumption\nthat the demonstrator’s behavior is optimal. Parameters θ and ω are the parameters of the\ngenerator and discriminator, respectively, which are represented as the deep neural network.\nThe generator is trained by a policy gradient method, such as trust region policy optimization\n[37] and proximal policy optimization (PPO) [38]. The discriminator is optimized using ADAM\n[39]. Some variants of GAIL have been proposed recently as well [40–43].\n2.3\nIntegration of Reinforcement Learning and Imitation Learning via PGM\nRL and IL are both well-established approaches; however, each has its own drawbacks. On the\none hand, RL has the following drawbacks. First, it is diﬃcult to manually design the appropriate\nreward function in complicated or high-dimensional tasks, such as robotic control. Second, the\ncomputational cost of learning is prohibitively expensive because of the exploration of the policy\nspace for the reward.\nOn the other hand, IL has the following drawbacks. First, its performance depends on the\nexpert, with no guarantee of acquiring the optimal policy. Second, if the number of expert\ndemonstrations is small, it is diﬃcult for agents to learn adequately. Third, Most IL methods\ncannot eﬀectively use environmental feedbacks.\nSeveral conventional works have attempted to improve performance by combining RL with\nthe learning from demonstrations [1, 3–5]. By adapting RL to the policy learned from demon-\nstrations, it is possible to avoid searching for unnecessary action space. The combination of RL\nand learning from demonstration can evaluate whether the task is performed well, such that the\nlearned policy is improved.\nIn an early work, Lin used a successful demonstration to improve RL more eﬃciently in a 2D-\ndynamics game [1]. The use of demonstrations becomes more eﬀective as the task complexity\nincreases [2]. The most famous approach of imitation and reinforcement learning is AlphaGo,\nwhich was used to learn the game Go and was proposed by Silver et al [3].\nAs a diﬀerent approach to the combination of RL and IL, Brys used demonstrations as a prior\nknowledge for the formation of reward functions [4]. This approach of using expert demonstra-\ntions to form a reward function is similar to that of inverse reinforcement learning. Levine and\nKoltun generated guide samples from human demonstrations and used them to explore high\nreward areas of the policy space [5]. In recent years, many approaches have been proposed to\nimprove performance by combining deep reinforcement learning and IL[6–14]\nHowever, most of the integration was performed in a heuristic manner and was not formulated\non a single probabilistic generative model. The main goal of this paper is to present a theoretical\nlearning framework that can be used in complicated tasks, such as robotic control, by integrating\nthese methods through pMDP-MO and making use of the GAIL discriminator as an optimality\ndistribution.\n5\nOctober 17, 2019\nAdvanced Robotics\n0.main\na1\na2\na3\na4\ns1\ns2\ns3\ns4\nO1\n1\nO1\n2\nO1\n3\nO1\n4\n...\n...\n...\n...\nON\n1\nON\n2\nON\n3\nON\n4\nFigure 2.\nGraphical model framework for reinforcement learning as probabilistic inference with multiple optimal emissions.\nIn this framework, the agent observes multiple optimalities O1\nt , ..., ON\nt\nand infers the optimal behavior.\n3.\nProbabilistic Graphical Model for Markov Decision Process with Multiple\nOptimality Emissions (pMDP-MO)\nIn this section, we develop the new PGM framework for representing MDP with simultaneous\nobservation of multiple optimality emissions, called pMDP-MO. This graphical model is obtained\nby extending the graphical model framework for control as inference, as shown in Figure 1. We\nassume a Markov decision process model in which multiple optimalities for the state-action pair\nat the time t are observed simultaneously.\nAssuming that the respective optimality variables are independent of each other, the proba-\nbility distribution that multiple optimality emissions are observed is described as follows:\np(O1\nt , O2\nt , . . . , ON\nt |st, at) =\nN\nY\nn=1\np(On|st, at)\n(7)\nwhere N is the number of optimality types observed simultaneously and On\nt is the n-th optimality.\nFigure 2 depicts a graphical model in this setting in which multiple types of optimality are\nobserved simultaneously.\nBased\non\nPGM,\nwe\ncan\nderive\nthe\noptimal\ntrajectory\nprobabilistic\ndistribution\np(τ|O1\n1:T , . . . , ON\n1:T ) by considering that the state-action pairs are optimal O1\nt = 1, . . . , ON\nt\n= 1\nat time to horizon T.\np(τ|O1\n1:T , ..., ON\n1:T ) ∝p(τ, o1\n1:T , ..., oN\n1:T ) = p(s1)\nT\nY\nt=1\n\"\np(st+1|st, at)\nN\nY\nn=1\np(On|st, at)\n#\n(8)\nUsing Equation 3, 8 and Jensen’s inequality, the variational lower bound of log-likelihood is\n6\nOctober 17, 2019\nAdvanced Robotics\n0.main\ngiven by\nlog p\n\u0000O1\n1:T , . . . , ON\n1:T\n\u0001\n= log\nZZ\np\n\u0000O1\n1:T , . . . , ON\n1:T , s1:T , a1:T\n\u0001\nds1:T da1:T\n= log\nZZ\np\n\u0000O1\n1:T , . . . , ON\n1:T , s1:T , a1:T\n\u0001 q (s1:T , a1:T )\nq (s1:T , a1:T )ds1:T da1:T\n= log E(s1:T ,a1:T )∼q(s1:T ,a1:T )\n\"\np\n\u0000O1\n1:T , . . . , ON\n1:T , s1:T , a1:T\n\u0001\nq (s1:T , a1:T )\n#\n≥E(s1:T ,a1:T )∼q(s1:T ,a1:T )\n\u0002\nlog p\n\u0000O1\n1:T , . . . , ON\n1:T , s1:T , a1:T\n\u0001\n−log q (s1:T , a1:T )\n\u0003\n= E(s1:T ,a1:T )∼q(s1:T ,a1:T )\nT\nX\nt=1\n\" N\nX\nn=1\nlog p(On\nt |st, at) −log q (st|at)\n#\n(9)\nThis is derived by using the same procedure as Equation 5. Thus, in the PGM framework for\nsimultaneously observing multiple optimality emissions, the probability that two optimalities\nare observed is denoted as Equation 9.\nIf the distribution of optimality follows Equation 2, i.e., p(On\nt |st, at) = exp(rn(st, at)), the\nvariational lower bound of log-likelihood is given by\nlog p\n\u0000O1\n1:T , . . . , ON\n1:T\n\u0001\n≥E(s1:T ,a1:T )∼q(s1:T ,a1:T )\nT\nX\nt=1\n\" N\nX\nn=1\nrn (st, at) −log q (st|at)\n#\n.\n(10)\nEquation 10 shows that assuming multiple types of optimalities is the same as providing several\ntypes of sub-rewards in an additive manner.\n4.\nGAIL using Task-achievement Reward (TRGAIL)\nIn this section, we show that the integrated learning of RL and IL could be formulated as\nthe maximum entropy reinforcement learning for optimality for the reward in RL, i.e., task-\nachievement reward, and optimality for IL, representing whether the action is similar to that of\nthe demonstrator. The probability for the latter is calculated by the GAIL discriminator.\nWe deﬁne two types of optimality, optimality for RL OR\nt and optimality for IL OI\nt . We deﬁne\nthe distribution over OR\nt as\np(OR\nt = 1|st, at) = exp(r(st, at)).\n(11)\nBy using this formulation, as with general RL, the agent learns a policy to maximize the accu-\nmulated expected designed rewards r in the PGM framework (see section 2.1). This deﬁnition\nof optimality represents the target of RL, i.e., maximizing cumulative rewards.\nIn contrast, OI\nt represents the goal of IL. The goal of IL is to simulate expert behavior, which is\nassumed to be optimal. We adopt GAIL discriminator Dω(st, at), which indicates the probability\nif the state and action pair is generated from experts, i.e., the optimal controller, to calculate\np(OI\nt = 1|st, at) ie. this can be interpreted as the optimality for imitation.\np(OI\nt = 1|st, at) = exp(log(Dω(st, at))) = Dω(st, at).\n(12)\n7\nOctober 17, 2019\nAdvanced Robotics\n0.main\nAlgorithm 1 GAIL using task-achievement reward\nInput: τE ∼πE,\n▷Sample expert’s trajectories\nInput: θ, ω\n▷Initialize network parameters\nfor each iteration i do\nfor each environment step t do\nat ∼πθ(at|st)\n▷Sample action from the agent policy\nst+1 ∼p(st+1|st, at)\n▷Sample transition from the environment\nD ←D ∪{(st, at, r(st, at), st+1)}\n▷Store the transition in the replay buﬀer\nfor each discriminator gradient step do\nSample trajectories τi ∼D\nwi ←wi −λD\n\u0010\nˆEτi[∇w log(Dw(s, a))] + ˆEτE[∇w log(1 −Dw(s, a))]\n\u0011\n▷Update the discriminator parameters\nfor each generator gradient step do\nθi ←θi + λπEπ\nhPT\nt=1 r(st, at) + log(Dω(st, at)) −log πθ(at|st)\ni\n▷Update the policy parameters\nIncreasing the probability p(OI\nt = 1|st, at) implies that the agent policy is updated to be close\nto the expert policy. The discriminator is parameterized with the parameter ω and learned\nalternately with optimality maximization by the generator in the same way as GAIL.\nFrom Equation (8,9,11,12), the variational lower bound of log-likelihood of optimality is derived\nas\nlog p(OR\n1:T , OI\n1:T ) ≥E(s1:T ,a1:T )∼q(s1:T ,a1:T )\n\" T\nX\nt=1\nr(st, at) + log(Dω(st, at)) −log q(at|st)\n#\n.\nFrom the above, the integrated learning of RL and IL is formulated as the maximum entropy\nreinforcement learning in which the reward function is deﬁned as r(st, at) + log(Dω(st, at)).\nIn this study, we use PPO [38], which is an RL method of policy gradient, to maximize\nthe reward function r(st, at) + log(Dω(st, at)). Note that other RL algorithms can be used in\nTRGAIL. By adding the policy entropy maximization term to the objective function of the actor-\npart of the policy gradient, we regard it as maximum entropy reinforcement learning. Therefore,\nthe generator maximizes reward r(st, at) + log(Dω(st, at)) and the entropy of the policy. The\ndiscriminator is trained to discriminate between the trajectory of the expert demonstration and\nthe trajectory generated from the generator.\nIn this study, we assume the task-achievement reward as a reward for RL. It is diﬃcult to solve\nRL with a simple task-achievement reward function. Therefore, in many studies of RL, designing\nthe rewards is a problem to be solved, and researchers manually designed reward functions to\nfacilitate RL. TRGAIL aims to solve the problem by making use of the trajectories given by\ndemonstrators, i.e., by integrating imitation learning.\nThe task-achievement reward is the binary reward that indicates whether the current state-\naction pair achieved the task. Based on the task-achievement reward and imitation reward\ntrained by GAIL, we refer to this method of learning as the generative adversarial imitation\nlearning using task-achievement reward (TRGAIL).\nFigure 4 depicts the model structure of TRGAIL. Algorithm 1 shows the algorithm ﬂow of\nthe TRGAIL.\n5.\nExperiment\nTo evaluate our algorithm, we used three physics-based control tasks–Pusher, Striker, and\nThrower–which are simulated using the MuJoCo physics simulator [44]. Then, using a num-\n8\nOctober 17, 2019\nAdvanced Robotics\n0.main\nPolicy\nDiscriminator\nEnvironment\nValue\nFunction\ntrajectory\n(𝒔𝒔𝟏𝟏:𝑻𝑻, 𝒂𝒂𝟏𝟏:𝑻𝑻)\nreward\n𝑫𝑫(𝒔𝒔𝒕𝒕, 𝒂𝒂𝒕𝒕)\nstate\n𝒔𝒔𝒕𝒕\nTD\nerror\nreward\n𝒓𝒓(𝒔𝒔𝒕𝒕, 𝒂𝒂𝒕𝒕)\naction\n𝒂𝒂𝒕𝒕\nt+1←t\nGenerator\ntrajectory\n(𝒔𝒔𝟏𝟏:𝑻𝑻, 𝒂𝒂𝟏𝟏:𝑻𝑻)\nExpert\nFigure 3.\nModel structure of the proposed method. The agent receives rewards from both the discriminator and the\nenvironment, and trains the value function, policy, and discriminator.\nber of trajectories generated by the expert policy, we trained our algorithm.\n5.1\nConditions\nWe used three physics-based control tasks, which were performed by a 7-degree-of-freedom\n(DOF) manipulation robot. Figure 4 depicts each task. In each task, the state space and action\nspace are the same. The action space is a 7-dimensional continuous value of the torque given to\nevery seven joints of the arm. The state space is a 23-dimensional continuous value, with the\nangles and angular velocities of the 7 joints, and the XYZ coordinates of objects, goals, and\nhands.\nWe generated expert behavior for these tasks by running PPO [38]. Each behavior is trained\non these true reward functions deﬁned in OpenAI Gym [45] to generate expert policies. Then,\nto evaluate the imitation performance, we sampled datasets of varying trajectory counts from\nexpert policies.\nWe tested the proposed method against four baselines: PPO, BC, PPO+BC, and GAIL.\nPPO+BC indicates that the PPO is learned using the policy pre-learned by BC as an initial\nparameter, which is the most standard approach to integrate IL and RL. We trained the proposed\nmethod based on the task-achievement reward alone and did not use the reward deﬁned in the\nOpenAI Gym [45] to demonstrate that the only requirement for learning by our method is to\ndeﬁne the task-achievement condition and collect expert trajectories. This experiment aims to\nshow that TRGAIL can learn a policy on only the task-achievement reward with the help of the\ndemonstrators’ trajectories.\n5.2\nResults\nFigure 5 depicts the performance of the learned policies of each task in the MuJoCo physics\nsimulator. As shown in the ﬁgure, in most tasks, the proposed method can learn faster than the\nconventional method can, and the episode score of the learned policy is also higher. In the Striker\ntask, GAIL cannot eﬃciently learn the optimal policy because the expert trajectories trained by\nPPO are sub-optimal. In contrast, TRGAIL learned a better score than that of the sub-optimal\ntrajectories because of the task-achievement reward. In the Thrower task, considering that 15\nexpert trajectories were given, classical IL could suﬃciently learn, and there was no signiﬁcant\n9\nOctober 17, 2019\nAdvanced Robotics\n0.main\n(a) Pusher Task\n(b) Striker Task\n(c) Thrower Task\nFigure 4.\n(a) Pusher The purpose of the Pusher task is to control the robot arm to move a white cylindrical object into\na red circle goal. In the initial state of the environment, the posture of the robot arm and the coordinates of the goal are\nﬁxed values, and the coordinates of the object are determined randomly. The task-achievement condition is that the XY\ncoordinates of the object must fall within the goal range.\n(b) Striker The purpose of the Striker task is to control the robot arm and strike a white ball into the goal with a white\nfence. In the initial state of the environment, the posture of the robot arm and the coordinates of the object are ﬁxed values,\nand the coordinates of the goal are determined randomly. The task-achievement condition is that the XY coordinates of the\nobject must fall within the goal range.\n(c) Thrower The purpose of the Thrower task is to control the robot arm and throw a white ball into the white box goal.\nIn the initial state of the environment, the posture of the robot arm and the coordinates of the object are ﬁxed values, and\nthe coordinates of the goal are determined randomly. The task-achievement condition is that the x and y coordinates of the\nobject must be within the goal range, and the ball must be in contact with the bottom of the box.\n0\n1\n2\n3\n4\n5\n1e7\n0\n20\n40\n60\n80\nPusher\n0\n1\n2\n3\n4\n5\n1e7\n0\n20\n40\n60\n80\nStriker\n0\n1\n2\n3\n4\n5\n1e7\n20\n0\n20\n40\n60\n80\n100\nThrower\nPPO\nGAIL\nTRGAIL\nBC\nExpert\nTraining Steps\nEpisode Scores\nFigure 5.\nPerformance of the learned policy considering that 15 expert trajectories are given to the agent in the Pusher\nTask, Striker Task, and Thrower Task. The x-axis represents the training time steps. One episode consists of 100 time steps.\nThe y-axis represents the episode score, which is the number of time steps that achieved the goal in one episode.\ndiﬀerence between the proposed method and the conventional method.\nTable 1 and Figure 6 present the experimental results when we change the number of expert\ntrajectories given in each task. We found that if the number of expert trajectories was decreased,\nTRGAIL exhibited a higher learning performance than that exhibited by GAIL in most tasks,\nas GAIL could not learn enough.\n6.\nConclusion\nIn this study, we developed the new probabilistic graphical model framework for simultane-\nously observing multiple optimality emissions. Furthermore, we demonstrated that the inte-\ngrated learning method of RL and IL can be formulated as a probabilistic inference maximizing\n10\nOctober 17, 2019\nAdvanced Robotics\n0.main\n0\n1\n2\n3\n4\n5\n1e7\n0\n10\n20\n30\n40\n50\n60\n70\nPusher\n0\n1\n2\n3\n4\n5\n1e7\n0\n10\n20\n30\n40\n50\n60\n70\nStriker\n0\n1\n2\n3\n4\n5\n1e7\n0\n20\n40\n60\n80\nThrower\nGAIL(1)\nGAIL(5)\nGAIL(10)\nTRGAIL(1)\nTRGAIL(5)\nTRGAIL(10)\nExpert\nTraining Steps\nEpisode Scores\nFigure 6.\nPerformance of the learned policy in the case of changing the number of expert trajectories given to the agent\nin the Pusher Task, Striker Task, and Thrower Task. The x-axis represents the training time steps. One episode consists\nof 100 time steps. The y-axis represents the episode score, which is the number of time steps that achieved the goal in one\nepisode.\nTask\nPusher\nStriker\nThrower\nnum of traj\n1\n5\n10\n15\n1\n5\n10\n15\n1\n5\n10\n15\nExpert\n72.0\n70.0\n69.7\n69.0\n43.0\n41.2\n46.2\n38.7\n87.0\n87.0\n87.0\n87.0\nBC\n0.0\n1.4\n8.4\n34.0\n0.0\n2.5\n7.6\n2.3\n0.1\n26.0\n45.2\n63.5\nGAIL\n61.0\n58.4\n55.3\n61.1\n15.0\n22.3\n40.1\n30.9\n2.5\n57.2\n82.6\n86.1\nBC+PPO\n49.2\n74.6\n55.2\n69.3\n31.2\n56.5\n56.1\n56.7\n67.4\n84.5\n84.9\n85.3\nTRGAIL\n65.7\n72.2\n71.8\n72.4\n20.3\n72.6\n72.4\n67.8\n76.0\n86.6\n86.0\n86.9\nTable 1.\nExperimental result of IL and combination approach of IL and RL in the case of changing the number of experts\ngiven in each task. Episode scores indicate the number of time steps that achieved the goal in one episode. The score value\nis the average of 100 episodes performed using the highest rated policy during the trial.\nmultiple optimalities to PGM with multiple optimality emission. In experiments, our proposed\nmethod, which adapts generative adversarial imitation learning and task-achievement rewards\nto our framework, achieved signiﬁcantly better performance than that achieved by the agents\ntrained with RL or IL alone.\nHowever, we sometimes observed that the ﬁnal performance of TRGAIL decreased as the\nnumber of experts given to the agent increased. This is considered to be caused by the fact that\nIL inhibits the improvement of the score when a non-optimal expert is given. Basically, RL has a\nlow learning eﬃciency at the beginning of learning but can eﬃciently learn at a later stage where\nit can stably obtain a reward. On the other hand, IL contributes signiﬁcantly at the beginning\nof learning when no reward signal can be obtained, but at a later stage, it suﬀers a penalty for\nthe distance from the expert. From the above, we assume that it is preferable to learn while\nchanging the weight parameter of RL and IL according to the progress of learning. Speciﬁcally,\napproaches may be considered, such as setting a parameter of weight and decreasing this value\nas learning progresses. Our future study will focus on the formulation of this phenomenon in\nPGM.\nExamining the relevance of other GAIL extension methods and our proposed method is also\nimportant. The GAIL approach for obtaining desirable strategies by adding human-designed\nrewards has been previously proposed [41]. Comparing these conventional approaches with our\nproposed method, the feature of the proposed method is to reformulate this approach as prob-\nabilistic inference on PGM. By showing the relationship between such a problem and the prob-\nabilistic inference, various methods, such as existing probabilistic inference methods, can be\napplied.\n11\nOctober 17, 2019\nAdvanced Robotics\n0.main\nCompared to the conventional IL, TRGAIL is superior in that it can be learned with a minimal\nnumber of experts. Because it may be diﬃcult to prepare many expert trajectories in IL, it is\nworth being able to learn with as few expert trajectories as possible. In addition, because this\nmethod is applied to RL, it is an advantage that the expert trajectory to be given does not have\nto be optimal. TRGAIL will learn well even when only an incomplete expert trajectory is given,\nwhich performs only part of the task.\nIn our future work, we will extend our proposed framework for hierarchical models. In tasks\nfor which the process to achieve the goal is complicated, the agent should segment a trajectory\nand learn the policy for each skill. We need to propose hierarchical learning by extending the\nproposed framework using the connection with probabilistic inference proposed in this study.\nAlthough this study focuses on the integration of RL and IL, the framework for simultaneously\nobserving multiple optimality emissions is a general framework. This framework could be applied\nin other tasks with the goal of maximizing with respect to multiple optimalities.\nReferences\n[1] Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching.\nMachine learning, Vol. 8, No. 3-4, pp. 293–321, 1992.\n[2] Long Ji Lin. Programming robots using reinforcement learning and teaching. In Proceedings of the\nassociation for the advancement of artiﬁcial intelligence conference, pp. 781–786, 1991.\n[3] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,\nJulian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the\ngame of go with deep neural networks and tree search. Nature, Vol. 529, No. 7587, pp. 484–489,\n2016.\n[4] Tim Brys, Anna Harutyunyan, Halit Bener Suay, Sonia Chernova, Matthew E Taylor, and Ann Now´e.\nReinforcement learning from demonstration through shaping. In Twenty-Fourth International Joint\nConference on Artiﬁcial Intelligence, 2015.\n[5] Sergey Levine and Vladlen Koltun. Guided policy search. In International Conference on Machine\nLearning, pp. 1–9, 2013.\n[6] Yuke Zhu, Ziyu Wang, Josh Merel, Andrei Rusu, Tom Erez, Serkan Cabi, Saran Tunyasuvunakool,\nJ´anos Kram´ar, Raia Hadsell, Nando de Freitas, et al.\nReinforcement and imitation learning for\ndiverse visuomotor skills. arXiv preprint arXiv:1802.09564, 2018.\n[7] Josh Merel, Yuval Tassa, Sriram Srinivasan, Jay Lemmon, Ziyu Wang, Greg Wayne, and Nicolas\nHeess. Learning human behaviors from motion capture by adversarial imitation. arXiv preprint\narXiv:1707.02201, 2017.\n[8] Keita Hamahata, Tadahiro Taniguchi, Kazutoshi Sakakibara, Ikuko Nishikawa, Kazuma Tabuchi,\nand Tetsuo Sawaragi.\nEﬀective integration of imitation learning and reinforcement learning by\ngenerating internal reward. In 2008 Eighth International Conference on Intelligent Systems Design\nand Applications, Vol. 3, pp. 121–126, 2008.\n[9] Nan Jiang Alekh Agarwal Miroslav Dudk Yisong Yue Le, Hoang Minh and Hal Daum. Hierarchical\nimitation and reinforcement learning. International Conference on Machine Learning, 2018.\n[10] Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Over-\ncoming exploration in reinforcement learning with demonstrations.\nIn 2018 IEEE International\nConference on Robotics and Automation (ICRA), pp. 6292–6299, 2018.\n[11] Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Dan Horgan,\nJohn Quan, Andrew Sendonaris, Ian Osband, et al. Deep q-learning from demonstrations. In Thirty-\nSecond AAAI Conference on Artiﬁcial Intelligence, 2018.\n[12] Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with policy\nsketches. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp.\n166–175, 2017.\n[13] Wen Sun, Arun Venkatraman, Geoﬀrey J Gordon, Byron Boots, and J Andrew Bagnell. Deeply\naggrevated: Diﬀerentiable imitation learning for sequential prediction. In Proceedings of the 34th\nInternational Conference on Machine Learning-Volume 70, pp. 3309–3318, 2017.\n[14] Nicholas Rhinehart, Rowan McAllister, and Sergey Levine. Deep imitative models for ﬂexible infer-\n12\nOctober 17, 2019\nAdvanced Robotics\n0.main\nence, planning, and control. arXiv preprint arXiv:1810.06544, 2018.\n[15] Hagai Attias. Planning by probabilistic inference. In Proceedings of the 9th International Workshop\non Artiﬁcial Intelligence and Statistics., 2003.\n[16] Emanuel Todorov. Linearly-solvable markov decision problems. In Advances in neural information\nprocessing systems, pp. 1369–1376, 2007.\n[17] Hilbert J Kappen, Vicen¸c G´omez, and Manfred Opper. Optimal control as a graphical model infer-\nence problem. Machine learning, Vol. 87, No. 2, pp. 159–182, 2012.\n[18] Marc Toussaint. Robot trajectory optimization using approximate inference. In Proceedings of the\n26th annual international conference on machine learning, pp. 1049–1056, 2009.\n[19] Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar. On stochastic optimal control and rein-\nforcement learning by approximate inference. In Twenty-Third International Joint Conference on\nArtiﬁcial Intelligence, 2013.\n[20] Brian D Ziebart, J Andrew Bagnell, and Anind K Dey. Modeling interaction via the principle of\nmaximum causal entropy. 2010.\n[21] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural\nInformation Processing Systems, pp. 4565–4573, 2016.\n[22] Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review.\narXiv preprint arXiv:1805.00909, 2018.\n[23] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-\nstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. In Deep Learning,\nNeural Information Processing Systems Workshop, 2013.\n[24] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,\nAlex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control\nthrough deep reinforcement learning. Nature, Vol. 518, No. 7540, pp. 529–533, 2015.\n[25] Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin\nRiedmiller. Maximum a posteriori policy optimisation. arXiv preprint arXiv:1806.06920, 2018.\n[26] Justin Fu, Avi Singh, Dibya Ghosh, Larry Yang, and Sergey Levine. Variational inverse control with\nevents: A general framework for data-driven reward deﬁnition. In Advances in Neural Information\nProcessing Systems, pp. 8538–8547, 2018.\n[27] Michael Bain and Claude Sommut. A framework for behavioural claning. Machine intelligence,\nVol. 15, No. 15, p. 103, 1999.\n[28] Dean A Pomerleau. Eﬃcient training of artiﬁcial neural networks for autonomous navigation. Neural\nComputation, Vol. 3, No. 1, pp. 88–97, 1991.\n[29] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon\nGoyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning\nfor self-driving cars. arXiv preprint arXiv:1604.07316, 2016.\n[30] Jun Nakanishi, Jun Morimoto, Gen Endo, Gordon Cheng, Stefan Schaal, and Mitsuo Kawato. Learn-\ning from demonstration and adaptation of biped locomotion. Robotics and autonomous systems,\nVol. 47, No. 2-3, pp. 79–91, 2004.\n[31] Mrinal Kalakrishnan, Jonas Buchli, Peter Pastor, and Stefan Schaal. Learning locomotion over rough\nterrain using terrain templates. In Intelligent Robots and Systems, 2009. IROS 2009. IEEE/RSJ\nInternational Conference on, pp. 167–172, 2009.\n[32] Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse\nreinforcement learning. In Proceedings of the association for the advancement of artiﬁcial intelligence\nconference, Vol. 8, pp. 1433–1438, 2008.\n[33] Markus Wulfmeier, Peter Ondruska, and Ingmar Posner. Maximum entropy deep inverse reinforce-\nment learning. arXiv preprint arXiv:1507.04888, 2015.\n[34] Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control\nvia policy optimization. In International Conference on Machine Learning, pp. 49–58, 2016.\n[35] Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforce-\nment learning. InInternational Conference on Learning Representations (ICLR), 2018.\n[36] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information\nprocessing systems, pp. 2672–2680, 2014.\n[37] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region\npolicy optimization.\nIn Proceedings of the 32nd International Conference on Machine Learning\n(ICML-15), pp. 1889–1897, 2015.\n13\nOctober 17, 2019\nAdvanced Robotics\n0.main\n[38] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n[39] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n[40] Nir Baram, Oron Anschel, Itai Caspi, and Shie Mannor. End-to-end diﬀerentiable adversarial imi-\ntation learning. In Proceedings of the 34th International Conference on Machine Learning-Volume\n70, pp. 390–399, 2017.\n[41] Yunzhu Li, Jiaming Song, and Stefano Ermon. Infogail: Interpretable imitation learning from visual\ndemonstrations. In Advances in Neural Information Processing Systems, pp. 3815–3825, 2017.\n[42] Karol Hausman, Yevgen Chebotar, Stefan Schaal, Gaurav Sukhatme, and Joseph J Lim. Multi-\nmodal imitation learning from unstructured demonstrations using generative adversarial nets. In\nAdvances in Neural Information Processing Systems, pp. 1235–1245, 2017.\n[43] Jiahao Lin and Zongzhang Zhang. Acgail: Imitation learning about multiple intentions with auxiliary\nclassiﬁer gans. In Paciﬁc Rim International Conference on Artiﬁcial Intelligence, pp. 321–334, 2018.\n[44] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.\nIn Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026–\n5033, 2012.\n[45] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\nWojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\n14\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2019-07-03",
  "updated": "2019-10-16"
}