{
  "id": "http://arxiv.org/abs/1907.03626v4",
  "title": "Benchmarking Contemporary Deep Learning Hardware and Frameworks:A Survey of Qualitative Metrics",
  "authors": [
    "Wei Dai",
    "Daniel Berleant"
  ],
  "abstract": "This paper surveys benchmarking principles, machine learning devices\nincluding GPUs, FPGAs, and ASICs, and deep learning software frameworks. It\nalso reviews these technologies with respect to benchmarking from the\nperspectives of a 6-metric approach to frameworks and an 11-metric approach to\nhardware platforms. Because MLPerf is a benchmark organization working with\nindustry and academia, and offering deep learning benchmarks that evaluate\ntraining and inference on deep learning hardware devices, the survey also\nmentions MLPerf benchmark results, benchmark metrics, datasets, deep learning\nframeworks and algorithms. We summarize seven benchmarking principles,\ndifferential characteristics of mainstream AI devices, and qualitative\ncomparison of deep learning hardware and frameworks.",
  "text": "XXX-X-XXXX-XXXX-X/XX/$XX.00 ©20XX IEEE \nBenchmarking Contemporary Deep Learning \nHardware and Frameworks:A Survey  \nof Qualitative Metrics  \nWei Dai  \nDepartment of Computer Science \nSoutheast Missouri State University \nCape Girardeau, MO, USA \nwdai@semo.edu \nDaniel Berleant \nDepartment of Information Science \nUniversity of Arkansas at Little Rock \nLittle Rock, AR, USA \njdberleant@ualr.edu\nAbstract—This paper surveys benchmarking principles, \nmachine learning devices including GPUs, FPGAs, and \nASICs, and deep learning software frameworks. It also \nreviews these technologies with respect to benchmarking from \nthe perspectives of a 6-metric approach to frameworks and an \n11-metric approach to hardware platforms. Because MLPerf \nis a benchmark organization working with industry and \nacademia, and offering deep learning benchmarks that \nevaluate training and inference on deep learning hardware \ndevices, the survey also mentions MLPerf benchmark results, \nbenchmark metrics, datasets, deep learning frameworks and \nalgorithms. We summarize seven benchmarking principles, \ndifferential characteristics of mainstream AI devices, and \nqualitative comparison of deep learning hardware and \nframeworks. \nKeywords— Deep Learning benchmark, AI hardware and \nsoftware, MLPerf, AI metrics \nI. \nINTRODUCTION  \nAfter developing for about 75 years, deep learning \ntechnologies are still maturing. In July 2018, Gartner, an IT \nresearch and consultancy company, pointed out that deep \nlearning \ntechnologies \nare \nin \nthe \nPeak-of-Inflated-\nExpectations (PoIE) stage on the Gartner Hype Cycle \ndiagram [1] as shown in Figure 2, which means deep \nlearning networks trigger many industry projects as well as \nresearch topics [2][3][4]. \nImage data impacts accuracy of deep learning \nalgorithms. The accuracy of deep learning algorithms can \nbe improved by feeding  high quality images into these \nalgorithms. Well-known image sets useful for this include \nCIFAR-10 [5], MNIST[6], ImageNet [7], and Pascal \nVisual Object Classes (P-VOC) [8]. The CIFAR-10 dataset \nhas 10 groups, and all images are 32×32 color images. \nMNIST has digital handwriting images, and these images \nare black and white. ImageNet and P-VOC are high quality \nimage datasets, and are broadly used in visual object \ncategory recognition and detection. \nBenchmarking is useful in both industry and academia. \nThe definition from the Oxford English Dictionary [9]  \nstates that a benchmark is \"To evaluate or check \n(something) by comparison with an established standard.\" \nDeep neural learning networks are leading technologies that \nextend their computing performance and capability based \non flexibility, distributed architectures, creative algorithms, \nand large volume datasets. Comparing them via \nbenchmarking is increasingly important.  \nEven though previous research papers provide \nknowledge of deep learning, it is hard to find a survey \ndiscussing qualitative benchmarks for machine learning \nhardware devices and deep learning software frameworks as \nshown in Figure 1. In this paper we introduce 11 qualitative \nbenchmarking metrics for hardware devices and six metrics \nfor software frameworks in deep learning, respectively. The \npaper also provides qualitative benchmark results for major \ndeep learning devices, and compares 18 deep learning \nframeworks.  \nAccording to [16],[17], and [18], there are seven vital \ncharacteristics for benchmarks. These key properties are: \n \n1) Relevance: Benchmarks should measure important \nfeatures. \n2) Representativeness: \nBenchmark \nperformance \nmetrics should be broadly accepted by industry and \nacademia. \n3) Equity: All systems should be fairly compared. \n4) Repeatability: Benchmark results \nshould be \nverifiable.  \n5) Cost-effectiveness: Benchmark tests should be \neconomical. \n6) Scalability: Benchmark tests should measure from \nsingle server to multiple servers. \n7) Transparency: Benchmark metrics should be \nreadily understandable. \n \nEvaluating artificial intelligence (AI) hardware and \ndeep learning frameworks allows discovering both strengths \nand weaknesses of deep learning technologies. To \nilluminate, this paper is organized as follows. Section II \nreviews hardware platforms including CPUs, GPUs, \nFPGAs, and ASICs, qualitative metrics for benchmarking \nhardware, and qualitative results on benchmarking devices. \nSection III introduces qualitative metrics for benchmarking \nframeworks and results. Section IV introduces a machine \nlearning benchmark organization named MLPerf and their \ndeep learning benchmarking metrics. Section V presents our \nconclusions. Section VI discusses future work. \n   \nFig. 1. Benchmarking Metrics and AI Architectures \nII. \nDEEP LEARNING HARDWARE  \nAI algorithms often benefit from many-core hardware \nand high bandwidth memory, in comparison to many non-\nAI \nalgorithms \nthat \nare \noften \nencountered. \nThus \ncomputational power is not just a one-dimensional concept. \nThe type of computations the hardware design is best suited \nfor must be considered, since a hardware platform can have \nmore or less computational power depending on the type of \ncomputation on which it is measured. GPUs (graphics \nprocessing units) do well on the kind of parallelism often \nbeneficial to AI algorithms, in comparison to CPUs (central \nprocessing units), and thus tend to be well suited to AI \napplications. FPGAs (field programmable gate arrays), \nbeing configurable, can be configured to perform well on AI \nalgorithms although currently they lack the rich software \nlayer needed to fully achieve their potential in the AI \ndomain. ASICs (application specific integrated circuits) are \nsimilar to FPGAs in this regard, since in principle a specially \nconfigured FPGA is a kind of ASIC. Thus GPUs, FPGAs \nand ASICs have the potential to expedite machine learning \nalgorithms in part because of their capabilities for parallel \ncomputing and high-speed internal memory.  \nNevertheless, while earlier generation CPUs have had \nperformance bottlenecks while training or using deep \nlearning algorithms, cutting edge CPUs can provide better \nperformance and thus better support for deep learning \nalgorithms. In 2017, Intel released Intel Xeon Scalable \nprocessors, which includes Intel Advance Vector Extension \n512 (Intel AVX-512) instruction set and Intel Math Kernel \nLibrary for Deep Neural Networks (Intel MKL-DNN) [10]. \nThe Intel AVX-512 and MKL-DNN accelerate deep \nlearning algorithms on lower precision tasks. Comparing \nmainstream 32-bit floating point precision (fp32) on GPUs, \nthe 16-bit and 8-bits floating-point precisions (fp16/fp8) are \nlower in precision,  but can be sufficient for the inference of \ndeep learning application domain. In addition, Lower \nprecision also can enhance usage of cache and memory, and \ncan maximize memory bandwidth. Let us look specifically \nat GPUs, FPGAs, and ASICs next. \nA. GPU Devices \nGPUs are specified unitary processors that are \ndedicated to accelerating real time three-dimensional (3D) \ngraphics.  GPUs contain an internal cache, high speed \nbandwidth, and quick parallel performance. The GPU \ncache accelerates matrix multiplication routines because \nthese routines do not need to access global memory.  \nGPUs are universal hardware devices for deep \nlearning. After testing neural networks including ones with \n200 hidden layers on MNIST handwritten data sets, GPU \nperformance was found to be better than CPUs [11]. The \ntest results show NVIDIA GeForce 6800 Ultra has a 3.3X \nspeedup compared to the Intel 3GHz P4; ATI Radeon X800 \nhas 2.4-3.4X speedup. NVIDIA GPUs increase FLOPS \n(floating point operations per second) performance. In \n[12], a single NVIDIA GeForce 8800 GTX, released in \nNovember 2006, had 575 CUDA cores with 345.6 \ngigaflops, and its memory bandwidth was 86.4 GB/s; by \nSeptember 2018, a NVIDIA GeForce RTX 2080 Ti [13] \nhad 4,352 CUDA cores with 13.4 Teraflops, and its \nmemory bandwidth had increased to 616 GB/s.  \nB. FPGA Devices \nFPGAs have dynamical hardware configurations, so \nhardware engineers developed FPGAs using hardware \ndescription language (HDL), including VHDL or Verilog \n[14][15]. However, some use cases will always involve \nenergy-sensitive scenarios. FPGA devices offer better \nperformance per watt than GPUs. According to[16], while \ncomparing gigaflops per watt, FPGA devices often have a \n3-4X speed-up compared to GPUs. After comparing \nperformances of FPGAs and GPUs [17] on ImageNet 1K \ndata sets, Ovtcharov et al. [18] confirmed that the Arria 10 \nGX1150 FPGA devices handled about 233 images/sec. \nwhile device power is 25 watts. In comparison, NVIDIA \nK40 GPUs handled 500-824 images/sec. with device power \nof 235 watts. Briefly, [17] demonstrates FPGAs can process \n9.3 images/joule, but these GPUs can only process 2.1-3.4 \nimages/joule. \n \n \nFig. 2 Milestones of Deep learning on the Gartner hyper cycle. We inserted some  \ndeep learning historical milestones, modifying the figure of Gartner [1]. \nC. ASIC Devices \nUsually, ASIC devices have high throughout and low \nenergy consumption because ASICs are fabricated chips \ndesigned for special applications instead of generic tasks. \nWhile testing AlexNet, one of the convolutional neural \nnetworks, \nthe \nEyeriss \nconsumed \n278 \nmW \n[18]. \nFurthermore, the Eyeriss  achieved 125.9 images/joule (with \nbatch size N=4) [19]. In [12], Google researchers confirm \nthat the TPU 1.0, based on ASIC technologies, has about \n15-30X speed-up compared to GPUs or CPUs during the \nsame period, with TOPS/watt of about 30-80X better. \nD. Enhance Hardware Performance \nEven though multiple cores, CPUs, and hyper-threading \nare mainstream technologies, these technologies still show \nweaknesses in the big data era. For example, deep learning \nmodels usually have products and matrix transpositions \n[11], so that these algorithms require intensive computing \nresources. GPUs, FPGAs, and ASICs have better computing \nperformance with lower latency than conventional CPUs \nbecause these specialized chipsets consist of many cores and \non-chip memory. The memory hierarchy on these hardware \ndevices is usually separated into two layers: 1) off-chip \nmemory, named global memory or main memory; and 2) \non-chip memory, termed local memory or shared memory. \nAfter copying data from global memory, deep learning \nalgorithms can use high-speed shared memory to expedite \ncomputing performance. Specific program libraries provide \ndedicated application programming interfaces (APIs) for \nhardware devices, abstract complex parallel programming, \nand increased executive performance. For instance, the \nCuDNN library, released by NVIDIA, can improve \nperformance of the Apache MXNet and the Caffe on \nNVIDIA GPUs [20][17]. \nTraditionally, multiple cores, improved I/O bandwidth, \nand increased core clock speed can improve hardware \nspeeds [21]. In Figure 3, Arithmetic Logic Unit (ALU), \nsingle instruction, multiple data (SIMD), and single \ninstruction, multiple thread (SIMT) systems concurrently \nexecute multiply-accumulate (MACs) tasks based on shared \nmemory and configuration files. \nHowever, there are new algorithms to improve \ncomputing  performance. GPUs are low-latency temporary \nstorage architectures, so the Toeplitz matrix, fast Fourier \ntransform (FFT), and Winograd and Strassen algorithms can \nbe used for improving GPU performance [21]. Data \nmovement consumes energy. FPGAs and ASICs are spatial \narchitectures. These devices contain low-energy on-chip \nmemory, so that reusable dataflow algorithms provide \nsolutions for reducing data movements. Weight stationary \ndataflow, output stationary dataflow, no local reuse \ndataflow, and row stationary dataflow were developed for \ndecreasing energy consumption of FPGAs and ASICs [21]. \nIn addition, co-design of deep learning algorithms and \nhardware devices are other approaches. According to [21], \nthere are two solutions. 1) Decrease precision: There are \nseveral algorithms to decrease precision of operations and \noperands of DNN, such as 8-bit fixed point, binary weight \nsharing, and log domain quantization. 2) Reduce number of \noperations and model size: Some algorithms need to be \nhighlighted, such as exploiting activation statistics, network \npruning algorithms, and knowledge distillation algorithms.  \nE. Qualitative Benchmarking Metrics on Machine \nLearning Hardware \nGPUs, FPGAs, and ASICs can be used in different \ndomains besides deep learning, including cloud servers and \nedge devices. There are 11 qualitative benchmarking \nmetrics we distinguish on machine learning devices, as \nfollows. In addition, results of the benchmarks are shown in \nTable I.  \nTABLE I.  QUALITATIVE BENCHMARKING HARDWARE         \nFOR MACHINE LEARNING ([10]-[20])    \n# \nAttributes \nASICs \nFPGAs \nGPUs \n1 \nComputing Performance \n High  \nLow \nModerate \n2 \nLow Latency  \n High  \nModerate \nLow \n3 \nEnergy efficiency  \n High  \nModerate \nGood \n4 \nCompatibility  \n Low  \nModerate \nHigh \n5 \nResearch Costs  \n High  \nModerate \nLow \n6 \nResearch Risks  \n High  \nLow \nModerate \n7 \nUpgradability \n Low \nModerate \nHigh \n8 \nScalability  \n High  \nLow \nModerate \n9 \nChip Price  \n Low  \nHigh \nModerate \n10 \nUbicomp  \n Low  \nHigh \nHigh \n11 \nTime-to-Market  \n Low  \nHigh \nHigh \n \n1) Computing Performance can be measured by \nFLOPS. For measuring ASICs and GPUs, a \nquadrillion (thousand trillion) FLOPS (petaflops) \nare used in testing modern chipsets. In May 2017, \nGoogle announced Tensor Processor Unit 2.0 \n(TPU 2.0), which provides 11.5 petaflops per chip \n[22]. TPU 3.0, released in May 2018, offers 23.0 \npetaflops [23]. However, NVIDIA GeForce RTX \n2080 Ti has only 13.4 Teraflops [13]. According to \n[24] and [25],  ASICs have the most FLOPs, and \nGPUs are better than FPGAs. \n2) Low latency describes an important chipset \ncapability [26], and is distinguished from \nthroughout [12]. In [12][24], ASICs have the lowest \nlatency, while FPGAs are lower than GPUs. \n3) Energy efficiency in computing is particularly \nimportant for edge nodes because  mobile devices \ngenerally have limited power. In [12][24] ASICs \nhave the highest energy efficiency, and FPGAs and \nGPUs come in second and third, respectively. \n4) \nCompatibility means devices can be supported by \nmultiple deep learning frameworks and popular \nprogramming languages. FPGAs needs specially \ndeveloping libraries, so that FPGAs are not that \ngood with respect to compatibility. GPUs have the \nbest compatibilities [24]. ASICs currently are \nFig. 3. Parallel Chipsets and memory diagrams (after [21]) \nsecond. For example, TPUs support TensorFlow, \ncafe, etc. \n5) \nResearch costs refer to the total costs for \ndeveloping devices incurred from designing \narchitectures, \ndeveloping \nalgorithms, \nand \ndeploying chip sets on hardware devices. GPUs are \naffordable devices [24]. ASICs are expensive, and \nFPGAs are between GPUs and ASICs. \n6) \nResearch risks are determined by hardware \narchitectures, development risks, and deployed \nchip sets. ASICs have the highest risks before \nmarket scaling. FPGAs are very flexible, so that \ntheir risks are limited. GPUs are in the middle. \n7) \nUpgradability is a challenge for most hardware \ndevices. In [24], GPUs are the most flexible after \ndeployment, and are better than FPGAs. ASICs are \nthe most difficult to update after delivery. \n8) \nScalability means hardware devices can scale up \nquickly with low costs. Scalability is vital for clouds \nand data centres. ASICs have excellent scalability. \nGPUs have good scalability, but not as good as \nASICs.  FPGAs are the lowest on this dimension. \n9) \nChip Price means price of each unit chip after \nindustrial-scale production. In [27], FPGAs have \nthe highest chip cost after production scale-up. \nASICs have the lowest cost, and GPUs are in the \nmiddle. \n10) Ubicomp (also named ubiquitous computing) \nindicates hardware devices used extensively for \nvaried use cases including e.g. large scale clouds \nand low energy mobile devices. FPGAs are very \nflexible, so that the devices can be used in different \nindustries and scientific fields. ASICs usually are \ndedicated to specific industry needs. GPUs like \nFPGAs can be developed for many research fields \nand industry domains.  \n11) Time-to-market means the length of time from \ndesign to sale of products. According to [15],[24], \nand [27], FPGAs and GPUs have \nlower \ndevelopment time than ASICs. \nIII. \nMAINSTREAM DEEP LEARNING FRAMEWORKS \nOpen source deep learning frameworks allow engineers \nand scientists to define activation functions, develop special \nalgorithms, train big data, and deploy neural networks on \ndifferent hardware platforms, from x86 servers to mobile \ndevices. \nBased on the wide variety of usages, support teams, and \ndevelopment interfaces, we split 18 frameworks into three \nsets including mature frameworks, developing frameworks, \nand inactive frameworks. The 10 mature frameworks can be \nused currently to enhance training speed, improve scalable \nperformance, \nand \nreduce \ndevelopment \nrisks. \nThe \ndeveloping frameworks are not yet broadly used in \nindustries or research projects, but some developing \nframeworks could be used in specific fields. Retired \nframeworks are largely inactive.  \nA. Mature Frameworks \n1) Caffe and Facebook Caffe2: Caffe [28] was \ndeveloped at the University of California, Berkeley \nin C++. According to [29], Caffe can be used on \nFPGA platforms. Caffe 2 [30] is an updated \nframework supported by Facebook.  \n2) Chainer Framework: Chainer [31], written in \nPython, can be extended to multiple nodes and \nGPU \nplatformws \nthrough \nthe \nCuPy \nand \nMPI4Python libraries [32][33]. \n3) DyNet Framework: DyNet [34] was written in \nC++. The framework can readily define dynamic \ncomputation graphs, so DyNet can help improve \ndevelopment \nspeed. \nCurrently, \nDyNet \nonly \nsupports single nodes and not multiple node \nplatforms. \n  \nFig. 4. Popular Deep learning Frameworks. From right column to left one is hardware, frameworks, license types,core codes, and API codes \n4) MXNet: the Apache MXNet [35][36] is a well \nknown deep learning framework. This framework \nwas built in C++, and MXNet supports NVIDIA \nGPUs through the NVIDIA CuDNN library. In \n[37], the GLUNO is a development interface for \nMXNet. \n5) Microsoft CNTK: The Microsoft Cognitive Toolkit \n(Microsoft CNTK) [38][39], funded by Microsoft \nand written in C++, supports distributed \nplatforms. \n6) Google TensorFlow: In 2011, Google released \nDistBelief [40], but the framework was not an open \nsource project. In 2016, the project was merged \nwith TensorFlow [41][42], an open source deep \nlearning framework. \n7) Keras [43][44] is a Python library for TensorFlow, \nTheano, and Microsoft CNTK. Keras has a \nreasonable development interface that can help \ndevelopers to quickly develop demo systems and \nreduce development costs and risks. \n8) Neon and PlaidML are partially supported by \nIntel: Neon [45], supported by Nervana Systems \nand Intel, may improve performance for deep \nlearning on diverse platforms. PLaidML[46] was \nreleased by Vertex.AI in 2017; Intel will soon fund \nPlaidML.   \n9) PyTorch Framework: PyTorch [47][48], written in \nPython, can be integrated with Jupyter Notebook. \nFastAI [49] is another development interface for \nPyTorch. \n10) Theano Framework: The core language of Theano \n[50][51] is Python with a BSD license. Lasagne \n[52][53] is an additional development library for \nTheano.  \nB. Developing Frameworks \nIn addition, some deep learning frameworks are less \nfrequently mentioned by academic papers because of their \nlimited functions. For example,  \n1. Apache SINGA [54] was developed in C++. The \nframework is supported by the Apache group [44] \n[45].  \n2. BigDL [46][47], built with Scale codes, is a deep \nlearning framework that can run on Apache Spark \nand Apache Hadoops.  \n3. In [59], the authors mentioned DeepLearning4J \n(DL4J), which can be accelerated by cuDNN.  \n4. The PaddlePaddle deep learning framework was \ndeveloped by Baidu using  Python [60]. \nC. Inactive Frameworks \nWe mention two of these. (1) Torch [61], was written in \nLua. It is inactive. (2) Purine [53][54] is open source and not \nupdated since 2014. \nD. Qualitative Benchmarking Metrics for Deep            \nLearning Frameworks  \nBenchmarking metrics for frameworks for deep learning \ninclude six qualitative metrics described next.  \n1) License Type: Open source software licenses \nimpose a variety of restrictions. In [64], degree of \nopenness is used as a metric for ranking open \nsource licenses. Apache license 2.0 has relatively \nfew restrictions. The MIT license requires the most \nlimitations. BSD is in the middle. So, in comparing \ndegree of openness, Apache 2.0 > BSD > MIT.  \n2) Interface Codes (also called the API): The more \nfunctionality the API offers, the better it tends to \nsupport development. A good API can increase \ndevelopment productivity, reduce development \ncost and enhance functionality of the framework.  \n3) Compatible Hardware: Computing hardware \ndevices including CPUs and GPUs constitute the \nunderlying support for deep learning frameworks. \nThe more different hardware devices a deep \nlearning framework can run on, the better it is on \nthis dimension.    \n4) Reliability: No single point of failure (NSPOF) is a \nrisk minimizing design strategy. This approach \nensures that one fault in a framework will not break \nan entire system. For avoiding single points of \nfailure, a mature framework might run on multi-\nserver platforms rather than a single node. \n5) Tested Deep Learning Networks: Evaluating \nsoftware could discover potential problems, \nmeasure performance metrics, and highlight \nstrengths and weaknesses. If a framework can be \nofficially verified by a variety of deep learning \nnetworks, then the framework is correspondingly \nmore suitable as a mainstream production \nframework. \n6) Tested Datasets: Image datasets, voice datasets, \nand text datasets are among those used for training \nand testing deep learning networks. If a framework \nwas verifed by diverse datasets, we are able to \nknow its performance, strengths, and weaknesses.  \n Consistent with these six metrics, there are 16 \nmainstream deep learning frameworks as shown in Figure 4 \nand Table II (shown after the references). \nIV. A MACHINE LEARNING BENCHMARK ORGANIZATION \nMLPerf is a machine learning benchmark organization \nthat offers useful benchmarks that evaluate training and \ninference on deep learning hardware devices. MLPerf and \nits members are associated with advanced chip hardware \ncompanies and leading research universities. Hardware \ncompanies include Google, Nvidia, and Intel. Research \nuniversities \ninclude \nStanford \nUniversity, \nHarvard \nUniversity,  and University of Texas at Austin.  \nMLPerf members share their benchmarking results. \nBenchmark results, source codes, deep learning algorithms \n(also called deep learning models), and configuration files \nare submitted to a website on github.com. Currently MLPerf \nmembers already have submitted the MLPerf Training \nResults v0.5 and MLPerf Training Results v0.6, and the \ndeep learning reference results v0.5 will be released soon. \nMLPerf benchmarks involve benchmark metrics, \ndatasets, deep learning algorithms, and deep learning \nframeworks. MLPerf members execute deep learning \nalgorithms on hardware devices, then record execution time, \ndeep learning algorithms, deep learning frameworks, and \ntested open datasets. Time is a critical metric for measuring \nMLPerf training or inference benchmarks [65]. Short run \ntime is associated with high performance of deep learning \ndevices. Benchmark datasets consist of image datasets, \ntranslation \ndatasets, \nand \nrecommendation \ndatasets. \nImageNet and COCO [66] are among the image datasets. \nWMT English-German [67] and MovieLens-20M [68] are \ntranslation and recommendation datasets, respectively. \nMLPerf benchmark frameworks are TensorFlow, PyTorch, \nMXNet, Intel Caffe, and Sinian. MLPerf deep learning \nalgorithms benchmarked [69] include ResNet50-v1.5, \nMobileNet-v1, SSD-MobileNet, and SSD-ResNet34.  \nV. CONCLUSIONS \nDeep learning has increased in popularity dramatically \nin recent years. This technology can be used in image \nclassification, speech recognition, and language translation. \nIn addition, deep learning technology is continually \ndeveloping. Many innovative chipsets, useful frameworks, \ncreative models, and big data sets are emerging, resulting in \nextending the markets and uses for deep learning. \nWhile deep learning technology is expanding, it is \nuseful to understand the dimensions and methods for \nmeasuring \ndeep \nlearning \nhardware \nand \nsoftware. \nBenchmarking \nprinciples \ninclude representativeness, \nrelevance, equity, repeatability, affordable cost, scalability, \nand transparency. Major deep learning hardware platform \ntypes include CPUs, GPUs, FPGAs, and ASICs. We \ndiscussed machine learning platforms, and mentioned \napproaches that enhance performance of these platforms. In \naddition, we listed 11 qualitative benchmarking features for \ncomparing deep learning hardware. \nAI algorithms often benefit from many-core hardware \nand high bandwidth memory, in comparison to many non-\nAI algorithms that are often encountered in practice [70]. \nThus it is not just the computational power of hardware as \na one-dimensional concept that makes it more (or less) \nsuited to AI applications, but also the type of computations \nthe hardware excels in. A hardware platform can have more \nor less computational power depending on the type of \ncomputation on which it is measured. GPUs (graphics \nprocessing units) often do comparatively well on the kind \nof parallelism often beneficial to AI algorithms, and thus \ntend to be well suited to AI applications. FPGAs, being \nconfigurable, can be configured to perform well on AI \nalgorithms although currently they lack the rich software \nlayer needed to be as useful for this as they could become. \nASICs are similar to FPGAs in this regard, since in \nprinciple a specially configured FPGA is a kind of ASIC. \n        Software frameworks for deep learning are diverse. \nWe compared 16 mainstream frameworks through license \ntypes, compliant hardware devices, and tested deep \nlearning algorithms. We split popular deep learning \nframeworks into three categories: mature deep learning \nframeworks, \ndeveloping \nframeworks, \nand \nretired \nframeworks.  \n       Deep learning benchmarks can help link industry and \nacademia. MLPerf is a new and preeminent deep learning \nbenchmark \norganization. \nThe \norganization \noffers \nbenchmarking metrics, dataset evaluation, test codes, and \nresult sharing.       \nVI. FUTURE WORK \n      Deep learning technology including \nsupporting \nhardware devices and software frameworks is increasing in \nimportance, so scientists and engineers are developing new \nhardware and creative frameworks. We are planning a \nwebsite \nnamed \nBenchmarking \nPerformance \nSuite \n(http://www.animpala.com/research.html) for collecting \nand updating results of benchmarking hardware and \nframeworks. Users will be able to access the website for \nsharing deep learning knowledge. \nACKNOWLEDGMENT \nWe are grateful to Google for partial support of this \nproject in 2019. \nREFERENCES \n[1] \nJ. Hare and P. Krensky, “Hype Cycle for Data Science and \nMachine Learning, 2018,” Gartner Company, 2018. [Online]. \nAvailable: \nhttps://www.gartner.com/doc/3883664/hype-cycle-\ndata-science-machine. \n[2] \nW. Dai, K. Yoshigoe, and W. Parsley, “Improving data quality \nthrough deep learning and statistical models,” in Advances in \nIntelligent Systems and Computing, 2018. \n[3] \nW. Dai and N. Wu, “Profiling essential professional skills of chief \ndata officers through topic modeling algorithms,” in AMCIS 2017 \n- America’s Conference on Information Systems: A Tradition of \nInnovation, 2017, vol. 2017-Augus. \n[4] \nR. Keefer and N. Bourbakis, “A Survey on Document Image \nProcessing Methods Useful for Assistive Technology for the \nBlind,” Int. J. Image Graph., 2015. \n[5] \nA. Torralba, R. Fergus, and W. T. Freeman, “80 million tiny \nimages: A large data set for nonparametric object and scene \nrecognition,” IEEE Trans. Pattern Anal. Mach. Intell., 2008. \n[6] \nLeCun Yann, Cortes Corinna, and Burges Christopher, “THE \nMNIST DATABASE of handwritten digits,” Courant Inst. Math. \nSci., 1998. \n[7] \nJia Deng, Wei Dong, R. Socher, Li-Jia Li, Kai Li, and Li Fei-Fei, \n“ImageNet: A large-scale hierarchical image database,” in 2009 \nIEEE Conference on Computer Vision and Pattern Recognition, \n2009. \n[8] \nM. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. \nZisserman, “The pascal visual object classes (VOC) challenge,” \nInt. J. Comput. Vis., 2010. \n[9] \nO. E. D. Online, “Oxford English Dictionary Online,” Oxford \nEnglish Dict., 2010. \n[10] \nA. Rodriguez, E. Segal, E. Meiri, E. Fomenko, Y. J. Kim, and H. \nShen, “Lower Numerical Precision Deep Learning Inference and \nTraining,” Intel White Pap., 2018. \n[11] \nD. Steinkraus, I. Buck, and P. Y. Simard, “Using GPUs for \nmachine learning algorithms,” in Eighth International Conference \non Document Analysis and Recognition (ICDAR’05), 2005. \n[12] \nJ. D. Owens, M. Houston, D. Luebke, S. Green, J. E. Stone, and J. \nC. Phillips, “GPU computing,” Proc. IEEE, vol. 96, no. 5, pp. 879–\n899, 2008. \n[13] \n“Graphics Reinvented: NVIDIA GeForce RTX 2080 Ti Graphics \nCard,” NVIDIA. NVIDIA COMPANY. \n[14] \nD. Galloway, “The Transmogrifier C hardware description \nlanguage and compiler for FPGAs,” Proc. IEEE Symp. FPGAs \nCust. Comput. Mach., 1995. \n[15] \nG. Lacey, G. W. Taylor, and S. Areibi, “Deep Learning on FPGAs: \nPast, Present, and Future,” arXiv Prepr. arXiv1602.04283, 2016. \n[16] \nE. Nurvitadhi et al., “Can FPGAs beat GPUs in accelerating next-\ngeneration deep neural networks?,” in FPGA 2017 - Proceedings \nof the 2017 ACM/SIGDA International Symposium on Field-\nProgrammable Gate Arrays, 2017. \n[17] \nK. Ovtcharov, O. Ruwase, J. Kim, J. Fowers, K. Strauss, and E. S. \nChung, “Accelerating Deep Convolutional Neural Networks Using \nSpecialized Hardware,” Microsoft Res. Whitepaper, 2015. \n[18] \nA. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet \nClassification with Deep Convolutional Neural Networks,” Adv. \nNeural Inf. Process. Syst., 2012. \n[19] \nY. H. Chen, T. Krishna, J. S. Emer, and V. Sze, “Eyeriss: An \nEnergy-Efficient \nReconfigurable \nAccelerator \nfor \nDeep \nConvolutional Neural Networks,” IEEE J. Solid-State Circuits, \n2017. \n[20] \nMxn. Developers, “Apache MXNet(incubating) - A Flexible and \nEfficient Library for Deep Learning,” Apache, 2018. [Online]. \nAvailable: https://mxnet.apache.org/. \n[21] \nV. Sze, Y. H. Chen, T. J. Yang, and J. S. Emer, “Efficient \nProcessing of Deep Neural Networks: A Tutorial and Survey,” \nProceedings of the IEEE. 2017. \n[22] \n“Google reveals more details about its second-gen TPU AI chips,” \ntechcrunch. \n[Online]. \nAvailable: \nhttps://www.theinquirer.net/inquirer/news/3023202/google-\nreveals-more-details-about-its-second-gen-tpu-ai-chips. \n[23] \n“Google announces a new generation for its TPU machine learning \nhardware,” \ntechcrunch. \n[Online]. \nAvailable: \nhttps://techcrunch.com/2018/05/08/google-announces-a-new-\ngeneration-for-its-tpu-machine-learning-hardware/. \n[24] \nBERTEN DSP, “GPU vs FPGA Performance Comparison,” 2016. \n[Online]. \nAvailable: \nhttp://www.bertendsp.com/pdf/whitepaper/BWP001_GPU_vs_FP\nGA_Performance_Comparison_v1.0.pdf. \n[25] \nM. Parker, “Understanding Peak Floating-Point Performance \nClaims,” Intel FPGA White Paper, 2016. \n[26] \nD. A. Patterson, “LATENCY LAGS BANDWITH.,” Commun. \nACM, 2004. \n[27] \nE. Vansteenkiste, “New FPGA Design Tools and Architectures,” \nGhent University. Faculty of Engineering and Architecture, 2016. \n[28] \nY. Jia et al., “Caffe: Convolutional architecture for fast feature \nembedding,” in Proceedings of the 22nd ACM international \nconference on Multimedia, 2014, pp. 675–678. \n[29] \nJ. Xu, Z. Liu, J. Jiang, Y. Dou, and S. Li, “CaFPGA: An automatic \ngeneration model for CNN accelerator,” Microprocess. Microsyst., \n2018. \n[30] \n“Caffe2, GitHub Repository,” 2018. [Online]. Available: \nhttps://caffe2.ai/. \n[31] \nC. Developers, “Chainer Repository,” GitHub repository, 2018. \n[Online]. Available: https://github.com/chainer. \n[32] \nS. Tokui, K. Oono, S. Hido, and J. Clayton, “Chainer: a Next-\nGeneration Open Source Framework for Deep Learning,” in \nProceedings of Workshop on Machine Learning Systems \n(LearningSys) in The Twenty-ninth Annual Conference on Neural \nInformation Processing Systems (NIPS), 2015. \n[33] \nT. Akiba, K. Fukuda, and S. Suzuki, “ChainerMN: Scalable \nDistributed Deep Learning Framework,” in Proceedings of \nWorkshop on ML Systems in The Thirty-first Annual Conference \non Neural Information Processing Systems (NIPS), 2017. \n[34] \nG. Neubig et al., “Dynet: The dynamic neural network toolkit,” \narXiv Prepr. arXiv1701.03980, 2017. \n[35] \nT. Chen et al., “MXNet: A Flexible and Efficient Machine \nLearning Library for Heterogeneous Distributed Systems,” arXiv \nPrepr. arXiv1512.01274, 2015. \n[36] \nMxn. J. Developers, “MXNetJS Deep Learning in Browser,” \nGitHub \nrepository, \n2018. \n[Online]. \nAvailable: \nhttps://github.com/dmlc/mxnet.js/. \n[37] \n“GLUON,” \n2018. \n[Online]. \nAvailable: \nhttps://gluon.mxnet.io/index.html. \n[38] \n“Microsoft2018CNTK,” \n2018. \n[Online]. \nAvailable: \nhttps://www.microsoft.com/en-us/cognitive-toolkit/. \n[39] \nF. Seide and A. Agarwal, “CNTK: Microsoft’s open-source deep-\nlearning toolkit,” in 22nd ACM International Conference on \nKnowledge Discovery and Data Mining (KDD), 2016. \n[40] \nJ. Dean et al., “Large Scale Distributed Deep Networks,” Adv. \nNeural Inf. Process. Syst., 2012. \n[41] \n“Tensorflow: An open source library,” 2018. [Online]. Available: \nhttps://www.tensorflow.org/. \n[42] \nM. Abadi et al., “TensorFlow : A System for Large-Scale Machine \nLearning,” Proc 12th USENIX Conf. Oper. Syst. Des. Implement., \n2016. \n[43] \nC. François, “Keras,” https://github.com/fchollet/keras, 2015. \n[Online]. Available: https://keras.io/. \n[44] \nChollet François, “Keras: The Python Deep Learning library,” \nkeras.io. 2015. \n[45] \n“Neon, \nGitHub \nRepository,” \n2018. \n[Online]. \nAvailable: \nhttps://github.com/NervanaSystems/neon. \n[46] \nC. Ng, “Announcing PlaidML: Open Source Deep Learning for \nEvery Platform,” 2017. \n[47] \n“PyTorch: An open source deep learning platform,” 2018. \n[Online]. Available: https://pytorch.org/. \n[48] \nA. Paszke et al., “Automatic differentiation in PyTorch,” 31st \nConf. Neural Inf. Process. Syst., 2017. \n[49] \n“FastAI, GitHub Repository,” 2018. [Online]. Available: \nhttps://www.fast.ai/. \n[50] \n“Theano,” \n2018. \n[Online]. \nAvailable: \nhttp://deeplearning.net/software/theano/. \n[51] \nR. Al-Rfou et al., “Theano: A Python framework for fast \ncomputation of mathematical expressions,” arXiv Prepr., 2016. \n[52] \n“Lasagne, GitHub Repository,” 2018. [Online]. Available: \nhttps://github.com/Lasagne/Lasagne. \n[53] \nB. Van Merriënboer et al., “Blocks and fuel: Frameworks for deep \nlearning,” arXiv Prepr. arXiv1506.00619, 2015. \n[54] \nB. C. Ooi et al., “SINGA: A Distributed Deep Learning Platform,” \nProc. 23rd ACM Int. Conf. Multimed. - MM ’15, 2015. \n[55] \nW. Wang et al., “SINGA : Putting Deep Learning in the Hands of \nMultimedia Users,” Multimedia, 2015. \n[56] \nA. G. T. N. Daniel Dai Ted Dunning, “Apache SINGA,” Apache, \n2018. [Online]. Available: https://singa.incubator.apache.org/. \n[57] \n“BigDL,” \nApache, \n2018. \n[Online]. \nAvailable: \nhttps://github.com/intel-analytics/BigDL. \n[58] \nYiheng Wang et al., “BigDL: A Distributed Deep Learning \nFramework for Big Data,” arXiv Prepr. arXiv1804.05839, 2018. \n[59] \n“Deeplearning4j: Open-source distributed deep learning for the \njvm,” Apache Softw. Found. Licens., 2018. \n[60] \nB. Company, “PaddlePaddle-based AI.” [Online]. Available: \nhttp://en.paddlepaddle.org/. \n[61] \n“Torch,GitHub \nrepository,” \n2018. \n[Online]. \nAvailable: \nhttps://github.com/torch/torch7. \n[62] \n“Purine, GitHub Repository,” 2018. [Online]. Available: \nhttps://github.com/purine/purine2. \n[63] \nM. Lin, S. Li, X. Luo, and S. Yan, “Purine: A bi-graph based deep \nlearning framework,” arXiv Prepr. arXiv1412.6249, 2014. \n[64] \nY. H. Lin, T. M. Ko, T. R. Chuang, and K. J. Lin, “Open source \nlicenses and the creative commons framework: License selection \nand comparison,” J. Inf. Sci. Eng., 2006. \n[65] \nC. Coleman et al., “Analysis of DAWNBench, a Time-to-Accuracy \nMachine Learning Performance Benchmark,” arXiv Prepr. \narXiv1806.01427, 2018. \n[66] \nT. Y. Lin et al., “Microsoft COCO: Common objects in context,” \nin Lecture Notes in Computer Science (including subseries Lecture \nNotes \nin \nArtificial \nIntelligence \nand \nLecture \nNotes \nin \nBioinformatics), 2014. \n[67] \nW.-2016 and 2017, “Third Conference on Machine Translation.” \n[Online]. \nAvailable: \nhttp://www.statmt.org/wmt18/WMT-\n2018.pdf. \n[68] \n“GroupLens: Movielens-20m data sets.” [Online]. Available: \nhttps://grouplens.org/datasets/%0Amovielens/20m/. \n[69] \nP. Mattson, “MLPerf Training Algorithms.” [Online]. Available: \nhttps://github.com/mlperf/training. \n[70] \nG. Anadiotis, “AI chips for big data and machine learning: GPUs, \nFPGAs, and hard choices in the cloud and on-premise,” ZDNet, \n2019. [Online]. Available: https://www.zdnet.com/article/ai-chips-\nfor-big-data-and-machine-learning-gpus-fpgas-and-hard-choices-\nin-the-cloud-and-on-premise/. \n \nTABLE II.  \nCOMPARING POPULAR DEEP LEARNING FRAMEWORKS \n \na. alphabetical order \nb. In License Type column, Apache 2.0 means the Apache 2.0 license \n \n \n# \nFrameworks a   \nLicense Type b  \n \nCore Codes \nAPI Codes \nHardware Devices \nReliability \nTested Networks \nRelated Datasets \n \n1 \nBigDL \nApache 2.0 \n \nC/C++ \nScala \nCPU/GPU \nMulti-Server \nVGG,Inception,ResNet,GoogleNet \nImageNet, CIFAR-10 \n \n2 \nCaffe/Caffe2 \nBSD License \n \nC/C++ \nPython, C++ \nMATLAB \nCPU/GPU \n/FPGA/Mobile \nMulti-Server \nLeNet, RNN \nCIFAR-10,MNIST, ImageNet \n \n3 \nChainer \nMIT License \n \nC/C++ \nPython \nCPU/GPU \nMulti-Server \nRNN \nCIFAR-10, ImageNet \n \n \n4 \nDeepLearning4j \nApache 2.0 \n \n \nJava \nJava, Scala, Clojure,  \nPython, Kotlin \nCPU/GPU \nMulti-Server \nAlexNet,LeNet,Inception, \n ResNet, RNN, LSTM,  \nVGG,Xception, \nImageNet \n \n5 \nDyNet \nApache 2.0 \n \nC/C++ \nC++, Python \nCPU/GPU \nSingle Node \nRNN, LSTM \nImageNet \n \n6 \nFastAI \nApache 2.0 \n \nPython \nPython \nCPU/GPU \nMulti-Server \nResNet \nCIFAR-10, ImageNet \n \n7 \nKeras \nMIT  License \n \nPython \nPython, R \nCPU/GPU \nMulti-Server \nCNN, RNN \nCIFAR-10,MNIST \n \n8 \nMicrosoft CNTK \nMIT License \n \nC/C++ \nC++, C#, Python, Java \nCPU/GPU \nMulti-Server \nCNN, RNN,LSTM \nCIFAR-10, \nMNIST,ImageNet,P-VOC \n \n \n9 \nMXNet \nApache 2.0 \n \n \nC/C++ \nC++, Python, Clojure, \n Julia, Perl, R, Scala,  \nJava,JavaScript,Matlab \nCPU/GPU \n/Mobile \nMulti-Server \nCNN, RNN,Inception \nCIFAR-10, \nMNIST,ImageNet,P-VOC \n \n10 \nNeon \nApache 2.0 \n \nPython \nPython \nCPU/GPU \nMulti-Server \nAlexNet,  ResNet, LSTM \nCIFAR-10, mnist,ImageNet \n \n11 \nPaddlePaddle \nApache 2.0 \n \nC/C++ \nPython \nCPU/GPU \n/Mobile \nMulti-Server \nAlexNet,GoogleNet,LSTM \nCIFAR-10, ImageNet \n \n \n12 \nPlaidML \nApache 2.0 \n \n \nC/C++ \nPython, C++ \nCPU/GPU \nMulti-Server \nInception, ResNet, VGG,  \nXception, MobileNet, DenseNet, \n ShuffleNet, LSTM \nCIFAR-10, ImageNet \n \n13 \nPyTorch \nBSD License \n \nPython \nPython \nCPU/GPU \nMulti-Server \nAlexNet,Inception, ResNet, \n VGG, DenseNet, SqueezeNet \nCIFAR-10, ImageNet \n \n \n14 \nSINGA \nApache 2.0 \n \n \nC/C++ \nPython \nCPU/GPU \nMulti-Server \nRNN, AlexNet,DenseNet, \n GoogleNet, Inception, \n ResidualNet,VGG \nMNIST, ImageNet \n \n \n15 \nTensorFlow \nApache 2.0 \n \n \nC/C++ \nPython, C++, Java,  \nGo, JavaScript, Scala, \n Julia, Swift \nCPU/GPU \n/TPU/Mobile \nMulti-Server \nAlexNet,Inception, ResNet,  \nVGG, LeNet, MobileNet \nCIFAR-10, mnist,ImageNet \n \n16 \nTheano \nBSD License \n \nPython \nPython (Keras) \nCPU/GPU \nMulti-Server \nAlexNet, VGG, GoogleNet \nCIFAR-10, ImageNet \n",
  "categories": [
    "cs.DC",
    "cs.LG",
    "cs.PF"
  ],
  "published": "2019-07-05",
  "updated": "2019-11-23"
}