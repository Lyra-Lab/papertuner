{
  "id": "http://arxiv.org/abs/1907.05403v1",
  "title": "Incrementalizing RASA's Open-Source Natural Language Understanding Pipeline",
  "authors": [
    "Andrew Rafla",
    "Casey Kennington"
  ],
  "abstract": "As spoken dialogue systems and chatbots are gaining more widespread adoption,\ncommercial and open-sourced services for natural language understanding are\nemerging. In this paper, we explain how we altered the open-source RASA natural\nlanguage understanding pipeline to process incrementally (i.e., word-by-word),\nfollowing the incremental unit framework proposed by Schlangen and Skantze. To\ndo so, we altered existing RASA components to process incrementally, and added\nan update-incremental intent recognition model as a component to RASA. Our\nevaluations on the Snips dataset show that our changes allow RASA to function\nas an effective incremental natural language understanding service.",
  "text": "Incrementalizing RASA’s Open-Source\nNatural Language Understanding Pipeline\nAndrew Raﬂa\nDepartment of Computer Science\nBoise State University\n1910 University Dr.\nandrewrafla@\nu.boisestate.edu\nCasey Kennington\nDepartment of Computer Science\nBoise State University\n1910 University Dr.\ncaseykennington@\nboisestate.edu\nAbstract\nAs spoken dialogue systems and chatbots\nare gaining more widespread adoption, com-\nmercial and open-sourced services for nat-\nural language understanding are emerging.\nIn this paper, we explain how we altered\nthe open-source RASA natural language un-\nderstanding pipeline to process incrementally\n(i.e., word-by-word), following the incremen-\ntal unit framework proposed by Schlangen\nand Skantze.\nTo do so, we altered existing\nRASA components to process incrementally,\nand added an update-incremental intent recog-\nnition model as a component to RASA. Our\nevaluations on the Snips dataset show that our\nchanges allow RASA to function as an effec-\ntive incremental natural language understand-\ning service.\n1\nIntroduction\nThere is no shortage of services that are marketed\nas natural language understanding (NLU) solutions\nfor use in chatbots, digital personal assistants, or\nspoken dialogue systems (SDS). Recently, Braun\net al. (2017) systematically evaluated several such\nservices, including Microsoft LUIS,1 IBM Watson\nConversation, API.ai,2 wit.ai,3 Amazon Lex,4 and\nRASA (Bocklisch et al., 2017).5 More recently,\nLiu et al. (2019) evaluated LUIS, Watson, RASA,\nand DialogFlow using some established bench-\nmarks.6 Some NLU services work better than oth-\ners in certain tasks and domains with a perhaps\nsurprising pattern: RASA, the only fully open-\nsource NLU service among those evaluated, con-\nsistently performs on par with the commercial ser-\nvices.\n1https://www.luis.ai\n2https://www.api.ai\n3https://www.wit.ai\n4https://aws.amazon.com/lex\n5https://www.rasa.ai\n6https://dialogflow.com/\nThough these services yield state-of-the-art per-\nformance on a handful of NLU tasks, one draw-\nback to SDS and robotics researchers is the fact\nthat all of these NLU solutions process input at the\nutterance level; none of them process incremen-\ntally at the word-level. Yet, research has shown\nthat humans comprehend utterances as they un-\nfold (Tanenhaus et al., 1995).\nMoreover, when\na listener feels they are missing some crucial in-\nformation mid-utterance, they can interject with a\nclariﬁcation request, so as to ensure they and the\nspeaker are maintaining common ground (Clark,\n1996).\nUsers who interact with SDSs perceive\nincremental systems as being more natural than\ntraditional, turn-based systems (Aist et al., 2006;\nSkantze and Schlangen, 2009; Asri et al., 2014),\noffer a more human-like experience (Edlund et al.,\n2008) and are more satisfying to interact with than\nnon-incremental systems (Aist et al., 2007). Users\neven prefer interacting with an incremental SDS\nwhen the system is less accurate or requires ﬁlled\npauses while replying (Baumann, 2013) or oper-\nates in a limited domain as long as there is in-\ncremental feedback (Kennington and Schlangen,\n2016).\nIn this paper, we report our recent efforts in\nmaking the RASA NLU pipeline process incre-\nmentally.\nWe explain brieﬂy the RASA frame-\nwork and pipeline, explain how we altered the\nRASA framework and individual components (in-\ncluding a new component which we added) to al-\nlow it to process incrementally, then we explain\nhow we evaluated the system to ensure that RASA\nworks as intended and how researchers can lever-\nage this tool.\n2\nThe RASA NLU Pipeline\nRASA consists of NLU and core modules, the lat-\nter of which is akin to a dialogue manager; our\narXiv:1907.05403v1  [cs.CL]  11 Jul 2019\nfocus here is on the NLU. The NLU itself is further\nmodularized as pipelines which deﬁne how user\nutterances are processed, for example an utterance\ncan pass through a tokenizer, named entity recog-\nnizer, then an intent classiﬁer before producing a\ndistribution over possible dialogue acts or intents.\nThe pipeline and the training data are authorable\n(following a markdown representation; json for-\nmat can also be used for the training data) allow-\ning users to easily setup and run experiments in\nany domain as a standalone NLU component or as\na module in a SDS or chatbot. Importantly, RASA\nhas provisions for authoring new components as\nwell as altering existing ones.\nFigure 1: The lifecycle of RASA components (from\nhttps://rasa.com/docs/nlu/)\nFigure 1 shows a schematic of a pipeline for\nthree components. The context (i.e., training data)\nis passed to Component A which performs its\ntraining, then persists a trained model for that\ncomponent. Then the data is passed through Com-\nponent A as input for Component B which also\ntrains and persists, and so on for Component C.\nDuring runtime, the persisted models are loaded\ninto memory and together form the NLU module.\n3\nIncrementalizing RASA\nOur approach to making RASA incremental\nfollows the incremental unit (IU) framework\nSchlangen and Skantze (2011) as has been done\nin previous work for dialogue processing toolk-\nits (Baumann and Schlangen, 2012).\nWe treat\neach module in RASA as an IU processing module\nand speciﬁcally make use of the ADD and RE-\nVOKE IU operations; for example, ADD when a\nnew word is typed or recognized by a speech rec-\nognizer, and REVOKE if that word is identiﬁed\nas having been erroneously recognized in light of\nnew information.\nBy default, RASA components expect full ut-\nterances, not single words. In addition to the chal-\nlenge of making components in the NLU pipeline\nprocess word-by-word, we encounter another im-\nportant problem: there is no ready-made signal for\nthe end of an utterance. To solve this, we added\nfunctionality to signal the end of an utterance; this\nsignal can be triggered by any component, includ-\ning the speech recognizer where it has traditionally\noriginated via endpointing. With this ﬂexibility,\nany component (or set of components) can make a\nmore informed decision about when an utterance\nis complete (e.g., if a user is uttering installments,\nendpointing may occur, but the intent behind the\nuser’s installments is not yet complete; the deci-\nsion as to when an utterance is complete can be\nmade by the NLU or dialogue manager).\nTraining RASA NLU proceeds as explained\nabove (i.e., non-incrementally). For runtime, pro-\ncessing incrementally through the RASA pipeline\nis challenging because each component must have\nprovisions for handling word-level input and must\nbe able to handle ADD and REVOKE IU opera-\ntions. Each component in a pipeline, for example,\nas depicted in Figure 1, must operate in lock-step\nwith each other where a word is ADDed to Com-\nponent A which beings processing immediately,\nthen ADDs its processing result to Component B,\nthen Component B processes and passes output to\nComponent C all before the next word is produced\nfor Component A.\n3.1\nIncrementalizing RASA Components\nWe now explain how we altered speciﬁc RASA\ncomponents to make them work incrementally.\nMessage\nThe Message class in RASA NLU\nis the main message bus between components in\nthe pipeline. Message follows a blackboard ap-\nproach to passing information between compo-\nnents. For example, in a pipeline containing a to-\nkenizer, intent classiﬁer, and entity extractor, each\nof the components would store the tokens, intent\nclass, and entities in the Message object, respec-\ntively. Our modiﬁcations to Message were min-\nimal; we simply used it to store IUs and corre-\nsponding edit types (i.e., ADD or REVOKE).\nComponent\nIn order to incrementalize RASA\nNLU,\nwe\nextended\nthe\nbase\nComponent\nto make an addition of a new component,\nIncrementalComponent.\nA\nuser\nwho\ndeﬁnes their own IncrementalComponent\nunderstands\nthe\ndifference\nin\nfunctionality,\nnotably in the parse method.\nAt runtime, a\nnon-incremental component expects a full utter-\nance, whereas an incremental one expects only a\nsingle IU. Because non-incremental components\nexpect the entire utterance, they have no need\nto save any internal state across process calls,\nand can clear any internal data at the end of the\nmethod. However, with incremental components,\nthat workﬂow changes; each call to process\nmust maintain its internal state, so that it can\nbe updated as it receives new IUs.\nMoreover,\nIncrementalComponents additionally have\na new utterance method. In non-incremental\nsystems, the call to process implicitly signals\nthat the utterance has been completed, and there\nis no need to store internal data across process\ncalls,\nwhereas incremental systems lose that\nsignal as a result. The new utterance method\nacts as that signal.\nInterpreter\nThe\nInterpreter\nclass\nin\nRASA NLU is the main interface between user in-\nput (e.g., ASR) and the series of components in the\npipeline.\nOn training, the Interpreter pre-\npares the training data, and serially calls train on\neach of the components in the pipeline. Similarly,\nto process input, one uses the Interpreter’s\nparse method, where the Interpreter pre-\npares the input (i.e., the ongoing utterance) and\nserially calls process on the components in the\npipeline (analgous to left buffer updates in the IU\nframework). As a result of its design, we were\nable to leverage the Interpreter class for\nincremental processing, notably because of its\nuse of a persistent Message object as a bus of\ncommunication between Components.\nAs\nwith\nour\nimplementation\nof\nthe\nIncrementalComponent\nclass,\nwe\ncre-\nated the IncrementalInterpreter.\nThe\nIncrementalInterpreter class adds two\nnew methods:\n• new utterance\n• parse incremental\nThe\nnew utterance\nmethod\nis\nfairly\nstraightforward;\nit\nclears\nRASA\nNLU’s\ninternal\nMessage\nobject\nthat\nis\nshared\nbetween\ncomponents,\nand\ncalls\neach\nIncrementalComponent in the pipeline’s\nnew utterance method, signaling that the\nutterance has been completed,\nand for each\ncomponent to clear their internal states.\nThe\nparse incremental method takes the\nIU\nfrom the calling input (e.g., ASR), and appends\nit to a list of previous IUs being stored in the\nMessage object. After the IU has been added to\nthe Message, the IncrementalInterpreter\ncalls each component’s process method, where\nthey can operate on the newest IU.\nThis was\nintentionally designed to be generalizable, so that\nfuture incremental components can use different\nformats or edit types for their respective\nIU\nframework implementation.\n3.2\nIncremental Intent Recognizer\nComponents\nSimple Incremental Update Model\nWith the\nincremental framework in place, we further devel-\noped a sample incremental component to test the\nfunctionality of our changes. For this, we used\nthe Simple Incremental Update Model (SIUM)\ndescribed in (Kennington and Schlangen, 2017).\nThis model is a generative factored joint distri-\nbution, which uses a simple Bayesian update as\nnew words are added. At each IU, a distribution of\nintents and entities are generated with conﬁdence\nscores, and the intent can be classiﬁed at each step\nas the output with the highest conﬁdence value.\nEntities on the other hand, can be extracted if their\nconﬁdence exceeds a predetermined threshold.\nRestart-Incremental TensorFlow Embeddings\nFollowing Khouzaimi et al. (2014)), we incremen-\ntalizaed RASA’s existing Tensorﬂow Embedding\ncomponent for intent recognition as an incremen-\ntal component. The pipeline consists of a whites-\npace tokenizer, scikit-learn Conditional Random\nField (CRF) entity extractor, Bag-of-Words featur-\nizer, and lastly, a TensorFlow Neural Network for\nintent classiﬁcation. To start with incrementaliz-\ning, we modiﬁed the whitespace tokenizer to work\non word-level increments, rather than the entire ut-\nterance. For the CRF entity extractor, we modiﬁed\nit to update the entities up to that point in the utter-\nance with each process call, and then modiﬁed the\nBag-of-Words featurizer to update its embeddings\nwith each process call by vectorizing the individ-\nual word in the IU, and summing that vector with\nthe existing embeddings. At each word IU incre-\nment, we treat the entire utterance preﬁx to that\nImplementation\nIntent F1-Score\nEntities F1-Score\nTensorﬂow (non-incremental)\n0.93\n0.86\nTensorﬂow (restart-incremental)\n0.93\n0.85\nSIUM (non-incremental)\n0.37\n0.34\nSIUM (update-incremental)\n0.36\n0.34\nTable 1: Test data results of non-incremental TensorFlow, restart-incremental TensorFlow, non-incremental SIUM,\nand update-incremental SIUM.\npoint as a full utterance as input to the Tensorﬂow\nEmbeddings component, which returns a distribu-\ntion over intents. This process is repeated until all\nwords in the utterance have been added to the pre-\nﬁx. In this way, the component differs from SIUM\nin that it doesn’t update its internal state; rather, it\ntreats each preﬁx as a full utterance (i.e., so-called\nrestart-incrementality).\n4\nExperiment\nIn this section, we explain a simple experiment we\nconducted to evaluate our work in incrementaliz-\ning RASA by using the update-incremental SIUM\nand\nrestart-incremental\ntensorﬂow-embedding\nmodules in a known NLU task.\n4.1\nData, Task, Metrics\nTo evaluate the performance of our approach,\nwe used a subset of the SNIPS (Coucke et al.,\n2018) dataset,\nwhich is readily available in\nRASA NLU format. Our training data consisted\nof 700 utterances,\nacross 7 different intents\n(AddToPlaylist,\nBookRestaurant,\nGetWeather,\nPlayMusic,\nRateBook,\nSearchCreativeWork,\nand\nSearchScreeningEvent).\nIn order to test\nour implementation of incremental components,\nwe initially benchmarked their non-incremental\ncounterparts, and used that as a baseline for the\nincremental versions (to treat the SIUM compo-\nnent as non-incremental, we simply applied all\nwords in each utterance to it and obtained the\ndistribution over intents after each full utterance\nhad been processed).\nWe use accuracy of intent and entity recog-\nnition as our task and metric.\nTo evaluate the\ncomponents worked as intended, we then used\nthe IncrementalInterpreter to parse the\nmessages as individual IUs. To ensure REVOKE\nworked as intended, we injected random incorrect\nwords at a rate of 40%, followed by subsequent\nrevokes, ensuring that an ADD followed by a re-\nvoke resulted in the same output as if the incor-\nrect word had never been added. While we imple-\nmented both an update-incremental and a restart-\nincremental RASA NLU component, the results\nof the two cannot be directly compared for accu-\nracy as the underlying models differ greatly (i.e.,\nSIUM is generative, whereas Tensorﬂow Embed-\nding is a discriminative neural network; more-\nover, SIUM was designed to work as a reference\nresolution component to physical objects, not ab-\nstract intents), nor are these results conducive to\nan argument of update- vs.\nrestart-incremental\napproaches, as the underlying architecture of the\nmodels vary greatly.\n4.2\nResults\nThe results of our evaluation can be found in Table\n1. These results show that our incremental imple-\nmentation works as intended, as the incremental\nand non-incremental version of each component\nyieled the same results.\nWhile there is a small\nvariation between the F1 scores between the non-\nincremental and incremental components, 1% is\nwell within a reasonable tolerance as there is some\nrandomness in training the underlying model.\n5\nConclusion\nRASA NLU is a useful and well-evaluated toolkit\nfor developing NLU components in SDS and chat-\nbot systems. We extended RASA by adding provi-\nsions for incremental processing generally, and we\nimplemented two components for intent recogni-\ntion that used update- and restart-incremental ap-\nproaches.\nOur results show that the incremen-\ntalizing worked as expected.\nFor ongoing and\nfuture work, we plan on developing an update-\nincremental counterpart to the Tensorﬂow Embed-\ndings component that uses a recurrent neural net-\nwork to maintain the state. We will further evalu-\nate our work with incremental ASR in live dialogue\ntasks. We will make our code available upon ac-\nceptance of this publication.\nReferences\nGregory Aist, James Allen, Ellen Campana, Lucian\nGalescu, Carlos Gallo, Scott Stoness, Mary Swift,\nand Michael Tanenhaus. 2006.\nSoftware archi-\ntectures for incremental understanding of human\nspeech.\nIn Proceedings of CSLP, pages 1922—-\n1925.\nGregory Aist, James Allen, Ellen Campana, Car-\nlos Gomez Gallo, Scott Stoness, and Mary Swift.\n2007.\nIncremental understanding in human-\ncomputer dialogue and experimental evidence for\nadvantages over nonincremental methods. In Prag-\nmatics, volume 1, pages 149–154, Trento, Italy.\nLayla El Asri, Romain Laroche, Olivier Pietquin, and\nHatim Khouzaimi. 2014.\nNASTIA: Negotiating\nAppointment Setting Interface. In Proceedings of\nLREC, pages 266–271.\nTimo Baumann. 2013.\nIncremental spoken dialogue\nprocessing:\nArchitecture and lower-level compo-\nnents. Ph.D. thesis, Bielefeld University.\nTimo Baumann and David Schlangen. 2012. The In-\nproTK 2012 release. In NAACL-HLT Workshop on\nFuture directions and needs in the Spoken Dialog\nCommunity: Tools and Data (SDCTD 2012), pages\n29–32.\nTom\nBocklisch,\nRasa\nTom@rasa\nAi,\nJoey\nFaulkner, Rasa Joey@rasa Ai, Nick Pawlowski,\nRasa\nNick@rasa\nAi,\nAlan\nNichol,\nand\nRasa Alan@rasa Ai. 2017. Rasa: Open Source Lan-\nguage Understanding and Dialogue Management.\nIn Proceedings of the 31st Conference on Neural\nInformation Processing Systems, Long Beach, CA.\nDaniel Braun, Adrian Hernandez Mendez, Florian\nMatthes, and Manfred Langen. 2017.\nEvaluating\nNatural Language Understanding Services for Con-\nversational Question Answering Systems. In Pro-\nceedings of the SIGDIAL 2017 Conference, pages\n174–185.\nHerbert H Clark. 1996. Using Language. Cambridge\nUniversity Press.\nAlice Coucke, Alaa Saade, Adrien Ball, Th´eodore\nBluche, Alexandre Caulier, David Leroy, Cl´ement\nDoumouro, Thibault Gisselbrecht, Francesco Calt-\nagirone, Thibaut Lavril, Ma¨el Primet, and Joseph\nDureau. 2018. Snips Voice Platform: an embedded\nSpoken Language Understanding system for private-\nby-design voice interfaces. arXiv.\nJens Edlund, Joakim Gustafson, Mattias Heldner, and\nAnna Hjalmarsson. 2008. Towards human-like spo-\nken dialogue systems.\nSpeech Communication,\n50(8-9):630–645.\nC. Kennington and D. Schlangen. 2017. A simple gen-\nerative model of incremental reference resolution for\nsituated dialogue. Computer Speech and Language,\n41.\nCasey Kennington and David Schlangen. 2016. Sup-\nporting Spoken Assistant Systems with a Graphical\nUser Interface that Signals Incremental Understand-\ning and Prediction State. In Proceedings of the 17th\nAnnual Meeting of the Special Interest Group on\nDiscourse and Dialogue, pages 242–251, Los An-\ngeles. Association for Computational Linguistics.\nHatim Khouzaimi, Romain Laroche, and Fabrice\nLefevre. 2014. An easy method to make dialogue\nsystems incremental. In Proceedings of the 15th An-\nnual Meeting of the Special Interest Group on Dis-\ncourse and Dialogue (SIGDIAL), June, pages 98–\n107, Philadelphia, PA, U.S.A. Association for Com-\nputational Linguistics.\nXingkun Liu, Arash Eshghi, Pawel Swietojanski, and\nVerena Rieser. 2019. Benchmarking Natural Lan-\nguage Understanding Services for building Conver-\nsational Agents.\nDavid Schlangen and Gabriel Skantze. 2011. A Gen-\neral, Abstract Model of Incremental Dialogue Pro-\ncessing. In Dialogue & Discourse, volume 2, pages\n83–111.\nGabriel Skantze and David Schlangen. 2009.\nIncre-\nmental dialogue processing in a micro-domain. Pro-\nceedings of the 12th Conference of the European\nChapter of the Association for Computational Lin-\nguistics on EACL 09, (April):745–753.\nMichael Tanenhaus, Michael Spivey-Knowlton, Kath-\nleen Eberhard, and Julie Sedivy. 1995. Integration\nof visual and linguistic information in spoken lan-\nguage comprehension.\nScience (New York, N.Y.),\n268(5217):1632–1634.\nA\nAppendix\nlanguage: \"en\"\npipeline:\n- name: \"intent_featurizer_count_vectors\"\n- name: \"intent_..._tensorflow_embedding\"\nintent_tokenization_flag: true\nintent_split_symbol: \"+\"\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2019-07-11",
  "updated": "2019-07-11"
}