{
  "id": "http://arxiv.org/abs/1907.06396v1",
  "title": "A Dual Memory Structure for Efficient Use of Replay Memory in Deep Reinforcement Learning",
  "authors": [
    "Wonshick Ko",
    "Dong Eui Chang"
  ],
  "abstract": "In this paper, we propose a dual memory structure for reinforcement learning\nalgorithms with replay memory. The dual memory consists of a main memory that\nstores various data and a cache memory that manages the data and trains the\nreinforcement learning agent efficiently. Experimental results show that the\ndual memory structure achieves higher training and test scores than the\nconventional single memory structure in three selected environments of OpenAI\nGym. This implies that the dual memory structure enables better and more\nefficient training than the single memory structure.",
  "text": "2019 19th International Conference on Control, Automation and Systems (ICCAS 2019)\nOct. 15∼18, 2019; ICC Jeju, Jeju, Korea\nA Dual Memory Structure for Efﬁcient Use of Replay Memory in\nDeep Reinforcement Learning\nWonshick Ko1 and Dong Eui Chang1∗\n1School of Electrical Engineering, KAIST,\nDaejeon, 34141, Korea (kows93@kaist.ac.kr, dechang@kaist.ac.kr) ∗Corresponding author\nAbstract: In this paper, we propose a dual memory structure for reinforcement learning algorithms with replay memory.\nThe dual memory consists of a main memory that stores various data and a cache memory that manages the data and trains\nthe reinforcement learning agent efﬁciently. Experimental results show that the dual memory structure achieves higher\ntraining and test scores than the conventional single memory structure in three selected environments of OpenAI Gym.\nThis implies that the dual memory structure enables better and more efﬁcient training than the single memory structure.\nKeywords: Reinforcement Learning, Replay Memory, Prioritized Experience Replay (PER), Deep Q-Network (DQN)\n1. INTRODUCTION\nReplay memory plays an important role in stable\nlearning and fast convergence of deep reinforcement\nlearning algorithms [1] that are methods of approximat-\ning a value or a policy function using deep neural net-\nworks [2].\nThe study of replay memory in reinforce-\nment learning started from [3] and played a major role in\ntraining reinforcement learning agents to play Atari 2600\ngames with a Deep Q-Network (DQN) [4]. In addition,\nreplay memory is used in other off-policy reinforcement\nlearning algorithms such as DDPG [5] and ACER [6]. In\n[7], after analyzing the importance of the data in the re-\nplay memory, a probability distribution is assigned to en-\nable efﬁcient learning through prioritization based on the\nimportance. On the other hand, [8] proposed a method for\nstochastically eliminating data based on the importance\nin replay memory. However, each of the priority-based\nlearning method and the memory management method\ngreatly increases the cost of computing the importance\nof all data as the memory capacity increases. In general,\nthis problem may occur when deep reinforcement learn-\ning algorithms with replay memory set the memory size\nvery large.\nTo handle this problem, we propose a new memory\nstructure for efﬁcient use of replay memory in deep rein-\nforcement learning. The proposed memory structure con-\nsists of a main memory and a cache memory. The main\nmemory is created to store various data, and the cache\nmemory is created to efﬁciently manage the data and train\nthe agent based on the importance of data. We compare\nthe training performance of the proposed structure with\nthat of the conventional single memory structure in the\nOpenAI Gym [9] environment, and verify the effect of\nthe proposed structure.\n2. PROPOSED STRUCTURE\n2.1 Dual Memory Structure\nThe proposed memory structure is divided into two\nparts. One part has a large capacity and mainly stores\nvarious data, which is called main memory in this pa-\nFig. 1: Proposed dual memory structure.\nper. The other has a much smaller capacity relative to\nthe main memory and is used to efﬁciently manage the\ndata and train the agent. This memory shall be simply\ncalled cache memory. As shown in Fig. 1, this mem-\nory structure ﬁrst stores the new data obtained by inter-\nacting with the environment in the main memory, and\ncopies a certain portion of the main memory to the cache\nmemory for learning. The reason why the capacity of\nthe cache memory is made smaller is to reduce the cost\nof computing the importance of each data, thereby efﬁ-\nciently performing both prioritized data removal and pri-\noritized training based on the importance. So, we remove\nthe cache memory data using the Prioritized Stochastic\nMemory Management (PSMM) method [8] and sample\nthe training data using the Prioritized Experience Replay\n(PER) method [7]. If we use a single memory that is not\nseparated by their role, the larger the size of memory, the\nhigher the computation load. On the other hand, if the\nimportance of data is calculated only in a cache mem-\nory with a small capacity, much faster calculations can\nbe performed\n2.2 Cache Data Selection\nA method for selecting data to be transferred from the\nmain memory to the cache memory is as follows. First, it\narXiv:1907.06396v1  [cs.LG]  15 Jul 2019\nFig. 2: Cache data selection method.\nis assumed that data in the main memory with the capac-\nity m is arranged in the order of stored time. That is, if\nthe i-th data in the main memory is denoted by D(i), the\ndataset Mm of the main memory can be expressed as\nMm =\n\b\nD(1), D(2), · · · , D(m−1), D(m)\n\t\n.\n(1)\nIn Eq. (1), the smaller the index of of the data D, the\nolder the data; for example, D(1) is the oldest data in\nMm. Next, the entire main memory is divided into t re-\ngion according to stored time. In other words, we obtain\nt disjoint subsets from Mm where each subset has m\nt el-\nements, where m is assumed to be divisible by t. Let Gj\ndenote the subset j with j = 1, · · · , t. Then, the ﬁrst\nsubset G1 and t-th subset Gt can be expressed as\nG1 =\nn\nD(1), D(2), · · · , D( m\nt −1), D( m\nt )\no\n,\nGt =\nn\nD( (t−1)m\nt\n+1), D( (t−1)m\nt\n+2), · · · , D(m−1), D(m)\no\n.\nSo, the j-th subset Gj can be expressed as\nGj =\nn\nD( (j−1)m\nt\n+1), D( (j−1)m\nt\n+2), · · · , D( jm\nt )\no\n.\nIn fact, the number of the divided region of the entire\nmain memory is equal to the number of data copied\nfrom main memory to cache memory.\nAfter divid-\ning the entire dataset with t disjoint subsets as above,\nwe randomly sample the data one by one from each\nsubset.\nBy doing so, we obtain a sampled dataset\n\b\nd(1), d(2), · · · , d(t−1), d(t)\n\t\nfrom the main memory\nwhere d(j) is a randomly sampled data from Gj. The\nreason why we divide the main memory into some region\naccording to time and sample the data from each of the\nregion is to obtain data in various time intervals.\nIn addition, new data from the environment is also\ncopied to the cache memory. If a reinforcement learn-\ning training process is not executed at every time step\nbut at every n time steps, we copy to the cache mem-\nory n new data generated between training steps.\nIn\nFig. 2, new data from the environment is denoted by\n\b\nd(t+1), d(t+2), · · · , d(t+n−1), d(t+n)\n\t\n.\nIn summary, if we train the agent at every n time steps,\nt data from the main memory and n new data from the en-\nvironment are obtained and a total of t+n data are copied\nto the cache memory at every training step; refer to Table\n1 for summary of the selected cache data. However, if\nthe cache memory is already full, we remove t + n data\nfrom the cache memory using the Prioritized Stochastic\nMemory Management method [8] just before we copy the\nselected cache data to the cache memory.\n3. EXPERIMENTS\n3.1 Experimental Setup\nIn order to compare the performance of the proposed\ndual memory structure with that of the single memory\nstructure, we train reinforcement learning agents for three\ndifference cases: 1 Single memory structure with PER,\n2 Single memory structure with PSMM, and 3 Dual\nmemory structure with PER and PSMM. For the single\nmemory of 1 and 2 , the memory is set to store 10,000\nreinforcement learning data units, and for the dual mem-\nory of 3 the main memory stores 8,000 data units and\nthe cache memory stores 2,000 units of data.\nThe deep reinforcement learning algorithm used for\nthe experiment is DQN [4], and the importance of the\ndata is determined by the absolute value of the TD error;\nrefer to [7] for more detail on the importance of data in\nDQN. In fact, the PSMM method in [8] was proposed for\nthe Actor-Critic method [10], [11], so the Return as well\nas the TD error affected the importance of the data. In\nthese experiments, however, we consider only the TD er-\nror as the importance of the data. Each agent is trained\nin Assault-v0, SpaceInvaders-v0, and KungFuMaster-v0\nof OpenAI Gym [9] using the high-quality implemented\nalgorithm provided by OpenAI Baseline [12] for a total\nof 1 million steps.\n3.2 Results\nFigures 3−5 show the mean training score for past\n100 consecutive episodes and the mean test score for 10\ntest episodes during the training process in Assault-v0,\nSpaceInvaders-v0, and KungFuMaster-v0. Here, the blue\nline labeled PER indicates the result for the Single Mem-\nory Structure with PER, the red line labeled PSMM in-\ndicates the result for the Single Memory Structure with\nPSMM, and the green line labeled DMS indicates the\nTable 1: Selected cache data.\nAcquisition method\nSource\n# of data\nDataset\nRandomly sample from each subset G\nMain Memory\nt\n\b\nd(1), d(2), · · · , d(t−1), d(t)\n\t\nGet a new data by interacting with the environment\nEnvironment\nn\n\b\nd(t+1), d(t+2), · · · , d(t+n−1), d(t+n)\n\t\n(a) mean training score for past 100 consecutive episodes\n(b) mean test score for 10 test episodes\nFig. 3: The result on Assault-v0.\n(a) mean training score for past 100 consecutive episodes\n(b) mean test score for 10 test episodes\nFig. 4: The result on SpaceInvaders-v0.\n(a) mean training score for past 100 consecutive episodes\n(b) mean test score for 10 test episodes\nFig. 5: The result on KungFuMaster-v0.\nDual Memory Structure with PER and PSMM. In Figs.\n3−5, we can see that the DMS method has the highest\nmean training and test scores in all the three environ-\nments. In particular, in Fig. 3, which is the result on\nAssault-v0, the mean test score of the DMS method is\nabout ﬁve times higher than those of the two single mem-\nory methods.\nFrom the above experimental results, it can be seen\nthat the proposed dual memory structure designed to ef-\nﬁciently use both PER and PSMM shows higher training\nperformance than the single memory structure with either\nPER or PSMM. This means that the cache memory with\na small capacity can efﬁciently use the PER method and\nthe PSMM method to enhance the training performance\nof the reinforcement learning agent, and the main mem-\nory plays a role of maintaining diversity of data which\nmay be lacking in the cache memory.\n4. CONCLUSION\nIn this paper, we have proposed a dual memory struc-\nture to improve the performance of the reinforcement\nlearning algorithm using replay memory. The memory\nstructure is divided into a large-capacity main memory\npart for storing various data and a small-capacity cache\nmemory part for efﬁcient training. This research is an\nimprovement on the previous study on the efﬁcient usage\nof replay memory in reinforcement learning with a sin-\ngle memory. The experimental results show that the pro-\nposed dual memory structure with the Prioritized Experi-\nence Replay (PER) method and the Prioritized Stochastic\nMemory Management (PSMM) method achieves higher\ntraining and test scores than a single memory structure.\nThis implies that the memory structure divided into two\nparts enables better and more efﬁcient training than the\nconventional single memory structure. However, in this\npaper the dual memory structure has been applied only\nto DQN that is an algorithm for learning the optimal dis-\ncrete action. In future work, we will study the possibility\nof generalization of this memory structure to the contin-\nuous action case, by applying it to DDPG and ACER that\nare reinforcement learning algorithms for continuous ac-\ntion spaces.\nACKNOWLEDGEMENT\nThis research has been in part supported by the ICT\nR&D program of MSIP/IITP [2016-0-00563, Research\non Adaptive Machine Learning Technology Development\nfor Intelligent Autonomous Digital Companion].\nREFERENCES\n[1]\nR. Liu and J. Zou, “The effects of memory replay in\nreinforcement learning,” in 2018 56th Annual Aller-\nton Conference on Communication, Control, and\nComputing (Allerton), pp. 478–485, IEEE, 2018.\n[2]\nA. L. Caterini and D. E. Chang, Deep Neural Net-\nworks in a Mathematical Framework.\nSpringer,\n2018.\n[3]\nL.-J. Lin, “Reinforcement learning for robots using\nneural networks,” tech. rep., Carnegie-Mellon Univ\nPittsburgh PA School of Computer Science, 1993.\n[4]\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu,\nJ. Veness, M. G. Bellemare, A. Graves, M. Ried-\nmiller, A. K. Fidjeland, G. Ostrovski, et al.,\n“Human-level control through deep reinforcement\nlearning,” Nature, vol. 518, no. 7540, p. 529, 2015.\n[5]\nT. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess,\nT. Erez, Y. Tassa, D. Silver, and D. Wierstra, “Con-\ntinuous control with deep reinforcement learning,”\narXiv preprint arXiv:1509.02971, 2015.\n[6]\nZ. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos,\nK. Kavukcuoglu, and N. de Freitas, “Sample ef-\nﬁcient actor-critic with experience replay,” arXiv\npreprint arXiv:1611.01224, 2016.\n[7]\nT. Schaul, J. Quan, I. Antonoglou, and D. Sil-\nver, “Prioritized experience replay,” arXiv preprint\narXiv:1511.05952, 2015.\n[8]\nT. Kwon and D. E. Chang, “Prioritized stochas-\ntic memory management for enhanced reinforce-\nment learning,” in 2018 IEEE International Confer-\nence on Consumer Electronics-Asia (ICCE-Asia),\npp. 206–212, IEEE, 2018.\n[9]\nG. Brockman, V. Cheung, L. Pettersson, J. Schnei-\nder, J. Schulman, J. Tang, and W. Zaremba, “Openai\ngym,” arXiv preprint arXiv:1606.01540, 2016.\n[10] R. S. Sutton, D. A. McAllester, S. P. Singh, and\nY. Mansour, “Policy gradient methods for reinforce-\nment learning with function approximation,” in Ad-\nvances in neural information processing systems,\npp. 1057–1063, 2000.\n[11] J. Peters and S. Schaal, “Natural actor-critic,” Neu-\nrocomputing, vol. 71, no. 7-9, pp. 1180–1190, 2008.\n[12] P. Dhariwal, C. Hesse, O. Klimov, A. Nichol,\nM. Plappert, A. Radford, J. Schulman, S. Sidor,\nY.\nWu,\nand\nP.\nZhokhov,\n“Openai\nbase-\nlines.”\nhttps://github.com/openai/\nbaselines, 2017.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-07-15",
  "updated": "2019-07-15"
}