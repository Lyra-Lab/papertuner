{
  "id": "http://arxiv.org/abs/1907.06572v1",
  "title": "Deep network as memory space: complexity, generalization, disentangled representation and interpretability",
  "authors": [
    "X. Dong",
    "L. Zhou"
  ],
  "abstract": "By bridging deep networks and physics, the programme of geometrization of\ndeep networks was proposed as a framework for the interpretability of deep\nlearning systems. Following this programme we can apply two key ideas of\nphysics, the geometrization of physics and the least action principle, on deep\nnetworks and deliver a new picture of deep networks: deep networks as memory\nspace of information, where the capacity, robustness and efficiency of the\nmemory are closely related with the complexity, generalization and\ndisentanglement of deep networks. The key components of this understanding\ninclude:(1) a Fisher metric based formulation of the network complexity; (2)the\nleast action (complexity=action) principle on deep networks and (3)the geometry\nbuilt on deep network configurations. We will show how this picture will bring\nus a new understanding of the interpretability of deep learning systems.",
  "text": "1\nDeep network as memory space: complexity, generalization,\ndisentangled representation and interpretability\nXiao Dong, Ling Zhou\nFaculty of Computer Science and Engineering, Southeast University, Nanjing, China\nBy bridging deep networks and physics, the programme of geometrization of deep networks was proposed as a framework for the\ninterpretability of deep learning systems. Following this programme we can apply two key ideas of physics, the geometrization of\nphysics and the least action principle, on deep networks and deliver a new picture of deep networks: deep networks as memory space\nof information, where the capacity, robustness and efﬁciency of the memory are closely related with the complexity, generalization\nand disentanglement of deep networks. The key components of this understanding include:(1) a Fisher metric based formulation\nof the network complexity; (2)the least action (complexity=action) principle on deep networks and (3)the geometry built on deep\nnetwork conﬁgurations. We will show how this picture will bring us a new understanding of the interpretability of deep learning\nsystems.\nIndex Terms—deep networks, geometrization, interpretability, geometrization of physics, quantum information, Riemannian\ngeometry, complexity\nI. INTRODUCTION\nTill now the interpretability of deep learning systems re-\nmains the dark cloud above the sky of deep learning. Although\nwe have developed useful tools and methods to understand\ndeep learning systems, for example from the ordinary dif-\nferential equation and the optimal control perspectives, still\nwe are lacking of a general framework to describe and\nunderstand deep learning systems. For the interpretability of\ndeep learning systems, not only we need to ﬁnd out how deep\nnetworks process data to accomplish a speciﬁc task, also we\nneed to answer why deep networks work like that. To make\nthis point clearer, we can ﬁrst examine some examples in\nphysics. We all know spacetime is described by Einstein’s\ngeneral relativity, but till today we can not understand why\nspacetime obeys the gravitational equation. We know exactly\nhow electrons interact with EM waves, but we still can not\nanswer why we have electrons and EM waves. We believe the\ngoal of the interpretability of deep networks should also be to\nanswer those why questions instead of those how questions.\nSo we need a profound and primitive principle to derive a\nmathematical framework to interpret deep networks and deep\nlearning systems, or in another word, we need a programme for\nthe interpretability of deep learning systems. From this point of\nview, the ordinary differential equation (ODE) or the optimal\ncontrol perspective are more tools rather than the programme\nwe ask for. The goal of this paper is to give our ﬁrst attempt\nto construct the programme.\nIn our former work we proposed to understand the inter-\npretability of deep learning systems by geometrization[1][2].\nObviously this idea is adapted from the geometrization of\nphysics. Here we formally formulate our idea under the name\nof the programme of geometrization of deep networks. We\nexpect it can play a similar key role in the ﬁeld of deep\nlearning as its counterpart does in physics.\nThe motivation of the proposed framework stems from our\nbelief that the universal effectiveness of deep learning in\ndifferent ﬁelds must have a simple fundamental origin: deep\nnetwork is physical. The meaning of this slogan is two-folds.\nOn one hand, deep networks are effective descriptors for our\nphysical world so that all physical systems can be effectively\ndescribed by deep networks. On the other hand, the physical\nworld emerges from deep networks, to be more speciﬁc,\nfrom the deep networks of quantum computation. For every\nphysical system, there exists a correspondent deep network\nto generate and describe it. While for every deep network,\nit also possesses a physical picture. So in this programme,\nphysics and deep networks are intertwined and deep networks\nof quantum computation provide a constructive realization of\nWheeler’s idea It from qubits, which claims that the physical\nworld emerges from information. With this belief in mind, the\ngoal of our programme of geometrization of deep networks is\nto bridge the concepts in physics and deep learning systems so\nthat both ﬁelds can beneﬁt from this programme. This is to say,\nin one direction, the idea that the physical world is emergent\nfrom quantum computation networks can help us to understand\nthe fundamental structure of our universe such as spacetime,\nmaterial and their interactions. In the other direction, key\nconcepts and methods in physics such as the least action\nprinciples and geometrization of physics can be transferred to\nthe ﬁeld of deep learning to provide an interpretation of deep\nnetworks. We hope such a bi-directional win-win pattern can\nhelp to answer those why questions in both ﬁelds and achieve\na better interpretability of our world.\nIndeed in the physics side, especially in the ﬁeld of quantum\ninformation and quantum gravity, there are already some\ninteresting results by considering how our physical world can\nemerge from quantum information processing procedure. Con-\ncepts such as computation, information and quantum complex-\nity are now playing a more important role in understanding the\nfundamental rules of physics[3][4][5][6][7]. One interesting\nobservation is that there exist a geometry/information duality,\nwhich means that spacetime emerges from the information of\na physical system and the emergent spacetime can be regarded\narXiv:1907.06572v1  [cs.LG]  12 Jul 2019\n2\nas a memory space for that information[8][9]. In this paper,\nwe will try to adapt this idea to deep networks so that deep\nnetworks can be understood as a memory device of the infor-\nmation represented by data, or, the geometry of a deep network\nencodes data information. This idea can be mathematically\nformulated by combining a Fisher information metric based\ndeﬁnition of deep network complexity and the least action\n(complexity) principle on the conﬁguration of deep networks.\nWe will show that deep network as the memory of information,\nits capacity, robustness and efﬁciency are closely related with\nthe complexity, generalization and disentangled representation\nof the deep network. We claim that these observations can\nbring us new understanding of the interpretability of deep\nlearning systems.\nThe remaining part of this paper will be arranged as\nfollows. We will ﬁrst give a brief summary of the key idea of\nour programme of geometrization of deep networks and the\ngeometry/information duality in physics. Then we will explain\nhow this idea can help us to understand the key concepts in\ndeep learning systems such as the network complexity, gen-\neralization performance and disentangled representation. For\nthe same perspective, we will also give a discussion of some\ninteresting recent works, including the Lottery hypothesis, the\nweight agnostic neural network, the disentangle representation\nand the information based interpretation of NLP systems.\nII. GEOMETRY/INFORMATION DUALITY IN PHYSICS\nGeometrization of physics is one of the most profound\nand the most successful idea in understanding the rules of\nour physical world in human history. But why can our world\nbe geometrized? In the last decade, we saw a new trend to\ncombine geometrization and quantum information processing\nto scratch a complete new picture of our world. Basically this\nis to regard our world, including spacetime, material and their\ninteractions, as emergent phenomena from a complex deep\nnetwork of quantum computation, where the deep network\naims to represent or generate the quantum state or the in-\nformation of the universe. This is the key idea of the so called\ncomputational universe. From this point of view, our world is\nbuilt from deep networks and the geometric structure of the\nphysical world emerges from the geometric structure of the\nunderlying deep networks. So the geometrization of physics is\nessentially the geometrization of the underlying quantum deep\nnetworks. The success of geometrization of physics is a hint\nthat geometrization may also be the key to understand deep\nnetworks. This is the motivation of our proposed programme\nof geometrization of deep networks. We wish this can serve as\na general framework for the interpretability of deep learning\nsystems.\nIn computational universe, there is a deep connection be-\ntween the information of the universe and the geometrization\nof the universe, which are connected by a complex quan-\ntum computation deep network[8][9]. In this section we will\nexplore the duality between geometric structures and infor-\nmation, which means that in our physical world, geometric\nstructures emerge from information and geometric structures\nencode information.\nThe basic logic of this geometry/information duality can be\nsummarized as follows.\n1. Firstly, for a given quantum system, its quantum state |ψ⟩\nhas an information pattern, which includes, for example, the\nentanglement entropy and correlation among subsystems. 2.\nWe may also have different descriptions D(|ψ⟩) of this quan-\ntum state |ψ⟩, for example either by a tensor network[9][10]\nor by a quantum circuit[11] which can be used to generate |ψ⟩\nfrom a simple reference quantum state such as a product state.\n3. By assigning a proper deﬁnition of complexity C(D(|ψ⟩))\nof the descriptions of |ψ⟩, we can apply the least action\nprinciple on the descriptions to ﬁnd the optimal description\nof |ψ⟩, where the action is given by the complexity following\nSusskind’s idea that Complexity=Action[12]. 4. And ﬁnally,\nthe optimal descriptor of |ψ⟩holds a geometry that encodes\nthe information pattern of |ψ⟩[9], which can be called the\nmemory space.\nIn the above mentioned steps, we in fact encounter two\ngeometric structures.\nThe ﬁrst geometric structure is the Riemannian manifold of\nall descriptions of all quantum states. A description D(|ψ⟩)\nis a point on this manifold and its complexity C(D(|ψ⟩)) is\ndeﬁned as the geodesic distance between C(D(|ψ⟩)) and the\ntrivial description I. We call it the geometry of complexity\nGCom. One example of this Riemannian geometry is the\ngeometry of the quantum circuit complexity of quantum com-\nputation deﬁned in [11]. In our former work on the geometry\nof deep networks[13][1], we have shown the Riemannian\nstructure of quantum computational complexity already pro-\nvides a geometric picture for deep networks, which is also\nrelated with geometric mechanics as in the diffeomorphic\nimage registration problem[14]. Here we point out that this\ngeometric structure is in fact compatible with the recent ODE\nand optimal control based picture of deep networks since\nthe diffeomorphic image registration problem is essentially\nbe formulated as both an ordinary differential equation and\nan optimal control procedure[15]. From this observation, the\nODE or optimal control picture of deep networks also explores\nthe geometrization of deep networks.\nBesides the Riemannian geometry of quantum complexity,\nthe optimal descriptor of a quantum state may also generate a\ngeometric structure called the geometry of information GInf.\nFor example a tensor network as a descriptor of quantum states\nmay have a physical geometry or a holographic geometry as\nin [9]. Another example is the theory of quantum computation\nbased gravity[3][8], where spacetime geometry emerges from\nhistories of quantum computation. The key concept of the\nsecond geometric structure is that the emergent geometry can\nencode the information pattern of the quantum state |ψ⟩. In\nanother word, the geometric structure is a memory to save\nthe quantum information of |ψ⟩. For example, MERA can\nbe regarded as a discrete realizations of the AdS/CFT duality\nand its holographic geometry encodes the information pattern\nof the boundary quantum states such as the area law of\nentanglement entropy and the correlation length[9][10][4].\nAs a summary, in physics, we have now a correspondence\nor duality between information and geometry. The duality is\nestablished by a least action principle where the action is given\n3\nby the complexity of the quantum state, or equivalently the\ncomplexity of the optimal descriptor of the quantum state.\nGiven this picture, a set of straight forward questions pop\nup. What’s the relationship between these two geometric\nstructures? If the second geometry GInf is a memory to\nencode information, how can we evaluate its capacity, ro-\nbustness/reliability and efﬁciency? Can we somehow unify\nthese two geometric structures? Because these two geometric\nstructures are related with the metric of quantum complexity\nand information respectively, a natural guess is that, they might\nbe uniﬁed by the Fisher information metric. In fact, it seems\nthey do! This means that in GCom the proper complexity\nmetric should be the information metric and it can also be\nused to evaluate the performance of the geometric memory of\ninformation GInf.\nAn intuitive understanding of this point is that information\nmetric evaluates how the information changes with respect\nto the conﬁguration parameters of an information processing\nsystem. And in our discussion of the information/geometry\nduality, all we are interested is just to describe the quantum\nstate |ψ⟩with a quantum information processing system, either\na tensor work or a quantum computation algorithm. In fact\nthe Fisher information metric has been used to derive the\ngravitation equation and the AdS/CFT correspondence. This\nis a strong evidence that information metric may play a key\nrole in understanding deep networks following the programme\nof geometrization of deep networks. We hope this can give a\nricher geometric picture of deep networks and help to clarify\nsome important issues in deep learning systems.\nIII. GEOMETRY/INFORMATION DUALITY IN DEEP\nNETWORKS\nSimilar to the information/geometry duality in physics, we\ncan also build two geometric structures of deep networks and\nexamine the geometry/information duality of deep networks.\nConceptually this is straight forward since as an information\nprocessing system, the deep network of quantum computation\nis more fundamental and its geometry/information duality\ncan be directly applied to our classical deep networks. We\nformalize this procedure as follows.\nFor a given task T with its training data set X, we build\na L-layered deep network with its structure and parameter\nset denoted by a graph G and θ respectively. This gives a\nsimpliﬁed description of deep networks.\nA. Information in deep networks\nTo build the geometry/information duality of deep networks,\nwe need ﬁrst to clarify what’s the information here. Obviously\nthe information is now encoded in data X. Depending on the\ntask T of the deep learning system, the information can be the\ndistribution f(X) of data X if we are building a generative\nmodel for data X, or it can be a pattern classiﬁcation of data X\nif we are working with an image classiﬁcation system. In these\nexamples, the information to be saved in the deep network\nis the different patterns of data or the structure of the error-\ncorrecting code since generator of a GAN is a cascaded error-\ncorrecting code. We can assume the information is represented\nby a set of random variables Y and this information should\nbe recorded by a deep network.\nFig. 1.\nGeometry of deep networks. GCom is a Riemannian manifold of\nthe functions that can be represented by deep network, where the network\nstructure G deﬁnes a submanifold and different network conﬁgurations with\nthe same structure G correspond to different curves on GCom connecting the\ntrivial identity function I and the target function T. Different deep networks\nalso have their correspondent emergent geometric structures GInf, which can\nencode the information of data so that deep networks can be regarded as a\nmemory space of information.\nB. GCom of deep networks\nAs an efﬁcient tool to approximate complex functions, deep\nnetworks can be understood as discretized ordinary differential\nequations. From this ODE perspective of deep networks, each\ndeep network is a state ﬂow determined by the correspon-\ndent ODE and it works as a continuous transformation ﬂow\nthat transforms the data from input to output of the deep\nnetwork. Obviously on the manifold of all functions that\ncan be approximated by deep networks, each deep network\ncorresponds to a continuous curve. This is exactly the same\nas in quantum computation, where a quantum computation\nalgorithm is a continuous quantum evolution path and the\nquantum circuit based realization of a quantum algorithm is a\ndiscretized curve[11]. Another analogue is the diffeomorphic\nimage registration problem, where each image transformation\ntrajectory is a continuous curve. Our former work has shown\nthat these three aforementioned systems have exactly the same\nRiemannian structure. For more analysis of GCom, please refer\nto [1][13].\nWhat’s the role of the deep network structure G and the\nparameter θ here? Obviously, G deﬁnes a submanifold of\nfunctions that can be represented by deep networks since a\ngiven network structure G can only approximate a subset of\nall the functions on this manifold. Different conﬁgurations of\nθ will then give different curves on this manifold determined\nby G. So training a deep network is to ﬁnd a curve on this\nmanifold to reach a target function that can accomplish a\ncertain task.\nThe geometric structure GCom is then the Riemannian\nstructure deﬁned on the manifold of all deep networks, or\nthe manifold of all the functions that can be achieved by deep\nnetworks. The Riemannian metric on this manifold is to deﬁne\nthe arc length of the curves on the manifold, or more precisely,\nan inner product on the tangent space of the manifold. Given\nthis Riemannian metric, we can then deﬁne the complexity of\na deep network, a curve on this manifold, as the length of\nits correspondent curve. The introduction of the Riemannian\n4\nmetric enables us to discriminate different deep networks even\nthey represent exactly the same target function since they may\nhave different complexities. We will see this is a key concept\nfor the interpretability of deep networks.\nHere we would like to clarify two problems.\n(1) How big is the manifold GCom?\nFollowing the universal approximation rule of neural networks,\nsome may think deep networks can approximate any functions.\nBut this is not true. The key property of deep networks is that\nthey are effective descriptors for our physical world. When\nwe are using deep networks to solve problems, we are only\nseeking for efﬁcient solutions. In physics, a generic n-qubit\nquantum system in a state |ψ⟩has a quantum state complexity\nof O(22n) and therefore it can not be efﬁciently described\nor prepared. Only a small subset of simple quantum states is\nphysically realizable and can be described efﬁciently, which\nis called the corner of physical world[16]. Similarly, not all\nfunctions can be efﬁciently approximated by a deep network\nwith a limited size. We can only work with a subset of all\nfunctions, which we can denote as the corner of physical func-\ntions. In quantum computation, we have the same problem.\nWhen we are working on a quantum information processing\nsystem with n qubits, a general algorithm can be described\nby a unitary operator U ∈U(2n). But a general U ∈U(2n)\nhas a complexity of 22n) and we are only interested in the\nalgorithms that can be efﬁciently achieved by a limited number\nN of simple quantum gates, for example a sequence of one\nand two qubit quantum gates with N << 2n. This is exactly\nwe are more interested in deep and narrow networks instead\nof shallow and extremely wide networks. Physics will never\naccept arguments that can not be physically veriﬁed and we\nbelieve this also applies to deep networks.\n(2) Which Riemannian metric should we choose to measure\nthe complexity of deep network?\nIn recent years, complexity is becoming a key concept in\nphysics including quantum information processing, quantum\ngravity and quantum phase transition. Susskind’s Complex-\nity=Action and Complexity=Volume hypothesis turn compu-\ntational complexity into a concrete physical concept with\nphysical correspondence[17][12]. In our former work we also\nproposed to connect quantum complexity with space and time\nstructure. According to the close connection between deep\nnetworks and physics, complexity should also be a key concept\nin deep learning systems.\nIf we agree that the complexity of a deep network is given\nby the length of the correspondent curve on the manifold of the\ncorner of physical functions, then basically the complexity can\nbe deﬁned by either an energy based metric or an information\nbased metric. The energy base metric aims to deﬁne the com-\nplexity by ﬁnding out how much energy should be consumed\nto carry out the information processing task on a deep network.\nThe quantum circuit complexity of Nielsen is one example\nof it. In deep learning systems, complexities deﬁned on the\nnorm of the network parameter θ, for example the 0-norm to\ncount the number of parameters or the 2-norm to compute\nthe energy of the parameters. On the contrary, the information\nbased metric also considers how the network conﬁguration will\ninﬂuence the information representation of the deep network.\nIntuitively it concerns not only how much effort we should pay\nto accomplish the information processing task, but also how\nour effort can inﬂuence the output effectively since different\noperations with the same energy consumption may lead to dif-\nferent effects on the information processing. In physic, Fisher\ninformation metric is commonly used as an information based\nmetric. It’s well known that Fisher information metric provides\na natural Riemannian metric to justify how the parameter of\nan information processing system will inﬂuence information\ndistribution. In fact this information metric has been used to\nexamine the AdS/CFT correspondence and gravity[18]. This\nis a convincing evidence that information metric might be the\noptimal complexity metric for deep network complexity. In\n[19], it was proved that on a closed manifold of dimension\ngreater than one, every smooth weak Riemannian metric on\nthe space of smooth positive probability densities, that is\ninvariant under the action of the diffeomorphism group, is\na multiple of the FisherRao metric. This diffeomorphism\ninvariant property perfectly matches the diffeomorphism in-\nvariance of spacetime. This is highly desirable for both the\ncomplexity of deep networks and the geometry GInf which\nis essentially a relational structure. Besides these, in [20][21]\nFisher information metric was used to analysis the inﬂuence of\ndata, and in [22][23][24] the Fisher information metric based\nnetwork complexity was used to evaluate the generalization\nproperty of deep networks. From all these former works, we\nbelieve Fisher information metric should be the optimal metric\nfor deep network complexity.\nIf the Fisher information metric is chosen to deﬁne network\ncomplexity, it leads to a way to evaluate how the network\nparameter θ will inﬂuence the information encoded in the\nnetwork. Assuming the information saved by the deep network\nis the distribution of a random variable y with a probability\ndistribution p(y; θ), then we have\nDKL ≈1\n2gµν(θ)dθµdθν\nγ(y; θ) = −ln(p(y; θ))\ngµν(θ) =\nZ\np(y; θ)dγ(y; θ)\ndθµ\ndγ(y; θ)\ndθν\ndx =< ∂µγ∂νγ >\n(1)\nwhere DKL is the Kullback-Leibler divergence of two\ndistributions.\nObviously the information of Y is saved in a spacetime\nwith coordinates θ where the distance is measured by the\nFisher metric. The matrix gµν(θ) encodes how robust the\ninformation of y with respect to the network parameters θ.\nAn intuitive picture of this is that, a larger gµν(θ) means the\ninformation encoded in the network is more sensitive to the\nnetwork parameters θµ, θν[18].\nC. GInf of deep networks\nFollowing the same logic of the geometry/information duality\nin physics, we can apply a least complexity principle on the\nnetwork conﬁguration and the network training will result in\na low complexity deep network, on which GInf will emerge.\nIn order to understand the structure of GInf, we need to recap\nhow the GInf can be built in physics.\nThe ﬁrst example is spacetime from quantum computation\n[8]. According to this theory, spacetime emerges from a\n5\ndeep network of quantum computation. This is an example\nof a purely relational theory since spacetime jumps out of\na history of quantum computation without any background\ngeometry. So this is a background independent construction\nof GInf. The building blocks of spacetime are the quantum\noperations so that each quantum gate in the deep network\ncan be regarded as a point with an unknown volume or an\nevent in spacetime. Since we are working with a background\nindependent scenario, all points are ﬂoating in nowhere before\nGInf is constructed. The task of building a geometric structure\nGInf is to embed all points in a geometry so that the volume,\narea and distances between points can be determined. In\nquantum computation based spacetime, the geometry is built\nby a variational principle so that an action is minimized with\nrespect to the geometry[8]. There the action is deﬁned on\nboth the curvature of the geometry and the operations in this\nquantum deep network. This is why the gravitational equation\ncan be reproduced, which connects both the geometry of\nspacetime and materials. We will not further dive into further\ndetails of this procedure, instead we only summarize some\ninteresting observations in this example.\n(1) The geometry is built on the operations of the quantum\ncomputation procedure, or the computation nodes of the deep\nnetwork. The computational nodes can be understood as\nevents in spacetime.\n(2) This is a background independent construction of a\ngeometric structure so that the geometry is completely\ndetermined by the relations between nodes. This is to say,\nwe are working with a completely relational theory. The\ndimension of the emergent geometry is determined by\nthe dimension of the quantum operation. In the quantum\ncomputation based spacetime, the quantum operation is a\n2-qubit gate with a spectrum of dimension of 4. This is why\nwe have a 4-dimensional spacetime.\n(3)\nThe\nresulting\ngeometry,\nthe\nspacetime\nwith\nthe\ngravitational equation, is diffeomorphism invariant.\n(4) The emergent geometry, the curvature of the Riemannian\nstructure, encodes the information of the quantum deep\nnetwork, which is the history of the quantum computation\ncarried out by the deep network.\n(5) The geometry is a result of a statistical average of the\nhistories of all data passing through the quantum computation\nnetwork. This is because depending on the input quantum\nstates, the quantum deep network will have different histories\nof quantum operations. This is the same as in a normal deep\nnetwork, different input data will have different patterns of\ninformation propagation in the deep network so that the\ngeometry GInf is a result of the statistical average of all the\ndata X. This is exactly why the Fisher information metric\ngµν(θ) is deﬁned by a statistical average in 1.\nAnother example of the emergent geometry in physics is\nthe geometry of tensor network states, where tensor networks\nare used to represent different quantum states[9]. Physical or\nholographic geometries can be built on tensor networks and the\ngeometry of a tensor network encodes the information pattern\nof the correspondent quantum states, for example its quantum\nentanglement and correlation between subsystems. For more\ndetails of the geometry of tensor networks, please refer to [9].\nSimilar to the spacetime structure from quantum computation\nexample, we only give a brief summary of the GInf as follows.\n(1) The geometry is built on the tensors of the tensor\nnetworks, which can also be regarded as deep networks.\n(2) This is a background dependent construction of geometric\nstructures so that the emergent geometry depends on the\ndimension of the background geometry.\n(3) The tensor network encodes the information of the\nquantum state in its geometry.\n(4) The geometry of a tensor network is also statistical since\nit encodes the information pattern of a set of quantum states\nin the same complexity class.\nThe aforementioned two different GInfs share some com-\nmon characteristics and also they differ from each other. The\npicture of our GInf of deep networks will be a combination\nof these two pictures.\nBefore we give the picture of GInf of general deep\nnetworks, we would like to have a short discussion of the\ngeometry of spacetime since the aforementioned two special\nGInfs are both spacetime geometries, i.e. the spacetime of our\nuniverse and the holographic spacetime respectively. We will\nsee some of their structural properties discussed here will also\nappear in GInf of deep networks.\nThe ﬁrst question is, in the quantum computation based\nspacetime, even the spectrum of the 2-qubit gate determines\nthe emergent spacetime is four dimensional, why do we\nhave a three dimensional space and a one dimensional time?\nShouldn’t the 4 dimensions of the spectrum of 2-qubit gates\nstand on the same foot? From the quantum information\npoint of view, time is related with the evolution of quantum\ncomplexity[25][26]. The reason that we have a thermodynamic\narrow of time is that statistically the quantum complexity\nof a quantum system can only increase. Similarly, along\nthe computation of the quantum deep network the quantum\ncomplexity of data will statistically increase, so we have\ntime in the deep network of quantum computation. But still\nthis can not explain how the 4 dimensions diverges into a\nthree dimensional space and a one dimension time. From\nthe geometry/information duality perspective, spacetime is a\nmemory to save the information of the quantum deep network,\ni.e. the quantum algorithm carried out by the deep network.\nAt each quantum operation node, the 4 dimensional variables\n(θ1, θ2, θ3, θ4) encode part of the information of the quantum\nalgorithm denoted as y. If we regard the quantum algorithm\nas a random variable, then its distribution can be written\nas p(y; θ1, θ2, θ3, θ4) so that the information of y depends\non (θ1, θ2, θ3, θ4). According to 1, we can easily see that\nthe information of y has different sensitivities on different\nquantum computation nodes. That’s to say, for different 2-\nqubit gates inside the quantum deep network, the variations of\ndifferent gates will have different effects on the information\nof y. From the knowledge of the complexity of quantum\ncomputation[11], statistically the sensitivity smoothly changes\nalong the layers of the quantum deep network so that the\ninformation of y has a higher sensitivity to the nodes at\nthe lower layers. So (θ1, θ2, θ3, θ4) can be normalized as\n6\n(θ1, θ2, θ3, θ4) = s(θ′\n1, θ′\n2, θ′\n3, θ′\n4) with s related with the\nsensitivity of the current node. After this normalization, the\ninformation of y has roughly an equal sensitivity on all the\nnodes except for a variable scaling factor s along the network.\nThe variable scaling factor s can then be regarded as time. Due\nto the renormalization operation, (θ′\n1, θ′\n2, θ′\n3, θ′\n4) is essentially\nonly 3 dimensional, which corresponds to the 3 dimensional\nspace. In fact the observation that time corresponds to the\nsensitivity also appeared in the work of Susskind [], which\naims to address the spacetime structure of black holes.\nAnother related question is about the Planck length lp in\nour spacetime, which is the minimum length that we can\ndistinguish. Why do we have lp? In fact we can give different\npictures about the existence of lp from different perspectives.\nFrom the uncertainty principle of quantum mechanics, lp exists\nsince we need a minimal time tp to evolve an initial quantum\nstate to a ﬁnal quantum state that is orthogonal/distinguishable\nto the initial state. From the geometric point of view, lp\nexists since the spacetime can not curve to more than it can\ncurve[8]. Finally from the geometry/information duality point\nof view, since geometry encodes information, the minimal\nlength lp means that a too small change of the geometry\nwill not signiﬁcantly change the information it encodes. Or\nequivalently the geometry is determined by θ, if we regard θ\nas an unknown parameter that should be estimated from the\ninformation (the quantum state) of our universe, the Fisher-\nRao limit of this parameter estimation will result in lp so\nthat the minimal variance of the estimate of θ corresponds\nto a perturbation of the spatial distance given by lp. So lp\nis a signature to indicate how robust the geometry can save\nthe information of our universe. A larger lp means a more\nrobust memory. Immediately we can ask, how about black\nholes? They are abnormal spacetime structures whose internal\ndetails can not be detected for observers outside the event\nhorizon. Intuitively the radius of a black hole corresponds to\nthe Fisher-Rao bound that an outside observer can estimate the\ninterior geometry of the black hole. So black holes are robust\nmemories of quantum state information and the geometry of a\nblack hole is not sensitive to the details of the quantum state of\nthe material that collapsed to the black hole. This is exactly the\nno-hair theorem of black holes. Intuitively, a robust memory\nspace means a smaller gµν(θ) so that a change of the parameter\nθ will lead to a minor change of information. Immediately we\ncan see, a robust memory space corresponds to a deep network\nwith a smaller complexity. We will see this is the key concept\nfor the generalization property of deep networks.\nGiven the pictures of n GInf in different physical systems,\nwe can now try to scratch a global picture of GInf for general\ndeep networks as follows. (1) The geometry is built on the\ncomputation nodes of deep networks, which are events of\nGInf. (2) The emergent geometry can either be background\nindependent or background dependent. In either case, the\ngeometry is determined by the information and also it encodes\nthe information of the deep network, i.e. the information of\nthe computation carried out by the deep network. (3) The\nrobustness of the emergent memory space depends on the\nFisher metric at different nodes. Generally speaking, a deep\nnetwork with a lower complexity is more robust and this is also\nrelated with the generalization of the deep network. (4) The\nemergent geometry is a statistical average on all the geometries\nof the data X. This can be understood as that the geometry\nof spacetime can be constructed by collecting the trajectories\nof different free falling particles, where each individual data\nx ∈X can generate a trajectory when it’s processed by the\ndeep network.\nSo how ca GInf save the information? Intuitively GInf\nis a complex landscape built from the computation nodes. A\nspeciﬁc data can be regarded as a particle with a certain mass\nand an initial velocity, or a wave with a certain wave form.\nThen the data passes the network just as a particle or a wave\npasses a complex landscape GInf. Different data, as particles\nor wave forms, will lead to different trajectories or histories.\nThis is very similar to the picture of how the wave function\nof a particle passes the spacetime in quantum ﬁeld theory. So\nthe information is saved in the geometry of GInf, which is\nused to guide the trajectories or the histories of different data.\nIV. DEEP NETWORKS AS MEMORY SPACE OF INFORMATION\nNow we will examine how the geometry/information duality\nof deep networks can bring us new understanding of the\ninterpretability of deep networks and deep learning systems.\nIn our former work we already addressed the picture of GCom\nand we showed GCom shares the same Riemannian structure\nas quantum computation and geometric mechanics[1]. In this\npaper we will focus on the situation that if the complexity\nmetric is ﬁxed as the Fisher information metric, what can\nwe learn by taking deep networks as an information memory\nspace. We will check the capacity, robustness and efﬁciency\nof the memory and we can see these issues are closely\nrelated with the complexity, convergence, generalization and\ndisentangled representation of deep networks.\nA. Capacity, over-parameterization and general relativ-\nity\nThe capacity of the memory is obviously related with the\nsize of the deep network and a larger (deeper and wider)\nnetwork has potentially a larger capacity. The widely used\nover-parameterized deep network is a memory with a high\ncapacity. The observation of no bad local minima of over-\nparameterized network means that the high capacity memory\ncan have different ways to save the same information by\nconstructing different geometric structures.\nThe geometry of deep networks, including both GCom and\nGInf, depends on both the structure G and the parameter\nθ. The network structure G sets a prior constraint on the\ngeometry and it has a strong impact on the geometry and also\nthe dynamics of the network training. For example the success\nof ResNet largely depends on the fact that as a discretized\ntime variant differential equation, the regular structure of\nResNet deﬁnes a submanifold of deep networks with smooth\ngeometries of GCom. So the training of ResNet is in fact\nto ﬁnd a curve on a smooth manifold and of course the\nconvergence property of ResNet is superior to general deep\nnetworks.\nHere we would like to point out that in quantum many-\nbody systems, we have an analogue of over-parameterized\n7\ndeep networks, which is the MPS state with its correspondent\nparent Hamiltonian and uncle Hamiltonian[27]. An MPS state\nis a low complexity quantum state on a 1-dimensional chain.\nIt can be represented by a MPS tensor network and also can\nbe represented as the ground state of a local Hamiltonian.\nIf we compare the MPS states and over-parameterized deep\nnetworks, we will ﬁnd (1)The local Hamiltonian corresponds\nto the cost function of deep networks. Parent Hamiltonian\nand uncle Hamiltonian are non-overparameterized and over-\nparameterized networks respectively. The ground state of the\nlocal Hamiltonian is the conﬁguration of the deep network\nwhich minimize the cost function.\n(2)The MPS state is the solution of the deep networks\nthat minimize the cost function. Or in the view of geome-\ntry/information duality, the MPS state is the optimal tensor\nnetwork representation of the quantum state.\n(3) The MPS state is an analytical solution of the network that\nminimize the cost function and we do not need to use gradient\nbased optimization to ﬁnd it.\n(4) The MPS state shows a perfect geometry/information cor-\nrespondence since the information represented by the Hamil-\ntonian and the geometry represent by the MPS tensor network\nare analytically connected. This shows that overparameterized\ndeep networks also appear in physics and the comparison\nbetween overparameterized deep networks in both ﬁelds may\nprovide an new approach to understand overparameterized\nnetworks, for example the geometry of the subspace of local\nminima of deep networks.\nAnother interesting observation is that when the Fisher\ninformation metric is used to deﬁne the network complexity,\nthe resulting GInf of the optimal network conﬁguration and\nthe information of data X show the same interaction pattern\nas the interaction between spacetime and material in general\nrelativity. Following John Wheeler,spacetime tells matter how\nto move, matter tells spacetime how to curve. In deep net-\nworks, network tells data (information) how to move, data\n(information) tells network how to curve. This observation\nlies in the fact that due to geometry/information duality,\nboth the spacetime structure and the optimal deep network\nconﬁguration emerge from the same least action principle,\nwhere the action is given by the Fisher information metric\nbased complexity, and therefor their dynamic rules share the\nsame property. The similarity between deep networks and\ngeneral relativity conﬁrms our belief that our programme of\ngeometrization of deep network is the right way to understand\ndeep learning systems.\nB. Robustness, generalization and quantum mechanics\nThe robustness of the memory is described by the Fisher infor-\nmation matrix, or the mean Hessian matrix as shown in 1. In\n[23] it was shown that for an over-parameterized deep network,\nnetwork training with a random initialization will converge\nto a conﬁguration with a better generalization performance\nwith a higher probability. They also assumed the conﬁguration\nwhose Hessian matrix has a spectrum with more small eigen\nvalues will have a larger attraction basin volume and also such\nconﬁguration has a better generalization performance. But they\ngave no proof on this point. Also in [24] the distribution of the\nspectrum of the Fisher information matrix was investigated to\nanalyse the generalization performance of deep networks.\nHere we see the robustness issue provides an answer to\nthe generalization problem of deep networks. For a network\nconﬁguration whose Hessian matrix spectrum has a lot of\nsmall eigen values, obviously the information it encodes is\nless sensitive to the network parameter as shown in 1. This\nmeans a small perturbation of the network conﬁguration,\nequivalently the geometry of GInf of the deep network,\nwill not signiﬁcantly change the information pattern that the\nnetwork encodes. This is exactly what a good generalization\nasks for. Now we can answer why over-parameterized network\nseems always ﬁnd a solution with a good generalization\nperformance. The arguments can be formulated as follows:\n(1) A network with a good generalization property means\nthat it can save the information with a higher robustness.\nOr equivalently, a low complexity network has a better\ngeneralization capability. This is because the geometry GInf\nof a low complexity network can be determined by a relatively\nsmaller training data set so that it has a better generalization\nproperty .\n(2) A high robustness means the information pattern is not\nsensitive to the perturbation on network conﬁguration, so\nthat the Fisher information matrix has a spectrum with more\nsmall eigen values.\n(3) Such a network conﬁguration has a larger attraction basin\nvolume.\n(4) Network training from a randomly initialization will fall\nin such a network conﬁguration with a higher probability.\nHere we would like to emphasize thatwith Fisher informa-\ntion metric, the network complexity is not proportional to the\nsize of the deep network. So a deep network with a large\nnumber of parameters can still result in a low complexity\nnetwork after training. This observation may resolve the\nconﬂicts between the theoretical analysis of generalization\nbound and experimental results in PAC Bayes learning. This is\nto say, there are experimental works show that training a larger\ndeep network results in a better generalization performance,\nwhich seems to contradict the PAC Bayes generalization\nbound. But if the network complexity is deﬁned by the Fisher\ninformation metric, then a larger deep network may in fact\nhave even a smaller network complexity after training, so that\nthe superfacial contradiction may actually be resolved.\nWe can also ﬁnd that the aforementioned observation that\nthe probability that the network reaches a certain conﬁguration\nfrom a random initialization is related with the complexity of\nthat conﬁguration[23] has a physical analogue. If we consider\nthat network complexity is actually deﬁned by a Riemannian\ndistance, then we have a connection between the probability\nand a Riemannian distance. This is exactly what happens in\nthe geometric formulation of quantum mechanics[28], where\nin quantum measurement the probability that a state collapses\ninto a certain state is related with the distance between the ini-\ntial and ﬁnal states. Maybe this can bring us a new perspective\nto understand the quantum measurement problem. Again, this\nanalogue is another evidence to support our programme of\ngeometrization of deep networks.\nC. Efﬁciency, disentangled representations and quantum\n8\nentanglement In the geometry/information duality, the least\ncomplexity principle means we are looking for a low com-\nplexity representation of the information so that the memory\nhas a high efﬁciency. We believe this is also related the\ndisentangled representation that deep learning systems are\nchasing for. From our geometrization point of view, the so\ncalled disentangled representation is just a natural byproduct\nof the least complexity oriented optimization. This is to say,\nthe optimal low complexity deep network must give a disen-\ntangled representation of the information and the disentangled\nrepresentation is just a way to achieve a low complexity.\nStill we can ﬁnd a counterpart in physics since the word\nentanglement is more popular in quantum computation. In\nquantum mechanics, the key feature of an entangled state\nis that its information is encoded in the global system but\nnot in its subsystems. Accordingly disentanglement means\ninformation is saved in individual subsystems and we can play\nwith the information just by manipulating subsystems. In fact\nthis is exactly what the disentangled representation means in\ndeep learning systems. In quantum information processing,\na quantum algorithm on n-qubits can be represented by an\nunitary operator U ∈SU(2n) and its optimal implementation\nis to ﬁnd a quantum circuit (a deep network of quantum\ncomputation) with a minimal complexity that can achieve U\nby simple 1 and 2 qubit operators. We can immediately see the\nleast complexity deep network for U is exactly a disentangled\nrepresentation of U since all the 1 and 2 qubit operators only\nwork on subsystems.\nSo network complexity is also a criteria for the disentangled\nrepresentation. The recent work[29] claimed that in unsuper-\nvised learning, the disentangled representation can not be ob-\ntained without inductive bias on both the data and models. But\nif the nature of disentangled representation is a low complexity\nrepresentation, to ﬁnd it we must take the network complexity\nas part of the cost function. The assumption of [29] that we can\nnot introduce any inductive bias in the unsupervised learning is\nmeaningless, since it’s not compatible with the essence of the\ndisentangled representation. Different representations for the\nsame information can be discriminated by their complexities.\nSo in order to ﬁnd disentangled representations, we must take\nthe deep network complexity into consideration.\nFor more information on the geometric structure of dis-\nentangled representations, please refer to [30] where a ﬁbre\nbundle based description of disentangled representations was\nintroduced. There we see disentangled representations can\nbe formulated and understood as a gauge theory, and the\narguments of [29] were analysed in more details.\nD. Data augmentation, data distillation and data re-\nweighting\nHow can GInf encode the information in its geometry?\nAccording to the construction of GInf, the geometry is built\non the deep network, where computation nodes are taking as\npoints and the landscape of GInf is given by the information\nmetric. Obviously the information is stored in the landscape\nof GInf. An intuitive picture of how GInf encodes the\ninformation is that:\nThe deep network constructs a complex landscape. An input\ndata is a particle or a wave form incident to this landscape with\nFig. 2. The ﬁbre bundle structure of disentangled representations[30]\na certain initial position and velocity. Along its travel across\nthe landscape, its trajectory is determined by the landscape\ntill it reaches its destination so that different particles or wave\nforms will have different trajectories. On the other hand, the\nlandscape is shaped by the trajectories of all training data\njust as the spacetime is built by the histories of quantum\ncomputations[8]. So the whole story of deep learning is to\nbuild a landscape by adjusting the trajectories of training\ndata so that the landscape can store the information of the\ncorrespondent task.\nNow we can check how the training data can shape GInf.\nThere are three different ways to manipulate the training\ndata, data augmentation, data distillation[31] and data re-\nweighting[21]. Intuitively data augmentation aims to construct\nGInf by checking trajectories of more data, while data distil-\nlation tries to shape GInf with a minimal number of particle\ntrajectories. The goal of data re-weighting is to construct a\nstable GInf which is the critical point with respect to the\nre-weighting of data so that the landscape will be stable\nagainst the perturbation of data weights. How the training\ndata can shape GInf and what’s the relationship between the\ntraining data size and the complexity of the deep network are\ninteresting problems to be further explored in the furture.\nE. Attention, GAN, interaction and ﬁbre bundle Atten-\ntion mechanism became one of the most important compo-\nnents in deep learning systems in recent years. Following the\nguideline of geometrization of deep networks, it’s interesting\nto examine the geometric and physical pictures of attention\nmechanism. Naturally the ﬁrst thought can be, a deep network\nusing attention mechanism is still a deep network so that its\ngeometry has nothing special. Is this true or can we ﬁnd a\nnew geometry from attention mechanism?\nThe key idea of attention is to re-weight the information\nby coupling a normal deep network with a network that\nimplements the attention mechanism. Intuitively if we take the\ndeep network without attention as a curve in the manifold of\nthe corner of physical functions, then coupling the attention\nmodule with the original deep network will shift the curve\nof the original network. This can be understood as that the\nattention mechanism applies a force on a particle along the\n9\ncurve so that the trajectory of the particle is shifted. So the\nattention mechanism is actually a mechanism to provide an\ninteraction between data information. In physics interactions\nare usually geometrically represented by connections on ﬁbre\nbundles. A geometric structure with connections and ﬁbre\nbundles is an extension of the Riemannian structure. Similarly\nmore complex deep networks such as the neural Turing\nmachine(NTM) and the differential neural computer(DNC)\ncan also be regarded as extended attention mechanisms. In\nour former work we have established a ﬁbre bundle structure\nfor the disentangled representation of deep networks[30].\nHow to build geometric structures of deep networks beyond\nRiemannian structure will also be our future work.\nAnother related system is GAN, which is also a coupled\ndeep network including the generator and the discriminator.\nHow can GAN be formulated as a ﬁbre bundle based interac-\ntion picture is an interesting issue. Another interesting question\nis how the structure of the generator of GAN can encode\nthe data pattern, for example the pattern of images of human\nfaces? Intuitively for a generator to generate human face im-\nages, it can be understood as a cascaded error correcting code.\nDuring the generation of human face images, the generator not\nonly encodes the input latent variable into the code words of\nhuman faces but also continuously corrects pattern errors so\nthat perturbations or errors can be projected out of the code\nword subspace, just as shown in the work of [32] where wrong\npositions of church doors can be suppressed by the generator\nnetwork. How can GInf of the image generator encode an\nerror correcting code in its geometry is an interesting topic to\nbe explored in the future.\nF. Deep learning, meta learning and geodesics From the\nRiemannian structure GCom and the least complexity princi-\nple, conceptually the normal deep learning procedure is to ﬁnd\na geodesic to connect the trivial identity transformation and\nthe target transformation on the corner of physical functions1.\nWhile meta learning such as MAML is to learn the geodesic\nequation on the manifold, which is encoded in the meta\nparameters, so that geodesics correspond to new tasks can be\ngenerated from the learned meta parameters given the target\ntransformation encoded in the data of new tasks. Similarly,\nnonparametric methods are to learn the metric of GCom so\nthat distances between transformations can be computed. The\ntransformations for the test tasks can then be obtained by\nan interpolation of the training tasks based on the distances\namong them.\nV. DISCUSSIONS\nThe idea of geometry/information duality of deep networks\ncan also be used to understand some interesting phenomena\nobserved in a few recent papers.\nWeight agnostic neural network (WANN)\nIn [33] the so called weight agnostic neural networks(WANN)\nwere introduced, whose function is more determined by the\nstructure of a deep network instead of the weights. From the\ngeometry/information view point, the encoded information of\nWANN is extremely robust with respect to the weights. From\nour discussion of the spacetime structure, we can see this can\nbe understood as a kind of blackhole. This is an interesting\nobservation since we can construct a black hole like structure\nin deep networks, which is a new evidence of the applicability\nof our programme of geometrization of deep networks. Here\nwe need to point out that this is not a real black hole since\nreal black holes have a more complex dynamics. We call\nWANN a black hole only because it has the same property\nof black holes that outside observer can not obtain a low\nvariance estimate of its interior parameters. A straight forward\nquestion is, can we ﬁnd worm holes in deep networks? From\nour understanding of the worm holes, they are related with\nquantum entanglement due to ER=EPR, so it’s very unlikely\nthat we can ﬁnd worm holes in classical deep networks. But\nwe can not exclude the possibility that we ﬁnd something\nlooks like a worm hole or some other interesting geometric\nstructures. In [34] Preskill modeled spacetime as a quantum\nerror correcting code. In fact in the dissection of the image\ngenerator of GANs [32] we also see that the image generator\nnetwork also works as a cascaded error correcting code so\nthat it can encode the input latent vector into an image, while\nduring this procedure possible error patterns can be corrected\nby the generator network. There is a high similarity between\nthe generator network as a cascaded error correcting code and\nthe QEC based spacetime structure.\nLottery ticket hypothesis\nAnother recent hot spot about deep network is the lottery\nticket hypothesis[35], which essentially revealed the coupling\nbetween the structure and parameters of deep networks. From\nthe geometry/information duality point of view, this observa-\ntion is trivial since the geometry GInf is determined by both\nthe structure and the parameters so that they are coupled and\nonly certain combinations of them can be used to represent the\ninformation of a certain application. What we are interested\nis the following up research[36] on the lottery hypothesis,\nwhere it claimed that ambitious information of the network\nparameters, such as the supermask or only the signs of the net-\nwork parameters, can already encode partially the information.\nThis means these ambitious information can already roughly\nshape the landscape of GInf. Obviously the robustness of the\nnetwork function against the network conﬁguration is closely\nrelated with the WANN case. How robust the geometry of\nGInf is with respect to the network structure and parameters\nis also an interesting issue to be exploited in the future.\nOverparameterized network and network pruning From the\ninformation/geometry duality point of view, overparameterized\nnetworks aim to build GInf with an over-abundant number of\nnodes. On the contrary network pruning tries to approximate\nor interpolate GInf of an overparameterized network with\nless points. Currently we see two different ways to achieve\nthis goal. The dominating solution is to eliminate nodes with\nsmall weights step by step[]. This strategy is very natural since\nnodes with small weights can be regarded as points with a\nsmall rise and fall in GInf so that they can be ﬁrst omitted.\nAnother idea is to ﬁrst delete nodes staying in the geometric\nmedian of the conﬁgurations of all nodes[37]. Geometrically\nsuch geometric medians are points that stay in the center\nof a set of nearby points so that theoretically such points\ncan be interpolated by its neighbours. So both of these two\n10\nstrategies are geometrically valid. But which one is better?\nTo see this, we can still go back to our geometric picture.\nIf we regard the procedure of network pruning as a diffeo-\nmorphic transformation to match the initial overparameterized\nGInf and the ﬁnal simpliﬁed GInf achieved by the pruned\nnetwork, then the network pruning leads to a continuous curve\nconnecting the two similar GInfs. This is an analogue of the\nquantum evolution between two quantum states. According to\nour understanding of the geometry of quantum computation,\nthe optimal strategy to achieve the evolution is the one with the\nminimal energy spending. Intuitively the evolution trajectory\nshould be smooth but not in a zig-zag pattern. From this point\nof view, of two different trajectories of the network pruning,\nwe still prefer the trajectory to ﬁrst delete nodes with small\nweights since it will deliver a smoother trajectory. On the other\nhand, if the strategy to delete geometric centers can converge,\nthen this means the information saved in GInf is robust to\nthe conﬁguration of the network so that the resulting network\nmay have a better generalization performance.\nVI. CONCLUSIONS\nGeometrization is not only the key idea of physics, it’s also\na framework to understand deep networks. Recently in physics\nresearch, the concept of computational complexity becomes a\nkey player in understanding the structure of spacetime and\nquantum phase. In this work, we formally propose the pro-\ngramme of geometrization of deep networks as a framework\nfor the interpretability of deep learning systems. Inspired by\nthe information/geometry duality in physics, we examined the\ngeometry/information duality in deep networks so that we can\nbuild two geometric structures on deep networks. As an ana-\nlogue of the geometric mechanics or the geometry of quantum\ncomputation, the ﬁrst geometric structure GCom deﬁned a\nRiemannian manifold on the space of the functions that can be\nrepresented by deep networks, where the Riemannian metric\nplays a role to deﬁne the complexity of deep networks. When\nthis Riemannian metric is chosen as the Fisher information\nmetric, which aims to represent how the conﬁguration of a\ndeep network is related with the information represented by\nthe deep network, a second geometric structure GInf emerges,\nwhich can be regarded as a memory spacetime that can encode\nthe information by its geometric structure.\nIn this geometric picture of deep networks, the Fisher\ninformation metric plays a central role to integrate the two\ngeometric structures, GCom and GInf. On one hand it deﬁnes\na Riemannian metric to measure the network complexity. On\nthe other hand, the complexity metric aims to optimize the\nway how the deep network encodes the information of data.\nApplying a least action principle on the Fisher information\nmetric based network complexity results in not only an optimal\ndeep network with a minimal network complexity, which is a\ngeodesic on the Riemannian manifold GCom connecting the\nidentity function and the target function that can accomplish\nthe task, but also a second geometric structure GInf which\ncan encode the information of data in its geometry with a\nhigh efﬁciency and robustness.\nThis geometric picture of deep networks can help us to\nhave a general framework to analysis the properties of deep\nnetworks including the network complexity, generalization,\ndisentangled representation and other related issues.\nWe hope this observation can serve as a strong evidence that\nour programme of geometrization of deep networks is not only\na promising framework for the interpretability of deep learning\nsystems, but also it can bridge the concepts of deep networks\nand physics so that it also provide the interpretability of our\nphysical world.\nREFERENCES\n[1] X. Dong and L. Zhou.\nGeometrization of deep networks for the\ninterpretability of deep learning systems. arxiv:1901.02354, 2019.\n[2] X. Dong and L. Zhou. Understanding over-parameterized deep networks\nby geometrization. arxiv:1902.03793, 2019.\n[3] S. Lloyd. A theory of quantum gravity based on quantum computation.\nClass.quant.grav, 2006.\n[4] Brian Swingle. Constructing holographic spacetimes using entanglement\nrenormalization. Physics, 2012.\n[5] M. van Raamsdonk. Building up spacetime with quantum entanglement.\nGeneral Relativity and Gravitation, 42(10):2323–2329, 2010.\n[6] Hiroaki Matsueda.\nDerivation of gravitational ﬁeld equation from\nentanglement entropy. arXiv:1408.5589v2, 70, 2014.\n[7] Wen Xiao-Gang. Quantum order from string-net condensations and the\norigin of light and massless fermions. Physical Review D, 68(6):484–\n504, 2003.\n[8] Seth Lloyd. A theory of quantum gravity based on quantum computation.\nClass.quant.grav, 2012.\n[9] G. Evenbly and G. Vidal. Tensor network states and geometry. Journal\nof Statistical Physics, 145(4):891–918, 2011.\n[10] Glen Evenbly.\nAlgorithms for tensor network renormalization.\nPhys.rev.b, 95(4), 2017.\n[11] M. R. Dowling and M. A. Nielsen.\nThe geometry of quantum\ncomputation. Quantum Information and Computation, 8(10):861–899,\n2008.\n[12] Leonard Susskind. Dear qubitzers, gr=qm. arXiv:1708.03040v1, 2017.\n[13] J.S. Wu X. Dong and L. Zhou. How deep learning works –the geometry\nof deep learning. arXiv:1710.10784, 2017.\n[14] Martins Bruveris and Darryl D. Holm. Geometry of image registration:\nThe diffeomorphism group and momentum maps.\nFields Institute\nCommunications, 73:19–56, 2013.\n[15] E Weinan, Jiequn Han, and Qianxiao Li. A mean-ﬁeld optimal control\nformulation of deep learning. arxiv:1807.01083v1, 2018.\n[16] Romn Ors. A practical introduction to tensor networks: Matrix prod-\nuct states and projected entangled pair states.\nAnnals of Physics,\n349(10):117–158, 2014.\n[17] L. Susskind. Entanglement is not enough. arXiv:1411.0690v1, 2014.\n[18] Hiroaki Matsueda. Emergent general relativity from ﬁsher information\nmetric. arXiv:1310.1831v2, 2013.\n[19] Martins Bruveris and Peter W. Michor. Geometry of the ﬁsher-rao metric\non the space of smooth densities on a compact manifold. 2016.\n[20] Pang Wei Koh and Percy Liang. Understanding black-box predictions\nvia inﬂuence functions. In Doina Precup and Yee Whye Teh, editors,\nProceedings of the 34th International Conference on Machine Learning,\nvolume 70 of Proceedings of Machine Learning Research, pages 1885–\n1894, International Convention Centre, Sydney, Australia, 06–11 Aug\n2017. PMLR.\n[21] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning\nto reweight examples for robust deep learning. In Jennifer Dy and An-\ndreas Krause, editors, Proceedings of the 35th International Conference\non Machine Learning, volume 80 of Proceedings of Machine Learning\nResearch, pages 4334–4343, Stockholmsmssan, Stockholm Sweden, 10–\n15 Jul 2018. PMLR.\n[22] Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James\nStokes. Fisher-rao metric, geometry, and complexity of neural networks.\narxiv:1711.01530, 2017.\n[23] Wu Lei, Zhanxing Zhu, and E Weinan.\nTowards understand-\ning generalization of deep learning: Perspective of loss landscapes.\narxiv:1706.10239v2, 2017.\n[24] Ryo Karakida, Shotaro Akaho, and Shun Ichi Amari. Universal statistics\nof ﬁsher information in deep neural networks: Mean ﬁeld approach.\n2018.\n[25] Leonard Susskind. The typical state paradox: diagnosing horizons with\ncomplexity. Fortschritte Der Physik, 64(1):84–91, 2016.\n11\n[26] L. Susskind and Y. Zhao.\nSwitchbacks and the bridge to nowhere.\narXiv:1408.2823v1, 2014.\n[27] C. Fernndez-Gonzlez, N. Schuch, M. M. Wolf, J. I. Cirac, and D. Prez-\nGarca. Frustration free gapless hamiltonians for matrix product states.\nCommunications in Mathematical Physics, 333(1):299–333, 2015.\n[28] H.\nHeydari.\nGeometric\nformulation\nof\nquantum\nmechanics.\narXiv:1503.00238, 2015.\n[29] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Rtsch, Sylvain\nGelly, Bernhard Schlkopf, and Olivier Bachem. Challenging common as-\nsumptions in the unsupervised learning of disentangled representations.\n2018.\n[30] X. Dong and Zhou. L. Gauge theory and twins paradox of disentangled\nrepresentations. arxiv:1906.10545, 2019.\n[31] Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A. Efros.\nDataset distillation. 2018.\n[32] David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B.\nTenenbaum, William T. Freeman, and Antonio Torralba. Gan dissection:\nVisualizing and understanding generative adversarial networks. 2018.\n[33] A. Gaier and D. Ha. Weight agnostic neural networks. arxiv:1906.04358,\n2019.\n[34] D. Harlow F. Pastawski, B. Yoshida and J. Preskill.\nHolographic\nquantum error-correcting codes: toy models for the bulk/boundary\ncorrespondence. Journal of High Energy Physics, 2015(6):1–55, 2015.\n[35] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis:\nFinding sparse, trainable neural networks. arxiv:1803.03635, 2018.\n[36] H. Zhou, J. Lan, R. Liu, and J. Yosinski. Deconstructing lottery tickets:\nZeros, signs, and the supermask. arxiv:1905.01067, 2019.\n[37] Y. He, P. Liu, Z.W. Wang, Z.L. Hu, and Yang Y. Filter pruning via\ngeometric median for deep convolutional neural networks acceleration.\narxiv:1811.00250, 2018.\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2019-07-12",
  "updated": "2019-07-12"
}