{
  "id": "http://arxiv.org/abs/1907.07543v1",
  "title": "Low-Shot Classification: A Comparison of Classical and Deep Transfer Machine Learning Approaches",
  "authors": [
    "Peter Usherwood",
    "Steven Smit"
  ],
  "abstract": "Despite the recent success of deep transfer learning approaches in NLP, there\nis a lack of quantitative studies demonstrating the gains these models offer in\nlow-shot text classification tasks over existing paradigms. Deep transfer\nlearning approaches such as BERT and ULMFiT demonstrate that they can beat\nstate-of-the-art results on larger datasets, however when one has only 100-1000\nlabelled examples per class, the choice of approach is less clear, with\nclassical machine learning and deep transfer learning representing valid\noptions. This paper compares the current best transfer learning approach with\ntop classical machine learning approaches on a trinary sentiment classification\ntask to assess the best paradigm. We find that BERT, representing the best of\ndeep transfer learning, is the best performing approach, outperforming top\nclassical machine learning algorithms by 9.7% on average when trained with 100\nexamples per class, narrowing to 1.8% at 1000 labels per class. We also show\nthe robustness of deep transfer learning in moving across domains, where the\nmaximum loss in accuracy is only 0.7% in similar domain tasks and 3.2% cross\ndomain, compared to classical machine learning which loses up to 20.6%.",
  "text": "arXiv:1907.07543v1  [cs.LG]  17 Jul 2019\nLow-Shot Classiﬁcation: A Comparison of Classical and Deep Transfer\nMachine Learning Approaches\nPeter Usherwood\npeter.usherwood@kantar.com\nSteven Smit\nsteven.smit@kantar.com\nJuly 18, 2019\nAbstract\nDespite the recent success of deep transfer learning approaches in NLP, there is a lack of quan-\ntitative studies demonstrating the gains these models oﬀer in low-shot text classiﬁcation tasks over\nexisting paradigms. Deep transfer learning approaches such as BERT and ULMFiT demonstrate\nthat they can beat state-of-the-art results on larger datasets, however when one has only 100-1000\nlabelled examples per class, the choice of approach is less clear, with classical machine learning and\ndeep transfer learning representing valid options. This paper compares the current best transfer\nlearning approach with top classical machine learning approaches on a trinary sentiment classiﬁca-\ntion task to assess the best paradigm. We ﬁnd that BERT, representing the best of deep transfer\nlearning, is the best performing approach, outperforming top classical machine learning algorithms\nby 9.7% on average when trained with 100 examples per class, narrowing to 1.8% at 1000 labels per\nclass. We also show the robustness of deep transfer learning in moving across domains, where the\nmaximum loss in accuracy is only 0.7% in similar domain tasks and 3.2% cross domain, compared\nto classical machine learning which loses up to 20.6%.\n1\n1\nIntroduction\nTransfer learning in the Natural Language Pro-\ncessing (NLP) ﬁeld has advanced signiﬁcantly\nin the last two years, introducing ﬁne-tuning\napproaches akin to those seen in computer vi-\nsion some years earlier (Donahue et al., 2013).\nThis growth originated from feature-based trans-\nfer learning, which in the form of word embed-\ndings has been in use for some years, particularly\ndriven\nby\n(Mikolov, Chen, Corrado, & Dean,\n2013).\nAs part of this new wave, we have\nseen advancements in feature-based transfer\nlearning in the form of ELMo (Peters et al.,\n2018).\nIn addition a characteristic trend in\nthis wave of transfer learning models is a class\nof algorithms that primarily focus on a ﬁne-\ntuning approach, where a base language model\n(Bengio, Ducharme, Vincent, & Jauvin, 2003) is\ntrained and then ﬁne-tuned on a target task.\nThis base language model is typically very large\n(100M+ parameters) and takes a relatively long\ntime to train.\nHowever, the ﬁne-tuning task\nis usually much quicker to train as only a few\nparameters are added to the model, typically\na single dense layer to the end of a multilayer\nLSTM or Transformer (Vaswani et al., 2017).\nThe model continues training either all, or part,\nof the network, but this is typically on much less\ndata and for much less time, as only the task spe-\nciﬁc information is being learned and the general\n”understanding” of the language is transferred.\nThese approaches have, on multiple occa-\nsions, broken the state-of-the-art records (SO-\nTAs) across the board on a range of NLP tasks\nand datasets (Devlin, Chang, Lee, & Toutanova,\n2018) (Howard & Ruder, 2018). However, all of\nthese datasets are designed for deep learning:\nthey are typically large enough that they war-\nrant the use of deep learning (5000+ examples\nper class), without the necessity of transfer learn-\ning. It is our view that what transfer learning\ndoes, in these cases, is push the boundaries of\nperformance.\nThe prevalence of deep learning algorithms in\nsurpassing SOTA records suggests quite clearly\nthat, for the datasets assessed, deep learning sur-\npasses the limits of classical machine learning al-\ngorithms in NLP tasks.\nLow-shot transfer learning is another use-\ncase for transfer learning in NLP, one of particu-\nlar interest to companies working with real-world\ndata. Low-shot transfer learning (also referred\nto as ”few-shot”) is the use of transfer learning\nin training models where we have little training\ndata available. This is important as many po-\ntential real-world applications of machine learn-\ning NLP do not have access to suﬃciently large\ndatasets to train deep learning algorithms, and\nobtaining such a dataset can often be too expen-\nsive or time consuming.\nHoward & Ruder (2018) note, and Devlin\net.\nal.\n(2018) hypothesize that their respec-\ntive approaches can be used with low quanti-\nties of data to give good results.\nHowever, in\nsources such as (Howard & Ruder, 2018), re-\nsults on low-shot learning are presented relative\nto training deep models from scratch, but as\nmentioned in (Goodfellow, Bengio, & Courville,\n2016), deep learning generally only achieves rea-\nsonable performance at about 5000 examples per\nclass and is therefore not necessarily the best\nparadigm at these scales. This is shown quan-\ntitatively in (Chen, Mckeever, & Delany, 2018)\nwhere, at scales of\n2000+ labels per class,\nan SVM outperforms several deep learning ap-\nproaches on text classiﬁcation tasks.\nAs such,\nwe propose that to evaluate the low-shot learn-\ning beneﬁts of deep transfer learning models,\nwe should in fact look at performance against\nthe strongest classical machine learning meth-\nods. However, we have yet to ﬁnd a comprehen-\nsive quantitative study performing this analysis\nand show that low-shot transfer learning in NLP\nis actually the optimal approach when dealing\nwith small quantities of data.\nIn this paper we attempt to answer this ques-\ntion in the context of classiﬁcation tasks. What\nis the best paradigm to use in the case where we\nhave 100 −1000 labelled training examples per\nclass - classical machine learning or deep transfer\nlearning? We seek to compare the best-in-class\napproaches from both deep transfer learning and\nclassical machine learning by training a variety of\nmodels and evaluating by analysing intra-domain\nand inter-domain performance (details in section\n2).\nThe choice of 100 −1000 is motivated by the\n2\namount of data feasible for companies and re-\nsearchers to tag in-house, as well as the scale of\ndata occurring organically through other means.\nFor example, in marketing these ﬁgures typically\nrepresent the base sizes of surveys that can be\nused as training data.\nThe rest of this paper is laid out as follows.\nSection 2 details the datasets we use.\nSection\n3 looks at the methodology used to evaluate the\noptimal paradigm. In section 4 we present the al-\ngorithms we use to test, along with related work\ninﬂuencing our choices in selecting those mod-\nels. Section 5 details our experiments including\nchoosing the optimal conﬁguration of hyperpa-\nrameters and preprocessing for each algorithm.\nIn section 6 we present the results followed by\nour comments and conclusions. Finally, we high-\nlight a few key points and considerations worthy\nof mention for the two paradigms in 7.\n2\nDatasets\nWe have sourced a range of publicly available\ndatasets for classiﬁcation tasks in an attempt to\nremove any potentially unknown biases of one\nparticular set.\nHowever, to aid in our goal of\nviewing cross-dataset and cross-domain perfor-\nmance, we have focused on sentiment based clas-\nsiﬁcation. This is one of the more popular classi-\nﬁcation problems as well as being one of critical\nimportance for many companies in areas such as\nchatbots and social media marketing. Potential\nbiases include the ability for a certain task to be\npredictable based oﬀof a few low level features\n(making the task more trivial), or similar data\nhaving been used in the pre-training of the deep\ntransfer learning approach, tainting the test set.\nThe datasets we consider fall into two do-\nmains: Amazon reviews (A), and Twitter (T).\nThe ﬁrst category consists of 3 Amazon datasets,\none consisting of movie reviews, and two of prod-\nuct reviews from diﬀerent product categories.\nWhilst these are very ”real world” datasets,\nwe describe this domain as clean data.\nThese\ndatasets typically have similar medium-length\ndocuments of 100−300 words, and are the kinds\nof datasets typically used in evaluating the per-\nformance of deep transfer learning: Pang, Lee %\nVaithyanathan (2002) use IMDB and Howard &\nRuder (2018) use SST-2 movie reviews.\nThe second domain has datasets sourced\nfrom Twitter, a social data source that diﬀers in\na few key properties from the Amazon sets. The\nvocabulary is much broader given the amount\nof slang, abbreviations, and the unique way in\nwhich hashtags are used grammatically. In ad-\ndition, Twitter datasets typically will have a\nstronger prevalence of emoji than in other do-\nmains, although in these datasets emoji were al-\nready removed. These can make Tweets much\nharder to classify, particularly for deep transfer\nlearning models that have pre-deﬁned vocabu-\nlaries.\nBERT relies on WordPiece embeddings\nwhich makes it more robust to new vocabularies\n(Wu et al., 2016), although it still can not han-\ndle emoji. On the other hand, approaches similar\nto ULMFiT rely on a set word-token vocabulary\ndeﬁned by the training set used in pre-training,\nwhich for ULMFiT is wikitext-103 (a wikipedia\nbased text) by default, so this will struggle both\nwith new vocabulary and emoji. We hypothesise\nthese models will suﬀer a greater loss in accu-\nracy on these Twitter datasets than the classical\nalgorithms because of this ﬁxed vocabulary lim-\nitation.\nBelow we introduce the ﬁve datasets we use.\nThree from the Amazon reviews domain: Ama-\nzon Movie Reviews, Amazon Book Reviews,\nAmazon Health and Personal Care Product Re-\nviews which we will refer to later in the paper as\nA1, A2, and A3 respectively. The two datasets\nfrom the Twitter domain are both from SemEval\n2017, we use subtask a and subtask ce, which we\nwill refer to as T1 and T2 respectively.\n2.1\nAmazon Movies A1\nThe\nAmazon\nmovies\ndataset\nsourced\nfrom\n(Leskovec & Krevl, 2014) is a huge collection of\nmovie reviews from Amazon, including reviews\nmade up to October 2012.\nWe use a random\nsubset of this for our purposes. All reviews are\non a ﬁve point scale which we re-sample to a\nthree point scale by binning 2 star with 1 star\nand 4 star with 5 star reviews. This is a proce-\ndure we use throughout this work to align all of\nour datasets onto a three point sentiment scale\nof negative, neutral and positive. In this dataset\n3\nwe also have knowledge of which product each\nreview belongs to, so the train, validation, and\ntest sets are split out so that no product appears\nin two or more sets.\n2.2\nAmazon Books A2\nThe second dataset we consider is from the Ama-\nzon product review database (He & McAuley,\n2016), and contains reviews of books.\nThis\ndataset was chosen as it is fairly similar to that\nof the Amazon movies, whilst still being in a dif-\nferent domain.\nThis makes it ideal in helping\nus avoid biases relating to the speciﬁcity of one\ndataset while training a very similar task.\nIts\nsimilarity also makes it a perfect candidate to\ntest how well classiﬁers perform cross-domain in\na best case scenario.\nThe dataset is structured similar to that of\nthe movies, with a star rating from 1 to 5 giving\nus a ﬁve point sentiment scale which we resam-\nple to three point. The review text also contains\nmedium-length documents similar to that of A1.\nIt also contains information about which prod-\nuct is being reviewed, so to ensure there is no\ninformation leakage into the test set, we ensure\nevery book in each set is exclusive.\n2.3\nAmazon\nHealth\nand\nPersonal\nCare A3\nThis dataset is almost equivalent to the above in\nterms of set-up with the reviews instead focusing\non health and beauty products.\n2.4\nSemEval 2017 Subtask A T1\nFor\nour\nTwitter\ndataset\nwe\nuse\nSemEval\n(Rosenthal, Farra, & Nakov, 2017).\nSemEval\n2017 Task A comes pre-tagged into negative, pos-\nitive, and neutral classes so no binning is neces-\nsary. This is the only dataset we use that does\nnot contain ”product” information so we simply\nrandomly divide the Tweets between the train,\nvalidation, and test sets.\n2.5\nSemEval 2017 Subtask CE T2\nAgain looking at SemEval 2017 we use their\nsubtask CE datasets to produce this set.\nThe\ndata here comes pre-split although as we have\na slightly unusual case here wanting only a spe-\nciﬁc amount of training data and more test data\nwe shuﬄe everything and re-split. Here we have\nproduct information as all tweets are labelled\nwith a topic, so we divide on that variable.\nTweets here are also on a ﬁve point scale but\nagain we bin to three point by grouping ”very\nnegative” and ”negative”, and ”very positive”\nand ”positive”.\n3\nMethodology\nTo answer the question as to what is the optimal\nparadigm to use in a low-shot classiﬁcation task,\nwe will compare the performance of the best in\nclass approach from classical machine learning\nand deep transfer learning on various low shot\ndatasets.\nThe metrics we consider will be the accuracy\non a held out test set for each model. In addi-\ntion we shall also test the models’ robustness by\nexamining how they perform making predictions\nacross datasets within a domain (intra-domain),\nand across domains (inter-domain). Here we de-\nﬁne a domain as a set of datasets that share sim-\nilar properties, in this paper we consider two do-\nmains: Twitter (T), and Amazon reviews (A).\nIn the academic literature it is rare to see inter-\ndomain accuracy reported except in the case\nof speciﬁcally designed inter-domain algorithms\n(Pan, Ni, Sun, Yang, & Chen, 2010). However,\nwe see this as a common practice in business:\ntrain a single classiﬁer and use it inter-domain.\nAs such we feel it should be evaluated since our\nend use-case is informing the opinion of which\napproaches to take when building real-world clas-\nsiﬁers. Our intention is that the range of datasets\nwill minimise any bias and make the study as rel-\nevant to industry use-cases as possible.\nWe shall also consider several levels of low-\nshot transfer learning, taking: 100, 300 and 1000\nlabelled examples per class (henceforth referred\nto as t100, t300, and t1000 respectively) in an at-\ntempt to guard against the potential of missing\nthe point at which deep learning/classical ma-\nchine learning surpasses the other in the low-shot\ncontext.\nFor every dataset, we set aside controlled test\n4\nand validation sets. For A datasets we have 5000\nexamples per class in the test and validation sets,\n3000 for T1 and 1000 for T2.\nAll ﬁne-tuning and hyperparameter selection\nis done on the validation set, and all values dis-\nplayed in this paper in section 5 are from the\nheld-out test sets. The test sets comprise prod-\nucts that are independent of the train and vali-\ndation sets where the deﬁnition of a product is\nspeciﬁc to each dataset. For example, in the A1\ndataset, a product is a speciﬁc ﬁlm. We do this\nto keep the test as fair as possible, and ensure\nthat if classiﬁers overﬁt and learn features such\nas the name of the ﬁlm as a deﬁning feature, that\nit is not rewarded in the evaluation.\n3.1\nMetrics\nAs our end goal is to simply compare classical\nmachine learning to deep transfer learning we ag-\ngregate a lot of the metrics when presenting the\nresults as the individual model-dataset combina-\ntion is not of key interest. Instead we investigate\nhow well the paradigm does when evaluated on\na set task of a set domain. We present six met-\nrics to compare performance, each of these were\nrun per model, per tier, totalling 72 results. The\nexact metrics and how they are deﬁned from the\nunderlying datasets are given below.\nA Self:\nAverage of the three diﬀerent A\ntrained classiﬁers accuracies as returned on the\nsame datasets held out test set.\nA Cross A: Average of the three diﬀerent A\ntrained classiﬁers accuracies as returned on the\nother two A datasets corresponding held out test\nset.\nA Cross T: Average of the three diﬀerent A\ntrained classiﬁers accuracies as returned on the\ntwo T datasets held out test sets.\nT\nSelf:\nAverage of the two diﬀerent T\ntrained classiﬁers accuracies as returned on the\nsame datasets held out test set.\nT Cross T: Average of the two diﬀerent T\ntrained classiﬁers accuracies as returned on the\nother T datasets corresponding held out test set.\nT Cross A: Average of the two diﬀerent T\ntrained classiﬁers accuracies as returned on the\ntwo A datasets corresponding held out test set.\n4\nModels and Related Work\nWe are trying to compare the best in class ap-\nproach from classical machine learning and ﬁne-\ntuning transfer learning, as such we have con-\nsidered popular high performing models used in\nother well referenced work, as these represent the\ntypes of approaches practitioners will look to.\nThe ones considered in this paper are introduced\nbelow.\n4.1\nClassical Machine Learning\nIt is well established that there is no sin-\ngle classical machine learning classiﬁer that\nconsistently\nachieves\nthe\nbest\nclassiﬁcation\nperformance.\nFor\nexample\nbetween\nthe\nworks in (Davidson, Warmsley, Macy, & Weber,\n2017)\n(Dadvar, Trieschnigg, & de Jong,\n2014)\n(Dinakar, Reichart, & Lieberman,\n2011)\nthey\nshowed that various classical machine learning\napproaches all slightly out performed each other.\nThis is a long known phenomenon that all of\nthese models have diﬀerent strengths depending\non the speciﬁc task and dataset. As such we have\nconsidered two of these (Na¨ıve Bayes and SVM)\nto give a fair representation and alleviate the bias\nof a single classiﬁer.\n4.1.1\nNa¨ıve Bayes\nThe Na¨ıve Bayes classiﬁer is a probabilistic clas-\nsical machine learning classiﬁcation algorithm\nthat has a long history of being used in text clas-\nsiﬁcation tasks including sentiment analysis. It is\nwell suited to this task given its speed to run and\nability to easily incorporate many features which\noften occurs with classical NLP approaches.\nHowever as with most algorithms from clas-\nsical machine learning they are not competi-\ntive on large datasets compared to deep learn-\ning models as such we struggled to ﬁnd an\nundisputed best in class Na¨ıve Bayes approach.\nWe decided to try the approaches outlined in\n(Narayanan, Arora, & Bhatia, 2013) as in the\npaper the authors clearly show the beneﬁts of\neach modiﬁcation they make which we are able\nto verify for our data in section 5, they also ran\ntheir classiﬁer on a very similar dataset to the\n5\nones used in this paper (movie sentiment classi-\nﬁcation).\n4.1.2\nSVM\nSupport Vector Machines (SVMs) attempt to\nseparate the data by ﬁnding the hyperplane\nin n-dimensional space that maximizes the dis-\ntance between the closest (support) vectors in\nthe dataset. SVMs also have long been used in\ntext classiﬁcation problems given they perform\nwell when our number of features is large com-\npared to the number of training examples as is\nthe case in our classical paradigm.\nIndeed we see in (Chen et al., 2018) the au-\nthors compare an approach using SVM with an\nn-gram approach to word vector deep learning\napproaches on datasets that are larger than the\nsmallest dataset considered in this work, and in\nall balanced cases the SVM n-gram models out-\nperform the deep learning approaches.\nAs such we consider the n-gram approach\nwith SVMs however given the results gained in\n(Narayanan et al., 2013) we also try all of the\nsame additions to the architecture for SVMs that\nwe trial for Na¨ıve Bayes to further improve per-\nformance, the results of this analysis are shown\nin section 5.\n4.2\nTransfer Learning\nIn the deep transfer learning paradigm we have\nseen a quick succession of models, as discussion\nin section 1, released in the last two years each of\nwhich has surpassed the previous in terms of per-\nformance on deep learning tasks. Recently, the\nleading approach has been BERT (Devlin et al.,\n2018) and hence this will be the principle ap-\nproach we consider here.\nAdditionally we also\nlook at ULMFiT (Howard & Ruder, 2018), as\nthis was one of the ﬁrst major breakthrough ap-\nproaches in NLP transfer learning and is widely\nused through their dedicated pytorch-based li-\nbrary.\nWe also look to quantify the claims in\nHoward & Ruder (2018) that it performs well on\nlow-shot tasks. At the time of writing, it appears\nthat Yang et. al. (2019) is the current SOTA,\nsurpassing BERT. However, this is very recent\nand we leave evaluation of this as a future area of\nresearch to build on the work documented here.\nIndeed this raises a key point: transfer learning\napproaches are still not fully developed and with\ntime we expect these approaches to further im-\nprove relative to classical machine learning.\n4.2.1\nULMFiT\nULMFiT was introduced by Howard & Ruder\n(2018) and was one of the ﬁrst popular appli-\ncations of ﬁne-tuning transfer learning in NLP.\nThey achieved SOTAs on various classiﬁcation\ndatasets: AG, DBpedia, Yelp-bi, and Yelp-full.\nTheir approach is to use an AWD-LSTM,\nan\narchitecture\noriginating\nin\nthe\nwork\nof\n(Merity, Keskar, & Socher,\n2017) as the lan-\nguage model at the core of the model. The model\nis then trained in 3 stages, ﬁrst (Stage 1) the core\nlanguage model is trained on a large general pur-\npose corpus. This is the pre-training step which\nis ideally done only once per language. Stage 2\nis the ﬁne-tuning step where the same core lan-\nguage model continues to be trained but now on\nthe target dataset, this aids the model in learn-\ning the nuances of the target task language which\nultimately improves results on the ﬁnal classiﬁ-\ncation. The training of the classiﬁcation task is\nthe third stage, in which two dense layers are ap-\npended to the ﬁnal hidden layer of the language\nmodel and the whole model is trained on the\nsupervised classiﬁcation task.\nAdvanced tech-\nniques such as slanted triangular learning rates\nand gradual unfreezing are used to negate the\nproblem of catastrophic forgetting, enabling the\nmodel to better retain earlier learned informa-\ntion.\nAlthough the model is pre-training dataset\nagnostic, the current published model was built\non wikitext-103. The potential issue with this is\nchoice of pre-training dataset is that it is not very\ngeneral purpose for many real world applications,\nsuch as social datasets. The model is also built\nusing a ﬁxed vocabulary which further limits its\ngeneralizability.\nWe use this pre-trained AWD-LSTM based\nmodel and follow the recommended ﬁne-tuning\nstages in this paper, whilst verify the choice of\nall hyperparameters on held out validation sets\nin section 5.\n6\n4.2.2\nBERT\nBERT (Bidirectional Encoder Representations\nfrom Transformers) was introduced in late 2018\nby Devlin et. al. (2018). We chose this algo-\nrithm for its performance (breaking various SO-\nTAs on NLP tasks, both in classiﬁcation and\nother challenges such as question answering) and\npopularity.\nConceptually it is similar to that\nof ULMFiT, a core language model trained on\na large general purpose corpus followed by a\nstage of task speciﬁc ﬁne-tuning to learn a su-\npervised task. BERT has the generalizability to\nwork with classiﬁcation or sequence based target\ntasks which gives it further utility, however in\nthis paper we focus on its ability in classiﬁcation\ntasks.\nOne important distinction between the\ntwo approaches for our purposes is that feature\nrepresentation in BERT is based on word pieces\n(Wu et al., 2016) which may aﬀord the model\nbetter generalizability than ULMFiT.\nOn release BERT was published with two\nmodels: BERT-base which uses 12 transformer\nlayers and has 110M parameters, and BERT-\nlarge with 24 layers and 340M parameters. In\nthis paper we use BERT-base and follow the sug-\ngestions of ﬁne-tuning as given in the original\npaper. We verify our model in section 5 on our\nvalidation sets.\nThe second and ﬁnal stage of training BERT\nfor classiﬁcation tasks is to append a single dense\nlayer to the ﬁnal hidden layer of the language\nmodel and continue training.\n5\nExperiments\nWe initially setup the models based on the ref-\nerenced papers and ﬁne-tuning on the validation\nsets. Given the volume of models and datasets\nin this work it is unfeasible to ﬁne-tune the hy-\nperparameters for every model to be trained. In-\nstead we originally intended to optimize only one\nset of hyperparameters and pre-processing stages\nfor each model, and then train all models on this\nconﬁguration. However when we looked at the\nA1 and T1: 100 and 1000 to optimize the hyper-\nparameters we saw signiﬁcant deviation in the\noptimal conﬁguration across these sets for the\nclassical machine learning approaches, particu-\nlarly on the SVM. As such we proceeded having\nup to four conﬁgurations per approach: A t100-\nt300, A t1000, T t100-t300, T t1000.\n5.1\nNa¨ıve\nBayes\nHyperparameter\nSearch\nWe mainly followed the approach laid out in\nNarayanan et.\nal (2013) for our Na¨ıve Bayes\napproach, where negation was handled by merg-\ning any tokens that were preceded by any in a\nset of pre-deﬁned negation terms, with ”not ”\nas a preﬁx and removing the original negation\nterm. We also followed Narayanan et. al’s (2013)\nsuggestion of using a Bernoulli term frequency\nmatrix and including bi-grams and tri-grams,\nas we found all of these methods independently\nboosted the accuracy on the validation sets for\nA1. However on T1 none of the approaches led\nto any increase in performance. We also exper-\nimented with reducing the number of features\nnfeatures = f · tier as a hyperparameter, where\nthe grid search begins at the maximum number\nof features and f is tuned to reduce the number\nof features downwards, selecting for a more par-\nsimonious model. We present our search on the\nvalidation sets in table 1, also included were the\npapers original gains on binary sentiment clas-\nsiﬁcation. We also tried other approaches such\nas stemming and feature selection with Part of\nSpeech (PoS) tagging although these showed no\nbeneﬁt on any dataset, although perhaps with\nmore manual feature selections gains could be\nmade.\n5.2\nSVM Hyperparameter Search\nLooking at SVMs we try to apply a similar ap-\nproach as we do for Na¨ıve Bayes. Here we add\nthe extra hyperparameters of C (the SVM regu-\nlarization hyperparameter), and the kernel type.\nWe initially select values of f = max and C = 1\nwhich showed good performance to trial the var-\nious additions to the architecture. Once we eval-\nuated the best architecture we then ﬁne-tuned\nC and f.\nFinally we veriﬁed all of the archi-\ntectures again to check there were no changes in\ntop performance. The ﬁnal results chosen were\nf = max, 50, 10, 10, C = 1, 1, .5, 1, and linear\n7\nA1-100\nA1-1000\nT 1-100\nT 1-1000\nIMDb (Pang et al., 2002) % binary gain\nOriginal\n48.06\n55.73\n49.35\n57.64\n23.77\nNegation\n48.54\n56.52\n48.29\n57.29\n9.03\nBernoulli\n49.26\n56.31\n48.77\n57.03\n-\nN-grams Bernoulli\n50.09\n61.20\n47.85\n56.65\n-\nBernoulli Negated\n49.97\n56.99\n48.23\n56.49\n0.86\nN-grams Bernoulli Negated\n50.19\n61.15\n47.02\n56.24\n1.54\nTable 1: Hyperparameter search of the best architecture to use for the Na¨ıve Bayes approach, with the\nfour models chosen shown in bold.\nkernels on A1-t100, A1-t1000, T1-t100, T1-t1000 re-\nspectively as shown in table 2.\n5.3\nULMFiT Hyperparameter Search\nOne of the immediate beneﬁts of ﬁne-tuned\ntransfer learning approaches is that complex fea-\ntures representations are transferred, this means\nthat a lot of the eﬀort in preprocessing and fea-\nture selecting is not required (or even possible)\nin these cases. Furthermore an additional beneﬁt\nis that the models are designed to need minimal\ntask speciﬁc architecture adjustments again re-\nducing the amount of parameter selection needed\non a validation set.\nIn our experiments we\nuse the pre-trained AWD-LSTM weights pub-\nlished with the original paper, and continue with\nthe proposed methodological approach for the\nclassiﬁcation task, as implemented in the fastai\npython package (Fast.ai are behind the ULMFiT\nmethodology).\nThis leaves us with six choices of hyperpa-\nrameters: the number of epochs for ﬁne-tuning\nand classiﬁcation training, and the correspond-\ning base learning rates and dropout scaling rate.\nBy dropout scaling rates, we mean a scale ap-\nplied to the packages preset dropout rates. We\npresent the results found on our four searches in\nthe validation sets in table 3.\n[t]\nIt should be noted that in stage two of train-\ning a ULMFiT we can train on more domain data\nthan we have labelled. We chose not to do this\nto give the worse case scenario for practitioners\nusing these models.\n5.4\nBERT Hyperparameter Search\nAs noted, one of the beneﬁts of transfer learning\nis that minimal task speciﬁc hyperparameters are\nneeded and we do not need to select features. As\nsuch we go with the standard approach used in\nthe original paper, of attaching a single dense\nlayer to the end of the BERT model.\nWe are\nusing the pre-trained BERT-base model: in the\noriginal paper, signiﬁcantly better results were\nobtained with the larger model. Due to the use of\nconsumer-grade hardware for this investigation,\nwe will be making use of BERT-base (which al-\nready requires 12GB of VRAM). It is suggested\nthat any results here would only be improved\nupon by practitioners capable of running BERT-\nlarge. We use the uncased version.\nSimilar to ULMFiT, we are left only choos-\ning the learning rate and number of epochs for\nthe classiﬁcation phase, however in BERT only\none phase is run, so we only need to select two\nhyperparameters.\nIn the original BERT paper the authors\ncomment that often hyperparameter tuning on\nBERT is unnecessary, particularly for larger\ndatasets, and that an ideal search area is with\n3 epochs and learning rates between 5· 10−5 and\n1 · 10−5. We followed this recommendation test-\ning learning rates of 1 · 10−4, 5 · 10−5, 3 · 10−5, 3 ·\n10−5, and 1 · 10−5 and 3, 4, 5, and 10 epochs and\nfound largely the same results. Our best results\nare shown in table 4. However as even our bigger\ndatasets are still low-shot and as such this step\nwas still necessary as in the given range we can\nstill see gains of 3%+ accuracy.\n6\nResults\nReferring to the results in tables 5, 6 & 7, there\nare a few immediately notable results. As men-\ntioned there is no consistently best performing\nclassical machine learning algorithm, in this case\nwe see that Na¨ıve Bayes outperforms SVM by\n8\nA1-100\nA1-1000\nT 1-100\nT 1-1000\nOriginal\n46.63\n54.76\n44.63\n52.82\nNegation\n46.48\n55.03\n43.67\n52.98\nBernoulli\n47.51\n55.08\n45.71\n52.82\nN-grams Bernoulli\n50.30\n58.10\n43.13\n52.82\nBernoulli Negation\n47.47\n56.01\n45.04\n52.76\nN-grams Bernoulli Negation\n49.59\n58.62\n42.23\n51.93\nN-grams Bernoulli Negation Stopped\n48.67\n57.80\n43.96\n51.39\nTf-idf\n48.88\n59.84\n48.84\n56.97\nN-grams Tf-idf\n44.23\n46.66\n45.01\n43.86\nTf-idf Negation\n48.80\n60.13\n48.39\n56.56\nN-grams Tf-idf Negation\n47.92\n61.41\n46.76\n55.69\nN-grams Tf-idf Negation Stopped\n46.45\n60.14\n47.72\n55.41\nTable 2: Hyperparameter search of the best architecture to use for the SVM approach, with the four\nmodels chosen shown in bold.\nDataset\nEpochs\nLearning\nRate\nDropout\nScaling\nLoss\nStage 2\nA1-t100\n13\n0.0025\n0.5\n3.55\nA1-t1000\n29\n0.0003\n0.5\n3.79\nT 1-t100\n115\n0.001\n1\n3.01\nT 1-t1000\n40\n0.001\n1\n3.57\nStage 3\nA1-t100\n12\n0.0015\n0.5\n0.99\nA1-t1000\n23\n0.0003\n0.6\n0.86\nT 1-t100\n5\n0.0015\n0\n1.02\nT 1-t1000\n17\n0.0005\n0.5\n0.86\nTable 3: Results of the hyperparameter search\nfor Stage 2 & 3 of the ULMFiT approach.\na small margin on most metrics and as such in\nthis study it represents the best in class metrics\nfor classical machine learning in most cases. It’s\nmargin over SVM is typically small and an ex-\npected discrepancy.\nIn the case of deep transfer learning we\ncan see a huge diﬀerence in the performance of\nBERT and ULMFiT, our ﬁrst major conclusion\nDataset\nEpochs\nLearning Rate\nAccuracy\nA1-t100\n4\n2 · 10−5\n59.04\nA1-t1000\n3\n2 · 10−5\n64.95\nT 1-t100\n5\n2 · 10−5\n66.57\nT 1-t1000\n3\n1 · 10−5\n70.33\nTable 4: Final results for the best hyperparame-\nters in BERT after running a grid search across\nthe number of epochs and learning rates.\nis that ULMFiT does not perform adequately\nat these scales, performing 14.02 ± 6.18% worse\nthan BERT, and worse than both classical ap-\nproaches.\nAs such all of our results and con-\nclusions take the results of BERT as represent-\ning the best in class approach from deep transfer\nlearning.\nAn interesting result is the ﬁnding that deep\ntransfer learning is clearly the strongest perform-\ning paradigm for A self & T self at low scales\n(t100). It gains +10.4% accuracy on A datasets\nand +8.9% on T datasets. However, this gain ta-\npers oﬀas one moves to higher training set sizes,\nand at the t1000 level, this gain has fallen away\nto +1.9% and 1.7% on A and T respectively. We\nwould expect this trend to swing back in favour\nof deep transfer learning at a certain point, as\nthe number of examples continues to climb, given\nthat classical machine learning performance is\nknown to plateau with increased data sizes.\nIn the authors’ opinion, perhaps the key ﬁnd-\ning here is how robust the transfer learning per-\nformance is inter-domain.\nWe can see that at\nall tiers, transfer learning exhibits practically\nno loss in performance (max −0.7%) for intra-\ndomain tasks (A cross A and T cross T). On the\nother hand, classical machine learning loses sig-\nniﬁcant performance in A cross A (max −16.5%),\nalthough it should be noted that classical ma-\nchine learning does not generally lose perfor-\nmance in T cross T.\nThis robustness is further exempliﬁed when\nconsidering inter-domain performance. Compar-\ning A cross T to T self and T cross A to A self,\n9\nA Self\nA Cross A\nA Cross T\nT Self\nT Cross T\nT Cross A\nULMFiT\n45.0\n41.6\n37.9\n41.7\n41.2\n36.6\nBERT\n59.5\n58.8\n52.3\n55.5\n55.4\n63.4\nNa¨ıve Bayes\n49.1\n45.6\n36.6\n46.6\n46.0\n37.7\nSVM\n48.8\n45.8\n36.2\n46.4\n44.6\n36.3\nTransfer Best\n59.5\n58.8\n52.3\n55.5\n55.4\n63.4\nClassic Best\n49.1\n45.8\n36.6\n46.6\n46.0\n37.7\nTable 5: Final results for t100.\nA Self\nA Cross A\nA Cross T\nT Self\nT Cross T\nT Cross A\nULMFiT\n51.0\n48.2\n41.2\n44.1\n44.5\n37.7\nBERT\n62.1\n61.9\n55.0\n55.7\n55.6\n63.3\nNa¨ıve Bayes\n55.8\n50.0\n37.3\n49.5\n50.1\n38.7\nSVM\n53.4\n49.0\n36.4\n48.7\n48.8\n37.8\nTransfer Best\n62.1\n61.9\n55.0\n55.7\n55.6\n63.3\nClassic Best\n55.8\n50.0\n37.3\n49.5\n50.1\n38.7\nTable 6: Final results for t300.\nA Self\nA Cross A\nA Cross T\nT Self\nT Cross T\nT Cross A\nULMFiT\n56.6\n52.0\n42.2\n51.3\n51.5\n39.0\nBERT\n63.3\n63.3\n55.6\n55.7\n55.8\n63.3\nNa¨ıve Bayes\n61.3\n54.6\n38.3\n54.0\n56.4\n40.0\nSVM\n61.4\n54.9\n39.2\n52.6\n54.9\n40.3\nTransfer Best\n63.3\n63.3\n55.6\n55.7\n55.8\n63.3\nClassic Best\n61.4\n54.9\n39.2\n54.0\n56.4\n40.3\nTable 7: Final results for t1000.\nthe biggest loss in performance the best transfer\nlearning approach is 3.2% on t100 and on higher\ntiers this drops to 0.1%, compared to the best\nclassical approach which drops on average 11.4%\nfor t100 and 21.1% at t1000, not managing to show\nmuch improvement over baseline accuracy.\nThese results suggest that the deep transfer\nlearning approaches are capable of much deeper\nand more complex representations, such that\nthey can utilize previously learned features for\nnewer documents, even when the type of docu-\nment diﬀers signiﬁcantly in key properties such\nas length and vocabulary. On the other hand, the\nclassical machine learning approaches are able\nto perform reasonably well on the target task,\nbut perhaps by learning more superﬁcial pat-\nterns that don’t transfer to diﬀerent domains\nwell. This result is particularly useful in industry\nwhere one classiﬁer is used on a range of domains\n(sentiment classiﬁers being a noteworthy exam-\nple). When doing so using classical approaches,\none must be very careful and understand exactly\nhow, and on what dataset, the accuracy of the\nclassiﬁer was assessed. With transfer learning,\nthis inter-domain usage appears to be a much\nsafer practice.\nWe do notice that across the tiers for T self\nwe see a slightly better relative performance for\nclassical machine learning than in the case of A,\neven though it still generally performs worse than\ntransfer learning. As mentioned in section 2, our\nmain hypothesis for this is the unique language\n(be that slang, misspellings, niche topics) used in\nT causes the predetermined vocabularies of the\nlanguage models to miss a lot of the nuance in the\ntext, to varying degrees (BERTs Wordpiece em-\nbeddings should make it slightly more adaptable\nand we see that in the results). We would expect\nfurther relative losses in performance should the\ntarget domain have even more out of vocabulary\ntokens such as emoji on these base models. How-\never this potential limitation could be overcome\nwith a diﬀerent tokenization method and more\ndiverse datasets during the pre-training stage for\ndeep transfer learning.\n10\n7\nDiscussion\nBased on the conclusion in section 6, we would\nsuggest that deep transfer learning is generally\nthe better paradigm in low-shot classiﬁcation\ntasks. It is worth noting however that the ac-\ncuracies achieved are still on the low-end, and\nmay not be at a high enough quality for some\napplications.\nA core focus in this research is to aid practi-\ntioners in choosing which paradigm to use when\ntrying to solve real-world problems, as such par-\nticular consideration needs to be paid to the\navailability of a core language model for the lan-\nguage of the task in question. The original au-\nthors of both the ULMFiT and BERT papers\nonly made English models available upon release.\nSince then, BERT has released a multi-language\nmodel, however no dedicated single language\ncore models exist for either BERT or ULMFiT\noutside of English.\nThis can be a problem as\ntraining the core models is time-consuming and\nexpensive, and may not be feasible in many com-\npanies. As a corollary point, the base language\nmodel must have been pre-trained on a suitably\nwide dataset, or one that is similar to the target\ntask’s, as the vocabulary limitation mentioned\nin section 6 could play a signiﬁcant role. This\ncould mean even publicly available base models\nin the required language are unsuitable if the tar-\nget task is signiﬁcantly dissimilar.\nProduction resource requirements are an-\nother area of concern.\nThe BERT-base model\nrequires 12GB of VRAM to ﬁne-tune and run\n- at the time of writing this is considered high-\nend consumer hardware. Other models such as\nBERT-large or OpenAI GPT-2 (Radford et al.,\n2019), are much too big to ﬁt on consumer hard-\nware. This makes it out of reach for many re-\nsearchers and undesirable for companies who are\ntrying to minimize costs and are often reluctant\nto build pipelines that rely on expensive GPU\ncompute instances.\nContrary to this, classical\nML models are typically small and can be trained\nand run on any modern laptop.\nDespite these issues, deep transfer learning\ndoes oﬀer cost saving beneﬁts.\nIt saves a lot\nof human time in two key areas: feature cre-\nation and hyperparameter selection. When con-\nsidering the scalability in application of models\nthroughout a company, these two beneﬁts cou-\npled with the robustness of using models in cross\ndomain applications, can represent much bigger\ngains in return on investment than what is spent\non hardware costs. Furthermore, the hidden rep-\nresentations of deep transfer learning models can\nalso be used in sequence to sequence tasks, fur-\nther broadening their applicability to a range of\ntasks faced in practice.\nIn conclusion, the evidence presented in this\nwork suggests that deep transfer learning is the\nbest approach to use in low-shot learning tasks,\nowing to the ability to eﬀectively transfer mod-\nels into diﬀerent domains, the quick and easy\nimplementation of these freely available mod-\nels, as well as no longer needing to conduct\nexpensive and time-consuming hyperparameter\nsearches and training schedules.\nHowever, the\nperformance achieved on low-shot tasks suggests\nthere is still much work to be done in obtaining\nhigh quality classiﬁers for low-shot task settings.\nReferences\nBengio, Y., Ducharme, R., Vincent, P., & Jau-\nvin, C. (2003). A neural probabilistic lan-\nguage model. Journal of machine learning\nresearch, 3(Feb), 1137–1155.\nChen, H., Mckeever, S., & Delany, S.\n(2018,\n09). A comparison of classical versus deep\nlearning techniques for abusive content de-\ntection on social media sites. In (p. 117-\n133). doi: 10.1007/978-3-030-01129-1\\ 8\nDadvar, M., Trieschnigg, D., & de Jong, F.\n(2014). Experts and machines against bul-\nlies: A hybrid approach to detect cyberbul-\nlies. In Canadian conference on artiﬁcial\nintelligence (pp. 275–281).\nDavidson, T., Warmsley, D., Macy, M., & Weber,\nI. (2017). Automated hate speech detection\nand the problem of oﬀensive language.\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova,\nK. (2018). Bert: Pre-training of deep bidi-\nrectional transformers for language under-\nstanding.\nDinakar, K., Reichart, R., & Lieberman, H.\n(2011). Modeling the detection of textual\n11\ncyberbullying.\nIn ﬁfth international aaai\nconference on weblogs and social media.\nDonahue, J., Jia, Y., Vinyals, O., Hoﬀman, J.,\nZhang, N., Tzeng, E., & Darrell, T. (2013).\nDecaf: A deep convolutional activation fea-\nture for generic visual recognition.\nGoodfellow, I., Bengio, Y., & Courville, A.\n(2016). Deep learning. The MIT Press.\nHe, R., & McAuley, J. (2016). Ups and downs:\nModeling the visual evolution of fashion\ntrends with one-class collaborative ﬁlter-\ning.\nIn proceedings of the 25th interna-\ntional conference on world wide web (pp.\n507–517).\nHoward, J., & Ruder, S. (2018). Universal lan-\nguage model ﬁne-tuning for text classiﬁca-\ntion.\nLeskovec,\nJ.,\n&\nKrevl,\nA.\n(2014,\nJune).\nSNAP\nDatasets:\nStan-\nford\nlarge\nnetwork\ndataset\ncollection.\nhttp://snap.stanford.edu/data.\nMerity, S., Keskar, N. S., & Socher, R. (2017).\nRegularizing and optimizing lstm language\nmodels.\nMikolov, T., Chen, K., Corrado, G., & Dean, J.\n(2013). Eﬃcient estimation of word repre-\nsentations in vector space.\nNarayanan, V., Arora, I., & Bhatia, A. (2013).\nFast and accurate sentiment classiﬁcation\nusing an enhanced naive bayes model. In\nInternational conference on intelligent data\nengineering and automated learning (pp.\n194–201).\nPan, S. J., Ni, X., Sun, J.-T., Yang, Q., & Chen,\nZ. (2010). Cross-domain sentiment classi-\nﬁcation via spectral feature alignment. In\nProceedings of the 19th international con-\nference on world wide web (pp. 751–760).\nPang, B., Lee, L., & Vaithyanathan, S. (2002).\nThumbs up?: sentiment classiﬁcation us-\ning machine learning techniques. In Pro-\nceedings of the acl-02 conference on empiri-\ncal methods in natural language processing-\nvolume 10 (pp. 79–86).\nPeters, M., Neumann, M., Iyyer, M., Gardner,\nM., Clark, C., Lee, K., & Zettlemoyer,\nL.\n(2018).\nDeep contextualized word\nrepresentations.\nProceedings of the 2018\nConference of the North American Chapter\nof the Association for Computational Lin-\nguistics:\nHuman Language Technologies,\nVolume 1 (Long Papers). Retrieved from\nhttp://dx.doi.org/10.18653/v1/N18-1202\ndoi: 10.18653/v1/n18-1202\nRadford, A., Wu, J., Child, R., Luan, D.,\nAmodei, D., & Sutskever, I. (2019). Lan-\nguage models are unsupervised multitask\nlearners. OpenAI Blog, 1(8).\nRosenthal, S., Farra, N., & Nakov, P.\n(2017).\nSemeval-2017 task 4: Sentiment analysis\nin twitter. In Proceedings of the 11th inter-\nnational workshop on semantic evaluation\n(semeval-2017) (pp. 502–518).\nVaswani, A., Shazeer, N., Parmar, N., Uszkor-\neit, J., Jones, L., Gomez, A. N., . . . Polo-\nsukhin, I. (2017). Attention is all you need.\nWu, Y., Schuster, M., Chen, Z., Le, Q. V.,\nNorouzi, M., Macherey, W., . . . Dean, J.\n(2016).\nGoogle’s neural machine transla-\ntion system: Bridging the gap between hu-\nman and machine translation.\nYang, Z., Dai, Z., Yang, Y., Carbonell, J.,\nSalakhutdinov, R., & Le, Q. V. (2019). Xl-\nnet: Generalized autoregressive pretraining\nfor language understanding.\n12\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-07-17",
  "updated": "2019-07-17"
}