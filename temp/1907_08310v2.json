{
  "id": "http://arxiv.org/abs/1907.08310v2",
  "title": "Deep Perceptual Compression",
  "authors": [
    "Yash Patel",
    "Srikar Appalaraju",
    "R. Manmatha"
  ],
  "abstract": "Several deep learned lossy compression techniques have been proposed in the\nrecent literature. Most of these are optimized by using either MS-SSIM\n(multi-scale structural similarity) or MSE (mean squared error) as a loss\nfunction. Unfortunately, neither of these correlate well with human perception\nand this is clearly visible from the resulting compressed images. In several\ncases, the MS-SSIM for deep learned techniques is higher than say a\nconventional, non-deep learned codec such as JPEG-2000 or BPG. However, the\nimages produced by these deep learned techniques are in many cases clearly\nworse to human eyes than those produced by JPEG-2000 or BPG.\n  We propose the use of an alternative, deep perceptual metric, which has been\nshown to align better with human perceptual similarity. We then propose Deep\nPerceptual Compression (DPC) which makes use of an encoder-decoder based image\ncompression model to jointly optimize on the deep perceptual metric and\nMS-SSIM. Via extensive human evaluations, we show that the proposed method\ngenerates visually better results than previous learning based compression\nmethods and JPEG-2000, and is comparable to BPG. Furthermore, we demonstrate\nthat for tasks like object-detection, images compressed with DPC give better\naccuracy.",
  "text": "Deep Perceptual Compression\nYash Patel2 , Srikar Appalaraju1, R. Manmatha1\n1Amazon\n2Center for Machine Perception, Czech Technical University, Prague, Czech Republic\npatelyas@cmp.felk.cvut.cz, {srikara,manmatha}@amazon.com\nAbstract\nSeveral deep learned lossy compression techniques have\nbeen proposed in the recent literature. Most of these are\noptimized by using either MS-SSIM (multi-scale structural\nsimilarity) or MSE (mean squared error) as a loss function.\nUnfortunately, neither of these correlate well with human\nperception and this is clearly visible from the resulting com-\npressed images. In several cases, the MS-SSIM for deep\nlearned techniques is higher than say a conventional, non-\ndeep learned codec such as JPEG-2000 or BPG. However,\nthe images produced by these deep learned techniques are\nin many cases clearly worse to human eyes than those pro-\nduced by JPEG-2000 or BPG.\nWe propose the use of an alternative, deep perceptual\nmetric, which has been shown to align better with hu-\nman perceptual similarity. We then propose Deep Percep-\ntual Compression (DPC) which makes use of an encoder-\ndecoder based image compression model to jointly optimize\non the deep perceptual metric and MS-SSIM. Via extensive\nhuman evaluations, we show that the proposed method gen-\nerates visually better results than previous learning based\ncompression methods and JPEG-2000, and is comparable\nto BPG. Furthermore, we demonstrate that for tasks like\nobject-detection, images compressed with DPC give better\naccuracy.\n1. Introduction\nImage compression takes advantage of the redundancy\nof information in an image to reduce its size.\nBoth the\nstorage and network bandwidth required for the image are\nreduced by compression and for large datasets the savings\ncan be large. While there are lossless compression formats\nsuch as PNG [9]), the bigger reductions are obtained using\nlossy compression formats such as JPEG [43], JPEG2000\n[41] or BPG [6]). These lossy formats are handcrafted (not\nlearned). While learned image compression from data us-\ning neural networks is not new [29, 18, 26], there has re-\ncently been a resurgence of deep learning based techniques\nfor solving this problem[4, 5, 27, 34, 23]. These compres-\nsion schemes often consist of an encoder-decoder network.\nThe loss function usually trades-off distortion and the bit\nrate [38]. The encoder creates a latent embedding from the\nimage. With this embedding as input and a combination of\na quantizer and an entropy coder generates a compact bit-\nstream for storage. For decompression, the entropy coding\nis reversed to produce an embedding which is then fed into\na decoder to give a reconstructed approximate image as out-\nput.\nTo evaluate the quality of the reconstructed image with\nrespect to the original, measures such as structural simi-\nlarity (SSIM) [44] or PSNR (a function of MSE - mean\nsquared error) have been proposed in the past. In recent\nwork, multi-scale structure similarity (MS-SSIM) [45] has\nbecome more popular. PSNR and MS-SSIM were origi-\nnally formulated as perceptual metrics but don’t seem to\ncompletely capture certain type of distortions created by\nlearned compression methods.\nResearchers have naturally tried to directly optimize on\nmeasures such as MS-SSIM or MSE (i.e. PSNR) by us-\ning it as a loss function [27, 34, 19, 5, 23]. The choice\nof whether PSNR or MS-SSIM is used for evaluation dic-\ntates which loss function is used since optimizing for the\nevaluation metric ensures that the technique achieves a high\nnumber on it and a lower number on the other metric. Sev-\neral claims have been made that such approaches are better\ncompared to engineered compression formats due to their\nhigher MS-SSIM but as Figure 1 shows this misleading.\nFrom left to right we have four different techniques ranked\nin descending order of MS-SSIM values. It is clear that\nthe ﬁrst two images have many more artifacts than the last\ntwo images (the text is not readable in the ﬁrst two images).\nClearly, PSNR and MS-SSIM scores do not reﬂect image\nquality or human perception well. While such scores may\nbe reasonable for measuring engineered codecs which can-\nnot directly optimize these measures, deep learning tech-\nniques can directly optimize such metrics leading to this sit-\nuation. The work done by [32] experimentally proves (using\nhuman evaluation) that this problem is indeed not conﬁned\n1\narXiv:1907.08310v2  [eess.IV]  31 Jul 2019\nFigure 1. An example from the Kodak dataset. In order of MS-SSIM values Mentzer et al. [27] > Ball´e et al. [4] > BPG [6] > JPEG-2000\n[41]. However, visually the foreground and text in BPG and JPEG-2000 are clearly better in quality. Best viewed in color.\nto one image but occurs across different image compression\ndatasets.\nThis paper proposes deep perceptual compression (DPC)\n- a deep learning approach for image compression which\nuses a Learned Perceptual Image Patch Similarity (LPIPS)\nmetric [48] (deep perceptual metric) as a loss function.\nZhang et al [48] use a CNN to compute this metric. Since\na CNN in general computes a function, we use their CNN\nto compute the deep perceptual loss metric. This perceptual\nmetric was trained by Zhang et al [48] on user judgments\non distorted images. To regularize this network we com-\nbine the deep perceptual metric with an MS-SSIM loss in\na multi-task learning setup (Figure 3) and train the network\nend-to-end.\nImage generation models with deconvolution based up-\nsampling are known to generate certain checkerboard pat-\nterns with some losses as reported in [36, 30]. In an attempt\nto minimize checkerboard patters in the reconstructed im-\nages, we set the deconvolution up-sampling in a way that\nkernel sizes are divisible by strides to avoid overlap issue\n(see [30] for a detailed explanation of checkerboard pattern\nproblem).\nWe show that DPC is better (as judged by humans) than a\ncouple of deep learning techniques [27, 4] as well as JPEG-\n2000 at a number of bit-rates by doing experiments on sev-\neral standard compression datasets 1. DPC is better than\nBPG at some bit-rates while BPG is better at others. Since\nhumans are more sensitive to certain compression artifacts\nas compared to others, as an alternative to human judg-\nments, we take a pre-trained object detector (ResNet-101)\non the COCO-dataset and run it on the images output by\neach compression algorithm. Absent ﬁne-tuning all algo-\nrithms cause some degradation in the object detector perfor-\nmance but DPC suffers the least degradation while at higher\n1Since we use human judgments, and therefore, require images from\neach technique for all datasets we were constrained to using deep learning\ntechniques for which the researchers made models available and for this\nwe are thankful\nbit-rates BPG comes close.\nThe rest of the paper is structured as follows. In Sec-\ntion 2, related work is reviewed. In Section 3, the proposed\nmethod is described and in Section 4 experiments and re-\nsults are discussed. The paper is concluded in Section 5.\n2. Related Work\nWe discuss some related work on learned image com-\npression and perceptual image quality. Many compression\nmodels use autoencoders. One difference between models\nis how the entropy of the data is learned. The entropy model\nis jointly trained with the encoder and decoder with a rate-\ndistortion trade-off as a loss function [38] i.e L = βR+αD\n(R is the rate and D is distortion)). To learn optimal R for\na particular D, some have used a fully factorized entropy\nmodel [4, 42], others use context in the quantized space to\nimprove compression using auto-regressive [31] approaches\n[27, 34, 24]. [28] jointly use factorized and auto-regressive\napproaches to learn entropy.\nApart from the innovation in entropy modeling, some pa-\npers have improved on the encoder decoder architectures.\n[3] use GDN activation instead of RELU, [42] adopt ideas\nfrom super-resolution work to use pixel-shufﬂe [39] in the\ndecoder to better reconstruct the image. [37, 34, 2] use ad-\nversarial training. On metrics such as MS-SSIM or PSNR\nthey do better than JPEG [43], JPEG2000 [41] and some\ndo better than BPG [7]. However, as we discussed in the\nintroduction these metrics are misleading.\nIn the super-resolution literature [22, 19], it has been\nshown that comparing activations obtained from a VGG-\n16 [40] network trained for the classiﬁcation task on Ima-\ngeNet [35] may be used as a perceptual loss function. In\nother contexts, this approach has been used for neural style\ntransfer [15], for conditional image synthesis [10, 12]. Re-\ncently, Zhang et al.[47, 11] investigate the effectiveness of\nthese deep CNN’s as a perceptual similarity metric. They\nﬁrst show humans a triplet of images which include two\nOriginal\nDPC (Ours)\nDPC (Ours)\nOriginal\nMentzer et al.\nBallé et al.\nBPG\nJPEG-2000\nFigure 2. An example from Kodak dataset compressed using different techniquess. Note, DPC (ours) is sharper. Best viewed in color.\ndistorted versions of an image patch and the original patch\nand ask which distorted patch is closer to the original. They\ncreate a net where the feature responses of standard CNN\narchitectures such as AlexNet [21] or VGG-16 [40] (pre-\ntrained on ImageNet) are fed to layers which learn to output\ndistance metrics which reﬂect the low-level human judg-\nments.\nIn our work we show that using the deep perceptual met-\nric as a loss function leads to improved image compression\nresults as judged by humans. We do need to regularize this\nwith an MS-SSIM loss.\n3. Deep Perceptual Compression\n3.1. Compression Model\nWe adapt the architecture of Mentzer et al. [27] with\ncertain essential modiﬁcations to optimize on deep percep-\ntual loss. We explicitly keep certain components (such as\nquantization and entropy coding) of the original approach\n[27] to investigate the effect of the proposed deep percep-\ntual loss. An auto-encoder framework is used consisting\nof stacked residual blocks for the encoder and decoder. In\nthe bottleneck, a quantizer is used for lossy data transfor-\nmation and an auto-regressive [31] entropy model is used\nfor estimating the probability distribution in the quantized\nspace. Formally, the compression model consists of an En-\ncoder Eθ, a Decoder Dψ, a Quantizer Qc and an Entropy\ncoding model Entγ; where θ and φ are the learnable pa-\nrameters represented by a deep residual neural network, c\nis the number of centers for the lossy quantizer and γ is a\nlearnable parameters for the entropy model represented by\na 3D pixel-CNN [31]. All these modules are trained and op-\ntimized jointly on a Rate-Distortion loss. Please see Fig.3\nfor a high-level illustration of the model architecture.\nEncoder is a function that takes an image x and com-\nputes Eθ : RN →RM where N > M.\ni.e.\nfor an\ninput image x, we get a ﬂoat-point latent representation\nq = E(x) where q is a point in a M dimensional space.\nNext, the Quantizer Qc discretizes q to ˆq. It’s a bounded\ndiscrete space with c centers. Note, this is a lossy trans-\nformation. In this work we adopt the differentiable soft-\nquantization idea from [42, 1]. We use nearest neighbor\nassignments to compute ˆqi = Q (qi) := arg minj ∥qi −cj∥\nwhere C = c1, c2, ...cL ⊂R. In our model we use c = 6\ncenters.\nEntropy model: Furthermore, Entγ learns the prob-\nability distribution of the quantized co-efﬁcients ˆy\n=\nEntγ(ˆq).\nˆy is then losslessly encoded into a binary bit-\nstream using arithmetic coding. In this work (and in [27]),\na variant of an auto-regressive model called PixelCNN[31]\nis used which requires that the next quantized value is\nconditional on previously seen quantized values p(ˆq) =\nQm\ni=1 p (ˆqi|ˆqi−1, . . . , ˆq1).\nSoftmax-cross-entropy loss is\nused as the Rate R coding cost.\nImportance map: We also used variable bit-allocation,\nsince in images there is great variability in information con-\ntent across spatial locations. Speciﬁcally, we take the last\nlayer of Encoder Eθ and add a single-channel 2D output of\nFeature \nEncoder\n!\nMS-SSIM\nDPL\nQuantization\nCoding\nDecoding\nInput Image\nReconstructed Image\n!^\nBit-Rate Loss\nFeature \nDecoder\nFigure 3. High-level illustration of our image compression model. An input image is fed to the feature encoder, the obtained activations\nare quantized and entropy-coded. During decompression, these steps are reversed and the decoder outputs the reonstructed image. Purple\nboxes indicate loss functions, optimization is done jointly on the bit-rate loss and the two distortion losses (DPL and MS-SSIM).\n!\nR\nMultiply \nL2 norm \nSpatial Average\nDPL\nF(!)\nF(R)\nNormalize \nSubtract\nw\nAverage\nSum\nF(!)\nF(!)\n^\n!\n!^\nFigure 4. Deep Perceptual Loss: To compute perceptual similarity distance between the original x and recontructed ˆx images - ﬁrst the deep\nembeddings F(x) and F(ˆx) are computed for both, normalized along the channel dimensions, scale each channel by vector w (learned on\nperceptual similarity dataset), and take the ℓ2 norm. Finally average across spatial dimensions and sum across channels.\nthe form: y ∈R\nW\n8 × H\n8 ×1. This y is further expanded into\na mask m ∈R\nW\n8 × H\n8 ×K with the same dimensionality as\nq. The following rules determine the values of the map m:\nmi,j,k=\n\n\n\n\n\n\n\n1\nif k < yi,j\n(yi,j −k)\nif k ≤yi,j ≤k + 1\n0\nif k + 1 > yi,j\nwhere yi,j de-\nnotes the value at (i, j). This mask is then point-wise mul-\ntiplied with q i.e. q ←q ⊙⌈m⌉to give a spatially adaptive\nquantized feature map q. Please refer to the original work\n[27] for more speciﬁcs.\nFinally, the Decoder reconstructs the quantized latent\nvector ˆq back to an image, Dψ : RM →RN i.e. ˆx = Dψ(ˆq).\nThe goal is to learn a compact quantized latent represen-\ntation ˆq such that the distortion between the original im-\nage x and the reconstructed image ˆx is minimum. This is\nachieved by using a rate-distortion based loss function i.e.\nL = βrateR + αdistortionD. In Mentzer et al [27], MS-\nSSIM[45] is used for measuring distortion between images\nD.\nCheckerboard patterns: Image generation models with\ndeconvolution based up-sampling are known to generate\ncertain checkerboard patterns depending on the loss func-\ntion.\nA number of proposals have been made to solve\nthem[36, 30]. To minimize checkerboard patters in the re-\nconstructed images, we set deconvolution up-sampling in a\nway that kernel sizes are divisible by strides to avoid overlap\nissue. We refer the readers to [30] for more on the checker-\nboard pattern problem. Speciﬁcally, we use we kernels of\nsize 2 and stride 2 in the Decoder.\nThe combination of the above mentioned modules match\nthe state-of-the-art compression performance on Kodak\ndataset on MS-SSIM metric (circa CVPR 2018). Newer\nwork since then has shown some improvements on MS-\nSSIM or PSRN ([5, 28, 23]). However, as we pointed out in\nthe introduction MS-SSIM is not a good evaluation metric.\nWe will instead use human judgments and these models are\nnot available for generating images across several datasets\nfor human judgments. We now discuss the internals of Per-\nceptual loss and its effects on lossy image compression.\n3.2. Deep Perceptual Loss\nZhang et al. [48] show the utility of deep CNNs to mea-\nsure perceptual similarity. It has been observed that com-\nparing internal activations from deep CNNs such as VGG-\n16 [40] or AlexNet [21] acts as a better perceptual similarity\nmetric than MS-SSIM or PSNR. We use the deep perceptual\nmetric for both training and as one of the evaluation metric\non test data. We make use of activations from ﬁve ReLU\nlayers after each conv block in the VGG-16 [40] architec-\nture with batch normalizations.\nFeed-forward is performed on the VGG-16 for both the\noriginal (x) and reconstructed images (ˆx). Let L be the set\nof layers used for loss calculation (ﬁve for our setup) and,\na function F(x) denotes feed-forward on an input image x.\nF(x) and F(ˆx) return two stacks of feature activations for\nall L layers. The deep perceptual loss is then computed as\nfollows:\n• F(x) and F(ˆx) are unit-normalized in the channel di-\nmension.\nLet us call these, zl\nx, zl\nˆx ∈RHl×Wl×Cl\nwhere l ∈L. (Hl, Wl are the spatial dimensions of\nthe given activation map and Cl is the number of chan-\nnels).\n• zl\nx, zl\nˆx are scaled channel wise by multiplying with the\nvector wl ∈RCl\n• The ℓ2 distance is then computed and an average over\nspatial dimensions are taken.\n• Finally, a channel-wise sum is taken, outputing the\ndeep perceptual loss.\nEquation.\n1 and Figure.\n4 summarize the deep per-\nceptual loss computation. Note that the weights in F are\nlearned for image classiﬁcation on the ImageNet dataset\n[35] and are kept ﬁxed. w are the linear weights learned\non top of F on the Berkeley-Adobe Perceptual Patch Simi-\nlarity Dataset [47]. Note we use the trained model provided\nby Zhang et al. [48] to compute the loss.\nDPL(x, ˆx) =\nX\nl\n1\nHlWl\nX\nh,w\n||wl ⊙(zl\nˆx,h,w −zl\nx,h,w)||2\n2\n(1)\nFor training we make use of MS-SSIM for regulariza-\ntion, resulting in the ﬁnal distortation loss to be: D(x, ˆx) =\nDPL(x, ˆx)+λMSSSIM(x, ˆx). In practice, we use λ = 1\nin our training setup.\n3.3. Training Details\nWe make use of the Adam optimizer [20] with an initial\nlearning rate of 4 × 10−3 and a batch-size of 30. The learn-\ning rate is decayed by a factor of 10 in every two epochs\n(step-decay).\nThe overall loss function is:\nL(x, ˆx) =\nαD + βR, where R is the rate loss and D is a distortion\nloss, which for DPC is a linear combination of the deep\nperceptual loss (see 3.2) and MS-SSIM (weighted equally).\nFurther, similar to [27], we clip the rate term to max(t, βR)\nto make the model converge to a certain bit-rate, t. The\ntraining is done on the training set of ImageNet dataset\nthe from Large Scale Visual Recognition Challenge 2012\n(ILSVRC2012) [35], with the mentioned setup, we observe\nconvergence in six epochs.\nBy varying the model hyper-parameters such as the num-\nber of channels in the bottleneck, weight for distortion loss\n(α), target bit-rate (t), we obtain multiple models in the bit-\nper-pixel range of 0.15 to 1.0. Similarly we reproduce the\nmodels for [27, 4] at different bpp values. 2\n2Note that in the case of [4] we used an MS-SSIM loss instead of MSE\nloss as was done in the original paper but this does not change the general\nconclusions of the paper.\n4. Experiments\nWe extensively evaluate image compression techniques\nfor human perceptual similarity using a two alternative\nforced choice (2AFC) approach. 2AFC is a known way\nof performing perceptual similarity evaluation and has been\nused by [36] for evaluating super-resolution techniques.\nThe study is conducted on the Amazon MTurk platform\nwhere an evaluator is show the original image along with\nthe compressed images from two techniques on each side.\nThey are asked to choose the image which is more similar\nto the original. We show the entire image along with a syn-\nchronized (on all three) magnifying glass to observe ﬁner\ndetails. This gives them a global context of the whole im-\nage and at the same time provides a quick way to access\nlocal regions. No time limit was placed for this human ex-\nperiment.\nIn this setup, we compare the proposed DPC, two engi-\nneered (JPEG-2000 [41], BPG [6]) and two learning based\n(Mentzer et al. [27], Ball´e et al. [4]) compression tech-\nniques by choosing all possible combinations (ten pairs in\ntotal). Further, we do this at four different compression lev-\nels - i.e. bits-per-pixel (bpp) values: 0.23, 0.37, 0.67, 1.0.\nThe study is conducted on four standard datasets: Kodak\n[14], Urban100 [17], Set14 [46] and Set5 [8].\nWe have a total of 5720 pairs (10 pairs for ﬁve methods,\n4 bpp values and 143 images in total). For each such pair,\nwe obtain 5 evaluations resulting in a total of 28600 HITs.\n4.1. Image Compression Results\nFor each test dataset, we compress all the images using\neach model. For each model and each bpp we compute an\naverage for all images for each metric. We do the same for\ndifferent bpp’s so that we get multiple points on the deep\nperceptual metric vs bpp, MS-SSIM vs bpp and PSNR vs\nbpp curves. We interpolate the values between two such\npoints and we do not extrapolate the values outside the bpp\nrange. Note that for the deep perceptual metric, a lower\nvalue is better and for MS-SSIM and PSNR higher is better.\nFor human evaluation, for each image and a given bpp\nvalue we have 5 pair-wise votes in the form of method-A\nvs method-B. Since we have all possible pairs for the ﬁve\nmethods under consideration, we aggregate these votes and\nobtain the method which does best for the given image (at\na particular bpp value) based on maximum votes. In the\nFigures 5, 6, 7 and 8, we show the number of images (y-\naxis) for which a method performs best.\nThe comparisons for Kodak [14] are shown in Figure 5.\nWe observe that Mentzer et al. [27] despite the highest MS-\nSSIM score for all bpp values ranks 4th in the human study.\nBall´e et al. [4] obtains a higher MS-SSIM score compared\nto DPC, BPG and JPEG-2000 after 0.3 bpp, despite which\nit ranks worst among the ﬁve. DPC with the lowest deep\nperceptual metric scores performs better than Mentzer et\nFigure 5. Evaluation on the Kodak dataset [14] using the deep perceptual metric (left), MS-SSIM (middle) and human study (right). In this\ncase, Ball´e et al. is never the best method for any image. Best viewed in color.\nFigure 6. Evaluation on the Urban100 dataset [17] using the deep perceptual metric (left), MS-SSIM (middle) and human study (right).\nBest viewed in color.\nFigure 7. Evaluation on Set14 dataset [46] using the deep perceptual metric (left), MS-SSIM (middle) and human study (right). In this\ncase, Ball´e et al. is never the best method for any image. Best viewed in color.\nal. [27], Ball´e et al. [4] and JPEG-2000 at all bit-rates. The\nperformance is comparable to BPG and better at 0.37 bpp\nvalue. PSNR plots on all four datasets are shown in Fig-\nure. 9, it can be observed that the conventional methods\n(JPEG-2000, BPG) have signiﬁcantly higher PSNR scores,\nalthough DPC outperforms JPEG-2000 and is comparable\nto BPG in human study. These observations show that both\nPSNR and MS-SSIM are inadequate metrices to judge per-\nFigure 8. Evaluation on Set5 dataset [8] using the deep perceptual metric (left), MS-SSIM (middle) and human study (right). In this case,\nBall´e et al. is never the best method for any image. Best viewed in color.\nFigure 9. PSNR plots for all four datasets (from left to right): Kodak [14], Urban100 [17], Set14 [46] and Set5 [8]. Best viewed in color.\nFigure 10. Object detection on the validation set of MS-COCO dataset [25]. The plot shows Average Precision (AP) against varying\nbits-per-pixel for various compression methods. Best viewed in color.\nceptual similarity for learned compression techniques.\nSimilarly comparisons for other datasets are made in:\nFigure 6 for Urban100 [17], Figure 7 for Set14 [46] and\nFigure 8 for Set5 [8].\n4.2. Object Detection Results\nWhile the compressed images need to be perceptually\ngood, they should also be useful for subsequent computer\nvision tasks. It was observed by Dwibedi et al. [13] that\nfor object detectors such as Faster-RCNN [33] region-based\nconsistency is important and pixel level artifacts can signif-\nicantly affect the performace. In this section, we evaluate\ndifferent compression techniques for a subsequent task of\nobject detection on MS-COCO validation dataset [25].\nWe use a pre-trained Faster-RCNN [33] model with a\nResNet−101[16] based backbone for its relatively high av-\nerage precision and capability to detect smaller objects. The\nperformance is measured using, average precision (AP), AP\nis the average over multiple IoU (the minimum IoU to con-\nsider a positive match). We use AP@[.5:.95] which corre-\nsponds to the average AP for IoU from 0.5 to 0.95 with a\nstep size of 0.05. With the original MS-COCO images, this\nmodel attains a performance of 40.1% AP . For each com-\npression method, we compress and reconstruct the image\nat four different bit-rate values: 0.23, 0.37, 0.67, 1.0 (same\nvalues as used for human evaluation) and then we evaluate\nthem for object detection. The performance of competing\ncompression methods are reported in Figure. 10. It can be\nclearly seen that at low bit-rates the proposed DPC signiﬁ-\ncantly outperforms the competing methods. At 1.0 bit-rate,\nthe performance is very close to that of BPG. Please note,\nas we did not ﬁne-tune networks with the compressed im-\nages, there is degradation in performance from the current\nstate-of-the-art.\n5. Conclusions\nWe have demonstrated that using a deep perceptual met-\nric as a loss with MS-SSIM as a regularizer one can obtain\ngood image compression as judged by humans on several\nstandard compression datasets. MS-SSIM and PSNR are\nnot good metrics for evaluating image compression and hu-\nman judgments are more reliable. We also show that DPC\ncompression causes less degradation in a pre-trained object\ndetector than a number of other approaches.\nAcknowledgment\nWe would like to thank Joel Chan and Peter Hallinan for\nhelping us in setting up the human evaluations.\nReferences\n[1] E. Agustsson, F. Mentzer, M. Tschannen, L. Cavigelli,\nR. Timofte, L. Benini, and L. V. Gool.\nSoft-to-hard vec-\ntor quantization for end-to-end learning compressible repre-\nsentations. In Advances in Neural Information Processing\nSystems, pages 1141–1151, 2017.\n[2] E. Agustsson,\nM. Tschannen,\nF. Mentzer,\nR. Timo-\nfte, and L. Van Gool.\nGenerative adversarial networks\nfor extreme learned image compression.\narXiv preprint\narXiv:1804.02958, 2018.\n[3] J. Ball´e, V. Laparra, and E. P. Simoncelli. Density modeling\nof images using a generalized normalization transformation.\narXiv preprint arXiv:1511.06281, 2015.\n[4] J. Ball´e,\nV. Laparra,\nand E. P. Simoncelli.\nEnd-\nto-end optimized image compression.\narXiv preprint\narXiv:1611.01704, 2016.\n[5] J. Ball´e, D. Minnen, S. Singh, S. J. Hwang, and N. Johnston.\nVariational image compression with a scale hyperprior. arXiv\npreprint arXiv:1802.01436, 2018.\n[6] F. Bellard. Bpg image format, 2014.\n[7] F. Bellard.\nBpg image format (http://bellard.org/bpg/)\naccessed:\n2017-01-30.\n[online]\navailable:\nhttp://bellard.org/bpg/. 2017.\n[8] M. Bevilacqua, A. Roumy, C. Guillemot, and M. L. Alberi-\nMorel. Low-complexity single-image super-resolution based\non nonnegative neighbor embedding. 2012.\n[9] T. Boutell.\nPng (portable network graphics) speciﬁcation\nversion 1.0. Technical report, 1997.\n[10] Q. Chen and V. Koltun. Photographic image synthesis with\ncascaded reﬁnement networks. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pages 1511–\n1520, 2017.\n[11] T. Chinen, J. Ball´e, C. Gu, S. J. Hwang, S. Ioffe, N. John-\nston, T. Leung, D. Minnen, S. O’Malley, C. Rosenberg, et al.\nTowards a semantic perceptual image metric. In 2018 25th\nIEEE International Conference on Image Processing (ICIP),\n2018.\n[12] A. Dosovitskiy and T. Brox. Generating images with percep-\ntual similarity metrics based on deep networks. In Advances\nin neural information processing systems, 2016.\n[13] D. Dwibedi, I. Misra, and M. Hebert. Cut, paste and learn:\nSurprisingly easy synthesis for instance detection. In Pro-\nceedings of the IEEE International Conference on Computer\nVision, pages 1301–1310, 2017.\n[14] R. Franzen. Kodak lossless true color image suite. source:\nhttp://r0k. us/graphics/kodak, 4, 1999.\n[15] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style trans-\nfer using convolutional neural networks. In Proceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition, 2016.\n[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition. In Proc. Conf. Comput. Vision Pattern\nRecognition, pages 770–778, 2016.\n[17] J.-B. Huang, A. Singh, and N. Ahuja. Single image super-\nresolution from transformed self-exemplars. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, 2015.\n[18] J. Jiang. Image compression with neural networks–a survey.\nSignal processing: image Communication, 1999.\n[19] N. Johnston, D. Vincent, D. Minnen, M. Covell, S. Singh,\nT. Chinen, S. J. Hwang, J. Shor, and G. Toderici. Improved\nlossy image compression with priming and spatially adaptive\nbit rates for recurrent networks. structure, 10:23, 2017.\n[20] D. P. Kingma and J. Ba. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980, 2014.\n[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\nImagenet\nclassiﬁcation with deep convolutional neural networks. In\nNeural Inform. Process. Syst., pages 1097–1105, 2012.\n[22] C. Ledig, L. Theis, F. Husz´ar, J. Caballero, A. Cunningham,\nA. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, et al.\nPhoto-realistic single image super-resolution using a genera-\ntive adversarial network. In Proceedings of the IEEE confer-\nence on computer vision and pattern recognition, 2017.\n[23] J. Lee, S. Cho, and S.-K. Beack. Context-adaptive entropy\nmodel for end-to-end optimized image compression. ICLR,\n2019.\n[24] M. Li, W. Zuo, S. Gu, D. Zhao, and D. Zhang. Learning con-\nvolutional networks for content-weighted image compres-\nsion. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 3214–3223, 2018.\n[25] T. Lin, M. Maire, S. J. Belongie, L. D. Bourdev, R. B.\nGirshick, J. Hays, P. Perona, D. Ramanan, P. Doll´ar, and\nC. L. Zitnick. Microsoft COCO: common objects in context.\nCoRR, abs/1405.0312, 2014.\n[26] S. Luttrell. Image compression using a neural network. In\nProc. IGARSS, 1988.\n[27] F. Mentzer, E. Agustsson, M. Tschannen, R. Timofte, and\nL. Van Gool. Conditional probability models for deep im-\nage compression. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2018.\n[28] D. Minnen, J. Ball´e, and G. D. Toderici. Joint autoregressive\nand hierarchical priors for learned image compression. In\nAdvances in Neural Information Processing Systems, pages\n10771–10780, 2018.\n[29] P. Munro and D. Zipser. Image compression by back propa-\ngation: an example of extensional programming. Models of\ncognition: rev. of cognitive science, 1989.\n[30] A. Odena, V. Dumoulin, and C. Olah. Deconvolution and\ncheckerboard artifacts. Distill, 2016.\n[31] A. v. d. Oord, N. Kalchbrenner, and K. Kavukcuoglu. Pixel\nrecurrent neural networks. arXiv preprint arXiv:1601.06759,\n2016.\n[32] Y. Patel, S. Appalaraju, and R. Manmatha. Human evalua-\ntions for image compression. 2019.\n[33] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards\nreal-time object detection with region proposal networks. In\nAdvances in neural information processing systems, pages\n91–99, 2015.\n[34] O. Rippel and L. Bourdev. Real-time adaptive image com-\npression. arXiv preprint arXiv:1705.05823, 2017.\n[35] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\net al. Imagenet large scale visual recognition challenge. In-\nternational journal of computer vision, 2015.\n[36] M. S. Sajjadi, B. Sch¨olkopf, and M. Hirsch. Enhancenet:\nSingle image super-resolution through automated texture\nsynthesis. In Computer Vision (ICCV), 2017 IEEE Interna-\ntional Conference on, pages 4501–4510. IEEE, 2017.\n[37] S. Santurkar, D. Budden, and N. Shavit. Generative com-\npression. In 2018 Picture Coding Symposium (PCS), pages\n258–262. IEEE, 2018.\n[38] C. E. Shannon. A mathematical theory of communication.\nBell system technical journal, 27(3):379–423, 1948.\n[39] W. Shi, J. Caballero, F. Husz´ar, J. Totz, A. P. Aitken,\nR. Bishop, D. Rueckert, and Z. Wang.\nReal-time single\nimage and video super-resolution using an efﬁcient sub-\npixel convolutional neural network. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 2016.\n[40] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. arXiv preprint\narXiv:1409.1556, 2014.\n[41] A. Skodras, C. Christopoulos, and T. Ebrahimi. The jpeg\n2000 still image compression standard. IEEE Signal pro-\ncessing magazine, 2001.\n[42] L. Theis, W. Shi, A. Cunningham, and F. Husz´ar.\nLossy\nimage compression with compressive autoencoders. arXiv\npreprint arXiv:1703.00395, 2017.\n[43] G. K. Wallace. The jpeg still picture compression standard.\nIEEE transactions on consumer electronics, 1992.\n[44] Z. Wang, A. C. Bovik, H. R. Sheikh, E. P. Simoncelli, et al.\nImage quality assessment: from error visibility to structural\nsimilarity. IEEE transactions on image processing, 2004.\n[45] Z. Wang, E. P. Simoncelli, and A. C. Bovik. Multiscale struc-\ntural similarity for image quality assessment. In The Thrity-\nSeventh Asilomar Conference on Signals, Systems & Com-\nputers, 2003, volume 2, pages 1398–1402. Ieee, 2003.\n[46] R. Zeyde, M. Elad, and M. Protter. On single image scale-up\nusing sparse-representations. In International conference on\ncurves and surfaces. Springer, 2010.\n[47] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang.\nThe unreasonable effectiveness of deep features as a percep-\ntual metric. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 586–595, 2018.\n[48] R. Zhang, P. Isola, E. Efros A.A., Schectman, and W. O. The\nunreasonable effectiveness of deep features as a perceptual\nmetric. In CVPR, 2018.\n",
  "categories": [
    "eess.IV",
    "cs.CV"
  ],
  "published": "2019-07-18",
  "updated": "2019-07-31"
}