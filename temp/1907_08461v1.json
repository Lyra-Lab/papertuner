{
  "id": "http://arxiv.org/abs/1907.08461v1",
  "title": "Delegative Reinforcement Learning: learning to avoid traps with a little help",
  "authors": [
    "Vanessa Kosoy"
  ],
  "abstract": "Most known regret bounds for reinforcement learning are either episodic or\nassume an environment without traps. We derive a regret bound without making\neither assumption, by allowing the algorithm to occasionally delegate an action\nto an external advisor. We thus arrive at a setting of active one-shot\nmodel-based reinforcement learning that we call DRL (delegative reinforcement\nlearning.) The algorithm we construct in order to demonstrate the regret bound\nis a variant of Posterior Sampling Reinforcement Learning supplemented by a\nsubroutine that decides which actions should be delegated. The algorithm is not\nanytime, since the parameters must be adjusted according to the target time\ndiscount. Currently, our analysis is limited to Markov decision processes with\nfinite numbers of hypotheses, states and actions.",
  "text": "arXiv:1907.08461v1  [cs.LG]  19 Jul 2019\nPublished as a conference paper at ICLR 2019\nDELEGATIVE REINFORCEMENT LEARNING:\nLEARN-\nING TO AVOID TRAPS WITH A LITTLE HELP\nVanessa Kosoy\nIndependent Researcher\nPetah Tikva, Israel\nvanessa.kosoy@intelligence.org\nABSTRACT\nMost known regret bounds for reinforcement learning are either episodic or as-\nsume an environment without traps. We derive a regret bound without making\neither assumption, by allowing the algorithm to occasionally delegate an action\nto an external advisor. We thus arrive at a setting of active one-shot model-based\nreinforcement learning that we call DRL (delegative reinforcement learning.) The\nalgorithm we construct in order to demonstrate the regret bound is a variant of\nPosterior Sampling Reinforcement Learning supplemented by a subroutine that\ndecides which actions should be delegated. The algorithm is not anytime, since\nthe parameters must be adjusted according to the target time discount. Currently,\nour analysis is limited to Markov decision processes with ﬁnite numbers of hy-\npotheses, states and actions.\n1\nINTRODUCTION\nA reinforcement learning agent is a system that interacts with an unknown environment in a manner\nthat is designed to maximize the expectation of a utility function that can be written as a sum of\nrewards over time (sometimes weighted by a time-discount function.) A standard metric for evalu-\nating the performance of such an agent is the regret: the difference between the expected utility of\nthe agent in a given environment, and the expected utility of an optimal policy for the same environ-\nment. This metric allows formalizing the notion of “the agent learns the environment” by requiring\nthat the regret has sublinear growth in the planning horizon (usually assuming the utility function is\na ﬁnite, undiscounted, sum of rewards.) For example, if we consider stateless environments, rein-\nforcement learning reduces to a multi-armed bandit for which algorithms with guaranteed sublinear\nregret bounds are well-known (see e.g. Bubeck & Cesa-Bianchi (2012).)\nHowever, the desideratum of sublinear regret is impossible to achieve even for a ﬁnite class of\nenvironments without making further assumptions, and this is because of the possible presence of\n“traps”. A trap is a state which, once reached, forces a linear lower bound on regret. Consider the\nfollowing example. The agent starts at state s1, and as long as it takes action a, it receives a reward\nof 1. However, if it ever takes action b, it will reach state s2 and remain there, receiving a reward\nof 0 forever, whatever it does. Thus, s2 is a trap. On the other hand, it is impossible to design an\nalgorithm which guarantees never entering traps for an arbitrary environment. For example, consider\nthe environment that has the same structure except actions a and b are exchanged. In this case, if\nthe transition matrix is not known a priori, no algorithm can learn the correct behavior, and every\nalgorithm will have linear regret in at least one of the two environments.\nThere are two widespread approaches to deriving regret bounds which circumvent this problem.\nOne is simply assuming that the environment contains no traps in some formal sense (see e.g.\nNguyen et al. (2013).) The other is “episodic learning” (see e.g. Osband & Van Roy (2014).) In\nepisodic learning, the timeline is divided into intervals (“episodes”) and, either the state is assumed\nto reset to the initial state after each episode, or regret is deﬁned s.t. the contribution of each episode\nis the difference between following the given policy and following the given policy during previous\nepisodes but the optimal policy in the current episode. The latter metric doesn’t consider entering\na trap to be a fatal event, since in the following episodes this event will be considered as “given.”\nThat is, a policy that enters trap can still achieve sublinear regret in this sense. In fact, algorithms\n1\nPublished as a conference paper at ICLR 2019\ndesigned to achieve sublinear regret for sufﬁciently general classes of environments have the prop-\nerty that they eventually enter every trap they encounter (such algorithms have a random exploration\nphase, like e.g. ǫ-exploration in Q-learning.)\nIn terms of practical applications, it means that most known approaches to reinforcement learning\nthat have theoretical performance guarantees either assume that no mistake is “fatal”, or that numer-\nous “fatal” mistakes in the training process are acceptable. These assumptions are unacceptable in\napplications such as controlling a very expensive, breakable piece of machinery (e.g. spaceship) or\nperforming a task that involves signiﬁcant risk to human lives (e.g. surgery or rescue,) assuming\nthat the algorithm cannot be reliably trained in a simulation since the simulation doesn’t reﬂect all\nthe intricacies of the physical world.\nThis problem clearly cannot be overcome without using prior knowledge about the environment.\nIn itself, prior knowledge is not such a strong assumption, since at least for any task that can be\naccomplished by a person, this prior knowledge is already available to us. The challenge is then\ntransferring this knowledge to algorithm. This transfer can be accomplished either by manually\ntransforming the knowledge into a formal mathematical speciﬁcation, or by establishing a learning\nprotocol that involves a human in the loop. Since human knowledge is often complex, difﬁcult to\nformalise and partly intuitive, the latter option seems especially attractive.\nThese idea of using prior knowledge or human intervention to avoid traps has been explored by sev-\neral authors (see Garc´ıa & Fern´andez (2015) for a survey.) However, to the best of our knowledge,\nno previous author has established a regret bound in such a setting. In the present work, we derive\nsuch a regret bound, speciﬁcally for the setting that Clouse (1997) called “ask for help” and we\ncall “delegative reinforcement learning” (DRL), and speciﬁcally for a class of environments which\nconsists of some ﬁnite number of Markov decision processes with a ﬁnite number of states.\nIn DRL, an agent interacts with an environment during an inﬁnite sequence of “rounds”. On each\nround, the agent selects an action and the environment transits to a new state which is observed by\nthe agent. The agent then receives a reward which depends on the state. There are two kinds of\nactions the agent can take: a “direct” action a ∈A and the special delegation action ⊥. If the agent\ntakes action ⊥, the advisor takes some action b ∈A which affects the environment in the same way\nas if it was taken directly. The agent then observes both b and the new state of the environment. The\nutility function and regret are deﬁned via geometric time discount with a constant γ.\nThe algorithm we construct in order to show the regret bound is a variant of posterior sampling\nreinforcement learning (see Osband et al. (2013)). Denoting α := 1 −γ, the timeline is divided\ninto intervals of length O\n\u0000α−1/4\u0001\n. At the start of each interval, the algorithm samples a hypothesis\nout of its current belief state, and starts carrying out an optimal policy for this hypothesis. On\neach round, it checks whether the desired action is known to be “safe” with high probability in a\nparticular formal sense. If it is safe, the action is taken. If it isn’t safe, delegation is performed.\nMoreover, the belief state evolves using all observations, but hypotheses whose probability falls\nbelow O\n\u0000α\n1/4\u0001\nare discarded altogether. We then show that (i) given relatively mild assumptions\nabout the advisor (namely, that it only takes safe actions and it takes the optimal action with at least\nsome small probability,) the regret is bounded by O\n\u0000α−3/4\u00011 (in particular it is sublinear in α−1)\nand (ii) the number of delegations behaves like O\n\u0000α−1/4\u00012. Here, we only gave the dependence\non α, but the expressions we obtain are more detailed and reﬂect the dependence on the number of\nhypothesis (which we assume to be ﬁnite), the derivative of the value functions of the hypotheses\nand the minimal probability with which the advisor takes an optimal action.\nThe structure of the paper is as follows. Section 2 gives all the necessary deﬁnition and formally\nstates the results. Appendix A explains the algorithm implicit in the main theorem and gives an\noutline of the proofs. Appendix B completes the details of the proofs.\n1See inequality (23). In our notation, regret is normalized by a factor of α to lie within [0, 1] (see Deﬁni-\ntion 7) so the bound is O(α\n1/4).\n2See inequality (24).\n2\nPublished as a conference paper at ICLR 2019\n2\nRESULTS\nWe start by recalling some basic deﬁnitions and properties of Markov decision processes. See e.g.\nFeinberg & Shwartz (2002) for a detailed overview with proofs. First, some notation.\nGiven measurable spaces X and Y , the notation K : X\nk−→Y means that K is a Markov kernel\nfrom X to Y . Given x ∈X, K(x) is the corresponding probability measure on Y . Given A ⊆Y\nmeasurable, K(A | x) := K(x)(A). Given y ∈Y , K(y | x) := K ({y} | x). Given J : Y\nk−→Z,\nJK : X\nk−→Z is the composition of J and K, and when Y = X, Kn is the n-th composition power.\nDeﬁnition 1. A (ﬁnite) Markov decision process (MDP) is a tuple\nM :=\n\u0010\nSM, AM, sM ∈SM, TM : SM × AM\nk−→SM, RM : SM →[0, 1]\n\u0011\nHere, SM is a ﬁnite set (the set of states,) AM is a non-empty ﬁnite set (the section of actions,) sM\nis the initial state, TM is the transition kernel and RM is the reward function3.\nDeﬁnition 2. Given M an MDP and some π : SM →AM, we deﬁne TMπ : SM\nk−→SM by\nTMπ(t | s) := TM (t | s, π(s))\n(1)\nThat is, TMπ is the transition kernel of the Markov chain resulting from policy π interacting with\nenvironment M.\nDeﬁnition 3. Given M an MDP, we deﬁne VM : SM × [0, 1) →[0, 1] and QM : SM × AM ×\n[0, 1) →[0, 1] by\nVM(s, γ) := (1 −γ)\nmax\nπ:SM→AM\n∞\nX\nn=0\nγn\nE\nT n\nMπ(s) [RM]\n(2)\nQM(s, a, γ) := (1 −γ)RM(s) + γ\nE\nt∼TM(s,a) [VM(t, γ)]\n(3)\nThus, VM(s, γ) is the maximal value that can be extracted from state s and QM(s, a, γ) is the\nmaximal value that can be extracted from state s after performing action a.\nDeﬁnition 4. Given M an MDP, we deﬁne V0\nM : SM →[0, 1] and Q0\nM : SM × AM →[0, 1] by\nV0\nM(s) := lim\nγ→1 VM(s, γ)\n(4)\nQ0\nM(s, a) := lim\nγ→1 QM(s, a, γ)\n(5)\nThe limits above are guaranteed to exist, thanks to our assumptions that S and A are ﬁnite.\nGiven a set A, the notation P (A) denotes the power set of A.\nDeﬁnition 5. Given M an MDP, we deﬁne A0\nM : SM →P (AM) by\nA0\nM(s) := arg max\na∈AM\nQ0\nM(s, a)\n(6)\nThat is, A0\nM(s) is the set of actions at state s that don’t enter traps (i.e. destroy value in the long\nrun.)\nDeﬁnition 6. Given M an MDP, it is well known that there are A⋆\nM : SM →P (AM) (the set of\nBlackwell optimal actions: see Feinberg & Shwartz (2002) chapter 8) and γM ∈[0, 1) s.t. for any\nγ ∈(γM, 1)\n3Sometimes the reward is assumed to depend on the action as well, or on the action and the next state, but\nthese formalisms are easily seen to be equivalent via redeﬁnitions of the state set.\n3\nPublished as a conference paper at ICLR 2019\nA⋆\nM(s) = arg max\na∈AM\nQM (s, a, γ)\n(7)\nThus, A⋆\nM(s) is the set of actions that are optimal at state s, assuming that we plan for sufﬁciently\nlong term.\nGiven a measurable space X, we denote ∆X the space of probability measures on X. Given a set\nA, the notation A∗will denote the set of ﬁnite strings over alphabet A, i.e.\nA∗:=\n∞\nG\nn=0\nAn\nAω denotes the space of inﬁnite strings over alphabet A, equipped with the product topology and\nthe corresponding Borel sigma-algebra. Given x ∈Aω and n ∈N, xn ∈A is the n-th symbol of\nthe string x (in our conventions, 0 ∈N so the string begins from the 0th symbol.) Given h ∈A∗\nand x ∈Aω, the notation h ⊏x means that h is a preﬁx of x.\nConsider an MDP M and some π : S∗\nM × SM\nk−→AM. We think of π as a policy, where the ﬁrst\nargument is the past history of states and the second argument is the current state. We denote Mπ ∈\n∆Sω\nM the probability measure over histories resulting from policy π interacting with environment\nM. That is, on each time step we sample an action from π applied to previous history and last state,\nand sample a new state from TM applied to last state and sampled action.\nDeﬁnition 7. Given an MDP M and some π : S∗\nM × SM\nk−→AM, we deﬁne UM : Sω\nM × [0, 1) →\n[0, 1] (the utility function,) EUπ\nM : [0, 1) →[0, 1] (expected utility of policy π,) EU⋆\nM : [0, 1) →\n[0, 1] (maximal expected utility) and Regπ\nM : [0, 1) →[0, 1] (regret of policy π) by\nUM(x, γ) := (1 −γ)\n∞\nX\nn=0\nγnRM (xn)\n(8)\nEUπ\nM(γ) :=\nE\nx∼Mπ [UM(x, γ)]\n(9)\nEU⋆\nM(γ) :=\nmax\nπ:S∗\nM×SM\nk−→AM\nEUπ\nM(γ) = VM (sM, γ)\n(10)\nRegπ\nM(γ) := EU⋆\nM(γ) −EUπ\nM(γ)\n(11)\nNext, we deﬁne the properties of a policy that make it a “satisfactory” advisor.\nGiven X a topological space and µ a Borel measure on X, supp µ ⊆X denotes the support of µ.\nDeﬁnition 8. Consider M an MDP, some ǫ ∈(0, 1) and some υ : SM\nk−→AM. υ is called ǫ-sane\nfor M when for any s ∈SM,\ni. supp υ(s) ⊆A0\nM (s)\nii. There is a ∈A⋆\nM(s) s.t. υ(a | s) > ǫ\nSo, a policy is ǫ-sane when it doesn’t enter traps (destroys long-term value) and when it has a\nprobability of more than ǫ to take a long-term optimal action.\nNext, we introduce a formalism describing a system of two agents where one (the “robot”) can\ndelegate actions to another (the “advisor”.)\nDeﬁnition 9. Given an MDP M and some υ : SM\nk−→AM (the advisor policy), we deﬁne the MDP\nM [υ] (the environment as perceived by the robot) by\n4\nPublished as a conference paper at ICLR 2019\nAM[υ] := AM ⊔{⊥}\n(12)\nSM[υ] := SM × AM[υ]\n(13)\nsM[υ] := (sM, ⊥)\n(14)\nTM[υ] ((t, c) | (s, b) , a) :=\n\n\n\nTM (t | s, a) if a ̸= ⊥and c = ⊥\nTM (t | s, c) υ (c | s) if a = ⊥and c ̸= ⊥\n0 otherwise\n(15)\nRM[υ](s, b) := RM(s)\n(16)\nHere, the action ⊥represents delegation and the AM[υ] factor in SM[υ] represents the action taken\nby the advisor in the last round (or ⊥if there was no delegation.)\nWe will also use the following shorthand notations\nDeﬁnition 10. Given any MDP M and γ ∈(γM, 1), we deﬁne\ntM(γ) := max\ns∈SM\nsup\nθ∈(γ,1)\n\f\f\f\f\ndVM(s, θ)\ndθ\n\f\f\f\f\n(17)\nThe above quantity is closely related to the bias span parameter, which is known to ﬁgure in regret\nbounds in the no-traps setting (see Bartlett (2009)). Intuitively, it measures how costly can a non-\nfatal error be (the normalized value lost as a result of such an error is approximately bounded by\n(1−γ)tM(γ)). It can also be related to the mixing time of the Markov chain resulting from following\nthe optimal policy in the MDP (if P is the maximal period of the chain, and the total variation\ndistance from equilibrium falls as Fλn, then tM ≤F 1+λ\n1−λ + P), but discussing this in detail is out\nof the present scope.\nDeﬁnition 11. Given any MDP M, we deﬁne DM : (SM × (AM ⊔{⊥}))ω →N by\nDM(x) := |{n ∈N | xn ∈SM × AM}|\n(18)\nWe think of DM(x) as the number of delegations in an inﬁnite history x of the MDP M [υ] for some\nυ.\nWe can now formulate the main theorem.\nFor any n ∈N, we use the notation\n[n] := {m ∈N | m < n}\nWe also denote\nN+ := {n ∈N | n > 0}\nTheorem 1. There is some constant C ∈(0, ∞) s.t. the following holds. Fix some ǫ, η ∈(0, 1),\nT ∈N+, non-empty ﬁnite sets S, A, some s0 ∈S and some R : S →[0, 1]. Consider some\nN ∈N which is ≥2,\nn\nT k : S × A\nk−→S\no\nk∈[N] and\nn\nυk : S\nk−→A\no\nk∈[N]. We regard the pairs\n(T k, υk) as the set of hypotheses, where T k represents the transition kernel and υk the advisor\npolicy. Assume that for each k ∈[N], υk is ǫ-sane for the MDP M k :=\n\u0000S, A, s0, T k, R\n\u0001\n. Denote\nA• := A⊔{⊥} and S• := S×A•. Fix some γ ∈(0, 1) s.t. for each k ∈[N], γMk < γ. Also, denote\nLk := M k \u0002\nυk\u0003\nand ¯t :=\n1\nN\nPN−1\nk=0 tMk(γ) (see Deﬁnition 10). Then, there is π† : S∗\n• × S•\nk−→A•4\ns.t.\n4π† implicitly depends on γ: in this sense, it is not anytime. It also depends on η, T and the set of\nhypotheses.\n5\nPublished as a conference paper at ICLR 2019\n1\nN\nN−1\nX\nk=0\nRegπ†\nLk(γ) ≤C\n \nηN +\n¯t\nT +\ns\n(1 −γ)T ln N\nη\n+ (1 −γ)T ln N\nη2\n\u00121\nǫ + |A|\n\u0013!\n(19)\n∀K ∈N : 1\nN\nN−1\nX\nk=0\nPr\nLkπ† [DMk > K] ≤C\n\u0012\nηN + ln N\nKη\n\u00121\nǫ + |A|\n\u0013\u0013\n(20)\nThat is, we have a Bayesian regret bound for learning the true MDP starting from a prior that is\na uniform distribution over N hypotheses, each of which is a joint hypothesis about the transition\nkernel and the advisor. The bound is formulated in terms of N. It trivially implies a worst-case\nregret bound as well, at the cost of another factor of N. No doubt it is possible to derive other type\nof regret bounds for the DRL setting, e.g. in terms of the number of states and actions, but we leave\nit for future work.\nObserve that Theorem 1 is non-trivial even without equation 20, since, Deﬁnition 8 is s.t. a policy\nthat always delegates might fail to achieve any meaningful regret bound. Indeed, we can consider\nthe special case of a multi-armed bandit, in which all actions are safe and therefore even the random\npolicy is ǫ-sane (as long as ǫ <\n1\n|A|). Such a policy has normalized regret Ω(1), except for the\ndegenerate case when all actions have the same reward.\nNote that η and T are external parameters of the policy that we can choose however we like (η is a\nprobability threshold below which we stop considering hypotheses, and T is the length of episodes\nfor the purpose of posterior sampling; see appendix A.) Taking appropriate values (that depend on\nγ, N, ǫ, |A| and ¯t; when γ approaches 1, η should fall as (1 −γ)\n1\n4 and T should grow as (1 −γ)−1\n4 )\nyields the following\nCorollary 1. There is some constant C ∈(0, ∞) s.t. the following holds. Assume the setting of\nTheorem 1. Assume further that\nγ ≥1 −(¯t + 1)3\nN 2 ln N · min\n\u0012\nǫ, 1\n|A|\n\u0013\n(21)\nDenote\nΞ :=\n\u0012\nN 6 (ln N)\n\u00121\nǫ + |A|\n\u0013\n(¯t + 1)\n\u00131/4\n(22)\nThen, there is π† : S∗\n• × S•\nk−→A• s.t. for any k ∈[N]\nRegπ†\nLk(γ) ≤CΞ(1 −γ)\n1/4\n(23)\n∀K ∈N : Pr\nLkπ† [DMk > K] ≤C\n\nΞ(1 −γ)\n1/4 + 1\nK\n \nN 6 (ln N)3\n1 −γ\n\u00121\nǫ + |A|\n\u00133!1/4\n\n(24)\nA\nPROOF OUTLINE\nWe start by giving an explicit description of an algorithm that implements the policy π†.\nBy condition ii of Deﬁnition 8, for each k ∈[N] we can choose some π⋆k : S →A s.t. for any\ns ∈S, π⋆k(s) ∈A⋆\nMk(s) and υk \u0000π⋆k(s)\n\f\f s\n\u0001\n> ǫ. The algorithm is then a variant of posterior\nsampling reinforcement learning in time intervals of size T (see Osband et al. (2013)), where sam-\npling hypothesis k leads to using policy π⋆k but delegating when we are uncertain the action is safe.\nAlso, we repeatedly discard hypotheses with probability below η from our belief distribution. If the\ncurrently sampled hypothesis is discarded, the algorithm continues to select safe actions until the\nend of the time interval, delegating whenever no action is certainly safe.\n6\nPublished as a conference paper at ICLR 2019\n1 state ←s0\n2 belief ←uniform distribution over [N]\n3 InﬁniteLoopBegin\n4\nhypothesis ←sample the distribution belief\n5\nfor m = 0 to T −1 do\n6\nif belief(hypothesis) > 0 then\n7\nagentAction ←π⋆hypothesis (state)\n8\nfor k = 0 to N −1 do\n9\nif belief(k) > 0 and υk (agentAction | state) = 0 then\n10\nagentAction ←⊥\n11\nend\n12\nend\n13\nelse\n14\nagentAction ←⊥\n15\nfor a ∈A do\n16\nisSafeAction ←TRUE\n17\nfor k = 0 to N −1 do\n18\nif belief(k) > 0 and υk (a | state) = 0 then\n19\nisSafeAction ←FALSE\n20\nend\n21\nend\n22\nif isSafeAction then\n23\nagentAction ←a\n24\nend\n25\nend\n26\nend\n27\ntake action agentAction\n28\n(newState, advisorAction) ←make observation\n29\nfor k = 0 to N −1 do\n30\nbelief(k) ←belief(k) · TLk (newState, advisorAction | state, agentAction)\n31\nend\n32\nbelief ←\n\u0010PN−1\nk=0 belief(k)\n\u0011−1\n· belief\n33\nfor k = 0 to N −1 do\n34\nif belief(k) < η then\n35\nbelief(k) ←0\n36\nend\n37\nend\n38\nbelief ←\n\u0010PN−1\nk=0 belief(k)\n\u0011−1\n· belief\n39\nstate ←newState\n40\nend\n41 InﬁniteLoopEnd\nNote that, when the algorithm references υk one lines 9 and 18, it doesn’t mean delegation. Instead,\nthe algorithm just examines the k-th hypothesis about what the advisor may do.\nThe form of inequalities (19) and (20) is s.t. we can assume w.l.o.g. that η <\n1\nN and ǫ <\n1\n|A|. In\nparticular, the former assumption ensures that we get no division by 0 in line 38 of the algorithm.\nLine 32 might in principle involve division by 0, in which case the behavior of the algorithm can\nbe arbitrary. For example, we may assume that in this case belief becomes the uniform distribution\nagain (but it doesn’t matter.)\nLines 33-38 discard hypotheses that are too unlikely in order for the agent to take calculated risks\n(take an action even when there is a small probability of it being unsafe). Technically, in the proof\nthey are necessary in order to apply certain mutual information inequalities (see below.) On the\nother hand, we also need belief to coincide with the actual posterior given all observations, which\nseems like a contradiction. In order to resolve this, we introduce a class of imaginary environments\n7\nPublished as a conference paper at ICLR 2019\n\b\nLk!\t\nk∈[N] in which there is an additional observed signal β taking values in [N] ⊔{⊥} that,\nin environment Lk!, takes the value k when belief(k) < η and ⊥otherwise. Lines 33-38 then\ncorrespond to conditioning belief on the observation β = ⊥. That is, in the imaginary setting these\nlines are replaced by the following:\n33 β ←observe\n34 if β = ⊥then\n35\nfor k = 0 to N −1 do\n36\nif belief(k) < η then\n37\nbelief(k) ←0\n38\nend\n39\nend\n40\nbelief ←\n\u0010PN−1\nk=0 belief(k)\n\u0011−1\n· belief\n41 else\n42\nbelief ←0\n43\nbelief(β) ←1\n44 end\nWe will thereby derive the regret bound by (i) deriving a regret bound in the imaginary setting and\n(ii) bounding the difference between the imaginary setting and the real setting.\nGiven an MDP M and any π : S∗\nM × SM\nk−→A, we deﬁne HMπ ⊆S∗\nM by\nHMπ :=\nn\nh ∈S∗\nM\n\f\f\f\nPr\nx∼Mπ [h ⊏x] > 0\no\n(25)\nObserve that, in the imaginary setting, the policy π!k : S∗\n• ×S•\nk−→A• implemented by our algorithm\n(which depends explicitly on k because k determines β) has the property\n∀h ∈HLkπ!k, s ∈S• : supp π!k(h, s) ⊆A0\nMk(s) ∪{⊥}\n(26)\nThis is thanks to the condition at line 9 and property i of Deﬁnition 8.\nCombining π!k with the advisor υk we get the policy\n\u0002\nυk\u0003\nπ!k : S∗× S\nk−→A which satisﬁes (using\nproperty i of Deﬁnition 8 again)\n∀h ∈HMk,[υk]π!k, s ∈S : supp\n\u0002\nυk\u0003\nπ!k(h, s) ⊆A0\nMk(s)\n(27)\nThe regret incurred during each “episode” of length T can be divided into short-term (associated\nwith the rewards during the episode) and long-term (associated with the rewards after the episode,\nor, equivalently, with the value of the state reached at the end of the episode.) To describe the\nshort-term regret, we introduce the policies\nn\nπ⋆k\nn : S∗× S\nk−→A\no\nn∈N deﬁned by\nπ⋆k\nn (h, s) :=\n\u001a\u0002\nυk\u0003\nπ!k(h, s) if |h| < nT\nπ⋆k(s) otherwise\n(28)\nHere, |h| denotes the length of h. That is, for h ∈Sm, |h| := m.\nDeﬁne R• : S• →[0, 1] by R•(s, a) := R(s). For each k ∈[N] and n ∈N, deﬁne EU⋆k\nn , EU!k\nn ∈\n[0, 1] by\nEU⋆k\nn := 1 −γ\n1 −γT\nT −1\nX\nm=0\nγm\nE\nx∼Mkπ⋆k\nn\n[R (xnT +m)]\n(29)\n8\nPublished as a conference paper at ICLR 2019\nEU!k\nn := 1 −γ\n1 −γT\nT −1\nX\nm=0\nγm\nE\nx∼Lkπ!k [R• (xnT +m)]\n(30)\nDue to equation (27), the long-term regret per episode is O (tMk(γ) · (1 −γ)). The number of\nepisodes that are signiﬁcant in terms of time discount is\n1\n(1−γ)T . Therefore, the total contribution of\nthe long-term regret is O\n\u0010 tMk(γ)\nT\n\u0011\n. This gives us5\nRegπ!k\nLk (γ) =\n\u00001 −γT \u0001 ∞\nX\nn=0\nγnT \u0010\nEU⋆k\nn −EU!k\nn\n\u0011\n+ O\n\u0012tMk(γ)\nT\n\u0013\n(31)\nIn order to further analyze the short-term regret, we introduce the policies\nn\nπ♯k\nn : S∗× S\nk−→A\no\nk∈[N],n∈N. These policies result from modifying the algorithm as follows\n(starting from line 3.)\n3 episodeNumber ←0\n4 InﬁniteLoopBegin\n5\nif episodeNumber < n then\n6\nhypothesis ←sample the distribution belief\n7\nelse\n8\nhypothesis ←k\n9\nend\n10\n...\n52\nepisodeNumber ←episodeNumber + 1\n53 InﬁniteLoopEnd\nWe also deﬁne EU♯k\nn ∈[0, 1] by\nEU♯k\nn := 1 −γ\n1 −γT\nT −1\nX\nm=0\nγm\nE\nx∼Lkπ♯k\nn\n[R• (xnT +m)]\n(32)\nWe can now rewrite EU⋆k\nn −EU!k\nn as\n\u0010\nEU⋆k\nn −EU♯k\nn\n\u0011\n+\n\u0010\nEU♯k\nn −EU!k\nn\n\u0011\nand bound the contribu-\ntion of each term separately.\nThe difference between π⋆k\nn and π♯k\nn is that the latter sometimes delegates even in the n-th (and later)\nepisodes6. Therefore, we can bound the difference in expected utilities by bounding the expected\nnumber of delegations. Now, delegation is only performed in one of two scenarios, corresponding\nto line 10 and line 14. In the scenario of line 10, we have the action π⋆hypothesis(state) which, with\nprobability at least η over hypotheses is taken with probability at least ǫ by the advisor (at least η\nsince this is the minimal value belief(hypothesis) can have.) On the other hand, with probability\nat least η over hypotheses, the same action is never taken by the advisor (otherwise we wouldn’t\ndelegate.) Therefore, observing whether this action is taken by the advisor provides an amount of\ninformation about the environment that can be bounded below in terms of η and ǫ. In the scenario of\nline 14, there is no action which is known with probability at least 1 −η over hypotheses to be taken\nby the advisor with positive probability. Since observing the action actually taken by the advisor\nprovides an example of an action which had positive probability, we gain an amount of information\nthat can be bounded from below in terms of η. In both cases, we can show information gain is Ω(ηǫ)\n(see Proposition 37.) Since the initial entropy is ln N, this means that the number of delegations is\n5See Proposition 2 for the detailed derivation.\n6Technically, π⋆k\nn is a policy for M k so it’s not strictly meaningful to say it “delegates” at all, but we think\nof it as delegating when the π!k “subroutine” inside it is called and delegates.\n7We think of K as the (unknown) correct hypothesis, X as the advisor action and a∗as π⋆hypothesis(state).\n9\nPublished as a conference paper at ICLR 2019\nO\n\u0010\nln N\nηǫ\n\u0011\n(see Proposition 48.) This is similar to the analysis done in Russo & Van Roy (2016) for\nordinary Thompson sampling.\nNow we use this to bound\n\f\f\fEU⋆\nn −EU♯\nn\n\f\f\f. We have\n\f\f\fEU⋆k\nn −EU♯k\nn\n\f\f\f ≤\nPr\nx∼Lkπ♯k\nn\n[∃m ∈[T ] : xnT +m+1 ∈S × A]\n(33)\nWe bounded the expected number of delegations for π!k but not π♯k\nn . Since π♯k\nn differs from π!k only\nby always selecting the correct hypothesis at the n-th and further episodes, and since the probability\nof selecting the correct hypothesis at line 4 is at least η, we get\n\f\f\fEU⋆k\nn −EU♯k\nn\n\f\f\f ≤1\nη\nPr\nx∼Lkπ!k [∃m ∈[T ] : xnT +m+1 ∈S × A]\n≤1\nη\nE\nx∼Lkπ!k [|{m ∈[T ] : xnT +m+1 ∈S × A}|]\n∞\nX\nn=0\n\f\f\fEU⋆k\nn −EU♯k\nn\n\f\f\f ≤1\nη\nE\nx∼Lkπ!k [|{n ∈N : xn ∈S × A}|]\n(34)\n1\nN\nN−1\nX\nk=0\n∞\nX\nn=0\n\f\f\fEU⋆k\nn −EU♯k\nn\n\f\f\f = O\n\u0012ln N\nη2ǫ\n\u0013\n(35)\nObserving the rewards received during an episode yields information about the environment. The\nexpected information gain can only vanish when you expect to receive the same rewards regardless of\nwhich hypothesis is correct, in which case the policy π⋆hypothesis is optimal regardless of hypothesis.\nThis allows us to derive a lower bound for the information gain in terms of the difference between\nthe rewards received by π!k and π♯k\nn . Denoting In the expected information gain in episode n, we\nhave (see Proposition 5 and further details in Appendix B)\n1 −γT\nN\nN−1\nX\nk=0\n∞\nX\nn=0\nγnT \u0010\nEU♯k\nn −EU!k\nn\n\u0011\n≤\nv\nu\nu\nt1 −γT\n2η\n∞\nX\nn=0\nγnT In\n(36)\nUsing once again the fact that the initial entropy is ln N, this implies\n1 −γT\nN\nN−1\nX\nk=0\n∞\nX\nn=0\nγnT \u0010\nEU♯k\nn −EU!k\nn\n\u0011\n≤\ns\n(1 −γT ) ln N\n2η\n(37)\nCombining inequalities (31), (35) and (37), we get\n1\nN\nN−1\nX\nk=0\nRegπ!k\nLk (γ) = O\n\n¯t\nT +\ns\n(1 −γT ) ln N\nη\n+\n\u00001 −γT \u0001\nln N\nη2ǫ\n\n\n(38)\nFinally, we observe that, in the real setting (without the β signal,) line 35 can be reached at most\nN −1 times before the ﬁrst division by zero at line 32. Moreover, such division by zero will never\nhappen unless the correct hypothesis is discarded. Each time line 35 is reached, the probability that\nbelief assigns probability below η to the correct hypothesis is at most η. Therefore, the probability\n8We think of ¯Θn as the information used to compute belief (including state,) Ψn as π⋆hypothesis(state)\nwhen belief(hypothesis) > 0 and ⊥otherwise, and Zn as belief.\n10\nPublished as a conference paper at ICLR 2019\nthat the correct hypothesis is discarded is at most η(N−1). This allows us to bound the total variation\ndistance between the real setting and imaginary setting by O(ηN), producing inequality (19)9.\nFor reasons we already outlined, we have\n1\nN\nN−1\nX\nk=0\nE\nLkπ!k [DMk] = O\n\u0012ln N\nηǫ\n\u0013\n(39)\nUsing Markov’s inequality, we get\n∀K ∈N : 1\nN\nN−1\nX\nk=0\nPr\nLkπ!k [DMk > K] = O\n\u0012ln N\nKηǫ\n\u0013\n(40)\nUsing again the relationship we established between the real and imaginary settings, we get inequal-\nity (20)10.\nB\nPROOF DETAILS\nDeﬁnition 12. Given an MDP M and π : S∗\nM ×SM\nk−→AM, we deﬁne QMπ : S∗\nM ×SM ×[0, 1) →\n[0, 1] by\nQMπ(h, s, γ) :=\nE\na∼π(h,s) [QM(s, a, γ)]\n(41)\nGiven a set A, x ∈Aω and n ∈N, the notation x:n will indicate the preﬁx of x of length n. That is,\nx:n ∈An and x:n ⊏x.\nProposition 1. Consider an MDP M, γ ∈(0, 1) and π : S∗\nM × SM\nk−→AM. Then,\nRegπ\nM(γ) =\n∞\nX\nn=0\nγn\nE\nx∼Mπ [VM (xn, γ) −QMπ (x:n+1, γ)]\n(42)\nProof. For the sake of encumbering the notation less, we will omit the argument γ in functions that\ndepend on it. We will also omit the subscript M and denote s0 := sM.\nFor any x ∈Sω s.t. s0 ⊏x, it is easy to see that\nEU⋆= V (s0) =\n∞\nX\nn=0\nγn (V (xn) −γ V (xn+1))\nU(x) = (1 −γ)\n∞\nX\nn=0\nγnR (xn)\n9The reason inequality (19) has 1\nǫ + |A| instead of ǫ is because we needed to assume w.l.o.g. that ǫ <\n1\n|A|.\nOn the other hand, the assumption 1\nη < N is justiﬁed by the appearance of the ηN term in the bound. Also, we\ncan use (1−γ)T instead of 1−γT because the form of the bound allows assuming w.l.o.g. that (1−γ)T ≪1.\n10The reason that the second term of the right hand side of inequality (20) has 1\nη +N instead of η and 1\nǫ +|A|\ninstead of ǫ is because we needed to assume w.l.o.g. that η <\n1\nN and ǫ <\n1\n|A|.\n11\nPublished as a conference paper at ICLR 2019\nEU⋆−U(x) =\n∞\nX\nn=0\nγn (V (xn) −(1 −γ)R (xn) −γ V (xn+1))\n=\n∞\nX\nn=0\nγn\u0010\nV (xn) −Qπ (x:n+1)\n+ Qπ (x:n+1) −(1 −γ)R (xn) −γ V (xn+1)\n\u0011\nTaking expected value over x w.r.t. Mπ, we get\nRegπ =\n∞\nX\nn=0\nγn\n \nE\nMπ [V (xn) −Qπ (x:n+1)]\n+ E\nMπ [Qπ (x:n+1) −(1 −γ)R (xn) −γ V (xn+1)]\n!\nEquation (3) implies that the second term vanishes, yielding the desired result.\nProposition 2. Consider an MDP M, γ ∈(γM, 1), T ∈N+, π⋆: SM\nk−→AM and π0 : S∗\nM ×\nSM\nk−→AM. For any n ∈N, deﬁne π⋆\nn : S∗\nM × SM\nk−→AM by\nπ⋆\nn(h, s) :=\n\u001aπ0(h, s) if |h| < nT\nπ⋆(s) otherwise\nAssume that\ni. For any s ∈SM, supp π⋆(s) ⊆A⋆\nM(s).\nii. For any h ∈HMπ0 and s ∈SM, supp π0(h, s) ⊆A0\nM (s).\nFor any n ∈N, deﬁne EU⋆\nn, EU0\nn ∈[0, 1] by\nEU⋆\nn := 1 −γ\n1 −γT\nT −1\nX\nm=0\nγm\nE\nx∼Mπ⋆n\n[R (xnT +m)]\n(43)\nEU0\nn := 1 −γ\n1 −γT\nT −1\nX\nm=0\nγm\nE\nx∼Mπ0 [R (xnT +m)]\n(44)\nThen,\nRegπ0\nM (γ) ≤\n\u00001 −γT \u0001 ∞\nX\nn=0\nγnT \u0000EU⋆\nn −EU0\nn\n\u0001\n+ 2tM(γ) · γT(1 −γ)\n1 −γT\n(45)\nProof. For the sake of encumbering the notation less, we will use the shorthands Rn := RM (xn),\nVn := VM (xn, γ), V0\nn := V0\nM (xn), Qπn := QMπ (x:n+1, γ) and t := tM(γ).\nBy Proposition 1, for any l ∈N\nRegπ⋆\nl\nM =\n∞\nX\nn=0\nγn E\nMπ⋆\nl\nh\nVn −Qπ⋆\nl n\ni\n12\nPublished as a conference paper at ICLR 2019\nπ⋆\nl coincides with π⋆after lT , therefore the corresponding terms on the right hand side vanish.\nRegπ⋆\nl\nM =\nlT −1\nX\nn=0\nγn E\nMπ0 [Vn −Qπ0n]\nSubtracting the equalities for l + 1 and l, we get\nEUπ⋆\nl\nM −EU\nπ⋆\nl+1\nM\n=\n(l+1)T −1\nX\nn=lT\nγn E\nMπ0 [Vn −Qπ0n]\n(1 −γ)\n∞\nX\nn=0\nγn\n\u0012\nE\nMπ⋆\nl\n[Rn] −\nE\nMπ⋆\nl+1\n[Rn]\n\u0013\n=\n(l+1)T −1\nX\nn=lT\nγn E\nMπ0 [Vn −Qπ0n]\nπ⋆\nl and π⋆\nl+1 coincide until lT , therefore\n(1 −γ)\n∞\nX\nn=lT\nγn\n\u0012\nE\nMπ⋆\nl\n[Rn] −\nE\nMπ⋆\nl+1\n[Rn]\n\u0013\n=\n(l+1)T −1\nX\nn=lT\nγn E\nMπ0 [Vn −Qπ0n]\nBoth π⋆\nl and π⋆\nl+1 coincide with π⋆after (l + 1)T , therefore\n(1 −γ)\n(l+1)T −1\nX\nn=lT\nγn\n\u0012\nE\nMπ⋆\nl\n[Rn] −E\nMπ0 [Rn]\n\u0013\n+ γ(l+1)T\n\u0012\nE\nMπ⋆\nl\n\u0002\nV(l+1)T\n\u0003\n−E\nMπ0\n\u0002\nV(l+1)T\n\u0003\u0013\n=\n(l+1)T −1\nX\nn=lT\nγn E\nMπ0 [Vn −Qπ0n]\nBy the mean value theorem, for each s ∈SM we have\nV0\nM(s) −t · (1 −γ) ≤VM(s, γ) ≤V0\nM(s) + t · (1 −γ)\nIt follows that\n(1 −γ)\n(l+1)T −1\nX\nn=lT\nγn\n\u0012\nE\nMπ⋆\nl\n[Rn] −E\nMπ0 [Rn]\n\u0013\n+ γ(l+1)T\n\u0012\nE\nMπ⋆\nl\nh\nV0\n(l+1)T\ni\n−E\nMπ0\nh\nV0\n(l+1)T\ni\n+ 2t · (1 −γ)\n\u0013\n≥\n(l+1)T −1\nX\nn=lT\nγn E\nMπ0 [Vn −Qπ0n]\nIt is easy to see that assumptions i and ii imply that V0\nn is a martingale for Mπ⋆and Mπ0 and\ntherefore\nE\nMπ⋆\nl\nh\nV0\n(l+1)T\ni\n= E\nMπ0\nh\nV0\n(l+1)T\ni\n= V0 (sM)\nWe get\n(1 −γ)\n(l+1)T −1\nX\nn=lT\nγn\n\u0012\nE\nMπ⋆\nl\n[Rn] −E\nMπ0 [Rn]\n\u0013\n+ 2tγ(l+1)T(1 −γ) ≥\n(l+1)T −1\nX\nn=lT\nγn E\nMπ0 [Vn −Qπ0n]\n13\nPublished as a conference paper at ICLR 2019\nSumming over l, we get\n(1 −γ)\n∞\nX\nl=0\n(l+1)T −1\nX\nn=lT\nγn\n\u0012\nE\nMπ⋆\nl\n[Rn] −E\nMπ0 [Rn]\n\u0013\n+ 2t · γT (1 −γ)\n1 −γT\n≥\n∞\nX\nn=0\nγn E\nMπ0 [Vn −Qπ0n]\nApplying Proposition 1 to the right hand side and using equations (43) and (44) we get the desired\nresult.\nGiven (Ω, P ∈∆Ω) a probability space, A, B ﬁnite sets and X : Ω→A, Y : Ω→B random\nvariables, I [X; Y ] denotes the mutual information between X and Y . Given C another ﬁnite set\nand Z : Ω→C another random variable, I [X; Y | Z] : Ω→R will denote the random variable\nobtained by ﬁrst conditioning on Z and then taking the mutual information between X and Y , not\nthe expected value of this quantity, as sometimes used. X∗P ∈∆A denotes the pushforward of P\nby X, i.e. the probability distribution of X. P | X : Ω→∆Ωdenotes the conditional probability\nmeasure (P conditioned on the value of X.) Given µ, ν ∈∆A, DKL (µ | ν) denotes the Kullb-\nLeibler divergence of µ from ν.\nProposition 3. Consider A a ﬁnite set, a∗∈A, N ∈N+, ǫ ∈\n\u0010\n0, |A|−1\u0011\n, and η ∈(0, 1). Consider\nalso (Ω, P) a probability space and random variables K : Ω→[N] and X : Ω→A. Suppose that\nfor every a ∈A\nPr\nh\nPr [X = a | K] > 0 ∧\n\u0010\na = a∗∨Pr [X = a∗| K] ≤ǫ\n\u0011i\n≤1 −η\n(46)\nThen\nI [K; X] ≥η ln\n\u0010\n1 + ǫ(1 −ǫ)\n1\nǫ −1\u0011\n(47)\nProof. Deﬁne q ∈(0, 1) by\nq :=\n1\nǫ + (1 −ǫ)1−1\nǫ\nLet aq ∈A be s.t. Pr [X = aq] > qǫ and either aq = a∗or Pr [X = a∗] ≤qǫ. For every\nk ∈supp K∗P, denote\nAk :=\nn\na ∈A\n\f\f\f Pr [X = a | K = k] > 0 ∧\n\u0010\na = a∗∨Pr [X = a∗| K = k] ≤ǫ\n\u0011o\nIf aq ̸∈Ak then either Pr [X = aq | K = k] = 0 or both Pr [X = a∗] ≤qǫ and\nPr [X = a∗| K = k] > ǫ. In this case, conditioning by K = k causes either the probability of\nX = aq to go down from at least qǫ to 0 or the probability of X = a∗to go up from at most qǫ to at\nleast ǫ. We get\nDKL (X∗(P | K = k) | X∗P) ≥min (DKL (0 | qǫ) , DKL (ǫ | qǫ))\nWe have\n14\nPublished as a conference paper at ICLR 2019\nDKL (0 | qǫ) = ln\n1\n1 −qǫ\n= ln\n1\n1 −\nǫ\nǫ+(1−ǫ)1−1\nǫ\n= ln\nǫ + (1 −ǫ)1−1\nǫ\nǫ + (1 −ǫ)1−1\nǫ −ǫ\n= ln\n\u0010\n1 + ǫ(1 −ǫ)\n1\nǫ −1\u0011\nDKL (ǫ | qǫ) = ǫ ln ǫ\nqǫ + (1 −ǫ) ln 1 −ǫ\n1 −qǫ\n= ǫ ln 1\nq + (1 −ǫ) ln (1 −ǫ) + ln\n1\n1 −qǫ −ǫ ln\n1\n1 −qǫ\n= ǫ ln 1 −qǫ\nq\n+ ln (1 −ǫ)1−ǫ + ln\n1\n1 −qǫ\n= ǫ ln\n\u00121\nq −ǫ\n\u0013\n+ ln (1 −ǫ)1−ǫ + ln\n1\n1 −qǫ\n= ǫ ln (1 −ǫ)1−1\nǫ + ln (1 −ǫ)1−ǫ + ln\n1\n1 −qǫ\n= ln (1 −ǫ)ǫ−1 + ln (1 −ǫ)1−ǫ + ln\n1\n1 −qǫ\n= ln\n1\n1 −qǫ\n= ln\n\u0010\n1 + ǫ(1 −ǫ)\n1\nǫ −1\u0011\nIt follows that\nI [K; X] = E [DKL (X∗(P | K) | X∗P)]\n≥Pr [aq ̸∈AK] ln\n\u0010\n1 + ǫ(1 −ǫ)\n1\nǫ −1\u0011\n≥η ln\n\u0010\n1 + ǫ(1 −ǫ)\n1\nǫ −1\u0011\nIn our notation, propositions about random variables are understood to hold almost surely.\nProposition 4. Consider non-empty ﬁnite sets A and B, N ∈N+, ǫ ∈\n\u0010\n0, |A|−1\u0011\n, η ∈(0, 1) and\nn\nυk : B\nk−→A\no\nk∈[N]. Consider also a probability space (Ω, P) and random variables K : Ω→\n[N],\n\b¯Θn : Ω→B\n\t\nn∈N, {Xn, Ψn : Ω→A ⊔{⊥}}n∈N, and {Zn : Ω→∆[N]}n∈N. Assume that\nfor any n ∈N, k ∈[N] and a ∈A\ni. Pr\n\u0002\nXn+1 = a\n\f\f K, ¯Θn, Ψn, Zn\n\u0003\n= Pr\n\u0002\nXn+1 ̸= ⊥\n\f\f K, ¯Θn, Ψn, Zn\n\u0003\nυK \u0000a\n\f\f ¯Θn\n\u0001\nii. Xn+1 = ⊥⇐⇒∃a ∈A∀k ∈supp Zn : υk \u0000a\n\f\f ¯Θn\n\u0001\n> 0 ∧\n\u0000a = Ψn ∨υk \u0000Ψn\n\f\f ¯Θn\n\u0001\n≤ǫ\n\u0001\niii. Zn(k) = Pr\n\u0002\nK = k\n\f\f ¯Θ0, ¯Θ1 . . . ¯Θn, Ψ0, Ψ1 . . . Ψn, X0, X1 . . . Xn\n\u0003\n15\nPublished as a conference paper at ICLR 2019\niv. Zn(k) ≥η\nThen,\nE\n\u0002\f\f\b\nn ∈N+ \f\f Xn ̸= ⊥\n\t\f\f\u0003\n≤\nln N\nη ln\n\u0010\n1 + ǫ(1 −ǫ)\n1\nǫ −1\u0011\n(48)\nProof.\nln N ≥E [H (Z0)]\n≥\n∞\nX\nn=0\nE [H (Zn) −H (Zn+1)]\n=\n∞\nX\nn=0\nE\nh\nE [H (Zn) −H (Zn+1) | Θn, Ψn, Zn]\ni\nUsing assumption iii, we get\nln N ≥\n∞\nX\nn=0\nE\nh\nI [K; Θn+1, Ψn+1, Xn+1 | Θn, Ψn, Zn]\ni\n≥\n∞\nX\nn=0\nE\nh\nI [K; Xn+1 | Θn, Ψn, Zn]\ni\n≥\n∞\nX\nn=0\nE\nh\nI [K; Xn+1 | Θn, Ψn, Zn] ; Xn+1 ̸= ⊥\ni\n(49)\nDeﬁne the random variables {Qnak : Ω→[0, 1]}n∈N,a∈A,k∈[N] by\nQnak := Pr [Xn+1 = a | K = k, Θn, Ψn, Zn]\nDeﬁne the events {Dnak ⊆Ω}n∈N,a∈A,k∈[N] by\nDnak := {Qnak > 0 ∧(a = Ψn ∨QnΨnk ≤ǫ)}\nBy assumption ii, the event Xn = ⊥is determined by Θn, Ψn and Zn. Using assumption i, it\nfollows that for any n ∈N, a ∈A and k ∈[K]\nXn+1 ̸= ⊥=⇒Qnak = υk (a | Θn)\nXn+1 ̸= ⊥=⇒\n\u0000Dnak ⇐⇒υk (a | Θn) > 0 ∧\n\u0000a = Ψn ∨υk (Ψn | Θn) ≤ǫ\n\u0001\u0001\nUsing assumption ii, we get\nXn+1 ̸= ⊥=⇒∃k ∈supp Zn : ¬Dnak\nUsing assumption iv\nXn+1 ̸= ⊥=⇒\nPr\nk∼Zn [Dnak] ≤1 −η\n16\nPublished as a conference paper at ICLR 2019\nUsing assumption iii\nXn+1 ̸= ⊥=⇒Pr [DnaK | Θn, Ψn, Zn] ≤1 −η\nApplying Proposition 3 we conclude\nXn+1 ̸= ⊥=⇒I [K; Xn+1 | Θn, Ψn, Zn] ≥η ln\n\u0010\n1 + ǫ(1 −ǫ)\n1\nǫ −1\u0011\n(50)\nCombining inequality (49) with inequality (50), we get\nln N ≥\n∞\nX\nn=0\nPr [Xn+1 ̸= ⊥] η ln\n\u0010\n1 + ǫ(1 −ǫ)\n1\nǫ −1\u0011\nNoticing that E [|{n ∈N+ | Xn ̸= ⊥}|] = P∞\nn=0 Pr [Xn+1 ̸= ⊥], we get the desired result.\nGiven a measurable space X and µ, ν ∈∆X, dtv (µ, ν) will denote the total variation distance\nbetween µ and ν.\nProposition 5. Consider a probability space (Ω, P), N ∈N, η ∈(0, 1), ζ ∈∆[N], a ﬁnite set\nR ⊆[0, 1] and random variables U : Ω→R, K : Ω→[N] and J : Ω→[N]. Assume that\ni. K∗P = J∗P = ζ\nii. I [K; J] = 0\niii. ∀k ∈supp ζ : ζ(k) ≥η\nThen,\nI [K; J, U] ≥2η\n\u0010\nE\nh\nE [U | K, J = K]\ni\n−E [U]\n\u00112\n(51)\nProof. Using the chain rule for mutual information\nI [K; J, U] = I [K; J] + E\nh\nI [K; U | J]\ni\nUsing assumption ii\nI [K; J, U] = E\nh\nI [K; U | J]\ni\n= E [DKL (U∗(P | K, J) | U∗(P | J))]\nUsing Pinsker’s inequality\nI [K; J, U] ≥2E\nh\ndtv (U∗(P | K, J) , U∗(P | J))2i\n≥2E\n\u0014\u0010\nE [U | K, J] −E [U | J]\n\u00112\u0015\nDenote Ukj := E [U | K = k, J = j]. Using assumptions i and ii, we have\n17\nPublished as a conference paper at ICLR 2019\nI [K; J, U] ≥2 E\nk∼ζ\nj∼ζ\n\"\u0012\nUkj −E\nk′∼ζ [Uk′j]\n\u00132#\n≥2 E\nk∼ζ\nj∼ζ\n\"\u0012\nUkj −E\nk′∼ζ [Uk′j]\n\u00132\n; k = j\n#\n≥2 E\nj∼ζ\n\"\nζ(j)\n\u0012\nUjj −E\nk∼ζ [Ukj]\n\u00132#\nUsing assumption iii\nI [K; J, U] ≥2η E\nj∼ζ\n\"\u0012\nUjj −E\nk∼ζ [Ukj]\n\u00132#\n≥2η\n\nE\nj∼ζ [Ujj] −E\nk∼ζ\nj∼ζ\n[Ukj]\n\n\n2\nUsing assumptions i and ii again, we get the desired result\nGiven a proposition π, the notation [[π]] ∈{0, 1} will mean 0 when the π is false and 1 when π is\ntrue.\nProof of Theorem 1. The form of inequalities (19) and (20) is s.t. we can assume w.l.o.g. that η < 1\nN\nand ǫ <\n1\n|A|.\nWe are going to construct a probability space (Ω, P) and the random variables K : Ω→[N] and\nfor each n ∈N\nZ†\nn, ˜Z†\nn : Ω→∆[N]\nJ†\nn : Ω→[N]\nΨ†\nn : Ω→A•\nA†\nn : Ω→A•\nX†\nn : Ω→A•\nΘ†\nn : Ω→S\nWe also deﬁne H†\nn : Ω→Sn by\nH†\nn :=\n\u0010\nΘ†\n0, Θ†\n1 . . . Θ†\nn−1\n\u0011\nBy condition ii of Deﬁnition 8, for each k ∈[N] we can choose some π⋆k : S →A s.t. for any\ns ∈S, π⋆k(s) ∈A⋆\nMk(s) and υk \u0000πk(s)\n\f\f s\n\u0001\n> ǫ.\nWe postulate that K is uniformly distributed and for any k ∈[N], l ∈N, m ∈[T ], s ∈S and\na ∈A•, denoting n = lT + m\nA†\nn =\n\n\n\nΨ†\nn if ∀k ∈supp Z†\nn : υk \u0000Ψ†\nn\n\f\f Θ†\nn\n\u0001\n> 0\nsome a ∈A s.t. ∀k ∈supp Z†\nn : υk \u0000a\n\f\f Θ†\nn\n\u0001\n> 0 if such exists and Ψ†\nn = ⊥\n⊥otherwise\n18\nPublished as a conference paper at ICLR 2019\n˜Z†\n0(k) = 1\nN\nZ†\nn(k) =\n˜Z†\nn(k)[[ ˜Z†\nn(k) ≥η]]\nPN−1\nj=0 ˜Z†\nn(j)[[ ˜Z†\nn(j) ≥η]]\nPr\nh\nJ†\nl = k\n\f\f\f Z†\nlT\ni\n= Z†\nlT (k)\nΨ†\nn =\n(\nπ⋆J†\nl \u0000Θ†\nn\n\u0001\nif Z†\nn\n\u0010\nJ†\nl\n\u0011\n> 0\n⊥otherwise\nΘ†\n0 = s0\nX†\n0 = ⊥\nPr\nh\nΘ†\nn+1 = s, X†\nn+1 = a\n\f\f\f Θ†\nn, A†\nn\ni\n= TLK\n\u0000s, a\n\f\f Θ†\nn, A†\nn\n\u0001\n˜Z†\nn+1(k)\nN−1\nX\nj=0\nZ†\nn(j)TLj\n\u0010\nΘ†\nn+1, X†\nn+1\n\f\f\f Θ†\nn, A†\nn\n\u0011\n= Z†\nn(k)TLk\n\u0010\nΘ†\nn+1, X†\nn+1\n\f\f\f Θ†\nn, A†\nn\n\u0011\nNote that the last equation has the form of a Bayesian update which is allowed to be arbitrary when\nupdate is on ”impossible” information.\nThis probability space can be constructed using standard arguments from the Kolmogorov extension\ntheorem.\nWe now deﬁne π† s.t. for any n ∈N, a ∈A•, h ∈Sn\n• and s ∈S•\nPr\n\u0002\nH†\nn = h,\n\u0000Θ†\nn, X†\nn\n\u0001\n= s\n\u0003\n> 0 =⇒π† (a | h, s) := Pr\n\u0002\nA†\nn = a\n\f\f H†\nn = h,\n\u0000Θ†\nn, X†\nn\n\u0001\n= s\n\u0003\nIn order to prove π† has the desired properties, we will deﬁne the stochastic processes Z, ˜Z, J,\nΨ, A, X and Θ, each process of the same type as its dagger counterpart (thus Ωis constructed to\naccommodate them.) These processes are required to satisfy the following:\nAn =\n\n\n\nΨn if ∀k ∈supp Zn : υk (Ψn | Θn) > 0\nsome a ∈A s.t. ∀k ∈supp Zn : υk (a | Θn) > 0 if such exists and Ψn = ⊥\n⊥otherwise\n˜Z0(k) = 1\nN\nZn(k) =\n˜Zn(k)[[ ˜Zn(k) ≥η]]\nPN−1\nj=0 ˜Zn(j)[[ ˜Zn(j) ≥η]]\n[[ ˜Zn(K) ≥η]]\n+ [[K = k]] · [[ ˜Zn(K) < η]]\nPr [Jl = k | ZlT ] = ZlT (k)\nΨn =\n\u001aπ⋆Jl (Θn) if Zn (Jl) > 0\n⊥otherwise\nΘ0 = s0\nX0 = s0\nPr [Θn+1 = s, Xn+1 = a | Θn, An] = TLK (s, a | Θn, An)\n˜Zn+1(k) =\nZn(k)TLk (Θn+1, Xn+1 | Θn, An)\nPN−1\nj=0 Zn(j)TLj (Θn+1, Xn+1 | Θn, An)\nAs before, we also deﬁne Hn := (Θ0, Θ1 . . . Θn−1).\n19\nPublished as a conference paper at ICLR 2019\nWe now construct\nn\nπ!k : S∗\n• × S•\nk−→A•\no\nk∈[N] s.t. for any n ∈N, a ∈A•, h ∈Sn\n• and s ∈S•\nPr [Hn = h, (Θn, Xn) = s, K = k] > 0 =⇒\nπ!k (a | h, s) := Pr [An = a | Hn = h, (Θn, Xn) = s, K = k]\nIt is easy to see equation (27) holds, allowing us to apply Proposition 2 and get\nRegπ!k\nLk (γ) ≤\n\u00001 −γT \u0001 ∞\nX\nn=0\nγnT \u0010\nEU⋆k\nn −EU!k\nn\n\u0011\n+ 2tMk(γ) · 1 −γ\n1 −γT\nHere, EU⋆k\nn and EU!k\nn are deﬁned according to equations (29) and (30) respectively. We also deﬁne\nthe π♯k\nn : S∗\n• × S•\nk−→A• by\nπ♯k\nn (a | h, s) :=\n(\nπ!k(a | h) if |h| < nT\nPr [An = a | Hn = h, (Θn, Xn) = s, K = k, Jn = k] otherwise\nDeﬁning EU♯k\nn according to equation (32), we have\nRegπ!k\nLk (γ) ≤\n\u00001 −γT \u0001 ∞\nX\nn=0\nγnT \u0010\nEU⋆k\nn −EU♯k\nn + EU♯k\nn −EU!k\nn\n\u0011\n+ 2tMk(γ) · 1 −γ\n1 −γT\nUsing equation (34), we can apply Proposition 4. Indeed, conditions i, iii and iv are straightforward\n(for an appropriate deﬁnition of ¯Θ.) To verify condition ii, consider two cases. In the case Zn (Jl) >\n0 (where l := ⌊n/T⌋,) we have Ψn ̸= ⊥and hence An = ⊥(equivalently Xn+1 ̸= ⊥) if and only if\n∃k ∈supp Zn : υk (Ψn | Θn) = 0. This is equivalent to condition ii since, for any a ̸= Ψn, taking\nk = Jl makes the proposition false due to the fact that υJl \u0000π⋆Jl (Θn)\n\f\f Θn\n\u0001\n> ǫ by construction of\nπ⋆k. In the case Zn (Jl) = 0, we have Ψn = ⊥and hence An = ⊥(equivalently Xn+1 ̸= ⊥) if\nand only if ∀a ∈A∃k ∈supp Zn : υk (a | Θn) = 0. This is equivalent to condition ii since, in this\ncase, a ̸= Ψn always. We get\n1\nN\nN−1\nX\nk=0\nRegπ!k\nLk (γ) ≤1 −γT\nN\nN−1\nX\nk=0\n∞\nX\nn=0\nγnT \u0010\nEU♯k\nn −EU!k\nn\n\u0011\n+ O\n \n¯t · 1 −γ\n1 −γT +\n\u00001 −γT \u0001\nln N\nη2ǫ\n!\nDeﬁne the random variables {Un : Ω→[0, 1]}n∈N by\nUn := 1 −γ\n1 −γT\nT −1\nX\nm=0\nγmR (ΘnT +m)\nWe get\n20\nPublished as a conference paper at ICLR 2019\n1\nN\nN−1\nX\nk=0\nRegπ!k\nLk (γ) ≤\n\u00001 −γT \u0001 ∞\nX\nn=0\nγnT E\nh\nE [Un | K, Jn = K, ZnT ] −E [Un | ZnT ]\ni\n+ O\n \n¯t · 1 −γ\n1 −γT +\n\u00001 −γT \u0001\nln N\nη2ǫ\n!\n≤\nv\nu\nu\nt(1 −γT )\n∞\nX\nn=0\nγnT E\n\u0014\u0010\nE [Un | K, Jn = K, ZnT ] −E [Un | ZnT ]\n\u00112\u0015\n+ O\n \n¯t · 1 −γ\n1 −γT +\n\u00001 −γT \u0001\nln N\nη2ǫ\n!\nWe apply Proposition 5 to each term in the sum over n.\n1\nN\nN−1\nX\nk=0\nRegπ!k\nLk (γ) =\nv\nu\nu\nt(1 −γT )\n∞\nX\nn=0\nγnT E\n\u0014 1\n2η I [K; Jn, Un | ZnT ]\n\u0015\n+ O\n \n¯t · 1 −γ\n1 −γT +\n\u00001 −γT \u0001\nln N\nη2ǫ\n!\n≤\nv\nu\nu\nt1 −γT\n2η\n∞\nX\nn=0\nγnT E\n\u0002\nH (ZnT ) −H\n\u0000Z(n+1)T\n\u0001\u0003\n+ O\n \n¯t · 1 −γ\n1 −γT +\n\u00001 −γT \u0001\nln N\nη2ǫ\n!\n= O\n\n¯t · 1 −γ\n1 −γT +\ns\n(1 −γT ) ln N\nη\n+\n\u00001 −γT \u0001\nln N\nη2ǫ\n\n\nThus, we derived equation (38) and the rest of the proof can be completed as in appendix A.\nProof of Corollary 1. We set\nη := (1 −γ)1/4N −1/2 (ln N)1/4\n\u00121\nǫ + |A|\n\u00131/4\n(¯t + 1)1/4\nT :=\n&\n(1 −γ)−1/4N −1/2 (ln N)−1/4\n\u00121\nǫ + |A|\n\u0013−1/4\n(¯t + 1)3/4\n'\nBy equation (21), the expression we round to get T is ≥1, therefore this rounding can be absorbed\nwithin the constant factor. Equations (23) and (24) follow straightforwardly11.\nACKNOWLEDGMENTS\nThis work was supported by the Machine Intelligence Research Institute (Berkeley, California,\nUSA).\nWe wish to thank Alexander Appel for reviewing drafts of this work and providing helpful feedback.\n11Note that there is an additional factor of N since we now consider bounds for ﬁxed k ∈[N] rather than\nfor average over k.\n21\nPublished as a conference paper at ICLR 2019\nREFERENCES\nPeter L. Bartlett. Regal: A regularization based algorithm for reinforcement learning in weakly com-\nmunicating mdps. In In Proceedings of the 25th Annual Conference on Uncertainty in Artiﬁcial\nIntelligence, 2009.\nSbastien\nBubeck\nand\nNicol\nCesa-Bianchi.\nRegret\nanalysis\nof\nstochastic\nand\nnon-\nstochastic multi-armed bandit problems.\nFoundations and Trends in Machine Learn-\ning,\n5(1):1–122,\n2012.\nISSN\n1935-8237.\ndoi:\n10.1561/2200000024.\nURL\nhttp://dx.doi.org/10.1561/2200000024.\nJ. Clouse. On integrating apprentice learning and reinforcement learning. Technical report, Amherst,\nMA, USA, 1997.\nEugene A. Feinberg and Adam Shwartz (eds.). Handbook of Markov Decision Processes. Springer,\n2002.\nJavier Garc´ıa and Fernando Fern´andez.\nA comprehensive survey on safe reinforcement learn-\ning.\nJ. Mach. Learn. Res., 16(1):1437–1480, January 2015.\nISSN 1532-4435.\nURL\nhttp://dl.acm.org/citation.cfm?id=2789272.2886795.\nPhuong Nguyen, Odalric-Ambrym Maillard, Daniil Ryabko, and Ronald Ortner.\nCompet-\ning with an inﬁnite set of models in reinforcement learning.\nIn Carlos M. Carvalho\nand Pradeep Ravikumar (eds.), Proceedings of the Sixteenth International Conference on\nArtiﬁcial Intelligence and Statistics, volume 31 of Proceedings of Machine Learning Re-\nsearch, pp. 463–471, Scottsdale, Arizona, USA, 29 Apr–01 May 2013. PMLR.\nURL\nhttp://proceedings.mlr.press/v31/nguyen13a.html.\nIan\nOsband\nand\nBenjamin\nVan\nRoy.\nModel-based\nreinforcement\nlearning\nand\nthe\neluder\ndimension.\nIn\nZ.\nGhahramani,\nM.\nWelling,\nC.\nCortes,\nN.\nD.\nLawrence,\nand\nK.\nQ.\nWeinberger\n(eds.),\nAdvances\nin\nNeural\nInformation\nPro-\ncessing\nSystems\n27,\npp.\n1466–1474.\nCurran\nAssociates,\nInc.,\n2014.\nURL\nhttp://papers.nips.cc/paper/5245-model-based-reinforcement-learning-and-the-eluder\nIan Osband, Benjamin Van Roy, and Daniel Russo. (more) efﬁcient reinforcement learning via\nposterior sampling. In Proceedings of the 26th International Conference on Neural Information\nProcessing Systems - Volume 2, NIPS’13, pp. 3003–3011, USA, 2013. Curran Associates Inc.\nURL http://dl.acm.org/citation.cfm?id=2999792.2999947.\nDaniel Russo and Benjamin Van Roy.\nAn information-theoretic analysis of thompson sam-\npling.\nJ. Mach. Learn. Res., 17(1):2442–2471, January 2016.\nISSN 1532-4435.\nURL\nhttp://dl.acm.org/citation.cfm?id=2946645.3007021.\n22\n",
  "categories": [
    "cs.LG",
    "stat.ML",
    "68Q32",
    "I.2.6"
  ],
  "published": "2019-07-19",
  "updated": "2019-07-19"
}