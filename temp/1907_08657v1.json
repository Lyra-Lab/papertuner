{
  "id": "http://arxiv.org/abs/1907.08657v1",
  "title": "Learning More From Less: Towards Strengthening Weak Supervision for Ad-Hoc Retrieval",
  "authors": [
    "Dany Haddad",
    "Joydeep Ghosh"
  ],
  "abstract": "The limited availability of ground truth relevance labels has been a major\nimpediment to the application of supervised methods to ad-hoc retrieval. As a\nresult, unsupervised scoring methods, such as BM25, remain strong competitors\nto deep learning techniques which have brought on dramatic improvements in\nother domains, such as computer vision and natural language processing. Recent\nworks have shown that it is possible to take advantage of the performance of\nthese unsupervised methods to generate training data for learning-to-rank\nmodels. The key limitation to this line of work is the size of the training set\nrequired to surpass the performance of the original unsupervised method, which\ncan be as large as $10^{13}$ training examples. Building on these insights, we\npropose two methods to reduce the amount of training data required. The first\nmethod takes inspiration from crowdsourcing, and leverages multiple\nunsupervised rankers to generate soft, or noise-aware, training labels. The\nsecond identifies harmful, or mislabeled, training examples and removes them\nfrom the training set. We show that our methods allow us to surpass the\nperformance of the unsupervised baseline with far fewer training examples than\nprevious works.",
  "text": "Learning More From Less\nTowards Strengthening Weak Supervision for Ad-Hoc Retrieval\nDany Haddad∗\ndanyhaddad@utexas.edu\nThe University of Texas at Austin\nJoydeep Ghosh\nghosh@ece.utexas.edu\nThe University of Texas at Austin\nABSTRACT\nThe limited availability of ground truth relevance labels has been\na major impediment to the application of supervised methods to\nad-hoc retrieval. As a result, unsupervised scoring methods, such\nas BM25, remain strong competitors to deep learning techniques\nwhich have brought on dramatic improvements in other domains,\nsuch as computer vision and natural language processing. Recent\nworks have shown that it is possible to take advantage of the per-\nformance of these unsupervised methods to generate training data\nfor learning-to-rank models. The key limitation to this line of work\nis the size of the training set required to surpass the performance\nof the original unsupervised method, which can be as large as 1013\ntraining examples. Building on these insights, we propose two meth-\nods to reduce the amount of training data required. The first method\ntakes inspiration from crowdsourcing, and leverages multiple un-\nsupervised rankers to generate soft, or noise-aware, training labels.\nThe second identifies harmful, or mislabeled, training examples and\nremoves them from the training set. We show that our methods\nallow us to surpass the performance of the unsupervised baseline\nwith far fewer training examples than previous works.\nCCS CONCEPTS\n• Information systems →Retrieval models and ranking.\nKEYWORDS\nInformation retrieval, Noisy Labels, Weak Supervision, Neural Net-\nwork, Deep Learning\nACM Reference Format:\nDany Haddad and Joydeep Ghosh. 2019. Learning More From Less: Towards\nStrengthening Weak Supervision for Ad-Hoc Retrieval. In Proceedings of the\n42nd International ACM SIGIR Conference on Research and Development in\nInformation Retrieval (SIGIR ’19), July 21–25, 2019, Paris, France. ACM, New\nYork, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331272\n1\nINTRODUCTION\nClassical ad-hoc retrieval methods have relied primarily on unsu-\npervised signals such as BM25, TF-IDF, and PageRank as inputs\n∗Work done while interning at CognitiveScale.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nSIGIR ’19, July 21–25, 2019, Paris, France\n© 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-6172-9/19/07...$15.00\nhttps://doi.org/10.1145/3331184.3331272\nto learning-to-rank (LeToR) models. Supervision for these models\nis often supplied in the form of click-stream logs or hand-curated\nrankings, both of which come with their issues and limitations.\nFirst, both sources are typically limited in availability and are often\nproprietary company resources. Second, click-stream data is typi-\ncally biased towards the first few elements in the ranking presented\nto the user [2] and are noisy in general. Finally, such logs are only\navailable after the fact, leading to a cold start problem. These issues\nmotivate the search for an alternate source of “ground truth” ranked\nlists to train our LeToR model on.\nIn [7], Dehghani et al. show that the output of an unsupervised\ndocument retrieval method can be used to train a supervised rank-\ning model that outperforms the original unsupervised ranker. More\nrecently, [13] proposed a hierarchical interaction based model that\nis trained on a similarly generated training set. These works have\nshown the potential of leveraging unsupervised methods as sources\nof weak supervision for the retrieval task. However, they require\ntraining on as many as 1013 training examples to surpass the per-\nformance of the unsupervised baseline [7, 13].\nIn this work, we substantially reduce this number by making\nmore effective use of the generated training data. We present two\nmethods that make improvements in this direction, and beat the\nunsupervised method using fewer than 10% of the training rankings\ncompared to previous techniques.\nIn the first method, we take a crowdsourcing approach and collect\nthe output of multiple unsupervised retrieval models. Following\n[19], we learn a joint distribution over the outputs of said retrieval\nmodels and generate a new training set of soft labels. We call this\nthe noise-aware model. The noise-aware model does not require\naccess to any gold labels1.\nOur second method builds on the idea of dataset debugging and\nidentifies training examples with the most harmful influence [10]\n(the labels most likely to be incorrect) and drops them from the\ntraining set. We call this the influence-aware model.\n2\nRELATED WORK\nMuch of the prior work in handling noisy datasets has been in the\ncontext of a classifier from noisy labels. In the binary classification\ncontext, noise is seen as a class-conditional probability that an\nobserved label is the opposite of the true label [8, 14]. In the ranking\ncontext, we typically expect that models trained using pairwise or\nlistwise loss functions will far outperform pointwise approaches\n[11]. Since the label of a pair is determined by the ordering of the\ndocuments within the pair, it is not immediately obvious how the\nclass-conditional flip probabilities translate to this formulation. The\nrelationship to listwise objectives is not straightforward either.\n1To differentiate them from labels originating from weak supervision sources, we refer\nto relevance scores assigned by a human as “gold” labels\narXiv:1907.08657v1  [cs.IR]  19 Jul 2019\nSIGIR ’19, July 21–25, 2019, Paris, France\nHaddad and Ghosh\nIn [5] and [6], Dehghani et al. introduce two semi-supervised\nstudent-teacher models where the teacher weights the contribution\nof each sample in the student model’s training loss based on its\nconfidence in the quality of the label. They train the teacher on a\nsmall subset of gold labels and use the model’s output as confidence\nweights for the student model. [5] shows that using this approach,\nthey can beat the unsupervised ranker using ~75% of the data re-\nquired when training directly on the noisy data. They train a cluster\nof 50 gaussian processes to form the teacher annotations which are\nused to generate soft labels to fine-tune the student model.\nIn [19], Ratner et al. transform a set of weak supervision sources,\nthat may disagree with each other, into soft labels used to train a\ndiscriminative model. They show experimentally that this approach\noutperforms the naïve majority voting strategy for generating the\ntarget labels. This inspires our noise-aware approach.\nIn [10], Koh et al. apply classical results from regression analysis\nto approximate the change in loss at a test point caused by removing\na specific point from the training set. They show experimentally\nthat their method approximates this change in loss well, even for\nhighly non-linear models, such as GoogLeNet. They also apply\ntheir method to prioritize training examples to check for labeling\nerrors. Our influence-aware approach uses influence functions [10]\nto identify mislabeled training examples.\n3\nPROPOSED METHODS\n3.1\nModel Architecture\nIn this work, we only explore pairwise loss functions since they typi-\ncally lead to better performing models than the pointwise approach.\nListwise approaches, although typically the most effective, tend to\nhave high training and inference time computational complexity\ndue to their inherently permutation based formulations [11].\nWe consider a slight variant of the Rank model proposed in [7]\nas our baseline model. We represent the tokens in the ith query\nas tq\ni and the tokens in the ith document as td\ni . We embed these\ntokens in a low dimensional space with a mapping E : V 7→Rl\nwhere V is the vocabulary and l is the embedding dimension. We\nalso learn token dependent weights W : V 7→R. Our final repre-\nsentation for a query q is a weighted sum of the word embeddings:\nvq = Í\ntq\nj ∈tq ˜Wq(tq\nj )E(tq\nj ) where ˜Wq indicates that the weights are\nnormalized to sum to 1 across tokens in the query q using a soft-\nmax operation. The vector representation for documents is defined\nsimilarly.\nIn addition, we take the difference and elementwise products\nof the document and query vectors and concatenate them into a\nsingle vector vq,d = [vq,vd,vq −vd,vq ⊙vd]. We compute the\nrelevance score of a document, d, to a query, q by passing vq,d\nthrough a feed-forward network with ReLU activations and scalar\noutput. We use a tanh at the output of the rank model and use the\nraw logit scores otherwise. We represent the output of our model\nparameterized by θ as f (x;θ).\nOur training set Z is a set of tuples z = (q,d1,d2,sq,d1,sq,d2)\nwhere sq,di is the relevance score of di to q given by the unsuper-\nvised ranker. The pairwise objective function we minimize is given\nby:\nL(Z;θ) =\nÕ\nz ∈Z\nL(f (vq,d1;θ) −f (vq,d2;θ),relq,(d1,d2))\n(1)\nLce(x,y) = y · log(σ(x)) + (1 −y) · log(1 −σ(x))\n(2)\nLhinдe(x,y) = max{0,ϵ −sign(y) · x}\n(3)\nWhere relq,(d1,d2) ∈[0, 1] gives the relative relevance of d1 and\nd2 to q. L is either Lce or Lhinдe for cross-entropy or hinge loss,\nrespectively. The key difference between the rank and noise-aware\nmodels is how relq,(d1,d2) is determined. As in [7], we train the rank\nmodel by minimizing the max-margin loss and compute relq,(d1,d2)\nas sign(sq,d1 −sq,d2).\nDespite the results in [21] showing that the max-margin loss\nexhibits stronger empirical risk guarantees for ranking tasks using\nnoisy training data, we minimize the cross-entropy loss in each of\nour proposed models for the following reasons: in the case of the\nnoise-aware model, each of our soft training labels are a distribution\nover {0, 1}, so we seek to learn a calibrated model rather than\none which maximizes the margin (as would be achieved using a\nhinge loss objective). For the influence-aware model, we minimize\nthe cross-entropy rather than the hinge loss since the method of\ninfluence functions relies on having a twice differentiable objective.\n3.2\nNoise-aware model\nIn this approach, relq,(di,dj) ∈[0, 1] are soft relevance labels. For\neach of the queries in the training set, we rank the top documents\nby relevance using k unsupervised rankers. Considering ordered\npairs of these documents, each ranker gives a value of 1 if it agrees\nwith the order, −1 if it disagrees and 0 if neither document appears\nin the top 10 positions of the ranking. We collect these values\ninto a matrix Λ ∈{−1, 0, 1}m×k for m document pairs. The joint\ndistribution over these pairwise preferences and the true pairwise\norderings y is given by:\nPw(Λ,y) =\n1\nZ(w) exp(\nm\nÕ\ni\nwT ϕ(Λi,yi))\n(4)\nWhere w is a vector of learned parameters and Z(w) is the par-\ntition function. A natural choice for ϕ is to model the accuracy\nof each individual ranker in addition to the pairwise correlations\nbetween each of the rankers. So for the ith document pair, we have\nthe following expression for ϕi B ϕ(Λi,yi):\nϕi = [{Λij = yi }1≤j ≤k ||{Λij = Λil , 0}j,l]\nSince the true relevance preferences are unknown, we treat them\nas latent. We learn the parameters for this model without any gold\nrelevance labels y by maximizing the marginal likelihood (as in\n[19]) given by:\nmax\nw\nlog\nÕ\ny\nPw(Λ,y)\n(5)\nWe use the Snorkel library2 to optimize equation 5 by stochastic\ngradient descent, where we perform Gibbs sampling to estimate\nthe gradient at each step. Once we have determined the parameters\nof the model, we can evaluate the posterior probabilities Pw(yi |Λi)\nwhich we use as our soft training labels.\n2https://github.com/HazyResearch/snorkel\nLearning More From Less\nSIGIR ’19, July 21–25, 2019, Paris, France\n3.3\nInfluence Aware Model\nIn this approach, we identify training examples that hurt the gen-\neralization performance of the trained model. We expect that many\nof these will be incorrectly labeled, and that our model will per-\nform better if we drop them from the training set. The influence of\nremoving a training example zi = (xi,yi) on the trained model’s\nloss at a test point ztest is computed as [10]:\n∆L(ztest ;θ) ≈Idrop(zi,ztest )\n(6)\n= 1\nn ∇θ L(ztest ;θ)T H−1\nθ ∇θ L(zi;θ)\n(7)\nwhereHθ is the Hessian of the objective function. If Idrop(zi,ztest )\nis negative, then zi is a harmful training example for ztest since it’s\ninclusion in the training set causes an increase in the loss at that\npoint. Summing this value over the entire test set gives us Idrop(zi).\nWe compute Idrop(zi) for each training example zi, expecting it\nto represent zi’s impact on the model’s performance at test time.\nIn our setup, we know that some of our training examples are\nmislabeled; we expect that these points will have a large negative\nvalue for Idrop. Of course, for a fair evaluation, the ztest points are\ntaken from the development set used for hyperparameter tuning\n(see section 4).\nWe address the computational constraints of computing (7) by\ntreating our trained model as a logistic regression on the bottle-\nneck features. We freeze all model parameters except the last layer\nof the feed-forward network and compute the gradient with re-\nspect to these parameters only. This gradients can be computed in\nclosed form in an easily parallelizable way, allowing us to avoid\ntechniques that rely on autodifferentiation operations [16]. We\ncompute H−1\nθ ∇θ L(ztest ;θ) for every ztest using the method of con-\njugate gradients following [20]. We also add a small damping term\nto the diagonal of the Hessian to ensure that it is positive definite\n[12].\n4\nDATA PREPROCESSING AND MODEL\nTRAINING\nWe evaluate the application of our methods to ad-hoc retrieval on\nthe Robust04 corpus with the associated test queries and relevance\nlabels. As in [7], our training data comes from the AOL query\nlogs [15] on which we perform similar preprocessing. We use the\nIndri3 search engine to conduct indexing and retrieval and use the\ndefault parameters for the query likelihood (QL) retrieval model\n[18] which we use as the weak supervision source. We fetch only\nthe top 10 documents from each ranking in comparison to previous\nworks which trained on as many as the top 1000 documents for\neach query. To compensate for this difference, we randomly sample\nnneд additional documents from the rest of the corpus for each of\nthese 10 documents. We train our model on a random subset of\n100k rankings generated by this process. This is fewer than 10% the\nnumber of rankings used in previous works [7, 13], each of which\ncontains far fewer document pairs.\n3https://www.lemurproject.org/indri.php\nTable 1: Results comparison with smoothing.\nRank Model\nNoise-\nAware\nInfluence-\nAware\nQL\nNDCG@10\n0.3881\n† 0.3952\n†0.4008\n0.3843\nPrec@10\n0.3535\n† 0.3621\n†0.3657\n0.3515\nMAP\n0.2675\n† 0.2774\n†0.2792\n0.2676\nFor the word embedding representations,W , we use the 840B.300d\nGloVe [17] pretrained word embedding set4. The feed-forward net-\nwork hidden layer sizes are chosen from {512, 256, 128, 64} with up\nto 5 layers. We use the first 50 queries in the Robust04 dataset as\nour development set for hyperparameter selection, computation of\nIdrop and early stopping. The remaining 200 queries are used for\nevaluation.\nDuring inference, we rank documents by the output of the feed-\nforward network. Since it is not feasible to rank all the documents\nin the corpus, we fetch the top 100 documents using the QL retrieval\nmodel and then rerank using the trained model’s scores.\n4.1\nModel Specific Details\nFor the noise-aware model, we generate separate rankings for each\nquery using the following retrieval methods: Okapi BM25, TF-IDF,\nQL, QL+RM3 [1] using Indri with the default parameters.\nFor the influence-aware model, we train the model once on the\nfull dataset and then compute Idrop(zi) for each training point\ndropping all training examples with a negative value for Idrop(zi)\nwhich we find to typically be around half of the original training\nset. We then retrain the model on this subset.\nInterestingly, we find that using a smaller margin, ϵ, in the train-\ning loss of the rank model leads to improved performance. Using a\nsmaller margin incurs 0 loss for a smaller difference in the model’s\nrelative preference between the two documents. Intuitively, this\nallows for less overfitting to the noisy data. We use a margin of 0.1\nchosen by cross-validation.\nThe noise-aware and influence-aware models train end-to-end in\naround 12 and 15 hours respectively on a single NVIDIA Titan Xp.\n5\nEXPERIMENTAL RESULTS\nWe compare our two methods against two baselines, the unsuper-\nvised ranker (QL) and the rank model. Compared to the other unsu-\npervised rankers (see section 4.1) used as input to the noise-aware\nmodel, the QL ranker performs the best on all metrics. Training\nthe rank model on the results of the majority vote of the set of\nunsupervised rankers used for the noise-aware model performed\nvery similarly to the rank model, so we only report results of the\nrank model. We also compare the results after smoothing with the\nnormalized QL document scores by linear interpolation.\nThe results in tables 1 and 2 show that the noise-aware and\ninfluence-aware models perform similarly, with both outperforming\nthe unsupervised baseline. Bold items are the largest in their row\nand daggers indicate statistically significant improvements over the\nrank model at a level of 0.05 using Bonferroni correction. Figure\n1 shows that the rank model quickly starts to overfit. This does\n4https://nlp.stanford.edu/projects/glove/\nSIGIR ’19, July 21–25, 2019, Paris, France\nHaddad and Ghosh\nTable 2: Results comparison without smoothing.\nRank Model\nNoise-Aware\nInfluence-Aware\nNDCG@10\n0.2610\n† 0.2886\n†0.2966\nPrec@10\n0.2399\n†0.2773\n† 0.2742\nMAP\n0.1566\n† 0.1831\n†0.1839\nFigure 1: Test NDCG@10 during training\nnot contradict the results in [7] since in our setup we train on far\nfewer pairs of documents for each query, so each relevance label\nerror has much greater impact. For each query, our distribution\nover documents is uniform outside the results from the weak su-\npervision source, so we expect to perform worse than if we had\na more faithful relevance distribution. Our proposed approaches\nuse an improved estimate of the relevance distribution at the most\nimportant positions in the ranking, allowing them to perform well.\nWe now present two representative training examples showing\nhow our methods overcome the limitations of the rank model.\nExample 5.1. The method in section 3.2 used to create labels for\nthe noise-aware model gives the following training example an un-\nconfident label (~0.5) rather than a relevance label of 1 or 0: (q=“town\nof davie post office”, (d1=FBIS3-25584, d2=FT933-13328)) where\nd1 is ranked above d2. Both of these documents are about people\nnamed “Davie” rather than about a town or a post office, so it is\nreasonable to avoid specifying a hard label indicating which one is\nexplicitly more relevant.\nExample 5.2. One of the most harmful training points as deter-\nmined by the method described in section 3.3 is the pair (q=“pictures\nof easter mice”, (d1=FT932-15650, d2=LA041590-0059)) where d1\nis ranked above d2. d1 discusses the computer input device and d2\nis about pictures that are reminiscent of the holiday. The incorrect\nrelevance label explains why the method identifies this as a harmful\ntraining example.\n6\nCONCLUSIONS AND FUTURE WORK\nWe have presented two approaches to reduce the amount of weak\ndata needed to surpass the performance of the unsupervised method\nthat generates the training data. The noise-aware model does not re-\nquire ground truth labels, but has an additional data dependency on\nmultiple unsupervised rankers. The influence-aware model requires\na small set of gold-labels in addition to a re-train of the model,\nalthough empirically, only around half the dataset is used when\ntraining the second time around.\nInteresting paths for future work involve learning a better joint\ndistribution for training the noise-aware model or leveraging ideas\nfrom [22] to construct soft training labels rather than for the query\nperformance prediction task. Similarly, we could apply ideas from\nunsupervised LeToR [4] to form better noise-aware labels. For the\ninfluence-aware model, we could use the softrank loss [3] rather\nthan cross-entropy and instead compute set influence rather than\nthe influence of a single training example [9].\nREFERENCES\n[1] Nasreen Abdul-Jaleel, James Allan, Bruce Croft, Fernando Diaz, Leah Larkey,\nXiaoyan Li, Donald Metzler, Mark D. Smucker, Trevor Strohman, Howard Turtle,\nand Courtney Wade. 2004. Umass at trec 2004: Notebook. academia.edu (2004).\n[2] Qingyao Ai, Keping Bi, Cheng Luo, Jiafeng Guo, and W Bruce Croft. 2018. Unbi-\nased Learning to Rank with Unbiased Propensity Estimation. In The 41st Interna-\ntional ACM SIGIR Conference. ACM Press, New York, New York, USA, 385–394.\nhttps://doi.org/10.1145/3209978.3209986\n[3] Ricardo Baeza-Yates, Berthier de Araújo Neto Ribeiro, et al. 2007. Learning to\nrank with nonsmooth cost functions. NIPS (2007).\n[4] Avradeep Bhowmik and Joydeep Ghosh. 2017. LETOR Methods for Unsupervised\nRank Aggregation. In the 26th International Conference. ACM Press, New York,\nNew York, USA, 1331–1340. https://doi.org/10.1145/3038912.3052689\n[5] Mostafa Dehghani, Arash Mehrjou, Stephan Gouws, Jaap Kamps, and Bern-\nhard Schölkopf. 2017.\nFidelity-Weighted Learning.\narXiv.org (Nov. 2017).\narXiv:cs.LG/1711.02799v2\n[6] Mostafa Dehghani, Aliaksei Severyn, Sascha Rothe, and Jaap Kamps. 2017. Learn-\ning to Learn from Weak Supervision by Full Supervision. arXiv.org (Nov. 2017),\n1–8. arXiv:1711.11383\n[7] Mostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, and W Bruce\nCroft. 2017. Neural Ranking Models with Weak Supervision. In the 40th Inter-\nnational ACM SIGIR Conference. ACM Press, New York, New York, USA, 65–74.\nhttps://doi.org/10.1145/3077136.3080832\n[8] Xinxin Jiang, Shirui Pan, Guodong Long, Fei Xiong, Jing Jiang, and Chengqi\nZhang. 2017. Cost-sensitive learning with noisy labels. JMLR (2017).\n[9] Rajiv Khanna, Been Kim, Joydeep Ghosh, and Oluwasanmi Koyejo. 2018. In-\nterpreting Black Box Predictions using Fisher Kernels. arXiv.org (Oct. 2018).\narXiv:cs.LG/1810.10118v1\n[10] Pang Wei Koh and Percy Liang. 2017. Understanding Black-box Predictions via\nInfluence Functions. arXiv.org (March 2017), 1–11. arXiv:1703.04730\n[11] Tie-Yan Liu. 2009. Learning to Rank for Information Retrieval. Foundations and\nTrends® in Information Retrieval 3, 3 (2009), 225–331. https://doi.org/10.1561/\n1500000016\n[12] James Martens. 2010. Deep learning via Hessian-free optimization. (2010).\n[13] Yifan Nie, Alessandro Sordoni, and Jian-Yun Nie. 2018. Multi-level Abstraction\nConvolutional Model with Weak Supervision for Information Retrieval. In The\n41st International ACM SIGIR Conference. ACM Press, New York, New York, USA,\n985–988. https://doi.org/10.1145/3209978.3210123\n[14] Curtis G Northcutt, Tailin Wu, and Isaac L Chuang. 2017. Learning with Confident\nExamples: Rank Pruning for Robust Classification with Noisy Labels. arXiv.org\n(May 2017). arXiv:1705.01936\n[15] Greg Pass, Abdur Chowdhury, and Cayley Torgeson. 2006. A Picture of Search.\nInfoscale (2006), 1–es. https://doi.org/10.1145/1146847.1146848\n[16] Barak Pearlmutter. 1994. Fast exact multiplication by the Hessian. MIT Press 6, 1\n(Jan. 1994), 147–160. https://doi.org/10.1162/neco.1994.6.1.147\n[17] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2015. GloVe:\nGlobal Vectors for Word Representation.\n[18] Jay M Ponte and W Bruce Croft. 1998. A Language Modeling Approach to\nInformation Retrieval. SIGIR (1998), 275–281. https://doi.org/10.1145/290941.\n291008\n[19] Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and\nChristopher Ré. 2017. Snorkel. Proceedings of the VLDB Endowment 11, 3 (Nov.\n2017), 269–282. https://doi.org/10.14778/3157794.3157797\n[20] Jonathan R Shewchuk. 1994. An introduction to the conjugate gradient method\nwithout the agonizing pain. (1994).\n[21] Hamed Zamani and W Bruce Croft. 2018. On the Theory of Weak Supervision for\nInformation Retrieval. ACM, New York, New York, USA. https://doi.org/10.1145/\n3234944.3234968\n[22] Hamed Zamani, W Bruce Croft, and J Shane Culpepper. 2018. Neural Query\nPerformance Prediction using Weak Supervision from Multiple Signals. In The\n41st International ACM SIGIR Conference. ACM Press, New York, New York, USA,\n105–114. https://doi.org/10.1145/3209978.3210041\n",
  "categories": [
    "cs.IR",
    "cs.LG"
  ],
  "published": "2019-07-19",
  "updated": "2019-07-19"
}