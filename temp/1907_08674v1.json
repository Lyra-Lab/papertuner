{
  "id": "http://arxiv.org/abs/1907.08674v1",
  "title": "Deep Learning to Address Candidate Generation and Cold Start Challenges in Recommender Systems: A Research Survey",
  "authors": [
    "Kiran Rama",
    "Pradeep Kumar",
    "Bharat Bhasker"
  ],
  "abstract": "Among the machine learning applications to business, recommender systems\nwould take one of the top places when it comes to success and adoption. They\nhelp the user in accelerating the process of search while helping businesses\nmaximize sales. Post phenomenal success in computer vision and speech\nrecognition, deep learning methods are beginning to get applied to recommender\nsystems. Current survey papers on deep learning in recommender systems provide\na historical overview and taxonomy of recommender systems based on type. Our\npaper addresses the gaps of providing a taxonomy of deep learning approaches to\naddress recommender systems problems in the areas of cold start and candidate\ngeneration in recommender systems. We outline different challenges in\nrecommender systems into those related to the recommendations themselves\n(include relevance, speed, accuracy and scalability), those related to the\nnature of the data (cold start problem, imbalance and sparsity) and candidate\ngeneration. We then provide a taxonomy of deep learning techniques to address\nthese challenges. Deep learning techniques are mapped to the different\nchallenges in recommender systems providing an overview of how deep learning\ntechniques can be used to address them. We contribute a taxonomy of deep\nlearning techniques to address the cold start and candidate generation problems\nin recommender systems. Cold Start is addressed through additional features\n(for audio, images, text) and by learning hidden user and item representations.\nCandidate generation has been addressed by separate networks, RNNs,\nautoencoders and hybrid methods. We also summarize the advantages and\nlimitations of these techniques while outlining areas for future research.",
  "text": " \n \nDeep Learning to Address Candidate Generation and \nCold Start Challenges in Recommender Systems: A \nResearch Survey \n \n \n \nKiran R, Pradeep Kumar, Bharat Bhasker \n (efpm04013@iiml.ac.in, pradeepkumar@iiml.ac.in, bhasker@iimraipur.ac.in) \nIT and Systems Department \nIndian Institute of Management Lucknow \nLucknow 226013, India \n \n \n \nCorresponding Author: Kiran Rama  \nAddress: No 295, 3rd Block, 6th Cross, HMT Layout, Vidyaranyapura PO, Bangalore - 560097  \nTelephone : +91 9900601013 \nEmail: efpm04013@iiml.ac.in \n \n \n \nAuthor: Prof Pradeep Kumar \nIndian Institute of Management Lucknow , Off Sitapur Road , Lucknow 226013, India \n \n \n \nAuthor: Prof Bharat Bhasker \nIndian Institute of Management Raipur, GEC Campus, Sejbahar, Raipur, Chhattisgarh 492015, \nIndia \n \n \n \n \nKeywords: Recommender Systems, Challenges in Recommender Systems, Deep Learning, \nCandidate Generation, Cold Start, Accuracy, CNN, RNN, GAN, Autoencoders, DNN, DBN \nABSTRACT \nAmong the machine learning applications to business, recommender systems would take one \nof the top places when it comes to success and adoption. They help the user in accelerating the \nprocess of search while helping businesses maximize sales. Post phenomenal success in \ncomputer vision and speech recognition, deep learning methods are beginning to get applied to \nrecommender systems. Current survey papers on deep learning in recommender systems provide \na historical overview and taxonomy of recommender systems based on type. Our paper addresses \nthe gaps of providing a taxonomy of deep learning approaches to address recommender systems \nproblems in the areas of cold start and candidate generation in recommender systems. We outline \ndifferent challenges in recommender systems into those related to the recommendations \nthemselves (include relevance, speed, accuracy and scalability), those related to the nature of the \ndata (cold start problem, imbalance and sparsity) and candidate generation. We then provide a \ntaxonomy of deep learning techniques to address these challenges. Deep learning techniques \nincluding convolutional neural networks, recurrent neural networks, autoencoders, stacked multi-\nlayer perceptron, stacked Restricted Boltzmann Machines and Generative Adversarial Networks \nare categorized as supervised and unsupervised, as well as based on their applicability to \ndifferent types of data. These techniques are mapped to the different challenges in recommender \nsystems providing an overview of how deep learning techniques can be used to address them. \nWe contribute a taxonomy of deep learning  techniques to address the cold start and candidate \ngeneration problems in recommender systems. Cold Start is addressed through additional \nfeatures (for audio, images, text) and by learning hidden user and item representations. Candidate \ngeneration has been addressed by separate networks, RNNs, autoencoders and hybrid methods. \nWe also summarize the advantages and limitations of these techniques while outlining areas for \nfuture research. \n1 INTRODUCTION  \nThe business world today is characterized by information overload and increased \ncompetition for customer’s attention and wallet share. In this context, Recommender Systems \nprovide several benefits to both consumers and business firms. They help consumers with \nreduced search costs, increased product fit and management of choice overload. They help \nbusiness firms with higher sales volume, higher web usage, higher customer retention and higher \nmargins (Lee, Huntsman, Fl, Huntsman, & Fl, 2014). Several successes of recommender systems \nhave been highlighted like 80% of movies watched on Netflix came from recommendation \n(Gomez-Uribe & Hunt, 2015)  and 60% of video clicks came from home page recommendations \non YouTube (Davidson et al., 2010). We define Recommender systems based on several \ndefinitions in literature as  \n“Systems that seek to predict the future preference of a set of items for a user either as a \nnumeric rating for the item or as a list of recommendations or as a binary score indicating \npreference for the item” (Handbook, RICCI, & ROKACH, 2011), (Portugal, Alencar, & Cowan, \n2017) (Cheng et al., 2016) (S. Zhang & Yao, 2017) \nAdvances in graphical processing unit hardware and decreasing costs have made GPUs \naccessible. With their multi-core nature, they enable highly parallel matrix computations that are \nat the core of Deep Learning. GPUs have been found to speed up the learning of a deep learning \nnetwork by a factor of more than 50 (Schmidhuber, 2015). Consequently, Deep Learning has \nseen tremendous success in computer vision and speech recognition. Deep Learning methods \nemploy neural networks with multiple hidden layers and are shown to be infinitely flexible \nfunctions capable of solving any problem provided there is sufficient data and computing power. \nThey fit these several thousands of parameters using gradient descent. There have been \ntremendous improvements in backpropagation and gradient descent since the seminal paper on \ngradient descent in 1986 (Rumelhart, Hinton, & William, 1985). There are several methods for \noptimization used like RMSProp (Dauphin, de Vries, & Bengio, 2015), AdaDelta, AdaMax, \nNadam, AMSGrad, Adam (Kingma & Ba, 2015) etc. The advancement of several frameworks \nlike PyTorch from Facebook (Ketkar, 2017), MXNet from Apache (Chen et al., 2015) and \nTensorFlow (Google, 2018) from Google have increased the adoption of deep learning.  \nThe motivation to write this survey paper is to not to communicate the list of results but \nto provide an understanding into deep learning applications in recommender systems, providing \na taxonomy of how deep learning techniques have been used to address the candidate generation \nand cold start challenges in recommender systems. \n1.1 RESEARCH GAP AND OBJECTIVE \nThere exist several surveys in traditional recommender systems but to the best of our \nknowledge, there are very few surveys on the application of deep learning to recommender \nsystems. (S. Zhang & Yao, 2017) classified the techniques into methods that solely relied on \ndeep learning and those that integrate deep learning with traditional recommender systems, \nfurther breaking them as loosely coupled and tightly coupled. The paper provides a historical \ncourse of development of techniques in the field. Another classification of deep learning for \nrecommender systems is bucketing them into content, collaborative and hybrid buckets (Singhal, \nSinha, & Pant, 2017). (Betru & Onana, 2017) is another survey paper on deep learning in \nrecommender systems.  None of these survey papers provide a taxonomy of deep learning \nmethods based on the challenges of recommender systems that they address. \nThe research objective of this paper is to address the literature gaps mapping different deep \nlearning techniques against the recommendation system challenges they address and provide a \nmapping of the techniques into applicable scenarios. The scope of this paper will be limited to \nthe application of deep learning techniques to address the cold start and candidate generation \nproblems in recommender systems. \n1.2 RESEARCH METHODOLOGY \nWe survey different research papers on the application of deep learning to recommender \nsystems. We extract articles from Google Scholar Search for the period 2009-present using \ncombinations of the following keywords: deep learning, recommender systems, rec sys, recurrent \nneural networks, convolutional neural networks, stacking Restricted Boltzmann Machines, Deep \nNeural Networks, Autoencoders, Candidate Generation and Cold start. The resulting papers are \nmanually curated based off the paper title and abstracts to find the articles. The deep learning \ntechniques in the papers are then mapped to recommender system challenges building out a \ntaxonomy. We avoid taking the historical course of development approach or the taxonomy \nbased on content-based and collaborative filtering methods as these have already been addressed \nby several other survey papers. We focus on the techniques used to address specific challenges, \ntheir advantages and limitations and areas for further research. \n1.3 ORGANIZATION OF THE PAPER \nIn the first section, we introduce recommender systems and deep learning topics. We also \nhighlight the research objective and methodology. In the second section, we provide a \nbackground to the recommender system challenges and deep learning by providing a taxonomy. \nIn the next section, we contribute to the research on the topic by coming up with a taxonomy of \ndeep learning methods to handle the problems of cold start and candidate generation in \nrecommender systems. In the last two sections, we provide a summary of the limitations of the \nresearch in the area and conclude by summarizing our contributions and areas for future \nresearch. \n2 BACKGROUND \nIn this section, we summarize the challenges associated with recommender systems. We then \nprovide a taxonomy of the deep learning methods, both based on the technique and based on the \narea of applicability.  \n2.1 RECOMMENDER SYSTEM CHALLENGES \nThe challenges in recommender systems can be classified into those related to the \nrecommendations themselves, nature of the data and candidate generation. This is summarized in \nFigure 2.1. Challenges related to the recommendations themselves include speed, scalability, \naccuracy and relevance. Speed and Scalability are satisficing criteria while accuracy and \nrelevance are optimizing criteria. Recommender systems are expected to provide the outputs in a \nreasonable amount of time while scaling to large amounts of data. Accuracy is measured by \ndifferent metrics based on the nature of the task like RMSE, RMSLE (for regression problems), \nF-score, AUC, accuracy (for classification problems), NDCG and MAP (for ranking problems). \nThe key goal here in relevance to avoid noise and provide fresh recommendations. Challenges \nrelated to the nature of the data include cold start problem, imbalance and sparsity. When there is \nno history about the user to whom the recommendation is to be made or the item that is to be \nrecommended, the recommender systems run into the ‘cold start’ problem. Funnily, the origin of \nthe cold start problem was from cars in cold regions that had problems starting up and running \ntill it reaches its optimum temperature. For recommender systems, this means that there are not \nenough historical interactions for item or user. \n \n \n \n \n \n \n \nFig 2.1: Challenges in Recommender Systems \n \nLarger share of the ratings matrix is empty with no ratings for a user-item pair and this leads to the \nsparsity problem. Very few item-user combinations in the data matrix are minority class examples \nleading to the problem of imbalance. The last bucket of problem relates to candidate generation. \nCreating an entry for all user-item combinations is an onerous task and it becomes necessary to \ngenerate pairs of users and items for training. For example: With 1 million videos and 100 million \nusers, YouTube has a candidate generation problem as it cannot create an entry for every user-\nvideo pair. (Covington, Adams, & Sargin, 2016). \n \nRecommender \nSystems \nChallenges\nRecommendations \nRelated\nCandidate \nGeneration Related\nPerformance \nRelated\nRelevance\nAccuracy\nScalability\nSpeed\nNature of Data\nCold Start Problem\nSparsity\nImbalance\n2.2 TAXONOMY OF DEEP LEARNING METHODS \nIn figure 2.2, we categorize the deep learning techniques based on their complexity and type. \n \n \n \n \nFigure 2.2: Taxonomy of Deep Learning Methods by Method and Architecture Type \nMulti-layer perceptrons (MLP) are feedforward neural networks with multiple hidden \nlayers between the input layer and the output layer. Restricted Boltzmann Machines (RBM) are \ntwo layered neural networks with a visible layer and a hidden layer. Both MLPs and RBMs are \ntraditional neural networks. By stacking many RBNs together, we get a Deep Belief Network \nand by stacking several hidden layers in a MLP together, we get a Deep Neural Network (DNN). \nCNNs have several convolutional layers that apply filters followed by pooling layers with non-\nlinear transformations and have found huge success in image processing. RNNs are suitable for \nmodelling sequential recommendations and they are a unique type of neural network that contain \nloops that enable the networks to have memory storage. RNNs are of two types namely Long \nShort Term Memory (LSTM) and Gated Recurrent Units (GRU). Generative Adversarial \nDeep-Learning \nTypes\nStacked Hidden \nLayers\nDeep Neural \nNetworks\nStacked Multi Layer \nPerceptrons\nDeep Belief \nNetworks\nStacked Restricted \nBoltzmann \nMachines\nConvolutional \nNeural Network\nRecurrent Neural \nNetwork\nLong Short Term \nMemory Networks \n(LSTM)\nGated Recurrent \nUntis (GRU)\nDiscriminatory & \nGenerator \nNetworks\nGenerative \nAdversarial Neural \nNetworks\nAutoencoders\nNetworks (GANs) are generative neural networks that consist of a discriminator and a generator \nnetwork with both trained simultaneously competing with one another. All the methods \ndiscussed so far are supervised deep learning methods. Autoencoders are unsupervised models \nthat attempt to reconstruct the input data in the output layer and use the bottleneck layer as a \nsalient representation of the input. Figure 2.3 shows the classification of the deep learning \nmethods based on the popular types of data they are applied to. It is to be noted that these are not \nthe “only” applications but rather the most popular applications \n \nFigure 2.3: Taxonomy of Deep Learning Methods by Popular application type \nData Type & The \nDominant Deep \nLearning Method\nComputer Vision\nCNN\nSpeech \nCNN\nStructured Data \nDNN\nText Data\nWord \nEmbeddings\nRNN\nSequential Data\nRNN\nJoint Learning of \nLatent Features\nGAN\n3 PROPOSED TAXONOMY OF DEEP LEARNING \nTECHNIQUES TO ADDRESS COLD START AND \nIMBALANCE \nIn the section, we map the different deep learning techniques and variants to the \nrecommender system challenges. We then summarize the advantages and disadvantages of each \nof these approaches. \n3.1 COLD START \nCold Start Problem is one of “Making recommendations where there are no prior interactions \navailable for an user or an item” (Lam, Vu, & Le, 2008). (Bernardi, Kamps, Kiseleva, & \nMueller, 2015; Lika, Kolomvatsos, & Hadjiefthymiades, 2014) highlight the cold start problem \nand classify it into user cold start and item cold start cases, referring to cases of insufficient \nexamples of items and users respectively.   \n \n \n \nFig 3.1: Taxonomy of Deep Learning Methods to address Cold Start Problem \nDeep Learning \nMethods for Cold \nStart\nAudio Features\nLatent Audio \nFeatures\nCNN\nSequential Audio \nFeatures\nRNN\nImage Features\nCNN\nText Features\nRNNs\nLSTM Methods\nOrder Sensitive \nText Encoders\nCollaborative \nSequence \nModels\nGRU based \nmethods\nUser & Item \nHidden \nRepresentations\nAutoencoders\nCo-operative \nNeural Networks\nGenerative \nAdversarial \nNetworks\nThe usual approach to solving the cold start problem is to include additional user features by \ncombining content based and collaborative filtering based methods (Chiliguano & Fazekas, \n2016).  \nModern Song Recommender Systems fail to recommend tracks that are less widely known \nand suffer from the cold start problem. Deep convolutional networks have been used to generate \nlatent factors for songs from their audio when no usage is available (Oord, Dieleman, & \nSchrauwen, 2013) and using audio content of the song to add some item based features alleviates \nthe cold start problem. CNNs have been applied to extract features from images (Oord et al., \n2013). The sequential property of audio has been used to build a LSTM-based recurrent neural \nnetwork architecture in combination with audio based features from a convolutional networks \nhave been attempted. (Balakrishnan, 2014). Content-Based Deep Learning methods include \nextracting latent factors for songs from their audio (Liang, 2014), enhancing content based quote \nrecommendations(Tan, Wan, & Xiao, 2016), converting item text into latent features. GRU \nbased recurrent neural networks have also been used to convert item text into latent features to \nimprove the collaborative filtering results for the cold start problem (Bansal, Belanger, & \nMcCallum, 2016).  \nFor textual data, order sensitive encoders in combination with latent factor models have been \nused to address the cold start problem by employing RNNs to encode text sequences into latent \nfactors (Bansal et al., 2016). Extracted item representations are combined with user embeddings \nto get predicted ratings for each user-item pair. Compared to traditional methods that ignore \nword order, leveraging the word order in recurrent neural networks helps soften the cold start \nproblem. Collaborative Sequence models based on recurrent neural networks have also shown \ngood results. (Ko, Maystre, Grossglauser, Durrant, & Kim, 2016). (Devooght & Bersini, 2016) \nshow that collaborative filtering can be viewed as a sequence prediction problem and application \nof LSTMs is very competitive in addressing the cold start problem. \nLatent representations from the data are learnt in an unsupervised manner and the implicit \nrelationships between items and users are learnt from both the content and the rating (Xiaopeng \nLi & She, 2017). The autoencoder is forced to learn a latent distribution for content in latent \nspace instead of observation space through an inference network and can easily be extended to \nnon-text type multimedia as well. Instead of learning the user latent factors and item latent \nfactors separately, Co-operative Neural Networks (CoNN) learn hidden latent features for users \nand items jointly using two coupled neural networks. These are examples of Generative \nAdversarial Networks (GANs) which consist of a discriminator and a generator networks and \nboth are trained simultaneously competing in a minimax game framework \n3.2 CANDIDATE GENERATION \nIf there are 1 million users and 0.3 million items, a data matrix at a user-item level would \nhave 0.3 * 10^12 combinations as an input into a data mining algorithm to solve a classification \nor prediction task. Such a huge data matrix lends itself unviable from a computational and \nimbalance perspective. Hence candidate generation is an important problem in recommender \nsystems. A summary of the methods in literature to solve the problem of candidate generation in \ndeep learning is shown in Figure 3.2 \n \n \n \n \nFig 3.2: Taxonomy of Deep Learning Methods to address Candidate Generation Problem \n \nYouTube used a candidate generation network (Covington et al., 2016) with inputs as events \nfrom user history. The problem of returning hundreds of videos from millions was treated as an \nextreme multi-class problem and is solved using a deep neural network that learnt the user \nembeddings as a function of the user’s history and context. RNNs have been used for candidate \ngeneration for sequential data in Question-Answer Systems and deep keyphrase generation. In \n(Shen, Chen, & Huang, 2016),  the authors turn to paraphrases as a means of capturing the way \nnatural language expresses the information needed and present a general framework using a \ndeep neural model which learns felicitous paraphrases for various  tasks. In (Y. Zhang, Fang, & \nDeep Learning \nMethods for Candidate \nGeneration\nSeparate Candidate \nGeneration Network\nVideo \nRecommendations on \nYouTube\nRecurrent Neural \nNetwork\nQuestion-Answer \nSystems\nDeep Keyphrase \ngeneration\nAutoencoders\nWord Embeddings\nHybrid Methods\nDeep Learning + ML\nk-NN\nAssociation Rules\nMulti-class \nClassification \nAlgorithms\nSimilarity Search\nHeuristic Methods + \nML\nBloom Filter Chains\nThumb Rules\nWeidong, 2018), first a list of phrase candidates with heuristic methods are returned followed \nby scoring each candidate phrase for its likelihood of being a keyphrase in the given document. \n(Xiaonan Li, Li, & Yu, 2012) combined word embeddings with L2 regularized large scale \nlogistic regression to search sentences similar to a query from Wikipedia articles and directly \nuse the human-annotated entities in the similar sentences as candidate entities for the query.  \n Trivial business rule-based techniques and simple machine learning methods have been \nused combined with deep learning extensively in literature to create hybrid systems to solve the \nproblem of candidate generation. In Google Apps, there are over a million apps in the database. \nEvery app cannot be scored for every query. Returning a short list of items that best match the \nquery using various signals is achieved using a combination of machine learned methods and \nhuman defined rules (Cheng et al., 2016). Bloom Filter Chains have also been used for \ncandidate generation for real-time tweet search involving sorting of tweets using a simple \nscoring function and then rescoring them with a separate component (Asadi & Lin, 2013). IBM \nWatson, a commercial tool calls out “Hypothesis Generation” as a key component and the same \nuses candidate generation to retrieve answers to a particular question (Chu-Carroll et al., 2012). \nAssociation Rules, a classic machine learning technique has also been used for candidate \ngeneration as Generalized Sequential Patterns (Srikant & Agrawal, 1996). Traditional methods \nhave also been combined with deep learning techniques to create hybrid systems. \n4 OPPORTUNITIES  \nOn candidate generation, while there have been novel implementations of deep learning, \nthey have been very domain focused like candidate generation network using Deep Neural \nNetwork for YouTube, RNNs for Question-Answer Systems and Deep Keyphrase generation and \nautoencoders for word embeddings. These methods have been shown to solve the candidate \ngeneration problem in a specific domain, but they do not generalize to all scenarios. Also, the \ncandidate generation methods apart from the YouTube example have not combined deep \nlearning with traditional machine learning techniques like k-NN, association rules, bloom filters \nand classification algorithms. These present an opportunity for future research.  \nOn cold start, hand-crafting features has been reduced greatly in the cases of images, audio \nand text where CNNs and RNNs have shown tremendous results. Still, for traditional structured \nrecommendations data, hand-crafting features is required in many cases, which means that \nfeatures need to be maintained and domain experts need to be deployed. Employing User and \nitem features effectively into deep network in an automated way into deep learning methods to \nsolve the cold start problem is very important. Common to both cold start and candidate \ngeneration, there are very few examples of using the different nature of deep learning methods in \na way complementary to one another. Some examples include combining RNNs and \nautoencoders. Deep Neural Networks on text ignore the word order and semantic meanings but \nRecurrent Neural Networks can take the word order into account. Autoencoders can leverage the \nsemantic order. Combining these methods can potentially give better results. The actual \nincremental improvement that can be expected from deep learning methods on candidate \ngeneration remains an important research area. An immediate improvement in alleviating cold \nstart becomes possible where there have been several implementations taking one of the \ntechniques into account. In one of the papers, latent factors learned from Weighted Matrix \nFactorization was not accurate enough to train a CNN, but could have easily adapted to a \ndifferent technique. \n4.1 RESEARCH OPPORTUNITIES \nSome of the research areas that we have identified include: \n• Using Autoencoders to effectively automatically engineer features to solve the cold start \nproblem. The autoencoder approach to feature engineering that converts a user vector to a \ndense vector could bring out a significant improvement in the performance on the \nexperimental datasets, but there is not much literature on it. Autoencoders seem to have \nbeen used heavily for text recommendations but not on structured datasets \n• Factorization machines can avoid tedious feature engineering for cold start problem and \ncould be promising to integrate them with deep learning techniques  \n• Exploring CNNs for structured data. Techniques from image processing to features like \nfilters, pooling, residual networks, one-pass object detections could find parallels in \nstructured data modeling. This is in addition to leveraging CNNs for extracting features \nfrom images data \n• Recommendations are a form of sequential data. There is not enough literature on \nrecurrent neural networks and their types LSTM and GRU to automate some feature \nengineering \n• All algorithms have tried to optimize for overall accuracy metrics like RMSE, RMSLE, \nAUC, F-score, NDCG, MAP etc., which are global measures for the whole dataset. There \ncould be potential in coming up with a custom evaluation metric that gives higher weight \nto cold start cases \n• Coming up with hybrid methods that combine Deep learning with Association rules or k-\nNN for a generalized framework for candidate generation is a promising area \n5 CONCLUSION \nCurrent survey papers on deep learning in recommender systems provide a historical \noverview and taxonomy of recommender systems based on type. We have grouped the \nrecommender system challenges into those related to recommendations, those related to \ncandidate generation and those related to the nature of the data. We have focused on candidate \ngeneration and cold start challenges in this paper. Our paper addresses the gaps of providing a \ntaxonomy of deep learning approaches to address recommender systems problems in the areas of \ncold start and candidate generation. Cold Start problem is due to insufficient information about \nusers or items. Deep Learning techniques have tried to address this by generating features for the \naudio (CNN, RNN), for the images (CNN), for the text (LSTM methods and GRU methods) and \nfor hidden user and item representations (Autoencoders and GANs). The candidate generation \nproblem has been addressed by creating separate candidate generation networks (DNN) and \nrecurrent neural networks. We have summarized the limitations of each of the methods and the \nimportant research opportunities \nOur key contribution in this survey paper is the taxonomy of deep learning methods to \naddress cold start and candidate generation with a summary of the limitations and opportunities \nfor research \n \n \n \n6 REFERENCES \nAsadi, N., & Lin, J. (2013). Fast candidate generation for real-time tweet search with bloom filter chains. \nACM Transactions on Information Systems, 31(3), 1–36. https://doi.org/10.1145/2493175.2493178 \nBalakrishnan, A. (2014). DeepPlaylist : Using Recurrent Neural Networks to Predict Song Similarity. \nStanfort University, 1–7. Retrieved from https://cs224d.stanford.edu/reports/BalakrishnanDixit.pdf \nBansal, T., Belanger, D., & McCallum, A. (2016). Ask the GRU. In Proceedings of the 10th ACM Conference \non Recommender Systems - RecSys ’16 (pp. 107–114). https://doi.org/10.1145/2959100.2959180 \nBernardi, L., Kamps, J., Kiseleva, J., & Mueller, M. J. I. (2015). The continuous cold start problem in e-\ncommerce recommender systems. In CEUR Workshop Proceedings (Vol. 1448, pp. 30–33). \nBetru, B. T., & Onana, C. A. (2017). Deep Learning Methods on Recommender System: A Survey of State-\nof-the-art. International Journal of Computer Applications, 162(10), 975–8887. \nhttps://doi.org/10.5120/ijca2017913361 \nChen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., … Zhang, Z. (2015). MXNet: A Flexible and Efficient \nMachine Learning Library for Heterogeneous Distributed Systems. eprint arXiv:1512.01274. \nhttps://doi.org/10.1145/2532637 \nCheng, H.-T., Ispir, M., Anil, R., Haque, Z., Hong, L., Jain, V., … Chai, W. (2016). Wide &amp; Deep \nLearning for Recommender Systems. In Proceedings of the 1st Workshop on Deep Learning for \nRecommender Systems - DLRS 2016 (pp. 7–10). https://doi.org/10.1145/2988450.2988454 \nChiliguano, P., & Fazekas, G. (2016). Hybrid music recommender using content-based and social \ninformation. In ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - \nProceedings (Vol. 2016–May, pp. 2618–2622). https://doi.org/10.1109/ICASSP.2016.7472151 \nChu-Carroll, J., Fan, J., Boguraev, B. K., Carmel, D., Sheinwald, D., & Welty, C. (2012). Finding needles in \nthe haystack: Search and candidate generation. IBM Journal of Research and Development, 56(3.4), \n6:1-6:12. https://doi.org/10.1147/JRD.2012.2186682 \nCovington, P., Adams, J., & Sargin, E. (2016). Deep Neural Networks for YouTube Recommendations. In \nProceedings of the 10th ACM Conference on Recommender Systems - RecSys ’16 (pp. 191–198). \nhttps://doi.org/10.1145/2959100.2959190 \nDauphin, Y. N., de Vries, H., & Bengio, Y. (2015). RMSProp and Equilibrated adaptive learning rates for \nnon-convex optimization. NIPS ’15, 10. Retrieved from http://arxiv.org/abs/1502.04390 \nDavidson, J., Livingston, B., Sampath, D., Liebald, B., Liu, J., Nandy, P., … Lambert, M. (2010). The \nYouTube video recommendation system. In Proceedings of the fourth ACM conference on \nRecommender systems - RecSys ’10 (p. 293). https://doi.org/10.1145/1864708.1864770 \nDevooght, R., & Bersini, H. (2016). Collaborative Filtering with Recurrent Neural Networks. CoRR, \nabs/1608.0. Retrieved from http://arxiv.org/abs/1608.07400 \nGomez-Uribe, C. A., & Hunt, N. (2015). The Netflix Recommender System: Algorithms, Business Value, \nand Innovation. ACM Trans. Manage. Inf. Syst., 6(4), 13:1--13:19. https://doi.org/10.1145/2843948 \nGoogle. (2018). TensorFlow. \nHandbook, R. S., RICCI, F., & ROKACH, L. (2011). Recommender Systems Handbook. Recommender \nSystems Handbook, 54, 39–72. https://doi.org/10.1007/978-0-387-85820-3 \nKetkar, N. (2017). Introduction to PyTorch. In Deep Learning with Python (pp. 195–208). \nhttps://doi.org/10.1007/978-1-4842-2766-4_12 \nKingma, D. P., & Ba, J. L. (2015). Adam: a Method for Stochastic Optimization. International Conference \non Learning Representations 2015, 1–15. \nhttps://doi.org/http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503 \nKo, Y.-J., Maystre, L., Grossglauser, M., Durrant, R. J., & Kim, K.-E. (2016). Collaborative Recurrent Neural \nNetworks for Dynamic Recommender Systems. JMLR, 63, 366–381. \nhttps://doi.org/10.1109/ICDE.2016.7498326 \nLam, X., Vu, T., & Le, T. (2008). Addressing cold-start problem in recommendation systems. Proceedings \nof the 2nd International, 208–211. https://doi.org/10.1145/1352793.1352837 \nLee, D., Huntsman, J. M., Fl, H., Huntsman, J. M., & Fl, H. (2014). Impact of Recommender Systems on \nSales Volume and Diversity. Icis, 1–15. \nLi, X., Li, C., & Yu, C. (2012). Entity-Relationship Queries over Wikipedia. ACM Transactions on Intelligent \nSystems and Technology, 3(4), 1–20. https://doi.org/10.1145/2337542.2337555 \nLi, X., & She, J. (2017). Collaborative Variational Autoencoder for Recommender Systems. In Proceedings \nof the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining  - KDD \n’17 (pp. 305–314). https://doi.org/10.1145/3097983.3098077 \nLiang, D. (2014). Content-Aware Collaborative Music Recommendation Using Pre-trained Neural \nNetworks. ISMIR 2015: Proceedings of the 16th International Society for Music Information \nRetrieval Conference, 295–301. \nLika, B., Kolomvatsos, K., & Hadjiefthymiades, S. (2014). Facing the cold start problem in recommender \nsystems. Expert Systems with Applications, 41(4 PART 2), 2065–2073. \nhttps://doi.org/10.1016/j.eswa.2013.09.005 \nOord, A. van den, Dieleman, S., & Schrauwen, B. (2013). Deep content-based music recommendation. \nElectronics and Information Systems Department (ELIS), 9. \nhttps://doi.org/10.1109/MMUL.2011.34.van \nPortugal, I., Alencar, P., & Cowan, D. (2017). The Use of Machine Learning Algorithms in Recommender \nSystems: A Systematic Review. Expert Systems with Applications. \nhttps://doi.org/10.1016/j.eswa.2017.12.020 \nRumelhart, D. E., Hinton, G. E., & William, R. J. (1985). Learning internal representation by back-\npropagation errors. In Nature (Vol. 323, pp. 533–536). \nSchmidhuber, J. (2015). Deep Learning in neural networks: An overview. Neural Networks. \nhttps://doi.org/10.1016/j.neunet.2014.09.003 \nShen, Y., Chen, J., & Huang, X. (2016). Bidirectional long short-term memory with gated relevance \nnetwork for paraphrase identification. In Lecture Notes in Computer Science (including subseries \nLecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) (Vol. 10102, pp. 39–50). \nhttps://doi.org/10.1007/978-3-319-50496-4_4 \nSinghal, A., Sinha, P., & Pant, R. (2017). Use of Deep Learning in Modern Recommendation System: A \nSummary of Recent Works. International Journal of Computer Applications, 180(7), 17–22. \nhttps://doi.org/10.5120/ijca2017916055 \nSrikant, R., & Agrawal, R. (1996). Mining sequential patterns: Generalizations and performance \nimprovements. Advances in Database Technology — EDBT ’96, 1057, 1–17. \nhttps://doi.org/10.1109/ICDE.1995.380415 \nTan, J., Wan, X., & Xiao, J. (2016). A Neural Network Approach to Quote Recommendation in Writings. \nProceedings of the 25th ACM International on Conference on Information and Knowledge \nManagement  - CIKM ’16, 65–74. https://doi.org/10.1145/2983323.2983788 \nZhang, S., & Yao, L. (2017). Deep Learning based Recommender System: A Survey and New Perspectives. \nACM J. Comput. Cult. Herit. Article, 1(35), 1–36. https://doi.org/10.1145/nnnnnnn.nnnnnnn \nZhang, Y., Fang, Y., & Weidong, X. (2018). Deep keyphrase generation with a convolutional sequence to \nsequence model. In 2017 4th International Conference on Systems and Informatics, ICSAI 2017 (Vol. \n2018–Janua, pp. 1477–1485). https://doi.org/10.1109/ICSAI.2017.8248519 \n \n \n \n",
  "categories": [
    "cs.IR",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-07-17",
  "updated": "2019-07-17"
}