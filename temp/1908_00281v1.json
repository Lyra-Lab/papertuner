{
  "id": "http://arxiv.org/abs/1908.00281v1",
  "title": "Featuring the topology with the unsupervised machine learning",
  "authors": [
    "Kenji Fukushima",
    "Shotaro Shiba Funai",
    "Hideaki Iida"
  ],
  "abstract": "Images of line drawings are generally composed of primitive elements. One of\nthe most fundamental elements to characterize images is the topology; line\nsegments belong to a category different from closed circles, and closed circles\nwith different winding degrees are nonequivalent. We investigate images with\nnontrivial winding using the unsupervised machine learning. We build an\nautoencoder model with a combination of convolutional and fully connected\nneural networks. We confirm that compressed data filtered from the trained\nmodel retain more than 90% of correct information on the topology, evidencing\nthat image clustering from the unsupervised learning features the topology.",
  "text": "Featuring the topology with the unsupervised machine learning\nKenji Fukushima,1, 2, ∗Shotaro Shiba Funai,3, † and Hideaki Iida1, ‡\n1Department of Physics, The University of Tokyo,\n7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan\n2Institute for Physics of Intelligence (IPI), The University of Tokyo,\n7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan\n3Physics and Biology Unit, Okinawa Institute of Science and Technology (OIST),\n1919-1 Tancha Onna-son, Kunigami-gun, Okinawa 904-0495, Japan\nImages of line drawings are generally composed of primitive elements. One of the\nmost fundamental elements to characterize images is the topology; line segments\nbelong to a category diﬀerent from closed circles, and closed circles with diﬀerent\nwinding degrees are nonequivalent. We investigate images with nontrivial winding\nusing the unsupervised machine learning. We build an autoencoder model with a\ncombination of convolutional and fully connected neural networks. We conﬁrm that\ncompressed data ﬁltered from the trained model retain more than 90% of correct\ninformation on the topology, evidencing that image clustering from the unsupervised\nlearning features the topology.\n∗fuku@nt.phys.s.u-tokyo.ac.jp\n† shotaro.funai@oist.jp\n‡ iida@nt.phys.s.u-tokyo.ac.jp\narXiv:1908.00281v1  [cs.LG]  1 Aug 2019\n2\nI.\nINTRODUCTION\nBrains ingeniously function with networks of neurons.\nFor understanding of intrinsic\nbrain dynamics, physicists would favorably decompose such an integral system into irre-\nducible elements, so that we can analyze relatively simpler function of each building element\nthat takes rather primitive actions. Then, numerical simulations on computer are handy\ndevices to test if a postulated mechanism of brains should go as expected. Such modeling\nembodies non-equilibrium processes of brains, which is an approach acknowledged commonly\nas computational neuroscience. Besides, a hypothesis called quantum brain dynamics im-\nplements quantum ﬂuctuations and Nambu-Goldstone bosons for brain sciences [1, 2] (for\ndiscussions for/against quantum phenomena in brain dynamics, see Ref. [3]), which bridges\na devide between computational neuroscience and modern physics.\nIn contrast to such “oﬀ-equilibrium” problems, in the language of physics, perception and\nrecognition are “static” problems. For the latter problems, model-independent research tools\nare available for computer simulations. That is, the machine learning enables us to emulate\nthe neural structure of brains on computer. One of intriguing attributes of the machine\nlearning, particularly with deep neural networks (NNs), is that any nonlinear mapping can\nbe represented by data transmission through multiple hidden layers [4].\nThese days we have witnessed tremendous progresses in the ﬁeld of image recognition and\nclassiﬁcation by means of the machine learning. In particular the progress has been driven\nby Convolutional Neural Network (CNN) [5], which was originally proposed as a multi-layer\nneural network imitating animal’s visual cortex [6]. The CNN has become the most common\napproach for high-level image recognition since an overwhelming victory of AlexNet [7] at\n“ImageNet Large Scale Visual Recognition Challenge 2012.” Challenges of minimizing inter-\nclass variability, reducing error rate, achieving large-scale image recognition, etc are ongoing\nimprovements and they are all crucial steps for practical usages.\nPhysicswise, the image handling with the deep learning has proved its strength in identi-\nfying phase transitions. Some successful attempts are found in Ref. [8] for two-dimensional\nsystems analyzed by the supervised learning, Ref. [9] for its extension to three-dimensional\nsystems, and Ref. [10] for statistical systems studied by the unsupervised learning. Here,\nwe point out an essential diﬀerence between the supervised and unsupervised learning; the\nformer is useful for regression and grouping problems, while the latter eﬃciently makes fea-\n3\nture extraction and clustering of data. Interestingly, similarity between the unsupervised\nlearning and the renormalization group in physics has also been investigated, see Ref. [11].\nIn the context of image recognition, at the same time, a distinct direction toward more\nfundamental research would be as important to demystify blackboxed artiﬁcial intelligence,\nwhich may be somehow beneﬁcial for so-called explainable artiﬁcial intelligence [12, 13].\nThe fundamental question of our interest in the present work is what would be the simplest\nelement of images that categorizes those images into representative clusters. For the sake of\nimage clustering, a useful mathematical notion, which underlies modern physics, has been\ndeveloped known as the “topology” theorized into the form of homotopy. The most well-\nknown example is that a mug with one handle and a torus-shaped donut belong to the same\ngrouping class; the shape can be smoothly deformed from one to the other, and they are of\nthe same homotopy type. In this work we report leading-edge results from our simulations\nwith the CNN supporting an idea that the topology is critical information for image feature\nextraction and clustering.\nII.\nTOPOLOGY AND THE WINDING NUMBER\nThe topology is classiﬁed by the homotopy group in mathematics. The simplest example\nis what is called the fundamental homotopy group denoted as π1(S1) = Z associated with a\nmapping from S1 (i.e., one dimensional unit sphere) to another S1 and an integer nW ∈Z\ncorresponds to the winding number. To demonstrate the idea concretely, let us consider the\nfollowing function on S1 of U(1),\nφ(x) = eiθ(x) = cos θ(x) + i sin θ(x) .\n(1)\nIf x is a coordinate on a circle with period L, the above function represents a mapping\nfrom S1 in coordinate space to S1 on Gauss’ plane with Euler’s angle θ (which is also called\nthe “lift” in homotopy theory). While x travels around from 0 to L under a condition,\nφ(0) = φ(L), Euler’s angle θ should return to the original position modulo 2π. The winding\nnumber associated with the above function (or the “degree” of this function) reads,\nnW = θ(L) −θ(0)\n2π\n= ln[φ(L)/φ(0)]\n2πi\n=\n1\n2πi\nZ L\n0\ndx φ−1(x)dφ(x)\ndx\n.\n(2)\nFigure 1 schematically illustrates one winding conﬁguration of φ(x) having nW = 1.\n4\n-1.0\n-0.5\n0.5\n1.0\nRe\nIm\nx\nx\nx\nRe\nIm\n-1.0\n-0.5\n0.5\n1.0\nFIG. 1. Schematic illustration of π1(S1) realized by φ(x) = eiθ(x). In the left representation the\nbehavior of θ(x) is manifest and nW = 1 is easily concluded, but in our simulation, only (Re φ, Im φ)\nas shown in the right is given to the NN model.\nFor us, human-beings, it would be an elementary-class exercise to discover a counting\nrule of nW. If we were given many diﬀerent images with correct answers of nW, it would be\njust a matter of time for us to eventually ﬁnd a right counting rule out. This description\nis nothing but the machinery of the supervised learning. Interestingly, it has been reported\nthat the deep neural network trained by the supervised learning correctly reproduced nW\nof π1(S1) [14]. There, the machine discovered a nice ﬁt of the formula (2) only from the\ninformation of given (Re φ, Im φ) and nW, but not referring to θ(x) directly. In the present\nwork, we are taking one step forward; we would like to see the NN model not only optimizing\na ﬁt from the supervised learning but featuring the topology from the unsupervised learning.\nMore speciﬁcally, we would like to think of classiﬁcation of many images without giving\nthe answers of nW. It would be very intriguing to ask such a question of how the CNN\nmakes clustering of diﬀerently winding data, which would be a prototype model of how our\nbrains categorize images based on the topological characterization. Thus, the unsupervised\nlearning as adopted in this work should tell us surpassing information than the supervised\nlearning for the purpose to dissect the topological contents.\nIII.\nRESULTS AND DISCUSSIONS\nIn the numerical procedure we represent φ(x) by a sequence of numbers on discretized\nx with L = 128 grids, i.e., we generate 2 × 128 = 256 sized data of (Re φi, Im φi) with\n5\nx\nL1\nL2\nL3\nL4\nL5\n2⇡\n4⇡\n6⇡\n+\n+\n+\n+\n−\nx\n2⇡\n4⇡\n6⇡\n✓\n+\n+\n+\n+\n−\nnW = +3\n`1\n`2\n`3\n`4\n`5\n0\nFIG. 2. One example of generated data for (+, +, +, −, +) with nW = +3.\ni = 1, . . . , L under the periodic boundary condition; (Re φL, Im φL) = (Re φ1, Im φ1). These\n256 numbers consist of the input data onto the CNN side.\nWe prepare the training and test data randomly. We will give more detailed explanations\non the numerical procedure in Method. Each data consists of distinct Ns segments along x\nwith either positive or negative winding, where the segment lengths, ℓm, are chosen randomly,\nwhere\nNs\nX\nm=1\nℓm = L should be kept. For the data used in this work, we take account of\nNs = 0, 1, 2, 3, 4, and 5. We then assign positive and negative winding randomly to each\nsegment, which is symbolically labeled as (p1, p2, . . . , pNs) with pm = ±1, where + (and −)\nstands for positive (and negative, respectively) winding. For example, (+, +, +, −, +) for\nNs = 5 is a conﬁguration with net winding, nW =\nNs\nX\nm=1\npm = +3. In m-th segment we ﬁrst\npostulate that θ(x) linearly changes its value by 2πpm. Then, zigzag lines are distorted\nwith random noises to enhance the learning quality and thus the adaptivity.\nFigure 2\ndepicts an example of generated data with a choice of (+, +, +, −, +). With Ns up to 5\n20 + 21 + 22 + 23 + 24 + 25 = 63 winding patterns are possible, and nW can take a value\nfrom −5 to +5. This setup is for the moment suﬃciently general for our goal to check\nperformance of the topology detection.\nThe unsupervised learning utilizes the autoencoder [15]; it ﬁrst encodes the data com-\npressed into smaller number of neurons in the CNN (in our case, 16 sites × 4 ﬁlters from\noriginal 256 sites) and then decodes the compressed data with fully connected NN into the\noriginal size. We repeat such encoding and decoding processes to minimize the loss function\n6\n1st filter\n0\n2\n4\n6\n8\n10\n12\n14\n0.0\n0.2\n0.4\n0.6\nneuron\nvalue\n# of segments Ns = 5\n1st filter\n0\n2\n4\n6\n8\n10\n12\n14\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nneuron\nvalue\nwinding number nW = +3 (Ns = 5)\n++++-\n+++-+\n++-++\n+-+++\n-++++\nFIG. 3. Examples of feature maps on the deepest CNN layer. (Left) Ns = 5 and all possible (i.e.,\n25 = 32) winding patterns averaged over 1,000 test data. (Right) Ns = 5 with nW = +3 ﬁxed and\nﬁve winding patterns averaged over 1,000 test data.\nmeasured by the squared diﬀerence between the original input data and the coarse-grained\noutput data. The learning process optimizes the ﬁlters in the CNN encoder and simultane-\nously the weights in the NN decoder.\nWe shall see the results from our NN model that has been optimized by the unsupervised\nlearning with the training dataset of 63 winding patterns times 1,000 randomly generated\ndata (i.e., 63,000 data in total). We input the test data into the optimized NN model and\nobserve feature maps of 16 neurons convoluted with 4 ﬁlters on the deepest CNN layer.\nFigure 3 summarizes sampled feature maps from the 1st ﬁlter. The left plot is for Ns = 5\nwith all possible winding patterns averaged over 1,000 test data. The right plot particularly\npicks up the averaged feature maps for ﬁve diﬀerent windings with Ns = 5 and nW = +3.\nAt a glance one may think that the behavior looks like coarse-grained Re φ(x) or Im φ(x)\nwith segment lengths normalized.\nNow, the most fascinated question is whether the compressed data on 16 sites convoluted\nwith 4 ﬁlters could retain information on the topology or not, and if yes, how we can retrieve\nit. From the left of Fig. 3 it is obvious that the peak heights reﬂect diﬀerent winding patterns.\nIn fact, the averaged feature maps exhibit a clear hierarchy of four separated heights with\none-to-one correspondance to the winding sequence; that is, the peak heights increase with\nsequential windings as\n(+, +)\n<\n(+, −)\n<\n(−, +)\n<\n(−, −) ,\n(3)\nand the height at the far right end is determined solely by ±, which is due to zero-padding\n7\n1st filter\n0\n2\n4\n6\n8\n10\n12\n14\n0.0\n0.2\n0.4\n0.6\n0.8\nneuron\nvalue\n2nd filter\n0\n2\n4\n6\n8\n10\n12\n14\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nneuron\nvalue\n3rd filter\n0\n2\n4\n6\n8\n10\n12\n14\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nneuron\nvalue\n4th filter\n0\n2\n4\n6\n8\n10\n12\n14\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nneuron\nvalue\nFIG. 4. Feature maps from the 1st ﬁlter (top-left), the 2nd ﬁlter (top-right), the 3rd ﬁlter (bottom-\nleft), and the 4th ﬁlter (bottom-right) for (+, +, +, −, +) data without taking the average. Four\nrandomly selected data are shown by four diﬀerent colors.\nin the convolution to keep the data size.\nFor example, if we see the far left peak around the 2nd neuron in the right of Fig. 3,\n(−, +, +, +, +) has the highest peak, (+, −, +, +, +) the second highest, and others are\ndegenerated in accord with Eq. (3).\nAlso, we notice in the right of Fig. 3 that, for\n(+, +, +, +, −), three consecutive short peaks appear from the left, one middle peak follows,\nand then one tall peak sits at the right end.\nThe short peaks correspond to (+, +) of\n((+, +), +, +, −), (+, (+, +), +, −), and (+, +, (+, +), −). The middle peak corresponds to\n(+, +, +, (+, −)) and the tall peak is sensitive only to −of (+, +, +, +, −).\nOne might have thought that such a clear hierarchy as in Fig. 3 is visible only after\ntaking the average. This is indeed the case, as exempliﬁed in Fig. 4; the feature maps for\none test data do not always show prominent peaks with well separated heights. Neverthe-\nless, surprisingly, we can see that such ﬂuctuating coarse-grained data in Fig. 4 still retain\ninformation on the topology!\nIt is impossible for our eyes to recognize any topological contents from Fig. 4, so we will\n8\n0% \n20% \n40% \n60% \n80% \n100% \n1 filter \n2 filters \n3 filters \n4 filters \nCorrect Answer Rate \n1st \n2nd \n3rd \n4th \n5th \nFIG. 5. Correct answer rate for nW guessed from the feature maps. For each of 63,000 test data we\nhave checked where the correct winding number ranks (1st, 2nd, ... in the legend) in the probability\noutput from the supervised NN model.\nask for a help of another machine learning device. For each of 63,000 training data, we\nhave such feature maps like Fig. 4 and also the corresponding nW. We can then perform\nthe supervised learning to train a fully connected NN (with one hidden layer) such that the\noutput gives the probability distribution of guessed nW (out of −5, . . . , 5) in response to the\ninput of feature maps. Figure 5 is the correct answer rate. If we input the feature maps\nfrom only 1 ﬁlter, the most-likely nW hits the correct value at the rate of 50%, and the\nsecond-likely one at the rate of 31%. If we use the feature maps from 2 ﬁlters, the available\ninformation is doubled, and the correct answer rate of the most-likely nW becomes 78%.\nAmazingly, for the feature maps from 3 ﬁlters, it increases up to 90%! There is almost no\ndiﬀerence between the 3-ﬁlter and the 4-ﬁlter results, and it seems that the correct answer\nrate is saturated at 90%.\nTo summarize, from these results and analyses, we can conclude that the coarse-grained\ndata of the feature maps through the unsupervised learning do retain information on the\ntopology. In other words, the unsupervised learning with autoencoder can provide us with\na nontrivial machinery to compress data without losing the topological contents.\n9\nIV.\nDISCUSSION\nWe demonstrated that the unsupervised machine learning of images makes feature extrac-\ntion without losing information on the topology. This evidences that the winding number\ncorresponding to the fundamental group, π1(S1), should be one of the essential indices that\ncharacterize image clustering. We trained an autoencoder with the CNN and the fully con-\nnected NN for the unsupervised learning using randomly generated data of functions from\nx on S1 to a U(1) element. We found that the averaged feature maps on the deepest CNN\nlayer show a clear hierarchy pattern of the conﬁgurations with one-to-one correspondence\nto the winding sequences. With help of the supervised learning technique we also revealed\nthat feature maps for each image data look coarse-grained images and such compressed data\nretain information on the topology correctly.\nThe extension of the present work, i.e., the unsupervised learning of higher-dimensional\nimages with nontrivial winding would be quite interesting. We note that the supervised\nmachine learning has been utilized for π2(S2) [14], but as we illustrated in this work, the\nunsupervised machine learning would be more interesting. Implications from this extension\ninclude intriguing applications in quantum ﬁeld theories. In fact, some classical solutions of\nthe equations of motion in quantum ﬁeld theory are topologically stabilized. In such cases\nthe ﬁeld conﬁgurations are classiﬁed according to the winding number. For representative\nexamples, π2(SU(2)/U(1)) = Z for monopoles, π3(SU(2)) = Z for Skyrmions (with no time-\ndependence), and π3(SU(2)) = Z for instantons (with the ﬁelds at inﬁnity identiﬁed) in\npure Yang-Mills theory [16, 17]. Actually, it is a long-standing problem how to visualize\nthe topological contents in quantum ﬁeld theories. The numerical lattice simulation is a\npowerful tool to solve quantum ﬁeld theories, and ﬁeld conﬁgurations should in principle\ncontain information on the topology. Some algorithms to extract topologically nontrivial\nconﬁgurations such as monopoles, Skyrmions, and instantons have been proposed [18–20].\nThe most well-known approach, i.e., the cooling method has a serious ﬂaw, however. If the\ncooling is applied too many times, the topology is lost and the ﬁeld conﬁguration would\nbecome trivially ﬂat. Therefore, the cooling procedures should be stopped at some point,\nand this artiﬁcial termination of the procedures causes uncertainties. Alternatively, we would\nemphasize that the compression of ﬁeld conﬁguration images by means of the unsupervised\nmachine learning is a promising candidate for the superior smearing algorithm not to lose\n10\nthe topological contents. We are testing this idea in simple two-dimensional lattice model,\nnamely, CP N−1 model that has π2(S2) instantons.\nMathematically, it would be also a very interesting question to consider not only the\nhomotopy groups but also the homology Hn(X) of a topological space X deﬁned by a coset\nof cycles in n dimensions over boundaries of (n + 1)-dimensional elements. For example,\nH0(X) counts the number of connected drawings of images, and H1(X) counts the number\nof loops of images, etc. This direction of research is now ongoing.\nACKNOWLEDGMENTS\nWe thank Yuya Abe and Yuki Fujimoto for discussions. K. F. was supported by Japan\nSociety for the Promotion of Science (JSPS) KAKENHI Grant No. 18H01211.\nMETHOD\nInput data of winding patterns\nInput data φi = eiθi (i = 1, . . . , L), including training data and test data, are generated\nin the following way. Note that since the data φi are complex numbers, we actually input\nthe combinations of their real and imaginary parts (Re φi, Im φi).\nFirst we impose the periodic boundary condition:\nRe φ1 = Re φL,\nIm φ1 = Im φL.\n(4)\nThen we divide L sites into Ns segments, and length of each segment ℓm (m = 1, . . . , Ns) is\nrandomly chosen as\nℓm = L −1\nNs\n[1 + 0.4(ξm −ξm−1)]\n(5)\nwhere ξm is a random number from a uniform distribution in the open interval (−1, 1). This\nmeans the lengthes in all the segments satisfy 0.2L−1\nNs < ℓm < 1.8L−1\nNs . We set ξ0 = ξNs = 0\nso that the total length L is kept. To be exact, the length ℓm should be an integer, so the\nright hand side is rounded to an integer.\nIn each segment m, the angle θi is composed of a linear part from 0 to ±2π and a random\nnoise:\nθi\n2π = pm\ni −i0\nℓm\n+ 0.1ζi\n(6)\n11\n128x1\n2ch\nConv1\n8x1x2ch\n+bias\n2 filters\nReLU\nPooling\n2x1\n64x1\n2ch\nPooling\n4x1\nInput data\n…\n…\nConv2\n8x1x2ch\n+bias\n4 filters\nReLU\n……\n……\n……\n……\n…\n1\n2\n3\n127\n128\n1\n2\n8\n4 filters\n…\n1\n2\n64\n…\n16x1x4\n128\n…\nOutput data\n…\n1\n16\n4\n……\n…\n1\n2\n3\n127\n128\n…\nFully\nconn.\nReLU&\ndropout\n……\n……\n1\n2\n3\n127\n128\n128x2\nFIG. 6. Schematic ﬁgure of the autoencoder used for our unsupervised learning.\nwhere pm = ±1 is the winding direction and i0 = 1 + Pm−1\nm′=1 ℓm′. The random noise ζi is\nfrom a Gaussian distribution with mean 0 and variance 1.\nIn our experiment, we set L = 128 and Ns = 0, 1, . . . , 5. Then all the combinations of\nwinding directions pm have P5\nNs=0 2Ns = 63 patterns. For each winding pattern, we generate\n1,000 + 1,000 input data with the parameters (ξm, ζi) chosen randomly. The ﬁrst 1,000 data\n(in total 63,000 data) is the training data, which is used for training our autoencoder and\nsupervised NN. The other 1,000 data is the test data for analyzing the feature maps in the\nautoencoder (see Figs. 3 and 4) and the output from the supervised NN (see Fig. 5).\nAutoencoder\nThe autoencoder for our unsupervised learning consists of the CNN (encoder) and the\nfully-connected NN (decoder). A schematic ﬁgure of our autoencoder is shown in Fig. 6.\nWe made a training code using TensorFlow [21].\nThe CNN encoder has two layers. In the both layers, we use the convolution with 8×1(×2\nchannel) sized ﬁlters and stride 1. We also use the zero padding to keep the size of data,\nand the ReLU as an activation function. In the convolution part, diﬀerence between the\nﬁrst and second layers is only the number of ﬁlters. After the convolution, our encoder has\nthe pooling part in each layer. We use the max pooling with 2 × 1 sized window and stride\n12\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\nLearning epoch\nLoss function\nTraining of autoencoder\nTraining data\nTest data\nFIG. 7. Loss function during training of our autoencoder.\n2 for each channel in the ﬁrst layer. This pooling compresses the data size into 1/stride of\nthe original size. The second layer has 4 × 1 sized window and stride 4, then as a result,\nthe CNN encoder outputs the feature map with L/2/4 = 16 sites (for each ﬁlter out of 4\nﬁlters), as we saw in Fig. 3.\nThe fully-connected NN decoder has two layers, too. In the ﬁrst layer, we use the dropout\nmethod with probability 0.5 to avoid overlearning, then use the ReLU again. The ﬁnal layer\nhas no dropout and no activation function, then outputs coarse-grained data (ϕi,1, ϕi,2) with\nthe same size as the input data (Re φi, Im φi).\nAs the loss function for the training, we choose the squared diﬀerence between input data\nφi and output data ϕi,j, that is,\n1\n2L\nL\nX\ni=1\n\u0002\n(Re φi −ϕi,1)2 + (Im φi −ϕi,2)2\u0003\n.\n(7)\nWe prepared the training and test data, both of which contain 63 winding patterns times\n1000 randomly generated data. Then we found the unsupervised learning with the learning\nrate 10−7 and the mini-batch size 10 decreases the loss function of the test data to its\nminimum around 6000 epochs, as shown in Fig. 7.\n[1] L. M. Ricciardi and H. Umezawa, Kybernetik 4, 44 (1967).\n13\n[2] M. Jibu and K. Yasue, Quantum Brain Dynamics and Consciousness: An Introduction, Ad-\nvances in consciousness research (J. Benjamins Publishing Company, 1995).\n[3] P. Jedlicka, Frontiers in Molecular Neuroscience 10, 366 (2017).\n[4] Y. LeCun, Y. Bengio, and G. Hinton, Nature 521, 436 (2015).\n[5] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard,\nand L. D.\nJackel, Neural Computation 1, 541 (1989).\n[6] K. Fukushima, Biological Cybernetics 36, 193 (1980).\n[7] A. Krizhevsky, I. Sutskever, and G. E. Hinton, in Advances in neural information processing\nsystems (2012) pp. 1097–1105.\n[8] T. Ohtsuki and T. Ohtsuki, Journal of the Physical Society of Japan 85, 123706 (2016).\n[9] T. Ohtsuki and T. Ohtsuki, Journal of the Physical Society of Japan 86, 044708 (2017).\n[10] S. S. Funai and D. Giataganas, (2018), arXiv:1810.08179 [cond-mat.stat-mech].\n[11] S. Iso, S. Shiba, and S. Yokoo, Phys. Rev. E97, 053304 (2018), arXiv:1801.07172 [hep-th].\n[12] W. Samek, T. Wiegand, and K. M¨uller, CoRR abs/1708.08296 (2017), arXiv:1708.08296.\n[13] F. K. Doˇsilovi´c, M. Brˇci´c, and N. Hlupi´c, in 2018 41st International Convention on Informa-\ntion and Communication Technology, Electronics and Microelectronics (MIPRO) (2018) pp.\n0210–0215.\n[14] P. Zhang, H. Shen, and H. Zhai, Phys. Rev. Lett. 120, 066401 (2018).\n[15] G. E. Hinton and R. R. Salakhutdinov, Science 313, 504 (2006).\n[16] A. A. Belavin, A. M. Polyakov, A. S. Schwartz,\nand Yu. S. Tyupkin, Phys. Lett. B59, 85\n(1975).\n[17] G. ’t Hooft, Phys. Rev. D14, 3432 (1976).\n[18] B. Berg, Phys. Lett. 104B, 475 (1981).\n[19] M. Teper, Phys. Lett. B171, 86 (1986).\n[20] E.-M. Ilgenfritz, M. M¨uller-Preuβker, G. Schierholz, H. Schiller, et al., Proceedings, 23RD\nInternational Conference on High Energy Physics, JULY 16-23, 1986, Berkeley, CA, Nucl.\nPhys. B268, 693 (1986).\n[21] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving,\nM. Isard, M. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner, P. Tucker,\nV. Vasudevan, P. Warden, M. Wicke, Y. Yu, and X. Zheng, in 12th USENIX Symposium on\nOperating Systems Design and Implementation (OSDI 16) (USENIX Association, Savannah,\n14\nGA, 2016) pp. 265–283.\n",
  "categories": [
    "cs.LG",
    "hep-th"
  ],
  "published": "2019-08-01",
  "updated": "2019-08-01"
}