{
  "id": "http://arxiv.org/abs/1908.00588v1",
  "title": "Visualizing RNN States with Predictive Semantic Encodings",
  "authors": [
    "Lindsey Sawatzky",
    "Steven Bergner",
    "Fred Popowich"
  ],
  "abstract": "Recurrent Neural Networks are an effective and prevalent tool used to model\nsequential data such as natural language text. However, their deep nature and\nmassive number of parameters pose a challenge for those intending to study\nprecisely how they work. We present a visual technique that gives a high level\nintuition behind the semantics of the hidden states within Recurrent Neural\nNetworks. This semantic encoding allows for hidden states to be compared\nthroughout the model independent of their internal details. The proposed\ntechnique is displayed in a proof of concept visualization tool which is\ndemonstrated to visualize the natural language processing task of language\nmodelling.",
  "text": "Visualizing RNN States with Predictive Semantic Encodings\nLindsey Sawatzky*\nSteven Bergner†\nFred Popowich‡\nSchool of Computing Science\nSimon Fraser University\nFigure 1: A subset of hidden state representations for the 7th and 8th timestep of the input sequence “ we stand in solidarity ,\n” she emphasized ., discussed in detail in Section 5.1. The semantics of hidden states are represented as their potential outputs,\nseen as colour rectangles and mini-bar charts.\nABSTRACT\nRecurrent Neural Networks are an effective and prevalent tool used\nto model sequential data such as natural language text. However,\ntheir deep nature and massive number of parameters pose a chal-\nlenge for those intending to study precisely how they work. We\npresent a visual technique that gives a high level intuition behind the\nsemantics of the hidden states within Recurrent Neural Networks.\nThis semantic encoding allows for hidden states to be compared\nthroughout the model independent of their internal details. The pro-\nposed technique is displayed in a proof of concept visualization tool\nwhich is demonstrated to visualize the natural language processing\ntask of language modelling.\nIndex Terms: Human-centered computing—Visualization—Visu-\nalization techniques\n1\nINTRODUCTION\nRecurrent Neural Networks (RNN) are a machine learning technique\nwhich specializes in modelling sequential data due to their ability\nto capture and retain long term information. Recent years have\nseen great success in applying these models to sequential data, such\nas language modelling [11], sentiment analysis [19], and machine\ntranslation [18]. However, despite their efﬁcacy, the actual features\nand patterns which RNNs learn to model can be opaque, difﬁcult to\n*e-mail: lsawatzk@sfu.ca\n†e-mail: steven bergner@sfu.ca\n‡e-mail: popowich@sfu.ca\nunderstand, and may even include unintended biases [2]. This is a\nresult of the black box nature of these systems which learn to model\ntraining data with little guidance about how to actually do so.\nVisualizations are a powerful tool that can be employed to expose\nthe inner details of these opaque models, taking a step towards better\nunderstanding and trusting their results [14]. However, understand-\ning and visualizing RNNs comes with its own set of challenges. One\nsuch challenge is that of interpreting the semantics of hidden states,\nvectors which encode information within the RNN. These hidden\nstates represent information in complex and highly non-linear ways,\nmaking them particularly difﬁcult to interpret.\nWe aim to visualize some of the intuitions behind the internal\nrepresentations RNNs learn to model. We design a novel visual\nmetaphor to address some of the challenges of visualizing RNNs,\nand in particular their hidden states. This is Predictive Semantic En-\ncodings (PSE) which are an interpretation of hidden state semantics\nthat can be easily visualized and facilitate high level comparisons.\nWe develop a proof of concept interactive tool that uses this tech-\nnique to visualize an overview of the Recurrent Neural Network.\n2\nRELATED WORK\nPrevious work exists in the ﬁeld of visualizing RNNs, and in par-\nticular in terms of interpreting their hidden states. One successful\napproach to this problem has been to interpret patterns of the hidden\nstate activations with respect to the task objectives. For example,\nKarpathy et al. [6] show that some activations within the RNNs\ntrack long term syntactic language features, while Li et al. [8] use\nheat maps to understand the salience of activations and words with\nrespect to the model output. A similar heat map technique is used to\nshow the “attention” a model pays to its sequential inputs at different\ntimesteps across the recurrence relation [1,7].\nAlthough these techniques mainly focus on static visualizations,\narXiv:1908.00588v1  [cs.CL]  1 Aug 2019\nthese and other works have been extended to incorporate interactive\nand complex visualizations. LSTMVis [17] follows the approach of\nvisualizing hidden state activations across time, allowing users to\nform and test hypothesis about the sequences evoking these patterns.\nRather than focusing on activation patterns across time, Ming et\nal. [12] relate hidden states to the model inputs and outputs through\nco-clustering and quantifying the model’s expected response to these\nhidden states. Kahng et al. [5] show and compare speciﬁc subsets of\nactivations based off similar test instances.\nMore recently, Strobelt et al. [16] incorporate interpretation of\nhidden states within the context of visualizing a machine transla-\ntion pipeline, allowing for users to debug multiple stages of the\ntranslation process. Rather than interpret the RNN hidden states of\na model with ﬁxed parameters, Cashman et al. [3] instead focus\non visualizing the training process itself by showing the gradient\nupdates during the RNN learning process.\nThese studies all focus on visualizing single types of the RNN\nhidden states, such as their embeddings or memory cells. They also\nuse various techniques to relate the hidden states back to the inputs\nor outputs of the task. However, to our knowledge previous work\ndoes not focus on representing the semantics of hidden states with\nrespect to the task input/outputs in such a way that this interpretation\nmay be visually compared across the various types of hidden states\nwithin the RNN.\n3\nBACKGROUND\nA Recurrent Neural Network is a mathematical framework which\nmaps a sequence of inputs {x1,..,xT },x ∈RV to a sequence of\ninternal states {h1,..,hT },h ∈RN. Each ht is updated by the non-\nlinear function ht ←RNN(xt,ht−1). This recurrent nature allows\nthe previous hidden states ht−1 to capture the context necessary to\ncompute the next RNN update.\nWithin the RNN formulation itself, any number of operations\nmay be performed when leading from xt and xt−1 to ht. Typically,\neach of these operations is written as a vector deﬁnition, further\ndescribing the different kinds of hidden states comprising the model.\nWe refer to the concept of a vector deﬁnition, including that of the\nﬁnal hidden state ht itself, as the “kinds” of hidden states of the\nRNN. Each kind of hidden state describes a speciﬁc component of\nthe recurrent model.\nDepending on the speciﬁc task against which the RNN is de-\nployed, the ﬁnal hidden state of a timestep is fed into a function\nF(h) to produce an output y. In the case of a classiﬁcation problem,\ny ∈[0,1]K represents a probability distribution across K labels.\nThis study looks speciﬁcally at the Long Short-Term Memory\narchitecture (LSTM) [4], a speciﬁc type of RNN. The detailed math-\nematical notation of the LSTM is laid out in the Appendix.\n4\nDESIGN\nWe set out to design a visualization technique to help researchers\nbetter understand the general intuitions RNNs capture. Our study\nfollows the nested model of visual design and validation described\nby Munzner [13] which lays out four layers, each built off the one\nbefore.\n4.1\nLayers 1 & 2: Domain & Abstraction\nWe preface the design by ﬁrst describing the user groups interested\nin studying RNNs at this level of detail.\nStrobelt et al. [17] develop three high level categories of user\ngroups interested in RNNs; Architects, Trainers, and End Users.\nWe look speciﬁcally at users from the various groups with some\ndegree of pre-existing understanding of neural networks, primarily\nArchitects and Trainers. These users want to understand the detailed\nbehaviours RNNs learn to model, whether that understanding may\nbe used to drive development of architectural improvements or to\ngain conﬁdence that their results generalize beyond the training data.\nMoreover, these users want an intuition for what the RNN represents\nin order to better grasp how it functions.\nFrom this group of users elucidate a series of questions which they\nare interested in asking. 1) What information ﬂows and changes from\nthe inputs through to the outputs of the RNN? 2) Where are changes\nmore and less prominent? 3) Does our intuition about the role of the\nmathematical components match that of the actual RNN behaviour?\nAt a high level, all these questions seek to understand how the model\nbehaves from a mathematical/computational perspective. From these\nquestions, we ascertain abstract operations and data types which can\nlater be mapped to visualization techniques.\nVectors are the most natural concept to focus on for study, not\nsimply because there is precedent in previous works [5,9,17], but\nmainly because the RNN description uses vectors as the core element\nof notation. Focusing on vectors ties directly to how these users\nnaturally describe these models, ensuring visualization accessibility.\nImplicit within the concept of RNN hidden state vectors are the\ndimensional values they encode, typically referred to as activations.\nThese activations are the focus of much of the previous work in\nvisualizing RNN details. However, in the context of visualizing\nhigh level information ﬂow, it is dubious what information may\nbe gleaned by observing the activation values themselves. That is,\nvisualizing these numeric values without a relatable interpretation\nof their meaning may not provide insights into the model behaviour.\nTherefore, for the purposes of this work which intends to explore\nthe ﬂow of information throughout the RNN, we choose to abstract\naway activation details into the concept of a hidden state as a whole.\nWith this data type in mind, we explore the options for visual-\nization operations that may be performed on hidden state vectors.\nSince users want to interpret hidden states as a whole, we propose\nhidden state comparisons as the primary visualization operation.\nBy comparing the change in hidden states along the components\nof the RNN, these types of high level questions about the ﬂow of\ninformation may begin to be answered.\n4.2\nLayers 3 & 4: Encoding-Interaction & Algorithm\nContinuing with the ﬁnal two layers of the nested model of visual\ndesign, this section describes the development of a visual encoding\nand algorithms to address requirements from the previous section.\nWe recall from the previous section the desire to express the\nintuitions encoded within RNN hidden states as a whole. To do\nso, we introduce the semantics of hidden states as a probability\ndistribution over the task output labels.\nMore formally, we consider the context free1 function G(γ,v)\nwhich produces a probability distribution over the outputs y, where\nγ denotes the speciﬁc kind of hidden state of the instance v. Indeed,\nthis formulation is analogous to the classiﬁcation task of the RNN\nitself, and the function F(h) can be seen as a specialization of G\nsuch that F(h) = G(ﬁnal-hidden-state,h).\nThe function G is modelled separately from that of the actual\nRNN and its classiﬁcation function F, allowing it to represent arbi-\ntrary hidden states. G is trained on the hidden state instances derived\nfrom the sequential dataset once training the RNN is complete so as\nto capture the ﬁnal semantics learned by the model.\nAlthough the task of the RNN is to speciﬁcally predict the output\nlabels, the semantic we propose may similarly be used to encode\nhidden states with respect to the input labels they correspond to. It\nalso may be used to predict the output labels at varying timesteps of\nthe RNN, so that differing layers of semantics may be expressed by\nany particular hidden state. The choice of which semantic(s) to use\nwill depend on the speciﬁc visualization task.\nNotice, the notion of relating hidden states in some way to the\ntask inputs or outputs is not a new one, as for example is done\nin RNNVis [12]. However, to our knowledge other work has not\n1The function is context free in that it makes a prediction similar to that\nof the RNN, but without any surrounding hidden state context.\nPSE Visual Encoding\nFigure 2: Visual encoding for the PSE with various probability distribu-\ntions. In each case, the dominant colour of the probability distribution\nis green to the degrees of 1) 90%, 2) 50%, and 3) 33.33%.\ndrawn a direct parallel between hidden states encoding a probability\ndistribution similar to that of the task itself.\nThe advantage to this semantic formulation is that hidden states\ncan share the same visual encoding as that of the RNN output pre-\ndiction. Moreover, this formulation describes hidden states in a way\nthat is invariant of their internal signiﬁcance. These attributes allow\nusers to seamlessly transition between comparisons of hidden state\nsemantics amongst the components of the RNN as well as the task\nitself.\nWith these semantics in place, we develop a simple visual en-\ncoding designed to represent probability distributions across typical\noutputs of RNNs. We use a mini-bar chart where bars represent\nclass labels and the bar length represents the probability magnitude.\nSince there may be many class labels, only the top-k probabilities\nare shown in the bar chart, where k is selected depending on the\nvisualization context. The class labels are shown in descending\norder based off the probability distribution so that the mini-bar chart\nrepresents the most likely outcomes. Finally, each bar is coloured ac-\ncording to a user speciﬁed colour mapping, e.g. to show information\nabout parts of speech. This allows for designers to draw attention to\nclass labels as applicable to the visualization task.\nWe augment the mini-bar chart with a redundant encoding that\ndescribes the probability distribution more holistically and facilitates\na quick, high level comparison of the hidden state semantic. The\nredundant encoding uses a simple coloured rectangle, where the\ncolour also comes from the visualization’s colour mapping. The\ncolour is produced by ﬁrst selecting which mapped colour holds\nthe largest sum of probabilities in the top-k predictions from the\ndistribution. Then, a point is interpolated inversely proportional\nbetween this most likely colour and the colour “white”.\nBy example, if the mass of the probability distribution describes\npredictions all mapped to the same colour, then the coloured rectan-\ngle will reﬂect this colour without any white dilution. However, if\nthe mass of the probability distribution is roughly uniform across the\ncolour mapping, then the most likely colour will be diluted to almost\ncompletely white. In this way, the colour interpolation encodes the\ndegree by which the probability distribution represents the dominant\nclass labels. Figure 2 shows examples of this visual encoding.\n4.3\nInteractive Visualization Framework\nWe implement a simple proof of concept tool focused on observing\nthe RNN hidden states and architecture as a whole.\nAs comparing hidden states is important for understanding the\nﬂow of information throughout the model, the visualization shows\nmany of the different kinds of hidden states comprising the model.\nThese are arranged according to their relative position in terms of\nthe progression of time t in the sequential input data, where time is\nrepresented as progressing from top to bottom in the display. Addi-\ntionally, the hidden states are arranged relative to their dependence\nin the RNN deﬁnition, so that those more directly dependent on the\ninputs xt are positioned to the left of the visualization, while those\ncloser to the outputs yt are placed on the right.\nThe visualization tool allows for users to provide arbitrary se-\nquential inputs to observe and compare the elicited hidden state\nrepresentations. It should be noted that this high level view of the\nRNN architecture is particularly suited for showing the intuition and\nconceptual interaction behind the detailed components of the model.\n5\nEVALUATION\nWe evaluate our design with the following set of studies. Indeed,\nthese are preliminary results that encourage further follow up.\nAlthough the proposed design element is applicable to any se-\nquential modelling task, we focus on the well known language model\ntask. A language model is a natural language processing problem\nwhereby each input word must produce a probability distribution\ndescribing the likelihood of the words that follow it.\nWe evaluate these studies on the Penn Treebank [10] language\nmodel dataset, trained on a 300×2 LSTM. Numerals and infrequent\nwords are mapped to the NUM and UNK tokens, so that the resulting\nvocabulary consists of 14787 unique words. The dataset contains\n54020 and 6754 training and testing sentences, and the model is\ntrained to achieve a perplexity of 66.5 on the held out test set. For\nthe visualization, we use a colour mapping based on coarse level part\nof speech tags to highlight grammatical switches in the language\nmodel. The colour interpolation for the PSE sets the top-k as 10%\nof the vocabulary.\n5.1\nCase Study: Visualizing Information Flow\nIn this case study, we explore the high level ﬂow of information\nthrough the RNN. The user chooses the input “ we stand in\nsolidarity , ” she emphasized . to observe the transition\nfrom the direct quote to end of the sentence. The full visualization\nfor this input and other examples can be found externally [15], while\nFigure 1 shows just hidden states from the ﬁnal layer of LSTM for\nthe 7th and 8th timesteps.\nThe language model, who’s outputs are visualized under the yt\nlabels in the ﬁgure, has learned to immediately predict verbs or\nnouns to follow the end of quotation sequence. By looking at the\nhidden state PSEs for the top row, although some of the colour\ninterpolations agree with this semantic, it appears that the RNN is\nrepresenting some degree of uncertainty at this stage in the state it\nmodels. This uncertainty is manifested two fold, ﬁrstly by the weakly\ncoloured purple and green interpolations, and secondly by the l2\n7 and\nc2\n7 hidden states instead forming a yellow-based interpolation.\nInterestingly, the bottom row corresponding to the input of the\nword she appears to represent much more certainty in its prediction\nof a verb to follow. This is an indication that the hidden states of the\nRNN at this point are generally conﬁdent about what will follow in\nthe language model. Since the input at this point is a noun, at a high\nlevel this explains how the internal state of the RNN has resolved\nthe uncertainty from the previous timestep.\nFocusing more closely on these PSE details, other aspects of the\nRNN internal behaviours are revealed. In both rows of the ﬁgure, the\nprevious Cell State c2\n6 and c2\n7 has interpolated a mainly white colour,\nindicating how the PSE for these hidden states represent a distribu-\ntion without any particularly dominant label grouping. This roughly\nuniform distribution can be further observed by looking at the top-3\npredicted classes, which all sit ﬂat together with equal probabilities.\nBy conveying this information, the PSE visual encoding shows how\nthe previous Cell States must be gated to formulate the Long-term\nMemory before they can indicate a meaningful prediction.\nAnother interesting example is in l2\n8, which indicates a probability\ndistribution generally predicting green nouns, despite the fact that\nthe input at this timestep should have switched the semantics to a\npurple verb prediction. Although this PSE interpolation is a faded\ngreen colour, it shows how the Long-term Memory is still retaining\nPSE Accuracy per Hidden State\nFigure 3: Per hidden state test set perplexity of the fully trained PSE\nusing a linear classiﬁer. Note the y-axis is truncated at 400, despite\nsome values exceeding this range.\nsemantics which indicate a noun prediction. However, when this\nhidden state is combined with the Short-term Memory of s2\n8 to\nproduce c2\n8 we see how the internal representation of the LSTM\nhas switched to indicating a verb. This reveals how the internal\nmechanics of the LSTM at this point in the model are combined to\nmake a prediction, with the semantics from the Short-term Memory\npassing through as a result of that interaction.\nNotice this effect is not predetermined, as seen for example across\nthe same interaction in the previous timestep. Instead there, it ap-\npears that the semantics of the Long-term Memory l2\n7 win out when\nbuilding c2\n7. At that point, it appears to be the Output Gate (not\nvisualized) which switches out the indication of a yellow-function\nword to produce the ﬁnal hidden state h2\n7 predicting a noun/verb to\nfollow in the model.\nObserving the RNN model at this level of detail begins to reveal\nimportant nuances to the decisions its makes. Moreover, these\nnuances are made available particularly through the visualization\nand comparison of the different hidden states comprising the model.\n5.2\nAccuracy of Predictive Semantic Encodings\nThis experiment investigates the accuracy of the Predictive Semantic\nEncoding model formulation. Additionally, we use this formulation\nas a means by which the hidden states of the RNN can be understood.\nIn order to draw parallels with the RNN, we train and test on\nthe full dataset of hidden states derived from the original sequential\ntraining and testing data. This results in a total of 20M training and\n2.6M testing data pairs across the 17 various LSTM hidden states (1\nword embedding + 8 hidden states across 2 layers).\nThe PSE is evaluated using perplexity, which is the same metric\nas that of evaluating the language model task.\nPerplexity(X) =\nT\nv\nu\nu\nt\nT\n∏\nt=1\n1\nP(xt|w1...wt−1)\n(1)\nWith X representing the sentence input and P(xt|xt...xt−1) as the\nmodel’s conditional probability of a word xt given the context of\nprevious words xt...xt−1. As perplexity is normalized over the length\nT of the input, it is suited for comparisons between the sequential\nRNN data and that of the non-sequential PSE.\nFigure 3 shows the results of this experiment using a simple\nlinear classiﬁer as the PSE function G. Notice, G is modelled with a\nseparate set of parameters for each kind of hidden state.\nFirstly, we ensure parity with the RNN by examining the 2nd layer\nOutput hidden state performance. This hidden state achieves a test\nset perplexity of 70, the best of all the hidden states, and critically\nthis value is on par with the RNN which achieved a perplexity\nscore of 66.5. This result establishes that the PSE are an accurate\ngeneralization of the RNN classiﬁcation function F.\nNext, the result that immediately stands out is at the 2nd layer for\nthe Long-term Memory and Cell hidden states. These values are\nseveral orders of magnitude larger than any other hidden states. De-\nspite this startling result, this ﬁnding actually explains an important\naspect to the RNN architecture. Speciﬁcally, the Long-term Memory\nand Cell States represent information that spans through the LSTM\nacross time, rather than only containing information relevant to the\ncurrent timestep. As a result, these hidden states, especially at the\nﬁnal layer of the LSTM, are not reliable indicators of what ﬁnal\nprediction will be made by the language model.\nAnother way to think about this is that the information from\nthese hidden states is highly dependent on other pieces of context\nrepresented throughout the LSTM in making a ﬁnal decision at any\ngiven timestep. Therefore, it is not surprising that the Long-term\nMemory and Cell behave so poorly for this task.\nWe observe that the best performing hidden state is the Output\nof the ﬁnal layer. This is what we expect, since the model for this\nhidden state is equivalent to that of the actual RNNs classiﬁer F.\nNotice, the only difference in the LSTM architecture between the\nOutput and the Cell State, which contrast the best and worst case\nresults in the ﬁgure, is the Output Gate. This fact highlights the\nimportant role this particular gating unit plays in the RNN.\nIt is worth noting that although these several kinds of hidden\nstates are poor estimators of the RNN itself, that does not mean\nthey perform poorly as Predictive Semantic Encodings. Rather, this\nﬁnding suggests they are working correctly, since the idea of a PSE is\nnot to mimic the RNN, but instead to encode some sense of relatable\nsemantics of hidden states as a whole. Since some hidden states\nnaturally bear little stand-alone representational power, we expect\ntheir semantics to be highly ambiguous as these ﬁndings indicate.\nWith an appropriate visual encoding, this ambiguity will pop out in\na visualization and help lead users to the same conclusion about the\nsemantics of these hidden states.\n6\nSUMMARY\nWe introduce a technique for representing hidden states called Pre-\ndictive Semantic Encodings (PSE). PSEs are a formulation that\nexpresses the meaning of hidden states as a whole in relation to\nthe task inputs or outputs. We propose a visual encoding for this\ntechnique that is comparable across differing kinds of hidden states,\nmaking it a powerful tool in visualizing the interaction between\ncomponents of RNNs.\nAlthough this idea is simple, its key contribution is allowing\nfor visualizations to show the high level intuitions captured within\nhidden states. Moreover, its compact form make rendering many\nhidden states at once possible, opening avenues for visualizations to\nbetter investigate the internal details of these complex models.\nWe use this technique to perform a preliminary investigation into\na realistic task and dataset; that of a language model. This visual\nencoding brings to light previously un-visualized aspects of the RNN\nmodel behaviour. Moreover, it highlights various areas of interest\nin the model such as where signiﬁcant shifts occur in hidden state\nsemantics. The evaluation concludes by showing how the PSE is\nequivalent to the model classiﬁer, as well as by using it to explain\nsome of the complexity represented across the hidden states of the\nRNN itself.\nACKNOWLEDGMENTS\nWe thank the anonymous reviewers who’s comments and suggestions\ngreatly helped to improve and clarify this manuscript.\nREFERENCES\n[1] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by\njointly learning to align and translate. arXiv preprint arXiv:1409.0473,\n2014.\n[2] T. Bolukbasi, K.-W. Chang, J. Y. Zou, V. Saligrama, and A. T. Kalai.\nMan is to computer programmer as woman is to homemaker? debiasing\nword embeddings.\nIn Advances in neural information processing\nsystems, pp. 4349–4357, 2016.\n[3] D. Cashman, G. Patterson, A. Mosca, and R. Chang. Rnnbow: Vi-\nsualizing learning via backpropagation gradients in recurrent neural\nnetworks. In Workshop on Visual Analytics for Deep Learning (VADL),\nvol. 4, 2017.\n[4] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural\ncomputation, 9(8):1735–1780, 1997.\n[5] M. Kahng, P. Y. Andrews, A. Kalro, and D. H. P. Chau. Activis:\nVisual exploration of industry-scale deep neural network models. IEEE\ntransactions on visualization and computer graphics, 24(1):88–97,\n2018.\n[6] A. Karpathy, J. Johnson, and L. Fei-Fei. Visualizing and understanding\nrecurrent networks. arXiv preprint arXiv:1506.02078, 2015.\n[7] O. Levy, K. Lee, N. FitzGerald, and L. Zettlemoyer. Long short-term\nmemory as a dynamically computed element-wise weighted sum. arXiv\npreprint arXiv:1805.03716, 2018.\n[8] J. Li, X. Chen, E. Hovy, and D. Jurafsky. Visualizing and understanding\nneural models in nlp. arXiv preprint arXiv:1506.01066, 2015.\n[9] J. Li, W. Monroe, and D. Jurafsky. Understanding neural networks\nthrough representation erasure.\narXiv preprint arXiv:1612.08220,\n2016.\n[10] M. Marcus, B. Santorini, and M. A. Marcinkiewicz. Building a large\nannotated corpus of english: The penn treebank. 1993.\n[11] T. Mikolov, M. Karaﬁ´at, L. Burget, J. ˇCernock`y, and S. Khudanpur.\nRecurrent neural network based language model. In Eleventh annual\nconference of the international speech communication association,\n2010.\n[12] Y. Ming, S. Cao, R. Zhang, Z. Li, Y. Chen, Y. Song, and H. Qu.\nUnderstanding hidden memories of recurrent neural networks. In 2017\nIEEE Conference on Visual Analytics Science and Technology (VAST),\npp. 13–24. IEEE, 2017.\n[13] T. Munzner. A nested model for visualization design and validation.\nIEEE transactions on visualization and computer graphics, 15(6):921–\n928, 2009.\n[14] M. T. Ribeiro, S. Singh, and C. Guestrin. Why should i trust you?:\nExplaining the predictions of any classiﬁer. In Proceedings of the 22nd\nACM SIGKDD international conference on knowledge discovery and\ndata mining, pp. 1135–1144. ACM, 2016.\n[15] L. Sawatzky. Visualizing rnn states with predictive semantic encodings\n- supplemental material, Jul 2019.\n[16] H. Strobelt, S. Gehrmann, M. Behrisch, A. Perer, H. Pﬁster, and A. M.\nRush. Seq-2-seq-vis: A visual debugging tool for sequence-to-sequence\nmodels. IEEE transactions on visualization and computer graphics,\n25(1):353–363, 2019.\n[17] H. Strobelt, S. Gehrmann, H. Pﬁster, and A. M. Rush. Lstmvis: A\ntool for visual analysis of hidden state dynamics in recurrent neural\nnetworks. IEEE transactions on visualization and computer graphics,\n24(1):667–676, 2018.\n[18] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning\nwith neural networks. In Advances in neural information processing\nsystems, pp. 3104–3112, 2014.\n[19] D. Tang, B. Qin, and T. Liu. Document modeling with gated recurrent\nneural network for sentiment classiﬁcation. In Proceedings of the 2015\nconference on empirical methods in natural language processing, pp.\n1422–1432, 2015.\nAPPENDIX: LONG SHORT-TERM MEMORY\nThe recurrence function of the LSTM follows. We use subscripts t\nto denote the timestep through the sequential data, and superscripts\nu to denoted the stacked layers (units) of the recurrent function.\nfu\nt = σ(Wf [hu−1\nt\n,hu\nt−1]+bf )\nForget Gate\niu\nt = σ(Wi[hu−1\nt\n,hu\nt−1]+bi)\nRemember Gate\nou\nt = σ(Wo[hu−1\nt\n,hu\nt−1]+bo)\nOutput Gate\n˜cu\nt = tanh(Wc[hu−1\nt\n,hu\nt−1]+bc)\nCell Input\nlu\nt = fu\nt ◦cu\nt−1\nLong-term Memory\nsu\nt = iu\nt ◦˜cu\nt\nShort-term Memory\ncu\nt = lu\nt +su\nt\nCell State\nhu\nt = ou\nt ◦tanh(cu\nt )\nOutput State\nWhere W∗represents a matrix of the size Nx2N and b∗a bias\nvector Nx1.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2019-08-01",
  "updated": "2019-08-01"
}