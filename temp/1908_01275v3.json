{
  "id": "http://arxiv.org/abs/1908.01275v3",
  "title": "A View on Deep Reinforcement Learning in System Optimization",
  "authors": [
    "Ameer Haj-Ali",
    "Nesreen K. Ahmed",
    "Ted Willke",
    "Joseph Gonzalez",
    "Krste Asanovic",
    "Ion Stoica"
  ],
  "abstract": "Many real-world systems problems require reasoning about the long term\nconsequences of actions taken to configure and manage the system. These\nproblems with delayed and often sequentially aggregated reward, are often\ninherently reinforcement learning problems and present the opportunity to\nleverage the recent substantial advances in deep reinforcement learning.\nHowever, in some cases, it is not clear why deep reinforcement learning is a\ngood fit for the problem. Sometimes, it does not perform better than the\nstate-of-the-art solutions. And in other cases, random search or greedy\nalgorithms could outperform deep reinforcement learning. In this paper, we\nreview, discuss, and evaluate the recent trends of using deep reinforcement\nlearning in system optimization. We propose a set of essential metrics to guide\nfuture works in evaluating the efficacy of using deep reinforcement learning in\nsystem optimization. Our evaluation includes challenges, the types of problems,\ntheir formulation in the deep reinforcement learning setting, embedding, the\nmodel used, efficiency, and robustness. We conclude with a discussion on open\nchallenges and potential directions for pushing further the integration of\nreinforcement learning in system optimization.",
  "text": "A VIEW ON DEEP REINFORCEMENT LEARNING IN SYSTEM OPTIMIZATION\nAmeer Haj-Ali 1 2 Nesreen K. Ahmed 1 Ted Willke 1 Joseph E. Gonzalez 2 Krste Asanovic 2 Ion Stoica 2\nABSTRACT\nMany real-world systems problems require reasoning about the long term consequences of actions taken to\nconﬁgure and manage the system. These problems with delayed and often sequentially aggregated reward, are\noften inherently reinforcement learning problems and present the opportunity to leverage the recent substantial\nadvances in deep reinforcement learning. However, in some cases, it is not clear why deep reinforcement learning\nis a good ﬁt for the problem. Sometimes, it does not perform better than the state-of-the-art solutions. And in\nother cases, random search or greedy algorithms could outperform deep reinforcement learning. In this paper, we\nreview, discuss, and evaluate the recent trends of using deep reinforcement learning in system optimization. We\npropose a set of essential metrics to guide future works in evaluating the efﬁcacy of using deep reinforcement\nlearning in system optimization. Our evaluation includes challenges, the types of problems, their formulation in\nthe deep reinforcement learning setting, embedding, the model used, efﬁciency, and robustness. We conclude\nwith a discussion on open challenges and potential directions for pushing further the integration of reinforcement\nlearning in system optimization.\n1\nINTRODUCTION\nReinforcement learning (RL) is a class of learning problems\nframed in the context of planning on a Markov Decision\nProcess (MDP) (Bellman, 1957), when the MDP is not\nknown. In RL, an agent continually interacts with the en-\nvironment (Kaelbling et al., 1996; Sutton et al., 2018). In\nparticular, the agent observes the state of the environment,\nand based on this observation takes an action. The goal of\nthe RL agent is then to compute a policy–a mapping be-\ntween the environment states and actions–that maximizes\na long term reward. There are multiple ways to extrapo-\nlate the policy. Non-approximation methods usually fail to\npredict good actions in states that were not visited in the\npast, and require storing all the action-reward pairs for every\nvisited state, a task that incurs a huge memory overhead\nand complex computation. Instead, approximation methods\nhave been proposed. Among the most successful ones is\nusing a neural network in conjunction with RL, also known\nas deep RL. Deep models allow RL algorithms to solve\ncomplex problems in an end-to-end fashion, handle unstruc-\ntured environments, learn complex functions, or predict\nactions in states that have not been visited in the past. Deep\nRL is gaining wide interest recently due to its success in\nrobotics, Atari games, and superhuman capabilities (Mnih\net al., 2013; Doya, 2000; Kober et al., 2013; Peters et al.,\n2003). Deep RL was the key technique behind defeating the\nhuman European champion in the game of Go, which has\nlong been viewed as the most challenging of classic games\nfor artiﬁcial intelligence (Silver et al., 2016).\n1Intel Labs 2University of California, Berkeley. Correspon-\ndence to: Ameer Haj-Ali <ameerh@berkeley.edu>.\nMany system optimization problems have a nature of de-\nlayed, sparse, aggregated or sequential rewards, where im-\nproving the long term sum of rewards is more important\nthan a single immediate reward. For example, an RL en-\nvironment can be a computer cluster. The state could be\ndeﬁned as a combination of the current resource utilization,\navailable resources, time of the day, duration of jobs waiting\nto run, etc. The action could be to determine on which re-\nsources to schedule each job. The reward could be the total\nrevenue, jobs served in a time window, wait time, energy\nefﬁciency, etc., depending on the objective. In this example,\nif the objective is to minimize the waiting time of all jobs,\nthen a good solution must interact with the computer cluster\nand monitor the overall wait time of the jobs to determine\ngood schedules. This behavior is inherent in RL. The RL\nagent has the advantage of not requiring expert labels or\nknowledge and instead the ability to learn directly from its\nown interaction with the world. RL can also learn sophisti-\ncated system characteristics that a straightforward solution\nlike ﬁrst come ﬁrst served allocation scheme cannot. For\ninstance, it could be better to put earlier long-running ar-\nrivals on hold if a shorter job requiring fewer resources is\nexpected shortly.\nIn this paper, we review different attempts to overcome sys-\ntem optimization challenges with the use of deep RL. Unlike\nprevious reviews (Hameed et al., 2016; Mahdavinejad et al.,\n2018; Krauter et al., 2002; Wang et al., 2018; Ashouri et al.,\n2018; Luong et al., 2019) that focus on machine learning\nmethods without discussing deep RL models or applying\nthem beyond a speciﬁc system problem, we focus on deep\nRL in system optimization in general. From reviewing prior\nwork, it is evident that standardized metrics for assessing\narXiv:1908.01275v3  [cs.LG]  4 Sep 2019\nA View on Deep Reinforcement Learning in System Optimization\nFigure 1. RL environment example. By observing the state of the\nenvironment (the cluster resources and arriving jobs’ demands), the\nRL agent makes resource allocation actions for which he receives\nrewards as revenues. The agent’s goal is to make allocations that\nmaximize cumulative revenue.\ndeep RL solutions in system optimization problems are lack-\ning. We thus propose quintessential metrics to guide future\nwork in evaluating the use of deep RL in system optimiza-\ntion. We also discuss and address multiple challenges that\nfaced when integrating deep RL into systems.\n2\nBACKGROUND\nOne of the promising machine learning approaches is rein-\nforcement learning (RL), in which an agent learns by con-\ntinually interacting with an environment (Kaelbling et al.,\n1996). In RL, the agent observes the state of the environ-\nment, and based on this state/observation takes an action\nas illustrated in ﬁgure 1. The ultimate goal is to compute\na policy–a mapping between the environment states and\nactions–that maximizes expected reward. RL can be viewed\nas a stochastic optimization solution for solving Markov De-\ncision Processes (MDPs) (Bellman, 1957), when the MDP\nis not known. An MDP is deﬁned by a tuple with four el-\nements: S, A, P(s, a), r(s, a) where S is the set of states\nof the environment, A describes the set of actions or tran-\nsitions between states, s′∼P(s, a) describes the probability\ndistribution of next states given the current state and action\nand r(s, a) : S × A →R is the reward of taking action a\nin state s. Given an MDP, the goal of the agent is to gain\nthe largest possible cumulative reward. The objective of an\nRL algorithm associated with an MDP is to ﬁnd a decision\npolicy π∗(a|s) : s →A that achieves this goal for that\nMDP:\nπ∗= arg max\nπ\nEτ∼π(τ) [τ] =\narg max\nπ\nEτ∼π(τ)\n\"X\nt\nr(st, at)\n#\n,\n(1)\nwhere τ is a sequence of states and actions that deﬁne a\nsingle episode, and T is the length of that episode. Deep\nRL leverages a neural network to learn the policy (and\nsometimes the reward function). Over the past couple of\nyears, a plethora of new deep RL techniques have been pro-\nposed (Mnih et al., 2016; Ross et al., 2011; Sutton et al.,\n2000; Schulman et al., 2017; Lillicrap et al., 2015).\nPolicy Gradient (PG) (Sutton et al., 2000), for example,\nuses a neural network to represent the policy. This policy is\nupdated directly by differentiating the term in Equation 1 as\nfollows:\n∇θJ = ∇θEτ∼π(τ)\n\"X\nt\nr(st, at)\n#\n= Eτ∼π(τ)\n\" X\nt\n∇θlogπθ(at|st)\n!  X\nt\nr(st, at)\n!#\n≈1\nN\nN\nX\ni=1\n\" X\nt\n∇θlogπθ(ai,t|si,t)\n!  X\nt\nr(si,t, ai,t)\n!#\n(2)\nand updating the network parameters (weights) in the direc-\ntion of the gradient:\nθ ←θ + α∇θJ,\n(3)\nProximal Policy Optimization (PPO) (Schulman et al., 2017)\nimproves on top of PG for more deterministic, stable, and\nrobust behavior by limiting the updates and ensuring the\ndeviation from the previous policy is not large.\nIn contrast, Q-Learning (Watkins et al., 1992), state-action-\nreward-state-action (SARSA) (Rummery et al., 1994) and\ndeep deterministic policy gradient (DDPG) (Lillicrap et al.,\n2015) are temporal difference methods, i.e., they update\nthe policy on every timestep (action) rather than on every\nepisode. Furthermore, these algorithms bootstrap and, in-\nstead of using a neural network for the policy itself, they\nlearn a Q-function, which estimates the long term reward\nfrom taking an action. The policy is then deﬁned using this\nQ-function. In Q-Learning the Q-function is updated as\nfollows:\nQ(st, at) ←Q(st, at)+r(st, at)+γmaxa′\nt[Q(s′\nt, a′\nt)].\n(4)\nIn other words, the Q-function updates are performed based\non the action that maximizes the value of that Q-function.\nOn the other hand, in SARSA, the Q-function is updated as\nfollows:\nQ(st, at) ←Q(st, at) + r(st, at) + γQ(st+1, at+1).\n(5)\nIn this case, the Q-function updates are performed based\non the action that the policy would select given state st.\nDDPG ﬁts multiple neural networks to the policy, including\nthe Q-function and target time-delayed copies that slowly\ntrack the learned networks and greatly improve stability in\nlearning.\nA View on Deep Reinforcement Learning in System Optimization\nAlgorithms such as upper-conﬁdence-bound and greedy\ncan then be used to determine the policy based on the Q-\nfunction (Auer, 2002; Sutton et al., 2018). The reviewed\nworks in this paper focus on the epsilon greedy method\nwhere the policy is deﬁned as follows:\nπ∗(at|st) =\n(\narg maxat Q(st, at),\nw.p. 1 −ϵ\nrandom action,\nw.p. ϵ\n(6)\nA method is considered to be on-policy if the new policy is\ncomputed directly from the decisions made by the current\npolicy. PG, PPO, and SARSA are thus on-policy while\nDDPG and Q-Learning are off-policy. All the mentioned\nmethods are model-free: they do not require a model of the\nenvironment to learn, but instead learn directly from the\nenvironment by trial and error. In some cases, a model of\nthe environment could be available. It may also be possible\nto learn a model of the environment. This model could be\nused for planning and enable more robust training as less\ninteraction with the environment may be required.\nMost RL methods considered in this review are structured\naround value function estimation (e.g., Q-values) and using\ngradients to update the policy. However, this is not always\nthe case. For example, genetic algorithms, simulated anneal-\ning, genetic programming, and other gradient-free optimiza-\ntion methods - often called evolutionary methods (Sutton\net al., 2018) - can also solve RL problems in a manner anal-\nogous to the way biological evolution produces organisms\nwith skilled behavior. Evolutionary methods can be effec-\ntive if the space of policies is sufﬁciently small, the policies\nare common and easy to ﬁnd, and the state of the environ-\nment is not fully observable. This review considers only the\ndeep versions of these methods, i.e., using a neural network\nin conjunction with evolutionary methods typically used to\nevolve and update the neural network parameters or vice\nversa.\nMulti-armed bandits (Berry et al., 1985; Auer et al., 2002)\nsimplify RL by removing the learning dependency on state\nand thus providing evaluative feedback that depends entirely\non the action taken (1-step RL problems). The actions usu-\nally are decided upon in a greedy manner by updating the\nbeneﬁt estimates of performing each action independently\nfrom other actions. To consider the state in a bandit solution,\ncontextual bandits may be used (Chu et al., 2011). In many\ncases, a bandit solution may perform as well as a more com-\nplicated RL solution or even better. Many Bandit algorithms\nenjoy stronger theoretical guarantees on their performance\neven under adversarial settings. These bounds would likely\nbe of great value to the systems world as they suggest in the\nlimit that the proposed algorithm would be no worse than\nusing the best ﬁxed system conﬁguration in hindsight.\n2.1\nPrior RL Works With Alternative Approximation\nMethods\nMultiple prior works have proposed to use non-deep neural\nnetwork approximation methods for RL in system optimiza-\ntion. These works include reliability and monitoring (Das\net al., 2014; Zhu et al., 2007; Zeppenfeld et al., 2008), mem-\nory management (Ipek et al., 2008; Andreasson et al., 2002;\nPeled et al., 2015; Diegues et al., 2014) in multicore systems,\ncongestion control (Li et al., 2016; Silva et al., 2016), packet\nrouting (Choi et al., 1996; Littman et al., 2013; Boyan et al.,\n1994), algorithm selection (Lagoudakis et al., 2000), cloud\ncaching (Sadeghi et al., 2017), energy efﬁciency (Farah-\nnakian et al., 2014) and performance (Peng et al., 2015;\nJamshidi et al., 2015; Barrett et al., 2013; Arabnejad et al.,\n2017; Mostafavi et al., 2018). Instead of using a neural\nnetwork to approximate the policy, these works used tables,\nlinear approximations, and other approximation methods to\ntrain and represent the policy. Tables were generally used to\nstore the Q-values, i.e., one value for each action, state pair,\nwhich are used in training, and this table becomes the ulti-\nmate policy. In general, deep neural networks allowed for\nmore complex forms of policies and Q functions (Lin, 1993),\nand can better approximate good actions in new states.\n3\nRL IN SYSTEM OPTIMIZATION\nIn this section, we discuss the different system challenges\ntackled using RL and divide them into two categories:\nEpisodic Tasks, in which the agent-environment interaction\nnaturally breaks down into a sequence of separate terminat-\ning episodes, and Continuing Tasks, in which it does not.\nFor example, when optimizing resources in the cloud, the\njobs arrive continuously and there is not a clear termination\nstate. But when optimizing the order of SQL joins, the query\nhas a ﬁnite number of joins, and thus after enough steps the\nagent arrives at a terminating state.\n3.1\nContinuing Tasks\nAn important feature of RL is that it can learn from sparse\nreward signals, does not need expert labels, and the ability\nto learn direction from its own interaction with the world.\nJobs in the cloud arrive in an unpredictable and continuous\nmanner. This might explain why many system optimization\nchallenges tackled with RL are in the cloud (Mao et al.,\n2016; He et al., 2017a;b; Tesauro et al., 2006; Xu et al.,\n2017; Liu et al., 2017; Xu et al., 2012; Rao et al., 2009).\nA good job scheduler in the cloud should make decisions\nthat are good in the long term. Such a scheduler should\nsometimes forgo short term gains in an effort to realise\ngreater long term beneﬁts. For example, it might be better\nto delay a long running job if a short running job is expected\nto arrive soon. The scheduler should also adapt to variations\nin the underlying resource performance and scale in the\npresence of new or unseen workloads combined with large\nnumbers of resources.\nA View on Deep Reinforcement Learning in System Optimization\nThese schedulers have a variety of objectives, including\nminimizing average performance of jobs and optimizing the\nresource allocation of virtual machines (Mao et al., 2016;\nTesauro et al., 2006; Xu et al., 2012; Rao et al., 2009),\noptimizing data caching on edge devices and base stations\n(He et al., 2017a;b), and maximizing energy efﬁciency (Xu\net al., 2017; Liu et al., 2017). The RL algorithms used for\naddressing each system problem are listed in Table 1 lists\nthe RL algorithms used for addressing each problem.\nInterestingly, for cloud challenges most works are driven by\nQ-learning (or the very similar SARSA). In the absence of\na complete environmental model, model-free Q-Learning\ncan be used to generate optimal policies. It is able to make\npredictions incrementally by bootstrapping the current es-\ntimate with previous estimates and provide good sample\nefﬁciency (Jin et al., 2018). Q-Learning is also character-\nized by inherent continuous temporal difference behavior\nwhere the policy can be updated immediately after each step\n(not the end of trajectory); something that might be very\nuseful for online adaptation.\n3.2\nEpisodic Tasks\nDue to the sequential nature of decision making in RL, the\norder of the actions taken has a major impact on the rewards\nthe RL agent collects. The agent can thus learn these pat-\nterns and select more rewarding actions. Previous works\ntook advantage of this behavior in RL to optimize conges-\ntion control (Jay et al., 2019; Ruffy et al., 2018), decision\ntrees for packet classiﬁcation (Liang et al., 2019), sequence\nto SQL/program translation (Zhong et al., 2017; Guu et al.,\n2017; Liang et al., 2016), ordering of SQL joins (Krishnan\net al., 2018; Ortiz et al., 2018; Marcus et al., 2018; 2019),\ncompiler phase ordering (Huang et al., 2019; Kulkarni et al.,\n2012) and device placement (Addanki et al., 2019; Paliwal\net al., 2019).\nAfter enough steps in these problems, the agent will always\narrive at a clear terminating step. For example, in query join\norder optimization, the number of joins is ﬁnite and known\nfrom the query. In congestion control – where the routers\nneed to adapt the sending rates to provide high throughput\nwithout comprising fairness – the updates are performed\non a ﬁxed number of senders/receivers known in advance.\nThese updates combined deﬁne one episode. This may ex-\nplain why there is a trend towards using PG methods for\nthese types of problems, as they don’t require a continu-\nous temporal difference behavior and can often operate in\nbatches of multiple queries. Nevertheless, in some cases,\nQ-learning is still used, mainly for sample efﬁciency as the\nenvironment step might take a relatively long time.\nTo improve the performance of PG methods, it is possible\nto take advantage of the way the gradient computation is\nperformed. If the environment is not needed to generate\nthe observation, it is possible to save many environment\nsteps. This is achieved by rolling out the whole episode\nfrom interacting only with the policy and performing one\nenvironment step at the very end. The sum of rewards will be\nthe same as the reward received from this environment step.\nFor example, in query optimization, since the observations\nare encoded directly from the actions, and the environment\nis mainly used to generate the rewards, it will be possible to\nrepeatedly perform an action, form the observation directly\nfrom this action, and feed it to the policy network. After the\nend of the episode, the environment can be triggered to get\nthe ﬁnal reward, which would be the sum of the intermediate\nrewards. This can signiﬁcantly reduce the training time.\n3.3\nDiscussion: Continuous vs. Episodic\nContinuous policies can handle both continuous and\nepisodic tasks, while episodic policies cannot. So, for ex-\nample, Q-Learning can handle all the tasks mentioned in\nthis work, while PG based methods cannot directly han-\ndle it without modiﬁcation. For example, in (Mao et al.,\n2016), the authors limited the the scheduler window of jobs\nto M, allowing the agent in every time step to schedule\nup to M jobs out of all arrived jobs. The authors also dis-\ncussed this issue of ”bounded time horizon” and hoped to\novercome it by using a value network to replace the time-\ndependent baseline. It is interesting to note that prior work\non continuous system optimization tasks using non deep RL\napproaches (Choi et al., 1996; Littman et al., 2013; Boyan\net al., 1994; Peng et al., 2015; Jamshidi et al., 2015; Barrett\net al., 2013; Arabnejad et al., 2017; Sadeghi et al., 2017;\nFarahnakian et al., 2014) used Q-Learning.\nOne solution for handling continuing problems without\nepisode boundaries with PG based methods is to deﬁne\nperformance in terms of the average rate of reward per time\nstep (Sutton et al., 2018) (Chapter 13.6). Such approaches\ncan help better ﬁt the continuous problems to episodic RL\nalgorithms.\n4\nFORMULATING THE RL ENVIRONMENT\nTable 1 lists all the works we reviewed and their problem\nformulations in the context of RL, i.e., the model, observa-\ntions, actions and rewards deﬁnitions. Among the major\nchallenges when formulating the problem in the RL envi-\nronment is properly deﬁning the system problem as an RL\nproblem, with all of the required inputs and outputs, i.e.,\nstate, action spaces and rewards. The rewards are generally\nsparse and behave similarly for different actions, making\nthe RL training ineffective due to bad gradients. The states\nare generally deﬁned using hand engineered features that\nare believed to encode the state of the system. This results\nin a large state space with some features that are less help-\nful than others and rarely captures the actual system state.\nUsing model-based RL can alleviate this bottleneck and\nprovide more sample efﬁciency. (Liu et al., 2017) used auto-\nA View on Deep Reinforcement Learning in System Optimization\nTable 1. Problem formulation in the deep RL setting. The model abbreviations are: fully connected neural networks (FCNN), convolutional\nneural network (CNN), recurrent neural network (RNN), graph neural network (GNN), gated recurrent unit (GRU), and long short-term\nmemory (LSTM).\nDescription\nReference\nState/Observation\nAction\nReward\nObjective\nAlgorithm\nModel\ncongestion control\n(Jay et al., 2019)1\n(Ruffy et al., 2018)2\nhistories of sending\nrates and resulting\nstatistics (e.g., loss rate)\nchanges to sending rate\nthroughput\nand negative of\nlatency or\nloss rate\nmaximize throughput\nwhile maintaining\nfairness\nPPO1,2/PG2/DDPG2\nFCNN\npacket classiﬁcation\n(Liang et al., 2019)\nencoding of the\ntree node, e.g.,\nsplit rules\ncutting a classiﬁcation\ntree node or partitioning\na set of rules\nclassiﬁcation time\n/memory\nfootprint\nbuild optimal decision\ntree for packet\nclassiﬁcation\nPPO\nFCNN\nSQL join\norder optimization\n(Krishnan et al., 2018)1\n(Ortiz et al., 2018)2\n(Marcus et al., 2019)3\n(Marcus et al., 2018)4\nencoding of\ncurrent join plan\nnext relation to join\nnegative cost1−3,\n1/cost4\nminimize execution\ntime\nQ-Learning1−3/PPO4\ntree conv.3,\nFCNN1−4\nsequence to\nSQL\n(Zhong et al., 2017)\nSQL vocabulary,\nquestion, column\nnames\nquery corresponding\nto the token\n-2 invalid query,\n-1 valid but wrong,\n+1 valid and right\ntokens in the\nWHERE clause\nPG\nLSTM\nlanguage to\nprogram translation\n(Guu et al., 2017)\nnatural language\nutterances\na sequence of\nprogram tokens\n1 if correct result\n0 otherwise\ngenerate equivalent\nprogram\nPG\nLSTM,\nFCNN\nsemantic parsing\n(Liang et al., 2016)\nembedding of\nthe words\na sequence of\nprogram tokens\npositive if correct\n0 otherwise\ngenerate equivalent\nprogram\nPG\nRNN,\nGRU\nresource allocation\nin the cloud\n(Mao et al., 2016)\ncurrent allocation of\ncluster resources &\nresource proﬁles of\nwaiting jobs\nnext job\nto schedule\nΣi( −1\nTi ) for\nall jobs in the\nsystem (Ti is the\nduration of job i)\nminimize average\njob slowdown\nPG\nFCNN\nresource allocation\n(He et al., 2017a;b)\nstatus of edge\ndevices, base stations,\ncontent caches\nwhich base station,\nto ofﬂoad/cache\nor not\ntotal revenue\nmaximize total\nrevenue\nQ-Learning\nCNN\nresource allocation\nin the cloud\n(Tesauro et al., 2006)\ncurrent allocation\n& demand\nnext resource\nto allocate\npayments\nmaximize revenue\nQ-Learning\nFCNN\nresource allocation\nin cloud radio\naccess networks\n(Xu et al., 2017)\nactive remote radio\nheads & user demands\nwhich remote\nradio heads\nto activate\nnegative power\nconsumption\npower\nefﬁciency\nQ-Learning\nFCNN\ncloud resource\nallocation &\npower management\n(Liu et al., 2017)\ncurrent allocation\n& demand\nnext resource\nto allocate\nlinear combination\nof total power ,\nVM latency, &\nreliability metrics\npower efﬁciency\nQ-Learning\nautoencoder,\nweight sharing\n& LSTM\nautomate virtual\nmachine (VM)\nconﬁguration process\n(Rao et al., 2009)\n(Xu et al., 2012)\ncurrent resource\nallocations\nincrease/decrease\nCPU/time/memory\nthroughput\n-response time\nmaximize performance\nQ-Learning\nFCNN,\nmodel-based\ncompiler phase\nordering\n(Kulkarni et al., 2012)1\n(Huang et al., 2019)2\nprogram features\nnext optimization\npass\nperformance\nimprovement\nminimize execution\ntime\nEvolutionary Methods1/\nQ-Learning2/PG2\nFCNN\ndevice placement\n(Paliwal et al., 2019)1\n(Addanki et al., 2019)2\ncomputation graph\nplacement/schedule\nof graph node\nspeedup\nmaximize performance\n& minimize peak\nmemory\nPG1,2/\nEvolutionary Methods1 GNN/FCNN\ndistributed instr-\nuction placement\n(Coons et al., 2008)\ninstruction features\ninstruction placement\nlocation\nspeedup\nmaximize performance Evolutionary Methods\nFCNN\nencoders to help reduce the state dimensionality. The action\nspace is also large but generally represents actions that are\ndirectly related to the objective. Another challenge is the\nenvironment step. Some tasks require a long time for the\nenvironment to perform one step, signiﬁcantly slowing the\nlearning process of the RL agent.\nInterestingly, most works focus on using simple out-of-the-\nbox FCNNs, while some works that targeted parsing and\ntranslation ((Liang et al., 2016; Guu et al., 2017; Zhong\net al., 2017)) used RNNs (Graves et al., 2013) due to their\nability to parse strings and natural language. While FCNNs\nare simple and easy to train to learn a linear and non-linear\nfunction policy mappings, sometimes having a more compli-\ncated network structure suited for the problem could further\nimprove the results.\n4.1\nEvaluation Results\nTable 2 lists training, and evaluation results of the reviewed\nworks. We consider the time it takes to perform a step in the\nenvironment, the number of steps needed in each iteration of\ntraining, number of training iterations, total number of steps\nneeded, and whether the prior work improves the state of\nthe art and compares against random search/bandit solution.\nThe total number of steps and the the cost of each environ-\nment step is important to understand the sample efﬁciency\nand practicality of the solution, especially when consider-\ning RLs inherent sample inefﬁciency (Schaal, 1997; Hester\net al., 2018). For different workloads, the number of samples\nneeded varies from thousands to millions. The environment\nstep time also varies from milliseconds to minutes. In multi-\nple cases, the interaction with the environment is very slow.\nNote that in most cases when the environment step time\nwas a few milliseconds, it was because it was a simulated\nenvironment, not a real one. We observe that for faster\nA View on Deep Reinforcement Learning in System Optimization\nTable 2. Evaluation results.\nWork\nProblem\nEnvironment\nStep Time\nNumber of Steps\nPer Iteration\nNumber of training\nIterations\nTotal Number\nOf Steps\nImproves State\nof the Art\nCompares Against\nBandit/Random\nSearch\npacket\nclassiﬁcation\n(Liang et al., 2019)\n20-600ms\nup to 60,000\nup to 167\n1,002,000\n(18%)\n\u0015\ncongestion\ncontrol\n(Jay et al., 2019)\n50-500ms\n8192\n1200\n9,830,400\n(similar)\ncongestion\ncontrol\n(Ruffy et al., 2018)\n0.5s\nN/A\nN/A\n50,000-100,000\n\u0015\n\u0015\nresource\nallocation\n(Mao et al., 2016)\n10-40ms\n20,000\n1000\n20,000,000\n(10-63%)\nresource\nallocation\n(He et al., 2017a)\n(He et al., 2017b)\nN/A\nN/A\n20,000\nN/A\nno comparison\n\u0015\nresource\nallocation\n(Tesauro et al., 2006)\nN/A\nN/A\n10,000-20,000\nN/A\nno comparison\nresource\nallocation\n(Xu et al., 2017)\nN/A\nN/A\nN/A\nN/A\nno comparison\nresource\nallocation\n(Liu et al., 2017)\n1-120 minutes\n100,000\n20\n2,000,000\nno comparison\n\u0015\nresource\nallocation\n(Rao et al., 2009)\n(Xu et al., 2012)\nN/A\nN/A\nN/A\nN/A\nno comparison\n\u0015\nSQL\nJoins\n(Krishnan et al., 2018)\n10ms\n640\n100\n64,000\n(70%)\nSQL\njoins\n(Ortiz et al., 2018)\nN/A\nN/A\nN/A\nN/A\nno comparison\n\u0015\nSQL\njoins\n(Marcus et al., 2019)\n250ms\n100-8,000\n100\n10,000-80,000\n(10-66%)\nSQL\njoins\n(Marcus et al., 2018)\n1.08s\nN/A\nN/A\n10,000\n(20%)\nsequence to\nSQL\n(Zhong et al., 2017)\nN/A\n80,654\n300\n24,196,200\n(similar)\n\u0015\nlanguage to\nprogram trans.\n(Guu et al., 2017)\nN/A\nN/A\nN/A\n13,000\n(56%)\n\u0015\nsemantic\nparsing\n(Liang et al., 2016)\nN/A\n3,098\n200\n619,600\n(3.4%)\n\u0015\nphase\nordering\n(Huang et al., 2019)\n1s\nN/A\nN/A\n1,000-10,000\n(similar)\nphase\nordering\n(Kulkarni et al., 2012)\n13.2 days\nfor all steps\nN/A\nN/A\nN/A\n\u0015\n\u0015\ndevice\nplacement\n(Addanki et al., 2019)N/A (seconds)\nN/A\nN/A\n1,600-94,000\n(3%)\ndevice\nplacement\n(Paliwal et al., 2019) N/A (seconds)\nN/A\nN/A\n400,000\n(5%)\ninstruction\nplacement\n(Coons et al., 2008) N/A (minutes)\nN/A\n200\nN/A (days)\n\u0015\n\u0015\nenvironment steps more training samples were gathered to\nleverage that and further improve the performance. This\nexcludes (Liu et al., 2017) where a cluster was used and\nthus more samples could be processed in parallel.\nAs listed in Table 2, many works did not provide sufﬁcient\ndata to reproduce the results. Reproducing the results is\nnecessary to further improve the solution and enable future\nevaluation and comparison against it.\n4.2\nFrameworks and Toolkits\nA few RL benchmark toolkits for developing and com-\nparing reinforcement learning algorithms, and providing\na faster simulated system environment, were recently pro-\nposed. OpenAI Gym (Brockman et al., 2016) supports an\nenvironment for teaching agents everything, from walking\nto playing games like Pong or Pinball. Iroko (Ruffy et al.,\n2018) provides a data center emulator to understand the\nrequirements and limitations of applying RL in data center\nnetworks. It interfaces with the OpenAI Gym and offers a\nway to evaluate centralized and decentralized RL algorithms\nagainst conventional trafﬁc control solutions.\nPark (Mao et al., 2019) proposes an open platform for easier\nformulation of the RL environment for twelve real world\nsystem optimization problems with one common easy to\nuse API. The platform provides a translation layer between\nA View on Deep Reinforcement Learning in System Optimization\nthe system and the RL environment making it easier for\nRL researchers to work on systems problems. That being\nsaid, the framework lacks the ability to change the action,\nstate and reward deﬁnitions, making it harder to improve\nthe performance by easily modifying these deﬁnitions.\n5\nCONSIDERATIONS FOR EVALUATING\nDEEP RL IN SYSTEM OPTIMIZATION\nIn this section, we propose a set of questions that can help\nsystem optimization researchers determine whether deep\nRL could be an effective tool in solving their systems opti-\nmization challenges.\nCan the System Optimization Problem Be Modeled by\nan MDP?\nThe problem of RL is the optimal control of an MDP. MDPs\nare a classical formalization of sequential decision making,\nwhere actions inﬂuence not just immediate rewards but also\nfuture states and rewards. This involves delayed rewards\nand the trade-off between delayed and immediate reward.\nIn MDPs, the new state and new reward are dependent only\non the preceding state and action. Given a perfect model of\nthe environment, an MDP can compute the optimal policy.\nMDPs are typically a straightforward formulation of the sys-\ntem problem, as an agent learns by continually interacting\nwith the system to achieve a particular goal, and the system\nresponds to these interactions with a new state and reward.\nThe agent’s goal is to maximize expected reward over time.\nIs It a Reinforcement Learning Problem?\nWhat distinguishes RL from other machine learning ap-\nproaches is the presence of self exploration and exploitation,\nand the tradeoff between them. For example, RL is different\nfrom supervised learning. The latter is learning from a train-\ning set with labels provided by an external supervisor that\nis knowledgeable. For each example the label is the correct\naction the system should take. The objective of this kind of\nlearning is to act correctly in new situations not present in\nthe training set. However, supervised learning is not suitable\nfor learning from interaction, as it is often impractical to\nobtain examples representative of all the cases in which the\nagent has to act.\nAre the Rewards Delayed?\nRL algorithms do not maximize the immediate reward of\ntaking actions but, rather, expected reward over time. For ex-\nample, an RL agent can choose to take actions that give low\nimmediate rewards but that lead to higher rewards overall,\ninstead of taking greedy actions every step that lead to high\nimmediate rewards but low rewards overall. If the objective\nis to maximize the immediate reward or the actions are not\ndependent, then other simpler approaches, such as bandits\nand greedy algorithms, will perform better than deep RL, as\ntheir objective is to maximize the immediate reward.\nWhat is Being Learned?\nIt is important to provide insights on what is being learned\nby the agent. For example, what actions are taken in which\nstates and why? Can the knowledge learned be applied to\nnew states/tasks? Is there a structure to the problem being\nlearned? If a brute-force solution is possible for simpler\ntasks, it will also be helpful to know how much better the\nperformance of the RL agent is than the brute force solution.\nIn some cases, not all hand-engineered features are useful.\nUsing all of them can result in high variance and prolonged\ntraining. Feature analysis can help overcome this challenge.\nFor example, in (Coons et al., 2008) signiﬁcant performance\ngaps were shown for different feature selection.\nDoes It Outperform Random Search and a Bandit\nSolution?\nIn some cases, the RL solution is just another form of a im-\nproved random search. In some cases, good RL results were\nachieved merely by chance. For instance, if the features\nused to represent the state are not good or do not have a\npattern that could be learned. In such cases, random search\nmight perform as well as RL, or even better, as it is less\ncomplicated. For example, in (Huang et al., 2019), the au-\nthors showed 10% improvement over the baseline by using\nrandom search. In some cases the actions are independent\nand a greedy or bandit solution can achieve the optimal or\nnear-optimal solution. Using a bandit method is equivalent\nusing a 1-step RL solution, in which the objective is to max-\nimize the immediate reward. Maximizing the immediate\nreward could deliver the overall maximum reward and, thus,\na comparison against a bandit solution can help reveal this.\nAre the Expert Actions Observable?\nIn some cases it might be possible to have access to expert\nactions, i.e., optimal actions. For example, if a brute force\nsearch is plausible and practical then it is possible to outper-\nform deep RL by using it or using imitation learning (Schaal,\n1999), which is a supervised learning approach that learns\nby imitating expert actions.\nIs It Possible to Reproduce/Generalize Good Results?\nThe learning process in deep RL is stochastic and thus good\nresults are sometimes achieved due to local maxima, simple\ntasks, and chance. In (Haarnoja et al., 2018) different re-\nsults were generated by just changing the random seeds. In\nmany cases, good results cannot be reproduced by retraining,\ntraining on new tasks, or generalizing to new tasks.\nDoes It Outperform the State of the Art?\nThe most important metric in the context of system opti-\nmization in general is outperforming the state of the art.\nImproving the state of the art includes different objectives,\nsuch as efﬁciency, performance, throughput, bandwidth,\nfault tolerance, security, utilization, reliability, robustness,\ncomplexity, and energy. If the proposed approach does not\nperform better than the state of the art in some metric then\nA View on Deep Reinforcement Learning in System Optimization\nit is hard to justify using it. Frequently, the state of the art\nsolution is also more stable, practical, and reliable than deep\nRL. In many prior works listed in Table 2 a comparison\nagainst the state of the art is not available or deep RL per-\nforms worse. In some cases deep RL can perform as good\nas the state of the art or slightly worse, but still be a useful\nsolution as it achieves an improvement on other metrics.\n6\nRL METHODS AND NEURAL NETWORK\nMODELS\nMultiple RL methods and neural network models can be\nused. RL frameworks like RLlib (Liang et al., 2017), In-\ntel’s Coach (Caspi et al., 2017), TensorForce (Kuhnle et al.,\n2017), Facebook Horizon (Gauci et al., 2018), and Google’s\nDopamine (Castro et al., 2018) can help the users pick the\nright RL model, as they provide implementations of many\npolicies and models for which a convenient interface is\navailable.\nAs a rule of thumb, we rank RL algorithms based on sam-\nple efﬁciency as follows: model-based approaches (most\nefﬁcient), temporal difference methods, PG methods, and\nevolutionary algorithms (least efﬁcient). In general, many\nRL environments run in a simulator. For example (Paliwal\net al., 2019; Mao et al., 2019; 2016), run in a simulator as\nthe real environment’s step would take minutes or hours,\nwhich signiﬁcantly slows down the training. If this simula-\ntor is fast enough or training time is not constrained then\nPG methods can perform well. If the simulator is not fast\nenough or training time is constrained then temporal differ-\nence methods can do better than PG methods as they are\nmore sample efﬁcient.\nIf the environment is the real one, then temporal difference\ncan do well, as long as interaction with the environment is\nnot slow. Model-based RL performs better if the environ-\nment is slow. Model-based methods require a model of the\nenvironment (that can be learned) and rely mainly on plan-\nning rather than learning (Deisenroth et al., 2011; Guo et al.,\n2014). Since planning is not done in the actual environment,\nbut in much faster simulation steps within the model, it\nrequires less samples from the real environment to learn.\nMany real-world system problems have well established and\noften highly accurate models, which model-based methods\ncan leverage. That being said, model-free methods are often\nused as they are simpler to deploy and have the potential to\ngeneralize better from exploration in a real environment.\nIf good policies are easy to ﬁnd and if either the space of\npolicies is small enough or time is not a bottleneck for the\nsearch, then evolutionary methods can be effective. Evo-\nlutionary methods also have advantages when the learning\nagent cannot observe the complete state of the environment.\nAs mentioned earlier, bandit solutions are good if the prob-\nlem can be viewed as a one-step RL problem.\nPG methods are in general more stable than methods like Q-\nLearning that do not directly use and derive a neural network\nto represent the agent’s policy. The greedy nature of directly\nderiving the policy and moving the gradient in the direction\nof the objective also make PG methods easier to reason\nabout and often more reliable. However, Q-Learning can\nbe applied to data collected from a running system more\nreadily than PG, which must interact with the system during\ntraining.\nThe RL methods may be implemented using any number of\ndeep neural network architectures. The preferred architec-\nture depends on the the nature of the observation and action\nspaces. CNNs that efﬁciently capture spatially-organized\nobservation spaces lend themselves visual data (e.g., images\nor video). Networks designed for sequential learning, such\nas RNNs, are appropriate for observation spaces involving\nsequence data (e.g., code, queries, temporal event streams).\nOtherwise, FCNNs are preferred for their general applica-\nbility and ease of use, although they tend to be the most\ncomputationally-intensive choice. Finally, GNNs or other\nnetworks that capture structure within observations can be\nused in the less frequent case that the designer has a priori\nknowledge of the representational structure. In this case,\nthe model can even generate structured action spaces (e.g.,\na query plan tree or computational graph).\n7\nCHALLENGES\nIn this section, we discuss the primary challenges that face\nthe application of deep RL in system optimization.\nInteractions with Real Systems Can Be Slow. General-\nizing from Faster Simulated Environments Can Be Re-\nstrictive. Unlike the case with simulated environments that\ncan run fast, when running on a real system, performing\nan action can trigger a reward after a lengthy delay. For\nexample, when scheduling jobs on a cluster of nodes, some\njobs might require hours to run, and thus improving their\nperformance by monitoring job execution time will be very\nslow. To speed up this process, some works use simulators\nas cost models instead of the actual system. These simu-\nlators often do not fully capture the actual behavior of the\nreal system and thus the RL agent may not work as well in\npractice. More comprehensive environment models can aid\ngeneralization from simulated environments. RL methods\nthat are more sample efﬁcient will speed up training in real\nsystem environments.\nInstability and High Variance. This is a common problem\nwhich leads to bad policies when tackling system problems\nwith deep RL. Such policies can generate a large perfor-\nmance gap when trained multiple times and behave in an\nunpredictable manner. This is mainly due to poor formula-\nA View on Deep Reinforcement Learning in System Optimization\ntion of the problem as an RL problem, limited observation\nof the state, i.e., the use of embeddings and input features\nthat are not sufﬁcient/meaningful, and sparse or similar re-\nwards. Sparse rewards can be due to bad reward deﬁnition\nor the fact that some rewards cannot be computed directly\nand are known only at the end of the episode. For example,\nin (Liang et al., 2019), where deep RL is used to optimize\ndecision trees for packet classiﬁcation, the reward (the per-\nformance of the tree) is known only when the whole tree is\nbuilt, or after approximately 15,000 steps. In some cases\nusing more robust and stable policies can help. For example,\nQ-learning is known to have good sample efﬁciency but\nunstable behavior. SARSA, double Q-learning (Van Hasselt\net al., 2016) and policy gradient methods, on the other hand,\nare more stable. Subtracting a bias in PG can also help\nreduce variance (Greensmith et al., 2004).\nLack of Reproducibility. Reproducibility is a frequent\nchallenge with many recent works in system optimization\nthat rely on deep RL. It becomes difﬁcult to reproduce the\nresults due to restricted access to the resources, code, and\nworkloads used, lack of a detailed list of the used network\nhyperparameters and lack of stable, predictable, and scalable\nbehavior of the different RL algorithms. This challenge\nprevents future deployment, incremental improvements, and\nproper evaluation.\nDeﬁning Appropriate Rewards, Actions and States. The\nproper deﬁnition of states, actions, and rewards is the key,\nsince otherwise the RL solution is not useful. In the general\nuse case of deep RL, deﬁning the states, actions and rewards\nis much more straightforward than in the case in system opti-\nmization. For example, in atari games, the state is an image\nrepresenting the current status of the game, the rewards are\nthe points collected while playing and the actions are moves\nin the game. However, often in system optimization, it is\nnot clear what are the appropriate deﬁnitions. Furthermore,\nin many cases the rewards are sparse or similar, the states\nare not fully observable to capture the whole system state\nand have limited features that capture only a small portion\nof the system state. This results in unstable and inadequate\npolicies. Generally, the action and state spaces are large,\nrequiring a lot of samples to learn and resulting in instabil-\nity and large variance in the learned network. Therefore,\nretraining often fails to generate the same results.\nLack of Generalization. The lack of generalization is an\nissue that deep RL solutions often suffer from. This might\nbe beneﬁcial when learning a particular structure. For exam-\nple, in NeuroCuts (Liang et al., 2019), the target is to build\nthe best decision tree for ﬁxed set of predeﬁned rules and\nthus the objective of the RL agent is to ﬁnd the optimal ﬁt\nfor these rules. However, lack of generalization sometimes\nresults in a solution that works for a particular workload\nor setting but overall, across various workloads, is not very\ngood. This problem manifests when generalization is im-\nportant and the RL agent has to deal with new states that\nit did not visit in the past. For example, in (Paliwal et al.,\n2019; Addanki et al., 2019), where the RL agent has to learn\ngood resource placements for different computation graphs,\nthe authors avoided the possibility of learning only good\nplacements for particular computation graphs by training\nand testing on a wide range graphs.\nLack of Standardized Benchmarks, Frameworks and\nEvaluation Metrics. The lack of standardized benchmarks,\nframeworks and evaluation metrics makes it very difﬁcult\nto evaluate the effectiveness of the deep RL methods in\nthe context of system optimization. Thus, it is crucial to\nhave proper standardized frameworks and evaluation met-\nrics that deﬁne success. Moreover, benchmarks are needed\nthat enable proper training, evaluation of the results, measur-\ning the generalization of the solution to new problems and\nperforming valid comparisons against baseline approaches.\n8\nAN ILLUSTRATIVE EXAMPLE\nWe put all the metrics (from Section 5) to work and further\nhighlight the challenges (from Section 7) of implementing\ndeep RL solutions using DeepRM (Mao et al., 2016) as\nan illustrative example. In DeepRM, the targeted system\nproblem is resource allocation in the cloud. The objective is\nto avoid job slowdown, i.e., the goal is to minimize the wait\ntime for all jobs. DeepRM uses PG in conjunction with a\nsimulated environment rather than a real cloud environment.\nThis signiﬁcantly improves the step time but can result in\nrestricted generalization when used in a real environment.\nFurthermore, since all the simulation parameters are known,\nthe full state of the simulated environment can be captured.\nThe actions are deﬁned as selecting which job should be\nscheduled next. The state is deﬁned as the current allocation\nof cluster resources, as well as the resource proﬁles of jobs\nwaiting to be scheduled. The reward is deﬁned as the sum of\nof job slowdowns: Σi( −1\nTi ) where Ti is the pure execution\ntime of job i without considering the wait time. This reward\nbasically gives a penalty of −1 for jobs that are waiting to\nbe scheduled. The penalty is divided by Ti to give a higher\npriority to shorter jobs.\nThe state, actions and reward clearly deﬁne an MDP and\na reinforcement learning problem. Speciﬁcally, the agent\ninteracts with the system by making sequential allocations,\nobserving the state of the current allocation of resources\nand receiving delayed long-term rewards as overall slow\ndowns of jobs. The rewards are delayed because the agent\ncannot know the effect of the current allocation action on\nthe overall slow down at any particular time step; the agent\nwould have to wait until all the other jobs are allocated to\nassess the full impact. The agent then learns which jobs to\nallocate in the current time step to minimize the average\njob slowdown, given the current resource allocation in the\nA View on Deep Reinforcement Learning in System Optimization\ncloud. Note that DeepRM also learns to withhold larger jobs\nto make room for smaller jobs to reduce the overall average\njob slowdown. DeepRM is shown to outperform random\nsearch.\nExpert actions are not available in this problem as there\nare no methods to ﬁnd the optimal allocation decision at\nany particular time step. During training in DeepRM, mul-\ntiple examples of job arrival sequences were considered\nto encourage policy generalization and robust decisions1.\nDeepRM is also shown to outperform the state-of-the-art by\n10–63%1.\nClearly, in the case of DeepRM, most of the challenges\nmentioned in Section 7 are manifested. The interaction\nwith the real cloud environment is slow and thus the authors\nopted for a simulated environment. This has the advantage\nof speeding up the training but may result in a policy that\ndoes not generalize to the real environment. Unfortunately,\ngeneralization tests in the real environment were not pro-\nvided. The instability and high variance were addressed by\nsubtracting a bias in the PG equation. The bias was deﬁned\nas the average of job slowdowns taken at a single time step\nacross all episodes. The implementation of DeepRM was\nopen sourced allowing others to reproduce the results. The\nrewards, actions, and states deﬁned allowed the agent to\nlearn a policy that performed well in the simulated environ-\nment. Note that deﬁning the state of the system was easier\nbecause the environment was simulated. The solution also\nconsidered multiple reward deﬁnitions. For example, −|J|,\nwhere J is the number of unﬁnished jobs in the system. This\nreward deﬁnition optimizes the average job completion time.\nThe jobs evaluated in DeepRM were considered to arrive\nonline according to a Bernoulli process. In addition, the\njobs were chosen randomly and it is unclear whether they\nrepresent real workload scenarios or not. This emphasizes\nthe need for standardized benchmarks and frameworks to\nevaluate the effectiveness of deep RL methods in scheduling\njobs in the cloud.\n9\nFUTURE DIRECTIONS\nWe see multiple future directions for the deployment of deep\nRL in system optimization tasks. The general assumption is\nthat deep RL may be useful in every system problem where\nthe problem can be formulated as a sequential decision mak-\ning process, and where meaningful action, state, and reward\ndeﬁnitions can be provided. The objective of deep RL in\nsuch systems may span a wide range of options, such as\nenergy efﬁciency, power, reliability, monitoring, revenue,\nperformance, and utilization. At the processor level, deep\nRL could be used in branch prediction, memory prefetch-\ning, caching, data alignment, garbage collection, thread/task\nscheduling, power management, reliability, and monitoring.\n1Results provided were only in the simulated system.\nCompilers may also beneﬁt from using deep RL to opti-\nmize the order of passes (optimizations), knobs/pragmas,\nunrolling factors, memory expansion, function inlining, vec-\ntorizing multiple instructions, tiling and instruction selec-\ntion. With advancement of in- and near-memory processing,\ndeep RL can be used to determine which portions of a work-\nload should be performed in/near memory and which outside\nthe memory.\nAt a higher system level, deep RL may be used in\nSQL/pandas query optimization, cloud computing, schedul-\ning, caching, monitoring (e.g., temperature/failure) and fault\ntolerance, packet routing and classiﬁcation, congestion con-\ntrol, FPGA allocation, and algorithm selection. While some\nof this has already been done, we believe there is big poten-\ntial for improvement. It is necessary to explore more bench-\nmarks, stable and generalizable learners, transfer learning\napproaches, RL algorithms, model-based RL and, more im-\nportantly, to provide better encoding of the states, actions\nand rewards to better represent the system and thus improve\nthe learning. For example, with SQL/pandas join order\noptimization, the contents of the database are critical for\ndetermining the best order, and thus somehow incorporat-\ning an encoding of these contents may further improve the\nperformance.\nThere is room for improvement in the RL algorithms as\nwell. Some action and state spaces can dynamically change\nwith time. For example, when adding a new node to a\ncluster, the RL agent will always skip the added node and\nit will not be captured in the environment state. Generally,\nthe state transition function of the environment is unknown\nto the agent. Therefore, there is no guarantee that if the\nagent takes a certain action, a certain state will follow in the\nenvironment. This issue was presented in (Kulkarni et al.,\n2012), where compiler optimization passes were selected\nusing deep RL. The authors mentioned a situation where\nthe agent is stuck in an inﬁnite loop of repeatedly picking\nthe same optimization (action) back to back. This issue\narose when a particular optimization did not change the\nfeatures that describe the state of the environment, causing\nthe neural network to apply the same optimization. To\nbreak this inﬁnite loop, the authors limited the number of\nrepetitions to ﬁve, and then instead, applied the second\nbest optimization. This was done by taking the actions\nthat corresponds to the second highest probability from the\nneural network’s probability distribution output.\n10\nCONCLUSION\nIn this work, we reviewed and discussed multiple challenges\nin applying deep reinforcement learning to system optimiza-\ntion problems and proposed a set of metrics that can help\nevaluate the effectiveness of these solutions. Recent appli-\ncations of deep RL in system optimization are mainly in\nA View on Deep Reinforcement Learning in System Optimization\npacket classiﬁcation, congestion control, compiler optimiza-\ntion, scheduling, query optimization and cloud computing.\nThe growing complexity in systems demands learning based\napproaches. Deep RL presents unique opportunity to ad-\ndress the dynamic behavior of systems. Applying deep RL\nto systems proposes new set of challenges on how to frame\nand evaluate deep RL techniques. We anticipate that solving\nthese challenges will enable system optimization with deep\nRL to grow.\nREFERENCES\nAddanki, R., Venkatakrishnan, S. B., Gupta, S., Mao, H.,\nand Alizadeh, M. Placeto: Learning generalizable device\nplacement algorithms for distributed machine learning.\narXiv preprint arXiv:1906.08879, 2019.\nAndreasson, E., Hoffmann, F., and Lindholm, O. To collect\nor not to collect? machine learning for memory manage-\nment. In Java Virtual Machine Research and Technology\nSymposium, pp. 27–39. Citeseer, 2002.\nArabnejad, H., Pahl, C., Jamshidi, P., and Estrada, G. A com-\nparison of reinforcement learning techniques for fuzzy\ncloud auto-scaling. In Proceedings of the 17th IEEE/ACM\nInternational Symposium on Cluster, Cloud and Grid\nComputing, pp. 64–73. IEEE Press, 2017.\nAshouri, A. H., Killian, W., Cavazos, J., Palermo, G., and\nSilvano, C. A survey on compiler autotuning using ma-\nchine learning. ACM Computing Surveys (CSUR), 51(5):\n96, 2018.\nAuer, P.\nUsing conﬁdence bounds for exploitation-\nexploration trade-offs.\nJournal of Machine Learning\nResearch, 3(Nov):397–422, 2002.\nAuer, P., Cesa-Bianchi, N., and Fischer, P.\nFinite-time\nanalysis of the multiarmed bandit problem. Machine\nlearning, 47(2-3):235–256, 2002.\nBarrett, E., Howley, E., and Duggan, J. Applying reinforce-\nment learning towards automating resource allocation\nand application scalability in the cloud. Concurrency and\nComputation: Practice and Experience, 25(12):1656–\n1674, 2013.\nBellman, R. A markovian decision process. In Journal of\nMathematics and Mechanics, pp. 679–684, 1957.\nBerry, D. A., and Fristedt, B. Bandit problems: sequential\nallocation of experiments (monographs on statistics and\napplied probability). London: Chapman and Hall, 5:\n71–87, 1985.\nBoyan, J. A., and Littman, M. L. Packet routing in dy-\nnamically changing networks: A reinforcement learning\napproach. In Advances in neural information processing\nsystems, pp. 671–678, 1994.\nBrockman, G., Cheung, V., Pettersson, L., Schneider, J.,\nSchulman, J., Tang, J., and Zaremba, W. Openai gym,\n2016.\nCaspi, I., Leibovich, G., Novik, G., and Endrawis, S. Rein-\nforcement learning coach, December 2017. URL https:\n//doi.org/10.5281/zenodo.1134899.\nCastro, P. S., Moitra, S., Gelada, C., Kumar, S., and Belle-\nmare, M. G. Dopamine: A Research Framework for\nDeep Reinforcement Learning.\n2018.\nURL http:\n//arxiv.org/abs/1812.06110.\nChoi, S. P., and Yeung, D.-Y.\nPredictive q-routing: A\nmemory-based reinforcement learning approach to adap-\ntive trafﬁc control. In Advances in Neural Information\nProcessing Systems, pp. 945–951, 1996.\nChu, W., Li, L., Reyzin, L., and Schapire, R. Contextual\nbandits with linear payoff functions.\nIn Proceedings\nof the Fourteenth International Conference on Artiﬁcial\nIntelligence and Statistics, pp. 208–214, 2011.\nCoons, K. E., Robatmili, B., Taylor, M. E., Maher, B. A.,\nBurger, D., and McKinley, K. S. Feature selection and\npolicy optimization for distributed instruction placement\nusing reinforcement learning. In Proceedings of the 17th\ninternational conference on Parallel architectures and\ncompilation techniques, pp. 32–42. ACM, 2008.\nDas, A., Shaﬁk, R. A., Merrett, G. V., Al-Hashimi, B. M.,\nKumar, A., and Veeravalli, B. Reinforcement learning-\nbased inter-and intra-application thermal optimization for\nlifetime improvement of multicore systems. In Proceed-\nings of the 51st Annual Design Automation Conference,\npp. 1–6. ACM, 2014.\nDeisenroth, M. P., Rasmussen, C. E., and Fox, D. Learning\nto control a low-cost manipulator using data-efﬁcient\nreinforcement learning. Robotics: Science and Systems\nV, pp. 57–64, 2011.\nDiegues, N., and Romano, P.\nSelf-tuning intel transac-\ntional synchronization extensions. In 11th International\nConference on Autonomic Computing ({ICAC} 14), pp.\n209–219, 2014.\nDoya, K. Reinforcement learning in continuous time and\nspace. Neural computation, 12(1):219–245, 2000.\nFarahnakian, F., Liljeberg, P., and Plosila, J. Energy-efﬁcient\nvirtual machines consolidation in cloud data centers us-\ning reinforcement learning. In 2014 22nd Euromicro\nInternational Conference on Parallel, Distributed, and\nNetwork-Based Processing, pp. 500–507. IEEE, 2014.\nA View on Deep Reinforcement Learning in System Optimization\nGauci, J., Conti, E., Liang, Y., Virochsiri, K., He, Y., Kaden,\nZ., Narayanan, V., and Ye, X. Horizon: Facebook’s open\nsource applied reinforcement learning platform. arXiv\npreprint arXiv:1811.00260, 2018.\nGraves, A., Mohamed, A.-r., and Hinton, G. Speech recog-\nnition with deep recurrent neural networks. In 2013 IEEE\ninternational conference on acoustics, speech and signal\nprocessing, pp. 6645–6649. IEEE, 2013.\nGreensmith, E., Bartlett, P. L., and Baxter, J. Variance reduc-\ntion techniques for gradient estimates in reinforcement\nlearning. Journal of Machine Learning Research, 5(Nov):\n1471–1530, 2004.\nGuo, X., Singh, S., Lee, H., Lewis, R. L., and Wang, X.\nDeep learning for real-time atari game play using ofﬂine\nmonte-carlo tree search planning. In Advances in neural\ninformation processing systems, pp. 3338–3346, 2014.\nGuu, K., Pasupat, P., Liu, E. Z., and Liang, P.\nFrom\nlanguage to programs: Bridging reinforcement learn-\ning and maximum marginal likelihood. arXiv preprint\narXiv:1704.07926, 2017.\nHaarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft\nactor-critic: Off-policy maximum entropy deep reinforce-\nment learning with a stochastic actor. arXiv preprint\narXiv:1801.01290, 2018.\nHameed, A., Khoshkbarforoushha, A., Ranjan, R., Jayara-\nman, P. P., Kolodziej, J., Balaji, P., Zeadally, S., Malluhi,\nQ. M., Tziritas, N., Vishnu, A., et al. A survey and taxon-\nomy on energy efﬁcient resource allocation techniques for\ncloud computing systems. Computing, 98(7):751–774,\n2016.\nHe, Y., Yu, F. R., Zhao, N., Leung, V. C., and Yin, H.\nSoftware-deﬁned networks with mobile edge computing\nand caching for smart cities: A big data deep reinforce-\nment learning approach. IEEE Communications Maga-\nzine, 55(12):31–37, 2017a.\nHe, Y., Zhao, N., and Yin, H.\nIntegrated networking,\ncaching, and computing for connected vehicles: A deep\nreinforcement learning approach. IEEE Transactions on\nVehicular Technology, 67(1):44–55, 2017b.\nHester, T., Vecerik, M., Pietquin, O., Lanctot, M., Schaul,\nT., Piot, B., Horgan, D., Quan, J., Sendonaris, A., Osband,\nI., et al. Deep q-learning from demonstrations. In Thirty-\nSecond AAAI Conference on Artiﬁcial Intelligence, 2018.\nHuang, Q., Haj-Ali, A., Moses, W., Xiang, J., Stoica, I.,\nAsanovic, K., and Wawrzynek, J. Autophase: Compiler\nphase-ordering for hls with deep reinforcement learn-\ning. In 2019 IEEE 27th Annual International Symposium\non Field-Programmable Custom Computing Machines\n(FCCM), pp. 308–308. IEEE, 2019.\nIpek, E., Mutlu, O., Mart´ınez, J. F., and Caruana, R. Self-\noptimizing memory controllers: A reinforcement learning\napproach. In ACM SIGARCH Computer Architecture\nNews, volume 36, pp. 39–50. IEEE Computer Society,\n2008.\nJamshidi, P., Shariﬂoo, A. M., Pahl, C., Metzger, A., and\nEstrada, G. Self-learning cloud controllers: Fuzzy q-\nlearning for knowledge evolution. In 2015 International\nConference on Cloud and Autonomic Computing, pp. 208–\n211. IEEE, 2015.\nJay, N., Rotman, N., Godfrey, B., Schapira, M., and Tamar,\nA. A deep reinforcement learning perspective on inter-\nnet congestion control. In International Conference on\nMachine Learning, pp. 3050–3059, 2019.\nJin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M. I. Is\nq-learning provably efﬁcient?\nIn Advances in Neural\nInformation Processing Systems, pp. 4863–4873, 2018.\nKaelbling, L. P., Littman, M. L., and Moore, A. W. Rein-\nforcement learning: A survey. volume 4, pp. 237–285,\n1996.\nKober, J., Bagnell, J. A., and Peters, J. Reinforcement\nlearning in robotics: A survey. The International Journal\nof Robotics Research, 32(11):1238–1274, 2013.\nKrauter, K., Buyya, R., and Maheswaran, M. A taxonomy\nand survey of grid resource management systems for dis-\ntributed computing. Software: Practice and Experience,\n32(2):135–164, 2002.\nKrishnan, S., Yang, Z., Goldberg, K., Hellerstein, J., and\nStoica, I. Learning to optimize join queries with deep\nreinforcement learning. arXiv preprint arXiv:1808.03196,\n2018.\nKuhnle, A., Schaarschmidt, M., and Fricke, K. Tensor-\nforce: a tensorﬂow library for applied reinforcement\nlearning. Web page, 2017. URL https://github.\ncom/tensorforce/tensorforce.\nKulkarni, S., and Cavazos, J. Mitigating the compiler op-\ntimization phase-ordering problem using machine learn-\ning. In ACM SIGPLAN Notices, volume 47, pp. 147–162.\nACM, 2012.\nLagoudakis, M. G., and Littman, M. L. Algorithm selection\nusing reinforcement learning. In ICML, pp. 511–518.\nCiteseer, 2000.\nLi, W., Zhou, F., Meleis, W., and Chowdhury, K. Learning-\nbased and data-driven tcp design for memory-constrained\niot. In 2016 International Conference on Distributed\nComputing in Sensor Systems (DCOSS), pp. 199–205.\nIEEE, 2016.\nA View on Deep Reinforcement Learning in System Optimization\nLiang, C., Berant, J., Le, Q., Forbus, K. D., and Lao, N.\nNeural symbolic machines: Learning semantic parsers\non freebase with weak supervision.\narXiv preprint\narXiv:1611.00020, 2016.\nLiang, E., Liaw, R., Nishihara, R., Moritz, P., Fox, R., Gon-\nzalez, J., Goldberg, K., and Stoica, I. Ray rllib: A com-\nposable and scalable reinforcement learning library. arXiv\npreprint arXiv:1712.09381, 2017.\nLiang, E., Zhu, H., Jin, X., and Stoica, I. Neural packet\nclassiﬁcation. arXiv preprint arXiv:1902.10319, 2019.\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,\nT., Tassa, Y., Silver, D., and Wierstra, D. Continuous\ncontrol with deep reinforcement learning. arXiv preprint\narXiv:1509.02971, 2015.\nLin, L.-J. Reinforcement learning for robots using neural\nnetworks. 1993.\nLittman, M., and Boyan, J. A distributed reinforcement\nlearning scheme for network routing. In Proceedings\nof the international workshop on applications of neural\nnetworks to telecommunications, pp. 55–61. Psychology\nPress, 2013.\nLiu, N., Li, Z., Xu, J., Xu, Z., Lin, S., Qiu, Q., Tang, J., and\nWang, Y. A hierarchical framework of cloud resource allo-\ncation and power management using deep reinforcement\nlearning. In 2017 IEEE 37th International Conference on\nDistributed Computing Systems (ICDCS), pp. 372–382.\nIEEE, 2017.\nLuong, N. C., Hoang, D. T., Gong, S., Niyato, D., Wang, P.,\nLiang, Y.-C., and Kim, D. I. Applications of deep rein-\nforcement learning in communications and networking:\nA survey. IEEE Communications Surveys & Tutorials,\n2019.\nMahdavinejad, M. S., Rezvan, M., Barekatain, M., Adibi,\nP., Barnaghi, P., and Sheth, A. P. Machine learning for\ninternet of things data analysis: A survey. Digital Com-\nmunications and Networks, 4(3):161–175, 2018.\nMao, H., Alizadeh, M., Menache, I., and Kandula, S. Re-\nsource management with deep reinforcement learning. In\nProceedings of the 15th ACM Workshop on Hot Topics in\nNetworks, pp. 50–56. ACM, 2016.\nMao, H., Negi, P., Narayan, A., Wang, H., Yang, J., Wang,\nH., Marcus, R., Addanki, R., Khani, M., He, S., et al.\nPark: An open platform for learning augmented computer\nsystems. 2019.\nMarcus, R., and Papaemmanouil, O. Deep reinforcement\nlearning for join order enumeration. In Proceedings of\nthe First International Workshop on Exploiting Artiﬁcial\nIntelligence Techniques for Data Management, pp. 3.\nACM, 2018.\nMarcus, R., Negi, P., Mao, H., Zhang, C., Alizadeh,\nM., Kraska, T., Papaemmanouil, O., and Tatbul, N.\nNeo:\nA learned query optimizer.\narXiv preprint\narXiv:1904.03711, 2019.\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,\nAntonoglou, I., Wierstra, D., and Riedmiller, M. Playing\natari with deep reinforcement learning. arXiv preprint\narXiv:1312.5602, 2013.\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,\nT., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn-\nchronous methods for deep reinforcement learning. In\nInternational conference on machine learning, pp. 1928–\n1937, 2016.\nMostafavi,\nS.,\nAhmadi,\nF.,\nand\nSarram,\nM.\nA.\nReinforcement-learning-based foresighted task schedul-\ning in cloud computing. arXiv preprint arXiv:1810.04718,\n2018.\nOrtiz, J., Balazinska, M., Gehrke, J., and Keerthi, S. S.\nLearning state representations for query optimization\nwith deep reinforcement learning.\narXiv preprint\narXiv:1803.08604, 2018.\nPaliwal, A., Gimeno, F., Nair, V., Li, Y., Lubin, M., Kohli,\nP., and Vinyals, O. Regal: Transfer learning for fast\noptimization of computation graphs.\narXiv preprint\narXiv:1905.02494, 2019.\nPeled, L., Mannor, S., Weiser, U., and Etsion, Y. Semantic\nlocality and context-based prefetching using reinforce-\nment learning. In 2015 ACM/IEEE 42nd Annual Interna-\ntional Symposium on Computer Architecture (ISCA), pp.\n285–297. IEEE, 2015.\nPeng, Z., Cui, D., Zuo, J., Li, Q., Xu, B., and Lin, W.\nRandom task scheduling scheme based on reinforcement\nlearning in cloud computing. Cluster computing, 18(4):\n1595–1607, 2015.\nPeters, J., Vijayakumar, S., and Schaal, S. Reinforcement\nlearning for humanoid robotics. In Proceedings of the\nthird IEEE-RAS international conference on humanoid\nrobots, pp. 1–20, 2003.\nRao, J., Bu, X., Xu, C.-Z., Wang, L., and Yin, G. Vconf: a\nreinforcement learning approach to virtual machines auto-\nconﬁguration. In Proceedings of the 6th international\nconference on Autonomic computing, pp. 137–146. ACM,\n2009.\nRoss, S., Gordon, G., and Bagnell, D. A reduction of imita-\ntion learning and structured prediction to no-regret online\nA View on Deep Reinforcement Learning in System Optimization\nlearning. In Proceedings of the fourteenth international\nconference on artiﬁcial intelligence and statistics, pp.\n627–635, 2011.\nRuffy, F., Przystupa, M., and Beschastnikh, I. Iroko: A\nframework to prototype reinforcement learning for data\ncenter trafﬁc control. arXiv preprint arXiv:1812.09975,\n2018.\nRummery, G. A., and Niranjan, M. On-line Q-learning\nusing connectionist systems, volume 37. University of\nCambridge, Department of Engineering Cambridge, Eng-\nland, 1994.\nSadeghi, A., Sheikholeslami, F., and Giannakis, G. B. Op-\ntimal and scalable caching for 5g using reinforcement\nlearning of space-time popularities. IEEE Journal of Se-\nlected Topics in Signal Processing, 12(1):180–190, 2017.\nSchaal, S. Learning from demonstration. In Advances in\nneural information processing systems, pp. 1040–1046,\n1997.\nSchaal, S.\nIs imitation learning the route to humanoid\nrobots? Trends in cognitive sciences, 3(6):233–242, 1999.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nKlimov, O. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347, 2017.\nSilva, A. P., Obraczka, K., Burleigh, S., and Hirata, C. M.\nSmart congestion control for delay-and disruption toler-\nant networks. In 2016 13th Annual IEEE International\nConference on Sensing, Communication, and Networking\n(SECON), pp. 1–9. IEEE, 2016.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,\nVan Den Driessche, G., Schrittwieser, J., Antonoglou, I.,\nPanneershelvam, V., Lanctot, M., et al. Mastering the\ngame of go with deep neural networks and tree search.\nnature, 529(7587):484, 2016.\nSutton, R. S., and Barto, A. G. Reinforcement learning: An\nintroduction. MIT press, 2018.\nSutton, R. S., McAllester, D. A., Singh, S. P., and Mansour,\nY. Policy gradient methods for reinforcement learning\nwith function approximation. In Advances in neural in-\nformation processing systems, pp. 1057–1063, 2000.\nTesauro, G., Das, R., and Jong, N. K. Online performance\nmanagement using hybrid reinforcement learning. Pro-\nceedings of SysML, 2006.\nVan Hasselt, H., Guez, A., and Silver, D. Deep reinforce-\nment learning with double q-learning. In Thirtieth AAAI\nconference on artiﬁcial intelligence, 2016.\nWang, Z., and OBoyle, M. Machine learning in compiler\noptimization. Proceedings of the IEEE, 106(11):1879–\n1901, 2018.\nWatkins, C. J., and Dayan, P. Q-learning. Machine learning,\n8(3-4):279–292, 1992.\nXu, C.-Z., Rao, J., and Bu, X. Url: A uniﬁed reinforce-\nment learning approach for autonomic cloud manage-\nment. Journal of Parallel and Distributed Computing, 72\n(2):95–105, 2012.\nXu, Z., Wang, Y., Tang, J., Wang, J., and Gursoy, M. C. A\ndeep reinforcement learning based framework for power-\nefﬁcient resource allocation in cloud rans. In 2017 IEEE\nInternational Conference on Communications (ICC), pp.\n1–6. IEEE, 2017.\nZeppenfeld, J., Bouajila, A., Stechele, W., and Herkersdorf,\nA. Learning classiﬁer tables for autonomic systems on\nchip. GI Jahrestagung (2), 134:771–778, 2008.\nZhong, V., Xiong, C., and Socher, R. Seq2sql: Generating\nstructured queries from natural language using reinforce-\nment learning. arXiv preprint arXiv:1709.00103, 2017.\nZhu, Q., and Yuan, C. A reinforcement learning approach\nto automatic error recovery. In 37th Annual IEEE/IFIP\nInternational Conference on Dependable Systems and\nNetworks (DSN’07), pp. 729–738. IEEE, 2007.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.SY",
    "eess.SY"
  ],
  "published": "2019-08-04",
  "updated": "2019-09-04"
}