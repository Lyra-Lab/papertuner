{
  "id": "http://arxiv.org/abs/1908.02130v1",
  "title": "Deep learning research landscape & roadmap in a nutshell: past, present and future -- Towards deep cortical learning",
  "authors": [
    "Aras R. Dargazany"
  ],
  "abstract": "The past, present and future of deep learning is presented in this work.\nGiven this landscape & roadmap, we predict that deep cortical learning will be\nthe convergence of deep learning & cortical learning which builds an artificial\ncortical column ultimately.",
  "text": "Deep learning research landscape & roadmap in a nutshell:\npast, present and future - Towards deep cortical learning\nAras R. Dargazany\nAugust 7, 2019\nAbstract\nThe past, present and future of deep learning is presented in this work. Given this landscape\n& roadmap, we predict that deep cortical learning will be the convergence of deep learning &\ncortical learning which builds an artiﬁcial cortical column ultimately.\n1\nPast: Deep learning inspirations\nDeep learning horizon, landscape and research roadmap in nutshell is presented in this ﬁgure 1.\nThe historical development and timeline of deep learning & neural network is separately illustrated\nFigure 1: Deep learning research landscape & roadmap: past, present, future.\nThe future is\nhighlighted as deep cortical learning.\nin ﬁgure 2. The Origin of neural nets [WR17] is thoroughly reviewed in terms of the evolutionary\nhistory of deep learning models. Vernon Mountcastle discovery of cortical columns in somatosen-\nsory cortex [Mou97] was a breakthrough in brain science. The big bang was the discovery of Hubel\n& Wiesel of simple cells and complex cell in visual cortex [HW59] which won the Nobel prize for\nthis discovery in 1981. This work was heavily founded on Vernon Mountcastle discovery of cortical\ncolumns in somatosensory cortex [Mou97]. After the discovery of Hubel & Wiesel, Fukushima\nproposed a pattern recognition architecture based on the simple cell and complex cell discovery,\nknown as NeoCognitron [FM82]. In this work, a deep neural network was proposed using simple\ncell layer and complex cell layer repeatedly. In 80s and maybe a bit earlier backpropagation have\nbeen proposed by multiple people but the ﬁrst time it was well-explained and applied for learning\nneural nets was done by Hinton and his colleagues in 1987 [RHW86].\n1\narXiv:1908.02130v1  [cs.NE]  30 Jul 2019\nFigure 2: Neural nets origin, timeline & history made by Favio Vazquez\n2\nPresent: Deep learning by LeCun, Bengio and Hinton\nConvolutional nets was invented by LeCun [LBD+89] which led to deep learning conspiracy which\nalso started by the three founding fathers of the ﬁeld: LeCun, Bengio and Hinton [LBH15]. The\nmain hype in deep learning happened in 2012 when the state-of-the-art result in Imagenet classi-\nﬁcation and TIMIT speech recognition task were dramatically reduced using an end-to-end deep\nconvolutional network [KSH12] and deep belief net [HDY+12].\nThe power of deep learning is scalability and the ability to learn in an end-to-end fashion.\nIn this sense, deep learning architectures are capable of learning big datasets such as Imagenet\n[KSH12, GDG+17] and TIMIT using multiple GPUs in an end-to-end fashion meaning directly\nfrom raw inputs, all the way the desired outputs. Alexnet [KSH12] used two GPUs for Imagenet\nclassiﬁcation which is a very big dataset of images, almost 1.5 million images of size 215x215.\nKaiming He et al. [GDG+17] proposed a highly scalable approach for training on Image using\n256 GPUs for almost an hour which shows an amazingly powerful approach based stochastic\ngradient descent for applying big cluster of GPUs on huge datasets. Very many application domains\nhave been revolutionized using deep learning architectures such as image classiﬁcations [KSH12],\nmachine translation [WSC+16, JSL+16], speech recognition [HDY+12], and robotics [MKS+15].\nThe Nobel Prize in Physiology or Medicine 2014 was given to John O’Keefe, May-Britt Moser\nand Edvard I. Moser “for their discoveries of cells that constitute a positioning system in the\nbrain.” [Bur14].\nThis study of cognitive neuroscience shed light on how the world is repre-\nsented within the brain. Hinton’s Capsule network [SFH17] and Hawkins’ cortical learning al-\ngorithm [HAD11] are highly inspired by this Nobel-prize winning work [Bur14].\n3\nFuture: Brain-plausible deep learning & cortical learning\nalgorithms\nThe main direction and inclination in the deep learning for future is the ability to bridge the gap\nbetween the cortical architecture and deep learning architectures, speciﬁcally convolutional nets.\nIn this quest, Hinton proposed capsule network [SFH17] as an eﬀort to get rid of pooling layers\nand replace it with capsules which are highly inspired bu cortical mini-columns in cortical columns\nand layers and include the location information or pose information of parts.\nAnother important quest in deep learning is understanding the biological root of learning in our\nbrain, speciﬁcally in our cortex. Backpropagation is not biologically inspired and plausible. Hinton\nand the other founding fathers of deep learning have been trying to understand how backprop\n2\nmight be feasible biologically in brain. Feedback alignment [LCTA16] and spike time-dependent\nplasticity or STDP-based backprop [BSR+18] are some of the works which have been done by\nTimothy Lillicrap, Blake Richards, and Hinton in order to model backprop biologically based on\nthe pyramidal neuron in the cortex.\nIn the far future, the main goal should be the merge of two very independent quest to build\ncortical structure in our brain: The ﬁrst one is heavily target by the big and active deep learning\ncommunity; The second one is targeted independently and neuroscientiﬁcally by Numenta and\nGeoﬀHawkins [HAD11]. These people argue that the cortical structure and our neocortex is the\nmain source of our intelligence and for building a true intelligent machine, we should be able to\nreconstruct the cortex and to do so, we should ﬁrst focus more on the cortex and understand what\ncortex is made out of.\n4\nFinale: Deep cortical learning as the merge of deep learning\nand cortical learning\nBy merging deep learning and cortical learning, a very more focused and detailed architectures,\nnamed deep cortical learning might be created. We might be able to understand and reconstruct\nthe cortical structure with much more accuracy and have a better idea what the true intelligence is\nand how artiﬁcial general intelligence or AGI might be reproducible. Deep cortical learning might\nbe the algorithm behind one cortical column in the neocortex.\nReferences\n[BSR+18]\nSergey Bartunov, Adam Santoro, Blake Richards, Luke Marris, Geoﬀrey E Hinton,\nand Timothy Lillicrap. Assessing the scalability of biologically-motivated deep learning\nalgorithms and architectures. In Advances in Neural Information Processing Systems,\npages 9368–9378, 2018.\n[Bur14]\nNeil Burgess. The 2014 nobel prize in physiology or medicine: a spatial model for\ncognitive neuroscience. Neuron, 84(6):1120–1125, 2014.\n[FM82]\nKunihiko Fukushima and Sei Miyake. Neocognitron: A self-organizing neural network\nmodel for a mechanism of visual pattern recognition. In Competition and cooperation\nin neural nets, pages 267–285. Springer, 1982.\n[GDG+17] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo\nKyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch\nsgd: training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.\n[HAD11]\nJeﬀ\nHawkins,\nSubutai\nAhmad,\nand\nD\nDubinsky.\nHierarchi-\ncal\ntemporal\nmemory\nincluding\nhtm\ncortical\nlearning\nalgorithms.\nTechnical\nreport,\nNumenta,\nInc.\nhttp://www.numenta.com/htm-\noverview/education/HTM/CorticalLearningAlgorithms.pdf, 2011.\n[HDY+12] Geoﬀrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep\nJaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep\nneural networks for acoustic modeling in speech recognition: The shared views of four\nresearch groups. IEEE Signal Processing Magazine, 29(6):82–97, 2012.\n[HW59]\nDavid H Hubel and Torsten N Wiesel. Receptive ﬁelds of single neurones in the cat’s\nstriate cortex. The Journal of physiology, 148(3):574–591, 1959.\n[JSL+16]\nMelvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng\nChen, Nikhil Thorat, Fernanda Viégas, Martin Wattenberg, Greg Corrado, et al.\nGoogle’s multilingual neural machine translation system: Enabling zero-shot trans-\nlation. arXiv preprint arXiv:1611.04558, 2016.\n[KSH12]\nAlex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁcation with\ndeep convolutional neural networks.\nIn Advances in neural information processing\nsystems, pages 1097–1105, 2012.\n3\n[LBD+89] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard,\nWayne Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip\ncode recognition. Neural computation, 1(4):541–551, 1989.\n[LBH15]\nYann LeCun, Yoshua Bengio, and Geoﬀrey Hinton.\nDeep learning.\nNature,\n521(7553):436–444, 2015.\n[LCTA16] Timothy P Lillicrap, Daniel Cownden, Douglas B Tweed, and Colin J Akerman. Ran-\ndom synaptic feedback weights support error backpropagation for deep learning. Nature\ncommunications, 7:13276, 2016.\n[MKS+15] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness,\nMarc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg\nOstrovski, et al. Human-level control through deep reinforcement learning. Nature,\n518(7540):529–533, 2015.\n[Mou97]\nVernon B Mountcastle. The columnar organization of the neocortex. Brain: a journal\nof neurology, 120(4):701–722, 1997.\n[RHW86]\nDavid E Rumelhart, Geoﬀrey E Hinton, and Ronald J Williams. Learning representa-\ntions by back-propagating errors. Nature, 323:533–536, 1986.\n[SFH17]\nSara Sabour, Nicholas Frosst, and Geoﬀrey E Hinton. Dynamic routing between cap-\nsules. In Advances in Neural Information Processing Systems, pages 3859–3869, 2017.\n[WR17]\nHaohan Wang and Bhiksha Raj.\nOn the origin of deep learning.\narXiv preprint\narXiv:1702.07800, 2017.\n[WSC+16] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural\nmachine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144, 2016.\n4\n",
  "categories": [
    "cs.NE",
    "cs.LG"
  ],
  "published": "2019-07-30",
  "updated": "2019-07-30"
}