{
  "id": "http://arxiv.org/abs/1908.02974v1",
  "title": "Incremental Reinforcement Learning --- a New Continuous Reinforcement Learning Frame Based on Stochastic Differential Equation methods",
  "authors": [
    "Tianhao Chen",
    "Limei Cheng",
    "Yang Liu",
    "Wenchuan Jia",
    "Shugen Ma"
  ],
  "abstract": "Continuous reinforcement learning such as DDPG and A3C are widely used in\nrobot control and autonomous driving. However, both methods have theoretical\nweaknesses. While DDPG cannot control noises in the control process, A3C does\nnot satisfy the continuity conditions under the Gaussian policy. To address\nthese concerns, we propose a new continues reinforcement learning method based\non stochastic differential equations and we call it Incremental Reinforcement\nLearning (IRL). This method not only guarantees the continuity of actions\nwithin any time interval, but controls the variance of actions in the training\nprocess. In addition, our method does not assume Markov control in agents'\naction control and allows agents to predict scene changes for action selection.\nWith our method, agents no longer passively adapt to the environment. Instead,\nthey positively interact with the environment for maximum rewards.",
  "text": "IEEE TRANSACTIONS ON CYBERNETICS\n1\n“This paper has been submitted to the IEEE transactions on cybernetics.”\n“@2019 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any\ncurrent or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new\ncollective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other\nworks.”\narXiv:1908.02974v1  [cs.LG]  8 Aug 2019\nIEEE TRANSACTIONS ON CYBERNETICS\n2\nIncremental Reinforcement Learning — a New\nContinuous Reinforcement Learning Frame Based\non Stochastic Differential Equation methods\nTianhao Chen, Limei Cheng, Yang Liu, Wenchuan Jia and Shugen Ma, Fellow, IEEE\nAbstract—Continuous reinforcement learning such as DDPG\nand A3C are widely used in robot control and autonomous\ndriving. However, both methods have theoretical weaknesses.\nWhile DDPG cannot control noises in the control process, A3C\ndoes not satisfy the continuity conditions under the Gaussian\npolicy. To address these concerns, we propose a new continues\nreinforcement learning method based on stochastic differential\nequations and we call it Incremental Reinforcement Learning\n(IRL). This method not only guarantees the continuity of actions\nwithin any time interval, but controls the variance of actions in\nthe training process. In addition, our method does not assume\nMarkov control in agents’ action control and allows agents to\npredict scene changes for action selection. With our method,\nagents no longer passively adapt to the environment. Instead, they\npositively interact with the environment for maximum rewards.\nIndex Terms—Reinforcement Learning, Continuous System,\nStochastic Differential Equation, Environment Prediction, Op-\ntimal Control\nI. INTRODUCTION\nD\nEEP reinforcement learning (DRL) is the love child of\ndeep learning (DL) and reinforcement learning (RL).\nThrough extensive data training and pattern recognition, DL\nmethods largely improve the efﬁciency of feature identiﬁca-\ntion. In order to achieve the maximum reward for character-\nizing the environment, the optimal state action function from\nBellman optimal equation are adopted in newer RL methods.\nDRL method combines the perceptive ability of DL with the\ndecision-making ability of RL and thus offers strong versatility\nand direct control from input to output [1]. These end-to-end\nlearning systems have achieved great success in dealing with\ncomplex tasks.\nAs shown in Figure 1, major RL methods are mostly Value\nbased, Policy based and Actor-critic based. Initially, high-\ndimensional input data could not be processed directly due\nto lack of training data and computational power. We rely on\nDeep Neural Networks (DNN) to reduce the dimensionality of\nthe input data for real world tasks. The well-known Q-learning\nalgorithm in RL, proposed by Watkins [2], is a very effective\nlearning algorithm that are widely used in traditional RL [3]–\n[5]. Various RL algorithms are developed from Q-learning\nsuch as Deep Q-network (DQN) algorithm [6], [7] (which\nTianhao Chen, Limei Cheng, Wenchuan Jia and Shugen Ma are with School\nof Mechatronic Engineering and Automation, Shanghai University, Shanghai,\nP. R. China. Shugen Ma is also with Department of Robotics, Ritsumeikan\nUniversity, Japan.\nYang Liu is with American University.\nWenchuan Jia is the corresponding author (e-mail: lovvchris@shu.edu.cn).\ncombines the convolutional network with the traditional Q-\nlearning algorithm in RL) and Deep Dual Q-network (DDQN)\nalgorithm [8].\nAlthough DQN and DDQN have achieved success in solv-\ning high-dimensional RL problems, their actions are based\non discrete time and thus cannot be applied to continuous\ncases. To extend Q-learning to continuous state and motion\nspaces, Gu et al. [9] propose a relatively simple solution\ncalled Normalized Advantage Functions (NAF). Although\nNAF can theoretically perform continuous action output, in\nfact, it is difﬁcult to apply because of its large computational\ncomplexity and training complexity. The Deep Deterministic\nPolicy Gradient (DDPG) based on the Actor-Critic (AC)\nframework [10] uses policy gradient to perform continuous\noperation so it can be applied to broader tasks such as robotic\ncontrol. Nevertheless, DDPG can easily expand rewards and\nrandomness of the policy in complex and noisy situations.\nThe Asynchronous Advantage Actor-Critic (A3C) algorithm\n[11] is widely used in tasks with both discrete and continuous\naction space, and arguably achieves the best results concerns.\nHowever, the delay in policy update leads to instability. The\nTrust Region Policy Optimization (TRPO) [1] improves the\nperformance of DDPG by introducing a surrogate objective\nfunction and a KL divergence constraint, which guarantees\na long-term reward without reductions. The Proximal Policy\nOptimization (PPO) [12] algorithm is built on TRPO algorithm\nbut as a policy gradient algorithm, it has the limitation on the\npolicy gradient algorithm – the parameters are updated along\nthe direction of the policy gradient despite that PPO optimize\nTRPO by modifying the objective function.\nFig. 1. Development of reinforcement learning methods\nIEEE TRANSACTIONS ON CYBERNETICS\n3\nDRL has been widely used in simulation [13], robotic\ncontrol [14], optimization and scheduling [15], [16], video\ngames [17], [18] and robotics [19]–[25]. For example, Zhang\net al. [26] show that the trained depth Q-network system\ncan use external visual observations to perform target contact\nlearning so camera images could be replaced by the network\ncomposite images.\nNevertheless, DRL still has many problems in industrial\napplications. The most important one of RL comes from\nits instability, which is also one of the most fundamental\ndifferences from the traditional adaptive control. The problem\nis related to the agent’s wide-area exploration capability.\nReducing the instability in the RL algorithm while ensuring\nand encouraging agents to explore during the training process\nis one of the biggest challenges in reinforcement learning. It is\nalso considered an inherent weakness of classical continuous\nRL algorithms.\nFor example in DDPG, the action policy at is generally\nat = f θ(st)+Nt, where st is environmental state, θ is the pa-\nrameter, Nt is the white noise or the Uhlenbeck-Ornstein ran-\ndom process. When the Uhlenbeck-Ornstein random process\nis chosen (which expression is dXt = θ (µ −Xt) dt + σdBt,\nθ,µ,σ > 0), despite the continuity condition might be satisﬁed,\nwe cannot guarantee the variance of the action output. When\nexamining the A3C algorithm under the Gaussian strategy, its\naction strategy is generally at = µθ (st) + σθ (st) · Nt, where\nNt is white noise. Although the accuracy can be controlled\naccording to Kolmogorov continuity theorem, A3C’s action\npolicy is not continuous, and it cannot ﬁnd a continuous\ncorrection (Appendix A.1). A3C has to use the policy gra-\ndient principle to update the network parameters. If Nt and\nDDPG adopt the Uhlenbeck-Ornstein stochastic process, the\ndistribution π (at|st) will be very difﬁcult to draw, and it is\nnot suitable to use ∇θJ (θ) = E [∇θπ (at|st) · Q (st, at)] to\nupdate the parameters, where Q (st, at) is the Q function in\nRL. In addition, because of the need for discretizing these\nformulas in practice, the above methods cannot control the\nsize of the time interval. Therefore, when the environmental\nstate is abrupt due to the Markov property, the range of motion\nchanges can reach an unintended situation, which may cause\ndamage to many RL applications such as robotics.\nTraditional RL methods have difﬁculties in balancing the\nuncertainties caused by continuity and variance control. This is\nalso a drawback of the Markov control in RL. Solutions might\nbe found in A3C’s action strategy. In stochastic differential\nequations (SDE) [27], the white noise Nt is often seen as\nthe “derivative” of the Brownian motion Bt [28], so the\ntotal derivative of A3C algorithm can be written as: ηtdt =\nµθ (st) dt + σθ (st) · dBt. This gives a starting point for our\nresearch. Without taking direct action in the Markov controlled\noutput, we focus on incremental changes denoted as dat =\nηtdt. Then we have the SDE: dat = µθ (st) dt+σθ (st)·dBt,\nat ∈Rm. In fact, this expression is quite common in practice.\nFor example, a robot joint is usually controlled by its angle\nrather than the angular velocity. The problem is difﬁcult to\nsolve in traditional RL because the environmental state can\nhardly be predicted and consequently agents can only adapts\nto the environment. If the form of the change st is not clear,\nit is almost impossible to make any effective description of\nthe above SDE. Hence, it is necessary to make corresponding\nestimates of the environmental state.\nIn traditional RL theory, the distribution of the environ-\nmental state at the next moment is based on the current\nenvironmental state and the agent’s action strategy (causing\nenvironmental changes), but in reality the environment state\nevolves continuously. Is it possible to construct a SDE similar\nto action strategy for the environmental state st? The answer\nis\ndst = f θp (st, at) dt + gθp (st, at) · dBt, st ∈Rn\nwhere f : Rn+m →Rn,g : Rn+m →Rn×n are functions\nfor environmental change (the network is used instead in the\nactual process); θp is the parameter set in the equation; Bt\nis the n-dimensional Brownian motion (each component is\nindependent). This SDE shows that the environment is not only\ncontinuously changing, but also depends on the current state of\nthe environment and actions, which satisﬁes both requirements\nfor a ideal environment state dynamic.\nIt is also necessary to modify the strategy followed by the\naction policy,\ndat = µθv (st, at) dt + σθv (st, at) · d eBt, at ∈Rm\nwhere µ : Rn+m →Rm, σ : Rn+m →Rm×m is the\nequation followed by environmental change (the network is\nused instead in the actual process); θp is the parameter set\nin the equation; eBt is the n-dimensional Brownian motion, it\nis also independent of Bt (each component is independent).\nSuch modiﬁcations are reasonable. For example, when a robot\nmakes an action under reinforcement learning, its current\naction needs to be considered for determining the amount of\nchange. When the allowable angle of the joint rotation is large,\nthe natural motion amplitude might be set larger; when it is\nclose to the critical point, the allowable motion change range\nis smaller.\nThe two core SDEs in this paper are therefore:\ndst = f θp (st, at) dt + gθp (st, at) · dBt, st ∈Rn\ndat = µθv (st, at) dt + σθv (st, at) · d eBt, at ∈Rm\nAlthough the two equations are observed separately, it must\nbe a non-time-aligned SDE. If Yt = (st, at), the two equations\ncan be merged into:\ndYt =\n\u0012\nf θp (Yt)\nµθv (Yt)\n\u0013\ndt +\n\u0012\ngθp (Yt)\n0\n0\nσθv (Yt)\n\u0013\n·\n\u0012 dBt\nd eBt\n\u0013\nor:\ndYt = F (θp,θv) (Yt) dt + G(θp,θv) (Yt) · dBt\nwhere Yt ∈Rn+m. Then Yt follows Iˆto diffusion. This has\nbrought great convenience to our discussion and it is also the\ncore concept of this paper. In fact, this is aligned with the\nreality - the action of the agent is always interactive with the\nchanging environment. Besides, the Q function in RL is often\nused to evaluate the future beneﬁt of the current action and\nstate, that is, Qθ(s, a), where θ is the parameter set of the\nQ-value network. This expression can be changed to Qθ (y)\nunder the current framework.\nIEEE TRANSACTIONS ON CYBERNETICS\n4\nIn summary, the entire Incremental Reinforcement learning\nframework consists of three main parts (and its network) (1)\nEnvironmental state estimator (ESE); (2) Action policy genera-\ntor (APG); (3) Value estimator (VE) (or Q function). The rest\nof the paper is arranged as following: Section II introduces\nour incremental reinforcement learning framework; Section\nIII shows how to update the parameters in our proposed\nframe. Section IV describes the experimental results; Section\nV concludes. Proofs for some steps are left in appendix.\nII. THEORY\nThis section consists of four parts: the regulation terms\nderived from the existence and uniqueness of stochastic differ-\nential equations and the description of - ESE, APG and VE.\nMost of the knowledge about stochastic differential equations\nis elaborated in [29] so we do not repeat their proof.\nAs mentioned in the introduction, the entire system of IRL\nis in the form:\ndst = f θp(st, at)dt + gθp(st, at) · dBt, st ∈Rn\n(1)\ndat = µθv(st, at)dt + σθv(st, at) · d ˜Bt, at ∈Rm\n(2)\nwhere f : Rm+n →Rn; µ : Rm+n →Rm; g : Rm+n →\nRn×n; σ : Rm+n →Rm×m are all measuable functions. Sup-\npose Yt = (st, at), then the stochastic differential equations\nof the whole system can be transformed into Iˆto diffusion\nequation:\ndYt = F (θp,θv)(Yt)dt + G(θp,θv)(Yt) · dBt, Yt ∈Rm+n (3)\nwhere Bt =\n\u0012 Bt\n˜Bt\n\u0013\nis a (n+m)-dimensional Brownian\nmotion, and:\nF (θp,θv)(Yt) = F (θp,θv)(st, at) =\n\u0012 f θp(st, at)\nµθv(st, at)\n\u0013\n(4)\nG(θp,θv)(Yt)=G(θp,θv)(st,at)=\n\u0012gθp(st, at)\n0\n0\nσθv(st, at)\n\u0013\n(5)\nAs the basic model of IRL, the discussion of the Iˆto\ndiffusion equation of the above system will run through the\nentire algorithm framework.\nA. The regulation terms derived from the existence and\nuniqueness of stochastic differential equations\nBefore discussing the other three core networks, it is nec-\nessary to discuss the existence and uniqueness of equation\n(3). It is obvious that if the solution of the diffusion equation\ndoes not exist, it would be hard to get a good interaction\nbetween the environment state and agent’s action through\nnetwork training. In order to ensure that the solution not only\nuniquely exist, but also satisfy the continuity condition, we\nuse the existence and uniqueness theorem of the solution to\nthe stochastic differential equation described below:\nLemma 1 Existence and uniqueness of solutions of SDE\nSuppose T\n> 0, b(·, ·) : [0, T] × Rn →Rn, σ(·, ·) :\n[0, T] × Rn →Rn×m are all measuable function. If there\nexists constant C and D that:\n(i)|b(t, x)| + |σ(t, x)| ≤C(1 + |x|), x ∈Rn, t ∈[0, T]\n(ii)|b(t, x) −b(t, y)| + |σ(t, x) −σ(t, y)| ≤D|x −y|,\nx, y ∈Rn, t ∈[0, T]\nwhere |σ|2 = P |σij|2 and Z is a random varible that\nis independent of the σ algebra F(m)\n∞\nand generated by\n{Bs; s ≤t}. It also satisﬁes E[|Z|2] < ∞, then the stochastic\ndifferential equation:\ndXt = b(t, Xt)dt + σ(t, Xt) · dBt\nhas a unique continuous solution Xt(ω), which is adapted\nto the ﬁltration FZ\nt\n:= σ{Z and Bs; s ≤t} and satisﬁes\nE[\nR T\n0 |Xt|2dt] < ∞.\nAcording to lemma 1, it is obvious that if the solution of\nthe stochastic differential equation (3) exists, the functions\nF (θp,θv)(y), G(θp,θv)(y) have to satisfy\n(i)|F (θp,θv)(y)| + |G(θp,θv)(y)| ≤C(1 + |y|), y ∈Rm+n,\nt ∈[0, T]\n(ii)|F (θp,θv)(x) −F (θp,θv)(y)| + |G(θp,θv)(x) −G(θp,θv)(y)|\n≤D|x −y|, x, y ∈Rm+n, t ∈[0, T]\nThe condition (i) is automatically satisﬁed because process\nYt is a diffusion process. For condition (ii), it means that\nF (θp,θv) and G(θp,θv) have to satisfy Lipschitz continuous\ncondition, (f θp, gθp, µθv and σθv). It also means that it is\nnecessary to limit the gradient of the parameters (θp, θv), so\na gradient penalty should be added in the objective functions\nas a regularizer. Hence, two regularizers are constructed\nJLip(θp) = λ1(∥∂f θp(s, a)∥2 + ∥∂gθp(s, a)∥2 −D1)+\n(6)\nJLip(θv) = λ2(∥∂µθv(s, a)∥2 + ∥∂σθv(s, a)∥2 −D2)+ (7)\nwhere the constants D1, D2 are the upper bounds of the\ngradients of the input pair of the network in ESE and APG;\nλ1, λ2 are coefﬁcients for regularizers; ∂is the differential\noperator (or divergence operator); ∥· ∥2 is 2-norm; and\n(x)+ = max(0, x) = ReLu(x). In practice, we often do\nsegmented trainings and collect parameters as well as gradient\nvalues associated with training date in each segment. If the\nsampling interval is ∆t, training period T = l∆t, sk = sk∆t,\nak = ak∆t, then the regularizers could be rewritten as :\nJLip(θp)=λ1\nl−1\nX\nk=0\n(∥∂f θp(s, a)∥2+∥∂gθp(s, a)∥2−D1)+ (8)\nJLip(θv)=λ2\nl−1\nX\nk=0\n(∥∂µθv(s, a)∥2+∥∂σθv(s, a)∥2−D2)+ (9)\nThese two regulation terms are added in the objective\nfunction of θp and θv respectively.\nIEEE TRANSACTIONS ON CYBERNETICS\n5\nB. Value Estimator(VE)\nWe introduce the value estimator VE ﬁrst since it is the\ncore of reinforcement learning. Agents base on the Q function\nin VE to form expectations on future reward under current\nenvironment state and its current action policy. Note that\nthe classical, discretized Q function under optimal parameters\nshould satisfy:\nQθ∗(sk, ak) = E(sk,ak)[Rk + γQθ∗(sk+1, ak+1)]\n(10)\nwhere E(sk,ak)[·] is the conditional expectation, which is the\nexpectation under known conditions (sk, ak), and θ∗is the op-\ntimal parameters. It is obvious that random varible Qθ∗(sk, ak)\nsatisiﬁes Markov property in discrete time. Qθ∗(s, a) is there-\nfore recursively determined by\nQθ∗(s, a) = Qθ∗(s0, a0)\n= E(s,a)[R0 + γQθ∗(s1, a1)]\n= E(s,a)[R0 + γE(s1,a1)[R1 + γQθ∗(s2, a2)]]\n= E(s,a)[R0 + γE(s,a)[R1 + γQθ∗(s2, a2)|N1]]\n= E(s,a)[R0 + γR1 + γ2Qθ∗(s2, a2)]\nwhere Nk represents the σ algebra which is deﬁned by\n{(sl, al)}l≤k; Rk is the reward at the k moment. So by\nrecursive substitution, it is easy to ﬁnd that at k moment,\nQθ∗(s, a) = E(s,a)\n\"k−1\nX\ni=0\nγiRi + γkQθ∗(sk, ak)\n#\n(11)\nThis constraint for Q function in discrete cases can be easily\nextended to the continuous case. If the time interval is ∆t, the\nreward in the k∆t moment is Rk, the discrete time k can be\nreplaced by the continuous time t:\nQθ∗(s, a) = E(s,a)\n\u0014Z t\n0\nγsrsdt + γtQθ∗(st, at)\n\u0015\n(12)\nwhere rs is the rate of the change of the reward. This equation\nis the constraint for Q function of VE in IRL. Now it raises\ntwo questions for Q function:\n• Does a Q function that satisﬁes the constraint equation\n(12) exist?\n• If such a Q function exists, how do we update the parame-\nters of the network to approximate the optimal parameters\nand simultaneously satisfy the constraint equation(12)\nduring the training process?\nBefore answering these two questions, it is necessary to\nmake some changes to the constraint equation (12). Let take\nthe derivative of Q function with respect to t and make the\nﬁrst order condition equal to 0:\n0= ∂\n∂tQθ∗(s, a)= ∂\n∂tE(s,a)\n\u0014Z t\n0\nγsrsdt+γtQθ∗(st,at)\n\u0015\n(13)\nSince the actual process\nR t\n0 γsrsdt + γtQθ∗(st, at) is always\nbounded, dominated convergence theorem allows us to substi-\ntute the partial operator with expectation operator, so\n0 = E(s,a)\n\u0014 ∂\n∂t\n\u0012Z t\n0\nγsrsdt + γtQθ∗(st, at)\n\u0013\u0015\n= E(s,a)[γtrt + lnγ · γtQθ∗(st, at) + γt ∂\n∂tQθ∗(st, at)]\n= E(s,a)[rt + lnγ · Qθ∗(st, at) + ∂\n∂tQθ∗(st, at)]\nBy setting y\n=\n(s, a), Yt\n=\n(st, at) and q(y, t)\n=\nEy[Qθ∗(Yt)], the following formula is obtained:\nE(s,a)[rt] + lnγ · q(y, t) + ∂\n∂tq(y, t) = 0\n(14)\nThe question is how to deal with the partial derivative of\nq(y, t), or E(s,a)[Qθ∗(st, at)] with respect to time. Fortunately,\nit can be solved by using the theory of stochastic differential\nequations. In the following we show the deﬁnition of the\ncharacteristic operator of stochastic differential equations.\nDeﬁnition. Characteristic Operators of Iˆto Diffusion and\nTheir Expressions\nSuppose {Xt} is a Iˆto diffusion process on Rn, the generator\nof Xt can be deﬁned as:\nAf(x) = lim\nt↓0\nEx[f(Xt)] −f(x)\nt\n, x ∈Rn\n(15)\nSo, the set of all functions f that have the upper limit at x\ncan be deﬁned as DA(x) where DA is the set of functions\nwhose upper limits exist when f ∈C2, f ∈DA. DA is\nalso called characteristic operator. These two operators are\nequivalent when f ∈C2.\nSuppose Xx\nt\nis a Iˆto diffusion process with an initial\ncondition Xx\n0 = x on Rn, so\nXx\nt (ω) = x +\nZ t\n0\nu(s, ω)ds +\nZ t\n0\nv(s, ω) · dBs(ω)\n(16)\nSuppose f ∈C2\n0(Rn), τ is a stopping time related to F(n)\nt\nwith the condition Ex[τ] < ∞, and Xx\nt is bounded on the\nsupport set of f, then\nEx[f(Xτ)]=f(x)+Ex\n\"Z τ\n0\n X\ni\nui(s, ω) ∂f\n∂xi\n(Ys)\n!\nds\n+1\n2\nZ τ\n0\n\nX\ni,j\n(vvT )ij(s, ω)\n∂2f\n∂xi∂xj\n(Ys)\n\nds\n\n\n(17)\nand the characteristic operator of Iˆto diffusion process A can\nbe deﬁned as:\nA =\nX\ni\nui(x) ∂\n∂xi\n+ 1\n2\nX\ni,j\n(vvT )ij(x)\n∂2\n∂xi∂xj\n(18)\nwhich is a second order linear partial differential operator.\nThe\nnext\nlemma\nshows\nthe\nrelationship\nbetween\n∂\n∂tEx[f(Xt)] and the characteristic operator.\nLemma 2 Kolmogorov backward equation\nIEEE TRANSACTIONS ON CYBERNETICS\n6\nSuppose f ∈C2\n0(Rn), deﬁne\nu(t, x) = Ex[f(x)]\n(19)\n(i) If for every t, u(t, ·) ∈DA, then\n∂u\n∂t = Au, t > 0, x ∈Rn\n(20)\nu(0, x) = f(x), x ∈Rn\n(21)\n(ii) If there exists a w(t, x) ∈C1,2(R×Rn) that satisﬁes two\nconditions above, we have\nw(t, x) = u(t, x) = Ex[f(Xt)].\n(22)\nThrough\napplying\nthe\nlemma\n2,\nKolmogorov\nback-\nward equation, to Q function, it is possible to calculate\n∂\n∂tEy[Qθ∗(Yt)]. From the diffusion equation of Yt, the char-\nacteristic operator AY of Yt can be written as\nAY =\nm+n\nX\ni=1\nF (θp,θv)\ni\n∂\n∂yi\n+ 1\n2\nm+n\nX\ni,j=1\n(GGT )(θp,θv)\nij\n∂2\n∂yi∂yj\n(23)\nor\nAY = F (θp,θv) · ∂+ 1\n2(GGT )(θp,θv) ∗∂2\n(24)\nwhere ∗is the operator that A ∗B = P\nij Aij · Bij. Let\nconsider\nq(y, t) = Ey[Qθ∗(Yt)] = E(s,a)[Qθ∗(st, at)]\n(25)\nAccording to Kolmogorov backward equation, it is obvious\nthat\n∂\n∂tq(y,t)= AY q(y, t)\n=\nm+n\nX\ni=1\nF\n(θp,θv)\ni\n(y) ∂q\n∂yi (y, t)\n+ 1\n2\nm+n\nX\ni,j=1\n(GGT )\n(θp,θv)\nij\n(y)\n∂2q\n∂yi∂yj (y, t)\n=F (θp,θv)(y)·∂yq(y,t)+ 1\n2(GGT)(θp,θv)(y)∗∂2\nyq(y,t)\nCombined with eq(14), we get the constraint equation of\nq(y, t):\nE(s,a)[rt] + lnγ · q(y, t) +\nm+n\nX\ni=1\nF (θp,θv)\ni\n(y) ∂q\n∂yi\n(y, t)\n+ 1\n2\nm+n\nX\ni,j=1\n(GGT )(θp,θv)\nij\n(y)\n∂2q\n∂yi∂yj\n(y, t) = 0\n(26)\nor\nE(s,a)[rt] + lnγ · q(y, t) + F (θp,θv)(y) · ∂yq(y, t)\n+ 1\n2(GGT )(θp,θv)(y) ∗∂2\nyq(y, t) = 0\n(27)\nA new second order elliptic partial differential operator\nappears as following\nLY =lnγ +AY =lnγ+F (θp,θv)·∂y+1\n2(GGT )(θp,θv)∗∂2\ny (28)\nAnd eq(27) can be rewritten as\nLY q(y, t) = −Ey[rt]\n(29)\nThis formula indicates that if Q function exists, q(y, t) could\nbe the solution of a class of second order elliptic partial\ndifferential equations. In other words, IRL is related to partial\ndifferential equation theory.\nThe next step is eliminating the inﬂuence of the time, we\ndiscuss the case for t = 0. By setting\nq(y) = q(y, 0) = Qθ∗(y) = Qθ∗(s, a)\n(30)\nfrom the initial formula eq(12) we get\nq(y) = Ey\n\u0014Z t\n0\nγsrsdt\n\u0015\n+ γtq(y, t)\n(31)\nThen apply operator LY to both sides of the equation:\nLY q(y) = LY Ey\n\u0014Z t\n0\nγsRsds\n\u0015\n+ γtLY q(y, t)\n= LY Ey\n\u0014Z t\n0\nγsRsds\n\u0015\n−γtEy[rt]\nThe left side of the equation doesn’t contain time variable, so\n∂\n∂t\n\u001a\nLY Ey\n\u0014Z t\n0\nγsRsdt\n\u0015\n−γtEy[rt]\n\u001b\n= 0\n(32)\nwhich means the right side of the equation only relate to y,\nwritten as Φ(y)(that is also the −Ey[r0] = −Ey[r]). Now we\nget a partial differential equation of q(y), Qθ∗(s, a) and such\na Q function dose exist according to the partial differential\nequation theory. (Actually it is a Dirichlet-Poisson question.)\nHowever, if we want to use PDE theory, it is necessary to\ndiscuss the boundary condition of the Q function. Fortunately\nin practice, agent’s action policies are generally bounded. It is\nhard to know the exact boundary conditions of environment\nstates, but they are bounded in a large enough area. We write\nthe domain of Qθ∗(s, a) as I(s, a) = I(y), and\nq(y) = 0 on Rm+n −I(y) and ∂I(y)\n(33)\nwhere ∂I(y) is the boundary of I(y).\nNow we have two equations:\nLY q(y) = Φ(y), y ∈I(y)\n(34)\nq(y) = 0, y ∈∂I(y)\n(35)\nwhere LY is a second-order elliptic partial differential op-\nerator. This is a typical Dirichlet question. Here we only\ndemonstrate the main theorem of the existence and uniqueness\ntheorem of the solution to Dirichlet question. Detailed proof\ncan be found in [30].\nLemma 3 Existence and uniqueness of the solution to Dirich-\nlet question\nSuppose\nL =\nX\ni,j\naij(x)\n∂2\n∂xi∂xj\n+\nX\ni\nbi(x) ∂\n∂xi\n+ c(x)\nis strictly elliptical in a bounded area Ω, and there exists a\npositive number λ such that\nX\ni,j\naij(x)ξiξj ≥λ∥ξ∥2\n2, ∀x ∈Ω, ξ ∈Rn\nIEEE TRANSACTIONS ON CYBERNETICS\n7\nof c(x) < 0. If f and the coefﬁcient function of L belong to\nCα(Ω), (which means they have α-order H¨older continuity)\nΩis a C2,α area and φ ∈C2,α(Ω), then Dirichlet question\nLu = f, in Ω\n; u = φ on ∂Ω\nhas a unique solution that belongs to C2,α(Ω).\nIn practice, when lnγ < 0, we might consider Φ(y) is at\nleast second order continuous and α-order H¨older continuous.\nThis gives the existence and uniqueness of the Q-function.\nHowever, if Q function exists, it must be at least second order\ncontinuous, which means any nonlinear functions that are not\nsecond order continuous (such as ReLU) cannot be used in\nthe nonlinear function layers in the networks. On the other\nhand, sigmoid, which is both bounded and C∞continuous, is\napparently a better choice.\nOnce Q function exists, we can use optimization methods\nsuch as gradient descent, SGD, Adam algorithm to optimize\nJQ(θ) = (LY Qθ(s, a) + Ey[r])2,\n(36)\nand derive the optimal parameters θ∗. In details,\nJQ(θ)=\n \nlnγ · Qθ(s, a)+\nm+n\nX\ni=1\nF (θp,θv)\ni\n(s, a)∂Qθ\n∂yi\n(s, a)\n+1\n2\nm+n\nX\ni,j=1\n(GGT)(θp,θv)\nij\n(s, a) ∂2Qθ\n∂yi∂yj\n(s,a)+Ey[r]\n\n\n2\n.\n(37)\nThis is the objective function used to update the network.\nBecause it satisﬁes the Markov property, we can also use the\ntraining data in each step (sk, ak) of the process as the initial\nvalue. Suppose the number of training samples in one step is\nl, then:\nJQ(θ) =\nl\nX\nk=0\n\u0010\nE(sk,ak)[rk]+ lnγ·Qθ(sk, ak)\n+1\n2\nm+n\nX\ni,j=1\n(GGT )(θp,θv)\nij\n(sk,ak) ∂2Qθ\n∂yi∂yj\n(sk,ak)\n+\nm+n\nX\ni=1\nF (θp,θv)\ni\n(sk,ak)∂Qθ\n∂yi\n(sk,ak)\n!2\n(38)\nIn practice, it is difﬁcult to solve such a partial differential\nequation with uncertain parameters by training the network\nthrough gradient descent, and the gradient can easily get too\nlarge and impede training preceeding. This can be alleviated by\nadding an additional objective function. Recall the Q function\nin equation (12). During the training process, we record the\nnext state (s′\nk, a′\nk) and the above Q function can be written in\ndiscrete time:\nQθ∗(sk, ak) ≈E(sk,ak) [rk∆t] + γ∆tQθ∗(s′\nk, a′\nk)\n(40)\nThen we get an additional objective function:\nJ′\nQ(θ) =\nl\nX\nk=0\n(Qθ(sk, ak) −rk∆t −γ∆tQθ(s′\nk, a′\nk))2\n(41)\nSince the integration is discretizated, this objective function\nis less precise than the one with second-order gradient terms\nthat we obtained earlier. However, this objective function is\nmore intuitive and its convergence speed can be accelerated\nin training.\nC. Environment State Estimator(ESE)\nESE is not necessary in classical reinforcement learning\nalgorithms such as A3C and DDPG. However in IRL, ESE\nis essential to make (st, at) Iˆto diffusion. Note that there is a\nfundamental difference bewteen θp and (θ, θv): θp comes from\nthe state change of the environment itself, so this parameter is\nobjective; (θ, θv) are related to rewards set by human, so they\nare subjective. The difference makes ESE network updating\nprocess in IRL a process of parameter estimation. Now we\nraise two theorems as the starting point of the ESE update\nprocess.\nLemma 4 Girsanov Theorem and Likelihood Function\nSuppose\nY (t) = ζ(t, ω)dt + ζ(t, ω) · dBt,\nwhere t <= T, β(t, ω) ∈Rn, ζ(t, ω) ∈Rn×m. Suppose\nthere exist processes u(t, ω) ∈Wm\nH , α(t, ω) ∈Wm\nH such that\nζ(t, ω)u(t, ω) = β(t, ω) −α(t, ω)\nand assume that u(t, ω) satisﬁes Novikov’s condition\nE\n\"\nexp\n \n1\n2\nZ T\n0\nu2(s, ω)ds\n!#\n< ∞.\nIf\nMt =exp\n\u0012\n−\nZ t\n0\nu(s, ω) · dBs−1\n2\nZ t\n0\n∥u(s, ω)∥2\n2ds\n\u0013\n, t<=T\nand\ndQ(ω) = MT (ω)dP(ω)\non F(m)\nT\n,\nthen\nˆBt :=\nZ t\n0\nu(s, ω)ds + B(t)\nis a Brownian motion w.r.t. Q and the process Y (t) has the\nstochastic integral representation\nY (t) = α(t, ω)dt + ζ(t, ω)d ˆBt.\nConsider the Iˆto difussion dXt = aθ(Xt)dt + bθ(Xt) · dBt,\napplying Girsanov Theorem conditional on b is reversible and\nα = 0, then\nuθ(Xt) = u(Xt, θ) = (b−1)θ(Xt) · aθ(Xt)\nand the log-likelihood function is deﬁned as:\nlT (θ) = lndQ(ω)\ndP(ω) = lnM θ\nT (ω)\nthat is\nlT (θ) = −\nZ t\n0\nu(s, ω) · dBs −1\n2\nZ t\n0\n∥u(s, ω)∥2\n2ds\n(42)\nIEEE TRANSACTIONS ON CYBERNETICS\n8\nRecall that in IRL, the difussion process Yt = (st, at) is\ndYt = F (θp,θv)(Yt)dt + G(θp,θv)(Yt) · dBt, Yt ∈Rm+n.\nAccording to lemma 4,\nu(Yt, θp) = uθp(Yt) = (G−1)(θp,θv)(Yt) · F (θp,θv)(Yt) (43)\nwhere u(Yt, θp) ∈Rn+m, θv is a constant parameter set that\ndoes not engage in ESE parameters update process. Then, the\nlog-likelihood function lT (θp) can be deﬁned as:\nlT (θp) = −\nZ t\n0\nu(Ys, θp) · dBs −1\n2\nZ t\n0\n∥u(Ys, θp)∥2\n2ds (44)\nThe next lemma shows how to use this log-likelihood function\nto estimate parameter θp. The detail of the proof can be found\nin [29].\nLemma 5 Likelihood Functions for θp Estimation\nSuppose Xt ∈Rn is a stationary and ergodic process, and\ndXt = aθ(Xt)dt + bθ(Xt) · dBt\nuθ(x) = u(x, θ) = (b−1)θ(x) · aθ(x)\nPθ is a probability measure obtained in the Girsanov theorem.\nlT (θ) is the log-likelihood function. If the conditions below\nhold:\n(A1) Pθ ̸= Pθ′, while θ is the true value and θ ̸= θ′;\n(A2) Pθ\n\u0010nR T\n0 ∥u(Xs, θ)∥2\n2ds < ∞\no\u0011\n= 1, ∀T > 0, while θ\nis the true value;\n(A3) Suppose I(θ) is the domain of θ of u(x, θ), then for\nr ∈I(θ),\n|u(x, r)| ≤M(x), Eθ[M(X0)]2 < ∞\n|∂su(x, r)| ≤Q(x), Eθ[Q(X0)]2 < ∞\nwhere Eθ is the expectation based on Pθ; | · | is some norm;\n(A4) ∀r ∈I(θ), Eθ\nhR T\n0 ∥u(Xs, r)∥2\n2ds\ni\n< ∞.\nThen\n(1) The solution ˆθT of equation l′\nT (θ) = 0 satisﬁes\nˆθT\na.s.\n−−→θ when T →∞\n(45)\n(2) When T →∞,\n√\nT(ˆθT −θ)\na.s.\n−−→N\n\u0010\n0, (Eθ [∂θu(X0, r)])−1\u0011\n(46)\nThis theorem gives the error distribution of\n√\nT(ˆθT −θ)\nwhen T approaches inﬁnity. The ergodicity of stationary states\nthe time average of a stochastic process (the mean of the\nintegral of time) equivalent to the space average (expectation),\n1\nT\nZ T\n0\nf(Xt)dt\na.s.\n−−→E[f(ξ)]\nf is a measurable function which satisﬁes E[f(ξ)] < ∞,\nwhere ξ is a random variable under a unknown distribution.\nGenerally, if Xt is a solution to a certain stochastic differential\nequation, the ergodicity can be satisﬁed by adding constraints\nto the coefﬁcients of the equation. If there are special require-\nments, regularizer can be added to the network to force it\nto meet the requirements. But in practice, it is not necessary\nbecause our approximation is always limited in time. We only\nneed to ensure the corresponding accuracy. Condition A1 is\nalways satisﬁed, and for other conditions, if we can guarantee\nthat u(y, θ) ∈C2\n0, they will be met automatically. Since\nthe input and output of the network are always bounded, we\nonly need to ensure that the non-linear layers of the network\nstructure are at least second order continuous. Unfortunately,\nReLU was once again disqualiﬁed under these conditions,\nwhile some C∞nonlinear function such as sigmoid meet all\nrequirements. From lemma 5, we can get the objective function\nof ESE, that is, the discretization of eq(45) and (46).\nu((sk, ak), θp)=uθp(sk, ak)\n=(G−1)(θp,θv)(sk,ak)·F (θp,θv)(sk,ak)\n(47)\nlT (θp)=−\n1\nX\nk=0\nuθp(sk, ak)·∆Bk−∆t\n2\nl\nX\nk=0\n∥uθp(sk, ak)∥2\n2 (48)\nwhere ∆Bk = Bk+1 −Bk (which is a main property of Iˆto\nintegral). From the properties of Brownian motion we can get\n∆Bk ∼N(0, ∆t). The random variables are eliminated after\ntaking expected values. Then the objective function of ESE\nbecomes:\nJE(θp) = ∆t\n2\nl\nX\nk=0\n∥uθp(sk, ak)∥2\n2\nBecause only θp is contained in the objective function, it\nis apparent that it is possible to eliminate some items which\nonly contain θv to reduce computation load. Since\nF (θp,θv)(sk, ak) =\n\u0012 f θp(sk, ak)\nµθv(sk, ak)\n\u0013\nG(θp,θv)(sk, ak) =\n\u0012\ngθp(sk, ak)\n0\n0\nσθv(sk, ak)\n\u0013\nthe uθp(sk, ak) has the form:\nuθp(sk, ak) =\n\u0012 (g−1)θp(sk, ak) · f θp(sk, ak)\n(σ−1)θv(sk, ak) · µθv(sk, ak)\n\u0013\n(49)\nwhich means θv can be eliminated during the update process\nof θp. Finally,\nuθp(sk, ak) = (g−1)θp(sk, ak) · f θp(sk, ak)\n(50)\nJE(θp) = ∆t\n2\nl\nX\nk=0\n∥uθp(sk, ak)∥2\n2\n(51)\nIn addition, we can control the accuracy of the error. From\nthe second conclusion of lemma 5, we can see that if the time\nperiod is long enough,\n√\nl∆t(θp −θ∗\np) ∼N\n\u0010\n0,\n\u0000Eθ\n\u0002\n∂θuθp(s0, a0)\n\u0003\u0001−1\u0011\n(52)\nwhere θ∗\np is the truth value. Generally, (s0, a0) in the control\nprocess can be approximated by ﬁxed endpoints (s, a), then\n√\nl∆t(θp −θ∗\np) ∼N\n\u0010\n0,\n\u0000∂θuθp(s, a)\n\u0001−1\u0011\n.\n(53)\nBy setting a reasonable initial value and a long enough\nruntime, the error of the estimated parameters can be properly\ncontrolled.\nIEEE TRANSACTIONS ON CYBERNETICS\n9\nOn the other hand, similar to previous value estimators,\nthe environmental state estimator introduces an additional\nobjective function to estimate the environmental state. Similar\nto our previous discussion, we record the next state (s′\nk, a′\nk)\nso the additional objective function of ESE is:\nJ′\nE(θp)=\nl\nX\nk=0\nE\n\u0002\n∥sk+f θp(sk, ak)∆t+gθp(sk, ak) · Bt−s′\nk∥2\n2\n\u0003\nThe simpliﬁed formula is as follows:\nJ′\nE(θp) =\nl\nX\nk=0\n\u0002\n(sk + f θp(sk, ak)∆t −s′\nk)2\n+∆t · det\n\u0000(ggT )θp(sk, ak)\n\u0001\u0003\n(48)\nGenerally, the variance term g can be neglected to simplify\nthe calculation, so\nJ′\nE(θp) =\nl\nX\nk=0\n\u0002\n(sk + f θp(sk, ak)∆t −s′\nk)2\u0003\n.\nThis additional objective function of ESE can be used to mea-\nsure the accuracy of ESE in estimating future state changes.\nD. Action Policy Generator (APG)\nThe policy gradient method can be used to update the\nnetwork parameters in the action policy generator (APG). In\nclassical reinforcement learning, given the objective function\nof action policy network JA(θv), we have\n∂JA(θv) = E\n\u0002\n∂θvlnπθv(ak|sk) · Qθ(sk, ak)\n\u0003\n(54)\nwhere πθv(ak|sk) is the conditional distribution of action ak\nwhen current environment state sk is known. This policy\ngradient updating method can be used in IRL after making\nsome modiﬁcations.\nRecall that\ndat = µθv(st, at)dt + σθv(st, at) · d ˜Bt.\nSuppose ∆t is the minimal time interval, so from the above\nSDE, we can see that in a short period of time\n∆ak ≈µθv(sk, ak)∆t + σθv(sk, ak) · ∆˜Bt\n(55)\nor\nak+1 ≈ak + µθv(sk, ak)∆t + σθv(sk, ak) · ∆˜Bk\n(56)\nwhere\n∆˜Bk = ˜Bk+1 −˜Bk ∼N(0, ∆t),\nso action ak+1 follows the conditional normal distribution with\nthe condition (sk, ak), or\nak+1 ∼N\n\u0010\nak+µθv(sk, ak)∆t,\n\u0000(σTσ)\n\u0001θv(sk, ak)∆t\n\u0011\n(57)\nThen we get the expression of conditional probability dis-\ntribution of ak+1 as follows,\nπθv(ak+1|sk, ak)\n=|det(σθv(sk, ak))|\n−1\n(2π)\nm\n2 √\n∆t\nexp\n\u0012\n−1\n2∆t∥(σ−1)\nθv\n·(sk, ak)(ak+1 −ak −µθv(sk, ak)∆t)∥2\n2\n\u0001\n(58)\ntake logarithms on both sides,\nlnπθv(ak+1|sk, ak) = −ln|det(σθv(sk, ak))| −\n1\n2∆t\n· ∥(σ−1)\nθv(sk, ak)(ak+1 −ak\n−µθv(sk, ak)∆t)∥2\n2 + C\n(59)\nwhere C is a constant independent of θv, so ∂JA(θv) can be\nwritten as\n∂JA(θv)≈∂θv\n\u001a\nln|(σθv(sk,ak))|+ 1\n2∆t∥(σ−1)θv(sk,ak)\n· (ak+1−ak−µθv(sk,ak)∆t)∥2\n2\n\u001b\n·Qθ(sk,ak)\n(60)\nAfter discretization and sampling data (sk, ak) for several\ntime periods, we have\n∂JA(θv)≈\nl\nX\nk=0\n∂θv\n\u001a\nln|det(σθv(sk,ak))|+ 1\n2∆t∥(σ−1)θv\n· (sk,ak)(ak+1−ak−µθv(sk,ak)∆t)∥2\n2\n\u001b\n·Qθ(sk,ak)\n(61)\nVariance can be ignored in the update period, so\nJA(θv) ≈\nl\nX\nk=0\n1\n2∆tQθ(sk, ak)∥(σ−1)θv(sk, ak)\n· (ak+1 −ak −µθv(sk, ak)∆t)∥2\n2\n(62)\nwhich is one objective function of APG.\nFrom the deﬁnition of the Q function, another objective\nfunction of action strategy generator is obtained. Obviously,\nwe hope that the action policy made by the agent maximize the\nQ value for the next moment. This means that agent’s action\npolicy should maximize the gradient of Q function.\nlim\nt→0\n∂\n∂tE(s,a) \u0002\nQθ(st, at)\n\u0003\n(63)\nWe have learned that according to Kolmogorov’s backward\nequation,\nlim\nt→0\n∂\n∂tE(s,a)\u0002\nQθ(st, at)\n\u0003\n= lim\nt→0 AY E(s,a)\u0002\nQθ(st, at)\n\u0003\n=AY Qθ(s, a)\n(64)\nwhere\nAY =\nm+n\nX\ni=1\nF (θp,θv)\ni\n∂\n∂yi\n+ 1\n2\nm+n\nX\ni,j=1\n(GGT )(θp,θv)\nij\n∂2\n∂yi∂yj\n(65)\nBy maximizing the parameters θv in AY Qθ(s, a) we ensure\nthat the action policy generated by the agent makes the Q\nvalue increasing continuously. Finally, we have\nJ′\nA(θv) = −\nl\nX\nk=0\n m+n\nX\ni=1\nF (θp,θv)\ni\n∂Qθ\n∂yi\n(sk, ak)\n+1\n2\nm+n\nX\ni,j=1\n(GGT )(θp,θv)\nij\n∂2Qθ\n∂yi∂yj\n(sk, ak)\n\n\n(66)\nIEEE TRANSACTIONS ON CYBERNETICS\n10\nFig. 2. The action process and updating process of IRL\nas the other obejctive function of APG. Because only θv\nis optimized in this process, our objective function can be\nsimpliﬁed as\nJ′\nA(θv) = −\nl\nX\nk=0\n m\nX\ni=1\nµθv\ni\n∂Qθ\n∂ai\n(sk, ak)\n+1\n2\nm\nX\ni,j=1\n(σσT )θv\nij\n∂2Qθ\n∂ai∂aj\n(sk, ak)\n\n\n(67)\nThe above two objective functions are derived from the\nclassical policy gradient methods and the framework of IRL.\nIn practice, although the second objective function is more\nsuitable for the overall IRL framework, its optimization pro-\ncess shows some difﬁculties in converging. The ﬁrst objec-\ntive function based on the classical policy gradient also has\nconverging and gradient explosion issues in the IRL training\nprocess. How to improve APG’s action strategy to make the\nestimation process more reliable remains an important research\nquestion.\nIn the next section, we introduce the algorithm, framework,\nnetwork structure, ﬂow chart and other technical challenges of\nIRL.\nIII. TECHNICAL FRAME\nIn the previous sections, we obtained the objective functions\n(38), (49), (62), and (67), which play a critical role in the\nnetwork training. There are some technical challenges in\nachieving IRL:\n• How to choose the network model and what problems\nshould be considered when choosing the model?\n• When the parameter update process is executed, what\nquantity is required in the parameter update process?\n• How to reduce the computational complexity?\nIn this section we discuss these technical problems and estab-\nlishes the framework for IRL.\nFigure 2 shows the execution procedure of IRL. At each\nstep, the system inputs the current environment state and\naction state (sk, ak), and returns the current increment ∆ak\ninside APG, while ESE and VE are dormant. In this process,\nAlgorithm 1 Incremental Reinforcement learning Algorithm\nInitialization:\n1: Randomly initialize network with parameters θ, θv, θp;\ntarget network with parameters θ′ ←−θ,θ′\nv ←−θv, θ′\np\n←−θ′\np;replay buffer R;the form of objective function\nJQ(θ),JE(θp), JA(θv) as eq(38), (49) ,(62) or (67); max\ntrain number Mmax; max train step number Nmax; con-\nstant time interval ∆t; dimensions of state and action\nare m and n respectively; the attenuation rate of future\nexpected reward γ\n2: while M ≤Mmax do\n3:\nrelease buffer R\n4:\n∂θ, ∂θv, ∂θp = 0\n5:\nθ′ = θ, θ′\nv = θv, θ′\np = θp\n6:\ndecide the initial state and action (s, a) and (s0, a0) =\n(s, a)\n7:\nwhile k ≤Nmax do\n8:\nperform the action ak\n9:\nrecive the reward Rk and sk+1\n10:\n∆Bk ∼N(0, ∆t · In)\n11:\n∆ak = µθv(sk, ak)∆t + σθv(sk, ak) · ∆Bk\n12:\nak+1 = ak + ∆ak\n13:\nstore (sk+1, ak+1) and Rk in R\n14:\nk = k + 1\n15:\nend while\n16:\nCalculate\nJQ(θ),\nJE(θp),\nJA(θv)\non\nthe\ndata\n{(sk, ak, Rk)} in the Memory\n17:\nupdate\nthe\nparameters(θ, θp, θv)\nby\n∂θ\n=\n∂JQ(θ), ∂θp = ∂JE(θp), ∂θv = ∂JA(θv)\n18:\nsample the train units in the training process and store\nit in the Memory\n19:\nM = M + 1\n20: end while\nit is necessary to store the training data in the memory to\nupdate the network by randomly sampling. The training units\nare (sk, ak, Rk, s′\nk, a′\nk), where (s′\nk, a′\nk) is the next state. The\nsampling method could be average sampling, or we can ﬁrst\ncompress the reward into certain interval, (e.g. (0.3, 0.9)), and\nBernoulli sampling to decide which training units should be\nstored in the Memory.\nBecause the network processes time series data, it seems\nthat the network model with hidden variables such as RNN\nand its variants LSTM, GRU, SRU is suitable for the network\nstructure of IRL. However, they are not ideal choices. As we\ncan see from eq (1) and eq (2), the increment of action and\nenvironment follows Markov property, and the output of the\nnetwork is incremental. If we adopt a recurrent neural network\nwith time series, the incremental generation will no longer\nfollow the Markov property, which is contrary to our model\nassumptions. The process of estimating the value of Q function\nin VE is to estimate the expected future revenue. From the\ndeduction process after eq (10), we can see that Q function\nalso satisﬁes Markov property. Therefore, it is not suitable\nto use recurrent neural network. In summary, although we\ndeal with time series data in the training process, it is not\nideal to use RNN and other recurrent neural networks, so\nIEEE TRANSACTIONS ON CYBERNETICS\n11\nFig. 3.\nMountainCarContinuous-v0, Cartpole-v1 and Pendulum-v0 training curves. The training curves in the ﬁgure are averaged the three times experiments,\nand the shaded region shows the standard deviation.\nwe just use general depth network. On the other hands, we\nhave mentioned earlier that because of the need to ensure\nthe second-order continuity of functions in ESE and VE,\nthe ReLU class of non-linear functions can not be used in\nthe network structure. Otherwise, we can not even calculate\nthe differentials of those objective functions. In the actual\ncomputation, we rarely encounter zero points which are the\nnon-differentiable points of ReLU, so we might still use ReLU\nor PreLU. The possible inﬂuence of taking this kind of non-\nlinear function as the non-linear layer of the network is beyond\nthe scope of this paper.\nThe current action output of the agent does not seem to\ndepend on the current state sk, but on the past (sk−1, ak−1),\nwhich means IRL is not Markov control method. Figure 2\nshows that the agent does not make actions based on the\ncurrent environment state, instead they estimates the increment\nof actions based on the previous environment and action state,\nwhich shows their ability to predict under IRL. This ability\nhelp adjust its actions continuously. In this situation, agents\nno longer adapt to the environment to get better rewards.\nInstead, they adjust according to the environment and its own\nstate. Taking robots in executing tasks for example, when the\nsensors of the robot transform physical signal and then process\nthe network to make actions, delay occurs, and the external\nenvironment of the robot could have changed during the delay.\nDespite that more advanced sensors, actuators and controllers\ncan largely reduce such a effect, its impacts cannot be ignored.\nConsequently, the real-time performance of Markov control is\nalways a big challenge. IRL, on the other hand, does not suffer\nfrom time delay at all.\nThe pseudo-code for ofﬂine IRL can be seen in Algorithm1.\nIn the next section, we show the performance of IRL, and com-\npare it with other classical continuous reinforcement learning\nmethods.\nIV. EXPERIMENTS RESULTS\nIn this section, we present a comparative experiment that\ncompares IRL and three classical continuous reinforcement\nlearning algorithms (DDPG, A2C and PPO) using three sets\nof OpenAI Baselines (which are sets of improved imple-\nmentations of RL algorithms and high-quality implementa-\ntions of reinforcement learning algorithms [31]). We conduct\nexperiments under three testing environments, Pendulum-v0,\nMountainCarContinuous-v0 and CartPole-v1 from OpenAI\nGym. We record the reward values of each learning method,\nand plot them against episodes in a episodes-rewards diagram.\nThe goal of our experiments is to verify the feasibility and\neffectiveness of IRL algorithm.\nIn our experiments, time interval ∆t is 0.05, γ is 0.6,\nthe step number in every epoch is 200. In APG, two kinds\nof object functions eq(62) and eq(67) jointly update APG\nnetwork. The weighting coefﬁcient of two objective functions\nis set to be (0.1,1). In DDPG, A2C and PPO, 200 steps were\ntrained in each episode for a total of 2000 training episodes.\nAs shown in the Figure 3, the experimental effect and\nperformance of IRL basically exceeds those of A2C method,\nbut generally under-performs comparing to DDPG method.\nEspecially in the MountainCarContinuous, IRL is less stable\nthan the other three methods and converges slower. This may\nbe attributed to the fact that the action policy of the network in\nthis experiment is discrete (positively and negatively correlated\nwith the output of the action network) so it is not a strictly\ncontinuous control problem and might cause unstability of IRL\nin the experiment. In the Pendulum-v0 experiment, despite a\nslower converging speed, IRL reaches same average rewards\nlevel as the DDPG. The stability of the dynamic in training\nprocess is also quite good. Similar performance is found in\nCartPole-v1.\nThe reason why our experiments results do not reﬂect the\nsuperiority of the theory might be understood as follows:\n• The updating process of IRL includes partial differential\nequations as constraints. However, the parameters of the\npartial differential equations vary during the algorithm\nupdating process. The inﬂuence of parameter changes\nduring estimation process due to parameter gradient up-\ndates on the constrains requires additional studies on the\npartial differential equation theory.\n• The convergence of the action network is slower than the\nvalue estimation network and the environment estimation\nnetwork. It indicates that the two action updates used\ncurrently are less convergent so the theory on parameter\nadjustment might need improvement.\n• The current discretization of our control problem may\nalso have some impacts. The theoretical derivation in this\npaper is based on stochastic differential equation, which is\ndiscretized in practical experiments. Other more complex\ndiscretization methods may yield different results.\n• One problem with IRL is that it does not constrain the\nIEEE TRANSACTIONS ON CYBERNETICS\n12\noutput of an action by adding tanh to the end like DDPG,\nwhich might cause boundary overﬂow problem. In this\npaper, a boundary penalty regulation term is added\nJRange(θ)= λRange\nl\nX\nk=0\n\u001a\n∥((sk+∆sk, ak+∆ak)\n−(smax, amax))+∥2\n2+∥((smin, amin)\n−(sk+∆sk, ak+∆ak))+∥2\n2\n\u001b\n(68)\nwhere + represent ReLU function, (smax, amax) and\n(smin, amin) are the upper and lower bounds of the\nparameter boundary conditions for the action and the\nenvironment. However, it is not clear whether this strategy\nwould affect the convergence of our algorithm.\nV. CONCLUSION\nWe propose a new continuous reinforcement learning frame\nIRL. This method guarantees the continuity of actions and the\nexistence and uniqueness of the value estimation network. On\none hand, the variance of action strategy is adjusted to control\nfor the randomness of action strategy. On the other hand,\nan environment estimation network is introduced to improve\nagents’ action adjustments. With our method, agents no longer\nblindly adapt to the environment but instead estimates the in-\ncrement of actions based on the previous environment & action\nstates and make corresponding adjustments continuously. Our\nmethod largely reduces the real time delay in the information\nupdates and decision making process so agents’ actions are\nsmoother and more accurate.\nMoreover, the IRL is constructed based on stochastic differ-\nential equation methods. Its differential behavior, which is also\nthe important inherent feature of the physical world, makes\nit suitable for controlling the continuous systems naturally.\nThis behavioral consistency between the target model and\nthe method not only makes the simulation be more realistic\nthan the classical continuous RL methods, such as A3C and\nDDPG, so as to obtain better continuity and stability, but also\nobtains better motion correction and control optimization, as\nshown in ﬁgure 4. In summary, the proposed IRL provides a\nnew theoretical basis and technical approach for the practical\ncontrol applications.\nOur stimulation results are largely consistent with our theory\nprediction. Although it does not fully reﬂect the advantage of\nour theory due to slow convergence issues, we have identiﬁed\nsome of the limitations in our experiment, so in the future re-\nsearch we will experiment on using partial differential equation\ntheory to estimate Q function and improve the convergence\nspeed during optimization. We will also solve the boundary\noverﬂow problem.\nFig. 4. The application and the advantages of IRL\nAPPENDIX\nIn the appendix we prove that the action policy of some\nclassical reinforcement learning methods such as A3C cannot\nguarantee the continuity of action, which means those methods\ncannon be applied in many tasks such as robot self-adaptive\ncontrol. We start with a deﬁnition of the modiﬁcation of\nstochastic process.\nDeﬁnition Suppose process {Xt} and {Yt} are both the\nstochastic processes on (Ω, F, P). If\nP({ω; Xt(ω) = Yt(ω)}) = 1 ∀t\nthen {Yt} is a modiﬁcation of {Xt}, and they will have the\nsame ﬁnite dimensional distribution.\nNow give an important continuity theorem – the Kol-\nmogorov continuity theorem.\nTheorem Suppose process X = {Xt}t≥0 meets the condi-\ntions: T > 0, if there exists positive constants m, n, D such\nthat\nE [|Xt −Xs|m] ≤D · |t −s|1+n; 0 ≤s, t ≤T\nthen there exists a continuous modiﬁcation of X.\nNow let’s prove that AC method using Gauss policy as\naction policy can’t satisfy the conditions for continuity of\nagents’ actions. The general form of this action policy is\nat = µ(st) + σ(st) · Nt\nwhere Nt is white noise. Suppose st≥0 are independent of\neach other at any time and at is one-dimensional.\nProof.\nat\nfollows\nGauss\ndistribution,\nthat\nat\n∼\nN(µ(st), σ2(st)), and\nat −ap ∼N(µ(st) −µ(sp), σ2(st) + σ2(sp))\nfor all m > 0, the moment conditions of absolute values can\nbe expressed in terms of conﬂuent hypergeometric functions:\nE [|at −ap|m]=\n\u0000σ2(st) + σ2(sp)\n\u0001 m\n2 · 2\nm\n2 Γ( 1+m\n2 )\n√π\n1\n· F1\n\u0012\n−m\n2 , 1\n2, −1\n2\n(µ(st) −µ(sp)2)\nσ2(st) + σ2(sp)\n\u0013 (69)\nIEEE TRANSACTIONS ON CYBERNETICS\n13\nwhere Γ is the Gamma function and\n1F1 is the Conﬂuent\nhypergeometric function:\n1F1(a, b, z) =\n∞\nX\nn=0\na(n)zn\nb(n)n!\nwherea(0) = 1, a(n) = a(a + 1)...(a + n −1). It is easy to see\nthere exists a positive\n\u0000σ2(st) + σ2(sp)\n\u0001 m\n2 ·2\nm\n2 Γ( 1+m\n2\n)\n√π\nthat is\nirrelevant to |t−s| even with µ satisfying H¨older condition, so\nat cannot satisfy Kolmogorov continuity and it doesn’t exists\na continuous modiﬁcation.\nREFERENCES\n[1] J. Schmidhuber, “Deep learning in neural networks: An overview,”\nNeural networks, vol. 61, pp. 85–117, 2015.\n[2] C. J. C. H. Watkins, “Learning from delayed rewards,” Ph.D. disserta-\ntion, King’s College, Cambridge, 1989.\n[3] R. H. Crites and A. G. Barto, “Improving elevator performance using\nreinforcement learning,” in Advances in neural information processing\nsystems, 1996, pp. 1017–1023.\n[4] W. D. Smart and L. P. Kaelbling, “Effective reinforcement learning for\nmobile robots,” in Proceedings 2002 IEEE International Conference on\nRobotics and Automation (Cat. No. 02CH37292), vol. 4.\nIEEE, 2002,\npp. 3404–3410.\n[5] M. A. Wiering and H. Van Hasselt, “Ensemble algorithms in reinforce-\nment learning,” IEEE Transactions on Systems, Man, and Cybernetics,\nPart B (Cybernetics), vol. 38, no. 4, pp. 930–936, 2008.\n[6] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier-\nstra, and M. Riedmiller, “Playing atari with deep reinforcement learn-\ning,” arXiv preprint arXiv:1312.5602, 2013.\n[7] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski\net al., “Human-level control through deep reinforcement learning,”\nNature, vol. 518, no. 7540, p. 529, 2015.\n[8] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning\nwith double q-learning,” in Thirtieth AAAI Conference on Artiﬁcial\nIntelligence, 2016.\n[9] S. Gu, T. Lillicrap, I. Sutskever, and S. Levine, “Continuous deep q-\nlearning with model-based acceleration,” in International Conference\non Machine Learning, 2016, pp. 2829–2838.\n[10] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,\nD. Silver, and D. Wierstra, “Continuous control with deep reinforcement\nlearning,” arXiv preprint arXiv:1509.02971, 2015.\n[11] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,\nD. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep rein-\nforcement learning,” in International conference on machine learning,\n2016, pp. 1928–1937.\n[12] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-\nimal policy optimization algorithms,” arXiv preprint arXiv:1707.06347,\n2017.\n[13] F. Qi-Ming, L. Quan, W. Hui, X. Fei, Y. Jun, and L. Jiao, “A novel off\npolicy q (λ) algorithm based on linear function approximation,” Chinese\nJournal of Computers, vol. 37, no. 3, pp. 677–686, 2014.\n[14] J. Kober, J. A. Bagnell, and J. Peters, “Reinforcement learning in\nrobotics: A survey,” The International Journal of Robotics Research,\nvol. 32, no. 11, pp. 1238–1274, 2013.\n[15] Y. Wei and M. Zhao, “A reinforcement learning-based approach to\ndynamic job-shop scheduling,” Acta Automatica Sinica, vol. 31, no. 5,\np. 765, 2005.\n[16] E. Ipek, O. Mutlu, J. F. Mart´ınez, and R. Caruana, “Self-optimizing\nmemory controllers: A reinforcement learning approach,” in ACM\nSIGARCH Computer Architecture News, vol. 36, no. 3. IEEE Computer\nSociety, 2008, pp. 39–50.\n[17] G. Tesauro, “Td-gammon, a self-teaching backgammon program,\nachieves master-level play,” Neural computation, vol. 6, no. 2, pp. 215–\n219, 1994.\n[18] L. Kocsis and C. Szepesv´ari, “Bandit based monte-carlo planning,” in\nEuropean conference on machine learning.\nSpringer, 2006, pp. 282–\n293.\n[19] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van\nDen Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,\nM. Lanctot et al., “Mastering the game of go with deep neural networks\nand tree search,” nature, vol. 529, no. 7587, p. 484, 2016.\n[20] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel, “Bench-\nmarking deep reinforcement learning for continuous control,” in Inter-\nnational Conference on Machine Learning, 2016, pp. 1329–1338.\n[21] S. Gu, T. Lillicrap, I. Sutskever, and S. Levine, “Continuous deep q-\nlearning with model-based acceleration,” in International Conference\non Machine Learning, 2016, pp. 2829–2838.\n[22] S. Hansen, “Using deep q-learning to control optimization hyperparam-\neters,” arXiv preprint arXiv:1602.04062, 2016.\n[23] M. Andrychowicz, M. Denil, S. Gomez, M. W. Hoffman, D. Pfau,\nT. Schaul, B. Shillingford, and N. De Freitas, “Learning to learn by\ngradient descent by gradient descent,” in Advances in Neural Information\nProcessing Systems, 2016, pp. 3981–3989.\n[24] J. Oh, X. Guo, H. Lee, R. L. Lewis, and S. Singh, “Action-conditional\nvideo prediction using deep networks in atari games,” in Advances in\nneural information processing systems, 2015, pp. 2863–2871.\n[25] J. C. Caicedo and S. Lazebnik, “Active object localization with deep\nreinforcement learning,” in Proceedings of the IEEE International Con-\nference on Computer Vision, 2015, pp. 2488–2496.\n[26] F. Zhang, J. Leitner, M. Milford, B. Upcroft, and P. Corke, “Towards\nvision-based deep reinforcement learning for robotic motion control,”\narXiv preprint arXiv:1511.03791, 2015.\n[27] B. Oksendal, Stochastic differential equations: an introduction with\napplications.\nSpringer Science & Business Media, 2013.\n[28] W. Huang, Y. Wang, and X. Yi, “A deep reinforcement learning\napproach to preserve connectivity for multi-robot systems,” in 2017 10th\nInternational Congress on Image and Signal Processing, BioMedical\nEngineering and Informatics (CISP-BMEI).\nIEEE, 2017, pp. 1–7.\n[29] C. Wei and H. Shu, “Maximum likelihood estimation for the drift\nparameter in diffusion processes,” Stochastics, vol. 88, no. 5, pp. 699–\n710, 2016.\n[30] D. Gilbarg and N. S. Trudinger, Elliptic Partial Differential Equations\nof Second Order, 3rd ed.\nSpringer, 1998.\n[31] V. Firoiu, W. F. Whitney, and J. B. Tenenbaum, “Beating the world’s best\nat super smash bros. with deep reinforcement learning,” arXiv preprint\narXiv:1702.06230, 2017.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-08-08",
  "updated": "2019-08-08"
}