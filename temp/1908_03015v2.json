{
  "id": "http://arxiv.org/abs/1908.03015v2",
  "title": "Augmenting Variational Autoencoders with Sparse Labels: A Unified Framework for Unsupervised, Semi-(un)supervised, and Supervised Learning",
  "authors": [
    "Felix Berkhahn",
    "Richard Keys",
    "Wajih Ouertani",
    "Nikhil Shetty",
    "Dominik Geißler"
  ],
  "abstract": "We present a new flavor of Variational Autoencoder (VAE) that interpolates\nseamlessly between unsupervised, semi-supervised and fully supervised learning\ndomains. We show that unlabeled datapoints not only boost unsupervised tasks,\nbut also the classification performance. Vice versa, every label not only\nimproves classification, but also unsupervised tasks. The proposed architecture\nis simple: A classification layer is connected to the topmost encoder layer,\nand then combined with the resampled latent layer for the decoder. The usual\nevidence lower bound (ELBO) loss is supplemented with a supervised loss target\non this classification layer that is only applied for labeled datapoints. This\nsimplicity allows for extending any existing VAE model to our proposed\nsemi-supervised framework with minimal effort. In the context of\nclassification, we found that this approach even outperforms a direct\nsupervised setup.",
  "text": "AUGMENTING VARIATIONAL AUTOENCODERS WITH SPARSE\nLABELS:\nA UNIFIED FRAMEWORK FOR UNSUPERVISED,\nSEMI-(UN)SUPERVISED, AND SUPERVISED LEARNING\nA PREPRINT\nFelix Berkhahn ∗1, Richard Keys †1, Wajih Ouertani ‡1, Nikhil Shetty §1, and Dominik Geißler ¶1\n1Relayr (GmbH), Munich\nNovember 15, 2019\nABSTRACT\nWe present a new ﬂavor of Variational Autoencoder (VAE) that interpolates seamlessly between\nunsupervised, semi-supervised and fully supervised learning domains. We show that unlabeled\ndatapoints not only boost unsupervised tasks, but also the classiﬁcation performance. Vice versa,\nevery label not only improves classiﬁcation, but also unsupervised tasks. The proposed architecture is\nsimple: A classiﬁcation layer is connected to the topmost encoder layer, and then combined with the\nresampled latent layer for the decoder. The usual evidence lower bound (ELBO) loss is supplemented\nwith a supervised loss target on this classiﬁcation layer that is only applied for labeled datapoints. This\nsimplicity allows for extending any existing VAE model to our proposed semi-supervised framework\nwith minimal effort. In the context of classiﬁcation, we found that this approach even outperforms a\ndirect supervised setup.\nKeywords Machine Learning, Semi-supervised learning, Variational Autoencoder, Anomaly Detection, Transfer\nLearning, Representation Learning\n1\nIntroduction\nIn many domains, unlabeled data is abundant, whereas obtaining rich labels may be time consuming, expensive and rely\non manual annotations. As such, the value proposition of semi-supervised learning algorithms is immense: It allows us\nto train well performing predictive systems with only a fraction of labeled datapoints.\nIn this paper, we present a new ﬂavor of Variational Autoencoder (VAE) that enables semi-supervised learning. The\nmodel architecture requires only minimal modiﬁcations on any given purely unsupervised VAE. The semi-supervised\nclassiﬁcation accuracy has similar performance as slightly more complex approaches known in the literature [1, 16].\nThis was benchmarked using the MNIST (section 3.1.1), Fashion-MNIST (section 3.2.1) and UCI-HAR (section 3.3)\ndata sets. We veriﬁed that even if every single datapoint is labeled, framing the training process in the context of VAE\ntraining improves the classiﬁcation accuracy compared to the common way of training the classiﬁcation network in\nisolation. We conjecture that supplementing the classiﬁcation loss with the VAE loss forces the network to learn better\nrepresentations of the data. Here the VAE reconstruction task acts as a regularizer during training of the classiﬁcation\nnetwork.\n∗felix.berkhahn@relayr.io\n†richard.keys@relayr.io\n‡wajih.ouertani@relayr.io\n§nikhil.shetty@relayr.io\n¶dominik.geissler@relayr.io\narXiv:1908.03015v2  [cs.LG]  14 Nov 2019\nAugmenting Variational Autoencoders with Sparse Labels:\nA uniﬁed framework for unsupervised, semi-(un)supervised, and supervised learning\nA PREPRINT\nWe also veriﬁed that the availability of labels helps the model to ﬁnd better latent representations of the data: We used\nthe betaVAE disentanglement metric to asses the quality of the found representations (section 4). Furthermore, we\napplied the VAEs to the problem of anomaly detection, and observed that its performance increases when the model is\ntrained with additional labeled samples - see sections 3.1.3, 3.2.3 and 3.3 for benchmarks on MNIST, Fashion-MNIST\nand UCI-HAR respectively. In that sense, not only is the reconstruction of the model boosted by the availability of\nunlabeled datapoints (which is the normal semi-supervised setup), but vice versa the anomaly detection performance is\nalso improved by the availability of labels.\nIn summary, we have developed a model which adapts seamlessly on the full 0-100% range of available labels. The\nresult is a ‘uniﬁed’ model in which the anomaly detection capability is improved by any available label, and vice versa\nin which the predictive capability is signiﬁcantly boosted by the abundance of unlabeled data. This paper provides a\nmore thorough investigation and benchmark of the concepts which were published in a blog post in 2018 [5].\n2\nModel\n2.1\nModel architecture\nThe general model architecture is depicted in Figure 1a. As can be seen, the model is an extension of the original VAE\n[2] which is depicted in Figure 1c. The only addition is that a classiﬁcation layer π (typically a one-hot classifying layer\nusing softmax activation) is introduced that is attached to the topmost encoder layer. The µ and σ layer encode the\nmean and standard deviation of the gaussian prior in the latent layer:\np(z) = N(z|µ, σ)\n(1)\nAfter sampling the latent variable z using the probability distribution (1), z and the activations of π are merged and fed\ninto the decoder pθ:\nxrecon ∼pθ(π, z) = pθ(π ⊕z)\n(2)\nwhere xrecon denotes the reconstructed data of the decoder. Hence, the classiﬁcation predictions are also contributing to\nthe reconstruction of the data.\n(a) Semi-Supervised VAE, model SS\n(b) Supervised Classiﬁer, model ES\n(c) Unsupervised Anomaly Detector, model\nEU\nFigure 1: Comparison of our model architecture (a) to the supervised (b) / unsupervised (c) equivalents. The greyed out\ncells shown in (b) and (c) are not part of the models, and highlight the difference to our model (a). Note that the π layer\n(and its loss) represents the extension to the standard VAE proposed in this paper.\n2\nAugmenting Variational Autoencoders with Sparse Labels:\nA uniﬁed framework for unsupervised, semi-(un)supervised, and supervised learning\nA PREPRINT\n2.2\nLoss function\nWe propose an ad-hoc modiﬁcation of the standard VAE evidence lower bound LELBO [2] loss function:\nL = LELBO + Lcl\n(3)\nwhere\nLELBO\n=\nEz∼p(z) [log (pθ(x|y, z))] −KL [pφ(z)||N(0, I)]\n(4)\nLcl\n=\n−\nα(y)\n#labeled\nX\ni\nyi · log(πi)\n(5)\nKL [pφ(z)||N(0, I)] denotes the Kullback-Leibler divergence of pφ(z) and a standard normal distribution, which is\nonly applied to the latent variables z, but not the labels π. pφ(z) is the probability distribution of the latent variable\ngenerated by the encoder. y represents the label of the datapoint. α(y) is equal to zero if there is no label (ie y belongs\nto the ’unlabeled class’), else it is one. Normalizing α to the number of all labeled datapoints per batch aids stabilizing\ntraining. Lcl denotes the classiﬁcation log-loss.\n2.3\nUpsampling the labeled data\nIn order to prevent artiﬁcial noise from a stochastic number of labeled contributions in the log-loss term (5), we\nchose to not only normalize this term but also ﬁx the number of labeled samples per batch: Besides the completely\n(un)supervised edge cases, we sampled datasets such that each batch contained labeled and unlabeled samples in a\nratio of 1:1. Additionally, this prevents unlabeled datapoints from dominating training in cases of very sparsely labeled\ndatasets.\n2.4\nDifferences to Kingma’s VAE [1]\nOur work is largely inspired by [1]. However, the model we are proposing differs from the model M2 of [1] in several\naspects as shown in Table 1.\nTable 1: Differences to Kingma et al.\nour model\nKingma et al\nencoder\nsingle encoder network\nsharing weights qφ(z, y|x)\ntwo independent encoder networks\nqφ(z, y|x) = qφ(z|x)qφ(y|x)\nlatent layer\nlatent activations only\ndepend on x: µ(x)\nlatent activations depend on\nboth x and y: µ(x, y)\ntreatment of unlabeled data\nα(y) will omit\ncontribution to Lcl\nunknown y is summed over\nThe simplicity of our model allows to turn any existent VAE into a semi-supervised VAE by simply adding the π layer\nand extending the loss function. In particular, all learned weights can directly be reused when transitioning into the\nsemi-supervised learning scenario. This is very useful, as in many real world applications, a labeled dataset (even\npartially labeled) is only built up over time and not available at project initiation.\n2.5\nClassiﬁcation - Decoder as a regularizer\nWe benchmarked the classiﬁcation performance of our model for various data sets (MNIST, Fashion-MNIST, UCI-HAR,\nsee sections 3.2.1, 3.2.1 and 3.3) as a function of available labels. Not surprisingly, more labeled or unlabeled samples\ngenerally improves performance.\nMoreover, we also tested our model in the scenario where all datapoints were labeled. Interestingly, we found that the\nobtained model was performing better than training the same classiﬁcation model (Figure 1b) in a standard supervised\nscenario. In other words, framing the training process in the context of VAE training allows the classiﬁcation network\nto learn better weights compared to training it the ’standard way’ with only the π classiﬁcation loss.\nThe additional training target of reproducing the input via the decoder forces the network to learn more meaningful\nrepresentations in its deeper layers, from which the classiﬁcation beneﬁts. The decoder and VAE training act as a\nregularizer, as it challenges the network to ﬁnd more subtle and granular representations of the input data, i.e. it will\ncombat overﬁtting. At the same time, these representations are meaningful, as they contain valuable information about\n3\nAugmenting Variational Autoencoders with Sparse Labels:\nA uniﬁed framework for unsupervised, semi-(un)supervised, and supervised learning\nA PREPRINT\nhow to reconstruct the datapoint properly - hence it is expected that they enhance any task built on top of them (for\ninstance, classiﬁcation).\n2.6\nSemi-Unsupervised learning\nAs we have seen in the previous sections, the availability of unlabeled datapoints aids the model to form better\nrepresentations in its deeper layers, hence enabling semi-supervised learning. Maybe the opposite is true as well: Does\nthe availability of labels also aid with ﬁnding better representations? Does it perform better on reconstruction related\ntasks such as anomaly detection?\nThis problem setup can be generally described as a ﬂavor of ’transfer learning’: can the model improve its task related\nto unsupervised learning by leveraging the availability of labels that are primarily associated to the supervised learning\ntask?\nThis was investigated in two different kind of experiments: (a) we benchmarked the quality of the representations\ndirectly via the betaVAE score as a function of available labels (section 4). In this case the added π layer can be\ninterpreted as an additional loss term directly reﬂecting the betaVEA score.\nAnd (b) we used the VAE as an anomaly detector (see sections 3.1.3, 3.2.3 and 3.3). In this case our approach can\nbe viewed as feature engineering: usually labels incorporate domain knowledge of some very speciﬁc, yet important,\nproperty of the data set. Thereby our method can guide the π layer towards an extractor for those very speciﬁc high-level\nfeatures. In this experiment, we contrasted the semi-supervised model to an equivalent purely unsupervised VAE by\nremoving the π layer from the network and the loss function (this corresponds to the left and right most panels of Figure\n1). We then compared its anomaly detection performance with the anomaly detection performance of our model trained\neither on a portion or all normal datapoints labeled.\nThe term ’semi-unsupervised learning’ is a perfect description of this task - as semi-supervised learning enhances the\nperformance of a supervised task by using unlabeled data, ’semi-unsupervised’ learning would enhance the performance\nof an unsupervised task by using labeled data. The only other mention of the term was used to describe experiments [3]\n[4] on some other variations of the classic VAE [2]. This unsupervised task was however quite different, in which the\nobjecting was to cluster unlabeled datapoints and subsequently classify them using one-shot-learning.\n3\nExperiments\nThe networks used are described in detail in appendix A. Each semi-supervised model SS (our architecture as described\nabove in 2.1 ), was contrasted with two sibling networks: (a) An equivalent supervised network ES, corresponding\nonly to the encoder plus the π layer of the SS, being trained only on the cross-entropy loss. And (b) an equivalent\nunsupervised network EU, which is identical to our SS architecture, but with the π layer removed. Throughout this\nsection, we will refer to our models using the abbreviations as shown in Table 2. The error bars were generated by\nre-running each scenario at least 10 times (unless speciﬁed otherwise).\nTable 2: Model abbreviations. Generally speaking the equivalent models ES and EU are derived from SS by either\nremoving the resampling step and decoder (thus making it supervised, ES) or by removing the π layer (thus making it\nfully unsupervised, EU).\nDense\nConvolutional\nRecurrent\nSemi-Supervised (ours)\nSSD\nSSCNN\nSSRNN\nEquivalent Supervised\nESD\nESCNN\nESRNN\nEquivalent Unsupervised\nEUD\nEUCNN\nEURNN\n3.1\nMNIST\nFor almost any type of image classiﬁer, the natural place to begin any benchmarking is by using the MNIST [6] dataset,\na well known dataset containing 70, 000 (60, 000 training and 10, 000 testing) grey-scale images of hand written digits.\nGiven the versatility of the model, we conducted the following three benchmarks.\n3.1.1\nSemi-Supervised performance\nThe ﬁrst task is semi-supervised learning, the area within which the model was designed to bring the most beneﬁt. To\ncreate a semi-supervised dataset, we simply discard a certain percentage of the labels, but not the images themselves.\n4\nAugmenting Variational Autoencoders with Sparse Labels:\nA uniﬁed framework for unsupervised, semi-(un)supervised, and supervised learning\nA PREPRINT\nThis means an equivalent supervised model will only be able to train on the sample of the dataset which is labeled,\nwhereas the semi-supervised model will beneﬁt from all the additional unlabeled samples.\nThe semi-supervised model was trained for 10 epochs, whilst the supervised equivalent was trained for 20 epochs,\nsuch that the comparison is made between fully converged models. For comparison, 10 different labeled subsets of the\ndataset were taken, with each model trained on identical data, the results of which are displayed in Table 3.\nTable 3: Semi-Supervised MNIST classiﬁcation results\nmodel\n100 labels\n1, 000 labels\naccuracy\nlog loss\naccuracy\nlog loss\nSSD\n0.808 ± 0.006\n0.80 ± 0.01\n0.9346 ± 0.0009\n0.215 ± 0.002\nESD\n0.763 ± 0.003\n0.75 ± 0.01\n0.902 ± 0.006\n0.41 ± 0.03\nSSCNN\n0.811 ± 0.006\n0.86 ± 0.03\n0.945 ± 0.001\n0.218 ± 0.06\nESCNN\n0.765 ± 0.003\n0.744 ± 0.009\n0.89 ± 0.02\n0.6 ± 0.2\nUnsurprisingly, both variants of the semi-supervised model outperform their purely supervised equivalents for both sets\nof labels. For the CNN variant, there was a clear increase in the accuracy of the classiﬁcation for both 100 and 1, 000\nlabels as it scored 4.6% and 5.5% higher than the supervised equivalent respectively.\nInterestingly, on 100 labels the log loss was lower for the supervised model than for the semi-supervised model for both\nthe dense and CNN variants; however when trained on 1, 000 labels this trend was reversed. A possible interpretation\nof this could be that the supervised model is trained on a much smaller dataset and hence starts to overﬁt, producing\npredictions with a higher certainty. This might be beneﬁcial for the log loss since the supervised model has predictive\npower nonetheless as is corroborated by the accuracy score.\n3.1.2\nDecoder as a regularizer\nThe next test for the model is in the purely supervised domain, testing the hypothesis that the decoder acts as a regularizer\nand assists the model in ﬁnding better representations of the dataset. For this test, both models were trained on the full\n(60, 000 sample) training dataset until converged, with the results displayed in Table 4.\nTable 4: Supervised MNIST classiﬁcation results\nmodel\naccuracy\nlog loss\nSSD\n0.9855 ± 0.0003\n0.062 ± 0.001\nESD\n0.9814 ± 0.0003\n0.131 ± 0.003\nSSCNN\n0.9916 ± 0.0003\n0.036 ± 0.002\nESCNN\n0.9904 ± 0.0009\n0.055 ± 0.009\nFor both the dense and CNN cases the accuracy scores were very similar and, in the case of the CNN model, the error\nbars are almost overlapping. The log loss of the semi-supervised model however was signiﬁcantly lower, in particular\nfor the dense model there was a 50% reduction when compared with that of the supervised model. This strongly\nsuggests that the addition of the reconstruction task introduces a much higher conﬁdence in the classiﬁcations of the\nsemi-supervised model.\n3.1.3\nSemi-Unsupervised Learning\nThe purpose of the semi-unsupervised task was to verify that the introduction of labels can improve the performance of\nan anomaly detection task. The results presented in Table 5 were obtained by training the model on 9 of the 10 classes\n(designated ’normal’ using the terminology of anomaly detection) in the MNIST dataset and inferring on all classes,\nessentially declaring the left out class to be ’anomalous’ data. The anomaly score returned by the models is the log\nreconstruction probability, which is expected to be higher for the anomalous classes. The performance of the scores are\nevaluated using the AUC (area under curve, also ROC) score.\nWith the exception of the digit 1, there was a considerable improvement for each anomalous class, with an average\nimprovement of 4.1% over the purely unsupervised model. It demonstrates that the addition of labels aids the model in\nlearning a better representation of normal data. Again, as hypothesized, this is most likely due to the additional label\ninformation assisting the model in identifying the category as important high level feature.\n5\nAugmenting Variational Autoencoders with Sparse Labels:\nA uniﬁed framework for unsupervised, semi-(un)supervised, and supervised learning\nA PREPRINT\nTable 5: Label assisted results of anomaly detection on MNIST\nanomalous class\nAUC of EUD\nAUC of SSD\n0\n0.949 ± 0.001\n0.969 ± 0.001\n1\n0.47 ± 0.03\n0.095 ± 0.006\n2\n0.9610 ± 0.0009\n0.9719 ± 0.0004\n3\n0.848 ± 0.003\n0.902 ± 0.002\n4\n0.708 ± 0.004\n0.751 ± 0.003\n5\n0.860 ± 0.003\n0.894 ± 0.001\n6\n0.9295 ± 0.0008\n0.960 ± 0.002\n7\n0.669 ± 0.002\n0.755 ± 0.005\n8\n0.891 ± 0.004\n0.922 ± 0.002\n9\n0.62 ± 0.02\n0.677 ± 0.005\nThe very poor performance of using the digit 1 as the anomaly class could also be evidence to support this. Given\nthe similarities between the digits 1 and 7, it is likely that the dense representation found by the model for the digit\n7 was also sufﬁcient for reconstructing the 1’s, especially given that the shape of the digit 1 is most often also found\nwithin the digit 7. This would also explain why there was not a similar drop in performance when considering 7 as\nthe anomaly class. Considering clustered embeddings, the dense representation of the digit 1 would not be enough to\nproperly reconstruct a 7, leading to a higher anomaly score\nBased on these results, a further experiment was run to assess the effect of the amount of available labels. Classes ’7’\nand ’9’ were chosen as they achieved the largest improvement over the unsupervised equivalent and the results are\ndisplayed in Table 6.\nTable 6: Anomaly detection AUC w.r.t. label availability\nlabel %\nanomaly class 7\nanomaly class 9\n1%\n0.751 ± 0.006\n0.670 ± 0.004\n10%\n0.736 ± 0.009\n0.679 ± 0.003\n25%\n0.744 ± 0.008\n0.680 ± 0.004\n50%\n0.752 ± 0.004\n0.677 ± 0.004\n75%\n0.744 ± 0.004\n0.673 ± 0.004\n99%\n0.745 ± 0.009\n0.652 ± 0.004\nFor this test, the model was trained by re-sampling the available labels such that the model is trained on an equal\namount of labeled and unlabeled data, without making any changes to the testing data. If the label fraction is below\n50%, this amounts to up-sampling the labeled data, while vice-versa for a label fraction above 50% to downsampling of\nthe labeled data. For both classes, there is almost no difference in the label percentage as the error bars all overlap. Not\nonly is this result somewhat surprising, but it is an advantage of such a model.\nFirstly, it demonstrates that a tiny fraction of labels is all that is required to bring a substantial increase in performance\ncompared to the unsupervised domain, i.e. almost the maximum pay-off can be achieved straight away.\nSecondly, although one would have intuitively expected the performance of the anomaly detector to increase with\nrespect to the number of labels, this is not the case and does not conﬂict with the hypothesis. Given that the labels are\nup-sampled during training to balance the learning objective, increasing the percentage of labels in the training set\nsimply increases the diversity of the labeled data rather than the quantity.\nConsidering the hypothesis that learning clustered representation of the data assists the model in identifying anomalies, a\npossible explanation for the lack of improvement in the anomaly detection performance could be reasoned as follows: if\na small fraction of labels is enough to push the model to ﬁnding such a clustered representation, a larger diversity within\nthe label classes may not provide any further contribution. Perhaps a more diverse representation within the clusters\nwould improve the performance by helping the model to identify anomalous data which lie on the class boundaries.\n3.1.4\nData generation\nThe decoder part of a VAE samples the latent layer and attempts to reconstruct the original input. An advantage of the\nsemi-supervised variant is that the decoder can be used as a generative model by providing both the target label and by\nsampling from the latent layer. Given that the prior distribution of the latent layer is Gaussian, we can simply sample\n6\nAugmenting Variational Autoencoders with Sparse Labels:\nA uniﬁed framework for unsupervised, semi-(un)supervised, and supervised learning\nA PREPRINT\nfrom a normal distribution to feed as an input to the decoder. Depending on where we sample the normal distribution,\nthe digit which is generated will be a representation of a different region of the training data. In other words, we can\nseparate both class and style. This is demonstrated in Figure 2, which illustrates the range of styles for each digit the\nmodel has learned.\nFigure 2: Digits generated by our Semi-Supervised model when trained on MNIST\n3.2\nFashion-MNIST\nFashion-MNIST [7] is an image dataset published by Zalando. It is designed as a drop-in replacement of MNIST, i.e.\nexactly like the original MNIST dataset, Fashion-MNIST contains of 60, 000 training (and 10, 000 testing) 28 × 28\ngrey-scale images with a total of 10 classes. However, there is much more variability within a given Fashion-MNIST\nclass than there is within a given MNIST class. For instance, the individual samples of the class ’Ankle Boot’ vary\nmuch more wildly than any digit in the MNIST dataset. As a consequence, Fashion-MNIST is a much more challenging\ndataset than MNIST, and hence serves as more realistic proxy to evaluate model performance; especially as most\nmodern image classiﬁcation models can almost perfectly solve MNIST. At the same time, Fashion-MNIST preserves\nthe big advantage of MNIST: It is still a small dataset that allows rapid training and experimentation when researching\nnew models.\n3.2.1\nSemi-Supervised performance\nThe set-up for the semi-supervised classiﬁcation task is the same as with the original MNIST dataset. That is, only the\nlabels for a subset of all samples the dataset are retained. The models ESD and ESCNN are then only trained on that\nsubset, whilst the SSD and SSCNN models are trained on all the samples, but only make use of the labels of the subset.\nThe other samples are treated as unlabeled. The results are shown in Table 7.\nTable 7: Semi-Supervised Fashion-MNIST classiﬁcation results\nmodel\n100 labels\n1, 000 labels\naccuracy\nlog loss\naccuracy\nlog loss\nSSD\n0.703 ± 0.007\n1.3 ± 0.2\n0.812 ± 0.004\n1.19 ± 0.07\nESD\n0.668 ± 0.008\n1.4 ± 0.1\n0.766 ± 0.008\n1.6 ± 0.1\nSSCNN\n0.724 ± 0.008\n1.19 ± 0.05\n0.836 ± 0.001\n0.83 ± 0.01\nESCNN\n0.66 ± 0.01\n1.5 ± 0.1\n0.803 ± 0.004\n1.20 ± 0.05\nThe difference in the accuracy scores between the semi-supervised and their supervised counterparts is almost identical\nas with the original MNIST. Unlike the original MNIST however, there is a clear improvement in the log loss from each\nof the semi-supervised models.\n7\nAugmenting Variational Autoencoders with Sparse Labels:\nA uniﬁed framework for unsupervised, semi-(un)supervised, and supervised learning\nA PREPRINT\n3.2.2\nDecoder as a regularizer\nThe results of training our model and an equivalent supervised model on the full dataset with every datapoint labeled\n(see section 2.5) are displayed in Table 9.\nTable 8: Supervised Fashion-MNIST classiﬁcation results\nmodel\nmean accuracy\nlog loss\nSSD\n0.877 ± 0.004\n0.4 ± 0.02\nESD\n0.79 ± 0.01\n4.8 ± 0.4\nSSCNN\n0.925 ± 0.001\n0.33 ± 0.025\nESCNN\n0.922 ± 0.0015\n0.33 ± 0.015\nAs can be seen, the fully connected (dense) model proﬁts massively from using the decoder as a regularizer. For the\nCNN model, the effect is less pronounced. However, the best accuracy is still achieved by SSCNN.\n3.2.3\nSemi-unsupervised learning\nThe anomaly detection task was set up in the same way as with the original MNIST dataset. One of the 10 classes\nwas designated anomalous, the others were declared normal. The model is trained on a subset of the samples from the\nremaining nine classes. The held out samples from these nine classes and the samples from the anomalous class are\nused as a validation set, with the performance of the results evaluated using the AUC score. The results are summarized\nin Table 9. The error bounds were obtained by rerunning every experiment ﬁve times.\nTable 9: Label assisted results of anomaly detection on Fashion-MNIST\nanomalous class\nAUC of SSD\nAUC of EUD\nT-shirt/top\n0.712 ± 0.002\n0.695 ± 0.002\nTrouser\n0.371 ± 0.005\n0.347 ± 0.002\nPullover\n0.826 ± 0.005\n0.8 ± 0.001\nDress\n0.462 ± 0.003\n0.416 ± 0.004\nCoat\n0.719 ± 0.007\n0.681 ± 0.001\nSandal\n0.413 ± 0.007\n0.34 ± 0.002\nShirt\n0.76 ± 0.002\n0.762 ± 0.001\nSneaker\n0.191 ± 0.006\n0.21 ± 0.002\nBag\n0.899 ± 0.009\n0.871 ± 0.004\nAnkle Boot\n0.525 ± 0.006\n0.547 ± 0.004\nSSD performs better than EUD for most anomalous classes. The only exceptions are the ’Shirt class’, where there is a\ntie between both models within the denoted error bar, while EUD wins for the ’Sneaker class’ and the ’Ankle Boot\nclass’; although the performance for the ’Sneaker class’ for both models is extremely bad.\nIn general, the model performance varies drastically depending on the anomalous class. While they perform very well\nfor classes like ’Pullover’ or ’Bag’, they struggle with the ’Sneaker’, ’Ankle Boot’, ’Trouser’ and ’Dress’ classes. This\ncan be understood by looking at a samples of each class (see Figure 4). In particular the ’Sneaker’ and ’Ankle Boot’\nclasses bear a resemblances, which explains why an anomaly detector trained on the ’Sneaker’ class has hard time\nto ﬂag the ’Ankle Boot’ samples as anomalous (and vice versa). The same is true, though to a lesser degree, for the\n’Trouser’ and ’Dress’ classes.\n3.2.4\nData Generation\nFigure 3 shows an example of generated Fashion-MNIST data, using the same strategy as described in section 3.1.4.\nOnce again there is an indication of the styles the model has learned to the embed within the latent space. The style\nitself is mostly captured in the shape of the class and also the position of highlighted features, for example the position\nof the straps on the sandals. Comparing the generated data to some of the examples images from Figure 4, it can be\nseen that the generated data lacks any of the detailing of the original images. Given that the dataset is more detailed\nthan the digits, but the model architecture is the same, this is likely due to the size of the latent layer and its lack of\ncapacity for storing these additional details.\n8\nAugmenting Variational Autoencoders with Sparse Labels:\nA uniﬁed framework for unsupervised, semi-(un)supervised, and supervised learning\nA PREPRINT\nFigure 3: Samples generated by our Semi-Supervised model when trained on Fashion-MNIST\n3.3\nHuman activity recognition (UCI-HAR)\nThe UCI-HAR dataset [9] contains 7, 352 samples, consisting of gyroscopic data recorded from humans, labeled with\none of the following six activities: walking, walking upstairs, walking downstairs, sitting, standing and laying.\nThe testing conducted on the UCI-HAR dataset was not as thorough as with MNIST and Fashion-MNIST, and we\nperformed only one run per test scenario (hence no error bars are given). The classiﬁcation results are presented in\nTable 10 and compare the performance of the semi-supervised model to its supervised equivalent.\nTable 10: UCI-HAR classiﬁcation results\nmodel\n100 labels\n1, 000 labels\n7, 352 labels\naccuracy\nlog loss\naccuracy\nlog loss\naccuracy\nlog loss\nSSR\n0.630\n4.000\n0.839\n0.503\n0.917\n0.310\nESRNN\n0.381\n1.538\n0.690\n0.820\n0.909\n0.428\nAgain, the semi-supervised model is able to outperform its supervised equivalent for each subset of labels. In this case,\nthe performance gain is particularly high when labels are in short supply, as demonstrated by the 24% improvement\nover the supervised model for the 100 labels test.\nThe performance of the model as an anomaly detector was also brieﬂy evaluated, using ’walking’ as the anomaly class.\nThe results, displayed in Table 11, again show an improvement in performance when providing an anomaly detector\nwith labels.\nTable 11: UCI-HAR anomaly detection results\nmodel\nAUC score\nSSRNN\n0.682\nESRNN\n0.641\n4\nDisentangled representations\nOne major application of VAEs is to ﬁnd low dimensional representations of real world data [11, 15]. For this task,\ndisentanglement is considered an important quality metric [15, 14]. Generally speaking, disentanglement attempts to\nquantify how well a particular framework is able to identify important yet independent generating factors of its dataset.\nFor this, multiple distinct metrics and benchmark data sets have been suggested, yet they have been shown to agree at\nleast on a qualitative level [11]. In order to benchmark our semi-supervised model, we chose to benchmark via the\n9\nAugmenting Variational Autoencoders with Sparse Labels:\nA uniﬁed framework for unsupervised, semi-(un)supervised, and supervised learning\nA PREPRINT\nbetaVAE score [12] on the Small-NORB data set [13]. Our scores are calculated based on the latent layer (but not the π\nlayer), and are shown in Table 12.\nThe network architecture is described in the appendix, Table 16 and is similar to 14 with two modiﬁcations: (a) the\nlabels of the data set were incorporated using multiple cross-entropy loss terms (and one-hot sigmoid π layers) for each\nof the four dimensions, (b) in this architecture the π layers are intended to only function as an additional and sparse loss\nterm on the latent layer, and thus is not forwarded to the decoder (no connection between π and ’Merge’ in Figure 1a).\nLatter step is necessary, since otherwise the network could bypass the latent layer using the π layer, while maintaining\nreconstruction quality.\nThere are two hyperparameters, α, the overall weight of the supervised cross-entropy term (equ. 5) and βnorm the\noverall weight of the KL-divergence term. Both parameters were kept ﬁxed for all cases of this experiment. They were\noptimized for maximum disentanglement in the completely unsupervised case. Note that this puts all other cases at an\ndisadvantage, since their optimal hyperparameters presumably differ from the unsupervised case - but this is to emulate\na real life scenario in which labeled datapoints are either sparse and thus cannot be used for hyperparameter tuning. We\nempirically found that good results are achieved when α is selected such that its average contribution to the total loss is\nabout of the same order of magnitude as the reconstruction loss (after training converges). The optimal βnorm was\nfound to be rather small (0.25), in accordance with [11]. For the betaVAE score itself, we used a plain logistic regressor.\nEach datapoint for this classiﬁer was based on the averaged absolute differences between the representations of 2 × 64\nbatches. The classiﬁer was trained and tested with 2048 datapoints each.\nThe ﬁnal results are shown in Table 12: We always used about 38, 000 unlabeled datapoints, but augmented training\nby various amounts of labeled datapoints. As expected our approach can outperform both purely unsupervised and a\nsupervised scenarios signiﬁcantly. Even a relatively small amount of labeled datapoints (300, ∼1%) seems sufﬁcient.\nIt should be noted, that for few (around 100) labels there is a small, yet statistically signiﬁcant, decrease in the betaVAE\nscore. This could be due to the aforementioned fact, that the hyperparameters used where optimized for an unsupervised\nscenario, yet 100 labels were not sufﬁcient to offset this disadvantage.\nTable 12: betaVAE score of the representations generated by our semi-supervised model on varying label availability.\nAll results were obtained using the same hyperparameters, which were optimized for the ﬁrst row.\n# unlabeled data\n# labeled data\nbetaVAE score\n38, 000\n0\n82 ± 0.7\n38, 000\n100\n76 ± 1.6\n38, 000\n300\n83 ± 1.8\n38, 000\n1, 000\n92 ± 0.6\n38, 000\n3, 000\n95 ± 0.6\n0\n1, 000\n87 ± 0.8\n5\nFuture work\nIt would be interesting to see how much larger networks would beneﬁt from the suggested regularization technique. For\ninstance, the same technique could directly be applied to state-of-the-art computer vision networks like [10]. We leave\nthis avenue for future investigation.\nWhile the results of the semi-unpervised learning already look promising, this approach so far makes no use of an\nadditional input that labels could provide: incorporating user feedback by labeling a false-positive and false-negative\ndetection as such, with the goal of suppressing future false-positive/false-negative detections. In principle, our model\narchitecture should allow to incorporate such feedback.\nOne way to achieve this would be as follows: Prepare new classes for false-positive and false-negative anomaly\ndetections. In the beginning, there will be no samples in these classes. However, once a sufﬁcient amount of false-\npositive/false-negative detections accumulated, the model is re-trained. The anomaly score x could then be heuristically\nadjusted, for instance:\nx →1 −pfp\n1 −pfn\n· x\n(6)\nwhere pfp and pfn are the false-positive and false-negative probabilities, respectively, outputted by the model at\ninference time.\n10\nAugmenting Variational Autoencoders with Sparse Labels:\nA uniﬁed framework for unsupervised, semi-(un)supervised, and supervised learning\nA PREPRINT\nThis treatment would even work when there are no labels available except the false-positive/false-negative assignments.\n6\nConclusions\nWith one of the most common issues associated with training machine learning models being the availability of training\ndata, semi-supervised models present the perfect opportunity to take advantage of every available scrap of data. This is\na particularly valuable improvement in the supervised domain, given the availability of unlabeled data in comparison to\nlabeled data.\nThe value added by a semi-supervised approach is even greater when considering such a model in the context of failure\nprediction, given that with a purely supervised approach, the dataset would consist entirely of failure events. Depending\non the system in question, such a dataset could take decades to collect; an obstacle which would often make a predictive\nsystem unobtainable. Taking advantage of the often abundant and easy to produce unlabeled data, this semi-supervised\napproach demonstrates the ability to converge towards accurate predictions on only a fraction of the labels.\nThe versatility of the semi-supervised model proposed in this paper delivers concrete improvements across the entire\nspectrum of label availability.\nWithin the labeled domain, the value added by a semi-supervised approach is even greater when considering such a\nmodel in the context of failure prediction. With a purely supervised approach, a training dataset would consist entirely\nof failure events, requiring the system in question to fail hundreds if not thousands of times to gather a sizeable dataset.\nDepending on the system in question, collecting such a dataset from scratch could take decades; an obstacle which can\noften make a predictive system unobtainable. Taking advantage of the often abundant and easy to produce unlabeled\ndata, this semi-supervised approach demonstrates the ability to converge towards an accurate predictive system on only\na fraction of the labels.\nIn addition to reducing the time to deployment, the model offers further beneﬁts in the supervised learning domain, able\nto outperform equivalent classiﬁers due the regularizing effect of the decoder and its associated reconstruction task.\nIn the purely unsupervised domain, the model achieves identical performance to a VAE, yet demonstrates a huge\nincrease of performance with a tiny fraction of labels. Traditionally, a VAE must ﬁnd a suitable dense representation of\nthe system it’s modelling in the latent layer. With the introduction of labels, the latent activations must not only embed a\nrepresentation of the system, but also a classiﬁcation of the system state. Ultimately, this additional information results\nin improved embedding of the system state, not only enabling classiﬁcations, but improving reconstructions. In short,\nthe labels provide the model with a better understanding of the system which it is reconstructing.\nReferences\n[1] Diederik P. Kingma, Danilo J. Rezende, Shakir Mohamed, Max Welling Semi-supervised Learning with Deep\nGenerative Models In Advances in Neural Information Processing Systems, 3581-3589, 2014.\n[2] Diederik P. Kingma, Max Welling Auto-Encoding Variational Bayes In Proceedings of the 2nd International\nConference on Learning Representations (ICLR), 2014.\n[3] Matthew J.F. Willets, Stephen J. Roberts, Christopher C. Holmes Semi-Unsupervised Learning with Deep\nGenerative Models: Clustering and Classifying using Ultra-Sparse Labels arXiv:1901.08560 2019-01-24\n[4] Matthew J.F. Willets, Aiden Doherty, Stephen J. Roberts, Chris Holmes Semi-Unsupervised Learning of Human\nActivity using Deep Generative models arXiv:1810.12176 2018-10-29\n[5] Felix Berkhahn, Richard Keys, Wajih Ouertani, Nikhil Shetty One model to rule them all https:// relayr.io/ blog/\none-model-to-rule-them-all/ 2018-09-21\n[6] Y. LeCun, C.Cortes MNIST handwritten digit database http://yann.lecun.com/exdb/mnist/\n[7] Han Xiao and Kashif Rasul and Roland Vollgraf, Fashion-MNIST: a Novel Image Dataset for Benchmarking\nMachine Learning Algorithms, arXiv cs.LG/1708.07747 2017-08-28\n[8] Geoffrey Hinton, Nitish Srivastava, Kevin Swersky Overview of mini-batch gradient descent https:// www.cs.\ntoronto.edu/ ~tijmen/ csc321/ slides/ lecture_slides_lec6.pdf\n[9] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz, A Public Domain Dataset\nfor Human Activity Recognition Using Smartphones 21th European Symposium on Artiﬁcial Neural Networks,\nComputational Intelligence and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April\n11\nAugmenting Variational Autoencoders with Sparse Labels:\nA uniﬁed framework for unsupervised, semi-(un)supervised, and supervised learning\nA PREPRINT\n[10] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, Kaiming He Aggregated Residual Transformations for\nDeep Neural Networks IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017. Honolulu,\nHI, USA, 21-26 July 2017\n[11] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Rätsch, Sylvain Gelly, Bernhard Schölkopf, Olivier\nBachem Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations\narXiv:1811.12359 29-11-2018\n[12] Irina Higgins, Loïc Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed,\nAlexander Lerchner beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework\nInternational Conference on Learning Representations 2017\n[13] Yann LeCun, Fu Jie Huang, Léon Bottou Learning methods for generic object recognition with invariance to\npose and lighting Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition 2004. CVPR 2004.\n[14] Yoshua Bengio, Aaron Courville, Pascal Vincent Representation learning: A review and new perspectives IEEE\ntransactions on pattern analysis and machine intelligence 5(8): 1798–1828, 2013\n[15] Michael Tschannen, Olivier Bachem, Mario Lucic Recent advances in autoencoder-based representation learning\narXiv:1812.05069 12-12-2018\n[16] Ingo Kossyk, Zoltán-Csaba Márton Discriminative regularization of the latent manifold of variational auto-\nencoders Journal of Visual Communication and Image Representation 15-03-2019\nA\nNetwork architectures and training\nA.1\nMNIST and Fashion-MNIST: Model 1, FCN\nWe used the raw pixels, nromalized to (0, 1) as input, corresponing to size 28·28 = 784 feature vectors. The architecture\nis shown below in Table 13. All encoder and decoder layers are simply stacked. The latent layers (µ and σ for a\ngaussian prior) are forked from the last encoder layer, and merged together with the resampled latent layer as input to\nthe decoder. The model is trained with RMSprop [8] without decay and a momentum parameter of 0.9. If not explicitly\nmentioned otherwise, the learning rate was set to 0.0005.\nTable 13: Fully connected network architecture. The input image corresponds to a 28x28 image reshaped into a single\nfeature vector.\nlayer type\ndimensions\ncomments\nencoder\ninput layer\n784\nfully connected\n1024\nrelu activation\nfully connected\n1024\nrelu activation\nlatent layer\nfully connected\n2\nlinear activation; latent gaussian mean\nfully connected\n2\nlinear activation; latent gaussian variance\nfully connected\n10\nsoftmax activation; class prediction\ndecoder\nfully connected\n1024\nrelu activation\nfully connected\n1024\nrelu activation\noutput layer\n784\nsigmoid activation\nA.2\nMNIST and Fashion-MNIST: Model 2, CNN\nThe images were rescaled to (0, 1) but not reshaped. Here we used a series of convolutional layers as detailed in Table\nA.1. The last dimension in the dimensions column corresponds to the feature dimension, while the ﬁrst two correspond\nto the image dimensions. Again, all encoder and decoder layers were simply stacked, whereas the latent and π layer\nwere forked from the last encoder layer. The resampled latent layer and π layer were concatenated as input for the\ndecoder. The model is trained with RMSprop without decay and a momentum parameter of 0.9. If not explicitly\nmentioned otherwise, the learning rate was set to 0.0005.\n12\nAugmenting Variational Autoencoders with Sparse Labels:\nA uniﬁed framework for unsupervised, semi-(un)supervised, and supervised learning\nA PREPRINT\nTable 14: Convolutional network architecture (CNN). The input shape corresponds to a single greysclae image (x-\naxis,y-axis, channel). BN is an abbriavtaion for batch normalization layer, ConvCNN for a transposed convolution\nlayer.\nlayer type\ndimensions\ncomments\nencoder\ninput layer\n(28, 28, 1)\nCNN\n(28, 28, 64)\nkernel 3 × 3; stride 1\nBN\n(28, 28, 64)\nwith relu activation\nCNN\n(28, 28, 64)\nkernel 3 × 3; stride 1\nBN\n(28, 28, 64)\nwith relu activation\nCNN\n(14, 14, 64)\nkernel 3 × 3; stride 2\nBN\n(14, 14, 64)\nwith relu activation\nFlatten\n12544\nfully connected\n512\nBN\n512\nwith relu activation\nDropout\n512\ndropout rate = 0.5\nlatent layer\nfully connected\n2\nlinear activation; latent gaussian mean\nfully connected\n2\nlinear activation; latent gaussian variance\nfully connected\n10\nsoftmax activation; class prediction\ndecoder\nfully connected\n12544\nBN\n12544\nwith relu activation\nDropout\n12544\ndropout rate = 0.5\nReshape\n(14, 14, 64)\nConvCNN\n(14, 14, 64)\nkernel 3 × 3; stride 1\nBN\n(14, 14, 64)\nwith relu activation\nConvCNN\n(14, 14, 64)\nkernel 3 × 3; stride 1\nBN\n(14, 14, 64)\nwith relu activation\nConvCNN\n(28, 28, 64)\nkernel 3 × 3; stride 2\nBN\n(28, 28, 64)\nwith relu activation\nConvCNN (output)\n(28, 28, 1)\nkernel 1 × 1; stride 1\nA.3\nUCI-HAR dataset: RNN\nThe recurrent VAE ﬂavor was applied on the UCI HAR dataset [9]. We used a look-back dimension of 128 with the full\narchitecture described in the appendix in Table 15. The ﬁrst dimension in the dimensions column corresponds to the\nlook-back dimension, while the second corresponds to the feature dimension. The latent layers are forked from the last\nencoder layer, and concatenated prior to the ﬁrst decoder layer. The model is trained with RMSprop without decay and\na momentum parameter of 0.9. If not explicitly mentioned otherwise, the learning rate was set to 0.001.\nTable 15: RNN network architecture.The input shape corresponds to a single time series with (128 time steps, 6\nfeatures).\nlayer type\ndimensions\ncomments\nencoder\ninput layer\n(128, 6)\nfully connected along feature dim\n(128, 40)\nrelu activation\nLSTM\n(128, 40)\nfully connected along feature dim\n(128, 30)\nrelu activation\nlatent layer\nfully connected\n(128, 2)\nlinear activation; latent gaussian mean\nfully connected\n(128, 2)\nlinear activation; latent gaussian variance\nfully connected\n(128, 6)\nsoftmax activation; class prediction\ndecoder\nfully connected along feature dim\n(128, 30)\nrelu activation\nLSTM\n(128, 40)\nfully connected along feature dim\n(128, 40)\nrelu activation\noutput\nfully connected along feature dim\n(128, 6)\nlinear activation; gaussian mean µ\nfully connected along feature dim\n(128, 6)\nsoftplus activation; gaussian variance σ\n13\nAugmenting Variational Autoencoders with Sparse Labels:\nA uniﬁed framework for unsupervised, semi-(un)supervised, and supervised learning\nA PREPRINT\nTable 16: CNN network architecture used for generating represenations of the Small-NORB data set. The input shape\ncorresponds to a single stero image pair (x-axis, y-axis, left/right). BN is an abbriavtaion for batch normalization layer,\nConvCNN for a transposed convolution layer.\nlayer type\ndimensions\ncomments\nencoder\ninput layer\n(96, 96, 2)\nCNN\n(48, 48, 32)\nkernel 7 × 7; stride 2\nBN\n(48, 48, 32)\nwith relu activation\nCNN\n(24, 24, 32)\nkernel 7 × 7; stride 2\nBN\n(24, 24, 32)\nwith relu activation\nCNN\n(12, 12, 64)\nkernel 7 × 7; stride 2\nBN\n(12, 12, 64)\nwith relu activation\nCNN\n(6, 6, 64)\nkernel 7 × 7; stride 2\nBN\n(6, 6, 64)\nwith relu activation\nFlatten\n2304\nfully connected\n256\nBN\n256\nwith relu activation\nDropout\n256\ndropout rate = 0.5\nlatent layer\nfully connected\n32\nlinear activation; latent gaussian mean\nfully connected\n32\nlinear activation; latent gaussian variance\nfully connected\n5\nsoftmax activation; one-hot prediction (category)\nfully connected\n9\nsoftmax activation; one-hot prediction (elevation)\nfully connected\n18\nsoftmax activation; one-hot predictioon (azimuth)\nfully connected\n6\nsoftmax activation; one-hot prediction (lighting)\ndecoder\nfully connected\n2304\nbased on resampled gaussian latent layers\nBN\n2304\nwith relu activation\nDropout\n2304\ndropout rate = 0.5\nReshape\n(6, 6, 64)\nConvCNN\n(12, 12, 64)\nkernel 7 × 7; stride 2\nBN\n(12, 12, 64)\nwith relu activation\nConvCNN\n(24, 24, 64)\nkernel 7 × 7; stride 2\nBN\n(24, 24, 64)\nwith relu activation\nConvCNN\n(48, 48, 32)\nkernel 7 × 7; stride 2\nBN\n(48, 48, 32)\nwith relu activation\nConvCNN\n(96, 96, 32)\nkernel 7 × 7; stride 2\nBN\n(96, 96, 32)\nwith relu activation\nConvCNN (output)\n(96, 96, 2)\nkernel 7 × 7; stride 1\nA.4\nSmall-NORB dataset: CNN\nThe input images were rescaled to (0, 1). Each stero image pair was stacked into a single feature vector of shape\n(96, 96, 2). The encoder and decoder are simply stacked and the full architecture is shown in Table 16. For each of the\nfour generating factors of this data set (category, elevation, azimuth and lighting) a sperate π layer was added after the\nencoder. The latent layers (µ and σ) of a gaussian prior are also added on top of the encoder. By contrast to the other\nmodels, only the resampled latent layer is used for the decoder. The model is trained with RMSprop without decay and\na momentum parameter of 0.9. If not explicitly mentioned otherwise, the learning rate was set to 0.001.\n14\nAugmenting Variational Autoencoders with Sparse Labels:\nA uniﬁed framework for unsupervised, semi-(un)supervised, and supervised learning\nA PREPRINT\nB\nSamples from Fashion-MNIST data set\nFigure 4: This ﬁgure shows some examples from the Fashion-MNIST dataset. Every class always corresponds to three\nconsecutive rows. The classes are (from top to down): T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker,\nBag and Ankle Boot.\n15\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.NE",
    "stat.ML"
  ],
  "published": "2019-08-08",
  "updated": "2019-11-14"
}