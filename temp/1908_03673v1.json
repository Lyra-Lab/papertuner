{
  "id": "http://arxiv.org/abs/1908.03673v1",
  "title": "Recent Advances in Deep Learning for Object Detection",
  "authors": [
    "Xiongwei Wu",
    "Doyen Sahoo",
    "Steven C. H. Hoi"
  ],
  "abstract": "Object detection is a fundamental visual recognition problem in computer\nvision and has been widely studied in the past decades. Visual object detection\naims to find objects of certain target classes with precise localization in a\ngiven image and assign each object instance a corresponding class label. Due to\nthe tremendous successes of deep learning based image classification, object\ndetection techniques using deep learning have been actively studied in recent\nyears. In this paper, we give a comprehensive survey of recent advances in\nvisual object detection with deep learning. By reviewing a large body of recent\nrelated work in literature, we systematically analyze the existing object\ndetection frameworks and organize the survey into three major parts: (i)\ndetection components, (ii) learning strategies, and (iii) applications &\nbenchmarks. In the survey, we cover a variety of factors affecting the\ndetection performance in detail, such as detector architectures, feature\nlearning, proposal generation, sampling strategies, etc. Finally, we discuss\nseveral future directions to facilitate and spur future research for visual\nobject detection with deep learning. Keywords: Object Detection, Deep Learning,\nDeep Convolutional Neural Networks",
  "text": "Recent Advances in Deep Learning for Object Detection\nXiongwei Wua,∗, Doyen Sahooa, Steven C.H. Hoia,b\naSchool of Information System, Singapore Management University\nbSalesforce Research Asia\nAbstract\nObject detection is a fundamental visual recognition problem in computer vision and has been widely studied in the past decades.\nVisual object detection aims to ﬁnd objects of certain target classes with precise localization in a given image and assign each\nobject instance a corresponding class label. Due to the tremendous successes of deep learning based image classiﬁcation, object\ndetection techniques using deep learning have been actively studied in recent years. In this paper, we give a comprehensive survey\nof recent advances in visual object detection with deep learning. By reviewing a large body of recent related work in literature,\nwe systematically analyze the existing object detection frameworks and organize the survey into three major parts: (i) detection\ncomponents, (ii) learning strategies, and (iii) applications & benchmarks. In the survey, we cover a variety of factors affecting\nthe detection performance in detail, such as detector architectures, feature learning, proposal generation, sampling strategies, etc.\nFinally, we discuss several future directions to facilitate and spur future research for visual object detection with deep learning.\nKeywords: Object Detection, Deep Learning, Deep Convolutional Neural Networks\n1. Introduction\nIn the ﬁeld of computer vision, there are several fundamen-\ntal visual recognition problems: image classiﬁcation [1], object\ndetection and instance segmentation [2, 3], and semantic seg-\nmentation [4] (see Fig. 1). In particular, image classiﬁcation\n(Fig 1.1(a)), aims to recognize semantic categories of objects in\na given image. Object detection not only recognizes object cat-\negories, but also predicts the location of each object by a bound-\ning box (Fig. 1(b)). Semantic segmentation (Fig. 1(c)) aims to\npredict pixel-wise classiﬁers to assign a speciﬁc category label\nto each pixel, thus providing an even richer understanding of an\nimage. However, in contrast to object detection, semantic seg-\nmentation does not distinguish between multiple objects of the\nsame category. A relatively new setting at the intersection of\nobject detection and semantic segmentation, named “instance\n∗Corresponding author\nEmail addresses: xwwu.2015@phdis.smu.edu.sg (Xiongwei Wu),\ndoyens.smu.edu.sg (Doyen Sahoo), chhoi@smu.edu.sg (Steven\nC.H. Hoi)\n(a) Image Classiﬁcation\n(b) Object Detection\n(c) Semantic Segmentation\n(d) Instance Segmentation\nFigure 1: Comparison of different visual recognition tasks in computer vision.\n(a) “Image Classiﬁcation” only needs to assign categorical class labels to the\nimage; (b) “Object detection” not only predict categorical labels but also lo-\ncalize each object instance via bounding boxes; (c) “Semantic segmentation”\naims to predict categorical labels for each pixel, without differentiating object\ninstances; (d) “Instance segmentation”, a special setting of object detection,\ndifferentiates different object instances by pixel-level segmentation masks.\nsegmentation” (Fig. 1(d)), is proposed to identify different ob-\njects and assign each of them a separate categorical pixel-level\nPreprint submitted to Elsevier\nAugust 13, 2019\narXiv:1908.03673v1  [cs.CV]  10 Aug 2019\nmask. In fact, instance segmentation can be viewed as a spe-\ncial setting of object detection, where instead of localizing an\nobject by a bounding box, pixel-level localization is desired. In\nthis survey, we direct our attention to review the major efforts\nin deep learning based object detection. A good detection al-\ngorithm should have a strong understanding of semantic cues\nas well as the spatial information about the image.\nIn fact,\nobject detection is the basic step towards many computer vi-\nsion applications, such as face recognition [5, 6, 7], pedestrian\ndetection [8, 9, 10], video analysis [11, 12], and logo detec-\ntion [13, 14, 15].\nIn the early stages, before the deep learning era, the pipeline\nof object detection was divided into three steps: i) proposal\ngeneration; ii) feature vector extraction; and iii) region clas-\nsiﬁcation.\nDuring proposal generation, the objective was to\nsearch locations in the image which may contain objects. These\nlocations are also called regions of interest (roi).\nAn intu-\nitive idea is to scan the whole image with sliding windows\n[16, 17, 18, 19, 20].\nIn order to capture information about\nmulti-scale and different aspect ratios of objects, input images\nwere resized into different scales and multi-scale windows were\nused to slide through these images. During the second step, on\neach location of the image, a ﬁxed-length feature vector was\nobtained from the sliding window, to capture discriminative se-\nmantic information of the region covered. This feature vector\nwas commonly encoded by low-level visual descriptors such\nas SIFT(Scale Invariant Feature Transform) [21], Haar [22],\nHOG(Histogram of Gradients) [19] or SURF(Speeded Up Ro-\nbust Features) [23], which showed a certain robustness to scale,\nillumination and rotation variance. Finally, in the third step,\nthe region classiﬁers were learned to assign categorical la-\nbels to the covered regions. Commonly, support vector ma-\nchines(SVM) [24] were used here due to their good perfor-\nmance on small scale training data. In addition, some classi-\nﬁcation techniques such as bagging [25], cascade learning [20]\nand adaboost [26] were used in region classiﬁcation step, lead-\ning to further improvements in detection accuracy.\nMost of the successful traditional methods for object de-\ntection focused on carefully designing feature descriptors to\nobtain embedding for a region of interest. With the help of\ngood feature representations as well as robust region classi-\nﬁers, impressive results [27, 28] were achieved on Pascal VOC\ndataset [29] (a publicly available dataset used for benchmark-\ning object detection). Notably, deformable part based machines\n(DPMs) [30], a breakthrough detection algorithm, were 3-time\nwinners on VOC challenges in 2007, 2008 and 2009. DPMs\nlearn and integrate multiple part models with a deformable\nloss and mine hard negative examples with a latent SVM for\ndiscriminative training.\nHowever, during 2008 to 2012, the\nprogress on Pascal VOC based on these traditional methods had\nbecome incremental, with minor gains from building compli-\ncated ensemble systems. This showed the limitations of these\ntraditional detectors. Most prominently, these limitations in-\ncluded: (i) during proposal generation, a huge number of pro-\nposals were generated, and many of them were redundant; this\nresulted in a large number of false positives during classiﬁca-\ntion. Moreover, window scales were designed manually and\nheuristically, and could not match the objects well; (ii) fea-\nture descriptors were hand-crafted based on low level visual\ncues [31, 32, 23], which made it difﬁcult to capture repre-\nsentative semantic information in complex contexts. (iii) each\nstep of the detection pipeline was designed and optimized sep-\narately, and thus could not obtain a global optimal solution for\nthe whole system.\nAfter the success of applying deep convolutional neural net-\nworks(DCNN) for image classiﬁcation [33, 1], object detection\nalso achieved remarkable progress based on deep learning tech-\nniques [34, 2]. The new deep learning based algorithms out-\nperformed the traditional detection algorithms by huge mar-\ngins.\nDeep convolutional neural network is a biologically-\ninspired structure for computing hierarchical features.\nAn\nearly attempt to build such a hierarchical and spatial-invariant\nmodel for image classiﬁcation was “neocognitron” [35] pro-\nposed by Fukushima. However, this early attempt lacked ef-\nfective optimization techniques for supervised learning. Based\non this model, Lecun et al. [36] optimized a convolutional neu-\n2\nAlexNet\n(Krizhevsky et al.)\nOverFeat\n(Sermanet et al.)\nGoogleNet\n(Szegedy et al.)\nVGGNet\n(Simonyan and \nZisserman)\nR-CNN\n(Girshick et al.)\nFast R-CNN\n(Girshick et al.)\nResNet\n(He et al.)\nFaster R-CNN\n(Ren et al.)\nFPN\n(Lin et al.)\nYOLO\n(Redmon et al.)\nSSD\n(Liu et al.)\nResNet v2\n(He et al.)\nR-FCN\n(Dai et al.)\nResNeXt\n(Lin et al.)\nDenseNet\n(Huang et al.)\nDPN\n(Chen et al.)\nYOLO9000\n(Redmon and \nFarhadi)\nHourglass\n(Newell et al.)\nMobileNet\n(Howard et al.)\nDCN\n(Dai et al.)\nRetinaNet\n(Lin et al.)\nMask R-CNN\n(He et al.)\nRefineDet\n(Zhang et al.)\nCascade RCNN\n(Cai et al.)\nNASNet\n(Zoph et al.)\nCornerNet\n(Law and Deng)\nFSAF\n(Zhu et al.)\nSENet\n(Hu et al.)\nExtremeNet\n(Zhou et al.)\nNAS-FPN\n(Ghiasi et al.)\nDetnas\n(Chen et al.)\nFCOS\n(Tian et al.)\nCenterNet\n(Duan et al.)\nEfficientNet\n(Tan and Le)\nFigure 2: Major milestone in object detection research based on deep convolution neural networks since 2012. The trend in the last year has been designing object\ndetectors based on anchor-free(in red) and AutoML(in green) techniques, which are potentially two important research directions in the future.\nral network by stochastic gradient descent (SGD) via back-\npropagation and showed competitive performance on digit\nrecognition. After that, however, deep convolutional neural net-\nworks were not heavily explored, with support vector machines\nbecoming more prominent. This was because deep learning had\nsome limitations: (i) lack of large scale annotated training data,\nwhich caused overﬁtting; (ii) limited computation resources;\nand (iii) weak theoretical support compared to SVMs. In 2009,\nJia et al. [37] collected a large scale annotated image dataset\nImageNet which contained 1.2M high resolution images, mak-\ning it possible to train deep models with large scale training\ndata. With the development of computing resources on parallel\ncomputing systems(such as GPU clusters), in 2012 Krizhevsky\net al. [33] trained a large deep convolutional model with Im-\nageNet dataset and showed signiﬁcant improvement on Large\nScale Visual Recognition Challenge(ILSVRC) compared to all\nother approaches.\nAfter the success of applying DCNN for\nclassiﬁcation, deep learning techniques were quickly adapted\nto other vision tasks and showed promising results compared to\nthe traditional methods.\nIn contrast to hand-crafted descriptors used in traditional de-\ntectors, deep convolutional neural networks generate hierarchi-\ncal feature representations from raw pixels to high level seman-\ntic information, which is learned automatically from the train-\ning data and shows more discriminative expression capability\nin complex contexts. Furthermore, beneﬁting from the pow-\nerful learning capacity, a deep convolutional neural network\ncan obtain a better feature representation with a larger dataset,\nwhile the learning capacity of traditional visual descriptors are\nﬁxed, and can not improve when more data becomes available.\nThese properties made it possible to design object detection al-\ngorithms based on deep convolutional neural networks which\ncould be optimized in an end-to-end manner, with more power-\nful feature representation capability.\nCurrently, deep learning based object detection frameworks\ncan be primarily divided into two families: (i) two-stage detec-\ntors, such as Region-based CNN (R-CNN) [2] and its variants\n[38, 34, 39] and (ii) one-stage detectors, such as YOLO [40] and\nits variants [41, 42]. Two-stage detectors ﬁrst use a proposal\ngenerator to generate a sparse set of proposals and extract fea-\ntures from each proposal, followed by region classiﬁers which\npredict the category of the proposed region. One-stage detec-\ntors directly make categorical prediction of objects on each lo-\ncation of the feature maps without the cascaded region classiﬁ-\ncation step. Two-stage detectors commonly achieve better de-\ntection performance and report state-of-the-art results on public\nbenchmarks, while one-stage detectors are signiﬁcantly more\ntime-efﬁcient and have greater applicability to real-time object\n3\nObject Detection\nDetection Components\nLearning Strategy\nApplications & \nBenchmarks\nDetection Settings\nBackbone Architecture\nProposal Generation\nFeature Representation\nTraining Stage\nTesting Stage\nApplications\nPublic Benchmarks\nBounding Box\nPixel Mask\nVGG16,ResNet,DenseNet\nMobileNet, ResNeXt\nDetNet, Hourglass Net\nMulti-scale Feature Learning\nRegion Feature Encoding\nContextual Reasoning\nDeformable Feature Learning\nTraditional Computer Vision Methods\nAnchor-based Methods\nKeypoint-based Methods\nOther Methods\nData Augmentation\nImbalance Sampling\nLocalization Refinement\nCascade Learning\nFace Detection\nPedestrian Detection\nOthers\nMSCOCO, Pascal VOC, \nOpen Images\nFDDB, WIDER FACE\nKITTI, ETH, CityPersons\nTwo-Stage Detectors\nOne-Stage Detectors\nDuplicate Removal\nModel Acceleration\nOthers\nOthers\nDetection Paradigms\nFigure 3: Taxonomy of key methodologies in this survey. We categorize various contributions for deep learning based object detection into three major categories:\nDetection Components, Learning Strategies, Applications and Benchmarks. We review each of these categories in detail.\ndetection. Figure 2 also illustrates the major developments and\nmilestones of deep learning based object detection techniques\nafter 2012. We will cover basic ideas of these key techniques\nand analyze them in a systematic manner in the survey.\nThe goal of this survey is to present a comprehensive un-\nderstanding of deep learning based object detection algorithms.\nFig.\n3 shows a taxonomy of key methodologies to be cov-\nered in this survey. We review various contributions in deep\nlearning based object detection and categorize them into three\ngroups: detection components, learning strategies, and appli-\ncations & benchmarks. For detection components, we ﬁrst in-\ntroduce two detection settings: bounding box level(bbox-level)\nand pixel mask level(mask-level) localization. Bbox-level algo-\nrithms require to localize objects by rectangle bounding boxes,\nwhile more precise pixel-wise masks are required to segment\nobjects in mask-level algorithms. Next, we summarize the rep-\nresentative frameworks of two detection families: two-stage de-\ntection and one-stage detection. Then we give a detailed survey\nof each detection component, including backbone architecture,\nproposal generation and feature learning. For learning strate-\ngies, we ﬁrst highlight the importance of learning strategy of\ndetection due to the difﬁculty of training detectors, and then in-\ntroduce the optimization techniques for both training and test-\ning stages in detail. Finally, we review some real-world object\ndetection based applications including face detection, pedes-\ntrian detection, logo detection and video analysis. We also dis-\ncuss publicly available and commonly used benchmarks and\nevaluation metrics for these detection tasks. Finally we show\nthe state-of-the-art results of generic detection on public bench-\nmarks over the recent years.\nWe hope our survey can provide a timely review for re-\nsearchers and practitioners to further catalyze research on de-\ntection systems. The rest of the paper are organized as follows:\nin Section 2, we give a standard problem setting of object detec-\ntion. The details of detector components are listed in Section 3.\nThen the learning strategies are presented in Section 4. Detec-\ntion algorithms for real-world applications and benchmarks are\nprovided in Section 5 and Section 6. State-of-the-art results of\ngeneric detection are listed in Section 7. Finally, we conclude\nand discuss future directions in Section 8.\n4\n2. Problem Settings\nIn this section, we present the formal problem setting for ob-\nject detection based on deep learning. Object detection involves\nboth recognition (e.g., “object classiﬁcation”) and localization\n(e.g., “location regression”) tasks. An object detector needs to\ndistinguish objects of certain target classes from backgrounds\nin the image with precise localization and correct categorical la-\nbel prediction to each object instance. Bounding boxes or pixel\nmasks are predicted to localize these target object instances.\nMore formally, assume we are given a collection of N anno-\ntated images\nn\nx1, x2, ..., xN\no\n, and for ith image xi, there are\nMi objects belonging to C categories with annotations:\nyi =\nn\n(ci\n1, bi\n1), (ci\n2, bi\n2), ..., (ci\nMi, bi\nMi)\no\n(1)\nwhere ci\nj( ci\nj ∈C) and bi\nj (bounding box or pixel mask of the\nobject) denote categorical and spatial labels of j-th object in xi\nrespectively. The detector is f parameterized by θ. For xi, the\nprediction yi\npred shares the same format as yi:\nyi\npred =\nn\n(ci\npred1, bi\npred1), (ci\npred2, bi\npred2), ...)\no\n(2)\nFinally a loss function ℓis set to optimize detector as:\nℓ(x, θ) = 1\nN\nN\nX\ni=1\nℓ(yi\npred, xi, yi; θ) + λ\n2 ∥θ∥2\n2\n(3)\nwhere the second term is a regularizer, with trade-off parameter\nλ. Different loss functions such as softmax loss [38] and focal\nloss [43] impact the ﬁnal detection performance, and we will\ndiscuss these functions in Section 4.\nAt the time of evaluation, a metric called intersection-over-\nunion (IoU) between objects and predictions is used to evaluate\nthe quality of localization(we omit index i here):\nIoU(bpred, bgt) = Area(bpred\nT bgt)\nArea(bpred\nS bgt)\n(4)\nHere, bgt refers to the ground truth bbox or mask.\nAn IoU\nthreshold Ωis set to determine whether a prediction tightly cov-\ners the object or not(i.e. IoU ≥Ω; commonly researchers set\nΩ= 0.5). For object detection, a prediction with correct cate-\ngorical label as well as successful localization prediction (meet-\ning the IoU criteria) is considered as positive, otherwise it’s a\nnegative prediction:\nPrediction =\n\n\n\nPositive\ncpred = cgt and IoU(bpred, bgt) > Ω\nNegative\notherwise\n(5)\nFor generic object detection problem evaluation, mean average\nprecision(mAP) over C classes is used for evaluation, and in\nreal world scenarios such as pedestrian detection, different eval-\nuation metrics are used which will be discussed in Section 5. In\naddition to detection accuracy, inference speed is also an im-\nportant metric to evaluate object detection algorithms. Specif-\nically, if we wish to detect objects in a video stream (real-time\ndetection), it is imperative to have a detector that can process\nthis information quickly. Thus, the detector efﬁciency is also\nevaluated on Frame per second (FPS), i.e., how many images it\ncan process per second. Commonly a detector that can achieve\nan inference speed of 20 FPS, is considered to be a real-time\ndetector.\n3. Detection Components\nIn this section, we introduce different components of ob-\nject detection. The ﬁrst is about the choice of object detection\nparadigm. We ﬁrst introduce the concepts of two detection set-\ntings: bbox-level and mask-level algorithms. Then, We intro-\nduce two major object detection paradigms: two-stage detectors\nand one-stage detectors. Under these paradigms, detectors can\nuse a variety of deep learning backbone architectures, proposal\ngenerators, and feature representation modules.\n3.1. Detection Settings\nThere are two settings in object detection: i) vanilla object\ndetection (bbox-level localization) and ii) instance segmenta-\ntion (pixel-level or mask-level localization). Vanilla object de-\ntection has been more extensively studied and is considered as\n5\nthe traditional detection setting, where the goal is to localize ob-\njects by rectangle bounding boxes. In vanilla object detection\nalgorithms, only bbox annotations are required, and in evalua-\ntion, the IoU between predicted bounding box with the ground\ntruth is calculated to measure the performance. Instance seg-\nmentation is a relatively new setting and is based on traditional\ndetection setting. Instance segmentation requires to segment\neach object by a pixel-wise mask instead of a rough rectangle\nbounding box. Due to more precise pixel-level prediction, in-\nstance segmentation is more sensitive to spatial misalignment,\nand thus has higher requirement to process the spatial informa-\ntion. The evaluation metric of instance segmentation is almost\nidentical to the bbox-level detection, except that the IoU com-\nputation is performed on mask predictions. Though the two\ndetection settings are slightly different, the main components\nintroduced later can mostly be shared by the two settings.\n3.2. Detection Paradigms\nCurrent state-of-the-art object detectors with deep learning\ncan be mainly divided into two major categories: two-stage de-\ntectors and one-stage detectors. For a two-stage detector, in the\nﬁrst stage, a sparse set of proposals is generated; and in the\nsecond stage, the feature vectors of generated proposals are en-\ncoded by deep convolutional neural networks followed by mak-\ning the object class predictions. An one-stage detector does\nnot have a separate stage for proposal generation (or learning a\nproposal generation). They typically consider all positions on\nthe image as potential objects, and try to classify each region\nof interest as either background or a target object. Two-stage\ndetectors often reported state-of-the-art results on many public\nbenchmark datasets. However, they generally fall short in terms\nof lower inference speeds. One-stage detectors are much faster\nand more desired for real-time object detection applications, but\nhave a relatively poor performance compared to the two-stage\ndetectors.\n3.2.1. Two-stage Detectors\nTwo-stage detectors split the detection task into two stages:\n(i) proposal generation; and (ii) making predictions for these\nproposals. During the proposal generation phase, the detector\nwill try to identify regions in the image which may potentially\nbe objects. The idea is to propose regions with a high recall,\nsuch that all objects in the image belong to at least one of these\nproposed region. In the second stage, a deep-learning based\nmodel is used to classify these proposals with the right categor-\nical labels. The region may either be a background, or an object\nfrom one of the predeﬁned class labels . Additionally, the model\nmay reﬁne the original localization suggested by the proposal\ngenerator. Next, we review some of the most inﬂuential efforts\namong two-stage detectors.\nR-CNN [2] is a pioneering two-stage object detector pro-\nposed by Girshick et al. in 2014. Compared to the previous\nstate-of-the-art methods based on a traditional detection frame-\nwork SegDPM [44] with 40.4% mAP on Pascal VOC2010, R-\nCNN signiﬁcantly improved the detection performance and ob-\ntained 53.7% mAP. The pipeline of R-CNN can be divided into\nthree components: i) proposal generation, ii) feature extraction\nand iii) region classiﬁcation. For each image, R-CNN generates\na sparse set of proposals (around 2,000 proposals) via Selective\nSearch [45], which is designed to reject regions that can eas-\nily be identiﬁed as background regions. Then, each proposal is\ncropped and resized into a ﬁxed-size region and is encoded into\na (e.g. 4,096 dimensional) feature vector by a deep convolu-\ntional neural network, followed by a one-vs-all SVM classiﬁer.\nFinally the bounding box regressors are learned using the ex-\ntracted features as input in order to make the original proposals\ntightly bound the objects. Compared to traditional hand-crafted\nfeature descriptors, deep neural networks generate hierarchical\nfeatures and capture different scale information in different lay-\ners, and ﬁnally produce robust and discriminative features for\nclassiﬁcation. utilize the power of transfer learning, R-CNN\nadopts weights of convolutional networks pre-trained on Ima-\ngeNet. The last fully connected layer (FC layer) is re-initialized\nfor the detection task. The whole detector is then ﬁnetuned on\nthe pre-trained model. This transfer of knowledge from the Im-\nagenet dataset offers signiﬁcant performance gains. In addition,\nR-CNN rejects huge number of easy negatives before training,\n6\nwhich helps improve learning speed and reduce false positives.\nHowever, R-CNN faces some critical shortcomings: i) the\nfeatures of each proposal were extracted by deep convolutional\nnetworks separately (i.e., computation was not shared), which\nled to heavily duplicated computations. Thus, R-CNN was ex-\ntremely time-consuming for training and testing; ii) the three\nsteps of R-CNN (proposal generation, feature extraction and\nregion classiﬁcation) were independent components and the\nwhole detection framework could not be optimized in an end-\nto-end manner, making it difﬁcult to obtain global optimal so-\nlution; and iii) Selective Search relied on low-level visual cues\nand thus struggled to generate high quality proposals in com-\nplex contexts. Moreover, it is unable to enjoy the beneﬁts of\nGPU acceleration.\nInspired by the idea of spatial pyramid matching (SPM) [46],\nHe et al. proposed SPP-net [47] to accelerate R-CNN as well\nas learn more discriminative features. Instead of cropping pro-\nposal regions and feeding into CNN model separately, SPP-net\ncomputes the feature map from the whole image using a deep\nconvolutional network and extracts ﬁxed-length feature vectors\non the feature map by a Spatial Pyramid Pooling (SPP) layer.\nSPP partitions the feature map into an N ×N grid, for multiple\nvalues of N (thus allowing obtaining information at different\nscales), and performs pooling on each cell of the grid, to give a\nfeature vector. The feature vectors obtained from each N × N\ngrid are concatenated to give the representation for the region.\nThe extracted features are fed into region SVM classiﬁers and\nbounding box regressors. In contrast to RCNN, SPP-layer can\nalso work on images/regions at various scales and aspect ratios\nwithout resizing them. Thus, it does not suffer from informa-\ntion loss and unwanted geometric distortion.\nSPP-net achieved better results and had a signiﬁcantly faster\ninference speed compared to R-CNN. However, the training of\nSPP-net was still multi-stage and thus it could not be optimized\nend-to-end (and required extra cache memory to store extracted\nfeatures). In addition, SPP layer did not back-propagate gra-\ndients to convolutional kernels and thus all the parameters be-\nfore the SPP layer were frozen. This signiﬁcantly limited the\nlearning capability of deep backbone architectures. Girshick et\nal. proposed Fast R-CNN [38], a multi-task learning detector\nwhich addressed these two limitations of SPP-net. Fast R-CNN\n(like SPP-Net) also computed a feature map for the whole im-\nage and extracted ﬁxed-length region features on the feature\nmap. Different from SPP-net, Fast R-CNN used ROI Pooling\nlayer to extract region features. ROI pooling layer is a special\ncase of SPP which only takes a single scale (i.e., only one value\nof N for the N × N grid) to partition the proposal into ﬁxed\nnumber of divisions, and also backpropagated error signals to\nthe convolution kernels. After feature extraction, feature vec-\ntors were fed into a sequence of fully connected layers before\ntwo sibling output layers: classiﬁcation layer(cls) and regres-\nsion layer(reg). Classiﬁcation layer was responsible for gen-\nerating softmax probabilities over C+1 classes(C classes plus\none background class), while regression layer encoded 4 real-\nvalued parameters to reﬁne bounding boxes. In Fast RCNN,\nthe feature extraction, region classiﬁcation and bounding box\nregression steps can all be optimized end-to-end, without extra\ncache space to store features (unlike SPP Net). Fast R-CNN\nachieved a much better detection accuracy than R-CNN and\nSPP-net, and had a better training and inference speed.\nDespite the progress in learning detectors, the proposal gen-\neration step still relied on traditional methods such as Selec-\ntive Search [45] or Edge Boxes [48], which were based on low-\nlevel visual cues and could not be learned in a data-driven man-\nner. To address this issue, Faster R-CNN [34] was developed\nwhich relied on a novel proposal generator: Region Proposal\nNetwork(RPN). This proposal generator could be learned via\nsupervised learning methods. RPN is a fully convolutional net-\nwork which takes an image of arbitrary size and generates a set\nof object proposals on each position of the feature map. The\nnetwork slid over the feature map using an n × n sliding win-\ndow, and generated a feature vector for each position. The fea-\nture vector was then fed into two sibling output branches, ob-\nject classiﬁcation layer (which classiﬁed whether the proposal\nwas an object or not) and bounding box regression layer. These\nresults were then fed into the ﬁnal layer for the actual object\n7\nclassiﬁcation and bounding box localization. RPN could be in-\nserted into Fast R-CNN and thus the whole framework could be\noptimized in an end-to-end manner on training data. This way\nRPN enabled proposal generation in a data driven manner, and\nwas also able to enjoy the discriminative power of deep back-\nbone networks. Faster R-CNN was able to make predictions\nat 5FPS on GPU and achieved state-of-the-art results on many\npublic benchmark datasets, such as Pascal VOC 2007, 2012 and\nMSCOCO. Currently, there are huge number of detector vari-\nants based on Faster R-CNN for different usage [49, 39, 50, 51].\nFaster R-CNN computed feature map of the input image and\nextracted region features on the feature map, which shared fea-\nture extraction computation across different regions. However,\nthe computation was not shared in the region classiﬁcation step,\nwhere each feature vector still needed to go through a sequence\nof FC layers separately. Such extra computation could be ex-\ntremely large as each image may have hundreds of proposals.\nSimply removing the fully connected layers would result in\nthe drastic decline of detection performance, as the deep net-\nwork would have reduced the spatial information of propos-\nals. Dai et al. [52] proposed Region-based Fully Convolutional\nNetworks (R-FCN) which shared the computation cost in the\nregion classiﬁcation step. R-FCN generated a Position Sen-\nsitive Score Map which encoded relative position information\nof different classes, and used a Position Sensitive ROI Pooling\nlayer (PSROI Pooling) to extract spatial-aware region features\nby encoding each relative position of the target regions. The ex-\ntracted feature vectors maintained spatial information and thus\nthe detector achieved competitive results compared to Faster R-\nCNN without region-wise fully connected layer operations.\nAnother issue with Faster R-CNN was that it used a sin-\ngle deep layer feature map to make the ﬁnal prediction. This\nmade it difﬁcult to detect objects at different scales. In partic-\nular, it was difﬁcult to detect small objects. In DCNN feature\nrepresentations, deep layer features are semantically-strong but\nspatially-weak, while shallow layer features are semantically-\nweak but spatially-strong. Lin et al. [39] exploited this property\nand proposed Feature Pyramid Networks(FPN) which com-\nbined deep layer features with shallow layer features to enable\nobject detection in feature maps at different scales. The main\nidea was to strengthen the spatially strong shallow layer fea-\ntures with rich semantic information from the deeper layers.\nFPN achieved signiﬁcant progress in detecting multi-scale ob-\njects and has been widely used in many other domains such as\nvideo detection [53, 54] and human pose recognition [55, 56].\nMost instance segmentation algorithms are extended from\nvanilla object detection algorithms. Early methods [57, 58, 59]\ncommonly generated segment proposals, followed by Fast\nRCNN for segments classiﬁcation. Later, Dai et al. [59] pro-\nposed a multi-stage algorithm named “MNC” which divided the\nwhole detection framework into multiple stages and predicted\nsegmentation masks from the learned bounding box propos-\nals, which were later categorized by region classiﬁers. These\nearly works performed bbox and mask prediction in multiple\nstages. To make the whole process more ﬂexible, He et al. [3]\nproposed Mask R-CNN, which predicted bounding boxes and\nsegmentation masks in parallel based on the proposals and re-\nported state-of-the-art results. Based on Mask R-CNN, Huang\net al. [60] proposed a mask-quality aware framework, named\nMask Scoring R-CNN, which learned the quality of the pre-\ndicted masks and calibrated the misalignment between mask\nquality and mask conﬁdence score.\nFigure 4 gives an overview of the detection frameworks for\nseveral representative two-stage detectors.\n3.2.2. One-stage Detectors\nDifferent from two-stage detection algorithms which divide\nthe detection pipeline into two parts: proposal generation and\nregion classiﬁcation; one-stage detectors do not have a sepa-\nrate stage for proposal generation (or learning a proposal gen-\neration). They typically consider all positions on the image as\npotential objects, and try to classify each region of interest as\neither background or a target object.\nOne of the early successful one-stage detectors based on deep\nlearning was developed by Sermanet et al. [61] named Over-\nFeat. OverFeat performed object detection by casting DCNN\n8\nSPP-net\nSS \nProposal\nInput \nImage\nRCNN\nFor each region\nStore all region \nfeats in Cache\nFast RCNN\nC+1 Softmax\nRegressors\nC SVMs\nRegressors\n~2k\nproposals\nCrop \nRegion\nResize\n224x224\nCNN\nFeature \nVector\nStage 1\nStage 2\nSS \nProposal\nInput \nImage\n~2k\nproposals\nCNN\nCrop \nRegion\nFor each region\nSPP \nLayer\nStore all region \nfeats in Cache\nC SVMs\nRegressors\n…\nConcate\nSS \nProposal\nInput \nImage\n~2k\nproposals\nCrop \nRegion\nROI \nLayer\nFor each region\nFC\nFC\nFaster RCNN\nC+1 Softmax\nRegressors\nCNN\nInput \nImage\nFeature map\nROI \nLayer\nOn each spatial \nlocation\nFC\nFC\nRPN\nCNN\n2 Softmax\nRegressors\nFor each region\nR-FCN\nC+1 Softmax\nRegressors\nCNN\nInput \nImage\n1x1 \nConv\nRPN\nPosition-sensitive \nfeature map\nPsROI\nPooling\nStage 1\nStage 2\nStage 1\nStage 2\nStage 1\nStage 2\nStage 1\nStage 2\nFeature map\nFor each region\nCNN\nFigure 4: Overview of different two-stage detection frameworks for generic object detection. Red dotted rectangles denote the outputs that deﬁne the loss functions.\nclassiﬁer into a fully convolutional object detector. Object de-\ntection can be viewed as a ”multi-region classiﬁcation” prob-\nlem, and thus OverFeat extended the original classiﬁer into de-\ntector by viewing the last FC layers as 1x1 convolutional layers\nto allow arbitrary input. The classiﬁcation network output a grid\nof predictions on each region of the input to indicate the pres-\nence of an object. After identifying the objects, bounding box\nregressors were learned to reﬁne the predicted regions based on\nthe same DCNN features of classiﬁer. In order to detect multi-\nscale objects, the input image was resized into multiple scales\nwhich were fed into the network. Finally, the predictions across\nall the scales were merged together. OverFeat showed signiﬁ-\ncant speed strength compared with RCNN by sharing the com-\nputation of overlapping regions using convolutional layers, and\nonly a single pass forward through the network was required.\nHowever, the training of classiﬁers and regressors were sepa-\nrated without being jointly optimized.\nLater, Redmon et al. [40] developed a real-time detector\ncalled YOLO (You Only Look Once). YOLO considered ob-\nject detection as a regression problem and spatially divided the\nwhole image into ﬁxed number of grid cells (e.g. using a 7 × 7\ngrid). Each cell was considered as a proposal to detect the pres-\nence of one or more objects. In the original implementation,\neach cell was considered to contain the center of (upto) two ob-\njects. For each cell, a prediction was made which comprised\nthe following information: whether that location had an object,\nthe bounding box coordinates and size(width and height), and\nthe class of the object. The whole framework was a single net-\nwork and it omitted proposal generation step which could be\noptimized in an end-to-end manner. Based on a carefully de-\nsigned lightweight architecture, YOLO could make prediction\nat 45 FPS, and reach 155 FPS with a more simpliﬁed backbone.\nHowever, YOLO faced some challenges: i) it could detect upto\nonly two objects at a given location, which made it difﬁcult to\ndetect small objects and crowded objects [40]. ii) only the last\nfeature map was used for prediction, which was not suitable for\npredicting objects at multiple scales and aspect ratios.\nIn 2016, Liu et al.\nproposed another one-stage detector\nSingle-Shot Mulibox Detector (SSD) [42] which addressed the\nlimitations of YOLO. SSD also divided images into grid cells,\nbut in each grid cell, a set of anchors with multiple scales and\naspect-ratios were generated to discretize the output space of\nbounding boxes (unlike predicting from ﬁxed grid cells adopted\nin YOLO). Each anchor was reﬁned by 4-value offsets learned\n9\nby the regressors and was assigned (C+1) categorical probabil-\nities by the classiﬁers. In addition, SSD predicted objects on\nmultiple feature maps, and each of these feature maps was re-\nsponsible for detecting a certain scale of objects according to\nits receptive ﬁelds. In order to detect large objects and increase\nreceptive ﬁelds, several extra convolutional feature maps were\nadded to the original backbone architecture. The whole net-\nwork was optimized with a weighted sum of localization loss\nand classiﬁcation loss over all prediction maps via an end-to-\nend training scheme. The ﬁnal prediction was made by merg-\ning all detection results from different feature maps. In order to\navoid huge number of negative proposals dominating training\ngradients, hard negative mining was used to train the detector.\nIntensive data augmentation was also applied to improve de-\ntection accuracy. SSD achieved comparable detection accuracy\nwith Faster R-CNN but enjoyed the ability to do real-time in-\nference.\nWithout proposal generation to ﬁlter easy negative samples,\nthe class imbalance between foreground and background is a\nsevere problem in one-stage detector. Lin et al. [43] proposed\na one-stage detector RetinaNet which addressed class imbal-\nance problem in a more ﬂexible manner. RetinaNet used focal\nloss which suppressed the gradients of easy negative samples\ninstead of simply discarding them. Further, they used feature\npyramid networks to detect multi-scale objects at different lev-\nels of feature maps. Their proposed focal loss outperformed\nnaive hard negative mining strategy by large margins.\nRedmon et al.\nproposed an improved YOLO version,\nYOLOv2 [41] which signiﬁcantly improved detection perfor-\nmance but still maintained real-time inference speed. YOLOv2\nadopted a more powerful deep convolutional backbone archi-\ntecture which was pre-trained on higher resolution images from\nImageNet(from 224 × 224 to 448 × 448), and thus the weights\nlearned were more sensitive to capturing ﬁne-grained informa-\ntion. In addition, inspired by the anchor strategy used in SSD,\nYOLOv2 deﬁned better anchor priors by k-means clustering\nfrom the training data (instead of setting manually). This helped\nin reducing optimizing difﬁculties in localization. Finally inte-\ngrating with Batch Normalization layers [62] and multi-scale\ntraining techniques, YOLOv2 achieved state-of-the-art detec-\ntion results at that time.\nThe previous approaches required designing anchor boxes\nmanually to train a detector. Later a series of anchor-free object\ndetectors were developed, where the goal was to predict key-\npoints of the bounding box, instead of trying to ﬁt an object to\nan anchor. Law and Deng proposed a novel anchor-free frame-\nwork CornerNet [63] which detected objects as a pair of cor-\nners. On each position of the feature map, class heatmaps, pair\nembeddings and corner offsets were predicted. Class heatmaps\ncalculated the probabilities of being corners, and corner offsets\nwere used to regress the corner location. And the pair embed-\ndings served to group a pair of corners which belong to the\nsame objects. Without relying on manually designed anchors\nto match objects, CornerNet obtained signiﬁcant improvement\non MSCOCO datasets. Later there were several other variants\nof keypoint detection based one-stage detectors [64, 65].\nFigures 5 gives an overview of different detection frame-\nworks for several representative one-stage detectors.\n3.3. Backbone Architecture\nR-CNN [2] showed adopting convolutional weights from\nmodels pre-trained on large scale image classiﬁcation problem\ncould provide richer semantic information to train detectors and\nenhanced the detection performance. During the later years,\nthis approach had become the default strategy for most object\ndetectors. In this section, we will ﬁrst brieﬂy introduce the basic\nconcept of deep convolutional neural networks and then review\nsome architectures which are widely used for detection.\n3.3.1. Basic Architecture of a CNN\nDeep convolutional neural network (DCNN) is a typical deep\nneural network and has proven extremely effective in visual\nunderstanding [36, 33]. Deep convolutional neural networks\nare commonly composed of a sequence of convolutional layers,\npooling layers, nonlinear activation layers and fully connected\nlayers (FC layers). Convolutional layer takes an image input\nand convolves over it by n×n kernels to generate a feature map.\n10\nSSD\nYOLO\nInput \nImage\nFeature \nvector\nDivide into nxn Grids\nFor \neach grid\nCLS Softmax\nExtra Added Feature Maps\nCornerNet\nCLS Heatmaps\nCorner offsets\nTop-left corner\nPair Embeddings\nCorner \nPooling\nRetinaNet\nCNN\nResize\nRegression\nCNN\nConv\nConv\nConv\nConv\n…\nConv\nInput \nImage\nPrediction \nModule\nFor each  spatial location\nCLS Softmax\nRegression\n3x3 \nConv\nExtra Added Feature \nMaps\nCNN\nConv\nConv\nConv\nConv\n…\nConv\nInput \nImage\nFeature \nPyramid\nFeature \nPyramid\nFeature \nPyramid\nFeature \nPyramid\nFeature \nPyramid\n…\n…\nConv\nFor each  spatial location\nFocal Loss\nRegression\nConv\nInput \nImage\nCNN\nConv\nConv\nBottom-right corner\nPrediction \nModule\nFeature \nmap\nFor each  spatial location\nPrediction \nModule\nPrediction \nModule\nPrediction \nModule\nPrediction \nModule\nPrediction \nModule\n3x3 \nConv\nPrediction \nModule\nPrediction \nModule\nPrediction \nModule\nPrediction \nModule\nPrediction \nModule\nFigure 5: Overview of different one-stage detection frameworks for generic object detection. Red rectangles denotes the outputs that deﬁne the objective functions.\nThe generated feature map can be regarded as a multi-channel\nimage and each channel represents different information about\nthe image. Each pixel in the feature map (named neuron) is\nconnected to a small portion of adjacent neurons from the pre-\nvious map, which is called the receptive ﬁeld. After generating\nfeature maps, a non-linear activation layer is applied. Pooling\nlayers are used to summarize the signals within the receptive\nﬁelds, to enlarge receptive ﬁelds as well as reduce computation\ncost, .\nWith the combination of a sequence of convolutional layers,\npooling layers and non-linear activation layers, the deep convo-\nlutional neural network is built. The whole network can be op-\ntimized via a deﬁned loss function by gradient-based optimiza-\ntion method (stochastic gradient descent [66], Adam [67], etc.).\nA typical convolutional neural network is AlexNet [33], which\ncontains ﬁve convolutional layers, three max-pooling layers and\nthree fully connected layers. Each convolutional layer is fol-\nlowed by a ReLU [68] non-linear activation layer.\n3.3.2. CNN Backbone for Object Detection\nIn this section, we will review some architectures which are\nwidely used in object detection tasks with state-of-the-art re-\nsults, such as VGG16 [34, 38], ResNet [1, 52], ResNeXt [43]\nand Hourglass [63].\nVGG16 [69] was developed based on AlexNet.\nVGG16\nis composed of ﬁve groups of convolutional layers and three\nFC layers. There are two convolutional layers in the ﬁrst two\ngroups and three convolutional layers in the next three groups.\nBetween each group, a Max Pooling layer is applied to decrease\nspatial dimension.\nVGG16 showed that increasing depth of\nnetworks by stacking convolutional layers could increase the\nmodel’s expression capability, and led to a better performance.\nHowever, increasing model depth to 20 layers by simply stack-\ning convolutional layers led to optimization challenges with\nSGD. The performance declined signiﬁcantly and was inferior\nto shallower models, even during the training stages. Based\non this observation, He et al. [1] proposed ResNet which re-\nduced optimization difﬁculties by introducing shortcut connec-\ntions. Here, a layer could skip the nonlinear transformation and\ndirectly pass the values to the next layer as is (thus giving us an\nimplicit identity layer). This is given as:\nxl+1 = xl + fl+1(xl, θ)\n(6)\nwhere xl is the input feature in l-th layer and fl+1 denotes op-\nerations on input xl such as convolution, normalization or non-\nlinear activation. fl+1(xl, θ) is the residual function to xl, so\nthe feature map of any deep layer can be viewed as the sum of\nthe activation of shallow layer and the residual function. Short-\ncut connection creates a highway which directly propagates the\n11\ngradients from deep layers to shallow units and thus, signiﬁ-\ncantly reduces training difﬁculty. With residual blocks effec-\ntively training networks, the model depth could be increased\n(e.g. from 16 to 152), allowing us to train very high capacity\nmodels. Later, He et al. [70] proposed a pre-activation variant of\nResNet, named ResNet-v2. Their experiments showed appro-\npriate ordering of the Batch Normalization [62] could further\nperform better than original ResNet. This simple but effective\nmodiﬁcation of ResNet made it possible to successfully train\na network with more than 1000 layers, and still enjoyed im-\nproved performance due to the increase in depth. Huang et al.\nargued that although ResNet reduced the training difﬁculty via\nshortcut connection, it did not fully utilize features from previ-\nous layers. The original features in shallow layers were missing\nin element-wise operation and thus could not be directly used\nlater. They proposed DenseNet [71], which retained the shallow\nlayer features, and improved information ﬂow, by concatenat-\ning the input with the residual output instead of element-wise\naddition:\nxl+1 = xl ◦fl+1(xl, θ)\n(7)\nwhere ◦denotes concatenation. Chen [72] et al. argued that\nin DenseNet, the majority of new exploited features from shal-\nlow layers were duplicated and incurred high computation cost.\nIntegrating the advantages of both ResNet and DenseNet, they\npropose a Dual Path Network(DPN) which divides xl channels\ninto two parts: xd\nl and xr\nl .\nxd\nl was used for dense connec-\ntion computation and xr\nl was used for element-wise summation,\nwith unshared residual learning branch f d\nl+1 and f r\nl+1. The ﬁnal\nresult was the concatenated output of the two branches:\nxl+1 = (xr\nl + f r\nl+1(xr\nl , θr)) ◦(xd\nl ◦f d\nl+1(xd\nl , θd))\n(8)\nBased on ResNet, Xie et al. [73] proposed ResNeXt which\nconsiderably reduced computation and memory cost while\nmaintaining comparable classiﬁcation accuracy.\nResNeXt\nadopted group convolution layers [33] which sparsely connects\nfeature map channels to reduce computation cost. By increas-\ning group number to keep computation cost consistent to the\noriginal ResNet, ResNeXt captures richer semantic feature rep-\nresentation from the training data and thus improves backbone\naccuracy. Later, Howard et al. [74] set the coordinates equal\nto number of channels of each feature map and developed Mo-\nbileNet. MobileNet signiﬁcantly reduced computation cost as\nwell as number of parameters without signiﬁcant loss in clas-\nsiﬁcation accuracy. This model was speciﬁcally designed for\nusage on a mobile platform.\nIn addition to increasing model depth, some efforts explored\nbeneﬁts from increasing model width to improve the learning\ncapacity. Szegedy et al. proposed GoogleNet with an inception\nmodule [75] which applied different scale convolution kernels\n(1 × 1, 3 × 3 and 5 × 5) on the same feature map in a given\nlayer. This way it captured multi-scale features and summa-\nrized these features together as an output feature map. Bet-\nter versions of this model were developed later with different\ndesign of choice of convolution kernels [76], and introducing\nresidual blocks [77].\nThe network structures introduced above were all designed\nfor image classiﬁcation. Typically these models trained on Im-\nageNet are adopted as initialization of the model used for object\ndetection. However, directly applying this pre-trained model\nfrom classiﬁcation to detection is sub-optimal due to a poten-\ntial conﬂict between classiﬁcation and detection tasks. Specif-\nically, i) classiﬁcation requires large receptive ﬁelds and wants\nto maintain spatial invariance. Thus multiple downsampling op-\neration (such as pooling layer) are applied to decrease feature\nmap resolution. The feature maps generated are low-resolution\nand spatially invariant and have large receptive ﬁelds. However,\nin detection, high-resolution spatial information is required to\ncorrectly localize objects; and ii) classiﬁcation makes predic-\ntions on a single feature map, while detection requires feature\nmaps with multiple representations to detect objects at multi-\nple scales. To bridge the difﬁculties between the two tasks,\nLi et al. introduced DetNet [78] which was designed speciﬁ-\ncally for detection. DetNet kept high resolution feature maps\nfor prediction with dilated convolutions to increase receptive\n12\nﬁelds. In addition, DetNet detected objects on multi-scale fea-\nture maps, which provided richer information. DetNet was pre-\ntrained on large scale classiﬁcation dataset while the network\nstructure was designed for detection.\nHourglass Network [79] is another architecture, which was\nnot designed speciﬁcally for image classiﬁcation. Hourglass\nNetwork ﬁrst appeared in human pose recognition task [79],\nand was a fully convolutional structure with a sequence of hour-\nglass modules. Hourglass module ﬁrst downsampled the input\nimage via a sequence of convolutional layer or pooling layer,\nand upsampled the feature map via deconvolutional operation.\nTo avoid information loss in downsampling stage, skip con-\nnection were used between downsampling and upsampling fea-\ntures. Hourglass module could capture both local and global\ninformation and thus was very suitable for object detection.\nCurrently Hourglass Network is widely used in state-of-the-art\ndetection frameworks [63, 64, 65].\n3.4. Proposal Generation\nProposal generation plays a very important role in the object\ndetection framework. A proposal generator generates a set of\nrectangle bounding boxes, which are potentially objects. These\nproposals are then used for classiﬁcation and localization re-\nﬁnement. We categorize proposal generation methods into four\ncategories: traditional computer vision methods, anchor-based\nsupervised learning methods, keypoint based methods and other\nmethods. Notably, both one-stage detectors and two-stage de-\ntectors generate proposals, the main difference is two-stage de-\ntectors generates a sparse set of proposals with only foreground\nor background information, while one-stage detectors consider\neach region in the image as a potential proposal, and accord-\ningly estimates the class and bounding box coordinates of po-\ntential objects at each location.\n3.4.1. Traditional Computer Vision Methods\nThese methods generate proposals in images using tradi-\ntional computer vision methods based on low-level cues, such\nas edges, corners, color, etc. These techniques can be catego-\nrized into three principles: i) computing the ’objectness score’\nof a candidate box; ii) merging super-pixels from original im-\nages; iii) generating multiple foreground and background seg-\nments;\nObjectness Score based methods predict an objectness score\nof each candidate box measuring how likely it may contain an\nobject. Arbelaez et al. [80] assigned objectness score to propos-\nals by classiﬁcation based on visual cues such as color contrast,\nedge density and saliency. Rahtu et al.[81] revisited the idea of\nArbelaez et al. [80] and introduced a more efﬁcient cascaded\nlearning method to rank the objectness score of candidate pro-\nposals.\nSuperpixels Merging is based on merging superpixels gen-\nerated from segmentation results. Selective Search [45] was a\nproposal generation algorithm based on merging super-pixels.\nIt computed the multiple hierarchical segments generated by\nsegmentation method [82], which were merged according to\ntheir visual factors(color, areas, etc.), and ﬁnally bounding\nboxes were placed on the merged segments. Manen et al. [83]\nproposed a similar idea to merge superpixels. The difference\nwas that the weight of the merging function was learned and the\nmerging process was randomized. Selective Search is widely\nused in many detection frameworks due to its efﬁciency and\nhigh recall compared to other traditional methods.\nSeed Segmentation starts with multiple seed regions, and for\neach seed, foreground and background segments are generated.\nTo avoid building up hierarchical segmentation, CPMC [84]\ngenerated a set of overlapping segments initialized with di-\nverse seeds. Each proposal segment was the solution of a bi-\nnary(foreground or background) segmentation problem. Enreds\nand Hoiem [85] combined the idea of Selective Search [45] and\nCPMC [84]. It started with super-pixels and merged them with\nnew designed features. These merged segments were used as\nseeds to generate larger segments, which was similar to CPMC.\nHowever, producing high quality segmentation masks is very\ntime-consuming and it’s not applicable to large scale datasets.\nThe primary advantage of these traditional computer vision\nmethods is that they are very simple and can generate propos-\nals with high recall (e.g. on medium scale datasets such as\n13\nPascal VOC). However, these methods are mainly based on\nlow level visual cues such as color or edges. They cannot be\njointly optimized with the whole detection pipeline. Thus they\nare unable to exploit the power of large scale datasets to im-\nprove representation learning. On challenging datasets such as\nMSCOCO [86], traditional computer vision methods struggled\nto generate high quality proposals due to these limitations.\n3.4.2. Anchor-based Methods\nOne large family of supervised proposal generators is anchor-\nbased methods. They generate proposals based on pre-deﬁned\nanchors.\nRen et al.\nproposed Region Proposal Network\n(RPN) [34] to generate proposals in a supervised way based\non deep convolutional feature maps. The network slid over the\nentire feature map using 3 × 3 convolution ﬁlters. For each\nposition, k anchors (or initial estimates of bounding boxes) of\nvarying size and aspect ratios were considered. These sizes and\nratios allowed for matching objects at different scales in the\nentire image. Based on the ground truth bonding boxes, the ob-\nject locations were matched with the most appropriate anchors\nto obtain the supervision signal for the anchor estimation. A\n256−dimensional feature vector was extracted from each an-\nchor and was fed into two sibling branches - classiﬁcation layer\nand regression layer.\nClassiﬁcation branch was responsible\nfor modeling objectness score while regression branch encoded\nfour real-values to reﬁne location of the bounding box from the\noriginal anchor estimation. Based on the ground truth, each an-\nchor was predicted to either be an object, or just background by\nthe classiﬁcation branch (See Fig. 6). Later, SSD [42] adopted\na similar idea of anchors in RPN by using multi-scale anchors\nto match objects. The main difference was that SSD assigned\ncategorical probabilities to each anchor proposal, while RPN\nﬁrst evaluated whether the anchor proposal was foreground or\nbackground and performed the categorical classiﬁcation in the\nnext stage.\nDespite promising performance, the anchor priors are manu-\nally designed with multiple scales and aspect ratios in a heuris-\ntic manner.\nThese design choices may not be optimal, and\ndifferent datasets would require different anchor design strate-\ngies.\nMany efforts have been made to improve the design\nchoice of anchors. Zhang et al. proposed Single Shot Scale-\ninvariant Face Detector (S3FD) [87] based on SSD with care-\nfully designed anchors to match the objects. According to the\neffective receptive ﬁeld [88] of different feature maps, differ-\nent anchor priors were designed. Zhu et al. [89] introduced\nan anchor design method for matching small objects by en-\nlarging input image size and reducing anchor strides. Xie et\nal. proposed Dimension-Decomposition Region Proposal Net-\nwork (DeRPN) [90] which decomposed the dimension of an-\nchor boxes based on RPN. DeRPN used an anchor string mech-\nanism to independently match objects width and height. This\nhelped match objects with large scale variance and reduced the\nsearching space.\nGhodrati et al. developed DeepProposals [91] which pre-\ndicted proposals on the low-resolution deeper layer feature\nmap. These were then projected back onto the high-resolution\nshallow layer feature maps, where they are further reﬁned. Red-\nmon et al. [41] designed anchor priors by learning priors from\nthe training data using k-means clustering.\nLater, Zhang et\nal. introduced Single-Shot Reﬁnement Neural Network (Re-\nﬁneDet) [92] which reﬁned the manually deﬁned anchors in\ntwo steps. In the ﬁrst step, ReﬁneDet learned a set of local-\nization offsets based on the original hand-designed anchors and\nthese anchors were reﬁned by the learned offsets. In the sec-\nond stage, a new set of localization offsets were learned based\non the reﬁned anchors from the ﬁrst step for further reﬁne-\nment. This cascaded optimization framework signiﬁcantly im-\nproved the anchor quality and ﬁnal prediction accuracy in a\ndata-driven manner. Cai et al. proposed Cascade R-CNN [49]\nwhich adopted a similar idea as ReﬁneDet by reﬁning proposals\nin a cascaded way. Yang et al. [93] modeled anchors as func-\ntions implemented by neural networks which was computed\nfrom customized anchors. Their method MetaAnchor showed\ncomprehensive improvement compared to other manually de-\nﬁned methods but the customized anchors were still designed\nmanually.\n14\nFeature Map For Prediction\n3x3 conv, ReLU\nFeature Vector\nreg layer\ncls layer\n2*K scores\n4*K offsets\n…\n.\nK anchors\n…\n.\n.\n.\nFigure 6: Diagram of RPN [34]. Each position of the feature map connects\nwith a sliding windows, followed with two sibling branches.\n3.4.3. Keypoints-based Methods\nAnother proposal generation approach is based on keypoint\ndetection, which can be divided into two families: corner-based\nmethods and center-based methods.\nCorner-based methods\npredict bounding boxes by merging pairs of corners learned\nfrom the feature map. Denet [94] reformulated the object detec-\ntion problem in a probabilistic way. For each point on the fea-\nture map, Denet modeled the distribution of being one of the 4\ncorner types of objects (top-left, top-right, bottom-left, bottom-\nright), and applied a naive bayesian classiﬁers over each cor-\nner of the objects to estimate the conﬁdence score of a bound-\ning box. This corner-based algorithm eliminated the design of\nanchors and became a more effective method to produce high\nquality proposals. Later based on Denet, Law and Deng pro-\nposed CornerNet [63] which directly modeled categorical infor-\nmation on corners. CornerNet modeled information of top-left\nand bottom-right corners with novel feature embedding meth-\nods and corner pooling layer to correctly match keypoints be-\nlonging to the same objects, obtaining state-of-the-art results on\npublic benchmarks. For center-based methods, the probability\nof being the center of the objects is predicted on each position\nof the feature map, and the height and width are directly re-\ngressed without any anchor priors. Zhu et al. [95] presented a\nfeature-selection-anchor-free (FSAF) framework which could\nbe plugged into one-stage detectors with FPN structure.\nIn\nFSAF, an online feature selection block is applied to train multi-\nlevel center-based branches attached in each level of the feature\npyramid. During training, FSAF dynamically assigned each ob-\nject to the most suitable feature level to train the center-based\nbranch. Similar to FSAF, Zhou et al. proposed a new center-\nbased framework [64] based on a single Hourglass network [63]\nwithout FPN structure. Furthermore, they applied center-based\nmethod into higher-level problems such as 3D-detection and\nhuman pose recognition, and all achieved state-of-the-art re-\nsults. Duan et al. [65] proposed CenterNet, which combined the\nidea of center-based methods and corner-based methods. Cen-\nterNet ﬁrst predicted bounding boxes by pairs of corners, and\nthen predicted center probabilities of the initial prediction to\nreject easy negatives. CenterNet obtained signiﬁcant improve-\nments compared with baselines. These anchor-free methods\nform a promising research direction in the future.\n3.4.4. Other Methods\nThere are some other proposal generation algorithms which\nare not based on keypoints or anchors but also offer competi-\ntive performances. Lu et al. proposed AZnet [96] which auto-\nmatically focused on regions of high interest. AZnet adopted a\nsearch strategy that adaptively directed computation resources\nto sub-regions which were likely contain objects. For each re-\ngion, AZnet predicted two values: zoom indicator and adja-\ncency scores. Zoom indicator determined whether to further\ndivide this region which may contain smaller objects and ad-\njacency scores denoted its objectness. The starting point was\nthe entire image and each divided sub-region is recursively pro-\ncessed in this way until the zoom indicator is too small. AZnet\nwas better at matching sparse and small objects compared to\nRPN’s anchor-object matching approach.\n3.5. Feature Representation Learning\nFeature Representation Learning is a critical component in\nthe whole detection framework. Target objects lie in complex\nenvironments and have large variance in scale and aspect ratios.\nThere is a need to train a robust and discriminative feature em-\nbedding of objects to obtain a good detection performance. In\n15\nthis section, we introduce feature representation learning strate-\ngies for object detection. Speciﬁcally, we identify three cate-\ngories: multi-scale feature learning, contextual reasoning, and\ndeformable feature learning.\n3.5.1. Multi-scale Feature Learning\nTypical object detection algorithms based on deep convo-\nlutional networks such as Fast R-CNN [38] and Faster R-\nCNN [34] use only a single layer’s feature map to detect ob-\njects. However, detecting objects across large range of scales\nand aspect ratios is quite challenging on a single feature map.\nDeep convolutional networks learn hierarchical features in dif-\nferent layers which capture different scale information. Specif-\nically, shallow layer features with spatial-rich information have\nhigher resolution and smaller receptive ﬁelds and thus are more\nsuitable for detecting small objects, while semantic-rich fea-\ntures in deep layers are more robust to illumination, translation\nand have larger receptive ﬁelds (but coarse resolutions), and\nare more suitable for detecting large objects. When detecting\nsmall objects, high resolution representations are required and\nthe representation of these objects may not even be available in\nthe deep layer features, making small object detection difﬁcult.\nSome techniques such as dilated/atrous convolutions [97, 52]\nwere proposed to avoid downsampling, and used the high reso-\nlution information even in the deeper layers. At the same time,\ndetecting large objects in shallow layers are also non-optimal\nwithout large enough receptive ﬁelds. Thus, handling feature\nscale issues has become a fundamental research problem within\nobject detection. There are four main paradigms addressing\nmulti-scale feature learning problem: Image Pyramid, Predic-\ntion Pyramid, Integrated Features and Feature Pyramid. These\nare brieﬂy illustrated in the Fig. 7.\nImage Pyramid: An intuitive idea is to resize input images\ninto a number of different scales (Image Pyramid) and to train\nmultiple detectors, each of which is responsible for a certain\nrange of scales [98, 99, 100, 101]. During testing, images are\nresized to different scales followed by multiple detectors and\nthe detection results are merged. This can be computationally\npredict\npredict\npredict\npredict\npredict\nFuse\npredict\npredict\npredict\npredict\npredict\npredict\npredict\nFigure 7: Four paradigms for multi-scale feature learning. Top Left: Image\nPyramid, which learns multiple detectors from different scale images; Top\nRight: Prediction Pyramid, which predicts on multiple feature maps; Bottom\nLeft: Integrated Features, which predicts on single feature map generated\nfrom multiple features; Bottom Right: Feature Pyramid which combines the\nstructure of Prediction Pyramid and Integrated Features.\n2x Up\nFuse\nTop-Down Features\nBottom-Up Features\nFigure 8: General framework for feature combination. Top-down features are 2\ntimes up-sampled and fuse with bottom-up features. The fuse methods can be\nelement-wise sum, multiplication, concatenation and so on. Convolution and\nnormalization layers can be inserted in to this general framework to enhance\nsemantic information and reduce memory cost.\nexpensive. Liu et al. [101] ﬁrst learned a light-weight scale-\naware network to resize images such that all objects were in\na similar scale. This was followed by learning a single scale\ndetector. Singh et. al. [98] conducted comprehensive exper-\niments on small object detection. They argued that learning a\nsingle scale-robust detector to handle all scale objects was much\nmore difﬁcult than learning scale-dependent detectors with im-\nage pyramids. In their work, they proposed a novel framework\nScale Normalization for Image Pyramids (SNIP) [98] which\ntrained multiple scale-dependent detectors and each of them\nwas responsible for a certain scale objects.\nIntegrated Features: Another approach is to construct a\nsingle feature map by combining features in multiple layers\nand making ﬁnal predictions based on the new constructed\nmap [102, 51, 50, 103, 104, 105]. By fusing spatially rich shal-\nlow layer features and semantic-rich deep layer features, the\n16\nnew constructed features contain rich information and thus can\ndetect objects at different scales. These combinations are com-\nmonly achieved by using skip connections [1]. Feature nor-\nmalization is required as feature norms of different layers have\na high variance. Bell et al. proposed Inside-Outside Network\n(ION) [51] which cropped region features from different lay-\ners via ROI Pooling [38], and combined these multi-scale re-\ngion features for the ﬁnal prediction. Kong et. al. proposed\nHyperNet [50] which adopted a similar idea as IoN. They care-\nfully designed high resolution hyper feature maps by integrat-\ning intermediate and shallow layer features to generate pro-\nposals and detect objects. Deconvolutional layers were used\nto up-sample deep layer feature maps and batch normaliza-\ntion layers were used to normalize input blobs in their work.\nThe constructed hyper feature maps could also implicitly en-\ncode contextual information from different layers.\nInspired\nby ﬁne-grained classiﬁcation algorithms which integrate high-\norder representation instead of exploiting simple ﬁrst-order rep-\nresentations of object proposals, Wang et al. proposed a novel\nframework Multi-scale Location-aware Kernel Representation\n(MLKP) [103] which captured high-order statistics of proposal\nfeatures and generated more discriminative feature representa-\ntions efﬁciently. The combined feature representation was more\ndescriptive and provides both semantic and spatial information\nfor both classiﬁcation and localization.\nPrediction Pyramid: Liu et al.’s SSD [42] combined coarse\nand ﬁne features from multiple layers together. In SSD, pre-\ndictions were made from multiple layers, where each layer\nwas responsible for a certain scale of objects. Later, many ef-\nforts [106, 107, 108] followed this principle to detect multi-\nscale objects. Yang et al. [100] also exploited appropriate fea-\nture maps to generate certain scale of object proposals and these\nfeature maps were fed into multiple scale-dependent classiﬁers\nto predict objects. In their work, cascaded rejection classiﬁers\nwere learned to reject easy background proposals in early stages\nto accelerate detection speed. Multi-scale Deep Convolutional\nNeural Network (MSCNN) [106] applied deconvolutional lay-\ners on multiple feature maps to improve their resolutions, and\nlater these reﬁned feature maps were used to make predic-\ntions. Liu et al. proposed a Receptive Field Block Net (RF-\nBNet) [108] to enhance the robustness and receptive ﬁelds via\na receptive ﬁeld block (RFB block). RFB block adopted simi-\nlar ideas as the inception module [75] which captured features\nfrom multiple scale and receptive ﬁelds via multiple branches\nwith different convolution kernels and ﬁnally merged them to-\ngether.\nFeature Pyramid: To combine the advantage of Integrated\nFeatures and Prediction Pyramid, Lin et al.\nproposed Fea-\nture Pyramid Network(FPN) [39] which integrated different\nscale features with lateral connections in a top-down fashion\nto build a set of scale invariant feature maps, and multiple\nscale-dependent classiﬁers were learned on these feature pyra-\nmids. Speciﬁcally, the deep semantic-rich features were used to\nstrengthen the shallow spatially-rich features. These top-down\nand lateral features were combined by element-wise summation\nor concatenation, with small convolutions reducing the dimen-\nsions. FPN showed signiﬁcant improvement in object detec-\ntion, as well as other applications, and achieved state-of-the art\nresults in learning multi-scale features. Many variants of FPN\nwere later developed [109, 110, 109, 111, 112, 92, 113, 114,\n115, 116, 117, 118, 119], with modiﬁcations to the feature pyra-\nmid block (see Fig. 8). Kong et al. [120] and Zhang et. al. [92]\nbuilt scale invariant feature maps with lateral connections. Dif-\nferent from FPN which generated region proposals followed by\ncategorical classiﬁers, their methods omitted proposal gener-\nation and thus were more efﬁcient than original FPN. Ren et\nal. [109] and Jeong et al.\n[110] developed a novel structure\nwhich gradually and selectively encoded contextual informa-\ntion between different layer features. Inspired by super resolu-\ntion tasks [121, 122], Zhou et al. [111] developed high resolu-\ntion feature maps using a novel transform block which explic-\nitly explored the inter-scale consistency nature across multiple\ndetection scales.\n17\n3.5.2. Region Feature Encoding\nFor two-stage detectors, region feature encoding is a critical\nstep to extract features from proposals into ﬁxed length feature\nvectors. In R-CNN, Girshick et al. [2] cropped region proposals\nfrom the whole image and resized the cropped regions into ﬁxed\nsized patches(224 × 224) via bilinear interpolation, followed\nby a deep convolution feature extractor. Their method encoded\nhigh resolution region features but the computation was expen-\nsive.\nLater Girshick et al. [38] and Ren [34] proposed ROI Pooling\nlayer to encode region features. ROI Pooling divided each re-\ngion into n×n cells (e.g. 7×7 by default) and only the neuron\nwith the maximum signal would go ahead in the feedforward\nstage. This is similar to max-pooling, but across (potentially)\ndifferent sized regions. ROI Pooling extracted features from\nthe down-sampled feature map and as a result struggled to han-\ndle small objects. Dai [59] proposed ROI Warping layer which\nencoded region features via bilinear interpolation. Due to the\ndownsampling operation in DCNN, there can be a misalign-\nment of the object position in the original image and the down-\nsampled feature maps, which RoI Pooling and RoI Warping lay-\ners are not able to handle. Instead of quantizing grids border as\nROI Warping and ROI Pooling do, He et al. [3] proposed ROI\nAlign layer which addressed the quantization issue by bilinear\ninterpolation at fractionally sampled positions within each grid.\nBased on ROI Align, Jiang et al. [123] presented Precise ROI\nPooing (PrROI Pooling), which avoided any quantization of co-\nordinates and had a continuous gradient on bounding box coor-\ndinates.\nIn order to enhance spatial information of the downsampled\nregion features, Dai et al. [52] proposed Position Sensitive ROI\nPooing (PSROI Pooling) which kept relative spatial information\nof downsampled features. Each channel of generated region\nfeature map only corresponded to a subset channels of input re-\ngion according to its relative spatial position. Based on PSROI\nPooling, Zhai et al. [124] presented feature selective networks\nto learn robust region features by exploiting disparities among\nsub-region and aspect ratios. The proposed network encoded\nsub-region and aspect ratio information which were selectively\npooled to reﬁne initial region features by a light-weight head.\nLater, more algorithms were proposed to well encode region\nfeatures from different viewpoints. Zhu et al. proposed Cou-\npleNet [125] which extracted region features by combining out-\nputs generated from both ROI Pooling layer and PSROI Pool-\ning layer. ROI Pooling layer extracted global region informa-\ntion but struggled for objects with high occlusion while PSROI\nPooling layer focused more on local information. CoupleNet\nenhanced features generated from ROI Pooling and PSROI\nPooling by element-wise summation and generated more pow-\nerful features. Later Dai et al. proposed Deformable ROI Pool-\ning [97] which generalized aligned RoI pooling by learning an\noffset for each grid and adding it to the grid center. The sub-grid\nstart with a regular ROI Pooling layer to extract initial region\nfeatures and the extracted features were used to regress offset\nby an auxiliary network. Deformable ROI Pooling can auto-\nmatically model the image content without being constrained\nby ﬁxed receptive ﬁelds.\n3.5.3. Contextual Reasoning\nContextual information plays an important role in object de-\ntection. Objects often tend to appear in speciﬁc environments\nand sometimes also coexist with other objects. For each ex-\nample, birds commonly ﬂy in the sky. Effectively using con-\ntextual information can help improve detection performance,\nespecially for detecting objects with insufﬁcient cues(small ob-\nject, occlusion etc.) Learning the relationship between objects\nwith their surrounding context can improve detector’s ability to\nunderstand the scenario. For traditional object detection algo-\nrithms, there have been several efforts exploring context [126],\nbut for object detection based on deep learning, context has not\nbeen extensively explored. This is because convolutional net-\nworks implicitly already capture contextual information from\nhierarchical feature representations. However, some recent ef-\nforts [1, 127, 128, 129, 3, 59, 3, 130, 131, 106] still try to\nexploit contextual information. Some works [132] have even\nshown that in some cases context information may even harm\nthe detection performance. In this section we review contextual\n18\nreasoning for object detection from two aspects: global context\nand region context.\nGlobal context reasoning refers to learning from the context\nin the whole image. Unlike traditional detectors which attempt\nto classify speciﬁc regions in the image as objects, the idea here\nis to use the contextual information (i.e., information from the\nrest of the image) to classify a particular region of interest. For\nexample, detecting a baseball ball from an image can be chal-\nlenging for a traditional detector (as it may be confused with\nballs from other sports); but if the contextual information from\nthe rest of the image is used (e.g. baseball ﬁeld, players, bat), it\nbecomes easier to identify the baseball ball object.\nSome representative efforts include ION [51], DeepId [127]\nand improved version of Faster R-CNN [1]. In ION, Bell et\nal. used recurrent neural network to encode contextual infor-\nmation across the whole image from four directions. Ouyang\net al. [127] learned a categorical score for each image which\nis used as contextual features concatenated with the object de-\ntection results. He et al. [1] extracted feature embedding of\nthe entire image and concatenate it with region features to im-\nprove detection results. In addition, some methods [129, 3, 59,\n133, 134, 135, 136] exploit global contextual information via\nsemantic segmentation. Due to precise pixel-level annotation,\nsegmentation feature maps capture strong spatial information.\nHe et al. [3] and Dai et al. [59] learn uniﬁed instance seg-\nmentation framework and optimize the detector with pixel-level\nsupervision. They jointly optimized detection and segmenta-\ntion objectives as a multi-task optimization. Though segmenta-\ntion can signiﬁcantly improve detection performance, obtaining\nthe pixel-level annotation is very expensive. Zhao et al. [133]\noptimized detectors with pseudo segmentation annotation and\nshowed promising results. Zhang et al.’s work Detection with\nEnriched Semantics (DES) [134], introduced contextual infor-\nmation by learning a segmentation mask without segemtation\nannotations. It also jointly optimized object detection and seg-\nmentation objectives and enriched original feature map with a\nmore discriminative feature map.\nRegion Context Reasoning encodes contextual information\nsurrounding regions and learns interactions between the objects\nwith their surrounding area. Directly modeling different lo-\ncations and categories objects relations with the contextual is\nvery challenging. Chen et al. proposed Spatial Memory Net-\nwork (SMN) [130] which introduced a spatial memory based\nmodule. The spatial memory module captured instance-level\ncontexts by assembling object instances back into a pseudo\n”image” representations which were later used for object re-\nlations reasoning. Liu et al. proposed Structure Inference Net\n(SIN) [137] which formulated object detection as a graph in-\nference problem by considering scene contextual information\nand object relationships. In SIN, each object was treated as a\ngraph node and the relationship between different objects were\nregarded as graph edges. Hu et al. [138] proposed a lightweight\nframework relation network which formulated the interaction\nbetween different objects between their appearance and image\nlocations. The new proposed framework did not need addi-\ntional annotation and showed improvements in object detection\nperformance. Based on Hu et al., Gu et al. [139] proposed a\nfully learnable object detector which proposed a general view-\npoint that uniﬁed existing region feature extraction methods.\nTheir proposed method removed heuristic choices in ROI pool-\ning methods and automatically select the most signiﬁcant parts,\nincluding contexts beyond proposals. Another method to en-\ncode contextual information is to implicitly encode region fea-\ntures by adding image features surrounding region proposals\nand a large number of approaches have been proposed based\non this idea [131, 106, 140, 141, 142, 143]. In addition to en-\ncode features from region proposals, Gidaris et al. [131] ex-\ntracted features from a number of different sub-regions of the\noriginal object proposals(border regions, central regions, con-\ntextual regions etc.) and concatenated these features with the\noriginal region features.\nSimilar to their method, [106] ex-\ntracted local contexts by enlarging the proposal window size\nand concatenating these features with the original ones. Zeng et\nal. [142] proposed Gated Bi-Directional CNN (GBDNet) which\nextracted features from multi-scale subregions. Notably, GBD-\nNet learned a gated function to control the transmission of dif-\n19\nferent region information because not all contextual informa-\ntion is helpful for detection.\n3.5.4. Deformable Feature Learning\nA good detector should be robust to nonrigid deformation of\nobjects. Before the deep learning era, Deformable Part based\nModels(DPMs) [28] had been successfully used for object de-\ntection. DPMs represented objects by multiple component parts\nusing a deformable coding method, making the detector robust\nto nonrigid object transformation. In order to enable detec-\ntors based on deep learning to model deformations of object\nparts, many researchers have developed detection frameworks\nto explicitly model object parts [97, 127, 144, 145]. DeepID-\nNet [127] developed a deformable-aware pooling layer to en-\ncode the deformation information across different object cate-\ngories. Dai et al. [97] and Zhu et al. [144] designed deformable\nconvolutional layers which automatically learned the auxiliary\nposition offsets to augment information sampled in regular sam-\npling locations of the feature map.\n4. Learning Strategy\nIn contrast to image classiﬁcation, object detection requires\noptimizing both localization and classiﬁcation tasks, which\nmakes it more difﬁcult to train robust detectors. In addition,\nthere are several issues that need to be addressed, such as im-\nbalance sampling, localization, acceleration etc. Thus there is a\nneed to develop innovative learning strategies to train effective\nand efﬁcient detectors. In this section, we review some of the\nlearning strategies for object detection.\n4.1. Training Stage\nIn this section, we review the learning strategies for training\nobject detectors. Speciﬁcally we discuss, data augmentation,\nimbalance sampling, cascade learning, localization reﬁnement\nand some other learning strategies.\n4.1.1. Data Augmentation.\nData augmentation is important for nearly all deep learning\nmethods as they are often data-hungry and more training data\nleads to better results. In object detection, in order to increase\ntraining data as well as generate training patches with multiple\nvisual properties, Horizontal ﬂips of training images is used in\ntraining Faster R-CNN detector [38]. A more intensive data\naugmentation strategy is used in one-stage detectors including\nrotation, random crops, expanding and color jittering [42, 106,\n146]. This data augmentation strategy has shown signiﬁcant\nimprovement in detection accuracy.\n4.1.2. Imbalance Sampling\nIn object detection, imbalance of negative and positive sam-\nples is a critical issue. That is, most of the regions of interest\nestimated as proposals are in fact just background images. Very\nfew of them are positive instances (or objects). This results\nin problem of imbalance while training detectors. Speciﬁcally,\ntwo issues arise, which need to be addressed: class imbalance\nand difﬁculty imbalance. The class imbalance issue is that most\ncandidate proposals belong to the background and only a few\nof proposals contain objects. This results in the background\nproposals dominating the gradients during training. The difﬁ-\nculty imbalance is closely related to the ﬁrst issue, where due\nto the class imbalance, it becomes much easier to classify most\nof the background proposals easily, while the objects become\nharder to classify. A variety of strategies have been developed\nto tackle the class imbalance issue. Two-stage detectors such\nas R-CNN and Fast R-CNN will ﬁrst reject majority of nega-\ntive samples and keep 2,000 proposals for further classiﬁcation.\nIn Fast R-CNN [38], negative samples were randomly sampled\nfrom these 2k proposals and the ratio of positive and negative\nwas ﬁxed as 1:3 in each mini-batch, to further reduce the ad-\nverse effects of class imbalance. Random sample can address\nclass imbalance issue but are not able to fully utilize informa-\ntion from negative proposals. Some negative proposals may\ncontain rich context information about the images, and some\nhard proposals can help to improve detection accuracy. To ad-\ndress this, Liu et al. [42] proposed hard negative sampling strat-\negy which ﬁxed the foreground and background ratio but sam-\npled most difﬁcult negative proposals for updating the model.\n20\nSpeciﬁcally, negative proposals with higher classiﬁcation loss\nwere selected for training.\nTo address difﬁculty imbalance, most sampling strategies\nare based on carefully designed loss functions.\nFor obejct\ndetection, a multi-class classiﬁer is learned over C+1 cate-\ngories(C target categories plus one background category). As-\nsume the region is labeled with ground truth class u, and p is the\noutput discrete probability distribution over C+1 classes(p =\n{p0, ..., pC}). The loss function is given by:\nLcls(p, u) = −log pu\n(9)\nLin et al. proposed a novel focal loss[43] which suppressed sig-\nnals from easy samples. Instead of discarding all easy samples,\nthey assigned an importance weight to each sample w.r.t its loss\nvalue as:\nLFL = −α(1 −pu)γ log(pu)\n(10)\nwhere α and γ were parameters to control the importance\nweight. The gradient signals of easy samples got suppressed\nwhich led the training process to focus more on hard propos-\nals. Li et al. [147] adopt a similar idea from focal loss and\npropose a novel gradient harmonizing mechanism (GHM). The\nnew proposed GHM not only suppressed easy proposals but\nalso avoided negative impact of outliers. Shrivastava et al. [148]\nproposed an online hard example mining strategy which was\nbased on a similar principle as Liu et al.’s SSD [42] to auto-\nmatically select hard examples for training. Different from Liu\net al., online hard negative mining only considered difﬁculty\ninformation but ignored categorical information, which meant\nthe ratio of foreground and background was not ﬁxed in each\nmini-batch. They argued that difﬁcult samples played a more\nimportant role than class imbalance in object detection task.\n4.1.3. Localization Reﬁnement\nAn object detector must provide a tight localization predic-\ntion (bbox or mask) for each object. To do this, many efforts\nreﬁne the preliminary proposal prediction to improve the local-\nization. Precise localization is challenging because predictions\nare commonly focused on the most discriminative part of the\nobjects, and not necessarily the region containing the object. In\nsome scenarios, the detection algorithms are required to make\nhigh quality predictions (high IoU threshold) See Fig. 9 for an\nillustration of how a detector may fail in a high IoU threshold\nregime. A general approach for localization reﬁnement is to\ngenerate high quality proposals (See Sec 3.4). In this section,\nwe will review some other methods for localization reﬁnement.\nIn R-CNN framework, the L-2 auxiliary bounding box regres-\nsors were learned to reﬁne localizations, and in Fast R-CNN,\nthe smooth L1 regressors were learned via an end-to-end train-\ning scheme as:\nLreg(tc, v) =\nX\ni∈{x,y,w,h}\nSmoothL1(tc\ni −vi)\n(11)\nSmoothL1(x) =\n\n\n\n0.5x2\nif |x| < 1\n|x| −0.5\notherwise\n(12)\nwhere the predicted offset is given by tc = (tc\nx, tc\ny, tc\nw, tc\nh) for\neach target class, and v denotes ground truth of object bounding\nboxes(v = (vx, vy, vw, vh)). x, y, w, h denote bounding box\ncenter, width and height respectively.\nBeyond the default localization reﬁnement, some methods\nlearn auxiliary models to further reﬁne localizations. Gidaris\net al. [131] introduced an iterative bounding box regression\nmethod, where an R-CNN was applied to reﬁne learned predic-\ntions. Here the predictions were reﬁned multiple times. Gidaris\net al. [149] proposed LocNet which modeled the distribution of\neach bounding box and reﬁned the learned predictions. Both\nthese approaches required a separate component in the detec-\ntion pipeline, and prevent joint optimization.\nSome other efforts\n[150, 151] focus on designing a uni-\nﬁed framework with modiﬁed objective functions. In Multi-\nPath Network, Zagoruyko et al. [150] developed an ensemble\nof classiﬁers which were optimized with an integral loss target-\ning various quality metrics. Each classiﬁer was optimized for\n21\nIoU=0.52\nFigure 9: Example of failure case of detection in high IoU threshold. Purple box\nis ground truth and yellow box is prediction. In low IoU requirement scenario,\nthis prediction is correct while in high IoU threshold, it’s a false positive due to\ninsufﬁcient overlap with objects.\na speciﬁc IoU threshold and the ﬁnal prediction results were\nmerged from these classiﬁers. Tychsen et al. proposed Fitness-\nNMS [152] which learned novel ﬁtness score function of IoU\nbetween proposals and objects. They argued that existing detec-\ntors aimed to ﬁnd qualiﬁed predictions instead of best predic-\ntions and thus highly quality and low quality proposals received\nequal importance. Fitness-IoU assigned higher importance to\nhighly overlapped proposals. They also derived a bounding box\nregression loss based on a set of IoU upper bounds to maximum\nthe IoU of predictions with objects. Inspired by CornerNet [63]\nand DeNet [94], Lu et al. [151] proposed a Grid R-CNN which\nreplaced linear bounding box regressor with the principle of lo-\ncating corner keypoints corner-based mechanism.\n4.1.4. Cascade Learning\nCascade learning is a coarse-to-ﬁne learning strategy which\ncollects information from the output of the given classiﬁers to\nbuild stronger classiﬁers in a cascaded manner. Cascade learn-\ning strategy was ﬁrst used by Viola and Jones [17] to train the\nrobust face detectors. In their models, a lightweight detector\nﬁrst rejects the majority easy negatives and feeds hard propos-\nals to train detectors in next stage. For deep learning based\ndetection algorithms, Yang et al. [153] proposed CRAFT (Cas-\ncade Region-proposal-network And FasT-rcnn) which learned\nRPN and region classiﬁers with a cascaded learning strategy.\nCRAFTS ﬁrst learned a standard RPN followed by a two-class\nFast RCNN which rejected the majority easy negatives. The\nremaining samples were used to build the cascade region clas-\nsiﬁers which consisted of two Fast RCNNs. Yang et al. [100]\nintroduced layer-wise cascade classiﬁers for different scale ob-\njects in different layers. Multiple classiﬁers were placed on dif-\nferent feature maps and classiﬁers on shallower layers would\nreject easy negatives. The remaining samples would be fed into\ndeeper layers for classiﬁcation. ReﬁneDet [92] and Cascade\nR-CNN [49] utilized cascade learning methods in reﬁning ob-\nject locations. They built multi-stage bounding box regressors\nand bounding box predictions were reﬁned in each stage trained\nwith different quality metrics. Cheng et al. [132] observed the\nfailure cases of Faster RCNN, and noticed that even though the\nlocalization of objects was good, there were several classiﬁca-\ntion errors. They attributed this to sub-optimal feature repre-\nsentation due to sharing of features and joint multi-task opti-\nmization, for classiﬁcation and regression; and they also argued\nthat the large receptive ﬁeld of Faster RCNN induce too much\nnoise in the detection process. They found that vanilla RCNN\nwas robust to these issues. Thus, they built a cascade detection\nsystem based on Faster RCNN and RCNN to complement each\nother. Speciﬁcally, A set of initial predictions were obtained\nfrom a well trained Faster RCNN, and these predictions were\nused to train RCNN to reﬁne the results.\n4.1.5. Others\nThere are some other learning strategies which offer interest-\ning directions, but have not yet been extensively explored. We\nsplit these approaches into four categories: adversarial learning,\ntraining from scratch and knowledge distillation.\nAdversarial Learning. Adversarial learning has shown sig-\nniﬁcant advances in generative models.\nThe most famous\nwork applying adversarial learning is generative adversarial\nnetwork(GAN) [154] where a generator is competing with a\ndiscriminator. The generator tries to model data distribution by\ngenerating fake images using a noise vector input and use these\nfake images to confuse the discriminator, while the discrimi-\nnator competes with the generator to identify the real images\nfrom fake images. GAN and its variants [155, 156, 157] have\n22\nshown effectiveness in many domains and have also found ap-\nplications in object detection. Li et al. [158] proposed a new\nframework Perceptual GAN for small object detection. The\nlearnable generator learned high-resolution feature representa-\ntions of small objects via an adversarial scheme. Speciﬁcally,\nits generator learned to transfer low-resolution small region fea-\ntures into high-resolution features and competed with the dis-\ncriminator which identiﬁed real high-resolution features. Fi-\nnally the generator learned to generate high quality features\nfor small objects. Wang et al. [159] proposed A-Fast-R-CNN\nwhich was trained by generated adversarial examples. They ar-\ngued the difﬁcult samples were on long tail so they introduced\ntwo novel blocks which automatically generated features with\nocclusion and deformation. Speciﬁcally, a learned mask was\ngenerated on region features followed by region classiﬁers. In\nthis case, the detectors could receive more adversarial examples\nand thus become more robust.\nTraining from Scratch. Modern object detectors heavily\nrely on pre-trained classiﬁcation models on ImageNet, how-\never, the bias of loss functions and data distribution between\nclassiﬁcation and detection can have an adversarial impact on\nthe performance. Finetuning on detection task can relieve this\nissue, but cannot fully get rid of the bias. Besides, transferring\na classiﬁcation model for detection in a new domain can lead\nto more challenges (from RGB to MRI data etc.). Due to these\nreasons, there is a need to train detectors from scratch, instead\nof relying on pretrained models. The main difﬁculty of train-\ning detectors from scratch is the training data of object detec-\ntion is often insufﬁcient and may lead to overﬁtting. Different\nfrom image classiﬁcation, object detection requires bounding\nbox level annotations and thus, annotating a large scale detec-\ntion dataset requires much more effort and time(ImageNet has\n1000 categories for image classiﬁcation while only 200 of them\nhave detection annotations).\nThere are some works [107, 160, 161] exploring training ob-\nject detectors from scratch. Shen et al. [107] ﬁrst proposed\na novel framework DSOD (Deeply Supervised Object Detec-\ntors) to train detectors from scratch. They argued deep super-\nvision with a densely connected network structure could sig-\nniﬁcantly reduce optimization difﬁculties. Based on DSOD,\nShen et al. [162] proposed a gated recurrent feature pyramid\nwhich dynamically adjusted supervision intensities of interme-\ndiate layers for objects with different scales. They deﬁned a\nrecurrent feature pyramid structure to squeeze both spatial and\nsemantic information into a single prediction layer, which fur-\nther reduced parameter numbers leading to faster convergence.\nIn addition, the gate-control structure on feature pyramids adap-\ntively adjusted the supervision at different scales based on the\nsize of objects. Their method was more powerful than original\nDSOD. However, later He et al. [160] validated the difﬁculty\nof training detectors from scratch on MSCOCO and found that\nthe vanilla detectors could obtain a competitive performance\nwith at least 10K annotated images. Their ﬁndings proved no\nspeciﬁc structure was required for training from scratch which\ncontradicted the previous work.\nKnowledge Distillation. Knowledge distillation is a training\nstrategy which distills the knowledge in an ensemble of models\ninto a single model via teacher-student training scheme. This\nlearning strategy was ﬁrst used in image classiﬁcation [163].\nIn object detection, some works [164, 132] also investigate\nthis training scheme to improve detection performance. Li et\nal. [164] proposed a light weight detector whose optimization\nwas carefully guided by a heavy but powerful detector. This\nlight detector could achieve comparable detection accuracy by\ndistilling knowledge from the heavy one, meanwhile having\nfaster inference speed. Cheng et al. [132] proposed a Faster R-\nCNN based detector which was optimized via teacher-student\ntraining scheme.\nAn R-CNN model is used as teacher net-\nwork to guide the training process. Their framework showed\nimprovement in detection accuracy compared with traditional\nsingle model optimization strategy.\n4.2. Testing Stage\nObject detection algorithms make a dense set of predictions\nand thus these predictions cannot be directly used for evalua-\ntion due to heavy duplication. In addition, some other learning\n23\nstrategies are required to further improve the detection accu-\nracy. These strategies improve the quality of prediction or ac-\ncelerate the inference speed. In this section, we introduce these\nstrategies in testing stage including duplicate removal, model\nacceleration and other effective techniques.\n4.2.1. Duplicate Removal\nNon maximum suppression(NMS) is an integral part of ob-\nject detection to remove duplicate false positive predictions\n(See Figure 10). Object detection algorithms make a dense set\nof predictions with several duplicate predictions. For one-stage\ndetection algorithms which generate a dense set of candidate\nproposals such as SSD [42] or DSSD (Deconvolutional Single\nShot Detector) [112], the proposals surrounding the same object\nmay have similar conﬁdence scores, leading to false positives.\nFor two-stage detection algorithms which generates a sparse set\nof proposals, the bounding box regressors will pull these pro-\nposals close to the same object and thus lead to the same prob-\nlem. The duplicate predictions are regarded as false positives\nand will receive penalties in evaluation, so NMS is needed to\nremove these duplicate predictions. Speciﬁcally, for each cat-\negory, the prediction boxes are sorted according to the conﬁ-\ndence score and the box with highest score is selected. This box\nis denoted as M. Then IoU of other boxes with M is calculated,\nand if the IoU value is larger than a predeﬁned threshold Ωtest,\nthese boxes will are removed. This process is repeated for all\nremaining predictions. More formally, the conﬁdence score of\nbox B which overlaps with M larger than Ωtest will be set to\nzero:\nScoreB =\n\n\n\nScoreB\nIoU(B, M) < Ωtest\n0\nIoU(B, M) ≥Ωtest\n(13)\nHowever, if an object just lies within Ωtest of M, NMS will\nresult in a missing prediction, and this scenario is very common\nin clustered object detection. Navaneeth et al. [165] introduced\na new algorithm Soft-NMS to address this issue. Instead of\ndirectly eliminating the prediction B, Soft-NMS decayed the\nconﬁdence score of B as a continuous function F(F can be\nNMS\nConfidence. Score: 0.5\nConfidence. Score: 0.9\nConfidence. Score: 0.6\nConfidence. Score: 0.7\nFigure 10: Duplicate predictions are eliminated by NMS operation. The most-\nconﬁdent box is kept, and all other boxes surrounding it will be removed.\nlinear function or guassian function) of its overlaps with M.\nThis is given by:\nScoreB =\n\n\n\nScoreB\nIoU(B, M) < Ωtest\nF(IoU(B, M))\nIoU(B, M) ≥Ωtest\n(14)\nSoft-NMS avoided eliminating prediction of clustered ob-\njects and showed improvement in many common benchmarks.\nHosong et al.[166] introduced a network architecture designed\nto perform NMS based on conﬁdence scores and bounding\nboxes, which was optimized separately from detector training\nin a supervised way. They argued the reason for duplicate pre-\ndictions was that the detector deliberately encouraged multiple\nhigh score detections per object instead of rewarding one high\nscore. Based on this, they designed the network following two\nmotivations: (i) a loss penalizing double detections to push de-\ntectors to predict exactly one precise detection per object; (ii)\njoint processing of detections nearby to give the detector infor-\nmation whether an object is detected more than once. The new\nproposed model did not discard detections but instead reformu-\nlated NMS as a re-scoring task that sought to decrease the score\nof detections that cover objects that already have been detected.\n4.2.2. Model Acceleration\nApplication of object detection for real world application re-\nquires the algorithms to function in an efﬁcient manner. Thus,\nevaluating detectors on efﬁciency metrics is important.\nAl-\nthough current state-of-the-art algorithms [167, 1] can achieve\nvery strong results on public datasets, their inference speeds\nmake it difﬁcult to apply them into real applications. In this sec-\n24\ntion we review several works on accelerating detectors. Two-\nstage detectors are usually slower than one-stage detectors be-\ncause they have two stages - one proposal generation and one\nregion classiﬁcation, which makes them computationally more\ntime consuming than one-stage detectors which directly use one\nnetwork for both proposal generation and region classiﬁcation.\nR-FCN [52] built spatially-sensitive feature maps and extracted\nfeatures with position sensitive ROI Pooling to share compu-\ntation costs. However, the number of channels of spatially-\nsensitive feature maps signiﬁcantly increased with the number\nof categories. Li et al. [168] proposed a new framework Light\nHead R-CNN which signiﬁcantly reduced the number of chan-\nnels in the ﬁnal feature map (from 1024 to 16) instead of shar-\ning all computation. Thus, though computation was not shared\nacross regions, but the cost could be neglected.\nFrom the aspect of backbone architecture, a major compu-\ntation cost in object detection is feature extraction [34].\nA\nsimple idea to accelerate detection speed is to replace the de-\ntection backbone with a more efﬁcient backbone, e.g., Mo-\nbileNet [74, 169] was an efﬁcient CNN model with depth-wise\nconvolution layers which was also adopted into many works\nsuch as [170] and [171].\nPVANet [104] was proposed as a\nnew network structure with CReLu [172] layer to reduce non-\nlinear computation and accelerated inference speed. Another\napproach is to optimize models off-line, such as model com-\npression and quantization [173, 174, 175, 176, 177, 178, 179]\non the learned models. Finally, NVIDIA Corporation1 released\nan acceleration toolkit TensorRT2 which optimized the compu-\ntation of learned models for deployment and thus signiﬁcantly\nsped up the inference.\n4.2.3. Others\nOther learning strategies in testing stage mainly comprise the\ntransformation of input image to improve the detection accu-\nracy. Image pyramids [1, 92] are a widely used technique to\nimprove detection results, which build a hierarchical image set\n1https://www.nvidia.com/en-us/\n2https://developer.nvidia.com/tensorrt\nat different scales and make predictions on all of these images.\nThe ﬁnal detection results are merged from the predictions of\neach image. Zhang et al. [87, 92] used a more extensive im-\nage pyramid structure to handle different scale objects. They\nresized the testing image to different scales and each scale was\nresponsible for a certain scale range of objects. Horizontal Flip-\nping [3, 92] was also used in the testing stage and also showed\nimprovement. These learning strategies largely improved the\nthe capability of detector to handle different scale objects and\nthus were widely used in public detection competitions. How-\never, they also increase computation cost and thus were not suit-\nable for real world applications.\n5. Applications\nObject detection is a fundamental computer vision task and\nthere are many real world applications based on this task. Dif-\nferent from generic object detection, these real world applica-\ntions commonly have their own speciﬁc properties and thus\ncarefully-designed detection algorithms are required. In this\nsection, we will introduce several real world applications such\nas face detection and pedestrian detection.\n5.1. Face Detection\nFace detection is a classical computer vision problem to de-\ntect human faces in the images, which is often the ﬁrst step\ntowards many real-world applications with human beings, such\nas face veriﬁcation, face alignment and face recognition. There\nare some critical differences between face detection and generic\ndetection: i) the range of scale for objects in face detection is\nmuch larger than objects in generic detection. Moreover occlu-\nsion and blurred cases are more common in face detection; ii)\nFace objects contain strong structural information, and there is\nonly one target category in face detection. Considering these\nproperties of face detection, directly applying generic detection\nalgorithms is not an optimal solution as there could be some\npriors that can exploited to improve face detection.\nIn early stages of research before the deep learning era,\nface detection [20, 180, 181, 182] was mainly based on slid-\n25\ning windows, and dense image grids were encoded by hand-\ncrafted features followed by training classiﬁers to ﬁnd and lo-\ncate objects.\nNotably, Viola and Jones [20] proposed a pi-\noneering cascaded classiﬁers using AdaBoost with Haar fea-\ntures for face detection and obtained excellent performance\nwith high real time prediction speed. After the progresses of\ndeep learning in image classiﬁcation, face detectors based on\ndeep learning signiﬁcantly outperformed traditional face detec-\ntors [183, 184, 185, 186, 187].\nCurrent face detection algorithms based on deep learning are\nmainly extended from generic detection frameworks such as\nFast R-CNN and SSD. These algorithms focus more on learning\nrobust feature representations. In order to handle extreme scale\nvariance, multi-scale feature learning methods discussed before\nhave been widely used in face detection. Sun et al. [183] pro-\nposed a Fast R-CNN based framework which integrated multi-\nscale features for prediction and converted the resulting detec-\ntion bounding boxes into ellipses as the regions of human faces\nare more elliptical than rectangular. Zhang et al. [87] proposed\none-stage S3FD which found faces on different feature maps to\ndetect faces at a large range of scales. They made predictions\non larger feature maps to capture small-scale face information.\nNotably, a set of anchors were carefully designed according to\nempirical receptive ﬁelds and thus provided a better match to\nthe faces. Based on S3FD, Zhang et al. [188] proposed a novel\nnetwork structure to capture multi-scale features in different\nstages. The new proposed feature agglomerate structure inte-\ngrated features at different scales in a hierarchical way. More-\nover, a hierarchical loss was proposed to reduce the training dif-\nﬁculties. Single Stage Headless Face Detector (SSH) [189] was\nanother one-stage face detector which combined different scale\nfeatures for prediction. Hu et al. [99] gave a detailed analysis of\nsmall face detection and proposed a light weight face detector\nconsisting of multiple RPNs, each of which was responsible for\na certain range of scales. Their method could effectively handle\nface scale variance but it was slow for real world usage. Unlike\nthis method, Hao et al. [190] proposed a Scale Aware Face net-\nwork which addresses scale issues without incurring signiﬁcant\ncomputation costs. They learned a scale aware network which\nmodeled the scale distribution of faces in a given image and\nguided zoom-in or zoom-out operations to make sure that the\nfaces were in desirable scale. The resized image was fed into\na single scale light weight face detector. Wang et al. [191] fol-\nlowed RetinaNet [43] and utilized more dense anchors to han-\ndle faces in a large range of scales. Moreover, they proposed\nan attention function to account for context information, and to\nhighlight the discriminative features. Zhang et al. [192] pro-\nposed a deep cascaded multi-task face detector with cascaded\nstructure (MTCNN). MTCNN had three stages of carefully de-\nsigned CNN models to predict faces in a coarse-to-ﬁne style.\nFurther, they also proposed a new online hard negative mining\nstrategy to improve the result. Samangouei et al. [193] pro-\nposed a Face MegNet which allowed information ﬂow of small\nfaces without any skip connections by placing a set of decon-\nvolution layers before RPN and ROI Pooling to build up ﬁner\nface representations.\nIn addition to multi-scale feature learning, some frameworks\nwere focused on contextual information.\nFace objects have\nstrong physical relationships with the surrounding contexts\n(commonly appearing with human bodies) and thus encoding\ncontextual information became an effective way to improve de-\ntection accuracy. Zhang et al. [194] proposed FDNet based on\nResNet with larger deformable convolutional kernels to cap-\nture image context. Zhu et al. [195] proposed a Contextual\nMulti-Scale Region-based Convolution Neural Network (CMS-\nRCNN) in which multi-scale information was grouped both in\nregion proposal and ROI detection to deal with faces at vari-\nous range of scale. In addition, contextual information around\nfaces is also considered in training detectors. Notably, Tang et\nal. [185] proposed a state-of-the-art context assisted single shot\nface detector, named PyramidBox to handle the hard face detec-\ntion problem. Observing the importance of the context, they im-\nproved the utilization of contextual information in the follow-\ning three aspects: i) ﬁrst, a novel context anchor was designed\nto supervise high-level contextual feature learning by a semi-\nsupervised method, dubbed as PyramidAnchors; ii) the Low-\n26\nlevel Feature Pyramid Network was developed to combine ade-\nquate high-level context semantic features and low-level facial\nfeatures together, which also allowed the PyramidBox to pre-\ndict faces at all scales in a single shot; and iii) they introduced a\ncontext sensitive structure to increase the capacity of prediction\nnetwork to improve the ﬁnal accuracy of output. In addition,\nthey used the method of data-anchor-sampling to augment the\ntraining samples across different scales, which increased the di-\nversity of training data for smaller faces. Yu et al.[196] intro-\nduced a context pyramid maxout mechanism to explore image\ncontexts and devised an efﬁcient anchor based cascade frame-\nwork for face detection which optimized anchor-based detector\nin cascaded manner. Zhang et al. [197] proposed a two-stream\ncontextual CNN to adaptively capture body part information.\nIn addition, they proposed to ﬁlter easy non-face regions in the\nshallow layers and leave difﬁcult samples to deeper layers.\nBeyond efforts on designing scale-robust or context-assistant\ndetectors, Wang et al. [191] developed a framework from the\nperspective of loss function design. Based on vanilla Faster\nR-CNN framework, they replaced original softmax loss with\na center loss which encouraged detectors to reduce the large\nintra-class variance in face detection. They explored multiple\ntechnologies in improving Faster R-CNN such as ﬁxed-ratio\nonline hard negative mining, multi-scale training and multi-\nscale testing, which made vanilla Faster R-CNN adaptable to\nface detection. Later, Wang et al. [198] proposed Face R-FCN\nwhich was based on vanilla R-FCN. Face R-FCN distinguished\nthe contribution of different facial parts and introduced a novel\nposition-sensitive average pooling to re-weight the response on\nﬁnal score maps. This method achieved state-of-the-art results\non many public benchmarks such as FDDB [199] and WIDER\nFACE[200].\n5.2. Pedestrian Detection\nPedestrian detection is an essential and signiﬁcant task in any\nintelligent video surveillance system. Different from generic\nobject detection, there are some properties of pedestrian de-\ntection different from generic object detection: i) Pedestrian\nobjects are well structured objects with nearly ﬁxed aspect ra-\ntios (about 1.5), but they also lie at a large range of scales; ii)\nPedestrian detection is a real world application, and hence the\nchallenges such as crowding, occlusion and blurring are com-\nmonly exhibited. For example, in the CityPersons dataset, there\nare a total of 3157 pedestrian annotations in the validation sub-\nset, among which 48.8% overlap with another annotated pedes-\ntrian with Intersection over Union (IoU) above 0.1. Moreover,\n26.4% of all pedestrians have considerable overlap with another\nannotated pedestrian with IoU above 0.3. The highly frequent\ncrowd occlusion harms the performance of pedestrian detec-\ntors; iii) There are more hard negative samples (such as trafﬁc\nlight, Mailbox etc.) in pedestrian detection due to complicated\ncontexts.\nBefore the deep learning era, pedestrian detection algorithms\n[19, 201, 202, 203, 204] were mainly extended from Viola\nJones frameworks [20] by exploiting Integral Channel Features\nwith a sliding window strategy to locate objects, followed by re-\ngion classiﬁers such as SVMs. The early works were mainly fo-\ncused on designing robust feature descriptors for classiﬁcation.\nFor example, Dalal and Triggs [19] proposed the histograms of\noriented gradient (HOG) descriptors, while Paisitkriangkrai et\nal. [204] designed a feature descriptor based on low-level visual\ncues and spatial pooling features. These methods show promis-\ning results on pedestrian detection benchmarks but were mainly\nbased on hand-crafted features.\nDeep learning based methods for pedestrian detection [8, 9,\n10, 205, 206, 207, 208, 209, 210, 211] showed excellent per-\nformance and achieved state-of-the-art results on public bench-\nmarks. Angelova et al [10] proposed a real-time pedestrian de-\ntection framework using a cascade of deep convolutional net-\nworks. In their work, a large number of easy negatives were\nrejected by a tiny model and the remaining hard proposals were\nthen classiﬁed by a large deep networks. Zhang et al. [212]\nproposed decision tree based framework. In their method, mul-\ntiscale feature maps were used to extract pedestrian features,\nwhich were later fed into boosted decision trees for classiﬁ-\ncation. Compared with FC layers, boosted decision trees ap-\n27\nplied bootstrapping strategy for mining hard negative samples\nand achieved better performance. Also to reduce the impact\nof large variance in scales, Li et al. [8] proposed Scale-aware\nFast R-CNN (SAF RCNN) which inserted multiple built-in net-\nworks into the whole detection framework. The proposed SAF\nRCNN detected different scale pedestrian instances using dif-\nferent sub-net. Further, Yang et al. [100] inserted Scale Depen-\ndent Pooling (SDP) and Cascaded Rejection Classiﬁers (CRC)\ninto Fast RCNN to handle pedestrian scale issue. According\nto the height of the instances, SDP extracted region features\nfrom suitable scale feature maps, while CRC rejected easy neg-\native samples in shallower layers. Wang et al. [213] proposed\na novel Repulsion Loss to detect pedestrians in a crowd. They\nargued that detecting a pedestrian in a crowd made it very sen-\nsitive to the NMS threshold, which led to more false positives\nand missing objects. The new proposed repulsion loss pushed\nthe proposals into their target objects but also pulled them away\nfrom other objects and their target proposals. Based on their\nidea, Zhang et al. [214] proposed an Occlusion-aware R-CNN\n(OR-CNN) which was optimized by Aggression Loss. The new\nloss function encouraged the proposals to be close to the objects\nand other proposals with the same targeted proposals. Mao et\nal. [215] claimed that properly aggregating extra features into\npedestrian detector can boost detection accuracy. In their pa-\nper, they explored different kinds of extra features useful in im-\nproving accuracy and proposed a new method to use these fea-\ntures. The new proposed component HyperLearner aggregated\nextra features into vanilla DCNN detector via jointly optimiza-\ntion fashion and no extra input was required in inference stage.\nFor pedestrian detection, one of the most signiﬁcant chal-\nlenges is to handle occlusion [216, 217, 218, 219, 220, 221,\n222, 223, 224, 225, 214, 226]. A straightforward method is\nto use part-based models which learn a series of part detectors\nand integrate the results of part detectors to locate and clas-\nsify objects. Tian et al. [216] proposed DeepParts which con-\nsisted of multiple parts detectors. During training, the impor-\ntant body parts were automatically selected from a part pool\ncovered all scale parts of the body, and for each selected part,\na detector was learned to handle occlusions. To avoid integrate\ninaccurate scores of part models, Ouyang and Wang [223] pro-\nposed a framework which modeled visible parts as hidden vari-\nables in training the models. In their paper, the visible rela-\ntionship of overlapping parts were learned by a discriminative\ndeep models, instead of being manually deﬁned or even being\nassumed independent.\nLater, Ouyang et al. [225] addressed\nthis issue from another aspect. They proposed a mixture net-\nwork to capture unique visual information which was formed by\ncrowded pedestrians. To enhance the ﬁnal predictions of single-\npedestrian detectors, a probabilistic framework was learned to\nmodel the relationship between the conﬁgurations estimated by\nsingle- and multi-pedestrian detectors. Zhang et al. [214] pro-\nposed an occlusion-aware ROI Pooling layer which integrated\nthe prior structure information of pedestrian with visibility pre-\ndiction into the ﬁnal feature representations. The original re-\ngion was divided into ﬁve parts and for each part a sub-network\nenhanced the original region feature via a learned visibility\nscore for better representations.\nZhou et al. [222] proposed\nBi-box which simultaneously estimated pedestrian detection as\nwell as visible parts by regressing two bounding boxes, one for\nthe full body and the other for visible part. In addition, a new\npositive-instance sampling criterion was proposed to bias pos-\nitive training examples with large visible area, which showed\neffectiveness in training occlusion-aware detectors.\n5.3. Others\nThere are some other real applications with object detection\ntechniques, such as logo detection and video object detection.\nLogo detection is an important research topic in e-commerce\nsystems. Compared to generic detection, logo instance is much\nsmaller with strong non-rigid transformation. Further, there are\nfew logo detection baselines available. To address this issue, Su\net al. [15] adopted the learning principle of webly data learning\nwhich automatically mined information from noisy web images\nand learns models with limited annotated data. Su et al. [14]\ndescribed an image synthesising method to successfully learn a\ndetector with limited logo instances. Hoi et al. [13] collected a\n28\nlarge scale logo dataset from an e-commerce website and con-\nducted a comprehensive analysis on the problem logo detection.\nExisting detection algorithms are mainly designed for still\nimages and are suboptimal for directly applying in videos for\nobject detection.\nTo detect objects in videos, there are two\nmajor differences from generic detection: temporal and con-\ntextual information. The location and appearance of objects in\nvideo should be temporally consistent between adjacent frames.\nMoreover, a video consists of hundreds of frames and thus con-\ntains far richer contextual information compared to a single\nstill image. Han et al. [54] proposed a Seq-NMS which as-\nsociates detection results of still images into sequences. Boxes\nof the same sequence are re-scored to the average score across\nframes, and other boxes along the sequence are suppressed by\nNMS. Kang et al. proposed Tubelets with Convolutional Neu-\nral Networks (T-CNN) [53] which was extended from Faster\nRCNN and incorporated the temporal and contextual informa-\ntion from tubelets(box sequence over time). T-CNN propagated\nthe detection results to the adjacent frames by optical ﬂow, and\ngenerated tubelets by applying tracking algorithms from high-\nconﬁdence bounding boxes. The boxes along the tubelets were\nre-scored based on tubelets classiﬁcation.\nThere are also many other real-world applications based on\nobject detection such as vehicle detection [227, 228, 229],\ntrafﬁc-sign detection [230, 231] and skeleton detection [232,\n233].\n6. Detection Benchmarks\nIn this section we will show some common benchmarks of\ngeneric object detection, face detection and pedestrian detec-\ntion. We will ﬁrst present some widely used datasets for each\ntask and then introduce the evaluation metrics.\n6.1. Generic Detection Benchmarks\nPascal VOC2007 [29] is a mid scale dataset for object detection\nwith 20 categories. There are three image splits in VOC2007:\ntraining, validation and test with 2501, 2510 and 5011 images\nrespectively.\nPascal VOC2012 [29] is a mid scale dataset for object detec-\ntion which shares the same 20 categories with Pascal VOC2007.\nThere are three image splits in VOC2012: training, validation\nand test with 5717, 5823 and 10991 images respectively. The\nannotation information of VOC2012 test set is not available.\nMSCOCO [86] is a large scale dataset for with 80 categories.\nThere are three image splits in MSCOCO: training, validation\nand test with 118287, 5000 and 40670 images respectively. The\nannotation information of MSCOCO test set is not available.\nOpen Images [234] contains 1.9M images with 15M objects\nof 600 categories. The 500 most frequent categories are used\nto evaluate detection benchmarks, and more than 70% of these\ncategories have over 1000 training samples.\nLVIS [235] is a new collected benchmark with 164000 images\nand 1000+ categories. It’s a new dataset without any existing\nresults so we leave the details of LVIS in future work section\n(Section 8).\nImageNet [37] is also a important dataset with 200 categories.\nHowever, the scale of ImageNet is huge and the object scale\nrange is similar to VOC datasets, so it is not a commonly used\nbenchmarks for detection algorithms.\nEvaluation Metrics:\nThe detail of evaluation metrics are\nshown in Tab. 1, both detection accuracy and inference speed\nare used to evaluate detection algorithms. For detection accu-\nracy, mean Average Precision(mAP) is used as evaluation met-\nric for all these challenges. For VOC2012, VOC2007 and Im-\nageNet, IoU threshold of mAP is set to 0.5, and for MSCOCO,\nmore comprehensive evaluation metrics are applied.\nThere\nare six evaluation scores which demonstrates different capabil-\nity of detection algorithms, including performance on different\nIoU thresholds and on different scale objects. Some examples\nof listed datasets(Pascal VOC, MSCOCO, Open Images and\nLVIS) are shown in Fig. 11.\n6.2. Face Detection Benchmarks\nIn this section, we introduce several widely used face detec-\ntion datasets (WIDER FACE, AFW, FDDB and Pascal Face)\nand the commonly used evaluation metrics.\n29\nPascal VOC\nMSCOCO\nOpen Images\nLVIS\nHorse\nPerson\nTV\nChair\nChair\nFigure 11: Some examples of Pascal VOC, MSCOCO, Open Images and LVIS.\nAlias\nMeaning\nDeﬁnition and Description\nFPS\nFrame per second The number of images processed per second.\nΩ\nIoU threshold\nThe IoU threshold to evaluate localization.\nDγ\nAll Predictions\nTop γ predictions returned by the detectors\nwith highest conﬁdence score.\nTPγ\nTrue Positive\nCorrect predictions from sampled predictions\nFPγ\nFalse Positive\nFalse predictions from sampled predictions.\nPγ\nPrecision\nThe fraction of TPγ out of Dγ.\nRγ\nRecall\nThe fraction of TPγ out of all positive samples.\nAP\nAverage Precision Computed over the different levels of recall by varying the γ.\nmAP\nmean AP\nAverage score of AP across all classes.\nTPR True Positive Rate The fraction of positive rate over false positives.\nFPPI\nFP Per Image\nThe fraction of false positive for each image.\nMR\nlog-average\nmissing rate\nAverage miss rate over different FPPI rates evenly spaced in log-space\nGeneric Object Detection\nmAP\nmean\nAverage\nPrecision\nVOC2007\nmAP at 0.50 IoU threshold over all 20 classes.\nVOC2012\nmAP at 0.50 IoU threshold over all 20 classes.\nOpenImages\nmAP at 0.50 IoU threshold over 500 most frequent classes.\nMSCOCO\n• APcoco: mAP averaged over ten Ω: {0.5 : 0.05 : 0.95};\n• AP50: mAP at 0.50 IoU threshold;\n• AP75: mAP at 0.75 IoU threshold;\n• APS: APcoco for small objects of area smaller than 322;\n• APM : APcoco for objects of area between 322 and 962;\n• APL: APcoco for large objects of area bigger than 962;\nFace Detection\nmAP\nmean\nAverage\nPrecision\nPascal Face\nmAP at 0.50 IoU threshold.\nAFW\nmAP at 0.50 IoU threshold.\nWIDER FACE\n• mAPeasy: mAP for easy level faces;\n• mAPmid: mAP for mid level faces;\n• mAPhard: mAP for hard level faces;\nTPR\nTrue\nPositive\nRate\nFDDB\n• TPRdis with 1k FP at 0.50 IoU threshold, with bbox level.\n• TPRcont with 1k FP at 0.50 IoU threshold, with eclipse level.\nPedestrian Detection\nmAP\nmean\nAverage\nPrecision\nKITTI\n• mAPeasy: mAP for easy level pedestrians;\n• mAPmid: mAP for mid level pedestrians;\n• mAPhard: mAP for hard level pedestrians;\nMR\nlog-average\nmiss rate\nCityPersons\nMR: ranging from 1e−2 to 100 FPPI\nCaltech\nMR: ranging from 1e−2 to 100 FPPI\nETH\nMR: ranging from 1e−2 to 100 FPPI\nINRIA\nMR: ranging from 1e−2 to 100 FPPI\nTable 1: Summary of common evaluation metrics for various detection tasks\nincluding generic object detection, face detection and pedestrian detection.\nWIDER FACE [200]. WIDER FACE has totally 32203 images\nwith about 400k faces for a large range of scales. It has three\nsubsets: 40% for training, 10% for validation, and 50% for test.\nThe annotations of training and validation sets are online avail-\nable. According to the difﬁculty of detection tasks, it has three\nsplits: Easy, Medium and Hard.\nFDDB [199].\nThe Face Detection Data set and Benchmark\n(FDDB) is a well-known benchmark with 5171 faces in 2845\nimages. Commonly face detectors will ﬁrst be trained on a large\nscale dataset(WIDERFACE etc. ) and tested on FDDB.\nPASCAL FACE [29]. This dataset was collected from PAS-\nCAL person layout test set, with 1335 labeled faces in 851 im-\nages. Similar to FDDB, it’s commonly used as test set only.\nEvaluation Metrics. As Tab. 1 shown, the evaluation metric\nfor WIDER FACE and PASCAL FACE is mean average preci-\nsion (mAP) with IoU threshold as 0.5, and for WIDER FACE\nthe results of each difﬁculty level will be reported. For FDDB,\ntrue positive rate (TPR) at 1k false positives are used for evalua-\ntion. There are two annotation types to evaluate FDDB dataset:\nbounding box level and eclipse level.\n6.3. Pedestrian Detection Benchmarks\nIn this section we will ﬁrst introduce ﬁve widely used\ndatasets(Caltech, ETH, INRIA, CityPersons and KITTI) for\npedestrian object detection and then introduce their evaluation\nmetrics.\nCityPersons [242] is a new pedestrian detection dataset on\ntop of the semantic segmentation dataset CityScapes [243], of\nwhich 5000 images are captured in several cities in Germany.\nA total of 35000 persons with an additional 13000 ignored re-\ngions, both bounding box annotation of all persons and annota-\ntion of visible parts are provided.\nCaltech [244]\nis one of the most popular and challenging\ndatasets for pedestrian detection, which comes from approxi-\nmately 10 hours 30Hz VGA video recorded by a car traversing\nthe streets in the greater Los Angeles metropolitan area. The\n30\nMethod\nBackbone\nProposed Year\nInput size(Test)\nmAP (%)\nVOC2007\nVOC2012\nTwo-stage Detectors:\nR-CNN [2]\nVGG-16\n2014\nArbitrary\n66.0∗\n62.4†\nSPP-net [2]\nVGG-16\n2014\n∼600 × 1000\n63.1∗\n-\nFast R-CNN [38]\nVGG-16\n2015\n∼600 × 1000\n70.0\n68.4\nFaster R-CNN [34]\nVGG-16\n2015\n∼600 × 1000\n73.2\n70.4\nMR-CNN [131]\nVGG-16\n2015\nMulti-Scale\n78.2\n73.9\nFaster R-CNN [1]\nResNet-101\n2016\n∼600 × 1000\n76.4\n73.8\nR-FCN [52]\nResNet-101\n2016\n∼600 × 1000\n80.5\n77.6\nOHEM [148]\nVGG-16\n2016\n∼600 × 1000\n74.6\n71.9\nHyperNet [50]\nVGG-16\n2016\n∼600 × 1000\n76.3\n71.4\nION [51]\nVGG-16\n2016\n∼600 × 1000\n79.2\n76.4\nCRAFT [153]\nVGG-16\n2016\n∼600 × 1000\n75.7\n71.3†\nLocNet [149]\nVGG-16\n2016\n∼600 × 1000\n78.4\n74.8†\nR-FCN w DCN [97]\nResNet-101\n2017\n∼600 × 1000\n82.6\n-\nCoupleNet [125]\nResNet-101\n2017\n∼600 × 1000\n82.7\n80.4\nDeNet512(wide) [94]\nResNet-101\n2017\n∼512 × 512\n77.1\n73.9\nFPN-Reconﬁg [115]\nResNet-101\n2018\n∼600 × 1000\n82.4\n81.1\nDeepRegionLet [140]\nResNet-101\n2018\n∼600 × 1000\n83.3\n81.3\nDCN+R-CNN [132]\nResNet-101+ResNet-152\n2018\nArbitrary\n84.0\n81.2\nOne-stage Detectors:\nYOLOv1 [40]\nVGG16\n2016\n448 × 448\n66.4\n57.9\nSSD512 [42]\nVGG-16\n2016\n512 × 512\n79.8\n78.5\nYOLOv2 [41]\nDarknet\n2017\n544 × 544\n78.6\n73.5\nDSSD513 [112]\nResNet-101\n2017\n513 × 513\n81.5\n80.0\nDSOD300 [107]\nDS/64-192-48-1\n2017\n300 × 300\n77.7\n76.3\nRON384 [120]\nVGG-16\n2017\n384 × 384\n75.4\n73.0\nSTDN513 [111]\nDenseNet-169\n2018\n513 × 513\n80.9\n-\nReﬁneDet512 [92]\nVGG-16\n2018\n512 × 512\n81.8\n80.1\nRFBNet512 [108]\nVGG16\n2018\n512 × 512\n82.2\n-\nCenterNet [64]\nResNet101\n2019\n512 × 512\n78.7\n-\nCenterNet [64]\nDLA [64]\n2019\n512 × 512\n80.7\n-\n∗This entry reports the the model is trained with VOC2007 trainval sets only.\n† This entry reports the the model are trained with VOC2012 trainval sets only .\nTable 2: Detection results on PASCAL VOC dataset. For VOC2007, the models are trained on VOC2007 and VOC2012 trainval sets and\ntested on VOC2007 test set. For VOC2012, the models are trained on VOC2007 and VOC2012 trainval sets plus VOC2007 test set\nand tested on VOC2012 test set by default. Since Pascal VOC datasets are well tuned and thus the number of detection frameworks for VOC\nreduces in recent years.\n31\nMethod\nBackbone\nYear\nAP\nAP50\nAP75\nAPS\nAPM\nAPL\nTwo-Stage Detectors:\nFast R-CNN [38]\nVGG-16\n2015\n19.7\n35.9\n-\n-\n-\n-\nFaster R-CNN [34]\nVGG-16\n2015\n21.9\n42.7\n-\n-\n-\n-\nOHEM [148]\nVGG-16\n2016\n22.6\n42.5\n22.2\n5.0\n23.7\n37.9\nION [51]\nVGG-16\n2016\n23.6\n43.2\n23.6\n6.4\n24.1\n38.3\nOHEM++ [148]\nVGG-16\n2016\n25.5\n45.9\n26.1\n7.4\n27.7\n40.3\nR-FCN [52]\nResNet-101\n2016\n29.9\n51.9\n-\n10.8\n32.8\n45.0\nFaster R-CNN+++ [1]\nResNet-101\n2016\n34.9\n55.7\n37.4\n15.6\n38.7\n50.9\nFaster R-CNN w FPN [39]\nResNet-101\n2016\n36.2\n59.1\n39.0\n18.2\n39.0\n48.2\nDeNet-101(wide) [94]\nResNet-101\n2017\n33.8\n53.4\n36.1\n12.3\n36.1\n50.8\nCoupleNet [125]\nResNet-101\n2017\n34.4\n54.8\n37.2\n13.4\n38.1\n50.8\nFaster R-CNN by G-RMI [167]\nInception-ResNet-v2\n2017\n34.7\n55.5\n36.7\n13.5\n38.1\n52.0\nDeformable R-FCN [52]\nAligned-Inception-ResNet\n2017\n37.5\n58.0\n40.8\n19.4\n40.1\n52.5\nMask-RCNN [3]\nResNeXt-101\n2017\n39.8\n62.3\n43.4\n22.1\n43.2\n51.2\numd det [236]\nResNet-101\n2017\n40.8\n62.4\n44.9\n23.0\n43.4\n53.2\nFitness-NMS [152]\nResNet-101\n2017\n41.8\n60.9\n44.9\n21.5\n45.0\n57.5\nDCN w Relation Net [138]\nResNet-101\n2018\n39.0\n58.6\n42.9\n-\n-\n-\nDeepRegionlets [140]\nResNet-101\n2018\n39.3\n59.8\n-\n21.7\n43.7\n50.9\nC-Mask RCNN [141]\nResNet-101\n2018\n42.0\n62.9\n46.4\n23.4\n44.7\n53.8\nGroup Norm [237]\nResNet-101\n2018\n42.3\n62.8\n46.2\n-\n-\n-\nDCN+R-CNN [132]\nResNet-101+ResNet-152\n2018\n42.6\n65.3\n46.5\n26.4\n46.1\n56.4\nCascade R-CNN [49]\nResNet-101\n2018\n42.8\n62.1\n46.3\n23.7\n45.5\n55.2\nSNIP++ [98]\nDPN-98\n2018\n45.7\n67.3\n51.1\n29.3\n48.8\n57.1\nSNIPER++ [146]\nResNet-101\n2018\n46.1\n67.0\n51.6\n29.6\n48.9\n58.1\nPANet++ [238]\nResNeXt-101\n2018\n47.4\n67.2\n51.8\n30.1\n51.7\n60.0\nGrid R-CNN [151]\nResNeXt-101\n2019\n43.2\n63.0\n46.6\n25.1\n46.5\n55.2\nDCN-v2 [144]\nResNet-101\n2019\n44.8\n66.3\n48.8\n24.4\n48.1\n59.6\nDCN-v2++ [144]\nResNet-101\n2019\n46.0\n67.9\n50.8\n27.8\n49.1\n59.5\nTridentNet [239]\nResNet-101\n2019\n42.7\n63.6\n46.5\n23.9\n46.6\n56.6\nTridentNet [239]\nResNet-101-Deformable\n2019\n48.4\n69.7\n53.5\n31.8\n51.3\n60.3\nSingle-Stage Detectors:\nSSD512 [42]\nVGG-16\n2016\n28.8\n48.5\n30.3\n10.9\n31.8\n43.5\nRON384++ [120]\nVGG-16\n2017\n27.4\n49.5\n27.1\n-\n-\n-\nYOLOv2 [41]\nDarkNet-19\n2017\n21.6\n44.0\n19.2\n5.0\n22.4\n35.5\nSSD513 [112]\nResNet-101\n2017\n31.2\n50.4\n33.3\n10.2\n34.5\n49.8\nDSSD513 [112]\nResNet-101\n2017\n33.2\n53.3\n35.2\n13.0\n35.4\n51.1\nRetinaNet800++ [43]\nResNet-101\n2017\n39.1\n59.1\n42.3\n21.8\n42.7\n50.2\nSTDN513 [111]\nDenseNet-169\n2018\n31.8\n51.0\n33.6\n14.4\n36.1\n43.4\nFPN-Reconﬁg [115]\nResNet-101\n2018\n34.6\n54.3\n37.3\n-\n-\n-\nReﬁneDet512 [92]\nResNet-101\n2018\n36.4\n57.5\n39.5\n16.6\n39.9\n51.4\nReﬁneDet512++ [92]\nResNet-101\n2018\n41.8\n62.9\n45.7\n25.6\n45.1\n54.1\nGHM SSD [147]\nResNeXt-101\n2018\n41.6\n62.8\n44.2\n22.3\n45.1\n55.3\nCornerNet511 [63]\nHourglass-104\n2018\n40.5\n56.5\n43.1\n19.4\n42.7\n53.9\nCornerNet511++ [63]\nHourglass-104\n2018\n42.1\n57.8\n45.3\n20.8\n44.8\n56.7\nM2Det800 [116]\nVGG-16\n2019\n41.0\n59.7\n45.0\n22.1\n46.5\n53.8\nM2Det800++ [116]\nVGG-16\n2019\n44.2\n64.6\n49.3\n29.2\n47.9\n55.1\nExtremeNet [240]\nHourglass-104\n2019\n40.2\n55.5\n43.2\n20.4\n43.2\n53.1\nCenterNet-HG [64]\nHourglass-104\n2019\n42.1\n61.1\n45.9\n24.1\n45.5\n52.8\nFCOS [241]\nResNeXt-101\n2019\n42.1\n62.1\n45.2\n25.6\n44.9\n52.0\nFSAF [95]\nResNeXt-101\n2019\n42.9\n63.8\n46.3\n26.6\n46.2\n52.7\nCenterNet511 [65]\nHourglass-104\n2019\n44.9\n62.4\n48.1\n25.6\n47.4\n57.4\nCenterNet511++ [65]\nHourglass-104\n2019\n47.0\n64.5\n50.7\n28.9\n49.9\n58.9\nTable 3: Detection performance on the MS COCO test-dev data set. ”++” denotes applying inference strategy such as multi scale test, horizontal ﬂip, etc.\n32\ntraining and testing sets contains 42782 and 4024 frames, re-\nspectively.\nETH [245] contains 1804 frames in three video clips and com-\nmonly it’s used as test set to evaluate performance of the models\ntrained on the large scale datasets(CityPersons dataset etc.).\nINRIA [19] contains images of high resolution pedestrians col-\nlected mostly from holiday photos, which consists of 2120\nimages, including 1832 images for training and 288 images.\nSpeciﬁcally, there are 614 positive images and 1218 negative\nimages in the training set.\nKITTI [246] contains 7481 labeled images of resolution\n1250x375 and another 7518 images for testing. The person\nclass in KITTI is divided into two subclasses: pedestrian and\ncyclist, both evaluated by mAP method. KITTI contains three\nevaluation metrics: easy, moderate and hard, with difference in\nthe min. bounding box height, max. occlusion level, etc.\nEvaluation Metrics. For CityPersons, INRIA and ETH, the\nlog-average miss rate over 9 points ranging from 1e−2 to 100\nFPPI(False Positive Per Image) is used to evaluate the perfor-\nmance of the detectors(lower is better). For KITTI, standard\nmean average precision is used as evaluation metric with 0.5\nIoU threshold.\n7. State-of-the-art for Generic Object Detection\nPascal VOC2007, VOC2007 and MSCOCO are three most\ncommonly used datasets for evaluating detection algorithms.\nPascal VOC2012 and VOC2007 are mid scale datasets with 2\nor 3 objects per image and the range of object size in VOC\ndataset is not large. For MSCOCO, there are nearly 10 objects\nper image and the majority objects are small objects with large\nscale ranges, which leads to a very challenge task for detection\nalgorithms. In Table 3 and Table 2 we give the benchmarks of\nVOC2007, VOC2012 and MSCOCO over the recent few years.\n8. Concluding Remarks and Future Directions\nObject detection has been actively investigated and new\nstate-of-the-art results have been reported almost every few\nmonths. However, there are still many open challenges. Be-\nlow we discuss sveral open challenges and future directions.\ni) Scalable Proposal Generation Strategy.\nAs claimed in\nSec. 3.4, currently most detectors are anchor-based methods,\nand there are some critical shortcomings which limit the detec-\ntion accuracy. Current anchor priors are mainly manually de-\nsigned which is difﬁcult to match multi-scale objects and the\nmatching strategy based on IoU is also heuristic.\nAlthough\nsome methods have been proposed to transform anchor-based\nmethods into anchor-free methods (e.g. methods based on key-\npoints), there are still some limitations(high computation cost\netc.) with large space to improve. From Figure 2, developing\nanchor-free methods becomes a very hot topic in object detec-\ntion [63, 95, 240, 241, 65], and thus designing an efﬁcient and\neffective proposal generation strategy is potentially a very im-\nportant research direction in the future.\nii) Effective Encoding of Contextual Information. Contexts\ncan contribute or impede visual object detection results, as ob-\njects in the visual world have strong relationships, and contexts\nare critical to better understand the visual worlds. However, lit-\ntle effort has been focused on how to correctly use contextual\ninformation. How to incorporate contexts for object detection\neffectively can be a promising future direction.\niii) Detection based on Auto Machine Learning(AutoML).\nTo design an optimal backbone architecture for a certain task\ncan signiﬁcantly improve the results but also requires huge\nengineering effort.\nThus to learn backbone architecture di-\nrectly on the datasets is a very interesting and important re-\nsearch direction. From Figure 2, inspired by the pioneering\nAutoML work on image classiﬁcation [247, 248], more rele-\nvant work has been proposed to address detection problems via\nAutoML [249, 250], such as learning FPN structure [250] and\nlearning data augmentation policies [251], which show signif-\nicant improvement over the baselines. However, the required\ncomputation resource for AutoML is unaffordable to most re-\nsearchers(more than 100 GPU cards to train a single model).\nThus, developing a low-computation framework shall have a\nlarge impact for object detection. Further, new structure poli-\n33\ncies (such as proposal generation and region encoding) of de-\ntection task can be explored in the future.\niv) Emerging Benchmarks for Object Detection. Currently\nMSCOCO is the most commonly used detection benchmark\ntestbed. However, MSCOCO has only 80 categories, which is\nstill too small to understand more complicated scenes in real\nworld.\nRecently, a new benchmark dataset LVIS [235] has\nbeen proposed in order to collect richer categorical informa-\ntion. LVIS contains 164000 images with 1000+ categories, and\nthere are total of 2.2 million high-quality instance segmenta-\ntion masks. Further, LVIS simulates the real-world low-shot\nscenario where a large number of categories are present but\nper-category data is sometimes scarce. LVIS will open a new\nbenchmark for more challenging detection, segmentation and\nlow-shot learning tasks in near future.\nv) Low-shot Object Detection. Training detectors with lim-\nited labeled data is dubbed as Low-shot detection. Deep learn-\ning based detectors often have huge amount of parameters and\nthus are data-hungry, which require large amount of labeled\ndata to achieve satisfactory performance. However, labeling\nobjects in images with bounding box level annotation is very\ntime-consuming. Low-shot learning has been actively studied\nfor classiﬁcation tasks, but only a few studies are focused on\ndetection tasks. For example, Multi-modal Self-Paced Learn-\ning for Detection (MSPLD) [252] addresses the low-shot de-\ntection problem in a semi-supervised learning setting where a\nlarge-scale unlabeled dataset is available. RepMet [253] adopts\na Deep Metric Learning (DML) structure, which jointly learns\nfeature embedding space and data distribution of training set\ncategories.\nHowever, RepMet was only tested on datasets\nwith similar concepts (animals). Low-Shot Transfer Detector\n(LSTD) [254] addresses low-shot detection based on transfer\nlearning which transfers the knowledge form large annotated\nexternal datasets to the target set by knowledge regularization.\nLSTD still suffers from overﬁtting. There is still a large room\nto improve the low-shot detection tasks.\nvi) Backbone Architecture for Detection Task. It has be-\ncome a paradigm to adopt weights of classiﬁcation models pre-\ntrained on large scale dataset into detection problem. However,\nthere still exist conﬂicts between classiﬁcation and detection\ntasks [78], and thus it’s not an optimal solution to do so. From\nTable 3, most state-of-the-art detection algorithms are based on\nclassiﬁcation backbones, and only a few of them try different\nselections (such as CornerNet based on Hourglass Net). Thus\nhow to develop a detection-aware backbone architecture is also\nan important research direction in the future.\nvii) Other Research Issues. In addition, there are some other\nopen research issues, such as large batch learning [255] and\nincremental learning [256]. Batch size is a key factor in DCNN\ntraining but has not been well studied in detection task. And\nfor incremental learning, detection algorithms still suffer from\ncatastrophic forgetting if adapted to a new task without initial\ntraining data. These open and fundamental research issues also\ndeserve more attention for future work.\nIn this survey, we give a comprehensive survey of recent ad-\nvances in deep learning techniques for object detection tasks.\nThe main contents of this survey are divided into three ma-\njor categories: object detector components, machine learning\nstrategies, real-world applications and benchmark evaluations.\nWe have reviewed a large body of representative articles in re-\ncent literature, and presented the contributions on this important\ntopic in a structured and systematic manner. We hope this sur-\nvey can give readers a comprehensive understanding of object\ndetection with deep learning and potentially spur more research\nwork on object detection techniques and their applications.\nReferences\nReferences\n[1] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recog-\nnition, in: CVPR, 2016.\n[2] R. Girshick, J. Donahue, T. Darrell, J. Malik, Rich feature hierarchies for\naccurate object detection and semantic segmentation, in: CVPR, 2014.\n[3] K. He, G. Gkioxari, P. Doll´ar, R. Girshick, Mask r-cnn, in: ICCV, 2017.\n[4] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, A. L. Yuille, Se-\nmantic image segmentation with deep convolutional nets and fully con-\nnected crfs, in: arXiv preprint arXiv:1412.7062, 2014.\n34\n[5] Y. Sun, D. Liang, X. Wang, X. Tang, Deepid3: Face recognition with\nvery deep neural networks, in: arXiv preprint arXiv:1502.00873, 2015.\n[6] Y. Sun, Y. Chen, X. Wang, X. Tang, Deep learning face representation\nby joint identiﬁcation-veriﬁcation, in: NeurIPS, 2014.\n[7] W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, L. Song, Sphereface: Deep hyper-\nsphere embedding for face recognition, in: CVPR, 2017.\n[8] J. Li, X. Liang, S. Shen, T. Xu, J. Feng, S. Yan, Scale-aware fast r-cnn\nfor pedestrian detection, in: IEEE Transactions on Multimedia, 2018.\n[9] J. Hosang, M. Omran, R. Benenson, B. Schiele, Taking a deeper look at\npedestrians, in: CVPR, 2015.\n[10] A. Angelova, A. Krizhevsky, V. Vanhoucke, A. S. Ogale, D. Ferguson,\nReal-time pedestrian detection with deep network cascades., in: BMVC,\n2015.\n[11] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, L. Fei-Fei,\nLarge-scale video classiﬁcation with convolutional neural networks, in:\nCVPR, 2014.\n[12] H. Mobahi, R. Collobert, J. Weston, Deep learning from temporal coher-\nence in video, in: Annual International Conference on Machine Learn-\ning, 2009.\n[13] S. C. Hoi, X. Wu, H. Liu, Y. Wu, H. Wang, H. Xue, Q. Wu, Logo-net:\nLarge-scale deep logo detection and brand recognition with deep region-\nbased convolutional networks, in: arXiv preprint arXiv:1511.02462,\n2015.\n[14] H. Su, X. Zhu, S. Gong, Deep learning logo detection with data ex-\npansion by synthesising context, in: 2017 IEEE Winter Conference on\nApplications of Computer Vision (WACV), 2017.\n[15] H. Su, S. Gong, X. Zhu, Scalable deep learning logo detection, in: arXiv\npreprint arXiv:1803.11417, 2018.\n[16] A. Vedaldi, V. Gulshan, M. Varma, A. Zisserman, Multiple kernels for\nobject detection, in: ICCV, 2009.\n[17] P. Viola, M. Jones, Rapid object detection using a boosted cascade of\nsimple features, in: CVPR, 2001.\n[18] H. Harzallah, F. Jurie, C. Schmid, Combining efﬁcient object localiza-\ntion and image classiﬁcation, in: ICCV, 2009.\n[19] N. Dalal, B. Triggs, Histograms of oriented gradients for human detec-\ntion, in: CVPR, 2005.\n[20] P. Viola, M. J. Jones, Robust real-time face detection, in: IJCV, 2004.\n[21] D. G. Lowe, Object recognition from local scale-invariant features, in:\nICCV, 1999.\n[22] R. Lienhart, J. Maydt, An extended set of haar-like features for rapid ob-\nject detection, in: International Conference on Image Processing, 2002.\n[23] H. Bay, T. Tuytelaars, L. Van Gool, Surf: Speeded up robust features,\nin: ECCV, 2006.\n[24] M. A. Hearst, S. T. Dumais, E. Osuna, J. Platt, B. Scholkopf, Support\nvector machines, in: IEEE Intelligent Systems and their applications,\n1998.\n[25] D. Opitz, R. Maclin, Popular ensemble methods: An empirical study, in:\nJournal of artiﬁcial intelligence research, 1999.\n[26] Y. Freund, R. E. Schapire, et al., Experiments with a new boosting algo-\nrithm, in: ICML, 1996.\n[27] Y. Yu, J. Zhang, Y. Huang, S. Zheng, W. Ren, C. Wang, K. Huang,\nT. Tan, Object detection by context and boosted hog-lbp, in: PASCAL\nVOC Challenge, 2010.\n[28] P. Felzenszwalb, R. Girshick, D. McAllester, D. Ramanan, Discrimina-\ntively trained mixtures of deformable part models, in: PASCAL VOC\nChallenge, 2008.\n[29] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, A. Zisserman,\nThe pascal visual object classes (voc) challenge, in: IJCV, 2010.\n[30] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, D. Ramanan, Object\ndetection with discriminatively trained part-based models, in: TPAMI,\n2010.\n[31] D. G. Lowe, Distinctive image features from scale-invariant keypoints,\nin: IJCV, 2004.\n[32] T. Ojala, M. Pietikainen, T. Maenpaa, Multiresolution gray-scale and\nrotation invariant texture classiﬁcation with local binary patterns, in:\nTPAMI, 2002.\n[33] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classiﬁcation with\ndeep convolutional neural networks, in: NeurIPS, 2012.\n[34] S. Ren, K. He, R. Girshick, J. Sun, Faster r-cnn: Towards real-time ob-\nject detection with region proposal networks, in: NeurIPS, 2015.\n[35] K. Fukushima, S. Miyake, Neocognitron: A self-organizing neural net-\nwork model for a mechanism of visual pattern recognition, in: Compe-\ntition and cooperation in neural nets, 1982.\n[36] Y. LeCun, L. Bottou, Y. Bengio, P. Haffner, Gradient-based learning\napplied to document recognition, in: Proceedings of the IEEE, 1998.\n[37] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, Imagenet: A\nlarge-scale hierarchical image database, in: CVPR, 2009.\n[38] R. Girshick, Fast r-cnn, in: ICCV, 2015.\n[39] T.-Y. Lin, P. Doll´ar, R. Girshick, K. He, B. Hariharan, S. Belongie, Fea-\nture pyramid networks for object detection, in: CVPR, 2017.\n[40] J. Redmon, S. Divvala, R. Girshick, A. Farhadi, You only look once:\nUniﬁed, real-time object detection, in: CVPR, 2016.\n[41] J. Redmon, A. Farhadi, Yolo9000: better, faster, stronger, in: CVPR,\n2017.\n[42] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, A. C.\nBerg, SSD: Single shot multibox detector, in: ECCV, 2016.\n[43] T.-Y. Lin, P. Goyal, R. Girshick, K. He, P. Doll´ar, Focal loss for dense\nobject detection, in: ICCV, 2017.\n[44] S. Fidler, R. Mottaghi, A. Yuille, R. Urtasun, Bottom-up segmentation\nfor top-down detection, in: CVPR, 2013.\n[45] J. R. Uijlings, K. E. Van De Sande, T. Gevers, A. W. Smeulders, Selec-\ntive search for object recognition, in: IJCV, 2013.\n[46] J. Kleban, X. Xie, W.-Y. Ma, Spatial pyramid mining for logo detection\nin natural scenes, in: Multimedia and Expo, 2008 IEEE International\nConference on, 2008.\n[47] K. He, X. Zhang, S. Ren, J. Sun, Spatial pyramid pooling in deep con-\n35\nvolutional networks for visual recognition, in: ECCV, 2014.\n[48] C. L. Zitnick, P. Doll´ar, Edge boxes: Locating object proposals from\nedges, in: ECCV, 2014.\n[49] Z. Cai, N. Vasconcelos, Cascade r-cnn: Delving into high quality object\ndetection, in: CVPR, 2018.\n[50] T. Kong, A. Yao, Y. Chen, F. Sun, Hypernet: Towards accurate region\nproposal generation and joint object detection, in: CVPR, 2016.\n[51] S. Bell, C. Lawrence Zitnick, K. Bala, R. Girshick, Inside-outside net:\nDetecting objects in context with skip pooling and recurrent neural net-\nworks, in: CVPR, 2016.\n[52] J. Dai, Y. Li, K. He, J. Sun, R-fcn: Object detection via region-based\nfully convolutional networks, in: NeurIPS, 2016.\n[53] K. Kang, W. Ouyang, H. Li, X. Wang, Object detection from video\ntubelets with convolutional neural networks, in: CVPR, 2016.\n[54] W. Han, P. Khorrami, T. L. Paine, P. Ramachandran, M. Babaeizadeh,\nH. Shi, J. Li, S. Yan, T. S. Huang, Seq-nms for video object detection,\nin: arXiv preprint arXiv:1602.08465, 2016.\n[55] M. Rayat Imtiaz Hossain, J. Little, Exploiting temporal information for\n3d human pose estimation, in: ECCV, 2018.\n[56] G. Pavlakos, X. Zhou, K. G. Derpanis, K. Daniilidis, Coarse-to-ﬁne vol-\numetric prediction for single-image 3d human pose, in: CVPR, 2017.\n[57] P. O. Pinheiro, T.-Y. Lin, R. Collobert, P. Doll´ar, Learning to reﬁne ob-\nject segments, in: ECCV, 2016.\n[58] P. O. Pinheiro, R. Collobert, P. Doll´ar, Learning to segment object can-\ndidates, in: NeurIPS, 2015.\n[59] J. Dai, K. He, J. Sun, Instance-aware semantic segmentation via multi-\ntask network cascades, in: CVPR, 2016.\n[60] Z. Huang, L. Huang, Y. Gong, C. Huang, X. Wang, Mask scoring r-cnn,\nin: CVPR, 2019.\n[61] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, Y. LeCun,\nOverfeat: Integrated recognition, localization and detection using con-\nvolutional networks, in: arXiv preprint arXiv:1312.6229, 2013.\n[62] S. Ioffe, C. Szegedy, Batch normalization: Accelerating deep network\ntraining by reducing internal covariate shift, in: ICML, 2015.\n[63] H. Law, J. Deng, Cornernet: Detecting objects as paired keypoints, in:\nECCV, 2018.\n[64] X. Zhou, D. Wang, P. Kr¨ahenb¨uhl, Objects as points, in: arXiv preprint\narXiv:1904.07850, 2019.\n[65] K. Duan, S. Bai, L. Xie, H. Qi, Q. Huang, Q. Tian, Centernet: Keypoint\ntriplets for object detection, in: arXiv preprint arXiv:1904.08189, 2019.\n[66] H. Robbins, S. Monro, A stochastic approximation method, in: The an-\nnals of mathematical statistics, 1951.\n[67] D. P. Kingma, J. Ba, Adam: A method for stochastic optimization, in:\narXiv preprint arXiv:1412.6980, 2014.\n[68] V. Nair, G. E. Hinton, Rectiﬁed linear units improve restricted boltzmann\nmachines, in: ICML, 2010.\n[69] K. Simonyan, A. Zisserman, Very deep convolutional networks for\nlarge-scale image recognition, in:\narXiv preprint arXiv:1409.1556,\n2014.\n[70] K. He, X. Zhang, S. Ren, J. Sun, Identity mappings in deep residual\nnetworks, in: ECCV, Springer, 2016.\n[71] G. Huang, Z. Liu, L. Van Der Maaten, K. Q. Weinberger, Densely con-\nnected convolutional networks., in: CVPR, 2017.\n[72] Y. Chen, J. Li, H. Xiao, X. Jin, S. Yan, J. Feng, Dual path networks, in:\nNeurIPS, 2017, pp. 4467–4475.\n[73] S. Xie, R. Girshick, P. Doll´ar, Z. Tu, K. He, Aggregated residual trans-\nformations for deep neural networks, in: CVPR, 2017.\n[74] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,\nT. Weyand, M. Andreetto, H. Adam, Mobilenets: Efﬁcient convolu-\ntional neural networks for mobile vision applications, in: arXiv preprint\narXiv:1704.04861, 2017.\n[75] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Er-\nhan, V. Vanhoucke, A. Rabinovich, Going deeper with convolutions, in:\nCVPR, 2015.\n[76] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, Z. Wojna, Rethinking the\ninception architecture for computer vision, in: CVPR, 2016.\n[77] C. Szegedy, S. Ioffe, V. Vanhoucke, A. A. Alemi, Inception-v4,\ninception-resnet and the impact of residual connections on learning., in:\nAAAI, 2017.\n[78] Z. Li, C. Peng, G. Yu, X. Zhang, Y. Deng, J. Sun, Detnet: A backbone\nnetwork for object detection, in: ECCV, 2018.\n[79] A. Newell, K. Yang, J. Deng, Stacked hourglass networks for human\npose estimation, in: ECCV, 2016.\n[80] B. Alexe, T. Deselaers, V. Ferrari, Measuring the objectness of image\nwindows, in: TPAMI, 2012.\n[81] E. Rahtu, J. Kannala, M. Blaschko, Learning a category independent\nobject detection cascade, in: ICCV, 2011.\n[82] P. F. Felzenszwalb, D. P. Huttenlocher, Efﬁcient graph-based image seg-\nmentation, in: IJCV, 2004.\n[83] S. Manen, M. Guillaumin, L. Van Gool, Prime object proposals with\nrandomized prim’s algorithm, in: CVPR, 2013.\n[84] J. Carreira, C. Sminchisescu, Cpmc: Automatic object segmentation us-\ning constrained parametric min-cuts, in: TPAMI, 2011.\n[85] I. Endres, D. Hoiem, Category-independent object proposals with di-\nverse ranking, in: TPAMI, 2014.\n[86] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\nP. Doll´ar, C. L. Zitnick, Microsoft coco: Common objects in context,\nin: ECCV, 2014.\n[87] S. Zhang, X. Zhu, Z. Lei, H. Shi, X. Wang, S. Z. Li, S3fd: Single shot\nscale-invariant face detector, in: ICCV, 2017.\n[88] W. Luo, Y. Li, R. Urtasun, R. Zemel, Understanding the effective recep-\ntive ﬁeld in deep convolutional neural networks, in: NeurIPS, 2016.\n[89] C. Zhu, R. Tao, K. Luu, M. Savvides, Seeing small faces from robust\nanchors perspective, in: CVPR, 2018.\n[90] L. J. Z. X. Lele Xie, Yuliang Liu, Derpn: Taking a further step toward\nmore general object detection, in: AAAI, 2019.\n36\n[91] A. Ghodrati, A. Diba, M. Pedersoli, T. Tuytelaars, L. Van Gool, Deep-\nproposal: Hunting objects by cascading deep convolutional layers, in:\nICCV, 2015.\n[92] S. Zhang, L. Wen, X. Bian, Z. Lei, S. Z. Li, Single-shot reﬁnement\nneural network for object detection, in: CVPR, 2018.\n[93] T. Yang, X. Zhang, Z. Li, W. Zhang, J. Sun, Metaanchor: Learning to\ndetect objects with customized anchors, in: NeurIPS, 2018.\n[94] L. Tychsen-Smith, L. Petersson, Denet: Scalable real-time object detec-\ntion with directed sparse sampling, in: ICCV, 2017.\n[95] C. Zhu, Y. He, M. Savvides, Feature selective anchor-free module for\nsingle-shot object detection, in: CVPR, 2019.\n[96] Y. Lu, T. Javidi, S. Lazebnik, Adaptive object detection using adjacency\nand zoom prediction, in: CVPR, 2016.\n[97] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, Y. Wei, Deformable\nconvolutional networks, in: ICCV, 2017.\n[98] B. Singh, L. S. Davis, An analysis of scale invariance in object\ndetection–snip, in: CVPR, 2018.\n[99] P. Hu, D. Ramanan, Finding tiny faces, in: CVPR, 2017.\n[100] F. Yang, W. Choi, Y. Lin, Exploit all the layers: Fast and accurate cnn ob-\nject detector with scale dependent pooling and cascaded rejection clas-\nsiﬁers, in: CVPR, 2016.\n[101] Y. Liu, H. Li, J. Yan, F. Wei, X. Wang, X. Tang, Recurrent scale approx-\nimation for object detection in cnn, in: ICCV, 2017.\n[102] A. Shrivastava, R. Sukthankar, J. Malik, A. Gupta, Beyond skip con-\nnections: Top-down modulation for object detection, in: arXiv preprint\narXiv:1612.06851, 2016.\n[103] H. Wang, Q. Wang, M. Gao, P. Li, W. Zuo, Multi-scale location-aware\nkernel representation for object detection, in: CVPR, 2018.\n[104] K.-H. Kim, S. Hong, B. Roh, Y. Cheon, M. Park, Pvanet: deep but\nlightweight neural networks for real-time object detection, in: arXiv\npreprint arXiv:1608.08021, 2016.\n[105] O. Ronneberger, P. Fischer, T. Brox, U-net: Convolutional networks for\nbiomedical image segmentation, in: International Conference on Medi-\ncal image computing and computer-assisted intervention, 2015.\n[106] Z. Cai, Q. Fan, R. S. Feris, N. Vasconcelos, A uniﬁed multi-scale deep\nconvolutional neural network for fast object detection, in: ECCV, 2016.\n[107] Z. Shen, Z. Liu, J. Li, Y.-G. Jiang, Y. Chen, X. Xue, Dsod: Learning\ndeeply supervised object detectors from scratch, in: ICCV, 2017.\n[108] S. Liu, D. Huang, Y. Wang, Receptive ﬁeld block net for accurate and\nfast object detection, in: ECCV, 2018.\n[109] J. Ren, X. Chen, J. Liu, W. Sun, J. Pang, Q. Yan, Y.-W. Tai, L. Xu,\nAccurate single stage detector using recurrent rolling convolution, in:\nCVPR, 2017.\n[110] J. Jeong, H. Park, N. Kwak, Enhancement of ssd by concatenating fea-\nture maps for object detection, in: arXiv preprint arXiv:1705.09587,\n2017.\n[111] P. Zhou, B. Ni, C. Geng, J. Hu, Y. Xu, Scale-transferrable object detec-\ntion, in: CVPR, 2018.\n[112] C.-Y. Fu, W. Liu, A. Ranga, A. Tyagi, A. C. Berg, Dssd: Deconvolu-\ntional single shot detector, in: arXiv preprint arXiv:1701.06659, 2017.\n[113] S. Woo, S. Hwang, I. S. Kweon, Stairnet: Top-down semantic aggrega-\ntion for accurate one shot detection, in: 2018 IEEE Winter Conference\non Applications of Computer Vision (WACV), 2018.\n[114] H. Li, Y. Liu, W. Ouyang, X. Wang, Zoom out-and-in network\nwith recursive training for object proposal,\nin:\narXiv preprint\narXiv:1702.05711, 2017.\n[115] T. Kong, F. Sun, W. Huang, H. Liu, Deep feature pyramid reconﬁgura-\ntion for object detection, in: ECCV, 2018.\n[116] Q. Zhao, T. Sheng, Y. Wang, Z. Tang, Y. Chen, L. Cai, H. Ling, M2det:\nA single-shot object detector based on multi-level feature pyramid net-\nwork, in: AAAI, 2019.\n[117] Z. Li, F. Zhou, Fssd: Feature fusion single shot multibox detector, in:\narXiv preprint arXiv:1712.00960, 2017.\n[118] K. Lee, J. Choi, J. Jeong, N. Kwak, Residual features and uni-\nﬁed prediction network for single stage detection, in: arXiv preprint\narXiv:1707.05031, 2017.\n[119] L. Cui, Mdssd: Multi-scale deconvolutional single shot detector for\nsmall objects, in: arXiv preprint arXiv:1805.07009, 2018.\n[120] T. Kong, F. Sun, A. Yao, H. Liu, M. Lu, Y. Chen, Ron: Reverse con-\nnection with objectness prior networks for object detection, in: CVPR,\n2017.\n[121] B. Lim, S. Son, H. Kim, S. Nah, K. Mu Lee, Enhanced deep residual\nnetworks for single image super-resolution, in: CVPR workshops, 2017.\n[122] W. Shi, J. Caballero, F. Husz´ar, J. Totz, A. P. Aitken, R. Bishop,\nD. Rueckert, Z. Wang, Real-time single image and video super-\nresolution using an efﬁcient sub-pixel convolutional neural network, in:\nCVPR, 2016.\n[123] B. Jiang, R. Luo, J. Mao, T. Xiao, Y. Jiang, Acquisition of localization\nconﬁdence for accurate object detection, in: ECCV, 2018.\n[124] Y. Zhai, J. Fu, Y. Lu, H. Li, Feature selective networks for object detec-\ntion, in: CVPR, 2018.\n[125] Y. Zhu, C. Zhao, J. Wang, X. Zhao, Y. Wu, H. Lu, Couplenet: Coupling\nglobal structure with local parts for object detection, in: ICCV, 2017.\n[126] C. Galleguillos, S. Belongie, Context based object categorization: A\ncritical survey, in: Computer vision and image understanding, 2010.\n[127] W. Ouyang, X. Wang, X. Zeng, S. Qiu, P. Luo, Y. Tian, H. Li, S. Yang,\nZ. Wang, C.-C. Loy, et al., Deepid-net: Deformable deep convolutional\nneural networks for object detection, in: CVPR, 2015.\n[128] W. Chu, D. Cai, Deep feature based contextual model for object detec-\ntion, in: Neurocomputing, 2018.\n[129] Y. Zhu, R. Urtasun, R. Salakhutdinov, S. Fidler, segdeepm: Exploiting\nsegmentation and context in deep neural networks for object detection,\nin: CVPR, 2015.\n[130] X. Chen, A. Gupta, Spatial memory for context reasoning in object de-\ntection, in: ICCV, 2017.\n[131] S. Gidaris, N. Komodakis, Object detection via a multi-region and se-\n37\nmantic segmentation-aware cnn model, in: ICCV, 2015.\n[132] B. Cheng, Y. Wei, H. Shi, R. Feris, J. Xiong, T. Huang, Revisiting rcnn:\nOn awakening the classiﬁcation power of faster rcnn, in: ECCV, 2018.\n[133] X. Zhao, S. Liang, Y. Wei, Pseudo mask augmented object detection, in:\nCVPR, 2018.\n[134] Z. Zhang, S. Qiao, C. Xie, W. Shen, B. Wang, A. L. Yuille, Single-shot\nobject detection with enriched semantics, Tech. rep. (2018).\n[135] A. Shrivastava, A. Gupta, Contextual priming and feedback for faster\nr-cnn, in: ECCV, 2016.\n[136] B. Li, T. Wu, L. Zhang, R. Chu, Auto-context r-cnn, in: arXiv preprint\narXiv:1807.02842, 2018.\n[137] Y. Liu, R. Wang, S. Shan, X. Chen, Structure inference net: Object\ndetection using scene-level context and instance-level relationships, in:\nCVPR, 2018.\n[138] H. Hu, J. Gu, Z. Zhang, J. Dai, Y. Wei, Relation networks for object\ndetection, in: CVPR, 2018.\n[139] J. Gu, H. Hu, L. Wang, Y. Wei, J. Dai, Learning region features for object\ndetection, in: ECCV, 2018.\n[140] H. Xu, X. Lv, X. Wang, Z. Ren, R. Chellappa, Deep regionlets for object\ndetection, in: ECCV, 2018.\n[141] Z. Chen, S. Huang, D. Tao, Context reﬁnement for object detection, in:\nECCV, 2018.\n[142] X. Zeng, W. Ouyang, B. Yang, J. Yan, X. Wang, Gated bi-directional\ncnn for object detection, in: ECCV, 2016.\n[143] J. Li, Y. Wei, X. Liang, J. Dong, T. Xu, J. Feng, S. Yan, Attentive con-\ntexts for object detection, in: IEEE Transactions on Multimedia, 2017.\n[144] S. L. Xizhou Zhu, Han Hu, J. Dai, Deformable convnets v2: More de-\nformable, better results, in: CVPR, 2019.\n[145] R. Girshick, F. Iandola, T. Darrell, J. Malik, Deformable part models are\nconvolutional neural networks, in: CVPR, 2015.\n[146] B. Singh, M. Najibi, L. S. Davis, Sniper: Efﬁcient multi-scale training,\nin: NeurIPS, 2018.\n[147] Y. L. Buyu Li, X. Wang, Gradient harmonized single-stage detector, in:\nAAAI, 2019.\n[148] A. Shrivastava, A. Gupta, R. Girshick, Training region-based object de-\ntectors with online hard example mining, in: CVPR, 2016.\n[149] S. Gidaris, N. Komodakis, Locnet: Improving localization accuracy for\nobject detection, in: CVPR, 2016.\n[150] S. Zagoruyko, A. Lerer, T.-Y. Lin, P. O. Pinheiro, S. Gross, S. Chintala,\nP. Doll´ar, A multipath network for object detection, in: BMVC, 2016.\n[151] X. Lu, B. Li, Y. Yue, Q. Li, J. Yan, Grid r-cnn, in: CVPR, 2019.\n[152] L. Tychsen-Smith, L. Petersson, Improving object localization with ﬁt-\nness nms and bounded iou loss, in: arXiv preprint arXiv:1711.00164,\n2017.\n[153] B. Yang, J. Yan, Z. Lei, S. Z. Li, Craft objects from images, in: CVPR,\n2016.\n[154] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, Y. Bengio, Generative adversarial nets, in:\nNeurIPS, 2014.\n[155] J.-Y. Zhu, T. Park, P. Isola, A. A. Efros, Unpaired image-to-image trans-\nlation using cycle-consistent adversarial networkss, in: ICCV, 2017.\n[156] A. Radford, L. Metz, S. Chintala, Unsupervised representation learn-\ning with deep convolutional generative adversarial networks, in: arXiv\npreprint arXiv:1511.06434, 2015.\n[157] A. Brock, J. Donahue, K. Simonyan, Large scale gan training for high\nﬁdelity natural image synthesis, in: arXiv preprint arXiv:1809.11096,\n2018.\n[158] J. Li, X. Liang, Y. Wei, T. Xu, J. Feng, S. Yan, Perceptual generative\nadversarial networks for small object detection, in: CVPR, 2017.\n[159] X. Wang, A. Shrivastava, A. Gupta, A-fast-rcnn: Hard positive genera-\ntion via adversary for object detection, in: CVPR, 2017.\n[160] R. G. Kaiming He, P. Dollro, Rethinking imagenet pre-training, in:\narXiv preprint arXiv:1811.08883, 2018.\n[161] R. Zhu, S. Zhang, X. Wang, L. Wen, H. Shi, L. Bo, T. Mei, Scratchdet:\nExploring to train single-shot object detectors from scratch, in: CVPR,\n2019.\n[162] Z. Shen, H. Shi, R. Feris, L. Cao, S. Yan, D. Liu, X. Wang, X. Xue,\nT. S. Huang, Learning object detectors from scratch with gated recurrent\nfeature pyramids, in: arXiv preprint arXiv:1712.00886, 2017.\n[163] G. Hinton, O. Vinyals, J. Dean, Distilling the knowledge in a neural\nnetwork, in: arXiv preprint arXiv:1503.02531, 2015.\n[164] Q. Li, S. Jin, J. Yan, Mimicking very efﬁcient network for object detec-\ntion, in: CVPR, 2017.\n[165] N. Bodla, B. Singh, R. Chellappa, L. S. Davis, Soft-nms – improving\nobject detection with one line of code, in: ICCV, 2017.\n[166] J. Hosang, R. Benenson, B. Schiele, Learning non-maximum suppres-\nsion, in: CVPR, 2017.\n[167] J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi, I. Fischer,\nZ. Wojna, Y. Song, S. Guadarrama, et al., Speed/accuracy trade-offs for\nmodern convolutional object detectors, in: CVPR, 2017.\n[168] Z. Li, C. Peng, G. Yu, X. Zhang, Y. Deng, J. Sun, Light-head r-\ncnn:\nIn defense of two-stage object detector, in:\narXiv preprint\narXiv:1711.07264, 2017.\n[169] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, L.-C. Chen, Inverted\nresiduals and linear bottlenecks: Mobile networks for classiﬁcation, de-\ntection and segmentation, in: arXiv preprint arXiv:1801.04381, 2018.\n[170] A. Wong, M. J. Shaﬁee, F. Li, B. Chwyl, Tiny ssd: A tiny single-shot\ndetection deep convolutional neural network for real-time embedded ob-\nject detection, in: arXiv preprint arXiv:1802.06488, 2018.\n[171] Y. Li, J. Li, W. Lin, J. Li, Tiny-dsod: Lightweight object detection for\nresource-restricted usages, in: arXiv preprint arXiv:1807.11013, 2018.\n[172] K. S. D. A. Shang, Wenling, H. Lee., Understanding and improving\nconvolutional neural networks via concatenated rectiﬁed linear units, in:\nICML, 2016.\n[173] Y. D. Kim, E. Park, S. Yoo, T. Choi, L. Yang, D. Shin, Compression\nof deep convolutional neural networks for fast and low power mobile\n38\napplications, in: Computer Science, 2015.\n[174] Y. He, X. Zhang, J. Sun, Channel pruning for accelerating very deep\nneural networks, in: ICCV, 2017.\n[175] Y. Gong, L. Liu, M. Yang, L. Bourdev, Compressing deep convolutional\nnetworks using vector quantization, in: Computer Science, 2014.\n[176] Y. Lin, S. Han, H. Mao, Y. Wang, W. J. Dally, Deep gradient compres-\nsion: Reducing the communication bandwidth for distributed training,\nin: arXiv preprint arXiv:1712.01887, 2017.\n[177] J. Wu, L. Cong, Y. Wang, Q. Hu, J. Cheng, Quantized convolutional\nneural networks for mobile devices, in: CVPR, 2016.\n[178] S. Han, H. Mao, W. J. Dally, Deep compression: Compressing deep\nneural networks with pruning, trained quantization and huffman coding,\nin: Fiber, 2015.\n[179] S. Han, J. Pool, J. Tran, W. Dally, Learning both weights and connec-\ntions for efﬁcient neural network, in: NeurIPS, 2015.\n[180] E. Osuna, R. Freund, F. Girosit, Training support vector machines: an\napplication to face detection, in: CVPR, 1997.\n[181] M. R¨atsch, S. Romdhani, T. Vetter, Efﬁcient face detection by a cas-\ncaded support vector machine using haar-like features, in: Joint Pattern\nRecognition Symposium, 2004.\n[182] S. Romdhani, P. Torr, B. Scholkopf, A. Blake, Computationally efﬁcient\nface detection, in: ICCV, 2001.\n[183] X. Sun, P. Wu, S. C. Hoi, Face detection using deep learning: An im-\nproved faster rcnn approach, in: Neurocomputing, 2018.\n[184] Y. Liu, M. D. Levine, Multi-path region-based convolutional neural net-\nwork for accurate detection of unconstrained” hard faces”, in: Computer\nand Robot Vision (CRV), 2017 14th Conference on, 2017.\n[185] X. Tang, D. K. Du, Z. He, J. Liu, Pyramidbox: A context-assisted single\nshot face detector, in: ECCV, 2018.\n[186] C. Chi, S. Zhang, J. Xing, Z. Lei, S. Z. Li, X. Zou, Selective reﬁne-\nment network for high performance face detection, in: arXiv preprint\narXiv:1809.02693, 2018.\n[187] J. Li, Y. Wang, C. Wang, Y. Tai, J. Qian, J. Yang, C. Wang, J. Li,\nF. Huang, Dsfd: Dual shot face detector, in: CVPR, 2019.\n[188] J. Zhang, X. Wu, J. Zhu, S. C. Hoi, Feature agglomeration networks for\nsingle stage face detection, in: arXiv preprint arXiv:1712.00721, 2017.\n[189] M. Najibi, P. Samangouei, R. Chellappa, L. Davis, Ssh: Single stage\nheadless face detector, in: ICCV, 2017.\n[190] Z. Hao, Y. Liu, H. Qin, J. Yan, X. Li, X. Hu, Scale-aware face detection,\nin: CVPR, 2017.\n[191] H. Wang, Z. Li, X. Ji, Y. Wang, Face r-cnn, in:\narXiv preprint\narXiv:1706.01061, 2017.\n[192] K. Zhang, Z. Zhang, Z. Li, Y. Qiao, Jjoint face detection and alignment\nusing multi-task cascaded convolutional networks, in: IEEE Signal Pro-\ncessing Letters, 2016.\n[193] P. Samangouei, M. Najibi, L. Davis, R. Chellappa, Face-magnet:\nMagnifying feature maps to detect small faces, in:\narXiv preprint\narXiv:1803.05258, 2018.\n[194] C. Zhang, X. Xu, D. Tu, Face detection using improved faster rcnn, in:\narXiv preprint arXiv:1802.02142, 2018.\n[195] C. Zhu, Y. Zheng, K. Luu, M. Savvides, Cms-rcnn: Contextual multi-\nscale region-based cnn for unconstrained face detection, in: Deep Learn-\ning for Biometrics, 2017.\n[196] B. Yu, D. Tao, Anchor cascade for efﬁcient face detection, in: arXiv\npreprint arXiv:1805.03363, 2018.\n[197] K. Zhang, Z. Zhang, H. Wang, Z. Li, Y. Qiao, W. Liu, Detecting faces\nusing inside cascaded contextual cnn, in: ICCV, 2017.\n[198] Y. Wang, X. Ji, Z. Zhou, H. Wang, Z. Li, Detecting faces us-\ning region-based fully convolutional networks, in:\narXiv preprint\narXiv:1709.05256, 2017.\n[199] V. Jain, E. Learned-Miller, Fddb: A benchmark for face detection in un-\nconstrained settings, Tech. Rep. UM-CS-2010-009, University of Mas-\nsachusetts, Amherst (2010).\n[200] S. Yang, P. Luo, C.-C. Loy, X. Tang, Wwider face: A face detection\nbenchmark, in: CVPR, 2016.\n[201] J. Han, W. Nam, P. Dollar, Local decorrelation for improved detection,\nin: NeurIPS, 2014.\n[202] P. Doll´ar, Z. Tu, P. Perona, S. Belongie, Integral channel features, in:\nBMVC, 2009.\n[203] P. Doll´ar, R. Appel, S. Belongie, P. Perona, Fast feature pyramids for\nobject detection, in: TPAMI, 2014.\n[204] C. P. Papageorgiou, M. Oren, T. Poggio, A general framework for object\ndetection, in: ICCV, 1998.\n[205] G. Brazil, X. Yin, X. Liu, Illuminating pedestrians via simultaneous de-\ntection & segmentation, in: arXiv preprint arXiv:1706.08564, 2017.\n[206] X. Du, M. El-Khamy, J. Lee, L. Davis, Fused dnn: A deep neural net-\nwork fusion approach to fast and robust pedestrian detection, in: IEEE\nWinter Conference on Applications of Computer Vision (WACV), 2017.\n[207] S. Wang, J. Cheng, H. Liu, M. Tang, Pcn: Part and context information\nfor pedestrian detection with cnns, in: arXiv preprint arXiv:1804.04483,\n2018.\n[208] D. Xu, W. Ouyang, E. Ricci, X. Wang, N. Sebe, Learning cross-modal\ndeep representations for robust pedestrian detection, in: CVPR, 2017.\n[209] R. Benenson, M. Omran, J. Hosang, B. Schiele, Ten years of pedestrian\ndetection, what have we learned?, in: ECCV, 2014.\n[210] Z. Cai, M. Saberian, N. Vasconcelos, Learning complexity-aware cas-\ncades for deep pedestrian detection, in: ICCV, 2015.\n[211] P. Sermanet, K. Kavukcuoglu, S. Chintala, Y. LeCun, Pedestrian detec-\ntion with unsupervised multi-stage feature learning, in: CVPR, 2013.\n[212] L. Zhang, L. Lin, X. Liang, K. He, Is faster r-cnn doing well for pedes-\ntrian detection?, in: ECCV, 2016.\n[213] X. Wang, T. Xiao, Y. Jiang, S. Shao, J. Sun, C. Shen, Repulsion loss:\nDetecting pedestrians in a crowd, in: CVPR, 2018.\n[214] S. Zhang, L. Wen, X. Bian, Z. Lei, S. Z. Li, Occlusion-aware r-cnn:\nDetecting pedestrians in a crowd, in: ECCV, 2018.\n[215] J. Mao, T. Xiao, Y. Jiang, Z. Cao, What can help pedestrian detection?,\n39\nin: CVPR, 2017.\n[216] Y. Tian, P. Luo, X. Wang, X. Tang, Deep learning strong parts for pedes-\ntrian detection, in: CVPR, 2015.\n[217] W. Ouyang, X. Wang, Joint deep learning for pedestrian detection, in:\nICCV, 2013.\n[218] M. Mathias, R. Benenson, R. Timofte, L. Van Gool, Handling occlusions\nwith franken-classiﬁers, in: ICCV, 2013.\n[219] W. Ouyang, X. Zeng, X. Wang, Modeling mutual visibility relationship\nin pedestrian detection, in: CVPR, 2013.\n[220] G. Duan, H. Ai, S. Lao, A structural ﬁlter approach to human detection,\nin: ECCV, 2010.\n[221] M. Enzweiler, A. Eigenstetter, B. Schiele, D. M. Gavrila, Multi-cue\npedestrian classiﬁcation with partial occlusion handling, in: CVPR,\n2010.\n[222] C. Zhou, J. Yuan, Bi-box regression for pedestrian detection and occlu-\nsion estimation, in: ECCV, 2018.\n[223] W. Ouyang, X. Wang, A discriminative deep model for pedestrian de-\ntection with occlusion handling, in: CVPR, 2012.\n[224] S. Tang, M. Andriluka, B. Schiele, Detection and tracking of occluded\npeople, in: IJCV, 2014.\n[225] W. Ouyang, X. Wang, Single-pedestrian detection aided by multi-\npedestrian detection, in: CVPR, 2013.\n[226] V. D. Shet, J. Neumann, V. Ramesh, L. S. Davis, Bilattice-based logical\nreasoning for human detection, in: CVPR, 2007.\n[227] Y. Zhou, L. Liu, L. Shao, M. Mellor, Dave: A uniﬁed framework for fast\nvehicle detection and annotation, in: ECCV, 2016.\n[228] T. Gebru, J. Krause, Y. Wang, D. Chen, J. Deng, L. Fei-Fei, Fine-grained\ncar detection for visual census estimation, in: AAAI, 2017.\n[229] S. Majid Azimi, Shufﬂedet: Real-time vehicle detection network in on-\nboard embedded uav imagery, in: ECCV, 2018.\n[230] Z. Zhu, D. Liang, S. Zhang, X. Huang, B. Li, S. Hu, Trafﬁc-sign detec-\ntion and classiﬁcation in the wild, in: CVPR, 2016.\n[231] A. Pon, O. Adrienko, A. Harakeh, S. L. Waslander, A hierarchical deep\narchitecture and mini-batch selection method for joint trafﬁc sign and\nlight detection, in: Conference on Computer and Robot Vision (CRV),\n2018.\n[232] W. Ke, J. Chen, J. Jiao, G. Zhao, Q. Ye, Srn: side-output residual net-\nwork for object symmetry detection in the wild, in: CVPR, 2017.\n[233] W. Shen, K. Zhao, Y. Jiang, Y. Wang, Z. Zhang, X. Bai, Object skeleton\nextraction in natural images by fusing scale-associated deep side out-\nputs, in: CVPR, 2016.\n[234] A. Kuznetsova, H. Rom, N. Alldrin, J. Uijlings, I. Krasin, J. Pont-Tuset,\nS. Kamali, S. Popov, M. Malloci, T. Duerig, et al., The open images\ndataset v4: Uniﬁed image classiﬁcation, object detection, and visual re-\nlationship detection at scale, in: arXiv preprint arXiv:1811.00982, 2018.\n[235] A. Gupta, P. Dollar, R. Girshick, Lvis: A dataset for large vocabulary\ninstance segmentation, in: CVPR, 2019.\n[236] N. Bodla, B. Singh, R. Chellappa, L. S. Davis, Soft-nms–improving ob-\nject detection with one line of code, in: ICCV, 2017.\n[237] Y. Wu, K. He, Group normalization, in: ECCV, 2018.\n[238] S. Liu, L. Qi, H. Qin, J. Shi, J. Jia, Path aggregation network for instance\nsegmentation, in: CVPR, 2018.\n[239] Y. Li, Y. Chen, N. Wang, Z. Zhang, Scale-aware trident networks for\nobject detection, in: arXiv preprint arXiv:1901.01892, 2019.\n[240] X. Zhou, J. Zhuo, P. Krahenbuhl, Bottom-up object detection by group-\ning extreme and center points, in: CVPR, 2019.\n[241] Z. Tian, C. Shen, H. Chen, T. He, Fcos: Fully convolutional one-stage\nobject detection, in: arXiv preprint arXiv:1904.01355, 2019.\n[242] S. Zhang, R. Benenson, B. Schiele, Citypersons: A diverse dataset for\npedestrian detection, in: CVPR, 2017.\n[243] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benen-\nson, U. Franke, S. Roth, B. Schiele, The cityscapes dataset for semantic\nurban scene understanding, in: CVPR, 2016.\n[244] P. Dollar, C. Wojek, B. Schiele, P. Perona, Pedestrian detection: An\nevaluation of the state of the art, in: TPAMI, 2012.\n[245] A. Ess, B. Leibe, L. Van Gool, Depth and appearance for mobile scene\nanalysis, in: ICCV, 2007.\n[246] A. Geiger, P. Lenz, C. Stiller, R. Urtasun, Vision meets robotics: The\nkitti dataset, 2013.\n[247] B. Zoph, V. Vasudevan, J. Shlens, Q. V. Le, Learning transferable archi-\ntectures for scalable image recognition, in: CVPR, 2018.\n[248] M. Tan, Q. V. Le, Efﬁcientnet: Rethinking model scaling for convolu-\ntional neural networks, in: arXiv preprint arXiv:1905.11946, 2019.\n[249] Y. Chen, T. Yang, X. Zhang, G. Meng, C. Pan, J. Sun, Detnas:\nNeural architecture search on object detection, in:\narXiv preprint\narXiv:1903.10979, 2019.\n[250] G. Ghiasi, T.-Y. Lin, Q. V. Le, Nas-fpn: Learning scalable feature pyra-\nmid architecture for object detection, in: CVPR, 2019.\n[251] B. Zoph, E. D. Cubuk, G. Ghiasi, T.-Y. Lin, J. Shlens, Q. V. Le, Learn-\ning data augmentation strategies for object detection, in: arXiv preprint\narXiv:1906.11172, 2019.\n[252] X. Dong, L. Zheng, F. Ma, Y. Yang, D. Meng, Few-example object de-\ntection with model communication, in: TPAMI, 2018.\n[253] E. Schwartz, L. Karlinsky, J. Shtok, S. Harary, M. Marder, S. Pankanti,\nR. Feris, A. Kumar, R. Giries, A. M. Bronstein, Repmet: Representative-\nbased metric learning for classiﬁcation and one-shot object detection, in:\nCVPR, 2019.\n[254] H. Chen, Y. Wang, G. Wang, Y. Qiao, Lstd: A low-shot transfer detector\nfor object detection, in: AAAI, 2018.\n[255] C. Peng, T. Xiao, Z. Li, Y. Jiang, X. Zhang, K. Jia, G. Yu, J. Sun, Megdet:\nA large mini-batch object detector, in: CVPR, 2018.\n[256] K. Shmelkov, C. Schmid, K. Alahari, Incremental learning of object de-\ntectors without catastrophic forgetting, in: ICCV, 2017.\n40\n",
  "categories": [
    "cs.CV",
    "cs.LG",
    "cs.MM"
  ],
  "published": "2019-08-10",
  "updated": "2019-08-10"
}