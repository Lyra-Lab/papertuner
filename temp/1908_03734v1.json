{
  "id": "http://arxiv.org/abs/1908.03734v1",
  "title": "Unsupervised Stemming based Language Model for Telugu Broadcast News Transcription",
  "authors": [
    "Mythili Sharan Pala",
    "Parayitam Laxminarayana",
    "A. V. Ramana"
  ],
  "abstract": "In Indian Languages , native speakers are able to understand new words formed\nby either combining or modifying root words with tense and / or gender. Due to\ndata insufficiency, Automatic Speech Recognition system (ASR) may not\naccommodate all the words in the language model irrespective of the size of the\ntext corpus. It also becomes computationally challenging if the volume of the\ndata increases exponentially due to morphological changes to the root word. In\nthis paper a new unsupervised method is proposed for a Indian language: Telugu,\nbased on the unsupervised method for Hindi, to generate the Out of Vocabulary\n(OOV) words in the language model. By using techniques like smoothing and\ninterpolation of pre-processed data with supervised and unsupervised stemming,\ndifferent issues in language model for Indian language: Telugu has been\naddressed. We observe that the smoothing techniques Witten-Bell and Kneser-Ney\nperform well when compared to other techniques on pre-processed data from\nsupervised learning. The ASRs accuracy is improved by 0.76% and 0.94% with\nsupervised and unsupervised stemming respectively.",
  "text": "1 \n \nUnsupervised Stemming based Language Model for Telugu \nBroadcast News Transcription \nAbstract :- In Indian Languages , native speakers are able to understand new words formed by either \ncombining or modifying root words with tense, number and / or gender. Due to data insufficiency, \nAutomatic Speech Recognition system (ASR) may not accommodate all the words in the language \nmodel irrespective of the size of the text corpus. It also becomes computationally challenging if the \nvolume of the data increases exponentially due to morphological changes to the root word.  \nIn this paper a new unsupervised method is proposed for one Indian language, Telugu, based on \nthe unsupervised method for Hindi, in order to generate the Out of Vocabulary (OOV) words in the \nlanguage model. By using techniques like smoothing and interpolation of pre-processed data with \nsupervised and unsupervised stemming, different issues in language model for Telugu has been \naddressed.  We observe that the smoothing techniques Witten-Bell and Kneser-Ney perform well \nwhen compared to other techniques on pre-processed data from supervised learning. The ASRs \naccuracy is improved by 0.76% and 0.94% with supervised and unsupervised stemming respectively.      \nI. \nINTRODUCTION  \nTelugu is one of four modern literary languages belonging to the Dravidian family. It is also one of \nthe six classical languages of India. With native speakers of 81 million, it stands as fourth most widely \nspoken language in the sub-continent. A comprehensive ASR for Telugu language has not been made \navailable due to lack of standard publicly accessible annotated speech corpus[1].  \nASR leverages acoustic model, language model and lexicons for recognition accuracy. Language \nmodel gives the distribution of probabilities on sequence of words, calculated using the available \ntraining text corpus. Test speech may contain few new words that may not have been acquainted in \nthe training corpus. These new words cannot be recognized by the decoder. New words appearing in \ntest speech are called as Out of Vocabulary (OOV). Complexity of OOV is more in Indian Languages \nthan European Languages due to hybridization of multiple words transforming to one complex word \nin verbal communication.  \nOOV is a common phenomenon in the Large Vocabulary Continuous Speech Recognition (LVCSR) \nsystems.  The methods used in addressing the OOV for English are not suitable for orthographies of \nIndian languages. Pertinent techniques are required to solve the problem of OOVs for Indian \nLanguages [2]. Since most Indian languages have agglutinative morphologies and are associated with \nphonemic orthography (akshara based orthographies) most words are inflected forms of their root \ncounterparts. Recognition accuracy of ASR may be improvised with generation of OOVs for LM, using \ninflections or morphological modifications of existing root words. If newly generated words are \nPala  Mythilisharan,   Parayitam  Laxminarayana  and  Appala Venkataramana                     \nOsmania University  \nHyderabad  - India \n2 \n \nincluded in the training text corpus to obtain Language Models, size of the vocabulary increases \nconsiderably. Therefore, it is proposed to include the root word and its prefixes or suffixes, as words \nin the training text corpus.  \nSame language models may be used for decoding speech into text. Wherein, the recognized text \nwhich appears with root word and prefix or suffix, is combined to form a meaningful new word. The \nperformance of ASR is evaluated against supervised and unsupervised methods to generate new \nwords with inflections or morphological modifications as reported in the literature[3]. \nThe other issue to be addressed in the language model is regarding unseen sequence of words in \nthe test speech. Language model is used in the ASR decoder, to constrain the search in various ways \nby giving favourable bias to the sequence of more probable words. It is a common phenomena to \nhave unseen sequence of words while performing recognition. The performance of the recognizer \nwill be degraded if unseen sequence of words appears in a test speech. In the LVCSR scenario, as per \nZipf’s laws [4]– irrespective of the size of text corpus used for training, it is not feasible to have all \npossible sequences of words. To reduce the effect of unseen sequence of words, various smoothing \nand discounting techniques can be employed for obtaining the language models. Well established \ntechniques like Good-Turing, linear discounting, absolute discounting, Witten-Bell, and Kneser-Ney \nalong with its modified version play a significant role in reducing the effect of unseen sequence of \nwords. However, these methods are verified only for English and other European languages[5].  \nIn this work, we build and evaluate existing LMs for Telugu language using supervised and \nunsupervised stemming. The Second section deals with building of annotated speech corpus, \ndevelopment of acoustic and language models for Telugu ASR. Third section deals with the analysis \nof performance of ASR accuracy with different smoothing and discounting techniques. Fourth \nsection explores the details about supervised and unsupervised stemming techniques used for pre-\nprocessing the text corpus for creating language models and test their performance. \nII. \n   BUILDING ANNOTATED SPEECH, TEXT CORPUS AND ACOUSTIC AND LANGUAGE MODELS FOR ASR USED IN \nTELUGU BROADCAST NEWS \nTo evaluate the performance of a LVCSR system, large amount of annotated speech database under \ndifferent environmental conditions and large text corpus are required for acoustic modelling and \nLanguage Model. \nA. \nSpeech Corpus \nAs the application is subjected for transcribing Telugu broadcast news, sixty five hours of video \ndata broadcasted by the Telugu TV channels is sourced from the YouTube system[[6]. Then the audio \nand video data is separated using open source tools [7]. The data consists of speech by news \nreaders, anchors, expert analysts invited for discussion for news analysis, live news reports and \n3 \n \nreporter’s interaction with political leaders, public etc. The speech data also consists of public \naddresses and snippets of discussions in legislative assembly, reported in the news bulletin. The \nspeech data explained above comprises of voices of 298 female and 327 male speakers. This speech \ndata after discarding the non-speech sounds from the audio stream is transcribed using the \ntransliteration tool BARAHA [8]. The speech segments with cross talk in the assembly or TV \ndiscussions or in the reporting news from public places is also excluded from the training data. \nLexicon is a representation of each unique word in the text corpus by a sequence of its \nrepresentative phonemes.  In this experiment, unique words are generated from the transcribed \ntext of speech data and text corpora that are collected for language modelling. Phonemes are \nacoustic units that are to be modelled. So, it is ensured that the training data covers all the \nphonemes for adequate number of times by choosing special speech files covering less frequently \nappearing phonemes. 60 phonemes are considered. They are listed in TABLE 1. The Indian Language \nSpeech Label (ILSL12) set [9] with 47 items for Telugu is used for the development of Telugu \nphoneme set and lexicon. In addition to ILSL12 set, thirteen geminates or double consonants in \nTelugu script are added as additional phonemes, as they do not follow the same pattern in \npronunciation[10][11].  \nTABLE 1TABLE OF TELUGU PHONEMES \n \nRoman \nrepresentation \nTelugu \nfont \nRoman \nrepresentation \nTelugu \nfont \nRoman \nrepresentation \nTelugu \nfont \na \nఅ \nK \nక \nm \nమ \naa \nఆ \nkh \nఖ \ny \nయ \ni \nఇ \ng \nగ \nr \nర \nii \nఈ \ngh \nఘ \nl \nల \nu \nఉ \nc \nచ \nlx \nళ \nuu \nఊ \nch \nఛ \nw \nవ \nrq \nఋ \nj \nజ \nsh \nశ \nrrq \nౠ \njh \nఝ \nsx \nష \ne \nఎ \nnj \nఞ \nh \nహ \nei \nఏ \ntx \nట \nrx \nఱ \n4 \n \nai \nఐ \ntxh \nఠ \nd \nద \no \nఒ \ndx \nడ \ndh \nధ \noo \nఓ \ndxh \nఢ \np \nప \nau \nఔ \nnx \nణ \nph \nఫ \nmn \nO \nn \nన \nb \nబ \n \n \n \n \nbh \nభ \ngeminate consonant phonemes \nkk \nక  \ncc \nచ  \ntxtx \nట  \ngg \nగ  \njj \nజ  \ndxdx \nడ  \ntt \n   \npp \nప  \nss \n   \ndd \nద  \nbb \nబ  \nnn \nన  \nmm \nమ  \nww \nవ  \n \n \nB. \n  Acoustic Models  \nAcoustic models can be generated using Hidden Markov Models (HMM) or Subspace Gaussian \nMixture Models (SGMM) or Deep Neural Networks (DNNs)[12][13][14]. Here, HMM and SGMM \nbased acoustic models are considered for carrying out proposed study of performance of ASR \naccuracy for different smoothing techniques used for language model.   \nHMM and SGMM are also generated using the Kaldi toolkit[15] for ASR. Standard Mel-frequency cepstral \ncoefficients (MFCCs) with delta and double delta coefficients are extracted and Linear Discriminant Analysis \n(LDA) and Maximum Likelihood Linear Transform (MLLT) feature-space transformations were applied on \nthese MFCC features for feature-space reduction. Feature-space Maximum Likelihood Linear Regression (f-\nMLLR) is used for building Gaussian Mixture Models (GMM) for tied state triphones. These GMMs were \nsubsequently used to build SGMMs again using Kaldi tool kit[13].   \nIII. \nPERFORMANCE OF ASR ACCURACY WITH DIFFERENT SMOOTHING TECHNIQUES FOR LANGUAGE MODELS  \nA. \nIntroduction to smoothing and discounting \nData sparsity is a common problem for many applications. Even though large amount of text data is \navailable for language modeling, it only develops higher order N-gram models, which reduces \nsparsity to a certain extent but does not remove completely. So, the probability of N-gram sequences \nwithout appearing in the training text corpora will be assigned a zero in the language model. With \nhigher order N-gram models, sparsity also increases. Here, the data sparsity refers to the N-grams \n5 \n \nwhich occur in test data but not in training data. While there are many techniques to address the sparse \ndata, of them smoothing technique is more popular for addressing sparse data issues in the language \nmodeling[16]. The structure of language model is unchanged but the method used to estimate the \nprobabilities of the model is modified by smoothing. Thus, irrespective of the size of data, smoothing \nis required to handle the sparsity and improve the performance of language modeling and the \nperformance of ASR system[17]. Smoothing techniques adjusts low probabilities such as zero \nprobabilities upward and high probabilities downward. The smoothing techniques adjust the \nprobabilities, such that the estimate of probabilities of the sequences should not deviate much from \nthe true or original probabilities. Smoothing methods prevent not only zero probabilities, but also \nattempt to improve the accuracy of the model[18].  \nB. \nDifferent smoothing techniques  \nMany variations are reported in the literature for smoothing; Jelinek and Mercer [19]; Katz [20]; Bell et \nal.[21]  Ney et al. [16], and Kneser and Ney[22] [17]. Comparison of different smoothing techniques for \ndifferent applications and different languages are reported in the literature[23][22]. However, published \naccounts of comparison with different smoothing techniques for Indian languages especially for Telugu is not \nfound in the literature[18]. Therefore, there is a need to study and build efficient language models for for \nTelugu, with proper smoothing techniques. So in this project different smoothing techniques are \nevaluated with the transcription of Telugu TV news using ASR. \nTo generate the LMs for Telugu, transcribed text of Training and testing speech corpus and \nanother 1,60,271 sentences consisting 19,41,832 words of Telugu text was collected from \ndifferent online news websites. This text corpus is used for evaluation of smoothing models for \nlanguage modeling. The number of unique words covered in the text data are 2,37,994. \nThe advanced smoothing techniques will assign nonzero probability for never seen things (sequence \nof words or individual words) based on probability of things appeared at least once in the training \ndata. In this paper, popular smoothing techniques like, Linear discounting, Good-Turing, Absolute \ndiscounting, Witten-Bell, Kneser Ney discounting are considered for evaluation [24][25][26][22].  \nThe cmuclmtk-0.7  [27] is used to generate the Linear discounting, Good-Turing, Absolute \ndiscounting and Witten-Bell. The cmuclmtk-0.7 which initially working for 65,535 words, is modified \nto work for 2,50,000 words. Using SRILM tool[28], we have generated Language model for   Kneser \nNey smoothing. The performance of different smoothing techniques is given in Error! Reference source not \nfound..Witten-Bell smoothing performs better among all tested smoothing techniques. So Witten-Bell \nsmoothing was selected for further analysis.  \nTABLE 2WER WITH MODIFYING THE LM USING LANGUAGE STRUCTURE TOTAL DATA CONSIDERED FOR TESTING IS 2-HOUR 30 MIN AS \nHMM AS ACOUSTIC MODEL \nS.No Type of smoothing \nCorrect \nErrors \nWER \nInsertions Deletions Substitutions TOTAL \n \n6 \n \n1 \nGood –Turing \n5091 \n68 \n534 \n1189 \n1791 \n26.28 \n2 \nLinear discounting  \n5084  \n61  \n555  \n1175 \n1791 \n26.28 \n3 \nAbsolute discounting 5101  \n72  \n523 \n1190 \n1785 \n26.20 \n4 \nWitten-Bell \n5376 \n73  \n437 \n1001 \n1511 \n22.17 \n5 \nKneser Ney \n5151  \n66  \n505  \n1158 \n1729 \n24.37 \n \nThe most common metric for evaluating a language model is  the probability that the model assigns \nto test data that is termed, perplexity [29]. Best language model obtained from the training data, is \nexpected to give higher probabilities to the events occurred in the test data.  \n \nFigure 1 WER with respect to perplexity and OOVs \n \nPerplexity typically depends on the training and testing data.  However, perplexity does not depend \nonly on the size of training or testing data, but it also depends on the occurrence of test events in the \ntraining data. So, the training data is modified by splitting the words in the training data to root-word \nand suffix or prefix, wherever it is possible. Even though some of the root words and prefix or suffix \ndoes not occur together in the training data, these combined words can be generated easily after \nrecognizing the roots, suffixes and prefixes of words. \nThe language model with Witten-Bell smoothing, which gives less WER without splitting any \nword, is considered to evaluate the performance of ASR with varying perplexity.  \nThe Language model is generated using Witten-Bell smoothing technique. The perplexity of \nlanguage model is calculated by varying amount of test text included in the text corpus. The \nperformance of ASR with text corpus for different perplexity is given in the Error! Reference source not \nfound.. Figure 1 shows the performance of ASR with respect to perplexity and OOVs obtained without pre-\nprocessing text corpus. \n \n7 \n \n \n \n \n \nTABLE 3 WER FOR THE WITTEN-BELL SMOOTHING LM WITH DIFFERENT PERPLEXITY AND OOVS TOTAL DATA CONSIDERED FOR \nTESTING IS 2:30 MIN AND WORDS ARE 6814 TOTAL UNIQUE WORDS ARE 3163AND HMM AS ACOUSTIC MODEL \nS.No Perplexity  OOVs  \nWER % Errors \n3-grams hit \n2-grams hit  \n1-grams hit \nInsertions Deletions  Substitutions \n1 \n1610.23 \n662 (9.72%) 61.7 \n167 \n743 \n3265 \n572 (9.30%) \n1919 (31.1%) 3661 (59.5%) \n2 \n2572.70 \n526 (7.72%) 63.8 \n114  \n997  \n3240 \n617  (9.81%) 2144  (34.1%) 3527  (56.0%) \n3 \n599.43 \n413 (6.06%) 54.36 \n106  \n865 \n2733 \n1888 (29.5%) 1749 (27.32%) 2764 \n(43.18%) \n4 \n169.20 \n338 (4.96%) 46.84 \n100  \n756  \n2336 \n3096 (47.81) 1290 (19.21%) 2090 \n(32.97%) \n5 \n52.66 \n226 (3.32%) 39.30 \n86  \n668  \n1924 \n4220 (64.0%) 900 (13.6 %) \n1468 \n(22.28%) \n6 \n13.22 \n117 (1.72%) 29.84 \n81 \n551  \n1401 \n5593  (83.%) 404  (6.03%) \n700  (10.45%) \n7 \n3.98 \n0  (0.00%) \n22.17 \n73  \n437 \n1001 \n6812  \n(99.9%) \n1  (0.01%) \n1  (0.01%) \n \nIV. \nPERFORMANCE OF ASR ACCURACY WITH PREPROCESSING THE TEXT CORPUS WITH THE  SUPERVISED AND UN \nSUPERVISED STEMMING TECHNIQUES \nA. \nSupervised Stemming Techniques  \nFor Indian languages, particularly for Telugu, most of the base/root words are inflected to match \nwith the context of tense, plural or singular, and gender. The base words are either nouns or verbs. \nThe nouns can be modified in two ways: First, the singular nouns are inflected to make plural. \nSecond, morphological modification through case markers: nominative, genitive, dative, accusative, \nvocative, instrumental and locative[30]. The morphology of Telugu words as described in \nliterature[31][32]. For example, particular type of suffix will be added to the verbs (base word) ending \nwith “uu” sound to make new words, depending on the gender, person, and number as shown in the \nTable 4. \nIf we have the base words along with their possible inflections (suffixes and prefixes) mentioned in \ntext corpus, the generated LM will not have the OOVs related to base words and their inflections. If \nwe include, all possible inflected words of root/base, which are not available in the training text \ncorpus into the lexicon, the probability of these words is minimal or zero The recognition probability \nof such set of words is minimal. In Telugu writing system, the minimal unit is orthographic syllable or \nakshara. It is possible to separate the words and their inflections. Therefore, instead of including \nevery possible inflecting form separately, the base words, prefix and suffix parts are included as \nindividual words in the phonetic lexicon. Very few Telugu words, mostly Sanskrit based ones have \n8 \n \nprefixes (e.g. aadaraNa vs. anaadaraNa in which ‘an’ is the prefix)[33]. In words like cittaciwara, \nmottamodalu the first parts are not prefixes for these words are compound words. Also, in one of the \nexamples listed in the Table 4, caduwucunnaadu ‘he is reading’, there are many morphemes and \nlinguists would separate this word as: caduwu-tuu-un-naa-Du  to indicate the base, present prog. \nTense, male gender [34]. since our focus is on obtaining better accuracy for ASR, by reducing the \nnumber of  OOVs, we do not need to isolate all the morphemes in this manner. The word ‘suffix’ and \nprefix do not refer to all the morpho-phonemic alternations. Language model will assign probability \nfor base words and all the inflecting prefix and suffix parts as words. Then the Language model will \nalso give the probabilities for unseen inflected words as bigrams in the training corpus using \nsmoothing techniques.  \n \nTable 4  VERB ENDING WITH UU ( IN GOOGLE TRANSLITERATION IT IS VU) AND ITS SUFFIX BASED ON GENDER \nAGREEMENT \n \nS.No \nCombined word \nVerb ending with uu \nSuffix \n1. \n \nచద వ  చ    డ  (Caduvu cunnāḍu) \nచద వ (caduvu) \nచ    డ (Cunnāḍu) \n2. \n \nచద వ  చ న   (Caduvu cunnadi  \nచద వ (caduvu) \nచ న  (cunnadi) \n3. \n \nచద వ  చ    వ  (caduvu cunnāvu) \nచద వ  (caduvu) \nచ    వ  (cunnāvu) \n4. \n \nచద వ చ    న (caduvu cunnānu) \nచద వ (caduvu) \nచ    న (Cunnānu) \n5. \n \nచద వ  చ    ర  (caduvu cunnāru) \nచద వ  (caduvu) \nచ    ర  (cunnāru) \n6. \n \nచద వ  చ న   (caduvu cunnavi) \nచద వ  (caduvu) \nచ న   (cunnavi) \n7. \n \nచద వ  చ    మ  (caduvu cunnāmu) \nచద వ  (caduvu) \nచ    మ  (Cunnāmu) \n8. \n \nచద వ  క    డ  (caduvu kunnāḍu) \nచద వ  (caduvu) \nక    డ  (kunnāḍu) \n9. \n \nచద వ  క న   (caduvu kunnadi) \nచద వ  (caduvu) \nక న   (kunnadi) \n10. \n \nచద వ  క    వ  (caduvu kunnāvu) \nచద వ  (caduvu) \nక    వ  (kunnāvu) \n11. \n \nచద వ  క    న  (caduvu kunnānu) \nచద వ  (caduvu) \nక    న  (kunnānu) \n12. \n \nచద వ  క    ర  (caduvu kunnāru) \nచద వ  (caduvu) \nక    ర  (kunnāru) \n13. \n \nచద వ  క న   (caduvu kunnavi) \nచద వ  (caduvu) \nక న    (kunnavi) \n14. \n \nచద వ  క    మ  (caduvu kunnāmu) \nచద వ  (caduvu) \nక    మ  (kunnāmu) \n15. \n \nచద వ       డ  (caduvu tunnāḍu) \nచద వ  (caduvu) \n     డ  (tunnāḍu)   \n16. \n \nచద వ    న   (caduvu tunnadi) \nచద వ  (caduvu) \n  న   (tunnadi) \n17. \n \nచద వ       వ  (caduvu tunnāvu) \nచద వ  (caduvu) \n     వ  (tunnāvu) \n18. \n \nచద వ       న  (caduvu tunnānu) \nచద వ  (caduvu) \n     న  (tunnānu) \n19. \n \nచద వ       ర  (Caduvu tunnāru) \nచద వ  (caduvu) \n     ర  (tunnāru) \n9 \n \n20. \n \nచద వ    న  (caduvu tunnavi) \nచద వ  (caduvu) \n  న   (tunnavi) \n21. \n \nచద వ       మ  (caduvu tunnāmu) \nచద వ  (caduvu) \n     మ  (tunnāmu) \n22. \n \nచద వ ట(caduvuṭa) \nచద వ (caduvu) \nట (ṭa) \n23. \n \nచద వ   న  (caduvutānu) \nచద వ  (caduvu) \n  న  (tānu) \n24. \n \nచద వ   వ (Caduvutāvu) \nచద వ (caduvu) \n  వ  (tāvu) \n25. \n \nచద వ మ  (caduvumu) \nచద వ (caduvu) \nమ  (mu ) \n26. \n \nచద వ   మ  (caduvutāmu) \nచద వ (caduvu) \n  మ (tāmu) \n27. \n \nచద వ   ర  (caduvutāru)  \nచద వ  (caduvu) \n  ర  (tāru) \n \nLet us consider the inflected words chan'dan'gaa , mukhyan'gaa , akhilapakshhan'gaa, \nkein'draman'trigaa , kein'dran'gaa. These four words have different base-words with a fixed suffix. \nSimilarly, some inflected words will have same base-word but different suffixes. Such as, \nakhilapakshhan’to vs akhilapakshhan'gaa;  kein'draman'tri'to vs  kein'draman'tri'gaa. If, the words \nare separated at inflections gaa and to, the word kein'dran' to ,   which is a combination of prefix and \nsuffix of  different words, is not available in the training text corpus, will have good statistics .  The \nfollowing table will give a list of possible inflected words for a single base word.  \nThe text is pre-processed to separate the words considering the two grammar rules of Telugu. First \nconsidering the 16 inflections given in the Table 5 second the verbs ending with UU phoneme will \nfollow one of the 21 fixed suffixes. Out of 1,60,271 sentences consisting 19,41,832 words and \n2,10,000 unique words in the text corpus, 26,213 unique words are identified in the text corpus for \nsplitting in to root/base and suffix or prefix as given in the Table 6.  \n \nTABLE 5 LIST OF POSSIBLE INFLECTIONS FOR A SINGLE BASE WORD \n   \n(gā) \n   \n(tō) \n  న(paina) న (nu)      \n(gānē) \nలల  (lalōni)   మ  \n(tāmu) \n  న  \n(tānu) \nల  \n(lō) \nక  \n(ku) \nల   \n(lōni) \nల    \n(lōnē) \n     \n(tōnē) \n  న  (tōnū) \n  ర  \n(tāru) \n  వ (tāvu) \nTABLE 6 THE BASE NOUN AND THEIR INFLECTIONS \nS.No \nOriginal word \nWord prefix \nWord suffix(inflections) case markers \n1 \nఆ ధ ప       \nఆ ధ ప     \n   (gā) \n2 \nఆ ధ ప    ల  \nఆ ధ ప     \nల (lō) \n3 \nఆ ధ ప      న \nఆ ధ ప     \n  న(paina) \n10 \n \n4 \nఆ ధ ప   శల   \nఆ ధ ప     \nల  (lōni) \n5 \nఆ ధ ప    లల   \nఆ ధ ప     \nలల  (lalōni) \n6 \nఆ ధ ప      న  \nఆ ధ ప     \n  న (tōnū) \n7 \nఆ ధ ప       \nఆ ధ ప     \n  (tō) \n8 \nఆ ధ ప   శక  \nఆ ధ ప     \nక (ku) \n9 \nఆ ధ ప    న  \nఆ ధ ప     \nన (nu) \n10 \nఆ ధ ప    ల    \nఆ ధ ప     \nల   (lōnē) \n11 \nఆ ధ ప         \nఆ ధ ప     \n    (gānē) \n12 \nఆ ధ ప         \nఆ ధ ప     \n    (tōnē) \n \nTABLE 7 WER FOR THE WITTEN-BELL SMOOTHING  LM WITH DIFFERENT PERPLEXITY AND OOVS TOTAL DATA CONSIDERED FOR \nTESTING IS  2:30 MIN AND WORDS ARE  7476 . \nS.No Perplexity  OOVs \n(%) \nWER % Errors \n3-grams \nhit (%) \n2-grams \nhit (%) \n1-grams \nhit(%) \nInsertions Deletions  Substitutions \n1 \n850.12 \n612 \n(8.18%) \n59.64 \n176   \n937  \n3349 \n1011  \n(14.72%) \n2473  \n(36.00%) \n3386  \n(49.29%) \n2 \n1276 \n491 \n(6.56%) \n62.20 \n136  \n1136  \n 3382 \n1127  \n(16.12%) \n2687  \n(38.44%) \n3177  \n(45.44%) \n3 \n373.34 \n386 \n(5.16%) \n53.80 \n135   \n1016 \n2874 \n2442  \n(34.41%) \n2172  \n(30.61%) \n2482  \n(34.98%) \n4 \n131.68 \n315  \n(4.21%) \n46.83 \n124  \n 890  \n2490 \n3679  \n(51.33%) \n1586  \n(22.13%) \n1902  \n(26.54%) \n5 \n49.41 \n207  \n(2.77%) \n40.08 \n97  \n808 \n2094 \n4816  \n(66.20%) \n1119  \n(15.38%) \n1340  \n(18.42%) \n6 \n15.31 \n108  \n(1.44%) \n31.02 \n98   \n688 \n1535 \n6217  \n(84.31%) \n513  \n(6.96%) \n644  \n(8.73%) \n7 \n5.43 \n0  (0.00%) 23.63 \n89  \n 558  \n1121 \n7480  \n(99.97%) \n1  (0.01%) \n1  (0.01%) \nFrom the Error! Reference source not found.Table 7 it observed that, the performance of Witten-Bell \nsmoothing degrades when text corpus is pre-processed with supervised stemming, Kneser-Ney \nsmoothing is considered as an alternative for pre-processed data. \n \nTABLE 8 PERFORMANCE OF ASR WITH LM GENERATED WITH THE TEXT CORPUS OF TRAINING DATA AND WITH/ WITHOUT TEST DATA \n11 \n \nLM generated with the text corpus of training data and \nWith test data \nwithout test data \nwithout test data but  including OOVs as words \nCombined \nwords \n \nsplit \nwords(IA) \n \nsplit words-\nunsupervised \n \nCombined words \n \nsplit words(IA) \n \nsplit words-\nunsupervised \n \nCombined \nwords \n \nsplit words(IA) \n \nsplit words-\nunsupervised \n \nType of \nsmoothing  \nWitte\nn bell \nKnese\nr -Ney \nWitten \nbell \nKneser \n-Ney \nWitten \nbell \nKneser \n-ney \nWitten \nbell \nKneser \n-ney \nWitten \nbell \nKneser \n-ney \nWitten \nbell \nKneser \n-ney \nWitten \nbell \nKneser \n-ney \nWitten \nbell \nKneser \n–ney \nWitten \nbell \nKneser \n–ney \nWER-\nHMM \n25.16  30.03 25.36  29.97  26.06 \n28.12 \n62.24  \n62.50  \n60.39  59.72  57.82  56.23 59.61  \n58.96  \n58.41  57.32  57.01 55.23 \nWER-\nSGMM \n15.28  18.69 15.78  17.93  15.36 \n16.97 \n52.94  \n53.21  \n50.86  49.98  47.94 43.21 52.94  \n49.23  \n47.74  44.24 46.54 42.24 \nPerplexity 85.00 172.7\n5 \n65.94 137.07 \n \n 3727.91 3577.09 1500.06 \n \n \n \n 6412.57 2654.48 \n \n \n \n \n To evaluate the performance of ASR ,  we considered three cases for language model generation. \nIn the first case, test text is included in the text corpus, for the second case, the test text is removed \nfrom the text corpus, in third case only OOVs are included as words in the text corpus.  Witten-bell \nand knser-ney are employed in all three cases. Performance of ASR with Witten-Bell and Kneser-Ney \nLanguage Model  with supervised stemming of  pre-processed text corpus is given in the TABLE 8. The \nresults are averaged each time considering the 5 hours 30 minutes speech for testing from the total \nspeech corpus of 65 hours. Whenever text corresponding to the test data is included in generating the \nlanguage model , Witten-Bell smoothing performs well when compared to Kneser-Ney smoothing.   \nB. \nUnsupervised Stemming Techniques  \nThe inflected words are bifurcated into, root word with suffix or prefix by forming the rules \ndrawing on the knowledge of the grammar. However, as a set of suffixes and prefixes are common for \nmost of the inflected words, new words are formed by combining two words with “sandhi” or \n“samaasamu” in Telugu and many other Indian languages like “antarvedi” “Antarangamu”, where \n“anta” is common prefix for these two words. There are some more words that have common suffix or \nprefix parts, without any meaning. So, statistical techniques for segmentation of inflected words are \ndeveloped for Hindi and Telugu languages[35][36]. As the knowledge over grammar of language is \nnot used for segmentation, this type of segmentation is called unsupervised segmentation. However, \nas the segmentation is done purely through statistics, it may not be assured that all the segmented \nwords are generated only from the inflected words. \nSupervised segmentation is essential for NLP, as the broken words are used to understand the \nmeaning of a word or sentence. However, the recognized words in the ASR, are combined to form the \nsingle word and automatically gives the meaning. So, an unsupervised segmentation of words is \nproposed in the literature for ASR for Hindi.  The same technique with few changes is implemented \nfor segmentation of text corpus. This approach is most similar to that of [36] which employs statistical \nanalysis to discover stems.  \n12 \n \nThis method uses an unsupervised algorithm that automatically discovers stems and suffixes from a \ncorpus vocabulary. It requires a corpus vocabulary, a stem frequency threshold and a suffix frequency \nthreshold as inputs. First step corresponds to splitting all the words in the vocabulary at every \ncharacter (according to the Unicode encoding) and then define a bi-partite graph that represents all the \nprefixes and suffices as its partite sets with edges between vertices corresponding to a valid word in \nthe given vocabulary. Then restrict this graph to the maximal set of prefixes and suffixes such that \neach prefix has minimum number of different suffixes and each suffix has at least minimum number \nof different prefixes. This maximal set is found using an algorithm Prune given in the[3], which \niteratively removes the prefixes and suffixes corresponding to every vertex, whose degree is below the \ncorresponding threshold. \nokkokka saari nijamngaa bayatxa unna imeij eidaitei umntxumndoo manamu daaniki saripootaamaa nijamngaa(  original \n) \nokasaari nijamngaa bayatxa unna eidai too umntxumndoo manamu daaniki saripoo tama nijamn gaa (split) \nokokasaari nijamngaa bayatxa unnaanani epitoo umntxumndoo manamu daaniki saripootaanu maa nijamngaa \n(combined) \n \nTABLE 9 THE NUMBER OF UNIQUE WORDS IN THE TRAINING CORPUS  \nS.No Type of splitting \nNumber of Unique words  \n1 \nWithout splitting  \n2,37,994 \n2 \nAfter splitting the inflections with supervision 2,01,657 \n3 \nUnsupervised splitting criteria \n1,96,435 \nTABLE 10 WORD ERROR RATE WITH DIFFERENT LANGUAGE MODELING TRAINING CORPUS WITH SGMM AS ACOUSTIC MODELLING \nS.No Training corpus included for LM generation  \nWER (%) \n1 \nWithout stemming \n18.69 \n2 \nAfter stemming the inflections with \nsupervision \n17.93 \n3 \nunsupervised stemming  \n16.97 \n4 \nWithout splitting + unsupervised stemming \n16.42 \n \nThere is a significant reduction in number of unique words from 2,37,994 to 1,96,435 after un \nsupervised stemming as mentioned in Table 9. From the TABLE 8 it is observed that Knser-Ney \nsmoothing is performing well when the text corpus is pre-processed with supervised technique. We \nconsidered Knser-Ney smoothing for further analysis. The performance of ASR before and after \nunsupervised training is shown in Error! Reference source not found.. There is 2% improvement in the \nASR performance after unsupervised stemming \nIt is also observed that because of the possibility of one or two words (which are sometimes critical) \nbeing mis recognized in a sentence, despite the fact that the words in the following sentences in the \nsame paragraph, are recognized correctly, the context is not understood and for this reason sometimes \nthe readability of the paragraph is lost. When the language models are built by stemming the words in \nthe corpus, some of the critical words are added automatically to the language models and these words \n13 \n \nadded to the language model are recognized properly for improving the readability. \nV. \nCONCLUSIONS \nSome of the frequently used smoothing techniques were evaluated for Telugu language. The Witten-Bell \nand Kneser-Ney methods performed better than other techniques. Kneser-Ney smoothing technique performed \nwell when the text corpus is pre-processed using both supervised and unsupervised methods. The ASR word \naccuracy of the ASR system built with the normal text corpus is 81.01%. The ASR word accuracy was \nimproved by 0.76% when the language models were built with normal text corpus was split using supervised \nmethod i.e. grammar/morphological rules. Further the ASR word accuracy was improved by 0.94%, when the \nlanguage models were built with text corpus split by unsupervised method. The improvement in accuracy is \nattributed to the reduction of OOVs due to splitting the words in the text corpus. Overall, these techniques \nimproved the readability in the transcribed text. \nVI. \n   REFERENCE  \n[1] \nV. V. R. Vegesna, K. Gurugubelli, H. K. Vydana, B. Pulugandla, M. Shrivastava, and A. K. \nVuppala, “DNN-HMM Acoustic Modeling for Large Vocabulary Telugu Speech Recognition,” in \nMining Intelligence and Knowledge Exploration. MIKE 2017., Lecture No., P. R. Ghosh A., Pal \nR., Ed. Springer, Cham, 2017, pp. 189–197. \n[2] \nM. A. Basha Shaik, D. Rybach, S. Hahn, R. Schlüter, and H. Ney, “Hierarchical Hybrid Language \nmodels for Open Vocabulary Continuous Speech Recognition using WFST,” Proc. Work. Stat. \nPercept. Audit. (SAPA - SCALE), pp. 46–51, 2012. \n[3] \nP. Jyothi and M. Hasegawa-Johnson, “Improved Hindi broadcast ASR by adapting the \nlanguage model and pronunciation model using a priori syntactic and morphophonemic \nknowledge,” in Proceedings of the Annual Conference of the International Speech \nCommunication Association, INTERSPEECH, 2015, vol. 2015-Janua, pp. 3164–3168. \n[4] \nD. M. W. Powers, “Applications and explanations of Zipf’s law,” in Proceedings of the joint \nconferences on new methods in language processing and computational natural language \nlearning, 1998, pp. 151–160. \n[5] \nS. Chen, D. Beeferman, and R. Rosenfeld, “Evaluation metrics for language models,” Proc. \nDARPA Broadcast News Transcr. Underst. Work., pp. 275– 280, 1998. \n[6] \n“https://www.youtube.com/.” . \n[7] \n“https://ffmpeg.org/.” . \n[8] \n“https://baraha.com/v10/index.php.” . \n[9] \n“https://www.iitm.ac.in/donlab/tts/downloads/cls/cls_v2.1.6.pdf.” . \n[10] \nG. Anumanchipalli et al., “Development of Indian Language Speech Databases for Large \nVocabulary Speech Recognition Systems,” Proc. SPECOM, 2005. \n[11] \nA. . R. Kalika Bali, Partha Pratim Talukdar, N Sridhar Krishna, “Tools for the Development of a \nHindi Speech Synthesis System.,” in 5th ISCA Speech Synthesis Workshop, 2004, pp. 109–114. \n[12] \nL. R. Rabiner, “A tutorial on hidden Markov models and selected applications in speech \nrecognition,” Proc. IEEE, vol. 77, no. 2, pp. 257–286, 1989. \n14 \n \n[13] \nD. Povey et al., “The subspace Gaussian mixture model - A structured model for speech \nrecognition,” Comput. Speech Lang., vol. 25, no. 2, pp. 404–439, Apr. 2011. \n[14] \nG. E. Dahl, D. Yu, L. Deng, and A. Acero, “Context-dependent pre-trained deep neural \nnetworks for large-vocabulary speech recognition,” IEEE Trans. Audio, Speech Lang. Process., \nvol. 20, no. 1, pp. 30–42, 2012. \n[15] \nD. Povey et al., “The Kaldi Speech Recognition Toolkit,” Proc. ASRU, 2011. \n[16] \nH. Ney, S. Martin, and F. Wessel, “Statistical Language Modeling Using Leaving-One-Out,” in \nCorpus-Based Methods in Language and Speech Processing., Text, Spee., B. G. Young S., Ed. \nSpringer, 1997, pp. 174–207. \n[17] \nS. Chen and J. Goodman, “An empirical study of smoothing techniques for language modeling \n(Tech. Rep. No. TR-10-98).” \n[18] \nS. F. Chen, D. Beeferman, and R. Rosenfeld, “Evaluation metrics for language models,” Proc. \nDARPA Broadcast News Transcr. Underst. Work., pp. 275– 280, 1998. \n[19] \nR. L. Jelinek, F. & Mercer, “Interpolated estimation of Markov source parameters from sparse \ndata.,” in Proceedings of the Workshop on Pattern Recognition in Practice, North-Holland, \nAmsterdam, The Netherlands, 1980, pp. 381–397. \n[20] \nS. M. Katz, “Estimation of Probabilities from Sparse Data for the Language Model Component \nof a Speech Recognizer,” 1987. \n[21] \nI. H. W. Timothy C Bell, John G Cleary, Text compression. Prentice-Hall, Inc. Upper Saddle \nRiver, NJ, USA , 1990. \n[22] \nR. Kneser and H. Ney, “Improved backing-off for M-gram language modeling,” in 1995 \nInternational Conference on Acoustics, Speech, and Signal Processing, 2002, vol. 1, pp. 181–\n184. \n[23] \nBo-June (Paul) Hsu, “Generalized linear interpolation of language models,” in 2007 IEEE \nWorkshop on Automatic Speech Recognition & Understanding (ASRU), 2007, pp. 136–140. \n[24] \nK. W. Church and W. A. Gale, “A comparison of the enhanced Good-Turing and deleted \nestimation methods for estimating probabilities of English bigrams,” 1991. \n[25] \nH. Ney, U. Essen, and R. Kneser, “On structuring probabilistic dependences in stochastic \nlanguage modelling,” Comput. Speech Lang., vol. 8, no. 1, pp. 1–38, Jan. 1994. \n[26] \nI. H. Witten and T. C. Bell, “The Zero-Frequency Problem: Estimating the Probabilities of Novel \nEvents in Adaptive Text Compression,” IEEE Trans. Inf. Theory, vol. 37, no. 4, pp. 1085–1094, \n1991. \n[27] \n“https://cmusphinx.github.io/wiki/tutoriallm/#training-an-arpa-model-with-cmuclmtk.” . \n[28] \nA. Stolcke, “SRILM AN EXTENSIBLE LANGUAGE MODELING TOOLKIT,” ICSLP 2002, pp. 901–\n904, 2002. \n[29] \nP. F. Brown, V. J. Della Pietra, R. L. Mercer, S. A. Della Pietra, and J. C. Lai, “An estimate of an \nupper bound for the entropy of English,” Comput. Linguist., vol. 18, no. 1, pp. 31–40, 1992. \n[30] \nM. Ganapathiraju and L. Levin, “TelMore: Morphological Generator for Telugu Nouns and \nverbs,” in Proceedings of the Second International Conference on Universal Digital Library, \n15 \n \nAlexandria, Egypt November 17-19, 2006, 2006, pp. 1–7. \n[31] \nDr.Divakarla venkatawdhani, telugu in thirty days. Hyderabad: Andhrapradesh Sahitya \nAcadamy, 1976. \n[32] \nB.Purushottam; R Srihari Shastri; D VenkataRama, Vyakarana padakosamu Sastra \nnighantuvu. . \n[33] \nB. Krishnamurti and J. P. L. Gwynn, A Grammar of Modern Telugu. Oxford: Oxford University \nPress, 1985. \n[34] \nB. Krishnamurti, Telugu Verbal Bases. Motilal Banarsidass Publishers Pvt. Limited, 2009. \n[35] \nA. P. Siva Kumar, P. Premchand, and A. Govardhan, “TelStem:An Unsupervised Telugu \nStemmer with Heuristic Improvements and Normalized Signatures,” 2011. \n[36] \nA. K. Pandey and T. J. Siddiqui, “An unsupervised Hindi stemmer with heuristic \nimprovements,” in Proceedings of the second workshop on Analytics for noisy unstructured \ntext data - AND ’08, 2008, pp. 99–105. \n \n",
  "categories": [
    "cs.CL",
    "eess.AS"
  ],
  "published": "2019-08-10",
  "updated": "2019-08-10"
}