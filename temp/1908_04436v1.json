{
  "id": "http://arxiv.org/abs/1908.04436v1",
  "title": "Superstition in the Network: Deep Reinforcement Learning Plays Deceptive Games",
  "authors": [
    "Philip Bontrager",
    "Ahmed Khalifa",
    "Damien Anderson",
    "Matthew Stephenson",
    "Christoph Salge",
    "Julian Togelius"
  ],
  "abstract": "Deep reinforcement learning has learned to play many games well, but failed\non others. To better characterize the modes and reasons of failure of deep\nreinforcement learners, we test the widely used Asynchronous Actor-Critic (A2C)\nalgorithm on four deceptive games, which are specially designed to provide\nchallenges to game-playing agents. These games are implemented in the General\nVideo Game AI framework, which allows us to compare the behavior of\nreinforcement learning-based agents with planning agents based on tree search.\nWe find that several of these games reliably deceive deep reinforcement\nlearners, and that the resulting behavior highlights the shortcomings of the\nlearning algorithm. The particular ways in which agents fail differ from how\nplanning-based agents fail, further illuminating the character of these\nalgorithms. We propose an initial typology of deceptions which could help us\nbetter understand pitfalls and failure modes of (deep) reinforcement learning.",
  "text": "“Superstition” in the Network:\nDeep Reinforcement Learning Plays Deceptive Games\nPhilip Bontrager,1 Ahmed Khalifa,1 Damien Anderson,2 Matthew Stephenson,3\nChristoph Salge, 4 Julian Togelius 1\n1New York University, 2University of Strathclyde, 3Maastricht University, 4University of Hertfordshire\n{philipjb, ahmed.khalifa}@nyu.edu, damien.anderson@strath.ac.uk, matthew.stephenson@maastrichtuniversity.nl,\nChristophSalge@gmail.com, julian@togelius.com\nAbstract\nDeep reinforcement learning has learned to play many games\nwell, but failed on others. To better characterize the modes\nand reasons of failure of deep reinforcement learners, we test\nthe widely used Asynchronous Actor-Critic (A2C) algorithm\non four deceptive games, which are specially designed to pro-\nvide challenges to game-playing agents. These games are im-\nplemented in the General Video Game AI framework, which\nallows us to compare the behavior of reinforcement learning-\nbased agents with planning agents based on tree search. We\nﬁnd that several of these games reliably deceive deep rein-\nforcement learners, and that the resulting behavior highlights\nthe shortcomings of the learning algorithm. The particular\nways in which agents fail differ from how planning-based\nagents fail, further illuminating the character of these algo-\nrithms. We propose an initial typology of deceptions which\ncould help us better understand pitfalls and failure modes of\n(deep) reinforcement learning.\nIntroduction\nIn reinforcement learning (RL) (Sutton and Barto 1998) an\nagent is tasked with learning a policy that maximizes ex-\npected reward based only on its interactions with the en-\nvironment. In general, there is no guarantee that any such\nprocedure will lead to an optimal policy; while convergence\nproofs exist, they only apply to a tiny and rather uninter-\nesting class of environments. Reinforcement learning still\nperforms well for a wide range of scenarios not covered by\nthose convergence proofs. However, while recent successes\nin game-playing with deep reinforcement learning (Justesen\net al. 2017) have led to a high degree of conﬁdence in the\ndeep RL approach, there are still scenarios or games where\ndeep RL fails. Some oft-mentioned reasons why RL algo-\nrithms fail are partial observability and long time spans be-\ntween actions and rewards. But are there other causes?\nIn this paper, we want to address these questions by look-\ning at games that are designed to be deliberately decep-\ntive. Deceptive games are deﬁned as those where the reward\nstructure is designed to lead away from an optimal policy.\nFor example, games where learning to take the action which\nproduces early rewards curtails further exploration. Decep-\ntion does not include outright lying (or presenting false in-\nCopyright c⃝2019, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nformation). More generally speaking, deception is the ex-\nploitation of cognitive biases. Better and faster AIs have to\nmake some assumptions to improve their performance or\ngeneralize over their observation (as per the no free lunch\ntheorem, an algorithm needs to be tailored to a class of\nproblems in order to improve performance on those prob-\nlems (Wolpert and Macready 1997)). These assumptions in\nturn make them susceptible to deceptions that subvert these\nvery assumptions. For example, evolutionary optimization\napproaches assume locality, i.e., that solutions that are close\nin genome space have a similar ﬁtness - but if very bad so-\nlutions surround a very good solution, then an evolutionary\nalgorithm would be less likely to ﬁnd it than random search.\nWhile we are speciﬁcally looking at digital games here,\nthe ideas we discuss are related to the question of optimiza-\ntion and decision making in a broader context. Many real-\nworld problems involve some form of deception; for ex-\nample, while eating sugar brings momentary satisfaction, a\nlong-term policy of eating as much sugar as possible is not\noptimal in terms of health outcomes.\nIn a recent paper, a handful of deceptive games were pro-\nposed, and the performance of a number of planning algo-\nrithms were tested on them (Anderson et al. 2018). It was\nshown that many otherwise competent game-playing agents\nsuccumbed to these deceptions and that different types of de-\nceptions affected different kinds of planning algorithms; for\nexample, agents that build up a model of the effects of in-\ngame objects are vulnerable to deceptions based on chang-\ning those effects. In this paper, we want to see how well\ndeep reinforcement learning performs on these games. This\napproach aims to gain a better understanding of the vulnera-\nbilities of deep reinforcement learning.\nBackground\nReinforcement learning algorithms learn through interact-\ning with an environment and receiving rewards (Sutton and\nBarto 1998). There are different types of algorithms that ﬁt\nthis bill. A core distinction between the types are between\nontogenetic algorithms, that learn within episodes from the\nreward that they encounter, and phylogenetic algorithms,\nthat learn between episodes based on the aggregate reward\nat the end of each episode (Togelius et al. 2009).\nFor some time, reinforcement learning had few clear suc-\ncesses. However, in the last ﬁve years, the combination of\narXiv:1908.04436v1  [cs.LG]  12 Aug 2019\nontogenetic RL algorithms with deep neural networks have\nseen signiﬁcant successes, in particular in playing video\ngames (Justesen et al. 2017) such as simple 2D arcade\ngames (Mnih et al. 2015) to more advanced games like Dota\n2 and Starcraft (OpenAI 2018; Vinyals et al. 2019). This\ncombination, generally referred to as deep reinforcement\nlearning, is the focus of much research.\nThe deceptive games presented in this paper were devel-\noped for the GVGAI (General Video Game Artiﬁcial Intelli-\ngence (Perez-Liebana et al. 2016)) framework. The GVGAI\nframework itself is based on VGDL (Video Game Descrip-\ntion Language (Ebner et al. 2013; Schaul 2013)) which is\na language that was developed to express a range of arcade\ngames, like Sokoban and Space Invaders. VGDL was devel-\noped to encourage research into more general video game\nplaying (Levine et al. 2013) by providing a language and an\ninterface to a range of arcade games. Currently the GVGAI\ncorpus has over 150 games. The deceptive games discussed\nin this paper are fully compatible with the framework.\nMethods\nTo empirically test the effectiveness of the deception in ev-\nery game, we train a reinforcement learning algorithm and\nrun six planning algorithms on each game. The beneﬁt of\nworking in GVGAI is that we are able to evaluate the same\ngame implementations with algorithms that require an avail-\nable forward model and with learning agents. GVGAI has a\nJava interface for planning agents as well as an OpenAI Gym\ninterface for learning agents (Perez-Liebana et al. 2016;\nRodriguez Torrado et al. 2018; Brockman et al. 2016).\nAll algorithms were evaluated on each game 150 times.\nThe agent’s scores are evaluated along with play through\nvideos. The qualitative analyses of the videos provide key\ninsights into the causes behind certain scores and into what\nan agent is actually learning. The quantitative and qualitative\nresults are then used for the ﬁnal analysis.\nReinforcement Learning\nTo test if these games are capable of deceiving an agent\ntrained via reinforcement learning, we use Advantage Actor-\nCritic (A2C) to learn to play the games (Mnih et al. 2016).\nA2C is a good benchmark algorithm and has been shown\nto be capable of playing GVGAI games with some success\n(Rodriguez Torrado et al. 2018; Justesen et al. 2018). A2C\nis a model-free,extrinsically driven algorithm that allows for\nexamining the effects of different reward patterns. A2C is\nalso relevant due to the popularity of model-free agents.\nDue to the arcade nature of GVGAI games, we train on\npixels with the same setup developed for the Atari Learning\nEnvironment framework (Bellemare et al. 2013). The atari\nconﬁguration has been shown to work well for GVGAI and\nallows a consistent baseline with which to compare all the\ngames (Rodriguez Torrado et al. 2018). Instead of tuning\nthe algorithms for the games, we designed the games for the\nalgorithms. We use the OpenAI Baselines implementation of\nA2C (Dhariwal et al. 2017). The neural network architecture\nis the same as the original designed by Mnih et al. (Mnih\net al. 2016). The hyper-parameters are the default from the\noriginal paper as implemented by OpenAI: step size of 5, no\nframe skipping, constant learning rate of 0.007, RMS, and\nwe used 12 workers.\nFor each environment, we trained ﬁve different A2C\nagents to play, each starting from random seeds. In initial\ntesting, we tried training for twenty million frames, and\nwe found that the agents converged very quickly, normally\nwithin two million frames of training. We therefore stan-\ndardized the experiments to all train for ﬁve million frames.\nOne stochastic environment, WaferThinMints, did not con-\nverge and might have beneﬁted from more training time.\nPlanning Agents\nFor comparison with previous work and better insight into\nthe universality of the deceptive problems posed here, we\ncompare our results to planning algorithms. What we mean\nby planning agents are algorithms that utilize a forward\nmodel to search for an ideal game state. In the GVGAI plan-\nning track, each algorithm is provided with the current state\nand a forward model and it has to return the next action in a\nsmall time frame (40 milliseconds). This time frame doesn’t\ngive the algorithm enough time to ﬁnd the best action. This\nlimitation forces traditional planning algorithms to be some-\nwhat greedy which, for most of these games, is a trap.\nIn this paper, we are using six different planning algo-\nrithms. Three of them (aStar, greedySearch, and sampleM-\nCTS) are directly from the GVGAI framework, while the\nrest (NovelTS, Return42, and YBCriber) are collected from\nthe previous GVGAI competitions. Two of these algorithms,\nReturn42, and YBCriber, are hybrid algorithms. They use\none approach for deterministic games, such as A* or Itera-\ntive Width, and a different one for stochastic games, such as\nrandom walk or MCTS. Both algorithms use hand designed\nheuristics to judge game states. These hybrid algorithms also\nuse online learning to bypass the small time per frame. The\nonline learning agents try to understand the game rules, from\nthe forward model during each time step, and then use that\nknowledge to improve the search algorithm.\nDeceptive Games\nIn our previous work, a suite of deceptive games was created\nin order to take a look at the effects that these deceptive me-\nchanics would have on agents (Anderson et al. 2018). These\ndeceptive games were designed in order to deceive different\ntypes of agents in different ways.\nFrom a game design perspective, the category of decep-\ntive games partially overlaps with “abusive games”, as de-\nﬁned by Wilson and Sicart (Wilson and Sicart 2010). In par-\nticular, the abuse modalities of “unfair design” can be said to\napply to some of the games we describe below. Wilson and\nSicart note that these modalities are present in many com-\nmercial games, even successful and beloved games, espe-\ncially those from the 8-bit era.\nThis section describes some of these games in detail, and\ndeﬁnes optimal play for an agent playing each game. We\nfocus on four key categories of deception that these games\nexploit. We believe these categories represent general prob-\nlems that learning agents face and these simple games allow\n(a) DeceptiCoins Level 1 \n(b) DeceptiCoins Level 2 \n(c) DeceptiCoins Level 3 \nFigure 1: DeceptiCoins Levels\nus to shine a spotlight on weaknesses that model-free, deep\nreinforcement learning agents still face. For a more compre-\nhensive list of types of deceptions and deceptive games see\nDeceptive Games (Anderson et al. 2018).\nThe following four different categories of deception will\nbe discussed further in the discussion section: Lack of Hier-\narchical Understanding, Subverted Generalization, Delayed\nGratiﬁcation, and Delayed Reward.\nDeceptiCoins (DC)\nGame\nDeceptiCoins, Figure 1, offers an agent two paths\nwhich both lead to the win condition. The ﬁrst path presents\nimmediate points to the agent, in the form of gold coins. The\nsecond path contains more gold coins, but they are further\naway and may not be immediately visible to a short-sighted\nagent. Once the agent selects a path, they become trapped\nwithin their chosen path and can only continue to the bottom\nexit. The levels used here are increasingly larger versions of\nthe same challenge, but remain relatively small overall.\nThe optimal strategy for DeceptiCoins is to select the path\nwith the highest overall number of points. For the levels\nshown in Figure 1, this is achieved by taking the right side\npath, as it leads to the highest total score (i.e., more gold\ncoins can be collected before completing the level).\nGoal\nThe game offers a simple form of deception that tar-\ngets the exploration versus exploitation problem that learn-\ning algorithms face. The only way for the learning agent to\ndiscover the higher reward is for it to forgo the natural re-\nward it discovers early on completely. By designing different\nsized levels, we can see how quickly the exploration space\nbecomes too large. At the same time, an agent that correctly\nlearns, on the short route, about coins and navigation could\nthen see that going right is superior.\nResults\nThe ﬁrst two levels of DeceptiCoins are very\nsmall, and the agent fairly quickly learns the optimal strat-\negy. However, In level two the agent took several times\nlonger to discover the optimal strategy, as expected from an\nagent that can only look at the rewards of individual moves.\nLevel 3 proves to be too hard, and the agent converges on the\nFigure 2: The ﬁrst level of WaferThinMints\nsuboptimal strategy. By comparison, a randomly initialized\nagent is very likely to select the easy path, since it starts next\nto it, before being forced to move toward the exit.\nThe training curve for level 3 shows a signiﬁcant drop in\nperformance at the beginning of training. The video footage\nsuggests that the agent learns the concept of the gold coins\nand is attempting to collect them all, but fails to understand\nthat once it takes the easy coin it will become trapped in the\nleft path. The agent will also move back and forth between\nthe paths at the beginning of the game, trying to decide.\nWaferThinMints (Mints)\nGame\nWaferThinMints is inspired by a scene in Monty\nPython’s The Meaning of Life. The game presents the agent\nwith easily obtainable points, but if the agent collects too\nmany it will lead to a loss condition. The idea of this game\nis to model a situation where a repeated action does not\nalways lead to the same outcome or has a diminishing re-\nturn over time. The levels for this game feature mints which\neach award a point when collected and also ﬁll up a resource\ngauge on the agent. The level used is shown in ﬁgure 2. If\nthe avatar’s resource gauge (green bar on avatar) is ﬁlled,\ndeﬁned in this case as nine mints, and the agent attempts\nto collect an additional mint, then the agent is killed and\na loss condition is reached. Losing the game also causes the\nagent to lose 20 points. A waiter (not seen in Figure 2) moves\naround the board distributing mints at random. This means\nit is possible for an agent to get trapped while the waiter\nplaces mint on the agent’s square, forcing the agent to eat it.\nThe agent must, therefore, try to avoid getting trapped.\nThe optimal strategy is to collect as many mints as pos-\nsible without collecting too many, which is currently set as\nnine. The player should avoid mints early on and try to avoid\ngetting trapped. Near the end of the game, the agent should\nthen eat the remaining mints to get to 9.\nGoal\nWaferThinMints is our primary example of the\nchanging heuristic deception. The mint goes from providing\na positive reward to giving a substantial negative reward with\nthe only visual indication being a green bar on the avatar that\nrepresents how full the character is. The agent must learn\nthat the value of the mints is dependent on that green bar.\nSince the bar moves with the Avatar, it cannot just memo-\nrize a ﬁxed state in which to stop eating the mints. The mint\nis distributed by a chef and left around the board at random.\nFor the agent to play optimally, it should also learn that it is\nnot good to get full early on because it might get trapped in\nFigure 3: Flower level 1\nand forced to eat another mint at some point.\nResults\nAs can be seen from the graph, this agent did not\nhave enough time to converge completely. This points to the\ndifﬁculty of learning in the noisy environment where even a\ngood strategy could result in a bad reward if the agent is un-\nlucky. This is necessary though, as in a simpler environment\nwith a ﬁxed mint layout, the agent would learn to memorize\na path that results in a perfect score. The agent shows some\nimprovement over time but still plays very poorly.\nBy observing the agent, we see that the agent uses loca-\ntion to solve this problem. At the beginning of the episode,\nthe agent rushes to the room where the initial mints are\nplaced. This is a guaranteed source of rewards. The agent\nwill mostly stay in the room, a safe place, unless chased\nout by the chef’s mint placement. After the initial mints, the\nagent attempts to avoid mints until it’s trapped by them.\nIt is not clear whether the agent understands its fullness\nbar or uses the amount of mints placed in the game to as-\nsess the risk of eating more mints. The agent seems to have\nlearned that the mints become dangerous, but it seems to use\nstrange state and location information to help it know when\nto eat mints. This is related to the behavior we see in the\ngame Invest. It also is incapable of reasoning about waiting\nuntil the end of the game to eat mints when it is safer to eat,\nan instance of the delayed gratiﬁcation deception.\nFlower (Flow)\nGame\nFlower is a game which rewards patient agents by\noffering the opportunity to collect a small number of points\nimmediately, but which will grow larger over time the longer\nit is not collected. As shown in ﬁgure 3, a few seeds are avail-\nable for the agent to collect, which are worth zero points.\nThe seeds will eventually grow into full ﬂowers and their\npoint values grow along with them up to ten points. Once a\nﬂower is collected, another will begin to grow as soon as the\nagent leaves the space from which it was collected.\nThe optimal strategy for Flower is to let the ﬂowers grow\nto their ﬁnal stage of development before collecting them.\nGoal\nIn Flower, an agent is rewarded every time it collects\na ﬂower. To get maximum points the agent should collect\neach ﬂower the moment it matures to 10 points. This will\nprovide a better score than constantly collecting seedlings.\nResults\nThe training graph for this game shows the agent\nfalling for the speciﬁc deception with the sudden drop-off in\nperformance. As the agent gets better at knowing where the\nﬂowers are, the score starts to improve. Then the agent gets\nFigure 4: Invest level 1\ntoo good at collecting the ﬂowers, and they no longer have a\nchance to grow, lowering the score. Watching agent replays\nfurther conﬁrms this, the agent ﬁnds a circuit through all the\nﬂowers and then gets better at quickly moving through this\ncircuit. The agent perfectly falls for the deceit and has no\nway back unless it ignores the immediate rewards.\nInvest (Inv)\nGame\nInvest is a game where agents can forgo a portion of\ntheir already accumulated reward, for the beneﬁt of receiv-\ning a larger reward in the future. The level used is shown\nin ﬁgure 4. The agent begins with no points but can col-\nlect a small number of coins around the level to get some\ninitial amount. These points can then be “spent” on certain\ninvestment options. Doing this will deduct a certain number\nof points from the agent’s current score, acting as an imme-\ndiate penalty, but will reward them with a greater number\nof points after some time has passed. The agent has several\ndifferent options on what they can invest in, represented by\nthe three human characters (referred to as bankers) in the\ntop half of the level. Each banker has different rules: Green\nbanker turns 3 into 5 after 30 ticks, Red turns 7 into 15 after\n60 ticks, and Blue turns 5 into 10 after 90 ticks. The agent\ncan decide to invest in any of these bankers by simply mov-\ning onto them, after which the chosen banker will take some\nof the agent’s points and disappear, returning a speciﬁc num-\nber of timesteps later with the agent’s reward. The agent will\nwin the game once the time limit for the level expires.\nThe optimal strategy for Invest is deﬁned as successfully\ninvesting with everyone as often as possible.\nGoal\nInvest is a game where the agent has to intentionally\nseek some negative reward to get a positive reward, and then\nwait for a certain amount of time to get the positive reward.\nThis delayed reward makes it very difﬁcult for the reinforce-\nment learning algorithm to assign credit to a speciﬁc assign-\nment. The initial investment will only be assigned a negative\nreward, and the agent then has to ﬁgure out that the reward\nthat happens later should also be assigned to this action.\nIn this case, the reward is deterministic, and the challenge\ncould be increased further by making the delay stochastic.\nResults\nThe agent learns a very particular strategy for all\nﬁve instances of training. The agent ﬁrst collects all the coins\nand then invests with the Green Banker. From there it runs\nto the far right corner and waits, some agents always choose\nthe top while others choose the bottom. As soon as the Green\nbanker returns, the agent runs back over and reinvests only\nto run back to its corner and wait. This at ﬁrst seems like\nAgent\nDC 1\nDC 2\nDC 3\nInv\nFlow\nMints\naStar\n3.36\n3.54\n1.33\n17.53\n604.99\n1.92\ngreedySearch\n5.0\n3.0\n1.23\n1.0\n6.83\n-5.15\nsampleMCTS\n2.0\n2.0\n1.99\n3.5\n392.73\n5.73\nNovelTS\n2.1\n2.0\n2.0\n4.8\n298.51\n8.75\nReturn42\n5.0\n2.0\n2.0\n190.12\n329.73\n-2.66\nYBCriber\n5.0\n4.0\n4.0\n10.91\n300.73\n5.2\nA2C\n5.0\n3.79\n2.0\n69.6\n228.86\n-6.21\nTable 1: Average score for different games using different agents.\nDarker blue entries have higher positive score values for that game\nbetween all the agents, while darker red entries have higher nega-\ntive score values.\npuzzling behavior as a better strategy would be to sit next to\nthe Green Banker and be able to reinvest faster and collect\nmore points. On closer inspection, it becomes apparent that\nthe time it takes the agent to reach the far corner correlates\nwith the arrival of the delayed reward. It appears that the\nagent learned that investing in the Green Banker and then\ntouching the far tile resulted in a large positive reward.\nThe size of the game board allowed the agent to embody\nthe delay through movement and predict the arrival of the\nreward through how long it takes to walk across the board. It\nis possible that the agent would have learned to invest with\nthe other bankers if the board was larger so the agent could\nhave found a location associated with the delayed reward.\nThe training graph shows an interesting story too. The ini-\ntial random agent would accidentally invest with all three\nbankers and get a fairly high score despite not consistently\ninvesting with anyone. The agent quickly learns to avoid the\nnegative reward associated with the bankers and its score\ndrops. It stops investing with the Blue Banker ﬁrst, then the\nRed, and ﬁnally the Green. After it discovers how to predict\nthe delayed reward for the Green Banker, it starts doing this\nmore regularly until its performance converges.\nComparison with planning algorithms\nIn this section we want to compare the results from some of\nthe planning agents in the previous paper (Anderson et al.\n2018) with the deep RL results in this paper. Table 1, shows\nthe average score respectively for all the games using six dif-\nferent planning agents and the trained reinforcement learn-\ning agents. Every agent plays each game around 150 times,\nand the average score is recorded. These are drastically dif-\nferent algorithms from A2C, but they provide context for\nhow different algorithms are affected by our deceptions.\nWhile the planning agents perform slightly better on av-\nerage, this depends highly on what exact planning algorithm\nwe are examining. The planning algorithms have an advan-\ntage over the reinforcement learning algorithm as they have\na running forward model that can predict the results of each\naction. On the other hand, the small time frame (40 millisec-\nonds), for deciding the next action, doesn’t give the algo-\nrithm enough time to ﬁnd the best action.\nIn an important way, both RL and planning are facing a\nsimilar problem here. In both cases, the algorithms can only\nquery the game environment a limited amount of times. This\nmakes it impossible to look at all possible futures and forces\nthe algorithms to prioritize. While most planning agents en-\ntirely rely on the given forward model, some, such Return42,\nalso use online learning. These agents initially play with the\nforward model but will try to learn and generalize the game\nrules while playing. As the game progresses, they rely more\nand more on those learned abstractions. In general, this is\nan efﬁcient and smart strategy but makes them vulnerable\nto deceptions where the game rules changed in the middle\nof the game, such as in Wafer Thin Mints. Here the agents\nmight get deceived if they do not verify the result using the\nforward model. This is very similar to the problem that A2C\nencounters since the network representation is tries to gen-\neralize the states of the game.\nIn summary, while the best planning agents seem to be\nstronger than A2C, they also are subject to different forms\nof deceptions, dependent on how they are implemented.\nDiscussion\nIn summary, while the A2C deep reinforcement learn-\ning(Mnih et al. 2016) approach performs somewhat well, it\nrarely achieves the optimal performance in our games and is\nvulnerable to most deceptions discussed here. In contrast,\nthe A2C algorithm performs quite well across the board\nfor different AI benchmarks and can be considered com-\npetitive (Arulkumaran et al. 2017; Justesen et al. 2017). It\nshould also be noted that the fast-moving ﬁeld of deep re-\ninforcement learning has already produced numerous mod-\niﬁcations that could potentially solve the games discussed\nhere(Arulkumaran et al. 2017). However, instead of dis-\ncussing possible modiﬁcations to overcome any particular\nchallenge presented here, we want to take a step back and\nrefocus back on the point of this exercise. We are interested\nin deceptions to gain a better understanding of the general\nvulnerabilities of AI approaches, and try to gain a more sys-\ntematic understanding of the ways deep learning in particu-\nlar, and AI, in general, might fail. With the previous games\nas concrete examples in mind, we now want to discuss four,\nnon-exhaustive, categories for deception.\nTypes of Deception\nLack of Hierarchical Understanding\nThe DeceptiCoin\ngames are relatively easy to solve if one thinks about them\nat the right level of abstractions. DeceptiCoins can be seen as\na single binary decision between one path and another. Once\nthis is clear, one can quickly evaluate the utility of choosing\nthe correct one and pick the correct path. The deceptive el-\nement here is the fact that this is presented to the AI as an\nincredibly large search space, as it takes many steps to com-\nplete the overall meta-action. Humans are usually quite good\nat ﬁnding these higher levels of abstraction, and hence this\nproblem might not look like much of a deception to us - but\nit is pretty hard for an AI. The large search space, paired with\nthe assumptions that all actions along the path of the larger\naction matter, makes it very hard to explore all possible steps\nuntil a possible reward is reached. This is a similar problem\nto the famous problem in Montezuma’s Revenge, where the\nAI could not reach the goal, and its random exploration did\nnot even get close. This problem was only recently solved\nwith forced exploration (Ecoffet et al. 2019).\nFinding a good hierarchical abstraction can actually solve\nthe problem. For example, in DeceptiCoins we can look at\nthe path from one point to another as one action - something\nthat has been explored in GVGAI playing agents before.\nSubverted Generalization\nWafterthinmints is a game\nspeciﬁcally designed to trick agents that generalize. Agents\nthat simply use a forward model to plan their next step\nperform quite well here, as they realize that their next ac-\ntion will kill them. But in general, we do not have access\nto a forward model, so there is a need to generalize from\npast experience and use induction. The fact that each mint\nup to the 9th gives a positive rewards reinforces the idea\nthat eating a mint will be good. The 10th mint then kills\nyou. This is not only a problem for reinforcement learning,\nbut has been discussed in both epistemology (Hume 1739;\nRussell 1912) and philosophy of AI - with the consensus\nthat induction in general does not work, and that there is not\nreally a way to avoid this problem. The subverted general-\nization is also a really good example of how more advanced\nAIs become more vulnerable to certain deceptions. On aver-\nage, generalization is a good skill to have and can make an\nAI much faster, up to the point where it fails.\nDelayed Reward\nThe big challenge in reinforcement\nlearning is to associate what actions lead to the reward (Sut-\nton 1992). One way to complicate this is to delay the pay-\nment of this reward, as we did in the example of invest. The\nplayer ﬁrst has to incur a negative reward to invest, and then,\nafter a certain amount of time steps gets a larger positive re-\nward. The RL agent had two problems with Invest. First, it\nonly ever invests with the investor with the shortest repay-\nment time. The Red Banker would, overall, offer the best\npayout, but the RL agent either does not realize this rela-\ntionship, or does not associate the reward correctly.\nFurthermore, the RL agents also seems to be learning\n“superstitions”. When we examined the behaviour of the\nevolved RL agent, we see that the agent invests with the\nGreen Banker and then runs to a speciﬁc spot in the level,\nwaiting there for the reward payout. This behaviour is then\nrepeated, the agent runs to the banker and then back to the\nspot to wait for its reward. We reran the training for the RL\nagent and saw the same behaviour, albeit with a different\nspot that the agent runs to. We assume that this superstition\narose because the agent initially wandered off after invest-\ning in the Green Banker, and then received the reward when\nit was in that spot. It seems to have learned that it needs to\ninvest in the banker - as varying this behaviour would re-\nsult in no payout. But there is little pressure to move it away\nfrom its superstition of waiting for the result in a speciﬁc\nspot, even though this has no impact on the payout. In fact,\nit makes the behaviour, even with just the Green Banker sub-\noptimal, as it delays the time until it can invest again, as it\nhas to run back to the green banker.\nWhat was exciting about this behavior, was the fact that\nsimilar behavior was also observed in early reinforcement\nlearning studies with animals (Skinner 1948). Pigeons that\nwere regularly fed by an automatic mechanism (regard-\nless of their behaviour) developed different superstitious be-\nhaviours, like elaborate dance and motions, which Skinner\nhypothesized were assumed (by the pigeon) to causally in-\nﬂuence the food delivery. In our game, the agent seems to\ndevelop similar superstitions.\nDelayed Gratiﬁcation\nThere is a famous experiment\n(Mischel, Ebbesen, and Raskoff Zeiss 1972) about delayed\ngratiﬁcation that confronts 4 year old children with a marsh-\nmallow, and asks them not to eat it while the experimenter\nleaves the room. They are told that they will get another\nmarshmallow, if they can just hold off eating the ﬁrst marsh-\nmallow now. This task proves difﬁcult for some children,\nand it is also difﬁcult for our agent. Flower is a game where\nthe agent actually gets worse over time. This is because it\ninitially is not very good at collecting the ﬂowers, which al-\nlows the ﬂowers time to mature. The optimal strategy would\nbe to wait for the ﬂowers to grow fully, and then go around\nand collect them. The agent learns the expected reward of\ncollecting seeds early on but does not realize that this reward\nchanges with faster collection. When it updates its expected\nreward based on its new speed, it forgets that it could get\nhigher rewards when it was slower. While some of the plan-\nning algorithms perform better here, it is likely that they did\nnot actually “understand” this problem, but are simply much\nworse at collecting the ﬂowers (like the untrained RL agent).\nThis example demonstrates that we can design a problem\nwhere the AI gets worse over time by “learning” to play.\nConclusion\nIt appears that deep reinforcement learners are easily de-\nceived. We have devised a set of games speciﬁcally to show-\ncase different forms of deception, and tested one of the most\nwidely used RL algorithms, Advantage Actor-Critic (A2C),\non them. In all games, the reinforcement learners failed to\nﬁnd the optimal policy (with the exception that it found the\noptimal policy on one level of one game), as it evidently fell\nfor the various traps laid in the levels.\nAs the games were implemented in the GVGAI frame-\nwork, it was also possible for us to compare with tree search-\nbased planning agents, including those based on MCTS.\n(This is very much a comparison of apples and oranges, as\nthe planning agents have access to a forward model and di-\nrect object representation but are not given any kind of train-\ning time.) We can see that for every game, there is a plan-\nning agent which performs better than the A2C agent, but\nthat there is in most cases also a planning agent that per-\nforms worse. It is clear that some kinds of deception affect\nour reinforcement learning algorithm much more severely\nthan it affects the planning algorithms; in particular, the sub-\nverted generalization of WaferThinMints. On the other hand,\nit performed better than most planning algorithms given the\ndelayed reward in Invest, even though the policy it arrived at\nis bizarre to a human observer and suggests a warped asso-\nciation between cause and effect.\nWe look forward to testing other kinds of algorithms on\nthese games, including phylogenetic reinforcement learning\nmethods such as neuroevolution. We also hope that other re-\nsearchers will use these games to test the susceptibility of\ntheir agents to speciﬁc deceptions.\nReferences\n[Anderson et al. 2018] Anderson, D.; Stephenson, M.; Togelius, J.;\nSalge, C.; Levine, J.; and Renz, J. 2018. Deceptive games. In In-\nternational Conference on the Applications of Evolutionary Com-\nputation, 376–391. Springer.\n[Arulkumaran et al. 2017] Arulkumaran, K.; Deisenroth, M. P.;\nBrundage, M.; and Bharath, A. A. 2017. Deep reinforcement learn-\ning: A brief survey. IEEE Signal Processing Magazine 34(6):26–\n38.\n[Bellemare et al. 2013] Bellemare, M. G.; Naddaf, Y.; Veness, J.;\nand Bowling, M. 2013. The arcade learning environment: An eval-\nuation platform for general agents. Journal of Artiﬁcial Intelligence\nResearch 47:253–279.\n[Brockman et al. 2016] Brockman, G.; Cheung, V.; Pettersson, L.;\nSchneider, J.; Schulman, J.; Tang, J.; and Zaremba, W. 2016. Ope-\nnai gym. arXiv preprint arXiv:1606.01540.\n[Dhariwal et al. 2017] Dhariwal, P.; Hesse, C.; Klimov, O.; Nichol,\nA.; Plappert, M.; Radford, A.; Schulman, J.; Sidor, S.; and Wu, Y.\n2017. Openai baselines. https://github.com/openai/baselines.\n[Ebner et al. 2013] Ebner, M.; Levine, J.; Lucas, S. M.; Schaul, T.;\nThompson, T.; and Togelius, J. 2013. Towards a video game de-\nscription language. In Dagstuhl Follow-Ups, volume 6. Schloss\nDagstuhl-Leibniz-Zentrum fuer Informatik.\n[Ecoffet et al. 2019] Ecoffet, A.; Huizinga, J.; Lehman, J.; Stanley,\nK. O.; and Clune, J. 2019. Go-explore: a new approach for hard-\nexploration problems. arXiv preprint arXiv:1901.10995.\n[Hume 1739] Hume, D. 1739. A Treatise of Human Nature. Oxford\nUniversity Press.\n[Justesen et al. 2017] Justesen, N.; Bontrager, P.; Togelius, J.; and\nRisi, S. 2017. Deep learning for video game playing. arXiv preprint\narXiv:1708.07902.\n[Justesen et al. 2018] Justesen, N.; Torrado, R. R.; Bontrager, P.;\nKhalifa, A.; Togelius, J.; and Risi, S. 2018. Procedural level gen-\neration improves generality of deep reinforcement learning. arXiv\npreprint arXiv:1806.10729.\n[Levine et al. 2013] Levine, J.; Bates Congdon, C.; Ebner, M.;\nKendall, G.; Lucas, S. M.; Miikkulainen, R.; Schaul, T.; and\nThompson, T. 2013. General video game playing. Artiﬁcial and\nComputational Intelligence in Games.\n[Mischel, Ebbesen, and Raskoff Zeiss 1972] Mischel, W.; Ebbesen,\nE. B.; and Raskoff Zeiss, A. 1972. Cognitive and attentional mech-\nanisms in delay of gratiﬁcation. Journal of personality and social\npsychology 21(2):204.\n[Mnih et al. 2015] Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu,\nA. A.; Veness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.;\nFidjeland, A. K.; Ostrovski, G.; et al. 2015. Human-level control\nthrough deep reinforcement learning. Nature 518(7540):529.\n[Mnih et al. 2016] Mnih, V.; Badia, A. P.; Mirza, M.; Graves, A.;\nLillicrap, T.; Harley, T.; Silver, D.; and Kavukcuoglu, K. 2016.\nAsynchronous methods for deep reinforcement learning. In Inter-\nnational Conference on Machine Learning, 1928–1937.\n[OpenAI 2018] OpenAI. 2018. Openai ﬁve. https://blog.openai.\ncom/openai-ﬁve/.\n[Perez-Liebana et al. 2016] Perez-Liebana, D.; Samothrakis, S.;\nTogelius, J.; Lucas, S. M.; and Schaul, T. 2016. General video\ngame ai: Competition, challenges and opportunities. In Thirtieth\nAAAI Conference on Artiﬁcial Intelligence.\n[Rodriguez Torrado et al. 2018] Rodriguez Torrado, R.; Bontrager,\nP.; Togelius, J.; Liu, J.; and Perez-Liebana, D. 2018. Deep rein-\nforcement learning for general video game ai. In Computational\nIntelligence and Games (CIG), 2018 IEEE Conference on. IEEE.\n[Russell 1912] Russell, B.\n1912.\nThe Problems of Philosophy.\nWilliams and Norgate. chapter On Induction.\n[Schaul 2013] Schaul, T. 2013. A video game description language\nfor model-based or interactive learning. In IEEE Conference on\nComputatonal Intelligence and Games, CIG.\n[Skinner 1948] Skinner, B. F. 1948. ’superstition’in the pigeon.\nJournal of experimental psychology 38(2):168.\n[Sutton and Barto 1998] Sutton, R. S., and Barto, A. G. 1998. Re-\ninforcement learning: An introduction. MIT press.\n[Sutton 1992] Sutton, R. S. 1992. Introduction: The Challenge of\nReinforcement Learning. Boston, MA: Springer US. 1–3.\n[Togelius et al. 2009] Togelius, J.; Schaul, T.; Wierstra, D.; Igel, C.;\nGomez, F.; and Schmidhuber, J. 2009. Ontogenetic and phyloge-\nnetic reinforcement learning. K¨unstliche Intelligenz 23(3):30–33.\n[Vinyals et al. 2019] Vinyals, O.; Babuschkin, I.; Chung, J.; Math-\nieu, M.; Jaderberg, M.; Czarnecki, W. M.; Dudzik, A.; Huang,\nA.; Georgiev, P.; Powell, R.; Ewalds, T.; Horgan, D.; Kroiss, M.;\nDanihelka, I.; Agapiou, J.; Oh, J.; Dalibard, V.; Choi, D.; Sifre,\nL.; Sulsky, Y.; Vezhnevets, S.; Molloy, J.; Cai, T.; Budden, D.;\nPaine, T.; Gulcehre, C.; Wang, Z.; Pfaff, T.; Pohlen, T.; Wu, Y.;\nYogatama, D.; Cohen, J.; McKinney, K.; Smith, O.; Schaul, T.;\nLillicrap, T.; Apps, C.; Kavukcuoglu, K.; Hassabis, D.; and Sil-\nver, D. 2019. AlphaStar: Mastering the Real-Time Strategy Game\nStarCraft II. https://deepmind.com/blog/alphastar-mastering-real-\ntime-strategy-game-starcraft-ii/.\n[Wilson and Sicart 2010] Wilson, D., and Sicart, M. 2010. Now\nit’s personal: on abusive game design. In Proceedings of the Inter-\nnational Academic Conference on the Future of Game Design and\nTechnology, 40–47. ACM.\n[Wolpert and Macready 1997] Wolpert, D. H., and Macready, W. G.\n1997. No free lunch theorems for optimization. IEEE transactions\non evolutionary computation 1(1):67–82.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2019-08-12",
  "updated": "2019-08-12"
}