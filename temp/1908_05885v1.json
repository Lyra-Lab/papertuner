{
  "id": "http://arxiv.org/abs/1908.05885v1",
  "title": "Regression on imperfect class labels derived by unsupervised clustering",
  "authors": [
    "Rasmus Froberg Brøndum",
    "Thomas Yssing Michaelsen",
    "Martin Bøgsted"
  ],
  "abstract": "Outcome regressed on class labels identified by unsupervised clustering is\ncustom in many applications. However, it is common to ignore the\nmisclassification of class labels caused by the learning algorithm, which\npotentially leads to serious bias of the estimated effect parameters. Due to\nits generality we suggest to redress the situation by use of the simulation and\nextrapolation method. Performance is illustrated by simulated data from\nGaussian mixture models. Finally, we apply our method to a study which\nregressed overall survival on class labels derived from unsupervised clustering\nof gene expression data from bone marrow samples of multiple myeloma patients.",
  "text": "Regression on imperfect class labels derived by\nunsupervised clustering\nR.F. Brøndum1,2, T.Y. Michaelsen3, and M. Bøgsted1,2,4\n1Department of Haematology, Aalborg University Hospital, Denmark, 2Clinical\nCancer Research Center, Aalborg University Hospital, Denmark, 3Center for\nMicrobial Communities, Aalborg University, Denmark, 4Department of Clinical\nMedicine, Aalborg University, Denmark\nAugust 2019\nAbstract\nOutcome regressed on class labels identiﬁed by unsupervised clustering\nis custom in many applications.\nHowever, it is common to ignore the\nmisclassiﬁcation of class labels caused by the learning algorithm, which\npotentially leads to serious bias of the estimated eﬀect parameters. Due to\nits generality we suggest to redress the situation by use of the simulation\nand extrapolation method. Performance is illustrated by simulated data\nfrom Gaussian mixture models. Finally, we apply our method to a study\nwhich regressed overall survival on class labels derived from unsupervised\nclustering of gene expression data from bone marrow samples of multiple\nmyeloma patients.\n1\nIntroduction\nIn biomarker studies it is popular to perform an unsupervised clustering of\nhigh-dimensional variables like genome wide screens of SNPs, gene expressions,\nand protein data and regress for example treatment response, patient recorded\noutcome measures, time to disease progression, or overall survival on these po-\ntentially mislabelled clusters.\nIt is well-known from the statistical literature\nthat errors in continuous and categorical covariates can lead to loss of impor-\ntant information about eﬀects on outcome (Carroll et al., 2006). However, to\nour surprise this is often ignored when regressing outcome on classes identiﬁed\nby unsupervised learning, which might lead to important clinical eﬀect mea-\nsures being overlooked (Alizadeh et al., 2000; Veer et al., 2002; Guinney et al.,\n2015; Zhan et al., 2006; Broyl et al., 2010). We suggest to cast the problem as a\ncovariate misclassiﬁcation problem. This leaves us with a concourse of possible\nmodelling and analysis options, see for example the book by Carroll et al. (2006)\nor the recent review by Brakenhoﬀet al. (2018). A general approach, with good\n1\narXiv:1908.05885v1  [stat.ML]  16 Aug 2019\nstatistical properties, is to maximize the likelihood of a latent variable model\njoining the regression and classiﬁcation models (Nevo et al., 2016; Skrondal and\nRabe-Hesketh, 2004).\nFirst, this process does not mimic the workﬂow of the biologists, for whom\nthe basic question is to identify important biological processes and next to re-\nlate the clusters to clinical consequences. Secondly, upon parameter estimation\ncluster membership needs post hoc to be estimated by e.g. the maximum a\nposteriori probability, whereby direct connection to the regression parameter\nis lost.\nThirdly, this approach requires a statistical model of the clustering\nprocess, leaving out the possibility to combine popular unsupervised clustering\nalgorithms, such as hierarchical clustering, with popular parametric regression\nmodels like generalized linear models and Cox’s proportional hazards model.\nDue to its generality, we chose to study the two-stage modelling process.\nA number of ad-hoc methods have been developed in various speciﬁc settings\nwhich could be adapted to this situation. Examples include matrix methods\n(Morrissey and Spiegelman, 1999), regression calibration (Rosner et al., 1989),\npooled estimation (Spiegelman et al., 2001), multiple imputation (Cole et al.,\n2006), corrected score estimation (Nakamura, 1990), and simulation and extrap-\nolation (simex) (Cook and Stefanski, 1994; Carroll et al., 1996, 1999). Among\nthese methods, simex has become a useful tool for correcting eﬀect estimates\nin the presence of additive measurement error. The simex idea has been ex-\ntended to the misclassiﬁcation simulation and extrapolation (mcsimex) method\nfor correcting eﬀect estimates in the presence of errors in categorical covariates\n(K¨uchenhoﬀet al., 2006). In this paper, we chose to focus on mcsimex because\nof its generality, simplicity, and direct applicability.\nThe article is organized as follows. In Section 2, we detail the underlying\nhierarchical model (Section 2.1), describe how unsupervised learning and sta-\ntistical inference are performed (Section 2.2), and outline the mcsimex method\n(Section 2.3). Simulation studies in Section 3 show the performance of our pro-\nposal. An application to a study on unsupervised clustering of gene expression\ndata from bone marrow samples of multiple myeloma patients is given in Section\n4. The results of the paper is discussed in Section 5 followed by computational\ndetails of the the mcsimex method in Section 6.\n2\nStatistical setting\n2.1\nThe hierarchical model\nThe m-component Gaussian mixture model (GMM) of Z = (Z1, ..., Zp)⊤has\nthe following distribution\nGMM:\n\u001a H ∼Categorical(α1, ..., αm)\nZ|H = h ∼Np(µh, Σh)\n(1)\nwhere H ∈{1, 2, ..., m} corresponds to the class and α1, ..., αm, (≥0) are the\nmixture proportions satisfying Pm\nh=1 αh = 1. Thus, the GMM is parameterized\n2\nby\nθ = (α1, ..., αm, µ1, ..., µm, Σ1, ..., Σm).\nWe denote the possibly misspeciﬁed variable H∗for the corresponding correctly\nspeciﬁed H, i.e.\nH∗| H = j ∼Categorical(π1j, ..., πmj).\n(2)\nThus, the misclassiﬁcation error is characterized by the m × m misclassiﬁcation\nmatrix Π = [πij]m×m, which is deﬁned by its components\nπij = P(H∗= i | H = j), i, j = 1, ..., m.\n(3)\nThe outcome Y is here modelled as a generalized linear model\ng(E(Y | H = h)) = x⊤\nh β,\n(4)\nwhere β = (β1, ..., βm)⊤, xh = e1 + eh1[h ̸= 1] is encoded by treatment con-\ntrasts, g is the link function, and Y is assumed to be generated from an expo-\nnential family distribution (McCullagh and Nelder, 1989).\n2.2\nStatistical inference\nAssume we have n i.i.d. realizations {(yi, zi), i = 1, ..., n} of (Y, Z) and assume\nwe want an estimate of the eﬀect sizes β of each of the unobserved m com-\nponents. First, we estimate the parameters of the Gaussian mixture model by\nmaximizing the log-likelihood function to obtain\nˆθ = ( ˆα1, ..., ˆ\nαm, ˆµ1, ..., ˆµm, bΣ1, ..., bΣm).\nNow, we can estimate the class relationship for any covariate z ∈Rp by the\nfollowing hard clustering rule\nˆh(z) = arg max\nh\nˆph(z),\n(5)\nwhere ˆph is the density of the Nd(ˆµh, bΣh)-distribution and for the observed\ncovariate we write\nˆhi = ˆh(zi)\n(6)\nfor short. The misclassiﬁcation matrix Π can be consistently estimated in the\nfollowing way\nbΠ =\n\u001aZ\n1[ˆh(z) = i]ˆpj(z)dz\n\u001bm\ni,j=1\n.\n(7)\nThe next step is to optimize the log-likelihood of the generalized linear model\nbased on the assumed i.i.d. data y1|ˆh1, ..., yn|ˆhn, to obtain the naive estimate\n3\nˆβ({(yi, ˆhi), i = 1, ..., n}) (McCullagh and Nelder, 1989). The procedure outlined\nabove will in the following be referred to as the naive method.\nIt is important to notice that maximum likelihood estimation under the\nnaive method is estimation of a misspeciﬁed model. Convergence of estimates\nunder misspeciﬁed models is ensured by the results of White (1982). In general\nwe will denote the limit of a maximum likelihood estimate by β(Π) for a model\nmisspeciﬁed by the misclassiﬁcation matrix Π.\n2.3\nThe mcsimex method\nIt is well-known that estimating β by the naive method leads to a biased estimate\n(K¨uchenhoﬀet al., 2006). In order to formulate the mcsimex method to redress\nthis situation, we deﬁne the function G : [−1, ∞) →Rm by\nG(λ) = β(Π(1+λ)),\n(8)\nwhere Πλ can be expressed as Πλ = EΛλE−1 via the spectral decomposition,\nwith Λ being the diagonal matrix of eigenvalues and E the corresponding matrix\nof eigenvectors. For the function (8) to be well-deﬁned, we need to ensure the\nexistence of Πλ and that it is a misclassiﬁcation matrix for λ ≥0. Criteria for\nexistence are given in K¨uchenhoﬀet al. (2006).\nWe notice that G parameterizes the amount of misclassiﬁcation, where G(−1) =\nβ(Im×m) corresponds to no misclassﬁcation, G(0) = β(Π) corresponds to the\npresent misclassiﬁcation, and G(λ) for λ ≥0, corresponds to increasing mis-\nclassiﬁcation. The fundamental idea behind mcsimex is to simulate G(λ) for\nincreasing λ ≥0 and then extrapolate back to λ = −1.\nIn a few situations explicit forms of G as a function of λ can be calculated,\nbut they tend to be unstable to estimate, wherefore mcsimex relies on ﬁnite-\ndimensional parametric approximations G(λ, γ), γ ∈Rk, of G(λ). One example\nfrom the R-package simex is the quadratic approximation G(λ, (γ0, γ1, γ2)) =\nγ0+γ1∗λ+γ2∗λ2 (Lederer et al., 2017). It is custom in the mcsimex litterature to\nassume one either has the misclassiﬁcation matrix at hand or it can be estimated\nfrom training data. However, in our case the misclassiﬁcation matrix will be\n(consistently) estimated from data in the following way\nbΠ = {ˆπij} =\n\u001aZ\n1[ˆh(z) = i]ˆpj(z)dz\n\u001b\n.\n(9)\nThe mcsimex method (or algorithm) can now be formulated as:\nAlgorithm (K¨uchenhoﬀet al., 2006)\n1. For a ﬁxed grid of values 1 < λ1 < · · · < λk, simulate i.i.d. random\nvariables (condition upon {(yi, zi), i = 1, ..., n})\nH∗\nb,i(λk) ∼Categorical\n\u0010\nbΠλk\n•ˆhi\n\u0011\n,\n4\nwhere b = 1, ..., B, i = 1, ..., n, k = 1, ..., m, and bΠλk\n•ˆhi is the ˆhi’th column\nof bΠλk.\n2. Set\nˆβ(λk) = B−1\nB\nX\nb=1\nˆβ({(yi, h∗\nb,i(λk)), i = 1, ..., n}).\n3. Estimate γ by the least squares method\nˆγ = arg min\nγ∈Γ\nk\nX\ni=0\n\u0010\nˆβ(λk) −G(λk, γ)\n\u00112\n,\nwhere λ0 = 1 and ˆβ(λ0) = ˆβ({(yi, ˆhi), i = 1, ..., n}) is the naive estimator.\n4. The mcsimex estimate is then given by the extrapolation to λ = −1\nˆβS = G(−1, ˆγ).\n3\nSimulation studies\n3.1\nLogistic regression\nIn this section, we investigate empirically how diﬀerent sample sizes, imbalances\nbetween classes, and clustering algorithms aﬀect the eﬀect estimates. For the\nsimulations, we generate n = 200, n = 500, or n = 1000 independent sam-\nples from a 2-class Gaussian mixture model, where the prior probabilities of\nclass 1 is π1 = 5/10 or π1 = 2/10 to generate balanced or imbalanced classes,\nrespectively, and set π2 = 1 −π1. Class 1 and 2 observations have bivariate\nnormal distributions with means µ1 = (−1, 0) and µ2 = (1, 0) respectively, and\na common identity covariance matrix. The outcome is modelled by a logistic\nregression with linear predictor x⊤\nh β where the intercept and class eﬀect are\ngiven by (β1, β2) = (−1, 2).\nUnsupervised clustering was performed using either Gaussian mixture mod-\nels as implemented in the R-package mcclust (Scrucca et al., 2016), or k-means\nfrom the base implementation in R. We estimated the parameter β using either\nthe true class labels, class labels inferred from unsupervised clustering, and\nthe mcsimex method. The misclassiﬁcation matrix for the mcsimex approach\nwas inferred by drawing 100, 000 bivariate normal samples using the means and\nvariance-covariance matrix from each of the inferred clusters and counting how\noften they were misclassiﬁed by the ﬁtted clustering model.\nEach scenario was repeated 1000 times and the results are summarized by\nbias, mean-square error, and coverage of the estimated conﬁdence intervals. Re-\nsults from the balanced scenario π1 = 5/10 are shown in Table 1. We see that\nusing the true class labels there is a small bias in both of the estimated param-\neters and the coverage is close to 95%. When using the estimated class labels\n5\nTable 1: Results from applying simex to simulated data with balanced classes.\nA logistic regression model was ﬁtted with the true or inferred class labels, from\neither gaussian mixture models (GMM) or k-means with and without simex\ncorrection. Simulations were done with 200, 500, or 1000 samples.\nGMM\nKmeans\nTrue\nNaive\nSimex\nTrue\nNaive\n200\nbias β1\n-0.01\n0.34\n0.07\n0.34\n0.07\nbias β2\n0.03\n-0.69\n-0.17\n-0.67\n-0.12\ncoverage β1\n0.94\n0.55\n0.86\n0.6\n0.92\ncoverage β2\n0.94\n0.39\n0.9\n0.39\n0.93\n500\nbias β1\n0\n0.36\n0.08\n0.34\n0.06\nbias β2\n0\n-0.71\n-0.15\n-0.69\n-0.14\ncoverage β1\n0.96\n0.28\n0.88\n0.28\n0.92\ncoverage β2\n0.96\n0.06\n0.91\n0.06\n0.9\n1000\nbias β1\n0\n0.35\n0.06\n0.35\n0.08\nbias β2\n0.01\n-0.69\n-0.12\n-0.69\n-0.15\ncoverage β1\n0.95\n0.08\n0.88\n0.05\n0.87\ncoverage β2\n0.96\n0\n0.9\n0\n0.88\ninferred from either the GMM or K-means clustering without taking misclas-\nsiﬁcation into account, i.e. the na¨ıve model, both parameters are biased and\ncoverage is far from the assumed 95%. The bias is not alleviated by increas-\ning the number of samples, the coverage is however smaller, due to a smaller\nstandard error. When taking misclassiﬁcation into account, using the simex\napproach, both bias and coverage of the parameters is improved, and the im-\nprovement is similar across sample sizes. Results from the imbalanced scenario\nare shown in Table 2. These also show smaller bias and better coverage for\nthe simex model. Results are, however, better when using GMM than k-means.\nThe k-means algorithm tends to estimate clusters of uniform size (Hui Xiong\net al., 2009), leading to poor performance with imbalanced clusters, and since\nwe used the ﬁtted k-means clusters to infer the misclassiﬁcation matrix this also\nbecomes misspeciﬁed and the simex approach cannot fully correct for the added\nnoise. Some work has been done to alleviate this bias in the k-means algorithm,\ne.g. using multicenters (Liang et al., 2012) or undersampling (Kumar et al.,\n2014). We did not pursue this further in the present paper, but just notice that\none might instead use the GMM to infer clusters in the imbalanced case.\n6\nTable 2: Results from applying simex to simulated data with imbalanced classes.\nA logistic regression model was ﬁtted with the true or inferred class labels, from\neither Gaussian mixture models (GMM), or k-means with and without simex\ncorrection. Simulations were done with 200, 500 or 1000 samples.\nGMM\nKmeans\nTrue\nNaive\nSimex\nTrue\nNaive\n200\nbias β1\n-0.06\n0.52\n0.14\n1.09\n0.83\nbias β2\n0.06\n-0.7\n-0.17\n-1.16\n-0.74\ncoverage β1\n0.96\n0.55\n0.81\n0.01\n0.25\ncoverage β2\n0.95\n0.49\n0.89\n0.05\n0.61\n500\nbias β1\n0\n0.57\n0.14\n1.09\n0.82\nbias β2\n0.01\n-0.75\n-0.16\n-1.15\n-0.73\ncoverage β1\n0.95\n0.32\n0.82\n0\n0.04\ncoverage β2\n0.95\n0.2\n0.89\n0\n0.32\n1000\nbias β1\n-0.01\n0.53\n0.08\n1.08\n0.81\nbias β2\n0.01\n-0.71\n-0.1\n-1.14\n-0.72\ncoverage β1\n0.94\n0.16\n0.86\n0\n0\ncoverage β2\n0.95\n0.04\n0.89\n0\n0.08\n7\n3.2\nCox Proportional Hazards\nIn survival analysis the Cox proportional hazards model is often used as the\nmodel of choice for inferring the impact of covariates on the rate of events. The\nR package simex, used for this paper, did not previously support this model,\nwhich poses a problem for using mcsimex in survival analysis.\nThis can be\ncircumvented by using the Poisson approximation as done in Bang et al. (2013),\nbut we chose instead to augment the source code of the simex package to include\nthe coxph model class from the survival package (Therneau, 2015; Therneau and\nGrambsch, 2000).\nTo test the performance of our implementation we performed a second round\nof simulations. Here we simulated 200, 500, or 1000 samples, where class labels,\n(0, 1), were drawn from a binomal distribution with parameter π = 0.5 and\nadded misclassiﬁcation noise at a rate of 0.1, 0.2, or 0.3 (oﬀdiagonal values in\nthe misclassiﬁcation matrix). Survival times were drawn from an exponential\ndistribution with parameter λ = class + 1 and censoring times were drawn from\nand exponential distribution with parameter λ = 0.5. The results from 1000\nsimulations of each scenario are shown in Table 3. Results conﬁrm that the\nsimex method also reduces biases in the estimated parameters from the Cox\nproportional hazards model. However, the improvement in bias as well as the\ncoverage is better for lower rates of misclassiﬁcation. The coverage indicates that\nstandard errors for the estimates are too small, which is likely caused by estimat-\ning them with the jackknife variance estimator in the simex package. This has\npreviously been shown to underestimate the variance, so using an asymptotic\nvariance estimate is preferred (K¨uchenhoﬀet al., 2007). An asymptotic variance\nestimate for the Cox proportional hazards model, incorporating misclassiﬁcaion,\nhas to the best of our knowledge not yet been derived, but the problem can be\nalleviated by using a bootstrap approach to estimate the variance, which also\nadds the possibility to include additional variance resulting from estimation of\nthe misclassiﬁcation matrix (K¨uchenhoﬀet al., 2007). However, the bootstrap\napproach drastically increases computational cost as the simex model has to be\nﬁtted for each bootstrap sample.\n4\nCancer sub-classiﬁcation\nSince the invention of high dimensional gene expression proﬁling, just before this\nmillennium, a popular task has been to perform unsupervised cluster analysis on\nsuch data to identify new subgroups and correlate these subgroups to biological\ninformation, clinical data, and outcome. In this paper, we consider an exam-\nple from sub-classiﬁcation of multiple myleoma (MM). MM is a malignancy of\nend stage B cells that expand in the bone marrow, resulting in anemia, bone\ndestruction, and renal failure. The data set contains 414 gene expressions sets\nfrom multiple myeloma patient’s bone marrow (Zhan et al., 2006). The gene\nexpressions were proﬁled on Aﬀymetrix HGU133 Plus 2 arrays and exported\nto .CEL-ﬁles by the Aﬀymetrix Genomics Console. To replicate the analysis\n8\nTable 3: Results from cox estimation of 1000 simulations using a misclassiﬁca-\ntion probability (mp) of 0.1, 0.2 or 0.3\nmp = 0.1\nmp = 0.2\nmp = 0.3\nTrue\nNaive\nSimex\nNaive\nSimex\nNaive\nSimex\n200\nbias\n0.06\n−0.23\n0.07\n−0.48\n−0.03\n−0.68\n−0.22\ncoverage\n0.95\n0.87\n0.94\n0.60\n0.93\n0.29\n0.83\n500\nbias\n0.01\n−0.27\n0.00\n−0.50\n−0.10\n−0.70\n−0.30\ncoverage\n0.94\n0.71\n0.94\n0.22\n0.88\n0.02\n0.74\n1000\nbias\n0.01\n−0.28\n−0.01\n−0.51\n−0.11\n−0.69\n−0.31\ncoverage\n0.95\n0.48\n0.93\n0.04\n0.86\n0.00\n0.64\nof Zhan et al. (2006) we downloaded raw .CEL ﬁles from the GEO repository\nGS24080 and matched these by patient IDs to cases included in Zhan et al.\n(2006), and were able to match 407 out of the 414 cases. This was done since\nraw data were not available in the repository indicated in the original paper, so\nwe resorted to a later study from the same group. Data were MAS5 normal-\nized and ﬁltered according to instructions from Zhan et al. (2006) resulting in a\ndataset with gene expressions for 2,169 genes. This is a higher number of genes\nthan the 1,559 reported in the original paper, and is possibly caused by the dif-\nferent number of samples and/or slight diﬀerences in the MAS5 normalization\nprocedure as implemented in the R package aﬀy (Gautier et al., 2004) and the\nAﬀymetrix Microarray Suite GCOS 1.1.\nAfter ﬁltering, they performed hierarchical clustering on the remaining genes\nand chose to cut the tree, so seven clusters were formed. In order to mimic\ntheir analysis ﬂow we identiﬁed seven sub-groups by estimating a 7-component\nGMM. The classes were labelled according to the most similar class from Zhan\net al. (2006) as shown in the confusion matrix in Table 4. The classes estimated\nfrom the GMM had an accuracy of 0.9 compared to the original classes. Survival\ncurves are shown for the original classes and GMM classes in respectively panels\nA and B of Figure 1.\nThese data conﬁrm, that a Gaussian mixture model\nreasonably well approximates the hierarchical clustering of Zhan et al. (2006).\nThe 7 classes in Zhan et al. (2006) were split into a low risk group consisting\nof the CD1, CD2, HY, and LB classes, and a high risk group with the MF, MS,\nand PR classes with survival diﬀerences as shown in panel C of Figure 1 for the\noriginal classes and panel D for the GMM classes. A Cox proportional hazards\nmodel was employed to estimate the hazard ratio of high vs low risk group, but\nthis analysis did not take any possible misclassiﬁcation of the inferred classes\nfrom the unsupervised clustering into account. To investigate the impact of\n9\nTable 4: Confusion matrix for training set of GSE4581, accuracy = 0.9. Rows\nshow original classes from Zhan et al. (2006) and columns show clusters deter-\nmined with the Gaussian mixture model.\n1\n2\n3\n4\n5\n6\n7\nCD1\n15\n6\n0\n0\n0\n1\n0\nCD2\n0\n40\n0\n0\n0\n0\n1\nHY\n0\n2\n63\n0\n0\n0\n0\nMS\n0\n0\n0\n42\n0\n0\n0\nMF\n0\n0\n0\n0\n19\n1\n0\nPR\n0\n0\n1\n1\n0\n16\n11\nLB\n0\n0\n2\n0\n0\n0\n29\n++++ ++++ +++\n+++++ +\n++++++++++++++++++++++++++++++ +++ +\n++++++++++++++++++++++++++++++++++++++++++++++++++\n++++++++++++ +++++++++++++ +\n++\n+++++ +++++\n+\n+++++++++++\n++++++++++++ +\n+++++++++++\n+\n++\np = 0.0048\n0.00\n0.25\n0.50\n0.75\n1.00\n0\n20\n40\n60\n80\nTime\nSurvival probability\nStrata +\n+\n+\n+\n+\n+\n+\nCD1\nCD2\nHY\nLB\nMF\nMS\nPR\nZhan Classes\nA\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n+++++++++++++++++++++++++++++++++++++++++++++++ ++\np < 0.0001\n0.00\n0.25\n0.50\n0.75\n1.00\n0\n20\n40\n60\n80\nTime\nSurvival probability\nStrata +\n+\nHigh Risk\nLow Risk\nZhan Risk Class\nC\n++ +\n+++\n++++ +\n++++++++++++++++++++++++++++++++++ +++++ +\n++++++++++++++++++++++++++++++++++++++++++++++++++\n+++++++++++++++++ +++++++++++++\n++ +\n++\n+++++ +++++\n+\n+++++++++++\n++++++++++++ +\n+ ++++\n++\np = 0.00031\n0.00\n0.25\n0.50\n0.75\n1.00\n0\n20\n40\n60\n80\nTime\nSurvival probability\nStrata +\n+\n+\n+\n+\n+\n+\nCD1\nCD2\nHY\nLB\nMF\nMS\nPR\nGMM Classes\nB\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n++++++++++++++++++++++++++++++++++++++++++ ++\np < 0.0001\n0.00\n0.25\n0.50\n0.75\n1.00\n0\n20\n40\n60\n80\nTime\nSurvival probability\nStrata +\n+\nHigh Risk\nLow Risk\nGMM Risk Class\nD\nFigure 1: Survival curves for the identiﬁed classes, and risk groups in the original\nstudy and reﬁtted with GMM respectively.\n10\nTable 5: Hazard ratio of high vs low risk groups in the training set from Zhan\net al. (2006) with the na¨ıve and simex corrected models using the GMM classes.\nHR\nLower 95\nUpper 95\nP-value\nZhan - Na¨ıve\n2.53\n1.58\n4.06\n1.19e −04\nGMM - Na¨ıve\n2.55\n1.59\n4.09\n9.57e −05\nGMM - Simex (Average MC)\n3.15\n1.76\n5.66\n1.53e −04\nGMM - Simex (Full bootstrap)\n3.12\n1.72\n5.66\n2.28e −04\ncorrection for misclassiﬁcation we applied the mcsimex method to the data at\nhand. As shown in the simulation results the variance estimates of the built-\nin jackknife estimate are underestimated so we chose to do the analysis using\nbootstraps as well.\nWe performed 1000 bootstraps iterations where at each\nstep a sample of size n = 407 was drawn with replacement from the available\ndata. A GMM was ﬁtted to the sample and the out-of-bag samples was used to\ninfer the misclassiﬁcation matrix by comparing the predicted class from the in-\nbag GMM model to the class obtained from the full data. A Cox proportional\nhazards model was then ﬁtted to the in-bag sample and the mcsimex model\nwas applied using the estimated misclassiﬁcation matrix.\nBy estimating the\nmisclassiﬁcation matrix at each iteration of the bootstrap procedure we factor in\nthe added variance from its estimation. Results from this procedure, compared\nto the naive estimate along with results from using mcsimex with an average\nmisclassiﬁcation matrix from 1000 bootstraps, but only ﬁtting the mcsimex\nmodel once, are shown in Table 5.\nFor the average misclassiﬁcation matrix\nwe observed a misclassiﬁcation probability of 0.07 for the low risk group, and\n0.13 for the high risk group. For the na¨ıve models we see similar results for\nthe original classes from Zhan et al. (2006) and the reﬁtted classes from the\nGMM, and both approaches for correction for misclassiﬁcation gives a higher\npoint estimate for the hazard ratio of high vs low risk, conﬁdence intervals are,\nhowever, wide and overlapping.\n5\nDiscussion\nIn this paper, we documented a bias on eﬀect estimates when regressing on\nmisclassﬁed labels arising from unsupervised learning.\nWe also suggested a\nworkﬂow for adjusting the eﬀect estimates based on the mcsimex method. We\nhad to extend existing software to appropriately handle regression based on\ntime to event outcome. The eﬀectiveness of the workﬂow was documented on\nsimulated data and we also shed new light on bias and variance of eﬀect estimates\nin an existing cancer sub-classiﬁcation study.\nThe strength of the suggested workﬂow is essentially its general applicabil-\nity and seemingly robustness. However, these advantages come at the cost of\ncomputational intensiveness of the Monte Carlo approach in mcsimex.\n11\nAs mentioned in Section 1 there exists a number of alternative methods\nto handle misclassifed labels in regression models. Latent variable models seem\nmost interesting as they are built on parametric models and maximum likelihood\nestimation. Nevo et al. (2016) is the only paper we have come across dealing with\nthe misclassiﬁcation problem arising from unsupervised learning. In this paper,\nthough, a slightly diﬀerent set-up is studied, as they formulate a latent variable\nmodel for class risk given measured class features and additional covariates.\nIn the light of our results, we encourage researchers to adjust for bias when\nregressing on potentially misclassiﬁed labels. We also encourage biomarker re-\nsearchers to re-visit previous studies, especially those, which led to negative\nresults when regressing upon misclassiﬁed labels.\n6\nComputational details\nAll simulations and analyses were carried out by the statistical programming lan-\nguage R, using the mcsimex function from the simex package as the main vehicle\n(Lederer et al., 2017). This function contains functionality for mcsimex correc-\ntion of regressions from the lm, glm, gam, nls, polr, lme, and nlme functions.\nFor the current study we extended the package to accomodate Cox proportional\nhazards regression via the coxph function of the survival package (Therneau\nand Grambsch, 2000; Therneau, 2015). This extension was made by forking\nthe source code of the simex package from https://github.com/cran/simex.\nThe adapted code has been included in the simex package and is available at\nhttps://cran.r-project.org/package=simex\nWe utilized a number of other R and Bioconductor packages, notably the\nmclust package for clustering (Scrucca et al., 2016). For a complete list of pack-\nages see the Rmarkdown document available at https://github.com/HaemAalborg/\nmisClass, which details all steps in the analyses carried out in this paper.\n7\nAcknowledgements\nPart of the work was done while Martin Bøgsted visited University of Cam-\nbridge. He wants to thank Silvia Richardson, Rajen Shah, and Richard Sam-\nworth for their comments on the work and hospitality during the visit and the\nNorth Denmark Region’s Health Scientiﬁc Research Fund and the Lundbeck\nFoudation for ﬁnancial support.\nReferences\nAlizadeh, A. A., Eisen, M. B., Davis, R. E., Ma, C., Lossos, I. S., Rosenwald,\nA., Boldrick, J. C., Sabet, H., Tran, T., Yu, X., Powell, J. I., Yang, L., Marti,\nG. E., Moore, T., Hudson, J., Lu, L., Lewis, D. B., Tibshirani, R., Sherlock,\nG., Chan, W. C., Greiner, T. C., Weisenburger, D. D., Armitage, J. O.,\nWarnke, R., Levy, R., Wilson, W., Grever, M. R., Byrd, J. C., Botstein, D.,\n12\nBrown, P. O., and Staudt, L. M. (2000). Distinct types of diﬀuse large B-cell\nlymphoma identiﬁed by gene expression proﬁling. Nature, 403(6769):503–11.\nBang, H., Chiu, Y.-L., Kaufman, J. S., Patel, M. D., Heiss, G., and Rose,\nK. M. (2013). Bias Correction Methods for Misclassiﬁed Covariates in the\nCox Model: Comparison of Five Correction Methods by Simulation and Data\nAnalysis. Journal of Statistical Theory and Practice, 7(2):381–400.\nBrakenhoﬀ, T. B., Mitroiu, M., Keogh, R. H., Moons, K. G., Groenwold, R. H.,\nand van Smeden, M. (2018). Measurement error is often neglected in medical\nliterature: a systematic review. Journal of Clinical Epidemiology, 98:89–97.\nBroyl, A., Hose, D., Lokhorst, H., de Knegt, Y., Peeters, J., Jauch, A., Bertsch,\nU., Buijs, A., Stevens-Kroef, M., Berna Beverloo, H., Vellenga, E., Zweegman,\nS., Kersten, M.-J., van der Holt, B., el Jarari, L., Mulligan, G., Goldschmidt,\nH., van Duin, M., and Sonneveld, P. (2010). Gene expression proﬁling for\nmolecular classiﬁcation of multiple myeloma in newly diagnosed patients.\nBlood, 116(14):2543–2553.\nCarroll, R., Ruppert, D., Stefanski, L., and Crainiceanu, C. (2006).\nMea-\nsurement Error in Nonlinear Models. Chapman & Hall/CRC, Boca Raton,\nFlorida, second edition.\nCarroll, R. J., K¨uchenhoﬀ, H., Lombard, F., and Stefanski, L. A. (1996). Asymp-\ntotics for the SIMEX estimator in nonlinear measurement error models. Jour-\nnal of the American Statistical Association, 91(433):242–250.\nCarroll, R. J., Maca, J. D., and Ruppert, D. (1999). Nonparametric regression\nin the presence of measurement error. Biometrika, 86(3):541–554.\nCole, S. R., Chu, H., and Greenland, S. (2006).\nMultiple-imputation\nfor measurement-error correction.\nInternational Journal of Epidemiology,\n35(4):1074–1081.\nCook, J. R. and Stefanski, L. A. (1994). Simulation-extrapolation estimation\nin parametric measurement error models. Journal of the American Statistical\nAssociation, 89(428):1314–1328.\nGautier, L., Cope, L., Bolstad, B. M., and Irizarry, R. A. (2004). aﬀy—analysis\nof aﬀymetrix genechip data at the probe level. Bioinformatics, 20(3):307–315.\nGuinney, J., Dienstmann, R., Wang, X., de Reyni`es, A., Schlicker, A., Soneson,\nC., Marisa, L., Roepman, P., Nyamundanda, G., Angelino, P., Bot, B. M.,\nMorris, J. S., Simon, I. M., Gerster, S., Fessler, E., De Sousa E Melo, F.,\nMissiaglia, E., Ramay, H., Barras, D., Homicsko, K., Maru, D., Manyam,\nG. C., Broom, B., Boige, V., Perez-Villamil, B., Laderas, T., Salazar, R.,\nGray, J. W., Hanahan, D., Tabernero, J., Bernards, R., Friend, S. H., Laurent-\nPuig, P., Medema, J. P., Sadanandam, A., Wessels, L., Delorenzi, M., Kopetz,\nS., Vermeulen, L., and Tejpar, S. (2015). The consensus molecular subtypes\nof colorectal cancer. Nature Medicine, 21(11):1350–156.\n13\nHui Xiong, Junjie Wu, and Jian Chen (2009). K-Means Clustering Versus Val-\nidation Measures: A Data-Distribution Perspective. IEEE Transactions on\nSystems, Man, and Cybernetics, Part B (Cybernetics), 39(2):318–331.\nK¨uchenhoﬀ, H., Lederer, W., and Lesaﬀre, E. (2007).\nAsymptotic variance\nestimation for the misclassiﬁcation SIMEX. Computational Statistics & Data\nAnalysis, 51(12):6197–6211.\nK¨uchenhoﬀ, H., Mwalili, S. M., and Lesaﬀre, E. (2006).\nA general method\nfor dealing with misclassiﬁcation in regression: the misclassiﬁcation SIMEX.\nBiometrics, 62(1):85–96.\nKumar, N. S., Rao, K. N., Govardhan, A., Reddy, K. S., and Mahmood, A. M.\n(2014). Undersampled $$K$$ K -means approach for handling imbalanced\ndistributed data. Progress in Artiﬁcial Intelligence, 3(1):29–38.\nLederer, W., Seibold, H., and K¨uchenhoﬀ, H. (2017).\nsimex: SIMEX- and\nMCSIMEX-algorithm for measurement error models.\nLiang, J., Bai, L., Dang, C., and Cao, F. (2012). The K-Means-Type Algorithms\nVersus Imbalanced Data Distributions. IEEE Transactions on Fuzzy Systems,\n20(4):728–745.\nMcCullagh, P. and Nelder, J. A. (1989). Generalized Linear Models. Chapman\n& Hall/CRC, Boca Raton, Florida, second edition.\nMorrissey, M. J. and Spiegelman, D. (1999). Matrix methods for estimating\nodds ratios with misclassiﬁed exposure data: extensions and comparisons.\nBiometrics, 55(2):338–344.\nNakamura, T. (1990).\nCorrected score function for errors-in-variables mod-\nels: methodology and application to generalized linear models. Biometrika,\n77(1):127–137.\nNevo, D., Zucker, D. M., Tamimi, R. M., and Wang, M. (2016). Accounting for\nmeasurement error in biomarker data and misclassiﬁcation of subtypes in the\nanalysis of tumor data. Statistics in Medicine, 35(30):5686–5700.\nRosner, B., Willett, W. C., and Spiegelman, D. (1989). Correction of logis-\ntic regression relative risk estimates and conﬁdence intervals for systematic\nwithin-person measurement error. Statistics in Medicine, 8(9):1051–1069.\nScrucca, L., Fop, M., Murphy, T. B., and Raftery, A. E. (2016).\nmclust 5:\nclustering, classiﬁcation and density estimation using Gaussian ﬁnite mixture\nmodels. The R Journal, 8(1):205–233.\nSkrondal, A. and Rabe-Hesketh, S. (2004). Generalized latent variable model-\ning : multilevel, longitudinal, and structural equation models. Chapman &\nHall/CRC, Boca Raton, Florida.\n14\nSpiegelman, D., Carroll, R. J., and Kipnis, V. (2001). Eﬃcient regression cali-\nbration for logistic regression in main study/internal validation study designs\nwith an imperfect reference instrument. Statistics in Medicine, 20(1):139–160.\nTherneau, T. M. (2015). A package for survival analysis in S.\nTherneau, T. M. and Grambsch, P. M. (2000). Modeling Survival Data: Ex-\ntending the Cox Model. Springer, New York.\nVeer, L. J. V., Dai, H., Vijver, M. J. V. D., Schreiber, G. J., Kerkhoven,\nR. M., Roberts, C., Linsley, P. S., Bernards, R., and Friend, S. H. (2002).\nGene expression proﬁling predicts clinical outcome of breast cancer. Nature,\n415(6871):530–536.\nWhite, H. (1982). Maximum likelihood of misspeciﬁed models. Econometrica,\n50(1):1–25.\nZhan, F., Huang, Y., Colla, S., Stewart, J. P., Hanamura, I., Gupta, S., Epstein,\nJ., Yaccoby, S., Sawyer, J., Burington, B., Anaissie, E., Hollmig, K., Pineda-\nRoman, M., Tricot, G., van Rhee, F., Walker, R., Zangari, M., Crowley, J.,\nBarlogie, B., and Shaughnessy, J. D. (2006). The molecular classiﬁcation of\nmultiple myeloma. Blood, 108(6):2020–8.\n15\n",
  "categories": [
    "stat.ML",
    "cs.LG",
    "stat.ME"
  ],
  "published": "2019-08-16",
  "updated": "2019-08-16"
}