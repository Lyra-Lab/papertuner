{
  "id": "http://arxiv.org/abs/1908.06884v2",
  "title": "A Domain-Knowledge-Aided Deep Reinforcement Learning Approach for Flight Control Design",
  "authors": [
    "Hyo-Sang Shin",
    "Shaoming He",
    "Antonios Tsourdos"
  ],
  "abstract": "This paper aims to examine the potential of using the emerging deep\nreinforcement learning techniques in flight control. Instead of learning from\nscratch, we suggest to leverage domain knowledge available in learning to\nimprove learning efficiency and generalisability. More specifically, the\nproposed approach fixes the autopilot structure as typical three-loop autopilot\nand deep reinforcement learning is utilised to learn the autopilot gains. To\nsolve the flight control problem, we then formulate a Markovian decision\nprocess with a proper reward function that enable the application of\nreinforcement learning theory. Another type of domain knowledge is exploited\nfor defining the reward function, by shaping reference inputs in consideration\nof important control objectives and using the shaped reference inputs in the\nreward function. The state-of-the-art deep deterministic policy gradient\nalgorithm is utilised to learn an action policy that maps the observed states\nto the autopilot gains. Extensive empirical numerical simulations are performed\nto validate the proposed computational control algorithm.",
  "text": "1\nA Domain-Knowledge-Aided Deep Reinforcement\nLearning Approach for Flight Control Design\nHyo-Sang Shin, Shaoming He and Antonios Tsourdos\nAbstract—This paper aims to examine the potential of using\nthe emerging deep reinforcement learning techniques in ﬂight\ncontrol. Instead of learning from scratch, we suggest to leverage\ndomain knowledge available in learning to improve learning\nefﬁciency and generalisability. More speciﬁcally, the proposed\napproach ﬁxes the autopilot structure as typical three-loop\nautopilot and deep reinforcement learning is utilised to learn\nthe autopilot gains. To solve the ﬂight control problem, we\nthen formulate a Markovian decision process with a proper\nreward function that enable the application of reinforcement\nlearning theory. Another type of domain knowledge is exploited\nfor deﬁning the reward function, by shaping reference inputs\nin consideration of important control objectives and using the\nshaped reference inputs in the reward function. The state-of-\nthe-art deep deterministic policy gradient algorithm is utilised\nto learn an action policy that maps the observed states to\nthe autopilot gains. Extensive empirical numerical simulations\nare performed to validate the proposed computational control\nalgorithm.\nIndex Terms—Flight Control, Deep reinforcement learning,\nDeep deterministic policy gradient, Domain knowledge\nI. INTRODUCTION\nThe main objective of a ﬂight controller for modern air\nvehicles is to track a given command in a fast and stable\nmanner. Classical linear autopilot in conjunction with gain\nscheduling technique is one of widely-accepted frameworks\nfor ﬂight controller design due to its simplicity, local stability\nand ease of implementation [1]–[5]. This technique requires to\nlinearise the airframe dynamics around several characteristic\ntrim points and a static feedback linear controller is designed\nfor each operation point. The controller gains are then online\nscheduled through a proper interpolation algorithm to cover\nthe entire ﬂight envelop.\nThe systematic gain-scheduling approach provides engi-\nneers an intuitive framework to design simple and effective\ncontrollers for nonlinear airframes. The issue is that its per-\nformance might be signiﬁcantly degraded for a highly non-\nlinear and coupled system in which the assumptions on the\nconventional linear control theory could be violated. To resolve\nthe issue, there have been extensive studies on other control\nHyo-Sang Shin and Antonios Tsourdos are with the School of Aerospace,\nTransport and Manufacturing, Cranﬁeld University, Cranﬁeld MK43 0AL, UK\n(email: {h.shin,a.tsourdos}@cranfield.ac.uk)\nShaoming\nHe\nis\nwith\nthe\nSchool\nof\nAerospace\nEngineering,\nBeijing\nInstitute\nof\nTechnology,\nBeijing\n100081,\nChina\n(email:\nshaoming.he@bit.edu.cn) and also with the School of Aerospace,\nTransport and Manufacturing, Cranﬁeld University, Cranﬁeld MK43 0AL,\nUK (email: shaoming.he@cranfield.ac.uk)\n“This work has been submitted to the IEEE for possible publication.\nCopyright may be transferred without notice, after which this version may\nno longer be accessible.”\ntheories, e.g., sliding mode control [6], [7], backstepping [8],\nadaptive control [9], [10], state-dependent Riccati equation\n(SDRE) method [11], [12] and H∞synthesis [13], [14].\nHowever, each control method has its own advantages and\nlimitations. For example, sliding mode control usually suffers\nfrom the chattering problem and therefore it is difﬁcult to\nimplement in practice. The backstepping autopilot requires\nto calculate the derivatives of the virtual commands, which\nnormally contain some information that cannot be directly\nmeasured. The implementation of SDRE controller requires\nto solve the complicated algebraic Riccati equation at each\nsampling instant. In a recent contribution [15], nonlinear ﬂight\ncontrollers have been proved to share the same structure\nwith linear gain-scheduling controllers by properly adjusting\nthe feedback gains and therefore might suffer from similar\ndrawbacks: requiring partial model information in controller\ndesign.\nThanks to the rapid development on embedded compu-\ntational capability, there has been an increasing attention\non the development of computational control or numerical\noptimisation algorithms in recent years [16]. Unlike classical\noptimal autopilot, computational control algorithms generate\nthe control input relies extensively on onboard computation\nand there is no analytic solution of any speciﬁc control law.\nGenerally, computational control algorithms can be classiﬁed\ninto two main categories: (1) model-based ; and (2) data-based.\nThe authors in [17]–[19] leveraged model predictive control\n(MPC) to design a robust autopilot for agile airframes. The\nbasic idea behind MPC is that it solves a constrained nonlinear\noptimisation problem at each time instant in a receding horizon\nmanner and therefore shows appealing advantages in autopilot\ndesign. Except for MPC, bio-inspired numerical optimisation\nalgorithms, e.g., genetic algorithm, particle swarm optimisa-\ntion, have also been reported for ﬂight controller design in\nrecent years [20], [21].\nMost of the ﬂight control algorithms discussed so far are\nmodel-based control algorithms: they are generally designed\nunder the assumption that the model information is correctly\nknown. It is clear that the performance of model-based optimi-\nsation approaches highly relies on the accuracy of the model\nutilised. For modern air vehicles that suffer from aerodynamic\nuncertainties, it would be more beneﬁcial to develop data-\nbased autopilot. Considering the properties of the autopilot\nproblem, leveraging the reinforcement learning (RL) concept\nmight be an attractive option for developing a data-based\ncontrol algorithm [22]–[28].\nTo this end, this paper aims to examine the potential of using\nthe emerging deep RL (DRL) techniques in ﬂight controller\narXiv:1908.06884v2  [cs.AI]  11 Nov 2020\n2\ndesign. The problem is formulated in the RL framework by\ndeﬁning a proper Markovian Decision Process (MDP) along\nwith a reward function. Since the problem considered is a\ncontinuous-time control problem, the state-of-the-art policy\ngradient algorithm, i.e., deep deterministic policy gradient\n(DDPG), is utilised to learn a deterministic action function.\nPrevious works using RL or DRL to solve ﬂight control\nproblems mainly try to obtain networks, mapping from states\ndirectly to control actions [29]–[34]. As initial investigations,\nwe followed previous relevant studies, applied them in an\naerial vehicle model for low-level attitude control and ex-\namined their performance. From the investigation, it is found\nthat the learning effectiveness is generally an issue and it is\nextremely difﬁcult to generalise, i.e. to train networks in a\nway providing a reasonable performance to different types of\ninputs to the control system. Our initial investigation suggests\nthat this becomes more problematic in the aerial vehicle whose\ndynamics is statically unstable as a small change in the control\ncommand could result in an unstable response.\nAn extensive literature survey indicated that there have\nsome efforts to utilise prior or domain knowledge to improve\nlearning effectiveness, generalisation or both [35]–[39]. In\n[35], the authors proposed an learning architecture by util-\nising background knowledge, i.e., an approximate model of\nthe behaviour, in reinforcement learning and demonstrated\nthat this concept enables fast learning with reduced size of\nlearning samples. The results of [36] revealed that using prior\nknowledge of the tasks can signiﬁcantly boost the learning\nperformance and generalisation capabilities. In [37]–[39], the\nauthors suggested a particular structure that guides the learner\nby incorporating prior knowledge in supervised learning and\nreinforcement learning. This concept is proved to help increase\nthe learning efﬁciency and avoid getting stuck of the training\nprocess both in terms of training and generalisation error.\nThe results from these studies strongly suggest that leveraging\ndomain knowledge can improve the learning effectiveness.\nTo this end, this paper focuses on developing a DDPG-\nbased control framework that exploits the domain knowledge\nto enhance the learning efﬁciency. The main contributions of\nthe development are three aspects:\n(1) We develop a DDPG-based control framework that\nutilises the domain knowledge, that is the control structure. As\ndiscussed, most of RL or Deep RL (DRL) based algorithms\ndirectly learn control actions from scratch that might hinder\nthe learning efﬁciency. Unlike these typical algorithms, the\nproposed framework is formulated by ﬁxing the autopilot\nstructure. Under the problem formulated, DRL learns a deter-\nministic action function that maps the observed engagements\nstates to the autopilot gains. This enables the DRL-based\ncontrol to enhance the learning efﬁciency, retain the strengths\nof simple structure and improve the performance of classical\nautopilot via learning.\n(2) The reference input to the control system is shaped\nby considering several important criteria, e.g., rising time,\ndamping ratio, overshooting, in control system design. Then,\nthe shaped reference input is leveraged in the reward function.\nThis greatly simpliﬁes the parameter tuning process in Multi-\nObjective Optimisation (MOO). Note that there are many\nobjectives required to be achieved in control system design\nand hence control problems are often expressed as an MOO\nproblem. The proposed approach also allows the DRL-based\ncontrol to resolve the particularity in applying RL/DRL to the\ncontrol system design problem.\n(3) In the proposed DDPG-based control framework, this\npaper suggests to use normalised observations and actions\nin the training process to tackle the issue with the scale of\nrewards and networks. Our initial investigation indicated that\ndifferent scales in states create different scales in rewards\nand networks. Consequently, the training becomes ineffective,\nwhich will be shown via numerical simulations in Section\nVI-B. The numerical simulations conﬁrm that this issue can\nbe resolved by the normalisation.\nThe proposed concepts and performance are examined\nthrough extensive numerical studies. The numerical analysis\nreveals that the proposed DDPG autopilot guarantees satis-\nfactory tracking performance and exhibits several advantages\nagainst traditional gain scheduling approach. Robustness of\nthe domain-knowledge-aided DDPG autopilot is also inves-\ntigated by examining the performance of the proposed ap-\nproach against model uncertainty. The simulation results on\nthe robustness examination conﬁrm that the DDPG autopilot\ndeveloped is robust against model uncertainty. Also, relative\nstability of the proposed autopilot is numerically investigated\nand its results show that the proposed autopilot meets the\ntypical design criteria on the phase and gain margins.\nThe rest of the paper is organised as follows. Sec. II\nintroduces of the basic concept of deep reinforcement learning.\nSec. III presents nonlinear dynamics of airframes, followed\nby the details of the proposed computational ﬂight control\nalgorithm in Sec. IV. Finally, some numerical simulations and\nconclusions are offered.\nII. DEEP REINFORCEMENT LEARNING\nFor the completeness of this paper, this section presents\nsome backgrounds and preliminaries of reinforcement learning\nand DDPG.\nA. Reinforcement Learning\nIn the RL framework, an agent learns an action policy\nthrough episodic interaction with an unknown environment.\nThe RL problem is often formalised as a MDP or a partially\nobservable MDP (POMDP). A MDP is described by a ﬁve-\ntuple (S, O, A, P, R), where S refers to the set of states, O the\nset of observations, A the set of actions, P the state transition\nprobability and R the reward function. If the process is fully\nobservable, we have S = O. Otherwise, S ̸= O.\nAt each time step t, an observation ot ∈O is generated\nfrom the internal state st ∈S and given to the agent. The\nagent utilises this observation to generate an action at ∈A\nthat is sent to the environment, based on speciﬁc action policy\nπ. The action policy is a function that maps observations to\na probability distribution over the actions. The environment\nthen leverages the action and the current state to generate the\nnext state st+1 with conditional probability P (st+1|st, at) and\na scalar reward signal rt ∼R (st, at). For any trajectory in\n3\nthe state-action space, the state transition in RL is assumed\nto follow a stationary transition dynamics distribution with\nconditional probability satisfying the Markov property, i.e.,\nP (st+1|s1, a1, · · · , st, at) = P (st+1|st, at)\n(1)\nThe goal of RL is to seek a policy for an agent to interact\nwith an unknown environment while maximising the expected\ntotal reward it received over a sequence of time steps. The\ntotal reward in RL is deﬁned as the summation of discounted\nreward as\nRt =\nN\nX\ni=t\nγi−tri\n(2)\nwhere γ ∈(0, 1] denotes the discounting factor.\nGiven current state st and action at, the expected total\nreward is also known as the action-value function\nQπ (st, at) = Eπ [Rt|st, at]\n(3)\nwhich satisﬁes a recursive form as\nQπ (st, at) = Eπ [R (st, at) + γEπ [Qπ (st+1, at+1)]]\n(4)\nTherefore, the optimal policy can be obtained by optimising\nthe action-value function. However, directly optimising the\naction-value function or action-value function requires accu-\nrate model information and therefore is difﬁcult to implement\nwith model uncertainties. Model-free RL algorithms relax the\nrequirement on accurate model information and hence can be\nutilised even with high model uncertainties.\nB. Deep Deterministic Policy Gradient\nFor the autopilot problem, the main goal is to ﬁnd a\ndeterministic actuator command that could drive the air vehicle\nto track the target lateral acceleration command in a rapid\nand stable manner. Since this is a continuous control problem,\nwe utilise the the model-free policy gradient RL approach to\nlearn a deterministic function that directly maps the states to\nthe actions, i.e., the action function is updated by following\nthe gradient direction of the value function with respect to\nthe action, thus termed as policy gradient. More speciﬁcally,\nthe state-of-the-art policy-gradient solution, DDPG proposed\nby Google Deepmind [40], is leveraged to develop a com-\nputational lateral acceleration autopilot for an air vehicle.\nDDPG is an Actor-Critic method that consists of two main\nfunction blocks: (1) Critic evaluates the given policy based\non current states to calculate the action-value function; (2)\nActor generates policy based on the evaluation of critic. DDPG\nutilises two different deep neural networks, i.e., actor network\nand critic network, to approximate the action function and the\naction-value function. The basic concept of DDPG is shown\nin Fig. 1.\nDenote Aµ (st) as the deterministic policy, which is a\nfunction that directly maps the states to the actions, i.e.,\nat = Aµ (st). Here, we assume that the action function\nAµ (st) parameterised by µ. In DDPG, the actor function is\noptimised by adjusting the parameter µ toward the gradient of\nthe expected total reward as [40]\n∇atQw (st, Aµ (st)) = ∇µAµ (st) ∇atQw (st, at)\n(5)\nActor (Provide\nControl Policy)\nCritic (Evaluate\nControl Policy)\nEnvironment\nReward\nOutput\nObservation\nPerformance Evaluation\nAction\nFig. 1. Basic concept of DDPG.\nwhere Qw (st, at) stands for the action-value function, which\nis parameterised by w.\nThe parameter µ is then updated by moving the policy in\nthe direction of the gradient of Qw in a recursive way as\nµt+1 = µt + αµ∇µAµ (st) ∇atQw (st, at)\n(6)\nwhere αµ refers to the learning rate of the actor network.\nSimilar to Q-learning, DDPG also utilises the temporal-\ndifference (TD) error δt in approximating the error of action-\nvalue function as\nδt = rt + γQw (st+1, Aµ (st+1)) −Qw (st, at)\n(7)\nDDPG utilises the square of TD error as the loss function\nL(w) in updating the critic network, i.e.,\nL(w) = δ2\ni\n(8)\nTaking the partial derivative of L(w) respect to w gives\n∇wL(w) = −2δi∇wQw (st, at)\n(9)\nThe parameter w is then updated using gradient descent by\nfollowing the negative gradient of L(w) as\nwt+1 = wt + αwδt∇wQw (st, at)\n(10)\nwhere αw stands for the learning rate of the critic network.\nOne major issue of using deep neural networks in RL is\nthat most neural network optimisation algorithms assume that\nthe samples for training are independently and identically\ndistributed. However, this assumption is violated if the training\nsamples are directly generated by sequentially exploring the\nenvironment. To resolve this issue, DDPG leverages a mini\nbatch buffer that stores the training samples using the ex-\nperience replay technique. Denote et = (st, at, rt, st+1) as\nthe transition experience of the tth step. DDPG utilises a\nbuffer D with its size being |D| to store transition experiences.\nDDPG stores the current transition experience in the buffer and\ndeletes the oldest one if the number of the transition experience\nreaches the maximum value |D|. At each time step during\ntraining, DDPG uniformly draws N transition experience\nsamples from the buffer D and utilises these random samples\nto train actor and critic networks. By utilising the experience\nbuffer, the critic network is updated as\n∇µJ (Aµ) = 1\nN\nN\nX\ni=1\n∇µAµ (si) ∇atQw (si, ai)\nµt+1 = µt + αµ∇µJ (Aµ)\n(11)\n4\nFig. 2. The longitudinal dynamics model and parameter deﬁnitions.\nWith N transition experience samples, the loss function in\nupdating the critic network now becomes\nL(w) = 1\nN\nN\nX\ni=1\nδ2\ni\n(12)\nThe parameter of the critic network is updated by gradient\ndecent as\n∇wL (w) = 1\nN\nN\nX\ni=1\nδi∇wQw (si, ai)\nwt+1 = wt + αw∇wL(w)\n(13)\nNotice that the update of the action-value function is also\nutilised as the target value as shown in Eq. (7), which might\ncause the divergence of critic network training [40]. To address\nthis problem, DDPG creates one target actor network and\none target critic network. Suppose the additional actor and\ncritic networks are paramterised by µ′ and w′, respectively.\nThese two target networks use soft update, rather than directly\ncopying the parameters from the original actor and critic\nnetworks, as\nµ′ = τµ + (1 −τ) µ′\nw′ = τw + (1 −τ) w′\n(14)\nwhere τ ≪1 is a small updating rate. This soft update law\nshares similar concept as low-frequency learning in model\nreference adaptive control to improve the robustness of the\nadaptive process [41], [42].\nThe soft-updated two target networks are then utilised in\ncalculating the TD-error as\nδt = rt + γQw′ \u0010\nst+1, Aµ′ (st+1)\n\u0011\n−Qw (st, at)\n(15)\nWith very small update rate, the stability of critic network\ntraining greatly improves at the expense of slow training\nprocess. Therefore, the update rate is a tradeoff between\ntraining stability and convergence speed.\nThe detailed pseudo code of DDPG is summarised in\nAlgorithm 1.\nIII. NONLINEAR AIRFRAME DYNAMICS MODEL\nFor the feasibility investigation of the proposed approach,\nthis paper utilises the longitudinal dynamics model of a tail-\ncontrolled skid-to-turn airframe in autopilot design [11], as\nshown in Fig. 2. For simplicity, we assume that the air vehicle\nAlgorithm 1 Deep deterministic policy gradient\n1: Initialise the actor and critic networks with random\nweights µ and w\n2: Initialise the target actor and critic networks with weights\nµ′ ←µ and w′ ←w\n3: Initialise the experience buffer D\n4: for episode = 1: MaxEpisode do\n5:\nfor t = 1 : MaxStep do\n6:\nGenerate an action from the actor network based\non current state at = Aµ(st)\n7:\nAdd a random noise vt to the action for exploration\na′\nt = at + vt\n8:\nExecute the action a′\nt and receive new state st+1\nand reward rt\n9:\nStore\nthe\ntransition\nexperience\net\n=\n(st, at, rt, st+1) in the experience buffer D\n10:\nUniformly draw N random samples ei from the\nexperience buffer D\n11:\nCalculate the TD error δi\nδi = ri + γQw′ \u0010\nsi+1, Aµ′ (si+1)\n\u0011\n−Qw (si, ai)\n12:\nCalculate the loss function L(w)\nL(w) = 1\nN\nN\nX\ni=1\nδ2\ni\n13:\nUpdate the critic network using gradient descent\nas\n∇wL (w) = 1\nN\nN\nX\ni=1\nδi∇wQw (si, ai)\nwt+1 = wt + αw∇wL(w)\n14:\nUpdate the actor network using policy gradient as\n∇µJ (Aµ) = 1\nN\nN\nX\ni=1\n∇µAµ (si) ∇atQw (si, ai)\nµt+1 = µt + αµ∇µJ (Aµ)\n15:\nUpdate the target networks as\nµ′ = τµ + (1 −τ) µ′\nw′ = τw + (1 −τ) w′\n16:\nif the task is accomplished then\n17:\nTerminate the current episode\n18:\nend if\n19:\nend for\n20: end for\n5\nTABLE I\nPHYSICAL PARAMETERS.\nSymbol\nName\nValue\nIyy\nMoment of Inertia\n247.439 kg · m2\nS\nReference Area\n0.0409 m2\nd\nReference Distance\n0.2286 m\nm\nMass\n204.02 kg\ng\nGravitational acceleration\n9.8 m/s2\nTABLE II\nAERODYNAMIC POLYNOMIAL COEFFICIENTS.\nSymbol\nValue\nSymbol\nValue\naa\n0.3\nam\n40.44\nan\n19.373\nbm\n-64.015\nbn\n-31.023\ncm\n2.922\ncn\n-9.717\ndm\n-11.803\ndn\n-1.948\nis roll-stabilised, e.g., zero roll angle and zero roll rate, and has\nconstant mass, i.e., after boost phase. Under these assumptions,\nthe nonlinear longitudinal dynamics model can be expressed\nas\n˙α = QS\nmV (CN cos α −CA sin α) + g\nV cos γ + q\n˙q = QSd\nIyy\nCM\n˙θ = q\nγ = θ −α\naz = V ˙γ\n(16)\nwhere the parameters α, θ, γ and az represent angle-of-\nattack, pitch attitude angle, ﬂight path angle and lateral ac-\nceleration, respectively. In Eq. (16), m, g and V stand for\nmass, gravitational acceleration and velocity, respectively. The\nvariable Q represents the dynamic pressure, which is deﬁned\nas Q = 0.5ρV 2 with ρ being the air density. Additionally, the\nparameters S, d, Iyy, and m denote reference area, diameter,\nmoment of inertia, and mass, respectively. The values of all\nphysical parameters are detailed in Table I.\nThe aerodynamic coefﬁcients CA, CN and CM are deter-\nmined as\nCA = aa\nCN = anα3 + bnα |α| + cn\n\u0012\n2 −M\n3\n\u0013\nα + dnδ\nCM = amα3 + bmα |α| + cm\n\u0012\n−7 + 8M\n3\n\u0013\nα + dmδ\n(17)\nwhere ai, bi, ci and di with i = a, n, m are constants and\nthe values are presented in Table II. The parameters M and δ\nrepresent Mach number and control ﬁn deﬂection, respectively.\nThe Mach number is subject to the following differential\nequation\n˙M = QS\nmVs\n(CN sin α + CA cos α) −g\nVs\nsin γ\n(18)\nwhere Vs is the speed of sound.\nThe actuator of an air vehicle is usually modelled by a\nsecond-order dynamic system as\n\u0014 ˙δ\n¨δ\n\u0015\n=\n\u0014\n0\n1\n−ω2\na\n−2ξaωa\n\u0015 \u0014 δ\n˙δ\n\u0015\n+\n\u0014\n0\nω2\na\n\u0015\nδc\n(19)\nwhere ξa = 0.7 and ωa = 150rad/s denote the damping ratio\nand natural frequency, respectively. The variable δc represents\nthe actuator command.\nSince the standard air density model is a function of height\nh, the following complementary function is introduced\n˙h = V sin γ\n(20)\nIn autopilot design, the angle-of-attack α and the pitch rate\n˙θ are considered as the state variables. The lateral acceleration\naz is considered as the control output variable and the actuator\ncommand δc is regarded as the control input, that drives the\nlateral acceleration to track a reference command az,c.\nIV. COMPUTATIONAL LATERAL ACCELERATION\nAUTOPILOT\nA. Learning Framework\nTo apply the DDPG algorithm in autopilot design, we\nneed to formulate the problem in the DRL framework. One\nintuitive choice is to utilise the entire dynamics model, detailed\nin Eqs. (16)-(20), to represent the environment and directly\nlearn the actuator command δc during agent training process.\nHowever, this simple learning procedure has been shown to be\nineffective from our extensive test results. Our investigation\nsuggests that it is mainly due to the nature of the vehicle\ndynamics: the considered longitudinal dynamics is statically\nunstable and therefore a small change in the control command\ncould result in an unstable response. This hinders the learning\neffectiveness. Therefore, instead of learning the control action\nfrom scratch, we propose a new framework that utilises the\ndomain knowledge to improve the learning effectiveness. To\nthis end, we ﬁx the autopilot structure with several feedback\nloops and leverage DDPG to learn the controller gains to\nimplement the feedback controller. With this framework, it\nis also expected that the learning efﬁciency can be greatly\nimproved. Notice that this concept is similar to ﬁxed-structure\nH∞control methodology [43], [44]. However, our approach\nis a data-driven ﬂight control algorithm and therefore has\nadvantages against ﬁxed-structure H∞method.\nOver the past several decades, classical three-loop autopilot\nstructures have been extensively employed for acceleration\ncontrol for air vehicles due to its simple structure and effective-\nness [1], [14], [15], [45], [46]. The classical three-loop autopi-\nlot is given by a simple structure with two feedback loops, as\nshown in Fig. 3. The inner loop utilises a proportional-integral\nfeedback of pitch rate and the outer loop leverages proportional\nfeedback of lateral acceleration. With this architecture, the\nautopilot gains KDC, KA, KI and Kg are usually designed\nusing linear control theory for several trim operation points\nindividually. [46] compared various three-loop topologies and\nshowed that the gains can be optimally derived by using the\nLQR concepts. Note that the three-loop autopilot is realised by\n6\n+ -\nKDC\nKA\n+ -\nKI\ns\n+ -\nKg\naz\naz,c\nq\nδc\nFig. 3. Three-loop autopilot structure.\nscheduling the gains with some external signals, e.g., angle-\nof-attack, Mach number, height in linear control. Due to this\nfact, implementing classical three-loop autopilot requires a\nlook-up table and a proper scheduling algorithm. This fact\ninevitably increases the complexity of the controller and results\nin some approximation errors during the scheduling process.\nFor modern air vehicles with large ﬂight envelope, massive\nad hoc trim operation points are required to guarantee the\nperformance of gain-scheduling. This further increases the\ncomplexity of autopilot design.\nIn recent study [15], there was an investigation to iden-\ntify the connection between linear and nonlinear autopilots\nthrough three-loop topology. This study revealed that non-\nlinear autopilot shares the three-loop topology and the gains\nare parameter varying. The issue is that the performance of\nthe non-linear controller can be signiﬁcantly degraded with\npresence of system uncertainties, which could be the case in\nthe modern air vehicles.\nThis paper ﬁxes the structure of the autopilot as three-\nloop topology. Note that we might be able to examine other\nautopilot topology to overcome the limitations of the conven-\ntional one and even the autopilot topology could be subject of\nlearning itself. We will handle these points in our future study.\nTo address the issues with conventional control theories\ndiscussed, this paper aims to utilise DDPG to provide a direct\nmapping from scheduling variables to autopilot gains, i.e.,\nKDC = f1 (α, M, h)\nKA = f2 (α, M, h)\nKI = f3 (α, M, h)\nKg = f4 (α, M, h)\n(21)\nwhere fi with i = 1, 2, 3, 4 are nonlinear functions.\nIn other words, we suggest to directly train a neural net-\nwork that provides nonlinear transformations from scheduling\nvariables α, M, h, to autopilot gains KDC, KA, KI, Kg.\nB. Problem Formulation\nTo learn the autopilot gains using DDPG, we need to\nformulate the problem in the RL framework by constructing\na MDP with a proper reward function.\n1) MDP deﬁnition: The dynamics of angle-of-attack, pitch\nangle, pitch rate, Mach number and height, shown in Eqs. (16)-\n(20), constitutes the environment, which is fully characterised\nby the system state\nst = (α, q, θ, M, h)\n(22)\nAgent (Flight\nControl System)\nFlight Vehicle\nReward\nObservation\nAction\nFig. 4. Information ﬂow of the proposed RL framework.\nAs stated before, the aim of DDPG here is to learn the\nautopilot gains. For this reason, the agent action is naturally\ndeﬁned as\nat = (KDC, KA, KI, Kg)\n(23)\nFrom Eq. (20), it can be noted that the autopilot gains are\nfunctions of angle-of-attack, Mach number and height and\nthese three variables are directly measurable from onboard\nsensors. For this reason, the agent observation is deﬁned as\not = (α, M, h)\n(24)\nwhich gives a partially observable MDP. Note that the DDPG\nalgorithm is applicable to partially observable MDP, as shown\nin [47]. It is worthy pointing out that we can also include\npitch angle and pitch rate in the observation vector during\ntraining. However, increasing the dimension of observation\nwill increase the difﬁculty for the training process as more\ncomplicated network for function approximation is required.\nThe relative kinematics (16)-(20), environmental state (22),\nagent action (23), agent observation (24), together with a\nproper reward function, constitute a complete MDP formu-\nlation of the autopilot problem. The conceptual ﬂowchart of\nthe proposed ﬂight control RL framework is shown in Fig. 4.\n2) Reward function shaping: The most challenging part of\nsolving the autopilot design problem using DDPG is the devel-\nopment of a proper reward function. Notice that the primary\nobjective of an acceleration autopilot is to drive the air vehicle\nto track a given acceleration command in a stable manner with\nacceptable transient as well as steady-state performance. In\nother words, the reward function should consider necessary\ntime-domain metrics, e.g., rising time, overshoot, damping\nratio, steady-state error, in an integrated manner. This means\nthat the reward function should be designed as a weighted sum\n7\nTABLE III\nHYPER-PARAMETERS IN SHAPING THE REWARD FUNCTION.\nka\nkδ\naz,max\n˙δmax\n1\n0.1\n100\n1.5\nof several individual objectives, which, by default, poses great\ndifﬁculty on tuning the weights of different metrics. This paper\nproposes to resolve this issue by utilising another domain\nknowledge, that is the desired performance. In other words,\nwe shape the original reference command, az,c, based on the\ndesired performance and propose to track the shaped command\n¯az,c rather than the original reference command. The shaped\ncommand ¯az,c satisﬁes the following two properties:\n(1) The shaped command is the output of a reference\nsystem;\n(2) The reference system has desired time-domain charac-\nteristics.\nThis approach also enables alleviation of the particularity in\napplying RL/DRL to the control system design problem. One\nof the main objectives of the tracking control is to minimise the\ntracking error, that is the error between the reference command\nand actual output. If the tracking error is directly incorporated\ninto the reward function with a discounting factor between\n0 and 1, the learning-based control algorithm will consider\nminimising the immediate tracking error is more or equally\nimportant than minimising the tracking error in the future.\nThis might cause instability issue and is not well aligned\nwith the control design principles. Shaping the command and\ndeﬁning the tracking error with respect to the shaped command\ncan relax this mismatch between the RL and control design\nconcepts.\nIn consideration of the properties of a tail-controlled air-\nframe, we propose the following reference system\n¯az,c (s)\naz,c (s) =\n−0.0363s + 1\n0.009s2 + 0.33s + 1\n(25)\nwhere the utilisation of an unstable zero naturally arises from\nthe non-minimum phase property of a tail-controlled airframe.\nThe proposed reward function considers tradeoff between\ntracking error and ﬁn deﬂection rate as\nrt = −ka\n\u0012az −¯az,c\naz,max\n\u00132\n−kδ\n \n˙δ\n˙δmax\n!2\n(26)\nwhere ka and kδ are two positive constants that quantify\nthe weights of two different objectives; az,max and ˙δmax are\ntwo normalisation constants that enforce these two metrics\nin approximately the same scale. Note that the consideration\nof ﬁn deﬂection rate is to constrain the maximum rate of\nthe actuator to meet physical limits. The hyper-parameters in\nshaping the reward function are summarised in Table III.\nV. TRAINING A DDPG AUTOPILOT AGENT\nGenerally, training a DDPG agent involves three main steps:\n(1) obtaining training scenarios; (2) building the actor and\ncritic networks; and (3) tuning the hyper parameters.\nTABLE IV\nFLIGHT ENVELOP.\nParameter\nMinimum value\nMaximum value\nAngle-of-attack α\n−20◦\n20◦\nHeight h\n6000m\n14000m\nMach number M\n2\n4\nTABLE V\nNETWORK LAYER SIZE.\nLayer\nActor network\nCritic network\nInput layer\n3 (Size of observations)\n7 (Size of observations +\nSize of actions)\nHidden layer 1\n64\n64\nHidden layer 2\n64\n64\nOutput layer\n4 (Size of action)\n1 (Size of action-value\nfunction)\n1) Training scenarios: In this paper, we consider an air-\nframe with its ﬂight envelop deﬁned in Table IV. At the\nbeginning of each episode, we randomly initialise the system\nstates with values uniformly distributed between the minimum\nand the maximum values. This random initialisation allows\nthe agent to explore the diversity of the state space. For all\nepisodes, the vehicle is required to track a reference command\naz,c, which is deﬁned as a step command with its magnitude\nbeing 100m/s2.\n2) Network construction: Inspired by the original DDPG\nalgorithm [40], the actor and critic are represented by four-\nlayer fully-connected neural networks. Note that this four-layer\nnetwork architecture is commonly utilised in deep reinforce-\nment learning applications [48]. The layer sizes of these two\nnetworks are summarised in Table V. Except for the actor\noutput layer, each neuron in other layers is activated by a\nrectiﬁed linear units (Relu) function, which is deﬁned as\ng(z) =\n\u001a z,\nif z > 0\n0,\nif z < 0\n(27)\nwhich provides faster processing speed than other nonlinear\nactivation functions due to the linear relationship property.\nThe output layer of the actor network is activated by the\ntanh function, which is give by\ng(z) = ez −e−z\nez + e−z\n(28)\nThe beneﬁt of the utilisation of tanh activation function in\nactor network is that it can prevent the control input from\nsaturation as the actor output is constrained by (−1, 1). Since\ndifferent autopilot gains have different scales, the output layer\nof the actor network is scaled by a constant vector\n[KDC,max, KA,max, KI,max, Kg,max]T\n(29)\nwhere (·)max stands for the normalisation constant of variable\n(·) and the detailed values are presented in Table VI.\nAs different observations have different scales and units,\nwe normalise the observations at the input layers of the\nnetworks, thus providing unitless observations hat belong to\napproximately the same scale. This normalisation procedure is\nshown to be of paramount importance for our problems and\n8\nTABLE VI\nNORMALISATION CONSTANTS.\nαmax\nMmax hmax\nKDC,max KA,max KI,max\nKg,max\n20◦\n4\n14000\n3\n0.05\n100\n2\nhelps to increase the training efﬁciency. Without normalisa-\ntion, the average reward function cannot converge and even\nshows divergent patterns after some episodes. Denote ¯(·) as\nthe normalised version of variable (·). The normalisation of\nobservations is deﬁned as\n¯α =\nα\nαmax\n,\n¯\nM =\nM\nMmax\n,\n¯h =\nh\nhmax\n(30)\nwhere (·)max stands for the normalisation constant of variable\n(·) and the detailed values are presented in Table VI.\nBoth actor and critic networks are trained using Adam\noptimiser with L2 regularisation to address the over-ﬁtting\nproblem for stabilising the learning process. With L2 regu-\nlarisation, the updates of actor and critic are modiﬁed as\nLactor = J (Aµ) + λ2LA\n2\nµt+1 = µt + αµ∇µLactor\n(31)\nLcritic = L(w) + λ2LC\n2\nwt+1 = wt + αw∇wLcritic\n(32)\nwhere LA\n2 and LC\n2 denote the L2 regularisation losses on the\nweights of the actor and the critic, respectively; λ2 is the\nregularisation constant.\nTo increase the stability of the network training process, we\nutilise the gradient clip technique to constrain the update of\nboth actor and critic networks. More speciﬁcally, if the norm\nof the gradient exceeds a given upper bound ρ, the gradient\nis scaled to equal with ρ. This helps to prevent a numerical\noverﬂow or underﬂow during the training process.\n3) Hyper parameter tuning: Each episode during training\nis terminated when the number of time steps exceeds the\nmaximum permissible value. All hyper parameters that are\nutilised in DDPG training for our problem are summarised in\nTable VII. Notice that the tuning of hyper parameters imposes\ngreat effects on the performance of DDPG and this tuning\nprocess is not consistent across different ranges of applications\n[48], [49], i.e., different works utilised different set of hyper\nparameters for their own problems. For this reason, we tune\nthese hyper parameters for our autopilot design problem based\non several trial and error tests.\nVI. RESULTS\nA. Training Results\nIn order to demonstrate the importance of the utilisation of\nreference command and domain knowledge, we also carry out\nsimulations without using the shaped reference command and\ndomain knowledge, i.e., learning from scratch. Without the\nshaped reference command, the reward function becomes\nrt = −ka\n\u0012az −az,c\naz,max\n\u00132\n−kδ\n \n˙δ\n˙δmax\n!2\n(33)\nTABLE VII\nHYPER PARAMETER SETTINGS.\nParameter\nValue\nParameter\nValue\nMaximum\npermissible steps\n200\nSize of experience\nbuffer |D|\n5 × 105\nMaximum\npermissible episodes\n1000\nSize of mini-batch\nsamples N\n64\nActor learning rate\nαµ\n10−3\nMean of exploration\nnoise µv\n0\nCritic learning rate\nαw\n10−3\nInitial variance of\nexploration noise Σ1\n0.1\nL2 regularisation\nconstant λ2\n6 × 10−3\nVariance decay rate ϵ\n10−6\nGradient upper\nbound ρ\n1\nMean attraction\nconstant βattract\n0.15\nDiscounting factor γ\n0.99\nTarget network\nsmoother constant τ\n0.001\nSampling time Ts\n0.01s\nThe learning curves of the training process that leverages\ndomain knowledge with random initial conditions are shown in\nthe ﬁrst row of Fig. 5, where Fig. 5 (a) is the result with shaped\nreference command and Fig. 5 (b) stands for the result without\nshaped reference command. The average reward is obtained\nby averaging the episode reward within 30 episodes. From\nFigs. 5 (a) and (b), it can be clearly noted that the average\nreward of the proposed DDPG autopilot agent converges to\nits steady-state value within 400 episodes, even with random\ninitial conditions. Also, the utilisation of shaped reference\ncommand provides relatively faster convergence speed and\nsmoother steady-state performance. To demonstrate this fact,\nFig. 5 (c) provides the learning curves of learning from scratch\nfor one ﬁxed set point. From this ﬁgure, it is clear that the\nreward convergence speed of learning from scratch for one\nﬁxed set point is slower than that of the proposed approach for\nall random initial conditions in the ﬂight envelope. This fact\nreveals that the utilisation of domain knowledge signiﬁcantly\nimproves the learning efﬁciency. The reason is that the agent\nhas already gained some experience by using the domain\nknowledge.\nTo investigate the importance of observation and action\nnormalisation in training DDPG autopilot agent, we perform\nsimulations with and without normalisation. Fig. 6 presents\nthe comparison results of average reward convergence. From\nthis ﬁgure, it is clear that utilising normalisation provides fast\nconvergence rate of the learning process and higher steady-\nstate value of the average reward function: the average reward\nwithout normalisation is not converged within 1,000 episodes,\nwhereas the one with normalisation converges withing around\n200 episodes. This means that leveraging observation and ac-\ntion normalisation helps to achieve more efﬁcient and effective\ntraining process. The reason can be attributed to the fact that\nnormalisation imposes equally importance on each element\nof the observation vector. Without normalisation, the scale\ndifference between the elements varies in a great deal, e.g.,\nthe magnitude of height is much lager than that of angle-of-\nattack, and therefore prohibits effective training of the actor\nand critic networks.\n9\n0\n200\n400\n600\n800\n1000\n-1500\n-1000\n-500\n0\n(a) With both domain knowledge and shaped refer-\nence command\n0\n200\n400\n600\n800\n1000\n1200\n-3000\n-2500\n-2000\n-1500\n-1000\n-500\n0\n(b) With only domain knowledge\n200\n400\n600\n800\n1000\n1200\n-3000\n-2500\n-2000\n-1500\n-1000\n-500\n0\n(c) Learning from scratch for one ﬁxed set point\nFig. 5. Comparisons of learning curves.\n0\n200\n400\n600\n800\n1000\n-2000\n-1500\n-1000\n-500\n0\nFig. 6. Learning process comparison with respect to normalisation.\nB. Test Results\nTo test the proposed DDPG three-loop autopilot under\nvarious conditions, the trained agent is applied to some ran-\ndom scenarios and compared with classical gain scheduling\napproach. Note that the trained agent by learning from scratch\nis incapable to track reference signals that are different from\nthe training process in our test. On the other hand, the trained\nagent by leveraging domain knowledge can track reference\nsignals which differ from those in the training process. There-\nfore, we only compare the performance of the proposed\nalgorithm with classical gain-scheduling and training with\ndomain knowledge, but without shaped reference command.\nThe testing DDPG agent is chosen from the one that generates\nthe largest episode reward during the training process, i.e., -\n0.5527 (with the shaped reference command ) and -18.1761\n(without the shaped reference command).\nThe comparison results, including acceleration response,\nangle-of-attack history, Mach number proﬁle and ﬁn deﬂection\nangle, are presented in Figs. 7-10. From these ﬁgures, it can\nbe observed that both the classical gain-scheduling and the\nproposed DDPG autopilots can somehow track the reference\nsignal. Note that although the gains given a set point are\ndesigned to meet the typical autopilot design criteria for the\ngain scheduling, the set points are not optimally selected\n. Hence, response of the gain scheduling could be further\nimproved by tuning the gains at more set points with more\naccurate model information. However, this is against with our\narguments in Introduction and Section IV. As a comparison,\nthe agent that learned from data without using the shaped ref-\nerence command provides poor transient performance, i.e., big\nundershoot at the beginning, and generates large performance\nvariations even for the steady-state performance. Although the\nacceleration response under the proposed algorithm has larger\novershoot than classical gain scheduling approach, the pro-\nposed autopilot shows more stable response with less response\noscillations and smaller undershoot in all set points tested,\nas shown in Figs. 7. The test results reassure that leveraging\ndomain knowledge can improve the learning effectiveness and\ngeneralisation.\nFig. 10 shows that gain scheduling autopilot requires faster\nactuator response and larger maximum ﬁn deﬂection angle.\nThis means that the proposed algorithm requires less actu-\nator resource in tracking the reference command than gain\nscheduling approach. Another characteristic of the proposed\nautopilot is that it provides a direct nonlinear mapping from\nthe scheduling variables to controller gains and therefore does\nnot require a look-up table for real implementation. Apart\nfrom the improved learning effectiveness and generalisation,\nthe numerical demonstration results suggest that the proposed\nDDPG three-loop autopilot provides comparable results with\nseveral advantages , compare to the classical gain scheduling\napproach, and could be a potential solution for three-loop\nautopilot design.\nC. Robustness Against Model Uncertainty\nThe aerodynamic model of a tail-controlled airframe might\nexperience inevitable uncertainties due to the change of centre\nof pressure or other environmental parameters. Moreover,\nthe data used in training could be different from reality.\nTherefore, it is critical to investigate the robustness of the\nlearning-based autopilot. To investigate the robustness of the\nproposed DRL-based acceleration autopilot, we perform nu-\nmerical simulations with uncertain models in this subsection.\n10\n0\n1\n2\n3\n4\n5\n6\n-40\n-20\n0\n20\n40\n60\n80\n100\n120\n(a) Gain-scheduling\n(b) DDPG without shaped reference command\n0\n1\n2\n3\n4\n5\n6\n-20\n0\n20\n40\n60\n80\n100\n120\n(c) DDPG with shaped reference command\nFig. 7. Comparison results of acceleration response.\n0\n1\n2\n3\n4\n5\n6\n-16\n-14\n-12\n-10\n-8\n-6\n-4\n-2\n0\n2\n(a) Gain-scheduling\n0\n1\n2\n3\n4\n5\n6\n-20\n-15\n-10\n-5\n0\n(b) DDPG without shaped reference command\n0\n1\n2\n3\n4\n5\n6\n-20\n-15\n-10\n-5\n0\n(c) DDPG with shaped reference command\nFig. 8. Comparison results of angle-of-attack.\n0\n1\n2\n3\n4\n5\n6\n3.1\n3.2\n3.3\n3.4\n3.5\n3.6\n3.7\n3.8\n3.9\n(a) Gain-scheduling\n0\n1\n2\n3\n4\n5\n6\n3.1\n3.2\n3.3\n3.4\n3.5\n3.6\n3.7\n3.8\n3.9\n(b) DDPG without shaped reference command\n0\n1\n2\n3\n4\n5\n6\n2.6\n2.7\n2.8\n2.9\n3\n3.1\n3.2\n3.3\n(c) DDPG with shaped reference command\nFig. 9. Comparison results of Mach number.\nThe aerodynamic coefﬁcients, detailed in Table II, are assumed\nto have random −40% to +40% uncertainty. To better show\nthe robustness of the proposed control algorithm, these model\nuncertainties are not included in the training scenarios. The\nsimulation results, including acceleration response, angle-of-\nattack, mach number and ﬁn deﬂection angle, are presented\nin Fig. 11. From this ﬁgure, it can be clearly noted that the\nproposed DRL-based control algorithm provides satisfactory\nperformance in the presence of aerodynamic model uncertain-\nties: the model uncertainties only inﬂuence the transient effect\nand the steady-state tracking error remains at the same level as\nthe case without aerodynamic uncertainty. These results clearly\nreveal that the proposed control approach has strong robustness\nagainst model uncertainties.\nD. Relative Stability\nThe relative stability is generally applied to examine the\nrobustness of the control system within the linear control\ntheory. However, since the proposed DRL autopilot is non-\nlinear and utilises the deep neural networks, it is hard to\nanalyse the stability using classical linear control theory. To\nthis end, this paper suggests to investigate relative stability\nof the proposed autopilot based on their physical concepts:\n(1) the gain margin determines the maximum allowable gain\nincrease while maintaining the control loop stability; (2) the\nphase margin denotes the maximum allowable time delay in\n11\n0\n1\n2\n3\n4\n5\n6\n-10\n-5\n0\n5\n10\n15\n20\n(a) Gain-scheduling\n0\n1\n2\n3\n4\n5\n6\n-10\n-5\n0\n5\n10\n15\n20\n25\n(b) DDPG without shaped reference command\n0\n1\n2\n3\n4\n5\n6\n0\n5\n10\n15\n20\n25\n(c) DDPG with shaped reference command\nFig. 10. Comparison results of ﬁn deﬂection angle.\n0\n1\n2\n3\n4\n5\n6\n-20\n0\n20\n40\n60\n80\n100\n120\n(a) Acceleration response\n0\n1\n2\n3\n4\n5\n6\n-30\n-25\n-20\n-15\n-10\n-5\n0\n5\n(b) Angle-of-attack\n0\n1\n2\n3\n4\n5\n6\n2.6\n2.7\n2.8\n2.9\n3\n3.1\n3.2\n3.3\n(c) Mach number\n0\n1\n2\n3\n4\n5\n6\n0\n5\n10\n15\n20\n25\n(d) Fin deﬂection angle\nFig. 11. Simulation results of the proposed autopilot with model uncertainties.\naz,c\nk\naz\nInput gain\nAutopilot\nSensor\nActuator\nAirframe\nFig. 12. Method to determine gain margin.\nthe control loop to guarantee stability. With these concepts\nin mind, the gain margin can by numerically computed as\naz,c\ne−∆ts\naz\nTime delay\nSensor\nAirframe\nActuator\nAutopilot\nFig. 13. Method to determine phase margin.\nfollows:\n1) Put an input gain k before the actuator, as shown in\n12\nFig. 12, and ﬁnd the maximum value of k that results\nin instability of the control loop.\n2) The gain margin can then be determined as\nGM = 20 log(k)\n(dB)\n(34)\nThe phase margin can be also numerically determined as:\n1) Put a time delay ∆t before the actuator, as shown in\nFig. 13, and ﬁnd the maximum value of ∆t that results\nin instability of the control loop.\n2) Compute the gain crossover frequency f from the ob-\ntained time response (the frequency can be directly read\nfrom the oscillation response).\n3) The phase margin can then be determined as\nPM = 360f∆t\n(deg)\n(35)\nWe investigated relative stability in various set points and\ntheir the results are similar. Hence, we select three different\nheights: 6km, 8km and 10km to demonstrate the results of\nrelative stability examination. Notice that when performing\nrelative stability analysis, the Mach number is time-varying.\nThe numerical tests of gain margin and phase margin are\npresented Figs. 14 and 15. From the results, we can readily\nobtain the result that the proposed autopilot satisﬁes typical\ndesign criteria:\nGM > 6dB,\nPM > 45◦\nVII. CONCLUSION\nWe have developed a computational acceleration autopilot\nalgorithm for a tail-controlled air vehicle using deep RL\ntechniques. The domain knowledge is utilised to help increase\nthe learning efﬁciency during the training process. The state-\nof-the-art DDPG approach is leveraged to train a RL agent\nwith a deterministic action policy that maximises the expected\ntotal reward. Extensive numerical simulations validate the\neffectiveness of the proposed approach. Future work includes\nextending the proposed autopilot to other types of vehicles.\nValidating the proposed computational autopilot algorithm\nunder uncertain environment is also an important issue and\nrequires further explorations.\nREFERENCES\n[1] P. Zarchan, Tactical and strategic missile guidance.\nAmerican Institute\nof Aeronautics and Astronautics, 2012.\n[2] D. J. Stilwell, “State-space interpolation for a gain-scheduled autopilot,”\nJournal of Guidance, Control, and Dynamics, vol. 24, no. 3, pp. 460–\n465, 2001.\n[3] D. J. Stilwell and W. J. Rugh, “Interpolation of observer state feed-\nback controllers for gain scheduling,” IEEE Transactions on Automatic\nControl, vol. 44, no. 6, pp. 1225–1229, 1999.\n[4] S. Theodoulis and G. Duc, “Missile autopilot design: gain-scheduling\nand the gap metric,” Journal of Guidance, Control, and Dynamics,\nvol. 32, no. 3, pp. 986–996, 2009.\n[5] H. Lhachemi, D. Saussi´e, and G. Zhu, “Gain-scheduling control design\nin the presence of hidden coupling terms,” Journal of Guidance, Control,\nand Dynamics, pp. 1872–1880, 2016.\n[6] A. Thukral and M. Innocenti, “A sliding mode missile pitch autopilot\nsynthesis for high angle of attack maneuvering,” IEEE Transactions on\nControl Systems Technology, vol. 6, no. 3, pp. 359–371, 1998.\n[7] I. Shkolnikov, Y. Shtessel, D. Lianos, and A. Thies, “Robust missile\nautopilot design via high-order sliding mode control,” in AIAA Guidance,\nnavigation, and control Conference and Exhibit, 2000.\n[8] G. Mattei and S. Monaco, “Nonlinear autopilot design for an asym-\nmetric missile using robust backstepping control,” Journal of Guidance,\nControl, and Dynamics, vol. 37, no. 5, pp. 1462–1476, 2014.\n[9] A. J. Calise, M. Sharma, and J. E. Corban, “Adaptive autopilot design\nfor guided munitions,” Journal of Guidance, Control, and Dynamics,\nvol. 23, no. 5, pp. 837–843, 2000.\n[10] J. Wang, C. Cao, N. Hovakimyan, R. Hindman, and D. B. Ridgely, “L1\nadaptive controller for a missile longitudinal autopilot design,” in AIAA\nGuidance, Navigation and Control Conference and Exhibit, 2008, p.\n6282.\n[11] C. Mracek, J. Cloutier, J. Cloutier, and C. Mracek, “Full envelope missile\nlongitudinal autopilot design using the state-dependent riccati equation\nmethod,” in Guidance, Navigation, and Control Conference, 1997.\n[12] C. P. Mracek, “SDRE autopilot for dual controlled missiles,” IFAC\nProceedings Volumes, vol. 40, no. 7, pp. 750–755, 2007.\n[13] H. Buschek, “Design and ﬂight test of a robust autopilot for the iris-\nt air-to-air missile,” Control Engineering Practice, vol. 11, no. 5, pp.\n551–558, 2003.\n[14] J.-H. Kim and I. H. Whang, “Augmented three-loop autopilot structure\nbased on mixed-sensitivity H∞optimization,” Journal of Guidance,\nControl, and Dynamics, vol. 41, no. 3, pp. 751–756, 2017.\n[15] C.-H. Lee, B.-E. Jun, and J.-I. Lee, “Connections between linear\nand nonlinear missile autopilots via three-loop topology,” Journal of\nGuidance, Control, and Dynamics, vol. 39, no. 6, pp. 1426–1432, 2016.\n[16] P. Lu, “Introducing computational guidance and control,” Journal of\nGuidance, Control, and Dynamics, vol. 40, no. 2, pp. 193–193, 2017.\n[17] W.-Q. Tang and Y.-L. Cai, “Predictive functional control-based missile\nautopilot design,” Journal of Guidance, Control, and Dynamics, vol. 35,\nno. 5, pp. 1450–1455, 2012.\n[18] V. Bachtiar, T. M¨uhlpfordt, W. Moase, T. Faulwasser, R. Findeisen, and\nC. Manzie, “Nonlinear model predictive missile control with a stabilising\nterminal constraint,” IFAC Proceedings Volumes, vol. 47, no. 3, pp. 457–\n462, 2014.\n[19] V. Bachtiar, C. Manzie, and E. C. Kerrigan, “Nonlinear model-predictive\nintegrated missile control and its multiobjective tuning,” Journal of\nGuidance, Control, and Dynamics, vol. 40, no. 11, pp. 2961–2970, 2017.\n[20] K. Krishnakumar and D. E. Goldberg, “Control system optimization\nusing genetic algorithms,” Journal of Guidance, Control, and Dynamics,\nvol. 15, no. 3, pp. 735–740, 1992.\n[21] B. Karimi, I. Saboori, and M. Lotﬁ-Forushani, “Multivariable con-\ntroller design for aircraft longitudinal autopilot based on particle swarm\noptimization algorithm,” in 2011 IEEE International Conference on\nComputational Intelligence for Measurement Systems and Applications\n(CIMSA) Proceedings.\nIEEE, 2011, pp. 1–6.\n[22] D. Wang, H. He, and D. Liu, “Adaptive critic nonlinear robust control:\nA survey,” IEEE Transactions on Cybernetics, vol. 47, no. 10, pp. 3429–\n3451, 2017.\n[23] Y. Li, Y. Wen, K. Guan, and D. Tao, “Transforming cooling optimization\nfor green data center via deep reinforcement learning,” IEEE Transac-\ntions on Cybernetics, 2019.\n[24] T. Tan, F. Bao, Y. Deng, A. Jin, Q. Dai, and J. Wang, “Cooperative deep\nreinforcement learning for large-scale trafﬁc grid signal control,” IEEE\nTransactions on Cybernetics, 2019.\n[25] W. Koch, R. Mancuso, R. West, and A. Bestavros, “Reinforcement\nlearning for uav attitude control,” ACM Transactions on Cyber-Physical\nSystems, vol. 3, no. 2, pp. 1–21, Feb 2019. [Online]. Available:\nhttp://dx.doi.org/10.1145/3301273\n[26] Z. Huang, X. Xu, H. He, J. Tan, and Z. Sun, “Parameterized batch\nreinforcement learning for longitudinal control of autonomous land ve-\nhicles,” IEEE Transactions on Systems, Man, and Cybernetics: Systems,\nvol. 49, no. 4, pp. 730–741, 2017.\n[27] L. Ding, S. Li, H. Gao, C. Chen, and Z. Deng, “Adaptive partial rein-\nforcement learning neural network-based tracking control for wheeled\nmobile robotic systems,” IEEE Transactions on Systems, Man, and\nCybernetics: Systems, 2018.\n[28] R. Cui, C. Yang, Y. Li, and S. Sharma, “Adaptive neural network control\nof auvs with control input nonlinearities using reinforcement learning,”\nIEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. 47,\nno. 6, pp. 1019–1029, 2017.\n[29] S. Ferrari and R. F. Stengel, “Online adaptive critic ﬂight control,”\nJournal of Guidance, Control, and Dynamics, vol. 27, no. 5, pp. 777–\n786, 2004.\n[30] R. Enns and J. Si, “Helicopter trimming and tracking control using direct\nneural dynamic programming,” IEEE Transactions on Neural networks,\nvol. 14, no. 4, pp. 929–939, 2003.\n13\n0\n1\n2\n3\n4\n5\n6\n-20\n0\n20\n40\n60\n80\n100\n120\n(a) h = 6km\n0\n1\n2\n3\n4\n5\n6\n-20\n0\n20\n40\n60\n80\n100\n120\n140\n(b) h = 8km\n0\n1\n2\n3\n4\n5\n6\n-20\n0\n20\n40\n60\n80\n100\n120\n140\n(c) h = 10km\nFig. 14. Relative stability analysis with gain margin.\n0\n1\n2\n3\n4\n5\n6\n-20\n0\n20\n40\n60\n80\n100\n120\n140\n(a) h = 6km\n0\n1\n2\n3\n4\n5\n6\n-20\n0\n20\n40\n60\n80\n100\n120\n140\n(b) h = 8km\n0\n1\n2\n3\n4\n5\n6\n-20\n0\n20\n40\n60\n80\n100\n120\n140\n(c) h = 10km\nFig. 15. Relative stability analysis with phase margin.\n[31] Y. Zhou, E.-J. van Kampen, and Q. P. Chu, “Incremental model based\nonline dual heuristic programming for nonlinear adaptive control,”\nControl Engineering Practice, vol. 73, pp. 13–25, 2018.\n[32] Y. Wang, J. Sun, H. He, and C. Sun, “Deterministic policy gradient with\nintegral compensator for robust quadrotor control,” IEEE Transactions\non Systems, Man, and Cybernetics: Systems, 2019.\n[33] D. Xu, Z. Hui, Y. Liu, and G. Chen, “Morphing control of a new bionic\nmorphing uav with deep reinforcement learning,” Aerospace Science and\nTechnology, 2019.\n[34] H. Wu, S. Song, K. You, and C. Wu, “Depth control of model-free auvs\nvia reinforcement learning,” IEEE Transactions on Systems, Man, and\nCybernetics: Systems, vol. 49, no. 12, pp. 2499–2510, 2018.\n[35] D. Shapiro, P. Langley, and R. Shachter, “Using background knowledge\nto speed reinforcement learning in physical agents,” in Proceedings of\nthe ﬁfth international conference on Autonomous agents, 2001, pp. 254–\n261.\n[36] T. Chen, “Deep reinforcement learning with prior knowledge,” Master’s\nthesis, Pittsburgh, PA, May 2019.\n[37] C¸ . G¨ulc¸ehre and Y. Bengio, “Knowledge matters: Importance of prior\ninformation for optimization,” The Journal of Machine Learning Re-\nsearch, vol. 17, no. 1, pp. 226–257, 2016.\n[38] D. Ferranti, D. Krane, and D. Craft, “The value of prior knowledge in\nmachine learning of complex network systems,” Bioinformatics, vol. 33,\nno. 22, pp. 3610–3618, 2017.\n[39] D. L. Moreno, C. V. Regueiro, R. Iglesias, and S. Barro, “Using prior\nknowledge to improve reinforcement learning in mobile robotics,” Proc.\nTowards Autonomous Robotics Systems. Univ. of Essex, UK, 2004.\n[40] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,\nD. Silver, and D. Wierstra, “Continuous control with deep reinforcement\nlearning,” arXiv preprint arXiv:1509.02971, 2015.\n[41] T. Yucelen and W. M. Haddad, “Low-frequency learning and fast\nadaptation in model reference adaptive control,” IEEE Transactions on\nAutomatic Control, vol. 58, no. 4, pp. 1080–1085, 2012.\n[42] J. E. Gaudio, T. E. Gibson, A. M. Annaswamy, M. A. Bolender, and\nE. Lavretsky, “Connections between adaptive control and optimization\nin machine learning,” arXiv preprint arXiv:1904.05856, 2019.\n[43] P. Apkarian and D. Noll, “Nonsmooth H∞synthesis,” IEEE Transac-\ntions on Automatic Control, vol. 51, no. 1, pp. 71–86, 2006.\n[44] P. Apkarian, M. N. Dao, and D. Noll, “Parametric robust structured\ncontrol design,” IEEE Transactions on Automatic Control, vol. 60, no. 7,\npp. 1857–1869, 2015.\n[45] N. Stein, H. Weiss, G. Hexner, and I. Rusnak, “Effect of missile conﬁg-\nuration and inertial measurement unit location on autopilot response,”\nJournal of Guidance, Control, and Dynamics, pp. 2740–2745, 2016.\n[46] C. Mracek and D. Ridgely, “Missile longitudinal autopilots: comparison\nof multiple three loop topologies,” in AIAA guidance, navigation, and\ncontrol conference and exhibit, 2005.\n[47] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel, “Bench-\nmarking deep reinforcement learning for continuous control,” in Inter-\nnational Conference on Machine Learning, 2016, pp. 1329–1338.\n[48] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger,\n“Deep reinforcement learning that matters,” in Thirty-Second AAAI\nConference on Artiﬁcial Intelligence, 2018.\n[49] R. Islam, P. Henderson, M. Gomrokchi, and D. Precup, “Reproducibil-\nity of benchmarked deep reinforcement learning tasks for continuous\ncontrol,” arXiv preprint arXiv:1708.04133, 2017.\nHyo-Sang Shin received his BSc on aerospace\nengineering from Pusan National University in 2004\nand gained an MSc on ﬂight dynamics, guidance\nand control in Aerospace Engineering from KAIST\nand a PhD on cooperative missile guidance from\nCranﬁeld University in 2006 and 2010, respectively.\nHe is currently a Professor of Guidance, Control and\nNavigation Systems and Head of Autonomous and\nIntelligent Systems Group at Cranﬁeld University.\nHis current research interests include multiple tar-\nget tracking, adaptive and sensor-based control, and\ndistributed control of multiple agent systems.\n14\nShaoming He received his BSc and MSc degrees\nin Aerospace Engineering from Beijing Institute of\nTechnology, in 2013 and 2016, respectively, and a\nPhD in Aerospace Cranﬁeld University. He is cur-\nrently an associate professor in School of Aerospace\nEngineering at Beijing Institute of Technology. His\nresearch interests include multi-target tracking, UAV\nguidance and trajectory optimization.\nAntonios Tsourdos obtained a MEng in electronic,\ncontrol and systems engineering from the University\nof Shefﬁeld (1995), an MSc in systems engineering\nfrom Cardiff University (1996), and a PhD in non-\nlinear robust missile autopilot design and analysis\nfrom Cranﬁeld University (1999). He is a Professor\nof Control Engineering with Cranﬁeld University,\nand was appointed Head of the Centre for Cyber-\nPhysical Systems in 2013. He was a member of the\nTeam Stellar, the winning team for the UK MoD\nGrand Challenge (2008) and the IET Innovation\nAward (Category Team, 2009).\n",
  "categories": [
    "cs.AI",
    "cs.LG",
    "cs.SY",
    "eess.SY"
  ],
  "published": "2019-08-19",
  "updated": "2020-11-11"
}