{
  "id": "http://arxiv.org/abs/1908.08843v2",
  "title": "Fairness in Deep Learning: A Computational Perspective",
  "authors": [
    "Mengnan Du",
    "Fan Yang",
    "Na Zou",
    "Xia Hu"
  ],
  "abstract": "Deep learning is increasingly being used in high-stake decision making\napplications that affect individual lives. However, deep learning models might\nexhibit algorithmic discrimination behaviors with respect to protected groups,\npotentially posing negative impacts on individuals and society. Therefore,\nfairness in deep learning has attracted tremendous attention recently. We\nprovide a review covering recent progresses to tackle algorithmic fairness\nproblems of deep learning from the computational perspective. Specifically, we\nshow that interpretability can serve as a useful ingredient to diagnose the\nreasons that lead to algorithmic discrimination. We also discuss fairness\nmitigation approaches categorized according to three stages of deep learning\nlife-cycle, aiming to push forward the area of fairness in deep learning and\nbuild genuinely fair and reliable deep learning systems.",
  "text": "1\nFairness in Deep Learning:\nA Computational Perspective\nMengnan Du, Fan Yang, Na Zou, Xia Hu\nAbstract—Deep learning is increasingly being used in high-stake decision making applications that affect individual lives. However,\ndeep learning models might exhibit algorithmic discrimination behaviors with respect to protected groups, potentially posing negative\nimpacts on individuals and society. Therefore, fairness in deep learning has attracted tremendous attention recently. We provide a\nreview covering recent progresses to tackle algorithmic fairness problems of deep learning from the computational perspective.\nSpeciﬁcally, we show that interpretability can serve as a useful ingredient to diagnose the reasons that lead to algorithmic\ndiscrimination. We also discuss fairness mitigation approaches categorized according to three stages of deep learning life-cycle,\naiming to push forward the area of fairness in deep learning and build genuinely fair and reliable deep learning systems.\nIndex Terms—Deep Learning, DNN, Fairness, Bias, Interpretability.\n!\n1\nINTRODUCTION\nM\nACHINE learning algorithms have achieved dramatic\nprogress nowadays, and are increasingly being de-\nployed in high-stake applications, including employment,\ncriminal justice, personalized medicine, etc [1]. Neverthe-\nless, fairness in machine learning remains a problem. Machine\nlearning algorithms have the risk of amplifying societal\nstereotypes by over associating protected attributes, e.g.,\nrace and gender, with the prediction task [2]. Eventually\nthey are capable of exhibiting discriminatory behaviors\nagainst certain subgroups. For example, a recruiting tool\nfor STEM jobs believes that men are more qualiﬁed and\nshows bias against women [3], facial recognition performs\nextremely poorly for female with darker skin [4], recognition\naccuracy is very low for subgroup of people in pedestrian\ndetection of self-driving cars [2]. The fairness problem might\ncause adverse impacts on individuals and society. It not only\nlimits a person’s opportunity that s/he is qualiﬁed, but also\nmight further exacerbates social inequity.\nAmong different machine learning models, the fairness\nproblem of deep learning models has attracted attention from\nacademia and industry recently. First, deep learning models\nhave achieved the state-of-the-art performance in many\ndomains. Their success can partially be attributed to the\ndata-driven learning paradigm, which enables the models\nto learn useful representations automatically from data.\nThe data might contain human biases, which reﬂect his-\ntorical prejudices against certain social groups and existing\ndemographic inequalities. The data-driven learning also\ninevitably causes deep learning models to replicate and\neven amplify biases present in data. Second, it remains a\nchallenge to diagnose and address the deep learning fair-\nness problem. Deep learning models are generally regarded\nas black-boxes, and their intermediate representations are\n•\nMengnan Du, Fan Yang, Xia Hu are with the Department of Computer\nScience and Engineering, Texas A&M University.\n•\nNa Zou is with the Department of Industrial and Systems Engineering,\nTexas A&M University.\n•\nE-mail: {dumengnan,nacoyang,nzou1,xiahu}@tamu.edu\nopaque and hard to comprehend. This is problematic and\nmakes it difﬁcult to identify whether these models make\ndecisions based on right and justiﬁed reasons, or due to\nbiases. In addition, this makes it challenging to design bias\ndetection and mitigation approaches.\nIn this article, we summarize fairness in deep learning\nwork from the computational perspective, and do not dis-\ncuss work from social science, law and many other disci-\nplines [1]. Particularly we show that interpretability could\nsigniﬁcantly contribute to better understandings of the rea-\nsons that affect fairness. We also review fairness mitigation\nstrategies categorized into three stages of deep learning\nlife-cycle. Finally, we propose open challenges and future\nresearch directions. Throughout this article, we don’t differen-\ntiate between deep learning and DNN (Deep neural network)\nand use them interchangeably. Besides, we abstract from the\nexact DNN architectures, e.g., convolutional neural network\n(CNN), recurrent neural network (RNN), and multi-layer\nperceptron (MLP), and focus more on conceptual aspects\nwhich underlie the success of DNN bias detection and\nmitigation techniques.\n2\nDNN FAIRNESS\nIn this section, we introduce the categorization of fair-\nness problem, measurements of fairness, and interpretation\nmethods closely relevant to understanding DNN fairness.\n2.1\nFairness Problem Categorization\nFrom the computational perspective, DNN unfairness can\nbe generally categorized into two classes: prediction outcome\ndiscrimination, and prediction quality disparity.\n2.1.1\nPrediction Outcome Discrimination\nDiscrimination refers to the phenomenon that DNN models\nproduce unfavourable treatment of people due to the mem-\nbership of certain demographic groups [1]. For instance, a\nrecruiting tool believes that men are more qualiﬁed and\narXiv:1908.08843v2  [cs.LG]  19 Mar 2020\n2\nTABLE 1\nDNN fairness problem categorization and representative examples.\nClass\nRepresentative examples\nDiscrimination via Input\nEmployment: Recruiting tool believes that men are more qualiﬁed and shows bias\nagainst women.\nLoan Approval: Loan eligibility system negatively rates people belonging to certain\nZIP code, causing discrimination for certain races.\nCriminal Justice: Recidivism prediction system predicts black inmates are three times\nmore likely to be classiﬁed as ‘high risk’ than white inmates.\nDiscrimination via Representation\nMedical Image Diagnosis: CNN model could identify patients’ self-reported sex from a\nretina image, and shows discrimination based on gender.\nCredit Scoring: Using raw texts as input, demographic information of authors is encoded\nin the intermediate representations DNN-based credit scoring classiﬁers.\nPrediction Quality Disparity\nFacial Recognition: Facial recognition performs very poorly for female with darker skin.\nLanguage processing: Language identiﬁcation models perform signiﬁcantly worse when\nprocessing text produced by people belonging to certain races.\nReadmission: ICU mortality and psychiatric 30-day readmission model prediction accuracy\nis signiﬁcantly different across gender and insurance types.\nshows bias against women, and loan eligibility system\nnegatively rates African Americans. Current DNNs gener-\nally follow the purely data-driven and end-to-end learning\nparadigm, which are trained with labeled data. The model\ntraining pipeline is illustrated in Fig. 1(a). Any training data\nmay contain some biases, either intrinsic noise or additional\nsignals inadvertently introduced by human annotators [5].\nDNNs are designed to ﬁt these skewed training data, and\nthus would naturally replicate the biases existed in data.\nEven worse, DNNs not only rely on these biases to make\ndecisions, but also make unwanted implicit associations\nand amplify societal stereotypes about people [6], [7]. This\neventually results in trained models with algorithmic dis-\ncrimination. Outcome discrimination can be further split\ninto Input and Representation prospective. We present below\ndetailed descriptions for these two subcategories, with rep-\nresentative examples in Tab. 1.\nDiscrimination via Input\nPrediction outcome discrimina-\ntion could be traced back to the input. Even though a DNN\nmodel does not explicitly take protected attributes as input,\ne.g., race, gender and age, it may still induce prediction\ndiscrimination [8]. In the context of DNN systems, protected\nattributes are often not observed in the input data, mainly due\nto two reasons. Firstly, most DNN models rely on raw\ndata, e.g., text, as input and thus protected attributes are\nnot explicitly encoded in the input. Secondly, collecting\nprotected attributes such as race and ethnicity information\nis often not allowed by the law in real-world applications.\nDespite the absence of explicit protected attributes, DNNs\nstill could exhibit unintentional discrimination, since there\nare some features highly correlated with class membership.\nFor instance, ZIP code and surname could indicate race,\nmany words within text input could be used to infer gen-\nder [8]. The model prediction might highly depends on the\nclass memberships, and eventually shows discrimination to\ncertain demographic group.\nDiscrimination via Representation\nSometimes prediction\noutcome discrimination needs to be diagnosed and miti-\ngated from the representation prospective. In some cases,\nattributing the bias to input is nearly impossible, e.g., for\nimage input. For instance, CNN model could identify pa-\ntients’ self-reported sex from a retina image, while humans\neven ophthalmologists cannot identify cues from the input\nimage. Besides, in some scenarios, ﬁnding the sensitive\ninput attributes are challenging if the input dimension is\ntoo large [9]. In those settings, different demographic groups\nwould have distinct DNN intermediate representations. The\nclass memberships of different protected attributes could be\nencoded in deep representations. DNN model will make\ndecisions based on the implicitly learned membership in-\nformation and produce discriminate classiﬁcation outcomes.\nThus prediction outcome discrimination could be detected\nand removed from the deep representation perspective.\n2.1.2\nPrediction Quality Disparity\nPrediction quality difference of models for different pro-\ntected groups is another important category of unfairness.\nDNN systems have shown lower quality for some groups\nof people as opposed to other groups. Different from pre-\ndiction outcome discrimination which is about resources and\nopportunities allocations harm in high-stake applications such\nas hiring, loan and credit, this category is about quality of\nservices harm that usually happen in general applications,\ne.g., facial recognition and language processing (See Tab. 1).\nExamples include the language identiﬁcation systems per-\nform signiﬁcantly worse when processing text produced\nby people belonging to certain races [10], [11], health care\napplications including ICU mortality and psychiatric 30-\nday readmission model prediction accuracy is signiﬁcantly\ndifferent across gender and insurance types [12]. This is\nusually due to the underrepresentation problem, where data\nmay be less informative or less reliably collected for certain\nparts of the population. Take the Imagenet dataset (ILSVRC\n2012 subset with 1000 categories) as an example: females\ncomprise only 41.62% of images, people over 60 are almost\nnon-existent [13]. The typical objective of DNN training is\nto minimize the overall error. If the model cannot simulta-\nneously ﬁt all populations optimally, it will ﬁt the major-\nity group. Although this may maximize overall prediction\naccuracy, it comes at the expense of the under-represented\npopulations and leads to their poor performance.\n3\n(c) Global Interpretation\n(b) Local Interpretation\nInput\nInterpretation\nZIP code\n“Rejection of loan”\nHigh-level concept \ne.g., race\nLow-level concept \ne.g., eye color\nOne \nneuron\nMultiple\nneurons\nSurname\nInterpretation\nDetection and mitigation bias\nData collecting\nDNN training\nvia chosen metrics\nHuman bias\nData Labeling\nAmplifying bias\n(a) Interpretation for debias\nFig. 1. (a) Bias exists in different stages of the DNN training pipeline, and interpretation could be utilized to detect and mitigate bias. (b) DNN local\ninterpretation, (c) DNN global interpretation.\n2.2\nMeasurements of Fairness\nMany different metrics have been proposed to measure\nthe fairness of machine learning models. One line of work\nmeasures individual fairness, which follows the philosophy\nthat similar inputs should yield similar predictions [14].\nNevertheless, this leaves the open question of how to deﬁne\ninput similarity [1], [15]. Another line of work focus on\ngroup fairness, where examples are grouped according to\na particular sensitive attribute, and statistics about model\npredictions is calculated for each group and compared\nacross groups [15]. Comparing to individual fairness, group\nfairness is more widely adopted in fairness research, and\nthus is the focus of this article. Different kinds of group\nfairness measurements have been proposed, and we will\nintroduce below three mostly used ones.\nDemographic Parity\nIt asserts that average of algorith-\nmic decisions should be similar across different groups:\np(ˆy=1|z=0)\np(ˆy=1|z=1) ≥τ, where τ is a given threshold, usually set\nas 0.8 [16], ˆy is a model prediction, 1 denotes favorable\noutcome, z denotes protected attribute, e.g., race, gender.\nDemographic parity is independent of the ground truth\nlabels. This is useful especially when reliable ground truth\ninformation is not available, e.g., employment, credit, and\ncriminal justice [1].\nEquality of Opportunity\nThis metric has taken into\nconsideration that different groups could have differ-\nent distribution in terms of label y. It is deﬁned as:\np(ˆy = 1|z = 0, y = 1) −p(ˆy = 1|z = 1, y = 1), where y is\nthe ground truth label [17]. Essentially this is comparing\nthe true positive rate across different groups. A symmet-\nric measurement can be calculated for false positive rate:\np(ˆy = 1|z = 0, y = 0) −p(ˆy = 1|z = 1, y = 0). Putting them\ntogether will result the Equality of Odds metric [17].\nPredictive Quality Parity This metric measures prediction\nquality difference between different subgroups. The quality\ndenotes quantitative model performance in terms of model\npredictions and ground truth, and in this work we focus on\naccuracy measurement for multi-class classiﬁcation [4]. It is\ndesirable that a model has equal prediction accuracy across\ndifferent demographic subgroups.\nFor a more comprehensive discussion of measurements,\nwe refer interested readers to the work [1]. It is worth noting\nthat different applications require different measurements\nwhich satisfy their speciﬁc ethical and legal requirements.\n2.3\nInterpretability for Addressing Fairness Problem\nDNNs are often regarded as black-boxes and criticized\nby the lack of interpretability, since these models cannot\nprovide meaningful interpretation on how a certain pre-\ndiction is made. Interpretability could be utilized as an\neffective debugging tool to analyze the models, and enhance\nthe transparency and fairness of models (Fig. 1(a)). Inter-\npretability can generally be grouped into two categories:\nlocal interpretation and global interpretation [18].\nLocal Interpretation Local interpretation could illustrate\nhow the model arrives at a certain prediction for a spe-\nciﬁc input (Fig. 1(b)). It is achieved by attributing model’s\nprediction in terms of its input features. The ﬁnal inter-\npretation is illustrated in the format of feature importance\nvisualization [19], [20]. Take loan prediction for example.\nThe model input is a vector containing categorical features,\nand the interpretation result is a heat map(or attribution\nmap), where features with higher scores represent higher\nrelevance for the prediction.\nGlobal Interpretation The goal is to provide a global under-\nstanding about what knowledge has been captured by a pre-\ntrained DNN, and illuminate the learned representations in\nan intuitive manner to humans (Fig. 1(c)). The simplest way\nis to comprehend the concept captured by a single neuron,\nwhich is the representation derived from a speciﬁc channel\nat a speciﬁc layer [21]. The combination of multiple neurons\nof different channels or even different layers could represent\nmore abstract concepts [22]. Those protected concepts are\nusually based on multiple elementary low-level concepts.\nFor instance, race concept can be indicated via multiple local\nclues such as eye color and hair color. Thus comparing to\nconcepts learned by a single neuron, concepts yielded by a\ncombination of neurons are more relevant to fairness.\n3\nDETECTION OF MODELING BIAS\nIn this section, we present methods for detecting and un-\nderstanding algorithmic discrimination, by making use of\nDNN interpretability as an effective computational tool.\n4\nCNN\nf (xn)\nvactruth.com\nwww.ausiliumsrl.com\nDoctor-ness\nNot-woman  Woman\nConcept score for Doctor\nWas gender concept important to \nthis doctor image classifier?\ng(h (x))  \nPredict sensitive attribute z\nx\nh (x)\nc(h (x))  \nPredict label y\nNegative \ngradient\n(a) Bias Detection\n(b) Bias Mitigation\nFig. 2. (a) Global interpretation for detection of discrimination. Results show that this CNN has captured gender concept, and the not-woman\nconcept would signiﬁcantly increase doctor prediction conﬁdence of the CNN classiﬁer. Thus it indicates the CNN’s discrimination towards woman.\n(b) Adversarial training for mitigation of discrimination. The intuition is to enforce deep representation to maximally predict main task labels, while\nat the same time minimally predict sensitive attributes.\n3.1\nDiscrimination via Input\nThe source of prediction outcome discrimination could be\ntraced back to the input features. As discussed in Sec. 2.1.1,\nprotected attributes are often not observed in the input data.\nDue to the redundant encodings, other seemingly innocuous\nfeatures may be highly correlated with protected attribute\nand cause model bias [17]. The goal here is to locate these\nfeatures via local DNN interpretation.\nThe ﬁrst solution is performed in a top-down manner,\nwhere local interpretation is employed to generate feature\nimportance vector. After getting feature importance for all\ninput features, we can take out those with relatively high\nimportance scores and further analyze them. Among this\nsubset of features, the focus is to identify those fairness\nsensitive features (in contrast to task relevant features). Take\nthe loan application for example. If the features contributing\nmost to DNN prediction include surname and ZIP code of\napplicants, we can assert that this model has discrimination\ntowards certain race, and surname and ZIP code here are\nfairness sensitive features (Fig. 1(b)). The second solution\nis implemented in the bottom-up manner. Humans ﬁrst\npre-choose features which they are skeptical to be asso-\nciated with protected attributes, and then analyze feature\nimportance of the identiﬁed features [3]. These subset of\nfeatures are perturbed to generate new data samples, i.e.,\ncounterfactual(s) [23]. We then feed the counterfactual to\nthe DNN and observe the model prediction difference. If\nthe perturbation of those suspected fairness sensitive fea-\ntures causes signiﬁcant model prediction change, it can be\nasserted that the DNN has made biased decisions based\non protected attributes. Note that statistical differences are\ncalculated over a set of similar instances, so as to validate\nwhether the model has violated group fairness.\nA representative example is using local interpretation to\ndetect race bias in sentiment analysis systems [3]. Common\nAfrican American ﬁrst names (e.g., Malik) and European\nAmerican ﬁrst names (e.g., Ellen) are chosen as sensitive\nfeatures. The comparison is between average prediction\nscores of sentences with ﬁrst name of these two races. The\nresults show statistically signiﬁcant race bias, where DNNs\nconsistently yield higher sentiment prediction with African\nAmerican name on the tasks of anger, fear and sadness\nintensity prediction. The results indicate the models have vi-\nolated demographic parity metric, and reﬂect the stereotypes\nthat African Americans are relevant to negative emotions.\n3.2\nDiscrimination via Representation\nSometimes it is hard to identify bias from the input perspec-\ntive, and detecting model bias from the deep representations\nis more convenient. DNN global interpretation could be\nexploited as a debugging tool to analyze the deep represen-\ntations. The goal is to identify whether a protected attribute\nhas been captured by the intermediate representation, and\nthe degree to which this protected attribute contributes to\nthe model prediction. Thus a two-stage scheme could be\napplied to detect discrimination.\nFirstly,\nglobal\ninterpretation\nis\nutilized\nto\nanalyze\nwhether a DNN has learned a protected concept. This is\nusually achieved by pointing to a direction in the activation\nspace of DNN’s intermediate layers [22], [24], [25]. A typical\nexample is the concept activation vector (CAV) method [22].\nHere CAV deﬁnes a high-level concept using a set of exam-\nple inputs. For example, to deﬁne concept African American\nrace, a set of darker skin Congoid images could be used.\nThe CAV vector is the direction of activation values for\nthe set of examples corresponding to that concept. This\nvector is obtained by training a linear classiﬁer between\nthe concept examples and a set of random counterexam-\nples, where the vector is the direction orthogonal to the\ndecision boundary. Secondly, after conﬁrming that a DNN\nhas learned a protected concept, we proceed to test the con-\ntribution of this concept towards model’s ﬁnal prediction.\nDifferent strategies can be used to quantify the conceptual\nsensitivity, including the top-down manner which calculates\nderivative of DNN’s prediction to the concept vector [22],\nor the bottom-up manner which adds this concept vector to\ndifferent inputs’ intermediate activation and then observe\nthe change of model predictions. Ultimately the represen-\ntation bias level for a protected attribute is described using\na numerical score. The higher of the numerical sensitivity\nscore, the more signiﬁcantly that this concept contributes to\nDNN’s prediction.\nA representative example is detecting the gender bias\nin deep representations of a CNN doctorness classiﬁer\n(Fig. 2(a)). TCAV indicates that the model indeed has\ncaptured the gender concept. Besides, not-woman concept\n5\nwould dramatically increase the model’s prediction con-\nﬁdence of doctorness. The ﬁndings have conformed the\nCNN’s discrimination towards women. They also reﬂect the\ncommonly held gender stereotype that doctors are men.\n3.3\nPrediction Quality Disparity\nThere usually happens that some groups appear more fre-\nquently than others in training data. The DNN model will\noptimize for those groups in order to boost the overall\nmodel performance, leading to low prediction accuracy for\nthe minority group.\nThe detection of prediction quality disparity is typically\nperformed in a two-step manner: splitting data into sub-\ngroups according to sensitive attributes, and calculating\nthe accuracy for each demographic groups. For instance,\nfacial recognition systems are analyzed in terms of their\nprediction quality [4]. Human face images are classiﬁed\ninto four categories: darker skin males, darker skin females,\nlighter skin males, and lighter skin females. Three gender\nclassiﬁcation systems are evaluated for the four groups,\nand substantial accuracy disparities are observed. For all\nthree systems, the darker skin females group yields the\nhighest mis-classiﬁcation rate, with error rate up to 34.7%.\nIn contrast, the maximum error rate for lighter skin males\nis 0.8%. These results conform that the model has violated\npredictive quality parity metric and raise an urgent need for\nbuilding fair facial analysis systems.\nBeyond the veriﬁcation accuracy, model interpretability\ncould be used to analyze the reasons of discrimination.\nA decomposition-based local DNN interpretation method,\ni.e., class activation maps (CAM) [26], is used to investigate\nthe regions of interest attended by the DNN models when\nmaking decisions. CAM is utilized to analyze two groups:\nlighter skin and darker skin group [27]. The visualization\nshows that the model needs to focus on eye region for lighter\nskin group, while focus on the nose region and chin region\nfor darker skin group. It suggests different strategies are\nneeded to make decisions for different demographic groups.\nIf the training dataset has inadequate samples for darker\nskin group, the trained model may capture representation\npreference for the majority group and fail to learn effective\nclassiﬁcation strategy for minority darker skin group, thus\nleading to poor performance for minority group.\n4\nMITIGATION OF MODELING BIAS\nAfter presenting bias detection approaches, we introduce\nbelow methods which could mitigate against adverse biases.\nA typical and simpliﬁed deep learning pipeline could be\nsplit into three stages: dataset construction, model train-\ning, and inference. Mitigation methods could be corre-\nspondingly divided into three broad groups: pre-processing,\nin-processing, and post-processing [43] (see Tab. 2). Pre-\nprocessing tries to debias and increase the quality of train-\ning set. In-processing adds auxiliary regularization term to\nthe overall objective function during training, explicitly or\nimplicitly enforcing constraints for certain fairness metric.\nPost-processing is performed after model training to cali-\nbrate the predictions of trained models.\nNormal data\nRight \nprediction\nFeature-wise\nannotation\nFairness\nTrained\nDNN\nFig. 3. Using regularization for mitigation of discrimination via input.\nBesides normal training data and ground truth, feature-wise annotations\nare also needed, specifying which subset of features is fairness sensi-\ntive, and which subset is task relevant. The key idea is to enforce DNNs\nto depend less on sensitive features.\n4.1\nDiscrimination via Input\nIn this section, we introduce some representative mitigation\nmethods as well as their empirical evaluation.\n4.1.1\nPre-processing\nA straightforward solution is to remove those fairness sen-\nsitive features from training data. For instance, surname\nand ZIP code can be deleted to reduce the discrimination\nof DNNs towards certain race. A drawback of directly\nremoving features is that this might lead to poor model\nperformance and thus reduces model utility. We can replace\nthese fairness sensitive features with alternative values. Take\nthe sentence ‘The conversation with Malik was heartbreaking’\nfor example, we can replace ‘Malik’ with ‘IDENTITY’ to re-\nduce the possibility that DNN model shows discrimination\nbased on races. In addition, there are some data-agnostic\npre-processing transformation techniques. For instance, the\nweights of each training sample is given differently to\nensure fairness before model training [31]. Another trans-\nformation is formulated in a probabilistic framework, where\nfeatures and labels are edited to ensure group fairness [33].\n4.1.2\nIn-processing\nAn alternative approach to mitigate discrimination is via\nmodel regularization. The regularization implicitly or ex-\nplicitly optimizes a fairness metric.\nImplicit Regularization: The ﬁrst category adds implicit\nconstraints which disentangle the association between\nmodel prediction and fairness sensitive attributes (Fig. 3).\nIt enforces DNN models to pay more attention to correct\nfeatures relevant to prediction task, rather than capture spu-\nrious correlations between prediction task and protected at-\ntributes. Speciﬁcally, the model training is regularized with\nlocal DNN interpretation [28], [29], [44]. Beyond ground\ntruth y for the input x, the regularization also needs feature-\nwise annotations r, specifying whether each feature within\nthe input correlates with protected attributes or not. An-\nnotation r either could be labelled by domain experts or\nidentiﬁed through the detection methods in Sec. 3.1. For in-\nstance, the annotation r for input ‘The conversation with Malik\nwas heartbreaking’ is [0, 0, 0, 1, 0, 0], indicating that ‘Malik’ is\ncorrelated with race, while the rest words are considered as\ntask relevant. The overall loss function is denoted as:\nL(θ, x, y, r) = d1(y, ˆy)\n| {z }\nP rediction\n+ λ1d2(floc(x), r)\n|\n{z\n}\nF airness\n+\nλ2R(θ)\n| {z }\nRegularizer\n,\n(1)\n6\nTABLE 2\nRepresentative algorithms for mitigating unfairness in DNN models. Pre-processing, in-processing, and post-processing correspond to three\nstages of deep learning pipeline: dataset construction, model training, and model inference.\nClass\nPre-processing\nIn-processing\nPost-processing\nDiscrimination\nSensitive features removal\nAttribution regularization [28], [29]\nCalibrated distribution [7]\nvia Input\nSensitive features replacement\nReduction game [30]\nCalibrated equalized odds [17]\nReweighing [31]\nPrejudice remover [32]\nOptimized pre-processing [33]\nDiscrimination\nBalanced dataset collection\nAdversarial training [2], [34]\nTroubling neurons turn off\nvia Representation\nAdversarial fairness desideratum [35]\nSemantic constraints [36]\nDistance metrics [37], [38]\nPrediction Quality\nDiverse dataset collection [39]\nTransfer learning [40]\nDisparity\nSynthetic data generation [41]\nMulti-task learning [42]\nwhere d1 is normal classiﬁcation loss function, e.g., cross\nentropy loss, and R(θ) is a regularization term. Function\nfloc(x) is local interpretation method, and d2 is a dis-\ntance metric function. The three terms are used to guide\nthe DNN model to make right prediction, make decision\nbased on right and unbiased evidences, and not overﬁt to\ntraining set respectively. Hyperparameters λ1 and λ2 are\nused to balance three terms. Note that floc(x) needs to be\nend-to-end differentiable, amenable for training with back-\npropagation and updating DNN parameters. The resulting\nfair model depends more on holistic information which is\ntask relevant, while at the same time conditions less on\nsensitive attributes. Besides, the trained models also satisfy\nbetter demographic parity criteria.\nExplicit Regularization: This category adds explicit con-\nstraints through updating model’s loss function to minimize\nthe performance difference between different groups [30],\n[32]. They optimize the trade-off between accuracy and a\nspeciﬁc kind of fairness metric given training-time access to\nprotected attributes. A representative example combines de-\nmographic parity and equality of odds into overall objective\nfunction [30]. Speciﬁcally, it deﬁne a reduction that treats\nthe accuracy-fairness trade-off as a sequential game between\ntwo players. At each step in the gaming sequence, one\nplayer maximizes accuracy and the other player imposes a\nparticular amount of fairness. This method is advantageous\nin that it is model agnostic and could be applied to different\nDNN architectures.\n4.1.3\nPost-processing\nPost-processing calibration takes the model’s prediction and\nprotected attribute to calibrate model’s prediction during\nthe inference time [7], [17]. The goal is to enforce prediction\ndistribution to approach either the training distribution, or\na speciﬁc fairness metric. Firstly, corpus-level constraints\nare utilized to enforce the model prediction distribution\nto follow the training data distribution [7]. Secondly, the\ncalibration could also be performed towards a fairness met-\nric. For instance, one technique takes as input an existing\nclassiﬁer and the sensitive feature, and derives a monotone\ntransformation of the classiﬁer’s prediction to enforce the\nspeciﬁed equalized of odds constraint [17]. These methods\nallow for diverse fairness metrics and prove to be effective\nto reduce discrimination. On the other hand, these methods\ncould be problematic since they require inference-time ac-\ncess to protected attributes, which however usually are not\navailable during inference time in real-world applications.\n4.1.4\nEvaluation of Mitigation Algorithms\nWe conduct experiments to evaluate performance of differ-\nent mitigation algorithms. We use Adult Census Income 1\nand COMPAS 2 two datasets, containing 48,842 and 6,167\ninstances respectively. We use gender and race as protected\nattribute for the two datasets respectively. Each dataset is\nsplit into 50% for training, 20% for validation and 30% for\ntesting. The base DNN model is a multilayer perceptron\n(MLP) with 3 layers 3. We evaluate the metrics with the\nbest performing model on validation set. The results are\ndisplayed in Tab. 3. Note that we average each number over\nthree runs, to eliminate inﬂuence of random initialization to\nDNN performance. We have the following key observations.\nFirtly, without using the debiasing algorithms, DNN models\nwould amplify bias existing in training data, as shown by\nthe comparison of Parity value between DNN and training\nset. Secondly, there is fairness utility trade-off, where most\nmitigation algorithms could compromise overall model ac-\ncuracy. Thirdly, fairness measurements could be conﬂicting\nwith others. Some mitigation methods may be fair in terms\nof demographic parity, but may result in unfairness with\nregard to equality of opportunity/odds. Fourthly, mitiga-\ntion could possibly lead to discrimination towards majority\ngroups, where equality of opportunity/odds metrics switch\nfrom negative values to positive values.\n4.2\nDiscrimination via Representation\nThe goal is to reduce representation bias while at the same\ntime preserve useful prediction properties of DNNs.\n4.2.1\nPre-processing\nCollecting balanced dataset is a possible way is alleviate rep-\nresentation bias, since prediction discrimination is partially\ncaused by difference of label distribution conditioning on\nprotected features in the training data. Take text dataset for\nexample, gender swapping can be used to create a dataset\nwhich is identical to the original one but biased towards\n1. https://archive.ics.uci.edu/ml/datasets/Adult\n2. https://github.com/propublica/compas-analysis\n3. https://scikit-learn.org/stable/modules/generated/sklearn.\nneural network.MLPClassiﬁer.html\n7\nTABLE 3\nMitigation comparison between 5 methods for discrimination via input.\nFor accuracy and demographic parity, the close to 1 the better. For\nequality of opportunity and equality of odds, the close to 0 the better.\nAdult Income\nCOMPAS\nModel/Data\nAcc\nParity\nOpty\nOdds\nAcc\nParity\nOpty\nOdds\nDataset bias\nn/a\n0.386\nn/a\nn/a\nn/a\n0.747\nn/a\nn/a\nDNN original\n0.836\n0.347\n-0.094\n-0.089\n0.658\n0.741\n-0.160\n-0.136\nReweighting [31]\n0.832\n0.654\n-0.106\n-0.090\n0.652\n0.788\n-0.186\n-0.149\nOptimized pre [33]\n0.778\n0.573\n-0.107\n-0.088\n0.665\n0.959\n-0.018\n-0.024\nPrejudice rem [32]\n0.817\n0.961\n0.005\n0.039\n0.635\n0.937\n0.008\n0.062\nCalibrated odds [17]\n0.804\n0.546\n0.148\n0.052\n0.639\n0.819\n0.036\n0.150\nanother gender. The union of the original and gender-\nswapping dataset would be gender balanced, which can\nbe used to retrain DNN models. However, it is still not\nguaranteed that balanced dataset could eliminate the rep-\nresentation bias. Previous studies show that even training\ndata is balanced, DNNs still could capture information like\ngender, race in intermediate representation [2], [34]. Thus\nmore fundamental changes in DNN models are needed to\nfurther reduce discrimination.\n4.2.2\nIn-processing\nAdversarial Learning: From model training perspective,\nadversarial training [45] is a representative solution to re-\nmove information about sensitive attributes from interme-\ndiate representation of DNNs [2], [34]. A predictor and\nan adversarial classiﬁer are learned simultaneously. The\ngoal of the predictor is to learn a high-level representation\nwhich is maximally informative for the major prediction\ntask, while the role of adversarial classiﬁer is to minimize\nthe predictor’s ability to predict the protected attribute\n(Fig. 2(b)). The DNN is denoted as f(x) = c(h(x)), where\nh(x) is the intermediate representation for input x, and c(·)\nis responsible to map intermediate representation to ﬁnal\nmodel prediction. The protected attribute is denoted using\nz. An adversarial classiﬁer g(h(x)) is also constructed to\npredict protected attribute z from representation h(x). The\nadversarial training process is denoted as follows:\narg min\ng\nL(g(h(x)), z)\narg min\nh,c\nL(c(h(x)), y) −λL(g(h(x)), z),\n(2)\nwhere the adversarial classiﬁer is to penalize the representa-\ntion of h(x) if protected attribute z is predictable, parameter\nλ is used to negotiate the trade-off between maximizing\nutility and fairness. The training is iteratively performed be-\ntween the main classiﬁer f(x) and the adversarial classiﬁer\ng(h(x)). Some methods implement adversarial using gen-\neral cross entropy loss [2], [34], while some others use ad-\nvanced adversarial objectives according to fairness desider-\natum [35]. The adversarial frameworks show improved\nperformance on metrics like demographic parity and equality\nof opportunity. In the meantime, adversarial training has\nsome pitfalls. Firstly, it could not fully retain the semantic\nmeaning of the data [36], thus could harm model accuracy,\nespecially when adding a strong regularization, i.e., a large\nλ. Secondly, it is also hard to stabilize the training, similar\nlike adversarial training in other applications.\nBeyond Adversarial Learning: Besides adversarial frame-\nwork, some other advanced fair representation learning\nmethods are proposed recently. For instance, residual de-\ncomposition is used for fair representation learning [36].\nBeyond enforcing inner representations to suppress pro-\ntected attribute and predict the main task label, this method\nalso adds regularization term to ensure that the debaised\nrepresentation lies in the same space with original input.\nWith the semantic meaning constraints, such representa-\ntion learning methods thus could achieve better trade-off\nbetween fairness metrics and accuracy. In addition, there\nexist non-adversarial methods using distance metrics, such\nas maximum mean discrepancy [37] and Wasserstein dis-\ntance [38], aiming to learn fair representations and eliminate\ndisparities between different sensitive groups.\n4.2.3\nPost-processing\nThe mitigation of discrimination through representation\ncould also be implemented at the inference stage. The key\nidea is to suppress the neurons that have captured protected\nattributes. The process is split into two-stages. Firstly, global\ninterpretation methods are used to locate the neurons that\nare highly related with protected attributes [22]. Secondly,\nthe activation values ﬂowing out from those neurons are set\nto zero, so as to turn off the correlation between protected\nattribute and DNN model prediction.\n4.3\nPrediction Quality Disparity\nIn this section, we introduce methods which could increase\nthe prediction quality for underrepresented minorities.\n4.3.1\nPre-processing\nFrom data perspective, one straightforward idea to increase\nthe prediction quality of underrepresented group is to en-\nforce the training dataset to be diverse. This can be achieved\nby collecting data from more comprehensive data sources.\nFor instance, the Faces of the World dataset is developed,\naiming to achieve a uniform distribution of face images\nacross two genders and four ethnic groups [39]. In some\ndomains collecting data might be expensive or impractical,\nand Generative Adversarial Networks (GANs) could be\nused to generate synthetic data [46]. For instance, GANs are\nutilized to generate face images across all age ranges [41].\nDNN models trained on this dataset are able to achieve\nequal predictive quality even for those previously minority\nage groups, e.g., age over 60.\n4.3.2\nIn-processing\nRegularizing model training is another perspective to in-\ncrease accuracy of the minority groups. This could be imple-\nmented using the transfer learning framework. For instance,\ntransfer learning is proposed to solve the problem of un-\nequal face attribute detection performance across different\nrace and gender subgroups [40]. A CNN model is ﬁrstly\ntrained using source domain dataset which is rich with data\nfor the minority group, i.e., the aforementioned Faces of the\nWorld dataset. Then the trained CNN model is transferred to\nthe target domain, i.e., face attribute detection, to improve\naccuracy of the minority group. Transfer learning could\n8\npromote both the overall accuracy and gender/race sub-\ngroup accuracy. Model training regularization could also be\nachieved under the multi-task learning setting. For instance,\na multi-task learning framework is designed for joint classi-\nﬁcation of gender, race, and age of faces images [42]. This\ncan yield signiﬁcant accuracy improvement for different\ndemographic subgroups, thus promoting model fairness in\nterms of predictive quality parity measurement.\n5\nRESEARCH CHALLENGES\nDespite signiﬁcant progresses for fairness in deep learning,\nthere are still some challenges deserving further research.\nBenchmark Datasets Benchmark datasets are lacking to\nsystematically examine the inappropriate biases in trained\nDNN systems [3]. Benchmark dataset means the dataset\nwhich has been teased out biases towards certain protected\ngroups. Current practice of testing DNNs’ performance is\nusing hold-out test sets, which usually contain the same\nbiases as training set. Test sets might fail to unveil the\nunfairness problem of trained models. Each DNN is rec-\nommended to evaluate its fairness on benchmark datasets,\nserving as supplementary test sets beyond normal test set.\nTo facilitate the construction of benchmark datasets, it is\nalso encouraged that statistics information including geog-\nraphy, gender, ethnicity and other demographic information\nshould be provided, for those datasets containing informa-\ntion about people.\nIntersectional Fairness The investigation of intersectional\nfairness, i.e., combination of multiple sensitive attributes,\nis relatively lacking in current research [47], [48]. Take bias\nmitigation for example, current work generally focus on one\nkind of bias. Although this may increase model fairness in\nterms of a speciﬁc bias, it is highly possible that the model is\nstill biased from the intersectional perspective. For instance,\na gender-debiased DNN could be fair to women, while\nexhibiting discrimination towards a subdivision group, e.g.,\nAfrican American women or women over the age of 60. Sim-\nilarly for DNN based job recruiting tool, even if the debiased\nmodel is free of gender bias, it is hard to guarantee that\nthe model is not biased towards other protected attributes,\ne.g., race, age. More work is needed to ﬁgure out methods\nwhich are effective for identiﬁcation and mitigation of inter-\nsectional biases.\nFairness and Utility Trade-off The removal of bias could\npossibly hurt the model’s ability for main prediction task.\nFor instance, adversarial training could increase fairness.\nHowever, it could compromise overall prediction accuracy,\nespecially the accuracy for non-protected groups. Thus this\nmight undermine the principle of beneﬁcence. It remains a\nchallenge to simultaneously reduce unintentional bias and\nmaintain satisfactory model prediction performance.\nFormalization of Fairness As the ﬁeld of fairness machine\nlearning is evolving quickly, there is still no consensus about\nthe measurements of fairness. In certain cases, some mea-\nsurements could be conﬂicting with others. A model may\nbe fair in terms of one metric, but may lead to other sorts\nof unfairness. For instance, a loan approval tool may satisfy\ndemographic parity measurement, while violating equality\nof opportunity measurement. There is no silver bullet, and\neach application domain calls for fairness measurements\nwhich meet its speciﬁc requirements [1].\n5.1\nFairness in Large-scale Training\nLarge-scale training is employed in some domains to boost\nmodel performance. Take NLP domain for instance, current\nparadigm is to pre-train language models (e.g., BERT [49]\nand XLNet [50]) on large-scale text corpus, which will be\nfurther ﬁne-tuned on downstream tasks such as machine\ntranslation. These powerful language models could capture\nbiases and propagate them to other tasks. Since these mod-\nels need to be trained on corpus with billion-scale words and\nare typically trained for days, bias mitigation either through\npreprocessing or training regularization remains a challenge\nand more research is needed in this direction.\n6\nCONCLUSIONS\nWith increasing adoption of DNNs in high-stake real world\napplications, e.g., job hunting, criminal justice and loan\napproval, their undesirable algorithmic unfairness problem\nhas attracted much attention recently. We give an overview\nof recent DNN bias detection and mitigation techniques\nfrom the computational perspective, with a particular focus\non interpretability. The model bias to some extent exposes\nbiases present in our society. To really beneﬁt our society,\nDNN models are supposed to reduce these biases instead\nof amplifying biases. In future, endeavor from different\ndisciplines, including computer science, statistics, cognitive\nscience, should be joined together to eliminate disparity\nand promote fairness. In this way, DNN systems could be\nreadily applied for fairness sensitive applications and really\nimprove beneﬁts of our society.\nREFERENCES\n[1]\nP. Gajane and M. Pechenizkiy, “On formalizing fairness in pre-\ndiction with machine learning,” Fairness, Accountability, and Trans-\nparency in Machine Learning (FAT/ML), 2018.\n[2]\nT. Wang, J. Zhao, M. Yatskar, K.-W. Chang, and V. Ordonez, “Bal-\nanced datasets are not enough: Estimating and mitigating gender\nbias in deep image representations,” International Conference on\nComputer Vision(ICCV), 2019.\n[3]\nS. Kiritchenko and S. M. Mohammad, “Examining gender and race\nbias in two hundred sentiment analysis systems,” Proceedings of the\n7th Joint Conference on Lexical and Computational Semantics, 2018.\n[4]\nJ. Buolamwini and T. Gebru, “Gender shades: Intersectional accu-\nracy disparities in commercial gender classiﬁcation,” in Conference\non Fairness, Accountability and Transparency (FAT*), 2018, pp. 77–91.\n[5]\nS. Gururangan, S. Swayamdipta, O. Levy, R. Schwartz, S. R.\nBowman, and N. A. Smith, “Annotation artifacts in natural lan-\nguage inference data,” North American Chapter of the Association for\nComputational Linguistics (NAACL), 2018.\n[6]\nT. Bolukbasi, K.-W. Chang, J. Y. Zou, V. Saligrama, and A. T. Kalai,\n“Man is to computer programmer as woman is to homemaker?\ndebiasing word embeddings,” in Thirtieth Conference on Neural\nInformation Processing Systems (NIPS), 2016.\n[7]\nJ. Zhao, T. Wang, M. Yatskar, V. Ordonez, and K.-W. Chang, “Men\nalso like shopping: Reducing gender bias ampliﬁcation using\ncorpus-level constraints,” 2017 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), 2017.\n[8]\nN. Kallus, X. Mao, and A. Zhou, “Assessing algorithmic fairness\nwith unobserved protected class using data combination,” arXiv\npreprint arXiv:1906.00285, 2019.\n9\n[9]\nA. Beutel, J. Chen, Z. Zhao, and E. H. Chi, “Data decisions and\ntheoretical implications when adversarially learning fair repre-\nsentations,” Fairness, Accountability, and Transparency in Machine\nLearning (FAT/ML), 2017.\n[10] S. L. Blodgett, L. Green, and B. O’Connor, “Demographic dialectal\nvariation in social media: A case study of african-american en-\nglish,” 2016 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), 2016.\n[11] D. Jurgens, Y. Tsvetkov, and D. Jurafsky, “Incorporating dialectal\nvariability for socially equitable language identiﬁcation,” in The\n55th Annual Meeting of the Association for Computational Linguistics\n(ACL), 2017.\n[12] I. Y. Chen, P. Szolovits, and M. Ghassemi, “Can ai help reduce\ndisparities in general medical and mental health care?” AMA\njournal of ethics, 2019.\n[13] C. Dulhanty and A. Wong, “Auditing imagenet: Towards a model-\ndriven framework for annotating demographic attributes of large-\nscale image datasets,” arXiv preprint arXiv:1905.01347, 2019.\n[14] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel,\n“Fairness through awareness,” in Proceedings of the 3rd innovations\nin theoretical computer science conference, 2012.\n[15] A. Beutel, J. Chen, T. Doshi, H. Qian, A. Woodruff, C. Luu, P. Kre-\nitmann, J. Bischof, and E. H. Chi, “Putting fairness principles into\npractice: Challenges, metrics, and improvements,” AAAI/ACM\nConference on Artiﬁcial Intelligence, Ethics, and Society (AIES), 2019.\n[16] M. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger, and\nS. Venkatasubramanian, “Certifying and removing disparate im-\npact,” in Proceedings of the 21th ACM SIGKDD International Confer-\nence on Knowledge Discovery and Data Mining (KDD).\nACM, 2015.\n[17] M. Hardt, E. Price, N. Srebro et al., “Equality of opportunity in\nsupervised learning,” in Advances in neural information processing\nsystems (NIPS), 2016.\n[18] M. Du, N. Liu, and X. Hu, “Techniques for interpretable machine\nlearning,” Communications of the ACM (CACM), 2020.\n[19] M. Du, N. Liu, Q. Song, and X. Hu, “Towards explanation of dnn-\nbased prediction with guided feature inversion,” ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining\n(KDD), 2018.\n[20] M. Du, N. Liu, F. Yang, and X. Hu, “On attribution of recurrent\nneural network predictions via additive decomposition,” The Web\nConference (WWW), 2019.\n[21] J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, and H. Lipson, “Un-\nderstanding neural networks through deep visualization,” ICLR\nworkshop, 2015.\n[22] B. Kim, M. Wattenberg, J. Gilmer, C. Cai, J. Wexler, F. Viegas, and\nR. Sayres, “Interpretability beyond feature attribution: Quantita-\ntive testing with concept activation vectors (tcav),” International\nConference on Machine Learning (ICML), 2018.\n[23] S. Sharma, J. Henderson, and J. Ghosh, “Certifai: A common\nframework to provide explanations and analyse the fairness and\nrobustness of black-box models,” in Proceedings of the AAAI/ACM\nConference on AI, Ethics, and Society (AIES), 2020.\n[24] B. Zhou, Y. Sun, D. Bau, and A. Torralba, “Interpretable basis\ndecomposition for visual explanation,” in European Conference on\nComputer Vision (ECCV), 2018.\n[25] R. Fong and A. Vedaldi, “Net2vec: Quantifying and explaining\nhow concepts are encoded by ﬁlters in deep neural networks,” in\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2018.\n[26] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba,\n“Learning deep features for discriminative localization,” in IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), 2016.\n[27] S. Nagpal, M. Singh, R. Singh, M. Vatsa, and N. Ratha, “Deep\nlearning for face recognition: Pride or prejudiced?” arXiv preprint\narXiv:1904.01219, 2019.\n[28] A. S. Ross, M. C. Hughes, and F. Doshi-Velez, “Right for the\nright reasons: Training differentiable models by constraining their\nexplanations,” Proceedings of the Twenty-Sixth International Joint\nConference on Artiﬁcial Intelligence (IJCAI), 2017.\n[29] F. Liu and B. Avci, “Incorporating priors with feature attribution\non text classiﬁcation,” 57th Annual Meeting of the Association for\nComputational Linguistics (ACL), 2019.\n[30] A. Agarwal, A. Beygelzimer, M. Dud´ık, J. Langford, and H. Wal-\nlach, “A reductions approach to fair classiﬁcation,” International\nConference on Machine Learning (ICML), 2018.\n[31] F. Kamiran and T. Calders, “Data preprocessing techniques for\nclassiﬁcation without discrimination,” Knowledge and Information\nSystems (KAIS), 2012.\n[32] T. Kamishima, S. Akaho, H. Asoh, and J. Sakuma, “Fairness-\naware classiﬁer with prejudice remover regularizer,” in Joint Eu-\nropean Conference on Machine Learning and Knowledge Discovery in\nDatabases.\nSpringer, 2012.\n[33] F. Calmon, D. Wei, B. Vinzamuri, K. N. Ramamurthy, and K. R.\nVarshney, “Optimized pre-processing for discrimination preven-\ntion,” in Advances in Neural Information Processing Systems (NIPS),\n2017, pp. 3992–4001.\n[34] Y. Elazar and Y. Goldberg, “Adversarial removal of demographic\nattributes from text data,” 2018 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), 2018.\n[35] D. Madras, E. Creager, T. Pitassi, and R. Zemel, “Learning adver-\nsarially fair and transferable representations,” International Confer-\nence on Machine Learning (ICML), 2018.\n[36] N. Quadrianto, V. Sharmanska, and O. Thomas, “Discovering\nfair representations in the data domain,” in IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2019.\n[37] C. Louizos, K. Swersky, Y. Li, M. Welling, and R. Zemel, “The\nvariational fair autoencoder,” arXiv preprint arXiv:1511.00830, 2015.\n[38] R. Jiang, A. Pacchiano, T. Stepleton, H. Jiang, and S. Chiappa,\n“Wasserstein fair classiﬁcation,” arXiv preprint arXiv:1907.12059,\n2019.\n[39] S. Escalera, M. Torres Torres, B. Martinez, X. Bar´o, H. Jair Es-\ncalante, I. Guyon, G. Tzimiropoulos, C. Corneou, M. Oliu,\nM. Ali Bagheri et al., “Chalearn looking at people and faces of the\nworld: Face analysis workshop and challenge 2016,” in Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition\nWorkshops, 2016.\n[40] H. J. Ryu, H. Adam, and M. Mitchell, “Inclusivefacenet: Improving\nface attribute detection with race and gender diversity,” Fairness,\nAccountability, and Transparency in Machine Learning (FAT/ML),\n2018.\n[41] Z. Zhang, Y. Song, and H. Qi, “Age progression/regression by\nconditional adversarial autoencoder,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), 2017.\n[42] A. Das, A. Dantcheva, and F. Bremond, “Mitigating bias in gender,\nage and ethnicity classiﬁcation: a multi-task convolution neural\nnetwork approach,” in Proceedings of the European Conference on\nComputer Vision (ECCV), 2018.\n[43] R. K. Bellamy, K. Dey, M. Hind, S. C. Hoffman, S. Houde,\nK. Kannan, P. Lohia, J. Martino, S. Mehta, A. Mojsilovic et al.,\n“Ai fairness 360: An extensible toolkit for detecting, understand-\ning, and mitigating unwanted algorithmic bias,” arXiv preprint\narXiv:1810.01943, 2018.\n[44] M. Du, N. Liu, F. Yang, and X. Hu, “Learning credible deep neu-\nral networks with rationale regularization,” in IEEE International\nConference on Data Mining (ICDM), 2019.\n[45] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,\nF.\nLaviolette,\nM.\nMarchand,\nand\nV.\nLempitsky,\n“Domain-\nadversarial training of neural networks,” The Journal of Machine\nLearning Research (JMLR), 2016.\n[46] M.\nFrid-Adar,\nE.\nKlang,\nM.\nAmitai,\nJ.\nGoldberger,\nand\nH. Greenspan, “Synthetic data augmentation using gan for im-\nproved liver lesion classiﬁcation,” in IEEE international symposium\non biomedical imaging (ISBI), 2018.\n[47] A. J. Bose and W. Hamilton, “Compositional fairness constraints\nfor graph embeddings,” International Conference on Machine Learn-\ning (ICML), 2019.\n[48] E. Creager, D. Madras, J.-H. Jacobsen, M. A. Weis, K. Swersky,\nT. Pitassi, and R. Zemel, “Flexibly fair representation learning\nby disentanglement,” International Conference on Machine Learning\n(ICML), 2019.\n[49] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,” 2019 Annual Conference of the North American Chapter of\nthe Association for Computational Linguistics (NAACL), 2019.\n[50] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, and Q. V.\nLe, “Xlnet: Generalized autoregressive pretraining for language\nunderstanding,” Advances in Neural Information Processing Systems\n(NeurIPS), 2019.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CY",
    "stat.ML"
  ],
  "published": "2019-08-23",
  "updated": "2020-03-19"
}