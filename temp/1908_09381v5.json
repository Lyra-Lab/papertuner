{
  "id": "http://arxiv.org/abs/1908.09381v5",
  "title": "Tutorial and Survey on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement Learning",
  "authors": [
    "Xudong Sun",
    "Bernd Bischl"
  ],
  "abstract": "Aiming at a comprehensive and concise tutorial survey, recap of variational\ninference and reinforcement learning with Probabilistic Graphical Models are\ngiven with detailed derivations. Reviews and comparisons on recent advances in\ndeep reinforcement learning are made from various aspects. We offer detailed\nderivations to a taxonomy of Probabilistic Graphical Model and Variational\nInference methods in deep reinforcement learning, which serves as a\ncomplementary material on top of the original contributions.",
  "text": "Tutorial and Survey on Probabilistic Graphical\nModel and Variational Inference in Deep\nReinforcement Learning\nXudong Sun\nDepartment of Statistics\nLudwig Maximillian University of Munich\nMunich, Germany\nEmail: xudong.sun@stat.uni-muenchen.de\nBernd Bischl\nDepartment of Statistics\nLudwig Maximillian University of Munich\nMunich, Germany\nAbstract—Aiming at a comprehensive and concise tutorial\nsurvey, recap of variational inference and reinforcement learning\nwith Probabilistic Graphical Models are given with detailed\nderivations. Reviews and comparisons on recent advances in deep\nreinforcement learning are made from various aspects. We offer\ndetailed derivations to a taxonomy of Probabilistic Graphical\nModel and Variational Inference methods in deep reinforcement\nlearning, which serves as a complementary material on top of\nthe original contributions.\nKeywords—Probabilistic Graphical Models; Variational Infer-\nence; Deep Reinforcement Learning\nI. INTRODUCTION\nDespite the recent successes of Reinforcement Learning,\npowered by Deep Neural Networks, in complicated tasks like\ngames [1] and robot locomotion [2], as well as optimization\ntasks like Automatic Machine Learning [3]. The ﬁeld still\nfaces many challenges including expressing high dimensional\nstate and policy, exploration in sparse reward, etc. Probabilistic\nGraphical Model and Variational Inference offers a great tool\nto express a wide spectrum of trajectory distributions as well as\nconducting inference which can serve as a control method. Due\nto the emerging popularity, we present a comprehensive and\nconcise tutorial survey paper with the following contributions:\n• We provide Probabilistic Graphical Models for many\nbasic concepts of Reinforcement Learning, which is\nrarely covered in literature. We also provide Probabilistic\nGraphical Models to some recent works on Deep Rein-\nforcement Learning [4], [5] which does not exist in the\noriginal contributions.\n• We cover a taxonomy of Probabilistic Graphical Model\nand Variational Inference [6] methods used in Deep\nReinforcement Learning and give detailed derivations to\nmany of the critical equations, which is not given in\nthe original contributions. Together with the recap of\nvariational inference and deep reinforcement learning, the\npaper serves as a self-inclusive tutorial to both beginner\nand advanced readers.\nA. Organization of the paper\nIn section I-B, we ﬁrst introduce the fundamentals of\nProbabilistic Graphical Models and Variational Inference, then\nwe review the basics about reinforcement learning by con-\nnecting probabilistic graphical models (PGM) in section II-A,\nII-B, II-C, as well as an overview about deep reinforcement\nlearning, accompanied with a comparison of different methods\nin section II-D. In section III-A, we discuss how undirected\ngraph could be used in modeling both the value function and\nthe policy, which works well on high dimensional discrete state\nand action spaces. In section III-B, we introduce the directed\nacyclic graph framework on how to treat the policy as posterior\non actions, while adding many proofs that does not exist\nin the original contributions. In section III-C, we introduce\nworks on how to use variational inference to approximate the\nenvironment model, while adding graphical models and proofs\nwhich does not exist in the original contributions.\nB. Prerequisite on Probabilistic Graphical Models and Vari-\national Inference, Terminologies and Conventions\nWe use capital letter to denote a Random Variable (RV),\nwhile using the lower case letter to represent the realization.\nTo avoid symbol collision of using A to represent advantage in\nmany RL literature, we use Aact explicitly to represent action.\nWe use (B ⊥⊥C) | A to represent B is conditionally indepen-\ndent from C, given A, or equivalently p(B|A, C) = p(B|A)\nor p(BC|A) = P(B|A)P(C|A). Directed Acyclic Graphs\n(DAG) [7] as a PGM offers an instinctive way of deﬁning fac-\ntorized joint distributions of RV by assuming the conditional\nindependence [7] through d-separation [7]. Undirected Graph\nincluding Markov Random Fields also speciﬁes the conditional\nindependence with local Markov property and global Markov\nproperty [8].\nVariational Inference (VI) approximates intractable posterior\ndistribution p(z | x) =\n1\nR\nz′ p(z′)p(x|z′)dz′ p(z)p(x | z) with\nlatent variable z speciﬁed in a probabilistic graphical model,\nby a variational proposal posterior distribution qφ(z | x),\ncharacterized by variational parameter φ. By optimizing the\narXiv:1908.09381v5  [cs.LG]  8 Dec 2019\nlog p(x)\n=Eq [log p(x | z)] −Eq\n\u0014\nlog qφ(z | x)\np(z)\n\u0015\n+ Eq\n\u0014\nlog qφ(z | x)\np(z | x)\n\u0015\n= −DKL(qφ(z|x)||p(x, z)) −Eq [log p(z|x)] +\nEq log qφ(z|x)\n= −F(φ, θ) + Hq(p) −H(q)\n=ELBO(φ, θ) + DKL(qφ(z | x)||p(z | x))\n(1)\nEvidence Lower Bound (ELBO) [6], VI assigns the values to\nobserved and latent variables at the same time. VI is widely\nused in Deep Learning Community like variational resampling\n[9]. VI is also used in approximating the posterior on the\nweights distribution of neural networks for Thompson Sam-\npling to tackle the exploration-exploitation trade off in bandit\nproblems [10], as well as approximating on the activations\ndistribution like Variational AutoEncoder [11].\nAs a contribution of this paper, we summarize the relation-\nship of evidence log p(x), KL divergence DKL, cross entropy\nHq(p), entropy H(q), free energy F(φ, θ) and ELBO(φ, θ)\nin Equation (1).\nII. REINFORCEMENT LEARNING AND DEEP\nREINFORCEMENT LEARNING\nA. Basics about Reinforcement Learning with graphical model\nAgent at St\nEnvironment\nAction at ∼π(a|St)\nState St+1\nReward Rt\nFig. 1. Concept of Reinforcement Learning\n1) RL Concepts, Terminology and Convention: As shown\nin Figure 1, Reinforcement Learning (RL) involves optimizing\nthe behavior of an agent via interaction with the environment.\nAt time t, the agent lives on state St, By executing an action at\naccording to a policy [12] π(a|St), the agent jumps to another\nstate St+1, while receiving a reward Rt. Let discount factor γ\ndecides how much the immediate reward is favored compared\nto longer term return, with which one could also allow\ntractability in inﬁnite horizon reinforcement learning [12], as\nwell as reducing variance in Monte Carlo setting [13]. The\ngoal is to maximize the accumulated rewards, G = PT\nt=0 γtRt\nwhich is usually termed return in RL literature.\nFor simplicity, we interchangeably use two conventions\nwhenever convenient: Suppose an episode last from t = 0 : T,\nwith T →∞correspond to continuous non-episodic reinforce-\nment learning. We use another convention of t ∈{0, · · · , ∞}\nby assuming when episode ends, the agent stays at a self\nabsorbing state with a null action, while receiving null reward.\nBy unrolling Figure 1, we get a sequence of state, action and\nreward tuples {(St, Aact\nt\n, Rt)} in an episode, which is coined\ntrajectory τ [14]. Figure 2 illustrates part of a trajectory in one\nrollout. The state space S and action space A, which can be\neither discrete or continuous and multi-dimensional, are each\nrepresented with one continuous dimension in Figure 2 and\nplotted in an orthogonal way with different colors, while we\nuse the thickness of the plate to represent the reward space R.\nt\nSt\nat ∼π(a|st)\nS\nrt = 0\nR\nat+N\nst+N\nrt+N ̸= 0\nrt+2N = 0\nA\nFig. 2. Illustration of State, Action and Reward Trajectory\n2) DAGs for (Partially Observed ) Markov Decision Pro-\ncess: Reinforcement Learning is a stochastic decision process,\nwhich usually comes with three folds of uncertainty. That is,\nunder a particular stochastic policy characterized by π(a|s) =\np(a|s), within a particular environment characterized by state\ntransition probability p(st+1|st, a) and reward distribution\nfunction p(rt|st, at), a learning agent could observe different\ntrajectories with different unrolling realizations. This is usually\nmodeled as a Markov Decision Process [12], with its graphical\nmodel shown in Figure 3, where we could deﬁne a joint\nprobability distribution over the trajectory of state , action and\nreward RVs. In Figure 3, we use dashed arrows connecting\nstate and action to represent the policy, upon ﬁxed policy π,\nwe have the trajectory likelihood in Equation (2)\np(τ) = p(s0)\nT\nY\nt=0\np(st+1|st, at)p(rt|st, at)π(at|st)\n(2)\nUpon observation of a state st in Figure 3, the action at the\ntime step in question is conditionally independent with the\nstate and action history Et = {S0, Aact\n0 , · · · , St−1}, which\ncould be denoted as (Aact\nt\n⊥⊥Et) | St. A more realistic model,\nAact\nt\nRt\nRt+1\nRt+2\nAact\nt+1\nSt\nSt+1\nSt+2\nAact\nt+2\np(at|st)\np(rt|st, at)\np(st+1|st, at)\nFig. 3. Directed Acyclic Graph For Markov Decision Process\nhowever, is the Partially Observable Markov Decision process\n[15], with its DAG representation shown in Figure 4, where\nthe agent could only observe the state partially by observing\nOt through a non invertible function of the next state St+1 and\nthe action at, as indicated the Figure by p(ot|st+1, at), while\nthe distributions on other edges are omitted since they are the\nsame as in Figure 3. Under the graph speciﬁcation of Figure\n4, the observable Ot is no longer Markov, but depends on the\nwhole history, however, the latent state St is still Markov. For\nPOMDP, belief state bt is deﬁned at time t, which is associated\nwith a probability distribution bt(st) over the hidden state\nSt, with P\nS b(St) = 1, where state S takes value in latent\nstate space S [15]. The latent state distribution associated with\nAact\nt−1\nOt−1\nOt\nOt+1\nAact\nt\nSt−1\nSt\nSt+1\nAact\nt+1\nRt−1\nRt\nRt+1\np(ot|st+1, at)\nFig. 4. Probabilistic Graphical Model for POMDP\nbelief state can be updated in a Bayesian way in Equation (6).\nB. Value Function, Bellman Equation, Policy Iteration\nDeﬁne state value function of state s ∈S in Equation\n(7), where the corresponding Bellman Equation is derived in\nEquation (8).\nV π(s) = Eπ,ε[\n∞\nX\ni=0\nγiRt+i(St+i, Aact\nt+i) | ∀St = s]\n(7)\n=Eπ,ε[Rt(St, Aact\nt\n) + γ\n∞\nX\ni=1\nγ(i−1)Rt+i(St+i, Aact\nt+i)]\n=Eπ,ε[Rt(St, Aact\nt\n) + γ\n∞\nX\ni′=0\nγi\n′\nRt+1+i′(St+1+i′, Aact\nt+1+i′)]\n=Eπ,ε[Rt(St, Aact\nt\n) + γV π(St+1)]\n(8)\nwhere St+i\n∼\np(st+i+1|st+i, at+i) takes value from S,\nAact\nt+i ∼π(a|St+i+1) taking value from A, and we have used\nthe π and ε in the subscript of the expectation E operation\nto represent the probability distribution of the policy and the\nbt+1(st+1)\n=p(st+1 | ot, at, bt)\n=p(st+1, ot, at, bt)\np(ot, at, bt)\np(st+1, at, bt)\np(st+1, at, bt)\n=p(ot | at, st+1, bt)p(st+1 | at, bt)\np(ot | at, bt)\n(3)\n=p(ot | st+1, at)\nP\nst p(st, st+1 | at, bt)\np(ot | at, bt)\n(4)\n=p(ot | st+1, at)\nP\nst p(st+1 | st, at, bt)p(st | at, bt)\np(ot | at, bt)\n(5)\n=p(ot | st+1, at)\nP\nst p(st+1 | st, at)p(st | at, bt)\np(ot | at, bt)\n(6)\nenvironment (including transition probability and reward prob-\nability) respectively. State action value function [12] is deﬁned\nin Equation (9), where in Equation (10), its relationship to the\nstate value function is stated.\nQπ(s, a) (∀St = s, Aact\nt\n= a)\n=Eπ,ε[Rt(St = s, Aact\nt\n= a) +\n∞\nX\ni=1\nγiRt+i(St+i, Aact\nt+i)] (9)\n=Eπ,ε[Rt(St = s, Aact\nt\n= a) + γV π(St+1)]\n(10)\nCombining Equation (8) and Equation (9), we have\nV (s) =\nX\na\nπ(a|s)Q(s, a)\n(11)\nDeﬁne optimal policy [12] to be\nπ∗= arg max\nπ\nV π(s), ∀s ∈S\n= arg max\nπ\nEπ[Rt + γV π(St+1)]\n(12)\nTaking the optimal policy π∗into the Bellman Equation in\nEquation (8), we have\nV π∗(s) = Eπ∗,ε\nh\nRt(s, Aact\nt\n) + γV π∗(St+1)\ni\n(13)\nTaking the optimal policy π∗into Equation (9), we have\nQπ∗(s, a) = Eπ∗,ε[Rt(s, a) +\n∞\nX\ni=1\nγiRt+i(St+i, Aact\nt+i)] (14)\nBased on Equation (14) and Equation (13), we get\nV π∗(s) = max\na\nQπ∗(s, a)\n(15)\nand\nQπ∗(s, a) = Eε,π∗\nh\nRt(s, a) + γmax\n¯a\nQπ∗(St+1, ¯a)\ni\n(16)\nFor learning the optimal policy and value function, General\nPolicy Iteration [12] can be conducted, as shown in Figure\n5, where a contracting process [12] is drawn. Starting from\ninitial policy π0, the corresponding value function V π0 could\nbe estimated, which could result in improved policy π1 by\ngreedy maximization over actions. The contracting process is\nsupposed to converge to the optimal policy π∗.\nAs theoretically fundamentals of learning algorithms, Dy-\nnamic programming and Monte Carlo learning serve as two ex-\ntremities of complete knowledge of environment and complete\nmodel free [12], while time difference learning [12] is more\nubiquitously used, like a bridge connecting the two extremities.\nTime difference learning is based on the Bellman update error\nδt = Q(st, at) −\n\u0010\nRt(s, a) + γmax\na\nQ(st+1, a)\n\u0011\n.\nC. Policy Gradient and Actor Critic\nReinforcement Learning could be viewed as a functional op-\ntimization process. We could deﬁne an objective function over\na policy πθ(a|s), as a functional, characterized by parameter\nθ, which could correspond to the neural network weights, for\nexample.\nπ0\nV π0\n· · ·\n· · ·\nπi\nπi+1\nV πi\nπ∗→V π∗\ni = 0\ni = M\npolicy evaluation\npolicy improve\nFig. 5. General Policy Iteration\nη(s) =\nX\nk=0\nγkP π(s0 →s, k + 1)\n(17)\n= h(s) +\nX\n¯s,a\nγη(¯s)πθ(a|¯s)P(s|¯s, a)\n(18)\nSuppose all episodes start from an auxiliary initial state s0,\nwhich with probability h(s), jumps to different state s ∈S\nwithout reward. h(s) characterizes the initial state distribution\nwhich only depends on the environment. Let η(s) represent\nthe expected number of steps spent on state s, which can\nbe calculated by summing up the γ discounted probability\nP π(s0 →s, k + 1) of entering state s with k + 1 steps\nfrom auxiliary state s0, as stated in Equation (17), which\ncan be thought of as the expectation of γk conditional on\nstate s. In Equation (18), the quantity is calculated by either\ndirectly starting from state s, which correspond to k = 0 in\nEquation (17), or entering state s from state ¯s with one step,\ncorresponding to k + 1 ≥2 in Equation (17).\nFor an arbitrary state s ∈S, using s\n′ and s\n′′ to repre-\nsent subsequent states as dummy index, the terms in square\nbrackets in Equation (22) are simply Equation (21) with a and\ns\n′ replaced by a\n′ and s\n′′. Since ∇θV π(θ)(s∞) = 0, Equation\n(22) could be written as Equation (23), where sk represent the\nstate of k steps after s and P π(s →sk, k) already includes\nintegration of intermediate state sk−1, . . . s1 before reaching\nstate sk.\n∇θV π(θ)(s) =\nX\na\n∇θπθ(a|s)Qπ(θ)(s, a)+\nX\nk=1\nX\nsk\nX\nak\nγkP π(s →sk, k)∇θπθ(ak|sk)Qπ(θ)(sk, ak)\n(23)\nLet objective function with respect to policy be deﬁned to\nbe the value function starting from auxiliary state s0 as in\nEquation (24).\nJ(πθ) = V π(s0) = Eπ,ε\n∞\nX\nt=0\nγtRt(S0 = s)\n(24)\nThe optimal policy could be obtained by gradient accent\noptimization, leading to the policy gradient algorithm [12],\n∇θV π(θ)(s) = ∇θ\n\"X\na\nQπ(θ)(s, a)πθ(a|s)\n#\n(19)\n=\nX\na\nh\n∇θQπ(θ)(s, a)πθ(a|s) + ∇θπθ(a|s)Qπ(θ)(s, a)\ni\n=\nX\na\n∇θ\n\nX\ns′,R\nP(s\n′, R|s, a)\n\u0010\nR + γV π(θ)(s\n′)\n\u0011\n\nπθ(a|s)\n+\nX\na\n∇θπθ(a|s)Qπ(θ)(s, a)\n(20)\n=\nX\na\nX\ns′\nγP(s\n′|s, a)∇θV π(θ)(s\n′)πθ(a|s)+\nX\na\n∇θπθ(a|s)Qπ(θ)(s, a)\n(21)\n=\nX\na\n∇θπθ(a|s)Qπ(θ)(s, a) +\nX\na\nX\ns′\nγP(s\n′|s, a)πθ(a|s)\n\nX\na′\nX\ns′′\nγP(s\n′′|s\n′, a\n′)∇θV π(θ)(s\n′′)πθ(a\n′|s\n′)+\nX\na′\n∇θπθ(a\n′|s\n′)Qπ(θ)(s\n′, a\n′)\n\n\n(22)\nas in Equation (29).\n∇θJ(πθ) = ∇θV π(s0)\n=\nX\nk=0\nX\nsk\nX\nak\nγkP π(s0 →sk, k)∇θπθ(ak|sk)Qπ(θ)(sk, ak)\n=\nX\ns\nX\na\nη(s)∇θπθ(a|s)Qπ(θ)(s, a)\n(25)\n=\nP\ns η(s)\nP\ns η(s)\nX\ns\nX\na\nη(s)∇θπθ(a|s)Qπ(θ)(s, a)\n(26)\n=\nX\n¯s\nη(¯s)\nX\ns\nX\na\nµ(s)∇θπθ(a|s)Qπ(θ)(s, a)\n(27)\n=\nX\n¯s\nη(¯s)\nX\ns\nX\na\nµ(s)πθ(a|s)\nπθ(a|s)∇θπθ(a|s)Qπ(θ)(s, a)\n(28)\n∝Eπ\n\u0014∇θπθ(A|S)\nπθ(A|S)\nˆQπ(θ)(S, A)\n\u0015\n(29)\nThe policy gradient could be augmented to include zero\ngradient baseline b(s), with respect to objective function J(πθ)\nin Equation (28), as a function of state s, which does not\ninclude parameters for policy θ, since P\na ∇θπθ(a|s) = 0. To\nreduce variance of the gradient, the baseline is usually chosen\nto be the state value function estimator ˆVw(s) to smooth out\nthe variation of Q(s, a) at each state, while ˆVw(s) is updated\nin a Monte Carlo way by comparing with ˆQπθ(S, A) = Gt.\nThe actor-critic algorithm [12] decomposes Gt −Vw(st) to\nbe Rt + γVw(st+1) −Vw(st), so bootstrap is used instead of\nMonte Carlo.\nD. Basics of Deep Reinforcement Learning\nDeep Q learning [1] makes a breakthrough in using neural\nnetwork as the functional approximator for value function on\ncomplicated tasks. It solves the transition correlation problem\nby random sampling from a replay memory. Speciﬁcally, the\nreinforcement learning is transformed in a supervised learning\ntask by ﬁtting on the target Rt + γmax\na\nQ(st+1, a) from the\nreplay memory with state st as input. However, the target\ncan get drifted easily which leads to unstable learning. In\n[1], a target network is used to provide a stable target for\nthe updating network to be learned before getting updated\noccasionally. Double Deep Q learning [16], however, solves\nthe problem by having two Q network and update the param-\neters in a alternating way. We review some state of art deep\nreinforcement learning algorithms from different aspects:\n1) Off Policy methods: Except for Deep Q Learning [1]\nmentioned above, DDPG [17] extends Deterministic Policy\nGradient (DPG) [18] with deep neural network functional\napproximator, which is an actor-critic algorithm and works\nwell in continuous action spaces.\n2) On Policy methods: A3C [19] stands out in the asyn-\nchronous methods in deep learning [19] which can be run\nin parallel on a single multi-core CPU. Trust Region Pol-\nicy Optimization [2] and Proximal Policy Optimization [20]\nassimilates the natural policy gradient, which use a local\napproximation to the expected return. The local approxima-\ntion could serve as a lower bound for the expected return,\nwhich can be optimized safely subject to the KL divergence\nconstraint between two subsequent policies, while in practice,\nthe constraint is relaxed to be a regularization.\n3) Goal based Reinforcement Learning: In robot manipu-\nlation tasks, the goal could be represented with state in some\ncases [14]. Universal Value Function Approximator (UVFA)\n[21] incorporate the goal into the deep neural network, which\nlet the neural network functional approximator also generalize\nto goal changes in tasks, similar to Recommendation System\n[22]. Work of this direction include [23], [14], for example.\n4) Replay Memory Manipulation based Method: Replay\nmemory is a critical component in Deep Reinforcement Learn-\ning, which solves the problem of correlated transition in one\nepisode. Beyond the uniform sampling of replay memory in\nDeep Q Network [1], Prioritized Experience Replay [24] im-\nproves the performance by giving priority to those transitions\nwith bigger TD error, while Hindsight Experience Replay\n(HER) [23] manipulate the replay memory with changing\ngoals to transition so as to change reward to promote explo-\nration. Maximum entropy regularized multi goal reinforcement\nlearning [14] gives priority to those rarely occurred trajectory\nin sampling, which has been shown to improve over HER [14].\n5) Surrogate policy optimization: Like surrogate model\nused in Bayesian Optimization [25], lower bound surrogate\nis also used in Reinforcement Learning. Trust Region Policy\nOptimization (TRPO) [2] is built on the identity from [26]\nin Equation (30), where ηπnew(s) means the state visitation\nfrequency under policy πnew and advantage Aπold(at, st) =\nJ(πnew) = J(πold) +\nX\ns\nηπnew(s)\nX\na\nπnew(a|s)Aπold(a, s)\n(30)\nLπold(πnew)\n=J(πold) +\nX\ns\nηπold(s)\nX\na\nπnew(a|s)Aπold(at, st)\n(31)\nQπold(at, st) −V πold(st). Based on Policy Advantage [26]\nAπold,ηold(πnew)\n=\nP\ns ηπold(s) P\na πnew(a|s)Aπold(a, s),\na local approximation Lπold(πnew) to Equation (30) can\nbe deﬁned in Equation (31), based on which, a surrogate\nfunction M(πnew, πold) is deﬁned in Equation (32) that\nminorizes J(πnew) at πold, where Dmax\nKL (πold, πnew)\n=\nmax\ns DKL(πold(a|s), πnew(a|s)) is the maximum KL diver-\ngence, so MM [2] algorithm could be used to improve the\npolicy, leading to the trust region method [2].\nTABLE I\nCOMPARISON OF DEEP REINFORCEMENT LEARNING METHODS: ”S”\nMEANS STATE AND ”A” MEANS ACTION, WHERE ”C” MEANS\nCONTINUOUS, ”D” MEANS DISCRETE. ”STANDALONE” MEANS WHETHER\nTHE ALGORITHM WORK INDEPENDENTLY OR NEEDS TO BE COMBINED\nWITH ANOTHER LEARNING ALGORITHM. ”VAR” MEANS WHICH\nPROBABILITY THE VARIATIONAL INFERENCE IS APPROXIMATING, ”P”\nMEANS WHETHER THE METHOD IS ON POLICY OR OFF POLICY. ”NA”\nMEANS NOT APPLICABLE\nAlgorithm\nS\nA\nstandalone\nvar\np\nDeep Q\nc\nd\ny\nna\noff\nA3C\nc\nc/d\ny\nna\non\nTRPO/PPO\nc\nc/d\ny\nna\non\nDDPG\nc\nc\ny\nna\noff\nBoltzmann\nd\nd\ny\nna\non\nVIME\nc\nc\nn\npθ(st+1|st, at)\nna\nVAST\nc\nd\nn\np(st|ot−k)\nna\nSoftQ\nc\nc/d\ny\np(at|st)\non\nIII. TAXONOMY OF PGM AND VI IN DEEP\nREINFORCEMENT LEARNING\nDespite the success of deep reinforcement learning in many\ntalks, the ﬁeld still faces some critical challenges. One prob-\nlem is exploration with sparse reward. In complicated real\nenvironment, an agent has to explore for a long trajectory\nbefore it can get any reward as feedback. Due to lack of\nenough rewards, traditional Reinforcement Learning methods\nM(πnew, πold)\n=Lπold(πnew) −\n4max\na,s |Aπold(s, a)|γ\n1 −γ2\nDmax\nKL (πold, πnew)\n(32)\nperforms poorly, which lead to a lot of recent contributions in\nthe exploration methods. Another challenge is how to represent\npolicy in extremely large state and action spaces. Furthermore,\nsometimes it is beneﬁcial to have multimodal behavior for\na agent when some trajectory might be equivalent to other\ntrajectories and we want to learn all of them.\nIn this section, we give detailed explanation on how graph-\nical model and variational inference could be used to model\nand optimize the reinforcement learning process under these\nchallenges and form a taxonomy of these methods.\nTogether with the deep reinforcement learning methods\nmentioned in section II-D, we make a comparison of them\nin Table I.\nA. Policy and value function with undirected graphs\nWe ﬁrst discuss the application of undirected graphs in\ndeep reinforcement learning, which models joint distribution\nof variables with cliques [7]. In [27], the authors use Restricted\nBoltzmann Machine (RBM) [8], which has nice property\nof tractable factorized posterior distribution over the latent\nvariables conditioned on observed variables. To deal with\nMDPs of large state and action spaces, they model the state-\naction value function with the negative free energy of a\nRestricted Boltzmann Machine. Speciﬁcally, the visible states\nof the Restricted Boltzmann Machine [27] consists of both\nstate s and action a binary variables, as shown in Figure 6,\nwhere the hidden nodes consist of L binary variables, while\nstate variables si are dark colored to represent it can be\nobserved and actions aj are light colored to represent it need\nto be sampled. Together with the auxiliary hidden variables,\nthe undirected graph deﬁnes a joint probability distribution\nover state and action pairs, which deﬁnes a stochastic policy\nnetwork that could sample actions out for on policy learning.\nSince it is pretty easy to calculate the derivative of the\nfree energy F(s, a) with respect to the coefﬁcient wk,j of\nthe network, one could use temporal difference learning to\nupdate the coefﬁcients in the network. Thanks to properties\nof Boltzmann Machine, the conditional distribution of action\nover state p(a|s), which could be used as a policy, is still\nBoltzmann distributed as in Equation (33), governed by the\nfree energy F(a, s), where Z(s) is the partition function [7]\nand the negative free energy to approximate the state action\nvalue function Q(s, a). By adjusting the temperature T, one\ncould also change between different exploration strength.\nsi\nsi+1\naj\naj+1 s,a ∈{0, 1}D\nhk+1\nhk+2\nhk+3\nh ∈{0, 1}L\nF(s, a)\nwi,1\nwj+1,3\nFig. 6. Restricted Boltzmann Machine Value and Policy\np(a|s) = 1/Z(s)e−F (s,a)/T = 1/Z(s)eQ(s,a)/T\n(33)\nA few steps of MCMC sampling [7] could be used to sample\nactions, as an approximation of the policy, which can be fed\ninto a time difference learning method like SARSA [12], to\nAact\nt−1\nOt−1\nOt\nOt+1\nAact\nt\nSt−1\nSt\nSt+1\nAact\nt+1\nexp (r(st−1, at−1))\np(Ot = 1|st, at)\np(st+1|st, at)\nFig. 7. Optimal Policy as posterior on actions: p(at|st, Ot:T = 1)\nupdate the state value function Q(s, a)’s estimation. Such an\non-policy process has been shown to be empirically effective\nin the large state actions spaces [27].\nB. Variational Inference on ”optimal” Policies\n1) policy as ”optimal” posterior: The Boltzmann Machine\ndeﬁned Product of Expert Model in [27] works well for large\nstate and action spaces, but are limited to discrete speciﬁcally\nbinary state and action variables. For continuous state and\naction spaces, in [28], the author proposed deep energy based\nmodels with Directed Acyclic Graphs (DAG) [7], which we\nre-organize in a different form in Figure 7 with annotations\nadded. The difference with respect to Figure 3 is that, in Figure\n7, the reward is not explicit expressed in the directed graphical\nmodel. Instead, an auxilliary binary Observable O is used to\ndeﬁne whether the corresponding action at the current step\nis optimal or not. The conditional probability of the action\nbeing optimal is p(Ot = 1|st, at) = exp(r(st, at)), which\nconnects conditional optimality with the amount of award\nreceived by encouraging the agent to take highly rewarded\nactions in an exponential manner. Note that the reward here\nmust be negative to ensure the validity of probability, which\ndoes not hurt generality since reward range can be translated\n[13].\nThe Graphical Model in Figure 7 in total deﬁnes the\ntrajectory likelihood or the evidence in Equation (34):\np(τ) =\n\"\np(s1)\nY\nt\np(st+1|st, at)\n#\nexp\n X\nt\nr(st, at)\n!\n(34)\n.\nBy doing so, the author is forcing a form of functional\nexpression on top of the conditional independence structure of\nthe graph by assigning a likelihood. In this way, calculating the\noptimal policy of actions distributions becomes an inference\nproblem of calculating the posterior p(at|st, Ot:T = 1), which\nreads as, conditional on optimality from current time step\nuntil end of episode, and the current current state to be st,\nthe distribution of action at, and this posterior corresponds to\nthe optimal policy. Observing the d-separation from Figure 7,\nO1:t−1 is conditionally independent of at given st, (O1:t−1 ⊥⊥\nAact\nt\n) | St, so p(at|st, O1:t−1, Ot:T ) = p(at|st, Ot:T )\n2) Message passing for exact inference on the posterior:\nIn this section, we give detailed derivation on conducting\nexact inference on the policy posterior which is not given\np(at|st, Ot:T = 1)\n=p(at, st, Ot:T = 1)\np(st, Ot:T = 1)\n=p(Ot:T = 1|at, st)p(at, st)\np(st, Ot:T = 1)\n=p(Ot:T = 1|at, st)p(at|st)p(st)\nR\nat′ p(st, a\n′\nt, Ot:T = 1)d{a\n′\nt}\n=\np(Ot:T = 1|at, st)p(at|st)p(st)\nR\na′\nt p(Ot:T = 1|at′, st)p(at′|st)p(st)d{a\n′\nt}\n=\np(Ot:T = 1|at, st)p(at|st)\nR\na′\nt p(Ot:T = 1|at′, st)p(at′|st)d{a\n′\nt}\n=\nβ(at, st)\nR\na′\nt β(a\n′\nt, st)d{a\n′\nt}\n=β(at, st)\nβ(st)\n(35)\nin [13]. Similar to the forward-backward message passing\nalgorithm [7] in Hidden Markov Models [7], the posterior\np(at|st, Ot:T\n=\n1) could also be calculated by passing\nmessages. We offer a detailed derivation of the decomposi-\ntion of the posterior p(at|st, Ot:T = 1) in Equation (35),\nwhich is not available in [13]. In Equation (35), we deﬁne\nmessage β(at, st) = p(Ot:T = 1|at, st)p(at|st) and message\nβ(st) =\nR\na′\nt β(a\n′\nt, st)d{a\n′\nt}. If we consider p(at|st) as a prior\nwith a trivial form of uniform distribution [13], the only policy\nrelated term becomes p(Ot:T = 1|at, st).\nIn contrast to HMM, here, only the backward messages are\nrelevant. Additionally, the backward message β(at, st) here\nis not a probability distribution as in HMM, instead, is just a\nprobability. In Figure 7, the backward message β(at, st) could\nbe decomposed recursively. Since in [13] the author only give\nthe conclusion without derivation, we give a detailed derivaion\nof this recursion in Equation (36). The recursion in Equation\n(36) start from the last time point T of an episode.\n3) Connection between Message Passing and Bellman\nequation: If we deﬁne Q function in Equation (37) and V\nfunction in Equation (38)\nQ(st, at) = log(β(at, st))\n(37)\nthen the corresponding policy could be written as Equation\n(39).\nπ(at|st) = p(at|st, Ot:T = 1) = exp(Q(st, at) −V (st))\n(39)\nTaking the logrithm of Equation (36), we get Equation (40)\nwhich reduces to the risk seeking backup in Equation (41) as\nmentioned in [13]:\nQ(st, at) = r(st, at) + log Est+1∼p(st+1|st,at)[exp(V (st+1))]\n(41)\nβ(st, at)\n=p(Ot = 1, Ot+1:T = 1|st, at)\n=\nR\np(Ot = 1, Ot+1:T = 1, st, at, st+1, at+1)d{st+1, at+1}\np(st, at)\n=\nZ\np(Ot+1:T = 1, st+1, at+1, Ot = 1|st, at)d{st+1, at+1}\n=\nZ\np(Ot+1:T = 1, st+1, at+1|st, at)p(Ot = 1|st, at)\nd{st+1, at+1}\n((Ot+1:T , St+1, At+1 ⊥⊥Ot) | St, At)\n=\nZ p(Ot+1:T = 1, st+1, at+1)\np(st+1, at+1)\np(st+1, st, at)\np(st, at)\np(Ot = 1|st, at)d{st+1, at+1}\n=\nZ\np(Ot+1:T = 1|st+1, at+1)p(st+1|st, at)p(Ot = 1|st, at)\nd{st+1, at+1}\n=\nZ\nβ(st+1)p(st+1|st, at)p(Ot = 1|st, at)dst+1\n(36)\nV (st) = log β(st) = log\nZ\nβ(st, at)dat\n= log\nZ\nexp(Q(st, at))dat ≈max\nat Q(st, at)\n(38)\nThe mathematical insight here is that if we deﬁne the\nmessages passed on the Directed Acyclic Graph in Figure 7,\nthen message passing correspond to a peculiar version Bellman\nEquation like backup, which lead to an unwanted risk seeking\nbehavior [13]: when compared to Equation (10), the Q function\nhere is taking a softmax instead of expectation over the next\nstate.\n4) Variational approximation to ”optimal” policy: Since\nthe exact inference lead to unexpected behavior, approximate\ninference could be used. The optimization of the policy could\nbe considered as a variational inference problem, and we\nuse the variational policy of the action posterior distribution\nq(at|st), which could be represented by a neural network, to\ncompose the proposal variational likelihood of the trajectory\nas in Equation (42): where the initial state distribution p(s1)\nand the environmental dynamics of state transmission is kept\nlog(β(st, at))\n= log\nZ\nβ(st+1)p(st+1|st, at)p(Ot = 1|st, at)dst+1\n= log\nZ\nexp[r(st, at) + V (st+1)]p(st+1|st, at)dst+1\n= r(st, at) + log\nZ\nexp(V (st+1))p(st+1|st, at)dst+1 (40)\nq(τ) = p(s1)\nY\nt\n[p(st+1|st, at)q(at|st)]\n(42)\nlog(p(O1:T ))\n= log\nZ\np(O1:T = 1, s1:T , a1:T )q(s1:T , a1:T )\nq(s1:T , a1:T )ds1:T da1:T\n= log Eq(s1:T ,a1:T )\np(O1:T = 1, s1:T , a1:T )\nq(s1:T , a1:T )\n≥Eq(s1:T ,a1:T )[log p(O1:T = 1, s1:T , a1:T ) −log q(s1:T , a1:T )]\n(43)\n= −DKL(q(τ)|p(τ))\n(44)\n=Eq(s1:T ,a1:T )[\nX\nt=1:T\n[r(st, at) −log q(at|st)]]\n=\nX\nt=1:T\nEst,at[r(st, at) + H(π(at|st))]\n(45)\nintact. Using the proposal trajectory as a pivot, we could derive\nthe Evidence Lower Bound (ELBO) of the optimal trajectory\nas in Equation (43), which correspond to an interesting ob-\njective function of reward plus entropy return, as in Equation\n(45).\n5) Examples: In [28], the state action value function is\ndeﬁned in Equation (46). and a soft version of Bellman update\nsimilar to Q Learning [12] is carried out, which lead to policy\nimprovement with respect to the corresponding functional\nobjective in Equation (47). Setting policy as Equation (39)\nlead to policy improvement. We offer a detailed proof for a\nkey formula in Equation (48), which is stated in Equation\n(19) of [28] without proof. In Equation (48), we use π(·|s) to\nimplicitly represent π(a|s) to avoid symbol aliasing whenever\nnecessary. For the rest of the proof, we invite the reader to read\nthe appendix of [28]. Algorithms of the this kind of maximum\nentropy family also include Soft Actor Critic [29].\nQπ\nsoft(s, a) = r0 + Er∼π,s0=s,a0=a[\n∞\nX\nt=1\nγt(rt + αH(π(.|st)))]\n(46)\nJ(π)\n=\nX\nt\nE(st,at)∼ρπ\n∞\nX\nl=t\nγl−tE(sl,al)[r(sl, al)+\nαH(π(.|sl))|st, at]]\n=\nX\nt\nE(st,at)∼ρπ[Qπ\nsoft(st, at) + αH(π(.|st))]\n(47)\nH(π(·|s)) + Ea∼π[Qπ\nsoft(s, a)]\n= −\nZ\na\nπ(a|s)[log π(a|s) −Qπ\nsoft(s, a)]da\n= −\nZ\na\nπ(a|s)[log π(a|s) −log[exp(Qπ\nsoft(s, a))]]da\n= −\nZ\na\nπ(a|s)[log π(a|s) −log[\nexp(Qπ\nsoft(s, a))\nR\nexp(Qπ\nsoft(s, a\n′))da\n′\nZ\nexp(Qπ\nsoft(s, a\n′))da\n′]]da\n= −\nZ\na\nπ(a|s)[log π(a|s) −log[˜π(a|s)]−\nlog\nZ\nexp(Qπ\nsoft(s, a\n′))]da\n′\n= −DKL(π(·|s)||˜π(·|s)) + log\nZ\nexp(Qπ\nsoft(s, a\n′))da\n′\n(48)\nC. Variational Inference on the Environment\nAnother direction of using Variational Inference in Rein-\nforcement Learning is to learn an environmental model, either\non the dynamics or the latent state space posterior.\n1) Variational inference on transition model: In Variational\nInformation Maximizing Exploration (VIME) [4], where dy-\nnamic model pθ(st+1|st, at) for the agent’s interaction with\nthe environment is modeled using Bayesian Neural Network\n[10]. The R.V. for θ is denoted by Θ, and is treated in a\nBayesian way by modeling the weight θ uncertainty of a\nneural network. We represent this model with the graphical\nmodel in Figure 8, which is not given in [4]. The belief\nuncertainty about the environment is modeled as entropy of the\nposterior distribution of the neural network weights H(Θ|ξt)\nbased on trajectory observations ξt = {s1:t, a1:t−1}. The\nmethod encourages taking exploratory actions by alleviating\nthe average information gain of the agent’s belief about\nthe environment after observing a new state st+1, which\nis Ep(st+1|ξt,at)DKL(p(θ|ξt+1)||p(θ|ξt)), and this is equiva-\nlent to the entropy minus conditional entropy H(Θ|ξt, at) −\nH(Θ|ξt, at, st+1) = H(Θ|ξt, at) −H(Θ|ξt+1). With the\nhelp of Equation (49), as derived following the deﬁnition\nof conditional mutual information, we derive in Equation\n(50) that the conditional entropy difference is actually the\naverage information gain, which is equal to the conditional\nmutual information I(Θ, St+1|ξt, at) between environmental\nparameter Θ and the new state St+1. Such a derivation is not\ngiven in [4].\nBased on Equation (50), an intrinsic reward\ncan be augmented from the environmental reward function,\nthus the method could be incorporated with any existing\nreinforcement learning algorithms for exploration, TRPO [2],\nfor example. Upon additional observation of action at and\nstate st+1 pair on top of trajectory history ξt, the posterior\non the distribution of the environmental parameter θ, p(θ|ξt),\ncould be updated to be p(θ|ξt+1) in a Bayesian way as derived\nAact\nt−1\nAact\nt\nSt−1\nSt\nΘ\nΦ\nSt+1\nAact\nt+1\np(st | st−1, at−1; θ)\nFig. 8. Probabilistic Graphical Model For VIME\nI(X; Y | Z) =\nZ\nx,y,z\np(z)p(x, y|z) log\np(x, y|z)\np(x|z)p(y|z)dxdydz\n= −\nZ\nx,y,z\np(z)p(x, y|z) log p(x|z)dxdzdy+\n+\nZ\nx,y,z\np(x, y, z) log p(x|y, z)dxdzdy\n= H(X | Z) −H(X | Y, Z)\n(49)\nin Equation (51), which is ﬁrst proposed in [30]. In Equation\n(51), the denominator can be written as Equation (52), so that\nthe dynamics of the environment modeled by neural network\nweights θ, p(st+1|θ, at, ξt), could be used. The last step of\nEquation (52) makes use of p(θ|ξt, at) = p(θ|ξt).\nSince the integral in Equation (52) is not tractable, vari-\national treatment over the neural network weights posterior\ndistribution p(θ|ξt) is used, characterized by variational pa-\nrameter φ, as shown in the dotted line in Figure 8. The\nvariational posterior about the model parameter θ, updated at\neach step, could than be used to calculate the intrinsic reward\nin Equation (50).\n2) Variational Inference on hidden state posterior: In Vari-\national State Tabulation (VaST) [5], the author assume the high\ndimensional observed state to be represented by Observable\nO, while the transition happens at the latent state space\nrepresented by S, which is ﬁnite and discrete. The author\nH(Θ|ξt, at) −H(Θ|ξt, at, st+1) = I(Θ, St+1|ξt, at)\n=Eξt,at\nZ\nΘ,S\np(st+1, θ|ξt, at) log[\np(st+1, θ|ξt, at)\np(θ|ξt)p(st+1|ξt, at)]dθ\ndst+1\n=Eξt,at\nZ\nΘ,S\np(st+1|ξt, at)p(θ|ξt+1) log[p(θ|ξt+1)\np(θ|ξt) ]dθdst+1\n=Eξt,atEp(st+1|ξt,at)DKL(p(θ|ξt+1)||p(θ|ξt))\n(50)\np(θ|ξt+1) =p(θ, ξt, at, st+1)\np(ξt, at, st+1)\n=p(st+1|θ, ξt, at)p(θ, ξt, at)\np(ξt, at, st+1)\n=p(st+1|θ, ξt, at)p(θ, ξt, at)\np(at, ξt)p(st+1|at, ξt)\n=p(st+1|θ, ξt, at)p(θ|ξt, at)\np(st+1|at, ξt)\n=p(st+1|θ, ξt, at)p(θ|ξt)\np(st+1|at, ξt)\n(51)\np(st+1|at, ξt)\n=\nZ\nΘ\np(st+1, θ|at, ξt)dθ\n=\nZ\nΘ\np(st+1, θ, at, ξt)\np(at, ξt)\ndθ\n=\nZ\nΘ\np(st+1|θ, at, ξt)p(θ, at, ξt)\np(at, ξt)\ndθ\n=\nZ\nΘ\np(st+1|θ, at, ξt)p(θ|ξt)dθ\n(52)\nassume a factorized form of observation and latent space joint\nprobability, which we explicitly state in Equation (53).\np(O, S) = πθ0(s0)\nT\nY\nt=0\npθR(ot|st)\nT\nY\nt=1\npθT (st|st−1, at−1)\n(53)\nAdditionally, we characterize Equation (53) with the prob-\nabilistic graphical model in Figure 9 which does not exist\nin [5]. Compared to Figure 7, here the latent state S is in\ndiscrete space instead of high dimension, and the observation\nis a high dimensional image instead of binary variable to\nindicate optimal action. By assuming a factorized form of the\nvariational posterior in Equation (54),\nq(S0:T |O0:T ) =\nT\nY\nt=0\nqφ(St|Ot−k:t)\n(54)\nThe author assume the episode length to be T, and default\nframe prior observation to be blank frames. The Evidence\nLower Bound (ELBO) of the observed trajectory of Equation\n(53) could be easily represented by a Varitional AutoEncoder\n[31] like architecture, where the encoder qφ, together with\nthe reparametrization trick [31], maps the observed state O\ninto parameters for the Con-crete distribution [32], so back-\nprobagation could be used on deterministic variables to update\nthe weight of the network based on the ELBO, which is\ndecomposed into different parts of the reconstruction losses\nof the variational autoencoder like architecture. Like VIME\n[4], VaSt could be combined with other reinforcement learning\nalgorithms. Here prioritized sweeping [12] is carried out on the\nAact\nt−1\nOt−1\nOt\nOt+1\nAact\nt\nSt−1\nSt\nSt+1\nAact\nt+1\np(st+1|st, at)\nFig. 9. Graphical Model for Variation State Tabulation\nHeviside activation of the encoder output directly, by counting\nthe transition frequency, instead of waiting for the slowly\nlearned environmental transition model pθT (st|st−1, at−1) in\nEquation (53). A potential problem of doing so is aliasing\nbetween latent state s and observed state o. To alleviate\nthis problem, in [5], the author actively relabel the transition\nhistory in the replay memory once found the observable has\nbeen assigned a different latent discrete state.\nIV. CONCLUSION\nAs a tutorial survey, we recap Reinforcement Learning with\nProbabilistic Graphical Models, summarizes recent advances\nof Deep Reinforcement Learning and offer a taxonomy of\nProbabilistic Graphical Model and Variational Inference in\nDRL with detailed derivations which are not included in the\noriginal contributions.\nREFERENCES\n[1] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski\net al., “Human-level control through deep reinforcement learning,”\nNature, vol. 518, no. 7540, p. 529, 2015.\n[2] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust\nregion policy optimization,” in International conference on machine\nlearning, 2015, pp. 1889–1897.\n[3] X. Sun, J. Lin, and B. Bischl, “Reinbo: Machine learning pipeline search\nand conﬁguration with bayesian optimization embedded reinforcement\nlearning,” 2019.\n[4] R. Houthooft, X. Chen, Y. Duan, J. Schulman, F. De Turck, and\nP. Abbeel, “Vime: Variational information maximizing exploration,” in\nAdvances in Neural Information Processing Systems, 2016, pp. 1109–\n1117.\n[5] D. Corneil, W. Gerstner, and J. Brea, “Efﬁcient model-based deep\nreinforcement learning with variational state tabulation,” arXiv preprint\narXiv:1802.04325, 2018.\n[6] D. M. Blei, A. Kucukelbir, and J. D. McAuliffe, “Variational inference:\nA review for statisticians,” Journal of the American Statistical Associa-\ntion, vol. 112, no. 518, pp. 859–877, 2017.\n[7] C. M. Bishop, Pattern recognition and machine learning.\nspringer,\n2006.\n[8] A. Fischer and C. Igel, “Training restricted boltzmann machines: An\nintroduction,” Pattern Recognition, vol. 47, pp. 25–39, 01 2014.\n[9] X. Sun, A. Gossmann, Y. Wang, and B. Bischl, “Variational resampling\nbased assessment of deep neural networks under distribution shift,” 2019.\n[10] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra, “Weight\nuncertainty in neural networks,” arXiv preprint arXiv:1505.05424, 2015.\n[11] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv\npreprint arXiv:1312.6114, 2013.\n[12] R. S. Sutton, A. G. Barto et al., Introduction to reinforcement learning.\nMIT press Cambridge, 1998, vol. 2, no. 4.\n[13] S. Levine, “Reinforcement learning and control as probabilistic infer-\nence: Tutorial and review,” arXiv preprint arXiv:1805.00909, 2018.\n[14] R. Zhao, X. Sun, and V. Tresp, “Maximum entropy-regularized multi-\ngoal reinforcement learning,” arXiv preprint arXiv:1905.08786, 2019.\n[15] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra, “Planning and\nacting in partially observable stochastic domains,” Artiﬁcial i ntelligence,\nvol. 101, no. 1-2, pp. 99–134, 1998.\n[16] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning\nwith double q-learning,” in Thirtieth AAAI conference on artiﬁcial\nintelligence, 2016.\n[17] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,\nD. Silver, and D. Wierstra, “Continuous control with deep reinforcement\nlearning,” arXiv preprint arXiv:1509.02971, 2015.\n[18] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller,\n“Deterministic policy gradient algorithms,” 2014.\n[19] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,\nD. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep rein-\nforcement learning,” in International conference on machine learning,\n2016, pp. 1928–1937.\n[20] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-\nimal policy optimization algorithms,” arXiv preprint arXiv:1707.06347,\n2017.\n[21] T. Schaul, D. Horgan, K. Gregor, and D. Silver, “Universal value func-\ntion approximators,” in International Conference on Machine Learning,\n2015, pp. 1312–1320.\n[22] N. Kushwaha, X. Sun, B. Singh, and O. Vyas, “A lesson learned from\npmf based approach for semantic recommender system,” Journal of\nIntelligent Information Systems, vol. 50, no. 3, pp. 441–453, 2018.\n[23] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder,\nB. McGrew, J. Tobin, O. P. Abbeel, and W. Zaremba, “Hindsight expe-\nrience replay,” in Advances in Neural Information Processing Systems,\n2017, pp. 5048–5058.\n[24] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience\nreplay,” arXiv preprint arXiv:1511.05952, 2015.\n[25] X. Sun, A. Bommert, F. Pﬁsterer, J. Rahnenf¨uhrer, M. Lang, and\nB. Bischl, “High dimensional restrictive federated model selection with\nmulti-objective bayesian optimization over shifted distributions,” arXiv\npreprint arXiv:1902.08999, 2019.\n[26] S. Kakade and J. Langford, “Approximately optimal approximate rein-\nforcement learning,” in ICML, vol. 2, 2002, pp. 267–274.\n[27] B. Sallans and G. E. Hinton, “Reinforcement learning with factored\nstates and actions,” Journal of Machine Learning Research, vol. 5, no.\nAug, pp. 1063–1088, 2004.\n[28] T. Haarnoja, H. Tang, P. Abbeel, and S. Levine, “Reinforcement learning\nwith deep energy-based policies,” in Proceedings of the 34th Interna-\ntional Conference on Machine Learning-Volume 70.\nJMLR. org, 2017,\npp. 1352–1361.\n[29] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-\npolicy maximum entropy deep reinforcement learning with a stochastic\nactor,” arXiv preprint arXiv:1801.01290, 2018.\n[30] Y. Sun, F. Gomez, and J. Schmidhuber, “Planning to be surprised: Op-\ntimal bayesian exploration in dynamic environments,” in International\nConference on Artiﬁcial General Intelligence.\nSpringer, 2011, pp. 41–\n51.\n[31] C. Doersch, “Tutorial on variational autoencoders,” arXiv preprint\narXiv:1606.05908, 2016.\n[32] C. J. Maddison, A. Mnih, and Y. W. Teh, “The concrete distribution:\nA continuous relaxation of discrete random variables,” arXiv preprint\narXiv:1611.00712, 2016.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2019-08-25",
  "updated": "2019-12-08"
}