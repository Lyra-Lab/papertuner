{
  "id": "http://arxiv.org/abs/1908.10206v1",
  "title": "The many faces of deep learning",
  "authors": [
    "Raul Vicente"
  ],
  "abstract": "Deep learning has sparked a network of mutual interactions between different\ndisciplines and AI. Naturally, each discipline focuses and interprets the\nworkings of deep learning in different ways. This diversity of perspectives on\ndeep learning, from neuroscience to statistical physics, is a rich source of\ninspiration that fuels novel developments in the theory and applications of\nmachine learning. In this perspective, we collect and synthesize different\nintuitions scattered across several communities as for how deep learning works.\nIn particular, we will briefly discuss the different perspectives that\ndisciplines across mathematics, physics, computation, and neuroscience take on\nhow deep learning does its tricks. Our discussion on each perspective is\nnecessarily shallow due to the multiple views that had to be covered. The\ndeepness in this case should come from putting all these faces of deep learning\ntogether in the reader's mind, so that one can look at the same problem from\ndifferent angles.",
  "text": "The many faces of deep learning \nRaul Vicente1 \n1Computational Neuroscience Lab, Institute of Computer Science, University of Tartu, Tartu, \nEstonia \n* Correspondence:  \nRaul Vicente \nraulvicente@gmail.com \nKeywords: deep learning, artificial intelligence, mathematics, physics, neuroscience. \nAbstract \nDeep learning has sparked a network of mutual interactions between different disciplines and AI. \nNaturally, each discipline focuses and interprets the workings of deep learning in different ways. \nThis diversity of perspectives on deep learning, from neuroscience to statistical physics, is a rich \nsource of inspiration that fuels novel developments in the theory and applications of machine \nlearning. In this perspective, we collect and synthesize different intuitions scattered across several \ncommunities as for how deep learning works. In particular, we will briefly discuss the different \nperspectives that disciplines across mathematics, physics, computation, and neuroscience take on \nhow deep learning does its tricks. Our discussion on each perspective is necessarily shallow due to \nthe multiple views that had to be covered. The deepness in this case should come from putting all \nthese faces of deep learning together in the reader's mind, so that one can look at the same problem \nfrom different angles. \n1.\nIntroduction \nWhy deep learning works so well? The typical answer to this question depends on to which \ncommunity is addressed. Mathematicians will rightly contend that we do not really know. Apart \nfrom theorems concerning the expressibility of neural networks (e.g. they are universal \napproximators of continuous functions given some mild conditions), there are hardly any \nmathematical guarantees of their ability to learn when deployed on real-world problems. This is not \nsurprising given that we cannot have a precise mathematical characterization of how real data looks \nlike. That would probably involve modeling the data-generating process or in other words, \napproximate how the laws of physics conspire to produce complex patterns in the data that we \ncollect about ourselves and our environment. How one would capture in a few manageable \nequations the precise distribution of pixels and object categories in an arbitrary set of natural or \nmedical images? Without such a mathematical characterization of the data, or assuming some \nwildly rough approximation, it is difficult to know the exact landscape against which optimization \nmethods have to navigate, and hence to provide sharp learning and generalization guarantees. \nThis does not mean that useful intuitions and insights have not been developed in different \ncommunities. After all, practitioners of deep learning do find successful models for real large-scale \nproblems, and while it is usually claimed that training neural networks is still an art more than a \nscience, it is not a blind shot either (see Schmidhuber, 2015 for a historically detailed review). Even \nwithout mathematical guarantees, many mathematical and technical concepts in computer science, \noptimization, statistics, physics, and neuroscience have inspired and continue being a guide to \nunderstand how deep learning achieves its results. \nIn this work, we wish to collect and synthesize different intuitions scattered across several fields as \nfor how deep learning works. The main goal of discussing these interpretations from different areas \nis akin as to why several proofs of the same theorem are useful: they give different perspectives to \nthe same object one wishes to understand better.  \nGiven the diversity of current architectures and algorithms, we will focus our attention on the \nsimplest setting and consider a feedforward neural network trained by supervised learning (e.g. data \nconsisting of a set of exemplary input-output pairs). Many architectures and types of learning are \nvariations building upon this fundamental configuration. For example, recurrent neural networks \ncan be understood as feedforward architectures unfolding over time, while most of the cleverness in \nseveral algorithms of unsupervised and reinforcement learning goes into how to efficiently convert \nthe problem into one of supervised nature.  \nImportantly, in our presentation we will focus on what makes deep learning special from other \nmachine learning methods. Hence, out of the four components of any machine learning algorithm \n(data, model, cost function and optimization method), we will mostly deal with the model \ncomponent since it is in this aspect where deep learning makes particular choices compared to other \nalgorithms (Goodfellow et al., 2016). The model or hypothesis space considered in deep learning \nconsist of the family of functions obtained by composing a finite number of non-linear functions \nwith adjustable parameters or weights.  \nBefore we proceed to describe different intuitions about what such a composition of adaptive \nfunctions does to your data, we will need to set some background and terminology that will be \nuseful for the rest of our discussion. Unless stated otherwise, we will consider a typical dataset with \nk samples and n numerical features that can be described as a k x n design matrix or more \ngeometrically as a set of k points in an n dimensional Euclidean space. In the supervised setting we \nwill consider either a classification problem for which there is a discrete number c of labels or \nclasses (which can be thought of as different colors of the sample points), or a regression problem \nwith real numerical values as targets. The goal of the training is then to produce a function, mapping \ninputs to outputs, such that a given cost function or error is minimized. In the probabilistic setting \nthe output of the learned function is used as parameter such as, for example, the mean in a \nconditional probability model describing the probability of a range of outputs given a certain input. \n2.       Perspectives\nDifferent disciplines have naturally developed distinct interests and points of view in regard to the \ntheory and application of deep learning. Here, we wish to expose the varied views on deep learning \nphenomena by discussing a few illustrative examples from each of the considered perspectives.\n2.1.    Topological perspective \nWhile random numbers are a precious computational resource, most data we generate or care about \nto measure contains certain structure. For example, the intensities of neighboring pixels in a picture \nare not independent, they usually exhibit similar values or low-frequency patterns (textures) that \nextend over large regions and objects across the image. These correlations imply that out of the set \nof all images that one could possibly imagine, we only have to deal with a fraction of structured \nimages which exhibit numerous and strong correlations among its features.   \nMore generally, when a dataset is considered as a set of points these will typically form a shaped \ncloud or manifold which only occupies a subspace of the possible volume of the n dimensional \nspace spanned by the different feature dimensions. By manifold here we do not mean the usual \ndefinition in topology (a surface that locally looks like Euclidean space near each point) but rather a \nslab of connected points that is relatively thin in many directions (Goodfellow et al., 2016). There \ncan even be several of such slabs. The important point is that this cloud can be of a very \ncomplicated and convoluted shape, and its division in regions where points belong to the same class \n(or colouring) can also be complicated. \nFrom a topological point of view one can ask what transformations does this cloud undergo when \nrepresented by successive layers of a neural network. The weighted sum of inputs to a layer (affine \ntransformation) simply stretches the cloud in some directions and compresses it in others. The \nadditive bias terms have the effect of shifting or translating the resulting cloud. However, simply \ncomposing these linear transformations will not produce too useful transformations to disentangle a \ncomplicated manifold or its colouring. Here is where the non-linearity of the activation of neurons \nor units becomes essential. A non-linear activation function maps a range of inputs into a range of \noutputs with certain “distortion”. In non-linear functions proportionality cannot be maintained for \nthe whole range of inputs, so, for example, doubling the input will not always result in doubling the \noutput that is produced by the function. This means that the shape of our cloud of points will be \nbent in some directions due to the non-linearity of the activation functions. An extreme case of such \ndistortions occurs, for example, when using the absolute value function as the activation function. \nIn that case, the V-shaped function makes that pairs of input values (mirror points at each side of the \nbottom of the V) are transformed into the same output value. So, in effect, this \ntransformation produces a perfect folding of the cloud along some axis because several points in the \ninput are projected to the same point at the output, as when one folds a piece of paper. As we \nproceed with the forward pass through the layers of the network and repeat this operation, one is \nsimply composing one folding over previous ones (Montufar et al, 2014). The training of the \nnetwork thus becomes interpreted as the learning of along which directions to fold our successive \nrepresentations of the cloud to disentangle the colouring of its points into regions that are linearly \nseparable (Montufar et al, 2014). Similar interpretations hold for more usual activation functions \nsuch as ReLU or sigmoid. In a loose sense, the training of a network becomes an exercise in high-\ndimensional origami on some elastic material! \nThe former interpretation is useful to understand how the chaining of simple transformations can \ntake a difficult classification problem in the original space to a much simpler one in a new distorted \nspace. Moreover, the composition of elementary transformations, each building on top of the \nprevious one, allows to unfold complex dependencies across distant regions of the cloud of data \nwhile using a reduced number of training samples. However, note that since the composition of \ncontinuous functions is just another continuous function, no matter how much we play with the \nweights, the transformation from inputs to outputs will send neighboring points to neighboring \npoints, and thus cutting a cloud in two or more pieces is not an operation allowed by continuous \nactivation functions. Everything must be obtained from stretching, compressing and bending. Given \nthe visual intuition and understanding provided by topological and geometrical thinking, it is \ninteresting to entertain what are the topological effects occurring through an autoencoder’s \nbottleneck, or in the transformations produced by skip connections and residual layers found in \nmore sophisticated architectures.   \nSo far we have considered the problem of discriminating or classifying points into different classes. \nA more ambitious challenge is that of building a generative model of the data distribution where \nnew samples and other manipulations beyond the discrimination of classes can be obtained. A \npopular architecture to produce generative models of real data is the Generative Adversarial \nNetwork or GAN (Goodfellow et al, 2014). Its training proceeds as a forger-police zero-sum game \nin which one network (the generator) tries to produce samples that look like the training dataset, \nwhile another network (the discriminator) tries to detect the counterfeit. Regardless of the specifics \nof its training the generator network can be thought as being fed a random sample in some latent \nspace and producing a real-data looking sample. Similarly as in the previous case, this very \ncomplex transformation can also be understood in geometric terms. In this case, the input consists \nof a cloud with the shape of some prior distribution such as Gaussian noise. The task of the \nsuccessive layers of the generator network is to convert this cloud into the target manifold of the \ntraining data. It is perhaps not surprising then, that the same architectures capable of disentangling a \nmanifold into linearly separable regions, can also be trained to produce the stretching and folding \nnecessary to forge an undifferentiated cloud into convoluted manifolds spanned by real data. The \nintended effect of the network is thus akin to a pizza maker who starting from a ball of dough \nstretches and folds it until producing some desired shape. One must note however that the training \nis not without difficulties and much effort has been invested in assisting the network to avoid mode \ncollapse (Arjovsky et al, 2017), the undesired fact that the transformation might end up targeting a \nparticular region or only one of the possible manifolds. \nAs a final case, it might be also elucidating to search for a topological interpretation of the \nphenomena of adversarial examples. Adversarial examples refer to certain small and almost \nunnoticeable (at least for humans) variations in the input which drastically change the output of a \nclassifier, and they pose a significant challenge to the practical application of neural networks as \nwell as other machine learning models (Yuan et al, 2019; Athalye et al, 2018). Given that the \ntransformations a neural network can perform are continuous, it still must be the case that \nneighborhoods in the input space are mapped into neighborhoods in the successive representations. \nWhat is then responsible for the existence of adversarial examples? It is important to note that the \nnotion of continuity actually refers to sufficiently small neighborhoods. While the changes in \nadversarial examples might look unnoticeable to the human eye, they do not violate the continuity \nproperty since the perturbations are not arbitrarily small. Indeed, it has been proposed that the \noriginal input and its adversarial perturbation can diverge into regions labeled with distinct classes \ndue to the almost-linearity of transformations realized by the layers of the network (Athalye et al, \n2018). That occurs for example when most ReLU units are activated within their linear range. The \nrepeated application of almost linear or affine maps can amplify small differences in certain \ndirections of the original input space so that the original point and its targeted perturbation end up in \nseparated regions when represented by the final layers (Goodfellow et al, 2014b). Interestingly, non-\nlinear transformations could in principle also contribute to adversarial examples if the training is not \nfully optimal. The reason is that the same recurrent stretching and folding process that is intended to \nproduce linearly separable classes is also a hallmark of the non-linear mixing that gives rise to the \nsensitivity to initial conditions found in chaotic dynamical systems.  \nAdversarial examples are still poorly understood and the arms race between adversarial attacks and \nthe fixes to protect the models from such attacks keeps escalating. In any case, topological and \ngeometrical intuitions are a powerful guide for understanding how deep learning works as well as \nhow it fails.\n2.2.    Metric Perspective \nIn many cases the encoding of input lacks any information about the similarity or distance between \ndifferent samples. For example, symbolic representations of words do not have any resemblance to \nthe objects or meanings they stand for. A potato and a bathtub are just as similar as horse and zebra \nin what concerns their symbolic representations.  \nIn particular, one-hot encodings represent different items by the position of a single “1” within a \nbinary vector with “0”’s in the rest of its entries. In essence, this representation just uses a mask \nwith one “1” to single out distinct categorical variables, and thus the length of this representation is \nas large as the number of different types of items, which can be dramatically large in many \napplications. In our point cloud view each different data point sits on a different orthogonal axis and \nis just as far away from any other point. This representation is hardly useful to tackle questions \nabout interpolation or generalization precisely because the lack of a meaningful notion of similarity \nor distance between these points or even of the space spanned by them.  \nNeural network embeddings address this problem by learning a representation that encodes each \nsample as a relatively short vector with continuous values (see Camacho-Collados, J. and Pilehvar, \nM.T., 2018 for a review). A neural network embedding can be obtained by using a short hidden \nlayer to produce a continuous representation on the way of solving some supervised task. In a \nclassical example, a simple network with a linear hidden layer is given one-hot encodings of a \nsequence of words and is trained to output the one-hot encoding of the next word (Mikolov et al, \n2013). The one-hot encoding of the raw input multiplied by the weight matrix corresponds to \nselecting a row of the matrix. Such weights matrix contains real continuous values, and after \ntraining, its rows are the representation for each different type of input. In this case of word \nembeddings, the prediction task forces that words that appear in similar contexts to get multiplied \nby similar weights, and therefore represented by similar activations in the hidden layer. This way \nthe inputs are represented by short and dense vectors which relative coordinates capture useful \ninformation. Indeed, the learned representations have been observed to acquire relevant semantic \nstructure (Mikolov et al, 2013). For example, in word embeddings the words of similar meaning are \nmapped to nearby vectors, and directions in the new space can correspond to particular relations \nbetween words. Similar phenomena occur for more sophisticated embeddings of full sentences and \nparagraphs. \nThe gain here is obtained not so much from the depth (the architecture in the example above is \nactually shallow since it used one linear hidden layer) as from the short and distributed nature of the \nlearned representation. This distributed representation is just a usual vector which lives or is \nembedded into a Euclidean space of the same dimension as the length of the hidden layer. Hence, \nthe new cloud of points or vectors is naturally equipped with the structures to measure distances and \nangles of the Euclidean space in which it is embedded. In other words, thanks to the training of the \nembedding a meaningful metric or similarity function between points is now available. This implies \nthe possibility to exploit notions such as neighborhood, an essential concept to support similarity \nand interpolation reasoning, as well as the general metric properties of Euclidean or manifold \nspaces. Moreover, in addition to the extrinsic notions of distance (inherited from the ambient space), \nit is also possible to explore whether intrinsic metric properties defined on the data manifold, such \nas for example geodesic or diffusion distances, capture additional structure of the data. \nImportantly, what previously was impossible with one-hot encodings can become possible with \nembedded representations. Equipped with distributed representations one can describe the position \nof a novel point in coordinates relative to the positions of other points. This relational aspect is key \nfor generalization and even one-shot transfer learning since training points can serve as anchor \npoints for describing novel inputs and inferring unseen correspondences between different domains \n(Goodfellow et al, 2016). Moreover, embeddings can also be applied to create join representations \nof inputs belonging to different modalities, such as an image and its accompanying text annotation, \nwhich otherwise are difficult to compare or align. Thus, the metric and similarity notions induced \nby embedded representations might be one of the fundamental ways to understand how humans and \nmachines support analogy reasoning and even high-level concepts.\n2.3.     Information Perspective \nIn the probabilistic setting supervised learning can be viewed as the learning of a conditional \nprobability model p(Y|X) that predicts the likelihood of different targets (Y=y) given an input \n(X=x). Thus, inputs and outputs in our training data can be considered as samples obtained from a \npair of random variables, X and Y, which the model has to relate. This is possible only if X contains \ninformation about Y, that is, if uncertainty about Y is reduced when observing X, a measure which \nis quantified using mutual information (Cover, T.M. and Thomas, J.A., 2012). An interesting \nquestion is how the successive layers in a neural network affect the amount of information retained \nabout the original input (Tishby, N. and Zaslavsky, N., 2015). \nIn standard feedforward networks, each layer applies a transformation to the input received from the \nprevious layer. Thus, the set of layers, from input to output, form a Markov chain in which the \nactivation values of each layer only depend on the previous ones via the immediately preceding \nlayer (Tishby, N. and Zaslavsky, N., 2015). In particular, whenever a layer involves a non-linear \nactivation function that is not invertible (two or more inputs produce the same activation, such as in \nthe flat part of a ReLU) or there is a reduction in the number of units compared to the previous layer \n(more generally when the weight matrix has no inverse), the result is that the transformation is non-\ninvertible. The non-invertibility implies that activations of the previous layer cannot be recovered \nfrom the set of activations of the present layer. Indeed, most interesting transformations or \nmathematical functions, including the usual sum of two numbers, are non-invertible. For example, \nin the case of the sum, given the result of the operation (7) one cannot unequivocally determine \nwhich were the specific inputs since several input combinations (3+4=2+5) are compatible with the \nsame output result. For the purpose of the computation such set of inputs are indistinguishable, or in \nother words, the computation output is invariant against changes within such set of inputs.   \nFrom an information point of view each layer is literally throwing away or discarding information \nthat was present in the input. Thus, the representations by intermediate layers cannot increase the \namount of information about the target beyond that already contained in the input. While at first it \nmight sound counterintuitive that learning is based upon throwing away or forgetting information, \nindeed discarding information is an essential feature of computation or information processing \n(Doya et al, 2007). The key is to selectively retain and make explicit the information that is relevant \nfor predicting the target while throwing the rest away. This is indeed the hallmark of the invariance \nnecessary to, for example, being able to recognize the same person regardless of her distance, face \norientation, hair style, illumination conditions, etc. In a sense, any interesting computation consists \nof carving out the relevant information from a sea of irrelevant one. The successive transformations \ncarried out by a neural network during its training process precisely aim to find the combination of \nfeatures that explicits out information (linearly extractable by the output layer) that is best \ncorrelated with the desired target. In the process, dimensions or features that contain noisy or \nirrelevant information with respect to the target classes have to be folded or projected out so their \nvalue does not affect the final representation, which is the geometric analog of throwing such \ninformation to the trash bin. \nMoreover, it has been proposed that the more compressed the learned representations get, while still \nretaining information about the target, the better are their generalization properties (Tishby, N. and \nZaslavsky, N., 2015). Thus, it might be possible to interpret that the optimal training of a neural \nnetwork aims to produce representations that are close to a minimally sufficient statistic of the input \nwith respect to the target random variable. This can be considered as some reflection of Occam \nprinciple by which, other things equal, simpler or shorter rules tend to avoid overfitting and \ngeneralize better (Calude, C.S. and Chaitin, G.J., 2007).  \nMore generally, it might be interesting to assess how the information contained in different parts or \ncomponents of the input interact to explicit out relevant information at the output layer. In \nparticular, one part of the input can contain information about the target that is unique, redundant, or \nsynergistic with respect information from another part of the input (Bertschinger et al, 2014). How \nthe training of neural networks acts on these channels of information or whether they can be \nexploited for better performance is currently unknown. In any case, we find that the use of \ninformation theoretic concepts and functionals is an important tool to understand and guide the \ntraining of deep learning architectures.  \n2.4.     Causal Perspective \nRecently, deep learning has been criticized as being nothing else than fancy curve fitting. As \nprovoking as it sounds, the critics do not wish to diminish the enormous practical value of deep \nlearning in large-scale problems. Rather, they point out that current deep learning models work at \nthe level of associations or correlations between inputs and outputs, describe the limitations inherent \nto operating at such a level, and propose causal models as the necessary level to overcome such \nlimitations (Pearl, 2019).  \nIndeed, in the supervised setting deep learning and other statistical models parametrize a conditional \nprobability p(Y|X) to predict the likelihood of a set of outcomes after observing a particular input. \nWhile the model can be more or less sophisticated, the learned conditional probability amounts to \nan interpolation by fitting parameters based on observed statistical associations between X and Y. \nOnly relying on these passive observations, critics argue, is difficult to empower a learning system \nwith many of the essential characteristics of human-like intelligence (Pearl, 2019). For example, we \nhumans are relatively good at adapting to domain or task shifts (transfer learning) even if only a few \nsamples are available in the novel domain, a feat that current learning systems can hardly achieve \ndespite numerous efforts. We are also able to reason why we make a certain prediction and how it \nwould change if conditions would have varied (explainability). More generally, these and other \ntasks are thought to be solved by humans by learning and employing causal models (Lake et al, \n2017). \nEfficient learning systems displaying these desired properties will likely require to also go beyond \nthe level of passive observations and endowing them with abilities to discover and model causal \nrelations of the data generating process or environment. That is, causal reasoning, the ability of \nidentifying a relationship of cause and effect, has been put forward as a fundamental challenge \ntowards human-like AI (Guo et al, 2018; Pearl, J. and Mackenzie, D., 2018). Discovery of causal \nrelations requires actors to either be able to intervene on the environment or have a model of the \nworld accurate enough to answer questions of the kind “what would have happened if instead of \ndoing A I would have done B?”, i.e. to imagine an alternative world where something different had \nhappened. These causal and counterfactual questions are a hallmark of human cognition and the \nbuilding blocks to generalize across different domains with related causal structures or form \nexplainable decisions and plans.  \nDo deep learning systems learn anything related to a causal model? In machine learning, the most \nnatural setup for studying these questions is that of reinforcement learning (RL) (for an introduction \nto RL see Sutton, R.S. and Barto, A.G., 2018). In reinforcement learning an agent repeatedly \ninteracts with an environment (real-world or simulated) by producing an action and receiving a new \npartial observation of the environment state and possibly a reward. In this general framework the \nagent is trained to learn a policy, that is a mapping between observations to actions, in order to \nmaximize its cumulative reward. Thus, RL agents can learn to model relations between their actions \nand state transitions and rewards in the environment. In particular, deep nets in RL are used as \nfunction approximators to mediate the learning, storage and interpolation of input representations to \npredict the value of states and actions or explicitly represent a policy (Arulkumaran et al, 2017).   \nSince an agent in RL can intervene on the environment with its own actions, build a world model \n(model-based RL), and in some algorithms even decouple the evaluation of its policy from its own \nactions (off-policy methods), RL models have the potential to reach certain, although perhaps not \nthe highest and more formal, levels of causal inference. In most cases the related causal problem of \ncredit assignment in RL, that is how actions and states in the past contribute to the current reward, is \nonly dealt by the passive diffusion of value from the reward to events in the recent past. Thus once a \nreward is obtained by the agent, states and actions visited in the most recent past change their value \naccording to the magnitude and their temporal distance to the reward event. States and actions that \nrepeatedly participate in recent trajectories leading to the obtention of reward will start to be singled \nout as valuable. Without being augmented by any strict causal model nor equipped with any other \nsort of information or bias, the assignation of credit or blame to different states and actions is a slow \nand passive process. \nFrom a human perspective, one often finds that an RL agent seems to exploit spurious correlations \nor shortcuts during learning that differ from the causal links that humans search for and recognize. \nThis is partly due to the large amount of inductive biases that humans assume from previous \nexperiences that are not accessible to agents trained only for playing one particular environment or \nvideo game (Zador, 2019). From an agent's point of view, that is the whole world that it has ever \nexisted and any cue with predictive value for that environment is as good as any other, regardless of \nour human causal interpretations. However, it seems reasonable that picking up \"unintended\" \ncorrelations comes with a price in more realistic and changing environments. In these cases, true \ncausal links describing mechanistic relations are more likely to remain invariant, and therefore, \nuseful for generalizing across tasks or environments. For example, cardiac muscle compression (A) \nleads to both pumping of blood (B) and heartbeat sound (C). The latter correlates with cardiac \nfunction and is commonly used for monitoring its state. But in case of sudden heart malfunctioning \nwe will certain like to act on the mechanisms of cardiac muscle compression rather than on the \nsoundtrack.  \nThus, distinguishing causal patterns such as common drive effects (A implies both B and C) and \ndirect (A implies C) vs indirect interactions (A implies B which in turn implies C) have implications \nfor the efficiency of learning and generalization to changing or diverse conditions. The fact that \nhumans are exposed to many different tasks and environments probably adds pressure for us to \ndiscover and intervene on causal links since they are likely to be more consistent and useful across \ntasks than mere correlations. However, in simple and stationary environments, in which most RL \nalgorithms are trained, the pressure and tools for agents to discover and transfer causal relations \nremains limited. An interesting avenue to add the necessary pressure and tools can be the use of \nintrinsic rewards for causal discovery and the incorporation of heuristics used by humans and other \nanimals in their modeling of causal relations (Oudeyer, P. Y. and Kaplan, F., 2009; Johnson, S.G. \nand Keil, F.C., 2014). While imperfect and not formally as strict as “do calculus” or structural \ncausal modeling, it seems that these ingredients have given our brains a boost in their handling of \ncausal relations in natural environments.  \nIn summary, regardless of how refined the state representations obtained by deep learning \narchitectures can be, if unassisted by particular pressures and tools they might not be enough for \nagents to jump to higher levels of the causal ladder (Pearl, J. and Mackenzie, D., 2018). Endowing \ndeep learning agents with pressures and tools to actively interrogate and build causal models of the \ndynamics of the environment and of other agents is one of the most significant challenges ahead. An \nimmediate reward will be a better understanding of how transfer learning and generalization \ndepends on how well agents internalize the causal structure of the world around them. \n2.5.     Physics Perspective \nOne way to understand an artificial neural network is as a large collection of simple interacting \nunits which cooperate to represent a function or probability distribution. A large ensemble of \ninteracting units, such as molecules in a gas, is also the natural setting for statistical physics, and \nideas from this community continue being influential on the theory and numerical algorithms of \nneural networks (Sompolinsky, 1988; Mézard, M. and Mora T., 2009; Advani et al, 2013, \nGoodfellow et al, 2016).  \nIn particular, statistical physics aims to relate the collective behavior of a system with the gross \nstatistical properties of its individual parts and their interactions. To do so, physicists define a scalar \nfunction called Hamiltonian or energy E which encodes how the parts interact with each other \n(often guided by some symmetry considerations) and maps every possible configuration of the parts \nto some real number. Importantly, the probability of finding the system in a configuration decreases \nexponentially with its energy level according to a Boltzmann distribution. In statistical physics the \nweights or couplings between the elementary units are usually fixed and one is interested in \naveraging or sampling from the associated Boltzmann probability distribution in order to derive the \nmacroscopic properties of interest.  \nAn important family of models in machine learning receives the name of energy-based models \nprecisely because they use the Boltzmann distribution of some energy function to parametrize a \nprobability distribution (LeCun et al, 2006). These models include Boltzmann machines which use \na family of energy functions known as Ising models which originated in the study of magnetism as \na collective property of a large collection of atoms (Ackley et al, 1985). Neurons in these models, \nsimilar to magnetic spins, can be understood as binary threshold units driven by external input as \nwell as by the weighted output of similar units with which they are coupled.  \nWhile in statistical physics the weights are usually fixed and obtaining averages or samples from \nthe Boltzmann distribution is the main goal, in machine learning the interest is in learning and \ninference. Thus, during learning of energy-based models the focus is on finding the weights \nbetween units so that the resulting Boltzmann distribution maximizes the likelihood of the training \ndataset. This is equivalent to search in the space of allowed energy functions for the weights that \nminimize the energy of the training data. Every term in the resulting energy function works as a soft \nconstraint between variables by favouring some correlations and discouraging others in order to \nmodel the dependencies existing between variables in the data (Hinton, 2002). During inference, \nafter the weights have been fixed, all observable variables or inputs are clamped and the rest of \nvariables sampled from the Boltzmann distribution (favouring states that minimize the energy \nfunction). Reading out these inferred values can be used to make predictions given the observed \ndata, or recall patterns that were stored as minimum-energy states.  \nRestricted Boltzmann Machines (RBM) are a type of Boltzmann machine with a bipartite \nconnectivity between visible and hidden nodes which makes them efficient to train. Stacking \nseveral RBMs and training them in a layer-wise manner was one of the first tractable procedures to \nlearn a hierarchy of representations (Salakhutdinov, R. and Hinton, G., 2009). This was indeed one \nof the essential steps in the pre-training of some of the early successful deep learning models. \nMoreover, the connection between RBMs and statistical physics has been strengthened by the \nrealization that in such models different layers can be put in correspondence with the iterations of \ncertain renormalization group mapping (Mehta, P. and Schwab, D.J., 2014; Koch-Janusz, M. and \nRingel, Z., 2018). This later procedure describes the behavior of a system when it is viewed at \nprogressively larger scales and offers yet another interpretation of how a stack of RBM layers \nprocesses information. \nOther relevant deep learning networks, in this case using ReLU units, have also been mapped to \nspin glass models studied in statistical physics of disordered systems (Choromanska et al, 2015). \nSuch mappings allow the transfer of many theoretical and numerical results from a well studied \nphysical system to recent deep learning models. In particular, these results have provided a better \nunderstanding of the energy landscapes occurring in deep learning models and why their local \nminima are typically as good as finding the intractable absolute minimum (Dauphin et al, 2014; \nChoromanska et al, 2015).  \nAnother example of fruitful interaction between physics and machine learning concerns the role of \nnoise. The effects of the injection of noise or intrinsic random fluctuations has been a subject of \nstudy in statistical physics and dynamical systems. Interestingly, in many conditions the injection of \ncertain amount of noise is observed to have constructive role rather than detrimental effects \n(Simonotto et al, 1997; Gammaitoni et al, 1998; Pikovsky, A.S. and Kurths, J., 1997). One example \nof controlling the amount of noise in a system to obtain beneficial effects is that of simulated \nannealing, in which the temperature of the model (scale factor in the energy function or equivalently \nthe inverse of the learning rate) slowly cools-down to help the optimization process to explore a \nlarge region until settling in a good minimum (Kirkpatrick et al, 1983). Another example of \nconstructive effect in the training of neural networks is the injection of additive noise to the input. \nThis has been shown to result in a regularizing effect which favors the network to settle in wider \nlocal minima of the energy landscape (Hochreiter, S. And Schmidhuber, J., 1995). Such flatter \nminima solutions provide robustness of the learned model under some variations occurring during \ntesting. Similarly, multiplicative noise can also result in strong regularizing effects (Srivastava et al, \n2014). In this case, the injection of multiplicative binary noise at input and higher layers of the \nmodel, such as it occurs in dropout methods, can result in beneficial effects and increase robustness \nat different scales by acting at all levels of the representation hierarchy. \nIn general, many connections have surfaced between the objectives and methods of statistical \nphysics and those of large collections of learning neurons. Originally, statistical physics was devised \nto deal with systems made up of an incredible amount of atoms or units without the need to describe \nthe dynamics of each of them but just their statistical behavior. A similar perspective and the formal \nmappings with statistical models add another view on how coalitions of tens of thousands of \nneurons self-organize to produce large-scale computations. Energy landscapes, simulated annealing, \ntemperature, Boltzmann sampling, noise, dynamical systems, or phase transitions are just some of \nthe concepts that have successfully transferred from physics communities to machine learning and \nmore recently to deep learning to improve and explain how it works. \n2.6.    Computational Perspective \nConsidering that any given neural network encodes a well-defined function, one can also view the \nprocessing by different layers as a series of computational steps which combination produces a \ndesired output (Goodfellow et al, 2016). A series of simple steps building on top of previous ones is \nalso how we usually find more natural to code a certain function or give complex instructions to a \ncomputer, possibly reusing definitions and outputs of previous computational steps to define new \nones. Thus, it is perhaps natural to imagine that while the expressibility of a neural network does \nnot increase beyond a single hidden layer, the learning of a complex function might be easier when \nassisted by a number of intermediate layers or steps. \nMore generally, modern neural networks possess increasingly complex and heterogeneous \narchitectures and are now better understood as a special case of computational graphs (Goodfellow \net al, 2016). In this framework, nodes of the computational graph represent different variables \n(scalars or tensors). Edges between nodes in the graph indicate how some variables are obtained \nfrom a set of simple operations on other variables or nodes. The graph can be of very complicated \ntopology and contain different classes of operations. Thankfully, the apparatus for the efficient \ncomputation of back-propagation can be generalized to these computational graphs, which allows \nfor general recipes of how to formalize and train modern neural networks. From a design point of \nview, these complex graphs can now be made at the level of connecting several high-level modules \neach with a well defined function within the architecture (e.g. vision module, attention module,...). \nFrom a computational point of view one can also ask how hard is to train neural networks. That is to \nask how the computational resources needed to solve the optimization problem during training scale \nwith the size of the problem. It is known that finding the absolute minimum in the energy or loss \nlandscape of networks with at least one hidden layer is an NP-problem (Blum, A. and Rivest, R.L., \n1989). However, under some approximations it has been shown that there is a strong relation \nbetween the index of a critical point (number of dimensions across which the point behaves like a \nlocal maximum) and its height in the loss function (Dauphin et al, 2014). In particular, local minima \n(index=0) tend to concentrate at energy levels almost as low as the lowest height possible attained \nby the absolute minima, explaining why in practice an absolutely optimal set of weights does not \nneed to be found. The study of critical points in loss landscapes, such as minima and saddle points, \nhas provided insights into the workings of numerical optimization methods in deep learning. \nHowever, the understanding of the full trajectory of the network during learning, including the \neffects of the weights initialization, the speed of convergence, or number of hidden layers, will \nrequire a more detailed description of the loss landscape than just its critical points.    \nThe lens of computational complexity (Wigderson, 2019) can also guide us to better compare across \ndeep learning algorithms in their efficiency at solving tasks. Recently, several deep learning \nalgorithms have even been ascribed the term of human-level or superhuman for their comparison to \nhuman performance when solving the same task. But it is essential that we distil how much of their \nsuccess can be attributed to the data (amount, richness of learning experiences), the algorithm \n(architecture, learning and decision making rules), the computational resources (amount of memory, \ntime, bandwidth), and even hardware (physical ability to respond fast and accurately). While \nperformance can measure the overall success at solving a task, computational complexity reminds \nus that a fair comparison between algorithms requires also to quantify the computational resources \nconsumed by the algorithms (in machines and humans) and how these scale up. This strategy will \nrequire to go beyond simply reporting the final performance and computer time needed to solve a \ntask, but a systematic study of how the performance varies when changing the complexity of the \nproblem and the available resources. Only then we will better understand where the merits and \ndisadvantages of each system lay.\n2.7.     Neuroscience Perspective \nThe language of deep learning is filled with terms such as neural network, attention, short-term \nmemory, episodic memory, reinforcement learning and many others which have been borrowed \nfrom neuroscience and cognitive sciences. Historically, human and animal brains and cognition has \nbeen the main source of inspiration for connectionist models such as deep learning. Indeed, the \nbasic scheme that information is processed by nodes receiving inputs from other nodes, and that \nlearning proceeds by modifications of such interconnections, possibly routing the information \ndifferently as one learns, goes back to the times of Ramón y Cajal when the modern study of \nneuroscience and psychology flourished. However, it took many decades to amass solid knowledge \nfor neuroscience to inspire the first formal models and implementations of primitive artificial neural \nnetworks. In the hands of McCulloch, Pitts, and Rosenblath the fundamental ideas of such network \nmodels took shape and still remain the backbone of deep learning.  \nSince then more detailed phenomena and concepts first observed in neuroscience have also been \nused in artificial neural network models. Activation functions, such as ReLU among others \n(Hanhloser et al, 2000), connectivity patterns such as feedback connections (Lotter et al, 2016), or \ndifferent plasticity and learning rules described in biological networks (Kirkpatrick et al, 2017) have \nbeen continuously transferred to their artificial counterparts. However, recreating a physiologically \nplausible brain might not always be the only guide and it can be a misleading strategy if \noverengineered (Theil, 2015). Biological networks evolved under different pressures and real \nneurons have a range of functions and constraints related to their biological origin that might not be \nwise to mimic in artificial networks in silico. Artificial neurons do not have to worry about \nmetabolism, homeostasis or the general task of being alive. Hence, at the implementation level we \nface the challenge to distinguish, within the vast but incomplete sea of biological data, which \nbiological processes realize an important computational function for the task at hand, and which \nreflect other functions or are simply side-effects. Luckily the transfer of ideas can also occur at \nhigher levels of analysis than the implementation level (Marr, D. and Poggio, T., 1976). \nIndeed, psychology and cognitive science have provided us with a rich source of inspiration for \ncomputational modules without an accompanying knowledge on how these are actually \nimplemented in brains. Working at the level of representational structures and computational \nprocesses these disciplines have identified many necessary or sufficient functions for solving \ndifferent tasks without an attachment to any specific \"hardware\" or mechanistic implementation. \nThus, constructs relying on computational and representational structures such as attention layers, \nsaliency maps, or different types of memory have been incorporated as key components of deep \nlearning models (Hassabis et al, 2017). Indeed, the design and understanding of complex deep \nlearning architectures is often driven by intuitions on such functional and modular levels while \nabstracting from particular implementations.  \nMethods to analyze biological neural signals have also being transferred to study artificial networks. \nFor example, the concept of receptive fields has been widely used in neuroscience to characterize \nthe activation of neurons by sensory stimuli. This concept has also been used to explain and \nvisualize the activations of artificial neurons in the presence of a complex and specific constellation \nof stimuli. Such neurons are often realized near the output layer of a classifier and can be \nunderstood by the formation of a hierarchy of increasingly complex receptive fields from the \nconjunction of simpler ones at previous levels (Gross, 2002). Moreover, the discoveries by \nneuroscientists of biological neurons with striking receptive fields such as grid cells (Moser et al, \n2008), mirror neurons (Rizzolatti, G., and Craighero, L., 2004), or simply cells that respond to a \nhigh-level concept regardless of the input modality (Quiroga et al, 2005), continue to inspire new \ncomputational models (Banino et al, 2018).  \nMore than with any other discipline, the bridge between AI and neuroscience has been a two-way \nroad with advances in each discipline fueling new ideas in the other.  However, a word of caveat \nmight be due. Brains and artificial networks share some obvious similarities but they are systems of \ndifferent complexity operating on different substrates which evolved under different pressures and \npossess different computational resources. Therefore, not all the state of the art results obtained by \ndeep learning must launch a desperate search for analog mechanisms in the brain. In the same way, \nnot all brain processes must be mimicked to reverse-engineer human intelligence. Nevertheless, the \ncomputational and representational similarities between deep learning algorithms and brains when \nthey are subject to similar tasks and computational resources can be an excellent guide for better \nunderstanding how each system works at the algorithmic level, which can then constrain the space \nof possible mechanisms. Hopefully, crossing the bridge in both directions will bring us to narrow \nthe gap in our understanding of what brains compute and create technology that improves our lives. \n3.       Discussion \nLooking at a problem through multiple angles can stimulate novel ideas. Our discussion on each \nperspective and examples of intuitions have been necessarily shallow due to the multiple views that \nhad to be covered. The deepness in this case should come from putting all these faces of deep \nlearning together in the reader's mind and entertain their interrelations. We hope that such multiple \nviews will help practitioners to get interested in different communities to complement their own \nviews and intuitions on how deep learning works. \n4.      Conflict of Interest \nThe authors declare that the research was conducted in the absence of any commercial or financial \nrelationships that could be construed as a potential conflict of interest. \n5.       Funding \nRV thanks the financial support by the Estonian Research Council (project number PUT 1476), and \nthe Estonian Centre of Excellence in IT (EXCITE) (project number TK148). \n6.       Acknowledgments \nRV thanks all members of the group of computational neuroscience (Jaan Aru, Tambet Matiisen, \nArdi Tampuu, Ilya Kuzovkin, Daniel Majoral, Aqeel Labash, Oriol Andreu, Roman Ring, Kristjan \nKorjus, Abdullah Makkeh) for enlightening discussions and patient explanations over the years on \nmany of the concepts covered in this article. \n7.      References \nSchmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural networks, 61, \n85-117. \nGoodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT press. \nMontufar, G. F., Pascanu, R., Cho, K., & Bengio, Y. (2014). On the number of linear regions of \ndeep neural networks. In Advances in neural information processing systems (pp. 2924-2932). \nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... & Bengio, Y. \n(2014). Generative adversarial nets. In Advances in neural information processing systems (pp. \n2672-2680). \nArjovsky, M., Chintala, S., & Bottou, L. (2017, July). Wasserstein generative adversarial networks. \nIn International conference on machine learning (pp. 214-223). \nYuan, X., He, P., Zhu, Q., & Li, X. (2019). Adversarial examples: Attacks and defenses for deep \nlearning. IEEE transactions on neural networks and learning systems. \nAthalye, A., Carlini, N., & Wagner, D. (2018). Obfuscated gradients give a false sense of security: \nCircumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420. \nGoodfellow, I. J., Shlens, J., & Szegedy, C. (2014). Explaining and harnessing adversarial \nexamples. arXiv preprint arXiv:1412.6572. \nCamacho-Collados, J., & Pilehvar, M. T. (2018). From word to sense embeddings: A survey on \nvector representations of meaning. Journal of Artificial Intelligence Research, 63, 743-788. \nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations \nof words and phrases and their compositionality. In Advances in neural information processing \nsystems (pp. 3111-3119). \nCover, T. M., & Thomas, J. A. (2012). Elements of information theory. John Wiley & Sons. \nTishby, N., & Zaslavsky, N. (2015, April). Deep learning and the information bottleneck principle. \nIn 2015 IEEE Information Theory Workshop (ITW) (pp. 1-5). IEEE. \nDoya, K., Ishii, S., Pouget, A., & Rao, R. P. (Eds.). (2007). Bayesian brain: Probabilistic \napproaches to neural coding. MIT press. \nCalude, C. S., & Chaitin, G. J. (2007). Randomness and complexity: from Leibniz to Chaitin. World \nScientific. \nBertschinger, N., Rauh, J., Olbrich, E., Jost, J., & Ay, N. (2014). Quantifying unique \ninformation. Entropy, 16(4), 2161-2183. \nPearl, J. (2019). The seven tools of causal inference, with reflections on machine \nlearning. Commun. ACM, 62(3), 54-60. \nLake, B. M., Ullman, T. D., Tenenbaum, J. B., & Gershman, S. J. (2017). Building machines that \nlearn and think like people. Behavioral and brain sciences, 40. \nGuo, R., Cheng, L., Li, J., Hahn, P. R., & Liu, H. (2018). A Survey of Learning Causality with Data: \nProblems and Methods. arXiv preprint arXiv:1809.09337. \nPearl, J., & Mackenzie, D. (2018). The book of why: the new science of cause and effect. Basic \nBooks. \nSutton, R. S. & Barto, A. G. (2018). Reinforcement learning: an introduction. Second edition. MIT \nPress. \nArulkumaran, K., Deisenroth, M. P., Brundage, M., & Bharath, A. A. (2017). A brief survey of deep \nreinforcement learning. arXiv preprint arXiv:1708.05866. \nZador, A. M. (2019). A critique of pure learning and what artificial neural networks can learn from \nanimal brains. Nature Communications, 10(1), 1-7. \nOudeyer, P. Y., & Kaplan, F. (2009). What is intrinsic motivation? A typology of computational \napproaches. Frontiers in neurorobotics, 1, 6. \nJohnson, S. G., & Keil, F. C. (2014). Causal inference and the hierarchical structure of \nexperience. Journal of Experimental Psychology: General, 143(6), 2223. \nSompolinsky, H. (1988). Statistical mechanics of neural networks. Physics Today, 41(12), 70-80. \nMézard, M., & Mora, T. (2009). Constraint satisfaction problems and neural networks: A statistical \nphysics perspective. Journal of Physiology-Paris, 103(1-2), 107-113. \nAdvani, M., Lahiri, S., & Ganguli, S. (2013). Statistical mechanics of complex neural systems and \nhigh dimensional data. Journal of Statistical Mechanics: Theory and Experiment, 2013(03), \nP03014. \nLeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., & Huang, F. (2006). A tutorial on energy-based \nlearning. Predicting structured data, 1(0). \nAckley, D. H., Hinton, G. E., & Sejnowski, T. J. (1985). A learning algorithm for Boltzmann \nmachines. Cognitive science, 9(1), 147-169. \nHinton, G. E. (2002). Training products of experts by minimizing contrastive divergence. Neural \ncomputation, 14(8), 1771-1800. \nSalakhutdinov, R., & Hinton, G. (2009, April). Deep boltzmann machines. In Artificial intelligence \nand statistics (pp. 448-455). \nMehta, P., & Schwab, D. J. (2014). An exact mapping between the variational renormalization \ngroup and deep learning. arXiv preprint arXiv:1410.3831. \nKoch-Janusz, M., & Ringel, Z. (2018). Mutual information, neural networks and the \nrenormalization group. Nature Physics, 14(6), 578. \nChoromanska, A., Henaff, M., Mathieu, M., Arous, G. B., & LeCun, Y. (2015, February). The loss \nsurfaces of multilayer networks. In Artificial Intelligence and Statistics (pp. 192-204). \nDauphin, Y. N., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., & Bengio, Y. (2014). Identifying \nand attacking the saddle point problem in high-dimensional non-convex optimization. In Advances \nin neural information processing systems (pp. 2933-2941). \nSimonotto, E., Riani, M., Seife, C., Roberts, M., Twitty, J., & Moss, F. (1997). Visual perception of \nstochastic resonance. Physical review letters, 78(6), 1186. \nGammaitoni, L., Hänggi, P., Jung, P., & Marchesoni, F. (1998). Stochastic resonance. Reviews of \nmodern physics, 70(1), 223. \nPikovsky, A. S., & Kurths, J. (1997). Coherence resonance in a noise-driven excitable \nsystem. Physical Review Letters, 78(5), 775. \nKirkpatrick, S., Gelatt, C. D., & Vecchi, M. P. (1983). Optimization by simulated \nannealing. Science, 220(4598), 671-680. \nHochreiter, S., & Schmidhuber, J. (1995). Simplifying neural nets by discovering flat minima. \nIn Advances in neural information processing systems (pp. 529-536). \nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: a \nsimple way to prevent neural networks from overfitting. The journal of machine learning \nresearch, 15(1), 1929-1958. \nBlum, A., & Rivest, R. L. (1989). Training a 3-node neural network is NP-complete. In Advances in \nneural information processing systems (pp. 494-501). \nWigderson, A. (2019). Mathematics and Computation. Princeton University Press. \nHahnloser, R. H., Sarpeshkar, R., Mahowald, M. A., Douglas, R. J., & Seung, H. S. (2000). Digital \nselection and analogue amplification coexist in a cortex-inspired silicon circuit. Nature, 405(6789), \n947. \nLotter, W., Kreiman, G., & Cox, D. (2016). Deep predictive coding networks for video prediction \nand unsupervised learning. arXiv preprint arXiv:1605.08104. \nKirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., ... & Hassabis, \nD. (2017). Overcoming catastrophic forgetting in neural networks. Proceedings of the national \nacademy of sciences, 114(13), 3521-3526. \nTheil, S. (2015). Why the Human Brain Project went wrong—and how to fix it. Scientific \nAmerican, 313(4), 36-42. \nMarr, D., & Poggio, T. (1976). From understanding computation to understanding neural circuitry. \nHassabis, D., Kumaran, D., Summerfield, C., & Botvinick, M. (2017). Neuroscience-inspired \nartificial intelligence. Neuron, 95(2), 245-258. \nGross, C. G. (2002). Genealogy of the “grandmother cell”. The Neuroscientist, 8(5), 512-518. \nMoser, E. I., Kropff, E., & Moser, M. B. (2008). Place cells, grid cells, and the brain's spatial \nrepresentation system. Annu. Rev. Neurosci., 31, 69-89. \nRizzolatti, G., & Craighero, L. (2004). The mirror-neuron system. Annu. Rev. Neurosci., 27, \n169-192. \nQuiroga, R. Q., Reddy, L., Kreiman, G., Koch, C., & Fried, I. (2005). Invariant visual \nrepresentation by single neurons in the human brain. Nature, 435(7045), 1102. \nBanino, A., Barry, C., Uria, B., Blundell, C., Lillicrap, T., Mirowski, P., ... & Wayne, G. (2018). \nVector-based navigation using grid-like representations in artificial agents. Nature, 557(7705), 429.\n",
  "categories": [
    "cs.LG",
    "physics.data-an",
    "q-bio.NC",
    "stat.ML"
  ],
  "published": "2019-08-25",
  "updated": "2019-08-25"
}