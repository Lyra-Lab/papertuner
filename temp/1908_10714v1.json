{
  "id": "http://arxiv.org/abs/1908.10714v1",
  "title": "Automated Architecture Design for Deep Neural Networks",
  "authors": [
    "Steven Abreu"
  ],
  "abstract": "Machine learning has made tremendous progress in recent years and received\nlarge amounts of public attention. Though we are still far from designing a\nfull artificially intelligent agent, machine learning has brought us many\napplications in which computers solve human learning tasks remarkably well.\nMuch of this progress comes from a recent trend within machine learning, called\ndeep learning. Deep learning models are responsible for many state-of-the-art\napplications of machine learning. Despite their success, deep learning models\nare hard to train, very difficult to understand, and often times so complex\nthat training is only possible on very large GPU clusters. Lots of work has\nbeen done on enabling neural networks to learn efficiently. However, the design\nand architecture of such neural networks is often done manually through trial\nand error and expert knowledge. This thesis inspects different approaches,\nexisting and novel, to automate the design of deep feedforward neural networks\nin an attempt to create less complex models with good performance that take\naway the burden of deciding on an architecture and make it more efficient to\ndesign and train such deep networks.",
  "text": "Automated Architecture Design for\nDeep Neural Networks\nby\nSteven Abreu\nJacobs University Bremen\nBachelor Thesis in Computer Science\nProf. Herbert Jaeger\nBachelor Thesis Supervisor\nDate of Submission: May 17th, 2019\nJacobs University — Focus Area Mobility\narXiv:1908.10714v1  [cs.LG]  22 Aug 2019\nWith my signature, I certify that this thesis has been written by me using only the in-\ndicated resources and materials. Where I have presented data and results, the data and\nresults are complete, genuine, and have been obtained by me unless otherwise acknowl-\nedged; where my results derive from computer programs, these computer programs have\nbeen written by me unless otherwise acknowledged. I further conﬁrm that this thesis has\nnot been submitted, either in part or as a whole, for any other academic degree at this or\nanother institution.\nSignature\nPlace, Date\nAbstract\nMachine learning has made tremendous progress in recent years and received large amounts\nof public attention. Though we are still far from designing a full artiﬁcially intelligent\nagent, machine learning has brought us many applications in which computers solve human\nlearning tasks remarkably well. Much of this progress comes from a recent trend within\nmachine learning, called deep learning. Deep learning models are responsible for many\nstate-of-the-art applications of machine learning.\nDespite their success, deep learning models are hard to train, very diﬃcult to understand,\nand often times so complex that training is only possible on very large GPU clusters.\nLots of work has been done on enabling neural networks to learn eﬃciently. However,\nthe design and architecture of such neural networks is often done manually through trial\nand error and expert knowledge. This thesis inspects diﬀerent approaches, existing and\nnovel, to automate the design of deep feedforward neural networks in an attempt to create\nless complex models with good performance that take away the burden of deciding on an\narchitecture and make it more eﬃcient to design and train such deep networks.\niii\nContents\n1\nMotivation\n1\n1.1\nRelevance of Machine Learning . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n1.2\nRelevance of Deep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n1.2.1\nIneﬃciencies of Deep Learning\n. . . . . . . . . . . . . . . . . . . . .\n1\n1.3\nNeural Network Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n2\nIntroduction\n3\n2.1\nSupervised Machine Learning . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n2.2\nDeep Learning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n2.2.1\nArtiﬁcial Neural Networks . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.2.2\nFeedforward Neural Networks . . . . . . . . . . . . . . . . . . . . . .\n4\n2.2.3\nNeural Networks as Universal Function Approximators . . . . . . . .\n4\n2.2.4\nRelevance of Depth in Neural Networks\n. . . . . . . . . . . . . . . .\n6\n2.2.5\nAdvantages of Deeper Neural Networks\n. . . . . . . . . . . . . . . .\n7\n2.2.6\nThe Learning Problem in Neural Networks\n. . . . . . . . . . . . . .\n8\n3\nAutomated Architecture Design\n9\n3.1\nNeural Architecture Search\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n3.1.1\nNon-Adaptive Search - Grid and Random Search . . . . . . . . . . .\n10\n3.1.2\nAdaptive Search - Evolutionary Search . . . . . . . . . . . . . . . . .\n10\n3.2\nDynamic Learning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n3.2.1\nRegularization Methods . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n3.2.2\nDestructive Dynamic Learning\n. . . . . . . . . . . . . . . . . . . . .\n12\n3.2.3\nConstructive Dynamic Learning . . . . . . . . . . . . . . . . . . . . .\n14\n3.2.4\nCombined Destructive and Constructive Dynamic Learning . . . . .\n17\n3.3\nSummary\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n4\nEmpirical Findings\n19\n4.1\nOutline of the Investigation . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n4.1.1\nInvestigated Techniques for Automated Architecture Design . . . . .\n19\n4.1.2\nBenchmark Learning Task . . . . . . . . . . . . . . . . . . . . . . . .\n20\n4.1.3\nEvaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n4.1.4\nImplementation Details\n. . . . . . . . . . . . . . . . . . . . . . . . .\n21\n4.2\nSearch Algorithms\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n4.2.1\nManual Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n4.2.2\nRandom Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n4.2.3\nEvolutionary Search . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n4.2.4\nConclusion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n4.3\nConstructive Dynamic Learning Algorithm\n. . . . . . . . . . . . . . . . . .\n30\n4.3.1\nCascade-Correlation Networks\n. . . . . . . . . . . . . . . . . . . . .\n30\n4.3.2\nForward Thinking\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n4.3.3\nAutomated Forward Thinking . . . . . . . . . . . . . . . . . . . . . .\n40\n4.3.4\nConclusion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\n4.4\nConclusion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n5\nFuture Work\n45\niv\n1\nMotivation\n1.1\nRelevance of Machine Learning\nMachine Learning has made tremendous progress in recent years. Although we are not\nable to replicate human-like intelligence with current state-of-the-art systems, machine\nlearning systems have outperformed humans in some domains. One of the ﬁrst important\nmilestones has been achieved when DeepBlue defeated the world champion Garry Kasparov\nin a game of chess in 1997. Machine learning research has been highly active since then\nand pushed the state-of-the-art in domains like image classiﬁcation, text classiﬁcation,\nlocalization, question answering, natural language translation and robotics further.\n1.2\nRelevance of Deep Learning\nMany of today’s state-of-the-art systems are powered by deep neural networks (see Section\n2.2). AlphaZero’s deep neural network coupled with a reinforcement learning algorithm\nbeat the world champion in Go - a game that was previously believed to be too complex\nto be played competitively by a machine [Silver et al., 2018]. Deep learning has also been\napplied to convolutional neural networks - a special kind of neural network architecture\nthat was initially proposed by Yann LeCun [LeCun and Bengio, 1998]. One of these deep\nconvolutional neural networks, using ﬁve layers, has been used to achieve state-of-the-art\nperformance in image classiﬁcation [Krizhevsky et al., 2017].\nOverfeat, an eight layer\ndeep convolutional neural network, has been trained on image localization, classiﬁcation\nand detection with very competitive results [Sermanet et al., 2013]. Another remarkably\ncomplex CNN has been trained with 29 convolutional layers to beat the state of the art in\nseveral text classiﬁcation tasks [Conneau et al., 2016]. Even a complex task that requires\ncoordination between vision and control, such as screwing a cap on a bottle, has been solved\ncompetitively using such deep architectures. Levine et al. [2016] used a deep convolutional\nneural network to represent policies to solve such robotic tasks. Recurrent networks are\nparticularly popular in time series domains. Deep recurrent networks have been trained\nto achieve state-of-the-art performance in generating captions for given images [Vinyals\net al., 2015]. Google uses a Long Short Term Memory (LSTM) network to achieve state-\nof-the-art performance in machine translation [Wu et al., 2016].\nOther deep network\narchitectures have been proposed and successfully achieved state-of-the-art performance,\nsuch as dynamic memory networks for natural language question answering [Kumar et al.,\n2016].\n1.2.1\nIneﬃciencies of Deep Learning\nEvidently, deep neural networks are currently powering many, if not most, state-of-the-art\nmachine learning systems. Many of these deep learning systems train model that are richer\nthan needed and use elaborate regularization techniques to keep the neural network from\noverﬁtting on the training data.\nMany modern deep learning systems achieve state-of-the-art performance using highly\ncomplex models by investing large amounts of GPU power and time as well as feeding\nthe system very large amounts of data. This has been made possible through the recent\n1\nexplosion of computational power as well as through the availability of large amounts of\ndata to train these systems.\nIt can be argued that deep learning is ineﬃcient because it trains bigger networks than\nneeded for the function that one desires to learn. This comes at a high expense in the\nform of computing power, time and the need for larger training datasets.\n1.3\nNeural Network Design\nThe goal of designing a neural network is manifold. The primary goal is to minimize the\nneural network’s expected loss for the learning task. Because the expected loss cannot\nalways be computed in practice, this goal is often re-deﬁned to minimizing the loss on a\nset of unseen test data.\nAside from maximizing performance, it is also desirable to minimize the resources needed\nto train this network. I diﬀerentiate between computational resources (such as computing\npower, time and space) and human resources (such as time and eﬀort).\nIn my opinion, the goal of minimizing human resources is often overlooked. Many models,\nespecially in deep learning, are designed through trial, error and expert knowledge. This\nmanual design process is rarely interpretable or reproducible and as such, little formal\nknowledge is gained about the working of neural networks - aside from having a neural\nnetwork design that may work well for a speciﬁc learning task.\nIn order to avoid the diﬃculties of deﬁning and assessing the amount of human resources\nneeded for the neural network design process, I am introducing a new goal for the design of\nneural networks: level of automaticity. The level of automaticity in neural network design\nis inversely proportional to the number of decision that need to be made by a human in\nthe neural network design process.\nWhen dealing with computational resources for neural networks, one might naturally focus\non optimizing the amount of computational resources needed during the training process.\nHowever, the amount of resources needed for utilizing the neural network in practice are\nalso very important. A neural network is commonly trained once and then used many times\nonce it is trained. The computational resources needed for the utilization of the trained\nneural network sums up and should be considered when designing a neural network. A\ngood measure is to reduce the model complexity or network size. This goal reduces the\ncomputational resources needed for the neural network in practice while simultaneously\nacting as a regularizer to incentivize neural networks to be smaller - hence prefering simpler\nmodels over more complex ones, as Occam’s razor states.\nTo conclude, the goal of designing a neural network is to maximize performance (usually\nby minimizing a chosen loss function on unseen test data), minimize computational re-\nsources (during training), maximize the level of automaticity (by minimizing the amount\nof decisions that need to be made by a human in the design process), and to minimize the\nmodel’s complexity (e.g. by minimizing the network’s size).\n2\n2\nIntroduction\n2.1\nSupervised Machine Learning\nIn this paper, I will be focusing on supervised machine learning. In supervised machine\nlearning, one tries to estimate a function\nf : EX 7→EY\nwhere typically EX ⊆Rm and EY ⊆Rn, given training data in the form of (xi, yi)i=1,..,N,\nwith yi ≈f(xi). This training data represents existing input-output pairs of the function\nthat is to be estimated.\nA machine learning algorithm takes the training data as input and outputs a function\nestimate fest with fest ≈f.\nThe goal of the supervised machine learning task is to\nminimize a loss function L:\nL : EY × EY 7→R≥0\nIn order to assess a function estimate’s accuracy, it should always be assessed on a set of\nunseen input-output pairs. This is due to overﬁtting, a common phenomenon in machine\nlearning in which a machine learning model memorizes part of the training data which\nleads to good performance on the training set and (often) bad generalization to unseen\npatterns. One of the biggest challenges in machine learning is to generalize well. It is\ntrivial to memorize training data and correctly classifying these memorized samples. The\nchallenge lies in correctly classifying previously unseen samples, based on what was seen\nin the training dataset.\nA supervised machine learning problem is speciﬁed by labeled training data (xi, yi)i=1,..,N\nwith xi ∈EX, yi ∈EY and a loss function which is to be minimized. Often times, the loss\nfunction is not part of the problem statement and instead needs to be deﬁned as part of\nsolving the problem.\nGiven training data and the loss function, one needs to decide on a candidate set C of\nfunctions that will be considered when estimating the function f.\nThe learning algorithm L is an eﬀective procedure to choose one or more particular func-\ntions as an estimate for the given function estimation task, minimizing the loss function\nin some way:\nL(C, L, (xi, yi)i=1,..N) ∈C\nTo summarize, a supervised learning problem is given by a set of labeled data points\n(xi, yi)i=1,..N which one typically calls the training data. The loss function L gives us a\nmeasure for how good a prediction is compared to the true target value and it can be\nincluded in the problem statement. The supervised learning task is to ﬁrst decide on a\ncandidate set C of functions that will be considered. Finally, the learning algorithm L\ngives an eﬀective procedure to choose one function estimate as the solution to the learning\nproblem.\n2.2\nDeep Learning\nDeep learning is a subﬁeld of machine learning that deals with deep artiﬁcial neural net-\nworks. These artiﬁcial neural networks (ANNs) can represent arbitrarily complex func-\ntions (see section 2.2.3).\n3\n2.2.1\nArtiﬁcial Neural Networks\nAn artiﬁcial neural network (ANN) (or simply, neural network) consists of a set V of\nv = |V | processing units, or neurons. Each neuron performs a transfer function of the\nform\nyi = fi\n\n\nn\nX\nj=1\nwijxj −θi\n\n\nwhere yi is the output of the neuron, fi is the activation function (usually a nonlinear\nfunction such as the sigmoid function), xj is the output of neuron j, wij is the connection\nweight from node j to node i and θi is the bias (or threshold) of the node. Input units are\nconstant, reﬂecting the function input values. Output units do not forward their output\nto any other neurons. Units that are neither input nor output units are called hidden\nunits.\nThe entire network can be described by a directed graph G = (V, E) where the directed\nedges E are given through a weight matrix W ∈Rv×v. Any non-zero entry in the weight\nmatrix at index (i, j), i.e. wij ̸= 0 denotes that there is a connection from neuron j to\nneuron i.\nA neural network is deﬁned by its architecture, a term that is used in diﬀerent ways. In\nthis paper, the architecture of a neural network will always refer to the network’s node\nconnectivity pattern and the nodes’ activation functions.\nANN’s can be segmented into feedforward and recurrent networks based on their network\ntopology. An ANN is feedforward if there exists an ordering of neurons such that every\nneuron is only connected to a neuron further down the ordering.\nIf such an ordering\ndoes not exist, then the network is recurrent. In this thesis, I will only be considering\nfeedforward neural networks.\n2.2.2\nFeedforward Neural Networks\nA feedforward network can be visualized as a layered network, with layers L0 through LK.\nThe layer L0 is called the input layer and LK is called the output layer. Intermediate\nlayers are called hidden layers.\nOne can think of the layers as subsequent feature extractors: the ﬁrst hidden layer L1 is\na feature extractor on the input unit. The second hidden layer L2 is a feature extractor\non the ﬁrst hidden layer - thus a second order feature extractor on the input. The hidden\nlayers can compute increasingly complex features on the input.\n2.2.3\nNeural Networks as Universal Function Approximators\nA classical universal approximation theorem states that standard feedforward neural net-\nworks with only one hidden layer using a squashing activation function (a function Ψ :\nR 7→[0, 1] is a squashing function, according to Hornik et al. [1989], if it is non-decreasing,\nΨλ→∞(λ) = 1 and Ψλ→−∞(λ) = 0) can be used to approximate any continuous function\non compact subsets of Rn with any desired non-zero amount of error [Hornik et al., 1989].\nThe only requirement is that the network must have suﬃciently many units in its hidden\nlayer.\n4\nA simple example can demonstrate this universal approximation theorem for neural net-\nworks. Consider the binary classiﬁcation problem in Figure 1 of the kind f : [0, 1]2 →\n{0, 1}. The function solving this classiﬁcation problem can be represented using an MLP.\nAs stated by the universal approximation theorem, one can approximate this function to\narbitrary precision using an MLP with one hidden layer.\nFigure 1: Binary classiﬁcation problem. Yellow area is one class, everything else is the\nother class. Right is the shallow neural network that should represent the classiﬁcation\nfunction. Figure taken from Bhiksha Raj’s lecture slides in CMU’s ’11-785 Introduction\nto Deep Learning’.\nThe diﬃculty in representing the desired classiﬁcation function is that the classiﬁcation\nis split into two separate, disconnected decision regions. Representing either one of these\nshapes is trivial. One can add one neuron per side of the polygon which acts as a feature\ndetector to detect the decision boundary represented by this side of the polygon. One can\nthen add a bias into the hidden layer with a value of bh = −N (N is the number of sides of\nthe polygon), use a relu-activated output unit and one has built a simple neural network\nwhich returns 1 iﬀall hidden neurons ﬁre, i.e. when the point lies within the boundary of\nevery side of the polygon, i.e. when the point lies within the polygon.\n(a) Decision bound-\nary for a square\n(b) Decision bound-\nary for a hexagon\n(c) Decision plot for\na square\n(d)\nDecision\nplot\nfor a hexagon\nFigure 2: Decision plots and boundaries for simple binary classiﬁcation problems. Figures\ntaken from Bhiksha Raj’s lecture slides in CMU’s ’11-785 Introduction to Deep Learning’.\nThis approach generalizes neither to shapes that are not convex nor to multiple, discon-\nnected shapes.\nIn order to approximate any decision boundary using just one hidden\nlayer, one can use an n-sided polygon. Figure 2a and 2b show the decision boundaries for\na square and a hexagon. A problem arises when the two shapes are close to each other; the\nareas outside the boundaries add up to values larger or equal to those within the bound-\naries of each shape. In the plots of Figure 2c and 2d, one can see that the boundaries of\n5\nthe decision regions don’t fall oﬀquickly enough and will add up to large values, if there\nare two or more such shapes in close proximity.\nFigure 3: Decision plot and corresponding MLP structure for approximating a circle.\nFigure taken from Bhiksha Raj’s lecture slides in CMU’s ’11-785 Introduction to Deep\nLearning’.\nHowever, as one increases the sides n of the polygon, the boundaries will fall oﬀmore\nquickly. In the limit of n →∞, the shape becomes a near perfect cylinder, with value n\nfor the area within the cylinder and n/2 outside. Using a bias unit of bh = −n/2, one can\nturn this into a near-circular shape with value n/2 in the shape and value 0 everywhere\nelse, as shown in Figure 3. One can now add multiple near-circles together in the same\nlayer of the neural network. Given this setup, one can now compose an arbitrary ﬁgure by\nﬁtting it with an arbitrary number of near-circles. The smaller these near-circles, the more\naccurate this classiﬁcation problem can be represented by a network. With this setup, it\nis possible to capture any decision boundary.\nThis procedure to build a neural network with one hidden layer to build a classiﬁer for\narbitrary ﬁgures has a problem: the number of hidden units needed to represent this\nfunction become arbitrarily high. In this procedure, I have set n, the number of hidden\nunits to represent a circle to be very large and I am using many of these circles to represent\nthe entire function. This will result in a very (very) large number of units in the hidden\nlayer.\nThis is a general phenomenon: even though a network with just one hidden layer can rep-\nresent any function (with some restrictions, see above) to arbitrary precision, the number\nof units in this hidden layer often becomes intractably large. Learning algorithms often\nfail to learn complicated functions correctly without overﬁtting the training data in such\n”shallow” networks.\n2.2.4\nRelevance of Depth in Neural Networks\nThe classiﬁcation function from Figure 1 can be built using a smaller network, if one allows\nfor multiple hidden layers. The ﬁrst layer is a feature detector for every polygon’s edge.\nThe second layer will act as an AND gate for every distinct polygon - detecting all those\npoints that lie within all the polygon’s edges. The output layer will then act as an OR\ngate for all neurons in the second layer, thus detecting all points that lie in any of the\npolygons. With this, one can build a simple network that perfectly represents the desired\nclassiﬁcation function. The network and decision boundaries are shown in Figure 4.\n6\nFigure 4: Decision boundary and corresponding two-layer classiﬁcation network. Figure\ntaken from Bhiksha Raj’s lecture slides in CMU’s ’11-785 Introduction to Deep Learning’.\nBy adding just one additional layer into the network, the number of hidden neurons has\nbeen reduced from nshallow →∞to ndeep = 12. This shows how the depth of a network\ncan increase the resulting model capacity faster than an increase in the number of units\nin the ﬁrst hidden layer.\n2.2.5\nAdvantages of Deeper Neural Networks\nIt is diﬃcult to understand how the depth of an arbitrary neural network inﬂuences what\nkind of functions the network can compute and how well these networks can be trained.\nEarly research has focused on shallow networks and their conclusions cannot be generalized\nto deeper architectures, such as the universal approximation theorem for networks with\none hidden layer [Hornik et al., 1989] or an analysis of a neural network’s expressivity\nbased on an analogy to boolean circuits by Maass et al. [1994].\nSeveral measures have been proposed to formalize the notion of model capacity and the\ncomplexity of functions which a statistical learning algorithm can represent. One of the\nmost famous such formalization is that of the Vapnik Chervonenkis dimension (VC di-\nmension) [Vapnik and Chervonenkis, 2015].\nRecent papers have focused on understanding the beneﬁts of depth in neural networks. The\nVC dimension as a measure of capacity has been applied to feedforward neural network\nwith piecewise polynomial activation functions, such as relu, to prove that a network’s\nmodel capacity grows by a factor of\nW\nlog W with depth compared to a similar growth in\nwidth [Bartlett et al., 1999].\nThere are examples of functions that a deeper network can express and a more shallow\nnetwork cannot approximate unless the width is exponential in the dimension of the input\n([Eldan and Shamir, 2016] and [Telgarsky, 2015]). Upper and lower bounds have been\nestablished on the network complexity for diﬀerent numbers of hidden units and activation\nfunctions. These show that deep architectures can, with the same number of hidden units,\nrealize maps of higher complexity than shallow architectures [Bianchini and Scarselli,\n2014].\nHowever, the aforementioned papers either do not take into account the depth of modern\ndeep learning models or only present ﬁndings for speciﬁc choices of weights of a deep\nneural network.\n7\nUsing Riemannian geometry and dynamical mean ﬁeld theory, Poole et al. [2016] show\nthat generic deep neural networks can ”eﬃciently compute highly expressive functions in\nways that shallow networks cannot” which ”quantiﬁes and demonstrates the power of deep\nneural networks to disentangle curved input manifolds” [Poole et al., 2016].\nRaghu et al. [2017] introduced the notion of a trajectory; given two points in the input\nspace x0, x1 ∈Rm, the trajectory x(t) is a curve parametrized by t ∈[0, 1] with x(0) = x0\nand x(1) = x1. They argue that the trajectory’s length serves as a measure of network\nexpressivity. By measuring the trajectory lengths of the input as it is transformed by\nthe neural network, they found that the network’s depth increases complexity (given by\nthe trajectory length) of the computed function exponentially, compared to the network’s\nwidth.\n2.2.6\nThe Learning Problem in Neural Networks\nA network architecture being able to approximate any function does not always mean\nthat a network of that architecture is able to learn any function. Whether or not neural\nnetwork of a ﬁxed architecture can be trained to represent a given function depends on\nthe learning algorithm used.\nThe learning algorithm needs to ﬁnd a set of parameters for which the neural network\ncomputes the desired function. Given a function, there exists a neural network to represent\nthis function. But even if such an architecture is given, there is no universal algorithm\nwhich, given training data, ﬁnds the correct set of parameters for this network such that\nit will also generalize well to unseen data points [Goodfellow et al., 2016].\nFinding the optimal neural network architecture for a given learning task is an unsolved\nproblem as well. Zhang et al. [2016] argue that most deep learning systems are built on\nmodels that are rich enough to memorize the training data.\nHence, in order for a neural network to learn a function from data, it has to learn the net-\nwork architecture and the parameters of the neural network (connection weights). This is\ncommonly done in sequence but it is also possible to do both simultaneously or iteratively.\n8\n3\nAutomated Architecture Design\nChoosing a ﬁtting architecture is a big challenge in deep learning. Choosing an unsuitable\narchitecture can make it impossible to learn the desired function. Choosing an optimal\narchitecture for a learning task is an unsolved problem. Currently, most deep learning\nsystems are designed by experts and the design relies on hyperparameter optimization\nthrough a combination of grid search and manual search [Bergstra and Bengio, 2012] (see\nLarochelle et al. [2007], LeCun et al. [2012], and Hinton [2012]).\nThis manual design is tedious, computationally expensive, and architecture decisions based\non experience and intuition are very diﬃcult to formalize and thus, reuse. Many algorithms\nhave been proposed for the architecture design of neural networks, with varying levels of\nautomaticity. In this thesis, I will be referring to these algorithms as automated architecture\ndesign algorithms.\nAutomated architecture algorithms can be broadly segmented into neural network archi-\ntecture search algorithms (also called neural architecture search, or NAS) and dynamic\nlearning algorithms, both of which are discussed in this section.\n3.1\nNeural Architecture Search\nNeural architecture search is a natural choice for the design of neural networks. NAS\nmethods are already outperforming manually designed architectures in image classiﬁcation\nand object detection ([Zoph et al., 2018] and [Real et al., 2018]).\nElsken et al. [2019] propose to categorize NAS algorithms according to three dimensions:\nsearch space, search strategy, and performance estimation strategy. The authors describe\nthese as follows. The search space deﬁnes the set of architectures that are considered by\nthe search algorithm. Prior knowledge can be incorporated into the search space, though\nthis may limit the exploration of novel architectures.\nThe search strategy deﬁnes the\nsearch algorithm that is used to explore the search space. The search algorithm deﬁnes\nhow the exploration-exploitation tradeoﬀis handled. The performance estimation strategy\ndeﬁnes how the performance of a neural network architecture is assessed. Naively, one may\ntrain a neural network architecture but this is object to random ﬂuctuations due to initial\nrandom weight initializations, and obviously very computationally expensive.\nIn this thesis, I will not be considering the search space part of the NAS algorithms.\nInstead, I will keep the search space constant across all NAS algorithms. I will not go in\ndepth about the performance estimation strategy in the algorithms either, instead using\none constant form of constant estimation - training a network architecture once for the\nsame number of epochs (depending on time constraints).\nMany search algorithms can be used in NAS algorithms.\nElsken et al. [2019] names\nrandom search, Bayesian optimization, evolutionary methods, reinforcement learning, and\ngradient-based methods. Search algorithms can be divided into adaptive and non-adaptive\nalgorithms, where adaptive search algorithms adapt future searches based on the perfor-\nmance of already tested instances. In this thesis, I will only consider grid search and\nrandom search as non-adaptive search algorithms, and evolutionary search as an adaptive\nsearch algorithm.\nFor the following discussion, let A be the set of all possible neural network architectures\n9\nand A′ ⊆A be the search space deﬁned for the NAS algorithm - a subset of all possible\narchitectures.\n3.1.1\nNon-Adaptive Search - Grid and Random Search\nThe simplest way to automatically design a neural network’s architecture may be to simply\ntry diﬀerent architectures from a deﬁned subset of all possible neural network architec-\ntures and choose the one that performs the best. One chooses elements ai ∈A′, tests these\nindividual architectures and chooses the one that performs the best. The performance is\nusually measured through evaluation on an unseen testing set or through a cross valida-\ntion procedure - a technique which artiﬁcially splits the training data into training and\nvalidation data and uses the unseen validation data to evaluate the model’s performance.\nThe two most widely known search algorithms that are frequently used for hyperparame-\nter optimization (which includes architecture search) are grid search and random search.\nNaive grid search performs an exhaustive, enumerated search within the chosen subset\nA′ of possible architectures - where one needs to also specify some kind of step size, a\ndiscretization scheme which determines how ”ﬁne” the search within the architecture sub-\nspace should be.\nAdaptive grid search algorithms use adaptive grid sizes and are not\nexhaustive. Random search does not need a discretization scheme, it chooses elements\nfrom A′ at random in each iteration. Both grid and random search are non-adaptive algo-\nrithms: they do not vary the course of the experiment by considering the performance of\nalready tested instances [Bergstra and Bengio, 2012]. Larochelle et al. [2007] ﬁnds that, in\nthe case of a 32-dimensional search problem of deep belief network optimization, random\nsearch was not as good as the sequential combination of manual and grid search from an\nexpert because the eﬃciency of sequential optimization overcame the ineﬃciency of the\ngrid search employed at every step [Bergstra and Bengio, 2012]. Bergstra and Bengio\n[2012] concludes that sequential, adaptive algorithms should be considered in future work\nand random search should be used as a performance baseline.\n3.1.2\nAdaptive Search - Evolutionary Search\nIn the past three decades, lots of research has been done on genetic algorithms and artiﬁcial\nneural networks. The two areas of research have also been combined and I shall refer to this\ncombination as evolving artiﬁcial neural networks (EANN), based on a literature review\nby Yao [1999]. Evolutionary algorithms have been applied to artiﬁcial neural networks to\nevolve connection weights, architectures, learning rules, or any combination of these three.\nThese EANN’s can be viewed as an adaptive system that is able to learn from data as\nwell as evolve (adapt) its architecture and learning rules - without human interaction.\nEvolutionary algorithms are population based search algorithms which are derived from the\nprinciples of natural evolution. They are very useful in complex domains with many local\noptima, as is the case in learning the parameters of a neural network [Choromanska et al.,\n2015]. They do not require gradient information which can be a computational advantage\nas the gradients for neural network weights can be quite expensive to compute, especially\nso in deep networks and recurrent networks. The simultaneous evolution of connection\nweights and network architecture can be seen as a fully automated ANN design. The\nevolution of learning rules can be seen as a way of ”learning how to learn”. In this paper,\n10\nI will be focusing on the evolution of neural network architectures, staying independent of\nthe algorithm that is used to optimize connection weights.\nThe two key issues in the design of an evolutionary algorithm are the representation and\nthe search operators. The architecture of a neural network is deﬁned by its nodes, their\nconnectivity and each node’s transfer function.\nThe architecture can be encoded as a\nstring in a multitude of ways, which will not be discussed in detail here.\nA general cycle for the evolution of network architectures has been proposed by Yao [1999]:\n1. Decode each individual in the current generation into an architecture.\n2. Train each ANN in the same way, using n distinct random initializations.\n3. Compute the ﬁtness of each architecture according to the averaged training results.\n4. Select parents from the population based on their ﬁtness.\n5. Apply search operators to parents and generate oﬀspring to form the next generation.\nIt is apparent that the performance of an EANN depends on the encoding scheme of the\narchitecture, the deﬁnition of the ﬁtness function, and the search operators applied to\nthe parents to generate oﬀspring. There will be some residual noise in the process due\nto the stochastic nature of ANN training. Hence, one should view the computed ﬁtness\nas a heuristic value, an approximation, for the true ﬁtness value of an architecture. The\nlarger the number n of diﬀerent random initializations that are run for each architecture,\nthe more accurate training results (and thus, the ﬁtness computation) becomes. However,\nincreasing n leads to a large increase in time needed for each iteration of the evolutionary\nalgorithm.\n3.2\nDynamic Learning\nDynamic learning algorithms in neural networks are algorithms that modify a neural\nnetwork’s hyperparameters and topology (here, I focus on the network architecture) dy-\nnamically as part of the learning algorithm, during training. These approaches present the\nopportunity to develop optimal network architectures that generalize well [Waugh, 1994].\nThe network architecture can be modiﬁed during training by adding complexity to the\nnetwork or by removing complexity from the network. The former is called a constructive\nalgorithm, the latter a destructive algorithm. Naturally, the two can be combined into an\nalgorithm that can increase and decrease the network’s complexity as needed, in so-called\ncombined dynamic learning algorithms. These changes can aﬀect the nodes, connections\nor weights of the network - a good overview of possible network changes is given by Waugh\n[1994], see Figure 5.\n3.2.1\nRegularization Methods\nBefore moving on to dynamic learning algorithms, it is necessary to clear up the clas-\nsiﬁcation of these dynamic learning algorithms and clarify some underlying terminology.\nThe set of destructive dynamic learning algorithms intersects with the set of so-called\nregularization methods in neural networks. The origin of this confusion is the deﬁnition\nof dynamic learning algorithms. Waugh [1994] deﬁnes dynamic learning algorithms to\nchange either the nodes, connections, or weights of the neural network. If we continue\n11\nFigure 5: Possible network topology changes, taken from Waugh [1994]\nwith this deﬁnition, we will include all algorithms that reduce the values of connections\nweights in the set of destructive dynamic learning, which includes regularization methods.\nRegularization methods penalize higher connection weights in the loss function (as a re-\nsult, connection weights are reduced in value). Regularization is based on Occam’s razor\nwhich states that the simplest explanation is more likely to be correct than more com-\nplex explanations. Regularization penalizes such complex explanations (by reducing the\nconnection weights’ values) in order to simplify the resulting model.\nRegularization methods include weight decay, in which a term is added to the loss function\nwhich penalizes large weights, and dropout, which is explained in Section 3.2.2.\nFor\ncompleteness, I will cover these techniques as instances of dynamic learning, however I\nwill not run any experiments on these regularization methods as the goal of this thesis\nis to inspect methods to automate the architecture design, for which the modiﬁcation of\nconnection weights is not relevant.\n3.2.2\nDestructive Dynamic Learning\nIn destructive dynamic learning, one starts with a network architecture that is larger than\nneeded and reduces complexity in the network by removing nodes, connections or reducing\nexisting connection weights.\nA key challenge in this destructive approach is the choice of starting network. As opposed\nto a minimal network - which could simply be a network without any hidden units - it is\ndiﬃcult to deﬁne a ”maximal” network because there is no upper bound on the network\nsize [Waugh, 1994]. A simple solution would be to choose a fully connected network with\nK layers, where K is dependent on the learning task.\nAn important downside to the use of destructive algorithms is the computational cost.\nStarting with a very large network and then cutting it down in size leads to many redundant\ncomputations on the large network.\nMost approaches to destructive dynamic learning that modify the nodes and connections\n(rather than just the connection weights) are concerned with the pruning of hidden nodes.\nThe general approach is to train a network that is larger than needed and prune parts of the\nnetwork that are not essential. Reed [1993] suggests that most pruning algorithms can be\n12\ndivided into two groups; algorithms that estimate the sensitivity of the loss function with\nrespect to the removal of an element and then removes those elements with the smallest\neﬀect on the loss function, and those that add terms to the objective function that rewards\nthe network for choosing the most eﬃcient solution - such as weight decay. I shall refer\nto those two groups of algorithms as sensitivity calculation methods and penalty-term\nmethods, respectively - as proposed by Waugh [1994].\nOther algorithms have been proposed but will not be included in this thesis for brevity\nreasons (most notably, principal components pruning [Levin et al., 1994] and soft weight-\nsharing as a more complex Penalty-Term method [Nowlan and Hinton, 1992]).\nDropout\nThis section follows Srivastava et al. [2014]. Dropout refers to a way of regularizing a\nneural network by randomly ”dropping out” entire nodes with a certain probability p in\neach layer of the network. At the end of training, each node’s outgoing weights are then\nmultiplied with its probability p of being dropped out. As the networks connection weights\nare multiplied with a certain probability value p, where p ∈[0, 1], one can consider this\ntechnique a kind of connection weight pruning and thus, in the following, I will consider\ndropout to be a destructive algorithm.\nIntuitively, dropout drives hidden units in a network to work with diﬀerent combinations\nof other hidden units, essentially driving the units to build useful features without relying\non other units. Dropout can be interpreted as a stochastic regularization technique that\nworks by introducing noise to its units.\nOne can also view this ”dropping out” in a diﬀerent way. If the network has n nodes\n(excluding output notes), dropout can either include or not include this node. This leads\nto a total of 2n diﬀerent network conﬁgurations. At each step during training, one of\nthese network conﬁgurations is chosen and the weights are optimized using some gradient\ndescent method. The entire training can hence be seen as training not just one network\nbut all possible 2n network architectures.\nIn order to get an ideal prediction from a\nﬂexible-sized model such as a neural network, one should average over the predictions of\nall possible settings of the parameters, weighing each setting by its posterior probability\ngiven the training data. This procedure quickly becomes intractable. In essence, dropout\nis a technique that can combine exponentially (exponential in the number of nodes) many\ndiﬀerent neural networks eﬃciently.\nDue to this model combination, dropout is reported to take 2-3 times longer to train than\na standard neural network without dropout. This makes dropout an eﬀective algorithm\nthat deals with a trade-oﬀbetween overﬁtting and training time.\nTo conclude, dropout can be seen as both a regularization technique and a form of model\naveraging.\nIt works remarkably well in practice.\nSrivastava et al. [2014] report large\nimprovements across all architectures in an extensive empirical study. The overall archi-\ntecture is not changed, as the pruning happens only in terms of the magnitude of the\nconnection weights.\nPenalty-Term Pruning through Weight Decay\n13\nWeight decay is the best-known regularization technique that is frequently used in deep\nlearning applications.\nIt works by penalizing network complexity in the loss function,\nthrough some complexity measure that is added into the loss function - such as the number\nof free parameters or the magnitude of connection weights. Krogh and Hertz [1992] show\nthat weight decay can improve generalization of a neural network by suppressing irrelevant\ncomponents of the weight vector and by suppressing some of the eﬀect of static noise on\nthe targets.\nSensitivity Calculation Pruning\nSietsma [1988] removes nodes which have little eﬀect on the overall network output and\nnodes that are duplicated by other nodes. The author also discusses removing entire layers,\nif they are found to be redundant [Waugh, 1994]. Skeletonization is based on the same\nidea of the network’s sensitivity to node removal and proposes to remove nodes from the\nnetwork based on their relevance during training [Mozer and Smolensky, 1989].\nOptimal brain damage (OBD) uses second-derivative information to automatically delete\nparameters based on the ”saliency” of each paramter - reducing the number of parameters\nby a factor of four and increasing its recognition accuracy slightly on a state-of-the-art\nnetwork [LeCun et al., 1990]. Optimal Brain Surgeon (OBS) enhances the OBD algorithm\nby dropping the assumption that the Hessian matrix of the neural network is diagonal (they\nreport that in most cases, the Hessian is actually strongly non-diagonal), and they report\neven better results [Hassibi et al., 1993]. The algorithm was extended again by the same\nauthors [Hassibi et al., 1994].\nHowever, methods based on sensitivity measures have the disadvantage that they do not\ndetect correlated elements - such as two nodes that cancel each other out and could be\nremoved without aﬀecting the networks performance [Reed, 1993].\n3.2.3\nConstructive Dynamic Learning\nIn constructive dynamic learning, one starts with a minimal network structure and itera-\ntively adds complexity to the network by adding new nodes or new connections to existing\nnodes.\nTwo algorithms for the dynamic construction of feed-forward neural networks are pre-\nsented in this section: the cascade-correlation algorithm (Cascor) and the forward thinking\nalgorithm.\nOther algorithms have been proposed but, for brevity, will not be included in this paper’s\nanalysis (node splitting [Wynne-Jones, 1992], the tiling algorithm [Mezard and Nadal,\n1989], the upstart algorithm [Frean, 1990], a procedure for determining the topology for\na three layer neural network [Wang et al., 1994], and meiosis networks that replace one\n”overtaxed” node by two nodes [Hanson, 1990]).\nCascade-correlation Networks\n14\nThe cascade-correlation learning architecture (short: Cascor) was proposed by Fahlman\nand Lebiere [1990]. It is a supervised learning algorithm for neural networks that contin-\nuously adds units into the network, trains them one by one and then freezes those unit’s\ninput connections. This results in a network that is not layered but has a structure in\nwhich all input units are connected to all hidden units and the hidden units have a hierar-\nchical ordering in which the one hidden unit’s output is fed into subsequent hidden units as\ninput. When training, Cascor keeps a ”pool” of candidate units - possibly using diﬀerent\nnonlinear activation functions - and chooses the best candidate unit. Figure 6 visualizes\nthis architecture. So-called residual neural networks have been very successful in tasks\nsuch as image recognition [He et al., 2016] through the use of similar skip connections.\nCascor takes the idea of skip connections and applies it to include network connections\nfrom the input to every hidden node in the network.\nFigure 6: The cascade correlation neural network architecture after adding two hidden\nunits. Squared connections are frozen after training them once, crossed connections are\nretrained in each training iteration. Figure taken and adapted from Fahlman and Lebiere\n[1990].\nCascor aims to solve two main problems that are found in the widely used backpropagation\nalgorithm: the step-size problem, and the moving target problem.\nThe step size problem occurs in gradient descent optimization methods because it is not\nclear how big the step in each parameter update should be. If the step size is too small,\nthe network takes too long to converge to a local minimum, if it is too large, the learning\nalgorithm will jump past local minima and possibly not converge to a good solution at all.\nAmong the most successful ways of dealing with this step size problem are higher-order\nmethods, which compute second derivatives in order to get a good estimate of what the\nstep size should be (which is very expensive and often times intractable), or some form of\n”momentum”, which keeps track of earlier steps taken to make an educated guess about\nhow large the step size should be at the current step.\nThe moving target problem occurs in most neural networks when all units are trained at\nthe same time and cannot communicate with each other. This leads to all units trying to\nsolve the same learning task - which changes constantly. Fahlman and Lebiere propose an\ninteresting manifestation of the moving target problem which they call the ”herd eﬀect”.\nGiven two sub-tasks, A and B, that must be performed by the hidden units in a network,\neach unit has to decide independently which of the two problems it will tackle. If task A\ngenerates a larger or more coherent error signal than task B, the hidden units will tend to\n15\nconcentrate on A and ignore B. Once A is solved, the units will then see B as a remaining\nsource of error.\nUnits will move towards task B and, in turn, problem A reappears.\nCascor aims to solve this moving target problem by only training one hidden unit at a\ntime. Other approaches, such as the forward thinking formulation, are less restricted and\nallow the training of one entire layer of units at a time [Hettinger et al., 2017].\nIn their original paper, Fahlman and Lebiere reported good benchmark results on the\ntwo-spirals problem and the n-input parity problem. The main advantages over networks\nusing backpropagation were faster training (though this might also be attributed to the\nuse of the Quickprop learning algorithm), deeper networks without problems of vanishing\ngradients, possibility of incremental learning and, in the n-input parity problem, fewer\nhidden units in total.\nIn the literature, Cascor has been criticized for poor performance on regression tasks due\nto an overcompensation of errors which comes from training on the error correlation rather\nthan on the error signal directly ([Littmann and Ritter, 1992], [Prechelt, 1997]). Cascor\nhas also been criticized for the use of its cascading structure rather than adding each\nhidden unit into the same hidden layer.\nLittmann and Ritter [1992] present a diﬀerent version of Cascor that is based on error\nminimization rather than error correlation maximization, called Caser. They also present\nanother modiﬁed version of Cascor, called Casqef, which is trained on error minimization\nand uses additional non-linear functions on the output of cascaded units. Caser doesn’t\ndo any better than Cascor, while Casqef outperforms Cascor in more complicated tasks\n- likely because of the additional nonlinearities introduced by the nonlinear functions on\nthe cascaded units.\nLittmann and Ritter [1993] show that Cascor is favorable for ”extracting information from\nsmall data sets without running the risk of overﬁtting” when compared with shallow broad\narchitectures that contain the same number of nodes. However, this comparison does not\ntake into account deep layered architectures that are popular in today’s deep learning\nlandscape.\nSjogaard [1991] suggests that the cascading of hidden units has no advantage over the\nsame algorithm adding each unit into the same hidden layer.\nPrechelt [1997] ﬁnds that Cascor’s cascading structure is sometimes better and sometimes\nworse than adding all the units into one single hidden layer - while in most cases it doesn’t\nmake a signiﬁcant diﬀerence. They also ﬁnd that training on covariance is more suitable\nfor classiﬁcation tasks while training on error minimization is more suitable for regression\ntasks.\nYang and Honavar [1998] ﬁnd that in their experiments, Cascor learns 1-2 orders of magni-\ntude faster than a network trained with backpropagation, results in substantially smaller\nnetworks and only a minor degradation of accuracy on the test data. They also ﬁnd that\nCascor has a large number of design parameters that need to be set, which is usually done\nthrough exploratory runs which, in turn, translates into increased computational costs.\nAccording to the authors, this might be worth it ”if the goal is to ﬁnd relatively small\nnetworks that perform the task well” but ”it can be impractical in situations where fast\nlearning is the primary goal”.\nMost of the literature available for Cascor is over 20 years old. Cascor seems to not have\nbeen actively investigated in recent years. Through email correspondence with the original\n16\npaper’s author, Scott E. Fahlman at CMU, and his PhD student Dean Alderucci, I was\nmade aware of the fact that research on Cascor has been inactive for over twenty years.\nHowever, Dean is currently working on establishing mathematical proofs involving how\nCascor operates, and adapting the recurrent version of Cascor tosentence classiﬁers and\npossibly language modeling. With my experiments, I am starting a preliminary investiga-\ntion into whether Cascor is still a promising learning algorithm after two decades.\nForward Thinking\nIn 2017, Hettinger et al. [2017] proposed a general framework for a greedy training of\nneural networks one layer at a time, which they call ”forward thinking”. They give a\ngeneral mathematical description of the forward thinking framework, in which one layer\nis added at a time, then trained on the desired output and ﬁnally added into the network\nwhile freezing the layer’s input weights and discarding its output weights. There are no\nskip connections, as in Cascor. The goal is to make the data ”more separable”, i.e. better\nbehaved after each layer.\nIn their experiments, Hettinger et al. [2017] used a fully-connected neural network with\nfour hidden layers to compare training using forward thinking against traditional back-\npropagation. They report similar test accuracy and higher training accuracy with the\nforward thinking network - which hints at overﬁtting, thus more needs to be done for\nregularization in the forward thinking framework. However, forward thinking was signiﬁ-\ncantly faster. Training with forward thinking was about 30% faster than backpropagation\n- even though they used libraries which were optimized for backpropagation. They also\nshowed that a convolutional network trained with forward thinking outperformed a net-\nwork trained with backpropagation in training accuracy, testing accuracy while each epoch\ntook about 50% less time. In fact, the CNN trained using forward thinking achieves near\nstate-of-the-art performance after being trained for only 90 minutes on a single desktop\nmachine.\nBoth Cascor and forward thinking construct neural networks in a greedy way, layer by\nlayer. However, forward thinking trains layers instead of individual units and while Cascor\nuses old data to train new units, forward thinking uses new, synthetic data to train a new\nlayer.\n3.2.4\nCombined Destructive and Constructive Dynamic Learning\nAs mentioned before, it is also possible to combine the destructive and constructive ap-\nproach to dynamic learning. I was not able to ﬁnd any algorithms that ﬁt into this area,\naside from Waugh [1994], who proposed a modiﬁcation to Cascor which also prunes the\nnetwork.\n3.3\nSummary\nMany current state-of-the-art machine learning solutions rely on deep neural networks\nwith architectures much larger than necessary in order to solve the task at hand. Through\nearly stopping, dropout and other regularization techniques, these overly large networks\nare prevented from overﬁtting on the data. Finding a way to eﬃciently automate the\n17\narchitecture design of neural networks could lead to better network architectures than\npreviously used. In the beginning of this section, I have presented some evidence for neural\nnetwork architectures that have been designed by algorithms and outperform manually\ndesigned architectures.\nAutomated architecture design algorithms might be the next step in deep learning. As\ndeep neural networks continue to increase in complexity, we may have to leverage neural\narchitecture search algorithms and dynamic learning algorithms to design deep lerning\nsystems that continue to push the boundary of what is possible with machine learning.\nSeveral algorithms have been proposed to dynamically and automatically choose a neu-\nral network’s architecture. This thesis aims to give an overview of the most popular of\nthese techniques and to present empirical results, comparing these techniques on diﬀerent\nbenchmark problems. Furthermore, in the following sections, I will also be introducing\nnew algorithms, based on existing algorithms.\n18\n4\nEmpirical Findings\n4.1\nOutline of the Investigation\nSo far, this thesis has demonstrated the relevance of deep neural networks in today’s\nmachine learning research and shown that deep neural networks are more powerful in\nrepresenting and learning complex functions than shallow neural networks. I have also\noutlined downsides to using such deep architectures; the trial and error approach to de-\nsigning a neural network’s architecture and the computational ineﬃciency of oversized\narchitectures that is found in many modern deep learning solutions.\nIn a preliminary literature review of possible solutions to combat the computational ineﬃ-\nciencies of deep learning in a more automated, dynamic way, I presented a few algorithms\nand techniques which aim to automate the design of deep neural networks. I introduced\ndiﬀerent categories of such techniques; search algorithms, constructive algorithms, de-\nstructive algorithms (including regularization techniques), and mixed constructive and\ndestructive algorithms.\nI will furthermore empirically investigate a chosen subset of the presented techniques and\ncompare them in terms of ﬁnal performance, computational requirements, complexity of\nthe resulting model and level of automation.\nThe results of this empirical study may\ngive a comparison of these techniques’ merit and guide future research into promising\ndirections.\nThe empirical study may also result in hypotheses about when to use the\ndiﬀerent algorithms that will require further study to verify.\nAs the scope of this thesis is limited, the results that will be presented hereby will not be\nsuﬃcient to conﬁrm or reject any hypotheses about the viability of diﬀerent approaches to\nautomated architecture design. The experiments presented in this program will act only\nas a ﬁrst step of the investigation into which algorithms are worthy of closer inspection\nand which approaches may be suited for diﬀerent learning tasks.\n4.1.1\nInvestigated Techniques for Automated Architecture Design\nThe investigated techniques for automated architecture design have been introduced in\nSection 3. This section outlines the techniques that will be investigated in more detail in\nan experimental comparison.\nAs search-based techniques for neural network architecture optimization, I will investigate\nrandom search and evolving neural networks.\nFurthermore, I am running experiments on the cascade-correlation learning algorithm\nand forward thinking neural networks as algorithms for the dynamical building of neural\nnetworks during training. In these algorithms, only one network is considered but each\nlayer is chosen from a set of possible layers from which the best one is chosen.\nI will not start an empirical investigation of destructive dynamic learning algorithm. I\ndo not consider any of the introduced destructive dynamic learning algorithms as auto-\nmated. Neither regularization nor pruning existing networks contribute to the automation\nof neural network architecture design. They are valuable techniques that can play a role in\nthe design of neural networks, in order to reduce the model’s complexity and/or improve\n19\nthe network’s peformance. However, as they are not automated algorithms, I will not be\nconsidering them in my empirical investigation.\nI furthermore declare the technique of manual search - the design of neural networks\nthrough trial and error - as the baseline for this experiment.\nThe following list shows all techniques that are to be investigated empirically:\n• Manual search (baseline)\n• Random search\n• Evolutionary search\n• Cascade-correlation networks\n• Forward thinking networks\n4.1.2\nBenchmark Learning Task\nIn order to compare diﬀerent automated learning algorithms, a set of learning tasks need to\nbe decided on which each architecture will be trained, in order to assess their performance.\nDue to the limited scope of this research project, I will limit myself to the MNIST digit\nrecognition dataset.\nMNIST is the most widely used dataset for digit recognition in machine learning, main-\ntained by LeCun et al. [1998].\nThe dataset contains handwritten digits that are size-\nnormalized and centered in an image of size 28x28 with pixel values ranging from 0 to\n255. The dataset contains 60,000 training and 10,000 testing examples. Benchmark re-\nsults reported using diﬀerent machine learning models are listed on the website here. The\nresulting function is\nfmnist : {0, .., 255}784 7→{0, .., 9}\nwhere\nfmnist(x) = i iﬀx shows the digit i\nThe MNIST dataset is divided into a training set and a testing set. I further divide the\ntraining set into a training set and a validation set. The validation set consists of 20% of\nthe training data. From this point onwards, I will be referring to the training set as the\n80% of the original training set that I am using to train the algorithms and the validation\nset as the 20% of the original training set that I am using for a performance metric during\ntraining. The testing set will not be used until the ﬁnal model architecture is decided on.\nAll model decisions (e.g. early stopping) will be based on the network’s performance on\nthe validation and training data - not the testing data.\n4.1.3\nEvaluation Metrics\nThe goal of neural network design was discussed in Section 1.3. Based on this, the following\nlist of metrics shows how the diﬀerent algorithms will be compared and assessed:\n• Model performance: assessed by accuracy on the unseen testing data.\n20\n• Computational requirements: assessed by the duration of training (subject to ad-\njustments, due to code optimization and computational power diﬀerence between\nmachines running the experiment).\n• Model complexity: assessed by the number of connections in the resulting network.\n• Level of automation: assessed by the number of parameters that require optimiza-\ntion.\n4.1.4\nImplementation Details\nI wrote the code for the experiments entirely by myself, unless otherwise speciﬁed. All\nmy implementations were done in Keras, a deep learning framework in Python, using\nTensorﬂow as a backend. Implementing everything with the same framework makes it\neasier to compare metrics such as training time easier.\nAll experiments were either run on my personal computer’s CPU or on a GPU cloud\ncomputing platform called Google Colab. Google Colab oﬀers free GPU power for research\npurposes. More speciﬁcally, for the experiments I had access to a Tesla K80 GPU with\n2496 CUDA cores, and 12GB of GDDR5 VRAM. My personal computer uses a 3.5 GHz\nIntel Core i7 CPU with 16 GB of memory.\nSome terminology is used without being formally deﬁned. The most important of these\nterms are deﬁned in the appendix, such as activation functions, loss functions and opti-\nmization algorithms that are used in the experiments.\n4.2\nSearch Algorithms\nThe most natural way to ﬁnd a good neural network architecture is to search for it.\nWhile the training of a neural network is an optimization problem itself, we can also\nview the search for an optimal (or simply, a good) neural network architecture as an\noptimization problem. Within the space of all neural network architectures (here only\nfeedforward architectures), we want to ﬁnd the architecture yielding the best performance\n(for example, the lowest validation error).\nThe obvious disadvantage is that searching is very expensive. A normal search consists\nof diﬀerent stages.\nFirst, we have to deﬁne the search space, i.e.\nall neural network\narchitectures that we will be considering in our search. Second, we will search through\nthis space of architectures, assessing the performance of each neural networks by training it\nuntil some stopping criterion (depending on the time available, one often does not train the\nnetworks until convergence). Third, one evaluates the search results and the performance\nof each architecture. Now, one can fully train some (or simply one) of the best candidates.\nAlternatively, we can use the information from the search results to restrict our search\nspace and re-run the search on this new, restricted search space.\nIt is important to note that this is not an ideal approach. Ideally, one would train each\nnetwork architecture to convergence (even multiple times, to get a more reliable perfor-\nmance metric) and then choose the best architecture. However, in order to save time,\nwe only train each network for a few epochs and assess its performance based on that.\nThere are other performance estimation techniques [Elsken et al., 2019], however in these\nexperiments I will train networks for a few epochs and assess their performance based\n21\non the resulting accuracy on the testing data. However, as a result of this performance\nestimation, the search results may be biased to prefer network architectures that perform\nwell in the ﬁrst few epochs.\n4.2.1\nManual Search\nOne of the most widely used approaches by researchers and students is manual search\n[Elsken et al., 2019]. I also found the names Grad Student Descent or Babysitting for it.\nThis approach is 100% manual and based on trial and error, as well as personal experience.\nOne iterates through diﬀerent neural network setups until one runs out of time or reaches\nsome pre-deﬁned stopping criterion.\nI am also including a research step: researching previously used network architectures that\nworked well on the learning task (or on similar learning tasks). I found an example MLP\narchitecture on the MNIST dataset in the code of the Keras deep learning framework.\nThey used a feedforward neural network with two hidden layers of 512 units each, using\nthe rectiﬁed linear units (relu) activation function and a dropout (with the probability\nof dropping out being p = 0.2) after each hidden layer. The output layer uses the soft-\nmax activation function (see Appendix A.2). The network is optimized using the Root\nMean Square Propagation algorithm (RMSProp, see Appendix A.3.2), with the categor-\nical crossentropy as a loss function (see Appendix A.1). They report a test accuracy of\n98.40% after 20 epochs [Keras, 2019].\nFor this thesis, I do not consider regularization techniques such as dropout, hence I am\ntraining a similar network architecture without using dropout. I trained a 2x512 neural\nnetwork using relu which didn’t perform very well so I used the tanh activation function\ninstead - classic manual search, trying diﬀerent architectures manually. The ﬁnal network’s\nperformance over the training epochs is shown in Figure 7.\nFigure 7: Performance of the neural network found using manual search. Two hidden layers\nof 512 units each, using the tanh activation function in the hidden units and softmax in\nthe output layer. Trained using RMSProp. Values averaged over 20 training runs.\nThe network’s average accuracy on the testing set is 97.3% with a standard deviation of\n0.15%. The training is stopped after an average of 23 epochs (standard deviation 5.5),\nafter the validation accuracy has not improved for ﬁve epochs in a row. Since I am not\nusing dropout (which is likely to improve performance), this result is in agreement with\nthe results reported by Keras [2019].\n22\n4.2.2\nRandom Search\nAs mentioned in Section 3.1.1, random search is a good non-adaptive search algorithm\n[Bergstra and Bengio, 2012]. For this thesis, I implemented a random search algorithm\nto ﬁnd a good network architecture (not optimizing hyperparameters for the learning\nalgorithm). I start by deﬁning the search space; it consists of:\n• Topology: how many hidden units per layer and how many layers in total. The\nnumber of hidden units per layer h is speciﬁed to be 100 ≤h ≤1000 (for simplicity,\nusing only multiples of 50) and the number of hidden layers l is speciﬁed to be\n1 ≤l ≤10.\n• Activation function: either the relu or tanh function in the hidden layers.\nThe\nactivation function on the output units is ﬁxed to be softmax.\n• Optimization algorithm: either stochastic gradient descent (SGD) (ﬁxed learning\nrate, weight decay, using momentum, see Appendix A.3) or RMSProp.\nIncluding the topology and activation function in the search space is necessary, as the goal\nis to search for a good network architecture. I chose not to optimize other hyperparameters,\nas the focus is to ﬁnd a good network architecture. However, I did include the choice of\noptimization algorithm (SGD or RMSProp) to ensure that the optimization algorithm\ncannot be blamed for bad performance of the networks. As shown in the experiments,\nRMSProp almost always outperformed SGD. Though I could have only used RMSProp\nas an optimization algorithm, I chose to leave the optimizer in the search space in order\nto assess how well the search algorithms performs with ”unnecessary” parameters in the\nsearch space (unnecessary because RMSProp is better than SGD in all relevant cases, as\nshown later).\nThe program will randomly sample 100 conﬁgurations from the search space. Each of the\nsampled networks will be trained on the training data for ﬁve epochs and the performance\nwill be assessed on the training set and the testing set. In order to reduce the noise in the\nexperiment, each network will be trained three times, with diﬀerent initial weights. All\nnetworks are trained using categorical crossentropy loss (see Appendix A.1 with a batch\nsize of 128 (see Appendix A.3).\nTable 1 shows the ten best results of the experiment. It becomes immediately obvious that\nRMSProp is a better ﬁt as training algorithm than SGD, as mentioned above. Tanh seems\nto outperform relu as an activation function in most cases. However, deep and narrow\n(few hidden units in each layer, with more than ﬁve layers) seem to perform better when\ntrained using the relu activation function.\nA similar architecture to the two layer architecture from Section 4.2.1 shows up in rank\n3, showing that manual search yielded a network setup performing (almost) as well as\nthe best network setup found through the random search experiment. However, note that\nthese are only preliminary results - the networks were only trained for three epochs, not\nuntil convergence.\nIt is important to note that the experiment was by far not exhaustive: many hyperparam-\neters were not considered in the random search and the parameters that were considered\ndid not cover all possible choices. This is a comparative study, hence the results of the ran-\ndom search algorithm are only meaningful in comparison to other automated architecture\ndesign algorithms.\n23\nTime\nTest acc\nTrain acc\nActivation\nLayers\nOptimizer\n7.76s\n96.41%\n96.11%\nrelu\n9 x 100\nRMSProp\n6.20s\n96.00%\n95.78%\ntanh\n3 x 800\nRMSProp\n5.19s\n95.85%\n95.86%\ntanh\n2 x 700\nRMSProp\n5.44s\n95.68%\n95.66%\ntanh\n3 x 550\nRMSProp\n5.63s\n95.56%\n95.85%\ntanh\n2 x 800\nRMSProp\n6.20s\n95.51%\n95.91%\nrelu\n6 x 150\nRMSProp\n5.00s\n95.42%\n95.66%\ntanh\n2 x 550\nRMSProp\n6.16s\n95.30%\n95.23%\ntanh\n4 x 600\nRMSProp\n5.18s\n95.18%\n95.17%\ntanh\n3 x 350\nRMSProp\n5.61s\n95.06%\n94.72%\ntanh\n4 x 300\nRMSProp\nTable 1: Ten best-performing network setups from random search results. All networks\ntrained using categorical cross entropy with softmax in the output layer.\nValues are\naveraged over three training runs. Each network was trained for three epochs.\nI continued by training the ten best-performing candidates (based on the averaged accuracy\non the validation set) found through the random search experiment until convergence\n(using early stopping, I stopped training the network once the accuracy on the validation\nset did not increase for ﬁve epochs in a row), I obtain the results shown in Table 2, sorted\nby their ﬁnal performance on the test data.\nEpochs\nTrain acc\nTest acc\nLayers\nActivation\nTime\n18 ± 5\n98.3% ± 0.2%\n97.3% ± 0.2%\n2 x 800\ntanh\n31.2s ± 8.1s\n24 ± 5\n98.5% ± 0.2%\n97.2% ± 0.2%\n2 x 550\ntanh\n37.8s ± 8.0s\n19 ± 5\n98.3% ± 0.2%\n97.1% ± 0.5%\n2 x 700\ntanh\n30.6s ± 8.0s\n22 ± 5\n98.2% ± 0.2%\n97.0% ± 0.2%\n3 x 350\ntanh\n36.9s ± 8.7s\n18 ± 4\n98.3% ± 0.2%\n97.0% ± 0.2%\n3 x 550\ntanh\n31.0s ± 6.3s\n18 ± 5\n98.1% ± 0.3%\n96.9% ± 0.3%\n3 x 800\ntanh\n34.8s ± 10.5s\n26 ± 5\n98.1% ± 0.2%\n96.8% ± 0.1%\n4 x 300\ntanh\n44.8s ± 8.1s\n17 ± 5\n97.9% ± 0.3%\n96.7% ± 0.5%\n9 x 100\nrelu\n38.5s ± 12.9s\n20 ± 6\n97.9% ± 0.3%\n96.7% ± 0.3%\n4 x 600\ntanh\n38.0s ± 11.6s\n13 ± 5\n71.8% ± 42.5%\n70.6% ± 41.7%\n6 x 150\nrelu\n26.2s ± 11.4s\nTable 2: Best-performing network architectures from random search, sorted by ﬁnal ac-\ncuracy on the testing data. The table shows average values and their standard deviations\nover ten training runs for each network architecture.\nThe results show that the networks using the tanh activation function mostly outperform\nthose using the relu activation function. The best-performing networks are those using two\nhidden layers, as the one that was trained through manual search. The ﬁnal performance\nof the best networks found through random search can be considered equal to the network\nfound through random search.\n4.2.3\nEvolutionary Search\nAs an adaptive search algorithm, I implemented an evolving artiﬁcial neural network which\nis basically an evolutionary search algorithm applied to neural network architectures, since\nI am not evolving the connection weights of the network. Evolutionary search algorithms\n24\napplied to neural networks are also called neuroevolution algorithms. The parameter space\nis the same as for random search, see Section 4.2.2.\nThere are several parameters that adjust the evolutionary search algorithm’s performance.\nThe parameters that can be adjusted in my implementation are:\n• Population size: number of network architectures that are assessed in each search\niteration.\n• Mutation chance: the probability of a random mutation taking place (after breeding).\n• Retain rate: how many of the ﬁttest parents should be selected for the next genera-\ntion.\n• Random selection rate: how many parents should be randomly selected (regardless\nof ﬁtness, after retaining the ﬁttest parents).\nThe listing in Figure 8 shows a simpliﬁed version of the search algorithm.\ndef evolving ann ( ) :\npopulation = Population ( parameter space ,\np o p ul a t io n s i ze )\nwhile not s t o p p i n g c r i t e r i o n :\npopulation . compute fitness values ()\nparents = population . f i t t e s t (k)\nparents += population . random( r )\nchildren = parents . randomly breed ()\nchildren . randomly mutate ()\npopulation = parents + children\nreturn population\nFigure 8: Simpliﬁed pseudo code for the implementation of evolving artiﬁcial neural net-\nworks\nIn my implementation, I set the population size to 50, the mutation chance to 10%, the\nretain rate to 40% and the random selection rate to 10%. These values for the algorithm’s\nparameters were taken from Harvey [2017] and adjusted. The ﬁtness is just the accuracy\nof the network on the testing set after training for three epochs. As was done in random\nsearch, each network is trained three times. The average test accuracy after three epochs\nis taken as the network’s ﬁtness.\nIn order to make the random search and the evolutionary search experiments comparable,\nthey are both testing the same number of networks.\nIn random search, I picked 200\nnetworks at random. In this evolutionary search algorithm, I stopped the search once\n200 networks have been trained. This happened after seven iterations in the evolutionary\nsearch.\nI ran the algorithm twice, once allowing for duplicate network architectures in the popu-\nlation and once removing these duplicates.\nWith duplicates\n25\nWithout removing duplicate conﬁgurations, the search algorithm converges to only six\ndiﬀerent conﬁgurations, shown in Table 3. The table shows these six conﬁgurations.\nIt is important to note that by allowing duplicate neural network conﬁgurations, the\nalgorithm is training multiple instances for each well-performing conﬁguration - hence\nimproving the overall network performance slightly by choosing the best random weight\ninitialization(s).\nLayers\nOptimizer\nHidden\nFitness\n3 x 450\nRMS Prop\ntanh\n95.95%\n4 x 600\nRMS Prop\ntanh\n95.90%\n2 x 450\nRMS Prop\ntanh\n95.70%\n3 x 350\nRMS Prop\ntanh\n95.59%\n2 x 350\nRMS Prop\ntanh\n95.45%\n1 x 500\nRMS Prop\ntanh\n94.25%\nTable 3: Network architectures from evolutionary search without removing duplicate con-\nﬁgurations.\nWhen fully training these conﬁgurations, I get the results shown in Table 4. The best\nnetwork architectures perform similarly to the best ones found through random search.\nNotably, all networks use tanh as activation function and RMSProp as optimizer.\nEpochs\nTrain acc\nTest acc\nLayers\nActivation\nTime\n22 ± 4\n98.2% ± 0.2%\n97.2% ± 0.1%\n2 x 350\ntanh\n33.8s ± 5.5s\n24 ± 6\n98.4% ± 0.2%\n97.2% ± 0.2%\n2 x 450\ntanh\n37.7s ± 10.2s\n22 ± 7\n98.4% ± 0.3%\n97.0% ± 0.1%\n3 x 450\ntanh\n37.2s ± 11.3s\n22 ± 5\n98.2% ± 0.2%\n96.9% ± 0.2%\n3 x 350\ntanh\n35.7s ± 8.1s\n18 ± 5\n97.9% ± 0.2%\n96.8% ± 0.2%\n4 x 600\ntanh\n33.8s ± 8.7s\n24 ± 9\n96.4% ± 0.2%\n96.0% ± 0.2%\n1 x 500\ntanh\n34.2s ± 13.0s\nTable 4: Fully trained networks obtained from evolutionary search without removing\nduplicate conﬁgurations.\nWithout duplicates\nWhen removing duplicate conﬁgurations, there will naturally be more variety in the neural\nnetwork conﬁgurations that will appear in later iterations of the search algorithm. Table\n5 shows the ten best neural network conﬁgurations found using the evolutionary search\nalgorithm when removing duplicate architectures.\nThe results are better than the ones obtained from the evolutionary search with duplicate\narchitectures. This is likely due to the increased variety in network architectures that are\nconsidered by the search algorithm. Fully training these networks yields the results in\nTable 6.\nThese results are also very similar to the ones obtained through random search and manual\nsearch. The best-performing architectures are using two hidden layers, though here the\nnumber of neurons in these hidden layers is larger than previously seen.\n26\nLayers\nOptimizer\nHidden\nTest accuracy\n9 x 150\nRMSProp\ntanh\n96.24%\n2 x 850\nRMSProp\ntanh\n96.23%\n2 x 950\nRMSProp\ntanh\n96.12%\n3 x 500\nRMSProp\ntanh\n95.78%\n9 x 100\nRMSProp\ntanh\n95.74%\n4 x 600\nRMSProp\ntanh\n95.71%\n4 x 800\nRMSProp\ntanh\n95.56%\n4 x 400\nRMSProp\ntanh\n95.42%\n9 x 100\nRMSProp\ntanh\n95.32%\n4 x 650\nRMSProp\ntanh\n95.31%\nTable 5: Top ten neural network conﬁgurations found using EANNs without duplicate\nconﬁgurations.\nEpochs\nTrain acc\nTest acc\nLayers\nAct.\ntime\n20 ± 6\n98.3% ± 0.3%\n97.3% ± 0.1%\n2 x 850\ntanh\n33.6s ± 10.3s\n18 ± 5\n98.2% ± 0.2%\n97.2% ± 0.3%\n2 x 950\ntanh\n31.2s ± 8.4s\n19 ± 5\n98.3% ± 0.2%\n96.9% ± 0.2%\n3 x 500\ntanh\n32.2s ± 7.8s\n25 ± 7\n98.2% ± 0.3%\n96.8% ± 0.2%\n4 x 400\ntanh\n43.3s ± 11.7s\n20 ± 6\n98.0% ± 0.2%\n96.7% ± 0.2%\n4 x 600\ntanh\n37.3s ± 10.7s\n21 ± 7\n97.9% ± 0.2%\n96.7% ± 0.3%\n4 x 650\ntanh\n41.7s ± 13.4s\n20 ± 5\n97.7% ± 0.2%\n96.7% ± 0.2%\n4 x 800\ntanh\n42.4s ± 10.5s\n27 ± 5\n96.5% ± 0.3%\n95.5% ± 0.3%\n9 x 150\ntanh\n62.6s ± 10.9s\n24 ± 7\n95.8% ± 0.4%\n94.9% ± 0.5%\n9 x 100\ntanh\n54.1s ± 16.5s\nTable 6: Top ten neural network conﬁgurations found using EANNs without duplicate\nconﬁgurations, fully trained (until validation accuracy hasn’t improved for ﬁve epochs in\na row).\nThe animation in Figure 9 shows how the population in this evolutionary search algorithm\nchanges between iterations. The animation demonstrates how the accuracy of the networks\nin the population increases with each search iteration, with some random ﬂuctuations\ndue to the random mutations that are sometimes disadvantageous. It also shows that\nRMSProp is quickly adopted as the optimizer mainly used in the iterations and that\ntanh is adopted as the activation function that is mainly used. The model complexity is\nshown on the x axis and the animation shows that the evolutionary search converges to\nresults at the lower end of the model complexity scale. This conﬁrms that smaller network\narchitectures are more suited for the learning task at hand than larger architectures.\n4.2.4\nConclusion\nAll three search algorithms yield the same ﬁnal performance, with minor diﬀerences. They\nall ﬁnd that architectures using two hidden layers seem to work the best and only diﬀer in\nthe width of these hidden layers. Hence, the performance of the three search algorithms\ncan be considered equal.\nThe complexity of the resulting model (measured by the number of hidden layers and the\nwidth of these layers) is also comparable between the three search algorithms, as they ﬁnd\n27\nFigure 9: Animation of how the population in the evolutionary search algorithm changes\nbetween iterations (best viewed in Adobe Acrobat).\nsimilar network architectures. To be very exact, evolutionary search (when allowing for\nduplicates in the population) ﬁnds the smallest network architecture (two hidden layers\nof 350 or 450 neurons each), followed by manual search (two hidden layers of 512 neurons\neach), then random search (two hidden layers of 800, 550, or 700 neurons each) and ﬁnal\nevolutionary search (when removing duplicate architectures from the population) with two\nhidden layers of 850 or 950 neurons each. However, I do not consider these ﬁndings very\nrelevant but consider them to be due to random noise in the experiments - multiple runs of\nthe search algorithms will give more statistically signiﬁcant results and may come up with\na diﬀerent ordering in the resulting network’s complexity, since the diﬀerence between the\nnetwork architectures does not seem very signiﬁcant in the experiments that I ran.\nThe level of automation diﬀers signiﬁcantly between the three algorithms. Manual search\nis obviously not automated at all. Evolutionary search is automated but still has a lot of\nhyperparameters that need to be decided (listed in Section 4.2.3). Random search is the\nmost automated algorithm, it merely requires the speciﬁcation of the search space.\nThe computational requirements for the diﬀerent search algorithms are diﬃcult to com-\npare. Technically, my implementation of manual search was very eﬃcient - I only trained\ntwo network architectures until reaching the architecture that I reported my ﬁndings for.\nHowever, in practice, manual search is often an iterative process, in which one tries dif-\nferent architectures and decides on an architecture based on this trial and error. This\nis diﬃcult, if not impossible, to quantify. Comparing the random search and evolution-\nary search algorithm with respect to computational requirements is not straight-forward\neither.\nTheir space requirements are similar (assuming an eﬃcient way of storing the\npopulation in evolutionary search, which is the case in my implementation). The time\nrequirements of the two algorithms is diﬃcult to compare. Due to the random nature of\nboth algorithms, and because I am only reporting one run for each of the search algo-\nrithms, it is not possible to compare the algorithm’s time requirements in a meaningful\nway based on the experiments I conducted.\nA meaningful comparison is the exploration of the search space, i.e. how much of the\nsearch space has been explored by the algorithm. Figure 10 shows how the two version\nof evolutionary search compare with the random search algorithm. As expected, random\nsearch explores the search space very evenly. When removing duplicates in the population,\n28\nthe evolutionary search algorithm explores more of the search space compared to not\nremoving duplicate architectures.\nWhen allowing for duplicates, the exploration looks\nvery clustered, indicating that the algorithm mainly stayed in the same areas of the search\nspace.\nWhen removing duplicates, the exploration is more spread out, though not as\nbalanced as random search.\nFigure 10: Exploration of the network architecture search space using diﬀerent search\nalgorithms. Hidden activation function and optimizer are omitted. The color encoding is\nthe same for all three plots.\nThe exploration of the evolutionary search algorithm is quite dependent on the initial\npopulation. Figure 11 shows how little the evolutionary search algorithm explores archi-\ntectures that are not in the initial population. When allowing for duplicates, the algorithm\nalmost exclusively checks the architectures from the initial population - only 2% of all ex-\nplored architectures were not in the initial population. When removing duplicates, the\nalgorithm explores signiﬁcantly more, though the initial population still makes up more\nthan 50% of all explored network architectures.\nFigure 11: Exploration of the neural architecture search space for evolutionary search\n(with or without duplicates in the population), when removing all those architectures\nthat were present in the initial population. The lower the activity in the search space, the\nmore the exploration depends on the initial population. Hidden activation function and\noptimizer are omitted. The color encoding is the same for all three plots.\nThis shows that my evolutionary search algorithm implementation is dependent on the\ninitial population. This opens up the possibility to encode prior knowledge into the evolu-\ntionary search. If one knows that a particular kind of network architecture is more likely\n29\nto perform well than another, this can be represented in the initial population for the\nsearch.\nTo summarize my ﬁndings of diﬀerent neural network architecture search algorithms, each\none of the three search algorithms has its advantages and disadvantages. When the de-\nsigner of the neural network is knowledgeable and experienced in the design of neural\nnetwork architectures, or has resources such as previously used networks for the learning\ntasks available, manual search is a good choice. It is very cheap and highly customizable.\nWhen the goal is to automate the architecture design, random search and evolutionary\nsearch are more suitable choices. Evolutionary search allows for more customization and\nthe encoding of prior knowledge which may save time during the search. Random search\nis good algorithm to explore the entire search space evenly, if the goal is to not overlook\nany architectures.\n4.3\nConstructive Dynamic Learning Algorithm\nIn constructive dynamic learning, it is not necessary to deﬁne the search space explic-\nitly. However, one can argue that diﬀerent constructive dynamic learning algorithms have\nimplicit restrictions on the type of network architecture that they consider. The cascade-\ncorrelation learning algorithm can only build network architectures that are cascaded in\na very particular way. The original forward thinking algorithm requires speciﬁcation of\nthe exact network architecture, thus not automating the architecture design. This is why\nI am proposing a new algorithm, based on forward thinking, which also automates the\narchitecture design.\n4.3.1\nCascade-Correlation Networks\nThe originally proposed Cascor algorithm requires many hyperparameters to be set [Yang\nand Honavar, 1998]. It does not specify when to stop training each unit before adding the\nnext one and it does not specify when to stop adding new units altogether. Other papers\nhave also questioned the choice of training on error correlation maximization rather than\n”standard” error minimization training [Littmann and Ritter, 1992]. I implemented and\nran experiments on several diﬀerent versions of Cascor, aiming to ﬁnd a version of Cascor\nthat is suitable to a more modern, higher-dimensional dataset such as MNIST (as opposed\nto the low dimensional, small datasets used in the original paper by Fahlman and Lebiere\n[1990]). The largest dataset for which I found evidence that Cascor had been trained on is\na learning task with 120 inputs and 3,175 samples, and a learning task with 21 inputs and\n7,100 samples reported by Littmann and Ritter [1992]. MNIST, the dataset I am using in\nthis thesis, has 784 inputs and 80,000 samples.\nAll experiments reported in this section were run on my personal computer, see Section\n4.1.4 for details.\nThe parameters that needed to be decided on for the Cascor algorithm are:\n• Activation function\n• Loss function: the originally proposed error correlation, or error minimization.\n• When to stop training each unit before adding a new one\n• When to stop adding new units\n30\nCascor\nThe originally proposed cascade-correlation learning algorithm was described in Section\n3.2.3. I implemented the algorithm, as well as the proposed error correlation training.\nThe error correlation loss is described in Appendix A.1.2.\nThe network performs very poorly when trained using the originally proposed error cor-\nrelation maximization. Training the network several times, it never reached a validation\naccuracy above 70%, as shown in Figure 12. I have tried diﬀerent approaches to improve\nthe network’s performance but I was not able to report any good ﬁndings.\nFigure 12: Cascade-correlation learning algorithm, as proposed by Fahlman and Lebiere\n[1990]. The algorithm was run ten times, with a candidate pool of size eight, training\neach hidden unit in the candidate pool for two epochs and then choosing the one with\nthe highest validation accuracy. This unit is then added into the network and trained\nuntil convergence (i.e. until the validation accuracy doesn’t improve for three epochs in a\nrow). Results are averaged over the ten runs, with the shaded area representing the 95%\nconﬁdence interval.\nLittmann and Ritter [1992] report that error correlation training is inferior to error mini-\nmization training on regression tasks. In classiﬁcation tasks, it converges faster - though\nthe ﬁnal performance seems to be the same for both (the authors do not explicitly state\nso, but it seems to be implied in their conclusion’s wording). It may be that the error cor-\nrelation training overcompensates for errors Prechelt [1997] due to the high dimensionality\nof the dataset, though this requires further investigation.\nCaser\nThe next approach is Caser, as proposed by Littmann and Ritter [1992] - a variation of\nCascor in which the network is trained on error minimization. My implementation of the\nnetwork is using softmax in the output layer, tanh in the hidden units and is trained on\nthe categorical cross entropy loss function. Hidden units are added into the network as\ndescribed in the original paper. I am using a candidate pool of eight units. Each candidate\nunit is trained for one epoch after which the candidate unit with the highest accuracy on\nthe validation set is inserted into the network. Once inserted, the unit is trained until\nconvergence using RMSProp (until the testing accuracy stops increasing for more than\ntwo epochs in a row) after which the unit’s input weights are frozen. The output weight\n31\nvector is discarded whenever a new unit is added into the network and retrained, similarly\nto forward thinking. Figure 13 shows the training graphs of this architecture, averaged\nover ten runs. Overall, this looks much better than the error correlation training in Figure\n12.\nFigure 13: Caser algorithm, as originally proposed by Littmann and Ritter [1992]. Results\nare averaged over the ten runs, with the shaded area representing the 95% conﬁdence\ninterval.\nRunning this architecture shows some interesting behavior when a new unit is added\ninto the network. Whenever a new hidden unit is added into the network, the network\nperformance changes - sometimes quite drastically. Figure 14 shows how unpredictable\nthis turns out in individual training runs. On the left, after adding the second hidden unit,\nthe network accuracy improves to over 90% but adding a third hidden unit decreases the\naccuracy down to 60%, even after training this third unit to convergence. The network\nnever recovers from this performance dip and doesn’t reach an accuracy better than 85%\nagain. This is likely because the output weight vector that the network converged to when\ntraining the second hidden unit was discarded and the network will choose a new output\nweight vector at random (from the pool of eight candidate units). If the candidate pool\nonly contains ”bad” weight vectors for the output layer, the network will be stuck in one\nof these bad local minima.\nFigure 14: Unpredictable behavior when adding new units into the Caser network. Left\nplot shows the Caser network using a candidate pool size of eight, whereas on the right, a\ncandidate pool of size 16 was used. Green dotted lines show the insertion of a new hidden\nunit into the network.\nIn order to remove these sudden (and seemingly uncontrollable) performance jumps, one\n32\nmay increase the candidate pool size, in an attempt to increase the probability of ﬁnding\na weight vector close to a good local minimum. The right plot in Figure 14 shows the\nperformance of a network that uses a candidate pool size of 16 (instead of eight, as the\nleft plot) and shows a large performance decrease after adding the second hidden unit, but\nrecovers to the previous ”good” performance with the insertion of the seventh hidden unit.\nIt decreases again with the eighth unit and increases to a new maximum performance with\nthe tenth hidden unit. Luckily, that was the last hidden unit so the ﬁnal network reaches a\ngood performance. Increasing the candidate pool size is not a deterministic way of ﬁnding\na better weight vector. A more reliable method is needed to improve Caser’s performance.\nCaserRe\nThe question of when to stop the training remains, and the random jumps in network\nperformance make it diﬃcult to decide on a stopping criterion. Instead of increasing the\ncandidate pool’s size, I initialized the weight vectors for new hidden units close to the\nlocal minimum that was found in training the previous hidden unit. As Figure 15 shows,\nthis removes performance decreases and yields ”smoother” training improvements. I am\ncalling this CaserRe because it is based on Caser and extends it by re-using the output\nweight vector when a new hidden unit is added into the network.\nFigure 15: Reusing the output weight for all units in the candidate pool for Caser. Results\nare averaged over the ten runs, with the shaded area representing the 95% conﬁdence\ninterval. Lighter colored lines show the single runs.\nHowever, this makes the network very dependent on the initially found local minimum. By\ntaking the weight vector from the previous hidden unit’s training I remove performance\ndips that would have appeared otherwise - but I also removed performance increases that\nwould otherwise be possible and would help the network jump to a better local minimum.\nThis is shown on individual training runs in Figure 16. If the ﬁrst hidden unit ﬁnds a\ngood local minimum, the overall result will be good, though only slightly improving on\nthe network’s performance with one hidden unit. However, if the initial local minimum is\nnot good, the network seems to be stuck.\nIn order to avoid the pitfalls of a bad weight initialization at the beginning of training, it\nmay help to train the candidate pool of hidden units, choose the best performing hidden\nunit and, if the performance is not signiﬁcantly worse than it was before adding this\nhidden unit, the unit should be added as it is. If the performance is signiﬁcantly worse\nthan before, the unit should be added reusing the previous output weight vector - thus\n33\nFigure 16: Caser’s dependence on the initial weight vector. On the left, the network ﬁnds a\ngood initial local minimum whereas on the right, the network ﬁnds a worse local minimum\nand does not improve its performance signiﬁcantly.\ninitializing the output weight vector close to the previously found local minimum. This\nwill remove performance dips, while keeping the chance to ﬁnd better local minima when\nadding new hidden units.\nFigure 17 reuses the previous output weight vector if the new unit decreases the validation\naccuracy by more than 5%. The overall performance of the network is improved, however,\nthe ﬁgure shows some drastic performance drops during training.\nFigure 17: Caser, reusing the previous output weight vector if all units in the candidate\npool decrease the networks accuracy by more than 5%.\nAnother approach is to modify the candidate pool. Instead of training eight candidate\nunits, we can train seven new candidate units and one candidate unit that reuses the\nprevious output weights. In this way, we will only change the output weights if it leads to\nan increase in test accuracy. Obviously, the newly trained units will only be trained for\none epoch while the unit reusing output weights has been trained to convergence. To make\nup for this diﬀerence, we could set a compensation factor. In the experiments plotted in\nFigure 18, I did not use such a compensation factor for the sake of automaticity (the fewer\ntunable parameters, the better).\nThis shows good results, with the network reaching an accuracy of over 90% in 7 out of\n10 training runs, with the remaining 3 runs achieving an accuracy of over 83%.\nSo far, it seems like all experiments on Cascor, Caser, and CaserRe have been underﬁtting\n34\nFigure 18: Using a candidate pool of seven new units and one unit reusing the previous\noutput weights. Results averaged over ten runs, with the shaded area representing a 95%\nconﬁdence interval. Lighter colored lines show the single runs.\non the MNIST learning task, as they have been using only ten hidden units in total - as\ncompared to standard MLPs that have hundreds of hidden units. I trained the algorithm\nwhose results are shown in Figure 18 for 100 cascading hidden units for two training runs,\nusing a candidate pool size of four. The results are shown in Figure 19; both networks\nreach a validation accuracy of 92.7%.\nFigure 19: Using a candidate pool of three new units and one unit reusing the previous\noutput weights. Adding a total of 100 cascading hidden units. Results averaged over two\nruns, with the shaded area representing a 95% conﬁdence interval. Lighter colored lines\nshow the single runs.\nA comparable MLP with one hidden layer of 100 neurons reaches a validation accuracy\nof around 94.0% (trained with RMSProp on crossentropy loss, using tanh in hidden units\nand softmax in the output layer).This shows that CaserRe is close to the performance\nof comparable layered networks.\nHowever, in order to be competitive on the MNIST\nlearning task, a testing accuracy of over 95% should be achieved. The complexity of the\nCaserRe network needs to be increased in an attempt to learn the MNIST task to a higher\naccuracy. The insertion of hidden units is computationally expensive due to the training\nof the candidate pool and modiﬁcations to the computational graph of the neural network.\nComplexity may be added into the network more eﬃciently by increasing the complexity\nof each hidden unit, e.g. by replacing a hidden unit by a hidden cascading layer. To the\nbest of my knowledge, this has not been done before.\n35\nI ran another experiment, using candidate layers rather than single candidate units. Each\ncandidate layer contains 50 neurons and a total of 50 of these cascading layers were inserted\ninto the network. I used a candidate pool of size four. The result is shown in Figure 20, the\nnetwork reaches a validation accuracy of 92.85% (averaged over ﬁve runs with a standard\ndeviation of 0.20%). This is slightly better than the Caser architecture with 100 cascading\nhidden units and worse than layered networks of similar architecture.\nFigure 20: Using a candidate pool of three new units and one unit reusing the previous\noutput weights. Adding a total of 50 cascading hidden layers of 50 units each. Results\naveraged over ﬁve runs, with the shaded area representing a 95% conﬁdence interval.\nLighter colored lines show the single runs.\nIn another experiment, I used layers of size 100, adding a total of 15 of these cascading\nlayers into the network - again using a candidate pool size of four. The results for this\narchitecture are shown in Figure 21. The network reaches a validation accuracy of 88.58%\n(averaged over ten runs and a standard deviation of 4.11%) with a maximum accuracy\nof 92.93% and a minimum of 83.57%.\nAgain, this is worse than comparable layered\narchitectures.\nFigure 21: Using a candidate pool of three new units and one unit reusing the previous\noutput weights. Adding a total of 15 cascading hidden layers of 100 units each. Results\naveraged over ﬁve runs, with the shaded area representing a 95% conﬁdence interval.\nLighter colored lines show the single runs.\nEven though the resulting networks are very large, they do not overﬁt on the MNIST\ndataset and the ﬁnal performance does not signiﬁcantly change when adding more com-\nplexity into the network (by adding cascading layers rather than single units to increase\nthe width or by increasing the depth of the cascading units/layers). A more detailed in-\n36\nvestigation into the connection weight values from the hidden activation vector compared\nto the input activation vector may bring some insights. In order to prioritize the cascaded\nhidden units/layers over the input vector, one may drop out or reduce some of the input-\nto-output connection weights (through dropout or weight decay) in order to incentivize\nthe network to make more use of the new hidden activation vector.\nCascor Summary\nAfter some additional work based on Cascor and Caser, I was able to ﬁnd a well-performing\nlearning algorithm, which I called CaserRe. Though the ﬁnal algorithm is able to ﬁnd good\nlocal minima with an average accuracy of over 90%, adding more units and layers into the\nnetwork does not increase performance to anything above 93% testing accuracy.\nOne reason for this may be that the input to each subsequent hidden unit is still very\nnoisy. Traditional layered neural networks map the input to a diﬀerent dimension through\nthe ﬁrst hidden layer.\nSubsequent hidden layers work only on the output of previous\nlayers. Hidden layers could be seen as making the data more well-behaved, as suggested\nby Hettinger et al. [2017]. This may be why the forward thinking algorithm seems to work\nmuch better than my current implementation of diﬀerent Cascor versions which are facing\nproblems with the aforementioned volatility.\nAnother way to look at is that the error surface (with respect to the weights) is very\nhigh dimensional, as the weight vector is very high dimensional. With each added unit,\nthe network tries to ﬁnd a new local minimum, with one weight being ﬁxed (i.e. one\ndegree of freedom on the error surface frozen) and the rest still to be varied. Since the\ninput dimension is much higher than the dimension of all hidden units combined (in\nmy experiments, no more than one hundred hidden units/layers have been inserted into\nthe network while the input layer has over 700 units), the error minimization problem is\ndominated by the connections weights from the input to the output. In order for this issue\nto disappear, one would have to train a very deep cascading network in order for the hidden\nweights to be more important in relation to the input-to-output-connection weights. This\nwould explain why Cascor performs well on datasets with lower dimensionality, such as\nthe problems treated in the original paper, because there the input-to-output-connection\nweights are much fewer and thus less relevant in comparison to the hidden weights.\nIn terms of performance, training these cascading networks can be very eﬃcient using\nmodern deep learning frameworks, with each epoch taking no more than a few seconds.\nHowever, the cascading structure requires making changes to the computational graph,\nwhich sum up to be a large overhead. The deeper networks (50 cascading layers of 50 units\neach, 100 cascading layers of single units, and 100 15 cascading layers of 100 units each)\ntook over 30 minutes to train, with the vast majority of the time spent on the training\nof candidate unit/layers. This can be done much more eﬃciently, since the candidate\ntraining allows for perfect parallelization. Hence the candidate unit training can be done\nin parallel and yield a time decrease of up to 8x.\nSince most modern neural networks deal with very high-dimensional data, more work on\nCascor is required in order to make it competitive in the world of modern neural networks.\nA comprehensive study on diﬀerent cascading architectures can give more conclusive ev-\nidence for whether or not these cascading architectures can perform as well, or better,\ncompared to similar layered architectures.\n37\n4.3.2\nForward Thinking\nThe forward thinking algorithm trains a fully-connected neural network by building up\nthe network one hidden layer at a time [Hettinger et al., 2017]. The originally proposed\nalgorithm does not automate the layer construction.\nOne needs to specify how many\nlayers can be added, as well as the width of the layer and the activation function used.\nThe networks in my experiments will be trained on cross entropy loss using RMS Prop.\nHidden units use the tanh or relu activation function, output units use softmax.\nParameters that needed to be decided on include:\n• Hidden layers: how many layers, how many units in each layer, activation functions.\n• Layer construction time: when to add new layers.\nFor this experiment, a new layer will be added when the training of the current layer has\nnot improved the accuracy on the validation data for two epochs in a row (and training will\nbe stopped after the validation accuracy hasn’t improved for three epochs in a row when\ntraining the last layer). I am running the forward thinking algorithm on three diﬀerent\narchitectures: two layers of 512 tanh units each, three layers of 850 tanh units each, and\nﬁve layers of 750 tanh units each - taking the best-performing neural network setups from\nthe random search results using two, three and ﬁve hidden layers.\nFigure 22 shows the performance of these networks. It is interesting to see that the testing\naccuracy seems to reach its maximum around half-way through each layer-wise training (or\neven slightly before) while the training accuracy continuously increases. Moreover, while\nthe training accuracy decreases signiﬁcantly when a new layer is inserted, the testing\naccuracy does not suﬀer from this decrease. Near the training’s end, the training accuracy\nkeeps increasing signiﬁcantly more than the validation accuracy. This looks strange - it\ndoesn’t seem to be overﬁtting, as the validation accuracy keeps improving as well. This is\nvery similar to the ﬁndings reported by Hettinger et al. [2017].\nFigure 22: Training and validation accuracy per epoch in forward thinking. Results are\naveraged over 20 runs, the shaded areas show the 95% conﬁdence interval.\nHowever, looking at the loss, shown in Figure 23, demonstrates that the network is indeed\nstarting to overﬁt, but the accuracy doesn’t suﬀer from the overﬁtting. This eﬀect is more\nsigniﬁcant in deeper networks.\nHettinger et al. [2017] do not report the loss of their network, hence a direct comparison\nis not possible. The accuracy is computed through an argmax operation on the output\n38\nFigure 23: Training and validation loss per epoch in forward thinking. Results are averaged\nover 20 runs, the shaded areas show the 95% conﬁdence interval.\nvector (see Appendix A.1). As long as the maximum value in the output vector belongs to\nthe same class, the accuracy does not change. However, if the output vector becomes less\ncertain about the class - meaning that the diﬀerence between the maximum argument and\nother arguments decreases - the loss will increase, penalizing this increased uncertainty.\nHence, the forward thinking algorithm is indeed starting to overﬁt on the training data,\nwith the overﬁtting being more signiﬁcant in deeper networks.\nEarly stopping on the accuracy doesn’t seem to avoid overﬁtting as well as early stopping\non the loss would. Hence, the following experiments will be applying early stopping to the\nvalidation loss, rather than the validation accuracy.\nThe ﬁnal performance of these networks is shown in Table 7 and for a direct comparison\nbetween forward thinking and ”standard” training, the same statistics are shown in Table\n8 for a network trained using backpropagation.\nLayers\nEpochs\nTrain Accuracy\nValidation Accuracy\nTime\n2 x 512\n18 ± 4\n98.75% ± 0.48%\n96.85% ± 0.28%\n28.9s ± 6.5s\n3 x 850\n21 ± 4\n99.30% ± 0.27%\n97.27% ± 0.23%\n35.7s ± 5.8s\n5 x 750\n29 ± 4\n99.91% ± 0.08%\n97.54% ± 0.12%\n47.4s ± 5.7s\nTable 7: Network performances when trained with forward thinking. Results show the\naverages and standard deviations over 20 training runs.\nLayers\nEpochs\nTrain Accuracy\nValidation Accuracy\nTime\n2x512\n23 ± 6\n98.45% ± 0.24%\n97.27% ± 0.15%\n36.5s ± 8.8s\n3x850\n18 ± 5\n98.09% ± 0.19%\n96.92% ± 0.26%\n36.2s ± 9.2s\n5x750\n20 ± 6\n97.19% ± 0.23%\n96.10% ± 0.32%\n48.4s ± 14.8s\nTable 8: Network performances when trained using backpropagation (for a direct com-\nparison between backpropagation and forward thinking. Results show the averages and\nstandard deviations over 20 training runs.\nThe results show that the two layer network performs 0.4% better (on average) when\ntrained using backpropagation. The three and ﬁve layer networks show a 0.3% and 1.5%\nincrease in validation accuracy (on average) when trained with forward thinking. This is\nin agreement with forward thinking being more eﬃcient in training deep neural networks,\n39\nas there is no need to propagate the error signal through many layers. More experiments\non other learning tasks are needed in order to solidify this hypothesis.\nHettinger et al. [2017] reported a 30% decrease in training time on a four-layer neural\nnetwork. Though forward thinking was, on average, faster for all three network archi-\ntectures, I cannot report the same magnitude of speedup. This may be due to the fact\nthat the training happens on the GPU but the computational graph is modiﬁed after\neach layer-wise training which entails that data has to be moved to and from the CPU.\nThis leads to a larger overhead in computation, as previously mentioned for the cascading\nnetworks in Section 4.3.1. In order to test this hypothesis, I ran the same experiment on\nmy personal computer’s CPU (running the training once for backpropagation and once for\nforward thinking due to time constraints). This indeed shows a much larger improvement\nin training time for forward thinking compared to backpropagation - 46% for the 5 x 750\nnetwork, 25% for the 2 x 512 network and 53% for the 3 x 850 network. The test accuracy\nis similar to the ones reported previously. The result is shown in Table 10 in Appendix\nA.4.\n4.3.3\nAutomated Forward Thinking\nIn order to automate forward thinking more, one might want to automate the choice of\nlayers that will be added into the network. Inspired by the original Cascor algorithm\n[Fahlman and Lebiere, 1990], I use a pool of candidate layers - training each one for a few\nepochs and choosing the best layer from the candidate pool to insert into the network. To\nthe best of my knowledge, this has not been done before.\nFigure 24: The automated forward thinking algorithm, trained for ten layers. Resulting\nnetwork has the layers: [950, 700, 700, 500, 50, 200, 500, 850, 550, 350].\nIn my experiments, I used a candidate pool of eight layers, each layer being trained for\ntwo epochs. The width of each candidate layer is chosen at random within the interval\n[50, 1000], restricted to multiples of 50.\nThe best performing of these eight candidate\nlayers will be inserted into the network and fully trained (until the validation accuracy\nstops improving). This already works reasonably well, as shown in Figure 24. However,\nnot all layers are needed for the ﬁnal model to perform as well as it does.\nThe ﬁrst\ntwo layers oﬀer signiﬁcant increases in accuracy, but this increase in model performance\nﬂattens quickly. A stopping criterion which detects this performance ﬂattening could yield\nsmaller networks with similar performance.\nThe stopping criterion is an opportunity to automate the algorithm further. Early stopping\n40\nseems to be a reasonable choice. I ran some experiments using early stopping, which ends\nthe training when the layer’s ﬁnal validation accuracy hasn’t improved over the previous\nlayer’s ﬁnal validation accuracy. Figure 25 shows that this approach is not ideal. In Figure\n25a, one can argue that training was stopped too early, the network could have improved\nfurther, whereas in Figure 25b, training was stopped too late, adding more layers than\nnecessary as one can see from the ﬂattened training accuracy after the fourth layer was\ninserted. It might help to train each layer for a longer time, in order to have a more\nreliable value for the ﬁnal layer’s validation accuracy.\n(a) Training stopped too early.\n(b) Training stopped too late.\nFigure 25: Automated forward thinking with early stopping when the validation accuracy\ndoes not increase after adding a layer. The network on the left has two layers: [950, 1000],\nwhereas the network on the right has six layers: [950, 500, 150, 300, 50, 300].\nEarly stopping is commonly used to stop training neural networks of ﬁxed architectures\nand to avoid overﬁtting. Normally, the penalty of training a neural network for one (or\na few) epochs is not very high. However, the penalty of adding one (or a few) layers\nmore into a neural network is very large - the complexity of the resulting model increases\nsubstantially. A stricter version of early stopping is needed.\nLayers\nTest Acc\nTrain Acc\nTotal\nTrain\nLayers\n4\n97.86%\n99.95%\n185.16s\n99.33s\n[900, 600, 600, 300]\n4\n97.68%\n100.00%\n180.60s\n99.77s\n[700, 700, 400, 300]\n4\n97.68%\n100.00%\n184.44s\n96.82s\n[900, 900, 300, 300]\n4\n97.64%\n99.99%\n184.97s\n103.40s\n[900, 500, 400, 200]\n4\n97.51%\n99.99%\n146.33s\n66.36s\n[800, 600, 100, 100]\n4\n97.47%\n99.53%\n163.28s\n84.37s\n[1000, 200, 100, 100]\n3\n97.46%\n100.00%\n148.40s\n91.36s\n[1000, 200, 100]\n3\n97.44%\n99.90%\n140.55s\n83.30s\n[900, 100, 100]\n3\n97.30%\n100.00%\n144.90s\n86.53s\n[600, 500, 300]\n3\n97.16%\n99.62%\n112.55s\n55.35s\n[800, 100, 100]\nTable 9: Ten smallest architectures found by running the automated forward thinking\nalgorithm 20 times. Train gives the actual training duration, while Total gives the total\ntraining time, including the candidate unit training.\nConsidering that the training using forward thinking is quite fast, it is computationally\nfeasible to insert more layers into the network than needed, storing the network perfor-\n41\nmance for all number of layers. Based on this, one may assess with how many layers the\ntraining reaches an optimal tradeoﬀof performance against model complexity. Finally,\nunnecessary layers can be removed from the network and the output weight vector can be\nretrained. I implemented and ran the algorithm 20 times, yielding 20 unique architectures.\nI furthermore restrict the algorithm to only use layers of subsequently decreasing widths\nas that is how most neural network architectures are designed. This decision is subject\nto more discussion, though I will ommit this discussion in my thesis. Table 9 shows all\narchitectures using fewer than ﬁve layers. Figure 26 shows the training graph for this.\nFigure 26: The automated forward thinking algorithm run 20 times. Shaded area shows\nthe 95% conﬁdence interval.\nAcross 20 runs of the algorithm, the average test accuracy is 97.54% (with a standard\ndeviation of only 0.17%) - which is better than any other algorithm I have investigated in\nthis thesis. Half of the architectures use below ﬁve layers, the other half uses ﬁve or more\nlayers. The best performing network architecture is [900, 600, 600, 300] with a testing\naccuracy of 97.86%.\nThe increased performance over layered neural networks likely stems from the diﬃculty of\ntraining deep networks with backpropagation. Training the network using forward think-\ning may enable the algorithm to take deeper, more complex architectures into consideration\nand train them more eﬃciently than backpropagation could.\n4.3.4\nConclusion\nIn this section of constructive dynamic learning algorithms, I compared cascading networks\nand forward thinking networks, each being a category of several learning algorithms. The\nmost promising algorithms are CaserRe, forward thinking and automated forward think-\ning. However, as forward thinking does not design its own architecture - it is an algorithm\nto train a neural network - I will not be considering it as an automated architecture design\nalgorithm.\nIn terms of automation, both automated forward thinking and CaserRe show a similar\nlevel of automaticity. Both algortihms search for a suitable architecture automatically, in a\nrandomized greedy way through the use of a candidate pool. Automated forward thinking\nneeds an upper and lower bound for the hidden layers’ widths. In CaserRe, one also needs\nto specify whether hidden units or hidden layers should be inserted in a cascading way\n(and how large these hidden layers may be).\n42\nAutomated forward thinking outperforms CaserRe in the MNIST learning task by 5% on\nthe testing accuracy (CaserRe with 50 cascading hidden layers of 50 units each).\nThe automated forward thinking and CaserRe algorithms have very similar computational\nrequirements (given the same candidate pool sizes). However, CaserRe needs to add more\ncascading units (or layers) into the network than automated forward thinking needs to\nadd layers, hence CaserRe could be said to be slower than automated forward thinking.\nHowever, as there is a signiﬁcant performance diﬀerence between the two algorithms, no\nexact comparison in terms of computational requirements can be made.\nThe resulting model complexity of automated forward thinking networks and CaserRe\nnetworks is diﬃcult to assess, as there is a performance diﬀerence between the two and\nbecause I have no basis for comparing layered networks with cascading networks - other\nthan the empirical evidence that cascading networks do not seem to be able to learn the\nMNIST learning task as well as automated forward thinking.\nIn summary, CaserRe is in need of further investigation in order to get its performance\nlevels to competitive standards, or in order to explain why this cascading structure may\nnot be suitable for a learning task such as MNIST. Automated forward thinking seems\nto be a very well-performing constructive learning algorithm, outperforming all neural\nnetworks trained using standard backpropagation that I covered in this thesis. Further\nempirical evidence is needed to conﬁrm the experimental results from my work in this\nthesis.\n4.4\nConclusion\nThe empirical investigation laid out in this thesis give a preliminary overview of some tech-\nniques for the automated architecture design of deep feedforward neural networks. Good\nresults have been reported and preliminary hypotheses about the suitability of diﬀerent\nalgorithms have been made.\nThe experimental ﬁndings show that diﬀerent neural architecture search algorithms are\nable to ﬁnd suitable network architectures that perform well on the learning task. The\nneural architecture search investigation hints at possible use cases to search for well-\nperforming architectures. Manual search is best used when a lot of knowledge about good\narchitectures is available, either through experience or through available results in the\nliterature. Random search can be used to evenly explore the search space, if the goal is\nto explore the entire search space without any bias introduced through prior knowledge.\nEvolutionary search strikes a compromise between the unbiasedness of random search and\nthe manual search algorithm driven primarily by prior (human) knowledge.\nFurthermore, as constructive dynamic learning algorithms, this thesis includes a prelim-\ninary investigation of two families of such algorithms: the recently proposed forward\nthinking algorithm and the cascade-correlation learning architecture that was proposed\nover twenty years ago. Both algorithms have been implemented on the digit classiﬁcation\nlearning task. I extended both algorithms to improve their performance and level of auto-\nmaticity. Results have been reported on the learning task and the algorithms’ merits have\nbeen discussed. The investigated cascading architectures were not able to perform as well\nas standard layered networks - more work is needed to assess, and possibly enhance, their\nviability on modern learning tasks. The forward thinking algorithm outperformed all lay-\nered neural networks investigated in this thesis and shows promise for future work, despite\n43\nmore work being needed on regularizing this architecture in order to combat overﬁtting\nand improve generalization.\nAutomated forward thinking extends the greedy-wise training proposed by forward think-\ning into a fully automated architecture design algorithm for neural networks. The algo-\nrithm builds a network deeper than the standard MLP architectures found with the search\nalgorithms described above and yields better performance on the test data than any MLP\ninvestigated in this thesis. As such, automated forward thinking shows a promising tech-\nnique that may further be investigated in more comprehensive studies.\nTo summarize, this thesis has given a preliminary overview of exisiting algorithms for\nthe automation of architecture design and reported some results on a selected learning\ntask of digit classiﬁcation. The results of this thesis may be used as a starting point for\nfurther work on fully, and partially, automated architecture design algorithms for deep\nneural networks. If the trend of creating more and more complex deep learning models\ncontinues, these automated architecture design algorithms may be the main tools to design\nneural networks for new learning tasks in the future.\n44\n5\nFuture Work\nAs stated previously, this thesis merely gives a preliminary overview of automated archi-\ntecture design algorithms for deep feedforward neural networks and empirical results to\nguide the direction of future research. Possible future research directions in the ﬁeld of\nautomated architecture design are outlined in this section.\nThe ﬁrst large restriction of this research project is the limitation to feedforward neural\nnetworks.\nFuture research may investigate techniques for the automated architecture\ndesign of other types of neural networks, most notably convolutional neural networks\nand recurrent neural networks. The original forward thinking algorithm has also been\napplied to convolutional neural networks [Hettinger et al., 2017] and a recurrent version\nof cascade-correlation neural networks was proposed by Fahlman [1991].\nNeural architecture search has already been applied to a large variety of diﬀerent neural\nnetworks. For example, Real et al. [2018] evolved a neural network architecture that ul-\ntimately outperformed manually crafted architectures for the ﬁrst time on the ImageNet\nlearning task. They are using the NASNet search space for the evolution of their archi-\ntecture that was designed by Zoph et al. [2018]. Real et al. [2018] further also compared\ntheir evolutionary search with diﬀerent neural architecture search algorithms, speciﬁcally\nwith random search and reinforcement learning applied to neural network architectures.\nFuture work in the ﬁeld may run more comparative studies on neural architecture search\nalgorithms, establishing some empirical evidence for the circumstances under which each\nneural architecture search algorithm performs well.\nMoreover, an in-depth analysis of\ndiﬀerent neural architecture search based on the properties of the search space may be\nable to establish some formal proofs or evidence of certain search algorithms being more\nadvantageous than others, for diﬀerent kinds of learning tasks. Such a general analysis\nis inherently diﬃcult and may only be possible after comprehensive empirical evidence is\navailable on a large set of diverse learning tasks. The survey provided by Elsken et al.\n[2019] on neural architecture search algorithms may be a starting point for such in-depth,\nlargely task-independent research.\nNeural networks that change their network architecture based on the learning task, i.e.\nlearning both the architecture and the connection weights simultaneously have not been\nworked on in the same magnitude as the ﬁeld of neural architecture search, to the best\nof my knowledge. This may be due to the lack of a unifying term of such algorithms.\nWaugh [1994] uses the term dynamic learning for such models, Cortes et al. [2017] uses\nthe term adaptive structural learning, and Yao [1999] uses the term evolving ANNs for\nneural networks whose architecture and parameters are learned simultaneously using evo-\nlutionary search algorithms.\nOne term that may contain all these terms is automated\nmachine learning, or AutoML. However, I was not able to ﬁnd such a term speciﬁcally\nfor neural networks, which could be seen as a subset of AutoML. Moreover, the most\nrecent survey of such models that I was able to ﬁnd at the beginning of this research\nproject was over 20 years old, by Waugh [1994]. In April 2019, Zoeller and Huber [2019]\nsubmitted a survey on automated machine learning to the Journal of Machine Learning\nResearch (pending review). The survey gives a good overview of recent work in the ﬁeld\nof automated machine learning, but I found it to not be comprehensive with respect to\nautomated architecture design for neural networks, as its focus lies more in the automation\nof the entire machine learning pipeline. As automated machine learning can be seen as a\nsuperset of automated architecture design for neural networks, the survey is still highly\n45\nrelevant but not comprehensive. Future work in the ﬁeld of automated neural network\narchitecture design should include a survey that gives an overview of the most relevant\ntechniques - techniques that learn both the architecture and the paramters of the networks\nsimultaneously.\nThe future work in the ﬁeld of automated architecture design for neural network with\nI am proposing in this thesis can be summarized as (1) compiling a survey of the most\nrelevant techniques for automated architecture design, (2) gathering empirical evidence\nfor the performance and comparison of diﬀerent algorithms on diverse learning tasks, and\n(3) establishing formal proofs or concrete evidence for task-independent performance of\ndiﬀerent algorithms.\n46\nA\nAppendix\nA.1\nLoss functions\nA.1.1\nCrossentropy Loss\nThe crossentropy loss is a loss function for multi-class classiﬁcation problems. The cate-\ngorical cross-entropy loss refers to the use of the softmax activation function on the output\nand then the cross-entropy loss.\nLet N be the number of patterns in the dataset, C the number of classes, and pmodel(yi ∈\nCc) is the probability given by the model that pattern i belongs to the class c.\n−1\nN\nN\nX\ni=1\nC\nX\nc=1\n1yi∈Cc log pmodel(yi ∈Cc)\nwhere\n1yi∈Cc =\n(\n1\nyi ∈Cc\n0\nyi /∈Cc\nA.1.2\nError Correlation Maximization\nThe error correlation maximization was proposed to train cascade-correlation neural net-\nwork by Fahlman and Lebiere [1990]. The objective of the algorithm is to maximize the\nerror correlation S, which is given by:\nS =\nX\no∈O\n\f\f\f\f\f\f\nX\np∈P\n(Vp −¯V )(Ep,o −¯Eo)\n\f\f\f\f\f\f\nwhere O is the set of output units and P is the training dataset. Vp is the hidden unit’s\nvalue (its activation) when the training pattern p was passed through the network. ¯V is\nthe hidden unit’s value averaged over all training patterns. Ep,o is the error at the output\nunit o on the training pattern p and ¯Eo is the error at output unit o averaged over all\ntraining patterns.\nA.1.3\nAccuracy Computation\nIn the experiments contained in this thesis, the primary performance metric is the accuracy\nof the neural network’s predictions on a classiﬁcation task. Let C be the set of |C| = c\nclasses. Let the output of the neural network be given by y where y ∈Rc. After passing\nthe output of the neural network through the softmax function σ, we obtain z = σ(y)\nwhere z ∈Rc. The accuracy τ can be computed as follows:\nτ = argmaxi z\n47\nwhere i ∈{1, . . . , c}.\nA.2\nActivation functions\nIn my thesis, I am using three diﬀerent activation functions, namely relu (Rectiﬁed Linear\nUnit), tanh (hyperbolic tangent), and softmax.\nThe relu function is a function relu : R →R:\nrelu(x) =\n(\n0\nx < 0\nx\nx\nThe tanh function is a function tanh : R →R:\ntanh(x) = ex −e−x\nex + e−x ∈[−1, 1]\nBoth the relu and the tanh function can be applied to vectors of real numbers by applying\nthe function to each of its elements individually.\nThe softmax function σ is deﬁned on a vector of K real numbers and normalizes that\nvector into a probability vector, σ : RK →RK:\nσ(z)i =\nezi\nPK\nj=1 ezj ∈[0, 1]\nwhere z ∈RK and 1 ≤i ≤K.\nA.3\nNeural Network Optimization Algorithms\nThis section closely follows and paraphrases the paper by Ruder [2016] which gives a good\noverview of diﬀerent gradient descent optimization algorithms commonly used for training\nneural networks.\nA.3.1\nStochastic Gradient Descent\nThere are diﬀerent variations of the standard gradient descent algorithm that vary in the\namount of data that they take in before updating the parameters.\nLet N be the number of patterns in the training data, η be the learning rate, θ be the\nparameter vector (the vector of all connection weights in a neural network), Li(θ) be the\nloss for pattern i (given parameter vector θ), then the standard (”batch”) gradient descent\nalgorithm updates the weight vector in the following way:\nθt+1 = θt −η 1\nN\nN\nX\ni=1\n∇Li(θt)\n48\nwhere t indicates the step of the gradient descent optimization.\nThis computation can be slow and for large datasets even intractable if they do not ﬁt\ninto memory. We can break down the update rule and update the parameter vector with\nevery single pattern that we train on. This is called stochastic gradient descent, which is\napplied for every pattern i ∈{1, . . . , N}:\nθt+1 = θt −η∇Li(θt)\nHowever, this is not very eﬃcient either, because we update the parameter vector for\nevery single pattern in the dataset. In order to strike a compromise between batch gradient\ndescent and stochastic gradient descent, one may use so-called ”mini-batches”, i.e. subsets\nof the total training data of size m, after each of which the parameters are updated as\nfollows:\nθt+1 = θt −η 1\nm\nm\nX\ni=1\n∇Li(θt)\nThis is called mini-batch gradient descent and it is the algorithm that I am referring\nto as SGD (stochastic gradient descent) because this is what the algorithm is called in\nKeras, the deep learning framework that I am using for my code implementations. For all\nexperiments found in this thesis, I used a mini-batch size of 128.\nA.3.2\nRMS Prop\nRMS Prop (Root Mean Square Propagation) is the optimization algorithm that I used\nto train most neural networks in this thesis. It deals with some of the challenges that\nvanilla gradient descent methods face. RMS Prop belongs to a family of gradient descent\noptimization algorithms that use momentum and/or adaptive learning rates.\nA more\ndetailed discussion of these methods can be found in Ruder [2016]. Herein, I am using\nRMS Prop without further discussion.\nIn RMS Prop, the learning rate is adapted for every single parameter in the parameter\nvector θ. The idea is to divide the learning rate for a weight by a running average of the\nmagnitudes of recent gradients for that weight [Tieleman and Hinton, 2012]. This running\naverage is computed by:\nvt+1(θ) = γ vt(θ) + (1 −γ)∇Li(θ)2\nwhere vt is the moving average at step t and γ is the momentum rate, or forgetting factor.\nThe parameter vector is then updated as follows:\nθt+1 = θt −\nη\np\nvt+1(θt)\n∇Li(θt)\nIn my implementations, I am using the recommended values γ = 0.9 and η = 0.001 [Ruder,\n2016] and [Tieleman and Hinton, 2012].\n49\nA.4\nFuther Results\nLayers\nEpochs\nTime\nTrain acc\nTest acc\n5 x 750\n50 / 27\n266.9s / 493.1s\n100.00% / 97.58%\n97.67% / 96.38%\n2 x 512\n33 / 28\n92.3s / 123.3s\n99.95% / 98.53%\n97.42% / 97.57%\n3 x 850\n24 / 21\n133.1s / 284.16s\n99.78% / 98.36%\n97.36% / 96.97%\nTable 10: Network performances when trained using forward thinking (left values) and\nbackpropagation (right values).\n50\nReferences\nBartlett, P. L., Maiorov, V., and Meir, R. (1999). Almost Linear VC Dimension Bounds\nfor Piecewise Polynomial Networks. In Kearns, M. J., Solla, S. A., and Cohn, D. A.,\neditors, Advances in Neural Information Processing Systems 11, pages 190–196. MIT\nPress.\nBergstra, J. and Bengio, Y. (2012). Random Search for Hyper-Parameter Optimization.\nThe Journal of Machine Learning Research, 13(Feb):281–305.\nBianchini, M. and Scarselli, F. (2014). On the Complexity of Shallow and Deep Neural\nNetwork Cassiﬁers. In European Symposium on Artiﬁcial Neural Networks, volume 22,\npages 371–376.\nChoromanska, A., Henaﬀ, M., Mathieu, M., Arous, G. B., and LeCun, Y. (2015). The Loss\nSurfaces of Multilayer Networks. In Proceedings of the 18th International Conference\non Artiﬁcial Intelligence and Statistics, volume 38 of JMLR: W&CP, pages 192–204.\nJMLR.org.\nConneau, A., Schwenk, H., Barrault, L., and Lecun, Y. (2016). Very Deep Convolutional\nNetworks for Text Classiﬁcation. arXiv preprint arXiv:1606.01781.\nCortes, C., Gonzalvo, X., Kuznetsov, V., Mohri, M., and Yang, S. (2017). Adanet: Adap-\ntive Structural Learning of Artiﬁcial Neural Networks. In Proceedings of the 34th In-\nternational Conference on Machine Learning, volume 70, pages 874–883. JMLR.\nEldan, R. and Shamir, O. (2016). The Power of Depth for Feedforward Neural Networks.\nIn Conference on Learning Theory, volume 49, pages 907–940.\nElsken, T., Metzen, J. H., and Hutter, F. (2019). Neural Architecture Search: A Survey.\nThe Journal of Machine Learning Research, 20(55):1–21.\nFahlman, S. E. (1991). The Recurrent Cascade-Correlation Architecture. In Lippmann,\nR. P., Moody, J. E., and Touretzky, D. S., editors, Advances in Neural Information\nProcessing Systems 3, pages 190–196. Morgan-Kaufmann.\nFahlman, S. E. and Lebiere, C. (1990). The Cascade-Correlation Learning Architecture.\nIn Touretzky, D. S., editor, Advances in Neural Information Processing Systems 2, pages\n524–532. Morgan-Kaufmann.\nFrean, M. (1990).\nThe Upstart Algorithm: A Method for Constructing and Training\nFeedforward Neural Networks. Neural Computation, 2(2):198–209.\nGoodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Learning. MIT press.\nHanson, S. J. (1990). Meiosis Networks. In Touretzky, D. S., editor, Advances in Neural\nInformation Processing Systems 2, pages 533–541. Morgan-Kaufmann.\nHarvey, M. (2017). Lets evolve a neural network with a genetic algorithm code included.\nHassibi, B., Stork, D. G., and Wolﬀ, G. (1994). Optimal Brain Surgeon: Extensions and\nPerformance Comparisons. In Cowan, J. D., Tesauro, G., and Alspector, J., editors, Ad-\nvances in Neural Information Processing Systems 6, pages 263–270. Morgan-Kaufmann.\nHassibi, B., Stork, D. G., and Wolﬀ, G. J. (1993). Optimal brain surgeon and general\nnetwork pruning. In IEEE International Conference on Neural Networks, volume 1,\npages 293–299. IEEE.\n51\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016).\nDeep Residual Learning for Image\nRecognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, volume 1, pages 770–778.\nHettinger, C., Christensen, T., Ehlert, B., Humpherys, J., Jarvis, T., and Wade, S. (2017).\nForward Thinking: Building and Training Neural Networks One Layer at a Time. arXiv\npreprint arXiv:1706.02480.\nHinton, G. E. (2012). A Practical Guide to Training Restricted Boltzmann Machines. In\nNeural Networks: Tricks of the Trade, pages 599–619. Springer.\nHornik, K., Stinchcombe, M., and White, H. (1989). Multilayer Feedforward Networks\nAre Universal Approximators. Neural Networks, 2(5):359–366.\nKeras (2019). Simple Deep Neural Network on the MNIST Dataset.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2017). ImageNet Classiﬁcation with\nDeep Convolutional Neural Networks. Commununity of the Association of Computing\nMachinery, 60(6):84–90.\nKrogh, A. and Hertz, J. A. (1992). A Simple Weight Decay Can Improve Generalization.\nIn Moody, J. E., Hanson, S. J., and Lippmann, R. P., editors, Advances in Neural\nInformation Processing Systems 4, pages 950–957. Morgan-Kaufmann.\nKumar, A., Irsoy, O., Ondruska, P., Iyyer, M., Bradbury, J., Gulrajani, I., Zhong, V.,\nPaulus, R., and Socher, R. (2016). Ask Me Anything: Dynamic Memory Networks for\nNatural Language Processing. In Proceedings of the 33rd International Conference on\nMachine Learning, volume 48, pages 1378–1387. JMLR.\nLarochelle, H., Erhan, D., Courville, A., Bergstra, J., and Bengio, Y. (2007). An Empirical\nEvaluation of Deep Architectures on Problems with Many Factors of Variation.\nIn\nProceedings of the 24th International Conference on Machine Learning, pages 473–480.\nACM.\nLeCun, Y. and Bengio, Y. (1998). Convolutional Networks for Images, Speech, and Time\nSeries. In Arbib, M. A., editor, The Handbook of Brain Theory and Neural Networks,\npages 255–258. MIT Press.\nLeCun, Y., Bottou, L., Bengio, Y., Haﬀner, P., et al. (1998). Gradient-Based Learning\nApplied to Document Recognition. Proceedings of the IEEE, 86(11):2278–2324.\nLeCun, Y., Denker, J. S., and Solla, S. A. (1990). Optimal Brain Damage. In Touretzky,\nD. S., editor, Advances in Neural Information Processing Systems 2, pages 598–605.\nMorgan-Kaufmann.\nLeCun, Y. A., Bottou, L., Orr, G. B., and M¨uller, K.-R. (2012). Eﬃcient Backprop. In\nNeural Networks: Tricks of the Trade, pages 9–48. Springer.\nLevin, A. U., Leen, T. K., and Moody, J. E. (1994). Fast Pruning Using Principal Com-\nponents. In Cowan, J. D., Tesauro, G., and Alspector, J., editors, Advances in Neural\nInformation Processing Systems 6, pages 35–42. Morgan-Kaufmann.\nLevine, S., Finn, C., Darrell, T., and Abbeel, P. (2016). End-To-End Training of Deep\nVisuomotor Policies. The Journal of Machine Learning Research, 17(1):1334–1373.\nLittmann, E. and Ritter, H. (1992). Cascade network architectures. In [Proceedings 1992]\n52\nIJCNN International Joint Conference on Neural Networks, volume 2, pages 398–404.\nIEEE.\nLittmann, E. and Ritter, H. (1993). Generalization Abilities of Cascade Network Archi-\ntecture. In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, Advances in Neural\nInformation Processing Systems 5, pages 188–195. Morgan-Kaufmann.\nMaass, W., Schnitger, G., and Sontag, E. D. (1994). A Comparison of the Computational\nPower of Sigmoid and Boolean Threshold Circuits. In Theoretical Advances in Neural\nComputation and Learning, pages 127–151. Springer.\nMezard, M. and Nadal, J.-P. (1989). Learning in Feedforward Layered Networks: The\nTiling Algorithm. Journal of Physics A: Mathematical and General, 22(12):2191.\nMozer, M. C. and Smolensky, P. (1989). Skeletonization: A Technique for Trimming the\nFat from a Network via Relevance Assessment. In Touretzky, D. S., editor, Advances in\nNeural Information Processing Systems 1, pages 107–115. Morgan-Kaufmann.\nNowlan, S. J. and Hinton, G. E. (1992). Simplifying Neural Networks by Soft Weight-\nSharing. Neural Computation, 4(4):473–493.\nPoole, B., Lahiri, S., Raghu, M., Sohl-Dickstein, J., and Ganguli, S. (2016). Exponen-\ntial Expressivity in Deep Neural Networks Through Transient Chaos. In Lee, D. D.,\nSugiyama, M., Luxburg, U. V., Guyon, I., and Garnett, R., editors, Advances in Neural\nInformation Processing Systems 29, pages 3360–3368. Curran Associates, Inc.\nPrechelt, L. (1997). Investigation of the Cascor Family of Learning Algorithms. Neural\nNetworks, 10(5):885–896.\nRaghu, M., Poole, B., Kleinberg, J., Ganguli, S., and Dickstein, J. S. (2017). On the Ex-\npressive Power of Deep Neural Networks. In Proceedings of the 34th International Con-\nference on Machine Learning, volume 70 of JMLR: W&CP, pages 2847–2854. JMLR.org.\nReal, E., Aggarwal, A., Huang, Y., and Le, Q. V. (2018). Regularized evolution for image\nclassiﬁer architecture search. arXiv preprint arXiv:1802.01548.\nReed, R. (1993). Pruning algorithms - a survey. IEEE Transactions on Neural Networks,\n4(5):740–747.\nRuder, S. (2016).\nAn Overview of Gradient Descent Optimization Algorithms.\narXiv\npreprint arXiv:1609.04747.\nSermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., and LeCun, Y. (2013). Over-\nfeat: Integrated Recognition, Localization and Detection Using Convolutional Networks.\narXiv preprint arXiv:1312.6229.\nSietsma, J. (1988). Neural Net Pruning - Why and How. In Proceedings of International\nConference on Neural Networks, volume 1, pages 325–333.\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M.,\nSifre, L., Kumaran, D., Graepel, T., et al. (2018). A General Reinforcement Learning Al-\ngorithm that Masters Chess, Shogi, and Go through Self-Play. Science, 362(6419):1140–\n1144.\nSjogaard, S. (1991). A Conceptual Approach to Generalisation in Dynamic Neural Net-\nworks. PhD thesis, Aarhus University.\n53\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014).\nDropout: A Simple Way to Prevent Neural Networks from Overﬁtting. The Journal of\nMachine Learning Research, 15(1):1929–1958.\nTelgarsky, M. (2015).\nRepresentation Beneﬁts of Deep Feedforward Networks.\narXiv\npreprint arXiv:1509.08101.\nTieleman, T. and Hinton, G. (2012). Lecture 6.5-rmsprop.\nVapnik, V. N. and Chervonenkis, A. Y. (2015). On the Uniform Convergence of Relative\nFrequencies of Events to Their Probabilities. In Measures of Complexity, pages 11–30.\nSpringer.\nVinyals, O., Toshev, A., Bengio, S., and Erhan, D. (2015). Show and tell: A neural image\ncaption generator. In IEEE Conference on Computer Vision and Pattern Recognition,\nvolume 28, pages 3156–3164.\nWang, Z., Di Massimo, C., Tham, M. T., and Morris, A. J. (1994). A Procedure for De-\ntermining the Topology of Multilayer Feedforward Neural Networks. Neural Networks,\n7(2):291–300.\nWaugh, S. (1994). Dynamic learning algorithms. Department of Computer Science, Uni-\nversity of Tasmania.\nWu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M.,\nCao, Y., Gao, Q., Macherey, K., et al. (2016). Google’s Neural Machine Translation\nSystem: Bridging the Gap Between Human and Machine Translation. arXiv preprint\narXiv:1609.08144.\nWynne-Jones, M. (1992). Node Splitting: A Constructive Algorithm for Feed-Forward\nNeural Networks. In Moody, J. E., Hanson, S. J., and Lippmann, R. P., editors, Advances\nin Neural Information Processing Systems 4, pages 1072–1079. Morgan-Kaufmann.\nYang, J. and Honavar, V. (1998). Experiments with the Cascade-Correlation Algorithm.\nMicrocomputer Applications, 17(2):40–46.\nYao, X. (1999). Evolving Artiﬁcial Neural Networks. Proceedings of the IEEE, 87(9):1423–\n1447.\nZhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2016). Understanding Deep\nLearning Requires Rethinking Generalization. arXiv preprint arXiv:1611.03530.\nZoeller, M. and Huber, M. (2019). Survey on automated machine learning. arXiv preprint\narXiv:1904.12054.\nZoph, B., Vasudevan, V., Shlens, J., and Le, Q. V. (2018). Learning Transferable Archi-\ntectures for Scalable Image Recognition. In The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), volume 1.\n54\n",
  "categories": [
    "cs.LG",
    "cs.NE",
    "stat.ML"
  ],
  "published": "2019-08-22",
  "updated": "2019-08-22"
}