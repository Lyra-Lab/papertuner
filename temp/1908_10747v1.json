{
  "id": "http://arxiv.org/abs/1908.10747v1",
  "title": "Language Tasks and Language Games: On Methodology in Current Natural Language Processing Research",
  "authors": [
    "David Schlangen"
  ],
  "abstract": "\"This paper introduces a new task and a new dataset\", \"we improve the state\nof the art in X by Y\" -- it is rare to find a current natural language\nprocessing paper (or AI paper more generally) that does not contain such\nstatements. What is mostly left implicit, however, is the assumption that this\nnecessarily constitutes progress, and what it constitutes progress towards.\nHere, we make more precise the normally impressionistically used notions of\nlanguage task and language game and ask how a research programme built on these\nmight make progress towards the goal of modelling general language competence.",
  "text": "Language Tasks and Language Games: On Methodology\nin Current Natural Language Processing Research\nDavid Schlangen\nComputational Linguistics / Department of Linguistics\nUniversity of Potsdam, Germany\ndavid.schlangen@uni-potsdam.de\nAbstract\n“This paper introduces a new task and a new\ndataset”, “we improve the state of the art in\nX by Y” – it is rare to ﬁnd a current natural\nlanguage processing paper (or AI paper more\ngenerally) that does not contain such state-\nments. What is mostly left implicit, however,\nis the assumption that this necessarily consti-\ntutes progress, and what it constitutes progress\ntowards. Here, we make more precise the nor-\nmally impressionistically used notions of lan-\nguage task and language game and ask how a\nresearch programme built on these might make\nprogress towards the goal of modelling general\nlanguage competence.\n1\nIntroduction\nRecently, seemingly ever other natural language\nprocessing paper introduces a new task and a new\ndataset.1 We join some other recent papers (e.g.\nYogatama et al., 2019) in asking whether there\nis any coherence to this research approach, under\nwhich conditions it can lead to progress, and to-\nwards what. What we do differently, however, is\nto look at the fundamental assumptions behind this\napproach. We try to deﬁne central notions, in order\nto be able to discuss the structure of the typically\nonly implicit formulated approach more clearly.\nIn our argumentation, we distinguish between\nlanguage tasks, such as for example “describe\nthis image”, or “translate this sentence”—that is,\nsingle-step tasks that involve in an essential way\nnatural language material, but not necessarily only\nlanguage material—; micro worlds, which are en-\nvironments that produce disinterested responses to\nactions, thereby possibly simulating the behaviour\nof independently existing systems; and dialogue\n1Not quite, but not very far. Looking at the 2018 long\nand short paper proceedings of ACL and EMNLP, we get 94\nhits for “introduce new dataset”, 20 hits for “introduce new\ncorpus”, and 101 hits for “introduce new task”.\ngames as repeated and connected language tasks,\nwhich these environments enable. We deﬁne these\nnotions ﬁrst and think about general ways of eval-\nuating their relevance. We close with some tenta-\ntive recommendations for how to connect individ-\nual modelling contributions with the larger enter-\nprise of modelling language processing.\n2\nTasks, Worlds, and Games\n2.1\nTasks\nA language task is a mapping between an input\nspace and an output or action space, at least one\nof which contains natural language expressions.2\nThe mapping has to conform to a task description,\nwhich is typically given only informally, mak-\ning reference to theoretical or pre-theoretical con-\nstructs external to the deﬁnition, such as “transla-\ntion” or “is true of”. We call this an intensional\ndescription. Often, a task is also speciﬁed exten-\nsionally through the provision of a dataset of ex-\namples of the mapping (that is, pairs of state and\naction), X = {(x1, y1), . . . , (xm, ym)}, where the\nassumption is that (x, y) ∈X\n→y = L(x) (L\nbeing the task mapping).\nThis very general deﬁnition essentially covers\nmuch, if not all of natural language processing.\nFor example, translation can be seen as a language\ntask where the state space consists of expressions\nin one language, the action space of expressions in\nanother language, and the task description is that\nin each pair in the mapping, the second element\nbe a translation of the ﬁrst. We can further dis-\ntinguish understanding tasks, where the mapping\nrequires demonstration of sensitivity to language\nmeaning (however that is to be further deﬁned); in-\nterpretation tasks, where the input space contains\nlanguage expressions that are to be “understood”;\n2A formal deﬁnition of this and the other notions is given\nin the Appendix.\narXiv:1908.10747v1  [cs.CL]  28 Aug 2019\nSet CL of capabilites of \ncompetent language user\nTask T \n• task \ndescription\nDataset D \n• collection \ninstructions\nModel M \n• architecture\nCognitive \nCapability C\nCognitive \nCapability C’\nCognitive \nCapability C’’\noptimized for\nexempliﬁes\ninvolves\ninvolves\nTask T’ \n• (task \ndescription)\nEnvironment E \n• micro-world descr.\nModel M’ \n• architecture\nprovides non-linguistic \ninput, rewards, etc. for\ninvolves\nGame G \n• game descr.\nDataset D’ \n• collection \ninstructions\nexempliﬁes\noptimized  \nfor\ninvolves\n…\nFigure 1: The structure of relations between the research objects model, dataset, task, game, environment, cognitive\ncapability.\ngeneration tasks, where the output does (with a\ngiven task potentially being both an interpretation\nand a generation task); reference tasks, where the\nunderstanding is shown by relating linguistic and\nnon-linguistic material, and inference tasks, where\nlinguistic material is related.\n2.2\nWorlds\nThe language competence of humans plays out in\nrepeated task, not single-step ones as described\nin the previous section, and it plays out in con-\ntexts where language use is embedded in a non-\nlinguistic context. To study such repeated, situ-\nated games, much recent work has made use of\nenvironment simulators that compute reactions to\nactions performed within them, in accordance to\nthe assumed (or actual) rules of the domain they\nrepresent.3 Such environments can again be de-\nscribed as mappings, in this case from an action to\nan environmental response (a state), where again\nthe mapping conforms to a description of which\nreal-world counterpart it is intended to model, and\nhow.4\n3See for example (Savva et al., 2019; Adams et al., 2012;\nJohnson et al., 2016; Urbanek et al., 2019; Baroni et al.,\n2017a; Xia et al., 2018; Yan et al., 2018; Misra et al., 2018;\nCˆot´e et al., 2018; Bennett and Shatkhin, 2018; Anderson\net al., 2018; Savva et al., 2017; Gordon et al., 2017; Brodeur\net al., 2017; Chang et al., 2017; Janarthanam and Lemon,\n2011; Baroni et al., 2017b; Byron et al., 2007; Yamauchi\net al., 2013).\n4In this desire to model the relevant aspects of a domain,\nand in the assumption that from dealing with a simulated\nenvironment transferable knowledge about dealing with the\noriginal enviroment can be achieved, this approach is remi-\nniscent of the AI microworlds of the 1970s—“we see solv-\ning a problem often as getting to know one’s way around a\n2.3\nGames\nAn interaction game is a setting where players\ncan produce actions (a special kind of which can\nbe messages), possibly regulated by some regime\non when they can do this and who can observe\nthem. A priviledged, but disinterested player Na-\nture can respond to those actions, by providing\ngame-relevant information (and interfacing with\nthe environment in which the game is embedded).\nAgain, we assume that there is an informal game\ndescription which speciﬁes which, if any, other-\nwise existing activity the game is meant to model.\nA language game is an interaction game that em-\nbeds language tasks which govern the actions of\nthe players. For example, one player asking and\nthe other player answering questions would be a\nlanguage game that poses repeated language tasks\nto the players.\nSummarising this section, Figure 1 shows the\nstructure of relations between the notions intro-\nduced here: Models relate to Tasks, via Datasets,\nGames are realised in Environments. Capabilities\nwill be discussed below.\n3\nWhat makes a good task, world, game?\nLet’s now assume we encounter a paper that pro-\nposes a new dataset, language task, microworld, or\nlanguage game. How can we evaluate the contri-\nbution that is made?\n‘micro-world’ in which the problem exists.” (Minsky and\nPapert, 1972)—and perhaps susceptible to similar kinds of\ncritiques as these attempts (Dreyfus, 1981; Marr, 1982).\n3.1\nTasks\n... and datasets\nAs mentioned, tasks are often\nexempliﬁed by the provision of a dataset of ex-\namples of the task being executed by agents that\nare assumed to be capable of doing so—typically,\nhuman participants in experiments or data collec-\ntion efforts. Evaluating such datasets in itself is\nrelatively straightforward. First, it should be ver-\niﬁed, which is to check whether the provided in-\nput/output pairs can indeed be judged correct rel-\native to the task (in its intensional description). If\nthe examples are collected speciﬁcally for the pur-\npose of exemplifying the task, this is the process\nof controlling annotation, and standard method-\nologies exists (Artstein and Poesio, 2008).\nValidating a dataset is a less formalised process.\nIt comprises arguing that the dataset indeed exem-\npliﬁes the task intension well. For example, pairs\nonly of images of giraffes and sentences describ-\ning them would arguably not exemplify the gen-\neral task of image description very well (even if\nthe descriptions are accurate), while perhaps ex-\nemplifying the task of giraffe image description.\nAnother way to evaluate datasets is by provid-\ning a model of the task learned on parts of it,\nand testing it on the remaining part (for which a\ncomparison, or loss, function on input/output pairs\nmust be provided as well). If a model can “solve”\nthe dataset even when not given information that\nfor theoretical or pre-theoretical reasons is seen to\nbe crucial, the dataset can be considered an un-\nsatisfactory exempliﬁcation of the task. E.g., in\na visual (polar) question answering setting (Antol\net al., 2015), if in a dataset all and only the expres-\nsions that mention giraffes are true, a model would\nnot need to take the images into account at all to\nperform well (as it would just need to detect the\npresence of giraffe-related words), which would\nbe evidence that the dataset is deﬁcient relative to\nthe task description.\n... in themselves\nHow can a task in itself be mo-\ntivated and evaluated? This is easy, if it has a direct\nvalue to a consumer (such as translation presum-\nably has), which can be measured. If the consumer\nis a computer system that processes the output of\nthe task further, the burden of evaluation is sim-\nply shifted to the system as a whole. If the inter-\nest is in replicating with a theoretically motivated\nmodel performance characteristics of humans at-\ntempting the task, the task can be evaluated for its\npower helping distinguish between different mod-\nelling choices.\nA recent trend, however, has been to motivate\ntasks in a different way, neither via their inher-\nent practical use, nor as answering questions about\nlanguage processing as implemented in humans.\nThe argument roughly goes as follows (even if typ-\nically only made implicitly): To be good at task\nT, an agent must possess a set CT of capabilities\n(of representational or computational nature). If\nthe c ∈CT are capabilities that competent lan-\nguage users can be shown or argued to possess and\nmake use of in using language—let’s call the set\nof these capabilities of a competent language user\nCL, so that CT ⊆CL— then being able to model\nthese capabilities (via modelling the task) results\nin progress towards the ultimate goal, which is to\nmodel competent language use. And hence, any\ntask T that comes with an interesting set CT is a\ngood task.5\nUnder what conditions does this argument\nwork? First of all, the assumed connection to the\nset of capability must indeed be there. We have\nalready seen a way to challenge a claimed con-\nnection, namely through providing a model that\ncan “solve” a given task (via a dataset) while not\nhaving access to information that should be in-\nvolved in the capability. (Although this challenge\n5To give some examples of informal versions of this ar-\ngument, and chosing papers more or less randomly, here are\nsome quotes (typically from the introduction sections of their\nrespective papers):\nFrom the paper that introduced the visual question answer-\ning task (Antol et al., 2015): “What makes for a compelling\nAI-complete task? We believe that in order to spawn the next\ngeneration of AI algorithms, an ideal task should (i) require\nmulti-modal knowledge beyond a single sub-domain (such as\nCV) and (ii) have a well-deﬁned quantitative evaluation met-\nric to track progress. [. . . ] Open-ended questions require a\npotentially vast set of AI capabilities to answer – ﬁne-grained\nrecognition (e.g., What kind of cheese is on the pizza?), ob-\nject detection (e.g., How many bikes are there?), activity\nrecognition (e.g., Is this man crying?), knowledge based rea-\nsoning (e.g., Is this a vegetarian pizza?), and commonsense\nreasoning (e.g., Does this person have 20/20 vision?, Is this\nperson expecting company?).”\nAbout the natural language inference problem, and at-\ntempting at least an implicit structuring of the space of ca-\npabilities, Condoravdi et al. (2003) write: “The ability to rec-\nognize such semantic relations is clearly not a sufﬁcient cri-\nterion for language understanding: there is more to language\nunderstanding than just being able to tell that one sentence\nfollows from another. But we would argue that it is a mini-\nmal, necessary criterion.”\nWilliams et al. (2018), on the modern version of this task:\n“The task of natural language inference (NLI) is well posi-\ntioned to serve as a benchmark task for research on NLU.\n[. . . ]\nIn particular, a model must handle phenomena like\nlexical entailment, quantiﬁcation, coreference, tense, belief,\nmodality, and lexical and syntactic ambiguity.”\nin the ﬁrst instance only targets the dataset and not\nthe task itself.) Secondly, following usual scien-\ntiﬁc methodology (Popper, 1934), we can rank the\nworth of an instantiation of this argument by how\nprecisely the capability is speciﬁed, from the triv-\nially correct “task T involves the capability to do\ntask T” to a statement that could be wrong (and\nhence involves other theoretical constructs), e.g.\n“task T involves the capability to compute the syn-\ntactic structure of a natural language sentence”.\nFurthermore, we can rank the motivation given\nfor a task by how explicit it is in delineating the set\nof capabilities it involves, along two dimensions.\nIn the one dimension (separability), in the most\nextreme form, the claim would be that the set of\ncapabilities CT is fully separated from CL \\ CT ,\nand hence there is no danger of overﬁtting solu-\ntions to CT in such a way as would be detrimental\nfor the remaining capabilities.6 In the other di-\nmension (exhaustivity), the strongest claim would\nbe that T brings out all there is to c ∈CT , and that\nanother task T ′, insofar as it requires c as well,\ncould be handled by a model of c built with only\nT in mind. In the other extreme, we only have\n“c as required by T”, which does less to indicate\nprogress beyond T.\nAs this discussion suggests, it seems difﬁcult to\nproperly motivate a task without relying, at least\nimplicitly, on assumptions about how CL decom-\nposes.\n3.2\nWorlds\nWe have introduced “worlds” (or environments, or\nsimulators) above as the settings that enable re-\npeated tasks (games), and in parts their evaluation\nis connected to those. However, as the providers\nof inputs to a task in reaction to its outputs, where\nthis mapping must also conﬁrm to a description,\nwe can also evaluate them qua environment.\nFirst, the notions of validation and veriﬁcation\napply as well: An environment should match, as\nwell as possible, its stated description of its re-\nlation to a real-world counterpart, and should be\nveriﬁed to be correct according to its speciﬁca-\ntion. Environments like those needed for games\nlike Chess and Go (e.g. Silver et al., 2017), or Set-\ntlers of Catan (e.g. Afantenos et al., 2012), can\nfully model their intended real-world counterpart,\nwhereas others can only be modelled aproximately\n6That is, unless the claim is that CT = CL; the use of\n“AI-complete” in the quote above from Antol et al. (2015)\nsuggest that is something that not everyone shies away from.\n(e.g., “the interior of a house, through which an\nagent moves, from the perspective of that agent”\n— as the body of the agent and the agent’s aware-\nness of it, arguably, is a part of the environment,\nthis would entail modelling this as well).\nA detailed list of desiderata for such “artiﬁ-\ncial general intelligence” environments is given\nby Adams et al. (2012) (see also Baroni et al.\n(2017b)): “C1. The environment is complex, with\ndiverse, interacting and richly structured objects.\nC2. The environment is dynamic and open. C3.\nTask-relevant regularities exist at multiple time\nscales. C4. Other agents impact performance. C5.\nTasks can be complex, diverse and novel. C6. In-\nteractions between agent, environment and tasks\nare complex and limited. C7. Computational re-\nsources of the agent are limited. C8. Agent exis-\ntence is long-term and continual.” While this list\nmixes what we separate as demands on environ-\nments and on games set in them, it should be use-\nful to evaluate proposed environments.\nIn analogy to what we observed for tasks, it\nseems that trying to make progress through mod-\nelling tasks in simulated worlds entails making an-\nother separability hypothesis, which assumes that\nthe natural competence of handling the world as a\nwhole is separable into handling various parts of it,\nwhich can be “knit” together to form the whole.7\n3.3\nGames\nWhat makes for an interesting language game?\nFirst we note that games seem to be less well ex-\nempliﬁed by datasets than (non-game) tasks are, as\nfor them the relation between an input and an out-\nput is much less constrained, and the output can be\na sequence of actions rather than a simple one. To\ngive an example, in the language-navigation task\n(e.g. Anderson et al., 2018; Ma et al., 2019), while\nthe input is a single datum (a verbal description of\na goal location), the output is a sequence of navi-\ngation actions.8\nAs the site for repeated and connected language\n7“[W]e feel [the micro-worlds] are so important that we\nplan to assign a large portion of our effort to developing a col-\nlection of these micro-worlds and ﬁnding how to embed their\nsuggestive and predictive powers in larger systems without\nbeing misled by their incompatibility with literal truth. [. . . ]\nIn order to study such problems, we would like to have collec-\ntions of knowledge for several ‘micro-worlds’, ultimately to\nlearn how to knit them together.” (Minsky and Papert, 1972).\n8On the role of environments/games vis-`a-vis datasets,\nSavva et al. (2019) state their belief that “simulators will as-\nsume the role played previously by datasets”; no further ar-\ngumentation is given to support this belief, however.\ntasks, we can evaluate a game again for how it\nconnects to language capabilities, and for how the\ngame setting improves over a (non-repeated) task\nsetting.\nFor example, some previous work has\nshown that language production under (interac-\ntive) task constraints is different from null-context\nlanguage production (Ilinykh et al., 2018; da Silva\nRocha and Paraboni, 2016).\nFinally, it seems that for language games there\nis a natural supremum, which would be “unre-\nstricted situated language interaction”.\nWe can\nthen also evaluate proposed tasks for how close\nthey come to this ultimate form, for some deﬁni-\ntion of “closeness”.\n4\nMaking Progress\n¨Uberhaupt hat der Fortschritt das an\nsich, daß er viel gr¨oßer ausschaut als\ner wirklich ist.\n(It is in the nature of\nprogress that it appears much greater\nthan it actually is.)\nNESTROY, via\nWittgenstein (1953/84)\nResearch is an incremental enterprise, and new\ntasks, datasets, models, environments, and games\nare introduced in a context of existing ones. We\ncan now distinguish several modes of making\nprogress in the general project, by relating new\nproposals to existing ones.\n4.1\nBetter Models\nThis is the mode of progress for most of the cur-\nrent work in NLP / AI, and it is also the one that\nneeds the least argumentative support: If a model\ntrained on a given dataset performs better, in terms\nof the same pre-deﬁned metrics, than a previous\nmodel, then progress has been made. Or would\nthat it were so simple: Even in this setting, con-\nsiderations of computing power spent should ar-\nguably also factor, and how to account for use of\nlarger datasets (for example in pre-training), when\njudging models that achieve better results.9 And\nultimately, the worth of progress in modelling a\nparticular task rests only on the worth of the task\nitself.\nA different way in which models can improve\nover others is in how well they are suited for trans-\nfer to other tasks (see Ruder (2019) for a recent\noverview of such transfer-learning).\n9See\ne.g.\nthe\nrecent\ndiscussion\non\nhttps://\nhackingsemantics.xyz/2019/leaderboards/.\n4.2\nBetter Datasets\nWe have discussed the relation between tasks and\ndatasets above. If a dataset can be shown to be\nsuccessfully modelled even in the absence of in-\nformation that is deemed criticially involved in the\ncapability of interest, it can be considered invalid\nrelative to the task description, and, if the interest\nin the task is kept, a new dataset must be found.\nThe task of visual question answering provides\nan interesting example case of such a develop-\nment. After Antol et al. (2015) introduced the ﬁrst\nlarge scale dataset for this task, it quickly became\nclear that this dataset could be handled competi-\ntively by models that were deprived of visual in-\nput (“language bias”, as noted e.g. by Jabri et al.,\n2016). This problem was then addressed by Goyal\net al. (2017) with the construction of a less biased\n(and hence more valid) corpus. Targetting the set\nof capabilities involved in the task, Andreas et al.\n(2016) noted that “questions in most existing nat-\nural image datasets are quite simple, for the most\npart requiring that only one or two pieces of infor-\nmation be extracted from an image in order to an-\nswer it successfully”, which makes it not challeng-\ning enough in terms of “[c]ompositionality, and\nthe corresponding ability to answer questions with\narbitrarily complex structure”. To improve on that,\nthey introduced the SHAPES dataset which pairs\nsynthetic images with synthetic, programmatically\ngenerated sentences that contain spatial relations.\nTwo more datasets explored this direction (John-\nson et al., 2017; Suhr et al., 2017), until another\ndataset (Suhr et al., 2018) progressed beyond the\nuse of synthetic images, in effect claiming that nat-\nural images are more valid for the task described\nas “visual question answering”.\n4.3\nBetter Tasks and Games\nWe see a movement towards involvement of ca-\npabilities already in the previous two sections.\nWhere a model can be judged to be better than an-\nother one (trained on the same data) simply by the\nlinear order imposed by the evaluation metric, a\ndataset must be argued to be more representative\nof a task by taking recourse to the capabilities of\ninterest (e.g., in the example discussed just above,\nthat of handling compositionality).\nAs shown in Figure 1, tasks are only grounded\n(to their left in the diagram) by capabilities (and\ngames by tasks), and hence an argumentation for\na task T ′ in relation to a previous task T should\nTask T \n• task \ndescription\nDataset D \n• collection \ninstructions\nModel M \n• architecture\noptimized for\nexempliﬁes\nSet CL of capabilites of \ncompetent language user\nCognitive \nCapability C\nCognitive \nCapability C’\nCognitive \nCapability C’’\ninvolves\ninvolves\n…\nModel M’ \n• architecture\nbetter optimized \nfor\nDataset D’ \n• collection \ninstructions\nexempliﬁes\nexempliﬁes \nT better as\nTask T’ \n• task \ndescription\ninvolves\nis more  \nchallenging than\ninvolves\nimproved model\nimproved dataset\nimproved task\nFigure 2: Three ways of making progress: improving models, improving datasets, improving tasks. (Not shown:\ndevising models that handle more than one dataset.)\nideally make mention of how CT ′ and CT relate.\n(Similarly for games.)\nFigure 2 again summarises this discussion in a di-\nagramm.\n4.4\nFrom Models to Capabilities\nAn interesting additional avenue of research has\nbeen explored in recent years (see for example the\npaper by Hewitt and Manning, 2019, and refer-\nences therein), where models trained on datasets\n(representing tasks or games) are explored for how\nthey decomposed their (representational) task. For\nexample, the question might be, as in the cited pa-\nper, whether a particular model trained on a par-\nticular task creates “internally” something akin to\nsyntax trees. Again, why this might be interesting\nis typically not spelled out in these papers, but one\ncan assume that the intended underlying argument\ngoes something like this: “If the theoretical con-\nstruct is to be found, this shows that postulating\nit is empirically validated (and it can be learned\nsimply through exposure to data); if it is not there,\nthen the task can be done without it, to the extent\nthat the model can do it, and it need not be postu-\nlated”.\n5\nConclusions: Making it Explicit\nThe discussion above has mixed normative (how\nit could, or should be) and descriptive (how it is)\naspects. I will close by commenting more directly\non what I think could be improved.\nAs discussed above, the most frequent way of\ncontributing to the project is by improving mod-\nels, for which established methods of evaluation\nexist. Next frequent is providing new or improved\ndatasets.\nHere already we ﬁnd a much larger\nvariety of how the contribution is motivated and\nframed, and often it is taken as self-evident that a\nnew dataset will drive progress. Here, more ex-\nplicitness about the assumed advantages, and how\nthey connect to the goal of modelling language\ncompetence, would be helpful.10\nThis holds even more when introducing new\ntasks or games. As the discussion above hopefully\nhas made plausible, motivating those in them-\nselves, and relating them to existing ones, requires\nmaking claims about language capabilities.\nTo\nmake these in a convincing way, it seems advan-\ntageous to strive for a renewed, stronger connec-\ntion to the sciences that study them: linguistics\nand cognitive psychology. Those ﬁelds provide\nthe theoretical terms and constructs that would al-\nlow us to make the, as discussed above, typically\nimplicit arguments more explicit (and hence con-\ntestable).\nAcknowledgements\nI thank Raquel Fern´andez, Manfred Stede, and Sina Zarrieß\nfor interesting discussions about topics related to this paper,\nwhile reserving the exclusive right to be blamed for any mis-\nunderstandings it might betray.\n10The recent initiative of formulating “Data Sheets” (Ge-\nbru et al., 2018), although developed for different purposes,\nwould be a good step also in this direction.\nReferences\nSam Adams,\nItmar Arel,\nJoscha Bach,\nRobert\nCoop, Rod Furlan, Ben Goertzel, J. Storrs Hall,\nAlexei Samsonovich, Matthias Scheutz, Matthew\nSchlesinger, Stuart C. Shapiro, and John Sowa.\n2012.\nMapping the Landscape of Human-Level\nArtiﬁcial General Intelligence.\nAI Magazine,\n33(1):25–42.\nStefanos Afantenos, Nicholas Asher, Farah Benamara,\nAnais Cadilhac, Cedric Degremont, Pascal De-\nnis, Markus Guhe, Simon Keizer, Alex Lascarides,\nOliver Lemon, Philippe Muller, Soumya Paul, Ver-\nena Rieser, and Laure Vieu. 2012. Developing a cor-\npus of strategic conversation in the settlers of catan.\nIn Proceedings of the 1st Workshop on Games and\nNLP, Kanazawa, Japan.\nPeter Anderson, Qi Wu, Damien Teney, Jake Bruce,\nMark Johnson, Niko S¨underhauf, Ian Reid, Stephen\nGould, and Anton van den Hengel. 2018. Vision-\nand-Language Navigation:\nInterpreting visually-\ngrounded navigation instructions in real environ-\nments. In CVPR 2018.\nJacob Andreas, Marcus Rohrbach, Trevor Darrell, and\nDan Klein. 2016. Neural Module Networks. In Pro-\nceedings of the 2016 Conference on Computer Vi-\nsion and Pattern Recognition (CVPR 2016).\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C. Lawrence Zitnick,\nand Devi Parikh. 2015. Vqa: Visual question an-\nswering. In International Conference on Computer\nVision (ICCV).\nRon Artstein and Massimo Poesio. 2008. Inter-Coder\nAgreement for Computational Linguistics. Compu-\ntational Linguistic, 34(4):555–596.\nMarco Baroni, Armand Joulin, Allan Jabri, Germ`an\nKruszewski, Angeliki Lazaridou, Klemen Simonic,\nand Tomas Mikolov. 2017a. CommAI: Evaluating\nthe ﬁrst steps towards a useful general AI. arXiv,\npages 1–9.\nMarco Baroni, Claes Stranneg˚ard, David L. Dowe,\nKatja Hofmann, Kristinn R. Th´orisson, Jordi Bieger,\nNader Chmait, Fernando Mart´ınez-Plumed, and Jos´e\nH´ernandez-Orallo. 2017b.\nA New AI Evaluation\nCosmos: Ready to Play the Game?\nAI Magazine,\n38(3):66.\nAndrew Bennett and Max Shatkhin. 2018. Mapping\nInstructions to Actions in 3D Environments with Vi-\nsual Goal Prediction. In EMNLP 2018, pages 2667–\n2678.\nSimon Brodeur, Ethan Perez, Ankesh Anand, Flo-\nrian Golemo, Luca Celotti, Florian Strub, Jean\nRouat, Hugo Larochelle, and Aaron Courville.\n2017.\nHoME: a Household Multimodal Environ-\nment. ArXiv.\nDonna Byron, Alexander Koller, Jon Oberlander, Laura\nStoia, and Kristina Striegnitz. 2007. Generating In-\nstructions in Virtual Environments ( GIVE ): A Chal-\nlenge and an Evaluation Testbed for NLG. In Pro-\nceedings of the Workshop on Shared Tasks and Com-\nparative Evaluation in Natural Language Genera-\ntion.\nAngel Chang, Angela Dai, Thomas Funkhouser, Mano-\nlis Savva, and Shuran Song. 2017.\nMatterport3D\n: Learning from RGB-D Data in Indoor Environ-\nments. ArXiv.\nCleo Condoravdi, Richard Crouch, Valeria de Paiva,\nReinhard Stolle, and Daniel G Bobrow. 2003. En-\ntailment, intensionality and text understanding. In\nProc. of the HLT-NAACL 2003 Workshop on Text\nMeaning, pages 38–45.\nMarc-Alexandre Cˆot´e, ´Akos K´ad´ar, Xingdi Yuan, Ben\nKybartas, Tavian Barnes, Emery Fine, James Moore,\nMatthew Hausknecht, Layla El Asri, Mahmoud\nAdada, Wendy Tay, and Adam Trischler. 2018.\nTextWorld: A Learning Environment for Text-based\nGames. ArXiv.\nDanillo da Silva Rocha and Ivandr´e Paraboni. 2016.\nReference production in human-computer interac-\ntion : Issues for Corpus-based Referring Expression\nGeneration. In LREC, pages 2994–2998.\nHubert L. Dreyfus. 1981.\nFrom micro-worlds to\nknowledge: AI at an impasse. In John Haugeland,\neditor, Mind Design. MIT Press.\nTimnit\nGebru,\nJamie\nMorgenstern,\nBriana\nVec-\nchione, Jennifer Wortman Vaughan, Hanna M. Wal-\nlach, Hal Daum´e III, and Kate Crawford. 2018.\nDatasheets for datasets. CoRR, abs/1803.09010.\nDaniel Gordon, Aniruddha Kembhavi, Mohammad\nRastegari, Joseph Redmon, Dieter Fox, and Ali\nFarhadi. 2017. IQA: Visual Question Answering in\nInteractive Environments. ArXiv.\nYash Goyal, Tejas Khot, Douglas Summers-Stay,\nDhruv Batra, and Devi Parikh. 2017. Making the V\nin VQA Matter: Elevating the Role of Image Under-\nstanding in Visual Question Answering. In CVPR\n2017.\nJohn Hewitt and Christopher D Manning. 2019.\nA\nStructural Probe for Finding Syntax in Word Rep-\nresentations. In NAACL-HLT 2019.\nNikolai Ilinykh, Sina Zarrieß, and David Schlangen.\n2018. The Task Matters. Comparing Image Caption-\ning and Task-Based Dialogical Image Description.\nIn Proceedings of 11th International Conference on\nNatural Language Generation (INLG 2018).\nAllan Jabri, Armand Joulin, and Laurens van der\nMaaten. 2016. Revisiting Visual Question Answer-\ning Baselines.\nIn European Conference on Com-\nputer Vision (ECCV).\nSrini Janarthanam and Oliver Lemon. 2011.\nThe\nGRUVE Challenge : Generating Routes under Un-\ncertainty in Virtual Environments.\nIn ENLG ’11\nProceedings of the 13th European Workshop on Nat-\nural Language Generation, pages 208–211.\nJustin Johnson, Bharath Hariharan, Laurens van der\nMaaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross\nGirshick. 2017.\nCLEVR: A Diagnostic Dataset\nfor Compositional Language and Elementary Visual\nReasoning. In CVPR 2017, pages 1988—-1997.\nMatthew Johnson, Katja Hofmann, Tim Hutton, and\nDavid Bignell. 2016. The malmo platform for ar-\ntiﬁcial intelligence experimentation.\nIJCAI Inter-\nnational Joint Conference on Artiﬁcial Intelligence,\n2016-Janua:4246–4247.\nChih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan Al-\nRegib, Zsolt Kira, Richard Socher, and Caiming\nXiong. 2019. Self-Monitoring Navigation Agent via\nAuxiliary Progress Estimation. ArXiv, pages 1–18.\nDavid Marr. 1982. Vision: A Computational Investi-\ngation into the Human Representation and Process-\ning of Visual Information. W.H. Freeman, San Fran-\ncisco, USA.\nMarvin Minsky and Seymour Papert. 1972. Progress\nReport on Artiﬁcial intelligence. Technical report,\nMIT Artiﬁcial Intelligence Laboratory, Cambridge,\nMass., USA.\nDipendra Misra, Andrew Bennett, Valts Blukis, Eyvind\nNiklasson, Max Shatkhin, and Yoav Artzi. 2018.\nMapping Instructions to Actions in 3D Environ-\nments with Visual Goal Prediction. ArXiv.\nKarl Popper. 1934.\nLogik der Forschung.\nMohr\nSiebeck.\nSebastian Ruder. 2019. Neural Transfer Learning for\nNatural Language Processing.\nPh.D. thesis, Na-\ntional University of Ireland, Galway.\nManolis Savva, Angel X. Chang, Alexey Dosovitskiy,\nThomas Funkhouser, and Vladlen Koltun. 2017.\nMINOS: Multimodal Indoor Simulator for Naviga-\ntion in Complex Environments. ArXiv, pages 1–14.\nManolis\nSavva,\nAbhishek\nKadian,\nOleksandr\nMaksymets,\nYili\nZhao,\nErik\nWijmans,\nBha-\nvana Jain, Julian Straub, Jia Liu, Vladlen Koltun,\nJitendra Malik, Devi Parikh, and Dhruv Batra. 2019.\nHabitat: A Platform for Embodied AI Research.\nArXiv.\nDavid Silver, Julian Schrittwieser, Karen Simonyan,\nIoannis Antonoglou, Aja Huang, Arthur Guez,\nThomas Hubert, Lucas Baker, Matthew Lai, Adrian\nBolton, Yutian Chen, Timothy Lillicrap, Fan Hui,\nLaurent Sifre, George Van Den Driessche, Thore\nGraepel, and Demis Hassabis. 2017. Mastering the\ngame of Go without human knowledge.\nNature,\n550(7676):354–359.\nAlane Suhr, Mike Lewis, James Yeh, and Yoav Artzi.\n2017.\nA Corpus of Natural Language for Visual\nReasoning. In Proceedings of the 2017 meeting of\nthe Association for Computational Linguistics (ACL\n2017).\nAlane Suhr, Stephanie Zhou, Iris Zhang, Huajun Bai,\nand Yoav Artzi. 2018.\nA Corpus for Reasoning\nAbout Natural Language Grounded in Photographs.\nIn Proceedings of NIPS 2018, Montreal, Canada.\nBernard Suits. 1978. The Grasshopper: Games, Life,\nand Utopia.\nThe University of Toronto Press,\nToronto, Canada.\nRichard S. Sutton and Andrew G. Barto. 1998. Rein-\nforcement Learning. MIT Press, Cambridge, USA.\nJack Urbanek, Angela Fan, Siddharth Karamcheti,\nSaachi Jain, Samuel Humeau, Emily Dinan, Tim\nRockt¨aschel, Douwe Kiela, Arthur Szlam, and Ja-\nson Weston. 2019. Learning to Speak and Act in a\nFantasy Text Adventure Game. ArXiv.\nAdina Williams, Nikita Nangia, and Samuel R. Bow-\nman. 2018. A Broad-Coverage Challenge Corpus\nfor Sentence Understanding through Inference. In\nProceedings of the Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics (NAACL).\nLudwig Wittgenstein. 1953/84.\nTractatus Logicus\nPhilosophicus und Philosophische Untersuchungen,\nvolume 1 of Werkausgabe. Suhrkamp, Frankfurt am\nMain.\nFei Xia, Amir Zamir, Zhi-Yang He, Alexander Sax,\nJitendra Malik, and Silvio Savarese. 2018. Gibson\nEnv: Real-World Perception for Embodied Agents.\nIn CVPR 2018.\nTakashi Yamauchi, Mikio Nakano, and Kotaro Fu-\nnakoshi. 2013. A Robotic Agent in a Virtual En-\nvironment that Performs Situated Incremental Un-\nderstanding of Navigational Utterances. In SIGdial\n2013, August, pages 369–371.\nClaudia Yan, Dipendra Misra, Andrew Bennett, Aaron\nWalsman, Yonatan Bisk, and Yoav Artzi. 2018.\nCHALET : Cornell House Agent Learning Environ-\nment. ArXiv.\nDani Yogatama,\nCyprien de Masson D’Autume,\nJerome Connor, Tomas Kocisky, Mike Chrzanowski,\nLingpeng Kong, Angeliki Lazaridou, Wang Ling,\nLei Yu, Chris Dyer, and Phil Blunsom. 2019. Learn-\ning and Evaluating General Linguistic Intelligence.\nArXiv, pages 1–14.\nA\nFormalising the Notions\nA.1\nTasks\nDeﬁnition 1 A\nLanguage\nTask\nis\na\ntuple\n(S, A, L, DT ), where:\n• S is a (possibly inﬁnite) set of states,\n• A is a (possibly inﬁnite) set of actions,\n• with either the states in S or the actions in\nA (or both) having as part natural language\nexpressions, and\n• L : S →A is a function that maps a state\ns ∈S to an action a ∈A, where\n• the mapping L conforms to task description\nDT .\nA.2\nWorlds\nDeﬁnition 2 A Micro-World or Environment is a\ntuple (S, A, E, R, DW ), where function E : S ×\nA →S×R maps an action a, taken in state s, to a\nstate s′ and a reward r, and the mapping conforms\nto the world description DW .\nA.3\nGames\nWe approach the deﬁnition of a language game via\nthe more general notion of interaction game:11\nDeﬁnition 3 An Interaction Game is a tuple\n(P, A, o, T, E, DG), where:\n• P = {p1, . . . , pn, N} is the set of n regular\nplayers p, together with one additional player\nN (for Nature).\nN has a special status in that it does not\nhave a strategic interest in the outcome of the\ngame.\n• A = {A1, . . . , An, AN} is the set of action\nspaces, with one space per player.\nAction\ntypes\ncan\nbe\ncomplex:\ne.g.,\n(nav, s),\nfor “navigation action,\nsouth”,\nor (utt, “I don’t know”) for “utterance of\nI don’t know”. If deﬁned in the right way\n(for example by a recursive grammar), the\nset of action types can be inﬁnite. Players\nchose actions from their space of available\nactions; the resulting action tokens aj are\nassociated with their originator through a\n11Pace Wittgenstein (1953/84), we get to deﬁne what a\ngame is. Or we could go with Suits (1978), who is happy\nto deﬁne games as rule-guided activities of voluntary attempt\nto overcome unnecessary obstacles. His concepts of prelu-\nsory goal, which can be stated independently of the game\n(e.g., in football (soccer), “make the ball be in the opponent\nteam’s goal”); constitutive rules, which make reaching that\ngoal more difﬁcult than necessary (e.g., by disallowing to\njust grab the ball and carry it to the goal); and lusory atti-\ntude, which is to accept the complications posed by the con-\nstitutive rules, can inform the design of games that work via\ncrowdsourcing.\nfunction a : Ai →P, and with a position in\nthe sequence of actions that have been per-\nformed since the beginning of the interaction\nthrough a function t : Ai →N.\n• o : Pi × Ai →P(P) (for Pi ∈P, Ai ∈\nA) is the observability function that speciﬁes\nwhich types of actions by which player can be\nobserved by which subset of the players.\nIn normal cases, one would assume that play-\ners can observe their own actions, and that\nNature observes all actions; but this allows\nfor the speciﬁcation of deviant cases.\n• T : ∅∪(P × A) →P(P) is the turn taking\nrule that speciﬁes who can act next, depend-\ning on who did what last. It also speciﬁes\nwho can start the game.\nIn a free initiative setting, any player can act\nat any time; in a strict turn based setting,\nthe current player would always be excluded\nfrom the set of next players.\n• E : S →V is the evaluation rule that maps a\nsequence of action tokens ⟨a1, . . . , am⟩into\nan evaluation, where the set of possible eval-\nuations V includes at least one positive one\n(e.g., success) and one negative one (e.g.,\nfailure).\nThe evaluation is made known to the players\nwhen a positive or negative outcome has been\nreached. If it is not, or if it is does not contain\noutcomes denoted as positive or negative, we\ncall the resulting structure an interaction set-\nting, rather than an interaction game.\n• DG ﬁnally is the game description which\nspeciﬁes which, if any, otherwise existing ac-\ntivity the game is meant to approximate.\nThe well-known Gridworld game (see e.g. Sut-\nton and Barto (1998)) for example can be repre-\nsented in these terms as being an interaction game\nwith one regular player (the agent) interacting with\nNature, P = {p1, N}. The agent can only per-\nform navigation actions: A1 = {(nav, n), . . . },\nNature informs on the resulting available naviga-\ntion options, AN = {(inform, (n, w)), . . . }, with\nthe information that Nature relays coming from a\nmicroworld that simulates the grid and the move-\nment on it.\nGridworld does not involve language, and\nhence is not an example of a language interaction\ngame. As an example of a language interaction\nsetting that is not a game, we can deﬁne free chat\ninteraction in our terms as involving two players\nand an inert Nature that does not intervene: P =\n{p1, p2, N}, A1 = A2 = {(utt, α)}, AN = ∅, T\nis a constant function into P (free initiative), all\nactions are observed by all, E is a constant func-\ntion into {undecided}.\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2019-08-28",
  "updated": "2019-08-28"
}