{
  "id": "http://arxiv.org/abs/1908.11494v4",
  "title": "Reinforcement learning with world model",
  "authors": [
    "Jingbin Liu",
    "Xinyang Gu",
    "Shuai Liu"
  ],
  "abstract": "Nowadays, model-free reinforcement learning algorithms have achieved\nremarkable performance on many decision making and control tasks, but high\nsample complexity and low sample efficiency still hinder the wide use of\nmodel-free reinforcement learning algorithms. In this paper, we argue that if\nwe intend to design an intelligent agent that learns fast and transfers well,\nthe agent must be able to reflect key elements of intelligence, like intuition,\nMemory, PredictionandCuriosity. We propose an agent framework that integrates\noff-policy reinforcement learning with world model learning, so as to embody\nthe important features of intelligence in our algorithm design. We adopt the\nstate-of-art model-free reinforcement learning algorithm, Soft Actor-Critic, as\nthe agent intuition, and world model learning through RNN to endow the agent\nwith memory, curiosity, and the ability to predict. We show that these ideas\ncan work collaboratively with each other and our agent (RMC) can give new\nstate-of-art results while maintaining sample efficiency and training\nstability. Moreover, our agent framework can be easily extended from MDP to\nPOMDP problems without performance loss.",
  "text": "REINFORCEMENT LEARNING WITH WORLD MODEL\nA PREPRINT\nJingbin Liu ∗\nCreateAmind\nXinyang Gu*\nCreateAmind\nShuai Liu\nCreateAmind\nOctober 27, 2020\nABSTRACT\nNowadays, model-free reinforcement learning algorithms have achieved remarkable performance on\nmany decision making and control tasks, but high sample complexity and low sample efﬁciency still\nhinders the wide use of model-free reinforcement learning algorithms. In this paper, we argue that if\nwe intend to design an intelligent agent that learns fast and transfers well, the agent must be able to\nreﬂect key elements of intelligence, like Intuition, Memory, Prediction and Curiosity. We propose\nan agent framework that integrates off-policy reinforcement learning with world model learning, so\nas to embody the important features of intelligence in our algorithm design. We adopt the state-of-art\nmodel-free reinforcement learning algorithm, Soft Actor-Critic, as the agent intuition, and world\nmodel learning through RNN to endow the agent with memory, curiosity and the ability to predict.\nWe show that these ideas can work collaboratively with each other and our agent (RMC) can give\nnew state-of-art results while maintaining sample efﬁciency and training stability. Moreover, our\nagent framework can be easily extended from MDP to POMDP problems without performance loss.\nKeywords RL · Model Learning · POMDP · Curiosity\n1\nINTRODUCTION\nModel-free deep reinforcement learning algorithms have shown powerful potential in many challenging domains, from\ntradition board game chess and go to continuous robotic control tasks [Henderson et al., 2018]. The combination of\nreinforcement learning and high-capacity function approximators such as neural networks hold the promise of solving a\nwide range of tasks, like decision making and robotic control, but when it comes to real-world application, most of\nthese learning methods become impractical. Three major challenges are faced in the current reinforcement learning\nrealm [Haarnoja et al., 2018a].\nFirst of all, model-free learning algorithms suffer from low sample efﬁciency and high sample complexity, For tasks\nas simple as gym Box2d games, millions of interaction with the environment are needed. When it comes to complex\ndecision-making problem, the total interaction steps could easily exceed 1010, which is inaccessible and unfriendly to\nmost reinforcement learning community researchers. As for projects like Dota2 and Starcraft II, the expenses become\nformidable.\nSecondly, deep reinforcement learning methods are often extremely brittle with respect to their hyper-parameters. Hyper-\nparameters tuning is time consuming and computing sources consuming and we need carefully tune the parameters\nin order to obtain the optimal result. Besides, agent training are likely to stuck at local optimum and many seeds for\nnetwork weights initialization are indispensable. In some cases, reward signal is not provided or is too sparse to use for\nthe environment we want to solve. We need to handcraft the reward function which brings in more hyper-parameters.\nEven when the reward signal is relatively dense, good results with stable training are not guaranteed. What is worse,\nreward design is not general but environment-speciﬁc [Guo, 2017].\nThirdly, most of the current reinforcement learning algorithms are designed to solve the MDP problems, but problems\nin the real world seldom follow the MDP assumption. They are often partially observation MDPs (POMDPs), which\nmeans that you must make a decision with imperfect information. What makes things worse, It is signiﬁcantly difﬁcult\n∗equal contribution\narXiv:1908.11494v4  [cs.AI]  26 Oct 2020\nif not impossible to construct and infer hidden states from the observations given by the environment. The hidden state\ndepends on the entire interaction history of the agent and the environment dynamics which requires substantial domain\nknowledge of the environment.\nIn order to mitigate these issues, we propose a new approach to deal with complex tasks with deep reinforcement\nlearning. We name our agent RMC (Recurrent Model with Curiosity). In this paper, we discuss the keys elements of\nintelligence and attempt to manifest them in a reinforcement learning framework design. From the knowledge of brain\nmechanisms and psychology, we argue that intuition, curiosity, memory and the ability to predict are the most important\nif not the entire facets of intelligence and the designed agent can beneﬁt from these ideas with respect to continuous\nlearning and transfer learning.\nLike other brain-inspired agent framework, we adopt RNN structure to handle long-term and short-term memory\n[Song et al., 2018][Kapturowski et al., 2018]. We investigate and devise a deep-learning approach to learning the\nrepresentation of states in fully or partially observable environment with minimal prior knowledge of the domain. Then\nwe use the learned hidden states as basis to construct different capabilities of the agent. In particular, we propose\na new family of hybrid models that take advantage of both supervised learning and reinforcement learning. The\nagent framework can be trained in a joint fashion: The supervised learning component can be a recurrent neural\nnetwork (RNN) combined with different heads, providing an effective way of learning the representation of hidden\nstates. The RL component, Soft Actor-Critic (SAC1) [Haarnoja et al., 2018a], learns to act in the environment by\nmaximizing long-term rewards. Furthermore, we design a curiosity bonus from the prediction error, which leads\nto better representation and exploration. Extensive experiments on both MDP and POMDP tasks demonstrate the\neffectiveness and advantages of our proposed agent framework, which outperforms several previous state-of-the-art\nmethods. The key features of our algorithm can be summarized as follow:\n• We employ recurrent neural network (GRU) and R2D2, an efﬁcient data structure for hidden states, to learn the\nrepresentation for RL. Since these recurrent models can aggregate partial information in the past and capture\nlong-term dependencies in the sequence information, their performance is expected to be superior compared to\nthe contextual window-based approaches, which are widely used in reinforcement learning models, such as\nDQN model [Mnih et al., 2015].\n• In order to leverage supervised signals in the training data, our method combines supervised learning and\nreinforcement learning and the proposed hybrid agent framework is jointly trained using stochastic gradient\ndescent (SGD): in each iteration, the representation of the hidden states is ﬁrst learned using supervised signals\n(i.e. next observation and reward) in the training data; then, the value function and policy function are updated\nusing SAC1 which takes the learned hidden states as input. By jointly training the world model and acting\npolicy, we can obtain a better representation to capture the environment dynamics.\n• Finally, in order to encourage our agent to explore more, we use a curiosity bonus as the decaying internal\nreward in addition to the external reward. The reward bonus is design according to the prediction error of the\nlearned world model. We expect our agent, guided by the curiosity reward, to avoid local optimum and reach\nthe goal robustly.\n2\nBACKGROUND\n2.1\nPOMDP\nThe key problem of reinforcement learning is to search for an optimal policy that maximizes the cumulative reward in\nMarkov Decision Process (MDP) or Partially Observable Markov Decision Process (POMDP). MDP can be seen as a\nspecial case of POMDP. POMDP can be deﬁned by the tuple (S, A, P, R, Ω, O) [Hafner et al., 2018], while MDP is\ndeﬁned by (S, A, P, R). The meanings of the notations are as follows:\n• S represents a set of states\n• A represents a set of actions,\n• P : S × A →P(S) stands for the transition function which maps state-action to the probability distributions\nof the next state P(s′|s, a)\n• R : S × A × S →R corresponds to the reward function, with rt = r(st, at, st+1)\n• Ωgives the observations potentially received by the agent\n• O gives the observation probabilities conditioned on action and next state.\n2\nWithin this framework, the agent acts in the environment according to a ∈A. the environment changes to a new\nstate following s′ ∼P(·|s, a). Next, an observation o ∈O and reward r ∼R(s, a) are received by the agent. The\nobservation may only contain partial information about the underlying state s ∈S. Thus we can additionally deﬁne an\ninference model as q(st|o≤t, a<t) = q(st|st−1, at−1, ot)\nAlthough there are many approaches suitable for POMDP process, we focus on using recurrent neural networks\n(RNNs) with backpropagation through time (BPTT) to learn a representation of the state for the POMDP. The Deep Q-\nNetwork agent (DQN)[Mnih et al., 2015] learns to play games from the Atari-57 benchmark by training a convolutional\nnetwork to represent a value function through Q-learning. The agent takes a frame-stacking of 4 consecutive frames\nas observations and the training data is continuously collected in a replay buffer. Other algorithms like the A3C,\nuse an LSTM and the training directly uses the online stream of experience without using a replay buffer. In paper\n[Song et al., 2018], DDPG is combined with an LSTM by storing sequences in the replay and initializing the recurrent\nstate to zero during training.\n2.2\nENTROPY-REGULARIZED REINFORCEMENT LEARNING\nWithin the entropy-regularized framework [Ziebart, 2010], along with environment reward our agent gets a bonus\nreward proportional to the entropy of the policy at each time-step as well. This changes the RL problem to:\nπ∗= arg max\nπ\nE\nτ∼π\n\u0014 ∞\nX\nt=0\nγt\n\u0012\nr(st, at, st+1) + αH (π(·|st))\n\u0013\u0015\n(1)\nThe temperature parameter α makes trade-off between the importance of the entropy bonus against the environment’s\nreward. When α is large, the policy tend to have larger entropy, which means the policy will be more stochastic, on the\ncontrary, if α become smaller, the policy will become more deterministic.\nIn the entropy-regularized framework, V π and Qπ should be modiﬁed to include the entropy term:\nV π(s) = E\nτ∼π\n\u0014 ∞\nX\nt=0\nγt\n\u0012\nr(st, at, st+1) + αH (π(·|st))\n\u0013\f\f\f\f\f s0 = s\n\u0015\n(2)\nQπ(s, a) = E\nτ∼π\n\u0014 ∞\nX\nt=0\nγt\n\u0012\nr(st, at, st+1) + αH (π(·|st))\n\u0013\f\f\f\f\f s0 = s, a0 = a\n\u0015\n(3)\nWith equation(2, 3), we can draw the connection between V π and Qπ. Meanwhile, we have the Bellman equation for\nV π and Qπ:\nV π(s) = E\na∼π[Qπ(s, a)]\n(4)\nQπ(s, a) =\nE\ns′∼P[r(s, a, s′) + αH (π(·|s)) + γV π(s′)]\n(5)\n3\nMETHOD\nInspired by our brain [Kennedy, 2006], we identify four key elements for intelligence, i.e.: Intuition, Memory,\nPrediction and Curiosity. We use these key elements in our agent design. The structure of our agent contains three\nparts: RNN, Model head, Intuition head, each part has a different role. In the real world, the agent can not observe\nthe full state of the environment. In other words, the Markov property rarely holds in real-world environments and\nthe tasks are often featured as incomplete and noisy state information because of partial observability. So we adopt an\nRNN-based architecture to capture the past information or experiences that is useful for future decision making.\nWe use RNN for inference and memory, which means learning a network to model q(st|o≤t, a<t). In practice we only\nprovide ot, at−1 at time-step t instead of o≤t, a<t so it is crucial for RNN to store past information into a hidden state\nst−1. The informative hidden state st is also used as an input of our RL algorithm. It is hard to identify the st as the\n”true” state which follows MDP property, but we can push the st toward the ”true” state by jointly training the agent\nwith intuition head and model head (as discussed in detail in section 3.2).\nThe main function of the model head is to provide a prediction of the future and use the predicted error as a curiosity\nbonus. As we all know the ability to understand and predict the future is fundamental for human. By putting a world\nmodel in our agent, we expect the agent to learn a better representation of hidden state and avoid local optimum.\n3\nAs for the intuition head, its key function is decision making. In this work, we choose SAC1 as our intuition head.\nSAC1 is based on the actor-critic framework and uses entropy regularization to encourage exploration. We combine the\noriginal algorithm with our model head and curiosity bonus to underlie the intuition for more robust decision making.\nFigure 1: Overall architecture of our RMC agent, which is composed of three main parts: RNN, Model head, Intuition\nhead. The RNN cell infers current state from previous state (Memory) and current observation. Model head is\naccountable for Prediction, Curiosity. Intuition head has two sub-heads: value head and policy head\n3.1\nMEMORY\n3.1.1\nRNN and INFERENCE\nThe main function of RNN is providing memory and inference. Due to the POMDP process, we can’t use observation\nat step t directly to make a decision or make a prediction, so we need an inference model to encode observation, action\ninto a hidden state.\np(st|o≤t, a<t) ⇒p(st|st−1, ot, at−1)\n(6)\nSince the model is non-linear, we cannot directly compute the state posteriors that are needed for parameter learning.\nInstead we can optimize the function approximator by backing up losses through the RNN cell. Gradients coming from\nthe policy head are blocked and only gradients originating from the Q-network head and model head are allowed to\nback-propagate into the RNN. We block gradients from the policy head for training stability, as this avoids positive\nfeedback loops between π and qi with shared representations. We will discuss the choice of back-propagating value\nloss and model loss later in section ??. In general, training RNN with model and value function jointly can help with a\nbetter representation of the hidden state.\n3.1.2\nRECURRENT EXPERIENCE REPLAY\nIn order to achieve good performance in a partially observed environment, an RL agent requires a state representation\nthat encodes information about its state-action trajectory in addition to its current observation. The most common way\nto achieve this is using an RNN as part of the agent’s state encoding. So as to train an RNN from replay and enable it to\nlearn meaningful long-term dependencies, the whole state-action trajectories need to be stored in the buffer and used\nfor training the network. Recent work [Kapturowski et al., 2018] compared four strategies of training an RNN from\nreplayed experience:\n• Zero start state: Using a zero start state to initialize the network at the beginning of sampled sequences.\n• Episode replay: Replaying whole episode trajectories.\n• Stored state: Storing the recurrent state in the replay and using it to initialize the network at training time.\n• Burn-in: Allow the network a ‘burn-in period’ by using a portion of the replay sequence only for unrolling\nthe network and producing a start state, and update the network only on the remaining part of the sequence.\nThe zero start state strategy’s appeal lies in its simplicity and it allows independent decorrelated sampling of relatively\nshort sequences, which is important for robust optimization of a neural network. On the other hand, it forces the RNN\nto learn to recover meaningful predictions from an atypical initial recurrent state, which may limit its ability to fully rely\non its recurrent state and learn to exploit long temporal correlations. The second strategy, on the other hand, avoids the\n4\nproblem of ﬁnding a suitable initial state, but creates a number of practical, computational, and algorithmic issues due\nto varying and potentially environment-dependent sequence length, and higher variance of network updates because\nof the highly correlated nature of states in a trajectory when compared to training on randomly sampled batches of\nexperience tuples. [Hausknecht and Stone, 2015] observed little difference between the two strategies for empirical\nagent performance on a set of Atari games, and therefore opted for the simpler zero start state strategy.\nOne possible explanation for this is that in some cases, an RNN tends to converge to a more ‘typical’ state if allowed a\ncertain number of ‘burn-in’ steps, and so recovers from a bad initial recurrent state on a sufﬁciently long sequence.\nWhile the zero start state strategy may sufﬁce in the most fully observable Atari domain, it prevents a recurrent network\nfrom learning actual long-term dependencies in more memory-critical domains.\nTo ﬁx these issues, Stored state with Burn-in is proposed by [Kapturowski et al., 2018]. Empirically, it translates into\nnoticeable performance improvements, as the only difference between the pure zero states and the burn-in strategy lies\nin the fact that the latter unrolls the network over a preﬁx of states on which the network does not receive updates. the\nbeneﬁcial effect of burn-in lies in the fact that it prevents ‘destructive updates’ to the RNN parameters resulting from\nhighly inaccurate initial outputs on the ﬁrst few time steps after a zero state initialization. The stored state strategy, on\nthe other hand, proves to be overall much more effective at mitigating state staleness in terms of the Q-value discrepancy,\nwhich also leads to clearer and more consistent improvements in empirical performance.\nIn all our experiments we use the proposed agent architecture with replay sequences of length ltrain = 15, with an\nburn-in preﬁx of lburn−in = 10.\n3.2\nPREDICTIVE MODEL\nHumans have a powerful mental model. All human knowledge are essentially predictive physical models. Finding and\nLearning these models are crucially important to intelligent agents. When it comes to decision making, our intuition\ncan provide actions without planning, but a world model can guide our intuition. Speciﬁcally, in our agent design the\nrole of the world model is to predict the future. Base on the learned hidden state in the model learning, we develop our\nintuition. Meanwhile, the model prediction loss serves as an curiosity to help explore the world. We hypothesize by\njointly training on model loss and intuition loss, we can get a better representation.\nThe transition dynamics can be written as s′ ∼P(·|s, a). Here we use an function approximator modeled with a\nfeed-forward neural network st+1 = ˆfψ(st, at) to capture the latent dynamic. We predict the change in state st+1 −st,\ngiven a state st and an action at as inputs. This relieves the neural network from memorizing the input state, especially\nwhen the change is small [Kurutach et al., 2018]. Instead of using the L2 one-step prediction loss as model loss, here\nwe use the L1 loss as proposed in [Luo et al., 2018], which also outperforms the L2 loss in our experimental results.\nLmodel(ψ) =\n1\n|D|\nX\n(st,at,st+1∈D)\n∥st+1 −ˆfψ(st, at)∥2\n(7)\n3.3\nCURIOSITY\nThere are many types of curiosities, but the most fundamental one is the curiosity which is inspired when we can’t\npredict correctly [Loewenstein, 1994]. In our agent design, we use the prediction error as curiosity. In addition to the\nextrinsic reward, curiosity-driven intrinsic reward is deﬁned as ri\nt = ˆst −st. Let the intrinsic curiosity reward generated\nby the agent at time t be ri\nt and the extrinsic reward be re\nt . The policy is trained to maximize the sum of the two kinds\nof reward:\nR(st, at, st+1) = β · ri\nt + re\nt\n(8)\nWe use a parameter β to represent the strength of intrinsic reward. In practice, We decay the parameter β to zero as\nthe learning process goes on. With the intrinsic reward, the agent is encouraged to explore more in the earlier stage of\ntraining and to focus on performance in the later stage of training.\n3.4\nINTUITION\nThe intuition is implemented by applying the SAC1 algorithm [Haarnoja et al., 2018b]. Function approximators is used\nfor both the Q-functions Qφ1, Qφ2 and the policy πθ. Instead of running evaluation and improvement to convergence,\nwe alternate between optimizing both networks with stochastic gradient descent.\n5\n3.4.1\nLEARNING Q-FUNCTIONS\nThe Q-functions are learned by applying the Bellman backups. Like in TD3 [Fujimoto et al., 2018] and SAC1\n[Haarnoja et al., 2018b], tow target Q networks are used, the backup Q target is deﬁned by the minimum of the\ntwo targets. As for the target network, we obtain it by Polyak averaging the value network parameters in the course of\ntraining. The loss for Q-function can be written:\nLQ(φi, D) =\nE\ns∼D\n˜a∼πθ\n\" \nQφi(s, a) −\n\u0012\nr + (1 −d)(min\ni=1,2 Qφi(s, ˜a) −α log πθ(˜a|s))\n\u0013 !2#\n(9)\nThe difference between the above soft Q backup and the standard Q backup is the entropy term. The The Q functions\nare updated in a off-policy fashion. A replay buffer is used for storing (s, a, s′, r, d) tuples. Training data are randomly\nsampled from the replay buffer.\n3.4.2\nLEARNING THE POLICY\nIn SAC1 [Haarnoja et al., 2018b], the policy improvement is conducted based on the learned Q functions. The policy\nnetwork is represented as an Gaussian distribution. In order to make the policy differentiable, which is crucial for\nback-propagation, we apply the reparameterization trick. As suggested in SAC1 [Haarnoja et al., 2018b], we use a\nsquashed Gaussian policy, which samples actions according to:\n˜aθ(s, ξ) = tanh (µθ(s) + σθ(s) ⊙ξ) ,\nξ ∼N(0, I)\n(10)\nIt should be noticed that the variance is local, instead of a global variance used in most other algorithms. The variance\nvaries with each individual state, which provides more representation capacity. The reparameterization trick allows us\nto convert the action sampling into Gaussian sampling. The policy loss can be written as:\nLpolicy(θ) =\nE\ns∼D\nξ∼N\n[Qφ1(s, ˜aθ(s, ξ)) −α log πθ(˜aθ(s, ξ)|s)]\n(11)\n3.4.3\nLEARNING THE TEMPERATURE\nThe temperature α is actually the reward scale of the entropy bonus. Thus it is the ratio of the temperature α and reward\nscale that determine the extent of exploration. α can be tuned for a speciﬁc environment. We can also adapt α during\ntraining by targeting a given entropy H for the policy, as suggested in [Haarnoja et al., 2018b]. The learning of the\ntemperature α is achieved by computing the gradient for α with the following objective:\nLtemp(α) = Ea∼πt[−α log πt(at|st) −αH]\n(12)\nThe ﬁnal algorithm is presented in Algorithm 1. The method alternates between collecting experience from the\nenvironment with the current policy and updating the function approximators using the stochastic gradients from batches\nsampled from the replay buffer. The agent is trained according the model loss and intuition loss in a concise way.\n4\nEXPERIMENT\nIn order to test our agent, We designed our experiments to answer the following questions:\n1. Can RMC solve challenging continuous control problems? How does it perform with regard to the ﬁnal\nperformance and sample complexity, comparing with other state-of-the-art methods?\n2. Can RMC handle POMDP environments? How well does it deal with the absence of information and\ngeneralize?\n3. We optimize RNN on model loss and intuition loss jointly. Does the model learning really help us improve the\nagent’s performance?\n4. Can we beneﬁt from the decaying curiosity bonus, which is based on the model prediction loss?\nTo answer (1), we compare the performance of our agent with other state-of-the-art methods in section 4.1. To answer\n(2), we purpose a modiﬁed mujoco environment, the ﬂicker mujoco, which is a POMDP environment. We will discuss\nthe details of the environment and the experiment setting in section 4.2. With regard to (3) and (4), we address the\nablation study on our algorithm in section 4.3, testing the inﬂuence of different training schemes and network designs\non the agent’s ﬁnal performance.\n6\nAlgorithm 1: RMC AGENT\nInput\n:Initial policy parameters θ\nTransition model parameter ψ\nQ-function parameters φ1, φ2\nTemperature α\nEmpty replay buffer D\n1 Set target parameters equal to main parameters θ ←θ, φ1 ←φ1, φ2 ←φ2\n2 while not converge do\n3\nfor each environment step do\n4\nat ∼πθ(at|st)\n// Sample action from the policy\n5\nst+1 ∼p(st+1|st, at)\n// Sample transition from the environment\n6\nD ←∪{(ot, st, at, r(st, at), ot+1, st+1}\n// Store the transition in the replay pool\n7\nend\n8\nfor each gradient step do\n9\n{(s, a, r, s′, d)}B\ni=1 ∼D\n// Randomly sample a batch of transitions\n10\nCompute model loss Lmodel(ψ) from equation(7)\n11\nψ ←ψ −λψ ▽ψ Lmodel(ψ)\n// Update model parameter\n12\nCompute Q-value loss LQ(φ) from equation(9)\n13\nφ ←φ −λφ ▽φ LQ(φ)\n// Update Q-value parameter\n14\nCompute policy loss Lpolicy(θ) from equation(11)\n15\nθ ←θ −λθ ▽θ Lpolicy(θ)\n// Update policy parameter\n16\nCompute temperature loss Ltemp(α) from equation(12)\n17\nα ←α −λα ▽α Ltemp(α)\n// Update temperature parameter\n18\nφi ←ρφi + (1 −ρ)φi, for i ∈{1, 2}\n// Update target network weights\n19\nend\n20 end\n4.1\nMUJOCO\nThe goal of this experimental evaluation is to understand how the sample complexity and stability of our method\ncompares with previous off-policy and on-policy deep reinforcement learning algorithms. We compare our method\nto previous techniques on a range of challenging continuous control tasks from the OpenAI gym benchmark suite.\nAlthough easier tasks can be solved by a wide range of different algorithms, the more complex benchmarks, such as\nthe 21-dimensional Humanoid, are exceptionally difﬁcult to solve with off-policy algorithms. For easier tasks, most\nof reinforcement algorithms can achieve good results by tuning hyper-parameters. While for the hard tasks, already\nnarrow basins of effective hyper-parameters become prohibitively small for hyper-parameters sensitive algorithms. The\nhard tasks can be effective algorithm test beds [Gu et al., 2016].\nWe compare our method with deep deterministic policy gradient (DDPG) [Lillicrap et al., 2015], an algorithm\nthat is regarded as an expansion to continuous action space of DQN algorithm; policy optimization (PPO)\n[Schulman et al., 2017], a stable and effective on-policy policy gradient algorithm; and soft actor-critic(SAC1)\n[Haarnoja et al., 2018b], a recent off-policy reinforcement learning algorithm with entropy regularization. We addition-\nally compare our method with twin delayed deep deterministic policy gradient algorithm (TD3) [Fujimoto et al., 2018].\nWe conduct the robotic locomotion experiments by using the MuJoCo simulator [Todorov et al., 2012]. The states\nof the robots are their generalized positions and velocities, and the controls are joint torques. High dimensionality\nand non-smooth dynamics due to contacts make these tasks very challenging. To allow for a reproducible and fair\ncomparison, we evaluate all the algorithm with a similar network structure. For the off-policy algorithm, we use a\ntwo-layer feed-forward neural network of 400 and 300 hidden nodes respectively, with rectiﬁed linear units (ReLU)\nbetween each layer for both the actor and critic. For on-policy algorithm, we use the parameters which is shown in\n7\n(a) Ant-v2(MDP)\n(b) HalfCheetah-v2(MDP)\n(c) Humanoid-v2(MDP)\n(d) Walker2d-v2(MDP)\n(e) HumanoidStandup-v2(MDP)\n(f) Flicker MuJoCo(POMDP)\nFigure 2: (a) to (e) are the training curves on continuous control benchmarks. RMC agent performs well consistently\nacross all tasks and outperforms both on-policy and off-policy methods in the challenging MuJoCo tasks. (f) shows the\nnoramlized score for training on Flicker MuJoCo with p = 0.5.\nprevious work [Henderson et al., 2018] as a comparison of our agent. Both network parameters are updated using\nAdam[Kingma and Ba, 2014] with a learning rate of 10−4. No modiﬁcations are made to the environment and reward.\nIn Fig. 2, we compare our method with different other methods in ﬁve individual runs initialized with different random\nseeds. RMC signiﬁcantly outperforms the baselines, indicating substantially better efﬁciency and stability. It shows that\nthe model training and the intrinsic curiosity return can actually promote the policy learning of the agent.\n4.2\nPOMDP MUJOCO\nIn order to test our agent’s ability to deal with imperfect information cases, we establish a new environment called\nFlickering MuJoCo, which is achieved by changing classic MuJoCo benchmark to a POMDP environment. At each\ntime-step, the screen is either fully revealed or fully obscured with a probability of p = 0.5. Obscured frames convert\nMuJoCo into a POMDP MuJoCo in a probabilistical manner.\nMany previous works deal with POMDP by using a long history of observations, while in our RMC agent we use\na recurrent network trained with a single observation at each timestep. As shown in Fig. 2f, our agent outperforms\nstandard SAC1 combine with frame stack. Our agent performs well at this task even when given only one input frame\nper time-step. RMC can successfully integrates noisy information through time to detect critical events.\nIt is interesting to ask a question: Can or to what extent can an agent which is trained on a standard MDP generalize\nto a POMDP at evaluation time? To answer this question, we evaluate the RMC agent and SAC1 trained on standard\nMuJoCo over the ﬂickering MuJoCo. Fig. 3a shows our agent preserves more of its previous performance than SAC1\nacross all levels of ﬂickering. We conclude that RMC is more robust against loss of information.\n4.3\nABLATION STUDY\n4.3.1\nTHE IMPACT OF DIFFERENT TRAINING SCHEMES\nThere are three major heads for RMC: the model head, the value head and the policy head. In the learning procedure,\nit is important to determine which of these head’s loss can be back-propagated into RNN. In order to address this\nproblem, we study the impact of different training schemes. We summarize six valid update schemes in Table 1.\nTheoretically, the model loss alone is sufﬁcient for the representation learning, we can block the policy and value loss\n8\n(a) from MDP to POMDP\n(b) 6 training schemes\n(c) curiosity\nFigure 3: (a) Agents are ﬁrst trained on MDP environments and then evaluated on POMDP environments. RMC’s\nperformance loss over obscured observation is less than SAC1. Each data point stands for the normalized score over\ndifferent ﬂickering probability, evaluated on environments shown in Fig. 2. (b) We test 6 different training schemes on\nHalfCheetah-v2. It shows that updating RNN on model loss, value loss and not on policy loss gets the best result. (c)\nThe impact of different scales of curiosity reward.\nfrom back-propagating into RNN. In practice, the collection of training data is controlled by the intuition, the value or\npolicy losses may be beneﬁcial for stabilizing the training process.\nFig. 3b shows the learning performances of different update schemes. For schemes 4, 5 and 6, the policy loss are\nback-propagated into RNN. We can see from the results that the policy loss can disrupt the learning process greatly.\nTheir policies become nearly random and fails to exploit the reward signal, resulting in substantial degradation of\nperformance. One possible explanation is that the policy loss is too strong to be used in the model learning. For schemes\n2 and 3, we can see that with the value loss alone or the model loss alone, the performance is enhanced. It is straight\nforward to infer that scheme 1 can have the best performance with value loss and model loss back-propagating into\nRNN jointly. For scheme 1, gradient calculated from the policy loss is blocked and only gradients originated from the\nvalue head and model head are allowed to back-propagate into the RNN. The model head makes sure the representation\nlearning of a state is good enough to predict its next state. At the same time, the value head can inject the information\nthat is needed to learn a good policy to the same representation. Just like our brain has two different kinds of thinking\npattern, the so-called intuition thinking and reasoning thinking, our agent can take advantage of joint optimization of\nintuition behavior and predictive model, by learning a representation of state that supports for a intuitive and predictive\nagent.\nIn a word, it is important to assign different tasks to different heads reasonably. The value and model bear the burden\nof representation learning, while the policy head is only responsible for learning the intuition. This combination can\nproduce best performance in our experimental setting.\n4.3.2\nTHE IMPACT OF CURIOSITY\nTable 1: All valid update schemes\nscheme\nModel\nValue\nPolicy\n1\nTrue\nTrue\nFalse\n2\nFalse\nTrue\nFalse\n3\nTrue\nFalse\nFalse\n4\nTrue\nTrue\nTrue\n5\nTrue\nFalse\nTrue\n6\nFalse\nTrue\nTrue\nAs discussed in section 3, the model head can provide a prediction\nof the future step and in the meanwhile provide a curiosity bonus for\npolicy learning. We’ve already analyzed the inﬂuence of different\nmodel update schemes and shown that joint training can improve\nperformance. In this section, we discuss the impact of curiosity on\nthe agent training.\nWe choose update scheme 1 to conduct the experiments on curiosity.\nAs illustrated in Fig 3c, when we set β to zero, the model can’t\nexplore well so both the sample efﬁciency and ﬁnal score is below\nthe benchmark. If we use a huge scale of β, the intrinsic reward\ndominates the policy signal and prevents the policy from utilizing\nexternal reward for learning.\nTheoretically, as the learning process goes on, the model loss will\nbecome smaller and smaller until zero so that β won’t have much\nimpact in the end and all the different β scales will lead to a similar\nﬁnal performance. But in practice, due to insufﬁcient training and stochasticity of the environment, the model loss can\nnever become zero. In order to achieve fast and stable learning, we decay the β scale from large to small, encouraging\nour agent to explore more at the beginning and exploit more in the end.\n9\n5\nCONCLUSION\nIn this paper, we argue that it is important for an intelligent agent design to embody the key elements reﬂected in human\nintelligence like intuition, memory, prediction and curiosity. We show that these ideas can work collaboratively with\neach other and our agent (RMC) can give new state-of-art results while maintaining sample efﬁciency and training\nstability. The representation learning of the environment and its dynamic is at the core of training an intelligent agent.\nJoint training of model-free intuition and model-based prediction can improve the representation learning of the task,\nwhich underlies and promotes the policy learning. Empirically results show that our agent framework can be easily\nextended from MDP to POMDP problems without performance loss, which sheds light on real-world applications.\nFurther study includes incorporating stochastic transition function like planet [Hafner et al., 2018] and world model\n[Ha and Schmidhuber, 2018] into our framework and theoretic analysis of the algorithms like our method that exploits\nmodel-free and model-based algorithms at the same time.\nReferences\n[Fujimoto et al., 2018] Fujimoto, S., van Hoof, H., and Meger, D. (2018). Addressing function approximation error in\nactor-critic methods. arXiv preprint arXiv:1802.09477.\n[Gu et al., 2016] Gu, S., Lillicrap, T. P., Ghahramani, Z., Turner, R. E., and Levine, S. (2016). Q-prop: Sample-efﬁcient\npolicy gradient with an off-policy critic. ArXiv, abs/1611.02247.\n[Guo, 2017] Guo, X. (2017). Deep Learning and Reward Design for Reinforcement Learning. PhD thesis.\n[Ha and Schmidhuber, 2018] Ha, D. and Schmidhuber, J. (2018). World models. arXiv preprint arXiv:1803.10122.\n[Haarnoja et al., 2018a] Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018a). Soft actor-critic: Off-policy\nmaximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290.\n[Haarnoja et al., 2018b] Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar, V., Zhu, H., Gupta,\nA., Abbeel, P., et al. (2018b). Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905.\n[Hafner et al., 2018] Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and Davidson, J. (2018).\nLearning latent dynamics for planning from pixels. arXiv preprint arXiv:1811.04551.\n[Hausknecht and Stone, 2015] Hausknecht, M. and Stone, P. (2015). Deep recurrent q-learning for partially observable\nmdps. In 2015 AAAI Fall Symposium Series.\n[Henderson et al., 2018] Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., and Meger, D. (2018). Deep\nreinforcement learning that matters. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence.\n[Kapturowski et al., 2018] Kapturowski, S., Ostrovski, G., Quan, J., Munos, R., and Dabney, W. (2018). Recurrent\nexperience replay in distributed reinforcement learning.\n[Kennedy, 2006] Kennedy, J. (2006). Swarm intelligence. In Handbook of nature-inspired and innovative computing,\npages 187–219. Springer.\n[Kingma and Ba, 2014] Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980.\n[Kurutach et al., 2018] Kurutach, T., Clavera, I., Duan, Y., Tamar, A., and Abbeel, P. (2018). Model-ensemble\ntrust-region policy optimization. arXiv preprint arXiv:1802.10592.\n[Lillicrap et al., 2015] Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D.\n(2015). Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971.\n[Loewenstein, 1994] Loewenstein, G. (1994). The psychology of curiosity: A review and reinterpretation. Psychologi-\ncal bulletin, 116(1):75.\n[Luo et al., 2018] Luo, Y., Xu, H., Li, Y., Tian, Y., Darrell, T., and Ma, T. (2018). Algorithmic framework for\nmodel-based deep reinforcement learning with theoretical guarantees. arXiv preprint arXiv:1807.03858.\n[Mnih et al., 2015] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A.,\nRiedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. (2015). Human-level control through deep reinforcement\nlearning. Nature, 518(7540):529.\n[Schulman et al., 2017] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347.\n[Song et al., 2018] Song, D. R., Yang, C., McGreavy, C., and Li, Z. (2018). Recurrent deterministic policy gradient\nmethod for bipedal locomotion on rough terrain challenge. In 2018 15th International Conference on Control,\nAutomation, Robotics and Vision (ICARCV), pages 311–318. IEEE.\n10\n[Todorov et al., 2012] Todorov, E., Erez, T., and Tassa, Y. (2012). Mujoco: A physics engine for model-based control.\nIn 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033. IEEE.\n[Ziebart, 2010] Ziebart, B. D. (2010). Modeling purposeful adaptive behavior with the principle of maximum causal\nentropy. PhD thesis, ﬁgshare.\n11\n",
  "categories": [
    "cs.AI"
  ],
  "published": "2019-08-30",
  "updated": "2020-10-26"
}