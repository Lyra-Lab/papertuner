{
  "id": "http://arxiv.org/abs/1909.01500v2",
  "title": "rlpyt: A Research Code Base for Deep Reinforcement Learning in PyTorch",
  "authors": [
    "Adam Stooke",
    "Pieter Abbeel"
  ],
  "abstract": "Since the recent advent of deep reinforcement learning for game play and\nsimulated robotic control, a multitude of new algorithms have flourished. Most\nare model-free algorithms which can be categorized into three families: deep\nQ-learning, policy gradients, and Q-value policy gradients. These have\ndeveloped along separate lines of research, such that few, if any, code bases\nincorporate all three kinds. Yet these algorithms share a great depth of common\ndeep reinforcement learning machinery. We are pleased to share rlpyt, which\nimplements all three algorithm families on top of a shared, optimized\ninfrastructure, in a single repository. It contains modular implementations of\nmany common deep RL algorithms in Python using PyTorch, a leading deep learning\nlibrary. rlpyt is designed as a high-throughput code base for small- to\nmedium-scale research in deep RL. This white paper summarizes its features,\nalgorithms implemented, and relation to prior work, and concludes with detailed\nimplementation and usage notes. rlpyt is available at\nhttps://github.com/astooke/rlpyt.",
  "text": "rlpyt: A Research Code Base for Deep Reinforcement\nLearning in PyTorch\nAdam Stooke\nUniversity of California, Berkeley\nadam.stooke@berkeley.edu\nPieter Abbeel\nUniversity of California, Berkeley\npabbeel@cs.berkeley.edu\nAbstract\nSince the recent advent of deep reinforcement learning for game play [1] and\nsimulated robotic control (e.g. [2]), a multitude of new algorithms have ﬂourished.\nMost are model-free algorithms which can be categorized into three families:\ndeep Q-learning, policy gradients, and Q-value policy gradients. These have\ndeveloped along separate lines of research, such that few, if any, code bases\nincorporate all three kinds. Yet these algorithms share a great depth of common\ndeep reinforcement learning machinery. We are pleased to share rlpyt, which\nimplements all three algorithm families on top of a shared, optimized infrastructure,\nin a single repository. It contains modular implementations of many common deep\nRL algorithms in Python using PyTorch [3], a leading deep learning library. rlpyt\nis designed as a high-throughput code base for small- to medium-scale research\nin deep RL. This white paper summarizes its features, algorithms implemented,\nand relation to prior work, and concludes with detailed implementation and usage\nnotes. rlpyt is available at https://github.com/astooke/rlpyt.\n1\nIntroduction\nSince the advent of deep reinforcement learning for game play in 2013 [1] and simulated robotic\ncontrol shortly after (e.g. [2]), a multitude of new algorithms have ﬂourished. Most are model-\nfree algorithms which can be categorized into three families: deep Q-learning, policy gradients,\nand Q-value policy gradients. These have developed along separate lines of research, such that\nfew, if any, code bases incorporate all three kinds. In fact, many of the original implementations\nremain unreleased. As a result, practitioners often must develop from different starting points\nand potentially learn a new code base for each algorithm of interest or baseline comparison. RL\nresearchers often reimplement algorithms–perhaps a valuable individual exercise, but one that\nincurs redundant effort across the community, or worse, one that presents a barrier to entry. Yet\nthese algorithms share a great depth of common deep reinforcement learning machinery. We are\npleased to share rlpyt, which implements all three algorithm families built on a shared, optimized\ninfrastructure, in a single repository. rlpyt contains modular implementations of many common\ndeep RL algorithms in Python using PyTorch [3], a leading deep learning library. Among numerous\nexisting implementations, rlpyt is a more comprehensive open-source resource for researchers. rlpyt\nis available at https://github.com/astooke/rlpyt.\nrlpyt is designed as a high-throughput code base for small- to medium-scale research in deep RL\n(large-scale being DeepMind AlphaStar [4] or OpenAI Five [5], for example). This white paper\nsummarizes its features, algorithms implemented, and relation to prior work. A small selection of\nlearning curves are provided to verify learning performance for some standard RL environments\nin discrete and continuous control. Notably, rlpyt reproduces record-setting results in the Atari\ndomain from “Recurrent Experience Replay in Distributed Reinforcement Learning” (R2D2) [6].\nThis benchmark requires on the order of 30 billion frames of game play and 1 million network\nupdates, which rlpyt achieves in reasonable time without the use of distributed compute infrastructure.\narXiv:1909.01500v2  [cs.LG]  24 Sep 2019\nCompatibility with the OpenAI Gym interface provides access to many existing learning environments\nand allows new ones to be freely customized. This paper also introduces the \"namedarraytuple\", a\nnew data structure for handling collections of arrays, which may be of outside interest. Finally, more\ndetailed implementation and usage notes are provided.\n1.1\nKey Features and Algorithms\nKey capabilities and features include:\n• Run experiments in serial mode (helpful for debugging, sufﬁcient for some experiments).\n• Run experiments parallelized, with options for parallel sampling and/or multi-GPU opti-\nmization.\n• Sampling and optimization synchronous or asynchronous (via replay buffer).\n• Use CPU or GPU for training and/or batched action selection during environment sampling.\n• Full support for recurrent agents.\n• Online or ofﬂine evaluation and logging of agent diagnostics during training.\n• Includes launching utilities for stacking / queueing sets of experiments on local computer.\n• Modularity for easy modiﬁcation and re-use of existing components.\n• Compatible with OpenAI Gym [7] environment interface.1\nImplemented algorithms include the following (check the repository for possible additions):\n• Policy Gradient: A2C [8], PPO [9].\n• Deep Q-Learning: DQN [1] + variants: Double [10], Dueling [11], Categorical [12],\nRainbow [13] (minus Noisy Nets), Recurrent (R2D2-like) [6], including vector-valued\nepsilon-greedy (Ape-X-like) [14] (coming soon: Implicit Quantile DQN [15]).\n• Q-Function Policy Gradient: DDPG [16], TD3 [17], SAC [18, 19], (coming soon: Distri-\nbutional DDPG [20]).\nReplay buffers support both the DQN and Q-function policy gradient algorithms and include the\nfollowing options: n-step returns; sequence replay (for recurrence); periodic storage of recurrent state\n(to save memory); prioritized replay (sum tree) [21]; frame-based buffer, to save memory e.g. by\nstoring only unique Atari frames.\n2\nParallel Computing Infrastructure for Faster Experimentation\nThe two phases of model-free RL–sampling environment interactions and training the agent–can be\nparallelized differently. rlpyt addresses both, as described here. In all arrangements, system shared\nmemory underlies inter-process communication of training data and model parameters, minimizing\ndata transfer time and memory footprint.\n2.1\nSampling\nFor sampling, rlpyt offers the following conﬁgurations, also depicted in Figure 1.\nSerial. Sampling occurs in the master process and can run one or more environment instances. The\nbuilt-in agent uses the same model for sampling and for optimization, so if optimizing on the GPU,\naction-selection during sampling also uses the GPU, batched over all environments. (If running\nmany time steps per batch with GPU optimization, it may be faster to use a separate CPU model for\naction-selection.)\nParallel-CPU. The sampler launches worker processes to run environments and perform action\nselection. If optimizing on the GPU, model parameters are copied to shared memory for CPU action\nselection in workers. Synchronization across workers only occurs per sampling batch.\n1See implementation details for required modiﬁcation.\n2\nParallel-GPU. The sampler launches worker processes to run environments, and observations are\ncommunicated back to the master process for action selection, which will use the GPU if optimizing\non GPU. All the environments’ observations are batched together for one call to the agent. Step-wise\ncommunication happens via another shared memory buffer, and light-weight semaphores enforce\nsynchronization across workers at every simulation batch-step.\nAlternating-GPU. Like parallel-GPU sampling but with two groups of workers; one group steps\nenvironments while the other group awaits action-selection. May provide speedups when the action-\nselection time is similar to but shorter than the batch environment simulation time.\nFigure 1: Environment interaction sampling schemes. (left) Serial: agent and environments execute\nwithin one Python process. (center) Parallel-CPU: agent and environments execute on CPU in parallel\nworker processes. (right) Parallel-GPU: environments execute on CPU in parallel workers processes,\nagent executes in central process, enabling batched action-selection.\n2.2\nOptimization\nSynchronous multi-GPU optimization is implemented using PyTorch’s DistributedDataParallel\nto wrap the model. A separate python process drives each GPU. As provided by PyTorch, NCCL\nis used to all-reduce every gradient, which can occur in chunks concurrently with backpropaga-\ntion, for better scaling on large models. The same applies for multi-CPU optimization, using\nDistributedDataParallelCPU and the “gloo” backend (may be faster than MKL threading for\nmultiple CPU cores). The arrangement is shown in Figure 2. The entire sampling-training stack is\nreplicated in each process and no training data is shared among them. Any of the serial or parallel\nsamplers can be used.\nFigure 2: Synchronous multi-process reinforcement learning. Each python process runs a copy of the\nfull sampler-algorithm stack, with synchronization enforced implicitly during backpropagation in\nPyTorch’s DistribuedDataParallel class. Both GPU (NCCL backend) and CPU (gloo backend)\nmodes are supported.\n3\n2.3\nAsynchronous Sampling-Optimization\nIn the conﬁgurations depicted so far, the sampler and optimizer operate sequentially in the same\nPython process. In some cases, however, running optimization and sampling asynchronously achieves\nbetter hardware utilization, by allowing both to run continuously. In asynchronous mode, separate\nPython processes run the training and sampling, tied together by a replay buffer built on shared\nmemory. Sampling runs uninterrupted by the use of a double buffer for data batches, which yet\nanother Python process copies into the main buffer, under a read-write lock. This is shown in Figure\n3. The optimizer and sampler may be parallelized independently, perhaps each using a different\nnumber of GPUs, to achieve best overall utilization and speed.\nFigure 3: Asynchronous sampling/optimization mode. Separate python processes run optimization\nand sampling via a shared-memory replay buffer under read-write lock. Memory copier processes\nwrite from the sampler batch buffer (a double buffer) to the replay buffer, freeing the sampler to\nproceed immediately from batch to batch of collection.\nSome level of control between the processes is maintained. A desired maximum replay ratio can be\nspeciﬁed (rate of consumption divided by rate of generation of training data), and the optimizer will\nbe throttled not to exceed this value. The sampler batch size (time-steps) determines rate of actor\nmodel update, if new parameters are available from the optimizer. All actors use the same parameters.\n2.4\nWhich Conﬁguration is Best?\nWhen creating or modifying agents, models, algorithms, and environments, serial mode will be the\neasiest for debugging. Once that runs smoothly, it is straightforward to explore the more sophisticated\ninfrastructures for parallel sampling, multi-GPU optimization, and asynchronous sampling, since they\nare built on largely the same interfaces. Of course, deviations from the standard RL work-ﬂow (i.e.\nthe runner) may require more care to parallelize–again it is recommended to start with the serial case.\nThe optimal conﬁguration may depend on the problem, available compute hardware, and the number\nof experiments to run. Currently, rlpyt implements only single-node parallelism, but its components\ncould form building blocks for a distributed framework.\n3\nLearning Performance\nThis section presents learning curves which verify the performance of the implementations against\npublished values. A subset of standard Atari games [22] and Mujoco [23] continuous control\nenvironments are shown. This is neither a comprehensive benchmark nor guide to scaling, but merely\nan exercise of each algorithm and infrastructure component. For Atari scaling guidelines, see e.g.\n[24], for Mujoco, [20] is a likely starting point.\n4\n3.1\nMujoco: Continuous Control from State\nHere we present reinforcement learning algorithms applied to continuous control from state on\na selection of Mujoco2 tasks in OpenAI Gym. For each algorithm, we used the same published\nhyperparameters across all environments and ran serial implementations.3\nFigure 4: Continuous control in Mujoco by RL algorithms–DDPG (settings from [17]), TD3, SAC,\nand PPO; 4 random seeds each.\n3.2\nAtari: Discrete Control from Vision\nHere we include learning curves for a small selection of Atari games learned by vision using both\npolicy gradient (Figure 5) and DQN algorithms (Figure 6).\nFigure 5: Policy gradient algorithms–A2C (feed-forward), A2C-LSTM (1-frame observation), A2C-\n2GPU (synchronous mode), PPO; 2 random seeds each.\nFigure 6: DQN plus variants–Categorical, Prioritized-Dueling-Double, Rainbow minus Noisy Nets,\nand asynchronous mode–all trained with batch size 128; 2 random seeds.\n2mujoco200.\n3A previous version of this paper showed lower scores for SAC and TD3, which have improved here by\nbootstrapping the value function when the trajectory ends due to time limit, as in the original SAC implementation.\nScores for SAC further improved by switching to the newer version, with entropy tuning and no state-value\nfunction.\n5\nR2D1 We highlight rlpyt’s reproduction of the state of the art performance of R2D2 [6], which\nwas previously only feasible using distributed computing. This benchmark includes a recurrent\nagent trained from a replay buffer for on the order of 10 billion samples (40 billion frames). R2D1\n(non-distributed R2D2) exercises several of rlpyt’s more advanced infrastructure components to\nachieve this, namely multi-GPU asynchronous sampling mode with the alternating sampler. In Figure\n7 we reproduce several learning curves which surpass any previous algorithm. Some slight differences\nin performance against published values most likely resulted from a difference in the prioritization\nfor new samples, which affected some games more than others,4 and a slightly lower replay ratio.5\nGiven the low replay ratio, initial priorities are very important in some games.\nFigure 7: Reproduction of R2D2 learning curves in rlpyt, a single seed each.\nThe original, distributed implementation of R2D2 quoted about 66,000 steps per second (SPS) using\n256 CPUs for sampling and 1 GPU for training. rlpyt achieves over 16,000 SPS when using only 24\nCPUs6 and 3 Titan-Xp GPUs in a single workstation (one GPU for training, two for action-serving in\nthe alternating sampler). This may be enough to enable experimentation without access to distributed\ninfrastructure. One possibility for future research is to increase the replay ratio (here set to 1) for\nfaster learning using multi-GPU optimization. Figure 8 shows the same learning curve over three\ndifferent measures: environment steps (i.e. 1 step = 4 frames), model updates, and time. This run\nreached 8 billion steps and 1 million updates in less than 138 hours.\nFigure 8: The same learning curve for the game Amidar over three horizontal axes: environment\nsteps, model updates, and wall-clock time, for rlpyt’s R2D1 implementation run in asynchronous\nsampling mode using 24 CPU cores and 3 GPUs.\n4Most curves used 1-step TD errors for prioritizing new samples and had unintentionally swapped the two\nreplay priority coefﬁcients. Furthermore, since collection ran in 40 time-step batches but training used 80-step\nsequences, we used only half the training segment to compute new priorities. Gravitar was especially sensitive\nand improved when we corrected to use 5-step TD initial priorities and by using the second half-batch, yet this\nrun still plateaued at a low score, below 6,000. Work to remedy this continues.\n5We used a replay ratio of 1, including the warmup samples, whereas the original authors ran a replay ratio\nnear 0.8 counting only the training samples; by their counting we ran at 0.67.\n62x Intel Xeon Gold 6126, circa 2017.\n6\n4\nNew Data Structure: namedarraytuple\nrlpyt introduces new object classes \"namedarraytuples\" for easier organization of collections of\nnumpy arrays or torch tensors. A namedarraytuple is essentially a namedtuple which exposes indexed\nor sliced read/writes into the structure. Consider writing into a (possibly nested) dictionary of arrays\nwhich share some common dimensions for addressing:\nfor k, v in src.items ():\nif isinstance(dest[k], dict):\n.. recurse ..\ndest[k][ slice_or_indexes ] = v\nThis code is replaced by the following:\ndest[ slice_or_indexes ] = src\nImportantly, the syntax is the same whether dest and src are individual numpy arrays or arbitrarily-\nstructured collections of arrays. The structures of dest and src must match, or src can be a single\nvalue to apply to all ﬁelds, and None is a special placeholder value for ﬁelds to ignore. rlpyt uses this\ndata structure extensively–different elements of training data are organized with the same leading\ndimensions, making it easy to interact with desired time- or batch-dimensions.\nThis is also intended to support environments with multi-modal observations or actions. Rather\nthan ﬂattening and merging, say, camera images and joint-angles into one observation vector, the\nenvironment can store them as-is into a namedarraytuple for the observation. In the forward method\nof the model, observation.joint and observation.image can be fed into the desired layers,\nwithout changing intermediate infrastructure code. For more details, see the code and documentation\nfor namedarraytuples in rlpyt/utils/collections.py.\nThe use of namedtuples and namedarraytuples may incur some programming overhead during setup\nor modiﬁcation of agents, algorithms, and environments. For example, for serialization7 they must be\ndeﬁned at the module-level, which can be accomplished dynamically via the use of a global variable\n(see the Gym wrappers). A beneﬁt of these explicitly-deﬁned interfaces is that they reduce chance of\nmistake by omission or replacement of a shared-memory buffer element by local memory.\n5\nRelated Work\nFor newcomers to deep RL, other resources may be better for familiarization with algorithms, such as\nOpenAI Spinning Up [25].8,9 rlpyt is a revision and extension of the accel_rl codebase, 10 which\nexplored scaling RL in the Atari domain using Theano [26], see [24] for results. For a further study\nof scaling in deep learning including RL, see [27]. rlpyt and accel_rl were originally inspired by rllab\n[28] (for example the logger remains nearly a direct copy)11.\nOther published research code bases include OpenAI Baselines [29] and Dopamine [30], both of\nwhich are implemented in Tensorﬂow [31], and neither of which are optimized to the extent of rlpyt\nnor contain all three algorithm families. Rllib [32], built on top of Ray [33], focuses on distributed\ncomputing, possibly complicating small experiments. Facebook Horizon [34] offers a subset of\nalgorithms and focuses on applications toward production at scale. In sum, rlpyt provides modular\nimplementations of more algorithms and modular infrastructure for parallelism, making it a distinct\ntool set supporting a wide range of research uses.\n7The only built-in use of serialization for samples data is the option of dropping into a subprocess while\ngenerating initial examples for buffer allocation. Model forward execution triggers MKL OpenMP threading\ninitialization which can affect subprocesses thereafter. For example, parallel-CPU sampler agents should be\ninitialized with 1 MKL thread if on 1 CPU core, whereas the optimizer might use multiple cores and threads.\nIncidentally, most rlpyt subprocesses set torch.num_threads(1) to avoid hanging on MKL, which might not\nbe fork-safe.\n8https://spinningup.openai.com/en/latest/index.html\n9https://github.com/openai/spinningup\n10https://github.com/astooke/accel_rl\n11https://github.com/rll/rllab\n7\n6\nImplementation and Usage Details\nTo get started, it is recommended to follow the example scripts provided in the repository and read\nthe notes therein. The following is a conceptual overview without code.\n6.1\nCode Structure.\nThe following tree and descriptions summarize the structure of classes and interfaces.\nRunner\nSampler\nCollector\nEnvironment\nObservation Space\nAction Space\nTrajectoryInfo\nAgent\nModel\nDistribution\nAlgorithm\nOptimizer\nOptimizationInfo\nLogger\nRunner - Connects the sampler, agent, and algorithm; manages the training loop and logging of\ndiagnostics.\nSampler - Manages agent-environment interaction to collect training data; can initialize parallel\nworkers.\nCollector - Steps environments (and maybe operates agent) and records samples.\nEnvironment - The task to be learned. As in previous implementations, at each step outputs:\n(observation, reward, done, env_info).\nObservation/Action Space - Interface speciﬁcations from environment to agent.\nTrajectoryInfo - Diagnostics logged on a per-trajectory basis.\nAgent - Chooses control action to the environment in sampler; trained by the algorithm; interface to\nmodel; holds model recurrent state during sampling. As in previous implementations, at each step\noutputs (action, agent_info).\nModel\n-\nPyTorch\nneural\nnetwork\nmodule\naccepting\n(observation, prev_action,\nprev_reward) and possibly initial_rnn_state arguments.\nDistribution - Samples actions for stochastic agents; deﬁnes related formulas for loss functions.\nAlgorithm - Uses gathered samples to train the agent, e.g. deﬁnes a loss function and performs\ngradient descent.\nOptimizer - Training update rule (e.g. Adam) for model parameters.\nOptimizationInfo - Diagnostics logged on a per-training batch basis.\nLogger - Available throughout all processes and classes for recording printed statements and/or\ntabular values.\n6.2\nNo Asynchronous Optimization\nRecent projects in large-scale RL, such as OpenAI Five [5] and DeepMind AlphaStar [4], have\nsucceeded using synchronous multi-device optimization (meaning every gradient is all-reduced across\ndevices, which hold the same parameter values). Previous experience in [24] found good scaling of\nasynchronous, multi-GPU A3C and PPO on Atari using a CPU parameter store, but this technique\n8\ndid not scale as well to larger networks with more training updates, such as in DQN. Therefore, rlpyt\ncurrently does not include asynchronous optimization schemes such as those in [8, 35].\n6.3\nRecurrent Agents\nAll agents receive the (observation, previous_action, previous_reward) inputs (see e.g.\n[8]), although standard feedforward agents might use only the observation. The recurrent state is\norganized into its own namedarraytuple and can be customized.\nSampling. The agent handles the recurrent state during environment sampling. This functionality is\nprovided in an optimized fashion according to the CuDNN [36] interface, agnostic to the structure of\nthat state. Separate mixin classes for custom agents are included for regular sampling and alternating\nsampling. Recurrent state is recorded under agent_info.\nTraining. Training data is organized with leading dimensions of [Time, Batch], matching the\nPyTorch/CuDNN implementations of recurrence. For CuDNN, the initial recurrent state must be\nre-organized into [Num_Layers, Batch, Hidden_Size] dimensions and made contiguous, as\nshown in the included recurrent agent classes.\n6.4\nData Organization Inferred in Model Forward Method\nThe same model can be used with different leading dimensions: a single input (no leading dims), a\nbatch [Batch, ..], or a time-batch [Time, Batch, ..]. In the model’s forward method, leading\ndimensions are inferred according to known dimension of the observation, for example. Inputs\nare reshaped accordingly for feed-forward or recurrent layers, and ﬁnally the outputs have their\nleading dimensions restored according to what was input. This way, the same model can be used for\naction-selection during sampling, for training, and for extracting single examples for constructing\nbuffers. See any of the included models for a template of this pattern which should be followed in\nany custom models.\n6.5\nOpenAI Gym Interface\nThe use of preallocated buffers requires one modiﬁcation to the Gym environment interface: the\nenv_info dictionary must provide the same keys/ﬁelds at every step. A Gym-style wrapper is\nincluded, which converts the env_info into a namedtuple for easy writing into the samples buffer.\nAn additional wrapper component is provided as one way to ensure all keys are present at every step.\nA wrapper is also provided for Gym spaces to convert them to the corresponding rlpyt space (notably\nthe multi-modal Gym Dictionary space becomes the rlpyt Composite space.)\n6.6\nLaunching Utilities\nLaunching utilities are included for building variants and stacking / queueing experiments on given\nlocal hardware resources. For example, on an 8-GPU, 40-CPU machine, one may want to run some\nnumber of variants (say, 30 different settings/seeds), each using 2 GPUs; the launcher will launch 4\nexperiments on non-overlapping resources (each with 2 GPUs and 10 CPUs), and as those ﬁnish, it\nwill launch the next in their places until all are complete. Results are recorded into a ﬁle structure\nwhich matches that of the variants generated (see the example scripts). Other scripting patterns may\nbe preferable for widely parallelized launching into the cloud.\n7\nConclusion\nWe hope that rlpyt can facilitate adoption of existing deep RL techniques and serve as a launching\npoint for research into new ones. For example, the more advanced topics of meta-learning, model-\nbased, and multi-agent RL are not explicitly addressed in rlpyt, but applicable code components may\nstill be helpful in accelerating their development. We expect the offering of algorithms to grow over\ntime as the ﬁeld matures.\n9\nAcknowledgments\nFirst we acknowledge the original authors of all algorithms and supporting libraries, as listed in the\nreferences. Thanks to Steven Kapturowski for clariﬁcation of several implementation details of R2D2,\nand to Josh Achiam and Wilson Yan for help debugging SAC. Adam Stooke gratefully acknowledges\nthe support of the Fannie and John Hertz Foundation and the NVIDIA Corporation.\nReferences\n[1] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan\nWierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint\narXiv:1312.5602, 2013.\n[2] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region\npolicy optimization. In International conference on machine learning, pages 1889–1897, 2015.\n[3] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,\nZeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in\nPyTorch. In NIPS Autodiff Workshop, 2017.\n[4] DeepMind.\nAlphastar.\nhttps://deepmind.com/blog/article/\nalphastar-mastering-real-time-strategy-game-starcraft-ii, 2019.\n[5] OpenAI. Openai ﬁve. https://blog.openai.com/openai-five/, 2018.\n[6] Steven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney. Recurrent\nexperience replay in distributed reinforcement learning. 2018.\n[7] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,\nand Wojciech Zaremba. Openai gym, 2016.\n[8] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lilli-\ncrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep\nreinforcement learning. In International conference on machine learning, pages 1928–1937,\n2016.\n[9] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n[10] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double\nq-learning. In Thirtieth AAAI conference on artiﬁcial intelligence, 2016.\n[11] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Van Hasselt, Marc Lanctot, and Nando\nDe Freitas. Dueling network architectures for deep reinforcement learning. arXiv preprint\narXiv:1511.06581, 2015.\n[12] Marc G Bellemare, Will Dabney, and Rémi Munos. A distributional perspective on reinforce-\nment learning. In Proceedings of the 34th International Conference on Machine Learning-\nVolume 70, pages 449–458. JMLR. org, 2017.\n[13] Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dab-\nney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining\nimprovements in deep reinforcement learning. In Thirty-Second AAAI Conference on Artiﬁcial\nIntelligence, 2018.\n[14] Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado\nVan Hasselt, and David Silver. Distributed prioritized experience replay. arXiv preprint\narXiv:1803.00933, 2018.\n[15] Will Dabney, Georg Ostrovski, David Silver, and Rémi Munos. Implicit quantile networks for\ndistributional reinforcement learning. arXiv preprint arXiv:1806.06923, 2018.\n10\n[16] Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous deep q-learning\nwith model-based acceleration. In International Conference on Machine Learning, pages\n2829–2838, 2016.\n[17] Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in\nactor-critic methods. arXiv preprint arXiv:1802.09477, 2018.\n[18] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-\npolicy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint\narXiv:1801.01290, 2018.\n[19] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan,\nVikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft actor-critic\nalgorithms and applications. CoRR, abs/1812.05905, 2018. URL http://arxiv.org/abs/\n1812.05905.\n[20] Gabriel Barth-Maron, Matthew W Hoffman, David Budden, Will Dabney, Dan Horgan, Alistair\nMuldal, Nicolas Heess, and Timothy Lillicrap. Distributed distributional deterministic policy\ngradients. arXiv preprint arXiv:1804.08617, 2018.\n[21] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay.\narXiv preprint arXiv:1511.05952, 2015.\n[22] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning\nenvironment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence\nResearch, 47:253–279, 2013.\n[23] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based\ncontrol. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages\n5026–5033. IEEE, 2012.\n[24] Adam Stooke and Pieter Abbeel. Accelerated methods for deep reinforcement learning. arXiv\npreprint arXiv:1803.02811, 2018.\n[25] Joshua Achiam. Openai spinning up. GitHub, GitHub repository, 2018.\n[26] Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian Goodfellow, Arnaud\nBergeron, Nicolas Bouchard, David Warde-Farley, and Yoshua Bengio. Theano: new features\nand speed improvements. arXiv preprint arXiv:1211.5590, 2012.\n[27] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model\nof large-batch training. arXiv preprint arXiv:1812.06162, 2018.\n[28] Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking\ndeep reinforcement learning for continuous control. In International Conference on Machine\nLearning, pages 1329–1338, 2016.\n[29] Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec\nRadford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines.\nGitHub, GitHub repository, 2017.\n[30] Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G Belle-\nmare. Dopamine: A research framework for deep reinforcement learning. arXiv preprint\narXiv:1812.06110, 2018.\n[31] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu\nDevin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow: A system for\nlarge-scale machine learning. In 12th {USENIX} Symposium on Operating Systems Design and\nImplementation ({OSDI} 16), pages 265–283, 2016.\n[32] Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Joseph Gonzalez, Ken\nGoldberg, and Ion Stoica. Ray rllib: A composable and scalable reinforcement learning library.\narXiv preprint arXiv:1712.09381, 2017.\n11\n[33] Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang,\nMelih Elibol, Zongheng Yang, William Paul, Michael I Jordan, et al. Ray: A distributed\nframework for emerging {AI} applications. In 13th {USENIX} Symposium on Operating\nSystems Design and Implementation ({OSDI} 18), pages 561–577, 2018.\n[34] Jason Gauci, Edoardo Conti, Yitao Liang, Kittipat Virochsiri, Yuchen He, Zachary Kaden,\nVivek Narayanan, and Xiaohui Ye. Horizon: Facebook’s open source applied reinforcement\nlearning platform. arXiv preprint arXiv:1811.00260, 2018.\n[35] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild: A lock-free\napproach to parallelizing stochastic gradient descent. In Advances in neural information\nprocessing systems, pages 693–701, 2011.\n[36] Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen, John Tran, Bryan\nCatanzaro, and Evan Shelhamer. cudnn: Efﬁcient primitives for deep learning. arXiv preprint\narXiv:1410.0759, 2014.\n12\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2019-09-03",
  "updated": "2019-09-24"
}