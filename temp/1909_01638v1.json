{
  "id": "http://arxiv.org/abs/1909.01638v1",
  "title": "Do We Really Need Fully Unsupervised Cross-Lingual Embeddings?",
  "authors": [
    "Ivan Vulić",
    "Goran Glavaš",
    "Roi Reichart",
    "Anna Korhonen"
  ],
  "abstract": "Recent efforts in cross-lingual word embedding (CLWE) learning have\npredominantly focused on fully unsupervised approaches that project monolingual\nembeddings into a shared cross-lingual space without any cross-lingual signal.\nThe lack of any supervision makes such approaches conceptually attractive. Yet,\ntheir only core difference from (weakly) supervised projection-based CLWE\nmethods is in the way they obtain a seed dictionary used to initialize an\niterative self-learning procedure. The fully unsupervised methods have arguably\nbecome more robust, and their primary use case is CLWE induction for pairs of\nresource-poor and distant languages. In this paper, we question the ability of\neven the most robust unsupervised CLWE approaches to induce meaningful CLWEs in\nthese more challenging settings. A series of bilingual lexicon induction (BLI)\nexperiments with 15 diverse languages (210 language pairs) show that fully\nunsupervised CLWE methods still fail for a large number of language pairs\n(e.g., they yield zero BLI performance for 87/210 pairs). Even when they\nsucceed, they never surpass the performance of weakly supervised methods\n(seeded with 500-1,000 translation pairs) using the same self-learning\nprocedure in any BLI setup, and the gaps are often substantial. These findings\ncall for revisiting the main motivations behind fully unsupervised CLWE\nmethods.",
  "text": "Do We Really Need Fully Unsupervised Cross-Lingual Embeddings?\nIvan Vuli´c1, Goran Glavaš2, Roi Reichart3, Anna Korhonen1\n1 Language Technology Lab, University of Cambridge\n2 Data and Web Science Group, University of Mannheim\n3 Faculty of Industrial Engineering and Management, Technion, IIT\n{iv250,alk23}@cam.ac.uk\ngoran@informatik.uni-mannheim.de\nroiri@ie.technion.ac.il\nAbstract\nRecent efforts in cross-lingual word embed-\nding (CLWE) learning have predominantly fo-\ncused on fully unsupervised approaches that\nproject monolingual embeddings into a shared\ncross-lingual space without any cross-lingual\nsignal.\nThe lack of any supervision makes\nsuch approaches conceptually attractive. Yet,\ntheir only core difference from (weakly) super-\nvised projection-based CLWE methods is in\nthe way they obtain a seed dictionary used to\ninitialize an iterative self-learning procedure.\nThe fully unsupervised methods have arguably\nbecome more robust, and their primary use\ncase is CLWE induction for pairs of resource-\npoor and distant languages. In this paper, we\nquestion the ability of even the most robust un-\nsupervised CLWE approaches to induce mean-\ningful CLWEs in these more challenging set-\ntings. A series of bilingual lexicon induction\n(BLI) experiments with 15 diverse languages\n(210 language pairs) show that fully unsuper-\nvised CLWE methods still fail for a large num-\nber of language pairs (e.g., they yield zero BLI\nperformance for 87/210 pairs).\nEven when\nthey succeed, they never surpass the perfor-\nmance of weakly supervised methods (seeded\nwith 500-1,000 translation pairs) using the\nsame self-learning procedure in any BLI setup,\nand the gaps are often substantial. These ﬁnd-\nings call for revisiting the main motivations be-\nhind fully unsupervised CLWE methods.\n1\nIntroduction and Motivation\nThe wide use and success of monolingual word\nembeddings in NLP tasks (Turian et al., 2010;\nChen and Manning, 2014) has inspired further\nresearch focus on the induction of cross-lingual\nword embeddings (CLWEs). CLWE methods learn\na shared cross-lingual word vector space where\nwords with similar meanings obtain similar vectors\nregardless of their actual language. CLWEs beneﬁt\ncross-lingual NLP, enabling multilingual modeling\nof meaning and supporting cross-lingual transfer\nfor downstream tasks and resource-lean languages.\nCLWEs provide invaluable cross-lingual knowl-\nedge for, inter alia, bilingual lexicon induction\n(Gouws et al., 2015; Heyman et al., 2017), infor-\nmation retrieval (Vuli´c and Moens, 2015; Litschko\net al., 2019), machine translation (Artetxe et al.,\n2018c; Lample et al., 2018b), document classiﬁca-\ntion (Klementiev et al., 2012), cross-lingual plagia-\nrism detection (Glavaš et al., 2018), domain adapta-\ntion (Ziser and Reichart, 2018), cross-lingual POS\ntagging (Gouws and Søgaard, 2015; Zhang et al.,\n2016), and cross-lingual dependency parsing (Guo\net al., 2015; Søgaard et al., 2015).\nThe landscape of CLWE methods has recently\nbeen dominated by the so-called projection-based\nmethods (Mikolov et al., 2013a; Ruder et al.,\n2019; Glavaš et al., 2019). They align two mono-\nlingual embedding spaces by learning a projec-\ntion/mapping based on a training dictionary of\ntranslation pairs. Besides their simple conceptual\ndesign and competitive performance, their popular-\nity originates from the fact that they rely on rather\nweak cross-lingual supervision. Originally, the\nseed dictionaries typically spanned several thou-\nsand word pairs (Mikolov et al., 2013a; Faruqui\nand Dyer, 2014; Xing et al., 2015), but more recent\nwork has shown that CLWEs can be induced with\neven weaker supervision from small dictionaries\nspanning several hundred pairs (Vuli´c and Korho-\nnen, 2016), identical strings (Smith et al., 2017), or\neven only shared numerals (Artetxe et al., 2017).\nTaking the idea of reducing cross-lingual super-\nvision to the extreme, the latest CLWE develop-\nments almost exclusively focus on fully unsuper-\nvised approaches (Conneau et al., 2018a; Artetxe\net al., 2018b; Dou et al., 2018; Chen and Cardie,\n2018; Alvarez-Melis and Jaakkola, 2018; Kim\net al., 2018; Alaux et al., 2019; Mohiuddin and Joty,\n2019, inter alia): they fully abandon any source of\narXiv:1909.01638v1  [cs.CL]  4 Sep 2019\n(even weak) supervision and extract the initial seed\ndictionary by exploiting topological similarities be-\ntween pre-trained monolingual embedding spaces.\nTheir modus operandi can roughly be described\nby three main components: C1) unsupervised ex-\ntraction of a seed dictionary; C2) a self-learning\nprocedure that iteratively reﬁnes the dictionary to\nlearn projections of increasingly higher quality;\nand C3) a set of preprocessing and postprocessing\nsteps (e.g., unit length normalization, mean cen-\ntering, (de)whitening) (Artetxe et al., 2018a) that\nmake the entire learning process more robust.\nThe induction of fully unsupervised CLWEs\nis an inherently interesting research topic per se.\nNonetheless, the main practical motivation for\ndeveloping such approaches in the ﬁrst place is\nto facilitate the construction of multilingual NLP\ntools and widen the access to language technology\nfor resource-poor languages and language pairs.\nHowever, the ﬁrst attempts at fully unsupervised\nCLWE induction failed exactly for these use cases,\nas shown by Søgaard et al. (2018).\nTherefore,\nthe follow-up work aimed to improve the robust-\nness of unsupervised CLWE induction by introduc-\ning more robust self-learning procedures (Artetxe\net al., 2018b; Kementchedjhieva et al., 2018). Be-\nsides increased robustness, recent work claims that\nfully unsupervised projection-based CLWEs can\neven match or surpass their supervised counter-\nparts (Conneau et al., 2018a; Artetxe et al., 2018b;\nAlvarez-Melis and Jaakkola, 2018; Hoshen and\nWolf, 2018; Heyman et al., 2019).\nIn this paper, we critically examine these claims\non robustness and improved performance of unsu-\npervised CLWEs by running a large-scale evalua-\ntion in the bilingual lexicon induction (BLI) task\non 15 languages (i.e., 210 languages pairs, see Ta-\nble 2 in §3). The languages were selected to repre-\nsent different language families and morphological\ntypes, as we argue that fully unsupervised CLWEs\nhave been designed to support exactly these se-\ntups. However, we show that even the most ro-\nbust unsupervised CLWE method (Artetxe et al.,\n2018b) still fails for a large number of language\npairs: 87/210 BLI setups are unsuccessful, yield-\ning (near-)zero BLI performance. Further, even\nwhen the unsupervised method succeeds, it is be-\ncause the components C2 (self-learning) and C3\n(pre-/post-processing) can mitigate the undesired\neffects of noisy seed lexicon extraction. We then\ndemonstrate that the combination of C2 and C3\nwith a small provided seed dictionary (e.g., 500\nor 1K pairs) outscores the unsupervised method in\nall cases, often with a huge margin, and does not\nfail for any language pair. Furthermore, we show\nthat the most robust unsupervised CLWE approach\nstill fails completely when it relies on monolingual\nword vectors trained on domain-dissimilar corpora.\nWe also empirically verify that unsupervised ap-\nproaches cannot outperform weakly supervised ap-\nproaches also for closely related languages (e.g.,\nSwedish–Danish, Spanish–Catalan).\nWhile the “no supervision at all” premise be-\nhind fully unsupervised CLWE methods is indeed\nseductive, our study strongly suggests that future\nresearch efforts should revisit the main motiva-\ntion behind these methods and focus on design-\ning even more robust solutions, given their cur-\nrent inability to support a wide spectrum of lan-\nguage pairs.\nIn hope of boosting induction of\nCLWEs for more diverse and distant language\npairs, we make all 210 training and test dictionar-\nies used in this work publicly available at: https:\n//github.com/ivulic/panlex-bli.\n2\nMethodology\nWe now dissect a general framework for unsuper-\nvised CLWE learning, and show that the “bag of\ntricks of the trade” used to increase their robustness\n(which often slips under the radar) can be equally\napplied to (weakly) supervised projection-based\napproaches, leading to their fair(er) comparison.\n2.1\nProjection-Based CLWE Approaches\nIn short, projection-based CLWE methods learn\nto (linearly) align independently trained monolin-\ngual spaces X and Z, using a word translation\ndictionary D0 to guide the alignment process. Let\nXD ⊂X and ZD ⊂Z be the row-aligned sub-\nsets of monolingual spaces containing vectors of\naligned words from D0. Alignment matrices XD\nand ZD are then used to learn orthogonal transfor-\nmations Wx and Wz that deﬁne the joint bilingual\nspace Y\n= XWx ∪ZWz. While supervised\nprojection-based CLWE models learn the mapping\nusing a provided external (clean) dictionary D0,\ntheir unsupervised counterparts automatically in-\nduce the seed dictionary in an unsupervised way\n(C1) and then reﬁne it in an iterative fashion (C2).\nUnsupervised CLWEs. These methods ﬁrst in-\nduce a seed dictionary D(1) leveraging only two\nunaligned monolingual spaces (C1). While the\nC3 (S1)\nX\nX’\nZ\nZ’\nC1\nX’\nZ’\nD(1)\n2.\n2.\nNormalization & centering\nInduction of seed dictionary\nC2 (self-learning)\nC3 (S2)\nWhitening\nXD\n(k),\nZD\n(k)\nZD\n(k)\nXD\n(k)\nLearning \nprojections\nY(k)\nMutual NN\nC3 (S3, S4)\nDe-whitening\nRe-weighting\nY(k)\nY(k)\nD(k+1)\n3.\n1.\nFigure 1: General unsupervised CLWE approach.\nalgorithms for unsupervised seed dictionary induc-\ntion differ, they all strongly rely on the assump-\ntion of similar topological structure between the\ntwo pretrained monolingual spaces. Once the seed\ndictionary is obtained, the two-step iterative self-\nlearning procedure (C2) takes place: 1) a dictio-\nnary D(k) is ﬁrst used to learn the joint space\nY (k) = XW (k)\nx\n∪ZW (k)\nz ; 2) the nearest neigh-\nbours in Y (k) then form the new dictionary D(k+1).\nWe illustrate the general structure in Figure 1.\nA recent empirical survey paper (Glavaš et al.,\n2019) has compared a variety of latest unsupervised\nCLWE methods (Conneau et al., 2018a; Alvarez-\nMelis and Jaakkola, 2018; Hoshen and Wolf, 2018;\nArtetxe et al., 2018b) in several downstream tasks\n(e.g., BLI, cross-lingual information retrieval, doc-\nument classiﬁcation). The results of their study\nindicate that the VECMAP model of Artetxe et al.\n(2018b) is by far the most robust and best perform-\ning unsupervised CLWE model. For the actual\nresults and analyses, we refer the interested reader\nto the original paper of Glavaš et al. (2019). An-\nother recent evaluation paper (Doval et al., 2019) as\nwell as our own preliminary BLI tests (not shown\nfor brevity) have further veriﬁed their ﬁndings. We\nthus focus on VECMAP in our analyses, and base\nthe following description of the components C1-C3\non that model.\n2.2\nThree Key Components\nC1.\nSeed Lexicon Extraction.\nVECMAP in-\nduces the initial seed dictionary using the follow-\ning heuristic: monolingual similarity distributions\nfor words with similar meaning will be similar\nacross languages.1\nThe monolingual similarity\n1For instance, zwei and two will have similar distributions\nof similarities over their respective language vocabularies –\ndistributions for the two languages are given as\nrows (or columns; the matrices are symmetric) of\nMx = XXT and Mz = ZZT . For the distri-\nbutions of similarity scores to be comparable, the\nvalues in each row of Mx and Mz are ﬁrst sorted.\nThe initial dictionary D(1) is ﬁnally obtained by\nsearching for mutual nearest neighbours between\nthe rows of √Mx and of √Mz.\nC2. Self-Learning. Not counting the preprocess-\ning and postprocessing steps (component C3), self-\nlearning then iteratively repeats two steps:\n1) Let D(k) be the binary matrix indicating the\naligned words in the dictionary D(k).2 The or-\nthogonal transformation matrices are then obtained\nas W (k)\nx\n= U and W (k)\nz\n= V , where UΣV T\nis the singular value decomposition of the matrix\nXT D(k)Z. The cross-lingual space of the k-th\niteration is then Y (k) = XW (k)\nx\n∪ZW (k)\nz\n.\n2) The new dictionary D(k+1) is then built by\nidentifying nearest neighbours in Y (k).\nThese\ncan be easily extracted from the matrix P\n=\nXW (k)\nx (ZW (k)\nz\n)T . All nearest neighbours can\nbe used, or additional symmetry constraints can be\nimposed to extract only mutual nearest neighbours:\nall pairs of indices (i, j) for which Pij is the largest\nvalue both in row i and column j.\nThe above procedure, however, often converges\nto poor local optima. To remedy for this, the sec-\nond step (i.e., dictionary induction) is extended\nwith techniques that make self-learning more ro-\nbust. First, the vocabularies of X and Z are cut to\nthe top k most frequent words.3 Second, similarity\nscores in P are kept with probability p, and set to\nzero otherwise. This dropout allows for a wider\nexploration of possible word pairs in the dictionary\nand contributes to escaping poor local optima given\nthe noisy seed lexicon in the ﬁrst iterations.\nC3.\nPreprocessing and Postprocessing Steps.\nWhile iteratively learning orthogonal transforma-\ntions Wx and Wz for X and Z is the central step\nof unsupervised projection-based CLWE methods,\npreprocessing and postprocessing techniques are\nadditionally applied before and after the transfor-\nmation.\nWhile such techniques are often over-\nzwei is expected to be roughly as (dis)similar to drei and Katze\nas two is to three and cat.\n2I.e., D(k)\nij\n= 1 ⇐⇒the i-th word of one language and\nthe j-th word of the other are a translation pair in D(k).\n3This is done to prevent spurious nearest neighbours con-\nsisting of infrequent words with unreliable vectors.\nlooked in model comparisons, they may have a\ngreat impact on the model’s ﬁnal performance, as\nwe validate in §4. We brieﬂy summarize two pre-\nprocessing (S1 and S2) and post-processing (S3\nand S4) steps used in our evaluation, originating\nfrom the framework of Artetxe et al. (2018a).\nS1) Normalization and mean centering. We ﬁrst\napply unit length normalization: all vectors in X\nand Z are normalized to have a unit Euclidean\nnorm. Following that, X and Z are mean centered\ndimension-wise and then again length-normalized.\nS2) Whitening.\nZCA whitening (Bell and Se-\njnowski, 1997) is applied on (S1-processed) X\nand Z: it transforms the matrices such that each di-\nmension has unit variance and that the dimensions\nare uncorrelated. Intuitively, the vector spaces are\neasier to align along directions of high variance.\nS3) Dewhitening. A transformation inverse to S2:\nfor improved performance it is important to restore\nthe variance information after the projection, if\nwhitening was applied in S2 (Artetxe et al., 2018a).\nS4) Symmetric re-weighting. This step attempts to\nfurther align the embeddings in the cross-lingual\nembedding space by measuring how well a dimen-\nsion in the space correlates across languages for\nthe current iteration dictionary D(k).4 The best re-\nsults are obtained when re-weighting is neutral to\nthe projection direction, that is, when it is applied\nsymmetrically in both languages.\nIn the actual implementation S1 is applied only\nonce, before self-learning. S2, S3 and S4 are ap-\nplied in each self-learning iteration.\nModel Conﬁgurations. Note that C2 and C3 can\nbe equally used on top of any (provided) seed lexi-\ncon (i.e., D(1):=D0) to enable weakly supervised\nlearning, as we propose here. In fact, the variations\nof the three key components, C1) seed lexicon,\nC2) self-learning, and C3) preprocessing and post-\nprocessing, construct various model conﬁgurations\nwhich can be analyzed to probe the importance of\neach component in the CLWE induction process. A\nselection of representative conﬁgurations evaluated\n4More formally, assume that we are working with matrices\nX and Z that already underwent all transformations described\nin S1-S3. Another matrix D represents the current bilingual\ndictionary D: Dij = 1 if the ith source word is translated\nby the jth target word and Dij = 0 otherwise. Then, given\nthe singular value decomposition USV T = XT DZ, the\nﬁnal re-weighted projection matrices are Wx = US\n1\n2 (and\nWz = V S\n1\n2 . We refer the reader to (Artetxe et al., 2018a)\nand (Artetxe et al., 2018b) for more details.\nlater in §4 is summarized in Table 1.\n3\nExperimental Setup\nEvaluation Task. Our task is bilingual lexicon in-\nduction (BLI). It has become the de facto standard\nevaluation for projection-based CLWEs (Ruder\net al., 2019; Glavaš et al., 2019). In short, after\na shared CLWE space has been induced, the task is\nto retrieve target language translations for a test set\nof source language words. Its lightweight nature\nallows us to conduct a comprehensive evaluation\nacross a large number of language pairs.5 Since\nBLI is cast as a ranking task, following Glavaš et al.\n(2019) we use mean average precision (MAP) as\nthe main evaluation metric: in our BLI setup with\nonly one correct translation for each “query” word,\nMAP is equal to mean reciprocal rank (MRR).6\n(Selection of) Language Pairs. Our selection of\ntest languages is guided by the following goals: a)\nfollowing recent initiatives in other NLP research\n(e.g., for language modeling) (Cotterell et al., 2018;\nGerz et al., 2018), we aim to ensure the coverage\nof different genealogical and typological language\nproperties, and b) we aim to analyze a large set of\nlanguage pairs and offer new evaluation data which\nextends and surpasses other work in the CLWE\nliterature. These two properties will facilitate anal-\nyses between (dis)similar language pairs and offer\na comprehensive set of evaluation setups that test\nthe robustness and portability of fully unsupervised\nCLWEs. The ﬁnal list of 15 diverse test languages\nis provided in Table 2, and includes samples from\ndifferent languages types and families. We run BLI\nevaluations for all language pairs in both directions,\nfor a total of 15×14=210 BLI setups.\nMonolingual Embeddings. We use the 300-dim\nvectors of Grave et al. (2018) for all 15 languages,\npretrained on Common Crawl and Wikipedia with\nfastText (Bojanowski et al., 2017).7 We trim all\n5While BLI is an intrinsic task, as discussed by Glavaš\net al. (2019) it is a strong indicator of CLWE quality also\nfor downstream tasks: relative performance in the BLI task\ncorrelates well with performance in cross-lingual information\nretrieval (Litschko et al., 2018) or natural language inference\n(Conneau et al., 2018b). More importantly, it also provides\na means to analyze whether a CLWE method manages to\nlearn anything meaningful at all, and can indicate “unsuccess-\nful” CLWE induction (e.g., when BLI performance is similar\nto a random baseline): detecting such CLWEs is especially\nimportant when dealing with fully unsupervised models.\n6MRR is more informative than the more common Preci-\nsion@1 (P@1); our main ﬁndings are also valid when P@1 is\nused; we do not report the results for brevity.\n7Experiments with other monolingual vectors such as the\nConﬁguration\nC1\nC2\nC3\nUNSUPERVISED\nunsupervised\nall tested, we always report the best one\nS1-S4 (FULL)\nORTHG-SUPER\nprovided\n–\nlength normalization only (partial S1)\nORTHG+SL+SYM\nprovided\nsymmetric: mutual nearest neighbours\nlength normalization only (partial S1)\nFULL-SUPER\nprovided\n–\nS1-S4 (FULL)\nFULL+SL\nprovided\n(Artetxe et al., 2018b) with dropout\nS1-S4 (FULL)\nFULL+SL+NOD\nprovided\n(Artetxe et al., 2018b) w/o dropout\nS1-S4 (FULL)\nFULL+SL+SYM\nprovided\nsymmetric: mutual nearest neighbours, w/o dropout\nS1-S4 (FULL)\nTable 1: Conﬁgurations obtained by varying components C1, C2, and C3 used in our empirical comparison in §4.\nLanguage\nFamily\nType\nISO 639-1\nBulgarian\nIE: Slavic\nfusional\nBG\nCatalan\nIE: Romance\nfusional\nCA\nEsperanto\n– (constructed)\nagglutinative\nEO\nEstonian\nUralic\nagglutinative\nET\nBasque\n– (isolate)\nagglutinative\nEU\nFinnish\nUralic\nagglutinative\nFI\nHebrew\nAfro-Asiatic\nintroﬂexive\nHE\nHungarian\nUralic\nagglutinative\nHU\nIndonesian\nAustronesian\nisolating\nID\nGeorgian\nKartvelian\nagglutinative\nKA\nKorean\nKoreanic\nagglutinative\nKO\nLithuanian\nIE: Baltic\nfusional\nLT\nBokmål\nIE: Germanic\nfusional\nNO\nThai\nKra-Dai\nisolating\nTH\nTurkish\nTurkic\nagglutinative\nTR\nTable 2:\nThe list of 15 languages from our main\nBLI experiments along with their corresponding lan-\nguage family (IE = Indo-European), broad morpholog-\nical type, and their ISO 639-1 code.\nvocabularies to the 200K most frequent words.\nTraining and Test Dictionaries.\nThey are de-\nrived from PanLex (Baldwin et al., 2010; Kamholz\net al., 2014), which was used in prior work on\ncross-lingual word embeddings (Duong et al., 2016;\nVuli´c et al., 2017). PanLex currently spans around\n1,300 language varieties with over 12M expres-\nsions: it offers some support and supervision also\nfor low-resource language pairs (Adams et al.,\n2017). For each source language (L1), we automat-\nically translate their vocabulary words (if they are\npresent in PanLex) to all 14 target (L2) languages.\nTo ensure the reliability of the translation pairs, we\nretain only unigrams found in the vocabularies of\nthe respective L2 monolingual spaces which scored\nabove a PanLex-predeﬁned threshold.\nAs in prior work (Conneau et al., 2018a; Glavaš\net al., 2019), we then reserve the 5K pairs cre-\nated from the more frequent L1 words for training,\nwhile the next 2K pairs are used for test. Smaller\ntraining dictionaries (1K and 500 pairs) are cre-\nated by again selecting pairs comprising the most\nfrequent L1 words.\noriginal fastText and skip-gram (Mikolov et al., 2013b) trained\non Wikipedia show the same trends in the ﬁnal results.\nTraining Setup. In all experiments, we set the\nhyper-parameters to values that were tuned in prior\nresearch. When extracting the UNSUPERVISED\nseed lexicon, the 4K most frequent words of each\nlanguage are used; self-learning operates on the\n20K most frequent words of each language; with\ndropout the keep probability p is 0.1; CSLS with\nk = 10 nearest neighbors (Artetxe et al., 2018b).\nAgain, Table 1 lists the main model conﬁgura-\ntions in our comparison. For the fully UNSUPER-\nVISED model we always report the best performing\nconﬁguration after probing different self-learning\nstrategies (i.e., +SL, +SL+NOD, and +SL+SYM are\ntested). The results for UNSUPERVISED are always\nreported as averages over 5 restarts: this means that\nwith UNSUPERVISED we count BLI setups as un-\nsuccessful only if the results are close to zero in all\n5/5 runs. ORTHG-SUPER is the standard supervised\nmodel with orthogonal projections from prior work\n(Smith et al., 2017; Glavaš et al., 2019).\n4\nResults and Discussion\nMain BLI results averaged over each source lan-\nguage (L1) are provided in Table 3 and Table 4.\nWe now summarize and discuss the main ﬁndings\nacross several dimensions of comparison.\nUnsupervised vs. (Weakly) Supervised. First,\nwhen exactly the same components C2 and C3 are\nused, UNSUPERVISED is unable to outperform a\n(weakly) supervised FULL+SL+SYM variant, and\nthe gap in ﬁnal performance is often substantial. In\nfact, FULL+SL+SYM and FULL+SL+NOD outper-\nform the best UNSUPERVISED for all 210/210 BLI\nsetups: we observe the same phenomenon with\nvarying dictionary sizes, that is, it equally holds\nwhen we seed self-learning with 5K, 1K, and 500\ntranslation pairs, see also Figure 2. This also sug-\ngests that the main reason why UNSUPERVISED ap-\nproaches were considered on-par with supervised\napproaches in prior work (Conneau et al., 2018a;\nArtetxe et al., 2018b) is because they were not com-\npared under fair circumstances: while UNSUPER-\nBG-*\nCA-*\nEO-*\nET-*\nEU-*\nFI-*\nHE-*\nHU-*\nUNSUPERVISED\n0.208\n0.224\n0.128\n0.155\n0.036\n0.181\n0.186\n0.206\nUnsuccessful setups\n3/14\n2/14\n3/14\n6/14\n10/14\n4/14\n2/14\n3/14\n5K:ORTHG-SUPER\n0.258\n0.237\n0.201\n0.210\n0.151\n0.233\n0.198\n0.259\n5K:ORTHG+SL+SYM\n0.281\n0.264\n0.219\n0.225\n0.164\n0.256\n0.217\n0.283\n5K:FULL-SUPER\n0.343\n0.335\n0.304\n0.301\n0.228\n0.324\n0.287\n0.354\n5K:FULL+SL\n0.271\n0.262\n0.240\n0.236\n0.161\n0.260\n0.217\n0.282\n5K:FULL+SL+NOD\n0.316\n0.311\n0.295\n0.276\n0.204\n0.320\n0.260\n0.330\n5K:FULL+SL+SYM\n0.361\n0.356\n0.336\n0.316\n0.244\n0.348\n0.294\n0.374\n1K:ORTHG-SUPER\n0.104\n0.088\n0.065\n0.082\n0.049\n0.088\n0.066\n0.101\n1K:ORTHG+SL+SYM\n0.203\n0.167\n0.106\n0.157\n0.079\n0.168\n0.133\n0.191\n1K:FULL-SUPER\n0.146\n0.129\n0.098\n0.117\n0.065\n0.117\n0.096\n0.143\n1K:FULL+SL\n0.268\n0.260\n0.238\n0.232\n0.158\n0.257\n0.217\n0.279\n1K:FULL+SL+NOD\n0.312\n0.307\n0.284\n0.272\n0.197\n0.311\n0.255\n0.327\n1K:FULL+SL+SYM\n0.341\n0.327\n0.302\n0.293\n0.212\n0.329\n0.268\n0.354\nID-*\nKA-*\nKO-*\nLT-*\nNO-*\nTH-*\nTR-*\nAvg\nUNSUPERVISED\n0.110\n0.106\n0.001\n0.179\n0.239\n0.000\n0.133\n0.140\nUnsuccessful setups\n7/14\n6/14\n14/14\n4/14\n3/14\n14/14\n6/14\n87/210\n5K:ORTHG-SUPER\n0.199\n0.163\n0.154\n0.194\n0.250\n0.109\n0.207\n0.201\n5K:ORTHG+SL+SYM\n0.216\n0.177\n0.166\n0.212\n0.273\n0.117\n0.226\n0.220\n5K:FULL-SUPER\n0.261\n0.250\n0.239\n0.302\n0.332\n0.154\n0.283\n0.286\n5K:FULL+SL\n0.180\n0.191\n0.152\n0.217\n0.274\n0.056\n0.204\n0.214\n5K:FULL+SL+NOD\n0.220\n0.229\n0.207\n0.272\n0.318\n0.106\n0.253\n0.261\n5K:FULL+SL+SYM\n0.272\n0.263\n0.251\n0.310\n0.356\n0.148\n0.299\n0.302\n1K:ORTHG-SUPER\n0.069\n0.050\n0.040\n0.067\n0.099\n0.034\n0.068\n0.071\n1K:ORTHG+SL+SYM\n0.119\n0.092\n0.063\n0.135\n0.186\n0.052\n0.129\n0.132\n1K:FULL-SUPER\n0.089\n0.079\n0.061\n0.111\n0.127\n0.044\n0.091\n0.101\n1K:FULL+SL\n0.180\n0.185\n0.148\n0.220\n0.274\n0.054\n0.204\n0.212\n1K:FULL+SL+NOD\n0.216\n0.223\n0.197\n0.269\n0.315\n0.096\n0.248\n0.255\n1K:FULL+SL+SYM\n0.243\n0.237\n0.203\n0.284\n0.337\n0.103\n0.274\n0.274\nTable 3: BLI scores (MRR) for all model conﬁgurations. The scores are averaged over all experimental setups\nwhere each of the 15 languages is used as L1: e.g., CA-* means that the translation direction is from Catalan (CA)\nas source (L1) to each of the remaining 14 languages listed in Table 2 as targets (L2), and we average over the\ncorresponding 14 CA-* BLI setups. 5k and 1k denote the seed dictionary size for (weakly) supervised methods\n(D0). Unsuccessful setups refer to the number of BLI experimental setups with the fully UNSUPERVISED model\nthat yield an MRR score ≤0.01. The Avg column refers to the averaged MRR scores of each model conﬁguration\nover all 15×14=210 BLI setups. The highest scores for two different seed dictionary sizes in each column are\nin bold, the second best are underlined. See Table 1 for the brief description of all model conﬁgurations in the\ncomparison. Full results for each particular language pair are available in the supplemental material.\n|D0| = 5k\n|D0| = 1k\nUnsuc.\nWin\nUnsuc.\nWin\nUNSUPERVISED\n87 (94)\n0\n87 (94)\n0\nORTHG-SUPER\n0 (2)\n0\n2 (82)\n0\nORTHG+SL+SYM\n0 (1)\n0\n1 (34)\n0\nFULL-SUPER\n0 (0)\n46\n0 (41)\n0\nFULL+SL\n0 (7)\n0\n0 (9)\n0\nFULL+SL+NOD\n0 (1)\n7\n0 (3)\n33\nFULL+SL+SYM\n0 (0)\n157\n0 (0)\n177\nTable 4:\nSummary statistics computed over all\n15×14=210 BLI setups. a) Unsuc. denotes the total\nnumber of unsuccessful setups, where a setup is consid-\nered unsuccessful if MRR ≤0.01 or MRR ≤0.05 (in\nthe parentheses); b) Win refers to the total number of\n“winning” setups, that is, for all language pairs it counts\nhow many times each particular model yields the best\noverall MRR score. We compute separate statistics for\ntwo settings (|D0| = 1k and |D0| = 5k).\nVISED relied heavily on the components C2 and C3,\nthese were omitted when running supervised base-\nlines. Our unbiased comparison reveals that there\nis a huge gap even when supervised projection-\nbased approaches consume only several hundred\ntranslation pairs to initiate self-learning.\nAre Unsupervised CLWEs Robust? The results\nalso indicate that, contrary to the beliefs established\nby very recent work (Artetxe et al., 2018b; Mohi-\nuddin and Joty, 2019), fully UNSUPERVISED ap-\nproaches are still prone to getting stuck in local op-\ntima, and still suffer from robustness issues when\ndealing with distant language pairs: 87 out of 210\nBLI setups (= 41.4%) result in (near-)zero BLI\nperformance, see also Table 4. At the same time,\nweakly supervised methods with a seed lexicon of\n1k or 500 pairs do not suffer from the robustness\nproblem and always converge to a good solution,\n500 1000\n5000\nSeed dictionary size (|D0|)\n0.1\n0.2\n0.3\nAverage BLI scores (MRR)\nUNSUPER\nSUPER\n+SL+NOD\n+SL+SYM\nFigure 2: A comparison of average BLI scores with\ndifferent seed dictionary sizes D0 between a fully un-\nsupervised method (UNSUPER), a supervised method\nwithout self-learning (SUPER), and two best perform-\ning weakly supervised methods with self learning\n(+SL+NOD and +SL+SYM). While SUPER without self-\nlearning displays a steep drop in performance with\nsmaller seed dictionaries, there is only a slight decrease\nwhen self-learning is used: e.g., 500 translation pairs\nare still sufﬁcient to initialize robust self-learning.\nas also illustrated by the results reported in Table 5.\nHow Important are Preprocessing and Post-\nprocessing? The comparisons between ORTHG-\nSUPER (and ORTHG+SL+SYM) on the one hand,\nand FULL-SUPER (and FULL+SL+SYM) on the\nother hand clearly indicate that the component\nC3 plays a substantial role in effective CLWE\nlearning. FULL-SUPER, which employs all steps\nS1-S4 (see §2), outperforms ORTHG-SUPER in\n208/210 setups with |D0|=5k and in 210/210 setups\nwith |D0|=1k. Similarly, FULL+SL+SYM is better\nthan ORTHG+SL+SYM in 210/210 setups (both for\n|D0|=1k,5k). The scores also indicate that dropout\nwith self-learning is useful only when we work with\nnoisy unsupervised seed lexicons: FULL+SL+NOD\nand FULL+SL+SYM without dropout consistently\noutperform FULL+SL across the board.\nHow Important is (Robust) Self-Learning? We\nnote that the best self-learning method is often use-\nful even when |D0| = 5k (i.e., FULL+SL+SYM\nis better than FULL-SUPER in 164/210 setups).\nHowever, the importance of robust self-learning\ngets more pronounced as we decrease the size of\nD0: FULL+SL+SYM is better than FULL-SUPER\nin 210/210 setups when |D0| = 500 or |D0| =\n1, 000. The gap between the two models, as shown\nin Figure 2, increases dramatically in favor of\nFULL+SL+SYM as we decrease |D0|.\nAgain, just comparing FULL-SUPER and UNSU-\nPERVISED in Figure 2 might give a false impres-\nsion that fully unsupervised CLWE methods can\nmatch their supervised counterparts, but the com-\nparison to FULL+SL+SYM reveals the true extent of\nperformance drop when we abandon even weak su-\npervision. The scores also reveal that the choice of\nself-learning (C2) does matter: all best performing\nBLI runs with |D0| = 1k are obtained by two con-\nﬁgs with self-learning, and FULL+SL+SYM is the\nbest conﬁguration for 177/210 setups (see Table 4).\nLanguage Pairs. As suggested before by Søgaard\net al. (2018) and further veriﬁed by Glavaš et al.\n(2019) and Doval et al. (2019), the language pair at\nhand can have a huge impact on CLWE induction:\nthe adversarial method of Conneau et al. (2018a)\noften gets stuck in poor local optima and yields de-\ngenerate solutions for distant language pairs such\nas English-Finnish. More recent CLWE methods\n(Artetxe et al., 2018b; Mohiuddin and Joty, 2019)\nfocus on mitigating this robustness issue. However,\nthey still rely on one critical assumption which\nleads them to degraded performance for distant\nlanguage pairs: they assume approximate isomor-\nphism (Barone, 2016; Søgaard et al., 2018) between\nmonolingual embedding spaces to learn the ini-\ntial seed dictionary. In other words, they assume\nvery similar geometric constellations between two\nmonolingual spaces: due to the Zipﬁan phenomena\nin language (Zipf, 1949) such near-isomorphism\ncan be satisﬁed only for similar languages and\nfor similar domains used for training monolingual\nvectors. This property is reﬂected in the results\nreported in Table 3, the number of unsuccessful\nsetups in Table 4, as well as later in Figure 4.\nFor instance, the largest number of unsuccess-\nful BLI setups with the UNSUPERVISED model is\nreported for Korean, Thai (a tonal language), and\nBasque (a language isolate): their morphological\nand genealogical properties are furthest away from\nother languages in our comparison. A substantial\nnumber of unsuccessful setups is also observed\nwith other two language outliers from our set (see\nTable 2 again), Georgian and Indonesian, as well\nas with morphologically-rich languages such as\nEstonian or Turkish.\nOne setting in which fully UNSUPERVISED meth-\nods did show impressive results in prior work are\nsimilar language pairs. However, even in these\nsettings when the comparison to the weakly super-\nvised FULL-SUPER+SYM is completely fair (i.e.,\nBG-EU\nEU-TR\nFI-KO\nID-ET\nID-TH\nKA-FI\nKA-ID\nKO-TR\nNO-EU\nTR-TH\nUNSUPERVISED\n0.005\n0.000\n0.000\n0.000\n0.000\n0.004\n0.000\n0.000\n0.000\n0.000\n1K:FULL+SL+SYM\n0.279\n0.212\n0.211\n0.213\n0.226\n0.306\n0.155\n0.279\n0.300\n0.137\n500:FULL+SL+SYM\n0.245\n0.189\n0.192\n0.195\n0.188\n0.285\n0.138\n0.264\n0.266\n0.109\nTable 5: Results for a selection of BLI setups which were unsuccessful with the UNSUPERVIED CLWE method.\n→Spanish\n→Italian\n→Dutch\nSource: English\n0.45\n0.50\n0.55\n0.60\n0.65\nBLI Score (MRR)\nUNSUPER\n|D0|=200\n|D0|=500\n(a) English→L2\n→Catalan\n→Portuguese\n→Italian\nSource: Spanish\n0.45\n0.50\n0.55\n0.60\n0.65\nBLI Score (MRR)\nUNSUPER\n|D0|=200\n|D0|=500\n(b) Spanish→L2\n→Danish\n→Norwegian\n→Dutch\nSource: Swedish\n0.45\n0.50\n0.55\n0.60\n0.65\nBLI Score (MRR)\nUNSUPER\n|D0|=200\n|D0|=500\n(c) Swedish→L2\nFigure 3: A comparison of BLI scores on “easy” (i.e., similar) language pairs between the fully UNSUPERVISED\nmodel and a weakly supervised model (seed dictionary size |D0| = 200 or |D0| = 500) which relies on the\nself-learning procedure with the symmetry constraint (FULL+SL+SYM).\nEN:EP\nEN:Wiki\nEN:EMEA\nDomain (English)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nBLI Scores (MRR)\n0.43\n0.0\n0.0\n0.0\n0.36\n0.0\n0.0\n0.0\n0.37\nES:EP\nES:Wiki\nES:EMEA\n(a) English→Spanish\nEN:EP\nEN:Wiki\nEN:EMEA\nDomain (English)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nBLI Scores (MRR)\n0.02\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.02\nFI:EP\nFI:Wiki\nFI:EMEA\n(b) English →Finnish\nEN:EP\nEN:Wiki\nEN:EMEA\nDomain (English)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nBLI Scores (MRR)\n0.03\n0.0\n0.0\n0.0\n0.02\n0.0\n0.0\n0.0\n0.01\nHU:EP\nHU:Wiki\nHU:EMEA\n(c) English →Hungarian\nFigure 4: BLI scores with the (most robust) fully UNSUPERVISED model for different language pairs when mono-\nlingual word embeddings are trained on dissimilar domains: parliamentary proceedings (EuroParl), Wikipedia\n(Wiki), and medical corpora (EMEA). Training and test data are the same as in (Søgaard et al., 2018).\nthe same components C2 and C3 are used for\nboth), UNSUPERVISED still falls short of FULL-\nSUPER+SYM. These results for three source lan-\nguages are summarized in Figure 3. What is more,\none could argue that we do not need unsupervised\nCLWEs for similar languages in the ﬁrst place:\nwe can harvest cheap supervision here, e.g., cog-\nnates. The main motivation behind unsupervised\napproaches is to support dissimilar and resource-\npoor language pairs for which supervision cannot\nbe guaranteed.\nDomain Differences. Finally, we also verify that\nUNSUPERVISED CLWEs still cannot account for\ndomain differences when training monolingual vec-\ntors. We rely on the probing test of Søgaard et al.\n(2018): 300-dim fastText vectors are trained on\n1.1M sentences on three corpora: 1) EuroParl.v7\n(Koehn, 2005) (parliamentary proceedings); 2)\nWikipedia (Al-Rfou et al., 2013), and 3) EMEA\n(Tiedemann, 2009) (medical), and BLI evaluation\nfor three language pairs is conducted on standard\nMUSE BLI test sets (Conneau et al., 2018a). The\nresults, summarized in Figure 4, reveal that UN-\nSUPERVISED methods are able to yield a good\nsolution only when there is no domain mismatch\nand for the pair with two most similar languages\n(English-Spanish), again questioning their robust-\nness and portability to truly low-resource and more\nchallenging setups. Weakly supervised methods\n(|D0| = 500 or D0 seeded with identical strings),\nin contrast, yield good solutions for all setups.\n5\nFurther Discussion and Conclusion\nThe superiority of weakly supervised methods\n(e.g., FULL+SL+SYM) over unsupervised methods\nis especially pronounced for distant and typologi-\ncally heterogeneous language pairs. However, our\nstudy also indicates that even carefully engineered\nprojection-based methods with some seed super-\nvision yield lower absolute performance for such\npairs. While we have witnessed the proliferation of\nfully unsupervised CLWE models recently, some\nfundamental questions still remain. For instance,\nthe underlying assumption of all projection-based\nmethods (both supervised and unsupervised) is the\ntopological similarity between monolingual spaces,\nwhich is why standard simple linear projections\nresult in lower absolute BLI scores for distant pairs\n(see Table 4 and results in the supplemental ma-\nterial). Unsupervised approaches even exploit the\nassumption twice as their seed extraction is fully\nbased on the topological similarity.\nFuture work should move beyond the restrictive\nassumption by exploring new methods that can,\ne.g., 1) increase the isomorphism between mono-\nlingual spaces (Zhang et al., 2019) by distinguish-\ning between language-speciﬁc and language-pair-\ninvariant subspaces; 2) learn effective non-linear\nor multiple local projections between monolingual\nspaces similar to the preliminary work of Nakas-\nhole (2018); 3) similar to Vuli´c and Korhonen\n(2016) and Lubin et al. (2019) “denoisify” seed\nlexicons during the self-learning procedure. For\ninstance, keeping only mutual/symmetric nearest\nneighbour as in FULL+SL+SYM can be seen as a\nform of rudimentary denoisifying: it is indicative\nto see that the best overall performance in this work\nis reported with that model conﬁguration.\nFurther, the most important contributions of un-\nsupervised CLWE models are, in fact, the improved\nand more robust self-learning procedures (compo-\nnent C2) and technical enhancements (component\nC3). In this work we have demonstrated that these\ncomponents can be equally applied to weakly su-\npervised approaches: starting from a set of only\nseveral hundred pairs, they can guarantee consis-\ntently improved performance across the board. As\nthere is still no clear-cut use case scenario for un-\nsupervised CLWEs,8 instead of “going fully unsu-\npervised”, one pragmatic approach to widen the\nscope of CLWE learning and its application might\ninvest more effort into extracting at least some seed\nsupervision for a variety of language pairs (Artetxe\n8E.g., unsupervised CLWEs are fully substitutable with\nthe superior weakly supervised CLWEs in unsupervised NMT\narchitectures (Artetxe et al., 2018c; Lample et al., 2018a,b), or\nin domain adaptation systems (Ziser and Reichart, 2018) and\nfully unsupervised cross-lingual IR (Litschko et al., 2018).\net al., 2017). This ﬁnding aligns well with the on-\ngoing initiatives of the PanLex project (Kamholz\net al., 2014) and the ASJP database (Wichmann\net al., 2018), which aim to collate at least some\ntranslation pairs in most of the world’s languages.9\nFinally, this paper demonstrates that, in order to\nenable fair comparisons, future work on CLWEs\nshould focus on evaluating the CLWE methods’\nconstituent components (e.g, components C1-C3\nfrom this work) instead of full-blown composite\nsystems directly. One goal of the paper is to ac-\nknowledge that the work on fully unsupervised\nCLWE methods has indeed advanced state-of-the-\nart in cross-lingual word representation learning\nby offering new solutions also to weakly super-\nvised CLWE methods. However, the robustness\nproblems are still prominent with fully unsuper-\nvised CLWEs, and future work should invest more\ntime and effort into developing more robust and\nmore effective methods, e.g., by reaching beyond\nprojection-based methods towards joint approaches\n(Ruder et al., 2019; Ormazabal et al., 2019).\nAcknowledgments\nThis work is supported by the ERC Consolidator\nGrant LEXICAL: Lexical Acquisition Across Lan-\nguages (no 648909). The work of Goran Glavaš\nis supported by the Baden-Württemberg Stiftung\n(AGREE grant of the Eliteprogramm). Roi Re-\nichart is partially funded by ISF personal grants No.\n1625/18. We thank the three anonymous reviewers\nfor their encouraging comments and suggestions.\nReferences\nOliver Adams, Adam Makarucha, Graham Neubig,\nSteven Bird, and Trevor Cohn. 2017. Cross-lingual\nword embeddings for low-resource language model-\ning. In Proceedings of EACL, pages 937–947.\nRami Al-Rfou, Bryan Perozzi, and Steven Skiena.\n2013.\nPolyglot: Distributed word representations\nfor multilingual NLP.\nIn Proceedings of CoNLL,\npages 183–192.\nJean Alaux, Edouard Grave, Marco Cuturi, and Ar-\nmand Joulin. 2019. Unsupervised hyperalignment\nfor multilingual word embeddings. In Proceedings\nof ICLR.\nDavid Alvarez-Melis and Tommi Jaakkola. 2018.\nGromov-Wasserstein alignment of word embedding\nspaces.\nIn Proceedings of EMNLP, pages 1881–\n1890.\n9https://asjp.clld.org/\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.\nLearning bilingual word embeddings with (almost)\nno bilingual data.\nIn Proceedings of ACL, pages\n451–462.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre.\n2018a. Generalizing and improving bilingual word\nembedding mappings with a multi-step framework\nof linear transformations. In Proceedings of AAAI,\npages 5012–5019.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre.\n2018b. A robust self-learning method for fully un-\nsupervised cross-lingual mappings of word embed-\ndings. In Proceedings of ACL, pages 789–798.\nMikel Artetxe, Gorka Labaka, Eneko Agirre, and\nKyunghyun Cho. 2018c. Unsupervised neural ma-\nchine translation. In Proceedings of ICLR.\nTimothy Baldwin, Jonathan Pool, and Susan Colow-\nick. 2010. PanLex and LEXTRACT: Translating all\nwords of all languages of the world. In Proceedings\nof COLING (Demo Papers), pages 37–40.\nAntonio Valerio Miceli Barone. 2016. Towards cross-\nlingual distributed representations without parallel\ntext trained with adversarial autoencoders. In Pro-\nceedings of the 1st Workshop on Representation\nLearning for NLP, pages 121–126.\nAnthony Bell and Terrence Sejnowski. 1997. The ’In-\ndependent Components’ of Natural Scenes are Edge\nFilters. Vision Research.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information.\nTransactions of the ACL,\n5:135–146.\nDanqi Chen and Christopher Manning. 2014. A fast\nand accurate dependency parser using neural net-\nworks. In Proceedings of EMNLP, pages 740–750.\nXilun Chen and Claire Cardie. 2018.\nUnsupervised\nmultilingual word embeddings. In Proceedings of\nEMNLP, pages 261–270.\nAlexis Conneau, Guillaume Lample, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Hervé Jégou. 2018a.\nWord translation without parallel data. In Proceed-\nings of ICLR.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018b.\nXNLI: Evaluating\ncross-lingual sentence representations. In Proceed-\nings of EMNLP, pages 2475–2485.\nRyan Cotterell, Sebastian J. Mielke, Jason Eisner, and\nBrian Roark. 2018. Are all languages equally hard\nto language-model?\nIn Proceedings of NAACL-\nHLT.\nZi-Yi Dou, Zhi-Hao Zhou, and Shujian Huang. 2018.\nUnsupervised bilingual lexicon induction via latent\nvariable models. In Proceedings of EMNLP, pages\n621–626.\nYerai Doval, Jose Camacho-Collados, Luis Espinosa-\nAnke, and Steven Schockaert. 2019.\nOn the\nrobustness of unsupervised and semi-supervised\ncross-lingual word embedding learning.\nCoRR,\nabs/1908.07742.\nLong Duong, Hiroshi Kanayama, Tengfei Ma, Steven\nBird, and Trevor Cohn. 2016. Learning crosslingual\nword embeddings without bilingual corpora. In Pro-\nceedings of EMNLP, pages 1285–1295.\nManaal Faruqui and Chris Dyer. 2014. Improving vec-\ntor space word representations using multilingual\ncorrelation.\nIn Proceedings of EACL, pages 462–\n471.\nDaniela Gerz, Ivan Vuli´c, Edoardo Maria Ponti, Roi\nReichart, and Anna Korhonen. 2018. On the rela-\ntion between linguistic typology and (limitations of)\nmultilingual language modeling. In Proceedings of\nEMNLP, pages 316–327.\nGoran Glavaš,\nMarc Franco-Salvador,\nSimone P\nPonzetto, and Paolo Rosso. 2018. A resource-light\nmethod for cross-lingual semantic textual similarity.\nKnowledge-Based Systems, 143:1–9.\nGoran Glavaš, Robert Litschko, Sebastian Ruder, and\nIvan Vuli´c. 2019. How to (properly) evaluate cross-\nlingual word embeddings: On strong baselines, com-\nparative analyses, and some misconceptions. In Pro-\nceedings of ACL, pages 710–721.\nStephan Gouws, Yoshua Bengio, and Greg Corrado.\n2015.\nBilBOWA: Fast bilingual distributed repre-\nsentations without word alignments. In Proceedings\nof ICML, pages 748–756.\nStephan Gouws and Anders Søgaard. 2015.\nSimple\ntask-speciﬁc bilingual word embeddings.\nIn Pro-\nceedings of NAACL-HLT, pages 1386–1390.\nEdouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-\nmand Joulin, and Tomas Mikolov. 2018. Learning\nword vectors for 157 languages. In Proceedings of\nLREC, pages 3483–3487.\nJiang Guo, Wanxiang Che, David Yarowsky, Haifeng\nWang, and Ting Liu. 2015.\nCross-lingual depen-\ndency parsing based on distributed representations.\nIn Proceedings of ACL, pages 1234–1244.\nGeert Heyman, Bregt Verreet, Ivan Vuli´c, and Marie-\nFrancine Moens. 2019. Learning unsupervised mul-\ntilingual word embeddings with incremental multi-\nlingual hubs. In Proceedings of NAACL-HLT, pages\n1890–1902.\nGeert Heyman, Ivan Vuli´c, and Marie-Francine Moens.\n2017.\nBilingual lexicon induction by learning to\ncombine word-level and character-level representa-\ntions. In Proceedings of EACL, pages 1085–1095.\nYedid Hoshen and Lior Wolf. 2018. Non-adversarial\nunsupervised word translation.\nIn Proceedings of\nEMNLP, pages 469–478.\nDavid Kamholz, Jonathan Pool, and Susan M. Colow-\nick. 2014. Panlex: Building a resource for panlin-\ngual lexical translation.\nIn Proceedings of LREC,\npages 3145–3150.\nYova Kementchedjhieva, Sebastian Ruder, Ryan Cot-\nterell, and Anders Søgaard. 2018. Generalizing Pro-\ncrustes analysis for better bilingual dictionary induc-\ntion. In Proceedings of CoNLL, pages 211–220.\nYunsu Kim, Jiahui Geng, and Hermann Ney. 2018.\nImproving unsupervised word-by-word translation\nwith language model and denoising autoencoder. In\nProceedings of EMNLP, pages 862–868.\nAlexandre Klementiev, Ivan Titov, and Binod Bhattarai.\n2012.\nInducing crosslingual distributed represen-\ntations of words. Proceedings of COLING, pages\n1459–1474.\nPhilipp Koehn. 2005. Europarl: A parallel corpus for\nstatistical machine translation. In Proceedings of the\n10th Machine Translation Summit (MT SUMMIT),\npages 79–86.\nGuillaume Lample, Alexis Conneau, Ludovic Denoyer,\nand Marc’Aurelio Ranzato. 2018a.\nUnsupervised\nmachine translation using monolingual corpora only.\nIn Proceedings of ICLR.\nGuillaume Lample, Myle Ott, Alexis Conneau, Lu-\ndovic Denoyer, and Marc’Aurelio Ranzato. 2018b.\nPhrase-based & neural unsupervised machine trans-\nlation.\nIn Proceedings of EMNLP, pages 5039–\n5049.\nRobert Litschko, Goran Glavaš, Ivan Vulic, and Laura\nDietz. 2019. Evaluating resource-lean cross-lingual\nembedding models in unsupervised retrieval. In Pro-\nceedings of SIGIR, pages 1109–1112.\nRobert\nLitschko,\nGoran\nGlavaš,\nSimone\nPaolo\nPonzetto, and Ivan Vuli´c. 2018. Unsupervised cross-\nlingual information retrieval using monolingual data\nonly. In Proceedings of SIGIR, pages 1253–1256.\nNoa Yehezkel Lubin, Jacob Goldberger, and Yoav\nGoldberg. 2019. Aligning vector-spaces with noisy\nsupervised lexicon. In Proceedings of NAACL-HLT,\npages 460–465.\nTomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013a.\nExploiting similarities among languages for ma-\nchine translation. CoRR, abs/1309.4168.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S\nCorrado, and Jeffrey Dean. 2013b. Distributed Rep-\nresentations of Words and Phrases and their Com-\npositionality. In Proceedings of NIPS, pages 3111–\n3119.\nTasnim Mohiuddin and Shaﬁq Joty. 2019. Revisiting\nadversarial autoencoder for unsupervised word trans-\nlation with cycle consistency and improved training.\nIn Proceedings of NAACL-HLT, pages 3857–3867.\nNdapa Nakashole. 2018. NORMA: Neighborhood sen-\nsitive maps for multilingual word embeddings. In\nProceedings of EMNLP, pages 512–522.\nAitor Ormazabal, Mikel Artetxe, Gorka Labaka, Aitor\nSoroa, and Eneko Agirre. 2019. Analyzing the lim-\nitations of cross-lingual word embedding mappings.\nIn Proceedings of ACL, pages 4990–4995.\nSebastian Ruder, Ivan Vuli´c, and Anders Søgaard.\n2019. A survey of cross-lingual word embedding\nmodels. Journal of Artiﬁcial Intelligence Research,\n65:569–631.\nSamuel L. Smith, David H.P. Turban, Steven Hamblin,\nand Nils Y. Hammerla. 2017. Ofﬂine bilingual word\nvectors, orthogonal transformations and the inverted\nsoftmax. In Proceedings of ICLR.\nAnders Søgaard, Željko Agi´c, Héctor Martínez Alonso,\nBarbara Plank, Bernd Bohnet, and Anders Jo-\nhannsen. 2015. Inverted indexing for cross-lingual\nNLP. In Proceedings of ACL, pages 1713–1722.\nAnders Søgaard, Sebastian Ruder, and Ivan Vuli´c.\n2018. On the limitations of unsupervised bilingual\ndictionary induction. In Proceedings of ACL, pages\n778–788.\nJörg Tiedemann. 2009. News from OPUS - A collec-\ntion of multilingual parallel corpora with tools and\ninterfaces. In Proceedings of RANLP, pages 237–\n248.\nJoseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.\n2010. Word representations: A simple and general\nmethod for semi-supervised learning. In Proceed-\nings of ACL, pages 384–394.\nIvan Vuli´c and Anna Korhonen. 2016. On the role of\nseed lexicons in learning bilingual word embeddings.\nIn Proceedings of ACL, pages 247–257.\nIvan Vuli´c and Marie-Francine Moens. 2015. Monolin-\ngual and cross-lingual information retrieval models\nbased on (bilingual) word embeddings. In Proceed-\nings of SIGIR, pages 363–372.\nIvan Vuli´c, Nikola Mrkši´c, and Anna Korhonen. 2017.\nCross-lingual induction and transfer of verb classes\nbased on word vector space specialisation. In Pro-\nceedings of EMNLP, pages 2536–2548.\nSøren Wichmann, André Müller, Viveka Velupillai, Ce-\ncil H Brown, Eric W Holman, Pamela Brown, Sebas-\ntian Sauppe, Oleg Belyaev, Matthias Urban, Zarina\nMolochieva, et al. 2018. The ASJP database (ver-\nsion 18).\nChao Xing, Dong Wang, Chao Liu, and Yiye Lin. 2015.\nNormalized word embedding and orthogonal trans-\nform for bilingual word translation. In Proceedings\nof NAACL-HLT, pages 1006–1011.\nMozhi Zhang, Keyulu Xu, Ken-ichi Kawarabayashi,\nStefanie Jegelka, and Jordan Boyd-Graber. 2019.\nAre girls neko or sh¯ojo? Cross-lingual alignment of\nnon-isomorphic embeddings with iterative normal-\nization. In Proceedings of ACL, pages 3180–3189.\nYuan Zhang, David Gaddy, Regina Barzilay, and\nTommi Jaakkola. 2016.\nTen pairs to tag – Multi-\nlingual POS tagging via coarse mapping between\nembeddings. In Proceedings of NAACL-HLT, pages\n1307–1317.\nGeorge Kingsley Zipf. 1949. Human behavior and the\nprinciple of least effort: An introduction to human\necology.\nYftah Ziser and Roi Reichart. 2018. Deep pivot-based\nmodeling for cross-language cross-domain transfer\nwith minimal guidance. In Proceedings of EMNLP,\npages 238–249.\nA\nSupplemental Material\nWe report main BLI results for all 15 × 14 =\n210 language pairs in the supplemental material,\ngrouped by the source language, and for two dictio-\nnary sizes: |D0| = 1, 000 and |D0| = 500. The re-\nsults are provided in Table 6–Table 20. As stressed\nin the table captions, for the descriptions of bench-\nmarked CLWE conﬁgurations, we refer the reader\nto the main paper: in particular to §2 and Table 1.\n(The actual tables start on the next page.)\nBulgarian: BG-\n-CA\n-EO\n-ET\n-EU\n-FI\n-HE\n-HU\n-ID\n-KA\n-KO\n-LT\n-NO\n-TH\n-TR\nUNSUPERVISED\n.353\n.147\n.262\n.005\n.096\n.262\n.386\n.270\n.259\n.004\n.247\n.368\n.001\n.254\n5K:ORTHG-SUPER\n.350\n.182\n.315\n.173\n.292\n.251\n.379\n.242\n.261\n.125\n.358\n.300\n.089\n.297\n5K:ORTHG+SL+SYM\n.380\n.215\n.327\n.179\n.319\n.277\n.402\n.271\n.286\n.142\n.372\n.341\n.105\n.321\n5K:FULL-SUPER\n.432\n.327\n.407\n.250\n.357\n.361\n.460\n.283\n.364\n.205\n.445\n.398\n.169\n.349\n5K:FULL+SL\n.383\n.273\n.297\n.198\n.203\n.279\n.394\n.277\n.268\n.136\n.301\n.388\n.102\n.291\n5K:FULL+SL+NOD\n.427\n.357\n.350\n.245\n.272\n.317\n.439\n.305\n.292\n.179\n.339\n.422\n.154\n.326\n5K:FULL+SL+SYM\n.456\n.370\n.405\n.296\n.374\n.368\n.475\n.325\n.367\n.215\n.407\n.446\n.179\n.374\n1K:ORTHG-SUPER\n.166\n.060\n.141\n.046\n.108\n.076\n.178\n.111\n.098\n.038\n.183\n.122\n.023\n.111\n1K:ORTHG+SL+SYM\n.310\n.135\n.270\n.087\n.230\n.180\n.346\n.191\n.200\n.071\n.312\n.258\n.030\n.226\n1K:FULL-SUPER\n.229\n.147\n.211\n.070\n.129\n.112\n.254\n.116\n.157\n.054\n.230\n.163\n.044\n.133\n1K:FULL+SL\n.381\n.275\n.291\n.198\n.211\n.280\n.392\n.270\n.275\n.127\n.303\n.365\n.101\n.285\n1K:FULL+SL+NOD\n.426\n.335\n.347\n.241\n.272\n.315\n.437\n.308\n.306\n.168\n.331\n.413\n.143\n.324\n1K:FULL+SL+SYM\n.444\n.357\n.388\n.279\n.361\n.345\n.467\n.314\n.333\n.186\n.369\n.441\n.128\n.357\nTable 6: All BLI scores (MRR) with Bulgarian (BG) as the source language. 5k and 1k denote the seed dictionary\nD0 size for (weakly) supervised methods. See Table 1 for a brief description of each model conﬁguration.\nCatalan: CA-\n-BG\n-EO\n-ET\n-EU\n-FI\n-HE\n-HU\n-ID\n-KA\n-KO\n-LT\n-NO\n-TH\n-TR\nUNSUPERVISED\n.330\n.310\n.198\n.258\n.228\n.239\n.334\n.302\n.177\n.002\n.163\n.347\n.000\n.260\n5K:ORTHG-SUPER\n.315\n.199\n.244\n.219\n.252\n.227\n.360\n.249\n.199\n.130\n.258\n.294\n.093\n.283\n5K:ORTHG+SL+SYM\n.331\n.239\n.269\n.244\n.288\n.258\n.390\n.280\n.213\n.149\n.282\n.340\n.108\n.301\n5K:FULL-SUPER\n.396\n.395\n.356\n.338\n.329\n.336\n.431\n.286\n.309\n.217\n.366\n.396\n.196\n.337\n5K:FULL+SL\n.331\n.357\n.223\n.289\n.214\n.257\n.368\n.299\n.174\n.128\n.215\n.366\n.171\n.275\n5K:FULL+SL+NOD\n.372\n.458\n.276\n.347\n.265\n.290\n.411\n.317\n.224\n.180\n.264\n.416\n.223\n.308\n5K:FULL+SL+SYM\n.414\n.456\n.352\n.391\n.356\n.357\n.449\n.322\n.302\n.245\n.343\n.433\n.218\n.348\n1K:ORTHG-SUPER\n.160\n.075\n.089\n.063\n.098\n.064\n.151\n.117\n.052\n.029\n.098\n.101\n.018\n.116\n1K:ORTHG+SL+SYM\n.274\n.151\n.174\n.126\n.168\n.143\n.298\n.207\n.096\n.047\n.198\n.235\n.025\n.193\n1K:FULL-SUPER\n.212\n.167\n.165\n.116\n.110\n.103\n.210\n.126\n.101\n.046\n.144\n.138\n.035\n.133\n1K:FULL+SL\n.329\n.361\n.223\n.282\n.218\n.252\n.366\n.299\n.166\n.129\n.225\n.358\n.162\n.272\n1K:FULL+SL+NOD\n.372\n.457\n.274\n.342\n.263\n.289\n.411\n.316\n.213\n.170\n.256\n.416\n.217\n.308\n1K:FULL+SL+SYM\n.395\n.446\n.300\n.370\n.319\n.335\n.435\n.320\n.253\n.202\n.295\n.424\n.142\n.334\nTable 7: All BLI scores (MRR) with Catalan (CA) as the source language. 5k and 1k denote the seed dictionary\nD0 size for (weakly) supervised methods. See Table 1 for a brief description of each model conﬁguration.\nEsperanto: EO-\n-BG\n-CA\n-ET\n-EU\n-FI\n-HE\n-HU\n-ID\n-KA\n-KO\n-LT\n-NO\n-TH\n-TR\nUNSUPERVISED\n.164\n.430\n.092\n.147\n.012\n.157\n.040\n.047\n.177\n.000\n.198\n.322\n.000\n.001\n5K:ORTHG-SUPER\n.247\n.367\n.209\n.169\n.246\n.138\n.319\n.171\n.165\n.082\n.221\n.225\n.046\n.210\n5K:ORTHG+SL+SYM\n.268\n.401\n.227\n.182\n.264\n.160\n.343\n.189\n.179\n.087\n.246\n.254\n.051\n.220\n5K:FULL-SUPER\n.367\n.491\n.334\n.294\n.329\n.258\n.400\n.267\n.281\n.171\n.343\n.337\n.107\n.285\n5K:FULL+SL\n.281\n.486\n.225\n.232\n.237\n.187\n.360\n.270\n.171\n.065\n.215\n.340\n.068\n.220\n5K:FULL+SL+NOD\n.367\n.524\n.284\n.273\n.322\n.223\n.409\n.321\n.210\n.161\n.292\n.385\n.103\n.262\n5K:FULL+SL+SYM\n.410\n.533\n.342\n.354\n.363\n.288\n.426\n.315\n.296\n.184\n.384\n.390\n.117\n.299\n1K:ORTHG-SUPER\n.103\n.161\n.064\n.043\n.065\n.022\n.107\n.072\n.038\n.015\n.075\n.061\n.007\n.074\n1K:ORTHG+SL+SYM\n.163\n.266\n.112\n.074\n.105\n.050\n.186\n.104\n.064\n.021\n.127\n.108\n.008\n.103\n1K:FULL-SUPER\n.152\n.221\n.136\n.083\n.080\n.044\n.145\n.099\n.078\n.024\n.120\n.083\n.017\n.087\n1K:FULL+SL\n.289\n.491\n.216\n.250\n.229\n.158\n.357\n.262\n.167\n.065\n.215\n.344\n.066\n.223\n1K:FULL+SL+NOD\n.371\n.511\n.272\n.264\n.304\n.194\n.407\n.313\n.205\n.142\n.274\n.383\n.080\n.257\n1K:FULL+SL+SYM\n.385\n.521\n.314\n.315\n.328\n.241\n.411\n.298\n.255\n.111\n.358\n.376\n.056\n.259\nTable 8: All BLI scores (MRR) with Esperanto (EO) as the source language. 5k and 1k denote the seed dictionary\nD0 size for (weakly) supervised methods. See Table 1 for a brief description of each model conﬁguration.\nEstonian: ET-\n-BG\n-CA\n-EO\n-EU\n-FI\n-HE\n-HU\n-ID\n-KA\n-KO\n-LT\n-NO\n-TH\n-TR\nUNSUPERVISED\n.283\n.220\n.105\n.004\n.370\n.208\n.377\n.000\n.002\n.001\n.282\n.316\n.000\n.002\n5K:ORTHG-SUPER\n.295\n.212\n.142\n.171\n.368\n.192\n.342\n.120\n.185\n.112\n.277\n.210\n.076\n.241\n5K:ORTHG+SL+SYM\n.301\n.234\n.160\n.172\n.388\n.209\n.365\n.136\n.192\n.121\n.286\n.246\n.089\n.255\n5K:FULL-SUPER\n.393\n.333\n.271\n.238\n.430\n.287\n.432\n.212\n.258\n.191\n.360\n.328\n.168\n.307\n5K:FULL+SL\n.309\n.258\n.217\n.128\n.393\n.221\n.382\n.160\n.183\n.114\n.286\n.320\n.090\n.249\n5K:FULL+SL+NOD\n.354\n.308\n.268\n.181\n.397\n.268\n.418\n.210\n.208\n.152\n.319\n.351\n.150\n.277\n5K:FULL+SL+SYM\n.404\n.357\n.307\n.238\n.443\n.301\n.459\n.223\n.251\n.185\n.358\n.383\n.178\n.331\n1K:ORTHG-SUPER\n.135\n.080\n.060\n.074\n.171\n.050\n.145\n.034\n.067\n.033\n.131\n.069\n.025\n.073\n1K:ORTHG+SL+SYM\n.238\n.153\n.096\n.108\n.347\n.124\n.307\n.075\n.121\n.060\n.228\n.158\n.033\n.152\n1K:FULL-SUPER\n.200\n.121\n.116\n.099\n.200\n.069\n.188\n.065\n.095\n.052\n.179\n.112\n.041\n.102\n1K:FULL+SL\n.289\n.240\n.221\n.127\n.395\n.211\n.380\n.163\n.183\n.117\n.286\n.315\n.074\n.248\n1K:FULL+SL+NOD\n.343\n.294\n.267\n.193\n.396\n.259\n.420\n.209\n.204\n.147\n.316\n.352\n.123\n.290\n1K:FULL+SL+SYM\n.381\n.346\n.297\n.208\n.437\n.277\n.449\n.204\n.215\n.148\n.337\n.377\n.108\n.313\nTable 9: All BLI scores (MRR) with Estonian (ET) as the source language. 5k and 1k denote the seed dictionary\nD0 size for (weakly) supervised methods. See Table 1 for a brief description of each model conﬁguration.\nBasque: EU-\n-BG\n-CA\n-EO\n-ET\n-FI\n-HE\n-HU\n-ID\n-KA\n-KO\n-LT\n-NO\n-TH\n-TR\nUNSUPERVISED\n.002\n.254\n.115\n.007\n.008\n.048\n.001\n.001\n.059\n.000\n.001\n.000\n.000\n.000\n5K:ORTHG-SUPER\n.190\n.279\n.122\n.174\n.155\n.134\n.178\n.134\n.154\n.069\n.159\n.138\n.045\n.176\n5K:ORTHG+SL+SYM\n.196\n.307\n.135\n.181\n.175\n.148\n.193\n.147\n.165\n.076\n.175\n.155\n.049\n.187\n5K:FULL-SUPER\n.292\n.391\n.245\n.250\n.233\n.211\n.259\n.183\n.197\n.109\n.242\n.240\n.095\n.240\n5K:FULL+SL\n.203\n.371\n.203\n.126\n.175\n.110\n.187\n.209\n.084\n.043\n.098\n.201\n.069\n.170\n5K:FULL+SL+NOD\n.248\n.423\n.243\n.181\n.219\n.163\n.228\n.244\n.121\n.081\n.148\n.252\n.093\n.208\n5K:FULL+SL+SYM\n.310\n.441\n.277\n.248\n.270\n.206\n.283\n.225\n.189\n.106\n.237\n.287\n.094\n.248\n1K:ORTHG-SUPER\n.090\n.117\n.038\n.061\n.045\n.029\n.054\n.044\n.051\n.019\n.052\n.037\n.012\n.044\n1K:ORTHG+SL+SYM\n.116\n.192\n.061\n.105\n.072\n.052\n.102\n.062\n.080\n.029\n.086\n.066\n.013\n.077\n1K:FULL-SUPER\n.120\n.142\n.077\n.088\n.048\n.037\n.077\n.049\n.059\n.021\n.071\n.053\n.018\n.055\n1K:FULL+SL\n.206\n.348\n.207\n.117\n.172\n.115\n.186\n.209\n.080\n.039\n.101\n.201\n.072\n.159\n1K:FULL+SL+NOD\n.244\n.418\n.230\n.179\n.214\n.163\n.225\n.236\n.111\n.073\n.136\n.248\n.077\n.202\n1K:FULL+SL+SYM\n.276\n.428\n.253\n.213\n.247\n.166\n.266\n.213\n.147\n.060\n.169\n.261\n.056\n.212\nTable 10: All BLI scores (MRR) with Basque (EU) as the source language. 5k and 1k denote the seed dictionary\nD0 size for (weakly) supervised methods. See Table 1 for a brief description of each model conﬁguration.\nFinnish: FI-\n-BG\n-CA\n-EO\n-ET\n-EU\n-HE\n-HU\n-ID\n-KA\n-KO\n-LT\n-NO\n-TH\n-TR\nUNSUPERVISED\n.120\n.304\n.013\n.361\n.004\n.243\n.411\n.230\n.003\n.000\n.284\n.395\n.000\n.162\n5K:ORTHG-SUPER\n.292\n.295\n.132\n.302\n.116\n.216\n.407\n.221\n.166\n.132\n.265\n.332\n.082\n.300\n5K:ORTHG+SL+SYM\n.311\n.326\n.147\n.336\n.130\n.245\n.428\n.243\n.185\n.150\n.292\n.370\n.089\n.325\n5K:FULL-SUPER\n.379\n.377\n.284\n.409\n.220\n.323\n.456\n.263\n.275\n.222\n.390\n.419\n.171\n.346\n5K:FULL+SL\n.269\n.325\n.216\n.360\n.153\n.245\n.408\n.260\n.189\n.136\n.280\n.404\n.093\n.303\n5K:FULL+SL+NOD\n.353\n.388\n.299\n.401\n.205\n.311\n.453\n.296\n.251\n.196\n.372\n.449\n.165\n.335\n5K:FULL+SL+SYM\n.397\n.404\n.320\n.424\n.271\n.351\n.474\n.298\n.289\n.243\n.405\n.460\n.168\n.365\n1K:ORTHG-SUPER\n.134\n.114\n.033\n.105\n.030\n.060\n.195\n.091\n.047\n.035\n.113\n.140\n.014\n.128\n1K:ORTHG+SL+SYM\n.246\n.198\n.049\n.275\n.047\n.129\n.368\n.141\n.086\n.059\n.228\n.285\n.017\n.230\n1K:FULL-SUPER\n.174\n.142\n.077\n.167\n.054\n.071\n.226\n.098\n.084\n.052\n.158\n.161\n.028\n.149\n1K:FULL+SL\n.258\n.276\n.222\n.361\n.154\n.253\n.400\n.255\n.199\n.129\n.309\n.402\n.065\n.309\n1K:FULL+SL+NOD\n.351\n.330\n.298\n.400\n.193\n.309\n.452\n.299\n.246\n.183\n.372\n.448\n.141\n.334\n1K:FULL+SL+SYM\n.381\n.396\n.304\n.416\n.235\n.331\n.463\n.300\n.270\n.211\n.389\n.455\n.107\n.353\nTable 11: All BLI scores (MRR) with Finnish (FI) as the source language. 5k and 1k denote the seed dictionary\nD0 size for (weakly) supervised methods. See Table 1 for a brief description of each model conﬁguration.\nHebrew: HE-\n-BG\n-CA\n-EO\n-ET\n-EU\n-FI\n-HU\n-ID\n-KA\n-KO\n-LT\n-NO\n-TH\n-TR\nUNSUPERVISED\n.297\n.319\n.172\n.180\n.101\n.214\n.318\n.212\n.123\n.004\n.106\n.292\n.000\n.262\n5K:ORTHG-SUPER\n.288\n.269\n.135\n.193\n.160\n.250\n.287\n.154\n.161\n.126\n.214\n.205\n.086\n.247\n5K:ORTHG+SL+SYM\n.307\n.298\n.149\n.206\n.161\n.279\n.315\n.181\n.167\n.135\n.229\n.238\n.107\n.274\n5K:FULL-SUPER\n.397\n.376\n.248\n.288\n.225\n.329\n.375\n.239\n.213\n.204\n.309\n.316\n.173\n.328\n5K:FULL+SL\n.304\n.316\n.193\n.187\n.137\n.223\n.314\n.212\n.142\n.142\n.193\n.284\n.124\n.267\n5K:FULL+SL+NOD\n.343\n.356\n.241\n.230\n.171\n.269\n.361\n.250\n.179\n.185\n.237\n.323\n.190\n.298\n5K:FULL+SL+SYM\n.378\n.384\n.278\n.278\n.211\n.320\n.393\n.266\n.217\n.218\n.301\n.349\n.192\n.337\n1K:ORTHG-SUPER\n.126\n.101\n.040\n.064\n.050\n.068\n.096\n.057\n.048\n.033\n.078\n.058\n.022\n.086\n1K:ORTHG+SL+SYM\n.243\n.204\n.079\n.134\n.097\n.144\n.199\n.116\n.098\n.061\n.154\n.130\n.031\n.170\n1K:FULL-SUPER\n.180\n.148\n.087\n.106\n.065\n.077\n.135\n.076\n.067\n.054\n.105\n.086\n.042\n.111\n1K:FULL+SL\n.309\n.317\n.195\n.191\n.141\n.226\n.319\n.209\n.138\n.140\n.189\n.288\n.105\n.271\n1K:FULL+SL+NOD\n.345\n.353\n.235\n.226\n.162\n.268\n.360\n.250\n.176\n.183\n.232\n.317\n.159\n.304\n1K:FULL+SL+SYM\n.360\n.371\n.252\n.250\n.182\n.293\n.383\n.251\n.188\n.187\n.254\n.343\n.114\n.321\nTable 12: All BLI scores (MRR) with Hebrew (HE) as the source language. 5k and 1k denote the seed dictionary\nD0 size for (weakly) supervised methods. See Table 1 for a brief description of each model conﬁguration.\nHungarian: HU-\n-BG\n-CA\n-EO\n-ET\n-EU\n-FI\n-HE\n-ID\n-KA\n-KO\n-LT\n-NO\n-TH\n-TR\nUNSUPERVISED\n.328\n.362\n.012\n.298\n.001\n.342\n.275\n.318\n.027\n.000\n.232\n.359\n.000\n.330\n5K:ORTHG-SUPER\n.336\n.360\n.198\n.309\n.191\n.339\n.227\n.254\n.218\n.158\n.294\n.310\n.096\n.331\n5K:ORTHG+SL+SYM\n.357\n.391\n.222\n.341\n.204\n.372\n.262\n.277\n.237\n.184\n.326\n.345\n.105\n.348\n5K:FULL-SUPER\n.431\n.443\n.344\n.423\n.282\n.397\n.349\n.338\n.326\n.259\n.411\n.406\n.173\n.372\n5K:FULL+SL\n.348\n.402\n.302\n.325\n.205\n.343\n.275\n.318\n.218\n.187\n.246\n.361\n.087\n.338\n5K:FULL+SL+NOD\n.397\n.445\n.378\n.370\n.252\n.372\n.319\n.364\n.265\n.228\n.319\n.402\n.145\n.361\n5K:FULL+SL+SYM\n.438\n.477\n.392\n.433\n.305\n.407\n.376\n.374\n.332\n.285\n.419\n.441\n.176\n.380\n1K:ORTHG-SUPER\n.164\n.170\n.063\n.125\n.056\n.149\n.068\n.113\n.063\n.039\n.128\n.102\n.008\n.161\n1K:ORTHG+SL+SYM\n.297\n.298\n.107\n.255\n.091\n.293\n.152\n.200\n.129\n.076\n.237\n.243\n.016\n.287\n1K:FULL-SUPER\n.241\n.221\n.125\n.196\n.094\n.168\n.098\n.147\n.112\n.063\n.183\n.149\n.026\n.184\n1K:FULL+SL\n.337\n.403\n.292\n.312\n.217\n.341\n.256\n.317\n.219\n.186\n.248\n.360\n.086\n.334\n1K:FULL+SL+NOD\n.395\n.447\n.374\n.364\n.260\n.375\n.312\n.359\n.264\n.231\n.317\n.398\n.132\n.355\n1K:FULL+SL+SYM\n.427\n.467\n.369\n.413\n.274\n.400\n.356\n.377\n.306\n.268\n.381\n.423\n.113\n.374\nTable 13: All BLI scores (MRR) with Hungarian (HU) as the source language. 5k and 1k denote the seed dictionary\nD0 size for (weakly) supervised methods. See Table 1 for a brief description of each model conﬁguration.\nIndonesian: ID-\n-BG\n-CA\n-EO\n-ET\n-EU\n-FI\n-HE\n-HU\n-KA\n-KO\n-LT\n-NO\n-TH\n-TR\nUNSUPERVISED\n.132\n.271\n.005\n.000\n.001\n.174\n.190\n.256\n.001\n.001\n.000\n.257\n.001\n.252\n5K:ORTHG-SUPER\n.223\n.256\n.153\n.217\n.111\n.182\n.222\n.273\n.162\n.126\n.201\n.233\n.147\n.276\n5K:ORTHG+SL+SYM\n.237\n.278\n.167\n.228\n.123\n.219\n.239\n.294\n.170\n.140\n.207\n.265\n.163\n.294\n5K:FULL-SUPER\n.281\n.300\n.247\n.281\n.173\n.233\n.290\n.349\n.222\n.193\n.260\n.294\n.218\n.316\n5K:FULL+SL\n.156\n.273\n.170\n.126\n.132\n.171\n.201\n.256\n.099\n.126\n.073\n.258\n.215\n.265\n5K:FULL+SL+NOD\n.210\n.300\n.218\n.174\n.166\n.226\n.238\n.300\n.133\n.172\n.110\n.288\n.253\n.292\n5K:FULL+SL+SYM\n.287\n.323\n.274\n.266\n.220\n.269\n.295\n.345\n.200\n.197\n.242\n.320\n.241\n.326\n1K:ORTHG-SUPER\n.092\n.098\n.050\n.075\n.025\n.053\n.068\n.109\n.039\n.043\n.069\n.074\n.037\n.129\n1K:ORTHG+SL+SYM\n.150\n.168\n.078\n.142\n.037\n.082\n.140\n.197\n.071\n.078\n.108\n.142\n.052\n.225\n1K:FULL-SUPER\n.121\n.114\n.092\n.115\n.038\n.053\n.093\n.129\n.063\n.062\n.086\n.081\n.052\n.152\n1K:FULL+SL\n.149\n.267\n.170\n.132\n.134\n.171\n.198\n.257\n.101\n.123\n.073\n.257\n.217\n.266\n1K:FULL+SL+NOD\n.208\n.301\n.213\n.167\n.165\n.218\n.240\n.299\n.126\n.165\n.101\n.287\n.246\n.290\n1K:FULL+SL+SYM\n.258\n.316\n.254\n.213\n.187\n.250\n.264\n.337\n.140\n.175\n.152\n.309\n.226\n.319\nTable 14: All BLI scores (MRR) with Indonesian (ID) as the source language. 5k and 1k denote the seed dictionary\nD0 size for (weakly) supervised methods. See Table 1 for a brief description of each model conﬁguration.\nGeorgian: KA-\n-BG\n-CA\n-EO\n-ET\n-EU\n-FI\n-HE\n-HU\n-ID\n-KO\n-LT\n-NO\n-TH\n-TR\nUNSUPERVISED\n.211\n.205\n.180\n.003\n.096\n.007\n.166\n.200\n.000\n.000\n.237\n.179\n.000\n.001\n5K:ORTHG-SUPER\n.251\n.187\n.116\n.195\n.154\n.210\n.155\n.218\n.088\n.085\n.234\n.123\n.056\n.204\n5K:ORTHG+SL+SYM\n.262\n.205\n.130\n.206\n.165\n.233\n.167\n.245\n.095\n.092\n.255\n.138\n.061\n.221\n5K:FULL-SUPER\n.372\n.297\n.243\n.282\n.217\n.292\n.245\n.308\n.169\n.154\n.327\n.214\n.127\n.257\n5K:FULL+SL\n.310\n.233\n.182\n.225\n.133\n.252\n.172\n.258\n.130\n.069\n.248\n.188\n.069\n.203\n5K:FULL+SL+NOD\n.337\n.285\n.226\n.257\n.165\n.288\n.216\n.290\n.160\n.117\n.292\n.217\n.114\n.242\n5K:FULL+SL+SYM\n.376\n.320\n.265\n.293\n.216\n.318\n.251\n.326\n.172\n.143\n.340\n.253\n.139\n.275\n1K:ORTHG-SUPER\n.093\n.051\n.036\n.069\n.040\n.049\n.026\n.069\n.024\n.023\n.101\n.026\n.013\n.076\n1K:ORTHG+SL+SYM\n.166\n.094\n.060\n.133\n.066\n.097\n.064\n.147\n.044\n.040\n.188\n.048\n.014\n.122\n1K:FULL-SUPER\n.153\n.088\n.083\n.112\n.068\n.065\n.046\n.103\n.048\n.036\n.138\n.048\n.025\n.091\n1K:FULL+SL\n.234\n.231\n.182\n.225\n.136\n.245\n.175\n.258\n.135\n.077\n.245\n.189\n.060\n.201\n1K:FULL+SL+NOD\n.307\n.284\n.215\n.256\n.164\n.286\n.208\n.292\n.159\n.105\n.291\n.220\n.099\n.237\n1K:FULL+SL+SYM\n.352\n.305\n.248\n.271\n.172\n.306\n.213\n.308\n.155\n.103\n.317\n.238\n.077\n.255\nTable 15: All BLI scores (MRR) with Georgian (KA) as the source language. 5k and 1k denote the seed dictionary\nD0 size for (weakly) supervised methods. See Table 1 for a brief description of each model conﬁguration.\nKorean: KO-\n-BG\n-CA\n-EO\n-ET\n-EU\n-FI\n-HE\n-HU\n-ID\n-KA\n-LT\n-NO\n-TH\n-TR\nUNSUPERVISED\n.010\n.001\n.000\n.001\n.001\n.000\n.002\n.000\n.001\n.000\n.000\n.001\n.000\n.000\n5K:ORTHG-SUPER\n.190\n.183\n.083\n.145\n.102\n.206\n.166\n.238\n.142\n.112\n.156\n.150\n.076\n.213\n5K:ORTHG+SL+SYM\n.198\n.204\n.088\n.145\n.106\n.229\n.180\n.263\n.162\n.107\n.154\n.166\n.083\n.232\n5K:FULL-SUPER\n.289\n.283\n.176\n.242\n.170\n.273\n.257\n.326\n.210\n.178\n.241\n.256\n.174\n.278\n5K:FULL+SL\n.161\n.183\n.065\n.141\n.080\n.180\n.187\n.281\n.162\n.065\n.089\n.193\n.101\n.235\n5K:FULL+SL+NOD\n.240\n.262\n.135\n.185\n.114\n.243\n.240\n.320\n.213\n.120\n.138\n.258\n.161\n.270\n5K:FULL+SL+SYM\n.303\n.299\n.178\n.229\n.174\n.289\n.282\n.358\n.233\n.176\n.224\n.289\n.176\n.302\n1K:ORTHG-SUPER\n.052\n.055\n.025\n.031\n.033\n.053\n.026\n.063\n.043\n.024\n.036\n.033\n.014\n.068\n1K:ORTHG+SL+SYM\n.081\n.071\n.029\n.055\n.039\n.084\n.048\n.122\n.073\n.040\n.055\n.050\n.017\n.112\n1K:FULL-SUPER\n.093\n.078\n.045\n.059\n.045\n.066\n.048\n.096\n.060\n.039\n.053\n.047\n.038\n.085\n1K:FULL+SL\n.135\n.187\n.058\n.133\n.073\n.174\n.180\n.278\n.166\n.064\n.098\n.189\n.107\n.233\n1K:FULL+SL+NOD\n.227\n.255\n.115\n.179\n.112\n.231\n.223\n.315\n.209\n.111\n.129\n.246\n.134\n.266\n1K:FULL+SL+SYM\n.245\n.253\n.110\n.191\n.108\n.266\n.232\n.343\n.206\n.122\n.150\n.244\n.089\n.279\nTable 16: All BLI scores (MRR) with Korean (KO) as the source language. 5k and 1k denote the seed dictionary\nD0 size for (weakly) supervised methods. See Table 1 for a brief description of each model conﬁguration.\nLithuanian: LT-\n-BG\n-CA\n-EO\n-ET\n-EU\n-FI\n-HE\n-HU\n-ID\n-KA\n-KO\n-NO\n-TH\n-TR\nUNSUPERVISED\n.336\n.194\n.259\n.308\n.001\n.296\n.153\n.286\n.000\n.232\n.000\n.264\n.000\n.181\n5K:ORTHG-SUPER\n.328\n.224\n.145\n.274\n.165\n.258\n.182\n.288\n.099\n.193\n.093\n.185\n.066\n.220\n5K:ORTHG+SL+SYM\n.347\n.248\n.164\n.295\n.169\n.290\n.209\n.316\n.108\n.208\n.096\n.207\n.077\n.239\n5K:FULL-SUPER\n.462\n.353\n.317\n.394\n.236\n.368\n.299\n.395\n.184\n.284\n.168\n.304\n.162\n.296\n5K:FULL+SL\n.340\n.246\n.264\n.311\n.122\n.273\n.225\n.296\n.136\n.237\n.071\n.258\n.068\n.192\n5K:FULL+SL+NOD\n.381\n.323\n.323\n.353\n.161\n.361\n.280\n.361\n.171\n.277\n.115\n.304\n.146\n.245\n5K:FULL+SL+SYM\n.437\n.363\n.348\n.383\n.222\n.385\n.316\n.413\n.191\n.304\n.160\n.336\n.168\n.319\n1K:ORTHG-SUPER\n.160\n.080\n.035\n.110\n.047\n.103\n.040\n.115\n.031\n.065\n.021\n.050\n.018\n.069\n1K:ORTHG+SL+SYM\n.294\n.171\n.072\n.236\n.075\n.224\n.103\n.216\n.064\n.136\n.038\n.109\n.023\n.135\n1K:FULL-SUPER\n.256\n.138\n.102\n.190\n.085\n.143\n.073\n.159\n.058\n.097\n.040\n.081\n.030\n.097\n1K:FULL+SL\n.342\n.250\n.264\n.312\n.124\n.314\n.228\n.294\n.133\n.240\n.065\n.263\n.052\n.197\n1K:FULL+SL+NOD\n.384\n.319\n.325\n.353\n.154\n.363\n.280\n.358\n.168\n.277\n.107\n.305\n.126\n.245\n1K:FULL+SL+SYM\n.408\n.345\n.332\n.361\n.181\n.380\n.286\n.399\n.168\n.288\n.109\n.322\n.094\n.302\nTable 17: All BLI scores (MRR) with Lithuanian (LT) as the source language. 5k and 1k denote the seed dictionary\nD0 size for (weakly) supervised methods. See Table 1 for a brief description of each model conﬁguration.\nNorwegian: NO-\n-BG\n-CA\n-EO\n-ET\n-EU\n-FI\n-HE\n-HU\n-ID\n-KA\n-KO\n-LT\n-TH\n-TR\nUNSUPERVISED\n.326\n.380\n.302\n.296\n.001\n.348\n.256\n.388\n.319\n.207\n.000\n.234\n.000\n.291\n5K:ORTHG-SUPER\n.316\n.359\n.180\n.284\n.171\n.344\n.224\n.368\n.270\n.189\n.148\n.264\n.101\n.280\n5K:ORTHG+SL+SYM\n.334\n.390\n.207\n.310\n.189\n.378\n.247\n.401\n.300\n.213\n.168\n.277\n.111\n.301\n5K:FULL-SUPER\n.394\n.424\n.323\n.389\n.261\n.396\n.319\n.441\n.306\n.291\n.220\n.366\n.188\n.325\n5K:FULL+SL\n.329\n.390\n.309\n.303\n.200\n.355\n.258\n.389\n.315\n.196\n.141\n.240\n.127\n.292\n5K:FULL+SL+NOD\n.374\n.439\n.381\n.345\n.253\n.387\n.292\n.426\n.341\n.235\n.193\n.281\n.182\n.319\n5K:FULL+SL+SYM\n.422\n.457\n.377\n.395\n.328\n.419\n.353\n.452\n.340\n.298\n.250\n.351\n.197\n.341\n1K:ORTHG-SUPER\n.166\n.162\n.069\n.120\n.041\n.153\n.065\n.166\n.121\n.054\n.040\n.110\n.021\n.104\n1K:ORTHG+SL+SYM\n.284\n.302\n.125\n.233\n.078\n.307\n.136\n.323\n.222\n.111\n.064\n.198\n.028\n.191\n1K:FULL-SUPER\n.203\n.198\n.128\n.172\n.075\n.153\n.078\n.206\n.132\n.088\n.057\n.132\n.032\n.123\n1K:FULL+SL\n.329\n.389\n.311\n.303\n.193\n.357\n.258\n.389\n.317\n.195\n.147\n.236\n.125\n.293\n1K:FULL+SL+NOD\n.373\n.437\n.374\n.341\n.249\n.389\n.291\n.425\n.338\n.229\n.189\n.281\n.178\n.319\n1K:FULL+SL+SYM\n.411\n.444\n.374\n.371\n.300\n.412\n.336\n.443\n.339\n.268\n.228\n.315\n.140\n.332\nTable 18: All BLI scores (MRR) with Norwegian (NO) as the source language. 5k and 1k denote the seed dictionary\nD0 size for (weakly) supervised methods. See Table 1 for a brief description of each model conﬁguration.\nThai: TH-\n-BG\n-CA\n-EO\n-ET\n-EU\n-FI\n-HE\n-HU\n-ID\n-KA\n-KO\n-LT\n-NO\n-TR\nUNSUPERVISED\n.000\n.000\n.000\n.000\n.000\n.000\n.000\n.000\n.002\n.000\n.000\n.000\n.000\n.000\n5K:ORTHG-SUPER\n.151\n.090\n.066\n.119\n.074\n.124\n.123\n.123\n.112\n.086\n.086\n.142\n.115\n.119\n5K:ORTHG+SL+SYM\n.148\n.096\n.063\n.127\n.073\n.134\n.147\n.139\n.123\n.093\n.090\n.152\n.121\n.130\n5K:FULL-SUPER\n.210\n.134\n.087\n.186\n.094\n.173\n.173\n.178\n.141\n.116\n.112\n.214\n.162\n.177\n5K:FULL+SL\n.051\n.058\n.017\n.071\n.035\n.041\n.122\n.051\n.119\n.018\n.036\n.044\n.050\n.071\n5K:FULL+SL+NOD\n.101\n.098\n.041\n.129\n.062\n.123\n.186\n.097\n.167\n.061\n.069\n.140\n.093\n.120\n5K:FULL+SL+SYM\n.174\n.123\n.073\n.164\n.093\n.167\n.203\n.160\n.170\n.126\n.097\n.215\n.147\n.160\n1K:ORTHG-SUPER\n.042\n.021\n.018\n.046\n.022\n.030\n.037\n.035\n.045\n.028\n.031\n.057\n.027\n.041\n1K:ORTHG+SL+SYM\n.066\n.027\n.024\n.062\n.024\n.038\n.062\n.060\n.067\n.045\n.047\n.085\n.050\n.067\n1K:FULL-SUPER\n.049\n.027\n.021\n.070\n.029\n.032\n.057\n.044\n.044\n.034\n.040\n.084\n.029\n.052\n1K:FULL+SL\n.049\n.047\n.016\n.059\n.030\n.045\n.112\n.054\n.122\n.018\n.037\n.047\n.057\n.067\n1K:FULL+SL+NOD\n.086\n.083\n.036\n.121\n.046\n.104\n.174\n.096\n.162\n.047\n.063\n.119\n.094\n.116\n1K:FULL+SL+SYM\n.108\n.084\n.036\n.128\n.057\n.094\n.152\n.111\n.168\n.073\n.065\n.145\n.098\n.121\nTable 19: All BLI scores (MRR) with Thai (TH) as the source language. 5k and 1k denote the seed dictionary D0\nsize for (weakly) supervised methods. See Table 1 for a brief description of each model conﬁguration.\nTurkish: TR-\n-BG\n-CA\n-EO\n-ET\n-EU\n-FI\n-HE\n-HU\n-ID\n-KA\n-KO\n-LT\n-NO\n-TH\nUNSUPERVISED\n.214\n.276\n.000\n.001\n.001\n.192\n.208\n.332\n.307\n.000\n.000\n.073\n.260\n.000\n5K:ORTHG-SUPER\n.265\n.288\n.112\n.212\n.164\n.247\n.214\n.328\n.267\n.131\n.126\n.209\n.226\n.103\n5K:ORTHG+SL+SYM\n.280\n.314\n.121\n.229\n.172\n.273\n.241\n.357\n.288\n.146\n.145\n.217\n.261\n.121\n5K:FULL-SUPER\n.344\n.360\n.215\n.307\n.230\n.294\n.319\n.378\n.336\n.205\n.196\n.295\n.311\n.170\n5K:FULL+SL\n.236\n.300\n.146\n.172\n.155\n.249\n.230\n.338\n.308\n.113\n.140\n.089\n.274\n.112\n5K:FULL+SL+NOD\n.283\n.355\n.221\n.216\n.198\n.281\n.275\n.375\n.358\n.168\n.171\n.137\n.325\n.176\n5K:FULL+SL+SYM\n.351\n.376\n.238\n.309\n.244\n.322\n.323\n.397\n.370\n.229\n.214\n.280\n.346\n.183\n1K:ORTHG-SUPER\n.107\n.103\n.024\n.065\n.048\n.089\n.058\n.143\n.112\n.029\n.036\n.059\n.058\n.023\n1K:ORTHG+SL+SYM\n.198\n.184\n.039\n.130\n.076\n.160\n.122\n.303\n.203\n.055\n.063\n.112\n.121\n.040\n1K:FULL-SUPER\n.150\n.133\n.052\n.112\n.062\n.093\n.076\n.167\n.131\n.053\n.050\n.099\n.073\n.028\n1K:FULL+SL\n.230\n.309\n.156\n.167\n.147\n.250\n.223\n.336\n.312\n.108\n.139\n.093\n.273\n.113\n1K:FULL+SL+NOD\n.280\n.354\n.213\n.218\n.183\n.281\n.269\n.371\n.352\n.160\n.167\n.138\n.323\n.166\n1K:FULL+SL+SYM\n.327\n.364\n.204\n.274\n.209\n.310\n.301\n.398\n.363\n.201\n.194\n.215\n.344\n.137\nTable 20: All BLI scores (MRR) with Turkish (TR) as the source language. 5k and 1k denote the seed dictionary\nD0 size for (weakly) supervised methods. See Table 1 for a brief description of each model conﬁguration.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2019-09-04",
  "updated": "2019-09-04"
}