{
  "id": "http://arxiv.org/abs/1909.04751v1",
  "title": "Reinforcement Learning and Video Games",
  "authors": [
    "Yue Zheng"
  ],
  "abstract": "Reinforcement learning has exceeded human-level performance in game playing\nAI with deep learning methods according to the experiments from DeepMind on Go\nand Atari games. Deep learning solves high dimension input problems which stop\nthe development of reinforcement for many years. This study uses both two\ntechniques to create several agents with different algorithms that successfully\nlearn to play T-rex Runner. Deep Q network algorithm and three types of\nimprovements are implemented to train the agent. The results from some of them\nare far from satisfactory but others are better than human experts. Batch\nnormalization is a method to solve internal covariate shift problems in deep\nneural network. The positive influence of this on reinforcement learning has\nalso been proved in this study.",
  "text": "University of Sheﬃeld\nReinforcement Learning and Video Games\nYue Zheng\nSupervisor: Prof. Eleni Vasilaki\nA report submitted in partial fulﬁlment of the requirements\nfor the degree of MSc Data Analytics in Computer Science\nin the\nDepartment of Computer Science\nSeptember 12, 2019\narXiv:1909.04751v1  [cs.LG]  10 Sep 2019\ni\nDeclaration\nAll sentences or passages quoted in this document from other people’s work have been specif-\nically acknowledged by clear cross-referencing to author, work and page(s). Any illustrations\nthat are not the work of the author of this report have been used with the explicit permis-\nsion of the originator and are speciﬁcally acknowledged. I understand that failure to do this\namounts to plagiarism and will be considered grounds for failure.\nName: Yue Zheng\nAbstract\nReinforcement learning has exceeded human-level performance in game playing AI with deep\nlearning methods according to the experiments from DeepMind on Go and Atari games. Deep\nlearning solves high dimension input problems which stop the development of reinforcement\nfor many years. This study uses both two techniques to create several agents with diﬀerent\nalgorithms that successfully learn to play T-rex Runner. Deep Q network algorithm and three\ntypes of improvements are implemented to train the agent. The results from some of them\nare far from satisfactory but others are better than human experts. Batch normalization is\na method to solve internal covariate shift problems in deep neural network. The positive\ninﬂuence of this on reinforcement learning has also been proved in this study.\nii\nAcknowledgement\nI would like to express many thanks to my supervisor, Prof. Dr. Eleni Vasilaki for assigning\nme this project and her guidance across this study. I would also like to acknowledge my dear\nfriends for helping me to solve diﬀerent problems and giving me inspiration. This include\nMwiza L Kunda, Wei Wei, Yuliang Li, Ziling Li, Zixuan Zhang. Finally, I would like to\nacknowledge my family as well as my girl friend Fangni Liu for their encouragement during\nmy project.\niii\nContents\n1\nIntroduction\n1\n1.1\nBackground . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n1.2\nAim of the project . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n1.3\nOverview\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n2\nLiterature Survey\n3\n2.1\nDeep Learning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n2.1.1\nHistory of Deep Learning\n. . . . . . . . . . . . . . . . . . . . . . . . .\n3\n2.1.2\nDeep Neural Network and Activation Function . . . . . . . . . . . . .\n4\n2.1.3\nBackpropagation Algorithm . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.1.4\nConvolutional Neural Network\n. . . . . . . . . . . . . . . . . . . . . .\n7\n2.1.5\nBatch Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.2\nReinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.2.1\nHistory of Reinforcement Learning . . . . . . . . . . . . . . . . . . . .\n10\n2.2.2\nMarkov Decision Processes\n. . . . . . . . . . . . . . . . . . . . . . . .\n12\n2.2.3\nBellman Equation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n2.2.4\nExploitation vs Exploration . . . . . . . . . . . . . . . . . . . . . . . .\n16\n2.2.5\nTemporal Diﬀerence Learning . . . . . . . . . . . . . . . . . . . . . . .\n17\n2.2.6\nDeep Q Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n3\nMethodology\n23\n3.1\nRequirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n3.1.1\nSoftware Requirement . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n3.1.2\nHardware Requirement\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n3.2\nGame Description\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n3.3\nModel Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n3.4\nImage Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n3.5\nConvolutional Neural Network Architecture . . . . . . . . . . . . . . . . . . .\n27\n3.6\nExperiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n3.6.1\nHyperparameter Tuning . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n3.6.2\nComparison of diﬀerent Deep Q Network Algorithms . . . . . . . . . .\n29\n3.6.3\nEﬀect of Batch Normalization . . . . . . . . . . . . . . . . . . . . . . .\n29\n3.7\nEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n4\nResults and Discussion\n31\niv\nCONTENTS\nv\n4.1\nHyper Parameter Tuning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n4.1.1\nLearning Rate\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n4.1.2\nBatch Size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n4.1.3\nEpsilon\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n4.1.4\nExplore Step\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n4.1.5\nGamma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n4.2\nTraining Results\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n4.2.1\nDQN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n4.2.2\nDouble DQN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n4.2.3\nDueling DQN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\n4.2.4\nDQN with Prioritized Experience Replay\n. . . . . . . . . . . . . . . .\n37\n4.2.5\nBatch Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n4.2.6\nFurther Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n4.3\nTesting Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n5\nConclusions and Future Works\n42\nList of Figures\n2.1\nThree type of activation functions. . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.2\nA simple neural network.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.3\nA simple deep neural network with three hidden layers.\n. . . . . . . . . . . .\n7\n2.4\nOperations in convolution layer. . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n2.5\nOperations in pooling layer. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n2.6\nA simple convolutional neural network. . . . . . . . . . . . . . . . . . . . . . .\n9\n2.7\nInteraction between the agent and the environment.\n. . . . . . . . . . . . . .\n11\n2.8\nMarkov decision process in reinforcement learning. . . . . . . . . . . . . . . .\n14\n2.9\nExample of cliﬀwalking from [43].\n. . . . . . . . . . . . . . . . . . . . . . . .\n19\n2.10 Architecture comparison between Dueling DQN and DQN [52]\n. . . . . . . .\n22\n3.1\nA screenshot of T-rex Runner.\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n3.2\nThree type of actions in T-rex Runner . . . . . . . . . . . . . . . . . . . . . .\n25\n3.3\nPreprocessing steps for T-rex Runner.\n. . . . . . . . . . . . . . . . . . . . . .\n27\n3.4\nConvolutional Neural Network architecture for Deep Q Network. . . . . . . .\n28\n3.5\nConvolutional Neural Network architecture for Dueling Deep Q Network.\n. .\n29\n4.1\nHyper parameter tuning for learning rate. . . . . . . . . . . . . . . . . . . . .\n32\n4.2\nHyper parameter tuning for batch size. . . . . . . . . . . . . . . . . . . . . . .\n32\n4.3\nHyper parameter tuning for explore probability ϵ. . . . . . . . . . . . . . . . .\n33\n4.4\nHyper parameter tuning for explore steps. . . . . . . . . . . . . . . . . . . . .\n34\n4.5\nHyper parameter tuning for discount factor γ. . . . . . . . . . . . . . . . . . .\n34\n4.6\nTraining result for DQN. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n4.7\nTraining result for Double DQN compared with DQN. . . . . . . . . . . . . .\n36\n4.8\nTraining result for Dueling DQN compared with DQN. . . . . . . . . . . . . .\n36\n4.9\nTraining result for DQN with prioritized experience replay compared with DQN. 37\n4.10 Batch normalization on DQN, Double DQN, Dueling DQN and DQN with\nprioritized experience replay . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n4.11 Boxplot for test result with eight diﬀerent algorithms . . . . . . . . . . . . . .\n40\nvi\nList of Tables\n2.1\nDimension description of the deep neural network . . . . . . . . . . . . . . . .\n6\n2.2\nProperty of convolutional layer and pooling layer . . . . . . . . . . . . . . . .\n9\n3.1\nSoftware requirement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n3.2\nHardware requirement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n4.1\nHyper parameters used in all experiments . . . . . . . . . . . . . . . . . . . .\n31\n4.2\nHyperparameters used in all experiments . . . . . . . . . . . . . . . . . . . . .\n35\n4.3\nStep size diﬀerence between DQN and DQN with PER . . . . . . . . . . . . .\n37\n4.4\nTraining results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n4.5\nTest results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\nvii\nChapter 1\nIntroduction\n1.1\nBackground\nThe applications of Artiﬁcial Intelligence are widely used in recent years. As one part of them,\nReinforcement Learning has achieved incredible results in game playing. An intelligent agent\nwill be created and trained with reinforcement learning algorithms to fulﬁll this tasks. In the\nFuture of Go Summit 2017, Alpha Go which is an AI player trained with deep reinforcement\nlearning algorithms won three games against the world best human player in Go. The success\nof reinforcement learning in this area shock the world and many researches are launched such\nas driverless cars. Deep learning methods such as convolutional neural network contributes a\nlot to this because these techniques solves the problem of dealing with high dimension input\ndata and feature extraction.\nT-rex Runner is a dinosaur game from Google Chrome oﬄine mode. The aim of the player\nis to escape all obstacles and get higher score until reaching the limitation which is 99999.\nThe moving speed of the obstacles will increase as time goes by which make it diﬃcult to get\nthe highest score.\nThe code of this project can be found in this link which is written in Python.\n1.2\nAim of the project\nThe aim of this project is to create an agent using diﬀerent algorithms to play T-rex Runner\nand compare the performance of them. Internal covariate shift is the change of distribution\nin each layer of during the training which may result in longer training time especially in deep\nneural network. To cope with this problem, batch normalization use linear transformation on\neach feature to normalize the data with the same mean and variance. The same problem may\nalso occur in deep reinforcement learning because the decision is based on neural network.\nBeyond the comparison of diﬀerent reinforcement learning algorithms, this project will also\ninvestigate the eﬀect of batch normalization. The overall objectives of this project are list\nbelow.\n1\nCHAPTER 1. INTRODUCTION\n2\n• Create an agent to play T-rex Runner\n• Compare the diﬀerence among diﬀerent reinforcement learning algorithms\n• Investigate the eﬀect of batch normalization in reinforcement learning\n1.3\nOverview\nThis study opens with a literature review on deep learning and reinforcement learning. Each\nsection includes the history of the ﬁeld and the techniques related to this study. Chapter\n3 includes the description of the game and the choice of algorithms according the literature\nreview. The entire processing step will be shown as well as the architecture of the model.\nThe design of the experiments and the evaluation methods are presented in this chapter too.\nChapter 4 shows the result of all the experiments and the discussion of each experiment.\nChapter 5 presents the conclusion of this study and the proposed future works.\nChapter 2\nLiterature Survey\nThis chapter introduces the techniques used in developing an agent to play T-rex Runner.\nThere are two main sections which are Deep Learning and Reinforcement Learning. Brief\nhistory and some milestones will be described and some important methods will be shown in\ndetail.\n2.1\nDeep Learning\nDeep learning is a class of Machine Learning model based on Artiﬁcial Neural Network (ANN).\nThere two kinds of deep learning model which is widely used in recent years. Recurrent Neural\nNetwork is one of them which shows its power in Natural Language Processing. The other one\nplays an important role in deep reinforcement learning called Convolutional Neural Network\n(CNN). It is one of the most eﬀective models for computer vision problems such as object\ndetection and image classiﬁcation. This section gives a brief introduction of deep learning\nand detailed information about convolutional neural network.\n2.1.1\nHistory of Deep Learning\nAn artiﬁcial neural network is a computation system inspired by biological neural networks\nwhich were ﬁrst proposed by McCulloch, a neurophysiologist [24]. In 1957, Perceptron was\ninvented by Frank [31]. Three years later, his experiments show this algorithm can recognize\nsome of alphabets [32]. However, Marvin proved that a single layer perceptron cannot deal\nwith XOR problem [25]. This stopped the development of ANN until Rumelhart et al. show\nthat some useful representations can be learned with multi-layer perceptron, which is also\ncalled neural network, and backpropagation algorithm [33] in 1988. One year later, LeCun\net al. ﬁrst used a ﬁve-layer neural network and backpropagation to solved digit classiﬁcation\nproblem and achieved great results [21]. His innovative model is known as LeNet which is\nthe beginning of the convolutional neural network.\n3\nCHAPTER 2. LITERATURE SURVEY\n4\nThe origin of CNN was proposed by Fukushima named Neocognitron which was a self-\norganized neural network model with multiple layers [10]. This model achieved a good result\nin object detection tasks because it is not position-sensitive. As mentioned before, LeCun et\nal. invented LeNet and got less than 1% error rate in mnist handwritten digits dataset in\n1998[21]. The model used convolutions and sub-sampling which is called convolution layer\nand pooling layer today to convert the original images into feature vectors and perform clas-\nsiﬁcation with fully connected layers. At the same time, some neural network models show\nsome acceptable results in face recognition [20], speech recognition [51] and object detection\n[49]. But the lack of reliable theory caused the research of CNN to stagnate for many years.\nIn the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012 [9], Alex and his\nteam got 16.4% error rate with an eight-layer deep neural network (AlexNet) [19]. This was\na signiﬁcant result compared with the one from second rank participant which was 26.2%.\nBeyond LeNet, AlexNet used eight layers to train the classiﬁer with data augmentation,\ndropout, ReLU, which mitigated overﬁtting problem. Another signiﬁcant discovery was that\nparallel computing with multiple GPUs can largely decrease the training time.\nTwo years later, Simonyan and Zisserman introduced a sixteen-layer neural network (VG-\nGNet) and won the ﬁrst prize in ILSVRC 2014 classiﬁcation and localization tasks [41]. This\nmodel got the state-of-the-art result with 7.3% error rate at that time. VGGNet also proved\nthat using smaller ﬁlter size and deeper network can improve the performance of CNN. The\nsize of all ﬁlters in the model was no greater than 3 × 3 while the ﬁrst two layers in AlexNet\nwere 11 × 11 and 5 × 5. In the same year, GoogLeNet [46], the best model of ILSVRC 2014\nclassiﬁcation and detection tasks, ﬁrst used inception which was proposed by Lin [23] to solve\nvanishing gradient problem. Inception replaced one node with a network which was consisted\nof several convolutional layers and pooling layers then concatenate them before passing to\nthe next layer. This change made the feature selection between two layers more ﬂexible. In\nother words, it can be updated by the backpropagation algorithm.\nAnother problem of the deep neural network was degradation resulting in high training error\ncaused by optimization diﬃculty. To solve that problem, He et al. proposed a deep residual\nlearning framework (ResNet) using a residual mapping instead of stacking layers directly\n[12]. This model won the championship in ILSVRC 2015 with only 3.57% error rate. His\nexperiments show that this new framework can not only solve degradation problems but\nalso can improve the computing eﬃciency.\nResNet There were many variants based on\nResNet such as Inception-ResNet [45] and DenseNet [14]. The former one combined improved\ninception techniques into ResNet. Every two convolutional layers were connected in the later\nmodel. This change mitigated vanishing gradient problems and improved the propagation of\nfeatures.\n2.1.2\nDeep Neural Network and Activation Function\nA neural network or multi-layer perceptron consists of three main components: the input\nlayer, the hidden layer, and the output layer. Each unit in one layer called a neuron. The\ninput data are fed into the input layer conducting linear transformation through weights\nin the hidden layer. Finally, the result will be given non-linear ability through activation\nCHAPTER 2. LITERATURE SURVEY\n5\nfunction and fed into the output layer.\nActivation function enables the network to learn more complicated relationships between\ninputs and outputs. There are three widely used activation functions shown in Figure 2.1:\nsigmoid, tanh and ReLU. ReLU is the most commonly used one in three because it has a low\ncomputational requirement and better performance in solving vanishing gradient problems\ncompared with the other two.\ntanh(x) = ex −e−x\nex + e−x\n(2.1)\nsigmoid(x) =\n1\n1 + e−x\n(2.2)\nReLU(x) = max(x, 0)\n(2.3)\nFigure 2.1: Three type of activation functions.\nTo illustrate the entire process in neural network, here is an example in Figure 2.2. Given\nan input data T = {(x1, y1), (x2, y2), (x3, y3), (x4, y4) | xi ∈Rm} and a randomly generated\nweight [w1, w2, w3, w4]T in the hidden layer, the output of the neural network is ˆy and the\nactivation of the hidden layer is f. Therefore, the estimated value ˆyi can be calculated by\nˆyi = f(wixi + b)\n(2.4)\nwhere b is the bias of the hidden layer and m is the number of features.\nIf the number\nof hidden layers in the neural network is greater than two, this is also called Deep Neural\nNetwork (DNN). Consider a simple DNN with three hidden layers shown in Figure 2.3. Given\nthe same input X = [x1, x2, x3, x4]T in matrix form and W i is the weight between (i−1)-th\nand i-th layer, the output of i-th layer ai can be calculated by\nai = fi(zi)\n(2.5)\nwhere fi is the activation function between (i −1)-th and i-th layer and zi = ai−1W i,\nespecially a0 = X. The dimension of those variables are shown in Table 2.1\nCHAPTER 2. LITERATURE SURVEY\n6\nFigure 2.2: A simple neural network.\nParameter\nDescription\nDimension\nX\nInput data\n4 × m\nW 1\nWeight between input layer and hidden layer 1\nm × 8\nW 2\nWeight between hidden layer 1 and hidden layer 2\n8 × 6\nW 3\nWeight between hidden layer 2 and hidden layer 3\n6 × 8\nW 4\nWeight between hidden layer 3 and output layer\n8 × 1\nTable 2.1: Dimension description of the deep neural network\n2.1.3\nBackpropagation Algorithm\nSection 2.1.2 introduces the way to estimated label using deep neural network. In order to\noptimize this estimation, a cost function J is used to quantify the diﬀerence between the\nestimated value ˆy and the true value y. To give a simple example, Mean Square Error [1]\nwhich is often applied to regression problems is used in this section to illustrate how the\nbackpropagation algorithm works. Equation 2.6 shows the form of mean square error.\nJ(w, x) = 1\n2\nn\nX\ni=1\n(yi −ˆyi)2\n(2.6)\nwhere x is the input data and w represent all weights used in the model. Thus, the optimiza-\ntion problem can be described as following\nmin\nw J(w, x)\n(2.7)\nStochastic Gradient Descent (SGD) is an eﬀective way to solve this optimization problem\nCHAPTER 2. LITERATURE SURVEY\n7\nFigure 2.3: A simple deep neural network with three hidden layers.\nif J(w, x) is convex. However, it still shows acceptable results in deep neural network even\nthough there is no guarantee for global optimal point in non-convex optimization [11]. Instead\nof ﬁnding the optimal point directly, SGD optimize the objective function 2.7 iteratively by\nfollowing equation\nw ←w −η∂J(w, x)\n∂w\n(2.8)\nwhere η is the learning rate which can control the update speed of the weight. Using gradient\nmethods such as SGD to optimize the cost function in neural network is called backpropa-\ngation algorithm. Considering the deep neural network shown in 2.3 and the techniques of\nmatrix calculus [13], the gradient of J with respect to W 4 is\n∂J\n∂W 4\n= aT\n3 ((a4 −y) ⊙∇f4(z4))\n(2.9)\nwhere ⊙is element-wise matrix multiplication and ∇f4(z4) is the gradient with respect to\nz4. Results for other W i can be calculated in a similar way. With equation 2.8, W i can be\nupdated during each iteration by\nW i ←W i −η ∂J\n∂W i\n(2.10)\n2.1.4\nConvolutional Neural Network\nCompared with a common deep neural network, a convolutional neural network has two\nextra components which are convolutional layer and pooling layer. The convolutional layers\nCHAPTER 2. LITERATURE SURVEY\n8\nmake use of several trainable ﬁlters to select diﬀerent features. The pooling layer reduces the\ndimension of the data by subsampling.\nIn the convolution layer, the output of the last layer is convolved by trainable ﬁlters with\nelement-wise matrix multiplication. The size and the number of each ﬁlter are deﬁned by\nthe user and the initial value is randomly generated. The moving step of a ﬁlter in each\nconvolution layer is decided by stride. In order to keep the information of the border during\nthe forward propagation, a series of zeros attached to the border of the image called padding.\nFigure 2.4 shows how the result of one neuron in a convolution layer comes from and how\nthe ﬁlter (x, y, z, w) will be updated in every iteration in backpropagation.\nFigure 2.4: Operations in convolution layer.\nThe reason for using a pooling layer is not only for dimension reduction but also for detecting\ninvariant features including translation, rotation, scale from the input [37]. There are two\ntypes of operations in pooling layer: max pooling and average pooling. In max pooling, only\nthe maximum value in user-deﬁned windows will be chosen while all values in the window will\nmake contributions to the output in average pooling. The choice of operation is dependent\non tasks and Boureau has made a theoretical comparison between those two [6]. Both max\nand average operation are shown in Figure 2.5.\nFigure 2.5: Operations in pooling layer.\nA complete convolutional neural network consists of several convolutional layers, pooling\nCHAPTER 2. LITERATURE SURVEY\n9\nlayers, and fully connected layers. The fully connected layer is the same concept of DNN\nwhich used the ﬂatten vector of the last output of the other two layers as input. Considering\na classiﬁcation problem as shown in Figure 2.6, an image with a size of 64 × 64 is fed into\nCNN and output a scalar which represents its class. Table 2.2 lists the ﬁlters information\nused in CNN.\nFigure 2.6: A simple convolutional neural network.\nLayer\nNumbers\nSize\nStride\nPadding\nOutput dimension\nConvolution 1\n8\n8×8\n2\n0\n8×28×28\nMax Pooling\n8\n2×2\n2\n/\n8×14×14\nConvolution 2\n16\n6×6\n2\n0\n16×4×4\nTable 2.2: Property of convolutional layer and pooling layer\nThe backpropagation algorithm in convolutional neural network is a little diﬀerent from de-\nscribed in section 2.1.3 because of two extra layer type. In average pooling layer, the error\nwill be divided by t × t which is the size of the ﬁlter and propagate to the last layer. In max\npooling layer, the position of the maximum value will be stored when forward propagating\nand the error will be directly passed through that position. In convolutional layer, backprop-\nagation can be calculated through basic diﬀerentiation. Consider the convolution operation\nin Figure 2.4, if the error from the output layer is δ, then we have\n∂δ\n∂x =\n∂δ\n∂O11\n∂O11\n∂x\n+\n∂δ\n∂O12\n∂O12\n∂x\n+\n∂δ\n∂O21\n∂O21\n∂x\n+\n∂δ\n∂O22\n∂O22\n∂x\n(2.11)\nwhere\n\u0014 O11\nO12\nO21\nO22\n\u0015\n(2.12)\nis the output matrix in Figure 2.4. The diﬀerentiation of δ with respect to y, z, w can be\ncomputed in a similar way.\nCHAPTER 2. LITERATURE SURVEY\n10\n2.1.5\nBatch Normalization\nWith the increasing depth of the neural network, the training time becomes longer. One of\nthe reason is the distribution of input in each layer changes when updating the weight which\nis called Internal Covariate Shift. In 2015, Ioﬀe proposed Batch Normalization (BN) which\nmake the distribution in each layer more stable and achieve shorter training time [15]. In\neach neuron, the input can be normalized by Equation 2.13\nbx =\nx −E[x]\np\nVar[x] + ϵ\n(2.13)\nwhere ϵ is used to avoid zero variance. Now data in each neuron follow the distribution with\nmean 0 and standard deviation 1. However, this changes the representation ability of the\nnetwork which may lead to the loss of information in the earlier layer. Therefore, Ioﬀe used\nanother linear transformation to restore that representation\n˜x = mbx + n\n(2.14)\nwhere m and n are learnable parameters, especially, the result is the same as original when\nm =\np\nVar[x] and n = E[x]. The mean and variance during training will be stored and will\nbe treated as the mean of the variance of test data. In their experiment, BN can not only\ndeal with Internal Covariate Shift problems but also mitigate vanishing gradient problems.\n2.2\nReinforcement Learning\nReinforcement Learning (RL) is a class of machine learning aiming at maximum the reward\nsignal when making decisions. The basic component of reinforcement learning is the agent\nand the environment.\nAs shown in Figure 2.7, the agent will receive feedback including\nobservation and reward from the environment after each action. To generate a better policy,\nit will keep interacting with the environment and improve its decision-making ability step by\nstep until the policy converges.\n2.2.1\nHistory of Reinforcement Learning\nIn recent years, reinforcement learning becomes popular because of Alpha Go, a program\nthat can beat human expert in Go [40]. In the Future of Go Summit 2017, Alpha Go Master\nshocked the world by winning all three games against Ke Jie, the world best player in Go.\nBut the research of reinforcement learning started very early. According to Sutton, the early\nhistory of RL can be divided into two main threads [43].\nOne of them was optimal control. To cope with arising optimal control problems which were\ncalled ”multi-stage decision processed” in 1954, the theory Dynamic Programming (DP) was\nintroduced by Bellman [5]. In the theory, he proposed the concept of ”functional equation”,\nCHAPTER 2. LITERATURE SURVEY\n11\nFigure 2.7: Interaction between the agent and the environment.\nwhich was often called the Bellman equation today.\nAlthough DP was one of the most\neﬀective approaches to solve optimal control problems at that time, the high computational\nrequirements which is called ”the curse of dimensionality” by Bellman were not easy to solve\n[4]. Three years later, he built a model called Markov Decision Processes (MDPs) to describe\na kind of discrete deterministic processes [3]. This deterministic system and the concept of\nvalue function which is described in the Bellman equation consists of the basic theory of\nmodern reinforcement learning.\nIn optimal control thread, solving problems required full knowledge of the environment and\nit was not a feasible way to deal with most problems in the real world. The trial-and-error\nthread focused more on the feedback rather than the environment itself. The ﬁrst expression\nabout the key idea of trail-and-error including ”selectional” and ”associative” called ”Law\nof Eﬀect” was written in Edward Thorndike’s book ”Animal Intelligence” [47]. Although\nsupervised learning was not ”selectional”, some researchers still mistook it for reinforcement\nlearning and concentrated on pattern recognition [8, 55]. This led to rare researches in actual\ntrial-and-error learning until Klopf recognized the diﬀerence between supervised learning and\nRL: the motivation to gain more rewards from the environment [16, 17]. However, there\nwere still some remarkable works such as the reinforcement learning rule called ”selective\nbootstrap adaptation” by Widrwo in 1973 [54].\nBoth of two threads came across in modern reinforcement learning. Temporal Diﬀerence\n(TD) learning was a method that predicts future values depend on the current signal which\noriginated from animal learning psychology. This idea was ﬁrst proposed and implemented\nby Samuel [35]. In 1972, Klopf developed the idea of ”generalized reinforcement” and linked\nthe trial-and-error learning with animal learning psychology [16].\nIn 1983, Sutton devel-\noped and implemented the actor-critic architecture in trial-and-error learning based on the\nidea from Klopf [2]. Five years later, he proposed TD(λ) algorithms which used additional\nstep information for update policy and made TD learning a general prediction method for\ndeterministic problems [42]. One year later, Chris used optimal control methods to solve\nCHAPTER 2. LITERATURE SURVEY\n12\ntemporal-diﬀerence problems and developed the Q-learning algorithm which estimated de-\nlayed reward by action value function [53]. In 1994, an online Q-learning was proposed by\nRummery and Niranjan which was known as SARSA [34]. The diﬀerence between Q-learning\nand SARSA was that the agent used the same policy during the learning process in SARSA\nwhile it always chooses the best action based on value function in Q-learning.\nWith the development of the deep neural network, DeepMind proposed Deep Q-learning\nNetwork (DQN) algorithm which used a convolutional neural network to solve high dimen-\nsionality of the state in reinforcement learning problems [27]. Two years later, they modiﬁed\nDQN by adding a target policy to improve its stability [28]. The highlight of the DQN was\nnot only the combination of deep learning and RL but also the experience replay mechanism.\nTo solve dependency problems when optimizing CNN, Mnih et al. stored the experiences\nto memory in each step and randomly sampled a mini-batch to optimize the neural network\nbased on the idea from Lin [22]. In 2015, this mechanism was improved by measuring the\nimportance of experience with temporal diﬀerence error [36]. Meanwhile, Wang proposed\nDueling DQN which used an advantage function learning how valuable a state was without\nestimating each action value for each state [52]. This new neural network architecture was\nhelpful when there was no strong relationship between actions and the environment. In 2016,\nDeepMind proposed Double DQN which show the higher stability of the policy by reducing\noverestimated action values [50].\nAlthough a series of algorithms based on DQN show human-level performance on Atari\ngames, they still failed to deal with some speciﬁc games. DQN was a value-based method\nwhich meant the choice of action was depend on the action values. However, choosing action\nrandomly may be the best policy in some games such as Rock−paper−scissors. To deal with\nthis problem, Sutton proposed policy gradient which enabled the agent to optimize the policy\ndirectly [44]. Based on this, OpenAI proposed a new family of algorithms such as Proximal\nPolicy Optimization (PPO) [38]. PPO used a statistical method called importance sampling\nwhich was used to estimate a distribution by sampling data from another distribution and\nthis simple modiﬁcation show a better performance in RoboschoolHumanoidFlagrun.\nSince the basic policy gradient method sampled data from completed episodes, the variance\nof the estimation was high because of the high dimension action space. Similar to value-\nbased method, Actor-critic method was proposed to solve this problem [18].\nCompared\nwith the policy gradient, this method used a critic to evaluate the chosen action.\nThis\nmade the policy can be updated after each decision which not only reduced the variance\nbut also accelerated the convergence. The famous improved actor-critic based algorithm is\nasynchronous advantage actor-critic (A3C) [26]. Similar to Dueling DQN, this method used\nadvantage function to estimate value function and performed computing in parallel which\ncan largely increase the learning speed.\n2.2.2\nMarkov Decision Processes\nAs mentioned in Section 2.2.1, the interaction between the agent and the environment can be\nmodeled as a Markov Decision Process which is based on Markov property. Markov property\ndescribes a kind of stochastic processes that the probability of next event occurring only\nCHAPTER 2. LITERATURE SURVEY\n13\ndepend on the current event.\nDeﬁnition 1 (Markov property [39]) Given a state St+1 at time t+1 in a ﬁnite sequence\n{S0, S1, S2, · · · , SN}. This sequence has Markov property, if and only if\nP [St+1|St] = P [St+1|S1, . . . , St]\n(2.15)\nA Markov Decision Process (MDP) is a random process with Markove property, values and\ndecisions.\nDeﬁnition 2 (Markov Decision Process [39]) A Markov Decision Process can be de-\nscribed as a tuple ⟨S, A, P, R, γ⟩\n• S is a ﬁnite set of states\n• A is a ﬁnite set of actions\n• P is a state transition probability matrix\nPa\nss′ = P\n\u0002\nSt+1 = s′|St = s, At = a\n\u0003\n(2.16)\n• R is a reward function\nRa\ns = E [Rt+1|St = s, At = a]\n(2.17)\n• γ is a discount factor γ ∈[0, 1]\nTo describe how the decision is made, a policy π is required to deﬁne the behaviour of the\nagent.\nDeﬁnition 3 (Policy [39]) A policy is a distribution over actions given states\nπ(a|s) = P [At = a|St = s]\n(2.18)\nIn MDP, the agent is expected to get as many rewards as it can from the environment.\nHowever, maximizing the reward at time-step t makes the agent short-sighted which means\nit only considers the reward from the next action rather the total reward of one episode.\nTherefore, return is deﬁned as the concept ”reward” which the agent is expected to maximize.\nDeﬁnition 4 (Return [39]) The return Gt is the total discounted reward Rt from time-step\nt.\nGt = Rt+1 + γRt+2 + . . . =\n∞\nX\nk=0\nγkRt+k+1\n(2.19)\nwhere γ ∈[0, 1] is the discount factor.\nThe value of the discount factor represents how far-sighted the agent will be. If this value is\n1, the agent will treat every reward in the future as the same. But this will also make the\nagent confused about which decision is not appropriate. At this point, the behavior of the\nagent can be described as in Figure 2.8\nCHAPTER 2. LITERATURE SURVEY\n14\nFigure 2.8: Markov decision process in reinforcement learning.\nSince the return is deﬁned in a random process, similar to reward function, the expectation\nof it can be deﬁned as following which is also called value function.\nDeﬁnition 5 (State Value Function [39]) The state-value function vπ(s) of an MDP is\nthe expected return starting from state s, and then following policy π\nvπ(s) = Eπ [Gt|St = s]\n(2.20)\nDeﬁnition 6 (Action Value Function [39]) The action-value function qπ(s, a) is the ex-\npected return starting from state s, taking action a, and then following policy π\nqπ(s, a) = Eπ [Gt|St = s, At = a]\n(2.21)\nWith the Deﬁnition 5, 6 and the deﬁnition of the expectation, we can simply write\nvπ(s) =\nX\na∈A\nπ(a|s)qπ(s, a)\n(2.22)\nwhere A is a set of action the agent can choose.\n2.2.3\nBellman Equation\nSince we deﬁne the Markov decision process in Section 2.2.2, the behavior of the agent can\nbe described mathematically. As mentioned in Section 2.2.1, this problem can be solved by\nthe Bellman equation.\nTheorem 1 (Bellman Expectation Equation [39]) The state-value function can be de-\ncomposed into immediate reward plus discounted value of successor state\nvπ(s) = Eπ [Rt+1 + γvπ (St+1) |St = s]\n(2.23)\nCHAPTER 2. LITERATURE SURVEY\n15\nThe action-value function can similarly be decomposed\nqπ(s, a) = Eπ [Rt+1 + γqπ (St+1, At+1) |St = s, At = a]\n(2.24)\nHere is a simple proof for Equation 2.24. According to the Deﬁnition 4, the return at time t\ncan be decomposed into two parts: the immediate reward and the discounted return at time\nt + 1\nGt = Rt+1 + γGt+1\n(2.25)\nSubstitute Gt with Equation 2.25 in Deﬁnition 6\nqπ(s, a) = Eπ [Rt+1 + γGt+1|St = s, At = a]\n(2.26)\nDue to the linearity of expectation, Gt+1 can be replaced by qπ (St+1, At+1) and then we\nobtain the Bellman equation for action-value function.\nThe state-value function can be\nproved in the same way. With the deﬁnition of optimal value function\nDeﬁnition 7 (Optimal Value Function [39]) The optimal state-value function v∗(s) is\nthe maximum value function over all policies\nv∗(s) = max\nπ\nvπ(s)\n(2.27)\nThe optimal action-value function q∗(s, a) is the maximum action-value function over all\npolicies\nq∗(s, a) = max\nπ\nqπ(s, a)\n(2.28)\nTheorem 1 can be extended to Bellman optimality equation\nTheorem 2 (Bellman Optimality Equation [39]) The optimal state-value function can\nbe decomposed into maximum immediate reward plus discounted optimal value of successor\nstate\nv∗(s) = max\na\nRa\ns + γ\nX\ns′∈S\nPa\nss′v∗\n\u0000s′\u0001\n(2.29)\nThe optimal action-value function can similarly be decomposed\nq∗(s, a) = Ra\ns + γ\nX\ns′∈S\nPa\nss′ max\na′\nq∗\n\u0000s′, a′\u0001\n(2.30)\nwhere s = St, a = At, s′ = St+1, a′ = At+1\nHere is a simple proof for Equation 2.30. Due to the linearity of expectation, Equation 2.26\ncan be decomposed into the expectation of the immediate reward\nEπ [Rt+1|St = s, At = a]\n(2.31)\nand the expectation of the discounted return at time t + 1\nCHAPTER 2. LITERATURE SURVEY\n16\nγEπ [Gt+1|St = s, At = a]\n(2.32)\nAccording to the deﬁnition of reward function in Deﬁnition 2, Equation 2.31 is equal to Ra\ns.\nIf next state is s′, Equation 2.32 can be written as following with the transition probability\nmatrix Pa\nss′\nγ\nX\ns′∈S\nPa\nss′Eπ\n\u0002\nGt+1|St = s, At = a, St+1 = s′\u0003\n(2.33)\nWith the Markov property, we know the expectation of the return in Equation 2.33 is not\nrelated to the current state s and action a and this is equal to the state-value function.\nTherefore, Equation 2.33 can be written as following\nγ\nX\ns′∈S\nPa\nss′v(s′)\n(2.34)\nConsidering 2.31, 2.34 and 2.22, the action-value function can be written as following\nqπ(s, a) = Ra\ns + γ\nX\ns′∈S\nPa\nss′\nX\na′∈A\nπ\n\u0000a′|s′\u0001\nqπ\n\u0000s′, a′\u0001\n(2.35)\nIt is easy to prove that there is always an optimal policy for any Markov decision process\nand it can be found by maximizing action-value function.\nπ∗(a|s) =\n( 1\nif a = argmax\na∈A\nq∗(s, a)\n0\notherwise\n(2.36)\nConsidering 2.36, Bellman optimality equation for action-value function can be obtained by\nreplacing the policy in Equation 2.35 with optimal policy. There are many ways to solve this\nequation such as Sarsa and Q-learning. This will be discussed in Section 2.2.5.\n2.2.4\nExploitation vs Exploration\nIf the agent has complete knowledge of the environment, in the other word, the transition\nprobability P a\nss′ can be calculated given state s and action a, Equation 2.30 can be solved\nby an iterative method with appropriate γ. However, this method is unable to deal with an\nunknown environment because a large amount of information has to be collected to estimate\nP a\nss′ before the convergence of action value function. If the q function tends to be stable\nbefore the environment has been fully explored, the performance of the model would be far\nfrom satisfactory, especially in high action space situation.\nTo deal with this problem, ϵ - greedy selection [43] is introduced to ensure the agent make\nenough exploration before the convergence of the action value function. Instead of choosing\nCHAPTER 2. LITERATURE SURVEY\n17\nthe best action estimated by q function, there is a probability of ϵ to randomly select from\nall actions. The mathematical expression of this method is shown as following\nπ(a|s) =\n( ϵ/m + 1 −ϵ\nif a∗= arg max\na∈A\nq(s, a)\nϵ/m\notherwise\n(2.37)\nwhere m is the number of actions. This method may have a bad eﬀect on the performance\nof the agent at ﬁrst several episodes during the training but it can widen the horizon of the\nagent in long term view.\n2.2.5\nTemporal Diﬀerence Learning\nAs mentioned in Section 2.2.4, most environment in the real world is unknown. To solve\nthis problem, a method called Monte Carlo (MC) is used to sample data for estimating value\nfunction. The agent can learn the environment from one episode experience and the value\nfunction can be approximated by the mean of the return instead of the expectation. The\nmathematical expression can be described as following\nv(St) = S(St)\nN(St)\n(2.38)\nwhere St is the state at time t, S(St) is the sum of return and N(St) is the counter to record\nthe visit number of state St. There are two kinds of visit: ﬁrst visit and every visit. The\nformer one means the model only need to record the ﬁrst visit of state St in one episode while\nall visit of St in one episode will be taken into consideration in every visit. Simplify equation\n2.38, we can get the recurrence equation for v(s)\nv(s) ←v(s) + η(Gt −v(s))\n(2.39)\nwhere η is the learning rate which can control the update speed of the value function and\ns is the state at time t. The problem of Monte Carlo method is all rewards in one episode\nhave to be collected to get Gt.\nThe value function can only be updated when reaching\nthe end of the episode which may lead to low training eﬃciency. To update value function\nwith an incomplete episode, the return can be replaced by estimated value function using\nbootstrapping. With the Bellman equation 2.23 and 2.39, we can write\nv(s) ←v(s) + η(Rt+1 + γv(s′) −v(s))\n(2.40)\nThis idea is called Temporal Diﬀerence (TD) Learning. In TD learning, value function will\nbe updated immediately after a new observation. Compared with MC methods, TD learning\nhas lower variance because there are too many random actions {At+1, At+2, · · · } in the Monte\nCarlo method which will lead to the high variance. Similarly, the recurrence equation for\naction value function can be written as following\nCHAPTER 2. LITERATURE SURVEY\n18\nq(s, a) ←q(s, a) + α\n\u0010\nRt+1 + γq(s′, a′) −q(s, a)\n\u0011\n(2.41)\nwhere s and a is the state and action at time t, s′ and a′ is the state and action at time t+1.\nEquation 2.41 shows an iterative method to get the optimal action value function q∗(s, a).\nWith this equation and ϵ - greedy policy, the RL problem can be solved by Sarsa [34].\nAlgorithm 1 Sarsa\n1: set learning rate α, number of episodes N, explore rate ϵ, discount factor γ\n2: set q(s, a) ←0, ∀s, a\n3: for episode ←1 to N do\n4:\ninitialize time t ←0\n5:\nget state s0 from the environment\n6:\nchoose action a0 following ϵ - greedy policy from q(s, a)\n7:\nwhile episode is incomplete do\n8:\ntake action and get next state st+1, reward rt+1 from the environment\n9:\nchoose action at+1 following ϵ - greedy policy from q(s, a)\n10:\nupdate q(st, at) ←q(st, at) + α\n\u0010\nrt+1 + γq(st+1, at+1) −q(st, at)\n\u0011\n11:\nt ←t + 1, st ←st+1, at ←at+1\n12:\nend while\n13: end for\nThe name of Sarsa is from the sequence {S0, A0, R1, S1, A1, R2, · · · }. Besides Sarsa, there is\nanother similar algorithm called Q learning [53].\nAlgorithm 2 Q learning\n1: set learning rate α, number of episodes N, explore rate ϵ, discount factor γ\n2: set q(s, a) ←0, ∀s, a\n3: for episode ←1 to N do\n4:\ninitialize time t ←0\n5:\nget state s0 from the environment\n6:\nwhile episode is incomplete do\n7:\nchoose action at following ϵ - greedy policy from q(s, a)\n8:\ntake action and get next state st+1, reward rt+1 from the environment\n9:\nupdate q(st, at) ←q(st, at) + α\n\u0010\nrt+1 + γ max\na\nq(st+1, a) −q(st, at)\n\u0011\n10:\nt ←t + 1, st ←st+1\n11:\nend while\n12: end for\nIn algorithm 2, there are two policies during the iteration. When choosing the action at+1\nfrom q(s, a) given st+1, Sarsa uses ϵ - greedy policy while Q learning uses greedy policy. But\nboth of them are choosing at with ϵ - greedy policy. Considering the example of CliﬀWalking\nshown in Figure 2.9 from Sutton’s book [43], every transition in the environment will get −1\nreward except next state is the cliﬀwhich the agent will get −100 reward, Sarsa is more likely\nCHAPTER 2. LITERATURE SURVEY\n19\nto choose the safe path while Q learning tends to choose the optimal path with ϵ - greedy\npolicy. But both of them can reach the optimal policy if reducing the value of ϵ.\nFigure 2.9: Example of cliﬀwalking from [43].\n2.2.6\nDeep Q Network\nQ learning is a powerful algorithm to solve simple reinforcement problems. However, it is\nunable to deal with continuous states or continuous actions. To solve the former problem,\ndeep learning method can be used to approximate action value function.\nGenerally, states are image data observed by the agent and convolutional neural network is\nan eﬀective way to extract features from this kind of data in convolution layers and feed\nthem into the fully connected layer to approximate q function. Several consistent stationary\nimages will be stacked into one input data to make the model understand that the agent is\nmoving. But the input data is highly dependent, the performance of the model will be largely\naﬀected by the dependency.\nAs mentioned in Section 2.2.1, DeepMind introduced experience replay pool which will store\nthe experience into the memory and sample some of them to optimize the neural network\nmodel in 2013 [27]. Using Q learning, deep learning and experience replay pool, the improved\nalgorithm named Deep Q Network (DQN) shows incredible performance on Atari games\naccording to their paper. Two years later, they found the agent became more stable by using\ntwo network [28]. This algorithm can be described as below\nAll states in Algorithm 3 have to be pre-processed before feeding into a neural network model.\nBased on Deep Q Network, there are three kinds of improved algorithms considering the\nstability of the training process, the importance of each experience and new neural network\narchitecture. Double DQN [50] utilizes the advantage of two networks. Instead of ﬁnding the\noptimal q value from target network q′(s, a) directly, this method chooses the optimal action\nfrom the policy network and ﬁnd the corresponding q value in the target network. Use the\nterm in Algorithm 3, the change can be illustrated as following\nCHAPTER 2. LITERATURE SURVEY\n20\nAlgorithm 3 Deep Q Network\n1: initialize policy network q(s, a) with random weights\n2: set learning rate α, number of episodes N, explore rate ϵ, discount factor γ\n3: set batch size M, update step T\n4: set target network q′(s, a) = q(s, a)\n5: for episode ←1 to N do\n6:\ninitialize time t ←0\n7:\nget state s0 from the environment\n8:\nwhile episode is incomplete do\n9:\nchoose action at following ϵ - greedy policy from policy network q(s, a)\n10:\ntake action and get next state st+1, reward rt+1 from the environment\n11:\nstore transition (st, at, st+1, rt+1) in experience replay pool\n12:\nrandom sample M batch experience (sk, ak, sk+1, rk+1) from the pool\n13:\ncalculate corresponding q(sk, ak) from policy network q(s, a)\n14:\ncalculate yk using target network q′(s, a)\nyk =\n(\nrk+1\nif next state is completed\nrk+1 + γ max\na\nq′(sk+1, a)\notherwise\n15:\noptimize the policy model with gradient (yk −q(sk, ak))2\n16:\nreplace target network with policy network when reach the update step T\n17:\nt ←t + 1, st ←st+1\n18:\nend while\n19: end for\nyk =\n(\nrk+1\nif next state is completed\nrk+1 + γq′(sk+1, arg max\na\nq(sk+1, a))\notherwise\n(2.42)\nPrioritized Experience Replay (PER) introduced a way to eﬃciently sample transitions from\nthe experience replay pool [36]. Instead of uniform random sampling, there is a priority of\neach transition\nP(i) =\npα\ni\nP\nk pα\nk\n(2.43)\nwhere pi > 0 is the priority of transition i and α is the indicator of the priority, especially\nα = 0 when using uniform random sampling. The priority can be measured by TD error δi,\nwhich is the following term\nδi = Ri + γ max\na\nq(si, a) −q(si−1, ai−1)\n(2.44)\nBased on TD error, p(i) can be calculated in two way. The ﬁrst is proportional prioritization\nwhich uses the absolute value of TD error\nCHAPTER 2. LITERATURE SURVEY\n21\np(i) = |δ| + ϵ\n(2.45)\nwhere ϵ is to avoid zero prioritization. The other one is rank-based\np(i) =\n1\nrank(i)\n(2.46)\nwhere rank(i) is the rank of transition by sorting TD error δi. According to Schaul, both\nproportional based and rank based prioritization can speed-up the training but the later one\nis more robust which has better performance when meeting outliers.\nHowever, the random sampling is abandoned after adding priority mechanism which will\nresult in high bias. In other words, those transitions with small TD error are unlikely to be\nsampled and the distribution is changed. Therefore, the ﬁnal model may far from the optimal\npolicy and performance of the agent even be lower than DQN. Important sampling (IS) [29] is\nan eﬀective technique to estimate a distribution by sample data from a diﬀerent distribution.\nGiven a probability density function p(x) over distribution D, with the deﬁnition of the\nexpectation\nEp [f(x)] =\nZ\nD\nf(x)p(x)dx\n(2.47)\nwhere Ep [·] denotes the expectation for x ∼p and f is the integrand. Given another proba-\nbility density function q(x), the expectation can be written as following\nZ\nD\nf(x)p(x)dx =\nZ\nD\nf(x)p(x)\nq(x)\nq(x)dx = Eq\n\u0014f(x)p(x)\nq(x)\n\u0015\n(2.48)\nwhere Eq [·] denotes the expectation for x ∼q. With Monte Carlo integration, the expectation\nEp [f(x)] can be estimated by\n1\nN\nN\nX\ni=1\nf(i)p(i)\nq(i)\n(2.49)\nwhere i is sampled from x. if p(i) is uniform distribution and q(i) refers to Equation 2.43,\nwe have\np(i)\nq(i) = 1\nN ·\n1\nP(i)\n(2.50)\nadding a tunable parameter β, we obtain the importance-sampling weights\nwi = (N · P(i))−β\nmax\ni\nwi\n(2.51)\nCHAPTER 2. LITERATURE SURVEY\n22\nwhere β will decay from a user-deﬁned initial value to 1 and the bias completely disappears\nwhen β = 1. Term max\ni\nwi is used to normalize the weight to increase stability. Use the term\nin Algorithm 3, the update of q function can be modiﬁed as following\nq(s, a) ←q(s, a) + η · wk · ∇(yk −q(sk, ak))2\n(2.52)\nwhere δk is TD error and η is learning rate.\nDueling DQN architecture used a new concept called advantage function which is the sub-\ntraction of the action value function and state value function [52].\nAπ(s, a) = qπ(s, a) −vπ(s)\n(2.53)\nAs shown in Figure 2.10, dueling network architecture use summation of two steams which\nis advantage function and state value function to get the q function. The state values can be\nupdated more accurately with this method.\nFigure 2.10: Architecture comparison between Dueling DQN and DQN [52]\nChapter 3\nMethodology\nThis chapter gives the requirements of the project, introduces the design of reward function\nand shows the preprocessing steps of the input image. The choice of the model, as well as the\narchitecture, will be discussed. Experiment design and evaluation methods will be illustrated\nin the last.\n3.1\nRequirements\n3.1.1\nSoftware Requirement\nConsidering the readability of the code, widely used additional frameworks such as Torch,\nPython is a suitable choice for this project. OpenCV is used to preprocess the image getting\nfrom the environment. Numpy is a Python library which accelerates matrices operations\nwith C. This enables the user to write eﬃcient scientiﬁc computing code with Python. There\nare plenty of deep learning frameworks like Tensorﬂow which has many extensive API and is\nwidely used in industrial products. However, it will take a relatively long time for the beginner\nto fully understand the usage of Tensorﬂow.\nPytorch is a recently developed framework\nwhich is described as ”Numpy with GPU”. The simplicity of Pytorch makes more and more\nacademic researchers using it to implement their new ideas in a much easier way. Because\nT-rex Runner is running on Chrome, the latest Chrome is used here. Gym is a game library\ndeveloped by OpenAI [7]. This framework provides a built-in environment for some famous\ngames such as Atari 2600 and it is easy for the user to customize their own environment.\nTable 3.1 shows all software requirement in this project.\n3.1.2\nHardware Requirement\nAs the game is running on Chrome, it is hard to use a Linux server to perform the experiments.\nAlthough headless Chrome is a plausible choice, there are some environmental issues during\nthe investigation. Therefore, all experiments will be running on the laptop from the author.\nThere will be some limitation such as 6GB GPU memory limits the size of experience replay\n23\nCHAPTER 3. METHODOLOGY\n24\nSoftware\nDescription\nOS\nWindows 10\nProgramming language\nPython 3.7.4\nFramework\nOpenCV, Pytorch, Numpy, Gym\nBrowser\nChrome 76\nTable 3.1: Software requirement\npool. Therefore, parameters related to hardware limitation will be suitably chosen without\ntuning in this project. Table 3.2 lists all hardware information used in this project.\nHardware\nDescription\nCPU\nIntel Core i5-8300H\nRAM\n16G\nGPU\nNvidia GTX 1060 6G\nTable 3.2: Hardware requirement\n3.2\nGame Description\nT-rex Runner is a dinosaur game from Google Chrome oﬄine mode. Everyone can access\nthis link on Chrome to play the game. The target for players is to control the dinosaur\novercoming as many obstacles as possible. The current score of the game will increase by\ntime if the dinosaur keeps alive as shown at the top right corner of Figure 3.1 as well as the\nhighest score. As shown in Figure 3.2, the dinosaur has three actions to choose in every state:\ndo nothing, jump or duck.\nFigure 3.1: A screenshot of T-rex Runner.\nEnvironment plays an important role in reinforcement learning because the agent will improve\nthe policy based on the feedback from it. However, it is diﬃcult to quantify the rewards for\neach action as well as the return for an entire episode. In most research for RL algorithms,\nmodifying reward will not be taken into consideration but it will signiﬁcantly impact the\nCHAPTER 3. METHODOLOGY\n25\n(a) Do nothing\n(b) Jump\n(c) Duck\nFigure 3.2: Three type of actions in T-rex Runner\nperformance of the model because it decides the behavior of the agent. For example, shaping\nreward shows a better performance in Andrew’s experiment[30]. It adds a new term F to\nmodify the original reward based on the goal\nR′ = R + F\n(3.1)\nThe closer the agent towards the goal, the larger the F is. However, the aim of this project\nis to train the agent to play the game and compare the performance between diﬀerent al-\ngorithms. So the eﬀect of reward function will not be taken into consideration and a ﬁxed\nreward function will be used across all experiments.\nSince there is no previous study on T-rex Runner with reinforcement learning, the design of\nreward function is a hard part of this project. Intuitively, the best design is awarding the\nagent for jumping over the obstacles and penalizing it for hitting the obstacles. The jumping\nreward will gradually increase as time goes by. However, object detection in moving pictures\nis required to fulﬁll this goal. As this task is out of the requirements of this project, we\nproposed a naive reward design as shown in Algorithm 4.\nAlgorithm 4 Reward Design in T-rex Runner\n1: if episode is completed then\n2:\nreturn reward as −1\n3: else\n4:\nif agent choose jump then\n5:\nreturn reward as 0\n6:\nelse\n7:\nreturn reward as 0.1\n8:\nend if\n9: end if\nThe basic idea of Algorithm 4 is giving a relatively small reward to the agent if it is alive and\npenalize it when hitting an obstacle. Zero reward for jumping is set to make the dinosaur\nonly jumps if it is very close to obstacles. The unexpected jump will limit the movement in\nthe next few states.\nCHAPTER 3. METHODOLOGY\n26\nAlthough there are three kinds of action in this game as introduced in Section 3.2, duck is\noptional because the agent can overcome the obstacle using jump under the same circum-\nstances. Considering most obstacles in the game are cactus which can only be overcome by\njumping, only two actions (do nothing and jump) will be used in this investigation.\n3.3\nModel Selection\nSince there are only two actions in T-rex Runner, according to the literature review on deep\nreinforcement learning in Section 2.2.1, value-based methods are proved to be powerful to\nhandle this game. Although policy-based methods such as proximal policy gradient is a good\nchoice too, only DQN, double DQN, DQN with prioritized experience replay and dueling\nDQN will be investigated in this project due to the time limitation.\nDeep Q network which is shown in Algorithm 3 is a basic reinforcement learning algorithm\nusing deep learning. According to the result from DeepMind, it is expected to achieve at\nleast human-level results with only DQN.\nDouble DQN mitigates the q value overestimation problems utilizing two advantage of two\nnetworks as shown in Equation 2.42 but it is not expected to achieve a higher performance in\nthis experiment because there is only two actions. The bad eﬀect of overestimation problems\nis not obvious under this circumstance.\nDueling DQN adds an advantage function which is the subtraction of action value function\nand state value function before the output layer in the convolutional neural network as shown\nin Equation 2.53. Since the evaluated game in [52] is a similar racing game overcoming ob-\nstacles compared with T-rex Runner, this algorithm is expected to have a better performance\nthan DQN.\nPrioritized Experience replay improves training eﬃciency by changing the distribution of the\nstored transitions. It assigns the weight for each experience by TD error. There are two\nways to calculate prioritization which is proportional based method and rank-based method.\nAccording to the [36], the former one has a relatively better performance, only this method\nwill be implemented in this investigation due to the time limitation. The performance is\nexpected to be the same as DQN because there is no change in the algorithm but it may be\nfaster to reach the same performance.\n3.4\nImage Preprocessing\nFollowing the preprocessing step in [27, 28], the raw observed image which is in RGB represen-\ntation will be converted to gray-scale representation. To make the network easier to recognize\ndinosaur and obstacles, unnecessary objects such as clouds and scores will be removed. In\nthis step, the color of the background and the object are reversed in order to perform erosion\nand dilation. These two basic morphological operations can help reduce small bright color\nwhich is often noisy data. Finally, the image is resized to 84 × 84 following the recipe from\nDeepMind. Since the movement should be recognized by the neural network, perform the\nCHAPTER 3. METHODOLOGY\n27\nsame preprocessing step for last four frames in the history and stack those four as one data\npoint which is also the input of CNN. The entire process is shown in Figure 3.3.\nFigure 3.3: Preprocessing steps for T-rex Runner.\n3.5\nConvolutional Neural Network Architecture\nThere are two kinds of convolutional neural network used in this project. The basic DQN\nis proposed in [27, 28] which used three convolutional layers and two fully connected layers.\nThe reason for not using pooling layer is to detect the movement of the agent. Both max\npooling and average pooling may make the neural network ignore a very small change in the\nimage. Therefore, there are only convolutional layers in this architecture. The architecture\nfor training the agent using DQN is shown in Figure 3.4.\nDueling architecture is proposed in [52] which divided the q network into two parts. One\nof them is only related to the state value function v(s), the other one is advantage function\nA(s, a) which is aﬀected by both state and action. The ﬁnal action value function is the\nsummation of those two.\nq(s, a; θ, ω1, ω2) = v(s; θ, ω1) + A(s, a; θ, ω2)\n(3.2)\nCHAPTER 3. METHODOLOGY\n28\nFigure 3.4: Convolutional Neural Network architecture for Deep Q Network.\nwhere θ is the shared parameter of CNN, ω1 is the value function only parameter and ω2 is the\nadvantage function only parameter. Both DQN and Dueling DQN are using Algorithm 3, the\nonly diﬀerence is the neural network architecture. RMSprop [48] which is an adaptive gradient\nmethod based on stochastic gradient descent will be used as the optimization algorithm in\nthis project. This is the same optimization method used by DeepMind [27, 28]. Figure 3.5\nshows the process of dueling DQN.\n3.6\nExperiments\n3.6.1\nHyperparameter Tuning\nBefore the comparison of algorithms, hyperparameter tuning is required to get high-performance\nmodels. As mentioned before, the memory size is ﬁxed to 3 × 105 due to the hardware lim-\nitation. Because there is no previous study on this game, and the hyperparameters list in\n[28] have a bad result on this game. All other hyperparameters have to be set to a suitable\nvalue. Grid search is performed to ﬁnd a workable combination of those parameters.\nDue to the time limitation, all parameters will only be slightly modiﬁed and only one hyper-\nparameter will vary during each tuning experiment. The choice of the parameter will consider\nboth score and stability. Each parameter will be tuned with 800 episodes.\nCHAPTER 3. METHODOLOGY\n29\nFigure 3.5: Convolutional Neural Network architecture for Dueling Deep Q Net-\nwork.\n3.6.2\nComparison of diﬀerent Deep Q Network Algorithms\nThere are three improved reinforcement algorithms based on DQN mentioned in Section 2.2.6.\nDouble DQN makes the performance of the agent more stable by solving the overestimated q\nvalue problem. Prioritized experience replay improves the training eﬃciency by sample more\nvaluable transitions. Dueling DQN modiﬁes the neural network architecture to get a better\nestimation of state values.\nIn this experiment, DQN will be ﬁrst used to train the agent based on the hyperparameters\ntuned in Section 3.6.1 and this result will be treated as a baseline across all the experiments.\nDouble DQN, DQN with prioritized experience replay and Dueling DQN will be applied to\nthe agent separately. The performance of those three is expected to be better than DQN\naccording to the related papers.\nDue to time limitation, no combination of those three\nalgorithms will be performed in this project. This section only compares the performance of\neach algorithm.\n3.6.3\nEﬀect of Batch Normalization\nAs mentioned in Section 2.1.5, it is proved that batch normalization can reduce training time\nand mitigate the vanishing gradient problem in a convolutional neural network. However,\nthere is no evidence that this method has the same eﬀect on reinforcement learning. This\nsection will perform experiments on this point. Based on the experiment in Section 3.6.2,\nadding batch normalization in each convolutional layer and compared with the results with\nthe outcome in previous experiments.\nCHAPTER 3. METHODOLOGY\n30\n3.7\nEvaluation\nTo evaluate the performance of the agent, DeepMind used trained agent playing the game\nfor 30 times for up to 5 min and ϵ - greedy policy with ϵ = 0.05 [28]. Considering only one\ngame is investigated in this project, the average score will be used instead of average reward\nbecause the number of jumps in each episode will aﬀect the total reward according to the\ndesigned reward function. The greedy policy will be used in the evaluation stage instead of ϵ\n- greedy policy because the later one will bring randomness to the decision which will aﬀect\nthe performance of the trained model. Therefore, the trained agent will play the game for 30\ntimes without time limitation and using greedy policy. All outcomes will be compared with\nthe results from a human expert.\nThe average scores during the training stage will be shown graphically. This is a clear way\nto show the learning eﬃciency of each algorithm. Both graphical and statistical results such\nas mean, variance and median will be analyzed. However, only statistical results will be\nanalyzed in the testing stage because the trained model for each algorithm are the same and\nthere is no increasing trend can be shown like in the training stage. These results will be\nvisualized with a boxplot.\nChapter 4\nResults and Discussion\n4.1\nHyper Parameter Tuning\nThe value of hyperparameters may aﬀect the performance of the model. However, there are\nso many parameters in reinforcement learning including optimization algorithm parameters\nsuch as learning rate. This may take a long time to ﬁnd the optimal combination of these\nparameters using a grid search. Since there is no metric like accuracy in RL which can easily\nreﬂect the performance of the model, we assume each parameter is independent of others.\nTherefore, each parameter can be tuned one after another. Because the objective of this\nproject is to compare the performance between diﬀerent algorithms and the eﬀect of batch\nnormalization, those tuned parameters by DQN will be used across all the experiments. The\nstart hyperparameters of DQN are shown in Table 4.1.\nHyper parameter\nValue\nDescription\nMemory Size\n3 × 105\nSize of experience replay pool\nBatch Size\n128\nSize of minibatch to optimize model\nGamma\n0.99\nDiscount factor\nInitial ϵ0\n1 × 10−1\nExplore probability at the start of the training\nFinal ϵ′\n1 × 10−3\nEnd point of explore probability in ϵ decay\nExplore steps\n1 × 105\nNumber of steps for ϵ decay from ϵ0 to ϵ′\nLearning Rate\n1 × 10−4\nLearning speed of the model\nTable 4.1: Hyper parameters used in all experiments\n4.1.1\nLearning Rate\nLearning rate controls the learning speed of the model, too large value will result in divergence\nand too small value may double the training time.\nFigure 4.1 shows four diﬀerent values of learning rate. Obviously, 1 × 10−5 is too small and\nthere is no increase trend during the entire process. Both 1 × 10−4 and 5 × 10−5 make the\n31\nCHAPTER 4. RESULTS AND DISCUSSION\n32\nFigure 4.1: Hyper parameter tuning for learning rate.\nscore unstable after 50th epoch. Considering the stability and 200 epochs will be trained in\nformal experiment, 2 × 10−5 will be chosen as learning rate.\n4.1.2\nBatch Size\nBatch size deﬁnes how many transitions will be used to update the neural network which may\naﬀect the training speed. But as mentioned in 2.2.1, too big size will cause the dependency\nproblems which may largely aﬀect the performance of the model.\nFigure 4.2: Hyper parameter tuning for batch size.\nAs shown in Figure 4.2, the average score of three curves at epoch 80 are all around 800.\nAmong those three, the most stable one is batch size 128.\nCHAPTER 4. RESULTS AND DISCUSSION\n33\n4.1.3\nEpsilon\nϵ - greedy policy determines the probability of exploration. In some games, especially with\nhigh action spaces, this value can aﬀect how good the model will converge. However, there\nare only two actions in T-rex Runner so it is unnecessary to random choose action at the\nbegin. Instead of initializing ϵ to 1 as DeepMind did in their paper [28], the start value is set\nto 0.1 in this model.\nFigure 4.3: Hyper parameter tuning for explore probability ϵ.\nAll experiments achieve acceptable results in Figure 4.3 except the one with ﬁxed ϵ = 0.1. In\nthis case, we select ϵ from 0.1 to 0.0001 but either of those three can be chosen according to\nthis graph. This experiment also demonstrates the positive eﬀect of linear annealing for ϵ.\n4.1.4\nExplore Step\nExplore step is the number of steps required to anneal ϵ from 0.1 to 0.0001. As mentioned\nthat hyperparameters related to exploration will not aﬀect too much in this game. The most\nstable one will be selected from Figure 4.4 which is 1 × 105.\n4.1.5\nGamma\nDiscount factor decides how far-sighted the agent will be. Too small value will make the\nagent consider more about the current reward and too big value will make the agent pay the\nsame attention to rewards after this time point. This may confuse the agent about which\naction leads to a high or low return.\nFigure 4.5 shows the average score for four diﬀerent gamma. Obviously, γ = 0.9 make the\nagent short-sighted and there is no signiﬁcant change during 80 epochs. When γ ≥0.999, the\naverage score ﬂuctuates widely after 50th epoch. Since γ = 0.99 has a gradually increasing\ntrend, this will be used as the ﬁnal discount factor.\nCHAPTER 4. RESULTS AND DISCUSSION\n34\nFigure 4.4: Hyper parameter tuning for explore steps.\nFigure 4.5: Hyper parameter tuning for discount factor γ.\n4.2\nTraining Results\nThe tuned hyperparameters from the previous experiment are listed in Table 4.2. Although\nthese parameters are tuned by DQN algorithm, they are expected to ﬁt other three improved\nalgorithms which are Double DQN, Dueling DQN and DQN with prioritized experience replay\nbecause there is no big diﬀerence among them. All algorithms will be only trained with 200\nepochs because of the time limitation. The total training time for each algorithm is shown\nin the last column of Table 4.4\nCHAPTER 4. RESULTS AND DISCUSSION\n35\nHyper parameter\nValue before tune\nValue after tune\nMemory Size\n3 × 105\n3 × 105\nBatch Size\n128\n128\nGamma\n0.99\n0.99\nInitial ϵ0\n1 × 10−1\n1 × 10−1\nFinal ϵ′\n1 × 10−3\n1 × 10−4\nExplore steps\n1 × 105\n1 × 105\nLearning Rate\n1 × 10−4\n2 × 10−5\nTable 4.2: Hyperparameters used in all experiments\n4.2.1\nDQN\nFigure 4.6 shows the result of DQN algorithm for 200 epochs with tuned parameters. A\ngradually increased average score can be seen from this graph. This not only proves that the\nagent can play the game through DQN but also shows that the design of the reward function\nis relatively reasonable. This result will be treated as a baseline and will be used to compare\nwith other algorithms.\nFigure 4.6: Training result for DQN.\n4.2.2\nDouble DQN\nDouble DQN has a similar performance in training compared with DQN. As mentioned\nbefore, the eﬀect of q overestimation is not so signiﬁcant in T-rex Runner because there are\nonly two actions. As shown in Figure 4.7, there are four data points with average scores\nbelow 200 while all average scores are above this value in DQN.\nCHAPTER 4. RESULTS AND DISCUSSION\n36\nFigure 4.7: Training result for Double DQN compared with DQN.\n4.2.3\nDueling DQN\nSurprisingly, dueling DQN shows an incredible training performance after 150th epoch while\nthe curve before that time seems similar. In Figure 4.8, the average score is above 5000 which\nis ten times higher than the maximum average score in DQN. However, these scores have a\nhigh variance which ﬂuctuates widely between 1000 and 5000. From the graph, the training\nprocess of dueling DQN is stable before 150th epoch and end up with an increasing trend.\nSince we tuned all hyperparameters based on DQN, these values may not be the best for\ndueling network which results in the stable and relatively low average scores before 150th\nepoch.\nFigure 4.8: Training result for Dueling DQN compared with DQN.\nCHAPTER 4. RESULTS AND DISCUSSION\n37\n4.2.4\nDQN with Prioritized Experience Replay\nAnother important ﬁnding in this section is the performance of prioritized experience replay.\nThis is expected to have a shorter training time and a higher performance compared with\nDQN. But the result shown in Figure 4.9 suggests that the agent failed to learn to play the\ngame with this method. There are two reasons for that.\nFigure 4.9: Training result for DQN with prioritized experience replay compared\nwith DQN.\nOne problem is from the algorithm. Compared with DQN, there are two extra steps have been\napplied to PER: weight calculation and prioritization update. Following the implementation\nin [36], sum tree which is a data structure with time complexity O(log N) for sampling and\nupdating is used to store transitions instead of a linear list to accelerate memory related\nmanipulation. The training time of PER is twice more than the one of DQN because of the\nbatch size. Since we know that all sampled transitions will be traversed when updating the\nprioritization, the larger batch size is the longer time is required to perform this operation.\nTable 4.3 shows that this process is very time-consuming even using the batch size 32. These\ndata are extracted from the training results choosing the same score of 43. The step size is\nthe average value from ten records.\nAlgorithm\nScore\nBatch Size\nStep Size\nDQN\n43\n128\n180\nDQN with PER\n43\n128\n7\nDQN with PER\n43\n32\n22\nTable 4.3: Step size diﬀerence between DQN and DQN with PER\nThe other problem is from the game. Because this game is based on Chrome, it continues\nrunning when performing optimization while the game from oﬃcial OpenAI Gym is paused\nduring this operation. Therefore, there is a delayed time before sending the action to Chrome.\nThis inﬂuence is enlarged in prioritized experience replay since the time for update operation\nCHAPTER 4. RESULTS AND DISCUSSION\n38\nwith batch size 128 takes approximately 10 times longer than normal DQN.\nChange the choice of hyperparameter can mitigate the ﬁrst problem but the result is not\nas good as other algorithms. One thing we can expect is PER is unable to help the agent\nto get a higher score under this circumstance because the game speed will increase as time\ngoes by. Since the time for updating the prioritization will not change too much, the time\ninterval between two consistent decisions will be longer. This may limit the performance of\nthe model. To eliminate the high computational eﬀect from updating prioritization, the best\nway is to redevelop the game but due to the time limitation and the primary objective of\nthis study, this result will be used as we can still compare the eﬀect of batch normalization\non this algorithm.\n4.2.5\nBatch Normalization\nSince the aim of this experiment is to ﬁnd how batch normalization aﬀects DQN algorithms,\neach result will be compared with the one without batch normalization which is shown in\nFigure 4.10.\nFigure 4.10: Batch normalization on DQN, Double DQN, Dueling DQN and\nDQN with prioritized experience replay\nFrom Figure 4.10, we can see that batch normalization can increase the mean of average\nscores in all experiments. But this also brings high variance which makes the average score\nCHAPTER 4. RESULTS AND DISCUSSION\n39\ndiverge. According to the top-left graph, the ﬁrst time for DQN agent to reach the average\n1000 is approximately 150th epoch while the agent using DQN with batch normalization\nreach the same average score at 60th epoch and it is easy for it to get the higher score after\nthat time. Double DQN curve has a similar trend but batch normalization in both of them\nalso result in wide ﬂuctuation. It is hard to say whether dueling network beneﬁts from the\nbatch normalization because there is a signiﬁcant increase trend on the bottom left graph.\nHowever, it is still can be seen that BN enable the agent to reach the same performance much\nearlier from 20th epoch to 90th epoch. For DQN with prioritized experience replay, even the\nperformance is limited by the game itself, the one with batch normalization still can get a\nrelatively higher score.\n4.2.6\nFurther Discussion\nAs graphical results and some explanation of them are shown above, this part will discuss\nnumerical results from the experiments. Table 4.4 shows some statistical data fro training\nprocess. The maximum score is pointless in most games but considering T-rex Runner is a\nracing game, we still include this in the table. The last three columns are percentile data\nwhich are calculated by sorting in ascending order and ﬁnding the x% observation. So 50%\nis the same as the median. The last column shows the training time for each algorithm.\nAlgorithm\nMean\nStd\nMax\n25%\n50%\n75%\nTime (h)\nDQN\n537.50\n393.61\n1915\n195.75\n481\n820\n25.87\nDouble DQN\n443.31\n394.01\n2366\n97.75\n337\n662.25\n21.36\nDueling DQN\n839.04\n1521.40\n25706\n155\n457\n956.5\n35.78\nDQN with PER\n43.50\n2.791\n71\n43\n43\n43\n3.31\nDQN (BN)\n777.54\n917.26\n8978\n97.75\n462.5\n1139.25\n32.59\nDouble DQN (BN)\n696.43\n758.81\n5521\n79\n430.5\n1104.25\n29.40\nDueling DQN (BN)\n1050.26\n1477.00\n14154\n84\n541.5\n1520\n40.12\nDQN with PER (BN)\n46.14\n7.54\n98\n43\n43\n43\n3.44\nTable 4.4: Training results\nIgnoring the result from prioritized experience replay because of the inappropriate game\nenvironment, all algorithms achieve great results according to Table 4.4. Two algorithms\nwith dueling network stand out from them. The one with batch normalization has the mean\nover 1000 which is 200 more than the one without BN. But the later one got the maximum\nscore of 25706 which means the agent can keep running for around half an hour in one episode.\nHowever, both of them have high variance which exceed the mean.\nDouble DQN both with BN and without BN perform worse than DQN. This indicates that\ndouble DQN may reduce the performance in low dimension action space. But batch normal-\nization shortens the gap between those two algorithms which can be seen from the median\nand 75% percentile.\nAlthough most of statistical metrics are improved by batch normalization, the variance is\nmuch higher than before. As shown in the table, the variance from DQN with BN is twice\nCHAPTER 4. RESULTS AND DISCUSSION\n40\nmore than the one without BN. Only the variance from dueling network is lower after BN.\nBut it is reasonable because there is an incredible increase in the very later stage of the\ntraining shown in Figure 4.10.\n4.3\nTesting Results\nAfter training the agent for 2000 episodes, we use the latest model with greedy policy and\nplay T-rex Runner for 30 times with each algorithm. Figure 4.11 shows the boxplot of those\nresults as well as the collected data from the human expert. It is obvious that the agent\ntrained by DQN with prioritized experience replay fail to learn to play the game because of\nthe game environment issue discussed in the last section. It is surprising that the performance\nof double DQN is far from satisfactory even though it has similar training results compared\nwith DQN. Table 4.5 shows that the mean of DQN results is three times higher than the one\nfrom double DQN. Dueling DQN algorithm achieves the highest score even though it still has\nthe highest variance which is three times more than the variance from DQN.\nFigure 4.11: Boxplot for test result with eight diﬀerent algorithms\nAccording to Table 4.5, batch normalization improves the performance of the model regardless\nof algorithms and even the mean of DQN with PER is increased. However, it is not easy to\nsay the eﬀect of BN in dueling DQN is positive or not. From Figure 4.11, the one without\nBN has more outliers which results in high variance even though its mean is higher. Consider\nthe median which is not sensitive with the outlier data, the one with BN is better and the\nminimum score is more than 200 which stands out from other algorithms. Since score 43\nindicates the ﬁrst time the agent meets the obstacle, it is easy to infer that all trained model\nfails to jump over the ﬁrst cacti at least once except dueling DQN with BN. But dueling DQN\nis not fully trained which can be seen from the training result in Figure 4.8. That’s also one\nCHAPTER 4. RESULTS AND DISCUSSION\n41\nreason for high variance as we can see in the boxplot. The agent trained with dueling DQN\nachieved over 8000 at least three times.\nAlgorithm\nMean\nStd\nMin\nMax\n25%\n50%\n75%\nHuman\n1121.9\n499.91\n268\n2384\n758\n992.5\n1508.5\nDQN\n1161.30\n814.36\n45\n3142\n321.5\n1277\n1729.5\nDouble DQN\n340.93\n251.40\n43\n942\n178.75\n259.5\n400.75\nDueling DQN\n2383.03\n2703.64\n44\n8943\n534.75\n1499.5\n2961\nDQN with PER\n43.30\n1.64\n43\n52\n43\n43\n43\nDQN (BN)\n2119.47\n1595.49\n44\n5823\n1218.75\n1909.5\n2979.75\nDouble DQN (BN)\n382.17\n188.74\n43\n738\n283.75\n356\n525.5\nDueling DQN (BN)\n2083.37\n1441.50\n213\n5389\n1142.5\n1912.5\n2659.75\nDQN with PER (BN)\n45.43\n7.384\n43\n78\n43\n43\n43\nTable 4.5: Test results\nChapter 5\nConclusions and Future Works\nThe project aims to create an agent trained by four types of algorithms to play T-rex Runner\nand investigate the inﬂuence of batch normalization in reinforcement learning.\nThe former aim is reached except the prioritized experience replay due to the game en-\nvironment issue. However, all other algorithms are successfully implemented and achieve\ngreat results, especially DQN and dueling DQN. Both of them can achieve better results\nthan human experts. Batch normalization has shown relatively positive eﬀects for all DQN\nalgorithms in this project despite the unstable average score in the training stage.\nIn further studies, the game environment should be ﬁrst redeveloped to add a pause function\nwhen the neural network is calculating q values or doing optimization. Prioritized experience\nreplay can be tested after that. In this project, only the proportional based method has been\nimplemented, so rank-based prioritization can also be investigated in the future. Further\ncombination of algorithms can be developed such as dueling DQN with prioritized experience\nreplay. Policy-based algorithms such as PPO can also be implemented to train the agent.\nThere is one interesting idea which has not been implemented yet. Considering the moving\nspeed of the obstacles are gradually increasing, we can divide the game into several stages.\nEach stage has a neural network which is initialized by the previous stage and will be trained\nindependently. The intuition of this idea is that the consequence of jumping will change\nwhen the agent is running in diﬀerent stages. This may also be one of the reasons for a high\nvariance because when the agent has learned how to get a better score in the later stage, it\nforgets the best policy in the early stage.\n42\nBibliography\n[1] D. M. Allen.\nMean square error of prediction as a criterion for selecting variables.\nTechnometrics, 13(3):469–475, 1971.\n[2] A. G. Barto, R. S. Sutton, and C. W. Anderson. Neuronlike adaptive elements that\ncan solve diﬃcult learning control problems. IEEE transactions on systems, man, and\ncybernetics, (5):834–846, 1983.\n[3] R. Bellman. A markov decision process. journal of mathematical mechanics. 1957.\n[4] R. Bellman.\nCombinatorial processes and dynamic programming.\nTechnical report,\nRAND CORP SANTA MONICA CA, 1958.\n[5] R. Bellman et al.\nThe theory of dynamic programming.\nBulletin of the American\nMathematical Society, 60(6):503–515, 1954.\n[6] Y.-L. Boureau, F. Bach, Y. LeCun, and J. Ponce. Learning mid-level features for recog-\nnition. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition, pages 2559–2566. Citeseer, 2010.\n[7] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and\nW. Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\n[8] W. A. Clark and B. G. Farley. Generalization of pattern recognition in a self-organizing\nsystem. In Proceedings of the March 1-3, 1955, western joint computer conference, pages\n86–91. ACM, 1955.\n[9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248–255. Ieee, 2009.\n[10] K. Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of\npattern recognition unaﬀected by shift in position. Biological cybernetics, 36(4):193–202,\n1980.\n[11] R. Ge, F. Huang, C. Jin, and Y. Yuan. Escaping from saddle pointsonline stochastic\ngradient for tensor decomposition. In Conference on Learning Theory, pages 797–842,\n2015.\n[12] K. He, X. Zhang, S. Ren, and J. Sun. Resnet-deep residual learning for image recognition.\nResNet: Deep Residual Learning for Image Recognition, 2015.\n43\nBIBLIOGRAPHY\n44\n[13] P. Hu. Matrix calculus: Derivation and simple application. Technical report, 2012.\n[14] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger.\nDensely connected\nconvolutional networks. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 4700–4708, 2017.\n[15] S. Ioﬀe and C. Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\n[16] A. H. Klopf. Brain function and adaptive systems: a heterostatic theory. Technical\nreport, AIR FORCE CAMBRIDGE RESEARCH LABS HANSCOM AFB MA, 1972.\n[17] A. H. Klopf. The hedonistic neuron: a theory of memory, learning, and intelligence.\nToxicology-Sci, 1982.\n[18] V. R. Konda and J. N. Tsitsiklis. Actor-critic algorithms. In Advances in neural infor-\nmation processing systems, pages 1008–1014, 2000.\n[19] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convo-\nlutional neural networks. In Advances in neural information processing systems, pages\n1097–1105, 2012.\n[20] S. Lawrence, C. L. Giles, A. C. Tsoi, and A. D. Back. Face recognition: A convolutional\nneural-network approach. IEEE transactions on neural networks, 8(1):98–113, 1997.\n[21] Y. LeCun, L. Bottou, Y. Bengio, P. Haﬀner, et al. Gradient-based learning applied to\ndocument recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n[22] L.-J. Lin. Self-improving reactive agents based on reinforcement learning, planning and\nteaching. Machine learning, 8(3-4):293–321, 1992.\n[23] M. Lin, Q. Chen, and S. Yan. Network in network. arXiv preprint arXiv:1312.4400,\n2013.\n[24] W. S. McCulloch and W. Pitts. A logical calculus of the ideas immanent in nervous\nactivity. The bulletin of mathematical biophysics, 5(4):115–133, 1943.\n[25] M. Minsky and S. Papert. Perceptron: an introduction to computational geometry. The\nMIT Press, Cambridge, expanded edition, 19(88):2, 1969.\n[26] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and\nK. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Interna-\ntional conference on machine learning, pages 1928–1937, 2016.\n[27] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Ried-\nmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602,\n2013.\n[28] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves,\nM. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep\nreinforcement learning. Nature, 518(7540):529, 2015.\nBIBLIOGRAPHY\n45\n[29] R. M. Neal. Annealed importance sampling. Statistics and computing, 11(2):125–139,\n2001.\n[30] A. Y. Ng, D. Harada, and S. Russell. Policy invariance under reward transformations:\nTheory and application to reward shaping. In ICML, volume 99, pages 278–287, 1999.\n[31] F. Rosenblatt. The perceptron, a perceiving and recognizing automaton Project Para.\nCornell Aeronautical Laboratory, 1957.\n[32] F. Rosenblatt. Perceptron simulation experiments. Proceedings of the IRE, 48(3):301–\n309, 1960.\n[33] D. E. Rumelhart, G. E. Hinton, R. J. Williams, et al.\nLearning representations by\nback-propagating errors. Cognitive modeling, 5(3):1, 1988.\n[34] G. A. Rummery and M. Niranjan.\nOn-line Q-learning using connectionist systems,\nvolume 37. University of Cambridge, Department of Engineering Cambridge, England,\n1994.\n[35] A. J. Samuel. Aerosol dispensers and like pressurized packages, Sept. 15 1959. US Patent\n2,904,229.\n[36] T. Schaul, J. Quan, I. Antonoglou, and D. Silver. Prioritized experience replay. arXiv\npreprint arXiv:1511.05952, 2015.\n[37] D. Scherer, A. M¨uller, and S. Behnke. Evaluation of pooling operations in convolutional\narchitectures for object recognition. In International conference on artiﬁcial neural net-\nworks, pages 92–101. Springer, 2010.\n[38] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov.\nProximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n[39] D. Silver. University college london course on reinforcement learning, 2015.\n[40] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot,\nL. Sifre, D. Kumaran, T. Graepel, et al. Mastering chess and shogi by self-play with a\ngeneral reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017.\n[41] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556, 2014.\n[42] R. S. Sutton. Learning to predict by the methods of temporal diﬀerences. Machine\nlearning, 3(1):9–44, 1988.\n[43] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press,\n2018.\n[44] R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour. Policy gradient methods for\nreinforcement learning with function approximation. In Advances in neural information\nprocessing systems, pages 1057–1063, 2000.\nBIBLIOGRAPHY\n46\n[45] C. Szegedy, S. Ioﬀe, V. Vanhoucke, and A. A. Alemi. Inception-v4, inception-resnet and\nthe impact of residual connections on learning. In Thirty-First AAAI Conference on\nArtiﬁcial Intelligence, 2017.\n[46] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Van-\nhoucke, and A. Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 1–9, 2015.\n[47] E. Thorndike. Animal intelligence; experimental studies, by edward l. thorndike, 1911.\n[48] T. Tieleman and G. Hinton. Lecture 6.5-rmsprop, coursera: Neural networks for machine\nlearning. University of Toronto, Technical Report, 2012.\n[49] R. Vaillant, C. Monrocq, and Y. Le Cun. Original approach for the localisation of objects\nin images. IEE Proceedings-Vision, Image and Signal Processing, 141(4):245–250, 1994.\n[50] H. Van Hasselt, A. Guez, and D. Silver. Deep reinforcement learning with double q-\nlearning. In Thirtieth AAAI conference on artiﬁcial intelligence, 2016.\n[51] A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. J. Lang.\nPhoneme recog-\nnition using time-delay neural networks. Backpropagation: Theory, Architectures and\nApplications, pages 35–61, 1995.\n[52] Z. Wang, T. Schaul, M. Hessel, H. Van Hasselt, M. Lanctot, and N. De Freitas. Dueling\nnetwork architectures for deep reinforcement learning. arXiv preprint arXiv:1511.06581,\n2015.\n[53] C. J. C. H. Watkins. Learning from delayed rewards. 1989.\n[54] B. Widrow, N. K. Gupta, and S. Maitra. Punish/reward: Learning with a critic in\nadaptive threshold systems.\nIEEE Transactions on Systems, Man, and Cybernetics,\n(5):455–465, 1973.\n[55] B. Widrow and M. E. Hoﬀ. Adaptive switching circuits. Technical report, Stanford Univ\nCa Stanford Electronics Labs, 1960.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-09-10",
  "updated": "2019-09-10"
}