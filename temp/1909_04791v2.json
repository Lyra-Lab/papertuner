{
  "id": "http://arxiv.org/abs/1909.04791v2",
  "title": "A Survey of Techniques All Classifiers Can Learn from Deep Networks: Models, Optimizations, and Regularization",
  "authors": [
    "Alireza Ghods",
    "Diane J Cook"
  ],
  "abstract": "Deep neural networks have introduced novel and useful tools to the machine\nlearning community. Other types of classifiers can potentially make use of\nthese tools as well to improve their performance and generality. This paper\nreviews the current state of the art for deep learning classifier technologies\nthat are being used outside of deep neural networks. Non-network classifiers\ncan employ many components found in deep neural network architectures. In this\npaper, we review the feature learning, optimization, and regularization methods\nthat form a core of deep network technologies. We then survey non-neural\nnetwork learning algorithms that make innovative use of these methods to\nimprove classification. Because many opportunities and challenges still exist,\nwe discuss directions that can be pursued to expand the area of deep learning\nfor a variety of classification algorithms.",
  "text": "arXiv:1909.04791v2  [cs.LG]  27 Sep 2019\nA SURVEY OF TECHNIQUES ALL CLASSIFIERS CAN LEARN\nFROM DEEP NETWORKS: MODELS, OPTIMIZATIONS, AND\nREGULARIZATION\nA PREPRINT\nAlireza Ghods\nDepartment of Computer Science\nWashington State University\nPullman, WA 99163\nalireza.ghods@wsu.edu\nDiane J. Cook\nDepartment of Computer Science\nWashington State University\nPullman, WA 99163\ndjcook@wsu.edu\nSeptember 30, 2019\nABSTRACT\nDeep neural networks have introduced novel and useful tools to the machine learning community.\nOther types of classiﬁers can potentially make use of these tools as well to improve their performance\nand generality. This paper reviews the current state of the art for deep learning classiﬁer technologies\nthat are being used outside of deep neural networks. Non-network classiﬁers can employ many\ncomponents found in deep neural network architectures. In this paper, we review the feature learning,\noptimization, and regularization methods that form a core of deep network technologies. We then\nsurvey non-neuralnetwork learning algorithms that make innovative use of these methods to improve\nclassiﬁcation. Because many opportunities and challenges still exist, we discuss directions that can\nbe pursued to expand the area of deep learning for a variety of classiﬁcation algorithms.\nKeywords Deep Learning · Deep Neural Networks, · , · Regularization\n1\nIntroduction\nThe objective of supervised learning algorithms is to identify an optimal mapping between input features and output\nvalues based on a given training dataset. A supervised learning method that is attracting substantial research and\nindustry attention is Deep Neural Networks (DNN). DNNs have a profound effect on our daily lives; they are found\nin search engines [1] [2], self-driving cars [3] [4] [5], health care systems [6], and consumer devices such as smart-\nphones and cameras [7]. Convolutional Neural Networks (CNN) have become the standard for processing images\n[8] [9] [10], whereas Recurrent Neural Networks (RNN) dominate the processing of sequential data such as text and\nvoice [11] [12] [13] [14]. DNNs allow machines to automatically discover the representations needed for detection or\nclassiﬁcation of raw input [15]. Additionally, the neural network community developed unsupervised algorithms to\nhelp with the learning of unlabeled data. These unsupervised methods have found their way to real-world applications,\nsuch as creating generative adversarial networks (GANs) that design clothes [16]. The term deep has been used to\ndistinguish these networks from shallow networks which have only one hidden layer; in contrast, DNNs have multiple\nhidden layers. The two terms deep learning and deep neural networks have been used synonymously. However, we\nobserve that deep leaning itself conveys a broader meaning, which can also shape the ﬁeld of machine learning outside\nthe realm of neural network algorithms.\nThe remarkable recent DNN advances were made possible by the availability of massive amounts of computational\npower and labeled data. However, these advances do not overcome all of the difﬁculties associated with DNNs.\nFor example, there are many real-world scenarios, such as analyzing power distribution data [17], for which large\nannotated datasets do not exist due to the complexity and expense of collecting data. While applications like clinical\ninterpretation of medical diagnoses require that the learned model be understandable, most DNNs resist interpretation\nA PREPRINT - SEPTEMBER 30, 2019\ndue to their complexity [18]. DNNs can be insensitive to noisy training data [19] [20] [21], and they also require\nappropriate parameter initialization to converge [22] [23].\nDespite these shortcomings, DNNs have reported higher predictive accuracy than other supervised learning methods\nfor many datasets, given enough supervised data and computational resources. Deep models offer structural advan-\ntages that may improve the quality of learning in complex datasets as empirically shown by Bengio [24]. Recently,\nresearchers have designed hybrid methods which combine unique DNN techniques with other classiﬁers to address\nsome of these identiﬁed problems or to boost other classiﬁers. This survey paper investigates these methods, reviewing\nclassiﬁers which have adapted DNN techniques to alternative classiﬁers.\n1.1\nResearch Objectives and Outline\nWhile DNN research is growing rapidly, this paper aims to draw a broader picture of deep learning methods. Although\nsome studies provide evidence that DNN models offer greater generalization than classic machine learning algorithms\nfor complex data [25] [26] [27] [28] [29], there is no “silver bullet” approach to concept learning [30]. Numerous\nstudies comparing DNNs and other supervised learning algorithms [31] [32] [33] [34] [35] observe that the choice of\nalgorithm depends on the data - no ideal algorithm exists which generalizes optimally on all types of data. Recognizing\nthe unique and important role other classiﬁers thus play, we aim to investigate how non-network machine learning\nalgorithms can beneﬁt from the advances in deep neural networks. Many deep learning survey papers have been\npublished that provide a primer on the topic [36] or highlight diverse applications such as object detection [37], medical\nrecord analysis [38], activity recognition [39], and natural language processing [40]. In this survey, we do not focus\nsolely on deep neural network models but rather on how deep learning can inspire a broader range of classiﬁers. We\nconcentrate on research breakthroughs that transform non-network classiﬁers into deep learners. Further, we review\ndeep network techniques such as stochastic gradient descent that can be used more broadly, and we discuss ways in\nwhich non-network models can beneﬁt from network-inspired deep learning innovations.\nThe literature provides evidence that non-network models may offer improved generalizability over deep networks,\ndepending on the amount and type of data that is available. By surveying methods for transforming non-network\nclassiﬁers into deep learners, these approaches can become stronger learners. To provide evidence of the need for con-\ntinued research on this topic, we also implement a collection of shallow and deep learners surveyed in this paper, both\nnetwork and non-network classiﬁers, to compare their performance. Figure 1 highlights deep learning components\nthat we discuss in this survey. This graph also summarizes the deep classiﬁers that we survey and the relationships\nthat we highlight between techniques.\nDeep Learning\nTraining Methodology\nOptimization\nGradient Decent\nDNDF\nmGBDT\nML-SVM\nRegularization\nFeature Penalty\nGRRF\nNLP-SVM\nRRF\nSCAD-SVM\nDropout\nDART\nClassiﬁers\nNetwork-Based\nAE\nVAE\nCNN\nGAN\nMLP\nSNN\nRNN\nLSTM\nGRU\nTree-Based\nANT\nDF\nDNDF\neForest\nFSDT\nGAF\nmGBDT\nSDF\nDNDT\nSVM-Based\nDeep SVM\nDTA-LS-SVM\nML-SVM\nR2SVM\nStatistical-Based\nDBN\nDGP\nDKF\nHybrid\nCondNN\nDBT\nDCCA\nDeep PCA\nDNDF\nLMM\nFigure 1: Content map of the methods covered in this survey.\n2\nA PREPRINT - SEPTEMBER 30, 2019\n2\nBrief Overview of Deep Neural Networks\n2.1\nThe Origin\nIn 1985, Rosenblatt introduced the Perceptron [41], an online binary classiﬁer which ﬂows input through a weight\nvector to an output layer. Perceptron learning uses a form of gradient descent to adjust the weights between the input\nand output layers to optimize a loss function [42]. A few years later, Minsky proved that a single-layer Perceptron is\nunable to learn nonlinear functions, including the XOR function [43]. Multilayer perceptrons (MLPs, see Table 4 for\na complete list of abbreviations) addressed the nonlinearity problem by adding layers of hidden units to the networks\nand applying alternative differentiable activation functions, such as sigmoid, to each node. Stochastic gradient descent\nwas then applied to MLPs to determine the weights between layers that minimize function approximation errors [44].\nHowever, the lack of computational power caused DNN research to stagnate for decades, and other classiﬁers rose\nin popularity. In 2006, a renaissance began in DNN research, spurred by the introduction of Deep Belief Networks\n(DBNs) [45].\n2.2\nDeep Neural Network Architectures\nDue to the increasing popularity of deep learning, many DNN architectures have been introduced with variations such\nas Neural Turing Machines [46] and Capsule Neural Networks [47]. In this paper, we summarize the general form of\nDNNs together with architectural components that not only appear in DNNs but can be incorporated into other models.\nWe start by reviewing popular types of DNNs that have been introduced and that play complementary learning roles.\n2.3\nSupervised Learning\n2.3.1\nMultilayer Perceptron\nA multilayer perceptron (MLP) is one of the essential bases of many deep learning algorithms. The goal of a MLP\nis to map input X to class y by learning a function y = f(X, θ), where θ represents the best possible function\napproximation. For example, in Figure 2 the MLP maps input X to y using function f(x) = f (3)(f (2)(f (1)(x))),\nwhere f (1) is the ﬁrst layer, f (2) is the second layer, and f (3) represents the third, output layer. This chain structure\nis a common component of many DNN architectures. The network depth is equal to the length of the chain, and the\nwidth of each layer represents the number of nodes in that layer [48].\nIn networks such as the MLP, the connections are not cyclic and thus belong to a class of DNNs called feedforward\nnetworks. Feedforward networks move information in only one direction, from the input to the output layer. Figure 2\ndepicts a particular type of feedforward network which is a fully-connected multilayer perceptron because each node\nat one layer is connected to all of the nodes at the next layer. Special cases of feedforward networks and MLPs have\ndrawn considerable recent attention, which we describe next.\n...\n...\n...\n...\nx1\nx2\nx3\nxm\nH1\nHk\nH1\nHj\ny1\nyn\nInput\nlayer\nHidden\nlayer\nHidden\nlayer\nOuput\nlayer\nFigure 2: An illustration of a three-layered MLP with j nodes at the ﬁrst hidden layer and k at the second layer.\n2.3.2\nDeep Convolutional Neural Network\nA convolutional neural network (CNN) [49] is a specialized class of feedforward DNNs for processing data that can\nbe discretely presented. Examples of data that can beneﬁt from CNNs include time series data that can be presented\nas samples of discrete regular time intervals and image data presented as samples of 2-D pixels at discrete locations.\n3\nA PREPRINT - SEPTEMBER 30, 2019\nMost CNNs involve three stages: a convolution operation; an activation function, such as the rectiﬁed linear activation\n(ReLU) function [50]; and a pooling function, such as max pooling [51]. A convolution operation is a weighted average\nor smooth estimation of a windowed input. One of the strengths of the convolution operation is that the connections\nbetween nodes in a network become sparser by learning a small kernel for unimportant features. Another beneﬁt of\nconvolution is parameter sharing. A CNN makes an assumption that a kernel learned for one input position can be\nused at every position, in contrast to a MLP which deploys a separate element of a weight matrix for each connection.\nApplying the convolution operator frequently improves the network’s learning ability.\nA pooling function replaces the output of speciﬁc nearby nodes by their statistical summary. For example, the max-\npooling function returns the maximum of a rectangular neighborhood. The motivation behind adding a pooling layer\nis that statistically down-sampling the number of features makes the representation approximately invariant to small\ntranslations of the input by maintaining the essential features. The ﬁnal output of the learner is generated via a Fully-\nConnected (FC) layer that appears after the convolutional and max-pooling layers (see Figure 3 for an illustration of\nthe process).\nInput\nlayer\nConvolutional\nlayer + ReLU\nMax Pooling\nlayer\nConvolutional\nlayer + ReLU\nMax Pooling\nlayer\nFC\nlayer\nFigure 3: An illustration of a three-layered CNN made of six convolution ﬁlters followed by six max pooling ﬁlters at\nthe ﬁrst layer, and eight convolution ﬁlters followed by seven max pooling ﬁlters at the second layer. The last layer is\na fully connected layer (FC).\n2.3.3\nRecurrent Neural Network\nA recurrent Neural Network (RNN) is a sequential model that can capture the relationship between items in a sequence.\nUnlike traditional neural networks, wherein all inputs are independent of each other, RNNs contain artiﬁcial neurons\nwith one or more feedback loops. Feedback loops are recurrent cycles over time or sequence, as shown in Figure\n4. An established RNN problem is exploding or vanishing gradients. For a long data sequence, the gradient could\nbecome increasingly smaller or increasingly larger, which halts the learning. To address this issue, Hochreiter et al.\n[52] introduced a long short-term memory (LSTM) model and Cho et al. [53] proposed a gated recurrent unit (GRU)\nmodel. Both of these networks allow the gradient to ﬂow unchanged in the network, thus preventing exploding or\nvanishing gradients.\nX\nh\nY\nUnfold\n· · ·\nxt−1\nht−1\nyt−1\nxt\nht\nyt\nxt+1\nht+1\nyt+1\n· · ·\nFigure 4: An illustration of a simple RNN and its unfolded structure through time t.\n2.3.4\nSiamese Neural Network\nThere are settings in which the number of training samples is limited, such as in facial recognition scenarios where\nonly one image is available per person. When there is a limited number of examples for each class, DNNs struggle\n4\nA PREPRINT - SEPTEMBER 30, 2019\nwith generalizing the model. One strategy for addressing this problem is to learn a similarity function. This function\ncomputes the degree of difference between two samples, instead of learning each class. As an example, let x1 represent\none facial image and x2 represent a second. If d(x1, x2) ≤τ, we can conclude that the images are of the same person\nwhile d(x1, x2) > τ implies that they are different people. Siamese Neural Networks (SNN) [54] build on this idea\nby encoding examples xi and xj on two separate DNNs with shared parameters. The SNN learns a function d using\nencoded features as shown in Figure 5. The network then outputs y > 0 for similar objects (i.e., when d is less then a\nthreshold value) and y < 0 otherwise. Thus, SNNs can be used for similarity learning by learning a distance function\nover objects. In addition to their value for supervised learning from limited samples, SNNs are also beneﬁcial for\nunsupervised learning tasks [55] [56].\nxi\nNetwork-1\nxj\nNetwork-2\nShared parameters\nhi\nhj\nd(hi, hj)\n−1 ≤yi,j ≤1\nFigure 5: An illustration of an SNN. In this ﬁgure, xi and xj are two data vectors corresponding to a pair of instances\nfrom the training set. Both networks share the same weights and map the input to a new representation. By comparing\nthe outputs of the networks using a distance measure such as Euclidean, we can determine the compatibility between\ninstances xi and xj.\n2.4\nUnsupervised Learning\n2.4.1\nGenerative Adversarial Network\nUntil this point in the survey, we have focused on deep learning for its power in classifying data points. However,\nresearchers have exploited deep learning for other uses as well, such as generating synthetic data that shares character-\nistics with known real data. One way to create synthetic data is to learn a generative model. A generative model learns\nthe parameters that govern a distribution based on observation of real data points from that distribution. The learned\nmodel can then be used to create arbitrary amounts of synthetic data that emulate the real data observations. Recently,\nresearchers have found a way to exploit multiplayer games for the purpose of improving generative machine learning\nalgorithms. In the adversarial training scenario, two agents compete against each other, as inspired by Samuel [57]\nwho designed a computer program to play checkers against itself. Goodfellow et al. [58] put this idea to use in devel-\noping Generative Adversarial Networks (GANs), in which a DNN (generator) tries to generate synthetic data that is so\nsimilar to real data that it fools its opponent DNN (discriminator), whose job is to distinguish real from fake data (see\nFigure 6 for an illustration). The goal of GANs is to simultaneously improve the ability of the generator to produce\nrealistic data and of the discriminator to distinguish synthetic from real data. GANs have found successful application\nin diverse tasks including translating text to images [59], discovering drugs [60], and transforming sketches to images\n[61] [62].\n2.4.2\nAutoencoder\nYet another purpose for deep neural networks is to provide data compression and dimensionality reduction. An Au-\ntoencoder (AE) is a DNN that accomplishes this goal by creating an output layer that resembles the input layer, using\na reduced set of terms represented by the middle layers [48]. Architecturally, an AE combines two networks. The\nﬁrst network, called the encoder, learns a new representation of input x with fewer features h = f(x); the second\npart, called the decoder, maps h onto a reconstruction of the input space ˆy = g(h), as shown in Figure 7. The goal\nof an AE is not simply to recreate the input features. Instead, an AE learns an approximation of the input features\nto identify useful properties of the data. AEs are vital tools for dimensionality reduction [63], feature learning [64],\nimage colorization [65], higher-resolution data generation [66], and latent space clustering [67]. Additionally, other\nversions of AEs such as variational autoencoders (VAEs) [68] can be used as generative models.\n5\nA PREPRINT - SEPTEMBER 30, 2019\nReal Data\nDiscriminator\nNetwork\nReal\nFake\nRandom Noise\nGenerator\nNetwork\nFake\nData\nFigure 6: An illustration of a GAN. The goal of the discriminator network is to distinguish real data from fake data, and\nthe goal of the generator network is to use the feedback from the discriminator to generate data that the discriminator\ncannot distinguish from real.\n...\n...\ny1\ny2\ny3\nym\nx1\nx2\nx3\nxm\nEncoder\nDecoder\nFigure 7: An illustration of an AE. The ﬁrst part of the network, called the encoder, compresses input into a latent-space\nby learning the function h = f(x). The second part, called the decoder, reconstructs the input from the latent-space\nrepresentation by learning the function ˆy = g(h).\n2.5\nOptimization for Training Deep Neural Networks\nIn the previous section, we described common DNN architecture components. In this section, we offer a brief overview\nof optimization approaches for training DNNs. Learning methods may optimize a function f(x) (e.g., minimize a loss\nfunction) by modifying model parameters (e.g., changing DNN weights). However, as Bengio et al. [69] point out,\nDNN optimization during training may be further complicated by local minima and ill-conditioning (see Figure 8 for\nan illustration of an ill-condition).\nThe most common type of optimization strategy employed by DNNs is gradient descent. This intuitive approach to\nlearns the weights of connections between layers which reduce the network’s objective function by computing the\nerror derivative with respect to a Ir-level layer of the network. Input x is fed forward through a network to predict\nˆy. A cost function J(θ) measures the error of the network at the output layer. Gradient descent then directs the cost\nvalue to ﬂow backward through the network by computing the gradient of the objective function ∇θJ(θ). This process\nis sometimes alternatively referred to as backpropagation because the training error propagates backward through the\nnetwork from output to input layers. Many variations of gradient descent have been tested for DNN optimization, such\nas stochastic gradient descent, mini-batch gradient descent, momentum [70], Ada-Grad [71], and Adam [72].\nDeep network optimization is an active area of research. Along with gradient descent, many other algorithms such as\nderivative-free optimization [73] and feedback-alignment [74] have appeared. However, none of these algorithms are\nas popular as the gradient descent algorithms.\n2.6\nRegularization\nRegularization was an optimization staple for decades prior to the development of DNNs. The rationale behind adding\na regularizer to a classiﬁer is to avoid the overﬁtting problem, where the classiﬁer ﬁts the training set too closely\ninstead of generalizing to the entire data space. Goodfellow et al. [48] deﬁned regularization as “any modiﬁcation to\na learning algorithm that is intended to reduce its generalization error but not its training error”. While regularization\n6\nA PREPRINT - SEPTEMBER 30, 2019\nFigure 8: The left-hand side loss surface depicts a well-conditioned model where local minima can be reached from all\ndirections. The right-hand side loss surface depicts an ill-conditioned model where there are several ways to overshoot\nor never reach the minima.\nmethods such as bagging have been popular for neural networks and other classiﬁers, recently the DNN community\nhas developed novel regularization methods that are unique to deep neural networks. In some cases, backpropagation\ntraining of fully-connected DNNs results in poorer performance than shallow structures because the deeper structure\nis prone to being trapped in local minima and overﬁtting the training data. To improve the generalizability of DNNs,\nregularization methods have thus been adopted during training. Here we review the intuition behind the most frequent\nregularization methods that are currently found in DNNs.\n2.6.1\nParameter Norm Penalty\nA conventional method for avoiding overﬁtting is to penalize large weights by adding a p-norm penalty function to\nthe optimization function of the form f(x)+ p-norm(x), where the p-norm p for weights w is denoted as ||w||p =\n(P\ni |wi|p)\n1\np . Popular p-norms are the L1 and L2 norms which have been used by other classiﬁers such as logistic\nregression and SVMs prior to the introduction of DNNs. L1 adds a regularization term Ω(θ) = ||w||1 to the objective\nfunction for weights w, while L2 adds a regularization term Ω(θ) = ||w||2. The difference between the L1 and L2\nnorm penalty functions is that L1 penalizes features more heavily by setting the corresponding edge weights to zero\ncompared to L2. Therefore, a classiﬁer with the L1 norm penalty tends to prefer a sparse model. The L2 norm penalty\nis more common than the L1 norm penalty. However, it is often advised to use the L1 norm penalty when the amount\nof training data is small and the number of features is large to avoid noisy and less-important features. Because of its\nsparsity property, the L1 penalty function is a key component of LASSO feature selection [75].\n2.6.2\nDropout\nA powerful method to reduce generalization error is to create an ensemble of classiﬁers. Multiple models are trained\nseparately, then as an ensemble they output a combination of the models’ predictions on test points. Some examples\nof ensemble methods included bagging [76], which trains k models on k different folds of random samples with\nreplacement, and boosting [77], which applies a similar process to weighted data. A variety of DNNs use boosting to\nachieve lower generalization error [45] [78] [79].\nDropout [80] is a popular regularization method for DNNs which can be viewed as a computationally-inexpensive\napplication of bagging to deep networks. A common way to apply dropout to a DNN is to deactivate a randomly-\nselected 50% of the hidden nodes and a randomly-selected 20% of the input nodes for each mini-batch of data. The\ndifference between bagging and dropout is that in bagging the models are independent of each other, while in dropout\neach model inherits a subset of parameters from the parent deep network.\n2.6.3\nData Augmentation\nDNNs can generalize better when they have more training data; however, the amount of available data is often limited.\nOne way to circumvent this limitation is to generate artiﬁcial data from the same distribution as the training set. Data\naugmentation has been particularly effective when used in the context of classiﬁcation. The goal of data augmentation\nis to generate new training samples from the original training set (X, y) by transforming the X inputs. Data augmen-\ntation may include generating noisy data to improve robustness (denoising) or creating additional training data for the\npurpose of regularization (synthetic data generation). Dataset augmentation has been adopted for a variety of tasks\nsuch as image recognition [81] [82], speech recognition [83], and activity recognition [84]. Additionally, GANs [85]\n[86] and AEs [87] [88], described in Sections 2.4.1 and 2.4.2, can be employed to generate such new examples.\nInjecting noise into a copy of the input is another data augmentation method. Although DNNs are not consistently\nrobust to noise [89], Poole et al. [90] show that DNNs can beneﬁt from carefully-tuned noise.\n7\nA PREPRINT - SEPTEMBER 30, 2019\n3\nDeep Learning Architectures Outside of Deep Neural Networks\nRecent research has introduced numerous enhancements to the basic neural network architecture that enhance network\nclassiﬁcation power, particularly for deep networks. In this section, we survey non-network classiﬁers that also make\nuse of these advances.\n3.1\nSupervised Learning\n3.1.1\nFeedforward Learning\nA DNN involves multiple layers of operations that are performed sequentially. The idea of creating a sequence of\noperations, each of which manipulates the data before passing them to the next operator, may be used to improve many\ntypes of classiﬁers. One way to construct a model with a deep feedforward architecture is to use stacked generalization\n[91] [92]. Stacked generalization classiﬁers are comprised of multiple layers of classiﬁers stacked on top of each other\nas found in DNNs. In stacked generalization classiﬁers, one layer generates the next layer’s input by concatenating its\nown input to its output. Stacked generalization classiﬁers typically only implement forward propagation, in contrast\nto DNNs which propagate information both forward and backward through the model.\nIn general, learning methods that employ stacked generalization can be categorized into two strategies. In the ﬁrst\nstacked generalization strategy, the new feature space for the current layer comes from the concatenation of the pre-\ndicted output of the previous layer with the original feature vector. Here, layers refer not to layers of neural network\noperations, but instead refer to sequences of other types of operations. Examples of this strategy include Deep Forest\n(DF) [93] and the Deep Transfer Additive Kernel Least Square SVM (DTA-LS-SVM) [94]. At any given layer, for\neach instance x, DF extends x’s previous feature vector to include the previous layer’s predicted class value for the\ninstance. The prediction represents a distribution over class values, averaged over all trees in the forest. Furthermore,\nZhou et al. [93] introduce a method called Multi-Grained Scanning for improving the accuracy of DFs. Inspired\nby CNNs and RNNs where spatial relationships between features are critical, Multi-Grained Scanning splits a D-\ndimensional feature vector into smaller segments by moving a window by moving a window over the features. For\nexample, given 400 features and a window size of 100, the original features convert to 301 features of length 100,\n{< 1 −100 >, < 2 −101 >, . . . , < 301 −400 >}, where the new instances have the same labels as the original\ninstances. The new samples which are described by a subset of the original features might have incorrectly-associated\nlabels. At a ﬁrst glance, it seems these noisy data could hurt the generalization. But as Breiman illustrates [95],\nperturbing a percentage of the training labels can actually help generalization.\nFurthermore, Ho [96] demonstrates that feature sub-sampling can enhance the generalization capability for RFs. Zhou\net al. [93] tested three different window sizes (D/4, D/8, and D/16), where data from each different window size ﬁts\na different level of a DF model. Then the newly-learned representation from these three layers are fed to a multilayer\nDF, applying subsampling when the transformed features are too long. Multi-Grained Scanning can improve the\nperformance of a DF model for continuous data, as Zhou et al. [93] report that accuracy increased by 1.24% on the\nMNIST [97] dataset. An alternative method, DTA-LS-SVM, applies an Additive Kernel Least Squares SVM (AK-LS-\nSVM) [98] [99] at each layer and concatenates the original feature vector x with the prediction of the previous level\nto feed to the next layer. In addition, DTA-LS-SVM incorporates a parameter-transfer approach between the source\n(previous-layer learner) and target (next-layer learner) to enhance the classiﬁcation capability of the higher level.\nIn the second stacked generalization strategy, the current layer’s new feature space comes from the concatenation of\npredictions from all previous layers with the original input feature vector. Examples of this strategy include the Deep\nSVM (D-SVM) [100] and the Random Recursive SVM (R2-SVM) [101]. The D-SVM contains multiple layers of\nSVMs, where the ﬁrst layer is trained in the normal fashion. Following this step, each successive layer employs the\nkernel activation from the previous layer with the desired labels. The R2-SVM is a multilayer SVM model which at\neach layer transforms the data based on the sigmoid of a projection of all previous layers’ outputs. For the data (X, Y )\nwhere X ∈RD and Y ∈RC, the random projection matrix is W ∈RD×C, where each element is sampled from\nN(0, 1). The input data for the next layer is:\nXl+1 = σ(d + βWl+1[oT\n1 , oT\n2 , ..., oT\nl ]T ),\n(1)\nwhere β is a weight parameter that controls the degree with which a data sample in Xl+1 moves from the previous layer,\nσ(.) is the sigmoid function, Wl+1 is the concatenation of l random projection matrices [Wl+1,1, Wl+1,2, ..., Wl+1,l],\none for each previous layer, and o is the output of each layer. Addition of a sigmoid function to the recursive model\nprevents deterioration to a trivial linear model in a similar fashion as MLPs. The purpose of the random projection is\nto push data from different classes in different directions.\nIt is important to note here that stacked generalization can be found in DNNs as well as non-network classiﬁers.\nExamples of DNNs with stacked generalization include Deep Stacking Networks [102] [103] and Convex Stacking\n8\nA PREPRINT - SEPTEMBER 30, 2019\nArchitectures [104] [102]. This is clearly one enhancement that beneﬁts all types of classiﬁer strategies. However,\nthere is no evidence that stack generalization could add nonlinearity to the model.\nDNN classiﬁers learn a new representation of data at each layer with a goal that the newly-learned representation\nmaximally separate the classes. Unsupervised DNNs often share this goal. As an example, Deep PCA’s model [105]\nis made of two layers that each learn a new data representation by applying a Zero Components Analysis (ZCA)\nwhitening ﬁlter [106] followed by a principal components analysis (PCA) [107]. The ﬁnal data representation is\nderived from concatenating the output of the two layers. The motivation behind applying a ZCA whitening ﬁlter is to\nforce the model to focus on higher-order correlations. One motivation for combining output from the ﬁrst and second\nlayers could be to preserve the learned representation from the ﬁrst layer and to prevent feature loss after applying\nPCA at each layer. Experiments demonstrate that Deep PCA exhibits superior performance for face recognition tasks\ncompared to standard PCA and a two-layer PCA without a whitening ﬁlter. However, as empirically conﬁrmed by\nDamianou et al. [108], stacking PCAs does not necessarily result in an improved representation of the data because\nDeep PCA is unable to learn a nonlinear representation of data at each layer. Damianou et al. [108] fed a Gaussian to\na Deep PCA and observed that the model learned just a lower rank of the input Gaussian at each layer.\nAs pointed out earlier in this survey, the invention of the deep belief net (DBN) [45] drew the attention of researchers\nto developing deep models. A DBN can be viewed as a stacked restricted Boltzmann machine (RBM), where each\nlayer is trained separately and alternates functionality between hidden and input units. In this model, features learned\nat hidden layers then represent inputs to the next layer. A RBM is a generative model that contains a single hidden\nlayer. Unlike the Boltzmann machine, hidden units in the restricted model are not connected to each other and contain\nundirected, symmetrical connections from a layer of visible units (inputs). All of the units in each layer of a RBM are\nupdated in parallel by inputting the current state of the unit to the other layer. This updating process repeats until the\nsystem is sampling from an equilibrium distribution. The RBM learning rule is shown in Equation 2.\n∂log P(v)\n∂Wij\n≈< vihj >data −< vihj >reconstruction\n(2)\nIn this equation, Wij represents the weight vector between a visible unit vi and a hidden unit hj, and < . > is the\naverage value over all training samples. Since the introduction of DBNs, many other different variations of Deep\nRBMs have been proposed such as temporal RBMs [109], gated RBMs [110], and cardinality RBMs [111].\nAnother novel form of a deep belief net is a deep Gaussian process (DGP) model [108]. DGP is a deep directed graph\nwhere multiple layers of Gaussian processes map the original features to a series of latent spaces. DGPs offer a more\ngeneral form of Gaussian Processes (GPs) [112] where a one-layer DGP consists of a single GP, f. In a multilayer\nDGP, each GP, fl, maps data from one latent space to the next. As shown in Equation 3, each data point Y is generated\nfrom the corresponding function fl with ǫ Guassian noise applied to data Xl that is obtained from a previous layer.\nY = fl(Xl) + ǫl, ǫl ∼N(0, σ2\nl I)\n(3)\nFigure 9 illustrates a DGP expressed as a series of Gaussian processes mapping data from one latent space to the next.\nFunctions fl are drawn from a Gaussian process, i.e. f(x) ∼GP(0, k(x, x′)). In this setting, the covariance function\nk deﬁnes the properties of the mapping function. DGP can be utilized for both supervised and unsupervised learning.\nIn the supervised setting, the top hidden layer is observed, whereas in the unsupervised setting, the top hidden layer\nis set to a unit Gaussian as a fairly uninformative prior. DGP is a powerful non-parametric model but it has only been\ntested on small datasets. Also, we note that researchers have developed deep Gaussian process models with alternative\narchitectures such as recurrent Gaussian processes [113], convolutional Gaussian processes [114] and variational auto-\nencoded deep Gaussian processes [115]. There exists a vast amount of literature on this topic that provides additional\ninsights on deep Gaussian processes [116] [117] [118].\nX\nf1\nf2\nf3\nY\nFigure 9: A deep Gaussian process with two hidden layers.\nAs we discussed, non-network classiﬁers have been designed that contain multiple layers of operations, similar to a\nDNN. We observe that a common strategy for creating a deep non-network model is to add the prediction of the previ-\nous layer or layers to the original input feature. Likewise, novel methods can be applied to learn a new representation\nof data at each layer. We discuss these methods next.\n9\nA PREPRINT - SEPTEMBER 30, 2019\n3.1.2\nSiamese Model\nAs discussed in Section 2.3.4, a SNN represents a powerful method for similarity learning. However, one problem\nwith SNNs is overﬁtting when there is a small number of training examples. The Siamese Deep Forest (SDF) [119] is\na method based on DF which offers an alternative to a standard SNN. The SDF, unlike the SNN, uses only one DF. The\nﬁrst step in training a SDF is to modify the training examples. The training set consists of the concatenation of each\npair of samples in the original set. If sample points xi and xj are semantically similar, the corresponding class label\nis set to zero; otherwise, the class label is set to one. The difference between the SDF and the DF in training is that\nthe Siamese Deep Forest concatenates the original feature vector with a weighted sum of the tree class probabilities.\nTraining of SDF is similar to DF; the primary difference is that SDF learns the class probability weights w for each\nforest separately at each layer. Learning the weights for each forest can be accomplished by minimizing the function\nin Equation 4.\nmin\nw Jq(w) = min\nw\nX\ni,j\nl(xi, xj, yij, w) + λR(w)\n(4)\nHere, w represents a concatenation of vectors wk, k = 1, ..., M, q is the SDF layer, R(w) is a regularization term,\nand λ is a hyper-parameter to control regularization. Detailed instructions on minimizing Equation 4 are found in the\nliterature [119]. The results of SDF experiments indicate that the SDF can achieve better classiﬁcation accuracy than\nDF for small datasets. In general, all non-network models that learn data representations can take advantage of the\nSiamese architecture like SDF.\n3.2\nUnsupervised Learning\n3.2.1\nGenerative Adversarial Model\nA common element found in GANs is inclusion of a FC layer in the discriminator. One issue with the FC layer is that\nit cannot deal with the ill-condition in which local minima are not surrounded by spherical wells as shown in Figure 8.\nThe Generative Adversarial Forest (GAF) [120] replaces the FC layer of the discriminator with a deep neural decision\nforest (DNDF), which is discussed in Section 4. GAF and DNDF are distinguished based on how leaf node values\nare learned. Instead of learning leaf node values iteratively, as DNDF does, GAF learns them in parallel across the\nensemble members. The strong discriminatory power of the decision forest is the reason the authors recommend this\nmethod in lieu of the fully-connected discriminator layer.\nIn this previous work, the discriminator is replaced by an unconventional model. We hypothesize that replacing the\ndiscriminator with other classiﬁers such as Random Forest, SVM, of K nearest neighbor based on the data could result\nin a diverse GAN strategies, each of which may offer beneﬁts for alternative learning problems.\n3.2.2\nAutoencoder\nAs we discussed in Section 2.4.2, AEs offer strategies for dimensionality reduction and data reconstruction from\ncompressed information. The autoencoding methodology can be found in neural networks, non-networks, and hybrid\nmethods. As an example, the multilayer SVM (ML-SVM) autoencoder is a variation of ML-SVM with the same\nnumber of output nodes as input features and a single hidden layer that consists of fewer nodes than the input features.\nML-SVM is a model with the same structure as a MLP. The distinction here is that the network contains SVM models\nas its nodes. A review of ML-SVM is discussed in Section 4. The outputs of hidden nodes are fed as input to each\nSVM output node c as follows:\ngc(f(X|θ)) =\nl\nX\ni=1\n(αc∗\ni −αc\ni)Ko(f(xi|θ), f(x|θ)) + bc,\n(5)\nwhere αc∗\ni\nand αc\ni are the support vector coefﬁcients, Ko is the kernel function, and bc is their bias. The error back-\npropagates through the network to update the parameters.\nAnother exciting emerging research area is the combination of Kalman ﬁlters with deep networks. A Kalman ﬁlter\nis a well-known algorithm that estimates the optimal state of a system from a series of noisy observations. The\nclassical Kalman ﬁlter [121] is a linear dynamical system and therefore is unable to model complex phenomena. For\nthis reason, researchers developed nonlinear versions of Kalman ﬁlters. In a seminal contribution, Krishnan et al.\n[122] introduced a model that combines a variational autoencoder with Kalman ﬁlters for counterfactual inference\nof patient information. In a standard autoencoder, the model learns a latent space that represents the original data\nminus extraneous information or “signal noise”. In contrast, a variational autoencoder (VAE) [68] adds a constraint\nto the encoder that it learn a Gaussian distribution of the original input data. Therefore, a VAE is able to generate\n10\nA PREPRINT - SEPTEMBER 30, 2019\na latent vector by sampling from the learned Gaussian distribution. Deep Kalman ﬁlters (DKF) learn a generative\nmodel from observed sequences ⃗x = (x1, · · · , xT ) and actions ⃗u = (u1, · · · uT −1), with a corresponding latent space\n⃗z = (z1, · · · , zT), as follows:\nz1 ∼N(µ0, Σ0)\nzt ∼N(Gα(zt−1, ut−1, ∆t), Sβ(zt−1, yt−1, ∆t))\nxt ∼Π(Fk(zt)),\n(6)\nwhere µ0 = 0 and Σ0 = Id, ∆t represents the difference between times t and t −1, and Π represents a distribution\n(e.g., Bernoulli for binary data) over observation xt. The functions Gα, Sβ, Fk are parameterized by a neural net.\nAs a result, the autoencoder will learn θ = {α, β, k} parameters. Additionally, Shashua et al. [123] introduced deep\nQ-learning with Kalman ﬁlters and Lu et al. [124] presented a deep Kalman ﬁlter model for video compression.\nAs we highlighted in this section, non-network methods have been designed that are inspired by AEs. Although ML-\nSVM mimics the architecture of AEs, its computational cost prevents the algorithm from being a practical choice.\nDKF takes advantage of the VAE idea by learning a Kalman Filter in its middle layer. Additionally, Feng et al. [125]\nintroduced an encoder forest, a model inspired by the DNN autoencoder. Because the encoder forest is not a deep\nmodel, we do not include the details of this algorithm in our survey.\n4\nDeep Learning Optimization Outside of Deep Neural Networks\nAs discussed in Section 2.5, gradient descent has been a prominent optimization algorithm for DNNs; however, it has\nbeen underutilized by non-network classiﬁers. Some notable exceptions are found in the literature. We discuss these\nhere.\nA resourceful method for constructing a deep model is to start with a DNN architecture and then replace nodes with\nnon-network classiﬁers. As an example, the multilayer SVM (ML-SVM) [126] replaces nodes in a MLP with standard\nSVMs. ML-SVM is a multiclass classiﬁer which contains SVMs within the network. At the output layer, the ML-SVM\ncontains the same number of SVMs as the number of classes learned at the perceptron output layer. Each SVM at the\nML-SVM output layer is trained in a one-versus-all fashion for one of the classes. When observing a new data point,\nML-SVM outputs the class label corresponding to the SVM that generates the highest conﬁdence. At each hidden\nlayer, SVMs are associated with each node that learn latent variables. These variables are then fed to the output layer.\nAt hidden layer f(X|θ) where X is the training set and θ denotes the trainable parameters of the SVM, ML-SVM\nmaps the hidden layer features to an output value as follows:\ng(f(X|θ)) =\nl\nX\ni=1\nyc\ni αc\niKo(f(xi|θ), f(X|θ)) + bc,\n(7)\nwhere g is the output layer function, yc\ni ∈{−1, 1} for each class c, Ko is the kernel function for the output layer, αc\ni\nare the support vector coefﬁcients for SVM nodes of the output layer, and bc is their bias. The goal of ML-SVM is to\nlearn the maximum support vector coefﬁcient of each SVM at the output layer with respect to the objective function\nJc(.), as shown in Equation 8.\nmin\nwc,b,ξ,θ Jc = 1\n2||wc||2 + C\nl\nX\ni\nξi\n(8)\nHere, wc represents the set of weights for class c, C represents a trade-off between margin width and misclassiﬁcation\nrisk and ξi are slack variables. ML-SVM applies gradient ascent to adapt its support vector coefﬁcient towards a local\nmaximum of Jc(.). The support vector coefﬁcient is deﬁned as zero for values less than zero and is assigned to C for\nvalues larger than C. The data is backpropagated through the network similar to traditional MLPs by calculating the\ngradient of the objective function.\nThe SVMs in the hidden layer are identical. Given the same inputs, they would thus generate the same outputs. To\ndiversity the SVMs, the hidden layers train on a perturbed version of the training set to eliminate producing similar\noutputs before training the combined ML-SVM model. The outputs of hidden layer nodes are constrained to generate\nvalues in the range [−1 . . . 1]. Despite the effort of ML-SVMs to learn a multi-layer data representation, this approach\nis currently not practical because adding a new node incurs a dramatic computational expense for large datasets.\nKontschieder et al. [127] further incorporate gradient descent into a Random Forest (RF), which is a popular classi-\nﬁcation method. One of the drawbacks of a RF is that does not traditionally learn new internal representations like\nDNNs. The Deep Network Decision Forest (DNDF) [127] integrates a DNN into each decision tree within the forest\nto reduce the uncertainty at each decision node. In DNDF, the result of a decision node dn(x, Θ) corresponds to the\n11\nA PREPRINT - SEPTEMBER 30, 2019\noutput of a DNN fn(x, Θ), where x is an input and Θ is the parameter of a decision node. DNDF must have differ-\nentiable decision trees to be able to apply gradient descent to the process of updating decision nodes. In a standard\ndecision tree, the result of a decision node dn(x, Θ) is deterministic. DNDF replaces the traditional decision node\nwith a sigmoid function dn(x, Θ) = σ(fn(x; Θ)) to create a stochastic decision node. The probability of reaching a\nleaf node l is calculated as the product of all decision node outputs from the root to the leaf l, which is expressed as µl\nin this context. The set of leaf nodes L learns the class distribution π, and the class with the highest probability is the\nprediction of the tree. The aim of DNDF is to minimize its empirical risk with respect to the decision node parameter\nΘ and the class distribution π of L under the log-loss function for a given data set.\nThe optimization of the empirical risk is a two-step process which is executed iteratively. The ﬁrst step is to optimize\nthe class distribution of leaf nodes πL while ﬁxing the decision node parameters and the corresponding DNN. At the\nstart of optimization (iteration 0), class distribution π0 is set to a uniform distribution across all leaves. DNDF then\niteratively updates the class distribution across the leaf nodes as follows for iteration t+1:\nπ(t+1)\nly\n=\n1\nZ(t)\nl\nX\n(x,y′)∈T\n1y=y′πl(t)\ny µl(x|Θ)\nPT [y|x, Θ, π(t)] ,\n(9)\nwhere Z(t)\nl\nis a normalization factor ensuring that P\ny πt+1\nly\n= 1,\n1q is the indicator function on the argument q, and\nPT is the prediction of the tree.\nThe second step is to optimize decision node parameters Θ while ﬁxing the class distribution πL. DNDF employs\ngradient descent to minimize log-loss with respect to Θ as follows:\n∂L\n∂Θ(Θ, π; x, y) =\nX\nn∈N\n∂L(Θ, π; x, y)\n∂fn(x; Θ)\n∂fn(x; Θ)\n∂Θ\n.\n(10)\nThe second term in Equation 10 is the gradient of the DNN. Because this is commonly known, we only discuss\ncalculating the gradient of the differentiable decision tree. Here, the gradient of the differentiable decision tree is\ngiven by:\n∂L(Θ, π; x, y)\n∂fn(x; Θ)\n= dn(x; Θ)Anr −¯dn(x; Θ)Anl,\n(11)\nwhere dn is the probability of transitioning to the left child, ¯dn = 1 −dn is the probability of transitioning to the\nright child calculated by a forward pass through the DNN, and nl and nr indicate the left and right children of node\nn. To calculate the term A in Equation 11, DNDF performs one forward pass and one backward pass through the\ndifferentiable decision tree. Upon completing the forward pass, a value Al can be initially computed for each leaf\nnode as follows:\nAl =\nπlyµl\nP\nl πlyµl\n.\n(12)\nNext, the values of Al for each leaf node are used to compute the values of Am for each internal node m. To do this, a\nbackward pass is made through the decision tree, during which the values are calculated as Am = Anl + Anr, where\nnl and nr represent the left and the right children of node m, respectively.\nEach layer of a standard DNN produces the output oi at layer i. As mentioned earlier, the goal of the DNN is to learn a\nmapping function Fi : oi−1 →oi that minimizes the empirical loss at the last layer of DNN on a training set. Because\neach Fi is differentiable, a DNN updates its parameters efﬁciently by applying gradient descent to reduce the empirical\nloss.\nAdopting a different methodology, Frosst et al. [128] distill a neural network into a soft decision tree. This model\nbeneﬁts from both neural network-based representation learning and decision tree-based concept explainability. In\nthe Frosst soft decision tree (FSDT), each tree’s inner node learns a ﬁlter wi and a bias bi, and leaf nodes l learn a\ndistribution of classes. Like the hidden units of a neural network, each inner node of the tree determines the probability\nof input x at node i as follows:\npi(x) = σ(β(xwi + bi))\n(13)\nwhere σ represents the sigmoid function and β represents an inverse temperature whose function is to avoid soft\ndecisions in the tree. Filter activation routes the sample x to the left branch for values of pi less than 0.5, and to the\nright branch otherwise. The probability distribution Ql for each leaf node l represents the learned parameter φl at that\nleaf over the possible k output classes:\nQl\nk =\nexp(φl\nk)\nP\nk′ exp(φl\nk′).\n(14)\n12\nA PREPRINT - SEPTEMBER 30, 2019\nThe predictive distribution over classes is calculated by traversing the greatest-probability path. To train this soft\ndecision tree, Frosst et al. [128] calculate a loss function L that minimizes the cross entropy between each leaf,\nweighted by input vector x path probability and target distribution T , as follows:\nL(x) = −log\n\u0010\nX\nl∈LeafNodes\nP l(x)\nX\nk\nTk log Ql\nk\n\u0011\n(15)\nwhere P l(x) is the probability of reaching leaf node l given input x. Frosst et al. [128] also introduce a regularization\nterm to avoid internal nodes routing all data points on one particular path and encourage them to equally route data\nalong the left and right branches. The penalty function calculates a sum over all internal nodes from the root to node\ni, as follows:\nC = −λ\nX\ni∈InnerNodes\n0.5 log(αi) + 0.5 log(1 −αi)\n(16)\nwhere λ is a hyper-parameter set prior to training to determine the effect of the penalty. The cross entropy α for a node\ni is the sum of the path probability P i(x) from the root to node i multiplied by the probability of that node pi divided\nby the path probability, as follows:\nαi =\nP\nx P i(x)pi(x)\nP\nx P i(x)\n.\n(17)\nBecause the probability distribution is not uniform across nodes in the penultimate level, this penalty function could\nactually hurt the generalization. The authors address this problem by decaying the strength of penalty function λ\nexponentially with the depth d of the node to 2d. Another challenge is that in any given batch of data, as the data\ndescends the tree, the number of samples decreases exponentially. Therefore, the estimated probability loses accuracy\nfurther down the tree. Frosst et al. recommend addressing this problem by decaying a running average of the actual\nprobabilities with a time window that is exponentially proportional to the depth of the nodes [128]. Although the\nauthors report that the accuracy of this model was less than the deep neural network, the model offers an advantage of\nconcept interpretability.\nBoth DNDF and the soft decision tree ﬁx the depth of the learned tree to a predeﬁned value. In contrast, Tanno et al.\n[129] introduced the Adaptive Neural Tree (ANT) which can grow to any arbitrary depth. The ANT architecture is\nsimilar to a decision tree but at each internal node and edge, ANT learns a new data representation. For example, an\nANT may contain one or more convolution layers followed by a fully-connected layer at each inner node, one or more\nconvolution layers followed by an activation function such as ReLU or tanh at each edge, and a linear classiﬁer at each\nleaf node.\nTraining an ANT requires two phases: growth and reﬁnement. In the growth phase, starting from the root in breadth-\nﬁrst order one of the nodes is selected. The learner then evaluates three choices: 1) split the node and add a sub-tree,\n2) deepen edge transformation by adding another layer of convolution, or 3) keep the current model. The model\noptimizes the parameters of the newly-added components by minimizing log likelihood via gradient descent while\nﬁxing the parameters of the previous portion of the tree. Eventually, the model selects the choice that yields the lowest\nlog likelihood. This process repeats until the model converges. In the reﬁnement phase, the model performs gradient\ndescent on the ﬁnal architecture. The purpose of the reﬁnement phase is to correct suboptimal decisions that may\nhave occurred during the growth phase. The authors evaluate their method on several standard testbeds and the results\nindicate that ANT is competitive with many deep network and non-network learners for these tasks.\nYang et al. [130] took a different approach; instead of integrating artiﬁcial neurons into the tree, they obtained a\ndecision tree using a neural network. The Deep Neural Decision Tree (DNDT) employs a soft binning function to\nlearn the split rules of the tree. DNDT construct a one-layer neural network with softmax as its activation function.\nThe objective function of this network is:\nsoftmax\n\u0000wx + b\nτ\n\u0001\n.\n(18)\nHere, for a continuous variable x, we want to bin it to n + 1, w = [1, 2, · · · , n + 1] is an untrainable constant, b is a\nlearnable bin or the cutting rule in the tree, and τ is a temperature variable. After training this model, the decision tree\nis constructed via the Kronecker product ⊗. Given an input x ∈RD with D features, the tree rule to reach a leaf node\nis:\nz = f1(x1) ⊗f2(x2) ⊗· · · ⊗fD(xD)\n(19)\nHere, z is an almost-one-hot encoded vector that indicates the index of leaf node. One of the shortcomings of this\nmethod is that it cannot handle a high-dimensional dataset because the cost of calculating the Kronecker product\nbecomes prohibitive. To overcome this problem, authors learn a classiﬁer forest by training each tree on a random\nsubset of features.\n13\nA PREPRINT - SEPTEMBER 30, 2019\nIn some cases, the mapping function is not differentiable. Feng et al. [131] propose a new learning paradigm for\ntraining a multilayer Gradient Boosting decision tree (mGBDT) [131] where Fi is not differentiable. Gradient boosting\ndecision tree (GBDT) is an iterative method which learns an ensemble of regression predictors. In GBDT, a decision\ntree ﬁrst learns a model on a training set, then it computes the corresponding error residual for each training sample. A\nnew tree learns a model on the error residuals, and by combining these two trees GBDT is able to learn a more complex\nmodel. The algorithm follows this procedure iteratively until it meets a prespeciﬁed number of trees for training.\nSince gradient descent is not applicable to mGBDT, Feng et al. [131] obtain a “pseudo-inverse” mapping. In this\nmapping, Gt\ni represents the pseudo-inverse of F t−1\ni\nat iteration t, such that Gt\ni(F t−1\ni\n(oi−1)) ∼oi−1. After performing\nbackward propagation and calculating Gt\ni, forward propagation is performed by ﬁtting a pseudo-label zt\ni−1 from Gt\ni\nto F t−1\ni\n. The last layer Fm computes zt\nm based on the true labels at iteration t, where i ∈{2 . . . m}. After this step,\npseudo-labels for previous layers are computed via pseudo-inverse mapping. To initialize mGBDT at iteration t = 0,\neach intermediate (hidden) layer outputs Gaussian noise and F 0\ni represent depth-constrained trees that will later be\nreﬁned. Feng et al. [131] thus create a method that is inspired by gradient descent yet is applicable in situations where\ntrue gradient descent cannot be effectively applied.\nIn this section, we examine methods that apply gradient descent to non-network models. As we observed, one way of\nutilizing gradient descent is to replace the hidden units in a network with a differentiable algorithm like SVM. Another\ncommon theme we recognized was to transform deterministic decision-tree nodes into stochastic versions that offer\ngreater representational power. Alternatively, trees or other ruled-based models can be built using neural networks.\n5\nDeep Learning Regularization Outside of Deep Neural Networks\nWe have discussed some of the common regularization methods used by DNNs in Section 2.6. Now we focus on how\nthese methods have been applied to non-network classiﬁers in the literature. It is worth mentioning that while most\nmodels introduced in this section are not deep models, we investigate how non-network models can improve their\nperformance by applying regularization methods typically associated with the deep operations found in DNNs.\n5.1\nParameter Norm Penalty\nProblems arise when a model is learned from data that contain a large number of redundant features. For example,\nselecting relevant genes associated with different types of cancer is challenging because of a large number of redun-\ndancies may exist in the gene’s long string of features. There are two common ways to eliminate redundant features:\nthe ﬁrst way is to perform feature selection and then train a classiﬁer from the selected features; the second way is\nto simultaneously perform feature selection and classiﬁcation. As we discussed in Section 2.6.1, DNNs apply a L1\nor L2 penalty function to penalize large weights. In this section, we investigate how the traditional DNN idea of pe-\nnalizing features can be applied to non-network classiﬁers to simultaneously select high-ranked features and perform\nclassiﬁcation.\nStandard SVMs employ the L2 norm penalty to penalize weights in a manner similar to DNNs. However, the Newton\nLinear Programming SVM (NLP-SVM) [132] replaces the L2 norm penalty with the L1 norm penalty. This has\nthe effect of setting small hyperparameter coefﬁcients to zero, thus enabling NLP-SVM to select important features\nautomatically. A different way to penalize non-important features in SVMs is to employ a Smoothly Clipped Absolute\nDeviation (SCAD) [133] function. The L1 penalty function can be biased because it imposes a larger penalty on large\ncoefﬁcients; in contrast, SCAD can give a nearly unbiased estimation of large coefﬁcients. SCAD learns a non-convex\npenalty function as shown in Equation 20.\npλ(|w|) =\n\n\n\n\n\nλ|w|\nif |w| ≤λ\n−(|w|2−2aλ|w|+λ2)\n2(a−1)\nif λ < |w| ≤aλ\n(a+1)λ2\n2\nif |w| > aλ\n(20)\nSCAD equates with L1 penalty function until |w| = λ, then smoothly transitions to a quadratic function until |w| = aλ,\nafter which it remains a constant for all |w| > aλ. As shown by Fan et al. [134], SCAD has better theoretical properties\nthan the L1 function.\nOne limitation of decision tree classiﬁers is that the number of training instances that can be selected at each branch\nin the tree decreases with the tree depth. This downward sampling may cause less relevant or redundant features to be\nselected near the bottom of the tree. To address this issue, Dang et al. [135] proposed to penalize features that were\nnever selected in the process of making a tree. In a Regularized Random Forest (RRF) [135], the information gain for\n14\nA PREPRINT - SEPTEMBER 30, 2019\na feature j is speciﬁed as follows:\nGain(j) =\n\u001aλ.Gain(j)\nj ̸∈F\nGain(fi)\nj ∈F\n(21)\nwhere F is the set of features used earlier in the path, fi ∈F, and λ ∈[0, 1] is the penalty coefﬁcient. RRF avoids\nincluding a new feature j, except when the value of Gain(j) is greater than max\ni\n\u0000Gain(fi)\n\u0001\n.\nTo improve RRF, Guided RRF (GRRF) [136] assigns a different penalty coefﬁcient λj to each feature instead of\nassigning the same penalty coefﬁcient to all features. GRRF employs the importance score from a pre-trained RF\non the training set to reﬁne the selection of features at a given node. The importance score of feature j in an RF\nwith T trees is the mean of gain for features in the RF. The important scores evaluate the contribution of features for\npredicting classes. The GRRF uses the normalized importance score to control the degree of regularization of the\npenalty coefﬁcient as follows:\nλj = (1 −γ)λ0 + γImp′\nj,\n(22)\nwhere λ0 ∈(0, 1] is the base penalty coefﬁcient and γ ∈[0, 1] controls the weight of the normalized importance\nscore. The GRRF and RRF are computationally inexpensive methods that are able to select stronger features and\navoid redundant features.\n5.2\nDropout\nAs detailed in Section 2.6.2, dropout is a method that prevents DNNs from overﬁtting by randomly dropping nodes\nduring the training. Dropout can be added to other machine learning algorithms through two methods: by dropping\nfeatures or by dropping models in the case of ensemble methods. Dropout has also been employed by dropping input\nfeatures during training [137] [138]. Here we look at techniques that have been investigated for dropping input features,\nparticularly in non-network classiﬁers.\nRashmi et al. [139] applied dropout to Multiple Additive Regression Trees (MART) [140] [141]. MART is a regression\ntree ensemble which iteratively reﬁnes its model by continually adding trees that ﬁt the loss function derivatives from\nthe previous version of the ensemble. Because trees added at later iterations may only impact a small fraction of the\ntraining set and thus over-specialize, researchers previously used shrinkage to exclude a random subset of leaf nodes\nduring each tree-adding step. More recently, Rashmi et al. integrated the deep-learning idea of dropout into MART.\nUsing dropout, a subset of the trees are temporarily dropped. A new tree is created based on the loss function for the\non-dropped trees. This new tree is combined with the previously-dropped trees into a new ensemble. This method,\nDropout Multiple Additive Regression Trees (DART) [139], weights the votes for the new and re-integrated trees to\nhave the same effect on the ﬁnal model output as the original set of trees. Other researchers have experimented with\npermanently removing a strategic subset of the dropped trees as well [142].\n5.3\nEarly Stopping\nThe core concept of early stopping is to terminate DNN training once performance on the validation set is not im-\nproving. One potential advantage of Deep Forest [93] over DNNs is that DF can determine the depth of a model\nautomatically. In DF, if the model performance does not increase on the validation set after adding a new layer, the\nlearning terminates. Unlike DNNs, DF may avoid the tendency to overﬁt as more layers are added. Thus, while\nearly stopping does not necessarily enjoy the primary outcome of preventing such overﬁtting, it can provide additional\nbeneﬁts such as shortening the validation cycle in the search for the optimal tree depth.\n5.4\nData Augmentation\nAs discussed in Section 2.6.3, data augmentation is a powerful method for improving DNN generalization. However,\nlittle research has investigated the effects of data augmentation methods on non-network classiﬁers. As demonstrated\nby Wong et al. [143], the SVM classiﬁer does not always beneﬁt from data augmentation, in contrast to DNNs. How-\never, Xu [144] ran several data augmentation experiments on synthetic datasets and observed that data augmentation\ndid enhance the performance of random forest classiﬁers. Offering explanations for the circumstances in which such\naugmentation is beneﬁcial is a needed area for future research.\n15\nA PREPRINT - SEPTEMBER 30, 2019\n6\nHybrid Models\nHybrid models can be deﬁned as a combination of two or more classes of models. There are many ways to construct\nhybrid models, such as DNDF [127] which integrates a deep network into a decision forest as explained in Section 4.\nIn this section we discuss other examples of hybrid models.\nOne motivation for combining aspects of multiple models is to ﬁnd a balance between classiﬁcation accuracy and\ncomputational cost. Energy consumption by mobile devices and cloud servers is an increasing concern for responsive\napplications and green computing. Decision forests are computationally inexpensive models because of the conditional\nproperty of decision trees. Conversely, while CNNs are less efﬁcient, they can achieve higher accuracy because of their\nrepresentation-learning capabilities. Ioannou et al. [145] introduced the Conditional Neural Network (CondNN) to re-\nduce computation in a CNN model by introducing a routing method similar to that found in decision trees. In CondNN,\neach node in layer l is connected to a subset of nodes from the previous layer, l −1. Given a fully trained network,\nfor every two consecutive layers a matrix Λ(l−1,l) stores the activation values of these two layers. By rearranging\nelements of Λ(l−1,l) based on highly-active pairs for each class in the diagonal and zeroing out off-diagonal elements,\nthe CondNN develops explicit routes Λroute\n(l,l−1) through nodes in the network. CondNN incurs profoundly lower com-\nputation cost compared to other DNNs at test time; whereas, CondNN’s accuracy remains similar to larger models.\nWe note that DNN size can be also be reduced by employing Bayesian optimization, as investigated by Blundell et al.\n[146] and by Fortunato et al. [147]. These earlier efforts provide evidence that Bayesian neural networks are able to\ndecrease network size even more than CondNNs while maintaining a similar level of accuracy.\nAnother direction for blending a deep network with a non-network classiﬁer is to improve the non-network model by\nlearning a better representation of data via a deep network. Zoran et al. [148] introduce the differentiable boundary\ntree (DBT) in order to integrate a DNN into the boundary tree [149] to learn a better representation of data. The newly-\nlearned data representation leads to a simpler boundary tree because the classes are well separated. The boundary tree\nis an online algorithm in which each node in the tree corresponds to a sample in the training set. The ﬁrst sample\ntogether with its label are established as the tree root. Given a new query sample z, the sample traverses through the\ntree from the root to ﬁnd the closest node n based on some distance function like the Euclidean distance function. If\nthe label of the nearest node in the tree is different from the query sample, a new node containing the query z is added\nas a child of the closest node n in the tree; otherwise the query node z is discarded. Therefore, each edge in the tree\nmarks the boundary between two classes and each node tends to be close to these boundaries.\nTransitions between nodes in a standard boundary tree are deterministic. DBT combines a SoftMax cost function with\na boundary tree, resulting in stochastic transitions. Let x be a training sample and c be the one-hot encoding label\nof that sample. Given the current node xi in the tree and a query node z, the transition probability from node xi to\nnode xj, where xj ∈{child(xi), xi} is the SoftMax of the negative distance between xj and z. This is shown in\nEquation 23.\np(xi →xj|z) = SoftMax\ni,j∈child(i)(−d(xj, z))\n=\nexp(−d(xj, z))\nP\nj′∈{i,j∈child(i)}\nexp(−d(xj, z))\n(23)\nThe probability of traversing a particular path in the boundary tree, given a query node z, is the product of the probabil-\nity of each transition along the path from the root to the ﬁnal node xfinal∗in the tree. The ﬁnal class log probability of\nDBT is computed by summing the probabilities of all transitions to the parent of xfinal∗together with the probabilities\nof the ﬁnal node and its siblings. The set sibling(xi) consists of all nodes sharing the same parent with node xi and\nthe node xi itself. As discussed earlier, a DNN fθ(x) transforms the inputs to learn a better representation. The ﬁnal\nclass log probabilities for the query node z are calculated as follows:\nlog p(c|fθ(z)) =\nX\nxi→xj∈path†|fθ(z)\nlog p(fθ(xi) →fθ(xj)|fθ(z))\n+ log\nX\nxk∈sibling(xfinal∗)\np(parent(fθ(xk)) →fθ(xk)|fθ(z))c(xk).\n(24)\nIn Equation 24, path† denotes path∗(the path to the ﬁnal node xfinal∗) without the last transition, and sibling(x)\nrepresents node x and all other nodes sharing the same parent with node x. The gradient descent algorithm can be\napplied to Equation 24 by plugging in a loss function to learn parameter θ of the DNN. However, gradient descent\ncannot be applied easily to DBT because of the node and edge manipulations in the graph. To address this issue,\nDBT transforms a small subset of training examples via a DNN and builds a boundary tree based on the transformed\nexamples. Next, DBT transforms a query node z via the same DNN and calculates the log probability of a class\n16\nA PREPRINT - SEPTEMBER 30, 2019\naccording to Equation 24. The DNN employs gradient descent to update its parameters by propagating the gradient of\nlog loss probability. DBT discards this boundary tree and iteratively builds a new boundary tree as described until a\nconvergence criteria is met. In the described method, the authors set a speciﬁc threshold for the loss value to terminate\nthe training. DBT is able to achieve greater accuracy with a simpler tree than original boundary tree as shown by\nthe authors on the MNIST dataset [97]. One of the biggest advantages of DBT is its interpretability. However, DBT\nis computationally an expensive method because a new computation graph needs to be built, which makes batching\ninefﬁcient. Another limitation is that the algorithm needs to switch between building the tree and updating the tree.\nTherefore, scaling to large datasets is fairly prohibitive.\nYet another way of building a hybrid model is to learn a new representation of data with a DNN, then hand the resulting\nfeature vectors off to other classiﬁers to learn a model. Tang [150] explored replacing the last layer of DNNs with a\nlinear SVM for classiﬁcation tasks. The activation values of the penultimate layer are fed as input to an SVM with a\nL2 regularizer. The weights of the lower layer are learned through momentum gradient descent by differentiating the\nSVM objective function with respect to activation of the penultimate layer. The author’s experiments on the MNIST\n[97] and CIFAR-10 [151] datasets demonstrate that replacing a CNN’s SoftMax output layer with SVM yields a lower\ntest error. Tang et al. [150] postulate that the performance gain is due to the superior regularization effect of the SVM\nloss function.\nIt is worth mentioning that in their experiment on MNIST [97], Tang ﬁrst used PCA to reduce the features and then\nfed the reduced feature vectors as input to their model. Also, Niu et al. [152] replaced the last layer of a CNN with an\nSVM which similarly resulted in lowering test error of the model compare to a CNN on the MNIST dataset. Similar\nto these methods, Zareapoor et al. [153], Nagi et al. [154], Bellili et al. [155], and Azevedo et al. [156] replace the\nlast layer of a DNN with an SVM. In these cases, their results from multiple datasets reveal that employing a SVM as\nthe last layer of a neural network can improve the generalization of the network.\nZhao et al. [157] replace the last layer of a deep network with a visual hierarchical tree to learn a better solution\nfor image classiﬁcation problems. A visual hierarchical tree with L levels organizes N objects classes based on their\nvisual similarities in its nodes. Deeper in the tree, groups become more separated wherein each leaf node should\ncontain instances of one class. The class similarity between the class ci and cj is deﬁned as follows:\nSi,j = S(ci, cj) = exp\n\u0010\n−d(xi, xj)\nσ\n\u0011\n.\n(25)\nHere, d(xi, xj) represents the distance between the deep representation of instances of classes ci and cj, and σ is\nautomatically determined by a self-tuning technique. After calculating matrix S, hierarchical clustering is employed\nto learn a visual hierarchical tree.\nIn a traditional visual hierarchical tree, some objects might be assigned to incorrect groups. A level-wise mixture\nmodel (LMM) [157] aims to improve this visual hierarchical tree by learning a new representation of data via a DNN\nthen updating the tree during training. For a given tree, matrix Ψyi,ti denotes the probability of objects with label y\nbelonging to group t in the tree. First, LMM updates the DNN parameters and the visual hierarchical tree as is done\nwith a traditional DNN. The only difference is a calculation of two gradients, one based on the parameters of the DNN\nand other one based on the parameters of the tree. Second, LMM updates the matrix Ψyi,ti for each training sample\nseparately and updates the parameters of the DNN and the tree afterwards. To update the Ψ, the posterior probability\nof the assigning group ti for the object xi is calculated based on the number of samples having the same label y as the\nlabel of xi in group t. For a given test image, LMM learns a new representation of the image based on the DNN and\nthen obtains a prediction by traversing the tree. One of the advantages of a LMM is that over time, by learning a better\nrepresentation of data via DNN, the algorithm can update the visual hierarchical tree.\nIn some cases, two different data views are available. As an example, one view might contain video and the another\nsound. Canonical correlation analysis (CCA) [158] and kernel canonical correlation analysis (KCCA) [159] offer stan-\ndard statistical methods for learning view representations that each the most predictable by the other view. Nonlinear\nrepresentations learned by KCCA can achieve a higher correlation than linear representations learned by CCA. Despite\nthe advantages of KCCA, the kernel function faces some drawbacks. Speciﬁcally, the representation is bound to the\nﬁxed kernel. Furthermore, because is model is nonparametric, training time as well as the time to compute the new\ndata representation scales poorly with the size of a training set.\nAndrews et al. [160] proposed to apply deep networks to learn a nonlinear data representation instead of employing a\nkernel function. Their resulting deep canonical correlation analysis (DCCA) consists of two separate deep networks\nfor learning a new representation for each view. The new representation learned by the ﬁnal layer of networks H1 and\nH2 is fed to CCA. To compute the objective gradient of DCCA, the gradient of the output of the correlation objective\n17\nA PREPRINT - SEPTEMBER 30, 2019\nwith respect to the new representation can be calculated as follows:\n∂corr(H1, H2)\n∂H1\n(26)\nAfter this computation, backpropagation is applied to ﬁnd the gradient with respect to all parameters. The details of\ncalculating the gradient in Equation 26 are provided by the authors [160].\nWhile researchers have also created LSTM methods that employ tree structures [161] [162], these methods utilize\nthe data structure to improve a network model rather than employing tree-based learning algorithms. Similarly, other\nresearches integrate non-network classiﬁers into a network structure. Cimino et al. [163] and Agarap [164] introduce\nhybrid models. These two methods apply LSTM and GRU, respectively, to learn a network representation. Unlike\ntraditional DNNs, the last layer employs a SVM for classiﬁcation.\nThe work surveyed in this section provides evidence that deep neural nets are capable methods for learning high-\nlevel features. These features, in turn, can be used to improve the modeling capability for many types of supervised\nclassiﬁers.\nTable 1: Summary of classiﬁers which integrate deep network components into non-network classiﬁers.\nMethods\nClassiﬁers\nArchitecture\nFeedforward\nANT [129], DNDT [130], DBN [45], Deep PCA [105], DF [93], DPG [116], R2-SVM [101], D-SVM [100], DTA-LS-SVM [94], SFDT [128]\nAutoencoder\nDKF [122], eForest [125], ML-SVM [126]\nSiamese Model\nSDF [119]\nGenerative Adversarial Model\nGAF [120]\nOptimization\nGradient Decent\nDNDF [127], mGBDT [131], ML-SVM [126]\nRegularization\nParameter Norm Penalty\nNLP-SVM [132], GRRF [136], RRF [135] , SCAD-SVM [133]\nDropout\nDART [139]\nHybrid Model\nCondCNN [145], DBT [148], DCCA [160], DNDF [127], DNN+SVM [150] [152] [153] [154] [155] [156], LMM [157]\nIn this survey, we aim to provide a thorough review of non-network models that utilize the unique features of deep\nnetwork models. Table 1 provides a summary of such non-network models, organized based on four aspects of\ndeep networks: model architecture, optimization, regularization, and hybrid model fusing. A known advantage of\ntraditional deep networks compared with non-network models has been the ability to learn a better representation of\ninput features. Inspired by various deep network architectures, deep learning of non-network classiﬁers has resulted\nin methods to also learn new feature representations. Another area where non-network classiﬁers have beneﬁted\nfrom recent deep network research is applying backpropagation optimization to improve generalization. This table\nsummarizes published efforts to apply regularization techniques that improve neural network generalization. The last\ncategory of models combines deep network classiﬁers and non-network classiﬁers to increase overall performance.\n7\nExperiments\nIn this paper, we survey a wide variety of models and methods. Our goal is to demonstrate that diverse types of models\ncan beneﬁt from deep learning techniques. To highlight this point, we empirically compare the performance of many\ntechniques described in this survey. This comparison includes deep and shallow networks as well as and non-network\nlearning algorithms. Because of the variety of classiﬁers that are surveyed, we organize the comparison based on the\nlearned model structure.\nFirst, we compare the models that are most similar to DNNs. These models should be able to learn a better repre-\nsentation of data when a large dataset is available. We report the test error provided by the authors for MNIST and\nCIFAR-10 dataset in Table 2. If the performance of a model was not available for any of these datasets, we ran that\nexperiment with the authors’ code. Default parameters are employed for parameter values that are not speciﬁed in the\noriginal papers. In the event that the authors did not provide their code, we did not report any results. These omissions\nprevent the report of erroneous performances that result from implementation differences.\nThe MNIST dataset has been a popular testbed dataset for comparing model choices within the computer vision and\ndeep network communities. MNIST instances contain 28 × 28 pixel grayscale images of handwritten digits and their\nlabels. The MNIST labels are drawn from 10 object classes, with a total of 6000 training samples and 1000 testing\nsamples. The CIFAR-10 is also a well-known dataset containing 10 object classes with approximately 5000 examples\nper class, where each sample is a 32 × 32 pixel RGB image.\n18\nA PREPRINT - SEPTEMBER 30, 2019\nTable 2: Classiﬁcation error rate (%) comparison.\nDataset/Models\nRF\nANT† [129]\nDF [93]\nR2SVM [101]\nSFDT [128]\nDNDF [127]\nCondCNN [145]\nDBT [148]\nDNN+SVM [150]\nMNIST\n3.21\n0.29\n0.74\n4.03∗\n5.55\n0.7\n-\n1.85\n0.87\nCIFAR-10\n50.17\n7.71\n31.0\n20.3\n-\n-\n10.12\n13.06\n11.9\n* Based on the authors’ code.\n† The reported result reﬂects an ANT ensemble.\nIn the next set of experiments, we compare models designed to handle small or structured datasets, as shown in Table\n3. The authors of these methods tested a wide range of datasets to evaluate their their approach. In this survey, we\nconducted a series of experiments on the UCI human activity recognition (HAR) dataset [165]. Here, human activity\nrecognition data were collected from 30 participants performing six scripted activities (walking, walking upstairs,\nwalking downstairs, sitting, standing, and laying) while wearing smartphones. The dataset contains 561 features\nextracted from sensors including an accelerometer and a gyroscope. The training set contains 7352 samples from 70%\nof the volunteers and the testing set contains 2947 samples from the remaining 30% of volunteers.\nTable 3: Classiﬁcation error rate (%) comparison.\nDataset/Models\nRF\nSVM\nMLP\nDART [139]\nRRF [135]\nGRRF [136]\nmGBDT [131]\nHAR\n6.96\n4.69\n4.69\n6.55\n3.77\n3.74\n7.68\nFrom the table, we observe that models representing multiple layers of machine learning models such as DF, R2SVM,\nand mGBDT did not perform well on the MNIST and CIFAR-10 datasets. Compared to DNNs, these models are com-\nputationally more expensive, require an excessive amount of resources, and do not offer advantages of interpretability.\nAnother category of models utilizes a neural network to learn a better representation of data. These models such\nas DNDF, and DNN+SVM applied a more traditional machine learning model on the newly-learned representation.\nThis technique could be beneﬁcial when the neural network has been trained on a large dataset. For example, DNDF\nutilized GoogLeNet for extracting features, and subsequently achieved a 6.38% error rate on the ImageNet testset.\nIn contrast, the GoogLeNet error rate is 10.02%. Another class of models enhances the tree model by integrating\nartiﬁcial neurons such as ANT, SFDT, and DBT. These models cleverly combine neural networks with decision trees\nthat improve interpretation while offering representation-learning beneﬁts.\nAnother hybrid strategy focused on decreasing the computational complexity of the DNNs. CondCNN is such a neural\nnetwork that employs a routing mechanism similar to a decision tree to achieve this goal. Another successful line of\nresearch is to add regularizers frequently used by the neural network to other classiﬁers similar to DART, RRF, and\nGRRF.\nThe results from our experiments reveal that both network classiﬁers and non-network classiﬁers beneﬁt from deep\nlearning. The methods surveyed in this paper and evaluated in these experiments demonstrate that non-network ma-\nchine learning models do improve performance by incorporating DNN components into their algorithms. Whereas\nmodels without feature learning such as RF usually do not perform well on unstructured data such as images, we\nobserve that adding deep learning to these models drastically improve their performance, as shown in Table 2. Addi-\ntionally, non-deep models may achieve improved performance on structured data by adding regularizers, as shown in\nTable 3. The methods surveyed in this paper demonstrate that deep learning components can be added to any type of\nmachine learning model, and are not speciﬁc to DNNs. The incorporation of deep learning strategies is a promising\ndirection for all types of classiﬁers, both network and non-network methods.\n8\nConclusions and Directions for Ongoing Research\nDNNs have emerged as a powerful force in the machine learning ﬁeld for the past few years. This survey paper reviews\nthe latest attempts to incorporate methods that are traditionally found in DNNs into other learning algorithms. DNNs\nwork well when there is a large body of training data and available computational power. DNNs have consistently\nyielded strong results for a variety of datasets and competitions, such as winning the Large Scale Visual Recognition\nChallenge [166] and achieving strong results for energy demand prediction [167], identifying gender of a text author\n[168], stroke prediction [169], network intrusion detection [170], speech emotion recognition [171], and taxi destina-\ntion prediction [172]. Since there are many applications which lack large amounts of training data or for which the\ninterpretability of a learned model is important, there is a need to integrate the beneﬁts of DNNs with other classiﬁer\n19\nA PREPRINT - SEPTEMBER 30, 2019\nalgorithms. Other classiﬁers have demonstrated improved performance on some types of data, therefore the ﬁeld can\nbeneﬁt from examining ways of combining deep learning elements between network and non-network methods.\nAlthough some work to date provides evidence that DNN techniques can be used effectively by other classiﬁers, there\nare still many challenges that researchers need to address, both to improve DNNs and to extend deep learning to other\ntypes of classiﬁers. Based on our survey of existing work, some related areas where supervised learners can beneﬁt\nfrom unique DNN methods are outlined below.\nThe most characteristic feature of DNNs is a deep architecture and its ability to learn a new representation of data.\nA variety of stacked generalization methods have been developed to allow other machine learning methods to utilize\ndeep architectures as well. These methods incorporate multiple classiﬁcation steps in which the input of the next layer\nrepresents the concatenation of the output of the previous layer and the original feature vector as discussed in Section\n3.1.1. Future work can explore the many other possibilities that exist for reﬁning the input features to each layer to\nbetter separate instances of each class at each layer.\nPrevious studies provide evidence that DNNs are effective data generators [173] [174], while in some cases non-\nnetwork classiﬁers may actually be the better discriminators. Future research can consider using a DNN as a generator\nand an alternative classiﬁer as a discriminator in generative adversarial models. Incorporating this type of model\ndiversity could improve the robustness of the models.\nGradient descent can be applied to any differentiable algorithm. We observed that Kontschieder et al. [127], Frosst et al.\n[128], Tanno et al. [129], and Zoran et al. [148] all applied gradient descent to two different tree-based algorithms by\nmaking them differentiable. In the future, additional classiﬁers can be altered to be differentiable. Applying gradient\ndescent to other algorithms could be an effective way to adjust the probability distribution of parameters.\nAnother area which is vital to investigate is the application of network-customized regularization methods unique to\nnon-network classiﬁers. As discussed in Section 5, the non-network classiﬁers can beneﬁt from the regularization\nmethods that are unique to DNNs. However, there exist many different ways that these regularization methods can be\nadapted by non-network classiﬁers to improve model generalization.\nAn important area of research is interpretable models. There exist applications such as credit score, insurance risk,\nhealth status because of their sensitivity, models need to be interpretable. Further research needs to exploit the use of\nDNNs in interpretable models such as DNDT [130].\nAs we discussed in this survey, an emerging area of research is to combine the complementary beneﬁts of statistical\nmodels with neural networks. Statistical models offer mathematical formalisms as well as possible explanatory power.\nThis combination may provide a more effective model than either approach used in isolation.\nThere are cases in which the amount of ground truth-labeled data is limited, but a large body of labeled data from\nthe same or similar distribution is available. One possible area of ongoing exploration is to couple the use of DNNs\nfor learning from unlabeled data with the use of other classiﬁer strategies for learning from labeled data. The simple\nmodel learned from labeled data can be exploited to further tune and improve learned representation patterns in the\nDNN.\nWe observe that currently, there is a general interest among the machine learning community to transfer new deep\nnetwork developments to other classiﬁers. While a substantial effort has been made to incorporate deep learning\nideas into the general machine learning ﬁeld, continuing this work may spark the creation of new learning paradigms.\nHowever, the beneﬁt between network-based learners and non-network learners can be bi-directional. Because a\ntremendous variety of classiﬁers has shown superior performance for a wide range of applications, future research can\nfocus not only on how DNN techniques can improve non-network classiﬁers but on how DNNs can incorporate and\nbeneﬁt from non-network learning ideas as well.\nAcknowledgment\nThe authors would like to thank Tharindu Adikari, Chris Choy, Ji Feng, Yani Ioannou, Stanislaw Jastrzebski and Marco\nA. Wiering for their valuable assistance in providing code and additional implementation details of the algorithms that\nwere evaluated in this paper. We would also like to thank Samaneh Aminikhanghahi and Tinghui Wang for their\nfeedback and guidance on the methods described in this survey. This material is based upon work supported by the\nNational Science Foundation under Grant No. 1543656.\n20\nA PREPRINT - SEPTEMBER 30, 2019\nTable 4: The list of abbreviations and their descriptions utilized in this survey.\nAbbreviation\nDescription\nAE\nAutoencoder\nANT\nAdaptive Neural Tree\nCNN\nConvolutional Neural Network\nCondNN\nConditional Neural Network\nDART\nDropout Multiple Additive Regression Trees\nDBT\nDifferentiable Boundary Tree\nDBN\nDeep Belief Network\nDCCA\nDeep Canonical Correlation Analysis\nDeep PCA\nDeep principal components analysis\nDF\nDeep Forest\nDGP\nDeep Gaussian Processes\nDKF\nDeep Kalman Filters\nDNDT\nDeep Network Decision Tree\nDNDF\nDeep Network Decision Forest\nDNN\nDeep Neural Network\nDSVM\nDeep SVM\nDTA-LS-SVM\nDeep Transfer Additive Kernel Least Square SVM\neForest\nEncoder Forest\nFC\nFully Connected\nGAF\nGenerative Adversarial Forest\nGAN\nGenerative Adversarial Network\nGRRF\nGuided Regularized Random Forest\nLMM\nLevel-wise Mixture Model\nmGBDT\nMultilayer Gradient Decision Tree\nML-SVM\nMultilayer SVM\nMLP\nMultilayer perceptron\nNLP-SVM\nNewton Linear Programming SVM\nR2-SVM\nRandom Recursive SVM\nRBM\nRestricted Boltzmann Machine\nRNN\nRecurrent Neural Network\nRRF\nRegularized Random Forest\nSCAD-SVM\nSmoothly Clipped Absolute Deviation SVM\nSDF\nSiamese Deep Forest\nSNN\nSiamese Neural Network\nFSDT\nFrosst Soft Decision Tree\nVAE\nVariational Autoencoder\nReferences\n[1] Cade Metz. AI is transforming google search. the rest of the web is next, 2016.\n[2] Nathan Sikes. Deep learning and the future of search engine optimization, 2015.\n[3] Alex Davies. The numbers don’t lie: Self-driving cars are getting good, 2017.\n[4] Neal Boudette. Tesla’s self-driving system cleared in deadly crash, 2017.\n[5] Autonomous vehicle disengagement reports 2017, 2017.\n[6] Fei Jiang, Yong Jiang, Hui Zhi, Yi Dong, Hao Li, Sufeng Ma, Yilong Wang, Qiang Dong, Haipeng Shen, and\nYongjun Wang. Artiﬁcial intelligence in healthcare: past, present and future. Stroke and vascular neurology,\n2(4):230–243, 2017.\n[7] Liang-Chieh Chen and Yukun Zhu. Semantic image segmentation with deeplab in tensorﬂow, 2018.\n[8] Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural networks using\ndropconnect. In International Conference on Machine Learning, pages 1058–1066, 2013.\n[9] Benjamin Graham. Fractional max-pooling. arXiv preprint arXiv:1412.6071, 2014.\n21\nA PREPRINT - SEPTEMBER 30, 2019\n[10] Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by\nexponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.\n[11] Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and\nPhil Blunsom. Teaching machines to read and comprehend. In Advances in Neural Information Processing\nSystems, pages 1693–1701, 2015.\n[12] Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. Neural\narchitectures for named entity recognition. arXiv preprint arXiv:1603.01360, 2016.\n[13] Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel, and Yoshua Bengio.\nEnd-to-end\nattention-based large vocabulary speech recognition. In Acoustics, Speech and Signal Processing (ICASSP),\n2016 IEEE International Conference on, pages 4945–4949. IEEE, 2016.\n[14] Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg, Carl Case, Jared\nCasper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al. Deep speech 2: End-to-end speech recognition\nin english and mandarin. In International Conference on Machine Learning, pages 173–182, 2016.\n[15] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436, 2015.\n[16] Thuy Ong. Amazon’s new algorithm designs clothing by analyzing a bunch of pictures, 2017.\n[17] Chenghui Tang, Yishen Wang, Jian Xu, Yuanzhang Sun, and Baosen Zhang. Efﬁcient scenario generation of\nmultiple renewable power plants considering spatial and temporal correlations. Applied Energy, 221:348–357,\n2018.\n[18] Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. Intelligible models\nfor healthcare: Predicting pneumonia risk and hospital 30-day readmission. In Proceedings of the 21th ACM\nSIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1721–1730. ACM, 2015.\n[19] Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High conﬁdence pre-\ndictions for unrecognizable images. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 427–436, 2015.\n[20] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning\nrequires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.\n[21] David Krueger, Nicolas Ballas, Stanislaw Jastrzebski, Devansh Arpit, Maxinder S Kanwal, Tegan Maharaj,\nEmmanuel Bengio, Asja Fischer, and Aaron Courville. Deep nets don’t learn via memorization. 2017.\n[22] Dmytro Mishkin and Jiri Matas. All you need is a good init. arXiv preprint arXiv:1511.06422, 2015.\n[23] Siddharth Krishna Kumar. On weight initialization in deep neural networks. arXiv preprint arXiv:1704.08863,\n2017.\n[24] Yoshua Bengio et al. Learning deep architectures for ai. Foundations and trends R⃝in Machine Learning,\n2(1):1–127, 2009.\n[25] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan,\nVincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 1–9, 2015.\n[26] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the\ngap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.\n[27] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of\nlanguage modeling. arXiv preprint arXiv:1602.02410, 2016.\n[28] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural\nnetworks. In 2013 IEEE international conference on acoustics, speech and signal processing, pages 6645–6649.\nIEEE, 2013.\n[29] Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convolutional neural networks for human action recognition.\nIEEE transactions on pattern analysis and machine intelligence, 35(1):221–231, 2013.\n[30] David H Wolpert, William G Macready, et al. No free lunch theorems for optimization. IEEE transactions on\nevolutionary computation, 1(1):67–82, 1997.\n[31] Ross D. King, Cao Feng, and Alistair Sutherland. Statlog: comparison of classiﬁcation algorithms on large\nreal-world problems. Applied Artiﬁcial Intelligence an International Journal, 9(3):289–333, 1995.\n[32] Tjen-Sien Lim, Wei-Yin Loh, and Yu-Shan Shih. A comparison of prediction accuracy, complexity, and training\ntime of thirty-three old and new classiﬁcation algorithms. Machine learning, 40(3):203–228, 2000.\n22\nA PREPRINT - SEPTEMBER 30, 2019\n[33] Rich Caruana and Alexandru Niculescu-Mizil. An empirical comparison of supervised learning algorithms. In\nProceedings of the 23rd international conference on Machine learning, pages 161–168. ACM, 2006.\n[34] Rich Caruana, Nikos Karampatziakis, and Ainur Yessenalina. An empirical evaluation of supervised learning\nin high dimensions. In Proceedings of the 25th international conference on Machine learning, pages 96–103.\nACM, 2008.\n[35] Philipp Baumann, DS Hochbaum, and YT Yang. A comparative study of the leading machine learning tech-\nniques and two new optimization algorithms. European journal of operational research, 272(3):1041–1057,\n2019.\n[36] Samira Pouyanfar, Saad Sadiq, Yilin Yan, Haiman Tian, Yudong Tao, Maria Presa Reyes, Mei-Ling Shyu, Shu-\nChing Chen, and SS Iyengar. A survey on deep learning: Algorithms, techniques, and applications. ACM\nComputing Surveys (CSUR), 51(5):92, 2018.\n[37] Benjamin Shickel, Patrick James Tighe, Azra Bihorac, and Parisa Rashidi.\nDeep ehr: A survey of recent\nadvances in deep learning techniques for electronic health record (ehr) analysis. IEEE journal of biomedical\nand health informatics, 22(5):1589–1604, 2018.\n[38] Junwei Han, Dingwen Zhang, Gong Cheng, Nian Liu, and Dong Xu. Advanced deep-learning techniques for\nsalient and category-speciﬁc object detection: a survey. IEEE Signal Processing Magazine, 35(1):84–100, 2018.\n[39] Jindong Wang, Yiqiang Chen, Shuji Hao, Xiaohui Peng, and Lisha Hu. Deep learning for sensor-based activity\nrecognition: A survey. Pattern Recognition Letters, 119:3–11, 2019.\n[40] William Grant Hatcher and Wei Yu. A survey of deep learning: platforms, applications and emerging research\ntrends. IEEE Access, 6:24411–24432, 2018.\n[41] Frank Rosenblatt. The perceptron: a probabilistic model for information storage and organization in the brain.\nPsychological review, 65(6):386, 1958.\n[42] Bernard Widrow and Marcian E Hoff. Adaptive switching circuits. Technical report, Stanford Univ Ca Stanford\nElectronics Labs, 1960.\n[43] Marvin Minsky and Seymour A Papert. Perceptrons: An introduction to computational geometry. MIT press,\n1969.\n[44] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations by error\npropagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive Science, 1985.\n[45] Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief nets. Neural\ncomputation, 18(7):1527–1554, 2006.\n[46] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014.\n[47] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between capsules. In I. Guyon, U. V.\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural\nInformation Processing Systems 30, pages 3856–3866. Curran Associates, Inc., 2017.\n[48] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1. MIT press\nCambridge, 2016.\n[49] Yann LeCun et al. Generalization and network design strategies. Connectionism in perspective, pages 143–155,\n1989.\n[50] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classiﬁcation with deep convolutional\nneural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural\nInformation Processing Systems 25, pages 1097–1105. Curran Associates, Inc., 2012.\n[51] Yi-Tong Zhou and Rama Chellappa. Computation of optical ﬂow using a neural network. In IEEE International\nConference on Neural Networks, volume 1998, pages 71–78, 1988.\n[52] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780,\n1997.\n[53] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine transla-\ntion. arXiv preprint arXiv:1406.1078, 2014.\n[54] Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. Deepface: Closing the gap to human-\nlevel performance in face veriﬁcation. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 1701–1708, 2014.\n23\nA PREPRINT - SEPTEMBER 30, 2019\n[55] Rachid Riad, Corentin Dancette, Julien Karadayi, Neil Zeghidour, Thomas Schatz, and Emmanuel Dupoux.\nSampling strategies in siamese networks for unsupervised speech representation learning.\narXiv preprint\narXiv:1804.11297, 2018.\n[56] Zara Alaverdyan, Julien Jung, Romain Bouet, and Carole Lartizien. Regularized siamese neural network for\nunsupervised outlier detection on brain multiparametric magnetic resonance imaging: application to epilepsy\nlesion screening. 2018.\n[57] Arthur L Samuel. Some studies in machine learning using the game of checkers. IBM Journal of research and\ndevelopment, 3(3):210–229, 1959.\n[58] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. D.\nLawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 2672–\n2680. Curran Associates, Inc., 2014.\n[59] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative\nadversarial text to image synthesis. arXiv preprint arXiv:1605.05396, 2016.\n[60] Artur Kadurin, Alexander Aliper, Andrey Kazennov, Polina Mamoshina, Quentin Vanhaelen, Kuzma Khrabrov,\nand Alex Zhavoronkov. The cornucopia of meaningful leads: Applying deep adversarial autoencoders for new\nmolecule development in oncology. Oncotarget, 8(7):10883, 2017.\n[61] Wengling Chen and James Hays. Sketchygan: Towards diverse and realistic sketch to image synthesis. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9416–9425, 2018.\n[62] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Gaugan: semantic image synthesis with\nspatially adaptive normalization. In ACM SIGGRAPH 2019 Real-Time Live!, page 2. ACM, 2019.\n[63] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks.\nscience, 313(5786):504–507, 2006.\n[64] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing\nrobust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine\nlearning, pages 1096–1103. ACM, 2008.\n[65] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In European Conference on\nComputer Vision, pages 649–666. Springer, 2016.\n[66] Detian Huang, Weiqin Huang, Zhenguo Yuan, Yanming Lin, Jian Zhang, and Lixin Zheng.\nImage super-\nresolution algorithm based on an improved sparse autoencoder. Information, 9(1):11, 2018.\n[67] Chih-Kuan Yeh, Wei-Chieh Wu, Wei-Jen Ko, and Yu-Chiang Frank Wang. Learning deep latent space for\nmulti-label classiﬁcation. In AAAI, pages 2838–2844, 2017.\n[68] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\n[69] Yoshua Bengio. Deep learning of representations: Looking forward. In International Conference on Statistical\nLanguage and Speech Processing, pages 1–37. Springer, 2013.\n[70] Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and\nmomentum in deep learning. In International conference on machine learning, pages 1139–1147, 2013.\n[71] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic\noptimization. Journal of Machine Learning Research, 12(Jul):2121–2159, 2011.\n[72] Diederik P Kingma and Jimmy Ba.\nAdam:\nA method for stochastic optimization.\narXiv preprint\narXiv:1412.6980, 2014.\n[73] Luis Miguel Rios and Nikolaos V Sahinidis. Derivative-free optimization: a review of algorithms and compari-\nson of software implementations. Journal of Global Optimization, 56(3):1247–1293, 2013.\n[74] Arild Nøkland. Direct feedback alignment provides learning in deep neural networks. In Advances in neural\ninformation processing systems, pages 1037–1045, 2016.\n[75] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society.\nSeries B (Methodological), pages 267–288, 1996.\n[76] Leo Breiman. Bagging predictors. Machine learning, 24(2):123–140, 1996.\n[77] Yoav Freund. Boosting a weak learning algorithm by majority. Information and computation, 121(2):256–285,\n1995.\n24\nA PREPRINT - SEPTEMBER 30, 2019\n[78] Mohammad Moghimi, Serge J Belongie, Mohammad J Saberian, Jian Yang, Nuno Vasconcelos, and Li-Jia Li.\nBoosted convolutional neural networks. In BMVC, 2016.\n[79] Jesse Eickholt and Jianlin Cheng. Dndisorder: predicting protein disorder using boosting and deep networks.\nBMC bioinformatics, 14(1):88, 2013.\n[80] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a sim-\nple way to prevent neural networks from overﬁtting. The Journal of Machine Learning Research, 15(1):1929–\n1958, 2014.\n[81] Luis Perez and Jason Wang. The effectiveness of data augmentation in image classiﬁcation using deep learning.\narXiv preprint arXiv:1712.04621, 2017.\n[82] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning\naugmentation policies from data. arXiv preprint arXiv:1805.09501, 2018.\n[83] Navdeep Jaitly and Geoffrey E Hinton. Vocal tract length perturbation (vtlp) improves speech recognition. In\nProc. ICML Workshop on Deep Learning for Audio, Speech and Language, volume 117, 2013.\n[84] Hiroki OHASHI, Mohammad AL-NASER, Sheraz AHMED, Takayuki AKIYAMA, Takuto SATO, Phong\nNGUYEN, Katsuyuki NAKAMURA, and Andreas DENGEL. Augmenting wearable sensor data with phys-\nical constraint for dnn-based human-action recognition. 2017.\n[85] Christopher Bowles, Liang Chen, Ricardo Guerrero, Paul Bentley, Roger Gunn, Alexander Hammers,\nDavid Alexander Dickie, Maria Valdés Hernández, Joanna Wardlaw, and Daniel Rueckert. Gan augmentation:\nAugmenting training data using generative adversarial networks. arXiv preprint arXiv:1810.10863, 2018.\n[86] Antreas Antoniou, Amos Storkey, and Harrison Edwards. Data augmentation generative adversarial networks.\narXiv preprint arXiv:1711.04340, 2017.\n[87] Javier Jorge, Jesús Vieco, Roberto Paredes, Joan-Andreu Sánchez, and José-Miguel Benedí. Empirical evalua-\ntion of variational autoencoders for data augmentation. In VISIGRAPP (5: VISAPP), pages 96–104, 2018.\n[88] Xiaofeng Liu, Yang Zou, Lingsheng Kong, Zhihui Diao, Junliang Yan, Jun Wang, Site Li, Ping Jia, and Jane\nYou. Data augmentation via latent space interpolation for image classiﬁcation. In 2018 24th International\nConference on Pattern Recognition (ICPR), pages 728–733. IEEE, 2018.\n[89] Yichuan Tang and Chris Eliasmith. Deep networks for robust visual recognition. In Proceedings of the 27th\nInternational Conference on Machine Learning (ICML-10), pages 1055–1062. Citeseer, 2010.\n[90] Ben Poole, Jascha Sohl-Dickstein, and Surya Ganguli. Analyzing noise in autoencoders and deep networks.\narXiv preprint arXiv:1406.1831, 2014.\n[91] David H Wolpert. Stacked generalization. Neural networks, 5(2):241–259, 1992.\n[92] Kai Ming Ting and Ian H Witten. Issues in stacked generalization. Journal of artiﬁcial intelligence research,\n10:271–289, 1999.\n[93] Zhi-Hua Zhou and Ji Feng. Deep forest: Towards an alternative to deep neural networks. arXiv preprint\narXiv:1702.08835, 2017.\n[94] Guanjin Wang, Guangquan Zhang, Kup-Sze Choi, and Jie Lu. Deep additive least squares support vector\nmachines for classiﬁcation with model transfer. IEEE Transactions on Systems, Man, and Cybernetics: Systems,\n2017.\n[95] Leo Breiman. Randomizing outputs to increase prediction accuracy. Machine Learning, 40(3):229–242, 2000.\n[96] Tin Kam Ho. Random decision forests. In Document analysis and recognition, 1995., proceedings of the third\ninternational conference on, volume 1, pages 278–282. IEEE, 1995.\n[97] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.\n[98] Gavin C Cawley. Leave-one-out cross-validation based model selection criteria for weighted ls-svms. In Neural\nNetworks, 2006. IJCNN’06. International Joint Conference on, pages 1661–1668. IEEE, 2006.\n[99] Hao Yang and Jianxin Wu. Practical large scale classiﬁcation with additive kernels. In Asian Conference on\nMachine Learning, pages 523–538, 2012.\n[100] Azizi Abdullah, Remco C Veltkamp, and Marco A Wiering. An ensemble of deep support vector machines for\nimage categorization. In Soft Computing and Pattern Recognition, 2009. SOCPAR’09. International Conference\nof, pages 301–306. IEEE, 2009.\n[101] Oriol Vinyals, Yangqing Jia, Li Deng, and Trevor Darrell. Learning with recursive perceptual representations.\nIn Advances in Neural Information Processing Systems, pages 2825–2833, 2012.\n25\nA PREPRINT - SEPTEMBER 30, 2019\n[102] Li Deng, Dong Yu, and John Platt. Scalable stacking and learning for building deep architectures. In Acoustics,\nSpeech and Signal Processing (ICASSP), 2012 IEEE International Conference on, pages 2133–2136. IEEE,\n2012.\n[103] Brian Hutchinson, Li Deng, and Dong Yu. Tensor deep stacking networks. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 35(8):1944–1957, 2013.\n[104] Li Deng and Dong Yu. Deep convex net: A scalable architecture for speech pattern classiﬁcation. In Twelfth\nAnnual Conference of the International Speech Communication Association, 2011.\n[105] Venice Erin Liong, Jiwen Lu, and Gang Wang. Face recognition using deep pca. In Information, Communica-\ntions and Signal Processing (ICICS) 2013 9th International Conference on, pages 1–5. IEEE, 2013.\n[106] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report,\nCiteseer, 2009.\n[107] Jonathon Shlens. A tutorial on principal component analysis. arXiv preprint arXiv:1404.1100, 2014.\n[108] Andreas Damianou and Neil Lawrence. Deep gaussian processes. In Artiﬁcial Intelligence and Statistics, pages\n207–215, 2013.\n[109] Ilya Sutskever and Geoffrey Hinton. Learning multilevel distributed representations for high-dimensional se-\nquences. In Artiﬁcial intelligence and statistics, pages 548–555, 2007.\n[110] Roland Memisevic and Geoffrey Hinton. Unsupervised learning of image transformations. In 2007 IEEE\nConference on Computer Vision and Pattern Recognition, pages 1–8. IEEE, 2007.\n[111] Kevin Swersky, Ilya Sutskever, Daniel Tarlow, Richard S Zemel, Ruslan R Salakhutdinov, and Ryan P Adams.\nCardinality restricted boltzmann machines. In Advances in neural information processing systems, pages 3293–\n3301, 2012.\n[112] Carl Edward Rasmussen. Gaussian processes in machine learning. In Summer School on Machine Learning,\npages 63–71. Springer, 2003.\n[113] César Lincoln C Mattos, Zhenwen Dai, Andreas Damianou, Jeremy Forth, Guilherme A Barreto, and Neil D\nLawrence. Recurrent gaussian processes. arXiv preprint arXiv:1511.06644, 2015.\n[114] Mark Van der Wilk, Carl Edward Rasmussen, and James Hensman. Convolutional gaussian processes. In\nAdvances in Neural Information Processing Systems, pages 2849–2858, 2017.\n[115] Zhenwen Dai, Andreas Damianou, Javier González, and Neil Lawrence. Variational auto-encoded deep gaus-\nsian processes. arXiv preprint arXiv:1511.06455, 2015.\n[116] Andreas Damianou. Deep Gaussian processes and variational propagation of uncertainty. PhD thesis, Univer-\nsity of Shefﬁeld, 2015.\n[117] Matthew M Dunlop, Mark A Girolami, Andrew M Stuart, and Aretha L Teckentrup. How deep are deep\ngaussian processes? The Journal of Machine Learning Research, 19(1):2100–2145, 2018.\n[118] David Duvenaud, Oren Rippel, Ryan Adams, and Zoubin Ghahramani. Avoiding pathologies in very deep\nnetworks. In Artiﬁcial Intelligence and Statistics, pages 202–210, 2014.\n[119] Lev V Utkin and Mikhail A Ryabinin. A siamese deep forest. arXiv preprint arXiv:1704.08715, 2017.\n[120] Yan Zuo, Gil Avraham, and Tom Drummond. Generative adversarial forests for better conditioned adversarial\nlearning. arXiv preprint arXiv:1805.05185, 2018.\n[121] Rudolph Emil Kalman. A new approach to linear ﬁltering and prediction problems. Journal of basic Engineer-\ning, 82(1):35–45, 1960.\n[122] Rahul G. Krishnan, Uri Shalit, and David Sontag. Deep kalman ﬁlters. arXiv preprint arXiv:1511.05121, 2015.\n[123] Shirli Di-Castro Shashua and Shie Mannor. Deep robust kalman ﬁlter. arXiv preprint arXiv:1703.02310, 2017.\n[124] Guo Lu, Wanli Ouyang, Dong Xu, Xiaoyun Zhang, Zhiyong Gao, and Ming-Ting Sun. Deep kalman ﬁltering\nnetwork for video compression artifact reduction. In Proceedings of the European Conference on Computer\nVision (ECCV), pages 568–584, 2018.\n[125] Ji Feng and Zhi-Hua Zhou. Autoencoder by forest. arXiv preprint arXiv:1709.09018, 2017.\n[126] Marco A Wiering and Lambert RB Schomaker. Multi-layer support vector machines. Regularization, optimiza-\ntion, kernels, and support vector machines, page 457, 2014.\n[127] Peter Kontschieder, Madalina Fiterau, Antonio Criminisi, and Samuel Rota Bulo. Deep neural decision forests.\nIn Computer Vision (ICCV), 2015 IEEE International Conference on, pages 1467–1475. IEEE, 2015.\n26\nA PREPRINT - SEPTEMBER 30, 2019\n[128] Nicholas Frosst and Geoffrey Hinton. Distilling a neural network into a soft decision tree. arXiv preprint\narXiv:1711.09784, 2017.\n[129] Ryutaro Tanno, Kai Arulkumaran, Daniel C Alexander, Antonio Criminisi, and Aditya Nori. Adaptive neural\ntrees. arXiv preprint arXiv:1807.06699, 2018.\n[130] Yongxin Yang, Irene Garcia Morillo, and Timothy M Hospedales. Deep neural decision trees. arXiv preprint\narXiv:1806.06988, 2018.\n[131] Ji Feng, Yang Yu, and Zhi-Hua Zhou.\nMulti-layered gradient boosting decision trees.\narXiv preprint\narXiv:1806.00007, 2018.\n[132] Glenn M Fung and Olvi L Mangasarian. A feature selection newton method for support vector machine classi-\nﬁcation. Computational optimization and applications, 28(2):185–202, 2004.\n[133] Hao Helen Zhang, Jeongyoun Ahn, Xiaodong Lin, and Cheolwoo Park. Gene selection using support vector\nmachines with non-convex penalty. bioinformatics, 22(1):88–95, 2005.\n[134] Jianqing Fan and Runze Li. Variable selection via nonconcave penalized likelihood and its oracle properties.\nJournal of the American statistical Association, 96(456):1348–1360, 2001.\n[135] Houtao Deng and George Runger. Feature selection via regularized trees. In Neural Networks (IJCNN), The\n2012 International Joint Conference on, pages 1–8. IEEE, 2012.\n[136] Houtao Deng and George Runger. Gene selection with guided regularized random forest. Pattern Recognition,\n46(12):3483–3489, 2013.\n[137] Sida Wang and Christopher D Manning. Baselines and bigrams: Simple, good sentiment and topic classiﬁcation.\nIn Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-\nVolume 2, pages 90–94. Association for Computational Linguistics, 2012.\n[138] Sida Wang and Christopher Manning. Fast dropout training. In international conference on machine learning,\npages 118–126, 2013.\n[139] Rashmi Korlakai Vinayak and Ran Gilad-Bachrach. Dart: Dropouts meet multiple additive regression trees. In\nArtiﬁcial Intelligence and Statistics, pages 489–497, 2015.\n[140] Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics, pages\n1189–1232, 2001.\n[141] Jerome H Friedman. Stochastic gradient boosting. Computational Statistics & Data Analysis, 38(4):367–378,\n2002.\n[142] Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Raffaele Perego, and Salvatore Trani. X-dart:\nBlending dropout and pruning for efﬁcient learning to rank. In Proceedings of the 40th International ACM\nSIGIR Conference on Research and Development in Information Retrieval, pages 1077–1080. ACM, 2017.\n[143] Sebastien C Wong, Adam Gatt, Victor Stamatescu, and Mark D McDonnell. Understanding data augmentation\nfor classiﬁcation: when to warp? arXiv preprint arXiv:1609.08764, 2016.\n[144] Ruo Xu. Improvements to random forest methodology. 2013.\n[145] Yani Ioannou, Duncan Robertson, Darko Zikic, Peter Kontschieder, Jamie Shotton, Matthew Brown, and\nAntonio Criminisi.\nDecision forests, convolutional networks and the models in-between.\narXiv preprint\narXiv:1603.01250, 2016.\n[146] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural\nnetworks. arXiv preprint arXiv:1505.05424, 2015.\n[147] Meire Fortunato, Charles Blundell, and Oriol Vinyals.\nBayesian recurrent neural networks.\nCoRR,\nabs/1704.02798, 2017.\n[148] Daniel Zoran, Balaji Lakshminarayanan, and Charles Blundell. Learning deep nearest neighbor representations\nusing differentiable boundary trees. arXiv preprint arXiv:1702.08833, 2017.\n[149] Charles Mathy, Nate Derbinsky, José Bento, Jonathan Rosenthal, and Jonathan S Yedidia. The boundary forest\nalgorithm for online supervised and unsupervised learning. In AAAI, pages 2864–2870, 2015.\n[150] Yichuan Tang. Deep learning using linear support vector machines. arXiv preprint arXiv:1306.0239, 2013.\n[151] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced research).\n[152] Xiao-Xiao Niu and Ching Y Suen. A novel hybrid cnn–svm classiﬁer for recognizing handwritten digits. Pattern\nRecognition, 45(4):1318–1325, 2012.\n27\nA PREPRINT - SEPTEMBER 30, 2019\n[153] Masoumeh Zareapoor, Pourya Shamsolmoali, Deepak Kumar Jain, Haoxiang Wang, and Jie Yang. Kernel-\nized support vector machine with deep learning: an efﬁcient approach for extreme multiclass dataset. Pattern\nRecognition Letters, 2017.\n[154] Jawad Nagi, Gianni A Di Caro, Alessandro Giusti, Farrukh Nagi, Luca Maria Gambardella, et al. Convolutional\nneural support vector machines: Hybrid visual pattern classiﬁers for multi-robot systems. In ICMLA (1), pages\n27–32, 2012.\n[155] Abdel Bellili, Michel Gilloux, and Patrick Gallinari. An hybrid mlp-svm handwritten digit recognizer. In\nDocument Analysis and Recognition, 2001. Proceedings. Sixth International Conference on, pages 28–32. IEEE,\n2001.\n[156] Washington W Azevedo and Cleber Zanchet. A mlp-svm hybrid model for cursive handwriting recognition. In\nNeural Networks (IJCNN), The 2011 International Joint Conference on, pages 843–850. IEEE, 2011.\n[157] Tianyi Zhao, Baopeng Zhang, Ming He, Wei Zhanga, Ning Zhou, Jun Yu, and Jianping Fan. Embedding visual\nhierarchy with deep networks for large-scale visual recognition. IEEE Transactions on Image Processing, 2018.\n[158] Harold Hotelling.\nRelations between two sets of variates.\nIn Breakthroughs in statistics, pages 162–190.\nSpringer, 1992.\n[159] David R Hardoon, Sandor Szedmak, and John Shawe-Taylor. Canonical correlation analysis: An overview with\napplication to learning methods. Neural computation, 16(12):2639–2664, 2004.\n[160] Galen Andrew, Raman Arora, Jeff Bilmes, and Karen Livescu. Deep canonical correlation analysis. In Interna-\ntional conference on machine learning, pages 1247–1255, 2013.\n[161] Kai Sheng Tai, Richard Socher, and Christopher D Manning. Improved semantic representations from tree-\nstructured long short-term memory networks. arXiv preprint arXiv:1503.00075, 2015.\n[162] David Alvarez-Melis and Tommi S Jaakkola. Tree-structured decoding with doubly-recurrent neural networks.\n2016.\n[163] Andrea Cimino and Felice Dell’Orletta. Tandem lstm-svm approach for sentiment analysis. In of the Final\nWorkshop 7 December 2016, Naples, page 172, 2016.\n[164] Abien Fred M Agarap. A neural network architecture combining gated recurrent unit (gru) and support vector\nmachine (svm) for intrusion detection in network trafﬁc data. In Proceedings of the 2018 10th International\nConference on Machine Learning and Computing, pages 26–30. ACM, 2018.\n[165] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, and Jorge Luis Reyes-Ortiz. A public domain\ndataset for human activity recognition using smartphones. In ESANN, 2013.\n[166] Large scale visual recognition challenge 2017 (ilsvrc2017), 2017.\n[167] Nikolaos G Paterakis, Elena Mocanu, Madeleine Gibescu, Bart Stappers, and Walter van Alst. Deep learning\nversus traditional machine learning methods for aggregated energy demand prediction. In Innovative Smart\nGrid Technologies Conference Europe (ISGT-Europe), 2017 IEEE PES, pages 1–6. IEEE, 2017.\n[168] Alexander Sboev, Ivan Moloshnikov, Dmitry Gudovskikh, Anton Selivanov, Roman Rybka, and Tatiana Litvi-\nnova.\nDeep learning neural nets versus traditional machine learning in gender identiﬁcation of authors of\nrusproﬁling texts. Procedia Computer Science, 123:424–431, 2018.\n[169] Chen-Ying Hung, Wei-Chen Chen, Po-Tsun Lai, Ching-Heng Lin, and Chi-Chun Lee. Comparing deep neural\nnetwork and other machine learning algorithms for stroke prediction in a large-scale population-based elec-\ntronic medical claims database. In Engineering in Medicine and Biology Society (EMBC), 2017 39th Annual\nInternational Conference of the IEEE, pages 3110–3113. IEEE, 2017.\n[170] Chuanlong Yin, Yuefei Zhu, Jinlong Fei, and Xinzheng He. A deep learning approach for intrusion detection\nusing recurrent neural networks. IEEE Access, 5:21954–21961, 2017.\n[171] Haytham M Fayek, Margaret Lech, and Lawrence Cavedon. Evaluating deep learning architectures for speech\nemotion recognition. Neural Networks, 92:60–68, 2017.\n[172] Alexandre De Brébisson, Étienne Simon, Alex Auvolat, Pascal Vincent, and Yoshua Bengio. Artiﬁcial neural\nnetworks applied to taxi destination prediction. arXiv preprint arXiv:1508.00021, 2015.\n[173] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional\ngenerative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.\n[174] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei A Efros, and Trevor\nDarrell. Cycada: Cycle-consistent adversarial domain adaptation. arXiv preprint arXiv:1711.03213, 2017.\n28\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-09-10",
  "updated": "2019-09-27"
}