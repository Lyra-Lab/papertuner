{
  "id": "http://arxiv.org/abs/1909.05815v1",
  "title": "Modeling Sensorimotor Coordination as Multi-Agent Reinforcement Learning with Differentiable Communication",
  "authors": [
    "Bowen Jing",
    "William Yin"
  ],
  "abstract": "Multi-agent reinforcement learning has shown promise on a variety of\ncooperative tasks as a consequence of recent developments in differentiable\ninter-agent communication. However, most architectures are limited to pools of\nhomogeneous agents, limiting their applicability. Here we propose a modular\nframework for learning complex tasks in which a traditional monolithic agent is\nframed as a collection of cooperating heterogeneous agents. We apply this\napproach to model sensorimotor coordination in the neocortex as a multi-agent\nreinforcement learning problem. Our results demonstrate proof-of-concept of the\nproposed architecture and open new avenues for learning complex tasks and for\nunderstanding functional localization in the brain and future intelligent\nsystems.",
  "text": "Modeling Sensorimotor Coordination as Multi-Agent\nReinforcement Learning with Differentiable\nCommunication\nBowen Jing\nDepartment of Computer Science\nStanford University\nbjing@stanford.edu\nWilliam Yin\nDepartment of Symbolic Systems\nStanford University\nwyin@stanford.edu\nAbstract\nMulti-agent reinforcement learning has shown promise on a variety of cooperative tasks as a\nconsequence of recent developments in differentiable inter-agent communication. However,\nmost architectures are limited to pools of homogeneous agents, limiting their applicability.\nHere we propose a modular framework for learning complex tasks in which a traditional\nmonolithic agent is framed as a collection of cooperating heterogeneous agents. We ap-\nply this approach to model sensorimotor coordination in the neocortex as a multi-agent\nreinforcement learning problem. Our results demonstrate proof-of-concept of the pro-\nposed architecture and open new avenues for learning complex tasks and for understanding\nfunctional localization in the brain and future intelligent systems.\n1\nIntroduction\nMotor coordination tasks have been among the most popular applications for reinforcement learning models\nand signiﬁcant milestones have been achieved in a diverse array of tasks in environments like MuJoCo[1]\nand OpenAI Gym[2]. Nearly all models of motor coordination have followed the paradigm of a single agent\nlearning to process observations from the environment and direct all actions accordingly. In this paradigm, a\nsingle agent is analogous to a single individual who learns to perform a certain motor task.\nIn this paper, we propose modeling the sensorimotor cortex as a collection of individual agents. We draw\nmajor inspiration from the neuro-anatomical localization of visual processing, proprio-sensory perception\nand motor planning to the occipital, parietal, and frontal lobes, respectively. Due to the spatial separation of\nthese functions in the brain, we explore the perspective that each cortex performs internal computations on\nsigniﬁcantly shorter timescales and signicantly higher bandwidths than messages between cortices. Given this,\nit is more appropriate to model the sensorimotor cortex as a group of smaller agents which communicate and\ncoordinate their actions to achieve a broader goal. Our approach is made possible by recent developments\nin multi-agent reinforcement learning which permit the learning of differentiable communication protocols\namongst cooperating agents, and to our knowledge is novel in the literature.\n2\nMulti-Agent Reinforcement Learning\nMulti-agent reinforcement learning (MARL) is a promising approach for modeling the dynamics of cooperative,\ncompetitive, or predator-and-prey relationships between agents, where each agent has some unique observation\nof the state of the world and then proceeds to make individual decisions which contribute to the world state\nat the next timestep. A recent example from Liu et al. — in which emergent cooperative behaviors were\nstudied in agents playing a game of soccer modeled in MuJoCo — highlights the extraordinary potential for\nmulti-agent learning within such a model[3], as does the work of Bard et al. on Hanabi, a cooperative card\ngame[4].\narXiv:1909.05815v1  [cs.MA]  12 Sep 2019\nThe mode of communication between agents remains an active area of research. Foerster et al.[5] recently pro-\nposed two means of inter-agent communication: Reinforced Inter-Agent Learning (RIAL) and Differentiable\nInter-Agent Learning (DIAL). RIAL describes an approach which employs deep Q-learning and allows com-\nmunication during action selection, while DIAL describes a means to backpropogate error derivatives through\nnoisy communication channels. DIAL is particularly interesting because it employs centralized learning but\ndecentralized execution; in other words, learning occurs across all agents as one unit, while action selection\nhappens on a per-agent basis where each agent treats the other agents as elements of the environment. On the\nother hand, Sukhbaatar et al.[6] describe the development of a neural model as a continuous communication\nchannel between agents (called CommNet) which facilitates the learning of a communication protocol among\nagents. In both of these examples, the protocol itself is not speciﬁed to the agents; rather, the agents are\n’tasked’ with learning the protocol themselves, and in doing so revealed interesting strategies to approaching\ninter-agent communication and information transfer.\nDespite these advancements, however, state-of-the-art MARL models still remain relatively structurally simple.\nIn particular, agents in MARL models remain relatively homogeneous, in that agent parameters are drawn\nfrom the same distributions. The agents themselves also have the same objectives, regardless of whether the\ntask is cooperative or competitive. In this sense, these MARL environments frequently fail to extend well to\nscenarios in which different agents have different roles or objectives.\nIn this sense, we see extraordinary potential in extending MARL models to scenarios in which agents may be\ndeﬁned with unique roles and properties. In particular, we hypothesized that such a model would be especially\nappropriate in modeling motor control tasks. There already exists a wealth of research in employing policy\ngradient techniques in modeling motor skills and control[7], but none which aim to model each component\nof a motor agent as a unique agent with unique properties. The sensorimotor cortex seems to be a perfect\ncandidate for such a model due to the separation of distinct components which interact[8]. Hence, our paper\nproposes the use of continuous communication protocols for message-passing across distinct, role-speciﬁc\nsensorimotor agents as models for components of a motor control task.\n3\nArchitecture\n3.1\nModel\nIn our architecture we model the sensorimotor cortex as a set S of ns sensory agents and a set M of nm\nmotor agents embedded in a fully deterministic Markov decision process with a set of discrete or continuous\nobservation spaces and a set of discrete action spaces1. Each sensory agent Si ∈S makes observations o[t]\ni in\nsome input domain and outputs a message mi,j to each motor agent2 (Mj, Qj) ∈M. The sensory agents do\nnot have access to the decision process and do not themselves take any actions. Rather, their sole effect is\nachieved by communicating a summary of the sensory information to the motor agents.\nEach motor agent consists of an M-net Mi and a Q-net Qi. The M-net takes in is all incoming messages (from\nall sensory agents as well as all other M-nets), in addition to the most recent action a[t−1]\ni\nof the corresponding\nmotor agent, and produces a message for each of the other motor units (Mj, Qj) ∈M, i ̸= j. The Q-net\nis a standard action-value function which takes as input all incoming messages from sensory agents and all\nmotor units, as well the previous action a[t−1]\ni\nand predicts a value for each possible action ai given the input\nmessages. Messages are not sent from the M-net of each motor unit to its corresponding Q-net because doing\nso would give encourage the M-net to serve as intermediate information-processing agents unafﬁliated with a\nparticular Q-net.\nIn our model of message passing, we set that each message is received by the receiving agent one time step\nafter it is sent. This serves dual purposes: ﬁrst, it captures the longer period of time it would take information\nto propogate from one part of the cortex to another; second, it enables the training of a message protocol via\ngradient descent despite a cyclic communication network.\nMore formally, each of our model’s sensory agents Si ∈S computes:\n\u0010\nm[t]\ni,1 . . . m[t]\ni,nm\n\u0011\n= Si\n\u0010\no[t]\ni\n\u0011\n1The model can easily be adapted to continuous action spaces.\n2Each motor agent is an ordered pair, as explained in the following paragraph.\n2\nwhere mi,j indicates a message vector from sensory agent Si to motor agent Mj. Each of the model’s motor\nunits (Mi, Qi) ∈M computes\n\u0010\n. . . m′[t]\ni,i−1, m′[t]\ni,i+1 . . .\n\u0011\n=\nMi\n\u0010\nm[t−1]\n1,i\n. . . m[t−1]\nns,i , . . . m′[t−1]\ni,i−1 , m′[t−1]\ni,i+1 . . . , a[t−1]\ni\n\u0011\nQ[t]\ni (a)\n=\nQi\n\u0010\nm[t−1]\n1,i\n. . . m[t−1]\nns,i , . . . m′[t−1]\ni,i−1 , m′[t−1]\ni,i+1 . . . , a[t−1]\ni\n, a\n\u0011\nwhere\n• m′\ni,j indicates a message vector from motor agent (Mi, Qi) to motor agent (Mj, Qj)\n• . . . m′\ni,i−1, m′\ni,i+1 . . . denotes the set of all m′\ni,j where i ̸= j\n• a is an element of Ai, the action space of agent (Mi, Qi)\n• Q[t]\ni is the action-value function for motor agent (Mi, Qi) at time t\nfor a total of nm(nm + ns −1) message streams. The controller then chooses the actions for each motor agent\naccording to\na[t]\ni ∼soft max\na∈Ai Q[t]\ni (a)\nif the controller is exploratory / learning mode and\na[t]\ni = arg max\na∈Ai Q[t]\ni (a)\nif it is in greedy mode. At each timestep t, the environment provides a reward R[t] in response to the actions a[t]\ni\ntaken. We train the action-value functions to predict Q[t]\ni (a[t]\ni ) = E[P∞\nτ=t γτ−tR[τ]] by performing stochastic\ngradient descent on the temporal-difference loss\n\u0012\nQ[t]\ni (a[t]\ni ) −R[t]\ni −γ arg max\na∈Ai Q[t+1]\ni\n(a)\n\u00132\n.\n3.2\nImplementation\nFor our training environment, we instantiate the architecture to feature three sensory agents and two motor\nagents. The sensory agents Sl, Se, and Sr represent the left arm propriosensory cortex, the visual cortex, and\nthe right arm propriosensory cortex, respectively, and the motor motor agents (Ml, Ql) and (Mr, Qr) controls\nthe left and right arm, respectively. The observation domains Sl, Sr ∈R2 represent the (x, y) coordinates of\nthe respective arm (we restrict the environment to two dimensions for simplicity), while Se ∈R4 encodes the\n(x, y) location and ˙x, ˙y velocity of an object of interest — in our case, a ball3. The action spaces Al and Ar\nconsist of ﬁve actions: moving the respective arms a ﬁxed amount in one of four directions, or making no\nmovements4.\nWe implement each functional component of the system (the sensory agents, the M-nets, and the Q-nets) as\na neural network with one ten-neuron hidden layer and ReLU activations operating on a concatenation of\nits inputs and outputting a concatenation of its outputs. While the architecture permits each message stream\nto be of different length, we implement all message channels to be of ﬁxed dimensionality dm = 5. This\nﬁxed dimensionality, the number of layers and hidden units in each neural network, the learning rate, and the\ndiscount factor 0 < γ < 1 are the hyperparameters of our model.\nThe model is trained by regarding the o[t−1]s, m[t−2]s, and m′[t−2]s as constant inputs to the model whose\noutput is the set of Q[t] functions at time t. Gradient descent is then simultaneously performed on all the\nnetwork weights.\n3In a more sophisticated system, the sensory agent may instead be a convolutional network operating on an input image\nstream.\n4In an ideal, more realistic model of muscle movement, the outputs would affect the acceleration of the arm.\n3\n4\nEnvironment\nWe train the model on a simpliﬁed juggling-like task, in which the model must bounce a ball using paddles in\neach hand and prevent it from touching the ground (Figure 1). Collisions are perfectly elastic, such that the\ntotal number of bounces is principally unbounded and the bounce frequency and heights is relatively constant5.\nEach bounce imparts a Gaussian-distributed random horizontal impulse to the ball; if the ball hits the wall, it is\ndeﬂected. The simulation resets when the ball hits the ground. Rewards are as follows:\n• A penalty of 50 if the ball hits the ground and the simulation resets.\n• A contact award of 50 each time the agent successfully bounces the ball. We found this shortening of\nthe time horizon to be necessary for a resonable rate of learning.\n• To incentivize coordination between the motor agents, a global penalty of 0.5 is applied to each\nmovement of any arm6.\nFigure 1: The environment consists of a ball which the agent must keep in the air by moving paddles in each\nhand. See details in text.\nThe values of these rewards, the bounce height, and simulation frequency may be adjusted as hyperparameters.\nIn our implementation of the environment, the physical parameters of the model are analogous to a human\nagent with arms at height 1m, bouncing a ball dropped from a height of 3m, with paddles 30cm across, in a\nbounding box of width 2m, arm movements of 15cm, and a message transit time of 0.1 seconds7.\n5\nExperiments\nWe train each of n = 10 instances of the model for 5000 sessions, where each session is deﬁned as ending\nwhen the ball hits the ground. We alternate 100-session exploratory learning epochs with 100-session greedy\nlearning epochs8. The greedy epochs revealed that the model learned to minimize movements by restricting\naction to one of the two agents while the other agent became increasingly dormant (Figure 2). This was\nan interesting and unexpected result, as it is highly analogous to the notion of hand dominance. However,\nno such trend was observed in the exploratory epochs, suggesting the preference was very slight (Figure 3).\nTaken at face value, the hand dominance in our model developed because the model found it more effective to\nuniversally suppress the action of one agent rather than trying to develop nuanced communications among\nagents.\n5This has the same effect as the more realistic assumption that the agent imparts an compensatory upward impulse after\na non-zero contact time, but that more sophisticated learning requirement may be better suited for a larger model.\n6An alternative perspective is to apply a penalty whenever both agents move.\n7Of course many alternative formulations exist, but we choose this distance scale for intuitive simplicity.\n8The exploratory and greedy controllers are as described above. Alternatively, the \"temperature\" of the softmax is may\nbe tuned.\n4\nFigure 2: Left arm movements as a fraction of total movements during the greedy training epochs. Notice that\nby the later epochs, a majority of models have a \"dominant\" or preferred agent to take action with.\nFigure 3: Left arm movements as a fraction of total movements during the exploratory training epochs. No\nsigniﬁcant deviation from an even distribution is observed.\nFigures 4 and 5 show the performance of the multi-agent model over the course of training. Despite performing\nthe best among reasonable conﬁgurations explored9, the model appears to exhibit only relatively weak learning.\nAs shown in Figure 4, the mean number of bounces per 100-session greedy learning epoch rose from 26.5 to\n41.8 in the ﬁrst 10 epochs (t(18) = −2.816, p = 0.011) but no signiﬁcant further improvement was observed\nin the remaining 15 epochs (t(18) = −0.327, p = 0.747). Note that this corresponds to an average of only\n0.4 bounces before the ball hits the ground, even at the end of training. In the exploratory learning epochs,\nperformance remained at around 37 bounces per epoch throughout, with no signs of learning observed (Figure\n5). This is reasonable, as the model may not be conﬁdent enough yet in the learned policy to achieve consistent\nperformance by drawing actions from a softmax rather than an argmax.\n9We were able to get much more impressive results by making the paddle comically large and the learning task almost\ntrivial.\n5\nFigure 4: Number of bounces per greedy learning epoch.\nFigure 5: Number of bounces per exploratory learning epoch.\n6\nDiscussion and Future Work\nWe have demonstrated the implementation of a multi-agent model of the sensorimotor cortex trained on a\nsimple ball-bouncing task. Although further hyperparameter tuning may improve performance to reasonable\nhuman-level expectations, we have demonstrated proof of principle that the proposed architecture is capable of\nlearning a sensorimotor coordination task. In particular, we have demonstrated that delayed message passing\nfrom sensory agents to motor agents is sufﬁcient to allow the learning of sensory-guided motor actions.\nFurther work is merited to conﬁrm the model of message-passing between motor agents as a means of\ncoordinating actions. Such validation could be performed by ablation studies in which message-passing\nbetween such agents is removed. The hand-dominance observed in the present experiments offers no evidence\nof multi-agent coordination, but is instead the chief unexpected result of this paper. Because all awards and\npenalties are shared by both agents, there is no feature of the architecture which would encourage the unilateral,\nconsistent suppression of a single agent. It is possible that the stochastic preferential usage of a particular agent\n6\nearly in training reinforces differences in the learned Q-values in such a way that further biases the model\ntowards the usage of that agent. The trajectories in Figure 2 support this possibility, as they generally converge\ntoward the dominance which was present to a slight degree in early epochs. If so, then hand dominance would\nappear to be a stable conﬁguration which naturally arises from an motion-conservative reward system and\ntrial-and-error learning. It is worth investigating or speculating if this has any connection with the formation\nof hand-dominance in humans.\nReferences\n[1] E. Todorov, T. Erez, and Y. Tassa, “MuJoCo: A physics engine for model-based control,” IEEE/RSJ International\nConference on Intelligent Robots and Systems, 2012.\n[2] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba, “OpenAI Gym,” arXiv\ne-prints, Jun. 2016.\n[3] S. Liu, G. Lever, J. Merel, S. Tunyasuvunakool, N. Heess, and T. Graepel, “Emergent Coordination Through\nCompetition,” arXiv e-prints, Feb. 2019.\n[4] N. Bard, J. N. Foerster, S. Chandar, N. Burch, M. Lanctot, H. F. Song, E. Parisotto, V. Dumoulin, S. Moitra, E. Hughes,\nI. Dunning, S. Mourad, H. Larochelle, M. G. Bellemare, and M. Bowling, “The Hanabi Challenge: A New Frontier\nfor AI Research,” arXiv e-prints, Feb. 2019.\n[5] J. N. Foerster, Y. M. Assael, N. de Freitas, and S. Whiteson, “Learning to Communicate with Deep Multi-Agent\nReinforcement Learning,” arXiv e-prints, May 2016.\n[6] S. Sukhbaatar, A. Szlam, and R. Fergus, “Learning Multiagent Communication with Backpropagation,” arXiv e-prints,\nMay 2016.\n[7] J. Peters and S. Schaal, “Reinforcement learning of motor skills with policy gradients,” Neural Networks, 2008.\n[8] T. Grent-’t-Jong, R. Oostenveld, W. P. Medendorp, and P. Praamstra, “Separating Visual and Motor Components of\nMotor Cortex Activation for Multiple Reach Targets: A Visuomotor Adaptation Study,” Journal of Neuroscience,\n2015.\n7\n",
  "categories": [
    "cs.MA",
    "cs.AI"
  ],
  "published": "2019-09-12",
  "updated": "2019-09-12"
}