{
  "id": "http://arxiv.org/abs/1909.06564v1",
  "title": "ALTER: Auxiliary Text Rewriting Tool for Natural Language Generation",
  "authors": [
    "Qiongkai Xu",
    "Chenchen Xu",
    "Lizhen Qu"
  ],
  "abstract": "In this paper, we describe ALTER, an auxiliary text rewriting tool that\nfacilitates the rewriting process for natural language generation tasks, such\nas paraphrasing, text simplification, fairness-aware text rewriting, and text\nstyle transfer. Our tool is characterized by two features, i) recording of\nword-level revision histories and ii) flexible auxiliary edit support and\nfeedback to annotators. The text rewriting assist and traceable rewriting\nhistory are potentially beneficial to the future research of natural language\ngeneration.",
  "text": "ALTER: Auxiliary Text Rewriting Tool for Natural Language Generation\nQiongkai Xu\nChenchen Xu\nThe Australian National University\nData61 CSIRO\nQiongkai.Xu@anu.edu.au\nChenchen.Xu@anu.edu.au\nLizhen Qu\nLaboratory for Dialogue Research\nMonash University\nLizhen.Qu@monash.edu\nAbstract\nIn this paper, we describe ALTER, an aux-\niliary text rewriting tool that facilitates the\nrewriting process for natural language gener-\nation tasks, such as paraphrasing, text simpli-\nﬁcation, fairness-aware text rewriting, and text\nstyle transfer. Our tool is characterized by two\nfeatures, i) recording of word-level revision\nhistories and ii) ﬂexible auxiliary edit support\nand feedback to annotators. The text rewrit-\ning assist and traceable rewriting history are\npotentially beneﬁcial to the future research of\nnatural language generation.\n1\nIntroduction\nGenerative modeling of editing text with respect\nto control attributes, coined GMETCA, has seen\nincreasing progress over the past few years. Such\na generative task is referred to as style transfer,\nwhen the control attributes indicate a change of\nwriting styles (Mir et al., 2019; Fu et al., 2018).\nThis generative task subsumes also gender obfus-\ncation (Reddy and Knight, 2016), authorship ob-\nfuscation (Shetty et al., 2018), and text simpliﬁca-\ntion (Xu et al., 2015), when the control attributes\nindicate protection of gender information, protec-\ntion of authorship, and simplifying the content and\nstructure of the text, respectively.\nThe research on GMETCA are impeded by the\nlack of standard evaluation practices (Mir et al.,\n2019; Tikhonov and Yamshchikov, 2018). Differ-\nent evaluation methods make system comparison\nacross publications difﬁcult. In light of this, Mir\net al. (2019); Fu et al. (2018) proposed both human\nevaluation and automated methods to judge style\ntransfer models on three aspects: a) style trans-\nfer intensity; b) content preservation; c) natural-\nness. However, it is still difﬁcult to reach an agree-\nment on how to measure to what extent a gener-\nated text satisfy all three criterion. Moreover, the\nlack of human generated gold references hinders\nthe progress of related research, as they i) auto-\nmate error analysis as in (Li et al., 2018); ii) avoid\nrepeated efforts in user studies to check if sys-\ntem outputs reproduce human-like editing. There-\nfore, it is beneﬁcial to collect gold references, hu-\nman edited text, as test corpora for those emerging\ntasks.\nThe collection of gold references can be con-\nducted on a crowd-sourcing platform, such as\nAmazon Mechanical Turk1, or through existing\nwriting tools (Goldfarb-Tarrant et al., 2019). How-\never, the existing crowd-sourcing platforms and\nannotation tools do not have the ﬂexibility to\nadd task-speciﬁc classiﬁers and language models,\nwhich are widely used for evaluating GMETCA\nmodels (Mir et al., 2019). As pointed out by Dow\net al. (2011), it is important to incorporate task-\nspeciﬁc feedback to achieve the improvement of\nuser engagement and quality of results. Feedback\nis particularly important for GMETCA according\nto our user study (details in Section 4.1), because\nannotators fail to capture the weak associations\nbetween certain textual patterns and attribute val-\nues. For example, for gender obfuscation on ‘The\ndessert is yummy !’, people can easily overlook the\nimplicit indicator ‘yummy’ of female authors.\nTo tackle the aforementioned challenges, we de-\nsign ALTER, an AuxiLiary TExt Rewriting tool,\nto collect gold references for GMETCA. Our tool\ncontains multiple models to provide feedback on\nrewriting quality and also allows easy incorpo-\nration of more task-speciﬁc evaluation models.\nIn addition, our tool has a module to record\nword-level revision histories with edit operations.\nThe revisions are decomposed into a sequence\nof word-level edit operations, such as insertions\n(I), deletions (D), and replacements (R), as illus-\n1https://www.mturk.com/\narXiv:1909.06564v1  [cs.CL]  14 Sep 2019\nOri: My husband and I enjoy LA Hilton Hotel.\nP1: Family enjoy LA Hilton Hotel. (Rs)\nP2: Family enjoy Hilton Hotel in LA. (Ro)\nP3: All family members enjoy Hilton Hotel in LA. (I)\nP4: All family members love Hilton Hotel in LA. (Rv)\n(a) Revision history 1 (RH1)\nOri: My husband and I enjoy LA Hilton Hotel.\nP1: My husband and I love LA Hilton Hotel. (Rv)\nP2: My husband and I love Hilton Hotel. (D)\nP3: My husband and I love Hilton Hotel in Los Angeles. (I)\nP4: My husband and I love Hilton Hotel in LA. (Ro)\nP5: Family love Hilton Hotel in LA. (Rs)\nP6: All family members love Hilton Hotel in LA. (I)\n(b) Revision history 2 (RH2)\nTable 1: Two revision histories, RH1 and RH2, from\n‘My husband and I enjoy LA Hilton Hotel.’ to ‘All\nfamily members love Hilton Hotel in LA.’. Although\nthe overall transformations of RH1 and RH2 are simi-\nlar, they follow different revision histories.\ntrated in Table 1. The beneﬁts of revision histories\nare three-fold. Firstly, revision histories can pro-\nvide supervision signals for the generative mod-\nels, which consider rewriting as applying a se-\nquence of edit operations on text (Li et al., 2018;\nGuu et al., 2018). Secondly, revision histories can\npotentially provide deep insights regarding cogni-\ntive process and human edit behaviours in vary-\ning demographic groups. For example, in Table 1,\nhuman writers could prefer replacing the subject\n(Rs) and the object (Ro) as RH1 than replacing\nthe verb (Rv) as RH2. Statistics on revision his-\ntories could provide supporting evidence about re-\nlated assumptions. Thirdly, there are often multi-\nple gold references for the same text. It is more ac-\ncessible using revision histories to acquire multi-\nple references than rewriting every reference from\nscratch. As shown in Table 1, P3, P4 in RH1 and\nP1, P3, P4 and P6 in RH2 are all valid revisions of\nthe original sentence.\nTo sum up, our contributions are:\n• We implemented a tool ALTER, which is ca-\npable of providing instant task-speciﬁc feed-\nback on rewriting quality for GMETCA.\n• ALTER records revision histories with edit\noperations, which are useful for comparing\nand analyzing human edit behaviours.\nThe code of ALTER is publicly available un-\nder MIT license at https://github.com/\nxuqiongkai/ALTER. A screencast video demo\nof our system is provided at Google drive.\n2\nRelated Work\nOur work is related to the research on edit history\nof text and assistant text rewriting.\nDocument-level edit records were used as data\nto analyze the evolution of knowledge base (Fer-\nschke et al., 2011; Medelyan et al., 2009) and re-\ntrieve sentence paraphrases (Max and Wisniewski,\n2010).\nIn contrast, our work focuses on word-\nlevel edit operations with order. We believe such\nparadigm introduces more linguistic features, that\nwill beneﬁt both linguistic and social behavior re-\nsearch. Recently, there has been a series of work\non conducting edit operations on text to advance\nautomatic natural language generation (Guu et al.,\n2018; Li et al., 2018). We believe the real-world\nhuman rewriting history collected by our system\nwill strengthen these works.\nA writing assistant has been proposed to facili-\ntate users, organizing and revising their document.\nZhang et al. (2016) proposed to detect the writers’\npurpose in the revised sentences. Goldfarb-Tarrant\net al. (2019) developed a collaborative human-\nmachine story-writing tool that assists writers with\nstory-line planning and story-detail writing. The\nassistant and feedback generally improved the user\nengagement and the quality of generated text in\nthose works.\n3\nALTER\nIn this section, we describe the design of ALTER,\nan auxiliary text rewriting tool that is able to i)\nprovide instant task-speciﬁc feedback to encour-\nage user engagement, and ii) trace the word-level\nrevision histories. We demonstrate an example of\nadapting our system on a GMETCA task, namely\ngenerating the gender-aware rewritten text, which\nis i) semantically relevant, ii) grammatically ﬂu-\nent, and iii) gender neutral.\n3.1\nSystem Overview\nFigure 1 depicts the overall architecture of ALTER,\nwhich consists of a rewriting module, an admin-\nistrative module, and multiple machine assistance\nservices. The rewriting module offers annotators\na user friendly interface for editing a given sen-\ntence with instant feedback.\nThe feedback and\nrevision histories in the interface are provided by\nthe machine assistance services. Moreover, the ad-\nministrative module provides administrators an in-\nterface for user management and assigning target\nFigure 1: System architecture of the Auxiliary Text Rewriting Tool (ALTER).\ntasks, which are basically a set of sentences for\nrewriting, as jobs to individual annotators.\nALTER is based on an easy-to-extend web-\nbased framework that follows the Model-View-\nController (Krasner et al., 1988) software de-\nsign pattern.\nThe models are the wrappers of\nthe databases (DB). The controller decides what\nshould be displayed on the interfaces, which are\nconsidered as the views.\nThis ﬂexible design\nenables various feedback providers to be easily\nplugged in and out, making it possible to support\ndifferent text generation tasks. The front-end is\ndeveloped with React2 that enables cross-platform\nsupport for major operating systems.\n3.2\nRewriting Interface\nFigure 2 illustrates a screenshot of the annotator\ninterface. In the left column, there is a list of jobs,\nwhich are the sentences assigned to the annotator.\nThe completed jobs are marked in blue. An anno-\ntator starts with selecting an incomplete job from\nthe job list, which will be shown in the auxiliary\nedit panel in the right column. We support two\nedit modes:\n• Direct typing mode: Annotators can directly\ntype a whole sentence into the text input ﬁeld.\nThis mode is provided for the annotators who\nprefer typing to clicking. To save time, the\noriginal sentence is copied to the input ﬁeld\nas default value.\n• Auxiliary mode: Annotators can click on a\nword shown above the text input ﬁeld, and\nchoose one of the edit operations from a set,\n2https://reactjs.org\nS = {Word Typing, Deletion, Substitution,\nReordering}. If the annotator chooses Substi-\ntution, he can select to show a list of words in\nthe gray panel recommended by either word\nsimilarity or a pre-trained language model.\nIn this mode, the annotator receives feedback\nfrom the upper right corner. Each feedback\nis a numerical score computed by a feedback\nprovider based on the current sentence. After\neach edition, a record is added to the revision\nhistory below, with the corresponding edit\noperation and the modiﬁed sentences. The\nannotators are also allowed to roll back the\nsentences to a previous status by clicking the\ncorresponding record in a history.\n3.3\nMachine Assistance Services\nThe machine assistance services in our system in-\nclude feedback providers and word recommenda-\ntion services. The machine assistance services can\nbe categorized as sentence-level and word-level.\nAt the sentence-level, we provide automatic\nsentence evaluation scores as feedback.\nIn our\ncurrent system, we consider evaluation metrics\nwidely used in style transfer and obfuscation of\ndemographic attributes (Mir et al., 2019; Zhao\net al., 2018; Fu et al., 2018).\n• PPL. PPL denotes the perplexity score of\nthe edited sentences based on the language\nmodel BERT3 (Devlin et al., 2019).\n• WMD. WMD is the word mover dis-\ntance (Kusner et al., 2015) between the origi-\n3https://github.com/google-research/\nbert\nFigure 2: The Auxiliary Text Rewriting Interface is composed of (a) a job list, (b) an auxiliary edit panel and (c) a\nlist of the revision history of current job.\nnal sentence and the edited sentence based on\nGoogle’s pre-trained Word2Vec model4.\n• ED. ED denotes the word edit distance be-\ntween the original sentence and the rewritten\nsentence.\n• Class. Class denotes the probability of the at-\ntribute value given the edited sentence. It is\nused to measure style transfer intensity or the\ndegree of obfuscation. In our user study, we\nemploy a transformer-based (Vaswani et al.,\n2017) binary classiﬁer trained on the Gen-\nder (Reddy and Knight, 2016) corpus, which\ncontains 2.6M balanced training samples.\nAt the word-level, we provide two word recom-\nmendation services for word substitution, which\nare based on word embedding similarity and lan-\nguage model, respectively.\nWe include also a\nword-level feedback provider, which character-\nizes the contributions of individual words to the\nsentence-level classiﬁcation results.\n• Word Similarity Recommendation. Given\na selected word, this service recommends a\nlist of words ranked by the cosine similarity\n4https://code.google.com/archive/p/\nword2vec\ncomputed based on pre-trained Google word\nembeddings.\n• Language Model Recommendation.\nThe\nservices apply a pre-trained language model\nBERT to the context around the selected\nword to predict top-k most likely words.\n• Salience.\nThis module utilizes the sen-\ntence classiﬁer trained on the Gender corpus\nto compute a salience score for each word.\nA salience score is deﬁned as S(X, i) =\nP(Y |X)−P(Y |X \\xi), where P(Y |X) de-\nnotes the probability of an attribute value Y\ngiven the input sentence X, and X \\ xi de-\nnotes the sentence X excluding the ith word.\n4\nUser Study\nWe conduct empirical studies to demonstrate i)\nannotators fail to capture certain textual patterns\nleading to worse estimation accuracy than the clas-\nsiﬁer; ii) ALTER improves user engagement; iii)\nmachine assistance consistently collects more ref-\nerences per sentence than asking annotators di-\nrectly typing edited sentences. Both studies are\nbased on the Gender (Reddy and Knight, 2016)\ndataset, which consists of reviews from Yelp an-\nnotated with the gender of the authors. In the ﬁrst\nstudy, we ask annotators to estimate the gender of\nauthors given a sentence. In the second study, We\nconsider a privacy-aware text rewriting task. We\nask annotators to rewrite sentences that i) leak less\ngender information, ii) maximally preserve con-\ntent; iii) are grammatically ﬂuent.\n4.1\nAwareness of Gender Information\nIn the ﬁrst study, we compare the accuracy of\npredicting gender information between two hu-\nman annotators and the classiﬁer5. Both of them\npredict the authors’ gender of 300 sentences ran-\ndomly sampled from the test set. Human anno-\ntators obtain merely 66.0 of accuracy on average,\nwhile the classiﬁer achieves 77.3. We have care-\nfully investigated the prediction results and the\nsampled sentences.\nWe found out that it is in-\ndeed difﬁcult for humans to estimate correctly the\nauthors’ gender based on a short piece of text,\ne.g.,“the food is delicious” and “the people were\nnice”. Both examples are perceived as neutral for\nour annotators. Apart from human failure to cap-\nture weak associations between certain textual pat-\nterns and gender, we conjecture that the bias in the\ncorpus may help the classiﬁer achieve better per-\nformance.\n4.2\nUser Engagement\nIn this study, three graduate students are invited to\nrewrite 100 sentences randomly selected from the\ntest set of the Gender corpus. All students take two\nsteps to rewrite each sentence:\n1. In the direct typing mode, type the edited sen-\ntence directly in the input ﬁeld .\n2. In the auxiliary mode, improve the edited\nsentence from the ﬁrst step when necessary.\nThe annotators are instructed that i) it is ﬁne\nto leave the sentences as they are if feed-\nback do not provide useful clues; ii) all feed-\nback and recommendations are machine gen-\nerated, thus not perfect.\nWe consider the two-step approach to compare the\ndifferences between the two modes while mini-\nmizing individual differences between annotators.\nWe analyze the revision history collected in the\nsecond step, and found out that feedback indeed\nleads to signiﬁcant improvement of user engage-\nment. In the second step, 89.67% of the sentences\n5We use a linear SVM model trained on Gender.\nFigure 3: Distribution of operations in revision his-\ntory by Word Typing, Deletion, Substitution, Reorder-\ning and Sentence Typing.\nwere modiﬁed in the auxiliary mode. The aver-\nage number of edit operations in the second step is\n4.63, showing the willingness of writers to further\nedit the text under auxiliary mode. The distribu-\ntion of edit operations is illustrated in Figure 3,\nword typing and deletion are clearly the most pop-\nular edit operations. Word recommendation ser-\nvices are also effective, contributing more than\n10% of the new edits in the auxiliary mode.\nThe references collected in the second step re-\nsult in less leakage of gender information than the\nones in the ﬁrst step. We measure the leakage of\ngender information by applying the transformer-\nbased classiﬁer on references collected in both\nsteps.\nWe compute averaged entropy score,\n−P\ni pi log pi, based on the predication of each\nclass pi. Higher entropy indicates better obfus-\ncation of gender. The sentences collected in the\nﬁrst step and the second step achieve 0.347 and\n0.535 respectively. The entropy of the sentences\ncollected in the ﬁrst step is just 0.027 better than\nthat of the original sentences.\nWe further investigate the revision histories, and\nﬁnd more gold references per sentence in the sec-\nond step than in the ﬁrst step. We consider se-\nmantically relevant and grammatically ﬂuent sen-\ntences as valid references. The average number of\nthe valid references generated in auxiliary mode\nis 3.79, while we can merely obtain one reference\nper sentence in the direct typing mode.\n5\nConclusion and Future Work\nIn this paper, we demonstrate our auxiliary text\nrewriting tool ALTER to collect gold references for\nGMETCA, assisted with word-level revision histo-\nries and task-speciﬁc instant feedback. In the fu-\nture, we will apply ALTER to collect high-quality\nbenchmarks for GMETCA.\nAcknowledgement\nThis project is supported by the partnership be-\ntween ANU and Data61/CSIRO. We also grate-\nfully acknowledge the funding from Data61 schol-\narship that supports Qiongkai Xu and Chenchen\nXu’s PhD research.\nReferences\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nSteven Dow, Anand Kulkarni, Brie Bunge, Truc\nNguyen, Scott Klemmer, and Bj¨orn Hartmann.\n2011. Shepherding the crowd: managing and pro-\nviding feedback to crowd workers. In CHI’11 Ex-\ntended Abstracts on Human Factors in Computing\nSystems, pages 1669–1674. ACM.\nOliver Ferschke, Torsten Zesch, and Iryna Gurevych.\n2011. Wikipedia revision toolkit: efﬁciently access-\ning wikipedia’s edit history. In Proceedings of the\n49th Annual Meeting of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies: Systems Demonstrations, pages 97–102.\nZhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan\nZhao, and Rui Yan. 2018.\nStyle transfer in text:\nExploration and evaluation. In Thirty-Second AAAI\nConference on Artiﬁcial Intelligence.\nSeraphina\nGoldfarb-Tarrant,\nHaining\nFeng,\nand\nNanyun Peng. 2019.\nPlan, write, and revise: an\ninteractive system for open-domain story gener-\nation.\nIn Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics (Demonstrations), pages\n89–97.\nKelvin Guu, Tatsunori B Hashimoto, Yonatan Oren,\nand Percy Liang. 2018.\nGenerating sentences by\nediting prototypes. Transactions of the Association\nof Computational Linguistics, 6:437–450.\nGlenn E Krasner, Stephen T Pope, et al. 1988. A de-\nscription of the model-view-controller user interface\nparadigm in the smalltalk-80 system. Journal of ob-\nject oriented programming, 1(3):26–49.\nMatt Kusner, Yu Sun, Nicholas Kolkin, and Kilian\nWeinberger. 2015. From word embeddings to docu-\nment distances. In International Conference on Ma-\nchine Learning, pages 957–966.\nJuncen Li, Robin Jia, He He, and Percy Liang. 2018.\nDelete, retrieve, generate: a simple approach to sen-\ntiment and style transfer. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers),\npages 1865–1874.\nAur´elien Max and Guillaume Wisniewski. 2010. Min-\ning naturally-occurring corrections and paraphrases\nfrom wikipedia’s revision history. In LREC.\nOlena Medelyan, David Milne, Catherine Legg, and\nIan H Witten. 2009.\nMining meaning from\nwikipedia.\nInternational Journal of Human-\nComputer Studies, 67(9):716–754.\nRemi Mir, Bjarke Felbo, Nick Obradovich, and Iyad\nRahwan. 2019.\nEvaluating style transfer for text.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 495–504.\nSravana Reddy and Kevin Knight. 2016. Obfuscating\ngender in social media writing. In Proceedings of\nthe First Workshop on NLP and Computational So-\ncial Science, pages 17–26.\nRakshith Shetty, Bernt Schiele, and Mario Fritz. 2018.\nA4nt: Author attribute anonymity by adversarial\ntraining of neural machine translation.\nIn 27th\n{USENIX} Security Symposium ({USENIX} Secu-\nrity 18), pages 1633–1650.\nAlexey Tikhonov and Ivan P. Yamshchikov. 2018.\nWhat is wrong with style transfer for texts? CoRR,\nabs/1808.04365.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nWei Xu, Chris Callison-Burch, and Courtney Napoles.\n2015.\nProblems in current text simpliﬁcation re-\nsearch: New data can help. Transactions of the As-\nsociation for Computational Linguistics, 3:283–297.\nFan Zhang, Rebecca Hwa, Diane Litman, and Homa B\nHashemi. 2016. Argrewrite: A web-based revision\nassistant for argumentative writings.\nIn Proceed-\nings of the 2016 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Demonstrations, pages 37–41.\nJunbo Zhao, Yoon Kim, Kelly Zhang, Alexander Rush,\nand Yann LeCun. 2018. Adversarially regularized\nautoencoders. In International Conference on Ma-\nchine Learning, pages 5897–5906.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2019-09-14",
  "updated": "2019-09-14"
}