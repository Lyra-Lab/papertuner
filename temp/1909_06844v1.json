{
  "id": "http://arxiv.org/abs/1909.06844v1",
  "title": "Wield: Systematic Reinforcement Learning With Progressive Randomization",
  "authors": [
    "Michael Schaarschmidt",
    "Kai Fricke",
    "Eiko Yoneki"
  ],
  "abstract": "Reinforcement learning frameworks have introduced abstractions to implement\nand execute algorithms at scale. They assume standardized simulator interfaces\nbut are not concerned with identifying suitable task representations. We\npresent Wield, a first-of-its kind system to facilitate task design for\npractical reinforcement learning. Through software primitives, Wield enables\npractitioners to decouple system-interface and deployment-specific\nconfiguration from state and action design. To guide experimentation, Wield\nfurther introduces a novel task design protocol and classification scheme\ncentred around staged randomization to incrementally evaluate model\ncapabilities.",
  "text": "WIELD: SYSTEMATIC REINFORCEMENT LEARNING\nWITH PROGRESSIVE RANDOMIZATION\nMichael Schaarschmidt 1 Kai Fricke 2 Eiko Yoneki 1\nABSTRACT\nReinforcement learning frameworks have introduced abstractions to implement and execute algorithms at scale.\nThey assume standardized simulator interfaces but are not concerned with identifying suitable task representations.\nWe present Wield, a ﬁrst-of-its kind system to facilitate task design for practical reinforcement learning. Through\nsoftware primitives, Wield enables practitioners to decouple system-interface and deployment-speciﬁc conﬁgura-\ntion from state and action design. To guide experimentation, Wield further introduces a novel task design protocol\nand classiﬁcation scheme centred around staged randomization to incrementally evaluate model capabilities.\n1\nINTRODUCTION\nFollowing high proﬁle successes in domains like games\nand robotics, interest in applications of deep reinforcement\nlearning (RL) has seen explosive growth. In computer sys-\ntems, RL has found applications across a diverse range of\ndomains such as scheduling (Mao et al., 2019b), networking\n(Valadarsky et al., 2017), database management (Marcus\n& Papaemmanouil, 2018; Marcus et al., 2019), and device\nplacement optimization (Mirhoseini et al., 2017; 2018).\nEssential for the proliferation of RL applications have been\nopen source implementations of popular algorithms. Algo-\nrithmic frameworks like RLlib (Liang et al., 2019), RLgraph\n(Schaarschmidt et al., 2019) or OpenAI baselines (Dhari-\nwal et al., 2017) allow practitioners to execute off-the-shelf\nalgorithms at scale. These frameworks standardize task\nexecution through shared interfaces such as OpenAI gym\n(Brockman et al., 2016). They do not concern themselves\nwith identifying problem representations.\nIn consequence, RL applications in systems have seen lim-\nited standardization. While a multitude of experimental\nsuccesses have been reported in controlled environments,\nreal-world data processing systems are yet to widely utilize\nRL. Experimental research often relies on highly customized\nbenchmarks, hardware setups, state-action representations,\nand proprietary simulators. Moreover, assessing evaluations\nis complicated by the use of ﬁxed workloads and limited\nreporting on the impact of random seeds and workload vari-\nation. Hence, applied research is fragmented, and novel\n1University\nof\nCambridge\n2Helmut-Schmidt-University\nHamburg.\nCorrespondence to:\nMichael Schaarschmidt\n<michael.schaarschmidt@cl.cam.ac.uk>.\napproaches are difﬁcult to reproduce. Their viability across\nlarger deployments or different tasks remains unclear.\nRL algorithms suffer from known limitations due to large\nsample requirements, sensitivity to hyper-parameters (Hen-\nderson et al., 2017), random weight initialization, and small\ninput perturbations (Kansky et al., 2017). In this paper, we\nargue that another root cause of limited real-world progress\nis a lack of shared evaluation protocols and design tools.\nSpeciﬁcally, both reinforcement learning agents and systems\nworkloads can exhibit several degrees of non-determinism.\nFor example, RL agents have been used in blackbox opti-\nmization settings where an agent is trained to optimize a\nsingle ﬁxed workload instance (e.g. a single computation\ngraph (Mirhoseini et al., 2018)). Within a blackbox setting,\nexperiments with varied random seeds can each perform on\nthe same ﬁxed task (ﬁxed blackbox), or sample a random\ntask instance per experiment to illustrate robustness to task\nvariation (randomized blackbox). Similar evaluation modes\ncan be applied to generalization problems, with additional\nconsideration for within- and out-of-distribution evaluation.\nTo begin addressing these difﬁculties, we present Wield, a\nﬁrst-of-its-kind system towards systematic task design and\nevaluation in applied RL. Wield makes two contributions:\nFirst, Wield provides a small set of reusable software primi-\ntives which decouple system interface and RL representa-\ntion from deployment-speciﬁc data and task layouts. These\nprimitives are coordinated through standardized workﬂows\nwhich help researchers explore new state, action, and reward\nmodels independent of system-speciﬁcs.\nSecond, we introduce progressive randomization, a novel\ntask evaluation protocol and classiﬁcation scheme which ex-\nplicitly delineates sources of non-determinism. Progressive\narXiv:1909.06844v1  [cs.LG]  15 Sep 2019\nWield: Systematic Reinforcement Learning With Progressive Randomization\nSystem under control\nDeﬁne layout in schema\nOrganize agent\nimplementations in\ntask graph\nTask graph\nRLlib\nRLgraph\nIncrementally\nincrease workload\ndifﬁculty /\nrandomization\nC_0\nProgressive\nrandomization\nC_1\n...\nA\nB\n+\nDBMS\nComputation graph\nWield\n...\nConverter\nState and action schema\nTranslate between \nagents and system\nFigure 1. Wield overview. To interface a system, users ﬁrst imple-\nment a schema specifying data layouts. Next, a converter uses the\nschema to implement a mapping between agent and system view.\nFinally, Wield coordinates RL-agents arranged in a task graph.\nProgressive randomization guides incremental evaluation.\nrandomization enables practitioners to communicate eval-\nuation assumptions, and to incrementally evaluate model\ncapabilities.\nIn the remainder of the paper, we ﬁrst introduce Wield’s de-\nsign abstractions and discuss common issues around model\ndesign in systems-RL (RL applied to systems). We then\nintroduce progressive randomization and use it to review re-\ncent work in systems-RL. In the evaluation, we demonstrate\nWield’s utility by reviewing the device placement problem,\na popular task in systems-RL. We reproduce prior work to\nclassify its capabilities through progressive randomization,\nand subsequently implement a novel placer with Wield. Our\nresults illustrate the true cost of evaluating RL solutions,\nand further call into question common evaluation practices\non ﬁxed datasets.\n2\nWIELD\n2.1\nOverview\nDelineating practical progress requires systematic assess-\nment and comparison of approaches. The aim of Wield is to\nprovide re-usable abstractions to standardize task design for\nsystems applications of reinforcement learning.\nFigure 1 gives a conceptual overview of Wield. On a high\nlevel, Wield acts as an interface between a data-processing\nsystem (e.g. database, scheduler, distributed execution en-\ngine) and a reinforcement learning framework such as RL-\ngraph or RLlib, auto-tuners, or any implementation exposing\na task interface. We implemented our Wield prototype us-\ning RLgraph’s agent implementations (Schaarschmidt et al.,\n2019). The highest level abstraction in Wield are workﬂows\nwhich coordinate execution of online (interacting with a\nsystem) or ofﬂine (e.g. log data) training, evaluation, and\nserialization of data and models.\nModels use task graphs to describe hierarchies of tasks\nwherein a single node may be a single differentiable agent\narchitecture, a blackbox auto-tuner, or a supervised model.\nTasks use converters to map between agent and system view\nof data, and schemas to standardize programmatic layouts\nof input states and actions. By separating representation\ndesign and system-speciﬁcs, task architectures can be used\nacross similar systems or problem structures which only\ndiffer in the system interface (e.g. different databases with\ndistinct query languages).\n2.2\nTask design abstractions\nWield’s task abstractions unify workﬂow streams via stan-\ndardized task and data layouts. Layout refers to the concrete\ndimensions, data types and processing steps for all inputs\nand outputs of an RL agent. As a running example, we use\nthe popular problem of placing a computational graph (e.g.\na TensorFlow graph) across a heterogeneous set of devices\nto minimize runtime of the update operation (Mirhoseini\net al., 2017; 2018; Addanki et al., 2019).\n2.3\nTask schemas\nState design. Task schemas are motivated by the observa-\ntion that states for systems problems need to be explicitly\ndesigned. In contrast, game simulators like Atari have ﬁxed\nstate dimensions (e.g. 640 × 480 images) across games.\nAll methods can rely on a ﬁxed base representation (i.e.\nthe original game frame) for reproducible and comparable\nexperiments. Wield schemas encapsulate input-dependent\nstate and action layout construction.\nConsider a state model for encoding an operation in a com-\nputational graph. The state may encode various types of\nsemantic problem information such as operator type embed-\ndings, tensor shapes, current device, and topology of the\nlocal graph neighborhood (Mirhoseini et al., 2018). While\niteratively exploring different representations, schemas cap-\nture the layout of the resulting states. Moreover, different\nresource types and layouts, i.e. dimensions of state arrays,\nmay be required per deployment due to different number of\ndevices and nodes. Schemas allow developers to express a\nstate layout as a function of system parameters. In practice,\ndevelopers may implement multiple schemas to iteratively\ncompare representations.\nStates can also encode bias towards decision horizons. For\nexample, Tesauro et al. describe a choice of state encoding\nin the context of server resource allocation via a discretized\nmean request arrival rate (Tesauro et al., 2006). Their state\nincludes both the current mean arrival rate and the one\nfrom the prior observation interval to relate the impact of\nactions to arrival rate. In workload management tasks, the\nWield: Systematic Reinforcement Learning With Progressive Randomization\nworkload generating process is generally unknown, and\nfuture workloads (e.g. request rates or job size) may be\nindependent or correlated to current decisions. State features\nand preprocessing, e.g. temporal smoothing, must encode\nsuch assumptions. In the device placement problem, states\nare deterministically computed as the graph is traversed in\ntopological order.\nIn summary, state design for systems-RL is an iterative\nprocess which differs from feature design for supervised\nlearning as the state must also capture transition dynam-\nics. To help researchers explore, compare and version state\ndesigns, Wield standardizes them through schemas.\nAction design. Similar to state design, action structure must\nbe designed manually as agent outputs need to be translated\nto structured system calls, e.g. by generating a special query\nto update the state of a database. Simple action representa-\ntions include single binary or categorical decisions where\nan action selects one of a small number of resources or task\nslots, e.g. which task to schedule next from a task queue,\nor which device to assign to an operation on a single node.\nThe term ’action structure’ refers to interpreting the outputs\nof a neural network. For example, in Q-learning, a neural\nnetwork used to represent the Q-function is designed by\ncreating a ﬁnal action selection layer with one neuron per\npossible integer action. The outputs are interpreted as Q-\nvalues. To output multiple distinct actions, multiple action\nlayers may be created. RL practitioners must explicitly con-\nsider how decision problems can be mapped to convenient\n(i.e. as few distinct actions as possible) representations.\nSuch representations may not scale to larger problem in-\nstances if the number of actions directly corresponds to prob-\nlem size. Consider a device placement problem scheduling a\nlarge computational graph across a cluster with tens of thou-\nsands of devices where each device would then correspond\nto an action. Without an informative prior, an agent would\nhave to ﬁrst observe performance dependencies across all\ntypes of devices. This would require an impractical number\nof samples to explore action combinations.\nThis is in contrast to well-conditioned continuous action\nspaces (e.g. for a physical actuator) where small changes\nin output correlate to small and predictable changes in the\nstate trajectory. Large discrete action spaces may require\ntask decomposition. If similarity between actions in large\ndiscrete action spaces is known in advance, actions can\nalso be selected in a multi-stage approach whereby ﬁrst an\naction in a promising region of the action space is selected,\nand a nearest-neighbor lookup is subsequently performed to\nidentify a ﬁtting local action (Dulac-Arnold et al., 2015).\nListing 1 shows two simpliﬁed single-task schemas for the\ndevice placement problem. One schema deﬁnes a one-\ndimensional input-array for a recurrent architecture (Mirho-\n1 NODE_OPTIONS = [’is_current_node’, ’is_placed’]\n2 MAX_NEIGHBORS = 5\n3\n4 class PlacementSchema(Schema):\n5\ndef _build_outputs(self, devices):\n6\nreturn IntBox(low=0, high=len(devices))\n7\n8 class RecurrentSchema(PlacementSchema):\n9\ndef _build_inputs(self, input_graph, devices):\n10\nnum_ops = count_ops(input_graph)\n11\nreturn FloatBox(shape=(num_ops, len(devices)))\n12\n13 class GraphSchema(PlacementSchema):\n14\ndef _build_inputs(self, input_graph, devices):\n15\nnum_ops = count_ops(input_graph)\n16\nnum_options = len(NODE_OPTIONS)\n17\nreturn Dict({\n18\n’embeddings’: FloatBox(\n19\nshape=(num_ops, num_options + len(devices))),\n20\n’current_node_num’: IntBox(low=0, high=num_ops),\n21\n’in_neighbors’: IntBox(shape=(num_ops, MAX_NEIGHBORS)),\n22\n’out_neighbors’: IntBox(shape=(num_ops, MAX_NEIGHBORS))\n23\n})\nListing 1. Task schemas deﬁne programmatic layout based on\ndeployment-speciﬁc parameters.\nseini et al., 2018) while the other deﬁnes layouts for a graph\nneural network (Addanki et al., 2019). Both share the same\naction layout based on available input devices. The example\nillustrates how system-speciﬁc conﬁgurations are used to\ndeﬁne layouts for states and actions.\nIn summary, schemas deﬁne physical layouts of states and\nactions, and decouple them from transition dynamics.\n2.4\nConverters\nWhere schemas correspond to physical layouts, converters\nare adapters expressing how system metrics, conﬁguration\nparameters, and query languages or custom protocols corre-\nspond to numerical representations within an optimization.\nThere is a many-to-many relationship between schemas and\nconverters. A schema specifying a layout can be used by\ndifferent converters, and a converter may work with different\nschemas. Schemas constrain how decision model is encoded\nstructurally (layout), converters specify how this encoding\nis achieved from raw system information (content). Listing\n2 shows the conversion API provided by Wield with an\nexample implementation for device placement. Workﬂows\ninvoke the converter API to translate between system and\nagent representation.\n2.5\nTask architectures\nSchemas and converters help decouple system-speciﬁcs\nfrom task representation in RL for a single task. Task graphs\norganize tasks into independent sub- and multi-task archi-\ntectures.\nShared-parameter tasks are multi-task architectures\nWield: Systematic Reinforcement Learning With Progressive Randomization\n1 # Maps system metrics to state inputs.\n2 def system_to_agent_state(system_state)\n3\ncurrent_op_id = system_state.current_op.id\n4\n5\nembeddings = []\n6\nfor op in self.schema.ops:\n7\nis_current_node = op.id == current_op_id\n8\nis_placed = op.id < current_op_id\n9\none_hot_devices = one_hot(\n10\nop.node.id, len=self.schema.num_devices))\n11\nempbeddings.append(\n12\n(is_current_node, is_placed, one_hot_devices))\n13\nin_neighbors = get_input_neighbors(system_state.current_op)\n14\nout_neighbors = get_output_neighbors(system_state.current_op)\n15\nreturn (embeddings, current_op_id,\n16\nin_neighbors, out_neighbors)\n17\n18 # Maps system command to numerical representation.\n19 def system_to_agent_action(system_action):\n20\n# System action is device name\n21\nreturn self.schema.device_name_to_index[system_action]\n22\n23 # Maps system metrics to single numerical reward.\n24 def system_to_agent_reward(system_metrics)\n25\nreturn -system_metrics[’run_time’]\n26\n27 # Maps agent outputs to system command\n28 def agent_to_system_action(agent_action)\n29\nreturn self.schema.index_to_device_name[agent_action]\nListing 2. Wield converter API example to translate between agent\nand system views..\nwhere a single end-to-end differentiable architecture has\nmultiple task output networks which each emit separate ac-\ntions per step. Independent tasks are task architectures\nwhere separate learners focus on different sub-tasks, e.g. in\nthe case of hierarchical decomposition or parallel indepen-\ndent tasks.\nNon-trivial task graphs occur through task decomposition\n(Figure 2). Hierarchical task decomposition refers to tasks\norganized as directed acyclic graphs where outputs from sin-\ngle tasks (vertices) are used as input states (edges) to other\ntask vertices. Independent tasks refers to a scenario where\nmultiple learners interact with an environment, possibly\nlearning at different time scales. Hierarchical reinforcement\nlearning has been studied in a variety of contexts with the\nmost well known approach being the options framework\n(Sutton et al., 1999). There, a top-level policy chooses\nbetween different sub-policies (options) to execute over a\ntime-frame (until the sub-task terminates). In Wield, we\nfocus on work ﬂows where users manually identify task\nhierarchies as a means of encoding domain knowledge.\nHierarchical designs to organize resources at different gran-\nularities are also a core element of systems research (e.g.\ncache hierarchies, hierarchical scheduling). However, hi-\nerarchical RL has found limited attention in the systems\ncommunity as a means to manage large state and action\nspaces. This could be due to most open source implemen-\ntations focusing on single-agent scenarios or unstructured\n(a) Single node task (b) Independent tasks\nT1\nT1\nT2\n(c) Hierarchical tasks\nT1\nT2\nFigure 2. Basic task architectures. (a) single task node contains\nmulti-task architecture with shared network (b) multiple indepen-\ndent learner instances. (c) a hierarchical task dependency.\ncollections of policies (e.g. RLlib (Liang et al., 2018)).\nTask graphs in Wield simplify factorization of tasks into\ndifferent sub-tasks which may train and act jointly, or at\ndifferent time-scales. Task objects primarily encapsulate\ndistinct agents or any other optimization implementing Ope-\nnAI gym (Brockman et al., 2016) interfaces. Hierarchical\ntasks often require to transform the output of one task before\ninputting it to a subsequent task, e.g. by enriching it with\nadditional environment information or preparing a speciﬁc\ninput format. Nodes in a task graph hence further encap-\nsulate pre-and post-processing for each sub-task. Edges in\nthe graph are implicitly created by creating one task as a\nsub-task of another task in the same task-graph. When per-\nforming inference, task outputs are routed through the task\ngraph based on user-deﬁned directed edges between tasks,\nand the results of all tasks during execution are returned.\nIn this section, we introduced a light-weight set of software\nprimitives for modularized task design. Next, we present\nprogressive randomization as the guidance mechanism to\nevaluate representations.\n3\nPROGRESSIVE RANDOMIZATION\n3.1\nThe case for task randomization\nA key obstacle when assessing model capabilities is the\nuse of ﬁxed workloads. In domains like database manage-\nment, query processing benchmarks often focus on narrow\napplication scenarios with small query sets (e.g. TPC-H,\nTPC-C). Leis et al. proposed the Join Order Benchmark\nwhich contains 113 queries speciﬁcally designed to investi-\ngate join estimation capabilities in query optimisers (Leis\net al., 2015). In device placement, researchers have relied\non ﬁxed graphs of standard architectures which may dif-\nfer in implementations (Mirhoseini et al., 2018), custom\nvariants of common architectures (Addanki et al., 2019), or\nproprietary datasets (Paliwal et al., 2019).\nWhile hand-designed workloads can highlight particular\nweaknesses or strengths of a system, they nevertheless are\nprone to over-ﬁtting small test sets. We argue that the design\nof RL mechanisms for systems can beneﬁt from synthetic\nworkload mechanisms with conﬁgurable task difﬁculty as a\nmeans to understand both training and inference behaviour.\nWield: Systematic Reinforcement Learning With Progressive Randomization\nTable 1. Progressive randomization protocol overview. Each class speciﬁes a different level of non-determinism.\nClass\nOptimization seed\nWorkload randomization\nExample use cases\nC0\nFixed\nFixed blackbox task\nIterate and debug representation\nC1\nRandom\nFixed blackbox task\nWeight initialization sensitivity\nC2\nRandom\nRandomized blackbox task\nModel sensitivity to task parameters\nC3\nRandom\nFixed in-distribution generalization\nUnderstand sample requirements\nC4\nRandom\nRandomized in-distribution generalization\nCustomized production use\nC5\nRandom\nFixed out-of-distribution generalization\nRobustness against unforeseen inputs\nC6\nRandom\nRandomized out-of-distribution generalization\nProduction use without customization\nReasoning about non-determinism when evaluating stochas-\ntic optimization mechanisms requires distinguishing deter-\nministic and non-deterministic elements in both workload\nand optimization procedure (network weight initialization).\nIn Wield, we construct workloads from the perspective of\nchanging between several evaluation and randomization\nmodes. We distinguish between blackbox and generaliza-\ntion mode. In blackbox mode, a single workload instance\n(e.g. a single set of queries or jobs) is generated and a model\nis trained and evaluated on that same instance. In generaliza-\ntion mode, training is executed on different instances than\nthe ones used in the ﬁnal evaluation.\nBoth modes can be executed with varying levels of ran-\ndomization. Workload determinism refers to deterministic\nbehaviour of task instances. Training determinism refers\nto deterministic initialization and sampling during training.\nFor example, in blackbox mode the generation of the single\ntask instance and the training initialization can both be de-\nterministic. Similarly, in generalization, both the instances\nused during training and the ﬁnal test instances can be ran-\ndomly generated or held ﬁxed. This invites problematic\npractices such as cherry-picking and presenting only suc-\ncessful combinations of workloads and weight initialization\nvalues (while omitting this selection).\nIn the RL literature, all combinations of blackbox and gener-\nalization modes can be found. Comparing results is difﬁcult\nif authors do not to report which workload elements are held\nﬁxed or are subject to randomization, or why a particular\nsample was chosen.\n3.2\nProgressive randomization\nOverview. Progressive randomization is based on the ob-\nservation that different randomization modes can serve dif-\nferent phases of design. For example, holding a workload\nﬁxed to study robustness against random initialization is\nvaluable when a designer is uncertain if a model design can\nsolve a task at all. Conversely, using a ﬁxed optimization\ncan be useful to study the impact of workload parameters\non optimization outcomes. Evaluation difﬁculties are not\ninherent to a speciﬁc mode of randomization or evaluation.\nThey arise when conﬂating sources of performance variation\nor misinterpreting model capabilities.\nIn supervised learning, projects such as DAWNBench (Cole-\nman et al., 2018) have suggested metrics like time-to-\naccuracy to compare model designs and hardware choices to\nunderstand trade-offs in deep learning systems. In contrast,\nshared RL tasks such as the Malmo Minecraft challenge\n(Johnson et al., 2016) or Unity agents (Juliani et al., 2018)\nare focused on performance in simulated worlds where ran-\ndomization is incidental. That is, workloads may include\nsome degree of randomization and generalization but these\nare not varied to analyse their contribution to agent perfor-\nmance (or lack thereof). Task variation in these scenarios\nis further constrained by experimental cost. The purpose\nof randomization is to evaluate model robustness to both\nsubtle and fundamental changes in workloads. For exam-\nple, in systems-RL, this requires gathering evidence about\nplausible workload distributions a controller may encounter.\nTable 1 lists the different evaluation modes in the protocol\nand their purpose. It also lists example applications. Fixed\noptimization parameters in practice refer to the random\nweight initialization strategies in neural networks, and fur-\nther to the random seed used when sampling mini-batches\nfor stochastic gradient descent as well as policy decisions.\nFixed blackbox refers to always training on the same work-\nload or problem instance, while ﬁxed generalization refers\nto an unseen but ﬁxed test task. Randomized generalization\nimplies that for each reported experiment result, a new test\ninstance was generated.\nNot all possible combinations of non-determinism are\npresent in the protocol. Fixed optimization parameters on\nﬁxed workloads are initially useful to produce repeatable\nresults and debug non-optimization components of a task\n(C0). For subsequent evaluation concerns, they should be\nrandomized to avoid cherry-picking ’lucky’ seeds. In C1\nWield: Systematic Reinforcement Learning With Progressive Randomization\nTable 2. Progressive randomization protocol overview. Each class speciﬁes a different level of non-determinism. If a range is given\nwithout an approximate estimate, this refers to different data sets being reported at different sample sizes. A * refers to researchers\nreporting median or mean across random optimization seeds.\nWork\nObjective\nHighest Ci reported\nNeural packet classiﬁcation (Liang et al., 2019)\nClassiﬁcation time/memory\nC0(n = 107, s =?, f =?)\nDevice placement (Mirhoseini et al., 2018)\nSGD iteration time\nC1(n = 103, s =?, f =?)\nDevice placement (Mirhoseini et al., 2017)\nSGD iteration time\nC1(n = 2 × 105, s = 4, f = 1.0)∗\nJoin order (Marcus & Papaemmanouil, 2018)\nQuery execution time\nC3(n ≈104, s =?, f =?)\nDevice placement (Addanki et al., 2019)\nSGD iteration time\nC3(n = 103 −105, s =?, f =?)\nCardinality predictions (Ortiz et al., 2018)\nPrediction error\nC3(n ≈105 −106, s =?, f =?)\nLanguage to program (Guu et al., 2017)\nProgram generation\nC3(n ≈104, s = 5, f = 1.0)∗\nSpark scheduling (Mao et al., 2018)\nSpark job completion times\nC4(n = 108, s =?, f =?)\nCongestion control (Jay et al., 2019)\nThroughput, latency\nC4(n ≈106 −107, s =?, f =?)\nQuery Optimizer (Marcus et al., 2019)\nImprove query latency\nC5(n ≈104 −106, s = 50, f = 1.0)∗\nComputation graph rewriting (Paliwal et al., 2019)\nMemory usage\nC5(n = 4 × 105, s = 20, f =?)\nAlphaZero (Silver et al., 2018)\nWin game of chess\nC6(n ≈109, s =?, f =?)\nand C2, weight initialization and workload instance (e.g.\na set of jobs sampled from a workload distribution) are\nincrementally randomized.\nGeneralization. Subsequent levels evaluate performance\non unseen problem instances. In-distribution generaliza-\ntion refers to workload assumptions where the test task is\ntaken from the same distribution training tasks were gen-\nerated from. For example, the device placement problem\ncan be randomized by varying batch size and sequence un-\nroll lengths on the same architecture (Addanki et al., 2019)\n(in-distribution), or by testing on entirely new architectures\n(out-of-distribution). Generalization semantics are compli-\ncated by task-speciﬁc concerns. For ﬁxed or randomized\ngeneralization tasks, there may be no useful measures of\nhow different test tasks are from training examples. Parts or\nthe entire test task could be seen during training, unless the\ntest task is held out and rejected.\nNonetheless, the description of a model to e.g. be in C3 for\na certain task gives useful indication of expected behaviour.\nWe refer to being in a class as to meeting application-speciﬁc\nperformance objectives under the given randomization as-\nsumptions. For example, a model in C2 which meets ran-\ndomized blackbox objectives can be used as a direct search\ntool in practice without requiring to retune hyper-parameters,\nwhereas a model in C1 tuned for a ﬁxed blackbox objec-\ntive is customized to a single deployment or task context.\nDistinguishing model classes sets expectations and allows\nresearchers to effectively communicate evaluation designs.\nGeneralization concerns in deep reinforcement learning are\npoorly understood. They are not well captured analytically\nbut rather empirically per task. This is primarily a conse-\nquence of limited understanding on generalization capabili-\nties of neural networks as policy vehicles. A model may be\nin different classes depending on the number the samples\nis trained on. In particular, researchers at OpenAI high-\nlighted in their work on competitive DOTA that massively\nincreasing model and sample size can induce qualitatively\ndifferent generalization behaviour (OpenAI, 2018). More-\nover, even for the same hyper-parameters, a large fraction\nof random weight initialization and optimization seeds may\nvary drastically in performance (Henderson et al., 2017).\nWe propose to describe models based on the these empirical\nproperties. Progressive randomization classiﬁcation thus in-\ncludes (i) the number of state transitions experienced during\ntraining n, (ii) the number of random seeds used for weight\ninitialization and optimizations sr, and (iii) the observed\nfrequency f where learning objectives were achieved.\nFor example, a model may be described as C1(n = 107, s =\n10, f = 0.4) to communicate empirical success when train-\ning 10 million samples and trying 10 different random seeds,\nwhere 4 of 10 trials met the objective. In the following nota-\ntion, we omit s and f from notation when only discussing\nsample count or class membership. Communicating success\nrates is especially important when considering the training\ncost on single-tasks without generalization.\nAs sample collection cost varies drastically between tasks,\nconditioning class membership on sample size is useful for\nestimates on model transfer on tasks with different sample\ncollection cost. For example, the same model may be in\nC1(n = 104) but in C3(n = 106) as robustness to inputs in-\ncreases with experiences seen during training. The number\nof sample trajectories seen during training may also corre-\nlate with the observed frequency of reaching an objective.\nLimitations. Progressive randomization encourages shared\nunderstanding of model capabilities across problem do-\nmains. Practitioners can use it to incrementally test new\nWield: Systematic Reinforcement Learning With Progressive Randomization\nimplementations. Several dimensions regarding model scale,\nthe cost of featurization, and other hidden cost are not cap-\ntured. The protocol also does not replace standard consid-\nerations on experiment design or statistical analysis. The\nclassiﬁcation system is intentionally simple to serve as a\nlow-overhead summary of design assumptions. While only\na starting point, progressive randomization constitutes the\nﬁrst explicit evaluation protocol for focused on delineating\nworkload randomization.\n3.3\nPrior work viewed through progressive\nrandomization\nWe use progressive randomization as a lens on prior work\nin research and applied RL. Table 2 classiﬁes selected prior\nwork, sorted by class membership. The classiﬁcation im-\nmediately illustrates problem progress. For example, in the\ndevice placement problem, Mirhoseini et al.’s initial work\n(Mirhoseini et al., 2017) with manual operation grouping\nrequired orders of magnitudes more samples than their sub-\nsequent work using a hierarchical approach (Mirhoseini\net al., 2018). Both operated in a ﬁxed blackbox setting.\nAddanki’s et al.’s (Addanki et al., 2019) and Paliwal et al’s\nrecent work (Paliwal et al., 2019) utilizing graph neural\nnetworks then illustrates progress towards generalization\nthrough permutation-invariant representations.\nIn our survey, we found subtle differences in evaluation\nrandomization which can be made make explicit through\nprogressive randomization. For example, Addanki et al. gen-\nerate random variations of computation graphs for training\nand testing, but both sets are ﬁxed (C3).\nA similar progress pattern can be observed in database\ntasks. In their ﬁrst work on join order enumeration, Mar-\ncus et al. (Marcus & Papaemmanouil, 2018) used a pol-\nicy optimization method on a ﬁxed set of training and test\nqueries, the Join Order Benchmark (JOB) (Leis et al., 2015).\nTraining with randomized optimization parameters yields\nC3(n ≈105). In subsequent work, Marcus et al. proposed\na learned query optimiser (Marcus et al., 2019) which they\nevaluate on several tasks, including a ﬁxed set of out-of-\ndistribution queries C5(n ≈104 −106, s = 50, f = 1.0).\nWe also observe that training workloads in database appli-\ncations were often generated by augmenting ﬁxed exist-\ning query sets (TPC-H, IMDB). It would be desirable for\nthe systems-RL community to develop shared standards on\ntraining and test randomization.\nMany approaches do not report explicitly how workload\nrandomization and optimization parameters were selected\nwhich makes classiﬁcation difﬁcult. If a ﬁxed task is pre-\nsented without reporting number of training trials, seeds, or\nrandomization assumptions (i.e. a potentially cherry picked\nsingle random seed), we assume C0.\nFew of the applied works we surveyed explicitly report\non failure modes, despite often using appendices to com-\nmunicate other training hyper-parameters. This highlights\nthe need for more explicit evaluation protocols. Evalua-\ntion times for systems-RL can vary drastically between\nmicroseconds and minutes. Sample-size classiﬁcation helps\nresearchers evaluate if a simulator may be needed.\nFinally, prior systems applications of RL are not typically\ndeﬁned through a binary objective such as winning a game\nor reaching a score threshold. Performance objectives are\nexplorative, e.g. outperforming problem-speciﬁc baselines.\nThis can obfuscate practical utility without cost-beneﬁt anal-\nysis on implementation cost.\n0\n200\n400\n600\n800\n1000\nGraph evaluation\n2.25\n2.50\n2.75\n3.00\n3.25\n3.50\nRuntime (s)\nGrappler runtime\nRandom placement runtime\n0\n200\n400\n600\n800\n1000\nGraph evaluation\n2.0\n2.5\n3.0\nRuntime (s)\nGrappler runtime\nFigure 3. Reproducing the hierarchical placer on the ﬁxed NMT\ngraph. Top: Fixed seed. Bottom: Random seed sampled per trial.\n4\nEVALUATION\nWe illustrate systematic assessment on the device placement\nproblem. We then subsequently implement a competing\nmodel and again incrementally evaluate it.\n4.1\nReproducing prior work\nWe use progressive randomization to evaluate the open\nsource TensorFlow device placer (’Grappler’) available as\npart of the grappler module1 (Mirhoseini et al., 2018).\nSetup. As a benchmark, we use the neural machine transla-\n1https://github.com/tensorflow/\ntensorflow/tree/master/tensorflow/python/\ngrappler\nWield: Systematic Reinforcement Learning With Progressive Randomization\ntion (NMT) architecture evaluated by all prior device place-\nment work. While some models were trained in a distributed\nsetting, reported NMT results ((Mirhoseini et al., 2018), Fig-\nure 3) illustrate strong performance improvements within\nhours of training using a single node. NMT is an attractive\nbenchmark task because the variants tested consist of an\nencoder-decoder architecture with multiple LSTM layers.\nPrior work shows placements must split training batches\nnon-trivially across GPUs and time dimensions.\nThe open source placer was not able to run evaluations\non Google’s NMT implementation with its own evaluation\nutilities. Serialization of a number of related metagraph\ncomponents failed. The results presented here were ob-\ntained by directly instantiating the TF graph and calling\ntraining operations. This signiﬁcantly increases cost per\nmeasurement. Each measurement was given one warm-up\nrun, and the subsequent measurement was reported to the\ncontroller. The open source hierarchical placer decays its\nlearning rate to 0 within 1000 updates, corresponding to the\nresults reported in the paper. Here, all results were ran at\nleast 1000 steps.\nFixed blackbox. We begin with the ﬁxed random seed,\nﬁxed workload setting (C0). Figure 3 (top) illustrates run-\ntime of the training operation (i.e. a single iteration of\nmini-batch stochastic gradient descent). Results are aver-\naged across runs, shaded areas indicate 1 standard deviation\nconﬁdence intervals. We used the random seed supplied by\nthe default conﬁguration in the open source implementation\n(1234), and repeated the experiment 10 times.\nThe placer identiﬁed improved placements in most runs with\na mean ﬁnal improvement (measured as the mean of the ﬁnal\n10 steps against the initial runtime) of 52%. One run failed\nto substantially improve in the end (5%). Invalid placements\nwere removed from the ﬁgures (assigned runtime value 100\nin the implementation).\nWe break down both the ﬁnal relative improvements and\nthe best-seen solution during training in Table 3. Results\nshow that i) all trials identiﬁed signiﬁcant improvements\nduring training, and ii) some trials diverged. Divergence\neven occurs with a ﬁxed random seed and a ﬁxed workload,\nlikely due to noise in the reward. We also contrast learned\nvalues against entirely random placements and groupings in\nFigure 3 (top) which fail to ﬁnd good placements.\nIn Figure 3 (bottom), the same graph is evaluated on 10\nrandomly chosen seeds (ﬁxed workload, randomised opti-\nmization parameters, C1). Mean improvement was 72%\nwith all random seeds achieving over 70% improvement.\nThe ﬁxed seed led to higher variance than the random seeds.\nWhile restricted to a single graph, reproduced results con-\nﬁrm the paper’s claims of reliable improving runtimes in\nblack-box settings. With a success criterion of at least 30%\nTable 3. Fixed blackbox improvements found by Grappler.\nTrial\nFinal model\nBest seen\nTrial 1\n57.0%\n69.0%\nTrial 2\n67.0%\n71.0%\nTrial 3\n25.0%\n69.0%\nTrial 4\n51.0%\n73.0%\nTrial 5\n5.0%\n74.0%\nTrial 6\n70.0%\n73.0%\nTrial 7\n69.0%\n69.0%\nTrial 8\n47.0%\n73.0%\nTrial 9\n68.0%\n70.0%\nTrial 10\n66.0%\n69.0%\nimprovement (minimum improvement in paper 35.9%), we\nreport C1(n = 1000, s = 10, f = 1.0).\nRandomized blackbox. In Figure 4 (top), we modify task\nparameters by varying batch size and unroll lengths in the\nrecurrent network to create a randomized blackbox setting\n(C2) where both graph and optimization seed are varied.\nBest runtimes are expected to differ due to different graph\nsizes. Of six trials, three succeeded, one failed entirely\n(1), and one (5) diverged from an effective conﬁguration\n(C2(n = 1000, s = 6, f = 0.83)).\n0\n200\n400\n600\n800\n1000\nGraph evaluation\n2\n3\n4\n5\nRuntime (s)\nTrial 1\nTrial 2\nTrial 3\nTrial 4\nTrial 5\nTrial 6\n0\n250\n500\n750\n1000\nGraph evaluation\n2\n3\n4\n5\n6\nRuntime (s)\nTrial 1\nTrial 2\nTrial 3\nTrial 4\nTrial 5\nTrial 6\nTrial 7\nTrial 8\nTrial 9\nTrial 10\nFigure 4. Randomized blackbox evaluation. Top: Random black-\nbox, random seed. Bottom: Repeats of failed trial 2.\nDid trial 2 fail due to posing a more difﬁcult placement task,\nor due to random initialization? In case of unclear failures,\nwe decrease randomization levels and re-evaluate the failed\ntask. We repeated the failed trial as a ﬁxed blackbox task\nWield: Systematic Reinforcement Learning With Progressive Randomization\nwith randomized optimization parameters. Figure 4 (bot-\ntom) shows results of rerunning the same task nine more\ntimes for a total of ten trials. Results include more fails\nand diverged results but also succeeding runs. Three trials\nfailed to improve placements signiﬁcantly. Performance\nvariation is further higher than in the published result, as\ngraph variations affect failure rate.\nFixed in-distribution generalization. Next, we consider\ngeneralization capabilities. We ﬁrst evaluate the ﬁnal trained\nmodel in each trained randomized graph against all other\ntrials’ graphs. Figure 5 show two examples of cross-graph\ncomparisons. Final models perform signiﬁcantly worse\nagainst the best seen solutions when training speciﬁcally\non the respective graph, with overheads against the best\nsolution ranging between 20% - 50%. Analysis of place-\nments shows (i) during training on a particular graph, non-\ntrivial placements using all devices are identiﬁed during\nexploration, and (ii) diverged ﬁnal placements default to\nsingle-device (CPU only) or single-GPU. When evaluat-\ning generalization, placements for slightly varied graphs\nfrequently defaulted to trivial single-GPU decisions.\nA\nB\nC\nD\nE\nF\nEvaluating controller: A\n0.0\n0.1\n0.2\nOverhead against best\nA\nB\nC\nD\nE\nF\nEvaluating controller: E\n0.0\n0.2\n0.4\nOverhead against best\nFigure 5. Generalization overhead example.\nThe failure to identify non-trivial placements for varied\ngraphs is hence both a result of models diverging and lim-\nited model capability. We compare detailed generalization\nresults and classify against our placer in the next section.\n4.2\nImplementing a placer with Wield\nNext, we use Wield’s primitives to implement and evaluate a\nnew placer. Our implementation combines insights from the\nopen source hierarchical placer and Addanki’s recent work\non using graph neural networks (Addanki et al., 2019). Ad-\ndanki et al. rely on manual grouping and identify effective\ngeneralizable placements based on i) incremental evaluation\nof placements, and ii) a neighborhood embedding in a graph\nneural network based on computing parallel, parent, and\nchild-operation groups.\nOur goal is to investigate if Wield’s abstractions can be used\nto re-implement customized architectures. We loosely com-\nbine both approaches using Wield’s task graph abstraction\nto implement an hierarchical graph network placer. Grouper\nand placer are proximal policy optimization agents (Schul-\nman et al.) learning independently at different time scales.\n0\n200\n400\n600\n800\n1000\nGraph evaluation\n2.25\n2.50\n2.75\n3.00\n3.25\n3.50\nRuntime (s)\nGrappler\nWield\nFigure 6. Open source hierarchical placer versus Wield placer.\nTable 4. Relative improvements in randomized blackbox scenario.\nTrial\nGrappler best\nWield best\nTrial 1\n22.6%\n37.4%\nTrial 2\n37.4%\n38.2%\nTrial 3\n37.8%\n32%\nTrial 4\n36.7%\n37.4%\nTrial 5\n37.2%\n41.5%\nTrial 6\n34.5%\n48%\nMean\n34.4% (5.4%)\n39.1% (4.9%)\nWe repeat the same NMT experiments used on the hierarchi-\ncal placer. To calibrate hyper-parameters, we ran ﬁve trial\nexperiments to identify effective learning rate schedules and\nincremental reward modes before ﬁnal experiments. Our\nmain ﬁnding was that the grouper required a more aggres-\nsive learning rate schedule, because initially random group-\nings caused the neighborhood computation in the placer to\ndiverge (all groups were connected to all others).\nFigure 6 compares operation runtimes over the course of\ntraining. Due to cost, we terminated some experiments early\nafter no more improvement was observed. Both implemen-\ntations frequently found their best placements within 500\ngraph evaluations. We repeated the randomized blackbox\nexperiment with 6 new graphs (C2) to evaluate sensitivity to\ngraph variations. In table 4, we compare relative runtime im-\nprovements across trials between Grappler and Wield. Mean\nimprovements are similar and differences not statistically\nsigniﬁcant (for p < 0.05).\nFinally, we also repeated the cross-graph generalization ex-\nperiment to investigate generalization capabilities of the\nstructured neural network representation. Since the network\ncomputes a permutation invariant embedding of operation\nneighborhood, higher robustness to input variants should\nbe observed. We show a detailed breakdown of both ap-\nproaches’ generalization capability by showing how the\nﬁnal model trained on a graph (rows) performs in terms of\nrelative runtime improvement on another graph (columns)\n(Tables 5 and 6). Grappler’s placer only in few instances\nsigniﬁcantly improved the initial placement (> 30%), with a\nWield: Systematic Reinforcement Learning With Progressive Randomization\nTable 5. Cross graph generalization breakdown of Grappler models.\nOn graph\nA\nB\nC\nD\nE\nF\nModel A\n0.31\n0.35\n0.28\n0.31\n0.27\n0.23\nModel B\n0.31\n0.37\n0.29\n0.32\n0.27\n0.26\nModel C\n0.25\n0.30\n0.23\n0.26\n0.22\n0.18\nModel D\n0.10\n0.15\n0.06\n0.10\n0.06\n0.03\nModel E\n0.16\n0.21\n0.14\n0.16\n0.14\n0.09\nModel F\n0.28\n0.32\n0.26\n0.29\n0.25\n0.21\nTable 6. Cross graph generalization breakdown of Wield models.\nOn graph\nA\nB\nC\nD\nE\nF\nModel A\n0.21\n0.60\n0.54\n0.50\n0.39\n0.19\nModel B\n-0.30\n0.33\n0.24\n0.09\n-0.02\n-0.22\nModel C\n-0.08\n0.44\n0.36\n0.23\n0.15\n0.05\nModel D\n0.09\n0.34\n0.32\n0.16\n0.05\n-0.08\nModel E\n-0.04\n0.45\n0.42\n-0.49\n-0.62\n-0.83\nModel F\n0.39\n0.42\n0.65\n0.58\n0.53\n0.44\nclassiﬁcation of C4(n = 1000, s = 36, f = 0.22). Wield’s\nplacer achieves C4(n = 1000, s = 36, f = 0.47) and ex-\nhibits successful in-distribution generalization with model\nF. Overall, the Wield placer performs like the custom-built\ntuned Grappler on blackbox tasks, and indicates potential on\ngeneralization. We stress that generalization training would\nnormally be executed across a distribution of tasks (as Ad-\ndanki et al. do), whereas we (due to signiﬁcant cost) only\ntrained on one single graph. Due to limited generalization\nsuccess, we did not evaluate higher randomization levels.\n4.3\nDiscussion\nWe systematically evaluated the open source placer through\nprogressive randomization. The hierarchical placer with\nhigh frequency identiﬁed effective placements in a ran-\ndomised blackbox scenario (C2) but failed to generalize\neven to slight input variations (C4). Our experiments high-\nlight the signiﬁcant cost associated with evaluating a model\non real-world system, even with a full set of pre-tuned hyper-\nparameters. Including all calibrations of the custom evalua-\ntion due to bugs in the open source code, it cost us $5000 to\nassess the hierarchical placer on public cloud infrastructure.\nWe also showed that using Wield, a competitive placer could\nbe implemented by combining off-the-shelf algorithmic\ncomponents. In both placers, models diverged after iden-\ntifying effective placements, and learning rate schedules\nwould need to be tuned to high precision to prevent this.\nBoth the true cost of such calibrations and the impact of\nworkload randomization have not been widely discussed in\nthe systems-RL community. With Wield and progressive\nrandomization, we aim to simplify randomization through\nstandardized workﬂows.\n5\nRELATED WORK\nOur work is inspired by the observations around evaluation\ndifﬁculties in deep RL in various prior lines of research.\nHenderson et al. observed how subtle implementation is-\nsues and random initialization drastically affect performance\n(Henderson et al., 2017) across implementations. Mania et\nal. subsequently demonstrated that an augmented random\nsearch outperformed (Mania et al., 2018) several policy op-\ntimization algorithms on supposedly difﬁcult control tasks.\nFurther recent work on policy gradient algorithms observed\nthat the performance of popular algorithms may depend on\nimplementation heuristics (e.g. learning rate annealing, re-\nward normalization) which are not part of the core algorithm\n(Ilyas et al., 2018). For real-world RL, Dulac-Arnold et al.\n(Dulac-Arnold et al., 2019) recently summarized a set of\nnine core challenges needed to safely reach production.\nIn the wake of identifying evaluation challenges around\nAtari games, researchers have proposed specialized simula-\ntors to benchmark speciﬁc properties such as generalization\ncapabilities (CoinRun (Cobbe et al., 2018)) or agent safety\n(e.g. DeepMind safety gridworlds (Leike et al., 2017)).\nBsuite (Osband et al.) is a novel benchmark for analyz-\ning agent behaviour which varies random seeds to score\nagent performance but which does not distinguish different\ngeneralization modes or randomized tasks.\nTo interface open source algorithm implementations, practi-\ntionerss have adopted OpenAI gym interfaces in novel sim-\nulators. For example, Siemens introduced a benchmark for\nindustrial control tasks (Hein et al., 2017). Others have built\ngym-bridges and new problem scenarios on to of existing\nsimulators such as the ns3 networking simulator (ns3-gym\n(Gawlowicz & Zubow, 2018)). In systems-RL, Park is a\nbenchmarking framework providing a common interface to\na variety of problems in query processing, networking, or\nscheduling (Mao et al., 2019a). Park takes a ﬁrst step to-\nwards shared task-design but does not explicitly include ran-\ndomization and distinct blackbox and generalization modes.\n6\nCONCLUSION\nWe introduced Wield, a new tool towards systematic task\nconstruction and model evaluation for applied RL. Wield\ndecouples application-speciﬁc protocols from task represen-\ntation. We also introduced progressive randomization, an\ninstructive evaluation protocol and classiﬁcation scheme to\nanalyze model capabilities under different randomization\nassumptions. Our assessment highlights the exciting recent\nprogress in systems-RL, while demonstrating the substantial\ncost of delineating model capabilities.\nWield: Systematic Reinforcement Learning With Progressive Randomization\nACKNOWLEDGMENTS\nMichael Schaarschmidt is supported by a Google PhD Fel-\nlowship. We are also grateful for receiving research credits\nfrom Google Cloud.\nREFERENCES\nAddanki, R., Venkatakrishnan, S. B., Gupta, S., Mao, H.,\nand Alizadeh, M. Placeto: Learning generalizable device\nplacement algorithms for distributed machine learning.\nCoRR, abs/1906.08879, 2019. URL http://arxiv.\norg/abs/1906.08879.\nBrockman, G., Cheung, V., Pettersson, L., Schneider, J.,\nSchulman, J., Tang, J., and Zaremba, W. Openai gym.\nCoRR, abs/1606.01540, 2016. URL http://arxiv.\norg/abs/1606.01540.\nCobbe, K., Klimov, O., Hesse, C., Kim, T., and Schulman,\nJ. Quantifying generalization in reinforcement learning.\narXiv preprint arXiv:1812.02341, 2018.\nColeman, C., Kang, D., Narayanan, D., Nardi, L., Zhao,\nT., Zhang, J., Bailis, P., Olukotun, K., R´e, C., and Za-\nharia, M. Analysis of dawnbench, a time-to-accuracy\nmachine learning performance benchmark.\nCoRR,\nabs/1806.01427, 2018. URL http://arxiv.org/\nabs/1806.01427.\nDhariwal, P., Hesse, C., Klimov, O., Nichol, A., Plappert,\nM., Radford, A., Schulman, J., Sidor, S., Wu, Y., and\nZhokhov, P. Openai baselines. https://github.\ncom/openai/baselines, 2017.\nDulac-Arnold, G., Evans, R., van Hasselt, H., Sunehag, P.,\nLillicrap, T., Hunt, J., Mann, T., Weber, T., Degris, T., and\nCoppin, B. Deep reinforcement learning in large discrete\naction spaces. 2015.\nDulac-Arnold, G., Mankowitz, D. J., and Hester, T. Chal-\nlenges of real-world reinforcement learning.\nCoRR,\nabs/1904.12901, 2019. URL http://arxiv.org/\nabs/1904.12901.\nGawlowicz, P. and Zubow, A.\nns3-gym:\nExtend-\ning openai gym for networking research.\nCoRR,\nabs/1810.03943, 2018. URL http://arxiv.org/\nabs/1810.03943.\nGuu, K., Pasupat, P., Liu, E. Z., and Liang, P. From lan-\nguage to programs: Bridging reinforcement learning and\nmaximum marginal likelihood. In Barzilay, R. and Kan,\nM. (eds.), Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics, ACL 2017,\nVancouver, Canada, July 30 - August 4, Volume 1: Long\nPapers, pp. 1051–1062. Association for Computational\nLinguistics, 2017. doi: 10.18653/v1/P17-1097.\nHein, D., Depeweg, S., Tokic, M., Udluft, S., Hentschel,\nA., Runkler, T. A., and Sterzing, V. A benchmark envi-\nronment motivated by industrial control problems. arXiv\npreprint arXiv:1709.09480, 2017.\nHenderson, P., Islam, R., Bachman, P., Pineau, J., Precup,\nD., and Meger, D. Deep reinforcement learning that\nmatters. CoRR, abs/1709.06560, 2017. URL http:\n//arxiv.org/abs/1709.06560.\nIlyas, A., Engstrom, L., Santurkar, S., Tsipras, D., Janoos,\nF., Rudolph, L., and Madry, A. Are deep policy gradi-\nent algorithms truly policy gradient algorithms? CoRR,\nabs/1811.02553, 2018. URL http://arxiv.org/\nabs/1811.02553.\nJay, N., Rotman, N. H., Godfrey, B., Schapira, M., and\nTamar, A. A deep reinforcement learning perspective\non internet congestion control. In Chaudhuri, K. and\nSalakhutdinov, R. (eds.), Proceedings of the 36th Inter-\nnational Conference on Machine Learning, ICML 2019,\n9-15 June 2019, Long Beach, California, USA, volume 97\nof Proceedings of Machine Learning Research, pp. 3050–\n3059. PMLR, 2019. URL http://proceedings.\nmlr.press/v97/jay19a.html.\nJohnson, M., Hofmann, K., Hutton, T., and Bignell, D. The\nmalmo platform for artiﬁcial intelligence experimentation.\nIn IJCAI, pp. 4246–4247, 2016.\nJuliani, A., Berges, V.-P., Vckay, E., Gao, Y., Henry, H.,\nMattar, M., and Lange, D. Unity: A general platform\nfor intelligent agents. arXiv preprint arXiv:1809.02627,\n2018.\nKansky, K., Silver, T., M´ely, D. A., Eldawy, M., L´azaro-\nGredilla, M., Lou, X., Dorfman, N., Sidor, S., Phoenix,\nS., and George, D. Schema networks: Zero-shot transfer\nwith a generative causal model of intuitive physics. 2017.\nLeike, J., Martic, M., Krakovna, V., Ortega, P. A., Everitt,\nT., Lefrancq, A., Orseau, L., and Legg, S. AI safety\ngridworlds. CoRR, abs/1711.09883, 2017. URL http:\n//arxiv.org/abs/1711.09883.\nLeis, V., Gubichev, A., Mirchev, A., Boncz, P., Kemper,\nA., and Neumann, T. How good are query optimizers,\nreally? Proceedings of the VLDB Endowment, 9(3):204–\n215, 2015.\nLiang, E., Liaw, R., Nishihara, R., Moritz, P., Fox, R., Gold-\nberg, K., Gonzalez, J., Jordan, M., and Stoica, I. Rllib:\nAbstractions for distributed reinforcement learning. In\nInternational Conference on Machine Learning, pp. 3059–\n3068, 2018.\nLiang, E., Zhu, H., Jin, X., and Stoica, I. Neural packet\nclassiﬁcation. CoRR, abs/1902.10319, 2019. URL http:\n//arxiv.org/abs/1902.10319.\nWield: Systematic Reinforcement Learning With Progressive Randomization\nMania, H., Guy, A., and Recht, B.\nSimple random\nsearch provides a competitive approach to reinforce-\nment learning.\nCoRR, abs/1803.07055, 2018.\nURL\nhttp://arxiv.org/abs/1803.07055.\nMao, H., Schwarzkopf, M., Venkatakrishnan, S. B.,\nMeng, Z., and Alizadeh, M. Learning scheduling al-\ngorithms for data processing clusters. arXiv preprint\narXiv:1810.01963, 2018.\nMao, H., Negi, P., Narayan, A., Wang, H., Yang, J., Wang,\nH., Marcus, R., Addanki, R., Khani, M., He, S., et al.\nPark: An open platform for learning augmented computer\nsystems. Reinforcement Learning for Real Life Work-\nshop, ICML, 2019a. URL https://github.com/\npark-project/park.\nMao, H., Schwarzkopf, M., Venkatakrishnan, S. B., Meng,\nZ., and Alizadeh, M. Learning scheduling algorithms\nfor data processing clusters.\nIn Wu, J. and Hall, W.\n(eds.), Proceedings of the ACM Special Interest Group on\nData Communication, SIGCOMM 2019, Beijing, China,\nAugust 19-23, 2019, pp. 270–288. ACM, 2019b. doi:\n10.1145/3341302.3342080.\nMarcus, R. and Papaemmanouil, O. Deep reinforcement\nlearning for join order enumeration. In Bordawekar, R.\nand Shmueli, O. (eds.), Proceedings of the First Interna-\ntional Workshop on Exploiting Artiﬁcial Intelligence Tech-\nniques for Data Management, aiDM@SIGMOD 2018,\nHouston, TX, USA, June 10, 2018, pp. 3:1–3:4. ACM,\n2018. doi: 10.1145/3211954.3211957.\nMarcus, R., Negi, P., Mao, H., Zhang, C., Alizadeh,\nM., Kraska, T., Papaemmanouil, O., and Tatbul, N.\nNeo: A Learned Query Optimizer. arXiv e-prints, art.\narXiv:1904.03711, Apr 2019.\nMirhoseini, A., Pham, H., Le, Q. V., Steiner, B., Larsen, R.,\nZhou, Y., Kumar, N., Norouzi, M., Bengio, S., and Dean,\nJ. Device placement optimization with reinforcement\nlearning. arXiv preprint arXiv:1706.04972, 2017.\nMirhoseini, A., Goldie, A., Pham, H., Steiner, B., Le, Q. V.,\nand Dean, J.\nHierarchical planning for device place-\nment.\n2018.\nURL https://openreview.net/\npdf?id=Hkc-TeZ0W.\nOpenAI. OpenAI Five DOTA. Website, June 2018. URL\nhttps://blog.openai.com/openai-five/.\nOrtiz, J., Balazinska, M., Gehrke, J., and Keerthi, S. S.\nLearning state representations for query optimization with\ndeep reinforcement learning. In Schelter, S., Seufert,\nS., and Kumar, A. (eds.), Proceedings of the Second\nWorkshop on Data Management for End-To-End Ma-\nchine Learning, DEEM@SIGMOD 2018, Houston, TX,\nUSA, June 15, 2018, pp. 4:1–4:4. ACM, 2018.\ndoi:\n10.1145/3209889.3209890.\nOsband, I., Doron, Y., Hessel, M., Aslanides, J., Sezener, E.,\nSaraiva, A., McKinney, K., Lattimore, T., Szepezvari, C.,\nSingh, S., Roy, B. V., Sutton, R., Silver, D., and Hasselt,\nH. V. Behaviour suite for reinforcement learning.\nPaliwal, A., Gimeno, F., Nair, V., Li, Y., Lubin, M.,\nKohli, P., and Vinyals, O. REGAL: transfer learning\nfor fast optimization of computation graphs.\nCoRR,\nabs/1905.02494, 2019. URL http://arxiv.org/\nabs/1905.02494.\nSchaarschmidt, M., Mika, S., Fricke, K., and Yoneki, E. RL-\ngraph: Modular Computation Graphs for Deep Reinforce-\nment Learning. In Proceedings of the 2nd Conference on\nSystems and Machine Learning (SysML), April 2019.\nSchulman, J., Abbeel, P., and Chen, X. Equivalence between\npolicy gradients and soft q-learning.\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai,\nM., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Grae-\npel, T., Lillicrap, T., Simonyan, K., and Hassabis, D.\nA general reinforcement learning algorithm that mas-\nters chess, shogi, and go through self-play.\nScience,\n362(6419):1140–1144, 2018.\nISSN 0036-8075.\ndoi:\n10.1126/science.aar6404. URL https://science.\nsciencemag.org/content/362/6419/1140.\nSutton, R. S., Precup, D., and Singh, S. P. Between mdps\nand semi-mdps: A framework for temporal abstraction in\nreinforcement learning. Artif. Intell., 112(1-2):181–211,\n1999. doi: 10.1016/S0004-3702(99)00052-1.\nTesauro, G., Jong, N. K., Das, R., and Bennani, M. N. A\nhybrid reinforcement learning approach to autonomic re-\nsource allocation. In Proceedings of the 2006 IEEE Inter-\nnational Conference on Autonomic Computing, ICAC ’06,\npp. 65–73, Washington, DC, USA, 2006. IEEE Computer\nSociety. ISBN 1-4244-0175-5. doi: 10.1109/ICAC.2006.\n1662383.\nURL http://dx.doi.org/10.1109/\nICAC.2006.1662383.\nValadarsky, A., Schapira, M., Shahaf, D., and Tamar, A.\nA machine learning approach to routing. arXiv preprint\narXiv:1708.03074, 2017.\nWield: Systematic Reinforcement Learning With Progressive Randomization\nA\nEXPERIMENT HYPERPARAMETERS\nWe list all hyperparameters used in Wield’s hierarchical\nplacer. Table 7 lists grouper parameters.\nTable 7. Training parameters used for Wield’s grouper agent.\nParameter\nValue\nclip ratio\n0.25\ndiscount\n1.0\nGAE λ\n1.0\nbatch size\nnum ops in graph\nupdate iterations per batch\n10\nnum groups\n20\npolicy layer size\n32\nnum hidden layers\n2\nlayer activation\ntanh\nvalue function\nsame as policy\noptimizer\nAdam\nlearning rate\n[0.01, 0.00001]\nlinear decay steps\n600\nTable 8 lists placer parameters. The embedding implementa-\ntion is a faithful implementation of Addanki et al.’s descrip-\ntion (Addanki et al., 2019). The main difference is that their\nwork relies on manual grouping. When we tested initially\nrandom groupings produced by the grouper, neighborhood\nembeddings diverged due to large neighborhood sets. We\nhence used tanh activations instead of rectiﬁed linear units,\nand conﬁgured the grouper to a more aggressive learning\nrate schedule.\nTable 8. Training parameters used for Wield’s placer agent.\nParameter\nValue\nclip ratio\n0.2\ndiscount\n1.0\nGAE λ\n1.0\nbatch size\n10\nupdate iterations per batch\n1\nlayer size (all layers)\nnum groups\nnum in neighbors\n5\nnum out neighbors\n5\nneighborhood aggregation rounds\n10\nlayer activation\ntanh\nvalue function\nsame as policy\noptimizer\nAdam\nlearning rate\n[0.0003, 0.0001]\nlinear decay steps\n1000\ngroups placed per graph evaluation\n10\nThe NMT architecture was taken from Google’s NMT im-\nplementation2. We used the ’normed bahdanau’ attention\nlayer with the ’gnmt v2’ architecture.\n2https://github.com/tensorﬂow/nmt\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-09-15",
  "updated": "2019-09-15"
}