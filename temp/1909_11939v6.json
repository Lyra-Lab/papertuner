{
  "id": "http://arxiv.org/abs/1909.11939v6",
  "title": "MERL: Multi-Head Reinforcement Learning",
  "authors": [
    "Yannis Flet-Berliac",
    "Philippe Preux"
  ],
  "abstract": "A common challenge in reinforcement learning is how to convert the agent's\ninteractions with an environment into fast and robust learning. For instance,\nearlier work makes use of domain knowledge to improve existing reinforcement\nlearning algorithms in complex tasks. While promising, previously acquired\nknowledge is often costly and challenging to scale up. Instead, we decide to\nconsider problem knowledge with signals from quantities relevant to solve any\ntask, e.g., self-performance assessment and accurate expectations.\n$\\mathcal{V}^{ex}$ is such a quantity. It is the fraction of variance explained\nby the value function $V$ and measures the discrepancy between $V$ and the\nreturns. Taking advantage of $\\mathcal{V}^{ex}$, we propose MERL, a general\nframework for structuring reinforcement learning by injecting problem knowledge\ninto policy gradient updates. As a result, the agent is not only optimized for\na reward but learns using problem-focused quantities provided by MERL,\napplicable out-of-the-box to any task. In this paper: (a) We introduce and\ndefine MERL, the multi-head reinforcement learning framework we use throughout\nthis work. (b) We conduct experiments across a variety of standard benchmark\nenvironments, including 9 continuous control tasks, where results show improved\nperformance. (c) We demonstrate that MERL also improves transfer learning on a\nset of challenging pixel-based tasks. (d) We ponder how MERL tackles the\nproblem of reward sparsity and better conditions the feature space of\nreinforcement learning agents.",
  "text": "MERL: Multi-Head Reinforcement Learning\nYannis Flet-Berliac and Philippe Preux\nSequeL Inria, University of Lille\nCRIStAL, CNRS\nAbstract\nA common challenge in reinforcement learning is how to convert the agent’s inter-\nactions with an environment into fast and robust learning. For instance, earlier work\nmakes use of domain knowledge to improve existing reinforcement learning algo-\nrithms in complex tasks. While promising, previously acquired knowledge is often\ncostly and challenging to scale up. Instead, we decide to consider problem knowl-\nedge with signals from quantities relevant to solve any task, e.g., self-performance\nassessment and accurate expectations. Vex is such a quantity. It is the fraction\nof variance explained by the value function V and measures the discrepancy be-\ntween V and the returns. Taking advantage of Vex, we propose MERL, a general\nframework for structuring reinforcement learning by injecting problem knowledge\ninto policy gradient updates. As a result, the agent is not only optimized for a\nreward but learns using problem-focused quantities provided by MERL, applicable\nout-of-the-box to any task. In this paper: (a) We introduce and deﬁne MERL, the\nmulti-head reinforcement learning framework we use throughout this work. (b)\nWe conduct experiments across a variety of standard benchmark environments,\nincluding 9 continuous control tasks, where results show improved performance.\n(c) We demonstrate that MERL also improves transfer learning on a set of challeng-\ning pixel-based tasks. (d) We ponder how MERL tackles the problem of reward\nsparsity and better conditions the feature space of reinforcement learning agents.\n1\nIntroduction\nThe problem of learning how to act optimally in an unknown dynamic environment has been a source\nof many research efforts for decades (Nguyen & Widrow, 1990; Werbos, 1989; Schmidhuber &\nHuber, 1991) and is still at the forefront of recent work in deep Reinforcement Learning (RL) (Burda\net al., 2019; Ha & Schmidhuber, 2018; Silver et al., 2016; Espeholt et al., 2018). Nevertheless, current\nalgorithms tend to be fragile and opaque (Iyer et al., 2018): they require a large amount of training\ndata collected from an agent interacting with a simulated environment where the reward signal is\noften critically sparse. Collecting signals that will make the agent more efﬁcient is, therefore, at the\ncore of the algorithms designers’ concerns.\nPrevious work in RL uses prior knowledge (Lin, 1992; Clouse & Utgoff, 1992; Ribeiro, 1998;\nMoreno et al., 2004) to reduce sample inefﬁciency. While promising and unquestionably necessary,\nthe integration of such priors into current methods is likely costly to implement, it may cause\nundesired constraints and can hinder scaling up. Therefore, we propose a framework to directly\nintegrate non-limiting constraints in current RL algorithms while being applicable to any task. In\naddition to an increased efﬁciency, the agent should learn from all interactions, not just the rewards.\nIndeed, if the probability of receiving a reward by chance is arbitrarily low, then the time required\nto learn from it will be arbitrarily long (Whitehead, 1991). This barrier to learning will prevent\nagents from signiﬁcantly reducing learning time. One way to overcome this barrier is to learn\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\narXiv:1909.11939v6  [cs.LG]  31 Mar 2020\ncomplementary and task-agnostic signals of self-performance assessment and accurate expectations\nfrom different sources (Schmidhuber, 1991; Oudeyer & Kaplan, 2007), whatever the task to master.\nShared \nparameters \nnetwork\nHead 1\nHead 2\nHead n\nProblem \nknowledge \nconstraint\nV(θ)\nπ(θ)\nInput\nProblem \nknowledge \nconstraint\nMERL Agent\nEnvironment\nAc(ons\nQuan=ty 1\nQuan=ty n\nQuan=ty 2\nRewards\nStates\nShared \nparameters \nnetwork\nHead 1\nHead 2\nHead n\nProblem \nknowledge \nconstraint\nV(θ)\nπ(θ)\nInput\nProblem \nknowledge \nconstraint\nFigure 1: High-level overview of the Multi-hEad\nReinforcement Learning (MERL) framework.\nFrom the above considerations and building on\nexisting auxiliary task methods, we design a\nframework that integrates problem knowledge\nquantities into the learning process. In addition\nto providing a method technically applicable to\nany policy gradient method or environment, the\ncentral idea of MERL is to incorporate a mea-\nsure of the discrepancy between the estimated\nstate value and the observed returns as an aux-\niliary task. This discrepancy is formalized with\nthe notion of the fraction of variance explained\nVex (Kvålseth, 1985). One intuition the reader\ncan have is that MERL transforms a reward-\nfocused task into a task regularized with dense\nproblem knowledge signals.\nFig. 1 provides a preliminary understanding of\nMERL assets: an enriched actor-critic archi-\ntecture with a lightly modiﬁed learning algo-\nrithm places the agent amidst task-agnostic aux-\niliary quantities directly sampled from the en-\nvironment. In the sequel of this paper, we use\ntwo problem knowledge quantities to demon-\nstrate the performance of MERL: Vex, a com-\npelling measure of self-performance, and future\nstates prediction, commonly used in auxiliary\ntask methods. The reader is further encouraged\nto introduce many other relevant signals. We\ndemonstrate that while being able to predict the\nquantities from the different MERL heads correctly, the agent outperforms the on-policy baseline\nthat does not use the MERL framework on various continuous control tasks. We also show that,\ninterestingly, our framework allows to better transfer the learning, from one task to another, on several\nAtari 2600 games.\n2\nPreliminaries\nWe consider a Markov Decision Process (MDP) with states s ∈S, actions a ∈A, transition\ndistribution st+1 ∼P(st, at) and reward function r(s, a). Let π = {π(a|s), s ∈S, a ∈A} denote a\nstochastic policy and let the objective function be the traditional expected discounted reward:\nJ(π) ≜E\nτ∼π\n\" ∞\nX\nt=0\nγtr (st, at)\n#\n,\n(1)\nwhere γ ∈[0, 1) is a discount factor (Puterman, 1994) and τ = (s0, a0, s1, . . . ) is a trajectory\nsampled from the environment. Policy gradient methods aim at modelling and optimizing the policy\ndirectly (Williams, 1992). The policy π is generally implemented with a function parameterized\nby θ. In the sequel, we will use θ to denote the parameters as well as the policy. In deep RL, the\npolicy is represented in a neural network called the policy network and is assumed to be continuously\ndifferentiable with respect to its parameters θ.\n2.1\nFraction of Variance Explained: Vex\nThe fraction of variance that the value function explains about the returns corresponds to the proportion\nof the variance in the dependent variable V that is predictable from the independent variable st. We\n2\ndeﬁne Vex\nτ\nas the fraction of variance explained for a trajectory τ:\nVex\nτ\n≜1 −\nP\nt∈τ\n\u0010\nˆRt −V (st)\n\u00112\nP\nt∈τ\n\u0010\nˆRt −R\n\u00112\n,\n(2)\nwhere ˆRt and V (st) are respectively the return and the expected return from state st ∈τ, and R\nis the mean of all returns in trajectory τ. In statistics, this quantity is also known as the coefﬁ-\ncient of determination R2 and it should be noted that this criterion may be negative for non-linear\nmodels (Kvålseth, 1985), indicating a severe lack of ﬁt of the corresponding function:\n• Vex\nτ\n= 1: V perfectly explains the returns - V and the returns are correlated;\n• Vex\nτ\n= 0 corresponds to a simple average prediction - V and the returns are not correlated;\n• Vex\nτ\n< 0: V provides a worse ﬁt to the outcomes than the mean of the returns.\nOne can have the intuition that Vex\nτ\nclose to 1 implies that the trajectory τ provides valuable signals\nbecause they correspond to transitions sampled from an exercised behavior. On the other hand,\nVex\nτ\nclose to 0 indicates that the value function is not correlated with the returns, therefore, the\ncorresponding samples are not expected to provide as valuable information as before. Finally,\nVex\nτ\n< 0 corresponds to a high mean-squared error for the value function, which means for the\nrelated trajectory that the agent still has to learn to perform better. In Flet-Berliac & Preux (2019),\npolicy gradient methods are improved by using Vex as a criterion to dropout transitions before each\npolicy update. We will show that Vex is also a relevant indicator for assessing self-performance in\nthe context of MERL agents.\n2.2\nPolicy Gradient Method: PPO with Clipped Surrogate Objective\nIn this paper, we consider on-policy learning primarily for its unbiasedness and stability compared to\noff-policy methods (Nachum et al., 2017). On-policy is also empirically known as being less sample\nefﬁcient than off-policy learning hence this issue emerged as an interesting research topic. However,\nour method can be applied to off-policy methods as well, and we leave this investigation open for\nfuture work.\nPPO (Schulman et al., 2017) is among the most commonly used and state-of-the-art on-policy policy\ngradient methods. Indeed, PPO has been tested on a set of benchmark tasks and has proven to produce\nimpressive results in many cases despite a relatively simple implementation. For instance, instead of\nimposing a hard constraint like TRPO (Schulman et al., 2015), PPO formalizes the constraint as a\npenalty in the objective function. In PPO, at each iteration, the new policy θnew is obtained from the\nold policy θold:\nθnew ←argmax\nθ\nE\nst,at∼πθold\n\u0002\nLPPO (st, at, θold, θ)\n\u0003\n.\n(3)\nWe use the clipped version of PPO whose objective function is:\nLPPO(st, at, θold, θ) = min\n\u0012 πθ(at|st)\nπθold(at|st)Aπθold(st, at), g(ϵ, Aπθold(st, at))\n\u0013\n,\n(4)\nwhere\ng(ϵ, A) =\n\u001a\n(1 + ϵ)A, A ≥0\n(1 −ϵ)A, A < 0.\n(5)\nA is the advantage function, A(s, a) ≜Q(s, a) −V (s). The expected advantage function Aπθold is\nestimated by an old policy and then re-calibrated using the probability ratio between the new and\nthe old policy. In Eq. 4, this ratio is constrained to stay within a small interval around 1, making the\ntraining updates more stable.\n2.3\nRelated Work\nAuxiliary tasks have been adopted to facilitate representation learning for decades (Suddarth &\nKergosien, 1990; Klyubin et al., 2005), along with intrinsic motivation (Schmidhuber, 2010; Pathak\n3\net al., 2017) and artiﬁcial curiosity (Schmidhuber, 1991; Oudeyer & Kaplan, 2007). The use of\nauxiliary tasks to allow the agents to maximize other pseudo-reward functions simultaneously has\nbeen studied in a number of previous work (Shelhamer et al., 2016; Dosovitskiy & Koltun, 2016;\nBurda et al., 2018; Du et al., 2018; Riedmiller et al., 2018; Kartal et al., 2019), including incorporating\nunsupervised control tasks and reward predictions in the UNREAL framework (Jaderberg et al., 2016),\napplying auxiliary tasks to navigation problems (Mirowski et al., 2016), or for utilizing representation\nlearning (Lesort et al., 2018) in the context of model-based RL. Lastly, in imitation learning of\nsequences provided by experts, Li et al. (2016) introduces a supervised loss for ﬁtting a recurrent\nmodel on the hidden representations to predict the next observed state.\nOur method incorporates two key contributions: a multi-head layer with auxiliary task signals both\nenvironment-agnostic and technically applicable to any policy gradient method, and the use of\nVex\nτ\nas an auxiliary task to measure the discrepancy between the value function and the returns in\norder to allow for better self-performance assessment and eventually more efﬁcient learning. In\naddition, MERL differs from previous approaches in that its framework simultaneously addresses\nthe advantages mentioned hereafter: (a) neither the introduction of new neural networks (e.g., for\nmemory) nor the introduction of a replay buffer or an off-policy setting is needed, (b) all relevant\nquantities are compatible with any task and is not limited to pixel-based environments, (c) no\nadditional iterations are required, and no modiﬁcation to the reward function of the policy gradient\nalgorithms it is applied to is necessitated. The above reasons make MERL generally applicable and\ntechnically suitable out-of-the-box to most policy gradient algorithms with a negligible computational\ncost overhead.\nFrom a different perspective, Garcıa & Fernández (2015) gives a detailed overview of previous work\nthat has changed the optimality criterion as a safety factor. But most methods use a hard constraint\nrather than a penalty; one reason is that it is difﬁcult to choose a single coefﬁcient for this penalty\nthat works well for different problems. We are successfully addressing this problem with MERL.\nIn Lipton et al. (2016), catastrophic actions are avoided by training an intrinsic fear model to predict\nwhether a disaster will occur and using it to shape rewards. Compared to both methods, MERL\nis more scalable and lightweight while it successfully incorporates quantities of self-performance\nassessments (e.g., variance explained of the value function) and accurate expectations (e.g., next state\nprediction) leading to an improved performance.\n3\nMulti-Head Framework for Reinforcement Learning using Vex\nOur multi-head architecture and its associated learning algorithm are directly applicable to most\nstate-of-the-art policy gradient methods. Let h be the index of each MERL head: MERLh. We\npropose two of the quantities predicted by these heads and show how to incorporate them into PPO.\n3.1\nPolicy and Value Function Representation\nIn deep RL, the policy is generally represented in a neural network called the policy network, with\nparameters θ, and the value function is parameterized by the value network, with parameters φ.\nEach MERL head MERLh takes as input the last embedding layer from the value network and is\nconstituted of only one layer of fully-connected neurons, with parameters φh. The output size of each\nhead corresponds to the size of the predicted MERL quantity. Below, we elaborate on two.\n3.2\nVex Estimation\nIn order to have an estimate of the fraction of variance explained, we write MERLVE as the\ncorresponding MERL head with parameters φVE. Its objective function is deﬁned by:\nLMERLVE(τ, φ, φVE) = ∥MERLVE(τ) −Vex\nτ ∥2\n2.\n(6)\n3.3\nFuture States Estimation\nAuxiliary task methods based on next state prediction are, to the best of our knowledge, the most\ncommonly used in the RL literature. We include such auxiliary task into MERL, in order to assimilate\nour contribution to the previous work and to provide a enriched evaluation of the proposed framework.\n4\nAt each timestep, one of the agent’s MERL heads predicts a future state s′ from s. While a typical\nMERL quantity can be ﬁt by regression on mean-squared error, we observed that predictions of future\nstates are better ﬁtted with a cosine-distance error. We denote MERLFS the corresponding head, with\nparameters φFS, and S the observation space size (size of vector s). We deﬁne its objective function\nas:\nLMERLFS(s, φ, φFS) = 1 −\nPS\ni=1 MERLFS\ni (s) · s′\ni\nqPS\ni=1(MERLFS\ni (s))2\nqPS\ni=1(s′\ni)2\n.\n(7)\n3.4\nProblem-Constrained Policy Update\nOnce a set of MERL heads MERLh and their associated objective functions LMERLh have been\ndeﬁned, we modify the gradient update step of the policy gradient algorithms. The objective function\nincorporates all LMERLh. Of course, each MERL objective is associated with its coefﬁcient ch. It\nis worthy to note that we used the exact same MERL coefﬁcients for all our experiments, which\ndemonstrate the framework’s ease of applicability. Algorithm 1 illustrates how the learning is\nachieved. In Eq. 9, only the (boxed) MERL objectives parameterized by φ are added to the value\nupdate and modify the learning algorithm.\nAlgorithm 1 PPO+MERL update.\nInitialise policy parameters θ0\nInitialise value function and MERLh functions parameters φ0\nfor k = 0,1,2,... do\nCollect set of trajectories Dk = {τi} with horizon T by running policy πθk in the environment\nCompute MERLh estimates at timestep t from sampling the environment\nCompute advantage estimates At at timestep t based on the current value function Vφk\nCompute future rewards ˆRt from timestep t\nGradient Update\nθk+1 = argmax\nθ\nX\nτ∈Dk\nT\nX\nt=0\nmin\n\u0012 πθ (at|st)\nπθk (at|st)Aπθk (st, at) , g (ϵ, Aπθk (st, at))\n\u0013\n(8)\nφk+1 = argmin\nφ\nX\nτ∈Dk\nT\nX\nt=0\n\u0010\nVφk (st) −ˆRt\n\u00112\n+\nH\nX\nh=0\nchLMERLh\n(9)\n4\nMERL applied to Continuous Control Tasks and the Atari domain\n4.1\nMethodology\nWe evaluate MERL in multiple high-dimensional environments, ranging from MuJoCo (Todorov\net al., 2012) to the Atari 2600 games (Bellemare et al., 2013). The experiments in MuJoCo allow us\nto evaluate the performance of MERL on a large number of different continuous control problems. It\nis worthy to note that the universal characteristics of the auxiliary quantities we design ensure that\nMERL is directly applicable to any task. Other popular auxiliary task methods (Jaderberg et al., 2016;\nMirowski et al., 2016; Burda et al., 2018) are not out-of-the-box applicable to continuous control\ntasks like MuJoCo. Thus, we naturally compare the performance of our method with PPO (Schulman\net al., 2017) where MERL heads are not used. Later, we also experiment with MERL on the Atari\n2600 games to study the transfer learning abilities of our method on a set of diverse tasks.\nImplementation. For the continuous control MuJoCo tasks, the agents have learned using separated\npolicy and value networks. In this case, we build upon the value network to incorporate our\nframework’s heads. On the contrary, when playing Atari 2600 games from pixels, the agents were\n5\ngiven a CNN network (Krizhevsky et al., 2012) shared between the policy and the value function.\nIn that case, MERLh are naturally attached to the last embedding layer of the shared network. In\nboth conﬁgurations, the outputs of MERLh heads are the same size as the quantity they predict: for\ninstance, MERLVE is a scalar whereas MERLFS is a state.\nHyper-parameters Setting. We used the same hyper-parameters as in the main text of the corre-\nsponding paper. We made this choice within a clear and objective protocol of demonstrating the\nbeneﬁts of using MERL. Hence, its reported performance is not necessarily the best that can be\nobtained, but it still exceeds the baseline. Using MERL adds as many hyper-parameters as there are\nheads in the multi-head layer and it is worth noting that MERL hyper-parameters are the same for all\ntasks. We report all hyper-parameters in Tables 1 and 2.\nTable 1: Hyper-parameters used in PPO+MERL\nHyper-parameter\nValue\nHorizon (T)\n2048 (MuJoCo), 128 (Atari)\nAdam stepsize\n3 · 10−4 (MuJoCo), 2.5 · 10−4 (Atari)\nNb. epochs\n10 (MuJoCo), 3 (Atari)\nMinibatch size\n64 (MuJoCo), 32 (Atari)\nNumber of actors\n1 (MuJoCo), 4 (Atari)\nDiscount (γ)\n0.99\nGAE parameter (λ)\n0.95\nClipping parameter (ϵ)\n0.2 (MuJoCo), 0.1 (Atari)\nValue function coef\n0.5\nTable 2: MERL hyper-parameters\nHyper-parameter\nValue\nMERLVE coef cVE\n0.5\nMERLFS coef cFS\n0.01\nPerformance Measures. We examine the performance across a large number of trials (with different\nseeds for each task). Standard deviation of returns, and average return are generally considered to be\nthe most stable measures used to compare the performance of the algorithms being studied (Islam\net al., 2017). Thereby, in the rest of this work, we use those metrics to establish the performance of\nour framework quantitatively.\n4.2\nSingle-Task Learning: Continuous Control\nWe apply MERL to PPO in several continuous control tasks, where using auxiliary tasks has not\nbeen explored in detail in the literature. Speciﬁcally, we use 9 MuJoCo environments. Due to space\nconstraints, only 3 graphs from varied tasks are shown in Fig. 2. The complete set of 9 tasks is\nreported in Table 3.\nTable 3: Average total reward of the last 100 episodes over 7 runs on the 9 MuJoCo environments.\nBoldface mean ± std indicate statistically better performance.\nTask\nPPO\nOurs\nAnt\n1728 ± 64\n2157 ± 212\nHalfCheetah\n1557 ± 21\n2117 ± 370\nHopper\n2263 ± 125\n2105 ± 200\nHumanoid\n577 ± 10\n603 ± 8\nInvertedDoublePendulum\n5965 ± 108\n6604 ± 130\nInvertedPendulum\n474 ± 14\n497 ± 12\nReacher\n−7.84 ± 0.7\n−7.78 ± 0.8\nSwimmer\n93.2 ± 8.7\n124.6 ± 5.6\nWalker2d\n2309 ± 332\n2347 ± 353\n6\nThe results demonstrate that using MERL leads to better performance on a variety of continuous\ncontrol tasks. Moreover, learning seems to be faster for some tasks, suggesting that MERL takes\nadvantage of its heads to learn relevant quantities from the beginning of learning, when the reward\nsignals may be sparse. Interestingly, by looking at the performance across all 9 tasks, we observed\nbetter results by predicting only the next state and not the subsequent ones.\n0\n200000\n400000\n600000\n800000\n1000000\n0\n500\n1000\n1500\n2000\n2500\nAnt\nPPO+MERL\nPPO\n0\n200000\n400000\n600000\n800000\n1000000\n500\n0\n500\n1000\n1500\n2000\n2500\nHalfCheetah\nPPO+MERL\nPPO\n0\n200000\n400000\n600000\n800000\n1000000\n20\n40\n60\n80\n100\n120\nSwimmer\nPPO+MERL\nPPO\nFigure 2: Experiments on 3 MuJoCo environments (106 timesteps, 7 seeds) with PPO+MERL. Red\nis the baseline, blue is with our method. The line is the average performance, while the shaded area\nrepresents its standard deviation.\n4.3\nTransfer Learning: Atari Domain\nBecause of training time constraints, we consider a transfer learning setting where, after the ﬁrst 106\ntraining steps, the agent switches to a new task for another 106 steps. The agent is not aware of the\ntask switch. Atari 2600 has been a challenging testbed for many years due to its high-dimensional\nvideo input (210 x 160) and the discrepancy of tasks between games. To investigate the advantages\nof MERL in transfer learning, we choose a set of 6 Atari games with an action space of 9, which is\nthe average size of the action space in the Atari domain. This experimental choice is beneﬁcial in that\nthe 6 games provide a diverse range of game-play while sticking to the same size of action space.\n0\n200000\n400000\n600000\n800000\n1000000\n400\n500\n600\n700\n800\nEnduro \n MsPacman\nEnduro+PPO+MERL\nEnduro+PPO\nPPO\n0\n200000\n400000\n600000\n800000\n1000000\n300\n400\n500\n600\n700\n800\n900\nBeamRider \n MsPacman\nBeamRider+PPO+MERL\nBeamRider+PPO\nPPO\n0\n200000\n400000\n600000\n800000\n1000000\n300\n400\n500\n600\n700\n800\n900\nCrazyClimber \n MsPacman\nCrazyClimber+PPO+MERL\nCrazyClimber+PPO\nPPO\n0\n200000\n400000\n600000\n800000\n1000000\n300\n400\n500\n600\n700\n800\nVideoPinball \n MsPacman\nVideoPinball+PPO+MERL\nVideoPinball+PPO\nPPO\n0\n200000\n400000\n600000\n800000\n1000000\n400\n600\n800\n1000\n1200\nAsterix \n MsPacman\nAsterix+PPO+MERL\nAsterix+PPO\nPPO\nFigure 3: Transfer learning tasks from 5 Atari games to Ms. Pacman (2 × 106 timesteps, 4 seeds).\nPerformance on the second task. Orange is PPO solely trained on Ms. Pacman, red and blue are\nrespectively PPO and our method transferring the learning. The line is the average performance,\nwhile the shaded area represents its standard deviation.\nFig. 3 demonstrates that our method can better adapt to different tasks. This can suggest that MERL\nheads learn and help represent information that is more generally relevant for other tasks, such as\nself-performance assessment or accurate expectations. In addition to adding a regularization term to\nthe objective function with problem knowledge signals, those auxiliary quantities make the neural\nnetwork optimize for task-agnostic sub-objectives.\n4.4\nAblation Study\nWe conduct an ablation study to evaluate the separate and combined contributions of the two heads.\nFig. 4 shows the comparative results in HalfCheetah, Walker2d, and Swimmer. Interestingly, with\n7\n0\n200000\n400000\n600000\n800000\n1000000\n500\n0\n500\n1000\n1500\n2000\n2500\nHalfCheetah\nPPO+MERL\nPPO+FS\nPPO\nPPO+VE\n0\n200000\n400000\n600000\n800000\n1000000\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nWalker2d\nPPO+MERL\nPPO+FS\nPPO\nPPO+VE\n0\n200000\n400000\n600000\n800000\n1000000\n20\n40\n60\n80\n100\n120\nSwimmer\nPPO+MERL\nPPO+FS\nPPO\nPPO+VE\nFigure 4: Ablation experiments with only one MERL head (FS or VE) (106 timesteps, 4 seeds). Blue\nis MERL with the two heads, red with the FS head, green with the VE head and orange with no\nMERL head. The line is the average performance, the shaded area represents its standard deviation.\nHalfCheetah, using only the MERLVE head degrades the performance, but when it is combined with\nthe MERLFS head, it outperforms PPO+FS. Results of the complete ablation analysis demonstrate\nthat each head is potentially valuable for enhancing learning and that their combination can produce\nremarkable results. In addition, it may be intuited that ﬁnding a variety of complementary MERL\nheads to cover the scope of the problem in a holistic perspective can signiﬁcantly improve learning.\n4.5\nDiscussion\nThe experiments suggest that MERL successfully optimizes the policy according to complementary\nquantities seeking for good performance and safe realization of tasks, i.e. it does not only maximize\na reward but instead ensures the control problem is appropriately addressed. Moreover, we show\nthat MERL is directly applicable to policy gradient methods while adding a negligible computation\ncost. Indeed, for the MuJoCo and Atari tasks, the computational cost overhead is respectively 5% and\n7% with our training infrastructure. All of these factors result in a generally applicable algorithm\nthat more robustly solves difﬁcult problems in a variety of environments with continuous action\nspaces or by using only raw pixels for observations. Thanks to a consistent choice of complementary\nquantities injected in the optimization process, MERL is able to better align an agent’s objectives\nwith higher-level insights into how to solve a control problem. Besides, since many current methods\ninvolve that successful learning depends on the agent’s chance to reach the goal by chance in the ﬁrst\nplace, correctly predicting MERL heads gives the agent an opportunity to learn from useful signals\nwhile improving in a given task.\n5\nConclusion\nIn this paper, we introduced Vex, a new auxiliary task, to measure the discrepancy between the\nvalue function and the returns, which successfully assesses the agent’s performance and helps learn\nmore efﬁciently. We also proposed MERL, a generally applicable deep RL framework for learning\nproblem-focused representations, which we demonstrated the effectiveness with a combination of\ntwo auxiliary tasks. We established that injecting problem knowledge signals directly in the policy\ngradient optimization allows for a better state representation that is generalizable to many tasks. Vex\nprovides a more problem-focused state representation to the agent, which is, therefore, not only\nreward-centric. MERL can be labeled as being a hybrid model-free and model-based framework,\nformed with lightweight embedded models of self-performance assessment and accurate expectations.\nMERL heads introduce a regularization term to the function approximation while addressing the\nproblem of reward sparsity through auxiliary task learning. Those features nourish a framework\ntechnically applicable to any policy gradient algorithm or environment; it does not need to be\nredesigned for different problems and can be extended with other relevant problem-solving quantities,\ncomparable to Vex.\nAlthough the relevance and higher performance of MERL have only been shown empirically, we think\nit would be interesting to study the theoretical contribution of this framework from the perspective of\nan implicit regularization of the agent’s representation on the environment. We also believe that the\nidentiﬁcation of additional MERL quantities (e.g., prediction of time until the end of a trajectory) and\nthe effect of their combination is also a research topic that we ﬁnd most relevant for future work.\n8\nReferences\nMarc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-\nment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:\n253–279, 2013.\nYuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and Alexei A Efros.\nLarge-scale study of curiosity-driven learning. arXiv preprint arXiv:1808.04355, 2018.\nYuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network\ndistillation. In International Conference on Learning Representations, 2019.\nJeffery A Clouse and Paul E Utgoff. A teaching method for reinforcement learning. In Machine\nLearning, pp. 92–101. Elsevier, 1992.\nAlexey Dosovitskiy and Vladlen Koltun. Learning to act by predicting the future. In International\nConference on Learning Representations, 2016.\nYunshu Du, Wojciech M Czarnecki, Siddhant M Jayakumar, Razvan Pascanu, and Balaji Lakshmi-\nnarayanan. Adapting auxiliary losses using gradient similarity. arXiv preprint arXiv:1812.02224,\n2018.\nLasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam\nDoron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with\nimportance weighted actor-learner architectures. In International Conference on Machine Learning,\npp. 1406–1415, 2018.\nYannis Flet-Berliac and Philippe Preux. Samples are useful? not always: denoising policy gradient\nupdates using variance explained. arXiv preprint arXiv:1904.04025, 2019.\nJavier Garcıa and Fernando Fernández. A comprehensive survey on safe reinforcement learning.\nJournal of Machine Learning Research, 16(1):1437–1480, 2015.\nDavid Ha and Jürgen Schmidhuber. Recurrent world models facilitate policy evolution, 2018.\nRiashat Islam, Peter Henderson, Maziar Gomrokchi, and Doina Precup. Reproducibility of bench-\nmarked deep reinforcement learning tasks for continuous control. arXiv preprint arXiv:1708.04133,\n2017.\nRahul Iyer, Yuezhang Li, Huao Li, Michael Lewis, R Sundar, and Katia P Sycara. Transparency\nand explanation in deep reinforcement learning neural networks. In Proceedings of AAAI/ACM\nConference on Artiﬁcial Intelligence, Ethics, and Society. AAAI/ACM, 2018.\nMax Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David\nSilver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. arXiv\npreprint arXiv:1611.05397, 2016.\nBilal Kartal, Pablo Hernandez-Leal, and Matthew E Taylor. Terminal prediction as an auxiliary task\nfor deep reinforcement learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence\nand Interactive Digital Entertainment, volume 15, pp. 38–44, 2019.\nAS Klyubin, D Polani, and CL Nehaniv. Empowerment: a universal agent-centric measure of control.\nIn Proceedings of the 2005 IEEE Congress on Evolutionary Computation 1 pp. 128-, 2005.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolu-\ntional neural networks. In Advances in neural information processing systems, pp. 1097–1105,\n2012.\nTarald O Kvålseth. Cautionary Note about R2. The American Statistician, 39(4):279–285, 1985.\nTimothée Lesort, Natalia Díaz-Rodríguez, Jean-Franois Goudou, and David Filliat. State representa-\ntion learning for control: An overview. Neural Networks, 108:379–392, 2018.\nXiujun Li, Lihong Li, Jianfeng Gao, Xiaodong He, Jianshu Chen, Li Deng, and Ji He. Recurrent rein-\nforcement learning: a hybrid approach. In International Conference on Learning Representations,\n2016.\n9\nLong-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching.\nMachine learning, 8(3-4):293–321, 1992.\nZachary C Lipton, Kamyar Azizzadenesheli, Abhishek Kumar, Lihong Li, Jianfeng Gao, and\nLi Deng. Combating reinforcement learning’s sisyphean curse with intrinsic fear. arXiv preprint\narXiv:1611.01211, 2016.\nPiotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew J Ballard, Andrea Banino,\nMisha Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu, et al. Learning to navigate in\ncomplex environments. arXiv preprint arXiv:1611.03673, 2016.\nDavid L Moreno, Carlos V Regueiro, Roberto Iglesias, and Senén Barro. Using prior knowledge to\nimprove reinforcement learning in mobile robotics. In Proceedings Towards Autonomous Robotics\nSystems, 2004.\nOﬁr Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between\nvalue and policy based reinforcement learning. In Advances in Neural Information Processing\nSystems, pp. 2775–2785, 2017.\nDerrick Nguyen and Bernard Widrow. The truck backer-upper: An example of self-learning in neural\nnetworks. In Advanced Neural Computers, pp. 11–19, 1990.\nPierre-Yves Oudeyer and Frederic Kaplan. What is intrinsic motivation? a typology of computational\napproaches. Frontiers in Neurorobotics, 1:6, 2007.\nDeepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration\nby self-supervised prediction. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition Workshops, pp. 16–17, 2017.\nMartin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John\nWiley & Sons, Inc., New York, NY, USA, 1st edition, 1994. ISBN 0471619779.\nCarlos HC Ribeiro. Embedding a priori knowledge in reinforcement learning. Journal of Intelligent\nand Robotic Systems, 21(1):51–71, 1998.\nMartin Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave, Tom Wiele,\nVlad Mnih, Nicolas Heess, and Jost Tobias Springenberg. Learning by playing solving sparse\nreward tasks from scratch. In International Conference on Machine Learning, pp. 4341–4350,\n2018.\nJ Schmidhuber and R Huber. Learning to generate artiﬁcial fovea trajectories for target detection.\nInternational Journal of Neural Systems, 2(1/2):135–141, 1991.\nJürgen Schmidhuber. Curious model-building control systems. In IEEE International Joint Confer-\nence on Neural Networks, pp. 1458–1463. IEEE, 1991.\nJürgen Schmidhuber. Formal theory of creativity, fun, and intrinsic motivation (1990–2010). IEEE\nTransactions on Autonomous Mental Development, 2(3):230–247, 2010.\nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael I Jordan, and Philipp Moritz. Trust region\npolicy optimization. In International Conference on Machine Learning, pp. 1928–1937, 2015.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nEvan Shelhamer, Parsa Mahmoudieh, Max Argus, and Trevor Darrell. Loss is its own reward: Self-\nsupervision for reinforcement learning. In International Conference on Learning Representations,\n2016.\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,\nJulian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering\nthe game of Go with deep neural networks and tree search. Nature, 529:484, 2016.\nS Suddarth and Y Kergosien. Rule-injection hints as a means of improving network performance and\nlearning time. Neural Networks, pp. 120–129, 1990.\n10\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.\nIn 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026–5033.\nIEEE, 2012.\nPaul J Werbos. Neural networks for control and system identiﬁcation. In IEEE Conference on\nDecision and Control,, pp. 260–265, 1989.\nS.D. Whitehead. Complexity and cooperation in q-learning. In Eighth International Workshop on\nMachine Learning, pp. 363–367, 1991.\nRonald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. Machine learning, 8(3-4):229–256, 1992.\n11\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2019-09-26",
  "updated": "2020-03-31"
}