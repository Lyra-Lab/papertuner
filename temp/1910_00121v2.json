{
  "id": "http://arxiv.org/abs/1910.00121v2",
  "title": "Full error analysis for the training of deep neural networks",
  "authors": [
    "Christan Beck",
    "Arnulf Jentzen",
    "Benno Kuckuck"
  ],
  "abstract": "Deep learning algorithms have been applied very successfully in recent years\nto a range of problems out of reach for classical solution paradigms.\nNevertheless, there is no completely rigorous mathematical error and\nconvergence analysis which explains the success of deep learning algorithms.\nThe error of a deep learning algorithm can in many situations be decomposed\ninto three parts, the approximation error, the generalization error, and the\noptimization error. In this work we estimate for a certain deep learning\nalgorithm each of these three errors and combine these three error estimates to\nobtain an overall error analysis for the deep learning algorithm under\nconsideration. In particular, we thereby establish convergence with a suitable\nconvergence speed for the overall error of the deep learning algorithm under\nconsideration. Our convergence speed analysis is far from optimal and the\nconvergence speed that we establish is rather slow, increases exponentially in\nthe dimensions, and, in particular, suffers from the curse of dimensionality.\nThe main contribution of this work is, instead, to provide a full error\nanalysis (i) which covers each of the three different sources of errors usually\nemerging in deep learning algorithms and (ii) which merges these three sources\nof errors into one overall error estimate for the considered deep learning\nalgorithm.",
  "text": "arXiv:1910.00121v2  [math.NA]  30 Jan 2020\nFull error analysis for the\ntraining of deep neural networks\nChristian Beck1, Arnulf Jentzen2,3, and Benno Kuckuck4,5\n1 Department of Mathematics, ETH Zurich, Z¨urich,\nSwitzerland, e-mail: christian.beck@math.ethz.ch\n2 Department of Mathematics, ETH Zurich, Z¨urich,\nSwitzerland, e-mail: arnulf.jentzen@sam.math.ethz.ch\n3 Faculty of Mathematics and Computer Science, University of M¨unster,\nM¨unster, Germany, e-mail: ajentzen@uni-muenster.de\n4 Institute of Mathematics, University of D¨usseldorf, D¨usseldorf,\nGermany, e-mail: kuckuck@math.uni-duesseldorf.de\n5 Faculty of Mathematics and Computer Science, University of M¨unster,\nM¨unster, Germany, e-mail: bkuckuck@uni-muenster.de\nJanuary 31, 2020\nAbstract\nDeep learning algorithms have been applied very successfully in recent years to a range of prob-\nlems out of reach for classical solution paradigms.\nNevertheless, there is no completely rigorous\nmathematical error and convergence analysis which explains the success of deep learning algorithms.\nThe error of a deep learning algorithm can in many situations be decomposed into three parts, the\napproximation error, the generalization error, and the optimization error. In this work we estimate\nfor a certain deep learning algorithm each of these three errors and combine these three error es-\ntimates to obtain an overall error analysis for the deep learning algorithm under consideration. In\nparticular, we thereby establish convergence with a suitable convergence speed for the overall error of\nthe deep learning algorithm under consideration. Our convergence speed analysis is far from optimal\nand the convergence speed that we establish is rather slow, increases exponentially in the dimensions,\nand, in particular, suﬀers from the curse of dimensionality. The main contribution of this work is,\ninstead, to provide a full error analysis (i) which covers each of the three diﬀerent sources of errors\nusually emerging in deep learning algorithms and (ii) which merges these three sources of errors into\none overall error estimate for the considered deep learning algorithm.\nContents\n1\nIntroduction\n2\n2\nDeep neural networks (DNNs)\n4\n2.1\nVectorized description of DNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.1.1\nAﬃne functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.1.2\nVectorized description of DNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.1.3\nActivation functions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n1\n2.1.4\nRectiﬁed DNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.2\nStructured description of DNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.2.1\nStructured description of DNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.2.2\nRealizations of DNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.2.3\nOn the connection to the vectorized description of DNNs . . . . . . . . . . . . . . .\n6\n2.2.4\nParallelizations of DNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2.2.5\nBasic examples for DNNs\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2.2.6\nCompositions of DNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.2.7\nPowers and extensions of DNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.2.8\nEmbedding DNNs in larger architectures . . . . . . . . . . . . . . . . . . . . . . . .\n13\n2.3\nLocal Lipschitz continuity of the parametrization function\n. . . . . . . . . . . . . . . . . .\n16\n3\nSeparate analyses of the error sources\n21\n3.1\nAnalysis of the approximation error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n3.1.1\nApproximations for Lipschitz continuous functions . . . . . . . . . . . . . . . . . . .\n21\n3.1.2\nDNN representations for maxima . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n3.1.3\nInterpolations through DNNs\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n3.1.4\nExplicit approximations through DNNs . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n3.1.5\nImplicit approximations through DNNs . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n3.2\nAnalysis of the generalization error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n3.2.1\nHoeﬀding’s concentration inequality . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n3.2.2\nCovering number estimates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n3.2.3\nMeasurability properties for suprema . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n3.2.4\nConcentration inequalities for random ﬁelds\n. . . . . . . . . . . . . . . . . . . . . .\n34\n3.2.5\nUniform estimates for the statistical learning error . . . . . . . . . . . . . . . . . . .\n36\n3.3\nAnalysis of the optimization error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n3.3.1\nConvergence rates for the minimum Monte Carlo method . . . . . . . . . . . . . . .\n38\n3.3.2\nContinuous uniformly distributed samples\n. . . . . . . . . . . . . . . . . . . . . . .\n39\n4\nOverall error analysis\n39\n4.1\nBias-variance decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n4.2\nOverall error decomposition\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n4.3\nAnalysis of the convergence speed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\n4.3.1\nConvergence rates for convergence in probability . . . . . . . . . . . . . . . . . . . .\n42\n4.3.2\nConvergence rates for strong convergence . . . . . . . . . . . . . . . . . . . . . . . .\n48\n1\nIntroduction\nIn problems like image recognition, text analysis, speech recognition, or playing various games, to name\na few, it is very hard and seems at the moment entirely impossible to provide a function or to hard-code\na computer program which attaches to the input – be it a picture, a piece of text, an audio recording,\nor a certain game situation – a meaning or a recommended action. Nevertheless deep learning has been\napplied very successfully in recent years to such and related problems.\nThe success of deep learning\nin applications is even more surprising as, to this day, the reasons for its performance are not entirely\nrigorously understood. In particular, there is no rigorous mathematical error and convergence analysis\nwhich explains the success of deep learning algorithms.\nIn contrast to traditional approaches, machine learning methods in general and deep learning methods\nin particular attempt to infer the unknown target function or at least a good enough approximation thereof\nfrom examples encountered during the training. Often a deep learning algorithm has three ingredients: (i)\nthe hypothesis class, a parametrizable class of functions in which we try to ﬁnd a reasonable approximation\nof the unknown target function, (ii) a numerical approximation of the expected loss function based on the\n2\ntraining examples, and (iii) an optimization algorithm which tries to approximately calculate an element\nof the hypothesis class which minimizes the numerical approximation of the expected loss function from\n(ii) given the training examples. Common approaches are to choose a set of suitable fully connected deep\nneural networks (DNNs) as hypothesis class in (i), empirical risks as approximations of the expected loss\nfunction in (ii), and stochastic gradient descent-type algorithms with random initializations as optimiza-\ntion algorithms in (iii). Each of these three ingredients contributes to the overall error of the considered\napproximation algorithm. The choice of the hypothesis class results in the so-called approximation error\n(cf., e.g., [3, 4, 19, 38, 40, 41] and the references mentioned at the beginning of Section 3), replacing the\nexact expected loss function by a numerical approximation leads to the so-called generalization error (cf.,\ne.g., [5, 10, 18, 35, 51, 68, 71] and the references mentioned therein), and the employed optimization algo-\nrithm introduces the optimization error (cf., e.g., [2, 6, 9, 15, 20, 26, 43, 45] and the references mentioned\ntherein).\nIn this work we estimate the approximation error, the generalization error, as well as the optimization\nerror and we also combine these three errors to establish convergence with a suitable convergence speed\nfor the overall error of the deep learning algorithm under consideration. Our convergence speed analysis\nis far from optimal and the convergence speed that we establish is rather slow, increases exponentially in\nthe dimensions, and, in particular, suﬀers from the curse of dimensionality (cf., e.g., Bellman [8], Novak\n& Wo´zniakowski [56, Chapter 1], and Novak & Wo´zniakowski [57, Chapter 9]). The main contribution of\nthis work is, instead, to provide a full error analysis (i) which covers each of the three diﬀerent sources of\nerrors usually emerging in deep learning algorithms and (ii) which merges these three sources of errors into\none overall error estimate for the considered deep learning algorithm. In the next result, Theorem 1.1,\nwe brieﬂy illustrate the ﬁndings of this article in a special case and we refer to Section 4.2 below for the\nmore general convergence results which we develop in this article.\nTheorem 1.1. Let d ∈N, L, a ∈R, b ∈(a, ∞), R ∈[max{2, L, |a|, |b|}, ∞), let (Ω, F, P) be a probability\nspace, let Xm : Ω→[a, b]d, m ∈N, be i.i.d. random variables, let ∥·∥: Rd →[0, ∞) be the standard\nnorm on Rd, let ϕ: [a, b]d →[0, 1] satisfy for all x, y ∈[a, b]d that |ϕ(x) −ϕ(y)| ≤L∥x −y∥, for every\nd, r, s ∈N, δ ∈N0, θ = (θ1, θ2, . . . , θd) ∈Rd with d ≥δ + rs + r let Aθ,δ\nr,s : Rs →Rr satisfy for all\nx = (x1, x2, . . . , xs) ∈Rs that\nAθ,δ\nr,s(x) =\n\u0012\u0014 sP\ni=1\nxiθδ+i\n\u0015\n+ θδ+rs+1,\n\u0014 sP\ni=1\nxiθδ+s+i\n\u0015\n+ θδ+rs+2, . . . ,\n\u0014 sP\ni=1\nxiθδ+(r−1)s+i\n\u0015\n+ θδ+rs+r\n\u0013\n,\n(1)\nlet c: R →[0, 1] and Rτ : Rτ →Rτ, τ ∈N, satisfy for all τ ∈N, x = (x1, x2, . . . , xτ) ∈Rτ, y ∈R\nthat c(y) = min{1, max{0, y}} and Rτ(x) = (max{x1, 0}, max{x2, 0}, . . . , max{xτ, 0}), for every d, τ ∈\n{3, 4, . . .}, θ ∈Rd with d ≥τ(d + 1) + (τ −3)τ(τ + 1) + τ + 1 let Nθ,τ : Rd →R satisfy for all x ∈Rd that\n\u0000Nθ,τ\u0001\n(x) =\n\u0000c ◦Aθ,τ(d+1)+(τ−3)τ(τ+1)\n1,τ\n◦Rτ ◦Aθ,τ(d+1)+(τ−4)τ(τ+1)\nτ,τ\n◦Rτ ◦. . . ◦Aθ,τ(d+1)\nτ,τ\n◦Rτ ◦Aθ,0\nτ,d\n\u0001\n(x), (2)\nlet Ed,M,τ : [−R, R]d × Ω→[0, ∞), d, M, τ ∈N, satisfy for all d, M ∈N, τ ∈{3, 4, . . .}, θ ∈[−R, R]d,\nω ∈Ωwith d ≥τ(d + 1) + (τ −3)τ(τ + 1) + τ + 1 that\nEd,M,τ(θ, ω) = 1\nM\n\u0014 M\nP\nm=1\n|Nθ,τ(Xm(ω)) −ϕ(Xm(ω))|2\n\u0015\n,\n(3)\nfor every d ∈N let Θd,k : Ω→[−R, R]d, k ∈N, be i.i.d. random variables, assume for all d ∈N that\nΘd,1 is continuous uniformly distributed on [−R, R]d, and let Ξd,K,M,τ : Ω→[−R, R]d, d, K, M, τ ∈N,\nsatisfy for all d, K, M, τ ∈N that Ξd,K,M,τ = Θd,min{k∈{1,2,...,K}: Ed,M,τ (Θd,k)=minl∈{1,2,...,K} Ed,M,τ(Θd,l)}. Then\nthere exists c ∈(0, ∞) such that for all d, K, M, τ ∈N, ε ∈(0, 1] with τ ≥2d(2dLε−1 + 2)d and\nd ≥τ(d + 1) + (τ −3)τ(τ + 1) + τ + 1 it holds that\nP\n\u0012Z\n[a,b]d |NΞd,K,M,τ,τ(x) −ϕ(x)| PX1(dx) > ε\n\u0013\n≤exp\n\u0000−K(cτ)−τdε2d\u0001\n+ 2 exp\n\u0000d ln\n\u0000(cτ)τε−2\u0001\n−ε4M\nc\n\u0001\n. (4)\n3\nTheorem 1.1 is an immediate consequence of Corollary 4.8 in Section 4.2 below. Corollary 4.8 follows\nfrom Corollary 4.7 which, in turn, is implied by Theorem 4.5, the main result of this article. In the\nfollowing we add some comments and explanations regarding the mathematical objects which appear in\nTheorem 1.1 above. For every d, τ ∈{3, 4, ...}, θ ∈Rd with d ≥τ(d + 1) + (τ −3)τ(τ + 1) + τ + 1 the\nfunction Nθ,τ : Rd →R in (2) above describes the realization of a fully connected deep neural network with\nτ layers (1 input layer with d neurons [d dimensions], 1 output layer with 1 neuron [1 dimension], as well as\nτ −2 hidden layers with τ neurons on each hidden layer [τ dimensions in each hidden layer]). The vector\nθ ∈Rd in (2) in Theorem 1.1 above stores the real parameters (the weights and the biases) for the concrete\nconsidered neural network. In particular, the architecture of the deep neural network in (2) is chosen so\nthat we have τd + (τ −3)τ 2 + τ real parameters in the weight matrices and (τ −2)τ + 1 real parameters\nin the bias vectors resulting in [τd + (τ −3)τ 2 + τ] + [(τ −2)τ + 1] = τ(d + 1) + (τ −3)τ(τ + 1) + τ + 1\nreal parameters for the deep neural network overall. This explains why the dimension d of the parameter\nvector θ ∈Rd must be larger or equal than the number of real parameters used to describe the deep\nneural network in (2) in the sense that d ≥τ(d + 1) + (τ −3)τ(τ + 1) + τ + 1 (see above (2)). The aﬃne\nlinear transformations for the deep neural network, which appear just after the input layer and just after\neach hidden layer in (2), are speciﬁed in (1) above. The functions Rτ : Rτ →R, τ ∈N, describe the\nmulti-dimensional rectiﬁer functions which are employed as activation functions in (2). Realizations of\nthe random variables (Xm, Ym) := (Xm, ϕ(Xm)), m ∈{1, 2, . . . , M}, act as training data and the neural\nnetwork parameter vector θ ∈Rd should be chosen so that the empirical risk in (3) gets minimized.\nIn Theorem 1.1 above, we use as an optimization algorithm just random initializations and perform\nno gradient descent steps.\nThe inequality in (4) in Theorem 1.1 above provides a quantitative error\nestimate for the probability that the L1-distance between the trained deep neural network approximation\nNΞd,K,M,τ ,τ(x), x ∈[a, b]d, and the function ϕ(x), x ∈[a, b]d, which we actually want to learn, is larger\nthan a possibly arbitrarily small real number ε ∈(0, 1]. In (4) in Theorem 1.1 above we measure the error\nbetween the deep neural network and the function ϕ: [a, b]d →[0, 1], which we intend to learn, in the\nL1-distance instead of in the L2-distance. However, in the more general results in Section 4.2 below we\nmeasure the error in the L2-distance and, just to keep the statement in Theorem 1.1 as easily accessible as\npossible, we restrict ourselves in Theorem 1.1 above to the L1-distance. Observe that for every ε ∈(0, 1]\nand every d, τ ∈{3, 4, . . .} with d ≥τ(d + 1) + (τ −3)τ(τ + 1) + τ + 1 we have that the right hand side\nof (4) converges to zero as K and M tend to inﬁnity. The right hand side of (4) also speciﬁes a concrete\nspeed of convergence and in this sense Theorem 1.1 provides a full error analysis for the deep learning\nalgorithm under consideration. Our analysis is in parts inspired by Maggi [50], Berner et al. [10], Cucker\n& Smale [18], Beck et al. [6], and Fehrman et al. [26].\nThe remainder of this article is organized as follows. In Section 2 we present two elementary approaches\nhow DNNs can be described in a mathematical fashion.\nBoth approaches will be used in our error\nanalyses in the later parts of this article. In Section 3 we separately analyze the approximation error, the\ngeneralization error, and the optimization error of the considered algorithm. In Section 4 we combine the\nseparate error analyses in Section 3 to obtain an overall error analysis of the considered algorithm.\n2\nDeep neural networks (DNNs)\nIn this section we present two elementary approaches on how DNNs can be described in a mathematical\nfashion. More speciﬁcally, we present in Section 2.1 a vectorized description for DNNs and we present\nin Section 2.2 a structured description for DNNs. Both approaches will be used in our error analyses in\nthe later parts of this article. Sections 2.1, 2.2, and 2.3 are partially based on material in publications\nfrom the scientiﬁc literature such as Beck et al. [6, 7], Berner et al. [10], Goodfellow et al. [28], and\nGrohs et al. [31, 32]. In particular, Deﬁnition 2.1 is inspired by, e.g., (25) in [7], Deﬁnition 2.2 is inspired\nby, e.g., (26) in [7], Deﬁnition 2.3 is, e.g., [31, Deﬁnition 2.2], Deﬁnitions 2.4, 2.5, 2.6, 2.7, and 2.8 are\ninspired by, e.g., [10, Setting 2.3], Deﬁnition 2.9 is, e.g., [31, Deﬁnition 2.1], Deﬁnition 2.10 is, e.g., [31,\nDeﬁnition 2.3], Deﬁnition 2.16 is, e.g., [31, Deﬁnition 2.17], Deﬁnition 2.17 is, e.g., [32, Deﬁnition 3.10],\n4\nDeﬁnition 2.18 is, e.g., [32, Deﬁnition 3.15], Deﬁnition 2.19 is, e.g., [31, Deﬁnition 2.5], Deﬁnition 2.23 is,\ne.g., [31, Deﬁnition 2.11], Deﬁnition 2.24 is, e.g., [31, Deﬁnition 2.12], and Theorem 2.36 is a strengthened\nversion of [10, Theorem 4.2].\n2.1\nVectorized description of DNNs\n2.1.1\nAﬃne functions\nDeﬁnition 2.1 (Aﬃne function). Let d, r, s ∈N, δ ∈N0, θ = (θ1, θ2, . . . , θd) ∈Rd satisfy d ≥δ + rs + r.\nThen we denote by Aθ,δ\nr,s : Rs →Rr the function which satisﬁes for all x = (x1, x2, . . . , xs) ∈Rs that\nAθ,δ\nr,s(x) =\n\n\n\n\n\n\n\nθδ+1\nθδ+2\n· · ·\nθδ+s\nθδ+s+1\nθδ+s+2\n· · ·\nθδ+2s\nθδ+2s+1\nθδ+2s+2\n· · ·\nθδ+3s\n...\n...\n...\n...\nθδ+(r−1)s+1\nθδ+(r−1)s+2\n· · ·\nθδ+rs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx1\nx2\nx3\n...\nxs\n\n\n\n\n\n\n\n+\n\n\n\n\n\n\n\nθδ+rs+1\nθδ+rs+2\nθδ+rs+3\n...\nθδ+rs+r\n\n\n\n\n\n\n\n=\n\u0010h Ps\nk=1 xkθδ+k\ni\n+ θδ+rs+1,\nh Ps\nk=1 xkθδ+s+k\ni\n+ θδ+rs+2, . . . ,\nh Ps\nk=1 xkθδ+(r−1)s+k\ni\n+ θδ+rs+r\n\u0011\n.\n(5)\n2.1.2\nVectorized description of DNNs\nDeﬁnition 2.2. Let d, L ∈N, l0, l1, . . . , lL ∈N, δ ∈N0, θ ∈Rd satisfy\nd ≥δ +\nL\nX\nk=1\nlk(lk−1 + 1)\n(6)\nand let Ψk : Rlk →Rlk, k ∈{1, 2, . . . , L}, be functions. Then we denote by N θ,δ,l0\nΨ1,Ψ2,...,ΨL : Rl0 →RlL the\nfunction which satisﬁes for all x ∈Rl0 that\n\u0000N θ,δ,l0\nΨ1,Ψ2,...,ΨL\n\u0001\n(x) =\n\u0000ΨL ◦A\nθ,δ+PL−1\nk=1 lk(lk−1+1)\nlL,lL−1\n◦ΨL−1 ◦A\nθ,δ+PL−2\nk=1 lk(lk−1+1)\nlL−1,lL−2\n◦. . .\n. . . ◦Ψ2 ◦Aθ,δ+l1(l0+1)\nl2,l1\n◦Ψ1 ◦Aθ,δ\nl1,l0\n\u0001\n(x)\n(7)\n(cf. Deﬁnition 2.1).\n2.1.3\nActivation functions\nDeﬁnition 2.3 (Multidimensional version). Let d ∈N and let ψ: R →R be a function. Then we denote\nby Mψ,d: Rd →Rd the function which satisﬁes for all x = (x1, x2, . . . , xd) ∈Rd that\nMψ,d(x) = (ψ(x1), ψ(x2), . . . , ψ(xd)) .\n(8)\nDeﬁnition 2.4 (Rectiﬁer function). We denote by r: R →R the function which satisﬁes for all x ∈R\nthat\nr(x) = max{x, 0}.\n(9)\nDeﬁnition 2.5 (Multidimensional rectiﬁer function). Let d ∈N. Then we denote by Rd : Rd →Rd the\nfunction given by\nRd = Mr,d\n(10)\n(cf. Deﬁnitions 2.3 and 2.4).\nDeﬁnition 2.6 (Clipping function). Let u ∈[−∞, ∞), v ∈(u, ∞]. Then we denote by cu,v : R →R the\nfunction which satisﬁes for all x ∈R that\ncu,v(x) = max{u, min{x, v}}.\n(11)\n5\nDeﬁnition 2.7 (Multidimensional clipping function). Let d ∈N, u ∈[−∞, ∞), v ∈(u, ∞]. Then we\ndenote by Cu,v,d: Rd →Rd the function given by\nCu,v,d = Mcu,v,d\n(12)\n(cf. Deﬁnitions 2.3 and 2.6).\n2.1.4\nRectiﬁed DNNs\nDeﬁnition 2.8 (Rectiﬁed clipped DNN). Let L, d ∈N, u ∈[−∞, ∞), v ∈(u, ∞], l = (l0, l1, . . . , lL) ∈\nNL+1, θ ∈Rd satisfy\nd ≥\nL\nX\nk=1\nlk(lk−1 + 1).\n(13)\nThen we denote by N θ,l\nu,v : Rl0 →RlL the function which satisﬁes for all x ∈Rl0 that\nN θ,l\nu,v (x) =\n(\u0000N θ,0,l0\nCu,v,lL\n\u0001\n(x)\n: L = 1\n\u0000N θ,0,l0\nRl1,Rl2,...,RlL−1,Cu,v,lL\n\u0001\n(x)\n: L > 1\n(14)\n(cf. Deﬁnitions 2.7, 2.5, and 2.2).\n2.2\nStructured description of DNNs\n2.2.1\nStructured description of DNNs\nDeﬁnition 2.9. We denote by N the set given by\nN = S\nL∈N\nS\n(l0,l1,...,lL)∈NL+1\n\u0000\u0010L\nk=1(Rlk×lk−1 × Rlk)\n\u0001\n(15)\nand we denote by P, L, I, O: N →N, H: N →N0, and D: N →\n\u0000S∞\nL=2 NL\u0001\nthe functions which satisfy\nfor all L ∈N, l0, l1, . . . , lL ∈N, Φ ∈\n\u0000\u0010L\nk=1(Rlk×lk−1 × Rlk)\n\u0001\nthat P(Φ) = PL\nk=1 lk(lk−1 + 1), L(Φ) = L,\nI(Φ) = l0, O(Φ) = lL, H(Φ) = L −1, and D(Φ) = (l0, l1, . . . , lL).\n2.2.2\nRealizations of DNNs\nDeﬁnition 2.10 (Realization associated to a DNN). Let a ∈C(R, R). Then we denote by Ra : N →\n\u0000S\nk,l∈N C(Rk, Rl)\n\u0001\nthe function which satisﬁes for all L ∈N, l0, l1, . . . , lL ∈N, Φ = ((W1, B1), (W2, B2),\n. . . , (WL, BL)) ∈\n\u0000\u0010L\nk=1(Rlk×lk−1 × Rlk)\n\u0001\n, x0 ∈Rl0, x1 ∈Rl1, . . . , xL−1 ∈RlL−1 with ∀k ∈N ∩(0, L): xk =\nMa,lk(Wkxk−1 + Bk) that\nRa(Φ) ∈C(Rl0, RlL)\nand\n(Ra(Φ))(x0) = WLxL−1 + BL\n(16)\n(cf. Deﬁnitions 2.9 and 2.3).\n2.2.3\nOn the connection to the vectorized description of DNNs\nDeﬁnition 2.11. We denote by T : N →\n\u0000S\nd∈N Rd\u0001\nthe function which satisﬁes for all L, d ∈N,\nl0, l1, . . . , lL ∈N, Φ = ((W1, B1), (W2, B2), . . . , (WL, BL)) ∈\n\u0000\u0010L\nm=1(Rlm×lm−1×Rlm)\n\u0001\n, θ = (θ1, θ2, . . . , θd) ∈\n6\nRd, k ∈{1, 2, . . . , L} with T (Φ) = θ that\nd = P(Φ),\nBk =\n\n\n\n\n\n\n\n\nθ(Pk−1\ni=1 li(li−1+1))+lklk−1+1\nθ(Pk−1\ni=1 li(li−1+1))+lklk−1+2\nθ(Pk−1\ni=1 li(li−1+1))+lklk−1+3\n...\nθ(Pk−1\ni=1 li(li−1+1))+lklk−1+lk\n\n\n\n\n\n\n\n\n,\nand\nWk =\n\n\n\n\n\n\n\n\nθ(Pk−1\ni=1 li(li−1+1))+1\nθ(Pk−1\ni=1 li(li−1+1))+2\n· · ·\nθ(Pk−1\ni=1 li(li−1+1))+lk−1\nθ(Pk−1\ni=1 li(li−1+1))+lk−1+1\nθ(Pk−1\ni=1 li(li−1+1))+lk−1+2\n· · ·\nθ(Pk−1\ni=1 li(li−1+1))+2lk−1\nθ(Pk−1\ni=1 li(li−1+1))+2lk−1+1\nθ(Pk−1\ni=1 li(li−1+1))+2lk−1+2\n· · ·\nθ(Pk−1\ni=1 li(li−1+1))+3lk−1\n...\n...\n...\n...\nθ(Pk−1\ni=1 li(li−1+1))+(lk−1)lk−1+1\nθ(Pk−1\ni=1 li(li−1+1))+(lk−1)lk−1+2\n· · ·\nθ(Pk−1\ni=1 li(li−1+1))+lklk−1\n\n\n\n\n\n\n\n\n,\n(17)\n(cf. Deﬁnition 2.9).\nLemma 2.12. Let a, b ∈N, W = (Wi,j)(i,j)∈{1,2,...,a}×{1,2,...,b} ∈Ra×b, B = (Bi)i∈{1,2,...,a} ∈Ra. Then\nT\n\u0000((W, B))\n\u0001\n=\n\u0000W1,1, W1,2, . . . , W1,b, W2,1, W2,2, . . . , W2,b, . . . , Wa,1, Wa,2, . . . , Wa,b, B1, B2, . . . , Ba\n\u0001\n(18)\n(cf. Deﬁnition 2.11).\nProof of Lemma 2.12. Observe that (17) establishes (18). The proof of Lemma 2.12 is thus completed.\nLemma 2.13. Let L ∈N, l0, l1, . . . , lL ∈N, let Wk = (Wk,i,j)(i,j)∈{1,2,...,lk}×{1,2,...,lk−1} ∈Rlk×lk−1, k ∈\n{1, 2, . . . , L}, and let Bk = (Bk,i)i∈{1,2,...,lk} ∈Rlk, k ∈{1, 2, . . . , L}. Then\n(i) it holds for all k ∈{1, 2, . . . , L} that\nT\n\u0000((Wk, Bk))\n\u0001\n=\n\u0000Wk,1,1, Wk,1,2, . . . , Wk,1,lk−1, Wk,2,1, Wk,2,2, . . . , Wk,2,lk−1, . . . ,\nWk,lk,1, Wk,lk,2, . . . , Wk,lk,lk−1, Bk,1, Bk,2, . . . , Bk,lk\n\u0001\n(19)\nand\n(ii) it holds that\nT\n\u0010\u0000(W1, B1), (W2, B2), . . . , (WL, BL)\n\u0001\u0011\n=\n\u0010\nW1,1,1, W1,1,2, . . . , W1,1,l0, . . . , W1,l1,1, W1,l1,2, . . . , W1,l1,l0, B1,1, B1,2, . . . , B1,l1,\nW2,1,1, W2,1,2, . . . , W2,1,l1, . . . , W2,l2,1, W2,l2,2, . . . , W2,l2,l1, B2,1, B2,2, . . . , B2,l2,\n. . . ,\nWL,1,1, WL,1,2, . . . , WL,1,lL−1, . . . WL,lL,1, WL,lL,2, . . . , WL,lL,lL−1, BL,1, BL,2, . . . , BL,lL\n\u0011\n(20)\n(cf. Deﬁnition 2.11).\nProof of Lemma 2.13. Note that Lemma 2.12 proves item (i). Moreover, observe that (17) establishes\nitem (ii). The proof of Lemma 2.13 is thus completed.\nLemma 2.14. Let a ∈C(R, R), Φ ∈N, L ∈N, l0, l1, . . . , lL ∈N satisfy D(Φ) = (l0, l1, . . . , lL) (cf.\nDeﬁnition 2.9). Then it holds for all x ∈Rl0 that\n(Ra(Φ))(x) =\n\n\n\n\u0000N T (Φ),0,l0\nidRlL\n\u0001\n(x)\n: L = 1\n\u0000N T (Φ),0,l0\nMa,l1,Ma,l2,...,Ma,lL−1,idRlL\n\u0001\n(x)\n: L > 1\n(21)\n(cf. Deﬁnitions 2.10, 2.11, 2.3, and 2.2).\n7\nProof of Lemma 2.14. Throughout this proof let W1 ∈Rl1×l0, B1 ∈Rl1, W2 ∈Rl2×l1, B2 ∈Rl2, . . . ,\nWL ∈RlL×lL−1, BL ∈RlL satisfy Φ = ((W1, B1), (W2, B2), . . . , (WL, BL)). Note that (17) shows that for\nall k ∈{1, 2, . . . , L}, x ∈Rlk−1 it holds that\nWkx + Bk =\n\u0000A\nT (Φ),Pk−1\ni=1 li(li−1+1)\nlk,lk−1\n\u0001\n(x)\n(22)\n(cf. Deﬁnitions 2.11 and 2.1). This demonstrates that for all x0 ∈Rl0, x1 ∈Rl1, . . . , xL−1 ∈RlL−1 with\n∀k ∈N ∩(0, L): xk = Ma,lk(Wkxk−1 + Bk) it holds that\nxL−1 =\n(23)\n(\nx0\n: L = 1\n\u0000Ma,lL−1 ◦A\nT (Φ),PL−2\ni=1 li(li−1+1)\nlL−1,lL−2\n◦Ma,lL−2 ◦A\nT (Φ),PL−3\ni=1 li(li−1+1)\nlL−2,lL−3\n◦. . . ◦Ma,l1 ◦AT (Φ),0\nl1,l0\n\u0001\n(x0)\n: L > 1\n(cf. Deﬁnition 2.3).\nCombining this and (22) with (7) and (16) proves that for all x0 ∈Rl0, x1 ∈\nRl1, . . . , xL−1 ∈RlL−1 with ∀k ∈N ∩(0, L): xk = Ma,lk(Wkxk−1 + Bk) it holds that\n\u0000Ra(Φ)\n\u0001\n(x0) = WLxL−1 + BL =\n\u0000A\nT (Φ),PL−1\ni=1 li(li−1+1)\nlL,lL−1\n\u0001\n(xL−1)\n=\n\n\n\n\u0000N T (Φ),0,l0\nidRlL\n\u0001\n(x0)\n: L = 1\n\u0000N T (Φ),0,l0\nMa,l1,Ma,l2,...,Ma,lL−1,idRlL\n\u0001\n(x0)\n: L > 1\n(24)\n(cf. Deﬁnitions 2.10 and 2.2). The proof of Lemma 2.14 is thus completed.\nCorollary 2.15. Let Φ ∈N (cf. Deﬁnition 2.9). Then it holds for all x ∈RI(Φ) that\n\u0000N T (Φ),D(Φ)\n−∞,∞\n\u0001\n(x) = (Rr(Φ))(x)\n(25)\n(cf. Deﬁnitions 2.11, 2.8, 2.4, and 2.10).\nProof of Corollary 2.15. Note that Lemma 2.14, (14), (10), and the fact that for all d ∈N it holds that\nC−∞,∞,d = idRd establish (25) (cf. Deﬁnition 2.7). The proof of Corollary 2.15 is thus completed.\n8\n2.2.4\nParallelizations of DNNs\nDeﬁnition 2.16 (Parallelization of DNNs). Let n ∈N. Then we denote by\nPn :\n\b\n(Φ1, Φ2, . . . , Φn) ∈Nn : L(Φ1) = L(Φ2) = . . . = L(Φn)\n\t\n→N\n(26)\nthe function which satisﬁes for all L ∈N, (l1,0, l1,1, . . . , l1,L), (l2,0, l2,1, . . . , l2,L), . . . , (ln,0, ln,1, . . . , ln,L) ∈\nNL+1, Φ1 = ((W1,1, B1,1), (W1,2, B1,2), . . . , (W1,L, B1,L)) ∈\n\u0000\u0010L\nk=1(Rl1,k×l1,k−1 × Rl1,k)\n\u0001\n, Φ2 = ((W2,1, B2,1),\n(W2,2, B2,2), . . . , (W2,L, B2,L)) ∈\n\u0000\u0010L\nk=1(Rl2,k×l2,k−1 × Rl2,k)\n\u0001\n, . . . , Φn = ((Wn,1, Bn,1), (Wn,2, Bn,2), . . . ,\n(Wn,L, Bn,L)) ∈\n\u0000\u0010L\nk=1(Rln,k×ln,k−1 × Rln,k)\n\u0001\nthat\nPn(Φ1, Φ2, . . . , Φn) =\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nW1,1\n0\n0\n· · ·\n0\n0\nW2,1\n0\n· · ·\n0\n0\n0\nW3,1\n· · ·\n0\n...\n...\n...\n...\n...\n0\n0\n0\n· · ·\nWn,1\n\n\n\n\n\n\n\n,\n\n\n\n\n\n\n\nB1,1\nB2,1\nB3,1\n...\nBn,1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n,\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nW1,2\n0\n0\n· · ·\n0\n0\nW2,2\n0\n· · ·\n0\n0\n0\nW3,2\n· · ·\n0\n...\n...\n...\n...\n...\n0\n0\n0\n· · ·\nWn,2\n\n\n\n\n\n\n\n,\n\n\n\n\n\n\n\nB1,2\nB2,2\nB3,2\n...\nBn,2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n, . . . ,\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nW1,L\n0\n0\n· · ·\n0\n0\nW2,L\n0\n· · ·\n0\n0\n0\nW3,L\n· · ·\n0\n...\n...\n...\n...\n...\n0\n0\n0\n· · ·\nWn,L\n\n\n\n\n\n\n\n,\n\n\n\n\n\n\n\nB1,L\nB2,L\nB3,L\n...\nBn,L\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(27)\n(cf. Deﬁnition 2.9).\n2.2.5\nBasic examples for DNNs\nDeﬁnition 2.17 (Linear transformations as DNNs). Let m, n ∈N, W ∈Rm×n. Then we denote by\nNW ∈Rm×n × Rm the pair given by NW = (W, 0).\nDeﬁnition 2.18. We denote by I = (Id)d∈N : N →N the function which satisﬁes for all d ∈N that\nI1 =\n\u0012\u0012\u0012 1\n−1\n\u0013\n,\n\u00120\n0\n\u0013\u0013\n,\n\u0010 \u00001\n−1\u0001\n, 0\n\u0011\u0013\n∈\n\u0000(R2×1 × R2) × (R1×2 × R1)\n\u0001\n(28)\nand\nId = Pd(I1, I1, . . . , I1)\n(29)\n(cf. Deﬁnitions 2.9 and 2.16).\n9\n2.2.6\nCompositions of DNNs\nDeﬁnition 2.19 (Composition of DNNs). We denote by (·) • (·): {(Φ1, Φ2) ∈N × N: I(Φ1) = O(Φ2)}\n→N the function which satisﬁes for all L, L ∈N, l0, l1, . . . , lL, l0, l1, . . . , lL ∈N, Φ1 = ((W1, B1), (W2, B2),\n. . . , (WL, BL)) ∈\n\u0000\u0010L\nk=1(Rlk×lk−1 × Rlk)\n\u0001\n, Φ2 = ((W1, B1), (W2, B2), . . . , (WL, BL)) ∈\n\u0000\u0010L\nk=1(Rlk×lk−1 ×\nRlk)\n\u0001\nwith l0 = I(Φ1) = O(Φ2) = lL that\nΦ1 • Φ2 =\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u0000(W1, B1), (W2, B2), . . . , (WL−1, BL−1), (W1WL, W1BL + B1),\n(W2, B2), (W3, B3), . . . , (WL, BL)\n\u0001\n: L > 1 < L\n\u0000(W1W1, W1B1 + B1), (W2, B2), (W3, B3), . . . , (WL, BL)\n\u0001\n: L > 1 = L\n\u0000(W1, B1), (W2, B2), . . . , (WL−1, BL−1), (W1WL, W1BL + B1)\n\u0001\n: L = 1 < L\n\u0000(W1W1, W1B1 + B1)\n\u0001\n: L = 1 = L\n(30)\n(cf. Deﬁnition 2.9).\nDeﬁnition 2.20 (Maximum norm). We denote by |||·|||:\n\u0000S∞\nd=1 Rd\u0001\n→[0, ∞) the function which satisﬁes\nfor all d ∈N, θ = (θ1, θ2, . . . , θd) ∈Rd that\n|||θ||| =\nmax\ni∈{1,2,...,d}|θi|.\n(31)\nLemma 2.21. Let L, L ∈N, l0, l1, . . . , lL, l0, l1, . . . , lL ∈N, Φ1 = ((W1, B1), (W2, B2), . . . , (WL, BL)) ∈\n\u0000\u0010L\nk=1(Rlk×lk−1 × Rlk)\n\u0001\n, Φ2 = ((W1, B1), (W2, B2), . . . , (WL, BL)) ∈\n\u0000\u0010L\nk=1(Rlk×lk−1 × Rlk)\n\u0001\n. Then\n|||T (Φ1 • Φ2)||| ≤max\n\b\n|||T (Φ1)|||, |||T (Φ2)|||,\n\f\f\f\f\f\fT\n\u0000((W1WL, W1BL + B1))\n\u0001\f\f\f\f\f\f\t\n(32)\n(cf. Deﬁnitions 2.19, 2.11, and 2.20).\nProof of Lemma 2.21. Note that (30) and Lemma 2.13 establish (32). The proof of Lemma 2.21 is thus\ncompleted.\n2.2.7\nPowers and extensions of DNNs\nDeﬁnition 2.22. Let d ∈N. Then we denote by Id ∈Rd×d the identity matrix in Rd×d.\nDeﬁnition 2.23. We denote by (·)•n : {Φ ∈N: I(Φ) = O(Φ)} →N, n ∈N0, the functions which satisfy\nfor all n ∈N0, Φ ∈N with I(Φ) = O(Φ) that\nΦ•n =\n(\u0000IO(Φ), (0, 0, . . . , 0)\n\u0001\n∈RO(Φ)×O(Φ) × RO(Φ)\n: n = 0\nΦ • (Φ•(n−1))\n: n ∈N\n(33)\n(cf. Deﬁnitions 2.9, 2.22, and 2.19).\nDeﬁnition 2.24 (Extension of DNNs). Let L ∈N, Ψ ∈N satisfy I(Ψ) = O(Ψ). Then we denote by\nEL,Ψ: {Φ ∈N: (L(Φ) ≤L and O(Φ) = I(Ψ))} →N the function which satisﬁes for all Φ ∈N with\nL(Φ) ≤L and O(Φ) = I(Ψ) that\nEL,Ψ(Φ) =\n\u0000Ψ•(L−L(Φ))\u0001\n• Φ\n(34)\n(cf. Deﬁnitions 2.9, 2.23, and 2.19).\n10\nLemma 2.25. Let d, i, L, L ∈N, l0, l1, . . . , lL−1 ∈N, Φ, Ψ ∈N satisfy L ≥L, D(Φ) = (l0, l1, . . . , lL−1, d)\nand D(Ψ) = (d, i, d) (cf. Deﬁnition 2.9). Then it holds that D(EL,Ψ(Φ)) ∈NL+1 and\nD(EL,Ψ(Φ)) =\n(\n(l0, l1, . . . , lL−1, d)\n: L = L\n(l0, l1, . . . , lL−1, i, i, . . ., i, d)\n: L > L\n(35)\n(cf. Deﬁnition 2.24).\nProof of Lemma 2.25. Observe that item (i) in [31, Lemma 2.13] ensures that L(Ψ•(L−L)) = L −L + 1,\nD(Ψ•(L−L)) ∈NL−L+2, and\nD(Ψ•(L−L)) =\n(\n(d, d)\n: L = L\n(d, i, i, . . ., i, d)\n: L > L\n(36)\n(cf. Deﬁnition 2.23). Combining this with [31, Proposition 2.6] shows that L((Ψ•(L−L)) • Φ) = L(Ψ•(L−L))+\nL(Φ) −1 = L, D((Ψ•(L−L)) • Φ) ∈NL+1, and\nD((Ψ•(L−L)) • Φ) =\n(\n(l0, l1, . . . , lL−1, d)\n: L = L\n(l0, l1, . . . , lL−1, i, i, . . . , i, d)\n: L > L.\n(37)\nThis and (34) establish (35). The proof of Lemma 2.25 is thus completed.\nLemma 2.26. Let d, L ∈N, Φ ∈N satisfy L ≥L(Φ) and d = O(Φ) (cf. Deﬁnition 2.9). Then\n|||T (EL,Id(Φ))||| ≤max{1, |||T (Φ)|||}\n(38)\n(cf. Deﬁnitions 2.18, 2.24, 2.11, and 2.20).\nProof of Lemma 2.26. Throughout this proof assume w.l.o.g. that L > L(Φ) and let l0, l1, . . . , lL−L(Φ)+1 ∈\nN satisfy (l0, l1, . . . , lL−L(Φ)+1) = (d, 2d, 2d, . . ., 2d, d). Note that [32, Lemma 3.16] ensures that D(Id) =\n(d, 2d, d) ∈N3 (cf. Deﬁnition 2.18). Item (i) in [31, Lemma 2.13] hence establishes that\nL((Id)•(L−L(Φ)) = L −L(Φ) + 1\nand\nD((Id)•(L−L(Φ)) = (l0, l1, . . . , lL−L(Φ)+1) ∈NL−L(Φ)+2\n(39)\n(cf. Deﬁnition 2.23). This shows that there exist Wk ∈Rlk×lk−1, k ∈{1, 2, . . . , L−L(Φ)+1}, and Bk ∈Rlk,\nk ∈{1, 2, . . . , L −L(Φ) + 1}, which satisfy\n(Id)•(L−L(Φ)) = ((W1, B1), (W2, B2), . . . , (WL−L(Φ)+1, BL−L(Φ)+1)).\n(40)\nNext observe that (27), (28), (29), (30), and (33) demonstrate that\nW1 =\n\n\n\n\n\n\n\n\n\n\n\n1\n0\n· · ·\n0\n−1\n0\n· · ·\n0\n0\n1\n· · ·\n0\n0\n−1\n· · ·\n0\n...\n...\n...\n...\n0\n0\n· · ·\n1\n0\n0\n· · ·\n−1\n\n\n\n\n\n\n\n\n\n\n\n∈R(2d)×d\nand\nWL−L(Φ)+1 =\n\n\n\n\n\n1\n−1\n0\n0\n· · ·\n0\n0\n0\n0\n1\n−1\n· · ·\n0\n0\n...\n...\n...\n...\n...\n...\n...\n0\n0\n0\n0\n· · ·\n1\n−1\n\n\n\n\n∈Rd×(2d).\n(41)\n11\nMoreover, note that (27), (28), (29), (30), and (33) prove that for all k ∈N ∩(1, L −L(Φ) + 1) it holds\nthat\nWk =\n\n\n\n\n\n1\n−1\n0\n0\n· · ·\n0\n0\n0\n0\n1\n−1\n· · ·\n0\n0\n...\n...\n...\n...\n...\n...\n...\n0\n0\n0\n0\n· · ·\n1\n−1\n\n\n\n\n\n|\n{z\n}\n∈Rd×(2d)\n\n\n\n\n\n\n\n\n\n\n\n1\n0\n· · ·\n0\n−1\n0\n· · ·\n0\n0\n1\n· · ·\n0\n0\n−1\n· · ·\n0\n...\n...\n...\n...\n0\n0\n· · ·\n1\n0\n0\n· · ·\n−1\n\n\n\n\n\n\n\n\n\n\n\n|\n{z\n}\n∈R(2d)×d\n=\n\n\n\n\n\n\n\n\n\n\n\n1\n−1\n0\n0\n· · ·\n0\n0\n−1\n1\n0\n0\n· · ·\n0\n0\n0\n0\n1\n−1\n· · ·\n0\n0\n0\n0\n−1\n1\n· · ·\n0\n0\n...\n...\n...\n...\n...\n...\n...\n0\n0\n0\n0\n· · ·\n1\n−1\n0\n0\n0\n0\n· · ·\n−1\n1\n\n\n\n\n\n\n\n\n\n\n\n∈R(2d)×(2d).\n(42)\nIn addition, observe that (28), (29), (27), (33), and (30) show that for all k ∈N ∩[1, L −L(Φ)] it holds\nthat\nBk = 0 ∈R2d\nand\nBL−L(Φ)+1 = 0 ∈Rd.\n(43)\nCombining this, (41), and (42) establishes that\n\f\f\f\f\f\fT\n\u0000(Id)•(L−L(Φ))\u0001\f\f\f\f\f\f = 1\n(44)\n(cf. Deﬁnitions 2.11 and 2.20).\nFurthermore, note that (41) demonstrates that for all k ∈N, W =\n(wi,j)(i,j)∈{1,2,...,d}×{1,2,...,k} ∈Rd×k it holds that\nW1W =\n\n\n\n\n\n\n\n\n\n\n\nw1,1\nw1,2\n· · ·\nw1,k\n−w1,1\n−w1,2\n· · ·\n−w1,k\nw2,1\nw2,2\n· · ·\nw2,k\n−w2,1\n−w2,2\n· · ·\n−w2,k\n...\n...\n...\n...\nwd,1\nwd,2\n· · ·\nwd,k\n−wd,1\n−wd,2\n· · ·\n−wd,k\n\n\n\n\n\n\n\n\n\n\n\n∈R(2d)×k.\n(45)\nNext observe that (41) and (43) show that for all B = (b1, b2, . . . , bd) ∈Rd it holds that\nW1B + B1 =\n\n\n\n\n\n\n\n\n\n\n\n1\n0\n· · ·\n0\n−1\n0\n· · ·\n0\n0\n1\n· · ·\n0\n0\n−1\n· · ·\n0\n...\n...\n...\n...\n0\n0\n· · ·\n1\n0\n0\n· · ·\n−1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nb1\nb2\n...\nbd\n\n\n\n\n=\n\n\n\n\n\n\n\n\n\n\n\nb1\n−b1\nb2\n−b2\n...\nbd\n−bd\n\n\n\n\n\n\n\n\n\n\n\n∈R2d.\n(46)\nCombining this with (45) proves that for all k ∈N, W ∈Rd×k, B ∈Rd it holds that\n\f\f\f\f\f\fT\n\u0000((W1W, W1B + B1))\n\u0001\f\f\f\f\f\f =\n\f\f\f\f\f\fT\n\u0000((W, B))\n\u0001\f\f\f\f\f\f .\n(47)\n12\nThis, Lemma 2.21, and (44) establish that\n|||T (EL,Id(Φ))||| =\n\f\f\f\f\f\fT\n\u0000((Id)•(L−L(Φ))) • Φ\n\u0001\f\f\f\f\f\f\n≤max\n\b\f\f\f\f\f\fT\n\u0000(Id)•(L−L(Φ))\u0001\f\f\f\f\f\f , |||T (Φ)|||\n\t\n= max{1, |||T (Φ)|||}\n(48)\n(cf. Deﬁnition 2.24). The proof of Lemma 2.26 is thus completed.\n2.2.8\nEmbedding DNNs in larger architectures\nLemma 2.27. Let a ∈C(R, R), L ∈N, l0, l1, . . . , lL, l0, l1, . . . , lL ∈N satisfy for all k ∈{1, 2, . . . , L} that\nl0 = l0, lL = lL, and lk ≥lk, for every k ∈{1, 2, . . . , L} let Wk = (Wk,i,j)(i,j)∈{1,2,...,lk}×{1,2,...,lk−1} ∈Rlk×lk−1,\nWk = (Wk,i,j)(i,j)∈{1,2,...,lk}×{1,2,...,lk−1} ∈Rlk×lk−1, Bk = (Bk,i)i∈{1,2,...,lk} ∈Rlk, Bk = (Bk,i)i∈{1,2,...,lk} ∈Rlk,\nassume for all k ∈{1, 2, . . . , L}, i ∈{1, 2, . . ., lk}, j ∈N ∩(0, lk−1] that Wk,i,j = Wk,i,j and Bk,i = Bk,i,\nand assume for all k ∈{1, 2, . . ., L}, i ∈{1, 2, . . . , lk}, j ∈N ∩(lk−1, lk−1 + 1) that Wk,i,j = 0. Then\nRa\n\u0000((W1, B1), (W2, B2), . . . , (WL, BL))\n\u0001\n= Ra\n\u0000((W1, B1), (W2, B2), . . . , (WL, BL))\n\u0001\n(49)\n(cf. Deﬁnition 2.10).\nProof of Lemma 2.27. Throughout this proof let πk : Rlk →Rlk, k ∈{0, 1, . . . , L}, satisfy for all k ∈\n{0, 1, . . . , L}, x = (x1, x2, . . . , xlk) that\nπk(x) = (x1, x2, . . . , xlk).\n(50)\nObserve that the hypothesis that l0 = l0 and lL = lL shows that\nRa\n\u0000((W1, B1), (W2, B2), . . . , (WL, BL))\n\u0001\n∈C(Rl0, RlL)\n(51)\n(cf. Deﬁnition 2.10). Furthermore, note that the hypothesis that for all k ∈{1, 2, . . . , L}, i ∈{1, 2, . . . , lk},\nj ∈N∩(lk−1, lk−1+1) it holds that Wk,i,j = 0 ensures that for all k ∈{1, 2, . . . , L}, x = (x1, x2, . . . , xlk−1) ∈\nRlk−1 it holds that\nπk(Wkx + Bk) =\n \"lk−1\nX\ni=1\nWk,1,ixi\n#\n+ Bk,1,\n\"lk−1\nX\ni=1\nWk,2,ixi\n#\n+ Bk,2, . . . ,\n\"lk−1\nX\ni=1\nWk,lk,ixi\n#\n+ Bk,lk\n!\n=\n \"lk−1\nX\ni=1\nWk,1,ixi\n#\n+ Bk,1,\n\"lk−1\nX\ni=1\nWk,2,ixi\n#\n+ Bk,2, . . . ,\n\"lk−1\nX\ni=1\nWk,lk,ixi\n#\n+ Bk,lk\n!\n.\n(52)\nCombining this with the hypothesis that for all k ∈{1, 2, . . . , L}, i ∈{1, 2, . . . , lk}, j ∈N∩(0, lk−1] it holds\nthat Wk,i,j = Wk,i,j and Bk,i = Bk,i shows that for all k ∈{1, 2, . . . , L}, x = (x1, x2, . . . , xlk−1) ∈Rlk−1 it\nholds that\nπk(Wkx + Bk) =\n \"lk−1\nX\ni=1\nWk,1,ixi\n#\n+ Bk,1,\n\"lk−1\nX\ni=1\nWk,2,ixi\n#\n+ Bk,2, . . . ,\n\"lk−1\nX\ni=1\nWk,lk,ixi\n#\n+ Bk,lk\n!\n= Wk(πk−1(x)) + Bk.\n(53)\nMoreover, observe that (50) and (8) ensure that for all k ∈{0, 1, . . . , L}, x = (x1, x2, . . . , xlk) ∈Rlk it\nholds that\nπk(Ma,lk(x)) = πk(a(x1), a(x2), . . . , a(xlk)) = (a(x1), a(x2), . . . , a(xlk)) = Ma,lk(πk(x)).\n(54)\nCombining this and (53) demonstrates that for all x0 ∈Rl0, x1 ∈Rl1, . . . , xL−1 ∈RlL−1, k ∈N ∩(0, L)\nwith ∀m ∈N ∩(0, L): xm = Ma,lm(Wmxm−1 + Bm) it holds that\nπk(xk) = πk(Ma,lk(Wkxk−1 + Bk)) = Ma,lk(πk(Wkxk−1 + Bk)) = Ma,lk(Wkπk−1(xk−1) + Bk)\n(55)\n13\n(cf. Deﬁnition 2.3).\nThe hypothesis that l0 = l0 and lL = lL and (53) therefore prove that for all\nx0 ∈Rl0, x1 ∈Rl1, . . . , xL−1 ∈RlL−1 with ∀k ∈N ∩(0, L): xk = Ma,lk(Wkxk−1 + Bk) it holds that\n\u0000Ra\n\u0000((W1, B1), (W2, B2), . . . , (WL, BL))\n\u0001\u0001\n(x0) =\n\u0000Ra\n\u0000((W1, B1), (W2, B2), . . . , (WL, BL))\n\u0001\u0001\n(π0(x0))\n= WLπL−1(xL−1) + BL\n= πL(WLxL−1 + BL) = WLxL−1 + BL\n=\n\u0000Ra\n\u0000((W1, B1), (W2, B2), . . . , (WL, BL))\n\u0001\u0001\n(x0)\n(56)\n(cf. Deﬁnition 2.10). The proof of Lemma 2.27 is thus completed.\nLemma 2.28. Let a ∈C(R, R), L ∈N, l0, l1, . . . , lL, l0, l1, . . . , lL ∈N satisfy for all k ∈{1, 2, . . . , L} that\nl0 = l0, lL = lL, and lk ≥lk and let Φ ∈N satisfy D(Φ) = (l0, l1, . . . , lL) (cf. Deﬁnition 2.9). Then there\nexists Ψ ∈N such that\nD(Ψ) = (l0, l1, . . . , lL),\n|||T (Ψ)||| = |||T (Φ)|||,\nand\nRa(Ψ) = Ra(Φ)\n(57)\n(cf. Deﬁnitions 2.11, 2.20, and 2.10).\nProof of Lemma 2.28. Throughout this proof let Bk = (Bk,i)i∈{1,2,...,lk} ∈Rlk, k ∈{1, 2, . . . , L}, and Wk =\n(Wk,i,j)(i,j)∈{1,2,...,lk}×{1,2,...,lk−1} ∈Rlk×lk−1, k ∈{1, 2, . . . , L}, satisfy Φ = ((W1, B1), (W2, B2), . . . , (WL, BL))\nand let Wk = (Wk,i,j)(i,j)∈{1,2,...,lk}×{1,2,...,lk−1} ∈Rlk×lk−1, k ∈{1, 2, . . . , L}, and Bk = (Bk,i)i∈{1,2,...,lk} ∈Rlk,\nk ∈{1, 2, . . . , L}, satisfy for all k ∈{1, 2, . . . , L}, i ∈{1, 2, . . ., lk}, j ∈{1, 2, . . . , lk−1} that\nWk,i,j =\n(\nWk,i,j\n: (i ≤lk) ∧(j ≤lk−1)\n0\n: (i > lk) ∨(j > lk−1)\nand\nBk,i =\n(\nBk,i\n: i ≤lk\n0\n: i > lk.\n(58)\nNote that (15) ensures that ((W1, B1), (W2, B2), . . . , (WL, BL)) ∈\n\u0000\u0010L\ni=1(Rli×li−1 × Rli)\n\u0001\n⊆N and\nD\n\u0000((W1, B1), (W2, B2), . . . , (WL, BL))\n\u0001\n= (l0, l1, . . . , lL).\n(59)\nFurthermore, observe that Lemma 2.13 and (58) show that\n|||T\n\u0000((W1, B1), (W2, B2), . . . , (WL, BL))\n\u0001\n||| = |||T (Φ)|||\n(60)\n(cf. Deﬁnitions 2.11 and 2.20). In addition, note that Lemma 2.27 establishes that\nRa(Φ) = Ra\n\u0000((W1, B1), (W2, B2), . . . , (WL, BL))\n\u0001\n= Ra\n\u0000((W1, B1), (W2, B2), . . . , (WL, BL))\n\u0001\n(61)\n(cf. Deﬁnition 2.10). The proof of Lemma 2.28 is thus completed.\nLemma 2.29. Let L, L ∈N, l0, l1, . . . , lL, l0, l1, . . . , lL ∈N satisfy for all i ∈N ∩[0, L) that L ≥L\nl0 = l0, lL = lL, and li ≥li, assume for all i ∈N ∩(L −1, L) that li ≥2lL, and let Φ ∈N satisfy\nD(Φ) = (l0, l1, . . . , lL) (cf. Deﬁnition 2.9). Then there exists Ψ ∈N such that\nD(Ψ) = (l0, l1, . . . , lL),\n|||T (Ψ)||| ≤max{1, |||T (Φ)|||},\nand\nRr(Ψ) = Rr(Φ)\n(62)\n(cf. Deﬁnitions 2.11, 2.20, 2.4, and 2.10).\nProof of Lemma 2.29. Throughout this proof let Ξ ∈N satisfy Ξ = EL,IlL(Φ) (cf. Deﬁnitions 2.18\nand 2.24). Note that item (i) in [32, Lemma 3.16] demonstrates that D(IlL) = (lL, 2lL, lL) ∈N3. Com-\nbining this with Lemma 2.25 shows that D(Ξ) ∈NL+1 and\nD(Ξ) =\n(\n(l0, l1, . . . , lL)\n: L = L\n(l0, l1, . . . , lL−1, 2lL, 2lL, . . . , 2lL, lL)\n: L > L.\n(63)\n14\nMoreover, observe that Lemma 2.26 (with d ←lL, L ←L, Φ ←Φ in the notation of Lemma 2.26)\nestablishes that\n|||T (Ξ)||| ≤max{1, |||T (Φ)|||}\n(64)\n(cf. Deﬁnitions 2.11 and 2.20). In addition, note that item (iii) in [32, Lemma 3.16] ensures that for all\nx ∈RlL it holds that\n(Rr(IlL))(x) = x\n(65)\n(cf. Deﬁnitions 2.4 and 2.10). This and item (ii) in [31, Lemma 2.14] prove that\nRr(Ξ) = Rr(Φ).\n(66)\nIn the next step, we observe that (63), the hypothesis that for all i ∈[0, L) it holds that l0 = l0, lL = lL,\nand li ≤li, the hypothesis that for all i ∈N∩(L−1, L) it holds that li ≥2lL, and Lemma 2.28 (with a ←r,\nL ←L, (l0, l1, . . . , lL) ←D(Ξ), (l0, l1, . . . , lL) ←(l0, l1, . . . , lL), Φ ←Ξ in the notation of Lemma 2.28)\nensure that there exists Ψ ∈N such that\nD(Ψ) = (l0, l1, . . . , lL),\n|||T (Ψ)||| = |||T (Ξ)|||,\nand\nRr(Ψ) = Rr(Ξ).\n(67)\nCombining this with (64) and (66) establishes (62). The proof of Lemma 2.29 is thus completed.\nLemma 2.30. Let u ∈[−∞, ∞), v ∈(u, ∞], L, L, d, d ∈N, θ ∈Rd, l0, l1, . . . , lL, l0, l1, . . . , lL ∈N satisfy\nfor all i ∈N ∩[0, L) that d ≥PL\ni=1 li(li−1 + 1), d ≥PL\ni=1 li(li−1 + 1), L ≥L, l0 = l0, lL = lL, and li ≥li\nand assume for all i ∈N ∩(L −1, L) that li ≥2lL. Then there exists ϑ ∈Rd such that\n|||ϑ||| ≤max{1, |||θ|||}\nand\nN ϑ,(l0,l1,...,lL)\nu,v\n= N θ,(l0,l1,...,lL)\nu,v\n(68)\n(cf. Deﬁnitions 2.20 and 2.8).\nProof of Lemma 2.30. Throughout this proof let η1, η2, . . . , ηd ∈R satisfy\nθ = (η1, η2, . . . , ηd)\n(69)\nand let Φ ∈\n\u0000\u0010L\ni=1 Rli×li−1 × Rli\u0001\nsatisfy\nT (Φ) = (η1, η2, . . . , ηP(Φ))\n(70)\n(cf. Deﬁnition 2.11). Note that Lemma 2.29 establishes that there exists Ψ ∈N which satisﬁes\nD(Ψ) = (l0, l1, . . . , lL),\n|||T (Ψ)||| ≤max{1, |||T (Φ)|||},\nand\nRr(Ψ) = Rr(Φ)\n(71)\n(cf. Deﬁnitions 2.9, 2.20, 2.4, and 2.10). Next let ϑ = (ϑ1, ϑ2, . . . , ϑd) ∈Rd satisfy\n(ϑ1, ϑ2, . . . , ϑP(Ψ)) = T (Ψ)\nand\n∀i ∈N ∩(P(Ψ), d + 1): ϑi = 0.\n(72)\nNote that (69), (70), (71), and (72) show that\n|||ϑ||| = |||T (Ψ)||| ≤max{1, |||T (Φ)|||} ≤max{1, |||θ|||}.\n(73)\nNext observe that Corollary 2.15 and (70) establish that for all x ∈Rl0 it holds that\n\u0000N θ,(l0,l1,...,lL)\n−∞,∞\n\u0001\n(x) =\n\u0000N T (Φ),D(Φ)\n−∞,∞\n\u0001\n(x) = (Rr(Φ))(x).\n(74)\nIn addition, observe that Corollary 2.15, (71), and (72) prove that for all x ∈Rl0 it holds that\n\u0000N ϑ,(l0,l1,...,lL)\n−∞,∞\n\u0001\n(x) =\n\u0000N T (Ψ),D(Ψ)\n−∞,∞\n\u0001\n(x) = (Rr(Ψ))(x).\n(75)\nCombining this and (74) with (71) and the hypothesis that l0 = l0 and lL = lL demonstrates that\nN θ,(l0,l1,...,lL)\n−∞,∞\n= N ϑ,(l0,l1,...,lL)\n−∞,∞\n.\n(76)\nHence, we obtain that\nN θ,(l0,l1,...,lL)\nu,v\n= Cu,v,lL ◦N θ,(l0,l1,...,lL)\n−∞,∞\n= Cu,v,lL ◦N ϑ,(l0,l1,...,lL)\n−∞,∞\n= N ϑ,(l0,l1,...,lL)\nu,v\n(77)\n(cf. Deﬁnition 2.7). This and (73) establish (68). The proof of Lemma 2.30 is thus completed.\n15\n2.3\nLocal Lipschitz continuity of the parametrization function\nLemma 2.31. Let a, x, y ∈R. Then\n|max{x, a} −max{y, a}| ≤max{x, y} −min{x, y} = |x −y|.\n(78)\nProof of Lemma 2.31. Observe that\n|max{x, a} −max{y, a}| = |max{max{x, y}, a} −max{min{x, y}, a}|\n= max\n\b\nmax{x, y}, a\n\t\n−max\n\b\nmin{x, y}, a\n\t\n= max\nn\nmax{x, y} −max\n\b\nmin{x, y}, a\n\t\n, a −max\n\b\nmin{x, y}, a\n\to\n≤max\nn\nmax{x, y} −max\n\b\nmin{x, y}, a\n\t\n, a −a\no\n= max\nn\nmax{x, y} −max\n\b\nmin{x, y}, a\n\t\n, 0\no\n≤max\nn\nmax{x, y} −min{x, y}, 0\no\n= max{x, y} −min{x, y} = |max{x, y} −min{x, y}| = |x −y| .\n(79)\nThe proof of Lemma 2.31 is thus completed.\nCorollary 2.32. Let a, x, y ∈R. Then\n|min{x, a} −min{y, a}| ≤max{x, y} −min{x, y} = |x −y|.\n(80)\nProof of Corollary 2.32. Note that Lemma 2.31 ensures that\n|min{x, a} −min{y, a}| = |−(min{x, a} −min{y, a})| = |max{−x, −a} −max{−y, −a}|\n≤|(−x) −(−y)| = |x −y| .\n(81)\nThe proof of Corollary 2.32 is thus completed.\nLemma 2.33. Let d ∈N, u ∈[−∞, ∞), v ∈(u, ∞]. Then it holds for all x, y ∈Rd that\n|||Cu,v,d(x) −Cu,v,d(y)||| ≤|||x −y|||\n(82)\n(cf. Deﬁnitions 2.7 and 2.20).\nProof of Lemma 2.33. Note that Lemma 2.31, Corollary 2.32, and the fact that for all x ∈R it holds that\nmax{−∞, x} = x = min{x, ∞} show that for all x, y ∈R it holds that\n|cu,v(x) −cu,v(y)| = |max{u, min{x, v}} −max{u, min{y, v}}| ≤|min{x, v} −min{y, v}| ≤|x −y| (83)\n(cf. Deﬁnition 2.6). Hence, we obtain that for all x = (x1, x2, . . . , xd), y = (y1, y2, . . . , yd) ∈Rd it holds\nthat\n|||Cu,v,d(x) −Cu,v,d(y)||| =\nmax\ni∈{1,2,...,d}|cu,v(xi) −cu,v(yi)| ≤\nmax\ni∈{1,2,...,d}|xi −yi| = |||x −y|||\n(84)\n(cf. Deﬁnitions 2.7 and 2.20). The proof of Lemma 2.33 is thus completed.\nLemma 2.34. Let d ∈N. Then it holds for all x, y ∈Rd that\n|||Rd(x) −Rd(y)||| ≤|||x −y|||\n(85)\n(cf. Deﬁnitions 2.5 and 2.20).\nProof of Lemma 2.34. Note that Lemma 2.33 and the fact that Rd = C0,∞,d establish (85). The proof of\nLemma 2.34 is thus completed.\n16\nLemma 2.35. Let a, b ∈N, M = (Mi,j)(i,j)∈{1,2,...,a}×{1,2,...,b} ∈Ra×b. Then\nsup\nv∈Rb\\{0}\n\u0014|||Mv|||\n|||v|||\n\u0015\n=\nmax\ni∈{1,2,...,a}\n\"\nbP\nj=1\n|Mi,j|\n#\n≤b\n\u0014\nmax\ni∈{1,2,...,a}\nmax\nj∈{1,2,...,b} |Mi,j|\n\u0015\n(86)\n(cf. Deﬁnition 2.20).\nProof of Lemma 2.35. Observe that\nsup\nv∈Rb\n\u0014|||Mv|||\n|||v|||\n\u0015\n=\nsup\nv∈Rb, |||v|||≤1\n|||Mv||| =\nsup\nv=(v1,v2,...,vb)∈[−1,1]b|||Mv|||\n=\nsup\nv=(v1,v2,...,vb)∈[−1,1]b\n \nmax\ni∈{1,2,...,a}\n\f\f\f\f\f\nbP\nj=1\nMi,jvj\n\f\f\f\f\f\n!\n=\nmax\ni∈{1,2,...,a}\n \nsup\nv=(v1,v2,...,vb)∈[−1,1]b\n\f\f\f\f\f\nbP\nj=1\nMi,jvj\n\f\f\f\f\f\n!\n=\nmax\ni∈{1,2,...,a}\n \nbP\nj=1\n|Mi,j|\n!\n(87)\n(cf. Deﬁnition 2.20). The proof of Lemma 2.35 is thus completed.\nTheorem 2.36. Let a ∈R, b ∈[a, ∞), d, L ∈N, l = (l0, l1, . . . , lL) ∈NL+1 satisfy\nd ≥\nL\nX\nk=1\nlk(lk−1 + 1).\n(88)\nThen it holds for all θ, ϑ ∈Rd that\nsup\nx∈[a,b]l0\n|||N θ,l\n−∞,∞(x) −N ϑ,l\n−∞,∞(x)|||\n≤max{1, |a|, |b|}|||θ −ϑ|||\n\"L−1\nY\nm=0\n(lm + 1)\n# \"L−1\nX\nn=0\n\u0010\nmax{1, |||θ|||n} |||ϑ|||L−1−n\u0011#\n≤L max{1, |a|, |b|} (max{1, |||θ|||, |||ϑ|||})L−1\n\"L−1\nY\nm=0\n(lm + 1)\n#\n|||θ −ϑ|||\n≤L max{1, |a|, |b|} (|||l||| + 1)L (max{1, |||θ|||, |||ϑ|||})L−1 |||θ −ϑ|||\n(89)\n(cf. Deﬁnitions 2.8 and 2.20).\nProof of Theorem 2.36. Throughout this proof let θj = (θj,1, θj,2, . . . , θj,d) ∈Rd, j ∈{1, 2}, let d ∈N\nsatisfy that\nd =\nL\nX\nk=1\nlk(lk−1 + 1),\n(90)\nlet Wj,k ∈Rlk×lk−1, k ∈{1, 2, . . . , L}, j ∈{1, 2}, and Bj,k ∈Rlk, k ∈{1, 2, . . . , L}, j ∈{1, 2}, satisfy for\nall j ∈{1, 2}, k ∈{1, 2, . . ., L} that\nT\n\u0000\u0000(Wj,1, Bj,1), (Wj,2, Bj,2), . . . , (Wj,L, Bj,L)\n\u0001\u0001\n= (θj,1, θj,2, . . . , θj,d),\n(91)\nlet φj,k ∈N, k ∈{1, 2, . . . , L}, j ∈{1, 2}, satisfy for all j ∈{1, 2}, k ∈{1, 2, . . . , L} that\nφj,k =\n\u0000(Wj,1, Bj,1), (Wj,2, Bj,2), . . . , (Wj,k, Bj,k)\n\u0001\n∈\n\u0002\u0010k\ni=1\n\u0000Rli×li−1 × Rli\u0001\u0003\n,\n(92)\nlet D = [a, b]l0, let mj,k ∈[0, ∞), j ∈{1, 2}, k ∈{0, 1, . . . , L}, satisfy for all j ∈{1, 2}, k ∈{0, 1, . . . , L}\nthat\nmj,k =\n(\nmax{1, |a|, |b|}\n: k = 0\nmax{1, supx∈D |||(Rr(φj,k))(x)|||}\n: k > 0,\n(93)\n17\nand let ek ∈[0, ∞), k ∈{0, 1, . . ., L}, satisfy for all k ∈{0, 1, . . . , L} that\nek =\n(\n0\n: k = 0\nsupx∈D |||(Rr(φ1,k))(x) −(Rr(φ2,k))(x)|||\n: k > 0\n(94)\n(cf. Deﬁnitions 2.11, 2.4, 2.10, and 2.20). Note that Lemma 2.35 demonstrates that\ne1 = sup\nx∈D\n|||(Rr(φ1,1))(x) −(Rr(φ2,1))(x)||| = sup\nx∈D\n|||(W1,1x + B1,1) −(W2,1x + B2,1)|||\n≤\n\u0014\nsup\nx∈D\n|||(W1,1 −W2,1)x|||\n\u0015\n+ |||B1,1 −B2,1|||\n≤\n\"\nsup\nv∈Rl0\\{0}\n\u0012|||(W1,1 −W2,1)v|||\n|||v|||\n\u0013# \u0014\nsup\nx∈D\n|||x|||\n\u0015\n+ |||B1,1 −B2,1|||\n≤l0 |||θ1 −θ2||| max{|a|, |b|} + |||B1,1 −B2,1||| ≤l0 |||θ1 −θ2||| max{|a|, |b|} + |||θ1 −θ2|||\n= |||θ1 −θ2||| (l0 max{|a|, |b|} + 1) ≤m1,0 |||θ1 −θ2||| (l0 + 1) .\n(95)\nMoreover, observe that the triangle inequality assures that for all k ∈{1, 2, . . . , L} ∩(1, ∞) it holds that\nek = sup\nx∈D\n|||(Rr(φ1,k))(x) −(Rr(φ2,k))(x)|||\n= sup\nx∈D\n\f\f\f\n\f\f\f\n\f\f\f\nh\nW1,k\n\u0010\nRlk−1\n\u0000(Rr(φ1,k−1))(x)\n\u0001\u0011\n+ B1,k\ni\n−\nh\nW2,k\n\u0010\nRlk−1\n\u0000(Rr(φ2,k−1))(x)\n\u0001\u0011\n+ B2,k\ni\f\f\f\n\f\f\f\n\f\f\f\n≤\n\u0014\nsup\nx∈D\n\f\f\f\n\f\f\f\n\f\f\fW1,k\n\u0010\nRlk−1\n\u0000(Rr(φ1,k−1))(x)\n\u0001\u0011\n−W2,k\n\u0010\nRlk−1\n\u0000(Rr(φ2,k−1))(x)\n\u0001\u0011\f\f\f\n\f\f\f\n\f\f\f\n\u0015\n+ |||θ1 −θ2|||.\n(96)\nThe triangle inequality hence implies that for all j ∈{1, 2}, k ∈{1, 2, . . . , L} ∩(1, ∞) it holds that\nek ≤\n\u0014\nsup\nx∈D\n\f\f\f\f\f\f\u0000W1,k −W2,k\n\u0001\u0000Rlk−1\n\u0000(Rr(φj,k−1))(x)\n\u0001\u0001\f\f\f\f\f\f\n\u0015\n+\n\u0014\nsup\nx∈D\n\f\f\f\n\f\f\f\n\f\f\fW3−j,k\n\u0010\nRlk−1\n\u0000(Rr(φ1,k−1))(x)\n\u0001\n−Rlk−1\n\u0000(Rr(φ2,k−1))(x)\n\u0001\u0011\f\f\f\n\f\f\f\n\f\f\f\n\u0015\n+ |||θ1 −θ2|||\n≤\n\"\nsup\nv∈Rlk−1\\{0}\n\u0012|||(W1,k −W2,k)v|||\n|||v|||\n\u0013# \u0014\nsup\nx∈D\n\f\f\f\f\f\fRlk−1\n\u0000(Rr(φj,k−1))(x)\n\u0001\f\f\f\f\f\f\n\u0015\n+ |||θ1 −θ2|||\n+\n\"\nsup\nv∈Rlk−1\\{0}\n\u0012|||W3−j,kv|||\n|||v|||\n\u0013# \u0014\nsup\nx∈D\n\f\f\f\f\f\fRlk−1\n\u0000(Rr(φ1,k−1))(x)\n\u0001\n−Rlk−1\n\u0000(Rr(φ2,k−1))(x)\n\u0001\f\f\f\f\f\f\n\u0015\n.\n(97)\nLemma 2.35 and Lemma 2.34 therefore show that for all j ∈{1, 2}, k ∈{1, 2, . . . , L} ∩(1, ∞) it holds\nthat\nek ≤lk−1 |||θ1 −θ2|||\n\u0014\nsup\nx∈D\n\f\f\f\f\f\fRlk−1\n\u0000(Rr(φj,k−1))(x)\n\u0001\f\f\f\f\f\f\n\u0015\n+ |||θ1 −θ2|||\n+ lk−1 |||θ3−j|||\n\u0014\nsup\nx∈D\n\f\f\f\f\f\fRlk−1\n\u0000(Rr(φ1,k−1))(x)\n\u0001\n−Rlk−1\n\u0000(Rr(φ2,k−1))(x)\n\u0001\f\f\f\f\f\f\n\u0015\n≤lk−1 |||θ1 −θ2|||\n\u0014\nsup\nx∈D\n|||(Rr(φj,k−1))(x)|||\n\u0015\n+ |||θ1 −θ2|||\n+ lk−1 |||θ3−j|||\n\u0014\nsup\nx∈D\n|||(Rr(φ1,k−1))(x) −(Rr(φ2,k−1))(x)|||\n\u0015\n≤|||θ1 −θ2||| (lk−1 mj,k−1 + 1) + lk−1 |||θ3−j||| ek−1.\n(98)\nHence, we obtain that for all j ∈{1, 2}, k ∈{1, 2, . . . , L} ∩(1, ∞) it holds that\nek ≤mj,k−1 |||θ1 −θ2||| (lk−1 + 1) + lk−1 |||θ3−j||| ek−1.\n(99)\n18\nCombining this with (95), the fact that e0 = 0, and the fact that m1,0 = m2,0 demonstrates that for all\nj ∈{1, 2}, k ∈{1, 2, . . . , L} it holds that\nek ≤mj,k−1 (lk−1 + 1) |||θ1 −θ2||| + lk−1 |||θ3−j||| ek−1.\n(100)\nThis shows that for all j = (jn)n∈{0,1,...,L}: {0, 1, . . . , L} →{1, 2} and all k ∈{1, 2, . . . , L} it holds that\nek ≤mjk−1,k−1 (lk−1 + 1) |||θ1 −θ2||| + lk−1 |||θ3−jk−1||| ek−1.\n(101)\nTherefore, we obtain that for all j = (jn)n∈{0,1,...,L}: {0, 1, . . . , L} →{1, 2} and all k ∈{1, 2, . . ., L} it\nholds that\nek ≤\nk−1\nX\nn=0\n \"\nk−1\nY\nm=n+1\n\u0000lm |||θ3−jm|||\n\u0001\n#\nmjn,n (ln + 1) |||θ1 −θ2|||\n!\n= |||θ1 −θ2|||\n\"k−1\nX\nn=0\n \"\nk−1\nY\nm=n+1\n\u0000lm |||θ3−jm|||\n\u0001\n#\nmjn,n (ln + 1)\n!#\n.\n(102)\nNext observe that Lemma 2.35 ensures that for all j ∈{1, 2}, k ∈{1, 2, . . . , L} ∩(1, ∞), x ∈D it holds\nthat\n|||(Rr(φj,k))(x)||| =\n\f\f\f\n\f\f\f\n\f\f\fWj,k\n\u0010\nRlk−1\n\u0000(Rr(φj,k−1))(x)\n\u0001\u0011\n+ Bj,k\n\f\f\f\n\f\f\f\n\f\f\f\n≤\n\"\nsup\nv∈Rlk−1\\{0}\n|||Wj,kv|||\n|||v|||\n#\n\f\f\f\f\f\fRlk−1\n\u0000(Rr(φj,k−1))(x)\n\u0001\f\f\f\f\f\f + |||Bj,k|||\n≤lk−1 |||θj|||\n\f\f\f\f\f\fRlk−1\n\u0000(Rr(φj,k−1))(x)\n\u0001\f\f\f\f\f\f + |||θj|||\n≤lk−1 |||θj||| |||(Rr(φj,k−1))(x)||| + |||θj|||\n= (lk−1 |||(Rr(φj,k−1))(x)||| + 1) |||θj|||\n≤(lk−1mj,k−1 + 1) |||θj||| ≤mj,k−1 (lk−1 + 1) |||θj|||.\n(103)\nHence, we obtain for all j ∈{1, 2}, k ∈{1, 2, . . . , L} ∩(1, ∞) that\nmj,k ≤max{1, mj,k−1 (lk−1 + 1) |||θj|||}.\n(104)\nFurthermore, note that Lemma 2.35 assures that for all j ∈{1, 2}, x ∈D it holds that\n|||(Rr(φj,1))(x)||| = |||Wj,1x + Bj,1|||\n≤\n\"\nsup\nv∈Rl0\\{0}\n|||Wj,1v|||\n|||v|||\n#\n|||x||| + |||Bj,1|||\n≤l0 |||θj||| |||x||| + |||θj||| ≤l0 |||θj||| max{|a|, |b|} + |||θj|||\n= (l0 max{|a|, |b|} + 1) |||θj||| ≤m1,0 (l0 + 1) |||θj|||.\n(105)\nTherefore, we obtain that for all j ∈{1, 2} it holds that\nmj,1 ≤max{1, mj,0 (l0 + 1) |||θj|||}.\n(106)\nCombining this with (104) demonstrates that for all j ∈{1, 2}, k ∈{1, 2, . . . , L} it holds that\nmj,k ≤max{1, mj,k−1 (lk−1 + 1) |||θj|||}.\n(107)\nHence, we obtain that for all j ∈{1, 2}, k ∈{0, 1, . . . , L} it holds that\nmj,k ≤mj,0\n\"k−1\nY\nn=0\n(ln + 1)\n#\n\u0002\nmax{1, |||θj|||}\n\u0003k.\n(108)\n19\nCombining this with (102) proves that for all j = (jn)n∈{0,1,...,L}: {0, 1, . . . , L} →{1, 2} and all k ∈\n{1, 2, . . . , L} it holds that\nek ≤|||θ1 −θ2|||\n\"k−1\nX\nn=0\n \"\nk−1\nY\nm=n+1\n\u0000lm |||θ3−jm|||\n\u0001\n#  \nmjn,0\n\"n−1\nY\nv=0\n(lv + 1)\n#\nmax{1, |||θjn|||n} (ln + 1)\n!!#\n= m1,0 |||θ1 −θ2|||\n\"k−1\nX\nn=0\n \"\nk−1\nY\nm=n+1\n\u0000lm |||θ3−jm|||\n\u0001\n#  \" n\nY\nv=0\n(lv + 1)\n#\nmax{1, |||θjn|||n}\n!!#\n≤m1,0 |||θ1 −θ2|||\n\"k−1\nX\nn=0\n \"\nk−1\nY\nm=n+1\n|||θ3−jm|||\n# \"k−1\nY\nv=0\n(lv + 1)\n#\nmax{1, |||θjn|||n}\n!#\n= m1,0 |||θ1 −θ2|||\n\"k−1\nY\nn=0\n(ln + 1)\n# \"k−1\nX\nn=0\n \"\nk−1\nY\nm=n+1\n|||θ3−jm|||\n#\nmax{1, |||θjn|||n}\n!#\n.\n(109)\nTherefore, we obtain that for all j ∈{1, 2}, k ∈{1, 2, . . . , L} it holds that\nek ≤m1,0 |||θ1 −θ2|||\n\"k−1\nY\nn=0\n(ln + 1)\n# \"k−1\nX\nn=0\n \"\nk−1\nY\nm=n+1\n|||θ3−j|||\n#\nmax{1, |||θj|||n}\n!#\n= m1,0 |||θ1 −θ2|||\n\"k−1\nY\nn=0\n(ln + 1)\n# \"k−1\nX\nn=0\n\u0010\nmax{1, |||θj|||n} |||θ3−j|||k−1−n\u0011#\n≤k m1,0 |||θ1 −θ2||| (max{1, |||θ1|||, |||θ2|||})k−1\n\" k−1\nY\nm=0\n\u0000lm + 1\n\u0001\n#\n.\n(110)\nThe proof of Theorem 2.36 is thus completed.\nCorollary 2.37. Let a ∈R, b ∈[a, ∞), u ∈[−∞, ∞), v ∈(u, ∞], d, L ∈N, l = (l0, l1, . . . , lL) ∈NL+1\nsatisfy\nd ≥\nL\nX\nk=1\nlk(lk−1 + 1).\n(111)\nThen it holds for all θ, ϑ ∈Rd that\nsup\nx∈[a,b]l0\n|||N θ,l\nu,v (x) −N ϑ,l\nu,v (x)||| ≤L max{1, |a|, |b|} (|||l||| + 1)L (max{1, |||θ|||, |||ϑ|||})L−1 |||θ −ϑ|||\n(112)\n(cf. Deﬁnitions 2.8 and 2.20).\nProof of Corollary 2.37. Observe that Theorem 2.36 and Lemma 2.33 demonstrate that for all θ, ϑ ∈Rd\nit holds that\nsup\nx∈[a,b]l0\n|||N θ,l\nu,v (x) −N ϑ,l\nu,v (x)||| =\nsup\nx∈[a,b]l0\n\f\f\f\f\f\fCu,v,lL\n\u0000(N θ,l\n−∞,∞)(x)\n\u0001\n−Cu,v,lL\n\u0000(N ϑ,l\n−∞,∞)(x)\n\u0001\f\f\f\f\f\f\n≤\nsup\nx∈[a,b]l0\n|||(N θ,l\n−∞,∞)(x) −(N ϑ,l\n−∞,∞)(x)|||\n≤L max{1, |a|, |b|} (|||l||| + 1)L (max{1, |||θ|||, |||ϑ|||})L−1 |||θ −ϑ|||\n(113)\n(cf. Deﬁnitions 2.8, 2.20, and 2.7). This completes the proof of Corollary 2.37.\n20\n3\nSeparate analyses of the error sources\nIn this section we study separately the approximation error (see Section 3.1 below), the generalization\nerror (see Section 3.2 below), and the optimization error (see Section 3.3 below).\nIn particular, the main result in Section 3.1, Proposition 3.5 below, establishes an upper bound for\nthe error in the approximation of a Lipschitz continuous function by DNNs. This approximation result\nis obtained by combining the essentially well-known approximation result in Lemma 3.1 with the DNN\ncalculus in Section 2.2 above (cf., e.g., Grohs et al. [31, 32]). Some of the results in Section 3.1 are partially\nbased on material in publications from the scientiﬁc literature. In particular, the elementary result in\nLemma 3.2 is basically well-known in the scientiﬁc literature. For further approximation results for DNNs\nwe refer, e.g., to [1, 3, 4, 11, 12, 13, 14, 16, 17, 19, 21, 22, 23, 24, 25, 27, 29, 30, 31, 33, 34, 36, 38, 39, 40,\n41, 42, 44, 47, 49, 52, 53, 54, 55, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 72, 73, 74] and the references\nmentioned therein.\nIn Lemmas 3.20 and 3.21 in Section 3.2 below we study the generalization error. Our analysis in\nSection 3.2 is in parts inspired by Berner et al. [10] and Cucker & Smale [18].\nProposition 3.10 in\nSection 3.2.1 is known as Hoeﬀding’s inequality in the scientiﬁc literature and Proposition 3.10 is, e.g.,\nproved as Theorem 2 in Hoeﬀding [37]. The proof of Proposition 3.12 can be found, e.g., in Cucker\n& Smale [18, Proposition 5] (cf. also Berner et al. [10, Proposition 4.3]).\nFor further results on the\ngeneralization error we refer, e.g., to [5, 35, 51, 68, 71] and the references mentioned therein.\nIn the two elementary results in Section 3.3, Lemmas 3.22 and 3.23, we study the optimization error of\nthe minimum Monte Carlo algorithm. A related result can be found, e.g., in [6, Lemma 3.5]. For further\nresults on the optimization error we refer, e.g., to [2, 9, 15, 20, 26, 43, 45, 46, 48] and the references\nmentioned therein.\n3.1\nAnalysis of the approximation error\n3.1.1\nApproximations for Lipschitz continuous functions\nLemma 3.1. Let (E, δ) be a metric space, let M ⊆E satisfy M ̸= ∅, let L ∈[0, ∞), let f : E →R\nsatisfy for all x ∈E, y ∈M that |f(x) −f(y)| ≤Lδ(x, y), and let F : E →R ∪{∞} satisfy for all x ∈E\nthat\nF(x) = sup\ny∈M\n[f(y) −Lδ(x, y)].\n(114)\nThen\n(i) it holds for all x ∈E that F(x) ≤f(x),\n(ii) it holds for all x ∈M that F(x) = f(x),\n(iii) it holds for all x, y ∈E that |F(x) −F(y)| ≤Lδ(x, y), and\n(iv) it holds for all x ∈E that\n|F(x) −f(x)| ≤2L\n\u0014\ninf\ny∈M δ(x, y)\n\u0015\n.\n(115)\nProof of Lemma 3.1. First, observe that the hypothesis that for all x ∈E, y ∈M it holds that |f(x) −\nf(y)| ≤Lδ(x, y) ensures that for all x ∈E, y ∈M it holds that\nf(x) ≥f(y) −Lδ(x, y).\n(116)\nHence, we obtain that for all x ∈E it holds that\nf(x) ≥sup\ny∈M\n[f(y) −Lδ(x, y)] = F(x).\n(117)\n21\nThis establishes item (i). Next observe that (114) implies that for all x ∈M it holds that\nF(x) ≥f(x) −Lδ(x, x) = f(x).\n(118)\nCombining this with item (i) establishes item (ii). In the next step we note that (114) and the fact that\nfor all x ∈E it holds that F(x) ≤f(x) < ∞show that for all x, y ∈E it holds that\nF(x) −F(y) =\n\u0014\nsup\nv∈M\n(f(v) −Lδ(x, v))\n\u0015\n−\n\u0014\nsup\nw∈M\n(f(w) −Lδ(y, w))\n\u0015\n= sup\nv∈M\n\u0014\nf(v) −Lδ(x, v) −sup\nw∈M\n(f(w) −Lδ(y, w))\n\u0015\n≤sup\nv∈M\n\u0002\nf(v) −Lδ(x, v) −(f(v) −Lδ(y, v))\n\u0003\n= L\n\u0014\nsup\nv∈M\n(δ(y, v) −δ(x, v))\n\u0015\n≤L\n\u0014\nsup\nv∈M\n(δ(y, x) + δ(x, v) −δ(x, v))\n\u0015\n= Lδ(x, y).\n(119)\nCombining this with the fact that for all x, y ∈E it holds that δ(x, y) = δ(y, x) establishes item (iii).\nObserve that item (ii), the triangle inequality, item (iii), and the hypothesis that for all x ∈E, y ∈M it\nholds that |f(x) −f(y)| ≤Lδ(x, y) ensure that for all x ∈E it holds that\n|F(x) −f(x)| = inf\ny∈M |F(x) −F(y) + f(y) −f(x)|\n≤inf\ny∈M (|F(x) −F(y)| + |f(y) −f(x)|)\n≤inf\ny∈M(2Lδ(x, y)) = 2L\n\u0014\ninf\ny∈M δ(x, y)\n\u0015\n.\n(120)\nThis establishes item (iv). The proof of Lemma 3.1 is thus completed.\n3.1.2\nDNN representations for maxima\nLemma 3.2. Let Φ ∈N satisfy\nΦ =\n\n\n\n\n\n\n\n1\n−1\n0\n1\n0\n−1\n\n,\n\n\n0\n0\n0\n\n\n\n,\n\u0000\u00001\n1\n−1\n\u0001\n, 0\n\u0001\n\n\n∈\n\u0000(R3×2 × R3) × (R1×3 × R)\n\u0001\n(121)\n(cf. Deﬁnition 2.9). Then\n(i) it holds for all k ∈N that L(Ik) = 2,\n(ii) there exist unique φk ∈N, k ∈{2, 3, . . .}, which satisfy for all k ∈{2, 3, . . .} that φ2 = Φ, I(φk) =\nO(P2(Φ, Ik−1)), and\nφk+1 = φk •\n\u0000P2(Φ, Ik−1)\n\u0001\n,\n(122)\n(iii) it holds for all k ∈{2, 3, . . .} that L(φk) = k, and\n(iv) it holds for all k ∈{2, 3, . . .} that D(φk) = (k, 2k −1, 2k −3, . . . , 3, 1) ∈Nk+1, and\n(v) it holds for all k ∈{2, 3, . . .}, x = (x1, x2, . . . , xk) ∈Rk that\n\u0000Rr(φk)\n\u0001\n(x) = max{x1, x2, . . . , xk}\n(123)\n22\n(cf. Deﬁnitions 2.18, 2.16, 2.19, 2.4, and 2.10).\nProof of Lemma 3.2. First, note that, e.g., item (i) in [32, Lemma 3.16] shows that for all k ∈N it holds\nthat\nD(Ik) = (k, 2k, k).\n(124)\nThis establishes item (i). Next note that (121) demonstrates that\nD(Φ) = (2, 3, 1).\n(125)\nCombining this and (124) with item (i) in [31, Proposition 2.20] shows that for all k ∈N it holds that\nD(P2(Φ, Ik)) = (k + 2, 2k + 3, k + 1)\n(126)\n(cf. Deﬁnition 2.16). Hence, we obtain that for all k ∈{2, 3, . . .} it holds that\nD(P2(Φ, Ik−1)) = (k + 1, 2k + 1, k).\n(127)\nCombining this with (126) ensures that for all k ∈{2, 3, . . .} it holds that\nO(P2(Φ, Ik)) = k + 1 = I(P2(Φ, Ik−1)).\n(128)\nMoreover, note that (121) and (126) assure that\nI(Φ) = 2 = O(P2(Φ, I1)).\n(129)\nFurthermore, observe that item (i) in [31, Proposition 2.6] and (128) show that for all k ∈{2, 3, . . .},\nψ ∈N with I(ψ) = O(P2(Φ, Ik−1)) it holds that\nI\n\u0000ψ •\n\u0000P2(Φ, Ik−1)\n\u0001\u0001\n= I(P2(Φ, Ik−1)) = O(P2(Φ, Ik))\n(130)\n(cf. Deﬁnition 2.19). Combining this and (129) with induction establishes item (ii). In the next step we\nnote that (122) and item (ii) in [31, Proposition 2.6] imply that for all k ∈{2, 3, . . .} it holds that\nL(φk+1) = L(φk) + L(P2(φ2, Ik−1)) −1 = L(φk) + 1.\n(131)\nCombining this and the fact that L(φ2) = 2 with induction establishes item (iii). Furthermore, observe\nthat (122), (127), and item (i) in [31, Proposition 2.6] demonstrate that for all k ∈{2, 3, . . .}, l0, l1, . . . , lk ∈\nN with D(φk) = (l0, l1, . . . , lk) it holds that\nD(φk+1) = D\n\u0000φk •\n\u0000P2(Φ, Ik−1)\n\u0001\u0001\n= (k + 1, 2k + 1, l1, l2, . . . , lk).\n(132)\nThis, item (iii), the fact that D(φ2) = (2, 3, 1), and induction establish item (iv). Moreover, note that\n(121) ensures that for all (x1, x2) ∈R2 it holds that\n\u0000Rr(Φ)\n\u0001\n(x1, x2) =\n\u00001\n1\n−1\n\u0001\n\nMr,3\n\n\n\n\n1\n−1\n0\n1\n0\n−1\n\n\n\u0012x1\nx2\n\u0013\n+\n\n\n0\n0\n0\n\n\n\n\n\n+ 0\n=\n\u00001\n1\n−1\u0001\n\n\nmax{x1 −x2, 0}\nmax{x2, 0}\nmax{−x2, 0}\n\n\n= max{x1 −x2, 0} + max{x2, 0} −max{−x2, 0}\n= max{x1 −x2, 0} + x2 = max{x1, x2}\n(133)\n23\n(cf. Deﬁnitions 2.4, 2.10, and 2.3). Combining this and item (iii) in [32, Lemma 3.16] with [31, Proposi-\ntion 2.19] proves that for all k ∈{2, 3, . . .}, x = (x1, x2, . . . , xk+1) ∈Rk+1 it holds that\n\u0000Rr(P2(Φ, Ik−1))\n\u0001\n(x) =\n\u0000\u0000Rr(Φ)\n\u0001\n(x1, x2),\n\u0000Rr(Ik−1)\n\u0001\n(x3, x4, . . . , xk+1)\n\u0001\n= (max{x1, x2}, x3, x4, . . . , xk+1) ∈Rk.\n(134)\nItem (v) in [31, Proposition 2.6] and (122) hence show that for all k ∈{2, 3, . . .}, x = (x1, x2, . . . , xk+1) ∈\nRk+1 it holds that\n\u0000Rr(φk+1)\n\u0001\n(x) =\n\u0010\nRr\n\u0000φk •\n\u0000P2(Φ, Ik−1)\n\u0001\u0001\u0011\n(x) =\n\u0000\u0002\nRr(φk)\n\u0003\n◦\n\u0002\nRr(P2(Φ, Ik−1))\n\u0003\u0001\n(x)\n=\n\u0000Rr(φk)\n\u0001\n(max{x1, x2}, x3, x4, . . . , xk+1).\n(135)\nThis, the fact that φ2 = Φ, (133), and induction establish item (v). The proof of Lemma 3.2 is thus\ncompleted.\nLemma 3.3. Let Ak ∈R(2k−1)×k, k ∈{2, 3, . . .}, and Ck ∈R(k−1)×(2k−1), k ∈{2, 3, . . .}, satisfy for all\nk ∈{2, 3, . . .} that\nAk =\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n−1\n0\n· · ·\n0\n0\n1\n0\n· · ·\n0\n0\n−1\n0\n· · ·\n0\n0\n0\n1\n· · ·\n0\n0\n0\n−1\n· · ·\n0\n...\n...\n...\n...\n...\n0\n0\n0\n· · ·\n1\n0\n0\n0\n· · ·\n−1\n\n\n\n\n\n\n\n\n\n\n\n\n\nand\nCk =\n\n\n\n\n\n1\n1\n−1\n0\n0\n· · ·\n0\n0\n0\n0\n0\n1\n−1\n· · ·\n0\n0\n...\n...\n...\n...\n...\n...\n...\n...\n0\n0\n0\n0\n0\n· · ·\n1\n−1\n\n\n\n\n\n(136)\nand let φk = ((Wk,1, Bk,1), (Wk,2, Bk,2), . . . , (Wk,k, Bk,k)) ∈N, k ∈{2, 3, . . .}, satisfy for all k ∈{2, 3, . . .}\nthat I(φk) = O(P2(φ2, Ik−1)), φk+1 = φk • (P2(φ2, Ik−1)), and\nφ2 =\n\n\n\n\n\n\n\n1\n−1\n0\n1\n0\n−1\n\n,\n\n\n0\n0\n0\n\n\n\n,\n\u0000\u00001\n1\n−1\n\u0001\n, 0\n\u0001\n\n\n∈\n\u0000(R3×2 × R3) × (R1×3 × R)\n\u0001\n(137)\n(cf. Deﬁnitions 2.9, 2.18, 2.19, and 2.16 and Lemma 3.2). Then\n(i) it holds for all k ∈{2, 3, . . .} that Wk,1 = Ak,\n(ii) it holds for all k ∈{2, 3, . . .}, l ∈{1, 2, . . . , k} that Bk,l = 0 ∈R2(k−l)+1,\n(iii) it holds for all k ∈{2, 3, . . .}, l ∈{3, 4, . . . , k + 1} that (Wk+1,l, Bk+1,l) = (Wk,l−1, Bk,l−1),\n(iv) it holds for all k ∈{2, 3, . . .} that Wk+1,2 = Wk,1Ck+1,\n(v) it holds for all k ∈{2, 3, . . .} that (0, 0, . . . , 0) ̸= T (φk) ∈\n\u0000{−1, 0, 1}P(φk)\u0001\n, and\n(vi) it holds for all k ∈{2, 3, . . .} that |||T (φk)||| = 1\n(cf. Deﬁnitions 2.11 and 2.20).\nProof of Lemma 3.3. First, note that (28), (29), (136), and (137) ensure that for all k ∈{2, 3, . . .} it holds\nthat\nP2(φ2, Ik−1) = (NAk+1, NCk+1)\n(138)\n24\n(cf. Deﬁnition 2.17). This and (137) imply that for all k ∈{2, 3, . . .} it holds that\nφk+1 = φk •\n\u0000P2(φ2, Ik−1)\n\u0001\n= ((Wk,1, Bk,1), (Wk,2, Bk,2), . . . , (Wk,k, Bk,k)) • (NAk+1, NCk+1)\n= (NAk+1, (Wk,1Ck+1, Bk,1), (Wk,2, Bk,2), . . . , (Wk,k, Bk,k)).\n(139)\nThis, (136), and (137) establish item (i). Next observe that (137), (139), item (iv) in Lemma 3.2, and\ninduction prove item (ii). Moreover, note that (139) establishes items (iii) and (iv). In addition, observe\nthat item (i) proves that for all k ∈{2, 3, . . .} it holds that\nWk,1Ck+1 = AkCk+1 =\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n−1\n0\n· · ·\n0\n0\n1\n0\n· · ·\n0\n0\n−1\n0\n· · ·\n0\n0\n0\n1\n· · ·\n0\n0\n0\n−1\n· · ·\n0\n...\n...\n...\n...\n...\n0\n0\n0\n· · ·\n1\n0\n0\n0\n· · ·\n−1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n1\n−1\n0\n0\n· · ·\n0\n0\n0\n0\n0\n1\n−1\n· · ·\n0\n0\n...\n...\n...\n...\n...\n...\n...\n...\n0\n0\n0\n0\n0\n· · ·\n1\n−1\n\n\n\n\n\n=\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n1\n−1\n−1\n1\n0\n0\n· · ·\n0\n0\n0\n0\n0\n1\n−1\n0\n0\n· · ·\n0\n0\n0\n0\n0\n−1\n1\n0\n0\n· · ·\n0\n0\n0\n0\n0\n0\n0\n1\n−1\n· · ·\n0\n0\n0\n0\n0\n0\n0\n−1\n1\n· · ·\n0\n0\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n0\n0\n0\n0\n0\n0\n0\n· · ·\n1\n−1\n0\n0\n0\n0\n0\n0\n0\n· · ·\n−1\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n.\n(140)\nCombining this, (137), and (139) with induction proves item (v). Next note that item (v) establishes\nitem (vi). The proof of Lemma 3.3 is thus completed.\n3.1.3\nInterpolations through DNNs\nLemma 3.4. Let φk ∈N, k ∈{2, 3, . . .}, satisfy for all k ∈{2, 3, . . .} that I(φk) = O(P2(φ2, Ik−1)),\nφk+1 = φk • (P2(φ2, Ik−1)), and\nφ2 =\n\n\n\n\n\n\n\n1\n−1\n0\n1\n0\n−1\n\n,\n\n\n0\n0\n0\n\n\n\n,\n\u0000\u00001\n1\n−1\n\u0001\n, 0\n\u0001\n\n\n∈\n\u0000(R3×2 × R3) × (R1×3 × R)\n\u0001\n,\n(141)\nlet d ∈N, L ∈[0, ∞), let M ⊆Rd satisfy |M| ∈{2, 3, . . .}, let m: {1, 2, . . . , |M|} →M be bijective, let\nf : M →R and F : Rd →R satisfy for all x = (x1, x2, . . . , xd) ∈Rd that\nF(x) =\nmax\ny=(y1,y2...,yd)∈M\n\"\nf(y) −L\n \nd\nX\ni=1\n|xi −yi|\n!#\n,\n(142)\nlet W1 ∈R(2d)×d, W2 ∈R1×(2d), and Bz ∈R2d, z ∈M, satisfy for all z = (z1, z2, . . . , zd) ∈M that\nW1 =\n\n\n\n\n\n\n\n\n\n\n\n1\n0\n· · ·\n0\n−1\n0\n· · ·\n0\n0\n1\n· · ·\n0\n0\n−1\n· · ·\n0\n...\n...\n...\n...\n0\n0\n· · ·\n1\n0\n0\n· · ·\n−1\n\n\n\n\n\n\n\n\n\n\n\n,\nBz =\n\n\n\n\n\n\n\n\n\n\n\n−z1\nz1\n−z2\nz2\n...\n−zd\nzd\n\n\n\n\n\n\n\n\n\n\n\n,\nand\nW2 =\n\u0000−L\n−L\n· · ·\n−L\n\u0001\n,\n(143)\n25\nlet W1 ∈R(2d|M|)×d, B1 ∈R2d|M|, W2 ∈R|M|×(2d|M|), B2 ∈R|M| satisfy\nW1 =\n\n\n\n\n\nW1\nW1\n...\nW1\n\n\n\n\n, B1 =\n\n\n\n\n\nBm(1)\nBm(2)\n...\nBm(|M|)\n\n\n\n\n, W2 =\n\n\n\n\n\nW2\n0\n· · ·\n0\n0\nW2\n· · ·\n0\n...\n...\n...\n...\n0\n0\n· · ·\nW2\n\n\n\n\n, and B2 =\n\n\n\n\n\nf(m(1))\nf(m(2))\n...\nf(m(|M|))\n\n\n\n\n,\n(144)\nand let Φ ∈N satisfy Φ = φ|M| • ((W1, B1), (W2, B2)) (cf. Deﬁnitions 2.9, 2.18, 2.16, and 2.19 and\nLemma 3.2). Then\n(i) it holds that D(Φ) = (d, 2d|M|, 2|M| −1, 2|M| −3, . . . , 3, 1) ∈N|M|+2,\n(ii) it holds that L(Φ) = |M| + 1,\n(iii) it holds that |||T (Φ)||| ≤max{1, L, supz∈M|||z|||, 2[supz∈M |f(z)|]}, and\n(iv) it holds that F = Rr(Φ)\n(cf. Deﬁnitions 2.11, 2.20, 2.4, and 2.10).\nProof of Lemma 3.4. Throughout this proof let Ψ ∈N satisfy Ψ = ((W1, B1), (W2, B2)) and let mi,j ∈R,\ni ∈{1, 2, . . . , |M|}, j ∈{1, 2 . . ., d}, satisfy for all i ∈{1, 2, . . . , |M|}, j ∈{1, 2 . . ., d} that m(i) =\n(mi,1, mi,2, . . . , mi,d). Note that Lemma 3.2 establishes that there exist W1 ∈R(2|M|−1)×|M|, B1 ∈R2|M|−1,\nW2 ∈R(2|M|−3)×(2|M|−1), B2 ∈R2|M|−3, . . . , W|M|−1 ∈R3×5, B|M|−1 ∈R3, W|M| ∈R1×3, B|M| ∈R such\nthat\nφ|M| = ((W1, B1), (W2, B2), . . . , (W|M|, B|M|)).\n(145)\nNext observe that (144) establishes that L(Ψ) = 2 and\nD(Ψ) = (d, 2d|M|, |M|).\n(146)\nMoreover, note that item (iv) in Lemma 3.2 ensures that\nD(φ|M|) = (|M|, 2|M| −1, 2|M| −3, . . . , 3, 1) ∈N|M|+1.\n(147)\nThis, the fact that Φ = φ|M| • Ψ, (146), and item (i) in [31, Proposition 2.6] show that L(Φ) = |M| + 1\nand\nD(Φ) = (d, 2d|M|, 2|M| −1, 2|M| −3, . . . , 3, 1) ∈N|M|+2.\n(148)\nThis establishes items (i) and (ii).\nIn the next step we note that the hypothesis that Φ = φ|M| •\n((W1, B1), (W2, B2)) and (145) ensure that\nΦ = ((W1, B1), (W2, B2), . . . , (W|M|, B|M|)) • ((W1, B1), (W2, B2))\n= ((W1, B1), (W1W2, W1B2 + B1), (W2, B2), . . . , (W|M|, B|M|)).\n(149)\nLemma 2.13 hence implies that\nT (Φ) =\n\u0000T\n\u0000((W1, B1))\n\u0001\n, T\n\u0000((W1W2, W1B2 + B1))\n\u0001\n, T\n\u0000((W2, B2))\n\u0001\n, . . . , T\n\u0000((W|M|, B|M|))\n\u0001\u0001\n(150)\n(cf. Deﬁnition 2.11). Moreover, note that (144) and item (i) in Lemma 3.3 imply that\nW1W2 =\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n−1\n0\n· · ·\n0\n0\n1\n0\n· · ·\n0\n0\n−1\n0\n· · ·\n0\n0\n0\n1\n· · ·\n0\n0\n0\n−1\n· · ·\n0\n...\n...\n...\n...\n...\n0\n0\n0\n· · ·\n1\n0\n0\n0\n· · ·\n−1\n\n\n\n\n\n\n\n\n\n\n\n\n\n|\n{z\n}\n∈R(2|M|−1)×|M|\nW2 =\n\n\n\n\n\n\n\n\n\n\n\n\n\nW2\n−W2\n0\n· · ·\n0\n0\nW2\n0\n· · ·\n0\n0\n−W2\n0\n· · ·\n0\n0\n0\nW2\n· · ·\n0\n0\n0\n−W2\n· · ·\n0\n...\n...\n...\n...\n...\n0\n0\n0\n· · ·\nW2\n0\n0\n0\n· · ·\n−W2\n\n\n\n\n\n\n\n\n\n\n\n\n\n|\n{z\n}\n∈R(2|M|−1)×(2d|M|)\n.\n(151)\n26\nIn addition, observe that (144) and items (i) and (ii) in Lemma 3.3 show that\nW1B2 + B1 =\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n−1\n0\n· · ·\n0\n0\n1\n0\n· · ·\n0\n0\n−1\n0\n· · ·\n0\n0\n0\n1\n· · ·\n0\n0\n0\n−1\n· · ·\n0\n...\n...\n...\n...\n...\n0\n0\n0\n· · ·\n1\n0\n0\n0\n· · ·\n−1\n\n\n\n\n\n\n\n\n\n\n\n\n\n|\n{z\n}\n∈R(2|M|−1)×|M|\nB2 +\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n0\n0\n0\n0\n...\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n| {z }\n∈R2|M|−1\n=\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n−1\n0\n· · ·\n0\n0\n1\n0\n· · ·\n0\n0\n−1\n0\n· · ·\n0\n0\n0\n1\n· · ·\n0\n0\n0\n−1\n· · ·\n0\n...\n...\n...\n...\n...\n0\n0\n0\n· · ·\n1\n0\n0\n0\n· · ·\n−1\n\n\n\n\n\n\n\n\n\n\n\n\n\n|\n{z\n}\n∈R(2|M|−1)×|M|\n\n\n\n\n\nf(m(1))\nf(m(2))\n...\nf(m(|M|))\n\n\n\n\n\n|\n{z\n}\n∈R|M|\n=\n\n\n\n\n\n\n\n\n\n\n\n\n\nf(m(1)) −f(m(2))\nf(m(2))\n−f(m(2))\nf(m(3))\n−f(m(3))\n...\nf(m(|M|))\n−f(m(|M|))\n\n\n\n\n\n\n\n\n\n\n\n\n\n|\n{z\n}\n∈R2|M|−1\n.\n(152)\nThis and (151) demonstrate that\n\f\f\f\f\f\fT\n\u0000((W1W2, W1B2 + B1))\n\u0001\f\f\f\f\f\f\n= max{L, |f(m(1)) −f(m(2))|, |f(m(2))|, |f(m(3))|, . . ., |f(m(|M|))|} ≤max\n\u001a\nL, 2\n\u0014\nsup\nz∈M\n|f(z)|\n\u0015\u001b\n(153)\n(cf. Deﬁnition 2.20). Combining this, (144), and item (vi) in Lemma 3.3 with (150) proves that\n|||T (Φ)||| ≤max\n\b\f\f\f\f\f\fT\n\u0000((W1, B1))\n\u0001\f\f\f\f\f\f ,\n\f\f\f\f\f\fT\n\u0000((W1W2, W1B2 + B1))\n\u0001\f\f\f\f\f\f , |||T (φ|M|)|||\n\t\n≤max\n\u001a\n1, sup\nz∈M\n|||z|||, L, 2\n\u0014\nsup\nz∈M\n|f(z)|\n\u0015\u001b\n.\n(154)\nThis establishes item (iii).\nObserve that (143) ensures that for all x = (x1, x2, . . . , xd) ∈Rd, z =\n(z1, z2, . . . , zd) ∈M it holds that\nW1x + Bz =\n\n\n\n\n\n\n\n\n\n\n\n1\n0\n· · ·\n0\n−1\n0\n· · ·\n0\n0\n1\n· · ·\n0\n0\n−1\n· · ·\n0\n...\n...\n...\n...\n0\n0\n· · ·\n1\n0\n0\n· · ·\n−1\n\n\n\n\n\n\n\n\n\n\n\n|\n{z\n}\n∈R(2d)×d\n\n\n\n\n\nx1\nx2\n...\nxd\n\n\n\n\n+\n\n\n\n\n\n\n\n\n\n\n\n−z1\nz1\n−z2\nz2\n...\n−zd\nzd\n\n\n\n\n\n\n\n\n\n\n\n=\n\n\n\n\n\n\n\n\n\n\n\nx1\n−x1\nx2\n−x2\n...\nxd\n−xd\n\n\n\n\n\n\n\n\n\n\n\n+\n\n\n\n\n\n\n\n\n\n\n\n−z1\nz1\n−z2\nz2\n...\n−zd\nzd\n\n\n\n\n\n\n\n\n\n\n\n=\n\n\n\n\n\n\n\n\n\n\n\nx1 −z1\n−(x1 −z1)\nx2 −z2\n−(x2 −z2)\n...\nxd −zd\n−(xd −zd)\n\n\n\n\n\n\n\n\n\n\n\n.\n(155)\n27\nThis and (144) prove that for all x = (x1, x2, . . . , xd) ∈Rd, z = (z1, z2, . . . , zd) ∈M it holds that\nW2\n\u0000R2d(W1x + Bz)\n\u0001\n=\n\u0000−L\n−L\n· · ·\n−L\n\u0001\n|\n{z\n}\n∈R1×(2d)\n\n\n\n\n\n\n\n\n\n\n\nmax{x1 −z1, 0}\nmax{z1 −x1, 0}\nmax{x2 −z2, 0}\nmax{z2 −x2, 0}\n...\nmax{xd −zd, 0}\nmax{zd −xd, 0}\n\n\n\n\n\n\n\n\n\n\n\n= −L\n\"\nd\nX\ni=1\n(max{xi −zi, 0} + max{zi −xi, 0})\n#\n= −L\n\"\nd\nX\ni=1\n|xi −zi|\n#\n(156)\n(cf. Deﬁnition 2.5). Moreover, note that (144) implies that for all x ∈Rd it holds that\nW1x + B1 =\n\n\n\n\n\nW1x + Bm(1)\nW1x + Bm(2)\n...\nW1x + Bm(|M|)\n\n\n\n\n.\n(157)\nTherefore, we obtain that for all x ∈Rd it holds that\nR2d|M|(W1x + B1) =\n\n\n\n\n\nR2d(W1x + Bm(1))\nR2d(W1x + Bm(2))\n...\nR2d(W1x + Bm(|M|))\n\n\n\n\n.\n(158)\nThis, (144), and (156) imply that for all x = (x1, x2, . . . , xd) ∈Rd it holds that\n\u0000Rr(Ψ)\n\u0001\n(x) = W2\n\u0000R2d|M|(W1x + B1)\n\u0001\n+ B2\n=\n\n\n\n\n\nW2\n0\n· · ·\n0\n0\nW2\n· · ·\n0\n...\n...\n...\n...\n0\n0\n· · ·\nW2\n\n\n\n\n\n\n\n\n\n\nR2d(W1x + Bm(1))\nR2d(W1x + Bm(2))\n...\nR2d(W1x + Bm(|M|))\n\n\n\n\n+\n\n\n\n\n\nf(m(1))\nf(m(2))\n...\nf(m(|M|))\n\n\n\n\n\n=\n\n\n\n\n\nW2\n\u0000R2d(W1x + Bm(1))\n\u0001\nW2\n\u0000R2d(W1x + Bm(2))\n\u0001\n...\nW2\n\u0000R2d(W1x + Bm(|M|))\n\u0001\n\n\n\n\n+\n\n\n\n\n\nf(m(1))\nf(m(2))\n...\nf(m(|M|))\n\n\n\n\n\n=\n\n\n\n\n\nf(m(1)) −L\n\u0002Pd\ni=1 |xi −m1,i|\n\u0003\nf(m(2)) −L\n\u0002Pd\ni=1 |xi −m2,i|\n\u0003\n...\nf(m(|M|)) −L\n\u0002Pd\ni=1 |xi −m|M|,i|\n\u0003\n\n\n\n\n\n(159)\n(cf. Deﬁnitions 2.4 and 2.10). This, the fact that Φ = φ|M| • Ψ, item (v) in Lemma 3.2, and item (v) in\n[31, Proposition 2.6] ensure that for all x = (x1, x2, . . . , xd) ∈Rd it holds that\n\u0000Rr(Φ)\n\u0001\n(x) =\n\u0000\u0002\nRr(φ|M|)\n\u0003\n◦\n\u0002\nRr(Ψ)\n\u0003\u0001\n(x) =\nmax\ni∈{1,2,...,|M|}\n\"\nf(m(i)) −L\n \nd\nX\nj=1\n|xj −mi,j|\n!#\n=\nmax\ny=(y1,y2...,yd)∈M\n\"\nf(z) −L\n \nd\nX\ni=1\n|xi −yi|\n!#\n.\n(160)\nThis establishes item (iv). The proof of Lemma 3.4 is thus completed.\n28\n3.1.4\nExplicit approximations through DNNs\nProposition 3.5. Let φk ∈N, k ∈{2, 3, . . .}, satisfy for all k ∈{2, 3, . . .} that I(φk) = O(P2(φ2, Ik−1)),\nφk+1 = φk • (P2(φ2, Ik−1)), and\nφ2 =\n\n\n\n\n\n\n\n1\n−1\n0\n1\n0\n−1\n\n,\n\n\n0\n0\n0\n\n\n\n,\n\u0000\u00001\n1\n−1\n\u0001\n, 0\n\u0001\n\n\n∈\n\u0000(R3×2 × R3) × (R1×3 × R)\n\u0001\n,\n(161)\nlet d ∈N, L ∈R, let D ⊆Rd be a set, let f : D →R satisfy for all x = (x1, x2, . . . , xd), y =\n(y1, y2, . . . , yd) ∈D that |f(x) −f(y)| ≤L\n\u0002Pd\ni=1 |xi −yi|\n\u0003\n, let M ⊆D satisfy |M| ∈{2, 3, . . .}, let\nm: {1, 2, . . . , |M|} →M be bijective, let W1 ∈R(2d)×d, W2 ∈R1×(2d), and Bz ∈R2d, z ∈M, satisfy for\nall z = (z1, z2, . . . , zd) ∈M that\nW1 =\n\n\n\n\n\n\n\n\n\n\n\n1\n0\n· · ·\n0\n−1\n0\n· · ·\n0\n0\n1\n· · ·\n0\n0\n−1\n· · ·\n0\n...\n...\n...\n...\n0\n0\n· · ·\n1\n0\n0\n· · ·\n−1\n\n\n\n\n\n\n\n\n\n\n\n,\nBz =\n\n\n\n\n\n\n\n\n\n\n\n−z1\nz1\n−z2\nz2\n...\n−zd\nzd\n\n\n\n\n\n\n\n\n\n\n\n,\nand\nW2 =\n\u0000−L\n−L\n· · ·\n−L\u0001\n,\n(162)\nlet W1 ∈R(2d|M|)×d, B1 ∈R2d|M|, W2 ∈R|M|×(2d|M|), B2 ∈R|M| satisfy\nW1 =\n\n\n\n\n\nW1\nW1\n...\nW1\n\n\n\n\n, B1 =\n\n\n\n\n\nBm(1)\nBm(2)\n...\nBm(|M|)\n\n\n\n\n, W2 =\n\n\n\n\n\nW2\n0\n· · ·\n0\n0\nW2\n· · ·\n0\n...\n...\n...\n...\n0\n0\n· · ·\nW2\n\n\n\n\n, and B2 =\n\n\n\n\n\nf(m(1))\nf(m(2))\n...\nf(m(|M|))\n\n\n\n\n,\n(163)\nand let Φ ∈N satisfy Φ = φ|M| • ((W1, B1), (W2, B2)) (cf. Deﬁnitions 2.9, 2.18, 2.16, and 2.19 and\nLemma 3.2). Then\n(i) it holds that D(Φ) = (d, 2d|M|, 2|M| −1, 2|M| −3, . . . , 3, 1) ∈N|M|+2,\n(ii) it holds that |||T (Φ)||| ≤max{1, L, supz∈M|||z|||, 2[supz∈M |f(z)|]}, and\n(iii) it holds that\nsup\nx∈D\n\f\ff(x) −\n\u0000Rr(Φ)\n\u0001\n(x)\n\f\f ≤2L\n\"\nsup\nx=(x1,x2,...,xd)∈D\n \ninf\ny=(y1,y2,...,yd)∈M\nd\nX\ni=1\n|xi −yi|\n!#\n(164)\n(cf. Deﬁnitions 2.11, 2.20, 2.4, and 2.10).\nProof of Proposition 3.5. Throughout this proof let F : Rd →R satisfy for all x = (x1, x2, . . . , xd) ∈Rd\nthat\nF(x) =\nmax\ny=(y1,y2...,yd)∈M\n\"\nf(y) −L\n \nd\nX\ni=1\n|xi −yi|\n!#\n.\n(165)\nObserve that Lemma 3.4 establishes that\n(A) it holds that D(Φ) = (d, 2d|M|, 2|M| −1, 2|M| −3, . . . , 3, 1) ∈N|M|+2,\n(B) it holds that |||T (Φ)||| ≤max{1, L, supz∈M|||z|||, 2[supz∈M |f(z)|]}, and\n29\n(C) it holds for all x ∈D that\n\u0000Rr(Φ)\n\u0001\n(x) = F(x)\n(cf. Deﬁnitions 2.11, 2.20, 2.4, and 2.10). Note that items (A) and (B) prove items (i) and (ii). Next\nobserve that item (C) and Lemma 3.1 (with E ←D, δ ←(D × D ∋((x1, x2, . . . , xd), (y1, y2, . . . , yd)) 7→\nPd\ni=1|xi −yi| ∈[0, ∞)), M ←M, L ←L, f ←f, F ←(D ∋x 7→F(x) ∈R ∪{∞}) in the notation of\nLemma 3.1) ensure that\nsup\nx∈D\n\f\ff(x) −\n\u0000Rr(Φ)\n\u0001\n(x)\n\f\f = sup\nx∈D\n|f(x) −F(x)|\n≤2L\n\"\nsup\nx=(x1,x2,...,xd)∈D\n \ninf\ny=(y1,y2,...,yd)∈M\nd\nX\ni=1\n|xi −yi|\n!#\n.\n(166)\nThe proof of Proposition 3.5 is thus completed.\n3.1.5\nImplicit approximations through DNNs\nCorollary 3.6. Let d, d ∈N, L ∈R, let D ⊆Rd be a set, let f : D →R satisfy for all x = (x1, x2, . . . , xd),\ny = (y1, y2, . . . , yd) ∈D that |f(x)−f(y)| ≤L\n\u0002Pd\ni=1 |xi−yi|\n\u0003\n, let M ⊆D satisfy |M| ∈{2, 3, . . .}, and let\nl = (l0, l1, . . . , l|M|+1) ∈N|M|+2 satisfy l = (d, 2d|M|, 2|M|−1, 2|M|−3, . . ., 3, 1) and P|M|+1\nk=1\nlk(lk−1+1) ≤\nd. Then there exists θ ∈Rd such that |||θ||| ≤max{1, L, supz∈M|||z|||, 2[supz∈M |f(z)|]} and\nsup\nx∈D\n\f\ff(x) −(N θ,l\n−∞,∞)(x)\n\f\f ≤2L\n\"\nsup\nx=(x1,x2,...,xd)∈D\n \ninf\ny=(y1,y2,...,yd)∈M\nd\nX\ni=1\n|xi −yi|\n!#\n(167)\n(cf. Deﬁnitions 2.20 and 2.8).\nProof of Corollary 3.6. Observe that Proposition 3.5 and item (ii) in Lemma 3.2 ensure that there exists\nΦ ∈N such that\n(A) it holds that D(Φ) = l,\n(B) it holds that |||T (Φ)||| ≤max{1, L, supz∈M|||z|||, 2[supz∈M |f(z)|]}, and\n(C) it holds that\nsup\nx∈D\n\f\ff(x) −\n\u0000Rr(Φ)\n\u0001\n(x)\n\f\f ≤2L\n\"\nsup\nx=(x1,x2,...,xd)∈D\n \ninf\ny=(y1,y2,...,yd)∈M\nd\nX\ni=1\n|xi −yi|\n!#\n(168)\n(cf. Deﬁnitions 2.9, 2.11, 2.20, 2.4, and 2.10). Combining this with Corollary 2.15 establishes (167). The\nproof of Corollary 3.6 is thus completed.\nCorollary 3.7. Let d, d ∈N, L ∈R, u ∈[−∞, ∞), v ∈(u, ∞], let D ⊆Rd be a set, let f : D →[u, v]\nsatisfy for all x = (x1, x2, . . . , xd), y = (y1, y2, . . . , yd) ∈D that |f(x) −f(y)| ≤L\n\u0002Pd\ni=1 |xi −yi|\n\u0003\n, let\nM ⊆D satisfy |M| ∈{2, 3, . . .}, let l = (l0, l1, . . . , l|M|+1) ∈N|M|+2 satisfy l = (d, 2d|M|, 2|M|−1, 2|M|−\n3, . . . , 3, 1) and d ≥P|M|+1\nk=1\nlk(lk−1 +1). Then there exists θ ∈Rd such that |||θ||| ≤max{1, L, supz∈M|||z|||,\n2[supz∈M |f(z)|]} and\nsup\nx∈D\n\f\ff(x) −N θ,l\nu,v (x)\n\f\f ≤2L\n\"\nsup\nx=(x1,x2,...,xd)∈D\n \ninf\ny=(y1,y2,...,yd)∈M\nd\nX\ni=1\n|xi −yi|\n!#\n(169)\n(cf. Deﬁnitions 2.20 and 2.8).\n30\nProof of Corollary 3.7. First, observe that Corollary 3.6 (with d ←d, d ←d, L ←L, D ←D, f ←(D ∋\nx 7→f(x) ∈R), M ←M, l ←l in the notation of Corollary 3.6) ensures that there exists θ ∈Rd which\nsatisﬁes |||θ||| ≤max{1, L, supz∈M|||z|||, 2[supz∈M |f(z)|]} and\nsup\nx∈D\n\f\ff(x) −(N θ,l\n−∞,∞)(x)\n\f\f ≤2L\n\"\nsup\nx=(x1,x2,...,xd)∈D\n \ninf\ny=(y1,y2,...,yd)∈M\nd\nX\ni=1\n|xi −yi|\n!#\n(170)\n(cf. Deﬁnitions 2.20 and 2.8).\nThe assumption that for all x ∈D it holds that u ≤f(x) ≤v and\nLemma 2.33 hence imply that\nsup\nx∈D\n\f\ff(x) −N θ,l\nu,v (x)\n\f\f = sup\nx∈D\n\f\fCu,v,1(f(x)) −Cu,v,1\n\u0000(N θ,l\n−∞,∞)(x)\n\u0001\f\f\n≤sup\nx∈D\n\f\ff(x) −(N θ,l\n−∞,∞)(x)\n\f\f ≤2L\n\"\nsup\nx=(x1,x2,...,xd)∈D\n \ninf\ny=(y1,y2,...,yd)∈M\nd\nX\ni=1\n|xi −yi|\n!#\n(171)\n(cf. Deﬁnition 2.7). The proof of Corollary 3.7 is thus completed.\nCorollary 3.8. Let d, d, L ∈N, L ∈R, u ∈[−∞, ∞), v ∈(u, ∞], let D ⊆Rd be a set, let f : D →([u, v]∩\nR) satisfy for all x = (x1, x2, . . . , xd), y = (y1, y2, . . . , yd) ∈D that |f(x) −f(y)| ≤L\n\u0002Pd\ni=1 |xi −yi|\n\u0003\n, let\nM ⊆D satisfy |M| ∈{2, 3, . . .}, let l = (l0, l1, . . . , lL) ∈NL+1 satisfy for all k ∈{2, 3, . . ., |M|} that L ≥\n|M|+1, PL\ni=1 li(li−1+1) ≤d, l0 = d, lL = 1, l1 ≥2d|M|, and lk ≥2|M|−2k+3, and assume for all i ∈N∩\n(|M|, L) that li ≥2. Then there exists θ ∈Rd such that |||θ||| ≤max{1, L, supz∈M|||z|||, 2[supz∈M |f(z)|]}\nand\nsup\nx∈D\n\f\ff(x) −N θ,l\nu,v (x)\n\f\f ≤2L\n\"\nsup\nx=(x1,x2,...,xd)∈D\n \ninf\ny=(y1,y2,...,yd)∈M\nd\nX\ni=1\n|xi −yi|\n!#\n(172)\n(cf. Deﬁnitions 2.20 and 2.8).\nProof of Corollary 3.8. Throughout this proof let l = (l0, l1, . . . , l|M|+1) ∈N|M|+2 satisfy l = (d, 2d|M|,\n2|M|−1, 2|M|−3, . . . , 3, 1). First, note that Corollary 3.7 (with d ←d, d ←P|M|+1\nk=1\nlk(lk−1 +1), L ←L,\nu ←u, v ←v, D ←D, f ←f, M ←M, l ←l in the notation of Corollary 3.7) establishes that there\nexists η ∈R\nP|M|+1\nk=1\nlk(lk−1+1) which satisﬁes |||η||| ≤max{1, L, supz∈M|||z|||, 2[supz∈M |f(z)|]} and\nsup\nx∈D\n\f\ff(x) −N η,l\nu,v (x)\n\f\f ≤2L\n\"\nsup\nx=(x1,x2,...,xd)∈D\n \ninf\ny=(y1,y2,...,yd)∈M\nd\nX\ni=1\n|xi −yi|\n!#\n(173)\n(cf. Deﬁnitions 2.20 and 2.8). Next observe that Lemma 2.30 (with u ←u, v ←v, L ←|M| + 1, L ←L,\nd ←P|M|+1\nk=1\nlk(lk−1 + 1), d ←d, θ ←η, (l0, l1, . . . , lL) ←(l0, l1, . . . , l|M|+1), (l0, l1, . . . , lL) ←(l0, l1, . . . , lL),\nin the notation of Lemma 2.30) shows that there exists θ ∈Rd such that\n|||θ||| ≤max{1, |||η|||}\nand\nN θ,l\nu,v = N η,l\nu,v .\n(174)\nCombining this with (173) proves (172). The proof of Corollary 3.8 is thus completed.\nCorollary 3.9. Let d, d, N ∈N, L ∈R, u ∈[−∞, ∞), v ∈(u, ∞] satisfy d ≥2d2(N +1)d +5d(N +1)2d +\n4\n3(N + 1)3d, let ∥·∥: Rd →[0, ∞) be the standard norm, let p = (p1, p2, . . . , pd), q = (q1, q2, . . . , qd) ∈Rd\nsatisfy for all i ∈{1, 2, . . . , d} that pi ≤qi and maxj∈{1,2,...,d}(qj −pj) > 0, let D = Qd\ni=1[pi, qi], let M ⊆D\nsatisfy\nM =\n\u001a\ny = (y1, y2, . . . , yd) ∈Rd:\n\u0012\n∃k1, k2, . . . , kd ∈{0, 1, . . ., N}:\n∀i ∈{1, 2, . . ., d}: yi = pi + ki\nN (qi −pi)\n\u0013\u001b\n,\n(175)\n31\nand let f : D →([u, v]∩R) satisfy for all x, y ∈D that |f(x)−f(y)| ≤L∥x−y∥. Then there exist θ ∈Rd,\nL ∈N, l = (l0, l1, . . . , lL) ∈NL+1 such that |||θ||| ≤max{1, L, |||p|||, |||q|||, 2[supz∈D |f(z)|]}, PL\nk=1 lk(lk−1 +\n1) ≤d, and\nsup\nx∈D\n\f\ff(x) −N θ,l\nu,v (x)\n\f\f ≤L\nN\n\"\nd\nX\ni=1\n|qi −pi|\n#\n(176)\n(cf. Deﬁnitions 2.20 and 2.8).\nProof of Corollary 3.9. Throughout this proof let l = (l0, l1, . . . , l|M|+1) ∈N|M|+2 satisfy l = (d, 2d|M|,\n2|M| −1, 2|M| −3, . . . , 3, 1). Observe that the fact that |M| ≤(N + 1)d, the fact that for all n ∈N it\nholds that Pn\ni=1(2i −1) = n2, and the fact that for all n ∈N it holds that Pn\ni=1 i2 = n(n+1)(2n+1)\n6\n≤(n+1)3\n3\nensure that\n|M|+1\nX\nk=1\nlk(lk−1 + 1)\n= d(2d|M|) + 2d|M|(2|M| −1) +\n\"|M|−1\nX\ni=1\n(2i + 1)(2i −1)\n#\n|\n{z\n}\nnumber of weights\n+ 2d|M| +\n\"|M|\nX\ni=1\n(2i −1)\n#\n|\n{z\n}\nnumber of biases\n= 2d2|M| + 4d|M|2 −2d|M| +\n\"|M|−1\nX\ni=1\n(4i2 −1)\n#\n+ 2d|M| + |M|2\n= 2d2|M| + (4d + 1)|M|2 + 4\n\"|M|−1\nX\ni=1\ni2\n#\n−(|M| −1)\n≤2d2|M| + 5d|M|2 + 4\n3|M|3 ≤2d2(N + 1)d + 5d(N + 1)2d + 4\n3(N + 1)3d ≤d.\n(177)\nIn addition, note that the hypothesis that for all x, y ∈D it holds that |f(x) −f(y)| ≤L∥x −y∥implies\nthat for all x = (x1, x2, . . . , xd), y = (y1, y2, . . . , yd) ∈D it holds that\n|f(x) −f(y)| ≤L\n\"\nd\nX\ni=1\n|xi −yi|\n#\n.\n(178)\nFurthermore, observe that the hypothesis that maxj∈{1,2,...,d}(qj −pj) > 0 ensures that |M| ≥2. Com-\nbining this, (177), and (178) with Corollary 3.7 establishes that there exists θ ∈Rd such that |||θ||| ≤\nmax{1, L, supz∈M|||z|||, 2[supz∈M |f(z)|]} and\nsup\nx∈D\n\f\ff(x) −N θ,l\nu,v (x)\n\f\f ≤2L\n\"\nsup\nx=(x1,x2,...,xd)∈D\n \ninf\ny=(y1,y2,...,yd)∈M\nd\nX\ni=1\n|xi −yi|\n!#\n(179)\n(cf. Deﬁnitions 2.20 and 2.8). Next note that the hypothesis that M ⊆D = Qd\ni=1[pi, qi] implies that for\nall z ∈M it holds that\n|||z||| ≤max{|||p|||, |||q|||}.\n(180)\nTherefore, we obtain that\n|||θ||| ≤max\n\u001a\n1, L, |||p|||, |||q|||, 2\n\u0014\nsup\nz∈M\n|f(z)|\n\u0015\u001b\n≤max\n\u001a\n1, L, |||p|||, |||q|||, 2\n\u0014\nsup\nz∈D\n|f(z)|\n\u0015\u001b\n.\n(181)\nIn the next step we note that the fact that for all N ∈N, r ∈R, s ∈[r, ∞), x ∈[r, s] there exists\nk ∈{0, 1, . . . , N} such that |x −(r + k\nN (s −r))| ≤s−r\n2N ensures that for all x = (x1, x2, . . . , xd) ∈D there\nexists y = (y1, y2, . . . , yd) ∈M such that\nd\nX\ni=1\n|xi −yi| ≤\n1\n2N\n\"\nd\nX\ni=1\n|qi −pi|\n#\n.\n(182)\nCombining this, (177), (179), and (181) establishes (176). The proof of Corollary 3.9 is thus completed.\n32\n3.2\nAnalysis of the generalization error\n3.2.1\nHoeﬀding’s concentration inequality\nProposition 3.10. Let (Ω, F, P) be a probability space, let N ∈N, ε ∈[0, ∞), a1, a2, . . . , aN ∈R,\nb1 ∈[a1, ∞), b2 ∈[a2, ∞), . . . , bN ∈[aN, ∞), assume PN\nn=1(bn −an)2 ̸= 0, and let Xn : Ω→[an, bn],\nn ∈{1, 2, . . . , N}, be independent random variables. Then\nP\n \n1\nN\n\f\f\f\f\f\nN\nX\nn=1\n\u0000Xn −E[Xn]\n\u0001\n\f\f\f\f\f ≥ε\n!\n≤2 exp\n \n−2ε2N2\nPN\nn=1(bn −an)2\n!\n.\n(183)\n3.2.2\nCovering number estimates\nDeﬁnition 3.11 (Covering number). Let (E, δ) be a metric space and let r ∈[0, ∞]. Then we denote by\nC(E,δ),r ∈N0 ∪{∞} (we denote by CE,r ∈N0 ∪{∞}) the extended real number given by\nC(E,δ),r = inf\n\u0010n\nn ∈N0 :\n\u0000∃A ⊆E :\n\u0002\n(|A| ≤n) ∧(∀x ∈E : ∃a ∈A: δ(a, x) ≤r)\n\u0003\u0001o\n∪{∞}\n\u0011\n.\n(184)\nProposition 3.12. Let (X, ∥·∥) be a ﬁnite-dimensional Banach space, let R, r ∈(0, ∞), B = {θ ∈\nX : ∥θ∥≤R}, and let δ: B × B →[0, ∞) satisfy for all θ, ϑ ∈B that δ(θ, ϑ) = ∥θ −ϑ∥. Then\nC(B,δ),r ≤\n(\n1\n: r ≥R\n\u00024R\nr\n\u0003dim(X)\n: r < R\n(185)\n(cf. Deﬁnition 3.11).\n3.2.3\nMeasurability properties for suprema\nLemma 3.13. Let (E, E ) be a topological space, assume E ̸= ∅, let E ⊆E be an at most countable\nset, assume that E is dense in E, let (Ω, F) be a measurable space, let fx : Ω→R, x ∈E, be F/B(R)-\nmeasurable functions, assume for all ω ∈Ωthat E ∋x 7→fx(ω) ∈R is a continuous function, and let\nF : Ω→R ∪{∞} satisfy for all ω ∈Ωthat F(ω) = supx∈E fx(ω). Then\n(i) it holds for all ω ∈Ωthat F(ω) = supx∈E fx(ω) and\n(ii) it holds that F is an F/B(R ∪{∞})-measurable function.\nProof of Lemma 3.13. Note that the hypothesis that E is dense in E implies that for all g ∈C(E, R) it\nholds that\nsup\nx∈E\ng(x) = sup\nx∈E\ng(x).\n(186)\nThis and the hypothesis that for all ω ∈Ωit holds that E ∋x 7→fx(ω) ∈R is a continuous function\nshow that for all ω ∈Ωit holds that\nF(ω) = sup\nx∈E\nfx(ω) = sup\nx∈E\nfx(ω).\n(187)\nThis establishes item (i). Next note that item (i) and the hypothesis that for all x ∈E it holds that\nfx : Ω→R is an F/B(R)-measurable function demonstrate item (ii). The proof of Lemma 3.13 is thus\ncompleted.\nLemma 3.14. Let (E, δ) be a separable metric space, assume E ̸= ∅, let (Ω, F, P) be a probability space,\nlet L ∈R, and let Zx : Ω→R, x ∈E, be random variables which satisfy for all x, y ∈E that E[|Zx|] < ∞\nand |Zx −Zy| ≤Lδ(x, y). Then\n33\n(i) it holds for all x, y ∈E, η ∈Ωthat |(Zx(η) −E[Zx]) −(Zy(η) −E[Zy])| ≤2Lδ(x, y) and\n(ii) it holds that Ω∋η 7→supx∈E|Zx(η) −E[Zx]| ∈[0, ∞] is an F/B([0, ∞])-measurable function.\nProof of Lemma 3.14. Note that the hypothesis that for all x, y ∈E it holds that |Zx −Zy| ≤Lδ(x, y)\nshows that for all x, y ∈E, η ∈Ωit holds that\n|(Zx(η) −E[Zx]) −(Zy(η) −E[Zy])| = |(Zx(η) −Zy(η)) + (E[Zy] −E[Zx])|\n≤|Zx(η) −Zy(η)| + |E[Zx] −E[Zy]| ≤Lδ(x, y) + |E[Zx] −E[Zy]|\n= Lδ(x, y) + |E[Zx −Zy]| ≤Lδ(x, y) + E[|Zx −Zy|] ≤Lδ(x, y) + Lδ(x, y) = 2Lδ(x, y).\n(188)\nThis proves item (i).\nNext observe that item (i) implies that for all η ∈Ωit holds that E ∋x 7→\n|Zx(η) −E[Zx]| ∈R is a continuous function. Combining this and the hypothesis that E is separable with\nLemma 3.13 establishes item (ii). The proof of Lemma 3.14 is thus completed.\n3.2.4\nConcentration inequalities for random ﬁelds\nLemma 3.15. Let (E, δ) be a separable metric space, let ε, L ∈R, N ∈N, z1, z2, . . . , zN ∈E satisfy\nE ⊆SN\ni=1{x ∈E : 2Lδ(x, zi) ≤ε}, let (Ω, F, P) be a probability space, and let Zx : Ω→R, x ∈E, be\nrandom variables which satisfy for all x, y ∈E that |Zx −Zy| ≤Lδ(x, y). Then\nP(supx∈E|Zx| ≥ε) ≤\nN\nX\ni=1\nP\n\u0000|Zzi| ≥ε\n2\n\u0001\n(189)\n(cf. Lemma 3.13).\nProof of Lemma 3.15. Throughout this proof let B1, B2, . . . , BN ⊆E satisfy for all i ∈{1, 2, . . ., N} that\nBi = {x ∈E : 2Lδ(x, zi) ≤ε}. Observe that the triangle inequality and the hypothesis that for all\nx, y ∈E it holds that |Zx −Zy| ≤Lδ(x, y) show that for all i ∈{1, 2, . . . , N}, x ∈Bi it holds that\n|Zx| = |Zx −Zzi + Zzi| ≤|Zx −Zzi| + |Zzi| ≤Lδ(x, zi) + |Zzi| ≤ε\n2 + |Zzi|.\n(190)\nCombining this with Lemma 3.13 proves that for all i ∈{1, 2, . . . , N} it holds that\nP\n\u0000supx∈Bi|Zx| ≥ε\n\u0001\n≤P\n\u0000 ε\n2 + |Zzi| ≥ε\n\u0001\n= P\n\u0000|Zzi| ≥ε\n2\n\u0001\n.\n(191)\nThis and Lemma 3.13 establish that\nP(supx∈E|Zx| ≥ε) = P\n\u0010\nsupx∈(\nSN\ni=1 Bi)|Zx| ≥ε\n\u0011\n= P\n\u0010SN\ni=1\n\b\nsupx∈Bi|Zx| ≥ε\n\t\u0011\n≤\nN\nX\ni=1\nP\n\u0000supx∈Bi|Zx| ≥ε\n\u0001\n≤\nN\nX\ni=1\nP\n\u0000|Zzi| ≥ε\n2\n\u0001\n.\n(192)\nThis completes the proof of Lemma 3.15.\nLemma 3.16. Let (E, δ) be a separable metric space, assume E ̸= ∅, let ε, L ∈(0, ∞), let (Ω, F, P) be\na probability space, and let Zx : Ω→R, x ∈E, be random variables which satisfy for all x, y ∈E that\n|Zx −Zy| ≤Lδ(x, y). Then\n\u0002\nC(E,δ), ε\n2L\n\u0003−1P(supx∈E|Zx| ≥ε) ≤supx∈E P\n\u0000|Zx| ≥ε\n2\n\u0001\n.\n(193)\n(cf. Deﬁnition 3.11 and Lemma 3.13).\n34\nProof of Lemma 3.16. Throughout this proof let N ∈N ∪{∞} satisfy N = C(E,δ), ε\n2L, assume without\nloss of generality that N < ∞, and let z1, z2, . . . , zN ∈E satisfy E ⊆SN\ni=1{x ∈E : δ(x, zi) ≤\nε\n2L} (cf.\nDeﬁnition 3.11). Observe that Lemma 3.13 and Lemma 3.15 establish that\nP(supx∈E|Zx| ≥ε) ≤\nN\nX\ni=1\nP\n\u0000|Zzi| ≥ε\n2\n\u0001\n≤N\n\u0002\nsupx∈E P\n\u0000|Zx| ≥ε\n2\n\u0001\u0003\n.\n(194)\nThis completes the proof of Lemma 3.16.\nLemma 3.17. Let (E, δ) be a separable metric space, assume E ̸= ∅, let ε, L ∈(0, ∞), let (Ω, F, P) be\na probability space, and let Zx : Ω→R, x ∈E, be random variables which satisfy for all x, y ∈E that\nE[|Zx|] < ∞and |Zx −Zy| ≤Lδ(x, y). Then\n\u0002\nC(E,δ), ε\n4L\n\u0003−1P(supx∈E|Zx −E[Zx]| ≥ε) ≤supx∈E P\n\u0000|Zx −E[Zx]| ≥ε\n2\n\u0001\n.\n(195)\n(cf. Deﬁnition 3.11 and Lemma 3.14).\nProof of Lemma 3.17. Throughout this proof let Yx : Ω→R, x ∈E, satisfy for all x ∈E, η ∈Ωthat\nYx(η) = Zx(η) −E[Zx]. Observe that Lemma 3.14 ensures that for all x, y ∈E it holds that\n|Yx −Yy| ≤2Lδ(x, y).\n(196)\nThis and Lemma 3.16 (with (E, δ) ←(E, δ), ε ←ε, L ←2L, (Ω, F, P) ←(Ω, F, P), (Zx)x∈E ←(Yx)x∈E\nin the notation of Lemma 3.16) establish (195). The proof of Lemma 3.17 is thus completed.\nLemma 3.18. Let (E, δ) be a separable metric space, assume E ̸= ∅, let M ∈N, ε, L, D ∈(0, ∞),\nlet (Ω, F, P) be a probability space, for every x ∈E let Yx,1, Yx,2, . . . , Yx,M : Ω→[0, D] be independent\nrandom variables, assume for all x, y ∈E, m ∈{1, 2, . . . , M} that |Yx,m −Yy,m| ≤Lδ(x, y), and let\nZx : Ω→[0, ∞), x ∈E, satisfy for all x ∈E that\nZx = 1\nM\n\" M\nX\nm=1\nYx,m\n#\n.\n(197)\nThen\n(i) it holds for all x ∈E that E[|Zx|] ≤D < ∞,\n(ii) it holds that Ω∋η 7→supx∈E|Zx(η) −E[Zx]| ∈[0, ∞] is an F/B([0, ∞])-measurable function, and\n(iii) it holds that\nP(supx∈E|Zx −E[Zx]| ≥ε) ≤2C(E,δ), ε\n4L exp\n\u0012−ε2M\n2D2\n\u0013\n(198)\n(cf. Deﬁnition 3.11).\nProof of Lemma 3.18. First, observe that the triangle inequality and the hypothesis that for all x, y ∈E,\nm ∈{1, 2, . . . , M} it holds that |Yx,m −Yy,m| ≤Lδ(x, y) imply that for all x, y ∈E it holds that\n|Zx −Zy| =\n\f\f\f\f\f\n1\nM\n\" M\nX\nm=1\nYx,m\n#\n−1\nM\n\" M\nX\nm=1\nYy,m\n#\f\f\f\f\f = 1\nM\n\f\f\f\f\f\nM\nX\nm=1\n\u0000Yx,m −Yy,m\n\u0001\n\f\f\f\f\f\n≤1\nM\n\" M\nX\nm=1\n\f\fYx,m −Yy,m\n\f\f\n#\n≤Lδ(x, y).\n(199)\n35\nNext note that the hypothesis that for all x ∈E, m ∈{1, 2, . . . , M}, ω ∈Ωit holds that |Yx,m(ω)| ∈[0, D]\nensures that for all x ∈E it holds that\nE\n\u0002\n|Zx|\n\u0003\n= E\n\"\n1\nM\n\" M\nX\nm=1\nYx,m\n##\n= 1\nM\n\" M\nX\nm=1\nE\n\u0002\nYx,m\n\u0003\n#\n≤D < ∞.\n(200)\nThis proves item (i). Furthermore, note that item (i), (199), and Lemma 3.14 establish item (ii). Next\nobserve that (197) shows that for all x ∈E it holds that\n|Zx −E[Zx]| =\n\f\f\f\f\f\n1\nM\n\" M\nX\nm=1\nYx,m\n#\n−E\n\"\n1\nM\n\" M\nX\nm=1\nYx,m\n##\f\f\f\f\f = 1\nM\n\f\f\f\f\f\nM\nX\nm=1\n\u0000Yx,m −E\n\u0002\nYx,m\n\u0003\u0001\n\f\f\f\f\f .\n(201)\nCombining this with Proposition 3.10 (with (Ω, F, P) ←(Ω, F, P), N ←M, ε ←ε\n2, (a1, a2, . . . , aN) ←\n(0, 0, . . . , 0), (b1, b2, . . . , bN) ←(D, D, . . . , D), (Xn)n∈{1,2,...,N} ←(Yx,m)m∈{1,2,...,M} for x ∈E in the nota-\ntion of Proposition 3.10) ensures that for all x ∈E it holds that\nP\n\u0000|Zx −E[Zx]| ≥ε\n2\n\u0001\n≤2 exp\n \n−2\n\u0002 ε\n2\n\u00032M2\nMD2\n!\n= 2 exp\n\u0012−ε2M\n2D2\n\u0013\n.\n(202)\nCombining this, (199), and (200) with Lemma 3.17 establishes item (iii). The proof of Lemma 3.18 is\nthus completed.\n3.2.5\nUniform estimates for the statistical learning error\nLemma 3.19. Let (E, δ) be a separable metric space, assume E ̸= ∅, let M ∈N, ε, L, D ∈(0, ∞), let\n(Ω, F, P) be a probability space, let Xx,m: Ω→R, x ∈E, m ∈{1, 2, . . . , M}, and Ym: Ω→R, m ∈\n{1, 2, . . . , M}, be functions, assume for all x ∈E that (Xx,m, Ym), m ∈{1, 2, . . . , M}, are i.i.d. random\nvariables, assume for all x, y ∈E, m ∈{1, 2, . . . , M} that |Xx,m −Xy,m| ≤Lδ(x, y) and |Xx,m −Ym| ≤D,\nlet Ex : Ω→[0, ∞), x ∈E, satisfy for all x ∈E that\nEx = 1\nM\n\" M\nX\nm=1\n|Xx,m −Ym|2\n#\n,\n(203)\nand let Ex ∈[0, ∞), x ∈E, satisfy for all x ∈E that Ex = E[|Xx,1 −Y1|2]. Then Ω∋ω 7→supx∈E|Ex(ω)−\nEx| ∈[0, ∞] is an F/B([0, ∞])-measurable function and\nP(supx∈E|Ex −Ex| ≥ε) ≤2C(E,δ),\nε\n8LD exp\n\u0012−ε2M\n2D4\n\u0013\n(204)\n(cf. Deﬁnition 3.11).\nProof of Lemma 3.19. Throughout this proof let Ex,m: Ω→[0, D2], x ∈E, m ∈{1, 2, . . . , M}, satisfy for\nall x ∈E, m ∈{1, 2, . . . , M} that\nEx,m = |Xx,m −Ym|2.\n(205)\nObserve that the fact that for all x1, x2, y ∈R it holds that (x1−y)2−(x2−y)2 = (x1−x2)((x1−y)+(x2−y)),\nthe hypothesis that for all x ∈E, m ∈{1, 2, . . ., M} it holds that |Xx,m −Ym| ≤D, and the hypothesis\nthat for all x, y ∈E, m ∈{1, 2, . . . , M} it holds that |Xx,m −Xy,m| ≤Lδ(x, y) imply that for all x, y ∈E,\nm ∈{1, 2, . . . , M} it holds that\n|Ex,m −Ey,m| =\n\f\f(Xx,m −Ym)2 −(Xy,m −Ym)2\f\f = |Xx,m −Xy,m|\n\f\f(Xx,m −Ym) + (Xy,m −Ym)\n\f\f\n≤|Xx,m −Xy,m|\n\u0000|Xx,m −Ym| + |Xy,m −Ym|\n\u0001\n≤2D|Xx,m −Xy,m| ≤2LDδ(x, y).\n(206)\n36\nIn addition, note that (203) and the hypothesis that for all x ∈E it holds that (Xx,m, Ym), m ∈\n{1, 2, . . . , M}, are i.i.d. random variables show that for all x ∈E it holds that\nE\n\u0002\nEx\n\u0003\n= 1\nM\n\" M\nX\nm=1\nE\n\u0002\n|Xx,m −Ym|2\u0003\n#\n= 1\nM\n\" M\nX\nm=1\nE\n\u0002\n|Xx,1 −Y1|2\u0003\n#\n= 1\nM\n\" M\nX\nm=1\nEx\n#\n= Ex.\n(207)\nFurthermore, observe that the hypothesis that for all x ∈E it holds that (Xx,m, Ym), m ∈{1, 2, . . . , M},\nare i.i.d. random variables ensures that for all x ∈E it holds that Ex,m, m ∈{1, 2, . . . , M}, are i.i.d.\nrandom variables. Combining this, (206), and (207) with Lemma 3.18 (with (E, δ) ←(E, δ), M ←M,\nε ←ε, L ←2LD, D ←D2, (Ω, F, P) ←(Ω, F, P), (Yx,m)x∈E, m∈{1,2,...,M} ←(Ex,m)x∈E, m∈{1,2,...,M},\n(Zx)x∈E = (Ex)x∈E in the notation of Lemma 3.18) establishes (204). The proof of Lemma 3.19 is thus\ncompleted.\nLemma 3.20. Let d, d, M ∈N, R, L, R, ε ∈(0, ∞), let D ⊆Rd be a compact set, let (Ω, F, P) be a\nprobability space, let Xm: Ω→D, m ∈{1, 2, . . . , M}, and Ym: Ω→R, m ∈{1, 2, . . . , M}, be functions,\nassume that (Xm, Ym), m ∈{1, 2, . . . , M}, are i.i.d. random variables, let H = (Hθ)θ∈[−R,R]d : [−R, R]d →\nC(D, R) satisfy for all θ, ϑ ∈[−R, R]d, x ∈D that |Hθ(x) −Hϑ(x)| ≤L|||θ −ϑ|||, assume for all θ ∈\n[−R, R]d, m ∈{1, 2, . . . , M} that |Hθ(Xm) −Ym| ≤R and E[|Y1|2] < ∞, let E : C(D, R) →[0, ∞) satisfy\nfor all f ∈C(D, R) that E(f) = E[|f(X1) −Y1|2], and let E: [−R, R]d × Ω→[0, ∞) satisfy for all\nθ ∈[−R, R]d, ω ∈Ωthat\nE(θ, ω) = 1\nM\n\" M\nX\nm=1\n|Hθ(Xm(ω)) −Ym(ω)|2\n#\n(208)\n(cf. Deﬁnition 2.20). Then Ω∋ω 7→supθ∈[−R,R]d|E(θ, ω) −E(Hθ)| ∈[0, ∞] is an F/B([0, ∞])-measurable\nfunction and\nP\n\u0000supθ∈[−R,R]d|E(θ) −E(Hθ)| ≥ε\n\u0001\n≤2 max\n\u001a\n1,\n\u001432LRR\nε\n\u0015d\u001b\nexp\n\u0012−ε2M\n2R4\n\u0013\n.\n(209)\nProof of Lemma 3.20. Throughout this proof let B ⊆Rd satisfy B = [−R, R]d = {θ ∈Rd : |||θ||| ≤R}\nand let δ: B × B →[0, ∞) satisfy for all θ, ϑ ∈B that\nδ(θ, ϑ) = |||θ −ϑ|||.\n(210)\nObserve that the hypothesis that (Xm, Ym), m ∈{1, 2, . . . , M}, are i.i.d. random variables and the hy-\npothesis that for all θ ∈[−R, R]d it holds that Hθ is a continuous function imply that for all θ ∈B it holds\nthat (Hθ(Xm), Ym), m ∈{1, 2, . . . , M}, are i.i.d. random variables. Combining this, the hypothesis that\nfor all θ, ϑ ∈B, x ∈D it holds that |Hθ(x) −Hϑ(x)| ≤L|||θ −ϑ|||, and the hypothesis that for all θ ∈B,\nm ∈{1, 2, . . ., M} it holds that |Hθ(Xm) −Ym| ≤R with Lemma 3.19 (with (E, δ) ←(B, δ), M ←M,\nε ←ε, L ←L, D ←R, (Ω, F, P) ←(Ω, F, P), (Xx,m)x∈E, m∈{1,2,...,M} ←(Hθ(Xm))θ∈B, m∈{1,2,...,M},\n(Ym)m∈{1,2,...,M} ←(Ym)m∈{1,2,...,M}, (Ex)x∈E ←\n\u0000(Ω∋ω 7→E(θ, ω) ∈[0, ∞))\n\u0001\nθ∈B, (Ex)x∈E ←(E(Hθ))θ∈B\nin the notation of Lemma 3.19) establishes that Ω∋ω 7→supθ∈B|E(θ, ω) −E(Hθ)| ∈[0, ∞] is an\nF/B([0, ∞])-measurable function and\nP\n\u0000supθ∈B|E(θ) −E(Hθ)| ≥ε\n\u0001\n≤2C(B,δ),\nε\n8LR exp\n\u0012−ε2M\n2R4\n\u0013\n(211)\n(cf. Deﬁnition 3.11). Moreover, note that Proposition 3.12 (with X ←Rd, ∥·∥←(Rd ∋x 7→|||x||| ∈\n[0, ∞)), R ←R, r ←\nε\n8LR, B ←B, δ ←δ in the notation of Proposition 3.12) demonstrates that\nC(B,δ),\nε\n8LR ≤max\n\u001a\n1,\n\u001432LRR\nε\n\u0015d\u001b\n.\n(212)\nThis and (211) prove (209). The proof of Lemma 3.20 is thus completed.\n37\nLemma 3.21. Let d, M, L ∈N, u ∈R, v ∈(u, ∞), R ∈[1, ∞), ε, b ∈(0, ∞), l = (l0, l1, . . . , lL) ∈NL+1\nsatisfy lL = 1 and PL\nk=1 lk(lk−1 + 1) ≤d, let D ⊆[−b, b]l0 be a compact set, let (Ω, F, P) be a probability\nspace, let Xm: Ω→D, m ∈{1, 2, . . . , M}, and Ym: Ω→[u, v], m ∈{1, 2, . . ., M}, be functions, assume\nthat (Xm, Ym), m ∈{1, 2, . . . , M}, are i.i.d. random variables, let E : C(D, R) →[0, ∞) satisfy for all\nf ∈C(D, R) that E(f) = E[|f(X1) −Y1|2], and let E: [−R, R]d × Ω→[0, ∞) satisfy for all θ ∈[−R, R]d,\nω ∈Ωthat\nE(θ, ω) = 1\nM\n\" M\nX\nm=1\n|N θ,l\nu,v (Xm(ω)) −Ym(ω)|2\n#\n(213)\n(cf. Deﬁnitions 2.20 and 2.8). Then Ω∋ω 7→supθ∈[−R,R]d\n\f\fE(θ, ω)−E\n\u0000N θ,l\nu,v |D\n\u0001\f\f ∈[0, ∞] is an F/B([0, ∞])-\nmeasurable function and\nP\n\u0000supθ∈[−R,R]d\n\f\fE(θ) −E\n\u0000N θ,l\nu,v |D\n\u0001\f\f ≥ε\n\u0001\n≤2 max\n\u001a\n1,\n\u001432L max{1, b}(|||l||| + 1)LRL(v −u)\nε\n\u0015d\u001b\nexp\n\u0012 −ε2M\n2(v −u)4\n\u0013\n.\n(214)\nProof of Lemma 3.21. Throughout this proof let L ∈(0, ∞) satisfy\nL = L max{1, b} (|||l||| + 1)LRL−1.\n(215)\nObserve that Corollary 2.37 (with a ←−b, b ←b, u ←u, v ←v, d ←d, L ←L, l ←l in the notation of\nCorollary 2.37) and the hypothesis that D ⊆[−b, b]l0 show that for all θ, ϑ ∈[−R, R]d it holds that\nsup\nx∈D\n|N θ,l\nu,v (x) −N ϑ,l\nu,v (x)| ≤\nsup\nx∈[−b,b]l0\n|N θ,l\nu,v (x) −N ϑ,l\nu,v (x)|\n≤L max{1, b} (|||l||| + 1)L (max{1, |||θ|||, |||ϑ|||})L−1|||θ −ϑ|||\n≤L max{1, b} (|||l||| + 1)LRL−1|||θ −ϑ||| = L|||θ −ϑ|||.\n(216)\nFurthermore, observe that the fact that for all θ ∈Rd, x ∈Rl0 it holds that N θ,l\nu,v (x) ∈[u, v] and the\nhypothesis that for all m ∈{1, 2, . . . , M}, ω ∈Ωit holds that Ym(ω) ∈[u, v] demonstrate that for all\nθ ∈[−R, R]d, m ∈{1, 2, . . . , M} it holds that\n|N θ,l\nu,v (Xm) −Ym| ≤v −u.\n(217)\nCombining this and (216) with Lemma 3.20 (with d ←l0, d ←d, M ←M, R ←R, L ←L, R ←v −u,\nε ←ε, D ←D, (Ω, F, P) ←(Ω, F, P), (Xm)m∈{1,2,...,M} ←(Xm)m∈{1,2,...,M}, (Ym)m∈{1,2,...,M} ←((Ω∋ω 7→\nYm(ω) ∈R))m∈{1,2,...,M}, H ←([−R, R]d ∋θ 7→N θ,l\nu,v |D ∈C(D, R)), E ←E, E ←E in the notation of\nLemma 3.20) establishes that Ω∋ω 7→supθ∈[−R,R]d\n\f\fE(θ, ω) −E\n\u0000N θ,l\nu,v |D\n\u0001\f\f ∈[0, ∞] is an F/B([0, ∞])-\nmeasurable function and\nP\n\u0000supθ∈[−R,R]d\n\f\fE(θ) −E\n\u0000N θ,l\nu,v |D\n\u0001\f\f ≥ε\n\u0001\n≤2 max\n\u001a\n1,\n\u001432LR(v −u)\nε\n\u0015d\u001b\nexp\n\u0012 −ε2M\n2(v −u)4\n\u0013\n.\n(218)\nThe proof of Lemma 3.21 is thus completed.\n3.3\nAnalysis of the optimization error\n3.3.1\nConvergence rates for the minimum Monte Carlo method\nLemma 3.22. Let (Ω, F, P) be a probability space, let d, N ∈N, let ∥·∥: Rd →[0, ∞) be a norm, let\nH ⊆Rd be a set, let ϑ ∈H, L, ε ∈(0, ∞), let E: H × Ω→R be a (B(H) ⊗F)/B(R)-measurable function,\nassume for all x, y ∈H, ω ∈Ωthat |E(x, ω)−E(y, ω)| ≤L∥x−y∥, and let Θn : Ω→H, n ∈{1, 2, . . . , N},\nbe i.i.d. random variables. Then\nP\n\u0000\u0002\nminn∈{1,2,...,N} E(Θn)\n\u0003\n−E(ϑ) > ε\n\u0001\n≤\n\u0002\nP\n\u0000∥Θ1 −ϑ∥> ε\nL\n\u0001\u0003N ≤exp\n\u0000−N P\n\u0000∥Θ1 −ϑ∥≤ε\nL\n\u0001\u0001\n.\n(219)\n38\nProof of Lemma 3.22. Note that the hypothesis that for all x, y ∈H, ω ∈Ωit holds that |E(x, ω) −\nE(y, ω)| ≤L∥x −y∥implies that\n\u0002\nminn∈{1,2,...,N} E(Θn)\n\u0003\n−E(ϑ) = minn∈{1,2,...,N} [E(Θn) −E(ϑ)]\n≤minn∈{1,2,...,N} |E(Θn) −E(ϑ)| ≤minn∈{1,2,...,N}\n\u0002\nL∥Θn −ϑ∥\n\u0003\n= L\n\u0002\nminn∈{1,2,...,N} ∥Θn −ϑ∥\n\u0003\n.\n(220)\nThe hypothesis that Θn, n ∈{1, 2, . . . , N}, are i.i.d. random variables and the fact that ∀x ∈R: 1 −x ≤\ne−x hence show that\nP\n\u0010\u0002\nminn∈{1,2,...,N} E(Θn)\n\u0003\n−E(ϑ) > ε\n\u0011\n≤P\n\u0010\nL\n\u0002\nminn∈{1,2,...,N} ∥Θn −ϑ∥\n\u0003\n> ε\n\u0011\n= P\n\u0000minn∈{1,2,...,N} ∥Θn −ϑ∥> ε\nL\n\u0001\n=\n\u0002\nP\n\u0000∥Θ1 −ϑ∥> ε\nL\n\u0001\u0003N\n=\n\u0002\n1 −P\n\u0000∥Θ1 −ϑ∥≤ε\nL\n\u0001\u0003N ≤exp\n\u0000−N P\n\u0000∥Θ1 −ϑ∥≤ε\nL\n\u0001\u0001\n.\n(221)\nThe proof of Lemma 3.22 is thus completed.\n3.3.2\nContinuous uniformly distributed samples\nLemma 3.23. Let (Ω, F, P) be a probability space, let d, N ∈N, a ∈R, b ∈(a, ∞), ϑ ∈[a, b]d, L, ε ∈\n(0, ∞), let E: [a, b]d ×Ω→R be a (B([a, b]d) ⊗F)/B(R)-measurable function, assume for all x, y ∈[a, b]d,\nω ∈Ωthat |E(x, ω) −E(y, ω)| ≤L|||x −y|||, let Θn : Ω→[a, b]d, n ∈{1, 2, . . . , N}, be i.i.d. random\nvariables, and assume that Θ1 is continuous uniformly distributed on [a, b]d (cf. Deﬁnition 2.20). Then\nP\n\u0000\u0002\nminn∈{1,2,...,N} E(Θn)\n\u0003\n−E(ϑ) > ε\n\u0001\n≤exp\n\u0012\n−N min\n\u001a\n1,\nεd\nLd(b −a)d\n\u001b\u0013\n.\n(222)\nProof of Lemma 3.23. Note that the hypothesis that Θ1 is continuous uniformly distributed on [a, b]d\nensures that\nP\n\u0000|||Θ1 −ϑ||| ≤ε\nL\n\u0001\n≥P\n\u0000|||Θ1 −(a, a, . . . , a)||| ≤ε\nL\n\u0001\n= P\n\u0000|||Θ1 −(a, a, . . . , a)||| ≤min{ ε\nL, b −a}\n\u0001\n=\n\u0014min{ ε\nL, b −a}\n(b −a)\n\u0015d\n= min\n(\n1,\n\u0014\nε\nL (b −a)\n\u0015d)\n.\n(223)\nCombining this with Lemma 3.22 proves (222). The proof of Lemma 3.23 is thus completed.\n4\nOverall error analysis\nIn this section we combine the separate error analyses of the approximation error, the generalization error,\nand the optimization error in Section 3 to obtain an overall analysis (cf. Theorem 4.5 below). We note that,\ne.g., [6, Lemma 2.4] ensures that the integral appearing on the left-hand side of (238) in Theorem 4.5\nand subsequent results (cf. (251) in Corollary 4.6, (259) in Corollary 4.7, (269) in Corollary 4.8, and\n(274) in Corollary 4.10) is indeed measurable.\nIn Lemma 4.1 below we present the well-known bias-\nvariance decomposition result. To formulate this bias-variance decomposition lemma we observe that\nfor every probability space (Ω, F, P), every measurable space (S, S), every random variable X : Ω→S,\nand every A ∈S it holds that PX(A) = P(X ∈A). Moreover, note that for every probability space\n(Ω, F, P), every measurable space (S, S), every random variable X : Ω→S, and every S/B(R)-measurable\nfunction f : S →R it holds that\nR\nS |f|2 dPX =\nR\nS |f(x)|2 PX(dx) =\nR\nΩ|f(X(ω))|2 P(dω) =\nR\nΩ|f(X)|2 dP =\nE\n\u0002\n|f(X)|2\u0003\n. A result related to Lemmas 4.1 and 4.2 can, e.g., be found in Berner et al. [10, Lemma 2.8].\n39\n4.1\nBias-variance decomposition\nLemma 4.1 (Bias-variance decomposition). Let (Ω, F, P) be a probability space, let (S, S) be a measurable\nspace, let X : Ω→S and Y : Ω→R be random variables with E[|Y |2] < ∞, and let E : L2(PX; R) →[0, ∞)\nsatisfy for all f ∈L2(PX; R) that E(f) = E[|f(X) −Y |2]. Then\n(i) it holds for all f ∈L2(PX; R) that\nE(f) = E\n\u0002\n|f(X) −E[Y |X]|2\u0003\n+ E\n\u0002\n|Y −E[Y |X]|2\u0003\n,\n(224)\n(ii) it holds for all f, g ∈L2(PX; R) that\nE(f) −E(g) = E\n\u0002\n|f(X) −E[Y |X]|2\u0003\n−E\n\u0002\n|g(X) −E[Y |X]|2\u0003\n,\n(225)\nand\n(iii) it holds for all f, g ∈L2(PX; R) that\nE\n\u0002\n|f(X) −E[Y |X]|2\u0003\n= E\n\u0002\n|g(X) −E[Y |X]|2\u0003\n+\n\u0000E(f) −E(g)\n\u0001\n.\n(226)\nProof of Lemma 4.1. First, observe that the hypothesis that for all f ∈L2(PX; R) it holds that E(f) =\nE[|f(X) −Y |2] shows that for all f ∈L2(PX; R) it holds that\nE(f) = E\n\u0002\n|f(X) −Y |2\u0003\n= E\n\u0002\n|(f(X) −E[Y |X]) + (E[Y |X] −Y )|2\u0003\n= E\n\u0002\n|f(X) −E[Y |X]|2\u0003\n+ 2 E\n\u0002\u0000f(X) −E[Y |X]\n\u0001\u0000E[Y |X] −Y\n\u0001\u0003\n+ E\n\u0002\n|E[Y |X] −Y |2\u0003\n= E\n\u0002\n|f(X) −E[Y |X]|2\u0003\n+ 2 E\nh\nE\n\u0002\u0000f(X) −E[Y |X]\n\u0001\u0000E[Y |X] −Y\n\u0001\f\fX\n\u0003i\n+ E\n\u0002\n|E[Y |X] −Y |2\u0003\n= E\n\u0002\n|f(X) −E[Y |X]|2\u0003\n+ 2 E\nh\u0000f(X) −E[Y |X]\n\u0001\nE\n\u0002\u0000E[Y |X] −Y\n\u0001\f\fX\n\u0003i\n+ E\n\u0002\n|E[Y |X] −Y |2\u0003\n= E\n\u0002\n|f(X) −E[Y |X]|2\u0003\n+ 2 E\n\u0002\u0000f(X) −E[Y |X]\n\u0001\u0000E[Y |X] −E[Y |X]\n\u0001\u0003\n+ E\n\u0002\n|E[Y |X] −Y |2\u0003\n= E\n\u0002\n|f(X) −E[Y |X]|2\u0003\n+ E\n\u0002\n|E[Y |X] −Y |2\u0003\n.\n(227)\nThis implies that for all f, g ∈L2(PX; R) it holds that\nE(f) −E(g) = E\n\u0002\n|f(X) −E[Y |X]|2\u0003\n−E\n\u0002\n|g(X) −E[Y |X]|2\u0003\n.\n(228)\nHence, we obtain that for all f, g ∈L2(PX; R) it holds that\nE\n\u0002\n|f(X) −E[Y |X]|2\u0003\n= E\n\u0002\n|g(X) −E[Y |X]|2\u0003\n+ E(f) −E(g).\n(229)\nCombining this with (227) and (228) establishes items (i), (ii), and (iii). The proof of Lemma 4.1 is thus\ncompleted.\n4.2\nOverall error decomposition\nLemma 4.2. Let (Ω, F, P) be a probability space, let d, M ∈N, let D ⊆Rd be a compact set, let Xm : Ω→\nD, m ∈{1, 2, . . ., M}, and Ym : Ω→R, m ∈{1, 2, . . . , M}, be functions, assume that (Xm, Ym), m ∈\n{1, 2, . . . , M}, are i.i.d. random variables, assume E[|Y1|2] < ∞, let E : C(D, R) →[0, ∞) satisfy for all\nf ∈C(D, R) that E(f) = E[|f(X1) −Y1|2], and let E: C(D, R) × Ω→[0, ∞) satisfy for all f ∈C(D, R),\nω ∈Ωthat\nE(f, ω) = 1\nM\n\" M\nX\nm=1\n|f(Xm(ω)) −Ym(ω)|2\n#\n.\n(230)\nThen it holds for all f, φ ∈C(D, R) that\nE\n\u0002\n|f(X1) −E[Y1|X1]|2\u0003\n= E\n\u0002\n|φ(X1) −E[Y1|X1]|2\u0003\n+ E(f) −E(φ)\n≤E\n\u0002\n|φ(X1) −E[Y1|X1]|2\u0003\n+\n\u0002\nE(f) −E(φ)\n\u0003\n+ 2\n\u0014\nmax\nv∈{f,φ} |E(v) −E(v)|\n\u0015\n.\n(231)\n40\nProof of Lemma 4.2. Note that Lemma 4.1 ensures that for all f, φ ∈C(D, R) it holds that\nE\n\u0002\n|f(X1) −E[Y1|X1]|2\u0003\n= E\n\u0002\n|φ(X1) −E[Y1|X1]|2\u0003\n+ E(f) −E(φ)\n= E\n\u0002\n|φ(X1) −E[Y1|X1]|2\u0003\n+ E(f) −E(f) + E(f) −E(φ) + E(φ) −E(φ)\n= E\n\u0002\n|φ(X1) −E[Y1|X1]|2\u0003\n+\n\u0002\u0000E(f) −E(f)\n\u0001\n+\n\u0000E(φ) −E(φ)\n\u0001\u0003\n+\n\u0002\nE(f) −E(φ)\n\u0003\n≤E\n\u0002\n|φ(X1) −E[Y1|X1]|2\u0003\n+\n\" X\nv∈{f,φ}\n|E(v) −E(v)|\n#\n+\n\u0002\nE(f) −E(φ)\n\u0003\n≤E\n\u0002\n|φ(X1) −E[Y1|X1]|2\u0003\n+ 2\n\u0014\nmax\nv∈{f,φ} |E(v) −E(v)|\n\u0015\n+\n\u0002\nE(f) −E(φ)\n\u0003\n.\n(232)\nThe proof of Lemma 4.2 is thus completed.\nLemma 4.3. Let (Ω, F, P) be a probability space, let d, d, M ∈N, let D ⊆Rd be a compact set, let\nB ⊆Rd be a set, let H = (Hθ)θ∈B : B →C(D, R) be a function, let Xm : Ω→D, m ∈{1, 2, . . ., M},\nand Ym : Ω→R, m ∈{1, 2, . . . , M}, be functions, assume that (Xm, Ym), m ∈{1, 2, . . ., M}, are i.i.d.\nrandom variables, assume E[|Y1|2] < ∞, let ϕ: D →R be a B(D)/B(R)-measurable function, assume\nthat it holds P-a.s. that ϕ(X1) = E[Y1|X1], let E : C(D, R) →[0, ∞) satisfy for all f ∈C(D, R) that\nE(f) = E[|f(X1) −Y1|2], and let E: B × Ω→[0, ∞) satisfy for all θ ∈B, ω ∈Ωthat\nE(θ, ω) = 1\nM\n\" M\nX\nm=1\n|Hθ(Xm(ω)) −Ym(ω)|2\n#\n.\n(233)\nThen it holds for all θ, ϑ ∈B that\nZ\nD\n|Hθ(x) −ϕ(x)|2 PX1(dx) =\nZ\nD\n|Hϑ(x) −ϕ(x)|2 PX1(dx) + E(Hθ) −E(Hϑ)\n≤\nZ\nD\n|Hϑ(x) −ϕ(x)|2 PX1(dx) +\n\u0002\nE(θ) −E(ϑ)\n\u0003\n+ 2\n\u0014\nsup\nη∈B\n|E(η) −E(Hη)|\n\u0015\n.\n(234)\nProof of Lemma 4.3. First, observe that Lemma 4.2 (with (Ω, F, P) ←(Ω, F, P), d ←d, M ←M, D ←\nD, (Xm)m∈{1,2,...,M} ←(Xm)m∈{1,2,...,M}, (Ym)m∈{1,2,...,M} ←(Ym)m∈{1,2,...,M}, E ←E, E ←\n\u0000C(D, R) × Ω∋\n(f, ω) 7→\n1\nM\n\u0002PM\nm=1 |f(Xm(ω)) −Ym(ω)|2\u0003\n∈[0, ∞)\n\u0001\nin the notation of Lemma 4.2) shows that for all\nθ, ϑ ∈B it holds that\nE\n\u0002\n|Hθ(X1) −E[Y1|X1]|2\u0003\n= E\n\u0002\n|Hϑ(X1) −E[Y1|X1]|2\u0003\n+ E(Hθ) −E(Hϑ)\n≤E\n\u0002\n|Hϑ(X1) −E[Y1|X1]|2\u0003\n+\n\u0002\nE(θ) −E(ϑ)\n\u0003\n+ 2\n\u0014\nmax\nη∈{θ,ϑ} |E(η) −E(Hη)|\n\u0015\n≤E\n\u0002\n|Hϑ(X1) −E[Y1|X1]|2\u0003\n+\n\u0002\nE(θ) −E(ϑ)\n\u0003\n+ 2\n\u0014\nsup\nη∈B\n|E(η) −E(Hη)|\n\u0015\n.\n(235)\nIn addition, note that the hypothesis that it holds P-a.s. that ϕ(X1) = E[Y1|X1] ensures that for all η ∈B\nit holds that\nE\n\u0002\n|Hη(X1) −E[Y1|X1]|2\u0003\n= E\n\u0002\n|Hη(X1) −ϕ(X1)|2\u0003\n=\nZ\nD\n|Hη(x) −ϕ(x)|2 PX1(dx).\n(236)\nCombining this with (235) establishes (234). The proof of Lemma 4.3 is thus completed.\n41\n4.3\nAnalysis of the convergence speed\n4.3.1\nConvergence rates for convergence in probability\nLemma 4.4. Let (Ω, F, P) be a probability space, let u ∈R, v ∈(u, ∞), d, L ∈N, let l = (l0, l1, . . . , lL) ∈\nNL+1 satisfy lL = 1 and PL\ni=1 li(li−1+1) ≤d, let B ⊆Rd be a non-empty compact set, and let X : Ω→Rl0\nand Y : Ω→[u, v] be random variables. Then\n(i) it holds for all θ ∈B, ω ∈Ωthat |N θ,l\nu,v (X(ω)) −Y (ω)|2 ∈[0, (v −u)2],\n(ii) it holds that B ∋θ 7→E\n\u0002\n|N θ,l\nu,v (X) −Y |2\u0003\n∈[0, ∞) is continuous, and\n(iii) there exists ϑ ∈B such that E\n\u0002\n|N ϑ,l\nu,v (X) −Y |2\u0003\n= inf\nθ∈B E\n\u0002\n|N θ,l\nu,v (X) −Y |2\u0003\n(cf. Deﬁnition 2.8).\nProof of Lemma 4.4. First, note that the fact that for all θ ∈Rd, x ∈Rl0 it holds that N θ,l\nu,v (x) ∈[u, v]\nand the hypothesis that for all ω ∈Ωit holds that Y (ω) ∈[u, v] demonstrate item (i). Next observe\nthat Corollary 2.37 ensures that for all ω ∈Ωit holds that B ∋θ 7→|N θ,l\nu,v (X(ω)) −Y (ω)|2 ∈[0, ∞)\nis a continuous function. Combining this and item (i) with Lebesgue’s dominated convergence theorem\nestablishes item (ii). Furthermore, note that item (ii) and the assumption that B ⊆Rd is a non-empty\ncompact set prove item (iii). The proof of Lemma 4.4 is thus completed.\nTheorem 4.5. Let (Ω, F, P) be a probability space, let d, d, K, M ∈N, ε ∈(0, ∞), L, u ∈R, v ∈(u, ∞),\nlet D ⊆Rd be a compact set, assume |D| ≥2, let Xm : Ω→D, m ∈{1, 2, . . . , M}, and Ym : Ω→[u, v],\nm ∈{1, 2, . . . , M}, be functions, assume that (Xm, Ym), m ∈{1, 2, . . . , M}, are i.i.d. random variables, let\nδ: D ×D →[0, ∞) satisfy for all x = (x1, x2, . . . , xd), y = (y1, y2, . . . , yd) ∈D that δ(x, y) = Pd\ni=1|xi −yi|,\nlet ϕ: D →[u, v] satisfy P-a.s. that ϕ(X1) = E[Y1|X1], assume for all x, y ∈D that |ϕ(x) −ϕ(y)| ≤\nLδ(x, y), let N ∈N ∩[max{2, C(D,δ), ε\n4L}, ∞), let l ∈N ∩(N, ∞), let l = (l0, l1, . . . , ll) ∈Nl+1 satisfy\nfor all i ∈N ∩[2, N], j ∈N ∩[N, l) that l0 = d, l1 ≥2dN, li ≥2N −2i + 3, lj ≥2, ll = 1,\nand Pl\nk=1 lk(lk−1 + 1) ≤d, let R ∈[max{1, L, supz∈D|||z|||, 2[supz∈D |ϕ(z)|]}, ∞), let B ⊆Rd satisfy\nB = [−R, R]d, let E: B × Ω→[0, ∞) satisfy for all θ ∈B, ω ∈Ωthat\nE(θ, ω) = 1\nM\n\" M\nX\nm=1\n|N θ,l\nu,v (Xm(ω)) −Ym(ω)|2\n#\n,\n(237)\nlet Θk : Ω→B, k ∈{1, 2, . . . , K}, be i.i.d. random variables, assume that Θ1 is continuous uniformly dis-\ntributed on B, and let Ξ: Ω→B satisfy Ξ = Θmin{k∈{1,2,...,K}: E(Θk)=minl∈{1,2,...,K} E(Θl)} (cf. Deﬁnitions 3.11,\n2.20, and 2.8). Then\nP\n\u0012Z\nD\n|N Ξ,l\nu,v (x) −ϕ(x)|2 PX1(dx) > ε2\n\u0013\n≤exp\n\u0012\n−K min\n\u001a\n1,\nε2d\n(16(v −u)l(|||l||| + 1)lRl+1)d\n\u001b\u0013\n+ 2 exp\n\u0012\nd ln\n\u0012\nmax\n\u001a\n1, 128l(|||l||| + 1)lRl+1(v −u)\nε2\n\u001b\u0013\n−\nε4M\n32(v −u)4\n\u0013\n.\n(238)\nProof of Theorem 4.5. Throughout this proof let M ⊆D satisfy |M| = max{2, C(D,δ), ε\n4L} and\n4L\n\u0014\nsup\nx∈D\n\u0012\ninf\ny∈M δ(x, y)\n\u0013\u0015\n≤ε,\n(239)\nlet b ∈[0, ∞) satisfy b = supz∈D|||z|||, let E : C(D, R) →[0, ∞) satisfy for all f ∈C(D, R) that E(f) =\nE[|f(X1) −Y1|2], and let ϑ ∈B satisfy E(N ϑ,l\nu,v |D) = infθ∈B E(N θ,l\nu,v |D) (cf. Lemma 4.4). Observe that the\nhypothesis that for all x, y ∈D it holds that |ϕ(x) −ϕ(y)| ≤Lδ(x, y) implies that ϕ is a B(D)/B([u, v])-\nmeasurable function. Lemma 4.3 (with (Ω, F, P) ←(Ω, F, P), d ←d, d ←d, M ←M, D ←D, B ←B,\n42\nH ←(B ∋θ 7→N θ,l\nu,v |D ∈C(D, R)), (Xm)m∈{1,2,...,M} ←(Xm)m∈{1,2,...,M}, (Ym)m∈{1,2,...,M} ←((Ω∋ω 7→\nYm(ω) ∈R))m∈{1,2,...,M}, ϕ ←(D ∋x 7→ϕ(x) ∈R), E ←E, E ←E in the notation of Lemma 4.3)\ntherefore ensures that for all ω ∈Ωit holds that\nZ\nD\n|N Ξ(ω),l\nu,v\n(x) −ϕ(x)|2 PX1(dx)\n≤\nZ\nD\n|N ϑ,l\nu,v (x) −ϕ(x)|2 PX1(dx)\n|\n{z\n}\nApproximation error\n+\n\u0002\nE(Ξ(ω), ω) −E(ϑ, ω)\n\u0003\n|\n{z\n}\nOptimization error\n+ 2\n\u0014\nsup\nθ∈B\n|E(θ, ω) −E(N θ,l\nu,v |D)|\n\u0015\n|\n{z\n}\nGeneralization error\n.\n(240)\nNext observe that the assumption that N ≥max{2, C(D,δ), ε\n4L} = |M| shows that for all i ∈N ∩[2, N]\nit holds that l ≥|M| + 1, l1 ≥2d|M| and li ≥2|M| −2i + 3. The hypothesis that for all x, y ∈D\nit holds that |ϕ(x) −ϕ(y)| ≤Lδ(x, y), the hypothesis that R ≥max{1, L, supz∈D|||z|||, 2[supz∈D |ϕ(z)|]},\nCorollary 3.8 (with d ←d, d ←d, L ←l, L ←L, u ←u, v ←v, D ←D, f ←ϕ, M ←M, l ←l in the\nnotation of Corollary 3.8), and (239) hence ensure that there exists η ∈B which satisﬁes\nsup\nx∈D\n|N η,l\nu,v (x) −ϕ(x)| ≤2L\n\"\nsup\nx=(x1,x2,...,xd)∈D\n \ninf\ny=(y1,y2,...,yd)∈M\nd\nX\ni=1\n|xi −yi|\n!#\n= 2L\n\u0014\nsup\nx∈D\n\u0012\ninf\ny∈M δ(x, y)\n\u0013\u0015\n≤ε\n2.\n(241)\nLemma 4.3 (with (Ω, F, P) ←(Ω, F, P), d ←d, d ←d, M ←M, D ←D, B ←B, H ←(B ∋\nθ 7→N θ,l\nu,v |D ∈C(D, R)), (Xm)m∈{1,2,...,M} ←(Xm)m∈{1,2,...,M}, (Ym)m∈{1,2,...,M} ←((Ω∋ω 7→Ym(ω) ∈\nR))m∈{1,2,...,M}, ϕ ←(D ∋x 7→ϕ(x) ∈R), E ←E, E ←E in the notation of Lemma 4.3) and the\nassumption that E(N ϑ,l\nu,v |D) = infθ∈B E(N θ,l\nu,v |D) therefore prove that\nZ\nD\n|N ϑ,l\nu,v (x) −ϕ(x)|2 PX1(dx) =\nZ\nD\n|N η,l\nu,v (x) −ϕ|2 PX1(dx) + E(N ϑ,l\nu,v |D) −E(N η,l\nu,v |D)\n|\n{z\n}\n≤0\n≤\nZ\nD\n|N η,l\nu,v (x) −ϕ(x)|2 PX1(dx) ≤sup\nx∈D\n|N η,l\nu,v (x) −ϕ(x)|2 ≤ε2\n4 .\n(242)\nCombining this with (240) shows that for all ω ∈Ωit holds that\nZ\nD\n|N Ξ(ω),l\nu,v\n(x) −ϕ(x)|2 PX1(dx) ≤ε2\n4 +\n\u0002\nE(Ξ(ω), ω) −E(ϑ, ω)\n\u0003\n+ 2\n\u0014\nsup\nθ∈B\n|E(θ, ω) −E(N θ,l\nu,v |D)|\n\u0015\n.\n(243)\nHence, we obtain that\nP\n\u0012Z\nD\n|N Ξ,l\nu,v (x) −ϕ(x)|2 PX1(dx) > ε2\n\u0013\n≤P\n\u0012\u0002\nE(Ξ) −E(ϑ)\n\u0003\n+ 2\n\u0014\nsup\nθ∈B\n|E(θ) −E(N θ,l\nu,v |D)|\n\u0015\n> 3ε2\n4\n\u0013\n≤P\n\u0012\nE(Ξ) −E(ϑ) > ε2\n4\n\u0013\n+ P\n\u0012\nsup\nθ∈B\n|E(θ) −E(N θ,l\nu,v |D)| > ε2\n4\n\u0013\n.\n(244)\nNext observe that Corollary 2.37 (with a ←−b, b ←b, u ←u, v ←v, d ←d, L ←l, l ←l in the notation\nof Corollary 2.37) demonstrates that for all θ, ξ ∈B it holds that\nsup\nx∈D\n|N θ,l\nu,v (x) −N ξ,l\nu,v (x)| ≤\nsup\nx∈[−b,b]d|N θ,l\nu,v (x) −N ξ,l\nu,v (x)|\n≤l max{1, b}(|||l||| + 1)l(max{1, |||θ|||, |||ξ|||})l−1|||θ −ξ|||\n≤lR(|||l||| + 1)lRl−1|||θ −ξ||| = l(|||l||| + 1)lRl|||θ −ξ|||.\n(245)\nCombining this with the fact that for all θ ∈Rd, x ∈D it holds that N θ,l\nu,v (x) ∈[u, v], the hypothesis that\nfor all m ∈{1, 2, . . . , M}, ω ∈Ωit holds that Ym(ω) ∈[u, v], the fact that for all x1, x2, y ∈R it holds\n43\nthat (x1 −y)2 −(x2 −y)2 = (x1 −x2)((x1 −y) + (x2 −y)), and (237) ensures that for all θ, ξ ∈B, ω ∈Ω\nit holds that\n|E(θ, ω) −E(ξ, ω)|\n=\n\f\f\f\f\f\n1\nM\n\" M\nX\nm=1\n|N θ,l\nu,v (Xm(ω)) −Ym(ω)|2\n#\n−1\nM\n\" M\nX\nm=1\n|N ξ,l\nu,v (Xm(ω)) −Ym(ω)|2\n#\f\f\f\f\f\n(246)\n= 1\nM\n\f\f\f\f\f\nM\nX\nm=1\n\u0010\u0000N θ,l\nu,v (Xm(ω)) −N ξ,l\nu,v (Xm(ω))\n\u0001 \u0002\u0000N θ,l\nu,v (Xm(ω)) −Ym(ω)\n\u0001\n+\n\u0000N ξ,l\nu,v (Xm(ω)) −Ym(ω)\n\u0001\u0003\u0011\f\f\f\f\f\n≤1\nM\n\" M\nX\nm=1\n\u0010\f\fN θ,l\nu,v (Xm(ω)) −N ξ,l\nu,v (Xm(ω))\n\f\f \u0002\n|N θ,l\nu,v (Xm(ω)) −Ym(ω)| + |N ξ,l\nu,v (Xm(ω)) −Ym(ω)|\n\u0003\n|\n{z\n}\n≤2(v−u)\n\u0011#\n≤2(v −u)l(|||l||| + 1)lRl|||θ −ξ|||.\nLemma 3.23 (with (Ω, F, P) ←(Ω, F, P), d ←d, N ←K, a ←−R, b ←R, ϑ ←ϑ, L ←2(v −u)l(|||l||| +\n1)lRl, ε ←ε2\n4 , E ←E, (Θn)n∈{1,2,...,N} ←(Θk)k∈{1,2,...,K} in the notation of Lemma 3.23) therefore shows\nthat\nP\n\u0012\nE(Ξ) −E(ϑ) > ε2\n4\n\u0013\n= P\n\u0012\u0014\nmin\nk∈{1,2,...,K}E(Θk)\n\u0015\n−E(ϑ) > ε2\n4\n\u0013\n≤exp\n\u0012\n−K min\n\u001a\n1,\n\u0000 ε2\n4\n\u0001d\n[2(v −u)l(|||l||| + 1)lRl]d(2R)d\n\u001b\u0013\n= exp\n\u0012\n−K min\n\u001a\n1,\nε2d\n(16(v −u)l(|||l||| + 1)lRl+1)d\n\u001b\u0013\n.\n(247)\nMoreover, note that Lemma 3.21 (with d ←d, M ←M, L ←l, u ←u, v ←v, R ←R, ε ←\nε2\n4 ,\nb ←b, l ←l, D ←D, (Ω, F, P) ←(Ω, F, P), (Xm)m∈{1,2,...,M} ←(Xm)m∈{1,2,...,M}, (Ym)m∈{1,2,...,M} ←\n(Ym)m∈{1,2,...,M}, E ←E, E ←E in the notation of Lemma 3.21) establishes that\nP\n\u0012\nsupθ∈B|E(θ) −E(N θ,l\nu,v |D)| ≥ε2\n4\n\u0013\n≤2 max\n\u001a\n1,\n\u0014128l max{1, b}(|||l||| + 1)lRl(v −u)\nε2\n\u0015d\u001b\nexp\n\u0012\n−ε4M\n32(v −u)4\n\u0013\n≤2 max\n\u001a\n1,\n\u0014128l(|||l||| + 1)lRl+1(v −u)\nε2\n\u0015d\u001b\nexp\n\u0012\n−ε4M\n32(v −u)4\n\u0013\n= 2 exp\n\u0012\nd ln\n\u0012\nmax\n\u001a\n1, 128l(|||l||| + 1)lRl+1(v −u)\nε2\n\u001b\u0013\n−\nε4M\n32(v −u)4\n\u0013\n.\n(248)\nCombining this and (247) with (244) proves that\nP\n\u0012Z\nD\n|N Ξ,l\nu,v (x) −ϕ(x)|2 PX1(dx) > ε2\n\u0013\n≤exp\n\u0012\n−K min\n\u001a\n1,\nε2d\n(16(v −u)l(|||l||| + 1)lRl+1)d\n\u001b\u0013\n+ 2 exp\n\u0012\nd ln\n\u0012\nmax\n\u001a\n1, 128l(|||l||| + 1)lRl+1(v −u)\nε2\n\u001b\u0013\n−\nε4M\n32(v −u)4\n\u0013\n.\n(249)\nThe proof of Theorem 4.5 is thus completed.\nCorollary 4.6. Let (Ω, F, P) be a probability space, let d, d, K, M, τ ∈N, ε ∈(0, ∞), L, a, u ∈R,\nb ∈(a, ∞), v ∈(u, ∞), R ∈[max{1, L, |a|, |b|, 2|u|, 2|v|}, ∞), let Xm : Ω→[a, b]d, m ∈{1, 2, . . ., M},\nbe i.i.d. random variables, let ∥·∥: Rd →[0, ∞) be the standard norm on Rd, let ϕ: [a, b]d →[u, v]\n44\nsatisfy for all x, y ∈[a, b]d that |ϕ(x) −ϕ(y)| ≤L∥x −y∥, assume τ ≥2d(2dL(b −a)ε−1 + 2)d and\nd ≥τ(d + 1) + (τ −3)τ(τ + 1) + τ + 1, let l ∈Nτ satisfy l = (d, τ, τ, . . . , τ, 1), let B ⊆Rd satisfy\nB = [−R, R]d, let E: B × Ω→[0, ∞) satisfy for all θ ∈B, ω ∈Ωthat\nE(θ, ω) = 1\nM\n\" M\nX\nm=1\n|N θ,l\nu,v (Xm(ω)) −ϕ(Xm(ω))|2\n#\n,\n(250)\nlet Θk : Ω→B, k ∈{1, 2, . . . , K}, be i.i.d. random variables, assume that Θ1 is continuous uniformly dis-\ntributed on B, and let Ξ: Ω→B satisfy Ξ = Θmin{k∈{1,2,...,K}: E(Θk)=minl∈{1,2,...,K} E(Θl)} (cf. Deﬁnition 2.8).\nThen\nP\n \u0014Z\n[a,b]d |N Ξ,l\nu,v (x) −ϕ(x)|2 PX1(dx)\n\u00151/2\n> ε\n!\n≤exp\n\u0012\n−K min\n\u001a\n1,\nε2d\n(16(v −u)(τ + 1)τRτ)d\n\u001b\u0013\n+ 2 exp\n\u0012\nd ln\n\u0012\nmax\n\u001a\n1, 128(τ + 1)τRτ(v −u)\nε2\n\u001b\u0013\n−\nε4M\n32(v −u)4\n\u0013\n.\n(251)\nProof of Corollary 4.6. Throughout this proof let N ∈N satisfy\nN = min\n\u001a\nk ∈N: k ≥2dL(b −a)\nε\n\u001b\n,\n(252)\nlet M ⊆[a, b]d satisfy M = {a, a + b−a\nN , . . . , a + (N−1)(b−a)\nN\n, b}d, let δ: [a, b]d × [a, b]d →[0, ∞) satisfy for\nall x = (x1, x2, . . . , xd), y = (y1, y2, . . . , yd) ∈[a, b]d that δ(x, y) = Pd\ni=1|xi −yi|, and let l0, l1, . . . , lτ−1 ∈N\nsatisfy l = (l0, l1, . . . , lτ−1). Observe that for all x ∈[a, b] there exists y ∈{a, a+ b−a\nN , . . . , a+ (N−1)(b−a)\nN\n, b}\nsuch that |x −y| ≤b−a\n2N . This demonstrates that\n4L\n\"\nsup\nx=(x1,x2,...,xd)∈[a,b]d\n \ninf\ny=(y1,y2,...,yd)∈M\nd\nX\ni=1\n|xi −yi|\n!#\n≤2Ld(b −a)\nN\n≤ε.\n(253)\nHence, we obtain that\nC([a,b]d,δ), ε\n4L ≤|M| = (N + 1)d.\n(254)\nNext note that (252) implies that N < 2dL(b−a)ε−1 +1. The hypothesis that τ ≥2d(2dL(b−a)ε−1 +2)d\ntherefore ensures that\nτ > 2d(N + 1)d ≥(N + 1)d + 2.\n(255)\nHence, we obtain that for all i ∈{2, 3, . . . , (N + 1)d}, j ∈{(N + 1)d + 1, (N + 1)d + 2, . . . , τ −2} it holds\nthat\nl0 = d,\nl1 = τ ≥2d(N + 1)d,\nlτ−1 = 1,\nli = τ ≥2(N + 1)d −2i + 3,\nand\nlj = τ ≥2.\n(256)\nFurthermore, observe that the hypothesis that for all x, y ∈[a, b]d it holds that |ϕ(x) −ϕ(y)| ≤L∥x −y∥\nimplies that for all x, y ∈[a, b]d it holds that |ϕ(x) −ϕ(y)| ≤Lδ(x, y). Combining this, (254), (255),\n(256), and the hypothesis that d ≥τ(d+1)+(τ −3)τ(τ +1)+τ +1 = Pτ−1\ni=1 li(li−1 +1) with Theorem 4.5\n(with (Ω, F, P) ←(Ω, F, P), d ←d, d ←d, K ←K, M ←M, ε ←ε, L ←L, u ←u, v ←v, D ←[a, b]d,\n(Xm)m∈{1,2,...,M} ←(Xm)m∈{1,2,...,M}, (Ym)m∈{1,2,...,M} ←(ϕ(Xm))m∈{1,2,...,M}, δ ←δ, ϕ ←ϕ, N ←(N +1)d,\nl ←τ −1, l ←l, R ←R, B ←B, E ←E, (Θk)k∈{1,2,...,K} ←(Θk)k∈{1,2,...,K}, Ξ ←Ξ in the notation of\n45\nTheorem 4.5) establishes that\nP\n \u0014Z\n[a,b]d |N Ξ,l\nu,v (x) −ϕ(x)|2 PX1(dx)\n\u00151/2\n> ε\n!\n≤exp\n\u0012\n−K min\n\u001a\n1,\nε2d\n(16(v −u)(τ −1)(τ + 1)τ−1Rτ)d\n\u001b\u0013\n+ 2 exp\n\u0012\nd ln\n\u0012\nmax\n\u001a\n1, 128(τ −1)(τ + 1)τ−1Rτ(v −u)\nε2\n\u001b\u0013\n−\nε4M\n32(v −u)4\n\u0013\n≤exp\n\u0012\n−K min\n\u001a\n1,\nε2d\n(16(v −u)(τ + 1)τRτ)d\n\u001b\u0013\n+ 2 exp\n\u0012\nd ln\n\u0012\nmax\n\u001a\n1, 128(τ + 1)τRτ(v −u)\nε2\n\u001b\u0013\n−\nε4M\n32(v −u)4\n\u0013\n.\n(257)\nThe proof of Corollary 4.6 is thus completed.\nCorollary 4.7. Let (Ω, F, P) be a probability space, let d ∈N, L, a, u ∈R, b ∈(a, ∞), v ∈(u, ∞),\nR ∈[max{1, L, |a|, |b|, 2|u|, 2|v|}, ∞), let Xm: Ω→[a, b]d, m ∈N, be i.i.d. random variables, let ∥·∥: Rd →\n[0, ∞) be the standard norm on Rd, let ϕ: [a, b]d →[u, v] satisfy for all x, y ∈[a, b]d that |ϕ(x) −ϕ(y)| ≤\nL∥x−y∥, let lτ ∈Nτ, τ ∈N, satisfy for all τ ∈N∩[3, ∞) that lτ = (d, τ, τ, . . . , τ, 1), let Ed,M,τ : [−R, R]d×\nΩ→[0, ∞), d, M, τ ∈N, satisfy for all d, M ∈N, τ ∈N ∩[3, ∞), θ ∈[−R, R]d, ω ∈Ωwith d ≥\nτ(d + 1) + (τ −3)τ(τ + 1) + τ + 1 that\nEd,M,τ(θ, ω) = 1\nM\n\" M\nX\nm=1\n|N θ,lτ\nu,v (Xm(ω)) −ϕ(Xm(ω))|2\n#\n,\n(258)\nfor every d ∈N let Θd,k : Ω→[−R, R]d, k ∈N, be i.i.d. random variables, assume for all d ∈N\nthat Θd,1 is continuous uniformly distributed on [−R, R]d, and let Ξd,K,M,τ : Ω→[−R, R]d, d, K, M, τ ∈\nN, satisfy for all d, K, M, τ ∈N that Ξd,K,M,τ = Θd,min{k∈{1,2,...,K}: Ed,M,τ (Θd,k)=minl∈{1,2,...,K} Ed,M,τ(Θd,l)} (cf.\nDeﬁnition 2.8). Then there exists c ∈(0, ∞) such that for all d, K, M, τ ∈N, ε ∈(0, √v −u] with\nτ ≥2d(2dL(b −a)ε−1 + 2)d and d ≥τ(d + 1) + (τ −3)τ(τ + 1) + τ + 1 it holds that\nP\n \u0014Z\n[a,b]d |N Ξd,K,M,τ ,lτ\nu,v\n(x) −ϕ(x)|2 PX1(dx)\n\u00151/2\n> ε\n!\n≤exp\n\u0000−K(cτ)−τdε2d\u0001\n+ 2 exp\n\u0000d ln\n\u0000(cτ)τε−2\u0001\n−c−1ε4M\n\u0001\n.\n(259)\nProof of Corollary 4.7. Throughout this proof let c ∈(0, ∞) satisfy\nc = max{32(v −u)4, 256(v −u + 1)R}.\n(260)\nNote that Corollary 4.6 establishes that for all d, K, M, τ ∈N, ε ∈(0, ∞) with τ ≥2d(2dL(b −a)ε−1+2)d\nand d ≥τ(d + 1) + (τ −3)τ(τ + 1) + τ + 1 it holds that\nP\n \u0014Z\n[a,b]d |N Ξd,K,M,τ ,lτ\nu,v\n(x) −ϕ(x)|2 PX1(dx)\n\u00151/2\n> ε\n!\n≤exp\n\u0012\n−K min\n\u001a\n1,\nε2d\n(16(v −u)(τ + 1)τRτ)d\n\u001b\u0013\n+ 2 exp\n\u0012\nd ln\n\u0012\nmax\n\u001a\n1, 128(τ + 1)τRτ(v −u)\nε2\n\u001b\u0013\n−\nε4M\n32(v −u)4\n\u0013\n.\n(261)\nNext observe that (260) ensures that for all τ ∈N it holds that\n16(v −u)(τ + 1)τRτ ≤(16(v −u + 1)(τ + 1)R)τ ≤(32(v −u + 1)Rτ)τ ≤(cτ)τ.\n(262)\n46\nThe fact that for all ε ∈(0, √v −u], τ ∈N it holds that ε2 ≤16(v −u)(τ + 1)τRτ therefore shows that\nfor all ε ∈(0, √v −u], τ ∈N it holds that\n−min\n\u001a\n1,\nε2d\n(16(v −u)(τ + 1)τRτ)d\n\u001b\n=\n−ε2d\n(16(v −u)(τ + 1)τRτ)d ≤−ε2d\n(cτ)τd.\n(263)\nFurthermore, note that (260) implies that for all τ ∈N it holds that\n128(τ + 1)τRτ(v −u) ≤128(2τ)τRτ(v −u) ≤(256Rτ(v −u + 1))τ ≤(cτ)τ.\n(264)\nThe fact that for all ε ∈(0, √v −u], τ ∈N it holds that ε2 ≤128(τ + 1)τRτ(v −u) hence proves that for\nall ε ∈(0, √v −u], τ ∈N it holds that\nln\n\u0012\nmax\n\u001a\n1, 128(τ + 1)τRτ(v −u)\nε2\n\u001b\u0013\n= ln\n\u0012128(τ + 1)τRτ(v −u)\nε2\n\u0013\n≤ln\n\u0012(cτ)τ\nε2\n\u0013\n(265)\nIn addition, observe that (260) ensures that\n−1\n32(v −u)4 ≤−1\nc .\n(266)\nCombining this, (263), and (265) with (261) proves that for all d, K, M, τ ∈N, ε ∈(0, √v −u] with\nτ ≥2d(2dL(b −a)ε−1 + 2)d and d ≥τ(d + 1) + (τ −3)τ(τ + 1) + τ + 1 it holds that\nP\n \u0014Z\n[u,v]d |N Ξd,K,M,τ ,lτ\nu,v\n(x) −ϕ(x)|2 PX1(dx)\n\u00151/2\n> ε\n!\n≤exp\n\u0012−Kε2d\n(cτ)τd\n\u0013\n+ 2 exp\n\u0012\nd ln\n\u0012(cτ)τ\nε2\n\u0013\n−ε4M\nc\n\u0013\n.\n(267)\nThe proof of Corollary 4.7 is thus completed.\nCorollary 4.8. Let (Ω, F, P) be a probability space, let d ∈N, L, a, u ∈R, b ∈(a, ∞), v ∈(u, ∞),\nR ∈[max{1, L, |a|, |b|, 2|u|, 2|v|}, ∞), let Xm: Ω→[a, b]d, m ∈N, be i.i.d. random variables, let ∥·∥: Rd →\n[0, ∞) be the standard norm on Rd, let ϕ: [a, b]d →[u, v] satisfy for all x, y ∈[a, b]d that |ϕ(x) −ϕ(y)| ≤\nL∥x−y∥, let lτ ∈Nτ, τ ∈N, satisfy for all τ ∈N∩[3, ∞) that lτ = (d, τ, τ, . . . , τ, 1), let Ed,M,τ : [−R, R]d×\nΩ→[0, ∞), d, M, τ ∈N, satisfy for all d, M ∈N, τ ∈N ∩[3, ∞), θ ∈[−R, R]d, ω ∈Ωwith d ≥\nτ(d + 1) + (τ −3)τ(τ + 1) + τ + 1 that\nEd,M,τ(θ, ω) = 1\nM\n\" M\nX\nm=1\n|N θ,lτ\nu,v (Xm(ω)) −ϕ(Xm(ω))|2\n#\n,\n(268)\nfor every d ∈N let Θd,k : Ω→[−R, R]d, k ∈N, be i.i.d. random variables, assume for all d ∈N\nthat Θd,1 is continuous uniformly distributed on [−R, R]d, and let Ξd,K,M,τ : Ω→[−R, R]d, d, K, M, τ ∈\nN, satisfy for all d, K, M, τ ∈N that Ξd,K,M,τ = Θd,min{k∈{1,2,...,K}: Ed,M,τ (Θd,k)=minl∈{1,2,...,K} Ed,M,τ(Θd,l)} (cf.\nDeﬁnition 2.8). Then there exists c ∈(0, ∞) such that for all d, K, M, τ ∈N, ε ∈(0, √v −u] with\nτ ≥2d(2dL(b −a)ε−1 + 2)d and d ≥τ(d + 1) + (τ −3)τ(τ + 1) + τ + 1 it holds that\nP\n\u0012Z\n[a,b]d |N Ξd,K,M,τ ,lτ\nu,v\n(x) −ϕ(x)| PX1(dx) > ε\n\u0013\n≤exp\n\u0000−K(cτ)−τdε2d\u0001\n+ 2 exp\n\u0000d ln\n\u0000(cτ)τε−2\u0001\n−c−1ε4M\n\u0001\n.\n(269)\nProof of Corollary 4.8. Note that Jensen’s inequality shows that for all f ∈C([a, b]d, R) it holds that\nZ\n[a,b]d|f(x)| PX1(dx) ≤\n\u0014Z\n[a,b]d|f(x)|2 PX1(dx)\n\u0015 1\n2\n.\n(270)\nCombining this with Corollary 4.7 proves (269). The proof of Corollary 4.8 is thus completed.\n47\n4.3.2\nConvergence rates for strong convergence\nLemma 4.9. Let (Ω, F, P) be a probability space, let c ∈[0, ∞), and let X : Ω→[−c, c] be a random\nvariable. Then it holds for all ε, p ∈(0, ∞) that\nE[|X|p] ≤εp P(|X| ≤ε) + cp P(|X| > ε) ≤εp + cp P(|X| > ε).\n(271)\nProof of Lemma 4.9. Observe that the hypothesis that for all ω ∈Ωit holds that |X(ω)| ≤c ensures that\nfor all ε, p ∈(0, ∞) it holds that\nE[|X|p] = E\n\u0002\n|X|p\n1{|X|≤ε}\n\u0003\n+ E\n\u0002\n|X|p\n1{|X|>ε}\n\u0003\n≤εp P(|X| ≤ε) + cp P(|X| > ε) ≤εp + cp P(|X| > ε) . (272)\nThe proof of Lemma 4.9 is thus completed.\nCorollary 4.10. Let (Ω, F, P) be a probability space, let d ∈N, L, a, u ∈R, b ∈(a, ∞), v ∈(u, ∞),\nR ∈[max{1, L, |a|, |b|, 2|u|, 2|v|}, ∞), let Xm: Ω→[a, b]d, m ∈N, be i.i.d. random variables, let ∥·∥: Rd →\n[0, ∞) be the standard norm on Rd, let ϕ: [a, b]d →[u, v] satisfy for all x, y ∈[a, b]d that |ϕ(x) −ϕ(y)| ≤\nL∥x−y∥, let lτ ∈Nτ, τ ∈N, satisfy for all τ ∈N∩[3, ∞) that lτ = (d, τ, τ, . . . , τ, 1), let Ed,M,τ : [−R, R]d×\nΩ→[0, ∞), d, M, τ ∈N, satisfy for all d, M ∈N, τ ∈N ∩[3, ∞), θ ∈[−R, R]d, ω ∈Ωwith d ≥\nτ(d + 1) + (τ −3)τ(τ + 1) + τ + 1 that\nEd,M,τ(θ, ω) = 1\nM\n\" M\nX\nm=1\n|N θ,lτ\nu,v (Xm(ω)) −ϕ(Xm(ω))|2\n#\n,\n(273)\nfor every d ∈N let Θd,k : Ω→[−R, R]d, k ∈N, be i.i.d. random variables, assume for all d ∈N that Θd,1 is\ncontinuous uniformly distributed on [−R, R]d, and let Ξd,K,M,τ : Ω→[−R, R]d, d, K, M, τ ∈N, satisfy for\nall d, K, M, τ ∈N that Ξd,K,M,τ = Θd,min{k∈{1,2,...,K}: Ed,M,τ(Θd,k)=minl∈{1,2,...,K} Ed,M,τ (Θd,l)} (cf. Deﬁnition 2.8).\nThen there exists c ∈(0, ∞) such that for all d, K, M, τ ∈N, p ∈[1, ∞), ε ∈(0, √v −u] with τ ≥\n2d(2dL(b −a)ε−1 + 2)d and d ≥τ(d + 1) + (τ −3)τ(τ + 1) + τ + 1 it holds that\n \nE\n\"\u0012Z\n[a,b]d |N Ξd,K,M,τ ,lτ\nu,v\n(x) −ϕ(x)|2 PX1(dx)\n\u0013p/2#!1/p\n≤(v −u)\n\u0002\nexp\n\u0000−K(cτ)−τdε2d\u0001\n+ 2 exp\n\u0000d ln\n\u0000(cτ)τε−2\u0001\n−c−1ε4M\n\u0001\u00031/p + ε.\n(274)\nProof of Corollary 4.10. First, observe that Corollary 4.7 ensures that there exists c ∈(0, ∞) which\nsatisﬁes for all d, K, M, τ ∈N, ε ∈(0, √v −u] with τ ≥2d(2dL(b −a)ε−1 + 2)d and d ≥τ(d + 1) + (τ −\n3)τ(τ + 1) + τ + 1 that\nP\n \u0014Z\n[a,b]d |N Ξd,K,M,τ ,lτ\nu,v\n(x) −ϕ(x)|2 PX1(dx)\n\u00151/2\n> ε\n!\n≤exp\n\u0000−K(cτ)−τdε2d\u0001\n+ 2 exp\n\u0000d ln\n\u0000(cτ)τε−2\u0001\n−c−1ε4M\n\u0001\n.\n(275)\nLemma 4.9 (with (Ω, F, P) ←(Ω, F, P), c ←v −u, X ←(Ω∋ω 7→\n\u0002R\n[a,b]d |N\nΞd,K,M,τ(ω),lτ\nu,v\n(x) −\nϕ(x)|2 PX1(dx)\n\u00031/2 ∈[u−v, v−u]) in the notation of Lemma 4.9) hence ensures that for all d, K, M, τ ∈N,\nε ∈(0, √v −u], p ∈(0, ∞) with τ ≥2d(2dL(b −a)ε−1 + 2)d and d ≥τ(d + 1) + (τ −3)τ(τ + 1) + τ + 1 it\nholds that\nE\n\"\u0012Z\n[a,b]d |N Ξd,K,M,τ ,lτ\nu,v\n(x) −ϕ(x)|2 PX1(dx)\n\u0013p/2#\n≤εp + (v −u)ph\nexp\n\u0000−K(cτ)−τdε2d\u0001\n+ 2 exp\n\u0000d ln\n\u0000(cτ)τε−2\u0001\n−c−1ε4M\n\u0001i\n.\n(276)\nThe fact that for all p ∈[1, ∞), x, y ∈[0, ∞) it holds that (x + y)\n1/p ≤x\n1/p + y\n1/p therefore establishes\n(274). The proof of Corollary 4.10 is thus completed.\n48\nAcknowledgements\nThis work has been funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Founda-\ntion) under Germany’s Excellence Strategy EXC 2044–390685587, Mathematics M¨unster: Dynamics–\nGeometry–Structure.\nReferences\n[1] Bach, F. Breaking the curse of dimensionality with convex neural networks. J. Mach. Learn. Res.\n18 (2017), 53 pages.\n[2] Bach, F., and Moulines, E. Non-strongly-convex smooth stochastic approximation with con-\nvergence rate O(1/n). In Proceedings of the 26th International Conference on Neural Information\nProcessing Systems (USA, 2013), NIPS’13, Curran Associates Inc., pp. 773–781.\n[3] Barron, A. R. Universal approximation bounds for superpositions of a sigmoidal function. IEEE\nTrans. Inform. Theory 39, 3 (1993), 930–945.\n[4] Barron, A. R. Approximation and estimation bounds for artiﬁcial neural networks.\nMachine\nLearning 14, 1 (1994), 115–133.\n[5] Bartlett, P. L., Bousquet, O., and Mendelson, S. Local Rademacher complexities. Ann.\nStatist. 33, 4 (2005), 1497–1537.\n[6] Beck, C., Becker, S., Grohs, P., Jaafari, N., and Jentzen, A. Solving stochastic diﬀerential\nequations and Kolmogorov equations by means of deep learning. arXiv:1806.00421 (2018), 56 pages.\n[7] Beck, C., E, W., and Jentzen, A.\nMachine Learning Approximation Algorithms for High-\nDimensional Fully Nonlinear Partial Diﬀerential Equations and Second-order Backward Stochastic\nDiﬀerential Equations. J. Nonlinear Sci. 29, 4 (2019), 1563–1619.\n[8] Bellman, R. Dynamic programming. Princeton Landmarks in Mathematics. Princeton University\nPress, Princeton, NJ, 2010. Reprint of the 1957 edition.\n[9] Bercu, B., and Fort, J.-C. Generic stochastic gradient methods. Wiley Encyclopedia of Opera-\ntions Research and Management Science (2011), 1–8.\n[10] Berner, J., Grohs, P., and Jentzen, A. Analysis of the generalization error: Empirical risk\nminimization over deep artiﬁcial neural networks overcomes the curse of dimensionality in the nu-\nmerical approximation of Black–Scholes partial diﬀerential equations. arXiv:1809.03062 (2018), 35\npages.\n[11] Blum, E. K., and Li, L. K. Approximation theory and feedforward networks. Neural Networks\n4, 4 (1991), 511–515.\n[12] B¨olcskei, H., Grohs, P., Kutyniok, G., and Petersen, P. Optimal approximation with\nsparsely connected deep neural networks. SIAM J. Math. Data Sci. 1, 1 (2019), 8–45.\n[13] Burger, M., and Neubauer, A.\nError bounds for approximation with neural networks.\nJ.\nApprox. Theory 112, 2 (2001), 235–250.\n[14] Candes, E. J. Ridgelets: theory and applications. PhD thesis, Stanford University Stanford, 1998.\n[15] Chau, N. H., Moulines, ´E., R´asonyi, M., Sabanis, S., and Zhang, Y. On stochastic gradient\nLangevin dynamics with dependent data streams: the fully non-convex case.\narXiv:1905.13142\n(2019), 27 pages.\n49\n[16] Chen, T., and Chen, H. Approximation capability to functions of several variables, nonlinear\nfunctionals, and operators by radial basis function neural networks. IEEE Trans. Neural Netw. 6, 4\n(1995), 904–910.\n[17] Chui, C. K., Li, X., and Mhaskar, H. N. Neural networks for localized approximation. Math.\nComp. 63, 208 (1994), 607–623.\n[18] Cucker, F., and Smale, S. On the mathematical foundations of learning. Bull. Amer. Math.\nSoc. (N.S.) 39, 1 (2002), 1–49.\n[19] Cybenko, G. Approximation by superpositions of a sigmoidal function. Math. Control Signals\nSystems 2, 4 (1989), 303–314.\n[20] Dereich, S., and M¨uller-Gronbach, T.\nGeneral multilevel adaptations for stochastic ap-\nproximation algorithms of Robbins-Monro and Polyak-Ruppert type. Numer. Math. 142, 2 (2019),\n279–328.\n[21] DeVore, R. A., Oskolkov, K. I., and Petrushev, P. P. Approximation by feed-forward\nneural networks. In The heritage of P. L. Chebyshev: a Festschrift in honor of the 70th birthday of\nT. J. Rivlin, vol. 4. Baltzer Science Publishers BV, Amsterdam, 1997, pp. 261–287.\n[22] E, W., and Wang, Q. Exponential convergence of the deep neural network approximation for\nanalytic functions. arXiv:1807.00297 (2018), 7 pages.\n[23] Elbr¨achter, D., Grohs, P., Jentzen, A., and Schwab, C. DNN expression rate analysis of\nhigh-dimensional PDEs: Application to option pricing. arXiv:1809.07669 (2018), 50 pages.\n[24] Eldan, R., and Shamir, O. The power of depth for feedforward neural networks. In 29th Annual\nConference on Learning Theory (Columbia University, New York, New York, USA, 23–26 Jun 2016),\nV. Feldman, A. Rakhlin, and O. Shamir, Eds., vol. 49 of Proceedings of Machine Learning Research,\nPMLR, pp. 907–940.\n[25] Ellacott, S. W. Aspects of the numerical analysis of neural networks. In Acta numerica, 1994,\nActa Numer. Cambridge University Press, Cambridge, 1994, pp. 145–202.\n[26] Fehrman, B., Gess, B., and Jentzen, A. Convergence rates for the stochastic gradient descent\nmethod for non-convex objective functions. arXiv:1904.01517 (2019), 59 pages.\n[27] Funahashi, K.-I.\nOn the approximate realization of continuous mappings by neural networks.\nNeural Networks 2, 3 (1989), 183–192.\n[28] Goodfellow, I., Bengio, Y., and Courville, A. Deep learning. Adaptive Computation and\nMachine Learning. MIT Press, Cambridge, MA, 2016.\n[29] Gribonval, R., Kutyniok, G., Nielsen, M., and Voigtlaender, F. Approximation spaces\nof deep neural networks. arXiv:1905.01208 (2019), 63 pages.\n[30] Grohs, P., Hornung, F., Jentzen, A., and von Wurstemberger, P. A proof that artiﬁcial\nneural networks overcome the curse of dimensionality in the numerical approximation of Black–\nScholes partial diﬀerential equations. arXiv:1809.02362 (2018), 124 pages. Revision requested from\nMemoirs of the AMS.\n[31] Grohs, P., Hornung, F., Jentzen, A., and Zimmermann, P. Space-time error estimates for\ndeep neural network approximations for diﬀerential equations. arXiv:1908.03833 (2019), 86 pages.\n50\n[32] Grohs, P., Jentzen, A., and Salimova, D. Deep neural network approximations for Monte\nCarlo algorithms. arXiv:1908.10828 (2019), 45 pages.\n[33] Grohs, P., Perekrestenko, D., Elbr¨achter, D., and B¨olcskei, H. Deep neural network\napproximation theory. arXiv:1901.02220 (2019), 60 pages.\n[34] G¨uhring, I., Kutyniok, G., and Petersen, P. Error bounds for approximations with deep\nReLU neural networks in W s,p norms. arXiv:1902.07896 (2019), 42 pages.\n[35] Gy¨orfi, L., Kohler, M., Krzy˙zak, A., and Walk, H. A distribution-free theory of nonpara-\nmetric regression. Springer Series in Statistics. Springer-Verlag, New York, 2002.\n[36] Hartman, E. J., Keeler, J. D., and Kowalski, J. M. Layered neural networks with Gaussian\nhidden units as universal approximations. Neural Comput. 2, 2 (1990), 210–215.\n[37] Hoeffding, W. Probability inequalities for sums of bounded random variables. J. Amer. Statist.\nAssoc. 58, 301 (1963), 13–30.\n[38] Hornik, K. Approximation capabilities of multilayer feedforward networks. Neural Networks 4, 2\n(1991), 251–257.\n[39] Hornik, K. Some new results on neural network approximation. Neural Networks 6, 8 (1993),\n1069–1072.\n[40] Hornik, K., Stinchcombe, M., and White, H. Multilayer feedforward networks are universal\napproximators. Neural Networks 2, 5 (1989), 359–366.\n[41] Hornik, K., Stinchcombe, M., and White, H. Universal approximation of an unknown map-\nping and its derivatives using multilayer feedforward networks. Neural Networks 3, 5 (1990), 551–560.\n[42] Hutzenthaler, M., Jentzen, A., Kruse, T., and Nguyen, T. A. A proof that rectiﬁed deep\nneural networks overcome the curse of dimensionality in the numerical approximation of semilinear\nheat equations. arXiv:1901.10854 (2019), 29 pages.\n[43] Jentzen, A., Kuckuck, B., Neufeld, A., and von Wurstemberger, P. Strong error analysis\nfor stochastic gradient descent optimization algorithms. arXiv:1801.09324 (2018), 75 pages. Revision\nrequested from IMA J. Numer. Anal.\n[44] Jentzen, A., Salimova, D., and Welti, T. A proof that deep artiﬁcial neural networks over-\ncome the curse of dimensionality in the numerical approximation of Kolmogorov partial diﬀerential\nequations with constant diﬀusion and nonlinear drift coeﬃcients. arXiv:1809.07321 (2018), 48 pages.\n[45] Jentzen, A., and von Wurstemberger, P. Lower error bounds for the stochastic gradient\ndescent optimization algorithm: Sharp convergence rates for slowly and fast decaying learning rates.\narXiv:1803.08600 (2018), 42 pages. To appear in J. Complex.\n[46] Karimi, B., Miasojedow, B., Moulines, E., and Wai, H.-T. Non-asymptotic Analysis of\nBiased Stochastic Approximation Scheme. arXiv:1902.00629 (2019), 32 pages.\n[47] Kutyniok, G., Petersen, P., Raslan, M., and Schneider, R. A theoretical analysis of deep\nneural networks and parametric PDEs. arXiv:1904.00377 (2019), 43 pages.\n[48] Lei, Y., Hu, T., Li, G., and Tang, K. Stochastic Gradient Descent for Nonconvex Learning\nwithout Bounded Gradient Assumptions. arXiv:1902.00908 (2019), 6 pages.\n51\n[49] Leshno, M., Lin, V. Y., Pinkus, A., and Schocken, S. Multilayer feedforward networks with\na nonpolynomial activation function can approximate any function. Neural Networks 6, 6 (1993),\n861–867.\n[50] Maggi, F. Sets of ﬁnite perimeter and geometric variational problems, vol. 135 of Cambridge Studies\nin Advanced Mathematics. Cambridge University Press, Cambridge, 2012.\n[51] Massart, P. Concentration inequalities and model selection, vol. 1896 of Lecture Notes in Mathe-\nmatics. Springer, Berlin, 2007. Lectures from the 33rd Summer School on Probability Theory held\nin Saint-Flour, July 6–23, 2003.\n[52] Mhaskar, H. N. Neural networks for optimal approximation of smooth and analytic functions.\nNeural Comput. 8, 1 (1996), 164–177.\n[53] Mhaskar, H. N., and Micchelli, C. A. Degree of approximation by neural and translation\nnetworks with a single hidden layer. Adv. in Appl. Math. 16, 2 (1995), 151–183.\n[54] Mhaskar, H. N., and Poggio, T. Deep vs. shallow networks: an approximation theory perspec-\ntive. Anal. Appl. (Singap.) 14, 6 (2016), 829–848.\n[55] Nguyen-Thien, T., and Tran-Cong, T. Approximation of functions and their derivatives: A\nneural network implementation with applications. Appl. Math. Model. 23, 9 (1999), 687–704.\n[56] Novak, E., and Wo´zniakowski, H. Tractability of multivariate problems. Vol. 1: Linear in-\nformation, vol. 6 of EMS Tracts in Mathematics. European Mathematical Society (EMS), Z¨urich,\n2008.\n[57] Novak, E., and Wo´zniakowski, H. Tractability of multivariate problems. Volume II: Standard\ninformation for functionals, vol. 12 of EMS Tracts in Mathematics. European Mathematical Society\n(EMS), Z¨urich, 2010.\n[58] Park, J., and Sandberg, I. W. Universal approximation using radial-basis-function networks.\nNeural Comput. 3, 2 (1991), 246–257.\n[59] Perekrestenko, D., Grohs, P., Elbr¨achter, D., and B¨olcskei, H. The universal approx-\nimation power of ﬁnite-width deep ReLU networks. arXiv:1806.01528 (2018), 16 pages.\n[60] Petersen, P., Raslan, M., and Voigtlaender, F. Topological properties of the set of functions\ngenerated by neural networks of ﬁxed size. arXiv:1806.08459 (2018), 56 pages.\n[61] Petersen, P., and Voigtlaender, F. Equivalence of approximation by convolutional neural\nnetworks and fully-connected networks. arXiv:1809.00973 (2018), 10 pages.\n[62] Petersen, P., and Voigtlaender, F. Optimal approximation of piecewise smooth functions\nusing deep ReLU neural networks. Neural Networks 108 (2018), 296–330.\n[63] Pinkus, A. Approximation theory of the MLP model in neural networks. In Acta numerica, 1999,\nvol. 8 of Acta Numer. Cambridge University Press, Cambridge, 1999, pp. 143–195.\n[64] Reisinger, C., and Zhang, Y. Rectiﬁed deep neural networks overcome the curse of dimension-\nality for nonsmooth value functions in zero-sum games of nonlinear stiﬀsystems. arXiv:1903.06652\n(2019), 34 pages.\n[65] Schmitt, M. Lower bounds on the complexity of approximating continuous functions by sigmoidal\nneural networks. In Proceedings of the 12th International Conference on Neural Information Process-\ning Systems (Cambridge, MA, USA, 1999), NIPS’99, MIT Press, pp. 328–334.\n52\n[66] Schwab, C., and Zech, J. Deep learning in high dimension: neural network expression rates for\ngeneralized polynomial chaos expansions in UQ. Anal. Appl. (Singap.) 17, 1 (2019), 19–55.\n[67] Shaham, U., Cloninger, A., and Coifman, R. R. Provable approximation properties for deep\nneural networks. Appl. Comput. Harmon. Anal. 44, 3 (2018), 537–557.\n[68] Shalev-Shwartz, S., and Ben-David, S.\nUnderstanding machine learning: From theory to\nalgorithms. Cambridge University Press, Cambridge, 2014.\n[69] Shen, Z., Yang, H., and Zhang, S. Deep network approximation characterized by number of\nneurons. arXiv:1906.05497 (2019), 36 pages.\n[70] Shen, Z., Yang, H., and Zhang, S. Nonlinear approximation via compositions. arXiv:1902.10170\n(2019), 19 pages.\n[71] van de Geer, S. A. Applications of empirical process theory, vol. 6 of Cambridge Series in Statistical\nand Probabilistic Mathematics. Cambridge University Press, Cambridge, 2000.\n[72] Voigtlaender, F., and Petersen, P. Approximation in Lp(µ) with deep ReLU neural networks.\narXiv:1904.04789 (2019), 4 pages.\n[73] Yarotsky, D. Error bounds for approximations with deep ReLU networks. Neural Networks 94\n(2017), 103–114.\n[74] Yarotsky, D. Universal approximations of invariant maps by neural networks. arXiv:1804.10306\n(2018), 64 pages.\n53\n",
  "categories": [
    "math.NA",
    "cs.LG",
    "cs.NA"
  ],
  "published": "2019-09-30",
  "updated": "2020-01-30"
}