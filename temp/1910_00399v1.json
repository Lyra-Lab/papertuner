{
  "id": "http://arxiv.org/abs/1910.00399v1",
  "title": "Safe Reinforcement Learning on Autonomous Vehicles",
  "authors": [
    "David Isele",
    "Alireza Nakhaei",
    "Kikuo Fujimura"
  ],
  "abstract": "There have been numerous advances in reinforcement learning, but the\ntypically unconstrained exploration of the learning process prevents the\nadoption of these methods in many safety critical applications. Recent work in\nsafe reinforcement learning uses idealized models to achieve their guarantees,\nbut these models do not easily accommodate the stochasticity or\nhigh-dimensionality of real world systems. We investigate how prediction\nprovides a general and intuitive framework to constraint exploration, and show\nhow it can be used to safely learn intersection handling behaviors on an\nautonomous vehicle.",
  "text": "Safe Reinforcement Learning on Autonomous Vehicles\nDavid Isele, Alireza Nakhaei, and Kikuo Fujimura\nHonda Research Institute USA\n{disele, anakhaei, kfujimura}@honda-ri.com\nAbstract— There have been numerous advances in reinforce-\nment learning, but the typically unconstrained exploration of\nthe learning process prevents the adoption of these methods\nin many safety critical applications. Recent work in safe\nreinforcement learning uses idealized models to achieve their\nguarantees, but these models do not easily accommodate the\nstochasticity or high-dimensionality of real world systems. We\ninvestigate how prediction provides a general and intuitive\nframework to constraint exploration, and show how it can\nbe used to safely learn intersection handling behaviors on an\nautonomous vehicle.\nI. INTRODUCTION\nWith the increasing complexity of robotic systems, and the\ncontinued advances in machine learning, it can be tempting\nto apply reinforcement learning (RL) to challenging control\nproblems. However the trial and error searches typical to RL\nmethods are not appropriate to physical systems which act in\nthe real world where failure cases result in real consequences.\nTo mitigate the safety concerns associated with training\nan RL agent, there have been various efforts at designing\nlearning processes with safe exploration. As noted by Garcia\nand Fernandez [1], these approaches can be broadly classi-\nﬁed into approaches that modify the objective function and\napproaches that constrain the search space.\nModifying the objective function mostly focuses on catas-\ntrophic rare events which do not necessarily have a large\nimpact on the expected return over many trials. Proposed\nmethods take into account the variance of return [2], the\nworst-outcome [3], [2], [4], and the probability of visiting\nerror states [5]. Modiﬁed objective functions may be useful\non robotic systems where a small number of failures are\nacceptable. However on safety critical systems, often a single\nfailure is prohibited and learning must be conﬁned to always\nsatisfy the safety constraints.\nFor this reason methods that constrain the search space are\noften preferable. Because these approaches can completely\nforbid undesirable states, they are usually accompanied by\nformal guarantees, however satisfying the necessary condi-\ntions on physical systems can be quite difﬁcult in prac-\ntice. For example, strategies have assumed a known safe\npolicy which can take over and return to safe operating\nconditions[6], a learning model that is restricted to tabular\nRL methods [7], [8], and states that can be deterministically\nperceived and mapped to logical expressions [9], [10].\nWhile there are some approaches that have been imple-\nmented on physical robots such as the work of Gillula et\nal. which uses reachability to enforce strict safety guarantees\n[11], these approaches tend to be computationally expensive,\nFig. 1.\nAn autonomous vehicle navigating an intersection. Prediction is\nused to shield the vehicle from making dangerous decisions, while allowing\nit to learn policies that are both efﬁcient and not disruptive to other vehicles.\npreventing their application to high dimensional problems\nsuch as domains with multiple agents.\nWe investigate how prediction can be used to achieve a\nsystem that scales better to higher dimensions and is more\nsuited to noisy measurements. Using prediction methods\nwe show that we can safely constrain learning to optimize\nintersection behaviors on an autonomous vehicle where it\nmust consider the behaviors of multiple other agents. While\nwe believe prediction is a very general framework that lends\nitself to implementations on a variety of stochastic physical\nsystems, we note that its safety constraints are weaker than\nother approaches in the literature: we assume other agents\n(trafﬁc vehicles) follow a distribution and are not adversarial.\nThe speciﬁc application we investigate is making a turn at\nan unsigned intersection. This problem was recently explored\nas a non-safety constrained RL domain [12] where it was\nnoted that the learned policy, which optimized efﬁciency,\nmight be disruptive to trafﬁc vehicles in practice. The pri-\nmary concerns of these maneuvers are safety and efﬁciency,\nbut balancing the two is a dynamic task. In dense trafﬁc we\nmay wish to seize an opportunity that leaves only several\nmeters of a safety margin, as we might have to wait an\nunacceptable amount of time for the next opportunity. How-\never in sparse trafﬁc, adding an increased margin will only\nadd a negligible delay and will likely be preferable to the\npassengers and other trafﬁc vehicles. Figure 2 demonstrates\nthe scenarios in dense and sparse trafﬁc.\nWe demonstrate the use of prediction as a safety constraint\nby learning a policy that minimizes disruption to trafﬁc\n(as measured by trafﬁc braking) while avoiding collisions.\nAdditionally, we learn a policy that maximizes distance to\narXiv:1910.00399v1  [cs.LG]  27 Sep 2019\nFig. 2. In dense trafﬁc, a human driver (red vehicle) might take the opening\nto minimize the wait that would result from the approaching heavy trafﬁc.\nHowever, in sparse trafﬁc, it would be more preferable to accept a small\ndelay and let the car pass.\nother vehicles, while still getting through the intersection in\na ﬁxed time window. We show that these two optimizations\nproduce different behaviors and that both can be learned\nusing RL with 0 collisions.\nII. PROBLEM STATEMENT\nIn this document we use the subscript/superscript notation\nvariableagent,action\ntime\n. We deﬁne a safe set of policies Πi as\nthe set of policies π that generates a trajectory τ that with\nprobability less than δ has agent i entering a danger state at\nany step in its execution1.\nTo ﬁnd a safe policy in a multi-agent setting, we formulate\nthe problem as a stochastic game. In a stochastic game, at\ntime t each agent i in state st takes an action ai\nt according to\nthe policy πi. All the agents then transition to the state st+1\nand receive a reward ri\nt. Stochastic games can be described\nas as tuple ⟨S, A, P, R⟩, where S is the set of states, and\nA = {A1, . . . , Am} is the joint action space consisting of\nthe set of each agent’s actions, where m is the number of\nagents. The reward functions R = {R1, . . . , Rm} describe\nthe reward for each agent S×A →R. The transition function\nP : S × A × S →[0, 1] describes how the state evolves in re-\nsponse to all the agents’ collective actions. Stochastic games\nare an extension to Markov Decision Processes (MDPs) that\n1Interesting corner cases were proposed for many existing deﬁnitions of\nsafety by Moldovan and Abbeel [13]. Their proposed deﬁnition of safety in\nterms of ergodicity does not easily extend to a multi-agent setting.\nFig. 3.\nProposed pipeline.\ngeneralize to multiple agents, each of which has its own\npolicy and reward function.\nLet xi\nt\nbe the local state of a single agent. We\nwill refer to the sequence of the local states, actions,\nand rewards for a single agent as a trajectory τ i\n=\n{(xi\n1, ai\n1, ri\n1), . . . , (xi\nT , ai\nT , ri\nT )} over a horizon T.\nThe goal is to learn an optimal ego-agent policy π∗ego\nwhere at every point in the learning process πego ∈Πego.\nIII. PROPOSED PIPELINE\nTo achieve safe learning we propose the following pipeline\npresented in Figure 3. In this pipeline, perception provides\ninput to both an RL network and a prediction module. The\nprediction module masks undesired actions at each time step.\nGiven predictions, we can mask actions that result in\nunsafe behaviors. Masking unsafe behaviors in RL has also\nvery recently been proposed in the RL community when\nstates can be mapped to linear temporal logic [9]. This\npipeline provides us with a mechanism to explore the safe\nsubspace of an agent’s possible behaviors during training\nand also guarantees that RL picks safe decisions during\nexecution.\nIV. PREDICTION\nWe propose using prediction models to mask unsafe\nactions from the agent, and then allow the agent to freely\nexplore the safe state space using traditional RL techniques.\nProbabilistic predictions serve as an approximation to the\ncomputationally expensive task of identifying safe trajecto-\nries.\nRL algorithms have been proposed for stochastic games\nwithout any restrictions on safety [14], [15]. However, in\norder to ensure that the agent never takes an unsafe action,\nwe must check not only that a given action will not cause\nthe agent to transition to an unsafe state in the next time\nstep, but also that the action will not force the agent into\nan unsafe state at some point in the future. Note that this\nis closely related to the credit assignment problem, but the\nrisk must be assigned prior to acting. One might imagine\nthat ensuring the agent safely avoids all dangerous situations\nrequires branching through all possible action combinations\nfor a ﬁxed time horizon T. Brute force implementations\nwould result in an intractable runtime of O\n\u0000|A|T \u0001\n, where\n|A| = |A1|×· · ·×|Am|. Indeed it has been shown that even\nfor the more restricted case of MDPs, identifying the set\nof safe policies is NP-Hard [13]. For this reason we look at\nefﬁcient approximations for restricting our exploration space.\nTo reduce the complexity we assume the actions at each\ntime step are components of a high-level action (known as\noptions in the RL literature, and intentions in the autonomous\ndriving literature). This has the effect of collapsing the\nbranching factor of time associated with the exponential\ncomplexity. The cost of this approximation is that, for the\nﬁxed horizon, each agent is restricted in their ability to react\nand interact with other agents.\nTo accommodate the breadth of low-level action sequences\nthat correspond to a single high-level action and also to\nallow for a bounded level of interaction, we make each high-\nlevel action a probability distribution over functions f. First\nwe describe the trajectory of agent i in terms of high-level\nactions p(τ i) ≈Q|hi|\nj=1 p(hi = j)phi,j(xi\n1, . . . , xi\nT ). Here\nj indexes the high-level action h. Then we describe the\nfunctional local-state update as xt+1 = f i,j(xt) + ϵt where\nwe model the noise as a Gaussian distribution ϵt = N(0, σt).\nThis means that the updated local state has a corresponding\nmean and variance.\nWithin the ﬁxed time horizon, each agent takes a sin-\ngle high-level action. The variance acts as a bound that\nencompasses the variety of low-level actions that produce\nsimilar high-level actions. Additionally, we will use the\nvariance to create safety bounds. These bounds allows for\na bounded ability of each agent to react to other agents\nwithout violating our safety constraints. This can be thought\nof as selecting an open-loop high-level decision followed by\nsubsequent bounded closed-loop low-level corrections. Note\nthat by restricting an agent’s ability to interact and limiting\neach agent to a restricted set of high-level actions, we are\nignoring the existence of many pathological cases that may\narise in an adversarial setting.\nGiven the assumption of high-level actions that follow a\ndistribution, satisfying safety constraints can be computed\nin O(|H|T) where |Hi| is the number of high level actions\navailable to agent i and |H| = |H1| × · · · × |Hm|. This is\nstill expensive for problems with a large number of actions\nor agents.\nA further simplifying assumption arises when we assume\nan agent’s action space is unimodal. This is the case when\nwe assume the agent has a single action (e.g. a constant\nvelocity assumption) or we make a hard prediction of the\nmost probable action. This reduce the time complexity of a\nforward safety-checking prediction to O(mT).\nV. SAFETY GUARANTEES\nOne might suspect that our relaxations make it difﬁcult\nto provide any safety guarantees. It does greatly limit the\nstrength of the guarantees we can make, however we can\nstill provide probabilistic guarantees on safety.\nFrom Chebyshev’s inequality we can state that the likeli-\nhood of an agent i taking action j leaving its safety margins\nkσi,j is p[|τ i,j −E(τ i,j)| ≥kσi,j] ≤\n1\nk2 . Where we deﬁne\n|τ i,j −E(τ i,j)| ≡maxk |xi,j\nk −E(xi,j\nk )|. Note that we use the\nweaker Chebyshev inequality for our bounds since according\nto the Fisher−Tippett−Gnedenko theorem the max operation\nresults in a distribution that is not Gaussian.\nSince we generally only care about one-sided error (e.g.\nif the trafﬁc car is further away than predicted, we do not\nrisk a collision) we can shrink the error by a factor of two.\np[τ i,j −E(τ i,j) ≥kσi,j] ≤\n1\n2k2 .\nIn our model, we assume our safety margins create an\nenvelope for an agent’s expected trajectory. Collecting suf-\nﬁcient samples of independent trials, we can assume the\npredicted trajectory roughly models the reachable space of\nthe agent. Now we probe the independence of trials. In\nexpectation the agent follows the mean, but on each trial\nthe deviations are likely not a purely random process, but\nare biased by a response to other agents.\nIn the autonomous driving literature it is assumed that\nagents behave with self-preservation [16]. We will assume\nthat the measured distribution of the trajectory is the sum\nof two normally distributed random processes: the ﬁrst\nassociated with the agent’s control and the second a random\nnoise variable. The measured variance of a trajectory σ2\nM is\nthe sum of controlled σ2\nc and noise σ2\nn variances. We can\nexpress these relative to the measured standard deviation of\nthe trajectory as αcσM and αnσM where α2\nc + α2\nn = 1\nIf we assume an agent controls away from the mean by\nκcαcσM < kσM the probability that an agent leaves its\nsafety margin is p[τ i,j −E(τ i,j) ≥κnαnσM] ≤\n1\n2κ2n ,\nwhere κn = k+κcαc\nαn\n. To put this in concrete terms for an\nautonomous driving scenario, if we assume a 5m measured\nstandard deviation, 4m control standard deviation, 3m noise\nstandard deviation, safety margin of 3σM, and control action\nof 2σc, the resulting safety margin is 7.6σn. This analysis\nneglects any corrective controls of the ego agent. Applying\nthe union bound and assuming a ﬁxed κn for notational\nclarity, we can achieve our desired conﬁdence δ by satisfying\nm\n2κ2n\n< δ .\n(1)\nVI. APPLICATION TO AUTONOMOUS DRIVING\nThere are many works in the autonomous driving literature\nthat look at risk-averse driving [17] and risk assessment [18],\n[17], [19]. Prediction is often used for safety in autonomous\ndriving and accurate prediction models are a current topic\nof research in the autonomous driving community [16].\nSimpler models are built upon kinematic motion models\n[20] with added uncertainty estimates to allow for errors in\nthe measurements and assumptions [21]. These methods are\nlimited in that the Gaussian probability models and kinematic\ntransitions assume cars roughly follow a known trajectory.\nMore sophisticated models allow for multiple maneuvers [22]\nwhich can be done by including road information (either\nheuristically or learned for particular intersections [23]) to\nallow for multiple possible maneuvers. More recent work\nin vehicle prediction is starting to consider the interactions\nbetween multiple vehicles [24], [25].\nRelated work has looked at learning policies for intersec-\ntion handling [26], [27], [28], [29] however these approaches\nare restricted to simulation and do not investigate the issue\nof preserving safety throughout the learning process under\nuncertainty. Using prediction as a safety constraint does not\nnecessarily require additional learning. Accurate prediction\nmodels could be sufﬁcient to create behaviors that enforce\nsafety constraints. A robust vehicle prediction module could\nitself be used to safely navigate intersections:\n1) Predict the movement of the ego car entering the\nintersection in conjunction with forward predictions of\nall other vehicles.\n2) If a collision is predicted, wait.\n3) Otherwise, go.\nThis however assumes a ﬁxed behavior for the ego car - a\nsingle acceleration proﬁle, a set time allowance to enter the\nintersection, and a set safety buffer to leave between cars.\nLimiting the behavior of the autonomous vehicle to a\none-size-ﬁts-all motion is likely to lead to sub optimal\nbehavior. Certain intersections may call for more aggressive\naccelerations to prevent excessive waiting. And the ability\nof other agents to interpret our actions can be just as\nimportant to safety as leaving sizable margins to allow for\nuncertainty. This work sets up a methodology by which we\ncan explore the more nuanced aspects of decision making.\nAs an example we consider learning a model that minimizes\ntrafﬁc disruptions.\nVII. EXPERIMENTS\nTo demonstrate how prediction can be used as a safety\nconstraint, we use deep Q-learning networks (DQNs) to learn\npolicies that optimize aspects of intersection handling on\nautonomous vehicles. We consider two objectives. The ﬁrst\nobjective is to learn an adaptive stand off which seeks to\nincrease the safety margin without compromising the ability\nto make the turn given a ﬁxed time window. The second\nmodel looks at minimizing the disruption to other vehicles\nwhile navigating the intersection in the given time.\nA. Prediction\nWe model trafﬁc vehicles using a constant velocity as-\nsumption based our Kalman ﬁlter estimates of the detected\nvehicle. Each vehicle is modeled with a ﬁxed 2m uncertainty\nin detection. An additional uncertainty per time step is\naccumulated forward in time following a quadratic curve\nwhich was ﬁt to data collected from errors in the forward\nvelocity assumption targeting a margin of six standard devi-\nations. This allows the model to make allowances for some\naccelerations and braking of the trafﬁc vehicles. The ego\ncar has similar forward predictions of its behavior based on\nthe target trajectory and three potential acceleration proﬁles.\nThe prediction errors are smaller for the ego car, since\nthe intentions are known in advance. At each time step,\ngoing forward in time until the ego car has completed the\nintersection maneuver, the predicted position of the ego car is\ncompared against the predicted position of all trafﬁc cars. If\nan overlap of the regions is detected, the action is marked as\nunsafe. Actions that are marked as safe are passed on to the\nnetwork as permissible actions. If there are no permissible\nactions, or the network chooses to wait, the system waits at\nthe intersection. Otherwise the vehicle moves forward with\nthe selected acceleration until it reaches its target speed.\nB. Simulation\nExperiments were run using the Sumo simulator [30],\nwhich is an open source trafﬁc simulation package. To\nsimulate trafﬁc in Sumo, users have control over the types\nof vehicles, road paths, vehicle density, and departure times.\nTrafﬁc cars follow the Intelligent Driver Model (IDM) [31]\nto control their motion. In Sumo, randomness is simulated\nthrough driver imperfection models (based on the Krauss\nstochastic driving model [32]). The simulator runs based on\na predeﬁned time interval which controls the length of every\nstep. For our experiments we use 0.2 second time step.\nEach lane has a 30 mile per hour (13.4 m/s) max speed.\nThe car begins from a stopped position. The maximum\nnumber of steps per trial is capped at 100 steps, which\nis equivalent to 20 seconds, starting from the ﬁrst time\nprediction says a safe action is possible. This guarantees that\na safe action is always possible in the allotted time. We use\na 0.1 probability that a vehicle will be emitted per second to\nset the trafﬁc density for our experiments.\nThe DQN architecture is modeled after the network pre-\nsented in [12]. The simulator is designed to see cars 100m in\neither direction. This was selected to correspond to 25mph\nintersections. If we assume that it takes roughly 5 seconds\nto enter an intersection from a stopped position (measured\nfrom human demonstrations), we would like to detect trafﬁc\nat a minimum of 55m from the intersection. This minimum\nincreases to 75m when we allow for trafﬁc vehicles that\ntravel slightly above the speed limit. The remaining 25m\nprovide an added buffer to allow for accurate detection and\ntracking. The IBEO sensors we use on the real vehicle are\nspeciﬁed at 200m max range. The representation bins the\ntrafﬁc car positions into 26 bins per lane. Each lane is\ndepicted as a separate row. Each spatial pixel, if occupied,\ncontains the normalized real valued heading angles, velocity,\nand binary indicator.\nThe network has four outputs corresponding to wait and\ngo commands where go can select from three different accel-\nerations (0.5, 1.0, and 1.5 m/s2). The network is optimized\nusing the RMSProp algorithm [33].\nEach network was trained on 20, 000 simulations. When\nlearning a network that seeks to minimize braking. The per\ntrial reward is +1 for successfully navigating the intersection\nwith a −0.1 penalty applied for every time step a trafﬁc\nvehicle was braking.\nWhen learning a behavior that seeks to maximize the\nsafety margin, the per trial reward is\nr =\n(\n0.1(d −10),\nif success\nz,\nif timeout\nWhere d is the minimum distance the ego car gets to\na trafﬁc vehicle during the trial. d can be a maximum of\n50m and the minimum observed distance during training\nis 4m. We conduct experiments with different z values\nz = {−1, −5, −10} to study the affect on timeouts.\nFig. 4.\nLeft: Minimum distance to trafﬁc vehicles throughout the training process. Center: Comparison of the minimum distance to trafﬁc vehicles using\ndifferent policies. Net B is trained to minimize braking, net M is trained to maximize the minimum distance. Right: The amount of time steps trafﬁc cars\nspend braking. We assume the ego vehicle is responsible for all trafﬁc braking.\nTo evaluate the learned models we use two metrics: the\naverage number of time steps per trial a trafﬁc vehicle is\nbraking, and the minimum distance between the ego car\nand the closest trafﬁc car per trial. Statistics are collected\nover 1000 trials. Since both objectives could be improved by\nincreasing the safety margin, we also conduct an experiment\nwhere we increase the safety margin of a rule-based only\nagent to ensure the learned policy gives us an improvement.\nC. Real vehicle\nWe train in simulation and verify the learned policy on an\nautonomous vehicle. We collected data from an autonomous\nvehicle in Mountain View, California, at an unsigned T-\njunction, where the objective is to make a left turn. A\npoint cloud, obtained from six IBEO Lidar sensors, is ﬁrst\npre-processed to remove points that reside outside the road\nboundaries. A clustering of the Lidar points with hand-tuned\ngeometric thresholds is combined with the output from three\nDelphi radars to create the estimates for vehicle detection.\nEach vehicle is tracked by a separate particle ﬁlter. Figure 1\ndepicts an intersection where our algorithm was evaluated.\nWe use the network trained to maximize the safety margin.\nVIII. RESULTS\nUsing prediction as a constraint, we trained a DQN to\nminimize distance to other trafﬁc vehicles. There were no\nrecorded collisions during the entire training process. The\nleft plot in Figure 4 shows how the minimum distance to\ntrafﬁc vehicles changes throughout the learning process when\nz = −1. Timeouts are shown as having a distance of -\n1 and are colored black. The minimum distance of each\nindividual trial is plotted as a point. The moving average\nusing a sliding window of 200 trials is shown in dark red.\nWe see the concentration of points with a distance of less\nthan 10m disappears at around 7000 training iterations, and\nthere is an increased density in the region of 20m. Training\ndoes increase the number of timeouts. The extent of this can\nbe changed by adjusting the penalty for timeouts, see Figure\n5. Note that larger penalties produce larger gradients which\ncan have an adverse affect on the learning process so there\nis a limit to how large the penalty can be set.\nThe number of trials that have a minimum distance of\n50m or more also increases. This is more clear in Figure 5\nwhere we show a histogram of the distances before and after\ntraining. Figure 6 shows that naively increasing the safety\nmargin with a rule-based strategy using prediction gives\nmuch worse performance compared to the learned networks.\nThe network we trained to minimize braking should leave\na large margin when moving in front of a car, however\nit may come up very close behind a car. We see that\nthis is the case in center plot of Figure 4. As expected\nthe network trained to maximize the minimum distance\n(Net M) greatly increases the average minimum distance.\nThe network trained to minimize braking does achieve a\nlarger average distance than a random policy acting in the\nsafety constrained prediction framework, but the difference\nis much less pronounced. The network that was trained to\nmaximize distance should also reduce braking. This result\ncan be seen in right plot of Figure 4. Again we see the\nnetwork speciﬁcally designed to minimize braking is better\nat satisfying the objective. The fact that these two objectives\nproduce different behaviors makes sense. If we think in terms\nof the gap between two cars, it would be optimal to be in\nthe middle of the gap if maximizing distance and the front\nof the gap if minimizing braking of other vehicles.\nQualitatively, we observe that the network trained to\nmaximize the safety margin will often add short delays after\nprediction determines the situation is safe if trafﬁc is sparse.\nIn cases where trafﬁc is more dense, the network is more\nlikely to move the moment an opportunity presents itself.\nIX. CONCLUSION\nIn this work we present a framework for safe RL using\npredictions to mask unsafe actions. We apply this method-\nology to an autonomous driving domain to learn policies\nthat improve the performance of unsigned intersection han-\ndling. Speciﬁcally we look at 1) minimizing disruption to\nother vehicles and 2) maximizing safety margins while still\nnavigating the intersection in a ﬁxed time window.\nWhile the safety guarantees we can make using prediction\nare not as strong as other approaches proposed in the\nliterature, the framework is more general and likely more\napplicable to many real world applications. Since we are\nmasking actions, some of which we know to be safe in order\nto provide safety margins when dealing with uncertainty,\nFig. 5.\nComparison of the effect of the timeout penalty on the network\ntrained to maximize the minimum distance. Timeouts are shown in black.\nFig. 6.\nComparison between the networks trained with different timeout\npenalties, and a rule-based method with a ﬁxed safety margin. The safety\nmargin for the rule-based method is varied across trials.\nthe ﬁnal policies are possibly suboptimal. This suggests\nopen problems both related to developing more sophisticated\nprediction modules and a more careful characterization of the\nregret associated with them.\nREFERENCES\n[1] J. Garcıa and F. Fern´andez, “A comprehensive survey on safe rein-\nforcement learning,” Journal of Machine Learning Research, vol. 16,\nno. 1, pp. 1437–1480, 2015.\n[2] R. A. Howard and J. E. Matheson, “Risk-sensitive markov decision\nprocesses,” Management science, vol. 18, no. 7, pp. 356–369, 1972.\n[3] M. Heger, “Consideration of risk in reinforcement learning,” in Ma-\nchine Learning Proceedings 1994.\nElsevier, 1994, pp. 105–111.\n[4] Z. C. Lipton, J. Gao, L. Li, J. Chen, and L. Deng, “Combating\nreinforcement learning’s sisyphean curse with intrinsic fear,” arXiv\npreprint arXiv:1611.01211, 2016.\n[5] P. Geibel and F. Wysotzki, “Risk-sensitive reinforcement learning\napplied to control under constraints.” J. Artif. Intell. Res.(JAIR),\nvol. 24, pp. 81–108, 2005.\n[6] A. Hans, D. Schneegaß, A. M. Sch¨afer, and S. Udluft, “Safe explo-\nration for reinforcement learning.” in ESANN, 2008, pp. 143–148.\n[7] M. Wen, R. Ehlers, and U. Topcu, “Correct-by-synthesis reinforcement\nlearning with temporal logic constraints,” in Intelligent Robots and\nSystems (IROS), 2015 IEEE/RSJ International Conference on.\nIEEE,\n2015, pp. 4983–4990.\n[8] M. Wen and U. Topcu, “Probably approximately correct learning in\nstochastic games with temporal logic speciﬁcations.” in IJCAI, 2016,\npp. 3630–3636.\n[9] M. Alshiekh, R. Bloem, R. Ehlers, B. K¨onighofer, S. Niekum, and\nU. Topcu, “Safe reinforcement learning via shielding,” AAAI Confer-\nence on Artiﬁcial Intelligence (AAAI), 2018.\n[10] S. Shalev-Shwartz, S. Shammah, and A. Shashua, “Safe, multi-\nagent, reinforcement learning for autonomous driving,” arXiv preprint\narXiv:1610.03295, 2016.\n[11] J. H. Gillula and C. J. Tomlin, “Reducing conservativeness in safety\nguarantees by learning disturbances online: iterated guaranteed safe\nonline learning,” Robotics: Science and Systems VIII, p. 81, 2013.\n[12] D. Isele, R. Rahimi, A. Cosgun, K. Subramanian, and K. Fujimura,\n“Navigating occluded intersections with autonomous vehicles using\ndeep reinforcement learning,” IEEE International Conference on\nRobotics and Automation (ICRA), 2018.\n[13] T. M. Moldovan and P. Abbeel, “Safe exploration in markov decision\nprocesses,” arXiv preprint arXiv:1205.4810, 2012.\n[14] M. L. Littman, “Markov games as a framework for multi-agent\nreinforcement learning,” in Machine Learning Proceedings 1994.\nElsevier, 1994, pp. 157–163.\n[15] J. Hu and M. P. Wellman, “Nash q-learning for general-sum stochastic\ngames,” Journal of machine learning research, vol. 4, no. Nov, pp.\n1039–1069, 2003.\n[16] S. Lef`evre, D. Vasquez, and C. Laugier, “A survey on motion predic-\ntion and risk assessment for intelligent vehicles,” Robomech Journal,\nvol. 1, no. 1, p. 1, 2014.\n[17] F. Damerow and J. Eggert, “Risk-aversive behavior planning under\nmultiple situations with uncertainty,” in Intelligent Transportation\nSystems (ITSC), 2015 IEEE 18th International Conference on. IEEE,\n2015, pp. 656–663.\n[18] S. Lef`evre, D. Vasquez, C. Laugier, and J. Iba˜nez-Guzm´an, “Intention-\naware risk estimation: Field results,” in Advanced Robotics and its\nSocial Impacts (ARSO), 2015 IEEE International Workshop on. IEEE,\n2015, pp. 1–8.\n[19] S. Brechtel, T. Gindele, and R. Dillmann, “Probabilistic mdp-behavior\nplanning for cars,” in Intelligent Transportation Systems (ITSC), 2011\n14th International IEEE Conference on. IEEE, 2011, pp. 1537–1542.\n[20] R. Schubert, E. Richter, and G. Wanielik, “Comparison and evaluation\nof advanced motion models for vehicle tracking,” in Information\nFusion.\nIEEE, 2008, pp. 1–6.\n[21] A. Carvalho, Y. Gao, S. Lefevre, and F. Borrelli, “Stochastic predictive\ncontrol of autonomous vehicles in uncertain environments,” in 12th\nInternational Symposium on Advanced Vehicle Control, 2014.\n[22] G. Aoude, J. Joseph, N. Roy, and J. How, “Mobile agent trajectory\nprediction using bayesian nonparametric reachability trees,” Proc. of\nAIAA Infotech@ Aerospace, pp. 1587–1593, 2011.\n[23] T. Streubel and K. H. Hoffmann, “Prediction of driver intended path\nat intersections,” in Intelligent Vehicles Symposium Proceedings, 2014\nIEEE.\nIEEE, 2014, pp. 134–139.\n[24] G. Agamennoni, J. I. Nieto, and E. M. Nebot, “Estimation of\nmultivehicle dynamics by considering contextual information,” IEEE\nTransactions on Robotics, vol. 28, no. 4, pp. 855–870, 2012.\n[25] A. Kueﬂer, J. Morton, T. Wheeler, and M. Kochenderfer, “Imitating\ndriver behavior with generative adversarial networks,” in Intelligent\nVehicles Symposium (IV), 2017 IEEE.\nIEEE, 2017, pp. 204–211.\n[26] W. Song, G. Xiong, and H. Chen, “Intention-aware autonomous driv-\ning decision-making in an uncontrolled intersection,” Mathematical\nProblems in Engineering, vol. 2016, 2016.\n[27] T. Gindele, S. Brechtel, and R. Dillmann, “Learning context sensitive\nbehavior models from observations for predicting trafﬁc situations,”\nin Intelligent Transportation Systems-(ITSC), 2013 16th International\nIEEE Conference on.\nIEEE, 2013, pp. 1764–1771.\n[28] D. Isele and A. Cosgun, “Selective experience replay for lifelong\nlearning,” AAAI Conference on Artiﬁcial Intelligence (AAAI), 2018.\n[29] M. Bouton, A. Cosgun, and M. J. Kochenderfer, “Belief state planning\nfor navigating urban intersections,” IEEE Intelligent Vehicles Sympo-\nsium (IV), 2017.\n[30] D. Krajzewicz, J. Erdmann, M. Behrisch, and L. Bieker, “Recent\ndevelopment and applications of SUMO–simulation of urban mobil-\nity,” International Journal on Advances in Systems and Measurements\n(IARIA), vol. 5, no. 3–4, 2012.\n[31] M. Treiber, A. Hennecke, and D. Helbing, “Congested trafﬁc states in\nempirical observations and microscopic simulations,” Physical Review\nE, vol. 62, no. 2, p. 1805, 2000.\n[32] S. Krauss, “Microscopic modeling of trafﬁc ﬂow: Investigation of col-\nlision free vehicle dynamics,” Ph.D. dissertation, Deutsches Zentrum\nfuer Luft-und Raumfahrt, 1998.\n[33] T. Tieleman and G. Hinton, “Lecture 6.5-rmsprop, coursera: Neural\nnetworks for machine learning,” University of Toronto, Tech. Rep,\n2012.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.RO",
    "stat.ML"
  ],
  "published": "2019-09-27",
  "updated": "2019-09-27"
}